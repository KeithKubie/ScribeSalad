WEBVTT
Kind: captions
Language: en

00:00:05.180 --> 00:00:07.840
Facebook first launched in 2004.

00:00:07.840 --> 00:00:10.020
YouTube came out in 2005.

00:00:10.020 --> 00:00:12.280
Twitter and Spotify: 2006.

00:00:12.280 --> 00:00:14.920
The first iPhone was released in 2007.

00:00:14.920 --> 00:00:18.180
Snapchat launched in 2011 and so did Siri.

00:00:18.180 --> 00:00:20.410
Tinder was founded in 2012.

00:00:20.410 --> 00:00:23.289
Google Glass was first released in 2013.

00:00:23.289 --> 00:00:26.940
Amazon’s Echo smart speaker came out in
2015.

00:00:26.940 --> 00:00:31.820
From social media to smartphones to augmented
reality devices to smart speakers.

00:00:31.820 --> 00:00:36.580
All of these inventions have changed how we
interact with each other, and especially with media.

00:00:36.580 --> 00:00:40.660
And yet, they’re all just getting started
– Well, maybe not Google Glass.

00:00:40.660 --> 00:00:42.220
It just wasn’t your time, man.

00:00:42.220 --> 00:00:48.460
Virtual reality and artificial intelligence
are already starting to work their way into
our media diets little by little.

00:00:48.460 --> 00:00:50.200
You thought internet ads were annoying?

00:00:50.210 --> 00:00:54.870
Wait until they can follow you around from
billboard to billboard thanks to facial recognition.

00:00:54.870 --> 00:00:57.960
You thought it was hard to decipher fabricated
news from truth?

00:00:57.960 --> 00:01:01.560
Wait until even videos can be Photoshopped.
Er...Videoshopped?

00:01:01.560 --> 00:01:05.680
Media literacy doesn’t just mean learning
how to navigate today’s media landscape.

00:01:05.680 --> 00:01:08.460
It means preparing yourself for tomorrow’s,
too.

00:01:08.460 --> 00:01:12.320
Today we’re talking all about literacies
of the future.

00:01:12.320 --> 00:01:22.500
[Theme Music]

00:01:22.500 --> 00:01:28.080
This is our last episode of Crash Course
Media Literacy, and I want to thank you all for
joining us this far.

00:01:28.080 --> 00:01:33.380
During this series we’ve made a lot of references to
new media – computers, the internet, social media –

00:01:33.380 --> 00:01:38.340
and how they change or challenge our traditional
relationships to media and media literacy.

00:01:38.340 --> 00:01:47.180
Today we’re going to really dig into that, and talk
about two new forms of literacy that promise to shape
the future: Data Literacy and Algorithmic Literacy.

00:01:47.180 --> 00:01:53.320
OK. First: Data literacy, as you might have
guessed from the name, deals with understanding
and analyzing data.

00:01:53.320 --> 00:01:55.160
And there’s a lot of data out there.

00:01:55.160 --> 00:02:01.200
You may have heard of “raw data” or “big
data” or, if you have a fitbit, “personal fitness
tracking data.”

00:02:01.200 --> 00:02:02.600
So what is data?

00:02:02.600 --> 00:02:07.900
For our purposes, data is: information about
the world that is stored in a specific format.

00:02:07.900 --> 00:02:09.740
And this is a pretty broad definition.

00:02:09.750 --> 00:02:14.190
With every passing year, online companies
think up more and more ways to track us.

00:02:14.190 --> 00:02:16.720
Links, cookies, IP addresses.

00:02:16.720 --> 00:02:21.940
Quite simply, every time you “step out”
onto the web, you leave a huge path of data
behind you.

00:02:21.940 --> 00:02:28.840
If you go back and look at the coverage of the
2016 U.S. election  you’ll find countless articles
about data collected from people:

00:02:28.840 --> 00:02:31.720
their preferences, their values, their political party.

00:02:31.720 --> 00:02:36.160
When data gets used this way, it’s often
to give an argument of sense of being scientific.

00:02:36.160 --> 00:02:39.820
But just because data exist, doesn’t mean
it’s accurate, or helpful.

00:02:39.820 --> 00:02:43.200
Humans are flawed. We have biases.
We have agendas.

00:02:43.200 --> 00:02:44.860
And humans make the data.

00:02:44.860 --> 00:02:47.560
It is not neutral, because we are not neutral.

00:02:47.560 --> 00:02:52.320
Say you see in a magazine that 30% of Americans
love chocolate ice cream the most.

00:02:52.320 --> 00:02:54.760
Ok, well what did everyone else like the most?

00:02:54.760 --> 00:02:56.220
Were they undecided?

00:02:56.220 --> 00:03:00.080
Did they prefer vanilla or chunky monkey or
cotton candy?

00:03:00.080 --> 00:03:05.160
Maybe 30% isn’t even the majority, and 65%
of people like peanut butter ice cream the best.

00:03:05.160 --> 00:03:07.640
Data only matters in context.

00:03:07.640 --> 00:03:09.420
And it can be helpful in many ways.

00:03:09.430 --> 00:03:13.349
It helps us track everything from personal
fitness goals to citywide poverty levels.

00:03:13.349 --> 00:03:19.840
It’s just very easy to misconstrue, because
humans are susceptible to nice, wholesome,
easy to believe numbers.

00:03:19.840 --> 00:03:25.400
I said we’d come back to how your personal data can
be used, and that brings us to: Algorithmic Literacy.

00:03:25.400 --> 00:03:30.360
Algorithms are basically sets of instructions
or calculations for a computer to run.

00:03:30.360 --> 00:03:36.500
Websites and apps take the data they collect
about us and send it through an algorithm,
and out pops some result.

00:03:36.500 --> 00:03:42.780
Like Facebook. It takes all your personal data
and what you’ve liked and shared and – serves
you ads, yes.

00:03:42.780 --> 00:03:47.800
But it also sticks info into an algorithm
to decide what appears in your news feed.

00:03:47.800 --> 00:03:52.760
Its goal is to keep you on Facebook, so it
shows you things it thinks you will really like.

00:03:52.760 --> 00:03:54.680
That’s why there’s no “dislike” button.

00:03:54.680 --> 00:03:58.420
The way algorithms personalize stuff for us
can be fun and useful.

00:03:58.420 --> 00:04:03.000
Plus it makes us feel a little special, like
those Christmas ornaments in the store that
have your name on them.

00:04:03.000 --> 00:04:07.320
It also makes us feel comfortable, like we’re
in our own little world of happy things.

00:04:07.320 --> 00:04:08.940
That’s because often, we are.

00:04:08.940 --> 00:04:17.000
Eli Pariser calls this the filter bubble, the media
world we create where we only see and interact
with things and people we already like.

00:04:17.000 --> 00:04:21.140
Sometimes we do this to ourselves by curating
feeds with our own interests in mind.

00:04:21.140 --> 00:04:25.440
But this also happens behind the scenes,
algorithmically, without our knowledge.

00:04:25.440 --> 00:04:29.820
Facebook might serve up posts from the half
of your friends that like the same things you do.

00:04:29.820 --> 00:04:33.840
Or Google news might show you a mix or articles
they know you’re likely to click on.

00:04:33.840 --> 00:04:39.740
This might be convenient, sure, but it can
also mean seeing a very different version of
the world than the people around you.

00:04:39.740 --> 00:04:47.990
Algorithmic literacy is knowing that any
information you see online is only one slice of the
pie, and one that’s been cut specifically for you.

00:04:47.990 --> 00:04:53.580
One of the most important things about Data and
Algorithmic literacy is that they always go hand in hand.

00:04:53.580 --> 00:04:59.560
And when you know about them, you can start
to ask classical media literacy questions, even
of the newest of media.

00:04:59.560 --> 00:05:06.020
It’s impossible to know what new media
technology we’re going to be dealing with next
year, or next decade, or next century.

00:05:06.020 --> 00:05:08.740
And there will always be new complications
to learn about.

00:05:08.740 --> 00:05:14.900
But no matter what form of future literacy
you develop, it’s sure to rest on the same
basic principle: Skepticism.

00:05:14.900 --> 00:05:20.280
So as we wrap up this series, and leave
you staring into the uncertain future, let us
leave you with this:

00:05:20.280 --> 00:05:24.440
Being skeptical means approaching everything
by questioning its truth.

00:05:24.440 --> 00:05:28.960
Every ad, every song, every book, every article
– everything

00:05:28.960 --> 00:05:32.040
Being skeptical doesn’t mean taking the
fun out of all media;

00:05:32.040 --> 00:05:38.680
it just means that instead of blindly accepting
whatever’s thrown your way, a little voice in your
head says, “But what about…”

00:05:38.680 --> 00:05:42.580
Our brains love to play little games with
the media. They love the familiar.

00:05:42.580 --> 00:05:45.960
They love things with easy explanations.
They love taking shortcuts.

00:05:45.960 --> 00:05:49.560
They even love believing things we’ve heard
already, even if they’re not true.

00:05:49.560 --> 00:05:56.680
Skepticism, adding in a dash of logic and
context to our media interactions, helps fight
our brains’ annoying habits.

00:05:56.680 --> 00:05:58.880
You know that saying, “follow your gut?”

00:05:58.880 --> 00:06:01.590
That is the opposite of what you should do
with media.

00:06:01.590 --> 00:06:07.180
“Follow your perception of bias and textual
analysis skills” should be the saying; it’s just
not as catchy.

00:06:07.180 --> 00:06:12.880
Media consumers fall into traps all the time
because they like when things are comfortable,
certain, and easy.

00:06:12.880 --> 00:06:15.000
But the world is not always comfortable.

00:06:15.000 --> 00:06:21.220
Nothing is certain but the fact that Prince
is greatest musician of all time, and that
life is rarely easy.

00:06:21.220 --> 00:06:27.460
Once you come to terms with the fact that
every bit of media isn’t as simple as it looks,
you’re that much closer to understanding it.

00:06:27.460 --> 00:06:34.180
Over the past dozen lessons, we’ve learned the
history of media literacy, how to read advertisements
and their darker cousin, propaganda.

00:06:34.180 --> 00:06:37.680
We have looked at our minds on media and our
media on money.

00:06:37.690 --> 00:06:41.949
We’ve explored media law and how to break
it, and even looked into the future of literacy.

00:06:41.949 --> 00:06:44.660
We shape the media as much as it shapes us.

00:06:44.660 --> 00:06:49.020
Our reactions to media are just as important
as what’s thrown our way.

00:06:49.020 --> 00:06:53.880
We’re all media creators now, like it or
not, and we’re certainly all consumers.

00:06:53.880 --> 00:06:59.820
Everyone has a role to play, and a responsibility
to share, and we all need to do it together.

00:06:59.830 --> 00:07:02.250
I’m glad you’ve joined us on this journey.

00:07:02.250 --> 00:07:06.300
Until next time, I’m Jay Smooth and this
has been Crash Course: Media Literacy.

00:07:06.300 --> 00:07:10.370
Crash Course Media Literacy is filmed in the
Dr. Cheryl C. Kinney Studio in Missoula, MT.

00:07:10.370 --> 00:07:14.160
It’s made with the help of all of these nice
people, and our animation team is Thought Cafe.

00:07:14.160 --> 00:07:16.060
Crash Course is a Complexly production.

00:07:16.060 --> 00:07:19.430
If you wanna keep imagining the world complexly
with us check out some of our other channels,

00:07:19.430 --> 00:07:22.500
like The Financial Diet, SciShow Space, and
Mental Floss.

00:07:22.500 --> 00:07:26.660
If you'd like to keep Crash Course free for
everyone, forever, you can support the series
at Patreon,

00:07:26.660 --> 00:07:29.800
a crowdfunding platform that allows
you to support the content you love.

00:07:29.800 --> 00:07:33.580
Thank you to all of our patrons for making
Crash Course possible with their continued
support.

