WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.008
So Thrust gives a host-side interface that can replace writing CUDA kernels in many cases.

00:00:05.008 --> 00:00:07.390
But for those times when you are writing CUDA kernels

00:00:07.390 --> 00:00:10.248
you'd still like to achieve software reuse.

00:00:10.248 --> 00:00:12.764
You'd like to identify the building blocks in your kernel

00:00:12.764 --> 00:00:16.486
and use somebody else's highly optimized implementation of those building blocks

00:00:16.486 --> 00:00:19.600
and then stitch them together with any custom code of your own.

00:00:19.600 --> 00:00:23.444
But software reuse in CUDA kernel code faces some kind of special challenges.

00:00:23.444 --> 00:00:29.044
So suppose, for example, that you are calling a library implementation of, say, radix sort,

00:00:29.044 --> 00:00:33.078
and this is somebody else's implementation; you're going to call it from inside your kernel.

00:00:33.078 --> 00:00:37.636
And so the question is, how does that implementation know how many threads it is running in?

00:00:37.636 --> 00:00:40.488
How much shared memory is it allowed to use?

00:00:40.488 --> 00:00:43.872
Can it use all of the shared memory, or have you reserved some for other purposes?

00:00:43.872 --> 00:00:48.121
Should it use a work-efficient or a step-efficient algorithm for this piece of the problem?

00:00:48.121 --> 00:00:51.130
The answer probably depends on what you're sorting, how much you're sorting,

00:00:51.130 --> 00:00:53.698
and exactly what's important to you right now.

00:00:53.698 --> 00:00:57.808
The CUB library, by Duane Merrill, provides a neat solution

00:00:57.808 --> 00:01:02.309
to these challenges of software reuse for CUDA kernel code at high performance.

00:01:02.309 --> 00:01:06.841
CUB stands for CUDA Unbound, and the reason it's called that

00:01:06.841 --> 00:01:09.788
is because the implementation in CUB leaves these decisions,

00:01:09.788 --> 00:01:13.195
like the number of threads and the amount of shared memory, unbound.

00:01:13.195 --> 00:01:16.929
You the programmer bind the necessary parameters when you write your program

00:01:16.929 --> 00:01:21.770
using templates and a novel binding approach that's embedded in the C++ type system.

00:01:21.770 --> 00:01:26.933
The building blocks that CUB provides are really high-performance implementations.

00:01:26.933 --> 00:01:30.738
So by stitching them together you can create a high-performance CUDA kernel.

00:01:30.738 --> 00:01:34.454
And finally, because CUB doesn't dictate things like thread block size

00:01:34.454 --> 00:01:37.614
or shared memory usage, it allows you to experiment with these values

00:01:37.614 --> 00:01:40.083
as you optimize or auto-tune your code.

00:01:40.083 --> 00:01:42.785
And one thing you'll find if you get into optimizing CUDA code

00:01:42.785 --> 00:01:45.723
is that this kind of auto-tuning where you experiment with

00:01:45.723 --> 00:01:50.340
or even automatically sweep the parameters of your code,

00:01:50.340 --> 00:01:52.356
things like the block size and the shared memory size,

00:01:52.356 --> 00:01:55.491
that's a really powerful tool for extracting the maximum amount of performance

00:01:55.491 --> 00:01:59.198
from your code without investing a whole lot of personal effort.

