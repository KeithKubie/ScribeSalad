WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:08.610
&gt;&gt; SLATKIN: Test, test, hello, hello, hey.
How is it going? All right, I think we're

00:00:08.610 --> 00:00:14.580
going to get started. So here, we're still
trickling in but we're all a little bit behind.

00:00:14.580 --> 00:00:18.400
Alpha just reminded me if you could please
turn off your Bluetooth devices. I'd really

00:00:18.400 --> 00:00:21.880
appreciate it. No, [INDISTINCT]
&gt;&gt; [INDISTINCT] box.

00:00:21.880 --> 00:00:30.200
&gt;&gt; SLATKIN: Yeah, yeah, let's [INDISTINCT]
box. So, all right, okay, my name is Brett

00:00:30.200 --> 00:00:35.280
Slatkin. I'm a software engineer on Google
App Engine Team. I'm going to tell you today

00:00:35.280 --> 00:00:41.620
about how to build a high-throughput data
pipelines on Google App Engine. And be sure,

00:00:41.620 --> 00:00:45.750
you know, either--even if you don't stay for
this talk, directly following my talk is another

00:00:45.750 --> 00:00:49.020
talk from Mike Aizatsky, who's also in the
team, that'll be talking about the new Mapper

00:00:49.020 --> 00:00:53.840
API and our kind of a report generation tools.
This is really a good talk. You should check

00:00:53.840 --> 00:01:02.500
out also. So you can view live notes TinyURL
App Engine pipelines or this in my blog, or

00:01:02.500 --> 00:01:07.270
you can follow me. So go on there. We also
have a moderator page for questions, so that

00:01:07.270 --> 00:01:13.720
you can vote on the questions you want to
ask. Okay, so here's the agenda for today.

00:01:13.720 --> 00:01:18.399
So, I'm going to do a little intro. I'm going
to talk about fan-out. I'm going to talk about

00:01:18.399 --> 00:01:24.260
transactional sequences, what those are? We're
going to talk about fan-in, it's different.

00:01:24.260 --> 00:01:29.020
And we're going to have a little bonus round
in there because I wanted to do more. And

00:01:29.020 --> 00:01:33.270
then, we'll talk about the future directions
of what's coming next and where we're going

00:01:33.270 --> 00:01:41.390
next. So, this is the wilderness. I--every
year, I get to go of into the wilderness and

00:01:41.390 --> 00:01:46.920
kind to think about things and learn about
new ways to count, basically for you. I started

00:01:46.920 --> 00:01:51.490
some sharded counters and some right behind
cache. And it seems like every year, I kind

00:01:51.490 --> 00:01:56.580
of figure out a new way to count things. And
that's because counting is actually represented

00:01:56.580 --> 00:02:01.170
with a very difficult problem. So a lot of
my example--and it's also a difficult problem

00:02:01.170 --> 00:02:05.369
if people really easily understand. So, you
know, come back from the wilderness about

00:02:05.369 --> 00:02:10.220
a year of experience here, messing around
with data pipelines because of my 20% project.

00:02:10.220 --> 00:02:15.940
It's called the PubSubHubbub which is a protocol
in system for making feeds real-time. So I've

00:02:15.940 --> 00:02:20.930
had a nice chance to learn about how to build
the data pipeline on App Engine where we run

00:02:20.930 --> 00:02:28.110
the Google PubSubHubbub hub. So, I'm back
from the wilderness. So what are pipelines?

00:02:28.110 --> 00:02:31.700
What have I learned? Well, pipelines are a
constant trickle of inputs and outputs. They're

00:02:31.700 --> 00:02:37.620
an assembly line, okay? They're optimized
for the latency of that line. And usually,

00:02:37.620 --> 00:02:42.160
you want it to be on the order of seconds,
right? And the whole idea is to minimize the

00:02:42.160 --> 00:02:46.040
incremental cost as much as possible, right?
That's how I would define a pipeline. They're

00:02:46.040 --> 00:02:50.240
also not supposed to be lossy. So it's not
like halfway through the assembly line you

00:02:50.240 --> 00:02:54.320
lose a car or something like that. So in the
same sense, you don't want to [INDISTINCT]

00:02:54.320 --> 00:02:58.250
want to ever lose your data. You want to make
sure that all inputs are served if they're

00:02:58.250 --> 00:03:05.620
supposed to be served. So what is not a pipeline?
Offline systems like MapReduce, batch-processing,

00:03:05.620 --> 00:03:11.819
any kind of report generation, you know, anytime
you're doing some kind of output from a snapshot

00:03:11.819 --> 00:03:18.980
of inputs, that's not, you know, not a pipeline.
You know, if you're doing year-end's calculations

00:03:18.980 --> 00:03:24.270
for a finance, that's not a pipeline. That's
just something you do at the end of the year,

00:03:24.270 --> 00:03:28.670
all right? And in that case, the latency from
input to output is on the order of hours or

00:03:28.670 --> 00:03:35.760
days, not on, you know, in seconds, so to
differentiate. So some actual example apps

00:03:35.760 --> 00:03:40.140
so email delivery, Twitter, Hubbub, all kind
of do routing, message routing, those are

00:03:40.140 --> 00:03:46.140
pipeline systems. They're latency-sensitive.
Reddit and Digg, they do voting ends. High-throughput

00:03:46.140 --> 00:03:49.360
aggregation, that's a pipeline. They want
the time from where you vote to when it goes

00:03:49.360 --> 00:03:54.390
up the page to be very low. And even kind
of customer-relationship management software

00:03:54.390 --> 00:03:58.730
is kind of a pipeline because you want customers
to work--move through a workflow as quickly

00:03:58.730 --> 00:04:03.300
as possible or get passed off between workflow
items as quickly as possible. So there's a

00:04:03.300 --> 00:04:08.230
lot of things you can model as a pipeline
that maybe aren't obvious. Things that aren't

00:04:08.230 --> 00:04:11.470
pipelines. A guestbook application doesn't
really do much at all, so it's very flat.

00:04:11.470 --> 00:04:17.489
Terasort, you know--if you're sorting a ton
of data using MapReduce or some other system,

00:04:17.489 --> 00:04:21.670
that's taking a snapshot of data and operating
on it. It's not a living and breathing system.

00:04:21.670 --> 00:04:26.760
And then chat is transient, you kind of throw
things away. So that's not a pipeline, although

00:04:26.760 --> 00:04:32.940
it has some pipeline aspects. And then hybrid
systems like, you know, basically anything

00:04:32.940 --> 00:04:36.150
with what you're doing, you want--you know,
you're latency-sensitive but you actually

00:04:36.150 --> 00:04:40.820
kind of have to do batch-processing to achieve
it. So, YouTube and Vimeo both do a lot of

00:04:40.820 --> 00:04:44.720
transcoding that takes a lot of time, but
they actually can stream back frames as they're

00:04:44.720 --> 00:04:49.270
doing it. So, they're latency-sensitive but
also require a lot of background processing.

00:04:49.270 --> 00:04:57.570
Flickr and Picasa do this also with face recognition.
Okay, so let's start off with fan-out. So

00:04:57.570 --> 00:05:01.810
this is something that if you've used the
task queue, you kind of have an idea of the

00:05:01.810 --> 00:05:07.060
general design pattern here, the idea of having
continuations. So, fan-out is where you have

00:05:07.060 --> 00:05:11.820
a single action either from a user or from
some other action in your system causing a

00:05:11.820 --> 00:05:17.770
lot of other actions to happen. So a common
use of this is kind of a Datastore-based inbox

00:05:17.770 --> 00:05:21.680
system where, you know, you have one person
posting a record and you want a lot of other

00:05:21.680 --> 00:05:26.530
people to get it, right? That's fan-out, that's
data fan-out. Sometimes, you also want to

00:05:26.530 --> 00:05:31.180
guarantee that notifications get there, so
you want to make sure that, you know, when

00:05:31.180 --> 00:05:35.010
somebody submits an expense report or something
like that that no matter what, they always

00:05:35.010 --> 00:05:40.520
send an email about it, you know, so that
it never gets dropped. Also, making Web service

00:05:40.520 --> 00:05:45.250
calls if you're trying to, you know, communicate
between systems that sometime you might have,

00:05:45.250 --> 00:05:50.110
you know, one task that actually needs to
do about 10 different calls. And then, kind

00:05:50.110 --> 00:05:52.520
of merge the results or enqueueing more tasks
to do more work. Those are kind of fan-out

00:05:52.520 --> 00:06:01.370
tasks where you're branching on multiple patents.
So, here's a really quick example of, you

00:06:01.370 --> 00:06:05.590
know, fan-out. So, let's say that I have some
party that I'm putting on, and I have some

00:06:05.590 --> 00:06:11.840
partygoers who are going to the party. I have
here, a party class, it's my data model. There's

00:06:11.840 --> 00:06:16.199
when the party is and who the host is. And
then, people who are going to the party, so

00:06:16.199 --> 00:06:23.060
a reference to the party, who's going, and
their email address, okay? Really simple.

00:06:23.060 --> 00:06:27.280
So if I have this and then, I want to send
everyone an email, let me just slowly get

00:06:27.280 --> 00:06:32.169
through this. So this is a task request handler.
I'm writing these in Python just because it's

00:06:32.169 --> 00:06:35.550
so much shorter and it's easier for me to
just go line-by-line with you. So, but all

00:06:35.550 --> 00:06:39.780
the stuff you can do in Java, exactly--almost
exactly the same way, just [INDISTINCT] moving

00:06:39.780 --> 00:06:49.860
the white space around. So, that's either
a Python joke or a Java joke depending on

00:06:49.860 --> 00:06:55.080
how you see it, right? Okay, so. First, I
find my party from the task parameters--I

00:06:55.080 --> 00:07:01.930
find the Datastore cursor if there is one.
I start a new query "PartyGoer.all" filter,

00:07:01.930 --> 00:07:06.900
look for people who are going to my party.
And if there is a Datastore cursor, I will

00:07:06.900 --> 00:07:10.921
use the cursor with that query which basically
lets me pick up where I left off in the query,

00:07:10.921 --> 00:07:16.980
okay? And then, I will send some emails, that's
what that quote is or do anything kind of

00:07:16.980 --> 00:07:23.350
fan-out activity, all right, and then, if
I've retrieved 10 partygoers, if you see a

00:07:23.350 --> 00:07:27.680
query.fetch(10), I received 10 items from
the query, and so if I did received 10 then

00:07:27.680 --> 00:07:32.930
there might be more, so then, I enqueue another
task which is a continuation of this task,

00:07:32.930 --> 00:07:36.720
okay? So that's why, it's a continuation,
it's kind of like, you know, maybe if you

00:07:36.720 --> 00:07:40.940
do any kind functional programming, you pass
forward a continuation or a call back, right?

00:07:40.940 --> 00:07:46.040
So if you do this, the idea is that every
time the next task runs, it'll use the next

00:07:46.040 --> 00:07:51.740
cursor which will pick up wherever you left
in your data source scan, do the next little

00:07:51.740 --> 00:07:56.380
bit, then it'll do another task and so on.
The problem is that if the task here ever

00:07:56.380 --> 00:08:02.320
fails, or your app ever fails, you get a fork
bomb. Each task enqueues new tasks, they fail

00:08:02.320 --> 00:08:05.110
right after they enqueue tasks, then they
start again, they enqueue more tasks, you

00:08:05.110 --> 00:08:09.419
actually get this exponential explosion of
tasks. If you've ever done basic continuation

00:08:09.419 --> 00:08:14.370
passing on App Engine and you know, hit an
error, had a syntax error or anything, you

00:08:14.370 --> 00:08:19.270
probably have this happen to you and it's
really terrible. So the task, you know, basically

00:08:19.270 --> 00:08:23.300
the one fails then that leads two, and then
the four and the eight and nine, it's just

00:08:23.300 --> 00:08:28.759
really terrible. So, that's the wrong way
to do it. And it doesn't really take much

00:08:28.759 --> 00:08:34.789
to prevent this from happening. All you have
to do is name your task. That's it. So in

00:08:34.789 --> 00:08:38.370
App Engine, tasks have this property called
the name, you get to assign it. Here, I'm

00:08:38.370 --> 00:08:43.159
just assigning it a sequence number. I'm saying
the generation numbers starts at 0. So the

00:08:43.159 --> 00:08:48.809
first task is generation 0 then I do 1, 2,
3, and I add 1, okay? So, it's exactly the

00:08:48.809 --> 00:08:52.860
same code like do the query, figure out what
you're going to do, do the work for fan-out,

00:08:52.860 --> 00:09:00.529
then enqueue another task, okay? But you also
notice at the bottom, I'm sending some emails

00:09:00.529 --> 00:09:06.759
after, I enqueue the next task as supposed
of before. And the reason I do that is because

00:09:06.759 --> 00:09:10.589
it lets me actually get more parallelism.
Essentially, I'm querying for the work to

00:09:10.589 --> 00:09:15.069
do. I figure out where the endpoint is and
then, I immediately enqueue the next task

00:09:15.069 --> 00:09:18.730
to do the same thing before sending my emails,
because sending my email is a high latency

00:09:18.730 --> 00:09:23.550
activity, right? So I should be able to, you
know, pass the baton to the next task to find

00:09:23.550 --> 00:09:27.779
the next people to send the emails to and
then, still do my work. So what you end up

00:09:27.779 --> 00:09:32.351
with is this nice kind of cascade of tasks
where each one queries the next little bit

00:09:32.351 --> 00:09:35.980
of work then enqueues the next one, and then
so on and so forth. So they actually will

00:09:35.980 --> 00:09:42.449
be overlapping as they execute, which is really
nice. And because you've named them, our task

00:09:42.449 --> 00:09:46.649
queue system on App Engine has the ability
to tombstone tasks. That means that you can

00:09:46.649 --> 00:09:52.990
only add a task with the same name once every
7 days. That's how it works. So if you re-add

00:09:52.990 --> 00:10:01.139
the same task, it'll raise an exemption to
you and say, "You have already added this

00:10:01.139 --> 00:10:05.189
task." The other thing is that, you know,
besides just your own coding mistakes, the

00:10:05.189 --> 00:10:09.139
task queued isn't a perfect system, it might
run task more than once. That's part of the

00:10:09.139 --> 00:10:13.559
kind of scalability trade-offs that we've
made with the system like that. So, this kind

00:10:13.559 --> 00:10:16.860
of gives you a level of idempotents on your
tasks. Basically, you're saying, "Hey, if

00:10:16.860 --> 00:10:20.329
this task gets added multiple times, that's
all right." So in this case, you know, task

00:10:20.329 --> 00:10:27.279
1 and task 3 spuriously have run and they
maybe send new emails to that small number

00:10:27.279 --> 00:10:31.459
of people but they don't cause a fork bomb.
They don't cause an exponential number of

00:10:31.459 --> 00:10:38.040
additional tasks to be added, okay? So, every
time, you know, say task 1 spuriously gets

00:10:38.040 --> 00:10:41.850
called, it'll enqueue task 2 and it'll--and
that will return an error and saying, "This

00:10:41.850 --> 00:10:46.670
task query exists, so don't do the next piece
of work." So this is the way to do a continuation

00:10:46.670 --> 00:10:50.670
passing. It's basically, figure out the cursor,
add the task, and then actually do the work

00:10:50.670 --> 00:10:56.839
you want to do with names. So that's--like
I was saying, this isolates the failures and

00:10:56.839 --> 00:11:02.589
your spurious retries, and lets you execute
the work in parallel, which is great. The

00:11:02.589 --> 00:11:07.089
other thing is that it works really well in
asynchronous APIs. So, if you ever have a

00:11:07.089 --> 00:11:09.369
situation where you have high-latency, you
want to iterate through a bunch of things

00:11:09.369 --> 00:11:13.170
in parallel, and each one takes about 10 seconds,
but you want to get them all done in a short

00:11:13.170 --> 00:11:17.630
period of time. If you do the query at the
beginning of the continuation, you can start

00:11:17.630 --> 00:11:22.220
passing that baton immediately. And so, you
know, as soon as you enqueue the task, it'll

00:11:22.220 --> 00:11:26.249
start running within, you know, 5, 10 milliseconds.
So you'll get, you know, 50 threads running

00:11:26.249 --> 00:11:30.110
in parallel or something like that. So that's
really a nice way to get a lot of work done

00:11:30.110 --> 00:11:36.449
all at the same time and do fan-out to--for,
you know, a higher-level of parallelism. And

00:11:36.449 --> 00:11:41.540
so we use this technique in the hub. And we
do between 100 and 300 worker request per

00:11:41.540 --> 00:11:45.640
second all day long using this style. So,
it's really effective. It works very well.

00:11:45.640 --> 00:11:51.610
And we've had async URLFetch in Python. This
has been Java support since February. There's

00:11:51.610 --> 00:11:55.569
also some really cool tools in Python and
Java for doing asynchronous Datastore calls

00:11:55.569 --> 00:12:01.019
which you should definitely check out async
tools and Twigg. They do a lot of asynchronous

00:12:01.019 --> 00:12:05.899
stuff of features and it's just really great
as a programming model. It's much more efficient

00:12:05.899 --> 00:12:09.489
than maybe what you're used to. So definitely
check those out. Okay. So, the next step--we

00:12:09.489 --> 00:12:15.929
got a lot of ground to cover, that's why I'm
going kind of quick. It makes sense so far?

00:12:15.929 --> 00:12:22.220
Any questions so far? Yes.
&gt;&gt; [INDISTINCT]

00:12:22.220 --> 00:12:26.600
&gt;&gt; SLATKIN: Yeah.
&gt;&gt; [INDISTINCT]

00:12:26.600 --> 00:12:32.279
&gt;&gt; SLATKIN: So, is that--yeah, every time
you run these things in parallel or in sequence,

00:12:32.279 --> 00:12:36.899
you lose--yeah, I'm repeating the question.
So the question is, "Every time you run tasks

00:12:36.899 --> 00:12:42.220
in parallel or in sequence, you lose atomicity."
Yes, you lose atomicity. Absolutely, that's

00:12:42.220 --> 00:12:47.709
part of the deal. I'm getting into how to
keep that if you need to. That's part of the

00:12:47.709 --> 00:12:51.759
trade-off of fan-out. Basically, you're saying,
"I know I need the fan-out." And then, if

00:12:51.759 --> 00:12:57.570
you need to apply some kind of atomicity or
sequencing, then that's where you get in to

00:12:57.570 --> 00:12:59.920
transactional sequences, so. Yes, natural
question, that's why it's the next topic.

00:12:59.920 --> 00:13:05.319
So, how do you--how do you guarantee [INDISTINCT]
across tasks? Okay, so what is a transactional

00:13:05.319 --> 00:13:10.709
sequence? So, we've had this for a little
while. Some people don't know it but you can

00:13:10.709 --> 00:13:15.639
actually use Datastore transactions with tasks
at the same time. What that means is that

00:13:15.639 --> 00:13:19.799
you can commit a transaction and a task at
the same time, which means that the task will

00:13:19.799 --> 00:13:25.410
not ever run until after the data has been
committed. And that the data won't get committed

00:13:25.410 --> 00:13:30.579
unless the task will run. So, you are guaranteed
that they will both be in or out, right? You

00:13:30.579 --> 00:13:34.819
don't have any extra tasks. You don't have
any missing data. So that means, you have

00:13:34.819 --> 00:13:39.269
strong consistency when the task is run and
you can do all kinds of really cool things

00:13:39.269 --> 00:13:45.230
with that pair. Specifically, what it'll let
you do is something like roll-forward semantics

00:13:45.230 --> 00:13:51.809
to fanned-out data. So, I'll explain what
that is, but in a nutshell, you can have roll-forward

00:13:51.809 --> 00:13:57.009
materialized views. So, what is a materialized
view? If you're a database person, you probably

00:13:57.009 --> 00:14:02.379
have heard that term before. So a materialized
view is just a query that you saved back into

00:14:02.379 --> 00:14:07.220
the database. That's all it is, do the query,
save it back in the database. The reason you

00:14:07.220 --> 00:14:10.819
want to do that is because you want to read
from it a lot. You want to cache it a lot.

00:14:10.819 --> 00:14:14.999
Or maybe, it's an index that's, you know,
more complicated required some work that you

00:14:14.999 --> 00:14:20.499
don't want to have to keep doing. And a lot
of the time, it's eventually consistent. So,

00:14:20.499 --> 00:14:23.370
with the materialized view, you don't care
if it's perfectly transactional as long as

00:14:23.370 --> 00:14:29.699
it's eventually correct, provides a good picture
of what's going on. And so, a materialized

00:14:29.699 --> 00:14:33.759
view also lets you do all kinds of cool tricks
like you can do incremental aggregations like

00:14:33.759 --> 00:14:38.769
counts and sums. You can do natural and left-joints
pretty easily. And you can do all kinds of

00:14:38.769 --> 00:14:42.559
filtering and querying on those materialized
results. So you can do a level of richness

00:14:42.559 --> 00:14:47.949
in querying that you couldn't do otherwise.
So let me show you what I mean. So, here's

00:14:47.949 --> 00:14:53.100
a SQL statement. This is something that people
do a lot, grade would be like, you know, first

00:14:53.100 --> 00:14:58.459
grade, second grade, you know, as kids gets
older, not like A, B, or C. So, we have Bob

00:14:58.459 --> 00:15:03.040
and Daisy on the student table, what grade
they're in. And here, I can figure out, you

00:15:03.040 --> 00:15:08.410
know, "SELECT grade, count(*) as count FROM
Student GROUP BY grade;" basically. So I'm

00:15:08.410 --> 00:15:12.670
saying, you know, what--how many people are
in each grade? It's really simple statement

00:15:12.670 --> 00:15:18.509
you can do in SQL, and it's something that's
actually kind hard to do in App Engine. So

00:15:18.509 --> 00:15:21.990
how would you build this on App Engine if
you want to do and make sure that it was correct?

00:15:21.990 --> 00:15:28.429
It's like that it eventually always came to
the correct answer. And so, what you can do

00:15:28.429 --> 00:15:33.049
is you can build a materialized view which
is this lower table, which gives you the grade-to-count

00:15:33.049 --> 00:15:36.429
mapping, right? And then, you can do all kinds
of cool stuff for like that. Like you could

00:15:36.429 --> 00:15:40.889
render the front page of people, you know,
with grades or maybe the dig name page, you

00:15:40.889 --> 00:15:47.850
know--a bunch of stuff. So, let me show you
how to do it on App Engine. So, this is the

00:15:47.850 --> 00:15:52.459
same model--same model as SQL. I'm just kind
of giving you a little more, you know, syntax

00:15:52.459 --> 00:15:56.720
around it so you understand exactly how it
works. So we have a student model class here.

00:15:56.720 --> 00:16:02.589
That's the person. That's the first table.
It's who and what grade they're in, okay?

00:16:02.589 --> 00:16:06.629
Then we have this marker class which is actually
just a random entity. I'll explain how it

00:16:06.629 --> 00:16:10.589
works. This is what we use for sequencing.
And then, at the bottom we have the group

00:16:10.589 --> 00:16:14.389
count. Which says, for each grade, what is
the count? So that's--the actual materialized

00:16:14.389 --> 00:16:22.449
view is the group count. That's what that
is, okay? So, this is the idea. You have,

00:16:22.449 --> 00:16:26.399
you want to have roll-forward semantics that
means the data rolls forward. That means that

00:16:26.399 --> 00:16:32.899
like at any time all that matters is what
is the next thing in the sequence of events?

00:16:32.899 --> 00:16:36.730
All you care about is the latest value. So
any time data falls behind you, you just want

00:16:36.730 --> 00:16:39.459
to roll it forward. That's all you care about.
That's what we're trying to achieve. We're

00:16:39.459 --> 00:16:44.919
not trying to have perfect transactional consistency
across all of our inputs and our aggregations.

00:16:44.919 --> 00:16:49.970
We're just trying to make sure that it's eventually
correct so that it reach to some stable value

00:16:49.970 --> 00:16:56.579
without any potential conflicts. So we have
Bob, let's say that Bob gets held back or

00:16:56.579 --> 00:17:02.209
push back maybe on a fight, I don't know.
So we're changing his grade from 4 to 3, sad.

00:17:02.209 --> 00:17:08.740
And he's a student. And so, we make a new
child entity, that's kind of a weird--weird

00:17:08.740 --> 00:17:12.030
relationship, but let's say the student has
a child entity which is the marker, okay?

00:17:12.030 --> 00:17:18.310
So you remember this marker. The marker has
a sequence number. And has a property of whether

00:17:18.310 --> 00:17:24.490
or not is present. I'll get into what that
means. But when Bob's grade change comes in,

00:17:24.490 --> 00:17:30.070
so someone typing in the change, we increment
the sequence number, let's say it was 16 before.

00:17:30.070 --> 00:17:33.530
This is just any sequence numbers. It's just
an ordering for the events, the changes. So

00:17:33.530 --> 00:17:38.610
this is the change number 17 that we're doing.
We apply that to this child entity, and this

00:17:38.610 --> 00:17:45.040
is part of a whole transaction. So then, we
transactionally, step 3, enqueue two tasks.

00:17:45.040 --> 00:17:50.511
One task says, "Hey, update the old value--the
old aggregations. Say, that Bob is no longer

00:17:50.511 --> 00:17:55.040
in the 4th grade." And the other one is update
the new value and say that Bob is now on the

00:17:55.040 --> 00:18:01.930
3rd grade. So we can update the materialized
view from the source data. But we update the

00:18:01.930 --> 00:18:05.870
source data in a transactional way so that
it's consistent and atomic on the source.

00:18:05.870 --> 00:18:14.430
So then we commit the transaction and the
tasks run. So, let me just show you the code

00:18:14.430 --> 00:18:20.330
for the update. So this is the update function,
I'm defining a transaction. I have--as parameters

00:18:20.330 --> 00:18:24.100
to the update function, I have the name of
the student, I have the new grade that they're

00:18:24.100 --> 00:18:31.380
going to be in, and I have their student ID.
So if the ID exists, like this is a new student,

00:18:31.380 --> 00:18:35.920
I'll get the student and then I'll keep track
of what their old grade was and then I'll

00:18:35.920 --> 00:18:41.150
update their grade to the new grade. That's
just a little swap line, that old student

00:18:41.150 --> 00:18:46.500
grade. If they don't have an ID, then I'll
create the new student, assign an ID by putting

00:18:46.500 --> 00:18:51.930
it and then do the same thing. And note that
they didn't have an old grade, so I don't

00:18:51.930 --> 00:18:59.400
have to remove them. Then I go and retrieve
the marker which is the sequencing entity,

00:18:59.400 --> 00:19:02.540
and I pull it out of the Datastore. If it
doesn't exist, I also create it. That's this

00:19:02.540 --> 00:19:09.410
"if not marker: Marker = [INDISTINCT]". That's
just, you know, kind of get it create. And

00:19:09.410 --> 00:19:13.980
then I increment the marker sequence number.
So this is how I guarantee atomicity of the

00:19:13.980 --> 00:19:20.260
student's record and what grade they're in,
okay? So this is just finding the student

00:19:20.260 --> 00:19:26.410
and then saying, "They were sequence number
16, now they're sequence number 17, and their

00:19:26.410 --> 00:19:32.671
grade is changing from 4 to 3." So that's
what I've done. I assigned their new grade

00:19:32.671 --> 00:19:39.160
level. I kept track of the old one. So, you
see, "old, student.grade = student.grade,

00:19:39.160 --> 00:19:44.610
new." So that means the old grade will equal
the new--the old grade and the new grade will

00:19:44.610 --> 00:19:50.320
equal to new grade, okay? That's all I'm doing.
And I'm saying that this is the 17th change

00:19:50.320 --> 00:19:56.390
or the nth change to this user's data, so
it's atomic, it's an order. So then I put

00:19:56.390 --> 00:20:02.120
those back on the Datastore. I enqueue a task
transactionally, that's what that "transactional

00:20:02.120 --> 00:20:10.300
 = true" is. I say this student ID has this
grade now. The sequence number is, say, 17

00:20:10.300 --> 00:20:16.920
and they're present. That means that they're
now in this grade, okay? Then, I also say,

00:20:16.920 --> 00:20:20.600
"Hey, if the old one is not None, that means
they had a previous grade." That means that

00:20:20.600 --> 00:20:24.130
they were--they've been moved between grades
so they graduated or whatever. And then I

00:20:24.130 --> 00:20:30.010
also enqueue another task for the same student
except grade is old. So I take the old grade,

00:20:30.010 --> 00:20:34.480
and then you'll notice that I use the same
sequence number but then I set presence to

00:20:34.480 --> 00:20:37.910
false. So I say that they are not present
in the old grade because they've been moved

00:20:37.910 --> 00:20:43.410
out of the old grade. That was the point because
we move Bob from grade four to grade three.

00:20:43.410 --> 00:20:48.350
And then I run that in the transaction--sorry,
I run the transaction that commits transaction.

00:20:48.350 --> 00:20:52.280
So all we've done here is change their grade
from four to three and then enqueued two tasks

00:20:52.280 --> 00:20:57.830
transactionally that are guaranteed to run
with the sequence number. So, okay, these

00:20:57.830 --> 00:21:04.310
are the models again, just to show you. So
that's the sequence ID, the marker. So now

00:21:04.310 --> 00:21:10.450
what happens? So let's say this is our initial
state. On the left, we have the aggregation

00:21:10.450 --> 00:21:15.770
group count for the third grade--I'm sorry,
for the fourth grade, okay? This is a parent

00:21:15.770 --> 00:21:19.270
entity which is the count. This is the aggregation.
This is the fourth grade, how many people

00:21:19.270 --> 00:21:23.490
are in it? That's all there is to it, right?
The one on the right is, "For the third grade,

00:21:23.490 --> 00:21:29.350
how many people are in it?" Okay? Now, child
entities of this group counts or these markers

00:21:29.350 --> 00:21:32.820
I was telling you about, these are the things
that they use for sequencing. They indicate

00:21:32.820 --> 00:21:38.720
if a student is actually been present in the
aggregation, okay? So you'll notice here that

00:21:38.720 --> 00:21:45.200
Bob was in the fourth grade. On the left side,
he has an entity that says he was present.

00:21:45.200 --> 00:21:51.660
It has sequence number 16. Okay. And it has
a plus, which means that he's present. That

00:21:51.660 --> 00:21:58.150
means he's in that grade. On the right side,
you'll see that Bob also has sequence number

00:21:58.150 --> 00:22:03.770
16, but he's got the presence indicator of
negative, which means he does not contribute

00:22:03.770 --> 00:22:10.510
to the aggregation. Okay? So then what we
do is these tasks come in, the two tasks that

00:22:10.510 --> 00:22:16.860
we enqueued before, these two here, the run
tasks, the two we enqueued, they run. So watch

00:22:16.860 --> 00:22:22.950
the left side. So we created--did the transaction
here and we update these two values. We say,

00:22:22.950 --> 00:22:30.871
"Transactionally, Bob is no longer in this
grade." Update the transactional sequence

00:22:30.871 --> 00:22:35.540
from 16 to 17 to make sure that it's ordered
and then reduced the group count from three

00:22:35.540 --> 00:22:40.390
to two. So we're just taking him out of the
grade transactionally. And we're sequencing

00:22:40.390 --> 00:22:45.510
it with that sequence number. That's how we
make sure that we don't mess up. We did the

00:22:45.510 --> 00:22:48.771
exact opposite on the other side, we take--we
do another transaction. This is in a second

00:22:48.771 --> 00:22:58.440
task and it updates the group count to three
and then marks Bob as present, okay? That's

00:22:58.440 --> 00:23:06.600
all that's going on. So, who coded this? Oh,
it's not wrapping right. Oh, it's okay, all

00:23:06.600 --> 00:23:11.110
right. So to apply this, this is what the
task does. It has a sequence number. It has

00:23:11.110 --> 00:23:14.230
whether or not they're present, the new grade
they're going to be--the grade I'm affecting

00:23:14.230 --> 00:23:19.410
and the ID of the student. So then, it figures
out the keys, start the transaction and it

00:23:19.410 --> 00:23:24.180
says, "Give me the aggregation count or the
aggregation row," which is the group count

00:23:24.180 --> 00:23:30.900
entity, and the marker, which is "For the
student, is Bob present as part of this aggregation?"

00:23:30.900 --> 00:23:35.100
Then it creates both the group count and the
marker if they don't exist already. And then

00:23:35.100 --> 00:23:41.310
there's this important line, if "marker.sequence"
is greater than sequence, roll back. This

00:23:41.310 --> 00:23:44.500
is the kind of magical part of this. Okay?
This is when I'm talking about, roll-forward

00:23:44.500 --> 00:23:48.510
semantics. This is what I'm talking about.
So if we accidentally moved Bob from grade

00:23:48.510 --> 00:23:52.770
four to grade three, and then back to grade
four again, we don't care what order the events

00:23:52.770 --> 00:23:59.560
come in because we know that when we changed
it, it went sequence number 16, 17, 18. So

00:23:59.560 --> 00:24:05.190
if you look at the aggregation and 18 is already
there, marker.sequence = 18, and the incoming

00:24:05.190 --> 00:24:10.560
task is from mark--sequence number 17, you're
like, "Oh, I already knew this. I already

00:24:10.560 --> 00:24:14.970
applied this. I don't care about this anymore
because it's an old update. I can ignore it.

00:24:14.970 --> 00:24:22.430
I'm rolling forward my data. That's the point."
So you're going to just drop the old stuff.

00:24:22.430 --> 00:24:27.200
And the roll-back will actually cause running
transaction to return without any kind of

00:24:27.200 --> 00:24:34.510
exceptions. So it'll look like it was successful,
which is what you want. So then on the next--the

00:24:34.510 --> 00:24:39.700
next step is in that transaction. I just say,
"Okay. This is transaction number 17. I assign

00:24:39.700 --> 00:24:48.320
it. And I keep track of kind of what the--whether
or not they were present before. That's what

00:24:48.320 --> 00:24:53.290
that old value is. I put the sequence number
back in there, "db.put(marker)", that's what

00:24:53.290 --> 00:24:59.010
I do. That's me updating the sequence number
to keep this atomic property. If they were

00:24:59.010 --> 00:25:04.320
present before, then I subtract. If they are
present now, then I add one. I could probably

00:25:04.320 --> 00:25:09.470
write that better but just trying to make
it really clear. If the count is now zero,

00:25:09.470 --> 00:25:12.570
that means there's nobody in the grade anymore.
So I just delete the grade. I don't want that

00:25:12.570 --> 00:25:17.960
table--I don't want that row in my aggregation
table. And then I either put the group or

00:25:17.960 --> 00:25:23.660
I delete it, and then I run that in the transaction.
Okay? So, again, the idea is that we're rolling

00:25:23.660 --> 00:25:30.070
forward these two independent entity groups
at separate times but able to keep atomicity

00:25:30.070 --> 00:25:36.670
across them. That's like the really cool thing.
Just to hit you over the head with this again,

00:25:36.670 --> 00:25:41.730
so here's the sequence of what we're doing.
Number one, change the student grade; two,

00:25:41.730 --> 00:25:48.450
increment their sequence number, so 16 to
17, old grade is four, new grade is three;

00:25:48.450 --> 00:25:54.460
three, enqueue two tasks; four, tasks are
running. We fetch the grade group, we compare

00:25:54.460 --> 00:25:59.500
the sequence numbers, we see that the current
one is 16, 16 is less than 17, we allow the

00:25:59.500 --> 00:26:07.010
transaction roll-forward to happen. We increment
the count. That's all there is to it. It's

00:26:07.010 --> 00:26:11.830
really simple. So let me show you this. I
have this running. Let's see. I think it's

00:26:11.830 --> 00:26:16.331
this one. Okay. So this is my little demo.
I'll be making this code available soon, so

00:26:16.331 --> 00:26:23.980
there's me in sixth grade and Jim in the second
grade. You can add one in the fourth grade,

00:26:23.980 --> 00:26:31.270
click "Add", okay? There we go. Now, I'm going
to move Jim into the fourth grade. See if

00:26:31.270 --> 00:26:36.100
it reloads. Here we go. Okay. So there's two
students in grade four, so, you shouldn't--now

00:26:36.100 --> 00:26:41.230
this happens so fast, you're like, "Well,
Brett, you're just doing that, you know, with

00:26:41.230 --> 00:26:42.691
the covers or whatever." It's happening so
fast. Well, that's because task queues run

00:26:42.691 --> 00:26:49.720
really fast, you know. But just to show you,
these are my admin logs. So you'll see, here's

00:26:49.720 --> 00:26:57.080
a 302, that's what I return when I--302 right
here, that's what I return when I have a successful

00:26:57.080 --> 00:27:05.890
post. Now, you'll see two work items run.
One, removing it from grade four and one adding

00:27:05.890 --> 00:27:11.950
it to grade six for the same student. Okay?
That's all I'm doing, really easy. But that

00:27:11.950 --> 00:27:20.690
happens so fast. You'll see--look at the time,
it's 33.36, 33.72, 33.73, 33.8, so the task

00:27:20.690 --> 00:27:30.390
ran from my browser even reloaded the page.
It's pretty cool. Okay. So, some final details

00:27:30.390 --> 00:27:35.480
about this. So each aggregation row is its
own entity group. That's how this works, that's

00:27:35.480 --> 00:27:42.200
how you guarantee that the aggregation rows
stay consistent within themselves. You update

00:27:42.200 --> 00:27:46.960
them in several transactions and the order
of the task application doesn't matter. And

00:27:46.960 --> 00:27:52.720
then that marker entity, child of each row
basically indicates the present of that--presence

00:27:52.720 --> 00:27:57.770
of that student in the aggregation. So that's
how you do this. That's how you bridge the

00:27:57.770 --> 00:28:02.400
transactions across these entity groups. And
the roll-forward part is that you can just

00:28:02.400 --> 00:28:11.340
ignore old updates because you don't care.
It works really well for a commutative operations,

00:28:11.340 --> 00:28:17.640
count, sum. If you're toggling presence, it's
really easy. You can actually do with some

00:28:17.640 --> 00:28:21.390
ancestor queries in that transaction because
it's across the same entity groups. So if

00:28:21.390 --> 00:28:25.491
you need to do something like some squares,
it's actually possible. And you can actually

00:28:25.491 --> 00:28:30.070
use continuations with cursors to do multiple
transactions comparing sequence numbers all

00:28:30.070 --> 00:28:35.550
along the way to actually do really elaborate
functions on all of your aggregation data.

00:28:35.550 --> 00:28:40.230
That's more complicated and more expensive
computationally, but it's kind of a simple

00:28:40.230 --> 00:28:44.790
model of doing work like this in a deferred
manner. So it's definitely something to check

00:28:44.790 --> 00:28:50.080
out. And you can kind of keep your aggregations
accurate, which is really nice. And then you

00:28:50.080 --> 00:28:55.230
can fan-out. So when the source data changes,
you can enqueue multiple tasks and update

00:28:55.230 --> 00:28:58.470
many aggregations at the same time, so you
don't have--you're not limited to just one

00:28:58.470 --> 00:29:05.910
materialized view, you can have many different
ones for the same data. Now, the thing is

00:29:05.910 --> 00:29:10.750
though, the key--the gotcha is that the maximum
throughput is proportional to how many aggregation

00:29:10.750 --> 00:29:14.930
rows you have. You need to do transactions
on each of those aggregation rows as you apply

00:29:14.930 --> 00:29:20.390
the transforms. So if you only end up having
one aggregation row, then you actually are

00:29:20.390 --> 00:29:24.400
going to be limited to updating that--how
fast that one aggregation row can update,

00:29:24.400 --> 00:29:31.860
right? And usually, you want to limit that
to about one transaction per second per aggregation

00:29:31.860 --> 00:29:36.809
row to make sure they have no comflicts and
no, you know, kind of a roll-back transactions,

00:29:36.809 --> 00:29:45.040
transaction collisions. So that's a problem.
For a lot of apps though that are trying to

00:29:45.040 --> 00:29:47.070
do complicated aggregations, they don't--really
don't care. That's actually--they just want

00:29:47.070 --> 00:29:51.809
the functionality. The other thing is you
create a storage cost for the presence entity.

00:29:51.809 --> 00:29:58.190
So to even indicate that, an aggregation contains
one of the students for instance, you need

00:29:58.190 --> 00:30:02.570
to write a little row. But that row is usually
on the same size--it's about the same size

00:30:02.570 --> 00:30:06.550
of the actual key of the original data, so
it's like a hundred bytes or so. You don't

00:30:06.550 --> 00:30:10.470
have to index it anyway. So you can do some
really nice things with those presence indicators

00:30:10.470 --> 00:30:21.220
to make them really cheap. Okay. I want you
to hold off on that one, I'm going to, maybe

00:30:21.220 --> 00:30:27.081
I'll descend that one too. So, all right,
so this gets to the next problem, you know,

00:30:27.081 --> 00:30:31.621
if I'm limited to one transaction per second
that's not really good and I actually need

00:30:31.621 --> 00:30:37.730
to do a lot of work in batch a lot of the
time. And what I find in building a pipeline

00:30:37.730 --> 00:30:42.301
in this system is that I often needed to do
a lot of work in batch, so that's when I want

00:30:42.301 --> 00:30:50.301
to fan-in. So what is fan-in? So it means
applying multiple data transforms in a batch,

00:30:50.301 --> 00:30:56.110
doing work in batches, taking counter updates,
aggregation updates, roll-ups of event data,

00:30:56.110 --> 00:31:04.750
you know, reservations of different resources,
doing them all at the same time. For instance,

00:31:04.750 --> 00:31:10.920
with Reddit or Digg-style voting, once a second
you can just pull for every update and then

00:31:10.920 --> 00:31:15.200
save the new values, you know, you only have
apply what's changed since the last time you

00:31:15.200 --> 00:31:18.601
looked, right? So it's fan-in, you take a
bunch of events that have come from a lot

00:31:18.601 --> 00:31:23.820
of different sources, combine them and aggregate
them together into a single batch of work.

00:31:23.820 --> 00:31:29.170
And this lets you, not as only--well, you
can beat the one second per entity group right

00:31:29.170 --> 00:31:33.770
throughput problem with this aggregation rows
because you can batch up your work into smaller

00:31:33.770 --> 00:31:40.390
pieces. It's also really good for high latency
API calls. So if you're doing a lot of URL

00:31:40.390 --> 00:31:45.070
fetches or doing any kind of, you know, complicated
data sub-queries or a lot of these queries

00:31:45.070 --> 00:31:49.680
at the same time. If you're building RSS aggregator
or you need a lot of throughput it's really

00:31:49.680 --> 00:31:53.470
good for that. Make sure that your queues
don't back up. So that's when you want to

00:31:53.470 --> 00:31:58.090
do fan-in. Basically, any time you want to
amortize the cost of doing a lot of work and

00:31:58.090 --> 00:32:01.070
you can do that by doing it in parallel? You
want to fan-in and--so that parallel--you

00:32:01.070 --> 00:32:07.880
can do your work in parallel at the same time
that's like pretty obvious idea. So, the problem

00:32:07.880 --> 00:32:12.700
is that when people do this in the real world,
they usually think of it this way. So, in

00:32:12.700 --> 00:32:20.020
the real world like the outside of Google,
I guess, the fantasyland of my wiliness. So,

00:32:20.020 --> 00:32:23.530
they have a queue and have a queue moderator
and have a bunch of workers and they pop off

00:32:23.530 --> 00:32:27.730
the head, maybe they grab five at a time or
20 at a time, right? And that's how basically

00:32:27.730 --> 00:32:33.300
every other queuing system out there works
and it works really well for a lot of things.

00:32:33.300 --> 00:32:38.840
The problem is the workers are always running.
And then what happens is, you know, remember

00:32:38.840 --> 00:32:42.920
we're trying to build a pipeline here. So
how many polling workers do you need to make

00:32:42.920 --> 00:32:50.140
sure that if you're inserting 50 tasks a second,
that each one of service within 500 milliseconds?

00:32:50.140 --> 00:32:54.880
And then what if the tasks take a long time,
each one? And then how do you make sure that

00:32:54.880 --> 00:32:58.500
the workers don't stand on each others--step
on each others toes, right? Like there's a

00:32:58.500 --> 00:33:02.490
whole series of problems that come out of
polling based workers. They can batch but

00:33:02.490 --> 00:33:08.980
they can't actually guarantee the latency
that you need. So I would claim that polling

00:33:08.980 --> 00:33:12.980
workers actually don't work for offline processing.
Oh, sorry, polling workers are offline processing

00:33:12.980 --> 00:33:17.620
they don't work for pipelines, that's what
I'm saying. Polling workers cannot do pipelines.

00:33:17.620 --> 00:33:22.670
They're not dynamic. The latency is not low
enough. You have to have workers allocated

00:33:22.670 --> 00:33:25.760
for your peak load at all times because you
need to make sure that you can guarantee those

00:33:25.760 --> 00:33:31.201
latency--that latency margin. And it basically
eliminates--if you're doing polling, it eliminates

00:33:31.201 --> 00:33:40.730
the benefits of the push based task queue
that App Engine has, okay? So what I've--what

00:33:40.730 --> 00:33:45.760
I would like to use instead is a fork-join
queue, okay. Well, the fork-join queue is--it's

00:33:45.760 --> 00:33:53.990
a queue were as work comes, we fork the items
apart into different segments, okay? And then,

00:33:53.990 --> 00:33:58.570
we make sure that all of that work will start
within a fixed interval after arrival, so

00:33:58.570 --> 00:34:04.940
each piece of work will be guaranteed to run
within 500 milliseconds. Then we execute the

00:34:04.940 --> 00:34:09.990
work in those batches, so we fork it and then
execute it in batches, and then at any time

00:34:09.990 --> 00:34:14.309
we can join that work back with some other
data records to leave the system that's the

00:34:14.309 --> 00:34:20.039
join part, that's optional, but it's part
of the same kind of idea, so I'm going to

00:34:20.039 --> 00:34:23.599
show you exactly what I mean. So here is a
fork-join queue that uses the Datastore along

00:34:23.599 --> 00:34:30.889
with the task queue. So, ideally, I would
like to have two batches run in parallel for

00:34:30.889 --> 00:34:37.710
two 500 millisecond periods let's say. So
time is going to the right, so a new work

00:34:37.710 --> 00:34:42.159
item comes in, inserts it to the Datastore,
so this is some work I need to do, so if I

00:34:42.159 --> 00:34:47.560
need to fetch, this is a vote on an item that
I need to upload. And then when I insert that

00:34:47.560 --> 00:34:55.609
item, I also insert a task within ETA of 500
milliseconds in the future. So presumably

00:34:55.609 --> 00:35:01.440
that task will run after the first batch has
queued up, right? That's what I'm trying to

00:35:01.440 --> 00:35:06.950
do. So then I insert another work item, and
then I insert the task again but the task

00:35:06.950 --> 00:35:10.769
is already there, that's that first property
when we're going through continuations that

00:35:10.769 --> 00:35:16.180
what happens. The task [INDISTINCT], so you're
guaranteed that if you insert it two times

00:35:16.180 --> 00:35:22.690
basically, that it only will run once, okay?
So that's the fan-in, that's the magical part

00:35:22.690 --> 00:35:27.849
but the task is already there. So the task
will only actually run once, so I can insert

00:35:27.849 --> 00:35:34.460
more work, again, I get the task, the conflicts
and it just skips it. The same thing, so the

00:35:34.460 --> 00:35:38.690
batch is full. I've got a whole 500 milliseconds
worth of work and then I execute the task

00:35:38.690 --> 00:35:46.140
and I take care of that batch. It's a straightforward
idea. Now the next batch starts. Start queuing

00:35:46.140 --> 00:35:53.049
work again, with the task which starts after
the batch two and so on. So it seems great,

00:35:53.049 --> 00:35:57.150
right? It's really easy. The problem is that
how do you coordinate when the batches start

00:35:57.150 --> 00:36:03.029
and stopped? So people will like "Well, let's
just use the clock, right?" Every 500 milliseconds

00:36:03.029 --> 00:36:06.180
just pick the current time and then pick on
the clock where you want it to start and boom

00:36:06.180 --> 00:36:11.249
you got it, except clocks aren't in-synch.
It doesn't work like that, even with NTP it

00:36:11.249 --> 00:36:18.309
doesn't work like that. I saw a server the
other day, it was 40 minutes off, so, yeah,

00:36:18.309 --> 00:36:25.480
not good. So we need another way to do that
and there is another problem. What happens

00:36:25.480 --> 00:36:31.190
if there's a race condition right at the edge
where the task just starts to execute? Write

00:36:31.190 --> 00:36:35.059
is another--or you know, the task executes
and finds a works it needs to run just as

00:36:35.059 --> 00:36:40.029
another work items is inserted. How does the
task know to wait? So we actually kind of

00:36:40.029 --> 00:36:44.430
need a lock there. We need to make sure that
the task doesn't run until the entire batch

00:36:44.430 --> 00:36:53.369
is committed. So that's a problem, right?
Similarly, if we have one of those machines

00:36:53.369 --> 00:36:59.470
that has a terrible clock, what if it inserts
into the wrong batch? Or inserts into a box

00:36:59.470 --> 00:37:04.690
that already executed a long time ago? We
needed ways to prevent those things too. So

00:37:04.690 --> 00:37:12.279
time alone doesn't work, we need something
better. So let me show you how to do it. So

00:37:12.279 --> 00:37:18.430
here I have some--another it's me counting
again, next day I'm going to wear cape that

00:37:18.430 --> 00:37:24.920
will be good. So I have my sum, here is an
aggregation of me counting. I have my work

00:37:24.920 --> 00:37:29.490
which is delta space [INDISTINCT] that I want
to apply to the sum. And then each piece of

00:37:29.490 --> 00:37:35.390
work has a work index which I'll explain what
that is and then the sum has a name, okay?

00:37:35.390 --> 00:37:39.930
Really simple. But you can think of the sum
is saying like, this is the most popular dig

00:37:39.930 --> 00:37:48.980
item right now and my work is my upload, okay.
So, I'll be on my, my wrapping does not look

00:37:48.980 --> 00:37:55.069
good, I'm sorry about that. It will be okay.
It's--yes, that fits right? So insert, this

00:37:55.069 --> 00:38:00.200
is inserting a work item, this is inserting
a upload or delta, a positive delta, negative

00:38:00.200 --> 00:38:05.960
delta. I have the sum name and then the delta
amount, that's my two parameters to the function.

00:38:05.960 --> 00:38:15.329
I will call "memcache.get" on an index. Okay.
So this is a counter that represents my batches,

00:38:15.329 --> 00:38:20.569
my batch count. So what I want to do here
is instead of using time for batch one, batch

00:38:20.569 --> 00:38:24.489
two, so I have, you know, 500 milliseconds
in the future, 1 second in the future, 1.5

00:38:24.489 --> 00:38:28.180
seconds in the future. What I'm going to do
is actually assign sequence numbers. Batch

00:38:28.180 --> 00:38:35.450
one, batch two, batch three, batch four; and
that's how I'm going to assign these groupings

00:38:35.450 --> 00:38:40.980
of this tasks. So I did that with memcache,
I do "memcache.get" on the index. If the index

00:38:40.980 --> 00:38:47.029
isn't there I add it starting at one and then--and
so then I have my work index. So how I know--this

00:38:47.029 --> 00:38:51.349
is the next batch number that I'm going to
use. Then I looked at memcache and I looked

00:38:51.349 --> 00:38:55.619
at the lock. This is a reader/writer lock
that I'm doing in memcache. Okay. Memcache

00:38:55.619 --> 00:39:01.630
is extremely fast for doing a reader/writer
lock. So I do a memcache increment and I increment

00:39:01.630 --> 00:39:04.730
the lock with this name. I'm formatting a
string here; I'm just putting the sum name

00:39:04.730 --> 00:39:11.880
and the index number into a memcache key.
Okay, that's all I'm doing. So if my sum were,

00:39:11.880 --> 00:39:17.950
you know, stuff, it would be "stuff-lock-index"
so and, you know, 400 or something. So I do

00:39:17.950 --> 00:39:24.499
memcache increment to that, that's atomic.
And the initial value I set 2 to the 16th.

00:39:24.499 --> 00:39:31.510
I'll explain that in a second why we need
to do that. Now this will get a little hairy

00:39:31.510 --> 00:39:35.480
and I'll come back to exactly how it works
but if the number--so I did the increment,

00:39:35.480 --> 00:39:40.710
it gives me back the new value of this lock,
this reader/writer lock. It's more like a

00:39:40.710 --> 00:39:45.711
semaphore. If the writer is way less than
2 to the 16th or anything less than 2 to the

00:39:45.711 --> 00:39:50.140
16th, I basically know that this lock is no
longer--this index is no longer available

00:39:50.140 --> 00:39:56.029
to new writers, okay. So you cannot use this
index anymore. It's what that negative is,

00:39:56.029 --> 00:40:00.230
that if writers is less than 2 to the 16th
that's saying, the index you just selected

00:40:00.230 --> 00:40:05.690
on the first line, memcache.get, this index
you can't use anymore. That solves the race

00:40:05.690 --> 00:40:10.380
condition of the task queue and the batch
coming at the same time. So in that case,

00:40:10.380 --> 00:40:13.630
I decrement the lock, I'm like "All right,
drop this lock." And I return false. This

00:40:13.630 --> 00:40:18.329
failed I didn't get to insert the work item.
If you want to make sure that's totally reliable,

00:40:18.329 --> 00:40:22.749
you just keep spinning until you get in one.
Memcache is fast enough, you usually have

00:40:22.749 --> 00:40:29.109
to try it like at most twice. Okay. But that's
just solving the race condition of the synchronization

00:40:29.109 --> 00:40:36.750
at the end of the batch. So then once I have
incremented the lock, that means I have incremented

00:40:36.750 --> 00:40:40.710
the writer lock, I have registered that I
am a writer for this lock, I'm going to be

00:40:40.710 --> 00:40:46.950
writing so that, you know, keep track of that
as if like a semaphore. So I add one to that

00:40:46.950 --> 00:40:53.289
reader/writer lock. Then I add--take my work,
I set the delta to whatever I supply, you

00:40:53.289 --> 00:41:00.000
know, add 1, minus 10 whatever. I set the
work index property to the index I got with

00:41:00.000 --> 00:41:08.720
the sum name and then a hash value; I'll get
into "Y." This hash is just [INDISTINCT] hash

00:41:08.720 --> 00:41:16.690
and then I put in the work committed to the
Datastore. Then I do the batching part and

00:41:16.690 --> 00:41:23.599
all I'm doing is I'm taking the present time
and I'm saying "I want to have some work start

00:41:23.599 --> 00:41:28.339
one second from now." So take the current
time, add one second, ETA, it's me setting

00:41:28.339 --> 00:41:33.490
an ETA, you can do it countdown too, either
way. But the important thing here is I'm setting

00:41:33.490 --> 00:41:38.619
the task name, the task name is that the sum
name plus now over 30, I'll explain why that's

00:41:38.619 --> 00:41:43.190
there, and the index, which means that if
a hundred at work items all have to do this

00:41:43.190 --> 00:41:48.549
at the same time, they'll all fan-in magically.
One of them will succeed and all the rest

00:41:48.549 --> 00:41:53.280
will go into this exception handler, task
[INDISTINCT] that exists there. And I passed

00:41:53.280 --> 00:41:59.380
there, which means I don't care. Okay. Now,
once I've put the work item, it's the last

00:41:59.380 --> 00:42:05.029
line, put I've--so, I've created my work item,
I have assigned it a batch number, I put it

00:42:05.029 --> 00:42:12.720
in the Datastore, I have in queued the task,
then I decrement the reader/writer lock. That

00:42:12.720 --> 00:42:20.069
means I'm no longer a writer on this lock.
I'm done. And I returned true to the writer.

00:42:20.069 --> 00:42:23.819
So it's relinquishing the writer lock. So
this lock allows multiple writers and a single

00:42:23.819 --> 00:42:35.569
reader, that's what we're trying to do. All
right, let's take a little break. A lot of

00:42:35.569 --> 00:42:42.640
code. So, now, joining, this is taking all
the batch items and running them together.

00:42:42.640 --> 00:42:45.960
So first thing I do, this is the task running,
this is the task I enqueued, that's were the

00:42:45.960 --> 00:42:50.760
join is happening. So, all of these input
items, enqueued the same tasks in a 500 milliseconds

00:42:50.760 --> 00:42:55.829
in the future. Eventually, the task wakes
up and it starts running. And it knows its

00:42:55.829 --> 00:43:00.640
own index because it's encoded in the names.
See that taskqueue add name equals, the index

00:43:00.640 --> 00:43:05.720
is the last parameter. It knows its own index.
Okay. So when the task wakes up it knows its

00:43:05.720 --> 00:43:12.220
name and its index. So then it will memcache
increment, it will push the batch index forward,

00:43:12.220 --> 00:43:17.451
it says "Okay, batch number 400 is done I'm
closing the gate." Push it forward so that

00:43:17.451 --> 00:43:23.269
batch 401 has to happen. Okay. So it's pushing
things forward. Then it goes to the lock that

00:43:23.269 --> 00:43:27.880
same reader/writer lock and it's decrements
it by a huge number. And this is the--you

00:43:27.880 --> 00:43:32.400
miss the boat signal. This basically handles
the race condition exactly at the border line

00:43:32.400 --> 00:43:39.779
when the items or sequence in such way that
the writer beat the task of getting the index

00:43:39.779 --> 00:43:46.220
before this first increment. So basically
the writer grabs 400 and then starts committing--oh,

00:43:46.220 --> 00:43:50.950
sorry--it grabs 400 and then starts to increment
the lock. So that's the race that you're trying

00:43:50.950 --> 00:43:55.900
to solve there. And by decrementing it massively
negative like this decrement 2 to 15th. That's

00:43:55.900 --> 00:44:00.240
what allowed us to do this if a writer isn't
less than 2 to 16th. All right, so just making

00:44:00.240 --> 00:44:04.970
sure that like I thought I acquired the writer
lock but I actually didn't. That's all your

00:44:04.970 --> 00:44:09.829
able to detect. So if writer's is less than
2 to 16th, what that actually says is the

00:44:09.829 --> 00:44:14.390
task is already running right these very second.
It's actually two threads running at the same

00:44:14.390 --> 00:44:19.220
time. It's given me a synchronization primitive
between my active writing code and my reading

00:44:19.220 --> 00:44:27.710
code that's what that does. That's the actual
mechanism. So now in the task, because I have

00:44:27.710 --> 00:44:31.950
multiple threads running at the same time
and I have the synchronization primitive,

00:44:31.950 --> 00:44:35.299
what I do is I need to wait for the writers
to finish. So sometimes the Datastore might

00:44:35.299 --> 00:44:38.220
take up to a second or two seconds depending
on, you know, what you're doing, how big your

00:44:38.220 --> 00:44:43.809
data is, whatever. So here actually just busy
wait and I keep getting that lock, its current

00:44:43.809 --> 00:44:51.220
value, and I wait for all of the writers to
finish. So as soon as the counter goes back

00:44:51.220 --> 00:44:57.529
to a known value, basically, the zero point,
then I'm like "All right, cool. All the writers

00:44:57.529 --> 00:45:03.690
are done I can now do my actual work." Okay.
And you can set a timeout here to give up

00:45:03.690 --> 00:45:08.200
after a few seconds. But the end of the day
all we've done, all we've achieved here is

00:45:08.200 --> 00:45:12.609
we're being able to batch a bunch of work
up at the same time. So it will execute--execute

00:45:12.609 --> 00:45:19.369
in parallel. So this is how I actually get
to work, it's just index by the work index.

00:45:19.369 --> 00:45:24.650
I say give me all the work with this index,
give it all to me, so do a filter, give me

00:45:24.650 --> 00:45:28.710
a list of everything, sum up all the deltas
for all the results, and then apply them in

00:45:28.710 --> 00:45:37.009
a transaction to my sum. Okay. So results
equals that's the query to get all the work

00:45:37.009 --> 00:45:42.059
with the certain index. Delta's just the sum.
The transaction just gets the sum, creates

00:45:42.059 --> 00:45:48.140
it if it's not there. Applies a delta and
submit it. At the end I delete the items if

00:45:48.140 --> 00:45:58.161
I wanted to.
So you see, again, inserting work into the

00:45:58.161 --> 00:46:03.119
Datastore and as the work gets inserted it
gets a batch sequence number, inserts a tasks

00:46:03.119 --> 00:46:08.089
with that sequence number. Then the next work
items do the same thing, right? Same sequence

00:46:08.089 --> 00:46:12.910
number that they have. As soon as the task
runs it prevents that sequence number from

00:46:12.910 --> 00:46:17.369
being used again. And then there are some
extra fancy bits around the reader and writer

00:46:17.369 --> 00:46:21.260
lock to make sure that this case were you
insert the work right at the exact same time

00:46:21.260 --> 00:46:26.670
the task is running, doesn't cause you to
drop things. Okay? It guarantees that it completes.

00:46:26.670 --> 00:46:35.940
It's kind of--that's the hard magical part
about this. Okay. So a demo, let's see. All

00:46:35.940 --> 00:46:41.700
right, so this is really a great app, this
is like one of the best ones I've ever written.

00:46:41.700 --> 00:46:47.759
It just says true. So that just means it inserted
some data. I was going to run httperf but

00:46:47.759 --> 00:46:53.569
apparently my laptop doesn't have [INDISTINCT]
right now. That's news to me. Yeah, I know,

00:46:53.569 --> 00:46:58.880
sad. So--and so again, I'll show you my admin
logs. Hopefully, I caught it fast enough.

00:46:58.880 --> 00:47:07.849
Okay. So you see here, these are me doing
increments with name, the sum name is food,

00:47:07.849 --> 00:47:14.029
the delta is one. They all get the same sequence
number, insert a task, and then the work will

00:47:14.029 --> 00:47:19.670
actually batch it up. So it says I'm combining
five tasks into a delta of five. That's what

00:47:19.670 --> 00:47:24.930
it did. And there are no race conditions.
It's fanned-in. It's much more efficient.

00:47:24.930 --> 00:47:33.749
You can do all the database work in parallel.
It's really great. So the details again, the

00:47:33.749 --> 00:47:37.619
task names are the fan-in mechanism. That's
the magic here. That's how we do it. We use

00:47:37.619 --> 00:47:43.119
the task ETA for the periodic batching. We
use memcache for reader/writer lock so that

00:47:43.119 --> 00:47:46.650
the batches can coordinate to make sure that
you don't start reading before the writers

00:47:46.650 --> 00:47:53.349
are done. And then we used spin locks to make
that coordination work correctly. And last,

00:47:53.349 --> 00:47:56.890
we used the Datastore to go and find the actual
works. So we apply this work index to our

00:47:56.890 --> 00:48:03.859
data, the batch ID, the batch number, so we
actually just query for it in the Datastore.

00:48:03.859 --> 00:48:06.410
And then if you do are getting any drops for
whatever reason like if you have a more lossy

00:48:06.410 --> 00:48:10.989
data model you can use an offline job to pick
those up. But given the scheme, you shouldn't

00:48:10.989 --> 00:48:19.940
have to. And it scales all the content. So
depends on your batch size, obviously, the

00:48:19.940 --> 00:48:24.099
data format you have, the model you have,
I just ran a few tests with this code. The

00:48:24.099 --> 00:48:32.729
model I was testing with was larger but I
can insert, you know, basically up to 75 items

00:48:32.729 --> 00:48:38.319
per second. The latency didn't change. So
just batching up every second works really

00:48:38.319 --> 00:48:48.230
well, it's great. Now, some important details
about the performance, the work index has

00:48:48.230 --> 00:48:53.509
to be hash. You have to hash it. That's why
I didn't use hash. And the reason is that

00:48:53.509 --> 00:48:58.539
you need to do that so that you distribute
the load across BigTable. A naive batching

00:48:58.539 --> 00:49:01.640
of limitation might use a timestamp to do
this. The problem with timestamps is they're

00:49:01.640 --> 00:49:06.579
always increasing. So in BigTable, you're
always writing to the beginning of the same

00:49:06.579 --> 00:49:11.400
row. BigTable always does things in order,
right? So if you are ordering things by time

00:49:11.400 --> 00:49:15.229
descending, you're basically always writing
into the same rows. So every time the tablet

00:49:15.229 --> 00:49:19.719
gets overloaded or gets too much data, it's
going to split itself in half and then keep

00:49:19.719 --> 00:49:23.670
doing it. But you'll just keep pounding the
same one over and over again. You don't actually

00:49:23.670 --> 00:49:27.799
split the load. So by putting a hash in your
work index you're making sure that you fan-out

00:49:27.799 --> 00:49:30.599
your works so that some of your writes go
over here and some go over here and some go

00:49:30.599 --> 00:49:34.140
over there. That's how you gain the--that's
how you get the parallelism of the fan-in.

00:49:34.140 --> 00:49:39.529
The other thing you need to do, if you really
want a lot of throughput on this is to eliminate

00:49:39.529 --> 00:49:43.880
all the other indexes you have, like global
indexes. Composite indexes are fine because

00:49:43.880 --> 00:49:50.200
usually they're prefaced to the unique key.
But global indexes have the same problem where

00:49:50.200 --> 00:49:55.950
if you're writing the ETA or update time of
an entity, that's in a global index, that

00:49:55.950 --> 00:50:08.400
top of the list BigTable tablet server will
be unhappy. But the other thing you see, I'll

00:50:08.400 --> 00:50:12.569
say here you can keep indexes in your "boxcar"-ing
transactions. So what I'm saying there is

00:50:12.569 --> 00:50:18.019
that if the whole point of this is just to
make it so you can do more work per transaction,

00:50:18.019 --> 00:50:21.349
then you really don't care so much about the
BigTable fan-out. All you're trying to do

00:50:21.349 --> 00:50:25.460
is say apply a hundred manipulations to the
same entity group at the same time. So you're

00:50:25.460 --> 00:50:28.910
just trying to do all your writes in combinations
so you can kind of move faster. But you're

00:50:28.910 --> 00:50:35.473
not actually trying to do like a hundred updates
per second, right? The last thing, I don't

00:50:35.473 --> 00:50:41.059
have too much time for, but it's just this
magic batch period equals zero. So in my testing,

00:50:41.059 --> 00:50:45.960
which is fine--and I been using this--I meant
to mention--I've been using this in the PubSubHubbub

00:50:45.960 --> 00:50:51.950
hub for a few days now at scale. So it works
very well, it actually reduced my total footprint

00:50:51.950 --> 00:50:58.720
by about 40%. So it's pretty serious. I actually
use a batch period of zero. And what that

00:50:58.720 --> 00:51:05.339
means is I wait for as long as the Datastore
takes to put. That's my batching period. So

00:51:05.339 --> 00:51:09.410
I start that put. And that's how long I'll
wait, so sometimes it's 50 milliseconds, sometimes

00:51:09.410 --> 00:51:14.869
it's 10, sometimes 100, so it's just kind
of expands based on my latency. And that kind

00:51:14.869 --> 00:51:18.890
of creates a self-regulating system that's
really cool. So you can experiment with that

00:51:18.890 --> 00:51:24.109
and there some more advanced code to do that,
some of it to just think about. All right,

00:51:24.109 --> 00:51:30.839
I've only got a few minutes left. I just have
one last slide then I'm basically done. All

00:51:30.839 --> 00:51:35.160
right, that was probably a lot. That's the
most code I put on the screen. So I appreciate

00:51:35.160 --> 00:51:40.579
you bearing with me. So hopefully, if you've
been thinking about this, the obvious combination

00:51:40.579 --> 00:51:45.839
of this thing is you can take fan-in fork-join
queues and the transactional sequences and

00:51:45.839 --> 00:51:49.799
materialized views, put them together and
get something really cool. And that's right.

00:51:49.799 --> 00:51:54.180
You can. That's the point that's what I'm
getting at. This is a huge wall of text. Don't

00:51:54.180 --> 00:52:00.249
even look at it. Don't even look at it. It's
just for me. Okay. So here's what you do,

00:52:00.249 --> 00:52:05.779
you have a fork-join queue that batches once
per second. The user updates some data like

00:52:05.779 --> 00:52:14.210
their votes. Okay. I upvote this item. They
insert that data and they get a work index

00:52:14.210 --> 00:52:18.280
for their fork-join queue. And apply that
to their vote. Okay? That's basically saying

00:52:18.280 --> 00:52:24.430
they add it into a fork-join queue. Then they
transactionally insert some tasks to go and

00:52:24.430 --> 00:52:31.220
update their materialized view. Then you also
go and insert the--optimistically you insert

00:52:31.220 --> 00:52:36.920
some name fan-in task. That's to do the magical
batching what the fork-join queue requires.

00:52:36.920 --> 00:52:40.920
And then later on, the fan-in worker will
query for all of the votes that have come

00:52:40.920 --> 00:52:47.460
in, in the last, you know, one second. And
then it'll do a transaction on all of the

00:52:47.460 --> 00:52:52.989
rows that make that--make up that aggregation
like the total votes for an item. It'll fetch

00:52:52.989 --> 00:52:57.519
all of the markers. It'll compare the batch
work that it just got in with what's present

00:52:57.519 --> 00:53:02.309
at the current time. It'll figure out the
difference of what's present and what's not

00:53:02.309 --> 00:53:05.600
present, like if someone changed their vote
from upvote to downvote, it will do that with

00:53:05.600 --> 00:53:09.900
sequence numbers to make sure it has row-forward
semantics, and then it will update the aggregation

00:53:09.900 --> 00:53:14.619
row and commit. So what does this mean? What
does this means is that you can actually get

00:53:14.619 --> 00:53:18.619
a lot of throughput out of those aggregation
rows I was telling you about. The materialized

00:53:18.619 --> 00:53:21.920
view thing I was telling you about, you can
actually write to those like a hundred updates

00:53:21.920 --> 00:53:26.609
per second but you just have to do it in batches
of a hundred ones per second. So you can combine

00:53:26.609 --> 00:53:31.079
the materialized view stuff I've told you
about today with the fork-join stuff, put

00:53:31.079 --> 00:53:34.930
them together and get a very high throughput
aggregation system to do all the aggregations

00:53:34.930 --> 00:53:42.210
you want, which is just great. So please build
this and send it to me or our team would love

00:53:42.210 --> 00:53:48.229
to see that. It's something you can do in
user space. I wish I had more time. So, yeah,

00:53:48.229 --> 00:53:51.609
thanks for bearing with me. So, okay, the
last slide; future directions, so, now, you're

00:53:51.609 --> 00:53:56.130
probably asking me, you know, this could be
a lot more efficient if I could go faster

00:53:56.130 --> 00:54:01.589
or longer batch more, right, and we're working
on it. This is on a road map. We want to have

00:54:01.589 --> 00:54:06.150
background servers. These are servers that
have no wall clock limits. That means we don't

00:54:06.150 --> 00:54:10.740
have a 30-second deadline anymore. So then
it's important to us for the purpose of pipelines

00:54:10.740 --> 00:54:16.680
like this and for the next talk about the
map--the Mapper API. We want you to be able

00:54:16.680 --> 00:54:22.760
to chunk through these fan-in queues in bulk
like huge amounts of bulk. It would also be

00:54:22.760 --> 00:54:26.369
even faster if I can do--have addressable
servers running on App Engine so that every

00:54:26.369 --> 00:54:30.069
time a new work item comes in, I would just
send a remote procedure call to some running

00:54:30.069 --> 00:54:34.811
server and have it enqueue in memory that
would make this extremely fast. It'd be lossy,

00:54:34.811 --> 00:54:41.180
but it would be, you know, an order of magnitude
faster. So we're working towards this kind

00:54:41.180 --> 00:54:45.619
of efficiency goals for both pipelines and
for offline processing both in the form of

00:54:45.619 --> 00:54:49.460
backgrounds servers and addressable servers.
So this is a ton of road maps that something

00:54:49.460 --> 00:54:56.190
that we think is really important. Okay. We
got a little bit of time for questions, Tiny

00:54:56.190 --> 00:55:02.099
URL, App Engine Pipelines is the--is where
the questions are. This is my blog if you

00:55:02.099 --> 00:55:12.359
want to follow me. And thanks for you sitting
through that. Let's see, I can have to--thanks.

00:55:12.359 --> 00:55:18.380
Let's see what happens here. I think there's
a mic here and a mic there. I don't have much

00:55:18.380 --> 00:55:22.259
time. I'm going to be going--well, directly
after me, before you leave, there's a talk

00:55:22.259 --> 00:55:26.470
from Mike Aizatsky who's also in the App Engine
team talking about the new Mapper API. I really

00:55:26.470 --> 00:55:30.590
encourage you to stay for that. He's talking
about mapper and the [INDISTINCT] generation

00:55:30.590 --> 00:55:34.630
and batch processing on App Engine. It's a
really good talk. I really encourage you to

00:55:34.630 --> 00:55:39.880
check it out. And it has, you know, a lot
of kind of--it's not overlapping with my talk

00:55:39.880 --> 00:55:46.940
at all, so it's very useful. So let me see
if there's any uploaded--will it be an extension

00:55:46.940 --> 00:55:56.089
or removal of the 10-second limit of the request
to external websites? So it's a 30-second

00:55:56.089 --> 00:56:03.299
limit now. Like I said, we're working on background
servers and addressable servers to--oh, I

00:56:03.299 --> 00:56:09.380
gotcha, URL fetches. Yeah, I think Kevin said
yesterday during the file search that we're

00:56:09.380 --> 00:56:13.329
trying our best to raise it into the 30-second
range and we understand that people want to

00:56:13.329 --> 00:56:18.219
go up to multiple hours or indefinite or something
that we're aware of and we're working towards.

00:56:18.219 --> 00:56:26.150
The memcaching can be a bottleneck. What are
the max throughputs? I've max out memcacher

00:56:26.150 --> 00:56:32.289
like 120,000 requests per second. I have a
feeling that's not going to be the problem.

00:56:32.289 --> 00:56:34.670
So, I mean, unless you're--I mean, that's
cool if you're doing that. That's great. That's

00:56:34.670 --> 00:56:39.679
like a massive success. Cool. Let's take one
from public. Yeah?

00:56:39.679 --> 00:56:46.890
&gt;&gt; Your ETA was it based upon an estimated
amount of work for the task or was it constant

00:56:46.890 --> 00:56:51.130
because you know how App Engine performs?
&gt;&gt; SLATKIN: It depends. So, yeah, so the question

00:56:51.130 --> 00:56:57.489
is the work--is the ETA on the batching based
on some constant idea or is it based on how

00:56:57.489 --> 00:57:01.309
App--like the work I'm doing or was it based
on some characters of App Engine itself, the

00:57:01.309 --> 00:57:07.459
characteristic of App Engine itself--so, both.
In the case of transactions, I know I shouldn't--to

00:57:07.459 --> 00:57:11.530
be really safe, I shouldn't go more than one
transaction per second if I'm actually trying

00:57:11.530 --> 00:57:17.209
to like write a lot of data at a time that's
why it's a good rule of thumb. For other stuff,

00:57:17.209 --> 00:57:20.839
you know, if I'm doing URL fetches and I know
the average latency is five seconds, it doesn't

00:57:20.839 --> 00:57:24.690
really help to batch more often than that
maybe. So it really depends on the problem

00:57:24.690 --> 00:57:29.069
that you're dealing with. If you know you
have a lot of latency, then, it helps to batch

00:57:29.069 --> 00:57:33.059
a lot, you know. For instance, if you're doing
a 20 URL fetch calls, you can do them all

00:57:33.059 --> 00:57:38.259
the same time and then you can use one thread
to hold on for 20 seconds of latency here

00:57:38.259 --> 00:57:44.309
or fit the 10 seconds latency. So, you can
eliminate your waiting cost by batching asynchronous

00:57:44.309 --> 00:57:51.420
calls together, so, yeah. Yeah?
&gt;&gt; So, it looks to me that you're trying to

00:57:51.420 --> 00:57:58.869
build a singleton pattern on top of Datastore
using materialized view and the other one

00:57:58.869 --> 00:58:07.000
on the transient memory on memcache. You know,
in the case of Datastore, you--if you run

00:58:07.000 --> 00:58:09.900
parallel tasks...
&gt;&gt; SLATKIN: Okay, yeah.

00:58:09.900 --> 00:58:14.140
&gt;&gt; ...you run into--in an optimistic concurrency
control, you run into still data. In the case

00:58:14.140 --> 00:58:21.011
of transient data, you lose the data if the
transient memory goes, you know, out of...

00:58:21.011 --> 00:58:27.319
&gt;&gt; SLATKIN: Uh-huh. So what was your question?
&gt;&gt; So, is it easier to have something like

00:58:27.319 --> 00:58:30.150
a transaction memory like a singleton pattern
instead of doing this?

00:58:30.150 --> 00:58:34.180
&gt;&gt; SLATKIN: So, yes, that's--all right. Basically,
the question is, why go to the trouble like

00:58:34.180 --> 00:58:39.640
this when you can just have global transactionality?
&gt;&gt; Like a singleton which is across distributed

00:58:39.640 --> 00:58:42.329
system of course.
&gt;&gt; SLATKIN: All right, so if you want the

00:58:42.329 --> 00:58:47.220
richness of the queries that we have, you
can achieve that. You cannot achieve global

00:58:47.220 --> 00:58:51.509
ordering like we have in our Datastore and
also be singleton. It just doesn't work. Like

00:58:51.509 --> 00:58:55.719
we have built something that, you know, it's
like the CAP theorem, right, you can't get

00:58:55.719 --> 00:59:00.549
everything. And so, we've got a nice tradeoff
of these properties that let you scale extremely

00:59:00.549 --> 00:59:05.219
wide and let you do rich queries with consistency,
but we have tradeoffs to come out of that

00:59:05.219 --> 00:59:09.599
and one of them is we can't apply global orderings
like having a single database. But a lot of

00:59:09.599 --> 00:59:13.760
people really want that which is part of why
we've announced that we're going to have SQL

00:59:13.760 --> 00:59:19.161
servers that people can get that singleton
locking performance that they want, you know,

00:59:19.161 --> 00:59:22.249
functionality that they want.
&gt;&gt; Addressable server is that going to solve

00:59:22.249 --> 00:59:26.999
our problem of--transaction?
&gt;&gt; SLATKIN: It could. Why don't you come on

00:59:26.999 --> 00:59:31.059
office hours? I think that's a really advanced
question, so we should talk about it then.

00:59:31.059 --> 00:59:33.849
Yeah. Yes?
&gt;&gt; What's the limit on number of simultaneous

00:59:33.849 --> 00:59:39.569
task queues running and is that something
we can control?

00:59:39.569 --> 00:59:42.150
&gt;&gt; SLATKIN: So are you saying task per second
or threads?

00:59:42.150 --> 00:59:47.039
&gt;&gt; Tasks that are running simultaneously.
&gt;&gt; SLATKIN: Yeah, it depends on their individual

00:59:47.039 --> 00:59:52.249
throughput. I don't remember if the numbers
offhand. If you come to office hours, we should

00:59:52.249 --> 00:59:58.050
have one for you, but I think the total is
50 tasks per second right now. Is that right?

00:59:58.050 --> 01:00:01.689
Is it higher? Yeah, so.
&gt;&gt; [INDISTINCT]

01:00:01.689 --> 01:00:06.299
&gt;&gt; SLATKIN: So--sorry, what?
&gt;&gt; [INDISTINCT]

01:00:06.299 --> 01:00:13.770
&gt;&gt; SLATKIN: Oh, what about BigQuery, how does
it relate to this? That's a good question.

01:00:13.770 --> 01:00:16.420
I don't know. I have to think about that.
&gt;&gt; [INDISTINCT]

01:00:16.420 --> 01:00:20.489
&gt;&gt; SLATKIN: Yeah, big lines, yeah, exactly.
So, big--yeah, BigQuery, I guess, is offline

01:00:20.489 --> 01:00:22.309
processing data mining. Yeah.
&gt;&gt; [INDISTINCT]

01:00:22.309 --> 01:00:28.849
&gt;&gt; SLATKIN: No--yeah, it's not--BigQuery is--I'm
pretty sure you upload a huge amount of data

01:00:28.849 --> 01:00:31.890
then delve into it. Yeah. So, I have time
for one last question.

01:00:31.890 --> 01:00:37.260
&gt;&gt; So just curious on the real life example
on this. What's happening during the scheduled

01:00:37.260 --> 01:00:40.890
outage when memcache is not available?
&gt;&gt; SLATKIN: Right, exactly. Well, when memcache

01:00:40.890 --> 01:00:44.930
is not available, the Datasource is also not
available for rates. So in my case, with the

01:00:44.930 --> 01:00:50.059
hub, I can't do anything anyway so I just
return 503s because memcache is down and then

01:00:50.059 --> 01:00:52.250
I just--like on a hub that's what I do. So,
there's...

01:00:52.250 --> 01:00:56.930
&gt;&gt; You don't schedule a whole bunch of tasks
that are accumulating during that time?

01:00:56.930 --> 01:01:03.039
&gt;&gt; SLATKIN: So the task queue will do its
own back off so that if it knows that if the

01:01:03.039 --> 01:01:08.229
Datastore is no longer can do rates, you can
determine that. The only thing that stops

01:01:08.229 --> 01:01:14.700
during the kind of scheduled maintenance is
the--is writing new tasks and completing tasks.

01:01:14.700 --> 01:01:18.529
But all the tasks that were already there
will complete after the scheduled maintenance

01:01:18.529 --> 01:01:24.160
because they already have--work index is assigned
and the data items too and the tasks are there,

01:01:24.160 --> 01:01:27.700
so if memcache disappears, they'll check for
the lock, the lock is not present, and then

01:01:27.700 --> 01:01:28.759
they'll just go along their way.
&gt;&gt; Okay.

01:01:28.759 --> 01:01:32.299
&gt;&gt; SLATKIN: So memcache--if memcache disappears
the tasks will still complete.

01:01:32.299 --> 01:01:35.479
&gt;&gt; But during the outage there's no new tasks
that can be generated?

01:01:35.479 --> 01:01:38.839
&gt;&gt; SLATKIN: That's correct, yes. That's--yes.
That's part of the tradeoff, yeah.

01:01:38.839 --> 01:01:41.489
&gt;&gt; Thanks.
&gt;&gt; SLATKIN: All right, that's it. I'll be

01:01:41.489 --> 01:01:44.559
outside to answer your question. But please
come to the next talk with Mike Aizatsky.

01:01:44.559 --> 01:01:45.910
I really--it's a good one.

