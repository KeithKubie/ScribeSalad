WEBVTT
Kind: captions
Language: en

00:00:02.750 --> 00:00:04.160
DENNIS HUO: Hi, I'm Dennis Huo.

00:00:04.160 --> 00:00:07.467
And I just want to show you how
simple, smooth, and effective

00:00:07.467 --> 00:00:09.050
running open source,
big data software

00:00:09.050 --> 00:00:11.450
can be on Google Cloud Platform.

00:00:11.450 --> 00:00:12.950
Now, let's just
begin by considering

00:00:12.950 --> 00:00:16.190
what we might expect out of any
modern programming language.

00:00:16.190 --> 00:00:18.630
It seems it's not quite
enough to be turning complete.

00:00:18.630 --> 00:00:21.350
But we probably at least
expect some standard libraries

00:00:21.350 --> 00:00:24.080
for common things like say,
sorting a one megabyte list

00:00:24.080 --> 00:00:25.346
in-memory.

00:00:25.346 --> 00:00:26.970
Well, nowadays, it
seems like everybody

00:00:26.970 --> 00:00:28.750
has a lot more
data lying around.

00:00:28.750 --> 00:00:32.090
So what if it could be just as
easy to sort a terabyte of data

00:00:32.090 --> 00:00:36.070
using hundreds or even
thousands of machines?

00:00:36.070 --> 00:00:37.670
More recently
programming languages

00:00:37.670 --> 00:00:40.090
have had to incorporate much
stronger support for things

00:00:40.090 --> 00:00:42.280
like threads and synchronization
as we all moved on

00:00:42.280 --> 00:00:45.650
to multi-processor,
multi-core systems.

00:00:45.650 --> 00:00:47.570
Nowadays, it's pretty
easy to distribute work

00:00:47.570 --> 00:00:48.950
onto hundreds of threads.

00:00:48.950 --> 00:00:51.280
What if it could be just
as easy to distribute work

00:00:51.280 --> 00:00:53.710
onto hundreds of machines?

00:00:53.710 --> 00:00:55.570
Nowadays, it's
also pretty common

00:00:55.570 --> 00:00:57.690
to find some fairly
sophisticated packages

00:00:57.690 --> 00:01:00.010
for all sorts of things
like cryptography, number

00:01:00.010 --> 00:01:02.690
theory, and graphics in
our standard libraries.

00:01:02.690 --> 00:01:06.130
What if we could also have state
of the art distributed machine

00:01:06.130 --> 00:01:09.156
learning packages as part
of our standard libraries?

00:01:09.156 --> 00:01:11.530
Well, if you've been watching
the code snippets over here

00:01:11.530 --> 00:01:13.140
the last few
slides, you may have

00:01:13.140 --> 00:01:15.850
noticed that none of what I
just said is hypothetical.

00:01:15.850 --> 00:01:18.560
It's all already possible
right now, today.

00:01:18.560 --> 00:01:21.150
Which is fortunate because just
as we spent the last decade

00:01:21.150 --> 00:01:23.180
finally getting the
hang of programming

00:01:23.180 --> 00:01:27.256
for multi-threaded systems
on a single machine, nowadays

00:01:27.256 --> 00:01:29.630
it seems more and more common
to find yourself developing

00:01:29.630 --> 00:01:33.700
for a massively distributed
system of hundreds of machines.

00:01:33.700 --> 00:01:35.570
And with these changes
in the landscape,

00:01:35.570 --> 00:01:38.390
it's pretty easy to
imagine that big data could

00:01:38.390 --> 00:01:40.420
be a perfect match for a cloud.

00:01:40.420 --> 00:01:43.270
All the key strengths of cloud
like on-demand provisioning,

00:01:43.270 --> 00:01:46.710
seamless scaling, the separation
of storage from computation,

00:01:46.710 --> 00:01:49.500
and of course, just
having such easy access

00:01:49.500 --> 00:01:51.924
to such a wide variety
of services and tools.

00:01:51.924 --> 00:01:53.340
These all contribute
to opening up

00:01:53.340 --> 00:01:55.190
a whole new universe
of possibilities

00:01:55.190 --> 00:01:57.620
with countless ways to
assemble these building

00:01:57.620 --> 00:02:00.500
blocks into something
new and unique.

00:02:00.500 --> 00:02:02.020
And through this
shared evolution

00:02:02.020 --> 00:02:04.730
of big data and
distributed computing,

00:02:04.730 --> 00:02:07.550
big data has become a very
core ingredient of innovation

00:02:07.550 --> 00:02:10.990
for companies big and
small, old and new alike.

00:02:10.990 --> 00:02:14.200
Take, for example, how streaming
media companies like Netflix,

00:02:14.200 --> 00:02:17.280
Pandora, and Last.fm
have contributed

00:02:17.280 --> 00:02:18.960
to changing the
way that consumers

00:02:18.960 --> 00:02:20.440
expect content to be presented.

00:02:20.440 --> 00:02:23.190
They've applied large scale
distributed machine learning

00:02:23.190 --> 00:02:26.220
techniques to power these
personalized recommendation

00:02:26.220 --> 00:02:27.140
engines.

00:02:27.140 --> 00:02:28.700
Millions of users
around the world

00:02:28.700 --> 00:02:32.130
now expect tailor-made
entertainment as the norm.

00:02:32.130 --> 00:02:35.580
As another example, just taking
a look at our Google's roots

00:02:35.580 --> 00:02:39.010
in web search and along side
a multitude of other search

00:02:39.010 --> 00:02:41.640
engines, news
aggregators, social media,

00:02:41.640 --> 00:02:44.220
and e-commerce
sites, we've all long

00:02:44.220 --> 00:02:47.320
grappled with applying
big data technologies just

00:02:47.320 --> 00:02:52.180
to tackle the sheer magnitude
of ever-growing web content.

00:02:52.180 --> 00:02:54.130
Users now expect
to find a needle

00:02:54.130 --> 00:02:56.360
in a haystack hundreds
of thousands of times,

00:02:56.360 --> 00:02:59.190
every second of every day.

00:02:59.190 --> 00:03:00.930
And amidst this
rise of big data,

00:03:00.930 --> 00:03:02.450
it's not that the
data or the tools

00:03:02.450 --> 00:03:04.570
have suddenly become magical.

00:03:04.570 --> 00:03:06.850
All these innovations
ultimately come from developers

00:03:06.850 --> 00:03:09.159
just like you, always
finding new ways

00:03:09.159 --> 00:03:11.200
to put it all together so
that big data can still

00:03:11.200 --> 00:03:13.510
mean something completely
different to each person

00:03:13.510 --> 00:03:15.250
or entrepreneur.

00:03:15.250 --> 00:03:17.340
On Google Cloud Platform,
through a combination

00:03:17.340 --> 00:03:19.980
of cloud services and a wealth
of open source technologies

00:03:19.980 --> 00:03:22.484
like Apache Hadoop
and Apache spark,

00:03:22.484 --> 00:03:24.150
you'll have everything
you need to focus

00:03:24.150 --> 00:03:27.170
on making a real impact instead
of having to worry about all

00:03:27.170 --> 00:03:30.230
the grungy little details
just to get started.

00:03:30.230 --> 00:03:32.730
Now for an idea of what
this all might look like,

00:03:32.730 --> 00:03:35.430
let's follow a data set in
the cloud on its journey

00:03:35.430 --> 00:03:38.940
through a series of
processing and analytic steps.

00:03:38.940 --> 00:03:42.190
And we'll watch as it
undergoes its transformation

00:03:42.190 --> 00:03:45.130
from raw data into
crucial insights.

00:03:45.130 --> 00:03:47.600
Now what I have
here is a set of CSV

00:03:47.600 --> 00:03:50.170
files from the Center for
Disease Control containing

00:03:50.170 --> 00:03:51.680
summary birth data
for the United

00:03:51.680 --> 00:03:55.410
States between 1969 and 2008.

00:03:55.410 --> 00:03:57.890
These files are just sitting
here in Google Cloud Storage.

00:03:57.890 --> 00:04:00.389
And, as you can see, it's pretty
easy to peek here and there

00:04:00.389 --> 00:04:02.210
just using gsutil cat.

00:04:02.210 --> 00:04:04.817
Now suppose we want to answer
some questions by analyzing

00:04:04.817 --> 00:04:06.650
this data like, for
example, whether there's

00:04:06.650 --> 00:04:10.400
a correlation between cigarette
usage and birth weight.

00:04:10.400 --> 00:04:11.960
At more than 100
million rows, it

00:04:11.960 --> 00:04:14.820
turns out we have at least
100 times too much data

00:04:14.820 --> 00:04:18.029
to fit into any normal
spreadsheet program.

00:04:18.029 --> 00:04:21.370
So that means it's time to
bring out the heavy machinery.

00:04:21.370 --> 00:04:23.760
With our command line
tool, bdutil, and just

00:04:23.760 --> 00:04:27.060
these two simple commands, you
can have your very own 100 VM

00:04:27.060 --> 00:04:29.740
cluster fully loaded up
with Apache Hadoop, Spark,

00:04:29.740 --> 00:04:32.770
and Shark, ready to go just five
minutes or so after kicking it

00:04:32.770 --> 00:04:33.910
off.

00:04:33.910 --> 00:04:36.530
Once it's done bdutil will
print out a command for SSHing

00:04:36.530 --> 00:04:38.392
into your cluster.

00:04:38.392 --> 00:04:39.850
Once we're logged
into the cluster,

00:04:39.850 --> 00:04:41.910
one easy way to get
started here is just

00:04:41.910 --> 00:04:43.740
to spin up a Shark shell.

00:04:43.740 --> 00:04:46.632
We just type shark and
this prompt will appear.

00:04:46.632 --> 00:04:48.340
Now, what we'll be
doing here is creating

00:04:48.340 --> 00:04:50.230
what's known as
an external table.

00:04:50.230 --> 00:04:52.720
So we'll just provide this
location parameter to Shark

00:04:52.720 --> 00:04:55.940
and point it at our existing
files in Google Cloud Storage.

00:04:55.940 --> 00:04:57.900
Shark will go in and
list all the files

00:04:57.900 --> 00:04:58.960
that match that location.

00:04:58.960 --> 00:05:01.850
And we can immediately start
querying those files in place,

00:05:01.850 --> 00:05:04.240
treating them just
like a SQL database.

00:05:04.240 --> 00:05:06.840
For example, once we
have this table loaded,

00:05:06.840 --> 00:05:09.680
we can select individual columns
and sort on other columns

00:05:09.680 --> 00:05:12.056
to take a peek at the data.

00:05:12.056 --> 00:05:13.930
We can also use some
handy built-in functions

00:05:13.930 --> 00:05:15.620
to calculate some
basic statistics

00:05:15.620 --> 00:05:18.660
like averages and correlations.

00:05:18.660 --> 00:05:21.920
Now one thing I've noticed about
this kind of manual analytics,

00:05:21.920 --> 00:05:26.060
is that while it's a great way
to answer some basic questions,

00:05:26.060 --> 00:05:29.780
it's an even better way
to discover new questions.

00:05:29.780 --> 00:05:32.540
For instance, over here we've
found a possible correlation

00:05:32.540 --> 00:05:35.460
between cigarette usage
and lower birth weights.

00:05:35.460 --> 00:05:38.810
So could we, perhaps,
apply what we

00:05:38.810 --> 00:05:42.380
found to build a prediction
engine for underweight births?

00:05:42.380 --> 00:05:44.730
And what about other factors
like the parent's age

00:05:44.730 --> 00:05:46.870
or alcohol consumption?

00:05:46.870 --> 00:05:50.390
Now, this kind of chain
reaction of answers leading

00:05:50.390 --> 00:05:54.450
to new questions is part of the
power of big data analytics.

00:05:54.450 --> 00:05:57.180
With the right tools at hand,
then every step along the way

00:05:57.180 --> 00:06:00.875
you tend to find new paths
and new possibilities.

00:06:00.875 --> 00:06:02.500
In our case, let's
go ahead and try out

00:06:02.500 --> 00:06:04.390
that idea of
building a prediction

00:06:04.390 --> 00:06:05.744
engine for underweight births.

00:06:05.744 --> 00:06:07.910
Now, to do this we're going
to need some distributed

00:06:07.910 --> 00:06:10.450
machine learning tools.

00:06:10.450 --> 00:06:13.750
Traditionally, building a
prediction model on 100 node

00:06:13.750 --> 00:06:15.620
cluster might have
meant years of study

00:06:15.620 --> 00:06:19.170
and maybe even getting
a Ph.D. Luckily for us,

00:06:19.170 --> 00:06:21.600
through the combined efforts
of the open source community

00:06:21.600 --> 00:06:25.410
and, in this case, especially
UC Berkeley's AMPLab,

00:06:25.410 --> 00:06:27.920
Apache Spark already comes
pre-equipped with a state

00:06:27.920 --> 00:06:30.819
of the art distributed machine
learning library called MLlib.

00:06:30.819 --> 00:06:32.610
So our Spark cluster
already has everything

00:06:32.610 --> 00:06:35.529
we need to get started
building our prediction engine.

00:06:35.529 --> 00:06:37.070
The first thing
we'll need to do here

00:06:37.070 --> 00:06:38.910
is just to extract
some of these columns

00:06:38.910 --> 00:06:42.240
as numerical feature vectors
to use as part of our model.

00:06:42.240 --> 00:06:43.890
And there's a few
ways to do this.

00:06:43.890 --> 00:06:46.140
But since we already
have a Shark prompt open,

00:06:46.140 --> 00:06:48.470
we'll just go ahead and
use a select statement

00:06:48.470 --> 00:06:50.570
to create our new data set.

00:06:50.570 --> 00:06:52.980
We'll start out with a rough
rule of thumb here, just

00:06:52.980 --> 00:06:56.360
defining underweight as being
anything less than 5.5 pounds.

00:06:56.360 --> 00:06:58.890
And we can just select a
few of these other columns

00:06:58.890 --> 00:07:02.360
that we want to use as part
of our feature vectors.

00:07:02.360 --> 00:07:06.070
We'll use this WHERE clause to
limit and sanitize our data.

00:07:06.070 --> 00:07:07.860
And we could also
just chop off the data

00:07:07.860 --> 00:07:10.080
from the year
2008, just for now,

00:07:10.080 --> 00:07:12.000
to put in a separate
location, so that we

00:07:12.000 --> 00:07:14.374
can use that as our test data,
separate from our training

00:07:14.374 --> 00:07:15.590
data.

00:07:15.590 --> 00:07:18.870
Now, since we're doing all
this as a single create table

00:07:18.870 --> 00:07:22.430
as a select query
statement, all we have to do

00:07:22.430 --> 00:07:24.990
is provide a location
parameter and tell Shark

00:07:24.990 --> 00:07:27.704
where to put these new files
once it's created them.

00:07:27.704 --> 00:07:30.120
We can also go ahead and run
that second query on the data

00:07:30.120 --> 00:07:33.070
from 2008, so that
we'll have our test

00:07:33.070 --> 00:07:36.590
data available in a separate
location, ready to go later.

00:07:36.590 --> 00:07:38.550
Sure enough, after
running these queries,

00:07:38.550 --> 00:07:41.390
we'll find a bunch of new files
have appeared in Google Cloud

00:07:41.390 --> 00:07:44.950
Storage, which we can look at
just using gsutil or Hadoop FS,

00:07:44.950 --> 00:07:47.170
for example.

00:07:47.170 --> 00:07:50.020
Now, with all our data already
prepared, all we have to do

00:07:50.020 --> 00:07:53.830
is spin up a Spark prompt so
that we have access to MLlib.

00:07:53.830 --> 00:07:56.290
It just so happens we can
pretty much copy and paste

00:07:56.290 --> 00:08:00.770
the entire MLlib Getting Started
example for support vector

00:08:00.770 --> 00:08:01.440
machines, here.

00:08:01.440 --> 00:08:03.610
And we'll just make a few
minor modifications that

00:08:03.610 --> 00:08:07.332
point it our training data
here, and plug that into Spark.

00:08:07.332 --> 00:08:09.540
Spark will go ahead and kick
off the distributed job,

00:08:09.540 --> 00:08:11.400
entering it over the
state at 600 times

00:08:11.400 --> 00:08:14.479
to train a brand new SVM model.

00:08:14.479 --> 00:08:16.020
Now, that will take
a couple minutes.

00:08:16.020 --> 00:08:19.710
And once that's done, we'll
have a fully trained model

00:08:19.710 --> 00:08:22.062
ready to go to make predictions.

00:08:22.062 --> 00:08:23.520
Here, for example,
we can just plug

00:08:23.520 --> 00:08:28.640
in a few examples of underweight
and non-underweight predictions

00:08:28.640 --> 00:08:31.310
using data from
our real data set.

00:08:31.310 --> 00:08:33.885
We can also go in and load
that separate data from 2008

00:08:33.885 --> 00:08:37.000
that we saved separately, and
rerun our model against it

00:08:37.000 --> 00:08:40.900
to make sure that our error
fraction is still comparable.

00:08:40.900 --> 00:08:43.870
Now, taking a look at
everything we've done here,

00:08:43.870 --> 00:08:46.540
I'll bet we've raised more
questions than we've answered,

00:08:46.540 --> 00:08:49.270
and probably planted
more new ideas

00:08:49.270 --> 00:08:51.180
than we've actually implemented.

00:08:51.180 --> 00:08:53.160
For instance, we could
try to extend this model

00:08:53.160 --> 00:08:55.110
to predict other
measures of health.

00:08:55.110 --> 00:08:56.870
Or maybe we can apply
the same principles

00:08:56.870 --> 00:08:59.881
but to something
other than obstetrics.

00:08:59.881 --> 00:09:01.380
We could also explore
a whole wealth

00:09:01.380 --> 00:09:03.300
of other possible
machine learning tools

00:09:03.300 --> 00:09:05.210
coming out of MLlib.

00:09:05.210 --> 00:09:08.820
Indeed, as we dive
deeper into any problem

00:09:08.820 --> 00:09:12.100
we'll usually find that the
possibilities are pretty much

00:09:12.100 --> 00:09:13.900
limitless.

00:09:13.900 --> 00:09:15.760
Now, sadly, since
we don't quite have

00:09:15.760 --> 00:09:18.900
limitless time in
this video session,

00:09:18.900 --> 00:09:21.860
we'll have to come to an end
of this leg of the journey.

00:09:21.860 --> 00:09:24.630
Now everything you've seen
here is only a tiny peek

00:09:24.630 --> 00:09:26.800
into the ever ongoing
voyage of data

00:09:26.800 --> 00:09:30.000
through ever-growing stacks
of big data analytics

00:09:30.000 --> 00:09:32.200
technologies.

00:09:32.200 --> 00:09:35.240
Hopefully, with the help
of Google Cloud Platform,

00:09:35.240 --> 00:09:37.830
your discoveries can reach
farther and spread faster

00:09:37.830 --> 00:09:40.560
by always having the right
tool for the right job,

00:09:40.560 --> 00:09:43.400
no matter what you
might come across.

00:09:43.400 --> 00:09:44.310
Thanks for tuning in.

00:09:44.310 --> 00:09:45.266
I'm Dennis Huo.

00:09:45.266 --> 00:09:46.640
And if you want
to find out more,

00:09:46.640 --> 00:09:49.055
come visit us at
developers.google.com/hadoop.

