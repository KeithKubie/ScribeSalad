WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.325
[MUSIC PLAYING]

00:00:08.090 --> 00:00:11.520
NEIL DAVIDSON: Thanks
again for the invitation.

00:00:11.520 --> 00:00:13.940
Yeah, the invitation, it came
through our communications

00:00:13.940 --> 00:00:16.040
department back in April.

00:00:16.040 --> 00:00:19.010
And obviously, it's a
time of intense debates

00:00:19.010 --> 00:00:22.790
within and outside Google about
the military application of AI.

00:00:22.790 --> 00:00:26.120
And since then, there've been
some significant developments--

00:00:26.120 --> 00:00:29.150
obviously, from your side,
Google's AI principles,

00:00:29.150 --> 00:00:32.090
but also at the international
level, which I'll touch on,

00:00:32.090 --> 00:00:34.850
governments increasingly
recognizing the need

00:00:34.850 --> 00:00:38.180
to retain human control
over weapons and decisions

00:00:38.180 --> 00:00:39.960
of the use of force.

00:00:39.960 --> 00:00:42.110
So it's a pleasure to be here.

00:00:42.110 --> 00:00:44.540
I think it's actually
a critical time

00:00:44.540 --> 00:00:47.810
for discussion on this issue.

00:00:47.810 --> 00:00:51.320
Militaries are heavily
invested in development

00:00:51.320 --> 00:00:54.420
of robotic and
digital technologies,

00:00:54.420 --> 00:00:58.220
including the software
that underpins both

00:00:58.220 --> 00:01:01.770
and, increasingly, of course,
AI and machine learning.

00:01:01.770 --> 00:01:06.500
I think it's also critical
that technology developers

00:01:06.500 --> 00:01:08.445
and the tech
industry are involved

00:01:08.445 --> 00:01:10.070
in the conversations
that are going on,

00:01:10.070 --> 00:01:14.510
both at national level
and international level.

00:01:14.510 --> 00:01:17.360
Now, I want to be totally
clear at the outset.

00:01:17.360 --> 00:01:19.340
I'm not going to solve
any of your personal

00:01:19.340 --> 00:01:22.190
or collective ethical
dilemmas, and I would never

00:01:22.190 --> 00:01:24.340
claim to or try to.

00:01:24.340 --> 00:01:26.390
But what I can do is
explain a little bit

00:01:26.390 --> 00:01:29.570
how the ICRC, the International
Committee of the Red Cross,

00:01:29.570 --> 00:01:34.250
approaches the issues of
new technologies of warfare.

00:01:34.250 --> 00:01:38.780
And so before I
get into that, I'm

00:01:38.780 --> 00:01:42.180
actually just going to say a bit
more about the bigger picture.

00:01:42.180 --> 00:01:44.600
So I'm going to actually talk
a bit more broadly than AI

00:01:44.600 --> 00:01:45.100
and ethics.

00:01:45.100 --> 00:01:46.850
I'm going to talk
about conflict.

00:01:46.850 --> 00:01:49.702
I'm going to talk about
the role of the ICRC.

00:01:49.702 --> 00:01:51.410
I'm going to talk
about new technologies.

00:01:51.410 --> 00:01:53.318
And then I'll get
into the implications,

00:01:53.318 --> 00:01:55.610
because I think it's important
to understand the bigger

00:01:55.610 --> 00:01:57.088
picture.

00:01:57.088 --> 00:01:59.630
So first of all, quickly, what
is the International Committee

00:01:59.630 --> 00:02:00.410
of the Red Cross?

00:02:00.410 --> 00:02:02.300
For those who
don't know, we're a

00:02:02.300 --> 00:02:05.120
neutral, independent, and
impartial humanitarian

00:02:05.120 --> 00:02:06.590
organization.

00:02:06.590 --> 00:02:10.370
So we work to assist and protect
victims of armed conflict

00:02:10.370 --> 00:02:12.560
and other situations of
violence around the world.

00:02:12.560 --> 00:02:15.560
Victims-- by victims,
predominantly civilians,

00:02:15.560 --> 00:02:19.970
but also combatants, fighters
who are injured or detained

00:02:19.970 --> 00:02:21.320
in armed conflict.

00:02:21.320 --> 00:02:24.220
So we're an international
organization.

00:02:24.220 --> 00:02:27.470
So our status is an
international organization,

00:02:27.470 --> 00:02:30.660
but we don't have any government
membership or participation.

00:02:30.660 --> 00:02:33.440
So we're independent.

00:02:33.440 --> 00:02:36.620
Our mandate comes from
the Geneva Conventions.

00:02:36.620 --> 00:02:39.050
International humanitarian
law, the rules of war

00:02:39.050 --> 00:02:42.110
are essentially the
basis for all our work,

00:02:42.110 --> 00:02:45.290
and I'll get a bit
more into that later.

00:02:45.290 --> 00:02:46.910
Quickly, this is where we work.

00:02:46.910 --> 00:02:50.560
Wherever there's armed
conflict, you'll find us.

00:02:50.560 --> 00:02:55.703
We're in about 87 countries,
over 14 and 1/2 thousand staff.

00:02:55.703 --> 00:02:57.620
Predominately, like I
say, where there's armed

00:02:57.620 --> 00:03:00.410
conflict, but also in
capitals around the world

00:03:00.410 --> 00:03:04.820
for humanitarian diplomacy
and preventive work,

00:03:04.820 --> 00:03:08.060
working with
governments and others.

00:03:08.060 --> 00:03:11.362
There should be probably
a small red dot--

00:03:11.362 --> 00:03:13.070
I don't know if I have
a red dot on here.

00:03:13.070 --> 00:03:14.830
I bet I do--

00:03:14.830 --> 00:03:16.960
somewhere out there
in Silicon Valley.

00:03:16.960 --> 00:03:19.100
There's now someone
this year who's

00:03:19.100 --> 00:03:20.690
working out of our
Washington office

00:03:20.690 --> 00:03:24.580
to engage with tech
companies in Silicon Valley.

00:03:28.100 --> 00:03:32.930
This is a funding slide, but
the point is not the funding.

00:03:32.930 --> 00:03:35.900
It's just to illustrate where
our major field operations are.

00:03:35.900 --> 00:03:39.980
So these are our 10 biggest
operations this year--

00:03:39.980 --> 00:03:43.190
as you can see, where there
are the major conflicts taking

00:03:43.190 --> 00:03:47.630
place or major
post-conflict countries.

00:03:47.630 --> 00:03:50.870
Funding, that slide is--

00:03:50.870 --> 00:03:52.330
it's disappeared.

00:03:52.330 --> 00:03:55.430
But essentially, the largest
portion of that graph

00:03:55.430 --> 00:03:56.510
is governments.

00:03:56.510 --> 00:03:58.820
Government's our
predominant funding.

00:03:58.820 --> 00:04:02.330
The second largest slice
is the European Commission,

00:04:02.330 --> 00:04:05.090
but we're increasingly
looking to improve funding

00:04:05.090 --> 00:04:08.312
from private donors.

00:04:08.312 --> 00:04:09.020
So what do we do?

00:04:09.020 --> 00:04:11.510
Just very briefly, one of
our major areas of work

00:04:11.510 --> 00:04:12.890
is protection.

00:04:12.890 --> 00:04:16.459
So we work to protect those
affected by armed conflict,

00:04:16.459 --> 00:04:19.880
whether it's by collecting
information on how conflicts

00:04:19.880 --> 00:04:22.070
are conducted and
raising concerns

00:04:22.070 --> 00:04:25.730
that we have with the
authorities and those party

00:04:25.730 --> 00:04:26.840
to the conflict.

00:04:26.840 --> 00:04:29.660
It could be visiting detainees.

00:04:29.660 --> 00:04:34.190
It could be helping to restore
family links between those

00:04:34.190 --> 00:04:37.060
who've been separated
during the conflict.

00:04:37.060 --> 00:04:39.000
So that's protection.

00:04:39.000 --> 00:04:40.410
There's also assistance.

00:04:40.410 --> 00:04:42.740
And this is both emergency
humanitarian assistance,

00:04:42.740 --> 00:04:46.550
but also, I would say, more
long-term capacity-building

00:04:46.550 --> 00:04:50.780
for essential services, whether
it's water and habitation,

00:04:50.780 --> 00:04:54.650
whether it's medical care
and medical services,

00:04:54.650 --> 00:04:58.340
or whether it's things
like risk awareness.

00:04:58.340 --> 00:05:00.928
The picture on the
right is risk awareness

00:05:00.928 --> 00:05:02.720
for the civilian
population about the risks

00:05:02.720 --> 00:05:04.250
of unexploded munitions.

00:05:08.180 --> 00:05:12.050
The other major aspect of
our work is prevention.

00:05:12.050 --> 00:05:16.330
And this really is
about long-term dialogue

00:05:16.330 --> 00:05:20.200
with weapon bearers, those
who carry weapons, militaries

00:05:20.200 --> 00:05:24.640
and others; the political
or military authorities;

00:05:24.640 --> 00:05:27.080
and civil society.

00:05:27.080 --> 00:05:29.680
That's a long-term
work, like I say,

00:05:29.680 --> 00:05:35.420
to promote and uphold
the rules of law--

00:05:35.420 --> 00:05:39.130
the rules of war, sorry,
the rules of law--

00:05:39.130 --> 00:05:41.890
and to carry out
humanitarian diplomacy,

00:05:41.890 --> 00:05:45.580
basically in support of
our humanitarian work.

00:05:45.580 --> 00:05:49.610
The picture on the right
is taken from our desk

00:05:49.610 --> 00:05:52.180
a few weeks ago in the Hague
at the meeting of the Chemical

00:05:52.180 --> 00:05:54.970
Weapons Convention.

00:05:54.970 --> 00:06:00.070
So rather than try and explain
to you the rules of war

00:06:00.070 --> 00:06:03.040
in a rather roundabout
fashion, I'm

00:06:03.040 --> 00:06:04.937
going to use this short video.

00:06:04.937 --> 00:06:05.604
[VIDEO PLAYBACK]

00:06:05.604 --> 00:06:08.436
[BIRDS CHIRPING]

00:06:12.220 --> 00:06:15.880
- Since the beginning, humans
have resorted to violence

00:06:15.880 --> 00:06:19.550
as a way to settle
disagreements.

00:06:19.550 --> 00:06:22.810
Yet, through the ages,
people from around the world

00:06:22.810 --> 00:06:25.756
have tried to limit
the brutality of war.

00:06:25.756 --> 00:06:28.036
[GRUNTING]

00:06:28.036 --> 00:06:29.820
[EXPLOSION]

00:06:30.320 --> 00:06:32.930
It was this
humanitarian spirit that

00:06:32.930 --> 00:06:36.380
led to the first Geneva
Convention of 1864

00:06:36.380 --> 00:06:40.250
and to the birth of modern
international humanitarian law.

00:06:40.250 --> 00:06:42.130
[CHATTER]

00:06:43.070 --> 00:06:45.260
Setting the basic
limits on how wars

00:06:45.260 --> 00:06:48.200
can be fought, these
universal laws of war

00:06:48.200 --> 00:06:49.745
protect those not fighting--

00:06:49.745 --> 00:06:51.200
[BABY LAUGHING]

00:06:51.200 --> 00:06:54.560
--as well as those
no longer able to.

00:06:54.560 --> 00:06:57.140
To do this, a
distinction must always

00:06:57.140 --> 00:07:00.470
be made between who or
what may be attacked

00:07:00.470 --> 00:07:03.740
and who or what must be
spared and protected.

00:07:03.740 --> 00:07:07.460
Most importantly, civilians
can never be targeted.

00:07:07.460 --> 00:07:09.270
To do so is a war crime.

00:07:09.270 --> 00:07:12.504
[BOY MIMICKING CAR SOUNDS]

00:07:14.360 --> 00:07:16.120
- When they drove
into our village,

00:07:16.120 --> 00:07:18.245
they shouted that they were
going to kill everyone.

00:07:18.245 --> 00:07:18.986
[CAR DOOR OPENING]

00:07:18.986 --> 00:07:19.472
[GASP]

00:07:19.472 --> 00:07:20.014
[DOG BARKING]

00:07:20.014 --> 00:07:20.930
[CAR DOOR CLOSING]

00:07:20.930 --> 00:07:22.160
I was so scared.

00:07:22.160 --> 00:07:24.926
I ran to hide in the bush.

00:07:24.926 --> 00:07:25.890
[WOMAN SCREAMING]

00:07:25.890 --> 00:07:28.640
I heard my mother screaming.

00:07:28.640 --> 00:07:31.228
I thought I would
never see her again.

00:07:31.228 --> 00:07:33.180
[GLASS SHATTERING]

00:07:34.160 --> 00:07:35.990
- Every possible
care must be taken

00:07:35.990 --> 00:07:39.650
to avoid harming civilians
or destroying things

00:07:39.650 --> 00:07:42.252
essential for their survival.

00:07:42.252 --> 00:07:45.690
[COW MOOING]

00:07:45.690 --> 00:07:50.470
They have a right to
receive the help they need.

00:07:50.470 --> 00:07:53.350
[JAIL DOOR OPENING]

00:07:54.310 --> 00:07:56.710
[WATER DRIPPING]

00:07:59.600 --> 00:08:02.840
- The conditions prisoners lived
in never used to bother me.

00:08:06.080 --> 00:08:08.660
People like him the reason
my brother was dead.

00:08:08.660 --> 00:08:10.160
[GRUNTING]

00:08:10.160 --> 00:08:13.190
He was the enemy and
was nothing to me.

00:08:13.190 --> 00:08:17.360
But then I realized that behind
bars, he was out of action

00:08:17.360 --> 00:08:21.110
and no longer a threat
to me and my family.

00:08:21.110 --> 00:08:24.680
- The laws of war prohibit
torture and other ill treatment

00:08:24.680 --> 00:08:26.840
of detainees,
whatever their past.

00:08:26.840 --> 00:08:28.028
[HEAVY BREATHING]

00:08:29.360 --> 00:08:32.400
They must be given
food and water,

00:08:32.400 --> 00:08:34.280
and allowed to communicate
with loved ones.

00:08:34.280 --> 00:08:35.480
[SCRIBBLING]

00:08:35.980 --> 00:08:36.830
[BIRD CHIRPING]

00:08:36.830 --> 00:08:39.544
This preserves their dignity
and keeps them alive.

00:08:42.595 --> 00:08:45.391
[HEART MONITOR BEEPING]

00:08:46.800 --> 00:08:49.890
Medical workers save
lives, sometimes

00:08:49.890 --> 00:08:51.420
in the most
dangerous conditions.

00:08:51.420 --> 00:08:52.336
[EXPLOSION]

00:08:52.336 --> 00:08:54.168
[MACHINE GUN RATTLING IN

00:08:54.168 --> 00:08:55.550
DISTANCE]

00:08:55.550 --> 00:08:59.540
- Fighters from both sides were
wounded in a deadly battle.

00:08:59.540 --> 00:09:02.870
We were taking them to
the nearest hospital.

00:09:02.870 --> 00:09:04.850
At the checkpoint,
a soldier threatened

00:09:04.850 --> 00:09:07.520
us to treat his men only.

00:09:07.520 --> 00:09:09.950
We were running out of
time, and I was afraid

00:09:09.950 --> 00:09:11.810
that now all of them
were going to die.

00:09:11.810 --> 00:09:13.170
[WHISTLES]

00:09:13.170 --> 00:09:16.980
- Medical workers must always
be allowed to do their job,

00:09:16.980 --> 00:09:20.550
and the Red Cross or Red
Crescent must not be attacked.

00:09:20.550 --> 00:09:23.820
The sick or wounded have a
right to be cared for regardless

00:09:23.820 --> 00:09:25.050
of whose side they are on.

00:09:25.050 --> 00:09:28.396
[HEART MONITOR BEEPING]

00:09:28.396 --> 00:09:31.742
[HEART MONITOR BEEPING
 INCREASES]

00:09:33.190 --> 00:09:35.740
- Advances in weapons
technology have

00:09:35.740 --> 00:09:39.760
meant that the rules of
war have also had to adapt.

00:09:39.760 --> 00:09:42.190
Because some weapons
and methods of warfare

00:09:42.190 --> 00:09:45.430
don't distinguish between
fighters and civilians,

00:09:45.430 --> 00:09:47.440
limits on their use
have been agreed.

00:09:47.440 --> 00:09:49.830
[SHOUTING]

00:09:49.830 --> 00:09:52.220
[EXPLOSION]

00:09:53.660 --> 00:09:56.090
In the future,
wars may be fought

00:09:56.090 --> 00:09:57.695
with fully autonomous robots.

00:10:00.530 --> 00:10:03.260
But will such robots
ever have the ability

00:10:03.260 --> 00:10:06.320
to distinguish between a
military target and someone who

00:10:06.320 --> 00:10:07.520
must never be attacked?

00:10:13.940 --> 00:10:17.540
No matter how sophisticated
weapons become,

00:10:17.540 --> 00:10:21.195
it is essential that they are
in line with the rules of war.

00:10:21.195 --> 00:10:23.570
[MUSIC PLAYING]

00:10:23.570 --> 00:10:26.390
International humanitarian
law is all about

00:10:26.390 --> 00:10:29.930
making choices that preserve
a minimum of human dignity

00:10:29.930 --> 00:10:34.100
in times of war and make sure
that living together again

00:10:34.100 --> 00:10:37.050
is possible once the last
bullet has been shot.

00:10:37.050 --> 00:10:41.026
[MUSIC PLAYING]

00:10:47.984 --> 00:10:48.980
[END PLAYBACK]

00:10:48.980 --> 00:10:50.897
NEIL DAVIDSON: So yeah,
it's all about limits,

00:10:50.897 --> 00:10:53.060
international humanitarian
law, the rules of war.

00:10:53.060 --> 00:10:55.730
So it doesn't say anything
about whether or not

00:10:55.730 --> 00:10:57.340
a war starts, whether it ends.

00:10:57.340 --> 00:10:59.960
It's about the
conduct, essentially.

00:10:59.960 --> 00:11:03.310
And it aims to limit
the consequences.

00:11:03.310 --> 00:11:06.640
As mentioned in
the video, weapons

00:11:06.640 --> 00:11:08.650
have particular
consequences, and the ICRC

00:11:08.650 --> 00:11:14.650
has worked on weapons
issues for over 150 years.

00:11:14.650 --> 00:11:19.210
Our two core, I
guess, raison d'etre

00:11:19.210 --> 00:11:21.210
in that area, our
core areas of work,

00:11:21.210 --> 00:11:23.800
are to look at the
humanitarian consequences, what

00:11:23.800 --> 00:11:27.653
we see in armed conflict, and,
interlinked, their compliance

00:11:27.653 --> 00:11:28.570
with the rules of war.

00:11:28.570 --> 00:11:30.778
And that includes considering
whether new rules might

00:11:30.778 --> 00:11:35.110
be needed, as, in the past,
we've called for new rules

00:11:35.110 --> 00:11:37.840
on chemical weapons,
banning them;

00:11:37.840 --> 00:11:41.470
banning blinding laser
weapons; banning landmines;

00:11:41.470 --> 00:11:44.380
banning cluster munitions.

00:11:44.380 --> 00:11:47.920
Rules to regulate the arms
trade-- even last year,

00:11:47.920 --> 00:11:54.010
over 120 governments agreed a
treaty to ban nuclear weapons.

00:11:54.010 --> 00:11:56.350
And all of this, our
contribution to this

00:11:56.350 --> 00:12:00.730
is based on either
the real or perceived

00:12:00.730 --> 00:12:03.010
potential humanitarian
consequences.

00:12:06.200 --> 00:12:09.130
In today's conflicts,
this is really

00:12:09.130 --> 00:12:11.820
what kills most civilians.

00:12:11.820 --> 00:12:14.920
It's heavy explosive
weapons in towns and cities,

00:12:14.920 --> 00:12:18.010
and it's arms trade
transferring weapons

00:12:18.010 --> 00:12:22.180
to those who aren't using them
in compliance with the law.

00:12:22.180 --> 00:12:25.720
So this is something we're
also working on, of course.

00:12:25.720 --> 00:12:27.730
But we also try to
look ahead and look

00:12:27.730 --> 00:12:31.110
at how new technologies
are changing conflict

00:12:31.110 --> 00:12:33.517
to look at what
the consequences,

00:12:33.517 --> 00:12:35.350
in humanitarian terms,
might be, and to look

00:12:35.350 --> 00:12:37.480
at what the legal
issues might be,

00:12:37.480 --> 00:12:39.760
look at the compatibility
with existing law.

00:12:39.760 --> 00:12:43.180
This is a meeting we
held a few weeks ago

00:12:43.180 --> 00:12:46.150
on the humanitarian
consequences of cyber weapons,

00:12:46.150 --> 00:12:49.000
looking at potential
risks for health care,

00:12:49.000 --> 00:12:53.080
for essential civilian services,
and even the internet core

00:12:53.080 --> 00:12:55.960
structure.

00:12:55.960 --> 00:13:02.380
Now, I would say our focus
over the last 15 years

00:13:02.380 --> 00:13:06.610
has been predominantly, as you
would expect, on increasingly

00:13:06.610 --> 00:13:10.330
autonomous robotic systems,
on cyber weapons and warfare,

00:13:10.330 --> 00:13:15.320
and now increasingly on the
software that underpins both.

00:13:15.320 --> 00:13:19.260
For us, I should say,
we're not anti-technology.

00:13:19.260 --> 00:13:21.560
It should be clear we're
not anti-technology.

00:13:21.560 --> 00:13:24.860
So it can be that
military technology, even

00:13:24.860 --> 00:13:28.520
in weapon systems, can help
with compliance with the rules.

00:13:28.520 --> 00:13:32.660
So a precision-guided bomb that
uses a laser or a GPS signal

00:13:32.660 --> 00:13:36.560
to precisely land on
a legitimate target

00:13:36.560 --> 00:13:40.310
can offer better compliance
with the rules and the necessity

00:13:40.310 --> 00:13:43.070
to distinguish between
civilians that can't be targeted

00:13:43.070 --> 00:13:45.500
and legitimate military targets.

00:13:45.500 --> 00:13:47.780
But that technology,
in itself, is not

00:13:47.780 --> 00:13:50.060
inherently good for civilians.

00:13:50.060 --> 00:13:51.830
Because if that
bomb is precisely

00:13:51.830 --> 00:13:54.810
on a hospital or
precisely on your house,

00:13:54.810 --> 00:13:58.040
then it doesn't matter that
that technology is precise.

00:13:58.040 --> 00:13:59.550
It's also the way it's used.

00:13:59.550 --> 00:14:03.910
So this is obvious, really,
but it's sometimes lost,

00:14:03.910 --> 00:14:05.077
and it's an important point.

00:14:05.077 --> 00:14:07.535
You have to look at the weapons
technology and also the way

00:14:07.535 --> 00:14:08.130
it's used.

00:14:08.130 --> 00:14:13.110
Now, this slide is in
South Lebanon, 2007.

00:14:13.110 --> 00:14:18.380
And it shows an
orchard contaminated

00:14:18.380 --> 00:14:19.775
with cluster munitions.

00:14:19.775 --> 00:14:22.310
Now, cluster munitions have
been around for a long time

00:14:22.310 --> 00:14:26.300
before they were
prohibited in 2008--

00:14:26.300 --> 00:14:29.750
designed for things like
destroying a military runway,

00:14:29.750 --> 00:14:33.650
loads of munitions
spread all over.

00:14:33.650 --> 00:14:37.040
But that's not how
they're always used.

00:14:37.040 --> 00:14:40.870
If they're deployed
around villages

00:14:40.870 --> 00:14:42.370
where people are
living and working,

00:14:42.370 --> 00:14:45.290
and if they land in
trees, then, in this case,

00:14:45.290 --> 00:14:47.640
they hang down like
Christmas decorations.

00:14:47.640 --> 00:14:49.790
As you can see, they have
a little tether on them.

00:14:49.790 --> 00:14:51.980
And when someone comes
into that orchard,

00:14:51.980 --> 00:14:55.410
they'll take your
arm off or kill you.

00:14:55.410 --> 00:14:59.690
So the point is is
that you may claim

00:14:59.690 --> 00:15:01.262
the reliability,
the effectiveness,

00:15:01.262 --> 00:15:02.720
the safety of a
certain technology,

00:15:02.720 --> 00:15:04.670
but it can't be
an abstract claim.

00:15:04.670 --> 00:15:07.340
It has to be in the real world
in the context in which it's

00:15:07.340 --> 00:15:08.990
used.

00:15:08.990 --> 00:15:10.490
And that's something
we keep in mind

00:15:10.490 --> 00:15:14.180
as we look at new technologies.

00:15:14.180 --> 00:15:16.790
So of course, everyone
has in their mind

00:15:16.790 --> 00:15:18.710
the idea of what an
autonomous weapon is.

00:15:18.710 --> 00:15:21.965
My favorite nightmare, if I can
put it that way, at the moment

00:15:21.965 --> 00:15:23.965
is from "Black Mirror,"
the episode "Metalhead."

00:15:23.965 --> 00:15:25.257
I don't know if you've seen it.

00:15:25.257 --> 00:15:27.620
I recommend it.

00:15:27.620 --> 00:15:29.960
And maybe we'll get there.

00:15:29.960 --> 00:15:32.280
I hope not, but maybe we will.

00:15:32.280 --> 00:15:38.870
But the fact is autonomous
weapons already exist,

00:15:38.870 --> 00:15:39.980
albeit a limited form.

00:15:39.980 --> 00:15:42.280
And they don't look
anything like "Black Mirror"

00:15:42.280 --> 00:15:44.990
or the Terminator.

00:15:44.990 --> 00:15:48.230
The way the ICRC defines
them, an autonomous weapon,

00:15:48.230 --> 00:15:51.710
is one that can select
and attack targets

00:15:51.710 --> 00:15:54.120
without human intervention.

00:15:54.120 --> 00:15:56.630
So it's the machine, based
on its sensors and then

00:15:56.630 --> 00:15:59.680
its programming, that is
triggered by its environments

00:15:59.680 --> 00:16:01.670
and self-initiates an attack.

00:16:01.670 --> 00:16:05.000
So it's a clear distinction
between a remote-controlled

00:16:05.000 --> 00:16:07.520
or a directly
human-controlled weapon.

00:16:07.520 --> 00:16:09.850
But critically, it's not to
do with the sophistication

00:16:09.850 --> 00:16:10.600
of the technology.

00:16:10.600 --> 00:16:13.220
It doesn't matter if it's
got quite simple programming

00:16:13.220 --> 00:16:15.140
or very advanced AI.

00:16:15.140 --> 00:16:21.570
It's the lack of human
involvement in its triggering.

00:16:21.570 --> 00:16:26.485
So I would say, if there was
one thing to think about when

00:16:26.485 --> 00:16:27.860
I think about
autonomous weapons,

00:16:27.860 --> 00:16:31.230
the first thing to think
about is unpredictability.

00:16:31.230 --> 00:16:34.550
So by definition,
a robot, a machine

00:16:34.550 --> 00:16:38.000
triggered by its environment
in a complex environment,

00:16:38.000 --> 00:16:40.820
you have some level
of unpredictability

00:16:40.820 --> 00:16:44.750
about when it's
going to fire, what

00:16:44.750 --> 00:16:49.570
it's going to fire against, and
where it's going to fire, even.

00:16:49.570 --> 00:16:52.490
And that depends a bit
the nature of the system,

00:16:52.490 --> 00:16:55.030
but there's already
inherent unpredictability.

00:16:55.030 --> 00:16:57.770
And I think that's important
to remember as we move on

00:16:57.770 --> 00:16:59.780
through this discussion.

00:16:59.780 --> 00:17:02.960
So today, what do we have?

00:17:02.960 --> 00:17:05.720
This is a report from Stockholm
International Peace Research

00:17:05.720 --> 00:17:08.690
Institute from last year.

00:17:08.690 --> 00:17:12.180
Most autonomous weapons today,
they're quite constrained.

00:17:12.180 --> 00:17:13.750
They mostly attack objects.

00:17:13.750 --> 00:17:15.500
They're mostly designed
to attack objects.

00:17:15.500 --> 00:17:19.680
And in fact, the vast majority
of air defense systems,

00:17:19.680 --> 00:17:20.587
there are--

00:17:20.587 --> 00:17:21.920
and I'll get to it in a minute--

00:17:21.920 --> 00:17:24.980
some which have some
degree of autonomy

00:17:24.980 --> 00:17:27.680
that search for targets
over a wide area there.

00:17:27.680 --> 00:17:31.940
And there are some
anti-personnel systems

00:17:31.940 --> 00:17:35.180
which have a degree of
autonomy in targeting,

00:17:35.180 --> 00:17:38.180
although they don't quite
fit the definition of being

00:17:38.180 --> 00:17:40.573
autonomous yet.

00:17:40.573 --> 00:17:42.740
I would say it's important
to say that, like I said,

00:17:42.740 --> 00:17:43.670
they're constrained.

00:17:43.670 --> 00:17:45.373
Most of them target objects.

00:17:45.373 --> 00:17:46.790
They're mostly
used in areas where

00:17:46.790 --> 00:17:48.260
there aren't many civilians.

00:17:48.260 --> 00:17:50.988
They're mostly
constrained in time

00:17:50.988 --> 00:17:52.280
that they operate autonomously.

00:17:52.280 --> 00:17:53.738
They're mostly
constrained in space

00:17:53.738 --> 00:17:55.130
that they operate autonomously.

00:17:55.130 --> 00:17:57.130
And critically, they're
mostly human-supervised.

00:17:57.130 --> 00:17:59.780
Often, the human can
intervene and deactivate

00:17:59.780 --> 00:18:04.010
even before the weapon system
carries out its attack.

00:18:04.010 --> 00:18:06.680
So air defense systems, like
I mentioned, on ships but also

00:18:06.680 --> 00:18:09.380
on military
installations on land,

00:18:09.380 --> 00:18:13.010
they detect incoming
missiles, rockets.

00:18:13.010 --> 00:18:16.500
Even some of these systems
that detect incoming rockets,

00:18:16.500 --> 00:18:20.210
they have, at the moment, the
ability for a human soldier

00:18:20.210 --> 00:18:21.692
to verify.

00:18:21.692 --> 00:18:23.150
The machine says
a rocket's coming.

00:18:23.150 --> 00:18:23.880
It's very simple.

00:18:23.880 --> 00:18:25.190
This is not advanced AI.

00:18:25.190 --> 00:18:26.805
This is trajectory and speed.

00:18:26.805 --> 00:18:28.580
If it fits in the
trajectory and speed,

00:18:28.580 --> 00:18:33.140
if it's coming to you within
this speed, then it fires.

00:18:33.140 --> 00:18:35.450
So they check it
if they have time.

00:18:35.450 --> 00:18:38.680
And they do have time, even
for an incoming rocket.

00:18:38.680 --> 00:18:39.990
The missiles, they don't.

00:18:39.990 --> 00:18:41.870
It's going too fast.

00:18:41.870 --> 00:18:44.800
And that's one of the drivers
for autonomy in general,

00:18:44.800 --> 00:18:49.003
is speed, from a
military perspective.

00:18:49.003 --> 00:18:50.420
Loitering munitions,
most of these

00:18:50.420 --> 00:18:53.100
are remote-controlled so far,
all different shapes and sizes.

00:18:53.100 --> 00:18:55.940
But some of them are
autonomous already.

00:18:55.940 --> 00:18:58.820
There's one which
searches for radars

00:18:58.820 --> 00:19:01.970
and can search for up to
nine hours over hundreds

00:19:01.970 --> 00:19:05.660
of kilometers for
a radar system.

00:19:05.660 --> 00:19:08.210
It uses a very simple
technical signature,

00:19:08.210 --> 00:19:10.670
electromagnetic
signature of a radar.

00:19:10.670 --> 00:19:14.178
But you don't know
where it's going to land

00:19:14.178 --> 00:19:15.220
over the next nine hours.

00:19:15.220 --> 00:19:17.930
You don't know when, actually.

00:19:17.930 --> 00:19:19.910
You don't even know
what it's going to hit

00:19:19.910 --> 00:19:23.220
or what's next to that radar.

00:19:23.220 --> 00:19:26.070
It illustrates the
issues with autonomy.

00:19:26.070 --> 00:19:31.130
This is the only type of
system, as far as I'm aware,

00:19:31.130 --> 00:19:34.340
that's deployed that is
directed specifically at humans.

00:19:34.340 --> 00:19:37.580
Obviously, you can have humans
inside objects like aircraft,

00:19:37.580 --> 00:19:40.400
but specifically at
humans, targeting.

00:19:40.400 --> 00:19:42.890
You find these in a handful
of countries at borders,

00:19:42.890 --> 00:19:44.660
at perimeters.

00:19:44.660 --> 00:19:48.800
Now, so far, as far
as is understood,

00:19:48.800 --> 00:19:53.810
these things can identify
human targets automatically.

00:19:53.810 --> 00:19:56.480
They use human shapes,
heat signatures.

00:19:56.480 --> 00:20:00.950
Again, I don't think it's
particularly sophisticated.

00:20:00.950 --> 00:20:04.160
But then they send a signal back
to the human operator who then

00:20:04.160 --> 00:20:07.922
decides whether it can fire.

00:20:07.922 --> 00:20:09.380
But, of course,
you don't even have

00:20:09.380 --> 00:20:11.422
to change the software
for that to be autonomous.

00:20:11.422 --> 00:20:13.610
And actually, the
manufacturers already

00:20:13.610 --> 00:20:15.935
say this could be
fully autonomous.

00:20:15.935 --> 00:20:19.070
The users, so far, doesn't
put it in that mode

00:20:19.070 --> 00:20:21.530
or decides to have it
set up differently.

00:20:21.530 --> 00:20:25.470
So the point is this
is a current issue.

00:20:25.470 --> 00:20:27.290
This is not a far-future issue.

00:20:27.290 --> 00:20:29.360
That's been around
for at least 10 years.

00:20:32.370 --> 00:20:35.930
The other thing to remember
about autonomy and weapons

00:20:35.930 --> 00:20:40.310
is that it's not about specific
categories of killer robots.

00:20:40.310 --> 00:20:41.460
It's a function.

00:20:41.460 --> 00:20:45.490
It's a function that could be
applied to any weapon system.

00:20:45.490 --> 00:20:47.897
And that's really
important to remember

00:20:47.897 --> 00:20:49.730
when you're thinking
about the implications,

00:20:49.730 --> 00:20:51.147
but also you're
thinking about how

00:20:51.147 --> 00:20:54.370
to address the implications.

00:20:54.370 --> 00:20:57.350
Militaries-- and
naturally for them--

00:20:57.350 --> 00:21:00.830
they're investing heavily
in robotic systems, armed

00:21:00.830 --> 00:21:04.370
and unarmed, air, land, and
sea of all different shapes

00:21:04.370 --> 00:21:05.640
and sizes.

00:21:05.640 --> 00:21:09.260
This is a capture from a video
explainer we did with Vox Media

00:21:09.260 --> 00:21:12.070
in the US.

00:21:12.070 --> 00:21:13.820
There's an infinite
range of the types

00:21:13.820 --> 00:21:16.000
of systems that could emerge.

00:21:16.000 --> 00:21:17.602
It could be remote-controlled.

00:21:17.602 --> 00:21:18.560
It could be autonomous.

00:21:18.560 --> 00:21:19.700
It's a function.

00:21:19.700 --> 00:21:22.160
Obviously, as
everyone's well aware,

00:21:22.160 --> 00:21:27.050
you can buy one of these type
of small drones in your local

00:21:27.050 --> 00:21:33.342
[INAUDIBLE] or electrical
shop, [INAUDIBLE] being--

00:21:33.342 --> 00:21:35.300
I don't know if you have
[INAUDIBLE] in Zurich.

00:21:35.300 --> 00:21:35.842
Probably not.

00:21:35.842 --> 00:21:37.880
What's your local
electrical shop in Zurich?

00:21:37.880 --> 00:21:39.140
AUDIENCE: MediaMarkt.

00:21:39.140 --> 00:21:41.420
NEIL DAVIDSON: MediaMarkt,
we have that in Geneva too.

00:21:41.420 --> 00:21:48.860
So yes, I was at a conference,
and the company well this

00:21:48.860 --> 00:21:50.510
was saying, well,
we could use this.

00:21:50.510 --> 00:21:52.950
We could put some grams
of explosive on it.

00:21:52.950 --> 00:21:58.160
It could be sent off to
kill people autonomously.

00:21:58.160 --> 00:22:01.190
I mean, that aside, there's
already a lot of investment

00:22:01.190 --> 00:22:05.360
in small drones and, of
course, in swarms of drones.

00:22:05.360 --> 00:22:07.490
And there's a
question about where

00:22:07.490 --> 00:22:09.320
the human role will be in that.

00:22:09.320 --> 00:22:13.640
In swarms, for example,
there's a massive incentive

00:22:13.640 --> 00:22:19.970
for autonomy, because how do
you control 500 small drones

00:22:19.970 --> 00:22:22.840
at the same time?

00:22:22.840 --> 00:22:24.590
Remote-control of small
drones are already

00:22:24.590 --> 00:22:29.875
being used by non-state armed
groups in Syria, Iraq, Ukraine.

00:22:29.875 --> 00:22:33.000
AUDIENCE: Yeah, it
was on the newspaper,

00:22:33.000 --> 00:22:36.048
ISIS uses swarm of drones
against the Russians.

00:22:36.048 --> 00:22:39.140
And Russia did not have
defense [INAUDIBLE]..

00:22:39.140 --> 00:22:41.140
NEIL DAVIDSON: Yeah, so
it's a serious question.

00:22:41.140 --> 00:22:43.510
I mean, the question is,
for all of these systems,

00:22:43.510 --> 00:22:45.580
what's the human role?

00:22:45.580 --> 00:22:49.080
To what degree would there
be autonomy in the targeting?

00:22:49.080 --> 00:22:52.570
And that's what
ICRC is focused on.

00:22:52.570 --> 00:22:54.730
Yes, flying,
navigating, driving--

00:22:54.730 --> 00:22:55.757
OK, fine.

00:22:55.757 --> 00:22:57.340
But what's the really
important point?

00:22:57.340 --> 00:22:59.410
The important point
is the targeting.

00:22:59.410 --> 00:23:02.110
And that's what's important from
a legal perspective and also

00:23:02.110 --> 00:23:03.580
an ethical perspective.

00:23:03.580 --> 00:23:08.867
Now, I would never come here
to lecture you about software.

00:23:08.867 --> 00:23:10.450
You'll know far more
about it than me.

00:23:10.450 --> 00:23:15.180
But just to say that obviously,
the central component

00:23:15.180 --> 00:23:16.930
of these future systems
is the software.

00:23:19.470 --> 00:23:24.140
It is where that
software will either

00:23:24.140 --> 00:23:26.120
be used to directly
initiate a weapon,

00:23:26.120 --> 00:23:27.800
making it an autonomous
weapon system.

00:23:27.800 --> 00:23:30.920
Or it may just be used as
a decision aid, a decision

00:23:30.920 --> 00:23:33.140
support system
for humans who may

00:23:33.140 --> 00:23:35.540
make, then, certain decisions.

00:23:35.540 --> 00:23:38.057
I think there are different
issues raised by those.

00:23:38.057 --> 00:23:40.640
I think they're more acute with
the one that directly triggers

00:23:40.640 --> 00:23:42.182
a weapon, but there
are also concerns

00:23:42.182 --> 00:23:46.832
where these systems are being
used to inform human decisions.

00:23:50.070 --> 00:23:52.090
It's not theoretical.

00:23:52.090 --> 00:23:56.880
And as you're well aware,
there's heavy military interest

00:23:56.880 --> 00:23:59.970
around different parts of the
world in military application

00:23:59.970 --> 00:24:03.750
of AI and, of course, for a
wide range of different purposes

00:24:03.750 --> 00:24:05.940
and decisions and so on.

00:24:05.940 --> 00:24:09.390
One of them is
targeting, what's known

00:24:09.390 --> 00:24:11.880
in the military as automatic
target recognition.

00:24:11.880 --> 00:24:13.530
And this is just an example--

00:24:13.530 --> 00:24:17.400
it could have been pulled
from anywhere-- about efforts

00:24:17.400 --> 00:24:20.160
to harness advances in
AI machine learning,

00:24:20.160 --> 00:24:22.260
in image recognition,
in facial recognition,

00:24:22.260 --> 00:24:23.910
in behavior
recognition, in pattern

00:24:23.910 --> 00:24:26.410
recognition for targeting.

00:24:26.410 --> 00:24:28.860
Like I say, it could be
directly to trigger a weapon,

00:24:28.860 --> 00:24:34.185
or as a decision
support aid for humans.

00:24:34.185 --> 00:24:36.960
It could be to identify
objects, people, patterns.

00:24:36.960 --> 00:24:39.840
It could be even
to predict, as I'm

00:24:39.840 --> 00:24:42.630
sure you could tell
me better than I

00:24:42.630 --> 00:24:46.320
could in terms of software
capability nowadays.

00:24:46.320 --> 00:24:49.110
So there's increasing talk,
in a conflict situation,

00:24:49.110 --> 00:24:51.740
about algorithmic warfare.

00:24:51.740 --> 00:24:56.310
And I think that's
a fair description.

00:24:56.310 --> 00:24:58.890
So the important
point I want to make

00:24:58.890 --> 00:25:03.875
is that the big picture is
this is all about decisions.

00:25:03.875 --> 00:25:06.000
It's about decisions on
the use of force, decisions

00:25:06.000 --> 00:25:07.740
to kill, injure, and destroy.

00:25:07.740 --> 00:25:11.220
It's about the relationship
between humans and machines

00:25:11.220 --> 00:25:12.570
in those decisions.

00:25:12.570 --> 00:25:17.280
And yes, for us, the most
acute decisions to look at now

00:25:17.280 --> 00:25:20.430
are decisions on
whether someone's killed

00:25:20.430 --> 00:25:22.320
or whether a building
is destroyed.

00:25:22.320 --> 00:25:25.020
But, of course, these could
have much wider implications

00:25:25.020 --> 00:25:27.810
on other decisions in
armed conflict-- arrest

00:25:27.810 --> 00:25:30.840
and detention, which type
of military operation

00:25:30.840 --> 00:25:36.102
is carried out, even nuclear
posture, strategic decisions.

00:25:36.102 --> 00:25:37.560
And there are
parallels, obviously,

00:25:37.560 --> 00:25:41.040
with the rest of society,
discussions in other areas--

00:25:41.040 --> 00:25:44.670
in transport, medicine, finance.

00:25:44.670 --> 00:25:48.360
Criminal justice is perhaps
one of the best examples where

00:25:48.360 --> 00:25:52.560
decisions affecting human lives
are already being influenced

00:25:52.560 --> 00:25:58.300
by algorithmic,
data-driven systems,

00:25:58.300 --> 00:26:03.630
but decisions which have
this kind of consequence.

00:26:03.630 --> 00:26:09.078
So at ICRC, we've been
trying to get a handle on how

00:26:09.078 --> 00:26:10.620
the technology is
developing in order

00:26:10.620 --> 00:26:14.260
to inform our legal
and ethical assessment.

00:26:14.260 --> 00:26:15.810
And like I say,
one of our concerns

00:26:15.810 --> 00:26:18.660
in general about autonomy
is unpredictability.

00:26:18.660 --> 00:26:22.810
And then we want to learn more
about AI and machine learning.

00:26:22.810 --> 00:26:25.350
And one thing that's--

00:26:25.350 --> 00:26:29.340
at least to us, and I'd really
be interested to discuss this

00:26:29.340 --> 00:26:32.010
after-- is we see
a potential problem

00:26:32.010 --> 00:26:35.310
of inherent unpredictability
with machine learning

00:26:35.310 --> 00:26:39.480
algorithms, the lack of
transparency in how they

00:26:39.480 --> 00:26:43.470
function, maybe a knowledge
of the input and the output

00:26:43.470 --> 00:26:48.480
but not what happens in the
middle, questions of bias,

00:26:48.480 --> 00:26:54.270
questions of safety, unknown
failures, and critically,

00:26:54.270 --> 00:26:55.860
changing functioning over time.

00:26:55.860 --> 00:26:59.250
Imagine a weapons system--

00:26:59.250 --> 00:27:02.580
before you introduce a weapon
system in armed conflict,

00:27:02.580 --> 00:27:03.420
you have to test it.

00:27:03.420 --> 00:27:05.920
You have to decide whether, in
the circumstances you use it,

00:27:05.920 --> 00:27:08.203
it's going to operate
within the law.

00:27:08.203 --> 00:27:10.120
Well, if it changes its
functioning over time,

00:27:10.120 --> 00:27:15.480
then you can forget it
because you can't assess that.

00:27:15.480 --> 00:27:21.870
But there are these technical
questions that are really

00:27:21.870 --> 00:27:26.450
pressing now, and urgent.

00:27:26.450 --> 00:27:30.790
The ICRC's approach-- I mean,
so what to do about all of this?

00:27:30.790 --> 00:27:35.260
Our approach is to say we
need to keep human control.

00:27:35.260 --> 00:27:37.930
We need to keep human
control over weapons systems

00:27:37.930 --> 00:27:40.040
and over decisions to use force.

00:27:40.040 --> 00:27:42.370
This is a piece by our
president earlier this year

00:27:42.370 --> 00:27:45.370
when governments were meeting
in Geneva to discuss autonomous

00:27:45.370 --> 00:27:46.620
weapons.

00:27:46.620 --> 00:27:50.080
And it's not an easy
question, actually,

00:27:50.080 --> 00:27:53.395
what is the required
level of human control.

00:27:55.930 --> 00:27:58.420
But it's a question that
needs to be answered

00:27:58.420 --> 00:28:02.290
from a legal and
ethical perspective

00:28:02.290 --> 00:28:07.300
because the loss of control
has really serious consequences

00:28:07.300 --> 00:28:11.680
for civilians, for aid workers,
for fighters, for soldiers

00:28:11.680 --> 00:28:13.980
as well.

00:28:13.980 --> 00:28:16.417
An example, my
colleague in the field

00:28:16.417 --> 00:28:18.250
mentions-- whenever I
talk about this issue,

00:28:18.250 --> 00:28:23.650
he goes, yeah, sentry weapons,
how would I negotiate access

00:28:23.650 --> 00:28:26.170
at a border or a checkpoint
with a sentry weapon?

00:28:30.490 --> 00:28:34.060
So like I there are important
both legal and ethical

00:28:34.060 --> 00:28:35.340
dimensions to this.

00:28:35.340 --> 00:28:41.470
On the legal dimension, there's
been a serious misconception

00:28:41.470 --> 00:28:45.850
over many years, this idea of
machines applying the law--

00:28:45.850 --> 00:28:49.970
somehow, inanimate
objects with legal agency.

00:28:49.970 --> 00:28:51.790
I don't know where it came from.

00:28:51.790 --> 00:28:53.840
At least from
ICRC's perspective,

00:28:53.840 --> 00:28:56.470
we've been crystal clear
that the rules of war,

00:28:56.470 --> 00:28:57.640
they apply to humans.

00:28:57.640 --> 00:28:59.590
They're applied by humans.

00:28:59.590 --> 00:29:01.900
Machines may carry out
functions with different degrees

00:29:01.900 --> 00:29:04.630
of automation, but
ultimately, human judgments

00:29:04.630 --> 00:29:07.040
are required to comply
with those laws.

00:29:07.040 --> 00:29:10.000
But then that means,
actually, the law

00:29:10.000 --> 00:29:14.230
already limits the degree of
autonomy that is acceptable.

00:29:14.230 --> 00:29:20.673
Because to explain this a bit
more, when human, soldiers,

00:29:20.673 --> 00:29:22.090
fighters are
carrying out attacks,

00:29:22.090 --> 00:29:25.300
they need to distinguish
between civilians and combatants

00:29:25.300 --> 00:29:27.520
in that specific
circumstance, not in general--

00:29:27.520 --> 00:29:29.050
there, now.

00:29:29.050 --> 00:29:31.300
They need to make a
judgment of whether,

00:29:31.300 --> 00:29:33.700
the attack they're
going to make,

00:29:33.700 --> 00:29:37.990
the risk it causes to
civilians is proportionate.

00:29:37.990 --> 00:29:40.760
So they may attack a
legitimate military target.

00:29:40.760 --> 00:29:42.010
It creates risk for civilians.

00:29:42.010 --> 00:29:43.150
Is that proportionate?

00:29:43.150 --> 00:29:47.470
They need to make that judgment
now, then, not in general.

00:29:47.470 --> 00:29:49.430
They also need to
take precautions.

00:29:49.430 --> 00:29:52.992
So the situation
changed over time.

00:29:52.992 --> 00:29:53.950
They need to cancel it.

00:29:53.950 --> 00:29:56.070
They might need to stop it.

00:29:56.070 --> 00:29:58.930
So the contextual,
they need to be there.

00:29:58.930 --> 00:30:03.070
So this means there needs
to be human involvement

00:30:03.070 --> 00:30:06.144
and it means limits on autonomy,
from a legal perspective.

00:30:08.592 --> 00:30:10.550
But, of course, there's
also the ethical issue.

00:30:10.550 --> 00:30:12.740
I mean, I know this
is something you've

00:30:12.740 --> 00:30:16.970
been grappling with in
probably a different sense.

00:30:16.970 --> 00:30:19.950
And again, it's across society.

00:30:19.950 --> 00:30:24.530
It's the role of humans and
the relationship with machines

00:30:24.530 --> 00:30:26.660
in decisions that
affect people's lives.

00:30:26.660 --> 00:30:29.840
And, of course, these ones
are the most significant types

00:30:29.840 --> 00:30:30.530
of decisions.

00:30:33.460 --> 00:30:37.070
Many governments, civil
society organizations,

00:30:37.070 --> 00:30:40.780
and much of the
public are adamant.

00:30:40.780 --> 00:30:43.405
We cannot delegate decisions
to kill to machines.

00:30:46.840 --> 00:30:48.250
What does that mean?

00:30:48.250 --> 00:30:49.750
We need to work out
what that means.

00:30:52.270 --> 00:30:55.660
For us, it means you need to
have sufficient human intent

00:30:55.660 --> 00:31:00.130
to link the intention in a
specific context of an attack

00:31:00.130 --> 00:31:02.030
to the consequences.

00:31:02.030 --> 00:31:05.110
This is not a
generalized decision

00:31:05.110 --> 00:31:06.500
to be made years in advance.

00:31:06.500 --> 00:31:10.960
This is, at that point in
time, a specific human role

00:31:10.960 --> 00:31:14.980
in that decision about
whether people are attacked,

00:31:14.980 --> 00:31:18.470
buildings are destroyed.

00:31:18.470 --> 00:31:22.300
There's also an issue
about human agency.

00:31:22.300 --> 00:31:25.000
This is something-- the
ICRC's strategy at the moment

00:31:25.000 --> 00:31:27.100
not only talks about human
control over decisions,

00:31:27.100 --> 00:31:27.970
but human agency.

00:31:27.970 --> 00:31:31.930
And I think this gets to this
idea about human intention.

00:31:31.930 --> 00:31:34.120
And there's also the
question of human dignity.

00:31:36.970 --> 00:31:37.750
What does it mean?

00:31:40.810 --> 00:31:44.020
What does it require in terms
of that human involvement,

00:31:44.020 --> 00:31:46.670
in those decisions, to
uphold human dignity?

00:31:46.670 --> 00:31:49.070
And this is about really--

00:31:49.070 --> 00:31:50.860
and people can fall
in different ways

00:31:50.860 --> 00:31:52.990
on this in terms of their
ethical perspective.

00:31:52.990 --> 00:31:55.715
Are you purely looking
at consequences,

00:31:55.715 --> 00:31:57.410
or are you also
looking at the process?

00:31:57.410 --> 00:32:00.735
Are you looking at just
if someone was killed,

00:32:00.735 --> 00:32:02.860
or are you looking at how
they were killed and why?

00:32:05.820 --> 00:32:07.830
This is an important
point about human dignity.

00:32:11.530 --> 00:32:16.320
So what's our role in all this?

00:32:16.320 --> 00:32:19.680
This is a picture from the first
meeting at the United Nations

00:32:19.680 --> 00:32:22.360
in Geneva, 2014.

00:32:22.360 --> 00:32:24.570
And I've had the pleasure--

00:32:24.570 --> 00:32:29.070
I would say sometimes not so
pleasurable experience of being

00:32:29.070 --> 00:32:30.750
there for every one of them.

00:32:35.390 --> 00:32:39.800
In a multilateral sense,
especially in the current world

00:32:39.800 --> 00:32:43.550
that we're living in,
there's been some progress,

00:32:43.550 --> 00:32:46.460
but governments are
completely divided,

00:32:46.460 --> 00:32:50.330
both on the scope of the problem
of autonomous weapons, and also

00:32:50.330 --> 00:32:52.590
what to do about it.

00:32:52.590 --> 00:32:54.260
So actually, the
majority of states

00:32:54.260 --> 00:32:58.580
now want to see new
law that would either

00:32:58.580 --> 00:33:02.060
have a requirement for
human control specified,

00:33:02.060 --> 00:33:07.820
or that would specify
category of autonomous weapons

00:33:07.820 --> 00:33:09.080
that you would prohibit.

00:33:09.080 --> 00:33:12.937
As you might have gathered
from this presentation,

00:33:12.937 --> 00:33:15.270
if you're going to have any
kind of regulatory approach,

00:33:15.270 --> 00:33:17.390
it's going to have to be along
the human control element

00:33:17.390 --> 00:33:19.682
because what I mentioned
about it being a function, not

00:33:19.682 --> 00:33:22.040
a specific category.

00:33:22.040 --> 00:33:26.150
Other governments, I would say,
want a kind of middle approach

00:33:26.150 --> 00:33:28.490
where they want to agree
some politically binding, not

00:33:28.490 --> 00:33:30.050
legally binding principles.

00:33:30.050 --> 00:33:31.910
We'd like to have
human control, and we

00:33:31.910 --> 00:33:37.060
think these are some
of the key elements.

00:33:37.060 --> 00:33:39.910
Other governments, some major
military powers among them,

00:33:39.910 --> 00:33:43.150
they said, no, we're
satisfied with existing rules.

00:33:43.150 --> 00:33:47.380
We just need to ensure
they're implemented.

00:33:47.380 --> 00:33:52.280
Now, that's on what
to do about it.

00:33:52.280 --> 00:33:56.110
Now, I think one way
to say or to clarify

00:33:56.110 --> 00:33:59.340
why some kind of international
limits are needed--

00:33:59.340 --> 00:34:01.690
and ICRC has, for
several years, been

00:34:01.690 --> 00:34:03.990
calling for international
limits to be agreed.

00:34:03.990 --> 00:34:06.640
It hasn't said whether they
should be new law, politically

00:34:06.640 --> 00:34:07.640
binding or otherwise.

00:34:07.640 --> 00:34:09.225
But there needs
to be some limits.

00:34:09.225 --> 00:34:10.600
And one of the
reasons is is when

00:34:10.600 --> 00:34:12.645
you ask someone, or a
different government

00:34:12.645 --> 00:34:14.770
or a different military or
a different person, what

00:34:14.770 --> 00:34:16.389
is sufficient human
control, they give you

00:34:16.389 --> 00:34:17.469
very different answers.

00:34:17.469 --> 00:34:20.350
Someone would say, that
means remote control,

00:34:20.350 --> 00:34:24.920
direct remote control in that
circumstance as, for example,

00:34:24.920 --> 00:34:26.545
with many existing armed drones.

00:34:26.545 --> 00:34:31.210
At the other end, someone would
say, at some point in time,

00:34:31.210 --> 00:34:34.090
I programmed this
weapons system.

00:34:34.090 --> 00:34:37.540
So therefore, in all
future circumstances,

00:34:37.540 --> 00:34:40.480
it's kind of under my control.

00:34:40.480 --> 00:34:45.460
Now, for me, that shows
there's a need to look actually

00:34:45.460 --> 00:34:48.980
what that means in practice.

00:34:48.980 --> 00:34:50.980
Like I say, we already
have some weapons systems

00:34:50.980 --> 00:34:54.310
that have been used lawfully
and without some ethical concern

00:34:54.310 --> 00:34:56.440
that aren't under
direct remote control.

00:34:56.440 --> 00:34:58.570
So perhaps it's not at that end.

00:34:58.570 --> 00:35:03.230
But then, I'm pretty doubtful
of it being at that end either.

00:35:03.230 --> 00:35:08.020
So there needs to
be work to determine

00:35:08.020 --> 00:35:10.287
what human control
means in practice,

00:35:10.287 --> 00:35:12.370
and this is where we've
been pressing governments,

00:35:12.370 --> 00:35:16.090
repeatedly and most recently
in November, to actually answer

00:35:16.090 --> 00:35:17.230
this substantive question.

00:35:17.230 --> 00:35:18.730
Because whatever
option you choose--

00:35:18.730 --> 00:35:21.022
new law, politically
binding, whatever--

00:35:21.022 --> 00:35:22.480
you need to work
out this question.

00:35:22.480 --> 00:35:23.470
What does it mean?

00:35:23.470 --> 00:35:25.240
Yes, all governments
have now agreed

00:35:25.240 --> 00:35:28.120
on the importance of the human
element, human responsibility.

00:35:28.120 --> 00:35:29.990
But what does it
mean in practice?

00:35:29.990 --> 00:35:33.060
What's required for
compliance with the law?

00:35:33.060 --> 00:35:36.100
What's required for
acceptability with our values?

00:35:36.100 --> 00:35:42.370
And this is the central question
that needs to be answered.

00:35:42.370 --> 00:35:46.870
Now, what's your role
as technologists that's

00:35:46.870 --> 00:35:48.140
working in the tech industry?

00:35:48.140 --> 00:35:52.150
And far be it for me to suggest
what your role is, of course.

00:35:52.150 --> 00:35:53.630
That's up to you.

00:35:53.630 --> 00:35:58.900
But I would say that the
engagement of technologists

00:35:58.900 --> 00:36:01.030
and technology developers
and the tech industry

00:36:01.030 --> 00:36:03.692
is critical in
these discussions,

00:36:03.692 --> 00:36:04.525
absolutely critical.

00:36:07.090 --> 00:36:10.240
And it's not just on
the technical questions.

00:36:10.240 --> 00:36:12.670
It's important on the
technical questions.

00:36:12.670 --> 00:36:16.030
So if someone says to me, this
weapons system or this type

00:36:16.030 --> 00:36:18.040
of technology, this
AI development,

00:36:18.040 --> 00:36:20.855
that's going to save civilians.

00:36:20.855 --> 00:36:22.105
That's going to protect lives.

00:36:24.880 --> 00:36:26.740
That needs to be interrogated.

00:36:26.740 --> 00:36:29.800
Someone needs to-- of course,
it's on the developer who

00:36:29.800 --> 00:36:31.108
claims that to say, well--

00:36:31.108 --> 00:36:32.650
to explain how that
technology works.

00:36:32.650 --> 00:36:37.330
But there needs to also be
some critical assessment

00:36:37.330 --> 00:36:40.480
of the technology, how it works,
not just the capabilities--

00:36:40.480 --> 00:36:42.970
I know you're all problem
solvers with new technologies--

00:36:42.970 --> 00:36:44.180
but also the limitations.

00:36:44.180 --> 00:36:46.180
Actually, I'm more
interested in the limitations

00:36:46.180 --> 00:36:50.450
than I am in the
capabilities, in some senses,

00:36:50.450 --> 00:36:52.210
not, again, because
anti-technology,

00:36:52.210 --> 00:36:54.627
but because it's important to
have a realistic assessment.

00:36:54.627 --> 00:36:56.530
Because this is not
just theoretical.

00:36:56.530 --> 00:36:59.225
It is already being
tested and used.

00:36:59.225 --> 00:37:01.600
So we need to know what the
limits of the technology are.

00:37:01.600 --> 00:37:05.830
It's not, well, in
20 years, we may

00:37:05.830 --> 00:37:07.690
reach artificial
general intelligence.

00:37:07.690 --> 00:37:10.510
It's more, we're using this
machine learning in this system

00:37:10.510 --> 00:37:11.230
now.

00:37:11.230 --> 00:37:12.063
What are the limits?

00:37:14.350 --> 00:37:17.170
So I think it's been pretty
encouraging, I would say,

00:37:17.170 --> 00:37:19.570
compared to other areas
that I've worked on,

00:37:19.570 --> 00:37:21.400
particularly biotechnologies
and the risks

00:37:21.400 --> 00:37:23.140
from biological weapons.

00:37:23.140 --> 00:37:26.170
It's been actually quite
difficult to get the engagement

00:37:26.170 --> 00:37:28.472
of biologists on
the dual-use/risks

00:37:28.472 --> 00:37:30.180
of their technologies
and their research.

00:37:30.180 --> 00:37:31.597
They say, well,
it's not relevant.

00:37:31.597 --> 00:37:33.780
We're not developing
biological weapons.

00:37:33.780 --> 00:37:34.720
It's true.

00:37:34.720 --> 00:37:39.370
They're developing vaccines
or new medicines, whatever.

00:37:39.370 --> 00:37:43.340
But there's a difference
in the tech industry,

00:37:43.340 --> 00:37:48.420
I would say, at least from my
perspective as an outsider.

00:37:48.420 --> 00:37:52.540
Technologists have been very
involved in the discussion.

00:37:52.540 --> 00:37:54.460
There have been,
of course, things

00:37:54.460 --> 00:37:56.920
like Google's AI principles.

00:37:56.920 --> 00:37:59.710
There's been Microsoft's
call for a digital peace

00:37:59.710 --> 00:38:01.000
in cyberspace.

00:38:01.000 --> 00:38:02.710
There's been a
call for regulation

00:38:02.710 --> 00:38:04.510
of facial recognition.

00:38:04.510 --> 00:38:07.470
Someone mentioned to me earlier
that also, someone at Google

00:38:07.470 --> 00:38:10.360
had been talking about concerns
about facial recognition.

00:38:10.360 --> 00:38:13.150
There have also been open
letters by scientists,

00:38:13.150 --> 00:38:17.950
by industry CEOs in robotics
and AI about autonomous weapons,

00:38:17.950 --> 00:38:20.710
raising some issues
and concerns.

00:38:20.710 --> 00:38:28.130
So I would suggest respectfully
that it's important

00:38:28.130 --> 00:38:31.710
that you stay involved and
that you get more involved.

00:38:31.710 --> 00:38:33.980
It could be to work
with organizations

00:38:33.980 --> 00:38:36.330
like ourselves to better
understand the technologies.

00:38:36.330 --> 00:38:39.890
It could be to work with NGOs.

00:38:39.890 --> 00:38:45.290
It could be more at a company
level or an industry level

00:38:45.290 --> 00:38:49.610
in terms of deciding how
technologies are applied

00:38:49.610 --> 00:38:54.297
and looking at the
risks that they raise,

00:38:54.297 --> 00:38:55.880
bringing the expertise
that you bring.

00:38:59.510 --> 00:39:02.240
Like I said, I've been at the
last five years of discussions

00:39:02.240 --> 00:39:05.930
at the UN in Geneva, and it's
predominately governments

00:39:05.930 --> 00:39:08.290
and NGOs.

00:39:08.290 --> 00:39:11.000
There's not many technologists.

00:39:11.000 --> 00:39:14.288
There's not many tech companies.

00:39:14.288 --> 00:39:19.127
And I think this
is an area where

00:39:19.127 --> 00:39:20.960
there could be more
involvement and it could

00:39:20.960 --> 00:39:24.450
be really quite beneficial.

00:39:24.450 --> 00:39:27.950
So with that, I would say, thank
you very much for listening,

00:39:27.950 --> 00:39:31.380
and I'll be pleased to carry
on the discussion with you

00:39:31.380 --> 00:39:34.040
in the time you have available.

00:39:34.040 --> 00:39:37.390
[APPLAUSE]

