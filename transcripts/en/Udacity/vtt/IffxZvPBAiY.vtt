WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:02.020
So we have the allusion of shared

00:00:02.020 --> 00:00:04.800
memory, which is implemented by physical memories that

00:00:04.800 --> 00:00:08.310
is strewn all over the entire cluster, and

00:00:08.310 --> 00:00:11.490
the hope is that the application that is

00:00:11.490 --> 00:00:14.950
running on the different nodes of this

00:00:14.950 --> 00:00:19.380
cluster will actually get speed up with increasing

00:00:19.380 --> 00:00:21.920
number of processors, but such speed up is

00:00:21.920 --> 00:00:25.120
not automatic. If the sharing that we're doing,

00:00:25.120 --> 00:00:27.690
even though DSM gives you the ability to

00:00:27.690 --> 00:00:31.110
share memory across the network, recall what our

00:00:31.110 --> 00:00:34.460
good friend Chuck Thacker told us. Shared memory

00:00:34.460 --> 00:00:38.150
scales really well when you don't share memory.

00:00:38.150 --> 00:00:41.050
So, if the sharing is too fine-grained, then

00:00:41.050 --> 00:00:43.470
no hope of speed up, especially with DSM

00:00:43.470 --> 00:00:46.580
systems. Because it is only an illusion of

00:00:46.580 --> 00:00:50.110
shared memory via software, not even physical shared memory.

00:00:50.110 --> 00:00:53.650
Even physical shared memory can lead to overheads.

00:00:53.650 --> 00:00:57.520
So, this illusion through software can result in

00:00:57.520 --> 00:01:01.780
even more overhead so you've gotta be very careful on how you share and what you

00:01:01.780 --> 00:01:05.010
share. So the basic principle is that, the

00:01:05.010 --> 00:01:08.730
computation to communication ratio has to be very

00:01:08.730 --> 00:01:15.120
high if you want any hope of speed up. So in other words, the critical sections

00:01:15.120 --> 00:01:17.750
that are execute to modify data structures

00:01:17.750 --> 00:01:21.910
better be really, really hefty critical structures before

00:01:21.910 --> 00:01:24.800
somebody else needs to access the same portion

00:01:24.800 --> 00:01:27.190
of the data. So what does this mean

00:01:27.190 --> 00:01:33.240
for shared memory codes? Well basically, if the code has a lot of dynamic data

00:01:33.240 --> 00:01:37.190
structures that are manipulated with pointers, then it

00:01:37.190 --> 00:01:40.490
can lead to a lot of implicit communication

00:01:40.490 --> 00:01:43.520
across the local area network. You think you're

00:01:43.520 --> 00:01:47.160
executing code accessing a pointer, but the pointer

00:01:47.160 --> 00:01:49.690
happens to be pointing to memory that's in

00:01:49.690 --> 00:01:53.380
a remote processor. So that implicit access to

00:01:53.380 --> 00:01:56.100
a data structure pointed to by a pointer

00:01:56.100 --> 00:01:58.520
in your program, can result in a network

00:01:58.520 --> 00:02:02.550
communication across the network. Fetching something from here,

00:02:02.550 --> 00:02:06.100
into your local memory. This is the bane

00:02:06.100 --> 00:02:08.758
of distributed shared memory. That pointer

00:02:08.758 --> 00:02:13.270
codes may result in increasing overhead

00:02:13.270 --> 00:02:15.790
for coherence maintenance, for distributed shared

00:02:15.790 --> 00:02:18.220
memory in a local area network. So

00:02:18.220 --> 00:02:22.460
you have to be very careful on how you structure codes that can execute

00:02:22.460 --> 00:02:28.420
efficiently in a cluster using DSM as the vehicle for programming.

