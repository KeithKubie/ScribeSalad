WEBVTT
Kind: captions
Language: en

00:00:00.090 --> 00:00:03.130
&gt;&gt; Okay Michael, so what I'm going to do, I'm going to step through how

00:00:03.130 --> 00:00:07.910
you find dependency trees and just stop me if it doesn't make any sense, okay?

00:00:07.910 --> 00:00:08.400
&gt;&gt; Sure.

00:00:08.400 --> 00:00:10.360
&gt;&gt; But hopefully it's fairly straightforward, at

00:00:10.360 --> 00:00:12.690
least if you understand information theory. And

00:00:12.690 --> 00:00:13.810
again I want to stress that although

00:00:13.810 --> 00:00:15.850
we're going to, we're going to spend some time doing

00:00:15.850 --> 00:00:18.210
this, this is just one example of

00:00:18.210 --> 00:00:20.620
ways that you could represent a probability distribution.

00:00:20.620 --> 00:00:21.520
It's just going to turn out that

00:00:21.520 --> 00:00:24.310
this one is particularly easy and powerful, okay?

00:00:24.310 --> 00:00:25.660
&gt;&gt; Exciting.

00:00:25.660 --> 00:00:27.680
&gt;&gt; Okay, so the first thing we have to remember, Michael,

00:00:27.680 --> 00:00:30.250
is that we have some true probability distribution which I'm just going

00:00:30.250 --> 00:00:33.070
to write as P, that we're trying to do. That's our P

00:00:33.070 --> 00:00:36.370
soup theta in this case. But the general question of how you

00:00:36.370 --> 00:00:39.310
represent a dependency tree doesn't depend on theta or anything else, there's

00:00:39.310 --> 00:00:43.040
just some underlying distribution we care about, let's call that P. Okay.

00:00:43.040 --> 00:00:46.505
And we're going to estimate it with another distribution which I'm going

00:00:46.505 --> 00:00:50.700
to represent as p-hat because, you know, for approximation. That's going to

00:00:50.700 --> 00:00:56.200
depend upon that parent function that we defined before. Okay?

00:00:56.200 --> 00:00:57.280
&gt;&gt; Alright.

00:00:57.280 --> 00:00:59.830
&gt;&gt; So somehow. You sound skeptical, Michael.

00:00:59.830 --> 00:01:02.200
&gt;&gt; Well, because I saw that you wrote those train tracks.

00:01:02.200 --> 00:01:04.060
&gt;&gt; And you figure they must mean something?

00:01:04.060 --> 00:01:05.360
&gt;&gt; Well I know they mean something. It's a,

00:01:05.360 --> 00:01:07.320
it's an information theory thing but I'm not going to

00:01:07.320 --> 00:01:09.470
remember what it is. You're going to, it's going to,

00:01:09.470 --> 00:01:11.130
you're going to say something like kl divergence or something.

00:01:11.130 --> 00:01:13.434
&gt;&gt; That's exactly right. So, somehow we want

00:01:13.434 --> 00:01:16.450
to not just find a p-hat sub theta,

00:01:16.450 --> 00:01:19.420
that is a dependency tree that represents the underlying

00:01:19.420 --> 00:01:22.050
distribution, but we want to find the best one. And

00:01:22.050 --> 00:01:24.350
so best one here sort of means closest one,

00:01:24.350 --> 00:01:26.590
or most similar, or the one that would generate the

00:01:26.590 --> 00:01:29.360
points in the best possible way. And it turns

00:01:29.360 --> 00:01:33.447
out, for those who remember information theory, that there's actually

00:01:33.447 --> 00:01:36.131
a particular measure for that and it's called the KL

00:01:36.131 --> 00:01:38.840
Divergence. You remember what the KL Divergence stands for, Michael?

00:01:38.840 --> 00:01:42.820
&gt;&gt; Kullback Leibler

00:01:42.820 --> 00:01:44.880
&gt;&gt; That's right. And it has a particular

00:01:44.880 --> 00:01:49.460
form and in this case noncontinuous variables. It

00:01:49.460 --> 00:01:53.480
has basically this form. And that's basically a

00:01:53.480 --> 00:01:56.220
measure of the divergence, that's what the D stands

00:01:56.220 --> 00:01:59.560
for, the divergence between the underlying distribution P

00:01:59.560 --> 00:02:01.855
that we care about and some other candidate distribution

00:02:01.855 --> 00:02:05.270
P-hat that we're trying to get as close

00:02:05.270 --> 00:02:08.342
as possible. So if these are the same distributions,

00:02:08.342 --> 00:02:10.070
if P-hat and P are the same

00:02:10.070 --> 00:02:12.930
distribution, the Kullback–Leibler divergence is equal to zero.

00:02:12.930 --> 00:02:14.234
&gt;&gt; Mm. Mm-hm.

00:02:14.234 --> 00:02:20.140
&gt;&gt; And as they differ or as they diverge this number gets larger and larger.

00:02:20.140 --> 00:02:24.609
Now it turns out that these numbers are unitless. They don't obey the triangle

00:02:24.609 --> 00:02:29.650
inequality. This is not a distance. This is truly a divergence. But if we

00:02:29.650 --> 00:02:33.915
can get this number to be minimized, then we know we have found a distribution

00:02:33.915 --> 00:02:38.490
P-hat, that is as close as we can get to P, okay? And

00:02:38.490 --> 00:02:42.820
we have a whole unit that Pushcar will do to remind everybody about information

00:02:42.820 --> 00:02:46.332
theory where this comes from. But basically this is the right way to define

00:02:46.332 --> 00:02:49.152
similarity between probability distributions. You just have

00:02:49.152 --> 00:02:50.500
to kind of take that on faith.

00:02:50.500 --> 00:02:53.080
&gt;&gt; Okay. Well, I'll, I'll pause this and go back and

00:02:53.080 --> 00:02:55.560
listen to Pushcar's lecture and then I'll be ready for you.

00:02:55.560 --> 00:02:59.140
&gt;&gt; Okay, beautiful. Okay. So what we really want to do is we want

00:02:59.140 --> 00:03:03.170
to minimize the Kullback–Leibler divergence the that is minimize the

00:03:03.170 --> 00:03:06.560
difference between the distribution that we're going to estimate with

00:03:06.560 --> 00:03:09.200
a dependency tree and the true underlying distribution. And just

00:03:09.200 --> 00:03:11.930
by doing some algebra, you end up getting down to

00:03:11.930 --> 00:03:15.640
what looks like a fairly simple function. So Michael, if

00:03:15.640 --> 00:03:17.880
you were, if you were paying close attention to the

00:03:17.880 --> 00:03:21.370
algebra, you will realize that well p log p, now

00:03:21.370 --> 00:03:24.330
that you've come back from Pushcar's lecture is just entropy.

00:03:24.330 --> 00:03:25.650
&gt;&gt; Yep.

00:03:25.650 --> 00:03:28.610
&gt;&gt; So or it's minus the entropy. And so

00:03:28.610 --> 00:03:31.340
I can rewrite this as simply minus the entropy

00:03:31.340 --> 00:03:35.270
of the underlying distribution, plus the sum of the

00:03:35.270 --> 00:03:39.740
conditional entropies, for each of the Xis, given its parent.

00:03:39.740 --> 00:03:42.454
Which has some, you know, sort of intuitive niceness

00:03:42.454 --> 00:03:44.250
do it, but, whatever. This is what you end

00:03:44.250 --> 00:03:46.400
up with just by doing the substitution. P log

00:03:46.400 --> 00:03:49.350
P gives you minus entropy of P minus P log

00:03:49.350 --> 00:03:53.330
P hat, which gives you the conditional entropy according

00:03:53.330 --> 00:03:57.950
to the function, the parent function pi. Okay. Well,

00:03:57.950 --> 00:04:00.480
in the end all we care about is finding

00:04:00.480 --> 00:04:03.500
the best pi. So, this term doesn't matter at all

00:04:03.500 --> 00:04:05.280
and so we end up with a kind of

00:04:05.280 --> 00:04:08.460
cost function that we would like to minimize. Which I'm

00:04:08.460 --> 00:04:11.700
going to call here J. Which depends upon pi

00:04:11.700 --> 00:04:13.880
which is just the sum of all the conditional entropies.

00:04:13.880 --> 00:04:16.260
Basically the best tree that we can find

00:04:16.260 --> 00:04:18.610
will be the one that minimizes all of the

00:04:18.610 --> 00:04:21.560
entropy for each of the features, given its

00:04:21.560 --> 00:04:24.230
parents. Does that make any intuitive sense to you?

00:04:24.230 --> 00:04:29.550
&gt;&gt; Yeah. I think so. Cause we want, we want to choose parents that are

00:04:29.550 --> 00:04:30.260
going to give us a lot of

00:04:30.260 --> 00:04:33.960
information about the values of the corresponding features.

