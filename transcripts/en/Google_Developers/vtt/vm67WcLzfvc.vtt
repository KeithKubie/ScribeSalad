WEBVTT
Kind: captions
Language: en

00:16:23.600 --> 00:16:26.686
&gt;&gt; Welcome.  Thank you for 
joining, our session will begin 

00:16:26.687 --> 00:16:28.687
soon.

00:18:43.078 --> 00:18:46.966
&gt;&gt; Hello, everyone, and welcome 
to effective maching learning 

00:18:46.967 --> 00:18:50.842
are Cloud TPUs.  I'm delighted 
to see all of you here and I'd 

00:18:50.843 --> 00:18:52.678
like to send a special welcome 
to everyone on the live stream 

00:18:52.679 --> 00:18:55.758
or anyone who is watching this 
as a recording later.  I'm Zak 

00:18:55.759 --> 00:19:00.740
Stone.  I'm the product manager 
for TensorFlow and Cloud TPUs on

00:19:00.741 --> 00:19:02.741
the Google Brain team 

00:19:04.418 --> 00:19:05.842
and this talk is it about 
supercomputers for maching 

00:19:05.843 --> 00:19:07.843
learning that are built 

00:19:09.292 --> 00:19:10.736
around Google's custom chips 
called Tensor Processing Units, 

00:19:10.737 --> 00:19:13.807
or TPUs.  We've had three 
generations of Tensor Processing

00:19:13.808 --> 00:19:15.808
Units, including the one on 

00:19:17.494 --> 00:19:19.494
the bottom that Sundar announced
in his 

00:19:20.747 --> 00:19:22.257
keynote earlier this morning but
before I get into the details 

00:19:22.258 --> 00:19:26.515
here, I just want to set the 
stage for a moment and share a 

00:19:26.516 --> 00:19:29.170
little bit of context about why 
this matters.  There's been a 

00:19:29.171 --> 00:19:31.171
tremendous amount of maching 
learning progress over the past 

00:19:33.087 --> 00:19:36.349
several years as you've probably
already heard earlier today.  If

00:19:36.350 --> 00:19:38.619
you look at the number of 
research publications on Arxiv, 

00:19:38.620 --> 00:19:40.620
which is an open site for 
sharing papers, the number of 

00:19:42.916 --> 00:19:44.979
papers is growing faster than 
Moore's Law and we're up to 

00:19:44.980 --> 00:19:46.980
something like 50 new maching 
learning papers every day.

00:19:48.091 --> 00:19:50.606
This is a tremendous rate of 
innovation.  A lot of it is 

00:19:50.607 --> 00:19:54.271
happening out in the open, which
is fantastic, and it's driving a

00:19:54.272 --> 00:19:56.099
lot of new applications and real
progress across a wide range of 

00:19:56.100 --> 00:20:00.633
fields.  As you can see here in 
just one domain computer vision,

00:20:00.634 --> 00:20:03.274
there's been a tremendous 
increase in accuracy on this 

00:20:05.924 --> 00:20:07.924
image net benchmark challenge 
just over 

00:20:10.866 --> 00:20:12.866
the past few years rising from 
the 

00:20:13.946 --> 00:20:15.768
original break through results 
with Alexnet all the way to some

00:20:15.769 --> 00:20:20.086
of the more recent models, but 
these accuracy trends come at a 

00:20:20.087 --> 00:20:22.087
cost that I'll tell you about in
a minute.

00:20:23.112 --> 00:20:24.255
First I want to help you 
understand what those sort of 

00:20:24.256 --> 00:20:26.256
applications are about.  You've 
heard a little bit about 

00:20:29.106 --> 00:20:31.106
diabetic

00:20:33.875 --> 00:20:36.314
retinopathy and also these new 
signals of heart health you can 

00:20:36.315 --> 00:20:39.969
get in a completely passive way 
just by taking a picture of the 

00:20:39.970 --> 00:20:41.970
back of the eye.  This is real 
world application that's 

00:20:44.493 --> 00:20:45.718
going to help improve people's 
lives that's only possible 

00:20:45.719 --> 00:20:47.719
because of these cutting edge 
models in maching learning.

00:20:49.405 --> 00:20:50.626
You've also probably heard about
tracking illegal logging in the 

00:20:50.627 --> 00:20:56.546
rain forest.  By putting 
recycled cell phones in trees 

00:20:56.547 --> 00:20:59.193
that can listen for sounds of 
chain saws or other activities 

00:20:59.194 --> 00:21:01.229
and help people protect rain 
forest in the central Amazon.

00:21:06.699 --> 00:21:08.699
Now, one thing that's remain 
changed

00:21:10.227 --> 00:21:12.227
even since I was in graduate 
school is 

00:21:13.728 --> 00:21:17.295
this unification across many 
different application domains.  

00:21:17.296 --> 00:21:19.968
Before, it might have been some 
one folks in vision had one 

00:21:19.969 --> 00:21:23.931
technique, the folks in speech 
had a different technique and 

00:21:23.932 --> 00:21:25.932
the folks doing machine 

00:21:27.384 --> 00:21:29.008
translation were doing yet a 
third set of techniques to go 

00:21:29.009 --> 00:21:31.009
from one language to another, 
but what we've been seeing over 

00:21:33.514 --> 00:21:35.547
the past few years is this 
consolidation, especially in 

00:21:35.548 --> 00:21:38.807
deep learning around these 
neural networks that have 

00:21:38.808 --> 00:21:40.440
similar components even in the 
details of their structure are 

00:21:40.441 --> 00:21:42.441
different.

00:21:43.701 --> 00:21:45.153
And these mural networks are now
achieving state of the art 

00:21:45.154 --> 00:21:49.244
results across all these 
different tasks.  Often these 

00:21:49.245 --> 00:21:50.869
results are at accuracies we 
don't know how to achieve any 

00:21:50.870 --> 00:21:53.563
other way.
So, now, as I mentioned earlier,

00:21:53.564 --> 00:21:57.798
this has come at a bit of a cost
which is, as these models get 

00:21:57.799 --> 00:22:00.911
more accurate, they tend to be 
larger, trained on larger data 

00:22:00.912 --> 00:22:03.888
sets and that means that you 
need more computation both to 

00:22:03.889 --> 00:22:05.938
train the model run it.

00:22:08.997 --> 00:22:10.997
What I'm showing here is this 
plot of 

00:22:15.233 --> 00:22:16.896
image recognition models on 
Image Net, this time ranked with

00:22:16.897 --> 00:22:18.897
accuracy on the 

00:22:19.992 --> 00:22:21.992
vertical axis and then the 
number of 

00:22:23.485 --> 00:22:25.485
multiply odds which is sort of a
rough 

00:22:26.554 --> 00:22:28.785
computational cost and as you 
can see, as you get to these 

00:22:28.786 --> 00:22:30.786
higher and higher accuracies, 
you're actually requiring 

00:22:34.062 --> 00:22:36.062
larger and larger amounts of 
computation.

00:22:40.067 --> 00:22:40.082
So the solution to this problem,
we believe, is specialized 

00:22:40.083 --> 00:22:42.289
hardware for really driven us to
develop these multiple 

00:22:42.290 --> 00:22:45.542
generations of Tensor Processing
Units or TPUs.  Our first TPUs 

00:22:45.543 --> 00:22:47.988
have been in our data center 
since 2015 and you've interacted

00:22:49.403 --> 00:22:51.403
with these every time you've run
a search.

00:22:56.359 --> 00:22:58.397
They're reranking the last 
several hundred links to choose 

00:22:58.398 --> 00:23:02.893
what they're sending you, 
they've been a part of Google 

00:23:02.894 --> 00:23:04.894
Photos and speech recognition 
and many of Google's large scale

00:23:05.139 --> 00:23:07.139
applications you use every day.

00:23:08.824 --> 00:23:09.843
Last year, we revealed our 
second generation Tensor 

00:23:09.844 --> 00:23:12.272
Processing Unit which are now in
public beta.

00:23:16.366 --> 00:23:18.366
Anyone can sign up, any TPU 
project 

00:23:19.855 --> 00:23:24.112
can start a TPU, you can follow 
the link g.co/Cloudtpu to learn 

00:23:24.113 --> 00:23:29.044
more.  These aren't just for 
training and inference on a 

00:23:29.045 --> 00:23:31.045
simple system, they're 

00:23:33.748 --> 00:23:35.748
really designed to be used 
together in 

00:23:36.786 --> 00:23:39.434
these large supercomputers we 
call TPU pods.  Here's a full 

00:23:39.435 --> 00:23:41.435
pod of the second generation 
TPUs on the screen right now.

00:23:44.198 --> 00:23:46.439
Finally, this morning, you're 
getting the first glimpse of our

00:23:46.440 --> 00:23:49.112
third generation TPUs assembled 
into an even 

00:23:53.800 --> 00:23:55.656
larger TPU pod that delivers 
more than  100petaflops of 

00:23:55.657 --> 00:23:57.657
compute for a single maching 
learning problem.

00:23:58.518 --> 00:24:00.518
So, I just want to emphasize 
that we are 

00:24:02.038 --> 00:24:04.679
committed to making relentless 
progress in this domain.  This 

00:24:04.680 --> 00:24:07.126
affects all of our products.  It
improves the lives of all of our

00:24:08.552 --> 00:24:10.800
users and through the Cloud, 
we're bringing these platforms 

00:24:10.801 --> 00:24:12.801
to you so you can train maching 
learning models and 

00:24:16.709 --> 00:24:19.993
run them faster and more cost 
effectively than ever before.  

00:24:19.994 --> 00:24:21.994
Just a few notes on performance,
you 

00:24:24.689 --> 00:24:26.689
can see TPU view one with 92 
teraops 

00:24:27.959 --> 00:24:29.959
only doing inference then going 
to the 

00:24:32.655 --> 00:24:35.286
Cloud TPU which is 180 
teraflops, suddenly you have 

00:24:35.287 --> 00:24:38.135
floating points, you don't have 
to worry as much.  Those are 

00:24:38.136 --> 00:24:40.004
connected together in these 
systems that as I mentioned 

00:24:40.005 --> 00:24:42.005
before are 

00:24:43.929 --> 00:24:47.795
up to 11 and a half petaflops 
and do training and inference.  

00:24:47.796 --> 00:24:50.054
You can slice them into smaller 
pieces to do many different 

00:24:50.055 --> 00:24:52.728
problems at once.
Just a year later now we're 

00:24:52.729 --> 00:24:56.424
making this leap to about 11 and
a half petaflops to more than 

00:24:56.425 --> 00:24:58.425
100.

00:24:59.482 --> 00:25:01.713
That's more than 8X the 
performance in this new system 

00:25:01.714 --> 00:25:03.714
and that's a combination of an 
entirely new chip wired together

00:25:06.195 --> 00:25:08.195
into an even larger scale 
system.

00:25:09.403 --> 00:25:11.403
So

00:25:14.621 --> 00:25:16.251
I hope the reason you're all 
here is that you're interested 

00:25:16.252 --> 00:25:18.912
in expanding the AI Frontier.  
That's what really motivates us 

00:25:18.913 --> 00:25:22.992
when we're  developing these 
platforms and building 

00:25:22.993 --> 00:25:24.819
applications on top of them.
Now, I'd really like to focus on

00:25:24.820 --> 00:25:28.277
what you can do right now with 
the Cloud TPUs that are 

00:25:28.278 --> 00:25:32.133
available today.  And that 
really starts with performance. 

00:25:32.134 --> 00:25:34.134
A lot of these maching learning 
models 

00:25:37.374 --> 00:25:39.374
can take days or weeks to train

00:25:40.956 --> 00:25:43.198
.  It hasn't been uncommon in 
the past for it to cost 

00:25:43.199 --> 00:25:45.199
thousands of dollars just to do 
one training model and we're 

00:25:46.656 --> 00:25:48.076
really interested in bringing 
down these costs and making 

00:25:48.077 --> 00:25:50.928
maching learning acceleration 
much more widely available.  

00:25:50.929 --> 00:25:53.153
But, when we're talking about 
performance, it's important to 

00:25:53.154 --> 00:25:56.614
be very specific about what we 
mean because it's quite subtle 

00:25:56.615 --> 00:26:01.184
sometimes to compare one system 
to another.  First of all, in 

00:26:01.185 --> 00:26:03.185
all of our 

00:26:05.333 --> 00:26:08.058
measurements, we try to focus on
real world data, time to 

00:26:08.059 --> 00:26:10.503
accuracy, and cost.  So, let me 
tell you what I mean.  When 

00:26:10.504 --> 00:26:11.739
you're training a maching 
learning model and you're 

00:26:11.740 --> 00:26:15.824
processing millions of images, 
it's convenient to look at how 

00:26:15.825 --> 00:26:17.459
many samples a second you're 
processing as a measure of 

00:26:17.460 --> 00:26:19.695
performance.
But those samples per second 

00:26:19.696 --> 00:26:21.696
numbers don't matter unless you 
ultimately get 

00:26:25.736 --> 00:26:27.736
where you need to go

00:26:28.846 --> 00:26:31.090
, reach the correct final 
accuracy.  So what we're really 

00:26:31.091 --> 00:26:34.139
looking to do is measure how 
fast the wheels are spinning on 

00:26:34.140 --> 00:26:35.593
the car but also make sure that 
the car gets to the right 

00:26:35.594 --> 00:26:37.639
destination and crosses the 
finish line.

00:26:40.110 --> 00:26:42.957
So, it's very important whenever
you're considering building, 

00:26:42.958 --> 00:26:44.958
buying, renting a maching 
learning system to make sure 

00:26:45.843 --> 00:26:48.326
that you're asking about not 
only steps per second or samples

00:26:48.327 --> 00:26:50.327
per second, but 

00:26:51.394 --> 00:26:53.244
also the training accuracy and 
the total time to accomplish the

00:26:53.245 --> 00:26:55.488
task that you really have in 
mind.

00:27:00.209 --> 00:27:02.849
We've also put in a lot of 
energy to try and make sure that

00:27:02.850 --> 00:27:06.304
all these ML benchmarks that we 
talk about are reproducible via 

00:27:06.305 --> 00:27:11.005
open source.  So, almost all the
numbers that I'm about to show 

00:27:11.006 --> 00:27:15.592
you.  Either you can reproduce 
today by going to -- or they'll 

00:27:15.593 --> 00:27:17.627
be open source soon so you'll be
able to reproduce them on your 

00:27:17.628 --> 00:27:19.703
own.  You don't just have to 
take my word for it.

00:27:23.774 --> 00:27:26.207
Now, Stanford recently hosted a 
competition called DAWNBench.  

00:27:26.208 --> 00:27:28.208
I've got a screen shot of the 
website here.

00:27:31.895 --> 00:27:33.554
And DAWNBench is the first 
public benchmark challenge that 

00:27:33.555 --> 00:27:35.555
we've seen really measure 
performance and accuracy 

00:27:36.606 --> 00:27:38.448
together which we think was 
great.  There was active 

00:27:38.449 --> 00:27:40.287
participation from many 
different companies and 

00:27:40.288 --> 00:27:42.288
individuals 

00:27:45.013 --> 00:27:47.013
and research groups and I'm 
happy to 

00:27:48.577 --> 00:27:49.581
announce that Cloud TPUs and TPU
pods did very well across 

00:27:49.582 --> 00:27:51.582
several of the categories.

00:27:53.568 --> 00:27:55.568
So, in the image Net training 
cost 

00:27:57.698 --> 00:28:00.412
competition we had the number 
one Don DAWNBench entry with a 

00:28:00.413 --> 00:28:03.887
model called AmoebaNet-D.  Now 
this is a really interesting 

00:28:05.313 --> 00:28:07.379
fusion of some of the work we've
been doing with these platforms 

00:28:07.380 --> 00:28:09.380
because 

00:28:11.264 --> 00:28:13.085
AmoebaNet was an architecture 
evolved from scratch on TPUs and

00:28:13.086 --> 00:28:18.029
that model not only if you train
it longer can get to state of 

00:28:18.030 --> 00:28:20.269
the art accuracy on Image Net 
but it can hit the target that 

00:28:20.270 --> 00:28:23.721
DAWNBench set of 93 percent in 
just seven and a half hours for 

00:28:23.722 --> 00:28:26.189
under $50.
This really makes that kind of 

00:28:26.190 --> 00:28:30.925
state of the art performance 
accessible to a much wider 

00:28:30.926 --> 00:28:32.537
audience than it's ever been 
before.  Now, another model that

00:28:32.538 --> 00:28:35.443
you may have heard of that's 
frequently benchmarked is ResNet

00:28:35.444 --> 00:28:40.545
50.  And so I just wanted to 
show a quick case study of 

00:28:40.546 --> 00:28:42.803
ResNet 50 running with 
TensorFlow 1.8 which is our most

00:28:42.804 --> 00:28:47.490
recent version.  We also 
submitted this to DAWNBench and 

00:28:47.491 --> 00:28:49.491
it also reaches this accuracy of
93 

00:28:50.994 --> 00:28:53.457
percent which is not -- this is 
top five accuracy.  That's not 

00:28:53.458 --> 00:28:55.458
easy to do with ResNet.  It's 
easy to fall just a little bit 

00:28:57.743 --> 00:28:59.180
short and you have to be very 
careful throughout the 

00:28:59.181 --> 00:29:01.181
implementation to achieve 

00:29:03.058 --> 00:29:05.058
that high accuracy while 
maintaining 

00:29:07.066 --> 00:29:07.096
this incredible performance of 
more than 3,250 images per 

00:29:07.097 --> 00:29:09.253
second.
Now, this took almost nine hours

00:29:09.254 --> 00:29:11.493
to train for 59 dollars, but if 
you want to go faster, part of 

00:29:11.494 --> 00:29:13.494
the reason that we 

00:29:14.768 --> 00:29:17.223
design these systems to be 
connected together is just by 

00:29:17.224 --> 00:29:19.448
changing the batch size of the 
model.  No code changes, no 

00:29:19.449 --> 00:29:22.294
complicated thinking about 
distributed systems.

00:29:25.366 --> 00:29:28.008
The we can run the same thing on
half of one of these TPU pods to

00:29:28.009 --> 00:29:30.009
hit the same level of accuracy 
in just 30 minutes.

00:29:31.878 --> 00:29:34.102
So, that's nine hours down to 30
minutes, and these are the pods 

00:29:34.103 --> 00:29:38.069
that are coming to Cloud later 
this year so you'll be able to 

00:29:38.070 --> 00:29:40.909
run results like this on your 
own or train on other models 

00:29:42.390 --> 00:29:44.390
with other data sets of your 
own.

00:29:45.432 --> 00:29:47.264
If you eliminate the check point
overhead which DAWNBench didn't 

00:29:47.265 --> 00:29:49.748
actually require us to measure, 
that's actually just 24 un 

00:29:49.749 --> 00:29:51.749
minutes.

00:29:54.029 --> 00:29:56.029
So, what I'd like to mention,  
though, 

00:29:57.037 --> 00:30:00.971
is the field is moving extremely
fast.  Just as an example, the 

00:30:00.972 --> 00:30:05.692
third place entry for training 
cost was from a nonprofit 

00:30:05.693 --> 00:30:08.557
organization called fast.AI and 
that actually came in at $72 but

00:30:10.390 --> 00:30:12.416
included some really interesting
ideas that weren't well-known.

00:30:16.591 --> 00:30:17.617
One was this progressive scaling
up of training images that 

00:30:17.618 --> 00:30:21.129
enabled the model to move even 
faster in the earlier stages of 

00:30:21.130 --> 00:30:23.182
training when it's just starting
to figure out how to complete 

00:30:25.227 --> 00:30:26.650
the task and they also used a 
much more aggressive learning 

00:30:26.651 --> 00:30:28.651
rate schedule that hadn't been 
widely used.

00:30:31.371 --> 00:30:33.620
The nice thing about Cloud TPU 
is it was easy for us to make 

00:30:33.621 --> 00:30:36.466
these changes in our own model 
and unofficially rerun the 

00:30:36.669 --> 00:30:38.912
experiment.
And these two simple changes 

00:30:38.913 --> 00:30:44.227
take our ResNet training cost 
from $59 down to just 25 with 

00:30:44.228 --> 00:30:48.525
today's on demand pricing that 
you can get on Google Cloud.  So

00:30:48.526 --> 00:30:51.362
there's an open source reference
implementation available.  We've

00:30:51.363 --> 00:30:53.234
built this into our B15 version 
of ResNet 50 and we'll have more

00:30:53.235 --> 00:30:59.833
to say about this soon.  But 
kudos to fast.AI for the great 

00:30:59.834 --> 00:31:04.502
algorithmic improvements and I'm
just delighted as a researcher, 

00:31:04.503 --> 00:31:06.503
practitioner and now a product 
manager that so much of this 

00:31:07.782 --> 00:31:09.802
progress is widely shared.  So, 
you might be wondering, well, 

00:31:12.280 --> 00:31:15.055
what's the next benchmark 
challenge.  DAWNBench finished 

00:31:15.056 --> 00:31:18.067
recently.  Where do we go next 
to compare these very different 

00:31:18.068 --> 00:31:20.706
systems, different sizes and 
scales and even different model 

00:31:21.928 --> 00:31:23.767
architectures against one 
another if we need to make a 

00:31:23.768 --> 00:31:25.837
decision about what to do when 
you want to accomplish a goal.

00:31:33.800 --> 00:31:37.669
Well, one new challenge that's 
been released recently.  There 

00:31:37.670 --> 00:31:39.504
are a bunch of researchers from 
different top Universities and 

00:31:39.505 --> 00:31:41.505
also many companies coming 
together.

00:31:44.179 --> 00:31:45.824
Trying to define a new next 
generation, it's even more 

00:31:45.825 --> 00:31:48.292
challenging than DAWNBench and 
covers many more problem 

00:31:48.293 --> 00:31:51.783
domains.  The first deadline is 
going to be in July.  This group

00:31:51.784 --> 00:31:55.037
is open to new collaborators, 
and also planning to iterate on 

00:31:55.038 --> 00:31:57.704
the definition of the benchmark 
to make sure that it really 

00:31:59.376 --> 00:32:00.188
tracks where the field is going 
so I think this is going to be 

00:32:00.189 --> 00:32:03.882
really exciting later this year 
and I encourage you to 

00:32:03.883 --> 00:32:08.393
participate and check it out.
And as I've mentioned before, I 

00:32:08.394 --> 00:32:11.871
just want to provide a little 
bit more information about what 

00:32:11.872 --> 00:32:14.758
you can do today with Cloud TPUs
if you want to experience this 

00:32:14.759 --> 00:32:17.817
kind of performance.  We've got 
this set of reference models.  

00:32:17.818 --> 00:32:19.818
I'll say more about them in a 
minute 

00:32:22.710 --> 00:32:24.940
but they're right there on 
GitHub/TensorFlow/TPU and we've 

00:32:24.941 --> 00:32:28.612
also got great documentation 
object Cloud website.  But you 

00:32:28.613 --> 00:32:33.915
don't have to remember the links
at the top.  Just g.co/Cloud TPU

00:32:33.916 --> 00:32:36.178
will take you where you need to 
go.  Now I'd like to give you a 

00:32:36.179 --> 00:32:40.242
sense of what it's like hands on
to use these Cloud TPUs.  First 

00:32:40.243 --> 00:32:42.755
of all, one thing that's a 
little different than other 

00:32:42.756 --> 00:32:45.603
accelerators is that they're 
network attached.  So what that 

00:32:45.604 --> 00:32:47.039
means is you can choose a 
virtual machine of any size or 

00:32:47.040 --> 00:32:50.722
shape.  You can, you know, 
attach other accelerators to it,

00:32:50.723 --> 00:32:52.979
if you want.  It can be large 
but we generally start 

00:32:56.256 --> 00:32:58.506
with a very tiny VM because this
is really playing just a 

00:32:58.507 --> 00:33:00.766
coordinating role.  As few as 
four cores are often enough 

00:33:03.191 --> 00:33:04.623
because all the real 
computational work are happening

00:33:04.624 --> 00:33:08.487
on the Cloud TPU.  The nice 
thing about this set-up is that 

00:33:08.488 --> 00:33:09.919
Cloud TPU can be a single device
or a slice of one of those pods.

00:33:09.920 --> 00:33:13.181
You don't have to worry about 
it.  All that's handled for you.

00:33:14.813 --> 00:33:15.628
Furthermore, you don't have to 
worry about messing with 

00:33:15.629 --> 00:33:21.129
drivers.  You can just use these
machine images that we provide. 

00:33:21.130 --> 00:33:23.566
You program these Cloud TPUs 
with TensorFlow.  Now, 

00:33:23.567 --> 00:33:24.987
TensorFlow is the most popular 
open source framework for 

00:33:24.988 --> 00:33:28.687
maching learning, one of the top
projects on GitHub, and it's 

00:33:28.688 --> 00:33:30.970
just taken off since it was open
sourced in November of 2015.

00:33:34.085 --> 00:33:35.730
We're now up to something like 
13 million downloads all around 

00:33:35.731 --> 00:33:38.832
the world.  TensorFlow supports 
many different kinds of maching 

00:33:38.833 --> 00:33:41.289
learning and has many different 
components.  When you're 

00:33:41.290 --> 00:33:44.157
programming Cloud TPUs today, 
you'll focus on layers, 

00:33:46.192 --> 00:33:48.192
estimators, the TF.

00:33:51.868 --> 00:33:54.758
data fast input pipelines, and 
then your code will be 

00:33:54.759 --> 00:33:59.055
transformed behind the scenes by
the XLA team to talk to the TPU.

00:34:03.384 --> 00:34:05.630
So, estimators and layers really
provide these high-level deep 

00:34:05.631 --> 00:34:07.631
learning 

00:34:09.084 --> 00:34:11.116
abtractions for TensorFlow had a
let you do a wide variety of 

00:34:11.117 --> 00:34:13.117
models of TPUs.

00:34:14.188 --> 00:34:16.188
Here's just a quick example of a
convolutional network.

00:34:18.857 --> 00:34:20.701
This is how you would write it 
with some estimators, what you 

00:34:20.702 --> 00:34:24.992
can see here is you just have to
make some minimal changes to map

00:34:24.993 --> 00:34:28.934
this over to the TPU.  Changes 
in the optimizer and estimator. 

00:34:28.935 --> 00:34:30.349
We're working over time to make 
these changes smaller and 

00:34:30.350 --> 00:34:32.843
smaller so it would be really 
convenient for you to take 

00:34:35.894 --> 00:34:37.894
throw code that runs on TPUs or 
GPUs, 

00:34:40.384 --> 00:34:41.667
run it on CPUs, move back and 
forth wherever you need to take 

00:34:41.668 --> 00:34:43.893
your code and run.
One important bit of advice, 

00:34:43.894 --> 00:34:48.995
though, that I'd like to just 
emphasize over and over, high 

00:34:48.996 --> 00:34:51.665
performance computing is hard on
any platform and these 

00:34:51.666 --> 00:34:55.327
accelerators are so fast that 
you need to really put in a lot 

00:34:55.328 --> 00:34:56.966
of energy to make sure that the 
whole system is balanced and 

00:34:56.967 --> 00:34:59.254
that every part of it is 
performing as well as it 

00:34:59.848 --> 00:35:04.143
possibly can.  We've done a 
tremendous amount of this work 

00:35:04.144 --> 00:35:05.997
for you with these reference 
models that I'm about to show 

00:35:05.998 --> 00:35:08.042
you that we've got open source 
on GitHub.

00:35:14.351 --> 00:35:15.571
For example, here are four 
different categories of very 

00:35:15.572 --> 00:35:17.572
different applications that are 
either supported 

00:35:20.291 --> 00:35:23.147
today or are about to be open 
source for Cloud TPU.  Over in 

00:35:23.148 --> 00:35:25.447
the image recognition column 
which is maybe best known we've 

00:35:25.448 --> 00:35:28.121
got AmoebaNet and ResNet 50, 
also bigger 

00:35:33.832 --> 00:35:35.919
versions of it, inceptions v2, 
v3, v4 but we can also do 

00:35:35.920 --> 00:35:40.211
October detection with RetinaNet
which is another one of these 

00:35:40.212 --> 00:35:42.873
models and I'll say more about 
that in a minute.  Also, if 

00:35:42.874 --> 00:35:43.894
you're interested in training a 
maching learning model and 

00:35:43.895 --> 00:35:48.854
deploying it at the edge on a 
phone or robot or vehicle or 

00:35:48.855 --> 00:35:51.508
somewhere oles, might want to 
train a smaller model with 

00:35:51.509 --> 00:35:53.509
something 

00:35:54.806 --> 00:35:55.830
like mobile net or squeeze net 
and Cloud TPUs support those as 

00:35:55.831 --> 00:36:01.734
well.  But we're not limited to 
image or object recognition.  

00:36:01.735 --> 00:36:02.964
TPUs can also do translation, 
language modeling, sentence 

00:36:02.965 --> 00:36:07.278
analysis, question answering all
with this state of the art model

00:36:07.279 --> 00:36:08.526
architecture called Transformer 
that's very powerful and 

00:36:08.527 --> 00:36:10.527
flexible.

00:36:11.609 --> 00:36:13.254
Turns out a version of 
Transformer can also perform 

00:36:13.255 --> 00:36:15.492
fantastic speech recognition and
we've just recently open 

00:36:19.721 --> 00:36:21.721
sourced an example of

00:36:28.159 --> 00:36:29.393
-- part of the Tensor to Tensor 
network.  I'll tell you more 

00:36:29.394 --> 00:36:32.036
about that in a minute.  And 
finally over in the image 

00:36:33.452 --> 00:36:35.452
generation category, we've got 
the image 

00:36:37.142 --> 00:36:39.180
Transformer and also DC gan so 
we've really got a huge range of

00:36:39.181 --> 00:36:42.071
applications that are not only 
possible to make work on the 

00:36:42.072 --> 00:36:43.918
Cloud TPU but they're working 
today with open source code that

00:36:43.919 --> 00:36:45.960
you can just download.
It's a really easy way to get 

00:36:45.961 --> 00:36:50.043
started.  So, for those of you 
who aren't as familiar with this

00:36:50.044 --> 00:36:54.585
domain, image classification is 
often taking these small images 

00:36:54.586 --> 00:36:57.005
like this one and mapping that 
to a category like lion whereas 

00:36:58.828 --> 00:37:01.472
object detection is taking 
larger images and looking for 

00:37:01.473 --> 00:37:05.560
many different objects in many 
different places in the image.  

00:37:05.561 --> 00:37:07.622
The cool thing about our 
RetinaNet implementation is it 

00:37:07.623 --> 00:37:09.623
lets you process 

00:37:11.973 --> 00:37:13.973
quite large images, 896 by 896, 
trains 

00:37:15.527 --> 00:37:17.794
to a very interesting accuracy, 
in just six hours for $40.

00:37:26.757 --> 00:37:28.757
Machine translation gets to very
close 

00:37:30.250 --> 00:37:32.711
to state of the art, again, in 
just 6.2 hours for $41.  And 

00:37:32.712 --> 00:37:34.593
these results are changing all 
the time as we continue to 

00:37:34.594 --> 00:37:36.773
optimize the platform, the 
implementations and new 

00:37:39.803 --> 00:37:40.552
research comes out, much of 
which at Google is conducted on 

00:37:40.553 --> 00:37:42.553
these TPUs.

00:37:44.625 --> 00:37:46.625
Now, language modeling, you're 
trying to predict the next word.

00:37:47.588 --> 00:37:51.454
This is an important component 
of applications like smart 

00:37:47.588 --> 00:37:49.588
reply.  And again, you take the 
language 

00:37:52.524 --> 00:37:54.524
modeling, the LM1B, the 1 
billion word 

00:37:55.973 --> 00:37:57.973
data set and train to a state of
the art 

00:38:04.137 --> 00:38:06.137
per plexity of 28 in 74 hours 
for under

00:38:10.563 --> 00:38:11.978
$500, that has the latest 
results for all these speech 

00:38:11.979 --> 00:38:14.614
recognition models.
For speech recognition we've got

00:38:14.615 --> 00:38:16.615
this 

00:38:18.720 --> 00:38:19.960
ASR Transformer which is 
training on LibriSpeech again to

00:38:19.961 --> 00:38:22.806
a really interesting error age. 
This is to just train your own 

00:38:22.807 --> 00:38:26.321
near state of the art speech 
recognition model from scratch 

00:38:26.322 --> 00:38:28.322
in whatever language or accent 
you care about and you can get 

00:38:31.226 --> 00:38:33.226
to a reasonable accuracy for 
under 100 dollars.

00:38:35.291 --> 00:38:38.154
Finally, question answering 
here, with QANet, this is just 

00:38:38.155 --> 00:38:42.225
the fast version we used to win 
the question/answer competition 

00:38:42.226 --> 00:38:44.226
for DAWNBench but there's a 

00:38:47.558 --> 00:38:48.978
question and answer that takes a
big chunk of text and then 

00:38:48.979 --> 00:38:50.979
presents a 

00:38:52.267 --> 00:38:54.312
question that you can infer from
some fact so this maching 

00:38:54.313 --> 00:38:56.545
learning model is looking at 
this whole about a paragraph 

00:39:00.636 --> 00:39:04.303
of unstructured text, reading 
the text, getting its answer.  

00:39:04.304 --> 00:39:06.580
It's really interesting and all 
this is possible on this 

00:39:06.581 --> 00:39:08.581
flexible, new, powerful platform
from maching learning.

00:39:09.222 --> 00:39:11.499
I forgot I was going to tell you
a little bit more about image 

00:39:11.500 --> 00:39:15.382
generation.  The metrics here is
actually bits per dimension and 

00:39:15.383 --> 00:39:17.383
lower is better.

00:39:18.442 --> 00:39:20.442
State of the art is 2.92 but 
under three is really good and 

00:39:21.295 --> 00:39:23.970
you can get under three on this 
metric in 30 hours for under 

00:39:23.971 --> 00:39:25.971
$200 if you're interested in 
exploring the state of the 

00:39:28.251 --> 00:39:30.080
art, cutting edge image 
generation examples, all those 

00:39:30.081 --> 00:39:32.097
images on the right were 
generated from scratch just 

00:39:33.368 --> 00:39:35.368
dreaming on the basis of a 
training set 

00:39:38.883 --> 00:39:40.883
using this image Transformer 
model

00:39:42.829 --> 00:39:45.265
so, again, why start with a 
reference model instead of just 

00:39:45.266 --> 00:39:48.143
opening a code editor and 
starting from scratch.  He's 

00:39:48.144 --> 00:39:49.783
reference models are high 
performance, they're open 

00:39:49.784 --> 00:39:55.072
source, haircut they're cutting 
edge.

00:39:58.135 --> 00:39:59.566
Often collecting, we 
continuously test them for 

00:39:59.567 --> 00:40:01.822
performance and accuracy which 
means you don't have to worry 

00:40:01.823 --> 00:40:05.602
that some subtle change in 
TensorFlow or some other part of

00:40:05.603 --> 00:40:07.472
the stack might reduce the 
accuracy without you realizing 

00:40:07.473 --> 00:40:11.685
it.  You can get up and running 
really quickly and then modify 

00:40:11.686 --> 00:40:13.536
as needed because you have the 
code, it's yours on your 

00:40:13.537 --> 00:40:15.537
infrastructure and you can train
and run on your own data.

00:40:17.196 --> 00:40:19.483
But, you're perfectly welcome to
do it the hard way.  We have 

00:40:19.484 --> 00:40:21.484
lots of detailed technical 

00:40:22.558 --> 00:40:24.558
advice in our troubleshooting 
guide and 

00:40:25.576 --> 00:40:27.576
you know,

00:40:28.930 --> 00:40:30.377
I'm not kidding that high 
performance computing is hard 

00:40:30.378 --> 00:40:32.378
and any part of the 

00:40:33.661 --> 00:40:35.097
system can become a bottle neck 
but we totally encourage you to 

00:40:35.098 --> 00:40:39.174
take the high level layers in 
API and build new models and let

00:40:39.175 --> 00:40:41.022
us know, what works for you?
Just to give you a little bit of

00:40:41.023 --> 00:40:44.884
a sense of what makes high 
performance computing so tricky,

00:40:44.885 --> 00:40:46.956
you know, if you're training 
with slower hardware, you might 

00:40:46.957 --> 00:40:50.865
find that you can overlap your 
input processing with your 

00:40:50.866 --> 00:40:52.866
training time and 

00:40:53.881 --> 00:40:55.881
so

00:40:57.814 --> 00:40:59.633
your accelerators are always 
saturated but when the 

00:40:59.634 --> 00:41:01.634
accelerators get faster 

00:41:04.522 --> 00:41:06.772
what you find as subtle is that 
you're accelerating on the -- so

00:41:06.773 --> 00:41:08.773
fortunately, we've provided a 
bunch of tools that are 

00:41:11.520 --> 00:41:13.755
connected into TensorBoard which
is this great visualization 

00:41:13.756 --> 00:41:15.756
framework associated 

00:41:17.436 --> 00:41:18.674
with TensorFlow to help you 
debug your performance, 

00:41:18.675 --> 00:41:20.675
understand what's going on, 

00:41:21.725 --> 00:41:23.544
here I'm showing steps per 
second and loss but with you 

00:41:23.545 --> 00:41:27.854
actually have this very detailed
profiler that lets you go in and

00:41:27.855 --> 00:41:29.855
get ops statistics on what's 

00:41:31.150 --> 00:41:33.150
going on in your model and we 
even have 

00:41:34.152 --> 00:41:37.257
high level recommendations.  I 
don't know if you can read but 

00:41:37.258 --> 00:41:41.354
it's telling you, in this case, 
your model is not input bound so

00:41:41.355 --> 00:41:44.835
you should spend more time on 
steps.  In other cases, it will 

00:41:44.836 --> 00:41:48.485
tell you, you're blocked on 
input.  You need a faster 

00:41:48.486 --> 00:41:52.093
pipeline.  So, we're really 
doing our best to present these 

00:41:52.094 --> 00:41:54.094
tools for experts and all 

00:41:55.783 --> 00:41:57.968
this documentation for experts 
but also to make it really easy 

00:41:57.969 --> 00:42:00.833
to get started with these 
reference models across this 

00:42:01.634 --> 00:42:03.471
huge range of applications.  
Now, those of you who are used 

00:42:03.472 --> 00:42:07.165
to training and modeling maching
learning applications, you might

00:42:07.166 --> 00:42:09.166
wonder, well, why should I jump 
to the Cloud.

00:42:13.702 --> 00:42:16.547
That seems like a different 
platform how do I think about 

00:42:16.548 --> 00:42:19.647
that?  The nice thing about 
training in the Cloud is you get

00:42:19.648 --> 00:42:22.128
all this infrastructure that you
can mix and match to get what 

00:42:24.566 --> 00:42:27.225
you need like those custom VMs I
was telling you about earlier.  

00:42:27.226 --> 00:42:29.656
You can scale up and down 
quickly.  You don't need to 

00:42:29.657 --> 00:42:33.549
build hardware, maybe scale a 
machine yourself, wait months 

00:42:33.550 --> 00:42:36.883
for hardware to arrive.  You can
sign on today.  Scale up to 

00:42:36.884 --> 00:42:38.884
hundreds of TPUs if you wish.

00:42:40.763 --> 00:42:42.807
There's great security all 
throughout Google Cloud.  Google

00:42:42.808 --> 00:42:45.463
is state of the art for 
enterprises that want cutting 

00:42:45.464 --> 00:42:47.464
edge 

00:42:48.691 --> 00:42:50.691
maching learning

00:42:52.846 --> 00:42:55.892
hardware Andrew able to reduce 
your capital expenditure.  You 

00:42:55.893 --> 00:42:57.537
can train one model for 50 bucks
and then turn the Cloud TPU off 

00:42:57.538 --> 00:43:00.209
and you're not paying for it 
anymore, which is great.

00:43:04.907 --> 00:43:06.907
So just a few words from two of 
our customers.

00:43:12.461 --> 00:43:14.084
Two Sigma is a financial firm 
and Lyft, both of them have had 

00:43:14.085 --> 00:43:17.384
great experience using Cloud 
TPUs to focus on the models 

00:43:17.385 --> 00:43:19.385
instead of worrying about 

00:43:21.485 --> 00:43:23.485
these distributed clusters of 
other 

00:43:25.982 --> 00:43:27.850
hardwares and taking month 
models that used to train for 

00:43:27.851 --> 00:43:30.104
days down to hours and with 
these larger and larger pods, 

00:43:30.105 --> 00:43:32.105
we're 

00:43:34.123 --> 00:43:36.123
really trying to drive that to 
minutes.

00:43:36.162 --> 00:43:37.815
So, let's go under the hood and 
talk in more detail about what's

00:43:37.816 --> 00:43:41.675
going on in the current 
generation of Cloud TPU.  So, 

00:43:41.676 --> 00:43:43.531
first of all, the clue actually 
includes a host with this 

00:43:43.532 --> 00:43:45.532
accelerator 

00:43:47.629 --> 00:43:49.469
all behind the scenes connected 
via PCI.  Each Cloud TPU device 

00:43:49.470 --> 00:43:53.012
has these four chips under the 
heat syncs here and I'm going to

00:43:53.013 --> 00:43:55.462
zoom in on one of them.  If you 
look at the Cloud TPU chip 

00:43:59.211 --> 00:44:00.229
layout, you'll see that it's 
broken down into two cores that 

00:44:00.230 --> 00:44:05.125
look like this.  Now, these 
cores are really built around 

00:44:05.126 --> 00:44:07.126
the systolic array which is an 

00:44:08.587 --> 00:44:10.230
old idea but it's newly relevant
in this age of maching learning 

00:44:10.231 --> 00:44:14.950
when so many of the computations
really look like gigantic matrix

00:44:14.951 --> 00:44:16.951
multipliers.

00:44:19.942 --> 00:44:23.224
But we've also got a vector unit
and scalar unit.  In this case, 

00:44:23.225 --> 00:44:27.313
it was eight gigabytes of high 
memory per core.  The new 

00:44:27.314 --> 00:44:28.942
actually doubles the memory per 
chip which is going to let you 

00:44:28.943 --> 00:44:30.943
train 

00:44:32.610 --> 00:44:34.610
even larger and more capable 
models.

00:44:38.568 --> 00:44:39.567
Focusing in on this matrix unit,
this systolic array, one thing 

00:44:39.568 --> 00:44:41.568
that's really interesting is 
we're doing it with a new 

00:44:41.595 --> 00:44:43.018
floating format called B float 
16.  I'll say more about that in

00:44:43.019 --> 00:44:46.107
a minute but it really gives us 
this great performance for 

00:44:46.108 --> 00:44:49.797
maching learning without a 
meaningful cost in accuracy.  

00:44:49.798 --> 00:44:51.798
We're still accumulating in 
float 32, though.

00:44:53.916 --> 00:44:56.173
So most of you are probably 
familiar with 32 bit floating 

00:44:56.174 --> 00:44:58.174
point.

00:45:00.261 --> 00:45:03.152
As you can see here, the orange 
bits are all devoted to 

00:45:03.153 --> 00:45:06.445
measuring these fine differences
between numbers which is really 

00:45:06.446 --> 00:45:08.446
important if you're doing high 
performance computing, detailed 

00:45:09.079 --> 00:45:10.727
simulations, that kind of thing.
But in maching learning it turns

00:45:10.728 --> 00:45:13.563
out that the range is more 
important and these maching 

00:45:13.564 --> 00:45:15.564
learning models are not super 
sensitive to these fine diverses

00:45:16.624 --> 00:45:18.624
by and large.

00:45:24.989 --> 00:45:26.212
So the IEEE start for a 16-bit 
floating points really cuts down

00:45:26.213 --> 00:45:29.043
the range of numbers that you 
can express.  One thing we're 

00:45:29.044 --> 00:45:31.044
finding with 

00:45:33.906 --> 00:45:35.906
TensorFlow is if you're training
with

00:45:38.472 --> 00:45:40.928
half precision IEEE like this 
you really have to pay attention

00:45:40.929 --> 00:45:45.227
on a model by model basis to 
make sure all of your numbers 

00:45:45.228 --> 00:45:46.842
stay in the right range and 
don't overflow and those 

00:45:46.843 --> 00:45:49.531
techniques are difficult to 
apply in general across a wide 

00:45:49.532 --> 00:45:51.532
range of models.

00:45:55.238 --> 00:45:57.238
With bfloat16, though, what 
we've done 

00:45:58.299 --> 00:46:00.119
instead is saved more bits 
fortuneit which supports the 

00:46:00.120 --> 00:46:02.589
range and it's supported 
seamlessly by our Cloud TPU 

00:46:06.866 --> 00:46:08.866
hardware so those ResNet 50 
result I was 

00:46:11.696 --> 00:46:14.627
talking about earlier were 
achieved with bfloat16 and we're

00:46:14.628 --> 00:46:18.103
still achieving that 95 percent 
accuracy with minimal effort and

00:46:18.104 --> 00:46:20.104
special casing on a model by 
model basis.

00:46:22.202 --> 00:46:24.202
So, here I'll show a brief 
animation of the systolic array.

00:46:25.658 --> 00:46:27.658
In this slide it's only three by
three 

00:46:28.695 --> 00:46:30.553
but imagine this is 128 by 128 
to get a sense of how different 

00:46:30.554 --> 00:46:34.864
this is from an ordinary central
processing unit and how much 

00:46:34.865 --> 00:46:36.280
parallelism there is which is 
where we're getting this 

00:46:36.281 --> 00:46:40.631
fantastic performance.  So, here
you have data streaming in and 

00:46:40.632 --> 00:46:43.095
what's happening as the systole 
eggic array sort of beats on 

00:46:43.096 --> 00:46:45.328
every step is you're getting 
great reuse of these 

00:46:49.225 --> 00:46:51.240
intermediate rums as the data is
flowing through the systolic 

00:46:51.241 --> 00:46:53.241
array and generating the output.

00:46:57.572 --> 00:47:00.089
So, this is super high flewput 
for flew

00:47:04.891 --> 00:47:06.551
throughput and also does a great
job of taking high level 

00:47:06.552 --> 00:47:10.028
mathematical expressions and 
then mapping them down into this

00:47:10.029 --> 00:47:11.866
coordinated dance of low level 
operations that gives us this 

00:47:11.867 --> 00:47:17.803
tremendous amount of speed.
So, where do we go from here?  

00:47:17.804 --> 00:47:19.804
Well first of all, I really want
to 

00:47:21.273 --> 00:47:24.126
emphasize something I haven't 
called out explicitly about 

00:47:24.127 --> 00:47:26.127
scaling up.

00:47:27.391 --> 00:47:28.199
I think the days of single 
systems are irrelevant.  They're

00:47:28.200 --> 00:47:30.200
over.

00:47:31.672 --> 00:47:33.672
Because what you care about 
ultimately 

00:47:35.544 --> 00:47:38.401
is how fast you get to your 
result, right?  And so what 

00:47:38.402 --> 00:47:40.240
we're trying to do with these 
Cloud TPUs and making them 

00:47:40.241 --> 00:47:42.241
widely 

00:47:45.077 --> 00:47:47.077
available in the Cloud is giving
you just a dial that you can 

00:47:47.078 --> 00:47:49.726
turn where you do your 
prototyping on an inexpensive 

00:47:51.599 --> 00:47:53.245
single device and then you 
increase the batch size without 

00:47:53.246 --> 00:47:57.521
doing any other code changes and
suddenly your training time is 

00:47:57.522 --> 00:47:59.522
going down from hours to 
minutes.

00:47:59.987 --> 00:48:02.820
And so, I'll show you two, you 
know, real measurements from our

00:48:02.821 --> 00:48:07.764
TPU systems in house here to 
give you a sense of how that's 

00:48:07.765 --> 00:48:12.030
possible.  First of all, what 
I'm demonstrating here if you 

00:48:12.031 --> 00:48:13.885
look at these TPU devices so the
second generation TPU pod has 64

00:48:13.886 --> 00:48:16.141
of these boards in it, and you 
can see 

00:48:19.830 --> 00:48:21.270
linear scaling of images per 
second up to the right herement 

00:48:21.271 --> 00:48:23.271
as you know, remember, if you 
hear images per second, 

00:48:27.381 --> 00:48:29.381
it's like, but wait, had a about
the accuracy?

00:48:31.057 --> 00:48:33.057
You can check our DAWNBench 
result.

00:48:35.062 --> 00:48:37.076
That also confirms we can get 
the high accuracy with this 

00:48:37.077 --> 00:48:39.142
scaleout but what you can see is
our interconnecting this pod is 

00:48:39.143 --> 00:48:40.068
so fast that you don't have to 
struggle the way you would with 

00:48:40.069 --> 00:48:42.925
mixing and matching hardware on 
your own, whether on premise or 

00:48:42.926 --> 00:48:47.667
in the Cloud.  You can just turn
this dial with no code changes 

00:48:47.668 --> 00:48:49.668
and benefit from this super fast
proprietary network.

00:48:51.535 --> 00:48:54.006
But, there's a more subtle point
here that's a mathematical 

00:48:54.007 --> 00:48:57.463
issue, which is, until recently 
it hasn't been clear how to take

00:48:57.464 --> 00:48:59.464
these maching learning models 

00:49:00.962 --> 00:49:02.962
and increase the batch size 
which is how 

00:49:04.491 --> 00:49:05.712
many of these images you're 
processing simultaneously sort 

00:49:05.713 --> 00:49:07.741
of in each step while you're 
training.  There's been a huge 

00:49:07.742 --> 00:49:09.742
amount of research energy on 
this recently and there have 

00:49:12.013 --> 00:49:14.262
been a lot of break throughs 
where we're now able to train 

00:49:14.263 --> 00:49:18.122
effectively on much larger batch
sizes from ever before.  That's 

00:49:18.123 --> 00:49:21.832
great news for scaling up with 
these TPU pods.  Let me help you

00:49:21.833 --> 00:49:23.833
interpret this graph.

00:49:25.972 --> 00:49:27.637
What you see on the horizontal 
axis here is time and what you 

00:49:27.638 --> 00:49:31.504
see on the vertical axis is 
accuracy.  And so you might 

00:49:31.505 --> 00:49:33.374
imagine your first 
implementation on your Cloud TPU

00:49:33.375 --> 00:49:35.375
thattor 

00:49:36.426 --> 00:49:38.426
renting today gets to, let's 
say, 76 

00:49:39.933 --> 00:49:40.933
percent accuracy on ResNet 50 
and that takes a certain amount 

00:49:40.934 --> 00:49:44.819
of time.  That's that orange 
curve out there.  But with no 

00:49:44.820 --> 00:49:46.820
code changes, just by changing 
the batch size in the right way 

00:49:47.868 --> 00:49:49.868
with the models that we've 
already open 

00:49:51.372 --> 00:49:53.212
sourced you can pull that time 
in as you're using larger and 

00:49:53.213 --> 00:49:56.893
larger sections of the pod until
you get down to these 30 minute 

00:49:56.894 --> 00:49:58.894
results or possibly even better.

00:50:01.610 --> 00:50:02.839
So, we think this is an 
extremely promising research 

00:50:02.840 --> 00:50:04.872
direction and we're doing our 
best in all these reference 

00:50:06.080 --> 00:50:08.052
implementations to make sure 
that it's immediately accessible

00:50:08.053 --> 00:50:12.365
to you so once these pods become
more widely available in Cloud, 

00:50:12.366 --> 00:50:13.581
then you'll be able to just 
transparently benefit from this 

00:50:13.582 --> 00:50:15.582
large batch training.

00:50:17.693 --> 00:50:19.693
I'd love to live in a world 
where you 

00:50:21.985 --> 00:50:24.234
can use an exoflop for a minute 
and then let it go while 

00:50:24.235 --> 00:50:26.235
somebody else is using it and 
swap back and forth.

00:50:29.164 --> 00:50:31.270
Now, here's another view of the 
full TPU v3 pod that Sundar 

00:50:31.271 --> 00:50:33.271
showed you this 

00:50:34.313 --> 00:50:35.743
morning and we are doing 
everything that we can to push 

00:50:35.744 --> 00:50:37.744
the boundaries of performance, 
and that's what's really 

00:50:42.503 --> 00:50:44.130
led us to liquid cool this pod 
in comparison to everything 

00:50:44.131 --> 00:50:47.427
we've done previously that's air
cooled.  If you want a cool, 

00:50:47.428 --> 00:50:50.084
close-up view, there it is.  A 
close-up view of just one of the

00:50:51.700 --> 00:50:53.327
boards in that pod, it looks 
sort of like this and you can 

00:50:53.328 --> 00:50:55.328
see the tubing.

00:50:57.661 --> 00:51:00.086
Now those plates are on each one
of these next generation TPU v3 

00:51:00.087 --> 00:51:03.764
processors.  So we're going to 
go to the largest batch sizes we

00:51:03.765 --> 00:51:05.765
can, we're  publishing 

00:51:08.286 --> 00:51:10.102
all this research that we're 
doing.  We're open sourcing as 

00:51:10.103 --> 00:51:12.955
much as it as we can on these 
Cloud TPU platforms that you can

00:51:12.956 --> 00:51:14.956
access today.

00:51:17.493 --> 00:51:19.123
And so we're really excited in 
going with you into this 

00:51:19.124 --> 00:51:21.772
exciting AI Frontier.  You can 
get started right now with 

00:51:25.420 --> 00:51:27.420
these high performance reference
models 

00:51:29.151 --> 00:51:31.067
and tutorials and please reach 
out to us.  Let us know.  We'd 

00:51:31.068 --> 00:51:32.713
be really interested in hearing 
your feedback on the Cloud TPU 

00:51:32.714 --> 00:51:36.591
platform of today.  We'd love to
know where you would like it to 

00:51:36.592 --> 00:51:38.592
go.

00:51:40.296 --> 00:51:41.728
We hope you enjoy all these 
across image generation, and 

00:51:41.729 --> 00:51:45.187
speech recognition.  Image 
translation.  And I'd be happy 

00:51:45.188 --> 00:51:47.188
to take some of your questions 
offline after the talk.

00:51:49.268 --> 00:51:50.707
So, thank you very much for 
listening.  I've included some 

00:51:50.708 --> 00:51:52.708
links here.  Thanks again.

00:51:54.388 --> 00:51:56.388
(Session was concluded at 4:34 
PM CT)

00:51:57.560 --> 00:51:59.893
***
This text, document, or file is 

00:51:57.560 --> 00:52:01.560
based on live transcription.  
Communication Access Realtime 

00:51:57.560 --> 00:52:01.693
Translation (CART), captioning, 
and/or live transcription are 

00:51:57.560 --> 00:52:00.560
provided in order to facilitate 
communication

00:51:57.560 --> 00:52:01.693
accessibility and may not be a 
totally verbatim record of the 

00:51:57.560 --> 00:52:01.293
proceedings.  This text, 
document, or file is not to be 

00:51:57.560 --> 00:52:01.693
distributed or used in any way 
that may violate copyright law.

00:51:57.560 --> 00:51:59.560
***

00:57:04.679 --> 00:57:06.679
RAW FILE

00:57:10.837 --> 00:57:12.837
GOOGLE I/O 2018

00:57:14.494 --> 00:57:16.494
MOUNTAIN VIEW, CALIFORNIA

00:57:18.644 --> 00:57:20.681
MAY 8, 2018
STAGE 8

00:57:24.155 --> 00:57:26.155
3:00 PM

00:57:34.794 --> 00:57:36.794
ANDROID VITALS:  VITALS: DEBUG 
APP 

00:57:38.239 --> 00:57:41.108
PERFORMANCE AND REAP REWARDS
TB8E1C

00:57:41.916 --> 00:57:44.582
Services Provided By:
Caption First, Inc.

00:57:41.916 --> 00:57:43.982
P.O. Box 3066
Monument, CO 80132

00:57:41.916 --> 00:57:43.982
1 877 825 5234
+001 719 481 9835

00:57:41.916 --> 00:57:43.351
www.captionfirst.com
***

00:57:43.352 --> 00:57:47.485
This text, document, or file is 
based on live transcription.  

00:57:43.352 --> 00:57:47.485
Communication Access Realtime 
Translation (CART), captioning, 

00:57:43.352 --> 00:57:47.485
and/or live transcription are 
provided in order to facilitate 

00:57:43.352 --> 00:57:46.285
communication
accessibility and may not be a 

00:57:43.352 --> 00:57:47.085
totally verbatim record of the 
proceedings.  This text, 

00:57:43.352 --> 00:57:47.485
document, or file is not to be 
distributed or used in any way 

00:57:43.352 --> 00:57:45.352
that may violate copyright law.
***

01:00:50.501 --> 01:00:52.501
GOOGLE I/O 2018

01:00:55.858 --> 01:00:57.858
STAGE 8

01:01:02.450 --> 01:01:04.450
3:00 PM

01:01:10.458 --> 01:01:12.458
LADY VITALS: DEBUG APP 
PERFORMANCE AND 

01:01:13.471 --> 01:01:15.471
REAP REWARDS

01:01:23.727 --> 01:01:25.727
M

01:01:33.387 --> 01:01:35.387
ANDROID VITALS: DEBUG APP 
PERFORMANCE AND REAP REWARDS

01:01:42.712 --> 01:01:44.712
TB8E1C

01:15:06.388 --> 01:15:10.062
&gt;&gt; Welcome.  Thank you for 
joining.  Our session will begin

01:15:10.063 --> 01:15:12.063
soon.

01:18:22.797 --> 01:18:27.753
&gt;&gt; Good afternoon, everybody.  
And welcome.  Today's session is

01:18:27.754 --> 01:18:29.754
all about app performance.

01:18:31.443 --> 01:18:33.443
My name is Joel Newman and I'm a

01:18:35.070 --> 01:18:37.871
product specialist on Google 
Play business development team. 

01:18:37.872 --> 01:18:39.496
In that capacity, I've worked 
with Android developers of all 

01:18:39.497 --> 01:18:41.788
shapes and sizes helping them to
leverage the tools 

01:18:45.194 --> 01:18:47.194
available and the

01:18:49.888 --> 01:18:53.772
Google Play Console to build 
successful apps and businesses. 

01:18:53.773 --> 01:18:55.773
Now I'm going to kick off with a
quick 

01:18:58.276 --> 01:19:00.589
reminder of how vital 
performance is.  Then my 

01:19:00.590 --> 01:19:02.433
colleague, Fergus, product 
manager for Android Vitals will 

01:19:02.434 --> 01:19:05.497
join me on stage to talk about 
some of the ways we're enhancing

01:19:05.498 --> 01:19:07.498
the platform to make it easier 
to understand and action your 

01:19:07.743 --> 01:19:09.743
performance information.

01:19:12.390 --> 01:19:14.390
Then, my colleague B

01:19:21.466 --> 01:19:24.735
Wojtek from our development team
will come on stage.  Finally, 

01:19:24.736 --> 01:19:26.736
we'll wrap things up by talking 
about some of the resources 

01:19:27.851 --> 01:19:30.098
available within the Play 
Console itself and beyond to 

01:19:30.099 --> 01:19:32.099
improve your performance, and if
we have time, we'll try to do a 

01:19:32.538 --> 01:19:34.625
brief Q&amp;A.
Now, first, I think it's 

01:19:34.626 --> 01:19:39.152
important to remind ourselves of
why performance is important.  

01:19:39.153 --> 01:19:41.153
Now, I think this is intuitive, 
right?

01:19:42.430 --> 01:19:44.430
We as users, nobody wants to 
deal with 

01:19:45.478 --> 01:19:47.478
an app that crashes frequent 
frequently 

01:19:51.066 --> 01:19:53.214
or freezes or is janky but at 
Google we have data that drives 

01:19:53.215 --> 01:19:55.892
home just how important 
performance is.  We did an 

01:19:55.893 --> 01:19:58.345
analysis of all the reviews on 
the play store over the past 

01:19:58.346 --> 01:20:03.845
year and found that when using a
one star review, over 40 percent

01:20:03.846 --> 01:20:06.300
of the time users cited negative
performance indicators like 

01:20:08.130 --> 01:20:11.024
stability and bugs.  In fact, 
this was the number one cited 

01:20:11.862 --> 01:20:14.098
issue within one star review.  
Now, on the flip side when using

01:20:14.099 --> 01:20:19.564
five star reviews, over 70 
percent of the users mentioned 

01:20:19.565 --> 01:20:22.779
positive performance indicators,
things like speed, design, and 

01:20:22.780 --> 01:20:25.615
usability.  Now, why is this 
important?

01:20:28.736 --> 01:20:29.538
Now, not only are ratings and 
reviews a direct indicator of 

01:20:29.539 --> 01:20:33.451
customer satisfaction and not 
only do they figure heavily in 

01:20:33.452 --> 01:20:35.452
the search algorithm, but 

01:20:36.721 --> 01:20:38.562
ratings and reviews correlate 
strongly with downstream 

01:20:38.563 --> 01:20:40.563
business APIs.

01:20:44.060 --> 01:20:46.060
Things like engagement, 
retention, and

01:20:49.211 --> 01:20:53.957
monetization.  A great example 
comes from this study.  We did 

01:20:53.958 --> 01:20:55.181
an analysis of all the games in 
Google Play and we compared 

01:20:55.182 --> 01:20:59.060
those with the lowest versus the
highest crash rate and we found 

01:20:59.061 --> 01:21:01.074
apps with the lower crash rate 
saw almost two and a half 

01:21:01.075 --> 01:21:03.075
minutes 

01:21:06.175 --> 01:21:08.885
more of game play per day and 
this was even a bigger trend 

01:21:08.886 --> 01:21:12.770
when we looked at A and Rs which
are more frequently looked over.

01:21:15.241 --> 01:21:17.299
We found that users spent almost
three more minutes per day in 

01:21:17.300 --> 01:21:19.300
the more stable apps.

01:21:21.225 --> 01:21:23.067
Now, this may not sound like a 
ton of time but when you look at

01:21:23.068 --> 01:21:27.531
this over a yearly basis, this 
would translate into 18 hours of

01:21:27.532 --> 01:21:29.799
additional game play per user 
for games that are played on a  

01:21:31.027 --> 01:21:33.062
taillie basis.  -- daily basis.

01:21:37.549 --> 01:21:39.549
Now, another reason why 
performance is 

01:21:42.248 --> 01:21:45.246
incredibly important is it's 
becoming an increasingly 

01:21:45.247 --> 01:21:47.247
important

01:21:48.568 --> 01:21:50.568
signal if Google Play.

01:21:52.246 --> 01:21:54.246
We're committed to only the 
highest 

01:21:55.963 --> 01:21:57.790
quality content to our users, 
taking into account performance 

01:21:57.791 --> 01:21:59.791
indicators 

01:22:01.074 --> 01:22:02.282
particularly those related to 
stability and battery 

01:22:02.283 --> 01:22:04.283
consumption in both our play 
store algorithm as well as our 

01:22:05.124 --> 01:22:08.014
algorithmically generated 
recommendations.  In addition to

01:22:08.015 --> 01:22:10.685
that, performance signals factor
heavily into our 

01:22:13.787 --> 01:22:15.626
decisions around our editorially
curated content.  This includes 

01:22:15.627 --> 01:22:17.627
our featured new and updated 
apps collection as well as 

01:22:21.127 --> 01:22:21.971
editor's choice and coveted 
Google Play awards and Android 

01:22:21.972 --> 01:22:25.481
excellence.  If you want your 
app business to succeed, your 

01:22:25.482 --> 01:22:27.482
app needs to be discovered.

01:22:30.586 --> 01:22:32.015
And technical performance is an 
increasing part of that 

01:22:32.016 --> 01:22:34.016
discovery within the play store.

01:22:38.359 --> 01:22:40.359
Now, it's

01:22:42.717 --> 01:22:44.555
because of that increasing 
importance of performance that 

01:22:44.556 --> 01:22:46.224
we launched Android Vitals.  
This is an initiative by Google 

01:22:46.225 --> 01:22:48.516
to improve the stability and 
performance of 

01:22:52.789 --> 01:22:54.832
Android devices by circulating 
to developers in an easily 

01:22:54.833 --> 01:22:56.833
understandable 

01:22:59.538 --> 01:23:00.989
and readily accessible way the 
most important performance 

01:23:00.990 --> 01:23:04.968
metrics in battery, stability, 
and rendering.  This comes from 

01:23:04.969 --> 01:23:07.049
over 100 million users who have 
opted into to share the data 

01:23:08.717 --> 01:23:10.717
with Google and with you, our 
developer partners.

01:23:11.161 --> 01:23:13.653
And since the program's launch a
year ago, it's been an 

01:23:13.654 --> 01:23:15.654
incredible success.

01:23:17.328 --> 01:23:20.203
We have over 100,000 developers 
including some of the biggest 

01:23:20.204 --> 01:23:22.204
names on Android platform 
regularly engaging with 

01:23:25.096 --> 01:23:27.096
the Vitals Console to understand
their 

01:23:30.050 --> 01:23:32.097
performance issue and create 
fixes.  Now it's because of this

01:23:32.098 --> 01:23:34.959
incredible engagement that we've
seen some really nice games at 

01:23:34.960 --> 01:23:36.960
an ecosystem.

01:23:38.659 --> 01:23:41.091
When we compare reviews in the 
Play Store today versus a year 

01:23:41.092 --> 01:23:44.825
ago when we launched Vitals we 
see a really nice uptick in 

01:23:44.826 --> 01:23:47.475
those people leaving reviews 
talking about things like speed,

01:23:47.476 --> 01:23:50.331
design, and usability.  But 
perhaps more importantly, when 

01:23:50.332 --> 01:23:54.618
we look at people using one star
reviews, we've seen almost a 20 

01:23:54.619 --> 01:23:56.619
percent decrease in the number 
of people talking about 

01:23:59.329 --> 01:24:01.609
things like stability and bugs 
and battery consumption and 

01:24:01.610 --> 01:24:04.679
that's only possible because of 
the incredible work that you 

01:24:04.680 --> 01:24:06.305
have done as a developer 
community to up the quality of 

01:24:06.306 --> 01:24:08.333
Android apps.
And for those of you who are new

01:24:08.334 --> 01:24:11.816
to the platform, it's really 
important that you keep pace 

01:24:11.817 --> 01:24:13.817
with the rest of the developer 

01:24:15.536 --> 01:24:18.414
community who is helping us move
in a positive performance 

01:24:18.415 --> 01:24:22.279
direction.  Now, I think these 
gains at an ecosystem level are 

01:24:22.280 --> 01:24:25.767
interesting, you might wonder as
a developer, yeah, but how does 

01:24:25.768 --> 01:24:27.391
that really matter to me some 
well I want to show case a 

01:24:27.392 --> 01:24:29.425
couple of examples of where 
individual developers 

01:24:34.089 --> 01:24:36.089
have seen incredible gains 
because of Android Vitals.

01:24:37.144 --> 01:24:39.144
One of my favorite case studies 
is Starbucks.

01:24:40.056 --> 01:24:44.100
Now, Starbucks is obviously an 
incredibly popular app.  They 

01:24:44.101 --> 01:24:48.217
have over 15 million downloads 
on the Google Play Store.  They 

01:24:48.218 --> 01:24:50.043
started using Android Vitals in 
December, admittedly several 

01:24:50.044 --> 01:24:53.927
months after the launch of the 
program.  When they started 

01:24:53.928 --> 01:24:55.928
using Android Vitals they were 
alerted to A and R issues that 

01:24:58.415 --> 01:25:00.415
were not even on their radar and
they 

01:25:02.087 --> 01:25:04.792
were able to use the reporting 
in the Console to identify 

01:25:04.793 --> 01:25:09.917
underlying causes of ANRs caused
by a third party library.  They 

01:25:09.918 --> 01:25:11.918
removed that third party library

01:25:12.984 --> 01:25:15.636
and saw a 70 percent reduction 
in ANR within a week.  They saw 

01:25:15.637 --> 01:25:18.685
a similar and actually better 
experience with crash rate.  

01:25:18.686 --> 01:25:20.521
Because Android Vitals is a 
platform tool it's able to 

01:25:20.522 --> 01:25:22.522
detect crashes that 

01:25:26.667 --> 01:25:31.582
were not detected by Starbucks' 
third party crash STK.  Because 

01:25:31.583 --> 01:25:33.583
they were happening before 

01:25:35.071 --> 01:25:37.511
Starbucks' third party crash STK
had even initiated.  They war 

01:25:37.512 --> 01:25:39.949
able to identify underlying 
cause, push a fix and saw an 85 

01:25:39.950 --> 01:25:43.033
percent reduction in crash rate 
within a week.  These are 

01:25:43.034 --> 01:25:45.059
impressive stats.
Now, it's worth calling out that

01:25:45.060 --> 01:25:49.166
Vitals, and this is a common 
misconception, is not just for 

01:25:49.167 --> 01:25:51.167
apps.  It's equally relevant for
games.

01:25:54.682 --> 01:25:57.117
A great example comes from 
Kaloo, developers of the super 

01:25:57.118 --> 01:25:59.561
popular subway surfers app which
was actually the first 

01:26:03.448 --> 01:26:05.448
game to pass the 1 billion 
install mark on Google Play.

01:26:07.954 --> 01:26:09.783
Which subway surfers started 
using Android Vitals, they 

01:26:09.784 --> 01:26:13.700
discovered a 70 percent increase
in ANR rate, used the reports to

01:26:13.701 --> 01:26:15.701
identify the cause, pushed a 

01:26:16.981 --> 01:26:21.262
fix and saw a reduction of their
ANR rate from 2.5 to below .1 

01:26:21.263 --> 01:26:23.263
percent, a 95 percent reduction.

01:26:25.130 --> 01:26:26.353
But I think what's worth calling
out here is what we found 

01:26:26.354 --> 01:26:28.354
afterwards.

01:26:29.396 --> 01:26:31.637
So, we did an analysis to Google
Play, what was the change in 

01:26:31.638 --> 01:26:33.638
engagement of the app before and
after the fix.

01:26:36.546 --> 01:26:38.178
We saw really subsequent gains 
in both daily app users and 

01:26:38.179 --> 01:26:40.179
subsequent users across the 
board.

01:26:43.396 --> 01:26:44.048
Now, these are just two examples
of developers who have been able

01:26:44.049 --> 01:26:47.119
to leverage the platform to see 
really significant gains both at

01:26:47.120 --> 01:26:49.120
a performance and business 
level.

01:26:52.645 --> 01:26:54.086
Now, I'm going to invite up 
Fergus, product manager for 

01:26:54.087 --> 01:26:56.087
Vitals who is going to talk 
about some of the ways we're 

01:26:57.379 --> 01:26:58.407
enhancing the platform to make 
it even more powerful for 

01:26:58.408 --> 01:27:00.408
developers.
(applause)

01:27:01.084 --> 01:27:03.084
&gt;&gt; FERGUS HURLEY: Great job.

01:27:07.234 --> 01:27:09.234
Hey, everyone.

01:27:11.940 --> 01:27:13.809
I'm Fergus Hurley, product 
manager for Android Vitals.  

01:27:13.810 --> 01:27:15.810
It's been incredible to see 

01:27:16.887 --> 01:27:18.550
improvements that app developers
like yourself have made and the 

01:27:18.551 --> 01:27:22.637
results.  It's also been 
incredible to get all the 

01:27:22.638 --> 01:27:25.290
feedback from you as to how we 
could improve Android Vitals.  

01:27:25.291 --> 01:27:27.157
I'm really excited to show with 
you the new features we have 

01:27:27.158 --> 01:27:29.615
available today as a result of 
your feedback, so thank you so 

01:27:29.616 --> 01:27:31.616
much.

01:27:34.120 --> 01:27:36.164
Last year, when we announced 
Android Vitals as Joel 

01:27:36.165 --> 01:27:38.218
mentioned, we launched three 
performance areas.  Battery, 

01:27:38.219 --> 01:27:40.057
stability, and rendering.  
Today, we're announcing two new 

01:27:40.058 --> 01:27:42.943
areas.  Start-up time and 
permissions.

01:27:48.104 --> 01:27:50.751
Battery is probably the most 
important performance area to 

01:27:50.752 --> 01:27:54.853
focus on.  This is where you 
should not unnecessarily use its

01:27:54.854 --> 01:27:56.854
CPU or radios on 

01:27:57.939 --> 01:28:00.375
the device which drains user's 
batteries which means they can't

01:28:00.376 --> 01:28:03.880
use your app as long.  Stability
is all about building a robust 

01:28:03.881 --> 01:28:05.497
and reliable application so that
users are using your application

01:28:05.498 --> 01:28:07.498
without 

01:28:10.331 --> 01:28:14.759
freezes or crashes and rendering
is all about building a fast and

01:28:14.760 --> 01:28:16.760
responsive application that's 

01:28:18.246 --> 01:28:20.246
silky smooth and doesn't lag or 
stall on users.

01:28:21.320 --> 01:28:23.769
With start-up time, we're 
enabling you to be able to 

01:28:23.770 --> 01:28:25.217
provide a quick launching 
application to users no matter 

01:28:25.218 --> 01:28:27.865
what state your app has been in.
There's three different metrics 

01:28:27.866 --> 01:28:29.866
we have here and I'm going to go
through each one.

01:28:32.579 --> 01:28:34.579
So, first, we have slow cold 
start.

01:28:36.300 --> 01:28:38.955
Slow here is defined as five 
seconds or more and cold start 

01:28:38.956 --> 01:28:43.034
is when your app has not been 
launched in a while and is going

01:28:43.035 --> 01:28:44.907
from actively launched to 
heavily running and the app is 

01:28:44.908 --> 01:28:46.908
not in memory.

01:28:49.438 --> 01:28:51.438
Slow warm start is where it 
takes two 

01:28:53.112 --> 01:28:54.525
seconds or more for your app to 
go in actively launch to 

01:28:54.526 --> 01:28:56.526
actively running where where you
are app is in memory.

01:29:02.924 --> 01:29:04.924
Final start time metric we have 
is slow hot start.

01:29:07.035 --> 01:29:09.035
Where your app goes from 
unrestart to 

01:29:10.095 --> 01:29:11.942
activity running and takes 
longer than 1.5 seconds where 

01:29:11.943 --> 01:29:13.943
your app and activity is in 
memory.

01:29:16.638 --> 01:29:18.460
Another new performance area we 
have is permissions.  This is 

01:29:18.461 --> 01:29:20.705
where we want to help you be 
able to only request permissions

01:29:20.706 --> 01:29:25.597
that users think are required 
for the core value of your 

01:29:25.598 --> 01:29:27.598
application to be provided 

01:29:29.106 --> 01:29:31.106
to them and if they don't think 
so then 

01:29:32.588 --> 01:29:33.605
provide them justification why 
you think they should actually 

01:29:33.606 --> 01:29:36.647
provide that information.  When 
we ask users, what are the 

01:29:38.792 --> 01:29:40.621
reasons that you deny 
permissions for applications, 

01:29:40.622 --> 01:29:42.622
what we find is that the vast 
majority of them just think that

01:29:43.892 --> 01:29:45.353
the app doesn't need those 
permissions so really providing 

01:29:45.354 --> 01:29:47.354
a justification is 

01:29:49.863 --> 01:29:52.534
critical for you to get users to
be able to give you the 

01:29:52.535 --> 01:29:55.037
permission you need to be able 
to offer the best functionality 

01:29:55.854 --> 01:29:57.854
possible in your application.

01:30:00.403 --> 01:30:02.034
In the details view for 
permissions we provide a 

01:30:02.035 --> 01:30:04.084
breakdown by permission group so
maybe one of your permissions 

01:30:04.085 --> 01:30:07.610
users think your app does need 
and you get a high percentage of

01:30:07.611 --> 01:30:10.878
acceptance for that one and then
another one, users don't think 

01:30:10.879 --> 01:30:12.917
you need and you get a high 
denial rate for that one.  And 

01:30:12.918 --> 01:30:14.352
maybe you don't need that 
permission in the first place, 

01:30:14.353 --> 01:30:17.609
so maybe you should rethink it.
Or maybe you just need to 

01:30:17.610 --> 01:30:20.274
explain to users why your app 
does need that permission.

01:30:23.776 --> 01:30:25.776
We have many other breakdowns 
across 

01:30:26.797 --> 01:30:28.797
the

01:30:30.718 --> 01:30:32.718
Vitals product.  For most, we 
break it down.

01:30:35.690 --> 01:30:37.690
For wake locks and  wake-ups, we

01:30:39.004 --> 01:30:41.004
provide by tag, ANR, ANR type 
and ANR name.

01:30:42.686 --> 01:30:44.515
We also provide the clusters 
view for grouping your ANRs 

01:30:44.516 --> 01:30:46.545
together so you can be able to 
see which ones are similar 

01:30:49.644 --> 01:30:51.644
and be able to debug those 
clusters.

01:30:55.520 --> 01:30:57.677
For crashes, we also provide 
clusters.

01:31:00.789 --> 01:31:02.789
When I've been building apps at 
Google 

01:31:03.889 --> 01:31:05.538
and prior to Google, one of the 
things I struggled with most is 

01:31:05.539 --> 01:31:08.588
being able to understand, how do
I compare?  Who also struggles 

01:31:08.589 --> 01:31:11.451
with understanding if their app 
is performing well compared to 

01:31:11.452 --> 01:31:13.452
other apps?  Okay.

01:31:17.335 --> 01:31:19.335
Lots of people share the same 
struggle

01:31:20.932 --> 01:31:22.986
.  We are now going to announce 
benchmarks in the play counsel.

01:31:26.446 --> 01:31:28.446
This enables you to be able to 
see 

01:31:29.469 --> 01:31:32.172
that for start time you can see 
board times are much faster.

01:31:35.449 --> 01:31:37.900
We also provide the 25th, 50th, 
and 75th percentile breakdown 

01:31:37.901 --> 01:31:40.986
for each of the category level 
benchmarks.  We'll default to 

01:31:40.987 --> 01:31:42.987
the category you list yourself 
in the floor but you can select 

01:31:43.242 --> 01:31:45.279
any other category that you want
to be able to compare yourself 

01:31:45.280 --> 01:31:47.280
with in case 

01:31:48.585 --> 01:31:50.585
you don't think your app is as 
relative to that category.

01:31:52.886 --> 01:31:54.886
On the details page for every 
vital, we 

01:31:56.180 --> 01:31:57.799
show you this breakdown for the 
category benchmark and then you 

01:31:57.800 --> 01:31:59.800
can be able to 

01:32:01.280 --> 01:32:03.280
also see what exact you are 
relative to 

01:32:05.058 --> 01:32:07.058
other apps in that category.

01:32:12.107 --> 01:32:14.107
On the redesigned Android vials

01:32:16.297 --> 01:32:18.730
page we show you each metric, 
what your percent of use 

01:32:18.731 --> 01:32:20.789
impacted in the last 30 days, 
what was the percent impacted in

01:32:23.640 --> 01:32:24.888
the previous 30 days, and we 
show you the benchmark 

01:32:24.889 --> 01:32:26.889
comparison.

01:32:27.958 --> 01:32:29.417
This enables you to quickly see,
which one of these performance 

01:32:29.418 --> 01:32:31.418
areas should I 

01:32:32.908 --> 01:32:33.314
look into and what specific 
metric might require more 

01:32:33.315 --> 01:32:36.962
attention.  You can see here on 
the list, if you're on the 12 

01:32:36.963 --> 01:32:38.963
percent, that's probably one you
want to investigate.

01:32:40.040 --> 01:32:42.040
The weather channel has been 
early 

01:32:43.522 --> 01:32:44.715
access partner for these 
features that we're showing 

01:32:44.716 --> 01:32:47.630
today for the past couple of 
weeks and one of the pieces is 

01:32:47.631 --> 01:32:49.631
it's 

01:32:51.097 --> 01:32:52.120
been really great for them to be
able to use category-level 

01:32:52.121 --> 01:32:54.154
benchmarks to make the case 
within their company as to why 

01:32:55.371 --> 01:32:57.371
they should invest in 
performance 

01:32:59.043 --> 01:33:02.323
because they can show how they 
stack up versus competing apps. 

01:33:02.324 --> 01:33:03.735
I hope you're able to use 
similar data to be able to make 

01:33:03.736 --> 01:33:05.736
the case within your company.

01:33:09.214 --> 01:33:11.214
If you want to learn more about

01:33:13.567 --> 01:33:17.465
benchmarks, we have a session 
tomorrow at 9:30 AM.  Although 

01:33:17.466 --> 01:33:19.908
all Android Vitals should be 
viewed as ways to be able to 

01:33:19.909 --> 01:33:21.909
enhance the 

01:33:23.159 --> 01:33:24.995
experience of your application, 
some Vitals are more important 

01:33:24.996 --> 01:33:26.996
to offering the best user 
experience possible.

01:33:30.920 --> 01:33:33.184
So, that's why we're putting 
this feature at the top of the 

01:33:33.185 --> 01:33:36.048
Android Vitals overview page 
which is core Vitals.  These are

01:33:36.049 --> 01:33:38.049
the Vitals that you should pay 
most attention to.

01:33:40.327 --> 01:33:41.948
These are the ones that will 
impact your promotability and 

01:33:41.949 --> 01:33:45.686
ranking in the store along with 
many other signals but we should

01:33:45.687 --> 01:33:47.687
really pay attention to these 
signals.

01:33:55.741 --> 01:33:58.033
You'll see the list of the 
Vitals.  Here we have initial 

01:33:58.034 --> 01:34:02.941
four, excessive wake-ups, where 
you have ten wake-ups happening 

01:34:02.942 --> 01:34:04.942
per hour, ANR rates, where you 

01:34:06.795 --> 01:34:06.810
have a user experience the app 
not responding for five seconds 

01:34:06.811 --> 01:34:11.332
or more in the day, and a crash 
rate, where they have one crash 

01:34:11.333 --> 01:34:13.333
in a day.

01:34:14.791 --> 01:34:16.791
If you're in the bottom 25 
percent for 

01:34:18.466 --> 01:34:19.885
any of these bad, core Vitals, 
then you'll be flagged as having

01:34:19.886 --> 01:34:21.886
a bad behavior.

01:34:25.123 --> 01:34:27.123
This is where you'll see

01:34:32.783 --> 01:34:34.455
the vital and when you click on 
the vital you'll be able to go 

01:34:34.456 --> 01:34:38.088
into the view and see which STK 
is exhibiting that information 

01:34:38.089 --> 01:34:40.526
and we provided you that 
information to help you debug 

01:34:40.527 --> 01:34:42.769
that issue.
One other major request we've 

01:34:42.770 --> 01:34:47.294
had over the past year is to get
alerted when there is an issue 

01:34:47.295 --> 01:34:49.295
that's IPed with your Vitals so 
we're really happy to announce 

01:34:50.200 --> 01:34:54.923
anomaly detection.  So, this is 
where you'll get flagged with an

01:34:54.924 --> 01:34:56.924
alert at the top of your Android

01:34:59.154 --> 01:35:01.154
Vitals section,

01:35:02.712 --> 01:35:04.762
and crash clusters to let you 
know what is the issue you have,

01:35:04.763 --> 01:35:07.601
you can click on that tile to be
able to go in and see the 

01:35:07.602 --> 01:35:11.466
details.  You can also be able 
to sign up to get email alerts 

01:35:11.467 --> 01:35:13.467
when one of these anomalies is 
detected and when you get one of

01:35:15.166 --> 01:35:18.004
those emails, there's a link 
directly to the issue so you can

01:35:18.005 --> 01:35:20.051
be able to investigate, okay, 
when did this issue 

01:35:25.328 --> 01:35:27.328
occur and for how long did it 
happen

01:35:31.144 --> 01:35:33.198
.  The Big Fish games team that 
have built many games have found

01:35:33.199 --> 01:35:35.199
great 

01:35:36.688 --> 01:35:38.559
results with using these tools 
over the past couple of months 

01:35:38.560 --> 01:35:41.043
and have reduced their crash 
rate by 45 percent for one 

01:35:45.635 --> 01:35:47.635
of their top games

01:36:03.065 --> 01:36:03.209
.  With that, I will introduce 
Wojtek.

01:36:03.210 --> 01:36:06.266
&gt;&gt; WOJTEK KALICINSKI: Hi, 
everyone.  My name is Wojtek.  

01:36:06.267 --> 01:36:09.140
Today, I want to talk about how 
you can fix some of these core 

01:36:09.141 --> 01:36:12.490
Vitals that Fergus mentioned in 
your app, in your code.  First, 

01:36:12.491 --> 01:36:14.530
I want to start with the 
stability.  Let's talk a little 

01:36:14.531 --> 01:36:19.463
bit about ANRs.  So, we keep 
saying this acronym, right, 

01:36:19.464 --> 01:36:22.110
ANRs, but what are they, really?
So ANRs, in other words, 

01:36:22.111 --> 01:36:24.111
application not responding 
events.

01:36:27.205 --> 01:36:29.037
Now, this happens whenever in 
your app some long running 

01:36:29.038 --> 01:36:31.279
operation locks your main 
thread.  Now, the main thread is

01:36:31.280 --> 01:36:33.280
responsible for things like 
responding to user 

01:36:37.055 --> 01:36:39.055
events such as taps or redrawing
the UI 

01:36:41.586 --> 01:36:44.061
for every frame so if we drop 
it, if the main thread cannot do

01:36:44.062 --> 01:36:47.114
that in a timely manner, the app
will just seem frozen and then 

01:36:47.115 --> 01:36:49.549
after a few seconds the system 
will notice that and will pop up

01:36:49.550 --> 01:36:52.186
this annoying dialogue that lets
the user actually close the app.

01:36:52.187 --> 01:36:58.085
As developers, we really don't 
want that to happen.  So, what 

01:36:58.086 --> 01:37:00.102
are some common causes of ANRs? 
Like, what kind of operations 

01:37:00.103 --> 01:37:02.103
can block our main thread?

01:37:06.148 --> 01:37:08.897
Well, probably the most common 
ones are network and disk 

01:37:08.898 --> 01:37:12.590
operations, also called I/O.  
Networks are congested, remote 

01:37:12.591 --> 01:37:17.906
servers are slow.  So can be 
your flash drives or the file 

01:37:17.907 --> 01:37:20.781
system can be busy with other 
operations so as developers, we 

01:37:20.782 --> 01:37:22.782
should 

01:37:25.505 --> 01:37:27.505
never, ever do disk operations 
on the main thread.

01:37:28.829 --> 01:37:31.282
But, you know, we just prepare 
code examples that it's not 

01:37:31.283 --> 01:37:33.283
always easy to do the right 
thing.

01:37:36.885 --> 01:37:38.330
Even though you all probably 
agree we can still make 

01:37:38.331 --> 01:37:42.465
mistakes.  So here, for example,
even though my activity, I 

01:37:42.466 --> 01:37:45.325
decided, I'm just going to 
prepare this to use later.  I'm 

01:37:45.326 --> 01:37:47.786
just going to create and not do 
anything with it right now.  And

01:37:47.787 --> 01:37:49.452
then somewhere else in my app 
probably on a background thread 

01:37:49.453 --> 01:37:51.453
I will 

01:37:53.127 --> 01:37:54.743
actually get some information 
like a string or whatever.  And 

01:37:54.744 --> 01:37:57.511
I thought, okay, that's good.
I will not be doing disk 

01:37:57.512 --> 01:37:59.967
operations on a main thread.  
What I didn't realize, though, 

01:37:59.968 --> 01:38:05.287
is that the implementation of 
preferences on Android actually 

01:38:05.288 --> 01:38:08.935
eagerly reads data from this the
moment you create them.  So, 

01:38:08.936 --> 01:38:10.936
that's actually the line of code

01:38:13.637 --> 01:38:15.637
that does disk access so I was 
wrong.  It's not always obvious.

01:38:16.723 --> 01:38:19.011
And let me show you another even
trickier example.

01:38:22.260 --> 01:38:23.065
So, here, I'm doing some 
hypothetical network operations 

01:38:23.066 --> 01:38:25.066
in my app.

01:38:27.176 --> 01:38:30.262
So, let's say I want to check if
my content has URL.  And then I 

01:38:30.263 --> 01:38:33.821
say I want to open connection to
that URL.  I'm going to save the

01:38:33.822 --> 01:38:35.822
headers to some 

01:38:38.271 --> 01:38:40.271
variable for later use

01:38:52.910 --> 01:38:54.548
.  So, how do I -- creates the 
connection and actually blocks 

01:38:54.549 --> 01:38:58.454
my thread, or is it the last one
where I actually request to read

01:38:58.455 --> 01:39:02.323
bytes of data to actually read 
them into my application?  Or is

01:39:02.324 --> 01:39:04.324
it something else?

01:39:05.837 --> 01:39:07.451
Well, it's not obvious.  
Actually, in this example, this 

01:39:07.452 --> 01:39:09.452
line 

01:39:11.391 --> 01:39:13.264
of code will perform a network 
operation.  Like the first time 

01:39:13.265 --> 01:39:16.745
I'm trying to access headers, 
only then will the connection be

01:39:16.746 --> 01:39:19.815
open and the first bytes be 
read.  So, as a developer, 

01:39:19.816 --> 01:39:23.904
again, I failed.  I didn't know 
which operation does the network

01:39:23.905 --> 01:39:25.905
access.

01:39:28.422 --> 01:39:30.053
But fun fact, on an even older 
Android version this line, just 

01:39:30.054 --> 01:39:32.309
creating, just checking if two 
URLs are equal can 

01:39:36.796 --> 01:39:39.725
perform network operations.
Okay. So why am I showing you 

01:39:39.726 --> 01:39:43.879
all this?  Because I want to 
since you that just as 

01:39:43.880 --> 01:39:45.880
developers, we cannot know 
everything.

01:39:46.955 --> 01:39:52.876
You will use libraries that 
you're not familiar with or 

01:39:52.877 --> 01:39:55.542
other APIs.  That's why I 
suggest you use trick mode.

01:40:00.271 --> 01:40:04.166
Now, strict mode existed on 
Android for a very long time.  

01:40:04.167 --> 01:40:06.822
It's pretty easy to set up.  You
just tell strict mode, I want 

01:40:06.823 --> 01:40:11.117
you to detect all disk 
operations in my application on 

01:40:11.118 --> 01:40:13.155
this thread and I also want you 
to detect network operations, 

01:40:15.214 --> 01:40:16.069
and then you tell strict mode 
what you want the result to be.

01:40:20.712 --> 01:40:22.712
So, first of all,

01:40:25.304 --> 01:40:27.605
of course we want to see it in 
logs so we can debug later but 

01:40:27.606 --> 01:40:29.606
logs can be easy to miss.

01:40:31.885 --> 01:40:34.373
So for my debug, I'm actually 
going to say penalty death which

01:40:34.374 --> 01:40:37.619
will actually crash my app 
whenever I do the wrong thing.  

01:40:37.620 --> 01:40:39.620
Now, like I said, you don't want
to do 

01:40:40.892 --> 01:40:42.746
any of that in your production 
release builds because you don't

01:40:42.747 --> 01:40:44.747
want your useer's app to crash.

01:40:47.753 --> 01:40:50.596
So I go back, run my app that I 
just showed you, look at my log,

01:40:50.597 --> 01:40:53.633
because my app crashed.  And I 
see the exact cause and I can go

01:40:58.134 --> 01:40:59.539
and debug that so I can see 
there was a strict mode disk 

01:40:59.540 --> 01:41:03.480
read error.  I can see it's the 
default preferences that I did 

01:41:03.481 --> 01:41:05.531
on my activity create.  So, 
great, this lets me actually go 

01:41:06.144 --> 01:41:08.144
and fix that.  Okay.

01:41:09.609 --> 01:41:10.830
So, we covered network and disk 
operations and how to catch 

01:41:10.831 --> 01:41:14.716
them.  But, you know, sometimes 
you don't even need to hit 

01:41:14.717 --> 01:41:18.013
remote servers or disk to cause 
a long running operation.  All 

01:41:18.014 --> 01:41:20.014
it takes is literally just 

01:41:21.493 --> 01:41:23.493
calculating something for for a 
long 

01:41:24.527 --> 01:41:27.190
time time, doing a very long 
loop and something like that.  

01:41:27.191 --> 01:41:29.191
So, here in this example, this 
is a 

01:41:30.472 --> 01:41:32.472
sample sudoku game and how it 
works is 

01:41:33.596 --> 01:41:35.036
when the use r opens the app, 
the Sudoku board is actually 

01:41:35.037 --> 01:41:38.297
generated from memory.  It's not
read from disk.  It's not pulled

01:41:38.298 --> 01:41:41.589
down from a server somewhere.  
There's actually a pretty long 

01:41:41.590 --> 01:41:44.849
complex calculation happening to
show that first board to the 

01:41:44.850 --> 01:41:47.543
users so they can play it.  So, 
it's just one code, generate 

01:41:47.544 --> 01:41:51.633
board.  And as you can see on a 
slow device, this could cause an

01:41:51.634 --> 01:41:54.077
ANR because I'm doing that on 
the main thread.  So, first of 

01:41:54.078 --> 01:41:56.957
all, how am I supposed to know 
as a developer that this call 

01:41:58.016 --> 01:42:00.480
generate board can be long.  
Well, I really suggest that you 

01:42:00.481 --> 01:42:03.763
use Android profiler regularly. 
If you ever suspect there is any

01:42:04.382 --> 01:42:07.028
performance problem in your app.
So, here, for example, I took a 

01:42:07.029 --> 01:42:09.880
CPU trace of the method generate
board and 

01:42:12.893 --> 01:42:16.882
the profiler and I can 
immediately see that it's a 

01:42:16.883 --> 01:42:18.100
complex recursive call that 
takes a long, long time to 

01:42:18.101 --> 01:42:21.550
finish.  So, now I know I should
not be running it on the main 

01:42:21.551 --> 01:42:25.037
thread.  Oh, and by the way, 
there's a new thing in the 

01:42:25.038 --> 01:42:27.896
Android profiler that we just 
released in the cannery today, 

01:42:27.897 --> 01:42:32.424
now you can debug and profile 
your app at start-up time so 

01:42:32.425 --> 01:42:35.069
that's pretty handy.
Okay. So going back to our 

01:42:35.070 --> 01:42:42.263
problem with our generate board 
call, what can I do with it.  

01:42:42.264 --> 01:42:44.264
So, strict mode has this method 
of 

01:42:48.593 --> 01:42:50.593
telling it that a call can be 
slow

01:42:59.569 --> 01:43:01.569
.  And now, whoever else uses 
code and 

01:43:03.056 --> 01:43:05.300
uses strict mode, if they also 
tag the custom slow call thread 

01:43:05.301 --> 01:43:07.950
policy, again, they will get 
notified.  Their app will clash,

01:43:07.951 --> 01:43:11.867
they will get the log and they 
will know to never call that 

01:43:11.868 --> 01:43:13.868
generate board method on the 
main thread.

01:43:14.556 --> 01:43:17.619
All right, so what are some 
other causes of ANRs?  Be 

01:43:17.620 --> 01:43:18.835
careful if you're doing any kind
of interprocess communication 

01:43:18.836 --> 01:43:21.925
and expect a result.  Don't do 
it synchronously on the main 

01:43:22.116 --> 01:43:25.618
thread.  If you're calling some 
other application, if that you 

01:43:25.619 --> 01:43:29.127
will is leading your process, 
you basically don't control what

01:43:29.128 --> 01:43:31.128
happens on the other side.

01:43:32.211 --> 01:43:34.211
I p, C goes through layers of 
the 

01:43:35.907 --> 01:43:37.546
operating system, the other 
process can access network and 

01:43:37.547 --> 01:43:39.547
disk to service your 

01:43:40.562 --> 01:43:42.568
request so just remember to do 
it on the background thread.

01:43:45.803 --> 01:43:46.629
The next thing is multithreader 
programming and that's a really,

01:43:46.630 --> 01:43:49.070
really difficult  topic.  Like, 
we could talk, have a whole 

01:43:49.696 --> 01:43:51.696
other session about this.  But 
if you ever find yourselves 

01:43:55.439 --> 01:43:57.878
touching, you know, low level 
primitives such as log 

01:43:57.879 --> 01:43:59.879
synchronization, eventually, 

01:44:01.956 --> 01:44:03.608
there's a high chance you'll 
create' deadlock in your 

01:44:03.609 --> 01:44:05.609
application or hit some other 
problem that blocks your main 

01:44:07.877 --> 01:44:10.546
thread and these are very tricky
to debug.  However, Android 

01:44:10.547 --> 01:44:12.547
Vitals does give you some 
information that will help you 

01:44:13.210 --> 01:44:15.210
debug these kinds of problems.

01:44:18.163 --> 01:44:19.789
So, if you open any ANR in the 
Vitals Console, we actually 

01:44:19.790 --> 01:44:22.069
provide you with the trace files
pulled from the devices 

01:44:25.339 --> 01:44:27.377
that suffered from these ANRs so
you can look at thread states 

01:44:27.378 --> 01:44:30.223
and see if they were blocked 
waiting on any resource to 

01:44:31.872 --> 01:44:33.084
become available and we even 
highlight the problems that we 

01:44:33.085 --> 01:44:39.191
think might be happening in this
trace.  Okay. And then the last 

01:44:39.192 --> 01:44:43.271
thing I want to mention as a 
cause of ANRs is slow broadcast 

01:44:43.272 --> 01:44:47.938
receiver handling.  Now, not 
every developer realizes that if

01:44:47.939 --> 01:44:50.421
you set a broadcast receiver in 
your Android manifest and then 

01:44:50.422 --> 01:44:53.886
it gets cold that the on receive
method actually happens in the 

01:44:53.887 --> 01:44:55.534
main thread and you're not 
supposed to perform any long 

01:44:55.535 --> 01:44:58.834
running operations in it and 
there's a timeout, about ten 

01:44:58.835 --> 01:45:00.892
seconds that the system will 
allow this method to run and 

01:45:00.893 --> 01:45:02.893
then it will kill your process.

01:45:07.495 --> 01:45:09.495
So sometimes, though, whenever 
you are 

01:45:11.765 --> 01:45:13.797
using a broadcast receiver maybe
to show a notification to the 

01:45:13.798 --> 01:45:17.328
user, maybe you still need a 
little piece of data maybe from 

01:45:17.329 --> 01:45:19.358
the disk, like reading some name
or profile picture to show a 

01:45:19.359 --> 01:45:21.822
notification.  So, you actually 
need to do that disk access.

01:45:25.110 --> 01:45:27.110
So, what can you do in this 
case?

01:45:28.165 --> 01:45:31.040
Now, broadcast receivers have 
this method called go asynch.

01:45:34.354 --> 01:45:35.998
Basically a way to tell the 
broadcast receiver from the -- 

01:45:35.999 --> 01:45:40.702
that you need to do some longer 
running operation and then you 

01:45:40.703 --> 01:45:42.325
spin up a separate thread 
yourself.  You do the operation 

01:45:42.326 --> 01:45:45.776
and then remember to code finish
when you're done.  If you don't 

01:45:45.777 --> 01:45:47.817
do that, the system will still 
realize that you're blocking and

01:45:49.989 --> 01:45:52.240
might potentially kill your app.
And remember, don't treat this 

01:45:52.241 --> 01:45:55.709
as some kind of long running 
service.  This is not the case 

01:45:55.710 --> 01:45:59.821
for broadcast receivers and the 
reasonable time limit still 

01:45:59.822 --> 01:46:01.873
applies so you should literally 
just grab the things you need 

01:46:01.874 --> 01:46:05.171
and the separate thread and 
finish as soon as possible.  

01:46:05.172 --> 01:46:07.172
Okay. Let's talk a little bit 
about caches of 

01:46:12.103 --> 01:46:14.103
and you know, it's really 
difficult to

01:46:15.448 --> 01:46:17.887
give general advice on how to 
avoid crashes in a setting like 

01:46:17.888 --> 01:46:19.927
this because every app will be 
different, every crash will be 

01:46:19.928 --> 01:46:24.244
different.  However, I think I 
can give you some advice that 

01:46:24.245 --> 01:46:26.245
might be helpful.

01:46:27.331 --> 01:46:29.173
So, for years as an Android 
developer, I've seen a code or 

01:46:29.174 --> 01:46:31.411
my code from other people start 
very simple.  And we release 

01:46:31.412 --> 01:46:33.412
something, and then 

01:46:35.126 --> 01:46:37.359
things start crashing on user's 
devices so we add all these 

01:46:37.360 --> 01:46:39.360
safety measures like checking 
for nulls everywhere and 

01:46:41.025 --> 01:46:43.068
checking for activity life 
cycles and then if that fails we

01:46:43.069 --> 01:46:47.162
just wrap everything in a 
tricatch and just hope that this

01:46:47.163 --> 01:46:49.427
problem doesn't hit too many 
users.  But, that's not the 

01:46:49.428 --> 01:46:55.341
greatest solution, right?  It 
doesn't actually solve the 

01:46:55.342 --> 01:46:57.342
underlying problem.

01:46:59.821 --> 01:47:01.821
That's why my first piece of 
advice to 

01:47:03.937 --> 01:47:05.599
you is to use readily available 
libraries that solve problems 

01:47:05.600 --> 01:47:07.600
that you as developers face.

01:47:09.507 --> 01:47:11.507
Now, some of the most difficult 
parts 

01:47:14.242 --> 01:47:16.480
of Android, we heard about it 
from you the developers is 

01:47:16.481 --> 01:47:18.481
handling life cycles.

01:47:19.541 --> 01:47:22.068
That's why last year, released 
some of the architecture 

01:47:22.069 --> 01:47:24.349
components such as life cycle, 
view model for handling that.

01:47:28.060 --> 01:47:29.894
Also, these ruined and page 
components for loading data into

01:47:29.895 --> 01:47:33.194
your application, into your UI. 
This year, at I/O, we're 

01:47:33.195 --> 01:47:36.918
announcing two new components as
part of the Android jetpack 

01:47:36.919 --> 01:47:38.919
initiative.

01:47:40.181 --> 01:47:42.181
This is navigation for handling 
all 

01:47:43.186 --> 01:47:45.186
kinds

01:48:03.099 --> 01:48:08.204
.  Okay. The second thing I want
to mention when talking about 

01:48:08.205 --> 01:48:10.205
crashes is Kotlin.

01:48:13.526 --> 01:48:16.416
Last year, we announced Kotlin 
as a fir class language for 

01:48:16.417 --> 01:48:20.296
Android developers.  Before I 
say any more, I know it's not a 

01:48:20.297 --> 01:48:22.993
silver bullets.  It will not 
solve all the crashes in your 

01:48:22.994 --> 01:48:27.912
application.  However, it can 
save you for some common classes

01:48:27.913 --> 01:48:30.141
of errors that you might 
otherwise hit if you build your 

01:48:30.142 --> 01:48:34.873
apps with just Java.  Some of 
the other night things about 

01:48:37.103 --> 01:48:38.930
Kotlin is that it's highly 
interoperable with existing Java

01:48:38.931 --> 01:48:42.417
code so you don't have to go all
in and transformed all your apps

01:48:42.418 --> 01:48:47.527
into Kotlin.  You can start 
slowly adding it into your 

01:48:47.528 --> 01:48:49.528
applications and see if it helps
you.

01:48:50.847 --> 01:48:52.847
Actually, the developers love  
Kotlin 

01:48:54.963 --> 01:48:58.066
and we are seeing a steady 
uptake in usage.  So, if you 

01:48:58.067 --> 01:49:01.004
haven't tried yet, it's fully 
supported by us and it's free to

01:49:02.894 --> 01:49:04.894
use in your applications.

01:49:06.400 --> 01:49:07.411
The last thing I want to talk 
about is private and hidden 

01:49:07.412 --> 01:49:09.937
APIs.  Now, we offer an STK to 
developers 

01:49:15.200 --> 01:49:17.200
that's backed by documentation

01:49:20.565 --> 01:49:23.013
public interfaces that you can 
use.  The developers that exist 

01:49:23.014 --> 01:49:27.158
on devices, they were never 
intended for you to use.  That's

01:49:27.159 --> 01:49:29.159
not because we don't like you 

01:49:32.064 --> 01:49:34.064
as developers, we don't want you
to do nice things.

01:49:36.943 --> 01:49:39.426
It's just that these APIs aren't
ready to lose.  They might be 

01:49:39.427 --> 01:49:41.427
implementation details 

01:49:42.492 --> 01:49:44.492
on a certain device.

01:49:47.147 --> 01:49:49.523
We might even remind you -- the 
thing is, you should 

01:49:52.876 --> 01:49:54.876
never use private or hidden API 
in your apps.

01:49:55.925 --> 01:49:58.433
But in order to enforce that and
to actually help you make your 

01:49:58.434 --> 01:50:00.434
apps crash 

01:50:01.441 --> 01:50:03.441
less

01:50:11.120 --> 01:50:13.120
means you have to start using 
private APIs.

01:50:16.258 --> 01:50:18.258
We did a similar thing with the 
NTK 

01:50:19.776 --> 01:50:21.403
where we said you can no longer 
access libraries that exist in 

01:50:21.404 --> 01:50:23.404
the system if they're not 
exposed by APIs.

01:50:26.069 --> 01:50:28.542
So, please, if you can migrate 
from private APIs to public 

01:50:28.543 --> 01:50:30.543
APIs, they are documented, 
guaranteed to work, to make 

01:50:32.230 --> 01:50:34.230
their apps safer.

01:50:39.550 --> 01:50:41.601
So, let's talk about wake locks.

01:50:44.651 --> 01:50:46.672
It's an API that lets you keep 
the device from entering into a 

01:50:46.673 --> 01:50:50.006
deep sleep, from conserving the 
barrier.  A stuck wake lock is 

01:50:50.007 --> 01:50:53.666
when you use the API and because
of a bug or because you forgot, 

01:50:53.667 --> 01:50:55.905
you acquire a wake lock but you 
never release it so it just 

01:50:55.906 --> 01:51:00.630
keeps the CPU running and it 
keeps draining the battery.

01:51:03.671 --> 01:51:06.135
So, what's the best way to avoid
having a stuck wake lock in your

01:51:06.344 --> 01:51:11.426
application.  You can't have one
if you don't use wake locks.  

01:51:11.427 --> 01:51:13.658
That's cultural the same thing. 
Also, in modern Android 

01:51:13.659 --> 01:51:16.298
development, there's rarely a 
situation when you have to use a

01:51:16.299 --> 01:51:19.069
wake lock.  If we think about 
activities, so 

01:51:23.274 --> 01:51:27.140
whenever the user is actually 
using your app and for some 

01:51:23.274 --> 01:51:25.516
reason you want to keep the 
screen on, instead of setting a 

01:51:25.517 --> 01:51:29.998
wake lock automatically, just 
use a flag in the window that 

01:51:29.999 --> 01:51:32.857
will tell the system to keep the
screen on.  For scheduled 

01:51:32.858 --> 01:51:34.858
services, if you schedule them 
as jobs instead of having 

01:51:37.534 --> 01:51:39.528
your custom free running 
services, the job scheduler will

01:51:39.529 --> 01:51:41.810
hold the wake lock for you so 
again, you don't have to do that

01:51:41.811 --> 01:51:44.266
yourselves.
Similarly, with alarm manager, 

01:51:44.267 --> 01:51:49.215
if you use it to schedule an 
alarm and a broadcast receiver, 

01:51:49.216 --> 01:51:51.682
there will be a wake lock held 
for you for the duration of 

01:51:51.881 --> 01:51:53.881
unreceive.

01:51:56.157 --> 01:51:58.157
So if you don't use wake locks, 
you 

01:51:59.449 --> 01:52:03.965
don't need.  So if you must use 
a wake long, remember to always 

01:52:03.966 --> 01:52:05.966
choose a partial wake 

01:52:07.070 --> 01:52:09.070
lock, all the other wake locks 
are 

01:52:10.524 --> 01:52:13.272
dedicated on your Android 
servers and then always set a 

01:52:13.273 --> 01:52:15.273
timeout value so that in case 
you use it, the system will 

01:52:17.047 --> 01:52:19.047
clean up the wake lock for you 
after the timeout expires.

01:52:19.909 --> 01:52:21.909
Remember to give a static 
descriptive 

01:52:23.164 --> 01:52:23.987
tag without any counters or even
worse, private sensitive 

01:52:23.988 --> 01:52:29.097
information.  That way, we can 
surface the name of the wake log

01:52:29.098 --> 01:52:31.098
and the Android Vitals for you 
for easier debugging.

01:52:33.854 --> 01:52:36.104
And then remember to always call
defensively and assume things 

01:52:36.105 --> 01:52:39.100
can go wrong

01:52:53.909 --> 01:52:56.704
next I want to talk about 
excessive wake-ups.  I think the

01:52:56.705 --> 01:52:59.649
biggest cause is specifically 
the wake-up types of alarms.

01:53:02.933 --> 01:53:04.933
Now, let me give you the 3Rs of 
alarm manager.

01:53:06.216 --> 01:53:08.257
Remove calls to wake-up alarms 
if possible.  Reduce the 

01:53:08.258 --> 01:53:10.258
frequency if you have to use 
them for some reason, and if you

01:53:14.018 --> 01:53:16.018
can, replace your alarms with 
other APIs 

01:53:18.081 --> 01:53:19.664
such as If firebase Cloud 
messaging whenever the wake-up 

01:53:19.665 --> 01:53:22.336
has to happen for something on 
your server and you can send it 

01:53:22.337 --> 01:53:27.452
as a push message.  Or use one 
of the job scheduling APIs such 

01:53:27.453 --> 01:53:29.453
as the newly announced work 
manager 

01:53:31.067 --> 01:53:33.073
or even job scheduler or sync 
manager.  You know, with 

01:53:33.074 --> 01:53:36.518
intelligent job scheduling, you 
get a lot of things for free 

01:53:36.519 --> 01:53:39.374
such as automatic back-up and 
retry.  If your job fails for 

01:53:39.375 --> 01:53:41.375
any reason, you get criteria 
based scheduling so you can 

01:53:44.581 --> 01:53:47.249
only wake the device when it has
network or is charging so you 

01:53:47.250 --> 01:53:50.143
don't run your code 
unnecessarily.  You get those 

01:53:50.144 --> 01:53:52.383
and standby compliance and like 
I mentioned before, even 

01:53:55.145 --> 01:53:56.771
automatic wakelock handling.  
And there's a whole session 

01:53:56.772 --> 01:54:01.694
about background processing and 
a work manager API where you can

01:54:01.695 --> 01:54:04.339
learn more about this today.  
So, you mentioned the Android 

01:54:04.340 --> 01:54:08.863
profiler before.  And I'm happy 
to say that today in Android 

01:54:08.864 --> 01:54:10.864
Studio, we're also announcing 

01:54:13.813 --> 01:54:15.667
the new energy profiler so you 
can debug any kinds of problems 

01:54:15.668 --> 01:54:18.165
with wake lock if you use them 
in your app and also see 

01:54:19.365 --> 01:54:21.604
detailed information about 
alarms and jobs scheduled in 

01:54:21.605 --> 01:54:23.250
your application.
If you're interested to hear 

01:54:23.251 --> 01:54:28.976
more about the profiler, please 
go to the session tomorrow.  And

01:54:28.977 --> 01:54:30.797
with that, let me invite Fergus 
back on stage to talk to you 

01:54:30.798 --> 01:54:34.904
about what more you can do to 
improve quality of your apps.  

01:54:34.905 --> 01:54:36.905
Thanks.

01:54:38.556 --> 01:54:40.631
(applause)
&gt;&gt; FERGUS HURLEY: Great.

01:54:43.900 --> 01:54:45.726
So, we've covered a lot about 
what's available in Android 

01:54:45.727 --> 01:54:48.211
Vitals and the Play Console.  
I'm just going to quickly go 

01:54:48.212 --> 01:54:51.080
through some of the other tools 
you can be able to use to be 

01:54:51.081 --> 01:54:53.749
able to improve the performance 
of your application.  All of 

01:54:53.750 --> 01:54:56.216
Android Vitals is based on data 
that's collected from users' 

01:54:56.217 --> 01:55:01.398
devices, so, these are users 
that are actually being affected

01:55:01.399 --> 01:55:06.136
by the issue itself.  We would 
love for users not to actually 

01:55:06.137 --> 01:55:08.020
be impacted by any of these 
issues at all so we'd encourage 

01:55:08.021 --> 01:55:10.021
you to 

01:55:11.082 --> 01:55:13.727
do more prelaunch testing of 
your applications to find 

01:55:13.728 --> 01:55:15.728
issues.

01:55:17.852 --> 01:55:19.691
There's a session on this 
Thursday at 9:30, I encourage 

01:55:19.692 --> 01:55:22.159
you to attend that one.
Over the past couple of years, 

01:55:22.160 --> 01:55:27.265
we've made radical improvements 
to the operating system.  To 

01:55:27.266 --> 01:55:28.521
help users be able to get more 
out of the device in terms of 

01:55:28.522 --> 01:55:30.522
performance and for the battery 
to last longer.

01:55:33.576 --> 01:55:35.576
There's major

01:55:37.794 --> 01:55:39.223
improvements with the latest 
version of Android covered in 

01:55:39.224 --> 01:55:41.288
the keynote today.  Can you 
learn a lot more about that on 

01:55:43.119 --> 01:55:45.119
Thursday at this session here.

01:55:50.354 --> 01:55:52.354
Boytech helped with you 
improving your 

01:55:53.429 --> 01:55:55.655
profilers but there's a bunch of
other tools to improve even if 

01:55:55.656 --> 01:55:58.105
you're using another ID to build
example of your game.

01:56:02.430 --> 01:56:04.695
Firebase offers many tools to 
help you be able to track the 

01:56:04.696 --> 01:56:06.696
performance of your 

01:56:09.202 --> 01:56:11.202
application, crash analytics and
Firebase performance monitoring.

01:56:13.314 --> 01:56:15.557
We encourage you to use those 
STKs if you want to be able to 

01:56:15.558 --> 01:56:17.811
do custom logging of your issues
that you might be having with 

01:56:17.812 --> 01:56:21.913
your application.  Note, there 
will be a difference in the data

01:56:21.914 --> 01:56:24.608
that's collected from the 
Firebase STK versus what's 

01:56:24.609 --> 01:56:29.098
collected true Android Vitals 
which is from users who have 

01:56:29.099 --> 01:56:32.799
opted into sharing the data with
Google and our partners.

01:56:36.241 --> 01:56:37.676
You want to learn more about 
Firebase performance monitoring 

01:56:37.677 --> 01:56:39.677
and crash 

01:56:42.433 --> 01:56:44.472
analytics, you can attend this 
talk at 11:30 tomorrow.  We 

01:56:44.473 --> 01:56:46.730
covered a lot in terms of what's
available in Android Vitals in 

01:56:46.731 --> 01:56:50.667
the Play Console today, but 
there's a lot more today, and 

01:56:50.668 --> 01:56:54.159
you can hear all about that on 
developers.Android.com/Vitals.

01:56:58.034 --> 01:56:59.686
Just to quickly recap, we 
started by covering why 

01:56:59.687 --> 01:57:01.687
performance is vital overall, 
and hopefully, you all agree 

01:57:02.744 --> 01:57:04.744
that it is.

01:57:07.225 --> 01:57:09.272
That's why you're here today so 
thank you.  Then we covered 

01:57:09.273 --> 01:57:11.514
what's new in Vitals and I hope 
you take advantage of these new 

01:57:11.515 --> 01:57:13.515
tools.

01:57:15.063 --> 01:57:16.698
The then we talked about how you
can debug these Vitals issues 

01:57:16.699 --> 01:57:20.783
and finally what's available.  
We'd love to be able to hear 

01:57:20.784 --> 01:57:24.861
your feedback on this talk if 
you can go to this URL, provide 

01:57:24.862 --> 01:57:27.737
this feedback, we'd love to hear
from you.  If you have any 

01:57:27.738 --> 01:57:29.738
questions for us, you 

01:57:31.037 --> 01:57:33.084
can meet us from 4:00 to 5:00 at
this stand.  It's on the other 

01:57:33.085 --> 01:57:35.085
side of the lot, and 

01:57:36.367 --> 01:57:38.811
we'll also be available at the 
Google Play Console sandbox from

01:57:38.812 --> 01:57:42.322
2:00 PM to 5:00 PM tomorrow to 
answer any of your questions.  

01:57:42.323 --> 01:57:44.323
Thank you all very much for 
joining us here today.

01:57:46.573 --> 01:57:47.555
We --
(applause)

01:57:47.556 --> 01:57:49.556
Yeah.

01:57:51.458 --> 01:57:52.900
We hope that you'll make radical
improvements to your application

01:57:52.901 --> 01:57:56.188
over the next year and that 
you'll get covered in our IO 

01:57:56.189 --> 01:57:58.189
talk over the next year.  So 
best of luck to you all with 

01:57:58.877 --> 01:58:00.877
improving your application.  
Thank you.

01:58:01.944 --> 01:58:03.616
(Session was concluded at 5:40 
PM CT)

01:58:03.617 --> 01:58:05.950
***
This text, document, or file is 

01:58:03.617 --> 01:58:07.617
based on live transcription.  
Communication Access Realtime 

01:58:03.617 --> 01:58:07.750
Translation (CART), captioning, 
and/or live transcription are 

01:58:03.617 --> 01:58:06.617
provided in order to facilitate 
communication

01:58:03.617 --> 01:58:07.750
accessibility and may not be a 
totally verbatim record of the 

01:58:03.617 --> 01:58:07.350
proceedings.  This text, 
document, or file is not to be 

01:58:03.617 --> 01:58:07.750
distributed or used in any way 
that may violate copyright law.

01:58:03.617 --> 01:58:05.617
***

