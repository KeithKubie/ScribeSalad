WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.480
[MUSIC PLAYING]

00:00:08.312 --> 00:00:09.520
IRENE ALVARADO: Hi, everyone.

00:00:09.520 --> 00:00:11.160
My name is Irene,
and I work at a place

00:00:11.160 --> 00:00:13.680
called the Creative Lab,
a team inside of Google.

00:00:13.680 --> 00:00:16.290
And some of us are
interested in creating

00:00:16.290 --> 00:00:18.600
what we call
experiments to showcase

00:00:18.600 --> 00:00:21.210
and make more accessible some
of the machine learning research

00:00:21.210 --> 00:00:22.470
that's coming out of Google.

00:00:22.470 --> 00:00:24.095
And a lot of our work
goes to this site

00:00:24.095 --> 00:00:26.280
called the Experiments
with Google site.

00:00:26.280 --> 00:00:28.170
Now, before I talk about some
of the products on the site,

00:00:28.170 --> 00:00:29.545
let me just say
that we're really

00:00:29.545 --> 00:00:32.202
inspired by pioneering AI
researcher Seymour Papert, who

00:00:32.202 --> 00:00:33.660
wrote a lot about
learning theories

00:00:33.660 --> 00:00:35.580
in humans and
essentially kind of how

00:00:35.580 --> 00:00:37.050
to make learning not suck.

00:00:37.050 --> 00:00:39.098
So this is one of
his great quotes.

00:00:39.098 --> 00:00:40.890
"Every maker of video
games knows something

00:00:40.890 --> 00:00:43.890
that the makers of curriculum
don't seem to understand.

00:00:43.890 --> 00:00:46.350
You'll never see a video being
advertised as being easy.

00:00:46.350 --> 00:00:47.700
Kids who do not like
school will tell you

00:00:47.700 --> 00:00:48.992
it's not because it's too hard.

00:00:48.992 --> 00:00:50.162
It's because it's boring."

00:00:50.162 --> 00:00:51.870
So if there are some
parents in the room,

00:00:51.870 --> 00:00:54.640
you might be agreeing
with this statement.

00:00:54.640 --> 00:00:56.400
So I'll show you
some projects that

00:00:56.400 --> 00:00:58.770
were inspired by this
thinking that learning should

00:00:58.770 --> 00:01:00.570
be engaging, made
in collaboration

00:01:00.570 --> 00:01:02.880
with the TensorFlow.js team
and many other research

00:01:02.880 --> 00:01:04.690
teams at Google.

00:01:04.690 --> 00:01:06.030
So this is the first one.

00:01:06.030 --> 00:01:07.350
It's called Teachable Machine.

00:01:07.350 --> 00:01:09.540
And essentially it's
a KNN classifier that

00:01:09.540 --> 00:01:11.175
runs entirely in the browser.

00:01:11.175 --> 00:01:13.050
And it lets you train
three classes of images

00:01:13.050 --> 00:01:15.930
that trigger different kinds
of inputs, like GIFs and sound.

00:01:15.930 --> 00:01:17.100
So I don't have time
to demo it, but I'll

00:01:17.100 --> 00:01:19.710
show you what happens after you
train a model with the tool.

00:01:19.710 --> 00:01:20.903
So can I get the video?

00:01:20.903 --> 00:01:21.570
[VIDEO PLAYBACK]

00:01:21.570 --> 00:01:24.045
[SPOOKY ORGAN MUSIC]

00:01:24.045 --> 00:01:26.025
[BIRDS CHIRPING]

00:01:26.025 --> 00:01:28.005
[SPOOKY ORGAN MUSIC]

00:01:28.005 --> 00:01:30.637
[BIRDS CHIRPING]

00:01:30.637 --> 00:01:31.470
[SPOOKY ORGAN MUSIC]

00:01:31.470 --> 00:01:33.161
[BIRDS CHIRPING]

00:01:33.161 --> 00:01:33.994
[SPOOKY ORGAN MUSIC]

00:01:33.994 --> 00:01:34.577
[END PLAYBACK]

00:01:34.577 --> 00:01:37.830
See it choosing
between two classes.

00:01:37.830 --> 00:01:40.310
Yeah, so, hopefully,
you get how it works.

00:01:40.310 --> 00:01:42.260
Alex Chen, the creator,
he trained a class

00:01:42.260 --> 00:01:45.260
to recognize the bird
origami and another class

00:01:45.260 --> 00:01:48.560
to recognize the
spooky person origami.

00:01:48.560 --> 00:01:51.330
OK, back to the slides.

00:01:51.330 --> 00:01:51.830
Thank you.

00:01:51.830 --> 00:01:53.870
So we released the
experiment online.

00:01:53.870 --> 00:01:56.370
All the inference and training
is happening in the browser.

00:01:56.370 --> 00:01:58.340
And we also released
the open source--

00:01:58.340 --> 00:02:01.730
we open sourced the
boilerplate code that

00:02:01.730 --> 00:02:03.260
went along with the experiment.

00:02:03.260 --> 00:02:06.440
And what happened next was that
we were really kind of taken

00:02:06.440 --> 00:02:08.690
aback by all the stories of
teachers around the world,

00:02:08.690 --> 00:02:11.780
like this one, who started using
Teachable Machine to introduce

00:02:11.780 --> 00:02:13.910
ML into the classroom.

00:02:13.910 --> 00:02:15.890
Here's another example
of kids learning

00:02:15.890 --> 00:02:17.750
about smart cities
and kind of training

00:02:17.750 --> 00:02:19.940
the computer to recognize
handmade stop signs.

00:02:19.940 --> 00:02:22.130
This was really amazing.

00:02:22.130 --> 00:02:25.040
And finally, we heard from
another renowned and pioneering

00:02:25.040 --> 00:02:28.250
researcher, Hal Abelson,
who teaches at MIT, that he

00:02:28.250 --> 00:02:30.350
had been using Teachable
Machine to introduce ML

00:02:30.350 --> 00:02:31.127
to policymakers.

00:02:31.127 --> 00:02:32.960
And for a lot of them,
it was the first time

00:02:32.960 --> 00:02:34.700
that they had ever
trained a model.

00:02:34.700 --> 00:02:37.190
So needless to say, we're really
happy that although simple

00:02:37.190 --> 00:02:39.260
in nature, Teachable
Machine ended up

00:02:39.260 --> 00:02:41.840
being a really good tool
for educators and people

00:02:41.840 --> 00:02:44.040
that were new to
machine learning.

00:02:44.040 --> 00:02:45.660
So here's another example.

00:02:45.660 --> 00:02:46.910
This one's called Move Mirror.

00:02:46.910 --> 00:02:48.480
And the concept
is really simple.

00:02:48.480 --> 00:02:50.390
You strike a pose in
front of a webcam,

00:02:50.390 --> 00:02:53.010
and you get an image
with a matching pose.

00:02:53.010 --> 00:02:56.550
And again, this is all
happening on the web.

00:02:56.550 --> 00:03:00.020
So here's another example
of, actually, people using it

00:03:00.020 --> 00:03:01.430
in the form of an installation.

00:03:01.430 --> 00:03:03.920
People do really funny moves.

00:03:03.920 --> 00:03:05.870
And again, this is
happening on a phone,

00:03:05.870 --> 00:03:08.210
but on the phone's browser.

00:03:08.210 --> 00:03:09.650
And so the story
for this one was

00:03:09.650 --> 00:03:11.960
that in order to make the
experiment really accessible,

00:03:11.960 --> 00:03:15.805
we had to take the
tech to the web,

00:03:15.805 --> 00:03:17.180
so that we wouldn't
require users

00:03:17.180 --> 00:03:20.840
to have a complicated tech setup
or to use IR cameras or depth

00:03:20.840 --> 00:03:22.890
sensors, which can be expensive.

00:03:22.890 --> 00:03:24.020
So PoseNet was born.

00:03:24.020 --> 00:03:26.360
To our knowledge, it's
the first pose estimation

00:03:26.360 --> 00:03:27.560
model for the web.

00:03:27.560 --> 00:03:28.747
And it's open source.

00:03:28.747 --> 00:03:30.080
It runs locally in your browser.

00:03:30.080 --> 00:03:33.260
And it uses good
ol' RGB webcams.

00:03:33.260 --> 00:03:34.910
So again, we were
really taken aback

00:03:34.910 --> 00:03:38.615
by all the creative projects
that we saw popping up online.

00:03:38.615 --> 00:03:40.490
Just to give you a sense,
the one on the left

00:03:40.490 --> 00:03:41.572
is a musical interface.

00:03:41.572 --> 00:03:43.280
The one in the middle
is a ping pong game

00:03:43.280 --> 00:03:44.613
that you can use with your head.

00:03:44.613 --> 00:03:46.610
I really want to play that one.

00:03:46.610 --> 00:03:49.910
And the one on the right is
a kind of performative motion

00:03:49.910 --> 00:03:52.340
capture animation.

00:03:52.340 --> 00:03:54.050
But we also started
hearing from people

00:03:54.050 --> 00:03:56.383
in the accessibility world
that they were using PoseNet.

00:03:56.383 --> 00:03:59.390
So we decided to partner
with a bunch of groups

00:03:59.390 --> 00:04:02.870
that work at the intersection
of disability and technology,

00:04:02.870 --> 00:04:07.700
like the NYU Ability Project,
and musicians, artists, makers

00:04:07.700 --> 00:04:09.230
in the accessibility world.

00:04:09.230 --> 00:04:11.690
And out of that collaboration
came a set of creative tools

00:04:11.690 --> 00:04:13.250
that we're calling Creatability.

00:04:13.250 --> 00:04:15.290
And a lot of them
use PoseNet for users

00:04:15.290 --> 00:04:17.480
who have motor impairments
to be able to interface

00:04:17.480 --> 00:04:19.310
with a computer with
their whole bodies

00:04:19.310 --> 00:04:21.290
instead of through a
keyboard and a mouse.

00:04:21.290 --> 00:04:23.040
So again, I don't have
time to demo these.

00:04:23.040 --> 00:04:26.060
But just give you a sense,
the one on the bottom left

00:04:26.060 --> 00:04:28.580
is a visualization tool
made by a musician named

00:04:28.580 --> 00:04:31.340
Jay Zimmerman, who's deaf,
and the one on the top right

00:04:31.340 --> 00:04:32.990
is an accessible
musical instrument

00:04:32.990 --> 00:04:35.005
made by a group
called Open Up Music.

00:04:35.005 --> 00:04:37.130
And we just took their
designs and kind of moved it

00:04:37.130 --> 00:04:38.250
to the web.

00:04:38.250 --> 00:04:41.030
So again, all of
the components that

00:04:41.030 --> 00:04:45.600
made this project are accessible
and they've been open sourced.

00:04:45.600 --> 00:04:47.160
So just a step
back for a second,

00:04:47.160 --> 00:04:50.390
if we were to think about what
made these projects successful

00:04:50.390 --> 00:04:52.460
or at least useful
for other people,

00:04:52.460 --> 00:04:54.800
we can see that they were all
interactive and accessible

00:04:54.800 --> 00:04:55.633
through the browser.

00:04:55.633 --> 00:04:58.287
So it really lowered the barrier
of entry for a lot of people.

00:04:58.287 --> 00:04:59.870
They all had an
open-source component,

00:04:59.870 --> 00:05:01.912
so that people could kind
of look under the hood,

00:05:01.912 --> 00:05:04.070
see what's happening,
modify them, play with them.

00:05:04.070 --> 00:05:05.670
And then, finally,
they're all free,

00:05:05.670 --> 00:05:08.660
because the processing
is happening locally

00:05:08.660 --> 00:05:10.490
in the browser
with TensorFlow.js.

00:05:10.490 --> 00:05:12.005
And that gave us
privacy, so that we

00:05:12.005 --> 00:05:13.880
didn't have to send
images of people's bodies

00:05:13.880 --> 00:05:16.565
and faces to any servers.

00:05:16.565 --> 00:05:19.190
So again, all the projects that
I went through kind of quickly,

00:05:19.190 --> 00:05:20.915
they're on the
Experiments.withGoogle.com

00:05:20.915 --> 00:05:21.770
site.

00:05:21.770 --> 00:05:23.690
And even though these
were created in-house,

00:05:23.690 --> 00:05:26.420
we actually feature work by
more than 1,700 developers

00:05:26.420 --> 00:05:27.690
from around the world.

00:05:27.690 --> 00:05:29.480
So if any of this
resonates with you,

00:05:29.480 --> 00:05:31.880
this is really an open
invitation for you

00:05:31.880 --> 00:05:32.750
to submit your work.

00:05:32.750 --> 00:05:35.060
And I hope to have
showed that you never

00:05:35.060 --> 00:05:36.470
know who you might
inspire or who

00:05:36.470 --> 00:05:38.720
might take your work and
kind of innovate on top of it

00:05:38.720 --> 00:05:40.460
and use in really creative ways.

00:05:40.460 --> 00:05:41.240
Thank you.

00:05:41.240 --> 00:05:42.740
[APPLAUSE]

00:05:42.740 --> 00:05:46.690
[MUSIC PLAYING]

