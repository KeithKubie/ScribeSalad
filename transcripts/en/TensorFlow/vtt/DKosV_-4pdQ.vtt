WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.339
[MUSIC PLAYING]

00:00:08.109 --> 00:00:09.930
RAZIEL ALVAREZ: Hi, everyone.

00:00:09.930 --> 00:00:11.300
My name is Raziel.

00:00:11.300 --> 00:00:14.060
I'm an engineering lead
in TensorFlow Lite.

00:00:14.060 --> 00:00:16.100
And today I will be
covering a lot of the work

00:00:16.100 --> 00:00:17.942
that the team has done
over the past year

00:00:17.942 --> 00:00:19.400
to bring your
features that you can

00:00:19.400 --> 00:00:21.380
take advantage of right away.

00:00:21.380 --> 00:00:23.390
Hopefully you're
already doing that.

00:00:23.390 --> 00:00:25.100
I will also cover
a lot of the work

00:00:25.100 --> 00:00:27.790
that we have for this year.

00:00:27.790 --> 00:00:30.830
But before we jump
into that, let

00:00:30.830 --> 00:00:34.010
me give you a brief introduction
of what TensorFlow Lite is.

00:00:34.010 --> 00:00:37.580
So TensorFlow Lite is our
infrastructure framework

00:00:37.580 --> 00:00:40.980
to run machine
learning on device.

00:00:40.980 --> 00:00:44.840
And the reason why we decided
to build TensorFlow Lite was

00:00:44.840 --> 00:00:47.000
because machine
learning, you typically

00:00:47.000 --> 00:00:49.700
think of it as running
on the server, right?

00:00:49.700 --> 00:00:52.490
But more and more is
moving to edge devices,

00:00:52.490 --> 00:00:57.080
like mobile devices, cars,
wearables, you name it.

00:00:57.080 --> 00:00:59.840
So there is more
machine learning moving

00:00:59.840 --> 00:01:01.550
to different kinds of devices.

00:01:01.550 --> 00:01:03.720
And there is good
reasons for that.

00:01:03.720 --> 00:01:06.680
One, you have access
to a lot more data.

00:01:06.680 --> 00:01:09.770
Because you have access to the
audio, to the camera, and you

00:01:09.770 --> 00:01:12.410
don't have to stream it
all the way to the server.

00:01:12.410 --> 00:01:14.130
So you can do a lot of stuff.

00:01:14.130 --> 00:01:15.740
You can do a [INAUDIBLE].

00:01:15.740 --> 00:01:18.590
And that means that
you can build faster

00:01:18.590 --> 00:01:21.637
and close-knit interactions.

00:01:21.637 --> 00:01:23.970
Another advantage of running
machine learning on devices

00:01:23.970 --> 00:01:27.500
is that it has a strong
privacy-preserving component,

00:01:27.500 --> 00:01:28.000
right?

00:01:28.000 --> 00:01:29.390
Because the data
stays on device.

00:01:29.390 --> 00:01:32.680
It doesn't necessarily
has to go onto the server.

00:01:32.680 --> 00:01:37.850
However, well, that means that
basically by building machine

00:01:37.850 --> 00:01:39.860
learning on device
by allowing that,

00:01:39.860 --> 00:01:42.200
we can build products
that otherwise wouldn't

00:01:42.200 --> 00:01:46.400
be able really to work
if you really rely only

00:01:46.400 --> 00:01:49.650
on server-side execution.

00:01:49.650 --> 00:01:53.090
However, it is hard.

00:01:53.090 --> 00:01:54.800
Perhaps one obvious
thing is that you

00:01:54.800 --> 00:01:56.300
don't have the same
compute power that you

00:01:56.300 --> 00:01:57.410
have in the server, right?

00:01:57.410 --> 00:02:01.100
So if you have a new, very cool
shiny machine learning model

00:02:01.100 --> 00:02:04.520
that consumes too
much compute power,

00:02:04.520 --> 00:02:06.020
it's not like in
the server that you

00:02:06.020 --> 00:02:10.020
can throw perhaps more servers
at it and be done with it.

00:02:10.020 --> 00:02:11.990
You have to be mindful
of the resources

00:02:11.990 --> 00:02:13.740
that you have in the device.

00:02:13.740 --> 00:02:16.580
Another of those
resources is the memory.

00:02:16.580 --> 00:02:18.150
So you have restricted memory.

00:02:18.150 --> 00:02:21.440
So you have to be careful
about how you craft your model.

00:02:21.440 --> 00:02:25.130
Perhaps try some
compression techniques.

00:02:25.130 --> 00:02:27.530
And last but not least,
many of these devices

00:02:27.530 --> 00:02:28.760
are battery powered, right?

00:02:28.760 --> 00:02:30.680
So even if you have
a model that fits

00:02:30.680 --> 00:02:34.070
in memory that can
execute, well, you

00:02:34.070 --> 00:02:36.140
don't want to consume
all the battery

00:02:36.140 --> 00:02:38.970
in a battery-powered device.

00:02:38.970 --> 00:02:42.680
So for all these reasons, that's
why we build TensorFlow Lite.

00:02:42.680 --> 00:02:47.150
We want to make deploying
machine learning on Edge

00:02:47.150 --> 00:02:50.250
devices as easily as possible.

00:02:50.250 --> 00:02:51.980
And what can you do
with it right now?

00:02:51.980 --> 00:02:54.230
Well, you can do
a lot of things.

00:02:54.230 --> 00:02:57.290
These are different
tasks that are supported

00:02:57.290 --> 00:02:59.580
by TensorFlow Lite already.

00:02:59.580 --> 00:03:03.000
And if you look at it, when
you hear machine learning,

00:03:03.000 --> 00:03:06.980
you typically fall into
one of these tasks--

00:03:06.980 --> 00:03:10.100
text, speech, image
processing, audio processing,

00:03:10.100 --> 00:03:12.520
a lot of content generation.

00:03:12.520 --> 00:03:16.790
So really everything
is already there

00:03:16.790 --> 00:03:18.750
for you to take advantage of.

00:03:18.750 --> 00:03:21.500
And because we are ready
are able to support

00:03:21.500 --> 00:03:25.190
all these tasks, it means that
it has a lot of users already.

00:03:25.190 --> 00:03:27.530
So TensorFlow Lite
is currently deployed

00:03:27.530 --> 00:03:31.340
in over two billion
devices in production.

00:03:31.340 --> 00:03:35.240
And this normally means Google
properties, and some of which

00:03:35.240 --> 00:03:38.540
are really core to Google, like
the Assistant, Google Photos--

00:03:38.540 --> 00:03:43.670
those from other companies, from
other large companies and also

00:03:43.670 --> 00:03:45.080
frameworks.

00:03:45.080 --> 00:03:48.110
So a lot of our infrastructure
is powering frameworks

00:03:48.110 --> 00:03:51.480
like AutoML and ML Kit.

00:03:51.480 --> 00:03:56.670
And I can tell you many
reasons why it's a good idea

00:03:56.670 --> 00:03:58.020
to use our infrastructure.

00:03:58.020 --> 00:04:00.480
But we saw that it will
be best if we bring

00:04:00.480 --> 00:04:02.640
two of our important
clients to tell you

00:04:02.640 --> 00:04:07.800
how they use TensorFlow Lite
and why they decided to use it.

00:04:07.800 --> 00:04:09.907
Our first presenter is Alex.

00:04:09.907 --> 00:04:11.490
He's coming from the
Google Assistant.

00:04:11.490 --> 00:04:14.430
And the Google Assistant, I
mean it has a wide variety

00:04:14.430 --> 00:04:17.200
of devices where it's running.

00:04:17.200 --> 00:04:19.550
So let's hear from him why
they're using TensorFlow Lite

00:04:19.550 --> 00:04:20.744
and how they are doing.

00:04:20.744 --> 00:04:24.202
[APPLAUSE]

00:04:25.137 --> 00:04:26.470
ALEX GRUENSTEIN: Thanks, Raziel.

00:04:26.470 --> 00:04:28.050
So I'm Alex.

00:04:28.050 --> 00:04:30.820
I'm the engineering lead on
our embedded speech recognition

00:04:30.820 --> 00:04:33.552
team at Google.

00:04:33.552 --> 00:04:35.760
So one of the key applications
for speech recognition

00:04:35.760 --> 00:04:38.080
at Google is the
Google Assistant.

00:04:38.080 --> 00:04:40.330
Google recently announced
that the Google Assistant

00:04:40.330 --> 00:04:44.770
is on about a billion devices,
ranging from phones, speakers,

00:04:44.770 --> 00:04:48.610
smart displays, cars,
TVs laptops, wearables,

00:04:48.610 --> 00:04:49.210
you name it.

00:04:49.210 --> 00:04:51.850
We're trying to put
the Assistant into it.

00:04:51.850 --> 00:04:54.880
From my perspective, that
means a wide range of devices--

00:04:54.880 --> 00:04:58.690
high-end devices, low-end
devices, ARM, x86,

00:04:58.690 --> 00:05:01.420
battery-powered,
plugged in, a wide range

00:05:01.420 --> 00:05:02.710
of operating systems.

00:05:02.710 --> 00:05:04.960
So basically the neural
nets that I build have

00:05:04.960 --> 00:05:08.800
to be able to run anywhere.

00:05:08.800 --> 00:05:12.060
For Assistant, we have a
couple of key speech on-device

00:05:12.060 --> 00:05:13.150
capabilities.

00:05:13.150 --> 00:05:15.330
The first is the hotword.

00:05:15.330 --> 00:05:18.780
You need to be able to say, hey,
Google, to trigger your device.

00:05:18.780 --> 00:05:20.730
We need to both
detect that hotword,

00:05:20.730 --> 00:05:22.410
and we need to
recognize that it's

00:05:22.410 --> 00:05:27.060
your voice and not somebody else
trying to activate your phone.

00:05:27.060 --> 00:05:30.220
This means because this
is running all the time,

00:05:30.220 --> 00:05:32.520
we have to have a tiny
memory and computation

00:05:32.520 --> 00:05:34.890
footprint running continuously.

00:05:34.890 --> 00:05:36.960
And we're extremely
latency sensitive.

00:05:36.960 --> 00:05:40.650
As soon as you say the hotword,
we need to immediately trigger.

00:05:40.650 --> 00:05:44.520
Our other major application is
on-device speech recognition.

00:05:44.520 --> 00:05:46.980
We use this often
when you're offline

00:05:46.980 --> 00:05:49.440
or you have a bad
network connection.

00:05:49.440 --> 00:05:51.720
So instead of using
the server, we actually

00:05:51.720 --> 00:05:53.802
run speech recognition
on the device.

00:05:53.802 --> 00:05:56.010
Now this is kind of on the
other end of the spectrum.

00:05:56.010 --> 00:05:59.400
Here we're trying to run large
models as fast as possible

00:05:59.400 --> 00:06:02.170
to get the best
possible accuracy.

00:06:02.170 --> 00:06:04.080
So it's a high
computational load,

00:06:04.080 --> 00:06:08.010
typically running
in a short burst.

00:06:08.010 --> 00:06:09.900
Over the years,
we've been working

00:06:09.900 --> 00:06:11.578
on this for a long time.

00:06:11.578 --> 00:06:13.620
Before there was TensorFlow
Lite, before the rest

00:06:13.620 --> 00:06:14.910
TensorFlow--

00:06:14.910 --> 00:06:17.550
so we've been investing
a lot for a long time

00:06:17.550 --> 00:06:20.640
in building our own on-device
neural net libraries

00:06:20.640 --> 00:06:22.380
and shipping them to
production in Google

00:06:22.380 --> 00:06:24.780
products for a long time.

00:06:24.780 --> 00:06:26.940
As a team, we've already
invested significantly

00:06:26.940 --> 00:06:30.540
in keeping our code size
tiny, our memory usage small,

00:06:30.540 --> 00:06:34.230
optimizing our compute for all
of these different platforms.

00:06:34.230 --> 00:06:36.960
And all of our code, after
many years of production use,

00:06:36.960 --> 00:06:42.680
is really hardened and hopefully
is bug free as you can get it.

00:06:42.680 --> 00:06:46.960
But over the last year, we
decided to migrate to TFLite.

00:06:46.960 --> 00:06:49.060
Now this wasn't an
easy decision for us

00:06:49.060 --> 00:06:52.330
because our libraries are
already so well optimized.

00:06:52.330 --> 00:06:55.150
And as we switched over,
we checked very carefully

00:06:55.150 --> 00:06:57.520
to make sure that TFLite
would meet or beat

00:06:57.520 --> 00:07:01.780
our existing libraries in
terms of their size and speed.

00:07:01.780 --> 00:07:03.920
And I'm happy to report
that over the last year,

00:07:03.920 --> 00:07:07.120
we've migrated all of the
computation we do on CPU

00:07:07.120 --> 00:07:10.180
for our models to TFLite.

00:07:10.180 --> 00:07:12.520
Now we're excited about
this, because this really

00:07:12.520 --> 00:07:17.080
lays the groundwork for us
moving to a new standard that's

00:07:17.080 --> 00:07:18.770
being widely adopted.

00:07:18.770 --> 00:07:21.730
And we think TFLite will help us
accelerate our models on things

00:07:21.730 --> 00:07:26.380
like GPUs and Edge TPUs, all
the new kinds of accelerators

00:07:26.380 --> 00:07:27.530
that are coming.

00:07:27.530 --> 00:07:28.030
So thanks.

00:07:28.030 --> 00:07:29.718
I'll turn it back
over to Raziel.

00:07:29.718 --> 00:07:33.122
[APPLAUSE]

00:07:33.622 --> 00:07:35.550
RAZIEL ALVAREZ: Thanks, Alex.

00:07:35.550 --> 00:07:39.060
So Alex highlights two key
goals for TensorFlow Lite.

00:07:39.060 --> 00:07:43.110
So we want to be able to
support your current use cases.

00:07:43.110 --> 00:07:46.830
But perhaps equally
important and more important

00:07:46.830 --> 00:07:49.635
is we want to
cater to your needs

00:07:49.635 --> 00:07:53.760
that you will have in one,
two years in the future.

00:07:53.760 --> 00:07:57.230
So our next presenter
is Huejie from NetEase.

00:07:57.230 --> 00:07:59.730
NetEase is a
China-based company that

00:07:59.730 --> 00:08:03.460
has several applications and
hundreds of millions of users.

00:08:03.460 --> 00:08:04.623
So let's welcome him.

00:08:04.623 --> 00:08:07.747
[APPLAUSE]

00:08:08.247 --> 00:08:10.126
HUIJIE LIN: Thanks, Raziel.

00:08:10.126 --> 00:08:12.720
I'm Huijie Lin from
NetEase Youdao.

00:08:12.720 --> 00:08:18.720
And in Youdao, we are
first the leading company

00:08:18.720 --> 00:08:21.990
in online education as well.

00:08:21.990 --> 00:08:26.100
And we have built a lot
of dictionary translation

00:08:26.100 --> 00:08:30.090
and note-taking
applications in China.

00:08:30.090 --> 00:08:33.780
We have over 800
million users in total.

00:08:33.780 --> 00:08:39.950
And we also have 22
million daily active users.

00:08:39.950 --> 00:08:45.560
In the past years, we have built
several dictionary translation

00:08:45.560 --> 00:08:47.900
and note-taking applications.

00:08:47.900 --> 00:08:52.970
For example, we have the most
popular dictionary application

00:08:52.970 --> 00:08:55.910
in China, which is
Youdao Dictionary.

00:08:55.910 --> 00:09:01.040
And we also have the most
popular translation application

00:09:01.040 --> 00:09:03.710
in China, which is
Youdao Translator.

00:09:03.710 --> 00:09:08.540
And we also have the most
popular dictionary and language

00:09:08.540 --> 00:09:13.250
learning application in
India, which is U-Dictionary.

00:09:13.250 --> 00:09:17.870
And in these applications, we
provide some features for users

00:09:17.870 --> 00:09:24.650
to conveniently look up
words in the scenarios.

00:09:24.650 --> 00:09:27.500
Such as for example,
you can use your camera

00:09:27.500 --> 00:09:32.270
to recognize the words
in from the images

00:09:32.270 --> 00:09:37.170
and do the OCR and the
translation on your devices.

00:09:37.170 --> 00:09:42.920
So we use the TensorFlow
Lite to accelerate the OCR

00:09:42.920 --> 00:09:44.820
and translation here.

00:09:44.820 --> 00:09:50.950
And also, we have provided
the photo translation

00:09:50.950 --> 00:09:52.400
in our applications.

00:09:52.400 --> 00:09:56.540
For example, you
can use the camera

00:09:56.540 --> 00:10:00.630
to take a photo from
the many scenarios.

00:10:00.630 --> 00:10:04.940
And it will do the
whole image OCR

00:10:04.940 --> 00:10:09.660
and translate the text
into another languages.

00:10:09.660 --> 00:10:14.870
And then it will erase
the text on the image

00:10:14.870 --> 00:10:22.220
and replace the original text
to the translated text and then

00:10:22.220 --> 00:10:25.700
to present users with
the translations.

00:10:25.700 --> 00:10:29.300
And in this scenario, we
also use the TensorFlow Lite

00:10:29.300 --> 00:10:34.920
to accelerate our OCR and the
translation services here.

00:10:34.920 --> 00:10:38.720
And we all know that the
OCR and the translation,

00:10:38.720 --> 00:10:43.760
it is very sensitive to
the binary size and also

00:10:43.760 --> 00:10:48.560
computation resources
and its responding time.

00:10:48.560 --> 00:10:54.500
So we choose the TensorFlow
Lite to accelerate our abilities

00:10:54.500 --> 00:11:00.720
on device to provide the more
efficient on-device [INAUDIBLE]

00:11:00.720 --> 00:11:01.340
here.

00:11:01.340 --> 00:11:04.550
So that's why we choose the
TensorFlow Lite to do this.

00:11:04.550 --> 00:11:06.437
Thanks.

00:11:06.437 --> 00:11:09.371
[APPLAUSE]

00:11:09.371 --> 00:11:11.910
RAZIEL ALVAREZ: Thanks, Huijie.

00:11:11.910 --> 00:11:14.220
It's really excited to see
how our infrastructure is

00:11:14.220 --> 00:11:16.850
used by hundreds of millions
of users around the world.

00:11:16.850 --> 00:11:19.410
And this is something that
I wanted to highlight.

00:11:19.410 --> 00:11:21.600
Just like the rest
of TensorFlow,

00:11:21.600 --> 00:11:24.570
TensorFlow Lite is geared
to not only towards Google

00:11:24.570 --> 00:11:26.430
but towards everybody out there.

00:11:26.430 --> 00:11:29.570
Everybody should be able
to take advantage of it.

00:11:29.570 --> 00:11:31.510
OK, so for the rest
of the presentation,

00:11:31.510 --> 00:11:34.140
I will cover what I
said at the start.

00:11:34.140 --> 00:11:36.690
A lot of the features that
are having made available

00:11:36.690 --> 00:11:41.540
over the past year and some
of our roadmap for this year.

00:11:41.540 --> 00:11:44.310
In the product, we
organize our engineering

00:11:44.310 --> 00:11:46.330
in four main themes.

00:11:46.330 --> 00:11:48.000
The first one is usability.

00:11:48.000 --> 00:11:50.580
Overall, it means we want
to be able to get you up

00:11:50.580 --> 00:11:54.480
and running and easily
and fast as possible.

00:11:54.480 --> 00:11:57.270
Now, once you have your
model, basically we

00:11:57.270 --> 00:12:00.690
want to get you executing
that model as fast as possible

00:12:00.690 --> 00:12:03.510
in the hardware
that you care about.

00:12:03.510 --> 00:12:06.998
Then with optimization,
we look at how

00:12:06.998 --> 00:12:08.790
certain things that we
can do to your model

00:12:08.790 --> 00:12:11.120
to make it even
faster and smaller.

00:12:11.120 --> 00:12:13.698
And finally, with all our
documentation efforts,

00:12:13.698 --> 00:12:15.240
we want to give you
all the resources

00:12:15.240 --> 00:12:19.330
that you need so you can get the
most out of our infrastructure.

00:12:19.330 --> 00:12:22.650
So let's start with usability.

00:12:22.650 --> 00:12:24.890
There is different ways
of getting up and running

00:12:24.890 --> 00:12:26.460
in TensorFlow Lite.

00:12:26.460 --> 00:12:28.620
But at the end of the
day, you most likely

00:12:28.620 --> 00:12:30.996
will end up wanting
at some point

00:12:30.996 --> 00:12:34.016
to train the model in
TensorFlow and convert it

00:12:34.016 --> 00:12:36.510
to TensorFlow Lite.

00:12:36.510 --> 00:12:38.550
Flow is logically
very simple, right?

00:12:38.550 --> 00:12:40.410
You train in TensorFlow.

00:12:40.410 --> 00:12:41.910
You get a saved model.

00:12:41.910 --> 00:12:43.770
Then you use a TensorFlow
Lite converter.

00:12:43.770 --> 00:12:45.510
And then you get a
TensorFlow Lite model

00:12:45.510 --> 00:12:49.380
that you can now execute on
different kinds of devices.

00:12:49.380 --> 00:12:53.120
However, there is some
points of failure.

00:12:53.120 --> 00:12:57.440
TensorFlow Lite, we
made the decision

00:12:57.440 --> 00:13:02.930
to take a subset of TensorFlow
ops and optimize those.

00:13:02.930 --> 00:13:05.270
And that means that
basically we don't

00:13:05.270 --> 00:13:10.080
have all the same hundreds of
operations that TensorFlow has.

00:13:10.080 --> 00:13:12.840
The other reason
is some semantics,

00:13:12.840 --> 00:13:15.740
we don't support Git
on TensorFlow Lite,

00:13:15.740 --> 00:13:18.440
like control flow,
which is typically used

00:13:18.440 --> 00:13:21.690
for recurrent neural networks.

00:13:21.690 --> 00:13:27.470
But we've made already strides
trying to basically make

00:13:27.470 --> 00:13:29.130
this easier and better for you.

00:13:29.130 --> 00:13:32.360
So last year, we launched
TensorFlow Select.

00:13:32.360 --> 00:13:34.580
And this basically
means that you

00:13:34.580 --> 00:13:38.810
can execute TensorFlow
ops in TensorFlow Lite.

00:13:38.810 --> 00:13:40.560
In the pipeline, we're
making improvements

00:13:40.560 --> 00:13:43.820
to TensorFlow Select by
enabling selective registration.

00:13:43.820 --> 00:13:47.810
And I will explain what all this
means in the following slides.

00:13:47.810 --> 00:13:51.280
And we're also working on
adding control flow support.

00:13:51.280 --> 00:13:53.570
OK, so select, like
I said, it means

00:13:53.570 --> 00:13:56.180
you can execute TensorFlow
Ops in TensorFlow Lite.

00:13:56.180 --> 00:13:59.660
It comes at the cost of
some binary size increase

00:13:59.660 --> 00:14:02.520
because right now, it's
basically all or nothing.

00:14:02.520 --> 00:14:05.180
But that's why we're building
selective registration.

00:14:05.180 --> 00:14:07.070
Selective registration
is already something

00:14:07.070 --> 00:14:09.230
that you can take
advantage on TensorFlow

00:14:09.230 --> 00:14:12.020
Lite for our building ops.

00:14:12.020 --> 00:14:13.700
So you only include
in the binary

00:14:13.700 --> 00:14:15.350
the ops that you
are really using.

00:14:15.350 --> 00:14:19.970
So you don't end up increasing
your binary size unnecessarily.

00:14:19.970 --> 00:14:22.580
So we're bringing that
to TensorFlow Select.

00:14:22.580 --> 00:14:24.680
And we are also trying
to blur the line of what

00:14:24.680 --> 00:14:27.680
the TensorFlow ops and the
TensorFlow Lite ops are.

00:14:27.680 --> 00:14:31.310
And one of the
key points of this

00:14:31.310 --> 00:14:33.470
is blur the performance
gap that there might

00:14:33.470 --> 00:14:34.730
be between one and the other.

00:14:37.770 --> 00:14:39.920
Now control flow, again,
where it's something

00:14:39.920 --> 00:14:41.610
that we are working on.

00:14:41.610 --> 00:14:45.920
And this is one of the
fairly larger pain points

00:14:45.920 --> 00:14:48.430
where you're trying to convert
recurrent neural network

00:14:48.430 --> 00:14:49.732
to TensorFlow Lite.

00:14:49.732 --> 00:14:51.440
So this is something
that is in progress.

00:14:51.440 --> 00:14:55.750
So we are rallying support
for loops and conditionals.

00:14:55.750 --> 00:14:58.510
And all this is part
of a bigger effort

00:14:58.510 --> 00:15:01.930
to basically to revamp
entirely our converter.

00:15:01.930 --> 00:15:05.110
And we took your
feedback very seriously.

00:15:05.110 --> 00:15:07.750
And we want these converter to
answer these three questions,

00:15:07.750 --> 00:15:08.250
right?

00:15:08.250 --> 00:15:12.460
If something fails, what went
wrong, where it went wrong,

00:15:12.460 --> 00:15:16.800
and what can you do to fix it?

00:15:16.800 --> 00:15:20.250
OK, so jumping to the
next theme is performance.

00:15:20.250 --> 00:15:22.710
We want your models to
execute as fast as possible

00:15:22.710 --> 00:15:25.650
in the hardware that
you have available.

00:15:25.650 --> 00:15:27.360
And if you saw in
[INAUDIBLE] slide,

00:15:27.360 --> 00:15:30.030
we've made huge strides
in this area, right?

00:15:30.030 --> 00:15:33.460
So this an example
of a MobileNet V1.

00:15:33.460 --> 00:15:36.540
The execution in
CPU floating point

00:15:36.540 --> 00:15:39.520
compared to the
quantized version in CPU.

00:15:39.520 --> 00:15:42.750
And now with the recently
developer preview

00:15:42.750 --> 00:15:45.930
that we launched for
our CPU, GPU delegate,

00:15:45.930 --> 00:15:49.930
we get huge gains, like
over 7x improvements.

00:15:49.930 --> 00:15:51.870
And then by using the
Edge TPU delegate,

00:15:51.870 --> 00:15:56.310
we get crazy speed-ups, 62x.

00:15:56.310 --> 00:16:00.870
So again, this is an area where
we're working really hard.

00:16:00.870 --> 00:16:03.180
Again, some things that
are really available, we

00:16:03.180 --> 00:16:07.520
have a very nice benchmark
and profiling tool.

00:16:07.520 --> 00:16:09.420
And we have a few delegates.

00:16:09.420 --> 00:16:11.670
For those of you who are not
familiar with the concept

00:16:11.670 --> 00:16:14.210
of TensorFlow Lite
delegates, basically it's

00:16:14.210 --> 00:16:17.850
our abstraction layer
that allows securing

00:16:17.850 --> 00:16:19.917
models [INAUDIBLE] the CPU.

00:16:19.917 --> 00:16:21.750
So right now, we have
these three delegates,

00:16:21.750 --> 00:16:26.400
like Edge TPU, GPU, and the
neural network API delegate.

00:16:26.400 --> 00:16:28.350
And in the pipeline,
we are working also

00:16:28.350 --> 00:16:32.220
on CPU improvements because
we know that CPU is important.

00:16:32.220 --> 00:16:34.410
I'm not listing here, in
the pipeline, anything

00:16:34.410 --> 00:16:37.930
around newer delegates because
it tends to be a partner

00:16:37.930 --> 00:16:40.660
issue with other companies.

00:16:40.660 --> 00:16:43.050
And I don't have
anything to announce yet.

00:16:43.050 --> 00:16:47.560
But we are working on that.

00:16:47.560 --> 00:16:52.152
OK, our benchmarking tool
is basically a profiler.

00:16:52.152 --> 00:16:53.360
We added some features to it.

00:16:53.360 --> 00:16:55.080
It supports multithreading.

00:16:55.080 --> 00:16:58.890
You can see statistics
per execution.

00:16:58.890 --> 00:17:01.050
And it supports the
neural network API.

00:17:01.050 --> 00:17:02.760
So when you execute
the profiler,

00:17:02.760 --> 00:17:05.262
you get a lot of information.

00:17:05.262 --> 00:17:07.470
This is just a snippet of
all the stuff that you get.

00:17:07.470 --> 00:17:10.260
So in this case,
we're showing you here

00:17:10.260 --> 00:17:12.869
information pair on each one
of the ops in your graph,

00:17:12.869 --> 00:17:15.819
how long it took to execute.

00:17:15.819 --> 00:17:18.160
Then you also get a nice
summary of all the op

00:17:18.160 --> 00:17:21.430
times in your graph and
many useful statistics.

00:17:21.430 --> 00:17:24.490
So you can decide
maybe there is a way

00:17:24.490 --> 00:17:26.859
that you can change one
out for an other type that

00:17:26.859 --> 00:17:29.770
is more efficient or tweak
your graph in some way.

00:17:32.660 --> 00:17:34.250
OK, now let's talk
about the delegate.

00:17:34.250 --> 00:17:36.950
So like I said, the delegate
is our abstraction layer

00:17:36.950 --> 00:17:40.940
for executing graphs
in different hardware.

00:17:40.940 --> 00:17:44.540
And the way it
works is very nice

00:17:44.540 --> 00:17:47.510
because basically in
the TensorFlow Lite,

00:17:47.510 --> 00:17:50.062
you prepare a model, right?

00:17:50.062 --> 00:17:51.770
You are not necessarily
saying this model

00:17:51.770 --> 00:17:52.645
is for this hardware.

00:17:52.645 --> 00:17:54.290
You just prepare your model.

00:17:54.290 --> 00:17:56.600
And then the interpreter
will take it.

00:17:56.600 --> 00:18:00.050
And then for those
delegates that you register,

00:18:00.050 --> 00:18:02.780
it will go, each one by one.

00:18:02.780 --> 00:18:07.910
And they will say, OK, how much
of this graph can you execute?

00:18:07.910 --> 00:18:09.720
And what the delegate
is going to execute,

00:18:09.720 --> 00:18:12.350
they will be executed in CPU.

00:18:12.350 --> 00:18:18.560
So by doing these, basically you
will always be able to execute.

00:18:18.560 --> 00:18:20.780
And if anything, you will
get better performance

00:18:20.780 --> 00:18:24.610
if the delegate can
consume more of your graph.

00:18:24.610 --> 00:18:28.660
So we have already
an Edge TPU delegate.

00:18:28.660 --> 00:18:31.800
Edge TPUs are Google's
ML hardware accelerator

00:18:31.800 --> 00:18:32.850
for the Edge.

00:18:32.850 --> 00:18:34.525
It's very high performance.

00:18:34.525 --> 00:18:35.400
It's also very small.

00:18:35.400 --> 00:18:38.010
And it has a small
power footprint.

00:18:38.010 --> 00:18:41.880
And later on, we'll have more
information about the Edge TPUs

00:18:41.880 --> 00:18:45.520
in a very cool demo.

00:18:45.520 --> 00:18:47.760
The next delegate
is the GPU delegate.

00:18:47.760 --> 00:18:50.880
These are developer
preview right now.

00:18:50.880 --> 00:18:54.330
And we see very nice performance
improvements, 2x to 7x

00:18:54.330 --> 00:18:57.840
compared to floating
point in CPU

00:18:57.840 --> 00:19:02.758
at the cost of our relatively
small binary size increase.

00:19:02.758 --> 00:19:05.360
And how do you enable
the GPU delegate?

00:19:05.360 --> 00:19:07.100
Well, just like
any other delegate,

00:19:07.100 --> 00:19:09.890
this is an example of
the interpreter and just

00:19:09.890 --> 00:19:12.387
executing one inference.

00:19:12.387 --> 00:19:13.970
And the only thing
that you need to do

00:19:13.970 --> 00:19:16.790
to enable, in this
case, the GPU delegate,

00:19:16.790 --> 00:19:19.357
you just instantiate it and
pass it to the interpreter.

00:19:19.357 --> 00:19:19.940
And that's it.

00:19:24.290 --> 00:19:27.440
So stuff that we're working
on for the GPU delegate, well,

00:19:27.440 --> 00:19:28.750
we want to add more ops.

00:19:28.750 --> 00:19:31.030
Right now, most of the
ops in the GPU delegate

00:19:31.030 --> 00:19:33.510
are covering
convolutional networks.

00:19:33.510 --> 00:19:36.350
But obviously, we want to cover
recurrent neural networks,

00:19:36.350 --> 00:19:37.640
for example.

00:19:37.640 --> 00:19:40.270
We want to improve
performance even further.

00:19:40.270 --> 00:19:41.840
And we want to finalize the API.

00:19:41.840 --> 00:19:47.190
So this is generally available,
not just a developer preview.

00:19:47.190 --> 00:19:49.020
The final delegate that
I wanted to mention

00:19:49.020 --> 00:19:51.190
is the Android
neural network API.

00:19:51.190 --> 00:19:56.160
So this will allow you to
execute any other hardware

00:19:56.160 --> 00:19:59.642
that the Android
NN API supports.

00:19:59.642 --> 00:20:01.350
So it should be just
transparent for you.

00:20:04.660 --> 00:20:09.810
Finally, CPU is still very
important for many use cases.

00:20:09.810 --> 00:20:12.660
Most devices have
some form of a CPU.

00:20:12.660 --> 00:20:15.090
So we are targeting
further improvements

00:20:15.090 --> 00:20:19.810
on ARM and x86 architectures.

00:20:19.810 --> 00:20:22.770
OK, so the next theme
is optimization.

00:20:22.770 --> 00:20:26.210
So in the performance
theme, we took a model,

00:20:26.210 --> 00:20:29.940
and we called it in a way
that can execute very fast.

00:20:29.940 --> 00:20:32.810
But maybe we can do
tweaks to your model

00:20:32.810 --> 00:20:36.570
to make it even faster
or even smaller.

00:20:36.570 --> 00:20:39.210
So we already did
some work here.

00:20:39.210 --> 00:20:43.700
So last year, we released this
post-training quantization tool

00:20:43.700 --> 00:20:45.530
that it gives very
nice performance

00:20:45.530 --> 00:20:47.300
improvements on CPU.

00:20:47.300 --> 00:20:49.880
And this is the
first launch that we

00:20:49.880 --> 00:20:52.760
have in what we are calling
a model optimization toolkit.

00:20:52.760 --> 00:20:56.910
So more techniques will
be part of this toolkit

00:20:56.910 --> 00:21:00.000
and available for
use for everybody.

00:21:00.000 --> 00:21:02.120
And in the pipeline, some
of these new techniques

00:21:02.120 --> 00:21:04.040
that we are going to
put in the toolkit

00:21:04.040 --> 00:21:06.710
is more quantization work.

00:21:06.710 --> 00:21:09.350
In these cases, what we call
fixed-point quantization,

00:21:09.350 --> 00:21:12.890
which gives you further
improvements and CPU

00:21:12.890 --> 00:21:14.840
and allows you to
execute in more hardware.

00:21:14.840 --> 00:21:17.160
Because a lot of the
specialized hardware,

00:21:17.160 --> 00:21:21.050
like neural processing units,
are use fixed point base.

00:21:21.050 --> 00:21:23.810
There is no floating
point support.

00:21:23.810 --> 00:21:26.960
We're also excited because
we're working on a new technique

00:21:26.960 --> 00:21:28.410
called connection pruning.

00:21:28.410 --> 00:21:32.990
And I will cover a little
bit of all these things next.

00:21:32.990 --> 00:21:35.420
So quantization
means just changing

00:21:35.420 --> 00:21:37.640
the precision of
your model and how

00:21:37.640 --> 00:21:40.100
it's executed on the device.

00:21:40.100 --> 00:21:42.760
We launched our post-training
quantization tool

00:21:42.760 --> 00:21:46.800
where basically, you
take your saved model.

00:21:46.800 --> 00:21:48.950
And when you convert it
to TensorFlow Lite format,

00:21:48.950 --> 00:21:50.390
you can just turn on a flag.

00:21:50.390 --> 00:21:52.820
And it will be quantized.

00:21:52.820 --> 00:21:54.740
And because it's
on CPU, basically

00:21:54.740 --> 00:21:58.280
we can do a mix of fixed
point and floating point math,

00:21:58.280 --> 00:22:01.940
try to get the most
performance and relatively

00:22:01.940 --> 00:22:04.040
small accuracy loss.

00:22:04.040 --> 00:22:07.820
And we see very nice
improvements this way.

00:22:07.820 --> 00:22:09.890
If the model gets
fully quantized

00:22:09.890 --> 00:22:14.990
and you get 4x compression,
and for convolutional models,

00:22:14.990 --> 00:22:19.320
we see 10% to 50%
performance improvement.

00:22:19.320 --> 00:22:23.360
But for fully connected
RNNs, we see up to 3x.

00:22:23.360 --> 00:22:29.270
So again, this is something
that is extremely simple to try.

00:22:29.270 --> 00:22:31.850
If you already are using
TensorFlow Lite or plan to use

00:22:31.850 --> 00:22:33.860
it, just give this a try.

00:22:33.860 --> 00:22:39.058
Obviously you have to validate
how the accuracy is affected

00:22:39.058 --> 00:22:40.100
in your particular model.

00:22:40.100 --> 00:22:42.560
But so far, we've seen
very good numbers.

00:22:42.560 --> 00:22:46.670
And many of the two billion
devices that we mentioned

00:22:46.670 --> 00:22:48.620
before are running this way.

00:22:51.250 --> 00:22:55.120
In the pipeline, again, we're
making further improvements

00:22:55.120 --> 00:22:58.940
in the quantization
approaches that we have.

00:22:58.940 --> 00:23:01.690
So we're working on this
fixed point quantization.

00:23:01.690 --> 00:23:04.330
And we want to make it
more powerful by enabling

00:23:04.330 --> 00:23:07.730
quantized training and
also very easy to use.

00:23:07.730 --> 00:23:10.300
And that's why we're also
bringing more post-training

00:23:10.300 --> 00:23:12.640
quantization support.

00:23:12.640 --> 00:23:15.645
So this is an example
of Keras model.

00:23:15.645 --> 00:23:16.270
It's very easy.

00:23:16.270 --> 00:23:18.060
You have used two dense layers.

00:23:18.060 --> 00:23:21.540
And now let's say that you
want to train with quantization

00:23:21.540 --> 00:23:23.820
those two dense layers.

00:23:23.820 --> 00:23:29.280
Well, soon enough, you will be
able to just import to our API

00:23:29.280 --> 00:23:31.260
and just say, quantize
those two layers.

00:23:33.780 --> 00:23:35.310
And the post-training
quantization

00:23:35.310 --> 00:23:38.310
will, just like for
fixed point, will

00:23:38.310 --> 00:23:41.250
work very much like the one
that we already support,

00:23:41.250 --> 00:23:44.220
where you have the
conversion path.

00:23:44.220 --> 00:23:46.530
And the only thing that
you need to pass extra

00:23:46.530 --> 00:23:48.670
is some calibration data.

00:23:48.670 --> 00:23:51.780
And calibration data is
just the typical data

00:23:51.780 --> 00:23:53.820
that your model
sees as the input.

00:23:53.820 --> 00:23:56.850
So for example, if you have
an input, an image processing

00:23:56.850 --> 00:24:00.240
model, you just need
to pass some images.

00:24:00.240 --> 00:24:02.910
And you don't really need
to pass a ton of images.

00:24:02.910 --> 00:24:05.610
So in preliminary numbers,
for example, for MobileNets,

00:24:05.610 --> 00:24:10.710
we see with 25 images,
we do very well.

00:24:10.710 --> 00:24:13.170
These are some of the numbers--
again, preliminary numbers

00:24:13.170 --> 00:24:14.910
that we see with this tool.

00:24:14.910 --> 00:24:17.310
We have the float baseline.

00:24:17.310 --> 00:24:19.590
And then if you do
quantized training,

00:24:19.590 --> 00:24:22.440
then you get almost
the same accuracy

00:24:22.440 --> 00:24:24.360
with all the benefits
of quantization,

00:24:24.360 --> 00:24:26.880
of compression and
fast execution.

00:24:26.880 --> 00:24:30.595
And then even further,
if you really--

00:24:30.595 --> 00:24:33.220
for some reason, you don't want
to invest i quantized training,

00:24:33.220 --> 00:24:37.680
or perhaps you just have a model
that you got somewhere, you

00:24:37.680 --> 00:24:40.740
can still quantize it with
post-training quantization.

00:24:40.740 --> 00:24:44.250
And you see there that
basically the accuracy

00:24:44.250 --> 00:24:45.210
is almost the same.

00:24:48.590 --> 00:24:51.230
The other technique that we
are heavily investing right now

00:24:51.230 --> 00:24:52.790
is connection pruning.

00:24:52.790 --> 00:24:55.220
What this means is
that at training time,

00:24:55.220 --> 00:24:57.480
we just prune connections
in the network.

00:24:57.480 --> 00:24:59.450
The pruned connections
becomes [INAUDIBLE]..

00:24:59.450 --> 00:25:03.140
And that creates
just a sparse tensor.

00:25:03.140 --> 00:25:05.540
The nice benefits
of sparse tensors

00:25:05.540 --> 00:25:10.070
is that you can compress them
because you can skip the zeros.

00:25:10.070 --> 00:25:13.010
And you can create
potentially faster models

00:25:13.010 --> 00:25:17.940
if you have less
computations to do.

00:25:17.940 --> 00:25:19.890
So coming very
soon, we are going

00:25:19.890 --> 00:25:26.320
to add a training time pruning,
again, as a Keras-based API.

00:25:26.320 --> 00:25:28.020
And in the pipeline,
we are working

00:25:28.020 --> 00:25:33.160
on first-class support of sparse
execution in TensorFlow Lite.

00:25:33.160 --> 00:25:34.960
So again, in our
example from Keras,

00:25:34.960 --> 00:25:37.950
this has your two dense layers.

00:25:37.950 --> 00:25:41.982
You want to prune connections
from those two layers.

00:25:41.982 --> 00:25:44.565
And then the only thing that you
need to do is just hit Prune.

00:25:44.565 --> 00:25:46.860
So it's very, very simple.

00:25:46.860 --> 00:25:50.730
At that point, when you compare
the model to TensorFlow Lite

00:25:50.730 --> 00:25:55.890
format, basically by using
just even zipping the file,

00:25:55.890 --> 00:25:59.720
you will get a nice compression.

00:25:59.720 --> 00:26:02.967
And these are some, again,
very preliminary numbers.

00:26:02.967 --> 00:26:04.800
In this case, a MobileNet
which are actually

00:26:04.800 --> 00:26:08.490
pretty hard to prune compared
to other perhaps larger models.

00:26:08.490 --> 00:26:11.920
But we see a very small
loss of 50% and even 75%.

00:26:15.700 --> 00:26:18.063
Finally, documentation--
well, again, we

00:26:18.063 --> 00:26:19.480
want to give you
all the resources

00:26:19.480 --> 00:26:23.940
that you need to get the
most out of TensorFlow Lite.

00:26:23.940 --> 00:26:27.130
And we have already done
really nice improvements.

00:26:27.130 --> 00:26:30.170
We have a new site that
is not only just pretty,

00:26:30.170 --> 00:26:31.840
but it has more content.

00:26:31.840 --> 00:26:33.670
We currently have
a model repository

00:26:33.670 --> 00:26:36.870
with five models and
applications that you can try.

00:26:36.870 --> 00:26:39.940
But we want to work, and we
are working on more tutorials.

00:26:39.940 --> 00:26:43.730
And we're also trying to
expand our repository.

00:26:43.730 --> 00:26:45.880
So this is how
the new site looks

00:26:45.880 --> 00:26:48.010
if you haven't taken a look.

00:26:48.010 --> 00:26:50.710
The goal is, again, not
only that it's pretty,

00:26:50.710 --> 00:26:54.070
but it answers your questions.

00:26:54.070 --> 00:26:55.570
So please give us
some more feedback

00:26:55.570 --> 00:26:58.180
if there is things that
are still lacking there.

00:27:00.750 --> 00:27:02.490
Yeah, so we revamped
our documentation.

00:27:02.490 --> 00:27:06.340
And very important, we
also have there the roadmap

00:27:06.340 --> 00:27:08.120
for the foreseeable future.

00:27:08.120 --> 00:27:10.190
So a lot of the stuff that
I'm talking about now,

00:27:10.190 --> 00:27:14.460
you can see it there and
in more detail perhaps.

00:27:14.460 --> 00:27:16.380
And then again, we
have a repository

00:27:16.380 --> 00:27:19.050
with all these
applications and tutorials.

00:27:19.050 --> 00:27:20.790
And we are expanding it.

00:27:20.790 --> 00:27:23.250
We want to make the
repository into something

00:27:23.250 --> 00:27:26.750
that if you don't want to go--

00:27:26.750 --> 00:27:29.310
or before you start
trying to train something,

00:27:29.310 --> 00:27:31.560
perhaps we already have a
model that you can reuse.

00:27:34.550 --> 00:27:38.738
By the way, TensorFlow
Mobile is deprecated,

00:27:38.738 --> 00:27:40.280
especially with the
work that we need

00:27:40.280 --> 00:27:42.050
to bring TensorFlow
Select, which

00:27:42.050 --> 00:27:45.410
allows you to execute TensorFlow
ops in TensorFlow Lite.

00:27:45.410 --> 00:27:49.040
There is no need for
TensorFlow Mobile except if you

00:27:49.040 --> 00:27:50.690
are doing training on-device.

00:27:50.690 --> 00:27:55.242
So that is the reason why
we're keeping that there.

00:27:55.242 --> 00:27:58.640
But yeah, training.

00:27:58.640 --> 00:28:03.370
So if you see, we're building a
lot of the basic infrastructure

00:28:03.370 --> 00:28:07.810
that you will need when you are
trying to prepare a training

00:28:07.810 --> 00:28:11.680
pipeline to execute on-device.

00:28:11.680 --> 00:28:14.590
I don't have anything
to announce now.

00:28:14.590 --> 00:28:17.143
But I just wanted to let you
know that we are working on it

00:28:17.143 --> 00:28:18.310
and thinking a lot about it.

00:28:21.690 --> 00:28:23.340
What about TensorFlow 2.0?

00:28:23.340 --> 00:28:26.990
Well, it should work.

00:28:26.990 --> 00:28:29.310
If you have a saved
model, you should

00:28:29.310 --> 00:28:33.980
be able to convert it
to TensorFlow Lite.

00:28:33.980 --> 00:28:37.410
And before we go,
two last things.

00:28:37.410 --> 00:28:40.430
One is there was a lot
of content in this talk.

00:28:40.430 --> 00:28:42.200
Tomorrow we have some
breakout sessions

00:28:42.200 --> 00:28:44.950
where we're going to cover
TensorFlow Lite in general

00:28:44.950 --> 00:28:48.650
and optimization in particular.

00:28:48.650 --> 00:28:51.310
And also I want to
introduce Pete Warden, who

00:28:51.310 --> 00:28:55.400
is going to talk about
some very cool project.

00:28:55.400 --> 00:28:56.762
PETE WARDEN: Awesome.

00:28:56.762 --> 00:28:59.996
[APPLAUSE]

00:29:00.920 --> 00:29:03.700
So thanks so much, Raziel.

00:29:03.700 --> 00:29:07.930
And I'm really
excited to be here

00:29:07.930 --> 00:29:12.980
to talk about a new project
that I think is pretty cool.

00:29:12.980 --> 00:29:15.550
So TensorFlow Lite
for microcontrollers.

00:29:15.550 --> 00:29:18.380
What's that all about?

00:29:18.380 --> 00:29:21.850
So this all comes back to when
I actually first joined Google

00:29:21.850 --> 00:29:23.570
back in 2014.

00:29:23.570 --> 00:29:25.570
And as you can imagine,
there were a whole bunch

00:29:25.570 --> 00:29:27.910
of internal projects
that I didn't actually

00:29:27.910 --> 00:29:30.340
know about as a member
of the public that

00:29:30.340 --> 00:29:31.840
sort of blew my mind.

00:29:31.840 --> 00:29:35.680
But one in particular came about
when I actually spoke to Raziel

00:29:35.680 --> 00:29:36.610
for the first time.

00:29:36.610 --> 00:29:38.150
And he explained.

00:29:38.150 --> 00:29:40.360
And he was on the speech
team at the time working

00:29:40.360 --> 00:29:43.000
with Alex, who you just saw.

00:29:43.000 --> 00:29:45.310
And he explained that
they use network models

00:29:45.310 --> 00:29:49.690
of only 13 kilobytes in size.

00:29:49.690 --> 00:29:53.560
At that time, I only really had
experience with image networks.

00:29:53.560 --> 00:29:57.560
And the very smallest of them
was still multiple megabytes.

00:29:57.560 --> 00:30:00.040
So this idea of having
a 13 kilobyte model

00:30:00.040 --> 00:30:03.190
was just amazing for me.

00:30:03.190 --> 00:30:05.740
And what amazed me
even more was when

00:30:05.740 --> 00:30:09.520
he told me why these
models had to be so small.

00:30:09.520 --> 00:30:11.020
They were running
them-- they needed

00:30:11.020 --> 00:30:13.810
to run them on these DSPs
and other embedded chips

00:30:13.810 --> 00:30:14.650
in smartphones.

00:30:14.650 --> 00:30:18.250
So Android could listen out
for wake words like, hey,

00:30:18.250 --> 00:30:21.340
Google, while the main
CPU was powered off

00:30:21.340 --> 00:30:23.090
to save the battery.

00:30:23.090 --> 00:30:24.520
These microcontrollers
often only

00:30:24.520 --> 00:30:28.550
had tens of kilobytes of
RAM and flash storage.

00:30:28.550 --> 00:30:31.060
So they simply couldn't
fit anything larger.

00:30:31.060 --> 00:30:33.880
They also couldn't rely
on cloud connectivity

00:30:33.880 --> 00:30:37.000
because the amount of power that
would have been drained just

00:30:37.000 --> 00:30:39.490
keeping a radio connection
alive to send data over

00:30:39.490 --> 00:30:43.320
would have just
been prohibitive.

00:30:43.320 --> 00:30:48.860
So that really struck
me, that conversation

00:30:48.860 --> 00:30:51.500
and the continued work that
we did with the speech team,

00:30:51.500 --> 00:30:54.710
because they had
so much experience

00:30:54.710 --> 00:30:58.190
doing all sorts of different
approaches with speech.

00:30:58.190 --> 00:31:01.730
They'd spent a lot of time and
a lot of energy experimenting.

00:31:01.730 --> 00:31:03.470
And even within the
tough constraints

00:31:03.470 --> 00:31:06.860
of these embedded
devices, neural networks

00:31:06.860 --> 00:31:11.780
were better than any of the
traditional methods they used.

00:31:11.780 --> 00:31:13.220
So I was left
wondering if they'd

00:31:13.220 --> 00:31:16.670
be really useful for other
embedded sensor applications

00:31:16.670 --> 00:31:17.820
as well.

00:31:17.820 --> 00:31:20.960
And it left me really wanting
to see if we could actually

00:31:20.960 --> 00:31:23.540
build support for
these kind of devices

00:31:23.540 --> 00:31:29.810
into TensorFlow itself so that
more people could actually

00:31:29.810 --> 00:31:30.920
get access.

00:31:30.920 --> 00:31:34.492
At the time, only people
in the speech community

00:31:34.492 --> 00:31:36.200
really knew about the
groundbreaking work

00:31:36.200 --> 00:31:37.380
that was being done.

00:31:37.380 --> 00:31:41.460
So I really wanted to
share it a lot more widely.

00:31:41.460 --> 00:31:46.380
So today, I am
pleased to announce

00:31:46.380 --> 00:31:49.620
that we are releasing the
first experimental support

00:31:49.620 --> 00:31:52.890
for embedded platforms
in TensorFlow Lite.

00:31:52.890 --> 00:31:54.900
And to show you
what I mean, here

00:31:54.900 --> 00:32:00.690
is a demonstration board that
I actually have in my pocket.

00:32:00.690 --> 00:32:04.170
And this is a prototype
of a development

00:32:04.170 --> 00:32:06.330
board built by SparkFun.

00:32:06.330 --> 00:32:12.780
And it has a Cortex-M4 processor
with 384 kilobytes of RAM

00:32:12.780 --> 00:32:16.290
and a whole megabyte
of flash storage.

00:32:16.290 --> 00:32:19.980
And it was built by Ambiq
to be extremely low power,

00:32:19.980 --> 00:32:23.890
drawing less than one
milliwatt in a lot of cases.

00:32:23.890 --> 00:32:28.800
So it's able to run on a
single coin battery like this

00:32:28.800 --> 00:32:32.130
for many days, potentially.

00:32:32.130 --> 00:32:34.680
And I'm actually going to
take my life in my hands

00:32:34.680 --> 00:32:38.632
now by trying a live demo.

00:32:38.632 --> 00:32:40.600
[LAUGHS]

00:32:40.600 --> 00:32:44.220
So let us see if
this is actually--

00:32:47.590 --> 00:32:51.190
it's going to be
extremely hard to see,

00:32:51.190 --> 00:32:53.360
unless we dim the lights.

00:32:53.360 --> 00:32:55.820
There we go.

00:32:55.820 --> 00:32:58.060
So what I'm going
to be doing here

00:32:58.060 --> 00:33:02.980
is by saying a particular word
and see if it actually lights

00:33:02.980 --> 00:33:04.960
up the little yellow light.

00:33:04.960 --> 00:33:06.770
You can see the blue
LED is flashing.

00:33:06.770 --> 00:33:09.140
That's just telling me that
it's running [INAUDIBLE]..

00:33:09.140 --> 00:33:18.410
So if I try saying,
yes, yes, yes.

00:33:18.410 --> 00:33:21.560
I knew I was taking my
life into my hands here.

00:33:21.560 --> 00:33:23.250
Yes.

00:33:23.250 --> 00:33:23.760
There we go.

00:33:23.760 --> 00:33:24.710
[LAUGHS]

00:33:24.710 --> 00:33:26.592
[APPLAUSE]

00:33:26.592 --> 00:33:31.800
So I'm going to quickly move
that out of the spotlight.

00:33:31.800 --> 00:33:36.280
So as you can see, it's
still far from perfect.

00:33:36.280 --> 00:33:40.290
But it is managing to do
a job of recognizing when

00:33:40.290 --> 00:33:42.480
I say the word and not
lighting up when there's

00:33:42.480 --> 00:33:45.630
unrelated conversations.

00:33:45.630 --> 00:33:49.030
So why is this useful?

00:33:49.030 --> 00:33:52.410
Well, first this is
running entirely locally

00:33:52.410 --> 00:33:53.760
on the embedded chip.

00:33:53.760 --> 00:33:57.480
So we don't need to have
any internet connection.

00:33:57.480 --> 00:34:01.020
So it's a good useful first
component of a voice interface

00:34:01.020 --> 00:34:01.950
system.

00:34:01.950 --> 00:34:05.820
And the model itself
isn't quite 13 kilobytes.

00:34:05.820 --> 00:34:08.909
But it is down to 20 kilobytes.

00:34:08.909 --> 00:34:11.219
So it only takes up 20
kilobytes of flash storage

00:34:11.219 --> 00:34:12.389
on this device.

00:34:12.389 --> 00:34:14.639
And the footprint of
the TensorFlow Lite

00:34:14.639 --> 00:34:19.949
code for microcontrollers is
only another 25 kilobytes.

00:34:19.949 --> 00:34:22.800
And it only needs about
30 kilobytes of RAM

00:34:22.800 --> 00:34:24.480
available to operate.

00:34:24.480 --> 00:34:27.460
So it's within the
capabilities of a lot

00:34:27.460 --> 00:34:28.710
of different embedded devices.

00:34:32.429 --> 00:34:36.989
Secondly, this is
all open source.

00:34:36.989 --> 00:34:42.050
So you can actually
grab the code yourself

00:34:42.050 --> 00:34:44.270
and build it yourself.

00:34:44.270 --> 00:34:46.699
And you can modify.

00:34:46.699 --> 00:34:48.889
I'm showing you here on
this particular platform.

00:34:48.889 --> 00:34:51.440
But it actually works
on a whole bunch

00:34:51.440 --> 00:34:53.719
of different embedded chips.

00:34:53.719 --> 00:34:57.800
And we really want to
see lots more supported.

00:34:57.800 --> 00:35:00.110
So we're keen to work
with the community

00:35:00.110 --> 00:35:06.570
on collaborating to get
more devices supported.

00:35:06.570 --> 00:35:10.340
You can also train
your own model.

00:35:10.340 --> 00:35:13.980
Just something that recognizes
yes isn't all that useful.

00:35:13.980 --> 00:35:18.980
But the key thing is that
this comes with a tutorial

00:35:18.980 --> 00:35:22.760
that you can use to actually
train your own models.

00:35:22.760 --> 00:35:25.310
And it also comes
with a data set

00:35:25.310 --> 00:35:30.290
of 100,000 utterances
of about 20 common words

00:35:30.290 --> 00:35:32.060
that you use as
your training set.

00:35:32.060 --> 00:35:35.150
And that first link
there, the AIY Projects

00:35:35.150 --> 00:35:38.660
one, if you could
actually go to that link

00:35:38.660 --> 00:35:43.610
and contribute your voice
to the open data set,

00:35:43.610 --> 00:35:46.610
it should actually
increase the size

00:35:46.610 --> 00:35:49.130
and the quality of the data
set that we can actually

00:35:49.130 --> 00:35:50.180
make available.

00:35:50.180 --> 00:35:51.260
So that would be awesome.

00:35:51.260 --> 00:35:54.110
And you can actually
use the same approach

00:35:54.110 --> 00:35:56.390
to do a lot of different
audio recognition

00:35:56.390 --> 00:35:58.520
to recognize different
kinds of sounds

00:35:58.520 --> 00:36:02.270
and even start to use it for
similar signal processing

00:36:02.270 --> 00:36:07.580
problems, things like
predictive maintenance.

00:36:07.580 --> 00:36:11.120
So how can you try
this out for yourself?

00:36:11.120 --> 00:36:15.040
If you're in the audience
here, at the end of today,

00:36:15.040 --> 00:36:18.020
you will find that
you get a gift box,

00:36:18.020 --> 00:36:21.190
and you actually have
one of these in there.

00:36:21.190 --> 00:36:24.480
[APPLAUSE]

00:36:27.300 --> 00:36:31.650
And all you should need to
do is remove the little tab

00:36:31.650 --> 00:36:32.600
between the battery.

00:36:32.600 --> 00:36:34.330
And it should
automatically boot up

00:36:34.330 --> 00:36:37.590
preflashed with
this yes example.

00:36:37.590 --> 00:36:39.930
[LAUGHTER]

00:36:39.930 --> 00:36:41.403
So you can try it
out for yourself.

00:36:41.403 --> 00:36:42.570
And let me know how it goes.

00:36:42.570 --> 00:36:47.150
Just say, yes, the
TensorFlow I liked, is the--

00:36:47.150 --> 00:36:48.760
and we also include
all the cables.

00:36:48.760 --> 00:36:51.360
So you should be able to
just program it yourself

00:36:51.360 --> 00:36:52.830
through the serial port.

00:36:52.830 --> 00:36:56.850
Now these are the first
700 boards ever built.

00:36:56.850 --> 00:36:59.140
So there is a wiring issue.

00:36:59.140 --> 00:37:01.050
So it will drain the battery.

00:37:01.050 --> 00:37:04.450
It would last more
like hours than days.

00:37:04.450 --> 00:37:07.420
But that will actually--

00:37:07.420 --> 00:37:12.150
knock on wood-- be fixed in the
final product that's shipping.

00:37:12.150 --> 00:37:13.560
And you should be
able to develop

00:37:13.560 --> 00:37:15.450
with these in the
exact same way that you

00:37:15.450 --> 00:37:18.550
will with the final
shipping product.

00:37:18.550 --> 00:37:21.000
And if you're
watching at home, you

00:37:21.000 --> 00:37:24.660
could preorder one of
these from SparkFun

00:37:24.660 --> 00:37:28.020
right now for, I
think, it's $15.

00:37:28.020 --> 00:37:32.820
And you'll also find lots
of other instructions

00:37:32.820 --> 00:37:36.220
for other platforms
in the documentation.

00:37:36.220 --> 00:37:38.940
So we are trying to
support all of the--

00:37:38.940 --> 00:37:42.620
or as many of the
modern microcontrollers

00:37:42.620 --> 00:37:45.990
that are out there that
people are using as possible.

00:37:45.990 --> 00:37:49.230
And we welcome
collaboration with everybody

00:37:49.230 --> 00:37:53.670
across the community to help
unlock all of the creativity

00:37:53.670 --> 00:37:55.170
that I know is out there.

00:37:55.170 --> 00:37:56.670
And I'm really
hoping that I'm going

00:37:56.670 --> 00:37:59.160
to be spending a lot of my
time over the next few months

00:37:59.160 --> 00:38:00.240
reviewing pull requests.

00:38:03.060 --> 00:38:07.320
And finally, this was my
first hardware project.

00:38:07.320 --> 00:38:10.680
So I needed a lot of
help from a lot of people

00:38:10.680 --> 00:38:13.930
to actually help bring
this prototype together,

00:38:13.930 --> 00:38:18.270
including the TFLite team,
especially Raziel, Rocky, Dan,

00:38:18.270 --> 00:38:20.040
Tim, and Andy.

00:38:20.040 --> 00:38:26.250
Alasdair, Nathan, Owen, and Jim
at SparkFun were lifesavers.

00:38:26.250 --> 00:38:30.420
We literally got these in
our hands middle of the day

00:38:30.420 --> 00:38:32.218
yesterday.

00:38:32.218 --> 00:38:34.260
So the fact that they
managed to pull it together

00:38:34.260 --> 00:38:36.000
is a massive tribute.

00:38:36.000 --> 00:38:40.350
And also Scott, Steve,
Arpit, and Andre

00:38:40.350 --> 00:38:44.610
at Ambiq, who actually designed
this process and helped us

00:38:44.610 --> 00:38:45.870
get the software going.

00:38:45.870 --> 00:38:48.990
And actually, a lot of people
at Arm as well, including a big

00:38:48.990 --> 00:38:50.880
shout out to Neil and Zach.

00:38:50.880 --> 00:38:54.450
So this is still a
very early experiment.

00:38:54.450 --> 00:38:58.420
But I really can't wait to see
what people build with this.

00:38:58.420 --> 00:39:03.480
And one final note, I will
be around to talk about MCUs

00:39:03.480 --> 00:39:06.150
with anybody who's interested
at the breakout session on day

00:39:06.150 --> 00:39:07.273
two.

00:39:07.273 --> 00:39:09.523
So I'm really looking forward
to chatting to everyone.

00:39:09.523 --> 00:39:10.023
Thank you.

00:39:10.023 --> 00:39:13.307
[APPLAUSE]

00:39:16.072 --> 00:39:17.280
RAZIEL ALVAREZ: Thanks, Pete.

00:39:17.280 --> 00:39:19.663
We really hope
that you try this.

00:39:19.663 --> 00:39:20.580
It's the early stages.

00:39:20.580 --> 00:39:25.110
But you see the huge effort
just to make this happen.

00:39:25.110 --> 00:39:29.580
We think that it will be
really impactful for everybody.

00:39:29.580 --> 00:39:31.920
Now before we go
again, and I promise

00:39:31.920 --> 00:39:34.350
is the last thing
you hear from me,

00:39:34.350 --> 00:39:39.990
I want to welcome June,
who's going to talk about how

00:39:39.990 --> 00:39:43.710
by using TensorFlow Lite,
with the Edge TPU delegate,

00:39:43.710 --> 00:39:47.268
are able to train these
teachable machines.

00:39:47.268 --> 00:39:50.740
[MUSIC PLAYING]

00:39:52.228 --> 00:39:55.700
[APPLAUSE]

00:39:55.700 --> 00:39:59.670
JUNE TATE-GANS: Thanks, Raziel.

00:39:59.670 --> 00:40:01.590
Hi, my name is June Tate-Gans.

00:40:01.590 --> 00:40:03.840
I'm actually one of the lead
software engineers inside

00:40:03.840 --> 00:40:05.428
of Google's new Coral group.

00:40:05.428 --> 00:40:07.470
And I've been asked to
give a talk about the Edge

00:40:07.470 --> 00:40:11.540
TPU-based teachable
machine demo.

00:40:11.540 --> 00:40:14.210
So first, I should
tell you what Coral is.

00:40:14.210 --> 00:40:17.840
Coral is a platform for products
with on-device machine learning

00:40:17.840 --> 00:40:20.420
using TensorFlow and TFLite.

00:40:20.420 --> 00:40:23.630
Our first two products are
a single-board computer

00:40:23.630 --> 00:40:24.898
and a USB stack.

00:40:27.650 --> 00:40:30.020
So what is the Edge TPU?

00:40:30.020 --> 00:40:33.410
It's a Google-designed ASIC that
accelerates inference directly

00:40:33.410 --> 00:40:35.270
on the device that
it's embedded in.

00:40:35.270 --> 00:40:39.680
Its very fast, localizes data
to the edge rather than cloud,

00:40:39.680 --> 00:40:42.150
doesn't require a network
connection to run,

00:40:42.150 --> 00:40:44.697
and this allows for a whole
new range of applications

00:40:44.697 --> 00:40:45.530
of machine learning.

00:40:50.750 --> 00:40:56.080
So the first product we
built is the Coral Dev Board.

00:40:56.080 --> 00:40:59.300
This is a single-board
computer with a removable SOM.

00:40:59.300 --> 00:41:01.420
It runs Linux and Android.

00:41:01.420 --> 00:41:04.960
And the SOM itself has a
gigabyte of RAM, a quad core

00:41:04.960 --> 00:41:11.645
A53 SoC, Wi-Fi and Bluetooth,
and of course, the Edge TPU.

00:41:11.645 --> 00:41:14.650
Now the second is our
Coral Accelerator board.

00:41:14.650 --> 00:41:18.863
Now this board is just the
Edge TPU connected via USB-C

00:41:18.863 --> 00:41:21.280
to whatever development system
you need, be it a Raspberry

00:41:21.280 --> 00:41:23.790
Pi or a Linux workstation.

00:41:26.910 --> 00:41:30.583
Now this teachable machine shows
off a form of edge training.

00:41:30.583 --> 00:41:33.000
Now traditionally, there's
three ways to do edge training.

00:41:33.000 --> 00:41:37.200
There's K-nearest neighbors,
weight imprinting,

00:41:37.200 --> 00:41:40.220
and last layer retraining.

00:41:40.220 --> 00:41:41.880
But for this demo,
we're actually

00:41:41.880 --> 00:41:43.770
using the K-nearest
neighbors approach.

00:41:46.710 --> 00:41:49.970
So in this animated GIF, you
can see that the TPU enables

00:41:49.970 --> 00:41:51.975
very high classification rates.

00:41:51.975 --> 00:41:53.600
The frame rate you
see here is actually

00:41:53.600 --> 00:41:55.700
the rate at which the
TPU is classifying

00:41:55.700 --> 00:41:57.227
the images that I'm showing it.

00:41:57.227 --> 00:41:59.060
In this case, you can
see that we're getting

00:41:59.060 --> 00:42:00.900
about 30 frames per second.

00:42:00.900 --> 00:42:03.004
It's essentially
real-time classification.

00:42:05.790 --> 00:42:08.280
And with that, I actually have
one of our teachable machine

00:42:08.280 --> 00:42:09.085
demos here.

00:42:11.880 --> 00:42:14.240
So if we can turn this on--

00:42:14.240 --> 00:42:15.980
there we go.

00:42:15.980 --> 00:42:24.500
OK, so on this board, we have
our Edge TPU development board

00:42:24.500 --> 00:42:27.565
assembled with a camera
and a series of buttons.

00:42:27.565 --> 00:42:29.690
Now each button corresponds
with a class and lights

00:42:29.690 --> 00:42:34.065
up when the model identifies
an object from the camera.

00:42:34.065 --> 00:42:35.482
But first we have
to plug this in.

00:42:39.100 --> 00:42:42.260
Now every time I take
a picture, by pressing

00:42:42.260 --> 00:42:44.420
one of these buttons, it
associates that picture

00:42:44.420 --> 00:42:46.400
with that particular class.

00:42:46.400 --> 00:42:49.270
And because it's running
inference on the Edge TPU,

00:42:49.270 --> 00:42:51.360
it lights up immediately.

00:42:51.360 --> 00:42:55.020
So once it's finished booting,
the first thing I have to do

00:42:55.020 --> 00:42:56.640
is train it on the background.

00:42:56.640 --> 00:42:58.230
So I'll press this blue button.

00:42:58.230 --> 00:43:00.090
And you can see it
immediately turns on.

00:43:00.090 --> 00:43:04.260
This is because, again, it's
doing inference in real time.

00:43:04.260 --> 00:43:09.400
Now, if I train one of the
other buttons using something

00:43:09.400 --> 00:43:15.790
like a tangerine,
press it a few times--

00:43:15.790 --> 00:43:21.580
OK, so now you can see, it can
classify between this tangerine

00:43:21.580 --> 00:43:22.660
and the background.

00:43:22.660 --> 00:43:27.460
And further, I can even
grab other objects,

00:43:27.460 --> 00:43:29.592
such as this TFLite sticker.

00:43:29.592 --> 00:43:30.800
It looks very similar, right?

00:43:30.800 --> 00:43:33.390
It's the same color.

00:43:33.390 --> 00:43:35.280
So let's see.

00:43:35.280 --> 00:43:36.650
What was the class I used?

00:43:36.650 --> 00:43:38.480
Yellow, OK.

00:43:38.480 --> 00:43:41.170
Sorry.

00:43:41.170 --> 00:43:43.420
So now, even though
it's a similar color,

00:43:43.420 --> 00:43:46.990
it can still discern
the TensorFlow Lite logo

00:43:46.990 --> 00:43:48.070
from the tangerine.

00:43:48.070 --> 00:43:49.810
Oh, sorry-- tangerine.

00:43:49.810 --> 00:43:52.102
There you go.

00:43:52.102 --> 00:43:55.588
[APPLAUSE]

00:43:59.080 --> 00:44:01.650
So you can imagine, in
a manufacturing context,

00:44:01.650 --> 00:44:03.930
your operators, with
no knowledge of machine

00:44:03.930 --> 00:44:06.150
learning or training
in machine learning,

00:44:06.150 --> 00:44:08.250
can adapt your system
easily and quickly

00:44:08.250 --> 00:44:10.480
using this exact technique.

00:44:10.480 --> 00:44:12.360
So that's about it for the demo.

00:44:12.360 --> 00:44:15.210
But before I go, I
should grab the clicker.

00:44:15.210 --> 00:44:20.460
And also I should say we're
also giving away some Edge TPU

00:44:20.460 --> 00:44:21.990
accelerators.

00:44:21.990 --> 00:44:23.700
For those of you
here today, we'll

00:44:23.700 --> 00:44:25.560
have one available
for you as well.

00:44:25.560 --> 00:44:27.300
And for those of you
on the live stream,

00:44:27.300 --> 00:44:31.860
you can purchase one at
coral.withgoogle.com.

00:44:31.860 --> 00:44:33.060
[APPLAUSE]

00:44:33.060 --> 00:44:34.560
OK.

00:44:34.560 --> 00:44:37.910
[MUSIC PLAYING]

