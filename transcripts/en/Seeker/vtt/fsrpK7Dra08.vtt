WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:03.769
Scientists have invented a machine that can
detect emotions even if someone’s expression

00:00:03.769 --> 00:00:04.910
doesn’t change.

00:00:04.910 --> 00:00:11.599
Can you read what I’m feeling?

00:00:11.599 --> 00:00:13.280
Hey everyone, Amy here for DNews.

00:00:13.280 --> 00:00:16.290
Part of what makes us human is that we experience
emotions.

00:00:16.290 --> 00:00:19.790
But it’s still really hard to know what
other people are feeling, especially when

00:00:19.790 --> 00:00:20.810
they’re trying to hide it.

00:00:20.810 --> 00:00:24.410
So, what if machines could read and interpret
people’s emotions for us?

00:00:24.410 --> 00:00:28.170
Well, scientists at MIT have thought of that,
and have developed a device that can detect

00:00:28.170 --> 00:00:30.550
how you’re feeling without even looking
at your face.

00:00:30.550 --> 00:00:34.930
See, we display emotions not just through
facial expressions and body language, but

00:00:34.930 --> 00:00:39.100
with our entire physiology, things like our
heart rate, breathing, blood pressure, and

00:00:39.100 --> 00:00:40.100
sweat.

00:00:40.100 --> 00:00:43.879
These are the exact changes that lie detectors
look for when you’re hooked up to a polygraph.

00:00:43.879 --> 00:00:47.969
But polygraphs require being physically plugged
in, which is an awfully cumbersome way to

00:00:47.969 --> 00:00:49.760
get a read on someone’s mental state.

00:00:49.760 --> 00:00:52.980
That’s where MIT’s device, dubbed “EQ-Radio”,
comes in.

00:00:52.980 --> 00:00:57.350
It works by bouncing radio waves off a person
and analyzing how those waves are affected

00:00:57.350 --> 00:01:02.120
by the rising and falling of their chest or
the vibrations of the pulse under their skin.

00:01:02.120 --> 00:01:05.760
With this information, it can measure the
time between heartbeats and breaths, and notice

00:01:05.760 --> 00:01:07.890
when they’re speeding up or slowing down.

00:01:07.890 --> 00:01:12.360
The device then uses an algorithm to determine
how the data matches up with different emotional

00:01:12.360 --> 00:01:13.540
states.

00:01:13.540 --> 00:01:18.010
Researchers trained to recognize the emotions
of joy, pleasure, sadness and anger in 12

00:01:18.010 --> 00:01:22.880
subjects, and it was able to identify these
emotions correctly 87% of the time.

00:01:22.880 --> 00:01:28.140
And when it was shown a new subject, it could
guess how they were feeling more than 73%

00:01:28.140 --> 00:01:29.140
of the time.

00:01:29.140 --> 00:01:32.460
The scientists think this technology could
be used for things like market research, like

00:01:32.460 --> 00:01:35.290
telling a production company what you’re
feeling when you watch their movie.

00:01:35.290 --> 00:01:39.640
It could also help monitor people’s emotional
states for medical reasons, such as observing

00:01:39.640 --> 00:01:41.130
people with depression.

00:01:41.130 --> 00:01:44.790
But let’s not forget about facial expressions,
because the technology for reading faces is

00:01:44.790 --> 00:01:45.980
also getting better.

00:01:45.980 --> 00:01:50.880
5 years ago, Google’s algorithms could pick
out faces less than 16% of the time, which

00:01:50.880 --> 00:01:52.200
isn’t very impressive.

00:01:52.200 --> 00:01:56.390
Now software is so good at seeing and identifying
faces, apps on your phone can automatically

00:01:56.390 --> 00:02:03.440
swap them.

00:02:03.440 --> 00:02:07.050
Computers can measure the details of your
face remarkably well, and by analyzing the

00:02:07.050 --> 00:02:11.220
relationship of certain points on your face,
they can guess how you feel with surprising

00:02:11.220 --> 00:02:12.319
accuracy.

00:02:12.319 --> 00:02:17.170
According to a 2014 study published in the
Proceedings of the National Academy of Sciences,

00:02:17.170 --> 00:02:21.280
researchers at The Ohio State University created
a program that could identify not just the

00:02:21.280 --> 00:02:25.890
six basic emotions of happiness, sadness,
fear, anger, surprise, and disgust, but a

00:02:25.890 --> 00:02:30.780
total of 21 complex emotions, things like
“happy surprise” or “angry fear”.

00:02:30.780 --> 00:02:35.459
They took 5,000 photographs of 230 volunteers
reacting to different scenarios.

00:02:35.459 --> 00:02:39.719
After pinpointing a variety of facial muscles
that are used to express emotion, the software

00:02:39.719 --> 00:02:45.739
was able to identify basic emotions 97% of
the time and complex emotions almost 77% of

00:02:45.739 --> 00:02:46.739
the time.

00:02:46.739 --> 00:02:50.659
This could help us diagnose and understand
people with autism, who have trouble expressing

00:02:50.659 --> 00:02:54.670
emotions, or people with post-traumatic stress
disorder, and it may even allow us to learn

00:02:54.670 --> 00:02:57.890
what’s happening in their brains when they
feel complex emotions.

00:02:57.890 --> 00:03:02.319
So, computers that know how you feel might
just improve medical science, even if it is

00:03:02.319 --> 00:03:03.319
a little creepy.

00:03:03.319 --> 00:03:08.569
Oh no, now they know I’m uncomfortable and
it’s just making me more uncomfortable...

00:03:08.569 --> 00:03:12.000
These algorithms that read your emotions wouldn’t
be possible without machine learning, the

00:03:12.000 --> 00:03:13.799
first step towards Artificial Intelligence.

00:03:13.799 --> 00:03:18.010
To learn why you shouldn’t panic about HAL
9000 just yet, check out Trace’s video on

00:03:18.010 --> 00:03:19.040
AI here.

00:03:19.040 --> 00:03:22.459
Do you see emotion recognition as a useful
tool or does it give you the heebee jeebees?

00:03:22.459 --> 00:03:23.999
Let us know in the comments below.

00:03:23.999 --> 00:03:27.029
Be sure to like this video and subscribe to
you never miss an episode of DNews.

