WEBVTT

1
00:00:00.570 --> 00:00:01.171
In this video,

2
00:00:01.171 --> 00:00:06.171
we'll see a very simple and common way of taking a word and representing it into

3
00:00:06.541 --> 00:00:11.220
a vector aerial form that we can feed to any machine learning algorithm.
Uh,

4
00:00:11.280 --> 00:00:14.250
this vectorial form is known as the one hot encoding.

5
00:00:17.190 --> 00:00:21.930
So in the previous video we saw some preprocessing steps for converting words

6
00:00:21.931 --> 00:00:26.400
into Integer Ids.
Um,
now these integer ids,
uh,

7
00:00:26.520 --> 00:00:28.620
they're easy to manipulate their numerical,

8
00:00:28.700 --> 00:00:33.150
so that's easy for a computer to manipulate and do numerical computations with

9
00:00:33.151 --> 00:00:36.840
it.
However,
we cannot really feed this,
uh,

10
00:00:36.900 --> 00:00:40.740
this id to a machine learning algorithm.
Um,

11
00:00:40.980 --> 00:00:45.030
I'll discuss a bit more about why that is at a,

12
00:00:45.120 --> 00:00:46.830
at the end of the slide.
Uh,

13
00:00:47.160 --> 00:00:52.160
but let me first introduce another representation that uses this word id and

14
00:00:52.351 --> 00:00:53.850
converts it into a vector,

15
00:00:53.851 --> 00:00:58.770
which is going to be a better representation to start with if want to do some

16
00:00:58.771 --> 00:01:01.170
machine learning on top of a words.

17
00:01:01.680 --> 00:01:05.280
So the one hot encoding,
uh,
like I said,

18
00:01:05.281 --> 00:01:09.410
it's just a basic representation for representing a word into effector.

19
00:01:10.370 --> 00:01:14.760
It's using,
uh,
the,
this one hot encoding approach,

20
00:01:14.970 --> 00:01:19.440
which simply corresponds to a,
taking a vector,

21
00:01:19.441 --> 00:01:20.970
fending it with Zeros,

22
00:01:21.390 --> 00:01:26.390
and then putting a one only at the position corresponding to the ID of the word.

23
00:01:28.050 --> 00:01:31.650
So let's do an example.
Uh,
if we had a vocabulary of size 10,

24
00:01:31.740 --> 00:01:33.410
and by that I mean,
uh,

25
00:01:33.510 --> 00:01:37.620
that are nine words in the vocabulary plus the,
uh,

26
00:01:37.650 --> 00:01:42.360
out of vocabulary token,
uh,
that we added,
which corresponds to the 10th,

27
00:01:42.450 --> 00:01:44.550
uh,
uh,
id.

28
00:01:45.240 --> 00:01:50.240
So then the one hot vector for the one on the coding vector for a word id,

29
00:01:51.270 --> 00:01:54.540
which is four would be the following.

30
00:01:54.541 --> 00:01:57.000
It would be a vector where everything is zero,

31
00:01:57.030 --> 00:02:01.380
except the element at the position four.
So one,
two,
three,
four.

32
00:02:01.381 --> 00:02:05.580
We see we have a one here.
I'm going to note that e of w.

33
00:02:05.581 --> 00:02:10.581
So W is for an e four is going to be this huge factor with just a single one in

34
00:02:11.071 --> 00:02:16.071
it at the position for what's Nice when the 100 and coding and 100 factor is

35
00:02:17.220 --> 00:02:22.150
that it actually makes no assumptions about the similarity of the words.
Uh,

36
00:02:22.200 --> 00:02:26.430
specifically,
if we're looking at the,
uh,
square,

37
00:02:26.431 --> 00:02:29.770
the Euclidean distance between the,
uh,
uh,

38
00:02:30.000 --> 00:02:34.050
the representation of a word w in the word ww prime.

39
00:02:34.500 --> 00:02:37.990
So if you take the difference and then look at the,
uh,

40
00:02:38.040 --> 00:02:41.940
Euclidean norm of that difference,
it's going to be zero.

41
00:02:42.000 --> 00:02:46.560
IfW and w prime are the same,
that is the corresponding exactly the same word.

42
00:02:47.010 --> 00:02:51.100
And otherwise,
uh,
by simple calculation,
you'll see that,
uh,

43
00:02:51.210 --> 00:02:55.740
we get a distance of two if w is not w prime.

44
00:02:55.741 --> 00:02:58.890
So if they actually correspond to different words in the vocabulary.

45
00:02:59.440 --> 00:03:04.270
So in other words,
all words ICWA,
Lee,
different under that representation.

46
00:03:04.810 --> 00:03:08.260
So that's a much more natural representation to start with.

47
00:03:08.261 --> 00:03:11.650
And not making any assumption about which forms are more similar to which other

48
00:03:11.651 --> 00:03:16.540
words,
uh,
going to leave that to a subsequent learning algorithm to learn this.

49
00:03:17.170 --> 00:03:21.940
So that's better than actually feeding the integer value w because otherwise,

50
00:03:21.941 --> 00:03:26.350
if we had a word,
which as the ID for,

51
00:03:26.500 --> 00:03:30.520
and then we had a word w prime,
which has the id five,
and then another word,

52
00:03:30.700 --> 00:03:32.950
which has the,
uh,
id.

53
00:03:33.130 --> 00:03:36.070
So doubling prime prime's say 10,000.

54
00:03:36.520 --> 00:03:39.850
So it just happened to be lower down the list and the vocabulary than this word.

55
00:03:40.450 --> 00:03:43.900
Then if we look at these,
uh,
distance between four and five,

56
00:03:43.901 --> 00:03:48.880
that's much smaller than between four and 10,000,
based on the integer value.

57
00:03:49.210 --> 00:03:53.680
However,
it might be that in my vocabulary,
well four is actually Doug.
Uh,

58
00:03:53.740 --> 00:03:56.560
he is,
uh,
maybe,
uh,

59
00:03:56.620 --> 00:03:58.890
jump and then Duh,

60
00:03:59.110 --> 00:04:02.620
w prime prime might be capped and uh,

61
00:04:02.621 --> 00:04:07.050
we might argue that dog and cat actually,
uh,
uh,
courser then,
then,
uh,

62
00:04:07.130 --> 00:04:11.930
jump and Doug points since,
and this is only because,
uh,

63
00:04:12.260 --> 00:04:17.020
uh,
we didn't put any thought into the ordering of the words and also there's no

64
00:04:17.021 --> 00:04:20.920
really simple way of putting each word on the single line and putting them in a

65
00:04:20.921 --> 00:04:25.360
position which is actually meaningful.
So,
uh,

66
00:04:25.361 --> 00:04:27.790
that's why we can't really use the ID for the words.

67
00:04:27.820 --> 00:04:31.330
They usually not going to represent a meaningful similarity between the words.

68
00:04:31.331 --> 00:04:35.260
So we're going to instead start with the 100 and coding representation,

69
00:04:35.261 --> 00:04:39.880
which is actually a,
encodes no particular makes,

70
00:04:39.881 --> 00:04:41.650
no assumptions about word similarities.

71
00:04:41.710 --> 00:04:45.940
And then we'll leave it to the learning algorithm to try to,
uh,

72
00:04:45.970 --> 00:04:48.040
learn which words are more similar than others.

73
00:04:52.060 --> 00:04:57.060
Now still there is a big problem with the 100 representation or 100 detector or

74
00:04:57.860 --> 00:05:00.910
encoding and it said it's very,
very high dimensional.

75
00:05:01.420 --> 00:05:06.260
So the dimentionality one hug vector is the size of the vocabulary and the,

76
00:05:06.310 --> 00:05:07.510
like we've mentioned before,

77
00:05:07.600 --> 00:05:12.600
typical vocabulary sizes can vary between 10,000 to a few hundred thousand.

78
00:05:14.620 --> 00:05:18.730
And that's just for one word.
Now imagine we had the,
a sequence of 10 words,

79
00:05:18.731 --> 00:05:23.530
a window,
uh,
in,
in some texts that contain 10 words.

80
00:05:24.010 --> 00:05:26.840
And we want to encode the position of these different words.

81
00:05:26.841 --> 00:05:29.230
So we want to concatenate these 100 factors.

82
00:05:29.231 --> 00:05:34.231
So that gives us a total vector of length 1 million if we had 100,000 for,

83
00:05:34.570 --> 00:05:39.220
uh,
the vocabulary size.
So that's much bigger than,

84
00:05:39.460 --> 00:05:42.720
uh,
most,
uh,
problems there we,
uh,

85
00:05:42.750 --> 00:05:46.810
usually facing with neural networks or most machine learning problems.

86
00:05:47.770 --> 00:05:52.560
So the consequence of having a very high dimensional input is first we're will

87
00:05:52.600 --> 00:05:56.590
be vulnerable to overfitting.
If we have millions of inputs,

88
00:05:56.810 --> 00:06:01.190
then it means that,
so the size of him,
our input is,
is in the millions,

89
00:06:01.191 --> 00:06:03.170
then we're probably gonna have a,

90
00:06:03.171 --> 00:06:08.171
in the millions number of parameters in our senior all network or any other

91
00:06:08.361 --> 00:06:11.510
learning algorithm really.
So if we have millions of Prevnar is,

92
00:06:11.511 --> 00:06:15.320
it means that we need a lot of data to estimate well these parameters without

93
00:06:15.321 --> 00:06:20.300
over fitting the other problem and consequences that this can,

94
00:06:20.360 --> 00:06:24.190
uh,
um,
imply some computational,
uh,

95
00:06:24.230 --> 00:06:26.750
it compassionately expensive,
uh,

96
00:06:26.960 --> 00:06:31.220
computations that we will have to do when we manipulate these factors.
Uh,

97
00:06:31.250 --> 00:06:35.630
specifically because not all calculations are computations can be specified.

98
00:06:35.631 --> 00:06:40.450
So the one that vector can be,
uh,
uh,
sparsely and coated,

99
00:06:40.880 --> 00:06:41.930
um,
so,

100
00:06:41.931 --> 00:06:46.931
and to a sparse data structure and vector as far as vectors multiply by matrices

101
00:06:48.981 --> 00:06:52.580
for instance,
can be done fairly efficiently using as Parson implementation.

102
00:06:52.820 --> 00:06:56.420
However,
the other,
uh,
um,
operations of,
for instance,

103
00:06:56.421 --> 00:06:58.230
the reconstruction in Anton quarter,

104
00:06:58.250 --> 00:07:02.660
what we'd want to find since reconstruct this whole window of words that cannot

105
00:07:02.661 --> 00:07:07.430
be specified easily and trivially.
Uh,
if we did an auto encoder,

106
00:07:07.431 --> 00:07:12.431
we would actually need to reconstruct each of the element in this vector of a

107
00:07:13.130 --> 00:07:14.390
million and a.

108
00:07:14.391 --> 00:07:19.100
So those are the two consequences of using this basic one hot representation.

109
00:07:19.160 --> 00:07:19.820
And,
and,

110
00:07:19.820 --> 00:07:24.590
and these are issues that we'll try to tackle in the different algorithms that

111
00:07:24.591 --> 00:07:28.910
we'll see in a,
and different ideas.
We'll see in the following videos.

