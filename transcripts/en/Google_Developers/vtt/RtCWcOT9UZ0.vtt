WEBVTT
Kind: captions
Language: en

00:00:04.040 --> 00:00:05.970
Good afternoon, everyone.

00:00:05.970 --> 00:00:07.560
My name is Dominic
Preuss, and I'm

00:00:07.560 --> 00:00:10.130
very excited to be here
today at Google I/O talking

00:00:10.130 --> 00:00:12.350
about the Google Cloud Platform.

00:00:12.350 --> 00:00:15.960
I'm the product lead for
Google's storage and database

00:00:15.960 --> 00:00:18.310
products on the
Google Cloud Platform.

00:00:18.310 --> 00:00:21.140
As many of you know,
data is everything,

00:00:21.140 --> 00:00:24.110
and it's at the core of
everything that Google does.

00:00:24.110 --> 00:00:27.102
Whether it be the original
BackRub algorithm or some

00:00:27.102 --> 00:00:29.310
of the machine learning
things you saw in the keynote

00:00:29.310 --> 00:00:32.420
yesterday from Sundar, it really
is the basis of everything

00:00:32.420 --> 00:00:34.250
we do at Google.

00:00:34.250 --> 00:00:37.699
So what we're trying to do
is provide that platform

00:00:37.699 --> 00:00:39.990
that is the best place for
you to bring your data to be

00:00:39.990 --> 00:00:41.364
able to do similar
things to what

00:00:41.364 --> 00:00:43.901
Google's been doing internally.

00:00:43.901 --> 00:00:45.400
The way that we
want to achieve this

00:00:45.400 --> 00:00:50.090
is by building a fully
integrated data platform

00:00:50.090 --> 00:00:53.430
that is converged and makes
it as easy as possible

00:00:53.430 --> 00:00:56.880
for you get your data into the
system, move it, process it,

00:00:56.880 --> 00:00:59.240
and enrich it, and then
derive value from it.

00:00:59.240 --> 00:01:02.110
We don't want you to worry
about where your data ends up

00:01:02.110 --> 00:01:04.129
or where you decide
to store your data.

00:01:04.129 --> 00:01:06.280
We want to make sure that
data is never in a silo

00:01:06.280 --> 00:01:08.420
and that it's easy to
do what you need to do

00:01:08.420 --> 00:01:10.040
with it to complete your task.

00:01:10.040 --> 00:01:12.720
So to achieve this
vision, we've built

00:01:12.720 --> 00:01:15.120
a suite of storage
and database products

00:01:15.120 --> 00:01:19.325
for customers to bring
their data into the system.

00:01:19.325 --> 00:01:21.200
This will give us a
little bit of a level set

00:01:21.200 --> 00:01:25.000
before I dig into some of
the later parts of the talk.

00:01:25.000 --> 00:01:27.950
The goal of this
talk is threefold.

00:01:27.950 --> 00:01:31.360
It's to give you an overview
of our storage products.

00:01:31.360 --> 00:01:33.950
It's to give you a couple of
simple reference architectures

00:01:33.950 --> 00:01:36.220
to give you an idea of how
you might build a solution.

00:01:36.220 --> 00:01:38.137
And then it's to give
you a couple of examples

00:01:38.137 --> 00:01:40.553
of customers that are currently
doing this on Google Cloud

00:01:40.553 --> 00:01:41.120
Platform.

00:01:41.120 --> 00:01:45.010
So in this portfolio,
we'll start all the way

00:01:45.010 --> 00:01:46.260
at the object store.

00:01:46.260 --> 00:01:49.310
So Google Cloud Storage
is our object store.

00:01:49.310 --> 00:01:52.420
This is where you put your
objects, videos, or data

00:01:52.420 --> 00:01:55.060
that you're trying to process
via different products

00:01:55.060 --> 00:01:56.900
in the platform.

00:01:56.900 --> 00:01:59.780
It's used for media serving
as well as a collection

00:01:59.780 --> 00:02:03.094
point for data before you run
your data processing pipelines.

00:02:03.094 --> 00:02:05.260
What's interesting about
Google Cloud Storage that's

00:02:05.260 --> 00:02:07.340
fundamentally different
from other offerings

00:02:07.340 --> 00:02:09.500
is that it is
inherently multi-region.

00:02:09.500 --> 00:02:12.510
So when you put an object
into the object storage

00:02:12.510 --> 00:02:14.140
using our Standard
class, your data

00:02:14.140 --> 00:02:16.730
is actually replicated
across multiple regions.

00:02:16.730 --> 00:02:20.260
So it gives you a higher level
of availability and ability

00:02:20.260 --> 00:02:23.376
to serve that data back
out to your customers.

00:02:23.376 --> 00:02:26.000
Google Cloud Storage comes with
three different storage classes

00:02:26.000 --> 00:02:28.190
to match your user need.

00:02:28.190 --> 00:02:30.660
The first one that I
mentioned already is Standard.

00:02:30.660 --> 00:02:33.360
The second one is Durable
Reduced Availability.

00:02:33.360 --> 00:02:35.500
And what that means is if
you're willing to accept

00:02:35.500 --> 00:02:38.262
slightly lower availability,
we'll give you a better price.

00:02:38.262 --> 00:02:39.970
And then our third
one that we introduced

00:02:39.970 --> 00:02:41.730
in the middle of last
year is Nearline,

00:02:41.730 --> 00:02:44.300
which basically allows you, for
$0.01 per gigabyte per month,

00:02:44.300 --> 00:02:47.360
to store your data in the same
interface and the same product

00:02:47.360 --> 00:02:52.030
that would store your more
frequently accessed data.

00:02:52.030 --> 00:02:54.740
Moving onto the
relational side, Cloud SQL

00:02:54.740 --> 00:02:58.150
is our managed RDBMS, Relational
Database Management System

00:02:58.150 --> 00:02:59.080
service.

00:02:59.080 --> 00:03:01.970
So this is where you can go
to get a fully managed MySQL

00:03:01.970 --> 00:03:05.870
database with high availability
and pay as you go semantics,

00:03:05.870 --> 00:03:08.252
just like you would if you
were rolling it on your own.

00:03:08.252 --> 00:03:10.210
But instead of you worrying
about the problems,

00:03:10.210 --> 00:03:12.170
you can let Google site
reliability engineers

00:03:12.170 --> 00:03:14.214
worry about those for you.

00:03:14.214 --> 00:03:16.380
These are really good if
you're-- Cloud SQL is great

00:03:16.380 --> 00:03:18.910
if you're building on a
framework or have a piece

00:03:18.910 --> 00:03:21.820
of software that requires a
specific piece of software,

00:03:21.820 --> 00:03:24.890
like MySQL, and really makes
it easy for you to get up

00:03:24.890 --> 00:03:27.380
and running with almost no work.

00:03:27.380 --> 00:03:29.757
If you want to go
down the NoSQL path,

00:03:29.757 --> 00:03:31.090
we have two different offerings.

00:03:31.090 --> 00:03:32.680
One is Cloud Datastore.

00:03:32.680 --> 00:03:34.980
Cloud Datastore is our
application database

00:03:34.980 --> 00:03:37.070
that allows you
to, from scratch,

00:03:37.070 --> 00:03:38.330
just start storing data.

00:03:38.330 --> 00:03:40.360
There's no requirements
or defining a schema.

00:03:40.360 --> 00:03:42.040
You don't have to
provision size.

00:03:42.040 --> 00:03:45.042
All you do is start storing your
data in an unstructured way.

00:03:45.042 --> 00:03:46.500
We then make it as
easy as possible

00:03:46.500 --> 00:03:48.120
for you to get
access to that data

00:03:48.120 --> 00:03:51.630
and allow you to
scale up to any level.

00:03:51.630 --> 00:03:55.430
We see this very much used in
the mobile and web applications

00:03:55.430 --> 00:03:57.240
as the default place
that people go,

00:03:57.240 --> 00:03:58.500
especially if they're
building applications

00:03:58.500 --> 00:03:59.460
on Google App Engine.

00:04:02.410 --> 00:04:04.710
Our other NoSQL database
is Cloud Bigtable.

00:04:04.710 --> 00:04:07.270
So we published the Bigtable
white paper many years ago

00:04:07.270 --> 00:04:09.070
and spawned a lot
of the work that's

00:04:09.070 --> 00:04:10.900
being done in the NoSQL world.

00:04:10.900 --> 00:04:13.640
We've now taken that
internal Bigtable service

00:04:13.640 --> 00:04:17.170
and externalized it as
a service for customers

00:04:17.170 --> 00:04:20.079
with a HBase-compatible
client library.

00:04:20.079 --> 00:04:23.400
So what this allows you to do
is to take your HBase and some

00:04:23.400 --> 00:04:26.000
of your Cassandra workloads,
and bring them over and start

00:04:26.000 --> 00:04:27.875
serving them off of
Cloud Bigtable in a fully

00:04:27.875 --> 00:04:28.700
managed way.

00:04:28.700 --> 00:04:31.070
You no longer have to
manage your clusters,

00:04:31.070 --> 00:04:32.160
your HBase clusters.

00:04:32.160 --> 00:04:35.460
You can just let Google
manage those for you.

00:04:35.460 --> 00:04:38.720
This is very frequently used
in heavy read/write workloads.

00:04:38.720 --> 00:04:41.580
So we see this in Internet
of Things workloads,

00:04:41.580 --> 00:04:45.040
collecting sensor data,
time series databases,

00:04:45.040 --> 00:04:46.810
and anything that
has high throughput

00:04:46.810 --> 00:04:50.170
and needs low latency reads.

00:04:50.170 --> 00:04:51.529
Our last one is for caching.

00:04:51.529 --> 00:04:53.320
So if you have an
application in App Engine

00:04:53.320 --> 00:04:54.950
that you need a
caching layer, we

00:04:54.950 --> 00:04:57.330
provide a fully managed
Memcache service.

00:04:57.330 --> 00:04:59.710
So again, this allows
you to just use Memcache

00:04:59.710 --> 00:05:02.200
without having to worry about
how many nodes is it on,

00:05:02.200 --> 00:05:04.900
how much storage do you have,
how much memory do you have

00:05:04.900 --> 00:05:07.870
on the node to be able to fit
all of your keys, what happens

00:05:07.870 --> 00:05:09.929
when your keys if you
have to reboot your node.

00:05:09.929 --> 00:05:12.595
All those things are handled for
you in a fully managed service.

00:05:14.895 --> 00:05:17.270
So now we'll start talking
about reference architectures.

00:05:17.270 --> 00:05:18.752
And I'm going to
start with-- I'm

00:05:18.752 --> 00:05:20.710
going to do all of these
in two different ways.

00:05:20.710 --> 00:05:22.700
We're going to start
with the NoSQL world,

00:05:22.700 --> 00:05:26.384
and then we'll do RDBMS,
and then we'll do NoSQL.

00:05:26.384 --> 00:05:28.550
I don't want to start a
religious debate about which

00:05:28.550 --> 00:05:30.770
is better, so I'm just
going to cover both.

00:05:30.770 --> 00:05:33.010
If you are working in
the relational database

00:05:33.010 --> 00:05:35.150
world, the quickest
way to get up

00:05:35.150 --> 00:05:37.830
and running from your
concept of your idea

00:05:37.830 --> 00:05:42.260
to a megabyte of data, is
either from your web application

00:05:42.260 --> 00:05:45.340
or from the web browser
client or the mobile client,

00:05:45.340 --> 00:05:49.380
to build your application,
deploy it on Compute Engine,

00:05:49.380 --> 00:05:51.490
and then store your
structured data

00:05:51.490 --> 00:05:53.390
in Cloud SQL and your
unstructured data

00:05:53.390 --> 00:05:54.800
in Cloud Storage.

00:05:54.800 --> 00:05:56.586
So this way you can
put all of your data

00:05:56.586 --> 00:05:58.460
that's being generated
off of your framework,

00:05:58.460 --> 00:06:01.473
whether it be Rails
or Django or any

00:06:01.473 --> 00:06:04.380
of the other popular
frameworks, and then you

00:06:04.380 --> 00:06:07.426
can put your static
data in Cloud Storage.

00:06:07.426 --> 00:06:09.800
Basically, it's a couple of
clicks to get these things up

00:06:09.800 --> 00:06:11.030
and running, and
then you can get--

00:06:11.030 --> 00:06:12.530
and start serving
your prototype out

00:06:12.530 --> 00:06:14.620
to a small number of people.

00:06:14.620 --> 00:06:17.670
Now, if you wanted to do
the same thing with NoSQL,

00:06:17.670 --> 00:06:19.710
the way that we do
this is those same web

00:06:19.710 --> 00:06:21.460
and mobile
applications or clients

00:06:21.460 --> 00:06:23.335
that are speaking
to App Engine, which

00:06:23.335 --> 00:06:24.990
then speaks to Cloud Datastore.

00:06:24.990 --> 00:06:26.740
Now, the thing that
we announced yesterday

00:06:26.740 --> 00:06:29.240
that we're very excited about
is adding Firebase support

00:06:29.240 --> 00:06:30.620
for Google Cloud Storage.

00:06:30.620 --> 00:06:32.210
So now those static
files and you're

00:06:32.210 --> 00:06:35.120
saving, either from your
JavaScript web applications

00:06:35.120 --> 00:06:37.200
or your mobile
applications, can be

00:06:37.200 --> 00:06:40.110
put into Google Cloud Storage
via the Firebase set of client

00:06:40.110 --> 00:06:41.734
libraries.

00:06:41.734 --> 00:06:44.400
If you were doing this yourself,
you'd write a few lines of code

00:06:44.400 --> 00:06:46.890
in App Engine to be able to
serve that information out,

00:06:46.890 --> 00:06:49.090
and that data would be
stored in Datastore for you.

00:06:49.090 --> 00:06:51.650
Again, it's replicated,
highly available,

00:06:51.650 --> 00:06:53.850
and allows you to start
taking your prototype out

00:06:53.850 --> 00:06:56.570
to a larger number of people.

00:06:56.570 --> 00:06:57.070
All right.

00:06:57.070 --> 00:06:58.736
So you've got your
prototype, and you've

00:06:58.736 --> 00:07:00.670
shown it to a few people,
and people like it.

00:07:00.670 --> 00:07:02.503
So now, how do you push
this into production

00:07:02.503 --> 00:07:05.470
so you can start to support
a real number of users?

00:07:05.470 --> 00:07:08.470
The way that we do this is,
we take those same web clients

00:07:08.470 --> 00:07:11.610
and point them at Google
Cloud Load Balancing.

00:07:11.610 --> 00:07:15.750
Google offers a global
load balancing solution.

00:07:15.750 --> 00:07:18.610
Again, very different than other
things that are on the market.

00:07:18.610 --> 00:07:22.930
And then you would, behind that,
put one or many Compute Engine

00:07:22.930 --> 00:07:25.311
virtual machines to actually
host your application.

00:07:25.311 --> 00:07:27.060
So you would actually
be hosting your code

00:07:27.060 --> 00:07:30.720
and serving that code
out of those VMs.

00:07:30.720 --> 00:07:33.120
Again, you're serving
your data into Cloud SQL.

00:07:33.120 --> 00:07:37.200
You can have a high availability
version of Cloud SQL

00:07:37.200 --> 00:07:39.020
so that you have
high availability

00:07:39.020 --> 00:07:42.390
and never lose your data, and
it's a single-click to do that.

00:07:42.390 --> 00:07:44.650
And again, you're storing
your data in Cloud Storage.

00:07:44.650 --> 00:07:46.460
So you haven't actually
changed anything.

00:07:46.460 --> 00:07:48.220
You're using the
same set of products,

00:07:48.220 --> 00:07:50.110
but you're just provisioning
them slightly differently

00:07:50.110 --> 00:07:52.193
to make sure that you'll
scale up to level of load

00:07:52.193 --> 00:07:54.430
that you have.

00:07:54.430 --> 00:07:56.080
Now, if we go back
to the NoSQL path,

00:07:56.080 --> 00:07:57.720
you can see that
this is suspiciously

00:07:57.720 --> 00:07:59.510
like our last slide.

00:07:59.510 --> 00:08:01.729
And again, you have
your web clients

00:08:01.729 --> 00:08:03.770
in your mobile applications
talking to App Engine

00:08:03.770 --> 00:08:07.020
or Firebase, and then storing
their data in Cloud Datastore

00:08:07.020 --> 00:08:08.534
and Cloud Storage.

00:08:08.534 --> 00:08:09.950
The beauty of going
down this path

00:08:09.950 --> 00:08:13.640
is that you can scale from
basically zero and your concept

00:08:13.640 --> 00:08:16.820
all the way up to terabytes
of data, and all of that

00:08:16.820 --> 00:08:19.526
happens completely without
any intervention on your side.

00:08:19.526 --> 00:08:20.900
You just keep
throwing data at it

00:08:20.900 --> 00:08:22.524
and we'll just keep
consuming the data.

00:08:22.524 --> 00:08:25.220
Whether it be in Datastore
or Cloud Storage,

00:08:25.220 --> 00:08:26.650
it's a per consumption basis.

00:08:26.650 --> 00:08:27.750
There's no provisioning.

00:08:27.750 --> 00:08:30.760
You just throw data at it and
we'll go ahead and store it.

00:08:30.760 --> 00:08:31.260
Great.

00:08:31.260 --> 00:08:33.450
So now we've got this
product on the market.

00:08:33.450 --> 00:08:34.559
It's in the App Store.

00:08:34.559 --> 00:08:36.690
It's doing amazingly well.

00:08:36.690 --> 00:08:37.530
App's getting slow.

00:08:37.530 --> 00:08:39.039
We need to really
scale this out.

00:08:39.039 --> 00:08:42.080
How do we support
millions of users?

00:08:42.080 --> 00:08:43.510
So the way that
you would do this

00:08:43.510 --> 00:08:47.640
is, if you're going down the
relational database model,

00:08:47.640 --> 00:08:51.490
is take your web and mobile
clients-- speaking to the Cloud

00:08:51.490 --> 00:08:54.634
Load Balancer, again, no
changes are needed on the load

00:08:54.634 --> 00:08:56.050
balancer-- and
then you would just

00:08:56.050 --> 00:08:58.247
be adding additional
numbers of compute engines.

00:08:58.247 --> 00:09:00.080
Now, you can use things
like instance groups

00:09:00.080 --> 00:09:01.945
and some of the other
features as well.

00:09:01.945 --> 00:09:03.320
But to keep it
simple, we'll just

00:09:03.320 --> 00:09:05.980
say that there's a number
of VMs necessary to serve

00:09:05.980 --> 00:09:09.110
your application, and
then all of those VMs

00:09:09.110 --> 00:09:11.600
and those applications are
speaking back to Cloud SQL.

00:09:11.600 --> 00:09:13.940
Now, the one thing we've
done differently in Cloud SQL

00:09:13.940 --> 00:09:16.070
is we've added read replicas.

00:09:16.070 --> 00:09:18.020
So within Cloud SQL,
you can, with one click,

00:09:18.020 --> 00:09:21.037
add read replicas to basically
scale out your read loads.

00:09:21.037 --> 00:09:23.120
So basically all of your
reads in your application

00:09:23.120 --> 00:09:25.370
would go to the read replicas,
and then you would just

00:09:25.370 --> 00:09:26.980
send your rights to the master.

00:09:26.980 --> 00:09:29.630
This allows you to continue
to scale on top of Cloud SQL

00:09:29.630 --> 00:09:33.320
without having to do anything
other than add read replicas

00:09:33.320 --> 00:09:35.420
and potentially change
the size of the VM

00:09:35.420 --> 00:09:38.005
that's hosting your database.

00:09:38.005 --> 00:09:40.740
The other thing that's
very-- is great about the way

00:09:40.740 --> 00:09:44.105
that we've set up Compute Engine
is that it's very easy for you

00:09:44.105 --> 00:09:45.210
to add capacity.

00:09:45.210 --> 00:09:48.600
So if you attach a certain
number of persistent disks,

00:09:48.600 --> 00:09:51.210
which is our block storage
solution, to those VMs that you

00:09:51.210 --> 00:09:53.160
need to add capacity,
you can just

00:09:53.160 --> 00:09:55.920
increase the size of the
volumes without having

00:09:55.920 --> 00:09:57.080
to restart the machines.

00:09:57.080 --> 00:09:58.740
We call it hot
grow, and it's a way

00:09:58.740 --> 00:10:01.140
for people to add capacity
to their applications

00:10:01.140 --> 00:10:04.080
without having any downtime.

00:10:04.080 --> 00:10:07.004
So now I want to start
adding in some customers,

00:10:07.004 --> 00:10:08.670
just to give you a
little bit of context

00:10:08.670 --> 00:10:09.919
of how people are using these.

00:10:09.919 --> 00:10:14.670
So Ocado is a UK
e-commerce provider.

00:10:14.670 --> 00:10:17.210
They basically use
Cloud SQL as a way

00:10:17.210 --> 00:10:20.619
to not have to worry about
managing their MySQL instances.

00:10:20.619 --> 00:10:22.160
They allow Google
to do that for them

00:10:22.160 --> 00:10:25.020
and they focus on building
their applications.

00:10:25.020 --> 00:10:27.560
Applications that are very
transactionally sensitive,

00:10:27.560 --> 00:10:30.380
whether it be payments or if you
have data that it has to always

00:10:30.380 --> 00:10:32.150
be consistent, a
lot of those tend

00:10:32.150 --> 00:10:33.750
to go down the Cloud SQL path.

00:10:33.750 --> 00:10:36.670
And so e-commerce
and that vertical

00:10:36.670 --> 00:10:40.740
tends to be very
popular with Cloud SQL.

00:10:40.740 --> 00:10:42.480
Now, if we go back
to the NoSQL path,

00:10:42.480 --> 00:10:45.390
again, this looks suspiciously
like our prior slide.

00:10:45.390 --> 00:10:47.580
Because to go from
literally concept

00:10:47.580 --> 00:10:49.810
up to terabytes of
data, nothing has

00:10:49.810 --> 00:10:51.520
to change in terms
of your architecture.

00:10:51.520 --> 00:10:53.710
As you add more
and more clients,

00:10:53.710 --> 00:10:57.147
both Firebase and App
Engine will scale up

00:10:57.147 --> 00:10:58.730
to serve those
requests, and that data

00:10:58.730 --> 00:11:00.880
goes into Cloud Datastore
or into Cloud Storage

00:11:00.880 --> 00:11:02.670
and there's no change necessary.

00:11:02.670 --> 00:11:05.000
You can throw as much
traffic at it as you want

00:11:05.000 --> 00:11:08.330
and they'll just scale
up to meet your needs.

00:11:08.330 --> 00:11:11.250
This is the promise of the
platform, where we can just

00:11:11.250 --> 00:11:13.540
respond to the amount of
demand that you're giving us,

00:11:13.540 --> 00:11:16.692
and you don't have to worry
about prepaying for capacity.

00:11:16.692 --> 00:11:19.150
You don't have to worry about
what the pricing does to you.

00:11:19.150 --> 00:11:20.990
You know that if
you use the product,

00:11:20.990 --> 00:11:23.520
we'll give you your sustained
use discounts on the VMs,

00:11:23.520 --> 00:11:27.512
and you'll only pay for the data
that you use in the database.

00:11:27.512 --> 00:11:29.470
So a good example of
going down this NoSQL path

00:11:29.470 --> 00:11:30.990
is Khan Academy.

00:11:30.990 --> 00:11:34.640
So Khan Academy uses Google
App Engine with Datastore.

00:11:34.640 --> 00:11:37.350
And we're basically able
to scale from a small idea

00:11:37.350 --> 00:11:39.657
up to a major website
without having

00:11:39.657 --> 00:11:41.490
to make any architectural
changes to the way

00:11:41.490 --> 00:11:44.477
that they build
their applications.

00:11:44.477 --> 00:11:46.060
The goal of all these
things is really

00:11:46.060 --> 00:11:49.330
to allow developers to focus
on writing code and not

00:11:49.330 --> 00:11:51.150
managing their services.

00:11:51.150 --> 00:11:53.200
So we're trying to
basically increase

00:11:53.200 --> 00:11:55.120
the velocity of
development to the maximum

00:11:55.120 --> 00:11:57.960
by removing anything that
isn't delivering value

00:11:57.960 --> 00:11:59.930
to your end users.

00:11:59.930 --> 00:12:02.180
So now we start to get into
some interesting areas.

00:12:02.180 --> 00:12:04.770
So now we're going past
a terabyte of data.

00:12:04.770 --> 00:12:07.160
And areas where we start to
see large amounts of data

00:12:07.160 --> 00:12:09.199
is really in the data
aggregation space.

00:12:09.199 --> 00:12:10.740
So there's a number
of different ways

00:12:10.740 --> 00:12:12.800
that you might be
collecting data.

00:12:12.800 --> 00:12:15.610
You might have a mobile
client application

00:12:15.610 --> 00:12:17.380
which is speaking back
to Cloud Endpoints.

00:12:17.380 --> 00:12:20.840
So Cloud Endpoints is
our hosted API endpoints

00:12:20.840 --> 00:12:23.930
that allows you to offload
all the management of your API

00:12:23.930 --> 00:12:26.100
to Google managed services.

00:12:26.100 --> 00:12:28.667
And then the data that's
coming in from those endpoints

00:12:28.667 --> 00:12:30.500
can be written into the
Cloud Bigtable which

00:12:30.500 --> 00:12:31.874
we talked about earlier.

00:12:31.874 --> 00:12:33.915
Another way is doing the
same thing with the web.

00:12:33.915 --> 00:12:35.230
You have a web application.

00:12:35.230 --> 00:12:38.150
You're collecting a bunch
of stats, time series data

00:12:38.150 --> 00:12:41.550
that you need to push back
to the Compute Engine, which

00:12:41.550 --> 00:12:43.242
then gets collected
in Cloud Datastore.

00:12:43.242 --> 00:12:45.450
Again, you're just pushing
data into Cloud Datastore.

00:12:45.450 --> 00:12:46.950
You're not worrying
about provisioning.

00:12:46.950 --> 00:12:48.533
You're not worrying
about, do you have

00:12:48.533 --> 00:12:49.700
enough disk in the cluster.

00:12:49.700 --> 00:12:52.725
All you're doing is pushing
data into Cloud Datastore.

00:12:52.725 --> 00:12:54.350
And the third one is
the one that we're

00:12:54.350 --> 00:12:56.391
seeing over and over again,
and one of the things

00:12:56.391 --> 00:12:57.560
that I'm most excited about.

00:12:57.560 --> 00:13:00.600
We keep seeing, over and over
again, people bringing us

00:13:00.600 --> 00:13:02.780
really interesting
data pipelines

00:13:02.780 --> 00:13:06.440
and ways of aggravating and
taking value of their data.

00:13:06.440 --> 00:13:09.310
So a good example of
this is sensor data.

00:13:09.310 --> 00:13:10.960
So if you have
millions of sensors

00:13:10.960 --> 00:13:13.140
out there that are
collecting data every minute,

00:13:13.140 --> 00:13:16.000
10 minutes, hour, they have
to send all that data back

00:13:16.000 --> 00:13:17.460
to a single place.

00:13:17.460 --> 00:13:20.770
What we do is we've seen people
using Cloud Pub/Sub, which

00:13:20.770 --> 00:13:22.930
is our messaging queue product.

00:13:22.930 --> 00:13:23.920
It's a global product.

00:13:23.920 --> 00:13:24.720
It's not zonal.

00:13:24.720 --> 00:13:26.490
So wherever your sensors
are in the world,

00:13:26.490 --> 00:13:28.990
you can just push your
data into Pub/Sub.

00:13:28.990 --> 00:13:31.030
And then using Cloud
Dataflow to then

00:13:31.030 --> 00:13:33.870
process that data, either
in a batch or stream mode,

00:13:33.870 --> 00:13:36.110
to push that data
into Cloud Bigtable.

00:13:36.110 --> 00:13:37.764
And now we really
are seeing people

00:13:37.764 --> 00:13:40.180
start to operate the same way
that Google does internally.

00:13:40.180 --> 00:13:42.340
So internally, we use
Pub/Sub for messaging.

00:13:42.340 --> 00:13:45.790
We use Dataflow, usually in
stream mode and not batch mode,

00:13:45.790 --> 00:13:48.540
to then push all this
data into Cloud Bigtable.

00:13:48.540 --> 00:13:51.142
You're now starting to see
people really implement

00:13:51.142 --> 00:13:53.600
their data pipelines and their
data collection the same way

00:13:53.600 --> 00:13:57.497
that we do inside Google,
and it's very exciting.

00:13:57.497 --> 00:13:59.080
One of the things
that we did recently

00:13:59.080 --> 00:14:01.670
that I'm very excited
about was a collaboration

00:14:01.670 --> 00:14:07.380
that we did with FIS that
basically they went and took

00:14:07.380 --> 00:14:12.920
and created 25 billion
synthetic market transactions

00:14:12.920 --> 00:14:15.080
and were able to
process them in an hour.

00:14:15.080 --> 00:14:18.110
And so they were
basically able to push

00:14:18.110 --> 00:14:20.740
from putting data into
Google Cloud Storage,

00:14:20.740 --> 00:14:23.210
processing with Dataflow,
putting it into Bigtable.

00:14:23.210 --> 00:14:25.710
They were able to process
those 25 billion transactions

00:14:25.710 --> 00:14:26.810
in under an hour.

00:14:26.810 --> 00:14:30.520
And at peak, they were pushing
34 million reads a second

00:14:30.520 --> 00:14:36.890
and 22 million writes a second
using 3,500 nodes of Bigtable.

00:14:36.890 --> 00:14:38.770
So they were able to
do that without having

00:14:38.770 --> 00:14:40.474
to buy hardware, provision it.

00:14:40.474 --> 00:14:42.140
We worked with them
to make sure that we

00:14:42.140 --> 00:14:44.223
had the capacity on hand
for them to run the test,

00:14:44.223 --> 00:14:46.395
and then they were
able to execute that.

00:14:46.395 --> 00:14:48.770
And it's something that we're
seeing over and over again,

00:14:48.770 --> 00:14:51.039
that these companies
aren't able to run

00:14:51.039 --> 00:14:52.080
at these levels of scale.

00:14:52.080 --> 00:14:53.630
Because of just a
procurement cycle

00:14:53.630 --> 00:14:55.588
that would be necessary
to acquire the hardware

00:14:55.588 --> 00:14:57.310
to run a test of
this size, we were

00:14:57.310 --> 00:15:00.530
able to spin up in
a couple of weeks.

00:15:00.530 --> 00:15:02.897
So now we've gotten up
to a petabyte of data.

00:15:02.897 --> 00:15:04.480
But it wouldn't be
Google if we didn't

00:15:04.480 --> 00:15:06.310
talk about exabytes of data.

00:15:06.310 --> 00:15:08.510
So how do you go from
having a petabyte of data

00:15:08.510 --> 00:15:10.110
into an exabyte of data?

00:15:10.110 --> 00:15:13.200
And where we start to see
this type of volume of data

00:15:13.200 --> 00:15:15.370
is really in the
archival workload.

00:15:15.370 --> 00:15:17.250
So we've got a couple
of ways for people

00:15:17.250 --> 00:15:20.570
to ingest data
into our platform.

00:15:20.570 --> 00:15:24.070
So one way to do this is
Google Cloud Transfer Service.

00:15:24.070 --> 00:15:26.290
So we offer a product
that will allow

00:15:26.290 --> 00:15:29.420
you to grab your data, either
from another Google Cloud

00:15:29.420 --> 00:15:32.420
Storage bucket or
from Amazon S3,

00:15:32.420 --> 00:15:35.310
and transfer that data
into Google Cloud Storage.

00:15:35.310 --> 00:15:36.730
We will do this on a schedule.

00:15:36.730 --> 00:15:40.300
If there are failures, we
will retry and make sure

00:15:40.300 --> 00:15:41.530
that all the data gets over.

00:15:41.530 --> 00:15:44.140
And we'll optimize and paralyze
the transfer to make sure

00:15:44.140 --> 00:15:47.200
that we do it as
quickly as possible.

00:15:47.200 --> 00:15:49.510
Another area that we're
seeing a lot of adoption,

00:15:49.510 --> 00:15:51.290
especially from our
enterprise customers,

00:15:51.290 --> 00:15:53.950
is in the storage
appliance space.

00:15:53.950 --> 00:15:56.930
So we have gone out and worked
with a number of the storage

00:15:56.930 --> 00:15:58.640
appliance companies
to partner with them

00:15:58.640 --> 00:16:00.860
to create integrations
for Google Cloud Storage.

00:16:00.860 --> 00:16:05.490
So companies like EMC,
NetApp, Veritas, Convault.

00:16:05.490 --> 00:16:10.412
And the idea here is that these
customers have several 1, 2, 3,

00:16:10.412 --> 00:16:13.530
10, 100 petabytes of data that
they've been sitting in a data

00:16:13.530 --> 00:16:17.070
store that they're paying to
buy disks to maintain this data.

00:16:17.070 --> 00:16:20.362
And so now, instead of
storing this on a capex basis

00:16:20.362 --> 00:16:21.820
where they have to
buy these disks,

00:16:21.820 --> 00:16:24.590
they can transfer it all into
Google Cloud Storage using

00:16:24.590 --> 00:16:26.910
the same storage appliance
that they use today,

00:16:26.910 --> 00:16:28.847
and back all that data
up into the cloud.

00:16:28.847 --> 00:16:30.930
And they've gone from a
capex model, where they're

00:16:30.930 --> 00:16:32.720
paying for actual
disks that they

00:16:32.720 --> 00:16:35.747
have to be buying and storage
arrays, to a pure opex

00:16:35.747 --> 00:16:37.830
model where they're purely
paying on a consumption

00:16:37.830 --> 00:16:39.651
basis for storing that data.

00:16:39.651 --> 00:16:41.150
We've helped companies
save millions

00:16:41.150 --> 00:16:42.816
of dollars doing this
and it's something

00:16:42.816 --> 00:16:45.560
that we continue to
be very committed to.

00:16:45.560 --> 00:16:48.320
Another way that you can ingest
your data is offline ingest.

00:16:48.320 --> 00:16:52.180
So we partner with Iron Mountain
in North America and Prime

00:16:52.180 --> 00:16:56.690
Focus Technologies in
EMEA and in Asia-Pacific.

00:16:56.690 --> 00:16:59.830
And you can actually
send them hard drives,

00:16:59.830 --> 00:17:03.340
and they will upload those
files from the hard drives

00:17:03.340 --> 00:17:04.849
into Google Cloud
Storage for you.

00:17:04.849 --> 00:17:06.119
So if you don't feel
comfortable doing

00:17:06.119 --> 00:17:07.619
all of that work
on your own, we're

00:17:07.619 --> 00:17:09.077
working with
companies to make sure

00:17:09.077 --> 00:17:12.900
that it's easy and secure for
you to upload those files.

00:17:12.900 --> 00:17:15.359
Now, any talk about storage
wouldn't be complete

00:17:15.359 --> 00:17:17.089
if we didn't talk
about our partnership

00:17:17.089 --> 00:17:18.609
and our marketplace.

00:17:18.609 --> 00:17:20.700
So Cloud Launcher is
where our partners

00:17:20.700 --> 00:17:23.230
come to be able to launch
solutions into the Google Cloud

00:17:23.230 --> 00:17:24.609
Platform.

00:17:24.609 --> 00:17:26.180
I've highlighted
a number of them.

00:17:26.180 --> 00:17:27.859
If what you really
need is Cassandra,

00:17:27.859 --> 00:17:30.500
we've got a preconfigured
image through Bitnami,

00:17:30.500 --> 00:17:34.550
one of our partners, to be able
to launch a Cassandra into GCE.

00:17:34.550 --> 00:17:35.960
Same as EnterpriseDB.

00:17:35.960 --> 00:17:38.760
Worked with EnterpriseDB to
get a solution up on GCE,

00:17:38.760 --> 00:17:41.510
so that you can launch these
solutions into the cloud,

00:17:41.510 --> 00:17:43.700
so that if we don't provide
a managed service that

00:17:43.700 --> 00:17:46.560
meets your needs, you can
manage your own solution on GCE

00:17:46.560 --> 00:17:48.570
with a single-click.

00:17:48.570 --> 00:17:50.110
MongoDB is another example.

00:17:50.110 --> 00:17:53.120
A lot of interest in MongoDB.

00:17:53.120 --> 00:17:55.360
Make it very easy for
you to be able to spin up

00:17:55.360 --> 00:17:58.010
those instances that
are running MongoDB

00:17:58.010 --> 00:18:00.264
to support your workload.

00:18:00.264 --> 00:18:01.180
Same thing with Redis.

00:18:01.180 --> 00:18:03.070
A very popular caching solution.

00:18:03.070 --> 00:18:05.640
So we make it one-click
possible to kick off

00:18:05.640 --> 00:18:09.422
a VM that is a preconfigured
version of Redis.

00:18:09.422 --> 00:18:11.630
That's all the concepts that
I wanted to cover today.

00:18:11.630 --> 00:18:13.460
But the key takeaway
that I want to make

00:18:13.460 --> 00:18:15.776
sure everyone
understands is that we

00:18:15.776 --> 00:18:17.150
are striving to
make Google Cloud

00:18:17.150 --> 00:18:20.520
Platform the place to store
your data in the public cloud.

00:18:20.520 --> 00:18:23.044
We are faster, cheaper,
and more reliable

00:18:23.044 --> 00:18:24.960
than many of the solutions
that are out there.

00:18:24.960 --> 00:18:27.390
And then we're being fully
integrated to make sure

00:18:27.390 --> 00:18:30.110
that we can actually
get your data,

00:18:30.110 --> 00:18:31.670
move it, process
it, and enrich it,

00:18:31.670 --> 00:18:33.730
and allow you to drive
value, whether that be

00:18:33.730 --> 00:18:35.870
through BigQuery or running
machine learning models

00:18:35.870 --> 00:18:38.650
on top of it, like the ones
that you saw at Sundar's keynote

00:18:38.650 --> 00:18:40.327
yesterday.

00:18:40.327 --> 00:18:41.160
Thank you, everyone.

00:18:44.680 --> 00:18:48.200
[MUSIC PLAYING]

