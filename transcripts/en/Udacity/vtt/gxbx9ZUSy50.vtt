WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.530
To help us see the need for caches, we'll examine

00:00:02.530 --> 00:00:06.400
this little snippet of code. From an application programmer's perspective,

00:00:06.400 --> 00:00:09.640
a statement like this one, the sum plus equals a

00:00:09.640 --> 00:00:12.950
of i, would seem to happen in one step. Even

00:00:12.950 --> 00:00:15.980
in assembly, it would only take a few instructions. In

00:00:15.980 --> 00:00:18.010
terms of time on the clock, however, not all of

00:00:18.010 --> 00:00:21.990
these instructions are the same. Sum, being a local variable,

00:00:21.990 --> 00:00:25.860
is most likely stored in a register in the CPU,

00:00:25.860 --> 00:00:29.070
therefore there is almost no cost to retrieving its value. a

00:00:29.070 --> 00:00:32.070
of i, however, is in main memory, and retrieving its value

00:00:32.070 --> 00:00:36.100
can take 100 CPU cycles. And even then, the bus bandwidth

00:00:36.100 --> 00:00:38.890
might only be able to pass back two bytes a cycle

00:00:38.890 --> 00:00:41.830
or thereabouts, to get the data back. Trips to main memory

00:00:41.830 --> 00:00:44.450
are expensive, and this trend has only become worse over the

00:00:44.450 --> 00:00:48.300
past few decades with hardware advances. The solution to this problem

00:00:48.300 --> 00:00:51.800
has been to place a smaller, faster set of memory made

00:00:51.800 --> 00:00:54.840
with faster technology, closer to the CPU. We call this

00:00:54.840 --> 00:00:58.950
memory a cache. And here, we're looking at a latency of

00:00:58.950 --> 00:01:02.930
about ten cycles. So, when the CPU wants the contents

00:01:02.930 --> 00:01:06.130
of some memory address, it looks in the cache first. If

00:01:06.130 --> 00:01:08.870
it finds the data it needs, great, we've just achieved

00:01:08.870 --> 00:01:11.990
a factor of ten speedups. We call this a cache hit.

00:01:11.990 --> 00:01:14.610
If the data it wants is not in the cache then

00:01:14.610 --> 00:01:16.800
we have to go to main memory. We call this a

00:01:16.800 --> 00:01:20.030
cache miss. The key is keeping these cache meshes

00:01:20.030 --> 00:01:23.670
infrequent, so that on average, time spent accessing is more

00:01:23.670 --> 00:01:26.350
like the time it takes to access the cache,

00:01:26.350 --> 00:01:29.380
than it is like the time to access main memory.

