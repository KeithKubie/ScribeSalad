WEBVTT
Kind: captions
Language: en

00:00:05.810 --> 00:00:06.290
MICHAEL MANOOCHEHRI:
Hey, everyone.

00:00:06.290 --> 00:00:08.000
Can you all hear me?

00:00:08.000 --> 00:00:08.540
All right.

00:00:08.540 --> 00:00:11.260
I hope you're having a good time
at Google I/O. We have a

00:00:11.260 --> 00:00:13.120
fun presentation
for you today.

00:00:13.120 --> 00:00:15.490
We're going to talk about
building data pipelines at

00:00:15.490 --> 00:00:16.580
Google Scale.

00:00:16.580 --> 00:00:17.980
My name is Michael
Manoochehri.

00:00:17.980 --> 00:00:20.310
I'm a developer programs
engineer working primarily

00:00:20.310 --> 00:00:23.270
with the Google BigQuery team.

00:00:23.270 --> 00:00:25.640
And this talk is very near
and dear to my heart.

00:00:25.640 --> 00:00:28.530
We're going to talk a little
bit about building data

00:00:28.530 --> 00:00:30.620
processing applications
using some of

00:00:30.620 --> 00:00:34.010
Google's cloud services.

00:00:34.010 --> 00:00:35.970
This is going to be a good
talk for people that are

00:00:35.970 --> 00:00:38.550
already building data processing
applications,

00:00:38.550 --> 00:00:40.550
distributed data applications.

00:00:40.550 --> 00:00:42.760
And if you're not very familiar
with the big data

00:00:42.760 --> 00:00:45.740
space, or as we call it at
Google, the data space,

00:00:45.740 --> 00:00:47.830
this'll be a good introduction
to some of the technologies

00:00:47.830 --> 00:00:50.870
that we use in general when
we're working with this type

00:00:50.870 --> 00:00:54.040
of data processing field.

00:00:54.040 --> 00:00:57.950
So in this talk, I'm going to
get started by talking about

00:00:57.950 --> 00:01:00.640
what a data pipeline is,
what do I mean by that.

00:01:00.640 --> 00:01:01.910
And then we're going to
introduce you to some of

00:01:01.910 --> 00:01:04.129
technologies involved with
building applications,

00:01:04.129 --> 00:01:06.610
end-to-end data intensive
applications, using Google

00:01:06.610 --> 00:01:09.840
technologies, including the App
Engine Pipeline API and

00:01:09.840 --> 00:01:12.200
the MapReduce library
for App Engine.

00:01:12.200 --> 00:01:14.570
We'll talk a little bit about
the App Engine Datastore and

00:01:14.570 --> 00:01:15.300
Google BigQuery.

00:01:15.300 --> 00:01:16.840
I'll show you that.

00:01:16.840 --> 00:01:18.230
And then we'll talk a little
bit about Google

00:01:18.230 --> 00:01:19.670
Cloud Storage as well.

00:01:19.670 --> 00:01:21.140
Then we'll dive right
into some use cases.

00:01:21.140 --> 00:01:24.190
I'm going to show you some
real use cases that we've

00:01:24.190 --> 00:01:27.080
developed working with some
customers, some common use

00:01:27.080 --> 00:01:30.390
cases for building data
applications on App Engine.

00:01:30.390 --> 00:01:32.350
And finally best practices, I'll
talk a little bit about

00:01:32.350 --> 00:01:33.740
performance.

00:01:33.740 --> 00:01:36.130
And then we'll have
time for Q and A.

00:01:36.130 --> 00:01:36.960
So let's get started.

00:01:36.960 --> 00:01:38.490
Let me sort of set the stage.

00:01:38.490 --> 00:01:39.850
And we'll get started with some

00:01:39.850 --> 00:01:41.520
data application patterns.

00:01:41.520 --> 00:01:45.510
So this is a use case we know
very well on the BigQuery team

00:01:45.510 --> 00:01:47.800
working with Claritics, who I
think are in our Sandbox right

00:01:47.800 --> 00:01:50.490
now, a social game analytics
company.

00:01:50.490 --> 00:01:54.920
So mobile games and social games
generate a lot of data.

00:01:54.920 --> 00:01:57.430
Web applications generate
a lot of data, as well.

00:01:57.430 --> 00:01:59.570
You have many clients--
sometimes you have millions of

00:01:59.570 --> 00:02:01.760
clients if a game goes
viral or a mobile

00:02:01.760 --> 00:02:03.050
application goes viral--

00:02:03.050 --> 00:02:04.750
generating data.

00:02:04.750 --> 00:02:09.389
Playing games, leveling up,
buying new items, sending data

00:02:09.389 --> 00:02:12.230
back to a web application
backend that you've developed.

00:02:12.230 --> 00:02:14.080
And you're sometimes
sending data back

00:02:14.080 --> 00:02:16.010
to the client itself.

00:02:16.010 --> 00:02:18.930
And so, in these situations,
you're

00:02:18.930 --> 00:02:20.740
generating a lot of data.

00:02:20.740 --> 00:02:23.520
And it can grow immensely.

00:02:23.520 --> 00:02:25.740
A lot of these applications
also need a place to store

00:02:25.740 --> 00:02:28.870
that data and analyze that data
and visualize that data.

00:02:28.870 --> 00:02:30.750
Think of a check
in application.

00:02:30.750 --> 00:02:32.640
Some kind of application where
you're going somewhere--

00:02:32.640 --> 00:02:35.130
geolocate yourself, maybe
at a business.

00:02:35.130 --> 00:02:36.050
And you check in.

00:02:36.050 --> 00:02:38.840
And that sends some data
to a web backend.

00:02:38.840 --> 00:02:41.100
And then your application also
needs to make some sense of

00:02:41.100 --> 00:02:42.300
this large amount of data.

00:02:42.300 --> 00:02:44.230
Let's say you have millions of
users checking in all the

00:02:44.230 --> 00:02:45.780
time, five times a day.

00:02:45.780 --> 00:02:47.690
You're going to want to
visualize that data.

00:02:47.690 --> 00:02:49.330
You're going to want to
analyze that data.

00:02:49.330 --> 00:02:51.240
And it is an immense
amount of data.

00:02:51.240 --> 00:02:52.920
You could be generating
gigabytes of data

00:02:52.920 --> 00:02:55.090
every single day.

00:02:55.090 --> 00:02:59.420
So the two faces of this
application required different

00:02:59.420 --> 00:03:00.870
optimizations.

00:03:00.870 --> 00:03:03.780
So your client facing web
application stack needs to be

00:03:03.780 --> 00:03:06.390
built for high availability
and performance.

00:03:06.390 --> 00:03:08.730
You need to support millions
of users making dozens of

00:03:08.730 --> 00:03:12.290
check ins a day or playing
millions of levels of your

00:03:12.290 --> 00:03:13.100
game a day.

00:03:13.100 --> 00:03:15.430
And that's going to generate a
lot of hits on your stack.

00:03:15.430 --> 00:03:16.350
And you're also going
to want to push some

00:03:16.350 --> 00:03:17.640
data back to them.

00:03:17.640 --> 00:03:20.550
And then on your back end,
you're going to want to store

00:03:20.550 --> 00:03:23.990
a lot of data, ask questions
about this data, run aggregate

00:03:23.990 --> 00:03:25.540
queries over the data.

00:03:25.540 --> 00:03:27.150
So there's different
optimizations for different

00:03:27.150 --> 00:03:29.390
part of these applications.

00:03:29.390 --> 00:03:31.520
Here's another application
pattern that you see a lot in

00:03:31.520 --> 00:03:32.300
this space.

00:03:32.300 --> 00:03:35.010
It's moving from unstructured
to structured data.

00:03:35.010 --> 00:03:38.330
Or data that's structured in
some way, and you need to put

00:03:38.330 --> 00:03:40.205
it into another format to
do something with, to

00:03:40.205 --> 00:03:41.810
do some work with.

00:03:41.810 --> 00:03:44.880
The canonical example
is web indexing.

00:03:44.880 --> 00:03:47.820
HTML is originally a
presentation layer.

00:03:47.820 --> 00:03:50.300
Millions, billions, trillions,
gazillions of web

00:03:50.300 --> 00:03:51.290
pages on the internet--

00:03:51.290 --> 00:03:54.110
Google crawls it every day and
creates an index of that.

00:03:54.110 --> 00:03:56.060
So you can actually
search for things.

00:03:56.060 --> 00:03:59.067
Other examples might be creating
Ngrams, doing word

00:03:59.067 --> 00:04:02.080
counts, doing lexical analysis
of millions of things.

00:04:02.080 --> 00:04:04.180
So you're taking data that's
structured in some way, or

00:04:04.180 --> 00:04:06.590
unstructured, and you're doing
something with it, and turning

00:04:06.590 --> 00:04:07.920
it into structured data.

00:04:07.920 --> 00:04:11.030
So these are examples
of data pipelines.

00:04:11.030 --> 00:04:15.530
In some cases, you can use the
same sort of software for the

00:04:15.530 --> 00:04:16.000
same thing.

00:04:16.000 --> 00:04:17.680
But let's take a closer look.

00:04:17.680 --> 00:04:19.470
When your data is small--

00:04:19.470 --> 00:04:22.060
when you just have 1 record,
1,000 records, 1 million

00:04:22.060 --> 00:04:26.800
records, you can work with this
data on a local machine.

00:04:26.800 --> 00:04:29.360
When your data's small enough,
you can work with your data on

00:04:29.360 --> 00:04:32.390
your local machine, develop
locally, and then send your

00:04:32.390 --> 00:04:34.230
software to a web application
stack.

00:04:34.230 --> 00:04:35.920
And it'll scale well.

00:04:35.920 --> 00:04:40.040
You can do queries using the
same software backend.

00:04:40.040 --> 00:04:42.050
You can collect data using
the same backend.

00:04:42.050 --> 00:04:46.180
But as your data gets large,
it creates challenges.

00:04:46.180 --> 00:04:48.840
So what are some
scale problems?

00:04:48.840 --> 00:04:50.820
When data sizes are small, yeah,
you can use MySQL for

00:04:50.820 --> 00:04:52.340
everything.

00:04:52.340 --> 00:04:55.050
But when it gets large, you have
to worry about the amount

00:04:55.050 --> 00:04:57.300
of data you have overwhelming
the memory that you have

00:04:57.300 --> 00:05:00.130
available or the storage
you have available.

00:05:00.130 --> 00:05:01.840
You get into performance
problems.

00:05:01.840 --> 00:05:04.330
So sometimes you have millions
of people using your web

00:05:04.330 --> 00:05:05.190
application.

00:05:05.190 --> 00:05:06.600
And it doesn't scale.

00:05:06.600 --> 00:05:08.080
You need to shard
your database.

00:05:08.080 --> 00:05:11.620
You need to have a distributed
system to make sure that this

00:05:11.620 --> 00:05:12.910
works for you.

00:05:12.910 --> 00:05:14.720
And what about asking questions
about your data?

00:05:14.720 --> 00:05:16.650
When you're using a traditional
relational

00:05:16.650 --> 00:05:19.360
database, and you need to run
aggregate queries, when you

00:05:19.360 --> 00:05:21.390
get into the hundreds of
millions of rows, the

00:05:21.390 --> 00:05:23.870
gigabytes and the terabytes,
sometimes it's impossible to

00:05:23.870 --> 00:05:26.130
run such queries.

00:05:26.130 --> 00:05:29.020
So basically, what I'm saying is
when you have massive data

00:05:29.020 --> 00:05:31.030
sets, you need to find
the right optimal

00:05:31.030 --> 00:05:33.140
technology for the task.

00:05:33.140 --> 00:05:35.390
For example, for high
availability and performance,

00:05:35.390 --> 00:05:38.170
you might want to use what
we call NoSQL database--

00:05:38.170 --> 00:05:41.330
maybe a document data store or
a key value data store--

00:05:41.330 --> 00:05:43.300
something that scales well and
is great on distributed

00:05:43.300 --> 00:05:44.120
architecture.

00:05:44.120 --> 00:05:46.230
And for analysis tools, you
might need an analysis tool

00:05:46.230 --> 00:05:48.100
that's optimized for
aggregate queries.

00:05:48.100 --> 00:05:50.750
And that might differ from
your NoSQL backend.

00:05:50.750 --> 00:05:53.040
And finally, you might need
ubiquitous storage--

00:05:53.040 --> 00:05:55.440
something that's great at
archiving, scaling--

00:05:55.440 --> 00:05:59.050
and a place to store transformed
data, so a place

00:05:59.050 --> 00:06:03.540
to store that structured data
that you just developed.

00:06:03.540 --> 00:06:04.870
So let me talk to you
a little bit.

00:06:04.870 --> 00:06:06.480
Before we get into pipelines.

00:06:06.480 --> 00:06:08.250
And when I talk about pipelines,
I'm talking about

00:06:08.250 --> 00:06:11.410
pipelining the data from
paradigm to paradigm, as

00:06:11.410 --> 00:06:12.910
necessary, while
you're scaling.

00:06:12.910 --> 00:06:14.050
But let's introduce
you to some of

00:06:14.050 --> 00:06:15.530
Google's cloud services.

00:06:15.530 --> 00:06:17.450
I just described a
NoSQL data store.

00:06:17.450 --> 00:06:18.990
We have the App Engine
data store.

00:06:18.990 --> 00:06:22.950
And hopefully, some of you
earlier were in a talk called

00:06:22.950 --> 00:06:25.220
SQL verses NoSQL, where
we talked a

00:06:25.220 --> 00:06:25.850
little bit about this.

00:06:25.850 --> 00:06:28.120
But basically, this is a
key value data store.

00:06:28.120 --> 00:06:28.950
It's highly available.

00:06:28.950 --> 00:06:30.350
It's very high performance.

00:06:30.350 --> 00:06:33.410
It allows for updating records,
creating records,

00:06:33.410 --> 00:06:34.150
deleting records.

00:06:34.150 --> 00:06:35.100
And it's got a fluid schema.

00:06:35.100 --> 00:06:36.270
It's very easy to work with.

00:06:36.270 --> 00:06:38.380
It's built for web scale
applications.

00:06:38.380 --> 00:06:40.860
And it's built on a technology
we use internally at Google

00:06:40.860 --> 00:06:42.300
called BigTable.

00:06:42.300 --> 00:06:44.470
In fact, the whole point of
some of Google's cloud

00:06:44.470 --> 00:06:46.850
services is to expose our
intrastructure to you

00:06:46.850 --> 00:06:49.880
developers to use for your own
planetary or web scale

00:06:49.880 --> 00:06:51.870
applications.

00:06:51.870 --> 00:06:53.660
Like I said, for more
information about that--

00:06:53.660 --> 00:06:55.300
I'm not going to dive too
deeply into data store.

00:06:55.300 --> 00:06:58.430
But definitely check out the
video for SQL versus NoSQL.

00:06:58.430 --> 00:07:00.920
I'm sure they figured out which
one is best and finally

00:07:00.920 --> 00:07:01.800
settled the score.

00:07:01.800 --> 00:07:04.100
Maybe next year we'll do a Vi
versus Emacs, and finally

00:07:04.100 --> 00:07:06.410
settle that score as well.

00:07:06.410 --> 00:07:06.660
OK.

00:07:06.660 --> 00:07:09.390
So Google BigQuery , this is the
project that I'm currently

00:07:09.390 --> 00:07:10.120
working on.

00:07:10.120 --> 00:07:14.080
Google BigQuery is also an
externalization of an internal

00:07:14.080 --> 00:07:16.580
software that we use
called Dremel.

00:07:16.580 --> 00:07:19.250
And there's also a research
paper, just like BigTable,

00:07:19.250 --> 00:07:19.790
about Dremel.

00:07:19.790 --> 00:07:21.570
You can read about that.

00:07:21.570 --> 00:07:24.920
So this allows you to write
SQL-like queries to analyze

00:07:24.920 --> 00:07:28.480
massive structured,
semi-structured, data sets.

00:07:28.480 --> 00:07:30.220
It uses a RESTful API.

00:07:30.220 --> 00:07:33.280
It's an append-only
data store.

00:07:33.280 --> 00:07:36.150
And unlike the App Engine
Datastore, it

00:07:36.150 --> 00:07:37.430
uses a fixed schema.

00:07:37.430 --> 00:07:39.220
So you can already see that
there's a difference between

00:07:39.220 --> 00:07:40.000
these two data stores.

00:07:40.000 --> 00:07:42.320
This is optimized for asking
questions about your data,

00:07:42.320 --> 00:07:44.800
whereas Datastore is optimized
for collecting

00:07:44.800 --> 00:07:46.340
and updating records.

00:07:46.340 --> 00:07:48.270
So if you want to use these
together, there is a

00:07:48.270 --> 00:07:48.740
difference there.

00:07:48.740 --> 00:07:50.480
It's the schema difference.

00:07:50.480 --> 00:07:52.610
Another one is that data
ingestion into BigQuery

00:07:52.610 --> 00:07:53.885
requires a CSV format.

00:07:53.885 --> 00:07:55.800
You have to stage your
data at CSV format.

00:07:55.800 --> 00:07:56.290
That's fine.

00:07:56.290 --> 00:07:58.620
So when we're building pipelines
to work with both of

00:07:58.620 --> 00:08:00.740
these technologies, we're going
to need to convert data

00:08:00.740 --> 00:08:04.470
store data first to CSV before
we use it with BigQuery.

00:08:04.470 --> 00:08:05.970
And if you've never seen
BigQuery before, this might be

00:08:05.970 --> 00:08:08.160
the first talk at I/O where
we're talking about BigQuery.

00:08:08.160 --> 00:08:11.595
Let me just show you
a quick example.

00:08:11.595 --> 00:08:15.180
So we have some public data
samples in BigQuery.

00:08:15.180 --> 00:08:16.770
We have the Wikipedia
revision history.

00:08:16.770 --> 00:08:18.150
You might have seen this
already, if you've seen us

00:08:18.150 --> 00:08:19.160
talk about this.

00:08:19.160 --> 00:08:21.940
Wikipedia revision history, up
until 2010, we're going to

00:08:21.940 --> 00:08:24.000
update that with some
newer data.

00:08:24.000 --> 00:08:29.150
So this is something like 300
million records, so kind of a

00:08:29.150 --> 00:08:30.510
small data set.

00:08:30.510 --> 00:08:33.110
So you can run things like--
just to give you an example of

00:08:33.110 --> 00:08:35.299
what you can run, you can ask
questions like, what are the

00:08:35.299 --> 00:08:38.289
top five articles based
on revision count?

00:08:38.289 --> 00:08:41.380
So you may have seen
this already.

00:08:41.380 --> 00:08:44.250
So we're querying over millions
and millions of

00:08:44.250 --> 00:08:47.250
records, hundreds of millions
of records,

00:08:47.250 --> 00:08:48.710
in just a few seconds.

00:08:48.710 --> 00:08:51.580
Hang on one second.

00:08:51.580 --> 00:08:52.020
Oh, there we go.

00:08:52.020 --> 00:08:52.850
So it took 12 seconds.

00:08:52.850 --> 00:08:55.170
But we just processed nine
gigabytes of data, hundreds of

00:08:55.170 --> 00:08:55.680
millions of records.

00:08:55.680 --> 00:08:57.960
And you can see, George W.
Bush is the most revised

00:08:57.960 --> 00:09:01.200
Wikipedia article, followed
by WWE wrestling,

00:09:01.200 --> 00:09:01.790
et cetera, et cetera.

00:09:01.790 --> 00:09:04.350
And Michael Jackson rounds
out the top five.

00:09:04.350 --> 00:09:04.670
All right.

00:09:04.670 --> 00:09:09.530
So that's just an example of
how fast BigQuery is to ask

00:09:09.530 --> 00:09:11.560
questions on aggregate
data sets.

00:09:11.560 --> 00:09:13.030
I'm not going to talk too
much more about it.

00:09:13.030 --> 00:09:15.310
We also have a couple demos.

00:09:15.310 --> 00:09:16.590
I'm not going to show
you this demo.

00:09:16.590 --> 00:09:18.180
You can see it in
another session.

00:09:18.180 --> 00:09:20.360
But QlikView is a visualization
partner.

00:09:20.360 --> 00:09:22.380
And they're actually building
dashboards with Big Query,

00:09:22.380 --> 00:09:26.010
allowing you to run real time
analysis on massive data sets,

00:09:26.010 --> 00:09:28.930
in this case, the American
birth statistics since

00:09:28.930 --> 00:09:31.160
something like 1975.

00:09:31.160 --> 00:09:33.270
And you can actually do
live queries on this.

00:09:33.270 --> 00:09:37.710
And we'll show you this
in another session.

00:09:37.710 --> 00:09:40.790
And let's see, this session will
be called Crunching Big

00:09:40.790 --> 00:09:41.760
Data with BigQuery.

00:09:41.760 --> 00:09:42.530
Definitely check it out.

00:09:42.530 --> 00:09:43.270
It's going to be really fun.

00:09:43.270 --> 00:09:45.440
I've seen the slides, and
it's going to be great.

00:09:45.440 --> 00:09:47.320
My buddies Ryan and Jordan from
the BigQuery team will be

00:09:47.320 --> 00:09:49.550
talking a little bit
more about that.

00:09:49.550 --> 00:09:50.650
And finally, Google
Cloud Storage.

00:09:50.650 --> 00:09:53.030
So Google Cloud Storage allows
you to store whatever you

00:09:53.030 --> 00:09:56.500
want, unstructured data,
binary objects, huge

00:09:56.500 --> 00:09:58.210
files in the cloud.

00:09:58.210 --> 00:10:00.370
It integrates well with
Google App Engine.

00:10:00.370 --> 00:10:02.860
So this is where we're going to
stage our large CSV files

00:10:02.860 --> 00:10:05.310
before ingestion into BigQuery,
so when we actually

00:10:05.310 --> 00:10:07.750
build our end to end
applications.

00:10:07.750 --> 00:10:08.230
It's great.

00:10:08.230 --> 00:10:09.600
It's very performant.

00:10:09.600 --> 00:10:13.140
And it's easy to use.

00:10:13.140 --> 00:10:13.390
OK.

00:10:13.390 --> 00:10:15.310
So you remember before, I talked
about the different

00:10:15.310 --> 00:10:19.730
paradigms, the different kinds
of technology, that are

00:10:19.730 --> 00:10:22.340
optimized for different
aspects here.

00:10:22.340 --> 00:10:26.090
So basically, for me, data
pipeline means that what we

00:10:26.090 --> 00:10:29.030
want to do is move data
from paradigm

00:10:29.030 --> 00:10:30.910
to paradigm to paradigm.

00:10:30.910 --> 00:10:33.460
We want to use App Engine
Datastore to collect and serve

00:10:33.460 --> 00:10:36.330
our web application, to collect
data, get back to the

00:10:36.330 --> 00:10:37.280
client machine.

00:10:37.280 --> 00:10:39.500
We want to use Google cloud
storage to store our archived

00:10:39.500 --> 00:10:42.390
data, our raw data, the
structured transformations

00:10:42.390 --> 00:10:43.750
from the Datastore,
and then Google

00:10:43.750 --> 00:10:47.350
BigQuery to do the analysis.

00:10:47.350 --> 00:10:51.290
So to me, a pipeline provides
a framework for automating

00:10:51.290 --> 00:10:52.510
these data transfers.

00:10:52.510 --> 00:10:55.080
And luckily, we have a great
library for App Engine called

00:10:55.080 --> 00:10:56.800
the App Engine Pipeline API.

00:10:56.800 --> 00:11:00.180
So this allows you to
automate workflows.

00:11:00.180 --> 00:11:03.150
So it's built on top of App
Engine task queues.

00:11:03.150 --> 00:11:06.080
And it abstracts away a lot of
the complexity of building

00:11:06.080 --> 00:11:08.880
applications that take data from
one place, do something

00:11:08.880 --> 00:11:11.080
with it, and move it
somewhere else.

00:11:11.080 --> 00:11:12.590
So let's give a quick example.

00:11:12.590 --> 00:11:14.860
This is the very simplest
example of using the Google

00:11:14.860 --> 00:11:18.080
Pipeline API written
in Python.

00:11:18.080 --> 00:11:19.530
It's the example we have
on our website.

00:11:19.530 --> 00:11:21.150
It's called the add one class.

00:11:21.150 --> 00:11:24.160
So in this case, what we're
going to do is we're going to

00:11:24.160 --> 00:11:26.095
extend the pipeline API class.

00:11:26.095 --> 00:11:29.350
And we're going to write a
function that just adds in

00:11:29.350 --> 00:11:31.300
increments and integer.

00:11:31.300 --> 00:11:33.580
And then the way we
use this is we

00:11:33.580 --> 00:11:35.220
create a pipeline object.

00:11:35.220 --> 00:11:37.420
And we just call the
start method.

00:11:37.420 --> 00:11:40.590
This will start up a task that
will run this pipeline and

00:11:40.590 --> 00:11:42.180
return a pipeline ID.

00:11:42.180 --> 00:11:44.250
And we can refer to that ID,
and see if the pipeline has

00:11:44.250 --> 00:11:46.020
completed its task, which
in this case is

00:11:46.020 --> 00:11:47.450
incrementing a number.

00:11:47.450 --> 00:11:48.150
It's pretty simple.

00:11:48.150 --> 00:11:50.260
This is as simple
as you can go.

00:11:50.260 --> 00:11:52.540
And then once we-- we're going
to check that pipeline and see

00:11:52.540 --> 00:11:53.200
if it's done.

00:11:53.200 --> 00:11:55.620
And if it's done, we're going
to print out the value.

00:11:55.620 --> 00:11:57.880
So basically, if
it's finalized,

00:11:57.880 --> 00:11:58.780
print out the value.

00:11:58.780 --> 00:12:01.660
So you can see in just a few
lines of code, we've actually

00:12:01.660 --> 00:12:04.600
created sort of an asynchronous
data pipeline,

00:12:04.600 --> 00:12:06.430
where all we need to do
is feed it a number.

00:12:06.430 --> 00:12:08.420
And it will give us a result
based on the transformation

00:12:08.420 --> 00:12:12.890
that we're employing, which
is just increment.

00:12:12.890 --> 00:12:14.870
The other cool thing about the
pipeline API is you can

00:12:14.870 --> 00:12:16.950
connect two pipelines
together.

00:12:16.950 --> 00:12:19.430
You can take the output
from one and pipe that

00:12:19.430 --> 00:12:20.610
into another pipeline.

00:12:20.610 --> 00:12:24.230
So in this case, I've actually
taken that add one pipeline,

00:12:24.230 --> 00:12:28.050
and I'm piping the result from
the first pipeline into

00:12:28.050 --> 00:12:29.000
another add one pipeline.

00:12:29.000 --> 00:12:30.570
So basically, it's add two.

00:12:30.570 --> 00:12:32.560
So I take the first step.

00:12:32.560 --> 00:12:33.310
I get a result.

00:12:33.310 --> 00:12:35.960
And then once the result
is done, I yield that.

00:12:35.960 --> 00:12:37.170
I generate the value.

00:12:37.170 --> 00:12:39.110
And I put it into the
pipeline again.

00:12:42.620 --> 00:12:43.540
So here's an example.

00:12:43.540 --> 00:12:45.400
This is actually the code
that I just showed you.

00:12:45.400 --> 00:12:48.340
I added a little Ajax
query, so you

00:12:48.340 --> 00:12:49.210
can just see it working.

00:12:49.210 --> 00:12:50.930
It's pretty trivial.

00:12:50.930 --> 00:12:51.440
Let's see.

00:12:51.440 --> 00:12:52.140
Somebody pick a number.

00:12:52.140 --> 00:12:53.020
I heard you say 42.

00:12:53.020 --> 00:12:54.410
Great.

00:12:54.410 --> 00:12:57.760
Let's see if the demo gods
are favoring me today.

00:12:57.760 --> 00:12:58.980
Oh, 43, OK.

00:12:58.980 --> 00:13:01.150
So what this is doing is it's
just calling that code I just

00:13:01.150 --> 00:13:01.610
showed you.

00:13:01.610 --> 00:13:03.540
It's just taking this
number, it's sending

00:13:03.540 --> 00:13:04.510
it to an App Engine.

00:13:04.510 --> 00:13:06.490
This is actually an App Engine
application running live.

00:13:06.490 --> 00:13:08.600
I'm sending it to an App Engine
application, starting

00:13:08.600 --> 00:13:11.370
the pipeline, checking every
second to see if it's done.

00:13:11.370 --> 00:13:12.770
When it's done, return
the number.

00:13:12.770 --> 00:13:14.580
And of course, we
have add two.

00:13:14.580 --> 00:13:19.760
So you can add two to
the number, 123.

00:13:19.760 --> 00:13:20.970
And there you go.

00:13:20.970 --> 00:13:22.690
So this is an example
of the code I just

00:13:22.690 --> 00:13:24.360
showed you running live.

00:13:24.360 --> 00:13:26.610
The other cool thing about the
pipeline API is you can just

00:13:26.610 --> 00:13:29.370
start up pipelines
asynchronously.

00:13:29.370 --> 00:13:30.530
I could run two at once.

00:13:30.530 --> 00:13:35.620
So I can say, one and
five or four.

00:13:35.620 --> 00:13:37.190
And I can run these
at the same time.

00:13:37.190 --> 00:13:39.720
So these can all be running
asynchronously.

00:13:39.720 --> 00:13:41.360
So basically, with just a
little bit of code, I've

00:13:41.360 --> 00:13:42.650
actually created a workflow.

00:13:42.650 --> 00:13:44.720
I've created a workflow that
increments integers.

00:13:44.720 --> 00:13:48.100
Now this is probably overkill
for incrementing integers.

00:13:48.100 --> 00:13:49.580
You're using the power
of Google's clouds

00:13:49.580 --> 00:13:50.540
to increment a number.

00:13:50.540 --> 00:13:55.260
But this sort of illustrates
what we can do with this API.

00:13:55.260 --> 00:13:55.750
OK.

00:13:55.750 --> 00:13:58.480
So the pipeline API--

00:13:58.480 --> 00:14:00.310
a couple more things to
say about it is--

00:14:00.310 --> 00:14:04.010
it lets you write these kind
of distributed asynchronous

00:14:04.010 --> 00:14:06.480
workflows using procedural
code.

00:14:06.480 --> 00:14:08.530
I can literally just
say, start here.

00:14:08.530 --> 00:14:09.760
Move the data here.

00:14:09.760 --> 00:14:10.670
Do this.

00:14:10.670 --> 00:14:13.670
And then the result will be x.

00:14:13.670 --> 00:14:15.970
It kind of lets you define
sources and syncs--

00:14:15.970 --> 00:14:19.210
and we'll show you a little
bit with that later--

00:14:19.210 --> 00:14:21.440
where your inputs can
be a certain thing.

00:14:21.440 --> 00:14:23.940
It can be a number.

00:14:23.940 --> 00:14:25.840
You can get input from
the Datastore.

00:14:25.840 --> 00:14:27.900
You can get input from
files or from the

00:14:27.900 --> 00:14:29.940
BlobStore in App Engine.

00:14:29.940 --> 00:14:31.710
And I'll show you this
a little later too.

00:14:31.710 --> 00:14:33.990
It provides a useful dashboard
for viewing your progress,

00:14:33.990 --> 00:14:36.270
because when you kick off a
pipeline, it's running on App

00:14:36.270 --> 00:14:38.190
Engine task queues in
the background.

00:14:38.190 --> 00:14:40.010
And so, you may not see--

00:14:40.010 --> 00:14:41.070
it's hard to see what's
going on.

00:14:41.070 --> 00:14:42.700
Of course, you can go to the App
Engine control panel, and

00:14:42.700 --> 00:14:44.250
check out the task
queues there.

00:14:44.250 --> 00:14:46.360
But this provides you a cool
dashboard to sort of see

00:14:46.360 --> 00:14:48.410
what's going on.

00:14:48.410 --> 00:14:51.900
So we talked a little bit about
the pipeline API for

00:14:51.900 --> 00:14:52.700
developing work flows.

00:14:52.700 --> 00:14:54.990
But what about that whole
transformation step?

00:14:54.990 --> 00:14:59.990
If I have gigabytes of data, how
am I going to distribute

00:14:59.990 --> 00:15:02.730
the transformation across
a number of machines

00:15:02.730 --> 00:15:04.120
to make it go fast?

00:15:04.120 --> 00:15:06.850
And what can I add to this
framework, or let's say map to

00:15:06.850 --> 00:15:10.290
this framework, to reduce the
pain of dealing with this?

00:15:10.290 --> 00:15:11.960
Well, MapReduce, of course.

00:15:11.960 --> 00:15:14.610
So MapReduce is an
abstraction.

00:15:14.610 --> 00:15:17.690
It's a powerful abstraction for
dealing with distributed

00:15:17.690 --> 00:15:18.760
data processing.

00:15:18.760 --> 00:15:22.420
We use it extensively
at Google.

00:15:22.420 --> 00:15:25.920
And it's used all over in
open source movement

00:15:25.920 --> 00:15:29.340
implementations built into all
kinds of data processing

00:15:29.340 --> 00:15:31.200
applications.

00:15:31.200 --> 00:15:33.730
And I'm going to show you
the MapReduce 101.

00:15:33.730 --> 00:15:36.950
But basically, it's a way to
take certain kinds of data

00:15:36.950 --> 00:15:39.500
processing tasks and distribute
them to multiple

00:15:39.500 --> 00:15:42.590
machines, to break them down
and to run them all in

00:15:42.590 --> 00:15:45.840
parallel to get them
done faster.

00:15:45.840 --> 00:15:47.920
So the Hello World
of MapReduce is--

00:15:47.920 --> 00:15:49.580
I'm sure some of you know this,
but in case you're not

00:15:49.580 --> 00:15:50.830
familiar with it-- the
Hello World of

00:15:50.830 --> 00:15:52.960
MapReduce is word counts.

00:15:52.960 --> 00:15:56.260
Word count lends itself really
well to MapReduce, because

00:15:56.260 --> 00:15:58.020
it's easy to distribute
the tasks into

00:15:58.020 --> 00:15:59.500
individual work units.

00:15:59.500 --> 00:16:02.800
So you can actually send them
off into individual nodes, and

00:16:02.800 --> 00:16:04.190
then aggregate the
results later.

00:16:04.190 --> 00:16:07.740
So of course, the first step of
a MapReduce word count is

00:16:07.740 --> 00:16:09.620
to have a collection
of documents.

00:16:09.620 --> 00:16:11.010
Three documents is not a lot.

00:16:11.010 --> 00:16:13.510
But you might be doing this on
300 million documents or the

00:16:13.510 --> 00:16:15.510
entire web.

00:16:15.510 --> 00:16:17.760
The first step is a map phase.

00:16:17.760 --> 00:16:19.510
This is a very simplistic
view of MapReduce.

00:16:19.510 --> 00:16:21.380
But really, this is all
there is to it.

00:16:21.380 --> 00:16:22.350
The first step is a map phase.

00:16:22.350 --> 00:16:23.980
We're going to map through
each document.

00:16:23.980 --> 00:16:26.920
And we're going to do an
individual word count.

00:16:26.920 --> 00:16:29.870
What we're going to do is we're
going to emit a key

00:16:29.870 --> 00:16:30.560
value pair.

00:16:30.560 --> 00:16:32.450
One, the key, will
be the word.

00:16:32.450 --> 00:16:34.030
And the value will be
the word count of

00:16:34.030 --> 00:16:35.160
an individual document.

00:16:35.160 --> 00:16:37.460
So we'll send each document
through this function to map

00:16:37.460 --> 00:16:38.370
through it.

00:16:38.370 --> 00:16:39.540
And it will go through
a shuffle phase.

00:16:39.540 --> 00:16:42.190
And this is sort of the magic
computationally intensive part

00:16:42.190 --> 00:16:44.910
of MapReduce, where we're
going to aggregate

00:16:44.910 --> 00:16:46.450
the results by key.

00:16:46.450 --> 00:16:48.630
So we're going to put all
the hellos in one place.

00:16:48.630 --> 00:16:50.190
And we're going to take all the
worlds, and put them in

00:16:50.190 --> 00:16:50.760
another place.

00:16:50.760 --> 00:16:55.060
And so, we're going to sort
through the data by key.

00:16:55.060 --> 00:16:56.430
And finally, we're going to
aggregate the result.

00:16:56.430 --> 00:16:58.800
We're going to go through what
we call a reduce phase.

00:16:58.800 --> 00:17:01.980
And we're going to take all
the individual hello--

00:17:01.980 --> 00:17:04.020
the dictionary that we created
with hello and the dictionary

00:17:04.020 --> 00:17:06.910
we created with world-- and
we're going to sum up the

00:17:06.910 --> 00:17:09.329
results, and then provide you
with, well, here's the final

00:17:09.329 --> 00:17:10.540
word count.

00:17:10.540 --> 00:17:13.030
The first step, sometimes we
call this the fan out, where

00:17:13.030 --> 00:17:15.210
we're mapping through and we're
fanning this out to

00:17:15.210 --> 00:17:16.359
multiple machines.

00:17:16.359 --> 00:17:19.470
And the reduce step, sometimes
we call that the fan in.

00:17:19.470 --> 00:17:22.010
And as you can see here, this
really lends itself well to

00:17:22.010 --> 00:17:24.900
distributive processing, because
each individual unit,

00:17:24.900 --> 00:17:26.470
each individual little
box here, can be

00:17:26.470 --> 00:17:27.460
on a separate machine.

00:17:27.460 --> 00:17:29.600
And when you have hundreds of
millions of documents to

00:17:29.600 --> 00:17:33.800
process, farming this out to
many, many nodes really helps,

00:17:33.800 --> 00:17:36.940
many workers.

00:17:36.940 --> 00:17:39.730
So that's basically--

00:17:39.730 --> 00:17:40.460
it seems simple.

00:17:40.460 --> 00:17:43.110
But what's not simple about this
is when I kind of hinted

00:17:43.110 --> 00:17:44.170
with the shuffle sort.

00:17:44.170 --> 00:17:47.440
What's not simple is
coordinating all of these

00:17:47.440 --> 00:17:50.640
different units, waiting for one
process to end before you

00:17:50.640 --> 00:17:51.750
get to the next one.

00:17:51.750 --> 00:17:54.220
Keeping track of each of
the individual workers.

00:17:54.220 --> 00:17:55.530
All of this is very complex.

00:17:55.530 --> 00:17:58.430
And when you're dealing with
a huge distributed network,

00:17:58.430 --> 00:18:00.560
that's where the kind of
MapReduce framework and all of

00:18:00.560 --> 00:18:02.760
these open source libraries
around MapReduce are really

00:18:02.760 --> 00:18:05.760
solving this distribution
problem.

00:18:05.760 --> 00:18:09.340
So as I just said, there's
not just Hadoop.

00:18:09.340 --> 00:18:11.310
But there's actually lots
of open source MapReduce

00:18:11.310 --> 00:18:13.030
implementations out there.

00:18:13.030 --> 00:18:14.470
And here's another one.

00:18:14.470 --> 00:18:15.730
The App Engine MapReduce
library.

00:18:15.730 --> 00:18:19.230
So App Engine has a MapReduce
library built on top of the

00:18:19.230 --> 00:18:22.010
pipeline's API that lets you
distribute your data

00:18:22.010 --> 00:18:24.260
processing task across
many machines.

00:18:24.260 --> 00:18:25.130
So it's open source.

00:18:25.130 --> 00:18:26.770
It's built on top
of task queues.

00:18:26.770 --> 00:18:31.380
And it stores individual data so
that every time you do one

00:18:31.380 --> 00:18:32.880
of these aggregations,
there's an individual

00:18:32.880 --> 00:18:34.230
data processing step.

00:18:34.230 --> 00:18:38.090
And the result of those steps
is stored in BlobStore.

00:18:38.090 --> 00:18:39.710
And of course, all the workflows
are coordinated with

00:18:39.710 --> 00:18:40.770
the pipeline API.

00:18:40.770 --> 00:18:43.340
I showed you the add one pipe,
where I'm just taking data

00:18:43.340 --> 00:18:45.810
from one place and piping
it through to another.

00:18:45.810 --> 00:18:47.760
This really coordinates
a lot of activity.

00:18:47.760 --> 00:18:50.210
So you have the shuffle
sort, and the reduce,

00:18:50.210 --> 00:18:51.190
and the mapper phase.

00:18:51.190 --> 00:18:52.460
And these are all being
coordinated with the

00:18:52.460 --> 00:18:53.630
pipeline's API.

00:18:53.630 --> 00:18:56.310
What's amazing is, using the
pipeline's API as a framework

00:18:56.310 --> 00:18:58.500
for the Google App Engine
MapReduce, the amount of code

00:18:58.500 --> 00:18:59.850
you have to write to coordinate
these kinds of

00:18:59.850 --> 00:19:01.680
tasks is really, really small.

00:19:01.680 --> 00:19:04.080
And I'll show you right now
what it looks like.

00:19:04.080 --> 00:19:07.330
So remember our add one pipe
code, where I was just saying,

00:19:07.330 --> 00:19:09.300
here is an input, do something
with it, and

00:19:09.300 --> 00:19:11.530
then yield a result?

00:19:11.530 --> 00:19:13.150
An App Engine MapReduce
pipeline is

00:19:13.150 --> 00:19:13.900
built on top of that.

00:19:13.900 --> 00:19:14.780
And it looks kind
of like this.

00:19:14.780 --> 00:19:15.660
It looks like a lot of code.

00:19:15.660 --> 00:19:18.170
But actually, it's
pretty trivial.

00:19:18.170 --> 00:19:21.480
The first thing you need is
just a name of the step.

00:19:21.480 --> 00:19:24.200
So this is going to be the name
of the pipeline step.

00:19:24.200 --> 00:19:26.390
And in the MapReduce
pipeline, we're

00:19:26.390 --> 00:19:27.690
going to call two functions.

00:19:27.690 --> 00:19:28.950
We're going to call
a mapper function

00:19:28.950 --> 00:19:30.130
and a reducer function.

00:19:30.130 --> 00:19:32.420
And in this particular
implementation, you don't have

00:19:32.420 --> 00:19:34.190
to worry about your
shuffle sort.

00:19:34.190 --> 00:19:36.480
It'll run the shuffle
sort by itself.

00:19:36.480 --> 00:19:38.440
So the mapper function-- for
example, the word count, maybe

00:19:38.440 --> 00:19:40.600
the mapper function is counting
each individual word,

00:19:40.600 --> 00:19:42.530
and then yielding that
key value pair

00:19:42.530 --> 00:19:43.220
that I talked about.

00:19:43.220 --> 00:19:45.750
And then the reduce will take
all the intermediate steps and

00:19:45.750 --> 00:19:47.510
aggregate them together.

00:19:47.510 --> 00:19:49.120
The other thing you can define--
if you can see it

00:19:49.120 --> 00:19:51.870
here-- is a source.

00:19:51.870 --> 00:19:53.990
So in this case, I'm asking
it to pull out

00:19:53.990 --> 00:19:55.050
data from the Datastore.

00:19:55.050 --> 00:19:56.290
And we have a Datastore
input reader.

00:19:56.290 --> 00:19:59.090
Actually, we have lots of input
and output readers.

00:19:59.090 --> 00:20:02.430
And then I'm saying, sink the
data into the Google Cloud

00:20:02.430 --> 00:20:04.120
Storage, which is our
file output writer.

00:20:04.120 --> 00:20:05.370
And that can also write
to BlobStore.

00:20:05.370 --> 00:20:06.930
We have other input,
output readers.

00:20:06.930 --> 00:20:08.520
We have readers that can read

00:20:08.520 --> 00:20:11.300
compressed data out of BlobStore.

00:20:11.300 --> 00:20:12.860
If those of you who were in
the code lab this morning

00:20:12.860 --> 00:20:15.010
about this, we also have an
App Engine logs reader.

00:20:15.010 --> 00:20:18.390
So there's all kinds of ways you
can read data in and out.

00:20:18.390 --> 00:20:20.100
And then we can send
the parameters to

00:20:20.100 --> 00:20:21.310
the custom map parameter.

00:20:21.310 --> 00:20:22.440
Maybe there's a particular--

00:20:22.440 --> 00:20:25.580
for example, I want to tell
the Datastore reader which

00:20:25.580 --> 00:20:27.390
entity to read out
of Datastore.

00:20:27.390 --> 00:20:28.320
There might be different
things.

00:20:28.320 --> 00:20:30.930
And for the reducer, I
might want to say--

00:20:30.930 --> 00:20:31.990
or I do need to say--

00:20:31.990 --> 00:20:34.100
which Google Cloud Storage
bucket I want to use.

00:20:34.100 --> 00:20:38.230
So you can just send those as a
list or a dictionary to the

00:20:38.230 --> 00:20:39.020
mapper functions.

00:20:39.020 --> 00:20:41.730
And then shards, you can
tell this pipeline how

00:20:41.730 --> 00:20:43.280
many workers to run.

00:20:43.280 --> 00:20:45.040
This is going to be
how many tasks get

00:20:45.040 --> 00:20:46.140
spun up on App Engine.

00:20:46.140 --> 00:20:48.760
It doesn't necessarily correlate
with instances.

00:20:48.760 --> 00:20:50.790
And I'll talk about that
later in the end.

00:20:50.790 --> 00:20:52.420
And finally, this can be
your whole pipeline.

00:20:52.420 --> 00:20:55.360
And that could run and dump the
resulting data into Google

00:20:55.360 --> 00:20:56.150
Cloud Storage.

00:20:56.150 --> 00:20:58.600
But you can also pipe this
to another pipeline.

00:20:58.600 --> 00:21:00.430
So I can say, when this is done,
then take that result

00:21:00.430 --> 00:21:02.910
and do something else with it,
and do something else with it.

00:21:02.910 --> 00:21:04.900
I talked to a couple people
internally at Google who are

00:21:04.900 --> 00:21:08.150
using this same process for
some ad related stuff.

00:21:08.150 --> 00:21:11.540
And they're doing like four,
five, six joins, piping the

00:21:11.540 --> 00:21:14.000
results here, doing this,
doing another MapReduce.

00:21:14.000 --> 00:21:14.890
It's pretty complex.

00:21:14.890 --> 00:21:19.400
But it's really easy to define
these using this API.

00:21:19.400 --> 00:21:20.900
So I'm going to show you
some real code from

00:21:20.900 --> 00:21:23.140
some real use cases.

00:21:23.140 --> 00:21:25.100
But before I go on, I just want
to talk about why use

00:21:25.100 --> 00:21:27.700
Google App Engine when you
have all sorts of other

00:21:27.700 --> 00:21:31.760
choices for data processing
tasks.

00:21:31.760 --> 00:21:34.110
So first and foremost, scaling
things can be really time

00:21:34.110 --> 00:21:36.080
consuming and difficult,
especially if you're moving

00:21:36.080 --> 00:21:39.280
from that relational database on
a web, like a LAMPStack, to

00:21:39.280 --> 00:21:40.500
something different, because
you're moving

00:21:40.500 --> 00:21:41.820
from software to software.

00:21:41.820 --> 00:21:43.780
Sometimes you're moving
from hosting provider

00:21:43.780 --> 00:21:44.660
to something else.

00:21:44.660 --> 00:21:47.600
And you have to learn how to use
the new software, optimize

00:21:47.600 --> 00:21:51.090
it, worry about the pitfalls
of tuning.

00:21:51.090 --> 00:21:53.990
That can be time consuming
and expensive.

00:21:53.990 --> 00:21:55.880
So no need to buy new hardware
infrastructures.

00:21:55.880 --> 00:21:57.870
A common pattern, actually, that
I've seen, working with

00:21:57.870 --> 00:22:00.960
customers, is that they'll have
some web application,

00:22:00.960 --> 00:22:03.110
then they'll put the data
internally into some internal

00:22:03.110 --> 00:22:05.070
data center, and then pipe
that data somewhere else.

00:22:05.070 --> 00:22:07.160
And all those things
are time consuming.

00:22:07.160 --> 00:22:09.460
And there's lots of places
where that can fail.

00:22:09.460 --> 00:22:11.240
And a lot of places where
you need to optimize

00:22:11.240 --> 00:22:12.520
those kind of pipes.

00:22:12.520 --> 00:22:14.210
And so, doing everything
on App Engine

00:22:14.210 --> 00:22:16.740
really simplifies that.

00:22:16.740 --> 00:22:18.750
It's easy to kind of
predict your costs.

00:22:18.750 --> 00:22:20.470
I always maintain that when
you move from software

00:22:20.470 --> 00:22:23.810
paradigm to software paradigm,
from relational to NoSQL,

00:22:23.810 --> 00:22:26.050
there's a cost there of
unpredictable cost.

00:22:26.050 --> 00:22:28.680
Because you don't know how well
the particular software

00:22:28.680 --> 00:22:30.720
will perform with
your use case.

00:22:30.720 --> 00:22:35.660
Or you may not know how best
to tune your stack.

00:22:35.660 --> 00:22:37.090
So there are always
unpredictable costs when you

00:22:37.090 --> 00:22:39.920
move from system to system.

00:22:39.920 --> 00:22:41.390
And finally, App Engine
is a great

00:22:41.390 --> 00:22:42.720
application development framework.

00:22:42.720 --> 00:22:45.650
It's a scalable web application
stack, like

00:22:45.650 --> 00:22:47.170
everything you need to build
a web app is there.

00:22:47.170 --> 00:22:49.830
So if you're writing a game and
you want to collect data

00:22:49.830 --> 00:22:53.870
for that game, analyze the data,
serve it back to your

00:22:53.870 --> 00:22:56.220
customers or your players,
something like that,

00:22:56.220 --> 00:22:57.500
everything you need is there.

00:22:57.500 --> 00:22:59.070
So you've got the web
application stack.

00:22:59.070 --> 00:23:00.510
You've got the Datastore.

00:23:00.510 --> 00:23:01.950
You can do analysis
with BigQuery,

00:23:01.950 --> 00:23:03.220
Cloud Storage for archiving.

00:23:03.220 --> 00:23:04.640
We provide a lot of things.

00:23:04.640 --> 00:23:06.050
And so it's just a great
place to be to

00:23:06.050 --> 00:23:08.630
write a web scale app.

00:23:08.630 --> 00:23:10.270
And it really just simplifies
development.

00:23:10.270 --> 00:23:12.500
That's why I like to use it.

00:23:12.500 --> 00:23:13.200
All right.

00:23:13.200 --> 00:23:14.470
So let's look at these
in practice.

00:23:14.470 --> 00:23:18.600
I'm going to show you some
actual code and some things

00:23:18.600 --> 00:23:21.760
that we've built just to sort
of test out these ideas.

00:23:21.760 --> 00:23:23.780
And also from working with
customers, I'll show you some

00:23:23.780 --> 00:23:24.790
of the typical use cases.

00:23:24.790 --> 00:23:28.030
I'll actually show you
two, in particular.

00:23:28.030 --> 00:23:31.430
So the first one is just a
simple data store mapper.

00:23:31.430 --> 00:23:32.990
Since I'm on the BigQuery team,
a lot of my time is

00:23:32.990 --> 00:23:35.350
spent worrying about how to
get large amounts, like

00:23:35.350 --> 00:23:36.830
gigabytes and terabytes,
of data into

00:23:36.830 --> 00:23:38.770
BigQuery for analysis.

00:23:38.770 --> 00:23:40.300
It's not as simple
as it seems.

00:23:40.300 --> 00:23:42.890
A lot of people don't have
terabytes of data.

00:23:42.890 --> 00:23:45.520
But a lot of people have a
lot of data in Datastore.

00:23:45.520 --> 00:23:46.790
And so I get this question
all the time.

00:23:46.790 --> 00:23:49.540
How do I take my data and
convert it into something that

00:23:49.540 --> 00:23:51.440
can used with BigQuery easily?

00:23:51.440 --> 00:23:55.390
So in this case, we're going to
convert Datastore data from

00:23:55.390 --> 00:23:58.060
the App Engine Datastore into,
first, a CSV file.

00:23:58.060 --> 00:24:00.930
And then we're going to ingest
that into BigQuery.

00:24:00.930 --> 00:24:03.470
And we're also going to do a
really simple transformation.

00:24:03.470 --> 00:24:05.800
We're going to transform
a timestamp.

00:24:05.800 --> 00:24:09.570
So BigQuery supports four data
types, integer, floating

00:24:09.570 --> 00:24:12.330
point, Boolean, and string.

00:24:12.330 --> 00:24:14.900
It doesn't have a unique
date time stamp.

00:24:14.900 --> 00:24:18.340
So date formats in BigQuery
should be converted to Unix

00:24:18.340 --> 00:24:20.600
Epoch time stamps
or into integer.

00:24:20.600 --> 00:24:23.690
But Datastore data can support
different kinds of time stamps

00:24:23.690 --> 00:24:24.760
and also different data types.

00:24:24.760 --> 00:24:26.670
So oftentimes, we have
to convert that data.

00:24:26.670 --> 00:24:28.310
It's a pretty trivial
transformation.

00:24:28.310 --> 00:24:32.595
But it lends itself great for
MapReduce when you're building

00:24:32.595 --> 00:24:34.800
these kind of applications.

00:24:34.800 --> 00:24:36.210
So just to simplify,
we're going to

00:24:36.210 --> 00:24:37.690
take Datastore entities.

00:24:37.690 --> 00:24:40.140
We're going to transform them,
put them in the Cloud Storage,

00:24:40.140 --> 00:24:44.530
and then pipe them into Google
BigQuery at the end.

00:24:44.530 --> 00:24:48.190
In this case, I wrote a little
sample app that uses an App

00:24:48.190 --> 00:24:50.830
Engine backend to collect
Google + statuses.

00:24:50.830 --> 00:24:53.790
I'm actually just collecting the
date it was published, the

00:24:53.790 --> 00:24:57.875
person who published it, the
URL, and the summary-- we call

00:24:57.875 --> 00:25:00.380
it the title-- the summary
of the post.

00:25:00.380 --> 00:25:01.840
So I wrote this little
backend.

00:25:01.840 --> 00:25:04.360
It just collects.

00:25:04.360 --> 00:25:05.930
It's running on a chron.

00:25:05.930 --> 00:25:07.050
Like every five minutes,
it collects

00:25:07.050 --> 00:25:08.800
some data from Google+.

00:25:08.800 --> 00:25:10.560
In fact, you can do
it yourself with

00:25:10.560 --> 00:25:11.750
the Google APIs Explorer.

00:25:11.750 --> 00:25:13.560
I'll show you which
API call I used.

00:25:13.560 --> 00:25:17.140
First, let me authorize
myself.

00:25:17.140 --> 00:25:17.630
Looks good.

00:25:17.630 --> 00:25:21.000
So this is an online API.

00:25:21.000 --> 00:25:21.840
You can use this yourself.

00:25:21.840 --> 00:25:24.430
Actually, it's on the Google
Developer site.

00:25:24.430 --> 00:25:29.620
So I'm going to run a query for
any Google+ posts recently

00:25:29.620 --> 00:25:30.560
that mention Google.

00:25:30.560 --> 00:25:32.040
Hopefully, there's one or two.

00:25:32.040 --> 00:25:35.100
So the JSON response looks
kind of like this.

00:25:35.100 --> 00:25:38.310
Yeah, I'm just collecting the
URL and, I think, the

00:25:38.310 --> 00:25:40.000
name and the title.

00:25:40.000 --> 00:25:42.380
So title's like a summary
of the post, itself.

00:25:42.380 --> 00:25:43.890
It's not all the text.

00:25:43.890 --> 00:25:45.680
And I'm taking that every
five minutes.

00:25:45.680 --> 00:25:47.480
And I'm dumping it into
the data store.

00:25:47.480 --> 00:25:50.880
So I'm kind of creating this
chron job streaming thing.

00:25:50.880 --> 00:25:52.900
You can do this with
any Datastore data.

00:25:52.900 --> 00:25:54.900
If you have a mobile application
and people are

00:25:54.900 --> 00:25:57.010
checking in, you could just use
App Engine, collect those

00:25:57.010 --> 00:25:59.920
check ins, dump them into
Datastore continuously.

00:25:59.920 --> 00:26:03.060
So that's basically what
I'm doing there.

00:26:03.060 --> 00:26:04.860
So that's piping
into Datastore.

00:26:04.860 --> 00:26:07.380
And then what I want to do is
take all that Datastore data--

00:26:07.380 --> 00:26:09.750
I actually didn't collect a lot,
because I want this demo

00:26:09.750 --> 00:26:12.120
to run fast enough
for this talk.

00:26:12.120 --> 00:26:13.910
If you've got gigabytes and
gigabytes of data, it might

00:26:13.910 --> 00:26:15.510
take a few minutes to do
these kind of MapReduce

00:26:15.510 --> 00:26:16.260
transformations.

00:26:16.260 --> 00:26:20.010
But I want to transform that
data from Datastore into Cloud

00:26:20.010 --> 00:26:22.250
Storage, into a CSV format.

00:26:22.250 --> 00:26:25.170
And then, once that's done, once
that workflow is done, I

00:26:25.170 --> 00:26:27.850
want to pipe that data
into Google BigQuery.

00:26:27.850 --> 00:26:30.940
So that's the end of
my data pipeline.

00:26:30.940 --> 00:26:33.590
For you BI people out there,
this is sort of a classic

00:26:33.590 --> 00:26:36.830
extract, transform,
load operation.

00:26:36.830 --> 00:26:39.510
This is kind of the business
lingo for BI when they talk

00:26:39.510 --> 00:26:42.090
about taking data from different
data silos, and then

00:26:42.090 --> 00:26:44.310
transforming it into something
they can work with, and then

00:26:44.310 --> 00:26:46.680
loading it into another
application.

00:26:46.680 --> 00:26:49.340
Because oftentimes, the data
you collect in a large

00:26:49.340 --> 00:26:51.870
business is siloed across
multiple data stores.

00:26:51.870 --> 00:26:53.620
So this is one way--

00:26:53.620 --> 00:26:56.465
this is sort of an example of
that kind of transformed to

00:26:56.465 --> 00:26:58.260
this sort of data pipeline
paradigm.

00:26:58.260 --> 00:27:00.950
And we'll talk a little bit
about a more complex silo

00:27:00.950 --> 00:27:03.330
problem in the next example.

00:27:03.330 --> 00:27:04.020
All right.

00:27:04.020 --> 00:27:04.880
So here's the code.

00:27:04.880 --> 00:27:07.630
So I created something called
the iterator pipeline.

00:27:07.630 --> 00:27:09.770
It's doing almost exactly
what I described before.

00:27:09.770 --> 00:27:11.250
But there's no reduce phase.

00:27:11.250 --> 00:27:14.060
I'm just mapping through every
single Datastore data entity.

00:27:14.060 --> 00:27:16.340
I'm doing a simple
transformation on that record.

00:27:16.340 --> 00:27:18.310
And I'm just dumping it
into a large file in

00:27:18.310 --> 00:27:20.050
Google Cloud Storage.

00:27:20.050 --> 00:27:22.570
So I've defined a
iterator, a map.

00:27:22.570 --> 00:27:24.100
This is actually the
mapper pipeline.

00:27:24.100 --> 00:27:25.550
It's another one of
the pipelines in

00:27:25.550 --> 00:27:26.730
the MapReduce library.

00:27:26.730 --> 00:27:27.940
It doesn't have a
reduce phase.

00:27:27.940 --> 00:27:29.730
So it's slightly simpler.

00:27:29.730 --> 00:27:32.590
And I'm telling it to put the
data into a Google storage

00:27:32.590 --> 00:27:34.510
bucket that I control.

00:27:34.510 --> 00:27:35.670
I'm saying output
sharding none.

00:27:35.670 --> 00:27:37.120
That just means it's going
to make one big file.

00:27:37.120 --> 00:27:39.820
You can say one big file or one
file per worker, if you

00:27:39.820 --> 00:27:41.140
want to do that.

00:27:41.140 --> 00:27:44.070
And then at the end, I'm saying
yield the result to

00:27:44.070 --> 00:27:46.300
this other pipeline that I
created called Google Cloud

00:27:46.300 --> 00:27:48.320
Storage to BigQuery.

00:27:48.320 --> 00:27:50.190
And here's the transformation
I'm trying to do.

00:27:50.190 --> 00:27:52.410
That's the time stamp,
that format, that

00:27:52.410 --> 00:27:53.420
comes out of Google+.

00:27:53.420 --> 00:27:55.350
It's like a UTC timestamp.

00:27:55.350 --> 00:27:58.260
I want to transform it into a
Unix Epoch timestamp, which

00:27:58.260 --> 00:27:58.860
looks like that.

00:27:58.860 --> 00:28:00.310
It's just a big integer.

00:28:00.310 --> 00:28:02.830
And so I'm incorporating a super
simple transformation.

00:28:02.830 --> 00:28:04.990
So every record, I'm looking
for that time stamp.

00:28:04.990 --> 00:28:06.450
And I'm just converting it.

00:28:06.450 --> 00:28:08.350
And that's it.

00:28:08.350 --> 00:28:12.340
And here's the Cloud Storage
to BigQuery pipeline.

00:28:12.340 --> 00:28:14.140
So this is just another
pipeline.

00:28:14.140 --> 00:28:17.760
Once all of that stuff is done
and the file has been created,

00:28:17.760 --> 00:28:21.310
the CSV file, I'm going to kick
off this new pipeline.

00:28:21.310 --> 00:28:23.330
So this is what's great about
the pipelines API is that it

00:28:23.330 --> 00:28:25.260
lets you finish your job before

00:28:25.260 --> 00:28:26.480
kicking off another one.

00:28:26.480 --> 00:28:29.590
So once everything is done, all
the data's been mapped out

00:28:29.590 --> 00:28:32.730
of Datastore and saved in the
files API, I'm going to kick

00:28:32.730 --> 00:28:37.300
off this ingestion job
into BigQuery.

00:28:37.300 --> 00:28:37.940
So here's an example.

00:28:37.940 --> 00:28:40.040
This is actual data from
the Datastore.

00:28:40.040 --> 00:28:41.250
Hopefully, it's large enough
for you can read.

00:28:41.250 --> 00:28:43.710
I just pulled out some random
ones to display here in this

00:28:43.710 --> 00:28:44.400
App Engine app.

00:28:44.400 --> 00:28:48.180
You can see the data format,
Late Show with Craig Ferguson,

00:28:48.180 --> 00:28:49.590
something about Morgan
Freeman.

00:28:49.590 --> 00:28:52.050
There's Lotus F1 team has
something to say.

00:28:52.050 --> 00:28:54.840
So this is just random data
I collected from Google+.

00:28:54.840 --> 00:28:57.100
So let's see if this'll work.

00:28:57.100 --> 00:28:59.880
I'm also going to open up my
Google Cloud Storage bucket.

00:28:59.880 --> 00:29:02.480
So this is the bucket where I'm
going to dump the data.

00:29:02.480 --> 00:29:03.190
So right now it's empty.

00:29:03.190 --> 00:29:05.620
And let me just make sure
it's still empty.

00:29:05.620 --> 00:29:08.710
So let's kick this
pipeline off.

00:29:08.710 --> 00:29:09.610
We're going to move some data.

00:29:09.610 --> 00:29:12.190
Here is the dashboard
that I talked about.

00:29:12.190 --> 00:29:14.080
Looks complex, don't worry
about all the details.

00:29:14.080 --> 00:29:16.470
I'm just going to show
you what it's doing.

00:29:16.470 --> 00:29:18.440
So this is going to start
the pipeline.

00:29:18.440 --> 00:29:20.570
And just like my little simple
pipeline example-- where I had

00:29:20.570 --> 00:29:23.010
the little spinner, and it was
checking to see-- this is

00:29:23.010 --> 00:29:25.160
doing kind of the same thing,
but on a larger scale.

00:29:25.160 --> 00:29:26.990
It's showing me which phase
of the pipeline

00:29:26.990 --> 00:29:28.290
it's on right now.

00:29:28.290 --> 00:29:30.290
And it's refreshing like every
10 seconds or something.

00:29:30.290 --> 00:29:31.060
Let me auto refresh it.

00:29:31.060 --> 00:29:31.400
OK.

00:29:31.400 --> 00:29:32.780
So now the entire pipeline
is done.

00:29:32.780 --> 00:29:35.160
I only had a few thousand
entities in there, I think.

00:29:35.160 --> 00:29:38.070
So basically, it went through
each pipeline, my iterator.

00:29:38.070 --> 00:29:39.270
It mapped through the data.

00:29:39.270 --> 00:29:40.820
And then it piped the
result into Google

00:29:40.820 --> 00:29:42.180
Cloud Storage to BigQuery.

00:29:42.180 --> 00:29:43.400
So that's pretty much it.

00:29:43.400 --> 00:29:46.220
You can inspect each
individual thing.

00:29:46.220 --> 00:29:48.250
I think I had 16 shards
for this function.

00:29:48.250 --> 00:29:50.710
It shows you how much data
each shard processed.

00:29:50.710 --> 00:29:52.790
So they're not equivalent,
because it was just grabbing

00:29:52.790 --> 00:29:55.780
data out of the Datastore
and how long it took.

00:29:55.780 --> 00:29:58.340
It took, whatever, 13
seconds to run that.

00:29:58.340 --> 00:29:59.930
And then I can look here.

00:29:59.930 --> 00:30:02.810
And this is the object in
cloud storage that it's

00:30:02.810 --> 00:30:04.010
working on to pipe
into BigQuery.

00:30:04.010 --> 00:30:05.140
So let's go back to
Cloud Storage and

00:30:05.140 --> 00:30:06.800
see if this is complete.

00:30:06.800 --> 00:30:07.480
Let me refresh.

00:30:07.480 --> 00:30:08.952
So there it is.

00:30:08.952 --> 00:30:13.370
Let me grab that real fast.

00:30:13.370 --> 00:30:15.180
And you can just take
a look at it.

00:30:15.180 --> 00:30:16.600
Let's open it with Vi.

00:30:19.240 --> 00:30:20.990
So can you see that?

00:30:20.990 --> 00:30:23.650
So as you can see, it's
just the same data.

00:30:23.650 --> 00:30:25.060
But it's now CSV format.

00:30:25.060 --> 00:30:29.550
And I've converted all the
timestamps to Unix Epoch.

00:30:29.550 --> 00:30:30.500
OK, great.

00:30:30.500 --> 00:30:32.210
So that looks good.

00:30:32.210 --> 00:30:36.770
The BigQuery ingestion step
takes a minute or two, because

00:30:36.770 --> 00:30:38.060
we stage the data in Google
Cloud Storage.

00:30:38.060 --> 00:30:39.300
And we tell BigQuery
to grab it.

00:30:39.300 --> 00:30:41.620
And it batches it and ingests
it into a format we can do

00:30:41.620 --> 00:30:42.720
queries on.

00:30:42.720 --> 00:30:43.500
That might take a minute.

00:30:43.500 --> 00:30:45.640
I'm not going to wait for that
step if it's still running.

00:30:45.640 --> 00:30:48.250
But OK, it looks like-- let me
just refresh this real fast.

00:30:48.250 --> 00:30:50.130
But I did this yesterday just
to have-- oh, there it is.

00:30:50.130 --> 00:30:50.540
It's already done.

00:30:50.540 --> 00:30:51.890
So this is actually done.

00:30:51.890 --> 00:30:53.340
I think it's this one.

00:30:53.340 --> 00:30:53.790
So here it is.

00:30:53.790 --> 00:30:55.770
So it's actually ingested
all those entities.

00:30:55.770 --> 00:30:56.680
There were only a
few thousand.

00:30:56.680 --> 00:30:58.750
But it's ingested all those
entities into BigQuery.

00:30:58.750 --> 00:31:00.740
And I can look at it
and see the data.

00:31:00.740 --> 00:31:04.560
And I can run queries.

00:31:04.560 --> 00:31:06.440
I could run a query like,
what are the top

00:31:06.440 --> 00:31:12.440
five people that post?

00:31:12.440 --> 00:31:14.970
Like, who are the biggest
posters, things like that.

00:31:14.970 --> 00:31:16.230
Actually, let's just
run that real fast.

00:31:24.350 --> 00:31:29.610
So this is called
Datastore data.

00:31:29.610 --> 00:31:31.860
So I'm just writing
a quick query.

00:31:34.730 --> 00:31:36.860
I didn't know what the name of
my newly created table was,

00:31:36.860 --> 00:31:38.730
because I named it after
the time stamp, the

00:31:38.730 --> 00:31:41.580
current time stamp.

00:31:41.580 --> 00:31:50.280
So let's just do like the
top posts where the post

00:31:50.280 --> 00:31:51.370
contains the word--

00:31:51.370 --> 00:31:52.620
I don't know--

00:31:55.430 --> 00:31:56.270
Google.

00:31:56.270 --> 00:31:59.740
Let's see if there's
even any in here.

00:31:59.740 --> 00:32:00.990
Ignore case.

00:32:05.104 --> 00:32:07.540
The top, and let's call--
let's do the

00:32:07.540 --> 00:32:08.790
names, the top name.

00:32:14.127 --> 00:32:16.526
Top five name and the count
from there, let's see

00:32:16.526 --> 00:32:17.776
if that will work.

00:32:25.190 --> 00:32:28.030
Oh, thank you.

00:32:28.030 --> 00:32:29.280
Good eyes, everyone.

00:32:32.970 --> 00:32:33.230
All right.

00:32:33.230 --> 00:32:33.610
There you go.

00:32:33.610 --> 00:32:35.460
The Hangout Show mentions
Google a lot.

00:32:35.460 --> 00:32:36.340
They probably say--

00:32:36.340 --> 00:32:37.280
I don't know what The
Hangout Show is.

00:32:37.280 --> 00:32:39.440
But I'm assuming it's some kind
of Google hangout show.

00:32:39.440 --> 00:32:41.230
And they're probably saying,
join our Google hangout.

00:32:43.810 --> 00:32:45.200
And The Crisis Show
on Google Hangout.

00:32:45.200 --> 00:32:45.980
I don't want to watch that.

00:32:45.980 --> 00:32:46.960
I know what that is.

00:32:46.960 --> 00:32:48.080
Anyway, so there's an example.

00:32:48.080 --> 00:32:49.700
So I've just built a data
pipeline application.

00:32:49.700 --> 00:32:50.850
Now imagine if I was collecting

00:32:50.850 --> 00:32:51.690
lots and lots of data.

00:32:51.690 --> 00:32:54.570
I was collecting all the Google+
posts from everybody I

00:32:54.570 --> 00:32:58.030
know or collecting
data from a game.

00:32:58.030 --> 00:32:59.590
So I could collect lots
of data from a game.

00:32:59.590 --> 00:33:00.690
I'm a developer.

00:33:00.690 --> 00:33:03.740
My analyst for my company comes
up and says, at what

00:33:03.740 --> 00:33:05.860
point-- at what level do people
drop off and go play

00:33:05.860 --> 00:33:06.600
another game?

00:33:06.600 --> 00:33:09.660
And I could pipe that data into
BigQuery and tell them

00:33:09.660 --> 00:33:11.650
with my application, which
is also the backend

00:33:11.650 --> 00:33:12.620
for my mobile game.

00:33:12.620 --> 00:33:15.210
So you can sort of see how you
can use this sort of MapReduce

00:33:15.210 --> 00:33:18.200
and Pipelines libraries
to build this.

00:33:18.200 --> 00:33:20.990
So we had a code lab this
morning, actually, that showed

00:33:20.990 --> 00:33:24.650
you how to do exactly what I did
but with App Engine logs.

00:33:24.650 --> 00:33:26.010
The slides of this presentation

00:33:26.010 --> 00:33:27.380
will be on the website.

00:33:27.380 --> 00:33:28.900
So you can actually refer
to this later.

00:33:28.900 --> 00:33:30.630
But the code lab is here.

00:33:30.630 --> 00:33:32.180
And you can kind of check
out how to do it.

00:33:32.180 --> 00:33:33.860
It's very cool.

00:33:33.860 --> 00:33:34.950
OK, here's another example.

00:33:34.950 --> 00:33:36.270
This is a more complex
example.

00:33:36.270 --> 00:33:38.640
This is another example
everybody asks me about.

00:33:38.640 --> 00:33:43.760
It's how to join two siloed data
stores into a single data

00:33:43.760 --> 00:33:47.610
store or database
by a unique key.

00:33:47.610 --> 00:33:51.310
So imagine two data
store entities.

00:33:51.310 --> 00:33:54.120
So you have an App Engine data
store entity called products.

00:33:54.120 --> 00:33:56.680
And this is a list or a catalog
of all the products

00:33:56.680 --> 00:33:58.490
that you're selling.

00:33:58.490 --> 00:34:00.150
So in this case, I have
an example of

00:34:00.150 --> 00:34:01.550
self driving glasses.

00:34:01.550 --> 00:34:03.400
I don't know why I wrote that,
self driving glasses

00:34:03.400 --> 00:34:04.890
manufactured by Google.

00:34:04.890 --> 00:34:06.790
And it has unique ID three.

00:34:06.790 --> 00:34:09.710
So this is your product--

00:34:09.710 --> 00:34:12.989
your database of the warehouse
or your description of what

00:34:12.989 --> 00:34:13.690
you're selling.

00:34:13.690 --> 00:34:16.409
And for sales, you've got
every time one of these

00:34:16.409 --> 00:34:18.880
products is sold, you also
have that unique ID.

00:34:18.880 --> 00:34:20.810
You have the store
that it sold at.

00:34:20.810 --> 00:34:22.139
And you have the date
and a bunch of other

00:34:22.139 --> 00:34:23.530
information, of course.

00:34:23.530 --> 00:34:25.110
But oftentimes, these are
in different places.

00:34:25.110 --> 00:34:26.949
And you want to put
them in one place.

00:34:26.949 --> 00:34:29.409
BigQuery is really great at
analyzing data that's in a

00:34:29.409 --> 00:34:31.739
single denormalized database.

00:34:31.739 --> 00:34:34.489
So this is kind of a normalized
database, where you

00:34:34.489 --> 00:34:36.690
might have one thing
per record.

00:34:36.690 --> 00:34:39.639
In BigQuery, you can have a
sparse database with redundant

00:34:39.639 --> 00:34:40.840
values and nulls.

00:34:40.840 --> 00:34:43.790
And it works really well on
single table data, where you

00:34:43.790 --> 00:34:45.230
don't have to run a join.

00:34:45.230 --> 00:34:47.260
So what I'm going to do is I'm
going to take this data, I'm

00:34:47.260 --> 00:34:49.639
going to join in on that
key, which is the SKU.

00:34:49.639 --> 00:34:51.469
It's the product identification
number.

00:34:51.469 --> 00:34:53.840
I'm going to take all that data
and join it together.

00:34:53.840 --> 00:34:55.510
And then I'm going to write
it out to a file that

00:34:55.510 --> 00:34:56.760
BigQuery can use.

00:34:59.130 --> 00:35:02.210
This is one example where the
workflow diagram is more

00:35:02.210 --> 00:35:03.680
complicated than the
actual code.

00:35:03.680 --> 00:35:05.420
So I'll show you the
workflow diagram.

00:35:05.420 --> 00:35:06.840
This is a lot like the
MapReduce sample

00:35:06.840 --> 00:35:08.290
I showed you before.

00:35:08.290 --> 00:35:10.510
So the first step I'm going to
do is map through each of

00:35:10.510 --> 00:35:11.290
these separately.

00:35:11.290 --> 00:35:14.070
I'm going to create two mapper
pipelines that are separate.

00:35:14.070 --> 00:35:15.860
And the first mapper pipeline
is going to go through the

00:35:15.860 --> 00:35:16.910
Datastore entities.

00:35:16.910 --> 00:35:18.880
And it's going to pull out
a key and a value.

00:35:18.880 --> 00:35:21.550
And the key will be that
product identification.

00:35:21.550 --> 00:35:23.260
And the value is going to
be all the other data.

00:35:23.260 --> 00:35:24.990
I'm just going to treat
it as a big string.

00:35:24.990 --> 00:35:27.140
I'm going to write it out
as a big CSV string.

00:35:27.140 --> 00:35:29.140
And I'm going to do that with
the sales database, as well.

00:35:29.140 --> 00:35:32.830
So I'll have two different
aggregate mappers.

00:35:32.830 --> 00:35:35.650
But the thing that's similar
between them is the key.

00:35:35.650 --> 00:35:39.670
There'll be a unique
key for each one.

00:35:39.670 --> 00:35:41.230
I'm going to do this
big shuffle sort.

00:35:41.230 --> 00:35:43.090
And I'm going to aggregate
them by key.

00:35:43.090 --> 00:35:46.080
So I'm going to take all the ID
ones, and I'm going to put

00:35:46.080 --> 00:35:48.300
them in one place, ID twos
and put them in one

00:35:48.300 --> 00:35:49.500
place, and ID threes.

00:35:49.500 --> 00:35:52.080
And this is another example
of a great for distributed

00:35:52.080 --> 00:35:52.920
processing.

00:35:52.920 --> 00:35:55.800
So I'm doing these big joins.

00:35:55.800 --> 00:35:57.610
I'm going to let a different
worker work on

00:35:57.610 --> 00:35:58.690
each of these processes.

00:35:58.690 --> 00:36:01.080
So this is a great way to
do distributed work.

00:36:01.080 --> 00:36:02.890
And then my reduce step, I'm
going to take all the

00:36:02.890 --> 00:36:03.400
aggregates.

00:36:03.400 --> 00:36:06.470
And I'm going to, for each
individual node in this step,

00:36:06.470 --> 00:36:11.050
for each one of these,
I'm going to

00:36:11.050 --> 00:36:12.330
concatenate them together.

00:36:12.330 --> 00:36:15.180
So I'm going to say for every
key one product, I'm going to

00:36:15.180 --> 00:36:18.770
join it together with every
other sales example.

00:36:18.770 --> 00:36:21.170
I'm going to dump that into a
big CSV file, and stick it

00:36:21.170 --> 00:36:21.580
into BigQuery.

00:36:21.580 --> 00:36:22.880
It looks very complicated.

00:36:22.880 --> 00:36:26.250
The code is much simpler
than the flow chart.

00:36:26.250 --> 00:36:27.720
The code looks like this.

00:36:27.720 --> 00:36:29.730
Two data store mappers--

00:36:29.730 --> 00:36:31.250
just like I showed you before--
iterate through the

00:36:31.250 --> 00:36:33.100
data store.

00:36:33.100 --> 00:36:35.270
I'm going to call this pipeline
join on SKU, because

00:36:35.270 --> 00:36:37.530
that's what I'm doing.

00:36:37.530 --> 00:36:39.280
I'm going to do something
called-- with the pipeline,

00:36:39.280 --> 00:36:40.790
I'm going to use a class called
extend or a method

00:36:40.790 --> 00:36:41.700
called extend.

00:36:41.700 --> 00:36:44.730
And I'm going to take both
mapper outputs, and I'm going

00:36:44.730 --> 00:36:47.110
to dump them into a new output
called all data.

00:36:47.110 --> 00:36:48.000
It's really simple.

00:36:48.000 --> 00:36:50.580
So basically, I'm taking two
data streams, and I'm piping

00:36:50.580 --> 00:36:52.800
the results together
in something

00:36:52.800 --> 00:36:53.890
I'm calling all data.

00:36:53.890 --> 00:36:55.790
And then I'm going to send
that to the shuffle sort.

00:36:55.790 --> 00:36:57.890
I'm going to say, shuffle sort,
take all of this data

00:36:57.890 --> 00:37:01.110
and do whatever magic thing
that you do with it.

00:37:01.110 --> 00:37:03.720
Take all the files generated by
all data and shuffle them.

00:37:03.720 --> 00:37:06.580
Aggregate them by key.

00:37:06.580 --> 00:37:09.050
And finally, I'm going to take
the result and stick it into a

00:37:09.050 --> 00:37:09.860
reduce pipeline.

00:37:09.860 --> 00:37:11.440
And that looks a lot
like the other

00:37:11.440 --> 00:37:12.160
pipelines you've seen before.

00:37:12.160 --> 00:37:15.530
It's just going to run a reduce
function over all that

00:37:15.530 --> 00:37:19.260
aggregate data, and then dump
the result into Google Cloud

00:37:19.260 --> 00:37:21.710
Storage bucket in CSV format.

00:37:21.710 --> 00:37:23.510
And then my reduce function
looks like-- well,

00:37:23.510 --> 00:37:24.130
I ran out of space.

00:37:24.130 --> 00:37:25.350
But it can be anything
you want.

00:37:25.350 --> 00:37:26.440
You can actually do a join.

00:37:26.440 --> 00:37:28.580
But this is also great
for aggregate count.

00:37:28.580 --> 00:37:32.580
You might want to see for every
item what store sold the

00:37:32.580 --> 00:37:33.540
most per item.

00:37:33.540 --> 00:37:36.510
And if you're a huge retailer
or you're a retailer working

00:37:36.510 --> 00:37:40.930
with loyalty cards or even if
you're somebody who's a game

00:37:40.930 --> 00:37:43.430
maker, you can use this kind of
processing to do these huge

00:37:43.430 --> 00:37:44.970
data processing tasks
and answer these

00:37:44.970 --> 00:37:46.480
really difficult questions.

00:37:46.480 --> 00:37:47.510
So you could do a count.

00:37:47.510 --> 00:37:49.400
I'm going to do a join.

00:37:49.400 --> 00:37:51.780
You name it.

00:37:51.780 --> 00:37:52.880
So let's look at it now.

00:37:52.880 --> 00:37:53.740
This is another example.

00:37:53.740 --> 00:37:57.180
This is actual data from an
App Engine app that I just

00:37:57.180 --> 00:37:58.530
randomly made stuff up.

00:37:58.530 --> 00:38:01.910
Here is Nexus tablet and Android
Jellybeans, and here

00:38:01.910 --> 00:38:03.460
are the manufacturers.

00:38:03.460 --> 00:38:06.360
So these are the IDs
of the actual data.

00:38:06.360 --> 00:38:08.570
And here are where
they were sold.

00:38:08.570 --> 00:38:11.840
So item number one, the
Galaxy Nexus, was

00:38:11.840 --> 00:38:13.610
sold at the web store.

00:38:13.610 --> 00:38:16.550
And item number four, YouTube
t-shirt, was sold at the New

00:38:16.550 --> 00:38:17.720
York store.

00:38:17.720 --> 00:38:21.380
Similar data connected by ID but
in different datastores.

00:38:21.380 --> 00:38:23.940
OK, so let's run this pipeline,
and see what it

00:38:23.940 --> 00:38:25.190
looks like.

00:38:29.050 --> 00:38:29.900
Demo gods?

00:38:29.900 --> 00:38:30.730
Yes, there we go.

00:38:30.730 --> 00:38:32.150
So it looks a little
more complicated

00:38:32.150 --> 00:38:32.840
than the one before.

00:38:32.840 --> 00:38:34.350
The one before, we really
had just two steps.

00:38:34.350 --> 00:38:36.750
It was like map and
then dump it into.

00:38:36.750 --> 00:38:38.630
This one is running
two data store

00:38:38.630 --> 00:38:40.410
mappers at the same time.

00:38:40.410 --> 00:38:41.740
Let me refresh this manually.

00:38:41.740 --> 00:38:42.740
So it's already finished one.

00:38:42.740 --> 00:38:46.170
There's not a lot of data
in this App Engine app.

00:38:46.170 --> 00:38:47.500
But basically it's
running both.

00:38:47.500 --> 00:38:49.870
And it's coordinating
the streams into a

00:38:49.870 --> 00:38:51.070
single shuffle step.

00:38:51.070 --> 00:38:52.110
So you don't have to do--

00:38:52.110 --> 00:38:54.372
all the code that I showed you
was really the entire code for

00:38:54.372 --> 00:38:57.360
this application-- besides the
little like the Chrome that

00:38:57.360 --> 00:38:59.320
actually holds the application
together--

00:38:59.320 --> 00:39:00.270
that's all you need to write.

00:39:00.270 --> 00:39:02.720
And the pipelines API and the
MapReduce library take care of

00:39:02.720 --> 00:39:04.000
all the rest.

00:39:04.000 --> 00:39:05.360
So it's already done with it.

00:39:05.360 --> 00:39:06.490
It's just mapped through
the datastore.

00:39:06.490 --> 00:39:07.580
It's sent both of
those streams.

00:39:07.580 --> 00:39:09.940
It's extending them to
the shuffle sort.

00:39:09.940 --> 00:39:11.470
It's running that now.

00:39:11.470 --> 00:39:12.820
This might take about
a minute.

00:39:12.820 --> 00:39:15.040
But once it's done with the
shuffle service, it's going to

00:39:15.040 --> 00:39:18.260
kick off the reduce step
all by itself.

00:39:18.260 --> 00:39:21.740
And hopefully, at the end of
this we'll have a new CSV file

00:39:21.740 --> 00:39:23.100
in Google Cloud Storage.

00:39:23.100 --> 00:39:25.740
And while this is running, I'm
just going to say, I've kicked

00:39:25.740 --> 00:39:28.890
off all of these MapReduce jobs
by clicking on a button,

00:39:28.890 --> 00:39:30.670
which is a valid way
to do things.

00:39:30.670 --> 00:39:32.640
You can also automate this
stuff of course.

00:39:32.640 --> 00:39:35.510
You could run a chron job that
kicks off a nightly MapReduce

00:39:35.510 --> 00:39:37.655
pipeline and does all these
things automatically.

00:39:37.655 --> 00:39:39.550
And that's probably what a
lot of people would do

00:39:39.550 --> 00:39:41.250
when they use this.

00:39:41.250 --> 00:39:46.450
The other thing you can do is
you can stop these in the

00:39:46.450 --> 00:39:48.630
middle or emit messages
during the pipeline.

00:39:48.630 --> 00:39:51.250
Like when one pipeline
is complete, you can

00:39:51.250 --> 00:39:52.220
send an email out.

00:39:52.220 --> 00:39:54.130
You could update the web app.

00:39:54.130 --> 00:39:56.510
There's all kinds of ways you
can use these to have a better

00:39:56.510 --> 00:39:59.710
user experience for people who
are interacting with these

00:39:59.710 --> 00:40:02.030
things through the apps.

00:40:02.030 --> 00:40:04.060
So this is going to
run for a while.

00:40:04.060 --> 00:40:05.330
I don't want to take
up too much time.

00:40:05.330 --> 00:40:08.210
We can actually come back to
this in just a second.

00:40:08.210 --> 00:40:09.370
Let's go to the next slide.

00:40:09.370 --> 00:40:11.035
And I'll pop back over
here and see how the

00:40:11.035 --> 00:40:11.830
MapReduce is going.

00:40:11.830 --> 00:40:13.600
It's going to take
another minute.

00:40:13.600 --> 00:40:16.040
OK, so performances and
best practices, here's

00:40:16.040 --> 00:40:16.760
what you can adjust.

00:40:16.760 --> 00:40:20.340
I already talked about
the shards per job.

00:40:20.340 --> 00:40:23.210
So shards just corresponds
to tasks or workers.

00:40:23.210 --> 00:40:27.330
It does not necessarily
correspond to instances.

00:40:27.330 --> 00:40:29.020
App Engine scales for you.

00:40:29.020 --> 00:40:31.680
And it'll choose the
amount of what we

00:40:31.680 --> 00:40:33.080
call front end instances.

00:40:33.080 --> 00:40:37.940
Those are basically sort of
analogous to a node or a

00:40:37.940 --> 00:40:40.070
computer running on a
distributed network to

00:40:40.070 --> 00:40:40.690
fulfill that job.

00:40:40.690 --> 00:40:45.370
So when you kick off a task,
it may spin up several

00:40:45.370 --> 00:40:47.810
instances with many tasks
running on each instance.

00:40:47.810 --> 00:40:52.460
Or if your job is
computationally intensive, it

00:40:52.460 --> 00:40:56.360
might actually spin up more
instances to handle it.

00:40:56.360 --> 00:40:57.830
But that's the thing
you adjust.

00:40:57.830 --> 00:41:00.660
And I think it's best to try to
use as few as possible for

00:41:00.660 --> 00:41:01.750
the data you're using.

00:41:01.750 --> 00:41:05.880
But if you spin up many, many
tasks-- like, for example, say

00:41:05.880 --> 00:41:09.990
I want to run this process with
15 shards or 20 shards--

00:41:09.990 --> 00:41:12.330
it's going to take longer than
running a distributive process

00:41:12.330 --> 00:41:13.890
with 500 shards.

00:41:13.890 --> 00:41:15.060
So you can actually
control that.

00:41:15.060 --> 00:41:17.530
You can also control that to
kind of stay under a quota.

00:41:17.530 --> 00:41:19.550
If you're trying to do a daily
quota, and you don't want to

00:41:19.550 --> 00:41:25.030
burst and do you like six
million datastore reads per

00:41:25.030 --> 00:41:25.930
hour, or whatever.

00:41:25.930 --> 00:41:27.040
You can adjust that.

00:41:27.040 --> 00:41:28.350
You can also adjust
instance size.

00:41:28.350 --> 00:41:29.590
You probably don't have
to do this much.

00:41:29.590 --> 00:41:31.900
But occasionally, you'll run
into a computational task that

00:41:31.900 --> 00:41:34.590
uses up all the memory on
a front end instance.

00:41:34.590 --> 00:41:36.950
And you might need to bump
up to a higher instance.

00:41:36.950 --> 00:41:38.420
You shouldn't have to worry
about that too much.

00:41:38.420 --> 00:41:40.190
If you're doing document
processing, like large

00:41:40.190 --> 00:41:42.020
documents, that may happen.

00:41:42.020 --> 00:41:44.190
And then task queue settings,
since this is all based on

00:41:44.190 --> 00:41:46.240
task queues, you can tweak
the task queues somewhat.

00:41:46.240 --> 00:41:49.060
Say I want to have more task
queues running at a particular

00:41:49.060 --> 00:41:53.950
time or just the refresh
rate or what have you.

00:41:53.950 --> 00:41:55.540
OK, let's go back to this and
see if it's still running.

00:41:55.540 --> 00:41:56.730
OK, it's done.

00:41:56.730 --> 00:41:57.560
So this thing is done.

00:41:57.560 --> 00:42:01.450
It took about a minute,
two minutes.

00:42:01.450 --> 00:42:02.650
Let's go to Google Cloud
Storage, see if

00:42:02.650 --> 00:42:04.140
there's a new file.

00:42:04.140 --> 00:42:05.720
OK, there is.

00:42:05.720 --> 00:42:06.970
Let me grab that.

00:42:11.740 --> 00:42:14.300
And let me open this one up.

00:42:14.300 --> 00:42:14.900
So there it is.

00:42:14.900 --> 00:42:16.180
Yeah, there wasn't much data
in there to begin with.

00:42:16.180 --> 00:42:19.080
But we've taken all that data
from different data stores,

00:42:19.080 --> 00:42:21.810
and we've munged it
together into CSV.

00:42:21.810 --> 00:42:24.500
I didn't add the cloud storage
to BigQuery pipe.

00:42:24.500 --> 00:42:25.730
But it's trivial to add that.

00:42:25.730 --> 00:42:30.370
I can just type this data into
BigQuery very simply.

00:42:30.370 --> 00:42:33.090
So that is an example
of the join.

00:42:33.090 --> 00:42:35.320
All right.

00:42:35.320 --> 00:42:37.670
OK, one more thing
to talk about.

00:42:37.670 --> 00:42:39.640
We have something internally
called--

00:42:39.640 --> 00:42:41.390
experimental for you
developers to use--

00:42:41.390 --> 00:42:43.830
experimental high performance
shuffle.

00:42:43.830 --> 00:42:45.010
We call it the BigShuffle.

00:42:45.010 --> 00:42:47.810
When your data gets really
large, like gigabytes and

00:42:47.810 --> 00:42:50.910
gigabytes of records, I told
you about the shuffle sort.

00:42:50.910 --> 00:42:52.530
It's computationally
intensive.

00:42:52.530 --> 00:42:58.000
So BigShuffle is a way to use
our shuffle infrastructure to

00:42:58.000 --> 00:42:59.700
run these big shuffle sorts.

00:42:59.700 --> 00:43:01.480
Normally, all of the stuff
is in user space

00:43:01.480 --> 00:43:03.640
or App Engine space.

00:43:03.640 --> 00:43:06.280
But sometimes, you need more
power to do these massive

00:43:06.280 --> 00:43:07.990
shuffle sorts, where
all of those

00:43:07.990 --> 00:43:08.960
aggregates are being built.

00:43:08.960 --> 00:43:13.740
And sorting is happening
for your MapReduce job.

00:43:13.740 --> 00:43:15.770
So anyway, we're currently
accepting a limited amount of

00:43:15.770 --> 00:43:17.290
BigShuffle testers.

00:43:17.290 --> 00:43:19.210
I'll update these slides
if you're interested.

00:43:19.210 --> 00:43:21.920
Or you can just come to me and
talk to me about how to become

00:43:21.920 --> 00:43:22.630
one of these shuffle testers.

00:43:22.630 --> 00:43:25.540
But I'll put a link for you to
fill out your information on

00:43:25.540 --> 00:43:28.190
this slide on the Google
I/O website.

00:43:28.190 --> 00:43:29.270
So you can actually
try it out.

00:43:29.270 --> 00:43:31.030
If you've got a lot of data,
we'd love to talk to you.

00:43:31.030 --> 00:43:33.890
And we'd love you to try out
this shuffle service, and

00:43:33.890 --> 00:43:35.620
build a pipeline that takes
advantage of our

00:43:35.620 --> 00:43:38.180
infrastructure.

00:43:38.180 --> 00:43:38.690
Let me go back.

00:43:38.690 --> 00:43:41.260
So let me just wrap up.

00:43:41.260 --> 00:43:43.800
I stole this from another talk,
from Jordan's talk,

00:43:43.800 --> 00:43:44.510
who's sitting right here.

00:43:44.510 --> 00:43:47.160
But build so that your
computation is close to where

00:43:47.160 --> 00:43:48.050
your data lives.

00:43:48.050 --> 00:43:49.160
I think this is a good adage.

00:43:49.160 --> 00:43:52.140
This really helps you build
these end to end pipelines and

00:43:52.140 --> 00:43:53.180
make them efficient.

00:43:53.180 --> 00:43:56.270
When you're dealing with data
that's all in the web.

00:43:56.270 --> 00:43:57.150
It's all in the same place.

00:43:57.150 --> 00:44:03.300
It's all being computed on by
a single software entity.

00:44:03.300 --> 00:44:05.100
Worry about your app and not
your infrastructure.

00:44:05.100 --> 00:44:06.790
You guys are software
developers.

00:44:06.790 --> 00:44:07.730
Some of you may be hardware

00:44:07.730 --> 00:44:08.940
developers or building clusters.

00:44:08.940 --> 00:44:11.610
But if you're a software
developer building a web app--

00:44:11.610 --> 00:44:14.030
building a game, building a
mobile device, a backend for

00:44:14.030 --> 00:44:15.270
your Android app--

00:44:15.270 --> 00:44:16.280
you want to worry
about the app.

00:44:16.280 --> 00:44:17.790
You don't want to worry about
all this infrastructure.

00:44:17.790 --> 00:44:20.720
So these libraries and all of
this technology that we're

00:44:20.720 --> 00:44:23.150
offering for you is to help
you build this stuff.

00:44:23.150 --> 00:44:24.790
And finally, the code
is so simple.

00:44:24.790 --> 00:44:25.660
Keep it simple.

00:44:25.660 --> 00:44:27.830
Keep it easy to maintain
and easy to test.

00:44:27.830 --> 00:44:30.530
So that's what we're going for
here is to help you do that.

00:44:30.530 --> 00:44:31.930
So I'll leave you with that.

00:44:31.930 --> 00:44:35.210
And if you guys have any
questions, now's the time.

00:44:35.210 --> 00:44:36.522
Thank you very much.

00:44:36.522 --> 00:44:42.700
[APPLAUSE]

00:44:42.700 --> 00:44:43.140
MICHAEL MANOOCHEHRI: Cool.

00:44:43.140 --> 00:44:44.280
So remind me.

00:44:44.280 --> 00:44:46.235
Do people come up to the
mike for questions?

00:44:46.235 --> 00:44:46.710
AUDIENCE: Yes, they do.

00:44:46.710 --> 00:44:47.080
MICHAEL MANOOCHEHRI:
All right.

00:44:47.080 --> 00:44:49.115
So there's a microphone set up
if you have any questions.

00:44:55.280 --> 00:44:57.750
AUDIENCE: If you have a large
process running, let's say, in

00:44:57.750 --> 00:45:01.560
the pipeline you've got
like five steps.

00:45:01.560 --> 00:45:03.520
And let's say the first
two steps use a lot of

00:45:03.520 --> 00:45:05.200
computational resources.

00:45:05.200 --> 00:45:08.480
At the third step, you
have a failure.

00:45:08.480 --> 00:45:12.480
How do you restart your pipeline
from the third step

00:45:12.480 --> 00:45:14.300
so you're not redoing those
first two steps?

00:45:14.300 --> 00:45:14.520
MICHAEL MANOOCHEHRI: Right.

00:45:14.520 --> 00:45:16.330
So there's different ways
you can do that.

00:45:16.330 --> 00:45:17.320
I've actually been playing
with this.

00:45:17.320 --> 00:45:20.540
One way is that you store the
aggregate steps in the

00:45:20.540 --> 00:45:22.470
BlobStore or in Google
Cloud Storage.

00:45:22.470 --> 00:45:23.520
You can actually do that.

00:45:23.520 --> 00:45:26.610
So I would recommend if you're
running a huge task to do not

00:45:26.610 --> 00:45:28.390
delete some of those
intermediate steps.

00:45:28.390 --> 00:45:30.240
The MapReduce pipeline actually
has a cleanup phase.

00:45:30.240 --> 00:45:31.650
But you don't have
to run that.

00:45:31.650 --> 00:45:33.410
You can actually run this,
and then save the

00:45:33.410 --> 00:45:34.770
aggregate steps in--

00:45:34.770 --> 00:45:37.740
I would use Google Cloud
Storage for that.

00:45:37.740 --> 00:45:40.910
AUDIENCE: But the pipeline
itself doesn't have any type

00:45:40.910 --> 00:45:46.382
of mechanism to restart from a
different step and say-- you

00:45:46.382 --> 00:45:49.470
know, so you see the failure
at step three.

00:45:49.470 --> 00:45:53.830
And you come in the next day,
let's say, you resolve that.

00:45:53.830 --> 00:45:57.080
And it doesn't have
a way to restart?

00:45:57.080 --> 00:45:57.160
MICHAEL MANOOCHEHRI:
It depends on

00:45:57.160 --> 00:45:58.540
what the failure is.

00:45:58.540 --> 00:46:01.020
So if failures are network
failures or something that's

00:46:01.020 --> 00:46:03.930
recoverable, it will actually
try to recover itself.

00:46:03.930 --> 00:46:05.440
It won't abort the process
completely.

00:46:05.440 --> 00:46:07.840
If it's something like the data
itself is in a format

00:46:07.840 --> 00:46:10.080
that the next step can't use
or you've done something

00:46:10.080 --> 00:46:12.520
completely wrong, then we don't
have a mechanism to

00:46:12.520 --> 00:46:13.480
restart that way.

00:46:13.480 --> 00:46:14.830
Thank you very much for
coming to this talk.

00:46:14.830 --> 00:46:16.680
And catch me afterwards if
you have more questions.

