WEBVTT
Kind: captions
Language: en

00:00:00.460 --> 00:00:02.469
Now we're going to talk about a pretty cool new

00:00:02.469 --> 00:00:05.300
tool. Something that goes between the map and the reduce

00:00:05.300 --> 00:00:08.530
phase, called the combiner. Before we do that, let's review

00:00:08.530 --> 00:00:10.780
how you may have calculated the mean in the last

00:00:10.780 --> 00:00:14.050
problem. And I don't know exactly what you did,

00:00:14.050 --> 00:00:16.440
but one thing you may have done is, something like

00:00:16.440 --> 00:00:21.040
this. First, you probably had your mapper go through the

00:00:21.040 --> 00:00:24.390
data. I should say your mappers, go through the data,

00:00:25.400 --> 00:00:29.500
and output, key value pairs that looked

00:00:29.500 --> 00:00:34.110
something like, day of week, amount spent. Then,

00:00:34.110 --> 00:00:39.050
for each day of the week, your reducer probably kept a total of the sum and

00:00:39.050 --> 00:00:42.140
a count. So actually maybe your value here,

00:00:42.140 --> 00:00:44.520
was the amount of money spent. And then

00:00:44.520 --> 00:00:48.080
also a one, to help increment the count.

00:00:48.080 --> 00:00:51.250
And then you probably would divide the sum

00:00:51.250 --> 00:00:53.560
by the count, and it looks pretty good. But

00:00:53.560 --> 00:00:57.230
remember, net produce is happening on this whole network

00:00:57.230 --> 00:01:00.300
of data nodes. Each of these little boxes, we'll

00:01:00.300 --> 00:01:03.470
pretend is a data node. And your data's spread

00:01:03.470 --> 00:01:06.740
out across all of these different machines. And now

00:01:06.740 --> 00:01:09.270
let's think about where each of these three steps

00:01:09.270 --> 00:01:12.360
took place. So, we're not going to focus on what

00:01:12.360 --> 00:01:16.250
happened but where it happened. Well, for step one,

00:01:16.250 --> 00:01:18.430
the mappers were all over the place. There were

00:01:18.430 --> 00:01:22.150
mappers, anywhere there is relevant data. But, for any

00:01:22.150 --> 00:01:25.210
given day, all of the reduction had to happen

00:01:25.210 --> 00:01:27.890
on a single machine, where that reducer was living.

00:01:27.890 --> 00:01:31.310
And actually, that's enough, we've already exposed a problem.

00:01:31.310 --> 00:01:34.160
Because if there's a lot of data, even if

00:01:34.160 --> 00:01:37.470
each individual output was simply a day and a

00:01:37.470 --> 00:01:41.070
number, depending on the number of records we have,

00:01:41.070 --> 00:01:44.360
transferring all of this, all this data to

00:01:44.360 --> 00:01:47.050
this machine, with reducer lift, that, that's a lot

00:01:47.050 --> 00:01:49.480
of bandwidth. That's going to be a lot of traffic

00:01:49.480 --> 00:01:51.760
on our network that we don't necessarily want, and

00:01:51.760 --> 00:01:53.660
it doesn't actually seem like we need to have.

00:01:54.850 --> 00:01:57.530
So what if we could do some of this

00:01:57.530 --> 00:02:02.750
reduction locally? What if on each machine, before we

00:02:02.750 --> 00:02:06.470
send our key-value pairs off, off to be reduced,

00:02:06.470 --> 00:02:08.990
what if we could do some pre-reduction? And it

00:02:08.990 --> 00:02:12.378
turns out we can. And that pre-reduction happens with these

00:02:12.378 --> 00:02:16.420
things called combiners. And before you practice using combiners

00:02:16.420 --> 00:02:18.320
let me try to sell you a bit more on

00:02:18.320 --> 00:02:20.750
their value. So as you know, when you use

00:02:20.750 --> 00:02:24.240
a map reduce job you get some output like this.

00:02:24.240 --> 00:02:27.210
This is where it's displaying how much mapping has happened

00:02:27.210 --> 00:02:30.230
and how much reducing. But you get this tracking URL.

00:02:31.440 --> 00:02:35.770
And if you open that URL in a browser you get a job page, and that job page

00:02:35.770 --> 00:02:38.290
looks something like this. And actually if you scroll

00:02:38.290 --> 00:02:41.430
down below here you'll see some more interesting statistics.

00:02:41.430 --> 00:02:44.910
And in fact we ran two jobs. They were

00:02:44.910 --> 00:02:49.490
identical jobs, except one implemented combiners, so they were

00:02:49.490 --> 00:02:52.720
reducing locally as much as possible, before shuttling that

00:02:52.720 --> 00:02:56.270
data off to that machine that held the end reducer.

00:02:56.270 --> 00:02:59.470
Let's compare the statistics from those two jobs. The

00:02:59.470 --> 00:03:02.150
one with combiners, the one without. So let's compare what

00:03:02.150 --> 00:03:05.320
happened on these job pages. The one without the combiner

00:03:05.320 --> 00:03:08.090
versus the one with combiner. So, a lot of stuff

00:03:08.090 --> 00:03:11.350
is the same. The mappers to the same amount,

00:03:11.350 --> 00:03:16.260
the input matches the output for without and with. That's

00:03:16.260 --> 00:03:18.380
as expected. In fact, a lot of things look pretty

00:03:18.380 --> 00:03:21.340
similar. But where things get really interesting is down here

00:03:21.340 --> 00:03:27.375
at reduce input records. So, the reducers, had to handle the whole

00:03:27.375 --> 00:03:32.929
4 million records that the macro had put when there is no combiner. But when

00:03:32.929 --> 00:03:35.800
there was a combiner, these 4 million

00:03:35.800 --> 00:03:39.280
mapped output records are combined into 412

00:03:39.280 --> 00:03:44.000
reduced input records, or reducer input records.

00:03:44.000 --> 00:03:47.020
That's pretty amazing. That's a huge change.

00:03:47.020 --> 00:03:51.220
In fact, the reducers had to shuffle far fewer bytes. And for time

00:03:51.220 --> 00:03:56.530
spent, with the combiner this job took 31 seconds to run versus without where

00:03:56.530 --> 00:04:02.020
it took 43. That's a significant difference. So you can really use combiners to

00:04:02.020 --> 00:04:04.520
improve the efficiency of your map reduced

00:04:04.520 --> 00:04:06.740
jobs. And we're going to try that next.

