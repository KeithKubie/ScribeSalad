WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:02.620
Let's suppose that x and y are numerical.

00:00:04.050 --> 00:00:06.340
Do x and y have anything to do with each other?

00:00:07.930 --> 00:00:10.450
Let's try to pose the question in a more precise way.

00:00:11.740 --> 00:00:13.260
You've been collecting records and

00:00:13.260 --> 00:00:17.870
so, you can form a joint probability table between x and y.

00:00:17.870 --> 00:00:23.330
So we can say in an absolute sense, in a prior probability sense.

00:00:23.330 --> 00:00:26.710
What degree of certainty do we have about x?

00:00:26.710 --> 00:00:30.600
If x takes on the values x1 or x2 we can ask,

00:00:30.600 --> 00:00:33.700
what's the probability with which it takes on those values?

00:00:33.700 --> 00:00:37.190
So, from the table here we know we can just evaluate those numbers.

00:00:37.190 --> 00:00:40.301
They're just 0.29 and 0.71.

00:00:40.301 --> 00:00:44.420
Now let's suppose that we know y is stuck at y1.

00:00:44.420 --> 00:00:48.810
Does this make us more certain at all about what x is?

00:00:48.810 --> 00:00:53.850
Specifically what's the conditional probability that x is x1 or

00:00:53.850 --> 00:00:56.630
x2, given that we know y is fixed at y1.

00:00:56.630 --> 00:01:00.144
Well again we can just directly evaluate it from the table.

00:01:00.144 --> 00:01:03.508
It's 0.16 and 0.84.

00:01:03.508 --> 00:01:07.340
So it seems that knowing that y is stuck at y1 has told us

00:01:07.340 --> 00:01:09.750
something about the value of x, right?

00:01:09.750 --> 00:01:11.340
Intuitively we know that.

00:01:11.340 --> 00:01:15.440
It caused the probability to skew a little bit to one side.

00:01:15.440 --> 00:01:17.300
In this case, towards x2.

00:01:17.300 --> 00:01:21.020
It turns out there's an expression to measure the degree of

00:01:21.020 --> 00:01:23.540
skew of a probability distribution.

00:01:23.540 --> 00:01:25.970
This expression is called entropy.

00:01:25.970 --> 00:01:31.600
Specifically, let's suppose that a random variable x takes on the values x1,

00:01:31.600 --> 00:01:33.320
x2 though xk.

00:01:33.320 --> 00:01:39.530
The entropy for the probability distribution of x is labeled h of x.

00:01:39.530 --> 00:01:45.470
And it is defined as minus p1 times log of p1 minus p2 times log of p2 and

00:01:45.470 --> 00:01:47.360
so on through pk.

00:01:47.360 --> 00:01:52.460
Otherwise written as minus 1 times the sum of pi log pi.

00:01:52.460 --> 00:01:56.090
Now the base of the logarithm isn't necessarily important.

00:01:56.090 --> 00:02:01.790
If the base is base two, then, the units of entropy is called bits.

00:02:01.790 --> 00:02:05.640
If the base is e, then the units are called nets.

00:02:05.640 --> 00:02:08.530
Traditionally in computer science, we use base two,

00:02:08.530 --> 00:02:11.600
but, it's not really super important.

00:02:11.600 --> 00:02:15.540
Now to see what's behind the expression h of x.

00:02:16.960 --> 00:02:21.580
And why this formula was chosen over other options for formulas and much more

00:02:21.580 --> 00:02:25.540
details about information theory in general, see the instructor's notes.

