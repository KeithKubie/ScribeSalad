WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:05.710
What it basically says is
that it is not possible to

00:00:05.710 --> 00:00:07.740
construct a fair voting system.

00:00:07.740 --> 00:00:11.130
The way that you can think about this
in very simple mathematical terms.

00:00:11.130 --> 00:00:13.240
That I know that I think you'll like,
Michael,

00:00:13.240 --> 00:00:17.580
is that all of these things
assume that everyone is

00:00:17.580 --> 00:00:22.590
using exactly the same units.

00:00:22.590 --> 00:00:26.600
Right, but if one of the agents is using
the metric system and someone else is

00:00:26.600 --> 00:00:30.160
using the imperial system it turns
out you can't even compare the two.

00:00:30.160 --> 00:00:33.570
There's absolutely no reason to believe,

00:00:33.570 --> 00:00:39.008
that the values that each of these
individual agents are developing,

00:00:39.008 --> 00:00:43.264
follow the same linear notion
of ordering of Q values.

00:00:43.264 --> 00:00:46.544
And if that's not the case then
all of these things are basically

00:00:46.544 --> 00:00:47.282
going to break.

00:00:47.282 --> 00:00:50.486
Because all of them boil down to doing
what looks like some kind of addition or

00:00:50.486 --> 00:00:51.750
averaging..

00:00:51.750 --> 00:00:52.480
&gt;&gt; Or maxing.

00:00:52.480 --> 00:00:54.690
&gt;&gt; Or maxing but maxing is just
kind of a weighted average.

00:00:55.760 --> 00:00:57.130
Kind of an exponential averaging.

00:00:58.140 --> 00:00:59.560
And so, you end up with a max that way.

00:00:59.560 --> 00:01:02.340
Same thing with negotiated W learning
you're subtracting things and

00:01:02.340 --> 00:01:03.600
taking mins.

00:01:03.600 --> 00:01:07.150
In the end you're basically assuming
that I can compare your seven with my

00:01:07.150 --> 00:01:09.350
seven and it means the same thing.

00:01:09.350 --> 00:01:12.340
Why would your seven of my
seven mean the same thing?

00:01:12.340 --> 00:01:14.320
So all of these have
that kind of problem.

00:01:14.320 --> 00:01:17.720
And in fact there's a paper that
written by a student of mine,

00:01:17.720 --> 00:01:21.643
that is a part of the readings,
that delve into this in more detail.

00:01:21.643 --> 00:01:24.950
And we could spend a lot of time on it,
but actually the idea is very simple.

00:01:24.950 --> 00:01:28.240
Which is really that once you
start thinking about goals, and

00:01:28.240 --> 00:01:32.400
you think about these
individual modules or

00:01:32.400 --> 00:01:35.450
individual options as trying to
accomplish different things.

00:01:35.450 --> 00:01:38.660
You have to deal with the fact that
their goals may not be completely

00:01:38.660 --> 00:01:39.840
compatible with one another.

00:01:39.840 --> 00:01:43.370
And you have to come up with some
way of making them compatible.

00:01:43.370 --> 00:01:46.040
And you can either assume that
compatibility by just pretending your

00:01:46.040 --> 00:01:48.320
seven and my seven are the same.

00:01:48.320 --> 00:01:52.210
Or you can say well there's some larger
goal you're trying to accomplish

00:01:52.210 --> 00:01:53.950
that the arbitrator knows about.

00:01:53.950 --> 00:01:56.800
The arbitrator is going to
make decisions based upon

00:01:56.800 --> 00:01:58.460
whether it accomplishes that big goal.

00:01:58.460 --> 00:01:59.870
In the predator prey case, for

00:01:59.870 --> 00:02:04.180
example, the big goal might be
living long enough to reproduce.

00:02:04.180 --> 00:02:07.690
And so I'm going to pick the subgoals
that allow me to live long enough

00:02:07.690 --> 00:02:08.630
to reproduce.

00:02:08.630 --> 00:02:10.150
&gt;&gt; I see, I think.

00:02:10.150 --> 00:02:11.670
So there's a high level goal, and

00:02:11.670 --> 00:02:16.720
then these subgoals that are chosen to
try to bring about the high level goal.

00:02:16.720 --> 00:02:20.740
&gt;&gt; Right, and so it's the arbitrator
that has to decide what's going on.

00:02:20.740 --> 00:02:23.040
So this is, this seems like
a kind of straightforward thing.

00:02:23.040 --> 00:02:24.440
But I actually think it's kind of cool,
right.

00:02:24.440 --> 00:02:29.410
That basically we went from actions
to abstract actions these options.

00:02:29.410 --> 00:02:32.200
Those abstract actions
actually turn out to be.

00:02:32.200 --> 00:02:34.460
Well at least one could
look at it this way.

00:02:34.460 --> 00:02:35.910
Ways of accomplishing goals.

00:02:37.360 --> 00:02:38.260
Once you have these ways of

00:02:38.260 --> 00:02:41.170
accomplishing goals then suddenly
you realize well these goals don't

00:02:41.170 --> 00:02:43.530
necessarily have to be
compatible with one another.

00:02:43.530 --> 00:02:45.140
And so
I have to deal with that compatibility.

00:02:45.140 --> 00:02:48.760
And so you move from an MDP
with primitive actions

00:02:48.760 --> 00:02:52.160
to this larger notion
of balancing tradeoffs.

00:02:52.160 --> 00:02:55.730
As you're trying to accomplish some
large goal for the entire system.

00:02:55.730 --> 00:02:59.050
And in the end, they're still all
MDPs because they're SNDPs and

00:02:59.050 --> 00:03:03.640
SNDPS are MDPs and in the end,
all of that reinforcement learning

00:03:03.640 --> 00:03:07.130
stuff that we've been talking about
throughout the entire term still applies

00:03:07.130 --> 00:03:09.040
&gt;&gt; And you've kind of brought together.

00:03:09.040 --> 00:03:11.740
Generalization with multi agent.

00:03:11.740 --> 00:03:13.940
You know type of thinking.

00:03:13.940 --> 00:03:16.794
&gt;&gt; Yeah, so we got multi-agents, we got
single agents, we got multiple goals,

00:03:16.794 --> 00:03:17.870
we got conflicting goals.

00:03:17.870 --> 00:03:21.900
We've got abstraction and this gives
us scale and everything's wonderful.

00:03:21.900 --> 00:03:22.400
&gt;&gt; Hallelujah.

