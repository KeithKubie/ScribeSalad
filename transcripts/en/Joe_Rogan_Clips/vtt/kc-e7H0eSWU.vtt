WEBVTT

1
00:00:00.000 --> 00:00:04.960
Did you,
are you scared of artificial life,
artificial intelligence?
Um,
you,

2
00:00:05.080 --> 00:00:08.950
I must scare the shit out of me.
Yeah.
When he talked about it,
like he,

3
00:00:08.951 --> 00:00:13.951
he talks about it like we're in the opening scene of a science fiction movie

4
00:00:14.490 --> 00:00:17.520
where he's trying to warn people and then they don't listen to,

5
00:00:17.610 --> 00:00:21.420
to the genius and it goes south.
So depends.

6
00:00:21.930 --> 00:00:24.280
I Chad at Ebay on this,
um,

7
00:00:24.370 --> 00:00:28.050
the royal society in London a few weeks ago and uh,
they,

8
00:00:28.580 --> 00:00:30.710
so he's treat now at the moment what,

9
00:00:30.780 --> 00:00:34.290
what people tend to be frightened of a general Ai's,
oh,
I AGI,

10
00:00:34.291 --> 00:00:36.000
they called artificial general intelligence,

11
00:00:36.300 --> 00:00:40.720
which is like what we talked about earlier,
a human like capability thing.
Yes.

12
00:00:41.110 --> 00:00:44.490
Um,
and we miles away from that.
We don't know how to do it.

13
00:00:44.850 --> 00:00:48.000
We haven't got them and we miles away.
So at the moment,

14
00:00:48.001 --> 00:00:52.080
artificial intelligence is expert systems and very focused systems that do

15
00:00:52.081 --> 00:00:53.040
particular things.

16
00:00:53.730 --> 00:00:58.170
You can be scared of them in a limited economic sense because they're going to

17
00:00:58.171 --> 00:01:00.660
displace people's jobs.
And actually,

18
00:01:00.661 --> 00:01:02.400
interestingly in this panel discussion we had,

19
00:01:02.401 --> 00:01:05.650
it's going to be like what you might call middle class jobs in the UK.

20
00:01:05.730 --> 00:01:06.930
So white collar jobs.

21
00:01:07.260 --> 00:01:11.490
It's not why people are interested in universal basic income to sort of replace

22
00:01:11.491 --> 00:01:14.880
money that's going to be lost because there will be no jobs for all these
people.

23
00:01:14.881 --> 00:01:17.850
Otherwise we have just a mass catastrophe.
They're very good.

24
00:01:17.880 --> 00:01:22.000
Someone said that these systems,
artificial intelligence systems at the moment,

25
00:01:22.001 --> 00:01:24.360
they're very good at doing things like law lawyer's work.

26
00:01:24.780 --> 00:01:27.000
So they're very good at reading contracts and things like that.

27
00:01:27.450 --> 00:01:29.100
There's interesting cause it's a,
it's a revolution.

28
00:01:29.101 --> 00:01:30.570
It's not like the industrial revolution.

29
00:01:30.571 --> 00:01:33.930
Where is manual labor that gets hit necessarily.

30
00:01:34.140 --> 00:01:38.190
This is kind of interesting because it hits that kind of intermediate level that

31
00:01:38.191 --> 00:01:43.020
usually escapes.
Um,
so you're right,
one of the answers is to tax,

32
00:01:43.460 --> 00:01:45.690
there was a example was a robot tax.

33
00:01:45.691 --> 00:01:49.710
So in a car factory you say to the manufacturer,
well okay you can have a robot.

34
00:01:49.711 --> 00:01:49.861
What?

35
00:01:49.861 --> 00:01:53.760
You pay the robot the same as you pay a person and then that money goes into

36
00:01:53.761 --> 00:01:56.670
funding universal basic income or something like that.
So I think the,

37
00:01:56.730 --> 00:02:00.060
there's gotta be an economic change because these systems will be there.

38
00:02:00.720 --> 00:02:05.720
But all the experts I spoke to agreed that the idea of a terminate a style,

39
00:02:06.900 --> 00:02:10.740
general intelligence taking over the world is miles away.

40
00:02:11.400 --> 00:02:15.630
And um,
so whilst we might start thinking about the regulation,

41
00:02:16.140 --> 00:02:20.970
it's not going to happen soon is the general point.

42
00:02:20.971 --> 00:02:25.170
I think so I would disagree with him on that.
I think,

43
00:02:25.171 --> 00:02:27.750
I think it's too far in the future.
At the moment.

44
00:02:27.810 --> 00:02:30.110
I thought I might be one of those people.
That's an ad.

45
00:02:30.111 --> 00:02:31.890
It's going to be all right.
And then,
and then,
you know,

46
00:02:32.250 --> 00:02:37.120
my iPhone takes me out on the way to the airport.
Yeah.
Thing.
I mean you,

47
00:02:37.220 --> 00:02:39.240
it's our choice at the moment,
isn't it?
I mean don't,

48
00:02:39.241 --> 00:02:42.960
don't give your iPhone a laser for example.

49
00:02:43.140 --> 00:02:46.230
And it doesn't matter if he goes crazy and tries to take over the world.
I know,

50
00:02:46.231 --> 00:02:48.060
I know that's a bit facetious.
Cause they can,

51
00:02:48.690 --> 00:02:51.510
he would say they could take over the power grids and all that kind of stuff I

52
00:02:51.511 --> 00:02:56.280
guess.
But well it's these concepts that are really hard to visualize,

53
00:02:56.340 --> 00:02:57.360
like sorta occurred.

54
00:02:57.380 --> 00:03:02.380
Swilles idea of the exponential increase of technology leading us to a point in

55
00:03:02.741 --> 00:03:05.860
the near future where you're gonna be able to download your consciousness into a

56
00:03:05.861 --> 00:03:08.230
computer.
You talked to computer experts,
it like this.

57
00:03:08.231 --> 00:03:12.960
Now way we're miles away from that on you're a scientist or scientists that you

58
00:03:14.330 --> 00:03:17.110
knew one brain cell properly.
We can't,

59
00:03:17.140 --> 00:03:21.220
but Kurzweil's convinced that what's going to happen is that as technology

60
00:03:21.221 --> 00:03:21.731
increases,

61
00:03:21.731 --> 00:03:26.731
it increases in this wildly exponential way where we really can't visualize it.

62
00:03:27.280 --> 00:03:32.280
We can't even imagine how much advancement will take place over 50 years,

63
00:03:32.560 --> 00:03:35.050
but in those 50 years something's going to happen.

64
00:03:35.051 --> 00:03:38.170
That radically changes our idea of what's possible.

65
00:03:38.290 --> 00:03:40.360
And I think Ilan shares this idea as well,

66
00:03:40.361 --> 00:03:43.780
that it's going to sneak up on us so quickly that when it does go live,

67
00:03:43.781 --> 00:03:48.250
it'll be too late.
Yeah,
I mean it's worth putting the,
the the framework in place.

68
00:03:48.610 --> 00:03:51.280
I think the regulatory framework,
even as you said,

69
00:03:51.281 --> 00:03:52.600
for the more realistic problem,

70
00:03:52.601 --> 00:03:57.070
which is people's jobs are going to get displaced.
Yes,
there's a great,
um,

71
00:03:57.160 --> 00:03:59.650
I always had a thing in some,
someone said,
I kind of,
it was,

72
00:03:59.651 --> 00:04:01.710
but they said that the GRE,
it was a politician.
The,

73
00:04:01.711 --> 00:04:06.460
the job of the innovation system is to create jobs faster than it destroys them.

74
00:04:07.000 --> 00:04:10.480
So you've always got to remember that as a government and as regulators,

75
00:04:10.660 --> 00:04:14.920
if you're going to allow technologies into the marketplace at destroy people's

76
00:04:14.921 --> 00:04:15.754
jobs,

77
00:04:15.970 --> 00:04:20.590
it is your responsibility to find a way of replacing those jobs or compensate in

78
00:04:20.591 --> 00:04:23.720
those people as you said,
otherwise you can break down.

79
00:04:23.721 --> 00:04:27.760
So for regular human being,
those that people need some meaning,

80
00:04:27.850 --> 00:04:31.040
like they just giving them income I think is just going to,

81
00:04:32.090 --> 00:04:36.460
I mean it's just my speculation,
but it can create mass despair.

82
00:04:36.640 --> 00:04:40.870
Even if we provide them,
you provide them with food and shelter,
they need,

83
00:04:40.871 --> 00:04:42.920
people need things to do.
So it's,
yeah,

84
00:04:43.000 --> 00:04:48.000
there's going to be some sort of a demand to find meaning for people,

85
00:04:48.100 --> 00:04:51.730
give them occupations,
give them something,
some task.

86
00:04:52.030 --> 00:04:56.630
Let's say it seems to be one of the critical parts of being a person.

87
00:04:56.630 --> 00:05:00.510
And so we need things to do that we find meaning in,
you know,

88
00:05:00.520 --> 00:05:04.120
like you were talking about where the only things that we know of that have

89
00:05:04.121 --> 00:05:09.121
meaning that find meaning and share meaning and believe in that we're gonna need

90
00:05:10.151 --> 00:05:12.700
something like that.
If universal basic income comes along,

91
00:05:12.880 --> 00:05:16.120
I don't think it's going to be enough to just feed people and house them.

92
00:05:16.450 --> 00:05:20.200
They're going to want something to do.
If a,
you know,
a person is,
uh,

93
00:05:20.350 --> 00:05:24.250
you're doing something for an occupation and this is your identity,

94
00:05:24.400 --> 00:05:26.980
and then all of sudden that occupation becomes irrelevant because the computer

95
00:05:26.981 --> 00:05:29.080
does it faster,
cheaper,
quicker.

96
00:05:30.070 --> 00:05:34.870
These people are going to have this incredible feeling of despair and just not

97
00:05:34.871 --> 00:05:38.620
being valuable.
Yeah.
I mean,
uh,
well,
the utopian,

98
00:05:38.920 --> 00:05:42.890
so the version of this is that everybody gets to do what we're doing now,
right?

99
00:05:42.910 --> 00:05:47.760
She's make a living.
So I think in creating that kind of,
you know,
so that's the,

100
00:05:47.761 --> 00:05:51.160
the Utopian ideal is you don't need to do that stuff.

101
00:05:51.400 --> 00:05:54.730
The job that you don't really want to do in the factory.
Right.

102
00:05:54.940 --> 00:05:58.990
You can do the thing that humans are best at that,
but I agree.

103
00:05:59.330 --> 00:06:02.690
That's a very utopian view.
Yeah.

104
00:06:02.750 --> 00:06:06.070
Does everybody want to do that or does everybody have their mindset?
Well,

105
00:06:06.071 --> 00:06:09.380
maybe because went to education.
If everybody had an interest like that,

106
00:06:09.381 --> 00:06:14.030
if everybody went on to make pottery and painting and doing all these different

107
00:06:14.031 --> 00:06:18.170
things that they've always really wanted to do and their needs are met by,

108
00:06:18.171 --> 00:06:22.940
you know,
the universal basic income money that they receive every month.
But boy,

109
00:06:23.150 --> 00:06:27.080
there's a lot of people I don't think have those desires or needs and to sort of

110
00:06:27.081 --> 00:06:31.090
force it onto them at age 55 or whatever it's going to be.
Yeah,

111
00:06:31.190 --> 00:06:34.570
it seems to be very,
very difficult.
Yeah.
Yeah.
I agree.

112
00:06:36.490 --> 00:06:41.000
Yeah.
It's a big challenge,
but I think that in concept at least,

113
00:06:41.210 --> 00:06:45.680
it's inevitable that we do have some sort of an artificial intelligence that

114
00:06:45.681 --> 00:06:49.850
resembles us or that resemble something like x Mokena.

115
00:06:50.030 --> 00:06:53.000
If people choose to create that,

116
00:06:53.660 --> 00:06:57.440
we choose to create it in our own image.
But that's very godlike,
isn't it?

117
00:06:57.441 --> 00:07:01.400
God created us in his own image.
Yeah.
And,
and again,
yeah.

118
00:07:02.990 --> 00:07:06.830
I don't know that when I talk to people in the field,

119
00:07:07.070 --> 00:07:11.270
as you probably have,
most of them say don't know how to do it.
Yes,

120
00:07:11.300 --> 00:07:13.700
it's really right.
It's going to be miles away.

121
00:07:14.180 --> 00:07:18.350
So maybe I'm hiding my head in the sand a bit,
but I don't think so.

122
00:07:18.680 --> 00:07:19.520
I think it's,

123
00:07:20.810 --> 00:07:23.780
I think we'll know it when I don't think anyone's going to do it accidentally.

124
00:07:23.900 --> 00:07:27.710
Right.
So I don't think it's just suddenly going to be upon us.
I,
I,

125
00:07:28.430 --> 00:07:32.510
I think we will see,
we will see ourselves getting,

126
00:07:32.840 --> 00:07:35.780
acquiring that capability.
We'll see ourselves getting close.

127
00:07:35.930 --> 00:07:40.070
We'll see those systems beginning to emerge and then we will think about it.

128
00:07:40.330 --> 00:07:43.910
I think 200 years ago,
if you wanted a photograph of something,

129
00:07:43.911 --> 00:07:47.960
you want a picture of something,
you had to draw it.
I mean,

130
00:07:48.020 --> 00:07:50.960
there was no photography 200 years ago.
Yeah.

131
00:07:51.200 --> 00:07:55.040
I mean just think of that is almost inconceivable.

132
00:07:56.000 --> 00:07:59.150
No automobiles,
no photography.
What was automobile?

133
00:07:59.390 --> 00:08:02.360
Maybe there was some sort of machines that drove people around.
Right.

134
00:08:02.870 --> 00:08:06.500
Something close there.
Some trains earlier than that,
right?

135
00:08:06.680 --> 00:08:09.590
You go back 500 years,
you have almost nothing.
Yeah.

136
00:08:09.840 --> 00:08:14.750
That's crazy how we've been quick so fast.
It's so fast.
I mean,

137
00:08:14.751 --> 00:08:16.520
and then this what we're doing right now,

138
00:08:16.550 --> 00:08:19.940
there's people right now in their car that are streaming this,

139
00:08:20.210 --> 00:08:22.790
so they're in their car and they're listening as they're driving on the road.

140
00:08:22.791 --> 00:08:25.520
Maybe they have a Tesla,
maybe they have an electric car.

141
00:08:25.960 --> 00:08:30.140
They're driving down the road streaming to people talking where it's ones and

142
00:08:30.141 --> 00:08:34.700
Zeros that are broken down and there's some audible form and you can listen to

143
00:08:34.701 --> 00:08:38.690
it in your car is bananas.
Yeah,

144
00:08:39.440 --> 00:08:43.220
I agree.
We've been quick so quick.
Well,
I think they will,
you know,
the Internet,

145
00:08:43.340 --> 00:08:46.400
I mean,
it's,
uh,
it's,
uh,
not long.
I mean,

146
00:08:46.401 --> 00:08:49.210
I remember it being invented.
Yeah.

