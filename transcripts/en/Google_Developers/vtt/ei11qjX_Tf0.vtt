WEBVTT
Kind: captions
Language: en

00:00:03.452 --> 00:00:05.160
MANDY WAITE: So we're
going to be talking

00:00:05.160 --> 00:00:06.970
to you about going
from data to meaning.

00:00:06.970 --> 00:00:10.039
So effectively what this
means is taking data and doing

00:00:10.039 --> 00:00:11.080
something useful with it.

00:00:11.080 --> 00:00:14.310
Making it mean something to your
business, to your organization.

00:00:15.400 --> 00:00:17.072
It's kind of a
step-by-step guide.

00:00:17.072 --> 00:00:18.530
It's a walkthrough
of the steps you

00:00:18.530 --> 00:00:20.920
would use to approach
a data problem.

00:00:20.920 --> 00:00:23.432
And we use a specific
example during the talk

00:00:23.432 --> 00:00:25.140
to illustrate this
and to highlight this.

00:00:26.320 --> 00:00:28.150
So the first thing
we're going to do

00:00:28.150 --> 00:00:32.680
is-- well, I don't
actually need this

00:00:32.680 --> 00:00:34.830
because we're
presenting from there.

00:00:34.830 --> 00:00:37.090
The first thing I need to
ask you is how many of you

00:00:37.090 --> 00:00:38.135
have heard of Hadoop?

00:00:41.140 --> 00:00:42.740
OK, so keep your hands up.

00:00:42.740 --> 00:00:45.240
How many of you have built your
own personal Hadoop cluster?

00:00:46.980 --> 00:00:48.290
OK, so not so many of you.

00:00:48.290 --> 00:00:50.890
OK, so, it sounds like
fun though, doesn't it?

00:00:50.890 --> 00:00:52.880
So, perhaps it's
something we should try.

00:00:54.790 --> 00:00:57.557
Let's try building
a Hadoop cluster.

00:00:57.557 --> 00:00:59.640
Can we switch to the demos,
switch to the machine?

00:01:03.592 --> 00:01:04.300
Right, brilliant.

00:01:07.620 --> 00:01:10.050
As in the best [? junior ?]
[? trial ?] with principles

00:01:10.050 --> 00:01:13.420
of demos, we have some
things we've set up already.

00:01:13.420 --> 00:01:16.500
We have a Compute Engine
virtual machine instance

00:01:16.500 --> 00:01:18.197
that we've set up
to run this demo.

00:01:18.197 --> 00:01:19.780
But that's pretty
much all we've done.

00:01:19.780 --> 00:01:22.740
Well, actually we've also done
some configuration as well.

00:01:22.740 --> 00:01:24.490
So, I have this
virtual machine here

00:01:24.490 --> 00:01:26.300
which I'm going to SSH into.

00:01:26.300 --> 00:01:28.360
And to do that, I'm
going to go my shell.

00:01:28.360 --> 00:01:29.910
Everybody see that?

00:01:29.910 --> 00:01:30.410
Yeah.

00:01:30.410 --> 00:01:31.720
I think it should be good.

00:01:31.720 --> 00:01:34.210
What I'm going to do is
SSH into this machine,

00:01:34.210 --> 00:01:36.990
if I can find the
appropriate command.

00:01:38.700 --> 00:01:41.265
So I'm using the command
line tool from our cloud SDK

00:01:41.265 --> 00:01:43.480
to SSH into that machine.

00:01:43.480 --> 00:01:45.010
And here we are.

00:01:45.010 --> 00:01:46.100
It's a real Linux box.

00:01:47.120 --> 00:01:51.500
We can do ps minus ef to
see what we've got running.

00:01:52.820 --> 00:01:54.920
And what we'll do is
we'll cd to slash user.

00:01:56.370 --> 00:01:58.440
That's bd, whoops.

00:02:02.992 --> 00:02:04.450
I'm not very good
at typing either.

00:02:06.090 --> 00:02:09.660
So, what I'm going to
do is run a script.

00:02:09.660 --> 00:02:12.790
And we have some configuration
we've already done beforehand

00:02:12.790 --> 00:02:13.964
on this.

00:02:13.964 --> 00:02:15.130
Whoops, I really can't type.

00:02:18.969 --> 00:02:21.260
So we're going to run this
command called bdutil, which

00:02:21.260 --> 00:02:23.100
I'll talk about more briefly.

00:02:23.100 --> 00:02:25.780
And for those of you who
have not heard of Hadoop,

00:02:25.780 --> 00:02:27.225
we'll have a slide
afterwards that

00:02:27.225 --> 00:02:29.680
will just kind of position that.

00:02:29.680 --> 00:02:31.822
For those of you who have,
this could, hopefully,

00:02:31.822 --> 00:02:32.405
be impressive.

00:02:32.841 --> 00:02:35.257
And one think I want to do
actually first because I forgot

00:02:35.257 --> 00:02:39.030
to do it earlier is change
my configuration file.

00:02:39.030 --> 00:02:42.365
So, whoops.

00:02:45.414 --> 00:02:47.582
I see the demo gods
are still favoring me

00:02:47.582 --> 00:02:49.165
because I haven't
broken anything yet.

00:02:53.760 --> 00:02:56.980
So, Hadoop is a
distributed system

00:02:56.980 --> 00:02:59.580
for performing data processing.

00:02:59.580 --> 00:03:01.620
And for that, you need
a number of workers.

00:03:01.620 --> 00:03:03.284
And I'm going to
provide 20 workers.

00:03:03.284 --> 00:03:04.700
So I'm going to
ask you to spin up

00:03:04.700 --> 00:03:07.140
20 workers in my
Hadoop cluster for me.

00:03:07.140 --> 00:03:12.095
So, what I do is-- I was
testing on smaller clusters

00:03:12.095 --> 00:03:13.470
because I've been
having problems

00:03:13.470 --> 00:03:14.700
with this particular demo.

00:03:14.700 --> 00:03:17.897
And I've been using a small
size to make sure it works,

00:03:17.897 --> 00:03:18.980
and I forgot to change it.

00:03:18.980 --> 00:03:19.771
So that's my fault.

00:03:21.220 --> 00:03:24.167
And .bdutil deploy.

00:03:24.167 --> 00:03:26.250
That gives us a list of
our configuration options.

00:03:26.250 --> 00:03:27.740
This is the
configuration that will

00:03:27.740 --> 00:03:29.910
be used to build
our Hadoop cluster.

00:03:29.910 --> 00:03:32.345
We have NUM_WORKERS 20,
that's what we just changed.

00:03:33.350 --> 00:03:37.270
It's going to create a bunch of
Compute Engine virtual machines

00:03:37.270 --> 00:03:40.624
to deploy Hadoop on, some more
information about our project,

00:03:40.624 --> 00:03:42.040
our image that
we're going to use,

00:03:42.040 --> 00:03:43.954
the zone we're going
to use, et cetera.

00:03:43.954 --> 00:03:46.370
And a prefix we're going to
use for these virtual machines

00:03:46.370 --> 00:03:49.461
is NYC just to prove it's real.

00:03:49.461 --> 00:03:51.710
And as you can see, we don't
have any virtual machines

00:03:51.710 --> 00:03:53.640
currently, apart
from the one we've

00:03:53.640 --> 00:03:55.310
been using to do the setup.

00:03:55.310 --> 00:03:56.070
So I'll say yes.

00:03:57.240 --> 00:03:58.520
And it goes off now.

00:03:58.520 --> 00:04:01.497
And it's effectively doing
a lot of asynchronous tasks

00:04:01.497 --> 00:04:02.330
to create instances.

00:04:03.780 --> 00:04:06.680
We have lots of machines,
workers, 20 of them,

00:04:06.680 --> 00:04:07.950
that are going to come up.

00:04:07.950 --> 00:04:12.239
And we have one name mode
server, a master server,

00:04:12.239 --> 00:04:14.530
that will manage and coordinate
the whole of the Hadoop

00:04:14.530 --> 00:04:15.305
cluster.

00:04:15.305 --> 00:04:16.930
So that's doing that
in the background.

00:04:16.930 --> 00:04:19.019
And that's going to take
a little while to finish.

00:04:19.019 --> 00:04:22.182
But we can see, probably,
if we hit refresh.

00:04:22.182 --> 00:04:24.140
Now the cloud console,
the developer's console,

00:04:24.140 --> 00:04:25.340
doesn't refresh very often.

00:04:25.340 --> 00:04:30.080
So you often have to go
into to enter a refresh.

00:04:30.080 --> 00:04:33.246
And here we can see more
instances coming up.

00:04:33.246 --> 00:04:34.371
There should be 20 of them.

00:04:37.080 --> 00:04:37.720
OK.

00:04:37.720 --> 00:04:38.595
Why is it doing that?

00:04:40.180 --> 00:04:40.880
Now I'm puzzled.

00:04:45.840 --> 00:04:47.747
OK, so this should
actually showed them

00:04:47.747 --> 00:04:49.580
in various different
states of provisioning.

00:04:49.580 --> 00:04:51.840
But it's actually showing
them all provisioned.

00:04:51.840 --> 00:04:53.375
So we have 20 nodes there.

00:04:53.375 --> 00:04:55.340
I think it's 20.

00:04:55.340 --> 00:04:56.490
I don't see any missing.

00:04:56.490 --> 00:04:58.115
But normally you
would see all of them.

00:04:58.115 --> 00:05:00.410
And you would see some of
them will be in provisioned

00:05:00.410 --> 00:05:01.826
and some of them
are up and ready.

00:05:01.826 --> 00:05:05.410
The green tick here told me
that that's up and running.

00:05:05.410 --> 00:05:10.150
We also have this nyc-nn, which
is our main node, our master

00:05:10.150 --> 00:05:12.750
coordinator for
the whole cluster.

00:05:12.750 --> 00:05:14.210
And I ought to discuss why.

00:05:16.494 --> 00:05:17.660
Checking on the wrong thing.

00:05:19.066 --> 00:05:20.940
So these are the instances
being provisioned.

00:05:20.940 --> 00:05:24.030
So this one, nycdn-18,
is all being provisioned.

00:05:24.030 --> 00:05:26.384
All the others are
already up and running.

00:05:26.384 --> 00:05:28.550
The disks that will be used
by these virtual machine

00:05:28.550 --> 00:05:30.380
are also being created as well.

00:05:30.380 --> 00:05:33.970
I can go to the IP address
of the NameNode, click on it,

00:05:33.970 --> 00:05:36.920
and I can simply
enable HTTP traffic

00:05:36.920 --> 00:05:38.089
for that virtual machine.

00:05:38.089 --> 00:05:39.130
We're going to need this.

00:05:39.130 --> 00:05:40.690
This is our Hadoop master.

00:05:40.690 --> 00:05:43.150
This is how we're going to
interface with the Hadoop

00:05:43.150 --> 00:05:44.930
cluster later on.

00:05:44.930 --> 00:05:46.110
So I click Apply on that.

00:05:46.110 --> 00:05:50.150
And that will go off and do
something in the background.

00:05:50.150 --> 00:05:53.679
And effectively
enable HTTP and HTTPS,

00:05:53.679 --> 00:05:55.220
open those ports up
on that instance.

00:05:56.950 --> 00:05:59.442
So because we don't have
any minions in this talk,

00:05:59.442 --> 00:06:00.650
I thought we should have one.

00:06:01.720 --> 00:06:05.080
So that's just an aside, though.

00:06:05.080 --> 00:06:07.130
That's not really
key to this talk.

00:06:07.130 --> 00:06:08.917
We do have minions
in the talk later.

00:06:08.917 --> 00:06:11.000
I hope Julia has minions
in the next talk as well.

00:06:12.480 --> 00:06:13.370
They're so cute.

00:06:14.222 --> 00:06:16.180
Right, so can we go back
to the slides, please?

00:06:18.500 --> 00:06:21.580
Unfortunately, this
kind of requires

00:06:21.580 --> 00:06:23.477
going backwards and
forwards to check.

00:06:23.477 --> 00:06:25.560
And because we're controlling
the size from there,

00:06:25.560 --> 00:06:27.476
I can't quickly switch
backwards and forwards.

00:06:27.476 --> 00:06:30.695
So we'll come back to
that Hadoop deployment

00:06:30.695 --> 00:06:32.230
soon and see what it's doing.

00:06:33.490 --> 00:06:38.610
So for those of you who
didn't know what Hadoop was,

00:06:38.610 --> 00:06:41.055
Hadoop is an open source
implementation of Mapreduce.

00:06:42.120 --> 00:06:45.210
Mapreduce is
effectively a mechanism

00:06:45.210 --> 00:06:47.040
by which we can do
distributed computing.

00:06:47.040 --> 00:06:48.890
It was invented by Google.

00:06:48.890 --> 00:06:52.070
We published a white
paper back in 2003.

00:06:52.070 --> 00:06:54.025
It consists of two main steps.

00:06:55.540 --> 00:06:57.160
The first is a map step.

00:06:57.160 --> 00:07:00.560
Effectively, this is performed
on small sets, small subsets

00:07:00.560 --> 00:07:02.910
of the data that
you want to process.

00:07:02.910 --> 00:07:06.640
And it's distributed across all
of these nodes in a cluster.

00:07:06.640 --> 00:07:09.340
We saw, we created one
with 20 in this case.

00:07:09.340 --> 00:07:11.270
And then all of the
output from these map jobs

00:07:11.270 --> 00:07:13.350
are pushed into a
reduced job, in which we

00:07:13.350 --> 00:07:14.960
can do other things as well.

00:07:14.960 --> 00:07:17.370
This is the next part of
the phase of the Mapreduce.

00:07:17.370 --> 00:07:19.749
In this case, we're going
to reduce the data set down.

00:07:19.749 --> 00:07:22.165
The demo we're going to look
at later only has a map step.

00:07:23.520 --> 00:07:25.170
And the reduce step
could, effectively,

00:07:25.170 --> 00:07:26.711
be run across multiple
nodes as well.

00:07:27.740 --> 00:07:31.100
Mapreduces can be run on both
structured and unstructured

00:07:31.100 --> 00:07:36.134
data, and Hadoop, to give it
its full name, is Apache Hadoop

00:07:36.134 --> 00:07:37.550
and has this nice
little elephant.

00:07:40.270 --> 00:07:44.580
So, Hadoop itself has a file
system, a distributed file

00:07:44.580 --> 00:07:46.830
system, that allows
you to support

00:07:46.830 --> 00:07:49.419
this notion of distributing
subsets of the data

00:07:49.419 --> 00:07:51.210
across a huge number
of nodes in a cluster.

00:07:51.210 --> 00:07:52.950
And we only have
20 nodes, but you

00:07:52.950 --> 00:07:55.350
could imagine a Hadoop
cluster with 1,000 nodes.

00:07:55.350 --> 00:07:58.490
Somehow we had to distribute the
data across all of those nodes.

00:07:58.490 --> 00:08:01.540
And the way Hadoop does that
is through a file system called

00:08:01.540 --> 00:08:04.020
HDFS, which is its
own file system.

00:08:04.020 --> 00:08:06.130
And Hadoop is completely
open source, by the way.

00:08:07.750 --> 00:08:10.230
When we're running Hadoop
on Google Cloud Platform,

00:08:10.230 --> 00:08:14.430
we have several managed data
services at our disposal,

00:08:14.430 --> 00:08:18.405
including BigQuery, Datastore,
and Google Cloud Storage.

00:08:18.405 --> 00:08:20.030
So what we want to
really be able to do

00:08:20.030 --> 00:08:23.080
is store data in
those data services

00:08:23.080 --> 00:08:25.900
and let Hadoop access the
data directly from them,

00:08:25.900 --> 00:08:29.620
rather than creating a HDFS
file system on multiple nodes

00:08:29.620 --> 00:08:31.170
and then copying
all the data around.

00:08:31.170 --> 00:08:33.350
We want to do it in situ
within the data service

00:08:33.350 --> 00:08:36.072
that we already have in place,
So we have these connectors,

00:08:36.072 --> 00:08:38.405
we have a BigQuery connector,
a Cloud Storage connector,

00:08:38.405 --> 00:08:39.611
and a Datastore connector.

00:08:42.980 --> 00:08:45.860
bdutil is a very simple script.

00:08:45.860 --> 00:08:48.170
It's effectively a wrapper
around the Cloud SDK.

00:08:48.170 --> 00:08:52.320
I think Jerome mentioned
the Cloud SDK in his talk.

00:08:52.320 --> 00:08:53.870
And the Cloud SDK
is a suite of tools

00:08:53.870 --> 00:08:56.210
that we use for managing
everything in the Google Cloud

00:08:56.210 --> 00:08:56.709
Platform.

00:08:57.810 --> 00:09:00.707
In this case, bdutil will deploy
the Hadoop cluster for us.

00:09:00.707 --> 00:09:02.290
It will create some
of those instances

00:09:02.290 --> 00:09:04.600
then configure them with
the Hadoop software.

00:09:05.497 --> 00:09:07.580
It also installs and
configures the GCS connector,

00:09:07.580 --> 00:09:09.740
the Google Cloud
Storage connector.

00:09:09.740 --> 00:09:11.650
It's also highly
extensible as well.

00:09:11.650 --> 00:09:14.230
We can run individual commands
on the nodes as they come up,

00:09:14.230 --> 00:09:16.200
or we can also run
scripts as well.

00:09:17.220 --> 00:09:18.230
That's what bdutil is.

00:09:18.230 --> 00:09:22.260
That's the util we used when
we looked at the demo earlier.

00:09:22.260 --> 00:09:25.000
I'd like to go back to
the demo now, if possible.

00:09:25.000 --> 00:09:25.700
Just briefly.

00:09:28.870 --> 00:09:29.850
OK, so.

00:09:32.460 --> 00:09:33.360
OK, it failed.

00:09:37.040 --> 00:09:37.935
Why has it failed?

00:09:40.100 --> 00:09:41.000
Sorry.

00:09:41.000 --> 00:09:42.420
Really, honestly.

00:09:42.420 --> 00:09:45.780
So the few times I run
it up there, it was fine.

00:09:45.780 --> 00:09:47.936
And it failed in that one.

00:09:47.936 --> 00:09:49.310
OK, I don't know
why it's failed.

00:09:50.810 --> 00:09:51.580
Do I care?

00:09:51.580 --> 00:09:53.950
I'm going to spend a
very short amount of time

00:09:53.950 --> 00:09:55.160
looking at the debug file.

00:09:58.710 --> 00:10:01.070
Because I want to
know why it failed.

00:10:01.070 --> 00:10:04.050
But we won't get hung up
on this, we'll continue.

00:10:04.050 --> 00:10:05.050
Because we can continue.

00:10:06.496 --> 00:10:08.370
In the face of adversity,
we always continue.

00:10:11.510 --> 00:10:12.930
See, you find it funny.

00:10:12.930 --> 00:10:14.640
It's me, I'm falling
apart on the stage.

00:10:18.870 --> 00:10:20.050
OK.

00:10:20.050 --> 00:10:21.383
You're not telling me much here.

00:10:22.720 --> 00:10:23.700
A backend error.

00:10:23.700 --> 00:10:25.070
OK, well that's very useful.

00:10:25.070 --> 00:10:25.569
OK.

00:10:26.820 --> 00:10:28.340
So this script is
highly reliable,

00:10:28.340 --> 00:10:29.970
but maybe needs a
little bit more work.

00:10:31.060 --> 00:10:33.210
OK, so, can we go back
to the slides, please?

00:10:38.300 --> 00:10:40.956
OK, so if you were a
room full of data--

00:10:40.956 --> 00:10:42.580
this is a room full
of data scientists.

00:10:42.580 --> 00:10:43.746
Are you all data scientists?

00:10:43.746 --> 00:10:44.955
Anybody not a data scientist?

00:10:44.955 --> 00:10:46.830
Obviously, there's quite
a lot of you, right?

00:10:47.839 --> 00:10:49.630
If this was a room full
of data scientists,

00:10:49.630 --> 00:10:51.100
I'd be talking about big data.

00:10:51.100 --> 00:10:53.700
But let's not get
hung up on that term

00:10:53.700 --> 00:10:56.130
because some people find
that kind of difficult

00:10:56.130 --> 00:10:57.300
to get their head around.

00:10:57.300 --> 00:10:59.260
Maybe, oh, big data,
that's not really for me.

00:10:59.260 --> 00:11:01.010
We're just going to
be talking about data,

00:11:01.010 --> 00:11:03.910
so let's forget about
that term, apart

00:11:03.910 --> 00:11:06.110
from giving it a
quick definition.

00:11:06.110 --> 00:11:09.670
Big data is effectively a
data set or a collection

00:11:09.670 --> 00:11:12.090
of data sets that are either
too computational or too

00:11:12.090 --> 00:11:15.320
large to be handled by
what would be called

00:11:15.320 --> 00:11:17.597
traditional data
processing techniques.

00:11:17.597 --> 00:11:19.930
I guess today's data processing
techniques in the future

00:11:19.930 --> 00:11:21.304
will be traditional
ones as well.

00:11:22.470 --> 00:11:24.310
But what it's really
become in reality

00:11:24.310 --> 00:11:27.780
is an approach to data
analysis that just would not

00:11:27.780 --> 00:11:30.664
be possible with those
older, more traditional data

00:11:30.664 --> 00:11:31.580
processing techniques.

00:11:36.200 --> 00:11:39.970
I really like this comic strip.

00:11:39.970 --> 00:11:43.240
Outside in the world,
we have flying cars.

00:11:44.982 --> 00:11:46.440
Imagine what it
takes to coordinate

00:11:46.440 --> 00:11:48.070
a bunch of flying cars.

00:11:48.070 --> 00:11:50.200
We have personalized
offers being made to a guy

00:11:50.200 --> 00:11:51.660
called John Anderson.

00:11:51.660 --> 00:11:54.260
We have the management of
traffic flow throughout a city

00:11:54.260 --> 00:11:56.430
center, making sure
that people could

00:11:56.430 --> 00:11:59.010
be guided to the
appropriate parking spaces.

00:11:59.010 --> 00:12:01.976
All of these are big data
problems being solved.

00:12:01.976 --> 00:12:03.350
And inside of the
office, there's

00:12:03.350 --> 00:12:04.970
an exec guy talking
to his minions.

00:12:06.280 --> 00:12:08.480
So I did minions in this sort.

00:12:08.480 --> 00:12:09.190
So, OK.

00:12:09.190 --> 00:12:10.940
We haven't really got
a bet on or anything

00:12:10.940 --> 00:12:12.960
that we can mention
minions in each talk,

00:12:12.960 --> 00:12:16.300
but they seem to come
up everywhere nowadays.

00:12:16.300 --> 00:12:17.910
The exec guy doesn't
really get it.

00:12:17.910 --> 00:12:20.059
He's not ignorant or
anything like that.

00:12:20.059 --> 00:12:22.600
It just hasn't hit him yet about
how important this stuff is.

00:12:22.600 --> 00:12:24.210
And it's really is important.

00:12:24.210 --> 00:12:26.150
And that's why we're
having this talk today.

00:12:26.150 --> 00:12:27.830
I just really like
this comic strip.

00:12:30.150 --> 00:12:32.910
So some more examples of
gaining insight into data.

00:12:33.636 --> 00:12:35.510
Personalised insights
into your customer base

00:12:35.510 --> 00:12:36.930
is extremely important.

00:12:36.930 --> 00:12:39.320
And I imagine in North
America, like in the UK,

00:12:39.320 --> 00:12:42.190
you have grocery stores,
loyalty cards, and other stores

00:12:42.190 --> 00:12:42.960
as well, I guess.

00:12:44.140 --> 00:12:46.544
All of the big stores
have loyalty cards

00:12:46.544 --> 00:12:48.210
which can track your
activities and make

00:12:48.210 --> 00:12:50.370
you special offers
at certain points.

00:12:50.370 --> 00:12:53.120
Things like in App purchases,
Points of Sale offers.

00:12:53.120 --> 00:12:55.300
Another interesting one,
you go into to a bank

00:12:55.300 --> 00:12:56.900
to make some query
into your account,

00:12:56.900 --> 00:12:58.730
the bank teller brings
up your details,

00:12:58.730 --> 00:13:00.880
and the screen also
tells them about things

00:13:00.880 --> 00:13:02.463
that might be
interesting to you given

00:13:02.463 --> 00:13:04.110
your current financial
circumstances,

00:13:04.110 --> 00:13:05.634
and he pitches them to you.

00:13:05.634 --> 00:13:07.925
User retention activities
are also extremely important.

00:13:09.090 --> 00:13:12.100
For games particularly,
things that have a pattern

00:13:12.100 --> 00:13:12.915
based usage.

00:13:12.915 --> 00:13:15.290
Where you could use the game
for a certain period of time

00:13:15.290 --> 00:13:16.930
and then stop using
it completely.

00:13:16.930 --> 00:13:19.690
And if we're monetizing that
game, on the usage of the game,

00:13:19.690 --> 00:13:21.950
we want people to
use the game more.

00:13:21.950 --> 00:13:24.320
So we can actually analyze
the way people interact

00:13:24.320 --> 00:13:28.570
with the game and make decisions
based on somebody's interaction

00:13:28.570 --> 00:13:30.145
and to when to make
them free offers.

00:13:30.145 --> 00:13:31.020
Those kind of things.

00:13:31.020 --> 00:13:33.610
Things that might
make them continue

00:13:33.610 --> 00:13:35.300
to engage them in
the game for longer

00:13:35.300 --> 00:13:36.758
than they would
have done normally.

00:13:37.486 --> 00:13:38.610
Predicting health problems.

00:13:38.610 --> 00:13:42.210
We've done this before at
Google with identifying patterns

00:13:42.210 --> 00:13:46.435
of searches for flu,
predicting flu epidemics.

00:13:47.460 --> 00:13:49.650
Optimization of
websites where analytics

00:13:49.650 --> 00:13:51.470
is extremely
important these days.

00:13:51.470 --> 00:13:52.560
That's a big data problem.

00:13:52.560 --> 00:13:54.860
It's solved by things
like Google Analytics.

00:13:54.860 --> 00:13:57.276
And then there's things like
enabling future breakthroughs

00:13:57.276 --> 00:13:59.380
in biology and medicine,
which we'll come back to.

00:13:59.380 --> 00:14:02.070
And finally, my favorite
one, which is autonomous

00:14:02.070 --> 00:14:04.030
traffic lights and flying cars.

00:14:04.030 --> 00:14:07.800
So I want traffic lights to
make decisions themselves rather

00:14:07.800 --> 00:14:09.460
than be red.

00:14:09.460 --> 00:14:11.349
And no cars, no traffic.

00:14:11.349 --> 00:14:13.140
I want them to say,
ah, there's no traffic.

00:14:13.140 --> 00:14:15.570
I can turn to green
and let me go.

00:14:15.570 --> 00:14:17.690
That's a problem I'm
really determined

00:14:17.690 --> 00:14:18.940
to solve in my lifetime.

00:14:21.620 --> 00:14:22.970
So if anybody wants a job.

00:14:25.360 --> 00:14:29.150
So now that we've looked
at the big picture,

00:14:29.150 --> 00:14:31.240
let's talk about a
specific application

00:14:31.240 --> 00:14:35.150
to the Google Cloud Platform
because we're a data problem.

00:14:35.150 --> 00:14:37.010
So for this, we're
going to need to walk

00:14:37.010 --> 00:14:38.792
through a specific example.

00:14:38.792 --> 00:14:40.750
And we saw a few of them
in the previous slide.

00:14:40.750 --> 00:14:41.540
There are many.

00:14:41.540 --> 00:14:43.990
There are probably an infinite
number of data problems

00:14:43.990 --> 00:14:45.179
we could look at.

00:14:45.179 --> 00:14:46.720
So we had to choose
one specifically.

00:14:47.780 --> 00:14:50.360
And we have chosen one that's
specifically about genomics.

00:14:51.440 --> 00:14:56.600
And the Google genomics team,
who were formed, I think,

00:14:56.600 --> 00:14:57.720
in April.

00:14:57.720 --> 00:15:01.030
They're busy making tools that
will be used by life scientists

00:15:01.030 --> 00:15:03.780
to really handle the incoming
flood of genomic data.

00:15:03.780 --> 00:15:05.730
And genomic data is
huge on it's own,

00:15:05.730 --> 00:15:07.910
but we also have
a huge population.

00:15:07.910 --> 00:15:10.500
So we'll have a huge
data set to mine

00:15:10.500 --> 00:15:12.880
to find information out from.

00:15:12.880 --> 00:15:15.380
This ultimately will
enable future breakthroughs

00:15:15.380 --> 00:15:16.570
in biology and medicine.

00:15:17.660 --> 00:15:19.400
The scientists
working with this data

00:15:19.400 --> 00:15:21.210
know what they want to do it.

00:15:21.210 --> 00:15:22.560
They just want it to be easy.

00:15:22.560 --> 00:15:25.770
And this probably resonates
a little bit with you.

00:15:25.770 --> 00:15:28.390
Where you probably have
some good idea about what

00:15:28.390 --> 00:15:30.660
you would like to do with
data that your organization

00:15:30.660 --> 00:15:31.740
generates.

00:15:31.740 --> 00:15:34.987
Bu you just need the tools that
would make it very easy to do

00:15:34.987 --> 00:15:35.820
and very fast to do.

00:15:40.710 --> 00:15:46.140
So at the intersection of
data science and life science

00:15:46.140 --> 00:15:48.960
there are discoveries and
breakthroughs to be made.

00:15:48.960 --> 00:15:52.930
And in that particular area
of science, maybe things

00:15:52.930 --> 00:15:55.930
that could change the
game for us as a race.

00:15:55.930 --> 00:15:58.104
But you're probably not
in the genomics business.

00:15:58.104 --> 00:15:59.520
And I know I've
said this already.

00:16:00.670 --> 00:16:02.170
Is there anybody
in the audience who

00:16:02.170 --> 00:16:03.336
is in the genomics business?

00:16:04.915 --> 00:16:05.540
AUDIENCE: I am.

00:16:06.039 --> 00:16:07.580
MANDY WAITE: So
we've already spoken.

00:16:09.130 --> 00:16:09.910
I knew.

00:16:09.910 --> 00:16:10.720
I was going to
ask this question,

00:16:10.720 --> 00:16:12.010
I didn't expect anyone
to put their hands up.

00:16:12.010 --> 00:16:13.910
But we have somebody who is in
the genomics business, which

00:16:13.910 --> 00:16:14.451
is fantastic.

00:16:15.910 --> 00:16:18.760
However, if you're
not a data scientist,

00:16:18.760 --> 00:16:20.780
your particular
business, no doubt,

00:16:20.780 --> 00:16:22.950
is also a complex affair.

00:16:22.950 --> 00:16:24.650
And it requires
specialized knowledge.

00:16:24.650 --> 00:16:27.430
And you're probably an
expert in your own domain.

00:16:27.430 --> 00:16:28.930
So no doubt there
are insights to be

00:16:28.930 --> 00:16:31.665
gained at the intersection
of your business

00:16:31.665 --> 00:16:32.415
with data science.

00:16:34.960 --> 00:16:38.495
So the first question we need
to ask is, where is your data?

00:16:42.310 --> 00:16:47.300
And most companies have a
lot of data lying around.

00:16:47.300 --> 00:16:50.400
Usually one or usually
more operational databases.

00:16:51.450 --> 00:16:53.375
The examples I'm going
to be showing you

00:16:53.375 --> 00:16:56.220
are from the Personal
Genome Project.

00:16:56.220 --> 00:16:58.720
I'm not sure if it's anything
you guys are familiar with,

00:16:58.720 --> 00:17:00.636
but this is what we're
going to be looking at,

00:17:00.636 --> 00:17:01.960
the Personal Genome Project.

00:17:01.960 --> 00:17:05.140
So the PGP is effectively
creating a freely available

00:17:05.140 --> 00:17:07.990
resource, a scientific
resource, that brings together

00:17:07.990 --> 00:17:09.940
three types of data.

00:17:09.940 --> 00:17:13.859
Genomic data, environmental,
and human trait data.

00:17:13.859 --> 00:17:16.329
So that's information about
the genetic makeup of a person.

00:17:16.329 --> 00:17:18.412
The environments in which
they were brought up in,

00:17:18.412 --> 00:17:21.319
in which they live in, and which
they work in from day to day.

00:17:21.319 --> 00:17:23.430
And also, traits
that they exhibit.

00:17:23.430 --> 00:17:24.690
Do they have diabetes?

00:17:24.690 --> 00:17:26.560
Did their mother
have a brain tumor?

00:17:26.560 --> 00:17:28.230
Are they susceptible
to broken limbs?

00:17:28.230 --> 00:17:29.820
Do they have green hair?

00:17:29.820 --> 00:17:34.964
Or do they have yellow skin
and look something like him?

00:17:34.964 --> 00:17:35.880
Oh, you don't see him.

00:17:36.335 --> 00:17:38.710
I'm showing a picture of a
minion, but you can't see him.

00:17:40.580 --> 00:17:44.830
So let's just go
back to that slide.

00:17:45.900 --> 00:17:46.790
Hello, go back.

00:17:49.382 --> 00:17:51.590
So this is a data set we're
going to be working with.

00:17:51.590 --> 00:17:54.981
And we have some genomic data
stored in Google Cloud Storage,

00:17:54.981 --> 00:17:56.230
which we're going to be using.

00:17:57.750 --> 00:18:01.300
So, no matter where
your data is now,

00:18:01.300 --> 00:18:03.830
there are usually two things
that are true about it.

00:18:03.830 --> 00:18:05.470
The first is it's
not in a place where

00:18:05.470 --> 00:18:09.800
it's best or most
easy to analyze.

00:18:09.800 --> 00:18:12.155
And it's also probably
not in the right format.

00:18:13.740 --> 00:18:16.570
So this brings us to
extract, transform, and load,

00:18:16.570 --> 00:18:19.770
which is a process of taking
data in one place in one format

00:18:19.770 --> 00:18:23.030
and moving it to another
place in a different format.

00:18:23.030 --> 00:18:25.200
Now, this might be fairly
straightforward and fairly

00:18:25.200 --> 00:18:26.930
simple to do on a
small amount of data.

00:18:26.930 --> 00:18:29.344
But what if you
have a lot of data?

00:18:29.344 --> 00:18:31.260
What happens if we have
a huge amount of data,

00:18:31.260 --> 00:18:33.830
so much it will overwhelm the
tools you have available to you

00:18:33.830 --> 00:18:34.330
today?

00:18:34.330 --> 00:18:36.163
That's what we're going
to be talking about,

00:18:36.163 --> 00:18:38.330
and that's what this dump
truck represents here.

00:18:40.130 --> 00:18:42.694
So let's talk about our
data in more detail.

00:18:42.694 --> 00:18:44.360
We're not going to
go into the demo yet.

00:18:44.360 --> 00:18:50.220
But we have all this data
stored in Google Cloud Storage.

00:18:50.220 --> 00:18:53.670
We have a tool here we're using
called gsutil to actually list

00:18:53.670 --> 00:18:54.500
that data.

00:18:55.510 --> 00:18:57.390
We can actually
effectively interact

00:18:57.390 --> 00:18:59.140
with this data stored
in cloud storage

00:18:59.140 --> 00:19:01.390
as if it's local just by
using this tool, gsutil ls.

00:19:02.760 --> 00:19:06.230
And we can see we have a couple
of files listed there, only two

00:19:06.230 --> 00:19:07.000
of them.

00:19:07.000 --> 00:19:10.160
There are 175 files in
the data set in total,

00:19:10.160 --> 00:19:13.500
representing 174
participants of the PGP

00:19:13.500 --> 00:19:15.300
and one duplicate
for some reason.

00:19:20.520 --> 00:19:23.330
Each file is about
2.3 gigabytes in size

00:19:23.330 --> 00:19:24.970
and has 17 million rows.

00:19:26.040 --> 00:19:28.050
They are identified
exclusively in the file

00:19:28.050 --> 00:19:30.380
name with a identifier.

00:19:30.380 --> 00:19:34.260
In this case, we
hu011C57 and hu016B28.

00:19:35.910 --> 00:19:37.969
These are individual
participants

00:19:37.969 --> 00:19:38.885
in the Genome Project.

00:19:41.500 --> 00:19:46.710
One problem with that is that
data, that participant ID,

00:19:46.710 --> 00:19:49.340
is not in the data in the file.

00:19:49.340 --> 00:19:53.020
So we have 17 million rows
of data, and none of it

00:19:53.020 --> 00:19:55.665
contains information about
who that participant is.

00:19:55.665 --> 00:19:57.790
So that's a problem we want
to solve with our demo.

00:19:58.724 --> 00:20:00.140
So can we switch
back to the demo?

00:20:06.070 --> 00:20:08.170
So this the list of all
the files, the 175 of them

00:20:08.170 --> 00:20:08.960
that we have.

00:20:11.990 --> 00:20:17.245
And I'm just going to
quickly run a script.

00:20:18.430 --> 00:20:18.976
But I can't.

00:20:18.976 --> 00:20:21.350
I have to kind of juggle
between going back to the slides

00:20:21.350 --> 00:20:22.590
and going back from the demo.

00:20:22.590 --> 00:20:25.910
So I'm just going to run
this script on this file now.

00:20:25.910 --> 00:20:28.105
And I'm going to
pipe that to more.

00:20:28.105 --> 00:20:30.710
And this is taking that
file that we had before.

00:20:30.710 --> 00:20:34.140
And it's prepended,
added a column

00:20:34.140 --> 00:20:40.300
to the data set which
represents the participant

00:20:40.300 --> 00:20:43.240
ID, the ID of the participant
in the Genome Project.

00:20:43.240 --> 00:20:45.366
And this what we want to output.

00:20:45.366 --> 00:20:46.740
What we could do
at this point is

00:20:46.740 --> 00:20:48.448
we could actually pipe
that out to a file

00:20:48.448 --> 00:20:50.900
or redirect that out to a file.

00:20:50.900 --> 00:20:53.080
But we have 175
files, and there's

00:20:53.080 --> 00:20:55.460
17 million rows of
data in that data set.

00:20:55.460 --> 00:20:57.624
It could take a long
time to process one file.

00:20:57.624 --> 00:20:59.540
Well, how are we going
to process 175 of them?

00:21:00.752 --> 00:21:02.460
So can you go back to
the slides, please?

00:21:14.500 --> 00:21:16.530
We can run that
script on one file,

00:21:16.530 --> 00:21:19.451
but what happens when we want to
run it across all 175 of them?

00:21:19.451 --> 00:21:20.950
So this is where
Mapreduce comes in.

00:21:22.020 --> 00:21:24.440
In this case, we can run
a command on the thing

00:21:24.440 --> 00:21:27.150
that we identified as a NameNode
earlier, the master machine.

00:21:28.547 --> 00:21:30.630
If our cluster was up and
running-- unfortunately,

00:21:30.630 --> 00:21:32.004
our cluster isn't
up and running,

00:21:32.004 --> 00:21:33.960
so I can't interact
with it-- we would

00:21:33.960 --> 00:21:36.250
run the command,
a Hadoop command,

00:21:36.250 --> 00:21:38.330
and pass in some parameters.

00:21:38.330 --> 00:21:40.320
One would be where
the input data was.

00:21:40.320 --> 00:21:44.695
And that's the third line in
this command string, the lower

00:21:44.695 --> 00:21:46.980
command string, minus input.

00:21:46.980 --> 00:21:48.840
Then we would specify
the mapper script.

00:21:48.840 --> 00:21:51.730
This is the script that would
be used for the map step.

00:21:51.730 --> 00:21:54.650
Then we would also specify
the number of reduce tasks.

00:21:54.650 --> 00:21:55.855
In this case it's none.

00:21:55.855 --> 00:21:57.530
We're not doing any
kind of reducing.

00:21:57.530 --> 00:21:59.200
Our map script is very simple.

00:21:59.200 --> 00:22:01.490
We need to remove
the duplicate data

00:22:01.490 --> 00:22:05.375
and add that column right
at the start of each row.

00:22:06.720 --> 00:22:08.940
And then we need to say
but we want to output it.

00:22:08.940 --> 00:22:10.990
And we're going output it
to Google Cloud Storage

00:22:10.990 --> 00:22:12.823
in a bucket called
big-data-roadshow/output.

00:22:18.860 --> 00:22:20.890
So if I were running
the demo now--

00:22:20.890 --> 00:22:23.340
and I can't because the cluster
didn't deploy correctly--

00:22:23.340 --> 00:22:25.150
I would submit that job.

00:22:25.150 --> 00:22:27.730
And we would see
in the console we

00:22:27.730 --> 00:22:30.040
have 20 nodes available to
us to actually run the job.

00:22:31.654 --> 00:22:33.195
And then we would
drill into the job,

00:22:33.195 --> 00:22:35.290
and look at the specific
details of the job,

00:22:35.290 --> 00:22:37.110
and we would wait
for it to complete.

00:22:37.110 --> 00:22:39.190
And this job takes about
5 minutes, 41 seconds.

00:22:39.190 --> 00:22:42.440
I would have actually
worked on the 17 file set,

00:22:42.440 --> 00:22:44.332
about a tenth of the
original data set, which

00:22:44.332 --> 00:22:46.040
would take considerably
less time to run.

00:22:47.280 --> 00:22:51.220
And we can see that the map
job, there were 704 of them,

00:22:51.220 --> 00:22:53.002
and they completed successfully.

00:22:53.002 --> 00:22:55.460
There was nothing to do in the
reduce step, no jobs at all.

00:22:55.460 --> 00:22:57.050
And that was successful.

00:22:57.050 --> 00:22:58.690
So that has now
processed our data.

00:22:58.690 --> 00:23:01.390
It's moved the data from
one place in one format

00:23:01.390 --> 00:23:03.200
to another place
in another format.

00:23:03.200 --> 00:23:06.465
Exactly what extract,
transform, and load specifies.

00:23:08.300 --> 00:23:09.020
So that's good.

00:23:09.020 --> 00:23:12.140
So that's more information.

00:23:12.140 --> 00:23:16.730
435 gigabytes roughly of data
was processed by the Hadoop job

00:23:16.730 --> 00:23:19.230
and takes about 5
and 1/2 minutes.

00:23:19.230 --> 00:23:20.970
And 3 million rows of data.

00:23:24.750 --> 00:23:26.250
So then we could
delete the cluster.

00:23:26.250 --> 00:23:27.958
I would really like
to delete the cluster

00:23:27.958 --> 00:23:31.050
because it failed me, didn't
it? [INAUDIBLE] Didn't it?

00:23:32.511 --> 00:23:34.260
One thing that never
fails me is BigQuery,

00:23:34.260 --> 00:23:36.176
though, so we're going
to talk about BigQuery.

00:23:37.010 --> 00:23:38.810
So there is more,
there's always more.

00:23:39.910 --> 00:23:41.987
At this point
though, that may be

00:23:41.987 --> 00:23:43.320
all you need to do in your data.

00:23:43.320 --> 00:23:45.028
And you may be doing
it on a daily basis.

00:23:45.028 --> 00:23:47.790
You may be processing your data
from one format in one place

00:23:47.790 --> 00:23:49.760
to another format in
a different place.

00:23:49.760 --> 00:23:51.730
We do that at
Google all the time,

00:23:51.730 --> 00:23:54.120
contrary to popular opinion.

00:23:54.120 --> 00:23:55.810
Some people may
actually get that joke,

00:23:55.810 --> 00:23:56.980
but others might not.

00:23:58.180 --> 00:23:59.900
We still do lots of Mapreduces.

00:23:59.900 --> 00:24:02.390
And mapreduces can be a key
part of an organization's data

00:24:02.390 --> 00:24:06.120
processing tool kit.

00:24:06.120 --> 00:24:07.395
But there is more.

00:24:07.395 --> 00:24:09.890
In this stage we want to
actually process the data

00:24:09.890 --> 00:24:11.610
and get insight from the data.

00:24:11.610 --> 00:24:13.350
So we're going to
use Google BigQuery.

00:24:13.350 --> 00:24:16.720
And I think Jerome
mentioned BigQuery already.

00:24:17.790 --> 00:24:21.090
BigQuery effectively allows
us to run interactive queries

00:24:21.090 --> 00:24:22.220
across massive data sets.

00:24:22.220 --> 00:24:24.345
And we're talking about
terabytes of data, billions

00:24:24.345 --> 00:24:25.990
and billions of rows of data.

00:24:25.990 --> 00:24:29.100
Interactive queries are great
because you can ask questions

00:24:29.100 --> 00:24:31.230
of the data and then you
can modify the question

00:24:31.230 --> 00:24:34.220
you asked based on the
results you've just seen.

00:24:34.220 --> 00:24:36.380
We do this all the time
with our log files.

00:24:36.380 --> 00:24:38.530
So we have lots of log
data internally at Google.

00:24:38.530 --> 00:24:40.542
We have the internal
version of BigQuery,

00:24:40.542 --> 00:24:41.500
which is called Dremel.

00:24:41.500 --> 00:24:44.215
We run interactive
queries across the log

00:24:44.215 --> 00:24:46.847
files that we have to
see what's going on.

00:24:46.847 --> 00:24:48.430
There's no limit on
the amount of data

00:24:48.430 --> 00:24:50.221
that you can store and
process in BigQuery.

00:24:51.500 --> 00:24:52.417
It's very easy to use.

00:24:52.417 --> 00:24:53.999
You don't have to
administer anything.

00:24:53.999 --> 00:24:55.670
You don't have to
create a machine.

00:24:55.670 --> 00:24:57.940
You don't have to do
the build process here

00:24:57.940 --> 00:24:59.140
that failed for us.

00:24:59.140 --> 00:25:00.780
You have the machines
available to you

00:25:00.780 --> 00:25:02.430
that are needed to
process the data.

00:25:02.430 --> 00:25:04.940
We provide those
dynamically, automatically.

00:25:05.950 --> 00:25:09.070
It has all of the convenience
of SQL, as we'll see shortly.

00:25:09.070 --> 00:25:10.210
It's an API.

00:25:10.210 --> 00:25:11.414
BigQuery is an API.

00:25:11.414 --> 00:25:13.330
We'll see a console
shortly, but that was just

00:25:13.330 --> 00:25:14.470
written on top of the API.

00:25:16.165 --> 00:25:18.540
The first one terabyte of data
that you process per month

00:25:18.540 --> 00:25:19.040
is free.

00:25:20.940 --> 00:25:22.990
It provides a very familiar
database structure.

00:25:22.990 --> 00:25:25.452
So you have databases, you
have tables within the data,

00:25:25.452 --> 00:25:26.285
within the database.

00:25:27.950 --> 00:25:31.200
Data management is very easy,
and it has the opportunity

00:25:31.200 --> 00:25:34.772
to import data atomically
very, very quickly.

00:25:34.772 --> 00:25:36.230
There are various
methodologies you

00:25:36.230 --> 00:25:37.646
can use to get
data into BigQuery.

00:25:39.850 --> 00:25:41.695
So can we jump back
to the demo, please?

00:25:49.310 --> 00:25:51.400
So this is the BigQuery console.

00:25:54.130 --> 00:25:55.267
Can everybody read that?

00:25:55.267 --> 00:25:55.767
OK.

00:25:56.900 --> 00:25:58.160
Anybody not read it?

00:25:58.160 --> 00:25:59.110
OK, cool, excellent.

00:26:00.340 --> 00:26:05.390
OK, so, we have plenty of public
data sets that are available

00:26:05.390 --> 00:26:06.740
and sample data sets as well.

00:26:06.740 --> 00:26:10.940
So BigQuery samples of data
sets we can play around with.

00:26:10.940 --> 00:26:13.184
We have one called-- I
can't see it completely

00:26:13.184 --> 00:26:14.350
there-- wikimedia_pageviews.

00:26:15.720 --> 00:26:20.290
And this is Wikimedia pages that
were viewed on a given date.

00:26:20.290 --> 00:26:24.150
And it's all sharded
into several tables.

00:26:24.150 --> 00:26:29.390
Here we see them by date and
go all the way down to 201207

00:26:29.390 --> 00:26:31.360
We haven't got any
later data than that,

00:26:31.360 --> 00:26:33.070
but we go up to 201207.

00:26:33.070 --> 00:26:35.640
So we have sharded data
by month, effectively,

00:26:35.640 --> 00:26:37.710
for these pageviews
in Wikimedia.

00:26:39.160 --> 00:26:42.575
And what I can do is
I can load up a query.

00:26:44.460 --> 00:26:53.370
And I can zoom in on my
query, make it a bit bigger.

00:26:54.649 --> 00:26:55.690
And I can run this query.

00:26:55.690 --> 00:26:59.000
So basically, this is BigQuery.

00:26:59.000 --> 00:27:02.530
It looks very much like SQL,
if anybody's used SQL already.

00:27:02.530 --> 00:27:05.610
We're going to be selecting the
title and the number of views,

00:27:05.610 --> 00:27:08.330
or we're going to sum the number
of views for a particular page.

00:27:09.820 --> 00:27:12.250
We're going to do it across
a number of those tables.

00:27:12.250 --> 00:27:15.150
So we can actually do this
across multiple tables,

00:27:15.150 --> 00:27:17.040
one query across
multiple tables.

00:27:17.040 --> 00:27:19.670
We're going to add
some where clauses.

00:27:19.670 --> 00:27:21.880
The important ones
are language equals

00:27:21.880 --> 00:27:25.690
en and the regex
expression here.

00:27:25.690 --> 00:27:28.110
Something beginning with G,
has an O in it, and ends in E.

00:27:28.110 --> 00:27:30.250
Can anybody think of a
word that might fit that?

00:27:31.380 --> 00:27:31.880
Not sure.

00:27:33.080 --> 00:27:33.750
Group by title.

00:27:33.750 --> 00:27:35.932
And views, order by
views descending.

00:27:35.932 --> 00:27:37.140
There's a little bug in this.

00:27:37.140 --> 00:27:39.348
I don't how I ended up with
that, but it still works.

00:27:39.348 --> 00:27:42.130
So I'm not going to
worry about it too much.

00:27:42.130 --> 00:27:43.660
Come back to here.

00:27:43.660 --> 00:27:46.362
I can click on this green arrow
down here and see how much

00:27:46.362 --> 00:27:47.320
data that will process.

00:27:48.640 --> 00:27:52.860
That's going to process 507
gigabytes when it's run.

00:27:52.860 --> 00:27:56.940
It also tells me that the
query I'm running is valid.

00:27:56.940 --> 00:27:59.520
So we get one terabyte of
data of free per month.

00:27:59.520 --> 00:28:01.850
This is going to use half
my quota up in a month.

00:28:01.850 --> 00:28:02.940
And obviously I've run
it a couple of times

00:28:02.940 --> 00:28:05.370
already just to practice to
make sure it was working.

00:28:05.370 --> 00:28:07.661
So I'm already over the top
in terms of using my quota.

00:28:07.661 --> 00:28:10.190
But don't do this
unless you want

00:28:10.190 --> 00:28:13.020
to spend money because
it's a big data set.

00:28:13.020 --> 00:28:14.544
So let's come back to here.

00:28:14.544 --> 00:28:15.710
I'm going to run that query.

00:28:18.960 --> 00:28:21.110
We go down here where
it says query running.

00:28:22.880 --> 00:28:25.400
And it takes probably
about 12, 30 seconds.

00:28:28.650 --> 00:28:31.390
If there's any problems
with the timing,

00:28:31.390 --> 00:28:34.580
it's because of it's
trying to contact

00:28:34.580 --> 00:28:36.394
the job that's running remotely.

00:28:36.394 --> 00:28:38.310
And that could be impacted
by the Wi-Fi speeds

00:28:38.310 --> 00:28:39.351
and various other things.

00:28:40.920 --> 00:28:42.050
So we processed that.

00:28:42.050 --> 00:28:45.120
507 gigabytes processed
in 18.4 seconds.

00:28:45.120 --> 00:28:50.380
And we can see there.

00:28:51.530 --> 00:28:54.677
Sometimes it's hard to
play with the real estate.

00:28:54.677 --> 00:28:55.900
Ah, Google!

00:28:55.900 --> 00:28:57.890
Of course, I didn't
think about that.

00:28:57.890 --> 00:28:59.795
And back in those days,
Gotye were famous.

00:29:00.609 --> 00:29:02.400
Again, that's another
joke that many of you

00:29:02.400 --> 00:29:05.862
might-- Google Chrome
is also quite popular.

00:29:05.862 --> 00:29:07.820
And again, sometimes when
I'm running this demo

00:29:07.820 --> 00:29:08.920
I'm do it in
different languages.

00:29:08.920 --> 00:29:11.440
But I run it in France, I'll
do it with fr as language,

00:29:11.440 --> 00:29:12.670
and we'll see other
interesting things.

00:29:12.670 --> 00:29:14.390
And you always have
to run it beforehand

00:29:14.390 --> 00:29:15.910
to make sure nothing
rude comes up.

00:29:18.790 --> 00:29:19.610
Well, you do.

00:29:19.610 --> 00:29:21.270
Can we go back to
the slides, please?

00:29:25.470 --> 00:29:27.430
OK, so we need to
load up our data.

00:29:27.430 --> 00:29:30.860
So imagine the data we created
from our mapreduce job.

00:29:30.860 --> 00:29:32.300
We can load it up here.

00:29:32.300 --> 00:29:37.290
We'll enter the path to
our cloud storage bucket.

00:29:37.290 --> 00:29:40.370
We'll say it's a
CSV separated file.

00:29:40.370 --> 00:29:43.060
And we'll click Next, specify
the schema for the data,

00:29:43.060 --> 00:29:44.994
and we can import that
data into BigQuery.

00:29:44.994 --> 00:29:46.410
Fortunately for
us, that's already

00:29:46.410 --> 00:29:49.779
been done for us
because we have--

00:29:49.779 --> 00:29:51.570
and we'll just close
this down a little bit

00:29:51.570 --> 00:29:55.985
and go to display project.

00:30:02.181 --> 00:30:02.680
Big Gene.

00:30:06.589 --> 00:30:08.130
And you can do this
yourself as well.

00:30:08.130 --> 00:30:11.300
So you can go to BigQuery, you
can add this data set yourself

00:30:11.300 --> 00:30:13.110
to your own BigQuery set up.

00:30:16.216 --> 00:30:17.840
I'll just zoom in a
little bit on that.

00:30:18.850 --> 00:30:24.160
We have three tables
in our PGP database,

00:30:24.160 --> 00:30:26.270
cgi_variants,
phenotypes, and variants.

00:30:26.270 --> 00:30:28.320
And we care about this
one, cgi_variants.

00:30:29.680 --> 00:30:31.170
So we can select
that and then we

00:30:31.170 --> 00:30:33.510
could look at information
about the schema.

00:30:38.124 --> 00:30:39.040
Bear with me a second.

00:30:42.730 --> 00:30:44.480
It's hard to know what
I'm scrolling here.

00:30:45.657 --> 00:30:47.990
And this has lots of information
that's genomic related.

00:30:47.990 --> 00:30:50.600
Remember, all this stuff could
apply equally to your own data.

00:30:50.600 --> 00:30:53.870
This just happens to
be a genomic data.

00:30:53.870 --> 00:30:56.010
So back to the slides, please.

00:30:57.692 --> 00:30:59.650
I'm going to run out of
time very quickly, see.

00:31:03.490 --> 00:31:05.960
So we can actually run
a query to work out

00:31:05.960 --> 00:31:07.860
how many rows are
in that data set.

00:31:07.860 --> 00:31:10.440
I'm not going to do that now
because we're short on time.

00:31:10.440 --> 00:31:12.779
But we have, from the
row count done there,

00:31:12.779 --> 00:31:13.820
about three million rows.

00:31:14.940 --> 00:31:17.710
And then we had to ask, what
question do we want to solve?

00:31:17.710 --> 00:31:20.480
What problem do we want to ask
of this data set that we have?

00:31:20.480 --> 00:31:24.870
So apparently there's
been some press

00:31:24.870 --> 00:31:27.550
about a study that
effectively identified

00:31:27.550 --> 00:31:30.089
that a variant, a
genetic variant,

00:31:30.089 --> 00:31:31.880
may have something to
do with intelligence.

00:31:31.880 --> 00:31:34.560
And that that, according
to this headline anyway,

00:31:34.560 --> 00:31:36.930
is actually related
to longevity.

00:31:36.930 --> 00:31:38.440
In fact, they've
given it to mice,

00:31:38.440 --> 00:31:40.550
and it's made the
mice live longer.

00:31:40.550 --> 00:31:41.730
It's also made them smarter.

00:31:41.730 --> 00:31:42.970
I'm not quite sure
how they measure that.

00:31:42.970 --> 00:31:45.386
I guess they have these little
mazes that they run around.

00:31:45.610 --> 00:31:47.360
I guess you can tell
how smart a mouse is.

00:31:47.972 --> 00:31:49.680
It sounds like a pretty
good deal, right?

00:31:49.680 --> 00:31:52.130
So let's find out
how many participants

00:31:52.130 --> 00:31:55.240
of the Human Genome Project,
Personal Genome Project,

00:31:55.240 --> 00:31:56.650
have this variant.

00:31:59.430 --> 00:32:02.430
We could also enter this
variant name by name.

00:32:05.040 --> 00:32:09.666
It's called RS9536314.

00:32:09.666 --> 00:32:11.540
But variants are a little
bit like asteroids.

00:32:11.540 --> 00:32:13.375
There's so many of
them, they don't even

00:32:13.375 --> 00:32:14.650
bother to name most of them.

00:32:14.650 --> 00:32:16.220
Until one of them
is interesting.

00:32:16.220 --> 00:32:18.120
Then they give it
some scientific name

00:32:18.120 --> 00:32:22.780
like RS956314, which is great.

00:32:22.780 --> 00:32:28.210
If you search for
RS956314, you'll

00:32:28.210 --> 00:32:30.810
get this page, which tells
you it's in chromosome number

00:32:30.810 --> 00:32:32.830
13, which is down
there at the bottom.

00:32:32.830 --> 00:32:35.280
So we want to search
chromosome 13 for that variant.

00:32:36.457 --> 00:32:38.040
We're going to do
this is in BigQuery.

00:32:39.590 --> 00:32:42.654
What we're going to find out is
that we have these two columns.

00:32:42.654 --> 00:32:44.820
One of them is sized badly
because of the projector.

00:32:45.920 --> 00:32:47.730
Allele1Seq, allele2Seq.

00:32:48.820 --> 00:32:51.770
If they are both empty, then
they don't have the variant.

00:32:51.770 --> 00:32:54.080
If one is G and
one is T, there's

00:32:54.080 --> 00:32:55.590
a variant in one allele.

00:32:55.590 --> 00:32:58.906
If they are G and G, both
alleles have the variant.

00:32:58.906 --> 00:33:01.030
If they're question marks,
it means we're not sure.

00:33:01.030 --> 00:33:02.988
We don't have that
information within the data.

00:33:05.360 --> 00:33:06.904
So let's run that in BigQuery.

00:33:06.904 --> 00:33:08.070
Can you go back to the demo?

00:33:17.500 --> 00:33:19.040
So here's my query.

00:33:22.881 --> 00:33:25.130
We're going to be selecting
things like the sample ID,

00:33:25.130 --> 00:33:26.595
this is the participant ID.

00:33:26.595 --> 00:33:27.220
The chromosome.

00:33:28.720 --> 00:33:31.114
The two alleles that we're
really interested in.

00:33:31.114 --> 00:33:33.280
We also are going to be
looking for a specific place

00:33:33.280 --> 00:33:35.970
because the list that we
looked at earlier actually

00:33:35.970 --> 00:33:36.590
identified it.

00:33:36.590 --> 00:33:38.835
It was in a specific place
within the chromosome.

00:33:40.170 --> 00:33:42.740
And we're going to be searching
the cgi_variant table.

00:33:43.566 --> 00:33:44.440
Does that make sense?

00:33:45.009 --> 00:33:46.800
OK, we're so going to
be looking chromosome

00:33:46.800 --> 00:33:49.700
13, at a specific location,
for those variants.

00:33:49.700 --> 00:33:55.140
So I come back
out, run the query,

00:33:55.140 --> 00:33:57.820
and that doesn't take
very long to run, I think.

00:34:00.022 --> 00:34:00.730
It's run already.

00:34:00.730 --> 00:34:02.810
So 5.7 seconds elapsed.

00:34:02.810 --> 00:34:04.175
117 gigabytes processed.

00:34:04.175 --> 00:34:07.220
Now you'll notice that's
less than the total size

00:34:07.220 --> 00:34:08.409
of the data.

00:34:08.409 --> 00:34:10.909
But that's because we're not
looking at the entire data set.

00:34:10.909 --> 00:34:12.810
So BigQuery uses
a columnar format

00:34:12.810 --> 00:34:14.699
for storing data
instead of rows.

00:34:14.699 --> 00:34:17.489
So when it reads data, it
only loads in the columns

00:34:17.489 --> 00:34:19.469
that you're actually
interested in.

00:34:19.469 --> 00:34:22.389
So that's only a small part of
the data set, as in this case.

00:34:22.389 --> 00:34:25.173
So we're only looking at a
small section of the data,

00:34:25.173 --> 00:34:26.464
but we're looking at every row.

00:34:28.139 --> 00:34:32.090
So in terms of results,
we have-- again,

00:34:32.090 --> 00:34:36.750
BigQuery getting in the way with
its-- We have lots of options.

00:34:36.750 --> 00:34:39.351
We have 174 rows processed here.

00:34:39.351 --> 00:34:41.100
We can see one immediately
that's actually

00:34:41.100 --> 00:34:44.290
got that chromosome in one
allele, which is great, OK.

00:34:44.290 --> 00:34:46.119
That's told us what
we do have patterns.

00:34:47.170 --> 00:34:50.929
We do have participants that
do have that genetic variant.

00:34:50.929 --> 00:34:53.540
We wanted to find out
how many of them had.

00:34:53.540 --> 00:34:55.070
So let's modify our query.

00:34:59.350 --> 00:35:01.480
And this time, I'll go
back to my queue history

00:35:01.480 --> 00:35:03.479
rather than typing it in,
because you've already

00:35:03.479 --> 00:35:04.665
seen my typing's lousy.

00:35:04.665 --> 00:35:08.170
And we'll run an aggregate
query across that same data set.

00:35:09.530 --> 00:35:10.290
And I'll zoom in.

00:35:12.400 --> 00:35:15.510
In this case, we're looking
for pretty much the same data,

00:35:15.510 --> 00:35:18.800
but we're counting the number
of sample IDs that match.

00:35:18.800 --> 00:35:20.172
We're looking the same place.

00:35:20.172 --> 00:35:22.130
And we're going to group
by it and order by it.

00:35:22.130 --> 00:35:24.710
So we can do aggregate
queries with BigQuery as well.

00:35:24.710 --> 00:35:26.160
So I'm going to run that query.

00:35:30.470 --> 00:35:32.755
And come down here,
query running.

00:35:38.770 --> 00:35:44.150
11.5 seconds elapsed,
117 gigabytes processed.

00:35:44.150 --> 00:35:45.360
Resize this.

00:35:45.360 --> 00:35:49.360
And we can see 135 participants
don't have that variant.

00:35:50.460 --> 00:35:54.350
32 of them have it in one
of their rows and 5 of them

00:35:54.350 --> 00:35:55.910
have it in both.

00:35:55.910 --> 00:35:56.940
Lucky people.

00:35:56.940 --> 00:35:58.970
They're smart, and they're
going to live long.

00:35:58.970 --> 00:36:00.280
I want some of that.

00:36:00.280 --> 00:36:02.820
So when they do genetic
therapy, I want this one.

00:36:04.560 --> 00:36:06.390
So can you go back to
the slides, please?

00:36:12.120 --> 00:36:13.220
So these are the results.

00:36:13.220 --> 00:36:14.830
We saw this already.

00:36:14.830 --> 00:36:17.019
They're just there in
case anything falls apart.

00:36:17.019 --> 00:36:19.060
So basically what we've
done here is effectively,

00:36:19.060 --> 00:36:20.560
we've gone from data to meaning.

00:36:20.560 --> 00:36:22.652
We've had to decide
what kind of questions

00:36:22.652 --> 00:36:25.110
we want to ask because we want
to have the data in the best

00:36:25.110 --> 00:36:26.676
possible format to answer them.

00:36:26.676 --> 00:36:28.800
We don't always know what
questions we want to ask,

00:36:28.800 --> 00:36:31.091
but we always have a good
idea of what kind of question

00:36:31.091 --> 00:36:32.330
we want to ask.

00:36:32.330 --> 00:36:34.674
Then we're going to
say, where is our data?

00:36:34.674 --> 00:36:35.715
Is it in the right place?

00:36:35.715 --> 00:36:36.870
Is it in the right format?

00:36:36.870 --> 00:36:40.259
In this example, we've taken
data stored in cloud storage.

00:36:40.259 --> 00:36:42.050
We've done an extract,
transform, and load.

00:36:42.050 --> 00:36:43.790
And we've stored
it into BigQuery.

00:36:43.790 --> 00:36:46.160
Once into BigQuery, we've
actually run queries over it

00:36:46.160 --> 00:36:47.882
to gain insight into that data.

00:36:47.882 --> 00:36:49.340
Now we could do
this interactively.

00:36:49.340 --> 00:36:50.540
We could do this
as much as we like.

00:36:50.540 --> 00:36:53.123
We could also program the whole
thing and do it automatically.

00:36:54.300 --> 00:36:55.410
The last couple of things.

00:36:55.410 --> 00:36:58.270
Coming soon, we have Cloud
Dataflow, Cloud Pub/Sub.

00:36:58.270 --> 00:37:01.390
These are really important
changes in the way we do data.

00:37:01.390 --> 00:37:03.835
We have Google I/O talks that
will introduce you to them.

00:37:03.835 --> 00:37:05.960
I can't do them justice
here, so talking about them

00:37:05.960 --> 00:37:07.170
would be pointless.

00:37:07.170 --> 00:37:10.082
We also have the ability
to build Apache Spark

00:37:10.082 --> 00:37:11.165
clusters using the bdutil.

00:37:12.300 --> 00:37:15.235
And spark is in memory,
effectively, cluster.

00:37:15.235 --> 00:37:16.890
It is much, much
faster than anything

00:37:16.890 --> 00:37:19.369
we've seen today
in terms of Hadoop,

00:37:19.369 --> 00:37:20.410
not in terms of BigQuery.

00:37:22.180 --> 00:37:23.490
And resources as well.

00:37:23.490 --> 00:37:25.252
I'll leave that slide
up while I bow out.

00:37:25.252 --> 00:37:26.710
But these are the
resources you can

00:37:26.710 --> 00:37:30.020
use to access to give you
more information about what

00:37:30.020 --> 00:37:31.410
we talked about today.

00:37:31.410 --> 00:37:32.820
So that's pretty much it.

00:37:32.820 --> 00:37:35.420
We can talk about this later
if you have any more questions.

00:37:35.420 --> 00:37:37.410
But thank you very much.

