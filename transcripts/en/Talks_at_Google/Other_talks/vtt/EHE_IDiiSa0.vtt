WEBVTT
Kind: captions
Language: en

00:00:01.577 --> 00:00:03.910
JOHN: What I'd like to do
this afternoon is to introduce

00:00:03.910 --> 00:00:08.780
Ozalp Babaoglu, who is a full
professor at Bologna Computer

00:00:08.780 --> 00:00:10.080
Science and Engineering.

00:00:10.080 --> 00:00:11.800
And he's going to
be talking to us

00:00:11.800 --> 00:00:14.874
about some stuff around his
ideas around sustainable

00:00:14.874 --> 00:00:16.040
computing with data science.

00:00:16.040 --> 00:00:17.640
Thank you.

00:00:17.640 --> 00:00:19.390
OZALP BABAOGLU: Thank you, John.

00:00:19.390 --> 00:00:21.880
I'd like to thank,
also, also Eric Schmidt,

00:00:21.880 --> 00:00:25.300
who organized our social
visit to Google today,

00:00:25.300 --> 00:00:27.200
with my family.

00:00:27.200 --> 00:00:30.750
First time I'm here at Google,
and I'm very, very delighted

00:00:30.750 --> 00:00:32.650
to be here.

00:00:32.650 --> 00:00:36.880
Thank John for
organizing the talk.

00:00:36.880 --> 00:00:41.510
So what I want to do is present
to you some ideas that we have.

00:00:41.510 --> 00:00:46.840
And this is joint work with
my post-doc Alina Sirbu.

00:00:46.840 --> 00:00:49.560
Questions regarding how
to make high performance

00:00:49.560 --> 00:00:51.480
computing sustainable.

00:00:51.480 --> 00:00:54.820
And the main point I want
to get through to you

00:00:54.820 --> 00:01:00.700
is the use of data science,
and data-driven techniques.

00:01:00.700 --> 00:01:04.319
So HPC, High-Performance
Computing,

00:01:04.319 --> 00:01:08.290
has this holy grail
of exascale computing,

00:01:08.290 --> 00:01:13.550
which is 10 to the 18th
operations per second.

00:01:13.550 --> 00:01:18.080
This is picked as a goal
because it's commonly

00:01:18.080 --> 00:01:22.050
believed that at that rate
of-- that speed of computing

00:01:22.050 --> 00:01:25.630
we could have machines
come close to the power

00:01:25.630 --> 00:01:28.200
of the human brain.

00:01:28.200 --> 00:01:32.912
So this is seen as a holy grail.

00:01:32.912 --> 00:01:33.796
[CHIME]

00:01:33.796 --> 00:01:37.910
Now the initial timeline
for reaching this goal

00:01:37.910 --> 00:01:41.070
was set to 2018,
but recently it's

00:01:41.070 --> 00:01:44.600
been pushed forward due to
all of the technical problems

00:01:44.600 --> 00:01:46.050
that you can imagine.

00:01:46.050 --> 00:01:52.770
So reaching this
goal in HPC obviously

00:01:52.770 --> 00:01:56.660
requires significant
advances initially.

00:01:56.660 --> 00:02:01.660
First, in hardware technologies
like raw processing power.

00:02:01.660 --> 00:02:04.850
Also parallelization techniques
at the hardware level

00:02:04.850 --> 00:02:08.680
and at the software level,
because to get to these speeds,

00:02:08.680 --> 00:02:11.120
obviously we need
massive parallelization.

00:02:11.120 --> 00:02:15.290
Both in the number of computers
and also in the software

00:02:15.290 --> 00:02:16.120
threads.

00:02:16.120 --> 00:02:19.460
So we imagine millions
of computing elements

00:02:19.460 --> 00:02:23.130
with billions of threads
to achieve the goal.

00:02:23.130 --> 00:02:27.430
And the other issues have to
do with power efficiency, power

00:02:27.430 --> 00:02:30.580
dissipation, and just
the sheer management

00:02:30.580 --> 00:02:34.660
and control of this entire
infrastructure because

00:02:34.660 --> 00:02:37.140
of the scale.

00:02:37.140 --> 00:02:42.630
And what I'm trying to convey
today is that these problems,

00:02:42.630 --> 00:02:46.570
to address them, we really need
to have a holistic approach.

00:02:46.570 --> 00:02:50.340
That is, we cannot concentrate
on subsystems in isolation,

00:02:50.340 --> 00:02:56.180
but need to look at the
big picture in its whole.

00:02:56.180 --> 00:03:02.670
So what I want to do is relate
this problem, perhaps to you,

00:03:02.670 --> 00:03:06.860
and Google, in that I believe
that many of the issues having

00:03:06.860 --> 00:03:10.360
to do with HPC reaching
exascale performance

00:03:10.360 --> 00:03:13.710
can be related also
to large data centers.

00:03:13.710 --> 00:03:16.020
Which, I think,
makes the problem

00:03:16.020 --> 00:03:19.090
quite relevant to an
audience such as yourself.

00:03:22.120 --> 00:03:28.310
Because a large data centers
essentially replicate many

00:03:28.310 --> 00:03:31.390
of the problems that
exascale HPC computing

00:03:31.390 --> 00:03:34.645
does, especially with regard
to management and control.

00:03:37.230 --> 00:03:44.000
So this is a data center,
in the right lower corner.

00:03:44.000 --> 00:03:45.880
But what I want to
do with this picture

00:03:45.880 --> 00:03:48.642
is just put that
in context to you.

00:03:48.642 --> 00:03:51.530
And this is what I mean
by a holistic approach.

00:03:51.530 --> 00:03:54.790
That is, we don't want to
look at the data center

00:03:54.790 --> 00:04:00.060
in isolation, because it's
part of a huge ecosystem that

00:04:00.060 --> 00:04:02.670
includes many, many,
many other elements.

00:04:02.670 --> 00:04:06.420
And I indicated some of
them in the block diagram,

00:04:06.420 --> 00:04:09.510
having to do with, all the
way from the power generation

00:04:09.510 --> 00:04:13.530
station that provides the
electrical power, to get it up

00:04:13.530 --> 00:04:17.320
to the data center,
the backup UPS

00:04:17.320 --> 00:04:23.640
facilities, the seismic security
having to do with earthquakes,

00:04:23.640 --> 00:04:29.660
and the entire non-negligible--
on the contrary,

00:04:29.660 --> 00:04:31.750
quite a considerable
part of a data center

00:04:31.750 --> 00:04:33.830
has to do with the cooling.

00:04:33.830 --> 00:04:38.560
So we need to dissipate all
of the heat that's generated.

00:04:38.560 --> 00:04:41.230
So the data center
itself is only

00:04:41.230 --> 00:04:43.080
a tiny piece of
this big picture.

00:04:43.080 --> 00:04:47.470
So what I'm advocating is that
the sustainability question

00:04:47.470 --> 00:04:50.320
really needs to be addressed
looking at the big picture,

00:04:50.320 --> 00:04:55.080
and not concentrating just
on the computing element.

00:04:55.080 --> 00:04:57.900
So how do we do
management today,

00:04:57.900 --> 00:05:00.310
at least in the context
of a data center?

00:05:00.310 --> 00:05:03.760
Well, these are some
of the tools that have

00:05:03.760 --> 00:05:05.770
been developed over the years.

00:05:05.770 --> 00:05:07.090
I've listed some of them.

00:05:07.090 --> 00:05:11.550
And many of them have been
developed right here at Google,

00:05:11.550 --> 00:05:14.660
by people like John.

00:05:14.660 --> 00:05:18.000
What I can say is that
most of these tools

00:05:18.000 --> 00:05:22.480
help us in automating some of
the low-level infrastructure

00:05:22.480 --> 00:05:25.020
provisioning and
resource allocation.

00:05:25.020 --> 00:05:30.300
But even at their best, the
human element, the operator,

00:05:30.300 --> 00:05:36.240
remains an integral part of
the management structure.

00:05:36.240 --> 00:05:39.130
That is, we cannot completely
take the human element out

00:05:39.130 --> 00:05:41.980
of the loop.

00:05:41.980 --> 00:05:45.510
And if we want to scale these
data centers to millions

00:05:45.510 --> 00:05:50.190
of computing
elements, then we're

00:05:50.190 --> 00:05:53.900
going-- if you scale the number
of operators currently employed

00:05:53.900 --> 00:05:57.710
in a typical data center by
several orders of magnitude,

00:05:57.710 --> 00:06:00.140
we end up with
armies of operators.

00:06:00.140 --> 00:06:04.950
And that, I claim,
is not sustainable.

00:06:04.950 --> 00:06:07.680
So we need to address that.

00:06:07.680 --> 00:06:12.660
And operators in a data center
are quite often detrimental,

00:06:12.660 --> 00:06:13.890
rather than useful.

00:06:13.890 --> 00:06:18.900
So this is a statistic taken
from a study, the most recent

00:06:18.900 --> 00:06:21.470
being 2013.

00:06:21.470 --> 00:06:25.090
So this is ranking
the root sources

00:06:25.090 --> 00:06:29.460
of outages in data centers.

00:06:29.460 --> 00:06:34.540
At the top we have the UPS power
system failure accounts to--

00:06:34.540 --> 00:06:38.630
and these are for two
fiscal years, 2013 and 2010.

00:06:38.630 --> 00:06:41.410
In the 2013 case,
the power failure

00:06:41.410 --> 00:06:43.990
accounts to about a
quarter of the outages.

00:06:43.990 --> 00:06:49.730
And if you notice, the second
most common source of outage

00:06:49.730 --> 00:06:50.790
is the human element.

00:06:50.790 --> 00:06:52.990
So this is the
accidental human error.

00:06:52.990 --> 00:06:55.300
So these are the
so-called oops errors

00:06:55.300 --> 00:07:00.430
that operators commit while
doing routine maintenance

00:07:00.430 --> 00:07:03.890
or update operations.

00:07:03.890 --> 00:07:07.400
So this is absolutely not
something that we cannot

00:07:07.400 --> 00:07:08.570
ignore.

00:07:08.570 --> 00:07:12.480
And if we increase the number
of operators in the data center,

00:07:12.480 --> 00:07:14.910
this percentage can
only get bigger.

00:07:14.910 --> 00:07:20.181
So the role of operators--
this is a quote,

00:07:20.181 --> 00:07:22.940
I'll just read it to you. "One
of the biggest opportunities

00:07:22.940 --> 00:07:26.120
for entrepreneurs
in the enterprise

00:07:26.120 --> 00:07:28.170
is cutting out
the human IT staff

00:07:28.170 --> 00:07:30.690
and going full
automation, potentially

00:07:30.690 --> 00:07:32.730
with technologies
like deep learning.

00:07:32.730 --> 00:07:35.960
It's ridiculous to have humans
manage the level of complexity

00:07:35.960 --> 00:07:38.250
that we have inside
the data center."

00:07:38.250 --> 00:07:41.770
And the quote is
due to Vinod Khosla,

00:07:41.770 --> 00:07:43.950
who I'm sure many of you know.

00:07:43.950 --> 00:07:47.540
He's a noted venture
capitalist and technologist,

00:07:47.540 --> 00:07:50.040
one of the founders
of Sun Microsystems.

00:07:50.040 --> 00:07:54.250
And this was something he
said at a meeting in 2014.

00:07:54.250 --> 00:07:59.995
So this vision of
the human operator

00:07:59.995 --> 00:08:01.986
in the data center
as being detrimental

00:08:01.986 --> 00:08:03.110
is something that's shared.

00:08:03.110 --> 00:08:06.580
It's not just my
paranoia, but it's also

00:08:06.580 --> 00:08:09.060
shared by other people.

00:08:09.060 --> 00:08:13.320
So what I want to propose
is a shift from human-driven

00:08:13.320 --> 00:08:14.880
to data-driven.

00:08:14.880 --> 00:08:19.960
And so, certainly, progress
in hardware technologies

00:08:19.960 --> 00:08:24.370
is going to be fundamental
and necessary for scaling up

00:08:24.370 --> 00:08:26.820
both the center and HPC.

00:08:26.820 --> 00:08:30.780
But we also need to scale up
the software technologies.

00:08:30.780 --> 00:08:37.450
In particular, in the form
of-- in the management arena,

00:08:37.450 --> 00:08:41.789
so as to get the role
of-- we redefine the role

00:08:41.789 --> 00:08:47.220
of human operators in
the data center shifting

00:08:47.220 --> 00:08:51.910
from nuts and bolts operations,
to setting up high level goals

00:08:51.910 --> 00:08:53.100
and objectives.

00:08:53.100 --> 00:08:56.000
So the human element
in the data center

00:08:56.000 --> 00:08:59.400
should only be setting
high level goals,

00:08:59.400 --> 00:09:02.000
and let the software
technologies

00:09:02.000 --> 00:09:07.536
that we develop do the routine
maintenance operations.

00:09:07.536 --> 00:09:12.220
And at that point we'll have
large computing centers and HP

00:09:12.220 --> 00:09:13.710
systems.

00:09:13.710 --> 00:09:19.580
Their operations should be based
on predictive computational

00:09:19.580 --> 00:09:21.720
models of things
through data science,

00:09:21.720 --> 00:09:24.550
and that's what I'm going to
be concentrating on today.

00:09:24.550 --> 00:09:27.380
So what I want to
do in this talk

00:09:27.380 --> 00:09:32.840
is give you one data point
towards this objective

00:09:32.840 --> 00:09:36.990
of achieving
shifting from human-

00:09:36.990 --> 00:09:39.920
to data-driven techniques.

00:09:39.920 --> 00:09:48.770
And I will do that by focusing
on the aspect of sustainability

00:09:48.770 --> 00:09:51.560
that has to do with
long-lasting computations.

00:09:51.560 --> 00:09:54.640
That is, computations,
to be sustainable,

00:09:54.640 --> 00:09:58.420
should not be
interrupted by events

00:09:58.420 --> 00:10:02.450
such as errors or failures.

00:10:02.450 --> 00:10:05.120
So the question that I'm
trying to-- the hypothesis

00:10:05.120 --> 00:10:10.320
that I'm trying to test is, can
we use data that is contained

00:10:10.320 --> 00:10:14.780
in routine typical data
logs that are collected

00:10:14.780 --> 00:10:19.260
in typical data centers,
can we use that information

00:10:19.260 --> 00:10:23.370
to build predictive models
to be useful towards

00:10:23.370 --> 00:10:24.700
the sustainability goal?

00:10:24.700 --> 00:10:27.370
And the sustainability
goal for this talk

00:10:27.370 --> 00:10:30.550
is going to failure
detection, failure prediction.

00:10:30.550 --> 00:10:33.360
But in general we can use
the same techniques, also,

00:10:33.360 --> 00:10:35.830
for other aspects
of sustainability

00:10:35.830 --> 00:10:41.730
having to do with energy
consumption, and cooling,

00:10:41.730 --> 00:10:45.480
and other aspects.

00:10:45.480 --> 00:10:48.270
And I've indicated,
through other work

00:10:48.270 --> 00:10:51.130
that we have done
exactly in this arena,

00:10:51.130 --> 00:10:55.530
having to do with building
predictive models for power

00:10:55.530 --> 00:10:59.230
consumption in a HPC setting,
not in a data center setting,

00:10:59.230 --> 00:11:01.100
but an HPC setting.

00:11:01.100 --> 00:11:05.260
One collected from an IBM
Blue Gene/Q installation

00:11:05.260 --> 00:11:07.610
that we have access
to in Bologna.

00:11:07.610 --> 00:11:12.850
And the second one being a
research machine, again built

00:11:12.850 --> 00:11:15.620
in Bologna, called the Aurora.

00:11:15.620 --> 00:11:20.504
That is a hybrid HPC
system that puts together

00:11:20.504 --> 00:11:25.450
CPUs, GPUs, and mix to
achieve very high performance.

00:11:25.450 --> 00:11:31.820
So what I propose is a
form of autonomic computing

00:11:31.820 --> 00:11:35.740
that I call
data-driven autonomics.

00:11:35.740 --> 00:11:38.560
And it's based on the idea
that, just as I showed you

00:11:38.560 --> 00:11:41.950
in the picture, the data center
is not an object in isolation,

00:11:41.950 --> 00:11:45.430
but it's embedded in a
data-rich environment.

00:11:45.430 --> 00:11:48.650
The data-rich environment
can be as large as you want.

00:11:48.650 --> 00:11:51.530
It can include the
cooling power structure.

00:11:51.530 --> 00:11:54.440
But it can also include
the physical environment,

00:11:54.440 --> 00:11:58.580
like the seismic activity,
automobile traffic

00:11:58.580 --> 00:12:00.370
in the arena, it
can even include

00:12:00.370 --> 00:12:04.510
geopolitical considerations
that take place in that arena.

00:12:04.510 --> 00:12:06.790
And earthquakes, why not?

00:12:06.790 --> 00:12:11.320
So it can be as large as you
want by bringing in more data

00:12:11.320 --> 00:12:14.300
sources into the consideration.

00:12:14.300 --> 00:12:17.860
So the question is
then, can these data

00:12:17.860 --> 00:12:20.510
be exploited to devise a
new class of autonomics

00:12:20.510 --> 00:12:24.100
that I call
data-driven autonomics?

00:12:24.100 --> 00:12:28.740
So what are the main salient
points of such an autonomic

00:12:28.740 --> 00:12:30.440
computing system?

00:12:30.440 --> 00:12:32.680
Well, first of
all, as I said, it

00:12:32.680 --> 00:12:36.570
should be holistic, meaning
that it should consider the data

00:12:36.570 --> 00:12:39.820
center not in isolation,
but in the bigger picture.

00:12:39.820 --> 00:12:42.210
And the bigger
picture, as I said,

00:12:42.210 --> 00:12:45.810
can be as big as you want,
depending on what you bring in.

00:12:45.810 --> 00:12:48.120
It should be data-driven.

00:12:48.120 --> 00:12:52.560
It should use modern
data science techniques

00:12:52.560 --> 00:12:55.880
to exploit massive amounts
of data that are collected

00:12:55.880 --> 00:12:57.550
from multiple sources.

00:12:57.550 --> 00:13:07.650
It should be predictive,
as opposed to descriptive.

00:13:07.650 --> 00:13:09.470
It should be
predictive in that it

00:13:09.470 --> 00:13:14.130
should anticipate
unwanted future states so

00:13:14.130 --> 00:13:16.100
that we can avoid them.

00:13:16.100 --> 00:13:18.560
It should be rule-based,
interpret the predictions

00:13:18.560 --> 00:13:22.000
in the context of larger
high-level policies.

00:13:22.000 --> 00:13:24.676
It should be proactive,
rather than reactive.

00:13:24.676 --> 00:13:28.800
That is, it should anticipate
unwanted, undesired states,

00:13:28.800 --> 00:13:30.220
and avoid them in
the first place,

00:13:30.220 --> 00:13:34.090
rather than reacting to them
after they have occurred.

00:13:34.090 --> 00:13:36.360
And the ultimate
goal, as I said,

00:13:36.360 --> 00:13:39.150
is to limit the role
of the human operator.

00:13:39.150 --> 00:13:41.380
So in that sense,
these goals are

00:13:41.380 --> 00:13:47.940
very similar to what we have for
things like self-driving cars.

00:13:47.940 --> 00:13:53.330
I can list, essentially, the
same properties that we list

00:13:53.330 --> 00:13:55.340
for a self-driving vehicle.

00:13:55.340 --> 00:13:58.010
Predictive,
rule-based, proactive.

00:13:58.010 --> 00:14:03.420
So what I want to do is
apply these same ideas

00:14:03.420 --> 00:14:08.510
in the case of sustainable
high-performance computing.

00:14:08.510 --> 00:14:14.090
So what we did was test this
idea on one particular data

00:14:14.090 --> 00:14:16.660
that we have access to, which
happens to be the Google

00:14:16.660 --> 00:14:22.340
dataset that John has
collected and made publicly

00:14:22.340 --> 00:14:24.290
available for researchers.

00:14:24.290 --> 00:14:27.410
So this is just a very
short summary of that data.

00:14:27.410 --> 00:14:30.212
Maybe I'm boring you.

00:14:30.212 --> 00:14:32.890
You probably know some of this.

00:14:32.890 --> 00:14:35.330
But anyway, it's a
dataset collected

00:14:35.330 --> 00:14:41.350
from about 12,000 machines
over a one month period.

00:14:41.350 --> 00:14:44.400
It's roughly 200
gigabytes of raw data.

00:14:44.400 --> 00:14:49.400
And the data contains
elements that

00:14:49.400 --> 00:14:52.340
have to do with task
events, regarding

00:14:52.340 --> 00:14:56.010
the state and the number
of tasks on each machine.

00:14:56.010 --> 00:15:00.430
It includes the task usage logs
having to do with the resource

00:15:00.430 --> 00:15:02.600
consumption of each task.

00:15:02.600 --> 00:15:05.740
And then machine
events that indicate

00:15:05.740 --> 00:15:10.040
when machines are added or
removed from the cluster,

00:15:10.040 --> 00:15:14.580
either due to failures or
due to maintenance events.

00:15:14.580 --> 00:15:20.150
So what we did was use this
data and apply machine learning

00:15:20.150 --> 00:15:22.650
techniques to build
a predictive model.

00:15:22.650 --> 00:15:25.630
And the first thing we did
was to select the features.

00:15:25.630 --> 00:15:30.330
So the first thing we
did, we extracted 12,

00:15:30.330 --> 00:15:34.640
what I call base features,
over five minute windows.

00:15:34.640 --> 00:15:38.680
And the 12 basic features
are the number of running,

00:15:38.680 --> 00:15:41.570
started, finished,
failed, killed, evicted,

00:15:41.570 --> 00:15:44.170
lost tasks on each machine.

00:15:44.170 --> 00:15:45.690
So these are the
different states

00:15:45.690 --> 00:15:48.100
that a task can be
on for each machine.

00:15:48.100 --> 00:15:52.780
And we count the number of tasks
in each one of those states.

00:15:52.780 --> 00:15:56.860
We also add to the feature
set the machine load

00:15:56.860 --> 00:16:02.330
in the form of average CPU load,
the memory load, the cycles

00:16:02.330 --> 00:16:04.780
per instruction--
the CPI-- load,

00:16:04.780 --> 00:16:08.010
the memory accesses per
instruction, and the disk time.

00:16:08.010 --> 00:16:10.710
So these are all
available in the raw data,

00:16:10.710 --> 00:16:15.160
so we extract these on 5
minute intervals per machine,

00:16:15.160 --> 00:16:17.100
and then aggregate them.

00:16:17.100 --> 00:16:20.310
And then we also compute
aggregate features.

00:16:20.310 --> 00:16:22.680
These are averages and
standard deviations

00:16:22.680 --> 00:16:26.500
and coefficient of variations
over longer intervals.

00:16:26.500 --> 00:16:28.400
Not five minutes, but longer.

00:16:28.400 --> 00:16:33.115
Longer being 1, 12, 24,
48, 72, and 96 hours.

00:16:35.810 --> 00:16:38.890
And then, finally, we
select seven features,

00:16:38.890 --> 00:16:42.850
and compute pairwise
correlations between them.

00:16:42.850 --> 00:16:47.920
So this adds 21 more features,
if you do the seven choose

00:16:47.920 --> 00:16:49.250
two, that's 21.

00:16:49.250 --> 00:16:51.030
So this adds 21 more features.

00:16:51.030 --> 00:16:56.760
So in the end, we end up
with 416 total features,

00:16:56.760 --> 00:17:01.960
accounting to a very modest
amount of data, 104 megabytes.

00:17:01.960 --> 00:17:05.301
So the size of the data
is not an issue at all.

00:17:05.301 --> 00:17:05.801
[CLICK]

00:17:06.243 --> 00:17:06.743
[CLICK]

00:17:07.690 --> 00:17:09.075
So this is a-- sorry.

00:17:09.075 --> 00:17:09.574
[CLICK]

00:17:10.500 --> 00:17:14.099
This is a picture
of a time series

00:17:14.099 --> 00:17:16.859
of some of these
features for one

00:17:16.859 --> 00:17:22.329
particular node in that 12,000
cluster that we have access to.

00:17:22.329 --> 00:17:24.680
So this is time
running horizontally.

00:17:24.680 --> 00:17:26.859
And the features
that I'm illustrating

00:17:26.859 --> 00:17:28.740
with the various
colors happen to be

00:17:28.740 --> 00:17:32.530
the CPU load at the
5 minute interval,

00:17:32.530 --> 00:17:38.310
the CPU load, but now at
the 12 hour average, the CPU

00:17:38.310 --> 00:17:41.420
load at the 12 hour
coefficient of variation,

00:17:41.420 --> 00:17:45.590
and then the pairwise CPU
load running job 12 hour

00:17:45.590 --> 00:17:47.460
correlation.

00:17:47.460 --> 00:17:50.980
The actual values are
not terribly important.

00:17:50.980 --> 00:17:53.990
I'm just showing you what
a typical time series

00:17:53.990 --> 00:17:54.930
data looks like.

00:17:54.930 --> 00:17:57.790
But what I'm also
showing is that big gap

00:17:57.790 --> 00:17:59.770
in the middle, that white space.

00:17:59.770 --> 00:18:03.450
For this particular
node that I'm examining,

00:18:03.450 --> 00:18:09.770
that node happens to have been
removed at around 250 hours,

00:18:09.770 --> 00:18:17.310
and then added back into the
cluster at 350 hours, roughly.

00:18:17.310 --> 00:18:21.200
So in that interval
the node was absent,

00:18:21.200 --> 00:18:23.830
either because it
failed, or it was

00:18:23.830 --> 00:18:28.900
undergoing some sort of
update maintenance operation.

00:18:28.900 --> 00:18:34.360
So then we analyze
these features

00:18:34.360 --> 00:18:37.150
using one of your
tools, BigQuery,

00:18:37.150 --> 00:18:40.470
that we have access to.

00:18:40.470 --> 00:18:46.230
So we did preprocessing of
the features using BigQuery.

00:18:46.230 --> 00:18:49.810
These are some features of
BigQuery that I'll skip,

00:18:49.810 --> 00:18:53.520
because I'm sure
you know about them.

00:18:53.520 --> 00:19:00.330
I'll only highlight the fact
that the whole preprocessing

00:19:00.330 --> 00:19:04.590
operation on BigQuery
costs less than $2,000

00:19:04.590 --> 00:19:07.260
using real Google pricing.

00:19:07.260 --> 00:19:12.460
So this is something quite--
it's not an unreasonable

00:19:12.460 --> 00:19:14.220
amount of expense.

00:19:14.220 --> 00:19:15.810
AUDIENCE: Highway robbery.

00:19:15.810 --> 00:19:16.310
OZALP BABAOGLU: Excuse me?

00:19:16.310 --> 00:19:16.775
[LAUGHTER]

00:19:16.775 --> 00:19:17.705
AUDIENCE: I said
highway robbery.

00:19:17.705 --> 00:19:19.100
OZALP BABAOGLU: Highway robbery.

00:19:19.100 --> 00:19:24.430
And this is a more detailed
breakdown of the BigQuery

00:19:24.430 --> 00:19:28.950
preprocessing in terms
of-- the first column

00:19:28.950 --> 00:19:31.890
are the aggregation interval,
ranging from one hour

00:19:31.890 --> 00:19:33.710
up to 96 hours.

00:19:33.710 --> 00:19:37.740
And then the second column
is the resulting data size,

00:19:37.740 --> 00:19:41.200
ranging from a
10th of a terabyte

00:19:41.200 --> 00:19:44.340
all the way up to
12.5 terabytes.

00:19:44.340 --> 00:19:48.540
And then the last two
columns are the actual times

00:19:48.540 --> 00:19:52.500
required to compute the
average standard deviation

00:19:52.500 --> 00:19:55.410
coefficient of variation,
which is the third column.

00:19:55.410 --> 00:19:59.240
And the last column, which
is the most time-consuming

00:19:59.240 --> 00:20:02.520
computation, having to do
with the pairwise coefficient

00:20:02.520 --> 00:20:04.710
of variation computation.

00:20:04.710 --> 00:20:07.740
This is interesting because
it's-- at least as far as I

00:20:07.740 --> 00:20:10.310
know, it's one of the
first quantitative results

00:20:10.310 --> 00:20:13.610
of BigQuery performance
on real data.

00:20:13.610 --> 00:20:16.720
In itself, it's not really
relevant to what I'm doing,

00:20:16.720 --> 00:20:19.080
but it's just
giving us a feeling

00:20:19.080 --> 00:20:22.400
for the performance of BigQuery,
which we were very, very

00:20:22.400 --> 00:20:23.610
happy to use.

00:20:23.610 --> 00:20:27.500
I mean, it made
our work feasible.

00:20:27.500 --> 00:20:31.370
Otherwise, if we were to do
this on local infrastructure,

00:20:31.370 --> 00:20:34.450
we would not have
been able to do it.

00:20:34.450 --> 00:20:38.470
So now, continuing on to
what I want to get at,

00:20:38.470 --> 00:20:40.500
which is a predictive
model for failures.

00:20:40.500 --> 00:20:42.530
The first thing we
need to recognize

00:20:42.530 --> 00:20:45.400
is the fact that the
Google trace does not

00:20:45.400 --> 00:20:47.420
have failure events.

00:20:47.420 --> 00:20:51.360
There is nothing in the Google
trace that's labeled a failure.

00:20:51.360 --> 00:20:54.780
There are machine ads
and machine removes--

00:20:54.780 --> 00:20:57.150
node adds and node removes.

00:20:57.150 --> 00:21:04.020
So failures can only be inferred
indirectly, and approximately,

00:21:04.020 --> 00:21:08.270
because obviously we
don't have certainty.

00:21:08.270 --> 00:21:15.400
And here we consulted John
on how we might do this.

00:21:15.400 --> 00:21:18.140
So we came up with the
following heuristic,

00:21:18.140 --> 00:21:24.000
which was, if a machine
remove event is not succeeded

00:21:24.000 --> 00:21:28.380
within the next two hours by
the same node being added,

00:21:28.380 --> 00:21:30.480
we deem it to be a failure.

00:21:30.480 --> 00:21:33.610
So the reasoning being, if
it were a routine maintenance

00:21:33.610 --> 00:21:35.310
or an update event,
the node would

00:21:35.310 --> 00:21:38.790
be removed from the
cluster, but then would

00:21:38.790 --> 00:21:41.830
reappear in the cluster after
some reasonable amount of time.

00:21:41.830 --> 00:21:46.140
And we chose two hours to be
this reasonable amount of time.

00:21:46.140 --> 00:21:47.890
Now you can question
that, obviously.

00:21:47.890 --> 00:21:51.450
And I don't have
any specific reason

00:21:51.450 --> 00:21:54.689
why that should be the--
but again, as I said,

00:21:54.689 --> 00:21:57.230
this was not something that we
made up completely on our own.

00:21:57.230 --> 00:22:00.010
We consulted our
colleagues here.

00:22:00.010 --> 00:22:02.590
So take that as it may.

00:22:02.590 --> 00:22:07.490
So less than two
hours is considered

00:22:07.490 --> 00:22:09.380
a routine maintenance event.

00:22:09.380 --> 00:22:11.190
If it's more than
two hours we assume

00:22:11.190 --> 00:22:14.310
that the machine failed, and
then was somehow repaired

00:22:14.310 --> 00:22:16.100
and put back into the cluster.

00:22:16.100 --> 00:22:19.290
So this is going to be
my definition of what

00:22:19.290 --> 00:22:21.150
a failure is.

00:22:21.150 --> 00:22:23.570
So at that point we
classify each data

00:22:23.570 --> 00:22:31.600
point as either a fail, which
I call the positive class,

00:22:31.600 --> 00:22:34.030
and safe, which is
the negative class.

00:22:34.030 --> 00:22:38.020
So in terms of the
classifier language,

00:22:38.020 --> 00:22:42.160
the positive class
corresponds to the failures

00:22:42.160 --> 00:22:43.940
and the negative
class corresponds

00:22:43.940 --> 00:22:46.120
to safe or nonfailure.

00:22:46.120 --> 00:22:50.560
So a fail, a positive
event, is considered

00:22:50.560 --> 00:22:56.960
to be some-- time to failure
is less than 24 hours.

00:22:56.960 --> 00:22:59.550
So we chose a window of a day.

00:22:59.550 --> 00:23:05.900
So if the machine happens to
be labeled in the failure state

00:23:05.900 --> 00:23:10.800
within a day, we call that data
point to be in the fail class.

00:23:10.800 --> 00:23:13.440
And if it's greater
than 24 hours,

00:23:13.440 --> 00:23:16.980
it's considered to
be in the safe class.

00:23:16.980 --> 00:23:18.030
Is that clear?

00:23:18.030 --> 00:23:19.910
OK.

00:23:19.910 --> 00:23:22.590
And the resulting
number of data points

00:23:22.590 --> 00:23:25.840
were given in the
previous slide.

00:23:25.840 --> 00:23:29.420
So we end up with some 100--
I should go back to that,

00:23:29.420 --> 00:23:31.450
because that's important.

00:23:31.450 --> 00:23:32.540
Sorry.

00:23:32.540 --> 00:23:38.030
So doing this, we end up
with 108,000 positive events,

00:23:38.030 --> 00:23:39.670
meaning failure events.

00:23:39.670 --> 00:23:43.230
And we end up with half
a million safe events,

00:23:43.230 --> 00:23:47.420
but that's only after a half
a percent random sampling.

00:23:47.420 --> 00:23:49.450
Because if we don't do
the random sampling,

00:23:49.450 --> 00:23:52.360
the number of safe
states is overwhelming,

00:23:52.360 --> 00:23:54.880
because most of the time
failures don't occur.

00:23:54.880 --> 00:23:59.350
So you have machines there
that typically don't fail.

00:23:59.350 --> 00:24:03.070
So if we were to keep
all of the safe states,

00:24:03.070 --> 00:24:07.660
then the two classes would
be extremely unbalanced,

00:24:07.660 --> 00:24:11.520
and most of the machine
learning techniques that we use

00:24:11.520 --> 00:24:15.270
do not perform well
when you do a classifier

00:24:15.270 --> 00:24:21.490
with such different
ratios of the two states.

00:24:21.490 --> 00:24:23.850
So we do a half a
percent subsampling,

00:24:23.850 --> 00:24:28.150
meaning that we throw away
95% of the safe states,

00:24:28.150 --> 00:24:32.310
and we pick a random
half a percent sample.

00:24:32.310 --> 00:24:33.300
Sorry, not 95.

00:24:33.300 --> 00:24:37.220
99.5% are thrown out,
and the half a percent

00:24:37.220 --> 00:24:40.530
are randomly sampled and
kept as the safe state.

00:24:40.530 --> 00:24:44.570
So the classifier itself is
a Random Forest classifier.

00:24:44.570 --> 00:24:46.870
This is a well-known
machine learning technique

00:24:46.870 --> 00:24:48.120
based on decision trees.

00:24:48.120 --> 00:24:52.610
I'll go very fast here,
since I assume most of you

00:24:52.610 --> 00:24:54.470
are familiar with
these techniques.

00:24:54.470 --> 00:24:58.730
So we generate a large set
of Random Forest classifiers,

00:24:58.730 --> 00:25:03.581
and we train them, and then
we compute their precision

00:25:03.581 --> 00:25:07.200
of each one, based
on a dataset which

00:25:07.200 --> 00:25:11.480
is different from the dataset
we used to do the evaluation.

00:25:11.480 --> 00:25:16.940
So the test dataset is
divided into halves.

00:25:16.940 --> 00:25:21.170
The individual dataset, which is
used to compute the precision,

00:25:21.170 --> 00:25:29.540
and the ensemble dataset, which
is used to build the ensemble

00:25:29.540 --> 00:25:32.410
classifier through precision
weighted voting, which I

00:25:32.410 --> 00:25:36.010
will explain in the next slide.

00:25:36.010 --> 00:25:42.500
So we end up with 420 individual
Random Forest classifiers,

00:25:42.500 --> 00:25:44.570
and we built them as follows.

00:25:44.570 --> 00:25:47.650
We varied the number of decision
trees between two and 15,

00:25:47.650 --> 00:25:49.170
so there are 14 of them.

00:25:49.170 --> 00:25:54.980
We varied the ratio between
safe and fail data events

00:25:54.980 --> 00:25:58.210
between a quarter and four.

00:25:58.210 --> 00:26:02.810
So this determines the number
of subsampling percentage

00:26:02.810 --> 00:26:05.260
that I talked about
just a few minutes ago.

00:26:05.260 --> 00:26:07.640
So we either throw
out more or less

00:26:07.640 --> 00:26:11.290
of the safe data
between those intervals.

00:26:11.290 --> 00:26:13.620
And then we repeat the
whole thing five times.

00:26:13.620 --> 00:26:16.700
So when you do all of the
math you end up with 420.

00:26:16.700 --> 00:26:19.520
So we end up with
420 RF classifiers,

00:26:19.520 --> 00:26:24.540
and we train each of them
on 10 consecutive days,

00:26:24.540 --> 00:26:28.890
and then we test on
the next disjoint day.

00:26:28.890 --> 00:26:33.550
So remember the Google data
trace has 29 days of data.

00:26:33.550 --> 00:26:38.770
So if you look at the picture in
the bottom-- so for benchmark 1

00:26:38.770 --> 00:26:42.930
we skip the first two
days, because we want

00:26:42.930 --> 00:26:47.170
the aggregate to start working.

00:26:47.170 --> 00:26:49.999
Because if we do aggregation
over the previous days,

00:26:49.999 --> 00:26:52.040
if we start from day zero,
we don't have anything

00:26:52.040 --> 00:26:52.690
to aggregate.

00:26:52.690 --> 00:26:54.420
So we start in day three.

00:26:54.420 --> 00:26:58.030
So the blue days are
the training data.

00:26:58.030 --> 00:26:59.450
Then we skip a day.

00:26:59.450 --> 00:27:02.760
And then on the 11th day,
that's our test date.

00:27:02.760 --> 00:27:07.490
And we do this by shifting this
picture one day to the right,

00:27:07.490 --> 00:27:12.600
and we end up with 15 benchmarks
that cover the entire 29

00:27:12.600 --> 00:27:13.810
days of the trace.

00:27:13.810 --> 00:27:18.140
So these are the-- we have 15
benchmarks to do the testing.

00:27:18.140 --> 00:27:20.620
So going back to the
ensemble classifier,

00:27:20.620 --> 00:27:25.520
we had 420 classifiers,
and we merge all of them

00:27:25.520 --> 00:27:27.150
into an ensemble classifier.

00:27:27.150 --> 00:27:28.890
So we have ensembles
of ensembles.

00:27:28.890 --> 00:27:30.540
Because remember,
Random Forest itself

00:27:30.540 --> 00:27:34.330
is an ensemble, because it's
based on individual trees which

00:27:34.330 --> 00:27:35.650
are built into a forest.

00:27:35.650 --> 00:27:41.160
So we have a forest of
forests, 420 of them.

00:27:41.160 --> 00:27:46.620
And we combined them
by doing a weighted sum

00:27:46.620 --> 00:27:48.610
based on the precision.

00:27:48.610 --> 00:27:54.010
So we take-- if you take the
Ith random forest classifier

00:27:54.010 --> 00:28:01.300
and take its output, which
is shown as sigma IJ, which

00:28:01.300 --> 00:28:04.050
is a binary value
indicating it's either

00:28:04.050 --> 00:28:06.415
in the fail or the safe state.

00:28:09.980 --> 00:28:13.760
So for the data point
J. And we compute

00:28:13.760 --> 00:28:18.890
the precision of
the Ith classifier,

00:28:18.890 --> 00:28:21.790
that's indicated as PI.

00:28:21.790 --> 00:28:27.460
And then the weighted sum of
the output with the precision

00:28:27.460 --> 00:28:32.950
summed over all of the 420
classifiers is labeled SJ.

00:28:32.950 --> 00:28:35.920
So the SJ value is no
longer a binary value.

00:28:35.920 --> 00:28:40.320
It's a real value,
because we scale it

00:28:40.320 --> 00:28:45.340
by the appropriate value so we
get a number between 0 and 1,

00:28:45.340 --> 00:28:47.810
so that we can interpret
it as a probability.

00:28:47.810 --> 00:28:51.790
So the weighted
value now can be seen

00:28:51.790 --> 00:28:53.127
as a likelihood of failure.

00:28:53.127 --> 00:28:54.460
So it's not a binary classifier.

00:28:54.460 --> 00:28:57.220
It's not one zero, but
it's a real number.

00:28:57.220 --> 00:29:00.250
So closer to one is
more likely to fail.

00:29:00.250 --> 00:29:02.880
Closer to 0 is less
likely to fail.

00:29:02.880 --> 00:29:06.140
So this is the
ensemble classifier.

00:29:06.140 --> 00:29:08.760
So now the prediction-- yeah.

00:29:08.760 --> 00:29:12.020
So the weighting is
done only on the outputs

00:29:12.020 --> 00:29:16.455
of the individual classifiers
as I have defined the states.

00:29:16.455 --> 00:29:19.270
So you're right that
some of the outputs

00:29:19.270 --> 00:29:22.800
may be missing, because
we simply did not

00:29:22.800 --> 00:29:28.600
consider-- those are excluded.

00:29:28.600 --> 00:29:31.640
I don't know what else to say.

00:29:31.640 --> 00:29:33.590
They're not included
in the ensemble.

00:29:33.590 --> 00:29:39.020
And obviously this will have
a consequence on the-- yeah.

00:29:39.020 --> 00:29:41.800
But we'll see the
evaluation results

00:29:41.800 --> 00:29:45.290
and you can judge whether
it's good enough or not.

00:29:45.290 --> 00:29:50.490
So now the prediction
of failure now becomes

00:29:50.490 --> 00:29:51.810
a classification problem.

00:29:54.852 --> 00:29:59.770
And by applying
the model you just

00:29:59.770 --> 00:30:03.900
evaluate the ensemble classifier
for a particular data point.

00:30:03.900 --> 00:30:05.570
And then you
interpret the result

00:30:05.570 --> 00:30:08.160
as the likelihood of a failure.

00:30:08.160 --> 00:30:09.640
OK?

00:30:09.640 --> 00:30:12.610
Then we can now convert this
back to a binary classifier

00:30:12.610 --> 00:30:14.280
by setting a threshold.

00:30:14.280 --> 00:30:17.420
So you can say, if the
likelihood is more than 80%,

00:30:17.420 --> 00:30:20.800
I'll consider the
output to be a failure.

00:30:20.800 --> 00:30:25.970
So the training was done
on an absolutely ordinary

00:30:25.970 --> 00:30:28.660
routine desktop computer.

00:30:28.660 --> 00:30:32.940
A very moderate
performance iMac.

00:30:32.940 --> 00:30:36.750
And this was just to show
you that the training is

00:30:36.750 --> 00:30:38.420
a very lightweight operation.

00:30:38.420 --> 00:30:42.560
It took between seven and
nine hours on a single desktop

00:30:42.560 --> 00:30:43.150
computer.

00:30:43.150 --> 00:30:46.850
So we don't need any sort
of high-power computing

00:30:46.850 --> 00:30:47.870
to do the training.

00:30:47.870 --> 00:30:49.140
So these are the results.

00:30:49.140 --> 00:30:51.940
So the results for
a binary classifier

00:30:51.940 --> 00:30:55.920
is typical to display
these ROC curves that I'm

00:30:55.920 --> 00:30:57.270
sure you're familiar with.

00:30:57.270 --> 00:31:00.030
So here I'm plotting
the true positive rate

00:31:00.030 --> 00:31:02.010
in the vertical
axis as a function

00:31:02.010 --> 00:31:04.300
of the false positive rate.

00:31:04.300 --> 00:31:08.500
So the true positive
rate is the number

00:31:08.500 --> 00:31:12.530
of positives that are actually
classified as positive.

00:31:12.530 --> 00:31:16.090
And the horizontal
scale is the number

00:31:16.090 --> 00:31:20.570
of negatives that are
identified as positive.

00:31:20.570 --> 00:31:22.570
So these are the false
alarms, if you will.

00:31:22.570 --> 00:31:26.660
So ideally we want to be in
the upper left hand corner,

00:31:26.660 --> 00:31:28.990
because you want the
true positive to be one,

00:31:28.990 --> 00:31:31.770
and you want the false
positive to be zero.

00:31:31.770 --> 00:31:35.250
But obviously life
is more complicated,

00:31:35.250 --> 00:31:36.970
so we get these curves.

00:31:36.970 --> 00:31:40.340
So the solid line is
the ensemble classifier.

00:31:40.340 --> 00:31:43.590
And I'm showing you this for
two different benchmarks,

00:31:43.590 --> 00:31:45.840
the left one being
the best case.

00:31:45.840 --> 00:31:48.030
Remember, we had 15 benchmarks?

00:31:48.030 --> 00:31:52.340
So this is the 14th
among those 15.

00:31:52.340 --> 00:31:56.400
And the worst case happens
to be benchmark number four,

00:31:56.400 --> 00:31:57.500
on the right.

00:31:57.500 --> 00:31:59.150
So let's look at the best case.

00:31:59.150 --> 00:32:01.920
So the solid line is the
ensemble classifier, it

00:32:01.920 --> 00:32:08.140
and the individual colored
dots are the individual pieces

00:32:08.140 --> 00:32:09.040
of the ensemble.

00:32:09.040 --> 00:32:12.780
And the colors correspond
to different ratios

00:32:12.780 --> 00:32:15.810
among the false fail
and safe events.

00:32:15.810 --> 00:32:18.925
Remember I defined
this-- I told you

00:32:18.925 --> 00:32:21.560
we were doing the subsampling
and we were varying it

00:32:21.560 --> 00:32:24.110
between a quarter and four?

00:32:24.110 --> 00:32:26.190
So these are the
different colors.

00:32:26.190 --> 00:32:29.270
The dark blue is the 0.25.

00:32:29.270 --> 00:32:32.990
And the whatever that
color-- that is yellow--

00:32:32.990 --> 00:32:36.730
is the case when the ratio is 4.

00:32:36.730 --> 00:32:39.435
4 to 1 as opposed
to a quarter to one.

00:32:39.435 --> 00:32:44.760
But anyway, the take away is
that the ensemble typically

00:32:44.760 --> 00:32:47.690
performs better than the
individual classifiers, which

00:32:47.690 --> 00:32:48.850
is good.

00:32:48.850 --> 00:32:53.310
So it means that by putting
individual classifiers together

00:32:53.310 --> 00:32:56.460
under a single ensemble we do
better than any one of them

00:32:56.460 --> 00:32:59.500
individually, because
rarely any of the dots

00:32:59.500 --> 00:33:01.410
are above the solid line.

00:33:01.410 --> 00:33:04.680
And the other encouraging
point is the curve.

00:33:04.680 --> 00:33:09.300
Even though it's not ideal,
it's close to our desire,

00:33:09.300 --> 00:33:13.778
which is being in the
upper right hand corner.

00:33:13.778 --> 00:33:18.800
And it's typical to pick the
false positive rate to be 5%.

00:33:18.800 --> 00:33:26.480
And if you do that-- so that's
the yellow vertical line,

00:33:26.480 --> 00:33:30.520
this point here-- you end up
with the true positive rate

00:33:30.520 --> 00:33:36.220
in the best case to be 88% and
in the worst case to be 27%.

00:33:39.040 --> 00:33:47.520
Now the other evaluation
of a predictive model

00:33:47.520 --> 00:33:50.240
is the precision recall curves.

00:33:50.240 --> 00:33:53.640
So these are plotting
the precision,

00:33:53.640 --> 00:33:56.500
meaning what is the probability
that an event marked

00:33:56.500 --> 00:34:01.900
as a failure is really a failure
as a function of the recall,

00:34:01.900 --> 00:34:04.160
which is the true positive rate.

00:34:04.160 --> 00:34:06.270
And in this case we
would like the curve

00:34:06.270 --> 00:34:09.710
to be essentially
a horizontal line,

00:34:09.710 --> 00:34:11.810
and then dropping off to zero.

00:34:11.810 --> 00:34:15.070
And here were we do less well.

00:34:15.070 --> 00:34:18.030
The best case looks
pretty reasonable,

00:34:18.030 --> 00:34:23.219
but the worst case is
significantly below the ideal.

00:34:23.219 --> 00:34:25.550
And in this case we
also see that some

00:34:25.550 --> 00:34:28.350
of the individual
members of the ensemble

00:34:28.350 --> 00:34:31.090
do better than the ensemble,
at least for the worse case.

00:34:31.090 --> 00:34:35.750
But again, if we pick the
5% false positive rate

00:34:35.750 --> 00:34:42.320
as a threshold, we end up with
the precision recall values

00:34:42.320 --> 00:34:45.889
that range from 72% and 50%.

00:34:45.889 --> 00:34:50.210
So just summing up, if we
limit the false alarms to 5%,

00:34:50.210 --> 00:34:57.340
in the best case we can identify
almost 90% of the failures.

00:34:57.340 --> 00:35:02.110
And 72% of those failures that
are identified as failures

00:35:02.110 --> 00:35:03.810
are indeed failures.

00:35:03.810 --> 00:35:08.620
And in the worst case we are
able to identify only 30%

00:35:08.620 --> 00:35:10.350
of the events as failures.

00:35:10.350 --> 00:35:14.950
And in that case, about half
of them are actually failures.

00:35:14.950 --> 00:35:18.930
So this sums up the evaluation.

00:35:18.930 --> 00:35:23.025
You can also summarize them as
these areas on the ROC and PR

00:35:23.025 --> 00:35:24.650
curves, but I'll skip that.

00:35:24.650 --> 00:35:25.150
[CLICK]

00:35:27.890 --> 00:35:33.980
So next you may ask, well,
prediction is not perfect.

00:35:33.980 --> 00:35:37.067
Sometimes we misclassify
events, so how bad

00:35:37.067 --> 00:35:38.192
are the misclassifications?

00:35:41.090 --> 00:35:42.490
So how wrong are they?

00:35:42.490 --> 00:35:47.940
So the classes for the fail
class, the positive class--

00:35:47.940 --> 00:35:51.090
remember, we identify
the fail class

00:35:51.090 --> 00:35:58.130
as being a failure occurring
within the next 24 hours.

00:35:58.130 --> 00:36:02.340
So an event would be
included in the fail class

00:36:02.340 --> 00:36:08.690
if the failure occurred even
one minute in the future,

00:36:08.690 --> 00:36:11.700
or 23 hours into the future,
because both of those

00:36:11.700 --> 00:36:13.510
are within the next day.

00:36:13.510 --> 00:36:17.760
But intuitively, obviously
those two scenarios

00:36:17.760 --> 00:36:19.780
would be very different.

00:36:19.780 --> 00:36:21.860
If you have a failure
coming up in a minute,

00:36:21.860 --> 00:36:25.960
as opposed to 23 hours, perhaps
you would do different things.

00:36:25.960 --> 00:36:28.760
So classifying both
of them as failure

00:36:28.760 --> 00:36:33.660
should not be--
technically it is correct,

00:36:33.660 --> 00:36:38.970
but qualitatively they
should be seen differently.

00:36:38.970 --> 00:36:41.530
And likewise, in
the negative case,

00:36:41.530 --> 00:36:44.390
the failure is outside
of the 24 hour window.

00:36:44.390 --> 00:36:47.360
If it occurs at 24
hours and 1 minute,

00:36:47.360 --> 00:36:49.300
as opposed to two weeks.

00:36:49.300 --> 00:36:52.220
Again, technically both
of those are safe states,

00:36:52.220 --> 00:36:55.870
but again, intuitively
you perhaps might

00:36:55.870 --> 00:36:59.190
do different things
in those two cases.

00:36:59.190 --> 00:37:03.290
How do the classifiers
perform in practice?

00:37:03.290 --> 00:37:06.190
I'm showing you this
for the benchmark four.

00:37:06.190 --> 00:37:11.020
So these are box
plots for the time,

00:37:11.020 --> 00:37:13.700
hours to next remove event.

00:37:13.700 --> 00:37:18.900
Hours to next remove event
as a function of-- I'm

00:37:18.900 --> 00:37:22.130
showing you the true positive
and the false negative.

00:37:22.130 --> 00:37:26.420
So on the left
picture, these are

00:37:26.420 --> 00:37:28.940
the case when the event
is marked as a failure.

00:37:28.940 --> 00:37:30.705
Either because it's
a true positive,

00:37:30.705 --> 00:37:34.920
a failure, or it's incorrectly
marked as a failure

00:37:34.920 --> 00:37:36.950
even though it was a safe event.

00:37:36.950 --> 00:37:42.974
So the thing to note is that
the case when the true positive

00:37:42.974 --> 00:37:50.450
median-- the red line in
the bow of the bow tie

00:37:50.450 --> 00:37:52.090
is the median value.

00:37:52.090 --> 00:37:54.290
In the case of TP
the true positive

00:37:54.290 --> 00:37:56.670
is lower than the
false negative.

00:37:56.670 --> 00:38:00.250
So what that means is that in
the case of a failure that's

00:38:00.250 --> 00:38:03.040
actually marked as a
failure, we mark it

00:38:03.040 --> 00:38:05.710
as a failure closer to
the failure than if it

00:38:05.710 --> 00:38:07.990
were a misclassification.

00:38:07.990 --> 00:38:10.500
Now why is that good?

00:38:10.500 --> 00:38:14.310
Well, if the event is
misclassified as a failure

00:38:14.310 --> 00:38:17.970
later, that means
that you have more

00:38:17.970 --> 00:38:19.590
time to-- because
remember, we're

00:38:19.590 --> 00:38:21.950
doing this at 5
minute intervals.

00:38:21.950 --> 00:38:27.390
So we have more opportunity
to mark it correctly

00:38:27.390 --> 00:38:29.120
in the future.

00:38:29.120 --> 00:38:33.150
So we mark it as a failure
if it's further away,

00:38:33.150 --> 00:38:36.080
because that gives us more
time to classify it correctly

00:38:36.080 --> 00:38:36.760
in the future.

00:38:36.760 --> 00:38:39.810
If it were misclassified
and it was imminent,

00:38:39.810 --> 00:38:41.560
then the classification
would be wrong

00:38:41.560 --> 00:38:43.980
and that's all there is to it.

00:38:43.980 --> 00:38:47.210
So misclassification,
if it's further away,

00:38:47.210 --> 00:38:50.350
is deemed to be better,
because as I said,

00:38:50.350 --> 00:38:52.940
it gives us more chance
to classify it correctly

00:38:52.940 --> 00:38:54.590
in the future.

00:38:54.590 --> 00:38:57.460
And I'll skip the right curve.

00:38:57.460 --> 00:39:01.050
So putting all this
together, how would you

00:39:01.050 --> 00:39:04.050
use such a predictive model?

00:39:04.050 --> 00:39:09.200
Well, you would perhaps use
it to do some intelligent form

00:39:09.200 --> 00:39:11.570
of scheduling.

00:39:11.570 --> 00:39:20.260
So one idea would be to define
this notion of a quarantine.

00:39:20.260 --> 00:39:23.870
So a node that it is
classified as a failure in two

00:39:23.870 --> 00:39:28.630
consecutive evaluations, we
put it into a quarantine state.

00:39:28.630 --> 00:39:32.180
And if a machine is in
the quarantine state

00:39:32.180 --> 00:39:37.910
the scheduler does not
dispatch any tasks to it.

00:39:37.910 --> 00:39:40.990
Tasks that would be
dispatched to that node

00:39:40.990 --> 00:39:44.820
are diverted to other nodes.

00:39:44.820 --> 00:39:49.840
Now, when you do that,
two things could happen.

00:39:49.840 --> 00:39:57.600
You could redirect a
task to a node that

00:39:57.600 --> 00:40:00.290
is in the quarantine
state, but the failure

00:40:00.290 --> 00:40:04.970
was sufficiently ahead, such
that the task would have

00:40:04.970 --> 00:40:06.350
finished before the failure.

00:40:06.350 --> 00:40:07.760
So that's bad.

00:40:07.760 --> 00:40:15.150
It means that you redirected
a task without reason.

00:40:15.150 --> 00:40:16.430
You could have avoided that.

00:40:16.430 --> 00:40:20.690
Or be you could have
redirected the task

00:40:20.690 --> 00:40:23.930
and it would have
been interrupted.

00:40:23.930 --> 00:40:27.100
That is, while it was running
you would have had the failure.

00:40:27.100 --> 00:40:29.290
And you avoided
that interruption,

00:40:29.290 --> 00:40:30.580
so that's the good case.

00:40:30.580 --> 00:40:33.040
So we have these two outputs.

00:40:33.040 --> 00:40:37.150
The redirected
without reason is bad.

00:40:37.150 --> 00:40:41.030
And recovered is what
I call the second case,

00:40:41.030 --> 00:40:43.910
meaning that you
avoided dispatching

00:40:43.910 --> 00:40:46.090
the job to a node which
was in the quarantine.

00:40:46.090 --> 00:40:48.210
And that's the good case.

00:40:48.210 --> 00:40:52.920
And you obviously want
to-- your objective

00:40:52.920 --> 00:40:58.280
is to minimize the redirected
and maximize the recovered.

00:40:58.280 --> 00:41:00.790
Obviously, if you
redirect everybody,

00:41:00.790 --> 00:41:02.724
nobody will be interrupted
and the recovered

00:41:02.724 --> 00:41:03.890
will be as high as possible.

00:41:03.890 --> 00:41:08.050
But you would have-- that would
be like taking that node out

00:41:08.050 --> 00:41:10.780
of the picture altogether,
so that's obviously

00:41:10.780 --> 00:41:12.150
something you want to avoid.

00:41:12.150 --> 00:41:15.120
So you want to keep the
node in the cluster,

00:41:15.120 --> 00:41:19.330
but you just want to
avoid sending tasks to it

00:41:19.330 --> 00:41:22.160
that you know will
be interrupted.

00:41:22.160 --> 00:41:25.210
So we evaluated this idea.

00:41:25.210 --> 00:41:28.890
But to do the evaluation
we needed a base case.

00:41:28.890 --> 00:41:35.310
And the base case was defined
to be the result of these two

00:41:35.310 --> 00:41:38.170
events, recovered
and redirected,

00:41:38.170 --> 00:41:41.890
if we had a perfect predictor.

00:41:41.890 --> 00:41:46.690
Meaning that, suppose we could
predict failures perfectly

00:41:46.690 --> 00:41:51.820
within the next 24 hours,
that is 24 hours in advance.

00:41:51.820 --> 00:41:57.830
So if a failure can be predicted
perfectly a day in advance,

00:41:57.830 --> 00:42:02.920
that case is what I'm taking
as my baseline, the best case.

00:42:02.920 --> 00:42:07.470
So doing that, we end up
with pictures like this.

00:42:07.470 --> 00:42:11.260
So here I'm plotting
tasks that are recovered

00:42:11.260 --> 00:42:15.020
in the top left picture.

00:42:15.020 --> 00:42:19.320
So the solid line at the
top is the base case,

00:42:19.320 --> 00:42:23.010
and the three dotted
curves correspond

00:42:23.010 --> 00:42:27.490
to our real predictive
model for varying amounts

00:42:27.490 --> 00:42:29.880
of false positive rates.

00:42:29.880 --> 00:42:36.970
So we considered false positive
rates of 5%, 1%, and 2%.

00:42:36.970 --> 00:42:42.430
So if you, for
example, as a function

00:42:42.430 --> 00:42:50.424
of the quarantine window
size, if you take 250

00:42:50.424 --> 00:42:54.300
as a concrete value, because
we take that because here

00:42:54.300 --> 00:42:57.650
we match the
baseline in the case

00:42:57.650 --> 00:43:02.010
of false positive rate of 2%.

00:43:02.010 --> 00:43:06.960
And the resulting
task recovered for 250

00:43:06.960 --> 00:43:11.110
happens to be about
half of the baseline.

00:43:11.110 --> 00:43:19.050
So if you limit the false
positive rates to 2%

00:43:19.050 --> 00:43:23.370
and your number of
redirected tasks

00:43:23.370 --> 00:43:28.340
are as good as in the baseline,
then you can recover about half

00:43:28.340 --> 00:43:33.170
of the tasks that you would
if you had a perfect 24 hour

00:43:33.170 --> 00:43:35.295
predictor.

00:43:35.295 --> 00:43:39.530
And the right hand curves
are the same two situations,

00:43:39.530 --> 00:43:44.260
but in terms of CPU hours
that are redirected and saved,

00:43:44.260 --> 00:43:47.070
as opposed to the
number of tasks.

00:43:47.070 --> 00:43:47.930
OK?

00:43:47.930 --> 00:43:50.240
Now there's some technical
details here which

00:43:50.240 --> 00:43:52.264
I've skipped, but they're
included in the paper

00:43:52.264 --> 00:43:53.680
there if you want
to look at them.

00:43:53.680 --> 00:43:57.780
Because the Google trace
also has different classes

00:43:57.780 --> 00:44:01.980
of the tasks depending
on their length.

00:44:01.980 --> 00:44:03.820
So we're only doing
this for the class

00:44:03.820 --> 00:44:05.840
zero, the short-running tasks.

00:44:05.840 --> 00:44:09.730
And we're not looking at the
long-running production tasks.

00:44:09.730 --> 00:44:12.680
So I skipped a lot
of detail here.

00:44:12.680 --> 00:44:17.670
So again, now going
back to the big picture,

00:44:17.670 --> 00:44:20.180
how could you put
these ideas and build

00:44:20.180 --> 00:44:22.800
an autonomic controller?

00:44:22.800 --> 00:44:29.060
So first of all, an autonomic
data-driven controller,

00:44:29.060 --> 00:44:31.780
I claim, should have
these requirements.

00:44:31.780 --> 00:44:34.020
It should be closed, in
the sense that it should

00:44:34.020 --> 00:44:36.180
run on the same
infrastructure that you're

00:44:36.180 --> 00:44:38.120
trying to control
and not require

00:44:38.120 --> 00:44:40.040
an infrastructure of its own.

00:44:40.040 --> 00:44:41.730
That is, to control
a data center

00:44:41.730 --> 00:44:44.690
you don't want to build
another data center next to it

00:44:44.690 --> 00:44:45.760
to do the management.

00:44:45.760 --> 00:44:48.980
That's obviously something
that you want to avoid.

00:44:48.980 --> 00:44:50.730
It should be non-intrusive.

00:44:50.730 --> 00:44:53.540
That is, the load
that it generates

00:44:53.540 --> 00:44:58.920
should be quite modest and
not be an overwhelming strain

00:44:58.920 --> 00:45:00.460
on the data center.

00:45:00.460 --> 00:45:03.240
And it should run in real time.

00:45:03.240 --> 00:45:07.270
That is, it should
run on data that's

00:45:07.270 --> 00:45:10.350
being streamed to it
as data is generated,

00:45:10.350 --> 00:45:14.350
and do the prediction
for the future.

00:45:14.350 --> 00:45:19.680
So we did a feasibility
study of this idea.

00:45:19.680 --> 00:45:24.010
So the monitoring system
to do the live streaming

00:45:24.010 --> 00:45:27.140
of the data directly
to BigQuery, at least

00:45:27.140 --> 00:45:30.730
for the amount of
data that we analyzed,

00:45:30.730 --> 00:45:37.180
would require only
$0.25 per day.

00:45:37.180 --> 00:45:40.890
And every 5 minutes recompute
features in parallel

00:45:40.890 --> 00:45:45.070
will require about 2/10
of a second per feature.

00:45:45.070 --> 00:45:48.310
And to store the previously
computed features

00:45:48.310 --> 00:45:50.970
for the last 12
days would require

00:45:50.970 --> 00:45:54.240
about 122 gigabytes of storage.

00:45:54.240 --> 00:46:00.260
And adding all of this up
at real Google pricing this

00:46:00.260 --> 00:46:05.380
would end up being about
$60 a day, which is nothing.

00:46:05.380 --> 00:46:06.121
Yeah?

00:46:06.121 --> 00:46:07.870
AUDIENCE: Yeah, so one
question I have is,

00:46:07.870 --> 00:46:09.560
are you taking
into consideration

00:46:09.560 --> 00:46:12.091
the failure of the
autonomic controller itself?

00:46:14.855 --> 00:46:16.730
OZALP BABAOGLU: Yeah,
that's a good question.

00:46:16.730 --> 00:46:20.290
Obviously the autonomic
controller itself

00:46:20.290 --> 00:46:22.070
is running on the
infrastructure which

00:46:22.070 --> 00:46:23.730
you are trying to
control and predict,

00:46:23.730 --> 00:46:26.875
and that's obviously subject
to the same type of events.

00:46:26.875 --> 00:46:30.110
So could it also
fail, in which case

00:46:30.110 --> 00:46:35.620
you would have to replicate
some of the pieces so as to--

00:46:35.620 --> 00:46:40.800
in this computation I'm
not considering that.

00:46:40.800 --> 00:46:44.980
So yes, if that's a
concern you would obviously

00:46:44.980 --> 00:46:49.960
render the autonomic
controller fault tolerant

00:46:49.960 --> 00:46:55.576
by using the appropriate
techniques of replication.

00:46:55.576 --> 00:46:56.490
AUDIENCE: OK.

00:46:56.490 --> 00:46:58.440
OZALP BABAOGLU: But
that's not included here.

00:46:58.440 --> 00:47:01.800
So once you have
the features, you

00:47:01.800 --> 00:47:05.670
would build the model from
the last 12 days of data,

00:47:05.670 --> 00:47:13.260
and then train it
on the data itself.

00:47:13.260 --> 00:47:18.760
Again, based on real numbers
from something like the Compute

00:47:18.760 --> 00:47:22.070
Engine that would require
about three minutes per Random

00:47:22.070 --> 00:47:23.750
Forest.

00:47:23.750 --> 00:47:26.590
Again, quite a reasonable
amount of time.

00:47:26.590 --> 00:47:31.160
And then once you have the
model, applying it every five

00:47:31.160 --> 00:47:36.800
minutes after computing the
features, applying the model

00:47:36.800 --> 00:47:37.910
is really negligible.

00:47:37.910 --> 00:47:41.000
It's extremely lightweight
and doesn't really

00:47:41.000 --> 00:47:45.680
require any heavy
computation, so I didn't even

00:47:45.680 --> 00:47:47.030
put a number for that.

00:47:47.030 --> 00:47:53.350
So the conclusion being that
the feasibility, at least

00:47:53.350 --> 00:47:56.780
for the dimensions, the
sizes that we considered,

00:47:56.780 --> 00:47:58.590
are quite practical.

00:47:58.590 --> 00:48:02.020
It's not something
that's infeasible.

00:48:02.020 --> 00:48:09.330
You could easily run it on
a typical infrastructure

00:48:09.330 --> 00:48:11.725
at costs that are
really very contained.

00:48:14.300 --> 00:48:20.190
So the takeaway points that
I try to communicate to you

00:48:20.190 --> 00:48:23.070
are the following.

00:48:23.070 --> 00:48:27.780
Autonomic controllers
need to be data-driven,

00:48:27.780 --> 00:48:30.870
and we can use modern
data science techniques

00:48:30.870 --> 00:48:36.170
to help us build
them to overcome

00:48:36.170 --> 00:48:39.280
some of the challenges
of scale and complexity.

00:48:39.280 --> 00:48:42.940
I have outlined such a
data-driven autonomic

00:48:42.940 --> 00:48:44.500
controller to you.

00:48:44.500 --> 00:48:49.310
Not in infinite detail, but a
very high level description.

00:48:49.310 --> 00:48:51.700
And the other main point
that I want to get across

00:48:51.700 --> 00:48:57.180
is that data that are
contained in typical logs, that

00:48:57.180 --> 00:49:01.350
are collected in
HPC or data centers,

00:49:01.350 --> 00:49:07.400
are rich enough to build
useful predictive models.

00:49:07.400 --> 00:49:11.120
Now, the situation can
only get better if you

00:49:11.120 --> 00:49:12.810
were to do this in-house.

00:49:12.810 --> 00:49:18.500
Remember, we're doing this from
an outside-- we're not Google.

00:49:18.500 --> 00:49:21.840
We use the public
Google trace to do this.

00:49:21.840 --> 00:49:25.760
If I were Google
itself I would also--

00:49:25.760 --> 00:49:28.360
I could collect more data.

00:49:28.360 --> 00:49:29.003
Right?

00:49:29.003 --> 00:49:29.690
AUDIENCE: We do.

00:49:29.690 --> 00:49:30.648
OZALP BABAOGLU: You do.

00:49:30.648 --> 00:49:31.520
[LAUGHTER]

00:49:31.520 --> 00:49:32.580
You do.

00:49:32.580 --> 00:49:34.737
Unfortunately, most of that is--

00:49:34.737 --> 00:49:35.570
AUDIENCE: You don't.

00:49:35.570 --> 00:49:37.278
OZALP BABAOGLU: --not
publicly available,

00:49:37.278 --> 00:49:43.560
so by using proprietary
in-house more detailed log data

00:49:43.560 --> 00:49:48.000
the situation can only
get better, is my point.

00:49:48.000 --> 00:49:49.900
And you would obviously do this.

00:49:49.900 --> 00:49:53.460
And the other point that I
tried to get across repeatedly

00:49:53.460 --> 00:49:55.460
is this holistic approach.

00:49:55.460 --> 00:50:01.480
In this particular
work I did not

00:50:01.480 --> 00:50:03.710
have any empirical
evidence for that.

00:50:03.710 --> 00:50:07.340
But in the energy consumption
predictive modeling work

00:50:07.340 --> 00:50:12.350
that I alluded to this holistic
approach is really fundamental.

00:50:12.350 --> 00:50:16.360
That is, you don't want to look
at subsystems in isolation,

00:50:16.360 --> 00:50:20.060
but you really want to
look at the big picture.

00:50:20.060 --> 00:50:23.820
Even by doing
cross-correlation of subsystems

00:50:23.820 --> 00:50:27.890
that you did not even
consider it would be related.

00:50:27.890 --> 00:50:37.370
By doing the wildest
crossing features

00:50:37.370 --> 00:50:41.230
that you thought were completely
irrelevant and unrelated,

00:50:41.230 --> 00:50:42.950
by crossing them
together you get

00:50:42.950 --> 00:50:48.470
features that are extremely
interesting and important.

00:50:48.470 --> 00:50:50.310
And the data-driven
autonomic controller

00:50:50.310 --> 00:50:51.880
can be made practical.

00:50:51.880 --> 00:50:57.730
So this is three papers that are
related to what I talked about.

00:50:57.730 --> 00:51:02.820
And if you want to look at
some of the technical detail,

00:51:02.820 --> 00:51:06.710
you can do that by
looking at the papers.

00:51:06.710 --> 00:51:07.370
Thank you.

00:51:07.370 --> 00:51:07.814
AUDIENCE: Over here.

00:51:07.814 --> 00:51:08.897
OZALP BABAOGLU: Oh, sorry.

00:51:08.897 --> 00:51:13.310
AUDIENCE: Can you say again how
many remove or true fail events

00:51:13.310 --> 00:51:16.070
there were in your dataset?

00:51:16.070 --> 00:51:17.590
Was it 100 some?

00:51:17.590 --> 00:51:19.870
OZALP BABAOGLU:
Well, I can tell you

00:51:19.870 --> 00:51:22.660
the number of remove events.

00:51:22.660 --> 00:51:25.870
If by true you mean true
failures, as I said,

00:51:25.870 --> 00:51:28.080
I don't know that.

00:51:28.080 --> 00:51:32.460
As I defined them, I had the
numbers some slides back.

00:51:32.460 --> 00:51:35.260
158,000 I think.

00:51:35.260 --> 00:51:35.760
Right.

00:51:38.270 --> 00:51:41.490
AUDIENCE: Of the features you
actually had in this dataset,

00:51:41.490 --> 00:51:43.490
is there is there one
where, if you took it out,

00:51:43.490 --> 00:51:44.835
considerably worsened things?

00:51:44.835 --> 00:51:46.460
OZALP BABAOGLU: That's
a good question.

00:51:46.460 --> 00:51:49.700
We did the principal component
analysis of the data,

00:51:49.700 --> 00:51:53.980
and the conclusion was
that, no, we were not

00:51:53.980 --> 00:51:58.430
able to really rank the futures
in any reasonable manner

00:51:58.430 --> 00:52:01.970
so that we could keep a subset
and throw out the others.

00:52:01.970 --> 00:52:07.760
So even though it might sound
somewhat counterintuitive,

00:52:07.760 --> 00:52:11.890
all of the 420 some
features were important,

00:52:11.890 --> 00:52:16.250
at least according to the
technique that we were using.

00:52:16.250 --> 00:52:19.200
So we could not throw
out any of them.

00:52:19.200 --> 00:52:20.450
AUDIENCE: Thank you very much.

00:52:20.450 --> 00:52:21.020
OZALP BABAOGLU: Thank you.

00:52:21.020 --> 00:52:21.853
Thank you very much.

00:52:21.853 --> 00:52:24.850
[APPLAUSE]

