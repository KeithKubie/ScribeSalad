WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.819
[MUSIC PLAYING]

00:00:05.819 --> 00:00:07.860
NOAH FIEDEL: All right,
good morning, Google I/O.

00:00:07.860 --> 00:00:10.090
I'm Noah Fiedel, and I'm
gonna speak with you today

00:00:10.090 --> 00:00:12.390
about how to go from
research to production

00:00:12.390 --> 00:00:15.138
with TensorFlow Serving.

00:00:15.138 --> 00:00:18.410
So I'm gonna start by sharing
a few stories from my past

00:00:18.410 --> 00:00:19.820
and observing industry.

00:00:19.820 --> 00:00:21.780
So the first one,
show of hands, who's

00:00:21.780 --> 00:00:24.872
taken a commercial flight
in the last 10 years?

00:00:24.872 --> 00:00:27.080
All right, everyone's awake
and you've taken flights,

00:00:27.080 --> 00:00:28.280
this is good.

00:00:28.280 --> 00:00:30.690
You might be shocked
to know that circa

00:00:30.690 --> 00:00:33.730
2005 I was working on the
flight planning software that

00:00:33.730 --> 00:00:36.239
plans the majority of
the world's flights.

00:00:36.239 --> 00:00:38.030
This does things like
compute how much fuel

00:00:38.030 --> 00:00:40.390
to take on the plane that
you probably care about.

00:00:40.390 --> 00:00:43.394
But before 2005 that system
did not use source control,

00:00:43.394 --> 00:00:45.310
and for those of you who
know what that means,

00:00:45.310 --> 00:00:47.180
that's pretty scary.

00:00:47.180 --> 00:00:49.570
It does now and
I raise this just

00:00:49.570 --> 00:00:52.590
to show how even though we
know what best practices are

00:00:52.590 --> 00:00:55.740
we publish them, and blog about
them, and talk about them,

00:00:55.740 --> 00:00:57.510
it sometimes takes
a while for them

00:00:57.510 --> 00:00:59.910
to be adopted across industry.

00:00:59.910 --> 00:01:01.580
Fast forward to 2010.

00:01:01.580 --> 00:01:05.099
I was at a startup, and we had
a mobile photo sharing app.

00:01:05.099 --> 00:01:07.140
And we tried to do
all the right things.

00:01:07.140 --> 00:01:09.580
We had source control,
continuous integration using

00:01:09.580 --> 00:01:12.400
Jenkins, and one day
we decided to add

00:01:12.400 --> 00:01:14.280
machine learning to our app.

00:01:14.280 --> 00:01:17.180
We trained up a model that
could you face detection, auto

00:01:17.180 --> 00:01:18.505
crop to faces.

00:01:18.505 --> 00:01:19.380
This is really great.

00:01:19.380 --> 00:01:20.900
Our users loved it.

00:01:20.900 --> 00:01:24.210
Our investors really
loved it in 2010.

00:01:24.210 --> 00:01:26.550
But for machine learning,
what is source control

00:01:26.550 --> 00:01:27.650
in continuous integration?

00:01:27.650 --> 00:01:30.480
These best practices
don't really exist.

00:01:30.480 --> 00:01:32.580
So I trained that model
on my work station.

00:01:32.580 --> 00:01:35.060
And if I got sick or if
my work station crashed,

00:01:35.060 --> 00:01:36.460
no more training for us.

00:01:36.460 --> 00:01:38.210
We'd have to start over.

00:01:38.210 --> 00:01:40.050
So here we are in 2017.

00:01:40.050 --> 00:01:43.340
And at talks like this and other
conferences around the world,

00:01:43.340 --> 00:01:46.600
we have all kinds of great
tools and best practices

00:01:46.600 --> 00:01:48.040
for machine learning.

00:01:48.040 --> 00:01:50.020
So we have TensorFlow
and a whole ecosystem

00:01:50.020 --> 00:01:52.050
around it and many other tools.

00:01:52.050 --> 00:01:54.760
But there are many areas
still to be defined.

00:01:54.760 --> 00:01:57.460
So just as one example
of many, what's

00:01:57.460 --> 00:02:00.560
a continuous integration test
look like for machine learning?

00:02:00.560 --> 00:02:02.970
So you're deploying
models every day.

00:02:02.970 --> 00:02:05.430
When you start doing
this, you might

00:02:05.430 --> 00:02:07.590
make a test that
runs some sample

00:02:07.590 --> 00:02:10.780
data from today for your model,
and it does not all the time.

00:02:10.780 --> 00:02:13.120
What happens when your
user's behavior changes?

00:02:13.120 --> 00:02:14.730
What happens if
they look at bigger

00:02:14.730 --> 00:02:16.454
objects, or different
kinds of objects,

00:02:16.454 --> 00:02:17.620
or the distribution changes?

00:02:17.620 --> 00:02:20.170
So there are many of these
things we are figuring out.

00:02:20.170 --> 00:02:21.670
And I'm gonna make
a bold statement,

00:02:21.670 --> 00:02:23.690
but I think machine
learning is 10 to 20 years

00:02:23.690 --> 00:02:26.270
behind the state of the art
in software engineering.

00:02:26.270 --> 00:02:28.190
And so we have a ways to go.

00:02:28.190 --> 00:02:30.720
One of the ways, and there many,
where TensorFlow Serving is

00:02:30.720 --> 00:02:34.380
helping is by making it easy
to safely and robustly push out

00:02:34.380 --> 00:02:36.530
multiple versions of
your models over time,

00:02:36.530 --> 00:02:38.510
as well as the
ability to roll back.

00:02:38.510 --> 00:02:40.260
And so this seems
pretty simple, but we've

00:02:40.260 --> 00:02:42.440
seen people inside and
outside Google that

00:02:42.440 --> 00:02:43.565
don't have that capability.

00:02:43.565 --> 00:02:48.790
And there are many other things
that are part of this as well.

00:02:48.790 --> 00:02:52.030
All right, so here's the
agenda for today's talk.

00:02:52.030 --> 00:02:55.910
For starters, TensorFlow Serving
is a flexible high performance

00:02:55.910 --> 00:02:57.900
serving system for
machine learned models,

00:02:57.900 --> 00:03:00.880
and it's designed for your
production environments.

00:03:00.880 --> 00:03:03.764
And before I dive into the
details of TensorFlow Serving,

00:03:03.764 --> 00:03:06.180
I'm going to describe what is
serving for those of you who

00:03:06.180 --> 00:03:07.096
might not be familiar.

00:03:09.530 --> 00:03:10.510
It's pretty simple.

00:03:10.510 --> 00:03:12.960
Serving is how you
apply a machine or model

00:03:12.960 --> 00:03:14.570
after you've trained it.

00:03:14.570 --> 00:03:16.910
Many of the talks
on machine learning,

00:03:16.910 --> 00:03:18.330
both in academia
and industry, are

00:03:18.330 --> 00:03:19.867
focused on the training side.

00:03:19.867 --> 00:03:22.450
But serving side is kind of left
as an exercise to the reader,

00:03:22.450 --> 00:03:23.790
so that's where we come in.

00:03:26.840 --> 00:03:28.847
On the right side of
this slide, you're

00:03:28.847 --> 00:03:30.930
hopefully all familiar
with the orange boxes here.

00:03:30.930 --> 00:03:32.710
It you're at a
TensorFlow talk, you

00:03:32.710 --> 00:03:34.720
should be pretty
familiar with these.

00:03:34.720 --> 00:03:36.150
You have a pile of data.

00:03:36.150 --> 00:03:39.209
You have somebody doing the
role of a data scientist.

00:03:39.209 --> 00:03:40.750
And you're training
model, and you'll

00:03:40.750 --> 00:03:43.000
use something like
TensorFlow to do that.

00:03:43.000 --> 00:03:45.660
Now you have your application
on the right side.

00:03:45.660 --> 00:03:47.730
And for the sake of
example, let's say

00:03:47.730 --> 00:03:49.120
that your ranking videos.

00:03:49.120 --> 00:03:51.330
So you have an app that
users come home from work

00:03:51.330 --> 00:03:54.020
and they want to look at some
fun videos and just relax.

00:03:54.020 --> 00:03:56.196
So your application has a
list of candidates videos,

00:03:56.196 --> 00:03:58.070
and your model is sitting
over there on disk.

00:03:58.070 --> 00:04:00.940
How are you actually
gonna apply it?

00:04:00.940 --> 00:04:03.040
So the really straightforward
and common answer

00:04:03.040 --> 00:04:05.020
is you're gonna
have an RPC server.

00:04:05.020 --> 00:04:07.090
So this server is going
to take your model,

00:04:07.090 --> 00:04:08.920
it's going to load
it off of desk,

00:04:08.920 --> 00:04:11.582
and it's gonna to make it
query-able by your application.

00:04:11.582 --> 00:04:13.040
So your application
can give a list

00:04:13.040 --> 00:04:14.810
of videos, along
with their features,

00:04:14.810 --> 00:04:17.450
and the server will reply back
with maybe the probability

00:04:17.450 --> 00:04:19.158
of each of those videos
being clicked on.

00:04:21.630 --> 00:04:24.780
All right, so moving on
to some goals for serving,

00:04:24.780 --> 00:04:27.260
and in particular for serving
machine learned models.

00:04:27.260 --> 00:04:30.530
So the first one-- and this
is where serving differs quite

00:04:30.530 --> 00:04:33.800
a bit from what we're used to in
talking about training of ML--

00:04:33.800 --> 00:04:36.500
is that requests are
coming in all the time

00:04:36.500 --> 00:04:38.360
asynchronously from users.

00:04:38.360 --> 00:04:41.244
You can't control them, you're
not reading them off of disk.

00:04:41.244 --> 00:04:43.910
But whenever a user sits down on
their couch and loads your app,

00:04:43.910 --> 00:04:45.900
that's when you're
gonna get a request.

00:04:45.900 --> 00:04:48.110
So we want to answer
these requests online

00:04:48.110 --> 00:04:51.090
and at very low latency in
consistently low latency

00:04:51.090 --> 00:04:53.750
for all of your users.

00:04:53.750 --> 00:04:57.080
The next ones are also subtle
departures from training.

00:04:57.080 --> 00:04:59.610
So the first is that you might
want have multiple models

00:04:59.610 --> 00:05:01.310
served at the same time.

00:05:01.310 --> 00:05:04.340
So when many groups start
with machine learning,

00:05:04.340 --> 00:05:06.890
they'll have one model that
might, as this example goes,

00:05:06.890 --> 00:05:09.220
serve rankings for videos.

00:05:09.220 --> 00:05:11.770
But now what happens when
week two comes along,

00:05:11.770 --> 00:05:13.620
you have a model it's
working on production,

00:05:13.620 --> 00:05:15.270
but now you want to
launch a new one.

00:05:15.270 --> 00:05:16.728
You're not sure if
it's gonna work,

00:05:16.728 --> 00:05:17.960
you might run an experiment.

00:05:17.960 --> 00:05:20.380
So now you want to run two
models and maybe have a couple

00:05:20.380 --> 00:05:22.270
different experiments
you want to run.

00:05:22.270 --> 00:05:25.120
So that's really common as well.

00:05:25.120 --> 00:05:29.630
And lastly, who's familiar
with mini-batching?

00:05:29.630 --> 00:05:30.610
OK, just a few of us.

00:05:30.610 --> 00:05:32.020
So I'm gonna describe
it so that we're

00:05:32.020 --> 00:05:34.700
familiar with this cause it's an
important part of both training

00:05:34.700 --> 00:05:35.870
and serving.

00:05:35.870 --> 00:05:39.310
So when you take a
neural network graph,

00:05:39.310 --> 00:05:41.890
and you process data
through it, there's

00:05:41.890 --> 00:05:45.340
some overhead that happens
with all the nodes of the graph

00:05:45.340 --> 00:05:47.220
and scheduling the
work across them.

00:05:47.220 --> 00:05:49.580
And to get good efficiency
almost all of the training

00:05:49.580 --> 00:05:52.280
libraries out there will produce
what's called a mini-batch.

00:05:52.280 --> 00:05:55.860
So instead of putting one video
through your trainer at a time,

00:05:55.860 --> 00:05:58.449
it'll come up with
patches of 32, 64, 128.

00:05:58.449 --> 00:05:59.990
Push them through
the graph together,

00:05:59.990 --> 00:06:01.970
and you get massive
throughput improvements

00:06:01.970 --> 00:06:08.200
by doing that, in particular
on GPUs and TPUs and so on.

00:06:08.200 --> 00:06:11.177
So we aim to achieve that
efficiency of mini-batching,

00:06:11.177 --> 00:06:13.260
but at serve time when all
the requests are coming

00:06:13.260 --> 00:06:16.380
in asynchronously, and you
don't have a nice neat 32 size

00:06:16.380 --> 00:06:18.130
batches off of disk.

00:06:18.130 --> 00:06:22.200
And the multiple model support
and efficiency, they're

00:06:22.200 --> 00:06:23.670
neat challenges on their own.

00:06:23.670 --> 00:06:25.520
But what makes them
particularly interesting

00:06:25.520 --> 00:06:28.000
is doing those while also
maintaining those standards

00:06:28.000 --> 00:06:31.687
for low latency all the time.

00:06:31.687 --> 00:06:34.020
All right, so I just wanted
to say again, and throw this

00:06:34.020 --> 00:06:36.380
up there for you all to
read, that TensorFlow Serving

00:06:36.380 --> 00:06:38.420
is a flexible, high
performance serving

00:06:38.420 --> 00:06:40.360
system for machinery models.

00:06:40.360 --> 00:06:45.490
And it's really designed for
your production environments.

00:06:45.490 --> 00:06:49.140
TensorFlow Serving has
about three major pillars.

00:06:49.140 --> 00:06:52.100
The first one is a
set of C++ libraries.

00:06:52.100 --> 00:06:55.260
These include standard support
for the saving and loading

00:06:55.260 --> 00:06:56.840
of TensorFlow models.

00:06:56.840 --> 00:06:58.548
They're pretty
straightforward, something

00:06:58.548 --> 00:07:01.570
you'd want to do with
a serving system.

00:07:01.570 --> 00:07:03.560
We also have a
generic core platform,

00:07:03.560 --> 00:07:04.890
and this is truly generic.

00:07:04.890 --> 00:07:07.307
It's not tied to TensorFlow,
although TensorFlow

00:07:07.307 --> 00:07:08.890
has first class
plugins for everything

00:07:08.890 --> 00:07:10.000
you might want to do.

00:07:10.000 --> 00:07:12.242
So let's say you have a
legacy ML system in house

00:07:12.242 --> 00:07:14.700
and you might want to mix and
match for some period of time

00:07:14.700 --> 00:07:15.960
with, say, TensorFlow.

00:07:15.960 --> 00:07:16.780
You can do that.

00:07:16.780 --> 00:07:20.290
You can write adapters that
plugin different ML platforms,

00:07:20.290 --> 00:07:23.170
and run them in
TensorFlow Serving.

00:07:23.170 --> 00:07:25.872
Building on top of our
libraries, we have binaries.

00:07:25.872 --> 00:07:27.830
And these incorporate
all of the best practices

00:07:27.830 --> 00:07:30.530
that we've learned out of the
box, so they make it easy.

00:07:30.530 --> 00:07:33.310
We have reference Docker
containers and tutorials

00:07:33.310 --> 00:07:36.040
and code labs for running on
Kubernetes with auto scaling

00:07:36.040 --> 00:07:37.440
and so on.

00:07:37.440 --> 00:07:41.020
And the third pillar is a hosted
service with Google Cloud ML,

00:07:41.020 --> 00:07:43.660
as well as our internal instance
that many Google products are

00:07:43.660 --> 00:07:45.730
using.

00:07:45.730 --> 00:07:48.690
Lastly, I wanted to highlight
that across all three

00:07:48.690 --> 00:07:51.900
of these different offerings
your models are portable.

00:07:51.900 --> 00:07:53.710
We worked really
hard to make sure

00:07:53.710 --> 00:07:56.860
that one model format will work
in any of these environments.

00:07:56.860 --> 00:07:59.070
So you can take your
model, try it on a binary.

00:07:59.070 --> 00:08:02.510
You want to do something custom,
try it in a library and so on.

00:08:02.510 --> 00:08:06.550
You can seamlessly
migrate back and forth.

00:08:06.550 --> 00:08:08.260
All right, so super
excited to show

00:08:08.260 --> 00:08:10.520
for the first time that
were used very, very

00:08:10.520 --> 00:08:11.760
broadly across Google.

00:08:11.760 --> 00:08:16.050
These are just some of our
key customers inside Google.

00:08:16.050 --> 00:08:17.650
For the first time
I can say that we

00:08:17.650 --> 00:08:20.390
have over 700
projects inside Google

00:08:20.390 --> 00:08:23.274
using TensorFlow
Serving, so woo-hoo.

00:08:23.274 --> 00:08:25.291
[APPLAUSE]

00:08:25.291 --> 00:08:25.790
Thank you.

00:08:28.690 --> 00:08:31.790
All right, so I'm gonna dive in
a little bit to the libraries.

00:08:31.790 --> 00:08:33.580
So I mentioned the
generic core platform.

00:08:33.580 --> 00:08:36.880
I wanted to mention also
that the components are all

00:08:36.880 --> 00:08:39.500
a la carte, and this means
that you can mix and match them

00:08:39.500 --> 00:08:40.919
to suit your needs.

00:08:40.919 --> 00:08:43.190
So if you're doing
something really advanced--

00:08:43.190 --> 00:08:45.760
and we have some incredibly
advanced internal customers--

00:08:45.760 --> 00:08:47.370
you can mix and match
these components

00:08:47.370 --> 00:08:49.550
and use just the ones that
accomplish your needs.

00:08:49.550 --> 00:08:51.880
You don't have to buy the
entire set of libraries.

00:08:51.880 --> 00:08:54.500
You can take the ones you want.

00:08:54.500 --> 00:08:55.000
Let's see.

00:08:55.000 --> 00:08:57.190
There's the batcher for
inference performance.

00:08:57.190 --> 00:09:00.870
This give us that mini-batching
performance, but at serve time.

00:09:00.870 --> 00:09:03.091
And lastly, you can
plug-in different sources

00:09:03.091 --> 00:09:04.340
if you have different storage.

00:09:04.340 --> 00:09:07.000
So maybe you have a cloud
storage of your choice.

00:09:07.000 --> 00:09:09.890
You can write a
plug-in for that.

00:09:09.890 --> 00:09:10.390
All right.

00:09:10.390 --> 00:09:13.340
So we're gonna go through
a pretty big slide here.

00:09:13.340 --> 00:09:16.930
I'll try and make this
digestable for you.

00:09:16.930 --> 00:09:18.900
And this is zooming
into the libraries

00:09:18.900 --> 00:09:22.900
as they would exist inside
our server in the binary.

00:09:22.900 --> 00:09:25.640
So say your server's
up and running,

00:09:25.640 --> 00:09:28.300
and it's serving your video
ranking model, version one.

00:09:28.300 --> 00:09:29.690
And it's cruising along.

00:09:29.690 --> 00:09:30.867
Everything's working well.

00:09:30.867 --> 00:09:33.200
And say you've trained version
two and you to deploy it.

00:09:33.200 --> 00:09:35.622
So I'm gonna walk you
through how that would work.

00:09:35.622 --> 00:09:37.330
So on the bottom right
of the slide here,

00:09:37.330 --> 00:09:41.670
you'll see the green source
API with the yellow orange file

00:09:41.670 --> 00:09:43.305
system plug-in.

00:09:43.305 --> 00:09:45.170
It's a really straight
forward plugin.

00:09:45.170 --> 00:09:46.830
It simply monitors
a file system,

00:09:46.830 --> 00:09:49.300
observes the new
versions of your model,

00:09:49.300 --> 00:09:52.470
in this case version two.

00:09:52.470 --> 00:09:54.290
And the source is
going to omit a loader.

00:09:54.290 --> 00:09:57.190
In this case, it's going to
admit a loader of a TensorFlow

00:09:57.190 --> 00:09:58.692
saved model.

00:09:58.692 --> 00:10:01.150
It's important to note that
the loader doesn't immediately

00:10:01.150 --> 00:10:03.300
load that TensorFlow graph.

00:10:03.300 --> 00:10:05.080
It keeps track of the metadata.

00:10:05.080 --> 00:10:07.220
It can estimate the
ram requirements

00:10:07.220 --> 00:10:09.750
and other resources
used by the model.

00:10:09.750 --> 00:10:11.960
And then it can load the
model when it's asked to.

00:10:11.960 --> 00:10:14.390
And this is really an
important differentiator

00:10:14.390 --> 00:10:16.780
between a straightforward
model server

00:10:16.780 --> 00:10:18.950
that you might build yourselves.

00:10:18.950 --> 00:10:20.950
So this loader that's
very lightweight,

00:10:20.950 --> 00:10:22.880
is omitted over to the manager.

00:10:22.880 --> 00:10:25.020
Now the manager
actually knows the state

00:10:25.020 --> 00:10:26.980
of the server, how
much RAM is available,

00:10:26.980 --> 00:10:30.200
and other resources such
as GPU or TPU memory.

00:10:30.200 --> 00:10:32.215
And only when it's
safe to do so is

00:10:32.215 --> 00:10:34.050
it gonna ask the loader
to go ahead and load

00:10:34.050 --> 00:10:36.400
version two of the model.

00:10:36.400 --> 00:10:39.180
So let's say that the
manager has plenty of memory,

00:10:39.180 --> 00:10:41.261
it goes ahead and loads
version two of the model.

00:10:41.261 --> 00:10:42.760
You might think
that the first thing

00:10:42.760 --> 00:10:45.135
it's going to do immediately
is let's unload version one.

00:10:45.135 --> 00:10:46.180
We don't need it anymore.

00:10:46.180 --> 00:10:47.930
But there's another
important detail here.

00:10:47.930 --> 00:10:52.200
I see some smiles, so you
probably know this is.

00:10:52.200 --> 00:10:55.949
Say you have client request
threads on the top left here.

00:10:55.949 --> 00:10:57.490
And they're actually
still processing

00:10:57.490 --> 00:10:59.420
some queries on model one.

00:10:59.420 --> 00:11:01.610
So what you actually
have to do is keep track.

00:11:01.610 --> 00:11:03.430
In this case, we
have a handle on top

00:11:03.430 --> 00:11:04.990
of that TensorFlow saved model.

00:11:04.990 --> 00:11:07.210
And it keeps track a
with a high performance

00:11:07.210 --> 00:11:10.940
ref counted pointer mechanism
of exactly how many clients

00:11:10.940 --> 00:11:14.320
are still outstanding
processing requests.

00:11:14.320 --> 00:11:16.520
Only when all of those
requests have quiesced,

00:11:16.520 --> 00:11:18.800
then the manager will go
ahead and unload version one.

00:11:20.592 --> 00:11:22.550
All right, so I'm going
to cover some strengths

00:11:22.550 --> 00:11:23.470
of the libraries.

00:11:23.470 --> 00:11:27.030
I'll just highlight a few
aspects of this slide.

00:11:27.030 --> 00:11:29.610
The one I wanna highlight
here is the fast model

00:11:29.610 --> 00:11:30.910
loading on server start up.

00:11:30.910 --> 00:11:33.330
This is another
really subtle detail,

00:11:33.330 --> 00:11:35.650
but it's really helped
a lot of our users.

00:11:35.650 --> 00:11:38.740
So let's say you're
starting up a server.

00:11:38.740 --> 00:11:41.030
There's a couple of reasons
you might want to do this.

00:11:41.030 --> 00:11:43.256
Let's say running an experiment
on your work station.

00:11:43.256 --> 00:11:45.130
You don't want to wait
a long period of time.

00:11:45.130 --> 00:11:48.170
You want that to load
as quickly as you can.

00:11:48.170 --> 00:11:51.640
Another example where it's even
more critical is auto scaling.

00:11:51.640 --> 00:11:53.882
So say your users all
get home at 6 o'clock.

00:11:53.882 --> 00:11:56.090
They all sit on the couch,
pull out your application,

00:11:56.090 --> 00:11:59.770
and send you a big traffic
spike of video ranking requests.

00:11:59.770 --> 00:12:02.430
Using a system like Kubernetes,
you're gonna want to auto scale

00:12:02.430 --> 00:12:04.620
as quickly as possible,
and you want those servers

00:12:04.620 --> 00:12:06.040
to start up quickly.

00:12:06.040 --> 00:12:09.020
So at the same time, most
of the time with servers

00:12:09.020 --> 00:12:11.650
you only want maybe one
or two of your threads

00:12:11.650 --> 00:12:12.510
to do model loading.

00:12:12.510 --> 00:12:14.510
You want all of your
threads and CPUs

00:12:14.510 --> 00:12:17.370
to be performing
inference for your users.

00:12:17.370 --> 00:12:21.147
But, say, on a 16 CPU machine,
why not use all of those cores

00:12:21.147 --> 00:12:22.980
and all of those threads
from model loading.

00:12:22.980 --> 00:12:25.170
It's just a small optimization
in one of many that

00:12:25.170 --> 00:12:27.250
we do to make it
easier to auto scale

00:12:27.250 --> 00:12:31.280
and run your actual
models in production.

00:12:31.280 --> 00:12:33.542
Let's see, we use the
re-copy update pattern,

00:12:33.542 --> 00:12:35.000
which is a high
performance pattern

00:12:35.000 --> 00:12:38.490
for doing concurrent memory
access of these models.

00:12:38.490 --> 00:12:42.360
I mentioned the ref-counted
pointers and the simple plugin

00:12:42.360 --> 00:12:42.940
interfaces.

00:12:42.940 --> 00:12:46.030
So it's really easy to extend
to support your own data

00:12:46.030 --> 00:12:51.321
store, cloud storage, or even
a database of your models.

00:12:51.321 --> 00:12:51.820
All right.

00:12:51.820 --> 00:12:54.111
Another show of hands, who
has used TensorFlow Servings

00:12:54.111 --> 00:12:56.310
libraries?

00:12:56.310 --> 00:12:58.450
OK, a few advanced users.

00:12:58.450 --> 00:13:01.710
So especially for you folks, but
also for anybody who's thinking

00:13:01.710 --> 00:13:03.770
that you might want
to use our libraries,

00:13:03.770 --> 00:13:05.420
definitely take a
look at ServerCore.

00:13:05.420 --> 00:13:08.280
So what we observed
was that our libraries

00:13:08.280 --> 00:13:12.160
were low level, very powerful,
and very configurable.

00:13:12.160 --> 00:13:13.810
But for most people
you really wanted

00:13:13.810 --> 00:13:16.000
most of the sane and
sensible defaults.

00:13:16.000 --> 00:13:17.850
You wanted to load
some set of models.

00:13:17.850 --> 00:13:19.890
You wanted to load new
versions over time.

00:13:19.890 --> 00:13:21.820
And so we made the
class ServerCore,

00:13:21.820 --> 00:13:26.880
which does this for you, and
it's configuration driven.

00:13:26.880 --> 00:13:28.900
If you move to it
from our libraries

00:13:28.900 --> 00:13:30.700
you can remove about
300 lines of code

00:13:30.700 --> 00:13:33.080
and just have a nice little
config, so give that a try.

00:13:36.650 --> 00:13:38.540
All right, so moving
on to our binaries,

00:13:38.540 --> 00:13:41.736
and this is what we
recommend for most users.

00:13:41.736 --> 00:13:43.110
I mentioned a few
times there are

00:13:43.110 --> 00:13:46.230
things like sensible defaults,
how many threads to use, and so

00:13:46.230 --> 00:13:47.294
on.

00:13:47.294 --> 00:13:49.460
The binaries come out of
the box with those enabled,

00:13:49.460 --> 00:13:52.700
so you only need to
specify the minimum amount

00:13:52.700 --> 00:13:54.409
of configuration.

00:13:54.409 --> 00:13:55.700
They're very easy to configure.

00:13:55.700 --> 00:13:58.180
I'll show you on a
coming soon slide

00:13:58.180 --> 00:14:00.610
on how easy it is
to launch these.

00:14:00.610 --> 00:14:03.600
And they're based on gRPC,
which is Google's open source

00:14:03.600 --> 00:14:07.840
high performance RPC framework.

00:14:07.840 --> 00:14:10.680
All right, so hopefully you
can read these code samples.

00:14:10.680 --> 00:14:13.180
The line at the top, this is
how you build the model server.

00:14:13.180 --> 00:14:16.120
It's a one liner with bazel.

00:14:16.120 --> 00:14:17.770
The second line is
how you're gonna

00:14:17.770 --> 00:14:20.380
run the server
for a single model

00:14:20.380 --> 00:14:23.080
in just a one line command,
no config files needed,

00:14:23.080 --> 00:14:24.415
just three flags.

00:14:24.415 --> 00:14:28.200
So you're gonna specify the
port, the name of your model,

00:14:28.200 --> 00:14:30.370
and the path to
the model on disk.

00:14:30.370 --> 00:14:32.150
And this is we
call it a base path

00:14:32.150 --> 00:14:34.252
because you might have
multiple versions overtime.

00:14:34.252 --> 00:14:36.460
So over time you can just
drop more and more versions

00:14:36.460 --> 00:14:38.880
in that directory and
they'll be loaded.

00:14:38.880 --> 00:14:41.470
And lastly, we have the
command to run the model server

00:14:41.470 --> 00:14:43.790
with a config file, and this
could have as many models

00:14:43.790 --> 00:14:45.960
as you want, as many as
will fit on your server.

00:14:48.710 --> 00:14:50.382
All right, so this is great.

00:14:50.382 --> 00:14:51.090
We have a server.

00:14:51.090 --> 00:14:53.542
How do we actually talk to it?

00:14:53.542 --> 00:14:56.000
I'm gonna speak a little bit
about our inference API's that

00:14:56.000 --> 00:14:58.590
are supported by
the model server.

00:14:58.590 --> 00:15:01.040
So the first one
is called Predict.

00:15:01.040 --> 00:15:03.700
It's very, very
flexible and powerful.

00:15:03.700 --> 00:15:08.300
It's tensor oriented, you
can specify one or many input

00:15:08.300 --> 00:15:10.750
tensors and output tensors.

00:15:10.750 --> 00:15:12.790
And so you can do basically
anything you can do,

00:15:12.790 --> 00:15:14.730
if you're familiar with
a TensorFlow session.

00:15:14.730 --> 00:15:16.970
If you've been playing
with that in, say, Python.

00:15:16.970 --> 00:15:20.660
You can do just about anything
there with a predict API.

00:15:20.660 --> 00:15:22.650
Now moving up a
layer of abstraction,

00:15:22.650 --> 00:15:25.190
we've observe that the vast
majority of machine learning

00:15:25.190 --> 00:15:27.320
used in production
is actually doing

00:15:27.320 --> 00:15:29.360
classification in regression.

00:15:29.360 --> 00:15:32.620
We have two APIs,
Regress and Classify.

00:15:32.620 --> 00:15:34.770
And these use
TensorFlow standard

00:15:34.770 --> 00:15:37.820
and structured input format
called TensorFlow example.

00:15:37.820 --> 00:15:39.590
It's a protocol message.

00:15:39.590 --> 00:15:42.440
It's feature oriented, so
you can have different values

00:15:42.440 --> 00:15:44.100
for different features.

00:15:44.100 --> 00:15:45.720
And a nice thing
about these APIs

00:15:45.720 --> 00:15:48.489
is you actually don't have to
think about tensors at all.

00:15:48.489 --> 00:15:50.030
So if you're new to
machine learning,

00:15:50.030 --> 00:15:52.520
you just want to try something
out, try code lab, or even

00:15:52.520 --> 00:15:54.520
if you want to deploy in
production which we see

00:15:54.520 --> 00:15:59.090
many production users inside
Google using these APIs,

00:15:59.090 --> 00:16:02.630
go ahead and try the
high level APIs first.

00:16:02.630 --> 00:16:04.830
Next up, and I'll talk
about a MultiInference

00:16:04.830 --> 00:16:06.630
a bit later in the talk.

00:16:06.630 --> 00:16:09.510
MultiInference allows you to
combine multiple regressions

00:16:09.510 --> 00:16:12.050
and classifications
into a single RPC,

00:16:12.050 --> 00:16:15.800
and that has some
really cool benefits.

00:16:15.800 --> 00:16:18.490
All right, here's a
total toy model for you

00:16:18.490 --> 00:16:21.330
and this is just to show what
does the syntax look like.

00:16:21.330 --> 00:16:24.110
I've talk about these APIs if
you haven't seen them before.

00:16:24.110 --> 00:16:26.240
So we have a model
spec, and this specifies

00:16:26.240 --> 00:16:27.370
the name of the model.

00:16:27.370 --> 00:16:29.786
And this is because your server
could have multiple models

00:16:29.786 --> 00:16:31.900
running at the same time.

00:16:31.900 --> 00:16:33.470
We'll have one feature.

00:16:33.470 --> 00:16:35.360
In this case, the key
for it is measurements.

00:16:35.360 --> 00:16:38.200
We have three
floating point values,

00:16:38.200 --> 00:16:40.499
and we have a structured
result. Again,

00:16:40.499 --> 00:16:42.540
you don't have to inspect
tensors you'll actually

00:16:42.540 --> 00:16:45.050
get a structured result
called regressions,

00:16:45.050 --> 00:16:46.610
which has one score.

00:16:46.610 --> 00:16:47.700
So just an example.

00:16:50.290 --> 00:16:52.850
All right, so I'm gonna move on
to some of our key challenges

00:16:52.850 --> 00:16:54.610
that for the most
part we've solved,

00:16:54.610 --> 00:16:57.870
but we still have
quite a ways to go.

00:16:57.870 --> 00:17:01.360
OK, so this is the story
of isolation and latency.

00:17:01.360 --> 00:17:03.880
So from anybody who's ever
looked at a latency graph

00:17:03.880 --> 00:17:06.040
before, you can probably
tell that those spikes

00:17:06.040 --> 00:17:07.040
are really bad.

00:17:07.040 --> 00:17:09.740
You don't want to
have those, right?

00:17:09.740 --> 00:17:11.990
Those spikes were happening
pretty much Monday

00:17:11.990 --> 00:17:15.349
through Friday, around
12:00 or 1 o'clock.

00:17:15.349 --> 00:17:16.390
Does anyone have an idea?

00:17:16.390 --> 00:17:17.599
Just shout it out.

00:17:17.599 --> 00:17:20.703
What caused those
latency spikes?

00:17:20.703 --> 00:17:21.599
AUDIENCE: Lunch time.

00:17:21.599 --> 00:17:23.579
SPEAKER 1: Lunch time.

00:17:23.579 --> 00:17:26.470
Strongly correlated.

00:17:26.470 --> 00:17:28.960
We have this fantastic engineer
we were working with on one

00:17:28.960 --> 00:17:30.300
of our largest client teams.

00:17:30.300 --> 00:17:31.940
They were serving
multiple models.

00:17:31.940 --> 00:17:33.820
These were large
multi gigabyte models.

00:17:33.820 --> 00:17:36.070
And they were serving several
hundred thousand queries

00:17:36.070 --> 00:17:38.230
per second on a
fleet of servers.

00:17:38.230 --> 00:17:41.080
And what was happening is
this great engineer, really

00:17:41.080 --> 00:17:44.820
a close collaborator,
he would go to lunch.

00:17:44.820 --> 00:17:46.620
But right before
he went to lunch,

00:17:46.620 --> 00:17:48.600
he would push a new
version of the model.

00:17:48.600 --> 00:17:50.475
So you can probably
pretty quickly figure out

00:17:50.475 --> 00:17:52.270
what's going on here.

00:17:52.270 --> 00:17:55.760
So every time a new model--
a particularly large one,

00:17:55.760 --> 00:17:58.480
about 5 gigabytes--
was loading, inference

00:17:58.480 --> 00:18:02.187
would slow down at the
upper end of percentile.

00:18:02.187 --> 00:18:03.770
You might wonder why
is this going on.

00:18:03.770 --> 00:18:06.450
Once we figured out the cause it
was pretty easy to figure out.

00:18:06.450 --> 00:18:08.370
So for most of
machine learning you

00:18:08.370 --> 00:18:11.190
have one model in a server at
a time, whether your training

00:18:11.190 --> 00:18:12.800
or serving until now.

00:18:12.800 --> 00:18:16.060
And so most systems are
optimized around throughput,

00:18:16.060 --> 00:18:17.210
not latency.

00:18:17.210 --> 00:18:20.990
And so all available threads are
available for any computation

00:18:20.990 --> 00:18:22.370
that's ready to run.

00:18:22.370 --> 00:18:26.690
In this case model loading
is penalizable in TensorFlow.

00:18:26.690 --> 00:18:30.080
And so TensorFlow would gladly
use all the threads available

00:18:30.080 --> 00:18:35.550
and go load that new model
starving inference of threads.

00:18:35.550 --> 00:18:37.660
So here's the after
slide, and definitely

00:18:37.660 --> 00:18:41.320
not that the right side
of this slide, the axis,

00:18:41.320 --> 00:18:43.150
just dropped by 10x.

00:18:43.150 --> 00:18:45.940
So we went from over a second
in terms of our latency spikes

00:18:45.940 --> 00:18:47.560
to about a tenth of a second.

00:18:47.560 --> 00:18:49.737
This fully met the
needs of the customer.

00:18:49.737 --> 00:18:50.570
They're serving SOA.

00:18:50.570 --> 00:18:51.734
They're happy.

00:18:51.734 --> 00:18:53.400
And the way that we
achieved this was we

00:18:53.400 --> 00:18:55.370
added an option
to TensorFlow that

00:18:55.370 --> 00:18:58.430
lets you explicitly specify
the number of threads

00:18:58.430 --> 00:19:00.670
to use for any
call to your graph.

00:19:00.670 --> 00:19:02.680
And in this case,
we wired them up so

00:19:02.680 --> 00:19:05.170
there's one thread pool
for model loading and one

00:19:05.170 --> 00:19:07.780
for inference, so
pretty straightforward.

00:19:07.780 --> 00:19:10.150
By default, since
all of users want

00:19:10.150 --> 00:19:12.410
to use all the threads for
inference all the time,

00:19:12.410 --> 00:19:15.090
we actually only need users to
configure the number of threads

00:19:15.090 --> 00:19:15.760
for loading.

00:19:15.760 --> 00:19:18.520
So usually specify one or two,
and then you're good to go.

00:19:21.790 --> 00:19:23.980
This is going to be
moving on to batching,

00:19:23.980 --> 00:19:26.100
several slides I mentioned
earlier in the talk.

00:19:26.100 --> 00:19:28.830
This is a really key challenge
to get great performance

00:19:28.830 --> 00:19:31.070
and throughput.

00:19:31.070 --> 00:19:34.090
And so let's look at the
right side of the slide.

00:19:34.090 --> 00:19:36.480
You have these Tetris blocks
that are falling down.

00:19:36.480 --> 00:19:38.660
So imagine these are
requests from your users.

00:19:38.660 --> 00:19:41.620
Maybe those blocks
represent individual videos

00:19:41.620 --> 00:19:43.740
that are being ranked
by your server.

00:19:43.740 --> 00:19:46.810
And their different height
represents the time at which

00:19:46.810 --> 00:19:49.030
they arrive at your server.

00:19:49.030 --> 00:19:52.140
So what we do is we wait for a
very small and tunable period

00:19:52.140 --> 00:19:53.320
of time.

00:19:53.320 --> 00:19:55.340
And we wait for a few queries.

00:19:55.340 --> 00:19:57.170
We aggregate them
together, and stitch them

00:19:57.170 --> 00:20:01.431
into one set of tensors that we
feed into the TensorFlow graph.

00:20:01.431 --> 00:20:05.240
So this enables you to
get very efficient use

00:20:05.240 --> 00:20:09.940
out of GPUs and TPUs that have
really good batch parallelism.

00:20:09.940 --> 00:20:11.530
One external paper,
the spin paper,

00:20:11.530 --> 00:20:14.260
saw that moving to a
batched inference on GPU

00:20:14.260 --> 00:20:17.540
led to a 25 times speed up.

00:20:17.540 --> 00:20:20.760
I'm also I excited to
share that on CPU we've

00:20:20.760 --> 00:20:23.910
seen for models up to a 50%
improvement in throughput.

00:20:23.910 --> 00:20:25.630
So it's a tiny
little tuning knob

00:20:25.630 --> 00:20:29.660
that can save you quite a
bit of your inference costs.

00:20:29.660 --> 00:20:32.990
Another thing is that
once you're actually

00:20:32.990 --> 00:20:35.430
doing batching on top of
custom hardware, hardware

00:20:35.430 --> 00:20:38.730
accelerators, and GPUs,
you'll typically only want

00:20:38.730 --> 00:20:41.800
to have one unit of work
being done on that chip

00:20:41.800 --> 00:20:43.330
or device at a time.

00:20:43.330 --> 00:20:45.070
And so one thing
we noticed is when

00:20:45.070 --> 00:20:48.320
you're going to have multiple
models and multiple versions

00:20:48.320 --> 00:20:50.631
but perhaps one
piece of hardware,

00:20:50.631 --> 00:20:52.630
you're gonna need to
schedule the work carefully

00:20:52.630 --> 00:20:53.860
across those models.

00:20:53.860 --> 00:20:57.500
We have a shared batch scheduler
that lets you do just that.

00:20:57.500 --> 00:20:59.850
Lastly, the batching
capabilities

00:20:59.850 --> 00:21:02.870
are all available
in both library form

00:21:02.870 --> 00:21:04.910
as well as they're
configurable in the binary.

00:21:04.910 --> 00:21:09.575
So you can access them very
easily in either approach.

00:21:09.575 --> 00:21:10.950
Now we're going
to dive into what

00:21:10.950 --> 00:21:15.270
happens inside one of
these session run calls.

00:21:15.270 --> 00:21:19.000
All right, so hopefully you can
parse this utilization graph.

00:21:19.000 --> 00:21:22.710
There are two different kinds
of utilization I'm showing here.

00:21:22.710 --> 00:21:25.310
So the first is
the blue dash line.

00:21:25.310 --> 00:21:27.910
This is that
utilization of your CPU.

00:21:27.910 --> 00:21:30.660
And you'll notice it's not
used through the whole period

00:21:30.660 --> 00:21:31.850
of this request.

00:21:31.850 --> 00:21:35.540
The orange line represents
the GPU or TPU utilization.

00:21:35.540 --> 00:21:39.820
So walking through, we'll start
with some single threaded CPU

00:21:39.820 --> 00:21:42.330
bound work, like, say,
parsing of your input.

00:21:42.330 --> 00:21:45.075
It's really common, pretty much
all models are gonna do this.

00:21:45.075 --> 00:21:46.700
Then you might do
something else that's

00:21:46.700 --> 00:21:51.010
single threaded, like do a vocab
lookup or an embedding lookup.

00:21:51.010 --> 00:21:53.680
OK, you're done
preprocessing your data.

00:21:53.680 --> 00:21:55.925
Now you're gonna shift
over and do computation

00:21:55.925 --> 00:21:57.900
on the GPU or TPU.

00:21:57.900 --> 00:22:00.110
So your CPU basically
goes to idle.

00:22:00.110 --> 00:22:03.470
Your TPU or GPU is maxed
out for some period of time.

00:22:03.470 --> 00:22:05.160
It returns, and
then again you're

00:22:05.160 --> 00:22:07.470
gonna some post
processing on CPU.

00:22:07.470 --> 00:22:10.440
And the reason I show this
is that, naively, if you just

00:22:10.440 --> 00:22:14.450
ran one inference at a time
on this set of CPU and GPU,

00:22:14.450 --> 00:22:16.630
you're gonna be
vastly under utilizing

00:22:16.630 --> 00:22:19.520
both pieces of hardware.

00:22:19.520 --> 00:22:21.130
So one of the ways
that we solve this

00:22:21.130 --> 00:22:24.790
is by having multiple
threads, very, very

00:22:24.790 --> 00:22:26.440
common in serving systems.

00:22:26.440 --> 00:22:28.790
And what you can
do is you can have

00:22:28.790 --> 00:22:30.350
a good number of
threads and limit

00:22:30.350 --> 00:22:32.560
how many are running
at the same time.

00:22:32.560 --> 00:22:34.530
And you can make it so
that you're constantly

00:22:34.530 --> 00:22:37.640
using your GPU and TPU, and
that most of the time you're

00:22:37.640 --> 00:22:39.920
using your CPU.

00:22:39.920 --> 00:22:42.400
But the key takeaway
from this slide

00:22:42.400 --> 00:22:44.620
is that there's this
queuing time that

00:22:44.620 --> 00:22:47.040
happens at the beginning
of each request.

00:22:47.040 --> 00:22:49.560
So once you enable batching,
I mentioned earlier

00:22:49.560 --> 00:22:51.660
there's this tunable
window of time where you're

00:22:51.660 --> 00:22:53.920
going to wait for a few
requests to come together,

00:22:53.920 --> 00:22:55.500
and then you'll process them.

00:22:55.500 --> 00:22:58.020
This totally makes sense
for the GPU and TPU work.

00:22:58.020 --> 00:23:00.890
But for the work done on CPU,
there's no point in waiting.

00:23:00.890 --> 00:23:05.210
All this does is add some
latency to that work.

00:23:05.210 --> 00:23:08.100
All right, so we have
some challenges here.

00:23:08.100 --> 00:23:10.310
There's the scheduling
and utilization

00:23:10.310 --> 00:23:13.885
of both the hardware
accelerator and CPU.

00:23:13.885 --> 00:23:15.760
There's the issue of
saturating one resource.

00:23:15.760 --> 00:23:16.880
So maybe you've
turned your model,

00:23:16.880 --> 00:23:18.240
it's working really great.

00:23:18.240 --> 00:23:20.890
You're using all of your CPU,
but only half of your GPU.

00:23:20.890 --> 00:23:23.290
That's not ideal,
and vice versa.

00:23:23.290 --> 00:23:24.880
I just mentioned
queuing latency.

00:23:24.880 --> 00:23:27.480
So you really don't want
CPU single threaded work

00:23:27.480 --> 00:23:29.330
to wait to be batched together.

00:23:29.330 --> 00:23:31.610
All that does is
add some latency.

00:23:31.610 --> 00:23:34.860
And the last one,
for sequence models

00:23:34.860 --> 00:23:37.350
where you have input
data of variable length,

00:23:37.350 --> 00:23:40.140
and the computation
will have variable cost,

00:23:40.140 --> 00:23:43.579
it's really challenging to batch
these pieces of work together.

00:23:43.579 --> 00:23:46.120
One of the common approaches is
to add padding to the shorter

00:23:46.120 --> 00:23:47.910
sequences so they all match.

00:23:47.910 --> 00:23:50.060
But this means that
your GPU and TPU

00:23:50.060 --> 00:23:52.570
will be doing work over
padding, which is wasteful.

00:23:52.570 --> 00:23:55.160
So there's challenging
trade offs there.

00:23:55.160 --> 00:23:56.990
So we think we know
the way forward.

00:23:56.990 --> 00:23:59.430
I'm gonna share with you some
work that we're currently

00:23:59.430 --> 00:23:59.929
doing.

00:23:59.929 --> 00:24:03.740
It's ongoing, but it's still
in the experimental phase.

00:24:03.740 --> 00:24:05.970
We are moving batching
inside the graph.

00:24:05.970 --> 00:24:07.580
And we think this
is going to be huge

00:24:07.580 --> 00:24:10.270
for throughput and performance,
in particular for challenging

00:24:10.270 --> 00:24:12.310
models.

00:24:12.310 --> 00:24:14.060
Moving batching
inside the graphs

00:24:14.060 --> 00:24:17.015
enables you to surgically
batch the specific sub

00:24:17.015 --> 00:24:19.670
graphs that benefit the most
from your custom hardware,

00:24:19.670 --> 00:24:22.650
your CPUs, and so on.

00:24:22.650 --> 00:24:25.690
For sequence models where you
have things like a while loop

00:24:25.690 --> 00:24:29.400
with a single step of code
being run inside the while loop,

00:24:29.400 --> 00:24:32.180
you can actually run
the batching only

00:24:32.180 --> 00:24:33.810
inside that while loop.

00:24:33.810 --> 00:24:34.740
So it's really neat.

00:24:34.740 --> 00:24:36.750
You're never doing batched
inference or batched

00:24:36.750 --> 00:24:39.240
computation over padding.

00:24:39.240 --> 00:24:42.580
And early results look
super promising here.

00:24:42.580 --> 00:24:46.580
Another area is for complex
models that might have,

00:24:46.580 --> 00:24:49.080
for example, a sequence
to sequence model.

00:24:49.080 --> 00:24:51.270
Maybe it has an encode
phase and a decode phase

00:24:51.270 --> 00:24:53.480
that have different costs.

00:24:53.480 --> 00:24:57.040
This would let you batch those
sub graph's separately and tune

00:24:57.040 --> 00:24:58.170
them separately.

00:25:00.750 --> 00:25:03.940
All right, next up I'm going
to share some new technology

00:25:03.940 --> 00:25:09.790
that we recently released
as part of TensorFlow 1.0.

00:25:09.790 --> 00:25:12.630
The main piece of tech
here is SavedModel.

00:25:12.630 --> 00:25:16.450
So SavedModel is the
serialization format

00:25:16.450 --> 00:25:18.010
for TensorFlow models.

00:25:18.010 --> 00:25:20.390
It's available for
very broad use.

00:25:20.390 --> 00:25:21.780
It's not specific to serving.

00:25:21.780 --> 00:25:23.130
You can use it for training.

00:25:23.130 --> 00:25:25.100
You can use it
for offline evals,

00:25:25.100 --> 00:25:27.340
and you can use it for
other interesting areas

00:25:27.340 --> 00:25:29.480
I'll get into.

00:25:29.480 --> 00:25:32.620
SavedModel has two new
features to TensorFlow.

00:25:32.620 --> 00:25:35.540
The first is that you
can in one saved model

00:25:35.540 --> 00:25:37.890
have multiple
MetaGraphs that share

00:25:37.890 --> 00:25:40.790
the same assets such as
vocabs and embeddings,

00:25:40.790 --> 00:25:44.632
as well as the same
set of trained weights.

00:25:44.632 --> 00:25:46.340
You might ask what
would I want that for,

00:25:46.340 --> 00:25:47.970
I've just been training models.

00:25:47.970 --> 00:25:51.260
Well, it turns out that
the MetaGraph for serving

00:25:51.260 --> 00:25:54.730
is very, very commonly different
than the one for training.

00:25:54.730 --> 00:25:57.820
And now let's say you want to
have a third MetaGraph that's

00:25:57.820 --> 00:25:59.880
gonna be used on GPU.

00:25:59.880 --> 00:26:02.140
That's also something you
would want to customize.

00:26:02.140 --> 00:26:03.730
So the multiple
MetaGraph support

00:26:03.730 --> 00:26:07.130
lets you have a MetaGraph
for training, one for serving

00:26:07.130 --> 00:26:09.400
on CPU, one for serving on GPU.

00:26:12.200 --> 00:26:14.840
All right, the second
major feature of SavedModel

00:26:14.840 --> 00:26:16.880
is SignatureDef.

00:26:16.880 --> 00:26:18.910
A SignatureDef
defines the signature

00:26:18.910 --> 00:26:22.320
of a computation supported
by a TensorFlow graph.

00:26:22.320 --> 00:26:24.570
You can think of it like a
function in any programming

00:26:24.570 --> 00:26:25.140
language.

00:26:25.140 --> 00:26:28.640
It has named and typed,
inputs and outputs.

00:26:28.640 --> 00:26:31.900
So as humans, we're really
good at looking at that graph,

00:26:31.900 --> 00:26:34.810
you can probably all guess what
the nodes in that graph are.

00:26:34.810 --> 00:26:36.840
I guess if we
polled us, all of us

00:26:36.840 --> 00:26:39.050
would figure out
that the middle node

00:26:39.050 --> 00:26:41.964
on the left side of this graph
is where you feed the input.

00:26:41.964 --> 00:26:43.380
But if you want
to take your model

00:26:43.380 --> 00:26:46.886
and have a whole ecosystem of
tools such as serving systems

00:26:46.886 --> 00:26:49.010
that can interpret those
graphs that they had never

00:26:49.010 --> 00:26:51.170
seen before, you
would need to annotate

00:26:51.170 --> 00:26:53.490
that graph in some way.

00:26:53.490 --> 00:26:55.617
That's exactly what
SignatureDef does.

00:26:55.617 --> 00:26:57.450
So in this case, the
middle node on the left

00:26:57.450 --> 00:26:58.900
is where you feed your inputs.

00:26:58.900 --> 00:27:01.880
The top right node
outputs a set of classes.

00:27:01.880 --> 00:27:03.760
And the bottom right
node outputs the scores.

00:27:06.440 --> 00:27:09.490
All right, so building
on top of SignatureDef

00:27:09.490 --> 00:27:11.750
is Multi-headed inference.

00:27:11.750 --> 00:27:14.900
This is also known as Multi-task
and Multi-tower inference.

00:27:14.900 --> 00:27:17.990
So another show hands, who's
familiar with Multi-headed,

00:27:17.990 --> 00:27:20.300
Multi-task inference?

00:27:20.300 --> 00:27:21.230
Oh wow, OK.

00:27:21.230 --> 00:27:22.811
So this is pretty new.

00:27:22.811 --> 00:27:25.310
There's some emerging research
and great work going on here.

00:27:25.310 --> 00:27:28.630
I think you'll be really
excited about this.

00:27:28.630 --> 00:27:31.790
So let's take that
example earlier.

00:27:31.790 --> 00:27:33.130
You're serving video rankings.

00:27:33.130 --> 00:27:34.380
Your users are using your app.

00:27:34.380 --> 00:27:35.960
You get lots of
content creators.

00:27:35.960 --> 00:27:37.280
You're successful now.

00:27:37.280 --> 00:27:39.290
And so users are drawn
to your platform,

00:27:39.290 --> 00:27:40.780
and maybe some
not so good people

00:27:40.780 --> 00:27:42.050
are drawn to your platform.

00:27:42.050 --> 00:27:43.770
Maybe they're
uploading click-bait.

00:27:43.770 --> 00:27:46.000
We've seen this in many places.

00:27:46.000 --> 00:27:47.780
So your first model,
the orange model,

00:27:47.780 --> 00:27:49.500
computes the click through rate.

00:27:49.500 --> 00:27:55.860
And these bad guys are training
models optimized to get clicks.

00:27:55.860 --> 00:27:57.960
You might decide, well,
let's define a new metric

00:27:57.960 --> 00:27:58.710
and train on that.

00:27:58.710 --> 00:28:00.310
Let's call it conversion rate.

00:28:00.310 --> 00:28:02.480
And it's going to
track users watching

00:28:02.480 --> 00:28:04.200
the majority of a video.

00:28:04.200 --> 00:28:06.370
So we'll call that CVR.

00:28:06.370 --> 00:28:09.550
And now we're gonna go
train a new model for this.

00:28:09.550 --> 00:28:12.950
But wait a second, all of the
careful future engineering

00:28:12.950 --> 00:28:14.740
and other things
that we did to get

00:28:14.740 --> 00:28:16.610
our data ready for
the first model

00:28:16.610 --> 00:28:18.045
apply to the second model.

00:28:18.045 --> 00:28:20.040
It's very, very likely,
and in many cases

00:28:20.040 --> 00:28:21.540
it's a certainty
that those features

00:28:21.540 --> 00:28:24.610
are usable in both contexts.

00:28:24.610 --> 00:28:28.160
You can actually train one
model for multiple objectives,

00:28:28.160 --> 00:28:30.800
in this case CTR and CVR.

00:28:30.800 --> 00:28:34.940
I've listed some infrastructure
benefits on this slide.

00:28:34.940 --> 00:28:37.520
And there's some really, really
big infrastructure benefits.

00:28:37.520 --> 00:28:39.735
Your inputs are only sent
once over the network,

00:28:39.735 --> 00:28:42.040
so that's already
a pretty big one.

00:28:42.040 --> 00:28:45.730
You only have to parse the
data in your model ones.

00:28:45.730 --> 00:28:48.920
You only have to compute
your hidden layers one time.

00:28:48.920 --> 00:28:52.147
So this is gonna save you
bandwidth, CPU, latency,

00:28:52.147 --> 00:28:53.730
and even ram overhead
on your servers.

00:28:53.730 --> 00:28:56.730
This is great, I could
stop right there.

00:28:56.730 --> 00:28:58.580
There's another really
important attribute

00:28:58.580 --> 00:29:01.060
of these, why
they're becoming more

00:29:01.060 --> 00:29:03.360
and more exciting in the
research that's going on.

00:29:03.360 --> 00:29:06.675
So you're likely in this example
to have many more clicks then

00:29:06.675 --> 00:29:07.770
you'll have conversions.

00:29:07.770 --> 00:29:09.270
So when you go to
train these models

00:29:09.270 --> 00:29:11.510
you have much more
label training data

00:29:11.510 --> 00:29:15.065
for your CTR objective than
for your conversion objective.

00:29:15.065 --> 00:29:16.440
So what happens
when you actually

00:29:16.440 --> 00:29:19.280
train one model for
multiple objectives?

00:29:19.280 --> 00:29:21.930
So we've actually seen some
early promising results

00:29:21.930 --> 00:29:24.080
where in one case
we were able to see

00:29:24.080 --> 00:29:26.900
a 20% improvement in
a key quality metric

00:29:26.900 --> 00:29:29.050
by moving an existing
separate model

00:29:29.050 --> 00:29:32.030
into a multi-objective model
with another objective.

00:29:32.030 --> 00:29:35.710
So there's wins here on
infrastructure and on quality.

00:29:35.710 --> 00:29:38.400
Multi-headed inference is
available in the TensorFlow

00:29:38.400 --> 00:29:39.490
estimator API.

00:29:39.490 --> 00:29:42.279
So you can train models with
multiple objectives today,

00:29:42.279 --> 00:29:43.570
and you can serve them as well.

00:29:43.570 --> 00:29:45.028
So we're really
excited about this.

00:29:47.510 --> 00:29:49.800
All right, I wanted to
show really quickly,

00:29:49.800 --> 00:29:52.270
what does a Multi-headed
inference request look like.

00:29:52.270 --> 00:29:53.760
So it's pretty simple.

00:29:53.760 --> 00:29:57.120
You can specify one or
more inference tasks.

00:29:57.120 --> 00:29:58.710
They each have the
name of a model

00:29:58.710 --> 00:30:01.360
as well as which signature
to use, so really simple.

00:30:04.730 --> 00:30:07.560
OK, all of this power
and functionality

00:30:07.560 --> 00:30:09.450
sounds really great.

00:30:09.450 --> 00:30:11.300
From early adopters
we also heard

00:30:11.300 --> 00:30:12.890
this is challenging
to get right.

00:30:12.890 --> 00:30:15.020
I might want to do things
like inspect a model

00:30:15.020 --> 00:30:17.950
and see what's actually
going on with it,

00:30:17.950 --> 00:30:21.380
run a sample query, look
at the metadata, and so on.

00:30:21.380 --> 00:30:24.470
We're introducing the saved
model command line tool,

00:30:24.470 --> 00:30:28.530
and it lets you do all
these things with a model.

00:30:28.530 --> 00:30:32.400
So the first where you can
do with a SavedModel cli.

00:30:32.400 --> 00:30:34.940
You can look at what
MetaGraphs are in the model,

00:30:34.940 --> 00:30:37.077
so pretty simple.

00:30:37.077 --> 00:30:38.660
So in this case, we
have a SavedModel.

00:30:38.660 --> 00:30:41.590
It has two MetaGraph's,
one is for serving

00:30:41.590 --> 00:30:43.200
and one is for serving on GPU.

00:30:46.090 --> 00:30:48.460
Now let's say we wanted to
look at the serving MetaGraph

00:30:48.460 --> 00:30:50.550
and see its metadata.

00:30:50.550 --> 00:30:53.410
In this case, the MetaGraph
contains two signatures.

00:30:53.410 --> 00:30:56.120
One is called classify x to y.

00:30:56.120 --> 00:30:58.230
The other one is
called serving default.

00:30:58.230 --> 00:31:00.850
So serving default is a
documented key and it's

00:31:00.850 --> 00:31:02.880
a constant in TensorFlow.

00:31:02.880 --> 00:31:05.130
And what this does is
it says, if the user

00:31:05.130 --> 00:31:07.335
has not specified
which signature to use,

00:31:07.335 --> 00:31:08.610
just use that one.

00:31:08.610 --> 00:31:10.170
And in many cases
for people getting

00:31:10.170 --> 00:31:12.349
started you have just
one signature, so it's

00:31:12.349 --> 00:31:13.390
really easy to get going.

00:31:16.170 --> 00:31:18.410
All right, now say you
wanted to actually look

00:31:18.410 --> 00:31:20.990
at the input and output
tensors in the serving default

00:31:20.990 --> 00:31:22.000
signature.

00:31:22.000 --> 00:31:24.100
So you add on one
more flag to the cli,

00:31:24.100 --> 00:31:26.310
and you'll see the inputs
and I skipped the outputs

00:31:26.310 --> 00:31:27.380
for this slide here.

00:31:27.380 --> 00:31:29.930
We can see there's one
input tensor called x,

00:31:29.930 --> 00:31:32.020
it's of type float.

00:31:32.020 --> 00:31:34.290
Note the method
name on the bottom.

00:31:34.290 --> 00:31:36.750
The method name here
TensorFlow serving predict.

00:31:36.750 --> 00:31:40.340
This is kind of like a type
in a programming language.

00:31:40.340 --> 00:31:43.020
It's informing another system,
in this case TensorFlow

00:31:43.020 --> 00:31:45.180
Serving, that this
signature was intended

00:31:45.180 --> 00:31:47.650
for use in the predict API.

00:31:47.650 --> 00:31:50.130
We have similar ones for
TensorFlow Serving regress

00:31:50.130 --> 00:31:52.840
and classify, but you
can override these.

00:31:52.840 --> 00:31:55.664
Maybe you have an in house
offline evaluation system.

00:31:55.664 --> 00:31:57.080
You can make your
own method names

00:31:57.080 --> 00:31:58.496
and check for them
in your models.

00:32:00.900 --> 00:32:03.902
All right, and lastly, let's say
you would like to run a graph.

00:32:03.902 --> 00:32:06.360
You have some input data, maybe
you're debugging the model,

00:32:06.360 --> 00:32:09.650
or you just want to
try it out for fun.

00:32:09.650 --> 00:32:11.310
I'll highlight here
all of the prams

00:32:11.310 --> 00:32:13.640
are the same, except
instead of show we've

00:32:13.640 --> 00:32:15.870
asked to run the model.

00:32:15.870 --> 00:32:18.230
We also have two ways
of expressing input

00:32:18.230 --> 00:32:19.410
that you can mix and match.

00:32:19.410 --> 00:32:21.070
So maybe you have a NumPy file.

00:32:21.070 --> 00:32:24.030
You can actually specify
the path to the NumPy file,

00:32:24.030 --> 00:32:25.520
and it will just be red.

00:32:25.520 --> 00:32:27.550
You can also specify
a Python expression,

00:32:27.550 --> 00:32:31.560
and it'll be interpreted.

00:32:31.560 --> 00:32:33.550
All right, so in
closing I wanted

00:32:33.550 --> 00:32:35.480
to say that collaboration
is very, very

00:32:35.480 --> 00:32:37.300
welcome on this project.

00:32:37.300 --> 00:32:41.520
We sink our internal repository
with Github about weekly.

00:32:41.520 --> 00:32:43.344
We have a developer
on call rotation

00:32:43.344 --> 00:32:45.260
that includes facilitating
your pool requests,

00:32:45.260 --> 00:32:47.590
answering questions, and so on.

00:32:47.590 --> 00:32:50.430
We have lots of open
ended research problems,

00:32:50.430 --> 00:32:53.650
so feedback is encouraged on
APIs, techniques, anything

00:32:53.650 --> 00:32:57.020
I've talked about, and any
challenges you have as well.

00:32:57.020 --> 00:33:01.550
You can reach us at
discuss@tensorflow.org.

00:33:01.550 --> 00:33:03.870
For some links on
how to get started,

00:33:03.870 --> 00:33:07.150
just search for TensorFlow
Serving, very easy to find.

00:33:07.150 --> 00:33:08.760
We have a great
Kubernetes tutorial.

00:33:08.760 --> 00:33:13.210
This will let you launch
an inception model server.

00:33:13.210 --> 00:33:16.860
And it's gonna run an
auto scale for you.

00:33:16.860 --> 00:33:19.562
We have Google
cloud ML, as well as

00:33:19.562 --> 00:33:20.770
I mentioned our mailing list.

00:33:20.770 --> 00:33:27.280
And you can also use our hashtag
on Stack Overflow, #tensorflow.

00:33:27.280 --> 00:33:29.130
All right, let's
go to questions.

00:33:29.130 --> 00:33:30.070
Thanks very much.

00:33:30.070 --> 00:33:32.770
[APPLAUSE]

00:33:33.670 --> 00:33:47.971
[MUSIC PLAYING]

