WEBVTT
Kind: captions
Language: en

00:00:01.210 --> 00:00:04.860
The first way we prevent over fitting
is by looking at the performance under

00:00:04.860 --> 00:00:09.590
validation set, and stopping to
train as soon as we stop improving.

00:00:09.590 --> 00:00:12.790
It's called early termination, and
it's still the best way to prevent your

00:00:12.790 --> 00:00:15.170
network from over-optimizing
on the training set.

00:00:16.420 --> 00:00:19.260
Another way is to apply regularization.

00:00:19.260 --> 00:00:23.480
Regularizing means applying artificial
constraints on your network

00:00:23.480 --> 00:00:26.700
that implicitly reduce
the number of free parameters

00:00:26.700 --> 00:00:28.700
while not making it more
difficult to optimize.

00:00:29.930 --> 00:00:33.220
In the skinny jeans analogy,
think stretch pants.

00:00:33.220 --> 00:00:34.680
They fit just as well, but

00:00:34.680 --> 00:00:38.620
because they're flexible,
they don't make things harder to fit in.

00:00:38.620 --> 00:00:42.620
The stretch pants of
are called L2 Regularization.

00:00:42.620 --> 00:00:48.200
The idea is to add another term to
the loss, which penalizes large weights.

00:00:48.200 --> 00:00:52.430
It's typically achieved by adding the L2
norm of your weights to the loss,

00:00:52.430 --> 00:00:54.620
multiplied by a small constant.

00:00:54.620 --> 00:00:57.530
And yes, yet another hyper-perimeter.

00:00:57.530 --> 00:00:58.230
Sorry about that.

