WEBVTT
Kind: captions
Language: en

00:00:00.199 --> 00:00:03.810
 in this capsule we will see 

00:00:02.070 --> 00:00:08.340
 reinforcement learning 

00:00:03.810 --> 00:00:09.690
 direct estimates so firstly 

00:00:08.340 --> 00:00:11.370
 we'll do a little reminder of 

00:00:09.690 --> 00:00:13.139
 process on marco well it's a 

00:00:11.370 --> 00:00:15.690
 concept that will come back often in 

00:00:13.139 --> 00:00:17.310
 learning does reinforcement then 

00:00:15.690 --> 00:00:19.020
 a brand decision process comes back 

00:00:17.310 --> 00:00:20.760
 it will be defined by a set 

00:00:19.020 --> 00:00:22.590
 of states are they states 

00:00:20.760 --> 00:00:24.750
 possible for our agenda 

00:00:22.590 --> 00:00:27.029
 the environment a set of actions 

00:00:24.750 --> 00:00:29.880
 that I noted actions of hesse where 

00:00:27.029 --> 00:00:31.470
 sometimes two is so that gives 

00:00:29.880 --> 00:00:33.980
 the set of possible actions for a 

00:00:31.470 --> 00:00:37.010
 agent if he is in ace state 

00:00:33.980 --> 00:00:39.690
 then have a transition model 

00:00:37.010 --> 00:00:42.300
 who tells us if the money this hole to 

00:00:39.690 --> 00:00:45.239
 help to the action stalled the 

00:00:42.300 --> 00:00:46.410
 distribution of states in which it 

00:00:45.239 --> 00:00:49.800
 could be after doing 

00:00:46.410 --> 00:00:53.010
 the action therefore has for a next state 

00:00:49.800 --> 00:00:55.050
 expresses what is its probability that this 

00:00:53.010 --> 00:00:57.239
 the new state for my agency 

00:00:55.050 --> 00:01:00.059
 was before in the state is what he has 

00:00:57.239 --> 00:01:02.100
 does the action to see it also a 

00:01:00.059 --> 00:01:03.930
 reward function that gives me the 

00:01:02.100 --> 00:01:07.260
 reward associated with being in 

00:01:03.930 --> 00:01:09.600
 the state is it finally the notion of 

00:01:07.260 --> 00:01:12.450
 plan or policy so these are 

00:01:09.600 --> 00:01:15.210
 synonyms i'm going to use them one or 

00:01:12.450 --> 00:01:17.250
 the other so interchangeably so a 

00:01:15.210 --> 00:01:19.619
 plan or policy worse if a whole 

00:01:17.250 --> 00:01:23.400
 of decisions that associates with a state 

00:01:19.619 --> 00:01:25.830
 is the action to which is taken by 

00:01:23.400 --> 00:01:32.430
 my agent who follows this policy that 

00:01:25.830 --> 00:01:36.360
 I note there is also a notion of 

00:01:32.430 --> 00:01:36.960
 value is this for a data plane pi on 

00:01:36.360 --> 00:01:39.780
 find here 

00:01:36.960 --> 00:01:42.630
 so this value is given 

00:01:39.780 --> 00:01:46.079
 as the sum of the rewards to 

00:01:42.630 --> 00:01:49.290
 from now on to infinity 

00:01:46.079 --> 00:01:51.720
 do it right so we take a nap the 

00:01:49.290 --> 00:01:54.659
 value to a state s are being the 

00:01:51.720 --> 00:01:59.579
 current reward plus a factor of this 

00:01:54.659 --> 00:02:03.390
 gamma tale times the sum of 

00:01:59.579 --> 00:02:04.860
 rewards future hope is what I 

00:02:03.390 --> 00:02:05.729
 can do it's the crimes to write it 

00:02:04.860 --> 00:02:07.710
 recurrently 

00:02:05.729 --> 00:02:11.370
 that is, I can because the 

00:02:07.710 --> 00:02:12.420
 values ​​correspond to the sum of 

00:02:11.370 --> 00:02:14.640
 rewards from 

00:02:12.420 --> 00:02:17.730
 until the end but I can say in fact 

00:02:14.640 --> 00:02:20.520
 that rewards future hope it's 

00:02:17.730 --> 00:02:22.980
 simply hope according to the 

00:02:20.520 --> 00:02:24.900
 transition probability of my 

00:02:22.980 --> 00:02:27.390
 environment for the action that I 

00:02:24.900 --> 00:02:29.900
 take but you have the hope according to 

00:02:27.390 --> 00:02:33.330
 its distribution the values ​​in 

00:02:29.900 --> 00:02:36.209
 expresses so where the probability of passing 

00:02:33.330 --> 00:02:39.050
 premium sas is used in my 

00:02:36.209 --> 00:02:43.980
 experience here and so that's the 

00:02:39.050 --> 00:02:45.690
 value for a given map so rather 

00:02:43.980 --> 00:02:49.470
 that notes is saved from an ace by 

00:02:45.690 --> 00:02:51.390
 against using v2 udder is this just for 

00:02:49.470 --> 00:02:53.819
 make explicit the fact that this is the 

00:02:51.390 --> 00:02:55.290
 value for a plan that gave and then 

00:02:53.819 --> 00:02:57.450
 for those who also look in 

00:02:55.290 --> 00:03:01.700
 books received the nordiques 

00:02:57.450 --> 00:03:05.940
 instead we study u2 notation ud 

00:03:01.700 --> 00:03:08.730
 so with one foot exposing him from 

00:03:05.940 --> 00:03:10.700
 but it's both are just two 

00:03:08.730 --> 00:03:14.730
 different ways of noting the notion of 

00:03:10.700 --> 00:03:16.260
 the value of a plan to give something 

00:03:14.730 --> 00:03:18.630
 we will add an apprenticeship 

00:03:16.260 --> 00:03:21.030
 parents necessarily is the existence of 

00:03:18.630 --> 00:03:23.070
 terminals in my environment 

00:03:21.030 --> 00:03:25.079
 if these terminals get bored of states or if 

00:03:23.070 --> 00:03:27.299
 the agent reaches the times question 

00:03:25.079 --> 00:03:28.530
 was terminal the vote simulation is 

00:03:27.299 --> 00:03:30.450
 stopped that is to say that there is no 

00:03:28.530 --> 00:03:33.660
 other states or as in power 

00:03:30.450 --> 00:03:35.250
 to achieve after that the experience of 

00:03:33.660 --> 00:03:38.549
 money in the environment is going 

00:03:35.250 --> 00:03:40.140
 just end and who will we 

00:03:38.549 --> 00:03:41.850
 to interest in fact is the sum of 

00:03:40.140 --> 00:03:44.160
 rewards but until we 

00:03:41.850 --> 00:03:45.950
 reaches a terminal state rather than the 

00:03:44.160 --> 00:03:49.380
 sum of rewards to infinity 

00:03:45.950 --> 00:03:51.900
 then to tick toe bath and to 

00:03:49.380 --> 00:03:52.769
 terminal it could be a grid of 

00:03:51.900 --> 00:03:54.660
 game over 

00:03:52.769 --> 00:03:56.370
 so once all the boxes in 

00:03:54.660 --> 00:03:58.079
 leaflets were filled more 

00:03:56.370 --> 00:04:00.209
 play spite of actions that are we 

00:03:58.079 --> 00:04:02.370
 can then take in this case this is 

00:04:00.209 --> 00:04:03.870
 the kind of state that would be who 

00:04:02.370 --> 00:04:08.000
 would correspond terminal states in 

00:04:03.870 --> 00:04:08.000
 this decision process on Wednesday 

00:04:09.850 --> 00:04:16.900
 so let's start by defining liguria 

00:04:14.640 --> 00:04:17.800
 passive reinforcement learning 

00:04:16.900 --> 00:04:21.190
 and then from its edge to its destination 

00:04:17.800 --> 00:04:23.260
 direct which is an example 1 4 and 5 of 

00:04:21.190 --> 00:04:24.600
 Steam flu and learning 

00:04:23.260 --> 00:04:26.890
 will necessarily speak easy 

00:04:24.600 --> 00:04:29.410
 then very strong advantage even 

00:04:26.890 --> 00:04:31.870
 passive that's it's going to solve the 

00:04:29.410 --> 00:04:35.560
 next problem given a plan worse 

00:04:31.870 --> 00:04:38.530
 given to take the value function 

00:04:35.560 --> 00:04:41.890
 so the value of the plan worse without 

00:04:38.530 --> 00:04:44.770
 know the transition model pds 

00:04:41.890 --> 00:04:46.240
 get your chance and so you're 

00:04:44.770 --> 00:04:47.770
 learning by necessarily at each 

00:04:46.240 --> 00:04:49.180
 time we can not assume we know 

00:04:47.770 --> 00:04:51.160
 the transition model 

00:04:49.180 --> 00:04:53.500
 learning parents inevitably passive 

00:04:51.160 --> 00:04:56.290
 it's a body where we know grandpa 

00:04:53.500 --> 00:04:58.480
 of mind thus knowing to but we are 

00:04:56.290 --> 00:05:00.130
 give a specific plan to follow and one 

00:04:58.480 --> 00:05:04.060
 just want to know what's the value 

00:05:00.130 --> 00:05:06.010
 exploit to illustrate giving 

00:05:04.060 --> 00:05:07.210
 examples of learning without 

00:05:06.010 --> 00:05:09.220
 necessarily pass the fund will use 

00:05:07.210 --> 00:05:10.960
 this example tic is taken from the book of 

00:05:09.220 --> 00:05:14.710
 russell and so would maginot we have 

00:05:10.960 --> 00:05:18.010
 a grid 3 by four and in this 

00:05:14.710 --> 00:05:21.580
 grid there in no we do not already specify a 

00:05:18.010 --> 00:05:24.250
 plan that followed in the grid this there the 

00:05:21.580 --> 00:05:26.860
 plan corresponds to the direction towards 

00:05:24.250 --> 00:05:28.630
 which was aja would move in 

00:05:26.860 --> 00:05:30.790
 following this plan for example here 

00:05:28.630 --> 00:05:33.250
 the agent is moving towards 

00:05:30.790 --> 00:05:35.770
 the top here agenzia by fee of 

00:05:33.250 --> 00:05:37.380
 move by the left institute see 

00:05:35.770 --> 00:05:41.200
 also a reinforcement function 

00:05:37.380 --> 00:05:43.840
 which are going to be minus 0.04 by all 

00:05:41.200 --> 00:05:46.960
 except at the terminal states that are here 

00:05:43.840 --> 00:05:50.050
 so these are times if the agent 

00:05:46.960 --> 00:05:51.700
 reached this state there simulation stops 

00:05:50.050 --> 00:05:52.810
 experience the agent to the environment 

00:05:51.700 --> 00:05:54.970
 16 

00:05:52.810 --> 00:05:56.650
 and one can from their calculations 

00:05:54.970 --> 00:05:59.350
 and the sum of the reinforcements and that that 

00:05:56.650 --> 00:06:00.700
 is going to be actually the cumulative buildup 

00:05:59.350 --> 00:06:03.190
 than 

00:06:00.700 --> 00:06:06.160
 the agent got in his experience 

00:06:03.190 --> 00:06:07.630
 in this environment and read and have 

00:06:06.160 --> 00:06:09.070
 completed also a value of 

00:06:07.630 --> 00:06:11.470
 reinforcement in this case it's going to be 

00:06:09.070 --> 00:06:14.070
 more and so the agent will be like that and 

00:06:11.470 --> 00:06:16.300
 means if the agent would it 

00:06:14.070 --> 00:06:19.470
 the environment and stochastic so 

00:06:16.300 --> 00:06:21.910
 in that if the agent wishes to pass 

00:06:19.470 --> 00:06:23.200
 by then jean pardon is in this 

00:06:21.910 --> 00:06:25.240
 state if and who wish to take 

00:06:23.200 --> 00:06:27.900
 the action go up and a 

00:06:25.240 --> 00:06:30.370
 certain probability that is going to be the most 

00:06:27.900 --> 00:06:31.660
 so the most likely thing that 

00:06:30.370 --> 00:06:33.610
 will happen is that the agent is going to 

00:06:31.660 --> 00:06:35.440
 move to that state though but some 

00:06:33.610 --> 00:06:37.720
 probability also which is to 

00:06:35.440 --> 00:06:39.750
 seven folders instead so it's a 

00:06:37.720 --> 00:06:41.650
 stochastic environment or the 

00:06:39.750 --> 00:06:45.430
 result of my actions and not 

00:06:41.650 --> 00:06:47.290
 fully determined and so this 

00:06:45.430 --> 00:06:48.970
 people stop at the finished states on 

00:06:47.290 --> 00:06:52.030
 terminals and we will use a 

00:06:48.970 --> 00:06:54.880
 constant seconds that will be equal 

00:06:52.030 --> 00:06:57.150
 so in the decision process 

00:06:54.880 --> 00:07:00.340
 mark ovi we saw previously 

00:06:57.150 --> 00:07:01.050
 when we talked about validation of their 

00:07:00.340 --> 00:07:04.570
 studies 

00:07:01.050 --> 00:07:05.680
 it's important to have a value of 

00:07:04.570 --> 00:07:07.510
 factors of this bridge that was 

00:07:05.680 --> 00:07:09.970
 strictly smaller than one otherwise 

00:07:07.510 --> 00:07:11.590
 had problems because in the 

00:07:09.970 --> 00:07:13.930
 definition of decision processes 

00:07:11.590 --> 00:07:15.900
 easy to mac os well before we 

00:07:13.930 --> 00:07:18.460
 did not have both terminals what what 

00:07:15.900 --> 00:07:20.590
 with chords of smaller seconds 

00:07:18.460 --> 00:07:22.810
 because we could culin 

00:07:20.590 --> 00:07:24.460
 reward a movie when also because 

00:07:22.810 --> 00:07:26.110
 we have completed stages we can 

00:07:24.460 --> 00:07:28.810
 allow to have a factor of this tale 

00:07:26.110 --> 00:07:32.560
 which is equal to 1 so it turns out that 

00:07:28.810 --> 00:07:34.720
 in this case if the notion of value 

00:07:32.560 --> 00:07:37.810
 of a plan so the sum of rewards 

00:07:34.720 --> 00:07:40.900
 hope is going to be something that goes that 

00:07:37.810 --> 00:07:41.710
 should normally converge so that's 

00:07:40.900 --> 00:07:46.320
 something we can afford 

00:07:41.710 --> 00:07:46.320
 here for this dish 

00:07:48.110 --> 00:07:53.260
 so because we do not know our 

00:07:51.620 --> 00:07:55.130
 transition model 

00:07:53.260 --> 00:07:56.840
 everything we can do and 

00:07:55.130 --> 00:07:58.400
 learn from simulations 

00:07:56.840 --> 00:08:01.520
 these simulations the man call them some 

00:07:58.400 --> 00:08:04.850
 English tests giles so that's it 

00:08:01.520 --> 00:08:07.040
 is the money that starts at a 

00:08:04.850 --> 00:08:09.080
 given state because if we are going to assume 

00:08:07.040 --> 00:08:09.890
 that the initial state is always the 

00:08:09.080 --> 00:08:12.850
 box 1 1 

00:08:09.890 --> 00:08:17.240
 and in that if we have an example of 

00:08:12.850 --> 00:08:19.640
 3.30 from three trials where money in 

00:08:17.240 --> 00:08:22.850
 following the actions specified by our 

00:08:19.640 --> 00:08:25.490
 policy and started at the state 1.1 to 

00:08:22.850 --> 00:08:28.580
 that time got a reward from 

00:08:25.490 --> 00:08:30.740
 - 0 point 0 4 and then of this in 

00:08:28.580 --> 00:08:33.080
 following the action of the policy in 

00:08:30.740 --> 00:08:37.010
 question this one conducted to test himself 

00:08:33.080 --> 00:08:40.460
 the at the state 1 2 which in reward 

00:08:37.010 --> 00:08:42.620
 partner of minus 0.04 to those already at 

00:08:40.460 --> 00:08:44.830
 the state 1 2 of tracking the action given by 

00:08:42.620 --> 00:08:49.130
 politics and that brought him to 

00:08:44.830 --> 00:08:52.130
 the hall and gold 1 3 who has a reward 

00:08:49.130 --> 00:08:54.050
 from - 0 point 0 k so on until 

00:08:52.130 --> 00:08:57.680
 what he eventually reaches a state 

00:08:54.050 --> 00:09:02.120
 final a 4-point terminal state and captured 

00:08:57.680 --> 00:09:03.620
 3 the fund 4.3 is one of the states of 

00:09:02.120 --> 00:09:06.620
 terminale seller terminal associate 

00:09:03.620 --> 00:09:08.270
 an extra reward in mexico at 

00:09:06.620 --> 00:09:11.840
 third try we had a simulation 

00:09:08.270 --> 00:09:14.230
 who led us to a different state for 

00:09:11.840 --> 00:09:16.550
 the second 

00:09:14.230 --> 00:09:19.250
 after taking the first action so 

00:09:16.550 --> 00:09:20.870
 from 1.1 to myself taking the same action as 

00:09:19.250 --> 00:09:22.550
 the action that was taken here because we 

00:09:20.870 --> 00:09:23.660
 always follows the same politics we 

00:09:22.550 --> 00:09:26.030
 has been given 

00:09:23.660 --> 00:09:27.130
 the agent ended up in the state earlier 

00:09:26.030 --> 00:09:29.660
 2 1 

00:09:27.130 --> 00:09:30.860
 because it's an environment that 

00:09:29.660 --> 00:09:32.930
 stochastic so there was a 

00:09:30.860 --> 00:09:36.440
 protein which he two years after the 

00:09:32.930 --> 00:09:39.500
 same action of happened to state two 

00:09:36.440 --> 00:09:41.870
 hours earlier than the condition 1,2 and dense 

00:09:39.500 --> 00:09:48.890
 that if the simulation bring him rather to 

00:09:41.870 --> 00:09:50.210
 the terminal state 4.2 in box 4 2 and that 

00:09:48.890 --> 00:09:52.600
 that gave him in this case also for 

00:09:50.210 --> 00:09:55.480
 this one 

00:09:52.600 --> 00:09:56.709
 reinforcement of -1 and there the problem of 

00:09:55.480 --> 00:10:00.730
 learning parents inevitably 

00:09:56.709 --> 00:10:02.259
 pacifies 6 and esteem of them to have a 

00:10:00.730 --> 00:10:05.560
 estimate of value 

00:10:02.259 --> 00:10:10.569
 in any case is this for this policy 

00:10:05.560 --> 00:10:13.410
 designed it first approach we're going 

00:10:10.569 --> 00:10:16.120
 see it's the one of direct estimation 

00:10:13.410 --> 00:10:17.769
 and senna pages that is super simple everything 

00:10:16.120 --> 00:10:19.690
 what are we going to do his calculations and the 

00:10:17.769 --> 00:10:23.490
 average of what is observed in the 

00:10:19.690 --> 00:10:27.940
 tests to calculate estimate value 

00:10:23.490 --> 00:10:31.269
 vds for the designed plan so I repeat if 

00:10:27.940 --> 00:10:33.130
 the tests so the three simulations of 

00:10:31.269 --> 00:10:35.709
 our agent who followed the policy 

00:10:33.130 --> 00:10:38.709
 then given when you want city but 

00:10:35.709 --> 00:10:41.190
 what would be the value for our 

00:10:38.709 --> 00:10:43.360
 policy followed in state 1.1 

00:10:41.190 --> 00:10:48.250
 we observe, among other things, that 

00:10:43.360 --> 00:10:50.579
 a hockey who visited state 1.1 but 

00:10:48.250 --> 00:10:54.630
 the sum of the rewards from 1 1 

00:10:50.579 --> 00:11:00.100
 to a terminal state it was less 

00:10:54.630 --> 00:11:02.500
 0.04 plus points - point 04 - points 0.4 

00:11:00.100 --> 00:11:06.189
 at 1.048 months after 0.4 

00:11:02.500 --> 00:11:07.509
 the smo 1.04 let me 1.04 +1 function 

00:11:06.189 --> 00:11:11.319
 to sum up all that in fact we have 

00:11:07.509 --> 00:11:13.110
 a reward of 0.72 so what we 

00:11:11.319 --> 00:11:17.649
 observe is that to leave one 

00:11:13.110 --> 00:11:19.589
 the sum of future rewards was to 

00:11:17.649 --> 00:11:21.970
 from state 1.1 

00:11:19.589 --> 00:11:26.529
 in fact the sum of the rewards to a 

00:11:21.970 --> 00:11:29.800
 the future rewards was 0.72 

00:11:26.529 --> 00:11:33.220
 in the sky two from 1 1 

00:11:29.800 --> 00:11:34.480
 we observe again 0.62 are there 

00:11:33.220 --> 00:11:37.449
 so many states with a reward 

00:11:34.480 --> 00:11:39.399
 negative of - 0 point 0 4 then 

00:11:37.449 --> 00:11:40.689
 finally the same and terminal with a 

00:11:39.399 --> 00:11:43.269
 reward no longer just that we have the 

00:11:40.689 --> 00:11:46.630
 same reward sum of rewards 

00:11:43.269 --> 00:11:48.370
 up to a terminal of 0.72 on trial 

00:11:46.630 --> 00:11:50.889
 three years something different 

00:11:48.370 --> 00:11:52.269
 we have a test that has been a little less 

00:11:50.889 --> 00:11:54.009
 long then finally we had a 

00:11:52.269 --> 00:11:58.329
 negative reward at the end of which 

00:11:54.009 --> 00:12:02.350
 gives a sum of the rewards of -1 

00:11:58.329 --> 00:12:04.569
 point and direct estimate 10 10 

00:12:02.350 --> 00:12:05.930
 to estimate the value 

00:12:04.569 --> 00:12:07.399
 go outside 

00:12:05.930 --> 00:12:10.120
 just average what has 

00:12:07.399 --> 00:12:14.330
 was observed stasi the average of 0.72 

00:12:10.120 --> 00:12:21.290
 0.72 - 1.16 and when if that gives us 

00:12:14.330 --> 00:12:25.010
 ride 0.10 0933 3 another example of 

00:12:21.290 --> 00:12:28.730
 the direct estimate when it's for 

00:12:25.010 --> 00:12:31.970
 state 1 2 part is observed in the test 

00:12:28.730 --> 00:12:35.149
 1 that the condition 1,2 visited twice and 

00:12:31.970 --> 00:12:39.380
 so the first time here is the 

00:12:35.149 --> 00:12:40.399
 second time it's here then for the 

00:12:39.380 --> 00:12:43.130
 first time they visit and the 

00:12:40.399 --> 00:12:46.490
 sum of the rewards of this state there 

00:12:43.130 --> 00:12:49.100
 until the end from this pass to here 

00:12:46.490 --> 00:12:53.779
 it's going to be a two three four five 

00:12:49.100 --> 00:12:59.209
 six times less points 0.4 +1 which we 

00:12:53.779 --> 00:13:02.209
 gives a total sum of 0.65 for 

00:12:59.209 --> 00:13:04.550
 stick here we have one two three four 

00:13:02.209 --> 00:13:06.440
 times less points 0.4 of rewards 

00:13:04.550 --> 00:13:12.140
 plus + 1 which would give confidence 

00:13:06.440 --> 00:13:14.690
 total slightly higher by 0.84 in 

00:13:12.140 --> 00:13:17.089
 the east it's hard we visit the state 1,2 here 

00:13:14.690 --> 00:13:20.450
 and as in the series we get a 

00:13:17.089 --> 00:13:23.209
 sum of the rewards of 0.76 then the c3 

00:13:20.450 --> 00:13:24.709
 do not visit state 1 2 so in that 

00:13:23.209 --> 00:13:30.080
 if we do not learn anything from the 

00:13:24.709 --> 00:13:32.420
 value at 1.2 for the 6 3 so in this 

00:13:30.080 --> 00:13:33.770
 that if the direct estimate what that 

00:13:32.420 --> 00:13:42.650
 would give us so simply the average 

00:13:33.770 --> 00:13:47.959
 of 0.76 0.84 and 0.76 which gives us 

00:13:42.650 --> 00:13:51.250
 an estimated value of 0.78 is therefore 

00:13:47.959 --> 00:13:54.680
 it's simply the procedure for 

00:13:51.250 --> 00:13:56.510
 get a direct estimate of the 

00:13:54.680 --> 00:13:58.130
 value of a body of water is so it's 

00:13:56.510 --> 00:14:01.180
 parenting precision 

00:13:58.130 --> 00:14:04.180
 necessarily passive based on the estimate 

00:14:01.180 --> 00:14:04.180
 direct 

