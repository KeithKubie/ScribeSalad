WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.820
[MUSIC PLAYING]

00:00:06.120 --> 00:00:10.320
TIM O'REILLY: I assume since
you're here that most of you

00:00:10.320 --> 00:00:11.640
know who I am.

00:00:11.640 --> 00:00:14.520
But just for a little
bit of context setting,

00:00:14.520 --> 00:00:18.090
I am going to talk a little
bit about the background

00:00:18.090 --> 00:00:20.910
of my company and
why that's relevant.

00:00:20.910 --> 00:00:22.680
Because the book
that I've written

00:00:22.680 --> 00:00:25.950
is a combination of a memoir
about my time in the technology

00:00:25.950 --> 00:00:30.840
industry, a business book, and
an economic call to action.

00:00:30.840 --> 00:00:35.070
On the economics, Hal
was my thesis advisor.

00:00:35.070 --> 00:00:40.140
The book was-- it was really--

00:00:40.140 --> 00:00:42.097
I don't really know that
much about economics.

00:00:42.097 --> 00:00:43.930
Although a couple of
people have said, well,

00:00:43.930 --> 00:00:45.820
but your instincts are good.

00:00:45.820 --> 00:00:49.080
But Hal corrected my
instincts in many, many cases.

00:00:49.080 --> 00:00:51.420
But any errors in the
economics portions

00:00:51.420 --> 00:00:53.710
are not Hal's responsibility.

00:00:53.710 --> 00:00:56.010
But he definitely
would read a passage,

00:00:56.010 --> 00:00:57.350
he read multiple times.

00:00:57.350 --> 00:00:58.930
He would say, no, you have
to go read this paper.

00:00:58.930 --> 00:01:00.420
Oh, you have to go
read this paper.

00:01:00.420 --> 00:01:01.878
Oh, you have to go
read this paper.

00:01:01.878 --> 00:01:04.620
It was a wonderful
education, and I

00:01:04.620 --> 00:01:07.410
hope to continue it as I
go further down this path.

00:01:07.410 --> 00:01:11.400
Anyway, so I'm the founder
and CEO of O'Reilly Media.

00:01:11.400 --> 00:01:16.650
Most of you probably,
maybe, first knew of us

00:01:16.650 --> 00:01:18.810
as a publisher of the
iconic animal books.

00:01:18.810 --> 00:01:20.630
This book now--

00:01:20.630 --> 00:01:23.470
I think it's still in its
sixth edition, [INAUDIBLE]..

00:01:23.470 --> 00:01:24.980
The first edition was in 1985.

00:01:24.980 --> 00:01:27.930
It was one of the first
books I ever published.

00:01:27.930 --> 00:01:30.750
I think it was in an
edition of 100 copies.

00:01:30.750 --> 00:01:33.270
It's gone on to sell, probably,
a million copies since then.

00:01:33.270 --> 00:01:36.690
It's still in print
how many years later,

00:01:36.690 --> 00:01:39.190
about 32 years later.

00:01:39.190 --> 00:01:41.550
But we continue
to be a publisher.

00:01:41.550 --> 00:01:43.920
Here's a book that we did
recently with Google on site

00:01:43.920 --> 00:01:46.290
reliability engineering,
another book

00:01:46.290 --> 00:01:49.440
on hands on machine
learning with Scikit-Learn

00:01:49.440 --> 00:01:50.760
and TensorFlow.

00:01:50.760 --> 00:01:54.780
These are the kind of thing that
a lot of people in the computer

00:01:54.780 --> 00:01:56.700
industry learned of us from.

00:01:56.700 --> 00:02:00.090
It's now only about
20% of our business.

00:02:00.090 --> 00:02:02.850
But the thing I'm kind
of the proudest of

00:02:02.850 --> 00:02:07.320
is the role that I've had in
spreading big ideas about where

00:02:07.320 --> 00:02:10.919
the industry is going
and what it ought to do.

00:02:10.919 --> 00:02:13.320
So very early in
the '90s, my company

00:02:13.320 --> 00:02:15.626
created the first
commercial website,

00:02:15.626 --> 00:02:17.500
the site called the
Global Network Navigator.

00:02:17.500 --> 00:02:20.020
It was sort of before Yahoo
was the first web portal.

00:02:20.020 --> 00:02:23.510
It was also the first
advertising supported site

00:02:23.510 --> 00:02:24.930
on the World Wide Web.

00:02:24.930 --> 00:02:26.690
The internet was
still noncommercial.

00:02:26.690 --> 00:02:32.340
And I did a lot of activism
for the commercial internet

00:02:32.340 --> 00:02:35.230
and also for the open
commercial internet.

00:02:35.230 --> 00:02:36.949
In the very early
days, for those of you

00:02:36.949 --> 00:02:38.490
who weren't around
then, the internet

00:02:38.490 --> 00:02:39.600
was a research network.

00:02:39.600 --> 00:02:41.100
And I still remember
a conversation

00:02:41.100 --> 00:02:42.990
I had with Steve
Wolf, who at the time

00:02:42.990 --> 00:02:46.260
was the NSF overseer, National
Science Foundation overseer,

00:02:46.260 --> 00:02:47.850
of the internet.

00:02:47.850 --> 00:02:51.110
And I said, well, here's
what we're planning on doing.

00:02:51.110 --> 00:02:53.446
We're going to build this
website and it will have ads.

00:02:53.446 --> 00:02:55.320
But the thing that is
different about the web

00:02:55.320 --> 00:02:56.460
is that people come to you.

00:02:56.460 --> 00:02:58.140
So we're not going to
be sending out anything

00:02:58.140 --> 00:02:59.320
unless people ask for it.

00:02:59.320 --> 00:03:00.850
So it's very different.

00:03:00.850 --> 00:03:03.510
And he said, well,
you know, the internet

00:03:03.510 --> 00:03:05.200
is about research and education.

00:03:05.200 --> 00:03:08.277
And if you guys aren't
research and education,

00:03:08.277 --> 00:03:09.110
I don't know who is.

00:03:09.110 --> 00:03:10.230
So go for it.

00:03:10.230 --> 00:03:11.500
And it was a wonderful moment.

00:03:11.500 --> 00:03:14.160
Anyway, the later I
organized a meeting

00:03:14.160 --> 00:03:17.310
where the term open source
software was widely adopted

00:03:17.310 --> 00:03:18.882
and promoted it.

00:03:18.882 --> 00:03:20.590
And particularly, told
the story about it

00:03:20.590 --> 00:03:23.910
that was bigger than
the political movement

00:03:23.910 --> 00:03:26.520
about free software and
being against Microsoft,

00:03:26.520 --> 00:03:29.220
by bringing in the story of
why the internet was also built

00:03:29.220 --> 00:03:31.100
on top of open source software.

00:03:31.100 --> 00:03:33.570
And that was probably
my first experience

00:03:33.570 --> 00:03:38.130
of the power of ideas to
change people's minds.

00:03:38.130 --> 00:03:39.990
You know, I remember
when I first--

00:03:39.990 --> 00:03:41.580
you know, I held
this press conference

00:03:41.580 --> 00:03:44.121
at the end of this day that came
to be called the Open Source

00:03:44.121 --> 00:03:44.760
Summit.

00:03:44.760 --> 00:03:47.760
And I had all these guys
up on a stage, and people

00:03:47.760 --> 00:03:51.770
who nobody had ever heard
of, most of them, you know.

00:03:51.770 --> 00:03:54.780
And the story I told
was so different--

00:03:54.780 --> 00:03:57.240
which had been this
story of free software

00:03:57.240 --> 00:03:59.700
is this rebel movement
that wants to bring down

00:03:59.700 --> 00:04:00.690
commercial software.

00:04:00.690 --> 00:04:02.250
Commercial software is evil.

00:04:02.250 --> 00:04:07.305
And I said, hey, if you
guys are on the internet

00:04:07.305 --> 00:04:09.930
and you have a domain name-- you
from "The New York Times," you

00:04:09.930 --> 00:04:12.630
from "The Wall Street
Journal," or whatever,

00:04:12.630 --> 00:04:15.390
this guy over here, Paul
Vixie wrote the software

00:04:15.390 --> 00:04:20.160
and gave it away so that that
domain name can be recognized.

00:04:20.160 --> 00:04:22.200
Oh, if you send email,
this guy, Eric Allman,

00:04:22.200 --> 00:04:25.170
wrote Sendmail, the program
that routes, at that point,

00:04:25.170 --> 00:04:27.090
about 70% of all the
email on the internet.

00:04:27.090 --> 00:04:29.089
Oh, if you have a website,
it's probably Apache.

00:04:29.089 --> 00:04:31.710
This guy Brian
Behlendorf started that.

00:04:31.710 --> 00:04:33.960
And it was interesting,
I did about two weeks

00:04:33.960 --> 00:04:34.890
worth of interviews.

00:04:34.890 --> 00:04:36.720
And in the beginning, what?

00:04:36.720 --> 00:04:39.580
The internet is based
on free software?

00:04:39.580 --> 00:04:41.431
It was this disbelief.

00:04:41.431 --> 00:04:42.930
And it felt a little
bit like you're

00:04:42.930 --> 00:04:46.420
trying to push something really
heavy, and it doesn't move.

00:04:46.420 --> 00:04:47.730
And then it starts to move.

00:04:47.730 --> 00:04:50.910
And within two weeks, it was
just the accepted wisdom.

00:04:50.910 --> 00:04:54.129
And that, in a
way, is a backdrop

00:04:54.129 --> 00:04:55.170
to the story of the book.

00:04:55.170 --> 00:04:58.410
Because I'm trying to change
the accepted wisdom again

00:04:58.410 --> 00:04:59.530
with this book.

00:04:59.530 --> 00:05:02.760
So anyway, I did it again
with Web 2.0, which is really

00:05:02.760 --> 00:05:07.440
the story, for me, of what came
after the dot.com bust, what

00:05:07.440 --> 00:05:09.155
was the Second
Coming of the web.

00:05:09.155 --> 00:05:10.530
And I had come to
the conclusion,

00:05:10.530 --> 00:05:13.620
from thinking about open
source in a very different way

00:05:13.620 --> 00:05:17.870
than other people
had, that there

00:05:17.870 --> 00:05:20.400
was something very different
about the companies that

00:05:20.400 --> 00:05:21.600
survived.

00:05:21.600 --> 00:05:23.340
And of course, this
led me down the path

00:05:23.340 --> 00:05:26.640
that we're really
moving out of the world

00:05:26.640 --> 00:05:30.300
that we knew in the PC era,
where software was an artifact,

00:05:30.300 --> 00:05:31.800
to where software
was increasingly

00:05:31.800 --> 00:05:35.850
a business process with these
vast cloud applications that

00:05:35.850 --> 00:05:38.220
actually had people
inside of them.

00:05:38.220 --> 00:05:39.930
And that data was
going to be the source

00:05:39.930 --> 00:05:41.080
of competitive advantage.

00:05:41.080 --> 00:05:43.470
And that was the heart of
what I talked about Web 2.0.

00:05:43.470 --> 00:05:45.636
We also launched something
called the Maker Movement

00:05:45.636 --> 00:05:50.100
with a magazine in 2004 called
"Make" and "Maker Faire."

00:05:50.100 --> 00:05:52.740
I spent a lot of time in
the last seven or eight

00:05:52.740 --> 00:05:54.480
years talking about
how government also

00:05:54.480 --> 00:05:57.490
needs to learn about platforms
from the technology industry.

00:05:57.490 --> 00:06:00.210
And most recently I've been
talking a lot in the same way

00:06:00.210 --> 00:06:02.940
about how to think
differently about AI

00:06:02.940 --> 00:06:05.730
and what I call the
next economy, technology

00:06:05.730 --> 00:06:07.290
and the future of work.

00:06:07.290 --> 00:06:11.040
So a big part of our business
today are conferences.

00:06:11.040 --> 00:06:13.500
We just had last week the
O'Reilly AI conference.

00:06:13.500 --> 00:06:16.470
This week in New York, this
Strate, The Business of Data.

00:06:16.470 --> 00:06:18.950
We're doing a
conference on Jupiter.

00:06:18.950 --> 00:06:20.460
Our original, very
first conference

00:06:20.460 --> 00:06:23.600
started in '97, the O'Reilly
Open Source Software Summit.

00:06:23.600 --> 00:06:25.245
We run Maker Faire.

00:06:25.245 --> 00:06:27.370
Something called Food Camp,
which Hal talked about,

00:06:27.370 --> 00:06:29.280
which is an unconference.

00:06:29.280 --> 00:06:31.620
And then, we have a
platform of our own,

00:06:31.620 --> 00:06:33.560
which we started in 2000.

00:06:33.560 --> 00:06:34.970
It's called Safari.

00:06:34.970 --> 00:06:36.570
It was originally
an ebook platform,

00:06:36.570 --> 00:06:38.950
but it's increasingly
a platform,

00:06:38.950 --> 00:06:41.910
yes, 40,000 plus e-books,
tens of thousands

00:06:41.910 --> 00:06:44.790
of hours of video training, live
training millions of customers.

00:06:44.790 --> 00:06:47.100
And it's really a platform
for knowledge exchange.

00:06:47.100 --> 00:06:48.766
That's really the
heart of our business.

00:06:48.766 --> 00:06:52.300
We're about 20% books,
30% events, and 50%

00:06:52.300 --> 00:06:55.440
this online learning platform.

00:06:55.440 --> 00:06:57.910
So I spend a lot of time
thinking about platforms

00:06:57.910 --> 00:06:59.910
because that really is
the heart of my business.

00:06:59.910 --> 00:07:02.460
And I think that
Google also, obviously,

00:07:02.460 --> 00:07:04.210
is a platform company.

00:07:04.210 --> 00:07:05.730
But I want to spend
a lot of time--

00:07:05.730 --> 00:07:07.479
I spent a lot of time
in the book thinking

00:07:07.479 --> 00:07:12.620
about digital platforms and what
they teach us about economies.

00:07:12.620 --> 00:07:14.160
So I'm going to
come back to that.

00:07:14.160 --> 00:07:17.390
So the title of
the book, "WTF"--

00:07:17.390 --> 00:07:20.360
you know, now I actually
used this at the White House

00:07:20.360 --> 00:07:21.320
Frontiers Conference.

00:07:21.320 --> 00:07:23.611
And I was really proud that
I got the White House comms

00:07:23.611 --> 00:07:27.410
team to sign off on
a talk called "WTF."

00:07:27.410 --> 00:07:29.380
And I did it by kind
of saying, well,

00:07:29.380 --> 00:07:31.280
it stands for what's
the future, of course.

00:07:31.280 --> 00:07:34.700
But really, the reason why
I wanted to use the term WTF

00:07:34.700 --> 00:07:38.360
is because it's a term
of astonishment that

00:07:38.360 --> 00:07:40.820
can be the
astonishment of delight

00:07:40.820 --> 00:07:43.400
or the astonishment of dismay.

00:07:43.400 --> 00:07:46.310
And I think that
really encapsulates

00:07:46.310 --> 00:07:51.430
the state of our dialogue
about technology today.

00:07:51.430 --> 00:07:55.610
It's the source of enormous
astonishment and wonder,

00:07:55.610 --> 00:07:58.310
and it is also a source of fear.

00:07:58.310 --> 00:08:02.822
And I started worrying
about this probably

00:08:02.822 --> 00:08:03.780
two or three years ago.

00:08:03.780 --> 00:08:06.020
I started an event
called The Next Economy

00:08:06.020 --> 00:08:08.750
Summit because I was trying
to get ahead of this issue.

00:08:08.750 --> 00:08:11.610
How do we think about
technology and the economy?

00:08:11.610 --> 00:08:15.020
How do we get technologists
to think about it

00:08:15.020 --> 00:08:16.700
and to talk about
it in a way that

00:08:16.700 --> 00:08:18.470
doesn't make people afraid?

00:08:18.470 --> 00:08:20.840
How do we get
business plans that

00:08:20.840 --> 00:08:24.590
are focused on empowering
people and building wealth

00:08:24.590 --> 00:08:26.450
for everyone rather
than simply, well, we're

00:08:26.450 --> 00:08:28.030
just going to disrupt?

00:08:28.030 --> 00:08:29.790
We're going to break things.

00:08:29.790 --> 00:08:32.364
You know we're going to make
ourselves really, really rich,

00:08:32.364 --> 00:08:34.280
and we're not going to
really worry about what

00:08:34.280 --> 00:08:35.480
happens to everybody else.

00:08:35.480 --> 00:08:36.980
And that's how the
rest of the world

00:08:36.980 --> 00:08:38.409
is starting to see
Silicon Valley.

00:08:38.409 --> 00:08:41.929
And in a lot of ways,
the book is an attempt

00:08:41.929 --> 00:08:45.770
to address that narrative
head on and to change it.

00:08:45.770 --> 00:08:51.249
And to change it by actually
inspiring software developers

00:08:51.249 --> 00:08:52.790
and entrepreneurs
to act differently,

00:08:52.790 --> 00:08:54.020
to talk differently.

00:08:54.020 --> 00:08:56.090
But also to persuade
policymakers

00:08:56.090 --> 00:08:57.587
to think and act differently.

00:08:57.587 --> 00:08:59.170
So I'm going to try
to give you a few,

00:08:59.170 --> 00:09:01.700
a sampling of some of
the ideas from the book.

00:09:01.700 --> 00:09:05.390
By the way, back to this
WTF of amazement or dismay,

00:09:05.390 --> 00:09:09.160
this is the world of techno
optimism that we all live in.

00:09:09.160 --> 00:09:13.450
That's a chart of
life expectancy.

00:09:13.450 --> 00:09:15.650
You know, life
expectancy at birth,

00:09:15.650 --> 00:09:19.310
pretty much flat, I mean,
there's a few really bad times.

00:09:19.310 --> 00:09:21.600
You can kind of see
where it really dropped.

00:09:21.600 --> 00:09:23.480
But then suddenly,
this magical thing

00:09:23.480 --> 00:09:27.040
happened in the mid 1800s,
when it started to climb.

00:09:27.040 --> 00:09:29.240
And then you see additional
countries sort of come

00:09:29.240 --> 00:09:30.350
on stream.

00:09:30.350 --> 00:09:33.110
This is the modern world
that we have every reason

00:09:33.110 --> 00:09:36.440
to be so proud of.

00:09:36.440 --> 00:09:38.540
There's a wonderful
site, Our World In Data,

00:09:38.540 --> 00:09:42.860
which is this incredible
collection of graphs

00:09:42.860 --> 00:09:44.900
and narratives about
the way that technology

00:09:44.900 --> 00:09:46.135
is making the world better.

00:09:46.135 --> 00:09:47.510
So I'm standing
in front of this.

00:09:47.510 --> 00:09:49.880
Sorry about that, guys.

00:09:49.880 --> 00:09:51.330
So I guess I'll
be back over here.

00:09:54.270 --> 00:09:57.140
But everyone is
not equally happy.

00:09:57.140 --> 00:09:59.330
We see Brexit,
the rise of Trump.

00:09:59.330 --> 00:10:02.420
And here's this WTF, you know,
on the "Daily Telegraph,"

00:10:02.420 --> 00:10:02.920
you know?

00:10:05.750 --> 00:10:08.510
And the question is-- this
is not the first time when

00:10:08.510 --> 00:10:12.620
we have had this kind of upset
and worry about technology.

00:10:12.620 --> 00:10:15.620
This is an etching about
the Luddite rebellion.

00:10:15.620 --> 00:10:18.590
Now one of the things I learned
that many of you may not know

00:10:18.590 --> 00:10:21.350
is that Ned Ludd did
not actually exist.

00:10:21.350 --> 00:10:23.540
He was not the leader
of the Luddites.

00:10:23.540 --> 00:10:26.840
He was a mythical figure that
this particular revolution,

00:10:26.840 --> 00:10:28.784
in 1811, 1812, cited.

00:10:28.784 --> 00:10:30.200
He was somebody
who apparently had

00:10:30.200 --> 00:10:32.424
smashed a loom 30 years before.

00:10:32.424 --> 00:10:34.340
And they went down his
story, and they kind of

00:10:34.340 --> 00:10:36.836
carried the banner of Ned Ludd.

00:10:36.836 --> 00:10:37.710
But here's the point.

00:10:37.710 --> 00:10:40.730
These guys were right to be
afraid, because the ensuing

00:10:40.730 --> 00:10:42.420
years were pretty bad.

00:10:42.420 --> 00:10:46.520
When you think about the
early years of the Industrial

00:10:46.520 --> 00:10:50.090
Revolution and William
Blake's description

00:10:50.090 --> 00:10:55.070
of the "dark, satanic mills,"
this was not a good time.

00:10:55.070 --> 00:10:59.720
But those weavers
could not imagine

00:10:59.720 --> 00:11:02.360
the wealth of modern society.

00:11:02.360 --> 00:11:05.930
They couldn't imagine
that their descendants,

00:11:05.930 --> 00:11:09.810
that this production, this
mass production of fabric

00:11:09.810 --> 00:11:13.940
would produce a world in
which their grandchildren

00:11:13.940 --> 00:11:15.740
and great grandchildren
would have more

00:11:15.740 --> 00:11:17.870
clothing than the kings
and queens of Europe

00:11:17.870 --> 00:11:22.930
did at the turn of the 18th
century, or the 19th century,

00:11:22.930 --> 00:11:23.660
rather.

00:11:23.660 --> 00:11:27.470
They couldn't imagine that
their descendants would have

00:11:27.470 --> 00:11:30.500
fruit in the middle of winter.

00:11:30.500 --> 00:11:32.600
They couldn't imagine
that we would actually

00:11:32.600 --> 00:11:35.090
build skyscrapers
half a mile high,

00:11:35.090 --> 00:11:39.590
that we dig a tunnel
under the English Channel

00:11:39.590 --> 00:11:41.697
to France, that we'd go
into space, that we'd

00:11:41.697 --> 00:11:42.530
fly through the air.

00:11:42.530 --> 00:11:44.360
All these things are amazing.

00:11:44.360 --> 00:11:48.470
And they couldn't imagine
that their descendants would

00:11:48.470 --> 00:11:51.260
find so much meaningful
work bringing

00:11:51.260 --> 00:11:53.100
all these things to life.

00:11:53.100 --> 00:11:56.630
And so one of the questions I'm
asking in the face of today's

00:11:56.630 --> 00:12:01.580
world of AI and all
these new technologies

00:12:01.580 --> 00:12:03.200
that we're being
told again and again

00:12:03.200 --> 00:12:08.000
are going to destroy jobs, what
is our failure of imagination?

00:12:08.000 --> 00:12:09.630
What are we not able to imagine?

00:12:09.630 --> 00:12:13.210
And what world are we not able
to paint for our grandchildren

00:12:13.210 --> 00:12:14.820
of the world to come?

00:12:14.820 --> 00:12:18.070
And so one of the things
that I start the book with

00:12:18.070 --> 00:12:21.740
is a little excursion into this
idea of fitness landscapes.

00:12:21.740 --> 00:12:23.882
And evolutionary
biology is this idea

00:12:23.882 --> 00:12:26.090
that genes contribute to
the survival of an organism.

00:12:26.090 --> 00:12:29.200
You can think about this as
kind of a landscape of peaks

00:12:29.200 --> 00:12:29.770
and valleys.

00:12:29.770 --> 00:12:33.500
And organisms evolve
towards the peaks,

00:12:33.500 --> 00:12:36.320
which are adapted to their
environment, or they die out.

00:12:36.320 --> 00:12:39.610
And so there is this the
concept of a local maximum.

00:12:39.610 --> 00:12:42.250
And a lot of what
happens is society

00:12:42.250 --> 00:12:47.020
and companies get comfortable
with this local maximum,

00:12:47.020 --> 00:12:48.850
and they don't know
how to move on.

00:12:48.850 --> 00:12:51.190
And often the only
way to move off of it

00:12:51.190 --> 00:12:53.080
is actually to go backwards.

00:12:53.080 --> 00:12:53.940
You have to go down.

00:12:53.940 --> 00:12:57.130
And that's why we have this
cycle sometimes of revolutions,

00:12:57.130 --> 00:13:00.490
of companies fall apart.

00:13:00.490 --> 00:13:04.850
Anyway, fitness
landscapes are dynamic.

00:13:04.850 --> 00:13:06.850
When conditions are stable,
you can kind of just

00:13:06.850 --> 00:13:10.780
stay there, but if things
are changing, not so good.

00:13:10.780 --> 00:13:14.410
And of course, we have
radically changing conditions.

00:13:14.410 --> 00:13:16.429
Climate change is
a great example.

00:13:16.429 --> 00:13:18.970
If you look at the failure of
many civilizations in the past,

00:13:18.970 --> 00:13:20.630
they were driven
by climate change.

00:13:20.630 --> 00:13:25.120
We may face a great deal
of pressure on our fitness

00:13:25.120 --> 00:13:29.130
landscape today as
a result of that.

00:13:29.130 --> 00:13:32.920
And technology also has
a fitness landscape.

00:13:32.920 --> 00:13:35.470
In my career, I watched
the fitness landscape

00:13:35.470 --> 00:13:40.620
of the personal computer,
the big data and AI world

00:13:40.620 --> 00:13:42.160
that Google lives in.

00:13:42.160 --> 00:13:45.550
The smartphone landscape
dominated, just

00:13:45.550 --> 00:13:47.500
really broken open by Apple.

00:13:47.500 --> 00:13:49.960
And now it's sort of a
subject of fierce competition

00:13:49.960 --> 00:13:51.400
between Google and Apple.

00:13:51.400 --> 00:13:54.610
And what's really
interesting when

00:13:54.610 --> 00:13:58.690
you think about what happened,
why was it hard for Microsoft

00:13:58.690 --> 00:14:00.250
to get to the big data world?

00:14:00.250 --> 00:14:02.230
Well, they had too much--

00:14:02.230 --> 00:14:04.590
this is a perfect illustration
of the fitness landscape.

00:14:04.590 --> 00:14:07.760
They had a business model
that really worked for them,

00:14:07.760 --> 00:14:09.162
and they fought--

00:14:09.162 --> 00:14:10.870
there were people who
were saying that we

00:14:10.870 --> 00:14:12.020
got to get with the internet.

00:14:12.020 --> 00:14:14.170
And they were kind of like, no,
we have to preserve Windows.

00:14:14.170 --> 00:14:15.650
And that was their priority.

00:14:15.650 --> 00:14:19.930
And I think it's something that
we always have to think about.

00:14:19.930 --> 00:14:24.700
The dominant companies
tend to be slow to adapt.

00:14:24.700 --> 00:14:29.880
So the thing is that
one of the problems

00:14:29.880 --> 00:14:31.950
is that those dominant
companies tend

00:14:31.950 --> 00:14:34.680
to extract too much of the
value from the ecosystem

00:14:34.680 --> 00:14:35.610
for themselves.

00:14:35.610 --> 00:14:38.171
You know, Microsoft basically
put other companies out

00:14:38.171 --> 00:14:38.670
of business.

00:14:38.670 --> 00:14:41.294
They would come down to Silicon
Valley, meet with VCs, and say,

00:14:41.294 --> 00:14:44.040
you can't invest there because
we're going to do that.

00:14:44.040 --> 00:14:47.700
And this is a real risk, I
think, for Google as well.

00:14:47.700 --> 00:14:54.630
It's really, I think, a mistake
to think about just value

00:14:54.630 --> 00:14:55.900
for users.

00:14:55.900 --> 00:14:59.250
Because Google will make a case,
well, we're doing this thing,

00:14:59.250 --> 00:15:01.376
and it's better
for our customers.

00:15:01.376 --> 00:15:03.750
But you actually have to think
about the whole ecosystem.

00:15:03.750 --> 00:15:06.420
And in particular, you have
to think about the ecosystem

00:15:06.420 --> 00:15:07.760
of developers.

00:15:07.760 --> 00:15:11.790
People went to Linux
and the World Wide Web

00:15:11.790 --> 00:15:14.677
because there was no room left
in the Microsoft ecosystem.

00:15:14.677 --> 00:15:15.510
They went somewhere.

00:15:15.510 --> 00:15:17.400
They didn't think they were
going to go make a lot of money

00:15:17.400 --> 00:15:18.000
over on the web.

00:15:18.000 --> 00:15:19.208
They just went, this is cool.

00:15:19.208 --> 00:15:22.290
This is interesting,
and it's free and open.

00:15:22.290 --> 00:15:24.600
And that sort of
"free and open" was

00:15:24.600 --> 00:15:26.400
what gave birth to
the new "fitness"

00:15:26.400 --> 00:15:30.300
landscape, the small mammals
coming up from the valleys

00:15:30.300 --> 00:15:32.190
into this new fitness peak.

00:15:32.190 --> 00:15:34.170
So again, it's always
something to worry about

00:15:34.170 --> 00:15:37.860
if you're as dominant as
Google Is, is to think about

00:15:37.860 --> 00:15:42.480
are we making enough value for
these developers who are part

00:15:42.480 --> 00:15:44.550
of our ecosystem, these
people who are creating

00:15:44.550 --> 00:15:46.380
on top of our
platform and making it

00:15:46.380 --> 00:15:48.450
a rich, stable ecosystem?

00:15:48.450 --> 00:15:53.640
Or are we basically saying we
have to do that for ourselves?

00:15:53.640 --> 00:15:55.770
I had that experience
many times with Microsoft.

00:15:55.770 --> 00:15:57.670
It was, like, oh,
that's a great idea.

00:15:57.670 --> 00:16:00.180
We have to do that, sorry.

00:16:00.180 --> 00:16:05.670
So in our political landscape,
we're seeing this same thing.

00:16:05.670 --> 00:16:09.600
We've had this idea that by
making a small number of people

00:16:09.600 --> 00:16:12.420
very, very, very wealthy,
it would trickle down

00:16:12.420 --> 00:16:13.440
to the rest of society.

00:16:13.440 --> 00:16:17.920
It hasn't entirely worked
that way, and guess what?

00:16:17.920 --> 00:16:20.110
The people are moving
somewhere else.

00:16:20.110 --> 00:16:22.380
They're saying, we don't
like that consensus

00:16:22.380 --> 00:16:24.035
about how to run the world.

00:16:24.035 --> 00:16:25.410
We're going to do
it differently.

00:16:25.410 --> 00:16:26.520
They may even be wrong.

00:16:26.520 --> 00:16:30.970
And in my opinion,
not even a may.

00:16:30.970 --> 00:16:37.860
But there's a real serious
risk that the conditions

00:16:37.860 --> 00:16:42.570
that we at Silicon Valley
have used to thrive

00:16:42.570 --> 00:16:44.250
are going to change
radically because

00:16:44.250 --> 00:16:46.230
of the political environment.

00:16:46.230 --> 00:16:52.000
And so one of the key pieces
of advice I give in my book

00:16:52.000 --> 00:16:55.440
is just to remember that a
successful ecosystem creates

00:16:55.440 --> 00:16:57.490
opportunity for everyone.

00:16:57.490 --> 00:17:01.830
And this, obviously, was brought
into the political discourse

00:17:01.830 --> 00:17:04.109
by Thomas Piketty's
book, "Capital,"

00:17:04.109 --> 00:17:06.450
in the 21st century,
where he really addressed

00:17:06.450 --> 00:17:09.240
this question of inequality and
got everybody talking about it.

00:17:09.240 --> 00:17:10.448
There have been other people.

00:17:10.448 --> 00:17:13.680
Joe Stiglitz had written
about the 99% versus the 1%

00:17:13.680 --> 00:17:15.160
before Piketty came out.

00:17:15.160 --> 00:17:17.160
And that almost caught.

00:17:17.160 --> 00:17:19.540
But when Piketty's
book came out,

00:17:19.540 --> 00:17:21.790
it was, like, everybody
started talking about it.

00:17:21.790 --> 00:17:27.390
And so I decided to take
a technological angle

00:17:27.390 --> 00:17:29.187
on all this, to kind
of tell this story

00:17:29.187 --> 00:17:31.020
from the point of view
of the tech industry.

00:17:31.020 --> 00:17:34.050
Because there's a lot of people
talking about it as economists.

00:17:34.050 --> 00:17:36.630
And in a lot of ways,
the heart of the book

00:17:36.630 --> 00:17:40.980
is a series of stories about
what the great technology

00:17:40.980 --> 00:17:44.010
platforms have to tell us
about the future of business

00:17:44.010 --> 00:17:45.074
and the economy.

00:17:45.074 --> 00:17:46.740
And the first is this
point I've already

00:17:46.740 --> 00:17:48.330
made, that platforms
have to work

00:17:48.330 --> 00:17:50.940
for all their participants,
not just for the users

00:17:50.940 --> 00:17:52.990
or the platform owner.

00:17:52.990 --> 00:17:56.700
There's also the point that
platforms today are no longer

00:17:56.700 --> 00:17:59.590
just about the digital, right?

00:17:59.590 --> 00:18:02.360
There enmeshed in
the real world.

00:18:02.360 --> 00:18:06.080
You think about Uber or Lyft.

00:18:06.080 --> 00:18:08.180
This is really this
system, you think

00:18:08.180 --> 00:18:11.090
about this huge,
algorithmic system.

00:18:11.090 --> 00:18:14.150
And sure, there are people at
the end points of all the touch

00:18:14.150 --> 00:18:17.330
points of Google, people who
are uploading YouTube videos,

00:18:17.330 --> 00:18:21.620
or they're clicking on links
or they're creating web pages.

00:18:21.620 --> 00:18:27.090
But boy, it becomes really
obvious that human beings are--

00:18:27.090 --> 00:18:29.750
as Sean McMullen the
science fiction author,

00:18:29.750 --> 00:18:32.120
are souls in the great machine--

00:18:32.120 --> 00:18:36.080
when you have these cars
dispatched by algorithm

00:18:36.080 --> 00:18:38.630
with drivers, you have
this swarming marketplace

00:18:38.630 --> 00:18:41.850
where people are doing
things in the real world

00:18:41.850 --> 00:18:44.180
at the beck and
call of algorithms.

00:18:46.880 --> 00:18:51.200
So we have to start thinking
about this interpenetration

00:18:51.200 --> 00:18:53.660
of the digital into all
of our business processes,

00:18:53.660 --> 00:18:58.550
into all of our world
as we effectively

00:18:58.550 --> 00:19:02.060
take the principles that we've
been practicing in the purely

00:19:02.060 --> 00:19:05.660
digital realm for the
last couple of decades

00:19:05.660 --> 00:19:08.480
and start to see them show up
everywhere in the real world.

00:19:08.480 --> 00:19:11.400
So it becomes even
more important.

00:19:11.400 --> 00:19:16.250
And the third key point is
that the fundamental function

00:19:16.250 --> 00:19:20.120
of every technology is
that it augments people

00:19:20.120 --> 00:19:24.180
so they can do things that
were previously impossible.

00:19:24.180 --> 00:19:27.300
We couldn't fly through
the air, and now we can.

00:19:27.300 --> 00:19:31.950
And so when you think about
what AI is going to do,

00:19:31.950 --> 00:19:34.140
there's a strong
narrative that it's going

00:19:34.140 --> 00:19:36.270
to put people out of work.

00:19:36.270 --> 00:19:38.620
And instead, I think
the narrative--

00:19:38.620 --> 00:19:41.700
we need to be seeking out
the narrative of what will it

00:19:41.700 --> 00:19:44.970
let us do that we can't do
today, that will be wonderful,

00:19:44.970 --> 00:19:48.780
that will make us full
of that WTF of delight?

00:19:48.780 --> 00:19:50.880
And we have to tell
that story powerfully.

00:19:50.880 --> 00:19:52.080
And we have to believe it.

00:19:52.080 --> 00:19:54.930
And we have to build
products and startups that

00:19:54.930 --> 00:19:57.280
deliver on that promise.

00:19:57.280 --> 00:20:00.870
So there is kind of
this interesting thing

00:20:00.870 --> 00:20:04.320
that, of course, the WTA
of delight becomes banal.

00:20:04.320 --> 00:20:09.000
I still remember once landing
in Sydney airport in Australia.

00:20:09.000 --> 00:20:10.880
And they had this giant mural.

00:20:10.880 --> 00:20:13.426
And I wish I could find the
name of the photographer,

00:20:13.426 --> 00:20:14.550
because it's long gone now.

00:20:14.550 --> 00:20:17.850
But it was thousands
of people turned out

00:20:17.850 --> 00:20:21.690
to see the first airplane
come to Australia.

00:20:21.690 --> 00:20:24.660
And it was just these
faces looking up in wonder.

00:20:24.660 --> 00:20:28.350
It has been a voyage
of many months,

00:20:28.350 --> 00:20:30.180
for many people that emigrated.

00:20:30.180 --> 00:20:32.550
And then it's, like, they're
connected to the world.

00:20:32.550 --> 00:20:33.890
And it's, like, oh, my god.

00:20:33.890 --> 00:20:37.800
And now you think about
airlines, oh, my god.

00:20:37.800 --> 00:20:41.850
So the WTF of wonder actually
later became the WTF of dismay.

00:20:41.850 --> 00:20:43.290
And we have to
watch out for that.

00:20:43.290 --> 00:20:50.350
But that source of
wonder is so important.

00:20:50.350 --> 00:20:54.060
But this idea of augmentation,
let me kind of come back

00:20:54.060 --> 00:20:56.400
to Uber and Lyft.

00:20:56.400 --> 00:20:59.755
When you think about
that experience,

00:20:59.755 --> 00:21:00.880
there's a couple of things.

00:21:00.880 --> 00:21:03.780
The first is this magical
experience, which goes away

00:21:03.780 --> 00:21:05.160
because you get used to it.

00:21:05.160 --> 00:21:09.550
Wow, I don't have to call a taxi
company and hope they show up.

00:21:09.550 --> 00:21:12.300
I don't have to stand out on
the street corner and wave

00:21:12.300 --> 00:21:14.670
and know they're all full,
or there's nobody there.

00:21:14.670 --> 00:21:15.670
I can summon a car.

00:21:15.670 --> 00:21:17.794
I'd get an estimate of when
it's going to be there.

00:21:17.794 --> 00:21:21.060
And I walk out, and
it just comes, magic.

00:21:21.060 --> 00:21:25.870
Realizing the capability
that was hidden in our phones

00:21:25.870 --> 00:21:29.020
to match people in a real
time-matching marketplace,

00:21:29.020 --> 00:21:29.520
right?

00:21:29.520 --> 00:21:32.160
But also-- and this is one of
the things that lot of people

00:21:32.160 --> 00:21:34.590
don't understand
about Uber and Lyft.

00:21:34.590 --> 00:21:38.280
It's like taxi companies go,
well, we have to have an app.

00:21:38.280 --> 00:21:40.860
But we hate this idea
of part-time drivers

00:21:40.860 --> 00:21:43.590
who just show up and
don't have licenses.

00:21:43.590 --> 00:21:45.210
And you go, well, guess what?

00:21:45.210 --> 00:21:48.900
All the parts of that
business model work together.

00:21:48.900 --> 00:21:51.150
It's because you have
those part-time drivers

00:21:51.150 --> 00:21:54.550
that you can have three minute
pickup times all the time.

00:21:54.550 --> 00:21:56.040
You don't run out
of cabs, right,

00:21:56.040 --> 00:21:58.470
because they're
harnessing the marketplace

00:21:58.470 --> 00:22:00.790
in this new algorithmic way.

00:22:00.790 --> 00:22:04.770
And the reason they can do
that is because the drivers are

00:22:04.770 --> 00:22:07.440
augmented with this new
cognitive augmentation

00:22:07.440 --> 00:22:09.480
called Google Maps.

00:22:09.480 --> 00:22:13.050
There are ways, it's
built now into the app.

00:22:13.050 --> 00:22:16.440
People don't have to have
the knowledge of the streets

00:22:16.440 --> 00:22:18.420
and monuments of London.

00:22:18.420 --> 00:22:20.010
They can just turn
on the app, and it

00:22:20.010 --> 00:22:23.130
says, turn right here, right?

00:22:23.130 --> 00:22:25.470
And literally, the
knowledge is a test

00:22:25.470 --> 00:22:26.620
where you are a human GPS.

00:22:26.620 --> 00:22:29.370
You're like a mentat out
of Frank Herbert's "Dune,"

00:22:29.370 --> 00:22:31.590
where, literally, they give
you two points in London

00:22:31.590 --> 00:22:33.240
and you have to recite
the turn by turn

00:22:33.240 --> 00:22:34.960
to get from one to the other.

00:22:34.960 --> 00:22:36.810
And now you don't
have to do that.

00:22:36.810 --> 00:22:39.180
So that that
cognitive augmentation

00:22:39.180 --> 00:22:41.640
also is going to make
new things possible.

00:22:41.640 --> 00:22:44.820
Just like the steam
shovel or the steel girder

00:22:44.820 --> 00:22:47.220
or the steel beam in
the physical world,

00:22:47.220 --> 00:22:50.550
cognitive augmentation allows
people to do new things

00:22:50.550 --> 00:22:52.330
that they couldn't do before.

00:22:52.330 --> 00:22:55.920
So I also talk a
lot about the fact

00:22:55.920 --> 00:22:58.200
that these systems are
algorithmic systems, at least

00:22:58.200 --> 00:22:58.920
today.

00:22:58.920 --> 00:23:00.000
They infused with AI.

00:23:00.000 --> 00:23:02.360
But these algorithmic
systems have

00:23:02.360 --> 00:23:04.720
a sort of a fitness function
or an objective function.

00:23:04.720 --> 00:23:06.553
And I want to talk about
that in the context

00:23:06.553 --> 00:23:07.680
of the economy in a bit.

00:23:07.680 --> 00:23:10.380
Because I think there's
real significance

00:23:10.380 --> 00:23:13.290
for society, the economy, and
the future of the human race

00:23:13.290 --> 00:23:16.320
in the algorithms that we
are building into our future,

00:23:16.320 --> 00:23:18.100
and I'm gong to talk
about that in minute.

00:23:18.100 --> 00:23:21.150
So a bunch of my writing on the
topic that's not in the book

00:23:21.150 --> 00:23:24.870
you can find at the site
WTFEconomy.com as well as

00:23:24.870 --> 00:23:26.530
some content that's in the book.

00:23:26.530 --> 00:23:30.150
Anyway, come back to this
notion-- a platform must create

00:23:30.150 --> 00:23:32.040
more value than it captures.

00:23:32.040 --> 00:23:34.140
That's been kind of
the heart of what

00:23:34.140 --> 00:23:37.620
we try to do it at O'Reilly.

00:23:37.620 --> 00:23:40.380
We invited in our biggest
competitors into our platform

00:23:40.380 --> 00:23:41.490
when we launched it.

00:23:41.490 --> 00:23:44.190
We continue to think
about how do we make money

00:23:44.190 --> 00:23:46.760
for them as well as for us.

00:23:46.760 --> 00:23:48.165
I think Google should--

00:23:48.165 --> 00:23:52.650
I've loved what Hal has done
with the Google economic impact

00:23:52.650 --> 00:23:55.500
reports, because it's starting
to tell a story which should

00:23:55.500 --> 00:23:57.660
be told by every
business in America,

00:23:57.660 --> 00:23:59.230
every business in the world.

00:23:59.230 --> 00:24:03.000
Which is, how are we creating
value for other people,

00:24:03.000 --> 00:24:05.040
for others, not
just for ourselves,

00:24:05.040 --> 00:24:07.860
all this financial reporting
that's just about wow,

00:24:07.860 --> 00:24:10.860
look how well
we're doing, great.

00:24:10.860 --> 00:24:13.770
There are companies
that are doing well

00:24:13.770 --> 00:24:16.230
by making other people do badly.

00:24:16.230 --> 00:24:18.930
And we have to actually have
a system-wide accounting

00:24:18.930 --> 00:24:23.390
of what people are putting in
and what they're taking out.

00:24:23.390 --> 00:24:27.380
And the alternative, if you are
a Google-like company, where

00:24:27.380 --> 00:24:29.150
you are a systematic
platform company,

00:24:29.150 --> 00:24:30.860
is you become a
regulated utility.

00:24:30.860 --> 00:24:34.610
Because if people
say, wow, you are not

00:24:34.610 --> 00:24:36.230
looking after your ecosystem.

00:24:36.230 --> 00:24:37.850
Well, we'll look
after it for you.

00:24:37.850 --> 00:24:40.308
So I think that's something
you guys should be very worried

00:24:40.308 --> 00:24:40.820
about.

00:24:40.820 --> 00:24:41.900
I want to kind of come back.

00:24:41.900 --> 00:24:43.566
I've talked about a
lot of this already.

00:24:43.566 --> 00:24:47.930
But this is something out of
the book, this notion of what

00:24:47.930 --> 00:24:49.670
I call a business model map.

00:24:49.670 --> 00:24:53.960
A business model is the way
that all the parts of a business

00:24:53.960 --> 00:24:57.140
work together to
create customer value.

00:24:57.140 --> 00:24:59.330
And I first was
introduced to this concept

00:24:59.330 --> 00:25:02.620
by some consultants called Dan
and Meredith Beam back in 2000.

00:25:02.620 --> 00:25:05.900
And they actually used the
example of Southwest Airlines

00:25:05.900 --> 00:25:07.310
versus hub and spoke airlines.

00:25:07.310 --> 00:25:08.720
And they kind of
went through how

00:25:08.720 --> 00:25:13.250
all the parts of
Southwest's business

00:25:13.250 --> 00:25:14.930
actually make them
very different

00:25:14.930 --> 00:25:17.060
than the other airlines,
even though they're

00:25:17.060 --> 00:25:18.410
all in the airline business.

00:25:18.410 --> 00:25:20.240
So they don't forward baggage.

00:25:20.240 --> 00:25:21.590
They don't have assigned seats.

00:25:21.590 --> 00:25:23.450
This actually is different.

00:25:23.450 --> 00:25:27.360
And so Google and Facebook, both
in the advertising business,

00:25:27.360 --> 00:25:30.390
but you have very different
parts of your business model.

00:25:30.390 --> 00:25:36.350
So for example, with
Google, right, your customer

00:25:36.350 --> 00:25:38.690
succeeds when they come to
Google, get what they want,

00:25:38.690 --> 00:25:40.700
and go away.

00:25:40.700 --> 00:25:43.910
So your success is aligned with
people leaving Google, coming

00:25:43.910 --> 00:25:44.960
and leaving.

00:25:44.960 --> 00:25:47.720
Facebook's success is
aligned with having

00:25:47.720 --> 00:25:49.580
people come and stay.

00:25:49.580 --> 00:25:51.770
And that's a huge difference
in your business model,

00:25:51.770 --> 00:25:54.230
and a huge lever if
you are competing

00:25:54.230 --> 00:25:55.680
with Facebook, for example.

00:25:55.680 --> 00:25:57.390
Because you have
to say, oh, wow,

00:25:57.390 --> 00:26:00.350
we have to find
more reasons to get

00:26:00.350 --> 00:26:05.160
people to go away and not to
get people to come and stay

00:26:05.160 --> 00:26:05.660
with us.

00:26:05.660 --> 00:26:10.595
Because then you're
playing on Facebook's turf.

00:26:10.595 --> 00:26:13.461
And Sun Tzu says, attack
your enemy where he is strong

00:26:13.461 --> 00:26:14.210
and you are weak--

00:26:14.210 --> 00:26:16.100
I mean, where he is
weak and you are strong.

00:26:16.100 --> 00:26:17.750
So anyway, back to this--

00:26:17.750 --> 00:26:20.060
I tried to draw a model of Uber.

00:26:20.060 --> 00:26:25.300
Because it seemed to me to be
kind of a template for what's

00:26:25.300 --> 00:26:27.149
starting to happen
in the economy.

00:26:27.149 --> 00:26:28.940
We have these companies
that are platforms.

00:26:28.940 --> 00:26:31.310
They're not just
traditional companies.

00:26:31.310 --> 00:26:34.070
They're replacing
ownership with access.

00:26:34.070 --> 00:26:39.980
And they've got this marketplace
managed by an algorithm,

00:26:39.980 --> 00:26:41.810
matching up passengers
and drivers.

00:26:41.810 --> 00:26:43.310
So there's a bunch of things--

00:26:43.310 --> 00:26:46.490
workers supplying their own
cars, independent contractors,

00:26:46.490 --> 00:26:51.120
that are really
part of this story.

00:26:51.120 --> 00:26:54.710
And this augmented workers
idea that we talked about,

00:26:54.710 --> 00:26:56.850
magical user experience.

00:26:56.850 --> 00:26:59.339
So all of these
things go together.

00:26:59.339 --> 00:27:01.130
And one of the things
we have to understand

00:27:01.130 --> 00:27:03.260
as we think about
technology and the economy

00:27:03.260 --> 00:27:06.930
is what things go together
in our business model?

00:27:06.930 --> 00:27:11.490
What things belong and what
things fight against it?

00:27:11.490 --> 00:27:13.040
And so for example,
at O'Reilly, one

00:27:13.040 --> 00:27:16.760
of the things that we realized
that made us able to transition

00:27:16.760 --> 00:27:20.510
from being a publishing
company to be a conference

00:27:20.510 --> 00:27:22.250
company and an online
learning company

00:27:22.250 --> 00:27:24.070
was that we realized
we were really

00:27:24.070 --> 00:27:25.820
a company that was
about connecting people

00:27:25.820 --> 00:27:27.440
who knew something
with people who

00:27:27.440 --> 00:27:29.480
wanted to learn it from them.

00:27:29.480 --> 00:27:33.530
And so we were able to evolve
how we did that because we

00:27:33.530 --> 00:27:34.550
really understood that.

00:27:34.550 --> 00:27:37.850
Whereas a lot of our
competitors, most of them

00:27:37.850 --> 00:27:40.190
no longer really in
the business very much,

00:27:40.190 --> 00:27:42.920
they just thought their job was
to capture knowledge into books

00:27:42.920 --> 00:27:44.780
and put it on shelves.

00:27:44.780 --> 00:27:49.430
So anyway, I spend
some time, really,

00:27:49.430 --> 00:27:52.790
trying to think through business
models and how they work

00:27:52.790 --> 00:27:55.370
and how they're changing
in the age of the platform.

00:27:55.370 --> 00:27:58.430
I've talked a little bit about
this thick marketplace idea.

00:27:58.430 --> 00:28:01.700
House student
Jonathan Hall, who's

00:28:01.700 --> 00:28:03.450
the chief economist
at Uber, turned me

00:28:03.450 --> 00:28:07.370
onto this book, Alvin Roth's
"Who Gets What and Why,"

00:28:07.370 --> 00:28:10.130
which is really
about how do you make

00:28:10.130 --> 00:28:11.910
a successful
matching marketplace,

00:28:11.910 --> 00:28:15.500
which is at the heart of so many
of the great platforms today.

00:28:15.500 --> 00:28:18.320
And so that kind of
led me to this path

00:28:18.320 --> 00:28:20.850
of thinking about marketplaces.

00:28:20.850 --> 00:28:24.220
But then, again, I talked about
this augmented worker concept.

00:28:24.220 --> 00:28:27.360
But there's a really
key part of it,

00:28:27.360 --> 00:28:30.460
and that is really something
I call the arc of knowledge.

00:28:30.460 --> 00:28:32.630
When we think about how do
we communicate knowledge,

00:28:32.630 --> 00:28:35.640
we first spoke it to each other.

00:28:35.640 --> 00:28:38.400
And then we wrote it
down, things like maps.

00:28:38.400 --> 00:28:43.760
And you think about the first
iterations of online maps.

00:28:43.760 --> 00:28:47.540
They were kind of reproductions
of printed maps, just

00:28:47.540 --> 00:28:49.320
online, which was kind of nice.

00:28:49.320 --> 00:28:51.020
And then you got
maps and directions.

00:28:51.020 --> 00:28:53.210
And you go all the way
up to automated vehicles,

00:28:53.210 --> 00:28:57.080
and you see that the knowledge
disappears into a service.

00:28:57.080 --> 00:29:00.350
And that seems to
me to be a key point

00:29:00.350 --> 00:29:04.310
to remember when you think about
how are we augmenting workers.

00:29:04.310 --> 00:29:08.240
We're basically building
these cognitive augmentations

00:29:08.240 --> 00:29:10.490
that will just
disappear into services,

00:29:10.490 --> 00:29:14.270
where you don't necessarily
need to know in advance things.

00:29:14.270 --> 00:29:15.800
You know, this is
already so true.

00:29:15.800 --> 00:29:19.497
We were talking over lunch about
how, oh, yeah, some technique.

00:29:19.497 --> 00:29:20.330
How do you learn it?

00:29:20.330 --> 00:29:22.280
You go to YouTube, and
you watch the video.

00:29:22.280 --> 00:29:24.980
And before long, maybe we will
be in the place like Trinity

00:29:24.980 --> 00:29:26.690
in "The Matrix,"
where we can say,

00:29:26.690 --> 00:29:28.760
I need to know how
to fly a helicopter.

00:29:28.760 --> 00:29:30.950
Remember, she downloads
the knowledge.

00:29:30.950 --> 00:29:34.460
But even without that, we can
have these augmented devices

00:29:34.460 --> 00:29:37.160
that capture and
share human knowledge.

00:29:37.160 --> 00:29:40.760
So when I think about AI,
that's my starting point--

00:29:40.760 --> 00:29:43.750
is that it's a tool
for human augmentation.

00:29:43.750 --> 00:29:46.892
It isn't this radical
discontinuity,

00:29:46.892 --> 00:29:48.350
this machine from
the future that's

00:29:48.350 --> 00:29:53.930
going to make humans obsolete
any more than machines

00:29:53.930 --> 00:29:56.270
of the industrial
era were somehow

00:29:56.270 --> 00:29:57.950
going to make humans obsolete.

00:29:57.950 --> 00:30:00.110
Yeah, sure, yeah, we're
not as strong probably

00:30:00.110 --> 00:30:01.570
as most of our--

00:30:01.570 --> 00:30:05.030
even if we work out a
lot, we're probably not

00:30:05.030 --> 00:30:06.980
as strong or as
fit as someone who

00:30:06.980 --> 00:30:10.730
worked every day on the farm
or doing [INAUDIBLE] work.

00:30:10.730 --> 00:30:14.320
I still remember going to Hawaii
once and huffing and puffing up

00:30:14.320 --> 00:30:15.029
over some hill.

00:30:15.029 --> 00:30:17.070
And it was like, oh, yeah,
the ancient Hawaiians,

00:30:17.070 --> 00:30:19.028
it took them about 20
minutes to go up and over

00:30:19.028 --> 00:30:20.570
to the other waterfall.

00:30:20.570 --> 00:30:22.650
It took us two hours.

00:30:22.650 --> 00:30:27.300
Yeah, we're not as fit,
but we are more capable.

00:30:27.300 --> 00:30:29.510
If you've ever read
"Guns, Germs and Steel,"

00:30:29.510 --> 00:30:32.240
you know it opens to
this wonderful passage.

00:30:32.240 --> 00:30:33.950
You get all these questions.

00:30:33.950 --> 00:30:39.560
A guy in New Guinea saying, you
know, you guys are so stupid.

00:30:39.560 --> 00:30:42.080
If I took you in the jungle,
you'd be dead in two weeks.

00:30:42.080 --> 00:30:45.020
How come you have all the cargo?

00:30:45.020 --> 00:30:48.680
And that is, in fact, the
question of our civilization.

00:30:48.680 --> 00:30:51.960
It's, like, we are
making ourselves smarter,

00:30:51.960 --> 00:30:53.630
stronger, faster, better.

00:30:53.630 --> 00:30:56.600
And that should be
how we frame and how

00:30:56.600 --> 00:30:59.920
we think about what we're doing
with technologies, not disrupt.

00:30:59.920 --> 00:31:00.980
It's not destroy.

00:31:00.980 --> 00:31:02.240
It's not eliminate.

00:31:02.240 --> 00:31:03.080
It's empower.

00:31:03.080 --> 00:31:03.830
It's lift up.

00:31:03.830 --> 00:31:06.140
It's make things possible.

00:31:06.140 --> 00:31:09.860
So technology is our superpower.

00:31:09.860 --> 00:31:14.570
And this is another one of the
charts from Our World in Data

00:31:14.570 --> 00:31:17.300
of the number of people
in the world living

00:31:17.300 --> 00:31:18.290
in absolute poverty.

00:31:18.290 --> 00:31:20.270
And you watch that
incredibly steep decline.

00:31:20.270 --> 00:31:21.350
And that's wonderful.

00:31:21.350 --> 00:31:23.930
But inequality is
our kryptonite.

00:31:23.930 --> 00:31:27.780
And this is from the
we are the 99% story.

00:31:27.780 --> 00:31:29.090
We have to confront that.

00:31:29.090 --> 00:31:32.820
We have to think about it,
particularly as a society,

00:31:32.820 --> 00:31:35.580
but also as an industry.

00:31:35.580 --> 00:31:39.129
So one of my
interesting questions

00:31:39.129 --> 00:31:40.670
that I try to explore
in the book is,

00:31:40.670 --> 00:31:45.530
what keeps us from creating
prosperity for everyone?

00:31:45.530 --> 00:31:47.960
And here's the answer.

00:31:47.960 --> 00:31:55.070
And this is the connected
taxicab circa 2005.

00:31:55.070 --> 00:31:56.970
It's, like, wow, we had
connected taxi cabs.

00:31:56.970 --> 00:31:57.595
What did we do?

00:31:57.595 --> 00:32:00.030
We put a screen
in the back where

00:32:00.030 --> 00:32:02.070
people could look
up information,

00:32:02.070 --> 00:32:04.650
where they can watch ads.

00:32:04.650 --> 00:32:07.290
And we totally missed--

00:32:07.290 --> 00:32:09.840
even though, actually,
Sunil Paul, in 2000,

00:32:09.840 --> 00:32:11.820
had written a series
of patents about what

00:32:11.820 --> 00:32:16.050
would be possible if you
tried to connect cars

00:32:16.050 --> 00:32:17.097
with smartphones.

00:32:17.097 --> 00:32:18.930
Actually, they weren't
even smartphones, but

00:32:18.930 --> 00:32:20.160
with GPS and phones.

00:32:20.160 --> 00:32:22.230
It was just too early, right?

00:32:22.230 --> 00:32:24.890
So there was this sort
of cognitive filter,

00:32:24.890 --> 00:32:27.150
this cognitive blindness.

00:32:27.150 --> 00:32:29.270
You were actually sort
of framing blindness,

00:32:29.270 --> 00:32:30.561
may be the right way to say it.

00:32:30.561 --> 00:32:33.000
Where we framed the world
in a particular way,

00:32:33.000 --> 00:32:35.500
and we couldn't see
what was possible.

00:32:35.500 --> 00:32:37.560
And I think this is
a critical problem.

00:32:37.560 --> 00:32:40.950
And I've watched it
throughout my career.

00:32:40.950 --> 00:32:46.150
In 2000, I led this protest
against Jeff Bezos's one click

00:32:46.150 --> 00:32:47.220
patent.

00:32:47.220 --> 00:32:48.780
And we went and we looked--

00:32:48.780 --> 00:32:52.950
Jeff kind of worked with
me to try to turn it around

00:32:52.950 --> 00:32:54.370
from a PR point of view.

00:32:54.370 --> 00:32:57.450
One of the things
we did was we funded

00:32:57.450 --> 00:33:00.750
a startup called Bounty Quest,
which looked for prior art

00:33:00.750 --> 00:33:02.850
on one click shopping.

00:33:02.850 --> 00:33:04.500
And we couldn't find it.

00:33:04.500 --> 00:33:07.200
Because, basically, it was sort
of unthinkable at the time.

00:33:07.200 --> 00:33:09.430
People were afraid to put
their credit cards online.

00:33:09.430 --> 00:33:11.564
They had the shopping
cart metaphor.

00:33:11.564 --> 00:33:12.980
They kind of
understood the world.

00:33:12.980 --> 00:33:16.470
It was very much like this
connected taxi cab circa 2005.

00:33:16.470 --> 00:33:20.460
So the reason I tell you
that story, actually,

00:33:20.460 --> 00:33:21.420
and here's another one.

00:33:21.420 --> 00:33:25.290
It's, like, I asked Tony
Fidel, why didn't you guys

00:33:25.290 --> 00:33:26.550
do the connected speaker?

00:33:26.550 --> 00:33:28.830
And he says, well,
can you imagine

00:33:28.830 --> 00:33:33.330
if people had thought
Google is listening to me?

00:33:33.330 --> 00:33:36.600
Somebody had to make
that possible first.

00:33:36.600 --> 00:33:38.040
Because there was just this--

00:33:38.040 --> 00:33:41.910
because of Google's particular
position in the market,

00:33:41.910 --> 00:33:43.750
you couldn't be the first mover.

00:33:43.750 --> 00:33:46.680
But you still have to understand
that these cognitive shifts do

00:33:46.680 --> 00:33:49.890
happen where people believe
that something else is possible.

00:33:49.890 --> 00:33:52.950
But more than that, I want to
actually take that thinking

00:33:52.950 --> 00:33:55.509
to the political and
the economic realm.

00:33:55.509 --> 00:33:58.050
And this is where I start to
get out of my depth in the book.

00:34:00.739 --> 00:34:02.280
Maybe it's just a
thought experiment.

00:34:02.280 --> 00:34:05.370
But maybe, just maybe,
it's something that's

00:34:05.370 --> 00:34:06.930
really worth thinking about.

00:34:06.930 --> 00:34:09.989
And this is the
beginning of, actually,

00:34:09.989 --> 00:34:14.250
an argument that runs through
a central section of the book.

00:34:14.250 --> 00:34:16.920
And it's about sort of AI
and algorithmic systems

00:34:16.920 --> 00:34:19.679
and their relationship
to human society.

00:34:19.679 --> 00:34:23.610
And I ask the question,
what if strong AI bears

00:34:23.610 --> 00:34:26.969
the same relationship
to today's narrow AI

00:34:26.969 --> 00:34:30.810
as multicellular life does to
its single-celled forebears?

00:34:30.810 --> 00:34:32.760
Now right now we keep
thinking that we're

00:34:32.760 --> 00:34:35.040
going to create a strong AI.

00:34:35.040 --> 00:34:37.889
Maybe it'll be 15,
20 years from now.

00:34:37.889 --> 00:34:38.670
Maybe it's be 50.

00:34:38.670 --> 00:34:39.929
Maybe it'll be 100.

00:34:39.929 --> 00:34:44.250
But it'll be like us, a
single, self-aware being.

00:34:44.250 --> 00:34:49.620
And I wonder if instead
it's a collective being.

00:34:49.620 --> 00:34:51.449
And it turns out
that that is what

00:34:51.449 --> 00:34:55.920
happened in this prokaryotic
to eukaryotic cell transition.

00:34:55.920 --> 00:34:58.950
Because it turns out that
there's this thing that Lynn

00:34:58.950 --> 00:35:03.660
Margulis in 1967-- actually, I
think it was originally, again,

00:35:03.660 --> 00:35:09.180
in [INAUDIBLE] in 1905
that first proposed it.

00:35:09.180 --> 00:35:11.040
But she picked it up
in '67 and actually

00:35:11.040 --> 00:35:16.290
proved it over a lot of years
of being ridiculed and pushed

00:35:16.290 --> 00:35:16.890
back on.

00:35:16.890 --> 00:35:20.970
And it was this idea
that multicelled

00:35:20.970 --> 00:35:24.570
organisms actually incorporated
bacteria into them.

00:35:24.570 --> 00:35:27.330
So it turns out, and
was later substantiated

00:35:27.330 --> 00:35:30.690
by genetic evidence, that
mitochondria in our body

00:35:30.690 --> 00:35:36.240
are actually bacteria that came
and lived inside of us, right?

00:35:36.240 --> 00:35:38.780
Same thing with
chloroplasts and plants,

00:35:38.780 --> 00:35:41.010
they have totally
different genetic material

00:35:41.010 --> 00:35:42.937
than the nucleus of the cell.

00:35:42.937 --> 00:35:45.020
And so in a similar way,
I started thinking, well,

00:35:45.020 --> 00:35:49.830
what if we are somehow like
that with our computers?

00:35:49.830 --> 00:35:52.590
And of course,
biological symbiosis

00:35:52.590 --> 00:35:54.477
doesn't start with
that symbiogenesis

00:35:54.477 --> 00:35:55.810
that Lynn Margulis talked about.

00:35:55.810 --> 00:35:57.393
There's also this
increasing knowledge

00:35:57.393 --> 00:36:01.290
of the microbiome, how we
are really a colony organism.

00:36:01.290 --> 00:36:04.170
Ed Yong in his wonderful book,
"I Contain Multitudes" says,

00:36:04.170 --> 00:36:07.750
all zoology is really ecology.

00:36:07.750 --> 00:36:10.210
When we look at beetles
and elephants, sea urchins

00:36:10.210 --> 00:36:12.150
and earthworms,
parents and friends,

00:36:12.150 --> 00:36:17.010
we see individuals driven by
a single brain and operating

00:36:17.010 --> 00:36:18.780
with a single genome.

00:36:18.780 --> 00:36:20.510
And this is a pleasant fiction.

00:36:20.510 --> 00:36:24.360
You know, we're legion, he
and Walt Whitman, "I am large.

00:36:24.360 --> 00:36:26.070
I contain multitudes."

00:36:26.070 --> 00:36:28.500
So think about this
in humans, too.

00:36:28.500 --> 00:36:32.700
When we talk about
unsupervised learning

00:36:32.700 --> 00:36:35.430
as the holy grail of AI,
I go, yeah, you know,

00:36:35.430 --> 00:36:37.830
humans have some amount
of unsupervised learning.

00:36:37.830 --> 00:36:40.950
We have a shit ton of
supervised learning.

00:36:40.950 --> 00:36:43.260
We learn our language
from our parents.

00:36:43.260 --> 00:36:47.220
Everything we know that makes us
able to do the kinds of things

00:36:47.220 --> 00:36:49.980
that we do here at Google,
we were taught by somebody.

00:36:49.980 --> 00:36:51.840
And then, on top
of that, yes, we're

00:36:51.840 --> 00:36:53.940
able to do
unsupervised learning.

00:36:53.940 --> 00:36:56.580
But we have a long
way to go, I think,

00:36:56.580 --> 00:37:00.120
in the supervised
learning of this organism

00:37:00.120 --> 00:37:01.969
before we can kind
of even expect to see

00:37:01.969 --> 00:37:03.135
it start to do unsupervised.

00:37:03.135 --> 00:37:05.850
It's not like you start,
necessarily, there.

00:37:08.530 --> 00:37:09.030
So

00:37:09.030 --> 00:37:11.067
Thinking about
this as a metaphor,

00:37:11.067 --> 00:37:12.150
I started thinking about--

00:37:12.150 --> 00:37:15.840
I've been really thinking about
this for the last 10, 15 years.

00:37:15.840 --> 00:37:18.900
How increasingly, we're
all, all of humanity,

00:37:18.900 --> 00:37:22.820
is being woven into this new
global mind, this super AI.

00:37:22.820 --> 00:37:26.040
And we're seeing in
things like fake news

00:37:26.040 --> 00:37:29.370
kind of the equivalent of
our neuroses, the equivalent

00:37:29.370 --> 00:37:32.670
of bad ideas.

00:37:32.670 --> 00:37:34.910
They spread
incredibly quickly now

00:37:34.910 --> 00:37:37.500
and are adopted by
millions of people.

00:37:37.500 --> 00:37:41.430
And then encoded further into
the systems that we build.

00:37:41.430 --> 00:37:44.900
So we're at this really
interesting phase of symbiosis.

00:37:44.900 --> 00:37:47.860
And we have to come
and understand it.

00:37:47.860 --> 00:37:53.640
And we think that our
emergent proto-AIs are also

00:37:53.640 --> 00:37:56.132
compound beings,
just like we are.

00:37:56.132 --> 00:37:57.340
What would we do differently?

00:37:57.340 --> 00:37:59.215
How would we think about
them, and what do we

00:37:59.215 --> 00:38:01.020
know about them today?

00:38:01.020 --> 00:38:05.010
And there's a lot
of fear of some kind

00:38:05.010 --> 00:38:07.350
of future, hostile AI.

00:38:07.350 --> 00:38:12.390
And I go, well, if there is
one, we are already teaching it.

00:38:12.390 --> 00:38:14.640
And what are we teaching it?

00:38:14.640 --> 00:38:17.040
So when you think
about this further,

00:38:17.040 --> 00:38:19.890
we're seeing this
compound being.

00:38:19.890 --> 00:38:21.900
We see the rise of
specialized chips,

00:38:21.900 --> 00:38:25.170
which is actually a pretty
interesting imitation

00:38:25.170 --> 00:38:26.310
of neuroscience.

00:38:26.310 --> 00:38:27.450
But here is a great one.

00:38:27.450 --> 00:38:33.570
Here's one of the mitochondria
inside the Google Brain, right?

00:38:33.570 --> 00:38:38.970
And here's the thing, we've even
defined in our legal system,

00:38:38.970 --> 00:38:42.870
starting in 1888, we said
that there's this thing called

00:38:42.870 --> 00:38:44.310
a corporate person.

00:38:44.310 --> 00:38:47.230
It's really just this
collective organism.

00:38:47.230 --> 00:38:50.100
That was the case called
"Consolidated Silver Mining

00:38:50.100 --> 00:38:51.870
v. Pennsylvania."

00:38:51.870 --> 00:38:53.404
"Under the
designation of person,

00:38:53.404 --> 00:38:55.320
there's no doubt that a
private corporation is

00:38:55.320 --> 00:38:56.880
included in the 14th Amendment.

00:38:56.880 --> 00:38:59.730
Such corporations are merely
associations of individuals

00:38:59.730 --> 00:39:01.110
united for a special purpose."

00:39:01.110 --> 00:39:01.610
Right?

00:39:01.610 --> 00:39:04.740
So a special purpose organism--

00:39:04.740 --> 00:39:05.550
and guess what?

00:39:05.550 --> 00:39:07.020
We now have these
special purposes

00:39:07.020 --> 00:39:10.620
organisms are increasingly
digital, increasingly quick,

00:39:10.620 --> 00:39:12.520
increasingly algorithmic.

00:39:12.520 --> 00:39:16.020
We're making these
special purpose organisms.

00:39:16.020 --> 00:39:18.730
And what are we
asking them to do?

00:39:18.730 --> 00:39:21.570
And I believe that one
of the things that we

00:39:21.570 --> 00:39:24.720
have to come to grips with
is that, in some ways,

00:39:24.720 --> 00:39:27.690
these AIs that are our
children are already

00:39:27.690 --> 00:39:30.090
ruling human society.

00:39:30.090 --> 00:39:31.530
And we have to ask
ourselves, what

00:39:31.530 --> 00:39:33.840
is the motivation
of this new species?

00:39:33.840 --> 00:39:36.010
Now this is obviously
kind of out there.

00:39:36.010 --> 00:39:38.100
But think about it.

00:39:38.100 --> 00:39:44.020
How does Google decide
what it's going to do?

00:39:44.020 --> 00:39:47.050
Well, we actually--
we collectively,

00:39:47.050 --> 00:39:49.660
starting with Larry
and Sergei and you

00:39:49.660 --> 00:39:53.500
know all the early founders, but
people like Hal and all of you,

00:39:53.500 --> 00:39:57.430
have actually put thoughts
into this shared Google brain.

00:39:57.430 --> 00:39:58.180
You've trained it.

00:39:58.180 --> 00:39:59.260
You've taught it.

00:39:59.260 --> 00:40:04.110
And you've taught it that it
should optimize for relevance.

00:40:04.110 --> 00:40:08.740
And this is this sort of master
organizing objective function

00:40:08.740 --> 00:40:10.750
of everything that
Google does, whether it's

00:40:10.750 --> 00:40:12.310
in search or advertising.

00:40:12.310 --> 00:40:15.140
It's, like, find the thing
that people are looking for.

00:40:15.140 --> 00:40:16.990
So you've kind of built
this special purpose

00:40:16.990 --> 00:40:24.160
organism that has these goals
built into it, baked into it.

00:40:24.160 --> 00:40:27.100
And those goals actually
now exist independently

00:40:27.100 --> 00:40:29.790
of any of the people
who created it.

00:40:29.790 --> 00:40:35.680
Again, it will die, just like
a biological organism will

00:40:35.680 --> 00:40:37.810
die under certain conditions.

00:40:37.810 --> 00:40:42.060
But it doesn't actually
depend on any individual cells

00:40:42.060 --> 00:40:43.060
to remain.

00:40:43.060 --> 00:40:46.000
We've all been told that
our bodies can completely

00:40:46.000 --> 00:40:50.230
recycle, be completely different
cells every seven years,

00:40:50.230 --> 00:40:51.310
I think it is.

00:40:51.310 --> 00:40:53.830
And same thing-- all the
people could cycle out,

00:40:53.830 --> 00:40:57.040
and this organism would continue
with the same programming,

00:40:57.040 --> 00:40:59.417
with its continual,
adapting program.

00:40:59.417 --> 00:41:00.250
So think about that.

00:41:00.250 --> 00:41:04.780
So Google, relevance, and
Facebook, engagement--

00:41:04.780 --> 00:41:06.950
and we saw with
Facebook and fake news

00:41:06.950 --> 00:41:08.860
how that can go wrong.

00:41:08.860 --> 00:41:10.840
And it basically
reminds me, I guess,

00:41:10.840 --> 00:41:15.400
of the story of Mickey Mouse
and the broomsticks in Fantasia.

00:41:15.400 --> 00:41:17.920
I mean, all of these systems,
we tell them what to do,

00:41:17.920 --> 00:41:19.660
and we set them going.

00:41:19.660 --> 00:41:21.890
And they do what
we told them to do.

00:41:21.890 --> 00:41:23.951
But we don't necessarily
quite understand it.

00:41:23.951 --> 00:41:25.450
And this is the
story that comes out

00:41:25.450 --> 00:41:28.090
again and again in Arabian
mythology with genies.

00:41:28.090 --> 00:41:31.340
And you told them--

00:41:31.340 --> 00:41:33.850
this is actually an
illustration by Edmund Dulac

00:41:33.850 --> 00:41:36.730
from the 1,001
nights, that is sort

00:41:36.730 --> 00:41:38.570
of a rising of great power.

00:41:38.570 --> 00:41:39.976
But what's it going to do?

00:41:39.976 --> 00:41:41.350
It's going to do
what we told it.

00:41:41.350 --> 00:41:42.790
But maybe we didn't
understand it.

00:41:42.790 --> 00:41:44.530
We didn't tell it quite right.

00:41:44.530 --> 00:41:49.000
Mark thought that by building
this social platform that

00:41:49.000 --> 00:41:52.490
was focused on engagement, he
would build real community.

00:41:52.490 --> 00:41:53.940
And then he found
instead that we

00:41:53.940 --> 00:41:56.020
have created this monster,
which he is now trying

00:41:56.020 --> 00:41:58.330
to bring back under control.

00:41:58.330 --> 00:42:00.710
That's actually part
of the experience.

00:42:00.710 --> 00:42:04.864
So many years ago, a
friend of mine said to me,

00:42:04.864 --> 00:42:07.030
this was in the early days
of Macintosh programming,

00:42:07.030 --> 00:42:09.066
he said the art of
debugging is figuring out

00:42:09.066 --> 00:42:10.690
what you really told
your program to do

00:42:10.690 --> 00:42:13.040
rather than what you
thought you told it to do.

00:42:13.040 --> 00:42:15.730
And we are, right now,
engaged in that process

00:42:15.730 --> 00:42:20.620
with these new digital
gens that we have created.

00:42:20.620 --> 00:42:24.750
We are saying, are they
doing what we meant?

00:42:24.750 --> 00:42:28.020
Are they doing what
we told them to do,

00:42:28.020 --> 00:42:30.060
but wasn't quite what we asked?

00:42:30.060 --> 00:42:31.025
And how do we fix that?

00:42:31.025 --> 00:42:32.650
That's what you do
every day at Google.

00:42:32.650 --> 00:42:34.950
You try to actually
go, yes, is it

00:42:34.950 --> 00:42:37.470
doing what we really
meant it to do?

00:42:37.470 --> 00:42:40.830
And now, my question is,
are we doing that same thing

00:42:40.830 --> 00:42:43.290
in our broader society?

00:42:43.290 --> 00:42:46.130
Because here's the
data that we see

00:42:46.130 --> 00:42:49.770
that is at the heart
of this economic unease

00:42:49.770 --> 00:42:53.370
that we're facing, which is
that this wonderful gift,

00:42:53.370 --> 00:42:56.760
the WTF of amazement
is that productivity

00:42:56.760 --> 00:42:58.320
has continued to go up.

00:42:58.320 --> 00:43:02.700
If you look, this is from
1945 up through 2015.

00:43:02.700 --> 00:43:05.830
And it's pretty, my
god, it's a graph

00:43:05.830 --> 00:43:07.330
that Ray Kurzweil
would be proud of.

00:43:07.330 --> 00:43:09.910
It's just up and to
the right, right?

00:43:09.910 --> 00:43:14.402
And then you see this
divergence starting around 1970s

00:43:14.402 --> 00:43:16.140
of real family income.

00:43:16.140 --> 00:43:18.360
Somehow that
productivity was not

00:43:18.360 --> 00:43:21.210
getting through, not being
distributed to ordinary people.

00:43:21.210 --> 00:43:24.400
Now there's a lot of reasons
why that may have gone wrong.

00:43:24.400 --> 00:43:29.310
And people like Hal
study this, and his ilk,

00:43:29.310 --> 00:43:32.500
this is what economists do.

00:43:32.500 --> 00:43:35.340
And I'm not saying what I'm
proposing is the only cause,

00:43:35.340 --> 00:43:37.290
but I think it is one of them.

00:43:37.290 --> 00:43:38.280
And that is this.

00:43:38.280 --> 00:43:42.570
Here's another one of
these AIs, these proto-AIs,

00:43:42.570 --> 00:43:44.570
that we tell it what to do.

00:43:44.570 --> 00:43:46.540
And is it really
doing what we think?

00:43:46.540 --> 00:43:49.350
And this is the Equinix
NY4 data center,

00:43:49.350 --> 00:43:54.510
which is one of the hearts of
our financial trading system.

00:43:54.510 --> 00:43:59.640
And this is kind of the
Skynet of the story.

00:43:59.640 --> 00:44:05.830
Because ultimately, we
told our financial system

00:44:05.830 --> 00:44:08.830
what its objective
function should be.

00:44:08.830 --> 00:44:13.210
Milton Friedman in 1970 wrote an
article in "The New York Times"

00:44:13.210 --> 00:44:15.580
called "The Social
Responsibility of Business

00:44:15.580 --> 00:44:17.650
is to Increase Its Profits."

00:44:17.650 --> 00:44:21.150
That businesses should not think
about anything other than that.

00:44:21.150 --> 00:44:23.560
And if they did that, they
would pass along the profits

00:44:23.560 --> 00:44:25.226
to the shareholders
and the shareholders

00:44:25.226 --> 00:44:26.630
could make up their own mind.

00:44:26.630 --> 00:44:29.380
This is a perfectly
reasonable thesis,

00:44:29.380 --> 00:44:33.170
just like some of the theses
that you've put into your code.

00:44:33.170 --> 00:44:36.370
But when you build
code, and you actually

00:44:36.370 --> 00:44:38.050
do that debugging
process-- you say,

00:44:38.050 --> 00:44:41.590
did it do what I asked it to do?

00:44:41.590 --> 00:44:46.270
And we have to ask ourselves,
when we said optimize

00:44:46.270 --> 00:44:50.650
for share price, in particular--
optimize for profits,

00:44:50.650 --> 00:44:53.320
and the profits, this
proxy is the share price

00:44:53.320 --> 00:44:56.980
of companies-- optimized
for that, not for people.

00:44:56.980 --> 00:45:00.910
Oh, yeah, if it makes sense,
outsource the factories.

00:45:00.910 --> 00:45:03.490
If it makes sense,
gut that community.

00:45:03.490 --> 00:45:06.790
because your master
fitness function,

00:45:06.790 --> 00:45:10.690
the wish that we expressed
to the genie we built

00:45:10.690 --> 00:45:15.130
was make my share price go up.

00:45:15.130 --> 00:45:16.532
And this is,
actually-- there are

00:45:16.532 --> 00:45:18.220
a lot of people in the
financial industry who are

00:45:18.220 --> 00:45:19.670
starting to take note of this.

00:45:19.670 --> 00:45:22.967
I mean, Larry Fink, who runs
BlackRock, the largest asset

00:45:22.967 --> 00:45:24.550
manager in the, world
has been railing

00:45:24.550 --> 00:45:27.567
about stock buybacks, Warren
Buffett questioning them.

00:45:27.567 --> 00:45:28.650
There's a number of books.

00:45:28.650 --> 00:45:30.750
This one "Makers and
Takers" is really good.

00:45:30.750 --> 00:45:32.680
Another one there,
"Golden Passport,"

00:45:32.680 --> 00:45:35.470
about how Harvard
Business School kind of

00:45:35.470 --> 00:45:36.910
wrecked the economy.

00:45:36.910 --> 00:45:39.910
But so this idea that the
financial markets, which

00:45:39.910 --> 00:45:42.670
are really meant to
support the human economy,

00:45:42.670 --> 00:45:46.420
have become this extractive
platform that basically

00:45:46.420 --> 00:45:49.240
says, no, give the money to us.

00:45:49.240 --> 00:45:51.670
And we have to
actually reverse that.

00:45:51.670 --> 00:45:53.260
And that's when I
want to come back

00:45:53.260 --> 00:45:57.850
to this story about
the little screen

00:45:57.850 --> 00:46:02.260
in the back of the taxicab and
the failure of imagination.

00:46:02.260 --> 00:46:04.930
Because it's very
hard for us to imagine

00:46:04.930 --> 00:46:07.960
a completely different world.

00:46:07.960 --> 00:46:10.090
So when we talk
about tax reform,

00:46:10.090 --> 00:46:12.730
it's like we'll push this
rate up and that rate down.

00:46:12.730 --> 00:46:14.650
That's not how it happens.

00:46:14.650 --> 00:46:17.200
The great revolutions
are ones where you really

00:46:17.200 --> 00:46:19.120
imagine the world anew.

00:46:19.120 --> 00:46:20.920
And we're good at
that in technology, we

00:46:20.920 --> 00:46:23.150
think about that all the time.

00:46:23.150 --> 00:46:27.130
And we need to do it also
in the world of politics

00:46:27.130 --> 00:46:28.640
and the economy.

00:46:28.640 --> 00:46:30.610
And we have done that before.

00:46:30.610 --> 00:46:37.610
When the founders of this
country got together and said,

00:46:37.610 --> 00:46:39.790
we're going to try to
build a new country

00:46:39.790 --> 00:46:42.800
with a new set of ideas.

00:46:42.800 --> 00:46:45.370
You know, oh, my god,
there is that famous story

00:46:45.370 --> 00:46:46.780
about King George.

00:46:46.780 --> 00:46:53.180
When George Washington stepped
down, so basically, originally,

00:46:53.180 --> 00:46:55.949
when he won the war,
all the Europeans

00:46:55.949 --> 00:46:57.490
expected that George
Washington would

00:46:57.490 --> 00:46:59.710
become the king of America.

00:46:59.710 --> 00:47:03.610
And George III is
reported to have said--

00:47:03.610 --> 00:47:06.130
he went back to
his farm instead.

00:47:06.130 --> 00:47:09.760
And King George said, if he has
done that, he is the greatest

00:47:09.760 --> 00:47:10.940
man the world has ever seen.

00:47:10.940 --> 00:47:12.590
It was this unthinkable thing.

00:47:12.590 --> 00:47:13.090
You know?

00:47:13.090 --> 00:47:16.570
So much more unthinkable than
Uber was to a taxi company,

00:47:16.570 --> 00:47:19.480
or that one click was
to an ecommerce site.

00:47:19.480 --> 00:47:20.530
It was amazing!

00:47:20.530 --> 00:47:22.570
And we can do that.

00:47:22.570 --> 00:47:26.290
So if we are entering a
world where AI can do so much

00:47:26.290 --> 00:47:30.580
more of the jobs that we
do today, we should not

00:47:30.580 --> 00:47:33.100
be content to say,
well, we'll just kind of

00:47:33.100 --> 00:47:37.640
keep feeding the current system
Cory Doctorow, the science

00:47:37.640 --> 00:47:40.780
fiction writer, had this great
statement on social media

00:47:40.780 --> 00:47:43.990
recently, where he said
economists use equations

00:47:43.990 --> 00:47:46.430
to justify the divine
right of capital,

00:47:46.430 --> 00:47:50.140
the way that court astrologers
used the stars to justify

00:47:50.140 --> 00:47:51.515
the divine right of kings.

00:47:51.515 --> 00:47:53.140
And you know, whether
it's true or not,

00:47:53.140 --> 00:47:55.300
it's true in the
spiritual world.

00:47:55.300 --> 00:47:58.510
This idea that we basically
justify the system

00:47:58.510 --> 00:48:02.440
as it is rather than imagining
the system as it could be.

00:48:02.440 --> 00:48:05.200
And I see this
enormous opportunity

00:48:05.200 --> 00:48:07.900
to make a more
prosperous world with all

00:48:07.900 --> 00:48:10.040
of these technologies we have.

00:48:10.040 --> 00:48:14.410
So what makes me
hopeful is first of all

00:48:14.410 --> 00:48:16.810
that when we build these
algorithmic systems,

00:48:16.810 --> 00:48:19.510
we can start to see
ourselves better.

00:48:19.510 --> 00:48:22.930
Bias encoded and taken
to scale becomes visible.

00:48:26.200 --> 00:48:31.910
When we have been arresting
blacks at a much higher rate,

00:48:31.910 --> 00:48:35.860
and not seeing it, it's just
kind of in the woodwork.

00:48:35.860 --> 00:48:38.200
We go, well, it's human
bias, you know, sure.

00:48:38.200 --> 00:48:40.075
But then, all of a
sudden, when you're going,

00:48:40.075 --> 00:48:41.800
wow, we're baking it
into the algorithms,

00:48:41.800 --> 00:48:43.390
because we're using
these training

00:48:43.390 --> 00:48:47.360
datasets that are based on
decades of biased policing.

00:48:47.360 --> 00:48:48.490
We can suddenly see it.

00:48:48.490 --> 00:48:49.690
We can fix it.

00:48:49.690 --> 00:48:52.420
We can debug our society.

00:48:52.420 --> 00:48:55.500
So that's the first thing,
this whole engagement,

00:48:55.500 --> 00:48:57.880
this digitalization
of our world,

00:48:57.880 --> 00:49:02.170
is going to help us understand
our world and make it better.

00:49:02.170 --> 00:49:04.240
And the second
thing, AI is going

00:49:04.240 --> 00:49:07.570
to help us to understand
more deeply what is human.

00:49:07.570 --> 00:49:10.630
And it can create
new kinds of beauty.

00:49:10.630 --> 00:49:15.420
I love this statement from the
37th move of the second game.

00:49:15.420 --> 00:49:18.390
It was a fan who, he
talking about it, he says,

00:49:18.390 --> 00:49:19.600
it's not a human move.

00:49:19.600 --> 00:49:21.400
I've never seen a
human play this move.

00:49:21.400 --> 00:49:23.050
It's so beautiful.

00:49:23.050 --> 00:49:26.410
You know, this idea that
something new that we have not

00:49:26.410 --> 00:49:30.160
imagined can be beautiful,
that it can not be something

00:49:30.160 --> 00:49:32.920
to be afraid of, that it can
be something wonderful-- that's

00:49:32.920 --> 00:49:35.680
the world that I think we
in the technology industry

00:49:35.680 --> 00:49:38.080
should be committed to creating.

00:49:38.080 --> 00:49:41.500
And finally, I just
want to remember

00:49:41.500 --> 00:49:44.500
that it isn't technology
that wants to eliminate jobs.

00:49:44.500 --> 00:49:46.300
My friend Nick Hanauer
said, "Technology is

00:49:46.300 --> 00:49:48.220
the solution to human problems.

00:49:48.220 --> 00:49:51.560
We won't run out of work
until we run out of problems."

00:49:51.560 --> 00:49:53.470
So I think we all,
in our industry,

00:49:53.470 --> 00:49:57.460
have to commit ourselves
to solving human problems.

00:49:57.460 --> 00:49:59.080
Make it work.

00:49:59.080 --> 00:50:01.340
Make the world a better place.

00:50:01.340 --> 00:50:03.250
So that's the master
design pattern

00:50:03.250 --> 00:50:05.920
of technology and, really,
the message of the book.

00:50:05.920 --> 00:50:09.250
Our job is to augment people
so they can do things that

00:50:09.250 --> 00:50:11.480
were previously impossible.

00:50:11.480 --> 00:50:11.980
Thanks.

00:50:11.980 --> 00:50:17.862
[APPLAUSE]

00:50:17.862 --> 00:50:19.820
AUDIENCE: Well, Tim,
thanks so much for coming.

00:50:19.820 --> 00:50:22.270
I'm really excited
to read your book.

00:50:22.270 --> 00:50:23.770
I was just curious,
as someone who's

00:50:23.770 --> 00:50:25.570
been in the publishing
and content industry

00:50:25.570 --> 00:50:28.720
for the past 20
years, why, when you

00:50:28.720 --> 00:50:32.080
think about transmitting these
ideas about the future, why

00:50:32.080 --> 00:50:34.740
you chose to do it in
the form of a book?

00:50:34.740 --> 00:50:37.660
When there's lots of different
tools out there today.

00:50:37.660 --> 00:50:40.300
And so, as somebody who's
thought a lot about this,

00:50:40.300 --> 00:50:41.770
I'm just kind of curious.

00:50:41.770 --> 00:50:44.630
Like, why did you choose to
canonicalize it in this form?

00:50:44.630 --> 00:50:46.338
TIM O'REILLY: That's
a really good point.

00:50:46.338 --> 00:50:49.330
Actually, probably
the trigger was

00:50:49.330 --> 00:50:52.690
that I had been
running this event

00:50:52.690 --> 00:50:55.000
called The Next Economy Summit.

00:50:55.000 --> 00:50:58.360
And I was trying to
get on Michael Krasny's

00:50:58.360 --> 00:51:00.430
show, the local PBS show.

00:51:00.430 --> 00:51:02.230
And I just was having
a lot of trouble.

00:51:02.230 --> 00:51:04.480
And then somebody who had
written a book that I really

00:51:04.480 --> 00:51:07.030
didn't respect
appears on the show.

00:51:07.030 --> 00:51:09.260
And I'm, like, damn.

00:51:09.260 --> 00:51:12.430
For a certain kind of
intellectual discussion,

00:51:12.430 --> 00:51:17.574
it is still the
little entry card.

00:51:17.574 --> 00:51:19.990
I couldn't get in this room
without the Google entry card.

00:51:19.990 --> 00:51:24.760
And writing the book gets you
discussed by the policymakers.

00:51:24.760 --> 00:51:27.100
Because that's the thing,
all these people who

00:51:27.100 --> 00:51:29.080
should be deeply
involved in the digital

00:51:29.080 --> 00:51:31.960
because it's the center
of our world, nah.

00:51:31.960 --> 00:51:34.000
I mean, this is a
book that I can send.

00:51:34.000 --> 00:51:37.780
I can put it in front of a
policy maker, have them read,

00:51:37.780 --> 00:51:40.780
have the think tanks
in DC talk about.

00:51:40.780 --> 00:51:44.110
I can give talks there
because I wrote a book.

00:51:44.110 --> 00:51:48.085
And otherwise, I'm some
weird new media guy.

00:51:48.085 --> 00:51:49.147
AUDIENCE: Yeah.

00:51:49.147 --> 00:51:51.730
AUDIENCE: And of course, you've
been a publisher for 30 years.

00:51:51.730 --> 00:51:52.700
TIM O'REILLY: Yeah,
which is ironic.

00:51:52.700 --> 00:51:54.620
And actually, I didn't
publish it myself.

00:51:54.620 --> 00:51:58.330
I actually went to one of the
traditional business publishing

00:51:58.330 --> 00:52:00.460
companies, because
again, it's just

00:52:00.460 --> 00:52:03.620
this credentialing function.

00:52:03.620 --> 00:52:07.445
AUDIENCE: Tim, excellent set
of hypotheses, but one of them

00:52:07.445 --> 00:52:09.070
is the core of the
actors have changed.

00:52:09.070 --> 00:52:11.740
Obviously, we talked about
big things, like Uber today,

00:52:11.740 --> 00:52:13.204
they started as
very small things.

00:52:13.204 --> 00:52:15.745
Do you have any conclusions you
draw as to whether innovation

00:52:15.745 --> 00:52:17.480
is possible as
things get, quote,

00:52:17.480 --> 00:52:19.860
"more multi-cellular" at
the small stage, garage

00:52:19.860 --> 00:52:21.410
stage, middle start up stage?

00:52:21.410 --> 00:52:23.410
Or would you be
thinking that, as data

00:52:23.410 --> 00:52:25.020
gets a gravity of
its own, that we're

00:52:25.020 --> 00:52:27.450
going to see a return to
bigness for its own sake--

00:52:27.450 --> 00:52:28.241
TIM O'REILLY: Yeah.

00:52:28.241 --> 00:52:31.300
AUDIENCE: --exponential
for startups in this world?

00:52:31.300 --> 00:52:33.400
TIM O'REILLY: Well, I
do think, first of all,

00:52:33.400 --> 00:52:38.220
the power of ideas is profound.

00:52:38.220 --> 00:52:43.840
Even if a startup
doesn't survive,

00:52:43.840 --> 00:52:47.500
it can actually place a
new idea into the world.

00:52:47.500 --> 00:52:49.600
And that idea can be taken up.

00:52:49.600 --> 00:52:54.850
And that is why create
more value than you capture

00:52:54.850 --> 00:52:57.040
is sort of part of my motto.

00:52:57.040 --> 00:53:02.820
I just believe, yes, in the
end, we're all going to fail.

00:53:02.820 --> 00:53:05.260
I've often quoted this
wonderful poem of Rilke, "what

00:53:05.260 --> 00:53:06.700
we fight with is so small.

00:53:06.700 --> 00:53:08.800
And when we win,
it makes us small.

00:53:08.800 --> 00:53:11.650
What we want is to be defeated
decisively by successively

00:53:11.650 --> 00:53:12.400
greater beings."

00:53:12.400 --> 00:53:14.650
He's talking about
wrestling with the angels.

00:53:14.650 --> 00:53:20.170
And I think, so yes, that
may be a risk of bigness.

00:53:20.170 --> 00:53:24.340
But I think, actually,
if you follow

00:53:24.340 --> 00:53:31.050
the job of doing something
that needs doing--

00:53:31.050 --> 00:53:33.670
one of the companies I talk
about at the end of the book,

00:53:33.670 --> 00:53:35.545
where I kind of try and
give some hopefulness

00:53:35.545 --> 00:53:36.880
is the company Zipline.

00:53:36.880 --> 00:53:40.354
So here's a company that's
using drones and on demand.

00:53:40.354 --> 00:53:41.770
And they're not,
like, we're going

00:53:41.770 --> 00:53:44.020
to be the Uber of dry cleaning.

00:53:44.020 --> 00:53:48.670
They're, like, we're going
to be the Uber of blood

00:53:48.670 --> 00:53:55.450
delivery in a country where the
leading cause of death in women

00:53:55.450 --> 00:53:57.310
is postpartum hemorrhage.

00:53:57.310 --> 00:54:00.020
There's no developed
hospital infrastructure.

00:54:00.020 --> 00:54:04.660
There's bad roads.

00:54:04.660 --> 00:54:09.580
But you can get a drone anywhere
in the country in 20 minutes.

00:54:09.580 --> 00:54:12.470
And bang, that's astonishing.

00:54:12.470 --> 00:54:16.060
That's like, wow, we can
solve a problem that's

00:54:16.060 --> 00:54:18.735
a real problem with
this magical technology.

00:54:18.735 --> 00:54:19.360
And guess what?

00:54:19.360 --> 00:54:24.130
They're kind of not there
in that normal competitive

00:54:24.130 --> 00:54:26.110
landscape, because
they've actually

00:54:26.110 --> 00:54:28.360
found this green field
of opportunity, which

00:54:28.360 --> 00:54:31.100
is solving a real problem.

00:54:31.100 --> 00:54:37.120
And I think sure, the me,
too space, pretty crowded.

00:54:37.120 --> 00:54:40.000
If you want to go, yeah, can
I make another social network?

00:54:40.000 --> 00:54:41.699
Kind of hard, prob--

00:54:41.699 --> 00:54:43.240
but there's another
piece to it, too.

00:54:43.240 --> 00:54:46.090
Which I had a debate with
Reid Hoffman recently.

00:54:46.090 --> 00:54:49.060
Because he's got a book
coming out on blitzscaling.

00:54:49.060 --> 00:54:51.405
And I was, like, look, Reid.

00:54:51.405 --> 00:54:52.780
You've got to
raise lots of money

00:54:52.780 --> 00:54:55.450
because you have to grow
faster than your competitors,

00:54:55.450 --> 00:54:56.860
so on and so forth.

00:54:56.860 --> 00:54:58.420
It's still a winner takes all.

00:54:58.420 --> 00:55:01.092
Somebody is going to win, and
somebody is going to lose.

00:55:01.092 --> 00:55:02.800
And one of the things
I want to hear more

00:55:02.800 --> 00:55:07.690
is how are these
platforms enabling

00:55:07.690 --> 00:55:10.750
an ecosystem, which kind of goes
back to this sort of platform

00:55:10.750 --> 00:55:11.670
thinking.

00:55:11.670 --> 00:55:17.170
When you look at how
Google has been an enabler,

00:55:17.170 --> 00:55:18.970
how YouTube is an enabler.

00:55:18.970 --> 00:55:21.610
There's people getting
work from this.

00:55:21.610 --> 00:55:24.340
And I think one of the
things that I think,

00:55:24.340 --> 00:55:26.770
in a world of winner
takes all platforms,

00:55:26.770 --> 00:55:29.140
the platforms themselves
have to think more

00:55:29.140 --> 00:55:31.840
about how are we going to enable
a network of small business.

00:55:31.840 --> 00:55:34.890
[APPLAUSE]

