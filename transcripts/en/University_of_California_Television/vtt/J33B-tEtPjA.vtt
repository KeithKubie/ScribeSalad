WEBVTT
Kind: captions
Language: en

00:00:00.980 --> 00:00:02.510
- [Narrator] This program is presented by

00:00:02.510 --> 00:00:04.750
University of California Television.

00:00:04.750 --> 00:00:05.900
Like what you learn?

00:00:05.900 --> 00:00:09.190
Visit our website or follow
us on Facebook and Twitter

00:00:09.190 --> 00:00:12.020
to keep up with the latest UCTV programs.

00:00:12.020 --> 00:00:14.490
Also, make sure to check out and subscribe

00:00:14.490 --> 00:00:17.900
to our YouTube original
channel, uctvPrime,

00:00:17.900 --> 00:00:19.990
available only on YouTube.

00:00:25.519 --> 00:00:28.436
(electronic music)

00:00:57.251 --> 00:00:59.156
- Welcome, everyone, good afternoon.

00:00:59.156 --> 00:01:02.050
My name is Bruno Olshausen
and today it's my very great

00:01:02.050 --> 00:01:04.920
pleasure to introduce Jeff
Hawkins for his second lecture

00:01:04.920 --> 00:01:06.860
as Hitchcock professor.

00:01:06.860 --> 00:01:10.472
So in 2002, Jeff founded the
Redwood Neuroscience Institute,

00:01:10.472 --> 00:01:13.390
which would focus on developing
a theoretical framework

00:01:13.390 --> 00:01:15.330
for neocortical function.

00:01:15.330 --> 00:01:18.060
I eagerly signed on as one
of his first scientists.

00:01:18.060 --> 00:01:21.400
RNI grew into a intellectually
rich and vibrantly

00:01:21.400 --> 00:01:24.080
resourced institute, and Jeff
was the person who brought

00:01:24.080 --> 00:01:27.220
life to it and made it
an exciting place to be.

00:01:27.220 --> 00:01:29.740
He grew and shaped the institute
into a group of individuals

00:01:29.740 --> 00:01:32.200
sharing a common vision
to develop a theoretical

00:01:32.200 --> 00:01:35.300
framework for thalamocortical
function, and who worked

00:01:35.300 --> 00:01:37.700
interactively on a daily basis.

00:01:37.700 --> 00:01:40.040
We held the weekly seminar
series featuring distinguished

00:01:40.040 --> 00:01:42.170
neuroscientists from across the globe.

00:01:42.170 --> 00:01:44.744
Jeff was known to interrupt
speakers at the beginning

00:01:44.744 --> 00:01:47.390
of their talks as they were
just describing the problem

00:01:47.390 --> 00:01:51.010
they were working on to ask,
why would you want to do that?

00:01:51.010 --> 00:01:53.000
What is it going to tell you?

00:01:53.000 --> 00:01:55.460
This produced a range of
interesting reactions.

00:01:55.460 --> 00:01:58.050
Some perhaps have never
been asked that question

00:01:58.050 --> 00:01:59.800
so point blankly before.

00:01:59.800 --> 00:02:01.940
But almost always, it lead
to an interesting and thought

00:02:01.940 --> 00:02:04.920
provoking discussion, and these
seminars went on literally,

00:02:04.920 --> 00:02:06.090
for hours.

00:02:06.090 --> 00:02:06.950
The speakers loved it.

00:02:06.950 --> 00:02:07.783
We loved it.

00:02:07.783 --> 00:02:09.480
And everyone learned a
lot from these exchanges

00:02:09.480 --> 00:02:11.440
that were in large part
driven by Jeff's intellectual

00:02:11.440 --> 00:02:15.530
curiosity, and his intense
desire to understand the science.

00:02:15.530 --> 00:02:18.640
In 2004, Jeff co-authored
together with Sandra Blasely

00:02:18.640 --> 00:02:22.560
the Book on Intelligence,
which lays out his ideas

00:02:22.560 --> 00:02:24.860
about the cortex as the
hierarchical membrane system

00:02:24.860 --> 00:02:27.490
that learns from the
environment and does prediction.

00:02:27.490 --> 00:02:30.230
This book has inspired countless
students to enter the field

00:02:30.230 --> 00:02:31.340
of neuroscience.

00:02:31.340 --> 00:02:33.790
I know this because my inbox
has been flooded with their

00:02:33.790 --> 00:02:36.140
emails since then on a daily basis.

00:02:36.140 --> 00:02:38.520
And also, I was on the
neuroscience admissions committee,

00:02:38.520 --> 00:02:42.219
and read many of their stories
in their application essays.

00:02:42.219 --> 00:02:46.460
These stories go on, these
stories go something like this:

00:02:46.460 --> 00:02:48.490
I thought I wanted to go into engineering.

00:02:48.490 --> 00:02:50.490
I thought I wanted to go into medicine.

00:02:50.490 --> 00:02:53.380
Or into law, or God forbid, banking.

00:02:53.380 --> 00:02:55.380
And then, I read Jeff's book.

00:02:55.380 --> 00:02:57.710
And now I want to study
neuroscience and work on models

00:02:57.710 --> 00:02:59.760
of the neocortex.

00:02:59.760 --> 00:03:03.870
Some students ended up
interviewing for our program,

00:03:03.870 --> 00:03:06.000
and they arrived asking questions like,

00:03:06.000 --> 00:03:09.040
do you believe in a
common cortical algorithm?

00:03:09.040 --> 00:03:10.810
They got that question from Jeff.

00:03:10.810 --> 00:03:14.370
And, that's a great question
for a student to be asking.

00:03:14.370 --> 00:03:17.000
In 2005, Jeff started
Numenta to push forward

00:03:17.000 --> 00:03:19.020
the development of intelligence
machines based on models

00:03:19.020 --> 00:03:21.070
of brain function, and
today he will tell us more

00:03:21.070 --> 00:03:22.830
about the models they have
developed and the principles

00:03:22.830 --> 00:03:24.420
that make them work.

00:03:24.420 --> 00:03:28.040
In that same year in 2005,
Jeff gifted RNI to UC Berkeley,

00:03:28.040 --> 00:03:30.030
where it now functions as the
Redwood Center for Theoretical

00:03:30.030 --> 00:03:32.300
Neuro Science, part of the
Helen Goals Neurosciences

00:03:32.300 --> 00:03:33.920
Institute.

00:03:33.920 --> 00:03:36.080
The Redwood Center
Endowment funds students

00:03:36.080 --> 00:03:38.440
in the Neuroscience Project
Program in addition to seminars,

00:03:38.440 --> 00:03:40.960
courses, and other programs
at the Redwood Center.

00:03:40.960 --> 00:03:43.270
The center now provides a
rich, intellectual environment

00:03:43.270 --> 00:03:45.107
for students doing work in
computational neuroscience,

00:03:45.107 --> 00:03:47.910
and their now 10 PhD
students who have graduated

00:03:47.910 --> 00:03:49.202
from Berkeley having
done their thesis work

00:03:49.202 --> 00:03:51.110
in the Redwood Center.

00:03:51.110 --> 00:03:52.860
So I would like to thank this
opportunity to thank you,

00:03:52.860 --> 00:03:56.300
Jeff, for what you have
brought to the Berkeley Campus

00:03:56.300 --> 00:03:59.230
not only in the intellectual
contribution, but also helping

00:03:59.230 --> 00:04:00.990
them to create an environment
that enables students

00:04:00.990 --> 00:04:03.460
to learn and to grow and to
contribute to an emerging

00:04:03.460 --> 00:04:05.420
area of neuroscience.

00:04:05.420 --> 00:04:08.310
So please help me in welcoming
Jeff Hawkins for a second

00:04:08.310 --> 00:04:10.323
lecture as Hitchcock Professors.

00:04:11.264 --> 00:04:14.264
(audience clapping)

00:04:16.430 --> 00:04:18.154
- Thank you, Bruno.

00:04:18.154 --> 00:04:20.850
That was a very nice introduction.

00:04:20.850 --> 00:04:22.873
I should tell you a story about Bruno.

00:04:23.950 --> 00:04:26.820
When I started the Redwood
Neuroscience Institute,

00:04:26.820 --> 00:04:28.990
my first question, who would
be crazy enough to come

00:04:28.990 --> 00:04:30.000
work here?

00:04:30.000 --> 00:04:31.440
I mean, it's a new institute!

00:04:31.440 --> 00:04:33.160
You know, how do you open up?

00:04:33.160 --> 00:04:35.370
You put a shingle on the
street and hope neuroscientists

00:04:35.370 --> 00:04:37.620
walk in the door saying
hey, I saw your sign!

00:04:38.780 --> 00:04:41.790
And, it's a very odd career move.

00:04:41.790 --> 00:04:46.550
You're going to a non-affiliated
institute, and you know,

00:04:46.550 --> 00:04:49.333
it's not leading to tenure
or anything like that.

00:04:50.260 --> 00:04:52.760
But, the people who really were passionate

00:04:52.760 --> 00:04:55.132
about understanding how the
brain works and the neocortex

00:04:55.132 --> 00:04:57.890
were willing to take that gamble.

00:04:57.890 --> 00:04:59.510
And Bruno was one of those guys.

00:04:59.510 --> 00:05:01.080
As he said, he signed right up.

00:05:01.080 --> 00:05:02.110
He says I want to do this.

00:05:02.110 --> 00:05:03.215
I care about this.

00:05:03.215 --> 00:05:06.550
Bruno became my right hand
man, or my other body,

00:05:06.550 --> 00:05:08.956
my doppler game or whatever
you want to call it.

00:05:08.956 --> 00:05:12.660
And, the great thing about
Bruno is, is he has this

00:05:12.660 --> 00:05:14.700
encyclopedic mind.

00:05:14.700 --> 00:05:16.980
He knows everything, everybody.

00:05:16.980 --> 00:05:18.397
It's like he can memorize dates,

00:05:18.397 --> 00:05:20.630
and I can't remember anything.

00:05:20.630 --> 00:05:22.840
So I just follow him around
and say Bruno, who wrote

00:05:22.840 --> 00:05:24.945
that paper about such and
such, and he goes oh that

00:05:24.945 --> 00:05:26.580
was so and so in 1964.

00:05:26.580 --> 00:05:27.600
Great, thank you.

00:05:27.600 --> 00:05:29.510
Bruno, who should we have to
speak about the basil ganglia?

00:05:29.510 --> 00:05:31.040
We should have these three people, great!

00:05:31.040 --> 00:05:32.100
Let's do that.

00:05:32.100 --> 00:05:34.840
So, been great collaboration,
I'm happy to have

00:05:34.840 --> 00:05:37.557
the introduction, be introduced by Bruno.

00:05:37.557 --> 00:05:41.180
Yesterday, I have two talks.

00:05:41.180 --> 00:05:42.250
And these are the titles.

00:05:42.250 --> 00:05:44.130
Intelligence in the Brain, that
was the title of yesterday's

00:05:44.130 --> 00:05:46.370
talk, and Intelligence and
Machines of the Brain in today's

00:05:46.370 --> 00:05:47.850
talk overlap.

00:05:47.850 --> 00:05:49.020
And I will talk about that in a bit.

00:05:49.020 --> 00:05:51.170
But I just want to ask
something of you right now.

00:05:51.170 --> 00:05:53.830
I'd like to know who in the
audience was not here yesterday?

00:05:53.830 --> 00:05:57.350
So if you were not here
yesterday, please raise your hand.

00:05:57.350 --> 00:05:58.700
Okay, that's a fair number.

00:06:00.340 --> 00:06:04.540
There is an overlap between
these talks, and I'm gonna,

00:06:04.540 --> 00:06:07.120
I have to decide how much
to repeat myself and so on,

00:06:07.120 --> 00:06:08.893
but I'll try to manage that.

00:06:10.992 --> 00:06:14.790
Okay, just to remind you
for those that were here,

00:06:14.790 --> 00:06:18.010
I can't imagine doing anything
else than studying the brain.

00:06:18.010 --> 00:06:22.300
To me, this is the most
interesting thing one can do.

00:06:22.300 --> 00:06:23.500
We are our brains.

00:06:23.500 --> 00:06:25.060
Everything we do with our brains.

00:06:25.060 --> 00:06:26.180
Our life is our brains.

00:06:26.180 --> 00:06:28.240
Our questions is the
product of our brains.

00:06:28.240 --> 00:06:30.140
Our knowledge is the
product of our brains.

00:06:30.140 --> 00:06:32.740
Our art, our science, our
literature is what we are.

00:06:34.074 --> 00:06:35.930
The ability to even
know anything of course,

00:06:35.930 --> 00:06:37.490
is a product of our brains.

00:06:37.490 --> 00:06:40.186
And so to know what humanity
is, you really should,

00:06:40.186 --> 00:06:43.083
I just wanted to know what am I?

00:06:43.083 --> 00:06:45.110
How do I do this?

00:06:45.110 --> 00:06:46.210
How did we come about?

00:06:46.210 --> 00:06:47.330
And where's all this gonna go?

00:06:47.330 --> 00:06:49.930
So, that's where my interest
in this field came from.

00:06:49.930 --> 00:06:52.863
Just deep curiosity about
myself and other humans.

00:06:52.863 --> 00:06:56.760
And then, as I got into it,
I realized that you could

00:06:56.760 --> 00:06:57.593
build machines.

00:06:57.593 --> 00:06:59.310
So, they work on those principles.

00:06:59.310 --> 00:07:01.750
So, I started out, let's
see, I had little problems

00:07:01.750 --> 00:07:02.871
with this yesterday.

00:07:02.871 --> 00:07:04.610
Hopefully it won't be a problem today.

00:07:04.610 --> 00:07:06.680
Okay, I started out
basically trying to answer

00:07:06.680 --> 00:07:07.530
these two questions.

00:07:07.530 --> 00:07:09.930
I said, okay, how can we
discover operating principles

00:07:09.930 --> 00:07:11.090
of the neocortex?

00:07:11.090 --> 00:07:13.620
And then I realized once we
understood those operating

00:07:13.620 --> 00:07:15.210
principles of the brain,
we could build machines

00:07:15.210 --> 00:07:16.630
that work on those principles.

00:07:16.630 --> 00:07:18.300
This is the order in
which I care about them,

00:07:18.300 --> 00:07:19.703
but they go hand in hand.

00:07:20.610 --> 00:07:21.890
So there you have it.

00:07:21.890 --> 00:07:23.770
Just to remind you, for those
of you that weren't here

00:07:23.770 --> 00:07:26.390
yesterday, the neocortex
is about 60% of the volume

00:07:26.390 --> 00:07:27.223
of the human brain.

00:07:27.223 --> 00:07:28.720
It's the big wrinkly thing on top.

00:07:28.720 --> 00:07:31.200
And it's the location of
all high level thought.

00:07:31.200 --> 00:07:34.530
Language, science, high
level motor planning.

00:07:34.530 --> 00:07:36.080
Anything you can tell me about,

00:07:36.080 --> 00:07:38.800
is stored in your neocortex pretty much.

00:07:38.800 --> 00:07:41.510
And so, it's the system we're
interested in when we wanna

00:07:41.510 --> 00:07:44.290
deal with intelligence.

00:07:44.290 --> 00:07:46.830
So, this is the process again,
I showed this slide yesterday

00:07:46.830 --> 00:07:48.511
this is the process I go about my work.

00:07:48.511 --> 00:07:53.511
I start with anatomy and
physiology, this is the detailed,

00:07:53.790 --> 00:07:57.850
very detailed knowledge about
how brains are constructed.

00:07:57.850 --> 00:08:00.385
We use that as a set of
constraints on our theories.

00:08:00.385 --> 00:08:03.730
Then we develop a principle
for how which that works.

00:08:03.730 --> 00:08:07.080
And then finally we can
develop hardware and software

00:08:07.080 --> 00:08:08.380
solutions that implement these things

00:08:08.380 --> 00:08:09.750
to build intelligent machines.

00:08:09.750 --> 00:08:12.960
Yesterday's talk was about
the first two of these items.

00:08:12.960 --> 00:08:14.870
And today's talk I'm gonna
have a little bit of a review

00:08:14.870 --> 00:08:17.000
on the principles, and then
I'll talk about the software

00:08:17.000 --> 00:08:19.120
and where this is going in the future.

00:08:19.120 --> 00:08:21.370
So here's my outline for my talk today.

00:08:21.370 --> 00:08:23.580
I'm gonna do a brief history
of Machine Intelligence.

00:08:23.580 --> 00:08:25.310
Very brief, I'm not trying to be complete.

00:08:25.310 --> 00:08:27.809
It's very opinionated, so
just take it for what it is.

00:08:27.809 --> 00:08:31.160
I'm gonna then tell you how I
define Machine Intelligence.

00:08:31.160 --> 00:08:32.500
What are we trying to do?

00:08:32.500 --> 00:08:35.040
How will we know if we
achieve it in some sense?

00:08:35.040 --> 00:08:37.050
And then I want to do a little
bit of a review about what

00:08:37.050 --> 00:08:39.220
we've learned about how
the neocortex works.

00:08:39.220 --> 00:08:40.250
And then I'm gonna talk about Grok.

00:08:40.250 --> 00:08:42.550
Grok is a product we've built.

00:08:42.550 --> 00:08:45.560
And it's an illustrative
product because it's based

00:08:45.560 --> 00:08:48.082
on these principles and it'll
tell you what we can do today.

00:08:48.082 --> 00:08:50.360
And then I'm gonna talk
briefly about the future

00:08:50.360 --> 00:08:53.690
of Machine Intelligence,
and answer in the end,

00:08:53.690 --> 00:08:56.320
why should we do this, why do we care.

00:08:56.320 --> 00:08:57.537
I'm gonna start with our brief history.

00:08:57.537 --> 00:09:01.370
Now this man, Alan Turing,
very famous, he was a British

00:09:01.370 --> 00:09:03.970
mathematician, most of you
probably have heard of him.

00:09:03.970 --> 00:09:05.780
He was one of the guys
who actually helped us win

00:09:05.780 --> 00:09:06.830
World War II.

00:09:06.830 --> 00:09:09.710
He was a key in deciphering
the Enigma Machine, which was

00:09:09.710 --> 00:09:11.500
an encryption machine for the Germans.

00:09:11.500 --> 00:09:13.199
A real hero.

00:09:13.199 --> 00:09:15.130
But he was interested, he was a founder

00:09:15.130 --> 00:09:15.963
of computing science.

00:09:15.963 --> 00:09:20.030
And he wrote a paper starting
in 1935 and then continuing

00:09:20.030 --> 00:09:22.770
on from that, which is really
a big foundational principle

00:09:22.770 --> 00:09:23.938
about how computers work.

00:09:23.938 --> 00:09:26.100
Now, this quote is not from Alan Turing.

00:09:26.100 --> 00:09:29.290
This is my paraphrasing
about what his paper starting

00:09:29.290 --> 00:09:30.900
in 1935 was all about.

00:09:30.900 --> 00:09:34.670
He basically said you know,
computers are actually

00:09:34.670 --> 00:09:37.410
a universal machine,
they can model anything.

00:09:37.410 --> 00:09:39.680
If there's anything you want in the world

00:09:39.680 --> 00:09:40.813
that can be modeled, computers can do it.

00:09:40.813 --> 00:09:42.637
There's no other reason
to do anything else.

00:09:42.637 --> 00:09:46.210
And all computers are essentially
the same, so any computer

00:09:46.210 --> 00:09:47.990
can do this, they're universal machines.

00:09:47.990 --> 00:09:50.500
And this is really a powerful idea.

00:09:50.500 --> 00:09:52.980
He then wrote another paper in 1950.

00:09:52.980 --> 00:09:55.300
Now he was very, very interested
in Machine Intelligence.

00:09:55.300 --> 00:09:57.970
He said oh, we can build
intelligent machines.

00:09:57.970 --> 00:09:59.470
And he wrote this paper in 1950.

00:09:59.470 --> 00:10:01.490
There is a picture of
it on the right there,

00:10:01.490 --> 00:10:02.323
the cover on it.

00:10:02.323 --> 00:10:04.490
It was called Computing
Machinery and Intelligence,

00:10:04.490 --> 00:10:06.760
and in here he basically
said you know what,

00:10:06.760 --> 00:10:08.056
we can build intelligent machines.

00:10:08.056 --> 00:10:10.280
But he was worried about something.

00:10:10.280 --> 00:10:12.480
He was afraid that people
were gonna get in endless

00:10:12.480 --> 00:10:14.700
arguments about whether
this could be done or not,

00:10:14.700 --> 00:10:17.690
whether you know is there a
soul in the machine and so on,

00:10:17.690 --> 00:10:18.570
he just didn't want to get into all this.

00:10:18.570 --> 00:10:19.403
He said this.

00:10:19.403 --> 00:10:20.236
He wrote this in the paper.

00:10:20.236 --> 00:10:22.090
He said, I don't want to get
into arguments with people.

00:10:22.090 --> 00:10:24.410
He said, let's just agree on the founding.

00:10:24.410 --> 00:10:26.600
I'm gonna come up with a test
for machine intelligence,

00:10:26.600 --> 00:10:30.050
and if we pass this test,
then we'll just have to agree,

00:10:30.050 --> 00:10:31.860
you know, proposing that we
can agree that the machine

00:10:31.860 --> 00:10:32.860
is intelligent.

00:10:32.860 --> 00:10:35.340
And the test that he
proposed is now what we call

00:10:35.340 --> 00:10:36.321
the Turing Test.

00:10:36.321 --> 00:10:39.740
And, the basic idea of the
Turing Test is shown in this

00:10:39.740 --> 00:10:40.770
little picture in the bottom here.

00:10:40.770 --> 00:10:43.242
It's just you know, he
called it the imitation game

00:10:43.242 --> 00:10:47.650
which someone sits a tele-typer
terminal and types questions

00:10:47.650 --> 00:10:49.540
to a computer and a human.

00:10:49.540 --> 00:10:51.150
And then he looks at the
answers, or she looks

00:10:51.150 --> 00:10:53.127
at the answers, and trying to
decide which is the computer

00:10:53.127 --> 00:10:54.080
and which is the human.

00:10:54.080 --> 00:10:55.820
If he can't decide, then we have to say

00:10:55.820 --> 00:10:56.911
the computer's intelligent.

00:10:56.911 --> 00:11:00.574
This is clever, but a really
bad idea in my opinion.

00:11:00.574 --> 00:11:04.870
It set things up for a lot
of problems going forward.

00:11:04.870 --> 00:11:07.020
First of all as I say here
at the top of the slide,

00:11:07.020 --> 00:11:10.050
it kinda set the bar of
that machine intelligence

00:11:10.050 --> 00:11:11.560
is about the human behavior.

00:11:11.560 --> 00:11:13.630
Is like we'll know when we
have machine intelligence

00:11:13.630 --> 00:11:16.630
when it's, you can't tell
the difference between

00:11:16.630 --> 00:11:18.380
an intelligent machine and a human.

00:11:18.380 --> 00:11:19.793
Well, there's a lot of problems with that.

00:11:19.793 --> 00:11:22.460
First of all, you can imagine
very, very intelligent

00:11:22.460 --> 00:11:24.289
machines that would not
pass the Turing Test.

00:11:24.289 --> 00:11:26.660
That maybe speak another language.

00:11:26.660 --> 00:11:28.680
Or don't speak a language
at all that are still very,

00:11:28.680 --> 00:11:29.660
very intelligent.

00:11:29.660 --> 00:11:31.560
We have lots of intelligence
in other species

00:11:31.560 --> 00:11:34.370
on this planet: dogs,
cats, monkeys, dolphins.

00:11:34.370 --> 00:11:36.190
None of them would pass the Turing Test.

00:11:36.190 --> 00:11:37.870
On the other hand, you can
imagine some of the machines

00:11:37.870 --> 00:11:39.240
that might pass the
Turing Test that really

00:11:39.240 --> 00:11:40.850
aren't intelligent at all.

00:11:40.850 --> 00:11:42.240
So this set us up.

00:11:42.240 --> 00:11:45.410
But these two principles
together based that computers

00:11:45.410 --> 00:11:48.910
can do anything, including
model the mind, model the brain,

00:11:48.910 --> 00:11:51.965
plus our ideas to
replicate human behavior,

00:11:51.965 --> 00:11:55.360
was really the foundational
principles are what came

00:11:55.360 --> 00:11:57.109
of the Artificial Intelligence movement.

00:11:57.109 --> 00:11:59.267
And, this went on for many years.

00:11:59.267 --> 00:12:02.070
I'm gonna summarize the
Artificial Intelligence world

00:12:02.070 --> 00:12:02.903
in one slide.

00:12:02.903 --> 00:12:06.160
Don't want to do anything
rude about that, there's a lot

00:12:06.160 --> 00:12:08.180
of great work went into this field.

00:12:08.180 --> 00:12:10.544
Notice I call this non, no neuroscience.

00:12:10.544 --> 00:12:12.090
In essence, that's my definition of AI.

00:12:12.090 --> 00:12:13.820
People trying to create
intelligent machine,

00:12:13.820 --> 00:12:16.244
but just don't look at brains,
don't care about brains,

00:12:16.244 --> 00:12:17.980
don't think about neuroscience at all.

00:12:17.980 --> 00:12:20.660
On the left, I listed
just a partial list of all

00:12:20.660 --> 00:12:22.670
these different initiative
techniques people have had

00:12:22.670 --> 00:12:23.503
over the years.

00:12:23.503 --> 00:12:25.350
I don't need to go into
them, they covered everything

00:12:25.350 --> 00:12:27.610
from robotics, there's a
picture of Shakey the Robot,

00:12:27.610 --> 00:12:29.020
that was one of the first robots.

00:12:29.020 --> 00:12:30.840
And then a recent one, Azanog.

00:12:30.840 --> 00:12:33.400
We have the famous idea
computer playing chess against

00:12:33.400 --> 00:12:35.792
Casper Aug, we not have Google
driving cars which we can

00:12:35.792 --> 00:12:37.825
do pretty soon, that's pretty cool.

00:12:37.825 --> 00:12:39.840
There's a little picture of these blocks,

00:12:39.840 --> 00:12:41.888
see that was something
called Blocks World.

00:12:41.888 --> 00:12:43.860
It's about natural language
processing, you ask questions

00:12:43.860 --> 00:12:44.693
about that.

00:12:44.693 --> 00:12:45.890
These are all things that
happened over the years.

00:12:45.890 --> 00:12:48.699
And recently we had IBM
and another computer

00:12:48.699 --> 00:12:50.150
playing Jeopardy.

00:12:50.150 --> 00:12:51.880
That was pretty cool.

00:12:51.880 --> 00:12:54.370
There also have been major AI initiative.

00:12:54.370 --> 00:12:57.460
The MIT AI Lab existed decades,
it's still in existence,

00:12:57.460 --> 00:12:58.293
excuse me.

00:12:58.293 --> 00:13:01.080
Which was the center of
AI research in the world.

00:13:01.080 --> 00:13:03.711
There's something called a 5th
Generation Computing Project.

00:13:03.711 --> 00:13:04.544
You might have heard of this.

00:13:04.544 --> 00:13:06.960
This is from Japan in 1982.

00:13:06.960 --> 00:13:09.210
And Japan announced that
they're gonna build machines

00:13:09.210 --> 00:13:11.580
that are smart or smarter
than humans, indistinguishable

00:13:11.580 --> 00:13:13.350
by humans in all ways.

00:13:13.350 --> 00:13:15.600
This scared the bajesus out of
people in the United States.

00:13:15.600 --> 00:13:17.670
Says oh my gosh, this was the
time when Japan was really

00:13:17.670 --> 00:13:19.020
rising into technology.

00:13:19.020 --> 00:13:22.450
And people says Japan's
gonna pass us in technology

00:13:22.450 --> 00:13:25.130
capabilities, and this got
a lot of people riled up.

00:13:25.130 --> 00:13:29.677
The next year in 1983, DARPA,
our military research group,

00:13:29.677 --> 00:13:32.120
they created the Strategic
Computing Initiative

00:13:32.120 --> 00:13:33.620
which went on for about 10 years.

00:13:33.620 --> 00:13:36.210
Billions of dollars were put
into these two initiatives.

00:13:36.210 --> 00:13:37.295
They failed miserably.

00:13:37.295 --> 00:13:38.637
We learned some things from them,

00:13:38.637 --> 00:13:40.220
but they failed miserably though.

00:13:40.220 --> 00:13:41.940
Then there was recently, you
might remember, the DARPA

00:13:41.940 --> 00:13:42.773
Grand Challenge.

00:13:42.773 --> 00:13:44.400
This the one to get the cars
to drive through the desert.

00:13:44.400 --> 00:13:46.020
That was very successful.

00:13:46.020 --> 00:13:48.450
So, there's a whole bunch of
things that have been going on

00:13:48.450 --> 00:13:50.310
here for decades, and if
you tried to follow this,

00:13:50.310 --> 00:13:51.160
it's very confusing.

00:13:51.160 --> 00:13:52.342
All these things, different ways.

00:13:52.342 --> 00:13:54.693
Here's how I summarize these things.

00:13:57.420 --> 00:13:59.540
These are good solutions,
many of them were very good

00:13:59.540 --> 00:14:02.130
solutions, but they're very task specific.

00:14:02.130 --> 00:14:03.620
They weren't general purpose solutions.

00:14:03.620 --> 00:14:07.200
Google self driving car
can't fold your laundry.

00:14:07.200 --> 00:14:09.675
And you know, it can't cook dinner.

00:14:09.675 --> 00:14:11.750
It's a very specific things.

00:14:11.750 --> 00:14:14.030
These were merely program solutions.

00:14:14.030 --> 00:14:17.940
Engineers design program
solutions, and with very limited

00:14:17.940 --> 00:14:19.530
learning, some of them
have learning capabilities,

00:14:19.530 --> 00:14:22.290
but generally, fairly
limited learning abilities.

00:14:22.290 --> 00:14:25.260
And, if you talk to AI
researchers, they will universally

00:14:25.260 --> 00:14:27.520
tell you one of the
biggest problems they had

00:14:27.520 --> 00:14:28.790
is knowledge representation.

00:14:28.790 --> 00:14:30.990
They couldn't figure out, and
they still can't figure out,

00:14:30.990 --> 00:14:34.520
how do you get knowledge about
the world into a computer.

00:14:34.520 --> 00:14:36.440
And you think, what does he
mean knowledge about the world?

00:14:36.440 --> 00:14:38.650
Like, well, what's a car,
and what does a car do?

00:14:38.650 --> 00:14:40.167
And what are all the attributes of a car?

00:14:40.167 --> 00:14:42.600
All the different types
of cars, et. cetera.

00:14:42.600 --> 00:14:46.080
It's a huge list of information
we kinda know, our brains

00:14:46.080 --> 00:14:48.480
know, but it's very, very
difficult to get computers

00:14:48.480 --> 00:14:49.313
to do this.

00:14:49.313 --> 00:14:50.900
And this is probably one
of the primary problems

00:14:50.900 --> 00:14:53.423
of AI research has had over the years.

00:14:53.423 --> 00:14:55.550
Now we're gonna switch gears
and talk about two 'nother

00:14:55.550 --> 00:14:58.140
guys back in the early
part of the 20th Century.

00:14:58.140 --> 00:15:00.731
This is Warren McCulloch and Walter Pitts.

00:15:00.731 --> 00:15:05.210
They also wrote a very important
seminal paper back in 1943.

00:15:05.210 --> 00:15:08.030
Here's a picture of it, had
a more, had a highfalutin

00:15:08.030 --> 00:15:10.807
title of A Logical
Calculus of Ideas Eminent

00:15:10.807 --> 00:15:13.650
in Nervous Activity.

00:15:13.650 --> 00:15:17.550
Not as simple as Alan Turing's
title, but the basic idea

00:15:17.550 --> 00:15:19.064
in here is they said you know what?

00:15:19.064 --> 00:15:23.240
We can think about neurons
in brains like computers.

00:15:23.240 --> 00:15:24.073
Here's what they said.

00:15:24.073 --> 00:15:26.630
They said okay, we know about
neurons, and we know that

00:15:26.630 --> 00:15:29.440
these are cells in the brain,
and we know they process

00:15:29.440 --> 00:15:32.030
information and they get these
inputs on their synapses.

00:15:32.030 --> 00:15:34.140
And they say, well let's
model that with a simple

00:15:34.140 --> 00:15:34.973
sort of model.

00:15:34.973 --> 00:15:38.220
These are the guys that first
invented the idea of a model

00:15:38.220 --> 00:15:39.550
neuron, or an artificial neuron.

00:15:39.550 --> 00:15:41.166
This is the first time this appeared.

00:15:41.166 --> 00:15:45.190
And, this basic model they
outlined is one that's still

00:15:45.190 --> 00:15:47.000
used all over the place today.

00:15:47.000 --> 00:15:49.220
And it says here, it says
well a neuron's like a cell

00:15:49.220 --> 00:15:50.429
that has these inputs this.

00:15:50.429 --> 00:15:52.850
Well, these neurons have
weights on them of sum of them

00:15:52.850 --> 00:15:55.040
together and if they above a
certain threshold, the neuron

00:15:55.040 --> 00:15:57.160
fires and has an activation schedule.

00:15:57.160 --> 00:15:58.517
This is as best as they
could do at the time.

00:15:58.517 --> 00:16:00.045
And it was pretty good.

00:16:00.045 --> 00:16:03.180
We now know this was a very
insufficient model of the neuron

00:16:03.180 --> 00:16:05.630
this was very much unlike real neurons.

00:16:05.630 --> 00:16:07.720
I'll talk a little bit more
about that in a second.

00:16:07.720 --> 00:16:10.550
But at the time it was the
best they could do, but it has

00:16:10.550 --> 00:16:11.611
some fundamental flaws.

00:16:11.611 --> 00:16:13.770
Then they went on to do
something more amazing,

00:16:13.770 --> 00:16:15.780
or interesting, or perhaps regrettable.

00:16:15.780 --> 00:16:19.200
They said well look, if we
design these neurons in a very,

00:16:19.200 --> 00:16:22.480
very weird way and put just
a few inputs of nothing

00:16:22.480 --> 00:16:25.790
neuron-like at all, we can
turn them into logic cases

00:16:25.790 --> 00:16:27.430
like ands, and ors, and nots.

00:16:27.430 --> 00:16:30.160
This is the basic language of computers.

00:16:30.160 --> 00:16:32.060
And then they can say why, guess what!

00:16:32.060 --> 00:16:36.240
You can make neuron-like
things, not very neuron-like,

00:16:36.240 --> 00:16:38.510
but we can call them artificial
neurons, and you can build

00:16:38.510 --> 00:16:40.100
entire computers with these things.

00:16:40.100 --> 00:16:41.920
You can do anything, and
therefore it's a universal

00:16:41.920 --> 00:16:42.951
Turing Machine again.

00:16:42.951 --> 00:16:45.600
And, so they saying,
well, let's look at that.

00:16:45.600 --> 00:16:47.550
And so people got this idea
like wow, let's use these

00:16:47.550 --> 00:16:50.790
artificial neurons and try to
build you know, Intelligent

00:16:50.790 --> 00:16:53.470
Machines using these artificial neurons.

00:16:53.470 --> 00:16:56.670
And so that in some ways
was the genesis of the whole

00:16:56.670 --> 00:16:58.870
field of artificial neural networks.

00:16:58.870 --> 00:17:03.430
Which is an entire genre
of machine intelligence

00:17:03.430 --> 00:17:05.314
that's been going on for decades.

00:17:05.314 --> 00:17:09.700
I call this minimal neuroscience
because it has this image

00:17:09.700 --> 00:17:12.790
to the neuron, but it's not
a very realistic neuron,

00:17:12.790 --> 00:17:16.840
not close, and beyond that,
they almost completely ignore,

00:17:16.840 --> 00:17:20.780
most of these models, 90 some
percent of them, completely

00:17:20.780 --> 00:17:23.630
ignore the anatomy and
physiology of the brain

00:17:23.630 --> 00:17:25.700
and all the details we have
about how brains are actually

00:17:25.700 --> 00:17:27.371
built, they just built
these simple networks.

00:17:27.371 --> 00:17:30.850
And I've shown here some pictures
of some of the artificial

00:17:30.850 --> 00:17:32.286
neural networks you'd
see in the literature.

00:17:32.286 --> 00:17:36.570
They have lots of similar characteristics.

00:17:36.570 --> 00:17:39.090
There are some names of
different types you might see.

00:17:39.090 --> 00:17:41.127
There's plenty of them: back
propagation, perceptrons

00:17:41.127 --> 00:17:43.440
was one of the first ones,
Bolzman machines, Hopfield

00:17:43.440 --> 00:17:44.720
networks, Kohonen networks.

00:17:44.720 --> 00:17:47.060
There's two books that came
out in the mid-80's called

00:17:47.060 --> 00:17:49.850
the Parallel Distributed
Processing Books, which created

00:17:49.850 --> 00:17:52.833
a whole flurry of activity in this field.

00:17:52.833 --> 00:17:55.440
And there was a PDP society and so on.

00:17:55.440 --> 00:17:56.530
That was going on.

00:17:56.530 --> 00:17:57.950
Here's how I summarize them.

00:17:57.950 --> 00:18:00.060
The good thing about these is
that they're learning systems.

00:18:00.060 --> 00:18:02.820
They essentially say we
need to learn from extremes

00:18:02.820 --> 00:18:04.700
of data, we need to learn
from data being presented

00:18:04.700 --> 00:18:05.800
to these things.

00:18:05.800 --> 00:18:09.190
They were useful if they're
still used in many applications.

00:18:09.190 --> 00:18:10.730
But they're very, very limited.

00:18:10.730 --> 00:18:12.200
They don't do much.

00:18:12.200 --> 00:18:14.400
They're essentially
classifiers of various sorts.

00:18:14.400 --> 00:18:15.880
You can give a pattern
that can sort of match it

00:18:15.880 --> 00:18:17.200
to something else.

00:18:17.200 --> 00:18:20.693
And again, they're not
brain-like at all, really.

00:18:21.900 --> 00:18:26.170
So, no one, AI and Artificial
Neural Network are not even

00:18:26.170 --> 00:18:28.280
close to producing
something we would say wow,

00:18:28.280 --> 00:18:30.180
that's an intelligent machine.

00:18:30.180 --> 00:18:34.000
We have, it's just, a huge
gap between where these fields

00:18:34.000 --> 00:18:36.160
are and what most people would
think about when they think

00:18:36.160 --> 00:18:37.860
about intelligent machines.

00:18:37.860 --> 00:18:42.060
So, this lately has been
another activity in this area

00:18:42.060 --> 00:18:43.120
you might want to know about.

00:18:43.120 --> 00:18:45.220
This is, I'm only going
to give one example.

00:18:45.220 --> 00:18:47.660
This is an interest that
people try to do whole brain

00:18:47.660 --> 00:18:50.500
stimulation, they say wow if
you have all this computing

00:18:50.500 --> 00:18:53.850
power and all this knowledge
about neuroscience,

00:18:53.850 --> 00:18:57.470
let's model the brain.

00:18:57.470 --> 00:19:00.720
And the premier example of
this is something called

00:19:00.720 --> 00:19:03.660
the Human Brain Project
which is centered in Europe

00:19:03.660 --> 00:19:06.670
and throughout Europe, and
it's very, very ambitious.

00:19:06.670 --> 00:19:08.840
The Human Brain Project is
trying to model a brain,

00:19:08.840 --> 00:19:12.850
all the way down from ion
channels and synapses and neurons

00:19:12.850 --> 00:19:15.895
and the entire brain and the
entire structure of everything.

00:19:15.895 --> 00:19:18.490
That's if I'm left here
for different scales,

00:19:18.490 --> 00:19:19.500
this is one of their brochures.

00:19:19.500 --> 00:19:21.600
All of this in scales in which
they're trying to deal with.

00:19:21.600 --> 00:19:23.560
That rosette in the center,
which you can't read

00:19:23.560 --> 00:19:26.480
any of the details on, those
are, each one of those leaf

00:19:26.480 --> 00:19:28.820
nodes on that picture is
a scientist who is one

00:19:28.820 --> 00:19:31.200
of the principal
investigators on this project.

00:19:31.200 --> 00:19:33.540
You can see there's hundreds
of them, hundreds of scientists

00:19:33.540 --> 00:19:35.840
in all these different
fields are coming together

00:19:35.840 --> 00:19:37.615
to build this monster brain simulator.

00:19:37.615 --> 00:19:41.350
Now on the right is a picture
of one of the early models

00:19:41.350 --> 00:19:42.960
they did from when they
were working with IBM

00:19:42.960 --> 00:19:43.793
called a Blue Brain.

00:19:43.793 --> 00:19:44.921
These are cool.

00:19:44.921 --> 00:19:45.754
I went and saw this.

00:19:45.754 --> 00:19:47.810
There's these walls of pictures,
you're surrounded by walls

00:19:47.810 --> 00:19:50.610
of these neurons are
being projected and spikes

00:19:50.610 --> 00:19:52.000
are going around.

00:19:52.000 --> 00:19:52.863
It's very, very impressive.

00:19:52.863 --> 00:19:55.560
Now, it's interesting
when you think about this,

00:19:55.560 --> 00:19:56.870
and they won't disagree with this.

00:19:56.870 --> 00:19:59.584
This is not a criticism,
they want this description.

00:19:59.584 --> 00:20:03.040
There's actually, first of
all there's no theory here.

00:20:03.040 --> 00:20:05.870
There's no expert, they have
no idea what this should

00:20:05.870 --> 00:20:08.970
be doing, they're just
making a software simulation

00:20:08.970 --> 00:20:11.080
of millions of neurons.

00:20:11.080 --> 00:20:14.350
And they're turning it on
and saying what happens?

00:20:14.350 --> 00:20:16.200
Well, they might be learning
something from that.

00:20:16.200 --> 00:20:17.370
But there's really no theory.

00:20:17.370 --> 00:20:20.139
And I just gotta believe you're
not gonna make any success

00:20:20.139 --> 00:20:22.970
if you have no idea what
this thing should do,

00:20:22.970 --> 00:20:26.000
or what pieces are important,
what pieces aren't important,

00:20:26.000 --> 00:20:28.200
what things are essential,
what things are not essential.

00:20:28.200 --> 00:20:29.500
You're just not gonna get it right.

00:20:29.500 --> 00:20:32.600
It's just impossible, and
that's my opinion about that.

00:20:32.600 --> 00:20:34.720
And they don't really disagree with that.

00:20:34.720 --> 00:20:36.780
The other thing is, there's
really no attempt at machine

00:20:36.780 --> 00:20:38.860
intelligence, we realize it's
not gonna happen this way

00:20:38.860 --> 00:20:42.290
because they really don't
know what you should do

00:20:42.290 --> 00:20:43.490
and how it's gonna work.

00:20:43.490 --> 00:20:44.340
But it's interesting.

00:20:44.340 --> 00:20:46.830
They're not viewing this
as a way of saying okay,

00:20:46.830 --> 00:20:49.560
we're gonna model the brain
in this huge simulator,

00:20:49.560 --> 00:20:51.820
we're gonna learn all kinds of
things maybe about diseases,

00:20:51.820 --> 00:20:54.920
it's gonna be a tool for
scientists, and maybe we'll figure

00:20:54.920 --> 00:20:56.890
out how intelligence comes
out of this a little bit

00:20:56.890 --> 00:20:57.910
down the road.

00:20:57.910 --> 00:21:00.100
So, that's pretty cool, very interesting.

00:21:00.100 --> 00:21:02.500
A lot of money and effort
being spent on these

00:21:02.500 --> 00:21:03.333
kind of things.

00:21:03.333 --> 00:21:04.810
This is not the only initiative,
there's some other ones

00:21:04.810 --> 00:21:07.970
that are similar to this, but
this is the one that is most

00:21:07.970 --> 00:21:09.430
in the news these days.

00:21:09.430 --> 00:21:12.370
Now, you might not, now I'm
not a big fan of all of these

00:21:12.370 --> 00:21:14.310
as a way of getting to
machine intelligence.

00:21:14.310 --> 00:21:17.002
I think we need a different
way of thinking about it,

00:21:17.002 --> 00:21:18.055
a different way of looking at it.

00:21:18.055 --> 00:21:19.810
And, you shouldn't be surprised.

00:21:19.810 --> 00:21:22.220
You know, what would by my
ultimate approach to this?

00:21:22.220 --> 00:21:24.810
Well, you've already seen it,
it's this stuff right here.

00:21:24.810 --> 00:21:27.060
My alternate approach is you first start

00:21:27.060 --> 00:21:29.150
with the neuroscience,
start with the brain,

00:21:29.150 --> 00:21:31.720
use that as a set of constraints,
build those principles,

00:21:31.720 --> 00:21:33.541
and then once you have those principles,

00:21:33.541 --> 00:21:35.740
then you can start implementing.

00:21:35.740 --> 00:21:38.850
My idea here is if I want to
build intelligent machines,

00:21:38.850 --> 00:21:40.610
it shouldn't be performance based.

00:21:40.610 --> 00:21:43.952
Think a mouse is an intelligent
machine, intelligent animal.

00:21:43.952 --> 00:21:47.540
A cat, a dog, a monkey, humans,
these all have different

00:21:47.540 --> 00:21:48.803
levels of intelligence.

00:21:48.803 --> 00:21:53.050
It's not that surpassing the
intelligence which we want

00:21:53.050 --> 00:21:55.710
to do, but if the goal is we
should be working on the right

00:21:55.710 --> 00:21:56.543
set of principles.

00:21:56.543 --> 00:21:58.880
If we realize what those
principles are, then we can say

00:21:58.880 --> 00:21:59.713
this machine is intelligent.

00:21:59.713 --> 00:22:01.770
It may not be very
intelligent, or it may be super

00:22:01.770 --> 00:22:04.200
intelligent, but it's the
principles that count.

00:22:04.200 --> 00:22:05.513
And, that's the approach I want to do.

00:22:05.513 --> 00:22:08.530
So, I want to go through
that list of principles now.

00:22:08.530 --> 00:22:10.060
For those that were here
yesterday, I talked about a few

00:22:10.060 --> 00:22:11.830
of them, I'm gonna add some more today.

00:22:11.830 --> 00:22:13.690
And I'm gonna review a few
of the ones that I talked

00:22:13.690 --> 00:22:14.523
about yesterday, so.

00:22:14.523 --> 00:22:16.950
And I'm gonna end up a little
bit of review for a few

00:22:16.950 --> 00:22:19.460
minutes, and for those of
you that saw this yesterday,

00:22:19.460 --> 00:22:21.060
think of it as a refresher course.

00:22:21.060 --> 00:22:23.370
But it'll be much quicker
than what I did yesterday.

00:22:23.370 --> 00:22:25.470
Okay, so one more thing
is you have to understand

00:22:25.470 --> 00:22:27.520
what the neocortex is doing.

00:22:27.520 --> 00:22:31.080
The neocortex is a memory
system, it's not a computer.

00:22:31.080 --> 00:22:31.913
It's a memory system.

00:22:31.913 --> 00:22:35.630
It stores information in it's
connections, and it builds

00:22:35.630 --> 00:22:36.650
a model of the world.

00:22:36.650 --> 00:22:38.201
When you're born, it
doesn't know about anything.

00:22:38.201 --> 00:22:40.540
It doesn't know about all
the objects in the world

00:22:40.540 --> 00:22:41.700
and how they relate to one other.

00:22:41.700 --> 00:22:44.020
It doesn't know about
presidential debates and computers

00:22:44.020 --> 00:22:44.853
and rooms and so on.

00:22:44.853 --> 00:22:47.220
And it has to build the knowledge of that.

00:22:47.220 --> 00:22:49.320
And it does that through its senses.

00:22:49.320 --> 00:22:51.417
So we have arrays of senses
here, the retina and the cochlea

00:22:51.417 --> 00:22:53.560
and the somatic sensors
of the body senses.

00:22:53.560 --> 00:22:56.100
Those are millions of sensory
bits coming into the brain,

00:22:56.100 --> 00:22:58.820
streaming in real time, high
velocity, and very rapidly

00:22:58.820 --> 00:23:00.933
changing; the brain gets
these in and it has to build

00:23:00.933 --> 00:23:03.400
them in from the world,
that sort of thing.

00:23:03.400 --> 00:23:05.350
In the model, it has several things.

00:23:05.350 --> 00:23:08.309
It makes predictions about the
future, it detects anomalies,

00:23:08.309 --> 00:23:10.700
and it generates actions.

00:23:10.700 --> 00:23:11.840
So, the exa--

00:23:11.840 --> 00:23:13.380
Give you an example, I
talked about this yesterday,

00:23:13.380 --> 00:23:15.860
but you know, your brain is
constantly making predictions

00:23:15.860 --> 00:23:17.781
about what it's gonna see, hear, and feel.

00:23:17.781 --> 00:23:20.060
You're not aware of most
of these predictions.

00:23:20.060 --> 00:23:23.630
They occur, it's all
subconsciously for the most part,

00:23:23.630 --> 00:23:26.160
but you know when your
predictions are violated

00:23:26.160 --> 00:23:28.820
that something's different
than what's expected.

00:23:28.820 --> 00:23:31.010
The example I used in my book,
I was sitting in my office

00:23:31.010 --> 00:23:33.260
one day and I realized
there's all these objects

00:23:33.260 --> 00:23:34.270
in my office!

00:23:34.270 --> 00:23:36.510
And yet, if any of them,
if some of them just moved

00:23:36.510 --> 00:23:38.720
a little bit or disappeared,
or some new object would appear

00:23:38.720 --> 00:23:40.320
I would notice it right away.

00:23:40.320 --> 00:23:41.300
But I'm not sitting there going,

00:23:41.300 --> 00:23:42.500
let's take inventory of my office.

00:23:42.500 --> 00:23:44.400
I'm just looking around and things happen.

00:23:44.400 --> 00:23:47.900
Similarly, your expectations
when I speak, you have

00:23:47.900 --> 00:23:50.230
expectations what you're gonna
feel when you touch things,

00:23:50.230 --> 00:23:51.690
you have expectations
what you're gonna see.

00:23:51.690 --> 00:23:53.300
This is proven, we know this is a fact.

00:23:53.300 --> 00:23:55.490
Your brain is constantly
making predictions.

00:23:55.490 --> 00:23:58.520
So, this was a way of figuring
out, a cornerstone for me,

00:23:58.520 --> 00:24:01.050
like, how is it, what kind
of memory system would make

00:24:01.050 --> 00:24:03.360
predictions, and the answer
to that is a memory system

00:24:03.360 --> 00:24:06.800
that learns temporal kinda,
what normally follows what

00:24:06.800 --> 00:24:08.310
in various ways.

00:24:08.310 --> 00:24:10.394
Okay, so that's the basic idea.

00:24:10.394 --> 00:24:13.180
Let me just go through, I'm
gonna list these principles

00:24:13.180 --> 00:24:15.285
the neuro cortical function
that I think are essential

00:24:15.285 --> 00:24:18.470
for true Machine Intelligence,
and then I'm going to go

00:24:18.470 --> 00:24:20.010
into detail about a couple of them.

00:24:20.010 --> 00:24:22.980
So here we have this cortex,
we have this high velocity

00:24:22.980 --> 00:24:24.760
data stream coming from a set of sensors.

00:24:24.760 --> 00:24:29.045
One of the action bits that I
think are really gonna define

00:24:29.045 --> 00:24:29.878
Machine Intelligence.

00:24:29.878 --> 00:24:31.750
The first one you might be surprised.

00:24:31.750 --> 00:24:33.630
You certainly would have
thought about this much,

00:24:33.630 --> 00:24:37.040
but when you think about
intelligent biological systems,

00:24:37.040 --> 00:24:39.580
they all have these sensory arrays.

00:24:39.580 --> 00:24:42.830
We don't have a camera for eyes,
we have an array of sensors

00:24:42.830 --> 00:24:46.140
and they are processed like a
whole, like millions, millions

00:24:46.140 --> 00:24:46.973
of sensors.

00:24:46.973 --> 00:24:49.290
There's nobody looking at that
entire picture in your brain.

00:24:49.290 --> 00:24:50.930
And so, the same with
the cochlea, and the same

00:24:50.930 --> 00:24:51.763
with your skin.

00:24:51.763 --> 00:24:55.690
And so, we now understand
this is an essential property

00:24:55.690 --> 00:24:57.600
of how the memory system
in your brain works,

00:24:57.600 --> 00:24:59.060
and I don't think you can get around it.

00:24:59.060 --> 00:25:02.080
So, Intelligent Machines are
going to have to have sensory,

00:25:02.080 --> 00:25:03.680
low level sensory arrays.

00:25:03.680 --> 00:25:06.380
You just can't go, no one
just pumps Shakespeare

00:25:06.380 --> 00:25:07.213
into your head.

00:25:07.213 --> 00:25:09.070
You know, that doesn't happen.

00:25:09.070 --> 00:25:11.970
You read it in a very complex
thing seen through your eyes.

00:25:11.970 --> 00:25:13.300
Or you will hear it being spoken.

00:25:13.300 --> 00:25:15.150
Or you can feel it through Braille.

00:25:15.150 --> 00:25:17.670
But the point is, it's
gotta go through this array

00:25:17.670 --> 00:25:20.350
of sensors through time
to get it in there.

00:25:20.350 --> 00:25:22.940
The second thing is that the
neocortex is a hierarchy,

00:25:22.940 --> 00:25:24.970
and I talked about this
a little bit yesterday.

00:25:24.970 --> 00:25:29.050
When we look at it, physically
the memory in the neocortex's

00:25:29.050 --> 00:25:31.470
sheath is arranged in
a hierarchical fashion.

00:25:31.470 --> 00:25:33.497
These regions are connected
to each other in a hierarchy,

00:25:33.497 --> 00:25:36.210
and the information flows
up and down the hierarchy.

00:25:36.210 --> 00:25:38.610
This is a physical fact about
the brain, and this is a very

00:25:38.610 --> 00:25:40.750
interesting observation about
the kind of memory system

00:25:40.750 --> 00:25:43.113
it is, it is a hierarchical memory system.

00:25:44.758 --> 00:25:45.770
And, you can't get around that.

00:25:45.770 --> 00:25:46.670
Intelligent Machines are going

00:25:46.670 --> 00:25:48.723
to have hierarchical structure.

00:25:49.860 --> 00:25:53.100
The next thing is, and
this is part of the theory

00:25:53.100 --> 00:25:55.470
that we've developed is that
each reach in the hierarchy

00:25:55.470 --> 00:25:57.920
is doing is a form of sequence
memory, or different type

00:25:57.920 --> 00:25:59.150
of sequence memory.

00:25:59.150 --> 00:26:02.010
And so, as you are, when
you're hearing my speech,

00:26:02.010 --> 00:26:04.100
and you're hearing these
complex patterns coming in

00:26:04.100 --> 00:26:06.273
through time, the way you
recognize my speech, 'cause you

00:26:06.273 --> 00:26:09.094
have memory of what words
sound like in order, and what

00:26:09.094 --> 00:26:11.740
sentences and phrases sound
like in order, and what things

00:26:11.740 --> 00:26:12.840
typically follow what.

00:26:12.840 --> 00:26:15.460
And so to recognize speech,
to recognize when you touch

00:26:15.460 --> 00:26:17.649
things, and even vision is a
hierarchical temporal process,

00:26:17.649 --> 00:26:19.620
and so there's this sequence memory.

00:26:19.620 --> 00:26:22.410
And the idea of sequence
memory in a hierarchy is that

00:26:22.410 --> 00:26:25.420
you've sequence memory at
the first levels, recognizing

00:26:25.420 --> 00:26:27.570
very small patterns in space and time,

00:26:27.570 --> 00:26:29.340
and then they collapse
into longer patterns

00:26:29.340 --> 00:26:31.500
of space and time and so
on going up the hierarchy.

00:26:31.500 --> 00:26:32.860
And you get to the top of the hierarchy,

00:26:32.860 --> 00:26:34.570
you have representations of higher level

00:26:34.570 --> 00:26:36.753
objects in the world and how they behave.

00:26:36.753 --> 00:26:39.170
Some of this is very well understood.

00:26:39.170 --> 00:26:41.000
Some of it's not understood at all.

00:26:41.000 --> 00:26:42.360
But, this is a fact.

00:26:42.360 --> 00:26:44.230
And, one of the things
about the hierarchy,

00:26:44.230 --> 00:26:46.340
which I mentioned yesterday
and haven't mentioned yet

00:26:46.340 --> 00:26:48.470
today is that all the regions
of the hierarchy are doing

00:26:48.470 --> 00:26:51.730
the same thing so once we
understand you know, how one

00:26:51.730 --> 00:26:54.057
part's doing it we understand
how all part's doing it.

00:26:54.057 --> 00:26:56.400
So, these are attributes
that an intelligent system

00:26:56.400 --> 00:26:57.700
are going to have to have.

00:26:59.110 --> 00:27:01.190
The next one is Sparse
Distributed Representation.

00:27:01.190 --> 00:27:02.903
This is the language of the
brain, and I'm gonna go over

00:27:02.903 --> 00:27:05.020
a bit more of this in a moment,
but it's the way when we,

00:27:05.020 --> 00:27:07.220
everywhere we look we find a
few things that are active,

00:27:07.220 --> 00:27:09.576
few cells that are active, and
most cells that are inactive.

00:27:09.576 --> 00:27:11.343
And they're properties of
this which are important

00:27:11.343 --> 00:27:13.140
that you're gonna need to understand.

00:27:13.140 --> 00:27:16.330
And, I contend that no
intelligent machine, biological

00:27:16.330 --> 00:27:18.660
or otherwise, can work
without Sparse Distributed

00:27:18.660 --> 00:27:20.520
Representations, so we're
gonna go a little bit

00:27:20.520 --> 00:27:21.650
more into that.

00:27:21.650 --> 00:27:24.090
And then, here's some
things I did not talk

00:27:24.090 --> 00:27:25.253
about much yesterday.

00:27:26.350 --> 00:27:29.670
Everywhere you go in the
neocortex, there's these cells

00:27:29.670 --> 00:27:32.660
and sequence memory that
are learning the patterns

00:27:32.660 --> 00:27:33.510
of the world.

00:27:33.510 --> 00:27:35.050
But everywhere you go,
no matter where you look

00:27:35.050 --> 00:27:38.145
in the neocortex, you see
outputs that are descending

00:27:38.145 --> 00:27:39.855
to motor parts of your brain.

00:27:39.855 --> 00:27:43.940
And, everywhere is that
sensing information is also

00:27:43.940 --> 00:27:45.570
directing behavior.

00:27:45.570 --> 00:27:49.340
You can't separate out behavior
from sensory information.

00:27:49.340 --> 00:27:52.070
You can't separate out
inference from behavior.

00:27:52.070 --> 00:27:54.330
And think about it, when
you, when you interact

00:27:54.330 --> 00:27:56.630
with the world, whether
you're just moving your eyes

00:27:56.630 --> 00:27:59.440
or walking around or touching
things with your hand,

00:27:59.440 --> 00:28:02.030
you're changing what your
senses are gonna feel.

00:28:02.030 --> 00:28:04.010
You know, I sense my fingers
aren't just gonna feel

00:28:04.010 --> 00:28:05.680
this podium because they happen
to be feeling the podium,

00:28:05.680 --> 00:28:07.880
it's 'cause I have moved my hand there.

00:28:07.880 --> 00:28:10.619
And there's a tight coupling
between your behavior

00:28:10.619 --> 00:28:12.150
and what you sense.

00:28:12.150 --> 00:28:15.010
This is the sensing motor
integration issue, and we have,

00:28:15.010 --> 00:28:17.100
this is part of how our brains work,

00:28:17.100 --> 00:28:19.640
and this is an essential
feature that our Intelligent

00:28:19.640 --> 00:28:21.520
Machines have to have.

00:28:21.520 --> 00:28:23.526
Then there's an attentional mechanism.

00:28:23.526 --> 00:28:26.430
I'm giving you the laundry list
here, okay, so bare with me.

00:28:26.430 --> 00:28:29.177
I think this might be the last
one, or maybe have one more.

00:28:29.177 --> 00:28:32.500
Attention mechanism, in the
hierarchy, there's information

00:28:32.500 --> 00:28:34.780
flowing up and flowing down,
but we know that there's

00:28:34.780 --> 00:28:36.830
actually ways of turning it off.

00:28:36.830 --> 00:28:41.079
And so, you can attend to some
subset of your sensory stream

00:28:41.079 --> 00:28:44.520
so you might be sensing,
you might be zoning out

00:28:44.520 --> 00:28:46.530
on your not really feeling
too much right now,

00:28:46.530 --> 00:28:49.200
you're listening carefully to my words.

00:28:49.200 --> 00:28:50.130
Or maybe reading, you're
not listening to my words,

00:28:50.130 --> 00:28:53.060
and you're paying attention
to the stuff on the screen.

00:28:53.060 --> 00:28:55.382
But I could also tell you
like, okay look at the word

00:28:55.382 --> 00:28:57.949
principles at the top of the slide.

00:28:57.949 --> 00:29:00.103
Now, there's a pattern coming
in your brain right now

00:29:00.103 --> 00:29:01.967
when you're seeing the word principles.

00:29:01.967 --> 00:29:03.750
And I say look at the I.

00:29:03.750 --> 00:29:05.400
The letter I.

00:29:05.400 --> 00:29:07.890
The same pattern's coming in
on your eye, but now you're

00:29:07.890 --> 00:29:09.730
focusing your attention,
you're attending to a subset

00:29:09.730 --> 00:29:10.563
of that information.

00:29:10.563 --> 00:29:12.740
Same information coming in the
brain, and you're attending

00:29:12.740 --> 00:29:13.640
to a subset.

00:29:13.640 --> 00:29:16.010
Now you say look at the
dot on top of the I.

00:29:16.010 --> 00:29:16.970
Focus on that.

00:29:16.970 --> 00:29:19.761
Same pattern coming in
your eye, the whole picture

00:29:19.761 --> 00:29:22.800
but you're now attending to
a smaller piece of the image.

00:29:22.800 --> 00:29:24.600
You're doing this all the time.

00:29:24.600 --> 00:29:25.640
You're not aware of it.

00:29:25.640 --> 00:29:27.630
You're doing this all the
time, tuning in some parts

00:29:27.630 --> 00:29:29.370
and taking out certain parts
and this is pretty important

00:29:29.370 --> 00:29:31.700
to have a complex sensory environment.

00:29:31.700 --> 00:29:33.343
So, this is going on in the brain.

00:29:33.343 --> 00:29:35.403
I've showed you in this picture
here where I say hey look,

00:29:35.403 --> 00:29:37.500
I can turn off those little
X's, we can turn off some

00:29:37.500 --> 00:29:39.660
of the input and focus on
other parts of the input.

00:29:39.660 --> 00:29:42.140
And this is a pathway through
the thalamus which does this.

00:29:42.140 --> 00:29:44.900
This is very important part
of how we interact with a very

00:29:44.900 --> 00:29:47.210
complex, rich world, and is an important

00:29:47.210 --> 00:29:50.060
part of intelligence.

00:29:50.060 --> 00:29:52.060
Now a few things on this,
there's a lot of things

00:29:52.060 --> 00:29:54.010
that aren't on this list I
just mentioned a couple things

00:29:54.010 --> 00:29:54.843
right here.

00:29:54.843 --> 00:29:57.390
One is, you notice I
don't mention emotions.

00:29:57.390 --> 00:30:00.310
There's books about emotional
intelligence and so on.

00:30:00.310 --> 00:30:02.317
I don't believe you actually
have to have emotions.

00:30:02.317 --> 00:30:05.370
You have to have emotions to
be human-like, you have to have

00:30:05.370 --> 00:30:07.350
emotions to pass the Turing Test,

00:30:07.350 --> 00:30:09.177
but to be purely intelligent
to build a model of the world

00:30:09.177 --> 00:30:12.469
that's very, very sophisticated
and build predictions,

00:30:12.469 --> 00:30:13.950
and detect anomalies, and
discover structure of the world,

00:30:13.950 --> 00:30:15.510
you do not have to have emotions.

00:30:15.510 --> 00:30:19.470
Emotions are all about, it's
all about an ubering system.

00:30:19.470 --> 00:30:21.560
It says you know what,
this is really dangerous,

00:30:21.560 --> 00:30:22.393
or this is really good.

00:30:22.393 --> 00:30:23.226
I need to remember that.

00:30:23.226 --> 00:30:24.420
Or I need to avoid that.

00:30:24.420 --> 00:30:25.410
But you can do an awful lot without that.

00:30:25.410 --> 00:30:27.670
And I do not believe this is
something that's essential

00:30:27.670 --> 00:30:28.850
for intelligence.

00:30:28.850 --> 00:30:30.930
And finally, you don't need
to have a human-like body.

00:30:30.930 --> 00:30:35.043
This isn't about building
robots, and you know, and yeah,

00:30:35.043 --> 00:30:36.740
it's not about building robots.

00:30:36.740 --> 00:30:40.510
And, you know, you want to
have a human body-like body,

00:30:40.510 --> 00:30:41.650
but it's not about that at all.

00:30:41.650 --> 00:30:44.010
We can have intelligence
embedded in all kinds of systems,

00:30:44.010 --> 00:30:44.843
you might not even be able to see it.

00:30:44.843 --> 00:30:46.640
It's a bunch of computers
running some place with some

00:30:46.640 --> 00:30:47.680
sensors on their own.

00:30:47.680 --> 00:30:49.130
So we don't need those
things, we're gonna get rid

00:30:49.130 --> 00:30:49.963
of those things.

00:30:51.080 --> 00:30:52.900
We'll just stick to these principles.

00:30:52.900 --> 00:30:57.360
Okay, so, by now there's two
here that I told you about

00:30:57.360 --> 00:30:59.610
yesterday that are
really, really important.

00:30:59.610 --> 00:31:03.080
And so, what I'm gonna do is
I'm gonna go through the slides

00:31:03.080 --> 00:31:04.970
I had two slides I had yesterday
about Sparse Distributed

00:31:04.970 --> 00:31:06.990
Representations, I'm
gonna do those completely.

00:31:06.990 --> 00:31:09.920
Because to me that is the most
important thing, if you want

00:31:09.920 --> 00:31:12.065
to operate anything, you have
to understand what Sparse

00:31:12.065 --> 00:31:13.180
Distributed Representations are.

00:31:13.180 --> 00:31:15.600
And the second one, I'm just
gonna give you a cursive review

00:31:15.600 --> 00:31:16.850
of the sequence memory.

00:31:16.850 --> 00:31:19.620
And I apologize for those
of you who totally got it

00:31:19.620 --> 00:31:21.410
yesterday, and say why
are you repeating that?

00:31:21.410 --> 00:31:23.660
But I doubt anybody
totally got it yesterday.

00:31:25.550 --> 00:31:26.400
It takes awhile.

00:31:26.400 --> 00:31:30.070
Okay, so here again, the two
slides on Sparse Distributed

00:31:30.070 --> 00:31:30.903
Representations--

00:31:30.903 --> 00:31:32.550
It's easiest to do this when
you compare it to a computer.

00:31:32.550 --> 00:31:35.630
If you know how computers
work, you can easily see how

00:31:35.630 --> 00:31:37.340
this works in the brain.

00:31:37.340 --> 00:31:39.880
And I'll do this a little
bit quicker than usual.

00:31:39.880 --> 00:31:43.140
In the computer, we have what
we call dense representations.

00:31:43.140 --> 00:31:47.330
We have 8-bit, 16-bit,
64-bit entities, you know,

00:31:47.330 --> 00:31:49.060
64-bits at a time or something like that.

00:31:49.060 --> 00:31:51.450
And we use all combinations
of ones and zeros.

00:31:51.450 --> 00:31:55.670
So if I have an 8-bit quantity,
a bite, all 256 possible

00:31:55.670 --> 00:31:58.793
combinations of ones and zeros
are used to represent things.

00:31:58.793 --> 00:32:00.430
That's why it's called dense.

00:32:00.430 --> 00:32:02.970
There's an example of the
asking code for the letter M.

00:32:02.970 --> 00:32:06.050
It's just some arbitrary
assignment of ones and zeros

00:32:06.050 --> 00:32:07.670
that says this is M.

00:32:07.670 --> 00:32:10.710
If I ask you what those different
bits mean, in the letter M

00:32:10.710 --> 00:32:12.030
they mean nothing.

00:32:12.030 --> 00:32:14.250
In fact, if I change one of
the bits, I get a completely

00:32:14.250 --> 00:32:15.083
different letter.

00:32:15.083 --> 00:32:17.710
So, I have to look at all
of them to get any idea

00:32:17.710 --> 00:32:18.543
what this is.

00:32:18.543 --> 00:32:20.577
And actually, nothing about
those bits would tell you

00:32:20.577 --> 00:32:21.410
what's M.

00:32:21.410 --> 00:32:24.510
It's placed elsewhere in some
table that someone says okay,

00:32:24.510 --> 00:32:26.120
that's the letter M.

00:32:26.120 --> 00:32:29.100
And then, finally, these
representations are assigned.

00:32:29.100 --> 00:32:31.080
They don't have any inherent,
you know, they're not learned.

00:32:31.080 --> 00:32:32.940
Someone just said here's the
ASCII code, we're gonna use

00:32:32.940 --> 00:32:34.343
this for the next 100 years.

00:32:35.388 --> 00:32:37.260
In the brain it's very different.

00:32:37.260 --> 00:32:40.170
In the brain we have cells,
and if you look at the cells

00:32:40.170 --> 00:32:42.870
very few are active at any point in time.

00:32:42.870 --> 00:32:46.070
Most of them are inactive,
or relatively inactive.

00:32:46.070 --> 00:32:48.860
And so it's like, and
there's thousands of them.

00:32:48.860 --> 00:32:50.670
And so in a Sparse
Distributed Representation,

00:32:50.670 --> 00:32:52.780
we can represent the
cells with zeros and ones,

00:32:52.780 --> 00:32:55.430
and we can say that those
are several thousand, or tens

00:32:55.430 --> 00:32:57.170
of thousands of these bits.

00:32:57.170 --> 00:32:59.590
We don't, we typically use
things about 2000 bits long

00:32:59.590 --> 00:33:01.518
in our work at Numenta,
and I'll talk about that

00:33:01.518 --> 00:33:02.770
in a little bit.

00:33:02.770 --> 00:33:04.210
And, they're mostly zeros.

00:33:04.210 --> 00:33:06.160
But we also typically have 2 percent on.

00:33:06.160 --> 00:33:10.268
So have 2000, I have 41-bits,
and I have 1906 zero bits.

00:33:10.268 --> 00:33:12.790
The, there's nothing
magic about those numbers.

00:33:12.790 --> 00:33:14.190
But sparsity is important.

00:33:14.190 --> 00:33:17.287
Most bits have meaning of some sort.

00:33:17.287 --> 00:33:19.770
I can, you know, they
don't change over time.

00:33:19.770 --> 00:33:22.060
I say here's a bit, it was
like here's a cell to cell

00:33:22.060 --> 00:33:25.530
represents something that
doesn't arbitrarily change

00:33:25.530 --> 00:33:26.550
from moment to moment.

00:33:26.550 --> 00:33:28.517
So this cell represents
a line in a certain part

00:33:28.517 --> 00:33:30.770
of the visual space, it's
always going to represent

00:33:30.770 --> 00:33:32.050
that line in that part
of your visual space.

00:33:32.050 --> 00:33:34.198
If this cell represents a
part of recognizing a face

00:33:34.198 --> 00:33:36.722
of a human, it's always
going to be that way.

00:33:36.722 --> 00:33:39.160
And what the basic idea here is,

00:33:39.160 --> 00:33:43.660
when you form a representation,
if I have 2000 bits,

00:33:43.660 --> 00:33:47.610
I've 2000 somatic meanings,
and I pick the top 40,

00:33:47.610 --> 00:33:50.480
the top two percent that
best represent this thing.

00:33:50.480 --> 00:33:53.010
The example I used yesterday
is I wanted to represent

00:33:53.010 --> 00:33:56.300
a letter, I might say here's
bits, and we wouldn't do this,

00:33:56.300 --> 00:33:58.890
this is an example, I
might have bits for okay,

00:33:58.890 --> 00:34:00.650
is the letter a consonant or a vowel,

00:34:00.650 --> 00:34:03.565
is it sound o sound or e
sound, is it hard or soft,

00:34:03.565 --> 00:34:06.677
where is it in the alphabet,
does it have ascender

00:34:06.677 --> 00:34:09.330
and descenders, and so on,
and I pick the bits that best

00:34:09.330 --> 00:34:10.620
represent that thing.

00:34:10.620 --> 00:34:13.710
And now my representation
actually tells me what it is.

00:34:13.710 --> 00:34:15.890
There is no external place
where I have to say oh,

00:34:15.890 --> 00:34:18.236
this code represents X.

00:34:18.236 --> 00:34:20.050
It's right there in the code.

00:34:20.050 --> 00:34:22.650
If I know what those bits
mean, the encoding tells you

00:34:22.650 --> 00:34:23.483
what it is.

00:34:23.483 --> 00:34:24.975
That is the entire definition.

00:34:24.975 --> 00:34:28.050
And so, if each bit has
semantic meaning, and these bit

00:34:28.050 --> 00:34:29.357
definitions have to be learned.

00:34:29.357 --> 00:34:32.307
Now there's these properties
that are SDR's which are very,

00:34:33.391 --> 00:34:34.224
very important.

00:34:34.224 --> 00:34:35.350
One is one of similarity.

00:34:35.350 --> 00:34:37.500
If I take two Sparse
Distributor Representations

00:34:37.500 --> 00:34:41.040
and I compare them bit by bit,
if the shared bit, that means

00:34:41.040 --> 00:34:44.000
they share semantic meaning
of some sort, and therefore

00:34:44.000 --> 00:34:45.560
they're similar semantically.

00:34:45.560 --> 00:34:48.110
The more bits they share,
the more semantically similar

00:34:48.110 --> 00:34:50.360
they are, very simple property.

00:34:50.360 --> 00:34:53.660
The next one is if I
want to store a pattern,

00:34:53.660 --> 00:34:56.780
a sparse representation, and
I want to recognize it again.

00:34:56.780 --> 00:34:59.140
So, here's a pattern, I've
seen this now, I want to see

00:34:59.140 --> 00:35:00.610
this accrue again.

00:35:00.610 --> 00:35:03.427
Well, I can save all 2000 bits,
that's one way of doing it,

00:35:03.427 --> 00:35:05.910
that's how a computer programmer
would do it, but we can do

00:35:05.910 --> 00:35:06.743
something simpler.

00:35:06.743 --> 00:35:09.930
We can just say well this sort
the locations of the ones.

00:35:09.930 --> 00:35:11.070
So I have 40 one bits,

00:35:11.070 --> 00:35:12.910
I'll just say where are those 40 one bits.

00:35:12.910 --> 00:35:16.100
And if I see ones in those
40 locations, I know I've got

00:35:16.100 --> 00:35:18.590
my pattern because that's
all there is is 40 ones.

00:35:18.590 --> 00:35:21.270
And then we can ask the next
question, which is what if I

00:35:21.270 --> 00:35:23.460
couldn't save all 40 indices.

00:35:23.460 --> 00:35:25.223
What if I can only save 10 of them?

00:35:26.440 --> 00:35:28.860
And so I say you can't store,
we're just gonna have to

00:35:28.860 --> 00:35:32.152
randomly sample the 40 and
just save the locations of 10.

00:35:32.152 --> 00:35:34.140
Well, so I do that.

00:35:34.140 --> 00:35:36.400
And I say sure I'm storing
some of them, but I'm not

00:35:36.400 --> 00:35:37.400
storing other ones.

00:35:37.400 --> 00:35:39.640
Now a pattern comes in, and
you say is it the same pattern

00:35:39.640 --> 00:35:41.808
or not, and I say well
look, the 10 are the same.

00:35:41.808 --> 00:35:44.290
What are the changes of the
other 30 being the same?

00:35:44.290 --> 00:35:45.240
You say well, I could be wrong.

00:35:45.240 --> 00:35:46.760
Those other 30 could be some place else.

00:35:46.760 --> 00:35:49.440
The chances are very, very, very unlikely

00:35:49.440 --> 00:35:52.980
that's going to occur, but
if it does occur, you made

00:35:52.980 --> 00:35:55.600
a mistake, but it's a mistake
for something semantically

00:35:55.600 --> 00:35:57.360
similar to the thing you did store.

00:35:57.360 --> 00:36:00.170
So it's like okay well, I made
a mistake but it's similar

00:36:00.170 --> 00:36:01.200
to the thing I stored before,

00:36:01.200 --> 00:36:03.620
and that's often very good enough.

00:36:03.620 --> 00:36:06.550
And then finally, this is
the most difficult property,

00:36:06.550 --> 00:36:07.810
is one of union.

00:36:07.810 --> 00:36:10.650
And you can take these Sparse
Distributed Representations

00:36:10.650 --> 00:36:13.260
and what if I took 10 of
them, and I say order them

00:36:13.260 --> 00:36:16.033
together, so I have these 10
patterns, each of two percent

00:36:16.033 --> 00:36:18.250
of the bits on them, I
already have one pattern,

00:36:18.250 --> 00:36:21.583
which is about 20 percent of
it was, of the bits are on,

00:36:22.531 --> 00:36:23.364
and I have this union.

00:36:23.364 --> 00:36:25.670
And I say well, can I tell
you what the first ten were?

00:36:25.670 --> 00:36:26.686
No, I can't do it.

00:36:26.686 --> 00:36:29.117
I cannot undo this operation.

00:36:29.117 --> 00:36:31.980
But I can do something very interesting.

00:36:31.980 --> 00:36:34.750
Is I can take a new Sparse
Distributed Representation,

00:36:34.750 --> 00:36:36.333
and ask is it one of the members.

00:36:36.333 --> 00:36:39.300
And the answer is, I can do that.

00:36:39.300 --> 00:36:42.630
I find that the patterns,
the ones in my unknown match

00:36:42.630 --> 00:36:46.410
the ones in the union, I can
be very certain that this thing

00:36:46.410 --> 00:36:48.288
is a member of the original 10.

00:36:48.288 --> 00:36:50.370
And you might again say well,
it could make a mistake.

00:36:50.370 --> 00:36:52.770
But simple math will tell
you it's not gonna happen.

00:36:52.770 --> 00:36:54.000
Very, very unlikely.

00:36:54.000 --> 00:36:57.950
And this is, I use this when
the brain makes a prediction,

00:36:57.950 --> 00:37:00.430
it's making predictions
in the activity of cells,

00:37:00.430 --> 00:37:03.770
and it essentially says I
can have multiple predictions

00:37:03.770 --> 00:37:04.603
going on at once.

00:37:04.603 --> 00:37:07.620
I can predict many things
that might happen next.

00:37:07.620 --> 00:37:10.090
And I can tell you if what
actually happens was one

00:37:10.090 --> 00:37:11.030
of those things.

00:37:11.030 --> 00:37:13.370
So when we make predictions,
you often can't tell me

00:37:13.370 --> 00:37:14.440
exactly what you're predicting.

00:37:14.440 --> 00:37:15.273
You don't know.

00:37:15.273 --> 00:37:17.666
But when something
unexpected occurs, you know.

00:37:17.666 --> 00:37:20.830
And that comes from that
property I just showed you there.

00:37:20.830 --> 00:37:22.830
'Cause again, this is
the most important thing

00:37:22.830 --> 00:37:24.920
about Machine Intelligence
and about brains.

00:37:24.920 --> 00:37:26.530
You want to walk away and
say hey I heard this guy

00:37:26.530 --> 00:37:29.650
Hawkins talk, he's kinda crazy
but one thing I remember,

00:37:29.650 --> 00:37:32.850
this would be it, brains
and brain machines

00:37:32.850 --> 00:37:34.610
are gonna be based on Sparse
Distributed Representations.

00:37:34.610 --> 00:37:36.059
You can bank on that.

00:37:36.059 --> 00:37:38.410
And many people will in the future.

00:37:38.410 --> 00:37:41.460
Okay, now I'm gonna talk very,
very briefly about sequence

00:37:41.460 --> 00:37:43.840
memory, I will not be able
to go into all details

00:37:43.840 --> 00:37:46.380
about this, but I want to
give you the flavor for it.

00:37:46.380 --> 00:37:48.400
Yesterday I gave you
many more of the details.

00:37:48.400 --> 00:37:50.570
Here we're seeing a Sparse
Disubstituted Representation,

00:37:50.570 --> 00:37:52.700
but instead of showing you
ones and zeros, we're showing

00:37:52.700 --> 00:37:53.650
it as little cubes.

00:37:53.650 --> 00:37:55.366
We can imagine those cubes
being cells in the brain.

00:37:55.366 --> 00:37:57.920
And, the red ones are active

00:37:57.920 --> 00:37:59.631
and the white ones are inactive.

00:37:59.631 --> 00:38:02.070
And at any point in time,
I'll have two percent of them

00:38:02.070 --> 00:38:04.210
active like this, and then
at another point in time

00:38:04.210 --> 00:38:05.610
we know they'll set like that.

00:38:05.610 --> 00:38:08.070
And the basic idea we want
to do when we want to run

00:38:08.070 --> 00:38:10.720
sequences, is we don't
try to learn the sequence

00:38:10.720 --> 00:38:13.660
of everything, but every
cell in itself, every little

00:38:13.660 --> 00:38:15.900
cube here tries to
predict its own activity.

00:38:15.900 --> 00:38:18.660
Tries to learn when it
follows something else.

00:38:18.660 --> 00:38:20.080
And, if you do that,

00:38:20.080 --> 00:38:22.910
you end up with a
Distributed Sequence Memory.

00:38:22.910 --> 00:38:25.090
So, when a cell becomes,
so we have these patterns

00:38:25.090 --> 00:38:27.000
coming into your brain
right now as I'm talking,

00:38:27.000 --> 00:38:28.660
these patterns are flashing
back and forth like this

00:38:28.660 --> 00:38:31.510
all the time, and when a
cell becomes active it looks

00:38:31.510 --> 00:38:33.420
for cells nearby, it doesn't
have to look into all of them,

00:38:33.420 --> 00:38:35.950
it just has to subset a few
and say memory I remember

00:38:35.950 --> 00:38:36.830
which ones are active.

00:38:36.830 --> 00:38:38.830
Those are like those indices,
which ones are active?

00:38:38.830 --> 00:38:41.560
And I'm gonna save those
ones, and know if I see them

00:38:41.560 --> 00:38:44.024
happen again, that I can
predict my own activity.

00:38:44.024 --> 00:38:46.997
And, here's a situation where
I have an input coming in

00:38:46.997 --> 00:38:49.220
and a whole bunch of cells,
yellow cells, predicting

00:38:49.220 --> 00:38:51.350
they're going to come next,
just to be in a situation

00:38:51.350 --> 00:38:52.283
where I'm predicting multiple things.

00:38:52.283 --> 00:38:55.430
Like if I had A followed
by B, and A followed by C,

00:38:55.430 --> 00:38:57.980
and A followed by D, I
show you A is gonna predict

00:38:57.980 --> 00:38:59.760
B, C, and D.

00:38:59.760 --> 00:39:02.290
Then I would show yesterday
that this is a first order

00:39:02.290 --> 00:39:04.960
memory, meaning it can only
predict based on the last

00:39:04.960 --> 00:39:05.793
thing that happened.

00:39:05.793 --> 00:39:06.660
It has no history.

00:39:06.660 --> 00:39:09.000
It can't tell you things
about how long in the past.

00:39:09.000 --> 00:39:09.970
Imagine a melody.

00:39:09.970 --> 00:39:11.790
A melody is a high-ordered pattern.

00:39:11.790 --> 00:39:13.590
Think about Beethoven's Fifth.

00:39:13.590 --> 00:39:16.050
It goes bum-bum-bum-bum,
bum-bum-bum-bum, bum-bum-bum-bum

00:39:16.050 --> 00:39:16.883
bum-bum-bum-bum--

00:39:16.883 --> 00:39:20.290
The first four notes,
bum-bum-bum-bum, are repeated

00:39:20.290 --> 00:39:22.470
as the ninth to twelfth notes.

00:39:22.470 --> 00:39:24.656
Exactly the same notes,
but you don't get confused.

00:39:24.656 --> 00:39:27.790
And, I don't get confused as
it goes through the entire

00:39:27.790 --> 00:39:30.180
melody, I never get lost and
say oh, it's the beginning

00:39:30.180 --> 00:39:31.940
again, starting over
again, starting over again.

00:39:31.940 --> 00:39:34.590
In order to do that, you have
to have a high-order memory.

00:39:34.590 --> 00:39:36.770
You don't want to get confused
if the beginning only sounds

00:39:36.770 --> 00:39:38.210
like the beginning, and you're lost.

00:39:38.210 --> 00:39:41.330
So, we need to do this and
without going into the details,

00:39:41.330 --> 00:39:44.450
the solution to this problem,
I believe, has to do with

00:39:44.450 --> 00:39:47.670
using columns of cells we
see in this in the brain.

00:39:47.670 --> 00:39:50.360
Instead of one cell for a
bit, we actually use multiples

00:39:50.360 --> 00:39:55.360
bits per, we use multiple
cells per bit in our Sparse

00:39:55.780 --> 00:39:58.380
Distributed Representation,
and this gives us this very,

00:39:58.380 --> 00:40:00.530
very high capacity memory.

00:40:00.530 --> 00:40:01.990
And I want to walk through it here.

00:40:01.990 --> 00:40:03.980
I'm just gonna tell you
that it's the variable

00:40:03.980 --> 00:40:07.130
order sequence memory, it
does multiple simultaneous

00:40:07.130 --> 00:40:09.350
predictions, it's extremely high capacity,

00:40:09.350 --> 00:40:11.310
it's a distributed memory
system meaning it's fault

00:40:11.310 --> 00:40:13.570
tolerant, you can drop out
cells, and neurons, and columns,

00:40:13.570 --> 00:40:15.592
it's just like in a real
brain, it keeps working,

00:40:15.592 --> 00:40:17.837
and it does semantic generalization.

00:40:17.837 --> 00:40:20.490
If you really want, if you
missed yesterday's talk,

00:40:20.490 --> 00:40:22.970
or if you had yesterday's
talk, you can read the full

00:40:22.970 --> 00:40:24.790
details of this, there's a
live paper on our website

00:40:24.790 --> 00:40:27.580
which tells you all about
this in great details.

00:40:27.580 --> 00:40:29.580
So you can get into well
what was he talking about,

00:40:29.580 --> 00:40:30.940
you can read about it.

00:40:30.940 --> 00:40:33.890
Okay, so that's it for my review.

00:40:33.890 --> 00:40:35.040
And now I'm gonna go forward.

00:40:35.040 --> 00:40:37.013
We're back to this situation right here.

00:40:37.013 --> 00:40:40.380
You say these are my six
attributes, I believe need to be

00:40:40.380 --> 00:40:41.770
a part of my intelligent machine.

00:40:41.770 --> 00:40:44.760
Now, you know, intelligence is a scale.

00:40:44.760 --> 00:40:46.473
It doesn't, you know, there
isn't some threshold to it.

00:40:46.473 --> 00:40:48.640
As I said, we have lots
of intelligent animals

00:40:48.640 --> 00:40:49.750
with different capabilities.

00:40:49.750 --> 00:40:52.920
And so, maybe you won't be
able to do all of these things.

00:40:52.920 --> 00:40:55.620
But a really intelligent
machine would do all these.

00:40:55.620 --> 00:40:57.340
Where are we today?

00:40:57.340 --> 00:40:58.760
Here's where we are today.

00:40:58.760 --> 00:41:01.960
Today we understand the sensory
arrays and streaming data.

00:41:01.960 --> 00:41:04.120
We are modeling this today.

00:41:04.120 --> 00:41:08.208
We understand the
sequence memory very well.

00:41:08.208 --> 00:41:10.326
We understand the Sparse
Distributed Representation

00:41:10.326 --> 00:41:11.179
very well.

00:41:11.179 --> 00:41:13.090
We partially understand the hierarchy.

00:41:13.090 --> 00:41:15.820
We have some pieces of
it, but not all of it.

00:41:15.820 --> 00:41:18.280
So some of it we've done some
simulations with hierarchy,

00:41:18.280 --> 00:41:19.920
some simulations without hierarchy.

00:41:19.920 --> 00:41:21.610
There's more work to be done there.

00:41:21.610 --> 00:41:25.490
Now I'm going to show you what
can you do with a very simple

00:41:25.490 --> 00:41:28.180
version of these, these capabilities.

00:41:28.180 --> 00:41:30.510
Can I do something, I'm not
gonna call it an intelligence

00:41:30.510 --> 00:41:31.890
machine, but it's on its way

00:41:31.890 --> 00:41:33.050
to being an intelligence machine.

00:41:33.050 --> 00:41:35.820
We're taking baby steps
there, but turns out I can do

00:41:35.820 --> 00:41:38.350
something very, very
commercially useful with just

00:41:38.350 --> 00:41:39.460
these properties.

00:41:39.460 --> 00:41:42.080
And that introduces the
concept of our product.

00:41:42.080 --> 00:41:43.780
And to tell you about our
product, I have to give you

00:41:43.780 --> 00:41:48.780
a little bit of a side version into data.

00:41:49.250 --> 00:41:51.810
I'm gonna have to talk about
world data a little bit.

00:41:51.810 --> 00:41:54.690
So bare with me, it's not
gonna be very technical at all.

00:41:54.690 --> 00:41:58.360
Today, we have the ability to
collect huge amounts of data.

00:41:58.360 --> 00:42:01.240
Perhaps you've heard about
Big Data, it's a popular term

00:42:01.240 --> 00:42:04.100
these days, and we're storing
so much data that it's sort

00:42:04.100 --> 00:42:07.000
of like the Library of Congress
is the metric for how much

00:42:07.000 --> 00:42:07.833
data we're storing.

00:42:07.833 --> 00:42:09.430
Oh, we're storing four
Libraries of Congress an hour

00:42:09.430 --> 00:42:11.080
or something like that, you know.

00:42:11.080 --> 00:42:12.420
Huge amounts of storage.

00:42:12.420 --> 00:42:13.743
And the problem is what
do we do with this data?

00:42:13.743 --> 00:42:17.009
I mean, we put it in databases
and people can look at it.

00:42:17.009 --> 00:42:20.480
Tools of visualizing, and
so on you see the patterns,

00:42:20.480 --> 00:42:22.280
and the build predictive models.

00:42:22.280 --> 00:42:24.200
They hire machine rental
is suppose to come in

00:42:24.200 --> 00:42:26.150
and they build these models,
it takes them months,

00:42:26.150 --> 00:42:28.637
and then the models get out
of date and they do it again,

00:42:28.637 --> 00:42:29.470
and so on.

00:42:29.470 --> 00:42:30.820
This is an un-scalable problem.

00:42:30.820 --> 00:42:33.520
It's not, this is not the
solution of the future.

00:42:33.520 --> 00:42:34.540
There's all these problems with it.

00:42:34.540 --> 00:42:36.340
There's problems with preparing the data.

00:42:36.340 --> 00:42:38.400
There's problems with the
models becomes obsolete.

00:42:38.400 --> 00:42:40.480
The patterns in the world
change, and someone builds

00:42:40.480 --> 00:42:42.830
a model and a lot of
them aren't good anymore.

00:42:42.830 --> 00:42:44.150
You see this in credit card fraud.

00:42:44.150 --> 00:42:46.620
People are always, there's
the fraudsters, that's what

00:42:46.620 --> 00:42:48.737
they call them, and then
there's the people who build

00:42:48.737 --> 00:42:50.900
the fraudsters detector.

00:42:50.900 --> 00:42:53.150
And so they build these models
to detect the fraudsters,

00:42:53.150 --> 00:42:54.850
and they say ah, the
model's working really well,

00:42:54.850 --> 00:42:57.010
but the fraudster goes out
and in a month they've got new

00:42:57.010 --> 00:42:58.170
ways of cheating.

00:42:58.170 --> 00:43:00.690
And it's just a race that
goes on and on all the time.

00:43:00.690 --> 00:43:02.600
And there's another problem
with this is that it tires

00:43:02.600 --> 00:43:05.270
people, lots and lots of people,
machine experts, and so on.

00:43:05.270 --> 00:43:08.120
So, this is not a scalable solution.

00:43:08.120 --> 00:43:09.740
And by the way, this is
an important problem.

00:43:09.740 --> 00:43:12.314
The world is going to be awash in data.

00:43:12.314 --> 00:43:13.840
We are going to have
trillions of data centers.

00:43:13.840 --> 00:43:16.340
You may have heard of
the Internet of Things.

00:43:16.340 --> 00:43:19.424
Everything in the world is gonna
be connected, and streaming

00:43:19.424 --> 00:43:20.257
data someplace.

00:43:20.257 --> 00:43:21.170
What are we going to do with this?

00:43:21.170 --> 00:43:22.047
Not this!

00:43:22.047 --> 00:43:25.000
Okay, the answer to this is the following.

00:43:25.000 --> 00:43:27.430
The answer is, you're gonna
take that data and you're gonna

00:43:27.430 --> 00:43:29.517
stream it to online, what
we call online mouse.

00:43:29.517 --> 00:43:32.580
There's a continuously learning
models, and you're gonna

00:43:32.580 --> 00:43:34.620
take actions directed from it.

00:43:34.620 --> 00:43:36.950
You're gonna take the data,
put it through these models,

00:43:36.950 --> 00:43:38.450
make predictions, detect anomalies,

00:43:38.450 --> 00:43:40.600
take actions, immediately.

00:43:40.600 --> 00:43:41.900
Does this sound familiar?

00:43:41.900 --> 00:43:43.300
This is what brains do.

00:43:43.300 --> 00:43:47.220
Streaming data, continuously
learning models, making

00:43:47.220 --> 00:43:50.220
predictions, detecting
anomalies, and taking action.

00:43:50.220 --> 00:43:53.210
And so there's an opportunity
here to rethink the way data

00:43:53.210 --> 00:43:54.890
is acted upon in the world.

00:43:54.890 --> 00:43:57.930
And this turns out to be a
very powerful idea, and we've

00:43:57.930 --> 00:44:00.293
built a product called
Grok which does this.

00:44:00.293 --> 00:44:04.260
The key criteria here is
the automated model creation

00:44:04.260 --> 00:44:06.630
and the continuous learning
because as the patterns

00:44:06.630 --> 00:44:08.500
change in the world, the
models have to adapt.

00:44:08.500 --> 00:44:11.021
Just like, you know, as
a human you do this too.

00:44:11.021 --> 00:44:12.980
Everyday you learn something new.

00:44:12.980 --> 00:44:14.930
You're adapting, bend some
things you've learned in the past

00:44:14.930 --> 00:44:15.763
and you're adapting new patterns.

00:44:15.763 --> 00:44:17.560
And so, you do this continuously.

00:44:17.560 --> 00:44:19.980
And it's very, very important
to find the temporal

00:44:19.980 --> 00:44:21.610
and spacial patterns in the data.

00:44:21.610 --> 00:44:23.440
Very few people look at the
temporal patterns in the data.

00:44:23.440 --> 00:44:24.720
That's what brains do.

00:44:24.720 --> 00:44:26.280
But the temporal patterns
in the data are very,

00:44:26.280 --> 00:44:27.230
very helpful.

00:44:27.230 --> 00:44:28.520
They tell you what's gonna happen next.

00:44:28.520 --> 00:44:30.800
They tell you when things are unexpected

00:44:30.800 --> 00:44:33.860
So, this is a great
model for what brains do.

00:44:33.860 --> 00:44:36.330
So we're gonna build miniature
brains, we are doing this!

00:44:36.330 --> 00:44:39.150
Build limited brains, if you
will, that take data streams

00:44:39.150 --> 00:44:41.410
and make predictions and take actions.

00:44:41.410 --> 00:44:44.420
So, building a product
is quite an endeavor.

00:44:44.420 --> 00:44:48.370
And so, I'll just walk you
through some of what this is.

00:44:48.370 --> 00:44:50.698
Here's a diagram of how we do this.

00:44:50.698 --> 00:44:53.292
On the left, I have these
three vertical bars.

00:44:53.292 --> 00:44:56.210
They're representing records
of data from some data stream.

00:44:56.210 --> 00:44:59.140
You can imagine this coming off
the same computer or server.

00:44:59.140 --> 00:45:01.310
You can imagine it coming
off sensors on a building.

00:45:01.310 --> 00:45:02.910
And these are records coming in in time.

00:45:02.910 --> 00:45:04.910
They may be coming in very
rapidly, once a minute,

00:45:04.910 --> 00:45:06.100
once every five seconds.

00:45:06.100 --> 00:45:08.099
They may be coming in
slower, like once an hour.

00:45:08.099 --> 00:45:10.350
They have multiple fields.

00:45:10.350 --> 00:45:12.580
They might be numbers and
categories, and things like that.

00:45:12.580 --> 00:45:14.450
But we feed them into our system.

00:45:14.450 --> 00:45:16.385
The first thing we have to
do is we have to turn these

00:45:16.385 --> 00:45:19.200
inputs into Sparse
Distributed Representations.

00:45:19.200 --> 00:45:20.790
That was one of my criterias.

00:45:20.790 --> 00:45:22.760
The sensors have to produce extremely

00:45:22.760 --> 00:45:24.330
Sparse Distributed Representations.

00:45:24.330 --> 00:45:25.580
So we have a way of doing that.

00:45:25.580 --> 00:45:26.600
I don't mind telling you about it.

00:45:26.600 --> 00:45:28.460
But I'm not gonna go
through it in this talk.

00:45:28.460 --> 00:45:30.141
We can take numbers and
turn them into Sparse

00:45:30.141 --> 00:45:32.360
Distributed Representations
that have the right properties.

00:45:32.360 --> 00:45:35.280
We can take categories of
information, like you know,

00:45:35.280 --> 00:45:37.287
male female, and days of the
week, and things like that,

00:45:37.287 --> 00:45:39.710
and we can turn them into Sparse
Distributed Representations

00:45:39.710 --> 00:45:40.850
and we do that.

00:45:40.850 --> 00:45:44.150
Then we feed them into
this sequence memory.

00:45:44.150 --> 00:45:48.876
It's corticolmar, 2000
columns, 60000 cells, you know,

00:45:48.876 --> 00:45:52.440
300 million synapses doing
the thing I just talked about

00:45:52.440 --> 00:45:54.110
before, those columns and all that stuff.

00:45:54.110 --> 00:45:56.654
And from that we make
predictions and take actions.

00:45:56.654 --> 00:45:59.360
Here's what a user would
do with this system.

00:45:59.360 --> 00:46:00.770
They don't need to know any of this stuff.

00:46:00.770 --> 00:46:02.750
The users essentially
says oh, here's my data,

00:46:02.750 --> 00:46:05.317
I have a stream of data
coming from this building.

00:46:05.317 --> 00:46:07.440
For example, I want to find
a problem and try to make

00:46:07.440 --> 00:46:09.760
predictions every so often,
and here's what I'm trying

00:46:09.760 --> 00:46:13.420
to predict, and then our
product Grok basically creates

00:46:13.420 --> 00:46:15.200
these models, figures out
how to do them and runs

00:46:15.200 --> 00:46:17.680
continuously, it finds
spatial temporal patterns

00:46:17.680 --> 00:46:19.570
in the data, and it makes predictions.

00:46:19.570 --> 00:46:22.575
And it can tell you the
probability of these predictions.

00:46:22.575 --> 00:46:25.434
And there's lots of areas
for applications here.

00:46:25.434 --> 00:46:28.900
Energy pricing, energy
demand, product forecasting,

00:46:28.900 --> 00:46:30.710
ad network returns, et. cetera.

00:46:30.710 --> 00:46:32.990
We have all kinds of people
trying to do machine efficiency,

00:46:32.990 --> 00:46:34.867
kind of predicting which machines to use

00:46:34.867 --> 00:46:36.490
and when to use them,
and things like that.

00:46:36.490 --> 00:46:38.140
I'm gonna give you just a
couple of examples so you get

00:46:38.140 --> 00:46:40.428
a flavor for what this is like.

00:46:40.428 --> 00:46:42.800
This is not an important detail,
but for the computer people

00:46:42.800 --> 00:46:44.020
in the room you might care about this.

00:46:44.020 --> 00:46:46.990
Today, this is implemented
on a cloud server,

00:46:46.990 --> 00:46:48.380
an Amazon cloud server.

00:46:48.380 --> 00:46:49.380
Doesn't have to be.

00:46:49.380 --> 00:46:51.250
This could be embedded in
things, could be embedded

00:46:51.250 --> 00:46:52.820
in cars, chips, whatever.

00:46:52.820 --> 00:46:55.150
But basically today, we've
implemented our first little

00:46:55.150 --> 00:46:57.870
brain models as a service on the cloud.

00:46:57.870 --> 00:47:01.680
So I'll walk you through
a couple of simple slides,

00:47:01.680 --> 00:47:04.830
an easy to understand version
of this which is energy.

00:47:04.830 --> 00:47:07.650
You may not be aware of
this, but large consumers

00:47:07.650 --> 00:47:12.650
of electricity, they pay and
they decide how much energy

00:47:13.090 --> 00:47:15.870
they're going to use sometimes
on an hour by hour basis.

00:47:15.870 --> 00:47:19.170
Now there's a market for like
a large factory when they

00:47:19.170 --> 00:47:21.910
talk to utilities, utilities
say well I'll sell you

00:47:21.910 --> 00:47:23.640
this electricity at four
o'clock in the afternoon

00:47:23.640 --> 00:47:26.930
at this price if you take so
much, or don't use so much.

00:47:26.930 --> 00:47:29.680
And then the consumer of that
energy says okay, I'll either

00:47:29.680 --> 00:47:30.804
do that now or not.

00:47:30.804 --> 00:47:33.960
Then some people they will
pre-cool buildings because they

00:47:33.960 --> 00:47:36.880
know that the electricity's
gonna be more expensive

00:47:36.880 --> 00:47:37.800
later in the day.

00:47:37.800 --> 00:47:40.990
So there's this market going
on for larger consumers

00:47:40.990 --> 00:47:43.460
of power that you're
probably not aware of.

00:47:43.460 --> 00:47:45.800
It's called the Man Response
Market, and if you can make

00:47:45.800 --> 00:47:48.960
that more efficient, you can
save energy and save money.

00:47:48.960 --> 00:47:52.120
So here's a typical example,
here's an energy profile

00:47:52.120 --> 00:47:55.440
of a building, it's some
factory, and you can see there's

00:47:55.440 --> 00:47:56.273
a pattern here.

00:47:56.273 --> 00:47:59.220
It turns out these peaks were
caused by day of the week,

00:47:59.220 --> 00:48:01.270
so you can see there's five
days a week and then a weekend

00:48:01.270 --> 00:48:02.750
comes along the factory shut down,

00:48:02.750 --> 00:48:04.080
and nothing's going on there.

00:48:04.080 --> 00:48:06.550
Now we can feed this kind
of energy thing into Grok,

00:48:06.550 --> 00:48:09.060
and it can learn this, this
looks like a fairly simple

00:48:09.060 --> 00:48:10.380
pattern, but it's not as
simple as you think it is.

00:48:10.380 --> 00:48:15.380
So, in this case, the customer
and what they want to do,

00:48:16.400 --> 00:48:19.520
is they said at midnight,
I want you to predict

00:48:19.520 --> 00:48:21.770
the amount of energy that's
going to be used every hour

00:48:21.770 --> 00:48:22.830
for the next 24 hours.

00:48:22.830 --> 00:48:24.187
That's their problem,
so we have to do that.

00:48:24.187 --> 00:48:26.530
And we can do that using
these kind of models.

00:48:26.530 --> 00:48:29.700
So, that little red line says
look, can we predict that.

00:48:29.700 --> 00:48:32.160
And so the next slide
here I'm gonna show you,

00:48:32.160 --> 00:48:34.616
this is just showing in
the red is all predicted

00:48:34.616 --> 00:48:35.880
in the blue, and the blue is our actuals.

00:48:35.880 --> 00:48:38.630
And, this is your streams
of data into Grok.

00:48:38.630 --> 00:48:39.910
Now Grok doesn't know anything
at all about what this

00:48:39.910 --> 00:48:40.990
information represents.

00:48:40.990 --> 00:48:43.431
It doesn't know if it's
energy or you know,

00:48:43.431 --> 00:48:45.110
grams of alcohol.

00:48:45.110 --> 00:48:46.010
It doesn't really care.

00:48:46.010 --> 00:48:48.560
It's just a number and it
looks and finds these temporal

00:48:48.560 --> 00:48:50.720
patterns and says oh, I can
see these temporal patterns.

00:48:50.720 --> 00:48:52.750
And it looks like it's
doing a pretty good job.

00:48:52.750 --> 00:48:55.010
It actually is in this case,
although you can't tell

00:48:55.010 --> 00:48:56.600
too well just by looking at these graphs.

00:48:56.600 --> 00:48:59.030
But trust me, the customer
is very happy with this,

00:48:59.030 --> 00:49:00.867
and it is doing something
very significant.

00:49:00.867 --> 00:49:04.139
And, here's a situation where Grok started

00:49:04.139 --> 00:49:04.972
to make a mistake.

00:49:04.972 --> 00:49:06.680
There was three days in the
week, and you can see right

00:49:06.680 --> 00:49:08.620
down here it starts
predicting the next day.

00:49:08.620 --> 00:49:10.930
Well, it turns out this
was a European holiday,

00:49:10.930 --> 00:49:14.430
and it hadn't seen that
before, and it says oh well,

00:49:14.430 --> 00:49:16.760
oops, oh no it's not, and it
quickly says no that's not

00:49:16.760 --> 00:49:17.593
the pattern.

00:49:17.593 --> 00:49:18.426
Here's another pattern.

00:49:18.426 --> 00:49:19.830
This looks like the one I'm
suppose is, it's suppose

00:49:19.830 --> 00:49:21.496
to be like this, and
it quickly be covered.

00:49:21.496 --> 00:49:23.871
Um, there's another example.

00:49:23.871 --> 00:49:25.980
This one looks a little bit harder.

00:49:25.980 --> 00:49:28.860
So I'm just gonna save you
flavor by looking at it.

00:49:28.860 --> 00:49:30.620
There's a company we work
with that's trying to demand,

00:49:30.620 --> 00:49:32.880
to predict how much
demand for their service.

00:49:32.880 --> 00:49:36.720
And they have a service which
is encoding videos on the web.

00:49:36.720 --> 00:49:39.180
And so customer sends a
video, it has to be encoded

00:49:39.180 --> 00:49:40.880
in many different formats,
so you can look at it on your

00:49:40.880 --> 00:49:42.270
phone and other things,

00:49:42.270 --> 00:49:43.900
and they want that immediate response.

00:49:43.900 --> 00:49:45.640
As soon as they start
sending that video, they want

00:49:45.640 --> 00:49:47.720
the encoding to appear on the web.

00:49:47.720 --> 00:49:51.001
And so this company has
to leave lots of computers

00:49:51.001 --> 00:49:52.930
around running all the time
doing nothing, because they want

00:49:52.930 --> 00:49:55.776
to be able to make sure they
can catch a spike in demand.

00:49:55.776 --> 00:49:58.300
And this graph showed you
that the demand is actually

00:49:58.300 --> 00:50:01.610
very spiked, goes up and down
all over the place, very hard

00:50:01.610 --> 00:50:03.390
to see what the patterns in there are.

00:50:03.390 --> 00:50:05.882
And yet, we can run something
like this through Grok,

00:50:05.882 --> 00:50:07.360
and Grok will say maybe
there's there's patterns

00:50:07.360 --> 00:50:09.640
in the afternoon, maybe when
school gets out, who knows

00:50:09.640 --> 00:50:11.010
what, it doesn't really matter.

00:50:11.010 --> 00:50:13.390
If there's patterns here,
I'm gonna try to find them.

00:50:13.390 --> 00:50:15.030
It can't do a perfect job.

00:50:15.030 --> 00:50:17.150
But in this case, it did a
good enough job that they can

00:50:17.150 --> 00:50:20.430
save about 15 percent of
their cost, which is very

00:50:20.430 --> 00:50:21.830
significant to them.

00:50:21.830 --> 00:50:24.850
So, this is the kind of
things we're applying it to.

00:50:24.850 --> 00:50:26.510
Now here I'm gonna show you
something a little bit more

00:50:26.510 --> 00:50:29.210
technical, and I hope you can follow this.

00:50:29.210 --> 00:50:31.119
On the right, just pay attention
to the thing on the right.

00:50:31.119 --> 00:50:34.060
I told you that models
have these like 2000 bits,

00:50:34.060 --> 00:50:36.620
these 2000 columns of cells if you will.

00:50:36.620 --> 00:50:39.550
There's 2000 little circles
on that drawing there.

00:50:39.550 --> 00:50:42.831
And these are representing
the activation, the internal

00:50:42.831 --> 00:50:43.664
activations of our cortical model.

00:50:43.664 --> 00:50:46.510
These are like the 2000
columns in our cortical model

00:50:46.510 --> 00:50:47.343
that we're running.

00:50:47.343 --> 00:50:48.900
We're just looking down on
top of them if you will.

00:50:48.900 --> 00:50:51.990
And the green dot means,
if you counted them,

00:50:51.990 --> 00:50:53.969
there should be 40 green
dots there don't bother,

00:50:53.969 --> 00:50:56.700
if you count them there'd
be 40 green dots there.

00:50:56.700 --> 00:51:00.320
These are both predicted
and what actually happened.

00:51:00.320 --> 00:51:03.010
So, Grok was saying okay,
in the next representation

00:51:03.010 --> 00:51:05.890
I'm expecting these 40
attributes to be active,

00:51:05.890 --> 00:51:08.050
and it turns out these 40
attributes were active.

00:51:08.050 --> 00:51:09.060
It was a perfect prediction.

00:51:09.060 --> 00:51:10.809
It happens a lot, but not always.

00:51:10.809 --> 00:51:14.560
Here's another one where
those little blue circles

00:51:14.560 --> 00:51:16.730
are things that were
predicted that didn't happen.

00:51:16.730 --> 00:51:18.540
But everything that did
happen was predicted.

00:51:18.540 --> 00:51:20.153
So this is again a
multi-prediction going on.

00:51:20.153 --> 00:51:23.170
This says okay, I can see
three or four different things

00:51:24.323 --> 00:51:25.170
occurring now, one of
them actually did occur.

00:51:25.170 --> 00:51:26.300
That's good, we like that.

00:51:26.300 --> 00:51:27.890
So that's what's representing there.

00:51:27.890 --> 00:51:30.084
The blue circles were
predicted but didn't happen.

00:51:30.084 --> 00:51:33.510
And finally, here's a situation
you probably have trouble

00:51:33.510 --> 00:51:35.490
seeing this, there's a bunch
of little red circles on here.

00:51:35.490 --> 00:51:37.290
If you can't see that, trust
me, there's a bunch of little

00:51:37.290 --> 00:51:39.720
red circles, so we have
red circles, green circles,

00:51:39.720 --> 00:51:40.660
and little blue circles.

00:51:40.660 --> 00:51:44.170
And what's going on here
is Grok made a prediction.

00:51:44.170 --> 00:51:47.690
Some of those attributes
turned out to be true,

00:51:47.690 --> 00:51:48.940
those are the green dots.

00:51:49.877 --> 00:51:51.090
Some of the things were
predicted didn't happen,

00:51:51.090 --> 00:51:52.250
not a problem.

00:51:52.250 --> 00:51:54.480
But some things occurred
which weren't predicted.

00:51:54.480 --> 00:51:56.300
Some attributes occurred
in the input stream

00:51:56.300 --> 00:51:58.190
which weren't predicted, and
those are the red circles.

00:51:58.190 --> 00:52:01.103
And, what the point of the
slide is to say when you make

00:52:01.103 --> 00:52:05.050
an error in prediction,
it's not a binary thing.

00:52:05.050 --> 00:52:06.280
It's not one or nothing.

00:52:06.280 --> 00:52:07.838
It's a very nuanced thing.

00:52:07.838 --> 00:52:10.070
There's somethings that
are right, and some things

00:52:10.070 --> 00:52:10.903
that are wrong.

00:52:10.903 --> 00:52:12.800
And if you actually could go
and look and probe into this,

00:52:12.800 --> 00:52:15.540
you'd find that you could
tell what semantic was correct

00:52:15.540 --> 00:52:17.340
and what semantically was incorrect.

00:52:18.430 --> 00:52:21.620
It's a nuanced thing about
what an anomaly is or what

00:52:21.620 --> 00:52:23.470
a prediction is and so on.

00:52:23.470 --> 00:52:25.430
So this is the kind of
stuff that we do internally,

00:52:25.430 --> 00:52:27.880
and you can use this in various ways.

00:52:27.880 --> 00:52:31.400
Here's an example, and this is
my last example in this thing

00:52:31.400 --> 00:52:34.900
of a windmill.

00:52:34.900 --> 00:52:36.980
This is one of those
huge windmills off shore,

00:52:36.980 --> 00:52:38.480
have you seen these
offshore windmill farms?

00:52:38.480 --> 00:52:39.313
It's amazing.

00:52:39.313 --> 00:52:40.870
These things are like monstrous.

00:52:40.870 --> 00:52:42.620
And the North Sea they have quite a few.

00:52:42.620 --> 00:52:45.400
And out there in the sea, these
monstrous windmills running

00:52:45.400 --> 00:52:47.930
24 hours a day, and they're
very, very expensive.

00:52:47.930 --> 00:52:50.940
And if they fail, it's very
expensive to replace the parts.

00:52:50.940 --> 00:52:52.940
Like, you know, the gear box
costs several hundred fifty

00:52:52.940 --> 00:52:55.657
thousands dollars, and probably
cost $100,000 to replace it.

00:52:55.657 --> 00:53:00.110
So, if they can, if they can
detect anomalies and detect

00:53:00.110 --> 00:53:02.590
before failures occur,
it's worth a lot of energy

00:53:02.590 --> 00:53:03.900
and money and so on.

00:53:03.900 --> 00:53:07.510
So this blue line happens
to be the energy consum--

00:53:07.510 --> 00:53:11.750
Excuse me, it's the temperature
of the oil in a gear box

00:53:11.750 --> 00:53:14.670
in a windmill in the North Sea, okay?

00:53:14.670 --> 00:53:17.100
I forget what year, what time
this is, maybe it says it.

00:53:17.100 --> 00:53:18.210
Yeah, in 2011.

00:53:18.210 --> 00:53:19.980
This is the last one.

00:53:19.980 --> 00:53:22.320
So, you can see that temperature
is going up and down,

00:53:22.320 --> 00:53:26.310
overlapping all the time as
the wind goes and changes.

00:53:26.310 --> 00:53:28.120
It's a very complex pattern.

00:53:28.120 --> 00:53:30.950
And, on the bottom on the
right there, you see sort of

00:53:30.950 --> 00:53:33.910
an aggregated anomaly score for Grok.

00:53:33.910 --> 00:53:35.540
Grok is saying oh I'm trying
to predict what's going on

00:53:35.540 --> 00:53:38.090
here, I can't predict all
this stuff, but I'm looking

00:53:38.090 --> 00:53:40.150
for patterns that I haven't seen before.

00:53:40.150 --> 00:53:44.453
And you can see on the down
here, there's a peak there,

00:53:44.453 --> 00:53:46.770
there's actually two peaks
on the anomaly score.

00:53:46.770 --> 00:53:49.460
And what's happening here
is, there's nothing, if you

00:53:49.460 --> 00:53:51.510
looked at the temperature of
the gear box, there's nothing

00:53:51.510 --> 00:53:54.240
wrong with the temperature, it's in range.

00:53:54.240 --> 00:53:56.200
But the pattern is wrong.

00:53:56.200 --> 00:53:58.900
It's like I'm listening
for a melody, and the notes

00:53:58.900 --> 00:54:00.270
are in the wrong order.

00:54:00.270 --> 00:54:01.330
And so, we can say you know what?

00:54:01.330 --> 00:54:03.810
There's nothing out of range
here, but it's not like I've

00:54:03.810 --> 00:54:04.940
seen it before.

00:54:04.940 --> 00:54:06.700
And it says, you know what, I
think you outta look at this.

00:54:06.700 --> 00:54:09.390
And it turns out, that indeed,
very shortly thereafter

00:54:09.390 --> 00:54:10.643
there was a maintenance event.

00:54:10.643 --> 00:54:12.780
So this is an encouraging type of time.

00:54:12.780 --> 00:54:16.100
And this is worth a lot of
money and saves a lot of energy

00:54:16.100 --> 00:54:16.933
and so on in the world.

00:54:16.933 --> 00:54:19.600
So, that's the end of
my discussion on Grok.

00:54:19.600 --> 00:54:22.900
So now I'm gonna talk to the
end of my talk, which is about

00:54:22.900 --> 00:54:24.320
the future of machine intelligence

00:54:24.320 --> 00:54:26.180
and why we should do this.

00:54:26.180 --> 00:54:27.490
Where is this all gonna go?

00:54:27.490 --> 00:54:30.760
Can we build you know,
really crazy great machine.

00:54:30.760 --> 00:54:32.200
Well, it turns out, yes we can.

00:54:32.200 --> 00:54:35.100
I'm absolutely certain we can
build amazingly intelligent

00:54:35.100 --> 00:54:37.520
machines, but the question,
what are they going to be like?

00:54:37.520 --> 00:54:38.353
And what are they going to do?

00:54:38.353 --> 00:54:39.817
Is this good or is this bad?

00:54:39.817 --> 00:54:42.250
And how are they going to be amazing?

00:54:42.250 --> 00:54:43.824
What's unusual about them?

00:54:43.824 --> 00:54:44.657
And so on.

00:54:44.657 --> 00:54:45.740
So, let's just walk through
some of these things.

00:54:45.740 --> 00:54:48.060
You know, there's two basic views on this.

00:54:48.060 --> 00:54:49.911
One view is this is bad.

00:54:49.911 --> 00:54:53.180
You know, Skynet, I'm not
a Science Fiction fan,

00:54:53.180 --> 00:54:56.980
but I know what these are now,
Skynet is this bad machine

00:54:56.980 --> 00:54:58.090
intelligence that takes over the word.

00:54:58.090 --> 00:54:59.575
So there's the Matrix.

00:54:59.575 --> 00:55:02.200
It's like we're all being plugged
in, we don't even know it.

00:55:02.200 --> 00:55:04.890
We're being consumed for
food or something like that.

00:55:04.890 --> 00:55:07.770
There's the Terminator I,
that was the bad Terminator.

00:55:07.770 --> 00:55:09.255
You know, robot guy.

00:55:09.255 --> 00:55:12.000
Then there's the benign view.

00:55:12.000 --> 00:55:14.560
You know we all wanna have
CP3 around our house, helping

00:55:14.560 --> 00:55:17.030
us out, do things, or maybe
we'll all being playing games

00:55:17.030 --> 00:55:19.470
with gloctomite things in the
future, or maybe we'll get our

00:55:19.470 --> 00:55:21.910
entertainment by donning one
of these hats and sitting

00:55:21.910 --> 00:55:24.276
back and going wow,
that's great, who knows.

00:55:24.276 --> 00:55:27.040
Then there's, the ambiguous
thing in the middle.

00:55:27.040 --> 00:55:29.679
Well, we thought it was good
and it turned bad, you know.

00:55:29.679 --> 00:55:30.933
That's how.

00:55:32.542 --> 00:55:34.620
But let's just talk about my
opinion about these things.

00:55:34.620 --> 00:55:36.170
I'm gonna talk about some
things that are definitely

00:55:36.170 --> 00:55:37.290
going to happen in my opinion.

00:55:37.290 --> 00:55:38.990
Absolutely gonna happen.

00:55:38.990 --> 00:55:42.990
Number one, we can make
artificial brains that are faster

00:55:42.990 --> 00:55:44.920
than biological brains.

00:55:44.920 --> 00:55:46.900
Biological brains are
really slow, actually.

00:55:46.900 --> 00:55:49.420
Neurons can't do anything
faster than five milliseconds.

00:55:49.420 --> 00:55:50.810
That's like the flaw.

00:55:50.810 --> 00:55:53.160
But we can do things in
silicon that are million times

00:55:53.160 --> 00:55:54.090
faster than that.

00:55:54.090 --> 00:55:57.150
So we build machines with
those principles, we can make

00:55:57.150 --> 00:56:00.600
brains that are millions of
times faster than humans.

00:56:00.600 --> 00:56:01.950
I'm sure this is gonna happen.

00:56:01.950 --> 00:56:05.290
And I have all kinds of
interesting ideas about how

00:56:05.290 --> 00:56:06.350
would you take advantage of that.

00:56:06.350 --> 00:56:08.710
Certainly you could get to
conclusions quicker, but more

00:56:08.710 --> 00:56:10.810
importantly can I deal with
things that are very high

00:56:10.810 --> 00:56:12.790
velocity data streamers.

00:56:12.790 --> 00:56:14.810
Can I do things that are working non-stop.

00:56:14.810 --> 00:56:17.580
Can I have a thousand physicists
working 24 hours a day

00:56:17.580 --> 00:56:19.260
on some problem that would
take a thousand years

00:56:19.260 --> 00:56:20.830
to figure it out, type of thing.

00:56:20.830 --> 00:56:22.190
I think that's gonna happen.

00:56:22.190 --> 00:56:23.690
The second thing, there's
no reason why we can't

00:56:23.690 --> 00:56:26.110
make artificial brains
Intelligence Machines

00:56:26.110 --> 00:56:27.960
that are bigger, not physically bigger,

00:56:27.960 --> 00:56:30.000
but bigger in their memory capacity.

00:56:30.000 --> 00:56:33.040
Bigger hierarchies, more
regions, more size, and so on.

00:56:33.040 --> 00:56:33.903
Why not?

00:56:33.903 --> 00:56:36.730
Our brains are constrained
by the birth canal.

00:56:36.730 --> 00:56:38.340
You know, we're pushing the limit.

00:56:38.340 --> 00:56:42.180
And we have a, naturally we
have a high death rate in birth

00:56:42.180 --> 00:56:44.620
of humans because of this problem.

00:56:44.620 --> 00:56:46.450
Nature might want to build
us bigger brains, but they

00:56:46.450 --> 00:56:47.622
don't come out.

00:56:47.622 --> 00:56:50.410
And so, you know, we
don't have that problem.

00:56:50.410 --> 00:56:52.130
We can make them as big as we want.

00:56:52.130 --> 00:56:54.370
And they will have more knowledge,

00:56:54.370 --> 00:56:56.209
and have deeper insights.

00:56:56.209 --> 00:56:58.940
We don't know yet, but I'm
certain this is going to happen.

00:56:58.940 --> 00:57:01.560
Another area that's very, very
interesting, we don't have

00:57:01.560 --> 00:57:04.170
to stick with any kind of
senses that humans or other

00:57:04.170 --> 00:57:05.003
kinds of animals have.

00:57:05.003 --> 00:57:06.438
We know first of all there's
a diversity of sensors

00:57:06.438 --> 00:57:09.990
in the biological world,
we just have one set,

00:57:09.990 --> 00:57:12.450
but you can imagine senses that are huge!

00:57:12.450 --> 00:57:14.490
Arrays that cover entire planet.

00:57:14.490 --> 00:57:17.520
You can imagine senses
that measure things really

00:57:17.520 --> 00:57:20.590
microscore, microscopically,
nano-sensors that are looking

00:57:20.590 --> 00:57:22.360
at protein folding and things like that.

00:57:22.360 --> 00:57:24.660
And then, these artificial
brains, these machine

00:57:24.660 --> 00:57:27.170
intelligence will be
able to understand worlds

00:57:27.170 --> 00:57:28.820
that we have trouble understanding.

00:57:28.820 --> 00:57:30.780
Everything a human has to
understand, we have to put into

00:57:30.780 --> 00:57:33.470
something that runs at our
speed and through our senses.

00:57:33.470 --> 00:57:35.420
We have to come up with
visualization tools,

00:57:35.420 --> 00:57:37.030
so if I'm trying to
understand protein folding,

00:57:37.030 --> 00:57:38.243
I gotta have visual graphics and so on.

00:57:38.243 --> 00:57:40.100
It's not a very good fit.

00:57:40.100 --> 00:57:44.420
But if I had sensors that
really could live in that world,

00:57:44.420 --> 00:57:45.840
they would think in that world.

00:57:45.840 --> 00:57:47.180
We could have 3D sensors.

00:57:47.180 --> 00:57:49.340
Right now our sensors are
all 3-dimensional seeds.

00:57:49.340 --> 00:57:50.940
The theory says that there's
no reason you can't have

00:57:50.940 --> 00:57:52.670
higher dimensional sensor arrays.

00:57:52.670 --> 00:57:53.530
So that's really cool.

00:57:53.530 --> 00:57:55.001
I think a lot of that
stuff's gonna happen.

00:57:55.001 --> 00:57:57.230
You know, fluid robotics.

00:57:57.230 --> 00:57:58.850
Today we have no fluid robotics.

00:57:58.850 --> 00:58:01.270
If you look at the robots today,
they're so clunky and slow

00:58:01.270 --> 00:58:02.103
and difficult.

00:58:02.103 --> 00:58:02.970
We're not even close to this.

00:58:02.970 --> 00:58:05.315
But we will have fluid robotics
based on these principles

00:58:05.315 --> 00:58:06.820
where machines can operate.

00:58:06.820 --> 00:58:09.300
Again, I'm not saying they
look like a human necessarily,

00:58:09.300 --> 00:58:11.750
but they'll be able to
do things very carefully.

00:58:11.750 --> 00:58:14.210
And I'm sure we can create
intelligent machines

00:58:14.210 --> 00:58:17.210
that will explode the universe for us.

00:58:17.210 --> 00:58:19.900
Okay, and another interesting
idea is the whole idea

00:58:19.900 --> 00:58:21.550
of the distributed hierarchy.

00:58:21.550 --> 00:58:23.020
Now I told you, you know,
we have this hierarchy

00:58:23.020 --> 00:58:25.250
in the brain, in our
heads and inner cortex.

00:58:25.250 --> 00:58:26.660
And that's how everything works.

00:58:26.660 --> 00:58:28.450
But what if I could distribute it.

00:58:28.450 --> 00:58:30.750
What if I could have parts
of the hierarchy all around

00:58:30.750 --> 00:58:31.710
the world?

00:58:31.710 --> 00:58:33.850
And then they combine together and so on.

00:58:33.850 --> 00:58:36.069
I can model very, very large systems.

00:58:36.069 --> 00:58:39.000
These are ideas that who
knows which of these are going

00:58:39.000 --> 00:58:40.920
to take place, we don't really
know where these are going

00:58:40.920 --> 00:58:44.580
to go, the history of
technology is it goes in ways

00:58:44.580 --> 00:58:46.009
we just don't anticipate.

00:58:46.009 --> 00:58:49.461
And often, to tremendous
light and benefit for us.

00:58:49.461 --> 00:58:51.760
And who could have anticipated
where computers were going

00:58:51.760 --> 00:58:54.100
to go, where they are today,
with GPS and cell phones

00:58:54.100 --> 00:58:54.933
in your pocket?

00:58:54.933 --> 00:58:57.000
50 years ago, no one
could have imagined that.

00:58:57.000 --> 00:58:59.280
Who could have predicted
our communications abilities

00:58:59.280 --> 00:59:01.230
starting with the telegraph,
what we can do today?

00:59:01.230 --> 00:59:02.670
Unbelievable.

00:59:02.670 --> 00:59:05.150
And so these are going to go
in these directions as well.

00:59:05.150 --> 00:59:07.330
Here's some things that might
happen, I don't really know.

00:59:07.330 --> 00:59:08.740
I'm kind of on the fence about it.

00:59:08.740 --> 00:59:10.310
One is, humanoid robots.

00:59:10.310 --> 00:59:12.250
Are we really gonna create CP3O's?

00:59:12.250 --> 00:59:13.083
I'm not so certain.

00:59:13.083 --> 00:59:14.910
You know, I suppose if you
really, really wanted to,

00:59:14.910 --> 00:59:16.700
I suppose you could, but I'm
not sure we really, really

00:59:16.700 --> 00:59:17.533
want to.

00:59:18.670 --> 00:59:20.860
This is not about you
know, science fiction.

00:59:20.860 --> 00:59:23.410
This is about building tools
for us as humans to make

00:59:23.410 --> 00:59:25.271
our lives better and discover things.

00:59:25.271 --> 00:59:27.400
And so will this happen?

00:59:27.400 --> 00:59:28.233
I don't know.

00:59:28.233 --> 00:59:30.000
If you want to do that, you
will have to build a lot

00:59:30.000 --> 00:59:32.270
of things that I didn't talk about today.

00:59:32.270 --> 00:59:34.320
Probably have to have emotions,
you'd have to have bodies...

00:59:34.320 --> 00:59:36.030
Oops.

00:59:36.030 --> 00:59:37.630
You have to have bodies and so on.

00:59:37.630 --> 00:59:39.151
So maybe that'll happen, maybe not.

00:59:39.151 --> 00:59:40.690
Who knows?

00:59:40.690 --> 00:59:43.730
I'm not so certain about this
computer/brain interface.

00:59:43.730 --> 00:59:45.620
Now this is really cool work
being done in this area,

00:59:45.620 --> 00:59:49.200
some here right at Berkeley,
where you know, for people

00:59:49.200 --> 00:59:54.200
who have damaged their nerve
system, or something like that,

00:59:54.960 --> 00:59:56.960
we can put patches on their
brains, and they can learn

00:59:56.960 --> 00:59:57.930
to control things.

00:59:57.930 --> 01:00:01.502
We already have artificial
cochleas that work very well.

01:00:01.502 --> 01:00:04.320
And so the idea that we
might create an interface

01:00:04.320 --> 01:00:07.830
between our brain and the
rest of the world, you know,

01:00:07.830 --> 01:00:10.240
certainly it's going to
happen at some level, but I'm

01:00:10.240 --> 01:00:12.520
asking the question like, are
we going to plug ourselves

01:00:12.520 --> 01:00:14.960
in in the evening and you
know, be totally wired?

01:00:14.960 --> 01:00:17.050
I don't think so, but
I'm not going to discount

01:00:17.050 --> 01:00:18.352
it completely.

01:00:18.352 --> 01:00:19.423
You know, who knows.

01:00:19.423 --> 01:00:21.060
It's very difficult to tell these things.

01:00:21.060 --> 01:00:24.050
Here's some things that I don't
think are going to happen.

01:00:24.050 --> 01:00:24.883
I really just don't.

01:00:24.883 --> 01:00:26.719
I'm very, very doubtful
this is going to happen.

01:00:26.719 --> 01:00:29.370
This is a popular thing these days.

01:00:29.370 --> 01:00:30.670
Ray Criswell talks about this.

01:00:30.670 --> 01:00:32.090
Like, you're gonna upload your brain.

01:00:32.090 --> 01:00:35.500
So, here I am, say okay, Jeff
Hawkins is standing here,

01:00:35.500 --> 01:00:38.260
I'm gonna take all my brain
connections, I'm gonna stick

01:00:38.260 --> 01:00:40.030
'em in some artificial
machine over here, and I will

01:00:40.030 --> 01:00:42.262
transfer myself from here
to here, and I will have

01:00:42.262 --> 01:00:45.480
immortality, or I'll have super powers

01:00:45.480 --> 01:00:46.630
or something like this.

01:00:47.500 --> 01:00:49.250
I believe this is a fantasy.

01:00:49.250 --> 01:00:52.820
And, theoretically it's possible,
but if you know anything

01:00:52.820 --> 01:00:56.200
about brains and how they
really work, I don't think

01:00:56.200 --> 01:00:57.033
it's possible.

01:00:57.033 --> 01:00:58.869
I don't think we'll ever get
the technology to do that.

01:00:58.869 --> 01:01:02.281
And, it's also, I think it
would be a very unsatisfactory

01:01:02.281 --> 01:01:03.513
experience.

01:01:05.060 --> 01:01:08.650
Now imagine I was sitting here
and we were able to transfer

01:01:08.650 --> 01:01:13.650
my connections into my little
machine doppler over here.

01:01:13.860 --> 01:01:17.070
And, and I flipped the
switch, lights flash,

01:01:17.070 --> 01:01:18.345
and bingo it happens.

01:01:18.345 --> 01:01:20.640
Now, this guy over here says wow!

01:01:20.640 --> 01:01:22.325
Jeff, I'm Jeff Hawkins, I'm over here.

01:01:22.325 --> 01:01:23.860
But I'm still here.

01:01:23.860 --> 01:01:25.540
I says no, I'm still here,
I didn't go anywhere.

01:01:25.540 --> 01:01:28.080
You know, we can get rid of
you, the biological Jeff,

01:01:28.080 --> 01:01:29.920
because we don't need you anymore.

01:01:29.920 --> 01:01:31.700
I'm like whoa, don't do that!

01:01:31.700 --> 01:01:33.970
You know, I don't think
this is kinda weird stuff.

01:01:33.970 --> 01:01:35.278
So I don't think this is gonna happen.

01:01:35.278 --> 01:01:38.512
I don't think we're
gonna have evil robots.

01:01:38.512 --> 01:01:40.890
This is another science fiction fantasy.

01:01:40.890 --> 01:01:43.300
Every time a new technology
comes along people imagine

01:01:43.300 --> 01:01:45.400
how it's gonna just destroy the universe.

01:01:45.400 --> 01:01:46.250
And so, it's not gonna happen.

01:01:46.250 --> 01:01:49.000
Someone has to go really,
really out of the way

01:01:49.000 --> 01:01:49.860
to make this happen.

01:01:49.860 --> 01:01:51.670
I tell you, the only thing
that could be really, really

01:01:51.670 --> 01:01:54.000
dangerous if self-replication.

01:01:54.000 --> 01:01:55.480
That's the dangerous thing in the world.

01:01:55.480 --> 01:01:56.920
Machines that can self-replicate.

01:01:56.920 --> 01:01:58.400
Or anything that can self-replicate.

01:01:58.400 --> 01:02:00.330
Viruses, and so on, so as
long as you don't make our

01:02:00.330 --> 01:02:03.000
intelligent machines
self-replicating, which is a totally

01:02:03.000 --> 01:02:04.810
separate field and I'm not gonna go there.

01:02:04.810 --> 01:02:06.589
Then, we don't have to
worry about hero robots.

01:02:06.589 --> 01:02:07.803
They're not gonna be feeling
imprisoned or any of that

01:02:07.803 --> 01:02:09.293
kind of stuff.

01:02:09.293 --> 01:02:12.550
Finally, I'm being realistic,
it's not only gonna be

01:02:12.550 --> 01:02:13.560
for friendly purposes.

01:02:13.560 --> 01:02:15.924
So my point of this double
negative here, whatever it is,

01:02:15.924 --> 01:02:19.400
is that sure the military's
gonna try to do things

01:02:19.400 --> 01:02:20.840
with intelligent machines.

01:02:20.840 --> 01:02:22.590
That happens, they do the
same thing with you know,

01:02:22.590 --> 01:02:24.350
cell phones, and radios, and so on.

01:02:24.350 --> 01:02:26.840
So, it's not always gonna be
for friendly, benign things.

01:02:26.840 --> 01:02:28.670
But I don't think it's a
threat to humanity in any

01:02:28.670 --> 01:02:29.503
central way.

01:02:29.503 --> 01:02:32.004
So, I'm not really worried
about this, these bad things.

01:02:32.004 --> 01:02:35.280
I don't think they're gonna
happen, and I think history

01:02:35.280 --> 01:02:36.680
is on the side of that.

01:02:36.680 --> 01:02:38.440
So we'll just get rid
of those guys like that.

01:02:38.440 --> 01:02:41.610
Okay, now I'm going to finally
talk about, this is my last

01:02:41.610 --> 01:02:43.700
slide, and we'll talk
why should we do this?

01:02:43.700 --> 01:02:45.429
Why, you know, why should you care?

01:02:45.429 --> 01:02:47.410
I actually think it's essential.

01:02:47.410 --> 01:02:50.348
I think it's essential for
the survival of our species,

01:02:50.348 --> 01:02:53.760
and I think it's essential
for our mission as a species

01:02:53.760 --> 01:02:54.890
if you will.

01:02:54.890 --> 01:02:56.140
The purpose of life, and so on.

01:02:56.140 --> 01:02:59.380
So, the number one thing
here is that we can make

01:02:59.380 --> 01:03:00.213
our world better.

01:03:00.213 --> 01:03:03.560
Every new technology can
make our world better.

01:03:03.560 --> 01:03:07.910
And I think having machines
that can help us be more

01:03:07.910 --> 01:03:11.210
efficient and make the world
safer, and produce better

01:03:11.210 --> 01:03:14.130
communications, and essentially
accentuate our lives,

01:03:14.130 --> 01:03:15.720
is essentially the same
way computers have made

01:03:15.720 --> 01:03:16.800
our lives better.

01:03:16.800 --> 01:03:20.320
And you know, computers are
a big plus over how things

01:03:20.320 --> 01:03:21.230
were before.

01:03:21.230 --> 01:03:23.750
And, there are some downsides,
but mostly it's a big

01:03:23.750 --> 01:03:26.798
platform, glad we have them
to do things we need them

01:03:26.798 --> 01:03:27.631
to do when they're there.

01:03:27.631 --> 01:03:29.010
And the same thing will be happening here.

01:03:29.010 --> 01:03:31.810
We'll become very reliant
upon artificial machines,

01:03:31.810 --> 01:03:34.490
not again robots, not
you know, evil things.

01:03:34.490 --> 01:03:37.020
Just machines that work on
the principles on your cortex

01:03:37.020 --> 01:03:39.417
that are helping us figure
out how to run the world

01:03:39.417 --> 01:03:40.290
and so on.

01:03:40.290 --> 01:03:42.642
But more importantly, if you
think what is the, what is

01:03:42.642 --> 01:03:44.730
the purpose in life?

01:03:44.730 --> 01:03:48.350
I don't know, but maybe
there is no purpose in life.

01:03:48.350 --> 01:03:49.662
But there is something I know.

01:03:49.662 --> 01:03:54.662
I know that we are as a
species have been discovering

01:03:55.134 --> 01:03:56.654
what the universe is.

01:03:56.654 --> 01:03:59.330
Our path has been to
discover more and more

01:03:59.330 --> 01:04:00.163
about the universe.

01:04:00.163 --> 01:04:03.360
And perhaps as we discover more
and more about the universe,

01:04:03.360 --> 01:04:08.180
we will discover why or how
the origins, we might come

01:04:08.180 --> 01:04:09.070
to closure about that.

01:04:09.070 --> 01:04:10.160
We might not.

01:04:10.160 --> 01:04:10.993
We don't know.

01:04:10.993 --> 01:04:13.260
But I don't know anything
else that's worth doing.

01:04:13.260 --> 01:04:15.700
And I think that's what
motivates many people of science,

01:04:15.700 --> 01:04:18.160
and the thing is to
everyone at some extent,

01:04:18.160 --> 01:04:18.993
we want to know.

01:04:18.993 --> 01:04:20.900
We want to know more about things.

01:04:20.900 --> 01:04:24.420
And today, our brains are
how we figure out more.

01:04:24.420 --> 01:04:26.250
And we have scientists who do this.

01:04:26.250 --> 01:04:27.083
This is what they do.

01:04:27.083 --> 01:04:28.840
They look at patterns in the world.

01:04:28.840 --> 01:04:31.470
They say can they discover
the structure and they look

01:04:31.470 --> 01:04:33.610
at data in the world, they said
can I just see the structure

01:04:33.610 --> 01:04:34.930
and patterns of that data.

01:04:34.930 --> 01:04:37.025
Let me test my model, and
build a model of that,

01:04:37.025 --> 01:04:39.110
and test is done, and I can
assume, and we build models

01:04:39.110 --> 01:04:40.880
on top of models.

01:04:40.880 --> 01:04:42.037
Well, that's what brains do, right?

01:04:42.037 --> 01:04:43.810
And that's what science is.

01:04:43.810 --> 01:04:47.113
And by having very, very
intelligent machines in ways

01:04:47.113 --> 01:04:51.040
that I can't even imagine
yet, I am sure that we will

01:04:51.040 --> 01:04:54.610
be able to discover
more about the universe.

01:04:54.610 --> 01:04:57.820
I doubt that humans are ever
going to go explore into space

01:04:57.820 --> 01:05:00.180
and spend years and years
traveling around the universe.

01:05:00.180 --> 01:05:02.020
Maybe it'll happen, but I doubt it.

01:05:02.020 --> 01:05:04.040
But can we send intelligent
machines in our place

01:05:04.040 --> 01:05:05.680
to go out and discover the
world, come back and tell us

01:05:05.680 --> 01:05:07.530
about it, yeah, we can do that.

01:05:07.530 --> 01:05:12.140
So, can we have those thousands
of physicists brains working

01:05:12.140 --> 01:05:12.973
around the clock?

01:05:12.973 --> 01:05:13.930
Yeah, we can do that.

01:05:13.930 --> 01:05:16.397
So, I think this is going to
be a way of really accelerating

01:05:16.397 --> 01:05:20.410
the accretion or the assimilation
of knowledge and data

01:05:20.410 --> 01:05:22.950
in the world that's
gonna be unprecedented.

01:05:22.950 --> 01:05:26.010
The human brain has been
unprecedented in the long history

01:05:26.010 --> 01:05:27.560
of biology and our ability to do this.

01:05:27.560 --> 01:05:29.727
But I think we can
celebrate this dramatic.

01:05:29.727 --> 01:05:32.030
So I find this all very exciting.

01:05:32.030 --> 01:05:34.449
I want, you know, this to
me, this is the future.

01:05:34.449 --> 01:05:35.282
This is so cool.

01:05:35.282 --> 01:05:36.780
I won't live to see most of it.

01:05:36.780 --> 01:05:39.230
But I think it's worth working on.

01:05:39.230 --> 01:05:41.340
And I think it's very
exciting for the future,

01:05:41.340 --> 01:05:44.070
and that's why I come and
do this, and come to speak

01:05:44.070 --> 01:05:45.200
to people like you.

01:05:45.200 --> 01:05:46.640
So that's it.

01:05:46.640 --> 01:05:47.473
Thank you very much.

01:05:47.473 --> 01:05:50.473
(audience clapping)

01:05:57.370 --> 01:05:58.930
Let me repeat the question
because not everyone else

01:05:58.930 --> 01:05:59.980
heard it.

01:05:59.980 --> 01:06:02.870
So, the gentleman pointed out
that in the model I presented

01:06:02.870 --> 01:06:05.070
there was no sensory motor
integration going on.

01:06:05.070 --> 01:06:07.621
There was no action in the model itself.

01:06:07.621 --> 01:06:10.460
And remember all the
attributes that I talked about,

01:06:10.460 --> 01:06:12.060
and the sensory motor
integration was one of them,

01:06:12.060 --> 01:06:14.970
I didn't put a green dot next to that one.

01:06:14.970 --> 01:06:17.730
So, that's an area where we're
working on, or I'm working

01:06:17.730 --> 01:06:20.570
on right now which is very,
very interesting and enticing.

01:06:20.570 --> 01:06:24.110
And, to understand how that same model--

01:06:24.110 --> 01:06:25.710
So what we know in the brain,
let me tell you what we know

01:06:25.710 --> 01:06:26.700
about in the brain.

01:06:26.700 --> 01:06:28.700
Everywhere you look in the
brain, and in the cortex,

01:06:28.700 --> 01:06:30.677
you look anywhere you'll
see those layers of cells

01:06:30.677 --> 01:06:33.180
sort of forming sort of sequence memory.

01:06:33.180 --> 01:06:36.500
Everywhere you look, there
are cells that project

01:06:36.500 --> 01:06:39.357
some motor sensor of the brain,
a motor section of the body.

01:06:39.357 --> 01:06:42.360
So, everywhere you go
there's a motor output.

01:06:42.360 --> 01:06:44.960
So we know that this is,
they're tied together.

01:06:44.960 --> 01:06:48.020
And, we have a lot of
interesting clues, a tremendous

01:06:48.020 --> 01:06:50.620
amount of clues, in the
neuroscience literature,

01:06:50.620 --> 01:06:52.013
about say how do I understand it?

01:06:52.013 --> 01:06:53.280
What is going on there?

01:06:53.280 --> 01:06:57.310
What is the theoretical principle
underlying how an action

01:06:57.310 --> 01:06:58.937
affects the input, and so on.

01:06:58.937 --> 01:07:01.424
I don't have the answer to that.

01:07:01.424 --> 01:07:04.450
But, we're, I'm hot on
the trail, I'm excited

01:07:04.450 --> 01:07:06.950
about the progress I made on
it, and maybe in a year or two

01:07:06.950 --> 01:07:08.400
I'll come back and tell you about it.

01:07:08.400 --> 01:07:10.880
In the meantime, the question
that we had at Numenta

01:07:10.880 --> 01:07:13.450
was can we build something
useful without it?

01:07:13.450 --> 01:07:15.460
'Cause like if I can't build
something useful from that,

01:07:15.460 --> 01:07:17.210
Numenta would still be a research company.

01:07:17.210 --> 01:07:18.250
And we'd be working on it.

01:07:18.250 --> 01:07:19.913
But it turns out we can.

01:07:21.180 --> 01:07:23.860
It turns out the problem they
talked about there are simple

01:07:23.860 --> 01:07:26.240
enough that we can do them
without the motor sensory loop

01:07:26.240 --> 01:07:27.630
in some sense.

01:07:27.630 --> 01:07:31.080
And so, I wouldn't, I
had those six attributes,

01:07:31.080 --> 01:07:33.630
and I had only three of
them actually in that model.

01:07:33.630 --> 01:07:35.070
So, there's a lot of things missing.

01:07:35.070 --> 01:07:36.940
But the question is can we get started?

01:07:36.940 --> 01:07:39.060
So, good observation, it's not there.

01:07:39.060 --> 01:07:42.711
But I, it looks like we can
go forward in getting started

01:07:42.711 --> 01:07:44.793
and we have a lot more to do.

01:07:46.130 --> 01:07:49.210
- [Male] I notice you had,
what was it, 2K cell, or 2K

01:07:49.210 --> 01:07:52.530
column, 6K Cells, that's
like 30 cells a column,

01:07:52.530 --> 01:07:54.680
I know the brain has like
seven in the cortex--

01:07:54.680 --> 01:07:55.513
- Are you saying does
that match up to what

01:07:55.513 --> 01:07:58.390
the biology says?

01:07:58.390 --> 01:07:59.750
- [Male] Yeah.

01:07:59.750 --> 01:08:01.170
- Alright so, it's a good
question, although your facts

01:08:01.170 --> 01:08:02.503
are a little bit wrong there.

01:08:02.503 --> 01:08:06.930
The neocortex, everyone
gets confused by this.

01:08:06.930 --> 01:08:11.930
The neocortex has essentially
five layers of cells, okay?

01:08:12.500 --> 01:08:14.360
Now, I say five layers
of cells, that doesn't

01:08:14.360 --> 01:08:16.001
mean five cells.

01:08:16.001 --> 01:08:19.920
That means five dense layers,
really dense cellular material

01:08:19.920 --> 01:08:22.910
and the columns span
across all those layers.

01:08:22.910 --> 01:08:24.910
People say six layers, but
one layer is not cellular,

01:08:24.910 --> 01:08:26.670
it's acellular, doesn't have cells.

01:08:26.670 --> 01:08:28.980
So we have five of the cells
and the columns go across

01:08:28.980 --> 01:08:31.690
all of them, and I was allowing
just one of those layers.

01:08:31.690 --> 01:08:33.870
Now if I look at the entire
column across the whole

01:08:33.870 --> 01:08:36.720
neocortex, in a real neocortex,
there's about, depending

01:08:36.720 --> 01:08:40.267
where you look, but many
places is about 100, 110 cells

01:08:40.267 --> 01:08:41.913
in a mini-column.

01:08:43.141 --> 01:08:43.974
We're not talking about
the ice cube column,

01:08:43.974 --> 01:08:45.177
but a mini column in the neocortex.

01:08:45.177 --> 01:08:48.953
And so if I were to say I'm
modeling one layer of cells,

01:08:48.953 --> 01:08:52.240
I could layer three, which is
one of the prominent layers

01:08:52.240 --> 01:08:54.701
in the neocortex, and I
say there are 30 cells

01:08:54.701 --> 01:08:57.070
in a microcolumn, that's
a realistic number.

01:08:57.070 --> 01:09:00.170
That's right in the ballpark
of out of the 110, maybe 30

01:09:00.170 --> 01:09:01.240
might be allocated there.

01:09:01.240 --> 01:09:02.850
The model is not particular about this.

01:09:02.850 --> 01:09:04.710
I could have 20 cells,
I could have 10 cells,

01:09:04.710 --> 01:09:05.730
I could have 50 cells.

01:09:05.730 --> 01:09:08.100
They all works, it's just brain capacity.

01:09:08.100 --> 01:09:11.020
So, and then, and so
if I have 2000 columns,

01:09:11.020 --> 01:09:13.380
this is very, very small
part of the neocortex.

01:09:13.380 --> 01:09:16.640
2000 micro columns, these
columns are only 30 microns wide.

01:09:16.640 --> 01:09:19.147
So, to put 2000 of them,
little tiny of those things

01:09:19.147 --> 01:09:21.940
in the cortex, one layer,
we're just modeling the tiniest

01:09:21.940 --> 01:09:22.878
little part of it.

01:09:22.878 --> 01:09:25.040
But, those numbers are realistic.

01:09:25.040 --> 01:09:26.940
I didn't go through, yesterday
I had a slide where I talked

01:09:26.940 --> 01:09:30.460
about the other numbers where
there's about 128 dendric

01:09:30.460 --> 01:09:32.960
segments per cell, right
in the ballpark of reality.

01:09:32.960 --> 01:09:35.510
Each cell is something
around 5000 synapses,

01:09:35.510 --> 01:09:37.120
right in the ballpark of reality.

01:09:37.120 --> 01:09:38.922
In fact, if you know
anything about neuroscience,

01:09:38.922 --> 01:09:42.240
a typical cell in the
neocortex is several thousand

01:09:42.240 --> 01:09:44.635
synapses, there are no
other theories that go

01:09:44.635 --> 01:09:45.468
what are all those synapses doing.

01:09:45.468 --> 01:09:47.730
There is a very concrete
theory explains all that.

01:09:47.730 --> 01:09:50.861
Anyway, so my point is, if
you, if you dive into it

01:09:50.861 --> 01:09:52.568
a little bit, the numbers
match up really well actually.

01:09:52.568 --> 01:09:53.480
- [Male] Alright, thanks.

01:09:53.480 --> 01:09:55.630
- Well that was a good enough
question, wanna ask one more?

01:09:55.630 --> 01:09:56.463
- [Male] Yeah, please.

01:09:56.463 --> 01:09:57.370
- Okay, one more.

01:09:57.370 --> 01:09:59.110
- [Male] I remember yesterday, oh sorry.

01:09:59.110 --> 01:10:01.160
I remember yesterday you were
talking a little bit about

01:10:01.160 --> 01:10:06.050
how, what's it called,
robust it is to degradation

01:10:06.050 --> 01:10:08.170
and stuff, so I was just
wondering, it made me think

01:10:08.170 --> 01:10:10.630
of if I have multiple Grok
instances, if you will,

01:10:10.630 --> 01:10:14.936
existing in the same environment,
and you were to like,

01:10:14.936 --> 01:10:19.110
if they could I guess make
predictions for the same problem,

01:10:19.110 --> 01:10:22.048
or be connected in some
way, would certain instances

01:10:22.048 --> 01:10:26.250
take over certain functions
and others sort of distribute

01:10:26.250 --> 01:10:27.790
their functionality?

01:10:27.790 --> 01:10:29.360
- Alright, that's a tricky
question, and it's probably

01:10:29.360 --> 01:10:31.862
deeper than most of the
audience cares about.

01:10:31.862 --> 01:10:34.657
But, I'll get into it very quickly.

01:10:34.657 --> 01:10:38.270
We actually do run multiple instances.

01:10:38.270 --> 01:10:40.190
Grok is the name of the whole
thing, but we can actually

01:10:40.190 --> 01:10:41.190
run multiple instances at the same time

01:10:41.190 --> 01:10:42.580
using these algorithms.

01:10:42.580 --> 01:10:45.730
And the reason we do that is
because there are some parts

01:10:45.730 --> 01:10:47.980
of the system which
are not really learned.

01:10:47.980 --> 01:10:51.650
Just like in your brain, like
your retina is genetically

01:10:51.650 --> 01:10:54.900
determined, and how it works
is genetically determined.

01:10:54.900 --> 01:10:57.020
And a different retina,
other animals have different

01:10:57.020 --> 01:10:58.910
retinas, they may not be as good.

01:10:58.910 --> 01:11:01.240
So a dog doesn't see, or
a cat doesn't see as good

01:11:01.240 --> 01:11:03.440
as you do in many situations.

01:11:03.440 --> 01:11:04.620
It has a different retina.

01:11:04.620 --> 01:11:07.380
So there's some, there's
things like that in our system,

01:11:07.380 --> 01:11:09.620
like learning rates and
how we encode the data,

01:11:09.620 --> 01:11:12.970
which are sort of more like
biological evolutionary thing.

01:11:12.970 --> 01:11:15.010
And so, we don't really know
the best way of encoding

01:11:15.010 --> 01:11:16.960
the data, we don't really
know the best learning rates

01:11:16.960 --> 01:11:17.793
for the system.

01:11:17.793 --> 01:11:19.510
So we actually run multiples
of these models at the same

01:11:19.510 --> 01:11:24.010
time and they end up working
to some extent, but some

01:11:24.010 --> 01:11:25.120
are better than others.

01:11:25.120 --> 01:11:28.160
And we don't, you can't view
them as, not really what you

01:11:28.160 --> 01:11:31.190
said like they're learning
different things, but they kind

01:11:31.190 --> 01:11:32.400
of view the world slightly different.

01:11:32.400 --> 01:11:34.050
This is like my wife and
I view the world slightly

01:11:34.050 --> 01:11:35.537
differently, she colors better than I do.

01:11:35.537 --> 01:11:38.220
I don't know what the hell
she's talking about sometimes.

01:11:38.220 --> 01:11:40.100
But she's all look at
that color, I'm like what?

01:11:40.100 --> 01:11:40.933
And she's an artist.

01:11:40.933 --> 01:11:42.290
And she's sensitive to those things.

01:11:42.290 --> 01:11:45.291
So, different versions of
Grok are like that too.

01:11:45.291 --> 01:11:48.986
So we're constantly running
a compilation of these guys

01:11:48.986 --> 01:11:51.190
and then some are better than others.

01:11:51.190 --> 01:11:52.267
We kill the ones that don't do very well.

01:11:52.267 --> 01:11:53.330
And we produce new ones.

01:11:53.330 --> 01:11:54.640
So, those were both good questions.

01:11:54.640 --> 01:11:55.980
I want to get someone
else to ask the question.

01:11:55.980 --> 01:11:56.813
- [Male] Thank you.

01:11:56.813 --> 01:11:57.646
- Sure.

01:11:58.810 --> 01:12:02.323
- [Man] Hi, how does the
performance of your model

01:12:02.323 --> 01:12:04.920
compare to other machine
learning algorithms?

01:12:04.920 --> 01:12:06.430
- So the question is how does
the performance of this model

01:12:06.430 --> 01:12:07.700
compare to other machine
learning algorithms.

01:12:07.700 --> 01:12:10.980
So, the machine learning
field is very, very large.

01:12:10.980 --> 01:12:13.850
And, let me just give you,
I'll answer that question

01:12:13.850 --> 01:12:14.750
in a couple ways.

01:12:14.750 --> 01:12:17.580
First of all, you have to ask
yourself what other machine

01:12:17.580 --> 01:12:20.130
learning models are really
good at time-based data?

01:12:21.290 --> 01:12:23.818
Very, very few.

01:12:23.818 --> 01:12:26.343
I can name them on like, two fingers.

01:12:27.650 --> 01:12:28.810
Then, the next question.

01:12:28.810 --> 01:12:31.590
How many of them are
machine continuous learning?

01:12:31.590 --> 01:12:32.423
Online learning?

01:12:32.423 --> 01:12:33.820
Not batch learning.

01:12:33.820 --> 01:12:35.490
But, continuous learning.

01:12:35.490 --> 01:12:36.723
Very, very few.

01:12:37.990 --> 01:12:42.990
And so you can, what we
claim, what we're doing here

01:12:43.110 --> 01:12:45.920
is we're solving problems
that other machine learning

01:12:45.920 --> 01:12:47.670
problems don't solve.

01:12:47.670 --> 01:12:49.890
It's not like oh, I can compare
the performance of yours

01:12:49.890 --> 01:12:50.783
with the compare of these.

01:12:50.783 --> 01:12:52.220
It's usually not possible.

01:12:52.220 --> 01:12:55.610
Now however, I'll say, that
when we go into a customer,

01:12:55.610 --> 01:12:57.580
they typically have some solution.

01:12:57.580 --> 01:13:01.060
It may be sophisticated, it
may be crude, but they have

01:13:01.060 --> 01:13:03.480
something and they're
not very happy with it.

01:13:03.480 --> 01:13:06.160
And so, instead of going in
claiming we don't go in claiming

01:13:06.160 --> 01:13:07.710
oh, our algorithm is
better than all the rest.

01:13:07.710 --> 01:13:09.920
We say no, just give us your
problem, and see if we can

01:13:09.920 --> 01:13:10.940
improve upon it.

01:13:10.940 --> 01:13:12.960
'Cause usually there aren't
very good solutions for these

01:13:12.960 --> 01:13:14.855
type of problems, there
just aren't good solutions.

01:13:14.855 --> 01:13:18.070
And so, we come in and we do
much, if we can do much better

01:13:18.070 --> 01:13:19.610
they're very, very happy.

01:13:19.610 --> 01:13:22.230
And so that's really the claim here.

01:13:22.230 --> 01:13:24.760
It's not like, you don't want
to get there in some sort of,

01:13:24.760 --> 01:13:26.460
we're do better at this,
and you do better at this

01:13:26.460 --> 01:13:27.884
by two percent or something like that.

01:13:27.884 --> 01:13:29.020
Nah, it's not about that.

01:13:29.020 --> 01:13:31.800
It's about streaming data,
it's about continuous learning,

01:13:31.800 --> 01:13:35.713
it's about making predictions,
you know, and automated,

01:13:35.713 --> 01:13:38.770
and automated model creation.

01:13:38.770 --> 01:13:41.566
And there just aren't
other solutions like that.

01:13:41.566 --> 01:13:42.399
- [Man] Thanks.

01:13:42.399 --> 01:13:43.232
- Yeah.

01:13:44.960 --> 01:13:46.923
- [Male] Hi, I have three questions.

01:13:47.833 --> 01:13:48.991
- How many?

01:13:48.991 --> 01:13:49.824
- [Male] What was that?

01:13:49.824 --> 01:13:50.657
- How many questions?

01:13:50.657 --> 01:13:51.490
- [Male] Three.

01:13:51.490 --> 01:13:52.323
- Three!

01:13:52.323 --> 01:13:53.233
Oh you gotta go for more
than the last guy, okay.

01:13:54.590 --> 01:13:56.260
- [Male] The first question I was asking,

01:13:56.260 --> 01:13:58.370
what's the equivalent of RAM in the brain?

01:13:58.370 --> 01:13:59.607
- What's the equivalent of RAM, like RAM--

01:13:59.607 --> 01:14:01.670
- Random access memory.
- Random access memory?

01:14:01.670 --> 01:14:03.310
- [Male] Yeah.

01:14:03.310 --> 01:14:05.072
- Well, did I say there
was an equivalent amount

01:14:05.072 --> 01:14:06.739
of RAM in the brain?

01:14:08.370 --> 01:14:11.050
Alright, I don't even know
how to answer that question.

01:14:11.050 --> 01:14:13.537
Alright, so, the memory we use, I'll try.

01:14:13.537 --> 01:14:18.420
The memory we use in the computer
of RAM is one type, okay.

01:14:18.420 --> 01:14:20.540
Random access means that
there's a big list of things

01:14:20.540 --> 01:14:22.790
you put in an address you
get the results back, right?

01:14:22.790 --> 01:14:23.930
That's a type of memory.

01:14:23.930 --> 01:14:26.020
It is structurally,
completely different than type

01:14:26.020 --> 01:14:26.870
of memory that I just talked about.

01:14:26.870 --> 01:14:29.080
There is nothing equivalent
about it what so ever.

01:14:29.080 --> 01:14:32.470
This is a hierarchical
temporal memory system, and RAM

01:14:32.470 --> 01:14:36.970
is a linear, flat random access memory.

01:14:36.970 --> 01:14:39.443
So there's no equivalent
to that in the brain.

01:14:40.606 --> 01:14:41.730
Now, I can answer this slightly different.

01:14:41.730 --> 01:14:44.638
If you thought of RAM in a
computer, typically what's in

01:14:44.638 --> 01:14:46.317
the RAM in the computer
is a temporary memory

01:14:46.317 --> 01:14:48.330
and it's a state of the system.

01:14:48.330 --> 01:14:50.410
Alright, like I got the hard
drive is where I'm storing

01:14:50.410 --> 01:14:52.610
data, and maybe the RAM is the
current state of the system.

01:14:52.610 --> 01:14:54.150
So if the RAM dies, then
you lose all of the data

01:14:54.150 --> 01:14:55.140
in the system.

01:14:55.140 --> 01:14:56.868
If that's the question you
were asking, and it looks

01:14:56.868 --> 01:14:58.355
from your face that wasn't
the question you're answering,

01:14:58.355 --> 01:14:59.600
but I'm gonna answer it anyway--

01:14:59.600 --> 01:15:00.530
- [Male] I'm still
learning about the, yeah.

01:15:00.530 --> 01:15:02.210
I mean, I'm still learning
about this field, so--

01:15:02.210 --> 01:15:04.700
- Okay well, let's put it this way.

01:15:04.700 --> 01:15:08.030
When our models are running
an instantaneous state,

01:15:08.030 --> 01:15:10.610
that is like the current
activation, and that's kinda

01:15:10.610 --> 01:15:12.840
like what's kept in a
RAM in a computer memory.

01:15:12.840 --> 01:15:13.700
But anyway, I'm gonna do the answer.

01:15:13.700 --> 01:15:17.020
- [Male] But you're saying
there's no real equivalent?

01:15:17.020 --> 01:15:18.070
- There's no real equivalent.

01:15:18.070 --> 01:15:19.300
I could have just answered it that way.

01:15:19.300 --> 01:15:20.626
No, there's no equivalent.

01:15:20.626 --> 01:15:23.940
- [Male] My second question
is yeah, I also, is computing

01:15:23.940 --> 01:15:27.500
model your company's developing,
is that the only computing

01:15:27.500 --> 01:15:30.620
model that's, I mean, I'm
assuming you're really cutting

01:15:30.620 --> 01:15:31.969
edge, so--

01:15:31.969 --> 01:15:36.040
Well, what I was wondering,
what models are your competitors

01:15:36.040 --> 01:15:38.502
using, I mean, like I said
I'm also kind of studying

01:15:38.502 --> 01:15:39.366
the companies--

01:15:39.366 --> 01:15:41.370
- Alright so again, I
don't want to get too much

01:15:41.370 --> 01:15:42.920
on the business side of
things, because that's not what

01:15:42.920 --> 01:15:46.650
this talk is about, but, we
you know, everyone likes to say

01:15:46.650 --> 01:15:48.700
this, I don't know if we
have real competitors.

01:15:48.700 --> 01:15:51.649
Again, you go in the attributes
of what we're offering

01:15:51.649 --> 01:15:54.749
are not available in other systems.

01:15:54.749 --> 01:15:57.700
Write this down, you got a
piece of paper, you know,

01:15:57.700 --> 01:16:01.360
continuous learning, temporal
data, automatic model creation

01:16:01.360 --> 01:16:06.360
okay, and go out and find
companies that do that, okay.

01:16:06.890 --> 01:16:08.410
So, this is a real good question.

01:16:08.410 --> 01:16:10.150
A little techy, I didn't
talk about it in the talk,

01:16:10.150 --> 01:16:11.020
but I'll go through it.

01:16:11.020 --> 01:16:14.320
So the question is, I said
magically here's these numbers

01:16:14.320 --> 01:16:16.366
and fields and so on, and
I turn them into Sparse

01:16:16.366 --> 01:16:17.670
Distributed Representations, right?

01:16:17.670 --> 01:16:18.670
How do you do that?

01:16:18.670 --> 01:16:22.690
Let me give you the one
example that's the simplest one

01:16:22.690 --> 01:16:26.940
to understand, and then
you can ponder about it.

01:16:26.940 --> 01:16:29.270
So, imagine I have a number.

01:16:29.270 --> 01:16:31.460
Energy, price, temperature, right?

01:16:31.460 --> 01:16:33.600
I got a number, I got
a scale of 45 numbers.

01:16:33.600 --> 01:16:35.110
It's on a number line.

01:16:35.110 --> 01:16:36.610
And I want to turn that
into a Sparse Distributed

01:16:36.610 --> 01:16:37.443
Representation.

01:16:37.443 --> 01:16:39.510
So imagine I have this number
line in front of me now,

01:16:39.510 --> 01:16:43.120
and imagine now I define
you a whole bunch of bits,

01:16:43.120 --> 01:16:43.953
200 bits.

01:16:43.953 --> 01:16:46.207
And the first bit represents zero to 10.

01:16:46.207 --> 01:16:48.367
And the next bit represents one to 11.

01:16:48.367 --> 01:16:50.820
And the next bit represents two to 12.

01:16:50.820 --> 01:16:51.653
And so on.

01:16:51.653 --> 01:16:54.280
And now, when I have a
number on the number line,

01:16:54.280 --> 01:16:58.100
say it's 22, I can go see
which of those bits overlap

01:16:58.100 --> 01:16:59.540
with that number.

01:16:59.540 --> 01:17:01.227
And, those are the bits gonna be one.

01:17:01.227 --> 01:17:02.810
Now this idea came from the cochlear.

01:17:02.810 --> 01:17:04.440
So we didn't make this up completely,

01:17:04.440 --> 01:17:06.240
this is kind of a little
bit how the cochlear works

01:17:06.240 --> 01:17:07.240
in your ear.

01:17:07.240 --> 01:17:11.023
And so, if I give you some
number, there will be some bits

01:17:11.023 --> 01:17:15.810
that come on, because their
bands overlap with that number.

01:17:15.810 --> 01:17:18.100
And if the number moves
upward a little bit, then one

01:17:18.100 --> 01:17:20.147
of those bits will turn off
and another bit will turn on.

01:17:20.147 --> 01:17:21.210
Are you following this?

01:17:21.210 --> 01:17:22.800
Can you visualize this?

01:17:22.800 --> 01:17:25.840
So, so we have a Sparse
Distributed Representation,

01:17:25.840 --> 01:17:27.220
each bit has semantic meaning.

01:17:27.220 --> 01:17:29.530
I have a number of bits on, it's sparse.

01:17:29.530 --> 01:17:34.294
And, similar values have
similar representations.

01:17:34.294 --> 01:17:36.474
And, that's how we do it.

01:17:36.474 --> 01:17:38.880
And it works pretty well.

01:17:38.880 --> 01:17:40.885
- [Male] When it comes
to semantic categories?

01:17:40.885 --> 01:17:44.420
- Ah, so when it's semantic
categories works, you have to

01:17:44.420 --> 01:17:48.740
understand that everyone in
the system we're constantly,

01:17:48.740 --> 01:17:50.580
it's learning semantic meanings, right?

01:17:50.580 --> 01:17:52.367
You don't, you start the
way it works in the brain,

01:17:52.367 --> 01:17:54.800
and the way it works in our
systems, you start with very

01:17:54.800 --> 01:17:56.830
low level semantic meanings
like a little bands

01:17:56.830 --> 01:17:59.620
of frequency, little bands
of numbers, or little patches

01:17:59.620 --> 01:18:01.680
of visual minds in individual
fields or little things

01:18:01.680 --> 01:18:02.513
like this.

01:18:02.513 --> 01:18:05.390
You have to build the more
sophisticated representations

01:18:05.390 --> 01:18:06.230
as you go.

01:18:06.230 --> 01:18:08.020
So the very first thing
that happens in our system,

01:18:08.020 --> 01:18:10.560
is I take all these inputs
from a bunch of fields,

01:18:10.560 --> 01:18:13.790
these sparse representations,
and I run through something

01:18:13.790 --> 01:18:14.690
I didn't talk about today.

01:18:14.690 --> 01:18:16.823
But basically I have to form
a new Sparse Representation

01:18:16.823 --> 01:18:18.090
with 2000 bits.

01:18:18.090 --> 01:18:19.580
My input may not be 2000 bits.

01:18:19.580 --> 01:18:22.340
My input may be more or less,
and one of two distinguished

01:18:22.340 --> 01:18:24.520
ones you have to representations
based on the common

01:18:24.520 --> 01:18:25.650
spacial patterns in the world.

01:18:25.650 --> 01:18:28.280
So basics says, if I see
coincidences that occur,

01:18:28.280 --> 01:18:30.660
spacial coincidences that
form representations of them,

01:18:30.660 --> 01:18:33.397
and then I have sequences
of those, I form--

01:18:34.650 --> 01:18:36.670
It's a pretty technical
thing, so I'll just leave it

01:18:36.670 --> 01:18:38.190
at that, okay?

01:18:38.190 --> 01:18:40.522
So the gentleman's asking
me he say oh, there's other

01:18:40.522 --> 01:18:44.246
algorithms like deep belief
networks that you might compare

01:18:44.246 --> 01:18:47.330
to this, by the way, we
call this cortical learning

01:18:47.330 --> 01:18:50.020
algorithm, CLA, so you
can compare it to the CLA.

01:18:50.020 --> 01:18:55.020
And, there's some, they produce
predictions about biology,

01:18:56.100 --> 01:18:57.990
about anatomy perhaps, is
that what you're saying,

01:18:57.990 --> 01:18:59.920
and then you have the
same type of thing here.

01:18:59.920 --> 01:19:01.240
Did that cover your question?

01:19:01.240 --> 01:19:04.741
Okay, first of all, I'll
disagree with something you said.

01:19:04.741 --> 01:19:08.870
Deep belief networks are not
nearly as biological grounded

01:19:08.870 --> 01:19:10.080
as what I presented here.

01:19:10.080 --> 01:19:10.920
Not even close.

01:19:10.920 --> 01:19:13.230
The order of the mind stood off.

01:19:13.230 --> 01:19:15.253
But they are hierarchical in
some sense, and they can deal

01:19:15.253 --> 01:19:17.610
with time in some sense,
and in that way I like them.

01:19:17.610 --> 01:19:19.560
They're in a good direction.

01:19:19.560 --> 01:19:21.250
But they're very, very quite different.

01:19:21.250 --> 01:19:23.289
And they don't do what I shared here.

01:19:23.289 --> 01:19:24.850
It's quite far apart.

01:19:24.850 --> 01:19:26.740
So they're not really, easily comparable.

01:19:26.740 --> 01:19:28.370
None of the customers
we're working with say hey,

01:19:28.370 --> 01:19:30.810
I can do this with deep
belief network, they can't.

01:19:30.810 --> 01:19:33.790
Now the second part of
your question is whether

01:19:33.790 --> 01:19:35.770
the biological predictions
are coming out of this.

01:19:35.770 --> 01:19:38.810
Or can I show behaviors
that we see in biology.

01:19:38.810 --> 01:19:40.310
And the answer is absolutely, yes.

01:19:40.310 --> 01:19:42.710
I think presenting that,
we did this original work

01:19:42.710 --> 01:19:46.803
in with division of
experiments and we're realizing

01:19:48.264 --> 01:19:49.918
a lot of physiological
properties that you see,

01:19:49.918 --> 01:19:51.740
in division system.

01:19:51.740 --> 01:19:53.910
The answer is absolutely yes.

01:19:53.910 --> 01:19:56.470
I wasn't trying to do that
here, I wasn't showing you that

01:19:56.470 --> 01:19:58.680
here, but it's actually very good at that.

01:19:58.680 --> 01:20:00.650
So I'll leave it at that.

01:20:00.650 --> 01:20:02.010
- [Male] What about the entire body?

01:20:02.010 --> 01:20:02.843
I'm just wondering about emotions.

01:20:02.843 --> 01:20:04.500
Like, if there's like
things that are happening

01:20:04.500 --> 01:20:07.210
in the body, could you put
sensors in there and use

01:20:07.210 --> 01:20:09.050
that information, or do you do that, or?

01:20:09.050 --> 01:20:12.210
- Okay, so that's kind
of a confusing question.

01:20:12.210 --> 01:20:13.050
But I'll try to part--

01:20:13.050 --> 01:20:15.425
- [Male] Well, I'm happy
to try to clarify that--

01:20:15.425 --> 01:20:16.330
- No, let me try to get it.

01:20:16.330 --> 01:20:18.260
Let me address the first
few things you said there.

01:20:18.260 --> 01:20:19.780
Like, what about a body?

01:20:19.780 --> 01:20:20.850
Can you have sensors in a body?

01:20:20.850 --> 01:20:22.387
Well, by the way, you do
have sensors in your body.

01:20:22.387 --> 01:20:25.858
The whole sensory system
called the poke-sensory system

01:20:25.858 --> 01:20:28.480
which measures the joint
angles and things like this,

01:20:28.480 --> 01:20:29.313
and we'll all fit it,
it's a whole 'nother thing

01:20:29.313 --> 01:20:31.260
a lot of people don't know about it.

01:20:31.260 --> 01:20:33.390
But, that's how you
model where your body is.

01:20:33.390 --> 01:20:35.293
'Cause there's all these
sensors that tell you this.

01:20:35.293 --> 01:20:39.100
And, we don't have bodies in our system.

01:20:39.100 --> 01:20:41.700
Now we're doing you know,
inference on data streams.

01:20:41.700 --> 01:20:46.700
And the day that I have a
sensory motor integration loops,

01:20:47.210 --> 01:20:49.650
and if there was an embodiment
of that, I probably would

01:20:49.650 --> 01:20:51.980
have bodies in the sensors,
but what kind of body

01:20:51.980 --> 01:20:52.813
would it be?

01:20:52.813 --> 01:20:54.362
Is it gonna be humanoid, or
is it gonna be something else?

01:20:54.362 --> 01:20:55.547
I have a feeling it
would be something else.

01:20:55.547 --> 01:20:58.122
By the way, this is to speculate here.

01:20:58.122 --> 01:21:00.500
When I think of sensory motor
integration, most people

01:21:00.500 --> 01:21:01.490
think of robots.

01:21:01.490 --> 01:21:02.930
I don't think about robots.

01:21:02.930 --> 01:21:06.600
I think about how it is I
would navigate through data.

01:21:06.600 --> 01:21:08.740
How I would navigate through
a sensory world to know

01:21:08.740 --> 01:21:11.002
which part of the sensory
stream to pay attention to,

01:21:11.002 --> 01:21:14.720
and it may not even have
physical movement, the sensory

01:21:14.720 --> 01:21:16.919
motor integration problem
can be basically dealing with

01:21:16.919 --> 01:21:19.670
this sort of whole idea of
sort of how do I interact

01:21:19.670 --> 01:21:22.160
with my world and the cause and effect.

01:21:22.160 --> 01:21:23.070
Something like that.

01:21:23.070 --> 01:21:25.020
So that was the last,
probably couple questions,

01:21:25.020 --> 01:21:27.672
did I miss an important
part of your question?

01:21:27.672 --> 01:21:29.099
- [Male] Yeah, I mean,
like I said, I guess--

01:21:29.099 --> 01:21:31.380
- Do you wanna ask a
question about the emotions?

01:21:31.380 --> 01:21:33.320
Can I really get away without emotions?

01:21:33.320 --> 01:21:35.470
- [Male] Yeah, I mean basically
could there be intelligence

01:21:35.470 --> 01:21:36.303
in the emotions?

01:21:36.303 --> 01:21:37.297
- Yeah, I think you can have it.

01:21:37.297 --> 01:21:41.370
But as I define it, if you want
to define it as human-like,

01:21:41.370 --> 01:21:42.940
then no.

01:21:42.940 --> 01:21:44.544
If you want to be human-like,

01:21:44.544 --> 01:21:45.377
then you have to have emotions.

01:21:45.377 --> 01:21:46.283
If you want to pass the Turing
Test, you're gonna have to

01:21:46.283 --> 01:21:47.680
have emotions.

01:21:47.680 --> 01:21:50.220
But to do the things that I
talked about, make the world

01:21:50.220 --> 01:21:52.505
better and explore the
universe, I don't think so.

01:21:52.505 --> 01:21:53.921
- [Male] Not necessary.

01:21:53.921 --> 01:21:54.834
- Not necessary.

01:21:54.834 --> 01:21:55.911
All you would--

01:21:55.911 --> 01:21:57.430
In the end, what emotions
do, from the biological

01:21:57.430 --> 01:22:00.590
point of view, they basically,
there's sort of a switch

01:22:00.590 --> 01:22:03.190
to tell the neocortex to
learn this very rapidly.

01:22:03.190 --> 01:22:06.410
So when an emotional thing
happens, the amygdala will say

01:22:06.410 --> 01:22:08.030
you know, remember this!

01:22:08.030 --> 01:22:09.620
And store it very quickly.

01:22:09.620 --> 01:22:12.330
And that's how it kind of
plays with the neocortex.

01:22:12.330 --> 01:22:13.350
We can do things like that.

01:22:13.350 --> 01:22:16.040
But, it's not an essential
component of what intelligence

01:22:16.040 --> 01:22:18.337
is, and by the way, you
mentioned the music thing.

01:22:18.337 --> 01:22:19.410
You said you're into music.

01:22:19.410 --> 01:22:21.690
I'll mention, there's a guy
Charlie Gillion, and I was,

01:22:21.690 --> 01:22:24.740
who plays with Counting Crows,
and he's one of the musicians

01:22:24.740 --> 01:22:28.800
there, and this guy is a
rock star computer machinery

01:22:28.800 --> 01:22:30.143
geek, you wouldn't know this.

01:22:30.143 --> 01:22:32.797
But when he's not turning,
this is what he does.

01:22:32.797 --> 01:22:37.797
And, he's talked about coming
and visit us, because he's

01:22:38.010 --> 01:22:40.719
trying to build, I don't
want to give too much away,

01:22:40.719 --> 01:22:41.780
but he's trying to build
a product in the world

01:22:41.780 --> 01:22:44.340
of machine generated music.

01:22:44.340 --> 01:22:46.310
And he's trying to
understand how you do this,

01:22:46.310 --> 01:22:47.950
and he's really intrigued
by the kind of algorithms

01:22:47.950 --> 01:22:51.030
we have, so we'll see if
something comes out of that.

01:22:51.030 --> 01:22:53.392
Alright, one more person
with questions, this woman

01:22:53.392 --> 01:22:55.088
who's up next.

01:22:55.088 --> 01:22:57.740
You're the last one, so make it great!

01:22:57.740 --> 01:22:59.030
Make it last!

01:22:59.030 --> 01:23:01.650
- Hi, well, it's probably
like the simplest question

01:23:01.650 --> 01:23:02.483
of all.

01:23:02.483 --> 01:23:03.877
Actually, was like really
similar to what he asked.

01:23:03.877 --> 01:23:07.160
But, I was going to ask
about, so I was going to ask

01:23:07.160 --> 01:23:09.580
about emotions if you felt
that the reason you disregarded

01:23:09.580 --> 01:23:12.562
it is because it's just
so hard to have an emotion

01:23:12.562 --> 01:23:16.580
in like, an artificial
intelligence, or because you just

01:23:16.580 --> 01:23:18.366
felt that that actually
hindered intelligence.

01:23:18.366 --> 01:23:19.332
So--

01:23:19.332 --> 01:23:20.795
- It's the latter.

01:23:20.795 --> 01:23:25.202
Actually, from a neuromechanism
point of view, emotion

01:23:25.202 --> 01:23:27.790
systems in your brain are fairly small.

01:23:27.790 --> 01:23:29.380
I'm not sure we understand them.

01:23:29.380 --> 01:23:31.020
I don't study them exactly.

01:23:31.020 --> 01:23:33.190
I understand how they
interact with the neocortex,

01:23:33.190 --> 01:23:34.901
which is fairly simple.

01:23:34.901 --> 01:23:38.070
And so I think I have a
big feeling about that.

01:23:38.070 --> 01:23:41.650
But, it's more, it's more
a matter of to what purpose

01:23:41.650 --> 01:23:43.440
do you want emotions?

01:23:43.440 --> 01:23:44.549
What's the purp--

01:23:44.549 --> 01:23:46.310
Why do you want to do this?

01:23:46.310 --> 01:23:48.576
If there was a need for
it, well, we'll do it!

01:23:48.576 --> 01:23:52.080
But, I don't see any
reason why you can't do it.

01:23:52.080 --> 01:23:54.600
But, but, the only reason
I didn't put it on my list

01:23:54.600 --> 01:23:56.210
is I don't think it's
an essential ingredient.

01:23:56.210 --> 01:23:59.140
I don't see situations that
when you absolutely have to have

01:23:59.140 --> 01:24:02.060
emotions to do the things I
talked about: streaming data,

01:24:02.060 --> 01:24:03.700
build models of the world,
understand the structure

01:24:03.700 --> 01:24:05.290
of the world, make
predictions about the world.

01:24:05.290 --> 01:24:06.850
Those do not require emotions.

01:24:06.850 --> 01:24:09.870
Emotions are more for
like, what was important

01:24:09.870 --> 01:24:10.970
or what wasn't important?

01:24:10.970 --> 01:24:13.219
How do I prioritize
between various things,

01:24:13.219 --> 01:24:15.110
and things like that.

01:24:15.110 --> 01:24:16.870
So, and I don't think
there's anything mystical

01:24:16.870 --> 01:24:19.690
about emotions, there's
nothing like oh you can't model

01:24:19.690 --> 01:24:22.730
that, you know, in the end
I mentioned this in my talk

01:24:22.730 --> 01:24:24.080
yesterday, I didn't bring
it up today, but you know,

01:24:24.080 --> 01:24:25.670
the brain is just a bunch of neurons.

01:24:25.670 --> 01:24:26.503
There's nothing else.

01:24:26.503 --> 01:24:28.200
Everything is just a bunch of neurons.

01:24:28.200 --> 01:24:30.790
And, you know, there's no
other magic going on up there.

01:24:30.790 --> 01:24:33.300
So, if you can understand
how those neurons work

01:24:33.300 --> 01:24:35.299
and how they play together in
the way I talked about then,

01:24:35.299 --> 01:24:37.300
yeah, you could model that too.

01:24:37.300 --> 01:24:39.298
- [Woman] So then, there's
nothing human that can

01:24:39.298 --> 01:24:40.360
be in a machine?

01:24:40.360 --> 01:24:43.990
- Is there nothing human
that can't be in a machine?

01:24:43.990 --> 01:24:46.320
Again, I said that I didn't
think that was the goal,

01:24:46.320 --> 01:24:48.111
and I don't think that's gonna happen.

01:24:48.111 --> 01:24:51.210
Because I don't think
there's no reason to do that.

01:24:51.210 --> 01:24:54.680
But on a theoretical level,
I don't see anything that's

01:24:54.680 --> 01:24:57.516
in a human that couldn't
be, and again, you're saying

01:24:57.516 --> 01:24:58.920
a machine, don't think
of it like some you know,

01:24:58.920 --> 01:25:00.330
mechanics and a machine.

01:25:00.330 --> 01:25:05.210
It's a very complex memory
system, and yeah, I don't think

01:25:05.210 --> 01:25:07.514
there's anything magic going on there.

01:25:07.514 --> 01:25:08.880
- [Woman] May I ask one more question?

01:25:08.880 --> 01:25:10.423
- You don't like my
answer, that's why you ask

01:25:10.423 --> 01:25:11.256
another question.

01:25:11.256 --> 01:25:12.800
(audience laughing)

01:25:12.800 --> 01:25:14.360
- [Woman] No, it's different, actually.

01:25:14.360 --> 01:25:17.121
It's on the side, but, kind of--

01:25:17.121 --> 01:25:18.220
- I'm gonna have to wrap
up soon, I only have one

01:25:18.220 --> 01:25:19.647
person so real quick--

01:25:19.647 --> 01:25:23.007
- [Woman] Okay, so basically
you said we have too much

01:25:23.007 --> 01:25:24.292
data and it's like, building up.

01:25:24.292 --> 01:25:25.900
And, do you propose a solution for that?

01:25:25.900 --> 01:25:26.805
- Yeah, in some sense.

01:25:26.805 --> 01:25:29.750
This is switching away from
brains and neuroscience.

01:25:29.750 --> 01:25:34.020
The solution is that we are
not gonna save all the data

01:25:34.020 --> 01:25:34.853
in the world.

01:25:34.853 --> 01:25:36.550
There's no point in saving it.

01:25:36.550 --> 01:25:38.840
Your brain doesn't save all
the sensory information,

01:25:38.840 --> 01:25:42.410
there's no point in saving
second by second energy

01:25:42.410 --> 01:25:44.720
information from a billion buildings.

01:25:44.720 --> 01:25:47.720
The point, the whole way of
getting around that problem

01:25:47.720 --> 01:25:51.110
is to handle the data
immediately, feed it into models,

01:25:51.110 --> 01:25:54.090
billions of models, have
them act on it immediately,

01:25:54.090 --> 01:25:55.332
and get rid of it.

01:25:55.332 --> 01:25:57.590
That's the solution to big data.

01:25:57.590 --> 01:25:59.984
Okay, let's the other woman,
do you want to ask a question?

01:25:59.984 --> 01:26:02.160
Okay, can I have one more question.

01:26:02.160 --> 01:26:04.540
I lied, I'll let one more question in.

01:26:04.540 --> 01:26:07.710
- [Female] I'm just curious
that like when do you think

01:26:07.710 --> 01:26:10.340
the fluid robot really exist?

01:26:10.340 --> 01:26:12.320
- When would the fluid
robotics really exist?

01:26:12.320 --> 01:26:14.380
- [Woman] When do you think, yeah.

01:26:14.380 --> 01:26:16.940
- Boy, that's a tough question.

01:26:16.940 --> 01:26:18.610
- [Woman] Can I see that before I die?

01:26:18.610 --> 01:26:20.050
- Will you see it before you die?

01:26:20.050 --> 01:26:21.390
How old are you?

01:26:21.390 --> 01:26:22.460
- [Woman] I'm 21.

01:26:22.460 --> 01:26:23.460
- 31?

01:26:23.460 --> 01:26:24.540
I think so, yeah.

01:26:24.540 --> 01:26:26.850
I'm not sure I'll see it before I die.

01:26:26.850 --> 01:26:28.433
Thank you very much, good luck.

01:26:28.433 --> 01:26:31.433
(audience clapping)

01:26:32.302 --> 01:26:35.302
(soft string music)

