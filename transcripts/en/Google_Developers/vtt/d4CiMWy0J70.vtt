WEBVTT
Kind: captions
Language: en

00:00:01.390 --> 00:00:04.360
We'll now discuss more advanced
Datastore topics on

00:00:04.360 --> 00:00:07.230
Query, Index, and
Transactions.

00:00:07.230 --> 00:00:10.160
These are three topics I'll
cover on this lesson.

00:00:10.160 --> 00:00:12.740
How does Datastore
work on Query?

00:00:12.740 --> 00:00:15.480
We'll discuss what the reason
is behind these kinds of

00:00:15.480 --> 00:00:18.320
restrictions, and how they're
actually an essential

00:00:18.320 --> 00:00:21.310
technology to make
it very scalable.

00:00:21.310 --> 00:00:24.800
The second topic will be the
workarounds based to overcome

00:00:24.800 --> 00:00:26.070
those restrictions.

00:00:26.070 --> 00:00:27.730
Then lastly, I'd like
to discuss about

00:00:27.730 --> 00:00:29.780
transactions in Datastore.

00:00:29.780 --> 00:00:32.870
Because Datastore is not a
traditional database, the

00:00:32.870 --> 00:00:35.710
behavior of the transaction
in the Datastore

00:00:35.710 --> 00:00:37.780
is completely different.

00:00:37.780 --> 00:00:40.980
There's an important concept
called entity groups, which

00:00:40.980 --> 00:00:43.360
we'll spend some time on.

00:00:43.360 --> 00:00:46.670
So how does Datastore
work on Query?

00:00:46.670 --> 00:00:49.480
Bigtable doesn't
support Query.

00:00:49.480 --> 00:00:52.970
In ordinary, traditional
relational databases, you'll

00:00:52.970 --> 00:00:56.600
always be able to search on the
value of a current column.

00:00:56.600 --> 00:00:59.460
But Bigtable doesn't allow
you to do that.

00:00:59.460 --> 00:01:01.920
Bigtable can search
only on a key.

00:01:01.920 --> 00:01:05.120
Then, how does Datastore
support a Query?

00:01:05.120 --> 00:01:08.260
In Datastore, Queries will
be executed as an

00:01:08.260 --> 00:01:10.170
index scan of Bigtable.

00:01:10.170 --> 00:01:12.570
So this is the big difference
between Datastore in

00:01:12.570 --> 00:01:15.660
traditional relational
databases.

00:01:15.660 --> 00:01:18.590
Query is executed as
an index scan.

00:01:18.590 --> 00:01:21.060
So think about a simple
query like this.

00:01:21.060 --> 00:01:24.360
When it's trying to select all
the person entities with

00:01:24.360 --> 00:01:28.440
conditions like height property
must be less than 72.

00:01:28.440 --> 00:01:31.920
For this query, it will be
executed as a range scan on

00:01:31.920 --> 00:01:35.480
Bigtable with an index
table for height.

00:01:35.480 --> 00:01:39.010
Recall in the Datastore
introduction session there was

00:01:39.010 --> 00:01:41.640
a function called mem scan.

00:01:41.640 --> 00:01:45.960
Bigtable can scan on the
key, not on the values.

00:01:45.960 --> 00:01:50.210
So by having an index table on
Bigtable like this, which has

00:01:50.210 --> 00:01:54.410
the property name and the
property values in its key,

00:01:54.410 --> 00:01:58.860
then you can implement a query
like this on Bigtable without

00:01:58.860 --> 00:02:01.520
reading the actual values.

00:02:01.520 --> 00:02:05.120
Actually, we're using values
from the columns in the key of

00:02:05.120 --> 00:02:08.570
the index itself, so that we
can search on the values by

00:02:08.570 --> 00:02:09.919
doing the index scan.

00:02:09.919 --> 00:02:13.530
This is how data store makes
queries so scalable.

00:02:13.530 --> 00:02:17.400
Without any index table, most
of the queries on Datastore

00:02:17.400 --> 00:02:21.640
are built through exception,
or you have an error.

00:02:21.640 --> 00:02:24.050
But this is quite different than
the traditional indexing

00:02:24.050 --> 00:02:25.780
in a traditional database.

00:02:25.780 --> 00:02:29.340
In relational databases, you
might want to add an index

00:02:29.340 --> 00:02:31.640
just for the performance
issue.

00:02:31.640 --> 00:02:34.170
If you don't have any
performance issues, like if

00:02:34.170 --> 00:02:37.730
you are just having 100 rows
in a table, then you don't

00:02:37.730 --> 00:02:40.600
have to put any index on the
traditional database table.

00:02:40.600 --> 00:02:43.830
You can still run a query
without having any index.

00:02:43.830 --> 00:02:47.880
But in Datastore, you cannot
execute almost any query on

00:02:47.880 --> 00:02:50.710
Datastore without
an index table.

00:02:50.710 --> 00:02:54.100
This is not a restriction,
this is a key technology.

00:02:54.100 --> 00:02:58.250
It's why Datastore query can
be practically limitless.

00:02:58.250 --> 00:03:01.280
If you are using a traditional
database, there's a certain

00:03:01.280 --> 00:03:03.640
scalability limit
on the query.

00:03:03.640 --> 00:03:07.530
But on Datastore, you can just
run a shard among the multiple

00:03:07.530 --> 00:03:08.990
physical servers.

00:03:08.990 --> 00:03:11.740
But you don't have to care about
that by yourself, all

00:03:11.740 --> 00:03:15.480
the underlying layers, like
Datastore and Bigtable layers,

00:03:15.480 --> 00:03:19.000
are taking care of all that,
sharding all the data into

00:03:19.000 --> 00:03:20.900
multiple physical servers.

00:03:20.900 --> 00:03:23.490
Or, dispatching the
queries onto your

00:03:23.490 --> 00:03:25.480
multiple physical servers.

00:03:25.480 --> 00:03:28.770
That's not achievable in
a relational database.

00:03:28.770 --> 00:03:32.690
The most important point is
that the index scan makes

00:03:32.690 --> 00:03:36.060
query performance for the size
of your results, not

00:03:36.060 --> 00:03:37.710
with the data set.

00:03:37.710 --> 00:03:41.320
So no matter how many entries
you have, it doesn't affect

00:03:41.320 --> 00:03:43.420
your performance
of your query.

00:03:43.420 --> 00:03:47.310
Your query performance will be
affected only by the size of

00:03:47.310 --> 00:03:49.000
the result set.

00:03:49.000 --> 00:03:51.700
If you are trying to get
millions of records as a

00:03:51.700 --> 00:03:55.040
result set, you will decrease
your query performance.

00:03:55.040 --> 00:03:58.840
The size of the whole data set
won't affect your query

00:03:58.840 --> 00:04:00.380
performance.

00:04:00.380 --> 00:04:02.580
But how does this support
various queries

00:04:02.580 --> 00:04:04.460
just by index scans?

00:04:04.460 --> 00:04:07.570
As you saw, the index scan
mechanism is so simple.

00:04:07.570 --> 00:04:10.710
So you might wonder how it
can support complex query

00:04:10.710 --> 00:04:14.860
conditions like WHERE
or ORDER BY clauses.

00:04:14.860 --> 00:04:19.709
The answer is various kinds of
index we have on Datastore.

00:04:19.709 --> 00:04:22.820
One is the single-property
index.

00:04:22.820 --> 00:04:26.480
Single-property index will be
working for queries with a

00:04:26.480 --> 00:04:30.530
simple sorting or filtering,
just like this.

00:04:30.530 --> 00:04:34.610
So here's a sample query with
very simple conditions, like

00:04:34.610 --> 00:04:36.800
name property equals John.

00:04:36.800 --> 00:04:40.200
If you ran a query like this,
this query will be converted

00:04:40.200 --> 00:04:43.070
into an index scan on
the single-property

00:04:43.070 --> 00:04:45.180
index for name property.

00:04:45.180 --> 00:04:48.580
For every single-property or
entity there will be a

00:04:48.580 --> 00:04:52.280
single-property index
automatically created for you.

00:04:52.280 --> 00:04:54.860
You don't have to define
explicitly

00:04:54.860 --> 00:04:56.620
to build that index.

00:04:56.620 --> 00:05:00.170
Every single property index will
be automatically created

00:05:00.170 --> 00:05:02.050
for all properties.

00:05:02.050 --> 00:05:05.310
So if you're entity has a
property called name, then the

00:05:05.310 --> 00:05:08.650
single-property index will
be created automatically.

00:05:08.650 --> 00:05:11.850
And if you execute a query
like this, then it'll be

00:05:11.850 --> 00:05:14.630
converted and executed
as a range scan

00:05:14.630 --> 00:05:16.540
on this index table.

00:05:16.540 --> 00:05:21.630
Or index scan for a key for
a parent name and John.

00:05:21.630 --> 00:05:24.570
This is actually a key
of the index table.

00:05:24.570 --> 00:05:28.250
So these are the keys of the
index table and they should be

00:05:28.250 --> 00:05:33.340
a value, which is the key
for another table.

00:05:33.340 --> 00:05:36.130
This is how you can write
the simple query on

00:05:36.130 --> 00:05:38.350
Datastore in Python.

00:05:38.350 --> 00:05:41.650
Because you are not directly
handling your Bigtable, you

00:05:41.650 --> 00:05:45.170
can just use the simple query
language like this.

00:05:45.170 --> 00:05:48.530
This is another example of the
queries that is executed to be

00:05:48.530 --> 00:05:52.050
a single-property index, which
has the unique quality of

00:05:52.050 --> 00:05:54.310
filtering, like the
greater than,

00:05:54.310 --> 00:05:57.090
equals, or if-then operators.

00:05:57.090 --> 00:05:59.190
And also, sorting using
the ORDER BY

00:05:59.190 --> 00:06:01.860
clause on a single property.

00:06:01.860 --> 00:06:04.910
We're still handling filtering
or sorting on just one

00:06:04.910 --> 00:06:08.320
property, so that those queries
can be handled by the

00:06:08.320 --> 00:06:10.350
single-property index.

00:06:10.350 --> 00:06:13.620
If you are handling just one
property on your query, then

00:06:13.620 --> 00:06:17.310
these queries can be executed
as a range scan on a single

00:06:17.310 --> 00:06:19.080
property index scan.

00:06:19.080 --> 00:06:22.260
In this case, it's trying to
find a name which starts with

00:06:22.260 --> 00:06:26.700
B, so it will be converted to a
range scan from Bob to Brad,

00:06:26.700 --> 00:06:27.820
for example.

00:06:27.820 --> 00:06:30.840
And there's something else you
should notice, that that

00:06:30.840 --> 00:06:35.930
single property index actually
uses two index tables.

00:06:35.930 --> 00:06:40.540
One is an ascending table and
another is a descending table.

00:06:40.540 --> 00:06:44.700
So every single property there
will be two index tables

00:06:44.700 --> 00:06:47.640
automatically created.

00:06:47.640 --> 00:06:51.170
So the single-property index can
support queries like this,

00:06:51.170 --> 00:06:52.990
like the equality filters.

00:06:52.990 --> 00:06:55.740
At this point, for this
particular query, you don't

00:06:55.740 --> 00:06:59.420
have to have a composite index
that'll be discussed later.

00:06:59.420 --> 00:07:03.040
Because Datastore will merge the
results of the queries for

00:07:03.040 --> 00:07:06.480
each properties into
one automatically.

00:07:06.480 --> 00:07:09.780
So Datastore will execute a
query for the first name

00:07:09.780 --> 00:07:12.810
equals Bob, and then execute
a query for our

00:07:12.810 --> 00:07:14.640
last name equals Jameson.

00:07:14.640 --> 00:07:17.280
Then, merge the result
into one.

00:07:17.280 --> 00:07:21.770
Also, inequality filters, or not
equals attribute, can be

00:07:21.770 --> 00:07:22.600
executed by the

00:07:22.600 --> 00:07:24.990
single-property index like this.

00:07:24.990 --> 00:07:29.290
And not equal to filter, like
last name not equal to Bob

00:07:29.290 --> 00:07:33.360
will be converted internally
as a query like this.

00:07:33.360 --> 00:07:35.700
This should be or actually.

00:07:35.700 --> 00:07:38.930
First name Bob, this will
also be handled as a

00:07:38.930 --> 00:07:42.430
single-property index, too.

00:07:42.430 --> 00:07:45.290
Let's discuss the more
complex query.

00:07:45.290 --> 00:07:48.250
For more complex queries that
would involve more than one

00:07:48.250 --> 00:07:51.190
property, you would need to
create an index code or

00:07:51.190 --> 00:07:52.700
composite index.

00:07:52.700 --> 00:07:56.050
This is also known as
a custom index.

00:07:56.050 --> 00:07:59.860
This is an example of the query
for composite index.

00:07:59.860 --> 00:08:03.500
If you are using an inequality
filter on one property and

00:08:03.500 --> 00:08:06.130
combining it with the inequality
filter on another

00:08:06.130 --> 00:08:08.690
property, then you would
need to build your

00:08:08.690 --> 00:08:10.810
composite index for that.

00:08:10.810 --> 00:08:13.390
And a composite index
has a logical value.

00:08:13.390 --> 00:08:17.740
It has values for logical
properties, like kind name and

00:08:17.740 --> 00:08:21.330
values for the last name and
values for the first name, so

00:08:21.330 --> 00:08:24.490
that this Datastore can convert
this kind of complex

00:08:24.490 --> 00:08:27.710
query into a simple
index scan.

00:08:27.710 --> 00:08:31.370
The combination can be very
complex, with lots of tables

00:08:31.370 --> 00:08:33.900
being created for that
composite index.

00:08:33.900 --> 00:08:37.159
You, the developer, have to
explicitly define that by your

00:08:37.159 --> 00:08:39.299
configuration file.

00:08:39.299 --> 00:08:42.970
This is an example of a query
for a composite index.

00:08:42.970 --> 00:08:46.070
In Java, you can specify that
in a file called Datastore

00:08:46.070 --> 00:08:49.450
index, or in the index.yaml
file.

00:08:49.450 --> 00:08:53.770
In Python, you can just use
your index.yaml file.

00:08:53.770 --> 00:08:56.000
Or you can write the definitions
of the composite

00:08:56.000 --> 00:08:57.440
index like this.

00:08:57.440 --> 00:09:00.720
You can specify the kind name
and the rest the properties

00:09:00.720 --> 00:09:04.620
with the property name and the
directions you want to have.

00:09:04.620 --> 00:09:07.490
By specifying this in deploying
this application

00:09:07.490 --> 00:09:10.970
onto the cloud, Datastore will
start building the composite

00:09:10.970 --> 00:09:13.300
index for you.

00:09:13.300 --> 00:09:16.880
Composite indexes can support
queries like this as we just

00:09:16.880 --> 00:09:18.330
saw in this example.

00:09:18.330 --> 00:09:21.280
The queries with the inequality
filters, inequality

00:09:21.280 --> 00:09:24.440
filters on other properties,
or the queries with logical

00:09:24.440 --> 00:09:25.860
sort orders.

00:09:25.860 --> 00:09:30.530
This'll be a very popular use
case for the composite index.

00:09:30.530 --> 00:09:32.950
So let's move on to the
workaround part.

00:09:32.950 --> 00:09:36.400
How can we make Datastore Query
operate at almost as

00:09:36.400 --> 00:09:40.840
flexibly as a relational
database query?

00:09:40.840 --> 00:09:44.450
We cannot say Datastore will be
super flexible just like an

00:09:44.450 --> 00:09:48.030
RDB because a single Datastore
Query will

00:09:48.030 --> 00:09:49.960
have to be super scalable.

00:09:49.960 --> 00:09:51.900
We'll have to have certain
restrictions.

00:09:51.900 --> 00:09:54.430
For the App Engine developer,
it's very important to

00:09:54.430 --> 00:09:57.410
understand those restrictions
and workarounds before you

00:09:57.410 --> 00:09:59.650
start designing your
data model.

00:09:59.650 --> 00:10:02.950
Like the restrictions,
inequality filters and sorting

00:10:02.950 --> 00:10:04.560
needs an index.

00:10:04.560 --> 00:10:08.360
Inequality filters like less
than or greater than are only

00:10:08.360 --> 00:10:11.700
applicable to the single index,
single properties.

00:10:11.700 --> 00:10:14.360
You cannot have the query
where you are applying

00:10:14.360 --> 00:10:17.120
inequality filters on
to two properties.

00:10:17.120 --> 00:10:20.580
So this is one of the major
restrictions we have.

00:10:20.580 --> 00:10:23.340
There's a very close
relationship with sorting and

00:10:23.340 --> 00:10:26.340
the inequality filters, because
every filtering or

00:10:26.340 --> 00:10:29.220
sorting is implemented
as an index scan.

00:10:29.220 --> 00:10:32.300
So it means if you want to do
any sorting, you have to

00:10:32.300 --> 00:10:36.620
prepare everything sorted
beforehand as an index table.

00:10:36.620 --> 00:10:40.560
One of the famous restrictions
for any NoSQL database is that

00:10:40.560 --> 00:10:42.790
JOIN is not supported.

00:10:42.790 --> 00:10:45.790
Whenever a mission-critical
system gets so big,

00:10:45.790 --> 00:10:48.470
denormalization issues occur.

00:10:48.470 --> 00:10:50.800
If you're splitting this
database into multiple

00:10:50.800 --> 00:10:55.190
instances, you cannot execute
joint operations anymore.

00:10:55.190 --> 00:10:58.570
So if your querying like this
where you'd want to join two

00:10:58.570 --> 00:11:01.730
different tables, like a person
and address, you may

00:11:01.730 --> 00:11:05.000
want to put the country property
into a person, not

00:11:05.000 --> 00:11:09.310
normalize the table,
denormalizing the table.

00:11:09.310 --> 00:11:12.300
This means there's a trade-off
between the performance, or

00:11:12.300 --> 00:11:15.830
scalability, and the redundancy,
or cost, of

00:11:15.830 --> 00:11:18.870
managing multiple duplicated
values onto the

00:11:18.870 --> 00:11:21.630
kind or many kinds.

00:11:21.630 --> 00:11:24.480
There are some tweaks you can
emulate the joint operation in

00:11:24.480 --> 00:11:28.430
Datastore, like this, if you
have to handle end-to-end

00:11:28.430 --> 00:11:29.860
relationships.

00:11:29.860 --> 00:11:34.000
In this example, each entity,
called Person, has friends.

00:11:34.000 --> 00:11:38.150
And they have an end-to-end
relationship between person.

00:11:38.150 --> 00:11:41.480
In a traditional relational
database, you'd have a JOIN

00:11:41.480 --> 00:11:44.750
table called Friends table,
which represents every

00:11:44.750 --> 00:11:47.490
relationship between
the single person.

00:11:47.490 --> 00:11:50.060
In Datastore, you don't
have to have another

00:11:50.060 --> 00:11:51.880
JOIN table to do that.

00:11:51.880 --> 00:11:53.660
You can just have
a multi-valued

00:11:53.660 --> 00:11:55.080
property like this.

00:11:55.080 --> 00:11:58.040
Friends property may have a list
of strings, list of the

00:11:58.040 --> 00:12:01.710
people names so that they can
write the query like this.

00:12:01.710 --> 00:12:05.000
If you want to find a mutual
friend, who is a friend for

00:12:05.000 --> 00:12:07.630
Bob and also a friend for
Willie, then you can write a

00:12:07.630 --> 00:12:10.440
query like this.

00:12:10.440 --> 00:12:14.570
Select from person where friends
equal Bob and friend

00:12:14.570 --> 00:12:16.470
equals to Willie.

00:12:16.470 --> 00:12:20.540
And location equals to San
Francisco, for example.

00:12:20.540 --> 00:12:24.650
This query can be executed by
using a single-property index.

00:12:24.650 --> 00:12:27.690
You don't have to prepare a
composite index for this query

00:12:27.690 --> 00:12:30.490
because the Datastore will be
emerging and joining the

00:12:30.490 --> 00:12:34.050
result from the two different
queries into one.

00:12:34.050 --> 00:12:36.480
This means that you can
implement the end-to-end

00:12:36.480 --> 00:12:40.130
relationship within the
Datastore result without any

00:12:40.130 --> 00:12:42.340
JOIN operation.

00:12:42.340 --> 00:12:43.780
You can still do this
for joining

00:12:43.780 --> 00:12:45.460
between the same entities.

00:12:45.460 --> 00:12:47.360
In this case, person.

00:12:47.360 --> 00:12:50.950
This example is joining two
different result sets for the

00:12:50.950 --> 00:12:54.140
same entities, but it's still
a major restriction that you

00:12:54.140 --> 00:12:57.650
cannot join the different
tables.

00:12:57.650 --> 00:12:59.880
There's another restriction, we
have a workaround for the

00:12:59.880 --> 00:13:01.500
App Engine DataStore.

00:13:01.500 --> 00:13:04.640
In Datastore, you cannot
execute any GROUP BY

00:13:04.640 --> 00:13:07.670
operations, or aggregate
functions.

00:13:07.670 --> 00:13:11.330
More specifically, you cannot
execute counts from table.

00:13:11.330 --> 00:13:13.450
So you cannot count the
number of entities

00:13:13.450 --> 00:13:15.040
you have right now.

00:13:15.040 --> 00:13:17.790
To be scalable, you don't have
time to look at all the

00:13:17.790 --> 00:13:20.120
entities and count them all.

00:13:20.120 --> 00:13:23.280
You do have a workaround having
a special entity that

00:13:23.280 --> 00:13:27.770
maintains aggregates by you,
like a counter entity, which

00:13:27.770 --> 00:13:30.650
holds the number of entities
you've created.

00:13:30.650 --> 00:13:33.190
If you're doing this, however,
you have to be careful about

00:13:33.190 --> 00:13:35.020
the entity's bottleneck.

00:13:35.020 --> 00:13:38.430
There are limitations, like the
one entity can only get

00:13:38.430 --> 00:13:40.580
one update per second.

00:13:40.580 --> 00:13:43.250
This is not a physical limit,
but it's kind of a design

00:13:43.250 --> 00:13:44.980
property you have to follow.

00:13:44.980 --> 00:13:48.230
Because of this restriction,
you cannot simply have one

00:13:48.230 --> 00:13:52.140
counter entity to count many
accesses from the users.

00:13:52.140 --> 00:13:55.320
There's a design element called
a sharding counter

00:13:55.320 --> 00:13:57.630
where you are sharding the
counters into multiple

00:13:57.630 --> 00:14:02.020
entities and gather aggregated
results later.

00:14:02.020 --> 00:14:04.980
You can alternatively use
memcache as a holder for that

00:14:04.980 --> 00:14:06.180
count value.

00:14:06.180 --> 00:14:08.390
Then, use Datastore as
well to back up the

00:14:08.390 --> 00:14:10.780
latest count number.

00:14:10.780 --> 00:14:13.210
It's hard to implement a counter
or any aggregate

00:14:13.210 --> 00:14:15.190
function on the Datastore.

00:14:15.190 --> 00:14:17.490
If it's not required to get
the aggregated value

00:14:17.490 --> 00:14:20.330
instantly, then you can do
it later by using batch

00:14:20.330 --> 00:14:23.620
processing by using a
back end instance.

00:14:23.620 --> 00:14:26.640
There's also a very useful open
source framework called

00:14:26.640 --> 00:14:30.190
App Engine MapReduce, which is
using task queue to emulate

00:14:30.190 --> 00:14:32.110
the MapReduce framework.

00:14:32.110 --> 00:14:35.420
We won't discuss details about
the App Engine MapReduce, but

00:14:35.420 --> 00:14:38.200
this can efficiently handle
batch operations on huge

00:14:38.200 --> 00:14:40.790
amounts of entities
on Datastore.

00:14:40.790 --> 00:14:43.710
Finally, if it's not urgent,
then you can just rely on the

00:14:43.710 --> 00:14:46.410
Statistics API of
the Datastore.

00:14:46.410 --> 00:14:50.250
Datastore provides statistics
data, like how many entries

00:14:50.250 --> 00:14:54.790
you have or what kind by
accessing the API for that,

00:14:54.790 --> 00:14:57.820
and those numbers will be
updated once per day.

00:14:57.820 --> 00:15:01.200
So it's a very large number,
but still, you can use that

00:15:01.200 --> 00:15:03.480
for certain usages.

00:15:03.480 --> 00:15:06.370
Moving onto the aggregation
function restrictions.

00:15:06.370 --> 00:15:09.130
You cannot use the minimum
function or maximum function

00:15:09.130 --> 00:15:12.050
like you have in your
relational database.

00:15:12.050 --> 00:15:15.280
You may want to have a single
property and sort it ascending

00:15:15.280 --> 00:15:18.620
or descending so that you can
find the first entity on the

00:15:18.620 --> 00:15:22.790
index to get the minimum
value or maximum value.

00:15:22.790 --> 00:15:25.350
This technique in Datastore
allows you to have an

00:15:25.350 --> 00:15:29.560
aggregated value, like a count
or average, or a maximum,

00:15:29.560 --> 00:15:33.220
while the order of login speed
by using a data structure

00:15:33.220 --> 00:15:35.220
called Skiplist.

00:15:35.220 --> 00:15:39.150
Skiplist is very similar
to a binary or B tree.

00:15:39.150 --> 00:15:41.950
Just like a tree search, you
don't have to look for every

00:15:41.950 --> 00:15:44.040
single node to find a node.

00:15:44.040 --> 00:15:46.770
You can start finding it
from the express lane.

00:15:46.770 --> 00:15:50.080
The trick is, you can just add
the aggregate values on each

00:15:50.080 --> 00:15:52.150
express lane to get
the aggregated

00:15:52.150 --> 00:15:54.250
value on the Datastore.

00:15:54.250 --> 00:15:57.690
Using this data structure, you
can obtain the average or a

00:15:57.690 --> 00:16:01.130
maximal or minimum values of a
range of entries in a very

00:16:01.130 --> 00:16:02.630
short time period.

00:16:02.630 --> 00:16:05.870
You don't have to scan all the
entities in the range.

00:16:05.870 --> 00:16:09.230
For example, how many entities
do you have from the

00:16:09.230 --> 00:16:11.920
node 1 to node 6?

00:16:11.920 --> 00:16:16.220
Using the level 3 express lane,
you find 3, and then you

00:16:16.220 --> 00:16:19.500
add 2 so that the answer
will be 5.

00:16:19.500 --> 00:16:23.270
There are 5 nodes between
node 1 and node 6.

00:16:23.270 --> 00:16:25.840
An important thing to
consider is the cost

00:16:25.840 --> 00:16:28.120
of indexing in Datastore.

00:16:28.120 --> 00:16:31.630
This is a real application which
has an entity with only

00:16:31.630 --> 00:16:33.560
25 megabytes.

00:16:33.560 --> 00:16:38.940
However, it consumes a total
of 238 megabytes somehow.

00:16:38.940 --> 00:16:42.090
The rest of the space is taken
up by the indexes because that

00:16:42.090 --> 00:16:44.680
entity has so many
single-property index tables

00:16:44.680 --> 00:16:46.190
for each property.

00:16:46.190 --> 00:16:49.820
It means you need to have 10
times larger index space than

00:16:49.820 --> 00:16:51.580
the entity itself.

00:16:51.580 --> 00:16:54.510
This is another big difference
between the Datastore and a

00:16:54.510 --> 00:16:56.390
traditional database.

00:16:56.390 --> 00:16:59.670
If you want to cost estimate
Datastore accurately, then you

00:16:59.670 --> 00:17:03.530
have to think about the cost
of index in terms of space.

00:17:03.530 --> 00:17:06.349
You may also want to check
your document called

00:17:06.349 --> 00:17:09.940
understanding right cost, which
explains the CPU cost

00:17:09.940 --> 00:17:12.390
you consume for building
the index.

00:17:12.390 --> 00:17:16.310
For every single entity write or
update, it will also update

00:17:16.310 --> 00:17:20.349
the single-property index or
custom index you have created.

00:17:20.349 --> 00:17:23.220
When you are using hundreds of
index tables, that means

00:17:23.220 --> 00:17:26.630
updates can take a
very long time.

00:17:26.630 --> 00:17:29.440
And now to discuss some
final caveats.

00:17:29.440 --> 00:17:32.120
There are no aggregation
functions like the lower or

00:17:32.120 --> 00:17:34.550
upper, or any other functions.

00:17:34.550 --> 00:17:37.680
You may want to pre-process or
pre-calculate the value,

00:17:37.680 --> 00:17:40.670
putting an additional property
on the entities so that you

00:17:40.670 --> 00:17:42.440
can implement it.

00:17:42.440 --> 00:17:45.820
Because Datastore doesn't
support Search, there's a very

00:17:45.820 --> 00:17:48.980
useful new API called
Search API, which

00:17:48.980 --> 00:17:52.060
supports full text search.

00:17:52.060 --> 00:17:56.370
It converts every query into the
index scan or key, so it

00:17:56.370 --> 00:17:59.390
means you cannot be applying the
irregular expressions Full

00:17:59.390 --> 00:18:02.080
Text Search on the index scan.

00:18:02.080 --> 00:18:05.890
So instead of using Datastore,
we may recommend you look into

00:18:05.890 --> 00:18:09.350
the Search API for
full text search.

00:18:09.350 --> 00:18:12.190
When you're using the query,
you may not see that result

00:18:12.190 --> 00:18:16.170
instantly, returning the
old data instead.

00:18:16.170 --> 00:18:19.190
To avoid this, you should
use Ancestor Query.

00:18:19.190 --> 00:18:22.690
Ancestor Query guarantees
strong consistency.

00:18:22.690 --> 00:18:24.540
It means you're always
getting the latest

00:18:24.540 --> 00:18:27.200
result of your updates.

00:18:27.200 --> 00:18:29.960
And lastly, there's a
combinational explosion

00:18:29.960 --> 00:18:33.780
phenomenon that can happen on
large value properties.

00:18:33.780 --> 00:18:36.560
If you're having multiple
large-valued properties on the

00:18:36.560 --> 00:18:39.510
query or index, because
multiple large-valued

00:18:39.510 --> 00:18:43.090
properties on the index, there
will be a 10 times 10 log of

00:18:43.090 --> 00:18:44.880
combinations of the index.

00:18:44.880 --> 00:18:47.540
So you need to be careful when
you're using the multiple

00:18:47.540 --> 00:18:52.310
large-valued properties on
your query or index.

00:18:52.310 --> 00:18:53.420
Let's now describe how

00:18:53.420 --> 00:18:56.640
transactions can work in Datastore.

00:18:56.640 --> 00:18:59.270
Now you understand how Datastore
can be scalable with

00:18:59.270 --> 00:19:02.340
its unique practice
like data scan.

00:19:02.340 --> 00:19:03.830
It's time now to understand the

00:19:03.830 --> 00:19:06.930
transactional aspects in Datastore.

00:19:06.930 --> 00:19:09.360
In Datastore, you design your
data model based on

00:19:09.360 --> 00:19:11.580
consistency requirements.

00:19:11.580 --> 00:19:16.110
This is one point which is not
required in traditional RDBs.

00:19:16.110 --> 00:19:19.290
In traditional relational
database models, you don't

00:19:19.290 --> 00:19:22.520
have to care about the
transactional requirement when

00:19:22.520 --> 00:19:24.420
you're designing your schema.

00:19:24.420 --> 00:19:27.750
But in Datastore, the data model
is closely related to

00:19:27.750 --> 00:19:29.850
the transactional requirement.

00:19:29.850 --> 00:19:33.150
So by designing carefully, your
data model can make your

00:19:33.150 --> 00:19:37.450
application super scalable
and yet, consistent.

00:19:37.450 --> 00:19:39.510
So what is a transaction?

00:19:39.510 --> 00:19:43.000
This is a recap of the general
concepts of transactions.

00:19:43.000 --> 00:19:45.880
If you have any experience on
the traditional database, then

00:19:45.880 --> 00:19:49.580
you already know the concept of
the ACID characteristics,

00:19:49.580 --> 00:19:51.270
but I'd like to recap.

00:19:51.270 --> 00:19:55.960
In short, it stands for the
Atomicity, Consistency, and

00:19:55.960 --> 00:19:59.620
Isolation and Durability
required for every

00:19:59.620 --> 00:20:01.390
transaction.

00:20:01.390 --> 00:20:04.000
A transaction is a series
of operations on

00:20:04.000 --> 00:20:06.360
your record or entity.

00:20:06.360 --> 00:20:09.210
If you have a user entity and
want to execute a series of

00:20:09.210 --> 00:20:13.040
operation, like updating the
name of a user or the age

00:20:13.040 --> 00:20:16.510
property of a user, those series
of operations can be a

00:20:16.510 --> 00:20:18.180
transaction.

00:20:18.180 --> 00:20:20.970
Originally, transactions were
invented for a mainframe

00:20:20.970 --> 00:20:25.260
environment, so were robust
and consistent.

00:20:25.260 --> 00:20:28.180
Whenever you have a hardware
failure or application bugs,

00:20:28.180 --> 00:20:31.560
or getting so many requests from
the users, no matter how

00:20:31.560 --> 00:20:34.970
things can happen, they want to
make everything consistent

00:20:34.970 --> 00:20:37.780
or reliable for business
purposes.

00:20:37.780 --> 00:20:41.770
To do that, the transaction
should support the atomicity,

00:20:41.770 --> 00:20:44.440
like whenever you have a
hardware failure or a software

00:20:44.440 --> 00:20:46.620
failure or a transaction
during the transaction

00:20:46.620 --> 00:20:50.470
execution, the result should
be all or nothing.

00:20:50.470 --> 00:20:52.500
There should be no partially
applied or

00:20:52.500 --> 00:20:54.860
inconsistent result.

00:20:54.860 --> 00:20:58.180
Ensure that any transaction will
bring the datastore from

00:20:58.180 --> 00:21:00.860
one valid state or another.

00:21:00.860 --> 00:21:03.580
As I mentioned, every
transaction should be applied

00:21:03.580 --> 00:21:07.800
in a way that there is no
partially applied state.

00:21:07.800 --> 00:21:11.250
Isolation means more like it's
talking about the situation

00:21:11.250 --> 00:21:14.460
where you're getting so many
requests from so many users at

00:21:14.460 --> 00:21:17.750
the same time, like updating one
bank account from multiple

00:21:17.750 --> 00:21:21.900
ATMs, can break isolation
or consistency.

00:21:21.900 --> 00:21:25.800
So your Datastore database has
to prevent that by isolating

00:21:25.800 --> 00:21:27.960
each transaction.

00:21:27.960 --> 00:21:31.210
Durability means you have to use
a hard disk drive to store

00:21:31.210 --> 00:21:34.150
the result of the Datastore
updates.

00:21:34.150 --> 00:21:36.890
If you're storing everything in
memory, you will not have

00:21:36.890 --> 00:21:38.920
any durability.

00:21:38.920 --> 00:21:42.130
So transaction means your
application is robust and

00:21:42.130 --> 00:21:43.125
consistent.

00:21:43.125 --> 00:21:45.940
For simpler applications,
which don't require

00:21:45.940 --> 00:21:48.420
robustness, then you don't
have to take care of

00:21:48.420 --> 00:21:49.840
transactions.

00:21:49.840 --> 00:21:53.040
A hobby application, for
example, you don't have to use

00:21:53.040 --> 00:21:56.830
the transaction Entity Group
I'll be discussing.

00:21:56.830 --> 00:21:59.550
Entity Group is the most
important concept when

00:21:59.550 --> 00:22:01.820
learning Datastore
transactions.

00:22:01.820 --> 00:22:04.840
Because of the name, you may
think that it's related to the

00:22:04.840 --> 00:22:07.880
one to n relationship
between tables in

00:22:07.880 --> 00:22:09.740
their traditional database.

00:22:09.740 --> 00:22:11.160
Actually, it's not.

00:22:11.160 --> 00:22:13.700
It's just a scope
of transaction.

00:22:13.700 --> 00:22:16.440
It doesn't need any entity
relationship we have in the

00:22:16.440 --> 00:22:18.230
traditional database.

00:22:18.230 --> 00:22:20.840
It's a scope of transactions.

00:22:20.840 --> 00:22:25.210
Each entity has its own Entity
Group by default.

00:22:25.210 --> 00:22:28.430
It's just like if you are
person, then you are a family.

00:22:28.430 --> 00:22:29.660
We all are.

00:22:29.660 --> 00:22:31.210
Your family originally
consists of

00:22:31.210 --> 00:22:33.410
just one, just yourself.

00:22:33.410 --> 00:22:35.510
But you can grow your family.

00:22:35.510 --> 00:22:38.200
Each entity has a single
Entity Group.

00:22:38.200 --> 00:22:41.580
So there are five different
Entity Groups here.

00:22:41.580 --> 00:22:44.770
Having a parent and child
relationship between them, it

00:22:44.770 --> 00:22:46.970
can create one Entity Group.

00:22:46.970 --> 00:22:51.615
So this grandpa has so many
children, a successor like dad

00:22:51.615 --> 00:22:53.090
and grandpa.

00:22:53.090 --> 00:22:56.260
This can work out as a scope
of a transaction.

00:22:56.260 --> 00:22:59.620
And transaction's integrity or
consistency can be secured

00:22:59.620 --> 00:23:03.505
with this group, not among the
different Entity Groups.

00:23:03.505 --> 00:23:07.600
A potentially confusing part
is the key of an entity is

00:23:07.600 --> 00:23:10.440
very important part to clear
this Entity Group

00:23:10.440 --> 00:23:11.820
relationship.

00:23:11.820 --> 00:23:15.700
If you're adding more and more
children or successors to an

00:23:15.700 --> 00:23:20.200
entity, the successor would
have an ancestor path.

00:23:20.200 --> 00:23:23.280
This is a totally new thing
that was not used in

00:23:23.280 --> 00:23:25.140
traditional database.

00:23:25.140 --> 00:23:30.960
Every single key has an ID to
identify each entity, but also

00:23:30.960 --> 00:23:32.720
has their ancestor, which

00:23:32.720 --> 00:23:34.640
describes who is the ancestor--

00:23:34.640 --> 00:23:37.190
dad, or grandpa, or yourself.

00:23:37.190 --> 00:23:40.640
Two formal Entity Group
relationships.

00:23:40.640 --> 00:23:43.120
This is somewhat difficult
to understand.

00:23:43.120 --> 00:23:45.160
This is all for transactions.

00:23:45.160 --> 00:23:47.740
This is not a relationship
between the tables in a

00:23:47.740 --> 00:23:49.830
traditional database.

00:23:49.830 --> 00:23:52.910
To create this kind of Entity
Group relationship, you may

00:23:52.910 --> 00:23:55.900
want to create this kind of code
when you're creating a

00:23:55.900 --> 00:23:57.160
new entity.

00:23:57.160 --> 00:24:00.390
You may want to specify the
kind of name and ID of

00:24:00.390 --> 00:24:04.140
yourself and of the entity, and
the parent's key, so that

00:24:04.140 --> 00:24:07.340
it would create the total
whole key like this--

00:24:07.340 --> 00:24:10.480
the key with the
ancestor path.

00:24:10.480 --> 00:24:14.000
To use the transaction, you
can write like this.

00:24:14.000 --> 00:24:16.330
This is very similar code to
the traditional way of

00:24:16.330 --> 00:24:19.350
handling a transaction
with an RDB.

00:24:19.350 --> 00:24:23.050
To tell the Datastore to begin
and after writing everything,

00:24:23.050 --> 00:24:26.120
then you would want to commit
that transaction.

00:24:26.120 --> 00:24:29.410
If an unexpected error or
exception occurs, then you

00:24:29.410 --> 00:24:31.530
would want to roll
back everything.

00:24:31.530 --> 00:24:34.350
This means canceling all the
updates done during the

00:24:34.350 --> 00:24:37.540
transaction, so that you'll
get the atomicity of the

00:24:37.540 --> 00:24:39.560
transaction.

00:24:39.560 --> 00:24:41.850
This is the same as the ordinary
way of controlling

00:24:41.850 --> 00:24:44.560
transactions, but the difference
is that the

00:24:44.560 --> 00:24:47.700
transaction will be rolled back
based on the Entity Group

00:24:47.700 --> 00:24:48.970
relationship.

00:24:48.970 --> 00:24:52.970
If you're updating 1,000 rows
during a transaction, then the

00:24:52.970 --> 00:24:55.020
rollback will be canceling
all the updates

00:24:55.020 --> 00:24:56.870
for the 1,000 rows.

00:24:56.870 --> 00:24:58.940
But in the case of a Datastore,
the rollback will

00:24:58.940 --> 00:25:01.770
be applied only to
the Entity Group.

00:25:01.770 --> 00:25:03.500
That's the difference.

00:25:03.500 --> 00:25:05.880
I'd like to mention an
important new feature

00:25:05.880 --> 00:25:09.730
introduced recently called
Cross-Group Transaction.

00:25:09.730 --> 00:25:13.440
You can have up to five Entity
Groups in a transaction.

00:25:13.440 --> 00:25:17.370
However, remember that all
queries will use eventual, not

00:25:17.370 --> 00:25:19.540
strong consistency.

00:25:19.540 --> 00:25:22.290
If you design an Entity Group,
you should make sure that

00:25:22.290 --> 00:25:25.620
every single Entity Group will
not receive over one update

00:25:25.620 --> 00:25:27.370
transaction per second.

00:25:27.370 --> 00:25:30.380
In practice, it can handle
closer to five per second, but

00:25:30.380 --> 00:25:33.180
this could be a bottleneck
for your application.

00:25:33.180 --> 00:25:36.120
Because the datastore needs to
control the transaction across

00:25:36.120 --> 00:25:40.170
each Entity Group, this means
sometimes we have to maintain

00:25:40.170 --> 00:25:43.800
consistency between
data centers.

00:25:43.800 --> 00:25:47.010
It can be a global
transaction.

00:25:47.010 --> 00:25:49.730
These are some of the
key takeaways--

00:25:49.730 --> 00:25:53.545
how a Datastore works in a
Query, restrictions of Queries

00:25:53.545 --> 00:25:58.040
and Workarounds, and how the
Entity Group is a scope of

00:25:58.040 --> 00:26:00.550
transactions in Datastore.

00:26:00.550 --> 00:26:03.450
Finally, here are some useful
resources to learn more about

00:26:03.450 --> 00:26:06.490
advanced Datastore concepts
discussed during this lesson.

