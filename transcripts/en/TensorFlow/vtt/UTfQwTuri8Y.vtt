WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.744
[MUSIC PLAYING]

00:00:05.480 --> 00:00:07.480
NICHOLAS THOMPSON: Hello,
I'm Nicholas Thompson.

00:00:07.480 --> 00:00:09.060
I'm the editor in
chief of "Wired."

00:00:09.060 --> 00:00:11.400
It is my honor today
to get the chance

00:00:11.400 --> 00:00:13.103
to interview Geoffrey Hinton.

00:00:13.103 --> 00:00:14.520
They're a couple--
well, there are

00:00:14.520 --> 00:00:16.260
many things I love about him.

00:00:16.260 --> 00:00:18.870
But two that I'll just
mention in the introduction.

00:00:18.870 --> 00:00:21.730
The first is that he persisted.

00:00:21.730 --> 00:00:24.450
He had an idea that
he really believed in

00:00:24.450 --> 00:00:26.760
that everybody
else said was bad.

00:00:26.760 --> 00:00:28.172
And he just kept at it.

00:00:28.172 --> 00:00:30.630
And it gives a lot of faith to
everybody who has bad ideas,

00:00:30.630 --> 00:00:32.070
myself included.

00:00:32.070 --> 00:00:34.590
Then the second, as someone
who spends half his life

00:00:34.590 --> 00:00:36.798
as a manager
adjudicating job titles,

00:00:36.798 --> 00:00:39.090
I was looking at his job
title before the introduction.

00:00:39.090 --> 00:00:41.760
And he has the most
non pretentious job

00:00:41.760 --> 00:00:43.080
title in history.

00:00:43.080 --> 00:00:46.440
So please welcome Geoffrey
Hinton, the engineering fellow

00:00:46.440 --> 00:00:47.454
at Google.

00:00:47.454 --> 00:00:49.350
[APPLAUSE]

00:00:55.490 --> 00:00:55.990
Welcome.

00:00:55.990 --> 00:00:56.560
GEOFFREY HINTON: Thank you.

00:00:56.560 --> 00:00:58.518
NICHOLAS THOMPSON: So
nice to be here with you.

00:00:58.518 --> 00:01:00.780
All right, so let us start.

00:01:00.780 --> 00:01:06.400
20 years ago when you write some
of your early very influential

00:01:06.400 --> 00:01:09.822
papers, everybody starts
to say, it's a smart idea,

00:01:09.822 --> 00:01:12.280
but we're not actually going
to be able to design computers

00:01:12.280 --> 00:01:13.450
this way.

00:01:13.450 --> 00:01:17.230
Explain why you persisted, why
you were so confident that you

00:01:17.230 --> 00:01:19.710
had found something important.

00:01:19.710 --> 00:01:23.450
GEOFFREY HINTON: So actually
it was 40 years ago.

00:01:23.450 --> 00:01:27.080
And it seemed to me there's no
other way the brain could work.

00:01:27.080 --> 00:01:31.200
It has to work by learning
the strengths of connections.

00:01:31.200 --> 00:01:34.142
And if you want to make a
device do something intelligent,

00:01:34.142 --> 00:01:35.100
you've got two options.

00:01:35.100 --> 00:01:37.280
You can program it,
or it can learn.

00:01:37.280 --> 00:01:39.770
And we certainly
weren't programmed.

00:01:39.770 --> 00:01:40.800
So we had to learn.

00:01:40.800 --> 00:01:42.915
So this had to be
the right way to go.

00:01:42.915 --> 00:01:44.540
NICHOLAS THOMPSON:
So explain, though--

00:01:44.540 --> 00:01:45.470
well, let's do this.

00:01:45.470 --> 00:01:46.700
Explain what neural
networks are.

00:01:46.700 --> 00:01:48.658
Most of the people here
will be quite familiar.

00:01:48.658 --> 00:01:51.440
But explain the
original insight and how

00:01:51.440 --> 00:01:52.575
it developed in your mind.

00:01:52.575 --> 00:01:54.950
GEOFFREY HINTON: So you have
relatively simple processing

00:01:54.950 --> 00:01:58.370
elements that are very
loosely models of neurons.

00:01:58.370 --> 00:02:00.230
They have connections coming in.

00:02:00.230 --> 00:02:02.260
Each connection
has a weight on it.

00:02:02.260 --> 00:02:04.340
That weight can be
changed to do learning.

00:02:04.340 --> 00:02:08.000
And what a neuron does
is take the activities

00:02:08.000 --> 00:02:10.940
on the connections times the
weights, adds them all up,

00:02:10.940 --> 00:02:13.680
and then decides whether
to send an output.

00:02:13.680 --> 00:02:16.320
And if it gets a big enough
sum, it sends an output.

00:02:16.320 --> 00:02:18.860
If the sum is negative,
it doesn't send anything.

00:02:18.860 --> 00:02:20.520
That's about it.

00:02:20.520 --> 00:02:22.310
And all you have to
do is just wire up

00:02:22.310 --> 00:02:26.690
a gazillion of those with
a gazillion squared weights

00:02:26.690 --> 00:02:29.110
and just figure out how
to change the weights,

00:02:29.110 --> 00:02:30.300
and it'll do anything.

00:02:30.300 --> 00:02:32.610
It's just a question of
how you change the weights.

00:02:32.610 --> 00:02:34.610
NICHOLAS THOMPSON:
So when did you

00:02:34.610 --> 00:02:38.510
come to understand that this was
an approximate representation

00:02:38.510 --> 00:02:39.723
of how the brain works?

00:02:39.723 --> 00:02:41.890
GEOFFREY HINTON: Oh, it was
always designed as that.

00:02:41.890 --> 00:02:42.220
NICHOLAS THOMPSON: Right.

00:02:42.220 --> 00:02:44.200
GEOFFREY HINTON: It was designed
to be like how the brain works.

00:02:44.200 --> 00:02:45.230
NICHOLAS THOMPSON: But
let me ask you this.

00:02:45.230 --> 00:02:46.810
So at some point
in your career, you

00:02:46.810 --> 00:02:48.100
start to understand
how the brain works.

00:02:48.100 --> 00:02:49.350
Maybe it was when you were 12.

00:02:49.350 --> 00:02:50.730
Maybe it was when you were 25.

00:02:50.730 --> 00:02:52.630
When do you make the
decision that you

00:02:52.630 --> 00:02:55.270
will try to model
computers after the brain?

00:02:55.270 --> 00:02:56.770
GEOFFREY HINTON:
Sort of right away.

00:02:56.770 --> 00:02:59.020
That was the whole point of it.

00:02:59.020 --> 00:03:01.480
The whole idea was to have
a learning device that

00:03:01.480 --> 00:03:03.437
learned like the
brain like people

00:03:03.437 --> 00:03:05.770
think the brain learns by
changing connection strengths.

00:03:05.770 --> 00:03:07.540
And this wasn't my idea.

00:03:07.540 --> 00:03:08.800
Turing had the same.

00:03:08.800 --> 00:03:12.130
Turing, even though
he invented a lot

00:03:12.130 --> 00:03:15.010
of the basis of standard
computer science,

00:03:15.010 --> 00:03:17.350
he believed that the brain
was this unorganized device

00:03:17.350 --> 00:03:18.760
with random weights.

00:03:18.760 --> 00:03:21.190
And it would use
reinforcement learning

00:03:21.190 --> 00:03:22.468
to change the connections.

00:03:22.468 --> 00:03:24.010
And it would learn
everything, and he

00:03:24.010 --> 00:03:26.170
thought that was the best
route to intelligence.

00:03:26.170 --> 00:03:28.240
NICHOLAS THOMPSON: And so you
were following Turing's idea

00:03:28.240 --> 00:03:30.490
that the best way to make
a machine is to model it

00:03:30.490 --> 00:03:31.627
after the human brain.

00:03:31.627 --> 00:03:32.960
This is how a human brain works.

00:03:32.960 --> 00:03:34.170
So let's make a
machine like that.

00:03:34.170 --> 00:03:35.290
GEOFFREY HINTON: Yeah, it
wasn't just Turing's idea.

00:03:35.290 --> 00:03:36.580
Lots of people thought
that back then.

00:03:36.580 --> 00:03:37.600
NICHOLAS THOMPSON: All
right, so you have this idea.

00:03:37.600 --> 00:03:39.550
Lots of people have this idea.

00:03:39.550 --> 00:03:41.980
You get a lot of credit.

00:03:41.980 --> 00:03:43.630
In the late '80s,
you start to come

00:03:43.630 --> 00:03:45.850
to fame with your published
work, is that correct?

00:03:45.850 --> 00:03:46.480
GEOFFREY HINTON: Yes.

00:03:46.480 --> 00:03:48.397
NICHOLAS THOMPSON: When
is the darkest moment.

00:03:48.397 --> 00:03:50.600
When is the moment
where other people who

00:03:50.600 --> 00:03:53.140
have been working who agreed
with this idea from Turing

00:03:53.140 --> 00:03:57.453
start to back away and yet
you continue to plunge ahead?

00:03:57.453 --> 00:03:58.870
GEOFFREY HINTON:
There were always

00:03:58.870 --> 00:04:01.420
a bunch of people who kept
believing in it, particularly

00:04:01.420 --> 00:04:02.710
in psychology.

00:04:02.710 --> 00:04:05.380
But among computer
scientists, I guess

00:04:05.380 --> 00:04:09.920
in the '90s, what happened was
data sets were quite small.

00:04:09.920 --> 00:04:12.010
And computers weren't that fast.

00:04:12.010 --> 00:04:15.220
And on small data sets,
other methods like things

00:04:15.220 --> 00:04:18.700
called support vector machines,
worked a little bit better.

00:04:18.700 --> 00:04:21.125
They didn't get confused
by noise so much.

00:04:21.125 --> 00:04:23.500
And so that was very depressing
because we developed back

00:04:23.500 --> 00:04:25.090
propagation in the '80s.

00:04:25.090 --> 00:04:26.650
We thought it would
solve everything.

00:04:26.650 --> 00:04:30.580
And we were a bit puzzled about
why it didn't solve everything.

00:04:30.580 --> 00:04:32.080
And it was just a
question of scale.

00:04:32.080 --> 00:04:33.960
But we didn't really
know that then.

00:04:33.960 --> 00:04:35.335
NICHOLAS THOMPSON:
And so why did

00:04:35.335 --> 00:04:37.210
you think it was not working?

00:04:37.210 --> 00:04:37.900
GEOFFREY HINTON: We
thought it was not

00:04:37.900 --> 00:04:40.000
working because we didn't have
quite the right algorithms.

00:04:40.000 --> 00:04:42.040
We didn't have quite the
right objective functions.

00:04:42.040 --> 00:04:43.748
I thought for a long
time it's because we

00:04:43.748 --> 00:04:45.910
were trying to do
supervised learning

00:04:45.910 --> 00:04:47.380
where you have to label data.

00:04:47.380 --> 00:04:49.797
And we should have been doing
unsupervised learning, where

00:04:49.797 --> 00:04:52.450
you just learn from the
data with no labels.

00:04:52.450 --> 00:04:54.510
It turned out it was
mainly a question of scale.

00:04:54.510 --> 00:04:55.210
NICHOLAS THOMPSON: Oh,
that's interesting.

00:04:55.210 --> 00:04:57.505
So the problem was you
didn't have enough data.

00:04:57.505 --> 00:04:59.380
You thought you had the
right amount of data,

00:04:59.380 --> 00:05:00.880
but you hadn't
labeled it correctly.

00:05:00.880 --> 00:05:02.980
So you just misidentified
the problem?

00:05:02.980 --> 00:05:05.350
GEOFFREY HINTON: I thought
that using labels at all

00:05:05.350 --> 00:05:05.980
was a mistake.

00:05:05.980 --> 00:05:07.397
You would do most
of your learning

00:05:07.397 --> 00:05:09.280
without making any
use of labels just

00:05:09.280 --> 00:05:11.290
by trying to model the
structure in the data.

00:05:11.290 --> 00:05:13.060
I actually still believe that.

00:05:13.060 --> 00:05:16.590
I think as computers get faster,
for any given size data set,

00:05:16.590 --> 00:05:18.280
if you make computers
fast enough,

00:05:18.280 --> 00:05:20.800
you're better off doing
unsupervised learning.

00:05:20.800 --> 00:05:23.210
And once you've done the
unsupervised learning,

00:05:23.210 --> 00:05:25.430
you'll be able to learn
from fewer labels.

00:05:25.430 --> 00:05:26.350
NICHOLAS THOMPSON:
So in the 1990s,

00:05:26.350 --> 00:05:27.660
you're continuing
with your research.

00:05:27.660 --> 00:05:28.510
You're in academia.

00:05:28.510 --> 00:05:31.480
You are still publishing, but
it's not coming to a claim.

00:05:31.480 --> 00:05:33.310
You aren't solving big problems.

00:05:33.310 --> 00:05:35.195
When do you start--

00:05:35.195 --> 00:05:36.820
well, actually, was
there ever a moment

00:05:36.820 --> 00:05:39.990
where you said, you know
what, enough of this.

00:05:39.990 --> 00:05:41.643
I'm going to go
try something else?

00:05:41.643 --> 00:05:42.810
GEOFFREY HINTON: Not really.

00:05:42.810 --> 00:05:45.060
NICHOLAS THOMPSON: Not that
I'm going to go sell burgers,

00:05:45.060 --> 00:05:46.860
but I'm going to figure out a
different way of doing this.

00:05:46.860 --> 00:05:49.020
You just said we're going
to keep doing deep learning.

00:05:49.020 --> 00:05:50.670
GEOFFREY HINTON: Yes, something
like this has to work.

00:05:50.670 --> 00:05:53.200
I mean, the connections in the
brain are learning somehow.

00:05:53.200 --> 00:05:55.080
And we just have
to figure it out.

00:05:55.080 --> 00:05:58.285
And probably there's a bunch
of different ways of learning

00:05:58.285 --> 00:05:59.160
connection strengths.

00:05:59.160 --> 00:06:00.480
The brains using one of them.

00:06:00.480 --> 00:06:01.980
There may be other
ways of doing it.

00:06:01.980 --> 00:06:03.750
But certainly, you have to
have something that can learn

00:06:03.750 --> 00:06:05.330
these connection strengths.

00:06:05.330 --> 00:06:06.705
And I never doubted that.

00:06:06.705 --> 00:06:08.580
NICHOLAS THOMPSON: OK,
so you never doubt it.

00:06:08.580 --> 00:06:12.030
When does it first start
to seem like it's working?

00:06:12.030 --> 00:06:13.693
OK, you know, we've got this.

00:06:13.693 --> 00:06:16.110
I believe in this idea, and
actually, if you look at that,

00:06:16.110 --> 00:06:17.970
if you squint, you
can see it's working.

00:06:17.970 --> 00:06:19.230
When did that happen?

00:06:19.230 --> 00:06:21.480
GEOFFREY HINTON: OK, so one
of the big disappointments

00:06:21.480 --> 00:06:23.130
in the '80s was if
you made networks

00:06:23.130 --> 00:06:26.498
with lots of hidden layers,
you couldn't train them.

00:06:26.498 --> 00:06:29.040
That's not quite true because
convolutional networks designed

00:06:29.040 --> 00:06:32.010
by Yann LeCun, you could
train for fairly simple tasks

00:06:32.010 --> 00:06:36.040
like recognizing handwriting.

00:06:36.040 --> 00:06:40.390
But most of the deep nets, we
didn't know how to train them.

00:06:40.390 --> 00:06:44.290
And in about 2005,
I came up with a way

00:06:44.290 --> 00:06:47.770
of doing unsupervised
training of deep nets.

00:06:47.770 --> 00:06:50.197
So you take your
input, say your pixels,

00:06:50.197 --> 00:06:52.030
and you'd learn a bunch
of feature detectors

00:06:52.030 --> 00:06:54.860
so that were just good at
explaining why the pixels were

00:06:54.860 --> 00:06:56.500
behaving like that.

00:06:56.500 --> 00:06:59.140
And then you treat those
feature detectors as the data

00:06:59.140 --> 00:07:00.940
and then you learn another
bunch of feature detectors.

00:07:00.940 --> 00:07:03.148
So we got to explain why
those feature detectors have

00:07:03.148 --> 00:07:04.280
those correlations.

00:07:04.280 --> 00:07:06.010
And you keep learning
less and less.

00:07:06.010 --> 00:07:09.070
And what was interesting
was you could do some math

00:07:09.070 --> 00:07:13.588
and prove that each time
you learned another layer,

00:07:13.588 --> 00:07:15.880
you didn't necessarily have
a better model of the data,

00:07:15.880 --> 00:07:18.102
but you had a band on
how good your model was.

00:07:18.102 --> 00:07:19.810
And you could get a
better band each time

00:07:19.810 --> 00:07:20.780
you added another layer.

00:07:20.780 --> 00:07:21.850
NICHOLAS THOMPSON: What do you
mean you had a band on how good

00:07:21.850 --> 00:07:23.110
your model was?

00:07:23.110 --> 00:07:26.380
GEOFFREY HINTON: OK, so
once you got a model,

00:07:26.380 --> 00:07:28.785
you can say how surprising
does a model find this data?

00:07:28.785 --> 00:07:30.160
You showed some
data and you say,

00:07:30.160 --> 00:07:32.077
is that the kind of thing
you believe in or is

00:07:32.077 --> 00:07:32.890
that surprising?

00:07:32.890 --> 00:07:35.410
And you can sort of measure
something that says that.

00:07:35.410 --> 00:07:38.215
And what you'd like
to do is have a model,

00:07:38.215 --> 00:07:40.590
a good model is one that looks
at the data and says yeah,

00:07:40.590 --> 00:07:41.530
I knew that.

00:07:41.530 --> 00:07:43.720
It's unsurprising, OK?

00:07:43.720 --> 00:07:46.630
And it's often very
hard to compute

00:07:46.630 --> 00:07:49.210
exactly how surprising
this model finds the data.

00:07:49.210 --> 00:07:50.860
But you can compute
a band on that.

00:07:50.860 --> 00:07:57.130
You can say this model finds the
data less surprising than this.

00:07:57.130 --> 00:08:00.980
And you could show that, as
you add extra layers of feature

00:08:00.980 --> 00:08:03.820
detectors, you get a model.

00:08:03.820 --> 00:08:05.560
And each time you
add a layer, it

00:08:05.560 --> 00:08:08.050
finds the data, the band
on how surprising it

00:08:08.050 --> 00:08:09.550
finds the data gets better.

00:08:09.550 --> 00:08:10.080
NICHOLAS THOMPSON: Oh, I see.

00:08:10.080 --> 00:08:11.080
OK, so that makes sense.

00:08:11.080 --> 00:08:13.570
So you're making observations,
and they're not correct.

00:08:13.570 --> 00:08:15.400
But you know they're closer
and closer to being correct.

00:08:15.400 --> 00:08:16.567
I'm looking at the audience.

00:08:16.567 --> 00:08:17.960
I'm making some generalization.

00:08:17.960 --> 00:08:20.085
It's not correct, but I'm
getting better and better

00:08:20.085 --> 00:08:21.078
at it, roughly?

00:08:21.078 --> 00:08:22.120
GEOFFREY HINTON: Roughly.

00:08:22.120 --> 00:08:23.980
NICHOLAS THOMPSON: OK,
so that's about 2005

00:08:23.980 --> 00:08:26.160
where you come up with that
mathematical breakthrough?

00:08:26.160 --> 00:08:26.560
GEOFFREY HINTON: Yeah.

00:08:26.560 --> 00:08:28.120
NICHOLAS THOMPSON: When do
you start getting answers

00:08:28.120 --> 00:08:30.670
that are correct and what
data are you working on?

00:08:30.670 --> 00:08:32.500
This is speech data where you
first have your break through.

00:08:32.500 --> 00:08:34.583
GEOFFREY HINTON: This was
just handwritten digits.

00:08:34.583 --> 00:08:35.530
Very simple data.

00:08:35.530 --> 00:08:40.120
Then around the same time,
they started developing GPUs.

00:08:40.120 --> 00:08:41.690
And the people doing
neural networks

00:08:41.690 --> 00:08:47.310
started using GPUs
in about 2007.

00:08:47.310 --> 00:08:49.050
I had one very
good student called

00:08:49.050 --> 00:08:53.130
Vlad Mnih, who started
using GPUs for finding roads

00:08:53.130 --> 00:08:54.960
in aerial images.

00:08:54.960 --> 00:08:58.320
He wrote some code that was
then used by other students

00:08:58.320 --> 00:09:03.770
for using GPUs to recognize
phonemes in speech.

00:09:03.770 --> 00:09:07.080
And so they were using
this idea pre-training.

00:09:07.080 --> 00:09:09.210
And after they'd done
all this pre-training,

00:09:09.210 --> 00:09:12.480
then they'd just stick labels
on top and use back propagation.

00:09:12.480 --> 00:09:15.880
And it turned out that way,
you could have a very deep net

00:09:15.880 --> 00:09:17.730
that was pre-trained this way.

00:09:17.730 --> 00:09:19.470
And you could then
use back propagation,

00:09:19.470 --> 00:09:21.000
and it actually worked.

00:09:21.000 --> 00:09:22.860
And it sort of
beat the benchmarks

00:09:22.860 --> 00:09:25.573
for speech recognition
initially just by a little bit.

00:09:25.573 --> 00:09:27.990
NICHOLAS THOMPSON: It beat the
best commercially available

00:09:27.990 --> 00:09:28.782
speech recognition.

00:09:28.782 --> 00:09:31.868
It beat the best academic
work on speech recognition.

00:09:31.868 --> 00:09:34.410
GEOFFREY HINTON: On a relatively
small data set called TIMIT,

00:09:34.410 --> 00:09:38.150
it did slightly better than
the best academic work.

00:09:38.150 --> 00:09:42.570
It also worked on at IBM.

00:09:42.570 --> 00:09:48.510
And very quickly people
realized that this stuff,

00:09:48.510 --> 00:09:51.285
since it was beating
standard models that

00:09:51.285 --> 00:09:54.030
are taking 30 years to develop
with a bit more development

00:09:54.030 --> 00:09:55.540
would do really well.

00:09:55.540 --> 00:09:59.670
And so my graduate students
went off to Microsoft and IBM

00:09:59.670 --> 00:10:01.140
and Google.

00:10:01.140 --> 00:10:03.930
And Google was the fastest
to turn it into a production

00:10:03.930 --> 00:10:05.150
speech recognizer.

00:10:05.150 --> 00:10:10.470
And by 2012, that work
that was first done in 2009

00:10:10.470 --> 00:10:11.952
came out in Android.

00:10:11.952 --> 00:10:14.410
And Android suddenly got much
better in speech recognition.

00:10:14.410 --> 00:10:16.910
NICHOLAS THOMPSON: So tell me
about that moment where you've

00:10:16.910 --> 00:10:19.230
had this idea for
40 years, you've

00:10:19.230 --> 00:10:21.360
been publishing on
it for 20 years,

00:10:21.360 --> 00:10:24.360
and you're finally better
than your colleagues?

00:10:24.360 --> 00:10:25.360
What did that feel like?

00:10:25.360 --> 00:10:26.860
GEOFFREY HINTON:
Well, back then I'd

00:10:26.860 --> 00:10:28.650
only had the idea for 30 years.

00:10:28.650 --> 00:10:30.810
NICHOLAS THOMPSON: Correct,
correct, sorry, sir.

00:10:30.810 --> 00:10:31.970
Just a new idea.

00:10:31.970 --> 00:10:32.880
It's fresh.

00:10:32.880 --> 00:10:34.380
GEOFFREY HINTON:
It felt really good

00:10:34.380 --> 00:10:37.630
that it finally got the state
of the art on a real problem.

00:10:37.630 --> 00:10:39.213
NICHOLAS THOMPSON:
And do you remember

00:10:39.213 --> 00:10:42.307
where you were when you first
got the revelatory data?

00:10:42.307 --> 00:10:43.140
GEOFFREY HINTON: No.

00:10:43.140 --> 00:10:45.060
NICHOLAS THOMPSON: No, no, OK.

00:10:45.060 --> 00:10:49.110
All right, so you realize it
works on speech recognition.

00:10:49.110 --> 00:10:51.195
When do you start applying
it to other problems?

00:10:51.195 --> 00:10:53.070
GEOFFREY HINTON: So then
we start applying it

00:10:53.070 --> 00:10:54.700
to all sorts of other problems.

00:10:54.700 --> 00:10:57.330
So George Dahl, who was
one of the people who

00:10:57.330 --> 00:11:00.830
did the original work on speech
recognition, applied it to--

00:11:00.830 --> 00:11:03.330
I give you a lot of
descriptors of a molecule

00:11:03.330 --> 00:11:07.020
and you want to predict if that
molecule will bind to something

00:11:07.020 --> 00:11:08.830
to act as a good drug.

00:11:08.830 --> 00:11:10.770
And there was a
competition on Kaggle.

00:11:10.770 --> 00:11:13.080
And he just applied our
standard technology design

00:11:13.080 --> 00:11:15.600
for speech recognition
to predicting

00:11:15.600 --> 00:11:18.660
the activity of drugs and
it won the competition.

00:11:18.660 --> 00:11:22.020
So that was a sign that this
stuff sort of fairly universal.

00:11:22.020 --> 00:11:26.760
And then I had a student
called [INAUDIBLE],, who said,

00:11:26.760 --> 00:11:28.260
you know, Geoff,
this stuff is going

00:11:28.260 --> 00:11:29.790
to work for image recognition.

00:11:29.790 --> 00:11:33.400
And Fei-Fei Li has created
the correct data set for it,

00:11:33.400 --> 00:11:34.650
and it's a public competition.

00:11:34.650 --> 00:11:37.000
We have to do that.

00:11:37.000 --> 00:11:40.800
And so what we did was take an
approach originally developed

00:11:40.800 --> 00:11:43.950
by Yann LeCun.

00:11:43.950 --> 00:11:46.410
A student called Alex
Krizhevsky was a real wizard.

00:11:46.410 --> 00:11:48.660
He could make GPUs do anything.

00:11:48.660 --> 00:11:51.720
Programmed the GPUs
really, really well.

00:11:51.720 --> 00:11:55.480
And we got results
that were a lot better

00:11:55.480 --> 00:11:56.730
than standard computer vision.

00:11:56.730 --> 00:11:58.190
That was 2012.

00:11:58.190 --> 00:12:00.987
And it was a coincidence I
think of the speech recognition

00:12:00.987 --> 00:12:02.070
coming out in the Android.

00:12:02.070 --> 00:12:05.160
So you knew this stuff could
solve production problems.

00:12:05.160 --> 00:12:09.240
And on vision in 2012,
it had done much better

00:12:09.240 --> 00:12:11.260
than the standard
computer vision.

00:12:11.260 --> 00:12:13.885
NICHOLAS THOMPSON: So those are
three areas where it succeeded.

00:12:13.885 --> 00:12:18.665
So modeling chemicals, speech,
voice, where was it failing?

00:12:18.665 --> 00:12:21.290
GEOFFREY HINTON: The failure is
only temporary, you understand.

00:12:21.290 --> 00:12:23.010
[LAUGHTER]

00:12:23.010 --> 00:12:25.702
NICHOLAS THOMPSON:
Where was it failing?

00:12:25.702 --> 00:12:27.910
GEOFFREY HINTON: For things
like machine translation,

00:12:27.910 --> 00:12:30.160
I thought it would be a very
long time before we could

00:12:30.160 --> 00:12:32.410
do that because
machine translation,

00:12:32.410 --> 00:12:34.970
you've got a string
of symbols comes in

00:12:34.970 --> 00:12:36.700
and a string of
symbols goes out.

00:12:36.700 --> 00:12:39.970
And it's fairly plausible to say
in between you do manipulations

00:12:39.970 --> 00:12:44.020
on strings of symbols, which
is what classical AI is.

00:12:44.020 --> 00:12:45.600
Actually, it doesn't
work like that.

00:12:45.600 --> 00:12:47.083
Strings and symbols come in.

00:12:47.083 --> 00:12:49.250
You turn those into great
big vectors in your brain.

00:12:49.250 --> 00:12:51.190
These vectors interact
with each other.

00:12:51.190 --> 00:12:53.398
And then you convert it back
into strings and symbols

00:12:53.398 --> 00:12:54.650
to go out.

00:12:54.650 --> 00:12:59.770
And if you told me in 2012
that in the next five years,

00:12:59.770 --> 00:13:04.200
we'll be able to translate
between many languages using

00:13:04.200 --> 00:13:07.110
just the same technology,
recurrent nets,

00:13:07.110 --> 00:13:10.960
but just the stochastic
gradient descent

00:13:10.960 --> 00:13:13.570
from random initial weights,
I wouldn't have believed you.

00:13:13.570 --> 00:13:15.577
It happened much
faster than expected.

00:13:15.577 --> 00:13:17.410
NICHOLAS THOMPSON: But
so what distinguishes

00:13:17.410 --> 00:13:22.030
the areas where it
works the most quickly

00:13:22.030 --> 00:13:24.350
and the areas where it
will take more time?

00:13:24.350 --> 00:13:27.440
It seems like the visual
processing, speech recognition,

00:13:27.440 --> 00:13:30.190
sort of core human
things that we

00:13:30.190 --> 00:13:33.460
do with our sensory perception
seem to be the first barriers

00:13:33.460 --> 00:13:34.360
to clear.

00:13:34.360 --> 00:13:35.560
Is that correct?

00:13:35.560 --> 00:13:37.060
GEOFFREY HINTON: Yes and no
because there's other things

00:13:37.060 --> 00:13:38.170
we do like motor control.

00:13:38.170 --> 00:13:39.545
We're very good
at motor control.

00:13:39.545 --> 00:13:41.530
Our brains are clearly
designed for that.

00:13:41.530 --> 00:13:44.920
And that's only just now
a neuron net's beginning

00:13:44.920 --> 00:13:48.010
to compete with the best
other technologies there.

00:13:48.010 --> 00:13:49.580
They will win in the end.

00:13:49.580 --> 00:13:52.780
But they're only
just winning now.

00:13:52.780 --> 00:13:57.790
I think things like
reasoning, abstract reasoning,

00:13:57.790 --> 00:14:00.107
they're the kind of last
things we learn to do.

00:14:00.107 --> 00:14:01.940
And I think they'll be
among the last things

00:14:01.940 --> 00:14:03.440
these neural nets learn to do.

00:14:03.440 --> 00:14:04.857
NICHOLAS THOMPSON:
And so you keep

00:14:04.857 --> 00:14:08.888
saying that neural nets will
win at everything eventually.

00:14:08.888 --> 00:14:10.930
GEOFFREY HINTON: Well, we
are neural nets, right?

00:14:10.930 --> 00:14:12.310
Anything we can do they can do.

00:14:12.310 --> 00:14:13.727
NICHOLAS THOMPSON:
Right, but just

00:14:13.727 --> 00:14:19.070
because the human brain is not
necessarily the most efficient

00:14:19.070 --> 00:14:21.180
computational
machine ever created.

00:14:21.180 --> 00:14:21.725
GEOFFREY HINTON:
Almost certainly not.

00:14:21.725 --> 00:14:22.900
NICHOLAS THOMPSON: So
why could there not be--

00:14:22.900 --> 00:14:24.590
certainly not my human brain.

00:14:24.590 --> 00:14:27.317
Couldn't there be a way
of modeling machines

00:14:27.317 --> 00:14:29.150
that is more efficient
than the human brain?

00:14:29.150 --> 00:14:30.470
GEOFFREY HINTON:
Philosophically, I

00:14:30.470 --> 00:14:32.780
have no objection to the idea
there could be some completely

00:14:32.780 --> 00:14:33.988
different way to do all this.

00:14:33.988 --> 00:14:37.040
It could be that if
you start with logic

00:14:37.040 --> 00:14:38.840
and you're trying
to automate logic,

00:14:38.840 --> 00:14:42.550
and you make some really
fancy theorem prover,

00:14:42.550 --> 00:14:44.690
and you do reasoning,
and then you decide

00:14:44.690 --> 00:14:47.302
you're going to do visual
perception by doing reasoning,

00:14:47.302 --> 00:14:49.010
it could be that that
approach would win.

00:14:49.010 --> 00:14:50.490
It turned out it didn't.

00:14:50.490 --> 00:14:53.210
But I have no philosophical
objection to that winning.

00:14:53.210 --> 00:14:56.480
It's just we know
that brains can do it.

00:14:56.480 --> 00:14:59.090
NICHOLAS THOMPSON: Right,
but there are also things

00:14:59.090 --> 00:15:01.850
that our brains can't do well.

00:15:01.850 --> 00:15:03.500
Are those things
that neural nets also

00:15:03.500 --> 00:15:05.978
won't be able to do well?

00:15:05.978 --> 00:15:07.520
GEOFFREY HINTON:
Quite possibly, yes.

00:15:07.520 --> 00:15:08.978
NICHOLAS THOMPSON:
And then there's

00:15:08.978 --> 00:15:12.050
a separate problem, which
is we don't know entirely

00:15:12.050 --> 00:15:13.550
how these things work, right?

00:15:13.550 --> 00:15:15.883
GEOFFREY HINTON: No, we really
don't know how they work.

00:15:15.883 --> 00:15:18.547
NICHOLAS THOMPSON: We don't
understand how top down neural

00:15:18.547 --> 00:15:19.130
networks work.

00:15:19.130 --> 00:15:20.600
There is even a
core element of how

00:15:20.600 --> 00:15:23.390
neural networks work that
we don't understand, right?

00:15:23.390 --> 00:15:23.550
GEOFFREY HINTON: Yes.

00:15:23.550 --> 00:15:24.300
NICHOLAS THOMPSON:
So explain that

00:15:24.300 --> 00:15:26.153
and then let me ask
the obvious follow up,

00:15:26.153 --> 00:15:28.070
which is, we don't know
how these things work.

00:15:28.070 --> 00:15:29.153
How can those things work?

00:15:29.153 --> 00:15:32.860
GEOFFREY HINTON: OK, you ask
that when I finish explaining.

00:15:32.860 --> 00:15:34.150
NICHOLAS THOMPSON: Yes.

00:15:34.150 --> 00:15:38.710
GEOFFREY HINTON: So if you
look at current computer vision

00:15:38.710 --> 00:15:42.320
systems, most of them, they're
basically feed forward.

00:15:42.320 --> 00:15:43.833
They don't use
feedback connections.

00:15:43.833 --> 00:15:46.000
There's something else about
current computer vision

00:15:46.000 --> 00:15:47.833
systems, which is they're
very prone to have

00:15:47.833 --> 00:15:49.420
visceral examples.

00:15:49.420 --> 00:15:53.860
You can change a
few pixels slightly

00:15:53.860 --> 00:15:56.380
and something that was
a picture of a panda

00:15:56.380 --> 00:15:58.710
and still looks exactly
like a panda to you,

00:15:58.710 --> 00:16:00.813
it suddenly says
that's an ostrich.

00:16:00.813 --> 00:16:02.980
Obviously, the way you
change the pixels is cleverly

00:16:02.980 --> 00:16:06.640
designed to fool it into
thinking it's an ostrich.

00:16:06.640 --> 00:16:09.490
But the point is it still
looks just like a panda to you.

00:16:09.490 --> 00:16:11.837
And initially, we thought
these things work really well.

00:16:11.837 --> 00:16:13.420
But then when
confronted with the fact

00:16:13.420 --> 00:16:16.920
that they look at a panda and
be confident it's an ostrich,

00:16:16.920 --> 00:16:18.600
you get a bit worried.

00:16:18.600 --> 00:16:20.500
And I think part of
the problem there

00:16:20.500 --> 00:16:23.710
is that they're not trying to
reconstruct from the high level

00:16:23.710 --> 00:16:24.737
representations.

00:16:24.737 --> 00:16:27.070
They're trying to do descriptive
learning where you just

00:16:27.070 --> 00:16:28.990
learn layers of
feature detectors

00:16:28.990 --> 00:16:33.350
and the whole, whole objective
is just to change the weights.

00:16:33.350 --> 00:16:35.830
So you get better at
getting the right answer.

00:16:35.830 --> 00:16:38.770
They're not doing things
like at each level of feature

00:16:38.770 --> 00:16:41.293
detectors, check that
you can reconstruct

00:16:41.293 --> 00:16:43.960
the data in the layer below from
the activities of these feature

00:16:43.960 --> 00:16:45.270
detectors.

00:16:45.270 --> 00:16:47.835
And recently in Toronto,
we've been discovering,

00:16:47.835 --> 00:16:49.210
or Nick Frost's
been discovering,

00:16:49.210 --> 00:16:54.400
that if you introduce
reconstruction then

00:16:54.400 --> 00:16:57.520
it helps you be more resistant
to adversarial attack.

00:16:57.520 --> 00:17:00.340
So I think in
human vision, to do

00:17:00.340 --> 00:17:02.530
the learning we do in
reconstruction and also

00:17:02.530 --> 00:17:04.599
because we're doing
a lot of learning

00:17:04.599 --> 00:17:07.162
by doing reconstructions,
we are much more resistant

00:17:07.162 --> 00:17:08.079
to adversarial attack.

00:17:08.079 --> 00:17:09.496
NICHOLAS THOMPSON:
But you believe

00:17:09.496 --> 00:17:12.880
that top down communication
in a neural network

00:17:12.880 --> 00:17:15.099
is how you test,
how you reconstruct,

00:17:15.099 --> 00:17:18.085
how you test and make sure
it's a panda not an ostrich?

00:17:18.085 --> 00:17:19.960
GEOFFREY HINTON: I think
that's crucial, yes.

00:17:19.960 --> 00:17:20.819
Because I think if you--

00:17:20.819 --> 00:17:22.444
NICHOLAS THOMPSON:
But brain scientists

00:17:22.444 --> 00:17:24.515
are not entirely agreed
on that, correct?

00:17:24.515 --> 00:17:25.890
GEOFFREY HINTON:
Brain scientists

00:17:25.890 --> 00:17:27.307
all agreed on the
idea that if you

00:17:27.307 --> 00:17:31.050
have two areas of the cortex
in a perceptual pathway,

00:17:31.050 --> 00:17:33.320
if there's connections
from one to the other,

00:17:33.320 --> 00:17:36.065
they'll always be backwards
connections, not necessarily

00:17:36.065 --> 00:17:36.690
point to point.

00:17:36.690 --> 00:17:39.040
But there will always
be a backwards pathway.

00:17:39.040 --> 00:17:41.390
They're not agreed
on what it's for.

00:17:41.390 --> 00:17:43.260
It could be for attention.

00:17:43.260 --> 00:17:46.585
It could be for learning, or
it could be for reconstruction,

00:17:46.585 --> 00:17:47.920
or it could be for all three.

00:17:47.920 --> 00:17:50.070
NICHOLAS THOMPSON: And
so we don't know what

00:17:50.070 --> 00:17:52.260
the backwards communication is.

00:17:52.260 --> 00:17:55.200
You are building your new neural
networks on the assumption

00:17:55.200 --> 00:17:57.900
that-- or you're building
backwards communication that

00:17:57.900 --> 00:17:59.940
is for reconstruction
into your neural networks

00:17:59.940 --> 00:18:02.010
even though we're not sure
that's how the brain works.

00:18:02.010 --> 00:18:02.450
GEOFFREY HINTON: Yes.

00:18:02.450 --> 00:18:03.640
NICHOLAS THOMPSON:
Isn't that cheating?

00:18:03.640 --> 00:18:04.040
GEOFFREY HINTON: Not at all

00:18:04.040 --> 00:18:04.500
NICHOLAS THOMPSON:
If you're trying

00:18:04.500 --> 00:18:06.298
to make it like
the brain, you're

00:18:06.298 --> 00:18:08.340
doing something we're not
sure is like the brain.

00:18:08.340 --> 00:18:09.320
GEOFFREY HINTON: Not at all.

00:18:09.320 --> 00:18:09.990
NICHOLAS THOMPSON: OK.

00:18:09.990 --> 00:18:11.240
GEOFFREY HINTON: There's two--

00:18:11.240 --> 00:18:13.282
I'm not doing computational
neuroscience science.

00:18:13.282 --> 00:18:15.990
That is, I'm not trying to make
a model of how the brain works.

00:18:15.990 --> 00:18:19.980
I'm looking at the brain
and saying this thing works.

00:18:19.980 --> 00:18:22.380
And if we want to make
something else that works,

00:18:22.380 --> 00:18:24.710
we should sort of look
to it for inspiration.

00:18:24.710 --> 00:18:27.750
So this is neuro inspired,
not a neural model.

00:18:27.750 --> 00:18:30.195
So the neurons we
use, they're inspired

00:18:30.195 --> 00:18:32.070
by the fact neurons have
a lot of connections

00:18:32.070 --> 00:18:33.250
and they change the strings.

00:18:33.250 --> 00:18:34.833
NICHOLAS THOMPSON:
That's interesting.

00:18:34.833 --> 00:18:37.110
So if I were in
computer science and I

00:18:37.110 --> 00:18:38.730
was working on neural
networks, and I

00:18:38.730 --> 00:18:41.970
wanted to beat Geoff
Hinton, one thing I could do

00:18:41.970 --> 00:18:45.000
is I could build in
top down communication

00:18:45.000 --> 00:18:47.427
and base it on other
models of brain science.

00:18:47.427 --> 00:18:49.260
So based on learning,
not on reconstructing.

00:18:49.260 --> 00:18:51.510
GEOFFREY HINTON: If they were
better models, then you'd win,

00:18:51.510 --> 00:18:51.880
yeah.

00:18:51.880 --> 00:18:53.400
NICHOLAS THOMPSON: That's
very, very interesting.

00:18:53.400 --> 00:18:55.750
All right, so let's move
to a more general topic.

00:18:55.750 --> 00:18:59.400
So neural networks will be able
to solve all kinds of problems.

00:18:59.400 --> 00:19:04.380
Are there any mysteries of the
human brain that will not be

00:19:04.380 --> 00:19:06.100
captured by neural
networks or cannot?

00:19:06.100 --> 00:19:08.147
For example, could the emotion--

00:19:08.147 --> 00:19:08.980
GEOFFREY HINTON: No.

00:19:08.980 --> 00:19:09.897
NICHOLAS THOMPSON: No.

00:19:09.897 --> 00:19:12.120
So love could be reconstructed
by a neural network?

00:19:12.120 --> 00:19:13.495
Consciousness can
be constructed?

00:19:13.495 --> 00:19:15.495
GEOFFREY HINTON: Absolutely,
once you've figured

00:19:15.495 --> 00:19:17.190
out what those things mean--

00:19:17.190 --> 00:19:20.360
we are neural networks, right?

00:19:20.360 --> 00:19:24.390
Now consciousness is something
I'm particularly interested in.

00:19:24.390 --> 00:19:25.700
I get by fine without it.

00:19:25.700 --> 00:19:26.855
But um--

00:19:26.855 --> 00:19:29.650
[LAUGHTER]

00:19:29.650 --> 00:19:31.878
So people don't really
know what they mean by it.

00:19:31.878 --> 00:19:33.670
There's all sorts of
different definitions.

00:19:33.670 --> 00:19:35.710
And I think it's a
pre-scientific term.

00:19:35.710 --> 00:19:40.822
So 100 years ago, if you
ask people what is life?

00:19:40.822 --> 00:19:43.280
They would have said, well,
living things have vital force.

00:19:43.280 --> 00:19:45.380
And when they die, the
vital force goes away.

00:19:45.380 --> 00:19:48.550
And that's the difference
between being alive and being

00:19:48.550 --> 00:19:50.890
dead, whether you got
vital force or not.

00:19:50.890 --> 00:19:53.290
And now we don't
think that sort of--

00:19:53.290 --> 00:19:54.950
we don't have vital force.

00:19:54.950 --> 00:19:57.230
We just think it's a
pre-scientific concept.

00:19:57.230 --> 00:19:59.380
And once you understand
some biochemistry

00:19:59.380 --> 00:20:02.290
and molecular biology, you
don't need vital force anymore.

00:20:02.290 --> 00:20:03.832
You understand how
it actually works.

00:20:03.832 --> 00:20:06.207
And I think it's going to be
the same with consciousness.

00:20:06.207 --> 00:20:07.870
I think consciousness
is an attempt

00:20:07.870 --> 00:20:12.730
to explain mental phenomena with
some kind of special essence.

00:20:12.730 --> 00:20:14.890
And this special essence,
you don't need it.

00:20:14.890 --> 00:20:18.070
Once you can really
explain it, then you'll

00:20:18.070 --> 00:20:20.372
explain how we do
the things that make

00:20:20.372 --> 00:20:21.580
people think we're conscious.

00:20:21.580 --> 00:20:23.538
And you'll explain all
these different meanings

00:20:23.538 --> 00:20:27.530
of consciousness without
having some special essence

00:20:27.530 --> 00:20:29.523
as consciousness.

00:20:29.523 --> 00:20:31.690
NICHOLAS THOMPSON: Right,
so there's no emotion that

00:20:31.690 --> 00:20:32.890
couldn't be created.

00:20:32.890 --> 00:20:34.965
There's no thought that
couldn't be created.

00:20:34.965 --> 00:20:36.340
There's nothing
that a human mind

00:20:36.340 --> 00:20:37.810
can do that couldn't
theoretically

00:20:37.810 --> 00:20:41.410
be recreated by a fully
functioning neural network

00:20:41.410 --> 00:20:43.420
once we truly understand
how the brain works.

00:20:43.420 --> 00:20:44.740
GEOFFREY HINTON: There's
something in a John Lennon song

00:20:44.740 --> 00:20:46.448
that sounds very like
what you just said.

00:20:46.448 --> 00:20:48.640
[LAUGHTER]

00:20:48.640 --> 00:20:51.180
NICHOLAS THOMPSON: And you're
100% confident of this?

00:20:51.180 --> 00:20:52.680
GEOFFREY HINTON:
No, I'm a Bayesian.

00:20:52.680 --> 00:20:56.110
So I'm 99.9% confident.

00:20:56.110 --> 00:20:59.160
NICHOLAS THOMPSON: OK,
and what is the point one?

00:20:59.160 --> 00:21:01.950
GEOFFREY HINTON: Well, we
might, for example, all

00:21:01.950 --> 00:21:04.735
be part of a big simulation.

00:21:04.735 --> 00:21:06.443
NICHOLAS THOMPSON:
True, fair enough, OK.

00:21:06.443 --> 00:21:08.415
[LAUGHTER]

00:21:08.415 --> 00:21:10.387
[APPLAUSE]

00:21:15.435 --> 00:21:19.140
That actually makes me think
it's more likely that we are.

00:21:19.140 --> 00:21:21.760
All right, so what are
we learning as we do this

00:21:21.760 --> 00:21:24.273
and as we study the brain
to improve computers?

00:21:24.273 --> 00:21:25.440
How does it work in reverse?

00:21:25.440 --> 00:21:27.180
What are we learning
about the brain

00:21:27.180 --> 00:21:28.568
from our work in computers?

00:21:28.568 --> 00:21:31.110
GEOFFREY HINTON: So I think what
we've learned in the last 10

00:21:31.110 --> 00:21:35.250
years is that if you take
a system with billions

00:21:35.250 --> 00:21:38.160
of parameters, and you'd
use stochastic gradient

00:21:38.160 --> 00:21:41.270
descent in some
objective function,

00:21:41.270 --> 00:21:43.880
and the objective function
might be to get the right labels

00:21:43.880 --> 00:21:47.800
or it might be to fill in
the gap in a string of words,

00:21:47.800 --> 00:21:52.680
or any objective function,
it works much better than it

00:21:52.680 --> 00:21:53.610
has any right to.

00:21:53.610 --> 00:21:55.550
It works much better
than you would expect.

00:21:55.550 --> 00:21:58.770
You would have thought, and
most people in conventional AI

00:21:58.770 --> 00:22:01.590
thought, take a system
with a billion parameters,

00:22:01.590 --> 00:22:04.710
start them off
with random values,

00:22:04.710 --> 00:22:06.990
measure the gradient of
the objective function.

00:22:06.990 --> 00:22:08.490
That is, for each
parameter figure

00:22:08.490 --> 00:22:10.770
out how the objective function
would change if you change

00:22:10.770 --> 00:22:11.937
that parameter a little bit.

00:22:14.147 --> 00:22:16.230
And then change it in that
direction that improves

00:22:16.230 --> 00:22:17.820
the objective function.

00:22:17.820 --> 00:22:19.195
You would have
thought that would

00:22:19.195 --> 00:22:21.660
be a kind of hopeless
algorithm that will get stuck.

00:22:21.660 --> 00:22:24.000
And it turns out, it's
a really good algorithm.

00:22:24.000 --> 00:22:27.750
And the bigger you scale
things, the better it works.

00:22:27.750 --> 00:22:30.155
And that's just an
empirical discovery really.

00:22:30.155 --> 00:22:31.530
There's some theory
coming along,

00:22:31.530 --> 00:22:33.280
but it's basically an
empirical discovery.

00:22:33.280 --> 00:22:35.550
Now because we've
discovered that,

00:22:35.550 --> 00:22:38.610
it makes it far more
plausible that the brain

00:22:38.610 --> 00:22:41.040
is computing the gradient
of some objective function

00:22:41.040 --> 00:22:43.380
and updating the weights
of strength of synapses

00:22:43.380 --> 00:22:45.185
to follow that gradient.

00:22:45.185 --> 00:22:47.310
We just have to figure out
how it gets the gradient

00:22:47.310 --> 00:22:48.600
and what the
objective function is.

00:22:48.600 --> 00:22:49.830
NICHOLAS THOMPSON: But
we didn't understand

00:22:49.830 --> 00:22:50.700
that about the brain.

00:22:50.700 --> 00:22:51.900
We didn't understand the
re-weighted [INAUDIBLE]..

00:22:51.900 --> 00:22:52.650
GEOFFREY HINTON:
It was a theory.

00:22:52.650 --> 00:22:54.130
It was-- I mean,
a long time ago,

00:22:54.130 --> 00:22:56.350
people thought
that's a possibility.

00:22:56.350 --> 00:22:58.650
But in the background,
there was always

00:22:58.650 --> 00:23:02.100
sort of conventional computer
scientists saying, yeah,

00:23:02.100 --> 00:23:03.680
but this idea of
everything's random,

00:23:03.680 --> 00:23:05.673
you just learn it all
by gradient descent,

00:23:05.673 --> 00:23:07.840
that's never going to work
for a billion parameters.

00:23:07.840 --> 00:23:09.670
You have to wire in
a lot of knowledge.

00:23:09.670 --> 00:23:10.150
NICHOLAS THOMPSON:
All right, so--

00:23:10.150 --> 00:23:11.620
GEOFFREY HINTON: And we
know now that's wrong.

00:23:11.620 --> 00:23:13.390
You can just put in
random parameters

00:23:13.390 --> 00:23:14.350
and learn everything.

00:23:14.350 --> 00:23:15.500
NICHOLAS THOMPSON: So
let's expand this out.

00:23:15.500 --> 00:23:17.560
So as we learn more and
more, we will presumably

00:23:17.560 --> 00:23:20.050
continue to learn more and
more about how the human brain

00:23:20.050 --> 00:23:22.510
functions as we run these
massive tests on models

00:23:22.510 --> 00:23:24.730
based on how we
think it functions.

00:23:24.730 --> 00:23:26.620
Once we understand
it better, is there

00:23:26.620 --> 00:23:29.620
a point where we
can, essentially,

00:23:29.620 --> 00:23:32.410
rewire our brains to
be more like the most

00:23:32.410 --> 00:23:34.918
efficient machines or
change the way we think?

00:23:34.918 --> 00:23:36.460
GEOFFREY HINTON:
You'd have thought--

00:23:36.460 --> 00:23:37.780
NICHOLAS THOMPSON: If it's a
simulation that should be easy,

00:23:37.780 --> 00:23:39.160
but not in a simulation.

00:23:39.160 --> 00:23:40.618
GEOFFREY HINTON:
You'd have thought

00:23:40.618 --> 00:23:44.120
that if we really
understand what's going on,

00:23:44.120 --> 00:23:46.806
we should be able to make things
like education work better,

00:23:46.806 --> 00:23:48.145
and I think we will.

00:23:48.145 --> 00:23:49.270
NICHOLAS THOMPSON: We will?

00:23:49.270 --> 00:23:50.187
GEOFFREY HINTON: Yeah.

00:23:50.187 --> 00:23:52.060
It would be very odd
if you could finally

00:23:52.060 --> 00:23:54.040
understand what's
going on in your brain

00:23:54.040 --> 00:23:57.700
and how it learns and not be
able to adapt the environment

00:23:57.700 --> 00:23:58.900
so you can learn better.

00:23:58.900 --> 00:23:59.820
NICHOLAS THOMPSON:
Well, OK, I don't want

00:23:59.820 --> 00:24:01.070
to go too far into the future.

00:24:01.070 --> 00:24:02.600
But a couple of
years from now, how

00:24:02.600 --> 00:24:04.600
do you think we will be
using what we've learned

00:24:04.600 --> 00:24:06.730
about the brain and about how
deep learning works to change

00:24:06.730 --> 00:24:08.040
how education functions?

00:24:08.040 --> 00:24:09.520
How would you change a class?

00:24:09.520 --> 00:24:11.103
GEOFFREY HINTON: In
a couple of years,

00:24:11.103 --> 00:24:13.180
I'm not sure we'll learn much.

00:24:13.180 --> 00:24:15.080
I think it's going to
change the education.

00:24:15.080 --> 00:24:16.080
It's going to be longer.

00:24:16.080 --> 00:24:18.220
But if you look
at it, Assistants

00:24:18.220 --> 00:24:20.050
are getting pretty smart now.

00:24:20.050 --> 00:24:22.878
And once Assistants can really
understand conversations,

00:24:22.878 --> 00:24:24.670
Assistants can have
conversations with kids

00:24:24.670 --> 00:24:25.900
and educate them.

00:24:25.900 --> 00:24:30.010
So already, I think most of
the new knowledge I acquire

00:24:30.010 --> 00:24:32.472
comes from me
thinking, I wonder,

00:24:32.472 --> 00:24:34.180
and typing something
to Google and Google

00:24:34.180 --> 00:24:35.555
tells me, if I could
just have a conversation,

00:24:35.555 --> 00:24:36.972
I'd acquire knowledge
even better.

00:24:36.972 --> 00:24:38.638
NICHOLAS THOMPSON:
And so theoretically,

00:24:38.638 --> 00:24:40.130
as we understand
the brain better,

00:24:40.130 --> 00:24:42.528
and as we set our children
up in front of Assistants.

00:24:42.528 --> 00:24:45.070
Mine right now almost certainly
based on the time in New York

00:24:45.070 --> 00:24:47.560
is yelling at Alexa to play
something on Spotify, probably

00:24:47.560 --> 00:24:48.670
"Baby Shark"--

00:24:48.670 --> 00:24:52.480
you will program the Assistants
to have better conversations

00:24:52.480 --> 00:24:55.133
with the children based on
how we know they'll learn?

00:24:55.133 --> 00:24:57.800
GEOFFREY HINTON: Yeah, I haven't
really thought much about this.

00:24:57.800 --> 00:24:58.910
It's not what I do.

00:24:58.910 --> 00:25:02.250
But it seems quite
plausible to me.

00:25:02.250 --> 00:25:04.450
NICHOLAS THOMPSON: Will
we be able to understand

00:25:04.450 --> 00:25:06.790
how dreams work, one
of the great mysteries?

00:25:06.790 --> 00:25:08.330
GEOFFREY HINTON: Yes, I'm
really interested in dreams.

00:25:08.330 --> 00:25:09.040
NICHOLAS THOMPSON: Good,
well, let's talk about that,

00:25:09.040 --> 00:25:09.820
GEOFFREY HINTON:
I'm so interested.

00:25:09.820 --> 00:25:11.930
I have at least four
different theories of dreams.

00:25:11.930 --> 00:25:13.597
NICHOLAS THOMPSON:
Let's hear them all--

00:25:13.597 --> 00:25:14.245
1, 2, 3, 4.

00:25:14.245 --> 00:25:16.870
GEOFFREY HINTON: So a long time
ago, there were things called--

00:25:16.870 --> 00:25:19.390
OK, a long time ago there
was Hopfield networks.

00:25:19.390 --> 00:25:23.440
And they would learn
memories as local attractors.

00:25:23.440 --> 00:25:25.540
And Hopfield discovered
that if you try and put

00:25:25.540 --> 00:25:28.695
too many memories in,
they get confused.

00:25:28.695 --> 00:25:30.070
They'll take two
local attractors

00:25:30.070 --> 00:25:32.570
of merged them into an attractor
sort of halfway in between.

00:25:35.120 --> 00:25:39.410
Then Francis Crick and Graeme
Mitchison came along and said,

00:25:39.410 --> 00:25:43.010
we can get rid of these false
minima by doing unlearning.

00:25:43.010 --> 00:25:45.620
So we turn off the input.

00:25:45.620 --> 00:25:48.290
We put the neural network
into a random state.

00:25:48.290 --> 00:25:51.260
We let it settle down,
and we say that's bad.

00:25:51.260 --> 00:25:54.010
Change the connections so you
don't settle to that state.

00:25:54.010 --> 00:25:58.490
And if you do a bit
of that, it will

00:25:58.490 --> 00:26:00.410
be able to store more memories.

00:26:00.410 --> 00:26:03.617
And then Terry Sejnowski and
I came along and said, look,

00:26:03.617 --> 00:26:05.450
if we have not just the
neurons where you're

00:26:05.450 --> 00:26:09.080
storing the memories, but
lots of other neurons, too,

00:26:09.080 --> 00:26:10.940
can we find an
algorithm that we'll

00:26:10.940 --> 00:26:13.232
use all these other neurons
to help you store memories?

00:26:13.232 --> 00:26:15.690
And it turned out in the end,
we came up with the Boltzmann

00:26:15.690 --> 00:26:17.020
machine learning algorithm.

00:26:17.020 --> 00:26:18.853
And the Boltzmann machine
learning algorithm

00:26:18.853 --> 00:26:22.020
had a very interesting property
which is I show you data.

00:26:22.020 --> 00:26:25.100
That is, I fixed the states
of the observable units.

00:26:25.100 --> 00:26:28.580
And it sort of rattles
around the other units

00:26:28.580 --> 00:26:30.698
until it's got a
fairly happy state.

00:26:30.698 --> 00:26:32.240
And once it's done
that, it increases

00:26:32.240 --> 00:26:34.280
the strength of all
the connections based

00:26:34.280 --> 00:26:36.710
on if two units
are both active, it

00:26:36.710 --> 00:26:38.230
increases connection strength.

00:26:38.230 --> 00:26:39.888
That's called kind
of Hebbian learning.

00:26:39.888 --> 00:26:41.930
But if you just do that,
the connection strengths

00:26:41.930 --> 00:26:43.957
just get bigger and bigger.

00:26:43.957 --> 00:26:46.040
You also have to have a
phase where you cut it off

00:26:46.040 --> 00:26:47.893
from the input.

00:26:47.893 --> 00:26:49.310
You let it rattle
around to settle

00:26:49.310 --> 00:26:50.720
into a state it's happy with.

00:26:50.720 --> 00:26:53.192
So now it's having a fantasy.

00:26:53.192 --> 00:26:55.250
And once it's had
the fantasy you

00:26:55.250 --> 00:26:58.280
say, take all passive
neurons that are active

00:26:58.280 --> 00:27:01.600
and decrease the strength
to the connection.

00:27:01.600 --> 00:27:05.050
So I'm explaining the algorithm
to you just as a procedure.

00:27:05.050 --> 00:27:09.490
But actually that algorithm is
the result of doing some math

00:27:09.490 --> 00:27:12.010
and saying, how should you
change these connection

00:27:12.010 --> 00:27:15.130
strengths so that this
neural network with all

00:27:15.130 --> 00:27:18.970
these hidden units finds
the data unsurprising?

00:27:18.970 --> 00:27:21.250
And it has to have
this other phase.

00:27:21.250 --> 00:27:23.890
It has to have this what we
call the negative phase when

00:27:23.890 --> 00:27:25.510
it's running with no input.

00:27:25.510 --> 00:27:27.260
And it's canceling out--

00:27:27.260 --> 00:27:30.185
its unlearning whatever
state it settles into.

00:27:30.185 --> 00:27:31.810
Now what Crick pointed
out about dreams

00:27:31.810 --> 00:27:35.870
is that, we know that you dream
for many hours every night.

00:27:35.870 --> 00:27:37.750
And if I wake you
up at random, you

00:27:37.750 --> 00:27:40.167
can tell me what you were just
dreaming about because it's

00:27:40.167 --> 00:27:41.250
in your short term memory.

00:27:41.250 --> 00:27:42.750
So we know you dream
for many hours.

00:27:42.750 --> 00:27:44.410
But in the morning,
you wake up, you

00:27:44.410 --> 00:27:45.670
can remember the
last dream, but you

00:27:45.670 --> 00:27:47.650
can't remember all the others,
which is lucky because you

00:27:47.650 --> 00:27:48.942
might mistake them for reality.

00:27:51.410 --> 00:27:53.830
So why is it that we don't
remember our dreams at all?

00:27:53.830 --> 00:27:57.370
And Crick's view was it's
the whole point of dreaming

00:27:57.370 --> 00:28:00.000
is to unlearn those things
so you put the learning

00:28:00.000 --> 00:28:01.400
rule in reverse.

00:28:01.400 --> 00:28:04.758
And Terry Sejnowski and I
showed that actually that

00:28:04.758 --> 00:28:06.550
is a maximum [INAUDIBLE]
learning procedure

00:28:06.550 --> 00:28:07.635
for Boltzmann machines.

00:28:07.635 --> 00:28:09.010
So that's one
theory of dreaming.

00:28:09.010 --> 00:28:10.390
NICHOLAS THOMPSON: You
showed that theoretically?

00:28:10.390 --> 00:28:12.307
GEOFFREY HINTON: Yeah,
we should theoretically

00:28:12.307 --> 00:28:14.260
that's the right thing
to do if you want

00:28:14.260 --> 00:28:19.720
to change the weights so that
your big neural network finds

00:28:19.720 --> 00:28:21.137
the observed data
less surprising.

00:28:21.137 --> 00:28:23.595
NICHOLAS THOMPSON: And I want
to go to your other theories,

00:28:23.595 --> 00:28:25.630
but before we lose
this thread, you've

00:28:25.630 --> 00:28:28.850
proved that it's efficient.

00:28:28.850 --> 00:28:32.230
Have you actually set any of
your deep learning algorithms

00:28:32.230 --> 00:28:33.520
to essentially dream?

00:28:33.520 --> 00:28:36.340
Right, study this image data
set for a period of time,

00:28:36.340 --> 00:28:38.980
resort, study again,
resort versus a machine

00:28:38.980 --> 00:28:40.480
that's running continuously?

00:28:40.480 --> 00:28:42.980
GEOFFREY HINTON: So yes, we had
machine learning algorithms.

00:28:42.980 --> 00:28:44.355
Some of the first
algorithms that

00:28:44.355 --> 00:28:46.690
could learn what to
do with hidden units

00:28:46.690 --> 00:28:49.370
were Boltzmann machines.

00:28:49.370 --> 00:28:51.130
They were very inefficient.

00:28:51.130 --> 00:28:54.340
But then later on, I found a
way of making approximations

00:28:54.340 --> 00:28:55.990
to them that was efficient.

00:28:55.990 --> 00:28:58.600
And those were
actually the trigger

00:28:58.600 --> 00:29:00.183
for getting deep
learning going again.

00:29:00.183 --> 00:29:02.350
Those were the things that
learned one layer feature

00:29:02.350 --> 00:29:04.270
detector at a time.

00:29:04.270 --> 00:29:08.440
And it was efficient form of
restricted Boltzmann machine.

00:29:08.440 --> 00:29:11.140
And so it was doing
this kind of unlearning.

00:29:11.140 --> 00:29:13.240
But rather than going
to sleep, that one

00:29:13.240 --> 00:29:16.540
would just fantasize
for a little bit

00:29:16.540 --> 00:29:17.870
after each data point.

00:29:17.870 --> 00:29:20.287
NICHOLAS THOMPSON: So Androids
do dream of electric sheep.

00:29:20.287 --> 00:29:23.120
So let's go to
theories 2, 3, and 4.

00:29:23.120 --> 00:29:25.870
GEOFFREY HINTON: OK,
theory 2 was called

00:29:25.870 --> 00:29:27.700
the wake-sleep algorithm.

00:29:27.700 --> 00:29:31.510
And you want to learn
a generative model.

00:29:31.510 --> 00:29:33.640
So you have the idea
that you're going to have

00:29:33.640 --> 00:29:36.050
a model that can generate data.

00:29:36.050 --> 00:29:37.752
It has layers of
features detectors.

00:29:37.752 --> 00:29:40.210
And it activates the high level
ones and the low level ones

00:29:40.210 --> 00:29:43.528
and so on, until it activates
pixels, and that's an image.

00:29:43.528 --> 00:29:45.070
You also want to
learn the other way.

00:29:45.070 --> 00:29:47.820
You want to learn
to recognize data.

00:29:47.820 --> 00:29:52.750
And so you're going to have an
algorithm that has two phases.

00:29:52.750 --> 00:29:56.080
In the wake phase,
data comes in.

00:29:56.080 --> 00:29:58.115
It tries to recognize it.

00:29:58.115 --> 00:29:59.740
And instead of learning
the connections

00:29:59.740 --> 00:30:01.570
it is using for
recognition, it's

00:30:01.570 --> 00:30:03.420
learning the
generative connections.

00:30:03.420 --> 00:30:05.670
So data comes in.

00:30:05.670 --> 00:30:07.343
I activate the hidden
units, and then I

00:30:07.343 --> 00:30:09.760
learn to make those hidden
units be good at reconstructing

00:30:09.760 --> 00:30:10.800
s that data.

00:30:10.800 --> 00:30:13.198
So it's learning to
reconstruct at every layer.

00:30:13.198 --> 00:30:15.740
But the question is, how do you
learn the forward connection?

00:30:15.740 --> 00:30:17.907
So the idea is, if you knew
the forward connections,

00:30:17.907 --> 00:30:20.073
you could learn the backward
connections because you

00:30:20.073 --> 00:30:21.230
could learn to reconstruct.

00:30:21.230 --> 00:30:22.597
NICHOLAS THOMPSON: Yeah.

00:30:22.597 --> 00:30:24.180
GEOFFREY HINTON: Now
it also turns out

00:30:24.180 --> 00:30:25.650
that if you knew the
backward connections,

00:30:25.650 --> 00:30:27.000
you could learn the
forward connections

00:30:27.000 --> 00:30:28.875
because what you could
do is start at the top

00:30:28.875 --> 00:30:30.330
and just generate some data.

00:30:30.330 --> 00:30:32.700
And because you
generated the data,

00:30:32.700 --> 00:30:35.133
you'd know the states of
all the hidden layers.

00:30:35.133 --> 00:30:37.050
And so you could learn
the forward connections

00:30:37.050 --> 00:30:40.120
to recover those states.

00:30:40.120 --> 00:30:43.330
So that would be
the sleep phase.

00:30:43.330 --> 00:30:48.010
When you turn off the input,
you just generate data

00:30:48.010 --> 00:30:50.850
and then you try and
reconstruct the hidden units

00:30:50.850 --> 00:30:53.030
that generated the data.

00:30:53.030 --> 00:30:55.868
And so if you know the
top down connections,

00:30:55.868 --> 00:30:57.160
you'd learn the bottom up ones.

00:30:57.160 --> 00:30:58.170
If you know the
bottom up ones, you

00:30:58.170 --> 00:30:59.640
could learn the top down ones.

00:30:59.640 --> 00:31:00.880
And so what's going
to happen if you start

00:31:00.880 --> 00:31:03.570
with random connections and
try doing both-- alternate both

00:31:03.570 --> 00:31:05.373
kinds of learning and it works.

00:31:05.373 --> 00:31:06.790
Now to make it
work well, you have

00:31:06.790 --> 00:31:08.290
to do all sorts of
variations of it.

00:31:08.290 --> 00:31:09.265
But it works.

00:31:09.265 --> 00:31:10.890
NICHOLAS THOMPSON:
All right, that is--

00:31:10.890 --> 00:31:12.220
do you want to go through
the other two theories?

00:31:12.220 --> 00:31:13.553
We only have eight minutes left.

00:31:13.553 --> 00:31:16.460
I think we should probably jump
through some other questions.

00:31:16.460 --> 00:31:16.720
We'll deal with--

00:31:16.720 --> 00:31:17.560
GEOFFREY HINTON: If you
give me another hour,

00:31:17.560 --> 00:31:18.977
I could do the
other two theories.

00:31:18.977 --> 00:31:19.653
[LAUGHTER]

00:31:19.653 --> 00:31:21.820
NICHOLAS THOMPSON: All
right, well, Google I/O 2020.

00:31:21.820 --> 00:31:24.860
So let's talk about
what comes next.

00:31:24.860 --> 00:31:26.620
So where is your
research headed?

00:31:26.620 --> 00:31:29.383
What problem are you
trying to solve now?

00:31:29.383 --> 00:31:31.550
GEOFFREY HINTON: The main
thing I'm trying to solve,

00:31:31.550 --> 00:31:33.592
which I've been doing for
a number of years now--

00:31:36.292 --> 00:31:38.250
actually, I'm reminded
of a soccer commentator.

00:31:38.250 --> 00:31:40.420
You may notice
soccer commentators,

00:31:40.420 --> 00:31:43.280
they always say things like
they're doing very well,

00:31:43.280 --> 00:31:45.268
but they always go
wrong on the last pass.

00:31:45.268 --> 00:31:47.560
And they never seem to sort
of notice there's something

00:31:47.560 --> 00:31:48.850
funny about that.

00:31:48.850 --> 00:31:51.370
It's a bit circular.

00:31:51.370 --> 00:31:53.950
So I'm working--
eventually, you're

00:31:53.950 --> 00:31:56.565
going to end up working on
something you don't finish.

00:31:56.565 --> 00:31:57.940
And I think I may
well be working

00:31:57.940 --> 00:31:59.107
on the thing I never finish.

00:31:59.107 --> 00:32:01.680
But it's called
capsules, and it's

00:32:01.680 --> 00:32:08.650
a theory of how you do visual
perception using reconstruction

00:32:08.650 --> 00:32:10.360
and also how you
root information

00:32:10.360 --> 00:32:12.160
to the right places.

00:32:12.160 --> 00:32:15.700
And the two main
motivating factors

00:32:15.700 --> 00:32:18.940
were in standard neural
nets, the information--

00:32:18.940 --> 00:32:22.300
the activity in the layer just
automatically goes somewhere.

00:32:22.300 --> 00:32:25.030
You don't make decisions
about where to send it.

00:32:25.030 --> 00:32:26.830
The idea of capsules
was to make decisions

00:32:26.830 --> 00:32:28.900
about where to send information.

00:32:28.900 --> 00:32:30.970
Now since I started
working on capsules,

00:32:30.970 --> 00:32:33.350
some other very smart
people at Google

00:32:33.350 --> 00:32:35.840
invented transformers, which
are doing the same thing.

00:32:35.840 --> 00:32:37.840
They're deciding where
to route information,

00:32:37.840 --> 00:32:40.000
and that's a big win.

00:32:40.000 --> 00:32:44.470
The other thing that motivated
capsules was coordinate frames.

00:32:44.470 --> 00:32:46.870
So when humans do
visual, they're

00:32:46.870 --> 00:32:48.890
always using coordinate frames.

00:32:48.890 --> 00:32:52.420
And if they impose the wrong
coordinate frame on an object,

00:32:52.420 --> 00:32:55.030
they don't even
recognize the object.

00:32:55.030 --> 00:32:57.400
So I'll give you a little task.

00:32:57.400 --> 00:32:59.200
Imagine a tetrahedron.

00:32:59.200 --> 00:33:01.450
It's got a triangular base
and three triangular faces,

00:33:01.450 --> 00:33:02.890
all equilateral triangles.

00:33:02.890 --> 00:33:06.340
Easy to imagine, right?

00:33:06.340 --> 00:33:09.070
Now imagine slicing
it with a plane.

00:33:09.070 --> 00:33:11.980
So you get a square
cross-section.

00:33:11.980 --> 00:33:14.020
That's not so easy, right?

00:33:14.020 --> 00:33:16.400
Every time you slice
it, you get a triangle.

00:33:16.400 --> 00:33:18.010
It's not obvious how
you get a square.

00:33:18.010 --> 00:33:20.110
It's not at all obvious.

00:33:20.110 --> 00:33:25.210
OK, but I give you the same
shape described differently.

00:33:25.210 --> 00:33:26.410
I need your pen.

00:33:26.410 --> 00:33:30.050
Imagine, the shape you
get, if you take a pen

00:33:30.050 --> 00:33:33.067
like that, another pen that
right angles like this,

00:33:33.067 --> 00:33:34.650
and you connect all
points on this pen

00:33:34.650 --> 00:33:37.320
to all points on this pen.

00:33:37.320 --> 00:33:40.040
That's a solid tetrahedron.

00:33:40.040 --> 00:33:45.820
OK, you're seeing it relative
to a different coordinate frame

00:33:45.820 --> 00:33:48.560
where the edges of
the tetrahedron--

00:33:48.560 --> 00:33:50.820
these two line up with
the coordinate frame.

00:33:50.820 --> 00:33:54.530
And for this, if you think
of the tetrahedron that way,

00:33:54.530 --> 00:33:56.250
it's pretty obvious
that at the top,

00:33:56.250 --> 00:33:57.860
you've got a long
rectangle this way.

00:33:57.860 --> 00:34:00.700
At the bottom, you get a
long rectangle that way.

00:34:00.700 --> 00:34:02.470
And there's [INAUDIBLE]
that you've got

00:34:02.470 --> 00:34:05.100
to get a square in the middle.

00:34:05.100 --> 00:34:07.600
So it's pretty obvious how you
can slice it to get a square.

00:34:07.600 --> 00:34:09.433
But that's only obviously
if you think of it

00:34:09.433 --> 00:34:10.860
with that coordinate frame.

00:34:10.860 --> 00:34:13.760
So it's obvious that for
humans, coordinate frames are

00:34:13.760 --> 00:34:15.739
very important for perception.

00:34:15.739 --> 00:34:18.139
And they're not at all
important for conv nets.

00:34:18.139 --> 00:34:22.420
For conv nets, if I
show you a tilted square

00:34:22.420 --> 00:34:25.420
and an upright diamond, which
is actually the same thing,

00:34:25.420 --> 00:34:27.350
they look the same
to a conv net.

00:34:27.350 --> 00:34:29.031
It doesn't have two
alternative ways

00:34:29.031 --> 00:34:30.239
of describing the same thing.

00:34:30.239 --> 00:34:32.489
NICHOLAS THOMPSON: But how
is adding coordinate frames

00:34:32.489 --> 00:34:34.489
to your model not
the same as the error

00:34:34.489 --> 00:34:35.989
you were making
in the '90s where

00:34:35.989 --> 00:34:38.877
you were trying to put rules
into the system as opposed

00:34:38.877 --> 00:34:40.460
to letting the system
be unsupervised?

00:34:40.460 --> 00:34:42.550
GEOFFREY HINTON: It
is exactly that error.

00:34:42.550 --> 00:34:45.870
And because I am so adamant
that that's a terrible error,

00:34:45.870 --> 00:34:48.300
I'm allowed to do
a tiny bit of it.

00:34:48.300 --> 00:34:50.828
It's sort of like Nixon
negotiating with China.

00:34:50.828 --> 00:34:54.080
[LAUGHTER]

00:34:54.080 --> 00:34:55.580
Actually that puts
me in a bad role.

00:34:55.580 --> 00:35:02.210
Anyway, so if you
look at conv nets,

00:35:02.210 --> 00:35:04.050
they're just neural
nets where you wired

00:35:04.050 --> 00:35:05.412
in a tiny bit of knowledge.

00:35:05.412 --> 00:35:07.870
You add in the knowledge that
if a feature detector is good

00:35:07.870 --> 00:35:10.617
here, it's good over there.

00:35:10.617 --> 00:35:12.700
And people would love to
wire in just a little bit

00:35:12.700 --> 00:35:15.322
more knowledge about
scale and orientation.

00:35:15.322 --> 00:35:16.780
But if you do it
in the obvious way

00:35:16.780 --> 00:35:19.060
of having a 4D grid
instead of a 2D grid,

00:35:19.060 --> 00:35:21.320
the whole thing blows up on you.

00:35:21.320 --> 00:35:26.350
But you can get in that
knowledge about what viewpoint

00:35:26.350 --> 00:35:30.280
does to an image by using
coordinate frames the same way

00:35:30.280 --> 00:35:32.080
they do them in graphics.

00:35:32.080 --> 00:35:35.113
So now you have a
representation in one layer.

00:35:35.113 --> 00:35:37.780
When you try and reconstruct the
parts of an object in the layer

00:35:37.780 --> 00:35:41.900
below, when you do
that reconstruction,

00:35:41.900 --> 00:35:44.450
you can take the coordinate
frame of the whole object

00:35:44.450 --> 00:35:47.210
and multiply it by the
part whole relationship

00:35:47.210 --> 00:35:49.400
to get the coordinate
frame of the part.

00:35:49.400 --> 00:35:51.210
And you can wire that
into the network.

00:35:51.210 --> 00:35:52.970
You can wire into the
network the ability

00:35:52.970 --> 00:35:55.052
to do those coordinate
transformations.

00:35:55.052 --> 00:35:57.260
And that should make it
generalize much, much better.

00:35:57.260 --> 00:36:00.800
It should mean the networks
just find viewpoint very easy

00:36:00.800 --> 00:36:01.727
to deal with.

00:36:01.727 --> 00:36:03.560
Current neural networks
find viewpoint other

00:36:03.560 --> 00:36:05.830
than translation very
hard to deal with.

00:36:05.830 --> 00:36:07.610
NICHOLAS THOMPSON:
So your current task

00:36:07.610 --> 00:36:09.740
is specific to
visual recognition,

00:36:09.740 --> 00:36:12.200
or it is a more general way
of improving or coming up

00:36:12.200 --> 00:36:14.070
with the rule set for
coordinate frames?

00:36:14.070 --> 00:36:16.362
GEOFFREY HINTON: OK, it could
be used for other things.

00:36:16.362 --> 00:36:19.628
But I'm really interested in
the use for visual recognition.

00:36:19.628 --> 00:36:21.170
NICHOLAS THOMPSON:
OK, last question.

00:36:21.170 --> 00:36:23.745
I was listening to a podcast
you gave the other day.

00:36:23.745 --> 00:36:26.120
And in it, you said that the
people whose ideas you value

00:36:26.120 --> 00:36:28.578
most are the young graduate
students who come into your lab

00:36:28.578 --> 00:36:31.560
because they aren't locked
into the old perceptions.

00:36:31.560 --> 00:36:35.270
They have fresh ideas, and
yet they also know a lot.

00:36:35.270 --> 00:36:38.120
Is there anything that you, sort
of looking outside yourself,

00:36:38.120 --> 00:36:40.370
you think you might be locked
into that a new graduate

00:36:40.370 --> 00:36:42.787
student or somebody in this
room who came to work with you

00:36:42.787 --> 00:36:43.942
would shake up?

00:36:43.942 --> 00:36:45.650
GEOFFREY HINTON: Yeah,
everything I said.

00:36:45.650 --> 00:36:46.320
NICHOLAS THOMPSON:
Everything you said.

00:36:46.320 --> 00:36:46.940
[LAUGHTER]

00:36:46.940 --> 00:36:48.707
Take out those coordinate units.

00:36:48.707 --> 00:36:50.540
Work on feature three,
work on feature four.

00:36:50.540 --> 00:36:52.430
I wanted to ask you
a separate question.

00:36:52.430 --> 00:36:55.540
So deep learning used
to be a distinct thing,

00:36:55.540 --> 00:36:59.210
and then it became sort of
synonymous with the phrase AI.

00:36:59.210 --> 00:37:01.220
And then AI is now
a marketing term

00:37:01.220 --> 00:37:03.928
that basically means using a
machine in any way whatsoever.

00:37:03.928 --> 00:37:05.470
How do you feel
about the terminology

00:37:05.470 --> 00:37:07.073
as the man who
helped create this?

00:37:07.073 --> 00:37:08.990
GEOFFREY HINTON: Well,
I was much happier when

00:37:08.990 --> 00:37:12.500
there was AI, which
meant your logic inspired

00:37:12.500 --> 00:37:15.410
and you do manipulations
on cymbal strings.

00:37:15.410 --> 00:37:17.230
And there was neural
nets, which means

00:37:17.230 --> 00:37:19.860
you want to do learning
in a neural network.

00:37:19.860 --> 00:37:22.460
And they were completely
different enterprises

00:37:22.460 --> 00:37:25.310
that really sort of
didn't get along too well

00:37:25.310 --> 00:37:27.350
and fought for money.

00:37:27.350 --> 00:37:29.720
That's how I grew up.

00:37:29.720 --> 00:37:32.530
And now I see sort
of people who spent

00:37:32.530 --> 00:37:34.280
years saying neural
networks are nonsense,

00:37:34.280 --> 00:37:35.480
saying I'm an AI professor.

00:37:35.480 --> 00:37:36.740
So I need money.

00:37:36.740 --> 00:37:38.440
And it's annoying.

00:37:38.440 --> 00:37:40.670
NICHOLAS THOMPSON: So
your field succeeded

00:37:40.670 --> 00:37:42.720
kind of ate or subsumed
the other field, which

00:37:42.720 --> 00:37:44.720
then gave them an advantage
in asking for money,

00:37:44.720 --> 00:37:46.110
which is frustrating?

00:37:46.110 --> 00:37:47.060
GEOFFREY HINTON: Yeah,
now it's not entirely fair

00:37:47.060 --> 00:37:48.977
because a lot of them
have actually converted.

00:37:48.977 --> 00:37:50.690
NICHOLAS THOMPSON:
Right, so wonderful.

00:37:50.690 --> 00:37:52.520
Well, I've got time
for one more question.

00:37:52.520 --> 00:37:55.730
So in that same interview,
you were talking about AI.

00:37:55.730 --> 00:37:58.100
And you said, think of it
like a backhoe, a backhoe that

00:37:58.100 --> 00:38:01.250
can build a hole, or if
not constructed properly,

00:38:01.250 --> 00:38:03.107
can wipe you out.

00:38:03.107 --> 00:38:04.940
And the key is when you
work on your backhoe

00:38:04.940 --> 00:38:07.315
to design it in such a way
that it's best to build a hole

00:38:07.315 --> 00:38:08.910
and not to clock
you in the head.

00:38:08.910 --> 00:38:10.040
As you think about
your work, what

00:38:10.040 --> 00:38:11.523
are the choices
you make like that?

00:38:17.450 --> 00:38:20.840
GEOFFREY HINTON: I guess
I would never deliberately

00:38:20.840 --> 00:38:22.640
work on making weapons.

00:38:24.923 --> 00:38:26.340
I mean, you could
design a backhoe

00:38:26.340 --> 00:38:28.490
that was very good at
knocking people's heads off.

00:38:28.490 --> 00:38:31.160
And I think that would be
a bad use of a backhoe,

00:38:31.160 --> 00:38:32.390
and I wouldn't work on it.

00:38:32.390 --> 00:38:34.310
NICHOLAS THOMPSON: All right,
well, Geoffrey Hinton--

00:38:34.310 --> 00:38:35.210
extraordinary interview.

00:38:35.210 --> 00:38:36.710
All kinds information--
will be back

00:38:36.710 --> 00:38:40.015
next year to talk about dreams
theories three and four.

00:38:40.015 --> 00:38:40.890
That was so much fun.

00:38:40.890 --> 00:38:41.600
Thank you.

00:38:41.600 --> 00:38:45.850
[MUSIC PLAYING]

