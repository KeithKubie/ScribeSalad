WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:02.840
Learning rate tuning
can be very strange.

00:00:02.840 --> 00:00:06.909
For example, you might think that using
a higher learning rate means you learn

00:00:06.909 --> 00:00:08.524
more or that you learn faster.

00:00:08.524 --> 00:00:10.260
That's just not true.

00:00:10.260 --> 00:00:14.270
In fact, you can often take a model,
lower the learning rate and

00:00:14.270 --> 00:00:16.379
get to a better model faster.

00:00:16.379 --> 00:00:17.558
It gets even worse.

00:00:17.558 --> 00:00:22.111
You might be tempted to look at the
curve that shows the loss over time to

00:00:22.111 --> 00:00:23.898
see how quickly you learn.

00:00:23.898 --> 00:00:27.325
Here the higher learning rate starts
faster, but then it plateaus,

00:00:27.325 --> 00:00:31.290
when the lower learning rate
keeps on going and gets better.

00:00:31.290 --> 00:00:35.410
It is a very familiar picture for
anyone who's trained neural networks.

00:00:35.410 --> 00:00:37.350
Never trust how quickly you learn.

00:00:37.350 --> 00:00:39.720
It has often little to do
with how well you train.

00:00:40.950 --> 00:00:44.540
This is where SGD gets its
reputation for being black magic.

00:00:44.540 --> 00:00:48.340
You have many, many hyper-parameters
that you could play with.

00:00:48.340 --> 00:00:52.510
Initialization parameters, learning
rate parameters, decay, momentum.

00:00:52.510 --> 00:00:54.870
And you have to get them right.

00:00:54.870 --> 00:00:56.820
In practice, it's not that bad.

00:00:56.820 --> 00:00:59.180
But if you have to
remember just one thing,

00:00:59.180 --> 00:01:03.760
it's that when things don't work, always
try to lower your learning rate first.

00:01:03.760 --> 00:01:06.140
There are lots of good solutions for
small models.

00:01:06.140 --> 00:01:09.830
But sadly, none that's completely
satisfactory so far for

00:01:09.830 --> 00:01:11.900
the very large models that
we really care about.

00:01:13.052 --> 00:01:17.640
I'll mention one approach called AdaGrad
that makes things a little bit easier.

00:01:17.640 --> 00:01:21.680
AdaGrad is a modification of SGD
which implicitly does momentum and

00:01:21.680 --> 00:01:23.620
learning rate decay for you.

00:01:23.620 --> 00:01:28.180
Using AdaGrad often makes learning
less sensitive to hyper-parameters.

00:01:28.180 --> 00:01:33.250
But it often tends to be a little worse
than precisely tuned SDG with momentum.

00:01:33.250 --> 00:01:34.540
It's still a very good option,

00:01:34.540 --> 00:01:36.535
though, if you're just trying
to get things to work.

00:01:37.595 --> 00:01:38.475
So, let's recap.

00:01:38.475 --> 00:01:42.195
We have this very simple linear
model which emits probabilities

00:01:42.195 --> 00:01:43.785
which we can use to classify things.

00:01:44.855 --> 00:01:47.715
We now know how to optimize
its parameters on lots and

00:01:47.715 --> 00:01:51.015
lots of data using SGD and its variants.

00:01:51.015 --> 00:01:53.615
It's still a linear,
shallow model, though, but

00:01:53.615 --> 00:01:55.775
now we have all the tools that we need.

00:01:55.775 --> 00:01:56.615
It's time to go deeper.

