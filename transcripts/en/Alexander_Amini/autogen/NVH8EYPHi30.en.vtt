WEBVTT
Kind: captions
Language: en

00:00:02.100 --> 00:00:04.730
let's get started so good morning

00:00:04.730 --> 00:00:04.740
let's get started so good morning
 

00:00:04.740 --> 00:00:07.610
let's get started so good morning
everyone my name is Alva and today we're

00:00:07.610 --> 00:00:07.620
everyone my name is Alva and today we're
 

00:00:07.620 --> 00:00:09.530
everyone my name is Alva and today we're
going to learn about how deep learning

00:00:09.530 --> 00:00:09.540
going to learn about how deep learning
 

00:00:09.540 --> 00:00:12.850
going to learn about how deep learning
can be used to build systems capable of

00:00:12.850 --> 00:00:12.860
can be used to build systems capable of
 

00:00:12.860 --> 00:00:15.530
can be used to build systems capable of
perceiving images and making decisions

00:00:15.530 --> 00:00:15.540
perceiving images and making decisions
 

00:00:15.540 --> 00:00:19.339
perceiving images and making decisions
based on visual information neural

00:00:19.339 --> 00:00:19.349
based on visual information neural
 

00:00:19.349 --> 00:00:20.839
based on visual information neural
networks have really made a tremendous

00:00:20.839 --> 00:00:20.849
networks have really made a tremendous
 

00:00:20.849 --> 00:00:23.450
networks have really made a tremendous
impact in this area over the past 20

00:00:23.450 --> 00:00:23.460
impact in this area over the past 20
 

00:00:23.460 --> 00:00:25.310
impact in this area over the past 20
years and I think to really appreciate

00:00:25.310 --> 00:00:25.320
years and I think to really appreciate
 

00:00:25.320 --> 00:00:28.490
years and I think to really appreciate
how and why this has been the case it

00:00:28.490 --> 00:00:28.500
how and why this has been the case it
 

00:00:28.500 --> 00:00:32.209
how and why this has been the case it
helps to take a step back way back to

00:00:32.209 --> 00:00:32.219
helps to take a step back way back to
 

00:00:32.219 --> 00:00:34.310
helps to take a step back way back to
five hundred forty million years ago and

00:00:34.310 --> 00:00:34.320
five hundred forty million years ago and
 

00:00:34.320 --> 00:00:37.250
five hundred forty million years ago and
the Cambrian explosion where biologists

00:00:37.250 --> 00:00:37.260
the Cambrian explosion where biologists
 

00:00:37.260 --> 00:00:39.410
the Cambrian explosion where biologists
traced the evolutionary origins of

00:00:39.410 --> 00:00:39.420
traced the evolutionary origins of
 

00:00:39.420 --> 00:00:42.590
traced the evolutionary origins of
vision the reason vision seemed so easy

00:00:42.590 --> 00:00:42.600
vision the reason vision seemed so easy
 

00:00:42.600 --> 00:00:44.810
vision the reason vision seemed so easy
for us as humans is because we have had

00:00:44.810 --> 00:00:44.820
for us as humans is because we have had
 

00:00:44.820 --> 00:00:48.410
for us as humans is because we have had
five hundred forty million years of data

00:00:48.410 --> 00:00:48.420
five hundred forty million years of data
 

00:00:48.420 --> 00:00:51.290
five hundred forty million years of data
for evolution to train on compare that

00:00:51.290 --> 00:00:51.300
for evolution to train on compare that
 

00:00:51.300 --> 00:00:53.990
for evolution to train on compare that
to bipedal movement human language and

00:00:53.990 --> 00:00:54.000
to bipedal movement human language and
 

00:00:54.000 --> 00:00:57.290
to bipedal movement human language and
the difference is significant starting

00:00:57.290 --> 00:00:57.300
the difference is significant starting
 

00:00:57.300 --> 00:00:59.870
the difference is significant starting
in around the 1960s there was a surge in

00:00:59.870 --> 00:00:59.880
in around the 1960s there was a surge in
 

00:00:59.880 --> 00:01:03.470
in around the 1960s there was a surge in
interest in in both the neural basis of

00:01:03.470 --> 00:01:03.480
interest in in both the neural basis of
 

00:01:03.480 --> 00:01:06.230
interest in in both the neural basis of
vision and in developing methods to

00:01:06.230 --> 00:01:06.240
vision and in developing methods to
 

00:01:06.240 --> 00:01:08.450
vision and in developing methods to
systematically characterize visual

00:01:08.450 --> 00:01:08.460
systematically characterize visual
 

00:01:08.460 --> 00:01:10.670
systematically characterize visual
processing and this eventually led to

00:01:10.670 --> 00:01:10.680
processing and this eventually led to
 

00:01:10.680 --> 00:01:13.820
processing and this eventually led to
computer scientists wondering about how

00:01:13.820 --> 00:01:13.830
computer scientists wondering about how
 

00:01:13.830 --> 00:01:16.160
computer scientists wondering about how
these findings from neuroscience could

00:01:16.160 --> 00:01:16.170
these findings from neuroscience could
 

00:01:16.170 --> 00:01:17.660
these findings from neuroscience could
be applied to artificial intelligence

00:01:17.660 --> 00:01:17.670
be applied to artificial intelligence
 

00:01:17.670 --> 00:01:20.630
be applied to artificial intelligence
and one of the biggest breakthroughs in

00:01:20.630 --> 00:01:20.640
and one of the biggest breakthroughs in
 

00:01:20.640 --> 00:01:22.850
and one of the biggest breakthroughs in
our understanding of the neural basis of

00:01:22.850 --> 00:01:22.860
our understanding of the neural basis of
 

00:01:22.860 --> 00:01:25.010
our understanding of the neural basis of
vision came from to scientists as at

00:01:25.010 --> 00:01:25.020
vision came from to scientists as at
 

00:01:25.020 --> 00:01:29.000
vision came from to scientists as at
Harvard Hubel and Wiesel and they had a

00:01:29.000 --> 00:01:29.010
Harvard Hubel and Wiesel and they had a
 

00:01:29.010 --> 00:01:31.940
Harvard Hubel and Wiesel and they had a
pretty simple experimental setup where

00:01:31.940 --> 00:01:31.950
pretty simple experimental setup where
 

00:01:31.950 --> 00:01:33.710
pretty simple experimental setup where
they will where they were able to

00:01:33.710 --> 00:01:33.720
they will where they were able to
 

00:01:33.720 --> 00:01:37.700
they will where they were able to
measure visual neural activity in the

00:01:37.700 --> 00:01:37.710
measure visual neural activity in the
 

00:01:37.710 --> 00:01:40.250
measure visual neural activity in the
visual cortex of cats by recording

00:01:40.250 --> 00:01:40.260
visual cortex of cats by recording
 

00:01:40.260 --> 00:01:43.730
visual cortex of cats by recording
directly for the electrical signals from

00:01:43.730 --> 00:01:43.740
directly for the electrical signals from
 

00:01:43.740 --> 00:01:47.030
directly for the electrical signals from
neurons in this region of the brain and

00:01:47.030 --> 00:01:47.040
neurons in this region of the brain and
 

00:01:47.040 --> 00:01:48.740
neurons in this region of the brain and
they displayed a simple stimulus on a

00:01:48.740 --> 00:01:48.750
they displayed a simple stimulus on a
 

00:01:48.750 --> 00:01:51.560
they displayed a simple stimulus on a
screen and then probed the visual cortex

00:01:51.560 --> 00:01:51.570
screen and then probed the visual cortex
 

00:01:51.570 --> 00:01:54.500
screen and then probed the visual cortex
to see which neurons fired in response

00:01:54.500 --> 00:01:54.510
to see which neurons fired in response
 

00:01:54.510 --> 00:01:57.800
to see which neurons fired in response
to that stimulus there were a few key

00:01:57.800 --> 00:01:57.810
to that stimulus there were a few key
 

00:01:57.810 --> 00:01:59.929
to that stimulus there were a few key
takeaways from this experiment that I'd

00:01:59.929 --> 00:01:59.939
takeaways from this experiment that I'd
 

00:01:59.939 --> 00:02:01.580
takeaways from this experiment that I'd
like you to keep in mind as we go

00:02:01.580 --> 00:02:01.590
like you to keep in mind as we go
 

00:02:01.590 --> 00:02:04.190
like you to keep in mind as we go
through today's lecture the first was

00:02:04.190 --> 00:02:04.200
through today's lecture the first was
 

00:02:04.200 --> 00:02:05.510
through today's lecture the first was
that they found that there was an

00:02:05.510 --> 00:02:05.520
that they found that there was an
 

00:02:05.520 --> 00:02:08.960
that they found that there was an
mechanism for spatial invariance they

00:02:08.960 --> 00:02:08.970
mechanism for spatial invariance they
 

00:02:08.970 --> 00:02:11.029
mechanism for spatial invariance they
could record neural responses to

00:02:11.029 --> 00:02:11.039
could record neural responses to
 

00:02:11.039 --> 00:02:13.729
could record neural responses to
particular patterns and this was

00:02:13.729 --> 00:02:13.739
particular patterns and this was
 

00:02:13.739 --> 00:02:15.470
particular patterns and this was
constant regardless of the location

00:02:15.470 --> 00:02:15.480
constant regardless of the location
 

00:02:15.480 --> 00:02:18.680
constant regardless of the location
of those patterns on the screen the

00:02:18.680 --> 00:02:18.690
of those patterns on the screen the
 

00:02:18.690 --> 00:02:20.300
of those patterns on the screen the
second was that the neurons they

00:02:20.300 --> 00:02:20.310
second was that the neurons they
 

00:02:20.310 --> 00:02:22.369
second was that the neurons they
recorded from had what they called a

00:02:22.369 --> 00:02:22.379
recorded from had what they called a
 

00:02:22.379 --> 00:02:25.369
recorded from had what they called a
receptive field they certain neurons

00:02:25.369 --> 00:02:25.379
receptive field they certain neurons
 

00:02:25.379 --> 00:02:27.589
receptive field they certain neurons
only responded to certain regions of the

00:02:27.589 --> 00:02:27.599
only responded to certain regions of the
 

00:02:27.599 --> 00:02:29.690
only responded to certain regions of the
input while others responded to other

00:02:29.690 --> 00:02:29.700
input while others responded to other
 

00:02:29.700 --> 00:02:32.570
input while others responded to other
regions finally they were able to tease

00:02:32.570 --> 00:02:32.580
regions finally they were able to tease
 

00:02:32.580 --> 00:02:35.119
regions finally they were able to tease
out that there was a mapping a hierarchy

00:02:35.119 --> 00:02:35.129
out that there was a mapping a hierarchy
 

00:02:35.129 --> 00:02:37.430
out that there was a mapping a hierarchy
to neural organization in the visual

00:02:37.430 --> 00:02:37.440
to neural organization in the visual
 

00:02:37.440 --> 00:02:40.220
to neural organization in the visual
cortex there are cells that responded to

00:02:40.220 --> 00:02:40.230
cortex there are cells that responded to
 

00:02:40.230 --> 00:02:42.229
cortex there are cells that responded to
more simple images such as rods and

00:02:42.229 --> 00:02:42.239
more simple images such as rods and
 

00:02:42.239 --> 00:02:44.630
more simple images such as rods and
rectangles and then downstream layers of

00:02:44.630 --> 00:02:44.640
rectangles and then downstream layers of
 

00:02:44.640 --> 00:02:46.940
rectangles and then downstream layers of
neurons they use the activations from

00:02:46.940 --> 00:02:46.950
neurons they use the activations from
 

00:02:46.950 --> 00:02:48.530
neurons they use the activations from
these upstream neurons in their

00:02:48.530 --> 00:02:48.540
these upstream neurons in their
 

00:02:48.540 --> 00:02:52.699
these upstream neurons in their
computation cognitive scientists and

00:02:52.699 --> 00:02:52.709
computation cognitive scientists and
 

00:02:52.709 --> 00:02:54.619
computation cognitive scientists and
neuroscientists have since built off

00:02:54.619 --> 00:02:54.629
neuroscientists have since built off
 

00:02:54.629 --> 00:02:57.140
neuroscientists have since built off
this early work to indeed confirm that

00:02:57.140 --> 00:02:57.150
this early work to indeed confirm that
 

00:02:57.150 --> 00:02:59.599
this early work to indeed confirm that
the visual cortex is organized into

00:02:59.599 --> 00:02:59.609
the visual cortex is organized into
 

00:02:59.609 --> 00:03:02.150
the visual cortex is organized into
layers and this hierarchy of layers

00:03:02.150 --> 00:03:02.160
layers and this hierarchy of layers
 

00:03:02.160 --> 00:03:04.130
layers and this hierarchy of layers
allows for the recognition of

00:03:04.130 --> 00:03:04.140
allows for the recognition of
 

00:03:04.140 --> 00:03:06.740
allows for the recognition of
increasingly complex features that for

00:03:06.740 --> 00:03:06.750
increasingly complex features that for
 

00:03:06.750 --> 00:03:08.900
increasingly complex features that for
example allow us to immediately

00:03:08.900 --> 00:03:08.910
example allow us to immediately
 

00:03:08.910 --> 00:03:12.619
example allow us to immediately
recognize the face of a friend so now

00:03:12.619 --> 00:03:12.629
recognize the face of a friend so now
 

00:03:12.629 --> 00:03:15.140
recognize the face of a friend so now
that we've gotten a sense at a very high

00:03:15.140 --> 00:03:15.150
that we've gotten a sense at a very high
 

00:03:15.150 --> 00:03:17.569
that we've gotten a sense at a very high
level of how our brains process visual

00:03:17.569 --> 00:03:17.579
level of how our brains process visual
 

00:03:17.579 --> 00:03:20.300
level of how our brains process visual
information we can turn our attention to

00:03:20.300 --> 00:03:20.310
information we can turn our attention to
 

00:03:20.310 --> 00:03:24.140
information we can turn our attention to
what computers see how does a computer

00:03:24.140 --> 00:03:24.150
what computers see how does a computer
 

00:03:24.150 --> 00:03:27.559
what computers see how does a computer
process an image well to a computer

00:03:27.559 --> 00:03:27.569
process an image well to a computer
 

00:03:27.569 --> 00:03:30.020
process an image well to a computer
images are just numbers so suppose we

00:03:30.020 --> 00:03:30.030
images are just numbers so suppose we
 

00:03:30.030 --> 00:03:33.170
images are just numbers so suppose we
have a picture of Abraham Lincoln it's

00:03:33.170 --> 00:03:33.180
have a picture of Abraham Lincoln it's
 

00:03:33.180 --> 00:03:35.659
have a picture of Abraham Lincoln it's
made up of pixels and since this is a

00:03:35.659 --> 00:03:35.669
made up of pixels and since this is a
 

00:03:35.669 --> 00:03:38.089
made up of pixels and since this is a
grayscale image each of these pixels can

00:03:38.089 --> 00:03:38.099
grayscale image each of these pixels can
 

00:03:38.099 --> 00:03:40.930
grayscale image each of these pixels can
be represented by just a single number

00:03:40.930 --> 00:03:40.940
be represented by just a single number
 

00:03:40.940 --> 00:03:44.030
be represented by just a single number
so we can represent our image as a 2d

00:03:44.030 --> 00:03:44.040
so we can represent our image as a 2d
 

00:03:44.040 --> 00:03:46.400
so we can represent our image as a 2d
array of numbers one for each pixel in

00:03:46.400 --> 00:03:46.410
array of numbers one for each pixel in
 

00:03:46.410 --> 00:03:49.490
array of numbers one for each pixel in
the image and if we were to have a RGB

00:03:49.490 --> 00:03:49.500
the image and if we were to have a RGB
 

00:03:49.500 --> 00:03:51.860
the image and if we were to have a RGB
color image not grayscale we can

00:03:51.860 --> 00:03:51.870
color image not grayscale we can
 

00:03:51.870 --> 00:03:54.140
color image not grayscale we can
represent that with a 3d array where we

00:03:54.140 --> 00:03:54.150
represent that with a 3d array where we
 

00:03:54.150 --> 00:03:57.080
represent that with a 3d array where we
have two D matrices for each of our g

00:03:57.080 --> 00:03:57.090
have two D matrices for each of our g
 

00:03:57.090 --> 00:04:00.170
have two D matrices for each of our g
and b now that we have a way to

00:04:00.170 --> 00:04:00.180
and b now that we have a way to
 

00:04:00.180 --> 00:04:02.750
and b now that we have a way to
represent images two computers we can

00:04:02.750 --> 00:04:02.760
represent images two computers we can
 

00:04:02.760 --> 00:04:05.000
represent images two computers we can
next think about what types of computer

00:04:05.000 --> 00:04:05.010
next think about what types of computer
 

00:04:05.010 --> 00:04:08.030
next think about what types of computer
vision tasks we can perform and in

00:04:08.030 --> 00:04:08.040
vision tasks we can perform and in
 

00:04:08.040 --> 00:04:10.309
vision tasks we can perform and in
machine learning more broadly we can

00:04:10.309 --> 00:04:10.319
machine learning more broadly we can
 

00:04:10.319 --> 00:04:13.099
machine learning more broadly we can
think of tasks of regression and those

00:04:13.099 --> 00:04:13.109
think of tasks of regression and those
 

00:04:13.109 --> 00:04:16.430
think of tasks of regression and those
of classification in regression our

00:04:16.430 --> 00:04:16.440
of classification in regression our
 

00:04:16.440 --> 00:04:18.409
of classification in regression our
output takes a continuous value and

00:04:18.409 --> 00:04:18.419
output takes a continuous value and
 

00:04:18.419 --> 00:04:20.180
output takes a continuous value and
named classification a single class

00:04:20.180 --> 00:04:20.190
named classification a single class
 

00:04:20.190 --> 00:04:22.879
named classification a single class
label so let's consider the task of

00:04:22.879 --> 00:04:22.889
label so let's consider the task of
 

00:04:22.889 --> 00:04:25.550
label so let's consider the task of
image classification we want to predict

00:04:25.550 --> 00:04:25.560
image classification we want to predict
 

00:04:25.560 --> 00:04:27.920
image classification we want to predict
a single label for a given input image

00:04:27.920 --> 00:04:27.930
a single label for a given input image
 

00:04:27.930 --> 00:04:29.100
a single label for a given input image
for example

00:04:29.100 --> 00:04:29.110
for example
 

00:04:29.110 --> 00:04:30.990
for example
let's say we have a bunch of images of

00:04:30.990 --> 00:04:31.000
let's say we have a bunch of images of
 

00:04:31.000 --> 00:04:33.330
let's say we have a bunch of images of
US presidents and we want to build a

00:04:33.330 --> 00:04:33.340
US presidents and we want to build a
 

00:04:33.340 --> 00:04:35.850
US presidents and we want to build a
classification pipeline to tell us which

00:04:35.850 --> 00:04:35.860
classification pipeline to tell us which
 

00:04:35.860 --> 00:04:38.189
classification pipeline to tell us which
President is in an image outputting the

00:04:38.189 --> 00:04:38.199
President is in an image outputting the
 

00:04:38.199 --> 00:04:40.649
President is in an image outputting the
probability that the image is of a

00:04:40.649 --> 00:04:40.659
probability that the image is of a
 

00:04:40.659 --> 00:04:43.950
probability that the image is of a
particular President so in order to be

00:04:43.950 --> 00:04:43.960
particular President so in order to be
 

00:04:43.960 --> 00:04:46.110
particular President so in order to be
able to classify these images our

00:04:46.110 --> 00:04:46.120
able to classify these images our
 

00:04:46.120 --> 00:04:48.240
able to classify these images our
pipeline needs to be able to tell what

00:04:48.240 --> 00:04:48.250
pipeline needs to be able to tell what
 

00:04:48.250 --> 00:04:49.909
pipeline needs to be able to tell what
is unique about a picture of Lincoln

00:04:49.909 --> 00:04:49.919
is unique about a picture of Lincoln
 

00:04:49.919 --> 00:04:52.589
is unique about a picture of Lincoln
versus a picture of Washington versus a

00:04:52.589 --> 00:04:52.599
versus a picture of Washington versus a
 

00:04:52.599 --> 00:04:55.770
versus a picture of Washington versus a
picture of Obama and another way to

00:04:55.770 --> 00:04:55.780
picture of Obama and another way to
 

00:04:55.780 --> 00:04:58.589
picture of Obama and another way to
think about this problem at a high level

00:04:58.589 --> 00:04:58.599
think about this problem at a high level
 

00:04:58.599 --> 00:05:00.360
think about this problem at a high level
is in terms of the features that are

00:05:00.360 --> 00:05:00.370
is in terms of the features that are
 

00:05:00.370 --> 00:05:02.430
is in terms of the features that are
characteristic of a particular class of

00:05:02.430 --> 00:05:02.440
characteristic of a particular class of
 

00:05:02.440 --> 00:05:05.100
characteristic of a particular class of
images classification would then be done

00:05:05.100 --> 00:05:05.110
images classification would then be done
 

00:05:05.110 --> 00:05:07.140
images classification would then be done
by detecting the presence of these

00:05:07.140 --> 00:05:07.150
by detecting the presence of these
 

00:05:07.150 --> 00:05:10.890
by detecting the presence of these
features in a given image if the if the

00:05:10.890 --> 00:05:10.900
features in a given image if the if the
 

00:05:10.900 --> 00:05:13.350
features in a given image if the if the
features for a particular image are all

00:05:13.350 --> 00:05:13.360
features for a particular image are all
 

00:05:13.360 --> 00:05:15.990
features for a particular image are all
present in an image we can then predict

00:05:15.990 --> 00:05:16.000
present in an image we can then predict
 

00:05:16.000 --> 00:05:19.980
present in an image we can then predict
that class with a high probability so

00:05:19.980 --> 00:05:19.990
that class with a high probability so
 

00:05:19.990 --> 00:05:22.439
that class with a high probability so
for our image classification pipeline

00:05:22.439 --> 00:05:22.449
for our image classification pipeline
 

00:05:22.449 --> 00:05:25.469
for our image classification pipeline
our model needs to know first what the

00:05:25.469 --> 00:05:25.479
our model needs to know first what the
 

00:05:25.479 --> 00:05:28.200
our model needs to know first what the
features are and secondly it needs to be

00:05:28.200 --> 00:05:28.210
features are and secondly it needs to be
 

00:05:28.210 --> 00:05:30.179
features are and secondly it needs to be
able to detect those features in an

00:05:30.179 --> 00:05:30.189
able to detect those features in an
 

00:05:30.189 --> 00:05:33.629
able to detect those features in an
image so one way we can solve this is to

00:05:33.629 --> 00:05:33.639
image so one way we can solve this is to
 

00:05:33.639 --> 00:05:35.159
image so one way we can solve this is to
leverage our knowledge about a

00:05:35.159 --> 00:05:35.169
leverage our knowledge about a
 

00:05:35.169 --> 00:05:37.680
leverage our knowledge about a
particular field and use this to

00:05:37.680 --> 00:05:37.690
particular field and use this to
 

00:05:37.690 --> 00:05:39.629
particular field and use this to
manually define the features ourselves

00:05:39.629 --> 00:05:39.639
manually define the features ourselves
 

00:05:39.639 --> 00:05:42.809
manually define the features ourselves
and classification pipeline would then

00:05:42.809 --> 00:05:42.819
and classification pipeline would then
 

00:05:42.819 --> 00:05:45.089
and classification pipeline would then
try to detect these manually defined

00:05:45.089 --> 00:05:45.099
try to detect these manually defined
 

00:05:45.099 --> 00:05:47.790
try to detect these manually defined
features in images and use the results

00:05:47.790 --> 00:05:47.800
features in images and use the results
 

00:05:47.800 --> 00:05:49.529
features in images and use the results
of some detection algorithm for

00:05:49.529 --> 00:05:49.539
of some detection algorithm for
 

00:05:49.539 --> 00:05:52.529
of some detection algorithm for
classification but there is a big

00:05:52.529 --> 00:05:52.539
classification but there is a big
 

00:05:52.539 --> 00:05:55.290
classification but there is a big
problem with this approach remember that

00:05:55.290 --> 00:05:55.300
problem with this approach remember that
 

00:05:55.300 --> 00:05:58.050
problem with this approach remember that
images are just 3d arrays of brightness

00:05:58.050 --> 00:05:58.060
images are just 3d arrays of brightness
 

00:05:58.060 --> 00:06:01.439
images are just 3d arrays of brightness
values and image data has a lot of

00:06:01.439 --> 00:06:01.449
values and image data has a lot of
 

00:06:01.449 --> 00:06:04.230
values and image data has a lot of
variation occlusion variations in

00:06:04.230 --> 00:06:04.240
variation occlusion variations in
 

00:06:04.240 --> 00:06:06.209
variation occlusion variations in
illumination viewpoint variation

00:06:06.209 --> 00:06:06.219
illumination viewpoint variation
 

00:06:06.219 --> 00:06:09.059
illumination viewpoint variation
intraclass variation and our

00:06:09.059 --> 00:06:09.069
intraclass variation and our
 

00:06:09.069 --> 00:06:10.890
intraclass variation and our
classification pipeline has to be

00:06:10.890 --> 00:06:10.900
classification pipeline has to be
 

00:06:10.900 --> 00:06:13.170
classification pipeline has to be
invariant to all these variations while

00:06:13.170 --> 00:06:13.180
invariant to all these variations while
 

00:06:13.180 --> 00:06:15.510
invariant to all these variations while
still being sensitive to the variability

00:06:15.510 --> 00:06:15.520
still being sensitive to the variability
 

00:06:15.520 --> 00:06:19.379
still being sensitive to the variability
between different classes even though

00:06:19.379 --> 00:06:19.389
between different classes even though
 

00:06:19.389 --> 00:06:21.600
between different classes even though
our pipeline could use features that we

00:06:21.600 --> 00:06:21.610
our pipeline could use features that we
 

00:06:21.610 --> 00:06:24.300
our pipeline could use features that we
the human define where this manual

00:06:24.300 --> 00:06:24.310
the human define where this manual
 

00:06:24.310 --> 00:06:26.129
the human define where this manual
extraction would break down is in the

00:06:26.129 --> 00:06:26.139
extraction would break down is in the
 

00:06:26.139 --> 00:06:29.339
extraction would break down is in the
detection task itself and this is due to

00:06:29.339 --> 00:06:29.349
detection task itself and this is due to
 

00:06:29.349 --> 00:06:31.469
detection task itself and this is due to
the incredible variability that I just

00:06:31.469 --> 00:06:31.479
the incredible variability that I just
 

00:06:31.479 --> 00:06:33.959
the incredible variability that I just
mentioned the detection of these

00:06:33.959 --> 00:06:33.969
mentioned the detection of these
 

00:06:33.969 --> 00:06:35.579
mentioned the detection of these
features would actually be really

00:06:35.579 --> 00:06:35.589
features would actually be really
 

00:06:35.589 --> 00:06:37.949
features would actually be really
difficult in practice because your

00:06:37.949 --> 00:06:37.959
difficult in practice because your
 

00:06:37.959 --> 00:06:39.899
difficult in practice because your
detection algorithm would need to

00:06:39.899 --> 00:06:39.909
detection algorithm would need to
 

00:06:39.909 --> 00:06:41.050
detection algorithm would need to
withstand each

00:06:41.050 --> 00:06:41.060
withstand each
 

00:06:41.060 --> 00:06:42.879
withstand each
and every one of these different

00:06:42.879 --> 00:06:42.889
and every one of these different
 

00:06:42.889 --> 00:06:47.080
and every one of these different
variations so how can we do better we

00:06:47.080 --> 00:06:47.090
variations so how can we do better we
 

00:06:47.090 --> 00:06:49.570
variations so how can we do better we
want a way to both extract features and

00:06:49.570 --> 00:06:49.580
want a way to both extract features and
 

00:06:49.580 --> 00:06:51.240
want a way to both extract features and
detect their presence in images

00:06:51.240 --> 00:06:51.250
detect their presence in images
 

00:06:51.250 --> 00:06:53.590
detect their presence in images
automatically in a hierarchical fashion

00:06:53.590 --> 00:06:53.600
automatically in a hierarchical fashion
 

00:06:53.600 --> 00:06:57.159
automatically in a hierarchical fashion
and we can use neural network based

00:06:57.159 --> 00:06:57.169
and we can use neural network based
 

00:06:57.169 --> 00:07:00.100
and we can use neural network based
approaches to do exactly this to learn

00:07:00.100 --> 00:07:00.110
approaches to do exactly this to learn
 

00:07:00.110 --> 00:07:02.260
approaches to do exactly this to learn
visual features directly from image data

00:07:02.260 --> 00:07:02.270
visual features directly from image data
 

00:07:02.270 --> 00:07:04.360
visual features directly from image data
and to learn a hierarchy of these

00:07:04.360 --> 00:07:04.370
and to learn a hierarchy of these
 

00:07:04.370 --> 00:07:06.550
and to learn a hierarchy of these
features to construct an internal

00:07:06.550 --> 00:07:06.560
features to construct an internal
 

00:07:06.560 --> 00:07:09.220
features to construct an internal
representation of the image for example

00:07:09.220 --> 00:07:09.230
representation of the image for example
 

00:07:09.230 --> 00:07:11.409
representation of the image for example
if we wanted to be able to classify

00:07:11.409 --> 00:07:11.419
if we wanted to be able to classify
 

00:07:11.419 --> 00:07:14.500
if we wanted to be able to classify
images of faces maybe we could first

00:07:14.500 --> 00:07:14.510
images of faces maybe we could first
 

00:07:14.510 --> 00:07:16.690
images of faces maybe we could first
learn and detect low-level features like

00:07:16.690 --> 00:07:16.700
learn and detect low-level features like
 

00:07:16.700 --> 00:07:19.360
learn and detect low-level features like
edges and dark spots mid level features

00:07:19.360 --> 00:07:19.370
edges and dark spots mid level features
 

00:07:19.370 --> 00:07:22.480
edges and dark spots mid level features
eyes ears and noses and then high-level

00:07:22.480 --> 00:07:22.490
eyes ears and noses and then high-level
 

00:07:22.490 --> 00:07:24.550
eyes ears and noses and then high-level
features that actually resemble facial

00:07:24.550 --> 00:07:24.560
features that actually resemble facial
 

00:07:24.560 --> 00:07:28.180
features that actually resemble facial
structure and neural networks will allow

00:07:28.180 --> 00:07:28.190
structure and neural networks will allow
 

00:07:28.190 --> 00:07:31.420
structure and neural networks will allow
us to directly learn visual features in

00:07:31.420 --> 00:07:31.430
us to directly learn visual features in
 

00:07:31.430 --> 00:07:33.250
us to directly learn visual features in
this manner if we construct them

00:07:33.250 --> 00:07:33.260
this manner if we construct them
 

00:07:33.260 --> 00:07:36.969
this manner if we construct them
cleverly so in yesterday's first lecture

00:07:36.969 --> 00:07:36.979
cleverly so in yesterday's first lecture
 

00:07:36.979 --> 00:07:38.740
cleverly so in yesterday's first lecture
we learned about fully connected neural

00:07:38.740 --> 00:07:38.750
we learned about fully connected neural
 

00:07:38.750 --> 00:07:40.659
we learned about fully connected neural
networks where you can have multiple

00:07:40.659 --> 00:07:40.669
networks where you can have multiple
 

00:07:40.669 --> 00:07:43.360
networks where you can have multiple
hidden layers and where each neuron in a

00:07:43.360 --> 00:07:43.370
hidden layers and where each neuron in a
 

00:07:43.370 --> 00:07:45.550
hidden layers and where each neuron in a
given layer is connected to every neuron

00:07:45.550 --> 00:07:45.560
given layer is connected to every neuron
 

00:07:45.560 --> 00:07:48.010
given layer is connected to every neuron
in the subsequent layer let's say we

00:07:48.010 --> 00:07:48.020
in the subsequent layer let's say we
 

00:07:48.020 --> 00:07:49.870
in the subsequent layer let's say we
wanted to use a fully connected and

00:07:49.870 --> 00:07:49.880
wanted to use a fully connected and
 

00:07:49.880 --> 00:07:51.580
wanted to use a fully connected and
neural network like the one you see here

00:07:51.580 --> 00:07:51.590
neural network like the one you see here
 

00:07:51.590 --> 00:07:54.790
neural network like the one you see here
for image classification in this case

00:07:54.790 --> 00:07:54.800
for image classification in this case
 

00:07:54.800 --> 00:07:56.860
for image classification in this case
our input image would be transformed

00:07:56.860 --> 00:07:56.870
our input image would be transformed
 

00:07:56.870 --> 00:07:59.590
our input image would be transformed
into a vector of pixel values fed into

00:07:59.590 --> 00:07:59.600
into a vector of pixel values fed into
 

00:07:59.600 --> 00:08:01.719
into a vector of pixel values fed into
the network and each neuron in the

00:08:01.719 --> 00:08:01.729
the network and each neuron in the
 

00:08:01.729 --> 00:08:03.880
the network and each neuron in the
hidden layer would be connected to all

00:08:03.880 --> 00:08:03.890
hidden layer would be connected to all
 

00:08:03.890 --> 00:08:06.940
hidden layer would be connected to all
neurons in the input layer I hope you

00:08:06.940 --> 00:08:06.950
neurons in the input layer I hope you
 

00:08:06.950 --> 00:08:09.190
neurons in the input layer I hope you
can appreciate that all spatial spatial

00:08:09.190 --> 00:08:09.200
can appreciate that all spatial spatial
 

00:08:09.200 --> 00:08:11.770
can appreciate that all spatial spatial
information from our 2d array would be

00:08:11.770 --> 00:08:11.780
information from our 2d array would be
 

00:08:11.780 --> 00:08:13.170
information from our 2d array would be
completely lost

00:08:13.170 --> 00:08:13.180
completely lost
 

00:08:13.180 --> 00:08:16.000
completely lost
additionally additionally we'd have many

00:08:16.000 --> 00:08:16.010
additionally additionally we'd have many
 

00:08:16.010 --> 00:08:18.219
additionally additionally we'd have many
many parameters because this is a fully

00:08:18.219 --> 00:08:18.229
many parameters because this is a fully
 

00:08:18.229 --> 00:08:20.830
many parameters because this is a fully
connected network so it's not going to

00:08:20.830 --> 00:08:20.840
connected network so it's not going to
 

00:08:20.840 --> 00:08:22.900
connected network so it's not going to
be really feasible to implement such a

00:08:22.900 --> 00:08:22.910
be really feasible to implement such a
 

00:08:22.910 --> 00:08:27.340
be really feasible to implement such a
network in practice so how can we use

00:08:27.340 --> 00:08:27.350
network in practice so how can we use
 

00:08:27.350 --> 00:08:29.860
network in practice so how can we use
the spatial structure that's inherent in

00:08:29.860 --> 00:08:29.870
the spatial structure that's inherent in
 

00:08:29.870 --> 00:08:32.050
the spatial structure that's inherent in
the input to inform the architecture of

00:08:32.050 --> 00:08:32.060
the input to inform the architecture of
 

00:08:32.060 --> 00:08:36.850
the input to inform the architecture of
our network the key insight in how we

00:08:36.850 --> 00:08:36.860
our network the key insight in how we
 

00:08:36.860 --> 00:08:40.269
our network the key insight in how we
can do this is to connect patches of the

00:08:40.269 --> 00:08:40.279
can do this is to connect patches of the
 

00:08:40.279 --> 00:08:43.449
can do this is to connect patches of the
input represented as a 2d array to

00:08:43.449 --> 00:08:43.459
input represented as a 2d array to
 

00:08:43.459 --> 00:08:46.240
input represented as a 2d array to
neurons in hidden layers this is to say

00:08:46.240 --> 00:08:46.250
neurons in hidden layers this is to say
 

00:08:46.250 --> 00:08:48.100
neurons in hidden layers this is to say
that each neuron in a hidden layer only

00:08:48.100 --> 00:08:48.110
that each neuron in a hidden layer only
 

00:08:48.110 --> 00:08:50.829
that each neuron in a hidden layer only
sees a particular region of what the

00:08:50.829 --> 00:08:50.839
sees a particular region of what the
 

00:08:50.839 --> 00:08:53.530
sees a particular region of what the
input to that layer is this will not

00:08:53.530 --> 00:08:53.540
input to that layer is this will not
 

00:08:53.540 --> 00:08:54.830
input to that layer is this will not
only reduce the number of

00:08:54.830 --> 00:08:54.840
only reduce the number of
 

00:08:54.840 --> 00:08:57.140
only reduce the number of
in our network but also allow us to

00:08:57.140 --> 00:08:57.150
in our network but also allow us to
 

00:08:57.150 --> 00:08:59.450
in our network but also allow us to
leverage the fact that in an image

00:08:59.450 --> 00:08:59.460
leverage the fact that in an image
 

00:08:59.460 --> 00:09:01.610
leverage the fact that in an image
pixels that are near each other are

00:09:01.610 --> 00:09:01.620
pixels that are near each other are
 

00:09:01.620 --> 00:09:05.510
pixels that are near each other are
probably somehow related to define

00:09:05.510 --> 00:09:05.520
probably somehow related to define
 

00:09:05.520 --> 00:09:08.060
probably somehow related to define
connections across the whole input we

00:09:08.060 --> 00:09:08.070
connections across the whole input we
 

00:09:08.070 --> 00:09:10.790
connections across the whole input we
can apply this same principle by sliding

00:09:10.790 --> 00:09:10.800
can apply this same principle by sliding
 

00:09:10.800 --> 00:09:13.520
can apply this same principle by sliding
up the patch window across the entirety

00:09:13.520 --> 00:09:13.530
up the patch window across the entirety
 

00:09:13.530 --> 00:09:16.100
up the patch window across the entirety
of the input image in this case by two

00:09:16.100 --> 00:09:16.110
of the input image in this case by two
 

00:09:16.110 --> 00:09:18.980
of the input image in this case by two
units in this way we'll take into

00:09:18.980 --> 00:09:18.990
units in this way we'll take into
 

00:09:18.990 --> 00:09:20.720
units in this way we'll take into
account the spatial structure that's

00:09:20.720 --> 00:09:20.730
account the spatial structure that's
 

00:09:20.730 --> 00:09:24.620
account the spatial structure that's
present but remember our ultimate task

00:09:24.620 --> 00:09:24.630
present but remember our ultimate task
 

00:09:24.630 --> 00:09:27.860
present but remember our ultimate task
is to learn visual features and we can

00:09:27.860 --> 00:09:27.870
is to learn visual features and we can
 

00:09:27.870 --> 00:09:29.840
is to learn visual features and we can
do this by smartly weighting the

00:09:29.840 --> 00:09:29.850
do this by smartly weighting the
 

00:09:29.850 --> 00:09:33.280
do this by smartly weighting the
connections between a patch of our input

00:09:33.280 --> 00:09:33.290
connections between a patch of our input
 

00:09:33.290 --> 00:09:36.080
connections between a patch of our input
to the neuron it's connected to in the

00:09:36.080 --> 00:09:36.090
to the neuron it's connected to in the
 

00:09:36.090 --> 00:09:38.240
to the neuron it's connected to in the
next hidden layer so as to detect

00:09:38.240 --> 00:09:38.250
next hidden layer so as to detect
 

00:09:38.250 --> 00:09:40.700
next hidden layer so as to detect
particular features present in that

00:09:40.700 --> 00:09:40.710
particular features present in that
 

00:09:40.710 --> 00:09:44.750
particular features present in that
input and essentially what this amounts

00:09:44.750 --> 00:09:44.760
input and essentially what this amounts
 

00:09:44.760 --> 00:09:47.300
input and essentially what this amounts
to is applying a filter a set of weights

00:09:47.300 --> 00:09:47.310
to is applying a filter a set of weights
 

00:09:47.310 --> 00:09:52.010
to is applying a filter a set of weights
to extract local features and it would

00:09:52.010 --> 00:09:52.020
to extract local features and it would
 

00:09:52.020 --> 00:09:53.780
to extract local features and it would
be useful for us in our classification

00:09:53.780 --> 00:09:53.790
be useful for us in our classification
 

00:09:53.790 --> 00:09:56.540
be useful for us in our classification
pipeline to have many different features

00:09:56.540 --> 00:09:56.550
pipeline to have many different features
 

00:09:56.550 --> 00:09:58.640
pipeline to have many different features
to work with and we can do this by using

00:09:58.640 --> 00:09:58.650
to work with and we can do this by using
 

00:09:58.650 --> 00:10:01.700
to work with and we can do this by using
multiple filters multiple sets of

00:10:01.700 --> 00:10:01.710
multiple filters multiple sets of
 

00:10:01.710 --> 00:10:05.090
multiple filters multiple sets of
weights and finally we want to be able

00:10:05.090 --> 00:10:05.100
weights and finally we want to be able
 

00:10:05.100 --> 00:10:07.130
weights and finally we want to be able
to share the parameters of each filter

00:10:07.130 --> 00:10:07.140
to share the parameters of each filter
 

00:10:07.140 --> 00:10:09.740
to share the parameters of each filter
across all the connections between the

00:10:09.740 --> 00:10:09.750
across all the connections between the
 

00:10:09.750 --> 00:10:12.290
across all the connections between the
input layer and the next layer because

00:10:12.290 --> 00:10:12.300
input layer and the next layer because
 

00:10:12.300 --> 00:10:15.020
input layer and the next layer because
the features that matter in one part of

00:10:15.020 --> 00:10:15.030
the features that matter in one part of
 

00:10:15.030 --> 00:10:16.970
the features that matter in one part of
the input should matter elsewhere this

00:10:16.970 --> 00:10:16.980
the input should matter elsewhere this
 

00:10:16.980 --> 00:10:20.030
the input should matter elsewhere this
is the same concept of spatial

00:10:20.030 --> 00:10:20.040
is the same concept of spatial
 

00:10:20.040 --> 00:10:23.680
is the same concept of spatial
invariants that I alluded to earlier in

00:10:23.680 --> 00:10:23.690
invariants that I alluded to earlier in
 

00:10:23.690 --> 00:10:26.870
invariants that I alluded to earlier in
practice this amounts to a patchy

00:10:26.870 --> 00:10:26.880
practice this amounts to a patchy
 

00:10:26.880 --> 00:10:30.620
practice this amounts to a patchy
operation known as convolution let's

00:10:30.620 --> 00:10:30.630
operation known as convolution let's
 

00:10:30.630 --> 00:10:32.200
operation known as convolution let's
first think about this at a high level

00:10:32.200 --> 00:10:32.210
first think about this at a high level
 

00:10:32.210 --> 00:10:34.750
first think about this at a high level
suppose we have a four by four filter

00:10:34.750 --> 00:10:34.760
suppose we have a four by four filter
 

00:10:34.760 --> 00:10:37.370
suppose we have a four by four filter
which means we have 16 different weights

00:10:37.370 --> 00:10:37.380
which means we have 16 different weights
 

00:10:37.380 --> 00:10:39.920
which means we have 16 different weights
and we're going to apply the same filter

00:10:39.920 --> 00:10:39.930
and we're going to apply the same filter
 

00:10:39.930 --> 00:10:42.610
and we're going to apply the same filter
to four by four patches in the input and

00:10:42.610 --> 00:10:42.620
to four by four patches in the input and
 

00:10:42.620 --> 00:10:45.530
to four by four patches in the input and
use the result of that operation to

00:10:45.530 --> 00:10:45.540
use the result of that operation to
 

00:10:45.540 --> 00:10:47.810
use the result of that operation to
somehow influence the state of the

00:10:47.810 --> 00:10:47.820
somehow influence the state of the
 

00:10:47.820 --> 00:10:49.670
somehow influence the state of the
neuron in the next layer that this patch

00:10:49.670 --> 00:10:49.680
neuron in the next layer that this patch
 

00:10:49.680 --> 00:10:52.490
neuron in the next layer that this patch
is connected to then we're going to

00:10:52.490 --> 00:10:52.500
is connected to then we're going to
 

00:10:52.500 --> 00:10:56.480
is connected to then we're going to
shift our filter by two pixels as for

00:10:56.480 --> 00:10:56.490
shift our filter by two pixels as for
 

00:10:56.490 --> 00:10:58.670
shift our filter by two pixels as for
example and grab the next patch of the

00:10:58.670 --> 00:10:58.680
example and grab the next patch of the
 

00:10:58.680 --> 00:11:02.480
example and grab the next patch of the
input so in this way we can start

00:11:02.480 --> 00:11:02.490
input so in this way we can start
 

00:11:02.490 --> 00:11:04.340
input so in this way we can start
thinking about convolution at a very

00:11:04.340 --> 00:11:04.350
thinking about convolution at a very
 

00:11:04.350 --> 00:11:07.250
thinking about convolution at a very
high level but you're probably wondering

00:11:07.250 --> 00:11:07.260
high level but you're probably wondering
 

00:11:07.260 --> 00:11:09.120
high level but you're probably wondering
how does this actually work

00:11:09.120 --> 00:11:09.130
how does this actually work
 

00:11:09.130 --> 00:11:11.610
how does this actually work
what do we mean by features and how does

00:11:11.610 --> 00:11:11.620
what do we mean by features and how does
 

00:11:11.620 --> 00:11:13.860
what do we mean by features and how does
this convolution operation allow us to

00:11:13.860 --> 00:11:13.870
this convolution operation allow us to
 

00:11:13.870 --> 00:11:16.590
this convolution operation allow us to
extract them hopefully we can make this

00:11:16.590 --> 00:11:16.600
extract them hopefully we can make this
 

00:11:16.600 --> 00:11:18.660
extract them hopefully we can make this
concrete by walking through a couple of

00:11:18.660 --> 00:11:18.670
concrete by walking through a couple of
 

00:11:18.670 --> 00:11:22.650
concrete by walking through a couple of
examples suppose we want to classify X's

00:11:22.650 --> 00:11:22.660
examples suppose we want to classify X's
 

00:11:22.660 --> 00:11:25.080
examples suppose we want to classify X's
from a set of black and white images of

00:11:25.080 --> 00:11:25.090
from a set of black and white images of
 

00:11:25.090 --> 00:11:27.450
from a set of black and white images of
letters where black is equal to negative

00:11:27.450 --> 00:11:27.460
letters where black is equal to negative
 

00:11:27.460 --> 00:11:30.600
letters where black is equal to negative
1 and y is represented by a value of 1

00:11:30.600 --> 00:11:30.610
1 and y is represented by a value of 1
 

00:11:30.610 --> 00:11:33.930
1 and y is represented by a value of 1
for classification it would clearly not

00:11:33.930 --> 00:11:33.940
for classification it would clearly not
 

00:11:33.940 --> 00:11:36.240
for classification it would clearly not
be possible to simply compare the two

00:11:36.240 --> 00:11:36.250
be possible to simply compare the two
 

00:11:36.250 --> 00:11:39.750
be possible to simply compare the two
matrices to see if they're equal we want

00:11:39.750 --> 00:11:39.760
matrices to see if they're equal we want
 

00:11:39.760 --> 00:11:43.350
matrices to see if they're equal we want
to be able to classify an X as an X even

00:11:43.350 --> 00:11:43.360
to be able to classify an X as an X even
 

00:11:43.360 --> 00:11:47.040
to be able to classify an X as an X even
if it's shifted shrunk rotated deformed

00:11:47.040 --> 00:11:47.050
if it's shifted shrunk rotated deformed
 

00:11:47.050 --> 00:11:50.970
if it's shifted shrunk rotated deformed
transformed in some way we want our

00:11:50.970 --> 00:11:50.980
transformed in some way we want our
 

00:11:50.980 --> 00:11:52.980
transformed in some way we want our
model to compare the images of an X

00:11:52.980 --> 00:11:52.990
model to compare the images of an X
 

00:11:52.990 --> 00:11:55.500
model to compare the images of an X
piece by piece and look for these

00:11:55.500 --> 00:11:55.510
piece by piece and look for these
 

00:11:55.510 --> 00:11:59.250
piece by piece and look for these
important pieces that define an X as an

00:11:59.250 --> 00:11:59.260
important pieces that define an X as an
 

00:11:59.260 --> 00:12:02.130
important pieces that define an X as an
X those are the features and if our

00:12:02.130 --> 00:12:02.140
X those are the features and if our
 

00:12:02.140 --> 00:12:04.290
X those are the features and if our
model can find rough feature matches in

00:12:04.290 --> 00:12:04.300
model can find rough feature matches in
 

00:12:04.300 --> 00:12:06.300
model can find rough feature matches in
roughly the same positions in two

00:12:06.300 --> 00:12:06.310
roughly the same positions in two
 

00:12:06.310 --> 00:12:08.910
roughly the same positions in two
different images it can get a lot better

00:12:08.910 --> 00:12:08.920
different images it can get a lot better
 

00:12:08.920 --> 00:12:10.950
different images it can get a lot better
at seeing the similarity between

00:12:10.950 --> 00:12:10.960
at seeing the similarity between
 

00:12:10.960 --> 00:12:15.390
at seeing the similarity between
different examples of X's you can think

00:12:15.390 --> 00:12:15.400
different examples of X's you can think
 

00:12:15.400 --> 00:12:18.140
different examples of X's you can think
of each feature as a mini image a small

00:12:18.140 --> 00:12:18.150
of each feature as a mini image a small
 

00:12:18.150 --> 00:12:20.190
of each feature as a mini image a small
two-dimensional array of values and

00:12:20.190 --> 00:12:20.200
two-dimensional array of values and
 

00:12:20.200 --> 00:12:22.290
two-dimensional array of values and
we're going to use filters to pick up on

00:12:22.290 --> 00:12:22.300
we're going to use filters to pick up on
 

00:12:22.300 --> 00:12:25.800
we're going to use filters to pick up on
the features common to X's in the case

00:12:25.800 --> 00:12:25.810
the features common to X's in the case
 

00:12:25.810 --> 00:12:28.200
the features common to X's in the case
of an X filters that can pick up on

00:12:28.200 --> 00:12:28.210
of an X filters that can pick up on
 

00:12:28.210 --> 00:12:30.630
of an X filters that can pick up on
diagonal lines and a crossing will

00:12:30.630 --> 00:12:30.640
diagonal lines and a crossing will
 

00:12:30.640 --> 00:12:32.840
diagonal lines and a crossing will
probably capture all the important

00:12:32.840 --> 00:12:32.850
probably capture all the important
 

00:12:32.850 --> 00:12:36.420
probably capture all the important
characteristics of most X's so know here

00:12:36.420 --> 00:12:36.430
characteristics of most X's so know here
 

00:12:36.430 --> 00:12:38.700
characteristics of most X's so know here
that these smaller matrices are the

00:12:38.700 --> 00:12:38.710
that these smaller matrices are the
 

00:12:38.710 --> 00:12:41.490
that these smaller matrices are the
filters of weights that we'll use in our

00:12:41.490 --> 00:12:41.500
filters of weights that we'll use in our
 

00:12:41.500 --> 00:12:43.770
filters of weights that we'll use in our
convolution operation to detect the

00:12:43.770 --> 00:12:43.780
convolution operation to detect the
 

00:12:43.780 --> 00:12:46.520
convolution operation to detect the
corresponding features in an input image

00:12:46.520 --> 00:12:46.530
corresponding features in an input image
 

00:12:46.530 --> 00:12:49.680
corresponding features in an input image
so now all that's left is to define this

00:12:49.680 --> 00:12:49.690
so now all that's left is to define this
 

00:12:49.690 --> 00:12:52.410
so now all that's left is to define this
operation that we'll pick up on when

00:12:52.410 --> 00:12:52.420
operation that we'll pick up on when
 

00:12:52.420 --> 00:12:54.920
operation that we'll pick up on when
these features pop up in our image and

00:12:54.920 --> 00:12:54.930
these features pop up in our image and
 

00:12:54.930 --> 00:12:57.800
these features pop up in our image and
that operation is convolution

00:12:57.800 --> 00:12:57.810
that operation is convolution
 

00:12:57.810 --> 00:13:00.270
that operation is convolution
convolution preserves the spatial

00:13:00.270 --> 00:13:00.280
convolution preserves the spatial
 

00:13:00.280 --> 00:13:03.840
convolution preserves the spatial
relationship between pixels by learning

00:13:03.840 --> 00:13:03.850
relationship between pixels by learning
 

00:13:03.850 --> 00:13:06.390
relationship between pixels by learning
image features in small squares of the

00:13:06.390 --> 00:13:06.400
image features in small squares of the
 

00:13:06.400 --> 00:13:09.540
image features in small squares of the
input to do this we perform an

00:13:09.540 --> 00:13:09.550
input to do this we perform an
 

00:13:09.550 --> 00:13:12.270
input to do this we perform an
element-wise multiplication between the

00:13:12.270 --> 00:13:12.280
element-wise multiplication between the
 

00:13:12.280 --> 00:13:14.850
element-wise multiplication between the
filter matrix and a patch of the input

00:13:14.850 --> 00:13:14.860
filter matrix and a patch of the input
 

00:13:14.860 --> 00:13:17.190
filter matrix and a patch of the input
image that's of the same dimensions as

00:13:17.190 --> 00:13:17.200
image that's of the same dimensions as
 

00:13:17.200 --> 00:13:20.310
image that's of the same dimensions as
the filter this results in a 3 by 3

00:13:20.310 --> 00:13:20.320
the filter this results in a 3 by 3
 

00:13:20.320 --> 00:13:22.009
the filter this results in a 3 by 3
matrix in

00:13:22.009 --> 00:13:22.019
matrix in
 

00:13:22.019 --> 00:13:24.139
matrix in
example you see here all entries in this

00:13:24.139 --> 00:13:24.149
example you see here all entries in this
 

00:13:24.149 --> 00:13:26.479
example you see here all entries in this
matrix are won and that's because

00:13:26.479 --> 00:13:26.489
matrix are won and that's because
 

00:13:26.489 --> 00:13:28.369
matrix are won and that's because
there's a perfect correspondence between

00:13:28.369 --> 00:13:28.379
there's a perfect correspondence between
 

00:13:28.379 --> 00:13:31.280
there's a perfect correspondence between
our filter and the region of the input

00:13:31.280 --> 00:13:31.290
our filter and the region of the input
 

00:13:31.290 --> 00:13:34.160
our filter and the region of the input
that we're convolving it with finally we

00:13:34.160 --> 00:13:34.170
that we're convolving it with finally we
 

00:13:34.170 --> 00:13:37.309
that we're convolving it with finally we
sum all the entries of this matrix get 9

00:13:37.309 --> 00:13:37.319
sum all the entries of this matrix get 9
 

00:13:37.319 --> 00:13:39.919
sum all the entries of this matrix get 9
and that's the result of our convolution

00:13:39.919 --> 00:13:39.929
and that's the result of our convolution
 

00:13:39.929 --> 00:13:44.239
and that's the result of our convolution
operation let's consider one more

00:13:44.239 --> 00:13:44.249
operation let's consider one more
 

00:13:44.249 --> 00:13:47.479
operation let's consider one more
example suppose now we want to compute

00:13:47.479 --> 00:13:47.489
example suppose now we want to compute
 

00:13:47.489 --> 00:13:49.789
example suppose now we want to compute
the convolution of a five by five image

00:13:49.789 --> 00:13:49.799
the convolution of a five by five image
 

00:13:49.799 --> 00:13:52.759
the convolution of a five by five image
and a three by three filter to do this

00:13:52.759 --> 00:13:52.769
and a three by three filter to do this
 

00:13:52.769 --> 00:13:55.249
and a three by three filter to do this
we need to cover the entirety of the

00:13:55.249 --> 00:13:55.259
we need to cover the entirety of the
 

00:13:55.259 --> 00:13:58.400
we need to cover the entirety of the
input image by sliding the filter over

00:13:58.400 --> 00:13:58.410
input image by sliding the filter over
 

00:13:58.410 --> 00:14:01.160
input image by sliding the filter over
the image performing the same element

00:14:01.160 --> 00:14:01.170
the image performing the same element
 

00:14:01.170 --> 00:14:04.280
the image performing the same element
wise multiplication and addition let's

00:14:04.280 --> 00:14:04.290
wise multiplication and addition let's
 

00:14:04.290 --> 00:14:06.590
wise multiplication and addition let's
see what this looks like we'll first

00:14:06.590 --> 00:14:06.600
see what this looks like we'll first
 

00:14:06.600 --> 00:14:08.559
see what this looks like we'll first
start off in the upper left corner

00:14:08.559 --> 00:14:08.569
start off in the upper left corner
 

00:14:08.569 --> 00:14:11.840
start off in the upper left corner
element wise multiply this 3 by 3 patch

00:14:11.840 --> 00:14:11.850
element wise multiply this 3 by 3 patch
 

00:14:11.850 --> 00:14:14.479
element wise multiply this 3 by 3 patch
with the entries of the filter and then

00:14:14.479 --> 00:14:14.489
with the entries of the filter and then
 

00:14:14.489 --> 00:14:17.359
with the entries of the filter and then
add this results in the first entry in

00:14:17.359 --> 00:14:17.369
add this results in the first entry in
 

00:14:17.369 --> 00:14:19.429
add this results in the first entry in
our output matrix called

00:14:19.429 --> 00:14:19.439
our output matrix called
 

00:14:19.439 --> 00:14:23.840
our output matrix called
the feature map we'll next slide the 3

00:14:23.840 --> 00:14:23.850
the feature map we'll next slide the 3
 

00:14:23.850 --> 00:14:27.769
the feature map we'll next slide the 3
by 3 filter over by 1 to grab the next

00:14:27.769 --> 00:14:27.779
by 3 filter over by 1 to grab the next
 

00:14:27.779 --> 00:14:30.139
by 3 filter over by 1 to grab the next
patch and repeat the same operation

00:14:30.139 --> 00:14:30.149
patch and repeat the same operation
 

00:14:30.149 --> 00:14:33.109
patch and repeat the same operation
amande wise multiplication addition this

00:14:33.109 --> 00:14:33.119
amande wise multiplication addition this
 

00:14:33.119 --> 00:14:35.600
amande wise multiplication addition this
results in the second entry and we

00:14:35.600 --> 00:14:35.610
results in the second entry and we
 

00:14:35.610 --> 00:14:37.699
results in the second entry and we
continue in this way until we have

00:14:37.699 --> 00:14:37.709
continue in this way until we have
 

00:14:37.709 --> 00:14:42.470
continue in this way until we have
covered the entirety of our 5x5 image

00:14:42.470 --> 00:14:42.480
covered the entirety of our 5x5 image
 

00:14:42.480 --> 00:14:46.429
covered the entirety of our 5x5 image
and that's in the feature map reflects

00:14:46.429 --> 00:14:46.439
and that's in the feature map reflects
 

00:14:46.439 --> 00:14:49.100
and that's in the feature map reflects
where in the input was activated by this

00:14:49.100 --> 00:14:49.110
where in the input was activated by this
 

00:14:49.110 --> 00:14:52.489
where in the input was activated by this
filter that we applied now that we've

00:14:52.489 --> 00:14:52.499
filter that we applied now that we've
 

00:14:52.499 --> 00:14:54.139
filter that we applied now that we've
gone through the mechanism of the

00:14:54.139 --> 00:14:54.149
gone through the mechanism of the
 

00:14:54.149 --> 00:14:57.470
gone through the mechanism of the
convolution operation let's see how

00:14:57.470 --> 00:14:57.480
convolution operation let's see how
 

00:14:57.480 --> 00:14:59.809
convolution operation let's see how
different filters can be used to produce

00:14:59.809 --> 00:14:59.819
different filters can be used to produce
 

00:14:59.819 --> 00:15:02.629
different filters can be used to produce
different feature Maps so on the Left

00:15:02.629 --> 00:15:02.639
different feature Maps so on the Left
 

00:15:02.639 --> 00:15:04.280
different feature Maps so on the Left
you'll see a picture of a woman's face

00:15:04.280 --> 00:15:04.290
you'll see a picture of a woman's face
 

00:15:04.290 --> 00:15:07.879
you'll see a picture of a woman's face
and next to it the output of applying

00:15:07.879 --> 00:15:07.889
and next to it the output of applying
 

00:15:07.889 --> 00:15:10.639
and next to it the output of applying
three different convolutional filters to

00:15:10.639 --> 00:15:10.649
three different convolutional filters to
 

00:15:10.649 --> 00:15:13.519
three different convolutional filters to
that same image and you can appreciate

00:15:13.519 --> 00:15:13.529
that same image and you can appreciate
 

00:15:13.529 --> 00:15:15.829
that same image and you can appreciate
that by simply changing the weights of

00:15:15.829 --> 00:15:15.839
that by simply changing the weights of
 

00:15:15.839 --> 00:15:18.229
that by simply changing the weights of
the filters we can detect different

00:15:18.229 --> 00:15:18.239
the filters we can detect different
 

00:15:18.239 --> 00:15:22.850
the filters we can detect different
features from the image so I hope you

00:15:22.850 --> 00:15:22.860
features from the image so I hope you
 

00:15:22.860 --> 00:15:25.909
features from the image so I hope you
can now see how convolution allows us to

00:15:25.909 --> 00:15:25.919
can now see how convolution allows us to
 

00:15:25.919 --> 00:15:27.789
can now see how convolution allows us to
capitalize on the spatial structure

00:15:27.789 --> 00:15:27.799
capitalize on the spatial structure
 

00:15:27.799 --> 00:15:31.489
capitalize on the spatial structure
inherent to image data and use sets of

00:15:31.489 --> 00:15:31.499
inherent to image data and use sets of
 

00:15:31.499 --> 00:15:34.460
inherent to image data and use sets of
weights to extract local features and to

00:15:34.460 --> 00:15:34.470
weights to extract local features and to
 

00:15:34.470 --> 00:15:35.870
weights to extract local features and to
very easily be able to

00:15:35.870 --> 00:15:35.880
very easily be able to
 

00:15:35.880 --> 00:15:38.060
very easily be able to
affect different types of features by

00:15:38.060 --> 00:15:38.070
affect different types of features by
 

00:15:38.070 --> 00:15:41.270
affect different types of features by
simply using different filters these

00:15:41.270 --> 00:15:41.280
simply using different filters these
 

00:15:41.280 --> 00:15:44.030
simply using different filters these
concepts of spatial structure and local

00:15:44.030 --> 00:15:44.040
concepts of spatial structure and local
 

00:15:44.040 --> 00:15:46.460
concepts of spatial structure and local
feature extraction using convolution are

00:15:46.460 --> 00:15:46.470
feature extraction using convolution are
 

00:15:46.470 --> 00:15:49.550
feature extraction using convolution are
at the core of the neural networks used

00:15:49.550 --> 00:15:49.560
at the core of the neural networks used
 

00:15:49.560 --> 00:15:52.880
at the core of the neural networks used
for computer vision tasks which are very

00:15:52.880 --> 00:15:52.890
for computer vision tasks which are very
 

00:15:52.890 --> 00:15:55.040
for computer vision tasks which are very
appropriately named convolutional neural

00:15:55.040 --> 00:15:55.050
appropriately named convolutional neural
 

00:15:55.050 --> 00:15:59.030
appropriately named convolutional neural
networks or CN NS so first we'll take a

00:15:59.030 --> 00:15:59.040
networks or CN NS so first we'll take a
 

00:15:59.040 --> 00:16:01.340
networks or CN NS so first we'll take a
look at a CN n architecture that's

00:16:01.340 --> 00:16:01.350
look at a CN n architecture that's
 

00:16:01.350 --> 00:16:04.400
look at a CN n architecture that's
designed for image classification now

00:16:04.400 --> 00:16:04.410
designed for image classification now
 

00:16:04.410 --> 00:16:07.040
designed for image classification now
there are three main operations to a CNN

00:16:07.040 --> 00:16:07.050
there are three main operations to a CNN
 

00:16:07.050 --> 00:16:10.880
there are three main operations to a CNN
first convolutions which as we saw can

00:16:10.880 --> 00:16:10.890
first convolutions which as we saw can
 

00:16:10.890 --> 00:16:14.320
first convolutions which as we saw can
be used to generate feature Maps second

00:16:14.320 --> 00:16:14.330
be used to generate feature Maps second
 

00:16:14.330 --> 00:16:16.880
be used to generate feature Maps second
non-linearity which we learned in the

00:16:16.880 --> 00:16:16.890
non-linearity which we learned in the
 

00:16:16.890 --> 00:16:20.090
non-linearity which we learned in the
first lecture yesterday because image

00:16:20.090 --> 00:16:20.100
first lecture yesterday because image
 

00:16:20.100 --> 00:16:23.300
first lecture yesterday because image
data is highly nonlinear and finally

00:16:23.300 --> 00:16:23.310
data is highly nonlinear and finally
 

00:16:23.310 --> 00:16:25.280
data is highly nonlinear and finally
pooling which is a down sampling

00:16:25.280 --> 00:16:25.290
pooling which is a down sampling
 

00:16:25.290 --> 00:16:28.880
pooling which is a down sampling
operation in training we train our model

00:16:28.880 --> 00:16:28.890
operation in training we train our model
 

00:16:28.890 --> 00:16:32.030
operation in training we train our model
our CNN model on a set of images and in

00:16:32.030 --> 00:16:32.040
our CNN model on a set of images and in
 

00:16:32.040 --> 00:16:33.610
our CNN model on a set of images and in
training we learn the weights of the

00:16:33.610 --> 00:16:33.620
training we learn the weights of the
 

00:16:33.620 --> 00:16:37.460
training we learn the weights of the
convolutional filters that correspond to

00:16:37.460 --> 00:16:37.470
convolutional filters that correspond to
 

00:16:37.470 --> 00:16:40.360
convolutional filters that correspond to
future maps in convolutional layers and

00:16:40.360 --> 00:16:40.370
future maps in convolutional layers and
 

00:16:40.370 --> 00:16:43.460
future maps in convolutional layers and
in the case of classification we can

00:16:43.460 --> 00:16:43.470
in the case of classification we can
 

00:16:43.470 --> 00:16:45.770
in the case of classification we can
feed the output of these convolutional

00:16:45.770 --> 00:16:45.780
feed the output of these convolutional
 

00:16:45.780 --> 00:16:48.230
feed the output of these convolutional
layers into a fully connected layer to

00:16:48.230 --> 00:16:48.240
layers into a fully connected layer to
 

00:16:48.240 --> 00:16:51.260
layers into a fully connected layer to
perform classification now we'll go

00:16:51.260 --> 00:16:51.270
perform classification now we'll go
 

00:16:51.270 --> 00:16:52.790
perform classification now we'll go
through each of these operations to

00:16:52.790 --> 00:16:52.800
through each of these operations to
 

00:16:52.800 --> 00:16:55.610
through each of these operations to
break down the basic architecture of a

00:16:55.610 --> 00:16:55.620
break down the basic architecture of a
 

00:16:55.620 --> 00:16:59.510
break down the basic architecture of a
CNN first let's consider the convolution

00:16:59.510 --> 00:16:59.520
CNN first let's consider the convolution
 

00:16:59.520 --> 00:17:04.550
CNN first let's consider the convolution
operation as we saw yesterday each

00:17:04.550 --> 00:17:04.560
operation as we saw yesterday each
 

00:17:04.560 --> 00:17:06.860
operation as we saw yesterday each
neuron and hidden layer will compute a

00:17:06.860 --> 00:17:06.870
neuron and hidden layer will compute a
 

00:17:06.870 --> 00:17:09.380
neuron and hidden layer will compute a
weighted sum of its inputs apply bias

00:17:09.380 --> 00:17:09.390
weighted sum of its inputs apply bias
 

00:17:09.390 --> 00:17:11.900
weighted sum of its inputs apply bias
and eventually activate with a

00:17:11.900 --> 00:17:11.910
and eventually activate with a
 

00:17:11.910 --> 00:17:14.809
and eventually activate with a
non-linearity what's special and CNN's

00:17:14.809 --> 00:17:14.819
non-linearity what's special and CNN's
 

00:17:14.819 --> 00:17:17.929
non-linearity what's special and CNN's
is this idea of local connectivity each

00:17:17.929 --> 00:17:17.939
is this idea of local connectivity each
 

00:17:17.939 --> 00:17:20.150
is this idea of local connectivity each
neuron and hidden layer only sees a

00:17:20.150 --> 00:17:20.160
neuron and hidden layer only sees a
 

00:17:20.160 --> 00:17:23.809
neuron and hidden layer only sees a
patch of its inputs we can now define

00:17:23.809 --> 00:17:23.819
patch of its inputs we can now define
 

00:17:23.819 --> 00:17:26.929
patch of its inputs we can now define
the actual computation for a neuron and

00:17:26.929 --> 00:17:26.939
the actual computation for a neuron and
 

00:17:26.939 --> 00:17:29.150
the actual computation for a neuron and
a hidden layer its inputs are those

00:17:29.150 --> 00:17:29.160
a hidden layer its inputs are those
 

00:17:29.160 --> 00:17:31.640
a hidden layer its inputs are those
neurons in the patch of the input layer

00:17:31.640 --> 00:17:31.650
neurons in the patch of the input layer
 

00:17:31.650 --> 00:17:34.610
neurons in the patch of the input layer
that it's connected to we apply a matrix

00:17:34.610 --> 00:17:34.620
that it's connected to we apply a matrix
 

00:17:34.620 --> 00:17:38.060
that it's connected to we apply a matrix
of weights our convolutional filter 4x4

00:17:38.060 --> 00:17:38.070
of weights our convolutional filter 4x4
 

00:17:38.070 --> 00:17:40.250
of weights our convolutional filter 4x4
in this example do an element-wise

00:17:40.250 --> 00:17:40.260
in this example do an element-wise
 

00:17:40.260 --> 00:17:43.070
in this example do an element-wise
multiplication add the outputs and apply

00:17:43.070 --> 00:17:43.080
multiplication add the outputs and apply
 

00:17:43.080 --> 00:17:48.440
multiplication add the outputs and apply
bias so this defines how neurons in

00:17:48.440 --> 00:17:48.450
bias so this defines how neurons in
 

00:17:48.450 --> 00:17:49.320
bias so this defines how neurons in
convolutional

00:17:49.320 --> 00:17:49.330
convolutional
 

00:17:49.330 --> 00:17:52.590
convolutional
are connected but within a single

00:17:52.590 --> 00:17:52.600
are connected but within a single
 

00:17:52.600 --> 00:17:55.169
are connected but within a single
convolutional layer we can have multiple

00:17:55.169 --> 00:17:55.179
convolutional layer we can have multiple
 

00:17:55.179 --> 00:17:57.870
convolutional layer we can have multiple
different filters that we are learning

00:17:57.870 --> 00:17:57.880
different filters that we are learning
 

00:17:57.880 --> 00:17:59.880
different filters that we are learning
to be able to extract different features

00:17:59.880 --> 00:17:59.890
to be able to extract different features
 

00:17:59.890 --> 00:18:03.060
to be able to extract different features
and what this means that is that the

00:18:03.060 --> 00:18:03.070
and what this means that is that the
 

00:18:03.070 --> 00:18:05.669
and what this means that is that the
output of a convolutional layer has a

00:18:05.669 --> 00:18:05.679
output of a convolutional layer has a
 

00:18:05.679 --> 00:18:08.220
output of a convolutional layer has a
volume where the height and the width

00:18:08.220 --> 00:18:08.230
volume where the height and the width
 

00:18:08.230 --> 00:18:10.740
volume where the height and the width
are spatial dimensions and these spatial

00:18:10.740 --> 00:18:10.750
are spatial dimensions and these spatial
 

00:18:10.750 --> 00:18:13.950
are spatial dimensions and these spatial
dimensions are dependent on the

00:18:13.950 --> 00:18:13.960
dimensions are dependent on the
 

00:18:13.960 --> 00:18:16.529
dimensions are dependent on the
dimensions of the input layer the

00:18:16.529 --> 00:18:16.539
dimensions of the input layer the
 

00:18:16.539 --> 00:18:19.560
dimensions of the input layer the
dimensions of our filter and how we're

00:18:19.560 --> 00:18:19.570
dimensions of our filter and how we're
 

00:18:19.570 --> 00:18:21.899
dimensions of our filter and how we're
sliding our filter over the input the

00:18:21.899 --> 00:18:21.909
sliding our filter over the input the
 

00:18:21.909 --> 00:18:25.740
sliding our filter over the input the
stride the depth of a this output volume

00:18:25.740 --> 00:18:25.750
stride the depth of a this output volume
 

00:18:25.750 --> 00:18:28.019
stride the depth of a this output volume
is then given by the number of different

00:18:28.019 --> 00:18:28.029
is then given by the number of different
 

00:18:28.029 --> 00:18:31.799
is then given by the number of different
filters we apply in that layer we can

00:18:31.799 --> 00:18:31.809
filters we apply in that layer we can
 

00:18:31.809 --> 00:18:33.930
filters we apply in that layer we can
also think of how neurons and

00:18:33.930 --> 00:18:33.940
also think of how neurons and
 

00:18:33.940 --> 00:18:35.700
also think of how neurons and
convolutional layers are connected in

00:18:35.700 --> 00:18:35.710
convolutional layers are connected in
 

00:18:35.710 --> 00:18:38.639
convolutional layers are connected in
terms of their receptive field the

00:18:38.639 --> 00:18:38.649
terms of their receptive field the
 

00:18:38.649 --> 00:18:41.190
terms of their receptive field the
locations in the original input image

00:18:41.190 --> 00:18:41.200
locations in the original input image
 

00:18:41.200 --> 00:18:44.310
locations in the original input image
that a node is path connected to these

00:18:44.310 --> 00:18:44.320
that a node is path connected to these
 

00:18:44.320 --> 00:18:46.049
that a node is path connected to these
parameters define the spatial

00:18:46.049 --> 00:18:46.059
parameters define the spatial
 

00:18:46.059 --> 00:18:47.759
parameters define the spatial
arrangement of the output of a

00:18:47.759 --> 00:18:47.769
arrangement of the output of a
 

00:18:47.769 --> 00:18:51.659
arrangement of the output of a
convolutional layer the next step is to

00:18:51.659 --> 00:18:51.669
convolutional layer the next step is to
 

00:18:51.669 --> 00:18:53.610
convolutional layer the next step is to
apply a non-linearity to this output

00:18:53.610 --> 00:18:53.620
apply a non-linearity to this output
 

00:18:53.620 --> 00:18:57.060
apply a non-linearity to this output
volume as was introduced in yesterday's

00:18:57.060 --> 00:18:57.070
volume as was introduced in yesterday's
 

00:18:57.070 --> 00:18:59.460
volume as was introduced in yesterday's
lecture we do this because data is

00:18:59.460 --> 00:18:59.470
lecture we do this because data is
 

00:18:59.470 --> 00:19:02.070
lecture we do this because data is
highly nonlinear and in cnn's it's

00:19:02.070 --> 00:19:02.080
highly nonlinear and in cnn's it's
 

00:19:02.080 --> 00:19:04.289
highly nonlinear and in cnn's it's
common practice to apply non-linearity

00:19:04.289 --> 00:19:04.299
common practice to apply non-linearity
 

00:19:04.299 --> 00:19:07.590
common practice to apply non-linearity
after every convolution operation and a

00:19:07.590 --> 00:19:07.600
after every convolution operation and a
 

00:19:07.600 --> 00:19:09.750
after every convolution operation and a
common activation function that's used

00:19:09.750 --> 00:19:09.760
common activation function that's used
 

00:19:09.760 --> 00:19:12.230
common activation function that's used
is the relu which is a pixel-by-pixel

00:19:12.230 --> 00:19:12.240
is the relu which is a pixel-by-pixel
 

00:19:12.240 --> 00:19:15.240
is the relu which is a pixel-by-pixel
operation that will replace all negative

00:19:15.240 --> 00:19:15.250
operation that will replace all negative
 

00:19:15.250 --> 00:19:18.649
operation that will replace all negative
values following convolution with a zero

00:19:18.649 --> 00:19:18.659
values following convolution with a zero
 

00:19:18.659 --> 00:19:22.830
values following convolution with a zero
and the last key operation to a CNN is

00:19:22.830 --> 00:19:22.840
and the last key operation to a CNN is
 

00:19:22.840 --> 00:19:25.799
and the last key operation to a CNN is
pooling and pooling is used to reduce

00:19:25.799 --> 00:19:25.809
pooling and pooling is used to reduce
 

00:19:25.809 --> 00:19:28.490
pooling and pooling is used to reduce
dimension dimension yeah excuse me

00:19:28.490 --> 00:19:28.500
dimension dimension yeah excuse me
 

00:19:28.500 --> 00:19:31.350
dimension dimension yeah excuse me
dimensionality and to preserve spatial

00:19:31.350 --> 00:19:31.360
dimensionality and to preserve spatial
 

00:19:31.360 --> 00:19:34.289
dimensionality and to preserve spatial
invariants a common text technique which

00:19:34.289 --> 00:19:34.299
invariants a common text technique which
 

00:19:34.299 --> 00:19:37.200
invariants a common text technique which
you'll very often see is max pooling as

00:19:37.200 --> 00:19:37.210
you'll very often see is max pooling as
 

00:19:37.210 --> 00:19:39.779
you'll very often see is max pooling as
shown in this example and it's exactly

00:19:39.779 --> 00:19:39.789
shown in this example and it's exactly
 

00:19:39.789 --> 00:19:42.180
shown in this example and it's exactly
what it sounds like we simply take the

00:19:42.180 --> 00:19:42.190
what it sounds like we simply take the
 

00:19:42.190 --> 00:19:44.730
what it sounds like we simply take the
maximum value in a patch of the input in

00:19:44.730 --> 00:19:44.740
maximum value in a patch of the input in
 

00:19:44.740 --> 00:19:47.970
maximum value in a patch of the input in
this case a two-by-two patch and that

00:19:47.970 --> 00:19:47.980
this case a two-by-two patch and that
 

00:19:47.980 --> 00:19:49.860
this case a two-by-two patch and that
determines the output of the pooling

00:19:49.860 --> 00:19:49.870
determines the output of the pooling
 

00:19:49.870 --> 00:19:53.100
determines the output of the pooling
operation and I encourage you to kind of

00:19:53.100 --> 00:19:53.110
operation and I encourage you to kind of
 

00:19:53.110 --> 00:19:56.310
operation and I encourage you to kind of
meditate on other ways in which we could

00:19:56.310 --> 00:19:56.320
meditate on other ways in which we could
 

00:19:56.320 --> 00:19:58.560
meditate on other ways in which we could
perform this sort of down sampling

00:19:58.560 --> 00:19:58.570
perform this sort of down sampling
 

00:19:58.570 --> 00:20:01.790
perform this sort of down sampling
operation

00:20:01.790 --> 00:20:01.800
 

00:20:01.800 --> 00:20:05.700
so these are the key operations of a CNN

00:20:05.700 --> 00:20:05.710
so these are the key operations of a CNN
 

00:20:05.710 --> 00:20:07.710
so these are the key operations of a CNN
and we're now ready to put them together

00:20:07.710 --> 00:20:07.720
and we're now ready to put them together
 

00:20:07.720 --> 00:20:10.680
and we're now ready to put them together
to actually construct our network we can

00:20:10.680 --> 00:20:10.690
to actually construct our network we can
 

00:20:10.690 --> 00:20:12.780
to actually construct our network we can
layer these operations to learn a

00:20:12.780 --> 00:20:12.790
layer these operations to learn a
 

00:20:12.790 --> 00:20:14.850
layer these operations to learn a
hierarchy of features present in the

00:20:14.850 --> 00:20:14.860
hierarchy of features present in the
 

00:20:14.860 --> 00:20:15.660
hierarchy of features present in the
image data

00:20:15.660 --> 00:20:15.670
image data
 

00:20:15.670 --> 00:20:20.430
image data
a CNN built for image classification can

00:20:20.430 --> 00:20:20.440
a CNN built for image classification can
 

00:20:20.440 --> 00:20:22.560
a CNN built for image classification can
roughly be broken down into two parts

00:20:22.560 --> 00:20:22.570
roughly be broken down into two parts
 

00:20:22.570 --> 00:20:24.810
roughly be broken down into two parts
the first is the feature learning

00:20:24.810 --> 00:20:24.820
the first is the feature learning
 

00:20:24.820 --> 00:20:27.210
the first is the feature learning
pipeline where we learn features in

00:20:27.210 --> 00:20:27.220
pipeline where we learn features in
 

00:20:27.220 --> 00:20:30.060
pipeline where we learn features in
input images through convolution through

00:20:30.060 --> 00:20:30.070
input images through convolution through
 

00:20:30.070 --> 00:20:32.210
input images through convolution through
the introduction of non-linearity and

00:20:32.210 --> 00:20:32.220
the introduction of non-linearity and
 

00:20:32.220 --> 00:20:36.090
the introduction of non-linearity and
the pooling operation and the second

00:20:36.090 --> 00:20:36.100
the pooling operation and the second
 

00:20:36.100 --> 00:20:39.510
the pooling operation and the second
part is how we're actually performing

00:20:39.510 --> 00:20:39.520
part is how we're actually performing
 

00:20:39.520 --> 00:20:42.660
part is how we're actually performing
the classification the convolutional and

00:20:42.660 --> 00:20:42.670
the classification the convolutional and
 

00:20:42.670 --> 00:20:44.640
the classification the convolutional and
pooling layers output high-level

00:20:44.640 --> 00:20:44.650
pooling layers output high-level
 

00:20:44.650 --> 00:20:46.860
pooling layers output high-level
features of the input data and we can

00:20:46.860 --> 00:20:46.870
features of the input data and we can
 

00:20:46.870 --> 00:20:49.110
features of the input data and we can
feed these into fully connected layers

00:20:49.110 --> 00:20:49.120
feed these into fully connected layers
 

00:20:49.120 --> 00:20:52.460
feed these into fully connected layers
to perform the actual classification and

00:20:52.460 --> 00:20:52.470
to perform the actual classification and
 

00:20:52.470 --> 00:20:54.960
to perform the actual classification and
the output of the fully connected layers

00:20:54.960 --> 00:20:54.970
the output of the fully connected layers
 

00:20:54.970 --> 00:20:56.970
the output of the fully connected layers
in practice is a probability

00:20:56.970 --> 00:20:56.980
in practice is a probability
 

00:20:56.980 --> 00:21:00.090
in practice is a probability
distribution for an input images

00:21:00.090 --> 00:21:00.100
distribution for an input images
 

00:21:00.100 --> 00:21:02.850
distribution for an input images
membership over set of possible classes

00:21:02.850 --> 00:21:02.860
membership over set of possible classes
 

00:21:02.860 --> 00:21:06.600
membership over set of possible classes
and a common way to do this is using a

00:21:06.600 --> 00:21:06.610
and a common way to do this is using a
 

00:21:06.610 --> 00:21:09.660
and a common way to do this is using a
function called softmax where the output

00:21:09.660 --> 00:21:09.670
function called softmax where the output
 

00:21:09.670 --> 00:21:11.940
function called softmax where the output
represents this categorical probability

00:21:11.940 --> 00:21:11.950
represents this categorical probability
 

00:21:11.950 --> 00:21:16.020
represents this categorical probability
distribution now that we've gone through

00:21:16.020 --> 00:21:16.030
distribution now that we've gone through
 

00:21:16.030 --> 00:21:18.690
distribution now that we've gone through
all the components of a CN n all that's

00:21:18.690 --> 00:21:18.700
all the components of a CN n all that's
 

00:21:18.700 --> 00:21:21.690
all the components of a CN n all that's
left is to discuss how to train it again

00:21:21.690 --> 00:21:21.700
left is to discuss how to train it again
 

00:21:21.700 --> 00:21:23.970
left is to discuss how to train it again
we'll go back to a CNN for image

00:21:23.970 --> 00:21:23.980
we'll go back to a CNN for image
 

00:21:23.980 --> 00:21:26.970
we'll go back to a CNN for image
classification during training we learn

00:21:26.970 --> 00:21:26.980
classification during training we learn
 

00:21:26.980 --> 00:21:29.100
classification during training we learn
the weights of the convolutional filters

00:21:29.100 --> 00:21:29.110
the weights of the convolutional filters
 

00:21:29.110 --> 00:21:31.560
the weights of the convolutional filters
what features the network is detecting

00:21:31.560 --> 00:21:31.570
what features the network is detecting
 

00:21:31.570 --> 00:21:33.570
what features the network is detecting
and in this case we'll also learn the

00:21:33.570 --> 00:21:33.580
and in this case we'll also learn the
 

00:21:33.580 --> 00:21:35.570
and in this case we'll also learn the
weights of the fully connected layers

00:21:35.570 --> 00:21:35.580
weights of the fully connected layers
 

00:21:35.580 --> 00:21:38.610
weights of the fully connected layers
since the output for classification is a

00:21:38.610 --> 00:21:38.620
since the output for classification is a
 

00:21:38.620 --> 00:21:41.370
since the output for classification is a
probability distribution we can use

00:21:41.370 --> 00:21:41.380
probability distribution we can use
 

00:21:41.380 --> 00:21:44.160
probability distribution we can use
cross entropy loss for optimization with

00:21:44.160 --> 00:21:44.170
cross entropy loss for optimization with
 

00:21:44.170 --> 00:21:48.930
cross entropy loss for optimization with
back prop okay so I'd like to take a

00:21:48.930 --> 00:21:48.940
back prop okay so I'd like to take a
 

00:21:48.940 --> 00:21:51.720
back prop okay so I'd like to take a
closer look at CN NS for classification

00:21:51.720 --> 00:21:51.730
closer look at CN NS for classification
 

00:21:51.730 --> 00:21:55.020
closer look at CN NS for classification
and discuss what is arguably the most

00:21:55.020 --> 00:21:55.030
and discuss what is arguably the most
 

00:21:55.030 --> 00:22:00.600
and discuss what is arguably the most
famous example of a CNN the the ones

00:22:00.600 --> 00:22:00.610
famous example of a CNN the the ones
 

00:22:00.610 --> 00:22:03.480
famous example of a CNN the the ones
trained and tested on the image net

00:22:03.480 --> 00:22:03.490
trained and tested on the image net
 

00:22:03.490 --> 00:22:06.720
trained and tested on the image net
dataset imagenet is a massive dataset

00:22:06.720 --> 00:22:06.730
dataset imagenet is a massive dataset
 

00:22:06.730 --> 00:22:09.890
dataset imagenet is a massive dataset
with over 14 million images spanning

00:22:09.890 --> 00:22:09.900
with over 14 million images spanning
 

00:22:09.900 --> 00:22:13.350
with over 14 million images spanning
20,000 different categories for example

00:22:13.350 --> 00:22:13.360
20,000 different categories for example
 

00:22:13.360 --> 00:22:15.240
20,000 different categories for example
there are

00:22:15.240 --> 00:22:15.250
there are
 

00:22:15.250 --> 00:22:17.490
there are
nine different pictures of bananas in

00:22:17.490 --> 00:22:17.500
nine different pictures of bananas in
 

00:22:17.500 --> 00:22:20.850
nine different pictures of bananas in
this dataset alone even better

00:22:20.850 --> 00:22:20.860
this dataset alone even better
 

00:22:20.860 --> 00:22:23.540
this dataset alone even better
bananas are succinctly described as an

00:22:23.540 --> 00:22:23.550
bananas are succinctly described as an
 

00:22:23.550 --> 00:22:26.670
bananas are succinctly described as an
elongated crescent-shaped yellow fruit

00:22:26.670 --> 00:22:26.680
elongated crescent-shaped yellow fruit
 

00:22:26.680 --> 00:22:30.360
elongated crescent-shaped yellow fruit
with soft sweet flesh which both gives a

00:22:30.360 --> 00:22:30.370
with soft sweet flesh which both gives a
 

00:22:30.370 --> 00:22:32.790
with soft sweet flesh which both gives a
pretty good descriptive value of what

00:22:32.790 --> 00:22:32.800
pretty good descriptive value of what
 

00:22:32.800 --> 00:22:35.160
pretty good descriptive value of what
about banana is and speaks to its

00:22:35.160 --> 00:22:35.170
about banana is and speaks to its
 

00:22:35.170 --> 00:22:37.250
about banana is and speaks to its
deliciousness

00:22:37.250 --> 00:22:37.260
deliciousness
 

00:22:37.260 --> 00:22:40.470
deliciousness
so the creators of the image net dataset

00:22:40.470 --> 00:22:40.480
so the creators of the image net dataset
 

00:22:40.480 --> 00:22:42.720
so the creators of the image net dataset
also have created a set of visual

00:22:42.720 --> 00:22:42.730
also have created a set of visual
 

00:22:42.730 --> 00:22:45.270
also have created a set of visual
recognition challenges beyond this data

00:22:45.270 --> 00:22:45.280
recognition challenges beyond this data
 

00:22:45.280 --> 00:22:48.390
recognition challenges beyond this data
set and what's most famous is the image

00:22:48.390 --> 00:22:48.400
set and what's most famous is the image
 

00:22:48.400 --> 00:22:52.220
set and what's most famous is the image
net classification task where users are

00:22:52.220 --> 00:22:52.230
net classification task where users are
 

00:22:52.230 --> 00:22:54.390
net classification task where users are
challengers are simply tasked with

00:22:54.390 --> 00:22:54.400
challengers are simply tasked with
 

00:22:54.400 --> 00:22:57.150
challengers are simply tasked with
producing a list of the object

00:22:57.150 --> 00:22:57.160
producing a list of the object
 

00:22:57.160 --> 00:22:59.430
producing a list of the object
categories present in a given image

00:22:59.430 --> 00:22:59.440
categories present in a given image
 

00:22:59.440 --> 00:23:04.020
categories present in a given image
across 1000 different categories and the

00:23:04.020 --> 00:23:04.030
across 1000 different categories and the
 

00:23:04.030 --> 00:23:06.570
across 1000 different categories and the
results of the neural networks have had

00:23:06.570 --> 00:23:06.580
results of the neural networks have had
 

00:23:06.580 --> 00:23:08.760
results of the neural networks have had
on this classification tasks are pretty

00:23:08.760 --> 00:23:08.770
on this classification tasks are pretty
 

00:23:08.770 --> 00:23:12.630
on this classification tasks are pretty
remarkable 2012 was the first time a CNN

00:23:12.630 --> 00:23:12.640
remarkable 2012 was the first time a CNN
 

00:23:12.640 --> 00:23:16.350
remarkable 2012 was the first time a CNN
won this challenge with the famous Alex

00:23:16.350 --> 00:23:16.360
won this challenge with the famous Alex
 

00:23:16.360 --> 00:23:19.140
won this challenge with the famous Alex
net CNN and since then neural networks

00:23:19.140 --> 00:23:19.150
net CNN and since then neural networks
 

00:23:19.150 --> 00:23:21.780
net CNN and since then neural networks
have dominated the competition and the

00:23:21.780 --> 00:23:21.790
have dominated the competition and the
 

00:23:21.790 --> 00:23:24.840
have dominated the competition and the
error has kept decreasing surpassing

00:23:24.840 --> 00:23:24.850
error has kept decreasing surpassing
 

00:23:24.850 --> 00:23:28.530
error has kept decreasing surpassing
human error in 2015 with the resonant

00:23:28.530 --> 00:23:28.540
human error in 2015 with the resonant
 

00:23:28.540 --> 00:23:32.580
human error in 2015 with the resonant
architecture but with improved accuracy

00:23:32.580 --> 00:23:32.590
architecture but with improved accuracy
 

00:23:32.590 --> 00:23:35.070
architecture but with improved accuracy
the number of layers in these networks

00:23:35.070 --> 00:23:35.080
the number of layers in these networks
 

00:23:35.080 --> 00:23:39.240
the number of layers in these networks
has been increasing quite dramatically

00:23:39.240 --> 00:23:39.250
has been increasing quite dramatically
 

00:23:39.250 --> 00:23:41.610
has been increasing quite dramatically
so there's a trade-off here right build

00:23:41.610 --> 00:23:41.620
so there's a trade-off here right build
 

00:23:41.620 --> 00:23:46.580
so there's a trade-off here right build
your network deeper how deep can you go

00:23:46.580 --> 00:23:46.590
your network deeper how deep can you go
 

00:23:46.590 --> 00:23:49.710
your network deeper how deep can you go
so so far we've talked only about using

00:23:49.710 --> 00:23:49.720
so so far we've talked only about using
 

00:23:49.720 --> 00:23:52.860
so so far we've talked only about using
CNN's for image classification but in

00:23:52.860 --> 00:23:52.870
CNN's for image classification but in
 

00:23:52.870 --> 00:23:55.200
CNN's for image classification but in
reality this idea of using convolutional

00:23:55.200 --> 00:23:55.210
reality this idea of using convolutional
 

00:23:55.210 --> 00:23:58.650
reality this idea of using convolutional
layers to extract features can extend to

00:23:58.650 --> 00:23:58.660
layers to extract features can extend to
 

00:23:58.660 --> 00:24:02.460
layers to extract features can extend to
a number of different applications when

00:24:02.460 --> 00:24:02.470
a number of different applications when
 

00:24:02.470 --> 00:24:04.650
a number of different applications when
we consider to CNN for classification we

00:24:04.650 --> 00:24:04.660
we consider to CNN for classification we
 

00:24:04.660 --> 00:24:06.270
we consider to CNN for classification we
saw that we could think of it in two

00:24:06.270 --> 00:24:06.280
saw that we could think of it in two
 

00:24:06.280 --> 00:24:08.310
saw that we could think of it in two
parts feature learning and the

00:24:08.310 --> 00:24:08.320
parts feature learning and the
 

00:24:08.320 --> 00:24:13.200
parts feature learning and the
classification what is at the core of

00:24:13.200 --> 00:24:13.210
classification what is at the core of
 

00:24:13.210 --> 00:24:16.380
classification what is at the core of
the convolutional neural networks is is

00:24:16.380 --> 00:24:16.390
the convolutional neural networks is is
 

00:24:16.390 --> 00:24:19.170
the convolutional neural networks is is
the feature learning pipeline after that

00:24:19.170 --> 00:24:19.180
the feature learning pipeline after that
 

00:24:19.180 --> 00:24:21.560
the feature learning pipeline after that
we can really change what follows to

00:24:21.560 --> 00:24:21.570
we can really change what follows to
 

00:24:21.570 --> 00:24:25.740
we can really change what follows to
suit the application that we desire for

00:24:25.740 --> 00:24:25.750
suit the application that we desire for
 

00:24:25.750 --> 00:24:28.509
suit the application that we desire for
example the portion following the Khan

00:24:28.509 --> 00:24:28.519
example the portion following the Khan
 

00:24:28.519 --> 00:24:30.399
example the portion following the Khan
lucien layers may look different for

00:24:30.399 --> 00:24:30.409
lucien layers may look different for
 

00:24:30.409 --> 00:24:32.199
lucien layers may look different for
different image classification domains

00:24:32.199 --> 00:24:32.209
different image classification domains
 

00:24:32.209 --> 00:24:35.529
different image classification domains
we can also introduce new architectures

00:24:35.529 --> 00:24:35.539
we can also introduce new architectures
 

00:24:35.539 --> 00:24:38.079
we can also introduce new architectures
beyond fully connected layers for tasks

00:24:38.079 --> 00:24:38.089
beyond fully connected layers for tasks
 

00:24:38.089 --> 00:24:40.180
beyond fully connected layers for tasks
such as segmentation and image

00:24:40.180 --> 00:24:40.190
such as segmentation and image
 

00:24:40.190 --> 00:24:43.899
such as segmentation and image
captioning so today I'd like to go

00:24:43.899 --> 00:24:43.909
captioning so today I'd like to go
 

00:24:43.909 --> 00:24:46.149
captioning so today I'd like to go
through three different applications of

00:24:46.149 --> 00:24:46.159
through three different applications of
 

00:24:46.159 --> 00:24:48.810
through three different applications of
CNN's beyond image classification

00:24:48.810 --> 00:24:48.820
CNN's beyond image classification
 

00:24:48.820 --> 00:24:51.549
CNN's beyond image classification
semantic segmentation where the task is

00:24:51.549 --> 00:24:51.559
semantic segmentation where the task is
 

00:24:51.559 --> 00:24:54.549
semantic segmentation where the task is
to assign each pixel in an image an

00:24:54.549 --> 00:24:54.559
to assign each pixel in an image an
 

00:24:54.559 --> 00:24:57.820
to assign each pixel in an image an
object class to produce a segmentation

00:24:57.820 --> 00:24:57.830
object class to produce a segmentation
 

00:24:57.830 --> 00:25:01.029
object class to produce a segmentation
of that image object detection where we

00:25:01.029 --> 00:25:01.039
of that image object detection where we
 

00:25:01.039 --> 00:25:03.669
of that image object detection where we
are tasked with detecting instances of

00:25:03.669 --> 00:25:03.679
are tasked with detecting instances of
 

00:25:03.679 --> 00:25:06.699
are tasked with detecting instances of
semantic objects in an image and image

00:25:06.699 --> 00:25:06.709
semantic objects in an image and image
 

00:25:06.709 --> 00:25:09.339
semantic objects in an image and image
captioning where the task is to generate

00:25:09.339 --> 00:25:09.349
captioning where the task is to generate
 

00:25:09.349 --> 00:25:11.589
captioning where the task is to generate
a short description of the image that

00:25:11.589 --> 00:25:11.599
a short description of the image that
 

00:25:11.599 --> 00:25:15.549
a short description of the image that
captures its semantic content so first

00:25:15.549 --> 00:25:15.559
captures its semantic content so first
 

00:25:15.559 --> 00:25:17.589
captures its semantic content so first
let's talk about semantic segmentation

00:25:17.589 --> 00:25:17.599
let's talk about semantic segmentation
 

00:25:17.599 --> 00:25:20.139
let's talk about semantic segmentation
with fully convolutional net works or

00:25:20.139 --> 00:25:20.149
with fully convolutional net works or
 

00:25:20.149 --> 00:25:23.829
with fully convolutional net works or
fcns here the network again takes an

00:25:23.829 --> 00:25:23.839
fcns here the network again takes an
 

00:25:23.839 --> 00:25:26.769
fcns here the network again takes an
image input of arbitrary size but

00:25:26.769 --> 00:25:26.779
image input of arbitrary size but
 

00:25:26.779 --> 00:25:28.389
image input of arbitrary size but
instead it has to produce a

00:25:28.389 --> 00:25:28.399
instead it has to produce a
 

00:25:28.399 --> 00:25:31.389
instead it has to produce a
correspondingly sized output where each

00:25:31.389 --> 00:25:31.399
correspondingly sized output where each
 

00:25:31.399 --> 00:25:34.060
correspondingly sized output where each
pixel has been assigned a class label

00:25:34.060 --> 00:25:34.070
pixel has been assigned a class label
 

00:25:34.070 --> 00:25:36.339
pixel has been assigned a class label
which we can then visualize as a

00:25:36.339 --> 00:25:36.349
which we can then visualize as a
 

00:25:36.349 --> 00:25:40.299
which we can then visualize as a
segmentation as we saw before with CNN s

00:25:40.299 --> 00:25:40.309
segmentation as we saw before with CNN s
 

00:25:40.309 --> 00:25:43.149
segmentation as we saw before with CNN s
for image classification we first have a

00:25:43.149 --> 00:25:43.159
for image classification we first have a
 

00:25:43.159 --> 00:25:45.190
for image classification we first have a
series of convolutional layers that are

00:25:45.190 --> 00:25:45.200
series of convolutional layers that are
 

00:25:45.200 --> 00:25:47.139
series of convolutional layers that are
down sampling operations for future

00:25:47.139 --> 00:25:47.149
down sampling operations for future
 

00:25:47.149 --> 00:25:49.539
down sampling operations for future
extraction and this results in a

00:25:49.539 --> 00:25:49.549
extraction and this results in a
 

00:25:49.549 --> 00:25:52.089
extraction and this results in a
hierarchy of learned features now the

00:25:52.089 --> 00:25:52.099
hierarchy of learned features now the
 

00:25:52.099 --> 00:25:55.180
hierarchy of learned features now the
difference is that we have a series of

00:25:55.180 --> 00:25:55.190
difference is that we have a series of
 

00:25:55.190 --> 00:25:58.569
difference is that we have a series of
up sampling operations to increase the

00:25:58.569 --> 00:25:58.579
up sampling operations to increase the
 

00:25:58.579 --> 00:26:01.149
up sampling operations to increase the
resolution of our output from the Foley

00:26:01.149 --> 00:26:01.159
resolution of our output from the Foley
 

00:26:01.159 --> 00:26:06.009
resolution of our output from the Foley
from the convolutional layers and to

00:26:06.009 --> 00:26:06.019
from the convolutional layers and to
 

00:26:06.019 --> 00:26:08.229
from the convolutional layers and to
then combine this output of the up

00:26:08.229 --> 00:26:08.239
then combine this output of the up
 

00:26:08.239 --> 00:26:10.869
then combine this output of the up
sampling operations with those from our

00:26:10.869 --> 00:26:10.879
sampling operations with those from our
 

00:26:10.879 --> 00:26:12.909
sampling operations with those from our
down sampling path to produce a

00:26:12.909 --> 00:26:12.919
down sampling path to produce a
 

00:26:12.919 --> 00:26:16.599
down sampling path to produce a
segmentation one application of this

00:26:16.599 --> 00:26:16.609
segmentation one application of this
 

00:26:16.609 --> 00:26:18.930
segmentation one application of this
sort of architecture is to the real-time

00:26:18.930 --> 00:26:18.940
sort of architecture is to the real-time
 

00:26:18.940 --> 00:26:22.209
sort of architecture is to the real-time
segmentation of the driving scene here

00:26:22.209 --> 00:26:22.219
segmentation of the driving scene here
 

00:26:22.219 --> 00:26:24.599
segmentation of the driving scene here
the network has this encoder decoder

00:26:24.599 --> 00:26:24.609
the network has this encoder decoder
 

00:26:24.609 --> 00:26:27.879
the network has this encoder decoder
architecture for encoding the

00:26:27.879 --> 00:26:27.889
architecture for encoding the
 

00:26:27.889 --> 00:26:30.039
architecture for encoding the
architecture is very similar to what we

00:26:30.039 --> 00:26:30.049
architecture is very similar to what we
 

00:26:30.049 --> 00:26:32.649
architecture is very similar to what we
discussed earlier earlier convolutional

00:26:32.649 --> 00:26:32.659
discussed earlier earlier convolutional
 

00:26:32.659 --> 00:26:34.869
discussed earlier earlier convolutional
layers to learn a hierarchy of future

00:26:34.869 --> 00:26:34.879
layers to learn a hierarchy of future
 

00:26:34.879 --> 00:26:37.539
layers to learn a hierarchy of future
maps and then the decoder portion of the

00:26:37.539 --> 00:26:37.549
maps and then the decoder portion of the
 

00:26:37.549 --> 00:26:39.880
maps and then the decoder portion of the
network actually uses the end

00:26:39.880 --> 00:26:39.890
network actually uses the end
 

00:26:39.890 --> 00:26:43.720
network actually uses the end
from pooling operations to up sample

00:26:43.720 --> 00:26:43.730
from pooling operations to up sample
 

00:26:43.730 --> 00:26:45.790
from pooling operations to up sample
from these feature maps and output a

00:26:45.790 --> 00:26:45.800
from these feature maps and output a
 

00:26:45.800 --> 00:26:50.260
from these feature maps and output a
segmentation another way CNN's have been

00:26:50.260 --> 00:26:50.270
segmentation another way CNN's have been
 

00:26:50.270 --> 00:26:53.020
segmentation another way CNN's have been
extended and applied is for object

00:26:53.020 --> 00:26:53.030
extended and applied is for object
 

00:26:53.030 --> 00:26:55.480
extended and applied is for object
detection where we're trying to learn

00:26:55.480 --> 00:26:55.490
detection where we're trying to learn
 

00:26:55.490 --> 00:26:57.370
detection where we're trying to learn
features that characterize particular

00:26:57.370 --> 00:26:57.380
features that characterize particular
 

00:26:57.380 --> 00:27:00.400
features that characterize particular
regions of the input and then classify

00:27:00.400 --> 00:27:00.410
regions of the input and then classify
 

00:27:00.410 --> 00:27:02.980
regions of the input and then classify
those regions the original pipeline for

00:27:02.980 --> 00:27:02.990
those regions the original pipeline for
 

00:27:02.990 --> 00:27:05.230
those regions the original pipeline for
doing this called our CNN is pretty

00:27:05.230 --> 00:27:05.240
doing this called our CNN is pretty
 

00:27:05.240 --> 00:27:06.010
doing this called our CNN is pretty
straightforward

00:27:06.010 --> 00:27:06.020
straightforward
 

00:27:06.020 --> 00:27:08.860
straightforward
given an input image the algorithm

00:27:08.860 --> 00:27:08.870
given an input image the algorithm
 

00:27:08.870 --> 00:27:11.340
given an input image the algorithm
extracts region proposals bottom-up

00:27:11.340 --> 00:27:11.350
extracts region proposals bottom-up
 

00:27:11.350 --> 00:27:13.750
extracts region proposals bottom-up
computes features for each of these

00:27:13.750 --> 00:27:13.760
computes features for each of these
 

00:27:13.760 --> 00:27:16.390
computes features for each of these
proposals using convolutional layers and

00:27:16.390 --> 00:27:16.400
proposals using convolutional layers and
 

00:27:16.400 --> 00:27:20.920
proposals using convolutional layers and
then classifies each region proposal but

00:27:20.920 --> 00:27:20.930
then classifies each region proposal but
 

00:27:20.930 --> 00:27:23.770
then classifies each region proposal but
there's a huge downside to this so in

00:27:23.770 --> 00:27:23.780
there's a huge downside to this so in
 

00:27:23.780 --> 00:27:26.230
there's a huge downside to this so in
their in their original pipeline this

00:27:26.230 --> 00:27:26.240
their in their original pipeline this
 

00:27:26.240 --> 00:27:29.290
their in their original pipeline this
group extracted about 2000 different

00:27:29.290 --> 00:27:29.300
group extracted about 2000 different
 

00:27:29.300 --> 00:27:31.780
group extracted about 2000 different
region proposals which meant that they

00:27:31.780 --> 00:27:31.790
region proposals which meant that they
 

00:27:31.790 --> 00:27:33.850
region proposals which meant that they
had to run two thousand cnn's for

00:27:33.850 --> 00:27:33.860
had to run two thousand cnn's for
 

00:27:33.860 --> 00:27:36.610
had to run two thousand cnn's for
feature extraction so since then they're

00:27:36.610 --> 00:27:36.620
feature extraction so since then they're
 

00:27:36.620 --> 00:27:39.570
feature extraction so since then they're
and that takes a really really long time

00:27:39.570 --> 00:27:39.580
and that takes a really really long time
 

00:27:39.580 --> 00:27:42.130
and that takes a really really long time
so since then there have been extensions

00:27:42.130 --> 00:27:42.140
so since then there have been extensions
 

00:27:42.140 --> 00:27:45.190
so since then there have been extensions
of this basic idea one being to first

00:27:45.190 --> 00:27:45.200
of this basic idea one being to first
 

00:27:45.200 --> 00:27:48.070
of this basic idea one being to first
run the CNN on the input image to first

00:27:48.070 --> 00:27:48.080
run the CNN on the input image to first
 

00:27:48.080 --> 00:27:50.920
run the CNN on the input image to first
extract features and then get region

00:27:50.920 --> 00:27:50.930
extract features and then get region
 

00:27:50.930 --> 00:27:55.150
extract features and then get region
proposals from the future Maps finally

00:27:55.150 --> 00:27:55.160
proposals from the future Maps finally
 

00:27:55.160 --> 00:27:57.370
proposals from the future Maps finally
let's consider image captioning so

00:27:57.370 --> 00:27:57.380
let's consider image captioning so
 

00:27:57.380 --> 00:27:59.800
let's consider image captioning so
suppose we're given an image of a cat

00:27:59.800 --> 00:27:59.810
suppose we're given an image of a cat
 

00:27:59.810 --> 00:28:02.680
suppose we're given an image of a cat
riding a skateboard in classification

00:28:02.680 --> 00:28:02.690
riding a skateboard in classification
 

00:28:02.690 --> 00:28:05.290
riding a skateboard in classification
our task is to output a class label for

00:28:05.290 --> 00:28:05.300
our task is to output a class label for
 

00:28:05.300 --> 00:28:09.040
our task is to output a class label for
this image cat and as we've probably

00:28:09.040 --> 00:28:09.050
this image cat and as we've probably
 

00:28:09.050 --> 00:28:11.800
this image cat and as we've probably
hammered home by now this is done by

00:28:11.800 --> 00:28:11.810
hammered home by now this is done by
 

00:28:11.810 --> 00:28:13.660
hammered home by now this is done by
feeding our input image through a set of

00:28:13.660 --> 00:28:13.670
feeding our input image through a set of
 

00:28:13.670 --> 00:28:16.450
feeding our input image through a set of
convolutional layers extracting features

00:28:16.450 --> 00:28:16.460
convolutional layers extracting features
 

00:28:16.460 --> 00:28:18.850
convolutional layers extracting features
and then passing these features on to

00:28:18.850 --> 00:28:18.860
and then passing these features on to
 

00:28:18.860 --> 00:28:21.370
and then passing these features on to
fully connected layers to predict a

00:28:21.370 --> 00:28:21.380
fully connected layers to predict a
 

00:28:21.380 --> 00:28:25.000
fully connected layers to predict a
label in image captioning we want to

00:28:25.000 --> 00:28:25.010
label in image captioning we want to
 

00:28:25.010 --> 00:28:27.490
label in image captioning we want to
generate a sentence that describes the

00:28:27.490 --> 00:28:27.500
generate a sentence that describes the
 

00:28:27.500 --> 00:28:30.580
generate a sentence that describes the
semantic content of the same image so

00:28:30.580 --> 00:28:30.590
semantic content of the same image so
 

00:28:30.590 --> 00:28:32.710
semantic content of the same image so
let's take the same network from before

00:28:32.710 --> 00:28:32.720
let's take the same network from before
 

00:28:32.720 --> 00:28:35.110
let's take the same network from before
and remove the fully connected layers at

00:28:35.110 --> 00:28:35.120
and remove the fully connected layers at
 

00:28:35.120 --> 00:28:38.230
and remove the fully connected layers at
the end now we only have convolutional

00:28:38.230 --> 00:28:38.240
the end now we only have convolutional
 

00:28:38.240 --> 00:28:40.840
the end now we only have convolutional
layers to extract features and will

00:28:40.840 --> 00:28:40.850
layers to extract features and will
 

00:28:40.850 --> 00:28:43.420
layers to extract features and will
replace the fully connected layers with

00:28:43.420 --> 00:28:43.430
replace the fully connected layers with
 

00:28:43.430 --> 00:28:46.480
replace the fully connected layers with
a recurrent neural network the output of

00:28:46.480 --> 00:28:46.490
a recurrent neural network the output of
 

00:28:46.490 --> 00:28:48.280
a recurrent neural network the output of
the convolutional layers gives us a

00:28:48.280 --> 00:28:48.290
the convolutional layers gives us a
 

00:28:48.290 --> 00:28:51.610
the convolutional layers gives us a
fixed length encoding of the features

00:28:51.610 --> 00:28:51.620
fixed length encoding of the features
 

00:28:51.620 --> 00:28:53.190
fixed length encoding of the features
present in our input image

00:28:53.190 --> 00:28:53.200
present in our input image
 

00:28:53.200 --> 00:28:55.620
present in our input image
which we can use to initialize an RNN

00:28:55.620 --> 00:28:55.630
which we can use to initialize an RNN
 

00:28:55.630 --> 00:28:58.770
which we can use to initialize an RNN
that we can then train to predict the

00:28:58.770 --> 00:28:58.780
that we can then train to predict the
 

00:28:58.780 --> 00:29:03.210
that we can then train to predict the
words that describe this image using

00:29:03.210 --> 00:29:03.220
words that describe this image using
 

00:29:03.220 --> 00:29:07.800
words that describe this image using
using the Arnon so now that we've talked

00:29:07.800 --> 00:29:07.810
using the Arnon so now that we've talked
 

00:29:07.810 --> 00:29:09.540
using the Arnon so now that we've talked
about convolutional neural networks

00:29:09.540 --> 00:29:09.550
about convolutional neural networks
 

00:29:09.550 --> 00:29:12.780
about convolutional neural networks
their applications we can introduce some

00:29:12.780 --> 00:29:12.790
their applications we can introduce some
 

00:29:12.790 --> 00:29:14.970
their applications we can introduce some
tools that have recently been been

00:29:14.970 --> 00:29:14.980
tools that have recently been been
 

00:29:14.980 --> 00:29:17.490
tools that have recently been been
designed to probe and visualize the

00:29:17.490 --> 00:29:17.500
designed to probe and visualize the
 

00:29:17.500 --> 00:29:20.310
designed to probe and visualize the
inner workings of a CN n to get at this

00:29:20.310 --> 00:29:20.320
inner workings of a CN n to get at this
 

00:29:20.320 --> 00:29:22.320
inner workings of a CN n to get at this
question of what is the network actually

00:29:22.320 --> 00:29:22.330
question of what is the network actually
 

00:29:22.330 --> 00:29:25.920
question of what is the network actually
seeing so first off a few years ago

00:29:25.920 --> 00:29:25.930
seeing so first off a few years ago
 

00:29:25.930 --> 00:29:27.980
seeing so first off a few years ago
there was a paper that published an

00:29:27.980 --> 00:29:27.990
there was a paper that published an
 

00:29:27.990 --> 00:29:31.470
there was a paper that published an
interactive visualization tool of a

00:29:31.470 --> 00:29:31.480
interactive visualization tool of a
 

00:29:31.480 --> 00:29:34.170
interactive visualization tool of a
convolutional neural network trained on

00:29:34.170 --> 00:29:34.180
convolutional neural network trained on
 

00:29:34.180 --> 00:29:36.720
convolutional neural network trained on
a dataset of handwritten digits a very

00:29:36.720 --> 00:29:36.730
a dataset of handwritten digits a very
 

00:29:36.730 --> 00:29:39.240
a dataset of handwritten digits a very
famous data set called M nest and you

00:29:39.240 --> 00:29:39.250
famous data set called M nest and you
 

00:29:39.250 --> 00:29:40.740
famous data set called M nest and you
can play around with this tool to

00:29:40.740 --> 00:29:40.750
can play around with this tool to
 

00:29:40.750 --> 00:29:42.840
can play around with this tool to
visualize the behavior of the network

00:29:42.840 --> 00:29:42.850
visualize the behavior of the network
 

00:29:42.850 --> 00:29:45.540
visualize the behavior of the network
given a number that you yourself draw in

00:29:45.540 --> 00:29:45.550
given a number that you yourself draw in
 

00:29:45.550 --> 00:29:48.030
given a number that you yourself draw in
and what you're seeing here is the

00:29:48.030 --> 00:29:48.040
and what you're seeing here is the
 

00:29:48.040 --> 00:29:50.790
and what you're seeing here is the
feature maps for this seven that someone

00:29:50.790 --> 00:29:50.800
feature maps for this seven that someone
 

00:29:50.800 --> 00:29:54.120
feature maps for this seven that someone
has drawn in and as we can see in the

00:29:54.120 --> 00:29:54.130
has drawn in and as we can see in the
 

00:29:54.130 --> 00:29:56.160
has drawn in and as we can see in the
first layer the first six filters are

00:29:56.160 --> 00:29:56.170
first layer the first six filters are
 

00:29:56.170 --> 00:29:59.490
first layer the first six filters are
showing primarily edged detection and

00:29:59.490 --> 00:29:59.500
showing primarily edged detection and
 

00:29:59.500 --> 00:30:01.590
showing primarily edged detection and
deeper layers will start to pick up on

00:30:01.590 --> 00:30:01.600
deeper layers will start to pick up on
 

00:30:01.600 --> 00:30:04.680
deeper layers will start to pick up on
corners crosses curves more complex

00:30:04.680 --> 00:30:04.690
corners crosses curves more complex
 

00:30:04.690 --> 00:30:08.250
corners crosses curves more complex
features the exact hierarchy that that

00:30:08.250 --> 00:30:08.260
features the exact hierarchy that that
 

00:30:08.260 --> 00:30:10.710
features the exact hierarchy that that
we introduced in the beginning of this

00:30:10.710 --> 00:30:10.720
we introduced in the beginning of this
 

00:30:10.720 --> 00:30:14.550
we introduced in the beginning of this
lecture a second method which you'll you

00:30:14.550 --> 00:30:14.560
lecture a second method which you'll you
 

00:30:14.560 --> 00:30:16.380
lecture a second method which you'll you
yourself will have the chance to play

00:30:16.380 --> 00:30:16.390
yourself will have the chance to play
 

00:30:16.390 --> 00:30:18.690
yourself will have the chance to play
around with in the second lap is called

00:30:18.690 --> 00:30:18.700
around with in the second lap is called
 

00:30:18.700 --> 00:30:22.290
around with in the second lap is called
class activation Maps or cams cams

00:30:22.290 --> 00:30:22.300
class activation Maps or cams cams
 

00:30:22.300 --> 00:30:25.050
class activation Maps or cams cams
generate a heat map that indicates the

00:30:25.050 --> 00:30:25.060
generate a heat map that indicates the
 

00:30:25.060 --> 00:30:28.920
generate a heat map that indicates the
regions of an image - which are CNN for

00:30:28.920 --> 00:30:28.930
regions of an image - which are CNN for
 

00:30:28.930 --> 00:30:31.740
regions of an image - which are CNN for
classification attends to in its final

00:30:31.740 --> 00:30:31.750
classification attends to in its final
 

00:30:31.750 --> 00:30:35.070
classification attends to in its final
layers and the way that this is computed

00:30:35.070 --> 00:30:35.080
layers and the way that this is computed
 

00:30:35.080 --> 00:30:38.370
layers and the way that this is computed
is is the following we choose an output

00:30:38.370 --> 00:30:38.380
is is the following we choose an output
 

00:30:38.380 --> 00:30:41.130
is is the following we choose an output
class that we want to visualize we can

00:30:41.130 --> 00:30:41.140
class that we want to visualize we can
 

00:30:41.140 --> 00:30:43.200
class that we want to visualize we can
then obtain the weights from the last

00:30:43.200 --> 00:30:43.210
then obtain the weights from the last
 

00:30:43.210 --> 00:30:45.570
then obtain the weights from the last
fully connected layer because these

00:30:45.570 --> 00:30:45.580
fully connected layer because these
 

00:30:45.580 --> 00:30:47.760
fully connected layer because these
represent the importance of each of the

00:30:47.760 --> 00:30:47.770
represent the importance of each of the
 

00:30:47.770 --> 00:30:50.280
represent the importance of each of the
final feature maps in outputting that

00:30:50.280 --> 00:30:50.290
final feature maps in outputting that
 

00:30:50.290 --> 00:30:53.400
final feature maps in outputting that
class we can compute our heat map as

00:30:53.400 --> 00:30:53.410
class we can compute our heat map as
 

00:30:53.410 --> 00:30:56.550
class we can compute our heat map as
simply a weighted combination of each of

00:30:56.550 --> 00:30:56.560
simply a weighted combination of each of
 

00:30:56.560 --> 00:30:58.650
simply a weighted combination of each of
the final convolutional feature maps

00:30:58.650 --> 00:30:58.660
the final convolutional feature maps
 

00:30:58.660 --> 00:31:01.050
the final convolutional feature maps
using the weights from the last fully

00:31:01.050 --> 00:31:01.060
using the weights from the last fully
 

00:31:01.060 --> 00:31:05.370
using the weights from the last fully
connected layer we can apply cams to

00:31:05.370 --> 00:31:05.380
connected layer we can apply cams to
 

00:31:05.380 --> 00:31:06.790
connected layer we can apply cams to
visualize both the

00:31:06.790 --> 00:31:06.800
visualize both the
 

00:31:06.800 --> 00:31:09.190
visualize both the
activation maps for the most likely

00:31:09.190 --> 00:31:09.200
activation maps for the most likely
 

00:31:09.200 --> 00:31:12.490
activation maps for the most likely
predictions of an object class for one

00:31:12.490 --> 00:31:12.500
predictions of an object class for one
 

00:31:12.500 --> 00:31:15.700
predictions of an object class for one
image as you see on the left and also to

00:31:15.700 --> 00:31:15.710
image as you see on the left and also to
 

00:31:15.710 --> 00:31:19.420
image as you see on the left and also to
visualize the image regions used by the

00:31:19.420 --> 00:31:19.430
visualize the image regions used by the
 

00:31:19.430 --> 00:31:22.000
visualize the image regions used by the
CNN to identify in different instances

00:31:22.000 --> 00:31:22.010
CNN to identify in different instances
 

00:31:22.010 --> 00:31:25.630
CNN to identify in different instances
of one object class as you see on the

00:31:25.630 --> 00:31:25.640
of one object class as you see on the
 

00:31:25.640 --> 00:31:30.220
of one object class as you see on the
right so to conclude this talk I'd like

00:31:30.220 --> 00:31:30.230
right so to conclude this talk I'd like
 

00:31:30.230 --> 00:31:34.390
right so to conclude this talk I'd like
to take a brief consideration of how

00:31:34.390 --> 00:31:34.400
to take a brief consideration of how
 

00:31:34.400 --> 00:31:36.490
to take a brief consideration of how
deep learning for computer vision has

00:31:36.490 --> 00:31:36.500
deep learning for computer vision has
 

00:31:36.500 --> 00:31:39.220
deep learning for computer vision has
really made an impact over the past

00:31:39.220 --> 00:31:39.230
really made an impact over the past
 

00:31:39.230 --> 00:31:42.400
really made an impact over the past
several years and the advances that have

00:31:42.400 --> 00:31:42.410
several years and the advances that have
 

00:31:42.410 --> 00:31:44.290
several years and the advances that have
been made in deep learning for computer

00:31:44.290 --> 00:31:44.300
been made in deep learning for computer
 

00:31:44.300 --> 00:31:46.180
been made in deep learning for computer
vision would really not be possible

00:31:46.180 --> 00:31:46.190
vision would really not be possible
 

00:31:46.190 --> 00:31:49.240
vision would really not be possible
without the available availability of

00:31:49.240 --> 00:31:49.250
without the available availability of
 

00:31:49.250 --> 00:31:52.110
without the available availability of
large and well annotated image datasets

00:31:52.110 --> 00:31:52.120
large and well annotated image datasets
 

00:31:52.120 --> 00:31:56.020
large and well annotated image datasets
and this has really facilitated the

00:31:56.020 --> 00:31:56.030
and this has really facilitated the
 

00:31:56.030 --> 00:31:58.150
and this has really facilitated the
progress that's been made some datasets

00:31:58.150 --> 00:31:58.160
progress that's been made some datasets
 

00:31:58.160 --> 00:32:01.570
progress that's been made some datasets
include image net which we discussed and

00:32:01.570 --> 00:32:01.580
include image net which we discussed and
 

00:32:01.580 --> 00:32:05.530
include image net which we discussed and
this a data set of handwritten digits

00:32:05.530 --> 00:32:05.540
this a data set of handwritten digits
 

00:32:05.540 --> 00:32:07.690
this a data set of handwritten digits
which was used in some of the first big

00:32:07.690 --> 00:32:07.700
which was used in some of the first big
 

00:32:07.700 --> 00:32:11.650
which was used in some of the first big
CNN papers places a database from here

00:32:11.650 --> 00:32:11.660
CNN papers places a database from here
 

00:32:11.660 --> 00:32:14.110
CNN papers places a database from here
at MIT of scenes and landscapes and

00:32:14.110 --> 00:32:14.120
at MIT of scenes and landscapes and
 

00:32:14.120 --> 00:32:17.860
at MIT of scenes and landscapes and
cypher 10 which contains images from ten

00:32:17.860 --> 00:32:17.870
cypher 10 which contains images from ten
 

00:32:17.870 --> 00:32:21.340
cypher 10 which contains images from ten
different categories listed here the

00:32:21.340 --> 00:32:21.350
different categories listed here the
 

00:32:21.350 --> 00:32:25.110
different categories listed here the
impact has been broad and deep and

00:32:25.110 --> 00:32:25.120
impact has been broad and deep and
 

00:32:25.120 --> 00:32:27.760
impact has been broad and deep and
spanning a variety of different fields

00:32:27.760 --> 00:32:27.770
spanning a variety of different fields
 

00:32:27.770 --> 00:32:32.380
spanning a variety of different fields
everything from medicine to self-driving

00:32:32.380 --> 00:32:32.390
everything from medicine to self-driving
 

00:32:32.390 --> 00:32:36.790
everything from medicine to self-driving
cars to security one area in which

00:32:36.790 --> 00:32:36.800
cars to security one area in which
 

00:32:36.800 --> 00:32:39.130
cars to security one area in which
convolutional neural networks really

00:32:39.130 --> 00:32:39.140
convolutional neural networks really
 

00:32:39.140 --> 00:32:41.560
convolutional neural networks really
made a big impact early on was in facial

00:32:41.560 --> 00:32:41.570
made a big impact early on was in facial
 

00:32:41.570 --> 00:32:43.570
made a big impact early on was in facial
recognition software and if you think

00:32:43.570 --> 00:32:43.580
recognition software and if you think
 

00:32:43.580 --> 00:32:45.430
recognition software and if you think
about it nowadays this software is

00:32:45.430 --> 00:32:45.440
about it nowadays this software is
 

00:32:45.440 --> 00:32:47.770
about it nowadays this software is
pretty much ubiquitous from social media

00:32:47.770 --> 00:32:47.780
pretty much ubiquitous from social media
 

00:32:47.780 --> 00:32:51.790
pretty much ubiquitous from social media
to security systems another area that's

00:32:51.790 --> 00:32:51.800
to security systems another area that's
 

00:32:51.800 --> 00:32:53.890
to security systems another area that's
generated generated a lot of excitement

00:32:53.890 --> 00:32:53.900
generated generated a lot of excitement
 

00:32:53.900 --> 00:32:57.190
generated generated a lot of excitement
as of late is in autonomous vehicles and

00:32:57.190 --> 00:32:57.200
as of late is in autonomous vehicles and
 

00:32:57.200 --> 00:32:59.830
as of late is in autonomous vehicles and
self-driving cars so NVIDIA has a

00:32:59.830 --> 00:32:59.840
self-driving cars so NVIDIA has a
 

00:32:59.840 --> 00:33:02.830
self-driving cars so NVIDIA has a
research team working on a CNN based

00:33:02.830 --> 00:33:02.840
research team working on a CNN based
 

00:33:02.840 --> 00:33:04.900
research team working on a CNN based
system for end-to-end control of

00:33:04.900 --> 00:33:04.910
system for end-to-end control of
 

00:33:04.910 --> 00:33:09.250
system for end-to-end control of
self-driving cars their pipeline feeds a

00:33:09.250 --> 00:33:09.260
self-driving cars their pipeline feeds a
 

00:33:09.260 --> 00:33:11.680
self-driving cars their pipeline feeds a
single image from a camera on the car to

00:33:11.680 --> 00:33:11.690
single image from a camera on the car to
 

00:33:11.690 --> 00:33:14.710
single image from a camera on the car to
a CNN that directly outputs a single

00:33:14.710 --> 00:33:14.720
a CNN that directly outputs a single
 

00:33:14.720 --> 00:33:17.020
a CNN that directly outputs a single
number which is the steering is the

00:33:17.020 --> 00:33:17.030
number which is the steering is the
 

00:33:17.030 --> 00:33:19.570
number which is the steering is the
predicted steering wheel angle and the

00:33:19.570 --> 00:33:19.580
predicted steering wheel angle and the
 

00:33:19.580 --> 00:33:19.869
predicted steering wheel angle and the
man

00:33:19.869 --> 00:33:19.879
man
 

00:33:19.879 --> 00:33:21.879
man
you see in this video is actually one of

00:33:21.879 --> 00:33:21.889
you see in this video is actually one of
 

00:33:21.889 --> 00:33:26.349
you see in this video is actually one of
our guest lectures and on Thursday we'll

00:33:26.349 --> 00:33:26.359
our guest lectures and on Thursday we'll
 

00:33:26.359 --> 00:33:29.199
our guest lectures and on Thursday we'll
hear about how his team is is developing

00:33:29.199 --> 00:33:29.209
hear about how his team is is developing
 

00:33:29.209 --> 00:33:33.189
hear about how his team is is developing
this platform finally we've also seen a

00:33:33.189 --> 00:33:33.199
this platform finally we've also seen a
 

00:33:33.199 --> 00:33:35.439
this platform finally we've also seen a
significant impact in the medical field

00:33:35.439 --> 00:33:35.449
significant impact in the medical field
 

00:33:35.449 --> 00:33:37.509
significant impact in the medical field
where deep learning models have been

00:33:37.509 --> 00:33:37.519
where deep learning models have been
 

00:33:37.519 --> 00:33:39.609
where deep learning models have been
applied to the analysis of a whole host

00:33:39.609 --> 00:33:39.619
applied to the analysis of a whole host
 

00:33:39.619 --> 00:33:41.799
applied to the analysis of a whole host
of different types of medical images

00:33:41.799 --> 00:33:41.809
of different types of medical images
 

00:33:41.809 --> 00:33:44.859
of different types of medical images
just this past year a team from Stanford

00:33:44.859 --> 00:33:44.869
just this past year a team from Stanford
 

00:33:44.869 --> 00:33:47.219
just this past year a team from Stanford
developed a CNN that could achieve

00:33:47.219 --> 00:33:47.229
developed a CNN that could achieve
 

00:33:47.229 --> 00:33:49.569
developed a CNN that could achieve
dermatologists level classification of

00:33:49.569 --> 00:33:49.579
dermatologists level classification of
 

00:33:49.579 --> 00:33:52.930
dermatologists level classification of
skin lesions so you can imagine and this

00:33:52.930 --> 00:33:52.940
skin lesions so you can imagine and this
 

00:33:52.940 --> 00:33:55.059
skin lesions so you can imagine and this
is what they actually did having an app

00:33:55.059 --> 00:33:55.069
is what they actually did having an app
 

00:33:55.069 --> 00:33:57.269
is what they actually did having an app
on your phone where you take a picture

00:33:57.269 --> 00:33:57.279
on your phone where you take a picture
 

00:33:57.279 --> 00:33:59.979
on your phone where you take a picture
upload that picture to the app it's fed

00:33:59.979 --> 00:33:59.989
upload that picture to the app it's fed
 

00:33:59.989 --> 00:34:03.399
upload that picture to the app it's fed
into a CNN that gen that then generates

00:34:03.399 --> 00:34:03.409
into a CNN that gen that then generates
 

00:34:03.409 --> 00:34:05.409
into a CNN that gen that then generates
a prediction of whether or not that

00:34:05.409 --> 00:34:05.419
a prediction of whether or not that
 

00:34:05.419 --> 00:34:10.059
a prediction of whether or not that
lesion is reason for concern so to

00:34:10.059 --> 00:34:10.069
lesion is reason for concern so to
 

00:34:10.069 --> 00:34:12.039
lesion is reason for concern so to
summarize what we've covered in today's

00:34:12.039 --> 00:34:12.049
summarize what we've covered in today's
 

00:34:12.049 --> 00:34:15.460
summarize what we've covered in today's
lecture we first consider the origins of

00:34:15.460 --> 00:34:15.470
lecture we first consider the origins of
 

00:34:15.470 --> 00:34:17.409
lecture we first consider the origins of
the computer vision problem how we can

00:34:17.409 --> 00:34:17.419
the computer vision problem how we can
 

00:34:17.419 --> 00:34:20.230
the computer vision problem how we can
represent images as arrays of pixel

00:34:20.230 --> 00:34:20.240
represent images as arrays of pixel
 

00:34:20.240 --> 00:34:22.749
represent images as arrays of pixel
values and what convolutions are how

00:34:22.749 --> 00:34:22.759
values and what convolutions are how
 

00:34:22.759 --> 00:34:25.269
values and what convolutions are how
they work we then discuss the basic

00:34:25.269 --> 00:34:25.279
they work we then discuss the basic
 

00:34:25.279 --> 00:34:27.549
they work we then discuss the basic
architecture of cnn's

00:34:27.549 --> 00:34:27.559
architecture of cnn's
 

00:34:27.559 --> 00:34:30.309
architecture of cnn's
and finally we extended this to consider

00:34:30.309 --> 00:34:30.319
and finally we extended this to consider
 

00:34:30.319 --> 00:34:33.089
and finally we extended this to consider
some different applications of

00:34:33.089 --> 00:34:33.099
some different applications of
 

00:34:33.099 --> 00:34:36.039
some different applications of
convolutional neural networks and also

00:34:36.039 --> 00:34:36.049
convolutional neural networks and also
 

00:34:36.049 --> 00:34:37.990
convolutional neural networks and also
talked a bit about how we can visualize

00:34:37.990 --> 00:34:38.000
talked a bit about how we can visualize
 

00:34:38.000 --> 00:34:41.470
talked a bit about how we can visualize
their behavior so with that I'd like to

00:34:41.470 --> 00:34:41.480
their behavior so with that I'd like to
 

00:34:41.480 --> 00:34:44.369
their behavior so with that I'd like to
conclude I'm happy to take questions

00:34:44.369 --> 00:34:44.379
conclude I'm happy to take questions
 

00:34:44.379 --> 00:34:48.249
conclude I'm happy to take questions
after the lecture portion is over it's

00:34:48.249 --> 00:34:48.259
after the lecture portion is over it's
 

00:34:48.259 --> 00:34:50.470
after the lecture portion is over it's
now my pleasure to introduce our next

00:34:50.470 --> 00:34:50.480
now my pleasure to introduce our next
 

00:34:50.480 --> 00:34:53.829
now my pleasure to introduce our next
speaker a special guest professor Aaron

00:34:53.829 --> 00:34:53.839
speaker a special guest professor Aaron
 

00:34:53.839 --> 00:34:55.240
speaker a special guest professor Aaron
Kerrville from the University of

00:34:55.240 --> 00:34:55.250
Kerrville from the University of
 

00:34:55.250 --> 00:34:56.039
Kerrville from the University of
Montreal

00:34:56.039 --> 00:34:56.049
Montreal
 

00:34:56.049 --> 00:34:58.299
Montreal
professor Kerrville is one of the

00:34:58.299 --> 00:34:58.309
professor Kerrville is one of the
 

00:34:58.309 --> 00:35:00.700
professor Kerrville is one of the
creators of generative adversarial

00:35:00.700 --> 00:35:00.710
creators of generative adversarial
 

00:35:00.710 --> 00:35:02.470
creators of generative adversarial
networks and he'll be talking to us

00:35:02.470 --> 00:35:02.480
networks and he'll be talking to us
 

00:35:02.480 --> 00:35:04.990
networks and he'll be talking to us
about deep generative models and their

00:35:04.990 --> 00:35:05.000
about deep generative models and their
 

00:35:05.000 --> 00:35:07.150
about deep generative models and their
applications so please join me in

00:35:07.150 --> 00:35:07.160
applications so please join me in
 

00:35:07.160 --> 00:35:07.950
applications so please join me in
welcoming him

00:35:07.950 --> 00:35:07.960
welcoming him
 

00:35:07.960 --> 00:35:11.340
welcoming him
[Applause]

