WEBVTT
Kind: captions
Language: en

00:00:03.330 --> 00:00:07.080
 
hi everyone so nice to see so many

00:00:07.080 --> 00:00:07.090
hi everyone so nice to see so many
 

00:00:07.090 --> 00:00:08.670
hi everyone so nice to see so many
familiar faces and glad we haven't

00:00:08.670 --> 00:00:08.680
familiar faces and glad we haven't
 

00:00:08.680 --> 00:00:10.530
familiar faces and glad we haven't
scared you off yet

00:00:10.530 --> 00:00:10.540
scared you off yet
 

00:00:10.540 --> 00:00:13.170
scared you off yet
so today I'm gonna be talking about an

00:00:13.170 --> 00:00:13.180
so today I'm gonna be talking about an
 

00:00:13.180 --> 00:00:15.480
so today I'm gonna be talking about an
extension on a lot of the computer

00:00:15.480 --> 00:00:15.490
extension on a lot of the computer
 

00:00:15.490 --> 00:00:16.679
extension on a lot of the computer
vision techniques that Avila was

00:00:16.679 --> 00:00:16.689
vision techniques that Avila was
 

00:00:16.689 --> 00:00:18.810
vision techniques that Avila was
discussing in the previous lecture and

00:00:18.810 --> 00:00:18.820
discussing in the previous lecture and
 

00:00:18.820 --> 00:00:21.089
discussing in the previous lecture and
specifically focusing on a class of

00:00:21.089 --> 00:00:21.099
specifically focusing on a class of
 

00:00:21.099 --> 00:00:23.999
specifically focusing on a class of
learning problems for generating new

00:00:23.999 --> 00:00:24.009
learning problems for generating new
 

00:00:24.009 --> 00:00:26.429
learning problems for generating new
data and we refer to these problems or

00:00:26.429 --> 00:00:26.439
data and we refer to these problems or
 

00:00:26.439 --> 00:00:29.519
data and we refer to these problems or
these models as generative models so

00:00:29.519 --> 00:00:29.529
these models as generative models so
 

00:00:29.529 --> 00:00:31.439
these models as generative models so
these are actually systems that don't

00:00:31.439 --> 00:00:31.449
these are actually systems that don't
 

00:00:31.449 --> 00:00:34.290
these are actually systems that don't
actually look to only extract patterns

00:00:34.290 --> 00:00:34.300
actually look to only extract patterns
 

00:00:34.300 --> 00:00:36.930
actually look to only extract patterns
and data but they go step beyond that

00:00:36.930 --> 00:00:36.940
and data but they go step beyond that
 

00:00:36.940 --> 00:00:39.120
and data but they go step beyond that
and actually use those patterns to

00:00:39.120 --> 00:00:39.130
and actually use those patterns to
 

00:00:39.130 --> 00:00:40.340
and actually use those patterns to
actually learn the underlying

00:00:40.340 --> 00:00:40.350
actually learn the underlying
 

00:00:40.350 --> 00:00:43.560
actually learn the underlying
distribution of that data and use it to

00:00:43.560 --> 00:00:43.570
distribution of that data and use it to
 

00:00:43.570 --> 00:00:46.110
distribution of that data and use it to
generate brand new data this is an

00:00:46.110 --> 00:00:46.120
generate brand new data this is an
 

00:00:46.120 --> 00:00:48.000
generate brand new data this is an
incredibly complex idea and it's

00:00:48.000 --> 00:00:48.010
incredibly complex idea and it's
 

00:00:48.010 --> 00:00:49.500
incredibly complex idea and it's
something that we really haven't seen in

00:00:49.500 --> 00:00:49.510
something that we really haven't seen in
 

00:00:49.510 --> 00:00:52.410
something that we really haven't seen in
this course previously and it's a

00:00:52.410 --> 00:00:52.420
this course previously and it's a
 

00:00:52.420 --> 00:00:55.110
this course previously and it's a
particular subset of deep learning and

00:00:55.110 --> 00:00:55.120
particular subset of deep learning and
 

00:00:55.120 --> 00:00:56.700
particular subset of deep learning and
machine learning that's enjoying a lot

00:00:56.700 --> 00:00:56.710
machine learning that's enjoying a lot
 

00:00:56.710 --> 00:00:58.530
machine learning that's enjoying a lot
of success especially in the past couple

00:00:58.530 --> 00:00:58.540
of success especially in the past couple
 

00:00:58.540 --> 00:01:01.500
of success especially in the past couple
years so first to start off I want to

00:01:01.500 --> 00:01:01.510
years so first to start off I want to
 

00:01:01.510 --> 00:01:05.369
years so first to start off I want to
show you an example of two images so

00:01:05.369 --> 00:01:05.379
show you an example of two images so
 

00:01:05.379 --> 00:01:08.160
show you an example of two images so
these are two faces and I want to take a

00:01:08.160 --> 00:01:08.170
these are two faces and I want to take a
 

00:01:08.170 --> 00:01:11.490
these are two faces and I want to take a
poll now about which face you guys think

00:01:11.490 --> 00:01:11.500
poll now about which face you guys think
 

00:01:11.500 --> 00:01:15.149
poll now about which face you guys think
is fake which is a which face is not

00:01:15.149 --> 00:01:15.159
is fake which is a which face is not
 

00:01:15.159 --> 00:01:17.550
is fake which is a which face is not
real so if you guys think that the left

00:01:17.550 --> 00:01:17.560
real so if you guys think that the left
 

00:01:17.560 --> 00:01:20.520
real so if you guys think that the left
face this one is not fake how about you

00:01:20.520 --> 00:01:20.530
face this one is not fake how about you
 

00:01:20.530 --> 00:01:23.940
face this one is not fake how about you
raise your hands sorry

00:01:23.940 --> 00:01:23.950
raise your hands sorry
 

00:01:23.950 --> 00:01:30.140
raise your hands sorry
fake okay I think like 20 30 percent

00:01:30.140 --> 00:01:30.150
fake okay I think like 20 30 percent
 

00:01:30.150 --> 00:01:34.410
fake okay I think like 20 30 percent
okay how about this one okay that's a

00:01:34.410 --> 00:01:34.420
okay how about this one okay that's a
 

00:01:34.420 --> 00:01:37.890
okay how about this one okay that's a
lot higher interesting okay so the

00:01:37.890 --> 00:01:37.900
lot higher interesting okay so the
 

00:01:37.900 --> 00:01:43.630
lot higher interesting okay so the
answer is actually both are fake okay

00:01:43.630 --> 00:01:43.640
answer is actually both are fake okay
 

00:01:43.640 --> 00:01:46.720
answer is actually both are fake okay
so this is incredibly amazing to me

00:01:46.720 --> 00:01:46.730
so this is incredibly amazing to me
 

00:01:46.730 --> 00:01:48.430
so this is incredibly amazing to me
because both of these faces when I first

00:01:48.430 --> 00:01:48.440
because both of these faces when I first
 

00:01:48.440 --> 00:01:50.710
because both of these faces when I first
looked at them first of all incredibly

00:01:50.710 --> 00:01:50.720
looked at them first of all incredibly
 

00:01:50.720 --> 00:01:52.120
looked at them first of all incredibly
high dimensional they're very

00:01:52.120 --> 00:01:52.130
high dimensional they're very
 

00:01:52.130 --> 00:01:55.000
high dimensional they're very
high-resolution and to me I see a lot of

00:01:55.000 --> 00:01:55.010
high-resolution and to me I see a lot of
 

00:01:55.010 --> 00:01:56.890
high-resolution and to me I see a lot of
details in the faces I choose a wrinkle

00:01:56.890 --> 00:01:56.900
details in the faces I choose a wrinkle
 

00:01:56.900 --> 00:02:00.940
details in the faces I choose a wrinkle
structure I see shadows I mean like I

00:02:00.940 --> 00:02:00.950
structure I see shadows I mean like I
 

00:02:00.950 --> 00:02:02.620
structure I see shadows I mean like I
even see coloring in the teeth and

00:02:02.620 --> 00:02:02.630
even see coloring in the teeth and
 

00:02:02.630 --> 00:02:04.690
even see coloring in the teeth and
special like defects in the teeth it's

00:02:04.690 --> 00:02:04.700
special like defects in the teeth it's
 

00:02:04.700 --> 00:02:07.090
special like defects in the teeth it's
incredibly detailed what's going on in

00:02:07.090 --> 00:02:07.100
incredibly detailed what's going on in
 

00:02:07.100 --> 00:02:09.249
incredibly detailed what's going on in
these images and they're both completely

00:02:09.249 --> 00:02:09.259
these images and they're both completely
 

00:02:09.259 --> 00:02:11.350
these images and they're both completely
artificial these people don't exist in

00:02:11.350 --> 00:02:11.360
artificial these people don't exist in
 

00:02:11.360 --> 00:02:16.210
artificial these people don't exist in
real life so now let's take a step back

00:02:16.210 --> 00:02:16.220
real life so now let's take a step back
 

00:02:16.220 --> 00:02:18.640
real life so now let's take a step back
and actually learn what is generative

00:02:18.640 --> 00:02:18.650
and actually learn what is generative
 

00:02:18.650 --> 00:02:21.070
and actually learn what is generative
modeling so generative modeling is a

00:02:21.070 --> 00:02:21.080
modeling so generative modeling is a
 

00:02:21.080 --> 00:02:23.500
modeling so generative modeling is a
subset of unsupervised learning so far

00:02:23.500 --> 00:02:23.510
subset of unsupervised learning so far
 

00:02:23.510 --> 00:02:24.699
subset of unsupervised learning so far
in this class we've been dealing with

00:02:24.699 --> 00:02:24.709
in this class we've been dealing with
 

00:02:24.709 --> 00:02:27.130
in this class we've been dealing with
models which are focused on supervised

00:02:27.130 --> 00:02:27.140
models which are focused on supervised
 

00:02:27.140 --> 00:02:29.500
models which are focused on supervised
learning models so supervised learning

00:02:29.500 --> 00:02:29.510
learning models so supervised learning
 

00:02:29.510 --> 00:02:33.520
learning models so supervised learning
takes as input the data X which we've

00:02:33.520 --> 00:02:33.530
takes as input the data X which we've
 

00:02:33.530 --> 00:02:36.070
takes as input the data X which we've
been calling it and the labels Y it

00:02:36.070 --> 00:02:36.080
been calling it and the labels Y it
 

00:02:36.080 --> 00:02:37.750
been calling it and the labels Y it
attempts to learn this functional

00:02:37.750 --> 00:02:37.760
attempts to learn this functional
 

00:02:37.760 --> 00:02:40.090
attempts to learn this functional
mapping from X to Y so you give it a lot

00:02:40.090 --> 00:02:40.100
mapping from X to Y so you give it a lot
 

00:02:40.100 --> 00:02:41.800
mapping from X to Y so you give it a lot
of data and you want to learn the

00:02:41.800 --> 00:02:41.810
of data and you want to learn the
 

00:02:41.810 --> 00:02:44.890
of data and you want to learn the
classification or you want to learn the

00:02:44.890 --> 00:02:44.900
classification or you want to learn the
 

00:02:44.900 --> 00:02:47.350
classification or you want to learn the
regression problem for example so in in

00:02:47.350 --> 00:02:47.360
regression problem for example so in in
 

00:02:47.360 --> 00:02:49.479
regression problem for example so in in
one example that Abba gave you give it

00:02:49.479 --> 00:02:49.489
one example that Abba gave you give it
 

00:02:49.489 --> 00:02:51.400
one example that Abba gave you give it
pictures of the road from self-driving

00:02:51.400 --> 00:02:51.410
pictures of the road from self-driving
 

00:02:51.410 --> 00:02:52.810
pictures of the road from self-driving
car and you want to learn the steering

00:02:52.810 --> 00:02:52.820
car and you want to learn the steering
 

00:02:52.820 --> 00:02:54.550
car and you want to learn the steering
wheel angle that's a single number

00:02:54.550 --> 00:02:54.560
wheel angle that's a single number
 

00:02:54.560 --> 00:02:58.240
wheel angle that's a single number
that's a supervised learning problem now

00:02:58.240 --> 00:02:58.250
that's a supervised learning problem now
 

00:02:58.250 --> 00:02:59.890
that's a supervised learning problem now
an unsupervised learning we're actually

00:02:59.890 --> 00:02:59.900
an unsupervised learning we're actually
 

00:02:59.900 --> 00:03:01.930
an unsupervised learning we're actually
talking a complete tackling a completely

00:03:01.930 --> 00:03:01.940
talking a complete tackling a completely
 

00:03:01.940 --> 00:03:04.420
talking a complete tackling a completely
different problem we're just giving it

00:03:04.420 --> 00:03:04.430
different problem we're just giving it
 

00:03:04.430 --> 00:03:06.550
different problem we're just giving it
data now and we're trying to learn some

00:03:06.550 --> 00:03:06.560
data now and we're trying to learn some
 

00:03:06.560 --> 00:03:08.410
data now and we're trying to learn some
underlying patterns or features from

00:03:08.410 --> 00:03:08.420
underlying patterns or features from
 

00:03:08.420 --> 00:03:10.750
underlying patterns or features from
this data so there's no labels involved

00:03:10.750 --> 00:03:10.760
this data so there's no labels involved
 

00:03:10.760 --> 00:03:12.070
this data so there's no labels involved
but we still want to learn something

00:03:12.070 --> 00:03:12.080
but we still want to learn something
 

00:03:12.080 --> 00:03:14.259
but we still want to learn something
meaningful we want to actually learn

00:03:14.259 --> 00:03:14.269
meaningful we want to actually learn
 

00:03:14.269 --> 00:03:16.479
meaningful we want to actually learn
some underlying structure that's capable

00:03:16.479 --> 00:03:16.489
some underlying structure that's capable
 

00:03:16.489 --> 00:03:18.880
some underlying structure that's capable
of generating brand-new data from this

00:03:18.880 --> 00:03:18.890
of generating brand-new data from this
 

00:03:18.890 --> 00:03:23.130
of generating brand-new data from this
from this set of inputs that we receive

00:03:23.130 --> 00:03:23.140
from this set of inputs that we receive
 

00:03:23.140 --> 00:03:25.840
from this set of inputs that we receive
so like I said the goal here is to take

00:03:25.840 --> 00:03:25.850
so like I said the goal here is to take
 

00:03:25.850 --> 00:03:27.670
so like I said the goal here is to take
training examples from one input

00:03:27.670 --> 00:03:27.680
training examples from one input
 

00:03:27.680 --> 00:03:30.400
training examples from one input
distribution and actually learn a model

00:03:30.400 --> 00:03:30.410
distribution and actually learn a model
 

00:03:30.410 --> 00:03:32.620
distribution and actually learn a model
that represents that distribution once

00:03:32.620 --> 00:03:32.630
that represents that distribution once
 

00:03:32.630 --> 00:03:34.509
that represents that distribution once
we have that model we can use that model

00:03:34.509 --> 00:03:34.519
we have that model we can use that model
 

00:03:34.519 --> 00:03:36.550
we have that model we can use that model
to then generate brand new data so let

00:03:36.550 --> 00:03:36.560
to then generate brand new data so let
 

00:03:36.560 --> 00:03:38.890
to then generate brand new data so let
me give you an example we can do this in

00:03:38.890 --> 00:03:38.900
me give you an example we can do this in
 

00:03:38.900 --> 00:03:41.050
me give you an example we can do this in
one of two ways the first way is to

00:03:41.050 --> 00:03:41.060
one of two ways the first way is to
 

00:03:41.060 --> 00:03:43.569
one of two ways the first way is to
density estimation so let's suppose I

00:03:43.569 --> 00:03:43.579
density estimation so let's suppose I
 

00:03:43.579 --> 00:03:44.949
density estimation so let's suppose I
give you a lot of points in a one

00:03:44.949 --> 00:03:44.959
give you a lot of points in a one
 

00:03:44.959 --> 00:03:47.560
give you a lot of points in a one
dimensional grid drawn at random from

00:03:47.560 --> 00:03:47.570
dimensional grid drawn at random from
 

00:03:47.570 --> 00:03:49.000
dimensional grid drawn at random from
this distribution that I'm not showing

00:03:49.000 --> 00:03:49.010
this distribution that I'm not showing
 

00:03:49.010 --> 00:03:50.890
this distribution that I'm not showing
you but I'm giving you the points here

00:03:50.890 --> 00:03:50.900
you but I'm giving you the points here
 

00:03:50.900 --> 00:03:53.140
you but I'm giving you the points here
can you create this density function

00:03:53.140 --> 00:03:53.150
can you create this density function
 

00:03:53.150 --> 00:03:55.930
can you create this density function
just by observing those points so can

00:03:55.930 --> 00:03:55.940
just by observing those points so can
 

00:03:55.940 --> 00:03:57.870
just by observing those points so can
you observe the underlying dis

00:03:57.870 --> 00:03:57.880
you observe the underlying dis
 

00:03:57.880 --> 00:03:59.700
you observe the underlying dis
abuse that generated those points if you

00:03:59.700 --> 00:03:59.710
abuse that generated those points if you
 

00:03:59.710 --> 00:04:01.560
abuse that generated those points if you
can create this probability distribution

00:04:01.560 --> 00:04:01.570
can create this probability distribution
 

00:04:01.570 --> 00:04:02.550
can create this probability distribution
here

00:04:02.550 --> 00:04:02.560
here
 

00:04:02.560 --> 00:04:04.500
here
well now you can actually generate brand

00:04:04.500 --> 00:04:04.510
well now you can actually generate brand
 

00:04:04.510 --> 00:04:05.670
well now you can actually generate brand
new points according to that

00:04:05.670 --> 00:04:05.680
new points according to that
 

00:04:05.680 --> 00:04:09.390
new points according to that
distribution another example more

00:04:09.390 --> 00:04:09.400
distribution another example more
 

00:04:09.400 --> 00:04:11.730
distribution another example more
complex example is using sample

00:04:11.730 --> 00:04:11.740
complex example is using sample
 

00:04:11.740 --> 00:04:13.620
complex example is using sample
generation so now you're not focused on

00:04:13.620 --> 00:04:13.630
generation so now you're not focused on
 

00:04:13.630 --> 00:04:15.780
generation so now you're not focused on
only estimating the density of this

00:04:15.780 --> 00:04:15.790
only estimating the density of this
 

00:04:15.790 --> 00:04:17.699
only estimating the density of this
function but now you actually want to

00:04:17.699 --> 00:04:17.709
function but now you actually want to
 

00:04:17.709 --> 00:04:19.650
function but now you actually want to
generate brand new samples go go

00:04:19.650 --> 00:04:19.660
generate brand new samples go go
 

00:04:19.660 --> 00:04:22.320
generate brand new samples go go
straight to the generation portion so

00:04:22.320 --> 00:04:22.330
straight to the generation portion so
 

00:04:22.330 --> 00:04:24.210
straight to the generation portion so
here I give you a lot of training

00:04:24.210 --> 00:04:24.220
here I give you a lot of training
 

00:04:24.220 --> 00:04:26.460
here I give you a lot of training
examples which are just images on the

00:04:26.460 --> 00:04:26.470
examples which are just images on the
 

00:04:26.470 --> 00:04:30.090
examples which are just images on the
left hand side right here and these were

00:04:30.090 --> 00:04:30.100
left hand side right here and these were
 

00:04:30.100 --> 00:04:32.640
left hand side right here and these were
drawn some sorry these were drawn from

00:04:32.640 --> 00:04:32.650
drawn some sorry these were drawn from
 

00:04:32.650 --> 00:04:35.310
drawn some sorry these were drawn from
some distribution which we'll call P

00:04:35.310 --> 00:04:35.320
some distribution which we'll call P
 

00:04:35.320 --> 00:04:37.470
some distribution which we'll call P
data okay this is a probability

00:04:37.470 --> 00:04:37.480
data okay this is a probability
 

00:04:37.480 --> 00:04:40.710
data okay this is a probability
distribution of the data and we want to

00:04:40.710 --> 00:04:40.720
distribution of the data and we want to
 

00:04:40.720 --> 00:04:43.230
distribution of the data and we want to
generate brand new training examples

00:04:43.230 --> 00:04:43.240
generate brand new training examples
 

00:04:43.240 --> 00:04:44.820
generate brand new training examples
that are drawn from another distribution

00:04:44.820 --> 00:04:44.830
that are drawn from another distribution
 

00:04:44.830 --> 00:04:47.370
that are drawn from another distribution
which we'll call from the probability of

00:04:47.370 --> 00:04:47.380
which we'll call from the probability of
 

00:04:47.380 --> 00:04:49.230
which we'll call from the probability of
the model the probability distribution

00:04:49.230 --> 00:04:49.240
the model the probability distribution
 

00:04:49.240 --> 00:04:51.870
the model the probability distribution
of the model and our goal in this whole

00:04:51.870 --> 00:04:51.880
of the model and our goal in this whole
 

00:04:51.880 --> 00:04:55.230
of the model and our goal in this whole
task is to actually learn a probability

00:04:55.230 --> 00:04:55.240
task is to actually learn a probability
 

00:04:55.240 --> 00:04:56.910
task is to actually learn a probability
distribution of the model that is as

00:04:56.910 --> 00:04:56.920
distribution of the model that is as
 

00:04:56.920 --> 00:04:59.130
distribution of the model that is as
similar as possible to the probability

00:04:59.130 --> 00:04:59.140
similar as possible to the probability
 

00:04:59.140 --> 00:05:01.350
similar as possible to the probability
distribution of the data so when we draw

00:05:01.350 --> 00:05:01.360
distribution of the data so when we draw
 

00:05:01.360 --> 00:05:03.840
distribution of the data so when we draw
samples from the model they almost look

00:05:03.840 --> 00:05:03.850
samples from the model they almost look
 

00:05:03.850 --> 00:05:05.100
samples from the model they almost look
like they were being drawn from the

00:05:05.100 --> 00:05:05.110
like they were being drawn from the
 

00:05:05.110 --> 00:05:07.530
like they were being drawn from the
original probability distribution that

00:05:07.530 --> 00:05:07.540
original probability distribution that
 

00:05:07.540 --> 00:05:11.760
original probability distribution that
generated the real data so why do we

00:05:11.760 --> 00:05:11.770
generated the real data so why do we
 

00:05:11.770 --> 00:05:13.620
generated the real data so why do we
care about actually learning the

00:05:13.620 --> 00:05:13.630
care about actually learning the
 

00:05:13.630 --> 00:05:16.350
care about actually learning the
underlying model here one really good

00:05:16.350 --> 00:05:16.360
underlying model here one really good
 

00:05:16.360 --> 00:05:17.610
underlying model here one really good
example is something that you'll get

00:05:17.610 --> 00:05:17.620
example is something that you'll get
 

00:05:17.620 --> 00:05:19.350
example is something that you'll get
exposure to in the lab that Abu is

00:05:19.350 --> 00:05:19.360
exposure to in the lab that Abu is
 

00:05:19.360 --> 00:05:20.700
exposure to in the lab that Abu is
mentioning where we're doing facial

00:05:20.700 --> 00:05:20.710
mentioning where we're doing facial
 

00:05:20.710 --> 00:05:23.520
mentioning where we're doing facial
classification for D biasing the key

00:05:23.520 --> 00:05:23.530
classification for D biasing the key
 

00:05:23.530 --> 00:05:26.400
classification for D biasing the key
aspect of D biasing is K is being able

00:05:26.400 --> 00:05:26.410
aspect of D biasing is K is being able
 

00:05:26.410 --> 00:05:28.740
aspect of D biasing is K is being able
to learn the underlying latent variables

00:05:28.740 --> 00:05:28.750
to learn the underlying latent variables
 

00:05:28.750 --> 00:05:31.320
to learn the underlying latent variables
intrinsic within the data set a lot of

00:05:31.320 --> 00:05:31.330
intrinsic within the data set a lot of
 

00:05:31.330 --> 00:05:32.550
intrinsic within the data set a lot of
times when we were presented with a

00:05:32.550 --> 00:05:32.560
times when we were presented with a
 

00:05:32.560 --> 00:05:34.650
times when we were presented with a
machine learning problem we're given a

00:05:34.650 --> 00:05:34.660
machine learning problem we're given a
 

00:05:34.660 --> 00:05:36.600
machine learning problem we're given a
lot of data sometimes it's not properly

00:05:36.600 --> 00:05:36.610
lot of data sometimes it's not properly
 

00:05:36.610 --> 00:05:38.760
lot of data sometimes it's not properly
vetted or labeled and we want to make

00:05:38.760 --> 00:05:38.770
vetted or labeled and we want to make
 

00:05:38.770 --> 00:05:40.590
vetted or labeled and we want to make
sure that there are no biases hidden

00:05:40.590 --> 00:05:40.600
sure that there are no biases hidden
 

00:05:40.600 --> 00:05:42.570
sure that there are no biases hidden
inside this data set so sometimes you

00:05:42.570 --> 00:05:42.580
inside this data set so sometimes you
 

00:05:42.580 --> 00:05:44.880
inside this data set so sometimes you
might see that maybe if you're given a

00:05:44.880 --> 00:05:44.890
might see that maybe if you're given a
 

00:05:44.890 --> 00:05:47.190
might see that maybe if you're given a
data set of a facial classifier you're

00:05:47.190 --> 00:05:47.200
data set of a facial classifier you're
 

00:05:47.200 --> 00:05:48.720
data set of a facial classifier you're
given a lot of faces and a lot of not

00:05:48.720 --> 00:05:48.730
given a lot of faces and a lot of not
 

00:05:48.730 --> 00:05:50.760
given a lot of faces and a lot of not
faces you want to learn now a classifier

00:05:50.760 --> 00:05:50.770
faces you want to learn now a classifier
 

00:05:50.770 --> 00:05:53.550
faces you want to learn now a classifier
to predict faces maybe all the faces

00:05:53.550 --> 00:05:53.560
to predict faces maybe all the faces
 

00:05:53.560 --> 00:05:55.080
to predict faces maybe all the faces
that you're given are falling from one

00:05:55.080 --> 00:05:55.090
that you're given are falling from one
 

00:05:55.090 --> 00:05:56.370
that you're given are falling from one
distribution that is not actually

00:05:56.370 --> 00:05:56.380
distribution that is not actually
 

00:05:56.380 --> 00:05:58.590
distribution that is not actually
representative of the real distribution

00:05:58.590 --> 00:05:58.600
representative of the real distribution
 

00:05:58.600 --> 00:06:00.750
representative of the real distribution
that you want to be testing on so let's

00:06:00.750 --> 00:06:00.760
that you want to be testing on so let's
 

00:06:00.760 --> 00:06:02.610
that you want to be testing on so let's
suppose maybe you have a lot of faces

00:06:02.610 --> 00:06:02.620
suppose maybe you have a lot of faces
 

00:06:02.620 --> 00:06:05.100
suppose maybe you have a lot of faces
from one ethnicity or one gender type

00:06:05.100 --> 00:06:05.110
from one ethnicity or one gender type
 

00:06:05.110 --> 00:06:07.890
from one ethnicity or one gender type
can you detect in an unsupervised manner

00:06:07.890 --> 00:06:07.900
can you detect in an unsupervised manner
 

00:06:07.900 --> 00:06:10.020
can you detect in an unsupervised manner
by learning the underlying distributions

00:06:10.020 --> 00:06:10.030
by learning the underlying distributions
 

00:06:10.030 --> 00:06:11.279
by learning the underlying distributions
without being told

00:06:11.279 --> 00:06:11.289
without being told
 

00:06:11.289 --> 00:06:12.899
without being told
to look for something like gender or

00:06:12.899 --> 00:06:12.909
to look for something like gender or
 

00:06:12.909 --> 00:06:15.329
to look for something like gender or
ethnicity can you just learn this

00:06:15.329 --> 00:06:15.339
ethnicity can you just learn this
 

00:06:15.339 --> 00:06:17.070
ethnicity can you just learn this
underlying distribution of the data and

00:06:17.070 --> 00:06:17.080
underlying distribution of the data and
 

00:06:17.080 --> 00:06:18.989
underlying distribution of the data and
then use that distribution during

00:06:18.989 --> 00:06:18.999
then use that distribution during
 

00:06:18.999 --> 00:06:21.749
then use that distribution during
training time 2d bias your model this is

00:06:21.749 --> 00:06:21.759
training time 2d bias your model this is
 

00:06:21.759 --> 00:06:23.309
training time 2d bias your model this is
something that you'll get exposure with

00:06:23.309 --> 00:06:23.319
something that you'll get exposure with
 

00:06:23.319 --> 00:06:24.959
something that you'll get exposure with
during the lab today and you'll actually

00:06:24.959 --> 00:06:24.969
during the lab today and you'll actually
 

00:06:24.969 --> 00:06:28.290
during the lab today and you'll actually
do biased biased facial classifiers that

00:06:28.290 --> 00:06:28.300
do biased biased facial classifiers that
 

00:06:28.300 --> 00:06:31.619
do biased biased facial classifiers that
you'll create another great example is

00:06:31.619 --> 00:06:31.629
you'll create another great example is
 

00:06:31.629 --> 00:06:33.839
you'll create another great example is
for outlier detection and it's actually

00:06:33.839 --> 00:06:33.849
for outlier detection and it's actually
 

00:06:33.849 --> 00:06:35.820
for outlier detection and it's actually
a very related example to the D biasing

00:06:35.820 --> 00:06:35.830
a very related example to the D biasing
 

00:06:35.830 --> 00:06:38.010
a very related example to the D biasing
one that I just gave here the problem is

00:06:38.010 --> 00:06:38.020
one that I just gave here the problem is
 

00:06:38.020 --> 00:06:40.739
one that I just gave here the problem is
a great example of the problem is in

00:06:40.739 --> 00:06:40.749
a great example of the problem is in
 

00:06:40.749 --> 00:06:42.689
a great example of the problem is in
self-driving cars so when we're

00:06:42.689 --> 00:06:42.699
self-driving cars so when we're
 

00:06:42.699 --> 00:06:44.909
self-driving cars so when we're
collecting self-driving car data we

00:06:44.909 --> 00:06:44.919
collecting self-driving car data we
 

00:06:44.919 --> 00:06:46.199
collecting self-driving car data we
collect a lot of data that looks

00:06:46.199 --> 00:06:46.209
collect a lot of data that looks
 

00:06:46.209 --> 00:06:49.619
collect a lot of data that looks
extremely similar so 95% of the data

00:06:49.619 --> 00:06:49.629
extremely similar so 95% of the data
 

00:06:49.629 --> 00:06:51.420
extremely similar so 95% of the data
that we collect is probably going to be

00:06:51.420 --> 00:06:51.430
that we collect is probably going to be
 

00:06:51.430 --> 00:06:53.459
that we collect is probably going to be
on very simple roads straight roads

00:06:53.459 --> 00:06:53.469
on very simple roads straight roads
 

00:06:53.469 --> 00:06:55.980
on very simple roads straight roads
maybe highway driving not very complex

00:06:55.980 --> 00:06:55.990
maybe highway driving not very complex
 

00:06:55.990 --> 00:06:57.600
maybe highway driving not very complex
scenarios and not really interesting

00:06:57.600 --> 00:06:57.610
scenarios and not really interesting
 

00:06:57.610 --> 00:06:59.100
scenarios and not really interesting
scenarios not the edge cases that we

00:06:59.100 --> 00:06:59.110
scenarios not the edge cases that we
 

00:06:59.110 --> 00:07:02.339
scenarios not the edge cases that we
really care about one strategy is to use

00:07:02.339 --> 00:07:02.349
really care about one strategy is to use
 

00:07:02.349 --> 00:07:05.850
really care about one strategy is to use
generative modeling to detect the

00:07:05.850 --> 00:07:05.860
generative modeling to detect the
 

00:07:05.860 --> 00:07:07.230
generative modeling to detect the
outliers here so we want to learn the

00:07:07.230 --> 00:07:07.240
outliers here so we want to learn the
 

00:07:07.240 --> 00:07:09.059
outliers here so we want to learn the
underlying distribution of the data and

00:07:09.059 --> 00:07:09.069
underlying distribution of the data and
 

00:07:09.069 --> 00:07:11.969
underlying distribution of the data and
then actually understand that when we

00:07:11.969 --> 00:07:11.979
then actually understand that when we
 

00:07:11.979 --> 00:07:14.279
then actually understand that when we
see events like this can we detect that

00:07:14.279 --> 00:07:14.289
see events like this can we detect that
 

00:07:14.289 --> 00:07:15.899
see events like this can we detect that
they are coming from the tail end of the

00:07:15.899 --> 00:07:15.909
they are coming from the tail end of the
 

00:07:15.909 --> 00:07:17.549
they are coming from the tail end of the
distribution that we should pay more

00:07:17.549 --> 00:07:17.559
distribution that we should pay more
 

00:07:17.559 --> 00:07:19.469
distribution that we should pay more
attention to these points instead of

00:07:19.469 --> 00:07:19.479
attention to these points instead of
 

00:07:19.479 --> 00:07:21.779
attention to these points instead of
always focusing on the very simple or

00:07:21.779 --> 00:07:21.789
always focusing on the very simple or
 

00:07:21.789 --> 00:07:23.399
always focusing on the very simple or
not complex examples which are coming

00:07:23.399 --> 00:07:23.409
not complex examples which are coming
 

00:07:23.409 --> 00:07:27.199
not complex examples which are coming
from the main mode of the distribution

00:07:27.199 --> 00:07:27.209
from the main mode of the distribution
 

00:07:27.209 --> 00:07:31.439
from the main mode of the distribution
so let's start discussing two classes of

00:07:31.439 --> 00:07:31.449
so let's start discussing two classes of
 

00:07:31.449 --> 00:07:33.179
so let's start discussing two classes of
models that we'll talk about in this

00:07:33.179 --> 00:07:33.189
models that we'll talk about in this
 

00:07:33.189 --> 00:07:36.509
models that we'll talk about in this
lecture the first being auto-encoders

00:07:36.509 --> 00:07:36.519
lecture the first being auto-encoders
 

00:07:36.519 --> 00:07:40.019
lecture the first being auto-encoders
and variational autoencoders the second

00:07:40.019 --> 00:07:40.029
and variational autoencoders the second
 

00:07:40.029 --> 00:07:41.939
and variational autoencoders the second
class of models I'm probably sure that

00:07:41.939 --> 00:07:41.949
class of models I'm probably sure that
 

00:07:41.949 --> 00:07:43.739
class of models I'm probably sure that
you've all heard of is called generative

00:07:43.739 --> 00:07:43.749
you've all heard of is called generative
 

00:07:43.749 --> 00:07:45.929
you've all heard of is called generative
adversarial networks and that's what was

00:07:45.929 --> 00:07:45.939
adversarial networks and that's what was
 

00:07:45.939 --> 00:07:47.549
adversarial networks and that's what was
creating the faces that I showed you on

00:07:47.549 --> 00:07:47.559
creating the faces that I showed you on
 

00:07:47.559 --> 00:07:48.439
creating the faces that I showed you on
the first slide

00:07:48.439 --> 00:07:48.449
the first slide
 

00:07:48.449 --> 00:07:51.089
the first slide
both of these models are classes of

00:07:51.089 --> 00:07:51.099
both of these models are classes of
 

00:07:51.099 --> 00:07:53.730
both of these models are classes of
latent variable models so let's actually

00:07:53.730 --> 00:07:53.740
latent variable models so let's actually
 

00:07:53.740 --> 00:07:56.939
latent variable models so let's actually
start by talking about what is a latent

00:07:56.939 --> 00:07:56.949
start by talking about what is a latent
 

00:07:56.949 --> 00:07:59.909
start by talking about what is a latent
variable I saw some giggles so does

00:07:59.909 --> 00:07:59.919
variable I saw some giggles so does
 

00:07:59.919 --> 00:08:03.379
variable I saw some giggles so does
anyone know what this image is yeah okay

00:08:03.379 --> 00:08:03.389
anyone know what this image is yeah okay
 

00:08:03.389 --> 00:08:08.219
anyone know what this image is yeah okay
nice so this is an image of it's called

00:08:08.219 --> 00:08:08.229
nice so this is an image of it's called
 

00:08:08.229 --> 00:08:09.689
nice so this is an image of it's called
the myth of the cave it's from one of

00:08:09.689 --> 00:08:09.699
the myth of the cave it's from one of
 

00:08:09.699 --> 00:08:13.019
the myth of the cave it's from one of
plato's books I believe and I'll quickly

00:08:13.019 --> 00:08:13.029
plato's books I believe and I'll quickly
 

00:08:13.029 --> 00:08:14.399
plato's books I believe and I'll quickly
describe what's going on in this image

00:08:14.399 --> 00:08:14.409
describe what's going on in this image
 

00:08:14.409 --> 00:08:15.989
describe what's going on in this image
it's a great example I think of

00:08:15.989 --> 00:08:15.999
it's a great example I think of
 

00:08:15.999 --> 00:08:19.589
it's a great example I think of
describing what is a latent variable and

00:08:19.589 --> 00:08:19.599
describing what is a latent variable and
 

00:08:19.599 --> 00:08:21.149
describing what is a latent variable and
maybe this helps motivate why we care

00:08:21.149 --> 00:08:21.159
maybe this helps motivate why we care
 

00:08:21.159 --> 00:08:23.220
maybe this helps motivate why we care
about latent variables even further

00:08:23.220 --> 00:08:23.230
about latent variables even further
 

00:08:23.230 --> 00:08:25.830
about latent variables even further
so in this example a group of prisoners

00:08:25.830 --> 00:08:25.840
so in this example a group of prisoners
 

00:08:25.840 --> 00:08:29.850
so in this example a group of prisoners
is actually chained their back start to

00:08:29.850 --> 00:08:29.860
is actually chained their back start to
 

00:08:29.860 --> 00:08:31.620
is actually chained their back start to
a wall and they're forced to look only

00:08:31.620 --> 00:08:31.630
a wall and they're forced to look only
 

00:08:31.630 --> 00:08:33.860
a wall and they're forced to look only
forward and they can see these shadows

00:08:33.860 --> 00:08:33.870
forward and they can see these shadows
 

00:08:33.870 --> 00:08:36.540
forward and they can see these shadows
projected onto the wall but they can

00:08:36.540 --> 00:08:36.550
projected onto the wall but they can
 

00:08:36.550 --> 00:08:38.070
projected onto the wall but they can
only see the shadows they'd never see

00:08:38.070 --> 00:08:38.080
only see the shadows they'd never see
 

00:08:38.080 --> 00:08:40.530
only see the shadows they'd never see
the actual objects behind them that are

00:08:40.530 --> 00:08:40.540
the actual objects behind them that are
 

00:08:40.540 --> 00:08:42.360
the actual objects behind them that are
generating the shadows because of this

00:08:42.360 --> 00:08:42.370
generating the shadows because of this
 

00:08:42.370 --> 00:08:47.160
generating the shadows because of this
fire to those prisoners the shadows are

00:08:47.160 --> 00:08:47.170
fire to those prisoners the shadows are
 

00:08:47.170 --> 00:08:49.200
fire to those prisoners the shadows are
the observed variables they can measure

00:08:49.200 --> 00:08:49.210
the observed variables they can measure
 

00:08:49.210 --> 00:08:51.090
the observed variables they can measure
them they can give them names even

00:08:51.090 --> 00:08:51.100
them they can give them names even
 

00:08:51.100 --> 00:08:52.740
them they can give them names even
though they aren't real objects they

00:08:52.740 --> 00:08:52.750
though they aren't real objects they
 

00:08:52.750 --> 00:08:54.480
though they aren't real objects they
aren't the real objects that are

00:08:54.480 --> 00:08:54.490
aren't the real objects that are
 

00:08:54.490 --> 00:08:57.690
aren't the real objects that are
underlying what is actually projected

00:08:57.690 --> 00:08:57.700
underlying what is actually projected
 

00:08:57.700 --> 00:09:01.560
underlying what is actually projected
onto this wall but because that's all I

00:09:01.560 --> 00:09:01.570
onto this wall but because that's all I
 

00:09:01.570 --> 00:09:03.120
onto this wall but because that's all I
can see that is their reality that's

00:09:03.120 --> 00:09:03.130
can see that is their reality that's
 

00:09:03.130 --> 00:09:05.760
can see that is their reality that's
their observations they cannot directly

00:09:05.760 --> 00:09:05.770
their observations they cannot directly
 

00:09:05.770 --> 00:09:07.440
their observations they cannot directly
observe or measure the true objects

00:09:07.440 --> 00:09:07.450
observe or measure the true objects
 

00:09:07.450 --> 00:09:08.670
observe or measure the true objects
behind them because they can't turn

00:09:08.670 --> 00:09:08.680
behind them because they can't turn
 

00:09:08.680 --> 00:09:12.030
behind them because they can't turn
around but they and they don't actually

00:09:12.030 --> 00:09:12.040
around but they and they don't actually
 

00:09:12.040 --> 00:09:13.380
around but they and they don't actually
know that these things are shadows they

00:09:13.380 --> 00:09:13.390
know that these things are shadows they
 

00:09:13.390 --> 00:09:15.420
know that these things are shadows they
have no concept of that the latent

00:09:15.420 --> 00:09:15.430
have no concept of that the latent
 

00:09:15.430 --> 00:09:17.070
have no concept of that the latent
variables here are like the objects that

00:09:17.070 --> 00:09:17.080
variables here are like the objects that
 

00:09:17.080 --> 00:09:19.560
variables here are like the objects that
they cannot directly observe but are the

00:09:19.560 --> 00:09:19.570
they cannot directly observe but are the
 

00:09:19.570 --> 00:09:22.200
they cannot directly observe but are the
ones that are informing or generating

00:09:22.200 --> 00:09:22.210
ones that are informing or generating
 

00:09:22.210 --> 00:09:26.310
ones that are informing or generating
these shadows so in this case the

00:09:26.310 --> 00:09:26.320
these shadows so in this case the
 

00:09:26.320 --> 00:09:27.720
these shadows so in this case the
observable variables just to reiterate

00:09:27.720 --> 00:09:27.730
observable variables just to reiterate
 

00:09:27.730 --> 00:09:29.490
observable variables just to reiterate
once again are the shadows that the

00:09:29.490 --> 00:09:29.500
once again are the shadows that the
 

00:09:29.500 --> 00:09:31.530
once again are the shadows that the
prisoners see the hidden variables are

00:09:31.530 --> 00:09:31.540
prisoners see the hidden variables are
 

00:09:31.540 --> 00:09:33.720
prisoners see the hidden variables are
the latent variables are the underlying

00:09:33.720 --> 00:09:33.730
the latent variables are the underlying
 

00:09:33.730 --> 00:09:36.540
the latent variables are the underlying
variables or the underlying objects that

00:09:36.540 --> 00:09:36.550
variables or the underlying objects that
 

00:09:36.550 --> 00:09:40.890
variables or the underlying objects that
are generating these shadows now the

00:09:40.890 --> 00:09:40.900
are generating these shadows now the
 

00:09:40.900 --> 00:09:44.010
are generating these shadows now the
question here is going back to like a

00:09:44.010 --> 00:09:44.020
question here is going back to like a
 

00:09:44.020 --> 00:09:46.410
question here is going back to like a
deep learning frame of mind can we have

00:09:46.410 --> 00:09:46.420
deep learning frame of mind can we have
 

00:09:46.420 --> 00:09:49.620
deep learning frame of mind can we have
learn the two variables the true latent

00:09:49.620 --> 00:09:49.630
learn the two variables the true latent
 

00:09:49.630 --> 00:09:53.130
learn the two variables the true latent
variables from only observable data so

00:09:53.130 --> 00:09:53.140
variables from only observable data so
 

00:09:53.140 --> 00:09:54.510
variables from only observable data so
if I give you a lot of data can you

00:09:54.510 --> 00:09:54.520
if I give you a lot of data can you
 

00:09:54.520 --> 00:09:56.730
if I give you a lot of data can you
learn some underlying latent variables

00:09:56.730 --> 00:09:56.740
learn some underlying latent variables
 

00:09:56.740 --> 00:10:00.720
learn some underlying latent variables
from that structure so to start with

00:10:00.720 --> 00:10:00.730
from that structure so to start with
 

00:10:00.730 --> 00:10:02.160
from that structure so to start with
we're going to talk about the first

00:10:02.160 --> 00:10:02.170
we're going to talk about the first
 

00:10:02.170 --> 00:10:03.390
we're going to talk about the first
class of models which is called

00:10:03.390 --> 00:10:03.400
class of models which is called
 

00:10:03.400 --> 00:10:05.670
class of models which is called
auto-encoders it's a very simple

00:10:05.670 --> 00:10:05.680
auto-encoders it's a very simple
 

00:10:05.680 --> 00:10:07.230
auto-encoders it's a very simple
generative model which actually tries to

00:10:07.230 --> 00:10:07.240
generative model which actually tries to
 

00:10:07.240 --> 00:10:11.490
generative model which actually tries to
do this by encoding its own input so

00:10:11.490 --> 00:10:11.500
do this by encoding its own input so
 

00:10:11.500 --> 00:10:14.430
do this by encoding its own input so
just to reiterate let's suppose we want

00:10:14.430 --> 00:10:14.440
just to reiterate let's suppose we want
 

00:10:14.440 --> 00:10:15.810
just to reiterate let's suppose we want
to have an unsupervised approach

00:10:15.810 --> 00:10:15.820
to have an unsupervised approach
 

00:10:15.820 --> 00:10:17.580
to have an unsupervised approach
remember again this is all unsupervised

00:10:17.580 --> 00:10:17.590
remember again this is all unsupervised
 

00:10:17.590 --> 00:10:20.070
remember again this is all unsupervised
so we're not given any labels here so we

00:10:20.070 --> 00:10:20.080
so we're not given any labels here so we
 

00:10:20.080 --> 00:10:21.450
so we're not given any labels here so we
don't know that this is a - we're simply

00:10:21.450 --> 00:10:21.460
don't know that this is a - we're simply
 

00:10:21.460 --> 00:10:23.250
don't know that this is a - we're simply
given an image and we don't know what it

00:10:23.250 --> 00:10:23.260
given an image and we don't know what it
 

00:10:23.260 --> 00:10:26.430
given an image and we don't know what it
is now we want to feed these raw input

00:10:26.430 --> 00:10:26.440
is now we want to feed these raw input
 

00:10:26.440 --> 00:10:29.100
is now we want to feed these raw input
image pixels and pass it through a

00:10:29.100 --> 00:10:29.110
image pixels and pass it through a
 

00:10:29.110 --> 00:10:30.780
image pixels and pass it through a
series of convolutional layers in this

00:10:30.780 --> 00:10:30.790
series of convolutional layers in this
 

00:10:30.790 --> 00:10:32.250
series of convolutional layers in this
case let's suppose just because this is

00:10:32.250 --> 00:10:32.260
case let's suppose just because this is
 

00:10:32.260 --> 00:10:34.470
case let's suppose just because this is
an image so the green the green layers

00:10:34.470 --> 00:10:34.480
an image so the green the green layers
 

00:10:34.480 --> 00:10:36.210
an image so the green the green layers
here you can see our convolutional

00:10:36.210 --> 00:10:36.220
here you can see our convolutional
 

00:10:36.220 --> 00:10:37.080
here you can see our convolutional
layers let's suppose

00:10:37.080 --> 00:10:37.090
layers let's suppose
 

00:10:37.090 --> 00:10:39.030
layers let's suppose
and you're going from your input X

00:10:39.030 --> 00:10:39.040
and you're going from your input X
 

00:10:39.040 --> 00:10:41.520
and you're going from your input X
through convolutional layers to this

00:10:41.520 --> 00:10:41.530
through convolutional layers to this
 

00:10:41.530 --> 00:10:44.010
through convolutional layers to this
output latent variable which we'll call

00:10:44.010 --> 00:10:44.020
output latent variable which we'll call
 

00:10:44.020 --> 00:10:48.810
output latent variable which we'll call
Z you might notice here that the latent

00:10:48.810 --> 00:10:48.820
Z you might notice here that the latent
 

00:10:48.820 --> 00:10:51.540
Z you might notice here that the latent
variable is much smaller than that of

00:10:51.540 --> 00:10:51.550
variable is much smaller than that of
 

00:10:51.550 --> 00:10:53.670
variable is much smaller than that of
the observed variable and then actually

00:10:53.670 --> 00:10:53.680
the observed variable and then actually
 

00:10:53.680 --> 00:10:55.440
the observed variable and then actually
this is a very common case so we're

00:10:55.440 --> 00:10:55.450
this is a very common case so we're
 

00:10:55.450 --> 00:10:57.420
this is a very common case so we're
calling this this model that I'm showing

00:10:57.420 --> 00:10:57.430
calling this this model that I'm showing
 

00:10:57.430 --> 00:10:59.370
calling this this model that I'm showing
here an encoder so this is encoding your

00:10:59.370 --> 00:10:59.380
here an encoder so this is encoding your
 

00:10:59.380 --> 00:11:02.340
here an encoder so this is encoding your
input X into a lower dimensional latent

00:11:02.340 --> 00:11:02.350
input X into a lower dimensional latent
 

00:11:02.350 --> 00:11:02.820
input X into a lower dimensional latent
space

00:11:02.820 --> 00:11:02.830
space
 

00:11:02.830 --> 00:11:04.680
space
Z and now I actually want to raise a

00:11:04.680 --> 00:11:04.690
Z and now I actually want to raise a
 

00:11:04.690 --> 00:11:06.840
Z and now I actually want to raise a
question to see how many of you are

00:11:06.840 --> 00:11:06.850
question to see how many of you are
 

00:11:06.850 --> 00:11:10.380
question to see how many of you are
following me why do we care about having

00:11:10.380 --> 00:11:10.390
following me why do we care about having
 

00:11:10.390 --> 00:11:12.930
following me why do we care about having
low dimensional Z's in this case is it

00:11:12.930 --> 00:11:12.940
low dimensional Z's in this case is it
 

00:11:12.940 --> 00:11:14.370
low dimensional Z's in this case is it
important that we have a low dimensional

00:11:14.370 --> 00:11:14.380
important that we have a low dimensional
 

00:11:14.380 --> 00:11:17.310
important that we have a low dimensional
z does it not really matter why is it

00:11:17.310 --> 00:11:17.320
z does it not really matter why is it
 

00:11:17.320 --> 00:11:19.110
z does it not really matter why is it
the case that usually we care only about

00:11:19.110 --> 00:11:19.120
the case that usually we care only about
 

00:11:19.120 --> 00:11:26.340
the case that usually we care only about
low dimensional Z's yeah exactly

00:11:26.340 --> 00:11:26.350
low dimensional Z's yeah exactly
 

00:11:26.350 --> 00:11:28.830
low dimensional Z's yeah exactly
yeah so I think so the suggestion was

00:11:28.830 --> 00:11:28.840
yeah so I think so the suggestion was
 

00:11:28.840 --> 00:11:31.650
yeah so I think so the suggestion was
get rid of noise that's a great idea by

00:11:31.650 --> 00:11:31.660
get rid of noise that's a great idea by
 

00:11:31.660 --> 00:11:33.420
get rid of noise that's a great idea by
getting along the same lines I think

00:11:33.420 --> 00:11:33.430
getting along the same lines I think
 

00:11:33.430 --> 00:11:34.950
getting along the same lines I think
what you're getting at is that you want

00:11:34.950 --> 00:11:34.960
what you're getting at is that you want
 

00:11:34.960 --> 00:11:36.990
what you're getting at is that you want
to find the most meaningful features in

00:11:36.990 --> 00:11:37.000
to find the most meaningful features in
 

00:11:37.000 --> 00:11:39.150
to find the most meaningful features in
your input right that are really

00:11:39.150 --> 00:11:39.160
your input right that are really
 

00:11:39.160 --> 00:11:42.690
your input right that are really
contributing to what's going on what's

00:11:42.690 --> 00:11:42.700
contributing to what's going on what's
 

00:11:42.700 --> 00:11:45.210
contributing to what's going on what's
underlying this data distribution pixels

00:11:45.210 --> 00:11:45.220
underlying this data distribution pixels
 

00:11:45.220 --> 00:11:47.520
underlying this data distribution pixels
are incredibly sparse representation of

00:11:47.520 --> 00:11:47.530
are incredibly sparse representation of
 

00:11:47.530 --> 00:11:49.890
are incredibly sparse representation of
images and often we don't need all that

00:11:49.890 --> 00:11:49.900
images and often we don't need all that
 

00:11:49.900 --> 00:11:51.750
images and often we don't need all that
information to accurately describe an

00:11:51.750 --> 00:11:51.760
information to accurately describe an
 

00:11:51.760 --> 00:11:55.890
information to accurately describe an
image so in this case this - it's found

00:11:55.890 --> 00:11:55.900
image so in this case this - it's found
 

00:11:55.900 --> 00:11:58.410
image so in this case this - it's found
that you can usually model n this digit

00:11:58.410 --> 00:11:58.420
that you can usually model n this digit
 

00:11:58.420 --> 00:12:00.510
that you can usually model n this digit
so that these images are drawn from the

00:12:00.510 --> 00:12:00.520
so that these images are drawn from the
 

00:12:00.520 --> 00:12:02.070
so that these images are drawn from the
data set called amnesty which is just

00:12:02.070 --> 00:12:02.080
data set called amnesty which is just
 

00:12:02.080 --> 00:12:04.320
data set called amnesty which is just
handwritten digits you can usually model

00:12:04.320 --> 00:12:04.330
handwritten digits you can usually model
 

00:12:04.330 --> 00:12:06.510
handwritten digits you can usually model
endless digits with about five latent

00:12:06.510 --> 00:12:06.520
endless digits with about five latent
 

00:12:06.520 --> 00:12:08.460
endless digits with about five latent
variables and these are images with

00:12:08.460 --> 00:12:08.470
variables and these are images with
 

00:12:08.470 --> 00:12:10.950
variables and these are images with
hundreds of pixels in them but you can

00:12:10.950 --> 00:12:10.960
hundreds of pixels in them but you can
 

00:12:10.960 --> 00:12:12.720
hundreds of pixels in them but you can
compress them down and very accurately

00:12:12.720 --> 00:12:12.730
compress them down and very accurately
 

00:12:12.730 --> 00:12:16.230
compress them down and very accurately
describe them with just five numbers so

00:12:16.230 --> 00:12:16.240
describe them with just five numbers so
 

00:12:16.240 --> 00:12:17.970
describe them with just five numbers so
doing this actually allows us to find

00:12:17.970 --> 00:12:17.980
doing this actually allows us to find
 

00:12:17.980 --> 00:12:20.790
doing this actually allows us to find
the most accurate or sorry the most rich

00:12:20.790 --> 00:12:20.800
the most accurate or sorry the most rich
 

00:12:20.800 --> 00:12:23.550
the most accurate or sorry the most rich
features in the image that accurately

00:12:23.550 --> 00:12:23.560
features in the image that accurately
 

00:12:23.560 --> 00:12:26.010
features in the image that accurately
describe and abstract away all of the

00:12:26.010 --> 00:12:26.020
describe and abstract away all of the
 

00:12:26.020 --> 00:12:29.730
describe and abstract away all of the
details underlying that image so how can

00:12:29.730 --> 00:12:29.740
details underlying that image so how can
 

00:12:29.740 --> 00:12:31.380
details underlying that image so how can
we learn this late in space I mentioned

00:12:31.380 --> 00:12:31.390
we learn this late in space I mentioned
 

00:12:31.390 --> 00:12:33.210
we learn this late in space I mentioned
it's not a supervised problem here so we

00:12:33.210 --> 00:12:33.220
it's not a supervised problem here so we
 

00:12:33.220 --> 00:12:35.100
it's not a supervised problem here so we
don't actually know z so we can't

00:12:35.100 --> 00:12:35.110
don't actually know z so we can't
 

00:12:35.110 --> 00:12:37.830
don't actually know z so we can't
directly just apply back propagation by

00:12:37.830 --> 00:12:37.840
directly just apply back propagation by
 

00:12:37.840 --> 00:12:41.010
directly just apply back propagation by
supervising on Z so instead let's apply

00:12:41.010 --> 00:12:41.020
supervising on Z so instead let's apply
 

00:12:41.020 --> 00:12:42.330
supervising on Z so instead let's apply
a trip where we actually trying to

00:12:42.330 --> 00:12:42.340
a trip where we actually trying to
 

00:12:42.340 --> 00:12:45.600
a trip where we actually trying to
reconstruct the original data so now on

00:12:45.600 --> 00:12:45.610
reconstruct the original data so now on
 

00:12:45.610 --> 00:12:47.190
reconstruct the original data so now on
the right hand side you can see a

00:12:47.190 --> 00:12:47.200
the right hand side you can see a
 

00:12:47.200 --> 00:12:48.870
the right hand side you can see a
reconstruction by applying these up

00:12:48.870 --> 00:12:48.880
reconstruction by applying these up
 

00:12:48.880 --> 00:12:51.050
reconstruction by applying these up
sampling convolutions

00:12:51.050 --> 00:12:51.060
sampling convolutions
 

00:12:51.060 --> 00:12:54.690
sampling convolutions
into a reconstructed version of the

00:12:54.690 --> 00:12:54.700
into a reconstructed version of the
 

00:12:54.700 --> 00:12:56.070
into a reconstructed version of the
original data so you can might notice

00:12:56.070 --> 00:12:56.080
original data so you can might notice
 

00:12:56.080 --> 00:12:58.019
original data so you can might notice
here that the right hand side is a

00:12:58.019 --> 00:12:58.029
here that the right hand side is a
 

00:12:58.029 --> 00:13:00.840
here that the right hand side is a
blurry version of the left hand side and

00:13:00.840 --> 00:13:00.850
blurry version of the left hand side and
 

00:13:00.850 --> 00:13:02.490
blurry version of the left hand side and
that's because we lose some information

00:13:02.490 --> 00:13:02.500
that's because we lose some information
 

00:13:02.500 --> 00:13:05.250
that's because we lose some information
by going through such a small latent

00:13:05.250 --> 00:13:05.260
by going through such a small latent
 

00:13:05.260 --> 00:13:06.780
by going through such a small latent
space and that's to be expected

00:13:06.780 --> 00:13:06.790
space and that's to be expected
 

00:13:06.790 --> 00:13:08.280
space and that's to be expected
that's the whole point of finding what

00:13:08.280 --> 00:13:08.290
that's the whole point of finding what
 

00:13:08.290 --> 00:13:10.410
that's the whole point of finding what
are the most important features and also

00:13:10.410 --> 00:13:10.420
are the most important features and also
 

00:13:10.420 --> 00:13:12.030
are the most important features and also
to get rid of noise like was the

00:13:12.030 --> 00:13:12.040
to get rid of noise like was the
 

00:13:12.040 --> 00:13:16.860
to get rid of noise like was the
suggestion in the audience so here we

00:13:16.860 --> 00:13:16.870
suggestion in the audience so here we
 

00:13:16.870 --> 00:13:19.199
suggestion in the audience so here we
denote that reconstruction as X hat

00:13:19.199 --> 00:13:19.209
denote that reconstruction as X hat
 

00:13:19.209 --> 00:13:21.660
denote that reconstruction as X hat
it's just denoting the estimated or

00:13:21.660 --> 00:13:21.670
it's just denoting the estimated or
 

00:13:21.670 --> 00:13:24.710
it's just denoting the estimated or
reconstructed version of the input data

00:13:24.710 --> 00:13:24.720
reconstructed version of the input data
 

00:13:24.720 --> 00:13:28.579
reconstructed version of the input data
we can supervise this by basically just

00:13:28.579 --> 00:13:28.589
we can supervise this by basically just
 

00:13:28.589 --> 00:13:31.050
we can supervise this by basically just
comparing the output which is the

00:13:31.050 --> 00:13:31.060
comparing the output which is the
 

00:13:31.060 --> 00:13:33.180
comparing the output which is the
reconstructed version of the input with

00:13:33.180 --> 00:13:33.190
reconstructed version of the input with
 

00:13:33.190 --> 00:13:35.280
reconstructed version of the input with
the original input and you might think

00:13:35.280 --> 00:13:35.290
the original input and you might think
 

00:13:35.290 --> 00:13:37.259
the original input and you might think
of something just like computing the

00:13:37.259 --> 00:13:37.269
of something just like computing the
 

00:13:37.269 --> 00:13:38.490
of something just like computing the
difference of these two images

00:13:38.490 --> 00:13:38.500
difference of these two images
 

00:13:38.500 --> 00:13:41.220
difference of these two images
subtracting them and computing their

00:13:41.220 --> 00:13:41.230
subtracting them and computing their
 

00:13:41.230 --> 00:13:42.509
subtracting them and computing their
square and adding up all these

00:13:42.509 --> 00:13:42.519
square and adding up all these
 

00:13:42.519 --> 00:13:45.300
square and adding up all these
differences that will give you a loss

00:13:45.300 --> 00:13:45.310
differences that will give you a loss
 

00:13:45.310 --> 00:13:46.889
differences that will give you a loss
term that we can use in back propagation

00:13:46.889 --> 00:13:46.899
term that we can use in back propagation
 

00:13:46.899 --> 00:13:48.690
term that we can use in back propagation
just like before so now we're kind of

00:13:48.690 --> 00:13:48.700
just like before so now we're kind of
 

00:13:48.700 --> 00:13:50.130
just like before so now we're kind of
shifting the problem from a completely

00:13:50.130 --> 00:13:50.140
shifting the problem from a completely
 

00:13:50.140 --> 00:13:51.509
shifting the problem from a completely
unsupervised problem where we have no

00:13:51.509 --> 00:13:51.519
unsupervised problem where we have no
 

00:13:51.519 --> 00:13:53.610
unsupervised problem where we have no
notion of how to create these latent

00:13:53.610 --> 00:13:53.620
notion of how to create these latent
 

00:13:53.620 --> 00:13:55.889
notion of how to create these latent
variable Z now we have some notion of

00:13:55.889 --> 00:13:55.899
variable Z now we have some notion of
 

00:13:55.899 --> 00:13:58.319
variable Z now we have some notion of
supervising it end to end and by doing

00:13:58.319 --> 00:13:58.329
supervising it end to end and by doing
 

00:13:58.329 --> 00:14:00.449
supervising it end to end and by doing
it end to end this reconstruction loss

00:14:00.449 --> 00:14:00.459
it end to end this reconstruction loss
 

00:14:00.459 --> 00:14:05.040
it end to end this reconstruction loss
is forcing the decoder to learn the most

00:14:05.040 --> 00:14:05.050
is forcing the decoder to learn the most
 

00:14:05.050 --> 00:14:07.620
is forcing the decoder to learn the most
accurate version or sorry the most rich

00:14:07.620 --> 00:14:07.630
accurate version or sorry the most rich
 

00:14:07.630 --> 00:14:10.260
accurate version or sorry the most rich
latent variable possible so that it can

00:14:10.260 --> 00:14:10.270
latent variable possible so that it can
 

00:14:10.270 --> 00:14:12.960
latent variable possible so that it can
describe for so that it can reconstruct

00:14:12.960 --> 00:14:12.970
describe for so that it can reconstruct
 

00:14:12.970 --> 00:14:16.440
describe for so that it can reconstruct
the image as as much as possible

00:14:16.440 --> 00:14:16.450
the image as as much as possible
 

00:14:16.450 --> 00:14:19.470
the image as as much as possible
right so if this latent space was not

00:14:19.470 --> 00:14:19.480
right so if this latent space was not
 

00:14:19.480 --> 00:14:23.069
right so if this latent space was not
descriptive at all of the image then the

00:14:23.069 --> 00:14:23.079
descriptive at all of the image then the
 

00:14:23.079 --> 00:14:25.680
descriptive at all of the image then the
decoder could never create such a such

00:14:25.680 --> 00:14:25.690
decoder could never create such a such
 

00:14:25.690 --> 00:14:28.410
decoder could never create such a such
an accurate image depicting its input so

00:14:28.410 --> 00:14:28.420
an accurate image depicting its input so
 

00:14:28.420 --> 00:14:30.990
an accurate image depicting its input so
by forcing it to reconstruct this too

00:14:30.990 --> 00:14:31.000
by forcing it to reconstruct this too
 

00:14:31.000 --> 00:14:33.300
by forcing it to reconstruct this too
you're forcing the lane space to be very

00:14:33.300 --> 00:14:33.310
you're forcing the lane space to be very
 

00:14:33.310 --> 00:14:38.189
you're forcing the lane space to be very
rich in its latent variables right and

00:14:38.189 --> 00:14:38.199
rich in its latent variables right and
 

00:14:38.199 --> 00:14:39.660
rich in its latent variables right and
just to reiterate it again this loss

00:14:39.660 --> 00:14:39.670
just to reiterate it again this loss
 

00:14:39.670 --> 00:14:41.939
just to reiterate it again this loss
function does not require any labels

00:14:41.939 --> 00:14:41.949
function does not require any labels
 

00:14:41.949 --> 00:14:43.319
function does not require any labels
we're not telling it that it's a two

00:14:43.319 --> 00:14:43.329
we're not telling it that it's a two
 

00:14:43.329 --> 00:14:45.389
we're not telling it that it's a two
here we're simply feeding it images of

00:14:45.389 --> 00:14:45.399
here we're simply feeding it images of
 

00:14:45.399 --> 00:14:47.519
here we're simply feeding it images of
twos or or whatever numbers and we're

00:14:47.519 --> 00:14:47.529
twos or or whatever numbers and we're
 

00:14:47.529 --> 00:14:49.439
twos or or whatever numbers and we're
learning the underlying latent variables

00:14:49.439 --> 00:14:49.449
learning the underlying latent variables
 

00:14:49.449 --> 00:14:52.139
learning the underlying latent variables
associated with this data now let's just

00:14:52.139 --> 00:14:52.149
associated with this data now let's just
 

00:14:52.149 --> 00:14:53.550
associated with this data now let's just
start abstracting away this image a

00:14:53.550 --> 00:14:53.560
start abstracting away this image a
 

00:14:53.560 --> 00:14:55.170
start abstracting away this image a
little bit so we can get more complexity

00:14:55.170 --> 00:14:55.180
little bit so we can get more complexity
 

00:14:55.180 --> 00:14:57.630
little bit so we can get more complexity
here I'm removing all of the

00:14:57.630 --> 00:14:57.640
here I'm removing all of the
 

00:14:57.640 --> 00:14:59.250
here I'm removing all of the
convolutional layers and I'm just

00:14:59.250 --> 00:14:59.260
convolutional layers and I'm just
 

00:14:59.260 --> 00:15:00.660
convolutional layers and I'm just
drawing the encoder with a single

00:15:00.660 --> 00:15:00.670
drawing the encoder with a single
 

00:15:00.670 --> 00:15:03.389
drawing the encoder with a single
trapezoid so basically denoting that

00:15:03.389 --> 00:15:03.399
trapezoid so basically denoting that
 

00:15:03.399 --> 00:15:04.590
trapezoid so basically denoting that
we're going from this X

00:15:04.590 --> 00:15:04.600
we're going from this X
 

00:15:04.600 --> 00:15:06.540
we're going from this X
to a low dimensional Z and then going

00:15:06.540 --> 00:15:06.550
to a low dimensional Z and then going
 

00:15:06.550 --> 00:15:09.800
to a low dimensional Z and then going
from Z to a higher dimensional X hat and

00:15:09.800 --> 00:15:09.810
from Z to a higher dimensional X hat and
 

00:15:09.810 --> 00:15:14.700
from Z to a higher dimensional X hat and
reconstructing it and this idea of

00:15:14.700 --> 00:15:14.710
reconstructing it and this idea of
 

00:15:14.710 --> 00:15:16.950
reconstructing it and this idea of
bottlenecking the network forcing all of

00:15:16.950 --> 00:15:16.960
bottlenecking the network forcing all of
 

00:15:16.960 --> 00:15:19.020
bottlenecking the network forcing all of
the activations and all of the

00:15:19.020 --> 00:15:19.030
the activations and all of the
 

00:15:19.030 --> 00:15:20.640
the activations and all of the
information to be bottle necked into

00:15:20.640 --> 00:15:20.650
information to be bottle necked into
 

00:15:20.650 --> 00:15:23.460
information to be bottle necked into
this Lane space is essentially this idea

00:15:23.460 --> 00:15:23.470
this Lane space is essentially this idea
 

00:15:23.470 --> 00:15:25.860
this Lane space is essentially this idea
of compression Auto encoding is a form

00:15:25.860 --> 00:15:25.870
of compression Auto encoding is a form
 

00:15:25.870 --> 00:15:28.740
of compression Auto encoding is a form
of compression and basically smaller

00:15:28.740 --> 00:15:28.750
of compression and basically smaller
 

00:15:28.750 --> 00:15:31.980
of compression and basically smaller
latent spaces will result in noisier and

00:15:31.980 --> 00:15:31.990
latent spaces will result in noisier and
 

00:15:31.990 --> 00:15:35.310
latent spaces will result in noisier and
noisier or should I say blurrier and

00:15:35.310 --> 00:15:35.320
noisier or should I say blurrier and
 

00:15:35.320 --> 00:15:38.190
noisier or should I say blurrier and
blurrier outputs the more bottleneck

00:15:38.190 --> 00:15:38.200
blurrier outputs the more bottleneck
 

00:15:38.200 --> 00:15:39.990
blurrier outputs the more bottleneck
that you apply on the network means the

00:15:39.990 --> 00:15:40.000
that you apply on the network means the
 

00:15:40.000 --> 00:15:41.520
that you apply on the network means the
smaller that your latent space will

00:15:41.520 --> 00:15:41.530
smaller that your latent space will
 

00:15:41.530 --> 00:15:44.490
smaller that your latent space will
become and the result applied on n this

00:15:44.490 --> 00:15:44.500
become and the result applied on n this
 

00:15:44.500 --> 00:15:45.960
become and the result applied on n this
you can see something like this so if

00:15:45.960 --> 00:15:45.970
you can see something like this so if
 

00:15:45.970 --> 00:15:48.090
you can see something like this so if
you only have two two variables in your

00:15:48.090 --> 00:15:48.100
you only have two two variables in your
 

00:15:48.100 --> 00:15:51.120
you only have two two variables in your
latent space you can abstract away a

00:15:51.120 --> 00:15:51.130
latent space you can abstract away a
 

00:15:51.130 --> 00:15:53.190
latent space you can abstract away a
little bit of the problem you can still

00:15:53.190 --> 00:15:53.200
little bit of the problem you can still
 

00:15:53.200 --> 00:15:55.350
little bit of the problem you can still
generate the images but you're losing a

00:15:55.350 --> 00:15:55.360
generate the images but you're losing a
 

00:15:55.360 --> 00:15:57.480
generate the images but you're losing a
lot of the crisp detail in the images so

00:15:57.480 --> 00:15:57.490
lot of the crisp detail in the images so
 

00:15:57.490 --> 00:15:58.770
lot of the crisp detail in the images so
you still get the high level structure

00:15:58.770 --> 00:15:58.780
you still get the high level structure
 

00:15:58.780 --> 00:16:00.510
you still get the high level structure
of what's going on here but you're

00:16:00.510 --> 00:16:00.520
of what's going on here but you're
 

00:16:00.520 --> 00:16:03.060
of what's going on here but you're
missing a lot of the details increasing

00:16:03.060 --> 00:16:03.070
missing a lot of the details increasing
 

00:16:03.070 --> 00:16:04.770
missing a lot of the details increasing
that up to five dimensional now you can

00:16:04.770 --> 00:16:04.780
that up to five dimensional now you can
 

00:16:04.780 --> 00:16:07.080
that up to five dimensional now you can
alway already see a lot of the details

00:16:07.080 --> 00:16:07.090
alway already see a lot of the details
 

00:16:07.090 --> 00:16:08.370
alway already see a lot of the details
that you had previously lost and

00:16:08.370 --> 00:16:08.380
that you had previously lost and
 

00:16:08.380 --> 00:16:09.720
that you had previously lost and
comparing that to ground truth you can

00:16:09.720 --> 00:16:09.730
comparing that to ground truth you can
 

00:16:09.730 --> 00:16:11.820
comparing that to ground truth you can
at least for these low dimensional

00:16:11.820 --> 00:16:11.830
at least for these low dimensional
 

00:16:11.830 --> 00:16:14.280
at least for these low dimensional
inputs it's very close at human high

00:16:14.280 --> 00:16:14.290
inputs it's very close at human high
 

00:16:14.290 --> 00:16:18.600
inputs it's very close at human high
level so in summary auto-encoders

00:16:18.600 --> 00:16:18.610
level so in summary auto-encoders
 

00:16:18.610 --> 00:16:20.490
level so in summary auto-encoders
utilize this bottleneck layer to

00:16:20.490 --> 00:16:20.500
utilize this bottleneck layer to
 

00:16:20.500 --> 00:16:22.980
utilize this bottleneck layer to
actually learn the rich representation

00:16:22.980 --> 00:16:22.990
actually learn the rich representation
 

00:16:22.990 --> 00:16:25.680
actually learn the rich representation
of the latent space and that forces the

00:16:25.680 --> 00:16:25.690
of the latent space and that forces the
 

00:16:25.690 --> 00:16:27.540
of the latent space and that forces the
network to learn a hidden representation

00:16:27.540 --> 00:16:27.550
network to learn a hidden representation
 

00:16:27.550 --> 00:16:30.540
network to learn a hidden representation
of the data it's a self supervising

00:16:30.540 --> 00:16:30.550
of the data it's a self supervising
 

00:16:30.550 --> 00:16:32.790
of the data it's a self supervising
approach because the latent loss that

00:16:32.790 --> 00:16:32.800
approach because the latent loss that
 

00:16:32.800 --> 00:16:35.340
approach because the latent loss that
actually forces the network to encode as

00:16:35.340 --> 00:16:35.350
actually forces the network to encode as
 

00:16:35.350 --> 00:16:37.710
actually forces the network to encode as
much information as possible in the data

00:16:37.710 --> 00:16:37.720
much information as possible in the data
 

00:16:37.720 --> 00:16:39.930
much information as possible in the data
in order to reconstruct it later on in

00:16:39.930 --> 00:16:39.940
in order to reconstruct it later on in
 

00:16:39.940 --> 00:16:41.850
in order to reconstruct it later on in
the network and that's where the name

00:16:41.850 --> 00:16:41.860
the network and that's where the name
 

00:16:41.860 --> 00:16:43.590
the network and that's where the name
come from so it's called Auto encoding

00:16:43.590 --> 00:16:43.600
come from so it's called Auto encoding
 

00:16:43.600 --> 00:16:45.090
come from so it's called Auto encoding
because it's trying to automatically

00:16:45.090 --> 00:16:45.100
because it's trying to automatically
 

00:16:45.100 --> 00:16:47.670
because it's trying to automatically
encode that data so that it can learn to

00:16:47.670 --> 00:16:47.680
encode that data so that it can learn to
 

00:16:47.680 --> 00:16:51.330
encode that data so that it can learn to
reconstruct it as well so now let's

00:16:51.330 --> 00:16:51.340
reconstruct it as well so now let's
 

00:16:51.340 --> 00:16:54.530
reconstruct it as well so now let's
build on this idea in a bit more

00:16:54.530 --> 00:16:54.540
build on this idea in a bit more
 

00:16:54.540 --> 00:16:57.810
build on this idea in a bit more
advanced a concept called variational

00:16:57.810 --> 00:16:57.820
advanced a concept called variational
 

00:16:57.820 --> 00:16:59.340
advanced a concept called variational
autoencoders which are simply just an

00:16:59.340 --> 00:16:59.350
autoencoders which are simply just an
 

00:16:59.350 --> 00:17:01.170
autoencoders which are simply just an
extension on auto-encoders with the

00:17:01.170 --> 00:17:01.180
extension on auto-encoders with the
 

00:17:01.180 --> 00:17:04.650
extension on auto-encoders with the
probabilistic spin so let's revisit the

00:17:04.650 --> 00:17:04.660
probabilistic spin so let's revisit the
 

00:17:04.660 --> 00:17:06.540
probabilistic spin so let's revisit the
standard auto encoder picture that I

00:17:06.540 --> 00:17:06.550
standard auto encoder picture that I
 

00:17:06.550 --> 00:17:09.210
standard auto encoder picture that I
just showed you here at the Leighton

00:17:09.210 --> 00:17:09.220
just showed you here at the Leighton
 

00:17:09.220 --> 00:17:13.260
just showed you here at the Leighton
space is a deterministic function right

00:17:13.260 --> 00:17:13.270
space is a deterministic function right
 

00:17:13.270 --> 00:17:15.600
space is a deterministic function right
so all of these layers are completely

00:17:15.600 --> 00:17:15.610
so all of these layers are completely
 

00:17:15.610 --> 00:17:17.910
so all of these layers are completely
deterministic if I feed in the same

00:17:17.910 --> 00:17:17.920
deterministic if I feed in the same
 

00:17:17.920 --> 00:17:20.430
deterministic if I feed in the same
input multiple times I will always get

00:17:20.430 --> 00:17:20.440
input multiple times I will always get
 

00:17:20.440 --> 00:17:22.590
input multiple times I will always get
the same output on the other side that's

00:17:22.590 --> 00:17:22.600
the same output on the other side that's
 

00:17:22.600 --> 00:17:26.190
the same output on the other side that's
the definition of determinism what's the

00:17:26.190 --> 00:17:26.200
the definition of determinism what's the
 

00:17:26.200 --> 00:17:27.600
the definition of determinism what's the
difference in a variational auto encoder

00:17:27.600 --> 00:17:27.610
difference in a variational auto encoder
 

00:17:27.610 --> 00:17:30.150
difference in a variational auto encoder
so now in a variational autoencoder

00:17:30.150 --> 00:17:30.160
so now in a variational autoencoder
 

00:17:30.160 --> 00:17:32.700
so now in a variational autoencoder
we're replacing that intermediate latent

00:17:32.700 --> 00:17:32.710
we're replacing that intermediate latent
 

00:17:32.710 --> 00:17:35.520
we're replacing that intermediate latent
space which was deterministic now with a

00:17:35.520 --> 00:17:35.530
space which was deterministic now with a
 

00:17:35.530 --> 00:17:38.910
space which was deterministic now with a
probabilistic or stochastic operation so

00:17:38.910 --> 00:17:38.920
probabilistic or stochastic operation so
 

00:17:38.920 --> 00:17:40.440
probabilistic or stochastic operation so
instead of predicting a single

00:17:40.440 --> 00:17:40.450
instead of predicting a single
 

00:17:40.450 --> 00:17:43.730
instead of predicting a single
deterministic Zee we're now predicting a

00:17:43.730 --> 00:17:43.740
deterministic Zee we're now predicting a
 

00:17:43.740 --> 00:17:47.040
deterministic Zee we're now predicting a
single mu and a single Sigma which will

00:17:47.040 --> 00:17:47.050
single mu and a single Sigma which will
 

00:17:47.050 --> 00:17:49.380
single mu and a single Sigma which will
then sample from in order to create a

00:17:49.380 --> 00:17:49.390
then sample from in order to create a
 

00:17:49.390 --> 00:17:51.290
then sample from in order to create a
stochastic Zee

00:17:51.290 --> 00:17:51.300
stochastic Zee
 

00:17:51.300 --> 00:17:55.050
stochastic Zee
so now if we feed in the same image this

00:17:55.050 --> 00:17:55.060
so now if we feed in the same image this
 

00:17:55.060 --> 00:17:57.150
so now if we feed in the same image this
two on the left-hand side multiple times

00:17:57.150 --> 00:17:57.160
two on the left-hand side multiple times
 

00:17:57.160 --> 00:17:59.550
two on the left-hand side multiple times
through the network we'll actually get

00:17:59.550 --> 00:17:59.560
through the network we'll actually get
 

00:17:59.560 --> 00:18:02.070
through the network we'll actually get
different twos appearing on the other

00:18:02.070 --> 00:18:02.080
different twos appearing on the other
 

00:18:02.080 --> 00:18:03.960
different twos appearing on the other
side and that's because of the sampling

00:18:03.960 --> 00:18:03.970
side and that's because of the sampling
 

00:18:03.970 --> 00:18:05.760
side and that's because of the sampling
operation from the muse and Sigma's that

00:18:05.760 --> 00:18:05.770
operation from the muse and Sigma's that
 

00:18:05.770 --> 00:18:07.980
operation from the muse and Sigma's that
we predict so again just to reiterate

00:18:07.980 --> 00:18:07.990
we predict so again just to reiterate
 

00:18:07.990 --> 00:18:10.410
we predict so again just to reiterate
the muse and Sigma is up until that

00:18:10.410 --> 00:18:10.420
the muse and Sigma is up until that
 

00:18:10.420 --> 00:18:12.150
the muse and Sigma is up until that
point these are computed

00:18:12.150 --> 00:18:12.160
point these are computed
 

00:18:12.160 --> 00:18:14.790
point these are computed
deterministically you take this as input

00:18:14.790 --> 00:18:14.800
deterministically you take this as input
 

00:18:14.800 --> 00:18:17.610
deterministically you take this as input
to then compute the stochastic sample of

00:18:17.610 --> 00:18:17.620
to then compute the stochastic sample of
 

00:18:17.620 --> 00:18:20.660
to then compute the stochastic sample of
Z which you then use for the decoding

00:18:20.660 --> 00:18:20.670
Z which you then use for the decoding
 

00:18:20.670 --> 00:18:24.780
Z which you then use for the decoding
and like I said before variational

00:18:24.780 --> 00:18:24.790
and like I said before variational
 

00:18:24.790 --> 00:18:26.640
and like I said before variational
auto-encoders essentially represent a

00:18:26.640 --> 00:18:26.650
auto-encoders essentially represent a
 

00:18:26.650 --> 00:18:29.610
auto-encoders essentially represent a
probabilistic spin on normal or vanilla

00:18:29.610 --> 00:18:29.620
probabilistic spin on normal or vanilla
 

00:18:29.620 --> 00:18:32.130
probabilistic spin on normal or vanilla
autoencoders where we can sample from

00:18:32.130 --> 00:18:32.140
autoencoders where we can sample from
 

00:18:32.140 --> 00:18:34.080
autoencoders where we can sample from
the mean and standard deviation of the

00:18:34.080 --> 00:18:34.090
the mean and standard deviation of the
 

00:18:34.090 --> 00:18:36.660
the mean and standard deviation of the
distributions that they predict in order

00:18:36.660 --> 00:18:36.670
distributions that they predict in order
 

00:18:36.670 --> 00:18:42.450
distributions that they predict in order
to compute their latent sample so let's

00:18:42.450 --> 00:18:42.460
to compute their latent sample so let's
 

00:18:42.460 --> 00:18:44.310
to compute their latent sample so let's
break this up a little bit more so it's

00:18:44.310 --> 00:18:44.320
break this up a little bit more so it's
 

00:18:44.320 --> 00:18:46.020
break this up a little bit more so it's
broken up just like a normal autoencoder

00:18:46.020 --> 00:18:46.030
broken up just like a normal autoencoder
 

00:18:46.030 --> 00:18:48.660
broken up just like a normal autoencoder
into two parts an encoder which takes

00:18:48.660 --> 00:18:48.670
into two parts an encoder which takes
 

00:18:48.670 --> 00:18:51.300
into two parts an encoder which takes
the image and produces now the

00:18:51.300 --> 00:18:51.310
the image and produces now the
 

00:18:51.310 --> 00:18:53.910
the image and produces now the
probabilistic a probabilistic a

00:18:53.910 --> 00:18:53.920
probabilistic a probabilistic a
 

00:18:53.920 --> 00:18:56.430
probabilistic a probabilistic a
probability distribution over the latent

00:18:56.430 --> 00:18:56.440
probability distribution over the latent
 

00:18:56.440 --> 00:18:58.740
probability distribution over the latent
space and as as opposed to before which

00:18:58.740 --> 00:18:58.750
space and as as opposed to before which
 

00:18:58.750 --> 00:19:00.480
space and as as opposed to before which
was deterministic now we have an actual

00:19:00.480 --> 00:19:00.490
was deterministic now we have an actual
 

00:19:00.490 --> 00:19:01.920
was deterministic now we have an actual
probability distribution here

00:19:01.920 --> 00:19:01.930
probability distribution here
 

00:19:01.930 --> 00:19:04.830
probability distribution here
parameterize by these weights phi which

00:19:04.830 --> 00:19:04.840
parameterize by these weights phi which
 

00:19:04.840 --> 00:19:07.830
parameterize by these weights phi which
are the parameters of the encoder and we

00:19:07.830 --> 00:19:07.840
are the parameters of the encoder and we
 

00:19:07.840 --> 00:19:09.600
are the parameters of the encoder and we
have a decoder which does the opposite

00:19:09.600 --> 00:19:09.610
have a decoder which does the opposite
 

00:19:09.610 --> 00:19:12.330
have a decoder which does the opposite
it takes as it finds a probability

00:19:12.330 --> 00:19:12.340
it takes as it finds a probability
 

00:19:12.340 --> 00:19:14.910
it takes as it finds a probability
distribution of the data given your

00:19:14.910 --> 00:19:14.920
distribution of the data given your
 

00:19:14.920 --> 00:19:17.700
distribution of the data given your
latent distribution and it has

00:19:17.700 --> 00:19:17.710
latent distribution and it has
 

00:19:17.710 --> 00:19:20.130
latent distribution and it has
parameters theta and these are just the

00:19:20.130 --> 00:19:20.140
parameters theta and these are just the
 

00:19:20.140 --> 00:19:22.530
parameters theta and these are just the
parameters of your decoder so you can

00:19:22.530 --> 00:19:22.540
parameters of your decoder so you can
 

00:19:22.540 --> 00:19:23.880
parameters of your decoder so you can
think of these parameters as the weights

00:19:23.880 --> 00:19:23.890
think of these parameters as the weights
 

00:19:23.890 --> 00:19:26.340
think of these parameters as the weights
of the convolutional neural network that

00:19:26.340 --> 00:19:26.350
of the convolutional neural network that
 

00:19:26.350 --> 00:19:28.440
of the convolutional neural network that
define this decoding operation and this

00:19:28.440 --> 00:19:28.450
define this decoding operation and this
 

00:19:28.450 --> 00:19:31.200
define this decoding operation and this
is V is the weights of the encoder the

00:19:31.200 --> 00:19:31.210
is V is the weights of the encoder the
 

00:19:31.210 --> 00:19:31.710
is V is the weights of the encoder the
weights of

00:19:31.710 --> 00:19:31.720
weights of
 

00:19:31.720 --> 00:19:33.510
weights of
convolutional layers that define the

00:19:33.510 --> 00:19:33.520
convolutional layers that define the
 

00:19:33.520 --> 00:19:39.140
convolutional layers that define the
encoding operation our loss function is

00:19:39.140 --> 00:19:39.150
encoding operation our loss function is
 

00:19:39.150 --> 00:19:41.610
encoding operation our loss function is
very similar to the loss function in

00:19:41.610 --> 00:19:41.620
very similar to the loss function in
 

00:19:41.620 --> 00:19:42.110
very similar to the loss function in
vanilla

00:19:42.110 --> 00:19:42.120
vanilla
 

00:19:42.120 --> 00:19:45.539
vanilla
auto-encoders we simply want to compute

00:19:45.539 --> 00:19:45.549
auto-encoders we simply want to compute
 

00:19:45.549 --> 00:19:47.640
auto-encoders we simply want to compute
the reconstruction loss like before that

00:19:47.640 --> 00:19:47.650
the reconstruction loss like before that
 

00:19:47.650 --> 00:19:49.980
the reconstruction loss like before that
forces this bottleneck layer to learn a

00:19:49.980 --> 00:19:49.990
forces this bottleneck layer to learn a
 

00:19:49.990 --> 00:19:51.779
forces this bottleneck layer to learn a
rich representation of the latent space

00:19:51.779 --> 00:19:51.789
rich representation of the latent space
 

00:19:51.789 --> 00:19:53.880
rich representation of the latent space
but now we also have this other term

00:19:53.880 --> 00:19:53.890
but now we also have this other term
 

00:19:53.890 --> 00:19:55.740
but now we also have this other term
which is called a regularization term or

00:19:55.740 --> 00:19:55.750
which is called a regularization term or
 

00:19:55.750 --> 00:19:59.430
which is called a regularization term or
a or sometimes it's called like the VA

00:19:59.430 --> 00:19:59.440
a or sometimes it's called like the VA
 

00:19:59.440 --> 00:20:04.100
a or sometimes it's called like the VA
II lost because it's specific to VA es

00:20:04.100 --> 00:20:04.110
 
 

00:20:04.110 --> 00:20:07.680
 
so what is this let's take a look at

00:20:07.680 --> 00:20:07.690
so what is this let's take a look at
 

00:20:07.690 --> 00:20:10.010
so what is this let's take a look at
this in a bit more detail so like I said

00:20:10.010 --> 00:20:10.020
this in a bit more detail so like I said
 

00:20:10.020 --> 00:20:12.779
this in a bit more detail so like I said
the loss function here is comprised of

00:20:12.779 --> 00:20:12.789
the loss function here is comprised of
 

00:20:12.789 --> 00:20:14.760
the loss function here is comprised of
inputs from both the encoder and the

00:20:14.760 --> 00:20:14.770
inputs from both the encoder and the
 

00:20:14.770 --> 00:20:15.299
inputs from both the encoder and the
decoder

00:20:15.299 --> 00:20:15.309
decoder
 

00:20:15.309 --> 00:20:17.970
decoder
it also takes us input the input data X

00:20:17.970 --> 00:20:17.980
it also takes us input the input data X
 

00:20:17.980 --> 00:20:20.730
it also takes us input the input data X
and it wants to actually by training

00:20:20.730 --> 00:20:20.740
and it wants to actually by training
 

00:20:20.740 --> 00:20:22.620
and it wants to actually by training
this you want to learn some loss

00:20:22.620 --> 00:20:22.630
this you want to learn some loss
 

00:20:22.630 --> 00:20:25.350
this you want to learn some loss
function that's a weighted sum between

00:20:25.350 --> 00:20:25.360
function that's a weighted sum between
 

00:20:25.360 --> 00:20:28.700
function that's a weighted sum between
the reconstructions and your latent loss

00:20:28.700 --> 00:20:28.710
the reconstructions and your latent loss
 

00:20:28.710 --> 00:20:30.899
the reconstructions and your latent loss
reconstruction is the same as before you

00:20:30.899 --> 00:20:30.909
reconstruction is the same as before you
 

00:20:30.909 --> 00:20:32.250
reconstruction is the same as before you
can think of it as just a pixel wise

00:20:32.250 --> 00:20:32.260
can think of it as just a pixel wise
 

00:20:32.260 --> 00:20:33.659
can think of it as just a pixel wise
difference between your inputs and your

00:20:33.659 --> 00:20:33.669
difference between your inputs and your
 

00:20:33.669 --> 00:20:35.730
difference between your inputs and your
outputs and you self supervise them to

00:20:35.730 --> 00:20:35.740
outputs and you self supervise them to
 

00:20:35.740 --> 00:20:39.270
outputs and you self supervise them to
force the lane spaces to learn the

00:20:39.270 --> 00:20:39.280
force the lane spaces to learn the
 

00:20:39.280 --> 00:20:40.890
force the lane spaces to learn the
regularization term here is a bit more

00:20:40.890 --> 00:20:40.900
regularization term here is a bit more
 

00:20:40.900 --> 00:20:42.299
regularization term here is a bit more
interesting so let's touch on it in a

00:20:42.299 --> 00:20:42.309
interesting so let's touch on it in a
 

00:20:42.309 --> 00:20:44.539
interesting so let's touch on it in a
little more detail before moving forward

00:20:44.539 --> 00:20:44.549
little more detail before moving forward
 

00:20:44.549 --> 00:20:47.640
little more detail before moving forward
so Phi or the probability distribution

00:20:47.640 --> 00:20:47.650
so Phi or the probability distribution
 

00:20:47.650 --> 00:20:50.880
so Phi or the probability distribution
Phi of Z given X is the probability

00:20:50.880 --> 00:20:50.890
Phi of Z given X is the probability
 

00:20:50.890 --> 00:20:54.060
Phi of Z given X is the probability
distribution it's the distribution of

00:20:54.060 --> 00:20:54.070
distribution it's the distribution of
 

00:20:54.070 --> 00:20:55.620
distribution it's the distribution of
the encoder it's the distribution of

00:20:55.620 --> 00:20:55.630
the encoder it's the distribution of
 

00:20:55.630 --> 00:20:58.289
the encoder it's the distribution of
your latent space given your data and

00:20:58.289 --> 00:20:58.299
your latent space given your data and
 

00:20:58.299 --> 00:21:00.950
your latent space given your data and
it's computed like I said by the encoder

00:21:00.950 --> 00:21:00.960
it's computed like I said by the encoder
 

00:21:00.960 --> 00:21:03.750
it's computed like I said by the encoder
as part of regularizing we want to place

00:21:03.750 --> 00:21:03.760
as part of regularizing we want to place
 

00:21:03.760 --> 00:21:05.970
as part of regularizing we want to place
a fixed prior on this distribution to

00:21:05.970 --> 00:21:05.980
a fixed prior on this distribution to
 

00:21:05.980 --> 00:21:08.610
a fixed prior on this distribution to
make sure that the Z's that we compute

00:21:08.610 --> 00:21:08.620
make sure that the Z's that we compute
 

00:21:08.620 --> 00:21:13.700
make sure that the Z's that we compute
follows some prior that we define so

00:21:13.700 --> 00:21:13.710
follows some prior that we define so
 

00:21:13.710 --> 00:21:15.990
follows some prior that we define so
essentially what this term here is

00:21:15.990 --> 00:21:16.000
essentially what this term here is
 

00:21:16.000 --> 00:21:18.930
essentially what this term here is
describing is d stands for distance so

00:21:18.930 --> 00:21:18.940
describing is d stands for distance so
 

00:21:18.940 --> 00:21:21.090
describing is d stands for distance so
the regularization term is minimizing

00:21:21.090 --> 00:21:21.100
the regularization term is minimizing
 

00:21:21.100 --> 00:21:23.279
the regularization term is minimizing
the distance between our inferred

00:21:23.279 --> 00:21:23.289
the distance between our inferred
 

00:21:23.289 --> 00:21:25.500
the distance between our inferred
distribution which is the Meuse and

00:21:25.500 --> 00:21:25.510
distribution which is the Meuse and
 

00:21:25.510 --> 00:21:28.649
distribution which is the Meuse and
sigmaz that we learn and we want to

00:21:28.649 --> 00:21:28.659
sigmaz that we learn and we want to
 

00:21:28.659 --> 00:21:30.029
sigmaz that we learn and we want to
minimize the distance between that

00:21:30.029 --> 00:21:30.039
minimize the distance between that
 

00:21:30.039 --> 00:21:32.370
minimize the distance between that
distribution and a distribution prior

00:21:32.370 --> 00:21:32.380
distribution and a distribution prior
 

00:21:32.380 --> 00:21:34.710
distribution and a distribution prior
distribution that we define I haven't

00:21:34.710 --> 00:21:34.720
distribution that we define I haven't
 

00:21:34.720 --> 00:21:36.060
distribution that we define I haven't
told you what that prior is yet and

00:21:36.060 --> 00:21:36.070
told you what that prior is yet and
 

00:21:36.070 --> 00:21:41.340
told you what that prior is yet and
we'll get to that soon so a common

00:21:41.340 --> 00:21:41.350
we'll get to that soon so a common
 

00:21:41.350 --> 00:21:43.049
we'll get to that soon so a common
choice of a prior here

00:21:43.049 --> 00:21:43.059
choice of a prior here
 

00:21:43.059 --> 00:21:44.760
choice of a prior here
a very common choice for VA E's and

00:21:44.760 --> 00:21:44.770
a very common choice for VA E's and
 

00:21:44.770 --> 00:21:45.510
a very common choice for VA E's and
specific

00:21:45.510 --> 00:21:45.520
specific
 

00:21:45.520 --> 00:21:47.670
specific
is to make this prior a normal Gaussian

00:21:47.670 --> 00:21:47.680
is to make this prior a normal Gaussian
 

00:21:47.680 --> 00:21:50.130
is to make this prior a normal Gaussian
prior so we want it to be centered at

00:21:50.130 --> 00:21:50.140
prior so we want it to be centered at
 

00:21:50.140 --> 00:21:53.390
prior so we want it to be centered at
meeting zero and standard deviation 1

00:21:53.390 --> 00:21:53.400
meeting zero and standard deviation 1
 

00:21:53.400 --> 00:21:55.950
meeting zero and standard deviation 1
what this does it basically encourages

00:21:55.950 --> 00:21:55.960
what this does it basically encourages
 

00:21:55.960 --> 00:21:58.440
what this does it basically encourages
our latent space or it encourages our

00:21:58.440 --> 00:21:58.450
our latent space or it encourages our
 

00:21:58.450 --> 00:22:01.260
our latent space or it encourages our
encoder to project all of the input

00:22:01.260 --> 00:22:01.270
encoder to project all of the input
 

00:22:01.270 --> 00:22:03.360
encoder to project all of the input
images or all of our input data in

00:22:03.360 --> 00:22:03.370
images or all of our input data in
 

00:22:03.370 --> 00:22:06.780
images or all of our input data in
general into a latent space which is

00:22:06.780 --> 00:22:06.790
general into a latent space which is
 

00:22:06.790 --> 00:22:08.580
general into a latent space which is
centered at zero and has roughly a

00:22:08.580 --> 00:22:08.590
centered at zero and has roughly a
 

00:22:08.590 --> 00:22:10.920
centered at zero and has roughly a
variance of 1 if the network tries to

00:22:10.920 --> 00:22:10.930
variance of 1 if the network tries to
 

00:22:10.930 --> 00:22:14.460
variance of 1 if the network tries to
deviate from that and place images on

00:22:14.460 --> 00:22:14.470
deviate from that and place images on
 

00:22:14.470 --> 00:22:15.720
deviate from that and place images on
different parts of latent space

00:22:15.720 --> 00:22:15.730
different parts of latent space
 

00:22:15.730 --> 00:22:17.760
different parts of latent space
potentially by trying to memorize or

00:22:17.760 --> 00:22:17.770
potentially by trying to memorize or
 

00:22:17.770 --> 00:22:19.800
potentially by trying to memorize or
cheat and put special inputs in

00:22:19.800 --> 00:22:19.810
cheat and put special inputs in
 

00:22:19.810 --> 00:22:21.240
cheat and put special inputs in
different parts of the latent space it's

00:22:21.240 --> 00:22:21.250
different parts of the latent space it's
 

00:22:21.250 --> 00:22:22.560
different parts of the latent space it's
going to be penalized for doing that

00:22:22.560 --> 00:22:22.570
going to be penalized for doing that
 

00:22:22.570 --> 00:22:24.330
going to be penalized for doing that
it's going to be kind of constrained to

00:22:24.330 --> 00:22:24.340
it's going to be kind of constrained to
 

00:22:24.340 --> 00:22:26.460
it's going to be kind of constrained to
work only in this probabilistic setting

00:22:26.460 --> 00:22:26.470
work only in this probabilistic setting
 

00:22:26.470 --> 00:22:28.470
work only in this probabilistic setting
by following this Gaussian normal

00:22:28.470 --> 00:22:28.480
by following this Gaussian normal
 

00:22:28.480 --> 00:22:33.660
by following this Gaussian normal
Gaussian prior and like we saw it's very

00:22:33.660 --> 00:22:33.670
Gaussian prior and like we saw it's very
 

00:22:33.670 --> 00:22:36.060
Gaussian prior and like we saw it's very
similar in the cross entropy loss we can

00:22:36.060 --> 00:22:36.070
similar in the cross entropy loss we can
 

00:22:36.070 --> 00:22:38.670
similar in the cross entropy loss we can
actually define this distance function

00:22:38.670 --> 00:22:38.680
actually define this distance function
 

00:22:38.680 --> 00:22:41.040
actually define this distance function
that we can use to regularize our neural

00:22:41.040 --> 00:22:41.050
that we can use to regularize our neural
 

00:22:41.050 --> 00:22:43.680
that we can use to regularize our neural
network or VI e and if we're using this

00:22:43.680 --> 00:22:43.690
network or VI e and if we're using this
 

00:22:43.690 --> 00:22:45.540
network or VI e and if we're using this
case like I said before where we want to

00:22:45.540 --> 00:22:45.550
case like I said before where we want to
 

00:22:45.550 --> 00:22:47.880
case like I said before where we want to
place a prior on a zero one normal

00:22:47.880 --> 00:22:47.890
place a prior on a zero one normal
 

00:22:47.890 --> 00:22:49.740
place a prior on a zero one normal
Gaussian it's just going to take this

00:22:49.740 --> 00:22:49.750
Gaussian it's just going to take this
 

00:22:49.750 --> 00:22:52.140
Gaussian it's just going to take this
form between the predicted muse and

00:22:52.140 --> 00:22:52.150
form between the predicted muse and
 

00:22:52.150 --> 00:22:54.450
form between the predicted muse and
sigmaz and we want to constrain those

00:22:54.450 --> 00:22:54.460
sigmaz and we want to constrain those
 

00:22:54.460 --> 00:22:55.710
sigmaz and we want to constrain those
Meuse and sigmaz to be as close as

00:22:55.710 --> 00:22:55.720
Meuse and sigmaz to be as close as
 

00:22:55.720 --> 00:22:57.870
Meuse and sigmaz to be as close as
possible to a normal Gaussian of zero

00:22:57.870 --> 00:22:57.880
possible to a normal Gaussian of zero
 

00:22:57.880 --> 00:23:01.070
possible to a normal Gaussian of zero
and one sometimes you'll see this term

00:23:01.070 --> 00:23:01.080
and one sometimes you'll see this term
 

00:23:01.080 --> 00:23:04.500
and one sometimes you'll see this term
denoted the KL divergence of the data

00:23:04.500 --> 00:23:04.510
denoted the KL divergence of the data
 

00:23:04.510 --> 00:23:08.690
denoted the KL divergence of the data
and and that's where this comes from

00:23:08.690 --> 00:23:08.700
and and that's where this comes from
 

00:23:08.700 --> 00:23:11.730
and and that's where this comes from
great so now we've defined our loss

00:23:11.730 --> 00:23:11.740
great so now we've defined our loss
 

00:23:11.740 --> 00:23:13.800
great so now we've defined our loss
function that lets us know how we can

00:23:13.800 --> 00:23:13.810
function that lets us know how we can
 

00:23:13.810 --> 00:23:15.570
function that lets us know how we can
actually train this network using back

00:23:15.570 --> 00:23:15.580
actually train this network using back
 

00:23:15.580 --> 00:23:17.790
actually train this network using back
propagation and reconstruct the inputs

00:23:17.790 --> 00:23:17.800
propagation and reconstruct the inputs
 

00:23:17.800 --> 00:23:20.700
propagation and reconstruct the inputs
and how that actually different differs

00:23:20.700 --> 00:23:20.710
and how that actually different differs
 

00:23:20.710 --> 00:23:23.520
and how that actually different differs
from a vanilla auto encoder the

00:23:23.520 --> 00:23:23.530
from a vanilla auto encoder the
 

00:23:23.530 --> 00:23:25.290
from a vanilla auto encoder the
regularization term here is taken care

00:23:25.290 --> 00:23:25.300
regularization term here is taken care
 

00:23:25.300 --> 00:23:28.350
regularization term here is taken care
of and we can use it with a normal

00:23:28.350 --> 00:23:28.360
of and we can use it with a normal
 

00:23:28.360 --> 00:23:29.940
of and we can use it with a normal
Gaussian prior let's assume for the rest

00:23:29.940 --> 00:23:29.950
Gaussian prior let's assume for the rest
 

00:23:29.950 --> 00:23:32.700
Gaussian prior let's assume for the rest
of this lecture but we actually still

00:23:32.700 --> 00:23:32.710
of this lecture but we actually still
 

00:23:32.710 --> 00:23:34.290
of this lecture but we actually still
have a big problem here so I've defined

00:23:34.290 --> 00:23:34.300
have a big problem here so I've defined
 

00:23:34.300 --> 00:23:36.270
have a big problem here so I've defined
the forward pass for you pretty clearly

00:23:36.270 --> 00:23:36.280
the forward pass for you pretty clearly
 

00:23:36.280 --> 00:23:38.280
the forward pass for you pretty clearly
I have to find the sampling operation

00:23:38.280 --> 00:23:38.290
I have to find the sampling operation
 

00:23:38.290 --> 00:23:40.080
I have to find the sampling operation
between the muse and the Sigma's to

00:23:40.080 --> 00:23:40.090
between the muse and the Sigma's to
 

00:23:40.090 --> 00:23:43.440
between the muse and the Sigma's to
create these Z's but I think I forgot a

00:23:43.440 --> 00:23:43.450
create these Z's but I think I forgot a
 

00:23:43.450 --> 00:23:45.660
create these Z's but I think I forgot a
very important step which is one of the

00:23:45.660 --> 00:23:45.670
very important step which is one of the
 

00:23:45.670 --> 00:23:48.330
very important step which is one of the
reasons why VA E's were never being used

00:23:48.330 --> 00:23:48.340
reasons why VA E's were never being used
 

00:23:48.340 --> 00:23:50.400
reasons why VA E's were never being used
until a few years ago because you could

00:23:50.400 --> 00:23:50.410
until a few years ago because you could
 

00:23:50.410 --> 00:23:52.290
until a few years ago because you could
never actually back propagate gradients

00:23:52.290 --> 00:23:52.300
never actually back propagate gradients
 

00:23:52.300 --> 00:23:55.650
never actually back propagate gradients
through a sampling layer so these

00:23:55.650 --> 00:23:55.660
through a sampling layer so these
 

00:23:55.660 --> 00:23:58.310
through a sampling layer so these
sampling layers that take as input stuff

00:23:58.310 --> 00:23:58.320
sampling layers that take as input stuff
 

00:23:58.320 --> 00:24:00.880
sampling layers that take as input stuff
deterministic and using Sigma's an

00:24:00.880 --> 00:24:00.890
deterministic and using Sigma's an
 

00:24:00.890 --> 00:24:03.620
deterministic and using Sigma's an
output a stochastic sample at the output

00:24:03.620 --> 00:24:03.630
output a stochastic sample at the output
 

00:24:03.630 --> 00:24:05.720
output a stochastic sample at the output
you can compute their forward pass it's

00:24:05.720 --> 00:24:05.730
you can compute their forward pass it's
 

00:24:05.730 --> 00:24:07.460
you can compute their forward pass it's
simple but then if you want a back

00:24:07.460 --> 00:24:07.470
simple but then if you want a back
 

00:24:07.470 --> 00:24:08.870
simple but then if you want a back
propagate through a sampling node

00:24:08.870 --> 00:24:08.880
propagate through a sampling node
 

00:24:08.880 --> 00:24:10.700
propagate through a sampling node
there's no notion of back propagation

00:24:10.700 --> 00:24:10.710
there's no notion of back propagation
 

00:24:10.710 --> 00:24:14.860
there's no notion of back propagation
through stochastic nodes or layers and

00:24:14.860 --> 00:24:14.870
through stochastic nodes or layers and
 

00:24:14.870 --> 00:24:17.990
through stochastic nodes or layers and
one of the key ideas that occurred about

00:24:17.990 --> 00:24:18.000
one of the key ideas that occurred about
 

00:24:18.000 --> 00:24:19.850
one of the key ideas that occurred about
a few years ago

00:24:19.850 --> 00:24:19.860
a few years ago
 

00:24:19.860 --> 00:24:23.150
a few years ago
was this idea of reprioritizing reaper a

00:24:23.150 --> 00:24:23.160
was this idea of reprioritizing reaper a
 

00:24:23.160 --> 00:24:25.700
was this idea of reprioritizing reaper a
motorizing the sampling layer so that

00:24:25.700 --> 00:24:25.710
motorizing the sampling layer so that
 

00:24:25.710 --> 00:24:27.200
motorizing the sampling layer so that
you could perform back propagation

00:24:27.200 --> 00:24:27.210
you could perform back propagation
 

00:24:27.210 --> 00:24:29.060
you could perform back propagation
through it and train this network and to

00:24:29.060 --> 00:24:29.070
through it and train this network and to
 

00:24:29.070 --> 00:24:31.760
through it and train this network and to
end I'll quickly give you a brief intro

00:24:31.760 --> 00:24:31.770
end I'll quickly give you a brief intro
 

00:24:31.770 --> 00:24:34.730
end I'll quickly give you a brief intro
or introduction to how the repairment

00:24:34.730 --> 00:24:34.740
or introduction to how the repairment
 

00:24:34.740 --> 00:24:38.510
or introduction to how the repairment
translation trick occurs so we already

00:24:38.510 --> 00:24:38.520
translation trick occurs so we already
 

00:24:38.520 --> 00:24:40.940
translation trick occurs so we already
learned that we can't just simply sample

00:24:40.940 --> 00:24:40.950
learned that we can't just simply sample
 

00:24:40.950 --> 00:24:42.920
learned that we can't just simply sample
from this distribution in a backward

00:24:42.920 --> 00:24:42.930
from this distribution in a backward
 

00:24:42.930 --> 00:24:44.780
from this distribution in a backward
pass because we can't compute that

00:24:44.780 --> 00:24:44.790
pass because we can't compute that
 

00:24:44.790 --> 00:24:46.280
pass because we can't compute that
backwards pass and we can't compute a

00:24:46.280 --> 00:24:46.290
backwards pass and we can't compute a
 

00:24:46.290 --> 00:24:48.740
backwards pass and we can't compute a
channel through it so let's instead

00:24:48.740 --> 00:24:48.750
channel through it so let's instead
 

00:24:48.750 --> 00:24:50.690
channel through it so let's instead
consider a different alternative which

00:24:50.690 --> 00:24:50.700
consider a different alternative which
 

00:24:50.700 --> 00:24:53.270
consider a different alternative which
is let's sample the latent vector as a

00:24:53.270 --> 00:24:53.280
is let's sample the latent vector as a
 

00:24:53.280 --> 00:24:56.960
is let's sample the latent vector as a
sum between the Meuse and the Sigma's so

00:24:56.960 --> 00:24:56.970
sum between the Meuse and the Sigma's so
 

00:24:56.970 --> 00:24:59.240
sum between the Meuse and the Sigma's so
we'll start with the Mews these are the

00:24:59.240 --> 00:24:59.250
we'll start with the Mews these are the
 

00:24:59.250 --> 00:25:01.070
we'll start with the Mews these are the
means of the distribution we'll add

00:25:01.070 --> 00:25:01.080
means of the distribution we'll add
 

00:25:01.080 --> 00:25:04.460
means of the distribution we'll add
those to a weighted version of the Sigma

00:25:04.460 --> 00:25:04.470
those to a weighted version of the Sigma
 

00:25:04.470 --> 00:25:06.170
those to a weighted version of the Sigma
vectors which is the standard deviation

00:25:06.170 --> 00:25:06.180
vectors which is the standard deviation
 

00:25:06.180 --> 00:25:08.660
vectors which is the standard deviation
and we'll just scale those standard

00:25:08.660 --> 00:25:08.670
and we'll just scale those standard
 

00:25:08.670 --> 00:25:10.910
and we'll just scale those standard
deviations by a random number which we

00:25:10.910 --> 00:25:10.920
deviations by a random number which we
 

00:25:10.920 --> 00:25:13.880
deviations by a random number which we
can normally which we can draw from a

00:25:13.880 --> 00:25:13.890
can normally which we can draw from a
 

00:25:13.890 --> 00:25:18.950
can normally which we can draw from a
normal distribution 0 1 so we still have

00:25:18.950 --> 00:25:18.960
normal distribution 0 1 so we still have
 

00:25:18.960 --> 00:25:20.450
normal distribution 0 1 so we still have
a stochastic node here because we're

00:25:20.450 --> 00:25:20.460
a stochastic node here because we're
 

00:25:20.460 --> 00:25:24.920
a stochastic node here because we're
doing this this sampling of epsilon but

00:25:24.920 --> 00:25:24.930
doing this this sampling of epsilon but
 

00:25:24.930 --> 00:25:26.540
doing this this sampling of epsilon but
the difference here is the sampling is

00:25:26.540 --> 00:25:26.550
the difference here is the sampling is
 

00:25:26.550 --> 00:25:28.160
the difference here is the sampling is
actually not occurring within the layer

00:25:28.160 --> 00:25:28.170
actually not occurring within the layer
 

00:25:28.170 --> 00:25:30.290
actually not occurring within the layer
itself it's occurring as a by-product

00:25:30.290 --> 00:25:30.300
itself it's occurring as a by-product
 

00:25:30.300 --> 00:25:32.420
itself it's occurring as a by-product
we're reaper amat rising where the

00:25:32.420 --> 00:25:32.430
we're reaper amat rising where the
 

00:25:32.430 --> 00:25:33.800
we're reaper amat rising where the
sampling is occurring so instead of a

00:25:33.800 --> 00:25:33.810
sampling is occurring so instead of a
 

00:25:33.810 --> 00:25:36.230
sampling is occurring so instead of a
cry instead of sampling directly within

00:25:36.230 --> 00:25:36.240
cry instead of sampling directly within
 

00:25:36.240 --> 00:25:38.330
cry instead of sampling directly within
this see you can imagine the sampling

00:25:38.330 --> 00:25:38.340
this see you can imagine the sampling
 

00:25:38.340 --> 00:25:40.250
this see you can imagine the sampling
occurring off to the side and being fed

00:25:40.250 --> 00:25:40.260
occurring off to the side and being fed
 

00:25:40.260 --> 00:25:42.800
occurring off to the side and being fed
into the sampling layer so let's take a

00:25:42.800 --> 00:25:42.810
into the sampling layer so let's take a
 

00:25:42.810 --> 00:25:44.690
into the sampling layer so let's take a
look at what that might look like so in

00:25:44.690 --> 00:25:44.700
look at what that might look like so in
 

00:25:44.700 --> 00:25:46.720
look at what that might look like so in
the original example we have

00:25:46.720 --> 00:25:46.730
the original example we have
 

00:25:46.730 --> 00:25:49.070
the original example we have
deterministic nodes which are just the

00:25:49.070 --> 00:25:49.080
deterministic nodes which are just the
 

00:25:49.080 --> 00:25:51.170
deterministic nodes which are just the
weights of the network and the input

00:25:51.170 --> 00:25:51.180
weights of the network and the input
 

00:25:51.180 --> 00:25:53.540
weights of the network and the input
data that we have and we have a sampling

00:25:53.540 --> 00:25:53.550
data that we have and we have a sampling
 

00:25:53.550 --> 00:25:55.910
data that we have and we have a sampling
layer which takes those two to compute a

00:25:55.910 --> 00:25:55.920
layer which takes those two to compute a
 

00:25:55.920 --> 00:25:57.470
layer which takes those two to compute a
sample according to the distribution

00:25:57.470 --> 00:25:57.480
sample according to the distribution
 

00:25:57.480 --> 00:26:00.470
sample according to the distribution
defined by the encoder we already

00:26:00.470 --> 00:26:00.480
defined by the encoder we already
 

00:26:00.480 --> 00:26:02.120
defined by the encoder we already
learned that we can't do back propagate

00:26:02.120 --> 00:26:02.130
learned that we can't do back propagate
 

00:26:02.130 --> 00:26:04.340
learned that we can't do back propagate
we can't do propagation through this

00:26:04.340 --> 00:26:04.350
we can't do propagation through this
 

00:26:04.350 --> 00:26:07.940
we can't do propagation through this
layer because of because of the nature

00:26:07.940 --> 00:26:07.950
layer because of because of the nature
 

00:26:07.950 --> 00:26:09.350
layer because of because of the nature
of the sampling node and it's not

00:26:09.350 --> 00:26:09.360
of the sampling node and it's not
 

00:26:09.360 --> 00:26:12.200
of the sampling node and it's not
deterministic so when we Reaper it

00:26:12.200 --> 00:26:12.210
deterministic so when we Reaper it
 

00:26:12.210 --> 00:26:14.030
deterministic so when we Reaper it
we get a chart that looks like this

00:26:14.030 --> 00:26:14.040
we get a chart that looks like this
 

00:26:14.040 --> 00:26:16.850
we get a chart that looks like this
which is a lot nicer and now if you look

00:26:16.850 --> 00:26:16.860
which is a lot nicer and now if you look
 

00:26:16.860 --> 00:26:19.400
which is a lot nicer and now if you look
at it we have the same encoder weights

00:26:19.400 --> 00:26:19.410
at it we have the same encoder weights
 

00:26:19.410 --> 00:26:21.770
at it we have the same encoder weights
we have the same data but now our

00:26:21.770 --> 00:26:21.780
we have the same data but now our
 

00:26:21.780 --> 00:26:24.190
we have the same data but now our
sampling node has moved off to the side

00:26:24.190 --> 00:26:24.200
sampling node has moved off to the side
 

00:26:24.200 --> 00:26:26.960
sampling node has moved off to the side
okay so the sampling node is being drawn

00:26:26.960 --> 00:26:26.970
okay so the sampling node is being drawn
 

00:26:26.970 --> 00:26:32.450
okay so the sampling node is being drawn
from normal distribution 0 1 and Z is

00:26:32.450 --> 00:26:32.460
from normal distribution 0 1 and Z is
 

00:26:32.460 --> 00:26:34.730
from normal distribution 0 1 and Z is
now deterministic with respect to that

00:26:34.730 --> 00:26:34.740
now deterministic with respect to that
 

00:26:34.740 --> 00:26:37.130
now deterministic with respect to that
sampling node okay so now when we want

00:26:37.130 --> 00:26:37.140
sampling node okay so now when we want
 

00:26:37.140 --> 00:26:38.870
sampling node okay so now when we want
to do back propagation and actually back

00:26:38.870 --> 00:26:38.880
to do back propagation and actually back
 

00:26:38.880 --> 00:26:40.910
to do back propagation and actually back
propagate gradients to update our

00:26:40.910 --> 00:26:40.920
propagate gradients to update our
 

00:26:40.920 --> 00:26:42.800
propagate gradients to update our
encoder because this is ultimately what

00:26:42.800 --> 00:26:42.810
encoder because this is ultimately what
 

00:26:42.810 --> 00:26:44.480
encoder because this is ultimately what
we want to back propagate gradients to

00:26:44.480 --> 00:26:44.490
we want to back propagate gradients to
 

00:26:44.490 --> 00:26:45.920
we want to back propagate gradients to
these are the weights of the neural

00:26:45.920 --> 00:26:45.930
these are the weights of the neural
 

00:26:45.930 --> 00:26:48.380
these are the weights of the neural
network that we want to update when we

00:26:48.380 --> 00:26:48.390
network that we want to update when we
 

00:26:48.390 --> 00:26:50.120
network that we want to update when we
back propagate we can back propagate

00:26:50.120 --> 00:26:50.130
back propagate we can back propagate
 

00:26:50.130 --> 00:26:52.880
back propagate we can back propagate
through this z now because these epsilon

00:26:52.880 --> 00:26:52.890
through this z now because these epsilon
 

00:26:52.890 --> 00:26:55.100
through this z now because these epsilon
czar just taken as constants so we don't

00:26:55.100 --> 00:26:55.110
czar just taken as constants so we don't
 

00:26:55.110 --> 00:26:56.480
czar just taken as constants so we don't
need to back propagate in this direction

00:26:56.480 --> 00:26:56.490
need to back propagate in this direction
 

00:26:56.490 --> 00:26:58.940
need to back propagate in this direction
if we did then that would be a problem

00:26:58.940 --> 00:26:58.950
if we did then that would be a problem
 

00:26:58.950 --> 00:27:00.470
if we did then that would be a problem
because this is a sampling node again

00:27:00.470 --> 00:27:00.480
because this is a sampling node again
 

00:27:00.480 --> 00:27:02.480
because this is a sampling node again
but we don't care about that we want to

00:27:02.480 --> 00:27:02.490
but we don't care about that we want to
 

00:27:02.490 --> 00:27:06.740
but we don't care about that we want to
back propagate this way now this is a

00:27:06.740 --> 00:27:06.750
back propagate this way now this is a
 

00:27:06.750 --> 00:27:08.240
back propagate this way now this is a
really powerful idea it lets us now

00:27:08.240 --> 00:27:08.250
really powerful idea it lets us now
 

00:27:08.250 --> 00:27:10.190
really powerful idea it lets us now
train these variational auto-encoders

00:27:10.190 --> 00:27:10.200
train these variational auto-encoders
 

00:27:10.200 --> 00:27:12.350
train these variational auto-encoders
end to end and since we impose these

00:27:12.350 --> 00:27:12.360
end to end and since we impose these
 

00:27:12.360 --> 00:27:14.780
end to end and since we impose these
distributional priors on them we can

00:27:14.780 --> 00:27:14.790
distributional priors on them we can
 

00:27:14.790 --> 00:27:17.090
distributional priors on them we can
actually slowly increase or decrease

00:27:17.090 --> 00:27:17.100
actually slowly increase or decrease
 

00:27:17.100 --> 00:27:19.010
actually slowly increase or decrease
latent variables that we learn to get

00:27:19.010 --> 00:27:19.020
latent variables that we learn to get
 

00:27:19.020 --> 00:27:20.930
latent variables that we learn to get
some really cool features of the output

00:27:20.930 --> 00:27:20.940
some really cool features of the output
 

00:27:20.940 --> 00:27:24.500
some really cool features of the output
and basically by taking a latent vector

00:27:24.500 --> 00:27:24.510
and basically by taking a latent vector
 

00:27:24.510 --> 00:27:26.510
and basically by taking a latent vector
you fix all of the data except for one

00:27:26.510 --> 00:27:26.520
you fix all of the data except for one
 

00:27:26.520 --> 00:27:28.190
you fix all of the data except for one
variable in that vector and you

00:27:28.190 --> 00:27:28.200
variable in that vector and you
 

00:27:28.200 --> 00:27:29.930
variable in that vector and you
basically increase or decrease that

00:27:29.930 --> 00:27:29.940
basically increase or decrease that
 

00:27:29.940 --> 00:27:32.180
basically increase or decrease that
variable and then you run the decoder

00:27:32.180 --> 00:27:32.190
variable and then you run the decoder
 

00:27:32.190 --> 00:27:33.890
variable and then you run the decoder
each time you increase or decrease it

00:27:33.890 --> 00:27:33.900
each time you increase or decrease it
 

00:27:33.900 --> 00:27:36.740
each time you increase or decrease it
and you can see that the output actually

00:27:36.740 --> 00:27:36.750
and you can see that the output actually
 

00:27:36.750 --> 00:27:39.560
and you can see that the output actually
has some semantic meaning so what does

00:27:39.560 --> 00:27:39.570
has some semantic meaning so what does
 

00:27:39.570 --> 00:27:40.940
has some semantic meaning so what does
it look like this variable is actually

00:27:40.940 --> 00:27:40.950
it look like this variable is actually
 

00:27:40.950 --> 00:27:44.570
it look like this variable is actually
capturing does anyone see it yeah

00:27:44.570 --> 00:27:44.580
capturing does anyone see it yeah
 

00:27:44.580 --> 00:27:46.490
capturing does anyone see it yeah
exactly it's the tilt of the face or the

00:27:46.490 --> 00:27:46.500
exactly it's the tilt of the face or the
 

00:27:46.500 --> 00:27:48.620
exactly it's the tilt of the face or the
pose of the face on the left hand side

00:27:48.620 --> 00:27:48.630
pose of the face on the left hand side
 

00:27:48.630 --> 00:27:51.200
pose of the face on the left hand side
the face is pointing to the right and on

00:27:51.200 --> 00:27:51.210
the face is pointing to the right and on
 

00:27:51.210 --> 00:27:52.760
the face is pointing to the right and on
the right hand side the face is pointing

00:27:52.760 --> 00:27:52.770
the right hand side the face is pointing
 

00:27:52.770 --> 00:27:54.740
the right hand side the face is pointing
on the left and as you move from left to

00:27:54.740 --> 00:27:54.750
on the left and as you move from left to
 

00:27:54.750 --> 00:27:56.090
on the left and as you move from left to
right you can see the face kind of

00:27:56.090 --> 00:27:56.100
right you can see the face kind of
 

00:27:56.100 --> 00:27:58.550
right you can see the face kind of
turning and it's a smooth transition and

00:27:58.550 --> 00:27:58.560
turning and it's a smooth transition and
 

00:27:58.560 --> 00:28:00.860
turning and it's a smooth transition and
you can actually do this because the

00:28:00.860 --> 00:28:00.870
you can actually do this because the
 

00:28:00.870 --> 00:28:02.450
you can actually do this because the
latent variables are continuous in

00:28:02.450 --> 00:28:02.460
latent variables are continuous in
 

00:28:02.460 --> 00:28:04.160
latent variables are continuous in
nature they're following that normal

00:28:04.160 --> 00:28:04.170
nature they're following that normal
 

00:28:04.170 --> 00:28:05.630
nature they're following that normal
distribution and you can just walk along

00:28:05.630 --> 00:28:05.640
distribution and you can just walk along
 

00:28:05.640 --> 00:28:10.180
distribution and you can just walk along
them and create the output at each step

00:28:10.180 --> 00:28:10.190
them and create the output at each step
 

00:28:10.190 --> 00:28:12.620
them and create the output at each step
this is just one example of a latent

00:28:12.620 --> 00:28:12.630
this is just one example of a latent
 

00:28:12.630 --> 00:28:14.300
this is just one example of a latent
variable but the network is learning

00:28:14.300 --> 00:28:14.310
variable but the network is learning
 

00:28:14.310 --> 00:28:15.800
variable but the network is learning
many different latent variables it's

00:28:15.800 --> 00:28:15.810
many different latent variables it's
 

00:28:15.810 --> 00:28:17.600
many different latent variables it's
that whole vector Z that we showed

00:28:17.600 --> 00:28:17.610
that whole vector Z that we showed
 

00:28:17.610 --> 00:28:20.210
that whole vector Z that we showed
before and each of these vector each of

00:28:20.210 --> 00:28:20.220
before and each of these vector each of
 

00:28:20.220 --> 00:28:22.400
before and each of these vector each of
these elements in Z is encoding a

00:28:22.400 --> 00:28:22.410
these elements in Z is encoding a
 

00:28:22.410 --> 00:28:25.970
these elements in Z is encoding a
different interpreted latent feature

00:28:25.970 --> 00:28:25.980
different interpreted latent feature
 

00:28:25.980 --> 00:28:27.680
different interpreted latent feature
ideally we want these features to be

00:28:27.680 --> 00:28:27.690
ideally we want these features to be
 

00:28:27.690 --> 00:28:30.259
ideally we want these features to be
independent and uncorrelated with each

00:28:30.259 --> 00:28:30.269
independent and uncorrelated with each
 

00:28:30.269 --> 00:28:33.799
independent and uncorrelated with each
other so that we can actually have we

00:28:33.799 --> 00:28:33.809
other so that we can actually have we
 

00:28:33.809 --> 00:28:35.389
other so that we can actually have we
can walk along each dimension and

00:28:35.389 --> 00:28:35.399
can walk along each dimension and
 

00:28:35.399 --> 00:28:37.100
can walk along each dimension and
observe different semantic meanings so

00:28:37.100 --> 00:28:37.110
observe different semantic meanings so
 

00:28:37.110 --> 00:28:38.750
observe different semantic meanings so
here's the same example as before where

00:28:38.750 --> 00:28:38.760
here's the same example as before where
 

00:28:38.760 --> 00:28:39.889
here's the same example as before where
we're walking left and right an

00:28:39.889 --> 00:28:39.899
we're walking left and right an
 

00:28:39.899 --> 00:28:42.049
we're walking left and right an
observing head pose we can also walk up

00:28:42.049 --> 00:28:42.059
observing head pose we can also walk up
 

00:28:42.059 --> 00:28:43.399
observing head pose we can also walk up
and down and observe something like

00:28:43.399 --> 00:28:43.409
and down and observe something like
 

00:28:43.409 --> 00:28:46.070
and down and observe something like
smile so these are all again fake images

00:28:46.070 --> 00:28:46.080
smile so these are all again fake images
 

00:28:46.080 --> 00:28:48.590
smile so these are all again fake images
this person doesn't exist but you can

00:28:48.590 --> 00:28:48.600
this person doesn't exist but you can
 

00:28:48.600 --> 00:28:51.409
this person doesn't exist but you can
actually create and remove smiles and

00:28:51.409 --> 00:28:51.419
actually create and remove smiles and
 

00:28:51.419 --> 00:28:53.570
actually create and remove smiles and
change the position of their head just

00:28:53.570 --> 00:28:53.580
change the position of their head just
 

00:28:53.580 --> 00:28:57.529
change the position of their head just
by perturbing these two numbers this is

00:28:57.529 --> 00:28:57.539
by perturbing these two numbers this is
 

00:28:57.539 --> 00:28:59.240
by perturbing these two numbers this is
the idea of disentanglement so when you

00:28:59.240 --> 00:28:59.250
the idea of disentanglement so when you
 

00:28:59.250 --> 00:29:01.399
the idea of disentanglement so when you
have two variables that are uncorrelated

00:29:01.399 --> 00:29:01.409
have two variables that are uncorrelated
 

00:29:01.409 --> 00:29:03.889
have two variables that are uncorrelated
with each other and when affecting one

00:29:03.889 --> 00:29:03.899
with each other and when affecting one
 

00:29:03.899 --> 00:29:06.379
with each other and when affecting one
of them affects some semantic meaning or

00:29:06.379 --> 00:29:06.389
of them affects some semantic meaning or
 

00:29:06.389 --> 00:29:07.639
of them affects some semantic meaning or
changing one of them affects some

00:29:07.639 --> 00:29:07.649
changing one of them affects some
 

00:29:07.649 --> 00:29:08.990
changing one of them affects some
semantic meaning and changing the other

00:29:08.990 --> 00:29:09.000
semantic meaning and changing the other
 

00:29:09.000 --> 00:29:10.159
semantic meaning and changing the other
one it affects a different semantic

00:29:10.159 --> 00:29:10.169
one it affects a different semantic
 

00:29:10.169 --> 00:29:12.200
one it affects a different semantic
meaning when those two semantic meanings

00:29:12.200 --> 00:29:12.210
meaning when those two semantic meanings
 

00:29:12.210 --> 00:29:14.389
meaning when those two semantic meanings
are disentangled that's when these

00:29:14.389 --> 00:29:14.399
are disentangled that's when these
 

00:29:14.399 --> 00:29:16.399
are disentangled that's when these
variables are uncorrelated with each

00:29:16.399 --> 00:29:16.409
variables are uncorrelated with each
 

00:29:16.409 --> 00:29:21.740
variables are uncorrelated with each
other right so those were in exam that

00:29:21.740 --> 00:29:21.750
other right so those were in exam that
 

00:29:21.750 --> 00:29:23.450
other right so those were in exam that
was an example for images

00:29:23.450 --> 00:29:23.460
was an example for images
 

00:29:23.460 --> 00:29:25.129
was an example for images
here's another pretty cool example for

00:29:25.129 --> 00:29:25.139
here's another pretty cool example for
 

00:29:25.139 --> 00:29:30.440
here's another pretty cool example for
music so let's see if this plays so here

00:29:30.440 --> 00:29:30.450
music so let's see if this plays so here
 

00:29:30.450 --> 00:29:32.779
music so let's see if this plays so here
the four quadrants are representing

00:29:32.779 --> 00:29:32.789
the four quadrants are representing
 

00:29:32.789 --> 00:29:34.970
the four quadrants are representing
different positions in the latent space

00:29:34.970 --> 00:29:34.980
different positions in the latent space
 

00:29:34.980 --> 00:29:38.320
different positions in the latent space
so this quadrant is one particular song

00:29:38.320 --> 00:29:38.330
so this quadrant is one particular song
 

00:29:38.330 --> 00:29:41.060
so this quadrant is one particular song
this quadrant is another particular song

00:29:41.060 --> 00:29:41.070
this quadrant is another particular song
 

00:29:41.070 --> 00:29:42.680
this quadrant is another particular song
and you're using now your auto encoder

00:29:42.680 --> 00:29:42.690
and you're using now your auto encoder
 

00:29:42.690 --> 00:29:45.409
and you're using now your auto encoder
to actually interpolate and walk

00:29:45.409 --> 00:29:45.419
to actually interpolate and walk
 

00:29:45.419 --> 00:29:47.389
to actually interpolate and walk
anywhere in this space so currently

00:29:47.389 --> 00:29:47.399
anywhere in this space so currently
 

00:29:47.399 --> 00:29:49.159
anywhere in this space so currently
we're at this location and we can just

00:29:49.159 --> 00:29:49.169
we're at this location and we can just
 

00:29:49.169 --> 00:29:50.930
we're at this location and we can just
move where we're sampling from and

00:29:50.930 --> 00:29:50.940
move where we're sampling from and
 

00:29:50.940 --> 00:29:52.639
move where we're sampling from and
generate brand new music that's

00:29:52.639 --> 00:29:52.649
generate brand new music that's
 

00:29:52.649 --> 00:29:54.620
generate brand new music that's
essentially an interpolation between any

00:29:54.620 --> 00:29:54.630
essentially an interpolation between any
 

00:29:54.630 --> 00:30:03.609
essentially an interpolation between any
of these songs so here's an example

00:30:03.609 --> 00:30:03.619
 
 

00:30:03.619 --> 00:30:07.279
 
so now you're moving down and the song

00:30:07.279 --> 00:30:07.289
so now you're moving down and the song
 

00:30:07.289 --> 00:30:25.039
so now you're moving down and the song
changes to the other style right so

00:30:25.039 --> 00:30:25.049
changes to the other style right so
 

00:30:25.049 --> 00:30:27.229
changes to the other style right so
going back to images we can get the same

00:30:27.229 --> 00:30:27.239
going back to images we can get the same
 

00:30:27.239 --> 00:30:30.049
going back to images we can get the same
idea again going back to endless we can

00:30:30.049 --> 00:30:30.059
idea again going back to endless we can
 

00:30:30.059 --> 00:30:31.639
idea again going back to endless we can
walk along different dimensions of a

00:30:31.639 --> 00:30:31.649
walk along different dimensions of a
 

00:30:31.649 --> 00:30:33.320
walk along different dimensions of a
two-dimensional amnesty problem and

00:30:33.320 --> 00:30:33.330
two-dimensional amnesty problem and
 

00:30:33.330 --> 00:30:35.409
two-dimensional amnesty problem and
observe that we can sample an

00:30:35.409 --> 00:30:35.419
observe that we can sample an
 

00:30:35.419 --> 00:30:38.029
observe that we can sample an
interpolation between the different

00:30:38.029 --> 00:30:38.039
interpolation between the different
 

00:30:38.039 --> 00:30:40.039
interpolation between the different
endless figures and actually see these

00:30:40.039 --> 00:30:40.049
endless figures and actually see these
 

00:30:40.049 --> 00:30:42.049
endless figures and actually see these
really cool visualizations where we can

00:30:42.049 --> 00:30:42.059
really cool visualizations where we can
 

00:30:42.059 --> 00:30:43.909
really cool visualizations where we can
generate all the different figures from

00:30:43.909 --> 00:30:43.919
generate all the different figures from
 

00:30:43.919 --> 00:30:48.979
generate all the different figures from
just two latent variables right ok so

00:30:48.979 --> 00:30:48.989
just two latent variables right ok so
 

00:30:48.989 --> 00:30:52.729
just two latent variables right ok so
just to summarize now in VI use we learn

00:30:52.729 --> 00:30:52.739
just to summarize now in VI use we learn
 

00:30:52.739 --> 00:30:55.759
just to summarize now in VI use we learn
to compress the world down into some low

00:30:55.759 --> 00:30:55.769
to compress the world down into some low
 

00:30:55.769 --> 00:30:57.259
to compress the world down into some low
dimensional latent space that we can use

00:30:57.259 --> 00:30:57.269
dimensional latent space that we can use
 

00:30:57.269 --> 00:31:01.219
dimensional latent space that we can use
to learn we learned that reconstruction

00:31:01.219 --> 00:31:01.229
to learn we learned that reconstruction
 

00:31:01.229 --> 00:31:03.589
to learn we learned that reconstruction
allows for unsupervised learning without

00:31:03.589 --> 00:31:03.599
allows for unsupervised learning without
 

00:31:03.599 --> 00:31:06.440
allows for unsupervised learning without
labels read parameterization trick to

00:31:06.440 --> 00:31:06.450
labels read parameterization trick to
 

00:31:06.450 --> 00:31:08.299
labels read parameterization trick to
actually train these networks end to end

00:31:08.299 --> 00:31:08.309
actually train these networks end to end
 

00:31:08.309 --> 00:31:11.119
actually train these networks end to end
and interpret the hidden layer hidden

00:31:11.119 --> 00:31:11.129
and interpret the hidden layer hidden
 

00:31:11.129 --> 00:31:14.149
and interpret the hidden layer hidden
latent variables using perturbations and

00:31:14.149 --> 00:31:14.159
latent variables using perturbations and
 

00:31:14.159 --> 00:31:15.409
latent variables using perturbations and
increasing and decreasing them

00:31:15.409 --> 00:31:15.419
increasing and decreasing them
 

00:31:15.419 --> 00:31:18.409
increasing and decreasing them
iteratively to actually generate brand

00:31:18.409 --> 00:31:18.419
iteratively to actually generate brand
 

00:31:18.419 --> 00:31:21.200
iteratively to actually generate brand
new examples so now the question I'd

00:31:21.200 --> 00:31:21.210
new examples so now the question I'd
 

00:31:21.210 --> 00:31:23.899
new examples so now the question I'd
like to bring up is in VA ease we

00:31:23.899 --> 00:31:23.909
like to bring up is in VA ease we
 

00:31:23.909 --> 00:31:26.749
like to bring up is in VA ease we
brought up density estimation as core

00:31:26.749 --> 00:31:26.759
brought up density estimation as core
 

00:31:26.759 --> 00:31:28.580
brought up density estimation as core
now we'll transition to a new type of

00:31:28.580 --> 00:31:28.590
now we'll transition to a new type of
 

00:31:28.590 --> 00:31:30.379
now we'll transition to a new type of
models called generative adversarial

00:31:30.379 --> 00:31:30.389
models called generative adversarial
 

00:31:30.389 --> 00:31:32.479
models called generative adversarial
networks which are focused on a slightly

00:31:32.479 --> 00:31:32.489
networks which are focused on a slightly
 

00:31:32.489 --> 00:31:34.339
networks which are focused on a slightly
different problem and that's focused

00:31:34.339 --> 00:31:34.349
different problem and that's focused
 

00:31:34.349 --> 00:31:38.149
different problem and that's focused
mainly on sample generation so now

00:31:38.149 --> 00:31:38.159
mainly on sample generation so now
 

00:31:38.159 --> 00:31:40.129
mainly on sample generation so now
you're not concerned as much with

00:31:40.129 --> 00:31:40.139
you're not concerned as much with
 

00:31:40.139 --> 00:31:42.080
you're not concerned as much with
estimating the density of your

00:31:42.080 --> 00:31:42.090
estimating the density of your
 

00:31:42.090 --> 00:31:43.820
estimating the density of your
distribution but you care more about

00:31:43.820 --> 00:31:43.830
distribution but you care more about
 

00:31:43.830 --> 00:31:45.769
distribution but you care more about
just generating samples at the output

00:31:45.769 --> 00:31:45.779
just generating samples at the output
 

00:31:45.779 --> 00:31:47.930
just generating samples at the output
and the key idea here is that you have

00:31:47.930 --> 00:31:47.940
and the key idea here is that you have
 

00:31:47.940 --> 00:31:52.789
and the key idea here is that you have
the same Z this encoding but now in

00:31:52.789 --> 00:31:52.799
the same Z this encoding but now in
 

00:31:52.799 --> 00:31:55.460
the same Z this encoding but now in
generative adversarial networks there's

00:31:55.460 --> 00:31:55.470
generative adversarial networks there's
 

00:31:55.470 --> 00:31:57.320
generative adversarial networks there's
not as much semantic meaning as there

00:31:57.320 --> 00:31:57.330
not as much semantic meaning as there
 

00:31:57.330 --> 00:31:59.810
not as much semantic meaning as there
was in via is instead you're just

00:31:59.810 --> 00:31:59.820
was in via is instead you're just
 

00:31:59.820 --> 00:32:01.759
was in via is instead you're just
feeding in raw noise so it's just a

00:32:01.759 --> 00:32:01.769
feeding in raw noise so it's just a
 

00:32:01.769 --> 00:32:03.799
feeding in raw noise so it's just a
random noise label just a random noise

00:32:03.799 --> 00:32:03.809
random noise label just a random noise
 

00:32:03.809 --> 00:32:06.379
random noise label just a random noise
vector and you train you want to train

00:32:06.379 --> 00:32:06.389
vector and you train you want to train
 

00:32:06.389 --> 00:32:09.379
vector and you train you want to train
this generator to predict these fake

00:32:09.379 --> 00:32:09.389
this generator to predict these fake
 

00:32:09.389 --> 00:32:12.079
this generator to predict these fake
images at the output you want these fake

00:32:12.079 --> 00:32:12.089
images at the output you want these fake
 

00:32:12.089 --> 00:32:14.560
images at the output you want these fake
images to be as close as possible

00:32:14.560 --> 00:32:14.570
images to be as close as possible
 

00:32:14.570 --> 00:32:16.120
images to be as close as possible
- the real images from your training

00:32:16.120 --> 00:32:16.130
- the real images from your training
 

00:32:16.130 --> 00:32:19.600
- the real images from your training
distribution and the way we can do this

00:32:19.600 --> 00:32:19.610
distribution and the way we can do this
 

00:32:19.610 --> 00:32:22.330
distribution and the way we can do this
is actually using generative adversarial

00:32:22.330 --> 00:32:22.340
is actually using generative adversarial
 

00:32:22.340 --> 00:32:25.030
is actually using generative adversarial
networks which we have this generator on

00:32:25.030 --> 00:32:25.040
networks which we have this generator on
 

00:32:25.040 --> 00:32:27.010
networks which we have this generator on
the bottom generating fake images we

00:32:27.010 --> 00:32:27.020
the bottom generating fake images we
 

00:32:27.020 --> 00:32:29.020
the bottom generating fake images we
have a discriminator which is taking in

00:32:29.020 --> 00:32:29.030
have a discriminator which is taking in
 

00:32:29.030 --> 00:32:31.480
have a discriminator which is taking in
the fake images and also the real images

00:32:31.480 --> 00:32:31.490
the fake images and also the real images
 

00:32:31.490 --> 00:32:34.240
the fake images and also the real images
and learning to distinguish between fake

00:32:34.240 --> 00:32:34.250
and learning to distinguish between fake
 

00:32:34.250 --> 00:32:36.850
and learning to distinguish between fake
and real and by having these two neural

00:32:36.850 --> 00:32:36.860
and real and by having these two neural
 

00:32:36.860 --> 00:32:38.730
and real and by having these two neural
networks - generator and discriminator

00:32:38.730 --> 00:32:38.740
networks - generator and discriminator
 

00:32:38.740 --> 00:32:41.560
networks - generator and discriminator
compete against each other you force the

00:32:41.560 --> 00:32:41.570
compete against each other you force the
 

00:32:41.570 --> 00:32:44.110
compete against each other you force the
discriminator to learn how to

00:32:44.110 --> 00:32:44.120
discriminator to learn how to
 

00:32:44.120 --> 00:32:47.050
discriminator to learn how to
distinguish between real and fake and

00:32:47.050 --> 00:32:47.060
distinguish between real and fake and
 

00:32:47.060 --> 00:32:49.060
distinguish between real and fake and
the better that the discriminator

00:32:49.060 --> 00:32:49.070
the better that the discriminator
 

00:32:49.070 --> 00:32:50.980
the better that the discriminator
becomes at distinguishing real from fake

00:32:50.980 --> 00:32:50.990
becomes at distinguishing real from fake
 

00:32:50.990 --> 00:32:53.620
becomes at distinguishing real from fake
it forces the generator to produce

00:32:53.620 --> 00:32:53.630
it forces the generator to produce
 

00:32:53.630 --> 00:32:56.530
it forces the generator to produce
better and better or more realistic and

00:32:56.530 --> 00:32:56.540
better and better or more realistic and
 

00:32:56.540 --> 00:32:58.870
better and better or more realistic and
more realistic fake examples to keep

00:32:58.870 --> 00:32:58.880
more realistic fake examples to keep
 

00:32:58.880 --> 00:33:01.540
more realistic fake examples to keep
fooling the generator so let's walk

00:33:01.540 --> 00:33:01.550
fooling the generator so let's walk
 

00:33:01.550 --> 00:33:02.830
fooling the generator so let's walk
through a really quick example a toy

00:33:02.830 --> 00:33:02.840
through a really quick example a toy
 

00:33:02.840 --> 00:33:06.370
through a really quick example a toy
example on the intuition behind guns so

00:33:06.370 --> 00:33:06.380
example on the intuition behind guns so
 

00:33:06.380 --> 00:33:07.840
example on the intuition behind guns so
the generator starts from noise and a

00:33:07.840 --> 00:33:07.850
the generator starts from noise and a
 

00:33:07.850 --> 00:33:09.760
the generator starts from noise and a
crowd tries to create some imitation of

00:33:09.760 --> 00:33:09.770
crowd tries to create some imitation of
 

00:33:09.770 --> 00:33:11.590
crowd tries to create some imitation of
data so here's one dimensional data is

00:33:11.590 --> 00:33:11.600
data so here's one dimensional data is
 

00:33:11.600 --> 00:33:14.860
data so here's one dimensional data is
trying to just just generate some random

00:33:14.860 --> 00:33:14.870
trying to just just generate some random
 

00:33:14.870 --> 00:33:16.360
trying to just just generate some random
data because it hasn't been trained so

00:33:16.360 --> 00:33:16.370
data because it hasn't been trained so
 

00:33:16.370 --> 00:33:17.350
data because it hasn't been trained so
these are the points on a one

00:33:17.350 --> 00:33:17.360
these are the points on a one
 

00:33:17.360 --> 00:33:20.770
these are the points on a one
dimensional line of fake data the

00:33:20.770 --> 00:33:20.780
dimensional line of fake data the
 

00:33:20.780 --> 00:33:22.990
dimensional line of fake data the
discriminator sees these points but it

00:33:22.990 --> 00:33:23.000
discriminator sees these points but it
 

00:33:23.000 --> 00:33:24.610
discriminator sees these points but it
also sees some real data

00:33:24.610 --> 00:33:24.620
also sees some real data
 

00:33:24.620 --> 00:33:27.280
also sees some real data
now you train the discriminator to

00:33:27.280 --> 00:33:27.290
now you train the discriminator to
 

00:33:27.290 --> 00:33:30.220
now you train the discriminator to
recognize what is the probability that

00:33:30.220 --> 00:33:30.230
recognize what is the probability that
 

00:33:30.230 --> 00:33:31.930
recognize what is the probability that
this is real the discriminator knows

00:33:31.930 --> 00:33:31.940
this is real the discriminator knows
 

00:33:31.940 --> 00:33:33.820
this is real the discriminator knows
what's real and what's fake so you train

00:33:33.820 --> 00:33:33.830
what's real and what's fake so you train
 

00:33:33.830 --> 00:33:35.290
what's real and what's fake so you train
it to recognize what's real and what's

00:33:35.290 --> 00:33:35.300
it to recognize what's real and what's
 

00:33:35.300 --> 00:33:37.570
it to recognize what's real and what's
fake and in the beginning again it's not

00:33:37.570 --> 00:33:37.580
fake and in the beginning again it's not
 

00:33:37.580 --> 00:33:39.580
fake and in the beginning again it's not
trained but then you train it and it

00:33:39.580 --> 00:33:39.590
trained but then you train it and it
 

00:33:39.590 --> 00:33:41.050
trained but then you train it and it
starts increasing the probabilities what

00:33:41.050 --> 00:33:41.060
starts increasing the probabilities what
 

00:33:41.060 --> 00:33:42.940
starts increasing the probabilities what
of what's real decreasing the

00:33:42.940 --> 00:33:42.950
of what's real decreasing the
 

00:33:42.950 --> 00:33:45.790
of what's real decreasing the
probabilities of what's fake until you

00:33:45.790 --> 00:33:45.800
probabilities of what's fake until you
 

00:33:45.800 --> 00:33:48.640
probabilities of what's fake until you
get this perfect separation point where

00:33:48.640 --> 00:33:48.650
get this perfect separation point where
 

00:33:48.650 --> 00:33:50.620
get this perfect separation point where
the discriminator is able to separate

00:33:50.620 --> 00:33:50.630
the discriminator is able to separate
 

00:33:50.630 --> 00:33:52.390
the discriminator is able to separate
and distinguish what is real from what

00:33:52.390 --> 00:33:52.400
and distinguish what is real from what
 

00:33:52.400 --> 00:33:55.240
and distinguish what is real from what
is fake now the generator comes back and

00:33:55.240 --> 00:33:55.250
is fake now the generator comes back and
 

00:33:55.250 --> 00:33:57.610
is fake now the generator comes back and
sees how well the discriminator is doing

00:33:57.610 --> 00:33:57.620
sees how well the discriminator is doing
 

00:33:57.620 --> 00:33:59.920
sees how well the discriminator is doing
and it tries to move its generated

00:33:59.920 --> 00:33:59.930
and it tries to move its generated
 

00:33:59.930 --> 00:34:03.310
and it tries to move its generated
points closer to the real data to start

00:34:03.310 --> 00:34:03.320
points closer to the real data to start
 

00:34:03.320 --> 00:34:04.900
points closer to the real data to start
fooling the generator to start fooling

00:34:04.900 --> 00:34:04.910
fooling the generator to start fooling
 

00:34:04.910 --> 00:34:05.860
fooling the generator to start fooling
the discriminator

00:34:05.860 --> 00:34:05.870
the discriminator
 

00:34:05.870 --> 00:34:07.720
the discriminator
so now it's moving those points closer

00:34:07.720 --> 00:34:07.730
so now it's moving those points closer
 

00:34:07.730 --> 00:34:10.270
so now it's moving those points closer
and closer to the green points now let's

00:34:10.270 --> 00:34:10.280
and closer to the green points now let's
 

00:34:10.280 --> 00:34:11.730
and closer to the green points now let's
go back to the discriminator

00:34:11.730 --> 00:34:11.740
go back to the discriminator
 

00:34:11.740 --> 00:34:15.070
go back to the discriminator
discriminator gets these new points now

00:34:15.070 --> 00:34:15.080
discriminator gets these new points now
 

00:34:15.080 --> 00:34:16.690
discriminator gets these new points now
its previous predictions are a little

00:34:16.690 --> 00:34:16.700
its previous predictions are a little
 

00:34:16.700 --> 00:34:18.970
its previous predictions are a little
bit messed up right so it's it's used to

00:34:18.970 --> 00:34:18.980
bit messed up right so it's it's used to
 

00:34:18.980 --> 00:34:20.890
bit messed up right so it's it's used to
seeing some of those red points farther

00:34:20.890 --> 00:34:20.900
seeing some of those red points farther
 

00:34:20.900 --> 00:34:23.620
seeing some of those red points farther
away it you can retrain it now it can

00:34:23.620 --> 00:34:23.630
away it you can retrain it now it can
 

00:34:23.630 --> 00:34:26.200
away it you can retrain it now it can
learn again decreasing the probability

00:34:26.200 --> 00:34:26.210
learn again decreasing the probability
 

00:34:26.210 --> 00:34:28.060
learn again decreasing the probability
of those red points coming in

00:34:28.060 --> 00:34:28.070
of those red points coming in
 

00:34:28.070 --> 00:34:29.560
of those red points coming in
increasing the probability of those

00:34:29.560 --> 00:34:29.570
increasing the probability of those
 

00:34:29.570 --> 00:34:34.149
increasing the probability of those
green or real points even more and we

00:34:34.149 --> 00:34:34.159
green or real points even more and we
 

00:34:34.159 --> 00:34:36.399
green or real points even more and we
repeat again now the generator one last

00:34:36.399 --> 00:34:36.409
repeat again now the generator one last
 

00:34:36.409 --> 00:34:38.500
repeat again now the generator one last
time starts moving those points even

00:34:38.500 --> 00:34:38.510
time starts moving those points even
 

00:34:38.510 --> 00:34:40.899
time starts moving those points even
closer to the real distribution what you

00:34:40.899 --> 00:34:40.909
closer to the real distribution what you
 

00:34:40.909 --> 00:34:42.460
closer to the real distribution what you
can see here is now that the points are

00:34:42.460 --> 00:34:42.470
can see here is now that the points are
 

00:34:42.470 --> 00:34:45.370
can see here is now that the points are
following almost in this toy example the

00:34:45.370 --> 00:34:45.380
following almost in this toy example the
 

00:34:45.380 --> 00:34:47.649
following almost in this toy example the
same distribution as the real data and

00:34:47.649 --> 00:34:47.659
same distribution as the real data and
 

00:34:47.659 --> 00:34:49.840
same distribution as the real data and
it's very hard for the discriminator to

00:34:49.840 --> 00:34:49.850
it's very hard for the discriminator to
 

00:34:49.850 --> 00:34:51.580
it's very hard for the discriminator to
distinguish between what is real and

00:34:51.580 --> 00:34:51.590
distinguish between what is real and
 

00:34:51.590 --> 00:34:54.520
distinguish between what is real and
what is fake and that's the idea behind

00:34:54.520 --> 00:34:54.530
what is fake and that's the idea behind
 

00:34:54.530 --> 00:34:57.550
what is fake and that's the idea behind
ganz so get in ganz we have a

00:34:57.550 --> 00:34:57.560
ganz so get in ganz we have a
 

00:34:57.560 --> 00:34:59.140
ganz so get in ganz we have a
discriminator just to summarize a

00:34:59.140 --> 00:34:59.150
discriminator just to summarize a
 

00:34:59.150 --> 00:35:00.550
discriminator just to summarize a
discriminator that is trying to identify

00:35:00.550 --> 00:35:00.560
discriminator that is trying to identify
 

00:35:00.560 --> 00:35:02.080
discriminator that is trying to identify
real from fake

00:35:02.080 --> 00:35:02.090
real from fake
 

00:35:02.090 --> 00:35:05.650
real from fake
well the generator tries to imitate real

00:35:05.650 --> 00:35:05.660
well the generator tries to imitate real
 

00:35:05.660 --> 00:35:09.160
well the generator tries to imitate real
data and fooled the discriminator this

00:35:09.160 --> 00:35:09.170
data and fooled the discriminator this
 

00:35:09.170 --> 00:35:11.310
data and fooled the discriminator this
can be formalizing using a min/max

00:35:11.310 --> 00:35:11.320
can be formalizing using a min/max
 

00:35:11.320 --> 00:35:13.390
can be formalizing using a min/max
objective function where the

00:35:13.390 --> 00:35:13.400
objective function where the
 

00:35:13.400 --> 00:35:16.810
objective function where the
discriminator d so the the parameters of

00:35:16.810 --> 00:35:16.820
discriminator d so the the parameters of
 

00:35:16.820 --> 00:35:19.030
discriminator d so the the parameters of
the discriminator d is trying to

00:35:19.030 --> 00:35:19.040
the discriminator d is trying to
 

00:35:19.040 --> 00:35:21.580
the discriminator d is trying to
maximize the likelihood objective and

00:35:21.580 --> 00:35:21.590
maximize the likelihood objective and
 

00:35:21.590 --> 00:35:23.620
maximize the likelihood objective and
increase the chances that this term on

00:35:23.620 --> 00:35:23.630
increase the chances that this term on
 

00:35:23.630 --> 00:35:27.000
increase the chances that this term on
the left this is the probability that

00:35:27.000 --> 00:35:27.010
the left this is the probability that
 

00:35:27.010 --> 00:35:29.680
the left this is the probability that
the real data is as close as possible to

00:35:29.680 --> 00:35:29.690
the real data is as close as possible to
 

00:35:29.690 --> 00:35:33.280
the real data is as close as possible to
one and it wants to also get this so

00:35:33.280 --> 00:35:33.290
one and it wants to also get this so
 

00:35:33.290 --> 00:35:34.630
one and it wants to also get this so
let's get this probability as close as

00:35:34.630 --> 00:35:34.640
let's get this probability as close as
 

00:35:34.640 --> 00:35:36.640
let's get this probability as close as
possible to one it wants to get this

00:35:36.640 --> 00:35:36.650
possible to one it wants to get this
 

00:35:36.650 --> 00:35:38.530
possible to one it wants to get this
probability as close as possible to zero

00:35:38.530 --> 00:35:38.540
probability as close as possible to zero
 

00:35:38.540 --> 00:35:39.760
probability as close as possible to zero
because that's the probability of seeing

00:35:39.760 --> 00:35:39.770
because that's the probability of seeing
 

00:35:39.770 --> 00:35:43.720
because that's the probability of seeing
fake data and on the other hand the

00:35:43.720 --> 00:35:43.730
fake data and on the other hand the
 

00:35:43.730 --> 00:35:47.650
fake data and on the other hand the
generator now which is theta g tries to

00:35:47.650 --> 00:35:47.660
generator now which is theta g tries to
 

00:35:47.660 --> 00:35:50.410
generator now which is theta g tries to
change its weights in order to maximize

00:35:50.410 --> 00:35:50.420
change its weights in order to maximize
 

00:35:50.420 --> 00:35:53.230
change its weights in order to maximize
that same objective so i minimize that

00:35:53.230 --> 00:35:53.240
that same objective so i minimize that
 

00:35:53.240 --> 00:35:55.570
that same objective so i minimize that
same objective so now it's going to

00:35:55.570 --> 00:35:55.580
same objective so now it's going to
 

00:35:55.580 --> 00:35:57.490
same objective so now it's going to
change its weights theta g inside of

00:35:57.490 --> 00:35:57.500
change its weights theta g inside of
 

00:35:57.500 --> 00:36:00.190
change its weights theta g inside of
here to generate new fake data that is

00:36:00.190 --> 00:36:00.200
here to generate new fake data that is
 

00:36:00.200 --> 00:36:02.140
here to generate new fake data that is
going to fool that generate that

00:36:02.140 --> 00:36:02.150
going to fool that generate that
 

00:36:02.150 --> 00:36:06.370
going to fool that generate that
discriminator so briefly what are the

00:36:06.370 --> 00:36:06.380
discriminator so briefly what are the
 

00:36:06.380 --> 00:36:07.840
discriminator so briefly what are the
benefits of gans as opposed to

00:36:07.840 --> 00:36:07.850
benefits of gans as opposed to
 

00:36:07.850 --> 00:36:09.460
benefits of gans as opposed to
variational autoencoder these are two

00:36:09.460 --> 00:36:09.470
variational autoencoder these are two
 

00:36:09.470 --> 00:36:12.240
variational autoencoder these are two
versions of very latent variable models

00:36:12.240 --> 00:36:12.250
versions of very latent variable models
 

00:36:12.250 --> 00:36:14.800
versions of very latent variable models
but there are some benefits to using

00:36:14.800 --> 00:36:14.810
but there are some benefits to using
 

00:36:14.810 --> 00:36:17.410
but there are some benefits to using
gans as opposed to VA e's imagine we're

00:36:17.410 --> 00:36:17.420
gans as opposed to VA e's imagine we're
 

00:36:17.420 --> 00:36:19.120
gans as opposed to VA e's imagine we're
trying to fit a model to this latent

00:36:19.120 --> 00:36:19.130
trying to fit a model to this latent
 

00:36:19.130 --> 00:36:21.490
trying to fit a model to this latent
manifold which you can see cartoon ax

00:36:21.490 --> 00:36:21.500
manifold which you can see cartoon ax
 

00:36:21.500 --> 00:36:24.430
manifold which you can see cartoon ax
fide here so if we're using a

00:36:24.430 --> 00:36:24.440
fide here so if we're using a
 

00:36:24.440 --> 00:36:26.320
fide here so if we're using a
traditional maximum likelihood

00:36:26.320 --> 00:36:26.330
traditional maximum likelihood
 

00:36:26.330 --> 00:36:28.900
traditional maximum likelihood
estimation we can see on the left-hand

00:36:28.900 --> 00:36:28.910
estimation we can see on the left-hand
 

00:36:28.910 --> 00:36:31.240
estimation we can see on the left-hand
side that we're having a very fuzzy

00:36:31.240 --> 00:36:31.250
side that we're having a very fuzzy
 

00:36:31.250 --> 00:36:35.080
side that we're having a very fuzzy
estimation here it's noisy and it's kind

00:36:35.080 --> 00:36:35.090
estimation here it's noisy and it's kind
 

00:36:35.090 --> 00:36:36.580
estimation here it's noisy and it's kind
of taking a smooth round not really

00:36:36.580 --> 00:36:36.590
of taking a smooth round not really
 

00:36:36.590 --> 00:36:38.140
of taking a smooth round not really
capturing a lot of the details of the

00:36:38.140 --> 00:36:38.150
capturing a lot of the details of the
 

00:36:38.150 --> 00:36:40.270
capturing a lot of the details of the
manifold the difference here is that

00:36:40.270 --> 00:36:40.280
manifold the difference here is that
 

00:36:40.280 --> 00:36:41.230
manifold the difference here is that
again

00:36:41.230 --> 00:36:41.240
again
 

00:36:41.240 --> 00:36:42.520
again
it's not using a maximum likelihood

00:36:42.520 --> 00:36:42.530
it's not using a maximum likelihood
 

00:36:42.530 --> 00:36:44.500
it's not using a maximum likelihood
estimate it's using this minimax

00:36:44.500 --> 00:36:44.510
estimate it's using this minimax
 

00:36:44.510 --> 00:36:46.960
estimate it's using this minimax
formulation between two neural networks

00:36:46.960 --> 00:36:46.970
formulation between two neural networks
 

00:36:46.970 --> 00:36:48.970
formulation between two neural networks
it's able to capture a lot of the

00:36:48.970 --> 00:36:48.980
it's able to capture a lot of the
 

00:36:48.980 --> 00:36:50.680
it's able to capture a lot of the
details and the nooks and crannies of

00:36:50.680 --> 00:36:50.690
details and the nooks and crannies of
 

00:36:50.690 --> 00:36:53.170
details and the nooks and crannies of
this manifold in that sense it's able to

00:36:53.170 --> 00:36:53.180
this manifold in that sense it's able to
 

00:36:53.180 --> 00:36:56.140
this manifold in that sense it's able to
create a lot crisper or it's able to

00:36:56.140 --> 00:36:56.150
create a lot crisper or it's able to
 

00:36:56.150 --> 00:36:59.880
create a lot crisper or it's able to
model a lot more detail in the real data

00:36:59.880 --> 00:36:59.890
model a lot more detail in the real data
 

00:36:59.890 --> 00:37:02.620
model a lot more detail in the real data
so let's explore what we can generate

00:37:02.620 --> 00:37:02.630
so let's explore what we can generate
 

00:37:02.630 --> 00:37:05.920
so let's explore what we can generate
with this new data and this actually

00:37:05.920 --> 00:37:05.930
with this new data and this actually
 

00:37:05.930 --> 00:37:08.290
with this new data and this actually
leads nicely into a lot of the recent

00:37:08.290 --> 00:37:08.300
leads nicely into a lot of the recent
 

00:37:08.300 --> 00:37:10.390
leads nicely into a lot of the recent
advances that Gant's have enjoyed in the

00:37:10.390 --> 00:37:10.400
advances that Gant's have enjoyed in the
 

00:37:10.400 --> 00:37:12.970
advances that Gant's have enjoyed in the
past couple of years and I'll talk about

00:37:12.970 --> 00:37:12.980
past couple of years and I'll talk about
 

00:37:12.980 --> 00:37:14.500
past couple of years and I'll talk about
some results even in the past couple

00:37:14.500 --> 00:37:14.510
some results even in the past couple
 

00:37:14.510 --> 00:37:16.870
some results even in the past couple
months that are really astonishing and

00:37:16.870 --> 00:37:16.880
months that are really astonishing and
 

00:37:16.880 --> 00:37:21.790
months that are really astonishing and
that starts with this idea from earlier

00:37:21.790 --> 00:37:21.800
that starts with this idea from earlier
 

00:37:21.800 --> 00:37:26.020
that starts with this idea from earlier
2018 involving the progressive growth of

00:37:26.020 --> 00:37:26.030
2018 involving the progressive growth of
 

00:37:26.030 --> 00:37:28.180
2018 involving the progressive growth of
Bret against so the idea here is that

00:37:28.180 --> 00:37:28.190
Bret against so the idea here is that
 

00:37:28.190 --> 00:37:32.109
Bret against so the idea here is that
you want to iteratively build more and

00:37:32.109 --> 00:37:32.119
you want to iteratively build more and
 

00:37:32.119 --> 00:37:34.420
you want to iteratively build more and
more detailed image generators so you

00:37:34.420 --> 00:37:34.430
more detailed image generators so you
 

00:37:34.430 --> 00:37:37.540
more detailed image generators so you
start your generator by just predicting

00:37:37.540 --> 00:37:37.550
start your generator by just predicting
 

00:37:37.550 --> 00:37:39.970
start your generator by just predicting
4x4 images very coarse-grained images

00:37:39.970 --> 00:37:39.980
4x4 images very coarse-grained images
 

00:37:39.980 --> 00:37:42.760
4x4 images very coarse-grained images
not detailed at all but when you start

00:37:42.760 --> 00:37:42.770
not detailed at all but when you start
 

00:37:42.770 --> 00:37:43.900
not detailed at all but when you start
with this it's able to learn a

00:37:43.900 --> 00:37:43.910
with this it's able to learn a
 

00:37:43.910 --> 00:37:45.430
with this it's able to learn a
representation a very coarse-grained

00:37:45.430 --> 00:37:45.440
representation a very coarse-grained
 

00:37:45.440 --> 00:37:47.410
representation a very coarse-grained
representation of how to generate these

00:37:47.410 --> 00:37:47.420
representation of how to generate these
 

00:37:47.420 --> 00:37:50.530
representation of how to generate these
coarse images once the generator has a

00:37:50.530 --> 00:37:50.540
coarse images once the generator has a
 

00:37:50.540 --> 00:37:52.240
coarse images once the generator has a
good intuition on this coarse-grained

00:37:52.240 --> 00:37:52.250
good intuition on this coarse-grained
 

00:37:52.250 --> 00:37:53.830
good intuition on this coarse-grained
representation you start progressively

00:37:53.830 --> 00:37:53.840
representation you start progressively
 

00:37:53.840 --> 00:37:55.870
representation you start progressively
growing its dimensionality and start

00:37:55.870 --> 00:37:55.880
growing its dimensionality and start
 

00:37:55.880 --> 00:37:58.150
growing its dimensionality and start
progressively adding new and new layers

00:37:58.150 --> 00:37:58.160
progressively adding new and new layers
 

00:37:58.160 --> 00:38:00.970
progressively adding new and new layers
and increasing the spatial resolution of

00:38:00.970 --> 00:38:00.980
and increasing the spatial resolution of
 

00:38:00.980 --> 00:38:03.430
and increasing the spatial resolution of
the generator this is good because it's

00:38:03.430 --> 00:38:03.440
the generator this is good because it's
 

00:38:03.440 --> 00:38:06.010
the generator this is good because it's
able to stable the synthesis of the

00:38:06.010 --> 00:38:06.020
able to stable the synthesis of the
 

00:38:06.020 --> 00:38:07.750
able to stable the synthesis of the
output and also it actually ends up

00:38:07.750 --> 00:38:07.760
output and also it actually ends up
 

00:38:07.760 --> 00:38:10.000
output and also it actually ends up
speeding up training as well and it's

00:38:10.000 --> 00:38:10.010
speeding up training as well and it's
 

00:38:10.010 --> 00:38:11.650
speeding up training as well and it's
able to create these high-resolution

00:38:11.650 --> 00:38:11.660
able to create these high-resolution
 

00:38:11.660 --> 00:38:15.460
able to create these high-resolution
input outputs at the end 1000 by a

00:38:15.460 --> 00:38:15.470
input outputs at the end 1000 by a
 

00:38:15.470 --> 00:38:18.070
input outputs at the end 1000 by a
thousand images that are very realistic

00:38:18.070 --> 00:38:18.080
thousand images that are very realistic
 

00:38:18.080 --> 00:38:19.660
thousand images that are very realistic
again these weren't the images that I

00:38:19.660 --> 00:38:19.670
again these weren't the images that I
 

00:38:19.670 --> 00:38:21.250
again these weren't the images that I
showed you I'll get to those in a second

00:38:21.250 --> 00:38:21.260
showed you I'll get to those in a second
 

00:38:21.260 --> 00:38:23.140
showed you I'll get to those in a second
but these are still incredibly realistic

00:38:23.140 --> 00:38:23.150
but these are still incredibly realistic
 

00:38:23.150 --> 00:38:26.170
but these are still incredibly realistic
images and here's some more examples of

00:38:26.170 --> 00:38:26.180
images and here's some more examples of
 

00:38:26.180 --> 00:38:29.010
images and here's some more examples of
that same network producing those images

00:38:29.010 --> 00:38:29.020
that same network producing those images
 

00:38:29.020 --> 00:38:32.260
that same network producing those images
the same team actually released another

00:38:32.260 --> 00:38:32.270
the same team actually released another
 

00:38:32.270 --> 00:38:36.160
the same team actually released another
paper just a month ago where they create

00:38:36.160 --> 00:38:36.170
paper just a month ago where they create
 

00:38:36.170 --> 00:38:38.050
paper just a month ago where they create
an extension of this work to a more

00:38:38.050 --> 00:38:38.060
an extension of this work to a more
 

00:38:38.060 --> 00:38:40.120
an extension of this work to a more
complex architecture that builds on this

00:38:40.120 --> 00:38:40.130
complex architecture that builds on this
 

00:38:40.130 --> 00:38:43.390
complex architecture that builds on this
work this progressive growing of gans to

00:38:43.390 --> 00:38:43.400
work this progressive growing of gans to
 

00:38:43.400 --> 00:38:46.420
work this progressive growing of gans to
actually use what they call a style

00:38:46.420 --> 00:38:46.430
actually use what they call a style
 

00:38:46.430 --> 00:38:47.890
actually use what they call a style
based approach where they're actually

00:38:47.890 --> 00:38:47.900
based approach where they're actually
 

00:38:47.900 --> 00:38:49.870
based approach where they're actually
using the underlying style which you can

00:38:49.870 --> 00:38:49.880
using the underlying style which you can
 

00:38:49.880 --> 00:38:51.730
using the underlying style which you can
think of as like a latent variable of

00:38:51.730 --> 00:38:51.740
think of as like a latent variable of
 

00:38:51.740 --> 00:38:54.340
think of as like a latent variable of
the faces and using that as an

00:38:54.340 --> 00:38:54.350
the faces and using that as an
 

00:38:54.350 --> 00:38:55.000
the faces and using that as an
intermediate

00:38:55.000 --> 00:38:55.010
intermediate
 

00:38:55.010 --> 00:38:56.440
intermediate
mapping to automatically learn

00:38:56.440 --> 00:38:56.450
mapping to automatically learn
 

00:38:56.450 --> 00:38:58.660
mapping to automatically learn
unsupervised separation of the

00:38:58.660 --> 00:38:58.670
unsupervised separation of the
 

00:38:58.670 --> 00:39:00.310
unsupervised separation of the
high-level attributes such as the pose

00:39:00.310 --> 00:39:00.320
high-level attributes such as the pose
 

00:39:00.320 --> 00:39:03.460
high-level attributes such as the pose
or like the subjects hair or skin color

00:39:03.460 --> 00:39:03.470
or like the subjects hair or skin color
 

00:39:03.470 --> 00:39:07.570
or like the subjects hair or skin color
etc as a result this approach which you

00:39:07.570 --> 00:39:07.580
etc as a result this approach which you
 

00:39:07.580 --> 00:39:08.950
etc as a result this approach which you
can see generated examples here these

00:39:08.950 --> 00:39:08.960
can see generated examples here these
 

00:39:08.960 --> 00:39:10.750
can see generated examples here these
are not a real examples these are fake

00:39:10.750 --> 00:39:10.760
are not a real examples these are fake
 

00:39:10.760 --> 00:39:13.570
are not a real examples these are fake
or generated examples the model is able

00:39:13.570 --> 00:39:13.580
or generated examples the model is able
 

00:39:13.580 --> 00:39:17.250
or generated examples the model is able
to generate highly varied outputs that

00:39:17.250 --> 00:39:17.260
to generate highly varied outputs that
 

00:39:17.260 --> 00:39:20.140
to generate highly varied outputs that
highlight that represent very realistic

00:39:20.140 --> 00:39:20.150
highlight that represent very realistic
 

00:39:20.150 --> 00:39:25.180
highlight that represent very realistic
human faces so the idea behind style

00:39:25.180 --> 00:39:25.190
human faces so the idea behind style
 

00:39:25.190 --> 00:39:27.880
human faces so the idea behind style
transfer is actually have the generator

00:39:27.880 --> 00:39:27.890
transfer is actually have the generator
 

00:39:27.890 --> 00:39:30.250
transfer is actually have the generator
transfer styles from a source image

00:39:30.250 --> 00:39:30.260
transfer styles from a source image
 

00:39:30.260 --> 00:39:32.800
transfer styles from a source image
which you can see on the top row so

00:39:32.800 --> 00:39:32.810
which you can see on the top row so
 

00:39:32.810 --> 00:39:35.170
which you can see on the top row so
these are source images to a destination

00:39:35.170 --> 00:39:35.180
these are source images to a destination
 

00:39:35.180 --> 00:39:37.240
these are source images to a destination
image which you can see on the right

00:39:37.240 --> 00:39:37.250
image which you can see on the right
 

00:39:37.250 --> 00:39:40.290
image which you can see on the right
hand side and it's taking the features

00:39:40.290 --> 00:39:40.300
hand side and it's taking the features
 

00:39:40.300 --> 00:39:43.480
hand side and it's taking the features
from the source image and applying it to

00:39:43.480 --> 00:39:43.490
from the source image and applying it to
 

00:39:43.490 --> 00:39:45.160
from the source image and applying it to
the destination image and it's actually

00:39:45.160 --> 00:39:45.170
the destination image and it's actually
 

00:39:45.170 --> 00:39:47.770
the destination image and it's actually
able to realize in certain cases that if

00:39:47.770 --> 00:39:47.780
able to realize in certain cases that if
 

00:39:47.780 --> 00:39:50.890
able to realize in certain cases that if
you try and apply it a male's face to a

00:39:50.890 --> 00:39:50.900
you try and apply it a male's face to a
 

00:39:50.900 --> 00:39:52.990
you try and apply it a male's face to a
female it actually realized that

00:39:52.990 --> 00:39:53.000
female it actually realized that
 

00:39:53.000 --> 00:39:54.460
female it actually realized that
something is wrong and starts to add

00:39:54.460 --> 00:39:54.470
something is wrong and starts to add
 

00:39:54.470 --> 00:39:57.220
something is wrong and starts to add
male features to this face it realizes

00:39:57.220 --> 00:39:57.230
male features to this face it realizes
 

00:39:57.230 --> 00:39:58.420
male features to this face it realizes
that something's wrong so it starts to

00:39:58.420 --> 00:39:58.430
that something's wrong so it starts to
 

00:39:58.430 --> 00:40:00.070
that something's wrong so it starts to
add a beard here even though there was

00:40:00.070 --> 00:40:00.080
add a beard here even though there was
 

00:40:00.080 --> 00:40:02.740
add a beard here even though there was
no beard on this destination image and

00:40:02.740 --> 00:40:02.750
no beard on this destination image and
 

00:40:02.750 --> 00:40:04.570
no beard on this destination image and
it's things like this that you're

00:40:04.570 --> 00:40:04.580
it's things like this that you're
 

00:40:04.580 --> 00:40:05.980
it's things like this that you're
actually understanding that it's able to

00:40:05.980 --> 00:40:05.990
actually understanding that it's able to
 

00:40:05.990 --> 00:40:08.290
actually understanding that it's able to
get some intuition about the underlying

00:40:08.290 --> 00:40:08.300
get some intuition about the underlying
 

00:40:08.300 --> 00:40:10.360
get some intuition about the underlying
distribution of males and females males

00:40:10.360 --> 00:40:10.370
distribution of males and females males
 

00:40:10.370 --> 00:40:12.310
distribution of males and females males
have beards facial hair whereas females

00:40:12.310 --> 00:40:12.320
have beards facial hair whereas females
 

00:40:12.320 --> 00:40:15.070
have beards facial hair whereas females
typically don't and there are other

00:40:15.070 --> 00:40:15.080
typically don't and there are other
 

00:40:15.080 --> 00:40:16.300
typically don't and there are other
examples here as well it's really

00:40:16.300 --> 00:40:16.310
examples here as well it's really
 

00:40:16.310 --> 00:40:19.240
examples here as well it's really
remarkable work transforming skin color

00:40:19.240 --> 00:40:19.250
remarkable work transforming skin color
 

00:40:19.250 --> 00:40:23.350
remarkable work transforming skin color
hair color even like patterns on the

00:40:23.350 --> 00:40:23.360
hair color even like patterns on the
 

00:40:23.360 --> 00:40:26.380
hair color even like patterns on the
face to very fine-grained details in the

00:40:26.380 --> 00:40:26.390
face to very fine-grained details in the
 

00:40:26.390 --> 00:40:30.240
face to very fine-grained details in the
in the image in the output image and

00:40:30.240 --> 00:40:30.250
in the image in the output image and
 

00:40:30.250 --> 00:40:33.460
in the image in the output image and
finally one very last applications I'll

00:40:33.460 --> 00:40:33.470
finally one very last applications I'll
 

00:40:33.470 --> 00:40:35.680
finally one very last applications I'll
touch on here is this notion of cycle

00:40:35.680 --> 00:40:35.690
touch on here is this notion of cycle
 

00:40:35.690 --> 00:40:37.870
touch on here is this notion of cycle
gang which is the idea of having

00:40:37.870 --> 00:40:37.880
gang which is the idea of having
 

00:40:37.880 --> 00:40:40.330
gang which is the idea of having
unpaired image to image translation

00:40:40.330 --> 00:40:40.340
unpaired image to image translation
 

00:40:40.340 --> 00:40:42.580
unpaired image to image translation
which is very closely related to what we

00:40:42.580 --> 00:40:42.590
which is very closely related to what we
 

00:40:42.590 --> 00:40:44.230
which is very closely related to what we
just discussed in the progressive

00:40:44.230 --> 00:40:44.240
just discussed in the progressive
 

00:40:44.240 --> 00:40:47.860
just discussed in the progressive
growing of Gans through styles and what

00:40:47.860 --> 00:40:47.870
growing of Gans through styles and what
 

00:40:47.870 --> 00:40:49.120
growing of Gans through styles and what
we want to do here is actually take a

00:40:49.120 --> 00:40:49.130
we want to do here is actually take a
 

00:40:49.130 --> 00:40:50.890
we want to do here is actually take a
bunch of images in one domain and

00:40:50.890 --> 00:40:50.900
bunch of images in one domain and
 

00:40:50.900 --> 00:40:53.200
bunch of images in one domain and
without having the corresponding image

00:40:53.200 --> 00:40:53.210
without having the corresponding image
 

00:40:53.210 --> 00:40:55.060
without having the corresponding image
in the say in a different domain we want

00:40:55.060 --> 00:40:55.070
in the say in a different domain we want
 

00:40:55.070 --> 00:40:58.090
in the say in a different domain we want
to just learn a generator to take an

00:40:58.090 --> 00:40:58.100
to just learn a generator to take an
 

00:40:58.100 --> 00:41:00.460
to just learn a generator to take an
image in one domain generate a new image

00:41:00.460 --> 00:41:00.470
image in one domain generate a new image
 

00:41:00.470 --> 00:41:03.730
image in one domain generate a new image
so take an image in Ex generate a new

00:41:03.730 --> 00:41:03.740
so take an image in Ex generate a new
 

00:41:03.740 --> 00:41:05.350
so take an image in Ex generate a new
image and why following wise

00:41:05.350 --> 00:41:05.360
image and why following wise
 

00:41:05.360 --> 00:41:09.520
image and why following wise
distribution and likewise take

00:41:09.520 --> 00:41:09.530
distribution and likewise take
 

00:41:09.530 --> 00:41:11.940
distribution and likewise take
image of why created an image in X and

00:41:11.940 --> 00:41:11.950
image of why created an image in X and
 

00:41:11.950 --> 00:41:14.140
image of why created an image in X and
the way they actually do this a really

00:41:14.140 --> 00:41:14.150
the way they actually do this a really
 

00:41:14.150 --> 00:41:16.000
the way they actually do this a really
cool advancement of this paper was what

00:41:16.000 --> 00:41:16.010
cool advancement of this paper was what
 

00:41:16.010 --> 00:41:17.380
cool advancement of this paper was what
they did was they actually created this

00:41:17.380 --> 00:41:17.390
they did was they actually created this
 

00:41:17.390 --> 00:41:20.440
they did was they actually created this
cycle lost the cycle consistency that

00:41:20.440 --> 00:41:20.450
cycle lost the cycle consistency that
 

00:41:20.450 --> 00:41:23.770
cycle lost the cycle consistency that
the network has to abide by where and if

00:41:23.770 --> 00:41:23.780
the network has to abide by where and if
 

00:41:23.780 --> 00:41:27.220
the network has to abide by where and if
they create going from X to Y they then

00:41:27.220 --> 00:41:27.230
they create going from X to Y they then
 

00:41:27.230 --> 00:41:29.440
they create going from X to Y they then
take that same generated output and go

00:41:29.440 --> 00:41:29.450
take that same generated output and go
 

00:41:29.450 --> 00:41:32.650
take that same generated output and go
back from Y to X and check how close

00:41:32.650 --> 00:41:32.660
back from Y to X and check how close
 

00:41:32.660 --> 00:41:34.180
back from Y to X and check how close
they are to the original input data and

00:41:34.180 --> 00:41:34.190
they are to the original input data and
 

00:41:34.190 --> 00:41:36.220
they are to the original input data and
they enforce another supervised loss

00:41:36.220 --> 00:41:36.230
they enforce another supervised loss
 

00:41:36.230 --> 00:41:37.690
they enforce another supervised loss
using that and that's why they call this

00:41:37.690 --> 00:41:37.700
using that and that's why they call this
 

00:41:37.700 --> 00:41:39.100
using that and that's why they call this
approach cycle again because you're

00:41:39.100 --> 00:41:39.110
approach cycle again because you're
 

00:41:39.110 --> 00:41:41.470
approach cycle again because you're
creating the cycle loss between the two

00:41:41.470 --> 00:41:41.480
creating the cycle loss between the two
 

00:41:41.480 --> 00:41:43.330
creating the cycle loss between the two
generators and you also have these two

00:41:43.330 --> 00:41:43.340
generators and you also have these two
 

00:41:43.340 --> 00:41:45.270
generators and you also have these two
discriminators that are trying to

00:41:45.270 --> 00:41:45.280
discriminators that are trying to
 

00:41:45.280 --> 00:41:47.620
discriminators that are trying to
distinguish real from fake in each of

00:41:47.620 --> 00:41:47.630
distinguish real from fake in each of
 

00:41:47.630 --> 00:41:50.920
distinguish real from fake in each of
those distributions and what you're able

00:41:50.920 --> 00:41:50.930
those distributions and what you're able
 

00:41:50.930 --> 00:41:53.940
those distributions and what you're able
to do is actually transfer domains from

00:41:53.940 --> 00:41:53.950
to do is actually transfer domains from
 

00:41:53.950 --> 00:41:56.620
to do is actually transfer domains from
unpaired images like horses on the left

00:41:56.620 --> 00:41:56.630
unpaired images like horses on the left
 

00:41:56.630 --> 00:41:58.660
unpaired images like horses on the left
hand side and you're now taking that

00:41:58.660 --> 00:41:58.670
hand side and you're now taking that
 

00:41:58.670 --> 00:42:01.960
hand side and you're now taking that
horse and making it into a zebra okay so

00:42:01.960 --> 00:42:01.970
horse and making it into a zebra okay so
 

00:42:01.970 --> 00:42:03.970
horse and making it into a zebra okay so
this is just taking images of a lot of

00:42:03.970 --> 00:42:03.980
this is just taking images of a lot of
 

00:42:03.980 --> 00:42:06.760
this is just taking images of a lot of
horses and a lot of zebras and without

00:42:06.760 --> 00:42:06.770
horses and a lot of zebras and without
 

00:42:06.770 --> 00:42:09.490
horses and a lot of zebras and without
actually supervising how to go from

00:42:09.490 --> 00:42:09.500
actually supervising how to go from
 

00:42:09.500 --> 00:42:11.050
actually supervising how to go from
horse to zebra it learns the underlying

00:42:11.050 --> 00:42:11.060
horse to zebra it learns the underlying
 

00:42:11.060 --> 00:42:12.280
horse to zebra it learns the underlying
distribution of what's a horse

00:42:12.280 --> 00:42:12.290
distribution of what's a horse
 

00:42:12.290 --> 00:42:14.530
distribution of what's a horse
what's a zebra so you can take a new

00:42:14.530 --> 00:42:14.540
what's a zebra so you can take a new
 

00:42:14.540 --> 00:42:17.410
what's a zebra so you can take a new
image of a horse and make it look like a

00:42:17.410 --> 00:42:17.420
image of a horse and make it look like a
 

00:42:17.420 --> 00:42:19.930
image of a horse and make it look like a
zebra and it's actually really

00:42:19.930 --> 00:42:19.940
zebra and it's actually really
 

00:42:19.940 --> 00:42:21.580
zebra and it's actually really
interesting here because you might

00:42:21.580 --> 00:42:21.590
interesting here because you might
 

00:42:21.590 --> 00:42:23.200
interesting here because you might
notice the obvious thing which is its

00:42:23.200 --> 00:42:23.210
notice the obvious thing which is its
 

00:42:23.210 --> 00:42:25.240
notice the obvious thing which is its
adding stripes what I actually noticed

00:42:25.240 --> 00:42:25.250
adding stripes what I actually noticed
 

00:42:25.250 --> 00:42:27.310
adding stripes what I actually noticed
here was even more interesting with it's

00:42:27.310 --> 00:42:27.320
here was even more interesting with it's
 

00:42:27.320 --> 00:42:30.220
here was even more interesting with it's
changing the color of the grass which if

00:42:30.220 --> 00:42:30.230
changing the color of the grass which if
 

00:42:30.230 --> 00:42:31.300
changing the color of the grass which if
you think about why is it doing this

00:42:31.300 --> 00:42:31.310
you think about why is it doing this
 

00:42:31.310 --> 00:42:33.460
you think about why is it doing this
zebras are typically found in drier

00:42:33.460 --> 00:42:33.470
zebras are typically found in drier
 

00:42:33.470 --> 00:42:35.740
zebras are typically found in drier
climates for example like in in Africa

00:42:35.740 --> 00:42:35.750
climates for example like in in Africa
 

00:42:35.750 --> 00:42:38.020
climates for example like in in Africa
zebras are often found and the grass in

00:42:38.020 --> 00:42:38.030
zebras are often found and the grass in
 

00:42:38.030 --> 00:42:41.170
zebras are often found and the grass in
these climates is probably not as green

00:42:41.170 --> 00:42:41.180
these climates is probably not as green
 

00:42:41.180 --> 00:42:43.780
these climates is probably not as green
as the one that this horse is it so it's

00:42:43.780 --> 00:42:43.790
as the one that this horse is it so it's
 

00:42:43.790 --> 00:42:45.520
as the one that this horse is it so it's
actually realizing not just the details

00:42:45.520 --> 00:42:45.530
actually realizing not just the details
 

00:42:45.530 --> 00:42:48.190
actually realizing not just the details
of the horse going to the zebra stripes

00:42:48.190 --> 00:42:48.200
of the horse going to the zebra stripes
 

00:42:48.200 --> 00:42:50.440
of the horse going to the zebra stripes
but also realizing the surroundings of

00:42:50.440 --> 00:42:50.450
but also realizing the surroundings of
 

00:42:50.450 --> 00:42:51.970
but also realizing the surroundings of
the horse and transferring these details

00:42:51.970 --> 00:42:51.980
the horse and transferring these details
 

00:42:51.980 --> 00:42:55.930
the horse and transferring these details
as well so finally I'll just summarize

00:42:55.930 --> 00:42:55.940
as well so finally I'll just summarize
 

00:42:55.940 --> 00:42:59.620
as well so finally I'll just summarize
this lecture and conclude we covered two

00:42:59.620 --> 00:42:59.630
this lecture and conclude we covered two
 

00:42:59.630 --> 00:43:01.300
this lecture and conclude we covered two
main techniques for generative modeling

00:43:01.300 --> 00:43:01.310
main techniques for generative modeling
 

00:43:01.310 --> 00:43:02.800
main techniques for generative modeling
focusing first on variational

00:43:02.800 --> 00:43:02.810
focusing first on variational
 

00:43:02.810 --> 00:43:05.380
focusing first on variational
autoencoders which are which we

00:43:05.380 --> 00:43:05.390
autoencoders which are which we
 

00:43:05.390 --> 00:43:08.440
autoencoders which are which we
introduce as latent variable models here

00:43:08.440 --> 00:43:08.450
introduce as latent variable models here
 

00:43:08.450 --> 00:43:10.510
introduce as latent variable models here
we try to learn a low dimensional input

00:43:10.510 --> 00:43:10.520
we try to learn a low dimensional input
 

00:43:10.520 --> 00:43:12.130
we try to learn a low dimensional input
or sorry although dimensional latent

00:43:12.130 --> 00:43:12.140
or sorry although dimensional latent
 

00:43:12.140 --> 00:43:14.140
or sorry although dimensional latent
space of our input so we can actually

00:43:14.140 --> 00:43:14.150
space of our input so we can actually
 

00:43:14.150 --> 00:43:16.570
space of our input so we can actually
get some intuition and interpretation

00:43:16.570 --> 00:43:16.580
get some intuition and interpretation
 

00:43:16.580 --> 00:43:18.300
get some intuition and interpretation
behind the underlying data distribution

00:43:18.300 --> 00:43:18.310
behind the underlying data distribution
 

00:43:18.310 --> 00:43:21.550
behind the underlying data distribution
then we extended this idea into gans

00:43:21.550 --> 00:43:21.560
then we extended this idea into gans
 

00:43:21.560 --> 00:43:22.880
then we extended this idea into gans
which is another form of

00:43:22.880 --> 00:43:22.890
which is another form of
 

00:43:22.890 --> 00:43:25.279
which is another form of
variable models but one now trained

00:43:25.279 --> 00:43:25.289
variable models but one now trained
 

00:43:25.289 --> 00:43:27.559
variable models but one now trained
between a minimax game using a generator

00:43:27.559 --> 00:43:27.569
between a minimax game using a generator
 

00:43:27.569 --> 00:43:29.960
between a minimax game using a generator
and the discriminator to complete to

00:43:29.960 --> 00:43:29.970
and the discriminator to complete to
 

00:43:29.970 --> 00:43:33.740
and the discriminator to complete to
create even more complex outputs or

00:43:33.740 --> 00:43:33.750
create even more complex outputs or
 

00:43:33.750 --> 00:43:36.880
create even more complex outputs or
generated samples from the distributions

00:43:36.880 --> 00:43:36.890
generated samples from the distributions
 

00:43:36.890 --> 00:43:38.660
generated samples from the distributions
and that's it

00:43:38.660 --> 00:43:38.670
and that's it
 

00:43:38.670 --> 00:43:39.990
and that's it
thank you

00:43:39.990 --> 00:43:40.000
thank you
 

00:43:40.000 --> 00:43:45.550
thank you
[Applause]

