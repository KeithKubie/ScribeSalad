WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.449
[INAUDIBLE]

00:00:08.710 --> 00:00:11.292
CENNYDD BOWLES: And we have
a couple of folks coming up.

00:00:11.292 --> 00:00:12.224
We have Karen?

00:00:12.224 --> 00:00:13.140
KAREN KAUSHANSKY: Yes.

00:00:13.140 --> 00:00:15.650
CENNYDD BOWLES: OK, fantastic--

00:00:15.650 --> 00:00:16.790
and Josh?

00:00:22.635 --> 00:00:23.260
Here, he comes.

00:00:23.260 --> 00:00:24.747
OK, excellent.

00:00:31.710 --> 00:00:35.140
So Kenneth, we just heard
from, so I won't introduce him.

00:00:35.140 --> 00:00:38.950
Karen, I will--
oh, Sarah, sorry.

00:00:38.950 --> 00:00:39.520
Yes.

00:00:39.520 --> 00:00:41.800
No, please, sorry.

00:00:41.800 --> 00:00:42.665
No, no, no.

00:00:42.665 --> 00:00:44.540
Sorry, I almost let
you off the hook.

00:00:47.600 --> 00:00:52.910
So Sarah and Kenneth, you've
just heard from today,

00:00:52.910 --> 00:00:54.740
so I won't introduce
them, necessarily.

00:00:54.740 --> 00:00:58.820
But Karen, I will attempt
an introduction of--

00:00:58.820 --> 00:01:03.600
a local I believe
[? Basel-- ?] and many,

00:01:03.600 --> 00:01:07.070
many years history working
in some of these making some

00:01:07.070 --> 00:01:10.430
of the speculative futures--
real wearables, robotics,

00:01:10.430 --> 00:01:11.440
voice interactions--

00:01:11.440 --> 00:01:13.481
KAREN KAUSHANSKY: It's
interactions starting back

00:01:13.481 --> 00:01:17.270
in 1996, so it's been
a long, long road.

00:01:17.270 --> 00:01:21.830
This is the year of speech, I've
heard that about seven times

00:01:21.830 --> 00:01:23.696
over the past 20 years.

00:01:23.696 --> 00:01:25.070
CENNYDD BOWLES:
And Josh Lovejoy,

00:01:25.070 --> 00:01:27.410
a colleague of mine
from Google, has

00:01:27.410 --> 00:01:31.250
worked on many of
the projects that

00:01:31.250 --> 00:01:33.500
were showed by
Jeremiah very early

00:01:33.500 --> 00:01:35.000
on this morning,
particularly Google

00:01:35.000 --> 00:01:39.779
Clips, the small robotic camera,
which I'm sure you might talk

00:01:39.779 --> 00:01:42.320
about a little bit in reference
to what you're talking about.

00:01:42.320 --> 00:01:45.740
We're now working a lot
on the PAIR initiative

00:01:45.740 --> 00:01:50.990
and issues of bias and fairness
in learning systems and machine

00:01:50.990 --> 00:01:53.060
learning systems.

00:01:53.060 --> 00:01:55.570
Would you like to say anything
more to introduce yourself?

00:01:55.570 --> 00:01:57.230
OK.

00:01:57.230 --> 00:02:02.180
So first of all, I wanted
to kind of throw it

00:02:02.180 --> 00:02:05.720
to Karen and Josh, a little
bit unfairly perhaps--

00:02:05.720 --> 00:02:07.950
but as people who have
not been speaking today,

00:02:07.950 --> 00:02:09.825
but as people who have
been in the audience--

00:02:09.825 --> 00:02:11.740
maybe just two minutes
each on threads

00:02:11.740 --> 00:02:14.530
that you've sort of seen
emerge through the day.

00:02:14.530 --> 00:02:16.250
I've notes of my
own, but I'd love

00:02:16.250 --> 00:02:19.100
to hear what your reflections
are on some of the talks

00:02:19.100 --> 00:02:22.310
you've heard today and what
was sort of resonated with you.

00:02:26.231 --> 00:02:28.230
KAREN KAUSHANSKY: Oh yeah,
thanks for having me.

00:02:28.230 --> 00:02:32.127
And I always love
coming to these.

00:02:32.127 --> 00:02:33.710
You just pick up
these nuggets, so you

00:02:33.710 --> 00:02:36.350
don't expect to be eye opening.

00:02:36.350 --> 00:02:40.730
And so, I think that a lot
of the thread, that even Alex

00:02:40.730 --> 00:02:43.640
kind of summed up, is a lot of
the design is in the writing,

00:02:43.640 --> 00:02:47.620
and we saw that with
Simone, and we saw that--

00:02:47.620 --> 00:02:50.180
but if it was just so
easy to take a black box

00:02:50.180 --> 00:02:55.340
and explain how things got into
the box, our job would be done.

00:02:55.340 --> 00:02:57.260
It's not that easy,
because once you

00:02:57.260 --> 00:03:03.920
start describing or explaining
with only a confidence of 43%,

00:03:03.920 --> 00:03:06.770
then somebody looks at that
and says, well, that's shit.

00:03:06.770 --> 00:03:11.340
Like, why am I going to use
that if it's 43% confident?

00:03:11.340 --> 00:03:14.430
And so, how do we--

00:03:14.430 --> 00:03:16.020
so two big things.

00:03:16.020 --> 00:03:20.180
One is how do we get
people to play along

00:03:20.180 --> 00:03:22.124
when the confidence
is low, and we're

00:03:22.124 --> 00:03:23.290
talking about this at lunch.

00:03:23.290 --> 00:03:24.950
Maybe it's like the eye
doctor a little bit.

00:03:24.950 --> 00:03:27.283
Like, is it better like this,
or is it better like this?

00:03:27.283 --> 00:03:31.060
Like, how do we get people to
play along and participate?

00:03:31.060 --> 00:03:33.930
And I think a big thing
that I haven't heard today,

00:03:33.930 --> 00:03:37.100
which is always what I-- a lot
of what I try to design for is,

00:03:37.100 --> 00:03:39.620
like, how do we know
what we don't know,

00:03:39.620 --> 00:03:43.640
and how do we explain
that in a way--

00:03:43.640 --> 00:03:45.860
of the things that
we don't know--

00:03:45.860 --> 00:03:47.176
it's like the why and why not.

00:03:47.176 --> 00:03:49.550
Like, this is what I know,
but this is what I don't know.

00:03:49.550 --> 00:03:51.480
And how do we design for that?

00:03:54.420 --> 00:03:57.690
JOSH LOVEJOY: Yeah, a
lot of [INAUDIBLE] voice

00:03:57.690 --> 00:03:59.670
is the future.

00:03:59.670 --> 00:04:01.560
A lot of threads--

00:04:01.560 --> 00:04:03.640
a couple of things
that spring to mind.

00:04:03.640 --> 00:04:05.820
One of them is just
the fundamental need

00:04:05.820 --> 00:04:09.810
for more just
dialectical discourse.

00:04:09.810 --> 00:04:13.920
So much today was about
inspecting, and turning over,

00:04:13.920 --> 00:04:17.820
and turning over again from
different perspectives.

00:04:17.820 --> 00:04:20.709
And so, I just think
one of the aspects

00:04:20.709 --> 00:04:24.880
that I really believe deeply
in, as a designer in this space,

00:04:24.880 --> 00:04:28.420
is the need to kind
of take a page out

00:04:28.420 --> 00:04:32.170
of the, kind of, social
systems thinking and especially

00:04:32.170 --> 00:04:33.550
perspective taking.

00:04:33.550 --> 00:04:36.580
The process of coming to
recognize the privilege

00:04:36.580 --> 00:04:40.552
that one has in any
environment requires

00:04:40.552 --> 00:04:42.010
stepping out of
your own experience

00:04:42.010 --> 00:04:44.320
and stepping into the
lived experience of others.

00:04:44.320 --> 00:04:46.450
Oftentimes, there's just
this huge disconnect

00:04:46.450 --> 00:04:48.430
between the work that
we do as technologists

00:04:48.430 --> 00:04:51.400
and the actual impact directly
on the human beings who

00:04:51.400 --> 00:04:53.670
are affected by
it, or consume it,

00:04:53.670 --> 00:04:56.410
and, in every instance, the ones
who train it which is, like,

00:04:56.410 --> 00:04:59.470
totally anonymised people.

00:04:59.470 --> 00:05:04.150
And sort of the way the other
thread that I've seen today

00:05:04.150 --> 00:05:08.560
is a lot about the word
perfect or accurate.

00:05:08.560 --> 00:05:11.770
I bristle a little bit when I
hear perfection or precision

00:05:11.770 --> 00:05:14.350
talked about, because
there's almost

00:05:14.350 --> 00:05:19.120
this underlying assumption
that future states can always

00:05:19.120 --> 00:05:21.670
be predicted based
on past states,

00:05:21.670 --> 00:05:25.030
and the reality that everyone,
I think, is pretty well aware of

00:05:25.030 --> 00:05:29.260
is that most often the past was
not representative of the just,

00:05:29.260 --> 00:05:32.470
you know, egalitarian
future that we want.

00:05:32.470 --> 00:05:35.800
So because machine learning
can't predict a future state

00:05:35.800 --> 00:05:38.680
that has no
precedent, how do you

00:05:38.680 --> 00:05:40.930
build a prediction
model that actually

00:05:40.930 --> 00:05:45.236
accounts for that complete lack
of the asymptotic sort of goal?

00:05:45.236 --> 00:05:47.110
So it's like unless
there is something that--

00:05:47.110 --> 00:05:49.540
like a state-- a
past historic state

00:05:49.540 --> 00:05:52.690
that you're comfortable
with persisting forever,

00:05:52.690 --> 00:05:55.990
then there's no such thing
as perfect or accurate--

00:05:55.990 --> 00:05:58.546
so just like things to question.

00:05:58.546 --> 00:06:02.690
CENNYDD BOWLES: So the
title of our panel,

00:06:02.690 --> 00:06:05.370
if I can pronounce it, is
Opportunities and Challenges

00:06:05.370 --> 00:06:06.960
for the UX of AI.

00:06:06.960 --> 00:06:10.980
And one challenge I think-- a
thread that's come up for me

00:06:10.980 --> 00:06:15.090
through the day and Kenneth
sort of put a nice hat on it

00:06:15.090 --> 00:06:18.030
at the end-- is kind of the
challenge of people who are

00:06:18.030 --> 00:06:19.975
involved in--

00:06:19.975 --> 00:06:22.350
and the lady who just asked
a question as well, I think--

00:06:22.350 --> 00:06:23.266
kind of talked about--

00:06:23.266 --> 00:06:25.710
a little bit is the challenge,
for people making product

00:06:25.710 --> 00:06:31.200
decisions, to
imagine or concretize

00:06:31.200 --> 00:06:35.010
those decisions ahead of time
at the appropriate scales.

00:06:35.010 --> 00:06:37.670
And I think one of the--
actually, when last year I

00:06:37.670 --> 00:06:39.840
was originally talking
about this event,

00:06:39.840 --> 00:06:41.940
we were talking about
scale and talking

00:06:41.940 --> 00:06:44.460
about the different scales
that these things happen at

00:06:44.460 --> 00:06:47.940
and the scale of the hypernudge
system that Kenneth was talking

00:06:47.940 --> 00:06:50.850
about is something,
which, you know,

00:06:50.850 --> 00:06:54.330
we can all recognize
that we're in now,

00:06:54.330 --> 00:06:56.970
but we can't sort of
see the edges of it.

00:06:56.970 --> 00:07:00.200
We can't sort of concretize
it, and I sort of--

00:07:00.200 --> 00:07:02.620
I wanted each one of
you to reflect perhaps

00:07:02.620 --> 00:07:06.810
on that thread of how do
designers working in concert

00:07:06.810 --> 00:07:12.435
with engineers, and product
managers, et cetera start to--

00:07:12.435 --> 00:07:15.120
helpfully, and then
time scales that

00:07:15.120 --> 00:07:18.600
work in terms of
making products start

00:07:18.600 --> 00:07:22.020
to sketch these scales
and these scales

00:07:22.020 --> 00:07:24.480
of these dynamic
systems for each other,

00:07:24.480 --> 00:07:28.430
as well as the eventual--

00:07:28.430 --> 00:07:29.980
I don't like using
the term users--

00:07:29.980 --> 00:07:33.530
but the humans involved
in each of these places

00:07:33.530 --> 00:07:35.480
in these systems.

00:07:35.480 --> 00:07:39.812
Let's come back to
Sarah on this one.

00:07:39.812 --> 00:07:42.490
SARAH GOLD: So I can talk
to you specifically what

00:07:42.490 --> 00:07:47.500
we do at F, as we went away
for retreat week the other week

00:07:47.500 --> 00:07:50.490
to discuss what other
kinds of design we do.

00:07:50.490 --> 00:07:52.370
Can we give them names?

00:07:52.370 --> 00:07:54.910
And one of the things
we came up with was

00:07:54.910 --> 00:07:58.600
service diagrams, which
might mean something very

00:07:58.600 --> 00:08:00.490
specific to you, but
what we talk about

00:08:00.490 --> 00:08:02.830
is when we are
designing anything,

00:08:02.830 --> 00:08:07.750
we try to think about
how that service will

00:08:07.750 --> 00:08:09.430
fit in a wider system.

00:08:09.430 --> 00:08:12.940
So what else
happens around that.

00:08:12.940 --> 00:08:16.390
An example might be if you're
designing, say, a new ticket

00:08:16.390 --> 00:08:20.629
system for TFL, you don't
have to draw sort of TFL.

00:08:20.629 --> 00:08:22.420
And then outside of
that, what does the law

00:08:22.420 --> 00:08:23.800
say about ticketing?

00:08:23.800 --> 00:08:28.890
And then you might have to draw
maybe someone getting cash out

00:08:28.890 --> 00:08:30.730
of their local ATM.

00:08:30.730 --> 00:08:33.289
Or it all depends on what the
thing is you're trying to do.

00:08:33.289 --> 00:08:37.510
But it's about drawing all the
other entities around that,

00:08:37.510 --> 00:08:40.780
and especially thinking
about regulation,

00:08:40.780 --> 00:08:45.185
thinking about also adversarial
access in that system too

00:08:45.185 --> 00:08:47.030
and what could go wrong.

00:08:47.030 --> 00:08:51.040
And I think for us, it's a
really helpful way of thinking

00:08:51.040 --> 00:08:52.660
about different
scales, because it

00:08:52.660 --> 00:08:55.930
forces some of the
discussions out from something

00:08:55.930 --> 00:08:58.660
that might be a very small
interaction that you're

00:08:58.660 --> 00:09:02.140
discovering into something
that speaks much more

00:09:02.140 --> 00:09:06.670
broadly about what
that decision means

00:09:06.670 --> 00:09:09.940
for other individuals and
other institutions that

00:09:09.940 --> 00:09:12.847
are a part of that service
in one way or another.

00:09:12.847 --> 00:09:14.680
So that's certainly a
technique that we use.

00:09:14.680 --> 00:09:16.600
And I think it's
quite an easy one.

00:09:16.600 --> 00:09:18.100
You should give it a go.

00:09:23.382 --> 00:09:25.490
JOSH LOVEJOY: A
couple of things, one,

00:09:25.490 --> 00:09:28.130
I try to say that I
think the burden of proof

00:09:28.130 --> 00:09:32.180
for whether or not something
should be automatable at all is

00:09:32.180 --> 00:09:35.840
on the level of
agreement between people

00:09:35.840 --> 00:09:39.560
about what good looks like, and
how robust that agreement is

00:09:39.560 --> 00:09:42.140
to the diversity of
the people involved

00:09:42.140 --> 00:09:44.360
and diversity of
the context of use.

00:09:44.360 --> 00:09:46.880
So it's just flipping
the default assumption

00:09:46.880 --> 00:09:51.820
that automation is good to
automation needs to be proven.

00:09:51.820 --> 00:09:54.320
The second-- and this
is one that I recently

00:09:54.320 --> 00:09:58.970
read a definition of
disability by the World Health

00:09:58.970 --> 00:10:01.580
Organization, which
describes it as being

00:10:01.580 --> 00:10:04.820
a mismatch between the
features of an individual

00:10:04.820 --> 00:10:07.460
and the features
of an environment.

00:10:07.460 --> 00:10:09.680
And I think talking about
that, we're all disabled.

00:10:09.680 --> 00:10:11.410
We all have disabilities.

00:10:11.410 --> 00:10:14.450
There are some more
persistent or more transient.

00:10:14.450 --> 00:10:17.940
But that is a starting space
for innovation thinking.

00:10:17.940 --> 00:10:19.490
I think, is just
really profound.

00:10:19.490 --> 00:10:22.190
Very frequently, we begin
the innovation discussion

00:10:22.190 --> 00:10:26.720
around the perfect alignments
between people and environment.

00:10:26.720 --> 00:10:28.010
And we're like, oh sweet.

00:10:28.010 --> 00:10:30.620
We can send them a notification.

00:10:30.620 --> 00:10:33.830
And so I just look at it as
a provocation to the room.

00:10:33.830 --> 00:10:36.260
I think that that's a
really concrete place

00:10:36.260 --> 00:10:40.880
that I like to nudge
teams at Google towards.

00:10:40.880 --> 00:10:43.820
And then the third is just
anything around this notion

00:10:43.820 --> 00:10:45.980
of augmentation.

00:10:45.980 --> 00:10:48.830
The most frequent question I
get asked by teams at Google

00:10:48.830 --> 00:10:51.230
is why doesn't anybody
trust the recommendations

00:10:51.230 --> 00:10:53.550
that my system is building?

00:10:53.550 --> 00:10:55.812
Whether it's from
here's a fitness program

00:10:55.812 --> 00:10:58.020
you should go on, or here's
a thing you should go do,

00:10:58.020 --> 00:11:01.700
or a flight you should take or
whatever, and every one of them

00:11:01.700 --> 00:11:04.940
starts with again,
this assumption

00:11:04.940 --> 00:11:07.850
that there's a great
answer that can be vended.

00:11:07.850 --> 00:11:09.740
And so there's a
little bit of a we'll

00:11:09.740 --> 00:11:15.530
go back and walk in the
shoes of the individual who,

00:11:15.530 --> 00:11:19.340
prior to this point, has
been making these decisions

00:11:19.340 --> 00:11:21.410
confidently for themselves.

00:11:21.410 --> 00:11:23.190
They're accountable
to themselves.

00:11:23.190 --> 00:11:26.730
And there's a sense of pride
and craft and authorship.

00:11:26.730 --> 00:11:28.550
So in that spectrum
of what it means

00:11:28.550 --> 00:11:31.360
to feel like you've
made something,

00:11:31.360 --> 00:11:33.680
there's a big space for--

00:11:33.680 --> 00:11:37.140
you can modify something and
still feel like you made it.

00:11:37.140 --> 00:11:38.610
You can pick out
from a selection

00:11:38.610 --> 00:11:40.980
and still feel like you made it.

00:11:40.980 --> 00:11:43.170
You can actually
hand craft things.

00:11:43.170 --> 00:11:45.260
But again, we tend to
jump into this spot

00:11:45.260 --> 00:11:46.969
where you can automate
the perfect thing.

00:11:46.969 --> 00:11:48.968
And at that point, you've
taken away authorship.

00:11:48.968 --> 00:11:50.040
You've taken away craft.

00:11:50.040 --> 00:11:51.925
You've taken away the
personal expressions.

00:11:51.925 --> 00:11:54.782
So how do you start in that
bigger space of a place

00:11:54.782 --> 00:11:56.490
where somebody feels
like they can attach

00:11:56.490 --> 00:11:57.690
their own personal touch?

00:11:57.690 --> 00:12:00.390
AUDIENCE: That seems to resonate
with what Kenneth was saying

00:12:00.390 --> 00:12:04.260
about the veil of ignorance
that John rules conception,

00:12:04.260 --> 00:12:09.330
being able to place
yourself in an actor network

00:12:09.330 --> 00:12:14.379
rather than being the designer
of a user centered design.

00:12:14.379 --> 00:12:16.920
And I think one of the threads
that I was picking up on today

00:12:16.920 --> 00:12:22.350
is do we come here to bury user
centered design rather than

00:12:22.350 --> 00:12:24.630
praise it?

00:12:24.630 --> 00:12:29.430
Is one of the opportunities
and challenges of the UX of AI

00:12:29.430 --> 00:12:32.040
is that there's no
U there any more?

00:12:32.040 --> 00:12:33.100
There shouldn't be.

00:12:33.100 --> 00:12:37.350
And what I've heard
through the presentations--

00:12:37.350 --> 00:12:41.845
this kind of thread of
thinking differently about--

00:12:41.845 --> 00:12:45.110
or ditching the idea of a user.

00:12:45.110 --> 00:12:46.820
I love to hear you
thoughts on that.

00:12:46.820 --> 00:12:49.990
CENNYDD BOWLES: I'd
like to chip in on that.

00:12:49.990 --> 00:12:52.290
Thomas Whent is a
designer in New York City.

00:12:52.290 --> 00:12:55.567
He gave a great presentation
at Interaction '17.

00:12:55.567 --> 00:12:57.900
I think it was called The
Critique of User Sense Design.

00:12:57.900 --> 00:12:59.820
And he pointed out
a couple of things,

00:12:59.820 --> 00:13:01.620
but one I'll focus on here.

00:13:01.620 --> 00:13:05.040
UCD has, for better
or for worse,

00:13:05.040 --> 00:13:07.110
often for worse,
focused relentlessly

00:13:07.110 --> 00:13:09.840
on individual success and
individual productivity.

00:13:09.840 --> 00:13:12.600
And Whent pointed
out that it hasn't

00:13:12.600 --> 00:13:15.840
taken into account the
well-being of ecologies,

00:13:15.840 --> 00:13:19.039
communities, democracies,
societies, et cetera.

00:13:19.039 --> 00:13:20.580
So I think that's
certainly something

00:13:20.580 --> 00:13:26.340
we've lost sight of
with that UCD focus.

00:13:26.340 --> 00:13:28.800
UCD, for me, runs
into some challenges

00:13:28.800 --> 00:13:33.329
when we are talking about
devices that live with us.

00:13:33.329 --> 00:13:35.370
Because the question then,
as you point out Matt,

00:13:35.370 --> 00:13:37.536
isn't really about how do
we use these technologies,

00:13:37.536 --> 00:13:39.700
but how do we cohabit with them?

00:13:39.700 --> 00:13:43.934
How do we share an environment
with them healthily?

00:13:43.934 --> 00:13:45.350
And also, if I
may, I'm just going

00:13:45.350 --> 00:13:46.766
to rewind to the
previous question

00:13:46.766 --> 00:13:50.980
as well, because I have a
little thought about scale.

00:13:50.980 --> 00:13:55.710
For me, scale is less
useful to have processes.

00:13:55.710 --> 00:13:58.960
Processes, I think, don't
scale as well as cultures.

00:13:58.960 --> 00:14:02.790
So I'm more interested
in bringing humans

00:14:02.790 --> 00:14:05.369
at the heart of this work
through cultural work.

00:14:05.369 --> 00:14:07.410
And I'm also interested
in looking at the default

00:14:07.410 --> 00:14:08.790
ideologies of the
tech community,

00:14:08.790 --> 00:14:11.700
because that really is what
sets our culture, which in turn,

00:14:11.700 --> 00:14:13.380
begets process, et cetera.

00:14:13.380 --> 00:14:16.620
And I want to have
a minor rant about

00:14:16.620 --> 00:14:18.330
the predominant
ideologies in tech

00:14:18.330 --> 00:14:20.620
right now, which are lean
startup and the Californian

00:14:20.620 --> 00:14:21.120
ideology.

00:14:21.120 --> 00:14:23.730
A lean startup is
a system that's

00:14:23.730 --> 00:14:25.740
predicated upon empiricism.

00:14:25.740 --> 00:14:31.420
The fact is offered that it's
futile to predict the future,

00:14:31.420 --> 00:14:34.977
therefore we should be learning
exclusively through empiricism,

00:14:34.977 --> 00:14:36.810
build measure, learn,
build, measure, learn.

00:14:36.810 --> 00:14:40.290
There's no space for ethical
discourse within that system.

00:14:40.290 --> 00:14:41.730
There's no space
for anticipation

00:14:41.730 --> 00:14:43.650
of potential harm
within that system

00:14:43.650 --> 00:14:46.650
until it's done, which I think
is something that has led us

00:14:46.650 --> 00:14:48.690
down the wrong path, ethically.

00:14:48.690 --> 00:14:49.965
And the California ideology--

00:14:49.965 --> 00:14:52.230
I recognized I am in
Switzerland, but at Google

00:14:52.230 --> 00:14:55.020
so there's an interesting
conflict there.

00:14:55.020 --> 00:14:57.210
If you haven't read the
initial paper from '95,

00:14:57.210 --> 00:15:00.015
it's Andy Cameron
and Richard Barbrook,

00:15:00.015 --> 00:15:03.690
a kind of slightly weird
but excellent piece

00:15:03.690 --> 00:15:10.440
about essentially the political
agendas of Silicon Valley.

00:15:10.440 --> 00:15:11.850
And it's still the case.

00:15:11.850 --> 00:15:13.539
And without getting
overly political,

00:15:13.539 --> 00:15:15.330
I don't think it's
necessarily a model that

00:15:15.330 --> 00:15:18.414
will help us become
more ethical and create

00:15:18.414 --> 00:15:19.830
better human-centered
technologies

00:15:19.830 --> 00:15:20.900
in the next decade.

00:15:20.900 --> 00:15:24.330
So when I hear London,
Berlin, et cetera,

00:15:24.330 --> 00:15:27.030
saying how do we capture some
of that Silicon Valley spirit,

00:15:27.030 --> 00:15:28.455
my response is always don't.

00:15:28.455 --> 00:15:29.670
We don't need more of it.

00:15:29.670 --> 00:15:30.930
We need something more local.

00:15:30.930 --> 00:15:33.086
We need something different
to change the pattern,

00:15:33.086 --> 00:15:34.710
to change the trajectory
that we're on.

00:15:40.470 --> 00:15:42.880
KAREN KAUSHANSKY: To
the scale question,

00:15:42.880 --> 00:15:47.770
I think as a designer who's
working on a day to day basis,

00:15:47.770 --> 00:15:49.750
before we get to
scale, let's just

00:15:49.750 --> 00:15:51.910
kind of make better
what we have today.

00:15:51.910 --> 00:15:54.580
And some of that is I
think, identifying the seams

00:15:54.580 --> 00:15:57.760
so we can make beautiful seams.

00:15:57.760 --> 00:15:59.740
And we can show those seams.

00:15:59.740 --> 00:16:02.710
And so it's a lot
about identifying

00:16:02.710 --> 00:16:05.230
every step of the way,
kind of what can go wrong.

00:16:05.230 --> 00:16:08.579
And that can be pretty tedious.

00:16:08.579 --> 00:16:10.412
I think it goes back
to what Sarah, what you

00:16:10.412 --> 00:16:14.470
were saying about the service
design, and just identification

00:16:14.470 --> 00:16:16.990
of all the different flows
and all the different ways

00:16:16.990 --> 00:16:19.630
that things can go.

00:16:19.630 --> 00:16:22.630
Chris Nestle, who wrote
"Identative Design"

00:16:22.630 --> 00:16:26.530
and "Make It So," he walked
us through this future's wheel

00:16:26.530 --> 00:16:30.520
exercise, which has you put
down kind of a prediction.

00:16:30.520 --> 00:16:32.650
And then from that
prediction, you

00:16:32.650 --> 00:16:35.320
write out direct consequences
of that prediction.

00:16:35.320 --> 00:16:39.460
And then for each
consequence, you

00:16:39.460 --> 00:16:42.280
write other direct consequences
of the consequences.

00:16:42.280 --> 00:16:44.590
And then you link up
indirect consequences.

00:16:44.590 --> 00:16:46.360
And you're going
to find the seams.

00:16:46.360 --> 00:16:48.310
What happens when this
indirect consequence

00:16:48.310 --> 00:16:51.024
and this indirect
consequence happens together?

00:16:51.024 --> 00:16:51.940
And you're like, whoa.

00:16:51.940 --> 00:16:54.790
That's kind of maybe where
the future is going to go

00:16:54.790 --> 00:16:56.480
and we didn't even realize it.

00:16:56.480 --> 00:16:58.796
So that's been a helpful tool.

00:16:58.796 --> 00:17:03.270
CENNYDD BOWLES: So can I bring
you back to burying the U in UX

00:17:03.270 --> 00:17:05.099
and your opinions of that?

00:17:05.099 --> 00:17:08.589
Just means I'll be
in a job correcting

00:17:08.589 --> 00:17:11.069
of stationary and
lanyards and slide decks,

00:17:11.069 --> 00:17:12.310
if we get to cross the U off.

00:17:15.579 --> 00:17:17.740
SARAH GOLD: I don't know
what to think about that.

00:17:17.740 --> 00:17:20.780
I know that I'm
designing for somebody.

00:17:20.780 --> 00:17:22.630
I'm designing for a need.

00:17:22.630 --> 00:17:24.859
I'm designing something
that I want somebody

00:17:24.859 --> 00:17:27.290
to use that hopefully
in some cases

00:17:27.290 --> 00:17:28.830
will make their lives better.

00:17:28.830 --> 00:17:32.722
I was hoping to design
autonomous vehicles.

00:17:32.722 --> 00:17:34.430
So everything that
Dan was talking about,

00:17:34.430 --> 00:17:36.380
bigger cities, better cities.

00:17:39.920 --> 00:17:41.960
My user is the reason
I'm doing this so.

00:17:41.960 --> 00:17:45.800
I have a hard time
saying we've given up.

00:17:45.800 --> 00:17:51.400
It should maybe be expanded,
cities, democracies,

00:17:51.400 --> 00:17:53.110
political agendas,
things like that.

00:17:53.110 --> 00:17:55.810
But I can't let go of my user.

00:17:59.275 --> 00:18:02.950
JOSH LOVEJOY: Yeah,
I would agree.

00:18:02.950 --> 00:18:05.620
When I hear arguments
against user-centered design,

00:18:05.620 --> 00:18:07.060
often what I feel
like I'm hearing

00:18:07.060 --> 00:18:12.220
are arguments against the
current optimization strategies

00:18:12.220 --> 00:18:17.050
that are way overblown
in UCD, particularly

00:18:17.050 --> 00:18:20.380
things like engagement through
clicks, or time on task,

00:18:20.380 --> 00:18:25.090
or conversion, and
things like that.

00:18:25.090 --> 00:18:30.910
And to go back to
your point, Kenneth,

00:18:30.910 --> 00:18:33.850
I think the prevailing
conventional wisdom

00:18:33.850 --> 00:18:35.920
is get to market,
and then all else

00:18:35.920 --> 00:18:38.320
will follow kind of a thing.

00:18:38.320 --> 00:18:43.150
And for the larger
companies, it's

00:18:43.150 --> 00:18:48.250
our responsibility to actually
think in total polar opposite.

00:18:48.250 --> 00:18:50.080
You can't actually
just say we're

00:18:50.080 --> 00:18:51.730
going to push
something out there,

00:18:51.730 --> 00:18:54.430
and then hopefully
more people come to it.

00:18:54.430 --> 00:18:56.687
And then we'll fix the
problems as they arise.

00:18:56.687 --> 00:18:59.020
Rather you have to actually
kind of do the due diligence

00:18:59.020 --> 00:19:02.284
upfront, and think about a
broader spectrum of users.

00:19:02.284 --> 00:19:04.450
The other aspect from a
machine learning perspective

00:19:04.450 --> 00:19:09.580
is if you actually want to
build for a robust system that's

00:19:09.580 --> 00:19:15.220
going to be iterative
and useful over time,

00:19:15.220 --> 00:19:18.950
it's incumbent on you to look
at a really broad spectrum

00:19:18.950 --> 00:19:21.830
and an intersectional
spectrum of user behavior.

00:19:21.830 --> 00:19:25.450
So you have to think about
it from through to the end

00:19:25.450 --> 00:19:29.350
application of technology,
therefore have a user.

00:19:29.350 --> 00:19:32.560
It's just our traditional
user has kind of played out.

00:19:32.560 --> 00:19:35.140
And it's not going to be useful
in modeling the effectiveness

00:19:35.140 --> 00:19:35.839
of the system.

00:19:39.671 --> 00:19:44.780
SARAH GOLD: So on
user-centered design,

00:19:44.780 --> 00:19:47.770
I think that the concept of
an individual that you're

00:19:47.770 --> 00:19:50.740
designing for is still
incredibly important.

00:19:50.740 --> 00:19:54.820
And I have lots of views on how
and why we should also think

00:19:54.820 --> 00:19:56.570
about large groups of people.

00:19:56.570 --> 00:19:59.650
But something very
specific about AI

00:19:59.650 --> 00:20:01.390
and user-centered
design is as soon

00:20:01.390 --> 00:20:04.669
as you bring data
into your design,

00:20:04.669 --> 00:20:06.460
which I think is really
important actually,

00:20:06.460 --> 00:20:09.400
is how you think about scale is
also thinking about data flows

00:20:09.400 --> 00:20:13.150
as well, as you think about
a kind of user journey.

00:20:13.150 --> 00:20:15.350
What's the data that's
being collected?

00:20:15.350 --> 00:20:16.850
How has it been collected?

00:20:16.850 --> 00:20:19.650
This idea of where ownership
of that data might be.

00:20:19.650 --> 00:20:21.150
But really, who has
rights to see it

00:20:21.150 --> 00:20:25.075
is really important, because--

00:20:25.075 --> 00:20:27.200
I'm trying to think of a
better way to describe it.

00:20:27.200 --> 00:20:28.630
I'm just going to
go for it, which

00:20:28.630 --> 00:20:31.960
is that if we continue to
design for an individual,

00:20:31.960 --> 00:20:35.170
but the data that we are
fundamentally asking them

00:20:35.170 --> 00:20:37.930
to give consent
for or to look at

00:20:37.930 --> 00:20:44.860
is many people have rights over
it, we can't design for it.

00:20:44.860 --> 00:20:48.230
So I think the
concept of a user need

00:20:48.230 --> 00:20:50.880
breaks down when you
realize the material stuff

00:20:50.880 --> 00:20:53.760
we're asking them or giving
them permission to do something

00:20:53.760 --> 00:20:56.070
with isn't owned by them.

00:20:56.070 --> 00:20:59.850
At the moment we have this
really strong story about I

00:20:59.850 --> 00:21:01.390
own my data.

00:21:01.390 --> 00:21:04.740
Well actually, there's not much
data that I do own, in fact.

00:21:04.740 --> 00:21:06.420
And I would encourage
you all to look

00:21:06.420 --> 00:21:10.980
at the Open Data Institute's
definition of kind of ownership

00:21:10.980 --> 00:21:14.010
on this, because in
fact, data about me

00:21:14.010 --> 00:21:17.970
is also data about my
sister or my mom or my dad.

00:21:17.970 --> 00:21:20.700
Data about me in this room
is actually about all of us.

00:21:20.700 --> 00:21:27.066
So there really is that kind of
collective right that we have.

00:21:27.066 --> 00:21:28.440
And it's only
something I've just

00:21:28.440 --> 00:21:31.110
thought about here, which is
that the user need is a very

00:21:31.110 --> 00:21:34.170
difficult concept for me to
start thinking about broader

00:21:34.170 --> 00:21:38.850
systems, because the material
stuff we're working with isn't

00:21:38.850 --> 00:21:40.690
kind of owned by an individual.

00:21:40.690 --> 00:21:43.160
We have to think about this
from the perspective of many.

00:21:43.160 --> 00:21:44.910
AUDIENCE: I guess
coming back to something

00:21:44.910 --> 00:21:49.830
that Josh and Fernando had
talking about as [INAUDIBLE]

00:21:49.830 --> 00:21:52.520
your sponsor.

00:21:52.520 --> 00:22:00.360
Is that kind of making a human
centered AI the easiest option?

00:22:00.360 --> 00:22:04.930
And some of that goes to the
practitioners of this as well.

00:22:04.930 --> 00:22:09.030
And so I think my call,
I guess, is can we

00:22:09.030 --> 00:22:12.180
make it easier to
incorporate in the day

00:22:12.180 --> 00:22:17.610
to day some of these multi-actor
networked collective ways

00:22:17.610 --> 00:22:21.220
of looking at ways in
which people are engaging

00:22:21.220 --> 00:22:24.500
with technology,
make that as easier

00:22:24.500 --> 00:22:30.970
as it's become through
various scenes of training

00:22:30.970 --> 00:22:33.280
and technology and
conference talks and tools

00:22:33.280 --> 00:22:35.740
and all the rest of it, that
we've had over the last 15

00:22:35.740 --> 00:22:39.910
years, as we've made or
as rigid as we've made

00:22:39.910 --> 00:22:41.470
user-center design in a way?

00:22:41.470 --> 00:22:44.140
Is there a way to
build our way out

00:22:44.140 --> 00:22:48.340
of this in ways that give
very busy people the tools

00:22:48.340 --> 00:22:50.782
to do so easily?

00:22:50.782 --> 00:22:51.615
CENNYDD BOWLES: Yes.

00:22:54.260 --> 00:22:57.540
And people will make a good
living over the next 5 to 10

00:22:57.540 --> 00:23:00.260
years writing books about
those, giving talks about it,

00:23:00.260 --> 00:23:02.830
doing training and saying,
here are some simple techniques

00:23:02.830 --> 00:23:05.920
that you can use to increase the
view of stakeholders you have,

00:23:05.920 --> 00:23:07.920
if you like broadening
the views of stakeholders

00:23:07.920 --> 00:23:12.840
to include societies and
populations and so on.

00:23:12.840 --> 00:23:15.090
Is that what we
actually want though?

00:23:15.090 --> 00:23:16.090
It may be what we want.

00:23:16.090 --> 00:23:17.820
Is that what we actually need?

00:23:17.820 --> 00:23:22.020
Do we want to make these
techniques low in friction,

00:23:22.020 --> 00:23:24.630
or do we want to
actually do them justice?

00:23:24.630 --> 00:23:26.540
Maybe we need to push
back from the idea

00:23:26.540 --> 00:23:29.580
that it has to be something
that subservient to the need

00:23:29.580 --> 00:23:31.020
to deliver quickly.

00:23:31.020 --> 00:23:32.610
Because maybe
that's the problem.

00:23:32.610 --> 00:23:35.970
Maybe it's we find ourselves
in this mess because we

00:23:35.970 --> 00:23:41.340
felt this deterministic pace of
change, this manifest destiny

00:23:41.340 --> 00:23:44.522
that technology has to
go out there tomorrow.

00:23:44.522 --> 00:23:46.980
Maybe that's what's got us in
this mess in the first place.

00:23:46.980 --> 00:23:50.040
Maybe we need to step back
and address these issues

00:23:50.040 --> 00:23:53.925
in a friction full way, rather
than a frictionless way.

00:23:53.925 --> 00:23:55.970
JOSH LOVEJOY: I would
kind of add to that.

00:23:55.970 --> 00:23:59.940
But that's kind of a pivot
from the statement of making it

00:23:59.940 --> 00:24:03.900
the easiest is understanding
that once you've

00:24:03.900 --> 00:24:08.190
seen some direct harm that's
been caused to a human being,

00:24:08.190 --> 00:24:11.150
it's impossible to
look away from it.

00:24:11.150 --> 00:24:17.992
And to the work, the two, three
years working on clips, trying

00:24:17.992 --> 00:24:19.950
to figure out what like
an intelligence program

00:24:19.950 --> 00:24:21.870
looks like to me
as a designer, I

00:24:21.870 --> 00:24:24.840
realized that it was
really user research that

00:24:24.840 --> 00:24:27.990
was the fundamental thing we've
been doing all along that we

00:24:27.990 --> 00:24:29.610
need to come back to.

00:24:29.610 --> 00:24:32.190
A UX researcher,
their primary job

00:24:32.190 --> 00:24:35.910
and the day to day comings and
goings of the product design

00:24:35.910 --> 00:24:39.640
is advocacy for human beings.

00:24:39.640 --> 00:24:42.304
They are closest to them
in the ideation phase,

00:24:42.304 --> 00:24:43.220
in the research phase.

00:24:43.220 --> 00:24:45.840
They're closest to them
in the evaluation phase.

00:24:45.840 --> 00:24:48.240
And it's this person that
we've delegated-- this role

00:24:48.240 --> 00:24:50.460
that we've delegated
to be the advocate.

00:24:50.460 --> 00:24:52.540
And they advocate
to product managers.

00:24:52.540 --> 00:24:53.790
They advocate engineers.

00:24:53.790 --> 00:24:55.500
The advocate to designers.

00:24:55.500 --> 00:25:00.440
And the most successful
that a UX researcher can be

00:25:00.440 --> 00:25:03.440
is in influencing the
decisions of others

00:25:03.440 --> 00:25:07.020
who get to shape the
code or the metal that

00:25:07.020 --> 00:25:09.630
goes out to a human being.

00:25:09.630 --> 00:25:11.940
Everyone else is like an
indirection layer away,

00:25:11.940 --> 00:25:13.680
farther away from them.

00:25:13.680 --> 00:25:16.239
And it's striking that the
majority of the training

00:25:16.239 --> 00:25:18.030
data that goes into
machine learning models

00:25:18.030 --> 00:25:21.046
is super distant from
actual user advocacy.

00:25:21.046 --> 00:25:22.920
The people who are
closest to it are the ones

00:25:22.920 --> 00:25:25.380
that often don't have the
closest access to understanding

00:25:25.380 --> 00:25:27.860
user need or human need.

00:25:27.860 --> 00:25:29.990
It's distant from user research.

00:25:29.990 --> 00:25:33.090
And there's no one advocating
for the protocol design

00:25:33.090 --> 00:25:35.470
and the labeling process,
the data collection process,

00:25:35.470 --> 00:25:37.380
the evaluation process.

00:25:37.380 --> 00:25:42.090
And to call back
to the practical,

00:25:42.090 --> 00:25:44.080
what can we do with the--

00:25:44.080 --> 00:25:45.480
how do we make it the easiest?

00:25:45.480 --> 00:25:47.550
I think we build on
the existing systems

00:25:47.550 --> 00:25:48.750
we have with UX research.

00:25:48.750 --> 00:25:52.200
And we expand that role to
encompass data collection

00:25:52.200 --> 00:25:55.710
and protocol design and
evaluation so that we expand

00:25:55.710 --> 00:25:58.050
advocacy, and we make
it harder to turn away

00:25:58.050 --> 00:26:02.400
from the impact of the
decisions that we make.

00:26:02.400 --> 00:26:04.950
Once you've seen that impact,
the easiest thing to do

00:26:04.950 --> 00:26:08.830
is address the problem.

00:26:08.830 --> 00:26:11.060
SARAH GOLD: I really agree
with what you're saying.

00:26:11.060 --> 00:26:13.870
And I want to add that in
certain areas that if it's been

00:26:13.870 --> 00:26:18.100
working, getting research though
into the places where data is

00:26:18.100 --> 00:26:21.550
collected, particularly
say, in a medical context,

00:26:21.550 --> 00:26:24.490
is so difficult. So it
just means that you are

00:26:24.490 --> 00:26:29.440
in a situation where the
products you want to ship

00:26:29.440 --> 00:26:33.070
haven't been fully tested
because you couldn't.

00:26:33.070 --> 00:26:35.650
And the data, even if you
do develop dummy data,

00:26:35.650 --> 00:26:38.920
it's never going to be as
rich as the real thing.

00:26:42.666 --> 00:26:45.040
I think also thinking about
the kind of ethical procedure

00:26:45.040 --> 00:26:47.320
of how we can get in
front of more people

00:26:47.320 --> 00:26:50.659
and have access to the
right kinds of information

00:26:50.659 --> 00:26:52.450
is really important to
make sure that we're

00:26:52.450 --> 00:26:56.290
making the right thing to
say I've totally hear you.

00:26:56.290 --> 00:26:59.380
And I think it's currently
the kind of ethical frameworks

00:26:59.380 --> 00:27:02.380
we have around access to
information about individuals

00:27:02.380 --> 00:27:09.290
to test products are
necessary and should be hard.

00:27:09.290 --> 00:27:12.340
But that I don't think they're
aligned with the process

00:27:12.340 --> 00:27:13.200
of making right now.

00:27:13.200 --> 00:27:16.322
So it is very difficult
to get to that point.

00:27:16.322 --> 00:27:17.280
KAREN KAUSHANSKY: Yeah.

00:27:17.280 --> 00:27:23.100
Just back to the friction
point that was made.

00:27:23.100 --> 00:27:25.920
So when the original
question was asked,

00:27:25.920 --> 00:27:28.540
I actually thought about
it in the opposite.

00:27:28.540 --> 00:27:30.660
I think what I heard
is it's OK to maybe put

00:27:30.660 --> 00:27:33.667
in some friction in the process.

00:27:33.667 --> 00:27:35.250
And then I just worry
that we're going

00:27:35.250 --> 00:27:40.260
to bias this towards people
that are suckers for friction,

00:27:40.260 --> 00:27:44.270
meaning they're willing to
take more time out of their day

00:27:44.270 --> 00:27:45.450
to do two more steps.

00:27:45.450 --> 00:27:47.500
So they're willing to
look under the hood.

00:27:47.500 --> 00:27:50.109
And then the people who
aren't, well then, I

00:27:50.109 --> 00:27:51.900
guess they're not going
to be part of this.

00:27:51.900 --> 00:27:53.070
CENNYDD BOWLES: Like,
slow food is easy

00:27:53.070 --> 00:27:54.510
if you're rich and
have lots of time.

00:27:54.510 --> 00:27:55.760
KAREN KAUSHANSKY: Yeah, right.

00:27:55.760 --> 00:28:01.850
So my first thought was
less work, more benefit.

00:28:01.850 --> 00:28:04.170
I can use this
because it's so easy.

00:28:04.170 --> 00:28:07.100
It's like implicit
data collection.

00:28:07.100 --> 00:28:09.980
But look at the
benefit that I get.

00:28:09.980 --> 00:28:12.740
And once I've come along with
that-- once I've said yeah,

00:28:12.740 --> 00:28:15.920
I understand all the
benefits, I'm hooked,

00:28:15.920 --> 00:28:18.217
then maybe you can introduce
some of that friction.

00:28:18.217 --> 00:28:20.300
CENNYDD BOWLES: Josh, did
you want to comment on--

00:28:20.300 --> 00:28:22.610
AUDIENCE: Yeah, you guys
kind of got to my question.

00:28:22.610 --> 00:28:24.110
Because I actually
wanted to revisit

00:28:24.110 --> 00:28:28.335
this point about friction,
and essentially the cost of it

00:28:28.335 --> 00:28:30.200
and who bears that cost?

00:28:30.200 --> 00:28:34.250
Because I can see very
much a world where

00:28:34.250 --> 00:28:39.530
we have organic free range
AI, which is totally unbiased

00:28:39.530 --> 00:28:41.750
and has been thought
through all the way through,

00:28:41.750 --> 00:28:44.670
and it comes to market later,
and you pay a premium for it.

00:28:44.670 --> 00:28:46.310
And the other stuff
is essentially

00:28:46.310 --> 00:28:48.450
becomes a tax on the poor.

00:28:48.450 --> 00:28:51.440
So there is kind of a theory
right now that advertising

00:28:51.440 --> 00:28:53.230
is becoming a tax
on the poor, people

00:28:53.230 --> 00:28:55.850
who can't pay for it to
go away until we could

00:28:55.850 --> 00:28:57.145
get in a similar situation.

00:28:57.145 --> 00:28:58.520
On the other hand,
you could also

00:28:58.520 --> 00:29:03.290
get into a place of who is this
person that does the extra two

00:29:03.290 --> 00:29:04.580
steps each time?

00:29:04.580 --> 00:29:07.130
Well, one is it's the
person who is rich and has

00:29:07.130 --> 00:29:08.510
the time on her hands.

00:29:08.510 --> 00:29:11.670
The other is that it's
actually passed down the chain.

00:29:11.670 --> 00:29:15.920
And those people will do
it to essentially avoid

00:29:15.920 --> 00:29:18.510
some advertising or get
a little bit more AI time

00:29:18.510 --> 00:29:21.590
or whatever it is, while
the rich are essentially

00:29:21.590 --> 00:29:23.960
paying for their labor,
or their subsidized labor.

00:29:24.740 --> 00:29:26.930
So I just want to throw
out that really happy topic

00:29:26.930 --> 00:29:28.140
to the group, and see if--

00:29:30.690 --> 00:29:33.230
SARAH GOLD: Last week, or the
week before, I think the Web

00:29:33.230 --> 00:29:38.960
1029 and Tim Berners Lee wrote
an article in "The Guardian."

00:29:38.960 --> 00:29:41.180
And I can't the
life of me remember

00:29:41.180 --> 00:29:42.800
the exact phrasing of it.

00:29:42.800 --> 00:29:46.850
But a bit of it said
that we're stuck

00:29:46.850 --> 00:29:49.370
in a place where we think that
advertising is the only way

00:29:49.370 --> 00:29:51.640
to pay for digital services.

00:29:51.640 --> 00:29:53.870
And we need a little
more creative thought.

00:29:53.870 --> 00:29:55.831
And I thought that was so good.

00:29:55.831 --> 00:29:57.080
Because I really believe that.

00:29:57.080 --> 00:29:58.835
I really think there
are other models.

00:29:58.835 --> 00:30:03.260
So there's a big economic
piece in there to discuss.

00:30:03.260 --> 00:30:05.614
Just wanted to jump
in there with that.

00:30:05.614 --> 00:30:08.280
CENNYDD BOWLES: I hear it's good
on panels to have disagreement.

00:30:08.280 --> 00:30:10.110
So I will disagree.

00:30:10.110 --> 00:30:13.520
I think the advertising use
cases is blamed unfairly

00:30:13.520 --> 00:30:17.630
for ethical violations,
particularly

00:30:17.630 --> 00:30:21.189
by a certain sort of
agitators and people

00:30:21.189 --> 00:30:22.730
who denigrate certain
business models

00:30:22.730 --> 00:30:26.090
for the benefits of their
alternative business models.

00:30:26.090 --> 00:30:29.280
The truth is, data is
valuable to any business now.

00:30:29.280 --> 00:30:31.800
There's nothing exclusive
about its advertising value.

00:30:31.800 --> 00:30:35.990
In, fact its value is
increasingly centered

00:30:35.990 --> 00:30:40.040
on its value as training data,
its analytics data and so on.

00:30:40.040 --> 00:30:43.250
And the value to advertising is
actually not strictly relevant.

00:30:43.250 --> 00:30:45.079
I think what we're seeing is--

00:30:45.079 --> 00:30:47.120
and we can get very
political here very quickly--

00:30:47.120 --> 00:30:51.650
but it's the natural outcomes
of a system that has always

00:30:51.650 --> 00:30:53.580
essentially shafted
certain people

00:30:53.580 --> 00:30:55.280
and rewarded certain people.

00:30:55.280 --> 00:31:00.320
I'm always skeptical of calls
to abandon ad funded models,

00:31:00.320 --> 00:31:04.550
because ad funded models have
brought free transformative

00:31:04.550 --> 00:31:06.180
technologies to billions.

00:31:06.180 --> 00:31:08.180
And I wouldn't want
to throw that away

00:31:08.180 --> 00:31:14.550
to see a system that essentially
makes technology a luxury good

00:31:14.550 --> 00:31:15.050
again.

00:31:20.320 --> 00:31:22.731
AUDIENCE: I have a question.

00:31:22.731 --> 00:31:24.400
CENNYDD BOWLES: You
sponsored the thing.

00:31:24.400 --> 00:31:24.900
Please--

00:31:29.660 --> 00:31:35.820
AUDIENCE: I would love to ask a
question about explainable AI.

00:31:35.820 --> 00:31:37.730
And I have a reason why.

00:31:37.730 --> 00:31:40.080
So I have both a question
and I have a comment.

00:31:40.080 --> 00:31:43.290
So my question is
for the entire panel,

00:31:43.290 --> 00:31:47.820
how useful do you think
that explaining AI is?

00:31:47.820 --> 00:31:51.090
And I'll qualify that question.

00:31:51.090 --> 00:31:52.470
So that's where the comment is.

00:31:52.470 --> 00:31:55.830
I think explainability is
super useful for practitioners

00:31:55.830 --> 00:31:58.080
and for builders,
because basically

00:31:58.080 --> 00:32:00.790
if you have explainability,
you can debug your system.

00:32:00.790 --> 00:32:02.790
So that's a granted thing.

00:32:02.790 --> 00:32:06.420
But for end users,
what I worry about is

00:32:06.420 --> 00:32:10.050
that I feel like in
a room like this,

00:32:10.050 --> 00:32:12.840
I think there's an assumption
that explainability

00:32:12.840 --> 00:32:15.180
for end users is super useful.

00:32:15.180 --> 00:32:18.180
And it's a desirable thing.

00:32:18.180 --> 00:32:20.790
And I believe that.

00:32:20.790 --> 00:32:23.130
I come from my data
visualization background.

00:32:23.130 --> 00:32:25.440
And if I can visualize
the heck of your data

00:32:25.440 --> 00:32:27.900
and show it to you as
a mirror of yourself,

00:32:27.900 --> 00:32:28.985
I will be super happy.

00:32:28.985 --> 00:32:31.110
Because I'm like, you're
going to be more informed.

00:32:31.110 --> 00:32:33.150
You're going to be
making better decisions.

00:32:33.150 --> 00:32:36.260
And I'm all for explainability.

00:32:36.260 --> 00:32:38.960
Then I came to Google.

00:32:38.960 --> 00:32:45.590
And I realized that end users,
most of the time, do not care.

00:32:45.590 --> 00:32:48.560
And they look at transparency
about their history

00:32:48.560 --> 00:32:52.880
of their data, and most
users never look at that.

00:32:52.880 --> 00:32:54.230
And they don't care.

00:32:54.230 --> 00:33:00.980
And so my question comes
from this point of view.

00:33:00.980 --> 00:33:03.140
Is it people like
us who are assuming

00:33:03.140 --> 00:33:05.960
that end users actually
want explainability?

00:33:05.960 --> 00:33:07.760
And if so, why do they want it?

00:33:07.760 --> 00:33:08.660
How is it useful?

00:33:08.660 --> 00:33:11.090
And how do we make
them care, basically,

00:33:11.090 --> 00:33:12.007
at the end of the day?

00:33:12.007 --> 00:33:14.215
AUDIENCE: Because of a thread
I was going to pick up,

00:33:14.215 --> 00:33:16.835
I'd like to throw into
that and see what happens.

00:33:20.020 --> 00:33:23.570
It was a quote from Patrick's
talk, but also I think,

00:33:23.570 --> 00:33:26.980
echoed through some of the
things we heard this morning.

00:33:26.980 --> 00:33:31.280
And Patrick said, blurring the
line between writing and using

00:33:31.280 --> 00:33:32.340
software.

00:33:32.340 --> 00:33:35.780
So again, we're
back to the role--

00:33:35.780 --> 00:33:37.055
what role do we have?

00:33:37.055 --> 00:33:40.430
Are we end users, or do we
have authorship or agency?

00:33:40.430 --> 00:33:46.290
And does that change the
demand for explicability

00:33:46.290 --> 00:33:49.290
and these sorts of things.

00:33:49.290 --> 00:33:52.100
For plus five difficulty.

00:33:52.100 --> 00:33:56.460
CENNYDD BOWLES: Well, I don't
want to hog the mic, here.

00:33:56.460 --> 00:33:58.110
I'm always worried
when I hear things

00:33:58.110 --> 00:34:00.300
like how do we make people
care about something.

00:34:00.300 --> 00:34:03.990
That strikes me as a
pretty loaded comment,

00:34:03.990 --> 00:34:07.380
no offense meant.

00:34:07.380 --> 00:34:11.080
Even if users don't
care, regulators will.

00:34:11.080 --> 00:34:16.050
And I think there comes a point
at which it's simply good AI

00:34:16.050 --> 00:34:18.350
hygiene to have
explainability baked in

00:34:18.350 --> 00:34:21.969
or to have at least an approach
to it within your systems,

00:34:21.969 --> 00:34:23.899
even if it's something
trivial, like should I

00:34:23.899 --> 00:34:24.690
have chicken curry?

00:34:24.690 --> 00:34:25.230
Yes, no?

00:34:25.230 --> 00:34:25.729
Yes?

00:34:25.729 --> 00:34:26.340
OK, great.

00:34:26.340 --> 00:34:28.440
Tell me how.

00:34:28.440 --> 00:34:31.130
We want to be sure
that certain systems--

00:34:31.130 --> 00:34:33.330
essentially those systems
that have a strong impact

00:34:33.330 --> 00:34:37.320
on human freedom such as
criminal justice, algorithms,

00:34:37.320 --> 00:34:40.110
sentencing algorithms,
and so on, those

00:34:40.110 --> 00:34:41.070
have to be explainable.

00:34:41.070 --> 00:34:44.725
And I don't buy any of the sort
of hand-waving and question

00:34:44.725 --> 00:34:47.100
dodging I hear sometimes from
data scientists and machine

00:34:47.100 --> 00:34:49.100
learning specialists
saying but, deep learning--

00:34:49.100 --> 00:34:50.260
we're eight layers deep.

00:34:50.260 --> 00:34:51.510
We just simply cannot do that.

00:34:51.510 --> 00:34:53.120
You find a way or don't
use those systems.

00:34:53.120 --> 00:34:55.120
AUDIENCE: That's a very
active area of research.

00:34:57.600 --> 00:34:59.790
As I was saying,
contextually speaking,

00:34:59.790 --> 00:35:02.460
I think there are scenarios
and reasons why we

00:35:02.460 --> 00:35:04.560
want explainable AI, for sure.

00:35:04.560 --> 00:35:05.640
Criminal justice?

00:35:05.640 --> 00:35:07.230
For sure.

00:35:07.230 --> 00:35:11.310
But again, I think it's
very dangerous for us

00:35:11.310 --> 00:35:14.610
as a community to be assuming
certain things that users

00:35:14.610 --> 00:35:16.000
want these things.

00:35:16.000 --> 00:35:19.770
And so I would challenge
us all to be thinking

00:35:19.770 --> 00:35:22.860
about creative ways in which
these things can be actually

00:35:22.860 --> 00:35:26.970
useful to users
instead of a you need

00:35:26.970 --> 00:35:28.920
to eat your broccoli today.

00:35:28.920 --> 00:35:30.610
It's going to be good for you.

00:35:30.610 --> 00:35:34.347
So I'm curious about are
there creative ways--

00:35:34.347 --> 00:35:36.180
because I feel like
that's one of the things

00:35:36.180 --> 00:35:39.990
that designers do so well is
you engage with the messiness.

00:35:39.990 --> 00:35:43.410
We engage with the realities
of the users and the user need.

00:35:43.410 --> 00:35:48.120
So I'm curious what user need
are we actually talking about?

00:35:48.120 --> 00:35:49.800
Granted, when things
in machine learning

00:35:49.800 --> 00:35:52.860
go wrong, if you get the
wrong recommendation,

00:35:52.860 --> 00:35:57.860
definitely you're going to want
an explanation, And absolutely.

00:35:57.860 --> 00:36:04.919
But 99% of the rest of the
time, why do we need that?

00:36:04.919 --> 00:36:06.960
KAREN KAUSHANSKY: That's
what I was going to say.

00:36:06.960 --> 00:36:09.030
It was like they don't
care until they will care.

00:36:09.030 --> 00:36:12.670
And they're going to care
when something goes wrong.

00:36:15.684 --> 00:36:17.850
Again, it's almost like
knowing what you don't know.

00:36:17.850 --> 00:36:21.780
How are we going to
know when we're wrong?

00:36:21.780 --> 00:36:23.619
And do we just
explain everything?

00:36:23.619 --> 00:36:26.160
But of course, you are going to
want a different lens of when

00:36:26.160 --> 00:36:27.690
something is wrong,
when something is right,

00:36:27.690 --> 00:36:28.740
and how you explain it.

00:36:28.740 --> 00:36:32.970
So it goes back to almost
what I started saying before.

00:36:32.970 --> 00:36:35.100
How do we know when we're wrong?

00:36:35.100 --> 00:36:38.190
How do we know
what we don't know?

00:36:38.190 --> 00:36:41.100
And maybe that's OK.

00:36:41.100 --> 00:36:45.337
Just knowing to explain
it when things go wrong.

00:36:45.337 --> 00:36:46.920
JOSH LOVEJOY: And
like, so many things

00:36:46.920 --> 00:36:48.336
in this area, that
are like, well,

00:36:48.336 --> 00:36:50.340
depends what you mean
by explainability.

00:36:53.420 --> 00:36:56.190
Throughout the process of trying
to figure out the fairness

00:36:56.190 --> 00:36:59.750
question at Google, and figure
out how to implement fairness

00:36:59.750 --> 00:37:04.080
into the lifecycle,
into the launch process,

00:37:04.080 --> 00:37:06.930
it always struck me as odd
that the starting point for all

00:37:06.930 --> 00:37:10.600
this stuff was to figure
out some universal metric.

00:37:10.600 --> 00:37:15.570
But whereas in human day
to day conversations, when

00:37:15.570 --> 00:37:17.730
we want to try to get
an answer from somebody,

00:37:17.730 --> 00:37:20.020
it's kind of a ludicrous
thing to ask somebody

00:37:20.020 --> 00:37:21.120
why did you do that?

00:37:21.120 --> 00:37:23.160
It's especially
ludicrous to ask a child.

00:37:23.160 --> 00:37:24.640
And yet, I do it
every single time.

00:37:25.140 --> 00:37:27.230
My six-year-old pushes my
3-year-old or something

00:37:27.230 --> 00:37:27.730
like that.

00:37:27.730 --> 00:37:28.680
Why did you do that?

00:37:28.680 --> 00:37:32.270
He's going to make up a reason
is what's going to happen.

00:37:32.270 --> 00:37:36.320
But what we do in
the more risky areas

00:37:36.320 --> 00:37:41.850
is we perform some layer of
calibration and accreditation.

00:37:41.850 --> 00:37:44.340
You have to take a
test to drive a car.

00:37:44.340 --> 00:37:48.350
You have to pass a bar exam
if you want to be a lawyer.

00:37:48.350 --> 00:37:49.530
There's systems.

00:37:49.530 --> 00:37:51.120
To be a doctor, you
can't just show up

00:37:51.120 --> 00:37:52.411
and think you have a good idea.

00:37:52.411 --> 00:37:53.880
You have to be certified.

00:37:53.880 --> 00:37:56.040
And so the sort of
interesting question

00:37:56.040 --> 00:37:58.740
I think we have
as a community is

00:37:58.740 --> 00:38:01.680
to talk about what it means for
something to be good enough.

00:38:01.680 --> 00:38:03.570
Like a data set
to be good enough

00:38:03.570 --> 00:38:07.320
or have enough sort of
quality or a thoroughness

00:38:07.320 --> 00:38:09.780
or a representativeness
into it, so that we know

00:38:09.780 --> 00:38:12.390
it's a credible source of data.

00:38:12.390 --> 00:38:14.250
How would we then
take that and to know

00:38:14.250 --> 00:38:17.477
that the model is
credible enough to operate

00:38:17.477 --> 00:38:18.435
within a given context?

00:38:20.980 --> 00:38:24.900
So I think that that ability
to track through from intent

00:38:24.900 --> 00:38:30.510
into application, and to know
that the sources, the euros

00:38:30.510 --> 00:38:33.340
collected, the people that
were involved in collecting it,

00:38:33.340 --> 00:38:35.877
the underlying
assumptions that went

00:38:35.877 --> 00:38:37.710
into the methodology
of collecting it-- it's

00:38:37.710 --> 00:38:40.400
important for the people
who are the makers.

00:38:40.400 --> 00:38:41.650
I think that's our first user.

00:38:41.650 --> 00:38:43.420
To answer maybe one
or two questions--

00:38:43.420 --> 00:38:46.980
our first user that
we need to hit--

00:38:46.980 --> 00:38:48.750
just why we focus
on it with pair--

00:38:48.750 --> 00:38:50.790
are the people who
are shipping code,

00:38:50.790 --> 00:38:52.920
who have to look at
it, who have to wear

00:38:52.920 --> 00:38:55.260
that iron ring so
to speak, and make

00:38:55.260 --> 00:38:56.630
the decisions thoughtfully.

00:38:56.630 --> 00:38:59.430
If they can look at
a data set and say

00:38:59.430 --> 00:39:02.190
yeah, that's a representative
of the intended application

00:39:02.190 --> 00:39:05.012
that I think I want
to train my model for,

00:39:05.012 --> 00:39:07.470
then that's a lifecycle that
I think we can start out with.

00:39:07.470 --> 00:39:09.180
And with that, comes
explainability I

00:39:09.180 --> 00:39:10.964
think in a certain--

00:39:10.964 --> 00:39:12.880
SARAH GOLD: I think there
are layers of users.

00:39:12.880 --> 00:39:14.080
And I think that's
kind of where we

00:39:14.080 --> 00:39:16.000
got to the previous
question-- that question

00:39:16.000 --> 00:39:18.990
of what is the user need can
be so harmful to a project.

00:39:18.990 --> 00:39:21.300
It can also be really
useful at the right time,

00:39:21.300 --> 00:39:23.300
but can also kill work.

00:39:23.300 --> 00:39:28.912
So I think it should be
used wisely as a question.

00:39:28.912 --> 00:39:31.810
I think it's used
too widely, actually.

00:39:31.810 --> 00:39:37.000
Because there are certainly
user needs of explainability

00:39:37.000 --> 00:39:38.950
that are to do with
when things go wrong.

00:39:38.950 --> 00:39:39.520
Absolutely.

00:39:39.520 --> 00:39:41.500
And those go through
lots and lots

00:39:41.500 --> 00:39:43.390
of different chains
of whether that

00:39:43.390 --> 00:39:46.000
be an individual
in an organization,

00:39:46.000 --> 00:39:48.535
or a parliamentary
committee, or there are so

00:39:48.535 --> 00:39:51.190
many different levels of users
the we need different kinds

00:39:51.190 --> 00:39:53.870
of explanations.

00:39:53.870 --> 00:39:56.380
I think in general,
for a sort of end user,

00:39:56.380 --> 00:39:58.139
so to speak, actually,
there's a lot

00:39:58.139 --> 00:39:59.680
to be said about
the sort of folklore

00:39:59.680 --> 00:40:01.930
that we tell ourselves about
the magic of technology

00:40:01.930 --> 00:40:03.305
and how things
work, and the sort

00:40:03.305 --> 00:40:04.990
of stories we end
up telling ourselves

00:40:04.990 --> 00:40:06.850
because we just don't get it.

00:40:06.850 --> 00:40:08.390
That will certainly continue.

00:40:08.390 --> 00:40:10.630
But I think we have
a duty of care to try

00:40:10.630 --> 00:40:12.940
and help people get this
stuff, because there's

00:40:12.940 --> 00:40:14.480
a danger in that magic.

00:40:14.480 --> 00:40:17.200
And I just always
think about individuals

00:40:17.200 --> 00:40:19.420
that I've watched
who might be trying

00:40:19.420 --> 00:40:24.250
to help set up an iPhone for
their elderly grandparent.

00:40:24.250 --> 00:40:28.090
And wanting to take a lot of
care over that, because you're

00:40:28.090 --> 00:40:30.154
making decisions on the
behalf of someone else.

00:40:30.154 --> 00:40:31.570
And I think sometimes
that's where

00:40:31.570 --> 00:40:34.240
you get to what the user
need to explainability is.

00:40:34.240 --> 00:40:36.970
It's kind of so you can
make the right decisions.

00:40:36.970 --> 00:40:39.000
And so you can be
in control of that.

00:40:39.000 --> 00:40:42.640
And I think that often when you
talk about I don't care-- well,

00:40:42.640 --> 00:40:45.730
think of all the times
you've helped your mom fix

00:40:45.730 --> 00:40:50.920
a piece of technology, or
even helped your daughter

00:40:50.920 --> 00:40:53.175
or son to make something work.

00:40:53.175 --> 00:40:55.300
You need to have a level
of understanding and trust

00:40:55.300 --> 00:40:57.550
in that you can make
decisions for other people.

00:40:57.550 --> 00:41:00.550
It's a really important part
of being human, and building

00:41:00.550 --> 00:41:01.720
trust and relationships.

00:41:01.720 --> 00:41:06.840
So I think to forget that, is to
forget what humanity is about,

00:41:06.840 --> 00:41:07.350
really.

00:41:07.350 --> 00:41:08.808
I know it goes
really, really meta.

00:41:08.808 --> 00:41:11.190
But that's what I believe.

00:41:11.190 --> 00:41:13.690
JOSH LOVEJOY: Actually, it's
just another echo from Patrick,

00:41:13.690 --> 00:41:14.680
from your presentation.

00:41:14.680 --> 00:41:17.230
You talked about teaching as
being this process of trying

00:41:17.230 --> 00:41:21.250
to piece back together the
process by which you attained

00:41:21.250 --> 00:41:23.150
the knowledge in
the first place.

00:41:23.150 --> 00:41:25.000
But the other person
you're trying to teach

00:41:25.000 --> 00:41:27.200
doesn't have the
shared experiences.

00:41:27.200 --> 00:41:32.845
And therein lies the magic and
the messiness to your point.

00:41:32.845 --> 00:41:39.150
AUDIENCE: Just to
editorialize, the explanation

00:41:39.150 --> 00:41:40.790
is an invitation.

00:41:40.790 --> 00:41:44.820
It's an invitation to
others to make it better,

00:41:44.820 --> 00:41:46.470
even if it hasn't gone wrong.

00:41:46.470 --> 00:41:53.430
And I think that's a place
where the refuge of magic

00:41:53.430 --> 00:41:56.520
as a metaphor for
technology design has--

00:42:00.900 --> 00:42:04.260
I think that's
where it went wrong.

00:42:04.260 --> 00:42:09.290
Is I think we should consider
magic harmful, like go-to

00:42:09.290 --> 00:42:10.170
statements.

00:42:15.240 --> 00:42:17.580
The explanation is
an invitation is

00:42:17.580 --> 00:42:21.210
the thing that gives me hope for
where we might take this stuff.

00:42:21.210 --> 00:42:25.050
I think Dan had a comment.

00:42:25.050 --> 00:42:27.300
AUDIENCE: I think I knew
what I was going to say

00:42:27.300 --> 00:42:29.690
when you asked the question.

00:42:29.690 --> 00:42:32.310
But then Sarah said it's all
about the essence of humanity.

00:42:32.310 --> 00:42:33.300
So I'm lost.

00:42:36.272 --> 00:42:38.730
I wonder if-- and this is a
sort of a half formed thought--

00:42:38.730 --> 00:42:44.070
that as designers, those of us
that grew up with the internet,

00:42:44.070 --> 00:42:47.520
for a long time, user-centered
design things took root

00:42:47.520 --> 00:42:50.000
because we were dealing
with a certain kind of thing

00:42:50.000 --> 00:42:53.770
as our [? outpolled. ?]
The first 15 years,

00:42:53.770 --> 00:42:55.560
20 years of the
internet or the web--

00:42:55.560 --> 00:42:58.880
which I will say the
web to be specific--

00:42:58.880 --> 00:43:01.351
we're dealing with a
certain type of material.

00:43:01.351 --> 00:43:03.475
It was sort of the media,
or it was music industry,

00:43:03.475 --> 00:43:06.940
or it was search results, or it
was advertising systems related

00:43:06.940 --> 00:43:07.440
to that.

00:43:07.440 --> 00:43:09.530
And I suppose where a few
of us have been going today

00:43:09.530 --> 00:43:11.100
is that now those
systems are beginning

00:43:11.100 --> 00:43:13.391
to embed but themselves in
the fabric of everyday life,

00:43:13.391 --> 00:43:15.210
the really physical stuff.

00:43:15.210 --> 00:43:16.260
It's different.

00:43:16.260 --> 00:43:18.840
And it's different if it
runs your energy system.

00:43:18.840 --> 00:43:21.880
It's different if it affects
whether the bus turns

00:43:21.880 --> 00:43:22.380
up or not.

00:43:22.380 --> 00:43:25.459
It's different whether you
get into the housing co-op.

00:43:25.459 --> 00:43:27.000
And it's beginning
to, as we can see,

00:43:27.000 --> 00:43:28.708
understood that it
actually maybe changes

00:43:28.708 --> 00:43:30.630
national politics,
all kinds of things.

00:43:30.630 --> 00:43:32.280
And it wasn't the
case some years ago.

00:43:32.280 --> 00:43:34.488
Maybe it was, because it
was running under the radar.

00:43:34.488 --> 00:43:36.330
We weren't explicitly
addressing that.

00:43:36.330 --> 00:43:39.510
So UCD and the techniques we
have could for a long time

00:43:39.510 --> 00:43:42.096
get by with a very
individualistic view.

00:43:42.096 --> 00:43:44.770
And just to be
super clear, I once

00:43:44.770 --> 00:43:47.190
designed the Spice
Girls website.

00:43:47.190 --> 00:43:50.240
That's the kind of stuff
that the first 10, 15 years

00:43:50.240 --> 00:43:50.740
was about.

00:43:54.060 --> 00:43:56.450
Turns out, that's incredibly
important apparently

00:43:56.450 --> 00:43:57.700
for the 14-year-old girls.

00:43:57.700 --> 00:44:03.570
But it's now become
as it's embedded

00:44:03.570 --> 00:44:06.930
into everyday
infrastructure, it's

00:44:06.930 --> 00:44:09.610
raised the bar in the type
of design that we have to do.

00:44:09.610 --> 00:44:13.310
So this explainability question
is now incredibly important.

00:44:13.310 --> 00:44:16.030
Because I can see why people
weren't interested particularly

00:44:16.030 --> 00:44:20.610
in Google Search data
for quite a long time.

00:44:20.610 --> 00:44:24.540
But if it now affects the
way that my life actually

00:44:24.540 --> 00:44:27.870
works in a very direct way,
really fundamental things

00:44:27.870 --> 00:44:31.080
like whether food trucks turn up
or not, then that's different.

00:44:31.080 --> 00:44:34.560
So it picks up your point
I think also about not just

00:44:34.560 --> 00:44:36.810
user-centered design, but
a system-center design,

00:44:36.810 --> 00:44:39.210
or a civic-center design.

00:44:39.210 --> 00:44:42.550
Each of those two systems has
an impact well outside of that.

00:44:42.550 --> 00:44:45.100
The explainability of a
bus stop is important.

00:44:45.100 --> 00:44:47.350
That's why I put the planning
notice up there as well.

00:44:47.350 --> 00:44:51.030
So really, that piece of paper
is a really ham fisted attempt

00:44:51.030 --> 00:44:53.700
to explain that something is
going to happen in your stream.

00:44:53.700 --> 00:44:56.220
This is the decision
making process around it.

00:44:56.220 --> 00:44:57.665
Here's who's behind that.

00:44:57.665 --> 00:44:59.123
And here's who the
arbiter of that,

00:44:59.123 --> 00:45:00.874
as in the local government.

00:45:00.874 --> 00:45:03.040
And here's where to go and
talk to someone about it.

00:45:03.040 --> 00:45:06.390
So as Matt says, that's an
invitation to get involved,

00:45:06.390 --> 00:45:09.580
or to figure out what that
is now for everything else.

00:45:16.356 --> 00:45:19.240
AUDIENCE: I'm going to use your
opportunity to thank everyone

00:45:19.240 --> 00:45:20.800
for sharing everything
you've shared

00:45:20.800 --> 00:45:23.410
this morning and this afternoon.

00:45:23.410 --> 00:45:26.530
I wonder if we are
blurring a lot of lines.

00:45:26.530 --> 00:45:30.040
Because I don't think
we are questioning

00:45:30.040 --> 00:45:31.930
the value of explainability.

00:45:31.930 --> 00:45:35.530
I wonder if there are
different layers of UCD?

00:45:35.530 --> 00:45:39.350
So is it that sometimes
a regulator is our user?

00:45:39.350 --> 00:45:45.210
Or other times where the actual
end consumer is our user?

00:45:45.210 --> 00:45:48.400
And so I wanted to hear your
thoughts on that, because I

00:45:48.400 --> 00:45:52.360
feel like with a lot of
the transparency, openness

00:45:52.360 --> 00:45:54.490
that we are asking
for, we are also

00:45:54.490 --> 00:45:59.830
almost pushing the
responsibility or use

00:45:59.830 --> 00:46:00.520
to the user.

00:46:00.520 --> 00:46:02.950
And I don't think that is
fair, because the education

00:46:02.950 --> 00:46:07.060
levels of our community are just
not at all in the same level.

00:46:07.060 --> 00:46:09.830
Until we do a lot
of education reform,

00:46:09.830 --> 00:46:11.340
we are not at that level.

00:46:11.340 --> 00:46:16.810
So I don't think it's fair for
a large amount of the population

00:46:16.810 --> 00:46:20.670
to ask people to be
vigilant about these things.

00:46:20.670 --> 00:46:22.300
I don't want to get
into a supermarket

00:46:22.300 --> 00:46:25.480
and feel afraid that the
ceiling is going to drop on me.

00:46:25.480 --> 00:46:28.900
But there are laws and
policies and liability rules

00:46:28.900 --> 00:46:30.220
that are protecting me.

00:46:30.220 --> 00:46:32.800
So is it that we are
blurring the lines?

00:46:32.800 --> 00:46:36.100
And we should think about
different layers of UCD?

00:46:36.100 --> 00:46:40.000
And how to design for
those different levels?

00:46:50.692 --> 00:46:51.530
AUDIENCE: Hello.

00:46:51.530 --> 00:46:53.280
I'm Rachel.

00:46:53.280 --> 00:47:01.110
I run a think tank in London
called [INAUDIBLE] everyone.

00:47:01.110 --> 00:47:08.910
We just did a piece of research
looking into people's feelings

00:47:08.910 --> 00:47:11.289
and at an attitudes about tech.

00:47:11.289 --> 00:47:13.080
And I think the really
important thing that

00:47:13.080 --> 00:47:16.260
hasn't been touched
on is I can feel

00:47:16.260 --> 00:47:19.380
20 different things at once.

00:47:19.380 --> 00:47:23.610
I can not care at all, because
I have to do it quickly.

00:47:23.610 --> 00:47:28.170
And I can simultaneously,
morally, and ethically

00:47:28.170 --> 00:47:30.280
care deeply.

00:47:30.280 --> 00:47:34.350
And I think that
that is glossed over.

00:47:34.350 --> 00:47:38.880
And in terms of thinking about
how we regulate and how we

00:47:38.880 --> 00:47:47.610
design for regulators, there's
a real issue around regulating

00:47:47.610 --> 00:47:54.950
for things that
cannot be anticipated.

00:47:54.950 --> 00:48:02.880
So everybody knows a
bridge is meant to stay up.

00:48:02.880 --> 00:48:07.700
The problem is in anticipating
both the things that

00:48:07.700 --> 00:48:12.650
are unlikely to go
wrong, and that actually

00:48:12.650 --> 00:48:17.630
how can we understand
people's reactions to things

00:48:17.630 --> 00:48:20.600
if we're presuming
they aren't interested?

00:48:20.600 --> 00:48:22.610
And then it has to be easy.

00:48:22.610 --> 00:48:26.870
And I think the
only thing I'd add

00:48:26.870 --> 00:48:29.900
is that maybe not everybody
has the time to have everything

00:48:29.900 --> 00:48:31.710
explained to them.

00:48:31.710 --> 00:48:36.290
But who is in
charge of choosing?

00:48:36.290 --> 00:48:39.770
And that it isn't a company.

00:48:39.770 --> 00:48:42.950
It isn't an
[INAUDIBLE] designers.

00:48:42.950 --> 00:48:46.880
There needs to be a democratic
accountability there

00:48:46.880 --> 00:48:51.410
which is likely to be different
countries, unfortunately.

00:48:51.410 --> 00:48:53.720
SARAH GOLD: The really
good example of that--

00:48:53.720 --> 00:48:56.095
I just really want to stay
before it goes out of my head,

00:48:56.095 --> 00:48:57.170
because it will--

00:48:57.170 --> 00:48:59.960
on this retreat we were
at, we were in Valencia

00:48:59.960 --> 00:49:01.970
for the Internet
Freedom Festival,

00:49:01.970 --> 00:49:04.670
which is something I
try to go to every year.

00:49:04.670 --> 00:49:07.414
And it's quite kind of
heartbreaking event actually.

00:49:07.414 --> 00:49:09.830
And it's sort of getting more
and more dystopic every year

00:49:09.830 --> 00:49:10.329
I go.

00:49:10.329 --> 00:49:13.850
But this year there
was a talk that

00:49:13.850 --> 00:49:18.622
was about abuse
towards women in India.

00:49:18.622 --> 00:49:20.330
And I think it might
have been last year,

00:49:20.330 --> 00:49:22.730
there were the horrific
stories of women

00:49:22.730 --> 00:49:25.270
being attacked on buses.

00:49:25.270 --> 00:49:26.940
And what happened to
that period of time

00:49:26.940 --> 00:49:30.650
was there was suddenly a
flood of apps onto various app

00:49:30.650 --> 00:49:33.500
stores for women to
say where they were

00:49:33.500 --> 00:49:34.880
and that they were safe.

00:49:34.880 --> 00:49:37.430
But many of those
apps were not secure.

00:49:37.430 --> 00:49:40.160
They were sending data
directly to abusers

00:49:40.160 --> 00:49:42.800
and to groups of attackers.

00:49:42.800 --> 00:49:46.640
Now this is a real example
of [INAUDIBLE] comment

00:49:46.640 --> 00:49:48.260
about I go into
supermarket and trust

00:49:48.260 --> 00:49:50.040
that the roof won't fall down.

00:49:50.040 --> 00:49:52.430
Is it the Google's Play Store--

00:49:52.430 --> 00:49:56.510
is it their responsibility to
have then banned those apps?

00:49:56.510 --> 00:49:58.700
How can regulation
keep pace with the fact

00:49:58.700 --> 00:50:01.400
that within the
space of 24 hours,

00:50:01.400 --> 00:50:03.410
there had been a flood
of apps on the market

00:50:03.410 --> 00:50:07.130
that then women had
downloaded and started to use?

00:50:07.130 --> 00:50:09.931
So there's a real question here
about sort of transparency.

00:50:09.931 --> 00:50:12.305
And as I mentioned before,
the kind of course correction.

00:50:12.305 --> 00:50:15.890
And where can we help people
to make better decisions?

00:50:15.890 --> 00:50:22.970
Because it's a really,
really hard issue to solve.

00:50:22.970 --> 00:50:25.280
But it's super
important because it's

00:50:25.280 --> 00:50:28.520
affecting individuals who are
less privileged in a really

00:50:28.520 --> 00:50:29.840
serious way every day.

00:50:29.840 --> 00:50:31.400
So it definitely can't be us.

00:50:31.400 --> 00:50:34.186
I completely agree.

00:50:34.186 --> 00:50:38.058
[INAUDIBLE]

00:50:46.286 --> 00:50:51.450
JOSH LOVEJOY: Well,
one just small note,

00:50:51.450 --> 00:50:52.650
interesting to note--

00:50:52.650 --> 00:50:54.050
and it's not perfect--

00:50:54.050 --> 00:51:00.120
but of all of the different fake
news and weird filter bubbly

00:51:00.120 --> 00:51:01.500
conversations--

00:51:01.500 --> 00:51:05.540
Wikipedia has been left
pretty much intact.

00:51:05.540 --> 00:51:07.410
It's an interesting
case study to look at.

00:51:07.410 --> 00:51:09.690
I feel at the risk
of saying something

00:51:09.690 --> 00:51:13.590
I'm probably not supposed
to say, the excuse of 400

00:51:13.590 --> 00:51:18.510
million hours of videos
uploaded to YouTube every hour

00:51:18.510 --> 00:51:19.530
or whatever--

00:51:19.530 --> 00:51:22.770
so it's just there's
too much to wrangle.

00:51:22.770 --> 00:51:25.650
I think we can probably
do better than just

00:51:25.650 --> 00:51:27.310
saying there's too much.

00:51:27.310 --> 00:51:31.620
So there's a question of
how you actually take maybe

00:51:31.620 --> 00:51:34.740
some pages out of the
leaning into dialogue

00:51:34.740 --> 00:51:37.470
and leaning into ways that
people can actually do some

00:51:37.470 --> 00:51:38.700
self-regulation.

00:51:38.700 --> 00:51:40.980
There's always going
to be a power dynamic

00:51:40.980 --> 00:51:42.990
or a power law of
not everyone is going

00:51:42.990 --> 00:51:44.510
to want to deeply introspect.

00:51:44.510 --> 00:51:47.850
But there is a trust and the
fundamental good of a system

00:51:47.850 --> 00:51:50.490
that someone
hopefully has thought

00:51:50.490 --> 00:51:53.640
about the structural
integrity of a system.

00:51:53.640 --> 00:51:56.580
And I think it can't
just be companies.

00:51:56.580 --> 00:51:58.390
And it can't just be regulators.

00:51:58.390 --> 00:52:01.360
And it can't always
be the individual.

00:52:01.360 --> 00:52:02.610
So just some food for thought.

00:52:09.670 --> 00:52:12.260
AUDIENCE: Hi, everyone.

00:52:12.260 --> 00:52:15.280
I just wanted to build on some
of those things that were said,

00:52:15.280 --> 00:52:18.560
or talk about a particular thing
that I was interested to see

00:52:18.560 --> 00:52:21.160
what your reaction was.

00:52:21.160 --> 00:52:24.970
I think we've all been talking
about design and technology

00:52:24.970 --> 00:52:25.690
and progression.

00:52:25.690 --> 00:52:28.000
And we're all kind of
the developers of that.

00:52:28.000 --> 00:52:29.895
And that is opportunistic.

00:52:29.895 --> 00:52:33.010
And in the way that we
create those opportunities,

00:52:33.010 --> 00:52:34.940
we become responsible for them.

00:52:34.940 --> 00:52:37.270
So it kind of fills
in the discussions

00:52:37.270 --> 00:52:40.630
that have gone on that we are
responsible as the developers

00:52:40.630 --> 00:52:43.030
and the creators.

00:52:43.030 --> 00:52:46.990
And I think that what I
was wanting to discuss

00:52:46.990 --> 00:52:50.530
was there seems to be a kind of
time lapse issue where you've

00:52:50.530 --> 00:52:52.870
got the creators
and the developers,

00:52:52.870 --> 00:52:54.045
people inside companies.

00:52:54.045 --> 00:52:55.545
It's not the companies
are, but it's

00:52:55.545 --> 00:52:57.380
the people inside
the companies are

00:52:57.380 --> 00:52:59.800
developing these opportunities.

00:52:59.800 --> 00:53:01.450
And then there's a
kind of time lapse.

00:53:01.450 --> 00:53:05.650
And then there's other
people that then take up

00:53:05.650 --> 00:53:10.000
these opportunities that could
be political opportunists,

00:53:10.000 --> 00:53:11.890
as we've seen last week.

00:53:11.890 --> 00:53:17.360
I think you are describing
that as a perverse distortion.

00:53:17.360 --> 00:53:18.140
Yes.

00:53:18.140 --> 00:53:21.460
So you've got that, which I
think is a kind of a time lag,

00:53:21.460 --> 00:53:23.202
is a second wave.

00:53:23.202 --> 00:53:25.160
And then you've got a
kind of final wave, which

00:53:25.160 --> 00:53:29.390
has to do with legals and
legislation, which is always

00:53:29.390 --> 00:53:31.070
behind.

00:53:31.070 --> 00:53:34.040
And it was very interesting
listening to Alex's talk

00:53:34.040 --> 00:53:37.940
about taking up the
kind of position

00:53:37.940 --> 00:53:40.730
of trying to validate
and certify the things

00:53:40.730 --> 00:53:44.210
that she was interested
in, in the first wave.

00:53:44.210 --> 00:53:46.640
So a kind of community
of first wavers

00:53:46.640 --> 00:53:49.940
taking up a position of
responsibility for things

00:53:49.940 --> 00:53:51.002
that they're creating.

00:53:51.002 --> 00:53:52.460
And I thought that
was interesting.

00:53:52.460 --> 00:53:56.090
But I'd like to just ask what do
you think about this time lag?

00:53:56.090 --> 00:53:57.450
And how does it work?

00:53:57.450 --> 00:53:59.876
Because it's gone wrong.

00:53:59.876 --> 00:54:05.295
JOSH LOVEJOY: One
quick anecdote on that.

00:54:05.295 --> 00:54:08.360
So there was an article recently
in the "Washington Post"

00:54:08.360 --> 00:54:12.440
after we shipped Google
Clips, that was something

00:54:12.440 --> 00:54:16.970
to the effect of Google hasn't
built an all seeing eye yet,

00:54:16.970 --> 00:54:19.880
because you've got to
have a click bait title.

00:54:19.880 --> 00:54:22.580
But the general provocation
or the conceit of the article

00:54:22.580 --> 00:54:24.650
was to try to see if
the camera was racist.

00:54:28.310 --> 00:54:31.054
Everyone in our group was
high fiving each other, not

00:54:31.054 --> 00:54:33.470
because of the result of that,
but because the article was

00:54:33.470 --> 00:54:34.550
written.

00:54:34.550 --> 00:54:36.690
Because we designed
that product so that

00:54:36.690 --> 00:54:38.990
that article could get written.

00:54:38.990 --> 00:54:41.090
It took a three or
four year out-view

00:54:41.090 --> 00:54:44.660
where we basically were at the
outset of thinking about how

00:54:44.660 --> 00:54:46.645
to design the product.

00:54:46.645 --> 00:54:48.770
We basically were tasked
with collectively thinking

00:54:48.770 --> 00:54:51.230
about the worst
possible applications

00:54:51.230 --> 00:54:54.560
of commoditizable
surveillance technology,

00:54:54.560 --> 00:54:58.340
and how it could be used,
in particular, to be

00:54:58.340 --> 00:55:02.680
harmful to people who were in
more sensitive places in life.

00:55:02.680 --> 00:55:05.480
And I'll just say
one of the personas

00:55:05.480 --> 00:55:08.216
that we used to drive
all of our ideation--

00:55:08.216 --> 00:55:09.590
there were many--
but one of them

00:55:09.590 --> 00:55:11.600
was think through
the individual who

00:55:11.600 --> 00:55:16.130
might be going to transition who
has told some of their people

00:55:16.130 --> 00:55:17.942
in their life, and
not everyone else.

00:55:17.942 --> 00:55:19.400
How would you avoid
unintentionally

00:55:19.400 --> 00:55:20.359
outing that individual?

00:55:20.359 --> 00:55:22.066
So that's an example
of some of the stuff

00:55:22.066 --> 00:55:23.000
that we thought about.

00:55:23.000 --> 00:55:24.250
That just stuff was pervasive.

00:55:24.250 --> 00:55:26.190
So I as the team grew from--

00:55:26.190 --> 00:55:30.050
Jess was maybe the fifth or
sixth person on the team,

00:55:30.050 --> 00:55:32.210
as it grew to a
couple of hundred--

00:55:32.210 --> 00:55:36.650
every single person got that
core question asked to them.

00:55:36.650 --> 00:55:39.350
And it's not like there's a
waterfall or a snowball effect

00:55:39.350 --> 00:55:40.500
that goes into that.

00:55:40.500 --> 00:55:43.070
So that by the time three
or four years rolled around,

00:55:43.070 --> 00:55:46.580
the answer was no, we hadn't
built a racist camera.

00:55:46.580 --> 00:55:48.830
It was the coolest adversarial--

00:55:48.830 --> 00:55:49.340
like, yeah!

00:55:49.340 --> 00:55:51.589
Someone tried to break our
thing because we wanted it.

00:55:56.575 --> 00:55:59.690
CENNYDD BOWLES: I was just
going to comment quickly.

00:55:59.690 --> 00:56:01.670
Eric Martin [INAUDIBLE]
wrote a book called

00:56:01.670 --> 00:56:04.542
"Design For Real Life."

00:56:04.542 --> 00:56:06.500
And one of the techniques
they talk about there

00:56:06.500 --> 00:56:09.230
is the idea for designated
descent, which is sort of maybe

00:56:09.230 --> 00:56:12.680
slightly similar to this
constructive antagonism

00:56:12.680 --> 00:56:14.150
position within a team.

00:56:14.150 --> 00:56:16.040
And that sounds like
a great example.

00:56:16.040 --> 00:56:17.780
The idea of this
time lag is true.

00:56:17.780 --> 00:56:18.366
It exists.

00:56:18.366 --> 00:56:19.490
But it's still our problem.

00:56:19.490 --> 00:56:20.740
We still have to deal with it.

00:56:20.740 --> 00:56:23.120
We're now getting a
better understanding

00:56:23.120 --> 00:56:25.610
of how the trajectory
of technology

00:56:25.610 --> 00:56:30.910
tends to evolve over four
five years of its life,

00:56:30.910 --> 00:56:33.140
when we recognize the
limitations of just focusing

00:56:33.140 --> 00:56:37.230
on that initial
early adopt use case.

00:56:37.230 --> 00:56:39.480
But all of those consequences,
they're not necessarily

00:56:39.480 --> 00:56:40.980
directly anticipatible,
but they are

00:56:40.980 --> 00:56:42.854
within the realm of
possibility, and as such,

00:56:42.854 --> 00:56:44.370
are something we should look at.

00:56:44.370 --> 00:56:46.380
So the designated
decenter are questions

00:56:46.380 --> 00:56:50.700
like that for the clips product,
goes some way to stepping

00:56:50.700 --> 00:56:52.380
in that direction I think.

00:56:52.380 --> 00:56:53.560
KAREN KAUSHANSKY: And
I'll just say there's

00:56:53.560 --> 00:56:54.670
been a couple of articles--

00:56:54.670 --> 00:56:56.800
I think even this past
week-- about data scientists

00:56:56.800 --> 00:56:58.340
and designers working together.

00:56:58.340 --> 00:57:00.040
And I think if we
work together more,

00:57:00.040 --> 00:57:01.570
we'll bring in
those time lapses.

00:57:01.570 --> 00:57:05.970
I think it is understanding
it is collaborative.

00:57:05.970 --> 00:57:07.480
It's a whole team
working together

00:57:07.480 --> 00:57:09.010
to bring something to market.

00:57:12.470 --> 00:57:24.060
AUDIENCE: [INAUDIBLE]
So I've heard the R word

00:57:24.060 --> 00:57:26.210
thrown around a few times.

00:57:26.210 --> 00:57:29.460
That's regulation, in
the context of a GDPR.

00:57:29.460 --> 00:57:33.660
But this is not going to be
the first wave of the internet,

00:57:33.660 --> 00:57:35.820
where aside from a little
bit of legislation,

00:57:35.820 --> 00:57:39.120
intermediary liability, you
could grow a nice big internet

00:57:39.120 --> 00:57:41.010
company and be left alone.

00:57:41.010 --> 00:57:44.216
Regulators are gunning
for AI, both in a positive

00:57:44.216 --> 00:57:45.090
and a negative sense.

00:57:45.090 --> 00:57:47.340
Because of course, there's
also a lot of positive change

00:57:47.340 --> 00:57:47.940
that has to happen.

00:57:47.940 --> 00:57:49.356
If you want
self-driving cars, you

00:57:49.356 --> 00:57:51.700
have change the
road regulations.

00:57:51.700 --> 00:57:53.360
But because of the worries.

00:57:53.360 --> 00:57:55.470
And so the EU is
going to come out

00:57:55.470 --> 00:57:58.560
with a big AI strategy
next month, which

00:57:58.560 --> 00:58:01.050
will include them
developing initially

00:58:01.050 --> 00:58:04.320
soft loss of voluntary
guidelines for ethics,

00:58:04.320 --> 00:58:05.920
sometime in the autumn.

00:58:05.920 --> 00:58:07.320
The Council of
Europe, which also

00:58:07.320 --> 00:58:12.145
includes Russia and Turkey, and
God forbid, even Switzerland--

00:58:14.820 --> 00:58:17.190
Well, that's again,
up to the same people

00:58:17.190 --> 00:58:18.300
who decided on Brexit.

00:58:18.300 --> 00:58:20.340
Because it's the European
Court of Human Rights

00:58:20.340 --> 00:58:21.904
is the council of Europe.

00:58:21.904 --> 00:58:23.820
So if they want to get
out of the jurisdiction

00:58:23.820 --> 00:58:26.970
of the European Court
of Human Rights.

00:58:26.970 --> 00:58:31.720
But so anyways, they're
also developing guidelines.

00:58:31.720 --> 00:58:34.095
There's an expert group
that has [INAUDIBLE]

00:58:34.095 --> 00:58:39.037
and Karen Young in it that will
produce something by next year.

00:58:39.037 --> 00:58:41.370
So you're going to start
seeing these initially soft law

00:58:41.370 --> 00:58:43.530
guidelines, but they
could become a hard law

00:58:43.530 --> 00:58:46.110
on some of the ethical and
human rights aspects of AI.

00:58:46.110 --> 00:58:48.300
I want to ask you all what
those should look like,

00:58:48.300 --> 00:58:52.080
because that's a whole
different day of discussion.

00:58:52.080 --> 00:58:53.880
And you know a lot of
the ideas out there.

00:58:53.880 --> 00:58:56.970
Are there any really
bad ideas out there?

00:58:56.970 --> 00:58:58.740
In addition to
all the good ideas

00:58:58.740 --> 00:59:00.660
that policy makers
want to act on,

00:59:00.660 --> 00:59:03.430
is there a do no harm
principle where we should not

00:59:03.430 --> 00:59:07.690
touch something as policy makers
that you would sort of warn us

00:59:07.690 --> 00:59:08.190
off from?

00:59:12.210 --> 00:59:13.550
SARAH GOLD: Probably a lot.

00:59:13.550 --> 00:59:17.300
I can speak to that 3D example
of the cookie law, which

00:59:17.300 --> 00:59:21.910
was awful, and it has made even
worse with the e-privacy law.

00:59:25.640 --> 00:59:28.190
I won't even try and explain
the history of the cookie law.

00:59:28.190 --> 00:59:30.740
But it's a bad design patent.

00:59:30.740 --> 00:59:31.760
No one really reads it.

00:59:31.760 --> 00:59:32.510
It's really annoying.

00:59:32.510 --> 00:59:34.343
Doesn't really help you
understand the thing

00:59:34.343 --> 00:59:36.080
they're trying to
tell you to do.

00:59:36.080 --> 00:59:38.720
But that then was
written into policy

00:59:38.720 --> 00:59:42.110
that every website had
to have a cookie notice.

00:59:42.110 --> 00:59:44.030
So this really
bad design pattern

00:59:44.030 --> 00:59:46.930
then became regulation
across all websites.

00:59:46.930 --> 00:59:49.070
And pretty much was oh, no.

00:59:49.070 --> 00:59:51.580
Don't talk to me yet until
you've clicked my thing

00:59:51.580 --> 00:59:56.150
and you've agreed that I can put
these cookies onto your laptop

00:59:56.150 --> 00:59:58.620
with really weird interaction.

01:00:01.430 --> 01:00:05.390
We recognize that cookie
law should probably never

01:00:05.390 --> 01:00:06.350
have happened.

01:00:06.350 --> 01:00:09.070
But the emphasis now
been on browsers.

01:00:09.070 --> 01:00:11.300
So soon as you
install a browser,

01:00:11.300 --> 01:00:16.190
you then have to go
through a whole cookie

01:00:16.190 --> 01:00:18.360
kind of process of deciding
your permissions up

01:00:18.360 --> 01:00:20.410
front there in browser.

01:00:20.410 --> 01:00:22.537
And that also isn't really
solving the problem.

01:00:22.537 --> 01:00:24.620
It's just putting the
problem elsewhere and making

01:00:24.620 --> 01:00:26.670
a whole load of other problems.

01:00:26.670 --> 01:00:28.940
So I think if there's one
thing I could say, is let's

01:00:28.940 --> 01:00:30.300
not do that anymore.

01:00:30.300 --> 01:00:32.390
As in, basically
move the problem

01:00:32.390 --> 01:00:34.130
from one area to another.

01:00:34.130 --> 01:00:39.470
And then by doing so, make
even loads of other problems.

01:00:39.470 --> 01:00:40.520
I could be really crass.

01:00:40.520 --> 01:00:41.060
I won't be.

01:00:41.060 --> 01:00:44.310
I think good policy is
made by actually policy

01:00:44.310 --> 01:00:49.340
makers, legal teams, designers,
developers all coming together

01:00:49.340 --> 01:00:51.320
so that we can have a
shared knowledge of what

01:00:51.320 --> 01:00:53.790
each other needs to do, so
we can help each other out.

01:00:53.790 --> 01:00:57.350
I really do think that policy
is so linked to delivery.

01:00:57.350 --> 01:00:59.360
That's the way that
good policy is made.

01:00:59.360 --> 01:01:03.240
So no more privacy laws
or cookie banners, please.

01:01:06.920 --> 01:01:07.820
It is, yeah.

01:01:11.292 --> 01:01:13.770
CENNYDD BOWLES: I am
going to say no, there's

01:01:13.770 --> 01:01:16.167
nothing you shouldn't touch.

01:01:16.167 --> 01:01:17.250
You should have a look at.

01:01:17.250 --> 01:01:19.470
Everything, nothing
should be off the table.

01:01:19.470 --> 01:01:20.620
You have smart people.

01:01:20.620 --> 01:01:22.710
You've got people from industry.

01:01:22.710 --> 01:01:24.820
You got people from academia.

01:01:24.820 --> 01:01:26.420
I'd trust you to
do the right thing.

01:01:26.420 --> 01:01:30.050
I would say don't necessarily
assume that all the tech

01:01:30.050 --> 01:01:32.942
industry is anti-regulation
or pro-self-regulation.

01:01:32.942 --> 01:01:34.400
I wish I could
remember the source.

01:01:34.400 --> 01:01:35.600
I'd have to look it up.

01:01:35.600 --> 01:01:38.480
83% of people in
the general public

01:01:38.480 --> 01:01:40.950
were in favor of increased
regulation of the tech

01:01:40.950 --> 01:01:41.450
industry.

01:01:41.450 --> 01:01:44.750
That figure was 93% if you
worked in the tech industry.

01:01:44.750 --> 01:01:47.522
I'm in the 93%.

01:01:47.522 --> 01:01:50.780
JOSH LOVEJOY: I would
just sort of maybe

01:01:50.780 --> 01:01:53.610
ask for some nuance
maybe in certain areas,

01:01:53.610 --> 01:01:54.600
or a lot of areas.

01:01:54.600 --> 01:01:56.930
But one that comes to mind
on this privacy notion

01:01:56.930 --> 01:02:01.210
is about how we can evaluate
the effectiveness and usefulness

01:02:01.210 --> 01:02:02.820
of a machine learning model.

01:02:02.820 --> 01:02:04.890
So there's a wide gap there.

01:02:04.890 --> 01:02:06.950
And it does come into
having more practitioners

01:02:06.950 --> 01:02:08.720
in the conversation.

01:02:08.720 --> 01:02:11.750
Machine learning model is only
as good as its optimization

01:02:11.750 --> 01:02:13.040
approach.

01:02:13.040 --> 01:02:16.580
And the way that you know how to
figure out what that precision

01:02:16.580 --> 01:02:18.170
recall measurement
looks like, is

01:02:18.170 --> 01:02:22.070
by having a good ways to slice
the results of the model.

01:02:22.070 --> 01:02:24.830
The only way you can slice
the results of the model

01:02:24.830 --> 01:02:28.010
is by having data sets
that are intentionally

01:02:28.010 --> 01:02:31.550
over representatives
of specific groups

01:02:31.550 --> 01:02:33.685
that need to be cared for.

01:02:33.685 --> 01:02:35.870
And so that question of
how do we care for people

01:02:35.870 --> 01:02:37.580
and take care of
one another, we need

01:02:37.580 --> 01:02:39.410
to know who we're
taking care of.

01:02:39.410 --> 01:02:41.870
We have to have some
notion of identity.

01:02:41.870 --> 01:02:45.470
And standard privacy leads us
in a direction of anonymity

01:02:45.470 --> 01:02:46.820
and data blindness.

01:02:46.820 --> 01:02:50.390
That's the default. And we have
to be conscientious about how

01:02:50.390 --> 01:02:55.370
we deal with the
considerations for people

01:02:55.370 --> 01:02:57.140
needing that right
to be forgotten

01:02:57.140 --> 01:03:00.620
and that right to their
privacy, while also being

01:03:00.620 --> 01:03:04.040
able to understand how we're
underperforming for groups that

01:03:04.040 --> 01:03:06.940
will, by definition,
always be at the margins.

01:03:06.940 --> 01:03:10.791
AUDIENCE: We have time
for a very last question.

01:03:10.791 --> 01:03:14.630
KAREN KAUSHANSKY: I was just
going to say, building on that,

01:03:14.630 --> 01:03:17.650
is let's make sure whoever
is steering and putting

01:03:17.650 --> 01:03:21.690
to this process, cover a
lot of the diverse groups

01:03:21.690 --> 01:03:23.500
that we have, as
well as making sure

01:03:23.500 --> 01:03:26.350
that design is part of that.

01:03:26.350 --> 01:03:28.520
Because I think we're
designing this for people.

01:03:28.520 --> 01:03:32.045
And so very often it's
not just engineering

01:03:32.045 --> 01:03:33.170
who should be driving that.

01:03:36.420 --> 01:03:37.580
AUDIENCE: Hello?

01:03:37.580 --> 01:03:38.620
It does work.

01:03:38.620 --> 01:03:42.170
Thank you so much for
your helpful comments.

01:03:42.170 --> 01:03:44.620
So this is just on the
question of explanation

01:03:44.620 --> 01:03:46.960
on whether people
want them or not.

01:03:46.960 --> 01:03:48.880
So I think people can
be owed to things, even

01:03:48.880 --> 01:03:50.670
if they don't want them.

01:03:50.670 --> 01:03:53.290
But the second
thing is with regard

01:03:53.290 --> 01:03:55.090
to whether people do
want explanations.

01:03:55.090 --> 01:03:58.090
We might think that in
some ways they don't

01:03:58.090 --> 01:03:59.890
understand the stakes yet.

01:03:59.890 --> 01:04:02.440
And that the reason that they
might not want information

01:04:02.440 --> 01:04:05.000
is because they believe
that even if they have it,

01:04:05.000 --> 01:04:06.100
they don't have control.

01:04:06.100 --> 01:04:09.130
So they can't alter anything
about that situation.

01:04:09.130 --> 01:04:10.990
I just wonder if that
might be a longer term

01:04:10.990 --> 01:04:14.402
aspiration of this community
to move from explanation

01:04:14.402 --> 01:04:16.360
to designing products
that actually give people

01:04:16.360 --> 01:04:20.155
meaningful control
over their lives?

01:04:20.155 --> 01:04:21.179
SARAH GOLD: Yes.

01:04:21.179 --> 01:04:21.970
I really love that.

01:04:21.970 --> 01:04:24.730
Everything about that
comment, I think was spot on.

01:04:24.730 --> 01:04:29.440
When we've done work looking at
people's mental models of risk

01:04:29.440 --> 01:04:31.253
in medical information--

01:04:31.253 --> 01:04:34.980
[INAUDIBLE] now I can't
use that quote because It's

01:04:34.980 --> 01:04:35.980
from a research session.

01:04:38.335 --> 01:04:40.460
This is difficult talking
about some of this stuff.

01:04:40.460 --> 01:04:41.250
Because you're like, oh!

01:04:41.250 --> 01:04:43.030
That thing that you
can't say about that.

01:04:43.030 --> 01:04:46.140
Which was that people don't
have a good idea of the risks

01:04:46.140 --> 01:04:48.650
at stake, I think, at all.

01:04:51.730 --> 01:04:57.760
That means it's
really hard for people

01:04:57.760 --> 01:04:59.470
to care about
explanations of it.

01:04:59.470 --> 01:05:01.090
So I think your
point is spot on.

01:05:03.604 --> 01:05:05.020
JOSH LOVEJOY: I
think we also fall

01:05:05.020 --> 01:05:08.350
into this trap of
trying to design AI

01:05:08.350 --> 01:05:11.069
as some sort of
universal affordance.

01:05:11.069 --> 01:05:12.610
You can to show up
to a blank screen,

01:05:12.610 --> 01:05:14.560
and then just emote
at it or something.

01:05:14.560 --> 01:05:18.920
And it'll just like, no what
you want, which is crazy.

01:05:18.920 --> 01:05:23.450
And I had this experience
this morning that made that.

01:05:23.450 --> 01:05:26.080
I like to sort of tie these
things into active decisions

01:05:26.080 --> 01:05:27.820
that I have to make.

01:05:27.820 --> 01:05:29.770
I was just walking
along the sidewalk.

01:05:29.770 --> 01:05:33.520
And [INAUDIBLE] she gave
me the insight yesterday

01:05:33.520 --> 01:05:36.160
that kids walk themselves
to school here.

01:05:36.160 --> 01:05:36.880
It's amazing.

01:05:36.880 --> 01:05:38.870
It's not at all how
it happens in America.

01:05:38.870 --> 01:05:39.240
KAREN KAUSHANSKY:
At six-years-old.

01:05:39.240 --> 01:05:41.260
JOSH LOVEJOY: It's unbelievable.

01:05:41.260 --> 01:05:43.660
So this is sort of
pack of little kids

01:05:43.660 --> 01:05:47.410
came at me with--
it wasn't a pack.

01:05:47.410 --> 01:05:49.540
It was four.

01:05:49.540 --> 01:05:51.890
--on little scooters.

01:05:51.890 --> 01:05:54.080
And I was paralyzed,
because I was like, wait.

01:05:54.080 --> 01:05:56.080
Which side of the road
am I supposed to walk on?

01:05:56.080 --> 01:05:57.280
There was that moment
where I was like,

01:05:57.280 --> 01:05:59.200
do I go pass on the
right or on the left?

01:05:59.200 --> 01:06:00.320
I'm in Europe.

01:06:00.320 --> 01:06:02.740
I don't know what to do.

01:06:02.740 --> 01:06:05.410
And so I realized
at that moment I

01:06:05.410 --> 01:06:07.870
was walking up to this
universal interface.

01:06:07.870 --> 01:06:08.920
And I was dumbstruck.

01:06:08.920 --> 01:06:12.540
And so I just stood still
and let the kids go past me.

01:06:12.540 --> 01:06:15.580
And I feel very frequently when
we design UX for these systems,

01:06:15.580 --> 01:06:18.840
we're expecting people to
just know how to use it.

01:06:18.840 --> 01:06:21.280
We come up with skew
morphism and UI design

01:06:21.280 --> 01:06:22.100
to deal with this.

01:06:22.100 --> 01:06:25.000
We come up with a personality
and conversation design

01:06:25.000 --> 01:06:29.470
to sort of try to give people
a sense of what to say at it.

01:06:29.470 --> 01:06:32.620
But more often than not,
systems fail for people

01:06:32.620 --> 01:06:34.644
or aren't thoughtful
about human need.

01:06:34.644 --> 01:06:37.060
And so the reference points
that people bring to the table

01:06:37.060 --> 01:06:39.230
are the thing is
probably going to fail.

01:06:39.230 --> 01:06:40.937
So I'm not going
to try anything.

01:06:40.937 --> 01:06:42.520
So I think one of
the things we can do

01:06:42.520 --> 01:06:46.240
is get more specific
and design systems that

01:06:46.240 --> 01:06:49.720
aren't AI, but are
just tools and products

01:06:49.720 --> 01:06:51.490
and will use normal language.

01:06:51.490 --> 01:06:53.350
And to get back to
the point about magic,

01:06:53.350 --> 01:06:58.450
I think that a
success state when

01:06:58.450 --> 01:07:00.730
we're talking about
generative systems

01:07:00.730 --> 01:07:05.470
or adaptations or machine
learning that helps facilitate

01:07:05.470 --> 01:07:09.340
a useful interaction,
and helps somebody just

01:07:09.340 --> 01:07:13.360
feel like the self-efficacy
has been improved,

01:07:13.360 --> 01:07:15.340
not the trust in the system.

01:07:15.340 --> 01:07:17.560
So more precision in the
targeting and the design.

01:07:17.560 --> 01:07:19.240
The use case is more focused.

01:07:19.240 --> 01:07:21.760
Less about this let the
user sort of figure it out.

01:07:21.760 --> 01:07:27.219
And then I think through that,
we can make it more tractable.

01:07:27.219 --> 01:07:29.760
KAREN KAUSHANSKY: And if we do
that-- just to build on that--

01:07:29.760 --> 01:07:32.740
if we also set the expectations
of what the system can do,

01:07:32.740 --> 01:07:35.320
what it does really
well, then it

01:07:35.320 --> 01:07:37.540
will better set the
expectations for the user

01:07:37.540 --> 01:07:41.590
and we'll have more successes
instead of just more failure.

01:07:41.590 --> 01:07:43.360
Because we've
trying things and it

01:07:43.360 --> 01:07:47.665
really was never designed to do.

01:07:47.665 --> 01:07:49.480
CENNYDD BOWLES: I agree
with the sentiment

01:07:49.480 --> 01:07:51.070
that yes, we should
try and offer

01:07:51.070 --> 01:07:53.741
people control of these systems.

01:07:53.741 --> 01:07:55.240
But realistically,
not everyone will

01:07:55.240 --> 01:07:57.310
have control over
their destinies

01:07:57.310 --> 01:07:59.240
within social systems.

01:07:59.240 --> 01:08:00.490
So where possible, yeah, sure.

01:08:00.490 --> 01:08:01.740
We can try and challenge that.

01:08:01.740 --> 01:08:04.040
We can try and
change that reality,

01:08:04.040 --> 01:08:05.470
that experience for that user.

01:08:05.470 --> 01:08:07.530
But I'm also wary of
offering false hope.

01:08:07.530 --> 01:08:11.010
There are many things that exist
outside of our direct control.

01:08:11.010 --> 01:08:14.260
And I think explainability and
not a shrug of the shoulders,

01:08:14.260 --> 01:08:16.240
but a definite sort
of that's the way it's

01:08:16.240 --> 01:08:18.819
going to be, I'm afraid,
mate, is sometimes going

01:08:18.819 --> 01:08:21.145
to be the solution as well.

01:08:21.145 --> 01:08:22.599
AUDIENCE: I don't know!

01:08:22.599 --> 01:08:24.585
[LAUGHTER]

01:08:24.585 --> 01:08:25.960
KAREN KAUSHANSKY:
Now we're going

01:08:25.960 --> 01:08:27.625
to talk about
opportunities, right?

01:08:27.625 --> 01:08:31.154
[LAUGHTER]

01:08:31.154 --> 01:08:32.529
AUDIENCE: There
is an opportunity

01:08:32.529 --> 01:08:35.710
for drinks and heavy
appetizers about to happen.

01:08:35.710 --> 01:08:39.359
Since that heavy appetizers
isn't a local Zurich punk band

01:08:39.359 --> 01:08:42.649
that we've hired, it's
something we're going to eat.

01:08:42.649 --> 01:08:44.185
But in terms of
opportunities, just

01:08:44.185 --> 01:08:45.310
to editorialize one thing--

01:08:45.310 --> 01:08:48.850
I think one thing to
your question, sir,

01:08:48.850 --> 01:08:51.760
about things to bear in mind--
something that I like to bear

01:08:51.760 --> 01:08:54.609
in mind, or at least
to keep myself sane--

01:08:54.609 --> 01:08:57.220
is one of the things
that technology does

01:08:57.220 --> 01:09:02.529
is it changes the structure of
things for good and for ill.

01:09:02.529 --> 01:09:07.180
And I think that we shouldn't
approach the business models

01:09:07.180 --> 01:09:09.819
for the use of
framings, the ways

01:09:09.819 --> 01:09:13.449
we create products for the
next 20 years in the way

01:09:13.449 --> 01:09:15.580
that we have the last 20 years.

01:09:15.580 --> 01:09:18.160
We have new technologies
coming down the pike,

01:09:18.160 --> 01:09:20.870
not just in terms of machine
learning and learning systems,

01:09:20.870 --> 01:09:23.740
but also in terms of some of
the fundamentals of computing,

01:09:23.740 --> 01:09:27.160
which are going to change the
topology of what we're doing.

01:09:27.160 --> 01:09:31.649
And we can judo flip that
into societies we want,

01:09:31.649 --> 01:09:36.100
or we can shrug and deal
with it in other ways.

01:09:36.100 --> 01:09:42.600
So I think modeling the future
in ways that don't necessarily

01:09:42.600 --> 01:09:45.430
reflect the language of the
past is something that designers

01:09:45.430 --> 01:09:46.830
are quite good at.

01:09:46.830 --> 01:09:49.090
So invites to some
of your meetings.

01:09:49.090 --> 01:09:49.789
Thank you.

01:09:49.789 --> 01:09:52.849
Um, beer and appetizers.

01:09:52.849 --> 01:09:53.640
AUDIENCE: Yes.

01:09:53.640 --> 01:09:56.910
Before we go then, thank
you to our panelists.

01:09:56.910 --> 01:09:58.020
AUDIENCE: Yes, thank you.

01:09:58.020 --> 01:10:00.822
[APPLAUSE]

01:10:00.822 --> 01:10:04.660
AUDIENCE: And thank
you all for coming.

01:10:04.660 --> 01:10:07.920
I think this was a
great opportunity to--

01:10:07.920 --> 01:10:11.400
selfishly speaking-- to
empower the UX community

01:10:11.400 --> 01:10:15.960
to be more intentional
and aware and enticed

01:10:15.960 --> 01:10:20.130
to shape the ecology in
the way that we actually

01:10:20.130 --> 01:10:21.570
wanted to impress society.

01:10:21.570 --> 01:10:24.630
And I think as someone who
is very deep in product,

01:10:24.630 --> 01:10:25.860
I really appreciated that.

01:10:25.860 --> 01:10:27.100
So thank you so much.

01:10:27.100 --> 01:10:29.500
This was great.

01:10:29.500 --> 01:10:30.940
A huge thanks to [INAUDIBLE].

01:10:30.940 --> 01:10:34.030
Where is [INAUDIBLE]?

01:10:34.030 --> 01:10:38.290
Thank you so much, as usual.

01:10:38.290 --> 01:10:41.770
Thanks to the Zurich
facilities and our sponsors,

01:10:41.770 --> 01:10:44.950
both [? pair, ?] as well
as actually [INAUDIBLE],,

01:10:44.950 --> 01:10:49.930
which is a user experience
team for our ads and payments

01:10:49.930 --> 01:10:52.210
and these ugly things.

01:10:52.210 --> 01:10:53.935
I'm just joking.

01:10:53.935 --> 01:10:55.720
I'm just joking.

01:10:55.720 --> 01:10:56.800
But this is really neat--

01:10:56.800 --> 01:10:59.920
an intention to really
kind of create a discourse

01:10:59.920 --> 01:11:02.650
and empower designers
and the researchers

01:11:02.650 --> 01:11:05.260
to really think about how
we're shipping technologies

01:11:05.260 --> 01:11:06.280
in our product.

01:11:06.280 --> 01:11:08.730
And then, last but--

