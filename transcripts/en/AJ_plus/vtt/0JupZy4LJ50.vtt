WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:04.020
Technology is being used to define us.

00:00:04.500 --> 00:00:05.780
And it’s criminalizing us.

00:00:05.980 --> 00:00:07.780
Congress has required

00:00:07.780 --> 00:00:11.700
biometric entry-exit visa tracking systems

00:00:12.280 --> 00:00:13.820
So, you see how fast that was.

00:00:13.820 --> 00:00:14.680
Yeah, instant.

00:00:14.980 --> 00:00:16.820
Federal thieves, terrorists,

00:00:16.820 --> 00:00:18.800
you know, they've all come across our cameras

00:00:19.080 --> 00:00:21.900
It's being done with no oversight.

00:00:22.160 --> 00:00:23.040
No regulation.

00:00:23.220 --> 00:00:25.220
Is facial recognition being used

00:00:25.220 --> 00:00:27.200
to over-police people like you?

00:00:27.260 --> 00:00:28.440
Most definitely.

00:00:35.100 --> 00:00:36.220
And now?

00:00:36.220 --> 00:00:37.846
Now you have to touch the screen.

00:00:37.846 --> 00:00:40.000
Your face is in it.

00:00:40.000 --> 00:00:41.320
So now I'm enrolled, so now I just touch it.

00:00:41.320 --> 00:00:42.280
Get my face there.

00:00:43.760 --> 00:00:45.900
Whoa, that was quick.

00:00:45.900 --> 00:00:47.100
That was less than a second

00:00:47.220 --> 00:00:48.500
I’m Ahmed Shihab-Eldin,

00:00:48.500 --> 00:00:49.760
and I came here to learn how

00:00:49.880 --> 00:00:52.000
facial recognition is evolving rapidly,

00:00:52.080 --> 00:00:54.940
through smartphones, machine learning and algorithms.

00:00:55.260 --> 00:00:57.040
We have tons of different threats coming in everywhere

00:00:57.260 --> 00:00:58.200
so it is very, very critical

00:00:58.200 --> 00:01:00.200
that we do have strong security in place.

00:01:00.620 --> 00:01:02.580
I learned that facial recognition

00:01:02.580 --> 00:01:04.380
is replacing passwords and fingerprints

00:01:04.380 --> 00:01:07.400
preventing online identity theft and increasing security.

00:01:07.840 --> 00:01:10.620
Banks are really looking into this for multiple of reasons

00:01:11.140 --> 00:01:13.220
One is they definitely want to reduce fraud

00:01:13.220 --> 00:01:16.680
But there are concerns about the technology's accuracy and bias,

00:01:17.300 --> 00:01:19.660
Members of law enforcement have facial recognition

00:01:19.660 --> 00:01:21.120
on their cell phones

00:01:21.120 --> 00:01:24.120
with little regulation over how it's actually used.

00:01:25.640 --> 00:01:27.360
San Diego's Police Department

00:01:27.360 --> 00:01:28.720
is one of many in the U.S.

00:01:28.720 --> 00:01:30.440
that use facial recognition.

00:01:33.100 --> 00:01:35.140
It's also Aaron Harvey's home.

00:01:35.720 --> 00:01:37.700
This is Lincoln Park.

00:01:37.700 --> 00:01:39.840
So you have memories all over these streets.

00:01:39.960 --> 00:01:41.880
Growing up, there wasn’t a point in time

00:01:41.880 --> 00:01:44.280
where I didn’t know everyone

00:01:44.280 --> 00:01:46.060
who lived on almost every street.

00:01:46.340 --> 00:01:48.040
When Aaron hit his teens,

00:01:48.040 --> 00:01:50.020
the city became two different worlds.

00:01:50.020 --> 00:01:52.660
Police began stopping him frequently.

00:01:52.700 --> 00:01:56.240
Today, he organizes against gang documentation.

00:01:56.620 --> 00:02:01.600
Aaron tells me facial recognition was used on him during one police stop in 2013

00:02:01.600 --> 00:02:03.640
while he was trying to get to work

00:02:03.760 --> 00:02:06.100
I’m like “Look here man,

00:02:06.100 --> 00:02:07.440
I got my license and registration, insurance.

00:02:07.440 --> 00:02:09.980
I don’t even know why you’re pulling me over.”

00:02:10.440 --> 00:02:13.000
Aaron says police insisted they take his photo.

00:02:13.200 --> 00:02:14.960
I wouldn’t take it, right.

00:02:15.140 --> 00:02:16.260
So then they put me in the car.

00:02:16.260 --> 00:02:17.320
That’s when he was like,

00:02:17.320 --> 00:02:18.120
“Okay. Look, you’re going to take this picture

00:02:18.120 --> 00:02:18.840
or you’re going to go to jail.”

00:02:18.900 --> 00:02:19.780
What went through your mind

00:02:19.780 --> 00:02:22.660
as he was taking the picture?

00:02:22.980 --> 00:02:27.140
I hope that they're not trying to connect me

00:02:27.140 --> 00:02:28.720
into something, right.

00:02:28.980 --> 00:02:31.420
Because why else do you need my picture?

00:02:31.420 --> 00:02:33.840
San Diego police declined to be interviewed

00:02:33.840 --> 00:02:34.580
for this story

00:02:34.580 --> 00:02:36.620
and told AJ+ they have no records

00:02:36.620 --> 00:02:38.720
of using facial recognition on Aaron.

00:02:38.880 --> 00:02:41.200
Aaron told me he’s been stopped by police so many times,

00:02:41.200 --> 00:02:43.380
that he no longer counts.

00:02:43.600 --> 00:02:47.200
You think you're about to die, right.

00:02:48.320 --> 00:02:50.040
Your heart is racing.

00:02:50.040 --> 00:02:52.080
You almost kind of like,

00:02:52.080 --> 00:02:54.060
it's like you got concrete boots on.

00:02:54.060 --> 00:02:55.160
You almost can't even move.

00:02:55.160 --> 00:02:57.740
You're immediately immobilized.

00:02:57.840 --> 00:03:00.660
Because it's a terrifying experience, right.

00:03:01.340 --> 00:03:03.080
Facial recognition software creates

00:03:03.080 --> 00:03:05.640
data points that compare facial features.

00:03:05.760 --> 00:03:08.020
Databases are built from drivers’ licenses,

00:03:08.040 --> 00:03:10.720
mugshots and surveillance video.

00:03:11.120 --> 00:03:13.160
The initial technology was developed

00:03:13.160 --> 00:03:15.100
for the CIA in the ‘60s.

00:03:15.100 --> 00:03:19.100
Today, it’s replacing fingerprinting and police line-ups.

00:03:19.680 --> 00:03:21.340
Facial recognition databases

00:03:21.340 --> 00:03:23.600
used by law enforcement in the U.S.

00:03:23.600 --> 00:03:25.400
include 117 million adults.

00:03:25.400 --> 00:03:28.160
That’s one in three Americans.

00:03:29.900 --> 00:03:31.520
I met up with Clare Garvie,

00:03:31.520 --> 00:03:33.360
co-author of  “The Perpetual Line-Up.”

00:03:33.720 --> 00:03:34.980
She’s a leading expert

00:03:34.980 --> 00:03:37.700
on how law enforcement uses facial recognition.

00:03:37.980 --> 00:03:40.080
We have to start being aware that

00:03:40.200 --> 00:03:42.440
a photograph is no longer just a photograph.

00:03:42.500 --> 00:03:44.240
It’s a point of identity.

00:03:44.580 --> 00:03:46.940
Clare says facial recognition will have a larger impact

00:03:46.940 --> 00:03:49.220
on people of color and women.

00:03:49.860 --> 00:03:52.740
Face recognition was originally a military

00:03:52.740 --> 00:03:55.360
technology that was deployed in Iraq and Afghanistan.

00:03:55.360 --> 00:03:58.460
And we haven't really thought through the process of oh,

00:03:58.460 --> 00:04:00.460
we start deploying it domestically

00:04:00.480 --> 00:04:02.520
What are the different considerations?

00:04:02.520 --> 00:04:06.800
or do we just deploy the same as in a conflict zone?

00:04:07.040 --> 00:04:09.200
A year after Aaron was photographed by police,

00:04:09.200 --> 00:04:12.120
he was arrested in a gang conspiracy case.

00:04:12.300 --> 00:04:13.760
Do you think facial recognition factored into

00:04:13.760 --> 00:04:15.380
you being implicated?

00:04:15.500 --> 00:04:16.400
Most definitely, right.

00:04:16.980 --> 00:04:18.900
Aaron says the District Attorney’s office

00:04:18.900 --> 00:04:19.940
used police stops

00:04:19.940 --> 00:04:22.000
and photos to frame him as a gang member.

00:04:22.080 --> 00:04:24.880
It concerns me that, you know,

00:04:24.880 --> 00:04:27.740
we’re becoming normalized

00:04:27.740 --> 00:04:31.560
to be surveilled and also traumatized.

00:04:32.120 --> 00:04:33.880
He spent nearly seven months in jail

00:04:33.880 --> 00:04:36.700
before the case was dropped due to lack of evidence.

00:04:36.800 --> 00:04:38.540
Was it as rough as I would imagine?

00:04:38.840 --> 00:04:42.360
I lived in a war zone for seven months.

00:04:46.960 --> 00:04:49.700
At KBLK RADIO, Aaron is a regular guest

00:04:49.700 --> 00:04:51.940
who speaks out against gang documentation.

00:04:52.600 --> 00:04:54.000
I went on air with his friends

00:04:54.000 --> 00:04:56.200
to discuss facial recognition and over-policing.

00:04:56.680 --> 00:05:01.680
OK, you got 20 seconds, 20 seconds!

00:05:02.120 --> 00:05:04.240
Twenty-six San Diego law enforcement agencies

00:05:04.240 --> 00:05:06.240
used facial recognition software

00:05:06.240 --> 00:05:09.280
on more than 20,600 occassions.

00:05:09.820 --> 00:05:12.020
No, so, they're just collecting data.

00:05:12.200 --> 00:05:13.900
They're just collecting data.

00:05:14.200 --> 00:05:16.560
The police are a data collection agency.

00:05:16.940 --> 00:05:19.560
There’s no clarity to their tactics at all.

00:05:19.560 --> 00:05:21.440
It’s not like they’re holding seiminars

00:05:21.440 --> 00:05:22.900
or sending public service announcements

00:05:22.900 --> 00:05:24.080
to the schools.

00:05:24.340 --> 00:05:25.200
Or informing you.

00:05:25.400 --> 00:05:27.100
They're not informing you of nothing.

00:05:27.360 --> 00:05:31.520
But when they stop you, they don't tell you why.

00:05:31.540 --> 00:05:32.600
Or what the rights are.

00:05:32.600 --> 00:05:34.620
Parents, tell your kids

00:05:34.620 --> 00:05:36.200
they do not have to their picture taken.

00:05:36.660 --> 00:05:37.720
Because they are getting them

00:05:37.720 --> 00:05:39.000
15 and 16 year olds to get

00:05:39.000 --> 00:05:42.000
their picture taken, and they so scared and shook up,

00:05:42.000 --> 00:05:43.160
they just do it.

00:05:43.220 --> 00:05:46.200
What we are looking to get

00:05:46.200 --> 00:05:48.440
about 100 pixels in between the eyes.

00:05:49.020 --> 00:05:50.260
I wanted to understand

00:05:50.260 --> 00:05:52.660
why law enforcement uses facial recognition

00:05:52.840 --> 00:05:54.680
so I sat down with Peter Trepp,

00:05:54.680 --> 00:05:56.680
the chief executive officer of FaceFirst.

00:05:57.180 --> 00:05:59.460
The company creates a mobile facial recognition app

00:05:59.460 --> 00:06:01.060
used by law enforcement.

00:06:01.440 --> 00:06:03.480
They've got a  pretty long range.

00:06:04.020 --> 00:06:08.960
They can get 80 to 100 feet away from the subjects.

00:06:09.360 --> 00:06:11.660
We’ve caught a number of really bad people out there.

00:06:11.820 --> 00:06:12.320
Really?

00:06:12.540 --> 00:06:13.620
Yeah. Absolutely.

00:06:13.980 --> 00:06:18.380
Child predators, federal thieves, terrorists -

00:06:18.380 --> 00:06:20.400
they’ve all come across our cameras.

00:06:20.520 --> 00:06:22.200
Facial recognition developers say

00:06:22.200 --> 00:06:24.200
the technology has become more accurate

00:06:24.200 --> 00:06:26.100
with the advancement of machine learning.

00:06:26.660 --> 00:06:29.580
Cops are put in all kinds of very difficult positions

00:06:29.580 --> 00:06:31.500
every single day.

00:06:31.500 --> 00:06:34.920
If they can get tools in their hands

00:06:34.960 --> 00:06:37.020
that helps them know who they're talking to,

00:06:37.600 --> 00:06:39.800
What that person may be all about,

00:06:39.800 --> 00:06:41.680
what their intentions are,

00:06:41.680 --> 00:06:43.391
and what their history is.

00:06:43.400 --> 00:06:45.760
I think that helps them do their job better.

00:06:47.380 --> 00:06:50.380
We will have a proper tracking system.

00:06:50.560 --> 00:06:56.320
It will be on land, it will be on sea, it will be in air.

00:06:56.620 --> 00:06:58.840
We’ve all heard about how president Donald Trump

00:06:58.840 --> 00:07:00.400
wants to build a massive new wall

00:07:00.400 --> 00:07:01.560
here on the border with Mexico,

00:07:01.560 --> 00:07:03.360
to crack down on immigration.

00:07:03.580 --> 00:07:04.820
But what you might not know

00:07:04.820 --> 00:07:07.040
is he also wants to build a virtual wall

00:07:07.040 --> 00:07:08.420
with facial recognition

00:07:08.480 --> 00:07:10.620
to track people crossing the border.

00:07:12.020 --> 00:07:15.400
The technology was piloted at the border in 2015.

00:07:15.880 --> 00:07:17.160
But records show that

00:07:17.160 --> 00:07:18.960
Immigration and Customs Enforcement

00:07:18.960 --> 00:07:21.800
had one of the lowest match rates in the region.

00:07:22.220 --> 00:07:24.620
Last year, the Department of Homeland Security

00:07:24.620 --> 00:07:26.420
asked companies in Silicon Valley

00:07:26.420 --> 00:07:29.240
to research technology that could identify people

00:07:29.240 --> 00:07:31.640
through the windows of cars crossing the border.

00:07:32.640 --> 00:07:33.820
Accuracy is an issue

00:07:33.820 --> 00:07:36.440
and low quality images can lead to mismatches.

00:07:36.440 --> 00:07:39.540
In London, when police tested the technology

00:07:39.540 --> 00:07:41.440
at an African-Caribbean celebration;

00:07:41.440 --> 00:07:44.000
it had a 2 percent accuracy rate.

00:07:44.600 --> 00:07:47.920
But in China, the technology has become so advanced

00:07:47.920 --> 00:07:49.800
that authorities identified a person

00:07:49.800 --> 00:07:52.940
in a crowd of 60,000 people. At night.

00:08:06.080 --> 00:08:08.540
The Chinese police released this video

00:08:08.540 --> 00:08:10.460
filmed at a concert in Nanchang city.

00:08:10.480 --> 00:08:14.200
The man traveled 56 miles to the concert with his wife.

00:08:14.760 --> 00:08:15.760
The Chinese government

00:08:15.760 --> 00:08:17.820
accused him of economic crimes.

00:08:18.780 --> 00:08:21.860
The applications for face recognition

00:08:21.860 --> 00:08:25.220
are really open to the imagination

00:08:25.220 --> 00:08:29.220
of the agencies that have access to these databases.

00:08:29.560 --> 00:08:32.120
Student activists organized a protest last fall

00:08:32.120 --> 00:08:35.340
to voice opposition to police and border patrol brutality.

00:08:35.340 --> 00:08:38.640
That led to the development of a new combined

00:08:38.640 --> 00:08:40.640
Black-Chicano history class.

00:08:40.760 --> 00:08:42.920
We need to find ways to work together

00:08:42.920 --> 00:08:45.820
in order for both of our people to be liberated.

00:08:46.140 --> 00:08:48.600
On our last stop, I head to Pillars of the Community,

00:08:48.600 --> 00:08:51.460
where Aaron organizes with Khalid Alexander.

00:08:51.840 --> 00:08:54.720
We've seen lessons from the war on drugs

00:08:54.720 --> 00:08:55.820
and gang documentation.

00:08:55.820 --> 00:08:57.020
All these things and how they intersect.

00:08:57.260 --> 00:09:00.880
Is it possible to think that  facial recognition technology

00:09:00.980 --> 00:09:03.240
won’t be used by law enforcement in this country?

00:09:03.300 --> 00:09:04.780
No, I don’t think it’s too late,

00:09:04.780 --> 00:09:07.220
I think there’s an opportunity to fight back

00:09:07.280 --> 00:09:10.160
and to push back against these types of systems.

00:09:11.260 --> 00:09:12.360
Aaron still has nightmares

00:09:12.360 --> 00:09:14.760
of getting pulled over again by police.

00:09:14.900 --> 00:09:16.820
I don't think we need it.

00:09:16.820 --> 00:09:18.240
Law enforcement has enough tools.

00:09:18.240 --> 00:09:20.400
They’re just casting a wide net.

00:09:20.780 --> 00:09:24.200
We don’t want this. We didn’t even ask for it.

00:09:24.780 --> 00:09:26.800
The work of an organizer is draining,

00:09:26.800 --> 00:09:28.800
but he continues to speak out.

00:09:28.920 --> 00:09:33.620
I’m angry, scared, terrified, afraid, paranoid

00:09:33.640 --> 00:09:36.020
Anything negative you could possibly think of

00:09:36.020 --> 00:09:38.940
of emotions that I feel about them having

00:09:38.940 --> 00:09:40.060
this type of technology.

00:09:40.160 --> 00:09:42.540
Why was my freedom taken away from me?

00:09:42.840 --> 00:09:44.360
Hi guys, it's Ahmed.

00:09:44.360 --> 00:09:47.400
That was episode one of our five-part

00:09:47.400 --> 00:09:49.600
series on the dark side of tech.

00:09:49.600 --> 00:09:51.360
Be sure to watch the rest of the series

00:09:51.360 --> 00:09:54.780
and don't forget to subscribe to AJ+.

