WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.240
 
why does regularization help with

00:00:02.240 --> 00:00:02.250
why does regularization help with
 

00:00:02.250 --> 00:00:03.830
why does regularization help with
overfitting why does it help with

00:00:03.830 --> 00:00:03.840
overfitting why does it help with
 

00:00:03.840 --> 00:00:06.320
overfitting why does it help with
reducing variance problems let's go

00:00:06.320 --> 00:00:06.330
reducing variance problems let's go
 

00:00:06.330 --> 00:00:07.820
reducing variance problems let's go
through a couple examples to gain some

00:00:07.820 --> 00:00:07.830
through a couple examples to gain some
 

00:00:07.830 --> 00:00:11.509
through a couple examples to gain some
intuition about how it works so recall

00:00:11.509 --> 00:00:11.519
intuition about how it works so recall
 

00:00:11.519 --> 00:00:18.370
intuition about how it works so recall
that our high bias high variance and

00:00:18.370 --> 00:00:18.380
 
 

00:00:18.380 --> 00:00:23.090
 
right just right pictures from earlier

00:00:23.090 --> 00:00:23.100
right just right pictures from earlier
 

00:00:23.100 --> 00:00:24.590
right just right pictures from earlier
video I look something like this now

00:00:24.590 --> 00:00:24.600
video I look something like this now
 

00:00:24.600 --> 00:00:27.439
video I look something like this now
let's be a fitting a large and deep

00:00:27.439 --> 00:00:27.449
let's be a fitting a large and deep
 

00:00:27.449 --> 00:00:29.029
let's be a fitting a large and deep
neural network I know I haven't drawn

00:00:29.029 --> 00:00:29.039
neural network I know I haven't drawn
 

00:00:29.039 --> 00:00:30.769
neural network I know I haven't drawn
this one too large or too deep but let's

00:00:30.769 --> 00:00:30.779
this one too large or too deep but let's
 

00:00:30.779 --> 00:00:32.600
this one too large or too deep but let's
do things on your network and is

00:00:32.600 --> 00:00:32.610
do things on your network and is
 

00:00:32.610 --> 00:00:35.420
do things on your network and is
currently overfitting so you have some

00:00:35.420 --> 00:00:35.430
currently overfitting so you have some
 

00:00:35.430 --> 00:00:41.900
currently overfitting so you have some
cost function right J of W B equals some

00:00:41.900 --> 00:00:41.910
cost function right J of W B equals some
 

00:00:41.910 --> 00:00:48.260
cost function right J of W B equals some
of the losses like so all right

00:00:48.260 --> 00:00:48.270
of the losses like so all right
 

00:00:48.270 --> 00:00:50.569
of the losses like so all right
and so what we do for a regularization

00:00:50.569 --> 00:00:50.579
and so what we do for a regularization
 

00:00:50.579 --> 00:00:57.760
and so what we do for a regularization
was add this extra term that penalizes

00:00:57.760 --> 00:00:57.770
was add this extra term that penalizes
 

00:00:57.770 --> 00:01:01.869
was add this extra term that penalizes
the weight matrices from being to launch

00:01:01.869 --> 00:01:01.879
the weight matrices from being to launch
 

00:01:01.879 --> 00:01:04.700
the weight matrices from being to launch
we said that was a for being a small so

00:01:04.700 --> 00:01:04.710
we said that was a for being a small so
 

00:01:04.710 --> 00:01:08.149
we said that was a for being a small so
why is it that shrinking the l2 norm or

00:01:08.149 --> 00:01:08.159
why is it that shrinking the l2 norm or
 

00:01:08.159 --> 00:01:09.920
why is it that shrinking the l2 norm or
the Frobenius norm of the parameters

00:01:09.920 --> 00:01:09.930
the Frobenius norm of the parameters
 

00:01:09.930 --> 00:01:12.800
the Frobenius norm of the parameters
might cause less overfitting one piece

00:01:12.800 --> 00:01:12.810
might cause less overfitting one piece
 

00:01:12.810 --> 00:01:14.899
might cause less overfitting one piece
of intuition is that if you you know

00:01:14.899 --> 00:01:14.909
of intuition is that if you you know
 

00:01:14.909 --> 00:01:16.640
of intuition is that if you you know
crank the regularization lambda to be

00:01:16.640 --> 00:01:16.650
crank the regularization lambda to be
 

00:01:16.650 --> 00:01:18.380
crank the regularization lambda to be
really really base they'll be really

00:01:18.380 --> 00:01:18.390
really really base they'll be really
 

00:01:18.390 --> 00:01:20.899
really really base they'll be really
incentivized to set the weight matrices

00:01:20.899 --> 00:01:20.909
incentivized to set the weight matrices
 

00:01:20.909 --> 00:01:25.039
incentivized to set the weight matrices
W to be reasonably close to zero so one

00:01:25.039 --> 00:01:25.049
W to be reasonably close to zero so one
 

00:01:25.049 --> 00:01:28.429
W to be reasonably close to zero so one
piece of inversion is maybe set the ways

00:01:28.429 --> 00:01:28.439
piece of inversion is maybe set the ways
 

00:01:28.439 --> 00:01:30.830
piece of inversion is maybe set the ways
to be so close to zero for a lot of

00:01:30.830 --> 00:01:30.840
to be so close to zero for a lot of
 

00:01:30.840 --> 00:01:33.230
to be so close to zero for a lot of
hidden units there's basically fevering

00:01:33.230 --> 00:01:33.240
hidden units there's basically fevering
 

00:01:33.240 --> 00:01:35.929
hidden units there's basically fevering
out along the impact of these hidden

00:01:35.929 --> 00:01:35.939
out along the impact of these hidden
 

00:01:35.939 --> 00:01:39.310
out along the impact of these hidden
units an adapter case then you know this

00:01:39.310 --> 00:01:39.320
units an adapter case then you know this
 

00:01:39.320 --> 00:01:42.859
units an adapter case then you know this
much simplified new network becomes a

00:01:42.859 --> 00:01:42.869
much simplified new network becomes a
 

00:01:42.869 --> 00:01:45.200
much simplified new network becomes a
much smaller neural network in fact it's

00:01:45.200 --> 00:01:45.210
much smaller neural network in fact it's
 

00:01:45.210 --> 00:01:46.789
much smaller neural network in fact it's
almost like the logistic regression

00:01:46.789 --> 00:01:46.799
almost like the logistic regression
 

00:01:46.799 --> 00:01:49.219
almost like the logistic regression
Union you know bin stack multiple layers

00:01:49.219 --> 00:01:49.229
Union you know bin stack multiple layers
 

00:01:49.229 --> 00:01:52.039
Union you know bin stack multiple layers
B and so that will take you from this

00:01:52.039 --> 00:01:52.049
B and so that will take you from this
 

00:01:52.049 --> 00:01:55.490
B and so that will take you from this
overfitting case much closer to the left

00:01:55.490 --> 00:01:55.500
overfitting case much closer to the left
 

00:01:55.500 --> 00:01:58.160
overfitting case much closer to the left
towards a high bias case but hopefully

00:01:58.160 --> 00:01:58.170
towards a high bias case but hopefully
 

00:01:58.170 --> 00:01:59.630
towards a high bias case but hopefully
there'll be an intermediate value of

00:01:59.630 --> 00:01:59.640
there'll be an intermediate value of
 

00:01:59.640 --> 00:02:02.240
there'll be an intermediate value of
lambda the results in the result closer

00:02:02.240 --> 00:02:02.250
lambda the results in the result closer
 

00:02:02.250 --> 00:02:04.060
lambda the results in the result closer
to this just right case in the middle

00:02:04.060 --> 00:02:04.070
to this just right case in the middle
 

00:02:04.070 --> 00:02:06.920
to this just right case in the middle
but the intuition is that by cranking up

00:02:06.920 --> 00:02:06.930
but the intuition is that by cranking up
 

00:02:06.930 --> 00:02:09.260
but the intuition is that by cranking up
lambda to be really big it will set W

00:02:09.260 --> 00:02:09.270
lambda to be really big it will set W
 

00:02:09.270 --> 00:02:11.930
lambda to be really big it will set W
close to zero which in practice this

00:02:11.930 --> 00:02:11.940
close to zero which in practice this
 

00:02:11.940 --> 00:02:13.400
close to zero which in practice this
isn't actually what happens the

00:02:13.400 --> 00:02:13.410
isn't actually what happens the
 

00:02:13.410 --> 00:02:16.280
isn't actually what happens the
can think of it as zeroing out or at

00:02:16.280 --> 00:02:16.290
can think of it as zeroing out or at
 

00:02:16.290 --> 00:02:17.870
can think of it as zeroing out or at
least reducing the impacted law the

00:02:17.870 --> 00:02:17.880
least reducing the impacted law the
 

00:02:17.880 --> 00:02:20.240
least reducing the impacted law the
hidden units so you end up with what

00:02:20.240 --> 00:02:20.250
hidden units so you end up with what
 

00:02:20.250 --> 00:02:22.490
hidden units so you end up with what
might feel like a simpler network like

00:02:22.490 --> 00:02:22.500
might feel like a simpler network like
 

00:02:22.500 --> 00:02:24.440
might feel like a simpler network like
this closer and closer to as if you were

00:02:24.440 --> 00:02:24.450
this closer and closer to as if you were
 

00:02:24.450 --> 00:02:26.570
this closer and closer to as if you were
just using logistic progression the

00:02:26.570 --> 00:02:26.580
just using logistic progression the
 

00:02:26.580 --> 00:02:29.150
just using logistic progression the
intuition of completely zeroing out a

00:02:29.150 --> 00:02:29.160
intuition of completely zeroing out a
 

00:02:29.160 --> 00:02:31.100
intuition of completely zeroing out a
bunch of hidden units isn't quite right

00:02:31.100 --> 00:02:31.110
bunch of hidden units isn't quite right
 

00:02:31.110 --> 00:02:32.990
bunch of hidden units isn't quite right
it turns out that what actually happens

00:02:32.990 --> 00:02:33.000
it turns out that what actually happens
 

00:02:33.000 --> 00:02:34.730
it turns out that what actually happens
and it will still use all the hidden

00:02:34.730 --> 00:02:34.740
and it will still use all the hidden
 

00:02:34.740 --> 00:02:36.380
and it will still use all the hidden
units but each of them will just have a

00:02:36.380 --> 00:02:36.390
units but each of them will just have a
 

00:02:36.390 --> 00:02:38.390
units but each of them will just have a
much smaller effect but you do end up

00:02:38.390 --> 00:02:38.400
much smaller effect but you do end up
 

00:02:38.400 --> 00:02:41.540
much smaller effect but you do end up
with a simple network and as if you have

00:02:41.540 --> 00:02:41.550
with a simple network and as if you have
 

00:02:41.550 --> 00:02:43.850
with a simple network and as if you have
a smaller network that is therefore less

00:02:43.850 --> 00:02:43.860
a smaller network that is therefore less
 

00:02:43.860 --> 00:02:45.650
a smaller network that is therefore less
prone to overfitting so I'm not sure

00:02:45.650 --> 00:02:45.660
prone to overfitting so I'm not sure
 

00:02:45.660 --> 00:02:48.140
prone to overfitting so I'm not sure
this intuition helps but when you

00:02:48.140 --> 00:02:48.150
this intuition helps but when you
 

00:02:48.150 --> 00:02:50.240
this intuition helps but when you
implement regularization in the primary

00:02:50.240 --> 00:02:50.250
implement regularization in the primary
 

00:02:50.250 --> 00:02:51.830
implement regularization in the primary
exercise you actually see some of these

00:02:51.830 --> 00:02:51.840
exercise you actually see some of these
 

00:02:51.840 --> 00:02:54.980
exercise you actually see some of these
variance reduction results yourself

00:02:54.980 --> 00:02:54.990
variance reduction results yourself
 

00:02:54.990 --> 00:02:57.140
variance reduction results yourself
here's another attempt at additional

00:02:57.140 --> 00:02:57.150
here's another attempt at additional
 

00:02:57.150 --> 00:03:00.590
here's another attempt at additional
intuition for why regularization helps

00:03:00.590 --> 00:03:00.600
intuition for why regularization helps
 

00:03:00.600 --> 00:03:02.570
intuition for why regularization helps
prevent overfitting and for this I'm

00:03:02.570 --> 00:03:02.580
prevent overfitting and for this I'm
 

00:03:02.580 --> 00:03:06.070
prevent overfitting and for this I'm
going to assume that we're using the 10h

00:03:06.070 --> 00:03:06.080
going to assume that we're using the 10h
 

00:03:06.080 --> 00:03:08.000
going to assume that we're using the 10h
activation function which looks like

00:03:08.000 --> 00:03:08.010
activation function which looks like
 

00:03:08.010 --> 00:03:11.060
activation function which looks like
this right so there's a G of Z equals 10

00:03:11.060 --> 00:03:11.070
this right so there's a G of Z equals 10
 

00:03:11.070 --> 00:03:16.190
this right so there's a G of Z equals 10
H of Z so if that's the case notice that

00:03:16.190 --> 00:03:16.200
H of Z so if that's the case notice that
 

00:03:16.200 --> 00:03:20.090
H of Z so if that's the case notice that
so long as Z is quite small so the Z

00:03:20.090 --> 00:03:20.100
so long as Z is quite small so the Z
 

00:03:20.100 --> 00:03:22.640
so long as Z is quite small so the Z
takes on only a smallish range of

00:03:22.640 --> 00:03:22.650
takes on only a smallish range of
 

00:03:22.650 --> 00:03:25.220
takes on only a smallish range of
parameters maybe around here then you're

00:03:25.220 --> 00:03:25.230
parameters maybe around here then you're
 

00:03:25.230 --> 00:03:27.020
parameters maybe around here then you're
just using the linear regime of the

00:03:27.020 --> 00:03:27.030
just using the linear regime of the
 

00:03:27.030 --> 00:03:29.210
just using the linear regime of the
Technische function there's only a Z is

00:03:29.210 --> 00:03:29.220
Technische function there's only a Z is
 

00:03:29.220 --> 00:03:31.820
Technische function there's only a Z is
allowed to wonder up you know to larger

00:03:31.820 --> 00:03:31.830
allowed to wonder up you know to larger
 

00:03:31.830 --> 00:03:34.340
allowed to wonder up you know to larger
values or smaller values like so that

00:03:34.340 --> 00:03:34.350
values or smaller values like so that
 

00:03:34.350 --> 00:03:36.110
values or smaller values like so that
the activation function starts to become

00:03:36.110 --> 00:03:36.120
the activation function starts to become
 

00:03:36.120 --> 00:03:38.630
the activation function starts to become
less linear so the intuition you might

00:03:38.630 --> 00:03:38.640
less linear so the intuition you might
 

00:03:38.640 --> 00:03:40.280
less linear so the intuition you might
take away from this is that it launder

00:03:40.280 --> 00:03:40.290
take away from this is that it launder
 

00:03:40.290 --> 00:03:42.260
take away from this is that it launder
the regularization parameter is launched

00:03:42.260 --> 00:03:42.270
the regularization parameter is launched
 

00:03:42.270 --> 00:03:45.380
the regularization parameter is launched
then you have that your parameters will

00:03:45.380 --> 00:03:45.390
then you have that your parameters will
 

00:03:45.390 --> 00:03:47.060
then you have that your parameters will
be relatively small because they are

00:03:47.060 --> 00:03:47.070
be relatively small because they are
 

00:03:47.070 --> 00:03:50.479
be relatively small because they are
penalized to be large in the cost

00:03:50.479 --> 00:03:50.489
penalized to be large in the cost
 

00:03:50.489 --> 00:03:53.390
penalized to be large in the cost
function and so the weights W are small

00:03:53.390 --> 00:03:53.400
function and so the weights W are small
 

00:03:53.400 --> 00:04:00.650
function and so the weights W are small
then because Z is equal to w right and

00:04:00.650 --> 00:04:00.660
then because Z is equal to w right and
 

00:04:00.660 --> 00:04:03.320
then because Z is equal to w right and
then technically plus B or but if W

00:04:03.320 --> 00:04:03.330
then technically plus B or but if W
 

00:04:03.330 --> 00:04:05.690
then technically plus B or but if W
tells you very small then Z will also be

00:04:05.690 --> 00:04:05.700
tells you very small then Z will also be
 

00:04:05.700 --> 00:04:08.540
tells you very small then Z will also be
low to be small and in particular is Z

00:04:08.540 --> 00:04:08.550
low to be small and in particular is Z
 

00:04:08.550 --> 00:04:10.460
low to be small and in particular is Z
ends up taking relatively small values

00:04:10.460 --> 00:04:10.470
ends up taking relatively small values
 

00:04:10.470 --> 00:04:14.300
ends up taking relatively small values
just invicible range then G of Z will be

00:04:14.300 --> 00:04:14.310
just invicible range then G of Z will be
 

00:04:14.310 --> 00:04:19.900
just invicible range then G of Z will be
roughly linear so it's as if every layer

00:04:19.900 --> 00:04:19.910
 
 

00:04:19.910 --> 00:04:21.170
 
will be

00:04:21.170 --> 00:04:21.180
will be
 

00:04:21.180 --> 00:04:23.779
will be
roughly linear as it is just linear

00:04:23.779 --> 00:04:23.789
roughly linear as it is just linear
 

00:04:23.789 --> 00:04:26.420
roughly linear as it is just linear
regression and we saw on course one that

00:04:26.420 --> 00:04:26.430
regression and we saw on course one that
 

00:04:26.430 --> 00:04:28.999
regression and we saw on course one that
if every layer is linear then your whole

00:04:28.999 --> 00:04:29.009
if every layer is linear then your whole
 

00:04:29.009 --> 00:04:31.610
if every layer is linear then your whole
network is just a linear network and so

00:04:31.610 --> 00:04:31.620
network is just a linear network and so
 

00:04:31.620 --> 00:04:33.590
network is just a linear network and so
even a very deep network but a deep

00:04:33.590 --> 00:04:33.600
even a very deep network but a deep
 

00:04:33.600 --> 00:04:35.090
even a very deep network but a deep
network where the linear activation

00:04:35.090 --> 00:04:35.100
network where the linear activation
 

00:04:35.100 --> 00:04:37.610
network where the linear activation
function is at the end they only able to

00:04:37.610 --> 00:04:37.620
function is at the end they only able to
 

00:04:37.620 --> 00:04:39.650
function is at the end they only able to
compute the linear function so it's not

00:04:39.650 --> 00:04:39.660
compute the linear function so it's not
 

00:04:39.660 --> 00:04:41.749
compute the linear function so it's not
able to you know fit those very very

00:04:41.749 --> 00:04:41.759
able to you know fit those very very
 

00:04:41.759 --> 00:04:44.930
able to you know fit those very very
complicated decisions very nonlinear

00:04:44.930 --> 00:04:44.940
complicated decisions very nonlinear
 

00:04:44.940 --> 00:04:48.200
complicated decisions very nonlinear
decision boundaries that allow it to you

00:04:48.200 --> 00:04:48.210
decision boundaries that allow it to you
 

00:04:48.210 --> 00:04:50.810
decision boundaries that allow it to you
know really a over fit right the

00:04:50.810 --> 00:04:50.820
know really a over fit right the
 

00:04:50.820 --> 00:04:54.080
know really a over fit right the
datasets like we saw on the overfitting

00:04:54.080 --> 00:04:54.090
datasets like we saw on the overfitting
 

00:04:54.090 --> 00:04:56.810
datasets like we saw on the overfitting
high variance case on the previous slide

00:04:56.810 --> 00:04:56.820
high variance case on the previous slide
 

00:04:56.820 --> 00:04:59.900
high variance case on the previous slide
so just to summarize um if the

00:04:59.900 --> 00:04:59.910
so just to summarize um if the
 

00:04:59.910 --> 00:05:01.939
so just to summarize um if the
regularization term is very large the

00:05:01.939 --> 00:05:01.949
regularization term is very large the
 

00:05:01.949 --> 00:05:05.180
regularization term is very large the
parameters W very small so Z will be

00:05:05.180 --> 00:05:05.190
parameters W very small so Z will be
 

00:05:05.190 --> 00:05:07.100
parameters W very small so Z will be
relatively small kind of ignoring the

00:05:07.100 --> 00:05:07.110
relatively small kind of ignoring the
 

00:05:07.110 --> 00:05:09.560
relatively small kind of ignoring the
effect would be for now but so Z is

00:05:09.560 --> 00:05:09.570
effect would be for now but so Z is
 

00:05:09.570 --> 00:05:12.560
effect would be for now but so Z is
relatively so Z be relatively small or

00:05:12.560 --> 00:05:12.570
relatively so Z be relatively small or
 

00:05:12.570 --> 00:05:14.810
relatively so Z be relatively small or
really should say it takes on a small

00:05:14.810 --> 00:05:14.820
really should say it takes on a small
 

00:05:14.820 --> 00:05:18.050
really should say it takes on a small
range of values and so the activation

00:05:18.050 --> 00:05:18.060
range of values and so the activation
 

00:05:18.060 --> 00:05:20.390
range of values and so the activation
function this chain HCA will be

00:05:20.390 --> 00:05:20.400
function this chain HCA will be
 

00:05:20.400 --> 00:05:22.490
function this chain HCA will be
relatively linear and so your whole

00:05:22.490 --> 00:05:22.500
relatively linear and so your whole
 

00:05:22.500 --> 00:05:24.080
relatively linear and so your whole
neural network will be computing

00:05:24.080 --> 00:05:24.090
neural network will be computing
 

00:05:24.090 --> 00:05:26.750
neural network will be computing
something not too far from a big linear

00:05:26.750 --> 00:05:26.760
something not too far from a big linear
 

00:05:26.760 --> 00:05:28.490
something not too far from a big linear
function which is therefore a pretty

00:05:28.490 --> 00:05:28.500
function which is therefore a pretty
 

00:05:28.500 --> 00:05:30.379
function which is therefore a pretty
simple function about in a very complex

00:05:30.379 --> 00:05:30.389
simple function about in a very complex
 

00:05:30.389 --> 00:05:32.629
simple function about in a very complex
highly nonlinear function and so it's

00:05:32.629 --> 00:05:32.639
highly nonlinear function and so it's
 

00:05:32.639 --> 00:05:35.000
highly nonlinear function and so it's
also much less able to open it and again

00:05:35.000 --> 00:05:35.010
also much less able to open it and again
 

00:05:35.010 --> 00:05:36.800
also much less able to open it and again
when you implement regularization for

00:05:36.800 --> 00:05:36.810
when you implement regularization for
 

00:05:36.810 --> 00:05:39.200
when you implement regularization for
yourself in the current exercise you'll

00:05:39.200 --> 00:05:39.210
yourself in the current exercise you'll
 

00:05:39.210 --> 00:05:40.760
yourself in the current exercise you'll
be able to see some of these effects

00:05:40.760 --> 00:05:40.770
be able to see some of these effects
 

00:05:40.770 --> 00:05:43.629
be able to see some of these effects
yourself before wrapping up our

00:05:43.629 --> 00:05:43.639
yourself before wrapping up our
 

00:05:43.639 --> 00:05:46.100
yourself before wrapping up our
discussion on regularization I just want

00:05:46.100 --> 00:05:46.110
discussion on regularization I just want
 

00:05:46.110 --> 00:05:47.960
discussion on regularization I just want
to give you one implementational tip

00:05:47.960 --> 00:05:47.970
to give you one implementational tip
 

00:05:47.970 --> 00:05:51.320
to give you one implementational tip
which is that when influencing

00:05:51.320 --> 00:05:51.330
which is that when influencing
 

00:05:51.330 --> 00:05:53.390
which is that when influencing
regularization we took our definition of

00:05:53.390 --> 00:05:53.400
regularization we took our definition of
 

00:05:53.400 --> 00:05:57.800
regularization we took our definition of
the cost function J and we actually

00:05:57.800 --> 00:05:57.810
the cost function J and we actually
 

00:05:57.810 --> 00:06:01.159
the cost function J and we actually
modified it by adding this extra term

00:06:01.159 --> 00:06:01.169
modified it by adding this extra term
 

00:06:01.169 --> 00:06:05.330
modified it by adding this extra term
that penalizes the waste being too large

00:06:05.330 --> 00:06:05.340
that penalizes the waste being too large
 

00:06:05.340 --> 00:06:09.430
that penalizes the waste being too large
and so if you implement gradient descent

00:06:09.430 --> 00:06:09.440
and so if you implement gradient descent
 

00:06:09.440 --> 00:06:12.439
and so if you implement gradient descent
one of the steps to debug gradient

00:06:12.439 --> 00:06:12.449
one of the steps to debug gradient
 

00:06:12.449 --> 00:06:16.909
one of the steps to debug gradient
descent is to plot the cost function J

00:06:16.909 --> 00:06:16.919
descent is to plot the cost function J
 

00:06:16.919 --> 00:06:19.339
descent is to plot the cost function J
as a function of the number of

00:06:19.339 --> 00:06:19.349
as a function of the number of
 

00:06:19.349 --> 00:06:21.200
as a function of the number of
iterations of gradient descent and you

00:06:21.200 --> 00:06:21.210
iterations of gradient descent and you
 

00:06:21.210 --> 00:06:23.240
iterations of gradient descent and you
want to see that the cost function J

00:06:23.240 --> 00:06:23.250
want to see that the cost function J
 

00:06:23.250 --> 00:06:26.180
want to see that the cost function J
your decreases monotonically after every

00:06:26.180 --> 00:06:26.190
your decreases monotonically after every
 

00:06:26.190 --> 00:06:27.530
your decreases monotonically after every
iteration of gradient

00:06:27.530 --> 00:06:27.540
iteration of gradient
 

00:06:27.540 --> 00:06:29.630
iteration of gradient
and if you're implementing

00:06:29.630 --> 00:06:29.640
and if you're implementing
 

00:06:29.640 --> 00:06:32.300
and if you're implementing
regularization then please remember that

00:06:32.300 --> 00:06:32.310
regularization then please remember that
 

00:06:32.310 --> 00:06:35.780
regularization then please remember that
J now has this new definition if you

00:06:35.780 --> 00:06:35.790
J now has this new definition if you
 

00:06:35.790 --> 00:06:38.120
J now has this new definition if you
plot the old definition of J just this

00:06:38.120 --> 00:06:38.130
plot the old definition of J just this
 

00:06:38.130 --> 00:06:40.430
plot the old definition of J just this
first term then you might not see a

00:06:40.430 --> 00:06:40.440
first term then you might not see a
 

00:06:40.440 --> 00:06:42.980
first term then you might not see a
decrease monotonically so to divide

00:06:42.980 --> 00:06:42.990
decrease monotonically so to divide
 

00:06:42.990 --> 00:06:44.450
decrease monotonically so to divide
gradients and make sure you're plotting

00:06:44.450 --> 00:06:44.460
gradients and make sure you're plotting
 

00:06:44.460 --> 00:06:47.240
gradients and make sure you're plotting
you know this new definition of J that

00:06:47.240 --> 00:06:47.250
you know this new definition of J that
 

00:06:47.250 --> 00:06:49.240
you know this new definition of J that
includes this second term as well

00:06:49.240 --> 00:06:49.250
includes this second term as well
 

00:06:49.250 --> 00:06:51.770
includes this second term as well
otherwise you might not see J decrease

00:06:51.770 --> 00:06:51.780
otherwise you might not see J decrease
 

00:06:51.780 --> 00:06:53.390
otherwise you might not see J decrease
monotonically on every single iteration

00:06:53.390 --> 00:06:53.400
monotonically on every single iteration
 

00:06:53.400 --> 00:06:56.570
monotonically on every single iteration
so that's it for l2 regularization which

00:06:56.570 --> 00:06:56.580
so that's it for l2 regularization which
 

00:06:56.580 --> 00:06:58.160
so that's it for l2 regularization which
is actually a regularization technique

00:06:58.160 --> 00:06:58.170
is actually a regularization technique
 

00:06:58.170 --> 00:07:00.230
is actually a regularization technique
that I use the most in training people

00:07:00.230 --> 00:07:00.240
that I use the most in training people
 

00:07:00.240 --> 00:07:02.480
that I use the most in training people
learning models in deep learning does

00:07:02.480 --> 00:07:02.490
learning models in deep learning does
 

00:07:02.490 --> 00:07:04.490
learning models in deep learning does
another sometimes use regularization

00:07:04.490 --> 00:07:04.500
another sometimes use regularization
 

00:07:04.500 --> 00:07:06.170
another sometimes use regularization
techniques called drop out

00:07:06.170 --> 00:07:06.180
techniques called drop out
 

00:07:06.180 --> 00:07:08.240
techniques called drop out
regularization let's take a look at that

00:07:08.240 --> 00:07:08.250
regularization let's take a look at that
 

00:07:08.250 --> 00:07:11.060
regularization let's take a look at that
in the next video

