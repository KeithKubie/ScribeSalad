WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.948
[MUSIC PLAYING]

00:00:08.290 --> 00:00:12.440
SPEAKER 1: All right, well,
thank you everyone for coming.

00:00:12.440 --> 00:00:17.530
Today, we're delighted to
welcome Ramesh Srinivasan.

00:00:17.530 --> 00:00:23.080
Ramesh studies the intersection
of technology, politics,

00:00:23.080 --> 00:00:24.770
and society.

00:00:24.770 --> 00:00:29.410
Since 2005, he's been a member
of the faculty and professor

00:00:29.410 --> 00:00:33.940
at UCLA in the Information
Studies Department.

00:00:33.940 --> 00:00:38.110
Prior to that, he took
his PhD from Harvard,

00:00:38.110 --> 00:00:43.180
his master's from MIT, and
his bachelor's from Stanford

00:00:43.180 --> 00:00:46.270
and then went on to become
a fellow at the MIT Media

00:00:46.270 --> 00:00:50.500
Lab and a teaching fellow
at Harvard's Graduate

00:00:50.500 --> 00:00:53.150
School of Design.

00:00:53.150 --> 00:00:57.460
He's here today to discuss
his first book, "Whose Global

00:00:57.460 --> 00:00:58.540
Village?

00:00:58.540 --> 00:01:02.830
Rethinking How Technology
Shapes Our World."

00:01:02.830 --> 00:01:06.910
The book is a sort
of call to action

00:01:06.910 --> 00:01:13.180
to include marginalized,
non-western countries

00:01:13.180 --> 00:01:15.270
in the digital revolution.

00:01:15.270 --> 00:01:18.550
And so without any further
ado, please join me

00:01:18.550 --> 00:01:21.320
in welcoming to Google
Ramesh Srinivasan.

00:01:21.320 --> 00:01:22.730
[APPLAUSE]

00:01:24.140 --> 00:01:26.490
RAMESH SRINIVASAN: Thank you.

00:01:26.490 --> 00:01:30.070
All right, it's absolutely
a pleasure to be here.

00:01:30.070 --> 00:01:33.540
I have a bunch of friends at
Google, two of whom are here,

00:01:33.540 --> 00:01:35.190
and my new friend David.

00:01:35.190 --> 00:01:38.550
I wanted to thank you all
for making the time to listen

00:01:38.550 --> 00:01:39.790
to some of these thoughts.

00:01:39.790 --> 00:01:41.290
What I'm going to
do is set a timer.

00:01:41.290 --> 00:01:44.280
And I'm going to go about
45 minutes, maybe even less,

00:01:44.280 --> 00:01:45.720
and then we'll
have a conversation

00:01:45.720 --> 00:01:47.140
about this material.

00:01:47.140 --> 00:01:50.544
So what you can do also
is if there are thoughts,

00:01:50.544 --> 00:01:51.960
or reactions, or
comments that you

00:01:51.960 --> 00:01:54.700
have to any of this
material, by all means,

00:01:54.700 --> 00:01:56.880
you can tweet about
it and link it to me,

00:01:56.880 --> 00:01:59.590
and we can have a kind of
asynchronous conversation

00:01:59.590 --> 00:02:00.889
afterward.

00:02:00.889 --> 00:02:02.430
And you can also
get in touch with me

00:02:02.430 --> 00:02:05.130
through my email address
and my website, which are

00:02:05.130 --> 00:02:08.190
listed on this link right here.

00:02:08.190 --> 00:02:14.350
OK so many of us know what this
diagram represents, but let me

00:02:14.350 --> 00:02:15.940
kind of unpack it a little bit.

00:02:15.940 --> 00:02:19.420
This represents the fiber
optic cable infrastructure

00:02:19.420 --> 00:02:22.000
that provides sort of the
backhaul, if you will,

00:02:22.000 --> 00:02:26.020
the very basis of
what the internet is.

00:02:26.020 --> 00:02:29.830
And, you know, many
of us think about how

00:02:29.830 --> 00:02:33.420
this sort of environment and
this landscape is shifting.

00:02:33.420 --> 00:02:37.210
But it's very important to note
when you see diagrams like this

00:02:37.210 --> 00:02:40.030
that the internet is
actually not immaterial,

00:02:40.030 --> 00:02:41.170
but it's actually material.

00:02:41.170 --> 00:02:44.230
It's rooted in the
things we build

00:02:44.230 --> 00:02:46.060
and the places we connect.

00:02:46.060 --> 00:02:49.390
But what's also striking when
you see a diagram like this

00:02:49.390 --> 00:02:51.670
is how the networks
that are actually

00:02:51.670 --> 00:02:54.712
drawn by fiber optic cables
that represent the internet,

00:02:54.712 --> 00:02:56.920
and there's also, of course,
satellites that are part

00:02:56.920 --> 00:03:00.160
of the internet, as well,
represent the places that

00:03:00.160 --> 00:03:02.890
are already, in a sense,
very well connected

00:03:02.890 --> 00:03:04.450
in our world today, right?

00:03:04.450 --> 00:03:07.180
So if you look at a map, for
example, of plane flights,

00:03:07.180 --> 00:03:09.820
you'll see that there'll be
some pretty high correlation

00:03:09.820 --> 00:03:13.090
between plane flights and other
forms of traffic and exchange

00:03:13.090 --> 00:03:14.477
and these mappings as well.

00:03:14.477 --> 00:03:16.060
So that explains why
we see, you know,

00:03:16.060 --> 00:03:17.590
the West Coast of
the United States,

00:03:17.590 --> 00:03:20.680
for example, well
connected with,

00:03:20.680 --> 00:03:23.650
you know, Shanghai and
Beijing in this map.

00:03:23.650 --> 00:03:26.200
It also explains why we see
New York City well connected

00:03:26.200 --> 00:03:28.510
with London and Western Europe.

00:03:28.510 --> 00:03:32.140
But what's particularly notable
when you look at this diagram

00:03:32.140 --> 00:03:34.750
is what is not
connected and what

00:03:34.750 --> 00:03:37.890
is not connected to what,
particularly speaking,

00:03:37.890 --> 00:03:41.620
that we're talking about just a
couple fiber optic cables that

00:03:41.620 --> 00:03:45.970
connect the two major
continents of the global south,

00:03:45.970 --> 00:03:49.040
right, that of Africa
and South America.

00:03:49.040 --> 00:03:52.970
So what that means is this sort
of global village, so to speak,

00:03:52.970 --> 00:03:55.480
which is, you know, partly
the title of my book, a term

00:03:55.480 --> 00:03:58.690
in my book, has not
actually come to be,

00:03:58.690 --> 00:04:01.150
that if anything, this
vision that Marshall

00:04:01.150 --> 00:04:04.030
McLuhan and many others had of
sort of an internet that brings

00:04:04.030 --> 00:04:06.170
everybody together in
equitable, and flattened,

00:04:06.170 --> 00:04:09.040
and democratic
fashions, has not really

00:04:09.040 --> 00:04:12.520
been the experience on a very,
very material and specific

00:04:12.520 --> 00:04:14.180
level itself.

00:04:14.180 --> 00:04:17.140
So to kind of continue,
let's think about one

00:04:17.140 --> 00:04:19.291
of the major
metaphors by which we

00:04:19.291 --> 00:04:21.040
think about the internet
today and that we

00:04:21.040 --> 00:04:23.590
think about data today more
broadly, which is the cloud,

00:04:23.590 --> 00:04:24.310
right?

00:04:24.310 --> 00:04:27.430
So as we all know, as we
think about the of metaphor

00:04:27.430 --> 00:04:30.340
in our lives, metaphors both
open up our understandings,

00:04:30.340 --> 00:04:33.430
but in some cases, they also
close our understandings,

00:04:33.430 --> 00:04:34.090
right?

00:04:34.090 --> 00:04:35.506
So when you think
about the cloud,

00:04:35.506 --> 00:04:37.840
you say, hey, who has a
problem with the cloud.

00:04:37.840 --> 00:04:39.670
The cloud is immaterial.

00:04:39.670 --> 00:04:41.050
The cloud is made of water.

00:04:41.050 --> 00:04:42.460
Our bodies are made of water.

00:04:42.460 --> 00:04:44.180
Clouds are everywhere, right?

00:04:44.180 --> 00:04:46.570
Who has a problem
with cloud itself?

00:04:46.570 --> 00:04:49.210
But, of course, as we
all know here at Google,

00:04:49.210 --> 00:04:51.280
but we also know
in other companies,

00:04:51.280 --> 00:04:54.670
the cloud is very much
transacted and determined

00:04:54.670 --> 00:04:57.880
by a few companies and their
terms of service in relation

00:04:57.880 --> 00:04:59.950
to personal data, right?

00:04:59.950 --> 00:05:03.370
And so the reason I have
these three logos here

00:05:03.370 --> 00:05:05.620
on this map,
including your own, is

00:05:05.620 --> 00:05:07.667
because in the
country of Mexico,

00:05:07.667 --> 00:05:09.250
and I'll speak about
this very briefly

00:05:09.250 --> 00:05:13.750
in this talk today, where I'm
currently doing my field work,

00:05:13.750 --> 00:05:14.990
it's really fascinating.

00:05:14.990 --> 00:05:17.140
There have been some
informal studies done,

00:05:17.140 --> 00:05:20.350
but emails and various forms
of specific communication

00:05:20.350 --> 00:05:23.230
that are sent between one
neighbor and their neighbor

00:05:23.230 --> 00:05:26.110
just next to them
are often forms

00:05:26.110 --> 00:05:30.160
of data that are transacted
on cloud-based servers

00:05:30.160 --> 00:05:31.410
through these three companies.

00:05:31.410 --> 00:05:33.880
So we all know this,
but if you send an email

00:05:33.880 --> 00:05:35.500
to someone who's
right next to you,

00:05:35.500 --> 00:05:37.660
that email might actually
go through a server

00:05:37.660 --> 00:05:39.520
that's outside your
country and that

00:05:39.520 --> 00:05:42.460
is part of a private corporate
kind of proprietary terms

00:05:42.460 --> 00:05:43.490
of service.

00:05:43.490 --> 00:05:45.310
So I think it's
important to read

00:05:45.310 --> 00:05:49.450
our experiences of the internet
more largely and data as well

00:05:49.450 --> 00:05:52.060
through the specific
mechanisms by which it's

00:05:52.060 --> 00:05:54.820
transacted and arranged.

00:05:54.820 --> 00:05:56.250
The reason why I
say that is we're

00:05:56.250 --> 00:05:58.420
at an incredibly
staggering moment,

00:05:58.420 --> 00:06:00.340
I would argue an
inflection point,

00:06:00.340 --> 00:06:01.870
and how we think
about everything

00:06:01.870 --> 00:06:05.680
from how money is made, to
how labor is accumulated,

00:06:05.680 --> 00:06:07.900
to how services
are drawn, right?

00:06:07.900 --> 00:06:10.840
We're at a point right now where
platforms, and specifically

00:06:10.840 --> 00:06:14.170
the so-called looseness
or flexibility

00:06:14.170 --> 00:06:18.400
of platform infrastructures,
mean, everything, right?

00:06:18.400 --> 00:06:20.680
So you think about these
particular examples,

00:06:20.680 --> 00:06:22.870
and you can substitute
very easily Alibaba,

00:06:22.870 --> 00:06:25.990
right, the retailer one
won in yellow there,

00:06:25.990 --> 00:06:27.910
with Amazon as well.

00:06:27.910 --> 00:06:30.850
And you can actually realize
that ownership and labor

00:06:30.850 --> 00:06:33.520
are actually, in a large sense,
something of the past, right?

00:06:33.520 --> 00:06:36.610
Incredible amounts of money
are accumulated simply

00:06:36.610 --> 00:06:40.000
by being some sort of
transactional agent online.

00:06:40.000 --> 00:06:43.270
And I think that that is really,
really important because that

00:06:43.270 --> 00:06:44.530
of course is brilliant.

00:06:44.530 --> 00:06:45.560
It's disruptive.

00:06:45.560 --> 00:06:49.330
It's an example of an incredibly
creative and efficient

00:06:49.330 --> 00:06:51.670
technological
advancement, but it also

00:06:51.670 --> 00:06:56.270
has massive, massive, massive
social and economic effects.

00:06:56.270 --> 00:06:59.360
And we need to be responsible
and aware of those effects.

00:06:59.360 --> 00:07:02.860
So I'm very happy to know
that there is increasingly

00:07:02.860 --> 00:07:06.400
a movement toward ethics and
AI, and ethics and algorithms,

00:07:06.400 --> 00:07:09.250
and ethics and thinking about
the effects of technology,

00:07:09.250 --> 00:07:11.120
not just when you
click on something,

00:07:11.120 --> 00:07:13.520
but more rooted
in our societies.

00:07:13.520 --> 00:07:16.490
And we can see that from
everything from our rent prices

00:07:16.490 --> 00:07:19.790
in San Francisco to
these sorts of questions

00:07:19.790 --> 00:07:22.940
about labor and the
independent gig economy, right?

00:07:22.940 --> 00:07:25.910
So I want you to just kind
of keep this thought in mind

00:07:25.910 --> 00:07:28.640
as we think again about
how not only the cloud has

00:07:28.640 --> 00:07:31.100
very specific transactional
arrangements that

00:07:31.100 --> 00:07:34.880
are associated with companies,
but also our experiences

00:07:34.880 --> 00:07:38.120
of taking taxis,
staying in houses,

00:07:38.120 --> 00:07:41.060
sharing media content via
Facebook, for example.

00:07:41.060 --> 00:07:44.360
Those are all
transacted by platforms

00:07:44.360 --> 00:07:48.590
that are accruing a great
amount of money and power.

00:07:48.590 --> 00:07:50.400
So the reason why I'm
getting into all this

00:07:50.400 --> 00:07:51.930
is because we're
building systems

00:07:51.930 --> 00:07:53.790
right now here at
Google, and of course

00:07:53.790 --> 00:07:56.670
in many other capacities,
many other companies, that

00:07:56.670 --> 00:07:59.520
are so-called learning
from the world, right?

00:07:59.520 --> 00:08:02.730
And as my colleague Cathy O'Neil
says in her recent book called

00:08:02.730 --> 00:08:05.366
"Weapons of Math Destruction,"
generally speaking,

00:08:05.366 --> 00:08:06.990
and you all know this
better than I do,

00:08:06.990 --> 00:08:08.490
when you create an
algorithm, you're

00:08:08.490 --> 00:08:10.680
basically thinking about
what is the recipe,

00:08:10.680 --> 00:08:12.960
and what is the success, right?

00:08:12.960 --> 00:08:14.970
You're thinking about
the set of instructions

00:08:14.970 --> 00:08:17.940
by which some sort of
deterministic or successful

00:08:17.940 --> 00:08:21.540
outcome can be programmed
or created, right?

00:08:21.540 --> 00:08:23.800
And so the reason that
that is problematic,

00:08:23.800 --> 00:08:26.850
and this is sort of sad but
kind of humorous example

00:08:26.850 --> 00:08:30.300
around this, is if we
are building systems

00:08:30.300 --> 00:08:33.940
that people, generally speaking,
treat as neutral and natural,

00:08:33.940 --> 00:08:35.970
because none of us have
any time anymore, right,

00:08:35.970 --> 00:08:38.669
we're all completely
multitasking,

00:08:38.669 --> 00:08:40.720
and the technologies
allow us to do that,

00:08:40.720 --> 00:08:42.600
but they also put
us in a position

00:08:42.600 --> 00:08:45.060
where our minds,
we sort of blindly

00:08:45.060 --> 00:08:47.280
trust that which we see, right?

00:08:47.280 --> 00:08:49.290
But if those
systems are learning

00:08:49.290 --> 00:08:52.496
from a world that already
features various forms of bias,

00:08:52.496 --> 00:08:54.120
and there's quite a
bit of science that

00:08:54.120 --> 00:08:57.300
shows that at the minimum,
racial implicit bias is

00:08:57.300 --> 00:09:00.180
the norm rather
than the exception,

00:09:00.180 --> 00:09:03.390
then those systems are
going to create outputs

00:09:03.390 --> 00:09:05.460
that we might treat
as neutral that

00:09:05.460 --> 00:09:08.460
actually reflect those
biases and normalize

00:09:08.460 --> 00:09:10.320
those biases, right?

00:09:10.320 --> 00:09:13.170
So generally speaking,
you all know this again.

00:09:13.170 --> 00:09:15.300
What we end up creating
algorithmically

00:09:15.300 --> 00:09:17.550
is based on who we are,
and what we design,

00:09:17.550 --> 00:09:19.620
and what software
we build, right?

00:09:19.620 --> 00:09:21.930
The kind of learning
model that we apply

00:09:21.930 --> 00:09:24.510
and kind of our ability to
design that learning model,

00:09:24.510 --> 00:09:27.630
and the data sets, of course,
by which that learning model is

00:09:27.630 --> 00:09:30.990
learning, and the outcomes
for which we are optimizing

00:09:30.990 --> 00:09:32.640
those algorithmic systems.

00:09:32.640 --> 00:09:36.570
So this is sort of, again, kind
of a weird and humorous example

00:09:36.570 --> 00:09:39.600
of a company out of
Russia called FaceApp,

00:09:39.600 --> 00:09:41.610
I don't know how many
of you heard of this,

00:09:41.610 --> 00:09:43.910
out of St. Petersburg
in Russia that

00:09:43.910 --> 00:09:46.170
was attempting to go
through the internet

00:09:46.170 --> 00:09:49.360
and take people's faces and
make them more attractive.

00:09:49.360 --> 00:09:52.830
So I think most people think
that President Obama is

00:09:52.830 --> 00:09:55.710
pretty attractive,
right, at least in 2008,

00:09:55.710 --> 00:09:57.930
right, until he
had to age by being

00:09:57.930 --> 00:09:59.740
president of this country.

00:09:59.740 --> 00:10:02.670
And so President Obama's face,
right, we know he's mixed race,

00:10:02.670 --> 00:10:04.530
but President Obama's
face was turned white

00:10:04.530 --> 00:10:06.670
by this algorithmic system.

00:10:06.670 --> 00:10:09.660
I'm not really sure he looks
that much better thanks

00:10:09.660 --> 00:10:11.670
to the system than not.

00:10:11.670 --> 00:10:14.550
OK, so now I want to kind
of touch on other issues

00:10:14.550 --> 00:10:17.220
just to kind of push
these ideas and kind of

00:10:17.220 --> 00:10:18.690
engage with you
in a conversation

00:10:18.690 --> 00:10:21.150
around these issues.

00:10:21.150 --> 00:10:23.160
I don't know how many of
you read this article,

00:10:23.160 --> 00:10:28.590
but it's quite persuasive for
me, published by the outlet

00:10:28.590 --> 00:10:30.060
ProPublica.

00:10:30.060 --> 00:10:33.780
And this is a really,
really interesting issue

00:10:33.780 --> 00:10:37.290
where a private
company was charged

00:10:37.290 --> 00:10:40.980
with building a system
that could predict,

00:10:40.980 --> 00:10:43.410
in theory, the
rate of recidivism.

00:10:43.410 --> 00:10:45.510
So what does recidivism mean?

00:10:45.510 --> 00:10:46.450
You committed a crime.

00:10:46.450 --> 00:10:49.080
What are the chances that you're
going to commit a crime again,

00:10:49.080 --> 00:10:50.220
right?

00:10:50.220 --> 00:10:52.600
And what it turns out
in this particular case,

00:10:52.600 --> 00:10:56.040
and this company is called North
Point, in this particular case

00:10:56.040 --> 00:10:58.590
is, the Caucasian
man on the right,

00:10:58.590 --> 00:11:01.740
who actually had a
felony conviction already

00:11:01.740 --> 00:11:09.050
on his record, was at a
70% rate less predicted--

00:11:09.050 --> 00:11:11.690
so the black man was
basically predicted

00:11:11.690 --> 00:11:15.880
with a 70% higher rate
to commit a crime again

00:11:15.880 --> 00:11:16.730
than the white man.

00:11:16.730 --> 00:11:19.340
And the black man did not
have a felony conviction

00:11:19.340 --> 00:11:20.450
on his record.

00:11:20.450 --> 00:11:23.360
Now, the systems
themselves are not

00:11:23.360 --> 00:11:25.430
directly trained
on racial issues.

00:11:25.430 --> 00:11:26.930
So it's important
to point that out.

00:11:26.930 --> 00:11:30.080
This is far more implicit and
pervasive as a form of learning

00:11:30.080 --> 00:11:33.282
phenomenon than actually
saying, oh, you're black,

00:11:33.282 --> 00:11:35.240
therefore you're going
to commit a crime again.

00:11:35.240 --> 00:11:37.280
They were asking
people other questions

00:11:37.280 --> 00:11:41.600
like, "Was one of your parents
ever sent to jail or to prison?

00:11:41.600 --> 00:11:43.400
How many of your friends
and acquaintances

00:11:43.400 --> 00:11:44.960
are taking drugs illegally?"

00:11:44.960 --> 00:11:47.270
Not sure why they answer
that question correctly.

00:11:47.270 --> 00:11:50.870
Or "How often did you get in
fights while at school," right?

00:11:50.870 --> 00:11:54.650
And we environmental
factors are correlated

00:11:54.650 --> 00:11:58.400
with racial factors that
might allow this gentleman

00:11:58.400 --> 00:12:00.952
on the left to answer yes
to some of those questions

00:12:00.952 --> 00:12:02.660
and this gentlemen on
the right to answer

00:12:02.660 --> 00:12:04.550
no to these questions.

00:12:04.550 --> 00:12:08.060
So this is a huge issue because,
increasingly more and more,

00:12:08.060 --> 00:12:10.070
states and cities
themselves, as I'm

00:12:10.070 --> 00:12:12.410
going to show in just
a moment, are actually

00:12:12.410 --> 00:12:15.110
applying various sorts
of technological systems,

00:12:15.110 --> 00:12:18.470
in theory, to overcome
human bias, right?

00:12:18.470 --> 00:12:20.330
And in many cases, it is true.

00:12:20.330 --> 00:12:21.830
Humans are biased, right?

00:12:21.830 --> 00:12:23.420
But the problem
is, is that systems

00:12:23.420 --> 00:12:27.110
that we're treating as neutral
and natural or actually complex

00:12:27.110 --> 00:12:29.330
reflections of
those forms of bias

00:12:29.330 --> 00:12:32.580
and highly inductive in
environmental matters.

00:12:32.580 --> 00:12:35.435
So this is something
that you all at Google,

00:12:35.435 --> 00:12:37.370
the most powerful
technology company

00:12:37.370 --> 00:12:40.460
in the world in my mind, has
got to do something about this,

00:12:40.460 --> 00:12:43.670
help us with these issues,
because the impacts of what

00:12:43.670 --> 00:12:45.052
you do are profound.

00:12:45.052 --> 00:12:47.510
Are profound, and I'm going to
talk a little bit about some

00:12:47.510 --> 00:12:51.130
of Google's projects in
relation to this space as well.

00:12:51.130 --> 00:12:56.600
OK, so this is another example
that's made the rounds.

00:12:56.600 --> 00:12:59.750
This is actually research
done by a colleague of mine

00:12:59.750 --> 00:13:02.780
at the UCLA Anthropology
Department, who's

00:13:02.780 --> 00:13:06.170
built neural network
machine learning models

00:13:06.170 --> 00:13:08.170
for the Iraq battlefield.

00:13:08.170 --> 00:13:11.000
He had Pentagon-funded research.

00:13:11.000 --> 00:13:13.520
He also was building
learning models

00:13:13.520 --> 00:13:15.650
based on archaeological data.

00:13:15.650 --> 00:13:18.200
And, now, there is an attempt
to apply some of these kind

00:13:18.200 --> 00:13:22.490
of, if you will, learning
ontological models to the theme

00:13:22.490 --> 00:13:24.667
of predictive policing.

00:13:24.667 --> 00:13:26.750
And I think all of us have
seen "Minority Report."

00:13:26.750 --> 00:13:29.780
So we know this
idea of predicting

00:13:29.780 --> 00:13:34.040
whether a given crime in this
particular case is gang related

00:13:34.040 --> 00:13:35.450
or not, right?

00:13:35.450 --> 00:13:36.980
And we all know
that gang related

00:13:36.980 --> 00:13:39.090
itself is a quite murky
definition, right?

00:13:39.090 --> 00:13:41.390
Like, what is and is
not considered a gang?

00:13:41.390 --> 00:13:43.001
Is it simply a yes or no?

00:13:43.001 --> 00:13:45.500
Are there gangs that might be
construed more as communities?

00:13:45.500 --> 00:13:47.630
Or are there gangs that
are actually violent

00:13:47.630 --> 00:13:50.520
and a threat to society?

00:13:50.520 --> 00:13:53.660
So this is actually
being potentially

00:13:53.660 --> 00:13:56.510
implemented by the Los Angeles
Police Department right

00:13:56.510 --> 00:14:00.170
where I live in Los Angeles,
a predictive policing system,

00:14:00.170 --> 00:14:02.510
again, that will--
what this will do

00:14:02.510 --> 00:14:04.700
is it will be fed crime reports.

00:14:04.700 --> 00:14:07.550
It will actually develop-- its
called partially-generative

00:14:07.550 --> 00:14:09.410
research--

00:14:09.410 --> 00:14:11.450
partially-generative algorithms.

00:14:11.450 --> 00:14:13.670
And in the absence of
a written description,

00:14:13.670 --> 00:14:15.740
the neural network
will generate new text,

00:14:15.740 --> 00:14:19.250
an algorithmically-written crime
report that isn't actually read

00:14:19.250 --> 00:14:22.370
by anyone, but it's supposed
to provide context to a police

00:14:22.370 --> 00:14:26.660
report, and then it's turned
into a mathematical vector that

00:14:26.660 --> 00:14:30.260
will relate to the prediction
of whether a crime is

00:14:30.260 --> 00:14:33.050
gang related or not.

00:14:33.050 --> 00:14:35.270
And so, of course,
as you can imagine,

00:14:35.270 --> 00:14:37.550
and I'm kind of
implying here very--

00:14:37.550 --> 00:14:39.260
I don't even know
if I'm implying,

00:14:39.260 --> 00:14:41.300
I'm being pretty
clear, this, to me,

00:14:41.300 --> 00:14:43.910
is problematic, because
it doesn't really consider

00:14:43.910 --> 00:14:46.340
all these sort of
qualitative social issues

00:14:46.340 --> 00:14:48.990
in the construction of
an algorithm, let alone

00:14:48.990 --> 00:14:52.790
considers how such an
algorithm might be regulated,

00:14:52.790 --> 00:14:55.250
how there might be oversight,
how it might be checked,

00:14:55.250 --> 00:14:57.230
the checks and
balances, and so on.

00:14:57.230 --> 00:15:00.650
So often I get the response to
this, well, people are biased.

00:15:00.650 --> 00:15:01.730
Judges are biased.

00:15:01.730 --> 00:15:05.040
The police department is biased,
no question about it at all.

00:15:05.040 --> 00:15:07.040
But for me, setting
this up as kind

00:15:07.040 --> 00:15:09.650
of algorithmic
bias or human bias

00:15:09.650 --> 00:15:11.500
is a bit of a false
positive, right,

00:15:11.500 --> 00:15:12.920
or it's a false comparison.

00:15:12.920 --> 00:15:14.960
The question is is, what
kinds of technologies

00:15:14.960 --> 00:15:17.270
should we aspire to work?

00:15:17.270 --> 00:15:20.330
What kinds of relationships
between humans and machines

00:15:20.330 --> 00:15:23.750
are ethical and healthy for
us in our society, everything

00:15:23.750 --> 00:15:26.150
from labor, to questions
of criminality,

00:15:26.150 --> 00:15:28.880
to questions of racial bias,
which I know none of us

00:15:28.880 --> 00:15:32.640
really want to
perpetuate, right?

00:15:32.640 --> 00:15:38.900
So one of the junior authors
to this research with Jeff

00:15:38.900 --> 00:15:44.330
Brantingham, my colleague,
presented this research.

00:15:44.330 --> 00:15:47.030
Before, this kind of made
the rounds in the media,

00:15:47.030 --> 00:15:51.110
and he was asked quite a
few critical questions,

00:15:51.110 --> 00:15:52.250
if you will.

00:15:52.250 --> 00:15:54.140
And he left the room saying--

00:15:54.140 --> 00:15:55.790
he kind of ran out
of the room saying,

00:15:55.790 --> 00:15:58.650
I'm just an engineer, right?

00:15:58.650 --> 00:16:00.830
But as we all
know, and you know,

00:16:00.830 --> 00:16:02.630
this is part of our
engineering education,

00:16:02.630 --> 00:16:05.450
we're seeing a movement toward
ethics courses in our AI,

00:16:05.450 --> 00:16:09.050
in our AI classes as well,
to integrate the two,

00:16:09.050 --> 00:16:11.660
and other colleagues are
saying we should integrate

00:16:11.660 --> 00:16:14.720
moral philosophy classes
with computational sciences

00:16:14.720 --> 00:16:16.460
classes or
anthropological classes

00:16:16.460 --> 00:16:17.900
with computer science classes.

00:16:17.900 --> 00:16:20.920
I would love to co-teach
classes like this at UCLA.

00:16:20.920 --> 00:16:22.830
And we struggled
to do this frankly.

00:16:22.830 --> 00:16:24.650
This is a pervasive issue.

00:16:24.650 --> 00:16:27.320
But we know that we're
not simply engineers,

00:16:27.320 --> 00:16:30.110
that what we're building has
massive impacts on society,

00:16:30.110 --> 00:16:33.170
and what we build has a lot
to do with the cultures,

00:16:33.170 --> 00:16:36.335
not only of ourselves, but the
organizations of which we're

00:16:36.335 --> 00:16:39.240
a part, you know, we
all know that, right?

00:16:39.240 --> 00:16:42.740
So I guess I just want to kind
of use these examples to talk

00:16:42.740 --> 00:16:45.810
about the world that we're
constructing right now

00:16:45.810 --> 00:16:50.420
and what happens when we give
private organizations that

00:16:50.420 --> 00:16:53.450
are quite secretive, because
they have intellectual property

00:16:53.450 --> 00:16:57.802
and copyright issues, all the
power in public life, right?

00:16:57.802 --> 00:16:58.760
There is that blurring.

00:16:58.760 --> 00:17:00.950
And I think, to my
next point, this

00:17:00.950 --> 00:17:04.280
is why so many people are
upset at Facebook today.

00:17:04.280 --> 00:17:06.619
OK, so this is
research that actually

00:17:06.619 --> 00:17:08.569
informs some of the
algorithms that were

00:17:08.569 --> 00:17:10.199
used by Cambridge Analytica.

00:17:10.199 --> 00:17:11.990
I don't think I need
to introduce Cambridge

00:17:11.990 --> 00:17:13.464
Analytica to this crowd.

00:17:13.464 --> 00:17:15.380
I've been able to do
some interviews with them

00:17:15.380 --> 00:17:16.190
for my new book.

00:17:16.190 --> 00:17:18.416
I'll talk about
that in a moment.

00:17:18.416 --> 00:17:19.790
But this relates
to research done

00:17:19.790 --> 00:17:21.687
by a scholar at the
Stanford Business School

00:17:21.687 --> 00:17:22.520
right up the street.

00:17:22.520 --> 00:17:24.780
I highly recommend
you having him here.

00:17:24.780 --> 00:17:27.290
He's a very nice guy
named Michal Kosinski.

00:17:27.290 --> 00:17:29.060
And Michal has
made the argument,

00:17:29.060 --> 00:17:31.190
and it was a real barnburner
when I brought him

00:17:31.190 --> 00:17:34.840
to my university for people
to ask him some questions,

00:17:34.840 --> 00:17:37.220
that with 10 likes,
this is on Facebook,

00:17:37.220 --> 00:17:40.580
a computer knows you better
than a colleague, with 70 likes,

00:17:40.580 --> 00:17:43.100
it knows you better than
a friend or a roommate,

00:17:43.100 --> 00:17:47.370
150, better than a family
member, and 300 likes,

00:17:47.370 --> 00:17:49.130
it knows you better
than a spouse.

00:17:49.130 --> 00:17:52.420
But it's not simply about likes
on Facebook because, you know,

00:17:52.420 --> 00:17:55.430
to be honest, I don't
think my sort of lens

00:17:55.430 --> 00:17:58.490
to my personality is that well
revealed through Facebook.

00:17:58.490 --> 00:18:00.860
There is a lot of things I
believe that I don't share

00:18:00.860 --> 00:18:03.620
on Facebook and may not
even be that implicit,

00:18:03.620 --> 00:18:06.360
but it's the
aggregation of data.

00:18:06.360 --> 00:18:09.080
So what we think of as
disaggregated activities

00:18:09.080 --> 00:18:11.330
in our lives are
being aggregated.

00:18:11.330 --> 00:18:12.980
They're being brought
together, right?

00:18:12.980 --> 00:18:15.560
I never thought that what
I might buy at RiteAid

00:18:15.560 --> 00:18:17.360
would somehow have
any relationship

00:18:17.360 --> 00:18:20.240
to what I post, or like,
or comment when I connect

00:18:20.240 --> 00:18:22.430
with Alex on Facebook,
right, or with Matt

00:18:22.430 --> 00:18:24.590
on Facebook, both my
Facebook friends, right?

00:18:24.590 --> 00:18:28.250
So I think that those
parts of my life

00:18:28.250 --> 00:18:31.250
are independent of one
another as a human being.

00:18:31.250 --> 00:18:34.820
I think that I want the right
to be who I am, where I am.

00:18:34.820 --> 00:18:35.580
When I go home.

00:18:35.580 --> 00:18:38.430
I'd like to be who I
am in a different way,

00:18:38.430 --> 00:18:41.090
in a different fashion, than
I am when I'm here right now.

00:18:41.090 --> 00:18:42.560
Who I am in front
of my mother is

00:18:42.560 --> 00:18:45.290
different than who I am in
front of my friends, right?

00:18:45.290 --> 00:18:47.840
I'm kind of the same
person, but still.

00:18:47.840 --> 00:18:49.800
My mom's right here.

00:18:49.800 --> 00:18:52.880
So I think these are things
that are really, really

00:18:52.880 --> 00:18:54.950
important to kind
of consider, right,

00:18:54.950 --> 00:18:57.860
like the power over our
own agency as human beings.

00:18:57.860 --> 00:19:01.550
And more generally, the idea
of knowing someone, knowing

00:19:01.550 --> 00:19:05.510
someone, true, that's
true on some sort snapshot

00:19:05.510 --> 00:19:08.060
in a specific set
of activities, I

00:19:08.060 --> 00:19:10.310
might be able to
predict one's behavior.

00:19:10.310 --> 00:19:13.130
But shouldn't I have
the power to change?

00:19:13.130 --> 00:19:14.930
Should I have systems
that reinforce

00:19:14.930 --> 00:19:18.020
certain types of behavior,
feeding me information

00:19:18.020 --> 00:19:21.620
that then I have no power
to necessarily overcome,

00:19:21.620 --> 00:19:25.130
because we all are conditioned
by systems of all forms,

00:19:25.130 --> 00:19:27.900
not just technological
ones, school systems,

00:19:27.900 --> 00:19:30.860
et cetera, policing systems,
governmental systems that

00:19:30.860 --> 00:19:32.150
influence us in our lives.

00:19:32.150 --> 00:19:35.000
So we are socialized by the
systems of which we're part.

00:19:35.000 --> 00:19:36.950
To think that people
somehow can overcome

00:19:36.950 --> 00:19:40.190
those forms of conditioning
is naive, right?

00:19:40.190 --> 00:19:44.270
So the question for me
is are multiple questions

00:19:44.270 --> 00:19:47.750
about morality, about ethics,
and about the agency of us

00:19:47.750 --> 00:19:50.490
as human beings.

00:19:50.490 --> 00:19:54.320
So what I'm getting at is
sort of a set of these issues

00:19:54.320 --> 00:19:56.270
that we're starting to
normalize in our lives,

00:19:56.270 --> 00:19:58.000
and I'm going to give you one--

00:19:58.000 --> 00:20:01.880
well, kind of two very quick
final examples of this.

00:20:01.880 --> 00:20:04.310
This is a story that's
made the rounds based

00:20:04.310 --> 00:20:06.650
on original research that
was published in "Science,"

00:20:06.650 --> 00:20:08.810
you know, which is like
the dream for all of us

00:20:08.810 --> 00:20:11.510
to publish in the
journal "Science."

00:20:11.510 --> 00:20:14.430
This is related to the
word embedding algorithm.

00:20:14.430 --> 00:20:16.520
I don't how many of you
know about this research,

00:20:16.520 --> 00:20:18.560
but it's very much
worth knowing.

00:20:18.560 --> 00:20:22.700
So this research has been built
into automated systems that

00:20:22.700 --> 00:20:25.710
are being piloted by various
companies for human resources

00:20:25.710 --> 00:20:26.210
labor.

00:20:26.210 --> 00:20:28.880
As we all know, automated
labor, or at least

00:20:28.880 --> 00:20:32.740
semi-automated labor,
through computational systems

00:20:32.740 --> 00:20:36.010
are going to replace, if
not supplant, or perhaps

00:20:36.010 --> 00:20:38.964
create new opportunities,
I hope, for people's jobs.

00:20:38.964 --> 00:20:41.380
At the end of the day, we need
to be building technologies

00:20:41.380 --> 00:20:43.120
that serve all of
us as human beings,

00:20:43.120 --> 00:20:45.310
even on a kind of
company-base level, right?

00:20:45.310 --> 00:20:47.710
We shouldn't be losing
people in our jobs

00:20:47.710 --> 00:20:51.170
unless the goal is to lose
people and cut costs, right?

00:20:51.170 --> 00:20:52.780
So what these
systems were doing,

00:20:52.780 --> 00:20:55.420
word embedding in
particular, has

00:20:55.420 --> 00:20:58.720
had a 50% bias on
its CV scanning

00:20:58.720 --> 00:21:03.970
between African-Americans'
names and Caucasian names,

00:21:03.970 --> 00:21:08.060
even when the CVs are normalized
around other variables

00:21:08.060 --> 00:21:10.540
for being more or less
equivalent in terms

00:21:10.540 --> 00:21:11.920
of their level of quality.

00:21:11.920 --> 00:21:14.650
I mean, how that's even done
is a very interesting question.

00:21:14.650 --> 00:21:16.330
But what is
essentially happening

00:21:16.330 --> 00:21:19.690
is, again, not a problem
necessarily with the system

00:21:19.690 --> 00:21:21.840
itself, but what
it's learning from

00:21:21.840 --> 00:21:24.280
and the overall environment
around which it's

00:21:24.280 --> 00:21:25.780
kind of computing, right?

00:21:25.780 --> 00:21:30.250
So words-- so for example,
words in this system

00:21:30.250 --> 00:21:33.190
and in generally in the
corpus of English language,

00:21:33.190 --> 00:21:35.350
like female and woman
were more closely

00:21:35.350 --> 00:21:40.060
associated with arts and
humanities, surprise, surprise,

00:21:40.060 --> 00:21:43.390
while male and men were
closer to math and engineering

00:21:43.390 --> 00:21:45.100
professions.

00:21:45.100 --> 00:21:46.870
European-American
names, in terms

00:21:46.870 --> 00:21:50.530
of the outputs of these systems,
were more closely associated

00:21:50.530 --> 00:21:53.400
with words like gift and happy.

00:21:53.400 --> 00:21:55.810
And African-American
names were more commonly

00:21:55.810 --> 00:21:59.060
associated with
unpleasant words.

00:21:59.060 --> 00:22:03.410
So the issue is that unless
algorithms are explicitly

00:22:03.410 --> 00:22:06.020
programmed to address
some of these issues,

00:22:06.020 --> 00:22:09.380
they're going to be riddled
with the same social prejudices,

00:22:09.380 --> 00:22:09.950
right?

00:22:09.950 --> 00:22:13.430
And so the question really
is, how do we do better?

00:22:13.430 --> 00:22:18.680
840 billion words were
used to train the system,

00:22:18.680 --> 00:22:20.645
and it was providing
these types of outputs.

00:22:24.520 --> 00:22:26.270
So what's going on here?

00:22:26.270 --> 00:22:30.040
Well, four or five factors
that I want to identify.

00:22:30.040 --> 00:22:32.951
First, questions of
diversity and inclusion,

00:22:32.951 --> 00:22:34.450
which is really
central to this book

00:22:34.450 --> 00:22:37.330
that you now have in your
hands around the design

00:22:37.330 --> 00:22:39.040
and engineering of technologies.

00:22:39.040 --> 00:22:41.830
Second, our data sets,
as I mentioned already,

00:22:41.830 --> 00:22:44.470
what kinds of data sets are
these systems learning from?

00:22:44.470 --> 00:22:46.990
Third, and this is really
central to my work,

00:22:46.990 --> 00:22:48.790
the ontological learning models.

00:22:48.790 --> 00:22:49.710
That's a fancy term.

00:22:49.710 --> 00:22:50.870
What do I mean by that?

00:22:50.870 --> 00:22:53.080
When I say I know
something, how do I

00:22:53.080 --> 00:22:54.457
articulate that which I know?

00:22:54.457 --> 00:22:56.290
When I say-- when I say
I believe something,

00:22:56.290 --> 00:22:57.640
that's epistemological.

00:22:57.640 --> 00:22:59.500
When I want to articulate
that knowledge,

00:22:59.500 --> 00:23:01.470
that's ontological,
how I express.

00:23:01.470 --> 00:23:04.120
So, similarly, the
systems we build

00:23:04.120 --> 00:23:06.760
can be built with particular
learning models that

00:23:06.760 --> 00:23:10.870
might be more inclusive where
people who might be targeted

00:23:10.870 --> 00:23:13.210
or unfairly excluded
by such systems

00:23:13.210 --> 00:23:16.600
can be part of the process
of designing and developing

00:23:16.600 --> 00:23:18.520
such systems, et cetera.

00:23:18.520 --> 00:23:20.620
And then I also
really believe that we

00:23:20.620 --> 00:23:23.200
need independent
oversight, particularly

00:23:23.200 --> 00:23:25.300
when these are
privatized services being

00:23:25.300 --> 00:23:27.550
used in public context, right?

00:23:27.550 --> 00:23:31.330
And the best example of all is
obviously our election, right,

00:23:31.330 --> 00:23:33.080
and elections across the world.

00:23:33.080 --> 00:23:35.650
So I write in my new
book about examples

00:23:35.650 --> 00:23:39.170
like this from my
Myanmar, Sri Lanka, India,

00:23:39.170 --> 00:23:41.110
the Philippines
with Rodrigo Duterte

00:23:41.110 --> 00:23:43.870
and the cyber troops
issue, all of these issues,

00:23:43.870 --> 00:23:45.959
right, and a lot of this
implicates Facebook.

00:23:45.959 --> 00:23:47.500
But, of course,
since you're building

00:23:47.500 --> 00:23:49.630
a technology, many
technologies, that

00:23:49.630 --> 00:23:52.639
are accessed by
2-plus billion people.

00:23:52.639 --> 00:23:54.430
You guys should tell
me the actual numbers.

00:23:54.430 --> 00:23:56.230
I think it's 2-plus
billion people.

00:23:56.230 --> 00:23:59.440
All I do is scrape
journalism, right?

00:23:59.440 --> 00:24:00.580
Pretty impactful stuff.

00:24:00.580 --> 00:24:06.160
So we're going to have to think
about, what are the values that

00:24:06.160 --> 00:24:08.650
influence how we build systems?

00:24:08.650 --> 00:24:10.240
And how do we think
about the other?

00:24:10.240 --> 00:24:11.950
Especially because
the places where

00:24:11.950 --> 00:24:14.320
Google, and Facebook,
and other big companies

00:24:14.320 --> 00:24:15.760
are going to expand
in their reach

00:24:15.760 --> 00:24:19.060
are, guess what, of course,
the global south, right, places

00:24:19.060 --> 00:24:22.090
where there are more people,
higher density of population,

00:24:22.090 --> 00:24:23.680
less connected, right?

00:24:23.680 --> 00:24:25.220
But who are those people?

00:24:25.220 --> 00:24:28.840
How do we understand them,
not just as users, but as

00:24:28.840 --> 00:24:31.840
people who have voices, and
values, and certain histories,

00:24:31.840 --> 00:24:36.070
and things to say to us, you
know, like that they're living,

00:24:36.070 --> 00:24:39.130
they can express and communicate
with us as we build and design

00:24:39.130 --> 00:24:41.260
such systems and as we
think about the effects

00:24:41.260 --> 00:24:43.660
that those systems pose.

00:24:43.660 --> 00:24:45.961
So the reason I
show this one is I

00:24:45.961 --> 00:24:48.210
think this one made the
rounds, but of course, there's

00:24:48.210 --> 00:24:50.730
also a little bit of a movement
here in Silicon Valley,

00:24:50.730 --> 00:24:54.030
and I've interviewed Sam Altman
from Y Combinator about this

00:24:54.030 --> 00:24:56.700
from my new book and a couple
of other folks, concerns

00:24:56.700 --> 00:24:58.650
about this kind of
superintelligence

00:24:58.650 --> 00:25:00.750
that might be emerging
on a recursive level

00:25:00.750 --> 00:25:03.310
as various sorts of
algorithmic systems

00:25:03.310 --> 00:25:06.120
start to learn, not only
internally, and develop

00:25:06.120 --> 00:25:08.940
their own sorts of forms of
complex adaptive behavior,

00:25:08.940 --> 00:25:10.570
but they learn from one another.

00:25:10.570 --> 00:25:12.030
I think this was
totally overhyped.

00:25:12.030 --> 00:25:14.040
It was basically an
interface language

00:25:14.040 --> 00:25:17.190
that was developed between
two AI bot systems.

00:25:17.190 --> 00:25:18.690
I don't know if you
know this story,

00:25:18.690 --> 00:25:22.350
but I still think that it
speaks to all the different ways

00:25:22.350 --> 00:25:24.874
in which we're talking
about and thinking about AI,

00:25:24.874 --> 00:25:26.790
and it means a lot of
different things, right?

00:25:26.790 --> 00:25:28.680
Automation is not
the same as AI.

00:25:28.680 --> 00:25:31.260
Specialized AI is different
than generalized AI.

00:25:31.260 --> 00:25:33.870
AI and biases, like the
examples I gave earlier,

00:25:33.870 --> 00:25:35.479
is different than
this kind of AI.

00:25:35.479 --> 00:25:37.770
So these are all things that
are worth thinking through

00:25:37.770 --> 00:25:39.910
in my mind.

00:25:39.910 --> 00:25:44.820
So of all people, my favorite,
Tucker Carlson, that's a joke,

00:25:44.820 --> 00:25:47.895
is his--

00:25:47.895 --> 00:25:49.800
I'm showing my
biases, that's fine,

00:25:49.800 --> 00:25:52.380
we all should just
show our biases, right,

00:25:52.380 --> 00:25:56.160
has actually been speaking
to cognitive scientists.

00:25:56.160 --> 00:25:59.700
He is concerned about the
effects of this centralization

00:25:59.700 --> 00:26:02.790
of power around technology
on our political system

00:26:02.790 --> 00:26:06.120
because he such a
great patriot, right?

00:26:06.120 --> 00:26:08.430
So he did-- he interviewed
a colleague of mine,

00:26:08.430 --> 00:26:11.090
who has been quite critical
of Google, full disclosure,

00:26:11.090 --> 00:26:12.540
named Robert Epstein.

00:26:12.540 --> 00:26:16.470
And Epstein's research has
been looking at search results

00:26:16.470 --> 00:26:18.857
in large-scale controlled
studies in different parts

00:26:18.857 --> 00:26:19.440
of the world--

00:26:19.440 --> 00:26:22.284
Australia, India, and
the United States.

00:26:22.284 --> 00:26:24.450
And he's been doing really,
really interesting work,

00:26:24.450 --> 00:26:27.090
and he published one
piece in the "Proceedings

00:26:27.090 --> 00:26:30.090
of the National
Academy of Sciences,"

00:26:30.090 --> 00:26:32.280
and he was the head
of "Psychology Today."

00:26:32.280 --> 00:26:34.020
And what this research
showed, and this

00:26:34.020 --> 00:26:35.460
might be very
interesting to you,

00:26:35.460 --> 00:26:37.740
that if you searched
for political issues

00:26:37.740 --> 00:26:41.550
with Google Search results,
simple masking the ordering

00:26:41.550 --> 00:26:44.310
of those results could
flip undecided voters

00:26:44.310 --> 00:26:46.650
from 50-50 to 90-10.

00:26:46.650 --> 00:26:48.280
Let me explain what that means.

00:26:48.280 --> 00:26:50.890
So Hillary Clinton-Donald
Trump, let's say they were both,

00:26:50.890 --> 00:26:53.670
you know, I'm totally undecided
between the two of them.

00:26:53.670 --> 00:26:57.610
And I have Trump at 1 and
3 and Hillary at 2 and 4.

00:26:57.610 --> 00:27:01.050
I could flip it from
that to Hillary 1 and 3,

00:27:01.050 --> 00:27:04.350
Trump 2 and 4, 50-50 to 90-10.

00:27:04.350 --> 00:27:06.060
So that's really,
really interesting.

00:27:06.060 --> 00:27:07.050
Did that make sense?

00:27:07.050 --> 00:27:09.720
Basically, how you
order, even if it

00:27:09.720 --> 00:27:12.810
seems sort of not
necessarily that significant,

00:27:12.810 --> 00:27:16.030
could actually largely
affect people's voting--

00:27:16.030 --> 00:27:19.450
their kind of-- their
biases toward voting.

00:27:19.450 --> 00:27:22.890
So I think that's a significant
issue because, again, that's

00:27:22.890 --> 00:27:24.960
a sort of transgression
into the public space.

00:27:24.960 --> 00:27:26.520
And one of the most
famous examples

00:27:26.520 --> 00:27:29.070
of this from a few years ago,
you all may remember this,

00:27:29.070 --> 00:27:33.330
was Facebook's A/B testing on
the Get Out the Vote button.

00:27:33.330 --> 00:27:34.710
Do y'all remember this?

00:27:34.710 --> 00:27:37.644
So they kind of said, I voted,
like, click on this, right,

00:27:37.644 --> 00:27:38.310
remember, David?

00:27:38.310 --> 00:27:41.196
It's like, you know, click
on this to say like, I voted

00:27:41.196 --> 00:27:42.570
and share this
with your friends.

00:27:42.570 --> 00:27:45.720
So it turns out that that
influenced Facebook users who

00:27:45.720 --> 00:27:48.630
were seeing that, right,
the A group, to vote

00:27:48.630 --> 00:27:53.219
at a 0.5% higher rate, which
doesn't seem like a lot,

00:27:53.219 --> 00:27:55.260
but we all know when we're
talking about millions

00:27:55.260 --> 00:27:57.640
of people, that's
very, very significant.

00:27:57.640 --> 00:28:02.520
So simply Facebook
saying you know, I voted,

00:28:02.520 --> 00:28:05.880
will push voting to that
significant an extent.

00:28:05.880 --> 00:28:09.360
And we know that those
numbers are way, way higher

00:28:09.360 --> 00:28:12.850
than the amounts by which
Trump won in Ohio, Michigan,

00:28:12.850 --> 00:28:14.840
and Pennsylvania, in
fact, put together.

00:28:14.840 --> 00:28:18.070
I simply-- I just recently
looked at those numbers.

00:28:18.070 --> 00:28:19.710
So we know--

00:28:19.710 --> 00:28:21.270
I mean, to me, that's
not necessarily

00:28:21.270 --> 00:28:23.460
a critique of Facebook,
but it's meant

00:28:23.460 --> 00:28:25.450
to understand that
these platforms have

00:28:25.450 --> 00:28:28.750
a profound impact
on our behavior.

00:28:28.750 --> 00:28:32.710
So this is all to kind of
get at some of the work

00:28:32.710 --> 00:28:34.879
that I've been
doing in this space.

00:28:34.879 --> 00:28:37.170
So when this book came out
that you have in your hands,

00:28:37.170 --> 00:28:38.380
I started to make the rounds.

00:28:38.380 --> 00:28:39.670
I think it's because
I was concerned

00:28:39.670 --> 00:28:41.470
about some of these
global, and cultural,

00:28:41.470 --> 00:28:43.990
and even these blurring into
like political and economic

00:28:43.990 --> 00:28:45.670
questions that I've
already brought up

00:28:45.670 --> 00:28:47.080
in this introduction.

00:28:47.080 --> 00:28:50.650
So of all people, "Morning
Joe," MSNBC's "Morning Joe,"

00:28:50.650 --> 00:28:54.670
had me on like two or three
weeks right after the election.

00:28:54.670 --> 00:28:59.200
And Mika Brzezinski, I literally
saw her jaw drop in front me

00:28:59.200 --> 00:29:02.350
when I spoke about concerns
with groups like Cambridge

00:29:02.350 --> 00:29:04.420
Analytica, right, how
when you create sort

00:29:04.420 --> 00:29:07.740
of so-called open
ecosystems for advertisers,

00:29:07.740 --> 00:29:09.550
but closed off
for users, you can

00:29:09.550 --> 00:29:13.510
have these pernicious
effects on our democracy

00:29:13.510 --> 00:29:16.684
or what we fight for and
aspire to in a democracy.

00:29:16.684 --> 00:29:18.100
And, of course,
this is not simply

00:29:18.100 --> 00:29:19.900
an issue with
Cambridge Analytica.

00:29:19.900 --> 00:29:23.560
It's far more pervasive with
Russia and other examples

00:29:23.560 --> 00:29:24.640
like this, right?

00:29:24.640 --> 00:29:27.250
And, really, it's not really
even about the effects

00:29:27.250 --> 00:29:29.282
that these folks have
on our elections.

00:29:29.282 --> 00:29:31.240
We're not really sure
with Cambridge Analytica.

00:29:31.240 --> 00:29:33.760
I've yet to see a
solid piece of research

00:29:33.760 --> 00:29:38.230
to show that Cambridge Analytica
actually affected the election.

00:29:38.230 --> 00:29:40.090
And Cambridge
Analytica's content

00:29:40.090 --> 00:29:41.660
is not the same as fake news.

00:29:41.660 --> 00:29:45.640
It's more like framed news
based on psychometrics, right,

00:29:45.640 --> 00:29:48.010
you know like Michal's
work is influential there.

00:29:48.010 --> 00:29:50.290
He's not happy
about it, but still.

00:29:50.290 --> 00:29:54.820
So this is also meant to
make the point that we also

00:29:54.820 --> 00:29:58.630
need to understand, as we
design and build systems,

00:29:58.630 --> 00:30:04.320
what we value in terms of
advertisers versus users,

00:30:04.320 --> 00:30:06.490
right, and how do
we balance those?

00:30:06.490 --> 00:30:09.760
How do we think about not
just short-term effects

00:30:09.760 --> 00:30:11.470
of the technologies
that we build,

00:30:11.470 --> 00:30:15.160
but also these kind of
systemic effects on our larger

00:30:15.160 --> 00:30:17.079
political systems?

00:30:17.079 --> 00:30:19.120
And I think that that's
really, really important.

00:30:19.120 --> 00:30:22.690
And I am appreciative that a
number of folks from Google

00:30:22.690 --> 00:30:24.940
were working with the
Obama administration

00:30:24.940 --> 00:30:27.970
afterward in helping
advise them around this.

00:30:27.970 --> 00:30:32.710
And I hope that there is a
devotion to public and civic

00:30:32.710 --> 00:30:36.940
life and democratic ideals
from our tech companies

00:30:36.940 --> 00:30:41.140
because you all are
really powerful.

00:30:41.140 --> 00:30:44.906
So all right, so my
colleagues Zeynep Tufekci

00:30:44.906 --> 00:30:46.030
has been making the rounds.

00:30:46.030 --> 00:30:47.363
Some of you might know her work.

00:30:47.363 --> 00:30:51.630
She is quite a public critic
of what's going on these days.

00:30:51.630 --> 00:30:55.810
She in particular has been
concerned about some algorithms

00:30:55.810 --> 00:30:57.310
that have been
populating YouTube

00:30:57.310 --> 00:31:01.300
in particular, especially like
the recommendation systems.

00:31:01.300 --> 00:31:03.670
I've yet to see very
strong empirical work

00:31:03.670 --> 00:31:07.260
showing that the autosuggestion,
is it called autosuggestion,

00:31:07.260 --> 00:31:09.850
autosuggestion
feature of YouTube

00:31:09.850 --> 00:31:12.790
actually impacts
one's perceptions.

00:31:12.790 --> 00:31:14.920
But it's hard not to believe
that that is the case,

00:31:14.920 --> 00:31:17.410
because I'll speak
for myself and speak

00:31:17.410 --> 00:31:20.080
for some cognitive science
studies showing that what

00:31:20.080 --> 00:31:23.040
we see impacts what we believe.

00:31:23.040 --> 00:31:27.240
But Tufekci's critiques, which
I'm sure have been heard here,

00:31:27.240 --> 00:31:28.320
are that--

00:31:28.320 --> 00:31:31.200
maybe not here, but
YouTube, have been--

00:31:31.200 --> 00:31:33.430
are of her own experiences.

00:31:33.430 --> 00:31:35.400
And it is anecdotal
rather than large-scale

00:31:35.400 --> 00:31:36.390
quantitative evidence.

00:31:36.390 --> 00:31:38.400
And I think that that's
important to note.

00:31:38.400 --> 00:31:40.380
She is-- you know,
it was like, let

00:31:40.380 --> 00:31:43.770
me watch the Make America
Great Again rallies on YouTube.

00:31:43.770 --> 00:31:46.560
And then she's-- she makes the
point that she's started to see

00:31:46.560 --> 00:31:48.480
more and more radicalized
content, right?

00:31:48.480 --> 00:31:53.370
So like what was suggested
after seeing a Make America

00:31:53.370 --> 00:31:55.410
Great Again, Donald
Trump rally, might

00:31:55.410 --> 00:31:58.500
have been content that was a
bit more Alt Right, or White

00:31:58.500 --> 00:32:02.040
nationalist, or heaven
forbid, neo-Nazi, right?

00:32:02.040 --> 00:32:05.100
And, of course, it has nothing
to do with Facebook, Google,

00:32:05.100 --> 00:32:07.920
or any other company
being pro Alt Right.

00:32:07.920 --> 00:32:09.570
Of course not, right?

00:32:09.570 --> 00:32:13.410
If anything, as we saw with some
of the interrogations of Mark

00:32:13.410 --> 00:32:15.900
Zuckerberg by our
Congress, who don't

00:32:15.900 --> 00:32:20.430
seem to understand technology
very well, at least the ones--

00:32:20.430 --> 00:32:23.290
at least in the
Senate generally--

00:32:23.290 --> 00:32:26.760
I don't know why their
staffers weren't schooling them

00:32:26.760 --> 00:32:29.400
on some basic
concepts of computing.

00:32:29.400 --> 00:32:33.690
But anyway, as we saw, the
Republicans really, really

00:32:33.690 --> 00:32:36.180
did not like Mark Zuckerberg.

00:32:36.180 --> 00:32:38.001
I mean, they admired
I think his wealth,

00:32:38.001 --> 00:32:40.500
but I don't-- it didn't look
like they really liked him very

00:32:40.500 --> 00:32:40.720
much.

00:32:40.720 --> 00:32:42.469
And that's because,
generally speaking, we

00:32:42.469 --> 00:32:45.720
are seeing that Silicon Valley
tends to be more Democrat,

00:32:45.720 --> 00:32:47.040
you know, of these two parties.

00:32:47.040 --> 00:32:49.540
That's just sort of been
the case historically.

00:32:51.070 --> 00:32:55.930
So I guess the question is, how
do we sort of optimize, again,

00:32:55.930 --> 00:32:59.190
recommendation systems, in
this case, with YouTube,

00:32:59.190 --> 00:33:03.160
to balance again the
goal of maintaining

00:33:03.160 --> 00:33:05.650
a tension, as my colleague
Tim Wu points out

00:33:05.650 --> 00:33:09.370
in his excellent book "The
Attention Merchants," which he

00:33:09.370 --> 00:33:11.860
describes Google
within this as well,

00:33:11.860 --> 00:33:13.360
and therefore the
gathering of data,

00:33:13.360 --> 00:33:16.480
and click throughs, and
so on, with what might

00:33:16.480 --> 00:33:19.015
be, if you will, a vegetable.

00:33:19.015 --> 00:33:19.890
You know, I want to--

00:33:19.890 --> 00:33:22.810
I like I like my
information junk food.

00:33:22.810 --> 00:33:23.800
We all do, right?

00:33:23.800 --> 00:33:26.260
I like my french fries,
but I also want that salad.

00:33:26.260 --> 00:33:27.880
Or maybe I don't
want that salad.

00:33:27.880 --> 00:33:29.770
I want that salad
after I eat it,

00:33:29.770 --> 00:33:31.276
and I need that salad, right?

00:33:31.276 --> 00:33:32.650
So this is kind
of a point that's

00:33:32.650 --> 00:33:36.040
been really built into a
lot of the conversations

00:33:36.040 --> 00:33:39.230
that we've had about technology
for quite a bit of time,

00:33:39.230 --> 00:33:42.099
including work by my colleague
Eli Pariser and his work,

00:33:42.099 --> 00:33:44.140
"The Filter Bubble,"
several years ago, that even

00:33:44.140 --> 00:33:45.940
Obama referred to
in his interview

00:33:45.940 --> 00:33:49.270
with David Letterman
that was up on Netflix.

00:33:49.270 --> 00:33:53.440
So these are examples of these
effects in different parts

00:33:53.440 --> 00:33:54.040
of the world.

00:33:54.040 --> 00:33:55.289
I won't go too much into this.

00:33:55.289 --> 00:33:59.290
In Sri Lanka, Facebook was using
similar techniques, I suppose,

00:33:59.290 --> 00:34:01.700
to privilege more
hysterical content.

00:34:01.700 --> 00:34:03.970
There's not a strong,
independent, journalistic

00:34:03.970 --> 00:34:04.580
media.

00:34:04.580 --> 00:34:07.000
There are not very strong
governmental institutions

00:34:07.000 --> 00:34:09.614
in Sri Lanka that can
regulate and push this back.

00:34:09.614 --> 00:34:11.530
And that actually created
real-world violence.

00:34:11.530 --> 00:34:13.060
This is a "New
York Times" article

00:34:13.060 --> 00:34:14.380
that came out fairly recently.

00:34:14.380 --> 00:34:16.480
So these battles are
very significant,

00:34:16.480 --> 00:34:18.412
not just about
data and attention,

00:34:18.412 --> 00:34:19.870
but also about the
internet itself.

00:34:19.870 --> 00:34:22.600
This is me on Joy Reid a
few weeks ago, actually,

00:34:22.600 --> 00:34:25.969
a couple of months ago, talking
about net neutrality itself,

00:34:25.969 --> 00:34:26.469
right?

00:34:26.469 --> 00:34:28.120
And so there are
some conversations

00:34:28.120 --> 00:34:31.368
which I'm very happy that
Google supports net neutrality.

00:34:31.368 --> 00:34:33.159
But there are conversations
even about that

00:34:33.159 --> 00:34:35.770
because at this point,
many, many different forms

00:34:35.770 --> 00:34:39.310
of our democratic activities,
especially including online,

00:34:39.310 --> 00:34:41.159
are under attack.

00:34:41.159 --> 00:34:44.915
So I wrote a piece when
my book came out concerned

00:34:44.915 --> 00:34:46.040
about some of these issues.

00:34:46.040 --> 00:34:51.260
And I made the very simple point
that how we learn about others

00:34:51.260 --> 00:34:55.620
culturally is very determined
by the instruments of search

00:34:55.620 --> 00:34:57.920
and what we see in terms of
the ordering and filtering

00:34:57.920 --> 00:35:02.480
of search results that
then influence how we know

00:35:02.480 --> 00:35:03.770
and understand one another.

00:35:03.770 --> 00:35:06.170
And we even know
with Wikipedia, I

00:35:06.170 --> 00:35:07.790
don't know if you
know this research,

00:35:07.790 --> 00:35:10.160
that it tends to still
be very asymmetrically

00:35:10.160 --> 00:35:12.980
authored by men and
specifically men

00:35:12.980 --> 00:35:15.020
from Europe and North America.

00:35:15.020 --> 00:35:18.271
And that's generally a story
of technology at this point,

00:35:18.271 --> 00:35:18.770
right?

00:35:18.770 --> 00:35:20.610
But it doesn't have
to be that way.

00:35:20.610 --> 00:35:24.110
So I give the very simple
example in this article

00:35:24.110 --> 00:35:26.900
I wrote for "Quartz," and
also in my book itself,

00:35:26.900 --> 00:35:30.530
that we can do something
better about this.

00:35:30.530 --> 00:35:33.020
One way to-- and
the example I gave

00:35:33.020 --> 00:35:34.820
is of the country,
just kind of random,

00:35:34.820 --> 00:35:37.010
of the country Cameroon
in West Africa.

00:35:37.010 --> 00:35:40.230
I was invited by UNESCO
to visit that country.

00:35:40.230 --> 00:35:41.390
And I search for Cameroon.

00:35:41.390 --> 00:35:43.820
Of course, I use
Google like everybody.

00:35:43.820 --> 00:35:46.840
And the first couple of results
I get on Google, in fact,

00:35:46.840 --> 00:35:49.130
my first page of
search results, I

00:35:49.130 --> 00:35:51.350
didn't see a single
web page from Cameroon,

00:35:51.350 --> 00:35:54.770
which is actually pretty high
in internet penetration rate,

00:35:54.770 --> 00:35:57.800
quite educated, and
anglophone, and francophone.

00:35:57.800 --> 00:36:00.030
So it's not just a
francophone country.

00:36:00.030 --> 00:36:01.790
So why was it that
I was seeing that?

00:36:01.790 --> 00:36:03.500
I thought Google
might know me better.

00:36:03.500 --> 00:36:05.510
But in this
particular case, I was

00:36:05.510 --> 00:36:07.940
seeing content that likely
was correlated to that, which

00:36:07.940 --> 00:36:11.720
was more popular and
validated perhaps by PageRank

00:36:11.720 --> 00:36:14.810
and various forms of
back linking, you know,

00:36:14.810 --> 00:36:18.680
mass validation, which is not a
problematic concept in my mind.

00:36:18.680 --> 00:36:21.170
But mass validation
is not always

00:36:21.170 --> 00:36:22.940
what should count
as knowledge, and we

00:36:22.940 --> 00:36:25.310
know what comes up in our
search results essentially

00:36:25.310 --> 00:36:26.870
are treated as
truth and knowledge

00:36:26.870 --> 00:36:30.980
by a large percentage of users,
and in many cases, by myself.

00:36:30.980 --> 00:36:33.530
So I kind of am concerned
with these issues.

00:36:33.530 --> 00:36:36.110
I write about these
issues in this book

00:36:36.110 --> 00:36:38.660
and in my second book, which
came out with my colleague Adam

00:36:38.660 --> 00:36:40.820
Fish, where we write
about kind of hacker

00:36:40.820 --> 00:36:45.170
examples, and the Silk Road, and
the Pirate Party, and Icelands.

00:36:45.170 --> 00:36:48.110
You know, these are all things
you can ask me about later,

00:36:48.110 --> 00:36:50.870
and even the examples of
powerful uses of technology

00:36:50.870 --> 00:36:52.730
in the context of
the Arab Spring.

00:36:52.730 --> 00:36:55.970
I did my field work just kind
of after I wrote this first book

00:36:55.970 --> 00:36:57.710
in Egypt in the middle
of the Arab Spring

00:36:57.710 --> 00:37:00.050
looking at what people
were doing with technology,

00:37:00.050 --> 00:37:02.600
old and new, not
just new technology,

00:37:02.600 --> 00:37:06.470
also old technologies, and using
projectors, using bedsheets,

00:37:06.470 --> 00:37:08.470
taking stuff that
they saw on YouTube

00:37:08.470 --> 00:37:11.090
and projecting it
into public spaces.

00:37:11.090 --> 00:37:13.370
And all of this is
really, really powerful

00:37:13.370 --> 00:37:15.066
and has great promise.

00:37:15.066 --> 00:37:16.940
One of the major arguments
I make in the book

00:37:16.940 --> 00:37:19.970
that I spoke about earlier,
I referred to earlier,

00:37:19.970 --> 00:37:21.410
is this concept of ontology.

00:37:21.410 --> 00:37:23.360
I describe it in chapter--

00:37:23.360 --> 00:37:25.640
basically in chapter 1,
but I really elaborate it

00:37:25.640 --> 00:37:28.460
in chapter 3 and chapter
4 based on fieldwork

00:37:28.460 --> 00:37:34.310
that I did collaboratively
from about 2002 to about 2014

00:37:34.310 --> 00:37:36.629
with various Native
American populations, where

00:37:36.629 --> 00:37:38.420
I was thinking about
how do I build systems

00:37:38.420 --> 00:37:42.310
from languages, to databases,
to algorithms, to interfaces

00:37:42.310 --> 00:37:45.170
that those communities
themselves who are my partners

00:37:45.170 --> 00:37:46.820
could help design with me?

00:37:46.820 --> 00:37:49.760
And I'm not nearly the
engineer that most of you are,

00:37:49.760 --> 00:37:53.180
and I'm not nearly the engineer
that Google is as a whole.

00:37:53.180 --> 00:37:56.330
But I was attempting to deal
with some of these issues

00:37:56.330 --> 00:37:58.980
and think about some of these
issues in the context of trying

00:37:58.980 --> 00:38:00.980
to support communities
who are on the other side

00:38:00.980 --> 00:38:03.920
of the digital divide and
certainly simply being

00:38:03.920 --> 00:38:07.340
connected to the internet is not
good enough to actually support

00:38:07.340 --> 00:38:09.320
their voices and their
agendas, especially

00:38:09.320 --> 00:38:11.810
people who are, in
many cases, have

00:38:11.810 --> 00:38:15.230
had a great deal of cultural
and political trauma.

00:38:15.230 --> 00:38:18.320
So one example I give in
chapter three of the book

00:38:18.320 --> 00:38:21.620
is an ontology that I designed
with these communities,

00:38:21.620 --> 00:38:23.840
something like what we
call in information studies

00:38:23.840 --> 00:38:25.880
an information architecture.

00:38:25.880 --> 00:38:28.310
So folks in these
communities were building.

00:38:28.310 --> 00:38:31.550
This was a project I did with
19 Native American reservations

00:38:31.550 --> 00:38:33.530
between 2003 and 2005.

00:38:33.530 --> 00:38:36.530
It's described in
chapter 3 of the book.

00:38:36.530 --> 00:38:41.180
And in that part of the book, I
describe how these communities

00:38:41.180 --> 00:38:44.639
are attempting to take advantage
of this internet infrastructure

00:38:44.639 --> 00:38:46.930
that they have built and
owned with the help of Hewlett

00:38:46.930 --> 00:38:50.030
Packard and actually
build a system for them

00:38:50.030 --> 00:38:52.130
to communicate with one
another, and to build

00:38:52.130 --> 00:38:55.730
their own local economies, and
preserve culture, et cetera.

00:38:55.730 --> 00:38:57.860
And so they were building
and designing the system

00:38:57.860 --> 00:39:00.900
according to this architecture,
again, pretty hierarchical,

00:39:00.900 --> 00:39:02.660
but they were naming
categories back

00:39:02.660 --> 00:39:05.659
when we used to think
about tagging, and Web 2.0,

00:39:05.659 --> 00:39:07.950
and all this kind of like
what Tim O'Reilly wrote about

00:39:07.950 --> 00:39:09.950
back in the day, and
they were basically

00:39:09.950 --> 00:39:12.320
putting content and
sharing it with one another

00:39:12.320 --> 00:39:14.630
and deciding what
categories were relevant

00:39:14.630 --> 00:39:17.700
and how those categories would
be related to one another.

00:39:17.700 --> 00:39:20.810
So what this is, is the power
of naming and giving people

00:39:20.810 --> 00:39:24.350
the opportunity to classify
their own corpuses,

00:39:24.350 --> 00:39:26.960
if you will, within their
communications systems

00:39:26.960 --> 00:39:29.666
to support hopefully
one another.

00:39:29.666 --> 00:39:31.340
Another example I
give in the book

00:39:31.340 --> 00:39:34.100
in a large amount of
detail in chapter 4

00:39:34.100 --> 00:39:35.720
is of work I did with the Zuni.

00:39:35.720 --> 00:39:38.990
This has all been funded by the
National Science Foundation.

00:39:38.990 --> 00:39:41.570
And in this work,
I describe what

00:39:41.570 --> 00:39:44.780
happens when a group of people,
and a Native American community

00:39:44.780 --> 00:39:47.840
in New Mexico, are able
to build and design,

00:39:47.840 --> 00:39:51.650
with my help, a digital museum
system where they can actually

00:39:51.650 --> 00:39:54.380
get access to images
of objects that

00:39:54.380 --> 00:39:56.630
were taken from their
communities that are sitting

00:39:56.630 --> 00:39:58.100
in museums all over the world.

00:39:58.100 --> 00:40:00.140
And this is another
part and promise

00:40:00.140 --> 00:40:02.810
of the internet, the recovery
and hopefully promotion

00:40:02.810 --> 00:40:04.940
of cultural heritage
projects, which

00:40:04.940 --> 00:40:06.950
I'm also very interested in.

00:40:06.950 --> 00:40:08.870
And one of the coolest
parts of the book

00:40:08.870 --> 00:40:11.870
is the story I tell
toward the end of chapter

00:40:11.870 --> 00:40:15.770
4 of the Zuni coming together
to look at this system

00:40:15.770 --> 00:40:19.610
that we've built where
1,500 approximately objects

00:40:19.610 --> 00:40:22.340
from different museums are now
being made available to them

00:40:22.340 --> 00:40:24.020
as images.

00:40:24.020 --> 00:40:25.580
And look what's happening here.

00:40:25.580 --> 00:40:29.390
It's not one person per
computer, and it's not like--

00:40:29.390 --> 00:40:32.540
it's not this kind of
individualized experience.

00:40:32.540 --> 00:40:36.170
To look at an object, the whole
community, and different folks

00:40:36.170 --> 00:40:38.150
within the community of
different age groups,

00:40:38.150 --> 00:40:41.060
and different what we call
kiva, or kinship groups,

00:40:41.060 --> 00:40:42.530
come together.

00:40:42.530 --> 00:40:44.300
And as they're
looking at an object--

00:40:44.300 --> 00:40:46.758
in this particular case, I'm
talking about a project called

00:40:46.758 --> 00:40:49.400
the Anahoho, an
image of an Anahoho,

00:40:49.400 --> 00:40:52.730
which is like a kachina, people
are putting their fingers

00:40:52.730 --> 00:40:53.960
in their ears.

00:40:53.960 --> 00:40:55.644
And other people are
leaving the room.

00:40:55.644 --> 00:40:57.560
Other people from other
parts of the community

00:40:57.560 --> 00:40:58.970
are coming in the room.

00:40:58.970 --> 00:41:01.490
And that's because
knowledge at Zuni

00:41:01.490 --> 00:41:04.719
is really based on who
you are in the community.

00:41:04.719 --> 00:41:06.260
And I want to make
that point to make

00:41:06.260 --> 00:41:09.050
the point that as we think
about Google in relation

00:41:09.050 --> 00:41:11.550
to diverse cultures in
different parts of the world,

00:41:11.550 --> 00:41:14.840
we have to understand those
cultural norms and values that

00:41:14.840 --> 00:41:17.360
are part of how people--
what people know

00:41:17.360 --> 00:41:19.350
and how people
share information.

00:41:19.350 --> 00:41:22.212
And that's a really important
ethical question as well.

00:41:22.212 --> 00:41:24.170
So the reason people were
putting their fingers

00:41:24.170 --> 00:41:26.690
in their ears is
they were not yet

00:41:26.690 --> 00:41:29.834
at the point in the community
in terms of like kind of a--

00:41:29.834 --> 00:41:30.750
what do you call that?

00:41:30.750 --> 00:41:34.450
Like a serious
ceremonial sort of kind

00:41:34.450 --> 00:41:36.610
of becoming-- initiation, right?

00:41:36.610 --> 00:41:39.130
To actually know that
knowledge, right?

00:41:39.130 --> 00:41:40.910
And at different points--

00:41:40.910 --> 00:41:42.670
so looking at one
object and getting them

00:41:42.670 --> 00:41:45.970
to share information for
themselves and with the museums

00:41:45.970 --> 00:41:48.250
took like one and a half hours.

00:41:48.250 --> 00:41:49.750
And they would
change the languages

00:41:49.750 --> 00:41:52.467
with which they would
speak about the object.

00:41:52.467 --> 00:41:53.800
So it's really, really powerful.

00:41:53.800 --> 00:41:57.490
To me, this is an example
of technology as a catalyst

00:41:57.490 --> 00:42:00.370
for a culture as it is, right?

00:42:00.370 --> 00:42:02.954
And so my book is
not just critical,

00:42:02.954 --> 00:42:04.870
but it's really concerned
with these questions

00:42:04.870 --> 00:42:07.840
of how can we develop
and build technologies

00:42:07.840 --> 00:42:11.110
that serve people, that serve
the economic, and political,

00:42:11.110 --> 00:42:13.660
and cultural interests
of those communities?

00:42:13.660 --> 00:42:16.390
So just to kind of
like quickly wrap up,

00:42:16.390 --> 00:42:18.910
this is perhaps like the
coolest project in the world

00:42:18.910 --> 00:42:20.170
that I'm working on right now.

00:42:20.170 --> 00:42:22.060
I'm like really
excited about this.

00:42:22.060 --> 00:42:26.650
This is-- this is work I'm doing
with the group Rhizomatica, who

00:42:26.650 --> 00:42:30.130
by the way, recently
won a Google prize.

00:42:30.130 --> 00:42:32.740
So thank you for
supporting them.

00:42:32.740 --> 00:42:37.360
They are the largest
community-owned cell phone

00:42:37.360 --> 00:42:40.300
network in the world
in Southern Mexico.

00:42:40.300 --> 00:42:42.490
They're in the mountains
all around Oaxaca,

00:42:42.490 --> 00:42:44.980
which is like a magical,
magical place, one

00:42:44.980 --> 00:42:47.530
of the most biodiverse and
culturally-diverse parts

00:42:47.530 --> 00:42:51.850
of the world, dozens of Zapotec,
Mixtec, and Mixe languages.

00:42:51.850 --> 00:42:53.980
And these communities
were not served

00:42:53.980 --> 00:42:57.835
by big telecom, specifically
Carlos Slim and Telcel, who

00:42:57.835 --> 00:43:00.160
is one of the richest
guys in the world, right?

00:43:00.160 --> 00:43:03.670
So they said, hey, we want
these communication rights.

00:43:03.670 --> 00:43:06.370
Our constitution legitimates
the communication rights

00:43:06.370 --> 00:43:09.010
for indigenous
peoples in Mexico.

00:43:09.010 --> 00:43:11.060
We're going to build
our own networks.

00:43:11.060 --> 00:43:13.630
So they are building their own
collectively-owned networks.

00:43:13.630 --> 00:43:16.280
And we see examples of
that in the context also

00:43:16.280 --> 00:43:18.130
of net neutrality here
in the United States

00:43:18.130 --> 00:43:21.160
in places like Detroit,
in Redhook in Brooklyn,

00:43:21.160 --> 00:43:22.840
in other parts of
the world as well.

00:43:22.840 --> 00:43:25.870
The largest kind of
collectively-owned, sorry,

00:43:25.870 --> 00:43:28.810
internet network, mesh
network, in the world right

00:43:28.810 --> 00:43:32.140
now is a place called
Guifinet in Catalunya.

00:43:32.140 --> 00:43:36.640
And Guifi is how they say Wi-Fi,
so I think that's really funny.

00:43:36.640 --> 00:43:38.970
So this is really, really
an amazing project.

00:43:38.970 --> 00:43:41.530
I've been doing ethnographic
work out in these mountain

00:43:41.530 --> 00:43:44.860
regions, looking at how these
networks are being built,

00:43:44.860 --> 00:43:47.140
what's produced out of
collective ownership,

00:43:47.140 --> 00:43:49.120
does it support
people's economies?

00:43:49.120 --> 00:43:51.070
How does it support
people who speak

00:43:51.070 --> 00:43:52.750
languages that have
never been written

00:43:52.750 --> 00:43:54.580
because of colonial histories?

00:43:54.580 --> 00:43:55.810
All of this, right?

00:43:55.810 --> 00:43:58.150
What can emerge
out of the rhizome,

00:43:58.150 --> 00:44:01.570
the rhizomatica, the rhizome
that is this project, right?

00:44:01.570 --> 00:44:04.170
So the project called
Rhizomatica, in Spanish,

00:44:04.170 --> 00:44:07.120
they also call it Telefono
Indigena Comunitaria,

00:44:07.120 --> 00:44:09.190
Indigenous Community Telephones.

00:44:09.190 --> 00:44:11.560
So it's a really,
really amazing example

00:44:11.560 --> 00:44:14.860
of how not just we as human
beings, but our communities

00:44:14.860 --> 00:44:17.209
themselves can take
power over technology.

00:44:17.209 --> 00:44:19.000
And as I said, it's
really cool that Google

00:44:19.000 --> 00:44:21.730
has been supporting this
project without trying

00:44:21.730 --> 00:44:23.960
to own it or own its data.

00:44:23.960 --> 00:44:27.520
So I think that
that's to your credit.

00:44:27.520 --> 00:44:30.190
So the last thing I'll
just mention really briefly

00:44:30.190 --> 00:44:31.270
is I'm starting to write.

00:44:31.270 --> 00:44:33.180
This is from my new
book, my third book

00:44:33.180 --> 00:44:34.679
I'm working on right
now, which will

00:44:34.679 --> 00:44:36.580
be a trade book with
the MIT Press called--

00:44:36.580 --> 00:44:38.740
I think I'm going to
call it "We the Users,"

00:44:38.740 --> 00:44:40.390
and I'm writing
about examples of how

00:44:40.390 --> 00:44:42.760
we can head into a
technology future that

00:44:42.760 --> 00:44:44.860
is human and
collective that makes

00:44:44.860 --> 00:44:47.290
sure people have enough
money, make sure people

00:44:47.290 --> 00:44:49.160
have portable health benefits.

00:44:49.160 --> 00:44:51.430
So in this context, I have
been talking to people also

00:44:51.430 --> 00:44:53.860
like Sam Altman
from Y Combinator

00:44:53.860 --> 00:44:56.560
about the universal
basic income movement.

00:44:56.560 --> 00:44:58.249
I did a really cool--

00:44:58.249 --> 00:45:00.040
so I've looked at this
example from Sweden,

00:45:00.040 --> 00:45:02.040
and I did a really cool
interview with this guy.

00:45:02.040 --> 00:45:04.810
He's a Stanford graduate, one of
the youngest major-city mayors

00:45:04.810 --> 00:45:07.900
in the country, I interviewed
him yesterday, Michael Tubbs.

00:45:07.900 --> 00:45:10.820
He was on the Bill Maher
show just like two weeks ago.

00:45:10.820 --> 00:45:12.980
And Michael is
only 27 years old.

00:45:12.980 --> 00:45:16.150
He's a Stanford Rhodes scholar,
comes from single-family home.

00:45:16.150 --> 00:45:18.970
His mother was near
the poverty line,

00:45:18.970 --> 00:45:22.210
and he's implementing the
universal basic income project

00:45:22.210 --> 00:45:25.294
with the help of some folks
connected to the Obama

00:45:25.294 --> 00:45:26.710
administration,
actually, Google's

00:45:26.710 --> 00:45:29.466
been kind of on the
side involved with this,

00:45:29.466 --> 00:45:31.090
to try to think about
what happens when

00:45:31.090 --> 00:45:33.460
you give people, poor
people, especially

00:45:33.460 --> 00:45:34.900
in Stockton, $500 a month.

00:45:34.900 --> 00:45:36.910
What do they do with
the money, right?

00:45:36.910 --> 00:45:39.190
And as an experiment,
not necessarily as

00:45:39.190 --> 00:45:41.300
some sort of legitimate
path forward,

00:45:41.300 --> 00:45:43.171
but as an experiment itself.

00:45:43.171 --> 00:45:45.420
So these are a couple of the
things I'm writing about.

00:45:45.420 --> 00:45:47.140
In my new book, I'm
writing about where

00:45:47.140 --> 00:45:51.310
we can go to kind of balance
flexibility, creativity,

00:45:51.310 --> 00:45:54.430
innovation, as it's defined
here in Silicon Valley,

00:45:54.430 --> 00:45:58.240
with innovation in relation
to people and their lives

00:45:58.240 --> 00:46:02.440
and really, more than anything,
the implications of all this

00:46:02.440 --> 00:46:05.590
on our world in terms
of political equality,

00:46:05.590 --> 00:46:08.320
and democracy,
economic equality,

00:46:08.320 --> 00:46:10.990
and really allowing
our diverse world

00:46:10.990 --> 00:46:13.870
to maintain its diversity
through the technologies we

00:46:13.870 --> 00:46:16.210
built, right, rather
than flatten or reduce

00:46:16.210 --> 00:46:17.230
that diversity.

00:46:17.230 --> 00:46:18.592
I think all three of those--

00:46:18.592 --> 00:46:20.050
I'm kind of
overwhelmed because I'm

00:46:20.050 --> 00:46:22.440
trying to read about all
three at the same time,

00:46:22.440 --> 00:46:24.940
and I've had an opportunity to
talk to some really important

00:46:24.940 --> 00:46:30.040
and major figures, Vicente Fox,
the former president of Mexico.

00:46:30.040 --> 00:46:34.210
I'm talking to David Axelrod
from Obama 2008, and 2012,

00:46:34.210 --> 00:46:38.410
and on CNN next week, Van Jones,
other folks, Elizabeth Warren.

00:46:38.410 --> 00:46:41.140
I've had a chance to get these
people's voices very briefly

00:46:41.140 --> 00:46:43.780
in the book itself and
also a number of folks here

00:46:43.780 --> 00:46:44.530
in Silicon Valley.

00:46:44.530 --> 00:46:48.290
I spoke to the head of Diversity
Inclusion here at Google.

00:46:48.290 --> 00:46:50.260
You're the only
major tech company

00:46:50.260 --> 00:46:51.520
that's talked to me so far.

00:46:51.520 --> 00:46:52.720
So thank you.

00:46:52.720 --> 00:46:54.760
Thank you for that.

00:46:54.760 --> 00:46:57.160
And I'm also writing about
examples, not just like these,

00:46:57.160 --> 00:47:01.390
of Michael and the Swedish
example, but also of the AI lab

00:47:01.390 --> 00:47:04.630
in Makarere
University in Uganda,

00:47:04.630 --> 00:47:06.350
which is so interesting.

00:47:06.350 --> 00:47:08.470
It's an example of a company--

00:47:08.470 --> 00:47:10.900
sorry, an AI laboratory
that's attempting

00:47:10.900 --> 00:47:14.740
to build artificial intelligence
models that are somehow

00:47:14.740 --> 00:47:17.260
supportive of Ugandan
interests, but also

00:47:17.260 --> 00:47:19.772
are seeded with Ugandan
data, as well as

00:47:19.772 --> 00:47:21.730
built around learning
models that are hopefully

00:47:21.730 --> 00:47:26.720
expressive of the cultures and
communities of Uganda itself.

00:47:26.720 --> 00:47:28.780
So this is a whole body of work.

00:47:28.780 --> 00:47:32.500
I would love to come back and
share more from this new book

00:47:32.500 --> 00:47:33.915
when it comes out.

00:47:33.915 --> 00:47:36.040
And I'm really excited to
get some of your feedback

00:47:36.040 --> 00:47:37.330
and thoughts on all of this.

00:47:37.330 --> 00:47:39.970
I tried to throw a
lot at you, but that's

00:47:39.970 --> 00:47:42.760
because I am really excited
to hear your thoughts on this.

00:47:42.760 --> 00:47:44.075
Thank you for having me.

00:47:44.075 --> 00:47:45.560
[APPLAUSE]

00:47:53.242 --> 00:47:54.950
AUDIENCE: OK, I actually
have a question.

00:47:54.950 --> 00:48:03.220
So you mentioned the power
of these algorithms because

00:48:03.220 --> 00:48:05.170
of the scale that they have.

00:48:05.170 --> 00:48:06.860
And so when you take
that into account

00:48:06.860 --> 00:48:10.690
and then also take into account
the inevitable fallibility

00:48:10.690 --> 00:48:17.830
of humans who are creating
them, what can companies

00:48:17.830 --> 00:48:22.720
do to hold themselves
accountable ethically,

00:48:22.720 --> 00:48:25.150
like, you know,
maybe by training

00:48:25.150 --> 00:48:28.480
its employees a certain way
or whether there needs to be

00:48:28.480 --> 00:48:30.850
some sort of auditing process?

00:48:30.850 --> 00:48:34.787
Have you thought
about should that be--

00:48:34.787 --> 00:48:37.120
should the onus for that be
on the companies themselves,

00:48:37.120 --> 00:48:39.110
or should it be government?

00:48:39.110 --> 00:48:41.140
Yeah, maybe just talk a
little bit about that.

00:48:41.140 --> 00:48:42.430
RAMESH SRINIVASAN: Yeah,
thank you for asking that.

00:48:42.430 --> 00:48:44.290
I mean, I appreciate
the sympathy

00:48:44.290 --> 00:48:47.350
for what I'm trying to
say in that question.

00:48:47.350 --> 00:48:50.920
So absolutely, I
think that they're not

00:48:50.920 --> 00:48:54.250
only should be kind of
internal auditing processes.

00:48:54.250 --> 00:48:55.750
I think that--

00:48:55.750 --> 00:48:59.740
I think one of them more
brilliant design companies

00:48:59.740 --> 00:49:02.110
here in Silicon Valley
was IDO, and they

00:49:02.110 --> 00:49:04.930
were really smart for
having anthropologists

00:49:04.930 --> 00:49:07.660
and sociologists in the
room with their engineers

00:49:07.660 --> 00:49:08.530
and their designers.

00:49:08.530 --> 00:49:10.330
And I think that
having teams that

00:49:10.330 --> 00:49:12.760
are more inclusive
and multi-disciplinary

00:49:12.760 --> 00:49:14.930
in the design, and
kind of engineering,

00:49:14.930 --> 00:49:17.690
and even evaluation process
is really important.

00:49:17.690 --> 00:49:21.270
I think that one
proposal I've had

00:49:21.270 --> 00:49:23.770
that I've been talking about
for the last year or two

00:49:23.770 --> 00:49:27.300
is even giving folks an
opportunity on a heuristic

00:49:27.300 --> 00:49:30.550
level, right, like a
descriptive level, of helping

00:49:30.550 --> 00:49:33.700
them understand why they
see what they see, right?

00:49:33.700 --> 00:49:37.477
I understand you can't give up
private software code, right?

00:49:37.477 --> 00:49:39.310
I don't think people
are interested in that,

00:49:39.310 --> 00:49:41.130
nor would they even
understand it, right?

00:49:41.130 --> 00:49:43.600
Now, I don't think almost
any of us would, right?

00:49:43.600 --> 00:49:46.480
But at the same time, you
can explain to people,

00:49:46.480 --> 00:49:48.860
this is optimized
for this output.

00:49:48.860 --> 00:49:51.370
And here are some
other options in terms

00:49:51.370 --> 00:49:55.090
of what you could see based on
other sorts of, if you will,

00:49:55.090 --> 00:49:58.480
language or values by which
something is optimized for.

00:49:58.480 --> 00:50:01.750
But I also believe when
we talk about technologies

00:50:01.750 --> 00:50:03.850
that are kind of blurred
into other realms,

00:50:03.850 --> 00:50:06.430
right, like into
our political lives,

00:50:06.430 --> 00:50:09.640
into our educational systems,
into our criminal policing

00:50:09.640 --> 00:50:13.240
and justice systems,
economic systems, there

00:50:13.240 --> 00:50:16.510
has to be some sort
of third party,

00:50:16.510 --> 00:50:21.430
not necessarily regulating
as much as coordinating

00:50:21.430 --> 00:50:24.460
to ensure that there are
checks throughout this process

00:50:24.460 --> 00:50:27.100
to ensure that we do not
legitimated or naturalize

00:50:27.100 --> 00:50:29.800
the biases that we
have, especially,

00:50:29.800 --> 00:50:33.070
not even on an individual
level, but on a level that is

00:50:33.070 --> 00:50:34.930
collective and institutional.

00:50:34.930 --> 00:50:37.030
And that's why I tried
to talk about these more

00:50:37.030 --> 00:50:40.630
collective examples first
like the policing example,

00:50:40.630 --> 00:50:43.120
like the pro-public
article before I talked

00:50:43.120 --> 00:50:46.542
about what Zeynep was saying
about the YouTube algorithm

00:50:46.542 --> 00:50:48.250
because I think it's
far more pernicious,

00:50:48.250 --> 00:50:50.710
these collective
implementations, than just

00:50:50.710 --> 00:50:53.590
the individual, radicalized
content because I've seen

00:50:53.590 --> 00:50:54.962
radicalized content online.

00:50:54.962 --> 00:50:56.170
I don't mind looking at that.

00:50:56.170 --> 00:50:58.790
I don't think that
transforms me necessarily.

00:50:58.790 --> 00:51:01.150
But I think on a larger
scale level, it's an issue.

00:51:05.322 --> 00:51:07.030
AUDIENCE: So I guess
I actually just want

00:51:07.030 --> 00:51:09.730
to build off of what's
already been asked

00:51:09.730 --> 00:51:15.070
and what's been answered
because I think when

00:51:15.070 --> 00:51:17.440
Mark Zuckerberg first
gave his interview

00:51:17.440 --> 00:51:19.270
after the whole
Facebook-Russia-Cambridge

00:51:19.270 --> 00:51:24.250
Analytica scandal, the
summary was, who knew?

00:51:24.250 --> 00:51:27.130
I had no idea Facebook
would ever do this.

00:51:27.130 --> 00:51:31.420
And that seems to be a pretty
common critique of technocrats

00:51:31.420 --> 00:51:35.170
of CEOs of tech companies that,
they break first, and then

00:51:35.170 --> 00:51:38.620
think about the
consequences later.

00:51:38.620 --> 00:51:42.700
And I think that cycle is
further exacerbated by the fact

00:51:42.700 --> 00:51:45.940
that a lot of these companies
are going public really quickly

00:51:45.940 --> 00:51:51.260
and are essentially pushed
to grow, and grow, and grow.

00:51:51.260 --> 00:51:54.250
And so you'll see companies
like Facebook, and Apple,

00:51:54.250 --> 00:51:57.820
and Google try to break in in
China for the sake of growth,

00:51:57.820 --> 00:51:59.500
for the sake of
revenue, not necessarily

00:51:59.500 --> 00:52:01.440
for the sake of
inclusion, I think,

00:52:01.440 --> 00:52:03.470
but for the sake of money.

00:52:03.470 --> 00:52:09.619
And so how do we-- how do
we impose some kind of,

00:52:09.619 --> 00:52:12.160
not necessarily regulation, like
you said, but like some kind

00:52:12.160 --> 00:52:19.060
of like just balance, some
check to essentially guard

00:52:19.060 --> 00:52:21.970
for these biases for this
desire for growth as opposed

00:52:21.970 --> 00:52:25.780
to like inclusion and
diversity, and you

00:52:25.780 --> 00:52:30.100
know, corrections for
algorithmic biases in ML models

00:52:30.100 --> 00:52:32.340
and things similar.

00:52:32.340 --> 00:52:34.840
RAMESH SRINIVASAN: Yeah, what
a great question, and also, it

00:52:34.840 --> 00:52:37.600
allows me to mention one
other idea I had, which is,

00:52:37.600 --> 00:52:40.060
giving people the
opportunity to visualize

00:52:40.060 --> 00:52:42.710
why they see what they see and
choose alternatives, right?

00:52:42.710 --> 00:52:45.850
Like a lot of us as designers
use to build and design

00:52:45.850 --> 00:52:47.560
systems that were
visually based,

00:52:47.560 --> 00:52:49.944
that were kind of multi-variate
in their kind of scope,

00:52:49.944 --> 00:52:51.110
and we can think about that.

00:52:51.110 --> 00:52:54.210
So let me answer your question.

00:52:54.210 --> 00:52:56.430
It's an incredibly
difficult issue, right?

00:52:56.430 --> 00:52:59.490
Because, you know, Tim
O'Reilly makes the point

00:52:59.490 --> 00:53:03.579
that the master algorithm of all
is market cap valuation, right?

00:53:03.579 --> 00:53:06.120
And it's an interesting point
for a father figure in computer

00:53:06.120 --> 00:53:08.990
science to say that, right,
who has made a lot of money off

00:53:08.990 --> 00:53:13.470
of his own publishing industry.

00:53:13.470 --> 00:53:16.500
I think that we have
to figure out ways

00:53:16.500 --> 00:53:21.060
to experiment with other
models to see what--

00:53:21.060 --> 00:53:23.580
and we can do this in a kind
of small-scale, lightweight

00:53:23.580 --> 00:53:28.080
fashion, we being folks in these
in companies like yourselves,

00:53:28.080 --> 00:53:32.460
to see if they are
actually creating returns

00:53:32.460 --> 00:53:36.660
that are similar or perhaps
even close to the current model,

00:53:36.660 --> 00:53:37.170
right?

00:53:37.170 --> 00:53:39.070
So I've been looking
at some research that

00:53:39.070 --> 00:53:44.880
is showing that you can
build very persuasive, very

00:53:44.880 --> 00:53:50.430
strong engagement products with
more diverse design teams, that

00:53:50.430 --> 00:53:55.860
even having diversity in
terms of VC investment

00:53:55.860 --> 00:53:58.950
could actually incubate highly
lucrative and profitable

00:53:58.950 --> 00:54:00.780
industries as well.

00:54:00.780 --> 00:54:07.440
Of course, it's difficult to
want to break something or even

00:54:07.440 --> 00:54:10.140
tinker with something that
is so wildly successful.

00:54:10.140 --> 00:54:12.660
And I would say Google is
nothing if not successful,

00:54:12.660 --> 00:54:15.280
right, in terms of that level.

00:54:15.280 --> 00:54:17.970
However, move fast and
break things, right,

00:54:17.970 --> 00:54:20.730
which is kind of a casual
motto, right, that Facebook

00:54:20.730 --> 00:54:22.800
embraced and--

00:54:22.800 --> 00:54:23.940
you know, and we get it.

00:54:23.940 --> 00:54:27.240
Like, we're-- I'm a
former engineer, right?

00:54:27.240 --> 00:54:29.580
That's just meant in that
playful way that a lot of us

00:54:29.580 --> 00:54:30.080
talk.

00:54:30.080 --> 00:54:32.010
Just like at MIT, we use
the word hack, right,

00:54:32.010 --> 00:54:35.070
like in a very loose sense.

00:54:35.070 --> 00:54:38.050
But I think what
we're realizing is

00:54:38.050 --> 00:54:42.010
we can't break other
aspects of our lives

00:54:42.010 --> 00:54:46.860
and kind of overweight simply
our economic bottom lines.

00:54:46.860 --> 00:54:51.510
Otherwise, the blowback
in terms of public PR,

00:54:51.510 --> 00:54:54.900
but also maybe our own
internal notions of ethics

00:54:54.900 --> 00:54:57.840
and what we're standing for,
could be compromised, right?

00:54:57.840 --> 00:55:01.980
So you know-- so I
guess my question--

00:55:01.980 --> 00:55:05.460
I would just encourage
folks to think about

00:55:05.460 --> 00:55:08.560
whether there are some different
kinds of lightweight models.

00:55:08.560 --> 00:55:11.910
It's not a very radical
proposal, in my mind, at least,

00:55:11.910 --> 00:55:15.360
by which we could experiment
with other kinds of models

00:55:15.360 --> 00:55:19.470
of being inclusive, other kinds
of models of being transparent,

00:55:19.470 --> 00:55:22.350
other kinds of models
of being transparent,

00:55:22.350 --> 00:55:27.850
I'm sorry, kind of
accountable, right?

00:55:27.850 --> 00:55:32.010
And I guess the last thing I'll
say is the network effects,

00:55:32.010 --> 00:55:33.990
I didn't mention this,
the network effects

00:55:33.990 --> 00:55:37.620
of having mass amounts of users
without necessarily having

00:55:37.620 --> 00:55:43.410
to be as accountable in terms of
our governance of those systems

00:55:43.410 --> 00:55:45.285
is making a lot of money
for Facebook, right,

00:55:45.285 --> 00:55:47.243
and it probably makes a
lot of money at Google.

00:55:47.243 --> 00:55:49.260
You have a much larger
global governance team

00:55:49.260 --> 00:55:51.870
than Facebook does, which is
about two dozen people, two

00:55:51.870 --> 00:55:54.810
dozen people, for I don't know
how many countries Facebook

00:55:54.810 --> 00:55:56.610
has, dozens of countries, right?

00:55:56.610 --> 00:55:58.140
2-plus billion users.

00:55:58.140 --> 00:56:01.710
If you only invest that amount
of money for two dozen people

00:56:01.710 --> 00:56:04.680
to deal with all the global
effects of your technologies,

00:56:04.680 --> 00:56:06.870
you're saying something
about what you value there.

00:56:06.870 --> 00:56:10.110
It means you're a massive,
massively over-privileging

00:56:10.110 --> 00:56:13.650
the economic network
effects of your technology

00:56:13.650 --> 00:56:16.500
without necessarily
investing in trying

00:56:16.500 --> 00:56:19.980
to curb some of its
potentially pernicious effects.

00:56:19.980 --> 00:56:22.870
And so I guess all of this, we
should put it all on the table,

00:56:22.870 --> 00:56:24.710
we can make very
low-scale investments.

00:56:24.710 --> 00:56:27.750
Google is an experimental
space, experimental company,

00:56:27.750 --> 00:56:29.304
why not try it out?

00:56:29.304 --> 00:56:29.970
Let me help you.

00:56:34.450 --> 00:56:37.250
Going on from that,
this idea of governance,

00:56:37.250 --> 00:56:40.336
I'm interested from
you, from your travels,

00:56:40.336 --> 00:56:43.000
either your travels or
stories from colleagues

00:56:43.000 --> 00:56:46.150
and stuff like this, I'm
interested in the perception

00:56:46.150 --> 00:56:49.060
of governments, elected
governments, whether they

00:56:49.060 --> 00:56:51.910
be at a city level like
Stockton, or country level,

00:56:51.910 --> 00:56:54.674
or even an EU level,
what their thoughts are,

00:56:54.674 --> 00:56:56.590
what you've experienced
around their thoughts,

00:56:56.590 --> 00:56:58.930
around these issues that
you've talked about,

00:56:58.930 --> 00:57:01.570
specifically like the sort
of ontologies, and learning,

00:57:01.570 --> 00:57:05.020
and the biases that might
come from these huge private

00:57:05.020 --> 00:57:10.480
government private systems that
then are either incorporated

00:57:10.480 --> 00:57:15.045
directly or their ideas are
put into play in these systems.

00:57:15.045 --> 00:57:16.420
And I'm interested
in governments

00:57:16.420 --> 00:57:18.950
because governments are
ultimately, hopefully,

00:57:18.950 --> 00:57:23.860
theoretically, accountable
to their citizens.

00:57:23.860 --> 00:57:25.689
What's the intersection
looking like there?

00:57:25.689 --> 00:57:26.980
And what are the opportunities?

00:57:26.980 --> 00:57:28.330
Like, what are some
opportunities to do

00:57:28.330 --> 00:57:29.710
good that you see are
low-hanging fruit?

00:57:29.710 --> 00:57:31.210
RAMESH SRINIVASAN:
Yeah, absolutely.

00:57:31.210 --> 00:57:34.300
I think that it's not
that great right now

00:57:34.300 --> 00:57:36.740
when I kind of go to
different parts of the world.

00:57:36.740 --> 00:57:38.750
The kind of-- the
digital divide was--

00:57:38.750 --> 00:57:40.840
it really shouldn't have
been ever been framed

00:57:40.840 --> 00:57:42.180
about access to technology.

00:57:42.180 --> 00:57:44.890
It was more about the kind of
literacy and the opportunity

00:57:44.890 --> 00:57:47.410
to produce and
create technologies

00:57:47.410 --> 00:57:48.820
in one's own ecosystem.

00:57:48.820 --> 00:57:50.890
That's what the
ultimate divide was.

00:57:50.890 --> 00:57:53.650
And we should also presume
that the mere ability

00:57:53.650 --> 00:57:55.450
to create technology
in a place is somehow

00:57:55.450 --> 00:57:58.570
beneficial to a place
economically or politically.

00:57:58.570 --> 00:58:02.020
But I think that there are
two major strategies that are

00:58:02.020 --> 00:58:04.810
underway amongst the more
kind of, if you will,

00:58:04.810 --> 00:58:07.977
clued in or ahead-of-the-curve
kind of folks that I've seen

00:58:07.977 --> 00:58:09.310
in different parts of the world.

00:58:09.310 --> 00:58:10.990
First is to think
about how to piggy

00:58:10.990 --> 00:58:16.540
back off of these large-scale,
private, technological systems

00:58:16.540 --> 00:58:20.920
and infrastructures and build
local economic and political

00:58:20.920 --> 00:58:23.860
technological ecosystems
on top of it, right?

00:58:23.860 --> 00:58:26.140
So how do we-- so that's
been pretty successful

00:58:26.140 --> 00:58:27.700
in places like India, right?

00:58:27.700 --> 00:58:29.640
You can kind of
say, hey, you know,

00:58:29.640 --> 00:58:31.390
these Google services
are there, but we're

00:58:31.390 --> 00:58:33.370
going to innovate on
top of that and try

00:58:33.370 --> 00:58:37.750
to create systems, and
technologies, and firms

00:58:37.750 --> 00:58:41.560
that are beneficial
to our constituencies.

00:58:41.560 --> 00:58:45.110
I mean, to be honest,
that hasn't fully--

00:58:45.110 --> 00:58:48.790
to me, that's not like the full
way forward, just to be honest.

00:58:48.790 --> 00:58:52.174
The other idea is
to try to build,

00:58:52.174 --> 00:58:53.590
an d this is really
interesting, I

00:58:53.590 --> 00:58:56.060
will be spending some time in
Nairobi this summer, right,

00:58:56.060 --> 00:58:57.650
where Ushahidi was born.

00:58:57.650 --> 00:59:00.640
I think some of us probably know
Ushahidi and other companies.

00:59:00.640 --> 00:59:02.630
It turns out in many
parts of the world,

00:59:02.630 --> 00:59:06.280
I think you all know this,
that there are thriving

00:59:06.280 --> 00:59:08.290
tech incubation communities.

00:59:08.290 --> 00:59:11.080
And Nairobi is one of
the largest, right?

00:59:11.080 --> 00:59:12.700
And so the question
is, is how do we

00:59:12.700 --> 00:59:15.514
start to-- so I'm seeing that
happening in governments, less

00:59:15.514 --> 00:59:16.930
so the Kenyan
government, but it's

00:59:16.930 --> 00:59:19.780
the local kind of
municipal authorities

00:59:19.780 --> 00:59:21.730
are supporting that
form of growth?

00:59:21.730 --> 00:59:25.060
But the issue is the access
to capital and VC funding.

00:59:25.060 --> 00:59:26.450
That's been a big issue.

00:59:26.450 --> 00:59:29.230
So I think what we need
to do is pay attention

00:59:29.230 --> 00:59:32.105
to local innovations and
make them-- make the idea,

00:59:32.105 --> 00:59:34.480
and think about the idea that
innovation is not something

00:59:34.480 --> 00:59:38.800
that simply happens when we have
seemingly infinite resources,

00:59:38.800 --> 00:59:41.650
but innovation also happens
when we have very little

00:59:41.650 --> 00:59:42.910
and we got to hustle, right?

00:59:42.910 --> 00:59:46.330
We just got to like adjust
within constraints, right?

00:59:46.330 --> 00:59:48.820
You see that in all sorts of
parts of the world with what

00:59:48.820 --> 00:59:51.190
people do like on the street,
like, how many have you

00:59:51.190 --> 00:59:53.230
been in various parts
of the global south.

00:59:53.230 --> 00:59:55.480
New Delhi, you'll walk
through the middle of old--

00:59:55.480 --> 00:59:57.310
old parts of New
Delhi, people will

00:59:57.310 --> 01:00:00.310
be re-soldering and rejigging
phones in front of you

01:00:00.310 --> 01:00:03.590
and building informal
economies off of these systems.

01:00:03.590 --> 01:00:06.700
So I guess my point is, as
much as possible, I'm not

01:00:06.700 --> 01:00:10.030
seeing a lot of that promising,
but as much as possible, it

01:00:10.030 --> 01:00:13.180
would be great if local
institutions and government

01:00:13.180 --> 01:00:17.110
institutions could support
kind of other kinds of tech

01:00:17.110 --> 01:00:20.500
incubations and economies
in those parts of the world.

01:00:20.500 --> 01:00:21.800
I'm hopeful that that happens.

01:00:21.800 --> 01:00:24.260
I don't see a lot of
it though right now.

01:00:24.260 --> 01:00:26.840
But I think in general, just
really quick point, in general,

01:00:26.840 --> 01:00:29.650
I don't see governance officials
really understanding technology

01:00:29.650 --> 01:00:31.976
very significantly.

01:00:31.976 --> 01:00:33.726
SPEAKER 1: All right,
I think we have time

01:00:33.726 --> 01:00:37.453
for maybe one more question.

01:00:37.453 --> 01:00:40.930
AUDIENCE: Yeah, thanks,
last week, Google

01:00:40.930 --> 01:00:42.760
announced Duplex,
where we're going

01:00:42.760 --> 01:00:45.580
to have Google send
through algorithms actually

01:00:45.580 --> 01:00:47.110
communicating
directly with humans.

01:00:47.110 --> 01:00:48.850
And one interesting
aspect of this

01:00:48.850 --> 01:00:52.120
was they train the
algorithms, it seems,

01:00:52.120 --> 01:00:55.720
to actually say ums and mhms,
one aspect of our culture,

01:00:55.720 --> 01:00:58.360
here in the US when we talk
to people casually, built in.

01:00:58.360 --> 01:01:00.040
So I'm curious what
your thoughts are

01:01:00.040 --> 01:01:03.150
on the specific technology that
we're developing, how it will

01:01:03.150 --> 01:01:07.300
impact us worldwide culturally,
and what Google could be doing,

01:01:07.300 --> 01:01:09.880
from your perspective,
differently,

01:01:09.880 --> 01:01:11.890
to make sure that our
impacts are minimized?

01:01:11.890 --> 01:01:12.690
RAMESH SRINIVASAN:
I think in general,

01:01:12.690 --> 01:01:14.398
it would be-- it would
be great if Google

01:01:14.398 --> 01:01:15.966
could be pretty transparent.

01:01:15.966 --> 01:01:17.466
It allows me to say
something I want

01:01:17.466 --> 01:01:19.570
to say about kind of
what data it's collecting

01:01:19.570 --> 01:01:20.830
and how it's using that data.

01:01:20.830 --> 01:01:22.621
I mean, it doesn't have
to be too specific,

01:01:22.621 --> 01:01:24.680
but I think that that
could be, in general,

01:01:24.680 --> 01:01:27.220
as these sorts of like
useful technologies

01:01:27.220 --> 01:01:29.290
start to spread in their reach.

01:01:29.290 --> 01:01:31.490
Now, to directly
answer your question,

01:01:31.490 --> 01:01:34.910
I mean, unless these
technologies are--

01:01:34.910 --> 01:01:37.040
I don't know if it's so
much this specifically,

01:01:37.040 --> 01:01:41.610
but I see this as layered on top
of a larger potential problem,

01:01:41.610 --> 01:01:45.470
which call centers are
highly embedded within, which

01:01:45.470 --> 01:01:49.010
is the idea that based
on where the money is

01:01:49.010 --> 01:01:51.770
and where the customers are,
we're going to build protocols

01:01:51.770 --> 01:01:55.962
around technology that are
forced into that logic, right?

01:01:55.962 --> 01:01:57.670
So what I'm getting
at is I actually just

01:01:57.670 --> 01:01:59.430
assigned a paper in
my graduate course

01:01:59.430 --> 01:02:04.760
this last week about the
Americanization of call center

01:02:04.760 --> 01:02:06.210
workers.

01:02:06.210 --> 01:02:08.260
So we're in call centers--

01:02:08.260 --> 01:02:11.210
you all probably heard of
this, like, in call centers,

01:02:11.210 --> 01:02:15.590
folks are taught to take on
American or Western identities,

01:02:15.590 --> 01:02:17.930
to speak American
English, and to even learn

01:02:17.930 --> 01:02:22.230
those forms of speaking that
are familiar to all of us.

01:02:22.230 --> 01:02:25.070
So I think it's really
important, obviously,

01:02:25.070 --> 01:02:28.544
that we don't do the same thing
with the technologies that

01:02:28.544 --> 01:02:30.710
are going to be automated
that we're going to spread

01:02:30.710 --> 01:02:32.280
to other parts of the world.

01:02:32.280 --> 01:02:36.790
But I think, more generally, my
larger point is really about--

01:02:36.790 --> 01:02:39.260
a kind of a sense of
social responsibility

01:02:39.260 --> 01:02:43.340
recognizes that not simply in
the gathering and acquisition

01:02:43.340 --> 01:02:46.250
of data and attention,
that's not the only way

01:02:46.250 --> 01:02:49.640
to produce value, that there are
other ways of producing value,

01:02:49.640 --> 01:02:52.100
and we can have a more
balanced approach toward that.

01:02:52.100 --> 01:02:53.930
But to be honest,
I'm not an expert

01:02:53.930 --> 01:02:56.480
on this particular technology.

01:02:56.480 --> 01:02:58.970
I did see the video, and
I, like everyone else,

01:02:58.970 --> 01:03:00.710
was like wowed by it.

01:03:00.710 --> 01:03:01.910
It went super viral.

01:03:01.910 --> 01:03:04.830
David and I were just
talking about it,

01:03:04.830 --> 01:03:06.920
but I think the questions,
again, of design

01:03:06.920 --> 01:03:09.800
and what it means are
really central here.

01:03:09.800 --> 01:03:12.110
So feel free to ask
me more about it

01:03:12.110 --> 01:03:15.600
once I learn more
about it as well, yeah.

01:03:15.600 --> 01:03:16.500
Thank you.

01:03:16.500 --> 01:03:17.100
SPEAKER 1: Thank you, Ramesh.

01:03:17.100 --> 01:03:18.308
RAMESH SRINIVASAN: Thank you.

01:03:18.308 --> 01:03:19.850
[APPLAUSE]

