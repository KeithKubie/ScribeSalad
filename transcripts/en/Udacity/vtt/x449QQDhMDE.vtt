WEBVTT
Kind: captions
Language: en

00:00:00.330 --> 00:00:03.010
Okay, so now we have all
the pieces of our puzzle.

00:00:03.010 --> 00:00:07.080
The question of course is how we're
going to find those weights w and

00:00:07.080 --> 00:00:11.880
those biases b that will get our
classifier to do what we want it to do.

00:00:11.880 --> 00:00:15.760
That is, have a low distance for
the correct class but

00:00:15.760 --> 00:00:19.400
have a high distance for
the incorrect class.

00:00:19.400 --> 00:00:23.290
One thing you can do is measure that
distance averaged over the entire

00:00:23.290 --> 00:00:28.340
training sets for all the inputs and
all the labels that you have available.

00:00:28.340 --> 00:00:30.240
That's called the training loss.

00:00:30.240 --> 00:00:33.840
This loss, which is the average
cross-entropy over your entire training

00:00:33.840 --> 00:00:37.140
set, Is one humongous function.

00:00:37.140 --> 00:00:40.800
Every example in your training set gets
multiplied by this one big matrix W.

00:00:40.800 --> 00:00:44.190
And then they get all
added up in one big sum.

00:00:45.240 --> 00:00:48.020
We want all the distances to be small,
which would mean we're

00:00:48.020 --> 00:00:51.600
doing a good job at classifying
every example in the training data.

00:00:51.600 --> 00:00:53.130
So we want the loss to be small.

00:00:54.220 --> 00:00:56.890
The loss is a function of
the weights and the biases.

00:00:56.890 --> 00:00:59.140
So we are simply going to try and
minimize that function.

00:01:00.160 --> 00:01:02.730
Imagine that the loss is
a function of two weights.

00:01:02.730 --> 00:01:04.560
Weight one and weight two.

00:01:04.560 --> 00:01:05.970
Just for the sake of argument.

00:01:05.970 --> 00:01:09.670
It's going to be a function which
will be large in some areas, and

00:01:09.670 --> 00:01:11.200
small in others.

00:01:11.200 --> 00:01:15.540
We're going to try the weights which
cause this loss to be the smallest.

00:01:15.540 --> 00:01:18.210
We've just turned
the machine learning problem

00:01:18.210 --> 00:01:20.770
into one of numerical optimization.

00:01:20.770 --> 00:01:24.390
And there's lots of ways to solve
a numerical optimization problem.

00:01:24.390 --> 00:01:29.060
The simplest way is one you've probably
encountered before, gradient descent.

00:01:29.060 --> 00:01:32.260
Take the derivative of your loss,
with respect to your parameters, and

00:01:32.260 --> 00:01:35.635
follow that derivative by
taking a step backwards and

00:01:35.635 --> 00:01:37.890
repeat until you get to the bottom.

00:01:37.890 --> 00:01:40.760
Gradient descent is relatively simple,
especially when you

00:01:40.760 --> 00:01:44.310
have powerful numerical tools that
compute the derivatives for you.

00:01:44.310 --> 00:01:46.370
Remember, I'm showing
you the derivative for

00:01:46.370 --> 00:01:50.630
a function of just two parameters here,
but for a typical problem

00:01:50.630 --> 00:01:54.350
it could be a function of thousands,
millions or even billions of parameters.

