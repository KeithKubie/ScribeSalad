WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:05.170
So, we're gonna start on our very
last topic which is Markov Chains.

00:00:05.170 --> 00:00:09.020
And as you know, I have one more
homework due on the last day of class,

00:00:09.020 --> 00:00:09.690
which is next Friday.

00:00:11.840 --> 00:00:14.510
Almost all the sections
are still meeting today and

00:00:14.510 --> 00:00:19.140
tomorrow because there's really way too
much material for one more section.

00:00:19.140 --> 00:00:22.890
So this week we'll focus on law of
large numbers, central limit theorem,

00:00:22.890 --> 00:00:25.280
multivariate normal,
which we have been doing.

00:00:25.280 --> 00:00:28.750
Then the last section next week then

00:00:28.750 --> 00:00:33.130
the TFs will do examples of Markov chains,
which is our last topic.

00:00:34.900 --> 00:00:38.220
Okay, so I'll just start from
the beginning of Markov chains.

00:00:38.220 --> 00:00:42.000
There's also a handout I wrote on
Markov chains which you can read at

00:00:42.000 --> 00:00:44.800
some point if you haven't seen it already,
just posted under the notes.

00:00:45.890 --> 00:00:51.266
One other thing not to scare anyone
before Thanksgiving, but the finals

00:00:51.266 --> 00:00:55.280
are on December 15th, so it's not really
too soon to start thinking about it.

00:00:55.280 --> 00:01:02.505
I did post a final review handout,
which is 66 pages,

00:01:02.505 --> 00:01:07.745
so there's a lot of stuff you can study.

00:01:07.745 --> 00:01:11.425
The 66 pages consist of
23 pages of review and

00:01:11.425 --> 00:01:15.464
then the rest of it just
consists of literally just all

00:01:15.464 --> 00:01:20.429
the past 110 finals that have
been at Harvard since 2006.

00:01:20.429 --> 00:01:22.499
I'm teaching 110 at every Fall,

00:01:22.499 --> 00:01:26.841
and I just don't really believe in
withholding information, or some people

00:01:26.841 --> 00:01:31.010
get access to more exams than others
cuz they have a friend from whatever.

00:01:31.010 --> 00:01:34.850
This is the entire history of 110 finals,

00:01:34.850 --> 00:01:38.090
there's five full length practice exams,
and

00:01:38.090 --> 00:01:43.310
I think you should be, whenever you can,
starting practice, practice, practice.

00:01:43.310 --> 00:01:46.590
And I posted the solutions
to them as well, but

00:01:46.590 --> 00:01:51.270
you should try your best to
resist looking at the solutions,

00:01:52.870 --> 00:01:56.290
except to check your answers or
if you're really, really stuck.

00:01:56.290 --> 00:02:01.250
So I would highly recommend
doing it at least two or

00:02:01.250 --> 00:02:05.640
three of those exams under as close
as you can to exam conditions.

00:02:05.640 --> 00:02:08.590
That is, time yourself strictly.

00:02:08.590 --> 00:02:12.610
You have four pages of notes double-sided,
that kind of thing.

00:02:12.610 --> 00:02:14.460
Then after each one,
then you can go through it.

00:02:15.500 --> 00:02:19.290
If you run out of time and you're not
done, it's still a good idea to spend some

00:02:19.290 --> 00:02:22.630
time working on the problems still
without looking at the solution.

00:02:22.630 --> 00:02:25.990
And then finally look at the solutions,
study your notes,

00:02:25.990 --> 00:02:28.690
study everything to see
where you can improve.

00:02:30.180 --> 00:02:33.750
So we've covered everything now except for
Markov chains, and

00:02:33.750 --> 00:02:37.200
so if you're starting on
those practice before

00:02:37.200 --> 00:02:40.780
we've gotten far enough in Markov
chains it's not too much of a problem.

00:02:40.780 --> 00:02:45.670
Because in general the problems
are not intended to be in

00:02:45.670 --> 00:02:49.430
order of the first problem is the first
thing we did in the course and so on,

00:02:49.430 --> 00:02:51.600
it's not really chronological like that.

00:02:51.600 --> 00:02:56.610
But it has been true every year that I
put Markov chains as the last problem and

00:02:56.610 --> 00:02:57.540
no where else.

00:02:57.540 --> 00:03:00.540
So if you haven't gotten far
enough on Markov's chains yet,

00:03:00.540 --> 00:03:02.560
you can just skip the last
problem of each year.

00:03:02.560 --> 00:03:05.110
Until we get a little
farther with Markov chains.

00:03:05.110 --> 00:03:07.210
Everything else we've
covered the material so

00:03:07.210 --> 00:03:10.250
the sooner you start
practicing the better.

00:03:12.020 --> 00:03:16.960
So you can assume that the style would
be similar to those past exams and

00:03:16.960 --> 00:03:21.670
now that will give you
a good sense of everything.

00:03:21.670 --> 00:03:27.380
So Markov chains, what is it?

00:03:27.380 --> 00:03:30.300
Well, they are invented or

00:03:30.300 --> 00:03:33.350
discovered depending on your
philosophical point of view.

00:03:33.350 --> 00:03:34.150
They were invented or

00:03:34.150 --> 00:03:38.760
discovered by Markov who we've already
heard of through, Markov's inequality.

00:03:38.760 --> 00:03:41.550
He introduced them just
over a hundred years ago.

00:03:41.550 --> 00:03:43.491
I think that's 1906.

00:03:43.491 --> 00:03:47.410
Now I'll tell you a little about
Markov's original reason for

00:03:47.410 --> 00:03:50.930
studying Markov chains, but

00:03:50.930 --> 00:03:56.850
the number of applications they found
since then has just been unbelievable and

00:03:56.850 --> 00:04:01.950
a lot of directions it's gone in that
Markov would not have anticipated.

00:04:04.210 --> 00:04:07.790
So Markov chain,
it's an example of a stochastic process.

00:04:07.790 --> 00:04:10.700
I'll tell you just very briefly
what a stochastic process is.

00:04:14.166 --> 00:04:22.870
It's one of the most important
examples of stochastic processes.

00:04:22.870 --> 00:04:25.220
A stochastic process, we've been dealing,

00:04:25.220 --> 00:04:28.595
originally we were dealing with
one random variable at a time, and

00:04:28.595 --> 00:04:32.272
then we're sometimes doing two, and
with the central limit theorem and

00:04:32.272 --> 00:04:36.035
law of large numbers, we were looking
at sequences of random variables.

00:04:36.035 --> 00:04:39.321
So sometimes we have sequences
of random variables, and

00:04:39.321 --> 00:04:43.780
we're studying the sample mean, and
let n go to infinity, stuff like that.

00:04:43.780 --> 00:04:46.650
But when we're considering this
sequence of random variables,

00:04:46.650 --> 00:04:49.030
mostly we've been assuming
that they're i.i.d.

00:04:49.030 --> 00:04:53.330
The central limit theorem and law of large
numbers, there are more general versions

00:04:53.330 --> 00:04:58.380
of those theorems, but we've been assuming
i.i.d which is a very strong assumption.

00:04:58.380 --> 00:05:01.070
Independent identically distributed.

00:05:01.070 --> 00:05:05.030
So Markov chain is an example
of a stochastic process.

00:05:05.030 --> 00:05:10.162
Which basically just means random
variables evolving over time.

00:05:15.570 --> 00:05:20.480
So I'll just say examples
of stochastic processes.

00:05:20.480 --> 00:05:21.930
So, what does that mean?

00:05:21.930 --> 00:05:24.580
I mean,
it doesn't literally have to be time but

00:05:24.580 --> 00:05:26.960
that's the most common interpretation.

00:05:26.960 --> 00:05:31.731
So we have this sequence
of random variables X0,

00:05:31.731 --> 00:05:36.310
X1, X2, blah, blah, blah, blah, blah.

00:05:36.310 --> 00:05:39.710
We've been looking mostly at
the case where these are i.i.d.

00:05:39.710 --> 00:05:43.850
So we can think of the subscript as time,
but if they're i.i.d.

00:05:43.850 --> 00:05:46.680
Then is just like saying
it's resetting every

00:05:46.680 --> 00:05:49.940
time to a fresh independent
random variable.

00:05:49.940 --> 00:05:54.404
So from a Stochastic process point of view
more interesting is when it's kind of

00:05:54.404 --> 00:05:58.290
evolved in some way where there
is some kind of dependence.

00:06:00.170 --> 00:06:04.410
But if we allow these to be
completely dependent in some

00:06:04.410 --> 00:06:09.360
arbitrarily complicated way,
that can be extremely complicated.

00:06:09.360 --> 00:06:14.010
You have this and they can all relate to
each other in some very complicated way.

00:06:14.010 --> 00:06:17.570
So Markov chains are kind of a compromise.

00:06:17.570 --> 00:06:21.970
It's basically once going
one step beyond i.i.d.

00:06:21.970 --> 00:06:23.860
So they're gonna become dependent.

00:06:23.860 --> 00:06:27.510
But it's in a very special way
that I'm gonna tell you about.

00:06:27.510 --> 00:06:30.800
That has very nice properties.

00:06:30.800 --> 00:06:35.772
So, we're gonna think of

00:06:35.772 --> 00:06:40.970
Xn as the state of a system.

00:06:42.360 --> 00:06:44.912
So, it can be interpreted very generally.

00:06:44.912 --> 00:06:49.520
You have any kind of system,
we'll see examples,

00:06:49.520 --> 00:06:54.390
that you're imagining
some kind of particle or

00:06:54.390 --> 00:06:57.480
whatever that's wandering around
randomly from state to state.

00:06:57.480 --> 00:06:59.750
It's jumping from state to state, and

00:06:59.750 --> 00:07:03.970
that's at time n, which we're
assuming is discrete right now.

00:07:05.190 --> 00:07:09.314
That is n is a non-negative integer.

00:07:09.314 --> 00:07:14.837
So we can consider stochastic processes
in continuous-time or continuous-space.

00:07:14.837 --> 00:07:17.712
Continuous-time or discrete-time,

00:07:17.712 --> 00:07:23.050
continuous-space or discreet-space, so
there's basically four possibilities.

00:07:23.050 --> 00:07:26.850
That is this index here,
we just do X0, X1, X2.

00:07:26.850 --> 00:07:30.380
If we want a continuous time,
we could look at Xt,

00:07:30.380 --> 00:07:34.240
where t is time,
which could be continuous.

00:07:35.250 --> 00:07:42.700
Then we'd have a collection
indexed by a continuous variable.

00:07:42.700 --> 00:07:45.950
Here we're just indexing it discretely,
so this is the discrete time.

00:07:45.950 --> 00:07:50.730
So in STAT 110, we're only gonna
look at discrete time processes.

00:07:50.730 --> 00:07:54.190
If you wanna go further into stochastic
processes, I highly recommend STAT 171,

00:07:54.190 --> 00:07:57.700
which is an entire course
on stochastic processes.

00:07:57.700 --> 00:08:01.330
We're gonna do Markov
chains in discrete time, so

00:08:01.330 --> 00:08:05.147
this is the discrete time And
we're solving discrete space.

00:08:05.147 --> 00:08:10.216
Continuous space would mean that
each of these Xs is allowed to

00:08:10.216 --> 00:08:16.760
take values like let's say any real number
or values in some continuous space.

00:08:16.760 --> 00:08:21.941
But for our purposes we're gonna
assume that each x j takes

00:08:21.941 --> 00:08:26.912
values in same discreet space and
actually we're gonna

00:08:26.912 --> 00:08:31.895
assume that it's a finite
space to make things easier.

00:08:31.895 --> 00:08:37.207
So we're gonna be looking at Markov chains
where we have finitely many states and

00:08:37.207 --> 00:08:39.795
each of these Xs is one of those states.

00:08:39.795 --> 00:08:44.895
And we just have this process that's
bouncing around randomly from

00:08:44.895 --> 00:08:50.465
state to state, and then we wanna
describe what's the Markov property.

00:08:50.465 --> 00:08:56.819
Okay, so here's the mark off property,
what is Markovian mean?

00:08:56.819 --> 00:09:01.375
And then I'll tell you a little
bit about why did Markov

00:09:01.375 --> 00:09:05.051
introduce this and what are some examples.

00:09:05.051 --> 00:09:11.610
Markov property is a certain
conditional probability statement.

00:09:11.610 --> 00:09:15.673
It says if we look at the next,

00:09:15.673 --> 00:09:23.324
our interpretation is think
of Xn as the current state.

00:09:23.324 --> 00:09:26.661
I mean I don't know how to
define the word now but

00:09:26.661 --> 00:09:29.680
we have some intuitive conception of now.

00:09:29.680 --> 00:09:33.090
Now is now, that's the present,
and then after now is the future.

00:09:33.090 --> 00:09:35.970
And before now is the past, right?

00:09:35.970 --> 00:09:38.658
So a lot of Markov chain stuff
you can understand intuitively

00:09:38.658 --> 00:09:40.617
if you think about past present and
future.

00:09:40.617 --> 00:09:44.930
So let's think Xn is right now and
we wanna predict the future.

00:09:44.930 --> 00:09:48.158
We wanna know what's gonna happen at time
N plus one cuz we're letting time be

00:09:48.158 --> 00:09:48.662
discrete.

00:09:48.662 --> 00:09:55.000
So one step in the future and we have
Xn is today, Xn plus one is tomorrow.

00:09:55.000 --> 00:09:59.596
Okay we want to know what's
the probability that Xn plus one equals J.

00:09:59.596 --> 00:10:03.847
Or J is just some state, what we're
usually gonna assume that the states

00:10:03.847 --> 00:10:07.261
are numbered from 1 to capital
M just as a convention, but

00:10:07.261 --> 00:10:10.320
there's no rule saying
you have to do that.

00:10:10.320 --> 00:10:13.226
We're assuming there's
finitely many states.

00:10:13.226 --> 00:10:17.579
So we're saying what's the probability
that the state of the system tomorrow is

00:10:17.579 --> 00:10:20.600
state J, given the entire
past history of everything?

00:10:21.700 --> 00:10:24.601
So we're given,
well what's the state today?

00:10:24.601 --> 00:10:28.585
Xn = I, and what was the state yesterday?

00:10:28.585 --> 00:10:30.204
Yesterday was at xn minus one.

00:10:30.204 --> 00:10:35.089
It was at xn- 1, and
maybe that's i n- 1, and

00:10:35.089 --> 00:10:40.947
xn- 2 is in- 2,
all the way back to the very beginning.

00:10:40.947 --> 00:10:44.515
And x zero, let's say it's i zero.

00:10:44.515 --> 00:10:51.118
So this is saying conditional, what's
the probably of a certain state tomorrow,

00:10:51.118 --> 00:10:56.220
given the entire past
history of the system, right?

00:10:56.220 --> 00:10:57.660
That looks very complicated, right?

00:10:57.660 --> 00:11:01.628
It took a lot of time just to write all
these things and all these dot dot dots,

00:11:01.628 --> 00:11:03.560
and this is a big complicated thing.

00:11:04.740 --> 00:11:07.920
In a general stochastic process,
maybe you can't simplify this very much.

00:11:07.920 --> 00:11:10.120
But this is too complicated.

00:11:10.120 --> 00:11:14.665
The Markov structure means that only

00:11:14.665 --> 00:11:19.510
the most recent information matters.

00:11:19.510 --> 00:11:21.330
We can get rid of everything else.

00:11:21.330 --> 00:11:26.061
So this just becomes X
n+1 = j given Xn = i,

00:11:26.061 --> 00:11:29.310
you can get rid of all the rest.

00:11:29.310 --> 00:11:32.959
So all these concepts that we've been
doing the entire semester come back here

00:11:32.959 --> 00:11:36.400
at the difference between independence and
conditional independence.

00:11:37.850 --> 00:11:43.083
We're not saying that the past
is independent of the future,

00:11:43.083 --> 00:11:49.824
but we are saying if the properties holds
and where's that says that the past and

00:11:49.824 --> 00:11:54.873
future are conditionally
independent given the present?

00:11:54.873 --> 00:11:58.486
So that's kind of a handy
intuitive way to remember it, so

00:11:58.486 --> 00:12:02.710
past and future are conditionally
independent given the present.

00:12:07.175 --> 00:12:10.170
Okay, that's exactly
what this statement says.

00:12:10.170 --> 00:12:13.988
Given the present, that is,
if we know the current value, Xn,

00:12:13.988 --> 00:12:18.234
if we know that, then anything that's
further back is just obsolete,

00:12:18.234 --> 00:12:21.310
outdated information which
we can just get rid of.

00:12:21.310 --> 00:12:26.040
So I just crossed out everything
further back in time, right?

00:12:26.040 --> 00:12:28.570
Only the most recent one matters.

00:12:28.570 --> 00:12:33.252
Conditionally if we didn't get that
doesn't mean that if we had xn + 1 given

00:12:33.252 --> 00:12:37.861
xn- 1 ,then we are not given xn,
it doesn't mean we can get rid of xn- 1,

00:12:37.861 --> 00:12:38.600
right?

00:12:38.600 --> 00:12:39.525
It's conditional independence.

00:12:39.525 --> 00:12:43.360
So that's the Markov assumption.

00:12:43.360 --> 00:12:46.020
Whether that assumption is good or not for

00:12:46.020 --> 00:12:49.750
a real example completely
depends on the problem, okay?

00:12:49.750 --> 00:12:51.600
But that's the Markov property.

00:12:52.850 --> 00:12:55.811
And in particular for our purposes,

00:12:55.811 --> 00:13:00.310
let's actually assume that
this doesn't depend on n.

00:13:00.310 --> 00:13:06.110
So we'll usually call this thing Ii,
or you call it whatever you want.

00:13:06.110 --> 00:13:08.843
Let's call it a transition probability.

00:13:08.843 --> 00:13:14.128
So, sometimes this is called
homogeneous Markov chain.

00:13:14.128 --> 00:13:19.757
But a lot of times people will not bother
to say the word homogeneous, so you have

00:13:19.757 --> 00:13:25.870
to be a little careful from the context,
is it assumed to be homogeneous or not?

00:13:25.870 --> 00:13:30.769
Homogeneous means that this
doesn't depend on time, right?

00:13:30.769 --> 00:13:34.816
So this is saying we're looking at
the case where the probability,

00:13:34.816 --> 00:13:36.662
if the system is at state i, and

00:13:36.662 --> 00:13:40.640
then we wanna know what's
the probability at the next time point?

00:13:40.640 --> 00:13:44.324
It's at state j.
If that's always qij, if it doesn't change

00:13:44.324 --> 00:13:49.950
with time, then we would say that
that's homogeneous or time homogeneous.

00:13:49.950 --> 00:13:54.864
And for
our purposes we're just gonna be studying

00:13:54.864 --> 00:14:01.066
homogeneous Markov chains, so
these qujs don't depend on N,

00:14:01.066 --> 00:14:05.290
cuz that's a nicer case to look at, okay?

00:14:05.290 --> 00:14:12.200
So just to have a mental picture in mind,
I just made up a simple little example.

00:14:16.340 --> 00:14:20.440
It really helps to have pictures in mind,
so here's one Markov chain.

00:14:20.440 --> 00:14:23.140
You can make up your own, but
here's one that I just made up.

00:14:23.140 --> 00:14:28.050
So there are four states
we can represent as ovals.

00:14:28.050 --> 00:14:33.490
Let's number the states 1 through 4, okay?

00:14:33.490 --> 00:14:37.698
And then to specify the Markov chain
we just need to specify transition

00:14:37.698 --> 00:14:42.619
probabilities, that is what's the
probability going from state 1 to state 2,

00:14:42.619 --> 00:14:44.550
all the different states.

00:14:44.550 --> 00:14:48.282
So it may not be possible to go from
any state to any state in one step,

00:14:48.282 --> 00:14:50.323
in fact usually it's not possible.

00:14:50.323 --> 00:14:53.333
So for example,
that's just an example I made up,

00:14:53.333 --> 00:14:55.510
let's say the system is at state 1.

00:14:55.510 --> 00:14:58.480
It has probably one
third of staying there.

00:14:58.480 --> 00:15:02.380
And it has probably two
thirds of going to state 2.

00:15:02.380 --> 00:15:05.694
I'm just drawing arrows and I'm putting
probabilities on the arrows that

00:15:05.694 --> 00:15:08.928
reflect the probabilities of
those different transitions.

00:15:08.928 --> 00:15:13.805
From state 2, it's equally
likely to go back to state 1 or

00:15:13.805 --> 00:15:19.964
to go to state 2, I guess I'm putting
the probabilities above the arrow.

00:15:19.964 --> 00:15:27.165
So from state 2 either goes here or
here, equally likely from state 3.

00:15:27.165 --> 00:15:34.250
From state 3, it either goes back to
state 4 with probability one quarter.

00:15:34.250 --> 00:15:38.951
Or it goes to state, back to state one,

00:15:38.951 --> 00:15:42.228
with probably one half, or

00:15:42.228 --> 00:15:47.790
it stays there with
probability one quarter.

00:15:47.790 --> 00:15:49.920
And then, let's say from state 3,

00:15:49.920 --> 00:15:54.470
just to make life a little bit easier
from state 3, it always goes to state 4.

00:15:54.470 --> 00:15:56.070
So I'll just put a 1 there.

00:15:56.070 --> 00:15:57.595
You make your own example.

00:15:57.595 --> 00:16:01.809
Anyway, it just helps to have
a picture like this in mind,

00:16:01.809 --> 00:16:06.625
and you can have Markov chain that
has infinitely many states, and

00:16:06.625 --> 00:16:09.990
then you You can actually
draw infinitely many states.

00:16:09.990 --> 00:16:13.550
But we're only gonna be looking at the
case where there are finitely many states.

00:16:13.550 --> 00:16:15.050
So you can always, for our purposes,

00:16:15.050 --> 00:16:18.710
you can always imagine a picture like
this where we have some number of states.

00:16:20.940 --> 00:16:25.180
And then you have a particle randomly
bouncing around between states,

00:16:25.180 --> 00:16:28.239
following the arrows, and the arrows
may have different probabilities.

00:16:29.410 --> 00:16:31.200
Okay, that's the mental picture.

00:16:31.200 --> 00:16:36.250
And then to explain in that picture,
what does these property say?

00:16:36.250 --> 00:16:42.950
It says that you are wandering around
from state to state, like that.

00:16:42.950 --> 00:16:48.150
And then what this says is,
if I wanna know the future.

00:16:48.150 --> 00:16:49.270
Or predict the future.

00:16:50.490 --> 00:16:52.820
All I care about is the current state.

00:16:52.820 --> 00:16:58.030
I don't care about the whole history of
all of the long wanderings throughout time

00:16:58.030 --> 00:16:58.990
to get there.

00:16:58.990 --> 00:17:00.340
It doesn't matter how you got there.

00:17:00.340 --> 00:17:01.470
It only matters where you are.

00:17:02.650 --> 00:17:05.250
That's what the Markov
property says intuitively.

00:17:08.080 --> 00:17:11.500
So we have these transition
probabilities q ij,

00:17:11.500 --> 00:17:15.910
and we can write that as a matrix,
that's called the transition matrix.

00:17:19.560 --> 00:17:22.480
So we can always just draw a picture
like this, but sometimes it's easier to

00:17:22.480 --> 00:17:25.900
work with a matrix, rather than
having to draw a picture every time.

00:17:25.900 --> 00:17:31.150
So we can encode the same information as
this picture just written in matrix form.

00:17:32.280 --> 00:17:36.930
The transition matrix,
usually we call Q in this course, but

00:17:36.930 --> 00:17:38.420
you don't have to call it that.

00:17:38.420 --> 00:17:43.330
That's just the matrix of all the qijs,
all the transition probabilities.

00:17:43.330 --> 00:17:48.520
So for this example it's
gonna be a 4 by 4 matrix, and

00:17:48.520 --> 00:17:53.240
then I'm just gonna write, what's
the probability of going from 1 to 1?

00:17:53.240 --> 00:17:57.510
That's one-third,
because there's this loop there,

00:17:57.510 --> 00:17:58.680
that's has a probability of one-third.

00:17:58.680 --> 00:18:03.283
Probability of going from 1 to
2 is two-thirds, from 1 to 3 or

00:18:03.283 --> 00:18:06.670
1 to 4 is 0,
we can't go there in one step.

00:18:06.670 --> 00:18:07.950
We can go there in more than one step.

00:18:09.210 --> 00:18:12.770
Then from state 2,
it either goes back to state 1 or

00:18:12.770 --> 00:18:16.390
it goes to state 3 with
equal probabilities.

00:18:16.390 --> 00:18:20.070
So this will be,1/2 0 1/2 0.

00:18:20.070 --> 00:18:26.800
From state 3 it always goes to state 4,
so it's gonna be 0 0 0 1.

00:18:26.800 --> 00:18:29.768
I'm just writing down
this q ijs as a matrix.

00:18:29.768 --> 00:18:36.254
And then from state 4 it either goes
back to state 1 that's probably 1/2 or

00:18:36.254 --> 00:18:42.267
it goes to state 3 that's 1/4 or
it stays at state 4 that's 1/4.

00:18:42.267 --> 00:18:43.490
So that would be the transition matrix.

00:18:45.230 --> 00:18:49.019
Notice that every row sums to one.

00:18:51.544 --> 00:18:56.437
And, again, it's just a convention,
some people would use the transpose

00:18:56.437 --> 00:19:01.850
of this matrix instead, and then
the columns sum to one, just a convention.

00:19:01.850 --> 00:19:06.890
We're going to take the convention
that we write it this way, the i,

00:19:06.890 --> 00:19:11.810
j entry is the probability of
jumping from i to j in one step.

00:19:11.810 --> 00:19:14.950
So therefore each row sums to 1.

00:19:17.260 --> 00:19:21.930
Columns may or may not sum to 1,
but the rows sum to 1.

00:19:21.930 --> 00:19:23.430
So we wanna go the other way around and

00:19:23.430 --> 00:19:27.950
say, what's a valid Markov
chain transition matrix?

00:19:27.950 --> 00:19:32.050
Well, we can just write down
a blank square matrix, fill in

00:19:32.050 --> 00:19:36.010
any numbers you want as long as they're
non-negative and each row sums up to one.

00:19:36.010 --> 00:19:40.930
And then you can easily see how you would
then convert that to a picture like this.

00:19:40.930 --> 00:19:42.320
Just drawing errors with probability.

00:19:43.820 --> 00:19:47.150
So, that's what a transition matrix is.

00:19:47.150 --> 00:19:51.500
By the way, the two most important
concepts other than the basic

00:19:51.500 --> 00:19:56.060
definition of Markov property, two most
important concepts we're gonna need,

00:19:56.060 --> 00:19:57.370
one is transition matrix.

00:19:57.370 --> 00:20:01.900
Now you know what a transition matrix is,
the other one's stationary distribution,

00:20:01.900 --> 00:20:02.830
which we'll talk about later.

00:20:05.520 --> 00:20:08.070
So, that's the Markov property.

00:20:09.460 --> 00:20:18.370
So to tell you a little bit
about the history, well, so,

00:20:18.370 --> 00:20:24.190
how it's used now, it's being used now,
in two sort of different ways.

00:20:25.360 --> 00:20:30.270
One is as a model that, might be used

00:20:30.270 --> 00:20:35.140
in various problems in the social
sciences, physical sciences, and biology,

00:20:35.140 --> 00:20:41.090
where you actually, literally believe
that you have a Markov chain.

00:20:41.090 --> 00:20:46.030
Or using it as an approximation for
some system evolving over time.

00:20:46.030 --> 00:20:48.200
That's very, very useful.

00:20:48.200 --> 00:20:51.420
But that sort of limited in the sense that

00:20:51.420 --> 00:20:54.830
it seems like a pretty strong
conditional independence assumption.

00:20:54.830 --> 00:20:59.390
Where you can forget the entire past
as long as you have the present.

00:20:59.390 --> 00:21:04.120
So there have been thousands of pages
of debates about whether if you take

00:21:04.120 --> 00:21:09.970
the stock price over time,
is that Markovian or not?

00:21:09.970 --> 00:21:14.110
Or is the weather Markov,
things like that.

00:21:14.110 --> 00:21:20.548
So those become empirical questions,
which you can study.

00:21:20.548 --> 00:21:25.037
One thing to point out though, is it's not
as limiting as it looks in the sense of

00:21:25.037 --> 00:21:28.454
that, this is sometimes called
a First Order Markov Chain,

00:21:28.454 --> 00:21:30.700
where it only goes one step back.

00:21:30.700 --> 00:21:33.270
You can generalize this easily to the case

00:21:34.370 --> 00:21:39.270
where the conditional independence
is that it depends on Xn and Xn- 1.

00:21:39.270 --> 00:21:42.690
And it turns out that to understand
how that kind of thing works,

00:21:42.690 --> 00:21:45.830
the right starting point is
to be looking at this anyway.

00:21:45.830 --> 00:21:46.840
So you can extend this and

00:21:46.840 --> 00:21:51.900
consider, Markov chains where it goes,
ten steps back or think things like that.

00:21:51.900 --> 00:21:56.820
Anyway that's still still kind of
an empirical question, is that useful for

00:21:56.820 --> 00:21:58.010
a real system or

00:21:58.010 --> 00:22:02.700
not or is that too strong of an assumption
about this conditional independence?

00:22:02.700 --> 00:22:06.996
But in more another way
that this is used for

00:22:06.996 --> 00:22:11.300
Markov chain Monte Carlo which essentially

00:22:11.300 --> 00:22:16.920
created revolution in scientific
computing and is being used everywhere.

00:22:16.920 --> 00:22:21.750
And I like to challenge people
finding an example of some major

00:22:21.750 --> 00:22:27.830
fields of study where Markov chain
Monte Carlo has never been applied,

00:22:27.830 --> 00:22:32.710
and no one has successfully
met that challenge yet.

00:22:32.710 --> 00:22:35.617
So say you're interested in French poetry,

00:22:35.617 --> 00:22:39.679
you can find Markov chains being
applied to different poetry.

00:22:39.679 --> 00:22:41.530
But why is that?

00:22:41.530 --> 00:22:46.620
Do you really want to think of French
poetry as actually following this model?

00:22:46.620 --> 00:22:53.170
No, but, the idea of Markov Chain
Monte Carlo is that you don't have

00:22:53.170 --> 00:22:58.610
to worry about whether the actual process
you're observing follows a Markov Chain.

00:22:58.610 --> 00:23:03.190
Markov Chain Monte Carlo means that
you synthetically construct your own

00:23:03.190 --> 00:23:03.910
Markov Chain.

00:23:05.260 --> 00:23:08.740
You then can't argue is it Markov or
not, because you built your own chain.

00:23:09.900 --> 00:23:12.250
Why would you want to
build your own chain?

00:23:12.250 --> 00:23:16.680
Well, the idea, which is a very beautiful,
one of the most useful and

00:23:16.680 --> 00:23:19.950
beautiful ideas of the 20th century,
in my opinion.

00:23:19.950 --> 00:23:23.070
Is that you can construct
your own Markov chain

00:23:23.070 --> 00:23:26.020
that will converge to a distribution
that you're interested in.

00:23:26.020 --> 00:23:30.280
Cuz suppose you're trying to
simulate some complicated system and

00:23:30.280 --> 00:23:34.320
the computations are too hard to do
everything analytically and explicitly.

00:23:35.360 --> 00:23:40.710
There are some extremely clever
ways to construct a Markov

00:23:40.710 --> 00:23:45.220
chain synthetically, that will converge
to the thing you're interested in.

00:23:45.220 --> 00:23:49.690
And so then you just program your Markov
chain on the computer, so this is a very

00:23:49.690 --> 00:23:53.930
recent thing, cuz we need to have fast
computers in order for this to be useful.

00:23:53.930 --> 00:23:58.080
It's a recent idea, you program
your Markov chain on the computer,

00:23:58.080 --> 00:24:01.610
run the chain for a long time and

00:24:01.610 --> 00:24:06.500
then use those result to study the
distribution that you're interested in.

00:24:06.500 --> 00:24:11.630
So that is the idea I'm just
gonna write both acronym, MCMC.

00:24:11.630 --> 00:24:16.490
We don't really have time to
talk more about MCMC than that.

00:24:16.490 --> 00:24:21.660
But Markov chain Monte Carlo is
just getting more and more popular.

00:24:21.660 --> 00:24:26.667
Widely used every year as a way of doing
extremely complicated simulations, and

00:24:26.667 --> 00:24:31.762
computations that would have been
completely impossible, like 50 years ago.

00:24:33.341 --> 00:24:36.130
Or even 30 years ago.

00:24:36.130 --> 00:24:39.115
Okay, that's Markov chain Monte Carlo.

00:24:39.115 --> 00:24:43.255
So Markov chain Monte Carlo is when you
build your own Markov chain, right?

00:24:43.255 --> 00:24:47.325
And then the other application is just
that you have this natural system that

00:24:47.325 --> 00:24:50.085
you choose to model it,
using the Markov chain.

00:24:50.085 --> 00:24:53.375
So those are two important, but
different uses of Markov chains.

00:24:54.765 --> 00:24:57.225
Okay, so both of those ideas are very,

00:24:57.225 --> 00:25:01.670
very different from what Markov
himself originally had in mind.

00:25:01.670 --> 00:25:06.590
When he introduced Markov chains,
it was actually to help settle a religious

00:25:06.590 --> 00:25:10.000
debate over the existence of free will.

00:25:10.000 --> 00:25:13.850
Kind of a religious philosophical thing.

00:25:13.850 --> 00:25:17.660
So at the time, this was the early 1900s,

00:25:17.660 --> 00:25:21.080
the law of large numbers
had recently been proven.

00:25:21.080 --> 00:25:23.954
Like here, we recently proved
the law of large numbers.

00:25:23.954 --> 00:25:28.560
Okay, and I told you the story
of the athlete who was very

00:25:28.560 --> 00:25:32.310
upset about the law of large numbers
because he said in the long run,

00:25:32.310 --> 00:25:35.490
he'd always be average, and
so he couldn't improve.

00:25:35.490 --> 00:25:37.280
And so he hated statistics.

00:25:38.490 --> 00:25:41.233
Which is kind of like ridiculous.

00:25:41.233 --> 00:25:45.698
But that was actually very
similar kind of a concern that

00:25:45.698 --> 00:25:50.710
people had when the law of
large numbers was first proven.

00:25:50.710 --> 00:25:56.040
Which was that some of
the philosophers were upset that

00:25:56.040 --> 00:26:01.350
the law of large numbers might
prevent us from having free will.

00:26:01.350 --> 00:26:06.160
Because it says in the long run,
converge to the mean.

00:26:06.160 --> 00:26:10.326
Where's the scope for
having free will then, right?

00:26:10.326 --> 00:26:13.160
I'm saying it will converge to this,
right?

00:26:13.160 --> 00:26:15.850
Where's free will, okay?

00:26:15.850 --> 00:26:22.025
So one of Markov's rivals tried
to rescue free will by saying,

00:26:22.025 --> 00:26:26.233
well the law of large numbers assumes IID.

00:26:26.233 --> 00:26:31.330
And human behavior is not IID,
therefore we're okay, right?

00:26:32.420 --> 00:26:36.180
Okay, but you can see that that's not
a very convincing argument because

00:26:36.180 --> 00:26:40.330
we proved that if the random variables are
IID, then the law of large numbers holds.

00:26:40.330 --> 00:26:42.060
But we didn't show that if it's not IID,

00:26:42.060 --> 00:26:45.730
then the law of large numbers doesn't
hold and there are more general versions.

00:26:45.730 --> 00:26:50.695
So what Markov wanted to do was to find
a case that's find a process that's

00:26:50.695 --> 00:26:53.907
one step in complexity beyond the IID,
right?

00:26:53.907 --> 00:26:55.930
That's what this is, right?

00:26:55.930 --> 00:26:56.533
The IID case,

00:26:56.533 --> 00:27:00.020
would say, you can just completely forget
everything you're conditioning on.

00:27:00.020 --> 00:27:03.999
And he wanted to go one
step in complexity beyond

00:27:03.999 --> 00:27:07.700
IID where you go one step backward, right?

00:27:07.700 --> 00:27:12.360
So go one step forward in thinking by
going one step backward in conditioning.

00:27:12.360 --> 00:27:14.790
You're going one step beyond IID.

00:27:14.790 --> 00:27:18.300
And then he proved a version
of the law of large numbers

00:27:18.300 --> 00:27:19.980
that applies to Markov chains.

00:27:19.980 --> 00:27:23.900
And so then he said,
well look you don't actually need IID for

00:27:23.900 --> 00:27:29.210
the law of large numbers to hold, okay?

00:27:29.210 --> 00:27:32.483
And the chain that he
originally looked at,

00:27:32.483 --> 00:27:37.611
which might be the first Markov chain,
was he took a Russian novel and

00:27:37.611 --> 00:27:42.223
he empirically said,
was looking at consonants and vowels.

00:27:42.223 --> 00:27:43.269
And he said, okay.

00:27:43.269 --> 00:27:47.155
There are two states: vowel and
consonant, and

00:27:47.155 --> 00:27:51.340
a vowel is either followed by a vowel or
a consonant.

00:27:51.340 --> 00:27:54.220
A consonant is either followed
by a vowel or a consonant.

00:27:54.220 --> 00:27:58.039
And then we kind of empirically computed
the probability that vowel is followed

00:27:58.039 --> 00:28:00.605
by vowel, vowels followed by consonant,
and so on.

00:28:00.605 --> 00:28:02.970
And he was just studying
that simple chain.

00:28:02.970 --> 00:28:04.970
So just like this picture,
except even simpler.

00:28:04.970 --> 00:28:07.890
Only two states, and
I drew one with four states.

00:28:07.890 --> 00:28:09.730
But conceptually, it's the same thing.

00:28:09.730 --> 00:28:14.080
You're just bouncing around
between states, okay?

00:28:14.080 --> 00:28:17.130
So that's the whole
idea of a Markov chain.

00:28:17.130 --> 00:28:20.429
And if you keep these
simple pictures in mind,

00:28:20.429 --> 00:28:23.819
it will make things much,
much, much easier.

00:28:23.819 --> 00:28:30.533
Even for more, we could have a Markov
chain that has 10 to the 100 states,

00:28:30.533 --> 00:28:35.870
but you can still keep pictures
like this in mind, okay?

00:28:35.870 --> 00:28:38.170
So we've defined the transition matrix.

00:28:38.170 --> 00:28:40.820
It's just this matrix of
transition probabilities.

00:28:40.820 --> 00:28:45.700
But I wanna show you how do we actually
use the transition matrix to get

00:28:45.700 --> 00:28:49.589
higher order things,
like what's the probability.

00:28:49.589 --> 00:28:52.489
For example, we wanna answer the question,

00:28:52.489 --> 00:28:55.997
this is the probability of
going from i to j in one step,

00:28:55.997 --> 00:29:00.760
what's the probability of going from
i to j in two steps, or ten steps?

00:29:00.760 --> 00:29:02.060
Things like that.

00:29:02.060 --> 00:29:05.910
Those questions can be answered in
terms of the transition matrix.

00:29:07.350 --> 00:29:10.680
Okay, so let's do that now.

00:29:12.670 --> 00:29:16.220
So suppose that we start the chain.

00:29:17.630 --> 00:29:21.235
Suppose at time n which we're
considering as the present

00:29:25.341 --> 00:29:30.971
Xn, the chain, which is Xn at time n,
has distribution,

00:29:35.774 --> 00:29:40.358
Which I'm thinking of as s,
which is a row vector.

00:29:43.805 --> 00:29:48.160
Which we're gonna think of as, in other
words, we can think of it as a 1xM matrix.

00:29:50.810 --> 00:29:55.962
And all I mean by this is, we're taking
the PMF, but since we're assuming.

00:29:55.962 --> 00:30:00.459
I'm assuming that there are M states,
so here M = 4, in this example.

00:30:00.459 --> 00:30:04.520
M is the number of ovals in this picture.

00:30:04.520 --> 00:30:07.681
So we're assuming M states,
and we're saying, okay.

00:30:07.681 --> 00:30:11.692
The distribution,
I just mean the PMF cuz it's discrete, but

00:30:11.692 --> 00:30:16.850
since there only is finitely many values,
you may as well just list them all out.

00:30:17.860 --> 00:30:26.440
So this is, this thing is the PMF kind of
just listed as a vector listed out, right.

00:30:26.440 --> 00:30:30.472
So it's the probability of being in state
one, probability of being in state two,

00:30:30.472 --> 00:30:31.000
and so on.

00:30:31.000 --> 00:30:33.780
Just list out the probabilities,okay?

00:30:33.780 --> 00:30:37.915
So in particular, we know that s is
a vector and we're writing it as a row.

00:30:37.915 --> 00:30:43.140
And the entries are non-negative and
add up to one.

00:30:43.140 --> 00:30:48.740
Okay, so let's assume that that's
how things stand at time n,

00:30:48.740 --> 00:30:53.380
and then we wanna know what
happens at time n plus one.

00:30:54.710 --> 00:31:01.245
So we wanna know what's
the P (Xn + 1 = j).

00:31:01.245 --> 00:31:05.830
So we wanna know the PMF,
this is the PMF at time n.

00:31:05.830 --> 00:31:10.410
It's just we're writing it as
a vector because that's convenient.

00:31:10.410 --> 00:31:16.720
That's the PMF at time n, now we want the
PMF at time n + 1, one step in the future.

00:31:16.720 --> 00:31:20.700
Okay, well, to do that it's not hard
to guess that what we should do is

00:31:20.700 --> 00:31:23.280
condition on the value now, right?

00:31:23.280 --> 00:31:27.530
Condition on what we wish that we knew,
it would be convenient if we

00:31:27.530 --> 00:31:30.630
knew where the thing is now,
that would be very useful, right?

00:31:30.630 --> 00:31:35.168
So we're just gonna condition, so

00:31:35.168 --> 00:31:41.522
we're gonna sum over all
states i of of the P(Xn +

00:31:41.522 --> 00:31:47.740
1 = j given Xn = i) times P(Xn = i),
right?

00:31:48.830 --> 00:31:50.360
Just the law of total probability.

00:31:51.970 --> 00:31:53.400
But these are things we know.

00:31:54.660 --> 00:31:59.594
Just by definition, this is qij.

00:31:59.594 --> 00:32:05.330
That is the probability of going from
i to j, by definition that's qij.

00:32:05.330 --> 00:32:07.410
And this thing here.

00:32:07.410 --> 00:32:09.514
Is just SI, right?

00:32:09.514 --> 00:32:12.246
Because that's just
the probability of being at i and

00:32:12.246 --> 00:32:15.243
that's just the ith value in
this vector that's just SI.

00:32:15.243 --> 00:32:19.685
I'm gonna write it on the left,
because it looks a little bit nicer to me.

00:32:19.685 --> 00:32:25.824
So, That's the answer as a sum,

00:32:25.824 --> 00:32:30.920
but it's easier to think of
this in terms of a matrix.

00:32:30.920 --> 00:32:38.034
This is in fact the jth
entry of S times Q.

00:32:40.773 --> 00:32:45.685
Remember Q is an m x m matrix,
and S is a 1 x m matrix, so

00:32:45.685 --> 00:32:48.646
it's valid to multiply these.

00:32:48.646 --> 00:32:50.740
The inner numbers match up and

00:32:50.740 --> 00:32:54.938
then the dimensions of this
are the outer numbers, 1 by M.

00:32:54.938 --> 00:32:57.377
So S times Q is a 1 by M matrix.

00:32:57.377 --> 00:33:00.197
And you don't need a lot of
matrix stuff in this class but

00:33:00.197 --> 00:33:02.790
I'm assuming you can at
least multiply matrices.

00:33:02.790 --> 00:33:03.913
How do you multiply matrices?

00:33:03.913 --> 00:33:08.493
Well, you take a row of this and
do a dot product with a column of this.

00:33:08.493 --> 00:33:12.508
And when you do that,
it's exactly the sum, right,

00:33:12.508 --> 00:33:17.700
you're just going row dot column and
you're adding this up, okay?

00:33:17.700 --> 00:33:22.530
So the interpretation of
this sum is just it's just

00:33:22.530 --> 00:33:27.540
the entry of this vector
times this matrix.

00:33:28.760 --> 00:33:35.599
Therefore, S times Q is the distribution

00:33:35.599 --> 00:33:41.008
PMF written as a vector at time n + 1.

00:33:46.276 --> 00:33:48.231
Okay?

00:33:48.231 --> 00:33:51.130
So, that's now playing the role of S.

00:33:51.130 --> 00:33:53.023
So, by the same argument,

00:33:59.380 --> 00:34:04.889
If we do SQ squared,
that's the distribution at time n + 1.

00:34:09.220 --> 00:34:14.330
N + 2 sorry, two steps into the future,

00:34:14.330 --> 00:34:21.058
and SQ cubed is distribution
at time n + 3, and so on.

00:34:21.058 --> 00:34:23.869
Because it's just the same thing again,
right?

00:34:23.869 --> 00:34:30.551
Because now S times Q is now
playing the role of S, right?

00:34:30.551 --> 00:34:34.681
And what this calculation
says is to go one

00:34:34.681 --> 00:34:39.220
step to the future multiply
on the right by Q.

00:34:39.220 --> 00:34:43.680
So I multiply SQ by Q we get
SQ squared multiply this by Q

00:34:45.040 --> 00:34:48.410
you get SQ cubed, and so on, okay?

00:34:48.410 --> 00:34:53.530
So what that says is that our powers,

00:34:53.530 --> 00:35:01.360
if we multiply the initial vector
of probabilities by Q to a power,

00:35:01.360 --> 00:35:05.590
then that gives us the distribution
that number of steps in the future.

00:35:08.710 --> 00:35:10.275
So this is saying that by,

00:35:10.275 --> 00:35:14.626
of course this is gonna be pretty
tedious to do by hand for most problems.

00:35:14.626 --> 00:35:19.565
But using a calculator or
computer that can multiply matrices,

00:35:19.565 --> 00:35:24.870
then it just say, take powers of Q
you don't like think that hard each

00:35:24.870 --> 00:35:29.910
time you just use powers of Q and
you know how this is gonna evolve.

00:35:30.990 --> 00:35:32.715
Okay, so similarly,

00:35:37.219 --> 00:35:45.184
By definition Q gives the one step,

00:35:45.184 --> 00:35:49.952
Probabilities, so
the probability that Xn + 1 = j,

00:35:49.952 --> 00:35:54.112
given Xn = i,
by definition that's just qij,

00:35:56.145 --> 00:36:01.824
Okay, but
suppose we wanted the two step thing.

00:36:01.824 --> 00:36:08.400
Xn + 2 = j given Xn = i, okay.

00:36:08.400 --> 00:36:11.930
This is a similar calculation
to what we just did.

00:36:11.930 --> 00:36:14.160
This is the transition probabilities.

00:36:14.160 --> 00:36:17.568
This is saying go one step ahead in time.

00:36:17.568 --> 00:36:20.719
And now this says what happens
if we only know Xn but

00:36:20.719 --> 00:36:22.980
we want to predict two steps ahead?

00:36:22.980 --> 00:36:25.879
Well it's not hard to figure
what to condition on, right.

00:36:25.879 --> 00:36:30.775
The missing link here is Xn +
1 that's what's missing so,

00:36:30.775 --> 00:36:34.052
clearly we should condition on Xn + 1.

00:36:34.052 --> 00:36:37.133
So let's say we condition on Xn + 1.

00:36:37.133 --> 00:36:43.451
So this is P(Xn + 2 = j given Xn + 1 =,

00:36:43.451 --> 00:36:48.323
let's say k, Xn = i, I'm just

00:36:48.323 --> 00:36:53.573
conditioning on Xn + 1, right?

00:36:53.573 --> 00:36:58.312
Cuz that's the missing
intermediary that we want.

00:36:58.312 --> 00:37:05.054
Times the P(Xn + 1 = k given Xn = i).

00:37:05.054 --> 00:37:08.527
That's just loyalty of probability,

00:37:08.527 --> 00:37:13.480
the conditional version
everything is given, Xn = i.

00:37:13.480 --> 00:37:16.787
Let's simplify that into something
that's more interpretable.

00:37:16.787 --> 00:37:20.416
First of all,
the Markov property says that,

00:37:20.416 --> 00:37:26.293
if we know the value at time n + 1 and
the value at time n, this is obsolete.

00:37:26.293 --> 00:37:30.595
That's irrelevant now,
cuz of this conditional independence,

00:37:30.595 --> 00:37:33.682
we only want the most recent value,
all right?

00:37:33.682 --> 00:37:36.110
So we can get rid of that.

00:37:36.110 --> 00:37:43.860
So by definition, Now we're just saying
we're going in one step from k to j.

00:37:43.860 --> 00:37:47.225
By definition, that's just
the transition probability qkj.

00:37:51.240 --> 00:37:55.471
And then this one is also one step, right?

00:37:55.471 --> 00:38:01.050
This is saying go from i to k in one step.

00:38:01.050 --> 00:38:05.814
And again I'm gonna write this on
the other side just cuz it looks nicer

00:38:05.814 --> 00:38:06.637
qik, qkj.

00:38:06.637 --> 00:38:10.425
I just swapped the order of those
two terms to make it look nicer.

00:38:12.275 --> 00:38:16.500
We've expressed it in terms
of one-step things, okay.

00:38:16.500 --> 00:38:20.322
And now, something that looks like this,
that's of this form,

00:38:20.322 --> 00:38:24.503
it has a nice interpretation in terms
of matrix multiplication again.

00:38:24.503 --> 00:38:31.780
This is just the ij entry,
row i, column j of q squared.

00:38:34.469 --> 00:38:36.440
Because well, why is that true?

00:38:36.440 --> 00:38:38.424
Well, just think of,
how do you do q squared?

00:38:38.424 --> 00:38:39.280
You do q times q.

00:38:39.280 --> 00:38:46.556
So you take a row of q and .product with
a column of q, and you'll get this, okay?

00:38:46.556 --> 00:38:52.239
So similarly same idea,
that we wanna know what's the probability,

00:38:52.239 --> 00:38:55.143
just repeating the same argument.

00:38:55.143 --> 00:39:01.383
We wanna know like,
what's the probability that X,

00:39:01.383 --> 00:39:04.710
let's say, I don't know,

00:39:04.710 --> 00:39:10.130
m steps in the future,
Xn + m = j given Xn = i.

00:39:10.130 --> 00:39:15.059
So this is saying it's at i now
what's the probability that it'll

00:39:15.059 --> 00:39:17.181
be at j m steps in the future?

00:39:17.181 --> 00:39:22.230
That's just gonna be the ij
entry of q to the m.

00:39:29.188 --> 00:39:34.442
So, what works out pretty nicely at
the powers of the transition matrix,

00:39:34.442 --> 00:39:37.340
actually give us a lot of information.

00:39:37.340 --> 00:39:42.052
We don't need a separate study of
every different transition and

00:39:42.052 --> 00:39:44.118
different number of steps.

00:39:44.118 --> 00:39:49.097
Just work with powers of
transition matrix very, very nice.

00:39:49.097 --> 00:39:54.230
Okay, so
just a couple more quick concepts.

00:39:54.230 --> 00:39:59.782
Well one that's not so quick is
the notion of a stationary distribution.

00:39:59.782 --> 00:40:07.440
And I'm gonna define it now, but
we'll talk about it in detail next time.

00:40:07.440 --> 00:40:11.140
But I just want to mention it now just
cuz it's one of the most important ideas.

00:40:11.140 --> 00:40:16.125
Transition matrix and and
stationary distribution are the two

00:40:16.125 --> 00:40:20.550
most important ideas other
than the basic definition.

00:40:20.550 --> 00:40:22.430
What's a stationary
distribution of a chain.

00:40:22.430 --> 00:40:27.400
It's also called a steady stay or long
line or equilibrium, that kind of thing.

00:40:27.400 --> 00:40:30.800
It's supposed to capture the idea, so

00:40:30.800 --> 00:40:34.212
one reason it's important is if
we run the chain for a long time,

00:40:34.212 --> 00:40:39.220
we wanna know, does it converge
to some limiting distribution?

00:40:39.220 --> 00:40:42.226
Okay, and under some very mild conditions,
the answer is yes,

00:40:42.226 --> 00:40:45.915
it will converge to something that we
call a stationary distribution, okay?

00:40:45.915 --> 00:40:49.700
So that's gonna be be very important for
describing how the chain behaves.

00:40:49.700 --> 00:40:54.180
Especially in the long run, but it's
also just useful for other things too.

00:40:54.180 --> 00:41:01.070
So we say that S,
which is a probability vector.

00:41:04.260 --> 00:41:06.580
1 by M again.

00:41:06.580 --> 00:41:10.140
So again we're just thinking of
that as a PMF written as a row.

00:41:10.140 --> 00:41:17.348
And the definition is that
this is stationary, For

00:41:17.348 --> 00:41:21.964
the chain, for
the Markov chain that we are considering.

00:41:21.964 --> 00:41:31.957
If S times Q Which is,
just to make sure this makes sense again.

00:41:31.957 --> 00:41:34.505
This is a 1 x M * M x M.

00:41:34.505 --> 00:41:36.398
So this is gonna be a 1 by M matrix.

00:41:36.398 --> 00:41:41.860
If S * Q equals S, okay?

00:41:41.860 --> 00:41:46.720
So I'll tell you the intuition for
this definition but first of all for

00:41:46.720 --> 00:41:50.400
those of you who've seen eigenvalues and
eigenvectors before,

00:41:50.400 --> 00:41:54.420
this is pretty reminiscent of
an eigenvalue eigenvector equation.

00:41:54.420 --> 00:41:57.720
Usually you see eigenvalues and
eigenvectors written the other way,

00:41:57.720 --> 00:41:59.480
like a matrix times the vector but

00:41:59.480 --> 00:42:03.260
if you want to write it that way just
take the transpose of both sides.

00:42:03.260 --> 00:42:06.620
So that's exactly an eigenvalue
eigenvector equation, for

00:42:06.620 --> 00:42:07.570
those of you who know what that is.

00:42:07.570 --> 00:42:10.970
If you don't know what an eigenvalue is,
well you should learn but

00:42:10.970 --> 00:42:13.950
you don't need to know it for
this class, okay?

00:42:13.950 --> 00:42:19.920
Now for the intuition,
we just showed a few minutes ago,

00:42:19.920 --> 00:42:25.680
we just showed that if the chain F
time N follows the distribution S,

00:42:25.680 --> 00:42:29.540
then this is the distribution
one step later in time, right?

00:42:29.540 --> 00:42:33.860
So if SQ equals S,
it says if it starts out according to S,

00:42:33.860 --> 00:42:37.530
one step forward in time,
it's still following S.

00:42:37.530 --> 00:42:39.063
That's why it's called stationary, right?

00:42:39.063 --> 00:42:40.620
Cuz it didn't change.

00:42:40.620 --> 00:42:44.800
And we go another step forward in time,
the distribution is still S.

00:42:44.800 --> 00:42:47.760
And you go one more step,
it's still S, right?

00:42:47.760 --> 00:42:51.212
So if the chain starts out of
the stationary distribution,

00:42:51.212 --> 00:42:54.538
it's gonna have the stationary
distribution forever.

00:42:54.538 --> 00:42:56.160
It doesn't change,
that's why it's called stationary.

00:42:57.380 --> 00:43:03.688
Okay, well, there are a lot of questions,
we can ask though.

00:43:03.688 --> 00:43:05.623
I won't quite call this a cliffhanger but

00:43:05.623 --> 00:43:08.330
there's some important
questions we can ask, right?

00:43:08.330 --> 00:43:12.055
One is does a stationary
distribution exist?

00:43:16.290 --> 00:43:18.755
That is, can we solve this equation?

00:43:18.755 --> 00:43:24.350
Now, even if we solve this equation, if we
got an answer that had like some negative

00:43:24.350 --> 00:43:28.350
numbers and some positive numbers,
that's not gonna be useful, right?

00:43:28.350 --> 00:43:33.630
So we need to solve this for
S that is non negative and adds up to 1.

00:43:33.630 --> 00:43:39.296
So does such a solution
exist to this equation?

00:43:39.296 --> 00:43:41.603
Does it exist?

00:43:41.603 --> 00:43:45.738
Secondly, is it unique?

00:43:48.973 --> 00:43:54.833
Thirdly, just now,
I just kind of said intuitively that this

00:43:54.833 --> 00:44:01.800
has something to do with the long
run behavior of the chain, right?

00:44:01.800 --> 00:44:05.480
But this definition doesn't seem like it
has anything to do with any long run or

00:44:05.480 --> 00:44:06.100
limits, right?

00:44:06.100 --> 00:44:09.031
This is just S times Q equals S.

00:44:09.031 --> 00:44:11.953
So does the chain converge
to S in some sense?

00:44:15.034 --> 00:44:17.970
And we'll have to talk more about what
that means and whether it's true.

00:44:19.860 --> 00:44:23.750
Okay and
then there's a more practical question.

00:44:23.750 --> 00:44:28.123
Assuming that it exists and it's this
beautiful thing that tells us the long run

00:44:28.123 --> 00:44:32.210
behavior, there's still the question
of how can we compute it, right?

00:44:32.210 --> 00:44:34.243
So how can we compute it?

00:44:39.686 --> 00:44:44.762
So those are basic questions about

00:44:44.762 --> 00:44:50.197
stationary distributions, okay?

00:44:50.197 --> 00:44:56.140
So under some very mild conditions
the answer will be yes,

00:44:56.140 --> 00:45:00.766
to all three of these
first three questions.

00:45:00.766 --> 00:45:04.693
There are a few technical
conditions that we'll get into but

00:45:04.693 --> 00:45:07.610
under some mild technical conditions.

00:45:07.610 --> 00:45:08.460
It will exist.

00:45:08.460 --> 00:45:09.370
It will be unique.

00:45:09.370 --> 00:45:11.490
The chain will converge to
the stationary distribution.

00:45:11.490 --> 00:45:15.030
So it does capture the long run behavior.

00:45:15.030 --> 00:45:19.680
As for this last question though,
how to compute it?

00:45:19.680 --> 00:45:24.960
In principle, if you had enough time,
you can just use a computer or

00:45:24.960 --> 00:45:28.560
well, if you had enough time you
can do it by hand in principle.

00:45:28.560 --> 00:45:30.506
Solve this equation, right?

00:45:30.506 --> 00:45:32.660
Even if you haven't done matrices before.

00:45:32.660 --> 00:45:36.580
You could write this out as some
enormous linear system of equations.

00:45:36.580 --> 00:45:40.193
And if the solution exists in
the principle you can find it,

00:45:40.193 --> 00:45:42.875
you eliminate variables one by one, okay?

00:45:42.875 --> 00:45:46.960
That might take hundreds of years,
all right?

00:45:46.960 --> 00:45:53.290
So, can we compute it efficiently without
spending years doing tedious calculations?

00:45:53.290 --> 00:45:57.380
Or even with a computer,
it may get too nasty of a calculation.

00:45:58.460 --> 00:46:04.530
And so the answer to that,
in the very general case is that,

00:46:04.530 --> 00:46:10.260
it may be an extremely difficult
computational problem.

00:46:10.260 --> 00:46:16.090
But we're gonna study
certain examples of Markov

00:46:16.090 --> 00:46:21.510
chains where we can not only compute it,
we can compute it very quickly and

00:46:21.510 --> 00:46:26.710
easily without actually having
to even use matrices at all.

00:46:26.710 --> 00:46:32.230
So there's one really,
really nice case that we're gonna look at.

00:46:32.230 --> 00:46:33.977
Not today, though so.

00:46:33.977 --> 00:46:36.710
Okay, so that's all for today,
happy Thanksgiving, everyone.

