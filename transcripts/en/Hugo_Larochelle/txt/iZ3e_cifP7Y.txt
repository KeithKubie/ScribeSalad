Speaker 1:          00:00          In this video, we'll see a very simple and common way of taking a word and representing it into a vector aerial form that we can feed to any machine learning algorithm. Uh, this vectorial form is known as the one hot encoding. So in the previous video we saw some preprocessing steps for converting words into Integer Ids. Um, now these integer ids, uh, they're easy to manipulate their numerical, so that's easy for a computer to manipulate and do numerical computations with it. However, we cannot really feed this, uh, this id to a machine learning algorithm. Um, I'll discuss a bit more about why that is at a, at the end of the slide. Uh, but let me first introduce another representation that uses this word id and converts it into a vector, which is going to be a better representation to start with if want to do some machine learning on top of a words.

Speaker 1:          01:01          So the one hot encoding, uh, like I said, it's just a basic representation for representing a word into effector. It's using, uh, the, this one hot encoding approach, which simply corresponds to a, taking a vector, fending it with Zeros, and then putting a one only at the position corresponding to the ID of the word. So let's do an example. Uh, if we had a vocabulary of size 10, and by that I mean, uh, that are nine words in the vocabulary plus the, uh, out of vocabulary token, uh, that we added, which corresponds to the 10th, uh, uh, id. So then the one hot vector for the one on the coding vector for a word id, which is four would be the following. It would be a vector where everything is zero, except the element at the position four. So one, two, three, four. We see we have a one here.

Speaker 1:          02:03          I'm going to note that e of w. So W is for an e four is going to be this huge factor with just a single one in it at the position for what's Nice when the 100 and coding and 100 factor is that it actually makes no assumptions about the similarity of the words. Uh, specifically, if we're looking at the, uh, square, the Euclidean distance between the, uh, uh, the representation of a word w in the word ww prime. So if you take the difference and then look at the, uh, Euclidean norm of that difference, it's going to be zero. IfW and w prime are the same, that is the corresponding exactly the same word. And otherwise, uh, by simple calculation, you'll see that, uh, we get a distance of two if w is not w prime. So if they actually correspond to different words in the vocabulary.

Speaker 1:          02:59          So in other words, all words ICWA, Lee, different under that representation. So that's a much more natural representation to start with. And not making any assumption about which forms are more similar to which other words, uh, going to leave that to a subsequent learning algorithm to learn this. So that's better than actually feeding the integer value w because otherwise, if we had a word, which as the ID for, and then we had a word w prime, which has the id five, and then another word, which has the, uh, id. So doubling prime prime's say 10,000. So it just happened to be lower down the list and the vocabulary than this word. Then if we look at these, uh, distance between four and five, that's much smaller than between four and 10,000, based on the integer value. However, it might be that in my vocabulary, well four is actually Doug.

Speaker 1:          03:53          Uh, he is, uh, maybe, uh, jump and then Duh, w prime prime might be capped and uh, we might argue that dog and cat actually, uh, uh, courser then, then, uh, jump and Doug points since, and this is only because, uh, uh, we didn't put any thought into the ordering of the words and also there's no really simple way of putting each word on the single line and putting them in a position which is actually meaningful. So, uh, that's why we can't really use the ID for the words. They usually not going to represent a meaningful similarity between the words. So we're going to instead start with the 100 and coding representation, which is actually a, encodes no particular makes, no assumptions about word similarities. And then we'll leave it to the learning algorithm to try to, uh, learn which words are more similar than others.

Speaker 1:          04:52          Now still there is a big problem with the 100 representation or 100 detector or encoding and it said it's very, very high dimensional. So the dimentionality one hug vector is the size of the vocabulary and the, like we've mentioned before, typical vocabulary sizes can vary between 10,000 to a few hundred thousand. And that's just for one word. Now imagine we had the, a sequence of 10 words, a window, uh, in, in some texts that contain 10 words. And we want to encode the position of these different words. So we want to concatenate these 100 factors. So that gives us a total vector of length 1 million if we had 100,000 for, uh, the vocabulary size. So that's much bigger than, uh, most, uh, problems there we, uh, usually facing with neural networks or most machine learning problems. So the consequence of having a very high dimensional input is first we're will be vulnerable to overfitting.

Speaker 1:          05:54          If we have millions of inputs, then it means that, so the size of him, our input is, is in the millions, then we're probably gonna have a, in the millions number of parameters in our senior all network or any other learning algorithm really. So if we have millions of Prevnar is, it means that we need a lot of data to estimate well these parameters without over fitting the other problem and consequences that this can, uh, um, imply some computational, uh, it compassionately expensive, uh, computations that we will have to do when we manipulate these factors. Uh, specifically because not all calculations are computations can be specified. So the one that vector can be, uh, uh, sparsely and coated, um, so, and to a sparse data structure and vector as far as vectors multiply by matrices for instance, can be done fairly efficiently using as Parson implementation. However, the other, uh, um, operations of, for instance, the reconstruction in Anton quarter, what we'd want to find since reconstruct this whole window of words that cannot be specified easily and trivially. Uh, if we did an auto encoder, we would actually need to reconstruct each of the element in this vector of a million and a. So those are the two consequences of using this basic one hot representation. And, and, and these are issues that we'll try to tackle in the different algorithms that we'll see in a, and different ideas. We'll see in the following videos.