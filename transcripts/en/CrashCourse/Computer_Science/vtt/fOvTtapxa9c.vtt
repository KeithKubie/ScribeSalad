WEBVTT
Kind: captions
Language: en

00:00:03.180 --> 00:00:06.320
Hi, I’m Carrie Anne, and welcome to Crash
Course Computer Science!

00:00:06.330 --> 00:00:09.840
Last episode we talked about computer vision
– giving computers the ability to see and

00:00:09.840 --> 00:00:11.589
understand visual information.

00:00:11.589 --> 00:00:15.730
Today we’re going to talk about how to give
computers the ability to understand language.

00:00:15.730 --> 00:00:17.630
You might argue they’ve always had this
capability.

00:00:17.630 --> 00:00:21.680
Back in Episodes 9 and 12, we talked about
machine language instructions, as well as

00:00:21.680 --> 00:00:23.450
higher-level programming languages.

00:00:23.450 --> 00:00:26.980
While these certainly meet the definition
of a language, they also tend to have small

00:00:26.980 --> 00:00:30.370
vocabularies and follow highly structured
conventions.

00:00:30.370 --> 00:00:35.170
Code will only compile and run if it’s 100
percent free of spelling and syntactic errors.

00:00:35.170 --> 00:00:38.789
Of course, this is quite different from human
languages – what are called natural languages

00:00:38.789 --> 00:00:43.129
– containing large, diverse vocabularies,
words with several different meanings, speakers

00:00:43.129 --> 00:00:46.500
with different accents, and all sorts of interesting
word play.

00:00:46.500 --> 00:00:51.010
People also make linguistic faux pas when
writing and speaking, like slurring words

00:00:51.010 --> 00:00:54.929
together, leaving out key details so things
are ambiguous, and mispronouncing things.

00:00:54.929 --> 00:00:58.390
But, for the most part, humans can roll right
through these challenges.

00:00:58.390 --> 00:01:01.859
The skillful use of language is a major part
of what makes us human.

00:01:01.859 --> 00:01:05.910
And for this reason, the desire for computers
to understand and speak our language has been

00:01:05.910 --> 00:01:08.190
around since they were first conceived.

00:01:08.190 --> 00:01:13.190
This led to the creation of Natural Language
Processing, or NLP, an interdisciplinary field

00:01:13.190 --> 00:01:15.660
combining computer science and linguistics.

00:01:15.660 --> 00:01:24.640
INTRO

00:01:24.640 --> 00:01:28.660
There’s an essentially infinite number of
ways to arrange words in a sentence.

00:01:28.660 --> 00:01:32.700
We can’t give computers a dictionary of
all possible sentences to help them understand

00:01:32.700 --> 00:01:34.500
what humans are blabbing on about.

00:01:34.500 --> 00:01:39.790
So an early and fundamental NLP problem was
deconstructing sentences into bite-sized pieces,

00:01:39.790 --> 00:01:41.520
which could be more easily processed.

00:01:41.520 --> 00:01:46.640
In school, you learned about nine fundamental
types of English words: nouns, pronouns, articles,

00:01:46.640 --> 00:01:50.700
verbs, adjectives, adverbs, prepositions,
conjunctions, and interjections.

00:01:50.700 --> 00:01:52.620
These are called parts of speech.

00:01:52.630 --> 00:01:56.930
There are all sorts of subcategories too,
like singular vs. plural nouns and superlative

00:01:56.930 --> 00:01:59.950
vs. comparative adverbs, but we’re not going
to get into that.

00:01:59.950 --> 00:02:03.700
Knowing a word’s type is definitely useful,
but unfortunately, there are a lot words that

00:02:03.700 --> 00:02:07.730
have multiple meanings – like “rose”
and “leaves”, which can be used as nouns

00:02:07.730 --> 00:02:08.730
or verbs.

00:02:08.730 --> 00:02:12.950
A digital dictionary alone isn’t enough
to resolve this ambiguity, so computers also

00:02:12.950 --> 00:02:14.230
need to know some grammar.

00:02:14.230 --> 00:02:18.709
For this, phrase structure rules were developed,
which encapsulate the grammar of a language.

00:02:18.709 --> 00:02:22.430
For example, in English there’s a rule that
says a sentence can be comprised of a noun

00:02:22.430 --> 00:02:24.629
phrase followed by a verb phrase.

00:02:24.629 --> 00:02:28.870
Noun phrases can be an article, like “the”,
followed by a noun or they can be an adjective

00:02:28.870 --> 00:02:30.170
followed by a noun.

00:02:30.170 --> 00:02:32.599
And you can make rules like this for an entire
language.

00:02:32.599 --> 00:02:36.549
Then, using these rules, it’s fairly easy
to construct what’s called a parse tree,

00:02:36.549 --> 00:02:40.970
which not only tags every word with a likely
part of speech, but also reveals how the sentence

00:02:40.970 --> 00:02:41.760
is constructed.

00:02:41.760 --> 00:02:46.299
We now know, for example, that the noun focus
of this sentence is “the mongols”, and

00:02:46.299 --> 00:02:50.540
we know it’s about them doing the action
of “rising” from something, in this case,

00:02:50.540 --> 00:02:51.240
“leaves”.

00:02:51.240 --> 00:02:55.299
These smaller chunks of data allow computers
to more easily access, process and respond

00:02:55.299 --> 00:02:56.299
to information.

00:02:56.299 --> 00:02:59.969
Equivalent processes are happening every time
you do a voice search, like: “where’s

00:02:59.969 --> 00:03:01.200
the nearest pizza”.

00:03:01.200 --> 00:03:04.769
The computer can recognize that this is a
“where” question, knows you want the noun

00:03:04.769 --> 00:03:07.689
“pizza”, and the dimension you care about
is “nearest”.

00:03:07.689 --> 00:03:12.099
The same process applies to “what is the
biggest giraffe?” or “who sang thriller?”

00:03:12.099 --> 00:03:17.159
By treating language almost like lego, computers
can be quite adept at natural language tasks.

00:03:17.159 --> 00:03:21.370
They can answer questions and also process
commands, like “set an alarm for 2:20”

00:03:21.370 --> 00:03:23.600
or “play T-Swizzle on spotify”.

00:03:23.600 --> 00:03:27.700
But, as you’ve probably experienced, they
fail when you start getting too fancy, and

00:03:27.709 --> 00:03:31.799
they can no longer parse the sentence correctly,
or capture your intent.

00:03:31.799 --> 00:03:37.000
Hey Siri... methinks the mongols doth roam
too much, what think ye on this most gentle

00:03:37.000 --> 00:03:38.000
mid-summer’s day?

00:03:38.000 --> 00:03:40.340
Siri: I’m not sure I got that.

00:03:40.340 --> 00:03:44.540
I should also note that phrase structure rules,
and similar methods that codify language,

00:03:44.540 --> 00:03:47.870
can be used by computers to generate natural
language text.

00:03:47.870 --> 00:03:52.180
This works particularly well when data is
stored in a web of semantic information, where

00:03:52.180 --> 00:03:56.700
entities are linked to one another in meaningful
relationships, providing all the ingredients

00:03:56.700 --> 00:03:59.120
you need to craft informational sentences.

00:03:59.120 --> 00:04:02.930
Siri: Thriller was released in 1983 and sung
by Michael Jackson

00:04:02.930 --> 00:04:05.290
Google’s version of this is called Knowledge
Graph.

00:04:05.290 --> 00:04:11.230
At the end of 2016, it contained roughly seventy
billion facts about, and relationships between,

00:04:11.230 --> 00:04:12.230
different entities.

00:04:12.230 --> 00:04:17.049
These two processes, parsing and generating
text, are fundamental components of natural

00:04:17.049 --> 00:04:19.400
language chatbots - computer programs that
chat with you.

00:04:19.400 --> 00:04:23.500
Early chatbots were primarily rule-based,
where experts would encode hundreds of rules

00:04:23.509 --> 00:04:27.160
mapping what a user might say, to how a program
should reply.

00:04:27.160 --> 00:04:30.710
Obviously this was unwieldy to maintain and
limited the possible sophistication.

00:04:30.710 --> 00:04:35.880
A famous early example was ELIZA, created
in the mid-1960s at MIT.

00:04:35.880 --> 00:04:39.930
This was a chatbot that took on the role of
a therapist, and used basic syntactic rules

00:04:39.930 --> 00:04:44.100
to identify content in written exchanges,
which it would turn around and ask the user

00:04:44.100 --> 00:04:44.740
about.

00:04:44.740 --> 00:04:49.080
Sometimes, it felt very much like human-human
communication, but other times it would make

00:04:49.080 --> 00:04:50.940
simple and even comical mistakes.

00:04:50.940 --> 00:04:55.300
Chatbots, and more advanced dialog systems,
have come a long way in the last fifty years,

00:04:55.300 --> 00:04:57.340
and can be quite convincing today!

00:04:57.340 --> 00:05:01.389
Modern approaches are based on machine learning,
where gigabytes of real human-to-human chats

00:05:01.389 --> 00:05:02.980
are used to train chatbots.

00:05:02.980 --> 00:05:07.360
Today, the technology is finding use in customer
service applications, where there’s already

00:05:07.360 --> 00:05:09.780
heaps of example conversations to learn from.

00:05:09.780 --> 00:05:14.540
People have also been getting chatbots to
talk with one another, and in a Facebook experiment,

00:05:14.550 --> 00:05:17.190
chatbots even started to evolve their own
language.

00:05:17.190 --> 00:05:21.889
This experiment got a bunch of scary-sounding
press, but it was just the computers crafting

00:05:21.889 --> 00:05:24.659
a simplified protocol to negotiate with one
another.

00:05:24.660 --> 00:05:26.540
It wasn’t evil, it’s was efficient.

00:05:26.540 --> 00:05:30.860
But what about if something is spoken – how
does a computer get words from the sound?

00:05:30.860 --> 00:05:34.640
That’s the domain of speech recognition,
which has been the focus of research for many

00:05:34.640 --> 00:05:35.140
decades.

00:05:35.140 --> 00:05:41.240
Bell Labs debuted the first speech recognition
system in 1952, nicknamed Audrey – the automatic

00:05:41.240 --> 00:05:42.620
digit recognizer.

00:05:42.620 --> 00:05:46.540
It could recognize all ten numerical digits,
if you said them slowly enough.

00:05:46.580 --> 00:05:47.580
5…

00:05:48.180 --> 00:05:49.180
9…

00:05:49.460 --> 00:05:50.460
7?

00:05:50.640 --> 00:05:54.000
The project didn’t go anywhere because it
was much faster to enter telephone numbers

00:05:54.009 --> 00:05:55.370
with a finger.

00:05:55.370 --> 00:06:00.550
Ten years later, at the 1962 World's Fair,
IBM demonstrated a shoebox-sized machine capable

00:06:00.550 --> 00:06:02.539
of recognizing sixteen words.

00:06:02.539 --> 00:06:06.520
To boost research in the area, DARPA kicked
off an ambitious five-year funding initiative

00:06:06.520 --> 00:06:11.849
in 1971, which led to the development of Harpy
at Carnegie Mellon University.

00:06:11.849 --> 00:06:15.110
Harpy was the first system to recognize over
a thousand words.

00:06:15.110 --> 00:06:19.030
But, on computers of the era, transcription
was often ten or more times slower than the

00:06:19.030 --> 00:06:20.580
rate of natural speech.

00:06:20.580 --> 00:06:25.510
Fortunately, thanks to huge advances in computing
performance in the 1980s and 90s, continuous,

00:06:25.510 --> 00:06:27.550
real-time speech recognition became practical.

00:06:27.550 --> 00:06:32.110
There was simultaneous innovation in the algorithms
for processing natural language, moving from

00:06:32.110 --> 00:06:36.419
hand-crafted rules, to machine learning techniques
that could learn automatically from existing

00:06:36.419 --> 00:06:38.420
datasets of human language.

00:06:38.420 --> 00:06:43.580
Today, the speech recognition systems with
the best accuracy are using deep neural networks,

00:06:43.580 --> 00:06:45.259
which we touched on in Episode 34.

00:06:45.259 --> 00:06:49.860
To get a sense of how these techniques work,
let’s look at some speech, specifically,

00:06:49.860 --> 00:06:50.919
the acoustic signal.

00:06:50.919 --> 00:06:54.280
Let’s start by looking at vowel sounds,
like aaaaa…and Eeeeeee.

00:06:54.280 --> 00:06:58.379
These are the waveforms of those two sounds,
as captured by a computer’s microphone.

00:06:58.379 --> 00:07:02.810
As we discussed in Episode 21 – on Files
and File Formats – this signal is the magnitude

00:07:02.810 --> 00:07:07.879
of displacement, of a diaphragm inside of
a microphone, as sound waves cause it to oscillate.

00:07:07.879 --> 00:07:12.439
In this view of sound data, the horizontal
axis is time, and the vertical axis is the

00:07:12.439 --> 00:07:14.419
magnitude of displacement, or amplitude.

00:07:14.419 --> 00:07:18.229
Although we can see there are differences
between the waveforms, it’s not super obvious

00:07:18.229 --> 00:07:22.340
what you would point at to say, “ah ha!
this is definitely an eeee sound”.

00:07:22.340 --> 00:07:27.370
To really make this pop out, we need to view
the data in a totally different way: a spectrogram.

00:07:27.370 --> 00:07:31.539
In this view of the data, we still have time
along the horizontal axis, but now instead

00:07:31.539 --> 00:07:35.870
of amplitude on the vertical axis, we plot
the magnitude of the different frequencies

00:07:35.870 --> 00:07:37.539
that make up each sound.

00:07:37.539 --> 00:07:40.650
The brighter the color, the louder that frequency
component.

00:07:40.650 --> 00:07:44.610
This conversion from waveform to frequencies
is done with a very cool algorithm called

00:07:44.610 --> 00:07:46.600
a Fast Fourier Transform.

00:07:46.600 --> 00:07:50.560
If you’ve ever stared at a stereo system’s
EQ visualizer, it’s pretty much the same

00:07:50.580 --> 00:07:51.280
thing.

00:07:51.360 --> 00:07:54.320
A spectrogram is plotting that information
over time.

00:07:54.320 --> 00:07:57.770
You might have noticed that the signals have
a sort of ribbed pattern to them – that’s

00:07:57.770 --> 00:08:00.009
all the resonances of my vocal tract.

00:08:00.009 --> 00:08:04.139
To make different sounds, I squeeze my vocal
chords, mouth and tongue into different shapes,

00:08:04.139 --> 00:08:06.789
which amplifies or dampens different resonances.

00:08:06.789 --> 00:08:10.530
We can see this in the signal, with areas
that are brighter, and areas that are darker.

00:08:10.530 --> 00:08:14.290
If we work our way up from the bottom, labeling
where we see peaks in the spectrum – what

00:08:14.290 --> 00:08:18.340
are called formants – we can see the two
sounds have quite different arrangements.

00:08:18.340 --> 00:08:20.289
And this is true for all vowel sounds.

00:08:20.289 --> 00:08:24.590
It’s exactly this type of information that
lets computers recognize spoken vowels, and

00:08:24.590 --> 00:08:25.780
indeed, whole words.

00:08:25.780 --> 00:08:31.110
Let’s see a more complicated example, like
when I say: “she.. was.. happy”

00:08:31.110 --> 00:08:34.349
We can see our “eee” sound here, and “aaa”
sound here.

00:08:34.349 --> 00:08:38.610
We can also see a bunch of other distinctive
sounds, like the “shh” sound in “she”,

00:08:38.610 --> 00:08:40.860
the “wah” and “sss” in “was”,
and so on.

00:08:40.860 --> 00:08:43.520
These sound pieces, that make up words, are
called phonemes.

00:08:43.520 --> 00:08:46.960
Speech recognition software knows what all
these phonemes look like.

00:08:46.970 --> 00:08:51.690
In English, there are roughly forty-four,
so it mostly boils down to fancy pattern matching.

00:08:51.690 --> 00:08:55.920
Then you have to separate words from one another,
figure out when sentences begin and end...

00:08:55.920 --> 00:08:59.870
and ultimately, you end up with speech converted
into text, allowing for techniques like we

00:08:59.870 --> 00:09:02.110
discussed at the beginning of the episode.

00:09:02.110 --> 00:09:06.860
Because people say words in slightly different
ways, due to things like accents and mispronunciations,

00:09:06.860 --> 00:09:11.110
transcription accuracy is greatly improved
when combined with a language model, which

00:09:11.110 --> 00:09:13.610
contains statistics about sequences of words.

00:09:13.610 --> 00:09:17.540
For example “she was” is most likely to
be followed by an adjective, like “happy”.

00:09:17.540 --> 00:09:21.560
It’s uncommon for “she was” to be followed
immediately by a noun.

00:09:21.560 --> 00:09:26.350
So if the speech recognizer was unsure between,
“happy” and “harpy”, it’d pick “happy”,

00:09:26.350 --> 00:09:29.530
since the language model would report that
as a more likely choice.

00:09:29.530 --> 00:09:34.529
Finally, we need to talk about Speech Synthesis,
that is, giving computers the ability to output

00:09:34.529 --> 00:09:35.160
speech.

00:09:35.160 --> 00:09:38.320
This is very much like speech recognition,
but in reverse.

00:09:38.330 --> 00:09:42.410
We can take a sentence of text, and break
it down into its phonetic components, and

00:09:42.410 --> 00:09:45.480
then play those sounds back to back, out of
a computer speaker.

00:09:45.480 --> 00:09:50.120
You can hear this chaining of phonemes very
clearly with older speech synthesis technologies,

00:09:50.120 --> 00:09:53.780
like this 1937, hand-operated machine from
Bell Labs.

00:09:53.780 --> 00:09:56.880
Say, "she saw me" with no expression.

00:09:56.880 --> 00:09:59.220
She saw me.

00:09:59.220 --> 00:10:01.540
Now say it in answer to these questions.

00:10:01.540 --> 00:10:02.820
Who saw you?

00:10:02.820 --> 00:10:04.820
She saw me.

00:10:04.820 --> 00:10:05.920
Who did she see?

00:10:05.920 --> 00:10:07.920
She saw me.

00:10:07.920 --> 00:10:09.920
Did she see you or hear you?

00:10:09.920 --> 00:10:11.300
She saw me.

00:10:11.300 --> 00:10:15.360
By the 1980s, this had improved a lot, but
that discontinuous and awkward blending of

00:10:15.400 --> 00:10:18.540
phonemes still created that signature, robotic
sound.

00:10:18.540 --> 00:10:23.100
Thriller was released in 1983 and sung by Michael Jackson.

00:10:23.100 --> 00:10:27.740
Today, synthesized computer voices, like Siri,
Cortana and Alexa, have gotten much better,

00:10:27.740 --> 00:10:29.500
but they’re still not quite human.

00:10:29.519 --> 00:10:33.709
But we’re soo soo close, and it’s likely
to be a solved problem pretty soon.

00:10:33.709 --> 00:10:37.820
Especially because we’re now seeing an explosion
of voice user interfaces on our phones, in

00:10:37.820 --> 00:10:41.320
our cars and homes, and maybe soon, plugged
right into our ears.

00:10:41.320 --> 00:10:45.870
This ubiquity is creating a positive feedback
loop, where people are using voice interaction

00:10:45.870 --> 00:10:50.630
more often, which in turn, is giving companies
like Google, Amazon and Microsoft more data

00:10:50.630 --> 00:10:52.339
to train their systems on...

00:10:52.339 --> 00:10:56.320
Which is enabling better accuracy, which is
leading to people using voice more, which

00:10:56.320 --> 00:10:59.740
is enabling even better accuracy… and the
loop continues!

00:10:59.740 --> 00:11:03.570
Many predict that speech technologies will
become as common a form of interaction as

00:11:03.570 --> 00:11:07.800
screens, keyboards, trackpads and other physical
input-output devices that we use today.

00:11:07.800 --> 00:11:11.540
That’s particularly good news for robots,
who don’t want to have to walk around with

00:11:11.540 --> 00:11:13.780
keyboards in order to communicate with humans.

00:11:13.780 --> 00:11:16.220
But, we’ll talk more about them next week.

00:11:16.280 --> 00:11:17.200
See you then.

