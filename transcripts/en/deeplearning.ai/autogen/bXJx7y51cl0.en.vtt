WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.459
 
you know and they were pretty much all

00:00:01.459 --> 00:00:01.469
you know and they were pretty much all
 

00:00:01.469 --> 00:00:03.619
you know and they were pretty much all
the building blocks or building a full

00:00:03.619 --> 00:00:03.629
the building blocks or building a full
 

00:00:03.629 --> 00:00:05.780
the building blocks or building a full
convolutional neural network let's look

00:00:05.780 --> 00:00:05.790
convolutional neural network let's look
 

00:00:05.790 --> 00:00:08.120
convolutional neural network let's look
at an example let's say that you're

00:00:08.120 --> 00:00:08.130
at an example let's say that you're
 

00:00:08.130 --> 00:00:13.160
at an example let's say that you're
inputting an image which is 32 by 32 by

00:00:13.160 --> 00:00:13.170
inputting an image which is 32 by 32 by
 

00:00:13.170 --> 00:00:16.849
inputting an image which is 32 by 32 by
3 so it's an RGB image and maybe you're

00:00:16.849 --> 00:00:16.859
3 so it's an RGB image and maybe you're
 

00:00:16.859 --> 00:00:18.170
3 so it's an RGB image and maybe you're
trying to do handwritten digit

00:00:18.170 --> 00:00:18.180
trying to do handwritten digit
 

00:00:18.180 --> 00:00:19.460
trying to do handwritten digit
recognition you know so you have a

00:00:19.460 --> 00:00:19.470
recognition you know so you have a
 

00:00:19.470 --> 00:00:25.250
recognition you know so you have a
number like 7 in a 32 by 32 RGB image

00:00:25.250 --> 00:00:25.260
number like 7 in a 32 by 32 RGB image
 

00:00:25.260 --> 00:00:27.050
number like 7 in a 32 by 32 RGB image
mean you're trying to recognize which

00:00:27.050 --> 00:00:27.060
mean you're trying to recognize which
 

00:00:27.060 --> 00:00:29.810
mean you're trying to recognize which
one of the ten digits from 0 to 9 this

00:00:29.810 --> 00:00:29.820
one of the ten digits from 0 to 9 this
 

00:00:29.820 --> 00:00:32.240
one of the ten digits from 0 to 9 this
is let's build a neural network to do

00:00:32.240 --> 00:00:32.250
is let's build a neural network to do
 

00:00:32.250 --> 00:00:34.940
is let's build a neural network to do
this and what I'm gonna use in this

00:00:34.940 --> 00:00:34.950
this and what I'm gonna use in this
 

00:00:34.950 --> 00:00:37.610
this and what I'm gonna use in this
slide is inspired it's actually quite

00:00:37.610 --> 00:00:37.620
slide is inspired it's actually quite
 

00:00:37.620 --> 00:00:39.950
slide is inspired it's actually quite
similar to one of the classic neural

00:00:39.950 --> 00:00:39.960
similar to one of the classic neural
 

00:00:39.960 --> 00:00:42.470
similar to one of the classic neural
networks called the net 5 which is

00:00:42.470 --> 00:00:42.480
networks called the net 5 which is
 

00:00:42.480 --> 00:00:44.690
networks called the net 5 which is
created by Yama could many years ago

00:00:44.690 --> 00:00:44.700
created by Yama could many years ago
 

00:00:44.700 --> 00:00:46.729
created by Yama could many years ago
what up show here isn't exactly the net

00:00:46.729 --> 00:00:46.739
what up show here isn't exactly the net
 

00:00:46.739 --> 00:00:49.430
what up show here isn't exactly the net
5 buttons inspired by it but many of the

00:00:49.430 --> 00:00:49.440
5 buttons inspired by it but many of the
 

00:00:49.440 --> 00:00:52.389
5 buttons inspired by it but many of the
parameter choices was inspired by it

00:00:52.389 --> 00:00:52.399
parameter choices was inspired by it
 

00:00:52.399 --> 00:00:56.720
parameter choices was inspired by it
so we're by 32 by 32 by 3 input let's

00:00:56.720 --> 00:00:56.730
so we're by 32 by 32 by 3 input let's
 

00:00:56.730 --> 00:01:00.709
so we're by 32 by 32 by 3 input let's
say that the first layer armed uses a 5

00:01:00.709 --> 00:01:00.719
say that the first layer armed uses a 5
 

00:01:00.719 --> 00:01:03.410
say that the first layer armed uses a 5
by 5 filter and let's try that one and

00:01:03.410 --> 00:01:03.420
by 5 filter and let's try that one and
 

00:01:03.420 --> 00:01:07.130
by 5 filter and let's try that one and
no padding so the output of this layer

00:01:07.130 --> 00:01:07.140
no padding so the output of this layer
 

00:01:07.140 --> 00:01:13.060
no padding so the output of this layer
if you use 6 filters would be 28 by 28

00:01:13.060 --> 00:01:13.070
if you use 6 filters would be 28 by 28
 

00:01:13.070 --> 00:01:17.859
if you use 6 filters would be 28 by 28
by 6 I'm gonna call this layer com1

00:01:17.859 --> 00:01:17.869
by 6 I'm gonna call this layer com1
 

00:01:17.869 --> 00:01:21.260
by 6 I'm gonna call this layer com1
right see the PI 6 filters Adebayor is

00:01:21.260 --> 00:01:21.270
right see the PI 6 filters Adebayor is
 

00:01:21.270 --> 00:01:24.170
right see the PI 6 filters Adebayor is
apply the non-linearity maybe a Rayleigh

00:01:24.170 --> 00:01:24.180
apply the non-linearity maybe a Rayleigh
 

00:01:24.180 --> 00:01:26.780
apply the non-linearity maybe a Rayleigh
non-linearity and that's the content 1

00:01:26.780 --> 00:01:26.790
non-linearity and that's the content 1
 

00:01:26.790 --> 00:01:30.530
non-linearity and that's the content 1
output next let's apply a pooling layer

00:01:30.530 --> 00:01:30.540
output next let's apply a pooling layer
 

00:01:30.540 --> 00:01:34.819
output next let's apply a pooling layer
so I'm going to apply max pooling here

00:01:34.819 --> 00:01:34.829
so I'm going to apply max pooling here
 

00:01:34.829 --> 00:01:39.800
so I'm going to apply max pooling here
and let's use a F equals to s equals 2

00:01:39.800 --> 00:01:39.810
and let's use a F equals to s equals 2
 

00:01:39.810 --> 00:01:41.990
and let's use a F equals to s equals 2
or when I don't write up adding a music

00:01:41.990 --> 00:01:42.000
or when I don't write up adding a music
 

00:01:42.000 --> 00:01:45.440
or when I don't write up adding a music
padding is equal to 0 next let's apply a

00:01:45.440 --> 00:01:45.450
padding is equal to 0 next let's apply a
 

00:01:45.450 --> 00:01:48.679
padding is equal to 0 next let's apply a
pooling where we're going to apply say

00:01:48.679 --> 00:01:48.689
pooling where we're going to apply say
 

00:01:48.689 --> 00:01:52.840
pooling where we're going to apply say
max pooling with a 2 by 2 filter and

00:01:52.840 --> 00:01:52.850
max pooling with a 2 by 2 filter and
 

00:01:52.850 --> 00:01:55.850
max pooling with a 2 by 2 filter and
stride equals 2 so this should reduce

00:01:55.850 --> 00:01:55.860
stride equals 2 so this should reduce
 

00:01:55.860 --> 00:01:58.130
stride equals 2 so this should reduce
the height and width of the

00:01:58.130 --> 00:01:58.140
the height and width of the
 

00:01:58.140 --> 00:02:00.469
the height and width of the
representation by a factor of 2 so 28 by

00:02:00.469 --> 00:02:00.479
representation by a factor of 2 so 28 by
 

00:02:00.479 --> 00:02:06.770
representation by a factor of 2 so 28 by
28 now becomes 14 by 14 and the number

00:02:06.770 --> 00:02:06.780
28 now becomes 14 by 14 and the number
 

00:02:06.780 --> 00:02:08.690
28 now becomes 14 by 14 and the number
of channels remains the same so 14 by 14

00:02:08.690 --> 00:02:08.700
of channels remains the same so 14 by 14
 

00:02:08.700 --> 00:02:12.330
of channels remains the same so 14 by 14
by 6 and when the call there's some

00:02:12.330 --> 00:02:12.340
by 6 and when the call there's some
 

00:02:12.340 --> 00:02:16.920
by 6 and when the call there's some
the pool one output so it turns out that

00:02:16.920 --> 00:02:16.930
the pool one output so it turns out that
 

00:02:16.930 --> 00:02:20.130
the pool one output so it turns out that
in the literature of a continent there

00:02:20.130 --> 00:02:20.140
in the literature of a continent there
 

00:02:20.140 --> 00:02:22.440
in the literature of a continent there
are two conventions which are in

00:02:22.440 --> 00:02:22.450
are two conventions which are in
 

00:02:22.450 --> 00:02:24.000
are two conventions which are in
slightly inconsistent about what you

00:02:24.000 --> 00:02:24.010
slightly inconsistent about what you
 

00:02:24.010 --> 00:02:27.930
slightly inconsistent about what you
call a layer one convention is that this

00:02:27.930 --> 00:02:27.940
call a layer one convention is that this
 

00:02:27.940 --> 00:02:31.650
call a layer one convention is that this
is called one layer so this would be

00:02:31.650 --> 00:02:31.660
is called one layer so this would be
 

00:02:31.660 --> 00:02:35.729
is called one layer so this would be
layer one of the neural network another

00:02:35.729 --> 00:02:35.739
layer one of the neural network another
 

00:02:35.739 --> 00:02:37.530
layer one of the neural network another
convention would be to count the con

00:02:37.530 --> 00:02:37.540
convention would be to count the con
 

00:02:37.540 --> 00:02:39.809
convention would be to count the con
Flair as a layer and the pool layer as a

00:02:39.809 --> 00:02:39.819
Flair as a layer and the pool layer as a
 

00:02:39.819 --> 00:02:42.990
Flair as a layer and the pool layer as a
layer when people report the number of

00:02:42.990 --> 00:02:43.000
layer when people report the number of
 

00:02:43.000 --> 00:02:45.240
layer when people report the number of
layers in your network usually people

00:02:45.240 --> 00:02:45.250
layers in your network usually people
 

00:02:45.250 --> 00:02:46.890
layers in your network usually people
report just a number of layers that have

00:02:46.890 --> 00:02:46.900
report just a number of layers that have
 

00:02:46.900 --> 00:02:49.740
report just a number of layers that have
weights that have parameters and because

00:02:49.740 --> 00:02:49.750
weights that have parameters and because
 

00:02:49.750 --> 00:02:51.720
weights that have parameters and because
the pooling layer has no weights has no

00:02:51.720 --> 00:02:51.730
the pooling layer has no weights has no
 

00:02:51.730 --> 00:02:54.110
the pooling layer has no weights has no
parameters only a few hyper parameters

00:02:54.110 --> 00:02:54.120
parameters only a few hyper parameters
 

00:02:54.120 --> 00:02:56.190
parameters only a few hyper parameters
I'm going to use the convention that

00:02:56.190 --> 00:02:56.200
I'm going to use the convention that
 

00:02:56.200 --> 00:02:58.259
I'm going to use the convention that
they come one and prove one here

00:02:58.259 --> 00:02:58.269
they come one and prove one here
 

00:02:58.269 --> 00:02:59.880
they come one and prove one here
together I'm going to treat that as

00:02:59.880 --> 00:02:59.890
together I'm going to treat that as
 

00:02:59.890 --> 00:03:03.630
together I'm going to treat that as
layer one although sometimes you see

00:03:03.630 --> 00:03:03.640
layer one although sometimes you see
 

00:03:03.640 --> 00:03:05.789
layer one although sometimes you see
people if you read articles online or

00:03:05.789 --> 00:03:05.799
people if you read articles online or
 

00:03:05.799 --> 00:03:07.860
people if you read articles online or
research papers you hear about the

00:03:07.860 --> 00:03:07.870
research papers you hear about the
 

00:03:07.870 --> 00:03:10.410
research papers you hear about the
contraire and the pooling layer as if

00:03:10.410 --> 00:03:10.420
contraire and the pooling layer as if
 

00:03:10.420 --> 00:03:12.059
contraire and the pooling layer as if
there are two separate layers but this

00:03:12.059 --> 00:03:12.069
there are two separate layers but this
 

00:03:12.069 --> 00:03:14.550
there are two separate layers but this
is a maybe two slightly and consistent

00:03:14.550 --> 00:03:14.560
is a maybe two slightly and consistent
 

00:03:14.560 --> 00:03:17.490
is a maybe two slightly and consistent
notation terminologies but when I count

00:03:17.490 --> 00:03:17.500
notation terminologies but when I count
 

00:03:17.500 --> 00:03:20.009
notation terminologies but when I count
layers I'm just going to come layers

00:03:20.009 --> 00:03:20.019
layers I'm just going to come layers
 

00:03:20.019 --> 00:03:23.039
layers I'm just going to come layers
that have weights so they treat both of

00:03:23.039 --> 00:03:23.049
that have weights so they treat both of
 

00:03:23.049 --> 00:03:26.370
that have weights so they treat both of
these together as layer one and the name

00:03:26.370 --> 00:03:26.380
these together as layer one and the name
 

00:03:26.380 --> 00:03:28.530
these together as layer one and the name
con one and pool one that when they use

00:03:28.530 --> 00:03:28.540
con one and pool one that when they use
 

00:03:28.540 --> 00:03:31.710
con one and pool one that when they use
here the one at the end also refers to

00:03:31.710 --> 00:03:31.720
here the one at the end also refers to
 

00:03:31.720 --> 00:03:33.720
here the one at the end also refers to
the fact that I view both of these as

00:03:33.720 --> 00:03:33.730
the fact that I view both of these as
 

00:03:33.730 --> 00:03:36.800
the fact that I view both of these as
part of layer one of the neural network

00:03:36.800 --> 00:03:36.810
part of layer one of the neural network
 

00:03:36.810 --> 00:03:39.690
part of layer one of the neural network
and and pool one is grouped into layer

00:03:39.690 --> 00:03:39.700
and and pool one is grouped into layer
 

00:03:39.700 --> 00:03:41.190
and and pool one is grouped into layer
one because it doesn't have his own

00:03:41.190 --> 00:03:41.200
one because it doesn't have his own
 

00:03:41.200 --> 00:03:43.470
one because it doesn't have his own
wings now let's apply another

00:03:43.470 --> 00:03:43.480
wings now let's apply another
 

00:03:43.480 --> 00:03:45.539
wings now let's apply another
convolutional layer to this I'm going to

00:03:45.539 --> 00:03:45.549
convolutional layer to this I'm going to
 

00:03:45.549 --> 00:03:49.490
convolutional layer to this I'm going to
use a 5x5 filter so F equals five and

00:03:49.490 --> 00:03:49.500
use a 5x5 filter so F equals five and
 

00:03:49.500 --> 00:03:52.410
use a 5x5 filter so F equals five and
try this one and you know but I don't

00:03:52.410 --> 00:03:52.420
try this one and you know but I don't
 

00:03:52.420 --> 00:03:53.670
try this one and you know but I don't
read them having a means there's no

00:03:53.670 --> 00:03:53.680
read them having a means there's no
 

00:03:53.680 --> 00:03:56.729
read them having a means there's no
padding and this will give you the

00:03:56.729 --> 00:03:56.739
padding and this will give you the
 

00:03:56.739 --> 00:04:01.590
padding and this will give you the
confidence your 16 filters so this would

00:04:01.590 --> 00:04:01.600
confidence your 16 filters so this would
 

00:04:01.600 --> 00:04:05.720
confidence your 16 filters so this would
be a 10 by 10 by 16 dimensional output

00:04:05.720 --> 00:04:05.730
be a 10 by 10 by 16 dimensional output
 

00:04:05.730 --> 00:04:08.819
be a 10 by 10 by 16 dimensional output
so maybe look like that

00:04:08.819 --> 00:04:08.829
so maybe look like that
 

00:04:08.829 --> 00:04:13.420
so maybe look like that
and this is the conf to layer and

00:04:13.420 --> 00:04:13.430
and this is the conf to layer and
 

00:04:13.430 --> 00:04:18.039
and this is the conf to layer and
let's apply max pooling to this with F

00:04:18.039 --> 00:04:18.049
let's apply max pooling to this with F
 

00:04:18.049 --> 00:04:20.920
let's apply max pooling to this with F
equals to s equals to your prey gets the

00:04:20.920 --> 00:04:20.930
equals to s equals to your prey gets the
 

00:04:20.930 --> 00:04:23.980
equals to s equals to your prey gets the
output of this 10 by 10 by 16 what max

00:04:23.980 --> 00:04:23.990
output of this 10 by 10 by 16 what max
 

00:04:23.990 --> 00:04:27.340
output of this 10 by 10 by 16 what max
pooling with F equals to s equals 2 this

00:04:27.340 --> 00:04:27.350
pooling with F equals to s equals 2 this
 

00:04:27.350 --> 00:04:31.180
pooling with F equals to s equals 2 this
will half the height and width you can

00:04:31.180 --> 00:04:31.190
will half the height and width you can
 

00:04:31.190 --> 00:04:33.189
will half the height and width you can
pretty guessed the result of this right

00:04:33.189 --> 00:04:33.199
pretty guessed the result of this right
 

00:04:33.199 --> 00:04:35.350
pretty guessed the result of this right
well max pooling with F equals to s

00:04:35.350 --> 00:04:35.360
well max pooling with F equals to s
 

00:04:35.360 --> 00:04:37.810
well max pooling with F equals to s
equals to this should half the height

00:04:37.810 --> 00:04:37.820
equals to this should half the height
 

00:04:37.820 --> 00:04:40.779
equals to this should half the height
and width so you end up with a 5 by 5 by

00:04:40.779 --> 00:04:40.789
and width so you end up with a 5 by 5 by
 

00:04:40.789 --> 00:04:44.260
and width so you end up with a 5 by 5 by
16 volume same number of channels as

00:04:44.260 --> 00:04:44.270
16 volume same number of channels as
 

00:04:44.270 --> 00:04:49.379
16 volume same number of channels as
before I'm going to call this pool 2 and

00:04:49.379 --> 00:04:49.389
before I'm going to call this pool 2 and
 

00:04:49.389 --> 00:04:55.390
before I'm going to call this pool 2 and
in our convention this is layer 2

00:04:55.390 --> 00:04:55.400
in our convention this is layer 2
 

00:04:55.400 --> 00:04:57.430
in our convention this is layer 2
because you know this has a 1 set of

00:04:57.430 --> 00:04:57.440
because you know this has a 1 set of
 

00:04:57.440 --> 00:05:00.670
because you know this has a 1 set of
weights in the count of 3 we're now 5

00:05:00.670 --> 00:05:00.680
weights in the count of 3 we're now 5
 

00:05:00.680 --> 00:05:03.700
weights in the count of 3 we're now 5
point 5 by 16 5 times 5 times 16 is

00:05:03.700 --> 00:05:03.710
point 5 by 16 5 times 5 times 16 is
 

00:05:03.710 --> 00:05:07.810
point 5 by 16 5 times 5 times 16 is
equal to 400 so let's now fatten our

00:05:07.810 --> 00:05:07.820
equal to 400 so let's now fatten our
 

00:05:07.820 --> 00:05:12.969
equal to 400 so let's now fatten our
pool 2 into a 400 by 1 dimensional

00:05:12.969 --> 00:05:12.979
pool 2 into a 400 by 1 dimensional
 

00:05:12.979 --> 00:05:14.890
pool 2 into a 400 by 1 dimensional
vector you also think of this as flatten

00:05:14.890 --> 00:05:14.900
vector you also think of this as flatten
 

00:05:14.900 --> 00:05:17.920
vector you also think of this as flatten
this out into just a set of neurons like

00:05:17.920 --> 00:05:17.930
this out into just a set of neurons like
 

00:05:17.930 --> 00:05:21.189
this out into just a set of neurons like
so and what we're going to do is then

00:05:21.189 --> 00:05:21.199
so and what we're going to do is then
 

00:05:21.199 --> 00:05:26.860
so and what we're going to do is then
take this 400 units and let's build the

00:05:26.860 --> 00:05:26.870
take this 400 units and let's build the
 

00:05:26.870 --> 00:05:33.279
take this 400 units and let's build the
next layer as having 120 units so this

00:05:33.279 --> 00:05:33.289
next layer as having 120 units so this
 

00:05:33.289 --> 00:05:35.529
next layer as having 120 units so this
is actually our first fully connect to

00:05:35.529 --> 00:05:35.539
is actually our first fully connect to
 

00:05:35.539 --> 00:05:39.300
is actually our first fully connect to
layer I'm going to call this F C 3

00:05:39.300 --> 00:05:39.310
layer I'm going to call this F C 3
 

00:05:39.310 --> 00:05:44.230
layer I'm going to call this F C 3
because we have 400 units densely

00:05:44.230 --> 00:05:44.240
because we have 400 units densely
 

00:05:44.240 --> 00:05:50.050
because we have 400 units densely
connected to 120 units so this fully

00:05:50.050 --> 00:05:50.060
connected to 120 units so this fully
 

00:05:50.060 --> 00:05:51.730
connected to 120 units so this fully
connected unit this fully connected

00:05:51.730 --> 00:05:51.740
connected unit this fully connected
 

00:05:51.740 --> 00:05:55.629
connected unit this fully connected
layer is just like the single neural

00:05:55.629 --> 00:05:55.639
layer is just like the single neural
 

00:05:55.639 --> 00:05:58.629
layer is just like the single neural
network layer that you saw in causes 1 &amp;

00:05:58.629 --> 00:05:58.639
network layer that you saw in causes 1 &amp;
 

00:05:58.639 --> 00:06:00.520
network layer that you saw in causes 1 &amp;
2 it this is just a standard neural

00:06:00.520 --> 00:06:00.530
2 it this is just a standard neural
 

00:06:00.530 --> 00:06:03.899
2 it this is just a standard neural
network where you have a weight matrix

00:06:03.899 --> 00:06:03.909
network where you have a weight matrix
 

00:06:03.909 --> 00:06:10.330
network where you have a weight matrix
that's called w3 of dimension 120 by 400

00:06:10.330 --> 00:06:10.340
that's called w3 of dimension 120 by 400
 

00:06:10.340 --> 00:06:11.920
that's called w3 of dimension 120 by 400
and this is called fully connected

00:06:11.920 --> 00:06:11.930
and this is called fully connected
 

00:06:11.930 --> 00:06:13.719
and this is called fully connected
because each of the 400 units here is

00:06:13.719 --> 00:06:13.729
because each of the 400 units here is
 

00:06:13.729 --> 00:06:16.899
because each of the 400 units here is
connected to each of the 120 units here

00:06:16.899 --> 00:06:16.909
connected to each of the 120 units here
 

00:06:16.909 --> 00:06:19.870
connected to each of the 120 units here
and you also have a bias parameter yes

00:06:19.870 --> 00:06:19.880
and you also have a bias parameter yes
 

00:06:19.880 --> 00:06:21.640
and you also have a bias parameter yes
that's going to be just a 100

00:06:21.640 --> 00:06:21.650
that's going to be just a 100
 

00:06:21.650 --> 00:06:25.180
that's going to be just a 100
20 dimensional because if 120 outputs

00:06:25.180 --> 00:06:25.190
20 dimensional because if 120 outputs
 

00:06:25.190 --> 00:06:28.659
20 dimensional because if 120 outputs
and then lastly let's take 120 units and

00:06:28.659 --> 00:06:28.669
and then lastly let's take 120 units and
 

00:06:28.669 --> 00:06:31.270
and then lastly let's take 120 units and
add another layer this time a little bit

00:06:31.270 --> 00:06:31.280
add another layer this time a little bit
 

00:06:31.280 --> 00:06:34.570
add another layer this time a little bit
smaller but let's say we have 84 units

00:06:34.570 --> 00:06:34.580
smaller but let's say we have 84 units
 

00:06:34.580 --> 00:06:37.210
smaller but let's say we have 84 units
here we're gonna call this fully coming

00:06:37.210 --> 00:06:37.220
here we're gonna call this fully coming
 

00:06:37.220 --> 00:06:42.129
here we're gonna call this fully coming
to layer 4 and finally you now have 84

00:06:42.129 --> 00:06:42.139
to layer 4 and finally you now have 84
 

00:06:42.139 --> 00:06:44.560
to layer 4 and finally you now have 84
roll numbers that you can feed to a

00:06:44.560 --> 00:06:44.570
roll numbers that you can feed to a
 

00:06:44.570 --> 00:06:48.129
roll numbers that you can feed to a
softmax unit and if we're trying to do

00:06:48.129 --> 00:06:48.139
softmax unit and if we're trying to do
 

00:06:48.139 --> 00:06:49.749
softmax unit and if we're trying to do
handwritten digit recognition you know

00:06:49.749 --> 00:06:49.759
handwritten digit recognition you know
 

00:06:49.759 --> 00:06:52.360
handwritten digit recognition you know
to recognize is it the Hammerton 0 1 2

00:06:52.360 --> 00:06:52.370
to recognize is it the Hammerton 0 1 2
 

00:06:52.370 --> 00:06:55.270
to recognize is it the Hammerton 0 1 2
and so on up to 9 then this would be a

00:06:55.270 --> 00:06:55.280
and so on up to 9 then this would be a
 

00:06:55.280 --> 00:07:01.800
and so on up to 9 then this would be a
softmax with 10 outputs so this is a

00:07:01.800 --> 00:07:01.810
softmax with 10 outputs so this is a
 

00:07:01.810 --> 00:07:05.230
softmax with 10 outputs so this is a
reasonably typical example of what a

00:07:05.230 --> 00:07:05.240
reasonably typical example of what a
 

00:07:05.240 --> 00:07:07.240
reasonably typical example of what a
convolutional neural network might look

00:07:07.240 --> 00:07:07.250
convolutional neural network might look
 

00:07:07.250 --> 00:07:09.640
convolutional neural network might look
like and I know this seems like there

00:07:09.640 --> 00:07:09.650
like and I know this seems like there
 

00:07:09.650 --> 00:07:13.330
like and I know this seems like there
are a lot of hyper parameters we'll give

00:07:13.330 --> 00:07:13.340
are a lot of hyper parameters we'll give
 

00:07:13.340 --> 00:07:15.310
are a lot of hyper parameters we'll give
you some more specific suggestions later

00:07:15.310 --> 00:07:15.320
you some more specific suggestions later
 

00:07:15.320 --> 00:07:17.140
you some more specific suggestions later
for how to choose these types of hyper

00:07:17.140 --> 00:07:17.150
for how to choose these types of hyper
 

00:07:17.150 --> 00:07:21.040
for how to choose these types of hyper
parameters maybe one guideline is to one

00:07:21.040 --> 00:07:21.050
parameters maybe one guideline is to one
 

00:07:21.050 --> 00:07:23.230
parameters maybe one guideline is to one
common guideline is to actually not try

00:07:23.230 --> 00:07:23.240
common guideline is to actually not try
 

00:07:23.240 --> 00:07:24.820
common guideline is to actually not try
to invent your own settings of hybrid

00:07:24.820 --> 00:07:24.830
to invent your own settings of hybrid
 

00:07:24.830 --> 00:07:26.680
to invent your own settings of hybrid
parameters but to look in the literature

00:07:26.680 --> 00:07:26.690
parameters but to look in the literature
 

00:07:26.690 --> 00:07:28.870
parameters but to look in the literature
to see what hyper parameters you work

00:07:28.870 --> 00:07:28.880
to see what hyper parameters you work
 

00:07:28.880 --> 00:07:31.629
to see what hyper parameters you work
for others and to just choose an

00:07:31.629 --> 00:07:31.639
for others and to just choose an
 

00:07:31.639 --> 00:07:33.129
for others and to just choose an
architecture that has worked well for

00:07:33.129 --> 00:07:33.139
architecture that has worked well for
 

00:07:33.139 --> 00:07:34.390
architecture that has worked well for
someone else

00:07:34.390 --> 00:07:34.400
someone else
 

00:07:34.400 --> 00:07:35.890
someone else
and there's a chance that will work for

00:07:35.890 --> 00:07:35.900
and there's a chance that will work for
 

00:07:35.900 --> 00:07:38.560
and there's a chance that will work for
your application as well I will say more

00:07:38.560 --> 00:07:38.570
your application as well I will say more
 

00:07:38.570 --> 00:07:41.710
your application as well I will say more
about that next week but for now it

00:07:41.710 --> 00:07:41.720
about that next week but for now it
 

00:07:41.720 --> 00:07:44.020
about that next week but for now it
doesn't point out that as you go deeper

00:07:44.020 --> 00:07:44.030
doesn't point out that as you go deeper
 

00:07:44.030 --> 00:07:47.710
doesn't point out that as you go deeper
in the neural network usually an H and W

00:07:47.710 --> 00:07:47.720
in the neural network usually an H and W
 

00:07:47.720 --> 00:07:49.990
in the neural network usually an H and W
the height and width will decrease

00:07:49.990 --> 00:07:50.000
the height and width will decrease
 

00:07:50.000 --> 00:07:51.670
the height and width will decrease
quantities are earlier but it goes from

00:07:51.670 --> 00:07:51.680
quantities are earlier but it goes from
 

00:07:51.680 --> 00:07:54.760
quantities are earlier but it goes from
32 by 32 to 20 by 20 to 40 by 40 into 10

00:07:54.760 --> 00:07:54.770
32 by 32 to 20 by 20 to 40 by 40 into 10
 

00:07:54.770 --> 00:07:57.610
32 by 32 to 20 by 20 to 40 by 40 into 10
by 10 to 5 by 5 so as you go deeper

00:07:57.610 --> 00:07:57.620
by 10 to 5 by 5 so as you go deeper
 

00:07:57.620 --> 00:07:58.870
by 10 to 5 by 5 so as you go deeper
usually the height and width will

00:07:58.870 --> 00:07:58.880
usually the height and width will
 

00:07:58.880 --> 00:08:01.510
usually the height and width will
decrease whereas the number of channels

00:08:01.510 --> 00:08:01.520
decrease whereas the number of channels
 

00:08:01.520 --> 00:08:06.520
decrease whereas the number of channels
will increase gone from 3 to 6 to 16 and

00:08:06.520 --> 00:08:06.530
will increase gone from 3 to 6 to 16 and
 

00:08:06.530 --> 00:08:08.350
will increase gone from 3 to 6 to 16 and
then you're fully connected layers at

00:08:08.350 --> 00:08:08.360
then you're fully connected layers at
 

00:08:08.360 --> 00:08:11.710
then you're fully connected layers at
the end and another pretty common

00:08:11.710 --> 00:08:11.720
the end and another pretty common
 

00:08:11.720 --> 00:08:13.870
the end and another pretty common
pattern you see in neural networks is to

00:08:13.870 --> 00:08:13.880
pattern you see in neural networks is to
 

00:08:13.880 --> 00:08:16.689
pattern you see in neural networks is to
have cons layers maybe one or more

00:08:16.689 --> 00:08:16.699
have cons layers maybe one or more
 

00:08:16.699 --> 00:08:19.540
have cons layers maybe one or more
conflicts followed by proving layer and

00:08:19.540 --> 00:08:19.550
conflicts followed by proving layer and
 

00:08:19.550 --> 00:08:22.330
conflicts followed by proving layer and
then one or more convalesce followed by

00:08:22.330 --> 00:08:22.340
then one or more convalesce followed by
 

00:08:22.340 --> 00:08:25.000
then one or more convalesce followed by
pooling layer and then at the end to

00:08:25.000 --> 00:08:25.010
pooling layer and then at the end to
 

00:08:25.010 --> 00:08:27.399
pooling layer and then at the end to
have a few fully connect to layers and

00:08:27.399 --> 00:08:27.409
have a few fully connect to layers and
 

00:08:27.409 --> 00:08:29.589
have a few fully connect to layers and
then followed by maybe a soft max and

00:08:29.589 --> 00:08:29.599
then followed by maybe a soft max and
 

00:08:29.599 --> 00:08:31.980
then followed by maybe a soft max and
this is another pretty common

00:08:31.980 --> 00:08:31.990
this is another pretty common
 

00:08:31.990 --> 00:08:35.550
this is another pretty common
Pattin you see in new networks so let's

00:08:35.550 --> 00:08:35.560
Pattin you see in new networks so let's
 

00:08:35.560 --> 00:08:37.620
Pattin you see in new networks so let's
just go through for this new network

00:08:37.620 --> 00:08:37.630
just go through for this new network
 

00:08:37.630 --> 00:08:39.570
just go through for this new network
some more details of what are the

00:08:39.570 --> 00:08:39.580
some more details of what are the
 

00:08:39.580 --> 00:08:41.790
some more details of what are the
activation shape the activation size in

00:08:41.790 --> 00:08:41.800
activation shape the activation size in
 

00:08:41.800 --> 00:08:43.560
activation shape the activation size in
the number of parameters in this network

00:08:43.560 --> 00:08:43.570
the number of parameters in this network
 

00:08:43.570 --> 00:08:47.400
the number of parameters in this network
so the input was 32 by 32 by 3 and you

00:08:47.400 --> 00:08:47.410
so the input was 32 by 32 by 3 and you
 

00:08:47.410 --> 00:08:49.290
so the input was 32 by 32 by 3 and you
multiply out those numbers you should

00:08:49.290 --> 00:08:49.300
multiply out those numbers you should
 

00:08:49.300 --> 00:08:53.070
multiply out those numbers you should
get 3 0 72 so the activation you know a

00:08:53.070 --> 00:08:53.080
get 3 0 72 so the activation you know a
 

00:08:53.080 --> 00:08:57.960
get 3 0 72 so the activation you know a
0 has dimension 3 0 72 was really 32 by

00:08:57.960 --> 00:08:57.970
0 has dimension 3 0 72 was really 32 by
 

00:08:57.970 --> 00:09:03.420
0 has dimension 3 0 72 was really 32 by
32 by 3 and there are no parameters I

00:09:03.420 --> 00:09:03.430
32 by 3 and there are no parameters I
 

00:09:03.430 --> 00:09:06.600
32 by 3 and there are no parameters I
guess in the input layer and as you look

00:09:06.600 --> 00:09:06.610
guess in the input layer and as you look
 

00:09:06.610 --> 00:09:09.960
guess in the input layer and as you look
at the different layers feel free to

00:09:09.960 --> 00:09:09.970
at the different layers feel free to
 

00:09:09.970 --> 00:09:12.000
at the different layers feel free to
work of the details yourself these are

00:09:12.000 --> 00:09:12.010
work of the details yourself these are
 

00:09:12.010 --> 00:09:14.550
work of the details yourself these are
the activation shape and yet duration

00:09:14.550 --> 00:09:14.560
the activation shape and yet duration
 

00:09:14.560 --> 00:09:18.390
the activation shape and yet duration
sizes of these different layers so

00:09:18.390 --> 00:09:18.400
sizes of these different layers so
 

00:09:18.400 --> 00:09:19.920
sizes of these different layers so
doesn't point out a few things first

00:09:19.920 --> 00:09:19.930
doesn't point out a few things first
 

00:09:19.930 --> 00:09:22.320
doesn't point out a few things first
notice that the pooling layers the max

00:09:22.320 --> 00:09:22.330
notice that the pooling layers the max
 

00:09:22.330 --> 00:09:25.580
notice that the pooling layers the max
pooling layers don't have any parameters

00:09:25.580 --> 00:09:25.590
pooling layers don't have any parameters
 

00:09:25.590 --> 00:09:29.340
pooling layers don't have any parameters
second notice that the conflate tend to

00:09:29.340 --> 00:09:29.350
second notice that the conflate tend to
 

00:09:29.350 --> 00:09:32.310
second notice that the conflate tend to
have relatively few parameters as we

00:09:32.310 --> 00:09:32.320
have relatively few parameters as we
 

00:09:32.320 --> 00:09:35.340
have relatively few parameters as we
discussed in an earlier video and in

00:09:35.340 --> 00:09:35.350
discussed in an earlier video and in
 

00:09:35.350 --> 00:09:38.040
discussed in an earlier video and in
fact a lot of the parameters tend to be

00:09:38.040 --> 00:09:38.050
fact a lot of the parameters tend to be
 

00:09:38.050 --> 00:09:40.110
fact a lot of the parameters tend to be
in the fully collected layers of the new

00:09:40.110 --> 00:09:40.120
in the fully collected layers of the new
 

00:09:40.120 --> 00:09:43.350
in the fully collected layers of the new
network and then you notice also that

00:09:43.350 --> 00:09:43.360
network and then you notice also that
 

00:09:43.360 --> 00:09:48.270
network and then you notice also that
the activation size tends to maybe go

00:09:48.270 --> 00:09:48.280
the activation size tends to maybe go
 

00:09:48.280 --> 00:09:50.670
the activation size tends to maybe go
down gradually as you go deeper into

00:09:50.670 --> 00:09:50.680
down gradually as you go deeper into
 

00:09:50.680 --> 00:09:53.850
down gradually as you go deeper into
neural network if it drops too quickly

00:09:53.850 --> 00:09:53.860
neural network if it drops too quickly
 

00:09:53.860 --> 00:09:55.860
neural network if it drops too quickly
that's usually not great for performance

00:09:55.860 --> 00:09:55.870
that's usually not great for performance
 

00:09:55.870 --> 00:09:59.010
that's usually not great for performance
as well right so starts in the first

00:09:59.010 --> 00:09:59.020
as well right so starts in the first
 

00:09:59.020 --> 00:10:03.000
as well right so starts in the first
there were 6,000 and you know 1,500 600

00:10:03.000 --> 00:10:03.010
there were 6,000 and you know 1,500 600
 

00:10:03.010 --> 00:10:06.180
there were 6,000 and you know 1,500 600
and then slowly falls into 84 until

00:10:06.180 --> 00:10:06.190
and then slowly falls into 84 until
 

00:10:06.190 --> 00:10:08.270
and then slowly falls into 84 until
finally you have your soft max output

00:10:08.270 --> 00:10:08.280
finally you have your soft max output
 

00:10:08.280 --> 00:10:10.760
finally you have your soft max output
and you find that a lot of confidence

00:10:10.760 --> 00:10:10.770
and you find that a lot of confidence
 

00:10:10.770 --> 00:10:13.500
and you find that a lot of confidence
will help you know properties will have

00:10:13.500 --> 00:10:13.510
will help you know properties will have
 

00:10:13.510 --> 00:10:16.860
will help you know properties will have
patterns similar to these so you've now

00:10:16.860 --> 00:10:16.870
patterns similar to these so you've now
 

00:10:16.870 --> 00:10:18.930
patterns similar to these so you've now
seen the basic building blocks of neural

00:10:18.930 --> 00:10:18.940
seen the basic building blocks of neural
 

00:10:18.940 --> 00:10:20.250
seen the basic building blocks of neural
networks of convolutional neural

00:10:20.250 --> 00:10:20.260
networks of convolutional neural
 

00:10:20.260 --> 00:10:22.290
networks of convolutional neural
networks the conflate the pooling layer

00:10:22.290 --> 00:10:22.300
networks the conflate the pooling layer
 

00:10:22.300 --> 00:10:24.780
networks the conflate the pooling layer
and the fully connected layer a lot of

00:10:24.780 --> 00:10:24.790
and the fully connected layer a lot of
 

00:10:24.790 --> 00:10:26.220
and the fully connected layer a lot of
computer vision research has gone into

00:10:26.220 --> 00:10:26.230
computer vision research has gone into
 

00:10:26.230 --> 00:10:28.500
computer vision research has gone into
figuring out how to put together these

00:10:28.500 --> 00:10:28.510
figuring out how to put together these
 

00:10:28.510 --> 00:10:30.690
figuring out how to put together these
basic building blocks to build effective

00:10:30.690 --> 00:10:30.700
basic building blocks to build effective
 

00:10:30.700 --> 00:10:32.550
basic building blocks to build effective
neural networks and putting these things

00:10:32.550 --> 00:10:32.560
neural networks and putting these things
 

00:10:32.560 --> 00:10:34.320
neural networks and putting these things
together actually requires quite a bit

00:10:34.320 --> 00:10:34.330
together actually requires quite a bit
 

00:10:34.330 --> 00:10:37.200
together actually requires quite a bit
of insight I think that one of the best

00:10:37.200 --> 00:10:37.210
of insight I think that one of the best
 

00:10:37.210 --> 00:10:38.760
of insight I think that one of the best
ways through the gain intuitions about

00:10:38.760 --> 00:10:38.770
ways through the gain intuitions about
 

00:10:38.770 --> 00:10:40.650
ways through the gain intuitions about
how to put these things together is to

00:10:40.650 --> 00:10:40.660
how to put these things together is to
 

00:10:40.660 --> 00:10:41.449
how to put these things together is to
see

00:10:41.449 --> 00:10:41.459
see
 

00:10:41.459 --> 00:10:43.160
see
number of concrete examples of how

00:10:43.160 --> 00:10:43.170
number of concrete examples of how
 

00:10:43.170 --> 00:10:45.559
number of concrete examples of how
others have done it so what I want to do

00:10:45.559 --> 00:10:45.569
others have done it so what I want to do
 

00:10:45.569 --> 00:10:47.600
others have done it so what I want to do
next week to show you a few concrete

00:10:47.600 --> 00:10:47.610
next week to show you a few concrete
 

00:10:47.610 --> 00:10:49.819
next week to show you a few concrete
examples even beyond this first one that

00:10:49.819 --> 00:10:49.829
examples even beyond this first one that
 

00:10:49.829 --> 00:10:51.739
examples even beyond this first one that
you just saw on how people have

00:10:51.739 --> 00:10:51.749
you just saw on how people have
 

00:10:51.749 --> 00:10:53.359
you just saw on how people have
successfully put these things together

00:10:53.359 --> 00:10:53.369
successfully put these things together
 

00:10:53.369 --> 00:10:55.970
successfully put these things together
to about very effective neural networks

00:10:55.970 --> 00:10:55.980
to about very effective neural networks
 

00:10:55.980 --> 00:10:58.280
to about very effective neural networks
and through those videos next week I

00:10:58.280 --> 00:10:58.290
and through those videos next week I
 

00:10:58.290 --> 00:11:00.350
and through those videos next week I
hope they'll help you hone your own

00:11:00.350 --> 00:11:00.360
hope they'll help you hone your own
 

00:11:00.360 --> 00:11:02.179
hope they'll help you hone your own
intuitions about how these things are

00:11:02.179 --> 00:11:02.189
intuitions about how these things are
 

00:11:02.189 --> 00:11:03.739
intuitions about how these things are
built as well as give you concrete

00:11:03.739 --> 00:11:03.749
built as well as give you concrete
 

00:11:03.749 --> 00:11:06.470
built as well as give you concrete
examples of architectures that maybe you

00:11:06.470 --> 00:11:06.480
examples of architectures that maybe you
 

00:11:06.480 --> 00:11:08.629
examples of architectures that maybe you
can just use you know exactly as

00:11:08.629 --> 00:11:08.639
can just use you know exactly as
 

00:11:08.639 --> 00:11:10.609
can just use you know exactly as
developed by someone else or your own

00:11:10.609 --> 00:11:10.619
developed by someone else or your own
 

00:11:10.619 --> 00:11:12.980
developed by someone else or your own
application so we'll do that next week

00:11:12.980 --> 00:11:12.990
application so we'll do that next week
 

00:11:12.990 --> 00:11:14.960
application so we'll do that next week
but before wrapping up this week's

00:11:14.960 --> 00:11:14.970
but before wrapping up this week's
 

00:11:14.970 --> 00:11:17.960
but before wrapping up this week's
videos just one last thing which is one

00:11:17.960 --> 00:11:17.970
videos just one last thing which is one
 

00:11:17.970 --> 00:11:19.639
videos just one last thing which is one
a talk a little bit in the next video

00:11:19.639 --> 00:11:19.649
a talk a little bit in the next video
 

00:11:19.649 --> 00:11:21.619
a talk a little bit in the next video
about why you might want to use

00:11:21.619 --> 00:11:21.629
about why you might want to use
 

00:11:21.629 --> 00:11:23.480
about why you might want to use
convolutions some of the benefits and

00:11:23.480 --> 00:11:23.490
convolutions some of the benefits and
 

00:11:23.490 --> 00:11:25.819
convolutions some of the benefits and
advantages of using convolutions as well

00:11:25.819 --> 00:11:25.829
advantages of using convolutions as well
 

00:11:25.829 --> 00:11:28.069
advantages of using convolutions as well
as how to put it all together how to

00:11:28.069 --> 00:11:28.079
as how to put it all together how to
 

00:11:28.079 --> 00:11:29.509
as how to put it all together how to
take a neural network like the one you

00:11:29.509 --> 00:11:29.519
take a neural network like the one you
 

00:11:29.519 --> 00:11:31.369
take a neural network like the one you
just saw and actually train it on a

00:11:31.369 --> 00:11:31.379
just saw and actually train it on a
 

00:11:31.379 --> 00:11:33.559
just saw and actually train it on a
training set to perform image

00:11:33.559 --> 00:11:33.569
training set to perform image
 

00:11:33.569 --> 00:11:35.629
training set to perform image
recognition or some of the sauce so that

00:11:35.629 --> 00:11:35.639
recognition or some of the sauce so that
 

00:11:35.639 --> 00:11:38.090
recognition or some of the sauce so that
let's go on to the last video of this

00:11:38.090 --> 00:11:38.100
let's go on to the last video of this
 

00:11:38.100 --> 00:11:40.309
let's go on to the last video of this
week

