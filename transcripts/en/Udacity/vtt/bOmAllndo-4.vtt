WEBVTT
Kind: captions
Language: en

00:00:00.012 --> 00:00:03.132
So now we're going to start actually doing the matrix multiplication,

00:00:03.132 --> 00:00:05.499
and so we're going to show the first 3 steps of this.

00:00:05.499 --> 00:00:09.583
The first thing we're going to do is take the value vector and the row pointer vector

00:00:09.583 --> 00:00:14.229
and together we're going to create a segmented representation of the value vector.

00:00:14.229 --> 00:00:19.903
Note that each segment in this resulting vector represents 1 row of the sparse matrix

00:00:19.903 --> 00:00:24.443
and the row pointer shows where each segment head begins.

00:00:24.443 --> 00:00:29.920
The next thing that we're going to do is gather these vector values using these column indices

00:00:29.920 --> 00:00:36.839
to create a corresponding list from which we can multiply each of these individual matrix entries.

00:00:36.839 --> 00:00:40.964
So we see that value a is located in column 0,

00:00:40.964 --> 00:00:46.055
which means it needs to be multiplied by the vector element in row 0.

00:00:46.055 --> 00:00:51.427
So what we're going to do is use these column indices to gather from this vector,

00:00:51.427 --> 00:00:53.798
and that's going to give us the following array.

00:00:53.798 --> 00:00:57.169
Okay, now we actually need to do the pairwise multiplications.

00:00:57.169 --> 00:01:00.653
So we simply use a map operation to multiply this vector--

00:01:00.653 --> 00:01:04.043
not caring about the segmentation--by this vector here,

00:01:04.043 --> 00:01:07.166
and it's going to give us yet another vector of the same size.

00:01:07.166 --> 00:01:12.718
And the final step is that we need to add up the partial products within each segment

00:01:12.718 --> 00:01:18.771
to be able to get the final output vector, and that's where we can use segmented scan.

00:01:18.771 --> 00:01:23.262
But now we have to add up the partial products on each row to get the output vector.

00:01:23.262 --> 00:01:26.668
Specifically, we need to add the partial products within each segment,

00:01:26.668 --> 00:01:29.228
and that's where our segmented scan comes in.

00:01:29.228 --> 00:01:35.666
We need to do an exclusive segmented sum scan to sum up each segment of partial products.

00:01:35.666 --> 00:01:40.504
It's actually a little bit more convenient to do a backwards exclusive segmented sum scan

00:01:40.504 --> 00:01:43.494
so all the sums instead end up at the head of each segment

00:01:43.494 --> 00:01:49.020
since then the row pointer array can be used to gather those per row sums into a dense vector.

00:01:49.020 --> 00:01:54.056
Now here we're actually using a segmented scan to perform what's really a segmented reduce.

00:01:54.056 --> 00:01:57.674
And if you want a good challenge, think about how you might want to build a segmented reduce

00:01:57.674 --> 00:02:00.371
that's a little bit more efficient than a segmented scan

00:02:00.371 --> 00:02:04.505
just to go back to our original matrix to make sure we're actually going to get the same answers.

00:02:04.505 --> 00:02:08.106
So here's our original matrix here and our original vector.

00:02:08.106 --> 00:02:14.498
And we note that we dot product this vector with this vector to get this answer,

00:02:14.498 --> 00:02:18.372
this vector dotted with this vector to get this answer,

00:02:18.372 --> 00:02:21.952
and then this vector dotted with this vector to get this answer.

00:02:21.952 --> 00:02:26.097
And you'll see if you look at each one of these rows, they're going to correspond to the sums

00:02:26.097 --> 00:02:28.656
within each one of these segments.

00:02:28.656 --> 00:02:32.319
But the nice part about multiplying using the sparse matrix representation

00:02:32.319 --> 00:02:35.187
is that we're never multiplying by any zeroes at all.

00:02:35.187 --> 00:02:37.404
So here we've actually had to multiply by zeroes,

00:02:37.404 --> 00:02:40.174
and then we just throw away the answers because they're going to be 0.

00:02:40.174 --> 00:02:45.900
On the other hand, when we're actually implementing this using a true sparse matrix representation,

00:02:45.900 --> 00:02:48.123
we're not going to have any zeroes present at all,

00:02:48.123 --> 00:02:50.238
so this is going to be substantially more efficient

00:02:50.238 --> 00:02:53.299
for any real matrix that has a significant amount of zeroes.

