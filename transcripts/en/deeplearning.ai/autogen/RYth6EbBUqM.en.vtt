WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:03.290
so why do residents work so well let's

00:00:03.290 --> 00:00:03.300
so why do residents work so well let's
 

00:00:03.300 --> 00:00:05.329
so why do residents work so well let's
go through one example that illustrates

00:00:05.329 --> 00:00:05.339
go through one example that illustrates
 

00:00:05.339 --> 00:00:08.480
go through one example that illustrates
why residents work so well at least in

00:00:08.480 --> 00:00:08.490
why residents work so well at least in
 

00:00:08.490 --> 00:00:10.310
why residents work so well at least in
the sense of how you can make them

00:00:10.310 --> 00:00:10.320
the sense of how you can make them
 

00:00:10.320 --> 00:00:12.620
the sense of how you can make them
deeper and deeper without really hurting

00:00:12.620 --> 00:00:12.630
deeper and deeper without really hurting
 

00:00:12.630 --> 00:00:15.289
deeper and deeper without really hurting
your ability to at least get them to do

00:00:15.289 --> 00:00:15.299
your ability to at least get them to do
 

00:00:15.299 --> 00:00:18.200
your ability to at least get them to do
well on the training set and hopefully

00:00:18.200 --> 00:00:18.210
well on the training set and hopefully
 

00:00:18.210 --> 00:00:20.570
well on the training set and hopefully
as you've understood from the third

00:00:20.570 --> 00:00:20.580
as you've understood from the third
 

00:00:20.580 --> 00:00:23.120
as you've understood from the third
course in the sequence doing well on the

00:00:23.120 --> 00:00:23.130
course in the sequence doing well on the
 

00:00:23.130 --> 00:00:25.340
course in the sequence doing well on the
training set is usually a prerequisite

00:00:25.340 --> 00:00:25.350
training set is usually a prerequisite
 

00:00:25.350 --> 00:00:28.070
training set is usually a prerequisite
to doing well on your holdout or on your

00:00:28.070 --> 00:00:28.080
to doing well on your holdout or on your
 

00:00:28.080 --> 00:00:30.890
to doing well on your holdout or on your
death or on your test sets so being able

00:00:30.890 --> 00:00:30.900
death or on your test sets so being able
 

00:00:30.900 --> 00:00:33.319
death or on your test sets so being able
to at least training resident to do well

00:00:33.319 --> 00:00:33.329
to at least training resident to do well
 

00:00:33.329 --> 00:00:35.630
to at least training resident to do well
on the training set is a good first step

00:00:35.630 --> 00:00:35.640
on the training set is a good first step
 

00:00:35.640 --> 00:00:38.500
on the training set is a good first step
toward that let's look at an example

00:00:38.500 --> 00:00:38.510
toward that let's look at an example
 

00:00:38.510 --> 00:00:41.000
toward that let's look at an example
what we saw in the last video was that

00:00:41.000 --> 00:00:41.010
what we saw in the last video was that
 

00:00:41.010 --> 00:00:44.450
what we saw in the last video was that
if you make a network deeper it can hurt

00:00:44.450 --> 00:00:44.460
if you make a network deeper it can hurt
 

00:00:44.460 --> 00:00:47.150
if you make a network deeper it can hurt
your ability to train the network to do

00:00:47.150 --> 00:00:47.160
your ability to train the network to do
 

00:00:47.160 --> 00:00:50.029
your ability to train the network to do
well on the training set and that's why

00:00:50.029 --> 00:00:50.039
well on the training set and that's why
 

00:00:50.039 --> 00:00:52.040
well on the training set and that's why
sometimes you don't want a network that

00:00:52.040 --> 00:00:52.050
sometimes you don't want a network that
 

00:00:52.050 --> 00:00:54.979
sometimes you don't want a network that
is too deep but this is not true or at

00:00:54.979 --> 00:00:54.989
is too deep but this is not true or at
 

00:00:54.989 --> 00:00:56.900
is too deep but this is not true or at
least is much less true when you're

00:00:56.900 --> 00:00:56.910
least is much less true when you're
 

00:00:56.910 --> 00:00:59.630
least is much less true when you're
training a resident so let's go through

00:00:59.630 --> 00:00:59.640
training a resident so let's go through
 

00:00:59.640 --> 00:01:04.310
training a resident so let's go through
an example let's say you have X feeding

00:01:04.310 --> 00:01:04.320
an example let's say you have X feeding
 

00:01:04.320 --> 00:01:07.039
an example let's say you have X feeding
in to some big neural network and this

00:01:07.039 --> 00:01:07.049
in to some big neural network and this
 

00:01:07.049 --> 00:01:12.440
in to some big neural network and this
outputs some activation al let's say for

00:01:12.440 --> 00:01:12.450
outputs some activation al let's say for
 

00:01:12.450 --> 00:01:14.390
outputs some activation al let's say for
this example that you're gonna modify it

00:01:14.390 --> 00:01:14.400
this example that you're gonna modify it
 

00:01:14.400 --> 00:01:18.140
this example that you're gonna modify it
in your network to make it a little bit

00:01:18.140 --> 00:01:18.150
in your network to make it a little bit
 

00:01:18.150 --> 00:01:21.170
in your network to make it a little bit
deeper so you're the same big an end and

00:01:21.170 --> 00:01:21.180
deeper so you're the same big an end and
 

00:01:21.180 --> 00:01:25.640
deeper so you're the same big an end and
this outputs al and we're going to add a

00:01:25.640 --> 00:01:25.650
this outputs al and we're going to add a
 

00:01:25.650 --> 00:01:28.219
this outputs al and we're going to add a
couple extra layers to this network so

00:01:28.219 --> 00:01:28.229
couple extra layers to this network so
 

00:01:28.229 --> 00:01:32.359
couple extra layers to this network so
let's add one layer there and another

00:01:32.359 --> 00:01:32.369
let's add one layer there and another
 

00:01:32.369 --> 00:01:37.730
let's add one layer there and another
layer there and this will output a o+ to

00:01:37.730 --> 00:01:37.740
layer there and this will output a o+ to
 

00:01:37.740 --> 00:01:41.569
layer there and this will output a o+ to
only let's make this a resonant block a

00:01:41.569 --> 00:01:41.579
only let's make this a resonant block a
 

00:01:41.579 --> 00:01:44.630
only let's make this a resonant block a
residual block with that extra shortcut

00:01:44.630 --> 00:01:44.640
residual block with that extra shortcut
 

00:01:44.640 --> 00:01:47.690
residual block with that extra shortcut
and for the sake of argument let's say

00:01:47.690 --> 00:01:47.700
and for the sake of argument let's say
 

00:01:47.700 --> 00:01:50.990
and for the sake of argument let's say
throughout this network we're using the

00:01:50.990 --> 00:01:51.000
throughout this network we're using the
 

00:01:51.000 --> 00:01:53.539
throughout this network we're using the
value activation function so all the

00:01:53.539 --> 00:01:53.549
value activation function so all the
 

00:01:53.549 --> 00:01:56.630
value activation function so all the
activations are going to be you know

00:01:56.630 --> 00:01:56.640
activations are going to be you know
 

00:01:56.640 --> 00:01:58.550
activations are going to be you know
greater than or equal to zero with a

00:01:58.550 --> 00:01:58.560
greater than or equal to zero with a
 

00:01:58.560 --> 00:02:02.420
greater than or equal to zero with a
positive exception of the input X right

00:02:02.420 --> 00:02:02.430
positive exception of the input X right
 

00:02:02.430 --> 00:02:04.850
positive exception of the input X right
because the value activation outputs

00:02:04.850 --> 00:02:04.860
because the value activation outputs
 

00:02:04.860 --> 00:02:07.819
because the value activation outputs
numbers are either zero or positive now

00:02:07.819 --> 00:02:07.829
numbers are either zero or positive now
 

00:02:07.829 --> 00:02:10.309
numbers are either zero or positive now
let's look at what al plus two will be

00:02:10.309 --> 00:02:10.319
let's look at what al plus two will be
 

00:02:10.319 --> 00:02:12.140
let's look at what al plus two will be
so copy the

00:02:12.140 --> 00:02:12.150
so copy the
 

00:02:12.150 --> 00:02:15.589
so copy the
freshen from the previous video al plus

00:02:15.589 --> 00:02:15.599
freshen from the previous video al plus
 

00:02:15.599 --> 00:02:20.809
freshen from the previous video al plus
2 will be rarely applied to Z L plus 2

00:02:20.809 --> 00:02:20.819
2 will be rarely applied to Z L plus 2
 

00:02:20.819 --> 00:02:25.850
2 will be rarely applied to Z L plus 2
and then plus al where this addition of

00:02:25.850 --> 00:02:25.860
and then plus al where this addition of
 

00:02:25.860 --> 00:02:29.270
and then plus al where this addition of
Al comes from the short circuit from the

00:02:29.270 --> 00:02:29.280
Al comes from the short circuit from the
 

00:02:29.280 --> 00:02:31.250
Al comes from the short circuit from the
skipped connection that we just added

00:02:31.250 --> 00:02:31.260
skipped connection that we just added
 

00:02:31.260 --> 00:02:33.949
skipped connection that we just added
and if we expand this out this is equal

00:02:33.949 --> 00:02:33.959
and if we expand this out this is equal
 

00:02:33.959 --> 00:02:40.780
and if we expand this out this is equal
to G of WL plus 2 times a of L plus 1

00:02:40.780 --> 00:02:40.790
to G of WL plus 2 times a of L plus 1
 

00:02:40.790 --> 00:02:45.470
to G of WL plus 2 times a of L plus 1
plus BL plus 2 so that's ZL plus 2 is

00:02:45.470 --> 00:02:45.480
plus BL plus 2 so that's ZL plus 2 is
 

00:02:45.480 --> 00:02:49.339
plus BL plus 2 so that's ZL plus 2 is
equal to that plus al now notice

00:02:49.339 --> 00:02:49.349
equal to that plus al now notice
 

00:02:49.349 --> 00:02:51.530
equal to that plus al now notice
something if you are using l2

00:02:51.530 --> 00:02:51.540
something if you are using l2
 

00:02:51.540 --> 00:02:53.960
something if you are using l2
regularization or weight decay that will

00:02:53.960 --> 00:02:53.970
regularization or weight decay that will
 

00:02:53.970 --> 00:02:58.819
regularization or weight decay that will
tend to shrink the value of WL + 2 um if

00:02:58.819 --> 00:02:58.829
tend to shrink the value of WL + 2 um if
 

00:02:58.829 --> 00:03:00.920
tend to shrink the value of WL + 2 um if
you're applying weight decay to B that

00:03:00.920 --> 00:03:00.930
you're applying weight decay to B that
 

00:03:00.930 --> 00:03:02.780
you're applying weight decay to B that
will also strengthen Saul though I guess

00:03:02.780 --> 00:03:02.790
will also strengthen Saul though I guess
 

00:03:02.790 --> 00:03:04.129
will also strengthen Saul though I guess
in practice sometimes you do and

00:03:04.129 --> 00:03:04.139
in practice sometimes you do and
 

00:03:04.139 --> 00:03:05.960
in practice sometimes you do and
sometimes you don't apply weight to K to

00:03:05.960 --> 00:03:05.970
sometimes you don't apply weight to K to
 

00:03:05.970 --> 00:03:10.490
sometimes you don't apply weight to K to
B but W is really the key term to pay

00:03:10.490 --> 00:03:10.500
B but W is really the key term to pay
 

00:03:10.500 --> 00:03:14.990
B but W is really the key term to pay
attention to here and if WL plus 2 is

00:03:14.990 --> 00:03:15.000
attention to here and if WL plus 2 is
 

00:03:15.000 --> 00:03:17.059
attention to here and if WL plus 2 is
equal to zero and let's say for the sake

00:03:17.059 --> 00:03:17.069
equal to zero and let's say for the sake
 

00:03:17.069 --> 00:03:20.449
equal to zero and let's say for the sake
of argument that B is also equal to zero

00:03:20.449 --> 00:03:20.459
of argument that B is also equal to zero
 

00:03:20.459 --> 00:03:24.409
of argument that B is also equal to zero
then these terms go away because they

00:03:24.409 --> 00:03:24.419
then these terms go away because they
 

00:03:24.419 --> 00:03:29.479
then these terms go away because they
equal to zero and then G of Al this is

00:03:29.479 --> 00:03:29.489
equal to zero and then G of Al this is
 

00:03:29.489 --> 00:03:33.800
equal to zero and then G of Al this is
just equal to Al right because we

00:03:33.800 --> 00:03:33.810
just equal to Al right because we
 

00:03:33.810 --> 00:03:36.259
just equal to Al right because we
assumed we're using the value activation

00:03:36.259 --> 00:03:36.269
assumed we're using the value activation
 

00:03:36.269 --> 00:03:38.210
assumed we're using the value activation
function and so all the activations are

00:03:38.210 --> 00:03:38.220
function and so all the activations are
 

00:03:38.220 --> 00:03:41.330
function and so all the activations are
non-negative and so G of ALS the value

00:03:41.330 --> 00:03:41.340
non-negative and so G of ALS the value
 

00:03:41.340 --> 00:03:44.119
non-negative and so G of ALS the value
applied to a non-negative quantity so

00:03:44.119 --> 00:03:44.129
applied to a non-negative quantity so
 

00:03:44.129 --> 00:03:47.659
applied to a non-negative quantity so
you just get back al so what this shows

00:03:47.659 --> 00:03:47.669
you just get back al so what this shows
 

00:03:47.669 --> 00:03:52.330
you just get back al so what this shows
is that the identity function is easy

00:03:52.330 --> 00:03:52.340
is that the identity function is easy
 

00:03:52.340 --> 00:03:56.689
is that the identity function is easy
for residual block to learn and it's

00:03:56.689 --> 00:03:56.699
for residual block to learn and it's
 

00:03:56.699 --> 00:03:59.599
for residual block to learn and it's
easy to get al plus 2 equal to Al

00:03:59.599 --> 00:03:59.609
easy to get al plus 2 equal to Al
 

00:03:59.609 --> 00:04:03.379
easy to get al plus 2 equal to Al
because of this skipped connection and

00:04:03.379 --> 00:04:03.389
because of this skipped connection and
 

00:04:03.389 --> 00:04:06.740
because of this skipped connection and
what that means is that having these two

00:04:06.740 --> 00:04:06.750
what that means is that having these two
 

00:04:06.750 --> 00:04:08.869
what that means is that having these two
layers in your network it doesn't really

00:04:08.869 --> 00:04:08.879
layers in your network it doesn't really
 

00:04:08.879 --> 00:04:11.420
layers in your network it doesn't really
hurt your neural networks ability to do

00:04:11.420 --> 00:04:11.430
hurt your neural networks ability to do
 

00:04:11.430 --> 00:04:14.930
hurt your neural networks ability to do
as well as this simpler network without

00:04:14.930 --> 00:04:14.940
as well as this simpler network without
 

00:04:14.940 --> 00:04:16.020
as well as this simpler network without
these two extra

00:04:16.020 --> 00:04:16.030
these two extra
 

00:04:16.030 --> 00:04:18.539
these two extra
because it's quite easy for it to learn

00:04:18.539 --> 00:04:18.549
because it's quite easy for it to learn
 

00:04:18.549 --> 00:04:21.060
because it's quite easy for it to learn
the identity function to just copy a L

00:04:21.060 --> 00:04:21.070
the identity function to just copy a L
 

00:04:21.070 --> 00:04:24.330
the identity function to just copy a L
to L plus 2 using despite the addition

00:04:24.330 --> 00:04:24.340
to L plus 2 using despite the addition
 

00:04:24.340 --> 00:04:27.290
to L plus 2 using despite the addition
of these two layers and this is why

00:04:27.290 --> 00:04:27.300
of these two layers and this is why
 

00:04:27.300 --> 00:04:30.750
of these two layers and this is why
adding two extra layers having this

00:04:30.750 --> 00:04:30.760
adding two extra layers having this
 

00:04:30.760 --> 00:04:34.980
adding two extra layers having this
residual block to the somewhere in the

00:04:34.980 --> 00:04:34.990
residual block to the somewhere in the
 

00:04:34.990 --> 00:04:36.900
residual block to the somewhere in the
middle or to the end of this big neural

00:04:36.900 --> 00:04:36.910
middle or to the end of this big neural
 

00:04:36.910 --> 00:04:37.530
middle or to the end of this big neural
network

00:04:37.530 --> 00:04:37.540
network
 

00:04:37.540 --> 00:04:40.920
network
it doesn't hurt performance but of

00:04:40.920 --> 00:04:40.930
it doesn't hurt performance but of
 

00:04:40.930 --> 00:04:43.230
it doesn't hurt performance but of
course our goal is to not just not hurt

00:04:43.230 --> 00:04:43.240
course our goal is to not just not hurt
 

00:04:43.240 --> 00:04:45.150
course our goal is to not just not hurt
performance as the whole performance and

00:04:45.150 --> 00:04:45.160
performance as the whole performance and
 

00:04:45.160 --> 00:04:48.750
performance as the whole performance and
so you can imagine that if all of these

00:04:48.750 --> 00:04:48.760
so you can imagine that if all of these
 

00:04:48.760 --> 00:04:50.430
so you can imagine that if all of these
hidden units if they actually learned

00:04:50.430 --> 00:04:50.440
hidden units if they actually learned
 

00:04:50.440 --> 00:04:52.530
hidden units if they actually learned
something useful then maybe you can do

00:04:52.530 --> 00:04:52.540
something useful then maybe you can do
 

00:04:52.540 --> 00:04:54.510
something useful then maybe you can do
even better than learning the identity

00:04:54.510 --> 00:04:54.520
even better than learning the identity
 

00:04:54.520 --> 00:04:57.810
even better than learning the identity
function and what goes wrong in very

00:04:57.810 --> 00:04:57.820
function and what goes wrong in very
 

00:04:57.820 --> 00:05:00.540
function and what goes wrong in very
deep plane nets in very deep networks

00:05:00.540 --> 00:05:00.550
deep plane nets in very deep networks
 

00:05:00.550 --> 00:05:02.730
deep plane nets in very deep networks
without these message all these skipped

00:05:02.730 --> 00:05:02.740
without these message all these skipped
 

00:05:02.740 --> 00:05:04.860
without these message all these skipped
connections is that when you need make

00:05:04.860 --> 00:05:04.870
connections is that when you need make
 

00:05:04.870 --> 00:05:06.630
connections is that when you need make
the network deeper and deeper is

00:05:06.630 --> 00:05:06.640
the network deeper and deeper is
 

00:05:06.640 --> 00:05:08.640
the network deeper and deeper is
actually very difficult for it to choose

00:05:08.640 --> 00:05:08.650
actually very difficult for it to choose
 

00:05:08.650 --> 00:05:11.850
actually very difficult for it to choose
parameters that learn even the identity

00:05:11.850 --> 00:05:11.860
parameters that learn even the identity
 

00:05:11.860 --> 00:05:14.310
parameters that learn even the identity
function which is why a lot of layers

00:05:14.310 --> 00:05:14.320
function which is why a lot of layers
 

00:05:14.320 --> 00:05:16.920
function which is why a lot of layers
end up making your result worse rather

00:05:16.920 --> 00:05:16.930
end up making your result worse rather
 

00:05:16.930 --> 00:05:19.470
end up making your result worse rather
than making a result better and I think

00:05:19.470 --> 00:05:19.480
than making a result better and I think
 

00:05:19.480 --> 00:05:21.810
than making a result better and I think
the main reason the residual network

00:05:21.810 --> 00:05:21.820
the main reason the residual network
 

00:05:21.820 --> 00:05:24.480
the main reason the residual network
works is that is so easy for these extra

00:05:24.480 --> 00:05:24.490
works is that is so easy for these extra
 

00:05:24.490 --> 00:05:26.340
works is that is so easy for these extra
layers to learn the residual to learn

00:05:26.340 --> 00:05:26.350
layers to learn the residual to learn
 

00:05:26.350 --> 00:05:28.590
layers to learn the residual to learn
the identity function that you have kind

00:05:28.590 --> 00:05:28.600
the identity function that you have kind
 

00:05:28.600 --> 00:05:30.150
the identity function that you have kind
of guaranteed that it doesn't hurt

00:05:30.150 --> 00:05:30.160
of guaranteed that it doesn't hurt
 

00:05:30.160 --> 00:05:32.280
of guaranteed that it doesn't hurt
performance and then you know a lot of

00:05:32.280 --> 00:05:32.290
performance and then you know a lot of
 

00:05:32.290 --> 00:05:34.230
performance and then you know a lot of
time you maybe get lucky and it even

00:05:34.230 --> 00:05:34.240
time you maybe get lucky and it even
 

00:05:34.240 --> 00:05:36.870
time you maybe get lucky and it even
helps performance or this is easier to

00:05:36.870 --> 00:05:36.880
helps performance or this is easier to
 

00:05:36.880 --> 00:05:39.690
helps performance or this is easier to
go from a decent baseline of not hurting

00:05:39.690 --> 00:05:39.700
go from a decent baseline of not hurting
 

00:05:39.700 --> 00:05:41.850
go from a decent baseline of not hurting
performance and then gradient descent

00:05:41.850 --> 00:05:41.860
performance and then gradient descent
 

00:05:41.860 --> 00:05:43.860
performance and then gradient descent
can only improve the solution from there

00:05:43.860 --> 00:05:43.870
can only improve the solution from there
 

00:05:43.870 --> 00:05:46.469
can only improve the solution from there
it's one more detail in the residual

00:05:46.469 --> 00:05:46.479
it's one more detail in the residual
 

00:05:46.479 --> 00:05:48.600
it's one more detail in the residual
Network that's worth discussing which is

00:05:48.600 --> 00:05:48.610
Network that's worth discussing which is
 

00:05:48.610 --> 00:05:50.790
Network that's worth discussing which is
through this addition here we're

00:05:50.790 --> 00:05:50.800
through this addition here we're
 

00:05:50.800 --> 00:05:53.700
through this addition here we're
assuming that ZL plus 2 and al have the

00:05:53.700 --> 00:05:53.710
assuming that ZL plus 2 and al have the
 

00:05:53.710 --> 00:05:56.340
assuming that ZL plus 2 and al have the
same dimension and so what you see in

00:05:56.340 --> 00:05:56.350
same dimension and so what you see in
 

00:05:56.350 --> 00:06:00.000
same dimension and so what you see in
resonance is a lot of use of same

00:06:00.000 --> 00:06:00.010
resonance is a lot of use of same
 

00:06:00.010 --> 00:06:03.000
resonance is a lot of use of same
convolutions so that the dimension of

00:06:03.000 --> 00:06:03.010
convolutions so that the dimension of
 

00:06:03.010 --> 00:06:05.760
convolutions so that the dimension of
this is equal to the dimension I guess

00:06:05.760 --> 00:06:05.770
this is equal to the dimension I guess
 

00:06:05.770 --> 00:06:08.640
this is equal to the dimension I guess
of this layer of the output layer so

00:06:08.640 --> 00:06:08.650
of this layer of the output layer so
 

00:06:08.650 --> 00:06:10.950
of this layer of the output layer so
that you can actually do this short

00:06:10.950 --> 00:06:10.960
that you can actually do this short
 

00:06:10.960 --> 00:06:13.650
that you can actually do this short
circuit connection because the same

00:06:13.650 --> 00:06:13.660
circuit connection because the same
 

00:06:13.660 --> 00:06:15.840
circuit connection because the same
convolution preserves dimensions and so

00:06:15.840 --> 00:06:15.850
convolution preserves dimensions and so
 

00:06:15.850 --> 00:06:17.880
convolution preserves dimensions and so
it makes it easier for you to you know

00:06:17.880 --> 00:06:17.890
it makes it easier for you to you know
 

00:06:17.890 --> 00:06:20.340
it makes it easier for you to you know
carry out to have this a short circuit

00:06:20.340 --> 00:06:20.350
carry out to have this a short circuit
 

00:06:20.350 --> 00:06:23.130
carry out to have this a short circuit
and then carry out this edition of too

00:06:23.130 --> 00:06:23.140
and then carry out this edition of too
 

00:06:23.140 --> 00:06:27.690
and then carry out this edition of too
equal dimension vectors um in case the

00:06:27.690 --> 00:06:27.700
equal dimension vectors um in case the
 

00:06:27.700 --> 00:06:29.190
equal dimension vectors um in case the
input and output have different

00:06:29.190 --> 00:06:29.200
input and output have different
 

00:06:29.200 --> 00:06:31.560
input and output have different
dimensions so for example if this is

00:06:31.560 --> 00:06:31.570
dimensions so for example if this is
 

00:06:31.570 --> 00:06:33.890
dimensions so for example if this is
hundred and twenty eight dimensional and

00:06:33.890 --> 00:06:33.900
hundred and twenty eight dimensional and
 

00:06:33.900 --> 00:06:39.270
hundred and twenty eight dimensional and
Z or therefore al is 256 dimensional as

00:06:39.270 --> 00:06:39.280
Z or therefore al is 256 dimensional as
 

00:06:39.280 --> 00:06:42.420
Z or therefore al is 256 dimensional as
an example what you would do is add an

00:06:42.420 --> 00:06:42.430
an example what you would do is add an
 

00:06:42.430 --> 00:06:46.230
an example what you would do is add an
extra matrix they call that ws over here

00:06:46.230 --> 00:06:46.240
extra matrix they call that ws over here
 

00:06:46.240 --> 00:06:52.370
extra matrix they call that ws over here
and WS in this example would be a 256 by

00:06:52.370 --> 00:06:52.380
and WS in this example would be a 256 by
 

00:06:52.380 --> 00:06:56.850
and WS in this example would be a 256 by
128 dimensional matrix so then WS x al

00:06:56.850 --> 00:06:56.860
128 dimensional matrix so then WS x al
 

00:06:56.860 --> 00:07:00.060
128 dimensional matrix so then WS x al
becomes 256 dimensional and this

00:07:00.060 --> 00:07:00.070
becomes 256 dimensional and this
 

00:07:00.070 --> 00:07:02.940
becomes 256 dimensional and this
addition is now between two to five six

00:07:02.940 --> 00:07:02.950
addition is now between two to five six
 

00:07:02.950 --> 00:07:04.980
addition is now between two to five six
dimensional vectors and a few things you

00:07:04.980 --> 00:07:04.990
dimensional vectors and a few things you
 

00:07:04.990 --> 00:07:06.870
dimensional vectors and a few things you
could do if W s it could be a matrix of

00:07:06.870 --> 00:07:06.880
could do if W s it could be a matrix of
 

00:07:06.880 --> 00:07:09.000
could do if W s it could be a matrix of
parameters to be learned via fixed

00:07:09.000 --> 00:07:09.010
parameters to be learned via fixed
 

00:07:09.010 --> 00:07:11.159
parameters to be learned via fixed
matrix that just implement zero padding

00:07:11.159 --> 00:07:11.169
matrix that just implement zero padding
 

00:07:11.169 --> 00:07:15.600
matrix that just implement zero padding
so it takes al and then zero pads it to

00:07:15.600 --> 00:07:15.610
so it takes al and then zero pads it to
 

00:07:15.610 --> 00:07:18.330
so it takes al and then zero pads it to
be 256 dimensional and either of those

00:07:18.330 --> 00:07:18.340
be 256 dimensional and either of those
 

00:07:18.340 --> 00:07:20.760
be 256 dimensional and either of those
versions I guess could work so finally

00:07:20.760 --> 00:07:20.770
versions I guess could work so finally
 

00:07:20.770 --> 00:07:23.130
versions I guess could work so finally
let's take a look at resonance on images

00:07:23.130 --> 00:07:23.140
let's take a look at resonance on images
 

00:07:23.140 --> 00:07:25.469
let's take a look at resonance on images
so these are images I got from the paper

00:07:25.469 --> 00:07:25.479
so these are images I got from the paper
 

00:07:25.479 --> 00:07:29.550
so these are images I got from the paper
by her at all this is an example of a

00:07:29.550 --> 00:07:29.560
by her at all this is an example of a
 

00:07:29.560 --> 00:07:34.500
by her at all this is an example of a
plane Network and in which you input an

00:07:34.500 --> 00:07:34.510
plane Network and in which you input an
 

00:07:34.510 --> 00:07:37.310
plane Network and in which you input an
image and then have a number of

00:07:37.310 --> 00:07:37.320
image and then have a number of
 

00:07:37.320 --> 00:07:41.520
image and then have a number of
conflicts until eventually you have a

00:07:41.520 --> 00:07:41.530
conflicts until eventually you have a
 

00:07:41.530 --> 00:07:45.120
conflicts until eventually you have a
soft max output at the end to turn this

00:07:45.120 --> 00:07:45.130
soft max output at the end to turn this
 

00:07:45.130 --> 00:07:48.659
soft max output at the end to turn this
into a ResNet you add those extra

00:07:48.659 --> 00:07:48.669
into a ResNet you add those extra
 

00:07:48.669 --> 00:07:52.020
into a ResNet you add those extra
skipped connections and I just mention a

00:07:52.020 --> 00:07:52.030
skipped connections and I just mention a
 

00:07:52.030 --> 00:07:54.810
skipped connections and I just mention a
few details there are a lot of 3x3

00:07:54.810 --> 00:07:54.820
few details there are a lot of 3x3
 

00:07:54.820 --> 00:07:57.420
few details there are a lot of 3x3
convolutions here and all these most of

00:07:57.420 --> 00:07:57.430
convolutions here and all these most of
 

00:07:57.430 --> 00:08:01.310
convolutions here and all these most of
these are 3x3 same convolutions and

00:08:01.310 --> 00:08:01.320
these are 3x3 same convolutions and
 

00:08:01.320 --> 00:08:04.320
these are 3x3 same convolutions and
that's why you're adding your equal

00:08:04.320 --> 00:08:04.330
that's why you're adding your equal
 

00:08:04.330 --> 00:08:06.659
that's why you're adding your equal
dimension feature vectors so rather than

00:08:06.659 --> 00:08:06.669
dimension feature vectors so rather than
 

00:08:06.669 --> 00:08:08.700
dimension feature vectors so rather than
they fully commit to layer these are

00:08:08.700 --> 00:08:08.710
they fully commit to layer these are
 

00:08:08.710 --> 00:08:10.140
they fully commit to layer these are
actually convolutional layers but

00:08:10.140 --> 00:08:10.150
actually convolutional layers but
 

00:08:10.150 --> 00:08:12.210
actually convolutional layers but
because there are same convolutions the

00:08:12.210 --> 00:08:12.220
because there are same convolutions the
 

00:08:12.220 --> 00:08:15.659
because there are same convolutions the
dimension is preserved and so the ZL

00:08:15.659 --> 00:08:15.669
dimension is preserved and so the ZL
 

00:08:15.669 --> 00:08:20.700
dimension is preserved and so the ZL
plus 2 plus al right edition makes sense

00:08:20.700 --> 00:08:20.710
plus 2 plus al right edition makes sense
 

00:08:20.710 --> 00:08:23.820
plus 2 plus al right edition makes sense
and similar to what you've seen in a lot

00:08:23.820 --> 00:08:23.830
and similar to what you've seen in a lot
 

00:08:23.830 --> 00:08:26.670
and similar to what you've seen in a lot
of networks before you have a bunch of

00:08:26.670 --> 00:08:26.680
of networks before you have a bunch of
 

00:08:26.680 --> 00:08:29.399
of networks before you have a bunch of
convolutional layers and then in there

00:08:29.399 --> 00:08:29.409
convolutional layers and then in there
 

00:08:29.409 --> 00:08:30.300
convolutional layers and then in there
occasionally

00:08:30.300 --> 00:08:30.310
occasionally
 

00:08:30.310 --> 00:08:32.969
occasionally
pulling layers as well or pulling or

00:08:32.969 --> 00:08:32.979
pulling layers as well or pulling or
 

00:08:32.979 --> 00:08:34.350
pulling layers as well or pulling or
pulling like layers

00:08:34.350 --> 00:08:34.360
pulling like layers
 

00:08:34.360 --> 00:08:36.059
pulling like layers
and whenever one of those things happen

00:08:36.059 --> 00:08:36.069
and whenever one of those things happen
 

00:08:36.069 --> 00:08:37.649
and whenever one of those things happen
then you need to make an adjustment to

00:08:37.649 --> 00:08:37.659
then you need to make an adjustment to
 

00:08:37.659 --> 00:08:40.799
then you need to make an adjustment to
the dimension which we saw on the

00:08:40.799 --> 00:08:40.809
the dimension which we saw on the
 

00:08:40.809 --> 00:08:43.499
the dimension which we saw on the
previous slide you can do of the matrix

00:08:43.499 --> 00:08:43.509
previous slide you can do of the matrix
 

00:08:43.509 --> 00:08:46.289
previous slide you can do of the matrix
W s and then as is common in these

00:08:46.289 --> 00:08:46.299
W s and then as is common in these
 

00:08:46.299 --> 00:08:48.269
W s and then as is common in these
networks you have cough cough cough or

00:08:48.269 --> 00:08:48.279
networks you have cough cough cough or
 

00:08:48.279 --> 00:08:50.400
networks you have cough cough cough or
come from harmful come from Kanpur and

00:08:50.400 --> 00:08:50.410
come from harmful come from Kanpur and
 

00:08:50.410 --> 00:08:52.499
come from harmful come from Kanpur and
then at the end you know have a fully

00:08:52.499 --> 00:08:52.509
then at the end you know have a fully
 

00:08:52.509 --> 00:08:54.629
then at the end you know have a fully
connected layer that then makes a

00:08:54.629 --> 00:08:54.639
connected layer that then makes a
 

00:08:54.639 --> 00:08:57.689
connected layer that then makes a
prediction using a soft max so that's it

00:08:57.689 --> 00:08:57.699
prediction using a soft max so that's it
 

00:08:57.699 --> 00:08:59.789
prediction using a soft max so that's it
for resonance of Nyx

00:08:59.789 --> 00:08:59.799
for resonance of Nyx
 

00:08:59.799 --> 00:09:02.309
for resonance of Nyx
there's a very interesting idea behind

00:09:02.309 --> 00:09:02.319
there's a very interesting idea behind
 

00:09:02.319 --> 00:09:05.280
there's a very interesting idea behind
using neural networks with one by one

00:09:05.280 --> 00:09:05.290
using neural networks with one by one
 

00:09:05.290 --> 00:09:07.919
using neural networks with one by one
filters one by one convolutions so what

00:09:07.919 --> 00:09:07.929
filters one by one convolutions so what
 

00:09:07.929 --> 00:09:10.499
filters one by one convolutions so what
good is a one by one convolution let's

00:09:10.499 --> 00:09:10.509
good is a one by one convolution let's
 

00:09:10.509 --> 00:09:13.709
good is a one by one convolution let's
take a look at the next video

