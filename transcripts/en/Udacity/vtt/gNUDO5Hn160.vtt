WEBVTT
Kind: captions
Language: en

00:00:00.530 --> 00:00:07.862
The next myth is regarding memory effects. And that myth concerns the assertion

00:00:07.862 --> 00:00:13.210
that the lost of locality in a micro-kernel base design, is much more than in a

00:00:13.210 --> 00:00:16.530
monolithic structure. Or the structure that is

00:00:16.530 --> 00:00:19.640
advocated in spin and exo-kernel. But before we

00:00:19.640 --> 00:00:21.520
talk about the memory effects. There's a

00:00:21.520 --> 00:00:25.740
primer on the memory hierarchy. You know that,

00:00:25.740 --> 00:00:27.714
in the process of architecture, you have the

00:00:27.714 --> 00:00:30.600
CPU, you have the TLB. And you have several

00:00:30.600 --> 00:00:34.360
levels of caches, main memory. And the virtual memory

00:00:34.360 --> 00:00:37.810
that resides on the disk. And these caches are

00:00:37.810 --> 00:00:41.230
typically physically tagged. Now, what do we mean by

00:00:41.230 --> 00:00:44.450
memory effects? What we mean by that is, if

00:00:44.450 --> 00:00:47.150
you have this hardware address space, and this of

00:00:47.150 --> 00:00:50.940
course is much bigger than the amount of space

00:00:50.940 --> 00:00:54.540
that's available in these caches. And in

00:00:54.540 --> 00:00:58.610
fact, we know that the entire hardware address

00:00:58.610 --> 00:01:00.600
space may not even be in physical

00:01:00.600 --> 00:01:03.850
memory. Because In a demand-based system, when a

00:01:03.850 --> 00:01:07.460
process is executing, the pages that it

00:01:07.460 --> 00:01:11.460
needs will be demand paged from the virtual

00:01:11.460 --> 00:01:13.430
memory that's on the disk, and brought

00:01:13.430 --> 00:01:16.060
into physical memory. And when the processor is

00:01:16.060 --> 00:01:19.460
executing instructions, then the instructions and data

00:01:19.460 --> 00:01:22.000
contained in physical memory move into the memory

00:01:22.000 --> 00:01:28.990
hierarchy close to. The CPU, so that the CPU can access the working set of

00:01:28.990 --> 00:01:35.440
the currently running thread by getting it from the cache that is closest to it.

00:01:35.440 --> 00:01:38.360
That's the hope in this memory hierarchy. What

00:01:38.360 --> 00:01:41.188
we mean by memory effects is when we

00:01:41.188 --> 00:01:45.430
context switch between protection domains, how warm

00:01:45.430 --> 00:01:48.350
are the caches? Now recall that if

00:01:48.350 --> 00:01:54.480
we have small protection domains. P1, P2, P3, P4. Let's say they are all small

00:01:54.480 --> 00:01:57.800
protection domains. In that case, led case

00:01:57.800 --> 00:02:01.010
suggestion is, don't put each of these in

00:02:01.010 --> 00:02:03.560
its own hardware address space. Pack them

00:02:03.560 --> 00:02:06.320
together in the same hardware address space and

00:02:06.320 --> 00:02:10.199
then force protection, for these processes from

00:02:10.199 --> 00:02:14.410
one another, through segment registers. So when we

00:02:14.410 --> 00:02:18.650
have small protection domains, then, the caches

00:02:18.650 --> 00:02:20.640
are going to be pretty warm. Even when you

00:02:20.640 --> 00:02:23.030
do a context switch from one process

00:02:23.030 --> 00:02:25.670
to another process. There's a good chance, that

00:02:25.670 --> 00:02:28.860
the cache hierarchy is going to contain the working

00:02:28.860 --> 00:02:32.230
set of the newly scheduled small protection domain.

00:02:32.230 --> 00:02:38.710
So in other words, the memory effects can be mitigated by carefully structuring

00:02:38.710 --> 00:02:41.160
the protection domains, in the hardware

00:02:41.160 --> 00:02:44.810
address space. So debunking the second myth

00:02:44.810 --> 00:02:50.980
with respect to address space switching, also helps in reducing the ill effects

00:02:50.980 --> 00:02:54.840
of implicit costs, associated with address

00:02:54.840 --> 00:02:58.400
space switching, because these small protection domains.

00:02:58.400 --> 00:03:01.490
Occupy only a small memory footprint, and

00:03:01.490 --> 00:03:04.560
therefore occupy only a small memory footprint in

00:03:04.560 --> 00:03:07.050
the caches as well. And therefore, when we

00:03:07.050 --> 00:03:10.270
go across small protection domains, there's a good

00:03:10.270 --> 00:03:14.280
chance that, the locality for the newly

00:03:14.280 --> 00:03:17.980
scheduled small protection domain is going to be contained

00:03:17.980 --> 00:03:23.490
in the cache hierarchy. So the caches are probably going to be warm, when we do

00:03:23.490 --> 00:03:28.460
context switches, so long as the protection domains are small. We already

00:03:28.460 --> 00:03:31.730
mentioned that if the protection domains

00:03:31.730 --> 00:03:35.680
are large, you cannot avoid that. Whether

00:03:35.680 --> 00:03:42.100
it is a monolithic kernel or. The exokernel, or the spin type

00:03:42.100 --> 00:03:48.730
of extensibility. If the memory footprint of the system service is so big,

00:03:48.730 --> 00:03:50.850
it is going to pollute the cache when

00:03:50.850 --> 00:03:53.820
we visit that particular system service. So

00:03:53.820 --> 00:03:56.440
even if we have a monolithic kernel,

00:03:56.440 --> 00:04:01.260
and the monolithic kernel has subsystems, that occupy

00:04:01.260 --> 00:04:03.960
a significant portion of the hardware address

00:04:03.960 --> 00:04:06.810
space. Even though we are not doing any

00:04:06.810 --> 00:04:10.380
context for it, the ill effects of

00:04:10.380 --> 00:04:14.101
implicit costs in the memory hierarchy is going to

00:04:14.101 --> 00:04:16.100
be felt. Because the cache is after

00:04:16.100 --> 00:04:19.829
all a physically tagged. And therefore, when we

00:04:19.829 --> 00:04:24.190
go from large protection domains, or large subsystems

00:04:24.190 --> 00:04:26.830
in the context of a monolithic kernel. You

00:04:26.830 --> 00:04:30.820
have to incur the cache pollution. So that

00:04:30.820 --> 00:04:33.690
ill effect is unavoidable for large protection domains.

00:04:33.690 --> 00:04:40.110
So the only place where a monolithic kernel can win, or an exokernel can win, or

00:04:40.110 --> 00:04:46.160
a spin can win, is in small protection domains. And a microkernel can also

00:04:46.160 --> 00:04:48.870
win for a small protection domain, by

00:04:48.870 --> 00:04:52.140
packing multiple small protection domains in the

00:04:52.140 --> 00:04:58.085
same hardware address space. So this begs the question. Why was it so bad in

00:04:58.085 --> 00:05:05.400
Mac? Recall, I mentioned that, in Mac, on the same hardware, border crossing

00:05:05.400 --> 00:05:12.840
cost 800 more cycles, compared to the border crossing in the L3 microkernel.

00:05:12.840 --> 00:05:17.777
So we can ask the question, why was it so bad in the Mac microkernel?

