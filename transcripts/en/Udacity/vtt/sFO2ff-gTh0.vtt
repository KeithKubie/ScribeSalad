WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.310
Alright, so let me try to get to this concept

00:00:03.310 --> 00:00:06.610
of cross validation. So, imagine that we've got our data,

00:00:06.610 --> 00:00:09.960
this is our training set. We can, again, picture geometrically

00:00:09.960 --> 00:00:13.480
in the case of regression. And, ultimately what we're trying

00:00:13.480 --> 00:00:15.840
to do is find a way of predicting values and

00:00:15.840 --> 00:00:20.500
then testing them. So, what we imagine is we do

00:00:20.500 --> 00:00:22.310
some kind of regression and we might want to fit

00:00:22.310 --> 00:00:25.570
this too a line. And, you know, the line is

00:00:25.570 --> 00:00:27.590
good, it kind of captures what's going on and if we

00:00:27.590 --> 00:00:30.320
apply this to the testing set, maybe it's going to do

00:00:30.320 --> 00:00:35.170
a pretty good job. But, if we are, you know, feeling

00:00:35.170 --> 00:00:39.220
kind of obsessive compulsive about it we might say well in

00:00:39.220 --> 00:00:41.460
this particular case we didn't actually track all the ups and

00:00:41.460 --> 00:00:45.180
downs of the data. So what can we do in terms

00:00:45.180 --> 00:00:47.440
of if we, if we fit it with the line and

00:00:47.440 --> 00:00:51.120
the errors not so great. What else could we switch to Charles?

00:00:51.120 --> 00:00:54.250
&gt;&gt; We could just use the test. No, sorry. What, what I mean

00:00:54.250 --> 00:00:58.500
is if we fit, we fit this to a line and we're sort

00:00:58.500 --> 00:01:01.060
of not happy with the fact that the line isn't fitting all of

00:01:01.060 --> 00:01:05.950
the points exactly. We might want to use ,uh, maybe a higher order polynomial.

00:01:05.950 --> 00:01:07.280
&gt;&gt; Oh, I'm sorry, totally misunderstood you.

00:01:07.280 --> 00:01:11.390
&gt;&gt; To fit this better. So if we, we can fit this with a higher

00:01:11.390 --> 00:01:14.180
order polynomial and maybe it'll hit, all these

00:01:14.180 --> 00:01:16.240
points much better. You know, so we have

00:01:16.240 --> 00:01:18.590
this kind of, kind of other shape, and now it's doing this, it's

00:01:18.590 --> 00:01:23.720
making weird predictions in certain places. So, really what we'd like to

00:01:23.720 --> 00:01:25.940
do is, and what was your suggestion? If we trained on the

00:01:25.940 --> 00:01:29.650
test set, we would do much better on the test set, wouldn't we?

00:01:29.650 --> 00:01:29.830
&gt;&gt; Yes.

00:01:29.830 --> 00:01:31.970
&gt;&gt; But that, that, that's definitely cheating.

00:01:31.970 --> 00:01:32.740
&gt;&gt; Why is cheating?

00:01:32.740 --> 00:01:37.320
&gt;&gt; Is there some, why is it cheating? Well, if we

00:01:37.320 --> 00:01:41.540
exactly fit the error, the, the test set. That's not a function

00:01:41.540 --> 00:01:47.310
at all, is it? [LAUGH] If we exactly fit the, the test set, then

00:01:47.310 --> 00:01:50.920
again that's not going to generalize to how we use it in the real world.

00:01:50.920 --> 00:01:55.470
&gt;&gt; So the goal is always to generalize. The test set is just

00:01:55.470 --> 00:02:00.260
a stand-in For ,what we don't know we're going to see in the future.

00:02:00.260 --> 00:02:02.150
&gt;&gt; Yes, very well said. Thank you.

00:02:02.150 --> 00:02:03.470
&gt;&gt; Actually that suggests something very

00:02:03.470 --> 00:02:06.980
important, right, it suggest that ,um,

00:02:06.980 --> 00:02:11.774
nothing we do, on our training set or even if we cheat and use the test set

00:02:11.774 --> 00:02:15.970
.Actually makes sense unless we believe that somehow the

00:02:15.970 --> 00:02:19.350
training set and the test set represent the future.

00:02:19.350 --> 00:02:21.520
&gt;&gt; Yes, that's a very good point, that

00:02:21.520 --> 00:02:24.310
we are assuming that this data is representative of

00:02:24.310 --> 00:02:27.360
how the system is ultimately going to be used. In

00:02:27.360 --> 00:02:32.600
fact, there's an abbreviation that statisticians like to use.

00:02:32.600 --> 00:02:35.230
That the data, we really count on

00:02:35.230 --> 00:02:38.900
the data being independent and identically distributed,

00:02:38.900 --> 00:02:39.465
&gt;&gt; Mm-hm.

00:02:39.465 --> 00:02:41.650
&gt;&gt; which is to say that all the data that we have collected,

00:02:41.650 --> 00:02:44.590
it's all really coming from the same source, so there is no, no

00:02:44.590 --> 00:02:47.440
sort of weirdness that the training set looks different from testing set looks

00:02:47.440 --> 00:02:50.470
different from the world but they are all drawn from the same distribution.

00:02:51.620 --> 00:02:53.960
&gt;&gt; So would you call that a fundamental assumption of supervised learning?

00:02:53.960 --> 00:02:58.100
&gt;&gt; I don't know that I'd call it a fundamental of supervised learning per se,

00:02:58.100 --> 00:02:59.600
but it's a fundamental assumption in a lot

00:02:59.600 --> 00:03:01.230
of the algorithms that we run, that's for sure.

00:03:01.230 --> 00:03:01.490
&gt;&gt; Fair enough.

00:03:01.490 --> 00:03:03.850
&gt;&gt; There's definitely people who have looked at, well

00:03:03.850 --> 00:03:06.660
what happens in real data if these assumptions are violated?

00:03:06.660 --> 00:03:08.520
Are there algorithms that we can apply that still do

00:03:08.520 --> 00:03:11.160
reasonable things? But the stuff that we're talking about? Yes,

00:03:11.160 --> 00:03:15.880
this is absolutely. A fundamental assumption. Alright, but here's,

00:03:15.880 --> 00:03:18.910
here's where I'm trying to get with this stuff. So

00:03:18.910 --> 00:03:21.360
what we really would like to do, is that we'd

00:03:21.360 --> 00:03:23.330
like to use a model that's complex enough to actually

00:03:23.330 --> 00:03:25.950
model the structure that's in the data that we're training on,

00:03:25.950 --> 00:03:29.110
but no so complex that it's, it's matching that so directly that

00:03:29.110 --> 00:03:31.850
it doesn't really work well on the test set. But unfortunately we

00:03:31.850 --> 00:03:35.320
don't really have the test set to play with because that again,

00:03:35.320 --> 00:03:38.450
is going to, it's too much teaching to the test. We need

00:03:38.450 --> 00:03:41.690
to actually learn the true structure that is going to need to

00:03:41.690 --> 00:03:45.570
be generalized. So, so how do we find out. How can we,

00:03:45.570 --> 00:03:48.470
how can we pick a model that is complex enough to model

00:03:48.470 --> 00:03:52.110
the data while making sure that it hasn't started to kind of diverege in

00:03:52.110 --> 00:03:55.940
terms of how it's going to be applied to the test set. If we don't

00:03:55.940 --> 00:03:58.890
have access to the test set, is there something that we can use in

00:03:58.890 --> 00:04:03.200
the training set that we could have it kind of act like a test set?

00:04:03.200 --> 00:04:08.720
&gt;&gt; Well, we could take some of the training

00:04:08.720 --> 00:04:13.570
data and pretend its a test set and that wouldn't be cheating because its

00:04:13.570 --> 00:04:15.230
not really the test set.

00:04:15.230 --> 00:04:18.760
&gt;&gt; Excellent. Indeed, right, so there's nothing magic

00:04:18.760 --> 00:04:22.620
about the training set all needing to be

00:04:22.620 --> 00:04:28.760
used to fit the coefficient. It could be that we hold out some of it ,as

00:04:28.760 --> 00:04:33.350
a kind of make pretend test set, a test test set, a trial test set, a

00:04:33.350 --> 00:04:35.119
what we're going to say cross validation set.

00:04:36.220 --> 00:04:38.700
And it's going to be a stand in for the

00:04:38.700 --> 00:04:43.140
actual test data. That we can actually, make

00:04:43.140 --> 00:04:46.160
use of that doesn't involve actually using the test

00:04:46.160 --> 00:04:48.050
data directly which is ultimately going to be

00:04:48.050 --> 00:04:51.430
cheating. So, this cross validation set is going to

00:04:51.430 --> 00:04:53.160
be really helpful in figuring out what to

00:04:53.160 --> 00:04:56.610
do. So. Alright, so here's how we're going to do

00:04:56.610 --> 00:04:59.280
this, this concept of cross validation. We're going to take

00:04:59.280 --> 00:05:04.390
our training data, and we're going to split it into

00:05:04.390 --> 00:05:06.890
what are called folds. I'm not actually sure why they're

00:05:06.890 --> 00:05:09.700
called folds. I don't know if that's a sheep reference.

00:05:09.700 --> 00:05:12.640
&gt;&gt; Why would it be a sheep reference?

00:05:12.640 --> 00:05:16.060
&gt;&gt; I think there's a sheep-related concept that is called a

00:05:16.060 --> 00:05:19.470
fold. Like, You know, we're going to bring you back into the fold.

00:05:19.470 --> 00:05:19.810
&gt;&gt; Oh.

00:05:19.810 --> 00:05:21.790
&gt;&gt; It's like the, it's like the group of sheep.

00:05:21.790 --> 00:05:24.900
&gt;&gt; You are just trunk full of knowledge.

00:05:24.900 --> 00:05:27.080
&gt;&gt; Alright so what we're going to do is train

00:05:27.080 --> 00:05:29.600
on the first three folds, and use the fourth one

00:05:29.600 --> 00:05:33.140
to, to see how we did. Train on the [LAUGH]

00:05:33.140 --> 00:05:35.420
second there and fourth fold and check on the first

00:05:35.420 --> 00:05:37.430
one. And we're going to we're going to try all these different

00:05:37.430 --> 00:05:40.550
combinations leaving out each fold as a kind of a,

00:05:40.550 --> 00:05:43.965
a fake test set. And then average these errors. The

00:05:43.965 --> 00:05:47.850
,uh, the, the goodness of fit. Average them all together,

00:05:47.850 --> 00:05:52.270
to see how well we've done. And, the model class,

00:05:52.270 --> 00:05:55.140
so like the degree of the polynomial in this case that

00:05:55.140 --> 00:05:58.330
does the best job, the lowest error, is the

00:05:58.330 --> 00:06:01.070
one that we're going to go with. Alright, so if

00:06:01.070 --> 00:06:02.720
this is a little bit abstract still let me,

00:06:02.720 --> 00:06:05.590
let me ground this back out in the housing example.

