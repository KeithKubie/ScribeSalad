WEBVTT
Kind: captions
Language: en

00:00:01.685 --> 00:00:04.040
The phone and content is
provided under a Creative

00:00:04.040 --> 00:00:05.580
Commons license.

00:00:05.580 --> 00:00:07.880
Your support will help
MIT OpenCourseWare

00:00:07.880 --> 00:00:12.270
continue to offer high quality
educational resources for free.

00:00:12.270 --> 00:00:14.870
To make a donation or
view additional materials

00:00:14.870 --> 00:00:18.830
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.830 --> 00:00:20.000
at ocw.mit.edu.

00:00:21.881 --> 00:00:23.630
ETHAN MEYERS: What I'm
talking about today

00:00:23.630 --> 00:00:26.180
is neural population
decoding, which

00:00:26.180 --> 00:00:28.790
is very similar to what
Rebecca was talking about,

00:00:28.790 --> 00:00:32.330
except for I'm talking about now
more on the single neuron level

00:00:32.330 --> 00:00:35.810
and also talk a bit about
some MEG at the end.

00:00:35.810 --> 00:00:39.740
But kind of to tie it to what
was previously discussed,

00:00:39.740 --> 00:00:43.130
Rebecca talked a lot about, at
the end, the big catastrophe.

00:00:43.130 --> 00:00:45.350
Well, you don't know
if something is not

00:00:45.350 --> 00:00:49.340
there in the fMRI signal because
things could be masked when

00:00:49.340 --> 00:00:51.470
you're averaging
over a large region

00:00:51.470 --> 00:00:54.860
as you do when you're recording
from those bold signals.

00:00:54.860 --> 00:00:58.790
And when you're doing
decoding on single neurons,

00:00:58.790 --> 00:01:00.920
that is not really an issue
because you're actually

00:01:00.920 --> 00:01:03.420
going down and recording
those individual neurons.

00:01:03.420 --> 00:01:07.040
And so while in general
in hypothesis testing

00:01:07.040 --> 00:01:10.240
you can never really say
something doesn't exist,

00:01:10.240 --> 00:01:12.830
here you can feel fairly
confident that it probably

00:01:12.830 --> 00:01:14.460
doesn't, unless you--

00:01:14.460 --> 00:01:18.320
I mean, you could do
a Bayesian analysis.

00:01:18.320 --> 00:01:20.170
Anyway, all right.

00:01:20.170 --> 00:01:23.900
So kind of the very basic
motivation behind what I do

00:01:23.900 --> 00:01:28.400
is, you know, I'm interested in
all the questions the CBMM is

00:01:28.400 --> 00:01:33.050
interested in, how can we
algorithmically solve problems

00:01:33.050 --> 00:01:34.880
and perform behaviors.

00:01:34.880 --> 00:01:38.150
And so, you know,
basically as motivation,

00:01:38.150 --> 00:01:42.800
you know, as a theoretician,
we might have some great idea

00:01:42.800 --> 00:01:44.400
about how the brain works.

00:01:44.400 --> 00:01:47.450
And so what we do is we
come up with an experiment

00:01:47.450 --> 00:01:48.180
and we run it.

00:01:48.180 --> 00:01:50.382
And we record a
bunch of neural data.

00:01:50.382 --> 00:01:52.340
And then at the end of
it, what we're left with

00:01:52.340 --> 00:01:54.320
is just a bunch of data.

00:01:54.320 --> 00:01:56.820
It's not really an
answer to our question.

00:01:56.820 --> 00:02:00.012
So for example, if
you recorded spikes,

00:02:00.012 --> 00:02:01.970
you might end up with
something called a raster

00:02:01.970 --> 00:02:03.510
where you have trials and time.

00:02:03.510 --> 00:02:07.040
And you just end up with
little indications at what

00:02:07.040 --> 00:02:09.530
times did a neuron spike.

00:02:09.530 --> 00:02:11.480
Or if you did an
MEG experiment, you

00:02:11.480 --> 00:02:14.450
might end up with a bunch
of kind of waveforms

00:02:14.450 --> 00:02:16.550
that are kind of noisy.

00:02:16.550 --> 00:02:19.740
And so this is a
good first step,

00:02:19.740 --> 00:02:21.410
but obviously what
you need to do

00:02:21.410 --> 00:02:24.407
is take this and turn it
into some sort of answer

00:02:24.407 --> 00:02:25.115
to your question.

00:02:27.299 --> 00:02:29.090
Because if you can't
turn it into an answer

00:02:29.090 --> 00:02:30.556
to your question,
there is no point

00:02:30.556 --> 00:02:32.180
in doing that experiment
to begin with.

00:02:34.880 --> 00:02:37.430
So basically, what
I'm looking for is

00:02:37.430 --> 00:02:39.560
clear answers to questions.

00:02:39.560 --> 00:02:41.760
In particular I'm
interested in two things.

00:02:41.760 --> 00:02:43.730
One is neural content.

00:02:43.730 --> 00:02:46.610
And that is what information
is in a particular region

00:02:46.610 --> 00:02:49.029
of the brain, and at what time.

00:02:49.029 --> 00:02:50.570
And the other thing
I'm interested in

00:02:50.570 --> 00:02:55.130
is neural coding, or what
features of the neural activity

00:02:55.130 --> 00:02:58.980
contain that information.

00:02:58.980 --> 00:03:01.550
And so the idea is, basically,
if we can make recordings

00:03:01.550 --> 00:03:03.530
from a number of
different brain regions

00:03:03.530 --> 00:03:07.580
and tell what content was
in different parts, then

00:03:07.580 --> 00:03:10.520
we could, basically,
trace the information flow

00:03:10.520 --> 00:03:12.500
through the brain
and try to unravel

00:03:12.500 --> 00:03:15.997
the algorithms that enable us
to perform particular tasks.

00:03:15.997 --> 00:03:18.080
And then if we can do that,
we can do other things

00:03:18.080 --> 00:03:20.660
that the CBMM likes
to do, such as build

00:03:20.660 --> 00:03:24.920
helpful robots that will
either bring us drinks

00:03:24.920 --> 00:03:25.940
or create peace.

00:03:28.580 --> 00:03:30.770
So the outline
for the talk today

00:03:30.770 --> 00:03:34.256
is I'm going to talk about what
neural population decoding is.

00:03:34.256 --> 00:03:35.630
I'm going to show
you how you can

00:03:35.630 --> 00:03:38.960
use it to get at neural
content, so what information

00:03:38.960 --> 00:03:40.370
is in brain regions.

00:03:40.370 --> 00:03:42.911
Then I'm going to show how you
can use it to answer questions

00:03:42.911 --> 00:03:46.070
about neural coding, or how do
neurons contain information.

00:03:46.070 --> 00:03:48.930
And then I'm going to show you
a little bit how you can use

00:03:48.930 --> 00:03:50.130
it to analyze your own data.

00:03:50.130 --> 00:03:53.000
So very briefly, a
toolbox I created

00:03:53.000 --> 00:03:54.995
that makes it easy
to do these analyses.

00:03:57.970 --> 00:04:02.950
All right, so the basic
idea behind neural decoding

00:04:02.950 --> 00:04:04.630
is that what you
want to do is you

00:04:04.630 --> 00:04:07.300
want to take neural activity
and try to predict something

00:04:07.300 --> 00:04:09.160
about the stimulus
itself or about,

00:04:09.160 --> 00:04:11.150
let's say, an animal's behavior.

00:04:11.150 --> 00:04:14.560
So it's a function that
goes from neural activity

00:04:14.560 --> 00:04:15.430
to a stimulus.

00:04:19.430 --> 00:04:22.760
And decoding approaches
have been used for maybe

00:04:22.760 --> 00:04:24.130
about 30 years.

00:04:24.130 --> 00:04:27.110
So Rebecca was saying
MBPA goes back to 2001.

00:04:27.110 --> 00:04:29.160
Well, this goes
back much further.

00:04:29.160 --> 00:04:33.530
So in 1986, Georgopoulos
did some studies

00:04:33.530 --> 00:04:35.800
with monkeys showing
that he could

00:04:35.800 --> 00:04:38.540
decode where a monkey
was moving its arm based

00:04:38.540 --> 00:04:40.130
on neural activity.

00:04:40.130 --> 00:04:43.340
And there was other
studies in '93

00:04:43.340 --> 00:04:46.230
by Matt Wilson and McNaughton.

00:04:46.230 --> 00:04:49.040
Matt gave a talk here,
I think, as well.

00:04:49.040 --> 00:04:50.510
And what he tried
to do is decode

00:04:50.510 --> 00:04:53.180
where a rat is in a maze.

00:04:53.180 --> 00:04:55.160
So again, recording
from the hippocampus,

00:04:55.160 --> 00:04:57.440
trying to tell
where that rat is.

00:04:57.440 --> 00:05:02.120
And there's also been a large
amount of computational work,

00:05:02.120 --> 00:05:06.530
such as work by Selinas and
Larry Abbott, kind of comparing

00:05:06.530 --> 00:05:09.350
different decoding methods.

00:05:09.350 --> 00:05:12.200
But despite all of this work,
it's still not widely used.

00:05:12.200 --> 00:05:15.910
So Rebecca was saying that
MVPA has really taken off.

00:05:15.910 --> 00:05:17.870
Well, I'm still waiting
for population decoding

00:05:17.870 --> 00:05:19.389
in neural activity to take off.

00:05:19.389 --> 00:05:20.930
And so part of me
being up here today

00:05:20.930 --> 00:05:22.442
is to say you really
should do this.

00:05:22.442 --> 00:05:23.150
It's really good.

00:05:25.670 --> 00:05:28.160
And just a few other
names for decoding

00:05:28.160 --> 00:05:31.250
is MVPA, multi variant
pattern analysis.

00:05:31.250 --> 00:05:34.280
This is the terminology that
people in the fMRI community

00:05:34.280 --> 00:05:36.170
use and what Rebecca was using.

00:05:36.170 --> 00:05:37.781
It's also called read out.

00:05:37.781 --> 00:05:39.530
So if you've heard
those terms, it kind of

00:05:39.530 --> 00:05:40.571
refers to the same thing.

00:05:43.770 --> 00:05:47.090
All right, so let
me show you what

00:05:47.090 --> 00:05:51.390
decoding looks like in terms of
an experiment with, let's say,

00:05:51.390 --> 00:05:52.910
a monkey.

00:05:52.910 --> 00:05:55.700
So here we'd have an experiment
where we're showing the monkey

00:05:55.700 --> 00:05:58.220
different images on a screen.

00:05:58.220 --> 00:06:00.744
And so for example, we could
show it a picture of a kiwi.

00:06:00.744 --> 00:06:02.660
And then we'd be making
some neural recordings

00:06:02.660 --> 00:06:04.035
from this monkey,
so we'd get out

00:06:04.035 --> 00:06:05.990
a pattern of neural activity.

00:06:05.990 --> 00:06:08.120
And what we do in decoding
is we feed that pattern

00:06:08.120 --> 00:06:10.460
of neural activity
into a machine learning

00:06:10.460 --> 00:06:13.280
algorithm, which we call
pattern classifiers.

00:06:13.280 --> 00:06:16.532
Again, you've all
heard a lot about that.

00:06:16.532 --> 00:06:17.990
And so what this
algorithm does, is

00:06:17.990 --> 00:06:21.200
it learns to make an association
between this particular

00:06:21.200 --> 00:06:25.460
stimulus and this particular
pattern of neural activity.

00:06:25.460 --> 00:06:27.710
And so then we repeat that
process with another image,

00:06:27.710 --> 00:06:30.020
get another pattern of
neural activity out.

00:06:30.020 --> 00:06:31.850
Feed that into the classifier.

00:06:31.850 --> 00:06:34.190
And again, it learns
that association.

00:06:34.190 --> 00:06:37.130
And so we do that for every
single stimulus in our stimulus

00:06:37.130 --> 00:06:37.760
set.

00:06:37.760 --> 00:06:41.870
And for multiple repetitions
of each stimulus.

00:06:41.870 --> 00:06:43.870
So you know, once this
association is learned,

00:06:43.870 --> 00:06:46.140
what we do is we
use the classifier

00:06:46.140 --> 00:06:47.606
or test the classifier.

00:06:47.606 --> 00:06:48.730
Here we show another image.

00:06:48.730 --> 00:06:51.220
We get another pattern
of neural activity out.

00:06:51.220 --> 00:06:52.854
We feed that into
the classifier.

00:06:52.854 --> 00:06:54.520
But this time, instead
of the classifier

00:06:54.520 --> 00:06:57.430
learning the association,
it makes a prediction.

00:06:57.430 --> 00:07:00.730
And here it predicted the
kiwi, so we'd say it's correct.

00:07:00.730 --> 00:07:02.830
And then we can repeat
that with a car,

00:07:02.830 --> 00:07:04.802
get another pattern
of activity out.

00:07:04.802 --> 00:07:07.219
Feed it to the classifier,
get another prediction.

00:07:07.219 --> 00:07:09.010
And this time the
prediction was incorrect.

00:07:09.010 --> 00:07:11.350
It predicted a face, but
it was actually a car.

00:07:11.350 --> 00:07:14.380
And so what we do is
we just note how often

00:07:14.380 --> 00:07:15.820
are predictions correct.

00:07:15.820 --> 00:07:17.920
And we can plot that
as a function of time

00:07:17.920 --> 00:07:20.302
and kind of see the
evolution of information

00:07:20.302 --> 00:07:21.760
as it flows through
a brain region.

00:07:27.750 --> 00:07:30.690
All right, so in reality,
what we usually do is actually

00:07:30.690 --> 00:07:32.690
we run the full experiment.

00:07:32.690 --> 00:07:35.340
So we actually have collected
all the data beforehand.

00:07:35.340 --> 00:07:39.420
And then what we do is we split
it up into different splits.

00:07:39.420 --> 00:07:43.080
So here we had, you know,
this experiment, let's say,

00:07:43.080 --> 00:07:45.220
was faces and cars or something.

00:07:45.220 --> 00:07:47.370
So we have different
splits that have

00:07:47.370 --> 00:07:50.340
two repetitions of the
activity of different neurons

00:07:50.340 --> 00:07:53.490
do two faces and two cars, and
there's three different splits.

00:07:53.490 --> 00:07:56.492
And so what we do is we
take two of the splits

00:07:56.492 --> 00:07:58.950
and train the classifier, and
then have the remaining split

00:07:58.950 --> 00:08:00.000
and test it.

00:08:00.000 --> 00:08:05.940
And we do that for all
permutations of leaving out

00:08:05.940 --> 00:08:08.190
a different test split.

00:08:08.190 --> 00:08:11.390
So you all heard about
cross-validation before?

00:08:11.390 --> 00:08:14.040
OK.

00:08:14.040 --> 00:08:16.560
One thing to note about
neural populations

00:08:16.560 --> 00:08:18.240
is when you're
doing decoding, you

00:08:18.240 --> 00:08:22.050
don't actually need to record
all the neurons simultaneously.

00:08:22.050 --> 00:08:24.342
So I think this might be one
reason why a lot of people

00:08:24.342 --> 00:08:26.966
haven't jumped on the technique
because they feel like you need

00:08:26.966 --> 00:08:28.410
to do these massive recordings.

00:08:28.410 --> 00:08:30.660
But you can actually do
something what's called pseudo

00:08:30.660 --> 00:08:33.690
populations, where you build up
a virtual population that you

00:08:33.690 --> 00:08:35.940
pretend was recorded
simultaneously but really

00:08:35.940 --> 00:08:37.020
wasn't.

00:08:37.020 --> 00:08:40.779
So what you do with that is
you just, if on the first day

00:08:40.779 --> 00:08:42.570
you recorded one neuron,
and the second day

00:08:42.570 --> 00:08:44.891
you recorded the
second neuron, etc.

00:08:44.891 --> 00:08:46.890
What you can do is you
can just randomly select,

00:08:46.890 --> 00:08:48.390
let's say, one trial
when a kiwi was

00:08:48.390 --> 00:08:50.980
shown from the first
day, another trial

00:08:50.980 --> 00:08:52.785
from the second day, et cetera.

00:08:52.785 --> 00:08:54.270
You randomly pick them.

00:08:54.270 --> 00:08:58.721
And then you can just build
up this virtual population.

00:08:58.721 --> 00:09:00.720
And you can do that for
a few examples of kiwis,

00:09:00.720 --> 00:09:02.130
a few examples of cars.

00:09:02.130 --> 00:09:05.690
And then you just train and test
your classifier like normal.

00:09:05.690 --> 00:09:08.280
But this kind of broadens
the applicability.

00:09:08.280 --> 00:09:10.260
And then you can ask
questions about what

00:09:10.260 --> 00:09:13.110
is being lost by doing
this process versus if you

00:09:13.110 --> 00:09:15.382
had actually done the
simultaneous recordings.

00:09:15.382 --> 00:09:17.340
And we'll discuss that
a little bit more later.

00:09:21.710 --> 00:09:24.009
So I'll give you an
example of one classifier,

00:09:24.009 --> 00:09:25.550
again, I'm sure
you've seen much more

00:09:25.550 --> 00:09:27.091
sophisticated and
interesting methods

00:09:27.091 --> 00:09:29.390
but I'll show you a
very basic one that I

00:09:29.390 --> 00:09:31.482
have used a bit in the past.

00:09:31.482 --> 00:09:33.440
It's called the maximum
correlation coefficient

00:09:33.440 --> 00:09:34.390
classifier.

00:09:34.390 --> 00:09:36.890
It's, again, very similar to
what Rebecca was talking about.

00:09:36.890 --> 00:09:39.080
But all you do is--

00:09:39.080 --> 00:09:41.040
let's say this is
our training set.

00:09:41.040 --> 00:09:46.310
So we have four
vectors for each image,

00:09:46.310 --> 00:09:47.814
each thing we want to classify.

00:09:47.814 --> 00:09:49.230
And all we're going
to do is we're

00:09:49.230 --> 00:09:52.430
going to take the
average across neurons

00:09:52.430 --> 00:09:56.270
to reduce these four
vectors into a single factor

00:09:56.270 --> 00:09:57.790
for each stimulus.

00:09:57.790 --> 00:10:01.820
OK, so if we did that we'd
get one kind of prototype

00:10:01.820 --> 00:10:03.320
of each of the stimuli.

00:10:03.320 --> 00:10:06.050
And then to test the classifier,
all we're going to do

00:10:06.050 --> 00:10:07.610
is we're going to
take a test point

00:10:07.610 --> 00:10:09.901
and we're going to do the
correlation between this test

00:10:09.901 --> 00:10:13.220
point and each of the
kind of prototype vectors.

00:10:13.220 --> 00:10:15.830
Whichever one has the
highest correlation,

00:10:15.830 --> 00:10:19.910
we're going to say
that's the prediction.

00:10:19.910 --> 00:10:21.720
Hopefully pretty simple.

00:10:25.940 --> 00:10:28.520
The reason we often use
fairly simple classifiers,

00:10:28.520 --> 00:10:31.280
such as the maximum correlation
coefficient classifier,

00:10:31.280 --> 00:10:32.555
is because--

00:10:32.555 --> 00:10:34.490
or at least one
motivation is because it

00:10:34.490 --> 00:10:38.240
can be translated into what
information is directly

00:10:38.240 --> 00:10:42.650
available to a downstream
population that

00:10:42.650 --> 00:10:45.840
is reading the information
in the population you

00:10:45.840 --> 00:10:47.540
have recordings from.

00:10:47.540 --> 00:10:50.000
So you could actually
view what the classifier

00:10:50.000 --> 00:10:53.390
learns as synaptic
weights to a neuron.

00:10:53.390 --> 00:10:56.180
You could view the
pattern of activity

00:10:56.180 --> 00:10:59.360
you're trying to classify as
the pre-synaptic activity.

00:10:59.360 --> 00:11:02.870
And then by doing this dot
product multiplication, perhaps

00:11:02.870 --> 00:11:04.760
pass through some
non-linearity, you

00:11:04.760 --> 00:11:09.400
can kind of output a
prediction about whether there

00:11:09.400 --> 00:11:12.230
is evidence for a particular
stimulus being present.

00:11:12.230 --> 00:11:14.900
All right, so let's go into
talking about neural content,

00:11:14.900 --> 00:11:17.030
or what information
is in a brain region

00:11:17.030 --> 00:11:20.910
and how it needs
decoding to get at that.

00:11:20.910 --> 00:11:23.390
So as motivation, I'm
going to be talking

00:11:23.390 --> 00:11:26.210
about a very simple experiment.

00:11:26.210 --> 00:11:29.270
Basically, this experiment
involves a monkey

00:11:29.270 --> 00:11:32.114
fixating on a point for--

00:11:32.114 --> 00:11:33.780
well, through the
duration of the trial.

00:11:33.780 --> 00:11:35.840
But first, there's
a blank screen.

00:11:35.840 --> 00:11:40.520
And then after 500 milliseconds,
up is going to come a stimulus.

00:11:40.520 --> 00:11:42.350
And for this
experiment, there is

00:11:42.350 --> 00:11:45.039
going to be 7 different possible
stimuli that are shown here.

00:11:45.039 --> 00:11:46.580
And what we're going
to try to decode

00:11:46.580 --> 00:11:49.820
is which of these stimuli
was present on one

00:11:49.820 --> 00:11:51.470
particular trial.

00:11:51.470 --> 00:11:55.400
And we're going to do that
as a function of time.

00:11:55.400 --> 00:11:57.320
And the data I'm
going to use comes

00:11:57.320 --> 00:11:59.210
from the inferior
temporal cortex.

00:11:59.210 --> 00:12:02.690
We're going to look at 132
neuron pseudo populations.

00:12:02.690 --> 00:12:07.315
This was data recorded by Ying
Jang in Bob Desimone's lab.

00:12:07.315 --> 00:12:09.440
It's actually part of a
more complicated experiment

00:12:09.440 --> 00:12:12.380
but I've just reduced it here to
the simplest kind of bare bones

00:12:12.380 --> 00:12:12.880
nature.

00:12:15.374 --> 00:12:16.790
So what we're going
to do is we're

00:12:16.790 --> 00:12:21.770
going to basically train
the classifier on one time

00:12:21.770 --> 00:12:24.590
point with the average
firing rate in some bin.

00:12:24.590 --> 00:12:27.200
I think in this case
it's 100 milliseconds.

00:12:27.200 --> 00:12:29.270
And then we're going to
test at that time point.

00:12:29.270 --> 00:12:31.430
And then I'm going to slide
over by a small amount

00:12:31.430 --> 00:12:32.940
and repeat that process.

00:12:32.940 --> 00:12:35.690
So each time we are repeating
training and testing

00:12:35.690 --> 00:12:36.695
the classifier.

00:12:40.580 --> 00:12:42.830
Again, 100 milliseconds
sampled every 10 seconds,

00:12:42.830 --> 00:12:44.300
or sliding every 10 seconds.

00:12:44.300 --> 00:12:46.680
And this will give us a flow
of information over time.

00:12:46.680 --> 00:12:49.160
So during the baseline
period we should not

00:12:49.160 --> 00:12:51.320
be able to decode
what's about to be seen,

00:12:51.320 --> 00:12:54.662
unless the monkey is
psychic, in which case

00:12:54.662 --> 00:12:56.870
either there is something
wrong with your experiment,

00:12:56.870 --> 00:12:57.500
most likely.

00:12:57.500 --> 00:13:00.504
Or you should go to Wall
Street with your monkey.

00:13:00.504 --> 00:13:02.420
But you know, you shouldn't
get anything here.

00:13:02.420 --> 00:13:04.253
And then we should see
some sort of increase

00:13:04.253 --> 00:13:06.720
here if there is information.

00:13:06.720 --> 00:13:09.500
And this is kind of what it
looks like from the results.

00:13:09.500 --> 00:13:11.390
So this is zero.

00:13:11.390 --> 00:13:13.190
After here, we should
see information.

00:13:13.190 --> 00:13:16.670
This is chance, or 1 over 7.

00:13:16.670 --> 00:13:18.560
And so if we try this
decoding experiment,

00:13:18.560 --> 00:13:21.050
what we find is
during the baseline,

00:13:21.050 --> 00:13:23.390
our monkey is not psychic.

00:13:23.390 --> 00:13:26.630
But when we put
on a stimulus, we

00:13:26.630 --> 00:13:31.530
can tell what it is pretty
well, like almost perfectly.

00:13:31.530 --> 00:13:33.120
Pretty simple.

00:13:33.120 --> 00:13:36.000
All right, we can also
do some statistics

00:13:36.000 --> 00:13:40.410
to tell you when the decoding
results are above chance doing

00:13:40.410 --> 00:13:43.140
some sort of permutation test
where we shuffle the labels

00:13:43.140 --> 00:13:45.510
and try to do the decoding
on shuffled labels where

00:13:45.510 --> 00:13:47.940
we should get chance
decoding performance.

00:13:47.940 --> 00:13:51.690
And then we can see where is our
real result relative to chance,

00:13:51.690 --> 00:13:53.510
and get p values and
things like that.

00:13:56.640 --> 00:13:57.720
It's pretty simple.

00:13:57.720 --> 00:14:00.120
How does this stack up
against other methods

00:14:00.120 --> 00:14:02.200
that people commonly use?

00:14:02.200 --> 00:14:06.150
So here's our decoding
result. Here's another method.

00:14:06.150 --> 00:14:09.270
Here I'm applying an
ANOVA to each neuron

00:14:09.270 --> 00:14:12.000
individually and counting
the number of neurons that

00:14:12.000 --> 00:14:16.620
are deemed to be selective.

00:14:16.620 --> 00:14:18.954
And so what you see is that
there's basically no neurons

00:14:18.954 --> 00:14:19.911
in the baseline period.

00:14:19.911 --> 00:14:21.280
And then we have a huge number.

00:14:21.280 --> 00:14:25.200
OK, so it looks
pretty much identical.

00:14:25.200 --> 00:14:28.380
We can compute mutual
information on each neuron

00:14:28.380 --> 00:14:31.500
and then average that together
over a whole bunch of neurons.

00:14:31.500 --> 00:14:33.780
Again, looks pretty simple.

00:14:33.780 --> 00:14:35.490
Or similar, I should say.

00:14:35.490 --> 00:14:39.085
Or we can compute a
selectivity index.

00:14:39.085 --> 00:14:41.460
Take the best stimulus, subtract
from the worst stimulus,

00:14:41.460 --> 00:14:42.640
divide by the sum.

00:14:42.640 --> 00:14:43.770
Again, looks similar.

00:14:43.770 --> 00:14:47.560
So there's two
takeaway messages here.

00:14:47.560 --> 00:14:51.120
First of all, why do decoding if
all the other methods work just

00:14:51.120 --> 00:14:52.330
as well?

00:14:52.330 --> 00:14:54.990
And I'll show you in a
bit, they don't always.

00:14:54.990 --> 00:14:57.150
And then the other
take away message

00:14:57.150 --> 00:14:59.684
though is as a reassurance,
it is giving you

00:14:59.684 --> 00:15:00.600
the same thing, right?

00:15:00.600 --> 00:15:02.520
So you know we're
not completely crazy.

00:15:02.520 --> 00:15:04.645
It's a sensible thing to
do in the most basic case.

00:15:06.859 --> 00:15:08.400
One other thing
decoding can give you

00:15:08.400 --> 00:15:11.640
that these other methods can't
is something called a confusion

00:15:11.640 --> 00:15:13.090
matrix.

00:15:13.090 --> 00:15:16.020
So a confusion matrix,
Rebecca kind of talked

00:15:16.020 --> 00:15:18.450
a little bit about
related concepts,

00:15:18.450 --> 00:15:22.050
basically what you have is you
have the true classes here.

00:15:22.050 --> 00:15:25.380
So this is what was actually
shown on each trial.

00:15:25.380 --> 00:15:28.830
And this is what your
classifier predicted.

00:15:28.830 --> 00:15:30.930
So the diagonal elements
mean correct predictions.

00:15:30.930 --> 00:15:34.320
There actually was a car
shown and you predicted a car.

00:15:34.320 --> 00:15:36.780
But you can look at the
off diagonal elements

00:15:36.780 --> 00:15:41.340
and you can see what was
commonly made as a mistake.

00:15:41.340 --> 00:15:43.440
And this can tell you,
oh, these two stimuli

00:15:43.440 --> 00:15:47.280
are represented in a similar
way in a brain region, where

00:15:47.280 --> 00:15:48.476
the mistakes are happening.

00:15:53.310 --> 00:15:56.700
So another kind of
methods issue is,

00:15:56.700 --> 00:16:00.599
what is the effect of using
different classifiers?

00:16:00.599 --> 00:16:02.890
If the method is highly
dependent on the classifier you

00:16:02.890 --> 00:16:06.240
use, then that's
not a good thing

00:16:06.240 --> 00:16:08.540
because you're not
telling yourself

00:16:08.540 --> 00:16:10.290
anything about the
data, but you're really

00:16:10.290 --> 00:16:11.873
telling you something
about the method

00:16:11.873 --> 00:16:13.510
you use to extract that data.

00:16:13.510 --> 00:16:17.160
But in general, for at least
simple decoding questions,

00:16:17.160 --> 00:16:20.434
it's pretty robust to the choice
of classifier you would use.

00:16:20.434 --> 00:16:22.350
So here is the maximum
correlation coefficient

00:16:22.350 --> 00:16:24.290
classifier I told you about.

00:16:24.290 --> 00:16:25.740
Here's a support vector machine.

00:16:25.740 --> 00:16:28.350
You can see like almost
everything looks similar.

00:16:28.350 --> 00:16:30.930
And like when there's
something not working as well,

00:16:30.930 --> 00:16:33.300
it's generally a
slight downward shift.

00:16:33.300 --> 00:16:35.610
So you get the same
kind of estimation

00:16:35.610 --> 00:16:37.860
of how much information is
in a brain region flowing

00:16:37.860 --> 00:16:39.330
as a function of time.

00:16:39.330 --> 00:16:43.136
But maybe your absolute accuracy
is just a little bit lower

00:16:43.136 --> 00:16:44.760
if you're not using
the optimal method.

00:16:44.760 --> 00:16:47.301
But really, it seems like we're
assessing what is in the data

00:16:47.301 --> 00:16:50.250
and not so much
about the algorithm.

00:16:50.250 --> 00:16:51.960
So that was decoding
basic information

00:16:51.960 --> 00:16:54.480
in terms of content.

00:16:54.480 --> 00:16:56.760
But I think one of the most
powerful things decoding

00:16:56.760 --> 00:16:59.520
can do is it can
decode what I call

00:16:59.520 --> 00:17:02.094
abstract or
invariant information

00:17:02.094 --> 00:17:04.510
where you can get an assessment
of whether that's present.

00:17:04.510 --> 00:17:06.190
So what does that mean?

00:17:06.190 --> 00:17:09.810
Well, basically you can think of
something like the word hello.

00:17:09.810 --> 00:17:11.760
It has many different
pronunciations

00:17:11.760 --> 00:17:13.030
in different languages.

00:17:13.030 --> 00:17:14.821
But if you speak these
different languages,

00:17:14.821 --> 00:17:17.010
you can kind of
translate that word

00:17:17.010 --> 00:17:19.230
into some sort of meaning
that it's a greeting.

00:17:19.230 --> 00:17:21.460
And you know how to
respond appropriately.

00:17:21.460 --> 00:17:23.230
So that's kind of a
form of abstraction.

00:17:23.230 --> 00:17:26.250
It's going from very
different sound concepts

00:17:26.250 --> 00:17:28.496
into some sort of
abstract representation

00:17:28.496 --> 00:17:30.870
where I know how to respond
appropriately by saying hello

00:17:30.870 --> 00:17:33.330
back in that language.

00:17:33.330 --> 00:17:37.260
Or another example of this kind
of abstraction or invariance

00:17:37.260 --> 00:17:41.050
is the invariance of
the pose of a head.

00:17:41.050 --> 00:17:43.890
So for example, here is a bunch
of pictures of Hillary Clinton.

00:17:43.890 --> 00:17:46.590
You can see her head is
at very different angles.

00:17:46.590 --> 00:17:49.170
But we can still tell
it's Hillary Clinton.

00:17:49.170 --> 00:17:51.570
So we have some sort of
representation of Hillary

00:17:51.570 --> 00:17:54.270
that's abstracted from the
exact pose of her head,

00:17:54.270 --> 00:17:56.550
and also abstracted from
the color of her pantsuit.

00:17:56.550 --> 00:18:00.120
It's very highly
abstract, right?

00:18:00.120 --> 00:18:03.780
So that's pretty powerful to
know how the brain is dropping

00:18:03.780 --> 00:18:06.560
information in order to build
up these representations that

00:18:06.560 --> 00:18:08.220
are useful for behavior.

00:18:08.220 --> 00:18:09.720
And I think if we
were, again, going

00:18:09.720 --> 00:18:11.790
to build intelligent
robotic system,

00:18:11.790 --> 00:18:14.100
we'd want to build it
to have representations

00:18:14.100 --> 00:18:20.340
that have become more abstract
so it can perform correctly.

00:18:20.340 --> 00:18:22.590
So let's show you
the example of how

00:18:22.590 --> 00:18:28.220
we can assess abstract
representations in neural data.

00:18:28.220 --> 00:18:31.420
What I'm going to look at
is position invariance.

00:18:31.420 --> 00:18:33.720
So this is similar
to a study that

00:18:33.720 --> 00:18:36.571
was done in 2005 by Hung
and Kreiman in Science.

00:18:36.571 --> 00:18:38.070
And what I'm going
to do here is I'm

00:18:38.070 --> 00:18:42.730
going to train the classifier
with data at an upper location.

00:18:42.730 --> 00:18:44.790
So in this experiment,
the stimuli

00:18:44.790 --> 00:18:47.230
was shown at three
different locations.

00:18:47.230 --> 00:18:49.170
So on any given
trial, one stimulus

00:18:49.170 --> 00:18:50.701
was shown at one location.

00:18:50.701 --> 00:18:52.200
And these three
locations were used,

00:18:52.200 --> 00:18:55.410
so the 7 objects were all
shown at the upper location,

00:18:55.410 --> 00:18:57.290
or at the middle, at the lower.

00:18:57.290 --> 00:18:58.790
And here I'm training
the classifier

00:18:58.790 --> 00:19:01.130
using just the trials
when the stimuli was

00:19:01.130 --> 00:19:02.880
shown in the upper location.

00:19:02.880 --> 00:19:05.430
And then what we can do is we
can then test the classifier

00:19:05.430 --> 00:19:07.260
on those trials where
the stimuli were just

00:19:07.260 --> 00:19:09.080
shown at the lower location.

00:19:09.080 --> 00:19:11.350
And we can see, if we train
at the upper location,

00:19:11.350 --> 00:19:13.830
does it generalize to
the lower location.

00:19:13.830 --> 00:19:16.530
And if it does, it means there
is a representation that's

00:19:16.530 --> 00:19:18.580
invariant to position.

00:19:18.580 --> 00:19:22.320
Does that make
sense to everyone?

00:19:22.320 --> 00:19:23.910
So let's take a
look at the results

00:19:23.910 --> 00:19:27.810
for training at the upper
and testing at the lower.

00:19:27.810 --> 00:19:29.597
They're down here.

00:19:29.597 --> 00:19:31.680
So here again, I'm training
at the upper location.

00:19:31.680 --> 00:19:33.763
And this is the results
from testing at the lower.

00:19:33.763 --> 00:19:34.470
Here is chance.

00:19:34.470 --> 00:19:37.600
And you can see we're well
above chance in the decoding.

00:19:37.600 --> 00:19:40.680
So it's generalizing from the
upper location to the lower.

00:19:40.680 --> 00:19:45.150
We can also train at the upper
and test at the same upper,

00:19:45.150 --> 00:19:47.040
at the middle location.

00:19:47.040 --> 00:19:48.894
And what we find is
this pattern of results.

00:19:48.894 --> 00:19:51.060
So we're getting best results
when we train and test

00:19:51.060 --> 00:19:52.860
at exactly the same position.

00:19:52.860 --> 00:19:55.440
But we can see it does
generalize to other positions

00:19:55.440 --> 00:19:57.770
as well.

00:19:57.770 --> 00:20:00.692
And so we can do this full
permutations of things.

00:20:00.692 --> 00:20:02.150
So here we trained
at the upper, we

00:20:02.150 --> 00:20:05.282
could also train at the middle,
or train at the lower location.

00:20:05.282 --> 00:20:06.740
And here if we
train at the middle,

00:20:06.740 --> 00:20:08.720
we get the best decoding
performance when

00:20:08.720 --> 00:20:10.040
we decode at that same middle.

00:20:10.040 --> 00:20:12.680
But again, it's generalizing to
the upper and lower locations,

00:20:12.680 --> 00:20:14.250
and the same for
training at lower.

00:20:14.250 --> 00:20:15.875
Get the best performance
testing lower,

00:20:15.875 --> 00:20:18.350
but it again generalizes.

00:20:18.350 --> 00:20:22.340
So if you want to just conclude
this one mini study here,

00:20:22.340 --> 00:20:24.620
you know, information in
IT is position invariant

00:20:24.620 --> 00:20:26.150
but not you know 100%.

00:20:30.642 --> 00:20:31.850
So we can use this technique.

00:20:31.850 --> 00:20:33.050
I'll show you a
few other examples

00:20:33.050 --> 00:20:35.360
of how it can be used in
slightly more powerful ways,

00:20:35.360 --> 00:20:38.760
maybe, or to answer slightly
more interesting questions.

00:20:38.760 --> 00:20:42.520
So what another question
we might want to ask,

00:20:42.520 --> 00:20:46.280
actually we did ask in this
paper that just came out,

00:20:46.280 --> 00:20:50.150
was about the question
of pose invariant

00:20:50.150 --> 00:20:51.950
identity information,
so that same question

00:20:51.950 --> 00:20:55.820
about can a brain region
respond to Hillary Clinton

00:20:55.820 --> 00:20:58.550
regardless of where
she's looking.

00:20:58.550 --> 00:21:01.795
And so this is data recorded
by Winrich Freiwald and Doris

00:21:01.795 --> 00:21:02.295
Tsao.

00:21:02.295 --> 00:21:05.460
Winrich probably already
talked about this experiment.

00:21:05.460 --> 00:21:08.390
But what they did was they
had the face system here

00:21:08.390 --> 00:21:10.010
where they found
these little patches

00:21:10.010 --> 00:21:13.620
through fMRI that respond more
to faces than other stimuli.

00:21:13.620 --> 00:21:16.070
They went in and they
recorded from these patches.

00:21:16.070 --> 00:21:19.740
And in this study that we're
going to look at, they did a--

00:21:19.740 --> 00:21:23.840
they used these stimuli that
had 25 different individuals

00:21:23.840 --> 00:21:26.151
shown at eight different
head orientations.

00:21:26.151 --> 00:21:28.400
So this is Doris at eight
different head orientations,

00:21:28.400 --> 00:21:33.014
but there were 24 other
people who also were shown.

00:21:33.014 --> 00:21:34.430
And so what I'm
going to try to do

00:21:34.430 --> 00:21:37.730
is decode between the 25
different people and see,

00:21:37.730 --> 00:21:40.790
can it generalize if I
train at one orientation

00:21:40.790 --> 00:21:43.290
and test at a different one.

00:21:43.290 --> 00:21:46.010
And the three brain
regions we're going to use

00:21:46.010 --> 00:21:47.790
is the most posterior region.

00:21:47.790 --> 00:21:50.340
So in this case, the eyes
out here, this is like V1.

00:21:50.340 --> 00:21:51.960
This is the ventral pathway.

00:21:51.960 --> 00:21:55.200
So the most posterior region,
we can combine ML and MF.

00:21:55.200 --> 00:21:58.820
We compare that to AL and to AM.

00:21:58.820 --> 00:22:01.374
I'm going to see how much
position variance is there.

00:22:01.374 --> 00:22:03.290
So again, like I said,
let's start by training

00:22:03.290 --> 00:22:05.540
on the left profile
and then we can

00:22:05.540 --> 00:22:08.000
test on the left profile
in different trials.

00:22:08.000 --> 00:22:11.690
Or we can test on a
different set of images

00:22:11.690 --> 00:22:15.620
where the individuals were
looking straight forward.

00:22:15.620 --> 00:22:19.520
So here are the results from the
most posterior region, ML/MF.

00:22:19.520 --> 00:22:22.100
What we see is if we
train in the left profile

00:22:22.100 --> 00:22:24.440
and test on the
left profile here,

00:22:24.440 --> 00:22:27.140
we're getting results that
are above chance, as indicated

00:22:27.140 --> 00:22:30.530
by the lighter blue trace.

00:22:30.530 --> 00:22:33.050
But if we train on the
left profile and test

00:22:33.050 --> 00:22:34.670
in the straight
results, we're getting

00:22:34.670 --> 00:22:37.790
results that are at chance.

00:22:37.790 --> 00:22:42.587
So this patch here is not
showing very much pose

00:22:42.587 --> 00:22:43.540
invariance.

00:22:43.540 --> 00:22:45.540
So let's take a look at
the rest of the results.

00:22:45.540 --> 00:22:47.270
So this is ML/MF.

00:22:47.270 --> 00:22:49.779
If we look at AL,
what we see is,

00:22:49.779 --> 00:22:52.070
again, there's a big advantage
for training and testing

00:22:52.070 --> 00:22:53.390
at that same orientation.

00:22:53.390 --> 00:22:55.010
But now we're seeing
generalization

00:22:55.010 --> 00:22:56.870
to the other orientations.

00:22:56.870 --> 00:22:59.390
You're also seeing this "U"
pattern where you're actually

00:22:59.390 --> 00:23:01.550
generalizing better
from one profile

00:23:01.550 --> 00:23:03.920
to the opposite profile,
which was reported in some

00:23:03.920 --> 00:23:06.710
of their earlier papers.

00:23:06.710 --> 00:23:08.925
But yeah, here you're
seeing, statistically,

00:23:08.925 --> 00:23:09.800
that is above chance.

00:23:09.800 --> 00:23:12.622
Now it's not huge,
but it's above what

00:23:12.622 --> 00:23:13.580
you'd expect by chance.

00:23:13.580 --> 00:23:16.580
And if we look at
AM as well, we're

00:23:16.580 --> 00:23:19.640
seeing a higher degree
of invariance, again,

00:23:19.640 --> 00:23:23.390
a slight advantage to the exact
pose, but still pretty good.

00:23:23.390 --> 00:23:25.070
Again, this "U" a
little bit but yeah,

00:23:25.070 --> 00:23:26.300
we're going to the
back of the head.

00:23:26.300 --> 00:23:27.390
So what would that
tell you, the fact

00:23:27.390 --> 00:23:29.473
that it's going to the
back of the head, tells you

00:23:29.473 --> 00:23:31.510
it's probably representing
something about hair.

00:23:31.510 --> 00:23:32.900
What I'm going to do next,
rather than just training

00:23:32.900 --> 00:23:35.990
at the left profile, I'm going
to take the results of training

00:23:35.990 --> 00:23:40.070
at each of the profiles and
either testing at the same

00:23:40.070 --> 00:23:41.900
or testing at a
different profile.

00:23:41.900 --> 00:23:45.080
And then I'm going to plot
it as a function of time.

00:23:45.080 --> 00:23:49.070
So here are the results
of training and testing

00:23:49.070 --> 00:23:50.850
at the same pose.

00:23:50.850 --> 00:23:52.310
So the non-invariant case.

00:23:52.310 --> 00:23:54.650
This is ML/MF.

00:23:54.650 --> 00:23:56.135
And this AL and AM.

00:23:56.135 --> 00:23:59.420
So this is going from the
back of the head anterior.

00:23:59.420 --> 00:24:00.890
And what you see
is there is a kind

00:24:00.890 --> 00:24:05.430
of an increase in this
pose-specific information.

00:24:05.430 --> 00:24:07.850
Here the increase
is fairly small.

00:24:07.850 --> 00:24:10.100
But there is just
generally more information

00:24:10.100 --> 00:24:11.120
as you're going down.

00:24:11.120 --> 00:24:14.120
But the big increase is
really in this pose invariant

00:24:14.120 --> 00:24:14.756
information.

00:24:14.756 --> 00:24:16.880
When you train at one
location and test at another,

00:24:16.880 --> 00:24:18.260
that's these red traces here.

00:24:18.260 --> 00:24:21.740
And here you can see it's
really accelerating a lot.

00:24:21.740 --> 00:24:25.250
It's really that these
areas downstream are maybe

00:24:25.250 --> 00:24:27.380
pooling over the different
poses to create opposing

00:24:27.380 --> 00:24:30.770
invariant representation.

00:24:30.770 --> 00:24:35.240
So to carry on with this for
general concept of testing

00:24:35.240 --> 00:24:37.494
invariant representations
or abstract representations,

00:24:37.494 --> 00:24:39.410
let me just give you one
more example of that.

00:24:39.410 --> 00:24:41.460
Here was one of my
earlier studies.

00:24:41.460 --> 00:24:47.520
What I did was this study was
looking at categorization.

00:24:47.520 --> 00:24:49.460
It was a study done
in Earl Miller's lab.

00:24:49.460 --> 00:24:50.969
David Friedman
collected the data.

00:24:50.969 --> 00:24:52.760
And what they did was
they trained a monkey

00:24:52.760 --> 00:24:55.670
to group a bunch of images
together and called them cats.

00:24:55.670 --> 00:24:58.260
And then to group a number
of images together and called

00:24:58.260 --> 00:24:59.730
them dogs.

00:24:59.730 --> 00:25:01.620
It wasn't clear that
the images necessarily

00:25:01.620 --> 00:25:03.661
were more similar to each
other within a category

00:25:03.661 --> 00:25:05.070
versus out of the category.

00:25:05.070 --> 00:25:07.080
But through this
training, the monkeys

00:25:07.080 --> 00:25:11.170
could quite well group the
images together in a delayed

00:25:11.170 --> 00:25:12.900
match to sample task.

00:25:12.900 --> 00:25:14.790
And so what I
wanted to know was,

00:25:14.790 --> 00:25:17.250
is there information that is
kind of about the animal's

00:25:17.250 --> 00:25:21.630
category that is abstracted
away from the low level

00:25:21.630 --> 00:25:23.310
of visual features.

00:25:23.310 --> 00:25:25.440
OK, so was this
learning process,

00:25:25.440 --> 00:25:27.570
did they build neural
representations that

00:25:27.570 --> 00:25:29.940
are more similar to each other?

00:25:29.940 --> 00:25:34.770
So what I did here was I
trained the classifier on two

00:25:34.770 --> 00:25:36.720
of the prototype images.

00:25:36.720 --> 00:25:40.100
And then I tested it on
a left out prototype.

00:25:40.100 --> 00:25:42.450
And so if it's making
correct predictions here,

00:25:42.450 --> 00:25:45.180
then it is generalizing
to something

00:25:45.180 --> 00:25:47.930
that would only be available
in the data if the monkey had--

00:25:47.930 --> 00:25:51.750
due to the monkey's training.

00:25:51.750 --> 00:25:55.750
Modulo any low level compounds.

00:25:55.750 --> 00:25:59.280
And so here is decoding of
this abstract or invariant

00:25:59.280 --> 00:26:00.649
information from the two areas.

00:26:00.649 --> 00:26:02.190
And what you see,
indeed, there seems

00:26:02.190 --> 00:26:04.710
to be this kind of
grouping effect, where

00:26:04.710 --> 00:26:08.190
the category is represented
both in IT and PFC

00:26:08.190 --> 00:26:10.517
in this abstract way.

00:26:10.517 --> 00:26:12.600
So the same method can be
used to assess learning.

00:26:16.190 --> 00:26:19.370
So just to summarize
the neural content part,

00:26:19.370 --> 00:26:22.247
decoding offers a way to clearly
see what information is there

00:26:22.247 --> 00:26:24.080
and how it is flowing
through a brain region

00:26:24.080 --> 00:26:27.170
as a function of time.

00:26:27.170 --> 00:26:29.760
We can assess basic
information and often it

00:26:29.760 --> 00:26:32.090
yields similar results
to other methods.

00:26:32.090 --> 00:26:34.990
But we can also do
things like assess

00:26:34.990 --> 00:26:36.830
abstract or invariant
information, which

00:26:36.830 --> 00:26:38.570
is not really possible
with other methods

00:26:38.570 --> 00:26:41.180
as far as I can see how to
use those other methods.

00:26:44.330 --> 00:26:48.800
So for neural coding, my
motivation is the game poker.

00:26:48.800 --> 00:26:50.079
This one study I did.

00:26:50.079 --> 00:26:52.370
Basically, when I moved to
Boston I learned how to play

00:26:52.370 --> 00:26:54.200
Texas Hold'em.

00:26:54.200 --> 00:26:56.917
It's a card game where, you
know-- it's a variant of poker,

00:26:56.917 --> 00:26:59.000
I'm sure most of you know,
I didn't know the rules

00:26:59.000 --> 00:27:01.386
before but I learned the rules.

00:27:01.386 --> 00:27:03.260
And I could play the
game pretty successfully

00:27:03.260 --> 00:27:05.480
in terms of at least applying
those rules correctly,

00:27:05.480 --> 00:27:07.610
not necessarily in
terms of winning money.

00:27:07.610 --> 00:27:09.530
But I knew what to do.

00:27:09.530 --> 00:27:11.480
And prior to that, I
had known other games

00:27:11.480 --> 00:27:14.390
like Go Fish, or
War, or whatever.

00:27:14.390 --> 00:27:15.860
And me learning
how to play poker

00:27:15.860 --> 00:27:19.070
did not disrupt my
ability to play go fish.

00:27:19.070 --> 00:27:21.290
I was still bad at that as well.

00:27:21.290 --> 00:27:26.180
So somehow this information that
allowed me to play this game

00:27:26.180 --> 00:27:28.610
had to be added
into my brain if we

00:27:28.610 --> 00:27:30.929
believe brains cause behavior.

00:27:30.929 --> 00:27:33.470
And so in this study, we're kind
of getting at that question,

00:27:33.470 --> 00:27:37.490
what changed about a brain to
allow it to perform a new task?

00:27:40.740 --> 00:27:45.230
And so to do this in an
experiment with monkeys,

00:27:45.230 --> 00:27:46.730
basically, they
used a paradigm that

00:27:46.730 --> 00:27:49.084
had two different phases to it.

00:27:49.084 --> 00:27:51.500
The first phase, what they
did, was they had a monkey just

00:27:51.500 --> 00:27:54.050
do a passive fixation task.

00:27:54.050 --> 00:27:56.300
So what the monkey
did was, there

00:27:56.300 --> 00:27:58.700
would be a fixation
dot that came up.

00:27:58.700 --> 00:28:00.560
Up would come a stimulus.

00:28:00.560 --> 00:28:01.790
There would be a delay.

00:28:01.790 --> 00:28:03.560
There would be a
second stimulus.

00:28:03.560 --> 00:28:05.570
And there would
be a second delay.

00:28:05.570 --> 00:28:07.047
And then there
would be a reward.

00:28:07.047 --> 00:28:09.380
And the reward was given just
for the monkey maintaining

00:28:09.380 --> 00:28:09.994
fixation.

00:28:09.994 --> 00:28:11.660
The monkey did not
need to pay attention

00:28:11.660 --> 00:28:14.120
to what the stimuli were at all.

00:28:14.120 --> 00:28:16.630
And on some trials the
stimuli was the same.

00:28:16.630 --> 00:28:18.780
On other trials,
they were different.

00:28:18.780 --> 00:28:21.736
But the monkey did not
need to care about that.

00:28:21.736 --> 00:28:23.110
So monkey does
this passive task.

00:28:23.110 --> 00:28:26.690
They record like
over 750 neurons

00:28:26.690 --> 00:28:29.287
from the prefrontal cortex.

00:28:29.287 --> 00:28:31.370
And then what they did was
they trained the monkey

00:28:31.370 --> 00:28:34.220
to deal with delayed
match to sample task.

00:28:34.220 --> 00:28:37.290
And the delayed match to
sample task ran very similar.

00:28:37.290 --> 00:28:39.500
So it had a fixation.

00:28:39.500 --> 00:28:40.790
There was a first stimulus.

00:28:40.790 --> 00:28:44.297
There was a delay, a second
stimulus, a second delay.

00:28:44.297 --> 00:28:46.130
So up to this point,
the sequence of stimuli

00:28:46.130 --> 00:28:48.570
was exactly the same.

00:28:48.570 --> 00:28:51.080
But now after the
second delay, up came

00:28:51.080 --> 00:28:55.340
a choice target, a choice
image, and the monkey

00:28:55.340 --> 00:28:57.860
needed to make a saccade
to the green stimulus

00:28:57.860 --> 00:29:01.279
if these two stimuli
were matches.

00:29:01.279 --> 00:29:03.320
And needed to make a
saccade to the blue stimulus

00:29:03.320 --> 00:29:05.684
if they were different.

00:29:05.684 --> 00:29:07.850
And so what we wanted to
know was when the monkey is

00:29:07.850 --> 00:29:10.340
performing this task, it
needs to remember the stimuli

00:29:10.340 --> 00:29:12.000
and whether they
were matched or not,

00:29:12.000 --> 00:29:15.740
is there a change in
the monkey's brain.

00:29:15.740 --> 00:29:17.510
And so the way we're
going to get at this

00:29:17.510 --> 00:29:21.405
is, not surprisingly,
doing a decoding approach.

00:29:21.405 --> 00:29:23.780
And what we do is we're going
to use the same thing where

00:29:23.780 --> 00:29:25.760
we train to classify
at one point in time,

00:29:25.760 --> 00:29:28.070
test, and move on.

00:29:28.070 --> 00:29:31.190
And what we should
find is that we're

00:29:31.190 --> 00:29:33.200
going to try to decode
whether to stimuli

00:29:33.200 --> 00:29:34.280
matched or did not match.

00:29:34.280 --> 00:29:37.100
And so at the time when the
second stimulus was shown,

00:29:37.100 --> 00:29:39.020
we should have some sort
of information about

00:29:39.020 --> 00:29:40.478
whether it was a
match or non-match

00:29:40.478 --> 00:29:42.070
if any information is present.

00:29:42.070 --> 00:29:43.820
And we can see, was
that information there

00:29:43.820 --> 00:29:46.202
before when the monkey was
just passively fixating,

00:29:46.202 --> 00:29:48.410
or does that information
come on only after training.

00:29:51.530 --> 00:29:55.400
So here is a schematic of
the results for decoding.

00:29:55.400 --> 00:29:57.220
It's a binary task,
whether a trial

00:29:57.220 --> 00:29:58.700
was a match or a non-match.

00:29:58.700 --> 00:30:01.970
So chance is 50% if
you were guessing.

00:30:01.970 --> 00:30:04.220
This light gray shaded
region is the time

00:30:04.220 --> 00:30:05.780
when the first stimuli came on.

00:30:05.780 --> 00:30:09.440
This second region is the time
the second stimulus came on.

00:30:09.440 --> 00:30:11.840
And here is where we're
kind of going to ignore,

00:30:11.840 --> 00:30:14.060
this was either the
monkey was making a choice

00:30:14.060 --> 00:30:15.450
or got a juice reward.

00:30:15.450 --> 00:30:17.420
We just ignore that.

00:30:17.420 --> 00:30:19.400
So let's make this interactive.

00:30:19.400 --> 00:30:21.257
How many people thought
there was-- or think

00:30:21.257 --> 00:30:23.840
there might be information about
whether the two stimuli match

00:30:23.840 --> 00:30:27.740
or do not match prior to the
monkey doing the tasks, so

00:30:27.740 --> 00:30:30.840
just in the pacification task?

00:30:30.840 --> 00:30:33.530
Two, three, four, five--

00:30:33.530 --> 00:30:36.440
how many people
think there was not?

00:30:36.440 --> 00:30:38.610
OK, I'd say it's
about a 50/50 split.

00:30:38.610 --> 00:30:42.090
OK, so let's look at the
passive fixation task.

00:30:42.090 --> 00:30:45.300
And what we find is that there
really wasn't any information.

00:30:45.300 --> 00:30:47.580
So there's no blue
bar down here.

00:30:47.580 --> 00:30:50.270
So as far as the
decoding could tell,

00:30:50.270 --> 00:30:52.610
I cannot tell whether the two
stimuli match or not match

00:30:52.610 --> 00:30:55.190
in the passive fixation.

00:30:55.190 --> 00:30:58.340
What about in the active
delay match to sample task,

00:30:58.340 --> 00:31:00.860
how many people think--

00:31:00.860 --> 00:31:03.511
it would be a pretty boring
talk if there wasn't.

00:31:03.511 --> 00:31:04.010
what area?

00:31:04.010 --> 00:31:06.920
We're talking about
dorsolateral--

00:31:06.920 --> 00:31:10.780
actually, both dorsa and ventra
lateral prefrontal cortex.

00:31:17.310 --> 00:31:20.310
Yeah, indeed there
was information there.

00:31:20.310 --> 00:31:23.180
In fact, we could
decode nearly perfectly

00:31:23.180 --> 00:31:25.880
from that brain region.

00:31:25.880 --> 00:31:29.060
So way up here at the time when
the second stimulus was shown.

00:31:29.060 --> 00:31:33.212
So clearly performing
the task, or learning

00:31:33.212 --> 00:31:34.670
how to perform the
task, influenced

00:31:34.670 --> 00:31:37.331
what information was present
in the prefrontal cortex.

00:31:37.331 --> 00:31:39.080
I'm pretty convinced
that this information

00:31:39.080 --> 00:31:40.640
is present and real.

00:31:40.640 --> 00:31:43.100
Now the question is,
and why I'm using this

00:31:43.100 --> 00:31:45.170
as an example of coding,
how did this information

00:31:45.170 --> 00:31:48.350
get added into the population.

00:31:48.350 --> 00:31:50.577
We believe it's there
for real and probably

00:31:50.577 --> 00:31:52.660
contributing to behavior
it's a pretty big effect.

00:31:55.190 --> 00:31:58.610
All right, so here is just
some single neuron results.

00:31:58.610 --> 00:32:00.440
What I've plotted
here is this is

00:32:00.440 --> 00:32:02.990
a measure of how much of
the variability of a neuron

00:32:02.990 --> 00:32:08.060
is predicted about whether a
trial is match or non-match.

00:32:08.060 --> 00:32:10.550
And I've plotted
each dot as a neuron.

00:32:10.550 --> 00:32:12.530
I've plotted each
neuron at the time

00:32:12.530 --> 00:32:14.990
where it had this
maximum value of being

00:32:14.990 --> 00:32:17.680
able to predict whether a
trial is match or non-match.

00:32:17.680 --> 00:32:19.180
And so this is the passive case.

00:32:19.180 --> 00:32:20.930
And so this is kind
of a null distribution

00:32:20.930 --> 00:32:23.900
because we didn't
see any information

00:32:23.900 --> 00:32:27.840
present about match or
non-match in the passive case.

00:32:27.840 --> 00:32:29.840
When the monkey was
performing the delayed match

00:32:29.840 --> 00:32:31.970
to sample task, what
you see is that there's

00:32:31.970 --> 00:32:34.280
kind of a small
number of neurons

00:32:34.280 --> 00:32:38.570
that become selective after
the second stimulus is shown.

00:32:38.570 --> 00:32:41.900
So it seems like a few
neurons are carrying

00:32:41.900 --> 00:32:43.631
a bunch of the information.

00:32:43.631 --> 00:32:46.130
Let's see if we can quantify
this just maybe a little better

00:32:46.130 --> 00:32:47.970
using decoding.

00:32:47.970 --> 00:32:50.600
So what we're going
to do is we're

00:32:50.600 --> 00:32:53.870
going to take the
training set and we're

00:32:53.870 --> 00:32:58.580
going to do an ANOVA to find,
let's say, the eight neurons

00:32:58.580 --> 00:33:01.570
that carry the most information
out of the whole population.

00:33:01.570 --> 00:33:03.120
So the 750 neurons,
let's just find

00:33:03.120 --> 00:33:07.980
the eight that had the
smallest p value in an ANOVA.

00:33:07.980 --> 00:33:09.680
And so we can find
those neurons.

00:33:09.680 --> 00:33:10.760
And we can keep them.

00:33:10.760 --> 00:33:13.431
And we can delete all
the other neurons.

00:33:13.431 --> 00:33:14.930
And then now we
found those neurons,

00:33:14.930 --> 00:33:18.380
we'll also go to the test set
and we'll delete those neurons.

00:33:18.380 --> 00:33:22.340
And now we'll try doing the
whole decoding procedure

00:33:22.340 --> 00:33:24.811
on the smaller population.

00:33:24.811 --> 00:33:26.810
And by deleting the neurons
on the training set,

00:33:26.810 --> 00:33:28.550
we're not really
biasing our results

00:33:28.550 --> 00:33:32.690
when we start doing
the classification.

00:33:32.690 --> 00:33:36.200
So here are the results
using all 750 neurons

00:33:36.200 --> 00:33:38.300
that I showed you before.

00:33:38.300 --> 00:33:42.050
And here are the results using
just the eight best neurons.

00:33:42.050 --> 00:33:44.270
And what you can see is
that the eight best neurons

00:33:44.270 --> 00:33:48.327
are doing almost as well
as using all 750 neurons.

00:33:48.327 --> 00:33:50.410
Now I should say, there
might be a different eight

00:33:50.410 --> 00:33:51.770
best at each point
in time because I'm

00:33:51.770 --> 00:33:52.910
shifting that bin around.

00:33:52.910 --> 00:33:54.368
But still, at any
one point in time

00:33:54.368 --> 00:33:57.770
there are eight neurons that
are really, really good.

00:33:57.770 --> 00:34:00.920
So clearly there is kind of
this compact or small subset

00:34:00.920 --> 00:34:05.361
of neurons that carry the whole
information of the population.

00:34:05.361 --> 00:34:06.860
Once you've done
that, you might not

00:34:06.860 --> 00:34:09.830
want to know the flip of that,
how many redundant neurons are

00:34:09.830 --> 00:34:12.239
there that also carry
that information.

00:34:12.239 --> 00:34:15.949
So here are the results,
again, showing all 750 neurons

00:34:15.949 --> 00:34:16.812
as a comparison.

00:34:16.812 --> 00:34:18.270
And what I'm going
to do now is I'm

00:34:18.270 --> 00:34:20.311
going to take those eight
best neurons, find them

00:34:20.311 --> 00:34:22.560
in the training
set, throw them out.

00:34:22.560 --> 00:34:24.080
I'm going to also
throw another 120

00:34:24.080 --> 00:34:27.415
of the best neurons just to
get rid of a lot of stuff.

00:34:27.415 --> 00:34:29.040
So I'm going to throw
out the best 128.

00:34:29.040 --> 00:34:30.800
And then we'll look at the
remaining neurons and see,

00:34:30.800 --> 00:34:33.050
is there redundant
information in those neurons.

00:34:33.050 --> 00:34:36.949
It's still like 600
neurons or more.

00:34:36.949 --> 00:34:38.810
And so here are the
results from that.

00:34:38.810 --> 00:34:41.690
What you see is that there
is also redundant information

00:34:41.690 --> 00:34:43.130
in this kind of weaker tail.

00:34:43.130 --> 00:34:45.500
It's not quite as good as
the eight best or not as

00:34:45.500 --> 00:34:47.480
high decoding
accuracy, but there

00:34:47.480 --> 00:34:48.829
is redundant information to it.

00:34:51.679 --> 00:34:54.380
Just to summarize this
part, what we see here

00:34:54.380 --> 00:34:56.270
is that there is
a few neurons that

00:34:56.270 --> 00:34:58.775
really became highly, highly
selective due to this process.

00:35:02.420 --> 00:35:04.240
So we see that there's
a lot of information

00:35:04.240 --> 00:35:06.640
in this small, compact set.

00:35:06.640 --> 00:35:08.740
Here are the results from
a related experiment.

00:35:08.740 --> 00:35:10.885
This was in a task
where the monkey had

00:35:10.885 --> 00:35:12.760
to remember the spatial
location of a stimuli

00:35:12.760 --> 00:35:16.570
rather than what an image
was, like a square or circle.

00:35:16.570 --> 00:35:18.220
But anyway, small detail.

00:35:18.220 --> 00:35:20.710
Here's this big effect of
this is match information,

00:35:20.710 --> 00:35:23.536
this is non-match
information being decoded.

00:35:23.536 --> 00:35:24.910
So these are the
decoding results

00:35:24.910 --> 00:35:27.420
that I showed you before.

00:35:27.420 --> 00:35:31.870
Here's an analysis where an ROC
analysis was done on this data.

00:35:31.870 --> 00:35:33.850
So for each neuron,
they calculated

00:35:33.850 --> 00:35:36.910
how well does an individual
neuron separate the match

00:35:36.910 --> 00:35:38.920
and the non-match trials.

00:35:38.920 --> 00:35:41.040
And again, pre
and post training.

00:35:41.040 --> 00:35:44.560
And what you see is here, they
did not see this big split

00:35:44.560 --> 00:35:46.990
that I saw with the decoding.

00:35:46.990 --> 00:35:49.460
And this was published.

00:35:49.460 --> 00:35:53.830
So the question is, why
did they not see it.

00:35:53.830 --> 00:35:57.490
And the reason is because there
were only a few neurons that

00:35:57.490 --> 00:35:59.140
were really highly selective.

00:35:59.140 --> 00:36:00.770
That was enough to
drive the decoding

00:36:00.770 --> 00:36:03.370
but it wasn't enough if you
averaged over all the neurons

00:36:03.370 --> 00:36:04.910
to see this effect.

00:36:04.910 --> 00:36:07.600
So essentially, there's kind
of like two populations here.

00:36:07.600 --> 00:36:09.100
There's a huge
population of neurons

00:36:09.100 --> 00:36:10.840
that did pick up the
match information,

00:36:10.840 --> 00:36:12.280
or picked it up very weakly.

00:36:12.280 --> 00:36:14.020
And then there's a
small set of neurons

00:36:14.020 --> 00:36:16.780
that are very selective.

00:36:16.780 --> 00:36:20.930
And so if you take an average
of the nonselective population,

00:36:20.930 --> 00:36:22.440
it's just here.

00:36:22.440 --> 00:36:24.850
Let's say this is the
pre-training population.

00:36:24.850 --> 00:36:26.710
If you take an average
of post-training

00:36:26.710 --> 00:36:28.810
over all the
neurons, the average

00:36:28.810 --> 00:36:30.400
would shift slightly
to the right.

00:36:30.400 --> 00:36:32.440
But it might not
be very detectable

00:36:32.440 --> 00:36:34.960
from the pre-training
amount of information.

00:36:34.960 --> 00:36:38.020
But if you have weights on just
the highly selective neurons,

00:36:38.020 --> 00:36:39.634
you see a huge effect.

00:36:39.634 --> 00:36:41.800
So it's really important
that you don't average over

00:36:41.800 --> 00:36:45.280
all your neurons but you treat
the neurons as individuals,

00:36:45.280 --> 00:36:49.390
or maybe classes, because
they're doing different things.

00:36:49.390 --> 00:36:52.540
So the next coding
question I wanted to ask

00:36:52.540 --> 00:36:54.880
was, is information
contained in what I

00:36:54.880 --> 00:36:57.580
call a dynamic population code.

00:36:57.580 --> 00:37:00.980
OK, so let me explain
what that means.

00:37:00.980 --> 00:37:05.190
If we showed a stimulus, such
as a kiwi, which I like showing,

00:37:05.190 --> 00:37:08.800
we saw that there might be a
unique pattern for that kiwi.

00:37:08.800 --> 00:37:10.615
And that pattern
is what enables me

00:37:10.615 --> 00:37:12.490
to discriminate between
all the other stimuli

00:37:12.490 --> 00:37:14.089
and do the classification.

00:37:14.089 --> 00:37:15.880
But it might turn out
that there's not just

00:37:15.880 --> 00:37:18.340
one pattern for that
kiwi, but there's actually

00:37:18.340 --> 00:37:19.790
a sequence of patterns.

00:37:19.790 --> 00:37:22.270
So if we plotted the
patterns in time,

00:37:22.270 --> 00:37:24.940
they would actually change.

00:37:24.940 --> 00:37:27.340
So it's a sequence of patterns
that represents one thing.

00:37:29.890 --> 00:37:33.215
And this kind of thing has
been shown a little bit.

00:37:33.215 --> 00:37:34.840
And actually now it's
been shown a lot.

00:37:34.840 --> 00:37:38.350
But when I first did this
in 2008, the kind of one

00:37:38.350 --> 00:37:40.360
study I knew of
that kind of showed

00:37:40.360 --> 00:37:44.704
this was this paper by Ofer
Mazor and Gilles Laurent

00:37:44.704 --> 00:37:46.370
where they did kind
of the PCA analysis.

00:37:46.370 --> 00:37:49.030
And this is in like the locusts,
I think, olfactory bulb.

00:37:49.030 --> 00:37:51.446
And they showed that there
were these kind of trajectories

00:37:51.446 --> 00:37:53.830
in space where a particular
odor was represented

00:37:53.830 --> 00:37:57.400
by maybe different neurons.

00:37:57.400 --> 00:38:00.250
And again, I had a paper in
2008 where I examined this.

00:38:00.250 --> 00:38:02.710
And there's a review
paper by King and Dehaene

00:38:02.710 --> 00:38:03.980
in 2014 about this.

00:38:03.980 --> 00:38:06.830
And there's a lot of
people looking at this now.

00:38:06.830 --> 00:38:10.172
So how can we get at this
kind of thing in decoding?

00:38:10.172 --> 00:38:12.130
What you can do is you
can train the classifier

00:38:12.130 --> 00:38:14.530
at one point in time, and
test it at a point in time

00:38:14.530 --> 00:38:15.790
like we were doing before.

00:38:15.790 --> 00:38:19.260
But you can also test
at other points in time.

00:38:19.260 --> 00:38:22.000
And so what happens is if you
train at a point in time that

00:38:22.000 --> 00:38:24.760
should have the information,
and things are contained

00:38:24.760 --> 00:38:27.880
in a static code where there's
just one pattern, then if you

00:38:27.880 --> 00:38:30.544
test at other points in
time, you should do well.

00:38:30.544 --> 00:38:33.210
Because you capture that pattern
where there's good information,

00:38:33.210 --> 00:38:35.590
you should do well at
other points in time.

00:38:35.590 --> 00:38:38.292
However, if it's a changing
pattern of neural activity,

00:38:38.292 --> 00:38:40.000
then when you train
at one point in time,

00:38:40.000 --> 00:38:43.305
you won't do well at
other points in time.

00:38:43.305 --> 00:38:44.180
Does that make sense?

00:38:50.550 --> 00:38:54.900
So here are the results--

00:38:54.900 --> 00:38:55.950
if that will go away.

00:38:55.950 --> 00:38:57.400
Let me just orient you here.

00:38:57.400 --> 00:38:59.700
So this is the same
experiment, you know,

00:38:59.700 --> 00:39:03.390
time of the first stimulus, time
of the second stimulus, chance.

00:39:03.390 --> 00:39:05.880
This black trace is what we
saw before that I was always

00:39:05.880 --> 00:39:06.870
plotting in red.

00:39:06.870 --> 00:39:09.161
This is the standard decoding
when I trained and tested

00:39:09.161 --> 00:39:11.250
at each point in time.

00:39:11.250 --> 00:39:13.590
This blue trace is
where I train here

00:39:13.590 --> 00:39:16.920
and I tested all
other points in time.

00:39:16.920 --> 00:39:19.530
So if it's the case that
there's one pattern coding

00:39:19.530 --> 00:39:21.330
the information, what
you're going to find

00:39:21.330 --> 00:39:24.000
is that as soon as that
information becomes present,

00:39:24.000 --> 00:39:26.940
it will fill out
this whole curve.

00:39:26.940 --> 00:39:29.340
Conversely, if it's
changing, what you might see

00:39:29.340 --> 00:39:34.254
is just a localized
information just at one spot.

00:39:34.254 --> 00:39:35.670
So let's take a
look at the movie,

00:39:35.670 --> 00:39:36.878
if that moves out of the way.

00:39:36.878 --> 00:39:39.720
OK, here is the moment of truth.

00:39:39.720 --> 00:39:42.070
Information is rising.

00:39:42.070 --> 00:39:46.845
And what you see in
this second delay period

00:39:46.845 --> 00:39:50.646
is clearly we see this
little peak moving along.

00:39:50.646 --> 00:39:52.020
So it's not that
there's just one

00:39:52.020 --> 00:39:55.884
pattern that
contains information

00:39:55.884 --> 00:39:56.800
at all points in time.

00:39:56.800 --> 00:39:59.070
But in fact, it's a
sequence of patterns

00:39:59.070 --> 00:40:00.864
that each contain
that information.

00:40:07.640 --> 00:40:11.410
So here are the results just
plotted in a different format.

00:40:11.410 --> 00:40:13.570
This is what we call a
temporal cross training

00:40:13.570 --> 00:40:16.010
plot because I train
at one point and test

00:40:16.010 --> 00:40:18.062
at a different point in time.

00:40:18.062 --> 00:40:20.020
So this is the time I'm
testing the classifier.

00:40:20.020 --> 00:40:22.180
This is the time I'm
training the classifier.

00:40:22.180 --> 00:40:23.980
This is the passive
fixation stage,

00:40:23.980 --> 00:40:26.710
so there was no information
in the population.

00:40:26.710 --> 00:40:28.322
And this is just
how I often plot it.

00:40:28.322 --> 00:40:30.280
What you see is there's
this big diagonal band.

00:40:30.280 --> 00:40:31.930
Here you see it's
like widening a bit

00:40:31.930 --> 00:40:36.350
so it might be hitting some
sort of stationary point there.

00:40:36.350 --> 00:40:38.710
But you can see
that clearly there's

00:40:38.710 --> 00:40:40.979
these dynamics happening.

00:40:40.979 --> 00:40:43.145
And we can go and we can
look at individual neurons.

00:40:43.145 --> 00:40:45.700
So these are actually the
three most selective neurons.

00:40:45.700 --> 00:40:48.700
They're not randomly chosen.

00:40:48.700 --> 00:40:51.430
Red is the firing rate
to the non-match trials.

00:40:51.430 --> 00:40:53.380
Blue is the firing rate
to the match trials.

00:40:53.380 --> 00:40:56.910
This neuron has a pretty
wide window of selectivity.

00:40:56.910 --> 00:41:00.160
This other neuron here
has a really small window.

00:41:00.160 --> 00:41:02.650
There's just this little blip
where it's more selective

00:41:02.650 --> 00:41:05.652
or has a higher firing rate to
not match compared to match.

00:41:05.652 --> 00:41:08.110
And it's these neurons that
have these little kind of blips

00:41:08.110 --> 00:41:10.510
that are giving rise
to that dynamics.

00:41:10.510 --> 00:41:13.420
Here's something else we can
ask about with the paradigm

00:41:13.420 --> 00:41:17.111
of asking coding questions.

00:41:17.111 --> 00:41:18.610
What we're going
to do here is we're

00:41:18.610 --> 00:41:21.007
going to try a bunch of
different classifiers.

00:41:21.007 --> 00:41:22.840
And here, you know,
these are some questions

00:41:22.840 --> 00:41:23.570
that kind of came up.

00:41:23.570 --> 00:41:26.194
But can we tweak the classifier
to understand a little bit more

00:41:26.194 --> 00:41:27.290
about population code.

00:41:27.290 --> 00:41:29.320
So here is a fairly
simple example.

00:41:29.320 --> 00:41:31.702
But I compared three
different classifiers.

00:41:31.702 --> 00:41:33.160
And the question
I wanted to get at

00:41:33.160 --> 00:41:37.330
was, is information coded in the
total activity of a population.

00:41:37.330 --> 00:41:40.630
Or is it coded more so
in the relative activity

00:41:40.630 --> 00:41:42.080
of different neurons.

00:41:42.080 --> 00:41:44.920
So you know, in particular,
in the face patches,

00:41:44.920 --> 00:41:49.900
we see that information of all
neurons increases to faces.

00:41:49.900 --> 00:41:51.625
But if you think
about that from a--

00:41:51.625 --> 00:41:53.500
or maybe not information,
but the firing rate

00:41:53.500 --> 00:41:55.270
increases to all faces.

00:41:55.270 --> 00:41:57.340
But if the firing rate
increases to all faces,

00:41:57.340 --> 00:41:59.800
you've lost dynamic range
and you can't really

00:41:59.800 --> 00:42:02.237
tell what's happening
for individual faces.

00:42:02.237 --> 00:42:03.820
So what I wanted to
know was, how much

00:42:03.820 --> 00:42:06.279
information is coded by this
overall shift versus patterns.

00:42:06.279 --> 00:42:08.903
So what I did here was I used a
Poisson Naive Bayes classifier,

00:42:08.903 --> 00:42:11.950
which takes into account both
the overall magnitude and also

00:42:11.950 --> 00:42:12.940
the patterns.

00:42:12.940 --> 00:42:15.610
I used a classifier
minimum angle

00:42:15.610 --> 00:42:17.740
that took only the
patterns into account.

00:42:17.740 --> 00:42:20.140
And I used a classifier
called the total population

00:42:20.140 --> 00:42:23.200
activity that only took
the average activity

00:42:23.200 --> 00:42:25.440
of the whole population.

00:42:25.440 --> 00:42:28.270
This classifier's pretty
dumb, but in a certain sense,

00:42:28.270 --> 00:42:30.670
it's what fMRI is
doing, just averaging

00:42:30.670 --> 00:42:33.640
all your neurons together.

00:42:33.640 --> 00:42:36.050
So it's a little bit of a proxy.

00:42:36.050 --> 00:42:38.190
There's paper,
also, by Elias Issa

00:42:38.190 --> 00:42:41.255
and Jim DiCarlo where they show
that fMRI is actually fairly--

00:42:41.255 --> 00:42:43.630
or somewhat strongly correlated
with the average activity

00:42:43.630 --> 00:42:45.003
of a whole population.

00:42:47.590 --> 00:42:50.350
So let's see how these
classifiers compare

00:42:50.350 --> 00:42:52.540
to each other to see
where the information is

00:42:52.540 --> 00:42:54.180
being coded in the activity.

00:42:54.180 --> 00:42:58.570
Again, I'm going to use this
study from Doris and Winrich

00:42:58.570 --> 00:43:01.750
where we're going to be looking
at the pose specific phase

00:43:01.750 --> 00:43:03.440
information, just as an example.

00:43:03.440 --> 00:43:05.500
So this is decoding
those 25 individuals

00:43:05.500 --> 00:43:07.160
when they're shown,
trained, and tested

00:43:07.160 --> 00:43:08.350
that exact same head pose.

00:43:11.440 --> 00:43:15.730
And so what we see is we see
that when we use the Poisson

00:43:15.730 --> 00:43:19.030
Naive Bayes classifier that
took the pattern and also

00:43:19.030 --> 00:43:22.030
the total activity
into account, and when

00:43:22.030 --> 00:43:24.830
we used the classifier that took
just the pattern into account,

00:43:24.830 --> 00:43:28.610
the minimum angle, we're
getting similar results.

00:43:28.610 --> 00:43:31.305
So the overall activity
was not really adding much.

00:43:31.305 --> 00:43:33.430
But if you just use the
overall activity by itself,

00:43:33.430 --> 00:43:35.380
it was pretty poor.

00:43:35.380 --> 00:43:37.257
So this is, again,
touching on something

00:43:37.257 --> 00:43:39.340
about what Rebecca said,
when you start averaging,

00:43:39.340 --> 00:43:40.580
you can lose a lot.

00:43:40.580 --> 00:43:43.130
And so you might be blind
to a lot of what's going on

00:43:43.130 --> 00:43:45.190
if you're just using voxels.

00:43:49.060 --> 00:43:53.890
There is reasons to do
invasive recordings.

00:43:53.890 --> 00:43:58.180
All right, and I think this
might be my last point in terms

00:43:58.180 --> 00:43:59.470
of neural coding.

00:43:59.470 --> 00:44:02.830
But this is the question of
the independent neuron code.

00:44:02.830 --> 00:44:05.950
So is there more activity
if you took into account

00:44:05.950 --> 00:44:09.005
the joint activity of all
neurons simultaneously,

00:44:09.005 --> 00:44:11.015
so if you had
simultaneous recordings

00:44:11.015 --> 00:44:13.390
and took that into account,
versus the pseudo populations

00:44:13.390 --> 00:44:16.150
I'm doing where you are
treating each neuron as if they

00:44:16.150 --> 00:44:18.880
were statistically independent.

00:44:18.880 --> 00:44:21.910
And so this is a very,
very simple analysis.

00:44:21.910 --> 00:44:25.450
Here I just did the
decoding in an experiment

00:44:25.450 --> 00:44:27.130
where we had
simultaneous recordings

00:44:27.130 --> 00:44:29.560
and compared it to using
that same data but using

00:44:29.560 --> 00:44:35.050
pseudo populations on that data,
using very simple classifiers.

00:44:35.050 --> 00:44:36.460
And so here are the results.

00:44:36.460 --> 00:44:38.980
What I found was
that in this one case

00:44:38.980 --> 00:44:40.660
there was a little
bit extra information

00:44:40.660 --> 00:44:42.370
in the simultaneous
recordings as

00:44:42.370 --> 00:44:44.534
compared to the
pseudo populations.

00:44:44.534 --> 00:44:47.200
But you know, it wouldn't really
change many of your conclusions

00:44:47.200 --> 00:44:48.158
about what's happening.

00:44:48.158 --> 00:44:50.780
It's like, you know, maybe
a 5% increase or something.

00:44:50.780 --> 00:44:53.590
And then this has been seen
in a lot of the literature.

00:44:53.590 --> 00:44:56.110
This is the question
of temporal precision

00:44:56.110 --> 00:44:58.412
or what is sometimes
called temporal coding.

00:44:58.412 --> 00:45:00.370
What happens, you know,
some of the experiments

00:45:00.370 --> 00:45:03.130
I was using 100 millisecond
bin, sometimes I was using 500.

00:45:03.130 --> 00:45:05.141
What happens when you
change the bin size?

00:45:05.141 --> 00:45:06.890
What happens, this is
pretty clear, again,

00:45:06.890 --> 00:45:08.835
from a lot of studies
that I've done,

00:45:08.835 --> 00:45:11.460
when you increase the bin size,
generally the decoding accuracy

00:45:11.460 --> 00:45:13.350
goes up.

00:45:13.350 --> 00:45:15.270
What you lose is
temporal precision,

00:45:15.270 --> 00:45:17.730
because now you're blurring
over a much bigger area.

00:45:17.730 --> 00:45:21.330
So in terms of your
understanding what's going on,

00:45:21.330 --> 00:45:24.840
you have to find the
right point between having

00:45:24.840 --> 00:45:27.226
a very clear result by having
a larger bin versus you

00:45:27.226 --> 00:45:28.600
caring about the
time information

00:45:28.600 --> 00:45:29.600
and using a smaller bin.

00:45:32.700 --> 00:45:36.356
And I haven't seen that I need
like one millisecond resolution

00:45:36.356 --> 00:45:37.980
or a very complicated
classifier that's

00:45:37.980 --> 00:45:40.440
taking every single spike
time into account to help me.

00:45:40.440 --> 00:45:42.930
But again, I haven't explored
this as fully as I could.

00:45:42.930 --> 00:45:44.700
So it would be
interesting for someone

00:45:44.700 --> 00:45:47.280
to use a method [INAUDIBLE]
that people really

00:45:47.280 --> 00:45:50.940
love to claim that things are
coded in patterns in time.

00:45:50.940 --> 00:45:53.220
You know, if you
want to, go for it.

00:45:53.220 --> 00:45:54.260
Show me it.

00:45:54.260 --> 00:45:55.650
I've got some data available.

00:45:55.650 --> 00:45:58.650
Build a classifier that does
that and we can compare it.

00:45:58.650 --> 00:46:01.950
But I haven't seen it yet.

00:46:01.950 --> 00:46:03.645
So a summary of
the neural coding.

00:46:03.645 --> 00:46:07.290
Decoding allows you to examine
many questions, such as is

00:46:07.290 --> 00:46:08.540
there a compact code.

00:46:08.540 --> 00:46:11.040
So is there just a few neurons
that has all the information.

00:46:11.040 --> 00:46:12.180
Is there a dynamic code.

00:46:12.180 --> 00:46:13.971
So is the pattern of
activity that's coding

00:46:13.971 --> 00:46:16.350
information changing in time.

00:46:16.350 --> 00:46:19.670
Are neurons independent or is
there more information coded

00:46:19.670 --> 00:46:21.990
in their joint activity.

00:46:21.990 --> 00:46:24.010
And what is the
temporal precision.

00:46:24.010 --> 00:46:25.756
And this is, again,
not everything,

00:46:25.756 --> 00:46:27.750
there are many other
questions you could ask.

00:46:30.072 --> 00:46:31.905
Any other questions
about the neural coding?

00:46:37.670 --> 00:46:40.250
Just a few other
things to mention.

00:46:40.250 --> 00:46:43.490
So you know, I was talking all
about, basically, spiking data.

00:46:43.490 --> 00:46:46.880
But you can also do
decoding from MEG data.

00:46:46.880 --> 00:46:49.760
So there was a
great study by Leyla

00:46:49.760 --> 00:46:53.570
where she tried to
decode from MEG signals.

00:46:53.570 --> 00:46:56.170
Here's just one example
from that paper where

00:46:56.170 --> 00:46:59.740
she was trying to decode
which letter of the alphabet,

00:46:59.740 --> 00:47:01.730
or at least 25 of
the 26 letters,

00:47:01.730 --> 00:47:05.660
was shown to a subject, a human
subject in an MEG scanner.

00:47:05.660 --> 00:47:08.360
You know, see is
very nice, you know,

00:47:08.360 --> 00:47:10.522
people are not psychic either.

00:47:10.522 --> 00:47:12.980
And then at the time, slightly
after the stimulus is shown,

00:47:12.980 --> 00:47:14.780
you can decode quite well.

00:47:14.780 --> 00:47:17.030
And things are above chance.

00:47:17.030 --> 00:47:20.684
And then she went on to
examine position invariance

00:47:20.684 --> 00:47:22.850
in different parts of the
brain, the timing of that.

00:47:22.850 --> 00:47:24.590
So you can check out
that paper as well.

00:47:27.290 --> 00:47:34.280
And as Rebecca mentioned,
this kind of approach

00:47:34.280 --> 00:47:35.984
has really taken off in fMRI.

00:47:35.984 --> 00:47:37.400
Here are three
different toolboxes

00:47:37.400 --> 00:47:40.087
you could use if
you're doing fMRI.

00:47:40.087 --> 00:47:42.170
So I wrote a toolbox I
will talk about in a minute

00:47:42.170 --> 00:47:44.400
to do neural decoding, and
I recommend it for that.

00:47:44.400 --> 00:47:46.377
But if you're going
to do fMRI decoding,

00:47:46.377 --> 00:47:48.710
you probably are better off
using one of these toolboxes

00:47:48.710 --> 00:47:51.660
because they have certain
things that are fMRI specific,

00:47:51.660 --> 00:47:54.470
such as mapping back to voxels
that my toolbox doesn't have.

00:47:54.470 --> 00:47:56.590
Although you could,
in principle,

00:47:56.590 --> 00:47:58.580
throw fMRI data into
my toolbox as well.

00:48:02.440 --> 00:48:05.100
And then all these studies
so far I've mentioned

00:48:05.100 --> 00:48:08.760
have had kind of structure
where every trial is exactly

00:48:08.760 --> 00:48:12.630
the same length, as
Tyler pointed out.

00:48:12.630 --> 00:48:14.040
And if you wanted
to do something

00:48:14.040 --> 00:48:16.500
where it wasn't that structured
that well, such as decoding

00:48:16.500 --> 00:48:19.050
from a rat running around a maze
where it wasn't always doing

00:48:19.050 --> 00:48:21.860
things in the same
amount of time,

00:48:21.860 --> 00:48:27.060
there's a toolbox that came
out of Emory Brown's lab that

00:48:27.060 --> 00:48:28.710
should hopefully
enable you to do some

00:48:28.710 --> 00:48:30.108
of those kinds of analyses.

00:48:33.034 --> 00:48:35.450
All right, let me just briefly
talk about some limitations

00:48:35.450 --> 00:48:39.610
to decoding, just like Rebecca
did with the downer at the end.

00:48:39.610 --> 00:48:43.710
So some limitations are, this
is a hypothesis-based method.

00:48:43.710 --> 00:48:46.702
So we have specific questions
in mind that we want to test.

00:48:46.702 --> 00:48:49.160
And then we can assess whether
those questions are answered

00:48:49.160 --> 00:48:52.264
or not, to a certain degree.

00:48:52.264 --> 00:48:54.680
So that's kind of a good thing
but it's also a down thing.

00:48:54.680 --> 00:48:56.750
Like if we didn't think
about the right question,

00:48:56.750 --> 00:48:58.160
then we're not going to see it.

00:48:58.160 --> 00:48:59.300
So there could be
a lot happening

00:48:59.300 --> 00:49:01.883
in our neural activity that we
just didn't think to ask about.

00:49:04.250 --> 00:49:05.967
And so unsupervised
learning methods

00:49:05.967 --> 00:49:07.050
might get at some of that.

00:49:07.050 --> 00:49:09.560
And you could see about how
much is the variable of interest

00:49:09.560 --> 00:49:11.976
you're interested in, accounting
for the total variability

00:49:11.976 --> 00:49:14.090
in a population.

00:49:14.090 --> 00:49:16.070
Also, I hinted at this
throughout the talk,

00:49:16.070 --> 00:49:19.337
just because information is
present doesn't mean it's used.

00:49:19.337 --> 00:49:21.920
The back of the head stuff might
be an example of that or not,

00:49:21.920 --> 00:49:22.790
I don't know.

00:49:22.790 --> 00:49:24.710
But you just have to
interpret the results

00:49:24.710 --> 00:49:27.110
and don't know the
information there.

00:49:27.110 --> 00:49:29.650
Therefore, this is the
brain region doing x.

00:49:29.650 --> 00:49:32.510
A lot of stuff can
kind of sneak in.

00:49:32.510 --> 00:49:36.810
Timing information can be
also really interesting.

00:49:36.810 --> 00:49:38.200
I've been exploring this summer.

00:49:38.200 --> 00:49:41.390
So if you can know the relative
timing, when information

00:49:41.390 --> 00:49:43.010
is in one brain
region versus another,

00:49:43.010 --> 00:49:46.440
it can tell you a lot about
kind of the flow of information

00:49:46.440 --> 00:49:48.920
the computation that brain
regions might be doing.

00:49:48.920 --> 00:49:53.930
So I think that's another very
promising area to explore.

00:49:53.930 --> 00:49:57.290
Also, decoding kind of focuses
on the computational level

00:49:57.290 --> 00:50:00.202
or algorithmic level, or
really neural representations

00:50:00.202 --> 00:50:01.910
if you thought about
Marr's three levels.

00:50:01.910 --> 00:50:04.535
It doesn't talk about this kind
of implementational mechanistic

00:50:04.535 --> 00:50:05.240
level.

00:50:05.240 --> 00:50:07.670
So [INAUDIBLE] it's not
one thing it can do.

00:50:07.670 --> 00:50:09.789
Now if you have the flow
of information going

00:50:09.789 --> 00:50:12.080
through an area and you
understand that well and what's

00:50:12.080 --> 00:50:13.580
being represented,
I think you might

00:50:13.580 --> 00:50:16.580
be able to back out some of
these mechanisms or processes

00:50:16.580 --> 00:50:18.020
of how that can be built up.

00:50:18.020 --> 00:50:22.864
But in and of itself, decoding
doesn't give you that.

00:50:22.864 --> 00:50:24.530
Also, decoding methods,
computationally,

00:50:24.530 --> 00:50:25.820
can be intensive.

00:50:25.820 --> 00:50:27.645
can take up to an hour.

00:50:27.645 --> 00:50:29.270
If you do something
really complicated,

00:50:29.270 --> 00:50:31.832
it can take you a week to
run something very elaborate.

00:50:31.832 --> 00:50:33.290
You know, sometimes
it can be quick

00:50:33.290 --> 00:50:35.040
and you can do it
in a few minutes,

00:50:35.040 --> 00:50:38.000
but it's certainly a lot
slower than doing something

00:50:38.000 --> 00:50:41.154
like an activity index where
you're done in two seconds

00:50:41.154 --> 00:50:43.070
and then you have the
wrong answer right away.

00:50:48.410 --> 00:50:50.780
Let me just spend like
five more minutes talking

00:50:50.780 --> 00:50:52.580
about this toolbox
and then you can all

00:50:52.580 --> 00:50:54.990
go work on your projects
and do what you want to do.

00:50:54.990 --> 00:50:57.260
So this is a toolbox I made
called the neural decoding

00:50:57.260 --> 00:50:57.779
toolbox.

00:50:57.779 --> 00:50:59.320
There's a paper
about it in Frontiers

00:50:59.320 --> 00:51:01.505
in Neuroinfomatics in 2013.

00:51:01.505 --> 00:51:04.130
And the whole point of it was to
try to make it easy for people

00:51:04.130 --> 00:51:07.400
to do these analyses
because [INAUDIBLE]..

00:51:07.400 --> 00:51:10.550
And so basically, here
is like six lines of code

00:51:10.550 --> 00:51:13.599
that if you ran it would do
one of those analyses for you.

00:51:13.599 --> 00:51:15.140
And not only is it
six lines of code,

00:51:15.140 --> 00:51:17.972
but it's almost literally these
exact same six lines of code.

00:51:17.972 --> 00:51:19.430
The only thing
you'd, like, replace

00:51:19.430 --> 00:51:22.850
would be your data rather
than this data file.

00:51:22.850 --> 00:51:31.393
And so what you can do,
the whole idea behind it

00:51:31.393 --> 00:51:33.660
is it's a kind of open
science idea, you know,

00:51:33.660 --> 00:51:36.299
I want more transparency
so I'm sharing my code.

00:51:36.299 --> 00:51:38.840
If you use my code, ultimately,
if you could share your data,

00:51:38.840 --> 00:51:40.970
that would be great
because I think

00:51:40.970 --> 00:51:42.380
I wouldn't have been able
to develop any of this stuff

00:51:42.380 --> 00:51:43.940
if people hadn't
shared data with me.

00:51:43.940 --> 00:51:46.670
I think we'll make a lot
more progress in science

00:51:46.670 --> 00:51:49.877
if we're open and share.

00:51:49.877 --> 00:51:50.960
There you go, I'm a hippy.

00:51:55.920 --> 00:52:01.010
And here's the website for
the toolbox, www.readout.info.

00:52:01.010 --> 00:52:03.570
Just talk briefly a little
bit more about the toolbox.

00:52:03.570 --> 00:52:08.670
The way it was designed is
around four abstract classes.

00:52:08.670 --> 00:52:11.759
So these are kind of
major pieces or objects

00:52:11.759 --> 00:52:13.300
that you can kind
of swap in and out.

00:52:13.300 --> 00:52:14.810
They're like
components that allow

00:52:14.810 --> 00:52:16.950
you to do different things.

00:52:16.950 --> 00:52:20.100
So for example, one of the
components is a data source.

00:52:20.100 --> 00:52:23.487
This creates the training
and test set of data.

00:52:23.487 --> 00:52:25.320
You can separate that
out in different ways,

00:52:25.320 --> 00:52:28.430
like there's just a standard
one but you can swap it out

00:52:28.430 --> 00:52:32.600
to do that invariance
or abstract analysis.

00:52:32.600 --> 00:52:34.760
Or you can do things
like, I guess, change

00:52:34.760 --> 00:52:38.310
the different binning schemes
within that piece of code.

00:52:38.310 --> 00:52:40.310
So that's one component
you can swap in and out.

00:52:40.310 --> 00:52:42.560
Another one are
these preprocessors.

00:52:42.560 --> 00:52:45.140
What they do is they apply
pre-processing to your training

00:52:45.140 --> 00:52:47.690
data, and then use
those parameters that

00:52:47.690 --> 00:52:51.410
were learned on the training
set to do some mechanics

00:52:51.410 --> 00:52:53.370
to the test set as well.

00:52:53.370 --> 00:52:55.720
So for example, when I was
selecting the best neurons,

00:52:55.720 --> 00:52:58.490
I used a preprocessor
that just eliminated--

00:52:58.490 --> 00:53:00.620
found good neurons
in the training set,

00:53:00.620 --> 00:53:02.510
just used those, and
then also eliminated

00:53:02.510 --> 00:53:04.030
those neurons in the test set.

00:53:04.030 --> 00:53:05.446
And so there are
different, again,

00:53:05.446 --> 00:53:07.400
components you can swap
in and out with that.

00:53:07.400 --> 00:53:10.640
An obvious component you can
swap in and out, classifiers.

00:53:10.640 --> 00:53:13.040
You could throw in a classifier
that takes correlations

00:53:13.040 --> 00:53:14.502
into account or doesn't.

00:53:14.502 --> 00:53:15.710
Or do whatever you want here.

00:53:15.710 --> 00:53:18.860
You know, use some highly
nonlinear or somewhat nonlinear

00:53:18.860 --> 00:53:23.100
thing and see is the
brain doing it that way.

00:53:23.100 --> 00:53:26.930
And there's this final piece
called cross validator.

00:53:26.930 --> 00:53:29.330
It basically runs the whole
cross validation loop.

00:53:29.330 --> 00:53:31.520
It pulls data from
the data source,

00:53:31.520 --> 00:53:33.270
creating training and test sets.

00:53:33.270 --> 00:53:35.190
It applies the
future preprocessor.

00:53:35.190 --> 00:53:37.437
It trains the classifier
and reports the results.

00:53:37.437 --> 00:53:40.020
Generally, I've only written one
of these and it's pretty long

00:53:40.020 --> 00:53:41.010
and does a lot of
different things,

00:53:41.010 --> 00:53:42.759
like gives you different
types of results.

00:53:42.759 --> 00:53:44.610
So not just is there
information loss

00:53:44.610 --> 00:53:46.600
but gives you mutual information
and all these other things.

00:53:46.600 --> 00:53:48.808
But again, if you wanted
to, you could expand on that

00:53:48.808 --> 00:53:50.740
and do the cross-validation
in different ways.

00:53:54.690 --> 00:53:57.770
If you wanted to get
started on your own data,

00:53:57.770 --> 00:54:00.570
you just have to put your data
in a fairly simple format.

00:54:00.570 --> 00:54:03.770
It's a format I
call raster format.

00:54:03.770 --> 00:54:05.120
It's just in a raster.

00:54:05.120 --> 00:54:06.770
So you just have
trials going this way.

00:54:06.770 --> 00:54:07.604
Time going this way.

00:54:07.604 --> 00:54:09.061
And if it was
spikes, it would just

00:54:09.061 --> 00:54:11.690
be the ones and zeros that
happen on the different trials.

00:54:11.690 --> 00:54:14.390
If this was MEG data,
you'd have your MEG

00:54:14.390 --> 00:54:16.350
actual continuous
values in there.

00:54:16.350 --> 00:54:19.240
Again, trials and time.

00:54:19.240 --> 00:54:20.620
Or fMRI or whatever.

00:54:20.620 --> 00:54:25.010
fMRI might just be one vector
if you didn't have any time.

00:54:25.010 --> 00:54:26.970
And so again, this
is just blown up.

00:54:26.970 --> 00:54:27.890
This was trials.

00:54:27.890 --> 00:54:28.700
This is time.

00:54:28.700 --> 00:54:31.970
You can have the little
ones where a spike occurred.

00:54:31.970 --> 00:54:33.800
And then what corresponds
to each trial,

00:54:33.800 --> 00:54:36.500
you need to give the
labels about what happened.

00:54:36.500 --> 00:54:39.220
So you'd have just something
called raster labels.

00:54:39.220 --> 00:54:39.987
It's a structure.

00:54:39.987 --> 00:54:42.320
And you'd say, OK, on the
first trial I showed a flower.

00:54:42.320 --> 00:54:43.528
Second trial I showed a face.

00:54:43.528 --> 00:54:45.677
Third trial I showed a couch.

00:54:45.677 --> 00:54:47.760
And these could be numbers
or whatever you wanted.

00:54:47.760 --> 00:54:49.490
But it's just indicating
different things are

00:54:49.490 --> 00:54:50.810
happening in different trials.

00:54:50.810 --> 00:54:53.910
And you can also have
multiple ones of these.

00:54:53.910 --> 00:54:56.120
So if I want to decode
position, I also

00:54:56.120 --> 00:54:57.470
have upper, middle, lower.

00:54:57.470 --> 00:54:59.928
And so you can use the same
data and decode different types

00:54:59.928 --> 00:55:02.060
of things from that data set.

00:55:02.060 --> 00:55:03.950
And then there's this
final information

00:55:03.950 --> 00:55:05.160
that's kind of optional.

00:55:05.160 --> 00:55:06.530
It's just raster site info.

00:55:06.530 --> 00:55:09.200
So for each site you could
have just meta information.

00:55:09.200 --> 00:55:12.500
This is the recording
I made on January 14

00:55:12.500 --> 00:55:15.470
and it was recorded from IT.

00:55:18.054 --> 00:55:19.970
So you just define these
three things and then

00:55:19.970 --> 00:55:23.040
the toolbox plug and play.

00:55:23.040 --> 00:55:26.360
So with some experience you
should be able to do that.

00:55:26.360 --> 00:55:27.540
So that's it.

00:55:27.540 --> 00:55:30.410
I want to thank the Center
for Brains, Minds, Machines

00:55:30.410 --> 00:55:32.180
for funding this work.

00:55:32.180 --> 00:55:35.480
And all my collaborators who
collected the data or who

00:55:35.480 --> 00:55:38.180
worked with me to analyze it.

00:55:38.180 --> 00:55:41.840
And there is the
URL for the toolbox

00:55:41.840 --> 00:55:44.290
if you want to download it.

