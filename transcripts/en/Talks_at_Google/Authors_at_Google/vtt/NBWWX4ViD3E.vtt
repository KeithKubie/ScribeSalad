WEBVTT
Kind: captions
Language: en

00:00:07.310 --> 00:00:10.750
JEFF KAUFFMAN: This afternoon
we have Peter Singer with us

00:00:10.750 --> 00:00:14.275
to talk about his new book,
"The Most Good You Can Do."

00:00:14.275 --> 00:00:15.900
This is a book about
effective altruism

00:00:15.900 --> 00:00:21.500
which is the idea that you
should use your time and money

00:00:21.500 --> 00:00:24.370
to try and have as much of a
positive impact on the world

00:00:24.370 --> 00:00:26.680
as you can.

00:00:26.680 --> 00:00:29.050
Peter Singer has been
interested in this sort of thing

00:00:29.050 --> 00:00:31.480
since at least the
early '70s when

00:00:31.480 --> 00:00:34.560
he wrote "Famine, Affluence,
and Morality" which

00:00:34.560 --> 00:00:36.206
is a pretty well-known paper.

00:00:36.206 --> 00:00:37.330
It's been very influential.

00:00:37.330 --> 00:00:39.920
It's influenced a lot
of people, including me.

00:00:39.920 --> 00:00:42.400
Its basic idea is if
you came across someone

00:00:42.400 --> 00:00:44.150
in front of you who
very much needed help,

00:00:44.150 --> 00:00:45.479
you would help them.

00:00:45.479 --> 00:00:47.520
There are lots of people
who need help elsewhere.

00:00:47.520 --> 00:00:48.580
Why don't we help them?

00:00:48.580 --> 00:00:49.770
Perhaps we should.

00:00:49.770 --> 00:00:52.220
And expanding this
into effective altruism

00:00:52.220 --> 00:00:55.240
has happened sort
of over decades

00:00:55.240 --> 00:00:56.780
with a bunch of
ideas from elsewhere

00:00:56.780 --> 00:00:59.560
as well which Peter
will get into.

00:00:59.560 --> 00:01:04.430
He's currently a professor
of bioethics at Princeton

00:01:04.430 --> 00:01:07.160
and also, he's at the
University of Melbourne.

00:01:07.160 --> 00:01:09.330
And now he's here, so
please welcome Peter Singer.

00:01:15.005 --> 00:01:16.380
PETER SINGER:
Thank you very much

00:01:16.380 --> 00:01:19.670
for that introduction, Jeff.

00:01:19.670 --> 00:01:24.420
I'm very happy to
be here with you.

00:01:24.420 --> 00:01:27.780
So as Jeff said,
this is an issue

00:01:27.780 --> 00:01:31.370
that I've been interested
in for a very long time.

00:01:31.370 --> 00:01:37.240
And last night I was speaking
to a lecture theater in Harvard

00:01:37.240 --> 00:01:39.910
that was organized by the
Harvard Effective Altruism

00:01:39.910 --> 00:01:44.360
Group and I was introduced
by Josh Greene who

00:01:44.360 --> 00:01:50.550
is a professor who works in
psychology of-- essentially

00:01:50.550 --> 00:01:51.550
moral psychology.

00:01:51.550 --> 00:01:53.030
You could say
psychology about how

00:01:53.030 --> 00:01:54.950
we develop our moral beliefs.

00:01:54.950 --> 00:01:57.920
But he started off as
a philosophy student,

00:01:57.920 --> 00:02:03.010
he studied philosophy as an
undergraduate at Harvard.

00:02:03.010 --> 00:02:06.220
And he actually put
something very nicely

00:02:06.220 --> 00:02:09.720
that I'm going to repeat,
too, in terms of what's

00:02:09.720 --> 00:02:12.480
happened with the thought
that goes back to the article

00:02:12.480 --> 00:02:14.320
that Jeff just mentioned.

00:02:14.320 --> 00:02:17.690
It was published
in the early '70s.

00:02:17.690 --> 00:02:20.330
That article has been very
successful in one sense, that

00:02:20.330 --> 00:02:25.190
is it's one of the most
reprinted philosophy articles

00:02:25.190 --> 00:02:28.930
in anthologies that are used
for teaching philosophy.

00:02:28.930 --> 00:02:30.930
And many, many people
have come up to me

00:02:30.930 --> 00:02:32.600
and said, oh, I
read your article

00:02:32.600 --> 00:02:35.210
when I was doing an
undergraduate philosophy

00:02:35.210 --> 00:02:35.710
course.

00:02:38.490 --> 00:02:42.340
But as Josh put
it last night, it

00:02:42.340 --> 00:02:46.570
was generally taught in the
sense of here's a challenge

00:02:46.570 --> 00:02:47.070
to you.

00:02:47.070 --> 00:02:49.280
This article is obviously wrong.

00:02:49.280 --> 00:02:52.670
Find out why it's wrong,
tell me why it's wrong

00:02:52.670 --> 00:02:57.660
because the claims that it
makes or the conclusions that it

00:02:57.660 --> 00:03:00.610
draws are just too demanding.

00:03:00.610 --> 00:03:04.420
It has to be wrong, all right,
because essentially the idea is

00:03:04.420 --> 00:03:07.740
you would rescue-- here's
this child drowning in a pond,

00:03:07.740 --> 00:03:09.330
you can easily
rescue this child.

00:03:09.330 --> 00:03:11.920
No great risk to you,
it's just a shallow pond.

00:03:11.920 --> 00:03:14.390
But you're wearing some really
expensive clothing that's

00:03:14.390 --> 00:03:17.870
going to get ruined, so there's
going to be some cost to you.

00:03:17.870 --> 00:03:21.120
Not a life-changing
cost, but some cost you

00:03:21.120 --> 00:03:24.050
in order to save that child.

00:03:24.050 --> 00:03:25.732
OK, so everybody
in the-- everybody

00:03:25.732 --> 00:03:27.190
says, of course
I'd save the child.

00:03:27.190 --> 00:03:29.560
You know, what can you
compare some clothing

00:03:29.560 --> 00:03:32.370
with a child's life?

00:03:32.370 --> 00:03:36.560
And then I say, OK, but really
you're in that situation

00:03:36.560 --> 00:03:39.320
right now because
there are people dying

00:03:39.320 --> 00:03:44.930
from preventable poverty-related
diseases in the world

00:03:44.930 --> 00:03:50.692
but you can save-- at not
very great cost actually.

00:03:50.692 --> 00:03:52.275
You could debate the
point of the cost

00:03:52.275 --> 00:03:54.237
and whether it is
comparable to clothing

00:03:54.237 --> 00:03:55.320
that you might be wearing.

00:03:55.320 --> 00:03:58.820
That might depend on your
penchant for designer clothing,

00:03:58.820 --> 00:04:03.130
I guess, but
something like that.

00:04:03.130 --> 00:04:06.000
And then people say,
OK, but once you've

00:04:06.000 --> 00:04:08.720
done that, once you've
let's say donated

00:04:08.720 --> 00:04:12.180
the cost of one pair of
expensive shoes or a suit

00:04:12.180 --> 00:04:14.070
to save one child.

00:04:14.070 --> 00:04:16.779
Unlike the pond case,
there are more children

00:04:16.779 --> 00:04:19.560
so it seems that you
ought to do it again.

00:04:19.560 --> 00:04:21.670
And again and again and
again, and where do you

00:04:21.670 --> 00:04:25.710
stop until you reach the
point of marginal utility?

00:04:25.710 --> 00:04:29.980
That is the point at which
if you gave more then you'd

00:04:29.980 --> 00:04:32.470
be lowering yourself to the
level of the poor person

00:04:32.470 --> 00:04:35.150
you're helping or you'd be
doing as much harm to yourself

00:04:35.150 --> 00:04:37.080
or as much risk
to yourself as you

00:04:37.080 --> 00:04:41.150
would be alleviating in terms of
the person that you're helping.

00:04:41.150 --> 00:04:45.745
So that's the highly demanding
conclusion that can't be right,

00:04:45.745 --> 00:04:50.990
and that's why I think the
article was taught in that way.

00:04:50.990 --> 00:04:55.380
Seems plausible, but
what's wrong with it?

00:04:55.380 --> 00:05:00.810
But what Josh Greene
said yesterday

00:05:00.810 --> 00:05:06.480
is that an interesting thing has
happened in the last few years

00:05:06.480 --> 00:05:11.070
and that is that the reaction
that there's something

00:05:11.070 --> 00:05:16.040
wrong with this idea and
we have to find out what

00:05:16.040 --> 00:05:18.800
has switched at least with a
substantial number of people

00:05:18.800 --> 00:05:24.680
to saying no, this
is really right.

00:05:24.680 --> 00:05:27.880
Yes, it's very
demanding and maybe I'm

00:05:27.880 --> 00:05:31.470
not going to be able to go
all the way that this says

00:05:31.470 --> 00:05:34.570
I should be going,
but that doesn't

00:05:34.570 --> 00:05:38.170
mean that it wouldn't
be right to do that.

00:05:38.170 --> 00:05:41.620
That in some sense, anyway,
that's what we ought to do.

00:05:41.620 --> 00:05:46.630
And we should at least be
trying to do as much as we can

00:05:46.630 --> 00:05:49.480
in that direction.

00:05:49.480 --> 00:05:53.070
And I think it is
true and I find

00:05:53.070 --> 00:05:57.900
that have to say quite exciting
that there is now this emerging

00:05:57.900 --> 00:06:04.260
movement known as
effective altruism which

00:06:04.260 --> 00:06:05.750
is thinking along these lines.

00:06:05.750 --> 00:06:09.190
I won't say it's
thinking exactly

00:06:09.190 --> 00:06:12.740
in terms of "Famine, Affluence,
and Morality" article,

00:06:12.740 --> 00:06:16.780
but it's certainly
thinking in some way that

00:06:16.780 --> 00:06:20.340
is in that direction.

00:06:20.340 --> 00:06:23.760
So essentially what people
who are effective altruists

00:06:23.760 --> 00:06:30.650
are saying is I want
to do to something

00:06:30.650 --> 00:06:34.360
significant that is for
the good of the world that

00:06:34.360 --> 00:06:37.370
is going to make the
world a better place.

00:06:37.370 --> 00:06:42.650
And I want that to be an
important part of my life.

00:06:42.650 --> 00:06:44.190
Doesn't mean that
I'm going to go

00:06:44.190 --> 00:06:47.550
to the point of
marginal utility.

00:06:47.550 --> 00:06:48.660
Most of us are not saints.

00:06:48.660 --> 00:06:52.460
I don't claim to have
reached that point myself,

00:06:52.460 --> 00:06:57.020
but I do think that making that
an important part of your life

00:06:57.020 --> 00:07:01.010
is something that I want to do.

00:07:01.010 --> 00:07:07.330
So that's the altruism
part of effective altruism.

00:07:07.330 --> 00:07:09.950
And then the
effective part of it

00:07:09.950 --> 00:07:14.770
is it's not enough
just to say, I'm

00:07:14.770 --> 00:07:17.280
going to make the
world a better place.

00:07:17.280 --> 00:07:19.340
If I'm going to
put part of my life

00:07:19.340 --> 00:07:24.170
into thinking about
thinking altruistically

00:07:24.170 --> 00:07:26.920
about working for
some good cause,

00:07:26.920 --> 00:07:29.920
I want to actually make the
biggest possible difference

00:07:29.920 --> 00:07:35.150
that I can with the resources
that I have available

00:07:35.150 --> 00:07:39.250
and that I'm prepared to put
into this activity, whatever

00:07:39.250 --> 00:07:41.310
activity it happens to be.

00:07:41.310 --> 00:07:42.810
Might be donating
money, it might

00:07:42.810 --> 00:07:45.690
be donating my
skills and my time,

00:07:45.690 --> 00:07:49.980
might be some combination
of those things.

00:07:49.980 --> 00:07:53.370
So that's the
effectiveness part of it.

00:07:53.370 --> 00:07:56.860
We want to use our
abilities to reason

00:07:56.860 --> 00:08:01.470
and to think about things,
assess the evidence in order

00:08:01.470 --> 00:08:09.470
to make our altruism as
highly effective as possible.

00:08:09.470 --> 00:08:13.820
So to say this is now
clearly an emerging movement.

00:08:13.820 --> 00:08:17.290
There was a pretty large lecture
theater in the Science Center

00:08:17.290 --> 00:08:19.945
that was packed.

00:08:19.945 --> 00:08:22.320
I'm told that the [? Battle ?]
Theater where I'm speaking

00:08:22.320 --> 00:08:25.950
tonight is a sold-out event.

00:08:25.950 --> 00:08:28.190
And I've been
doing a little book

00:08:28.190 --> 00:08:30.970
tour, was in the San Francisco
Bay area and up in Seattle

00:08:30.970 --> 00:08:33.059
I also had full houses there.

00:08:33.059 --> 00:08:35.470
So there's clearly a
lot of interest in this

00:08:35.470 --> 00:08:38.080
and there are EA groups
as they're known,

00:08:38.080 --> 00:08:42.270
Effective Altruism groups, on
a number of different campuses

00:08:42.270 --> 00:08:43.559
around the country.

00:08:43.559 --> 00:08:46.780
And it's not specifically
a United States thing.

00:08:46.780 --> 00:08:49.670
In fact, I think if you
said where did this begin,

00:08:49.670 --> 00:08:53.610
I would say it began
at Oxford in England,

00:08:53.610 --> 00:08:57.620
but it certainly exists in quite
a number of other countries.

00:08:57.620 --> 00:09:00.410
There are groups
in Australia, where

00:09:00.410 --> 00:09:02.600
I spend part of each
year, but there are also

00:09:02.600 --> 00:09:05.470
groups in Switzerland
and Germany

00:09:05.470 --> 00:09:11.060
and Czech Republic and a
number of other places.

00:09:11.060 --> 00:09:12.820
So it's an interesting
emerging movement

00:09:12.820 --> 00:09:17.060
and there's quite a
lot online about it.

00:09:17.060 --> 00:09:19.670
And it's all fairly
new so there's things

00:09:19.670 --> 00:09:22.630
like the Wikipedia page
on effective altruism

00:09:22.630 --> 00:09:25.090
is only about two years old.

00:09:25.090 --> 00:09:28.440
And that I guess gives you a
sense of when people thought,

00:09:28.440 --> 00:09:31.820
well there ought to be
something more out there.

00:09:31.820 --> 00:09:38.020
So let me just say a
little bit about how it got

00:09:38.020 --> 00:09:42.510
started, because I said I
wrote this early article

00:09:42.510 --> 00:09:46.410
and I've been writing a bit
more about it in recent years.

00:09:46.410 --> 00:09:50.310
But I can't really take
credit for starting it

00:09:50.310 --> 00:09:51.985
in an organizational sense.

00:09:54.520 --> 00:09:56.340
To the best of my
understanding, the person

00:09:56.340 --> 00:10:00.050
who took the initial steps
was a philosopher at Oxford

00:10:00.050 --> 00:10:02.780
called Toby Ord.

00:10:02.780 --> 00:10:05.890
Toby told me I think
back maybe in 2007

00:10:05.890 --> 00:10:08.990
that he was thinking
of organizing something

00:10:08.990 --> 00:10:14.900
at Oxford to try to
let more people know

00:10:14.900 --> 00:10:19.870
about how effective
their giving could be.

00:10:19.870 --> 00:10:26.470
And what Toby did he was then a
PhD student at Oxford expecting

00:10:26.470 --> 00:10:29.180
to have an academic career--
which things are basically

00:10:29.180 --> 00:10:31.840
going on course, he's a research
fellow in philosophy at Oxford

00:10:31.840 --> 00:10:33.300
now.

00:10:33.300 --> 00:10:36.890
And he decided he would and
he was living on a Graduate

00:10:36.890 --> 00:10:43.370
Studentship so he decided to
work out how much money he was

00:10:43.370 --> 00:10:46.100
likely to earn over
his academic career

00:10:46.100 --> 00:10:48.690
and then assume that
he stayed roughly

00:10:48.690 --> 00:10:53.030
on the level of
personal expenditure

00:10:53.030 --> 00:10:55.400
that was his Graduate
Studentship, because he felt

00:10:55.400 --> 00:10:58.910
that was enough to cover
what was important to him.

00:10:58.910 --> 00:11:01.516
Well, adjusted for
inflation of course,

00:11:01.516 --> 00:11:03.640
maybe put it up a little
bit, but not too much more

00:11:03.640 --> 00:11:06.470
than that, and
see how much money

00:11:06.470 --> 00:11:10.390
he would be able to donate
to effective charities.

00:11:10.390 --> 00:11:15.720
And then when he did that sum,
so what his total earnings

00:11:15.720 --> 00:11:18.970
would be, deduct the
studentship equivalent

00:11:18.970 --> 00:11:22.070
for the rest of
his life and then

00:11:22.070 --> 00:11:24.800
divide that by the
cost of something

00:11:24.800 --> 00:11:26.920
that an effective
charity might do.

00:11:26.920 --> 00:11:33.430
An example he took was to either
treat or prevent blindness.

00:11:33.430 --> 00:11:37.660
So there's a lot of people
who are blind in the world

00:11:37.660 --> 00:11:41.210
because they can't afford
cataract surgery which

00:11:41.210 --> 00:11:45.230
is a very simple surgery that
everyone in the United States

00:11:45.230 --> 00:11:47.300
who is blind
because of cataracts

00:11:47.300 --> 00:11:49.310
would get either from
their health insurance

00:11:49.310 --> 00:11:53.050
or from Medicare
when they reach 65,

00:11:53.050 --> 00:11:55.520
Medicaid if they were poorer.

00:11:55.520 --> 00:11:58.294
But in developing
countries, there

00:11:58.294 --> 00:12:00.710
are millions of people who are
blind because of cataracts.

00:12:00.710 --> 00:12:04.400
There are also millions more
people who have become blind

00:12:04.400 --> 00:12:09.250
because of a condition
called trachoma caused

00:12:09.250 --> 00:12:11.270
by a microorganism
that gets into your eye

00:12:11.270 --> 00:12:14.470
when you're quite young and
gradually develops and causes

00:12:14.470 --> 00:12:18.140
blindness and can again be
very inexpensively treated.

00:12:18.140 --> 00:12:22.260
So putting some
reasonably good estimates

00:12:22.260 --> 00:12:25.870
of the cost of these
treatments, Toby

00:12:25.870 --> 00:12:29.420
calculated that if he did
live on the equivalent

00:12:29.420 --> 00:12:32.880
of a studentship and donated it
to one of these organizations,

00:12:32.880 --> 00:12:39.120
he could throughout his life
either prevent 80,000 people

00:12:39.120 --> 00:12:42.980
from becoming blind or
treat 80,000 blind people

00:12:42.980 --> 00:12:44.940
and restore their sight.

00:12:44.940 --> 00:12:49.340
And he thought that was
quite an amazing figure.

00:12:49.340 --> 00:12:53.130
He thought that would be an
incredibly important thing

00:12:53.130 --> 00:12:55.880
to do.

00:12:55.880 --> 00:12:59.150
So he thought firstly
that he ought to do that

00:12:59.150 --> 00:13:01.700
and he took a pledge
and made it public

00:13:01.700 --> 00:13:06.110
that he would live on
something not that much

00:13:06.110 --> 00:13:08.720
more than his
graduate studentship

00:13:08.720 --> 00:13:10.670
and donate the rest.

00:13:10.670 --> 00:13:12.390
And he's doing that.

00:13:12.390 --> 00:13:14.260
But he also set
up an organization

00:13:14.260 --> 00:13:17.970
called Giving What
We Can to provide

00:13:17.970 --> 00:13:21.100
this information for people, to
let people know which charities

00:13:21.100 --> 00:13:24.470
were highly effective at doing
not just treating blindness

00:13:24.470 --> 00:13:29.020
but a variety of other
things, for example,

00:13:29.020 --> 00:13:32.000
preventing child deaths from
malaria which is also something

00:13:32.000 --> 00:13:34.970
you can do quite
inexpensively by distributing

00:13:34.970 --> 00:13:37.940
bed nets in areas where
people don't have them

00:13:37.940 --> 00:13:40.570
and where they're
prone to malaria,

00:13:40.570 --> 00:13:45.290
or getting rid of intestinal
parasites in children,

00:13:45.290 --> 00:13:47.800
deworming them, which
isn't lifesaving

00:13:47.800 --> 00:13:50.840
because the parasites
aren't going to kill them

00:13:50.840 --> 00:13:55.130
but has been shown to be
highly cost effective in terms

00:13:55.130 --> 00:14:00.230
of their achievements at school,
both actually staying at school

00:14:00.230 --> 00:14:03.540
and doing better at school,

00:14:03.540 --> 00:14:07.340
because the parasites
obviously weaken them,

00:14:07.340 --> 00:14:10.740
make them more tired,
less energy, and so on.

00:14:10.740 --> 00:14:13.140
And again it's an extremely
inexpensive treatment,

00:14:13.140 --> 00:14:18.930
about $0.50 a year to get rid
of intestinal worms in children.

00:14:18.930 --> 00:14:22.710
So publicizing those
sorts of things.

00:14:22.710 --> 00:14:26.870
And I think that was probably
the first real effective

00:14:26.870 --> 00:14:30.920
altruism organization.

00:14:30.920 --> 00:14:34.630
There's also another thing
that happened around that time,

00:14:34.630 --> 00:14:39.340
around 2007, that was very
important for the movement,

00:14:39.340 --> 00:14:41.940
and that was an
organization that

00:14:41.940 --> 00:14:46.640
was set up to rigorously
assess charities

00:14:46.640 --> 00:14:49.990
for whether they really
were effective in what

00:14:49.990 --> 00:14:52.590
they were doing.

00:14:52.590 --> 00:14:56.620
This was set up by two hedge
fund analysts, Holden Karnofsky

00:14:56.620 --> 00:15:01.320
and Elie Hassenfeld, who had
made quite a lot of money

00:15:01.320 --> 00:15:03.320
when they were
still in their 20s

00:15:03.320 --> 00:15:06.010
and together with some
of their colleagues

00:15:06.010 --> 00:15:10.440
decided to give a
portion of it away.

00:15:10.440 --> 00:15:14.130
And then they debated among
themselves so where should we

00:15:14.130 --> 00:15:16.080
give it?

00:15:16.080 --> 00:15:18.530
And they had different
ideas so they said, well,

00:15:18.530 --> 00:15:20.780
why don't we all write
to our favorite charity

00:15:20.780 --> 00:15:23.120
and ask them to
say what they would

00:15:23.120 --> 00:15:25.990
do with a significant
donation and then we

00:15:25.990 --> 00:15:31.050
can pool the results
and decide what to do?

00:15:31.050 --> 00:15:33.690
So they all wrote to
their favorite charities,

00:15:33.690 --> 00:15:38.830
but instead of getting back
some real data about what

00:15:38.830 --> 00:15:40.860
the charity would
do with a donation--

00:15:40.860 --> 00:15:43.640
and of course these are people
who were used to analyzing lots

00:15:43.640 --> 00:15:48.550
of data for their hedge
fund-- they got back brochures

00:15:48.550 --> 00:15:52.570
with nice photos of smiling
children and a few words

00:15:52.570 --> 00:15:55.890
about how much good
your donation could do,

00:15:55.890 --> 00:15:59.287
but no hard information at all.

00:15:59.287 --> 00:16:00.870
Well, they weren't
satisfied with that

00:16:00.870 --> 00:16:03.230
and they tried to follow up.

00:16:03.230 --> 00:16:05.100
They called some of
the charities and said,

00:16:05.100 --> 00:16:07.230
look we're really
serious about giving you

00:16:07.230 --> 00:16:09.080
quite substantial
amounts of money,

00:16:09.080 --> 00:16:12.200
but we do want to know in
more detail what you would

00:16:12.200 --> 00:16:14.695
do with it, how you decide
to fund this program rather

00:16:14.695 --> 00:16:17.845
than that program,
what evidence you have

00:16:17.845 --> 00:16:21.480
that the program is actually
getting the results that you

00:16:21.480 --> 00:16:23.220
want it to have?

00:16:23.220 --> 00:16:28.050
And they still got
really no useful response

00:16:28.050 --> 00:16:31.470
and in some cases, they
got active hostility

00:16:31.470 --> 00:16:33.160
with the suggestion
that these people

00:16:33.160 --> 00:16:37.170
were making that their programs
were not working effectively.

00:16:37.170 --> 00:16:40.940
Or that in one case,
one organization

00:16:40.940 --> 00:16:43.570
suspected that they were from
a rival organization trying

00:16:43.570 --> 00:16:46.650
to know what their programs
were and whether they

00:16:46.650 --> 00:16:48.720
were going to copy
these programs

00:16:48.720 --> 00:16:50.760
or something like that.

00:16:50.760 --> 00:16:54.960
So Holden and Elie decided
that there was a vacuum here

00:16:54.960 --> 00:16:57.220
that they needed to fill,
and their colleagues

00:16:57.220 --> 00:17:02.940
agreed to support them for
awhile in setting up GiveWell

00:17:02.940 --> 00:17:06.960
and actually to do some real
research on which organizations

00:17:06.960 --> 00:17:10.160
were effective and to try
to get this information

00:17:10.160 --> 00:17:12.579
from those organizations.

00:17:12.579 --> 00:17:16.990
So GiveWell now has grown to
have quite a significant team,

00:17:16.990 --> 00:17:20.400
I think about a
dozen researchers,

00:17:20.400 --> 00:17:26.220
and it is-- it's really
raised the standard of what

00:17:26.220 --> 00:17:29.460
you can know about
charities being effective.

00:17:29.460 --> 00:17:32.730
Before GiveWell, and
some of you in the field

00:17:32.730 --> 00:17:36.650
might know there was things
like an organization called

00:17:36.650 --> 00:17:38.330
Charity Navigator
and another one

00:17:38.330 --> 00:17:41.010
called GuideStar, but
really what they were doing

00:17:41.010 --> 00:17:44.780
was getting the form
that the charity sends

00:17:44.780 --> 00:17:50.780
to IRS which shows or states--
I should say probably rather

00:17:50.780 --> 00:17:54.970
that claims to state the
proportion of expenses that

00:17:54.970 --> 00:17:58.910
go on administration,
fundraising, and programs.

00:17:58.910 --> 00:18:01.560
But firstly, that's
quite a rubbery figure.

00:18:01.560 --> 00:18:03.890
Any creative
accountant can reduce

00:18:03.890 --> 00:18:07.060
your administrative amount
and increase your programs

00:18:07.060 --> 00:18:08.400
quite significantly.

00:18:08.400 --> 00:18:10.550
Secondly, it doesn't
really tell you

00:18:10.550 --> 00:18:13.960
very much about how effective
the charity is, because it

00:18:13.960 --> 00:18:17.500
doesn't tell you anything about
how effective the programs are.

00:18:17.500 --> 00:18:23.210
And you could spend 90% of
your revenue on programs

00:18:23.210 --> 00:18:26.160
and still be much less effective
than another charity that

00:18:26.160 --> 00:18:31.340
spends 80 or 70 or even 60%
of its revenue on programs,

00:18:31.340 --> 00:18:33.820
if its programs
are more effective.

00:18:33.820 --> 00:18:36.790
And given that it has
presumably more staff

00:18:36.790 --> 00:18:39.780
to select those programs
and to supervise them,

00:18:39.780 --> 00:18:43.840
it's quite likely that they
would be much more effective.

00:18:43.840 --> 00:18:47.380
So you might do much
better to donate

00:18:47.380 --> 00:18:52.130
to the charity that has
higher administrative costs.

00:18:52.130 --> 00:18:55.500
So really we didn't know very
much until GiveWell came along.

00:18:55.500 --> 00:18:59.650
And GiveWell has certainly made
it have a lot easier for people

00:18:59.650 --> 00:19:03.620
who want to give particularly to
global poverty-related issues,

00:19:03.620 --> 00:19:07.630
because GiveWell decided
early on that charities that

00:19:07.630 --> 00:19:09.280
were spending
money domestically,

00:19:09.280 --> 00:19:12.720
say on poverty in
the United States

00:19:12.720 --> 00:19:15.420
just couldn't really compete
in terms of dollar for dollar

00:19:15.420 --> 00:19:17.430
cost effectiveness
with those spending

00:19:17.430 --> 00:19:21.880
money on global poverty.

00:19:21.880 --> 00:19:23.310
So if you want to
find that which

00:19:23.310 --> 00:19:25.260
are the most effective
charities, there are-- well,

00:19:25.260 --> 00:19:27.593
let's say if you want to find
out which of the ones that

00:19:27.593 --> 00:19:30.440
have most clearly demonstrated
their effectiveness,

00:19:30.440 --> 00:19:33.642
then GiveWell is
the place to go.

00:19:33.642 --> 00:19:38.100
And I put it that way because
the charities that they don't

00:19:38.100 --> 00:19:41.550
recommend might be highly
effective, too, or some of them

00:19:41.550 --> 00:19:45.282
might be highly
effective, but simply

00:19:45.282 --> 00:19:47.740
have not been able to produce
the kind of evidence GiveWell

00:19:47.740 --> 00:19:49.270
wants.

00:19:49.270 --> 00:19:54.250
And sometimes that's not because
they aren't doing good work

00:19:54.250 --> 00:19:59.037
but because they're more
diverse and more broad.

00:19:59.037 --> 00:20:00.620
The charities that
GiveWell recommends

00:20:00.620 --> 00:20:05.700
tend to be very focused on based
on one type of intervention.

00:20:05.700 --> 00:20:08.530
So for instance, bed
nets against malaria,

00:20:08.530 --> 00:20:10.090
de-worming kids.

00:20:10.090 --> 00:20:14.020
There's one called Give Directly
that hands out cash grants

00:20:14.020 --> 00:20:16.270
to very poor people.

00:20:16.270 --> 00:20:20.710
When you do those things, you
can evaluate very rigorously.

00:20:20.710 --> 00:20:26.120
You can actually do a randomized
study in which you get baseline

00:20:26.120 --> 00:20:29.080
measurements for
a lot of villages,

00:20:29.080 --> 00:20:31.710
let's say, or a lot of schools.

00:20:31.710 --> 00:20:33.470
And then, you do
the interventions

00:20:33.470 --> 00:20:36.610
in a randomly selected
portion of them.

00:20:36.610 --> 00:20:39.830
And you go back and you measure
what difference you're making.

00:20:39.830 --> 00:20:42.310
So you can do that
if you're doing

00:20:42.310 --> 00:20:44.580
those sort of interventions.

00:20:44.580 --> 00:20:46.630
If you're doing a
wide range of things,

00:20:46.630 --> 00:20:49.220
as the bigger organizations--
the Oxfams, the Save

00:20:49.220 --> 00:20:51.510
the Children's, the
Cares-- do, it's

00:20:51.510 --> 00:20:56.150
much harder to evaluate all of
the things across the board.

00:20:56.150 --> 00:20:59.200
Plus if you're doing
things like advocacy work--

00:20:59.200 --> 00:21:01.690
you're an advocate for
the poor, let's say,

00:21:01.690 --> 00:21:04.920
trying to prevent
mining industries

00:21:04.920 --> 00:21:09.270
from going into
areas where they're

00:21:09.270 --> 00:21:12.360
going to damage the environment
and perhaps pollute rivers

00:21:12.360 --> 00:21:17.010
that villages need for their
water or their fishing.

00:21:17.010 --> 00:21:20.606
It's very hard to get any kind
of rigorous evaluation of that.

00:21:20.606 --> 00:21:22.230
Because they're all
one-off situations.

00:21:22.230 --> 00:21:25.590
You can't do a random study
of a large number of mining

00:21:25.590 --> 00:21:29.030
companies, half of which you
tried to intervene on behalf

00:21:29.030 --> 00:21:31.410
of the poor and half
of which you didn't.

00:21:31.410 --> 00:21:34.474
So I'm not saying that these
other organizations are not

00:21:34.474 --> 00:21:35.140
doing good work.

00:21:35.140 --> 00:21:39.030
I'm just saying that
it's not been possible

00:21:39.030 --> 00:21:41.710
for them to demonstrate
what they're doing as well.

00:21:45.690 --> 00:21:51.090
Now, I think all of this is
a very significant movement.

00:21:51.090 --> 00:21:54.580
Because I think it
challenges traditional ideas

00:21:54.580 --> 00:21:56.480
of philanthropy.

00:21:56.480 --> 00:22:01.860
And philanthropy is a big
industry in the United States.

00:22:01.860 --> 00:22:05.990
Around $335 billion dollars
are given to charities

00:22:05.990 --> 00:22:07.360
in the United States each year.

00:22:07.360 --> 00:22:10.020
That's about 2% of GDP.

00:22:10.020 --> 00:22:14.100
So it's a pretty
large slice of money.

00:22:14.100 --> 00:22:17.760
And the majority of
that, about 2/3 of it,

00:22:17.760 --> 00:22:19.090
is given by individuals.

00:22:19.090 --> 00:22:22.140
The rest is given mostly by
foundations, but a small slice

00:22:22.140 --> 00:22:23.650
by corporations.

00:22:23.650 --> 00:22:25.760
The foundations, no
doubt, do some research

00:22:25.760 --> 00:22:27.790
and have people,
in terms of what

00:22:27.790 --> 00:22:29.320
they're going to donate to.

00:22:29.320 --> 00:22:32.090
But we know that
most individuals

00:22:32.090 --> 00:22:35.980
do no research at all before
donating to a charity.

00:22:35.980 --> 00:22:40.850
And the minority that
do some research,

00:22:40.850 --> 00:22:42.620
it's mostly very cursory.

00:22:42.620 --> 00:22:45.430
And it's often things
like what I mentioned,

00:22:45.430 --> 00:22:48.280
looking at the amount that
goes to administration rather

00:22:48.280 --> 00:22:48.990
than to programs.

00:22:48.990 --> 00:22:50.235
So not very useful research.

00:22:52.870 --> 00:22:57.730
And when you add to that the
fact that some charities do,

00:22:57.730 --> 00:23:02.630
I think, a very large amount of
good with your money and others

00:23:02.630 --> 00:23:07.250
do negligible amounts,
I think that it

00:23:07.250 --> 00:23:09.950
would be extremely
important to shift

00:23:09.950 --> 00:23:13.870
some of this $240
billion dollars,

00:23:13.870 --> 00:23:17.930
say, that's given by individuals
from the less effective

00:23:17.930 --> 00:23:20.960
to the more effective charities.

00:23:20.960 --> 00:23:25.180
And that may often mean shifting
the area in which it's given.

00:23:25.180 --> 00:23:32.170
In fact, of the total $335
billion dollars that's given

00:23:32.170 --> 00:23:35.240
to charity in the United States,
there's only a very small

00:23:35.240 --> 00:23:39.000
percentage-- really, there's
not even very good statistics

00:23:39.000 --> 00:23:46.520
to say what it is-- but perhaps
if it's something like 5%--

00:23:46.520 --> 00:23:48.700
that was probably
on the high side,

00:23:48.700 --> 00:23:53.390
in terms of what actually goes
to global poverty out of that

00:23:53.390 --> 00:23:54.270
amount.

00:23:54.270 --> 00:23:57.300
So if, say, GiveWell
thinks that that's clearly

00:23:57.300 --> 00:24:00.570
more cost effective
than even charities that

00:24:00.570 --> 00:24:03.040
are concerned to help the
poor in the United States,

00:24:03.040 --> 00:24:05.820
then there's a big
impact you could have.

00:24:05.820 --> 00:24:09.730
And then, when you add to
the fact that, in fact,

00:24:09.730 --> 00:24:13.320
the largest recipients of
charity in the United States

00:24:13.320 --> 00:24:16.160
are religious
institutions followed

00:24:16.160 --> 00:24:19.380
by educational institutions,
and then a lot goes

00:24:19.380 --> 00:24:22.220
to art and cultural
institutions,

00:24:22.220 --> 00:24:25.100
I think there's clearly
a lot that could

00:24:25.100 --> 00:24:27.710
be moved to better causes.

00:24:27.710 --> 00:24:30.955
Now, some of it might
not really be movable.

00:24:30.955 --> 00:24:33.580
I mean, perhaps people who give
to their religious institutions

00:24:33.580 --> 00:24:36.420
will keep doing that and
will be hard to persuade

00:24:36.420 --> 00:24:40.610
that they ought to get evidence
about the good that this does.

00:24:40.610 --> 00:24:42.190
And you know, if
what you're doing

00:24:42.190 --> 00:24:46.060
is giving to have a
better church built,

00:24:46.060 --> 00:24:48.670
and you believe that that
is going to save more souls,

00:24:48.670 --> 00:24:51.370
then it's certainly
difficult to produce evidence

00:24:51.370 --> 00:24:53.030
about how much good that does.

00:24:53.030 --> 00:24:59.090
But I've been trying--
I use in the book--

00:24:59.090 --> 00:25:03.530
to show that there are very
good grounds for thinking

00:25:03.530 --> 00:25:06.750
that you're going to do
more good if you give

00:25:06.750 --> 00:25:09.796
to the sorts of things I was
talking about, helping people

00:25:09.796 --> 00:25:13.060
who are blind to see,
helping children to survive,

00:25:13.060 --> 00:25:17.280
than if you give
to, for example,

00:25:17.280 --> 00:25:19.170
art galleries or museums.

00:25:19.170 --> 00:25:24.710
Or take an example that was in
the news just a week or so ago.

00:25:24.710 --> 00:25:27.530
If, like David Geffen,
you give $100 million

00:25:27.530 --> 00:25:33.310
to the restoration
of what has up to now

00:25:33.310 --> 00:25:36.280
been Avery Fisher Concert
Hall at the Lincoln Center

00:25:36.280 --> 00:25:39.455
in New York, but is about to
become David Geffen Concert

00:25:39.455 --> 00:25:41.220
Hall.

00:25:41.220 --> 00:25:46.280
So this may seem fairly
obvious, that it's better

00:25:46.280 --> 00:25:50.210
to give to the poor than to give
to the renovation of a concert

00:25:50.210 --> 00:25:56.170
hall in Manhattan, but the
philanthropy industry, in fact,

00:25:56.170 --> 00:26:01.270
resists the idea that you can
even make such comparisons

00:26:01.270 --> 00:26:04.340
or that we should
try to persuade

00:26:04.340 --> 00:26:08.810
people to go in one direction
rather than the other.

00:26:08.810 --> 00:26:12.090
In fact, Rockefeller
Philanthropy Advisors,

00:26:12.090 --> 00:26:15.300
one of the biggest philanthropy
advisors in the country--

00:26:15.300 --> 00:26:16.910
if you go and look
on their website,

00:26:16.910 --> 00:26:19.390
you can find a
brochure called Finding

00:26:19.390 --> 00:26:21.810
Your Focus in Philanthropy.

00:26:21.810 --> 00:26:24.060
And you could read
in that the idea

00:26:24.060 --> 00:26:26.310
that-- to the sort of
question I'm asking,

00:26:26.310 --> 00:26:28.310
what's the cause
you should give to?

00:26:28.310 --> 00:26:32.650
--there is, they say,
"obviously"-- that's quotes--

00:26:32.650 --> 00:26:36.140
no objective answer.

00:26:36.140 --> 00:26:38.950
Well, there are certainly
some questions where

00:26:38.950 --> 00:26:41.130
it's very difficult
to say, is it better

00:26:41.130 --> 00:26:42.960
to give to prevent
climate change?

00:26:42.960 --> 00:26:45.370
Or is better to give
to the poor now?

00:26:45.370 --> 00:26:52.050
Is it better to give
to still further reduce

00:26:52.050 --> 00:26:54.540
what are small risks
of human extinction

00:26:54.540 --> 00:27:00.430
from bioterrorism or collision
with an asteroid or possibly,

00:27:00.430 --> 00:27:03.580
some people think, the
singularity takeover

00:27:03.580 --> 00:27:06.410
by artificially
intelligent machines

00:27:06.410 --> 00:27:08.840
that are hostile
to human beings?

00:27:08.840 --> 00:27:11.200
How do you compare
that with giving

00:27:11.200 --> 00:27:13.870
to help the global poor now?

00:27:13.870 --> 00:27:17.626
So there are some genuinely
difficult questions

00:27:17.626 --> 00:27:19.500
in which you might
hesitate to say that there

00:27:19.500 --> 00:27:21.740
is an objective answer.

00:27:21.740 --> 00:27:25.290
But I think, if
you say, is there

00:27:25.290 --> 00:27:27.246
an objective answer
to the question,

00:27:27.246 --> 00:27:29.120
could David Geffen have
done something better

00:27:29.120 --> 00:27:32.980
with $100 million than give
it towards the renovation

00:27:32.980 --> 00:27:34.500
of a concert hall?

00:27:34.500 --> 00:27:38.030
I think the answer
is clearly, yes.

00:27:38.030 --> 00:27:40.700
And so, that's
part of the message

00:27:40.700 --> 00:27:42.970
that effective altruists
are trying to get out.

00:27:42.970 --> 00:27:45.850
Their trying to say, we
ought to be thinking harder

00:27:45.850 --> 00:27:46.860
about philanthropy.

00:27:46.860 --> 00:27:49.140
And we ought to be
trying to persuade

00:27:49.140 --> 00:27:54.230
people to move away from
some sectors towards others.

00:27:54.230 --> 00:27:55.630
And even if this
means people are

00:27:55.630 --> 00:28:00.590
going to think that we're
philistines, well, so be it.

00:28:00.590 --> 00:28:02.100
There are just
more urgent things

00:28:02.100 --> 00:28:04.760
to do in the world
at the moment.

00:28:04.760 --> 00:28:06.920
When we deal with those
more urgent things,

00:28:06.920 --> 00:28:11.970
sure, let's go back to
supporting the arts financially

00:28:11.970 --> 00:28:13.930
and a million other things.

00:28:13.930 --> 00:28:18.380
But the world in which we
live-- with more than six

00:28:18.380 --> 00:28:21.690
million children dying each
year, children under five,

00:28:21.690 --> 00:28:24.540
from poverty-related
causes and, as I've

00:28:24.540 --> 00:28:27.460
said, millions of people
blind, whose blindness could

00:28:27.460 --> 00:28:30.860
be cured-- that's
not a world in which

00:28:30.860 --> 00:28:34.746
we want to go on
supporting these things.

00:28:34.746 --> 00:28:36.120
I want to make
sure that you have

00:28:36.120 --> 00:28:38.270
time to ask for some questions.

00:28:38.270 --> 00:28:40.770
So I'll just say one
more thing very briefly.

00:28:40.770 --> 00:28:42.680
And then I'll stop.

00:28:42.680 --> 00:28:47.640
And you might ask, well,
why are people doing this?

00:28:47.640 --> 00:28:53.010
Are we supposed to
believe that they are just

00:28:53.010 --> 00:28:53.950
purely altruistic?

00:28:53.950 --> 00:28:57.861
And some people are
skeptical about altruism.

00:28:57.861 --> 00:29:01.390
The Effective Altruism
Movement is, despite its name,

00:29:01.390 --> 00:29:06.120
not really very concerned
about people's motives.

00:29:06.120 --> 00:29:08.560
That is, in particular,
it's not really concerned

00:29:08.560 --> 00:29:15.070
to focus on the idea of people
making sacrifices for others.

00:29:15.070 --> 00:29:16.640
That's a view of
altruism, right?

00:29:16.640 --> 00:29:18.400
That if you're not
making a sacrifice,

00:29:18.400 --> 00:29:21.180
if you're not somehow
making yourself worse off,

00:29:21.180 --> 00:29:22.400
then you're not an altruist.

00:29:22.400 --> 00:29:27.301
Or maybe, you're not even
a morally virtuous person.

00:29:27.301 --> 00:29:29.550
I don't need the Effective
Altruism Movement is really

00:29:29.550 --> 00:29:30.840
very interested in that.

00:29:30.840 --> 00:29:34.310
They're interested
more in the outcomes,

00:29:34.310 --> 00:29:38.790
in getting more good
done in the world.

00:29:38.790 --> 00:29:42.010
And certainly, I--
and I think most

00:29:42.010 --> 00:29:44.120
other effective
altruists-- are actually

00:29:44.120 --> 00:29:47.860
very happy if people find that
a rewarding and fulfilling thing

00:29:47.860 --> 00:29:49.230
to do.

00:29:49.230 --> 00:29:52.150
And there is a lot of
psychological evidence

00:29:52.150 --> 00:29:55.020
that people do find it
rewarding and fulfilling,

00:29:55.020 --> 00:30:00.160
that people who are generous--
if you do surveys in which you

00:30:00.160 --> 00:30:03.240
ask people whether they've given
to charity in the past month,

00:30:03.240 --> 00:30:07.400
and then you also ask them
questions about how happy

00:30:07.400 --> 00:30:07.920
are they?

00:30:07.920 --> 00:30:09.170
What sort of mood are they in?

00:30:09.170 --> 00:30:11.730
Are they satisfied
with their life?

00:30:11.730 --> 00:30:13.590
--the answers do correlate.

00:30:13.590 --> 00:30:16.220
And of course, correlation
is not causation.

00:30:16.220 --> 00:30:21.150
But it does seem that generous
people tend to be happier.

00:30:21.150 --> 00:30:23.960
Maybe they give generously
because they're happy.

00:30:23.960 --> 00:30:28.760
But there is some other evidence
that may suggest causation.

00:30:28.760 --> 00:30:32.130
There's evidence of
neuro-imaging studies

00:30:32.130 --> 00:30:36.820
where people are having their
brains scanned in real-time.

00:30:36.820 --> 00:30:40.387
And they're given a kind
of a kitty, some money.

00:30:40.387 --> 00:30:41.970
And they're asked
questions about what

00:30:41.970 --> 00:30:43.270
they would like to do with it.

00:30:43.270 --> 00:30:47.800
And they're given charitable
options as well as more selfish

00:30:47.800 --> 00:30:51.190
kind of spend-it-on-yourself
options.

00:30:51.190 --> 00:30:57.150
And when people make
the charitable choice,

00:30:57.150 --> 00:30:59.620
the reward areas of their
brain, as they're known-- those

00:30:59.620 --> 00:31:03.550
that light up when you have
delicious food or great sex--

00:31:03.550 --> 00:31:05.210
light up as well.

00:31:05.210 --> 00:31:08.860
So there is some evidence
that the causation

00:31:08.860 --> 00:31:11.210
goes in that direction.

00:31:11.210 --> 00:31:15.460
Anyway, as I say, I
think a lot of people

00:31:15.460 --> 00:31:17.370
in the effective
altruism movement

00:31:17.370 --> 00:31:20.640
do find this a fulfilling
and rewarding thing to do.

00:31:20.640 --> 00:31:23.500
And it's possible
that one reason why

00:31:23.500 --> 00:31:28.310
it's developed at this point is
that there are a lot of people

00:31:28.310 --> 00:31:32.180
who feel that they
are not worried

00:31:32.180 --> 00:31:34.760
about their economic
security, but they

00:31:34.760 --> 00:31:37.970
do lack a sense of
fulfillment in their life.

00:31:37.970 --> 00:31:43.240
And this gives them something
that can enhance that sense.

00:31:43.240 --> 00:31:44.420
OK, I'll stop there.

00:31:44.420 --> 00:31:49.980
And we have, anyway, at least
20 to 25 minutes for questions.

00:31:49.980 --> 00:31:51.534
So who would like
to ask a question?

00:31:51.534 --> 00:31:52.950
AUDIENCE: Hello,
Professor Singer.

00:31:52.950 --> 00:31:54.650
Thank you for coming today.

00:31:54.650 --> 00:31:59.012
I've been following GiveWell and
using that to guide my giving--

00:31:59.012 --> 00:31:59.970
PETER SINGER: Terrific.

00:31:59.970 --> 00:32:01.553
AUDIENCE: --for the
last couple years.

00:32:01.553 --> 00:32:02.990
It's been very useful.

00:32:02.990 --> 00:32:05.630
I'm also very concerned
about global climate change.

00:32:05.630 --> 00:32:09.380
And I feel like if we aren't
doing something about that now,

00:32:09.380 --> 00:32:11.520
there's not much point
doing anything else.

00:32:11.520 --> 00:32:13.990
Is there a GiveWell
that would help

00:32:13.990 --> 00:32:17.170
me decide which charity I should
give to around global climate

00:32:17.170 --> 00:32:18.650
change?

00:32:18.650 --> 00:32:20.840
PETER SINGER: Not yet.

00:32:20.840 --> 00:32:25.430
The closest it comes
to that is GiveWell

00:32:25.430 --> 00:32:29.160
has received significant
financial support

00:32:29.160 --> 00:32:32.420
from a foundation
called Good Ventures.

00:32:32.420 --> 00:32:35.320
And with that support,
their doing something

00:32:35.320 --> 00:32:38.730
called the Open
Philanthropy Project.

00:32:38.730 --> 00:32:42.601
So whereas the sort of more
traditional work-- if you

00:32:42.601 --> 00:32:44.350
can call something
traditional that's only

00:32:44.350 --> 00:32:48.490
been going about eight years--
is assessing these charities

00:32:48.490 --> 00:32:51.020
quite rigorously, the
Open Philanthropy Project

00:32:51.020 --> 00:32:56.050
is doing broad surveys
of different areas

00:32:56.050 --> 00:32:58.760
where it might be possible to
have some particular leverage,

00:32:58.760 --> 00:33:02.580
where there might be a tipping
point where some input will

00:33:02.580 --> 00:33:03.770
make a difference.

00:33:03.770 --> 00:33:07.190
So they are looking
at climate change

00:33:07.190 --> 00:33:10.100
among a range of other things.

00:33:10.100 --> 00:33:12.300
I haven't very
recently gone back

00:33:12.300 --> 00:33:14.060
to see what the state
of their report is.

00:33:14.060 --> 00:33:15.910
I don't know whether
anyone else in the room

00:33:15.910 --> 00:33:18.444
has and knows about that.

00:33:18.444 --> 00:33:19.860
But that would be
a place to look.

00:33:19.860 --> 00:33:22.790
And if they do find that
climate change is something

00:33:22.790 --> 00:33:25.290
that might be an effective
use of charitable dollars,

00:33:25.290 --> 00:33:29.450
they will then start looking
at potential organizations

00:33:29.450 --> 00:33:30.747
in that field.

00:33:30.747 --> 00:33:31.580
AUDIENCE: Thank you.

00:33:31.580 --> 00:33:32.630
PETER SINGER: Thank you.

00:33:32.630 --> 00:33:33.350
Yeah?

00:33:33.350 --> 00:33:33.940
AUDIENCE: Hi.

00:33:33.940 --> 00:33:36.590
Does the thinking around
effective altruism

00:33:36.590 --> 00:33:39.279
try to take into account
situations of leverage that

00:33:39.279 --> 00:33:41.320
is, for example, instead
of giving money directly

00:33:41.320 --> 00:33:43.540
to blindness-- to
lobbying for government

00:33:43.540 --> 00:33:45.250
to spend tax money on
that sort of thing?

00:33:45.250 --> 00:33:47.209
Or is that just too
difficult to measure?

00:33:47.209 --> 00:33:49.750
PETER SINGER: People certainly
do talk about in the Effective

00:33:49.750 --> 00:33:51.370
Altruism Movement?

00:33:51.370 --> 00:33:54.050
Again, as far as
GiveWell is concerned,

00:33:54.050 --> 00:33:57.140
that would be something that
comes under this newer Open

00:33:57.140 --> 00:34:00.740
Philanthropy Project rather
than its traditional assessment.

00:34:00.740 --> 00:34:04.770
In the book, I have a little bit
of discussion about advocacy.

00:34:04.770 --> 00:34:10.110
I have an example
from Oxfam, that they

00:34:10.110 --> 00:34:12.380
had an advocacy project.

00:34:12.380 --> 00:34:14.920
So Ghana discovered
oil offshore,

00:34:14.920 --> 00:34:18.610
which was some sort of
economic boom for them.

00:34:18.610 --> 00:34:22.337
It produced hundreds of
millions of dollars of revenue.

00:34:22.337 --> 00:34:24.670
And the question was, what
would happen to that revenue?

00:34:24.670 --> 00:34:29.605
And we've already seen, in other
African countries like Angola,

00:34:29.605 --> 00:34:32.290
that essentially it's
gone to the elite.

00:34:32.290 --> 00:34:33.730
It's being corruptly
siphoned off.

00:34:33.730 --> 00:34:35.780
Now, Ghana is a
more hopeful country

00:34:35.780 --> 00:34:38.610
in that it's more democratic,
got a more functioning

00:34:38.610 --> 00:34:42.780
civil society, unlike Angola.

00:34:42.780 --> 00:34:47.540
So Oxfam worked with local
civil society in Ghana

00:34:47.540 --> 00:34:53.580
to get passage of a law called
Oil for Agriculture, which

00:34:53.580 --> 00:34:56.760
allocated 15% of the
government's oil revenues

00:34:56.760 --> 00:35:01.860
to help develop agriculture
in impoverished regions.

00:35:01.860 --> 00:35:04.010
Particularly, the north
of Ghana is in the Sahel

00:35:04.010 --> 00:35:07.920
and very subject to
drought and so on.

00:35:07.920 --> 00:35:12.100
So for an expenditure
of a couple of $100,000,

00:35:12.100 --> 00:35:16.775
they will now have, I think,
15% of their oil revenues

00:35:16.775 --> 00:35:21.150
is over $100 million
going each year, hopefully

00:35:21.150 --> 00:35:23.400
effectively-- though we don't
quite know that really--

00:35:23.400 --> 00:35:25.080
to help Oil for Agriculture.

00:35:25.080 --> 00:35:28.870
So that's like winning
the lottery almost.

00:35:28.870 --> 00:35:32.170
Of course, you could do
advocacy on dozens of programs

00:35:32.170 --> 00:35:33.890
without it paying off like that.

00:35:33.890 --> 00:35:35.590
So it is pretty speculative.

00:35:35.590 --> 00:35:38.870
But at least, some
of these programs

00:35:38.870 --> 00:35:41.616
do seem to pay off well.

00:35:41.616 --> 00:35:45.220
You know, bigger things like
trying to change US government

00:35:45.220 --> 00:35:48.640
legislation-- Oxfam has also,
with many other organizations,

00:35:48.640 --> 00:35:51.720
tried to get the Farm Bill
changed so that we don't

00:35:51.720 --> 00:35:56.430
subsidize the US agricultural
producers that then undercut

00:35:56.430 --> 00:35:58.990
developing country agricultural
producers trying to sell

00:35:58.990 --> 00:36:00.700
into the same global market.

00:36:00.700 --> 00:36:04.276
As you would know, that's
been unsuccessful so far.

00:36:04.276 --> 00:36:05.529
Thanks.

00:36:05.529 --> 00:36:06.070
AUDIENCE: Hi.

00:36:06.070 --> 00:36:07.430
Thank you for coming.

00:36:07.430 --> 00:36:09.990
I don't know if you're up for
evaluating specific charities.

00:36:09.990 --> 00:36:12.470
Historically, I-- and I think
a number of other coworkers--

00:36:12.470 --> 00:36:14.530
have given to Doctors
Without Borders.

00:36:14.530 --> 00:36:18.810
I was wondering if you would
consider them effective.

00:36:18.810 --> 00:36:23.200
PETER SINGER: So again, this
is difficult for me to comment.

00:36:23.200 --> 00:36:26.070
I think they're-- like some
of the other larger ones I

00:36:26.070 --> 00:36:31.240
mentioned-- Oxfam and
Save the Children--

00:36:31.240 --> 00:36:34.250
GiveWell finds them
difficult to really evaluate.

00:36:34.250 --> 00:36:36.730
Because they do a number
of different things.

00:36:36.730 --> 00:36:43.360
And so I think you can certainly
look at what GiveWell says

00:36:43.360 --> 00:36:44.620
about Doctors Without Borders.

00:36:44.620 --> 00:36:47.170
And you'll see why they're
not one of GiveWell's

00:36:47.170 --> 00:36:48.450
top-ranked charities.

00:36:48.450 --> 00:36:51.260
And I think you should
then look at that

00:36:51.260 --> 00:36:55.940
and, then, decide for yourself
whether this is something that

00:36:55.940 --> 00:36:58.490
really counts against
them being effective

00:36:58.490 --> 00:37:00.980
or merely demonstrates
the difficulty

00:37:00.980 --> 00:37:05.171
of proving their effectiveness
by GiveWell standards.

00:37:05.171 --> 00:37:05.670
So--

00:37:05.670 --> 00:37:05.740
AUDIENCE: Thank you.

00:37:05.740 --> 00:37:07.260
PETER SINGER: --I think
that's about all I can say.

00:37:07.260 --> 00:37:08.030
Thank you.

00:37:08.030 --> 00:37:08.530
Yes?

00:37:08.530 --> 00:37:09.071
AUDIENCE: Hi.

00:37:09.071 --> 00:37:14.580
So your analogy of the
drowning child scenario

00:37:14.580 --> 00:37:16.600
reminds me of something
that I've seen a lot now

00:37:16.600 --> 00:37:20.290
on social media with these,
like, pop-up, GoFundMe

00:37:20.290 --> 00:37:21.420
campaigns.

00:37:21.420 --> 00:37:25.030
Where somebody says, like,
I just have X disease,

00:37:25.030 --> 00:37:28.060
and I'm losing my
house, and-- and it

00:37:28.060 --> 00:37:31.040
elicits that sort of
drowning-child reaction.

00:37:31.040 --> 00:37:32.450
And usually there are photos.

00:37:32.450 --> 00:37:34.590
And maybe it's a
friend of a friend,

00:37:34.590 --> 00:37:36.820
somebody you sort of know.

00:37:36.820 --> 00:37:40.250
And sometimes it elicits
a really huge response.

00:37:40.250 --> 00:37:43.070
And I've seen this many times.

00:37:43.070 --> 00:37:48.520
How do we weigh something that
does have that kind of impact--

00:37:48.520 --> 00:37:50.920
especially if it's somebody
that you know or it's

00:37:50.920 --> 00:37:57.260
somebody your friend
knows-- relative to people

00:37:57.260 --> 00:37:58.970
that seem so removed?

00:37:58.970 --> 00:38:00.980
Or are we just
overreacting, because it

00:38:00.980 --> 00:38:03.359
feels like this kind
of drowning child?

00:38:03.359 --> 00:38:03.900
I don't know.

00:38:03.900 --> 00:38:04.691
PETER SINGER: Yeah.

00:38:04.691 --> 00:38:07.060
AUDIENCE: I don't know
what to make of it.

00:38:07.060 --> 00:38:10.040
PETER SINGER: So I think
we are overreacting,

00:38:10.040 --> 00:38:11.780
which is not to say
that you may not want

00:38:11.780 --> 00:38:13.030
to do things for your friends.

00:38:13.030 --> 00:38:15.294
Even effective altruists
do things to their friends.

00:38:15.294 --> 00:38:17.960
Otherwise, they wouldn't be very
nice people to be friends with,

00:38:17.960 --> 00:38:19.616
I guess.

00:38:19.616 --> 00:38:20.990
But effective
altruists would see

00:38:20.990 --> 00:38:23.717
that as something different
from the effective altruism

00:38:23.717 --> 00:38:24.300
they would do.

00:38:24.300 --> 00:38:26.900
I mean, I think I have
one case I mention

00:38:26.900 --> 00:38:30.560
in my book of somebody who's
gives a huge proportion

00:38:30.560 --> 00:38:32.060
of his income away.

00:38:32.060 --> 00:38:34.760
But he paid a lot
of money to a friend

00:38:34.760 --> 00:38:37.105
whose dog was really ill,
so that the dog could

00:38:37.105 --> 00:38:40.190
have surgery that was
really quite expensive.

00:38:40.190 --> 00:38:42.310
And he sees this
sort of parallel

00:38:42.310 --> 00:38:46.090
to deciding to go on a
really expensive holiday

00:38:46.090 --> 00:38:48.510
to a tropical island
in winter, not

00:38:48.510 --> 00:38:51.790
something that's really
part of his altruism.

00:38:51.790 --> 00:38:53.354
But the phenomenon
that you mentioned

00:38:53.354 --> 00:38:54.270
is really interesting.

00:38:54.270 --> 00:38:57.060
And since writing
that article, I've

00:38:57.060 --> 00:38:59.630
learned a lot more about
the psychology of the way

00:38:59.630 --> 00:39:01.640
we respond to things.

00:39:01.640 --> 00:39:05.740
And one of the most
well-confirmed results

00:39:05.740 --> 00:39:10.360
is that we respond to
identifiable individuals.

00:39:10.360 --> 00:39:13.990
So Paul Slovic, a
psychologist at Oregon,

00:39:13.990 --> 00:39:16.062
did a study where
he got students

00:39:16.062 --> 00:39:17.270
to come in for an experiment.

00:39:17.270 --> 00:39:18.978
Didn't tell them what
the experiment was.

00:39:18.978 --> 00:39:21.660
Said they'd be paid $15.

00:39:21.660 --> 00:39:24.130
Gave them a clipboard with
some questions, which they then

00:39:24.130 --> 00:39:27.700
answered, assuming that
was the experiment.

00:39:27.700 --> 00:39:30.340
They then got paid
$15 in small bills.

00:39:30.340 --> 00:39:32.800
And as they were paid it,
the person paying them

00:39:32.800 --> 00:39:35.790
said, by the way, our
lab has a charity, which

00:39:35.790 --> 00:39:37.020
it supports each month.

00:39:37.020 --> 00:39:39.174
Here is some information
about our charity.

00:39:39.174 --> 00:39:40.340
Would you mind reading this?

00:39:40.340 --> 00:39:42.923
And then, perhaps, you'd like
to give some of what you've just

00:39:42.923 --> 00:39:44.070
earned to the charity.

00:39:44.070 --> 00:39:47.420
Now, randomly, some of them
got identifying information

00:39:47.420 --> 00:39:50.430
about an individual
girl, her name, her age,

00:39:50.430 --> 00:39:52.700
and a picture of her.

00:39:52.700 --> 00:39:53.750
And others didn't.

00:39:53.750 --> 00:39:55.420
They just got
statistical information

00:39:55.420 --> 00:39:56.840
that there are children, right?

00:39:56.840 --> 00:40:00.040
Well, as you would know from
what you've been saying,

00:40:00.040 --> 00:40:02.850
the response with the
identifiable information

00:40:02.850 --> 00:40:04.750
got a lot more.

00:40:04.750 --> 00:40:06.809
So that's what's going
on here, I think.

00:40:06.809 --> 00:40:08.350
And that was, I
guess, what was going

00:40:08.350 --> 00:40:10.020
on to some extent
in my pond example.

00:40:10.020 --> 00:40:12.250
I was asking you to
imagine there's this child.

00:40:12.250 --> 00:40:13.770
You can see this child.

00:40:13.770 --> 00:40:16.900
It's an identifiable
child in front of you.

00:40:16.900 --> 00:40:19.260
And so, people will give
more in those circumstances.

00:40:19.260 --> 00:40:23.650
But I think really, we ought
to become more self-aware

00:40:23.650 --> 00:40:28.420
of the little tricks that
our brain plays and try

00:40:28.420 --> 00:40:30.910
to compensate for them, really.

00:40:30.910 --> 00:40:33.600
We ought to know that,
just as perhaps we

00:40:33.600 --> 00:40:36.750
become aware that,
some of us, maybe

00:40:36.750 --> 00:40:39.360
there's some sort of tendency
for us to prefer people who

00:40:39.360 --> 00:40:41.050
look like us, and
this can fuel racism

00:40:41.050 --> 00:40:45.677
and we guard against
that-- so I think,

00:40:45.677 --> 00:40:47.510
we ought to be aware
that we have a tendency

00:40:47.510 --> 00:40:51.350
to prefer giving to these
identifiable individuals

00:40:51.350 --> 00:40:52.820
and guard against that too.

00:40:52.820 --> 00:40:53.653
AUDIENCE: Thank you.

00:40:53.653 --> 00:40:54.729
PETER SINGER: Thanks.

00:40:54.729 --> 00:40:55.270
AUDIENCE: Hi.

00:40:55.270 --> 00:40:56.240
Thank you for coming.

00:40:56.240 --> 00:40:59.060
I was recently introduced to
the Effective Altruism Movement

00:40:59.060 --> 00:41:00.760
by some of my colleagues here.

00:41:00.760 --> 00:41:02.770
So I consider
myself as a novice,

00:41:02.770 --> 00:41:05.000
but someone who is very
interested in learning more.

00:41:05.000 --> 00:41:10.290
I think my question today is--
so the effectiveness seems

00:41:10.290 --> 00:41:12.790
to be based on a
return or a yield.

00:41:12.790 --> 00:41:15.740
For every $100, I get this
X demonstrable effect.

00:41:15.740 --> 00:41:18.510
But earlier in your remarks,
you were talking about advocacy.

00:41:18.510 --> 00:41:21.370
So I'm wondering if effective
altruists have, sort of,

00:41:21.370 --> 00:41:25.210
the stack rank or the
hierarchy of goods

00:41:25.210 --> 00:41:27.210
that are worth pursuing?

00:41:27.210 --> 00:41:30.024
Because at the human level,
we connect with something,

00:41:30.024 --> 00:41:31.940
like a problem, you see
right in front of you.

00:41:31.940 --> 00:41:33.500
And that can be very motivating.

00:41:33.500 --> 00:41:36.832
It can help an
important cause and make

00:41:36.832 --> 00:41:37.790
you feel good about it.

00:41:37.790 --> 00:41:40.410
But it's just one of many.

00:41:40.410 --> 00:41:43.416
So particularly things,
like human rights

00:41:43.416 --> 00:41:44.540
is a hard thing to measure.

00:41:44.540 --> 00:41:44.970
PETER SINGER: Mm-hmm.

00:41:44.970 --> 00:41:46.820
AUDIENCE: You know,
when someone is working

00:41:46.820 --> 00:41:50.104
to create rights for
disenfranchised classes,

00:41:50.104 --> 00:41:52.020
whether you give to one
political organization

00:41:52.020 --> 00:41:54.860
or another, which one
is doing the most good?

00:41:54.860 --> 00:41:56.727
But maybe you can
help be broaden

00:41:56.727 --> 00:41:57.810
the question a little bit.

00:41:57.810 --> 00:42:01.144
But speak to, how do
sort or stack-rank--

00:42:01.144 --> 00:42:03.060
PETER SINGER: So I think
effective altruists--

00:42:03.060 --> 00:42:03.350
AUDIENCE: --effectively?

00:42:03.350 --> 00:42:04.150
Thank you.

00:42:04.150 --> 00:42:08.550
PETER SINGER: --mostly, are
concerned about well-being,

00:42:08.550 --> 00:42:09.050
really.

00:42:09.050 --> 00:42:12.600
So they're concerned
about reducing suffering,

00:42:12.600 --> 00:42:15.560
improving welfare, happiness.

00:42:15.560 --> 00:42:20.310
And again, not all of them,
but I think most of them

00:42:20.310 --> 00:42:22.820
would try to convert
other things into that.

00:42:22.820 --> 00:42:26.455
So if you talk about human
rights as an example,

00:42:26.455 --> 00:42:28.830
they would say, well, I think
human rights are important.

00:42:28.830 --> 00:42:32.380
But human rights are
important because, really,

00:42:32.380 --> 00:42:35.120
in a society that
respects human rights,

00:42:35.120 --> 00:42:40.440
there will be fewer abuses,
therefore less suffering.

00:42:40.440 --> 00:42:43.150
And there will be
higher welfare.

00:42:43.150 --> 00:42:46.370
So I would say, most of
them do have some idea

00:42:46.370 --> 00:42:52.060
that if you could somehow
cash-out how much you're

00:42:52.060 --> 00:42:56.370
improving welfare by
advocating for human rights,

00:42:56.370 --> 00:42:58.400
then you would have
a metric whereby

00:42:58.400 --> 00:43:01.870
you could tell whether
advocating for human rights

00:43:01.870 --> 00:43:05.740
compares favorably or
unfavorably with providing

00:43:05.740 --> 00:43:08.570
basic health care for example.

00:43:08.570 --> 00:43:13.360
And in the absence
of that information,

00:43:13.360 --> 00:43:17.442
of knowing how that cashes out,
it's very difficult to compare.

00:43:17.442 --> 00:43:19.400
And there may be some
people who would actually

00:43:19.400 --> 00:43:21.691
think that human rights are
intrinsically valuable even

00:43:21.691 --> 00:43:24.120
if they don't lead
to higher welfare.

00:43:24.120 --> 00:43:25.900
That's a possibility too.

00:43:25.900 --> 00:43:30.206
But that seems to me to be not
the mainstream of the movement.

00:43:30.206 --> 00:43:31.370
AUDIENCE: OK, thank you.

00:43:31.370 --> 00:43:32.170
PETER SINGER: OK, thanks.

00:43:32.170 --> 00:43:32.570
Yeah?

00:43:32.570 --> 00:43:33.986
AUDIENCE: Hi, I
have two questions

00:43:33.986 --> 00:43:36.280
on very different ends
of the spectrum of how

00:43:36.280 --> 00:43:37.540
to choose how much to give.

00:43:37.540 --> 00:43:42.580
And one is, is there thought
among the Effective Altruism

00:43:42.580 --> 00:43:46.600
Movement about what's enough
and about when to stop?

00:43:46.600 --> 00:43:48.172
And then, on the
other side, I think

00:43:48.172 --> 00:43:49.630
tying into some of
the conversation

00:43:49.630 --> 00:43:53.470
about human connection and
human psychology-- I wonder some

00:43:53.470 --> 00:43:56.400
if one of the reasons why
people prefer local charities

00:43:56.400 --> 00:43:58.730
is that it's easier to convince
themselves to give more

00:43:58.730 --> 00:44:04.442
or to give more wholeheartedly
or expand that pie of what do

00:44:04.442 --> 00:44:06.900
they consider for charity when
it's something that they can

00:44:06.900 --> 00:44:07.774
see and connect with?

00:44:07.774 --> 00:44:11.290
And if that doesn't
provide some value as well.

00:44:11.290 --> 00:44:14.880
But at the same time, there's
also that scaling factor

00:44:14.880 --> 00:44:16.469
of how effective they are.

00:44:16.469 --> 00:44:18.760
So that's two questions, sort
of on very different ends

00:44:18.760 --> 00:44:19.270
of the spectrum.

00:44:19.270 --> 00:44:20.240
PETER SINGER: OK, so sorry.

00:44:20.240 --> 00:44:22.150
Just while listening to the
second one, the first one

00:44:22.150 --> 00:44:23.000
went out of my head, I'm afraid.

00:44:23.000 --> 00:44:23.620
AUDIENCE: What's enough?

00:44:23.620 --> 00:44:24.310
PETER SINGER: Yeah,
what's enough.

00:44:24.310 --> 00:44:25.315
AUDIENCE: And how do you
decide what's enough?

00:44:25.315 --> 00:44:27.091
PETER SINGER: OK, good.

00:44:27.091 --> 00:44:29.590
So that's really the question
that, as I said, I began with.

00:44:29.590 --> 00:44:32.770
And my original
1970s article seemed

00:44:32.770 --> 00:44:35.560
to suggest that you never
got to enough until you

00:44:35.560 --> 00:44:37.695
got to the point of
marginal utility.

00:44:41.360 --> 00:44:45.410
And there's a sense in
which that is the only place

00:44:45.410 --> 00:44:46.720
you can draw the line.

00:44:46.720 --> 00:44:51.130
And anything else is going
to be unsatisfactory,

00:44:51.130 --> 00:44:54.820
if you're really trying to
draw a moral line, right?

00:44:54.820 --> 00:44:57.350
But you know, what
I now say-- and I

00:44:57.350 --> 00:44:59.880
think what a number of
other people in the movement

00:44:59.880 --> 00:45:08.430
would say-- is although that
is the ideal moral limit,

00:45:08.430 --> 00:45:11.810
you shouldn't think
of it as, somehow,

00:45:11.810 --> 00:45:15.600
if you don't reach that
limit, then you're a failure;

00:45:15.600 --> 00:45:20.360
then you're acting wrongly;
then you're an unethical person.

00:45:20.360 --> 00:45:25.000
We standardly think of
morality in terms of,

00:45:25.000 --> 00:45:28.740
if you don't do what's right,
then you doing what's wrong.

00:45:28.740 --> 00:45:32.470
And it's just the
either-or dichotomy there.

00:45:32.470 --> 00:45:34.290
And that works for things.

00:45:34.290 --> 00:45:37.080
Traditional morality is where
you have these simple rules.

00:45:37.080 --> 00:45:37.950
Don't murder.

00:45:37.950 --> 00:45:38.900
Don't lie.

00:45:38.900 --> 00:45:39.610
Don't cheat.

00:45:39.610 --> 00:45:41.230
Don't commit adultery.

00:45:41.230 --> 00:45:43.580
Those things you can
either do or not do.

00:45:43.580 --> 00:45:50.200
But once you take into account
positive obligations to assist,

00:45:50.200 --> 00:45:52.290
I think it's better to
think of morality there

00:45:52.290 --> 00:45:54.800
as on a spectrum.

00:45:54.800 --> 00:45:57.470
So instead of
saying you've either

00:45:57.470 --> 00:45:59.760
reached that point or
you haven't, you've

00:45:59.760 --> 00:46:02.370
said, well, I'm here
on the spectrum.

00:46:02.370 --> 00:46:03.870
It would be better,
perhaps, morally

00:46:03.870 --> 00:46:05.200
if I was further over there.

00:46:05.200 --> 00:46:09.320
But I do have these
other interests.

00:46:09.320 --> 00:46:12.460
I do have friends
that I care about,

00:46:12.460 --> 00:46:15.600
my own interests, those
of my family, and so on.

00:46:15.600 --> 00:46:19.580
And even if morality would
tell me I should do more,

00:46:19.580 --> 00:46:22.760
this is as far as I'm going.

00:46:22.760 --> 00:46:25.280
And you don't have to feel
really terrible about that.

00:46:25.280 --> 00:46:29.840
Because, firstly as I say, it's
not a black and white matter.

00:46:29.840 --> 00:46:31.930
Secondly, if you look
at how you compare

00:46:31.930 --> 00:46:34.320
with pretty much
everybody else in society,

00:46:34.320 --> 00:46:36.640
if you're in the Effective
Altruism Movement at all,

00:46:36.640 --> 00:46:39.370
you're doing something
slightly or more substantial

00:46:39.370 --> 00:46:43.890
than 99 point something of
the population are doing.

00:46:43.890 --> 00:46:47.267
So you needn't feel too
bad about that, right?

00:46:47.267 --> 00:46:49.600
And now, I've forgotten the
second half of the question,

00:46:49.600 --> 00:46:50.974
now that I've
answered the first.

00:46:50.974 --> 00:46:54.330
AUDIENCE: My other question
is, does perhaps local giving

00:46:54.330 --> 00:46:56.760
and that direct connection
because of human psychology--

00:46:56.760 --> 00:46:57.246
PETER SINGER: Oh, yeah.

00:46:57.246 --> 00:46:58.220
AUDIENCE: --give a
bit more or that--

00:46:58.220 --> 00:46:59.550
PETER SINGER: Yeah,
do they give more?

00:46:59.550 --> 00:47:00.160
Well, maybe.

00:47:00.160 --> 00:47:02.740
This is how some
people push back when

00:47:02.740 --> 00:47:04.320
I talk about global poverty.

00:47:04.320 --> 00:47:07.030
They say, look, you're
going to put off givers.

00:47:07.030 --> 00:47:10.300
People give because of their
emotions, their passions

00:47:10.300 --> 00:47:12.867
and, perhaps, sometimes also
their personal connections.

00:47:12.867 --> 00:47:14.325
And if you tell
them that they have

00:47:14.325 --> 00:47:20.640
to give in this more objective
sort of way, they'll give less.

00:47:20.640 --> 00:47:23.270
And that's quite possible.

00:47:23.270 --> 00:47:26.080
I can't say that that's untrue.

00:47:26.080 --> 00:47:29.230
What I would say
is that, sometimes,

00:47:29.230 --> 00:47:31.590
even if they're giving
less, because it

00:47:31.590 --> 00:47:35.260
will do a multiple times as
good, it might still be better.

00:47:35.260 --> 00:47:38.170
The other thing I'd
say is-- although I'm

00:47:38.170 --> 00:47:41.280
excited by the emergence of this
Effective Altruism Movement-- I

00:47:41.280 --> 00:47:45.260
don't think it's really ever
going to completely dominate

00:47:45.260 --> 00:47:46.430
philanthropy.

00:47:46.430 --> 00:47:49.430
I think there's always going
to be quite a lot of people

00:47:49.430 --> 00:47:53.090
who will give on an emotional
basis and will give locally.

00:47:53.090 --> 00:47:54.700
So I don't think
these other charities

00:47:54.700 --> 00:47:56.430
are going to disappear.

00:47:56.430 --> 00:47:58.870
I think they're still going
to get plenty of funding,

00:47:58.870 --> 00:48:00.258
whether we like it or not.

00:48:00.258 --> 00:48:01.134
Yep?

00:48:01.134 --> 00:48:03.140
AUDIENCE: OK, so I
wrote down part of this.

00:48:03.140 --> 00:48:05.890
Because it was a
lot of thoughts.

00:48:05.890 --> 00:48:10.640
Kind of on the spectrum
of environmental care

00:48:10.640 --> 00:48:14.090
versus poverty
and health care, I

00:48:14.090 --> 00:48:18.120
can understand both of those
actually being heavily weighted

00:48:18.120 --> 00:48:22.720
for the hierarchy of needs of
the survival of our species.

00:48:22.720 --> 00:48:31.000
But my question is pertaining to
the amount of discounting arts

00:48:31.000 --> 00:48:33.180
and humanities.

00:48:33.180 --> 00:48:37.130
Because, well yes,
it's important to take

00:48:37.130 --> 00:48:41.400
care of health and your
basic survival needs.

00:48:41.400 --> 00:48:45.620
Those problems-- I
might be pessimistic--

00:48:45.620 --> 00:48:49.510
don't see them going away
in a measurable future.

00:48:49.510 --> 00:48:51.830
And to say that we
should take care of arts

00:48:51.830 --> 00:48:57.620
after all of that is taken
care of-- what kind of humanity

00:48:57.620 --> 00:48:59.640
and what kind of
culture would we

00:48:59.640 --> 00:49:03.280
have if we focus in that way?

00:49:03.280 --> 00:49:08.220
And isn't art and
humanities and culture

00:49:08.220 --> 00:49:12.820
a part of well-being and
the full-rounded person

00:49:12.820 --> 00:49:16.100
and connection between
multiple people?

00:49:16.100 --> 00:49:17.020
PETER SINGER: Yeah.

00:49:17.020 --> 00:49:20.270
Look, I'm certainly not against
the arts and humanities.

00:49:20.270 --> 00:49:23.880
Especially, obviously, I've
spent my life doing philosophy

00:49:23.880 --> 00:49:24.630
in various ways.

00:49:24.630 --> 00:49:28.330
And not all of it is directed
to effective altruism.

00:49:28.330 --> 00:49:31.470
So I'm clearly interested in
philosophy in its own sake

00:49:31.470 --> 00:49:32.420
as well.

00:49:32.420 --> 00:49:35.080
But what we're
talking about here

00:49:35.080 --> 00:49:39.280
is the charitable
dollars, if you like,

00:49:39.280 --> 00:49:41.980
and what good they do.

00:49:41.980 --> 00:49:45.510
I think arts and
humanities would not

00:49:45.510 --> 00:49:50.890
disappear if we stopped donating
charitable dollars to them.

00:49:50.890 --> 00:49:55.610
I think that there is a
desire for artistic expression

00:49:55.610 --> 00:49:56.110
in people.

00:49:56.110 --> 00:50:00.520
That is something you see
in a huge range of cultures

00:50:00.520 --> 00:50:04.340
where there's no commercial
value in it at all.

00:50:04.340 --> 00:50:08.720
The indigenous Australians did
rock art, which they certainly

00:50:08.720 --> 00:50:09.850
couldn't sell.

00:50:09.850 --> 00:50:14.360
They did drawings in the sand,
which were quite ephemeral.

00:50:14.360 --> 00:50:17.360
People in concentration
camps tried to create art

00:50:17.360 --> 00:50:18.980
if they possibly could.

00:50:18.980 --> 00:50:21.860
So I think there is a
human desire to create art.

00:50:21.860 --> 00:50:28.330
And you don't really need to
give to support it financially.

00:50:28.330 --> 00:50:31.260
I guess what you do
need to do, given

00:50:31.260 --> 00:50:34.250
that there's a huge heritage of
art, you need to preserve it.

00:50:34.250 --> 00:50:38.070
I'm not suggesting that no money
should go to the Metropolitan

00:50:38.070 --> 00:50:40.275
Museum of Art or
Boston equivalent

00:50:40.275 --> 00:50:42.920
and, therefore, the building
should fall into disrepair.

00:50:42.920 --> 00:50:44.640
And eventually,
all the paintings

00:50:44.640 --> 00:50:46.340
there should get ruined.

00:50:46.340 --> 00:50:50.820
I certainly think that we
ought to maintain the heritage

00:50:50.820 --> 00:50:52.660
for future generations.

00:50:52.660 --> 00:50:56.570
But in terms of saying, do
we need to build a new wing?

00:50:56.570 --> 00:50:59.210
Or take an example I
used in an earlier book.

00:50:59.210 --> 00:51:03.620
Was it really important that
the Metropolitan Museum of Art

00:51:03.620 --> 00:51:08.450
acquired a small Madonna by
Duccio, the Sienese painter

00:51:08.450 --> 00:51:09.380
for $45 million?

00:51:09.380 --> 00:51:12.750
Would it have really
mattered if that painting had

00:51:12.750 --> 00:51:15.580
gone to some other art museum?

00:51:15.580 --> 00:51:18.750
Or perhaps even to
a private collector?

00:51:18.750 --> 00:51:20.890
I think it wouldn't have
mattered as much as what

00:51:20.890 --> 00:51:23.112
$45 million could've done.

00:51:23.112 --> 00:51:25.320
AUDIENCE: I think that I'm
thinking more of community

00:51:25.320 --> 00:51:28.080
based art that's
happening in real-time.

00:51:28.080 --> 00:51:31.910
I'm thinking of things that
bring together communities

00:51:31.910 --> 00:51:36.080
and liven up
environments to help

00:51:36.080 --> 00:51:41.310
connect humans no than
preservation of antiquities.

00:51:41.310 --> 00:51:43.197
PETER SINGER: Well,
that's fairly low cost

00:51:43.197 --> 00:51:44.280
I would've thought, right?

00:51:44.280 --> 00:51:44.780
And--

00:51:44.780 --> 00:51:45.390
AUDIENCE: No.

00:51:45.390 --> 00:51:48.220
I help throw some
events in Boston.

00:51:48.220 --> 00:51:50.846
And it's quite
expensive actually.

00:51:50.846 --> 00:51:51.345
It's--

00:51:51.345 --> 00:51:53.350
PETER SINGER: Well, there may
be expensive ways of doing it.

00:51:53.350 --> 00:51:55.910
But I mean, as I said, I think
communities will create art

00:51:55.910 --> 00:51:58.149
without a lot of funding.

00:51:58.149 --> 00:51:59.940
And I think that would
be as true in Boston

00:51:59.940 --> 00:52:02.245
as it would be in Outback,
Australia, really.

00:52:02.245 --> 00:52:04.370
AUDIENCE: Well actually,
funny you should say that.

00:52:04.370 --> 00:52:05.869
There's actually
an organization I'm

00:52:05.869 --> 00:52:07.310
involved with called Figment.

00:52:07.310 --> 00:52:10.030
And we both have events
in New York, Boston,

00:52:10.030 --> 00:52:12.830
around the United States,
and Geelong, Australia.

00:52:12.830 --> 00:52:15.240
And they're all
community-supported, free

00:52:15.240 --> 00:52:17.860
events that require fundraising.

00:52:17.860 --> 00:52:21.460
And as far as the value for your
dollar goes, all of the money

00:52:21.460 --> 00:52:24.040
that goes into those
foundations goes directly

00:52:24.040 --> 00:52:26.850
to the permits and
all of those things

00:52:26.850 --> 00:52:29.820
to pull to make these
festivals possible.

00:52:29.820 --> 00:52:31.830
And those, then, bring
together communities

00:52:31.830 --> 00:52:35.682
and cross socioeconomic
boundaries.

00:52:35.682 --> 00:52:36.390
PETER SINGER: OK.

00:52:36.390 --> 00:52:38.780
Well, you clearly
are more involved

00:52:38.780 --> 00:52:41.000
in this particular
area than I am.

00:52:41.000 --> 00:52:44.730
And I won't argue with you.

00:52:44.730 --> 00:52:48.930
I would still think that,
if it comes to a choice,

00:52:48.930 --> 00:52:51.970
I would rather make
it possible for people

00:52:51.970 --> 00:52:57.330
to see than make communities
come together over art.

00:52:57.330 --> 00:53:00.110
And I would hope
that you could find

00:53:00.110 --> 00:53:04.107
other ways of encouraging people
to have that artistic activity.

00:53:04.107 --> 00:53:04.940
AUDIENCE: Thank you.

00:53:04.940 --> 00:53:06.190
PETER SINGER: Thank you.

00:53:06.190 --> 00:53:07.280
Over to this side.

00:53:07.280 --> 00:53:10.160
AUDIENCE: Thank you once again
for your wonderful remarks.

00:53:10.160 --> 00:53:11.880
You are a philosopher.

00:53:11.880 --> 00:53:14.250
Most philosophers are
not effective altruists

00:53:14.250 --> 00:53:15.580
as far as I can tell.

00:53:15.580 --> 00:53:17.955
You visibly talk to a lot of
your philosophical coworkers

00:53:17.955 --> 00:53:19.380
from time to time.

00:53:19.380 --> 00:53:21.610
And philosophers do love
arguments, I'm told.

00:53:21.610 --> 00:53:24.491
What sort of arguments have
you heard from your colleagues

00:53:24.491 --> 00:53:25.615
against effective altruism?

00:53:28.770 --> 00:53:33.320
PETER SINGER: Well,
there are some

00:53:33.320 --> 00:53:37.250
who present a
different ethical view.

00:53:37.250 --> 00:53:40.960
Effective altruism goes
well with a consequentialist

00:53:40.960 --> 00:53:43.070
or utilitarian ethic.

00:53:43.070 --> 00:53:45.580
But when I say,
goes well with it,

00:53:45.580 --> 00:53:49.364
I think you can be an effective
altruist as a deontoligist,

00:53:49.364 --> 00:53:50.780
that's somebody
who thinks that we

00:53:50.780 --> 00:53:53.230
ought to observe moral rules.

00:53:53.230 --> 00:53:56.620
Because generally speaking,
being an effective altruist

00:53:56.620 --> 00:53:59.940
doesn't require you to
violate moral rules.

00:53:59.940 --> 00:54:03.590
So clearly, in a lot
of use-- for example,

00:54:03.590 --> 00:54:06.520
in a standard
Christian ethic, there

00:54:06.520 --> 00:54:09.070
is room for helping the poor.

00:54:09.070 --> 00:54:11.710
That's something
that's emphasized.

00:54:11.710 --> 00:54:13.985
So you certainly don't
have to be a utilitarian

00:54:13.985 --> 00:54:15.310
or a consequentialist.

00:54:15.310 --> 00:54:18.820
But there has been a
fair amount of discussion

00:54:18.820 --> 00:54:22.070
in the literature about
the sorts of arguments

00:54:22.070 --> 00:54:23.310
that I've put forward.

00:54:23.310 --> 00:54:29.550
Generally, people talk about
the importance to individuals

00:54:29.550 --> 00:54:31.490
of pursuing their own projects.

00:54:31.490 --> 00:54:34.680
This is something that goes
back to Bernard Williams,

00:54:34.680 --> 00:54:37.530
no longer alive,
a philosopher who

00:54:37.530 --> 00:54:42.290
claimed that,
somehow, if you like,

00:54:42.290 --> 00:54:44.160
every human being has
their own commitments,

00:54:44.160 --> 00:54:47.040
their own projects,
their own desires.

00:54:47.040 --> 00:54:49.220
And what I'm
suggesting is somehow

00:54:49.220 --> 00:54:52.870
to take an external point
of view to that, something

00:54:52.870 --> 00:54:55.009
that I've called the
point-of-view of the universe.

00:54:55.009 --> 00:54:57.050
Not that the universe
really has a point-of-view,

00:54:57.050 --> 00:54:59.950
but to get across
that sort of idea.

00:54:59.950 --> 00:55:03.390
And Williams thinks that this
is an abstract notion that

00:55:03.390 --> 00:55:06.240
doesn't really motivate
people and that

00:55:06.240 --> 00:55:08.550
shouldn't motivate
them, that people should

00:55:08.550 --> 00:55:10.110
feel as though
they're able to pursue

00:55:10.110 --> 00:55:14.520
their own projects independently
of the external circumstances.

00:55:14.520 --> 00:55:20.520
In other words, it's not
that you're free to do that,

00:55:20.520 --> 00:55:22.680
even if the circumstance
is such that you could

00:55:22.680 --> 00:55:26.340
do more good in another way.

00:55:26.340 --> 00:55:29.550
So others talk about
rights and say, well, we

00:55:29.550 --> 00:55:31.890
have a right to a
certain level of comfort

00:55:31.890 --> 00:55:36.000
or a certain level of
meeting our needs beyond just

00:55:36.000 --> 00:55:37.660
the basic needs.

00:55:37.660 --> 00:55:41.376
So there certainly is
a literature on that.

00:55:41.376 --> 00:55:42.990
I don't know.

00:55:42.990 --> 00:55:45.876
I guess that debate is
going to be an ongoing one.

00:55:45.876 --> 00:55:47.500
As you say, philosophers
like to argue.

00:55:47.500 --> 00:55:50.260
And for any philosophical
thesis that anyone puts up,

00:55:50.260 --> 00:55:52.970
there'll certainly be counters.

00:55:52.970 --> 00:55:53.470
OK.

00:55:53.470 --> 00:55:54.930
And this'll be
the last one then.

00:55:54.930 --> 00:55:57.740
AUDIENCE: OK, so I graduated
college two years ago.

00:55:57.740 --> 00:56:00.650
And so, I'm relatively new to
this whole having-a-paycheck

00:56:00.650 --> 00:56:01.230
thing.

00:56:01.230 --> 00:56:03.120
And the first thing
that you learn

00:56:03.120 --> 00:56:06.220
about when you get your
paycheck is, here's your 401K.

00:56:06.220 --> 00:56:08.479
And you should start saving
for retirement, which

00:56:08.479 --> 00:56:09.770
feels ridiculous at the moment.

00:56:09.770 --> 00:56:12.120
But then, you see all
these numbers about, well,

00:56:12.120 --> 00:56:14.162
if I save this much now,
well then, when I'm 65--

00:56:14.162 --> 00:56:15.661
and you don't know
what will happen.

00:56:15.661 --> 00:56:17.150
And maybe I'll
have medical costs.

00:56:17.150 --> 00:56:21.750
So how do you weigh-- especially
speaking towards the younger

00:56:21.750 --> 00:56:28.860
crowd-- preparing for
potential risk in the future

00:56:28.860 --> 00:56:33.340
and being safe, prepared,
saving for retirement,

00:56:33.340 --> 00:56:35.450
versus being an
altruistic person,

00:56:35.450 --> 00:56:38.917
particularly in the
earlier phases of life?

00:56:38.917 --> 00:56:41.500
PETER SINGER: So I think you'll
find your own balance in that.

00:56:41.500 --> 00:56:47.500
I think people do want to put
some money towards those plans.

00:56:47.500 --> 00:56:50.460
As far as medical
costs are concerned,

00:56:50.460 --> 00:56:53.910
I think, in the United States,
as long as you're working,

00:56:53.910 --> 00:56:56.410
you're going to have
good insurance cover.

00:56:56.410 --> 00:56:58.070
When you get to be
65, you're going

00:56:58.070 --> 00:57:00.820
to have access to
Medicare, which is vastly

00:57:00.820 --> 00:57:03.380
better than most people have.

00:57:03.380 --> 00:57:07.520
And should you get dismissed
and fall into serious poverty,

00:57:07.520 --> 00:57:11.170
you're going to have Medicaid,
which is not a great program.

00:57:11.170 --> 00:57:13.770
But it's probably
pretty unlikely, really,

00:57:13.770 --> 00:57:15.460
that you're going to need that.

00:57:15.460 --> 00:57:18.200
So that seems to me
to be less of a worry.

00:57:18.200 --> 00:57:25.290
Although, in-- I think--
all other affluent nations,

00:57:25.290 --> 00:57:27.680
It's less of a concern than
it is in the United States.

00:57:27.680 --> 00:57:29.430
Because they have a
better national health

00:57:29.430 --> 00:57:30.180
service provision.

00:57:30.180 --> 00:57:31.920
But admittedly, you're here.

00:57:31.920 --> 00:57:34.380
And then, there are some holes
that you could conceivably

00:57:34.380 --> 00:57:36.100
fall through.

00:57:36.100 --> 00:57:41.010
So I think you will
still find, though,

00:57:41.010 --> 00:57:43.880
that when you've covered
those to a reasonable degree,

00:57:43.880 --> 00:57:47.380
you have a surplus
that hangs over

00:57:47.380 --> 00:57:49.650
and that that would
enable you to make

00:57:49.650 --> 00:57:50.870
those sorts of decisions.

00:57:50.870 --> 00:57:53.610
But I don't know, Jeff, whether
you want to comment on this.

00:57:53.610 --> 00:57:55.780
Jeff Kauffman is
somebody who has

00:57:55.780 --> 00:58:00.720
faced these decisions together
with Julia Wise, his partner.

00:58:00.720 --> 00:58:03.060
And Julia has written
about it online.

00:58:03.060 --> 00:58:05.340
Shall I ask people or
suggest that they go to that?

00:58:05.340 --> 00:58:08.140
JEFF KAUFFMAN: We try
to half of what we earn.

00:58:08.140 --> 00:58:10.290
And we end up saving
for retirement.

00:58:10.290 --> 00:58:12.480
We do the full Google match.

00:58:12.480 --> 00:58:14.080
Because it's, sort
of, free money.

00:58:14.080 --> 00:58:23.450
And after that, we save maybe
another 5% to 10% on top

00:58:23.450 --> 00:58:24.281
of that.

00:58:24.281 --> 00:58:25.444
We were saving for a house.

00:58:25.444 --> 00:58:27.277
Now, we've actually
been spending that down.

00:58:27.277 --> 00:58:28.891
Because we just bought a house.

00:58:28.891 --> 00:58:36.150
But the basic idea being, that
we set a budget for donation.

00:58:36.150 --> 00:58:37.670
We put half our money into that.

00:58:37.670 --> 00:58:40.800
And then, the rest of it we
divide the way anyone who

00:58:40.800 --> 00:58:44.372
earned half as much
as us would divide,

00:58:44.372 --> 00:58:46.205
thinking about what
we'll need in the future

00:58:46.205 --> 00:58:47.166
and what we need now.

00:58:50.570 --> 00:58:52.160
AUDIENCE: I take a
similar but, maybe,

00:58:52.160 --> 00:58:53.660
opposite approach,
which is actually

00:58:53.660 --> 00:58:56.560
that I save, like, half of
my income for retirement.

00:58:56.560 --> 00:58:58.310
And I give a few percent now.

00:58:58.310 --> 00:59:01.710
And my thinking is, if I don't
want to retire in 10 years,

00:59:01.710 --> 00:59:04.420
I can keep working and just
donate all my income after that

00:59:04.420 --> 00:59:06.652
and still have a safety net.

00:59:06.652 --> 00:59:07.360
PETER SINGER: OK.

00:59:07.360 --> 00:59:08.170
AUDIENCE: You guys are awesome.

00:59:08.170 --> 00:59:08.940
It's great.

00:59:08.940 --> 00:59:12.000
PETER SINGER: A couple of
interesting and impressive

00:59:12.000 --> 00:59:13.282
strategies.

00:59:13.282 --> 00:59:14.630
Good to know that they're there.

00:59:14.630 --> 00:59:15.070
Have we got--?

00:59:15.070 --> 00:59:15.540
Lovely.

00:59:15.540 --> 00:59:17.165
AUDIENCE: You could
save for retirement

00:59:17.165 --> 00:59:19.335
and then put in a
will that whatever

00:59:19.335 --> 00:59:21.460
you didn't need to take
care of you in your old age

00:59:21.460 --> 00:59:22.793
could go to charity.

00:59:22.793 --> 00:59:26.430
PETER SINGER: You could do that.

00:59:26.430 --> 00:59:29.020
Well, for one thing,
I think it's actually

00:59:29.020 --> 00:59:31.240
a fulfilling thing to do
to give while you're alive.

00:59:31.240 --> 00:59:34.480
And you can see where it's going
and know what you're doing.

00:59:34.480 --> 00:59:36.910
And I think, in some
ways, that's better.

00:59:36.910 --> 00:59:39.600
Also, you do need to be careful.

00:59:39.600 --> 00:59:40.530
Because things change.

00:59:40.530 --> 00:59:43.750
For example, in giving,
GiveWell changes its list

00:59:43.750 --> 00:59:45.460
of most effective charities.

00:59:45.460 --> 00:59:47.740
Each year, it revises
it and those change.

00:59:47.740 --> 00:59:54.390
So if you make a will at some
time and revise it frequently,

00:59:54.390 --> 00:59:57.990
it can become out-of-date,
in terms of where it's going.

00:59:57.990 --> 00:59:59.758
AUDIENCE: Thank you for that.

00:59:59.758 --> 01:00:01.530
PETER SINGER: You're welcome.

01:00:01.530 --> 01:00:04.130
OK, I think we're
probably out of time.

01:00:04.130 --> 01:00:05.360
Thanks very much for coming.

01:00:05.360 --> 01:00:06.526
I appreciate your questions.

01:00:06.526 --> 01:00:08.410
[APPLAUSE]

