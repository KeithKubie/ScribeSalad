WEBVTT
Kind: captions
Language: en

00:00:01.970 --> 00:00:04.050
BORIS DEBIC: Welcome,
everybody to one more

00:00:04.050 --> 00:00:05.520
Authors at Google talk.

00:00:05.520 --> 00:00:09.330
Today with us, Brian
Christian and Tom Griffiths.

00:00:09.330 --> 00:00:12.390
Their book, "Algorithms
to Live By,"

00:00:12.390 --> 00:00:17.360
is now available in all the
fine bookstores in the area

00:00:17.360 --> 00:00:21.626
and in the US and around
the world and whatnot.

00:00:21.626 --> 00:00:25.500
Brian Christian is the author
of "The Most Human Human,"

00:00:25.500 --> 00:00:27.650
a "Wall Street Journal"
bestseller, "New York

00:00:27.650 --> 00:00:29.630
Times" editor's choice,
and "New Yorker"

00:00:29.630 --> 00:00:31.290
favorite book of the year.

00:00:31.290 --> 00:00:34.400
His writing has appeared in "The
New Yorker," "The Atlantic,"

00:00:34.400 --> 00:00:36.770
"Wired," the "Wall Street
Journal," "The Guardian,"

00:00:36.770 --> 00:00:39.840
and "The Paris Review," as
well as in scientific journals

00:00:39.840 --> 00:00:42.060
such as "Cognitive Science."

00:00:42.060 --> 00:00:44.590
He has been translated
into 11 languages.

00:00:44.590 --> 00:00:46.570
He lives in San Francisco.

00:00:46.570 --> 00:00:49.410
Tom Griffiths is a professor
of psychology and cognitive

00:00:49.410 --> 00:00:52.050
science at UC Berkeley,
where he directs

00:00:52.050 --> 00:00:54.320
the computational
cognitive science lab.

00:00:54.320 --> 00:00:57.390
He has published more
than 150 scientific papers

00:00:57.390 --> 00:01:00.600
on topics ranging from
cognitive science psychology

00:01:00.600 --> 00:01:03.330
to cultural evolution.

00:01:03.330 --> 00:01:06.270
He has received awards from the
National Science Foundation,

00:01:06.270 --> 00:01:08.480
the Sloan Foundation,
the American Psychology

00:01:08.480 --> 00:01:14.270
Association, and the Psychonomic
Society, among others.

00:01:14.270 --> 00:01:15.870
He lives in Berkeley.

00:01:15.870 --> 00:01:20.300
So I'm not going to talk
too much about the book

00:01:20.300 --> 00:01:22.330
because we have
Peter Norvig here,

00:01:22.330 --> 00:01:24.980
who will explain
to you why this may

00:01:24.980 --> 00:01:29.842
be the-- we call it the gateway
drug to computer science.

00:01:29.842 --> 00:01:30.760
Come on, Peter.

00:01:33.782 --> 00:01:34.490
PETER NORVIG: OK.

00:01:34.490 --> 00:01:37.670
So you remember Apple
had this campaign that

00:01:37.670 --> 00:01:39.170
said, think different?

00:01:39.170 --> 00:01:41.890
And I think we
realize that we all

00:01:41.890 --> 00:01:43.291
think different in some ways.

00:01:43.291 --> 00:01:43.790
Right?

00:01:43.790 --> 00:01:47.070
So we're all nerds or
whatever you want to call us.

00:01:47.070 --> 00:01:50.850
And sometimes when you go out
with your friends and family,

00:01:50.850 --> 00:01:53.830
you realize, oh, I think
differently than them.

00:01:53.830 --> 00:01:58.920
And I think that's what
this book is about.

00:01:58.920 --> 00:02:00.680
Boris talks about it
as a gateway drug,

00:02:00.680 --> 00:02:02.160
and I think that's great.

00:02:02.160 --> 00:02:05.030
That's a good way to think
about it because we're coming up

00:02:05.030 --> 00:02:07.641
now-- there's another
hour of code thing,

00:02:07.641 --> 00:02:08.849
and we're going to celebrate.

00:02:08.849 --> 00:02:11.940
And we're going to try to
teach everybody to code.

00:02:11.940 --> 00:02:16.410
But learning how to draw
a rectangle on the screen

00:02:16.410 --> 00:02:18.630
and figuring out that
the third argument is

00:02:18.630 --> 00:02:22.780
the color and the first two are
the x- and the y-coordinates,

00:02:22.780 --> 00:02:23.747
that's not really it.

00:02:23.747 --> 00:02:25.330
That doesn't change
the way you think.

00:02:25.330 --> 00:02:26.890
I mean, that's a
good skill, to be

00:02:26.890 --> 00:02:32.580
able to draw rectangles
and circles on the screen.

00:02:32.580 --> 00:02:37.220
But really what's important is
being able to model the world.

00:02:37.220 --> 00:02:41.600
Jeanette Wing talks about it
as computational thinking.

00:02:41.600 --> 00:02:43.930
And I think there's a mix
between different types

00:02:43.930 --> 00:02:44.470
of thinking.

00:02:44.470 --> 00:02:45.970
There's this
computational thinking.

00:02:45.970 --> 00:02:47.303
There's a mathematical thinking.

00:02:47.303 --> 00:02:49.910
There's a statistical thinking.

00:02:49.910 --> 00:02:51.380
And we all have
some combinations

00:02:51.380 --> 00:02:53.790
of all of those things.

00:02:53.790 --> 00:02:55.600
And those skills
together, I think,

00:02:55.600 --> 00:02:58.730
are more important than
the details of the API

00:02:58.730 --> 00:03:02.390
for JavaScript objects.

00:03:02.390 --> 00:03:04.050
And this book really gets at it.

00:03:04.050 --> 00:03:05.510
It doesn't teach
you how to code,

00:03:05.510 --> 00:03:07.910
but it teaches you how
to think in that way.

00:03:07.910 --> 00:03:11.940
And it gives you examples to
ponder about your everyday life

00:03:11.940 --> 00:03:14.144
and why it might be
important to think that way

00:03:14.144 --> 00:03:15.310
and when you can do with it.

00:03:15.310 --> 00:03:18.196
So tell us all about
it, Brian and Tom.

00:03:18.196 --> 00:03:21.682
[APPLAUSE]

00:03:22.652 --> 00:03:24.110
BRIAN CHRISTIAN:
Thank you so much,

00:03:24.110 --> 00:03:25.910
Boris and Peter, for
the introduction,

00:03:25.910 --> 00:03:31.450
and thanks to Google for the
invitation to come speak.

00:03:31.450 --> 00:03:33.730
The talk is "Algorithms
to Live By."

00:03:33.730 --> 00:03:34.850
I'm Brian.

00:03:34.850 --> 00:03:37.590
This is Tom, in case you were
curious which of us is which.

00:03:37.590 --> 00:03:40.620
We sometimes get that question
if we've forgotten to introduce

00:03:40.620 --> 00:03:42.060
ourselves at the top.

00:03:42.060 --> 00:03:45.270
And the book opens
with an example

00:03:45.270 --> 00:03:49.670
that I think will be acutely,
perhaps uncomfortably,

00:03:49.670 --> 00:03:51.860
familiar to many of us
here in the Bay Area, which

00:03:51.860 --> 00:03:55.470
is looking for housing.

00:03:55.470 --> 00:03:58.830
So in a typical consumer
situation, the way

00:03:58.830 --> 00:04:02.970
you make a choice is you
consider a pool of options.

00:04:02.970 --> 00:04:05.590
You think hard about which
one you like the best.

00:04:05.590 --> 00:04:07.460
And then you go with that.

00:04:07.460 --> 00:04:10.184
But in a sufficiently
crowded real estate market,

00:04:10.184 --> 00:04:12.350
in a sufficiently competitive
market-- which the Bay

00:04:12.350 --> 00:04:15.370
Area certainly
is-- you don't have

00:04:15.370 --> 00:04:17.640
the luxury of making the
decision in that way.

00:04:17.640 --> 00:04:20.660
Rather, the decision
takes the form

00:04:20.660 --> 00:04:25.900
of evaluating a sequence
of options one at a time,

00:04:25.900 --> 00:04:28.430
going to a number of
different open houses,

00:04:28.430 --> 00:04:31.290
and at each point
in time, you must

00:04:31.290 --> 00:04:33.670
make an irrevocable commitment.

00:04:33.670 --> 00:04:36.380
You either take the place that
you're looking at right there

00:04:36.380 --> 00:04:39.900
on the spot, never knowing what
else might have been out there,

00:04:39.900 --> 00:04:42.630
or you walk away,
never to return.

00:04:42.630 --> 00:04:44.910
You have almost no chance
of going back and getting

00:04:44.910 --> 00:04:47.490
the place.

00:04:47.490 --> 00:04:50.230
This is certainly a much
more fraught setting

00:04:50.230 --> 00:04:55.190
for making a decision because
here the critical question

00:04:55.190 --> 00:04:58.790
that you have to ask yourself
is not which option to pick

00:04:58.790 --> 00:05:02.210
but how many options
to even consider.

00:05:02.210 --> 00:05:06.860
Intuitively, we have
this idea that you

00:05:06.860 --> 00:05:08.410
want to look before you leap.

00:05:08.410 --> 00:05:11.180
You don't want to make
a premature choice.

00:05:11.180 --> 00:05:14.004
You want to get a sense
of what's out there.

00:05:14.004 --> 00:05:15.420
But you don't want
to hold out too

00:05:15.420 --> 00:05:18.020
long for some kind of perfect
thing that doesn't exist

00:05:18.020 --> 00:05:21.500
And let the best
options pass you by.

00:05:21.500 --> 00:05:23.870
So our intuition
tells us that we

00:05:23.870 --> 00:05:26.610
have to strike some
kind of balance

00:05:26.610 --> 00:05:29.160
between getting a feel
for what's out there

00:05:29.160 --> 00:05:31.640
and setting a standard and
knowing a good thing when

00:05:31.640 --> 00:05:34.070
we see it and being
ready to commit.

00:05:34.070 --> 00:05:36.860
And the story that we get
from mathematics and computer

00:05:36.860 --> 00:05:39.290
science is that this
notion of balance

00:05:39.290 --> 00:05:41.540
is, in fact, precisely correct.

00:05:41.540 --> 00:05:43.930
But our intuition
alone doesn't tell us

00:05:43.930 --> 00:05:45.640
what that balance should be.

00:05:45.640 --> 00:05:49.090
The answer is 37%.

00:05:49.090 --> 00:05:51.790
If you want the very
best odds of finding

00:05:51.790 --> 00:05:54.390
the very best
apartment, consider

00:05:54.390 --> 00:06:00.690
exactly 37% of the pool of
options in front of you.

00:06:00.690 --> 00:06:04.050
Or alternately, you can think
of it in terms of time--

00:06:04.050 --> 00:06:08.430
37% of your time
just calibrating.

00:06:08.430 --> 00:06:11.207
Leave your checkbook at home.

00:06:11.207 --> 00:06:13.790
If you've given yourself a month
for the search, in this case,

00:06:13.790 --> 00:06:15.770
that would be 11 days.

00:06:15.770 --> 00:06:20.040
After that point, be prepared
to immediately commit

00:06:20.040 --> 00:06:22.650
to the first thing you see
that's better than what

00:06:22.650 --> 00:06:26.300
you saw in that first 37%.

00:06:26.300 --> 00:06:29.360
This is not only the
intuitively-satisfying

00:06:29.360 --> 00:06:31.530
compromise between
looking and leaping,

00:06:31.530 --> 00:06:35.550
this is the provably optimal
solution to this problem.

00:06:35.550 --> 00:06:37.450
So, for example,
if you're committed

00:06:37.450 --> 00:06:41.130
to living in one of the painted
ladies here in San Francisco,

00:06:41.130 --> 00:06:44.510
and they have their open
houses on successive weekends,

00:06:44.510 --> 00:06:48.170
the algorithm tells you
to look at the first two,

00:06:48.170 --> 00:06:51.930
and no matter how tempting
they seem, hold tight.

00:06:51.930 --> 00:06:54.270
And then, starting
with the third one,

00:06:54.270 --> 00:06:56.010
immediately leap
for the first one

00:06:56.010 --> 00:07:00.020
better than what you
saw at the first two.

00:07:00.020 --> 00:07:03.810
Now, more broadly,
this is known as what's

00:07:03.810 --> 00:07:06.730
called an optimal
stopping problem.

00:07:06.730 --> 00:07:09.230
And this structure
of encountering

00:07:09.230 --> 00:07:13.580
a series of options
and being forced

00:07:13.580 --> 00:07:17.450
to make a commitment one way or
another-- either you're all in

00:07:17.450 --> 00:07:20.590
or you walk away--
some people have

00:07:20.590 --> 00:07:24.470
argued that this is a structure
that describes not only

00:07:24.470 --> 00:07:29.580
things like the apartment hunt
or real estate in general.

00:07:29.580 --> 00:07:33.540
It also, many people have
argued, describes dating.

00:07:33.540 --> 00:07:36.050
You're in a relationship,
and you have this decision

00:07:36.050 --> 00:07:39.050
of when to commit.

00:07:39.050 --> 00:07:41.960
Have you met enough people
to have a sense of who

00:07:41.960 --> 00:07:44.590
your best match really is?

00:07:44.590 --> 00:07:47.010
And so you can do some
back-of-the-envelope math

00:07:47.010 --> 00:07:51.370
and say things like, OK, the
average expected lifespan

00:07:51.370 --> 00:07:53.020
of an American is 79.

00:07:53.020 --> 00:07:56.110
37% of that gives me 29.

00:07:56.110 --> 00:07:58.460
And this roughly
divides my romantic life

00:07:58.460 --> 00:08:02.200
into dating for fun
versus dating seriously

00:08:02.200 --> 00:08:06.190
to really evaluate for a mate.

00:08:06.190 --> 00:08:10.520
Of course, as we will see, it
all depends on the assumptions

00:08:10.520 --> 00:08:12.320
that you're willing
to make about love.

00:08:17.570 --> 00:08:20.020
TOM GRIFFITHS: So
as you've seen,

00:08:20.020 --> 00:08:22.890
simple algorithms can solve
some of the problems which

00:08:22.890 --> 00:08:25.730
we actually normally think about
not as problems for computers

00:08:25.730 --> 00:08:27.070
but as problems for people.

00:08:27.070 --> 00:08:28.160
So there are a set
of problems which

00:08:28.160 --> 00:08:30.243
we have to solve just as
a consequence of the fact

00:08:30.243 --> 00:08:33.669
that our lives are lived in
finite space and finite time--

00:08:33.669 --> 00:08:36.320
so having to do things like try
and figure out how to organize

00:08:36.320 --> 00:08:38.640
our house or our closet
or our office in a way

00:08:38.640 --> 00:08:40.830
to make it most efficient
or trying to figure out

00:08:40.830 --> 00:08:42.530
how to schedule our
time so that we can

00:08:42.530 --> 00:08:44.166
do the most possible things.

00:08:44.166 --> 00:08:45.540
And we normally
think about those

00:08:45.540 --> 00:08:47.249
as being fundamentally
human problems.

00:08:47.249 --> 00:08:49.290
But really, the argument
that we make in the book

00:08:49.290 --> 00:08:50.830
is that they're not.

00:08:50.830 --> 00:08:53.120
They have analogs
to the problems

00:08:53.120 --> 00:08:54.490
that computers have to solve.

00:08:54.490 --> 00:08:56.370
So, you know, your
computer has to figure out

00:08:56.370 --> 00:08:58.800
how to manage its space--
its space on the hard disk

00:08:58.800 --> 00:09:00.116
and its space in memory.

00:09:00.116 --> 00:09:01.490
And it also has
to figure out how

00:09:01.490 --> 00:09:03.430
to manage its time-- what
it's going to do next,

00:09:03.430 --> 00:09:04.720
what program it's going to run.

00:09:04.720 --> 00:09:06.430
And as a consequence,
computer scientists

00:09:06.430 --> 00:09:08.360
have put a lot of thought into
coming up with good algorithms

00:09:08.360 --> 00:09:09.990
for solving those problems.

00:09:09.990 --> 00:09:12.060
So what we do in
the book is explore

00:09:12.060 --> 00:09:15.420
how taking this perspective
gives us insight

00:09:15.420 --> 00:09:17.990
into the problems that
human beings have to solve,

00:09:17.990 --> 00:09:20.760
in some cases offering us
nice, elegant, simple solutions

00:09:20.760 --> 00:09:22.990
to these problems,
like the 37% rule,

00:09:22.990 --> 00:09:25.990
in other cases giving us new
ways of thinking about how

00:09:25.990 --> 00:09:29.510
those problems might be
described in mathematical terms

00:09:29.510 --> 00:09:33.010
and given, perhaps, a kind
of quantitative analysis.

00:09:33.010 --> 00:09:35.849
So we consider a range of
different problems-- so

00:09:35.849 --> 00:09:38.390
the optimal stopping problem,
which you've heard a little bit

00:09:38.390 --> 00:09:40.910
about, as well as a
variety of other contexts

00:09:40.910 --> 00:09:42.950
in which thinking
algorithmically actually

00:09:42.950 --> 00:09:45.950
gives us a new insight into
a human sort of problem.

00:09:45.950 --> 00:09:49.570
And what we're going to do
today is talk about just a few

00:09:49.570 --> 00:09:50.840
of these problems.

00:09:50.840 --> 00:09:52.410
In the book, we
also talk about some

00:09:52.410 --> 00:09:53.909
of the more general
principles which

00:09:53.909 --> 00:09:55.640
go into designing
good algorithms

00:09:55.640 --> 00:09:59.830
or thinking about the
kinds of underlying ideas

00:09:59.830 --> 00:10:01.250
that are useful
when we're trying

00:10:01.250 --> 00:10:04.440
to engage with the world
in this computational way.

00:10:04.440 --> 00:10:06.890
So the three problems that
we're going to focus on today

00:10:06.890 --> 00:10:10.450
are optimal stopping, the
explore or exploit trade-off,

00:10:10.450 --> 00:10:11.870
and caching.

00:10:11.870 --> 00:10:13.710
And optimal stopping
you've already

00:10:13.710 --> 00:10:15.210
heard a little bit about.

00:10:15.210 --> 00:10:17.270
The 37% rule is
just one instance

00:10:17.270 --> 00:10:20.150
of a strategy which is useful
for solving an optimal stopping

00:10:20.150 --> 00:10:21.070
problem.

00:10:21.070 --> 00:10:22.570
It's the solution
to a problem which

00:10:22.570 --> 00:10:25.360
is known in the mathematical
literature as the secretary

00:10:25.360 --> 00:10:26.340
problem.

00:10:26.340 --> 00:10:29.070
So the canonical set-up
for this is that you're

00:10:29.070 --> 00:10:30.710
trying to hire a secretary.

00:10:30.710 --> 00:10:32.600
You have a pool of applicants.

00:10:32.600 --> 00:10:34.460
Each applicant comes
in and is interviewed.

00:10:34.460 --> 00:10:36.501
You don't have a way of
evaluating the applicants

00:10:36.501 --> 00:10:37.790
except against one another.

00:10:37.790 --> 00:10:39.880
So you can only make a
judgement of how good they

00:10:39.880 --> 00:10:42.880
are based on the applicants
that you've seen so far.

00:10:42.880 --> 00:10:44.350
And for each person
who comes in,

00:10:44.350 --> 00:10:46.100
you have to make a
decision immediately as

00:10:46.100 --> 00:10:48.200
to whether you hire that
person or whether you

00:10:48.200 --> 00:10:51.220
dismiss them, in which case
you'll never see them again.

00:10:51.220 --> 00:10:54.940
So this secretary
problem was first

00:10:54.940 --> 00:10:57.030
presented to the
public in the 1960s

00:10:57.030 --> 00:10:59.272
in a column that was
written by Martin Gardner.

00:10:59.272 --> 00:11:01.730
But in the book, we trace the
history of this problem back,

00:11:01.730 --> 00:11:04.240
and it turns out that
romance was at its core.

00:11:04.240 --> 00:11:07.190
So what we actually found by
doing some archival research

00:11:07.190 --> 00:11:10.320
is that the person who claims
to have originated the problem

00:11:10.320 --> 00:11:12.541
is a mathematician
called Merrill flood.

00:11:12.541 --> 00:11:14.040
And the story that
he tells about it

00:11:14.040 --> 00:11:16.180
is that his daughter,
who had just

00:11:16.180 --> 00:11:19.219
graduated from high school,
was engaging in a relationship

00:11:19.219 --> 00:11:21.010
which he and his wife
didn't really approve

00:11:21.010 --> 00:11:22.550
of with a much older man.

00:11:22.550 --> 00:11:25.310
And so he wanted to
somehow convince her

00:11:25.310 --> 00:11:26.870
that this was a bad idea.

00:11:26.870 --> 00:11:28.590
So being a
mathematician, the way

00:11:28.590 --> 00:11:31.234
that he approached
this problem was she

00:11:31.234 --> 00:11:33.400
turned out to be taking the
minutes at a mathematics

00:11:33.400 --> 00:11:35.730
conference that Flood was
scheduled to present at.

00:11:35.730 --> 00:11:38.740
And so he went up, and
he presented this problem

00:11:38.740 --> 00:11:42.444
of a woman who's entertaining
a series of suitors

00:11:42.444 --> 00:11:44.110
and faced with the
challenge of deciding

00:11:44.110 --> 00:11:46.450
which of those suitors'
proposals she should accept.

00:11:46.450 --> 00:11:48.250
And he didn't actually
know the solution

00:11:48.250 --> 00:11:49.190
to the problem at that point.

00:11:49.190 --> 00:11:51.580
But he was pretty sure that
the number was larger than one.

00:11:51.580 --> 00:11:53.680
And so he was hoping that
his daughter would sort of

00:11:53.680 --> 00:11:55.513
take the message as she
was writing it down,

00:11:55.513 --> 00:11:58.560
and sort of think about how it
applied to her own situation.

00:11:58.560 --> 00:12:00.850
So as we've seen, the
solution to this problem

00:12:00.850 --> 00:12:02.507
turns out to be 37%.

00:12:02.507 --> 00:12:04.090
But as Brian was
saying, a lot of that

00:12:04.090 --> 00:12:05.890
depends on the
assumptions that you make.

00:12:05.890 --> 00:12:08.150
And there's actually been
a history of mathematicians

00:12:08.150 --> 00:12:10.810
seeking love that provide some
cautionary tales and perhaps

00:12:10.810 --> 00:12:13.400
some insights into variants
on this problem, which

00:12:13.400 --> 00:12:15.920
are things that are worth
paying attention to.

00:12:15.920 --> 00:12:18.240
So one kind of
situation that you

00:12:18.240 --> 00:12:20.750
can encounter when trying
to pursue this approach

00:12:20.750 --> 00:12:21.990
is rejection.

00:12:21.990 --> 00:12:24.830
And we actually found a
story of Michael Trick,

00:12:24.830 --> 00:12:31.630
who's an operations researcher
at Carnegie Mellon University.

00:12:31.630 --> 00:12:36.920
And he told the story of
applying the secretary problem

00:12:36.920 --> 00:12:39.400
in very much the same way
that Brian was describing--

00:12:39.400 --> 00:12:40.880
calculating the
period over which

00:12:40.880 --> 00:12:43.880
he thought he'd be searching,
working out what 37% was.

00:12:43.880 --> 00:12:46.520
And it happened that the age
at which he should switch

00:12:46.520 --> 00:12:49.520
from having fun to being serious
was precisely his current age.

00:12:49.520 --> 00:12:52.120
And so he went and
proposed to his girlfriend,

00:12:52.120 --> 00:12:53.770
who turned him down.

00:12:53.770 --> 00:12:57.715
So Trick ran into one
of the assumptions that

00:12:57.715 --> 00:12:59.340
are being made here,
which is that when

00:12:59.340 --> 00:13:00.798
you make an offer
to somebody, they

00:13:00.798 --> 00:13:02.514
should be willing to accept it.

00:13:02.514 --> 00:13:03.930
And only under
those circumstances

00:13:03.930 --> 00:13:06.160
is the 37% rule valid.

00:13:06.160 --> 00:13:08.764
If somebody is potentially
able to reject your offer,

00:13:08.764 --> 00:13:10.930
then it changes the strategy
that you should follow.

00:13:10.930 --> 00:13:13.460
And in particular, it means
that the period that you spend

00:13:13.460 --> 00:13:14.840
looking should be reduced.

00:13:14.840 --> 00:13:19.160
So, for example, if you have a
50% chance of being rejected,

00:13:19.160 --> 00:13:22.290
then you should look
at the first 25%

00:13:22.290 --> 00:13:24.970
of your potential
candidates, and then

00:13:24.970 --> 00:13:27.680
be willing to make an offer
to the first person who's

00:13:27.680 --> 00:13:31.630
better than anybody you
saw in that first 25%.

00:13:31.630 --> 00:13:33.760
Another variant on
the secretary problem

00:13:33.760 --> 00:13:36.350
is what's called recall.

00:13:36.350 --> 00:13:38.160
And so this is having
the opportunity

00:13:38.160 --> 00:13:40.150
to go back to somebody
who you passed over.

00:13:40.150 --> 00:13:42.490
So you can imagine your
candidates come in, but rather

00:13:42.490 --> 00:13:44.240
than dismissing them meaning
that you'll never see them

00:13:44.240 --> 00:13:45.920
again, dismissing
them just decreases

00:13:45.920 --> 00:13:48.003
the chance that you'll be
able to go back to them.

00:13:48.003 --> 00:13:49.680
Or in the dating
setting, basically,

00:13:49.680 --> 00:13:52.210
breaking up with somebody
means that maybe there's

00:13:52.210 --> 00:13:54.390
a chance that they'd
take you back later on.

00:13:54.390 --> 00:13:55.960
So there's actually
a mathematician

00:13:55.960 --> 00:13:58.470
who experienced exactly
this phenomenon.

00:13:58.470 --> 00:14:01.420
This is the astronomer
Johannes Kepler, who

00:14:01.420 --> 00:14:04.380
after his first wife died went
through an extended period

00:14:04.380 --> 00:14:06.060
of courting various
women, trying

00:14:06.060 --> 00:14:08.860
to sort of evaluate and come up
with the person who he thought

00:14:08.860 --> 00:14:11.380
was going to be the very
best person for him.

00:14:11.380 --> 00:14:13.410
And so over an
extended period, he

00:14:13.410 --> 00:14:17.560
ended up interacting
with 11 women.

00:14:17.560 --> 00:14:20.820
And having sort of explored
those possibilities,

00:14:20.820 --> 00:14:23.260
he decided, getting to
the end of that process,

00:14:23.260 --> 00:14:25.970
that number five, who
he'd previously dismissed,

00:14:25.970 --> 00:14:28.160
was actually the
best one for him.

00:14:28.160 --> 00:14:30.160
So he went back, and he
made an offer to her.

00:14:30.160 --> 00:14:32.544
And it turned out she hadn't
accepted any other proposals

00:14:32.544 --> 00:14:33.210
in the meantime.

00:14:33.210 --> 00:14:35.043
And so luckily, they
were able to be married

00:14:35.043 --> 00:14:36.760
and had a happy
marriage together.

00:14:36.760 --> 00:14:39.060
So this possibility-- that
you can actually go back

00:14:39.060 --> 00:14:41.830
to somebody, and they
can still say yes--

00:14:41.830 --> 00:14:44.360
changes where you should
set your threshold again.

00:14:44.360 --> 00:14:46.100
So in this case, it
makes it something

00:14:46.100 --> 00:14:48.630
where you should spend a
little longer looking around.

00:14:48.630 --> 00:14:50.940
So, for example, if
there's a 50% chance

00:14:50.940 --> 00:14:54.820
that somebody who
you go back to is

00:14:54.820 --> 00:14:56.820
going to be willing to
accept your offer anyway,

00:14:56.820 --> 00:15:00.242
then you should look
at the first 61%,

00:15:00.242 --> 00:15:02.700
and then use that to set the
standard which you'll then use

00:15:02.700 --> 00:15:04.060
for evaluating the remainder.

00:15:04.060 --> 00:15:05.810
And then if you get to the
very end of this period

00:15:05.810 --> 00:15:07.275
and haven't found
anybody, then you

00:15:07.275 --> 00:15:08.900
go back and make an
offer to the person

00:15:08.900 --> 00:15:11.233
who was the very best, which
you now know because you've

00:15:11.233 --> 00:15:13.034
explored all of the options.

00:15:13.034 --> 00:15:14.456
PETER NORVIG: Tom?

00:15:14.456 --> 00:15:20.430
So if you had prior
knowledge of the distribution

00:15:20.430 --> 00:15:22.490
that you're drawing
from, that should also

00:15:22.490 --> 00:15:23.790
[INAUDIBLE] the period, right?

00:15:23.790 --> 00:15:24.956
TOM GRIFFITHS: That's right.

00:15:24.956 --> 00:15:27.000
So in this case, we're
assuming that the only way

00:15:27.000 --> 00:15:30.310
that you can evaluate people is
relative to one another, right?

00:15:30.310 --> 00:15:31.999
And so that's
something which then

00:15:31.999 --> 00:15:33.540
leads to this kind
of strategy, where

00:15:33.540 --> 00:15:34.980
you have to spend
some time building up

00:15:34.980 --> 00:15:37.220
an impression of what the
distribution looks like.

00:15:37.220 --> 00:15:39.102
So basically, pretty
much any variant

00:15:39.102 --> 00:15:40.560
on this problem
you can imagine has

00:15:40.560 --> 00:15:43.030
been explored by mathematicians
in the last 50 years.

00:15:43.030 --> 00:15:46.297
The case that you're talking
about, where you have what's

00:15:46.297 --> 00:15:47.880
called full information--
you actually

00:15:47.880 --> 00:15:49.790
know what the
distribution is like-- is

00:15:49.790 --> 00:15:52.580
one where the overall strategy
looks quite different.

00:15:52.580 --> 00:15:56.330
So in that case, what you
should do is set a threshold.

00:15:56.330 --> 00:15:58.660
And then as you go
through the pool

00:15:58.660 --> 00:16:01.950
and you're sort of remaining
candidates become sparser,

00:16:01.950 --> 00:16:03.795
that threshold becomes
lower and lower.

00:16:03.795 --> 00:16:05.170
Some of us might
have experienced

00:16:05.170 --> 00:16:08.650
this in the context of other
kinds of romantic interactions.

00:16:08.650 --> 00:16:11.200
But it certainly makes
dire predictions, perhaps,

00:16:11.200 --> 00:16:13.890
as you age and people
start getting married,

00:16:13.890 --> 00:16:17.660
that you should be willing
to accept, perhaps,

00:16:17.660 --> 00:16:20.097
a slightly-- set a
slightly lower threshold

00:16:20.097 --> 00:16:22.180
in terms of the way that
you approach the problem.

00:16:22.180 --> 00:16:23.670
And there are also
variants which are

00:16:23.670 --> 00:16:25.544
what are called partial
information versions,

00:16:25.544 --> 00:16:29.520
where you come in not knowing
what the distribution is but

00:16:29.520 --> 00:16:32.172
having some information about
what that distribution is.

00:16:32.172 --> 00:16:33.880
And those turn out to
be some of the sort

00:16:33.880 --> 00:16:37.890
of most interesting and
mathematically intricate cases.

00:16:37.890 --> 00:16:40.234
So these examples
illustrate some

00:16:40.234 --> 00:16:41.650
of the ways in
which the secretary

00:16:41.650 --> 00:16:43.380
problem can be made
more complicated

00:16:43.380 --> 00:16:48.290
and can accommodate some of
the other kinds of situations

00:16:48.290 --> 00:16:51.290
that we might encounter in
realistic romantic scenarios.

00:16:51.290 --> 00:16:54.840
But this is, by all
means, not the limits

00:16:54.840 --> 00:16:57.070
of optimal stopping in
terms of its implications

00:16:57.070 --> 00:16:58.080
for human lives.

00:16:58.080 --> 00:17:00.320
So basically, any
situation where

00:17:00.320 --> 00:17:05.880
you have to make a decision
about whether to continue

00:17:05.880 --> 00:17:08.530
to gather information or
to continue to consider

00:17:08.530 --> 00:17:11.161
opportunities or to
act on the information

00:17:11.161 --> 00:17:12.619
that you have so
far is going to be

00:17:12.619 --> 00:17:14.702
one which is going to fit
this same kind of schema

00:17:14.702 --> 00:17:15.670
of optimal stopping.

00:17:15.670 --> 00:17:19.550
So another example is deciding
when to sell an asset.

00:17:19.550 --> 00:17:22.300
So be it your house
or your company,

00:17:22.300 --> 00:17:24.050
you have to make a
decision in a situation

00:17:24.050 --> 00:17:25.990
where you might receive
a series of offers.

00:17:25.990 --> 00:17:27.814
But in this case,
each of those offers

00:17:27.814 --> 00:17:29.230
is going to cost
you money, right?

00:17:29.230 --> 00:17:30.390
You're going to
be waiting around

00:17:30.390 --> 00:17:32.550
for somebody to come along
and make another offer.

00:17:32.550 --> 00:17:34.920
And your goal is to
maximize the amount of money

00:17:34.920 --> 00:17:36.610
that you get out
of those offers.

00:17:36.610 --> 00:17:39.050
So this can be formulated as
an optimal stopping problem.

00:17:39.050 --> 00:17:40.210
You have to decide
for each offer

00:17:40.210 --> 00:17:41.876
whether you're going
to take that offer.

00:17:41.876 --> 00:17:45.177
And there's a
simple formula that

00:17:45.177 --> 00:17:46.760
describes the strategy
that you should

00:17:46.760 --> 00:17:49.010
take for solving this problem.

00:17:49.010 --> 00:17:51.060
So basically, the strategy
takes the general form

00:17:51.060 --> 00:17:52.090
of a threshold.

00:17:52.090 --> 00:17:54.267
You should set a
particular value

00:17:54.267 --> 00:17:56.600
based on your expectations
of the distribution of offers

00:17:56.600 --> 00:17:57.850
that you're going to receive.

00:17:57.850 --> 00:17:59.650
And then the first offer that
exceeds that value should

00:17:59.650 --> 00:18:00.774
be the one that you accept.

00:18:00.774 --> 00:18:02.840
This is more closely
analogous to the situation

00:18:02.840 --> 00:18:04.150
that you were talking about.

00:18:04.150 --> 00:18:09.160
So, for example, if you have,
say, a uniform distribution

00:18:09.160 --> 00:18:12.570
between getting an
offer of $400,000

00:18:12.570 --> 00:18:16.012
and an offer of $500,000,
then as a function of the cost

00:18:16.012 --> 00:18:17.470
per offer, we can
actually work out

00:18:17.470 --> 00:18:18.762
what this threshold looks like.

00:18:18.762 --> 00:18:20.845
And so it gives you a very
simple kind of approach

00:18:20.845 --> 00:18:23.550
that you can take in the context
of trying to decide whether you

00:18:23.550 --> 00:18:24.591
should evaluate an offer.

00:18:24.591 --> 00:18:28.660
And it also says that you
should never look back, right?

00:18:28.660 --> 00:18:30.780
So if you turned down
an offer in the past,

00:18:30.780 --> 00:18:33.052
that was because the offer
was below your threshold.

00:18:33.052 --> 00:18:34.510
And as a consequence,
you shouldn't

00:18:34.510 --> 00:18:36.100
regret having given up
on that opportunity.

00:18:36.100 --> 00:18:37.475
You should just
be waiting around

00:18:37.475 --> 00:18:40.222
for the next offer that
exceeds your threshold.

00:18:40.222 --> 00:18:42.180
Another example of an
optimal stopping problem,

00:18:42.180 --> 00:18:45.090
which I think many of
us have encountered,

00:18:45.090 --> 00:18:47.880
is the problem of figuring out
how to find a parking spot.

00:18:47.880 --> 00:18:50.997
So in this case, the
kind of ideal scenario

00:18:50.997 --> 00:18:53.080
that the mathematicians
imagine-- although, again,

00:18:53.080 --> 00:18:55.430
there are lots of
variants-- is one

00:18:55.430 --> 00:18:57.920
where you're driving
towards a destination,

00:18:57.920 --> 00:19:01.940
and you kind of have a series
of possibly available parking

00:19:01.940 --> 00:19:03.710
spots that are along
the side of the road.

00:19:03.710 --> 00:19:05.793
And your goal is to minimize
the distance that you

00:19:05.793 --> 00:19:06.899
have to end up walking.

00:19:06.899 --> 00:19:08.940
So it turns out that the
solution to this problem

00:19:08.940 --> 00:19:11.000
depends on what's
called the occupancy

00:19:11.000 --> 00:19:13.050
rate, the proportion
of those parking spots

00:19:13.050 --> 00:19:14.030
which are occupied.

00:19:14.030 --> 00:19:15.950
And so for each occupancy
rate, what we can do

00:19:15.950 --> 00:19:17.950
is identify at what
point you should

00:19:17.950 --> 00:19:21.440
switch from driving to
being willing to take

00:19:21.440 --> 00:19:23.690
the next available parking spot.

00:19:23.690 --> 00:19:26.740
And so we calculate for you
a nice convenient table.

00:19:26.740 --> 00:19:31.049
You can cut this out,
stick it on your dashboard.

00:19:31.049 --> 00:19:33.340
But basically, you can get
some interesting conclusions

00:19:33.340 --> 00:19:36.150
from this, such as if
10% of the parking spots

00:19:36.150 --> 00:19:38.720
are free-- so a 90%
occupancy rate-- then

00:19:38.720 --> 00:19:42.690
you should start looking
for a parking spot

00:19:42.690 --> 00:19:45.100
seven spaces away
from your destination.

00:19:45.100 --> 00:19:48.160
Whereas if 1% of those
parking spots are free,

00:19:48.160 --> 00:19:49.507
then it's more like 70.

00:19:49.507 --> 00:19:51.090
And maybe there's a
scenario down here

00:19:51.090 --> 00:19:54.790
which fits to some
parts of San Francisco.

00:19:54.790 --> 00:19:59.450
So another famous problem which
has been analyzed in this way

00:19:59.450 --> 00:20:02.410
is one which, unfortunately,
ruins the plot

00:20:02.410 --> 00:20:04.040
of a lot of heist movies.

00:20:04.040 --> 00:20:06.690
So this is the
problem of a burglar

00:20:06.690 --> 00:20:09.590
who has to make a
decision about when

00:20:09.590 --> 00:20:11.340
to give up a life of crime.

00:20:11.340 --> 00:20:13.740
So there's a scene that happens
in a lot of heist movies

00:20:13.740 --> 00:20:16.530
where the team has to sort
of coax the old thief out

00:20:16.530 --> 00:20:17.470
of retirement.

00:20:17.470 --> 00:20:19.152
The thief is kind
of going, well,

00:20:19.152 --> 00:20:21.210
you know, I kind of
like living in my castle

00:20:21.210 --> 00:20:24.800
and trying to figure out exactly
what they're going to do.

00:20:24.800 --> 00:20:28.150
But to sort of spoil all
of those plots, in fact,

00:20:28.150 --> 00:20:30.800
there's an exact
solution, and the thief

00:20:30.800 --> 00:20:34.520
need only crunch the numbers to
work out what should be done.

00:20:34.520 --> 00:20:39.400
And if you assume that you have
a thief who for each robbery

00:20:39.400 --> 00:20:42.290
is going to get, on average,
about the same amount of money

00:20:42.290 --> 00:20:45.230
and has some probability
that they succeed

00:20:45.230 --> 00:20:48.610
in executing those robberies,
and if they get caught,

00:20:48.610 --> 00:20:49.880
they lose everything.

00:20:49.880 --> 00:20:53.860
So the goal is to maximize
the expected amount of money

00:20:53.860 --> 00:20:56.830
that you end up with,
then the optimal stopping

00:20:56.830 --> 00:20:59.840
rule is to stop after
a number of robberies

00:20:59.840 --> 00:21:03.000
equal to the probability
of success divided

00:21:03.000 --> 00:21:04.840
by the probability of failure.

00:21:04.840 --> 00:21:07.970
So if you are a ham-fisted
thief who succeeds about half

00:21:07.970 --> 00:21:09.640
the time and fails
about half the time,

00:21:09.640 --> 00:21:11.424
you could pull off one robbery.

00:21:11.424 --> 00:21:13.840
And then you should just quit
if you got away with it away

00:21:13.840 --> 00:21:15.920
with it and you've been lucky.

00:21:15.920 --> 00:21:20.140
But if you're a skilled thief
who succeeds 90% of the time

00:21:20.140 --> 00:21:23.262
and fails 10% of the time, you
can pull off nine robberies,

00:21:23.262 --> 00:21:25.720
and then that's the point at
which you should call it quits

00:21:25.720 --> 00:21:27.470
and go and live in
the castle and not

00:21:27.470 --> 00:21:30.940
listen to any of these
guys who come calling.

00:21:30.940 --> 00:21:33.910
So these sorts of scenarios
might seem a little bit

00:21:33.910 --> 00:21:34.746
artificial, right?

00:21:34.746 --> 00:21:36.620
They're all cases where
you're kind of forced

00:21:36.620 --> 00:21:39.161
into a situation where you have
to make a decision about when

00:21:39.161 --> 00:21:40.284
to stop doing something.

00:21:40.284 --> 00:21:41.950
But I think there's
a deeper point here,

00:21:41.950 --> 00:21:45.337
which is that while we normally
think about decision-making

00:21:45.337 --> 00:21:46.920
as being something
where we've got all

00:21:46.920 --> 00:21:48.795
the information in front
of us, and it's just

00:21:48.795 --> 00:21:51.680
a matter of choosing what
we're going to pursue,

00:21:51.680 --> 00:21:53.392
in fact, in most
human decisions,

00:21:53.392 --> 00:21:55.350
we're in a scenario which
is much more like one

00:21:55.350 --> 00:21:56.808
of these optimal
stopping problems,

00:21:56.808 --> 00:21:58.680
where we have to be
gathering information.

00:21:58.680 --> 00:22:00.555
And one of the decisions
that we have to make

00:22:00.555 --> 00:22:02.570
is whether we've got
enough information to act.

00:22:02.570 --> 00:22:05.960
And so I think there's a
deeper point here about the way

00:22:05.960 --> 00:22:08.410
that we think about the
structure of human decisions,

00:22:08.410 --> 00:22:11.670
that often we're engaging in
exactly this kind of process

00:22:11.670 --> 00:22:13.560
even though we might
not realize it.

00:22:21.026 --> 00:22:22.400
BRIAN CHRISTIAN:
The second class

00:22:22.400 --> 00:22:25.740
of problems that we
consider in the book are

00:22:25.740 --> 00:22:30.940
ones that we describe as
explore/exploit trade-offs.

00:22:30.940 --> 00:22:33.000
And this encompasses
a wide range

00:22:33.000 --> 00:22:37.630
of problems where we get to
make a similar kind of decision

00:22:37.630 --> 00:22:41.210
over and over and over
in an iterated fashion.

00:22:41.210 --> 00:22:44.980
And typically,
that takes the form

00:22:44.980 --> 00:22:48.360
of a tension between doing our
favorite things-- the things we

00:22:48.360 --> 00:22:51.510
know and love-- or
trying new things.

00:22:51.510 --> 00:22:54.110
And this type of a
structure appears

00:22:54.110 --> 00:22:58.100
in a number of different
domains and in everyday life,

00:22:58.100 --> 00:23:00.090
restaurants being
the obvious case.

00:23:00.090 --> 00:23:02.080
Do you go to your
favorite place?

00:23:02.080 --> 00:23:05.140
Or do you try the new
place that just opened up?

00:23:05.140 --> 00:23:08.121
It happens in music,
where do you do you

00:23:08.121 --> 00:23:09.370
listen to your favorite album?

00:23:09.370 --> 00:23:11.510
Or do you put on the radio?

00:23:11.510 --> 00:23:13.470
It also describes
our social lives.

00:23:13.470 --> 00:23:17.160
How much time do you spend with
your close circle of friends,

00:23:17.160 --> 00:23:20.210
your spouse, your family,
your best friends,

00:23:20.210 --> 00:23:22.560
versus going out to
lunch with that colleague

00:23:22.560 --> 00:23:25.060
that you just met and you think
you have something in common

00:23:25.060 --> 00:23:29.180
and want to kind of foster
a new acquaintanceship?

00:23:29.180 --> 00:23:32.200
This same trade-off
also occurs in a number

00:23:32.200 --> 00:23:39.900
of different places in more
societal ways-- in business

00:23:39.900 --> 00:23:41.030
and also in medicine.

00:23:41.030 --> 00:23:44.020
So in business, this
comes up in the context

00:23:44.020 --> 00:23:45.720
of ads, which I
hear is something

00:23:45.720 --> 00:23:49.330
that you guys know a little
bit about here at Google.

00:23:49.330 --> 00:23:52.290
So when you're deciding what
ad to run next to a keyword,

00:23:52.290 --> 00:23:55.010
for example, you've got
this tension between there

00:23:55.010 --> 00:23:57.520
is some ad that historically
has the best track

00:23:57.520 --> 00:23:59.050
record of getting the clicks.

00:23:59.050 --> 00:24:01.880
But there's also this
ever-changing dynamic pool

00:24:01.880 --> 00:24:04.490
of new ads that are
entering the system that

00:24:04.490 --> 00:24:06.090
might be worth trying.

00:24:06.090 --> 00:24:06.980
They might be better.

00:24:06.980 --> 00:24:10.550
They probably aren't,
but they could be.

00:24:10.550 --> 00:24:13.780
In medicine, this
structure describes the way

00:24:13.780 --> 00:24:18.430
that clinical trials work,
where for any condition,

00:24:18.430 --> 00:24:23.170
there is some known best
treatment with some known

00:24:23.170 --> 00:24:25.670
chance of success and
some known drawbacks.

00:24:25.670 --> 00:24:27.140
And then there's
a series-- there's

00:24:27.140 --> 00:24:29.960
kind of a pipeline of these
experimental treatments that,

00:24:29.960 --> 00:24:32.950
again, might be
better, might be worse.

00:24:32.950 --> 00:24:37.730
And so we have to trade off
between exploring-- that is,

00:24:37.730 --> 00:24:41.010
spending our energy
gathering new information--

00:24:41.010 --> 00:24:44.590
and exploiting, which is
leveraging the information

00:24:44.590 --> 00:24:47.385
that we've gathered so far
to get a known good outcome.

00:24:50.890 --> 00:24:54.250
The canonical
explore/exploit problem

00:24:54.250 --> 00:24:56.790
that appears in the
computer science literature

00:24:56.790 --> 00:25:01.070
is known as the
multi-armed bandit problem.

00:25:01.070 --> 00:25:03.687
And this colorful
name-- I'm sure

00:25:03.687 --> 00:25:05.520
this is familiar to
some of you in the room.

00:25:05.520 --> 00:25:11.540
But the name comes from the
moniker of the one-armed bandit

00:25:11.540 --> 00:25:12.890
for a slot machine.

00:25:12.890 --> 00:25:16.370
So a multi-armed bandit you
can think of as just a roomful

00:25:16.370 --> 00:25:18.030
of different slot machines.

00:25:18.030 --> 00:25:19.840
So the basic setup
of the formal problem

00:25:19.840 --> 00:25:21.920
is this-- you walk
into a casino.

00:25:21.920 --> 00:25:25.790
There's all sorts of different
slot machines, each of which

00:25:25.790 --> 00:25:28.450
pays off with some probability.

00:25:28.450 --> 00:25:30.690
But every machine is different.

00:25:30.690 --> 00:25:33.540
Some of them are more
lucrative than others.

00:25:33.540 --> 00:25:36.310
But there's no way for you
to know that until you just

00:25:36.310 --> 00:25:38.860
start pulling the
levers and seeing

00:25:38.860 --> 00:25:41.260
which ones seem more promising.

00:25:41.260 --> 00:25:42.780
So again, here's
this case where you

00:25:42.780 --> 00:25:44.940
have to trade off
between gathering

00:25:44.940 --> 00:25:48.490
information and leveraging
the information that you have.

00:25:48.490 --> 00:25:51.030
And so there's this
question, which

00:25:51.030 --> 00:25:54.640
vexed an entire generation
of mathematicians, of, OK.

00:25:54.640 --> 00:25:58.210
Let's say you walk into the
casino, and you have 100 pulls.

00:25:58.210 --> 00:25:59.650
You're there for an afternoon.

00:25:59.650 --> 00:26:01.740
You have enough
time for 100 pulls.

00:26:01.740 --> 00:26:06.730
What strategy is going to give
you the highest expected payout

00:26:06.730 --> 00:26:09.330
before you leave the casino?

00:26:09.330 --> 00:26:11.350
In fact, for much
of the 20th century,

00:26:11.350 --> 00:26:14.080
this was considered unsolvable.

00:26:14.080 --> 00:26:18.730
And during World War II, Allied
mathematicians in Britain

00:26:18.730 --> 00:26:21.870
joked about dropping the
multi-armed bandit problem

00:26:21.870 --> 00:26:24.660
over Germany as the
ultimate instrument

00:26:24.660 --> 00:26:27.540
of intellectual
sabotage to just waste

00:26:27.540 --> 00:26:31.780
the brainpower of the
German scientists.

00:26:31.780 --> 00:26:33.820
And I think one of the
simplest explanations

00:26:33.820 --> 00:26:36.960
of the way in which this problem
can be very tricky to think

00:26:36.960 --> 00:26:39.140
about is the following choice.

00:26:39.140 --> 00:26:42.940
So let's say you've played
one machine 15 times.

00:26:42.940 --> 00:26:44.700
And nine times, it paid out.

00:26:44.700 --> 00:26:47.490
Six times it did not.

00:26:47.490 --> 00:26:49.760
Another machine
you've played twice.

00:26:49.760 --> 00:26:50.530
Once it paid out.

00:26:50.530 --> 00:26:52.990
Once it didn't.

00:26:52.990 --> 00:26:56.520
Now, if we just want to very
straightforwardly compute

00:26:56.520 --> 00:26:59.380
the expected value of
each of these machines,

00:26:59.380 --> 00:27:03.090
the nine-and-six machine's
got a payout rate of 60%.

00:27:03.090 --> 00:27:07.490
The one-and-one machine
has a payout rate of 50%.

00:27:07.490 --> 00:27:11.014
And so there are these two kind
of competing intuitions here.

00:27:11.014 --> 00:27:12.430
One is, well, you
should obviously

00:27:12.430 --> 00:27:15.144
just do the thing with
the better expected value.

00:27:15.144 --> 00:27:17.310
The other is, well, there's
a sense in which we just

00:27:17.310 --> 00:27:20.360
don't know enough about
the second machine

00:27:20.360 --> 00:27:22.930
to walk away from it forever.

00:27:22.930 --> 00:27:26.930
Certainly, it must be worth one
more pull or two more pulls.

00:27:26.930 --> 00:27:30.010
How do you decide what
that threshold is?

00:27:30.010 --> 00:27:33.160
And it turns out this is,
in a way, a trick question

00:27:33.160 --> 00:27:37.900
because it all depends on
something that we haven't given

00:27:37.900 --> 00:27:39.820
you in this description
of the problem yet,

00:27:39.820 --> 00:27:44.240
which is how long you
plan to be in the casino.

00:27:44.240 --> 00:27:48.370
So this is a concept
that sometimes

00:27:48.370 --> 00:27:52.000
gets referred to as the horizon,
we refer to as the interval.

00:27:52.000 --> 00:27:54.860
And this concept has,
just speaking personally,

00:27:54.860 --> 00:27:58.880
given me a bit of
an axe to grind

00:27:58.880 --> 00:28:01.620
with one of my favorite films
from my own childhood, which

00:28:01.620 --> 00:28:05.880
is the inspirational 1980s Robin
Williams movie, "Dead Poets

00:28:05.880 --> 00:28:07.380
Society."

00:28:07.380 --> 00:28:09.530
It's one of these
really feel-good movies,

00:28:09.530 --> 00:28:13.400
and he plays this
inspiring poetry teacher

00:28:13.400 --> 00:28:16.700
who says to his students
in this rousing monologue,

00:28:16.700 --> 00:28:18.510
"Seize the day, boys.

00:28:18.510 --> 00:28:21.000
Make your lives extraordinary."

00:28:21.000 --> 00:28:23.820
And going back through the
lens of the multi-armed bandit

00:28:23.820 --> 00:28:27.240
problem, I can't help feeling
that Robin Williams is actually

00:28:27.240 --> 00:28:31.117
giving two conflicting
pieces of advice here.

00:28:31.117 --> 00:28:32.700
If we're just trying
to seize the day,

00:28:32.700 --> 00:28:34.920
we probably want to pull
that nine-six because it's

00:28:34.920 --> 00:28:36.580
got the higher expected value.

00:28:36.580 --> 00:28:39.410
But if we want to make
our life extraordinary,

00:28:39.410 --> 00:28:41.120
then we should
certainly see if there

00:28:41.120 --> 00:28:44.330
isn't some value in trying
these new things because we

00:28:44.330 --> 00:28:47.070
can always go back.

00:28:47.070 --> 00:28:49.670
In standard American English,
we have all of these idioms

00:28:49.670 --> 00:28:54.530
like "eat, drink, and be
merry, for tomorrow we die."

00:28:54.530 --> 00:28:58.810
But it feels that we're missing
the idioms on the explore side

00:28:58.810 --> 00:29:01.790
of the equation, which are
things like, life is long,

00:29:01.790 --> 00:29:05.110
so learn a new language and
reach out to that new colleague

00:29:05.110 --> 00:29:08.416
because who knows what could
blossom over many years time.

00:29:08.416 --> 00:29:09.790
We're still honing
the messaging,

00:29:09.790 --> 00:29:12.890
but it does feel like there's
a gap in the culture that

00:29:12.890 --> 00:29:14.640
can be filled here.

00:29:14.640 --> 00:29:20.080
So when you're working
with a finite interval,

00:29:20.080 --> 00:29:22.620
the solution to the
multi-armed bandit problem

00:29:22.620 --> 00:29:27.120
comes from a method
described by Richard Bellman,

00:29:27.120 --> 00:29:30.110
dynamic programming,
where you basically work

00:29:30.110 --> 00:29:32.830
backwards and are
able to compute

00:29:32.830 --> 00:29:36.910
the expected value of every
pull given all of the possible

00:29:36.910 --> 00:29:39.870
pulls that you could make as a
result of whether that succeeds

00:29:39.870 --> 00:29:40.730
or fails.

00:29:40.730 --> 00:29:44.320
And you can actually work
out the expected value

00:29:44.320 --> 00:29:47.350
all the way back to walking in
the door of the casino, what

00:29:47.350 --> 00:29:49.020
should you do?

00:29:49.020 --> 00:29:51.460
And this provides
an exact solution

00:29:51.460 --> 00:29:53.300
to the multi-armed
bandit problem,

00:29:53.300 --> 00:29:59.380
but there's a catch, which is
that it requires that you know

00:29:59.380 --> 00:30:01.340
exactly how long you're
going to be there

00:30:01.340 --> 00:30:03.420
and exactly how many
machines there are.

00:30:03.420 --> 00:30:06.760
And it also requires doing a
lot of computation up front.

00:30:06.760 --> 00:30:08.820
But I think the
critical thing is

00:30:08.820 --> 00:30:12.050
that we're able to look
at these solutions,

00:30:12.050 --> 00:30:16.670
these exact solutions, and get
some broader principles out

00:30:16.670 --> 00:30:17.400
of it.

00:30:17.400 --> 00:30:20.850
So, for example, the
value of exploration

00:30:20.850 --> 00:30:23.840
is greatest the minute you
walk through the door for two

00:30:23.840 --> 00:30:24.720
reasons.

00:30:24.720 --> 00:30:27.500
The first is that if you think
about it in the restaurant

00:30:27.500 --> 00:30:30.270
analogy, you just
moved to Mountain View.

00:30:30.270 --> 00:30:32.230
You go out to eat that night.

00:30:32.230 --> 00:30:34.430
The first place you
try is guaranteed

00:30:34.430 --> 00:30:36.140
to be the best
restaurant you've ever

00:30:36.140 --> 00:30:38.671
experienced in Mountain View.

00:30:38.671 --> 00:30:40.420
The next night, you
try a different place.

00:30:40.420 --> 00:30:42.760
It has a 50% chance
of being the best

00:30:42.760 --> 00:30:44.650
restaurant you've ever
seen in Mountain View,

00:30:44.650 --> 00:30:46.660
and so on and so forth.

00:30:46.660 --> 00:30:50.142
So the likelihood
that something new

00:30:50.142 --> 00:30:51.850
is better than what
we already know about

00:30:51.850 --> 00:30:54.910
goes down as we gain experience.

00:30:54.910 --> 00:30:57.990
The second reason
that exploring is

00:30:57.990 --> 00:31:00.090
more valuable at the
beginning of an interval

00:31:00.090 --> 00:31:02.280
is that when we
make that discovery,

00:31:02.280 --> 00:31:04.520
we have more chances to go back.

00:31:04.520 --> 00:31:06.370
So discovering a really
amazing restaurant

00:31:06.370 --> 00:31:09.340
on your last night in town is
actually a little bit tragic.

00:31:09.340 --> 00:31:12.420
It would have been great
to find that sooner.

00:31:12.420 --> 00:31:15.150
And so this gives us
this general intuition

00:31:15.150 --> 00:31:18.330
that as we perceive ourselves
to be on some interval of time,

00:31:18.330 --> 00:31:21.790
we should kind of
front-load our exploration

00:31:21.790 --> 00:31:25.960
and weight our
exploitation to the end,

00:31:25.960 --> 00:31:29.420
when we both have the most
experience with what to exploit

00:31:29.420 --> 00:31:34.230
and the least time remaining
to discover and enjoy something

00:31:34.230 --> 00:31:36.500
new, even if we did find it.

00:31:36.500 --> 00:31:39.230
This is significant, I
think, because it gives us

00:31:39.230 --> 00:31:43.620
a new way of thinking about
the arc of a human lifespan.

00:31:43.620 --> 00:31:48.820
And so ideas from the
explore/exploit trade-off

00:31:48.820 --> 00:31:53.410
are now influencing
psychologists and changing

00:31:53.410 --> 00:31:58.110
the way we think about
both infancy and old age.

00:31:58.110 --> 00:32:01.110
So to demonstrate
infancy, we have a picture

00:32:01.110 --> 00:32:04.300
of a baby eating a power cord.

00:32:04.300 --> 00:32:06.200
And I think this
demonstrates what

00:32:06.200 --> 00:32:08.900
a lot of us kind of
culturally intuitively

00:32:08.900 --> 00:32:11.340
think of as the
irrationality of babies.

00:32:11.340 --> 00:32:16.010
They're totally-- have the
attention span of a goldfish.

00:32:16.010 --> 00:32:18.190
They put everything
in their mouth.

00:32:18.190 --> 00:32:19.820
They're distracted
really easily.

00:32:19.820 --> 00:32:24.450
They're really bad at just
generally doing things.

00:32:24.450 --> 00:32:27.830
We give the example in a
book-- like a baby gazelle

00:32:27.830 --> 00:32:29.750
is expected to be
able to run away

00:32:29.750 --> 00:32:32.810
from a wolf within the
first day of being alive.

00:32:32.810 --> 00:32:34.830
But, you know, it
takes us 18 years

00:32:34.830 --> 00:32:36.663
before we're allowed
to get behind the wheel

00:32:36.663 --> 00:32:38.680
of a car, that kind of thing.

00:32:38.680 --> 00:32:42.919
And the psychologist
Alison Gopnik uses ideas

00:32:42.919 --> 00:32:44.460
from the explore or
exploit trade-off

00:32:44.460 --> 00:32:47.950
as a way of saying, well,
maybe this extended period

00:32:47.950 --> 00:32:52.510
of dependency is actually
optimal in some sense

00:32:52.510 --> 00:32:56.760
because if you're in the
casino, for the first 18 years

00:32:56.760 --> 00:32:59.079
that you're in the casino,
someone else is buying

00:32:59.079 --> 00:33:00.370
your food and paying your rent.

00:33:00.370 --> 00:33:03.990
And so you don't need to be
getting those early payouts

00:33:03.990 --> 00:33:05.650
to buy your lunch.

00:33:05.650 --> 00:33:10.710
And so you can really use that
period of time to explore,

00:33:10.710 --> 00:33:12.580
which is exactly what
you should be doing

00:33:12.580 --> 00:33:14.140
at the beginning of your life.

00:33:14.140 --> 00:33:16.359
There's a sense in
which just putting

00:33:16.359 --> 00:33:17.900
every item in the
house in your mouth

00:33:17.900 --> 00:33:20.980
at least just once
sort of resembles

00:33:20.980 --> 00:33:25.580
walking into the casino and
just pulling all of the levers.

00:33:25.580 --> 00:33:27.930
Similarly, the idea
of exploitation

00:33:27.930 --> 00:33:30.530
is changing the way we
think about getting older.

00:33:30.530 --> 00:33:35.610
So here we have a gentleman
who I like think of, imagine it

00:33:35.610 --> 00:33:38.790
as enjoying the same
lunchtime restaurant

00:33:38.790 --> 00:33:40.959
that he's been to
hundreds of times.

00:33:40.959 --> 00:33:42.750
And he knows exactly
what he's going to get

00:33:42.750 --> 00:33:46.022
and exactly what he
likes, and it's great.

00:33:46.022 --> 00:33:47.480
There's a lot of
psychological data

00:33:47.480 --> 00:33:51.030
that says that as
we go through life,

00:33:51.030 --> 00:33:55.960
older folks have a smaller
circle of social connections.

00:33:55.960 --> 00:33:58.800
They spend their time
with fewer people.

00:33:58.800 --> 00:34:02.620
And there's one interpretation
of this that just says,

00:34:02.620 --> 00:34:05.420
well, they're lonely, or they're
detached or disinterested,

00:34:05.420 --> 00:34:11.270
or it's just kind
of sad to get older.

00:34:11.270 --> 00:34:15.320
But thinking about it from the
perspective of exploitation

00:34:15.320 --> 00:34:17.250
gives a totally different story.

00:34:17.250 --> 00:34:20.420
And this comes up in the work of
the Stanford psychologist Laura

00:34:20.420 --> 00:34:26.310
Carstensen, who studies aging in
an attempt to sort of overturn

00:34:26.310 --> 00:34:28.380
some of the prejudices we have.

00:34:28.380 --> 00:34:32.010
And so, for example,
one of the intuitions

00:34:32.010 --> 00:34:34.889
you get from the idea of the
explore or exploit trade-off

00:34:34.889 --> 00:34:36.600
is that towards the
end of your life,

00:34:36.600 --> 00:34:38.510
you really should
be spending more

00:34:38.510 --> 00:34:41.510
of your time doing the things
you already know and love

00:34:41.510 --> 00:34:45.960
both because it's unlikely
to make a discovery that's

00:34:45.960 --> 00:34:48.234
better than the things you
already really care about

00:34:48.234 --> 00:34:49.650
and also because
there's less time

00:34:49.650 --> 00:34:51.440
to enjoy it should you do that.

00:34:51.440 --> 00:34:55.050
And so it just makes more sense
to spend more of your energy

00:34:55.050 --> 00:34:58.810
on the things that you
already know and love.

00:34:58.810 --> 00:35:01.740
And so as a result,
what I think is actually

00:35:01.740 --> 00:35:06.570
a very encouraging story here
is that as you spend more time

00:35:06.570 --> 00:35:09.660
in the casino, your average
payouts per unit of time

00:35:09.660 --> 00:35:10.754
should go up.

00:35:10.754 --> 00:35:12.670
So there's a sense in
which we should actually

00:35:12.670 --> 00:35:15.610
expect to get steadily
happier as we go through life.

00:35:15.610 --> 00:35:18.430
We are less disappointed
and less stressed out.

00:35:18.430 --> 00:35:23.250
And her research supports this,
which I think is really lovely.

00:35:23.250 --> 00:35:26.240
Now, there are many cases
where we don't necessarily

00:35:26.240 --> 00:35:29.110
know where we are on
the interval of time,

00:35:29.110 --> 00:35:30.945
or it doesn't
necessarily make sense

00:35:30.945 --> 00:35:32.820
to think of there being
some finite interval.

00:35:32.820 --> 00:35:34.444
So maybe you move
to Mountain View,

00:35:34.444 --> 00:35:35.860
and you don't know
how long you're

00:35:35.860 --> 00:35:37.590
going to live in Mountain View.

00:35:37.590 --> 00:35:40.230
Or if you're a
company, you imagine

00:35:40.230 --> 00:35:43.877
yourself being interested in
being around indefinitely.

00:35:43.877 --> 00:35:45.460
But nonetheless,
there's still a sense

00:35:45.460 --> 00:35:49.780
in which you care about the
present more than the future.

00:35:49.780 --> 00:35:54.410
This framing of the problem
led to a different series

00:35:54.410 --> 00:35:57.750
of breakthroughs, starting
with an Oxford mathematician

00:35:57.750 --> 00:35:59.350
named John Gittins,
who was hired

00:35:59.350 --> 00:36:02.400
by the pharmaceutical
company Unilever

00:36:02.400 --> 00:36:04.400
to tell them, basically,
how much of their money

00:36:04.400 --> 00:36:09.320
to invest in R&amp;D. And so
he frames the-- he almost

00:36:09.320 --> 00:36:12.440
accidentally made this enormous
breakthrough in the problem

00:36:12.440 --> 00:36:14.520
by thinking of it
not as there being

00:36:14.520 --> 00:36:17.230
some finite interval of
time but as there being

00:36:17.230 --> 00:36:22.670
some indefinite future that
is geometrically discounted.

00:36:22.670 --> 00:36:24.390
So I don't know
how long I'm going

00:36:24.390 --> 00:36:26.270
to be living in the Bay Area.

00:36:26.270 --> 00:36:32.160
But a really good meal
next week is maybe only 90%

00:36:32.160 --> 00:36:35.390
as valuable as a really
good meal this week.

00:36:35.390 --> 00:36:39.110
Or making X dollars
next quarter is only 90%

00:36:39.110 --> 00:36:42.640
as valuable as making
those dollars this quarter.

00:36:42.640 --> 00:36:44.240
And it turns out
that, again, you

00:36:44.240 --> 00:36:47.930
can get a very precise
solution to this problem.

00:36:47.930 --> 00:36:52.640
So he explored it in
this business context.

00:36:52.640 --> 00:36:54.640
But it's also, I think,
very interesting that it

00:36:54.640 --> 00:36:57.210
was a pharmaceutical context,
because this also gives us

00:36:57.210 --> 00:37:00.110
a way of thinking
differently about how

00:37:00.110 --> 00:37:03.080
a clinical trial should be run.

00:37:03.080 --> 00:37:07.180
The basic idea behind
Gittins's breakthrough here,

00:37:07.180 --> 00:37:10.610
which is called
the Gittins index,

00:37:10.610 --> 00:37:15.920
is he imagines that-- the
word we use is a bribe.

00:37:15.920 --> 00:37:22.150
So if you think about-- there's
a game show that is on TV

00:37:22.150 --> 00:37:23.982
called "Deal or No
Deal," where you

00:37:23.982 --> 00:37:25.940
have a briefcase that
has somewhere between one

00:37:25.940 --> 00:37:27.870
and a million dollars in it.

00:37:27.870 --> 00:37:30.230
And someone calls you
on the phone and says,

00:37:30.230 --> 00:37:34.080
I will pay you $10,000 not
to open that briefcase.

00:37:34.080 --> 00:37:35.750
What do you do?

00:37:35.750 --> 00:37:38.250
This is the basic intuition
behind the Gittins index.

00:37:38.250 --> 00:37:40.420
So Gittins says,
for every machine

00:37:40.420 --> 00:37:43.970
that we've tried and have
incomplete knowledge of--

00:37:43.970 --> 00:37:46.390
or maybe we have no
knowledge of whatsoever--

00:37:46.390 --> 00:37:49.330
there's some machine with
a guaranteed payout that's

00:37:49.330 --> 00:37:53.860
so good, we'll never try
that machine ever again.

00:37:53.860 --> 00:37:57.480
Maybe the nine-six versus the
one-one is more of a toss-up.

00:37:57.480 --> 00:37:59.840
But if it was 9-0
and the one-one,

00:37:59.840 --> 00:38:02.020
then there's a sense
in which maybe it's

00:38:02.020 --> 00:38:06.270
just never worth pulling the
one-one machine ever again.

00:38:06.270 --> 00:38:08.430
And so Gittins comes
up with what he thinks

00:38:08.430 --> 00:38:11.760
is a nice approximation,
which is just always

00:38:11.760 --> 00:38:14.760
play the machine with
the highest bribe price.

00:38:14.760 --> 00:38:17.640
And to his own astonishment,
as well as the field's, this

00:38:17.640 --> 00:38:21.250
turns out to be, in fact, not
merely a good approximation

00:38:21.250 --> 00:38:23.030
but the solution.

00:38:23.030 --> 00:38:26.180
So this is another case where,
like with the parking problem,

00:38:26.180 --> 00:38:27.880
we present this
table in the book,

00:38:27.880 --> 00:38:31.320
and you can cut it out
and take it home with you.

00:38:31.320 --> 00:38:35.160
And it provides
values for situations

00:38:35.160 --> 00:38:37.691
where you're trying to
weigh the value between two

00:38:37.691 --> 00:38:38.440
different options.

00:38:38.440 --> 00:38:42.090
And so going back to our
two slot machines, the nine

00:38:42.090 --> 00:38:46.180
and six machine has a
Gittins index of 6,300.

00:38:46.180 --> 00:38:50.890
But the one-and-one machine
has a Gittins index of 6,346.

00:38:50.890 --> 00:38:52.300
So case closed.

00:38:52.300 --> 00:38:56.280
Pull the one-one
machine one more time.

00:38:56.280 --> 00:38:58.470
What I think is kind of
philosophically significant

00:38:58.470 --> 00:39:02.080
about this is the
zero-zero square has

00:39:02.080 --> 00:39:08.220
a value of 70.29%, which
means that you should consider

00:39:08.220 --> 00:39:12.740
something you've never tried as
being just as good as something

00:39:12.740 --> 00:39:16.050
that you know works
70% of the time,

00:39:16.050 --> 00:39:19.100
even though it only has
an expected value of 50%.

00:39:19.100 --> 00:39:23.700
And you can see if you follow
the diagonal down to the right,

00:39:23.700 --> 00:39:28.330
it goes from 70% to
63.46% to 60.10%, 58.09%.

00:39:28.330 --> 00:39:34.080
And it does indeed converge
on 0.5 as you gain experience.

00:39:34.080 --> 00:39:37.390
But there's this boost applied
for not having that experience.

00:39:41.120 --> 00:39:44.200
So we tongue-in-cheek suggest
that you just print this out

00:39:44.200 --> 00:39:46.970
and just use it to
decide where to eat.

00:39:46.970 --> 00:39:48.912
But there are
sometimes some problems

00:39:48.912 --> 00:39:49.870
that come up with this.

00:39:49.870 --> 00:39:52.390
One is that we don't always
geometrically discount

00:39:52.390 --> 00:39:53.400
our payoffs.

00:39:53.400 --> 00:39:54.900
The other is that
actually computing

00:39:54.900 --> 00:39:58.150
these values on the fly is kind
of computationally intensive.

00:39:58.150 --> 00:40:02.920
And so there's a third way of
thinking about the problem that

00:40:02.920 --> 00:40:05.030
has come in the wake
of Gittins's work,

00:40:05.030 --> 00:40:08.550
and that is the idea
of minimizing regret.

00:40:08.550 --> 00:40:11.300
So we give a lot of
examples that touch

00:40:11.300 --> 00:40:14.200
on Google's work, but the
best illustration of this

00:40:14.200 --> 00:40:15.730
actually comes from Amazon.

00:40:15.730 --> 00:40:19.150
So Jeff Bezos talks about being
in this really lucrative hedge

00:40:19.150 --> 00:40:21.170
fund position and deciding
whether to give up

00:40:21.170 --> 00:40:24.130
his cushy job to start
an online bookstore.

00:40:24.130 --> 00:40:26.950
And he approaches it from what
he calls a regret minimization

00:40:26.950 --> 00:40:27.570
framework.

00:40:27.570 --> 00:40:30.830
"I knew looking back
I wouldn't regret it."

00:40:30.830 --> 00:40:32.950
In the context of the
multi-armed bandit problem,

00:40:32.950 --> 00:40:35.560
you can formulate
regret as every time

00:40:35.560 --> 00:40:39.710
you tried something that wasn't
the best thing in hindsight.

00:40:39.710 --> 00:40:43.400
And so you can ask yourself,
how does my regret-- what

00:40:43.400 --> 00:40:46.910
does that look like as I proceed
through my time in the casino?

00:40:46.910 --> 00:40:49.940
And we have good
news and bad news.

00:40:49.940 --> 00:40:51.960
We'll start with the bad news.

00:40:51.960 --> 00:40:56.250
The bad news is that you will
never stop getting more regrets

00:40:56.250 --> 00:40:58.110
as you go through life.

00:40:58.110 --> 00:41:01.380
The good news is that the rate
at which you add new regrets

00:41:01.380 --> 00:41:02.600
goes down over time.

00:41:02.600 --> 00:41:06.700
Specifically, if you were
following an optimal algorithm,

00:41:06.700 --> 00:41:09.840
the rate at which
you add new regrets

00:41:09.840 --> 00:41:12.920
is logarithmic with
respect to time.

00:41:12.920 --> 00:41:15.970
And this has led to a series
of breakthroughs in computer

00:41:15.970 --> 00:41:19.640
scientists looking for simpler
solutions than the Gittins

00:41:19.640 --> 00:41:25.260
index that still have this
optimality of minimal regret.

00:41:25.260 --> 00:41:27.940
One of our favorites, which
I think is the most thematic,

00:41:27.940 --> 00:41:29.950
is called upper
confidence bound.

00:41:29.950 --> 00:41:32.630
And this says that for every
slot machine-- you know,

00:41:32.630 --> 00:41:35.040
you've got some error
bars around what

00:41:35.040 --> 00:41:36.340
you think the payoff might be.

00:41:36.340 --> 00:41:38.830
So the expected value would be
in the middle of that range.

00:41:38.830 --> 00:41:42.420
But there's some error bars
on either side of that.

00:41:42.420 --> 00:41:45.530
Upper confidence
bound says, simply,

00:41:45.530 --> 00:41:49.450
always do the thing with the
highest top of the range.

00:41:49.450 --> 00:41:51.460
Don't care about the
actual expected value,

00:41:51.460 --> 00:41:53.740
and don't care about
the worst case scenario.

00:41:53.740 --> 00:41:57.090
Just always do the thing with
the highest top of the range.

00:41:57.090 --> 00:42:00.830
And I think that's sort
of a lovely, lyrical note

00:42:00.830 --> 00:42:04.880
that the math brings us to,
which is that optimism is

00:42:04.880 --> 00:42:06.688
the best prevention for regret.

00:42:11.440 --> 00:42:12.960
TOM GRIFFITHS: So
our third example

00:42:12.960 --> 00:42:15.840
begins in a different place,
which is in the closet.

00:42:15.840 --> 00:42:19.080
So I think all of us have
encountered a problem

00:42:19.080 --> 00:42:20.252
of an overflowing closet.

00:42:20.252 --> 00:42:21.960
Things need to be
organized, but you also

00:42:21.960 --> 00:42:23.070
need to make a
decision about what

00:42:23.070 --> 00:42:24.252
you're going to get rid of.

00:42:24.252 --> 00:42:25.710
And in order to
solve this problem,

00:42:25.710 --> 00:42:27.376
we'd like to be able
to turn to experts.

00:42:27.376 --> 00:42:30.240
Fortunately, there are experts
on exactly this kind of thing.

00:42:30.240 --> 00:42:33.170
So we could consult one of
these-- Martha Stewart, who

00:42:33.170 --> 00:42:37.372
says to ask yourself a
series of questions--

00:42:37.372 --> 00:42:38.330
how long have I had it?

00:42:38.330 --> 00:42:39.380
Does it still function?

00:42:39.380 --> 00:42:41.360
Is it a duplicate of
something I already own?

00:42:41.360 --> 00:42:43.430
And when was the last
time I wore it or used it?

00:42:43.430 --> 00:42:45.090
And then based on the
answers to these questions,

00:42:45.090 --> 00:42:47.089
you can make a decision
about whether you should

00:42:47.089 --> 00:42:49.260
keep that thing or not,
give it away to charity,

00:42:49.260 --> 00:42:54.109
and, as a consequence, end up
with a more organized closet.

00:42:54.109 --> 00:42:56.400
So there are a couple of
interesting observations here.

00:42:56.400 --> 00:42:59.482
The first is that here there
are in fact, multiple questions

00:42:59.482 --> 00:43:00.690
that you should be answering.

00:43:00.690 --> 00:43:02.106
And the answers
to these questions

00:43:02.106 --> 00:43:03.240
could be quite different.

00:43:03.240 --> 00:43:05.409
And the other is
that, in fact, there's

00:43:05.409 --> 00:43:07.700
another group of experts who
have thought about exactly

00:43:07.700 --> 00:43:11.230
these problems and come up
with slightly different advice.

00:43:11.230 --> 00:43:13.940
In particular, they discovered
that one of these questions

00:43:13.940 --> 00:43:17.040
is, in fact, far better
than any of the others.

00:43:17.040 --> 00:43:19.540
So this other group of experts
don't think about closets.

00:43:19.540 --> 00:43:21.630
They think about the
memory of computers.

00:43:21.630 --> 00:43:24.920
This is the picture of
the Atlas computer, which

00:43:24.920 --> 00:43:27.260
was a computer which
was built at Manchester

00:43:27.260 --> 00:43:29.130
University in the 1960s.

00:43:29.130 --> 00:43:31.600
And Atlas had an
interesting structure, where

00:43:31.600 --> 00:43:33.180
it had two kinds of memory.

00:43:33.180 --> 00:43:36.660
It had a drum which could
store information in a way

00:43:36.660 --> 00:43:38.760
where it was very
slow to access.

00:43:38.760 --> 00:43:41.260
And then it also
had a set of sort

00:43:41.260 --> 00:43:44.480
of magnets which could be
used to store information

00:43:44.480 --> 00:43:48.157
in a format which was
relatively fast to access.

00:43:48.157 --> 00:43:49.740
And when they first
built the machine,

00:43:49.740 --> 00:43:52.690
the way that they were using it
was to read off the information

00:43:52.690 --> 00:43:54.690
would be needed for a
computation from the drum,

00:43:54.690 --> 00:43:57.000
and then store it in
the magnetic memory,

00:43:57.000 --> 00:43:59.730
and then do the operations, and
then write it back to the drum,

00:43:59.730 --> 00:44:01.270
and then take the next
part of the computation

00:44:01.270 --> 00:44:03.490
and read off all of the relevant
information from the drum,

00:44:03.490 --> 00:44:05.948
and then do the operations,
then write it back to the drum.

00:44:05.948 --> 00:44:08.890
But a mathematician who
was working on Atlas

00:44:08.890 --> 00:44:11.810
named Maurice Wilkes realized
that there was a better way

00:44:11.810 --> 00:44:13.039
to solve this problem.

00:44:13.039 --> 00:44:15.080
He realized that they
could make the whole system

00:44:15.080 --> 00:44:18.450
work much faster if
they didn't always

00:44:18.450 --> 00:44:21.489
take all of the information
out of the fast memory

00:44:21.489 --> 00:44:23.030
and put it back into
the slow memory,

00:44:23.030 --> 00:44:25.441
but rather they kept around
the pieces of information

00:44:25.441 --> 00:44:27.190
which they thought
they were going to need

00:44:27.190 --> 00:44:28.325
to use again in the future.

00:44:28.325 --> 00:44:29.950
So the reason why
this speeds things up

00:44:29.950 --> 00:44:31.030
is that then you
don't have to spend

00:44:31.030 --> 00:44:32.696
the extra time reading
those things back

00:44:32.696 --> 00:44:33.990
off the slow memory.

00:44:33.990 --> 00:44:36.960
And as a consequence, the
computer runs much faster.

00:44:36.960 --> 00:44:39.250
So this is an idea which
computer scientists now

00:44:39.250 --> 00:44:41.060
recognize as caching.

00:44:41.060 --> 00:44:43.890
So it's the idea of keeping
the information which you're

00:44:43.890 --> 00:44:47.330
most likely to need in the
future in the part of memory

00:44:47.330 --> 00:44:50.380
which is most easily and
most rapidly accessed.

00:44:50.380 --> 00:44:53.010
But it brings with
it another kind

00:44:53.010 --> 00:44:55.610
of algorithmic problem, which
is the problem of figuring out

00:44:55.610 --> 00:44:57.760
exactly what those
items that you're

00:44:57.760 --> 00:45:00.650
likely to need in the future
are going to be or, to put it

00:45:00.650 --> 00:45:03.160
another way, what it is
that you should throw away.

00:45:03.160 --> 00:45:04.700
And this is a
problem that's called

00:45:04.700 --> 00:45:06.860
the problem of cache eviction.

00:45:06.860 --> 00:45:09.420
So cache eviction
is something which

00:45:09.420 --> 00:45:11.250
requires us coming up
with good algorithms

00:45:11.250 --> 00:45:12.624
for deciding what
we're not going

00:45:12.624 --> 00:45:13.980
to need again in the future.

00:45:13.980 --> 00:45:16.700
And the person who really
made the first breakthrough

00:45:16.700 --> 00:45:20.240
in thinking about this is
this man, Laszlo Belady,

00:45:20.240 --> 00:45:22.700
who was working at IBM.

00:45:22.700 --> 00:45:25.530
So before he worked at IBM,
Belady had grown up in Hungary,

00:45:25.530 --> 00:45:29.530
and then fled during
the revolution

00:45:29.530 --> 00:45:33.660
there with only a bag containing
one change of underpants

00:45:33.660 --> 00:45:35.860
and his thesis paper.

00:45:35.860 --> 00:45:40.540
And then he ended up
having to then emigrate

00:45:40.540 --> 00:45:43.270
from Germany, which is
where he moved to, again

00:45:43.270 --> 00:45:45.200
with very minimal equipment.

00:45:45.200 --> 00:45:47.980
He just had $100 and his wife.

00:45:47.980 --> 00:45:51.530
So by the time he'd
reached IBM in the 1960s,

00:45:51.530 --> 00:45:53.017
he'd built up a
significant amount

00:45:53.017 --> 00:45:55.350
of experience in deciding
what it was that it made sense

00:45:55.350 --> 00:45:58.470
to leave behind.

00:45:58.470 --> 00:46:01.770
So Belady described
the optimal algorithm

00:46:01.770 --> 00:46:03.730
for solving the problem
of deciding what you

00:46:03.730 --> 00:46:05.420
should evict from your cache.

00:46:05.420 --> 00:46:08.040
And this optimal algorithm
is essentially clairvoyance.

00:46:08.040 --> 00:46:10.770
What you should do is evict
from the cache that piece

00:46:10.770 --> 00:46:12.770
of information which
you are going to need

00:46:12.770 --> 00:46:14.140
the furthest into the future.

00:46:14.140 --> 00:46:15.650
So as long as you can
see into the future

00:46:15.650 --> 00:46:17.983
as far as you need to go in
order to make that decision,

00:46:17.983 --> 00:46:19.430
then you can solve this problem.

00:46:19.430 --> 00:46:21.370
You can sort of do the
best possible solution

00:46:21.370 --> 00:46:23.274
to this problem that
you can imagine.

00:46:23.274 --> 00:46:24.690
Unfortunately,
when engineers have

00:46:24.690 --> 00:46:26.106
tried to implement
this algorithm,

00:46:26.106 --> 00:46:27.860
they've run into problems.

00:46:27.860 --> 00:46:30.710
And so for mere mortals, we need
to have some different kinds

00:46:30.710 --> 00:46:31.640
of algorithms.

00:46:31.640 --> 00:46:34.310
And Belady actually evaluated
three different kinds

00:46:34.310 --> 00:46:36.830
of algorithms-- one where
you just randomly evict items

00:46:36.830 --> 00:46:40.480
from the cache, one where the
things which were first entered

00:46:40.480 --> 00:46:42.890
into the cache are the
ones which first leave it,

00:46:42.890 --> 00:46:45.330
and one where you evict
those things which

00:46:45.330 --> 00:46:46.590
are least recently used.

00:46:46.590 --> 00:46:50.440
That is, those items which have
been used the furthest distance

00:46:50.440 --> 00:46:53.890
into the past are the ones
which leave the cache first.

00:46:53.890 --> 00:46:55.642
And doing an
empirical evaluation

00:46:55.642 --> 00:46:57.350
of these different
schemes, he discovered

00:46:57.350 --> 00:47:00.320
that there was one clear winner,
which is the least recently

00:47:00.320 --> 00:47:01.570
used algorithm.

00:47:01.570 --> 00:47:04.700
So basically, the idea
is that the information

00:47:04.700 --> 00:47:07.402
that you used least
recently is least

00:47:07.402 --> 00:47:09.110
likely to be the
information which you're

00:47:09.110 --> 00:47:11.360
going to need again in the
future as a consequence

00:47:11.360 --> 00:47:13.193
of a principle that
computer scientists call

00:47:13.193 --> 00:47:15.705
temporal locality-- basically
that if you just touched

00:47:15.705 --> 00:47:18.080
a piece of information, you're
going to be likely to need

00:47:18.080 --> 00:47:19.830
that information again
in the near future

00:47:19.830 --> 00:47:22.190
because there's a kind
of correlation over time

00:47:22.190 --> 00:47:26.020
in the pieces of information
which an algorithm might need

00:47:26.020 --> 00:47:27.890
to access.

00:47:27.890 --> 00:47:33.460
So taking this insight, you
can build caching systems

00:47:33.460 --> 00:47:35.220
which work very
efficiently in a variety

00:47:35.220 --> 00:47:36.530
of different situations.

00:47:36.530 --> 00:47:39.840
So nowadays, caches are
used all over the place.

00:47:39.840 --> 00:47:42.440
So if we look on
computers, we find

00:47:42.440 --> 00:47:45.421
that you'll see multiple chips
that are dedicated to caching.

00:47:45.421 --> 00:47:47.420
There are caches that are
built into hard disks.

00:47:47.420 --> 00:47:48.961
There are other
kinds of caches which

00:47:48.961 --> 00:47:51.280
are used in servers
for delivering websites

00:47:51.280 --> 00:47:53.950
to people as quickly and
efficiently as possible.

00:47:53.950 --> 00:47:56.840
But the one place where these
caching algorithms perhaps

00:47:56.840 --> 00:48:00.140
haven't been applied and perhaps
should is back in our closet.

00:48:00.140 --> 00:48:02.830
And if we look at these
ideas that Martha provides us

00:48:02.830 --> 00:48:06.230
with how to organize
our possessions,

00:48:06.230 --> 00:48:08.570
then as we go through
these possibilities,

00:48:08.570 --> 00:48:10.580
it's clear that
one of them might

00:48:10.580 --> 00:48:12.860
be a better recommendation
than the others, which

00:48:12.860 --> 00:48:15.242
is when was the last time
I wore it or used it,

00:48:15.242 --> 00:48:17.200
which is actually an
instantiation of the least

00:48:17.200 --> 00:48:18.710
recently used principle.

00:48:18.710 --> 00:48:20.990
So next time you're
thinking about trying

00:48:20.990 --> 00:48:23.080
to organize your closet,
it might be worth

00:48:23.080 --> 00:48:26.810
keeping this in mind, that as
long as your possessions obey

00:48:26.810 --> 00:48:29.002
the same kind of principle
of temporal locality,

00:48:29.002 --> 00:48:31.460
focusing on those possessions
that were least recently used

00:48:31.460 --> 00:48:33.293
might be the most
predictive of those things

00:48:33.293 --> 00:48:35.970
that you're least likely to
need again in the future.

00:48:35.970 --> 00:48:38.119
So this kind of principle
isn't just something

00:48:38.119 --> 00:48:39.660
which is useful in
thinking about how

00:48:39.660 --> 00:48:40.830
to organize your closet.

00:48:40.830 --> 00:48:42.205
It's also something
that might be

00:48:42.205 --> 00:48:44.620
useful in thinking about
how to organize your office.

00:48:44.620 --> 00:48:47.540
And this was a discovery that
was made somewhat accidentally

00:48:47.540 --> 00:48:50.540
by a Japanese economist
called Yukio Noguchi.

00:48:50.540 --> 00:48:52.490
Co Noguchi is a tax economist.

00:48:52.490 --> 00:48:54.110
He was constantly
receiving reports

00:48:54.110 --> 00:48:57.020
and papers and documents
that needed to be filed away.

00:48:57.020 --> 00:48:59.524
And he was kind of overwhelmed
with all of this information.

00:48:59.524 --> 00:49:01.190
He didn't have time
to file it properly,

00:49:01.190 --> 00:49:03.230
so he came up with
a simple solution,

00:49:03.230 --> 00:49:06.350
which was just to put all
of those papers into a box.

00:49:06.350 --> 00:49:08.290
But he didn't just
dump them into a box.

00:49:08.290 --> 00:49:09.970
What he did was
actually put them

00:49:09.970 --> 00:49:12.100
into a box in a
very orderly way.

00:49:12.100 --> 00:49:17.740
So basically, he had a box
which was sort of horizontally

00:49:17.740 --> 00:49:18.320
aligned.

00:49:18.320 --> 00:49:21.671
And he put the information into
the box at one end of the box.

00:49:21.671 --> 00:49:23.170
So as he'd get some
new papers, he'd

00:49:23.170 --> 00:49:25.060
put those papers in
at the left-hand side.

00:49:25.060 --> 00:49:27.250
And as a consequence,
you know, the papers

00:49:27.250 --> 00:49:30.612
would sort of move down the
box as new things came in.

00:49:30.612 --> 00:49:32.320
And then he did
something else important,

00:49:32.320 --> 00:49:37.700
which is that as he used
one of those papers,

00:49:37.700 --> 00:49:38.876
he'd pull it out of the box.

00:49:38.876 --> 00:49:40.500
And then when he was
finished using it,

00:49:40.500 --> 00:49:43.060
he'd put it back in again at
the left-hand side of the box.

00:49:43.060 --> 00:49:46.660
So this has a clear connection
to least recently used caching,

00:49:46.660 --> 00:49:47.160
right?

00:49:47.160 --> 00:49:49.610
Once your box fills up, you
need to get rid of something.

00:49:49.610 --> 00:49:51.193
And you can get rid
of the things that

00:49:51.193 --> 00:49:53.030
hit the right end of
the box because those

00:49:53.030 --> 00:49:56.140
are the things which you
used furthest in the past

00:49:56.140 --> 00:50:02.570
and consequently, least likely
to need in the near future.

00:50:02.570 --> 00:50:05.780
But this principle
of taking out a file

00:50:05.780 --> 00:50:09.310
and then putting it back at
the left-hand side of the box

00:50:09.310 --> 00:50:11.330
also corresponds to
another idea which

00:50:11.330 --> 00:50:13.425
has shown up in theoretical
computer science.

00:50:13.425 --> 00:50:15.550
So we can actually show
that this way of organizing

00:50:15.550 --> 00:50:16.966
his information
is something which

00:50:16.966 --> 00:50:18.600
is near optimal, or
at least as close

00:50:18.600 --> 00:50:20.620
to optimal as we're
likely to be able to get.

00:50:20.620 --> 00:50:23.940
So the actual data structure
that he had created here

00:50:23.940 --> 00:50:26.190
is something that a computer
scientist would recognize

00:50:26.190 --> 00:50:27.810
as a self-organizing list.

00:50:27.810 --> 00:50:29.820
So basically, in a
self-organizing list,

00:50:29.820 --> 00:50:32.100
you have a sequence of
pieces of information.

00:50:32.100 --> 00:50:34.740
And then as you access
those pieces of information,

00:50:34.740 --> 00:50:36.820
you have the opportunity
to change the order

00:50:36.820 --> 00:50:39.000
that those pieces of
information appear in.

00:50:39.000 --> 00:50:43.060
And so this idea of taking the
information that's accessed

00:50:43.060 --> 00:50:46.370
and then putting it at the
very front of the list,

00:50:46.370 --> 00:50:48.980
or at the left-hand
side of Noguchi's box,

00:50:48.980 --> 00:50:51.150
is actually something
which turns out

00:50:51.150 --> 00:50:53.595
to be a very
effective algorithm.

00:50:53.595 --> 00:50:55.220
So Robert [? Tagin ?]
and Daniel Slater

00:50:55.220 --> 00:50:58.750
proved in 1985 that moving
the most recent item

00:50:58.750 --> 00:51:01.800
to the front of the list
is, at worst, twice as bad

00:51:01.800 --> 00:51:02.850
as clairvoyance.

00:51:02.850 --> 00:51:05.647
So clairvoyance is the best
that you could possibly do.

00:51:05.647 --> 00:51:07.480
And it turns out, this
is the only algorithm

00:51:07.480 --> 00:51:09.290
that comes with a
theoretical guarantee

00:51:09.290 --> 00:51:11.700
of being at least
close to clairvoyance

00:51:11.700 --> 00:51:14.330
in multiplicative terms.

00:51:14.330 --> 00:51:16.880
So if you're thinking about
implementing the Noguchi filing

00:51:16.880 --> 00:51:19.480
system in your
office, it might be

00:51:19.480 --> 00:51:22.250
reassuring to realize that
perhaps you already have.

00:51:22.250 --> 00:51:24.610
So we normally think
about a messy office

00:51:24.610 --> 00:51:25.720
as being a bad thing.

00:51:25.720 --> 00:51:27.492
And in particular, a
giant pile of papers

00:51:27.492 --> 00:51:28.950
on your desk like
this is something

00:51:28.950 --> 00:51:32.310
which kind of seems like a
poor method of organizing

00:51:32.310 --> 00:51:33.031
information.

00:51:33.031 --> 00:51:34.530
But you can kind
of think about this

00:51:34.530 --> 00:51:37.520
as taking the Noguchi system,
and then literally turning it

00:51:37.520 --> 00:51:38.540
on its side.

00:51:38.540 --> 00:51:39.340
Right?

00:51:39.340 --> 00:51:44.289
So a big pile of papers is, in
fact, a self-organizing list.

00:51:44.289 --> 00:51:46.080
And if you're taking
things out of the pile

00:51:46.080 --> 00:51:47.330
and then sticking
them back on the top

00:51:47.330 --> 00:51:48.996
and putting the most
recently used items

00:51:48.996 --> 00:51:50.510
on the top of the
pile, then you're

00:51:50.510 --> 00:51:55.500
implementing exactly this
relatively optimal strategy

00:51:55.500 --> 00:51:58.820
for organizing the information
that's contained in that list.

00:51:58.820 --> 00:52:03.332
So if you're somebody who
is familiar with these kinds

00:52:03.332 --> 00:52:04.790
of messes, you'll
also be reassured

00:52:04.790 --> 00:52:07.280
by some of the message in
our sorting chapter, where

00:52:07.280 --> 00:52:12.576
we argue against sorting in
many domestic situations.

00:52:12.576 --> 00:52:13.950
Thinking about
caching isn't just

00:52:13.950 --> 00:52:15.860
useful for thinking about
how to organize information

00:52:15.860 --> 00:52:16.610
around you.

00:52:16.610 --> 00:52:19.770
It's also something which might
give you new insights into how

00:52:19.770 --> 00:52:20.941
human memory works.

00:52:20.941 --> 00:52:22.940
So I think there's an
intuition that a lot of us

00:52:22.940 --> 00:52:24.350
would have about
memory, which is

00:52:24.350 --> 00:52:26.150
kind of thinking
about it as something

00:52:26.150 --> 00:52:27.450
sort of like Noguchi's box.

00:52:27.450 --> 00:52:27.950
Right?

00:52:27.950 --> 00:52:32.230
You've got kind of a limited
amount of space in your memory.

00:52:32.230 --> 00:52:34.714
And so when you
learn something new,

00:52:34.714 --> 00:52:35.880
you have to put it in there.

00:52:35.880 --> 00:52:37.740
And when you do that, maybe
it pops something else out.

00:52:37.740 --> 00:52:39.620
And as a consequence,
you forget that thing.

00:52:39.620 --> 00:52:41.245
And so forgetting is
just a consequence

00:52:41.245 --> 00:52:43.500
of kind of hitting a
capacity limit in terms

00:52:43.500 --> 00:52:45.622
of the amount of information
that we can store.

00:52:45.622 --> 00:52:48.080
But a cognitive scientist called
John Anderson has actually

00:52:48.080 --> 00:52:50.121
proposed that there's a
different way of thinking

00:52:50.121 --> 00:52:52.470
about how memory works and
thinking that the analogy is

00:52:52.470 --> 00:52:56.110
less like a box of finite
size and more like a library

00:52:56.110 --> 00:52:57.387
of infinite size.

00:52:57.387 --> 00:52:59.720
So if you have a library which
has infinitely many books

00:52:59.720 --> 00:53:03.070
arrayed along a sort of
linear shelf like this,

00:53:03.070 --> 00:53:04.830
then the problem
that you have is

00:53:04.830 --> 00:53:08.100
one not of figuring out
where to fit information

00:53:08.100 --> 00:53:11.080
but rather one of figuring out
how to organize information.

00:53:11.080 --> 00:53:13.670
Another good analog of this
is thinking about something

00:53:13.670 --> 00:53:16.840
like web search, where you've
got a whole lot of web pages,

00:53:16.840 --> 00:53:19.370
and you want to organize
those web pages in such a way

00:53:19.370 --> 00:53:21.137
that you can find the
things that people

00:53:21.137 --> 00:53:23.470
are likely to be looking for
with high probability close

00:53:23.470 --> 00:53:26.250
to the top of the
list that you produce.

00:53:26.250 --> 00:53:29.030
So from this perspective,
forgetting something

00:53:29.030 --> 00:53:32.120
isn't that you've sort of
had that thing sort of pop

00:53:32.120 --> 00:53:34.680
out of your memory but
rather that the time it

00:53:34.680 --> 00:53:36.990
would take you to find
that thing is greater

00:53:36.990 --> 00:53:39.395
than the amount of time that
you're willing to spend,

00:53:39.395 --> 00:53:41.430
that the way that
information is organized

00:53:41.430 --> 00:53:43.240
is not sufficiently
good to allow you

00:53:43.240 --> 00:53:45.240
to identify that item quickly.

00:53:45.240 --> 00:53:47.340
And so this makes an
interesting prediction,

00:53:47.340 --> 00:53:49.100
which is that we
should be trying

00:53:49.100 --> 00:53:51.380
to organize those
items in memory

00:53:51.380 --> 00:53:53.880
such that the things that we
think we're most likely to need

00:53:53.880 --> 00:53:56.430
in the future are going
to be the things which

00:53:56.430 --> 00:53:58.300
we find easiest to recall.

00:53:58.300 --> 00:54:00.600
And Anderson actually showed
some evidence for this.

00:54:00.600 --> 00:54:02.039
So he took some famous data.

00:54:02.039 --> 00:54:03.580
This is data from
an experiment which

00:54:03.580 --> 00:54:08.680
was done in the 19th century
by an early psychologist,

00:54:08.680 --> 00:54:11.060
Ebbinghaus, who
basically taught himself

00:54:11.060 --> 00:54:13.240
some lists of
nonsense syllables,

00:54:13.240 --> 00:54:16.700
and then would look at how well
he could recall those lists

00:54:16.700 --> 00:54:20.005
some number of hours
after he'd memorized them.

00:54:20.005 --> 00:54:21.380
And so what Anderson
did was look

00:54:21.380 --> 00:54:24.600
at whether this pattern of
recall, where, basically, you

00:54:24.600 --> 00:54:27.840
get this kind of rapid
falloff followed by a slow,

00:54:27.840 --> 00:54:29.810
sort of long tail
could be predicted

00:54:29.810 --> 00:54:32.430
by looking at how
likely it is that we're

00:54:32.430 --> 00:54:34.780
going to encounter particular
pieces of information

00:54:34.780 --> 00:54:36.227
in our environment.

00:54:36.227 --> 00:54:38.310
And so what he did was go
to the "New York Times,"

00:54:38.310 --> 00:54:40.070
look at headlines in
the "New York Times,"

00:54:40.070 --> 00:54:41.900
and then look at how
likely a word was

00:54:41.900 --> 00:54:44.010
to appear in the headline
in the "New York Times"

00:54:44.010 --> 00:54:46.840
as a function of how
long ago it previously

00:54:46.840 --> 00:54:49.140
appeared in those headlines.

00:54:49.140 --> 00:54:51.530
So he could look at the
relationship between the number

00:54:51.530 --> 00:54:53.700
of days that had elapsed,
and then the probability

00:54:53.700 --> 00:54:54.750
of an item appearing.

00:54:54.750 --> 00:54:57.800
And he found that this showed
a very similar kind of curve.

00:54:57.800 --> 00:55:00.099
So this kind of correspondence
between the probability

00:55:00.099 --> 00:55:01.640
that something is
likely to be needed

00:55:01.640 --> 00:55:03.750
as a function of how
long ago it was used

00:55:03.750 --> 00:55:06.850
and the patterns of recall
that we see in human memory

00:55:06.850 --> 00:55:08.559
provides one suggestive
form of evidence,

00:55:08.559 --> 00:55:10.516
that one of the things
that our minds are doing

00:55:10.516 --> 00:55:12.500
is trying to organize
information in such a way

00:55:12.500 --> 00:55:13.830
that we are keeping
around those things

00:55:13.830 --> 00:55:15.788
that we are likely to
need again in the future.

00:55:23.010 --> 00:55:24.860
BRIAN CHRISTIAN:
Thinking computationally

00:55:24.860 --> 00:55:28.430
about the types of problems that
we encounter in everyday life

00:55:28.430 --> 00:55:31.998
has payoffs at a number
of different scales.

00:55:35.210 --> 00:55:37.400
The overarching
argument of the book

00:55:37.400 --> 00:55:40.550
is there are deep parallels
between the problems that we

00:55:40.550 --> 00:55:44.280
face in our lives and the
ones that are considered some

00:55:44.280 --> 00:55:47.640
of the fundamental and canonical
problems in mathematics

00:55:47.640 --> 00:55:49.750
and computer science.

00:55:49.750 --> 00:55:53.210
And this is significant
because as a result,

00:55:53.210 --> 00:55:56.010
there are these simple,
optimal strategies that

00:55:56.010 --> 00:55:58.610
are directly relevant to those
domains in our own lives.

00:55:58.610 --> 00:56:00.770
And there's something
very concrete

00:56:00.770 --> 00:56:03.940
that we can use and learn from.

00:56:03.940 --> 00:56:08.170
And even if we're not literally
going to stop at exactly,

00:56:08.170 --> 00:56:12.110
you know, 37% or so
forth, having a vocabulary

00:56:12.110 --> 00:56:14.119
and knowing what
an optimal stopping

00:56:14.119 --> 00:56:15.910
or an explore or exploit
problem looks like

00:56:15.910 --> 00:56:19.480
and having a general sense
of how the solution is

00:56:19.480 --> 00:56:23.310
structured gives us a way to
bolster our own intuitions when

00:56:23.310 --> 00:56:26.580
we find ourselves
in those situations.

00:56:26.580 --> 00:56:31.720
And I think most broadly,
a lot of the solutions

00:56:31.720 --> 00:56:34.800
that we explore
don't necessarily

00:56:34.800 --> 00:56:38.250
look like what we think of when
we think of what computers do,

00:56:38.250 --> 00:56:40.790
which gives us an opportunity
to actually rethink

00:56:40.790 --> 00:56:43.570
our notion of
rationality itself.

00:56:43.570 --> 00:56:50.660
So intuitively, we kind of have
this bias that being rational

00:56:50.660 --> 00:56:55.200
means being exhaustive, exact,
deterministic, considering

00:56:55.200 --> 00:56:59.090
everything, getting
an exact answer that's

00:56:59.090 --> 00:57:02.170
correct 100% of the time.

00:57:02.170 --> 00:57:06.180
In fact, this is
not what computers

00:57:06.180 --> 00:57:09.270
do when they're up against the
hardest classes of problems.

00:57:09.270 --> 00:57:12.920
This is kind of the
luxury of an easy problem.

00:57:12.920 --> 00:57:17.020
And up against an
intractable problem,

00:57:17.020 --> 00:57:19.490
computer scientists turn
to a totally different set

00:57:19.490 --> 00:57:22.000
of techniques.

00:57:22.000 --> 00:57:26.590
When we take into account the
cost, the labor of thought

00:57:26.590 --> 00:57:31.390
itself, the best strategies may
not be to consider everything,

00:57:31.390 --> 00:57:33.770
to think indefinitely, to
always get the right answer

00:57:33.770 --> 00:57:34.930
in each situation.

00:57:34.930 --> 00:57:39.910
We may want to, in fact,
trade off the labor

00:57:39.910 --> 00:57:42.880
of the computation versus
the quality of the result.

00:57:42.880 --> 00:57:46.990
And as the book progresses,
especially in the second half,

00:57:46.990 --> 00:57:49.880
we look at what
these strategies are

00:57:49.880 --> 00:57:52.390
for dealing with
intractable problems, which

00:57:52.390 --> 00:57:56.220
most of the ones that we
face in real life are.

00:57:56.220 --> 00:57:57.900
And that leads to
this conclusion,

00:57:57.900 --> 00:58:03.230
that what computer
scientists do up

00:58:03.230 --> 00:58:05.120
against the hardest
classes of problems

00:58:05.120 --> 00:58:07.650
is they use approximations.

00:58:07.650 --> 00:58:10.080
They trade off
the costs of error

00:58:10.080 --> 00:58:12.100
against the costs of delay.

00:58:12.100 --> 00:58:15.400
They relax some of the
constraints of the problem,

00:58:15.400 --> 00:58:17.460
and they turn to chance.

00:58:17.460 --> 00:58:19.830
These aren't the
concessions that we

00:58:19.830 --> 00:58:21.990
make when we can't be rational.

00:58:21.990 --> 00:58:25.860
These are what being
rational means.

00:58:25.860 --> 00:58:28.185
We explore this line of
thinking through a number

00:58:28.185 --> 00:58:29.060
of different domains.

00:58:29.060 --> 00:58:31.790
We've talked about three today.

00:58:31.790 --> 00:58:34.275
We also look at
sorting algorithms--

00:58:34.275 --> 00:58:36.650
what do they tell you about
how to arrange your bookshelf

00:58:36.650 --> 00:58:39.820
and, more importantly,
whether you should?

00:58:39.820 --> 00:58:41.540
We look at scheduling theory.

00:58:41.540 --> 00:58:43.200
Every operating
system has a scheduler

00:58:43.200 --> 00:58:46.820
that tells the CPU what to
be doing when, for how long,

00:58:46.820 --> 00:58:48.620
and what to do next.

00:58:48.620 --> 00:58:50.480
So we look at what the
parallels are there

00:58:50.480 --> 00:58:53.750
for thinking about time
management in our own lives.

00:58:53.750 --> 00:58:55.460
And in the context
of Bayes's rule,

00:58:55.460 --> 00:58:58.810
we think about problems of
predicting the future-- how

00:58:58.810 --> 00:59:02.180
long a process will go on, how
much money something will make

00:59:02.180 --> 00:59:04.040
based on what it's made so far.

00:59:04.040 --> 00:59:05.990
And, on a personal
level, you've been dating

00:59:05.990 --> 00:59:07.156
someone for a couple months.

00:59:07.156 --> 00:59:09.390
It's going pretty well so far.

00:59:09.390 --> 00:59:11.630
Is it premature to
book that weekend place

00:59:11.630 --> 00:59:13.550
in Tahoe at the
end of the summer?

00:59:13.550 --> 00:59:14.490
What should you do?

00:59:14.490 --> 00:59:18.470
And we provide some rational
answers there, as well.

00:59:18.470 --> 00:59:21.270
And unlike the types
of advice that you

00:59:21.270 --> 00:59:24.220
might find in, for
example, self-help books,

00:59:24.220 --> 00:59:27.080
the insights that are derived
from thinking computationally

00:59:27.080 --> 00:59:31.230
about these problems
are backed by proofs.

00:59:31.230 --> 00:59:32.570
Thank you so much.

00:59:32.570 --> 00:59:35.080
[APPLAUSE]

00:59:36.816 --> 00:59:37.774
BORIS DEBIC: Questions?

00:59:41.050 --> 00:59:43.740
BRIAN CHRISTIAN: Do you want
to get more in the light?

00:59:43.740 --> 00:59:44.240
Yeah.

00:59:47.740 --> 00:59:52.310
AUDIENCE: So how useful is it to
have a proof of an abstraction

00:59:52.310 --> 00:59:55.597
when the real life doesn't
match the abstraction?

00:59:55.597 --> 00:59:56.430
TOM GRIFFITHS: Yeah.

00:59:56.430 --> 00:59:59.030
So I think the
way that we really

00:59:59.030 --> 01:00:01.217
think about this is
there are definitely

01:00:01.217 --> 01:00:03.550
simplifications that go into
formulating these problems.

01:00:03.550 --> 01:00:06.220
But what you get out
of them is if you're

01:00:06.220 --> 01:00:07.686
in exactly that
situation, you know

01:00:07.686 --> 01:00:08.810
exactly what you should do.

01:00:08.810 --> 01:00:10.020
But more generally,
you get insights

01:00:10.020 --> 01:00:12.560
about how those solutions change
as a consequence of changing

01:00:12.560 --> 01:00:13.226
the assumptions.

01:00:13.226 --> 01:00:15.520
So, for example, I talked
about some of the variants

01:00:15.520 --> 01:00:16.860
on the secretary problem.

01:00:16.860 --> 01:00:19.289
And from that, you might
not know exactly what

01:00:19.289 --> 01:00:20.580
the number is in your scenario.

01:00:20.580 --> 01:00:21.770
But you can recognize, OK.

01:00:21.770 --> 01:00:23.440
Well, if it becomes
more permissive,

01:00:23.440 --> 01:00:27.710
then I should be more
willing to spend longer

01:00:27.710 --> 01:00:29.020
looking before I start leaping.

01:00:29.020 --> 01:00:31.180
And if it becomes
less permissive,

01:00:31.180 --> 01:00:33.430
than I should have a much
more rapid transition

01:00:33.430 --> 01:00:34.180
from those things.

01:00:34.180 --> 01:00:37.820
And I think there's a
related question here,

01:00:37.820 --> 01:00:40.650
which is if we think that
people should follow algorithms,

01:00:40.650 --> 01:00:42.269
why don't we just
make computers that

01:00:42.269 --> 01:00:43.810
will solve these
problems for people,

01:00:43.810 --> 01:00:45.730
and then people don't have
to make any decisions at all?

01:00:45.730 --> 01:00:47.438
And I think one of
the things that people

01:00:47.438 --> 01:00:49.214
are really good
at is figuring out

01:00:49.214 --> 01:00:51.130
how to interpolate between
these possibilities

01:00:51.130 --> 01:00:53.100
and how to kind of evaluate
some of the fuzziness

01:00:53.100 --> 01:00:55.141
around the particular
problems that we're facing.

01:00:55.141 --> 01:00:56.660
And so we're really
providing tools

01:00:56.660 --> 01:00:59.704
that can guide those
human capacities in terms

01:00:59.704 --> 01:01:01.995
of thinking about solutions
to more realistic problems.

01:01:07.990 --> 01:01:09.730
AUDIENCE: Very
interesting topic.

01:01:09.730 --> 01:01:14.600
How long it take you
guys to write this book?

01:01:14.600 --> 01:01:17.090
BRIAN CHRISTIAN:
There's a quote that we

01:01:17.090 --> 01:01:19.730
use at the beginning
of the book,

01:01:19.730 --> 01:01:21.770
of the chapter on scheduling.

01:01:21.770 --> 01:01:25.840
We have, the
epigraph says-- it's

01:01:25.840 --> 01:01:28.230
from Eugene Lawler, who was
a researcher in scheduling

01:01:28.230 --> 01:01:28.730
theory.

01:01:28.730 --> 01:01:31.630
And he says, "why don't we write
a book on scheduling theory,

01:01:31.630 --> 01:01:32.500
I asked.

01:01:32.500 --> 01:01:34.550
It shouldn't take much time.

01:01:34.550 --> 01:01:36.860
Book writing, like
warmaking, often

01:01:36.860 --> 01:01:40.040
entails grave miscalculations.

01:01:40.040 --> 01:01:44.760
15 years later, scheduling
is still unfinished."

01:01:44.760 --> 01:01:48.340
So I think Tom and I had
an analogous experience,

01:01:48.340 --> 01:01:52.560
where we-- I think of the book
as really emerging in a dinner

01:01:52.560 --> 01:01:55.720
that Tom and I had in
2011, where we-- I mean,

01:01:55.720 --> 01:01:57.260
we've known each
other for 11 years.

01:01:57.260 --> 01:01:59.122
These are shared
obsessions that we've

01:01:59.122 --> 01:02:00.330
talked about for a long time.

01:02:00.330 --> 01:02:01.837
But we basically
realized that we

01:02:01.837 --> 01:02:04.420
wanted to write the same book,
what amounted to the same book.

01:02:04.420 --> 01:02:08.630
And so that was when we decided
to team up and work together.

01:02:08.630 --> 01:02:12.400
And we had what in hindsight
was a sort of predictably

01:02:12.400 --> 01:02:15.160
naive sense of like,
oh, it should take

01:02:15.160 --> 01:02:16.990
about 18 months or something.

01:02:16.990 --> 01:02:19.690
It'll be out in 2013.

01:02:19.690 --> 01:02:20.330
So here we are.

01:02:20.330 --> 01:02:22.357
And you can tell how
that plan worked out.

01:02:22.357 --> 01:02:23.940
TOM GRIFFITHS: I
should also point out

01:02:23.940 --> 01:02:26.080
another terrible
thing that can happen

01:02:26.080 --> 01:02:28.030
to a book is having
a small child.

01:02:28.030 --> 01:02:31.590
So my wife and I
had our second child

01:02:31.590 --> 01:02:33.950
somewhere in that process,
which threw everything off.

01:02:37.649 --> 01:02:39.940
AUDIENCE: Are there any
rejected chapters in this book?

01:02:39.940 --> 01:02:41.640
Are there any
areas of life where

01:02:41.640 --> 01:02:44.424
you looked hard for an analog,
and it didn't work out?

01:02:44.424 --> 01:02:46.090
TOM GRIFFITHS: Yeah,
that's interesting.

01:02:46.090 --> 01:02:49.070
So our rejected
chapters are more

01:02:49.070 --> 01:02:51.404
that there are lots and
lots of algorithmic ideas

01:02:51.404 --> 01:02:52.820
that we would love
to write about,

01:02:52.820 --> 01:02:55.360
but we just ran out of
space and time to include.

01:02:55.360 --> 01:02:57.990
So we've actually
got a folder which

01:02:57.990 --> 01:03:01.920
is called Sequel, which
is the only way that we

01:03:01.920 --> 01:03:05.650
were happy cutting things,
so we could pretend.

01:03:05.650 --> 01:03:08.760
And so that contains a bunch
of chapters and proto-chapters

01:03:08.760 --> 01:03:11.191
that explored a lot of
different kinds of algorithms.

01:03:11.191 --> 01:03:13.440
And in some cases, it was
that there's algorithms that

01:03:13.440 --> 01:03:15.550
have already got good press.

01:03:15.550 --> 01:03:18.980
And in other cases,
it was that we really

01:03:18.980 --> 01:03:22.340
felt like there was a key
insight that those things could

01:03:22.340 --> 01:03:23.900
give you about human lives.

01:03:23.900 --> 01:03:27.550
But either you had to get
too far into the weeds

01:03:27.550 --> 01:03:34.000
in order to get out what that
insight was or it was something

01:03:34.000 --> 01:03:36.985
where-- like, basically,
the examples we give

01:03:36.985 --> 01:03:38.610
in the book have been
sort of carefully

01:03:38.610 --> 01:03:41.949
selected for exactly
the right blend of you

01:03:41.949 --> 01:03:43.740
need to understand a
certain amount of math

01:03:43.740 --> 01:03:45.660
in order to get a
practical payoff.

01:03:45.660 --> 01:03:48.589
And some things just didn't
reach that threshold.

01:03:48.589 --> 01:03:50.380
BRIAN CHRISTIAN: We
also had a beta version

01:03:50.380 --> 01:03:55.090
of a chapter on data structures
that had a lot of great stuff

01:03:55.090 --> 01:03:57.080
that we still kind of
wish we could have saved.

01:03:57.080 --> 01:03:59.246
But it just didn't fit under
the rubric of the book.

01:03:59.246 --> 01:04:02.046
So that, I guess, lives in
the Sequel folder now, too.

01:04:12.912 --> 01:04:14.870
AUDIENCE: Do you feel
like you learned anything

01:04:14.870 --> 01:04:20.200
about the limits or lack thereof
of what machine intelligences

01:04:20.200 --> 01:04:21.360
could be capable of?

01:04:21.360 --> 01:04:23.240
It seems like a lot
of these examples

01:04:23.240 --> 01:04:26.450
right now involve pretty
specific parameters for what

01:04:26.450 --> 01:04:28.870
the problem has to be,
but also that there's just

01:04:28.870 --> 01:04:31.910
a ton of similarity between
how humans actually do things

01:04:31.910 --> 01:04:34.184
and how computers
end up doing them.

01:04:34.184 --> 01:04:35.600
BRIAN CHRISTIAN:
This is one where

01:04:35.600 --> 01:04:38.810
I think Tom can speak as a
machine learning researcher.

01:04:38.810 --> 01:04:41.060
But I would just
say, as a preamble,

01:04:41.060 --> 01:04:43.210
I think exactly how
you've formulated

01:04:43.210 --> 01:04:45.320
the question is how
I think of it, which

01:04:45.320 --> 01:04:49.910
is that a lot of the
algorithms that we discuss

01:04:49.910 --> 01:04:54.810
involve assumptions about
the distribution of values.

01:04:54.810 --> 01:04:56.970
The Gittins index, the
way that we present it,

01:04:56.970 --> 01:04:59.840
assumes a uniform distribution,
or the house-selling problem

01:04:59.840 --> 01:05:02.340
assumes uniform distribution.

01:05:02.340 --> 01:05:05.260
And often, the hardest
versions of these problems

01:05:05.260 --> 01:05:07.860
are what are called partial
information problems,

01:05:07.860 --> 01:05:11.350
where you have to
be making inferences

01:05:11.350 --> 01:05:13.270
about what you think
the distribution looks

01:05:13.270 --> 01:05:17.720
like on the fly based on the
values that you've encountered.

01:05:17.720 --> 01:05:21.100
And those often turn out to
be the intractable problems.

01:05:21.100 --> 01:05:27.230
And so there is a sense in which
human intelligence involves

01:05:27.230 --> 01:05:29.090
making all of these
inferential leaps,

01:05:29.090 --> 01:05:31.030
but also deciding
how to parameterize

01:05:31.030 --> 01:05:32.545
the problem in the first place.

01:05:32.545 --> 01:05:33.170
AUDIENCE: Yeah.

01:05:33.170 --> 01:05:35.420
I think this also relates
to Peter's question.

01:05:35.420 --> 01:05:40.760
So I think as you get closer
to the actual problems

01:05:40.760 --> 01:05:43.240
that human beings solve,
you sort of start to hit

01:05:43.240 --> 01:05:45.050
the edges of the theory.

01:05:45.050 --> 01:05:48.610
Another good example of this
is in the explore or exploit

01:05:48.610 --> 01:05:52.894
setting, where-- so if you
get people and put them

01:05:52.894 --> 01:05:54.310
in an explore or
exploit scenario,

01:05:54.310 --> 01:05:57.904
you find pretty consistently
that people deviate

01:05:57.904 --> 01:05:59.320
from the predictions
that come out

01:05:59.320 --> 01:06:02.329
of these sort of standard
multi-armed bandit models.

01:06:02.329 --> 01:06:04.120
And they deviate in a
particular direction,

01:06:04.120 --> 01:06:06.600
which is that they
tend to over-explore.

01:06:06.600 --> 01:06:09.582
So kind of at the point where
the algorithm is committed

01:06:09.582 --> 01:06:11.040
and said, hey, just
keep on pulling

01:06:11.040 --> 01:06:12.520
that one lever, people
are still like, well,

01:06:12.520 --> 01:06:13.320
I'm going to go try that one.

01:06:13.320 --> 01:06:14.030
I'm going to try that one.

01:06:14.030 --> 01:06:15.400
I'm going to try that
one, and still trying

01:06:15.400 --> 01:06:16.600
out different options.

01:06:16.600 --> 01:06:19.090
And so that looks
mysterious until you

01:06:19.090 --> 01:06:22.470
realize that it's not the
people are being irrational.

01:06:22.470 --> 01:06:25.370
It's that, in fact, the
assumptions of the model

01:06:25.370 --> 01:06:27.370
are kind of wrong for
a real human situation.

01:06:27.370 --> 01:06:32.360
So the assumption is that
a slot machine pays off

01:06:32.360 --> 01:06:34.740
with the same probability.

01:06:34.740 --> 01:06:36.130
That's sort of fixed over time.

01:06:36.130 --> 01:06:39.629
Whereas in a human environment,
the payoff probabilities

01:06:39.629 --> 01:06:41.920
for different options are
things that change over time.

01:06:41.920 --> 01:06:42.420
Right?

01:06:42.420 --> 01:06:44.590
So they might be sort of
slowly drifting around.

01:06:44.590 --> 01:06:46.730
A particular restaurant is
getting better or worse,

01:06:46.730 --> 01:06:49.570
or a chef has changed, or
it's under new management,

01:06:49.570 --> 01:06:51.620
or-- all of those
things are things

01:06:51.620 --> 01:06:54.360
that over time mean that those
probabilities can be different.

01:06:54.360 --> 01:06:56.530
And so if you're in a
situation like that,

01:06:56.530 --> 01:06:58.360
then what you should
be doing is, in fact,

01:06:58.360 --> 01:07:00.526
exploring more because you
need to go back and check

01:07:00.526 --> 01:07:02.226
on the information that you had.

01:07:02.226 --> 01:07:04.350
But that problem is what's
called a restless bandit

01:07:04.350 --> 01:07:06.540
problem, and it's one
which still presents

01:07:06.540 --> 01:07:09.090
a challenge for developing
sort of good machine learning

01:07:09.090 --> 01:07:10.340
methods that can deal with it.

01:07:10.340 --> 01:07:14.580
So I think there's plenty
of room to take inspiration

01:07:14.580 --> 01:07:19.550
from human cognition, as well as
room for making recommendations

01:07:19.550 --> 01:07:21.430
about humankind.

01:07:21.430 --> 01:07:23.150
BORIS DEBIC: And with
that, let's please

01:07:23.150 --> 01:07:25.510
thank Tom and Michael
for coming to Google.

01:07:25.510 --> 01:07:27.360
BRIAN CHRISTIAN:
Thank you so much.

