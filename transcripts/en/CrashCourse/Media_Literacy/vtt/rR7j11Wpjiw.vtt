WEBVTT
Kind: captions
Language: en

00:00:05.240 --> 00:00:07.160
Say you have an evil twin.

00:00:07.160 --> 00:00:10.900
They’re just like you but...different – somehow
evil-er.

00:00:10.900 --> 00:00:15.860
Maybe they have a fancy twirling mustache
and are just constantly listening to emo music.

00:00:15.860 --> 00:00:20.500
Maybe they hate chocolate and fun and bubbles
and the greatest film of all time, Titanic.

00:00:20.500 --> 00:00:25.880
What if your evil twin not only looked and
sounded just like you, but pretended to be you.

00:00:25.880 --> 00:00:30.320
They stole your family and your friends and
your significant other and your favorite pair
of shoes.

00:00:30.320 --> 00:00:34.260
They persuaded everyone they were the real
you and you were the evil twin.

00:00:34.260 --> 00:00:38.540
And then you were left all confused and alone
and you didn’t even get a fancy twirling mustache.

00:00:38.540 --> 00:00:39.960
Sounds like a nightmare, right?

00:00:39.960 --> 00:00:44.760
We’ve talked all about persuasive
techniques and advertising and public relations
during this course.

00:00:44.760 --> 00:00:50.340
But we haven’t talked about their evil twins:
propaganda, misinformation and disinformation.

00:00:50.340 --> 00:00:55.160
These are the big baddies of the media world,
the villains you really need to watch out for.

00:00:55.160 --> 00:00:57.800
You could call them the dark side of media.

00:00:57.800 --> 00:01:03.020
Though, that would make advertisements
that sell you things you probably don’t really
need the “bright side.”

00:01:03.020 --> 00:01:04.640
Let’s go with the darker side.

00:01:04.640 --> 00:01:09.080
Either way, understanding these evil twins
in their many forms is mission critical.

00:01:09.080 --> 00:01:11.439
There’s no way to be media literate without
them.

00:01:11.440 --> 00:01:14.720
But to understand evil you’ll need to think
evil.

00:01:14.720 --> 00:01:15.860
Are you ready?

00:01:15.860 --> 00:01:26.000
[Theme Music]

00:01:26.000 --> 00:01:30.640
Remember, advertisements are public notices
promoting a product, event or service.

00:01:30.640 --> 00:01:34.600
Public relations is the management of the
relationship between the public and a brand.

00:01:34.600 --> 00:01:43.140
Both advertisers and PR people use campaigns, or
planned, systematic efforts to intentionally persuade
us of certain beliefs or to act a certain way.

00:01:43.140 --> 00:01:50.300
For example, a company that makes headphones
might launch an advertising campaign where a
dozen celebrities are filmed and photographed
using their product.

00:01:50.300 --> 00:01:56.080
These may all be released at the same time
and in different locations so that everyone sees
their favorite celeb wearing them.

00:01:56.080 --> 00:01:59.940
This campaign wants you to like their product
because you like their spokespeople.

00:01:59.940 --> 00:02:04.580
Or, a public relations firm might start a
publicity campaign to get their client all
over the media.

00:02:04.580 --> 00:02:13.000
Like when your favorite actor is in a new movie
and suddenly they’re singing carpool karaoke
and dancing with The Roots and reading mean
tweets about themselves on TV.

00:02:13.000 --> 00:02:17.140
This campaign wants you to be aware that this
star has a new project coming out.

00:02:17.140 --> 00:02:19.320
Hopefully you’ll want to experience it,
too.

00:02:19.320 --> 00:02:24.300
Campaigns that saturate the media landscape
with a united theme and message can be really
effective.

00:02:24.300 --> 00:02:30.340
They can convince us to buy new phones and
stop buying cigarettes and vote for one candidate
over another in the next election.

00:02:30.340 --> 00:02:34.020
One of the key components of a campaign is
its coordination.

00:02:34.020 --> 00:02:39.620
For a campaign to have the biggest impact
requires multiple people working in tandem
to accomplish a cohesive goal.

00:02:39.620 --> 00:02:45.920
But what happens when that same technique
– the widespread coordination of people bent
on shifting the media landscape –

00:02:45.920 --> 00:02:48.680
what happens when that’s taken up for evil?

00:02:48.680 --> 00:02:50.300
That’s where propaganda comes in.

00:02:50.300 --> 00:02:56.040
Propaganda is information used to promote
a particular point of view, change behavior
or motivate action.

00:02:56.040 --> 00:03:02.040
Sometimes that information is facts and
ideas, sometimes it’s opinions, or intentionally
misleading or biased.

00:03:02.040 --> 00:03:08.040
Though technically propaganda itself isn’t inherently
evil, it is usually associated with bad actors.

00:03:08.040 --> 00:03:13.140
That’s because it’s often used to manipulate
the public into things they might not naturally do,

00:03:13.140 --> 00:03:17.200
like supporting a war or believing harmful
stereotypes about others.

00:03:17.200 --> 00:03:21.980
And most typically, the people doing the coordinated
propaganda campaigns are part of governments.

00:03:21.980 --> 00:03:29.100
During World War I, the U.S. Committee on
Public Information was formed for just such a
purpose – to produce pro-war propaganda.

00:03:29.100 --> 00:03:32.000
In World War II it was the Office of War Information.

00:03:32.000 --> 00:03:37.300
They made films and posters and advertisements
and more to promote patriotism and nationalism.

00:03:37.300 --> 00:03:42.120
The government even teamed up with advertisers
to get them to push patriotic propaganda.

00:03:42.120 --> 00:03:49.560
The propaganda focused on fulfilling one’s
national duty to join the war or save food for
the war or buy bonds to support it.

00:03:49.560 --> 00:03:52.480
It was like peer pressure with beautifully
decorated posters.

00:03:52.480 --> 00:03:56.840
That famous image of Uncle Sam saying “I
want you for the U.S. army”?

00:03:56.840 --> 00:03:58.540
Oh yeah, that’s propaganda.

00:03:58.540 --> 00:04:00.860
And it was so good they brought it back for
World War II.

00:04:00.860 --> 00:04:03.200
Rosie the Riveter?
Oh yeah, she is too.

00:04:03.200 --> 00:04:05.380
Sorry if I just ruined your favorite Halloween
costume.

00:04:05.380 --> 00:04:11.180
Other types of wartime propaganda make the
opposition seem violent or barbaric to stoke
fear in the enemy.

00:04:11.180 --> 00:04:17.000
U.S. propaganda during World War II sometimes
featured racist depictions of Japanese people
for this purpose.

00:04:17.000 --> 00:04:23.000
Similarly, in Germany the Nazi party sent
around anti-Semitic propaganda before and
during World War II.

00:04:23.000 --> 00:04:26.000
If propaganda is used to psychologically persuade,

00:04:26.000 --> 00:04:32.360
disinformation is used to confuse and distract
the intended audience using deliberately false or
misleading information.

00:04:32.360 --> 00:04:37.560
Disinformation campaigns can be used to
poke and prod opposing groups and heighten
the tension between them.

00:04:37.560 --> 00:04:44.540
And just because these campaigns aren’t being
done by official government propaganda offices
doesn’t mean they’re small scale, or ineffective.

00:04:44.540 --> 00:04:51.440
With the reach of the internet, and the ability to
make digital media, people all over the globe can
organize themselves for coordinated campaigns.

00:04:51.440 --> 00:04:58.120
By working together, flooding different media
outlets with carefully crafted messages, a group
can drastically change public information.

00:04:58.120 --> 00:05:07.080
During the 2016 U.S. election, Russian operatives
purchased misleading and extreme Facebook ads
targeted to both liberal and conservative American voters.

00:05:07.080 --> 00:05:12.140
They even appeared to organize both sides
of a protest in front of a Texas Islamic Center.

00:05:12.140 --> 00:05:16.840
So sometimes disinformation can work like
propaganda, trying to get people to act.

00:05:16.840 --> 00:05:21.200
But more often, what disinformation is best
at is confusing the facts of an issue.

00:05:21.200 --> 00:05:25.580
Disinformation can whip up a smokescreen,
and disperse the attention of the masses.

00:05:25.580 --> 00:05:30.820
This style of disinformation can also be used
to excuse or dismiss bad actions or behavior.

00:05:30.820 --> 00:05:36.660
In Beijing in 1989, students led pro-democracy
demonstrations in the capital’s Tiananmen Square.

00:05:36.660 --> 00:05:41.880
The Chinese government responded violently,
killing hundreds or even thousands of peaceful
protestors.

00:05:41.880 --> 00:05:44.100
Why do I say hundreds or thousands?

00:05:44.100 --> 00:05:47.400
Because the government stymied efforts to
make a full accounting of the dead.

00:05:47.400 --> 00:05:55.480
Since the massacre, the Chinese government has called reports of the events misleading and suggested the Western media is exaggerating it just to demonize them.

00:05:55.480 --> 00:05:57.640
They still censor information about it today.

00:05:57.640 --> 00:06:03.360
When powerful governments become set on
disinformation campaigns, it can be difficult for
its citizens to discover the truth.

00:06:03.360 --> 00:06:07.440
It can be even more difficult for outsiders
to get well-sourced information, too.

00:06:07.440 --> 00:06:11.540
Disinformation can even include magic tricks,
to – well, kind of.

00:06:11.540 --> 00:06:13.460
Let’s head into the Thought Bubble for this.

00:06:13.460 --> 00:06:18.520
Some disinformation is full of lies, like we
said – but some of it is full of distraction, too.

00:06:18.520 --> 00:06:23.340
The art of active misdirection is often used
by political pundits and celebrity press agents.

00:06:23.340 --> 00:06:29.360
They’ll plant stories in the press about their
party or client or the opposition to distract from
something they don’t want to talk about.

00:06:29.360 --> 00:06:33.920
It’s like how a magician does that funny
thing with his hand to distract you from
wherever he got that rabbit.

00:06:33.920 --> 00:06:39.820
Or take, for example, this headline:
Pope Francis Shocks World, Endorses
Donald Trump for President.

00:06:39.820 --> 00:06:41.480
That sounds kind of wild, right?

00:06:41.480 --> 00:06:45.880
The Pope never gets involved in US politics like
that – an attention-grabbing headline for sure.

00:06:45.880 --> 00:06:48.940
The thing is, this headline is purely fabricated
news.

00:06:48.940 --> 00:06:56.440
Published in July 2016 by WTOE 5 News, a
now defunct website, it was entirely made up
by an unknown writer.

00:06:56.440 --> 00:07:02.040
The site was actually part of a network
of websites that published more than 750
similarly made up articles.

00:07:02.040 --> 00:07:04.100
Why?
Who would do such a thing?

00:07:04.100 --> 00:07:12.060
Well, apparently lots of Macedonian teenagers
distracted angry, partisan American voters with
stuff like this leading up to the 2016 election.

00:07:12.060 --> 00:07:17.520
Magician Sam Sharpe actually describes this
distraction as lowering our attention vigilance.

00:07:17.520 --> 00:07:25.260
By slightly shifting our gaze to something else, we’re
lulled into an atmosphere of susceptibility, making
us more gullible to improbable situations.

00:07:25.260 --> 00:07:32.160
When we find ourselves in an atmosphere we
usually trust, like Facebook for example, we’re
less likely to question the info we find.

00:07:32.160 --> 00:07:37.760
Plus, many of us only read headlines – 59% of
links shared on social media aren’t even clicked.

00:07:37.760 --> 00:07:39.760
We just share away without a second thought.

00:07:39.760 --> 00:07:42.740
Tricking us is like taking candy from a baby,
apparently.

00:07:42.740 --> 00:07:49.780
The moral of the story: always double check
the veracity of information and sources we see,
lest we become victims of misdirection.

00:07:49.780 --> 00:07:51.120
Thanks, Thought Bubble!

00:07:51.120 --> 00:07:55.400
The key thing to understand is just how coordinated
disinformation can be today.

00:07:55.400 --> 00:08:00.940
Not just a white lie told in a forum post,
but whole networks of people working to
create an alternate reality.

00:08:00.940 --> 00:08:07.160
One of the reasons disinformation is so effective
online is because of the existence of a related
phenomenon: Misinformation.

00:08:07.160 --> 00:08:12.520
This is a different beast altogether – misinformation
is unintentionally inaccurate information.

00:08:12.520 --> 00:08:14.520
Accidents, or mistakes in reporting.

00:08:14.520 --> 00:08:19.220
Often the most egregious examples of misinformation
happen during a breaking news situation.

00:08:19.220 --> 00:08:25.920
When there’s a lot of information floating around
during a crisis and members of the media want to be
the first to report on the news, mistakes happen.

00:08:25.920 --> 00:08:28.880
They get it wrong. They don’t double check.
They make a typo.

00:08:28.880 --> 00:08:32.880
Reputable news organizations will issue a
correction when they’ve made a mistake like this.

00:08:32.880 --> 00:08:35.680
Sometimes misinformation becomes a pretty
funny story.

00:08:35.680 --> 00:08:43.720
Like that time The Chicago Tribune printed
150,000 newspapers saying that Thomas Dewey had
beat Harry S. Truman in the 1948 election.

00:08:43.720 --> 00:08:46.360
Spoiler alert: he lost.
Awkward.

00:08:46.360 --> 00:08:48.460
Misinformation has always been a problem.

00:08:48.460 --> 00:08:52.620
As long as there have been news sources, there
have been errors and corrections and updates.

00:08:52.620 --> 00:08:58.180
But our new online media environment changes
how those mistakes get made, and the impact
they have on people.

00:08:58.180 --> 00:09:04.720
Increasingly, people get information from a
variety of sources online, often shared and
mixed together over social media,

00:09:04.720 --> 00:09:07.480
rather than from a small number of
central institutions.

00:09:07.480 --> 00:09:11.720
It can make for some laughable mistakes, but
the darker side of media is no joke.

00:09:11.720 --> 00:09:16.860
We base important decisions on the media every
day, from what we’ll buy to who we’ll vote for.

00:09:16.860 --> 00:09:20.720
Bad information can lead to bad decisions
with serious consequences.

00:09:20.720 --> 00:09:26.120
Disinformation, misinformation, and propaganda
are even easier to spread in the digital age.

00:09:26.120 --> 00:09:32.760
Media literacy scholar Renee Hobbs has even
said that today, “Everyone, it seems, has become
a propagandist.”

00:09:32.760 --> 00:09:34.780
Weeding through it all can be hard to do.

00:09:34.780 --> 00:09:37.360
Especially if the initial misinformation goes
viral.

00:09:37.360 --> 00:09:44.280
Once a consumer hears or reads misinformation,
it’s often hard to correct it in their minds, even when
confronted with the right information.

00:09:44.280 --> 00:09:50.020
Plus, once we’ve deemed a source trustworthy or
safe, it’s hard for us to even criticize their content.

00:09:50.020 --> 00:09:51.500
Our brains are pretty stubborn.

00:09:51.500 --> 00:09:56.100
What’s the best way to determine if what
you’re seeing is from the darker side of media?

00:09:56.100 --> 00:10:00.720
Don’t worry, we’re going to walk you through
it in our next episode on media skills.

00:10:00.720 --> 00:10:03.900
Until then, I’m Jay Smooth and this is Crash
Course Media Literacy.

00:10:03.900 --> 00:10:08.080
Crash Course Media Literacy is filmed in the
Dr. Cheryl C. Kinney Studio in Missoula, MT.

00:10:08.080 --> 00:10:12.060
It’s made with the help of all of these nice people,
and our animation team is Thought Cafe.

00:10:12.060 --> 00:10:14.060
Crash Course is a Complexly production.

00:10:14.060 --> 00:10:20.120
If you want to imagining the world complexly
with us, check out some of our other channels like
Eons, Animal Wonders, and SciShow Psych.

00:10:20.120 --> 00:10:24.480
If you'd like to keep Crash Course free for everyone,
forever, you can support the series at Patreon,

00:10:24.480 --> 00:10:27.540
a crowdfunding platform that allows
you to support the content you love.

00:10:27.540 --> 00:10:31.620
Thank you to all of our patrons for making
Crash Course possible with their continued
support.

