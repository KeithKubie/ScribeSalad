1
00:00:00,290 --> 00:00:04,190
[inaudible]

2
00:00:04,430 --> 00:00:09,050
joining me today is the author of the
New York Times best seller rise of the

3
00:00:09,051 --> 00:00:13,220
robots technology and the threat
of a jobless future. Martin Ford.

4
00:00:13,221 --> 00:00:16,670
Welcome to the Rubin report. Thanks for
having me. I'm glad to have you here sir.

5
00:00:16,671 --> 00:00:20,540
Because dystopian futures, robots, skynet,

6
00:00:20,630 --> 00:00:24,260
all of it very much in my wheelhouse.
And I want you to explain it all to me.

7
00:00:24,261 --> 00:00:27,370
Are you ready? Yes, definitely.
All right, let's do it. So, uh,

8
00:00:27,410 --> 00:00:31,340
first off before we dive directly
into robots and AI and all that, uh,

9
00:00:31,370 --> 00:00:33,170
just tell me a little bit
about your background.

10
00:00:33,171 --> 00:00:35,140
What brought you to writing a book?
Like?

11
00:00:35,210 --> 00:00:35,720
Okay,

12
00:00:35,720 --> 00:00:39,790
so I studied computer engineering in
college and then I worked as an engineer,

13
00:00:39,800 --> 00:00:43,460
design engineer for several years. Um,
then I went back and studied business.

14
00:00:43,461 --> 00:00:47,180
Eventually I ended up starting and
running a small software company up in

15
00:00:47,181 --> 00:00:50,270
Silicon Valley.
And I ran that for many years.

16
00:00:50,271 --> 00:00:51,710
But even in the course of running that,

17
00:00:51,711 --> 00:00:55,010
I saw the impact that all
this technology was having on,

18
00:00:55,040 --> 00:00:57,710
on jobs at my business
and businesses like it.

19
00:00:58,100 --> 00:00:59,870
And that really got me
thinking about this issue.

20
00:00:59,871 --> 00:01:04,400
And so about 10 years ago in 20, um, 2009,

21
00:01:04,401 --> 00:01:07,790
I wrote, uh, my first book
called delights in the tunnel,

22
00:01:07,791 --> 00:01:11,660
which really argued that artificial
intelligence was going to be the next big

23
00:01:11,661 --> 00:01:15,260
thing in computing and that it was going
to have a dramatic impact in particular

24
00:01:15,261 --> 00:01:16,370
on the job market.

25
00:01:16,371 --> 00:01:19,970
And that book did well enough that it
led to an opportunity to write this book

26
00:01:19,971 --> 00:01:23,330
in 2015 which really got, you
know, quite a bit of attention.

27
00:01:23,331 --> 00:01:28,331
And since then I've kind of shifted my
career to really be a futurist focused on

28
00:01:29,300 --> 00:01:33,860
what AI and robotics means for society
and for the economy and especially for

29
00:01:33,861 --> 00:01:37,610
the job market. So I think they're going
to be some huge challenges there for us.

30
00:01:37,710 --> 00:01:40,050
Right. So we're going to
unpack all of that stuff.

31
00:01:40,051 --> 00:01:44,100
But when you were writing about this
in 2009 where people say are now,

32
00:01:44,101 --> 00:01:45,750
this is just pure science.

33
00:01:45,840 --> 00:01:50,430
Yeah, I mean, I, I, this was initiative
is very much off the radar back then. Um,

34
00:01:50,610 --> 00:01:53,130
it came with a fair amount
of stigma and stigma.

35
00:01:53,131 --> 00:01:58,131
And the reason is that this concern or
fear that machines might take a lot of

36
00:01:58,561 --> 00:02:01,410
jobs and there might be unemployment,
it's, it's an old issue,

37
00:02:01,411 --> 00:02:05,040
has come up many times in the past
going all the way back to the Luddites,

38
00:02:05,100 --> 00:02:06,780
right in England 200 years ago.

39
00:02:07,050 --> 00:02:09,470
And so there's actually
this term neo luddite for,

40
00:02:09,490 --> 00:02:13,290
for a person that that is once
again worrying about this issue.

41
00:02:13,291 --> 00:02:18,030
And so it was quite stigmatized. Um,
so in 2009 when I wrote the book,

42
00:02:18,060 --> 00:02:21,630
um, you know, I was one of
the, the earliest people
to get out there with this.

43
00:02:21,690 --> 00:02:26,430
But since then things have definitely
changed and I see a lot of people much

44
00:02:26,431 --> 00:02:29,550
more concerned about this even
professional economists and so forth.

45
00:02:29,551 --> 00:02:34,140
So there definitely has been a shift in
mentality over this last 10 years when

46
00:02:34,141 --> 00:02:36,310
we've seen things like,
like the advent of,

47
00:02:36,340 --> 00:02:39,510
of self driving cars that
look like they're going to
be arriving soon. So forth.

48
00:02:40,090 --> 00:02:40,661
What were,

49
00:02:40,661 --> 00:02:45,010
what markers where you seeing back in
2009 or a little bit before that even that

50
00:02:45,011 --> 00:02:46,870
were sort of pushing you in.

51
00:02:47,460 --> 00:02:50,010
But the most important thing is
what you might call Moore's law.

52
00:02:50,011 --> 00:02:54,570
The fact that that the power of computers
is accelerating doubling every two

53
00:02:54,571 --> 00:02:54,871
years.

54
00:02:54,871 --> 00:02:59,100
And it was obvious that computers were
going to get dramatically more powerful

55
00:02:59,650 --> 00:03:01,450
and there had to be an
application for that.

56
00:03:01,451 --> 00:03:02,920
It has to be something
you can do with that.

57
00:03:02,921 --> 00:03:06,250
And it became obvious to me
that artificial intelligence
was going to be the

58
00:03:06,251 --> 00:03:07,084
thing.
And,

59
00:03:07,210 --> 00:03:12,070
and AI means essentially solving the same
kinds of problems that the human brain

60
00:03:12,071 --> 00:03:12,761
can solve,
right?

61
00:03:12,761 --> 00:03:17,410
And it means machines that in a limited
sense are taking on cognitive capability

62
00:03:17,411 --> 00:03:18,340
to beginning to think.

63
00:03:18,760 --> 00:03:23,080
And that means that technology is going
to begin to compete with and substitute

64
00:03:23,260 --> 00:03:27,250
for human being a human beings in,
in a unique way.

65
00:03:27,270 --> 00:03:28,870
Something that we've never seen before.

66
00:03:29,230 --> 00:03:33,310
And is that scales across the whole
economy has all kinds of jobs,

67
00:03:33,311 --> 00:03:35,970
skilled jobs and unskilled
jobs. I think that, um,

68
00:03:36,520 --> 00:03:38,620
it becomes pretty clear that the,
you know,

69
00:03:38,621 --> 00:03:40,480
it's going to have dramatic implications.

70
00:03:40,620 --> 00:03:44,040
So when people think about robots,
I think like there's a,

71
00:03:44,041 --> 00:03:45,930
there's a few different
ways you can think about it,

72
00:03:45,931 --> 00:03:47,310
that you can sort of think about AI,

73
00:03:47,311 --> 00:03:51,250
which is sort of the same morphous thing
that people sort of don't contextualize

74
00:03:51,251 --> 00:03:53,940
it to a physical object. Then they think
of robots. They think of like, you know,

75
00:03:54,000 --> 00:03:57,270
c three Po and R Two d two
and everything else. Um,

76
00:03:58,230 --> 00:03:59,063
what

77
00:03:59,310 --> 00:04:02,700
if you were just saying robots,
what exactly are you

78
00:04:03,180 --> 00:04:07,010
right? I, especially in this
book, rise of the robust,

79
00:04:07,011 --> 00:04:11,150
I used a very broad meaning for that.
I stay to mean anything that is,

80
00:04:11,210 --> 00:04:14,390
is automating something and taking over,
you know, things that people can do.

81
00:04:14,391 --> 00:04:16,750
And very often that's just
going to be software. Um,

82
00:04:16,910 --> 00:04:18,590
if you want to be more
precise and technical.

83
00:04:18,591 --> 00:04:23,090
A robot is when you take
artificial intelligence and
you put it into a physical

84
00:04:23,091 --> 00:04:25,820
machine that can physically
manipulate the environment.

85
00:04:26,270 --> 00:04:29,680
But what we're talking about is much
broader than that. Um, so we're gonna,

86
00:04:29,770 --> 00:04:30,320
you know,
we're,

87
00:04:30,320 --> 00:04:34,700
we're already seeing people like lawyers
and doctors being impacted and it's not

88
00:04:34,701 --> 00:04:38,370
physical robots. It's often just
software, artificial intelligence. Um,

89
00:04:38,371 --> 00:04:39,230
and actually the,
you know,

90
00:04:39,260 --> 00:04:43,760
building physical robots
that have dexterity that can
manipulate the environment.

91
00:04:43,761 --> 00:04:45,860
That's actually one of the
hardest aspects of this.

92
00:04:46,280 --> 00:04:50,510
And in some ways that where it may be
where progress is going to be slowest,

93
00:04:50,990 --> 00:04:53,210
where in knowledge type work,
you know,

94
00:04:53,211 --> 00:04:56,780
someone that's sitting in front of
a computer doing some routine tasks,

95
00:04:56,781 --> 00:04:59,330
cranking out the same
report again and again,

96
00:04:59,331 --> 00:05:03,680
that may actually be much easier to
automate them then something physical.

97
00:05:03,760 --> 00:05:04,840
So That's interesting.
So the,

98
00:05:04,841 --> 00:05:08,470
the idea part of it is easier to
replicate than the physical part,

99
00:05:08,471 --> 00:05:11,740
even though you'd think that just
building a robot that can move the way you

100
00:05:11,741 --> 00:05:16,180
want it to move or something that seems
technically easier than figuring out how

101
00:05:16,181 --> 00:05:17,080
to think like humans.

102
00:05:17,290 --> 00:05:21,700
It seems like that from our perspective
very often the reason is that to do

103
00:05:21,701 --> 00:05:26,230
these knowledge based jobs requires a
lot of education and training, right?

104
00:05:26,290 --> 00:05:30,190
Um, but actually once you
implement the technology,

105
00:05:30,191 --> 00:05:32,140
it actually can often be the reverse.

106
00:05:32,150 --> 00:05:35,950
The hardest thing to do is to build
a physical robot that has dexterity,

107
00:05:35,951 --> 00:05:37,360
that has visual perception,

108
00:05:37,361 --> 00:05:41,200
that can can move around the way a person
does it take to build, as you said,

109
00:05:41,201 --> 00:05:45,430
to kind of robot like c three Po from
Star Trek. That's totally science fiction.

110
00:05:45,431 --> 00:05:48,050
We don't have anything remotely like that.

111
00:05:48,090 --> 00:05:50,730
It seems like we're caught every now
and again you'll see a video on youtube

112
00:05:50,731 --> 00:05:53,400
where they're getting a little closer.
You know, they've got a robot, you know,

113
00:05:53,401 --> 00:05:56,110
jumping over something and
ducking under something. Exactly.

114
00:05:56,110 --> 00:05:59,640
You see those robots in particular from
a company called Boston dynamics is

115
00:05:59,641 --> 00:06:03,260
doing very impressive things,
but those videos are highly choreographed.

116
00:06:04,010 --> 00:06:07,100
The robots are controlled by
someone that's outside the picture.

117
00:06:07,370 --> 00:06:11,330
This is not a thinking autonomous robot
running around doing stuff by itself.

118
00:06:11,360 --> 00:06:14,090
Okay.
Terminator land just yet.

119
00:06:14,170 --> 00:06:18,910
Not, not anytime soon at all. That's
far in the future, but that, you know,

120
00:06:18,940 --> 00:06:21,010
we shouldn't allow,

121
00:06:21,580 --> 00:06:24,550
we shouldn't be distracted from the fact
that the are things happening now that

122
00:06:24,551 --> 00:06:25,990
are going to have a
really dramatic impact,

123
00:06:25,991 --> 00:06:29,750
but it's not the science fiction stuff
that you see in the I robot movie and,

124
00:06:29,751 --> 00:06:30,510
and stuff like.

125
00:06:30,510 --> 00:06:34,560
Right. So how much of the conversation
is about all of this is about what you

126
00:06:34,561 --> 00:06:39,000
referenced a moment ago about just the
speed of technology and that every two

127
00:06:39,001 --> 00:06:40,740
years the power doubles and all that,

128
00:06:40,741 --> 00:06:44,080
that we're all walking around
with iPhones or cell, you know,

129
00:06:44,130 --> 00:06:48,090
we have super computers in our pocket
and the way we can transmit information

130
00:06:48,091 --> 00:06:52,740
across the globe like that. And just how
much of this is just related to speed

131
00:06:53,100 --> 00:06:55,850
more than,
I know that's a very big part of it.

132
00:06:55,851 --> 00:06:59,310
It is not just the speed of computers
that have gotten faster and smaller of

133
00:06:59,311 --> 00:07:03,510
course, and now during our iPhones, but,
um, it's the speed of communications,

134
00:07:03,511 --> 00:07:05,490
bandwidth,
it's memory capacity.

135
00:07:05,730 --> 00:07:09,870
So we've seen this very
broad based acceleration in
technology and that's a huge

136
00:07:09,871 --> 00:07:11,190
part of it.
Um,

137
00:07:11,220 --> 00:07:15,120
the other things is that there have
been some breakthroughs in artificial

138
00:07:15,121 --> 00:07:18,150
intelligence,
especially in the hottest area of Ai,

139
00:07:18,151 --> 00:07:20,580
which is called deep learning
or deep neural networks.

140
00:07:20,581 --> 00:07:23,820
We've seen dramatic progress there
and that's the thing that's really

141
00:07:23,821 --> 00:07:27,430
revolutionizing the field.
And,

142
00:07:27,431 --> 00:07:30,210
and the other thing that's happened is
that we are now throughout our whole

143
00:07:30,211 --> 00:07:34,410
economy and society collecting
huge amounts of data, right?

144
00:07:34,411 --> 00:07:37,140
There's all this data out
there that wasn't there before.

145
00:07:37,141 --> 00:07:42,141
And this data is basically the resource
that is used to train these smart

146
00:07:42,721 --> 00:07:46,170
algorithms. And that's what artificial
intelligence looks like right now.

147
00:07:46,171 --> 00:07:48,810
It's primarily machine learning.
Um,

148
00:07:48,830 --> 00:07:50,850
and then he's just going to
be incredibly disruptive.

149
00:07:50,930 --> 00:07:51,171
Yeah.

150
00:07:51,171 --> 00:07:56,171
So as part of the potential problem here
that we're building things that will be

151
00:07:56,721 --> 00:07:59,720
more powerful than us and we
don't really understand that.

152
00:07:59,721 --> 00:08:03,410
So it's like we're putting so much
information in our brains all the time.

153
00:08:03,411 --> 00:08:07,100
Maybe our actual physical
brains can't take all this in.

154
00:08:07,130 --> 00:08:11,030
Like we don't have enough ram and our
physical brains for all of the information

155
00:08:11,031 --> 00:08:12,650
that we're constantly slamming ourselves.

156
00:08:13,010 --> 00:08:16,620
Well, it is definitely true that these
smart algorithms, I mean they can look at,

157
00:08:16,950 --> 00:08:21,600
you know, huge amounts of data, millions
and millions and millions of data points,

158
00:08:21,601 --> 00:08:23,520
right?
Which no human being could do.

159
00:08:23,521 --> 00:08:28,521
So we already have our rhythms that in
a very narrow sense in terms of doing

160
00:08:28,561 --> 00:08:32,970
very specific things are superhuman,
right? They can vastly outperform,

161
00:08:33,430 --> 00:08:35,250
um, want any person does. Um,

162
00:08:35,580 --> 00:08:37,370
and they do things that we
don't really understand it.

163
00:08:37,380 --> 00:08:39,060
Good example of that would be Wall Street,
right?

164
00:08:39,061 --> 00:08:44,061
Where you've got these trading algorithms
that can actually look at machine

165
00:08:44,491 --> 00:08:45,570
readable news.
I mean,

166
00:08:45,780 --> 00:08:50,070
companies like Bloomberg actually make
a news products that are designed for

167
00:08:50,071 --> 00:08:51,240
machines,
not for people.

168
00:08:51,660 --> 00:08:56,160
These algorithms can read that news and
then analyze it and then on it within,

169
00:08:56,161 --> 00:08:59,310
you know, tiny fractions of a second.
So that would be an example of,

170
00:08:59,311 --> 00:09:03,630
of where technology is already getting
ahead of what we can understand. Uh,

171
00:09:03,780 --> 00:09:08,080
what do we do to rein some of it back
in occasionally? Well, I, you know,

172
00:09:08,081 --> 00:09:11,850
if there are any lower or is it just
once we start the process with anything

173
00:09:12,060 --> 00:09:16,380
like this, we just don't know where
it is. It's a difficult question. Um,

174
00:09:17,310 --> 00:09:20,700
you know, they're going to be places
where we're going to need regulation. Um,

175
00:09:20,790 --> 00:09:23,850
you can't just rein it in. I mean
it's progress. It's happening.

176
00:09:23,851 --> 00:09:27,840
It says happening in part because
of a competitive dynamic, right?

177
00:09:27,841 --> 00:09:29,580
Within capitalism between companies,

178
00:09:29,581 --> 00:09:34,500
between Google and Facebook and Goldman
Sachs all competing to build, um,

179
00:09:34,530 --> 00:09:35,580
the latest technology.

180
00:09:35,581 --> 00:09:39,350
There is also a competition between
United States and China. Um,

181
00:09:39,440 --> 00:09:44,070
all of that is going to push it forward
relentlessly and trying to stop it. Um,

182
00:09:44,071 --> 00:09:47,670
it's probably kind of a fool's errand.
Um, it's probably not possible and,

183
00:09:47,671 --> 00:09:50,790
and probably not,
not really advisable.

184
00:09:50,791 --> 00:09:55,560
What I think we have to do are find
ways to adapt to all of this progress.

185
00:09:55,950 --> 00:09:59,370
And in some places that may
mean certainly regulation. Um,

186
00:09:59,700 --> 00:10:04,500
and in other cases it will mean finding
ways to address issues like unemployment

187
00:10:04,501 --> 00:10:08,310
in inequality that will, um,
result from all of this progress.

188
00:10:08,640 --> 00:10:12,210
So let's just define some basic terms
cause I think we ended up throwing out a

189
00:10:12,211 --> 00:10:13,800
lot of big terms here and
then people are confused.

190
00:10:13,801 --> 00:10:16,530
So just when people are
talking about the algorithm,

191
00:10:16,531 --> 00:10:21,120
can you just explain in simple terms
what is the algorithm? Well and that was,

192
00:10:21,121 --> 00:10:22,770
we're doing this on youtube right now,
right?

193
00:10:22,980 --> 00:10:25,280
We're always obsessed with
the algorithm and algorithm.

194
00:10:25,290 --> 00:10:27,060
It's just essentially a computer program.

195
00:10:27,061 --> 00:10:31,260
It's something that goes step
by step and does something. Um,

196
00:10:31,290 --> 00:10:36,090
what we've seen recently though is the
emergence of a new kind of algorithm

197
00:10:36,091 --> 00:10:37,740
called machine learning algorithms.

198
00:10:37,741 --> 00:10:41,160
And this is what's really disruptive
and the difference between a machine

199
00:10:41,161 --> 00:10:45,800
learning algorithm and a traditional
computer programming a algorithm is that,

200
00:10:45,870 --> 00:10:47,940
you know,
historically some programmer has,

201
00:10:48,030 --> 00:10:53,030
has sat down and told the computer to
do what to do step by step with machine

202
00:10:53,611 --> 00:10:54,030
learning.

203
00:10:54,030 --> 00:10:58,440
Instead you've got a smart algorithm
that looks at lots and lots of data and

204
00:10:58,441 --> 00:11:02,510
then figures out for itself what to do.
So in essence, it's kind of programming,

205
00:11:02,511 --> 00:11:05,370
it's programming itself,
right? Yeah. Um, so,

206
00:11:05,420 --> 00:11:07,860
so is there a way to control it then?

207
00:11:08,430 --> 00:11:11,370
Well you his interest actually
uncontrollable because once,

208
00:11:11,400 --> 00:11:15,020
once it's learned enough it
just doesn't need the player.

209
00:11:15,050 --> 00:11:19,110
It's not so much that it's
uncontrollable, but that, you know,

210
00:11:19,111 --> 00:11:20,450
we don't really want to control it.

211
00:11:20,460 --> 00:11:25,460
The whole point is to to unleash
it and let it learn and do things.

212
00:11:26,041 --> 00:11:29,160
That doesn't mean that it's in any sense
out of control or it's a danger to us

213
00:11:29,161 --> 00:11:29,641
or anything like,

214
00:11:29,641 --> 00:11:33,960
well the reason I was asking was sort of
through through Youtube Algorithm Lens.

215
00:11:33,961 --> 00:11:37,110
Like one of the things we're finding out
is they just want to keep you clicking

216
00:11:37,111 --> 00:11:40,620
all the time. You know where
we put out a long form show,

217
00:11:40,621 --> 00:11:42,750
people are going to watch a
full hour of our discussion.

218
00:11:42,930 --> 00:11:45,390
That's not really what
the algorithm wants.

219
00:11:45,391 --> 00:11:48,510
It wants you from the way we
understand it from some insiders,

220
00:11:48,511 --> 00:11:52,230
they want you to constantly be clicking
on videos and basically fall into this

221
00:11:52,231 --> 00:11:54,100
click hole to just keep the machine

222
00:11:54,100 --> 00:11:55,180
going.
More and more and more.

223
00:11:55,390 --> 00:11:59,620
Now I understand why they want that sort
of attention going in different places

224
00:11:59,621 --> 00:12:02,980
and all that. Um, but for what I do,

225
00:12:03,010 --> 00:12:04,900
I don't love the algorithm at the moment,

226
00:12:05,400 --> 00:12:07,880
right? Right. So that depends on
how they optimize the algorithm.

227
00:12:07,881 --> 00:12:10,460
But what's happening now is that you've
got millions and millions of people

228
00:12:11,030 --> 00:12:15,230
watching youtube videos.
And if they watch an entire video,

229
00:12:15,231 --> 00:12:17,270
then that will create a data
point that says, you know,

230
00:12:17,271 --> 00:12:21,170
they were really interested in this. If
they, if they start watching and you know,

231
00:12:21,230 --> 00:12:22,850
for a brief time and then they click away,

232
00:12:22,851 --> 00:12:26,300
then it'll show that they're
less interested and then
an algorithm comes around

233
00:12:26,301 --> 00:12:30,710
and looks at millions of those data
points and can make recommendations for

234
00:12:30,711 --> 00:12:32,150
other videos.
And as you said,

235
00:12:32,151 --> 00:12:37,151
I think what we've seen is that the video
shown to people become more extreme,

236
00:12:38,151 --> 00:12:38,391
right?

237
00:12:38,391 --> 00:12:41,300
So if you're interested in something and
then you'll get a more extreme version

238
00:12:41,301 --> 00:12:44,300
of that. And that's how to get people
to Click. And a lot of people have been,

239
00:12:44,860 --> 00:12:45,080
you know,

240
00:12:45,080 --> 00:12:48,710
raising the alarm over that because that
is kind of radicalizing people, right?

241
00:12:49,300 --> 00:12:50,620
So what do you do about that?
If you're,

242
00:12:50,621 --> 00:12:54,190
if you're a programmer and you're at
youtube and you don't want people to be

243
00:12:54,191 --> 00:12:55,660
radicalized or just,
you don't,

244
00:12:55,870 --> 00:12:58,510
even if you just don't want people to
have to just be endlessly clicking,

245
00:12:58,511 --> 00:13:01,720
like there's this game to keep people
addicted to all of these things.

246
00:13:01,960 --> 00:13:05,230
And it's like, I understand that we
could put out a zillion clips so I could,

247
00:13:05,440 --> 00:13:05,621
you know,

248
00:13:05,621 --> 00:13:08,260
I could chop everything into two minute
things and we could put them out and it

249
00:13:08,261 --> 00:13:12,040
would probably help us in terms of
views and all of those things. Um,

250
00:13:12,280 --> 00:13:14,020
but I just don't want
to play that game too.

251
00:13:14,090 --> 00:13:14,630
Right,
right.

252
00:13:14,630 --> 00:13:17,210
Technically I don't think
that's a difficult problem
that that depends on what

253
00:13:17,211 --> 00:13:18,450
the designer wants to do that.

254
00:13:18,470 --> 00:13:23,470
But the whole problem is that
the algorithms are designed
to make the most money

255
00:13:23,781 --> 00:13:27,140
for Google. Right? Right. And that's
what's driving that. So it's not a,

256
00:13:27,141 --> 00:13:30,790
I don't think it's a computer
design problem. It's,

257
00:13:30,840 --> 00:13:33,560
it's a capitalism problem,
a profitability problem.

258
00:13:33,561 --> 00:13:38,370
It's the fact that that Google is a
publicly traded companies. It, you know,

259
00:13:38,390 --> 00:13:41,570
it's, investors want it to
make as much money as possible.

260
00:13:41,571 --> 00:13:46,571
And that's what drives it to design
algorithms that maximize profitability.

261
00:13:46,641 --> 00:13:47,760
So keep your clicker.

262
00:13:47,770 --> 00:13:51,950
That may be the kind of place where maybe
some regulation has to come in and say,

263
00:13:51,951 --> 00:13:55,760
you know, well, you're going to have to
put some constraints on this. Um, if,

264
00:13:55,770 --> 00:13:58,490
if Google doesn't make the
decision to do that itself.

265
00:13:58,580 --> 00:14:01,280
Yes. I mean this is where
I'm not a regulation guy,

266
00:14:01,281 --> 00:14:04,100
but it's like they're pushing
me to my limits. I mean,

267
00:14:04,101 --> 00:14:07,100
this is what I keep saying about all the
tech companies right now when it comes

268
00:14:07,101 --> 00:14:09,470
to censorship and everything else,

269
00:14:09,471 --> 00:14:12,200
it's like they're not giving
us much of a choice here.

270
00:14:12,670 --> 00:14:17,050
Right? I mean, it's a challenging problem
for sure too, to figure out, you know,

271
00:14:18,070 --> 00:14:21,550
for AI to figure out what's in the
video and in terms of is there something

272
00:14:21,551 --> 00:14:24,400
there, then it should be censored or
not. That's the decision they're making.

273
00:14:24,730 --> 00:14:27,340
And that's, that's quite
different from just optimizing,

274
00:14:27,341 --> 00:14:30,490
getting people to watch videos,
because in order to do that, you know,

275
00:14:30,520 --> 00:14:32,800
you don't care what's in the video.
That's the whole problem. Right? Right.

276
00:14:33,100 --> 00:14:34,630
It's just tracking statistics.

277
00:14:34,631 --> 00:14:37,000
But if you actually want to
analyze what's in the video,

278
00:14:37,300 --> 00:14:40,720
is there something in there it dangerous
or you inciting people to violence or

279
00:14:40,721 --> 00:14:43,930
something like that for artificial
intelligence to figure that out,

280
00:14:43,931 --> 00:14:48,670
it's still much, much harder. And that's
why we're running into this problem.

281
00:14:48,671 --> 00:14:49,450
I think.

282
00:14:49,450 --> 00:14:52,340
How is deep learning different than
artificial intelligence? That's, yeah,

283
00:14:52,370 --> 00:14:53,920
that's just the next level of order.

284
00:14:53,930 --> 00:14:55,820
Official intelligence facing is a kind of,

285
00:14:56,030 --> 00:15:00,860
of artificial intelligence is right now
the hottest area of AI and deep learning

286
00:15:00,861 --> 00:15:05,861
or deep neural networks basically means
a system that is loosely designed on the

287
00:15:05,961 --> 00:15:06,890
way a brain would work.

288
00:15:06,891 --> 00:15:11,420
It has artificial neurons that are
roughly similar to to the neurons in your

289
00:15:11,421 --> 00:15:15,380
brain. And that's the way it works. Um,
and this is an idea that's been around,

290
00:15:15,470 --> 00:15:18,740
you know, for, since
the 1950s at least. Um,

291
00:15:18,770 --> 00:15:21,530
but just within the last six years or so,

292
00:15:21,531 --> 00:15:24,730
we've seen just an explosion
in this technology. Um,

293
00:15:25,220 --> 00:15:29,130
and we've now got systems that
can translate languages, um,

294
00:15:29,180 --> 00:15:33,890
from Chinese to English that can do
better than people at recognizing visual

295
00:15:33,891 --> 00:15:34,341
images.

296
00:15:34,341 --> 00:15:38,960
We've got radiology systems that can
look at medical images and find cancer

297
00:15:38,961 --> 00:15:42,020
there and in some cases can do
that better than human doctors.

298
00:15:42,140 --> 00:15:46,850
So this is absolutely, absolutely
the hardest area of Ai. It's, um,

299
00:15:46,880 --> 00:15:50,960
also what's enabling self
driving cars, for example. Um,

300
00:15:51,160 --> 00:15:53,650
so is this the great catch 22 of,

301
00:15:53,651 --> 00:15:57,190
of all of robotics is that it's doing
these incredible things and then as you

302
00:15:57,191 --> 00:15:59,560
talk about in the book, it's going
to put a lot of people out of work.

303
00:15:59,840 --> 00:16:04,350
Well, I think that's one of the real
problems with it. I mean, and you know,

304
00:16:04,370 --> 00:16:08,330
we're ultimately going to have to make
a choice as to whether we want to allow

305
00:16:08,331 --> 00:16:13,100
that progress to continue and get the
enormous benefits of that progress.

306
00:16:13,670 --> 00:16:15,650
But if that's going to come at a cost of,

307
00:16:15,651 --> 00:16:19,210
of making some set of our
population unemployable or,

308
00:16:19,211 --> 00:16:23,930
or maybe deskilling jobs to the point
where people just don't have adequate

309
00:16:23,931 --> 00:16:26,750
incomes, even if they don't,
even if they do have a job, um,

310
00:16:26,960 --> 00:16:29,330
then we've got to find a way to adapt to
that. Right? And that's why, for example,

311
00:16:29,331 --> 00:16:32,960
I've talked a lot about a universal
basic income as one possible approach to

312
00:16:32,961 --> 00:16:34,130
that.
Um,

313
00:16:34,160 --> 00:16:37,910
but I'm very much against the idea that
we should stop progress because this is

314
00:16:37,911 --> 00:16:38,720
where we are,
you know,

315
00:16:38,720 --> 00:16:42,290
this is what progress is going to look
like in the future and we don't want to

316
00:16:42,291 --> 00:16:45,900
stop it because progress is the thing
that is made us better off over.

317
00:16:46,030 --> 00:16:48,400
Is there any evidence that
you are ever in history?

318
00:16:48,401 --> 00:16:51,810
You could stop progress actually. And
even if we want it to stop it right now,

319
00:16:51,820 --> 00:16:54,700
let's say you laid out the greatest case
why this thing is going to run out of

320
00:16:54,701 --> 00:16:57,390
control. It's going to put half
of us out of business, you know,

321
00:16:57,530 --> 00:17:01,180
income inequality is going to go
crazy, poverty, et cetera, et cetera.

322
00:17:01,480 --> 00:17:05,740
Is there any case where technology
existed that we somehow put the,

323
00:17:05,920 --> 00:17:06,760
put the brakes on it?

324
00:17:07,170 --> 00:17:10,740
I'd be quite skeptical that we'd
be able to do that. Um, again,

325
00:17:10,741 --> 00:17:12,620
in part because of competition,

326
00:17:12,690 --> 00:17:17,130
not just between companies but between
countries. Maybe we would do it,

327
00:17:17,131 --> 00:17:21,390
but then China didn't put the brakes
on and they would pretty soon be vastly

328
00:17:21,391 --> 00:17:22,890
ahead of us. Right. So
that would be a problem.

329
00:17:23,010 --> 00:17:26,520
Is that the catch 22 then for regulation
because it's like we may try to

330
00:17:26,521 --> 00:17:29,590
regulate some of it,
but if China's not regulating it or,

331
00:17:29,890 --> 00:17:33,500
exactly. Yeah. One of that's one of
the biggest problems is that you would,

332
00:17:33,520 --> 00:17:37,780
you would put your country
at a disadvantage unless
you could do it on a global

333
00:17:37,781 --> 00:17:38,650
basis.
And of course,

334
00:17:38,651 --> 00:17:42,670
doing anything on a global basis is
incredibly hard as you see with climate

335
00:17:42,671 --> 00:17:45,190
change, for example. Um, so again,

336
00:17:45,191 --> 00:17:48,430
my perspective is that rather
than trying to slow it down,

337
00:17:49,530 --> 00:17:53,010
what we should do is find a way to
adapt to a ledge, just let it run,

338
00:17:53,820 --> 00:17:57,120
but understand what the implications
are going to be and figure out a way to

339
00:17:57,121 --> 00:18:00,780
adapt to that. And that's where the
idea of a basic income comes in.

340
00:18:00,840 --> 00:18:03,900
So I want to talk a little bit more
about ubi, but before we do that,

341
00:18:03,901 --> 00:18:06,990
can you talk a little bit just about
how this has affected certain industries

342
00:18:06,991 --> 00:18:10,950
and how some industries haven't quite
been affected yet? Right. So in general,

343
00:18:10,951 --> 00:18:13,470
the point that I'd make is that it's
going to affect everything. I mean,

344
00:18:13,471 --> 00:18:15,630
artificial intelligence is
going to be like a utility.

345
00:18:15,631 --> 00:18:17,610
It's going to be like electricity,
right?

346
00:18:17,611 --> 00:18:21,870
And no one says what industries are most
impacted by electricity, right? I mean,

347
00:18:21,880 --> 00:18:24,000
you know, everything relies
on electricity, right?

348
00:18:24,001 --> 00:18:28,710
And the same will be true of artificial
intelligence and machine learning. So, um,

349
00:18:28,740 --> 00:18:33,030
in the long run, it's everywhere. Um,
in the near term, clearly, you know,

350
00:18:33,031 --> 00:18:36,960
manufacturing has already been
impacted by automation. Generally.

351
00:18:36,961 --> 00:18:41,040
We've seen a dramatic decrease in advanced
countries and the number of people

352
00:18:41,310 --> 00:18:44,160
employed in manufacturing, and
that's going to continue, uh,

353
00:18:44,190 --> 00:18:47,370
the robots and the automation used in
factories are going to get a lot more

354
00:18:47,371 --> 00:18:50,730
effective, more dexterous. There'll
be able to do the jobs that,

355
00:18:50,731 --> 00:18:54,570
that right now only people can do.
Um, but it's gonna scale to many,

356
00:18:54,571 --> 00:18:58,160
many other areas in finance. Um,
it's going to have a dramatic impact.

357
00:18:58,180 --> 00:19:02,280
A lot of white collar jobs there where
people are sitting in front of a computer

358
00:19:02,310 --> 00:19:04,500
cracking out reports or
something. Right? Um,

359
00:19:05,250 --> 00:19:08,220
recently I saw something that
the CEO of a Deutscha Bank,

360
00:19:08,221 --> 00:19:12,300
one of the big bank said he thought he
could get rid of half of his employees in

361
00:19:12,301 --> 00:19:16,800
the relatively near future
using using this technology. Um,

362
00:19:16,830 --> 00:19:17,441
healthcare is,

363
00:19:17,441 --> 00:19:21,420
and it's an area where it's going to
be slower because it's really hard.

364
00:19:21,450 --> 00:19:26,450
You've got doctors and nurses that need
to engage with patients on a one on one

365
00:19:27,331 --> 00:19:31,740
basis, right? And provide a lot
of individual human like service.

366
00:19:32,190 --> 00:19:33,600
Um,
and it's been,

367
00:19:33,660 --> 00:19:37,080
and that's one of the reasons that
healthcare costs are so high in the United

368
00:19:37,080 --> 00:19:41,580
States right now because we have not
seen the kind of productivity increases

369
00:19:41,581 --> 00:19:45,090
there that we've seen in
say manufacturing. So what,

370
00:19:45,310 --> 00:19:47,430
what could we do to see that we're,
we're,

371
00:19:47,450 --> 00:19:50,100
we're beginning to see evidence of that.
As I mentioned,

372
00:19:50,101 --> 00:19:52,380
you've got systems now that
can read medical images.

373
00:19:52,381 --> 00:19:54,880
So you're going to begin to see,
uh,

374
00:19:54,930 --> 00:19:57,210
the introduction of artificial
intelligence in medicine.

375
00:19:57,211 --> 00:19:59,940
I don't think that it will for the
most part, at least in the near term,

376
00:19:59,941 --> 00:20:02,160
it's not going to
completely replace doctors,

377
00:20:02,550 --> 00:20:05,910
but it will become kind
of a second opinion. Um,

378
00:20:05,911 --> 00:20:09,950
it will run alongside a doctor, you know,
we'll always be there. We will make, um,

379
00:20:10,410 --> 00:20:14,610
every doctor be able to perform at the
level of the best doctor, right? Because,

380
00:20:14,611 --> 00:20:16,710
because there'll be this
incredible intelligence there.

381
00:20:16,980 --> 00:20:19,410
So that will be an enormous benefit.
Um,

382
00:20:19,650 --> 00:20:22,410
there are lower and then eventually if
you just extrapolate that down the road,

383
00:20:22,411 --> 00:20:24,370
he could replace the
doctor too. Right? I mean,

384
00:20:25,150 --> 00:20:29,610
I like in the movie alien in a
million other things. Exactly. I'm,

385
00:20:29,611 --> 00:20:31,170
although I would say in general,

386
00:20:31,230 --> 00:20:34,290
doctors are relatively safe because
they are highly regulated, right?

387
00:20:34,290 --> 00:20:38,400
There are all kinds of rules about
medicine and you need to have a doctor or,

388
00:20:38,420 --> 00:20:42,240
or a pharmacist there and, and those,
so those roles are relatively protected,

389
00:20:42,360 --> 00:20:46,440
protected where if you're a white collar
job and some corporations sitting in a

390
00:20:46,441 --> 00:20:50,320
cubicle somewhere, you don't
have any protections at
all. So for that reason, um,

391
00:20:50,321 --> 00:20:54,910
I would worry a bit less about doctors
a disappearing in the near term.

392
00:20:55,060 --> 00:20:55,710
But you know,

393
00:20:55,710 --> 00:20:57,970
in healthcare there definitely are
going to be lots of applications.

394
00:20:57,971 --> 00:21:01,060
You already see robots in
hospitals delivering things. Um,

395
00:21:01,420 --> 00:21:05,200
you see robots beginning to be used in
elder care looking after older people,

396
00:21:05,201 --> 00:21:09,700
which is certainly one of the biggest
opportunities because we have this aging

397
00:21:09,850 --> 00:21:13,750
population. Um, pharmacy
robots are, are huge things.

398
00:21:13,751 --> 00:21:15,490
They're already robots that do,
you know,

399
00:21:15,520 --> 00:21:20,260
thousands and thousands of
per Chris Prescriptions in,
in hospitals and so forth,

400
00:21:20,261 --> 00:21:22,870
very efficiently. So this is coming. Um,

401
00:21:22,930 --> 00:21:26,320
it will take a little bit longer in
healthcare than in some other areas. Um,

402
00:21:26,350 --> 00:21:30,370
but eventually it's going
to be everywhere. Um, in
retail, you know, they're,

403
00:21:30,520 --> 00:21:34,540
they're, uh, Walmart is
beginning to introduce, uh,

404
00:21:34,570 --> 00:21:39,520
robots and of course, retail in general
is migrating more and more toward Amazon.

405
00:21:40,140 --> 00:21:42,960
Um, which in theory means
that jobs, you know,

406
00:21:43,110 --> 00:21:46,270
they might move from a retail
store to an Amazon warehouse,

407
00:21:46,690 --> 00:21:49,330
but once the jobs go to
that Amazon warehouse,

408
00:21:49,331 --> 00:21:53,230
now they're in a very controlled
environment and there are already lots of

409
00:21:53,231 --> 00:21:54,190
robots there.

410
00:21:54,250 --> 00:21:57,310
And those robots are going to get
dramatically better in the next five or 10

411
00:21:57,311 --> 00:21:58,420
years, you know? So in effect,

412
00:21:58,421 --> 00:22:01,750
you could have a giant Amazon warehouse
that we've all driven by one of these,

413
00:22:01,810 --> 00:22:06,730
these huge monstrosities and it
could basically be run by all robots.

414
00:22:06,731 --> 00:22:10,510
And you're getting very close to that.
And definitely a lot fewer people. I mean,

415
00:22:10,750 --> 00:22:13,330
right now inside those robots,
inside those warehouses,

416
00:22:13,331 --> 00:22:17,110
you have huge numbers of robots and the
robots will do something like bring a

417
00:22:17,111 --> 00:22:19,330
whole shelf of inventory to a worker,

418
00:22:19,690 --> 00:22:24,520
but then the workers got to reach in
there and grab the item off the shelf and

419
00:22:24,521 --> 00:22:27,220
put it in the box because the
robot right now can't do that.

420
00:22:27,221 --> 00:22:30,190
It doesn't have the visual perception
and the dexterity to do that.

421
00:22:30,670 --> 00:22:33,610
But that will change over
the next five to 10 years.

422
00:22:34,000 --> 00:22:37,150
And so those environments are going
to become a lot less labor intensive.

423
00:22:37,180 --> 00:22:38,770
That's not to say they'll
be fully automated,

424
00:22:38,771 --> 00:22:40,150
but there are going to
be fewer jobs there.

425
00:22:40,151 --> 00:22:44,260
And that's something to worry about
because this is one of the brightest areas

426
00:22:44,261 --> 00:22:45,880
right now for job creation.
Right?

427
00:22:45,881 --> 00:22:49,450
So we're gonna so we're going to watch
a certain sector or many sectors of jobs

428
00:22:49,451 --> 00:22:51,520
just disappear altogether.
And yet at the same time,

429
00:22:51,521 --> 00:22:54,100
I guess the counter argument
or the people that would say,

430
00:22:54,101 --> 00:22:56,830
we shouldn't be so alarmed
about this, we'd say, well,

431
00:22:56,890 --> 00:23:00,520
all the costs of everything will go down
because the robots will be able to do

432
00:23:00,521 --> 00:23:02,830
things at a much more
efficient, cheaper level. Right.

433
00:23:03,130 --> 00:23:07,630
So people won't need as much disposable
income, that sort of thing. That's right.

434
00:23:07,660 --> 00:23:10,000
I mean that's absolutely true there.

435
00:23:10,510 --> 00:23:14,130
I am very skeptical that that's kind
of solves the problem though. Me,

436
00:23:14,131 --> 00:23:17,770
you can think about it. If you don't have
a job at all, then your income is zero.

437
00:23:18,490 --> 00:23:20,890
It doesn't matter how matter,
how cheap stuff is.

438
00:23:21,220 --> 00:23:23,800
The other thing is that the
really big ticket items,

439
00:23:23,801 --> 00:23:27,990
the things that really are putting people
under water, our housing, education,

440
00:23:28,000 --> 00:23:32,590
health care, and these are
exactly the areas where, um,

441
00:23:32,620 --> 00:23:36,640
technology is at least in the short
and medium term going to have the least

442
00:23:36,641 --> 00:23:38,770
impact, right? I mean
housing in particular,

443
00:23:38,771 --> 00:23:42,700
some day we might have big
three d printers that make
it really cheap to produce

444
00:23:42,701 --> 00:23:46,000
housing, but, but there's still
a problem with land right in the,

445
00:23:46,001 --> 00:23:50,120
if you're in Los Angeles or, or up in
San Francisco, then there's no land left,

446
00:23:50,121 --> 00:23:53,810
right? It's already know, you
know, it's already very scarce and,

447
00:23:53,811 --> 00:23:56,300
and that's what drives
property values so high.

448
00:23:56,301 --> 00:23:59,420
So you can't solve that problem and
necessarily just with technology.

449
00:23:59,870 --> 00:24:03,410
So as incomes fall, um, you know,

450
00:24:03,500 --> 00:24:07,490
many people are not going to have the
income to really cover the basics and that

451
00:24:07,491 --> 00:24:08,690
that's going to be a big problem.

452
00:24:08,740 --> 00:24:12,640
Okay. So that's the right transition
then to universal basic income. So my,

453
00:24:12,700 --> 00:24:15,730
my default position on UBI,

454
00:24:15,731 --> 00:24:19,030
and I've heard arguments on both sides
and I think I told you I have Andrew Yang

455
00:24:19,031 --> 00:24:21,130
coming in soon and we'll
discuss it further.

456
00:24:21,370 --> 00:24:25,870
My default position is that if you
give people just enough to survive,

457
00:24:26,140 --> 00:24:31,140
that you're sort of stealing just like
the most basic human right of just like

458
00:24:31,421 --> 00:24:35,530
go get something for yourself and that
it's going to create this class of people

459
00:24:35,590 --> 00:24:40,510
sort of not by their own fault that we'll
just have the bare minimum to get by

460
00:24:40,511 --> 00:24:44,470
and then there'll be able to stay at
home and play video games and watch porn

461
00:24:44,471 --> 00:24:47,440
and basically do nothing
all day long and will,

462
00:24:47,441 --> 00:24:52,090
and that's actually taking something
from them rather than giving something to

463
00:24:52,091 --> 00:24:56,290
them. That's sort of my sort of like
high level philosophic position on it.

464
00:24:56,450 --> 00:24:57,283
Right. I, I,

465
00:24:57,320 --> 00:25:01,190
the argument I would make is that once
the society reaches a certain level of

466
00:25:01,191 --> 00:25:03,140
prosperity as we have,
um,

467
00:25:03,170 --> 00:25:06,560
if you want to continue to have capitalism
and a market is very important to

468
00:25:06,561 --> 00:25:10,100
have the kind of incentive that,
that you're alluding to there.

469
00:25:10,520 --> 00:25:15,320
But I would argue that maybe the incentive
doesn't have to be so daunting that

470
00:25:15,620 --> 00:25:18,860
if you don't work, you're living on the
street or eating out of garbage cans,

471
00:25:18,920 --> 00:25:19,330
right.

472
00:25:19,330 --> 00:25:24,330
That maybe it's enough to say that you
can basically survive if you're not

473
00:25:25,160 --> 00:25:27,770
motivated, but you're not
going to have a terrific life.

474
00:25:27,771 --> 00:25:28,790
You're not going to have a great life.

475
00:25:28,791 --> 00:25:32,750
And I think that that a number of studies
have been done with basic income that

476
00:25:32,751 --> 00:25:35,360
showed that when you
give people this money,

477
00:25:35,361 --> 00:25:38,330
they don't in fact just drop out
and stay home and do nothing.

478
00:25:38,710 --> 00:25:41,270
They are actually motivated
to do something more. Um,

479
00:25:41,300 --> 00:25:45,020
they invest in their family and education.
They work if they can,

480
00:25:45,410 --> 00:25:47,060
they may be start a small business.

481
00:25:47,061 --> 00:25:50,600
So actually if you give people
that basic safety net, um,

482
00:25:50,601 --> 00:25:54,140
you can create an environment where
people are actually more willing to take

483
00:25:54,141 --> 00:25:56,510
risks. So for example, they
might start a new business.

484
00:25:57,020 --> 00:26:01,430
They might be willing to leave a safe
job where they're not learning anything.

485
00:26:01,431 --> 00:26:05,240
They're not growing and work for a
startup company, do something more risky.

486
00:26:05,420 --> 00:26:05,750
Right.

487
00:26:05,750 --> 00:26:09,860
Um, but as the inherent problem that
then if they start getting some success,

488
00:26:09,861 --> 00:26:11,550
then they lose the ubi.

489
00:26:11,780 --> 00:26:15,680
No, no, no. See that's, that's the
whole solution to a basic income.

490
00:26:15,681 --> 00:26:19,070
And that's what makes it different from
other forms of safety is that it is

491
00:26:19,071 --> 00:26:21,860
unconditional in a sense
that everyone gets it.

492
00:26:21,920 --> 00:26:26,510
Now what that means is that if I get my
basic income and I choose to just play

493
00:26:26,511 --> 00:26:28,580
video games,
then I'll have that basic income.

494
00:26:29,300 --> 00:26:32,000
But if someone else is more ambitious,

495
00:26:32,030 --> 00:26:35,450
they get their basic income and they go
and work, even if it's only part time,

496
00:26:35,451 --> 00:26:38,060
they start a small business,
then they,

497
00:26:38,390 --> 00:26:41,210
they get the basic income and they
also get that additional income.

498
00:26:41,270 --> 00:26:45,030
We don't tax it away,
at least not at the lower level.

499
00:26:45,120 --> 00:26:49,290
So the key point is that the person
that is productive that is willing to do

500
00:26:49,291 --> 00:26:53,850
something to work will always be better
off than the person that does nothing.

501
00:26:54,180 --> 00:26:57,390
Right? And that's really
key to it because, um,

502
00:26:57,540 --> 00:27:01,170
the problem with our existing social
safety net is exactly what you said,

503
00:27:01,171 --> 00:27:04,890
that if you do something, find a
job, then you lose those benefits.

504
00:27:06,050 --> 00:27:09,310
The cliff has to be really high. They
need to be willing to leave. Yeah.

505
00:27:09,370 --> 00:27:11,890
Right. Right. And that's exactly what
it's called, a poverty trap. Right?

506
00:27:11,891 --> 00:27:15,870
As you get into a situation where you
look at the options around you and,

507
00:27:15,871 --> 00:27:16,900
and anything you do,

508
00:27:16,901 --> 00:27:18,950
does it make you better offer you
gonna makes you work softened.

509
00:27:18,951 --> 00:27:20,110
So you're stuck there.
He can't move.

510
00:27:20,260 --> 00:27:25,260
The worst possible example of that in
United States is the social security

511
00:27:25,811 --> 00:27:27,100
disability program,
right?

512
00:27:27,101 --> 00:27:30,400
Which is intended for people that are
injured on the job and then they can't

513
00:27:30,401 --> 00:27:33,380
work. But actually a lot of people now
are gaming it probably because they're

514
00:27:33,400 --> 00:27:34,840
desperate.
They need an income.

515
00:27:35,200 --> 00:27:37,990
And so they'll go and tell their doctor
they've got pain in their back or

516
00:27:37,991 --> 00:27:41,680
something and they'll get through the
loops and they'll get onto this program,

517
00:27:41,681 --> 00:27:44,290
which gives you an income.
But once you get there,

518
00:27:44,770 --> 00:27:47,170
you can't even be seen to be able bodied.

519
00:27:47,171 --> 00:27:51,490
People are worried even to go and work
in their garden or something because

520
00:27:51,491 --> 00:27:54,850
someone will see them and then they'll
lose their benefits. Right. Um,

521
00:27:55,540 --> 00:27:59,410
so that's a really terrible example of
this kind of income where basic income,

522
00:27:59,800 --> 00:28:03,430
we give it to everyone and it's
unconditional. And then we,

523
00:28:03,431 --> 00:28:07,660
we encourage people to do more. Right.
And that, that, that's really important.

524
00:28:07,661 --> 00:28:11,380
That's one of the strongest
arguments for a basic income scheme.

525
00:28:11,480 --> 00:28:13,700
Yeah. So let's get into some of the
nuts and bolts of it. First off,

526
00:28:13,701 --> 00:28:16,100
do you view it as something that
would have to be done federally?

527
00:28:16,101 --> 00:28:19,010
Because obviously if you live
in Los Angeles or San Francisco,

528
00:28:19,011 --> 00:28:23,150
your cost of living is way higher
than say if you live in Missouri.

529
00:28:23,750 --> 00:28:26,840
So is this a, is this a federal program?
Are we throwing this to the states?

530
00:28:27,370 --> 00:28:30,560
Yeah, I would imagine it needs to
be done on a national level. Um,

531
00:28:30,561 --> 00:28:34,870
and the reason is what you can think
of as kind of like the kind of adverse

532
00:28:34,871 --> 00:28:38,700
selection problem you get
in in insurance, right? If,

533
00:28:38,701 --> 00:28:41,230
if Los Angeles has a basic income,

534
00:28:41,680 --> 00:28:44,320
then people all over the country are
going to move to Los Angeles to get that

535
00:28:44,321 --> 00:28:47,020
right. And they're going to show up here
and, and, and overwhelming the system.

536
00:28:47,021 --> 00:28:51,100
So it needs to be national
rather than, than local. Right.

537
00:28:51,190 --> 00:28:53,910
But what do you do about the
disparity in cost of living

538
00:28:53,930 --> 00:28:55,090
and all these places? Well, you know,

539
00:28:55,100 --> 00:28:58,910
one issue there is that a
basic income is mobile, right?

540
00:28:58,911 --> 00:29:02,450
So maybe you don't have to live
in Los Angeles or San Francisco.

541
00:29:02,451 --> 00:29:06,770
You can take your basic income and
you can move to Detroit, right?

542
00:29:06,771 --> 00:29:10,720
And Dare you might have a pretty
decent life. Um, you, you,

543
00:29:10,721 --> 00:29:12,890
you can have housing.
They're much cheaper. Right?

544
00:29:12,891 --> 00:29:16,400
So the difference between a basic
income and a job is that you can take it

545
00:29:16,401 --> 00:29:19,100
everywhere. So people would
kind of readjust and they might,

546
00:29:19,101 --> 00:29:22,900
some people might choose to leave,
leave high cost areas and,

547
00:29:22,901 --> 00:29:24,800
and live in cheaper places and so forth.

548
00:29:25,010 --> 00:29:27,560
So how do you find all this?
That always is the big one.

549
00:29:27,561 --> 00:29:30,870
Are you scrapping all the social programs
that exist right now or are your tax

550
00:29:30,871 --> 00:29:32,510
and billionaires out the Wazoo?

551
00:29:32,960 --> 00:29:37,130
Well, combination there. Nobody think
definitely you need to raise more revenue.

552
00:29:37,160 --> 00:29:37,993
Um,

553
00:29:38,600 --> 00:29:43,480
I think inevitably one of the things that
we are seeing with the economy largely

554
00:29:43,481 --> 00:29:44,410
as a result of,

555
00:29:44,411 --> 00:29:49,030
of technology is that more and more income
is going to capital unless it's going

556
00:29:49,031 --> 00:29:49,864
to labor.

557
00:29:50,020 --> 00:29:55,020
So businesses and investors and people
like that are getting more income and

558
00:29:56,351 --> 00:29:59,440
average working people are getting less.
So what that means for the future.

559
00:29:59,441 --> 00:30:03,160
I think it's inevitable that we're going
to have to tax capital more and labor

560
00:30:03,161 --> 00:30:04,780
less.
Um,

561
00:30:04,810 --> 00:30:09,340
and that may involve higher business
taxes are taxes on the wealthiest people

562
00:30:09,341 --> 00:30:13,870
that have access a lot. Lots of
capital mean that's as a libertarian,

563
00:30:13,871 --> 00:30:17,500
you might find that objectionable, but
I think it's inevitable. You ultimately,

564
00:30:17,501 --> 00:30:19,090
if you're going to have a taxation scheme,

565
00:30:19,510 --> 00:30:22,150
you have to tax the people that
have the money, right? You can't,

566
00:30:22,300 --> 00:30:24,990
you can't get blood from
a rock as they say. Right?

567
00:30:25,270 --> 00:30:27,910
So how do you decide
what the level is now?

568
00:30:27,911 --> 00:30:30,940
I get you could live in La and the cost
of living's high and then maybe you'd

569
00:30:30,941 --> 00:30:33,700
say, all right, well I can't, I can't
make it here in a way I wanted to do.

570
00:30:33,701 --> 00:30:36,970
So I want to go somewhere cheap.
But how do you figure out, well,

571
00:30:36,971 --> 00:30:40,990
what is it that is the basic stuff.
I mean,

572
00:30:41,000 --> 00:30:42,580
we're called UBI.

573
00:30:42,581 --> 00:30:47,380
So what is the basic stuff that people
are supposed to have? Well, I mean the,

574
00:30:47,470 --> 00:30:49,270
in terms of the level of the income,

575
00:30:49,271 --> 00:30:53,740
most people are talking about around
a thousand dollars a month. Um,

576
00:30:54,070 --> 00:30:58,030
Finland had a, an experiment where
it was like 600 euros or something.

577
00:30:58,031 --> 00:31:00,180
So these are pretty low
amounts. I mean, you know, try,

578
00:31:00,410 --> 00:31:04,470
can you imagine living on $1,000 a
month? Right? I used to do it. Yeah,

579
00:31:04,520 --> 00:31:06,790
it was not fun.
It's not so easy.

580
00:31:06,790 --> 00:31:11,030
So I think one advantage of these programs
is that they're going to start at a,

581
00:31:11,031 --> 00:31:11,771
at a low level,

582
00:31:11,771 --> 00:31:16,771
and we can imagine that as technology
advances and society becomes more

583
00:31:17,261 --> 00:31:19,810
prosperous,
that that could be raised over time.

584
00:31:19,840 --> 00:31:21,910
But initially it's going
to be a very low level.

585
00:31:21,911 --> 00:31:25,360
So I don't think we have to worry too
much about destroying the incentive for

586
00:31:25,361 --> 00:31:28,840
people to work and so forth.
It's going to give people a very,

587
00:31:28,841 --> 00:31:33,250
very minimal cushion. Um, but they're
still gonna have that incentive to work.

588
00:31:33,280 --> 00:31:33,610
Right.

589
00:31:33,610 --> 00:31:38,260
So interesting because it just does set
all my libertarian bells off that well,

590
00:31:38,261 --> 00:31:41,890
the second you give it to somebody, so
we give 1000 bucks to everybody. Well,

591
00:31:41,891 --> 00:31:45,670
immediately you're going
to have politicians saying
this isn't enough and we

592
00:31:45,671 --> 00:31:47,620
have to make it more and we
have to make it more in that.

593
00:31:47,860 --> 00:31:52,660
Then that becomes the cycle
where [inaudible] is shifting
money around and it's

594
00:31:52,661 --> 00:31:56,710
just because no one's ever gonna no
matter what basic level we get most people

595
00:31:56,711 --> 00:31:58,360
who no one's ever going to be like,
all right,

596
00:31:58,390 --> 00:32:02,050
well they were okay then
that's a real concern. Um,

597
00:32:02,470 --> 00:32:06,040
I would say though that,
you know, a basic income or,

598
00:32:06,041 --> 00:32:08,830
or there are other flavors of
and a guaranteed minimum income,

599
00:32:08,831 --> 00:32:09,850
a negative income tax.

600
00:32:09,851 --> 00:32:14,590
These are ideas that in the past
have been embraced by Libertarians.

601
00:32:14,830 --> 00:32:18,960
Friedrich Hayak was a big proponent
of, of a guaranteed minimum income. Um,

602
00:32:19,060 --> 00:32:22,840
Milton Friedman was for a negative
income tax and the idea is that you're

603
00:32:22,841 --> 00:32:26,890
creating really a market based safety
net, right? Rather than having government,

604
00:32:27,520 --> 00:32:30,790
um, how's people feed people, uh,

605
00:32:30,820 --> 00:32:35,820
control industries or try to take over
businesses and run them in a way that

606
00:32:36,071 --> 00:32:39,980
artificially creates jobs and so forth.
Rather doing that,

607
00:32:40,400 --> 00:32:43,580
just give people some money, let them
go out and participate in the market.

608
00:32:43,581 --> 00:32:48,581
So it actually is a market
oriented libertarian approach
to having some kind of

609
00:32:49,140 --> 00:32:52,490
safety net. But I think you're
the idea of it being politicized.

610
00:32:52,491 --> 00:32:53,830
That's a real concern.
And one,

611
00:32:53,831 --> 00:32:58,190
one thing I actually have suggested in
some of my writing is that we might set

612
00:32:58,191 --> 00:33:00,650
up a separate institution
to kind of manage that.

613
00:33:00,651 --> 00:33:04,910
Maybe something like the Federal Reserve
do it would be independent and not part

614
00:33:04,911 --> 00:33:09,100
of the political process and
might manage the level of,

615
00:33:09,480 --> 00:33:13,220
of a basic income because you
could actually use it also to,

616
00:33:13,760 --> 00:33:16,970
to respond to recessions. For example,
if there's an economic downturn,

617
00:33:16,971 --> 00:33:21,200
maybe pay people a bit more and then
that would help you get out of the

618
00:33:21,260 --> 00:33:24,640
recession. Right? It would be kind
of a Keynesian response to it. Um,

619
00:33:24,650 --> 00:33:26,930
so I think there are a lot
of possibilities there,
but you're right, we don't,

620
00:33:26,931 --> 00:33:30,620
we don't want every politician running
on the platform of I will increase your

621
00:33:30,621 --> 00:33:32,630
basic income, right. That
that wouldn't be good.

622
00:33:32,631 --> 00:33:37,030
So that's something that just strikes me
as sort of real politic related to all

623
00:33:37,040 --> 00:33:39,980
this is just the way people are. Once
you give them something, they want more.

624
00:33:39,981 --> 00:33:43,250
I don't blame people for that.
It's just sort of, exactly.

625
00:33:43,251 --> 00:33:44,830
So that's the way politics works.

626
00:33:44,831 --> 00:33:47,990
So that's something that we need to think
about from the beginning. As I said,

627
00:33:48,220 --> 00:33:50,630
you know, maybe you put it in the
hands of a separate institution.

628
00:33:50,631 --> 00:33:55,400
One other thing I proposed is that maybe
we can build incentives into a basic

629
00:33:55,401 --> 00:33:58,580
income. Um, if people stay in school,

630
00:33:59,060 --> 00:34:02,690
paid them a bit more than people that
just play video games where people go and

631
00:34:02,691 --> 00:34:06,440
work in the community, you know, help
other people pay, pay them a bit more.

632
00:34:06,441 --> 00:34:07,100
So that,

633
00:34:07,100 --> 00:34:11,240
I think it's really important to have
sort of a ladder for people that they feel

634
00:34:11,241 --> 00:34:15,230
they can somehow do better. Because that,
I mean, the issue you raised everywhere,

635
00:34:15,470 --> 00:34:15,920
arrays.

636
00:34:15,920 --> 00:34:20,790
Before that we could create this class
of people that just do nothing is

637
00:34:20,870 --> 00:34:24,410
something to be concerned about.
But there are ideas that we can,

638
00:34:24,440 --> 00:34:26,390
I think employee to,
to really address that.

