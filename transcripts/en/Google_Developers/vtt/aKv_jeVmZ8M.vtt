WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.485
[THEME MUSIC]

00:00:05.345 --> 00:00:07.720
TIMOTHY JORDON: This is I/O
live, and I'm Timothy Jordan.

00:00:07.720 --> 00:00:09.670
I'm standing here
with Riccardo, who's

00:00:09.670 --> 00:00:12.462
working on a really cool machine
learning project that he's

00:00:12.462 --> 00:00:13.420
going to tell us about.

00:00:13.420 --> 00:00:14.320
Hi, Riccardo.

00:00:14.320 --> 00:00:14.960
RICCARDO SABATINI: Hey, hello.

00:00:14.960 --> 00:00:15.730
Hi, everyone.

00:00:15.730 --> 00:00:17.800
TIMOTHY JORDON: Could you give
us an overview of the work

00:00:17.800 --> 00:00:18.580
that you're doing?

00:00:18.580 --> 00:00:19.538
RICCARDO SABATINI: Yes.

00:00:19.538 --> 00:00:23.170
So we are working on, and we
managed to close a technology

00:00:23.170 --> 00:00:26.275
to give us, visibility on how
a small molecule is interacting

00:00:26.275 --> 00:00:29.310
not with just a piece of your
genome-- the usual design--

00:00:29.310 --> 00:00:31.540
but on the entire protean.

00:00:31.540 --> 00:00:33.490
And with the best
approximation we can get.

00:00:33.490 --> 00:00:36.430
A nice assay biological test.

00:00:36.430 --> 00:00:40.120
And this is giving us visibility
that we never had before.

00:00:40.120 --> 00:00:42.400
So we are working to
release a database where

00:00:42.400 --> 00:00:44.770
these small molecule
interactions are going

00:00:44.770 --> 00:00:46.380
to map of all our genomes.

00:00:46.380 --> 00:00:49.405
And this I think will open up
a complete new design principle

00:00:49.405 --> 00:00:52.570
on small molecules, new
exciting algorithms, and better

00:00:52.570 --> 00:00:54.451
precision into robotics.

00:00:54.451 --> 00:00:55.450
TIMOTHY JORDON: Awesome.

00:00:55.450 --> 00:00:57.320
Now this is a
really big problem,

00:00:57.320 --> 00:00:58.510
with a really big solution.

00:00:58.510 --> 00:01:00.970
What about an everyday
problem that you would love

00:01:00.970 --> 00:01:03.160
to solve with machine learning?

00:01:03.160 --> 00:01:06.470
RICCARDO SABATINI: I think will
be probably choosing my food.

00:01:06.470 --> 00:01:10.660
So today we get to a grocery
store or to a restaurants,

00:01:10.660 --> 00:01:12.810
we pick our food mostly by use.

00:01:12.810 --> 00:01:15.270
So we are used to buy a
specific kind of things.

00:01:15.270 --> 00:01:17.140
And we repeat over
and over again.

00:01:17.140 --> 00:01:20.510
But if I had in my
calendar, or my agenda,

00:01:20.510 --> 00:01:22.690
when I go to the gym,
if I have meetings,

00:01:22.690 --> 00:01:26.020
I have my Fitbit or my
sleeping wearable device

00:01:26.020 --> 00:01:29.560
that tells me if I slept a
lot, or if I'm close to 40.

00:01:29.560 --> 00:01:31.080
I'd love someone to solve that.

00:01:31.080 --> 00:01:32.750
You should eat this,
this, and that,

00:01:32.750 --> 00:01:35.230
and remember that you
have an early meeting.

00:01:35.230 --> 00:01:37.670
Don't eat too much
or too late tonight.

00:01:37.670 --> 00:01:42.210
Or drink some milk, or get
some vitamin D in winter.

00:01:42.210 --> 00:01:42.760
Boom.

00:01:42.760 --> 00:01:43.870
I would change my life.

00:01:43.870 --> 00:01:44.500
TIMOTHY JORDON: I
would probably still

00:01:44.500 --> 00:01:45.880
sneak in some gummy
bears, though.

00:01:45.880 --> 00:01:48.130
RICCARDO SABATINI: You can
totally tweak the algorithm

00:01:48.130 --> 00:01:51.400
and add a naughty [INAUDIBLE]
so you can pick up some sugar,

00:01:51.400 --> 00:01:52.570
if you like it.

00:01:52.570 --> 00:01:53.230
TIMOTHY JORDON:
Awesome, Riccardo.

00:01:53.230 --> 00:01:55.000
Thanks so much for joining
us and sharing your story.

00:01:55.000 --> 00:01:56.410
RICCARDO SABATINI: Thanks
for having me here.

00:01:56.410 --> 00:01:57.220
Thank you.

00:01:57.220 --> 00:01:59.000
TIMOTHY JORDON: And thank
you, for watching I/O live.

00:01:59.000 --> 00:02:00.583
Now that we've heard
a little bit more

00:02:00.583 --> 00:02:03.010
about what ML is
being applied to,

00:02:03.010 --> 00:02:05.410
let's go check out some of
the technology behind all

00:02:05.410 --> 00:02:06.701
this machine learning research.

00:02:06.701 --> 00:02:07.600
Shall we?

00:02:07.600 --> 00:02:09.567
Head on over to the ML sandbox?

00:02:09.567 --> 00:02:10.150
Let's do that.

00:02:14.551 --> 00:02:18.246
[THEME MUSIC]

