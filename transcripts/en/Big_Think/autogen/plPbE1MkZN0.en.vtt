WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:04.710
 
[Music]

00:00:04.710 --> 00:00:04.720
[Music]
 

00:00:04.720 --> 00:00:07.450
[Music]
as you think about your talent and your

00:00:07.450 --> 00:00:07.460
as you think about your talent and your
 

00:00:07.460 --> 00:00:10.209
as you think about your talent and your
workforce for the for the age of AI and

00:00:10.209 --> 00:00:10.219
workforce for the for the age of AI and
 

00:00:10.219 --> 00:00:12.190
workforce for the for the age of AI and
we're removing - there's there's a few

00:00:12.190 --> 00:00:12.200
we're removing - there's there's a few
 

00:00:12.200 --> 00:00:13.600
we're removing - there's there's a few
things that are very important to think

00:00:13.600 --> 00:00:13.610
things that are very important to think
 

00:00:13.610 --> 00:00:16.179
things that are very important to think
about and I think about two broad

00:00:16.179 --> 00:00:16.189
about and I think about two broad
 

00:00:16.189 --> 00:00:19.060
about and I think about two broad
categories of skills that you need to

00:00:19.060 --> 00:00:19.070
categories of skills that you need to
 

00:00:19.070 --> 00:00:21.939
categories of skills that you need to
prepare for one is the talent or the

00:00:21.939 --> 00:00:21.949
prepare for one is the talent or the
 

00:00:21.949 --> 00:00:24.609
prepare for one is the talent or the
skills or that the people that do AI you

00:00:24.609 --> 00:00:24.619
skills or that the people that do AI you
 

00:00:24.619 --> 00:00:26.259
skills or that the people that do AI you
know who's going to develop the AI those

00:00:26.259 --> 00:00:26.269
know who's going to develop the AI those
 

00:00:26.269 --> 00:00:27.820
know who's going to develop the AI those
are the machine learning experts the

00:00:27.820 --> 00:00:27.830
are the machine learning experts the
 

00:00:27.830 --> 00:00:30.279
are the machine learning experts the
data scientists the people with the STEM

00:00:30.279 --> 00:00:30.289
data scientists the people with the STEM
 

00:00:30.289 --> 00:00:31.660
data scientists the people with the STEM
skills and coding skills that are going

00:00:31.660 --> 00:00:31.670
skills and coding skills that are going
 

00:00:31.670 --> 00:00:32.830
skills and coding skills that are going
to build the technology of the future

00:00:32.830 --> 00:00:32.840
to build the technology of the future
 

00:00:32.840 --> 00:00:35.680
to build the technology of the future
and that that is an important area that

00:00:35.680 --> 00:00:35.690
and that that is an important area that
 

00:00:35.690 --> 00:00:37.450
and that that is an important area that
every organization needs to be preparing

00:00:37.450 --> 00:00:37.460
every organization needs to be preparing
 

00:00:37.460 --> 00:00:39.340
every organization needs to be preparing
for you know developing and building

00:00:39.340 --> 00:00:39.350
for you know developing and building
 

00:00:39.350 --> 00:00:41.979
for you know developing and building
those types of skills and that's one

00:00:41.979 --> 00:00:41.989
those types of skills and that's one
 

00:00:41.989 --> 00:00:43.869
those types of skills and that's one
important thing but the bigger set of

00:00:43.869 --> 00:00:43.879
important thing but the bigger set of
 

00:00:43.879 --> 00:00:45.130
important thing but the bigger set of
skills that I think every organization

00:00:45.130 --> 00:00:45.140
skills that I think every organization
 

00:00:45.140 --> 00:00:47.710
skills that I think every organization
is to think about are the people who use

00:00:47.710 --> 00:00:47.720
is to think about are the people who use
 

00:00:47.720 --> 00:00:49.750
is to think about are the people who use
AI not the ones who do in build it but

00:00:49.750 --> 00:00:49.760
AI not the ones who do in build it but
 

00:00:49.760 --> 00:00:51.549
AI not the ones who do in build it but
the people who use AI and that's going

00:00:51.549 --> 00:00:51.559
the people who use AI and that's going
 

00:00:51.559 --> 00:00:53.469
the people who use AI and that's going
to be basically everybody in the work

00:00:53.469 --> 00:00:53.479
to be basically everybody in the work
 

00:00:53.479 --> 00:00:54.729
to be basically everybody in the work
force or almost everybody in the work

00:00:54.729 --> 00:00:54.739
force or almost everybody in the work
 

00:00:54.739 --> 00:00:56.590
force or almost everybody in the work
force in your work force as an

00:00:56.590 --> 00:00:56.600
force in your work force as an
 

00:00:56.600 --> 00:00:57.910
force in your work force as an
organization and I think that's an area

00:00:57.910 --> 00:00:57.920
organization and I think that's an area
 

00:00:57.920 --> 00:00:59.770
organization and I think that's an area
where organizations haven't spent enough

00:00:59.770 --> 00:00:59.780
where organizations haven't spent enough
 

00:00:59.780 --> 00:01:01.030
where organizations haven't spent enough
time everybody knows they have to

00:01:01.030 --> 00:01:01.040
time everybody knows they have to
 

00:01:01.040 --> 00:01:03.460
time everybody knows they have to
develop and hire the AI experts in the

00:01:03.460 --> 00:01:03.470
develop and hire the AI experts in the
 

00:01:03.470 --> 00:01:06.010
develop and hire the AI experts in the
coding experts I think that how the rest

00:01:06.010 --> 00:01:06.020
coding experts I think that how the rest
 

00:01:06.020 --> 00:01:07.360
coding experts I think that how the rest
of your organization is going to adapt

00:01:07.360 --> 00:01:07.370
of your organization is going to adapt
 

00:01:07.370 --> 00:01:09.490
of your organization is going to adapt
and use AI is the big question that

00:01:09.490 --> 00:01:09.500
and use AI is the big question that
 

00:01:09.500 --> 00:01:11.140
and use AI is the big question that
we're really trying to address in human

00:01:11.140 --> 00:01:11.150
we're really trying to address in human
 

00:01:11.150 --> 00:01:13.090
we're really trying to address in human
plus machine there's a few things that

00:01:13.090 --> 00:01:13.100
plus machine there's a few things that
 

00:01:13.100 --> 00:01:13.930
plus machine there's a few things that
I'd say that are that are really

00:01:13.930 --> 00:01:13.940
I'd say that are that are really
 

00:01:13.940 --> 00:01:16.720
I'd say that are that are really
important there one is is you need to

00:01:16.720 --> 00:01:16.730
important there one is is you need to
 

00:01:16.730 --> 00:01:18.970
important there one is is you need to
think about the learning platforms that

00:01:18.970 --> 00:01:18.980
think about the learning platforms that
 

00:01:18.980 --> 00:01:20.770
think about the learning platforms that
you're developing for your organization

00:01:20.770 --> 00:01:20.780
you're developing for your organization
 

00:01:20.780 --> 00:01:23.230
you're developing for your organization
one thing that we've we found in the

00:01:23.230 --> 00:01:23.240
one thing that we've we found in the
 

00:01:23.240 --> 00:01:25.120
one thing that we've we found in the
survey in the research work we did is

00:01:25.120 --> 00:01:25.130
survey in the research work we did is
 

00:01:25.130 --> 00:01:27.970
survey in the research work we did is
2/3 of organizations roughly believe

00:01:27.970 --> 00:01:27.980
2/3 of organizations roughly believe
 

00:01:27.980 --> 00:01:29.650
2/3 of organizations roughly believe
that their workforce isn't ready for AI

00:01:29.650 --> 00:01:29.660
that their workforce isn't ready for AI
 

00:01:29.660 --> 00:01:32.710
that their workforce isn't ready for AI
broadly for using AI a big number only

00:01:32.710 --> 00:01:32.720
broadly for using AI a big number only
 

00:01:32.720 --> 00:01:36.220
broadly for using AI a big number only
3% of organizations plan to increase

00:01:36.220 --> 00:01:36.230
3% of organizations plan to increase
 

00:01:36.230 --> 00:01:38.440
3% of organizations plan to increase
their training spending to account for

00:01:38.440 --> 00:01:38.450
their training spending to account for
 

00:01:38.450 --> 00:01:40.570
their training spending to account for
that which meat which isn't appropriate

00:01:40.570 --> 00:01:40.580
that which meat which isn't appropriate
 

00:01:40.580 --> 00:01:42.190
that which meat which isn't appropriate
that means that you know generally

00:01:42.190 --> 00:01:42.200
that means that you know generally
 

00:01:42.200 --> 00:01:43.360
that means that you know generally
people think it's somebody else's

00:01:43.360 --> 00:01:43.370
people think it's somebody else's
 

00:01:43.370 --> 00:01:45.580
people think it's somebody else's
problem to prepare the workforce and we

00:01:45.580 --> 00:01:45.590
problem to prepare the workforce and we
 

00:01:45.590 --> 00:01:47.050
problem to prepare the workforce and we
believe that that that's not the right

00:01:47.050 --> 00:01:47.060
believe that that that's not the right
 

00:01:47.060 --> 00:01:48.520
believe that that that's not the right
answer at Accenture we're investing

00:01:48.520 --> 00:01:48.530
answer at Accenture we're investing
 

00:01:48.530 --> 00:01:49.840
answer at Accenture we're investing
about a billion dollars a year in

00:01:49.840 --> 00:01:49.850
about a billion dollars a year in
 

00:01:49.850 --> 00:01:51.910
about a billion dollars a year in
training and retraining our workforce in

00:01:51.910 --> 00:01:51.920
training and retraining our workforce in
 

00:01:51.920 --> 00:01:53.800
training and retraining our workforce in
developing talent platforms that

00:01:53.800 --> 00:01:53.810
developing talent platforms that
 

00:01:53.810 --> 00:01:55.930
developing talent platforms that
continually retrain people and we think

00:01:55.930 --> 00:01:55.940
continually retrain people and we think
 

00:01:55.940 --> 00:01:57.160
continually retrain people and we think
that that's the approach you need to

00:01:57.160 --> 00:01:57.170
that that's the approach you need to
 

00:01:57.170 --> 00:01:58.719
that that's the approach you need to
take because we're in an age of

00:01:58.719 --> 00:01:58.729
take because we're in an age of
 

00:01:58.729 --> 00:02:01.180
take because we're in an age of
continuous innovation the roles of your

00:02:01.180 --> 00:02:01.190
continuous innovation the roles of your
 

00:02:01.190 --> 00:02:02.469
continuous innovation the roles of your
workforce are going to continue to

00:02:02.469 --> 00:02:02.479
workforce are going to continue to
 

00:02:02.479 --> 00:02:04.270
workforce are going to continue to
change and you can't flush and replace

00:02:04.270 --> 00:02:04.280
change and you can't flush and replace
 

00:02:04.280 --> 00:02:05.530
change and you can't flush and replace
the workforce and that's not the right

00:02:05.530 --> 00:02:05.540
the workforce and that's not the right
 

00:02:05.540 --> 00:02:07.030
the workforce and that's not the right
way to view it how do you how do you

00:02:07.030 --> 00:02:07.040
way to view it how do you how do you
 

00:02:07.040 --> 00:02:09.039
way to view it how do you how do you
look at your employees as an investable

00:02:09.039 --> 00:02:09.049
look at your employees as an investable
 

00:02:09.049 --> 00:02:10.990
look at your employees as an investable
resource where you're investing in the

00:02:10.990 --> 00:02:11.000
resource where you're investing in the
 

00:02:11.000 --> 00:02:12.280
resource where you're investing in the
talent and developing the right learning

00:02:12.280 --> 00:02:12.290
talent and developing the right learning
 

00:02:12.290 --> 00:02:14.440
talent and developing the right learning
platforms that they can learn how to use

00:02:14.440 --> 00:02:14.450
platforms that they can learn how to use
 

00:02:14.450 --> 00:02:16.270
platforms that they can learn how to use
AI in the initial applications or

00:02:16.270 --> 00:02:16.280
AI in the initial applications or
 

00:02:16.280 --> 00:02:17.680
AI in the initial applications or
rolling out now and continue to learn

00:02:17.680 --> 00:02:17.690
rolling out now and continue to learn
 

00:02:17.690 --> 00:02:19.510
rolling out now and continue to learn
that their skills are relevant in their

00:02:19.510 --> 00:02:19.520
that their skills are relevant in their
 

00:02:19.520 --> 00:02:20.650
that their skills are relevant in their
productive contributors to your

00:02:20.650 --> 00:02:20.660
productive contributors to your
 

00:02:20.660 --> 00:02:23.370
productive contributors to your
organization as you continue to progress

00:02:23.370 --> 00:02:23.380
organization as you continue to progress
 

00:02:23.380 --> 00:02:25.420
organization as you continue to progress
another air that we've really overlooked

00:02:25.420 --> 00:02:25.430
another air that we've really overlooked
 

00:02:25.430 --> 00:02:27.130
another air that we've really overlooked
in where there's huge potential is using

00:02:27.130 --> 00:02:27.140
in where there's huge potential is using
 

00:02:27.140 --> 00:02:29.800
in where there's huge potential is using
AI itself to help prepare the workforce

00:02:29.800 --> 00:02:29.810
AI itself to help prepare the workforce
 

00:02:29.810 --> 00:02:32.620
AI itself to help prepare the workforce
and I think there's huge opportunities

00:02:32.620 --> 00:02:32.630
and I think there's huge opportunities
 

00:02:32.630 --> 00:02:33.970
and I think there's huge opportunities
for innovation here we're starting to

00:02:33.970 --> 00:02:33.980
for innovation here we're starting to
 

00:02:33.980 --> 00:02:35.949
for innovation here we're starting to
see some you know some some real

00:02:35.949 --> 00:02:35.959
see some you know some some real
 

00:02:35.959 --> 00:02:37.930
see some you know some some real
interesting possibilities coming one

00:02:37.930 --> 00:02:37.940
interesting possibilities coming one
 

00:02:37.940 --> 00:02:39.580
interesting possibilities coming one
experiment we've done as an organization

00:02:39.580 --> 00:02:39.590
experiment we've done as an organization
 

00:02:39.590 --> 00:02:40.660
experiment we've done as an organization
this is still in the research and

00:02:40.660 --> 00:02:40.670
this is still in the research and
 

00:02:40.670 --> 00:02:42.699
this is still in the research and
development stage we've looked at all of

00:02:42.699 --> 00:02:42.709
development stage we've looked at all of
 

00:02:42.709 --> 00:02:44.410
development stage we've looked at all of
our employees in in in our Accenture

00:02:44.410 --> 00:02:44.420
our employees in in in our Accenture
 

00:02:44.420 --> 00:02:46.449
our employees in in in our Accenture
organization we have about over four

00:02:46.449 --> 00:02:46.459
organization we have about over four
 

00:02:46.459 --> 00:02:48.010
organization we have about over four
hundred thirty thousand people so large

00:02:48.010 --> 00:02:48.020
hundred thirty thousand people so large
 

00:02:48.020 --> 00:02:50.470
hundred thirty thousand people so large
workforce is we've developed a machine

00:02:50.470 --> 00:02:50.480
workforce is we've developed a machine
 

00:02:50.480 --> 00:02:52.240
workforce is we've developed a machine
learning model using artificial

00:02:52.240 --> 00:02:52.250
learning model using artificial
 

00:02:52.250 --> 00:02:54.580
learning model using artificial
intelligence that can take the resumes

00:02:54.580 --> 00:02:54.590
intelligence that can take the resumes
 

00:02:54.590 --> 00:02:56.020
intelligence that can take the resumes
and experience of any one of our

00:02:56.020 --> 00:02:56.030
and experience of any one of our
 

00:02:56.030 --> 00:02:58.060
and experience of any one of our
employees and this is something our

00:02:58.060 --> 00:02:58.070
employees and this is something our
 

00:02:58.070 --> 00:03:00.520
employees and this is something our
employees can use to understand how

00:03:00.520 --> 00:03:00.530
employees can use to understand how
 

00:03:00.530 --> 00:03:03.009
employees can use to understand how
their job will be impacted by AI and a

00:03:03.009 --> 00:03:03.019
their job will be impacted by AI and a
 

00:03:03.019 --> 00:03:04.870
their job will be impacted by AI and a
soda it might say that you know a feed

00:03:04.870 --> 00:03:04.880
soda it might say that you know a feed
 

00:03:04.880 --> 00:03:05.979
soda it might say that you know a feed
and all of your information it'll

00:03:05.979 --> 00:03:05.989
and all of your information it'll
 

00:03:05.989 --> 00:03:08.470
and all of your information it'll
compared to external job postings and

00:03:08.470 --> 00:03:08.480
compared to external job postings and
 

00:03:08.480 --> 00:03:09.670
compared to external job postings and
trends in the marketplace and it might

00:03:09.670 --> 00:03:09.680
trends in the marketplace and it might
 

00:03:09.680 --> 00:03:12.340
trends in the marketplace and it might
say well you've your skills are at risk

00:03:12.340 --> 00:03:12.350
say well you've your skills are at risk
 

00:03:12.350 --> 00:03:15.040
say well you've your skills are at risk
in about one to three years and it

00:03:15.040 --> 00:03:15.050
in about one to three years and it
 

00:03:15.050 --> 00:03:16.990
in about one to three years and it
doesn't stop there but it says and based

00:03:16.990 --> 00:03:17.000
doesn't stop there but it says and based
 

00:03:17.000 --> 00:03:19.000
doesn't stop there but it says and based
on what you know here's the adjacent

00:03:19.000 --> 00:03:19.010
on what you know here's the adjacent
 

00:03:19.010 --> 00:03:21.370
on what you know here's the adjacent
types of you know of jobs that you

00:03:21.370 --> 00:03:21.380
types of you know of jobs that you
 

00:03:21.380 --> 00:03:22.960
types of you know of jobs that you
should start looking to train yourself

00:03:22.960 --> 00:03:22.970
should start looking to train yourself
 

00:03:22.970 --> 00:03:25.060
should start looking to train yourself
for and again this is a rillette the

00:03:25.060 --> 00:03:25.070
for and again this is a rillette the
 

00:03:25.070 --> 00:03:26.350
for and again this is a rillette the
research and development stage I

00:03:26.350 --> 00:03:26.360
research and development stage I
 

00:03:26.360 --> 00:03:27.880
research and development stage I
wouldn't say it's it's a product out

00:03:27.880 --> 00:03:27.890
wouldn't say it's it's a product out
 

00:03:27.890 --> 00:03:29.140
wouldn't say it's it's a product out
there yeah but it shows the kind of

00:03:29.140 --> 00:03:29.150
there yeah but it shows the kind of
 

00:03:29.150 --> 00:03:31.420
there yeah but it shows the kind of
innovation and creativity and way that

00:03:31.420 --> 00:03:31.430
innovation and creativity and way that
 

00:03:31.430 --> 00:03:33.729
innovation and creativity and way that
we can use technology itself to help

00:03:33.729 --> 00:03:33.739
we can use technology itself to help
 

00:03:33.739 --> 00:03:35.380
we can use technology itself to help
repair the you know prepare workforces

00:03:35.380 --> 00:03:35.390
repair the you know prepare workforces
 

00:03:35.390 --> 00:03:36.729
repair the you know prepare workforces
for the changes that are coming down the

00:03:36.729 --> 00:03:36.739
for the changes that are coming down the
 

00:03:36.739 --> 00:03:38.440
for the changes that are coming down the
road there's a lot of voices out there

00:03:38.440 --> 00:03:38.450
road there's a lot of voices out there
 

00:03:38.450 --> 00:03:40.270
road there's a lot of voices out there
they're very well regarded voices Elon

00:03:40.270 --> 00:03:40.280
they're very well regarded voices Elon
 

00:03:40.280 --> 00:03:42.130
they're very well regarded voices Elon
Musk's you know the late Stephen Hawking

00:03:42.130 --> 00:03:42.140
Musk's you know the late Stephen Hawking
 

00:03:42.140 --> 00:03:44.500
Musk's you know the late Stephen Hawking
who who talked very eloquently about the

00:03:44.500 --> 00:03:44.510
who who talked very eloquently about the
 

00:03:44.510 --> 00:03:46.810
who who talked very eloquently about the
perils and dangers that we face with

00:03:46.810 --> 00:03:46.820
perils and dangers that we face with
 

00:03:46.820 --> 00:03:48.940
perils and dangers that we face with
with AI I do think we need to consider

00:03:48.940 --> 00:03:48.950
with AI I do think we need to consider
 

00:03:48.950 --> 00:03:50.259
with AI I do think we need to consider
those and we need to think about the

00:03:50.259 --> 00:03:50.269
those and we need to think about the
 

00:03:50.269 --> 00:03:52.569
those and we need to think about the
longer-term implications of AI like like

00:03:52.569 --> 00:03:52.579
longer-term implications of AI like like
 

00:03:52.579 --> 00:03:54.460
longer-term implications of AI like like
we do have any technology every

00:03:54.460 --> 00:03:54.470
we do have any technology every
 

00:03:54.470 --> 00:03:56.080
we do have any technology every
technology that's ever existed from the

00:03:56.080 --> 00:03:56.090
technology that's ever existed from the
 

00:03:56.090 --> 00:03:58.780
technology that's ever existed from the
first stone wedge that the cavemen you

00:03:58.780 --> 00:03:58.790
first stone wedge that the cavemen you
 

00:03:58.790 --> 00:04:01.120
first stone wedge that the cavemen you
know you know carved or the first you

00:04:01.120 --> 00:04:01.130
know you know carved or the first you
 

00:04:01.130 --> 00:04:03.729
know you know carved or the first you
know fire that was lit was used for good

00:04:03.729 --> 00:04:03.739
know fire that was lit was used for good
 

00:04:03.739 --> 00:04:05.319
know fire that was lit was used for good
and it could be used for bad as well ai

00:04:05.319 --> 00:04:05.329
and it could be used for bad as well ai
 

00:04:05.329 --> 00:04:08.050
and it could be used for bad as well ai
is no different and that the thing that

00:04:08.050 --> 00:04:08.060
is no different and that the thing that
 

00:04:08.060 --> 00:04:09.759
is no different and that the thing that
I think concerns people sometimes the

00:04:09.759 --> 00:04:09.769
I think concerns people sometimes the
 

00:04:09.769 --> 00:04:11.800
I think concerns people sometimes the
pace of AI and the ability of AI to make

00:04:11.800 --> 00:04:11.810
pace of AI and the ability of AI to make
 

00:04:11.810 --> 00:04:14.410
pace of AI and the ability of AI to make
decisions you know that that that are

00:04:14.410 --> 00:04:14.420
decisions you know that that that are
 

00:04:14.420 --> 00:04:16.029
decisions you know that that that are
not in the interest of us as human

00:04:16.029 --> 00:04:16.039
not in the interest of us as human
 

00:04:16.039 --> 00:04:18.340
not in the interest of us as human
beings the reality is that the risk of

00:04:18.340 --> 00:04:18.350
beings the reality is that the risk of
 

00:04:18.350 --> 00:04:21.039
beings the reality is that the risk of
that happening is as far away we do we

00:04:21.039 --> 00:04:21.049
that happening is as far away we do we
 

00:04:21.049 --> 00:04:22.480
that happening is as far away we do we
should be thinking about it there are

00:04:22.480 --> 00:04:22.490
should be thinking about it there are
 

00:04:22.490 --> 00:04:24.250
should be thinking about it there are
organizations that are set up to think

00:04:24.250 --> 00:04:24.260
organizations that are set up to think
 

00:04:24.260 --> 00:04:25.839
organizations that are set up to think
about those implications and we support

00:04:25.839 --> 00:04:25.849
about those implications and we support
 

00:04:25.849 --> 00:04:26.680
about those implications and we support
and are involved in some of those

00:04:26.680 --> 00:04:26.690
and are involved in some of those
 

00:04:26.690 --> 00:04:28.810
and are involved in some of those
organizations but that's you know for

00:04:28.810 --> 00:04:28.820
organizations but that's you know for
 

00:04:28.820 --> 00:04:30.170
organizations but that's you know for
the distant future not something

00:04:30.170 --> 00:04:30.180
the distant future not something
 

00:04:30.180 --> 00:04:32.060
the distant future not something
we need to think about in our generation

00:04:32.060 --> 00:04:32.070
we need to think about in our generation
 

00:04:32.070 --> 00:04:35.029
we need to think about in our generation
right now in terms of real serious

00:04:35.029 --> 00:04:35.039
right now in terms of real serious
 

00:04:35.039 --> 00:04:37.279
right now in terms of real serious
consequences the opportunity for us now

00:04:37.279 --> 00:04:37.289
consequences the opportunity for us now
 

00:04:37.289 --> 00:04:39.260
consequences the opportunity for us now
is to think about how do we apply this

00:04:39.260 --> 00:04:39.270
is to think about how do we apply this
 

00:04:39.270 --> 00:04:42.350
is to think about how do we apply this
to to live live more effectively on the

00:04:42.350 --> 00:04:42.360
to to live live more effectively on the
 

00:04:42.360 --> 00:04:44.180
to to live live more effectively on the
planet to better use our resources and

00:04:44.180 --> 00:04:44.190
planet to better use our resources and
 

00:04:44.190 --> 00:04:45.800
planet to better use our resources and
and to operate businesses and

00:04:45.800 --> 00:04:45.810
and to operate businesses and
 

00:04:45.810 --> 00:04:47.900
and to operate businesses and
educational institutions and our

00:04:47.900 --> 00:04:47.910
educational institutions and our
 

00:04:47.910 --> 00:04:48.980
educational institutions and our
governments more effective there's

00:04:48.980 --> 00:04:48.990
governments more effective there's
 

00:04:48.990 --> 00:04:51.080
governments more effective there's
massive opportunity for this so rather

00:04:51.080 --> 00:04:51.090
massive opportunity for this so rather
 

00:04:51.090 --> 00:04:54.140
massive opportunity for this so rather
than be consumed and stuck by you know

00:04:54.140 --> 00:04:54.150
than be consumed and stuck by you know
 

00:04:54.150 --> 00:04:55.580
than be consumed and stuck by you know
what might happen with a technology in

00:04:55.580 --> 00:04:55.590
what might happen with a technology in
 

00:04:55.590 --> 00:04:56.779
what might happen with a technology in
the future we need to think about that

00:04:56.779 --> 00:04:56.789
the future we need to think about that
 

00:04:56.789 --> 00:04:58.400
the future we need to think about that
prepare for it but let's apply the

00:04:58.400 --> 00:04:58.410
prepare for it but let's apply the
 

00:04:58.410 --> 00:05:00.710
prepare for it but let's apply the
technology in a responsible way which is

00:05:00.710 --> 00:05:00.720
technology in a responsible way which is
 

00:05:00.720 --> 00:05:02.360
technology in a responsible way which is
what we talked about in human plus

00:05:02.360 --> 00:05:02.370
what we talked about in human plus
 

00:05:02.370 --> 00:05:05.480
what we talked about in human plus
machine to solve these problems today

00:05:05.480 --> 00:05:05.490
machine to solve these problems today
 

00:05:05.490 --> 00:05:08.730
machine to solve these problems today
[Music]

