WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.946
[MUSIC PLAYING]

00:00:08.347 --> 00:00:10.390
IDO GREEN: Hello, and
welcome, everyone.

00:00:10.390 --> 00:00:12.160
Thank you for joining us.

00:00:12.160 --> 00:00:14.286
We are excited to be
here, and we really

00:00:14.286 --> 00:00:15.660
appreciate the
effort, especially

00:00:15.660 --> 00:00:16.980
at that hour after lunch.

00:00:16.980 --> 00:00:20.700
We know it's not as easy
as the early morning hours.

00:00:20.700 --> 00:00:22.800
We're here today
to share with you

00:00:22.800 --> 00:00:25.890
what's new and cool and
interesting in the new platform

00:00:25.890 --> 00:00:28.870
of the Google Assistant.

00:00:28.870 --> 00:00:30.950
At Google, as we heard
in the keynote earlier,

00:00:30.950 --> 00:00:34.104
we believe that the
future is AI first.

00:00:34.104 --> 00:00:35.020
And what does it mean?

00:00:35.020 --> 00:00:37.920
It means that we invested
a lot in machine learning,

00:00:37.920 --> 00:00:40.850
natural language processing,
and understanding context

00:00:40.850 --> 00:00:42.560
and what the user want.

00:00:42.560 --> 00:00:45.370
And it's all boiled down and
coming into that assistant.

00:00:45.370 --> 00:00:49.430
It's basically a way for you to
have a conversation with Google

00:00:49.430 --> 00:00:51.410
and get things done.

00:00:51.410 --> 00:00:53.870
And the beauty of
this new platform

00:00:53.870 --> 00:00:55.970
is that basically
it's everywhere.

00:00:55.970 --> 00:00:59.420
So from really small
devices up to cars.

00:00:59.420 --> 00:01:02.450
And very soon we're going
to have many more devices.

00:01:02.450 --> 00:01:06.290
It's going to be everywhere.

00:01:06.290 --> 00:01:07.010
Why?

00:01:07.010 --> 00:01:09.530
It's one of the most
important question

00:01:09.530 --> 00:01:12.440
that every time when
we have a new platform

00:01:12.440 --> 00:01:15.030
or a new system we ask ourself.

00:01:15.030 --> 00:01:18.140
And there are obvious
questions and obvious answers

00:01:18.140 --> 00:01:22.980
to this new platform, like it's
write once, run everywhere.

00:01:22.980 --> 00:01:23.910
It's very efficient.

00:01:23.910 --> 00:01:26.741
It is a great way
to get things done.

00:01:26.741 --> 00:01:28.490
But to me personally,
there are two things

00:01:28.490 --> 00:01:30.260
that I think if I
were in your shoes,

00:01:30.260 --> 00:01:32.240
I would look at
it very carefully.

00:01:32.240 --> 00:01:34.280
One is the productivity aspect.

00:01:34.280 --> 00:01:37.760
You could really help your
users interact with your service

00:01:37.760 --> 00:01:40.310
and get to the result
that they want efficiently

00:01:40.310 --> 00:01:42.860
and without too much trouble.

00:01:42.860 --> 00:01:45.470
And second, like the
photo up there, it's

00:01:45.470 --> 00:01:47.960
a blue ocean right now, so
think about the early days

00:01:47.960 --> 00:01:50.870
in each big new revolution.

00:01:50.870 --> 00:01:52.520
You could be one
of the first there.

00:01:52.520 --> 00:01:55.460
And we'll touch a
bit on it later.

00:01:55.460 --> 00:01:58.730
It's a great way for you to
be there with a quality app

00:01:58.730 --> 00:02:02.200
and then reach more users.

00:02:02.200 --> 00:02:03.960
So what does it mean
for us as developers?

00:02:03.960 --> 00:02:06.120
There are a lot of new terms.

00:02:06.120 --> 00:02:08.940
And it's great to understand
at the beginning what we mean

00:02:08.940 --> 00:02:11.009
with each and every component.

00:02:11.009 --> 00:02:13.500
So first and foremost,
we have the surfaces.

00:02:13.500 --> 00:02:15.760
Right now we have the
Google Home device,

00:02:15.760 --> 00:02:17.910
which is a smart speaker
that the Assistant

00:02:17.910 --> 00:02:19.090
is embedded in it.

00:02:19.090 --> 00:02:21.960
And it lets you talk and
get things done with it.

00:02:21.960 --> 00:02:24.540
And of course, mobile
devices, Android, iPhones--

00:02:24.540 --> 00:02:26.040
it's embedded there.

00:02:26.040 --> 00:02:28.500
The Google Assistant is
basically the conversation

00:02:28.500 --> 00:02:30.570
that you are doing with Google.

00:02:30.570 --> 00:02:32.850
And like we saw in
the impressive demos

00:02:32.850 --> 00:02:35.220
in the keynote, lots of
new interesting things

00:02:35.220 --> 00:02:38.190
to communicate and to
express what are the intent

00:02:38.190 --> 00:02:40.860
and what we want to do and get
the results very efficiently

00:02:40.860 --> 00:02:42.860
and quickly back to you.

00:02:42.860 --> 00:02:47.460
Action on Google is the way
to expose this new Assistant

00:02:47.460 --> 00:02:49.530
platform to
third-party developers,

00:02:49.530 --> 00:02:54.000
to you, so you could tap into
the investment that we did

00:02:54.000 --> 00:02:56.430
and leverage it
when you are coming

00:02:56.430 --> 00:02:58.320
to serve your
services and product

00:02:58.320 --> 00:03:02.630
and you hand them to your users.

00:03:02.630 --> 00:03:05.650
The major important
aspect here to remember

00:03:05.650 --> 00:03:08.500
is that when you
interact with the system,

00:03:08.500 --> 00:03:10.130
it's very straightforward.

00:03:10.130 --> 00:03:11.260
It's a conversation.

00:03:11.260 --> 00:03:15.610
And when you want to handle
your service and your users,

00:03:15.610 --> 00:03:18.530
you basically burst with OK,
Google, or hey, Google, talk

00:03:18.530 --> 00:03:22.990
to, and then you'll give it
your specific brand name or app

00:03:22.990 --> 00:03:26.800
so your users from that on
will move and be in your hands,

00:03:26.800 --> 00:03:30.130
and you could interact
with them and do--

00:03:30.130 --> 00:03:32.050
and help them reach the
goal that you want to.

00:03:34.990 --> 00:03:37.120
So how does the
Assistant app work?

00:03:37.120 --> 00:03:39.190
And again, it's very
important the beginning

00:03:39.190 --> 00:03:42.940
to have the basic understanding
of the moving parts here.

00:03:42.940 --> 00:03:45.200
And then everything
become much clearer.

00:03:45.200 --> 00:03:47.170
So as you can see
here, in our example,

00:03:47.170 --> 00:03:50.100
we did a small demo app that
is called Personal Chef that

00:03:50.100 --> 00:03:54.340
give you suggestion to a
recipe based on your flavor

00:03:54.340 --> 00:03:57.280
and what you feel like
in the right moment.

00:03:57.280 --> 00:04:00.740
And when you burst and say, OK,
Google, talk to Personal Chef,

00:04:00.740 --> 00:04:03.370
basically you're going
to Actions on Google

00:04:03.370 --> 00:04:07.660
and then the magic of
language understanding

00:04:07.660 --> 00:04:10.440
and gathering out from
what the user said

00:04:10.440 --> 00:04:13.930
their exact intents and
entities is being done for you.

00:04:13.930 --> 00:04:17.170
And from that moment, you
basically go to your service--

00:04:17.170 --> 00:04:20.470
here it will be Google Cloud,
but it could be any cloud

00:04:20.470 --> 00:04:21.640
service out there--

00:04:21.640 --> 00:04:23.470
and the user will
be in your hand,

00:04:23.470 --> 00:04:26.700
so you'll need to interpret
what they wanted to

00:04:26.700 --> 00:04:30.940
and return back the text
that, again, Actions on Google

00:04:30.940 --> 00:04:33.940
will take and produce
a speech out of it.

00:04:33.940 --> 00:04:36.820
So all the interaction
with the users

00:04:36.820 --> 00:04:39.760
will be done through that layer
that's basically giving you

00:04:39.760 --> 00:04:43.150
the ability to work
with nice JSON file

00:04:43.150 --> 00:04:46.210
in your server that is easy
to interpret and work with.

00:04:46.210 --> 00:04:49.360
But the heavy lifting
of combining and going

00:04:49.360 --> 00:04:51.805
from text to speech is being
done through that layer.

00:04:54.960 --> 00:04:59.750
I think the best will be just
to look at this show demo.

00:04:59.750 --> 00:05:01.610
It will make very
clear what I just said.

00:05:01.610 --> 00:05:02.276
[VIDEO PLAYBACK]

00:05:02.276 --> 00:05:05.450
- OK, Google, let me
talk to Personal Chef.

00:05:05.450 --> 00:05:05.950
- Sure.

00:05:05.950 --> 00:05:07.340
Here's Personal Chef.

00:05:07.340 --> 00:05:08.100
[BEEP]

00:05:08.100 --> 00:05:10.120
- Hi, I'm your Personal Chef.

00:05:10.120 --> 00:05:11.820
What are you in the mood for?

00:05:11.820 --> 00:05:13.410
- Well, it's kind
of cold outside,

00:05:13.410 --> 00:05:16.230
so I'd like something to
warm me up, like a hot soup.

00:05:16.230 --> 00:05:18.590
And I want it fast.

00:05:18.590 --> 00:05:19.220
- All right.

00:05:19.220 --> 00:05:21.100
What protein would
you like to use?

00:05:21.100 --> 00:05:25.360
- I have some chicken and
also some canned tomatoes.

00:05:25.360 --> 00:05:28.310
- OK, well, I think you should
try the chicken tomato soup

00:05:28.310 --> 00:05:30.880
recipe I found on example.com.

00:05:30.880 --> 00:05:32.010
[BEEP]

00:05:32.010 --> 00:05:33.798
- Sounds good to me.

00:05:33.798 --> 00:05:34.432
[END PLAYBACK]

00:05:34.432 --> 00:05:35.890
DANIEL IMRIE-SITUNAYAKE:
All right,

00:05:35.890 --> 00:05:38.340
so that was a pretty
simple interaction.

00:05:38.340 --> 00:05:41.280
It was like a simple thing to
be having a conversation about.

00:05:41.280 --> 00:05:44.270
But the conversation itself
was actually quite complex.

00:05:44.270 --> 00:05:46.640
So Wayne, our developer
advocate out there,

00:05:46.640 --> 00:05:50.500
said cold, warm, and hot
all in the same sentence,

00:05:50.500 --> 00:05:53.050
but the app managed to capture
the correct one referring

00:05:53.050 --> 00:05:54.040
to the soup.

00:05:54.040 --> 00:05:56.050
So can you imagine
trying to write

00:05:56.050 --> 00:05:58.300
a regular expression or
a parser that would try

00:05:58.300 --> 00:05:59.790
to extract meaning out of that?

00:05:59.790 --> 00:06:02.650
It basically-- there are
so many difficult cases

00:06:02.650 --> 00:06:05.140
that it ends up being pretty
much impossible for anything

00:06:05.140 --> 00:06:07.840
beyond a trivial example.

00:06:07.840 --> 00:06:09.340
So let's look at
some of the ways we

00:06:09.340 --> 00:06:11.470
could build this interaction.

00:06:11.470 --> 00:06:15.460
So one of the options is
to use the conversation API

00:06:15.460 --> 00:06:17.140
and the actions SDK.

00:06:17.140 --> 00:06:18.790
And in this case,
your Assistant app

00:06:18.790 --> 00:06:21.910
receives a request that
contains the spoken text

00:06:21.910 --> 00:06:24.650
from the user as a string.

00:06:24.650 --> 00:06:27.280
So Google handles the
speech recognition for you,

00:06:27.280 --> 00:06:30.700
but you pass the strings,
you generate a response,

00:06:30.700 --> 00:06:34.060
and then Google handles
speaking this back to the user.

00:06:34.060 --> 00:06:35.950
The thing is, as
we just mentioned,

00:06:35.950 --> 00:06:38.480
parsing natural language
can be really difficult.

00:06:38.480 --> 00:06:41.440
So fortunately,
we have some tools

00:06:41.440 --> 00:06:44.200
that make handling that
type of thing a lot easier.

00:06:44.200 --> 00:06:46.087
API.ai is one of these.

00:06:46.087 --> 00:06:48.420
It's a platform that makes
it incredibly straightforward

00:06:48.420 --> 00:06:51.430
to build conversational
experiences.

00:06:51.430 --> 00:06:54.040
You might not even have to write
any code if you're building

00:06:54.040 --> 00:06:55.642
something fairly simple.

00:06:55.642 --> 00:06:57.850
So we're going to give you
an overview of this today.

00:06:57.850 --> 00:06:59.800
And it's probably
what most of you

00:06:59.800 --> 00:07:01.540
should use if you're
going to implement

00:07:01.540 --> 00:07:03.681
your own assistant app.

00:07:03.681 --> 00:07:07.560
So API.ai basically provides
an intuitive graphical user

00:07:07.560 --> 00:07:10.570
interface to create
conversational experiences.

00:07:10.570 --> 00:07:15.660
So you program in a few example
sentences of the way users

00:07:15.660 --> 00:07:19.080
might express a certain
need, and you can also

00:07:19.080 --> 00:07:22.830
specify the values you
need to get from the user.

00:07:22.830 --> 00:07:25.650
And then we use machine
learning to train a model,

00:07:25.650 --> 00:07:27.360
or we train a machine
learning model,

00:07:27.360 --> 00:07:30.770
to understand these sentences
and manage a conversation.

00:07:30.770 --> 00:07:32.520
So the key part here
is that you no longer

00:07:32.520 --> 00:07:34.370
need to process raw strings.

00:07:34.370 --> 00:07:37.430
API.ai will do that for you.

00:07:37.430 --> 00:07:41.470
So you can see in our
diagram where API fits in.

00:07:41.470 --> 00:07:44.220
It handles the
conversation fulfillment

00:07:44.220 --> 00:07:48.554
in between the Assistant
itself and your back end.

00:07:48.554 --> 00:07:51.680
And API.ai handles the
conversation for you,

00:07:51.680 --> 00:07:53.620
so another way of
looking at this is like,

00:07:53.620 --> 00:07:55.390
the user says something,
so maybe they're

00:07:55.390 --> 00:07:57.340
asking for a certain recipe.

00:07:57.340 --> 00:08:00.700
The Assistant converts
their voice into text,

00:08:00.700 --> 00:08:03.700
and API.ai receives
that text string.

00:08:03.700 --> 00:08:07.120
It will decompose that, figure
out what it actually means,

00:08:07.120 --> 00:08:10.090
and hand you that meaning in
the form of structured data.

00:08:10.090 --> 00:08:11.980
You receive that
in your web hook,

00:08:11.980 --> 00:08:14.860
you do whatever you need to do,
like look it up in the database

00:08:14.860 --> 00:08:17.589
and find a matching recipe,
you build a response,

00:08:17.589 --> 00:08:19.630
and then you pass it back
to the Assistant, which

00:08:19.630 --> 00:08:21.830
will read it out to the user.

00:08:21.830 --> 00:08:25.900
So we're going to show you a
short demo of how we would work

00:08:25.900 --> 00:08:27.690
with API.ai to build this app.

00:08:36.440 --> 00:08:39.400
All right, so in
API.ai, we create

00:08:39.400 --> 00:08:41.710
an intent to
represent each thing

00:08:41.710 --> 00:08:43.880
that the user might want to do.

00:08:43.880 --> 00:08:46.330
So in this case,
we've built an intent

00:08:46.330 --> 00:08:49.750
that covers the user
asking for a recipe.

00:08:49.750 --> 00:08:52.750
So I'm going to open
the intent here.

00:08:52.750 --> 00:08:55.090
So right here you
can see how we've

00:08:55.090 --> 00:08:57.310
provided examples
of different ways

00:08:57.310 --> 00:09:01.060
the user might express
their desire for a recipe.

00:09:01.060 --> 00:09:02.860
And these examples
are used to train

00:09:02.860 --> 00:09:04.540
a machine learning
model that can

00:09:04.540 --> 00:09:06.820
recognize what the user wants.

00:09:06.820 --> 00:09:10.270
And as we add examples,
API.ai will automatically

00:09:10.270 --> 00:09:13.970
pick out important concepts
that are mentioned by the user.

00:09:13.970 --> 00:09:16.300
So the system actually
understands many concepts

00:09:16.300 --> 00:09:18.610
by itself, but you
can add custom,

00:09:18.610 --> 00:09:21.730
domain-specific information,
like in this case, recipe

00:09:21.730 --> 00:09:22.527
ingredients.

00:09:22.527 --> 00:09:24.360
So I'm going to add a
couple of things here.

00:09:26.887 --> 00:09:28.470
IDO GREEN: And as
Dan types, you could

00:09:28.470 --> 00:09:30.870
see that it will
mark immediately

00:09:30.870 --> 00:09:32.380
the entities that
it understands.

00:09:32.380 --> 00:09:34.380
And actually, if it doesn't
understand correctly

00:09:34.380 --> 00:09:36.810
the entity that you wish,
you could mark it for it.

00:09:36.810 --> 00:09:39.690
So you can see here, protein
and dish type immediately

00:09:39.690 --> 00:09:41.760
have been recognized
because we already

00:09:41.760 --> 00:09:43.557
have some examples beneath.

00:09:43.557 --> 00:09:45.390
DANIEL IMRIE-SITUNAYAKE:
So another example.

00:09:50.670 --> 00:09:52.940
So these are all entities
our system knows about,

00:09:52.940 --> 00:09:57.290
and API.ai is able to pluck
those from the user's statement

00:09:57.290 --> 00:09:59.910
and figure out what they mean.

00:09:59.910 --> 00:10:03.990
So we can actually also mark
this information as required.

00:10:03.990 --> 00:10:08.330
So here we've put in a
cooking speed and a dish type.

00:10:08.330 --> 00:10:11.900
Down here, we've actually
marked dish type and protein

00:10:11.900 --> 00:10:14.430
and vegetable as required.

00:10:14.430 --> 00:10:16.520
So in this case, our
app is automatically

00:10:16.520 --> 00:10:19.060
going to ask the user if
they didn't mention it.

00:10:19.060 --> 00:10:20.810
So I'm going to save
the intent, and we'll

00:10:20.810 --> 00:10:23.340
wait one moment for it to train
the machine learning model.

00:10:34.670 --> 00:10:36.320
OK, so training's completed.

00:10:36.320 --> 00:10:39.190
So now I'm going to enter
in our test console, which

00:10:39.190 --> 00:10:42.770
is kind of how we work while
we're developing the agent.

00:10:42.770 --> 00:10:47.800
I can just enter in, I
want something with beef.

00:10:47.800 --> 00:10:51.280
So in this case I'm
specifying a protein,

00:10:51.280 --> 00:10:55.330
but I didn't specify a
vegetable or a dish type.

00:10:55.330 --> 00:10:58.810
So the system actually knows
that it expects those things,

00:10:58.810 --> 00:11:01.690
and it should prompt for them
if they weren't supplied.

00:11:01.690 --> 00:11:03.250
So I can now say potato.

00:11:06.440 --> 00:11:09.247
And I still need to know a
dish type, so what kind of dish

00:11:09.247 --> 00:11:09.830
do you prefer?

00:11:09.830 --> 00:11:12.740
Let's have a main course.

00:11:12.740 --> 00:11:17.140
And so now this is where we
would hand off to our back end,

00:11:17.140 --> 00:11:18.810
and we would use
that structured data

00:11:18.810 --> 00:11:21.720
to look up a recipe
in the database

00:11:21.720 --> 00:11:23.680
and return it to the user.

00:11:23.680 --> 00:11:26.430
So it's pretty amazing to be
able to conduct a conversation

00:11:26.430 --> 00:11:28.320
dynamically in this
way, just based

00:11:28.320 --> 00:11:31.422
on the information the
user provided, on the fly,

00:11:31.422 --> 00:11:33.630
without knowing in advance
what they're going to say.

00:11:36.260 --> 00:11:39.650
Once the intent has captured
all of this information,

00:11:39.650 --> 00:11:42.110
it becomes available to
use on your back end, which

00:11:42.110 --> 00:11:44.510
is where you're going to make
stuff happen and generate

00:11:44.510 --> 00:11:45.060
a response.

00:11:45.060 --> 00:11:48.830
So you can see in here, we have
the action that we resolve to,

00:11:48.830 --> 00:11:52.910
which is the intent, and we
have this parameter data that we

00:11:52.910 --> 00:11:55.280
captured from the user's query.

00:11:55.280 --> 00:11:58.010
And so because we have those
values in a structured way,

00:11:58.010 --> 00:11:59.630
we can look those
up in our back end

00:11:59.630 --> 00:12:02.580
and return something useful.

00:12:02.580 --> 00:12:05.030
So once we're done,
it's only a few clicks

00:12:05.030 --> 00:12:07.610
to integrate the app with a
load of different platforms,

00:12:07.610 --> 00:12:10.380
so on the integration tab here.

00:12:10.380 --> 00:12:13.130
So we want to integrate
with Actions on Google

00:12:13.130 --> 00:12:15.770
so that this app is available
via the Google Home.

00:12:15.770 --> 00:12:19.010
So we can just click
here and load up

00:12:19.010 --> 00:12:20.840
the simulator to test this out.

00:12:26.750 --> 00:12:29.230
So the Actions on
Google web simulator

00:12:29.230 --> 00:12:32.500
allows us to test out
our action as if it's

00:12:32.500 --> 00:12:35.340
running on the actual platform.

00:12:35.340 --> 00:12:38.520
So I can say talk
to my test app.

00:12:41.346 --> 00:12:42.288
COMPUTER: All right.

00:12:42.288 --> 00:12:44.180
Let's get the test
version of my test--

00:12:44.180 --> 00:12:46.110
DANIEL IMRIE-SITUNAYAKE:
And so here we're

00:12:46.110 --> 00:12:48.990
seeing how the app
would be responding.

00:12:48.990 --> 00:12:53.450
And I can say let's try
a recipe with chicken.

00:12:58.840 --> 00:13:02.470
And this is now communicating
with API.ai in the same way

00:13:02.470 --> 00:13:04.180
that the Google Home might be.

00:13:04.180 --> 00:13:06.010
So it's asking what
kind of vegetables

00:13:06.010 --> 00:13:08.360
we have, so let's say potato.

00:13:08.360 --> 00:13:11.930
And again, we want
a main course.

00:13:11.930 --> 00:13:14.740
So we can see how it's
really easy to develop

00:13:14.740 --> 00:13:18.040
your app with an API.ai,
test it out in the simulator,

00:13:18.040 --> 00:13:19.690
and then the magical
thing is if you're

00:13:19.690 --> 00:13:23.020
signed in to a Google
Home or the Assistant app

00:13:23.020 --> 00:13:26.320
on your phone, this app is now
immediately available for you

00:13:26.320 --> 00:13:27.040
to test out.

00:13:27.040 --> 00:13:30.340
So you can try it straight away
and make modifications live,

00:13:30.340 --> 00:13:32.050
which is a really fun workflow.

00:13:32.050 --> 00:13:34.420
There's not really
any deployment step.

00:13:34.420 --> 00:13:36.740
So this is the workflow
for building an Action.

00:13:36.740 --> 00:13:39.040
It's super fast and simple,
and you should definitely

00:13:39.040 --> 00:13:41.470
attend the following
workshop session

00:13:41.470 --> 00:13:43.610
on building an assistant app.

00:13:43.610 --> 00:13:46.010
So give it a try
it for yourself.

00:13:46.010 --> 00:13:47.850
So let's switch back
over to our slides

00:13:47.850 --> 00:13:49.600
and talk a little more
about the platform.

00:13:55.560 --> 00:13:58.380
IDO GREEN: So we saw how
we could leverage API.ai,

00:13:58.380 --> 00:14:00.630
and the goal with any
app that we're building

00:14:00.630 --> 00:14:02.580
is actually to
have the wow effect

00:14:02.580 --> 00:14:05.590
and to have the best experience
that we could give our users.

00:14:05.590 --> 00:14:08.910
So that should be it
about what can we do.

00:14:08.910 --> 00:14:11.190
Before we're starting
to code our app,

00:14:11.190 --> 00:14:13.592
and it's actually
quite easy to do,

00:14:13.592 --> 00:14:15.300
I think one of the
most important aspects

00:14:15.300 --> 00:14:17.880
is to dive into all
the great content

00:14:17.880 --> 00:14:22.910
that our team produced
around design voice UIs.

00:14:22.910 --> 00:14:27.510
It's a totally different topic
than what we, or most of us,

00:14:27.510 --> 00:14:29.646
are used to in terms of
graphical user interface.

00:14:29.646 --> 00:14:31.020
And when coming
to voice, there's

00:14:31.020 --> 00:14:33.690
lots of constraints,
limitations that we

00:14:33.690 --> 00:14:36.520
could take into consideration.

00:14:36.520 --> 00:14:39.630
And there's some great
opportunities there,

00:14:39.630 --> 00:14:42.919
like a checklist and tips
that could really put you

00:14:42.919 --> 00:14:44.460
on the right track
when you're coming

00:14:44.460 --> 00:14:47.430
to design the conversation
and to see how those moving

00:14:47.430 --> 00:14:53.350
parts are going to work best
in your specific use case.

00:14:53.350 --> 00:14:55.150
When we're coming
to build it, we

00:14:55.150 --> 00:14:57.220
need to take into
consideration that, like we

00:14:57.220 --> 00:14:59.050
saw in the earlier
slide, we'll have

00:14:59.050 --> 00:15:00.700
different services out there.

00:15:00.700 --> 00:15:04.450
Right now, we have Google
Home and mobile devices,

00:15:04.450 --> 00:15:05.790
and soon we'll have more.

00:15:05.790 --> 00:15:09.537
And on mobile devices, we
could and actually should

00:15:09.537 --> 00:15:11.620
leverage the real estate
that the screen is giving

00:15:11.620 --> 00:15:14.600
us, as as you can see
here in the example,

00:15:14.600 --> 00:15:17.980
let's say that the user
wanted to get to someplace.

00:15:17.980 --> 00:15:20.780
On Google Home, we'll
tell them where to go

00:15:20.780 --> 00:15:24.040
and if we have the screen
and we know it in our app

00:15:24.040 --> 00:15:26.560
that we are now on this
surface, we could give them

00:15:26.560 --> 00:15:28.870
a small map and a
URL so they could

00:15:28.870 --> 00:15:32.440
get the directions immediately.

00:15:32.440 --> 00:15:36.840
At most basic, you can specify
on the screen what is the text

00:15:36.840 --> 00:15:38.580
that you are going to speak?

00:15:38.580 --> 00:15:41.370
And as you can see
here in the example,

00:15:41.370 --> 00:15:45.300
we have what we'll be
telling to the user

00:15:45.300 --> 00:15:46.800
and what will be the text.

00:15:46.800 --> 00:15:49.472
One of the most important
aspects to remember here

00:15:49.472 --> 00:15:51.180
is that we always want
that the text will

00:15:51.180 --> 00:15:54.240
be a summary or the
executive summary of what

00:15:54.240 --> 00:15:55.591
we will talk to the user.

00:15:55.591 --> 00:15:57.090
So if the user is
listening, they're

00:15:57.090 --> 00:15:59.220
getting the full-blown
answer, and if they're just

00:15:59.220 --> 00:16:01.320
skimming with their
eyes the screen,

00:16:01.320 --> 00:16:03.270
they still get to
see what we want

00:16:03.270 --> 00:16:05.700
or the most important aspect,
but like in the example

00:16:05.700 --> 00:16:10.500
here, we don't need to give
them the full-blown answer.

00:16:10.500 --> 00:16:14.970
When we have one of
the options or one

00:16:14.970 --> 00:16:17.760
of the most popular options,
one of the main design tips

00:16:17.760 --> 00:16:20.040
that we're giving is to
lead the conversation

00:16:20.040 --> 00:16:24.180
and to give the user the ability
to very quickly go en route

00:16:24.180 --> 00:16:26.340
to the path that they want to.

00:16:26.340 --> 00:16:28.800
So suggestion chips are one
of the most efficient ways

00:16:28.800 --> 00:16:29.490
to do it.

00:16:29.490 --> 00:16:31.260
If you have some
popular choices,

00:16:31.260 --> 00:16:32.877
you could suggest
it to the user.

00:16:32.877 --> 00:16:34.710
So on the screen, they
have this nice button

00:16:34.710 --> 00:16:35.910
that they could click on.

00:16:35.910 --> 00:16:37.950
And if of course we
are on Google Home,

00:16:37.950 --> 00:16:40.500
you could suggest it,
and the user will choose.

00:16:40.500 --> 00:16:42.300
They don't necessarily
force to it, right?

00:16:42.300 --> 00:16:44.790
They could choose, in this
example, any other number that

00:16:44.790 --> 00:16:45.670
is out there.

00:16:45.670 --> 00:16:48.270
But at least we suggest
to them what to do.

00:16:48.270 --> 00:16:50.520
And in our example,
it could be suggesting

00:16:50.520 --> 00:16:53.650
the most common vegetables that
are appearing in the recipe.

00:16:56.180 --> 00:17:00.260
Basic cards are actually
quite holistic and complex,

00:17:00.260 --> 00:17:02.630
in the sense that they are
giving the full-blown image,

00:17:02.630 --> 00:17:06.020
URL, and text, and its
giving you the ability

00:17:06.020 --> 00:17:08.310
to extend the experience.

00:17:08.310 --> 00:17:11.540
So in our case, we could show
a nice big photo of the dish

00:17:11.540 --> 00:17:13.730
itself and to give
the user a link

00:17:13.730 --> 00:17:16.369
so they could open
it in their app

00:17:16.369 --> 00:17:18.520
and see what we
are going to cook.

00:17:20.962 --> 00:17:22.420
There are many
cases where you want

00:17:22.420 --> 00:17:25.930
to set and show the
user a list of things

00:17:25.930 --> 00:17:28.810
and give the user a
visual selection of what's

00:17:28.810 --> 00:17:32.140
going on with the
different type of things

00:17:32.140 --> 00:17:33.510
that they could choose from.

00:17:33.510 --> 00:17:35.650
And the carousels
show big images.

00:17:35.650 --> 00:17:37.110
The main difference
is that we have

00:17:37.110 --> 00:17:39.310
a more limited set of items.

00:17:39.310 --> 00:17:42.900
And with lists, we'll have
smaller images and much longer

00:17:42.900 --> 00:17:45.610
lists that we could
show the user what

00:17:45.610 --> 00:17:46.930
are the different options.

00:17:46.930 --> 00:17:48.430
And in this example,
you could think

00:17:48.430 --> 00:17:50.560
about showing a couple
of different options

00:17:50.560 --> 00:17:52.127
for a type of dish.

00:17:52.127 --> 00:17:54.210
And that will be-- let's
say that the user already

00:17:54.210 --> 00:17:55.720
chose a chicken salad.

00:17:55.720 --> 00:17:59.380
We could use the
carousel to offer

00:17:59.380 --> 00:18:03.150
three, four different types
of recipes for chicken salad.

00:18:03.150 --> 00:18:06.970
On the other hand, if we want
to show a dozen different dishes

00:18:06.970 --> 00:18:09.190
that feature chicken
as an ingredient,

00:18:09.190 --> 00:18:11.539
we could use the list.

00:18:11.539 --> 00:18:13.080
DANIEL IMRIE-SITUNAYAKE:
So you might

00:18:13.080 --> 00:18:16.350
have a conversation where you
need to know the user's name

00:18:16.350 --> 00:18:17.430
or location.

00:18:17.430 --> 00:18:20.220
So one example might be if
you were helping the user find

00:18:20.220 --> 00:18:22.200
a local bookstore,
and you want to know

00:18:22.200 --> 00:18:24.330
the zip code or the postcode.

00:18:24.330 --> 00:18:27.030
So you can use our SDK
to request permissions

00:18:27.030 --> 00:18:29.670
for the name, the
coarse location,

00:18:29.670 --> 00:18:32.280
and the precise
location of the user.

00:18:32.280 --> 00:18:34.020
And when you invoke
this function,

00:18:34.020 --> 00:18:36.480
the Assistant's going to
ask the user for permission

00:18:36.480 --> 00:18:37.930
in the voice of your app.

00:18:37.930 --> 00:18:39.810
So it's really seamless.

00:18:39.810 --> 00:18:43.920
The name is the name of the user
who's registered to the device.

00:18:43.920 --> 00:18:47.550
The precise location is
the exact GPS coordinates

00:18:47.550 --> 00:18:48.780
and the street address.

00:18:48.780 --> 00:18:51.750
And then the coarse location is
just the zip code or postcode

00:18:51.750 --> 00:18:53.970
and the city.

00:18:53.970 --> 00:18:56.700
If you'd like to link a
user with their account

00:18:56.700 --> 00:19:00.050
on your own service and
you have an OAuth server,

00:19:00.050 --> 00:19:03.120
the Google Assistant can prompt
users to link their account.

00:19:03.120 --> 00:19:04.950
So at this point,
the requesting user

00:19:04.950 --> 00:19:07.500
is going to receive a card at
the top of their Google Home

00:19:07.500 --> 00:19:11.160
app on their phone that provides
a link to your login page.

00:19:11.160 --> 00:19:14.730
And they can follow that,
log into your service.

00:19:14.730 --> 00:19:16.980
Once the user's completed
the account linking flow

00:19:16.980 --> 00:19:19.380
on your web app, they
can invoke your Action,

00:19:19.380 --> 00:19:23.160
and your Action can authenticate
calls to your services

00:19:23.160 --> 00:19:24.570
through our API.

00:19:24.570 --> 00:19:28.920
So it's important that you
provide the OAuth end point.

00:19:28.920 --> 00:19:33.480
Right now, as part of
the approvals process,

00:19:33.480 --> 00:19:38.580
we basically want you to
run your own OAuth server.

00:19:38.580 --> 00:19:41.100
So if your experience
involves shopping or payments,

00:19:41.100 --> 00:19:43.050
we support rich
transactions, and we allow

00:19:43.050 --> 00:19:44.742
you to accept user payments.

00:19:44.742 --> 00:19:46.200
And the really cool
part about this

00:19:46.200 --> 00:19:48.930
is that customers can use
whatever payment information

00:19:48.930 --> 00:19:50.550
they have on file with Google.

00:19:50.550 --> 00:19:52.230
So payments can be super easy.

00:19:52.230 --> 00:19:54.360
There's no need to fumble
around with a credit card

00:19:54.360 --> 00:19:56.190
or read numbers out
loud, although they

00:19:56.190 --> 00:19:59.880
can use your payment
solution if you prefer.

00:19:59.880 --> 00:20:01.470
Transaction supports
are shopping

00:20:01.470 --> 00:20:05.400
carts, delivery options,
order summary, and payments.

00:20:05.400 --> 00:20:09.430
The user can see a history
of all their transactions.

00:20:09.430 --> 00:20:11.760
The Assistant also
supports home automation

00:20:11.760 --> 00:20:13.540
via our Smart Home integration.

00:20:13.540 --> 00:20:16.270
So if you're a device
maker, you can easily

00:20:16.270 --> 00:20:19.240
integrate your existing
devices with our Home Graph.

00:20:19.240 --> 00:20:21.760
And the Home Graph
basically knows the state

00:20:21.760 --> 00:20:23.680
of all connected
devices, so that when

00:20:23.680 --> 00:20:25.840
you're asked to dim the
lights a little bit,

00:20:25.840 --> 00:20:28.330
it knows how to do
that intelligently.

00:20:28.330 --> 00:20:30.220
So there's kind of
endless possibilities

00:20:30.220 --> 00:20:32.320
around how you can allow
the user to interact

00:20:32.320 --> 00:20:34.790
with home automation.

00:20:34.790 --> 00:20:37.650
We've also announced the
Google Assistant SDK,

00:20:37.650 --> 00:20:40.760
which enables you to embed the
Assistant in your own custom

00:20:40.760 --> 00:20:42.020
hardware projects.

00:20:42.020 --> 00:20:45.620
So "MagPi" magazine announced
the AIY Projects kit

00:20:45.620 --> 00:20:47.780
from Google, which
provides a cardboard

00:20:47.780 --> 00:20:50.900
housing with a button, a
speaker, and a microphone,

00:20:50.900 --> 00:20:53.270
and it wraps around
a Raspberry Pi

00:20:53.270 --> 00:20:56.130
and uses the Google
Assistant SDK.

00:20:56.130 --> 00:20:59.390
And at Google I/O, we
demoed this mocktails mixer

00:20:59.390 --> 00:21:03.400
from our partner Deeplocal,
which embeds the Assistant SDK,

00:21:03.400 --> 00:21:05.240
and you can walk
up to the device,

00:21:05.240 --> 00:21:07.010
tell it what kind
of drink you want,

00:21:07.010 --> 00:21:08.570
and it will mix it up for you.

00:21:08.570 --> 00:21:11.340
So you can embed the Assistant
into pretty much any hardware

00:21:11.340 --> 00:21:11.840
device.

00:21:15.374 --> 00:21:17.290
IDO GREEN: After we've
built our Assistant up,

00:21:17.290 --> 00:21:20.950
it's time to reach user and
see how we could drive traffic

00:21:20.950 --> 00:21:23.590
to it and what are there
different options for us

00:21:23.590 --> 00:21:25.160
at the moment.

00:21:25.160 --> 00:21:28.390
So the basic way to
invoke the app is to--

00:21:28.390 --> 00:21:30.140
after, of course,
you've submitted the app

00:21:30.140 --> 00:21:31.570
and it passed the review--

00:21:31.570 --> 00:21:35.170
is to provide a set of triggers
that the invocation knows.

00:21:35.170 --> 00:21:38.050
So when you'll say, OK,
Google, talk to, in our case,

00:21:38.050 --> 00:21:41.650
Personal Chef, it could
be another one or two

00:21:41.650 --> 00:21:44.020
different invocations,
and the user

00:21:44.020 --> 00:21:48.460
will ask according to your
brand name or to your app name,

00:21:48.460 --> 00:21:49.780
and it will invoke it.

00:21:49.780 --> 00:21:51.130
We also support deep linking.

00:21:51.130 --> 00:21:53.420
So as you can see
here in the example,

00:21:53.420 --> 00:21:56.200
you could say in one sentence
to talk to Personal Chef

00:21:56.200 --> 00:22:00.440
and get the recipe of the
hot soup or something else.

00:22:00.440 --> 00:22:03.520
There are also the
ability to ask Google

00:22:03.520 --> 00:22:05.470
for I want to work out.

00:22:05.470 --> 00:22:09.040
I want a yoga or something
more [INAUDIBLE]..

00:22:09.040 --> 00:22:11.980
And then obviously, if you
have a quality good assistant

00:22:11.980 --> 00:22:14.567
app in the directory,
Google will surface it.

00:22:14.567 --> 00:22:16.150
And that's come to
the point that it's

00:22:16.150 --> 00:22:18.110
a blue ocean and great
opportunities out there.

00:22:20.700 --> 00:22:23.190
Last but not least, we
have a full directory

00:22:23.190 --> 00:22:26.836
of apps that are sitting and
living inside the Assistant.

00:22:26.836 --> 00:22:28.710
On the top right corner,
you have the ability

00:22:28.710 --> 00:22:30.570
to click and open the directory.

00:22:30.570 --> 00:22:35.550
And again, it's opening and give
the user based on categories

00:22:35.550 --> 00:22:37.020
and based on what
the system thinks

00:22:37.020 --> 00:22:39.186
the user will enjoy most.

00:22:39.186 --> 00:22:40.560
What are the
different variations

00:22:40.560 --> 00:22:43.144
of apps that are out
there to try and use?

00:22:43.144 --> 00:22:44.060
There are quite a lot.

00:22:44.060 --> 00:22:47.940
So I really suggest you
to check some of them out.

00:22:47.940 --> 00:22:49.390
You could and should link.

00:22:49.390 --> 00:22:51.120
And we have a deep
link so you could,

00:22:51.120 --> 00:22:55.980
if you have a web
service or another app,

00:22:55.980 --> 00:22:59.190
you could always link from
it to the Assistant app.

00:22:59.190 --> 00:23:02.190
So in lots of cases
where the service might

00:23:02.190 --> 00:23:04.930
be more efficient for your
users from the Assistant app,

00:23:04.930 --> 00:23:06.570
I highly encourage
you to link to it.

00:23:06.570 --> 00:23:08.795
And then of course,
they could use it.

00:23:08.795 --> 00:23:10.170
DANIEL IMRIE-SITUNAYAKE:
So we've

00:23:10.170 --> 00:23:12.679
seen all this awesome stuff
you can do with the Actions

00:23:12.679 --> 00:23:14.220
platform, but let's
talk a little bit

00:23:14.220 --> 00:23:16.560
about how you can get started.

00:23:16.560 --> 00:23:20.790
So we've got a series of videos
that cover a lot of content

00:23:20.790 --> 00:23:23.010
on how to get up and
running with the Assistant.

00:23:23.010 --> 00:23:24.390
So you've got an
intro video that

00:23:24.390 --> 00:23:27.660
explains high-level concepts and
goes through the Personal Chef

00:23:27.660 --> 00:23:28.740
example.

00:23:28.740 --> 00:23:31.336
There's a video dedicated to
conversation design, which

00:23:31.336 --> 00:23:33.210
is definitely worth
checking out as you start

00:23:33.210 --> 00:23:35.790
to design and build your app.

00:23:35.790 --> 00:23:38.820
And we've also got a screencast
of every single step needed

00:23:38.820 --> 00:23:42.510
to build our Personal
Chef example.

00:23:42.510 --> 00:23:44.550
We also have several
code labs that

00:23:44.550 --> 00:23:46.740
will guide you through the
experience of designing

00:23:46.740 --> 00:23:50.750
and implementing your
first assistant app.

00:23:50.750 --> 00:23:53.470
And finally, when you want to
discuss this stuff with other

00:23:53.470 --> 00:23:56.260
developers, we have a really
active Actions on Google

00:23:56.260 --> 00:23:58.570
Developer Community on Google+.

00:23:58.570 --> 00:24:00.430
So we post regularly
there to keep you up

00:24:00.430 --> 00:24:02.950
to date with the latest
news, and we answer questions

00:24:02.950 --> 00:24:04.080
from developers.

00:24:04.080 --> 00:24:06.500
We've also got a great
community on Stack Overflow

00:24:06.500 --> 00:24:09.220
if you have technical questions.

00:24:09.220 --> 00:24:12.600
After us, there's really great
talk by Sachit and Shuyang

00:24:12.600 --> 00:24:15.040
that will guide you in
much more detail in how

00:24:15.040 --> 00:24:16.510
to build an assistant app.

00:24:16.510 --> 00:24:19.030
So you should definitely head
upstairs and check that out

00:24:19.030 --> 00:24:20.380
after this talk.

00:24:20.380 --> 00:24:24.250
And our whole team of assistant
dev rail people is here,

00:24:24.250 --> 00:24:26.770
so please feel free to
come by and ask questions

00:24:26.770 --> 00:24:27.591
about the platform.

00:24:27.591 --> 00:24:29.590
We can show you the Google
Home, and we can show

00:24:29.590 --> 00:24:31.660
you the Assistant SDK stuff.

00:24:31.660 --> 00:24:35.020
So thank you all for coming and
really encourage you to dive

00:24:35.020 --> 00:24:36.473
into this new platform.

00:24:36.473 --> 00:24:39.371
[APPLAUSE]

00:24:44.386 --> 00:24:46.010
IDO GREEN: Last but
not least, I forgot

00:24:46.010 --> 00:24:47.301
to mention it at the beginning.

00:24:47.301 --> 00:24:49.100
I saw people taking photos.

00:24:49.100 --> 00:24:50.970
All the slides are live.

00:24:50.970 --> 00:24:56.240
You could ping Dan or myself
on any channel that you wish

00:24:56.240 --> 00:24:59.220
and you could get
all the slides there.

00:24:59.220 --> 00:25:01.630
And if you have any questions,
we'll be at our booth

00:25:01.630 --> 00:25:05.210
at the Assistant
App room, so please

00:25:05.210 --> 00:25:09.140
feel free to come over and give
us your feedback and thoughts.

00:25:09.140 --> 00:25:10.130
Thank you very much.

00:25:10.130 --> 00:25:10.730
[APPLAUSE]

00:25:10.730 --> 00:25:12.063
DANIEL IMRIE-SITUNAYAKE: Cheers.

00:25:12.063 --> 00:25:14.680
[MUSIC PLAYING]

