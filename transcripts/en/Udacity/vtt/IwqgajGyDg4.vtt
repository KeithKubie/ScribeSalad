WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:02.650
So, let's summarize what we have learned now. So

00:00:02.650 --> 00:00:05.900
we, we first understood what is information and we

00:00:05.900 --> 00:00:08.290
found out that information can be measured in some

00:00:08.290 --> 00:00:12.030
way and we meausred it in terms of entropy.

00:00:12.030 --> 00:00:14.560
Then we started to understand how we can measure

00:00:14.560 --> 00:00:17.822
the information between two way variables. And there we

00:00:17.822 --> 00:00:21.160
defined terms as, terms like joint entropy, conditional entropy

00:00:21.160 --> 00:00:25.680
and mutual information. And then finally we introduced ourselves

00:00:25.680 --> 00:00:28.140
to a term called a KL divergence, which

00:00:28.140 --> 00:00:30.420
is very famously used as a distance measured between

00:00:30.420 --> 00:00:33.620
two distributions. So this is just a primer

00:00:33.620 --> 00:00:36.320
to information theory and it forms as a base

00:00:36.320 --> 00:00:38.230
to what is required for you to go

00:00:38.230 --> 00:00:40.590
through this machine learning course. If you want to

00:00:40.590 --> 00:00:42.910
learn more about information theory, follow the links

00:00:42.910 --> 00:00:45.450
in, in the Comments sections. And yeah. Thank you.

