WEBVTT
Kind: captions
Language: en

00:00:02.100 --> 00:00:04.730 align:start position:0%
 
let's<00:00:02.790><c> get</c><00:00:02.940><c> started</c><00:00:03.540><c> so</c><00:00:04.230><c> good</c><00:00:04.470><c> morning</c>

00:00:04.730 --> 00:00:04.740 align:start position:0%
let's get started so good morning
 

00:00:04.740 --> 00:00:07.610 align:start position:0%
let's get started so good morning
everyone<00:00:04.979><c> my</c><00:00:05.370><c> name</c><00:00:05.880><c> is</c><00:00:05.910><c> Alva</c><00:00:06.359><c> and</c><00:00:06.540><c> today</c><00:00:07.350><c> we're</c>

00:00:07.610 --> 00:00:07.620 align:start position:0%
everyone my name is Alva and today we're
 

00:00:07.620 --> 00:00:09.530 align:start position:0%
everyone my name is Alva and today we're
going<00:00:07.799><c> to</c><00:00:07.919><c> learn</c><00:00:08.069><c> about</c><00:00:08.400><c> how</c><00:00:08.790><c> deep</c><00:00:09.059><c> learning</c>

00:00:09.530 --> 00:00:09.540 align:start position:0%
going to learn about how deep learning
 

00:00:09.540 --> 00:00:12.850 align:start position:0%
going to learn about how deep learning
can<00:00:10.139><c> be</c><00:00:10.170><c> used</c><00:00:10.559><c> to</c><00:00:10.800><c> build</c><00:00:10.950><c> systems</c><00:00:11.580><c> capable</c><00:00:12.179><c> of</c>

00:00:12.850 --> 00:00:12.860 align:start position:0%
can be used to build systems capable of
 

00:00:12.860 --> 00:00:15.530 align:start position:0%
can be used to build systems capable of
perceiving<00:00:13.860><c> images</c><00:00:14.429><c> and</c><00:00:14.730><c> making</c><00:00:14.940><c> decisions</c>

00:00:15.530 --> 00:00:15.540 align:start position:0%
perceiving images and making decisions
 

00:00:15.540 --> 00:00:19.339 align:start position:0%
perceiving images and making decisions
based<00:00:16.110><c> on</c><00:00:16.380><c> visual</c><00:00:16.980><c> information</c><00:00:18.349><c> neural</c>

00:00:19.339 --> 00:00:19.349 align:start position:0%
based on visual information neural
 

00:00:19.349 --> 00:00:20.839 align:start position:0%
based on visual information neural
networks<00:00:19.710><c> have</c><00:00:19.950><c> really</c><00:00:20.220><c> made</c><00:00:20.430><c> a</c><00:00:20.489><c> tremendous</c>

00:00:20.839 --> 00:00:20.849 align:start position:0%
networks have really made a tremendous
 

00:00:20.849 --> 00:00:23.450 align:start position:0%
networks have really made a tremendous
impact<00:00:21.509><c> in</c><00:00:21.779><c> this</c><00:00:22.169><c> area</c><00:00:22.500><c> over</c><00:00:22.680><c> the</c><00:00:22.890><c> past</c><00:00:23.099><c> 20</c>

00:00:23.450 --> 00:00:23.460 align:start position:0%
impact in this area over the past 20
 

00:00:23.460 --> 00:00:25.310 align:start position:0%
impact in this area over the past 20
years<00:00:23.489><c> and</c><00:00:24.000><c> I</c><00:00:24.239><c> think</c><00:00:24.480><c> to</c><00:00:24.779><c> really</c><00:00:25.020><c> appreciate</c>

00:00:25.310 --> 00:00:25.320 align:start position:0%
years and I think to really appreciate
 

00:00:25.320 --> 00:00:28.490 align:start position:0%
years and I think to really appreciate
how<00:00:26.250><c> and</c><00:00:26.579><c> why</c><00:00:26.730><c> this</c><00:00:26.790><c> has</c><00:00:27.360><c> been</c><00:00:27.779><c> the</c><00:00:27.899><c> case</c><00:00:28.110><c> it</c>

00:00:28.490 --> 00:00:28.500 align:start position:0%
how and why this has been the case it
 

00:00:28.500 --> 00:00:32.209 align:start position:0%
how and why this has been the case it
helps<00:00:29.220><c> to</c><00:00:29.579><c> take</c><00:00:29.820><c> a</c><00:00:29.849><c> step</c><00:00:30.210><c> back</c><00:00:30.540><c> way</c><00:00:31.169><c> back</c><00:00:31.410><c> to</c>

00:00:32.209 --> 00:00:32.219 align:start position:0%
helps to take a step back way back to
 

00:00:32.219 --> 00:00:34.310 align:start position:0%
helps to take a step back way back to
five<00:00:32.579><c> hundred</c><00:00:32.790><c> forty</c><00:00:33.180><c> million</c><00:00:33.450><c> years</c><00:00:33.780><c> ago</c><00:00:34.050><c> and</c>

00:00:34.310 --> 00:00:34.320 align:start position:0%
five hundred forty million years ago and
 

00:00:34.320 --> 00:00:37.250 align:start position:0%
five hundred forty million years ago and
the<00:00:34.950><c> Cambrian</c><00:00:35.400><c> explosion</c><00:00:35.520><c> where</c><00:00:36.420><c> biologists</c>

00:00:37.250 --> 00:00:37.260 align:start position:0%
the Cambrian explosion where biologists
 

00:00:37.260 --> 00:00:39.410 align:start position:0%
the Cambrian explosion where biologists
traced<00:00:37.920><c> the</c><00:00:38.130><c> evolutionary</c><00:00:38.760><c> origins</c><00:00:39.270><c> of</c>

00:00:39.410 --> 00:00:39.420 align:start position:0%
traced the evolutionary origins of
 

00:00:39.420 --> 00:00:42.590 align:start position:0%
traced the evolutionary origins of
vision<00:00:39.770><c> the</c><00:00:40.770><c> reason</c><00:00:41.160><c> vision</c><00:00:41.610><c> seemed</c><00:00:41.940><c> so</c><00:00:42.330><c> easy</c>

00:00:42.590 --> 00:00:42.600 align:start position:0%
vision the reason vision seemed so easy
 

00:00:42.600 --> 00:00:44.810 align:start position:0%
vision the reason vision seemed so easy
for<00:00:42.930><c> us</c><00:00:43.050><c> as</c><00:00:43.320><c> humans</c><00:00:43.770><c> is</c><00:00:43.890><c> because</c><00:00:44.310><c> we</c><00:00:44.490><c> have</c><00:00:44.610><c> had</c>

00:00:44.810 --> 00:00:44.820 align:start position:0%
for us as humans is because we have had
 

00:00:44.820 --> 00:00:48.410 align:start position:0%
for us as humans is because we have had
five<00:00:45.480><c> hundred</c><00:00:46.080><c> forty</c><00:00:46.500><c> million</c><00:00:46.740><c> years</c><00:00:47.190><c> of</c><00:00:47.430><c> data</c>

00:00:48.410 --> 00:00:48.420 align:start position:0%
five hundred forty million years of data
 

00:00:48.420 --> 00:00:51.290 align:start position:0%
five hundred forty million years of data
for<00:00:49.170><c> evolution</c><00:00:49.680><c> to</c><00:00:49.710><c> train</c><00:00:50.070><c> on</c><00:00:50.250><c> compare</c><00:00:51.120><c> that</c>

00:00:51.290 --> 00:00:51.300 align:start position:0%
for evolution to train on compare that
 

00:00:51.300 --> 00:00:53.990 align:start position:0%
for evolution to train on compare that
to<00:00:51.360><c> bipedal</c><00:00:52.110><c> movement</c><00:00:52.380><c> human</c><00:00:53.370><c> language</c><00:00:53.760><c> and</c>

00:00:53.990 --> 00:00:54.000 align:start position:0%
to bipedal movement human language and
 

00:00:54.000 --> 00:00:57.290 align:start position:0%
to bipedal movement human language and
the<00:00:54.540><c> difference</c><00:00:54.930><c> is</c><00:00:55.110><c> significant</c><00:00:56.300><c> starting</c>

00:00:57.290 --> 00:00:57.300 align:start position:0%
the difference is significant starting
 

00:00:57.300 --> 00:00:59.870 align:start position:0%
the difference is significant starting
in<00:00:57.450><c> around</c><00:00:57.840><c> the</c><00:00:57.989><c> 1960s</c><00:00:58.829><c> there</c><00:00:59.070><c> was</c><00:00:59.220><c> a</c><00:00:59.520><c> surge</c><00:00:59.850><c> in</c>

00:00:59.870 --> 00:00:59.880 align:start position:0%
in around the 1960s there was a surge in
 

00:00:59.880 --> 00:01:03.470 align:start position:0%
in around the 1960s there was a surge in
interest<00:01:00.620><c> in</c><00:01:01.620><c> in</c><00:01:02.160><c> both</c><00:01:02.430><c> the</c><00:01:02.610><c> neural</c><00:01:02.880><c> basis</c><00:01:03.329><c> of</c>

00:01:03.470 --> 00:01:03.480 align:start position:0%
interest in in both the neural basis of
 

00:01:03.480 --> 00:01:06.230 align:start position:0%
interest in in both the neural basis of
vision<00:01:03.810><c> and</c><00:01:04.070><c> in</c><00:01:05.070><c> developing</c><00:01:05.670><c> methods</c><00:01:05.850><c> to</c>

00:01:06.230 --> 00:01:06.240 align:start position:0%
vision and in developing methods to
 

00:01:06.240 --> 00:01:08.450 align:start position:0%
vision and in developing methods to
systematically<00:01:06.960><c> characterize</c><00:01:07.620><c> visual</c>

00:01:08.450 --> 00:01:08.460 align:start position:0%
systematically characterize visual
 

00:01:08.460 --> 00:01:10.670 align:start position:0%
systematically characterize visual
processing<00:01:08.880><c> and</c><00:01:09.270><c> this</c><00:01:09.960><c> eventually</c><00:01:10.440><c> led</c><00:01:10.650><c> to</c>

00:01:10.670 --> 00:01:10.680 align:start position:0%
processing and this eventually led to
 

00:01:10.680 --> 00:01:13.820 align:start position:0%
processing and this eventually led to
computer<00:01:11.520><c> scientists</c><00:01:12.210><c> wondering</c><00:01:13.170><c> about</c><00:01:13.560><c> how</c>

00:01:13.820 --> 00:01:13.830 align:start position:0%
computer scientists wondering about how
 

00:01:13.830 --> 00:01:16.160 align:start position:0%
computer scientists wondering about how
these<00:01:14.040><c> findings</c><00:01:14.640><c> from</c><00:01:14.880><c> neuroscience</c><00:01:15.390><c> could</c>

00:01:16.160 --> 00:01:16.170 align:start position:0%
these findings from neuroscience could
 

00:01:16.170 --> 00:01:17.660 align:start position:0%
these findings from neuroscience could
be<00:01:16.320><c> applied</c><00:01:16.680><c> to</c><00:01:17.040><c> artificial</c><00:01:17.640><c> intelligence</c>

00:01:17.660 --> 00:01:17.670 align:start position:0%
be applied to artificial intelligence
 

00:01:17.670 --> 00:01:20.630 align:start position:0%
be applied to artificial intelligence
and<00:01:18.420><c> one</c><00:01:19.290><c> of</c><00:01:19.409><c> the</c><00:01:19.500><c> biggest</c><00:01:19.799><c> breakthroughs</c><00:01:19.920><c> in</c>

00:01:20.630 --> 00:01:20.640 align:start position:0%
and one of the biggest breakthroughs in
 

00:01:20.640 --> 00:01:22.850 align:start position:0%
and one of the biggest breakthroughs in
our<00:01:21.060><c> understanding</c><00:01:21.510><c> of</c><00:01:21.930><c> the</c><00:01:22.080><c> neural</c><00:01:22.320><c> basis</c><00:01:22.680><c> of</c>

00:01:22.850 --> 00:01:22.860 align:start position:0%
our understanding of the neural basis of
 

00:01:22.860 --> 00:01:25.010 align:start position:0%
our understanding of the neural basis of
vision<00:01:23.130><c> came</c><00:01:23.700><c> from</c><00:01:23.909><c> to</c><00:01:24.120><c> scientists</c><00:01:24.690><c> as</c><00:01:24.780><c> at</c>

00:01:25.010 --> 00:01:25.020 align:start position:0%
vision came from to scientists as at
 

00:01:25.020 --> 00:01:29.000 align:start position:0%
vision came from to scientists as at
Harvard<00:01:25.950><c> Hubel</c><00:01:26.670><c> and</c><00:01:26.700><c> Wiesel</c><00:01:27.060><c> and</c><00:01:27.799><c> they</c><00:01:28.799><c> had</c><00:01:28.979><c> a</c>

00:01:29.000 --> 00:01:29.010 align:start position:0%
Harvard Hubel and Wiesel and they had a
 

00:01:29.010 --> 00:01:31.940 align:start position:0%
Harvard Hubel and Wiesel and they had a
pretty<00:01:29.400><c> simple</c><00:01:29.909><c> experimental</c><00:01:30.479><c> setup</c><00:01:31.229><c> where</c>

00:01:31.940 --> 00:01:31.950 align:start position:0%
pretty simple experimental setup where
 

00:01:31.950 --> 00:01:33.710 align:start position:0%
pretty simple experimental setup where
they<00:01:32.100><c> will</c><00:01:32.280><c> where</c><00:01:33.090><c> they</c><00:01:33.210><c> were</c><00:01:33.270><c> able</c><00:01:33.690><c> to</c>

00:01:33.710 --> 00:01:33.720 align:start position:0%
they will where they were able to
 

00:01:33.720 --> 00:01:37.700 align:start position:0%
they will where they were able to
measure<00:01:34.650><c> visual</c><00:01:35.450><c> neural</c><00:01:36.450><c> activity</c><00:01:36.990><c> in</c><00:01:37.260><c> the</c>

00:01:37.700 --> 00:01:37.710 align:start position:0%
measure visual neural activity in the
 

00:01:37.710 --> 00:01:40.250 align:start position:0%
measure visual neural activity in the
visual<00:01:38.010><c> cortex</c><00:01:38.040><c> of</c><00:01:38.670><c> cats</c><00:01:39.000><c> by</c><00:01:39.750><c> recording</c>

00:01:40.250 --> 00:01:40.260 align:start position:0%
visual cortex of cats by recording
 

00:01:40.260 --> 00:01:43.730 align:start position:0%
visual cortex of cats by recording
directly<00:01:41.150><c> for</c><00:01:42.150><c> the</c><00:01:42.540><c> electrical</c><00:01:43.049><c> signals</c><00:01:43.470><c> from</c>

00:01:43.730 --> 00:01:43.740 align:start position:0%
directly for the electrical signals from
 

00:01:43.740 --> 00:01:47.030 align:start position:0%
directly for the electrical signals from
neurons<00:01:44.580><c> in</c><00:01:44.970><c> this</c><00:01:45.119><c> region</c><00:01:45.479><c> of</c><00:01:45.600><c> the</c><00:01:45.630><c> brain</c><00:01:46.040><c> and</c>

00:01:47.030 --> 00:01:47.040 align:start position:0%
neurons in this region of the brain and
 

00:01:47.040 --> 00:01:48.740 align:start position:0%
neurons in this region of the brain and
they<00:01:47.159><c> displayed</c><00:01:47.610><c> a</c><00:01:47.640><c> simple</c><00:01:48.119><c> stimulus</c><00:01:48.600><c> on</c><00:01:48.720><c> a</c>

00:01:48.740 --> 00:01:48.750 align:start position:0%
they displayed a simple stimulus on a
 

00:01:48.750 --> 00:01:51.560 align:start position:0%
they displayed a simple stimulus on a
screen<00:01:49.110><c> and</c><00:01:49.320><c> then</c><00:01:50.100><c> probed</c><00:01:50.490><c> the</c><00:01:50.820><c> visual</c><00:01:51.119><c> cortex</c>

00:01:51.560 --> 00:01:51.570 align:start position:0%
screen and then probed the visual cortex
 

00:01:51.570 --> 00:01:54.500 align:start position:0%
screen and then probed the visual cortex
to<00:01:51.840><c> see</c><00:01:52.200><c> which</c><00:01:52.439><c> neurons</c><00:01:52.710><c> fired</c><00:01:53.640><c> in</c><00:01:53.970><c> response</c>

00:01:54.500 --> 00:01:54.510 align:start position:0%
to see which neurons fired in response
 

00:01:54.510 --> 00:01:57.800 align:start position:0%
to see which neurons fired in response
to<00:01:54.780><c> that</c><00:01:54.900><c> stimulus</c><00:01:55.760><c> there</c><00:01:56.760><c> were</c><00:01:56.850><c> a</c><00:01:56.909><c> few</c><00:01:57.240><c> key</c>

00:01:57.800 --> 00:01:57.810 align:start position:0%
to that stimulus there were a few key
 

00:01:57.810 --> 00:01:59.929 align:start position:0%
to that stimulus there were a few key
takeaways<00:01:58.409><c> from</c><00:01:58.439><c> this</c><00:01:58.740><c> experiment</c><00:01:59.490><c> that</c><00:01:59.729><c> I'd</c>

00:01:59.929 --> 00:01:59.939 align:start position:0%
takeaways from this experiment that I'd
 

00:01:59.939 --> 00:02:01.580 align:start position:0%
takeaways from this experiment that I'd
like<00:02:00.210><c> you</c><00:02:00.240><c> to</c><00:02:00.509><c> keep</c><00:02:00.659><c> in</c><00:02:00.780><c> mind</c><00:02:00.990><c> as</c><00:02:01.229><c> we</c><00:02:01.409><c> go</c>

00:02:01.580 --> 00:02:01.590 align:start position:0%
like you to keep in mind as we go
 

00:02:01.590 --> 00:02:04.190 align:start position:0%
like you to keep in mind as we go
through<00:02:01.860><c> today's</c><00:02:02.130><c> lecture</c><00:02:02.700><c> the</c><00:02:02.930><c> first</c><00:02:03.930><c> was</c>

00:02:04.190 --> 00:02:04.200 align:start position:0%
through today's lecture the first was
 

00:02:04.200 --> 00:02:05.510 align:start position:0%
through today's lecture the first was
that<00:02:04.350><c> they</c><00:02:04.500><c> found</c><00:02:04.710><c> that</c><00:02:05.009><c> there</c><00:02:05.250><c> was</c><00:02:05.340><c> an</c>

00:02:05.510 --> 00:02:05.520 align:start position:0%
that they found that there was an
 

00:02:05.520 --> 00:02:08.960 align:start position:0%
that they found that there was an
mechanism<00:02:06.420><c> for</c><00:02:06.900><c> spatial</c><00:02:07.380><c> invariance</c><00:02:08.009><c> they</c>

00:02:08.960 --> 00:02:08.970 align:start position:0%
mechanism for spatial invariance they
 

00:02:08.970 --> 00:02:11.029 align:start position:0%
mechanism for spatial invariance they
could<00:02:09.149><c> record</c><00:02:09.509><c> neural</c><00:02:09.869><c> responses</c><00:02:10.469><c> to</c>

00:02:11.029 --> 00:02:11.039 align:start position:0%
could record neural responses to
 

00:02:11.039 --> 00:02:13.729 align:start position:0%
could record neural responses to
particular<00:02:11.549><c> patterns</c><00:02:12.029><c> and</c><00:02:12.959><c> this</c><00:02:13.499><c> was</c>

00:02:13.729 --> 00:02:13.739 align:start position:0%
particular patterns and this was
 

00:02:13.739 --> 00:02:15.470 align:start position:0%
particular patterns and this was
constant<00:02:14.249><c> regardless</c><00:02:14.370><c> of</c><00:02:14.909><c> the</c><00:02:15.029><c> location</c>

00:02:15.470 --> 00:02:15.480 align:start position:0%
constant regardless of the location
 

00:02:15.480 --> 00:02:18.680 align:start position:0%
constant regardless of the location
of<00:02:15.659><c> those</c><00:02:15.780><c> patterns</c><00:02:16.080><c> on</c><00:02:16.470><c> the</c><00:02:16.890><c> screen</c><00:02:17.690><c> the</c>

00:02:18.680 --> 00:02:18.690 align:start position:0%
of those patterns on the screen the
 

00:02:18.690 --> 00:02:20.300 align:start position:0%
of those patterns on the screen the
second<00:02:19.080><c> was</c><00:02:19.230><c> that</c><00:02:19.260><c> the</c><00:02:19.709><c> neurons</c><00:02:20.129><c> they</c>

00:02:20.300 --> 00:02:20.310 align:start position:0%
second was that the neurons they
 

00:02:20.310 --> 00:02:22.369 align:start position:0%
second was that the neurons they
recorded<00:02:20.819><c> from</c><00:02:20.970><c> had</c><00:02:21.569><c> what</c><00:02:21.780><c> they</c><00:02:21.930><c> called</c><00:02:22.200><c> a</c>

00:02:22.369 --> 00:02:22.379 align:start position:0%
recorded from had what they called a
 

00:02:22.379 --> 00:02:25.369 align:start position:0%
recorded from had what they called a
receptive<00:02:22.860><c> field</c><00:02:23.510><c> they</c><00:02:24.510><c> certain</c><00:02:25.260><c> neurons</c>

00:02:25.369 --> 00:02:25.379 align:start position:0%
receptive field they certain neurons
 

00:02:25.379 --> 00:02:27.589 align:start position:0%
receptive field they certain neurons
only<00:02:25.860><c> responded</c><00:02:26.459><c> to</c><00:02:26.580><c> certain</c><00:02:26.909><c> regions</c><00:02:27.180><c> of</c><00:02:27.480><c> the</c>

00:02:27.589 --> 00:02:27.599 align:start position:0%
only responded to certain regions of the
 

00:02:27.599 --> 00:02:29.690 align:start position:0%
only responded to certain regions of the
input<00:02:27.989><c> while</c><00:02:28.440><c> others</c><00:02:28.769><c> responded</c><00:02:29.400><c> to</c><00:02:29.549><c> other</c>

00:02:29.690 --> 00:02:29.700 align:start position:0%
input while others responded to other
 

00:02:29.700 --> 00:02:32.570 align:start position:0%
input while others responded to other
regions<00:02:30.650><c> finally</c><00:02:31.650><c> they</c><00:02:31.769><c> were</c><00:02:31.830><c> able</c><00:02:32.069><c> to</c><00:02:32.340><c> tease</c>

00:02:32.570 --> 00:02:32.580 align:start position:0%
regions finally they were able to tease
 

00:02:32.580 --> 00:02:35.119 align:start position:0%
regions finally they were able to tease
out<00:02:32.760><c> that</c><00:02:33.030><c> there</c><00:02:33.239><c> was</c><00:02:33.420><c> a</c><00:02:33.450><c> mapping</c><00:02:34.049><c> a</c><00:02:34.230><c> hierarchy</c>

00:02:35.119 --> 00:02:35.129 align:start position:0%
out that there was a mapping a hierarchy
 

00:02:35.129 --> 00:02:37.430 align:start position:0%
out that there was a mapping a hierarchy
to<00:02:35.400><c> neural</c><00:02:36.150><c> organization</c><00:02:36.360><c> in</c><00:02:37.019><c> the</c><00:02:37.140><c> visual</c>

00:02:37.430 --> 00:02:37.440 align:start position:0%
to neural organization in the visual
 

00:02:37.440 --> 00:02:40.220 align:start position:0%
to neural organization in the visual
cortex<00:02:37.940><c> there</c><00:02:38.940><c> are</c><00:02:39.060><c> cells</c><00:02:39.330><c> that</c><00:02:39.480><c> responded</c><00:02:40.110><c> to</c>

00:02:40.220 --> 00:02:40.230 align:start position:0%
cortex there are cells that responded to
 

00:02:40.230 --> 00:02:42.229 align:start position:0%
cortex there are cells that responded to
more<00:02:40.379><c> simple</c><00:02:40.980><c> images</c><00:02:41.340><c> such</c><00:02:41.549><c> as</c><00:02:41.579><c> rods</c><00:02:41.940><c> and</c>

00:02:42.229 --> 00:02:42.239 align:start position:0%
more simple images such as rods and
 

00:02:42.239 --> 00:02:44.630 align:start position:0%
more simple images such as rods and
rectangles<00:02:42.780><c> and</c><00:02:42.989><c> then</c><00:02:43.099><c> downstream</c><00:02:44.099><c> layers</c><00:02:44.400><c> of</c>

00:02:44.630 --> 00:02:44.640 align:start position:0%
rectangles and then downstream layers of
 

00:02:44.640 --> 00:02:46.940 align:start position:0%
rectangles and then downstream layers of
neurons<00:02:44.819><c> they</c><00:02:45.510><c> use</c><00:02:45.780><c> the</c><00:02:45.959><c> activations</c><00:02:46.620><c> from</c>

00:02:46.940 --> 00:02:46.950 align:start position:0%
neurons they use the activations from
 

00:02:46.950 --> 00:02:48.530 align:start position:0%
neurons they use the activations from
these<00:02:47.159><c> upstream</c><00:02:47.640><c> neurons</c><00:02:48.120><c> in</c><00:02:48.360><c> their</c>

00:02:48.530 --> 00:02:48.540 align:start position:0%
these upstream neurons in their
 

00:02:48.540 --> 00:02:52.699 align:start position:0%
these upstream neurons in their
computation<00:02:51.140><c> cognitive</c><00:02:52.140><c> scientists</c><00:02:52.590><c> and</c>

00:02:52.699 --> 00:02:52.709 align:start position:0%
computation cognitive scientists and
 

00:02:52.709 --> 00:02:54.619 align:start position:0%
computation cognitive scientists and
neuroscientists<00:02:53.430><c> have</c><00:02:53.519><c> since</c><00:02:53.849><c> built</c><00:02:54.359><c> off</c>

00:02:54.619 --> 00:02:54.629 align:start position:0%
neuroscientists have since built off
 

00:02:54.629 --> 00:02:57.140 align:start position:0%
neuroscientists have since built off
this<00:02:54.870><c> early</c><00:02:55.170><c> work</c><00:02:55.319><c> to</c><00:02:55.560><c> indeed</c><00:02:55.890><c> confirm</c><00:02:56.400><c> that</c>

00:02:57.140 --> 00:02:57.150 align:start position:0%
this early work to indeed confirm that
 

00:02:57.150 --> 00:02:59.599 align:start position:0%
this early work to indeed confirm that
the<00:02:57.329><c> visual</c><00:02:57.629><c> cortex</c><00:02:57.870><c> is</c><00:02:58.319><c> organized</c><00:02:59.250><c> into</c>

00:02:59.599 --> 00:02:59.609 align:start position:0%
the visual cortex is organized into
 

00:02:59.609 --> 00:03:02.150 align:start position:0%
the visual cortex is organized into
layers<00:02:59.940><c> and</c><00:03:00.359><c> this</c><00:03:01.140><c> hierarchy</c><00:03:01.829><c> of</c><00:03:01.950><c> layers</c>

00:03:02.150 --> 00:03:02.160 align:start position:0%
layers and this hierarchy of layers
 

00:03:02.160 --> 00:03:04.130 align:start position:0%
layers and this hierarchy of layers
allows<00:03:02.940><c> for</c><00:03:03.239><c> the</c><00:03:03.329><c> recognition</c><00:03:03.629><c> of</c>

00:03:04.130 --> 00:03:04.140 align:start position:0%
allows for the recognition of
 

00:03:04.140 --> 00:03:06.740 align:start position:0%
allows for the recognition of
increasingly<00:03:04.980><c> complex</c><00:03:05.099><c> features</c><00:03:05.970><c> that</c><00:03:06.390><c> for</c>

00:03:06.740 --> 00:03:06.750 align:start position:0%
increasingly complex features that for
 

00:03:06.750 --> 00:03:08.900 align:start position:0%
increasingly complex features that for
example<00:03:06.989><c> allow</c><00:03:07.739><c> us</c><00:03:08.040><c> to</c><00:03:08.430><c> immediately</c>

00:03:08.900 --> 00:03:08.910 align:start position:0%
example allow us to immediately
 

00:03:08.910 --> 00:03:12.619 align:start position:0%
example allow us to immediately
recognize<00:03:09.180><c> the</c><00:03:09.959><c> face</c><00:03:10.140><c> of</c><00:03:10.319><c> a</c><00:03:10.410><c> friend</c><00:03:11.450><c> so</c><00:03:12.450><c> now</c>

00:03:12.619 --> 00:03:12.629 align:start position:0%
recognize the face of a friend so now
 

00:03:12.629 --> 00:03:15.140 align:start position:0%
recognize the face of a friend so now
that<00:03:12.690><c> we've</c><00:03:13.019><c> gotten</c><00:03:13.290><c> a</c><00:03:13.349><c> sense</c><00:03:13.620><c> at</c><00:03:14.010><c> a</c><00:03:14.069><c> very</c><00:03:14.819><c> high</c>

00:03:15.140 --> 00:03:15.150 align:start position:0%
that we've gotten a sense at a very high
 

00:03:15.150 --> 00:03:17.569 align:start position:0%
that we've gotten a sense at a very high
level<00:03:15.180><c> of</c><00:03:15.720><c> how</c><00:03:16.260><c> our</c><00:03:16.560><c> brains</c><00:03:16.920><c> process</c><00:03:17.160><c> visual</c>

00:03:17.569 --> 00:03:17.579 align:start position:0%
level of how our brains process visual
 

00:03:17.579 --> 00:03:20.300 align:start position:0%
level of how our brains process visual
information<00:03:17.959><c> we</c><00:03:18.959><c> can</c><00:03:19.170><c> turn</c><00:03:19.379><c> our</c><00:03:19.590><c> attention</c><00:03:19.620><c> to</c>

00:03:20.300 --> 00:03:20.310 align:start position:0%
information we can turn our attention to
 

00:03:20.310 --> 00:03:24.140 align:start position:0%
information we can turn our attention to
what<00:03:20.579><c> computers</c><00:03:21.329><c> see</c><00:03:22.160><c> how</c><00:03:23.160><c> does</c><00:03:23.519><c> a</c><00:03:23.549><c> computer</c>

00:03:24.140 --> 00:03:24.150 align:start position:0%
what computers see how does a computer
 

00:03:24.150 --> 00:03:27.559 align:start position:0%
what computers see how does a computer
process<00:03:24.720><c> an</c><00:03:24.840><c> image</c><00:03:25.459><c> well</c><00:03:26.459><c> to</c><00:03:27.060><c> a</c><00:03:27.090><c> computer</c>

00:03:27.559 --> 00:03:27.569 align:start position:0%
process an image well to a computer
 

00:03:27.569 --> 00:03:30.020 align:start position:0%
process an image well to a computer
images<00:03:28.200><c> are</c><00:03:28.410><c> just</c><00:03:28.620><c> numbers</c><00:03:28.829><c> so</c><00:03:29.669><c> suppose</c><00:03:30.000><c> we</c>

00:03:30.020 --> 00:03:30.030 align:start position:0%
images are just numbers so suppose we
 

00:03:30.030 --> 00:03:33.170 align:start position:0%
images are just numbers so suppose we
have<00:03:30.209><c> a</c><00:03:30.419><c> picture</c><00:03:30.780><c> of</c><00:03:31.130><c> Abraham</c><00:03:32.130><c> Lincoln</c><00:03:32.519><c> it's</c>

00:03:33.170 --> 00:03:33.180 align:start position:0%
have a picture of Abraham Lincoln it's
 

00:03:33.180 --> 00:03:35.659 align:start position:0%
have a picture of Abraham Lincoln it's
made<00:03:33.450><c> up</c><00:03:33.569><c> of</c><00:03:33.690><c> pixels</c><00:03:33.959><c> and</c><00:03:34.410><c> since</c><00:03:35.280><c> this</c><00:03:35.459><c> is</c><00:03:35.639><c> a</c>

00:03:35.659 --> 00:03:35.669 align:start position:0%
made up of pixels and since this is a
 

00:03:35.669 --> 00:03:38.089 align:start position:0%
made up of pixels and since this is a
grayscale<00:03:36.239><c> image</c><00:03:36.269><c> each</c><00:03:37.049><c> of</c><00:03:37.319><c> these</c><00:03:37.440><c> pixels</c><00:03:37.709><c> can</c>

00:03:38.089 --> 00:03:38.099 align:start position:0%
grayscale image each of these pixels can
 

00:03:38.099 --> 00:03:40.930 align:start position:0%
grayscale image each of these pixels can
be<00:03:38.250><c> represented</c><00:03:38.849><c> by</c><00:03:38.880><c> just</c><00:03:39.419><c> a</c><00:03:39.569><c> single</c><00:03:39.900><c> number</c>

00:03:40.930 --> 00:03:40.940 align:start position:0%
be represented by just a single number
 

00:03:40.940 --> 00:03:44.030 align:start position:0%
be represented by just a single number
so<00:03:41.940><c> we</c><00:03:42.060><c> can</c><00:03:42.180><c> represent</c><00:03:42.389><c> our</c><00:03:42.810><c> image</c><00:03:43.230><c> as</c><00:03:43.410><c> a</c><00:03:43.470><c> 2d</c>

00:03:44.030 --> 00:03:44.040 align:start position:0%
so we can represent our image as a 2d
 

00:03:44.040 --> 00:03:46.400 align:start position:0%
so we can represent our image as a 2d
array<00:03:44.400><c> of</c><00:03:44.549><c> numbers</c><00:03:44.700><c> one</c><00:03:45.540><c> for</c><00:03:45.750><c> each</c><00:03:45.870><c> pixel</c><00:03:46.139><c> in</c>

00:03:46.400 --> 00:03:46.410 align:start position:0%
array of numbers one for each pixel in
 

00:03:46.410 --> 00:03:49.490 align:start position:0%
array of numbers one for each pixel in
the<00:03:46.500><c> image</c><00:03:46.530><c> and</c><00:03:47.150><c> if</c><00:03:48.150><c> we</c><00:03:48.299><c> were</c><00:03:48.419><c> to</c><00:03:48.480><c> have</c><00:03:48.750><c> a</c><00:03:48.780><c> RGB</c>

00:03:49.490 --> 00:03:49.500 align:start position:0%
the image and if we were to have a RGB
 

00:03:49.500 --> 00:03:51.860 align:start position:0%
the image and if we were to have a RGB
color<00:03:49.799><c> image</c><00:03:50.190><c> not</c><00:03:50.579><c> grayscale</c><00:03:51.090><c> we</c><00:03:51.690><c> can</c>

00:03:51.860 --> 00:03:51.870 align:start position:0%
color image not grayscale we can
 

00:03:51.870 --> 00:03:54.140 align:start position:0%
color image not grayscale we can
represent<00:03:52.049><c> that</c><00:03:52.410><c> with</c><00:03:52.919><c> a</c><00:03:52.950><c> 3d</c><00:03:53.430><c> array</c><00:03:53.700><c> where</c><00:03:54.120><c> we</c>

00:03:54.140 --> 00:03:54.150 align:start position:0%
represent that with a 3d array where we
 

00:03:54.150 --> 00:03:57.080 align:start position:0%
represent that with a 3d array where we
have<00:03:54.450><c> two</c><00:03:54.660><c> D</c><00:03:54.780><c> matrices</c><00:03:55.169><c> for</c><00:03:56.040><c> each</c><00:03:56.190><c> of</c><00:03:56.370><c> our</c><00:03:56.579><c> g</c>

00:03:57.080 --> 00:03:57.090 align:start position:0%
have two D matrices for each of our g
 

00:03:57.090 --> 00:04:00.170 align:start position:0%
have two D matrices for each of our g
and<00:03:57.299><c> b</c><00:03:58.340><c> now</c><00:03:59.340><c> that</c><00:03:59.549><c> we</c><00:03:59.669><c> have</c><00:03:59.849><c> a</c><00:03:59.879><c> way</c><00:04:00.120><c> to</c>

00:04:00.170 --> 00:04:00.180 align:start position:0%
and b now that we have a way to
 

00:04:00.180 --> 00:04:02.750 align:start position:0%
and b now that we have a way to
represent<00:04:00.750><c> images</c><00:04:01.500><c> two</c><00:04:01.650><c> computers</c><00:04:02.099><c> we</c><00:04:02.730><c> can</c>

00:04:02.750 --> 00:04:02.760 align:start position:0%
represent images two computers we can
 

00:04:02.760 --> 00:04:05.000 align:start position:0%
represent images two computers we can
next<00:04:03.120><c> think</c><00:04:03.389><c> about</c><00:04:03.599><c> what</c><00:04:04.169><c> types</c><00:04:04.440><c> of</c><00:04:04.620><c> computer</c>

00:04:05.000 --> 00:04:05.010 align:start position:0%
next think about what types of computer
 

00:04:05.010 --> 00:04:08.030 align:start position:0%
next think about what types of computer
vision<00:04:05.160><c> tasks</c><00:04:05.730><c> we</c><00:04:06.000><c> can</c><00:04:06.030><c> perform</c><00:04:06.569><c> and</c><00:04:07.079><c> in</c>

00:04:08.030 --> 00:04:08.040 align:start position:0%
vision tasks we can perform and in
 

00:04:08.040 --> 00:04:10.309 align:start position:0%
vision tasks we can perform and in
machine<00:04:08.579><c> learning</c><00:04:08.730><c> more</c><00:04:09.090><c> broadly</c><00:04:09.389><c> we</c><00:04:10.139><c> can</c>

00:04:10.309 --> 00:04:10.319 align:start position:0%
machine learning more broadly we can
 

00:04:10.319 --> 00:04:13.099 align:start position:0%
machine learning more broadly we can
think<00:04:10.530><c> of</c><00:04:10.769><c> tasks</c><00:04:11.310><c> of</c><00:04:11.629><c> regression</c><00:04:12.629><c> and</c><00:04:12.840><c> those</c>

00:04:13.099 --> 00:04:13.109 align:start position:0%
think of tasks of regression and those
 

00:04:13.109 --> 00:04:16.430 align:start position:0%
think of tasks of regression and those
of<00:04:13.349><c> classification</c><00:04:14.220><c> in</c><00:04:14.900><c> regression</c><00:04:15.900><c> our</c>

00:04:16.430 --> 00:04:16.440 align:start position:0%
of classification in regression our
 

00:04:16.440 --> 00:04:18.409 align:start position:0%
of classification in regression our
output<00:04:16.949><c> takes</c><00:04:17.130><c> a</c><00:04:17.250><c> continuous</c><00:04:17.760><c> value</c><00:04:17.940><c> and</c>

00:04:18.409 --> 00:04:18.419 align:start position:0%
output takes a continuous value and
 

00:04:18.419 --> 00:04:20.180 align:start position:0%
output takes a continuous value and
named<00:04:18.690><c> classification</c><00:04:19.500><c> a</c><00:04:19.620><c> single</c><00:04:19.979><c> class</c>

00:04:20.180 --> 00:04:20.190 align:start position:0%
named classification a single class
 

00:04:20.190 --> 00:04:22.879 align:start position:0%
named classification a single class
label<00:04:20.669><c> so</c><00:04:21.539><c> let's</c><00:04:21.930><c> consider</c><00:04:22.139><c> the</c><00:04:22.470><c> task</c><00:04:22.680><c> of</c>

00:04:22.879 --> 00:04:22.889 align:start position:0%
label so let's consider the task of
 

00:04:22.889 --> 00:04:25.550 align:start position:0%
label so let's consider the task of
image<00:04:23.160><c> classification</c><00:04:23.909><c> we</c><00:04:24.719><c> want</c><00:04:24.990><c> to</c><00:04:25.169><c> predict</c>

00:04:25.550 --> 00:04:25.560 align:start position:0%
image classification we want to predict
 

00:04:25.560 --> 00:04:27.920 align:start position:0%
image classification we want to predict
a<00:04:25.710><c> single</c><00:04:26.159><c> label</c><00:04:26.340><c> for</c><00:04:26.849><c> a</c><00:04:26.880><c> given</c><00:04:27.300><c> input</c><00:04:27.450><c> image</c>

00:04:27.920 --> 00:04:27.930 align:start position:0%
a single label for a given input image
 

00:04:27.930 --> 00:04:29.100 align:start position:0%
a single label for a given input image
for<00:04:28.860><c> example</c>

00:04:29.100 --> 00:04:29.110 align:start position:0%
for example
 

00:04:29.110 --> 00:04:30.990 align:start position:0%
for example
let's<00:04:29.530><c> say</c><00:04:29.710><c> we</c><00:04:29.860><c> have</c><00:04:30.009><c> a</c><00:04:30.039><c> bunch</c><00:04:30.250><c> of</c><00:04:30.430><c> images</c><00:04:30.849><c> of</c>

00:04:30.990 --> 00:04:31.000 align:start position:0%
let's say we have a bunch of images of
 

00:04:31.000 --> 00:04:33.330 align:start position:0%
let's say we have a bunch of images of
US<00:04:31.240><c> presidents</c><00:04:31.780><c> and</c><00:04:31.990><c> we</c><00:04:32.650><c> want</c><00:04:32.889><c> to</c><00:04:33.009><c> build</c><00:04:33.220><c> a</c>

00:04:33.330 --> 00:04:33.340 align:start position:0%
US presidents and we want to build a
 

00:04:33.340 --> 00:04:35.850 align:start position:0%
US presidents and we want to build a
classification<00:04:33.759><c> pipeline</c><00:04:34.479><c> to</c><00:04:34.930><c> tell</c><00:04:35.110><c> us</c><00:04:35.259><c> which</c>

00:04:35.850 --> 00:04:35.860 align:start position:0%
classification pipeline to tell us which
 

00:04:35.860 --> 00:04:38.189 align:start position:0%
classification pipeline to tell us which
President<00:04:36.340><c> is</c><00:04:36.460><c> in</c><00:04:36.699><c> an</c><00:04:36.819><c> image</c><00:04:37.169><c> outputting</c><00:04:38.169><c> the</c>

00:04:38.189 --> 00:04:38.199 align:start position:0%
President is in an image outputting the
 

00:04:38.199 --> 00:04:40.649 align:start position:0%
President is in an image outputting the
probability<00:04:38.949><c> that</c><00:04:39.189><c> the</c><00:04:39.639><c> image</c><00:04:39.879><c> is</c><00:04:40.000><c> of</c><00:04:40.180><c> a</c>

00:04:40.649 --> 00:04:40.659 align:start position:0%
probability that the image is of a
 

00:04:40.659 --> 00:04:43.950 align:start position:0%
probability that the image is of a
particular<00:04:40.810><c> President</c><00:04:42.389><c> so</c><00:04:43.389><c> in</c><00:04:43.509><c> order</c><00:04:43.629><c> to</c><00:04:43.780><c> be</c>

00:04:43.950 --> 00:04:43.960 align:start position:0%
particular President so in order to be
 

00:04:43.960 --> 00:04:46.110 align:start position:0%
particular President so in order to be
able<00:04:44.050><c> to</c><00:04:44.199><c> classify</c><00:04:44.530><c> these</c><00:04:45.039><c> images</c><00:04:45.550><c> our</c>

00:04:46.110 --> 00:04:46.120 align:start position:0%
able to classify these images our
 

00:04:46.120 --> 00:04:48.240 align:start position:0%
able to classify these images our
pipeline<00:04:46.599><c> needs</c><00:04:46.900><c> to</c><00:04:47.080><c> be</c><00:04:47.229><c> able</c><00:04:47.650><c> to</c><00:04:47.770><c> tell</c><00:04:47.979><c> what</c>

00:04:48.240 --> 00:04:48.250 align:start position:0%
pipeline needs to be able to tell what
 

00:04:48.250 --> 00:04:49.909 align:start position:0%
pipeline needs to be able to tell what
is<00:04:48.430><c> unique</c><00:04:48.849><c> about</c><00:04:48.969><c> a</c><00:04:49.120><c> picture</c><00:04:49.389><c> of</c><00:04:49.599><c> Lincoln</c>

00:04:49.909 --> 00:04:49.919 align:start position:0%
is unique about a picture of Lincoln
 

00:04:49.919 --> 00:04:52.589 align:start position:0%
is unique about a picture of Lincoln
versus<00:04:50.919><c> a</c><00:04:51.129><c> picture</c><00:04:51.430><c> of</c><00:04:51.520><c> Washington</c><00:04:52.090><c> versus</c><00:04:52.569><c> a</c>

00:04:52.589 --> 00:04:52.599 align:start position:0%
versus a picture of Washington versus a
 

00:04:52.599 --> 00:04:55.770 align:start position:0%
versus a picture of Washington versus a
picture<00:04:52.930><c> of</c><00:04:53.050><c> Obama</c><00:04:53.169><c> and</c><00:04:54.509><c> another</c><00:04:55.509><c> way</c><00:04:55.719><c> to</c>

00:04:55.770 --> 00:04:55.780 align:start position:0%
picture of Obama and another way to
 

00:04:55.780 --> 00:04:58.589 align:start position:0%
picture of Obama and another way to
think<00:04:56.139><c> about</c><00:04:56.319><c> this</c><00:04:57.009><c> problem</c><00:04:57.550><c> at</c><00:04:57.699><c> a</c><00:04:58.060><c> high</c><00:04:58.360><c> level</c>

00:04:58.589 --> 00:04:58.599 align:start position:0%
think about this problem at a high level
 

00:04:58.599 --> 00:05:00.360 align:start position:0%
think about this problem at a high level
is<00:04:58.840><c> in</c><00:04:58.990><c> terms</c><00:04:59.289><c> of</c><00:04:59.530><c> the</c><00:04:59.650><c> features</c><00:04:59.919><c> that</c><00:05:00.159><c> are</c>

00:05:00.360 --> 00:05:00.370 align:start position:0%
is in terms of the features that are
 

00:05:00.370 --> 00:05:02.430 align:start position:0%
is in terms of the features that are
characteristic<00:05:00.909><c> of</c><00:05:01.150><c> a</c><00:05:01.449><c> particular</c><00:05:01.960><c> class</c><00:05:02.139><c> of</c>

00:05:02.430 --> 00:05:02.440 align:start position:0%
characteristic of a particular class of
 

00:05:02.440 --> 00:05:05.100 align:start position:0%
characteristic of a particular class of
images<00:05:03.300><c> classification</c><00:05:04.300><c> would</c><00:05:04.449><c> then</c><00:05:04.690><c> be</c><00:05:04.840><c> done</c>

00:05:05.100 --> 00:05:05.110 align:start position:0%
images classification would then be done
 

00:05:05.110 --> 00:05:07.140 align:start position:0%
images classification would then be done
by<00:05:05.740><c> detecting</c><00:05:06.310><c> the</c><00:05:06.430><c> presence</c><00:05:06.460><c> of</c><00:05:06.969><c> these</c>

00:05:07.140 --> 00:05:07.150 align:start position:0%
by detecting the presence of these
 

00:05:07.150 --> 00:05:10.890 align:start position:0%
by detecting the presence of these
features<00:05:07.599><c> in</c><00:05:07.870><c> a</c><00:05:08.020><c> given</c><00:05:08.289><c> image</c><00:05:08.469><c> if</c><00:05:09.069><c> the</c><00:05:09.849><c> if</c><00:05:10.509><c> the</c>

00:05:10.890 --> 00:05:10.900 align:start position:0%
features in a given image if the if the
 

00:05:10.900 --> 00:05:13.350 align:start position:0%
features in a given image if the if the
features<00:05:11.259><c> for</c><00:05:11.469><c> a</c><00:05:11.530><c> particular</c><00:05:12.340><c> image</c><00:05:12.699><c> are</c><00:05:12.940><c> all</c>

00:05:13.350 --> 00:05:13.360 align:start position:0%
features for a particular image are all
 

00:05:13.360 --> 00:05:15.990 align:start position:0%
features for a particular image are all
present<00:05:13.870><c> in</c><00:05:13.930><c> an</c><00:05:14.110><c> image</c><00:05:14.409><c> we</c><00:05:15.159><c> can</c><00:05:15.340><c> then</c><00:05:15.520><c> predict</c>

00:05:15.990 --> 00:05:16.000 align:start position:0%
present in an image we can then predict
 

00:05:16.000 --> 00:05:19.980 align:start position:0%
present in an image we can then predict
that<00:05:16.389><c> class</c><00:05:16.690><c> with</c><00:05:17.319><c> a</c><00:05:17.620><c> high</c><00:05:17.979><c> probability</c><00:05:18.990><c> so</c>

00:05:19.980 --> 00:05:19.990 align:start position:0%
that class with a high probability so
 

00:05:19.990 --> 00:05:22.439 align:start position:0%
that class with a high probability so
for<00:05:20.830><c> our</c><00:05:20.949><c> image</c><00:05:21.219><c> classification</c><00:05:21.490><c> pipeline</c>

00:05:22.439 --> 00:05:22.449 align:start position:0%
for our image classification pipeline
 

00:05:22.449 --> 00:05:25.469 align:start position:0%
for our image classification pipeline
our<00:05:22.690><c> model</c><00:05:23.409><c> needs</c><00:05:23.590><c> to</c><00:05:23.770><c> know</c><00:05:24.009><c> first</c><00:05:24.879><c> what</c><00:05:25.330><c> the</c>

00:05:25.469 --> 00:05:25.479 align:start position:0%
our model needs to know first what the
 

00:05:25.479 --> 00:05:28.200 align:start position:0%
our model needs to know first what the
features<00:05:25.779><c> are</c><00:05:26.050><c> and</c><00:05:26.289><c> secondly</c><00:05:27.129><c> it</c><00:05:27.490><c> needs</c><00:05:28.000><c> to</c><00:05:28.120><c> be</c>

00:05:28.200 --> 00:05:28.210 align:start position:0%
features are and secondly it needs to be
 

00:05:28.210 --> 00:05:30.179 align:start position:0%
features are and secondly it needs to be
able<00:05:28.360><c> to</c><00:05:28.509><c> detect</c><00:05:28.990><c> those</c><00:05:29.289><c> features</c><00:05:29.740><c> in</c><00:05:29.949><c> an</c>

00:05:30.179 --> 00:05:30.189 align:start position:0%
able to detect those features in an
 

00:05:30.189 --> 00:05:33.629 align:start position:0%
able to detect those features in an
image<00:05:31.020><c> so</c><00:05:32.020><c> one</c><00:05:32.199><c> way</c><00:05:32.349><c> we</c><00:05:32.379><c> can</c><00:05:32.770><c> solve</c><00:05:33.009><c> this</c><00:05:33.279><c> is</c><00:05:33.460><c> to</c>

00:05:33.629 --> 00:05:33.639 align:start position:0%
image so one way we can solve this is to
 

00:05:33.639 --> 00:05:35.159 align:start position:0%
image so one way we can solve this is to
leverage<00:05:33.849><c> our</c><00:05:34.120><c> knowledge</c><00:05:34.210><c> about</c><00:05:34.960><c> a</c>

00:05:35.159 --> 00:05:35.169 align:start position:0%
leverage our knowledge about a
 

00:05:35.169 --> 00:05:37.680 align:start position:0%
leverage our knowledge about a
particular<00:05:35.680><c> field</c><00:05:36.009><c> and</c><00:05:36.219><c> use</c><00:05:37.090><c> this</c><00:05:37.330><c> to</c>

00:05:37.680 --> 00:05:37.690 align:start position:0%
particular field and use this to
 

00:05:37.690 --> 00:05:39.629 align:start position:0%
particular field and use this to
manually<00:05:38.259><c> define</c><00:05:38.440><c> the</c><00:05:38.770><c> features</c><00:05:39.099><c> ourselves</c>

00:05:39.629 --> 00:05:39.639 align:start position:0%
manually define the features ourselves
 

00:05:39.639 --> 00:05:42.809 align:start position:0%
manually define the features ourselves
and<00:05:40.440><c> classification</c><00:05:41.440><c> pipeline</c><00:05:41.889><c> would</c><00:05:42.610><c> then</c>

00:05:42.809 --> 00:05:42.819 align:start position:0%
and classification pipeline would then
 

00:05:42.819 --> 00:05:45.089 align:start position:0%
and classification pipeline would then
try<00:05:43.150><c> to</c><00:05:43.210><c> detect</c><00:05:43.719><c> these</c><00:05:43.900><c> manually</c><00:05:44.469><c> defined</c>

00:05:45.089 --> 00:05:45.099 align:start position:0%
try to detect these manually defined
 

00:05:45.099 --> 00:05:47.790 align:start position:0%
try to detect these manually defined
features<00:05:45.370><c> in</c><00:05:45.819><c> images</c><00:05:46.330><c> and</c><00:05:46.569><c> use</c><00:05:47.229><c> the</c><00:05:47.379><c> results</c>

00:05:47.790 --> 00:05:47.800 align:start position:0%
features in images and use the results
 

00:05:47.800 --> 00:05:49.529 align:start position:0%
features in images and use the results
of<00:05:47.919><c> some</c><00:05:48.219><c> detection</c><00:05:48.699><c> algorithm</c><00:05:49.210><c> for</c>

00:05:49.529 --> 00:05:49.539 align:start position:0%
of some detection algorithm for
 

00:05:49.539 --> 00:05:52.529 align:start position:0%
of some detection algorithm for
classification<00:05:50.490><c> but</c><00:05:51.490><c> there</c><00:05:51.940><c> is</c><00:05:52.060><c> a</c><00:05:52.270><c> big</c>

00:05:52.529 --> 00:05:52.539 align:start position:0%
classification but there is a big
 

00:05:52.539 --> 00:05:55.290 align:start position:0%
classification but there is a big
problem<00:05:52.990><c> with</c><00:05:53.139><c> this</c><00:05:53.289><c> approach</c><00:05:54.150><c> remember</c><00:05:55.150><c> that</c>

00:05:55.290 --> 00:05:55.300 align:start position:0%
problem with this approach remember that
 

00:05:55.300 --> 00:05:58.050 align:start position:0%
problem with this approach remember that
images<00:05:55.750><c> are</c><00:05:55.930><c> just</c><00:05:56.259><c> 3d</c><00:05:56.979><c> arrays</c><00:05:57.370><c> of</c><00:05:57.669><c> brightness</c>

00:05:58.050 --> 00:05:58.060 align:start position:0%
images are just 3d arrays of brightness
 

00:05:58.060 --> 00:06:01.439 align:start position:0%
images are just 3d arrays of brightness
values<00:05:58.449><c> and</c><00:05:59.099><c> image</c><00:06:00.099><c> data</c><00:06:00.370><c> has</c><00:06:00.639><c> a</c><00:06:00.789><c> lot</c><00:06:01.180><c> of</c>

00:06:01.439 --> 00:06:01.449 align:start position:0%
values and image data has a lot of
 

00:06:01.449 --> 00:06:04.230 align:start position:0%
values and image data has a lot of
variation<00:06:02.190><c> occlusion</c><00:06:03.190><c> variations</c><00:06:04.060><c> in</c>

00:06:04.230 --> 00:06:04.240 align:start position:0%
variation occlusion variations in
 

00:06:04.240 --> 00:06:06.209 align:start position:0%
variation occlusion variations in
illumination<00:06:04.779><c> viewpoint</c><00:06:05.560><c> variation</c>

00:06:06.209 --> 00:06:06.219 align:start position:0%
illumination viewpoint variation
 

00:06:06.219 --> 00:06:09.059 align:start position:0%
illumination viewpoint variation
intraclass<00:06:07.029><c> variation</c><00:06:07.870><c> and</c><00:06:08.169><c> our</c>

00:06:09.059 --> 00:06:09.069 align:start position:0%
intraclass variation and our
 

00:06:09.069 --> 00:06:10.890 align:start position:0%
intraclass variation and our
classification<00:06:09.909><c> pipeline</c><00:06:10.330><c> has</c><00:06:10.599><c> to</c><00:06:10.750><c> be</c>

00:06:10.890 --> 00:06:10.900 align:start position:0%
classification pipeline has to be
 

00:06:10.900 --> 00:06:13.170 align:start position:0%
classification pipeline has to be
invariant<00:06:11.409><c> to</c><00:06:11.469><c> all</c><00:06:11.680><c> these</c><00:06:11.949><c> variations</c><00:06:12.610><c> while</c>

00:06:13.170 --> 00:06:13.180 align:start position:0%
invariant to all these variations while
 

00:06:13.180 --> 00:06:15.510 align:start position:0%
invariant to all these variations while
still<00:06:13.599><c> being</c><00:06:13.810><c> sensitive</c><00:06:14.259><c> to</c><00:06:14.529><c> the</c><00:06:14.800><c> variability</c>

00:06:15.510 --> 00:06:15.520 align:start position:0%
still being sensitive to the variability
 

00:06:15.520 --> 00:06:19.379 align:start position:0%
still being sensitive to the variability
between<00:06:15.819><c> different</c><00:06:16.569><c> classes</c><00:06:17.969><c> even</c><00:06:18.969><c> though</c>

00:06:19.379 --> 00:06:19.389 align:start position:0%
between different classes even though
 

00:06:19.389 --> 00:06:21.600 align:start position:0%
between different classes even though
our<00:06:19.629><c> pipeline</c><00:06:20.050><c> could</c><00:06:20.259><c> use</c><00:06:20.560><c> features</c><00:06:21.190><c> that</c><00:06:21.370><c> we</c>

00:06:21.600 --> 00:06:21.610 align:start position:0%
our pipeline could use features that we
 

00:06:21.610 --> 00:06:24.300 align:start position:0%
our pipeline could use features that we
the<00:06:22.089><c> human</c><00:06:22.270><c> define</c><00:06:22.810><c> where</c><00:06:23.650><c> this</c><00:06:23.830><c> manual</c>

00:06:24.300 --> 00:06:24.310 align:start position:0%
the human define where this manual
 

00:06:24.310 --> 00:06:26.129 align:start position:0%
the human define where this manual
extraction<00:06:24.849><c> would</c><00:06:24.969><c> break</c><00:06:25.240><c> down</c><00:06:25.509><c> is</c><00:06:25.750><c> in</c><00:06:26.020><c> the</c>

00:06:26.129 --> 00:06:26.139 align:start position:0%
extraction would break down is in the
 

00:06:26.139 --> 00:06:29.339 align:start position:0%
extraction would break down is in the
detection<00:06:26.500><c> task</c><00:06:26.889><c> itself</c><00:06:27.610><c> and</c><00:06:28.060><c> this</c><00:06:28.960><c> is</c><00:06:29.139><c> due</c><00:06:29.319><c> to</c>

00:06:29.339 --> 00:06:29.349 align:start position:0%
detection task itself and this is due to
 

00:06:29.349 --> 00:06:31.469 align:start position:0%
detection task itself and this is due to
the<00:06:29.680><c> incredible</c><00:06:30.219><c> variability</c><00:06:30.520><c> that</c><00:06:31.150><c> I</c><00:06:31.180><c> just</c>

00:06:31.469 --> 00:06:31.479 align:start position:0%
the incredible variability that I just
 

00:06:31.479 --> 00:06:33.959 align:start position:0%
the incredible variability that I just
mentioned<00:06:31.900><c> the</c><00:06:32.860><c> detection</c><00:06:33.250><c> of</c><00:06:33.580><c> these</c>

00:06:33.959 --> 00:06:33.969 align:start position:0%
mentioned the detection of these
 

00:06:33.969 --> 00:06:35.579 align:start position:0%
mentioned the detection of these
features<00:06:34.509><c> would</c><00:06:34.719><c> actually</c><00:06:35.409><c> be</c><00:06:35.560><c> really</c>

00:06:35.579 --> 00:06:35.589 align:start position:0%
features would actually be really
 

00:06:35.589 --> 00:06:37.949 align:start position:0%
features would actually be really
difficult<00:06:36.129><c> in</c><00:06:36.520><c> practice</c><00:06:36.719><c> because</c><00:06:37.719><c> your</c>

00:06:37.949 --> 00:06:37.959 align:start position:0%
difficult in practice because your
 

00:06:37.959 --> 00:06:39.899 align:start position:0%
difficult in practice because your
detection<00:06:38.469><c> algorithm</c><00:06:38.979><c> would</c><00:06:39.520><c> need</c><00:06:39.759><c> to</c>

00:06:39.899 --> 00:06:39.909 align:start position:0%
detection algorithm would need to
 

00:06:39.909 --> 00:06:41.050 align:start position:0%
detection algorithm would need to
withstand<00:06:40.120><c> each</c>

00:06:41.050 --> 00:06:41.060 align:start position:0%
withstand each
 

00:06:41.060 --> 00:06:42.879 align:start position:0%
withstand each
and<00:06:41.150><c> every</c><00:06:41.630><c> one</c><00:06:41.900><c> of</c><00:06:42.110><c> these</c><00:06:42.500><c> different</c>

00:06:42.879 --> 00:06:42.889 align:start position:0%
and every one of these different
 

00:06:42.889 --> 00:06:47.080 align:start position:0%
and every one of these different
variations<00:06:43.400><c> so</c><00:06:44.389><c> how</c><00:06:45.080><c> can</c><00:06:45.290><c> we</c><00:06:45.410><c> do</c><00:06:45.560><c> better</c><00:06:46.090><c> we</c>

00:06:47.080 --> 00:06:47.090 align:start position:0%
variations so how can we do better we
 

00:06:47.090 --> 00:06:49.570 align:start position:0%
variations so how can we do better we
want<00:06:47.330><c> a</c><00:06:47.419><c> way</c><00:06:47.600><c> to</c><00:06:47.630><c> both</c><00:06:48.290><c> extract</c><00:06:48.919><c> features</c><00:06:49.130><c> and</c>

00:06:49.570 --> 00:06:49.580 align:start position:0%
want a way to both extract features and
 

00:06:49.580 --> 00:06:51.240 align:start position:0%
want a way to both extract features and
detect<00:06:49.940><c> their</c><00:06:50.120><c> presence</c><00:06:50.510><c> in</c><00:06:50.660><c> images</c>

00:06:51.240 --> 00:06:51.250 align:start position:0%
detect their presence in images
 

00:06:51.250 --> 00:06:53.590 align:start position:0%
detect their presence in images
automatically<00:06:52.250><c> in</c><00:06:52.460><c> a</c><00:06:52.820><c> hierarchical</c><00:06:53.450><c> fashion</c>

00:06:53.590 --> 00:06:53.600 align:start position:0%
automatically in a hierarchical fashion
 

00:06:53.600 --> 00:06:57.159 align:start position:0%
automatically in a hierarchical fashion
and<00:06:54.520><c> we</c><00:06:55.520><c> can</c><00:06:55.730><c> use</c><00:06:56.000><c> neural</c><00:06:56.360><c> network</c><00:06:56.930><c> based</c>

00:06:57.159 --> 00:06:57.169 align:start position:0%
and we can use neural network based
 

00:06:57.169 --> 00:07:00.100 align:start position:0%
and we can use neural network based
approaches<00:06:57.830><c> to</c><00:06:58.220><c> do</c><00:06:58.370><c> exactly</c><00:06:58.820><c> this</c><00:06:59.090><c> to</c><00:06:59.930><c> learn</c>

00:07:00.100 --> 00:07:00.110 align:start position:0%
approaches to do exactly this to learn
 

00:07:00.110 --> 00:07:02.260 align:start position:0%
approaches to do exactly this to learn
visual<00:07:00.590><c> features</c><00:07:00.950><c> directly</c><00:07:01.520><c> from</c><00:07:01.700><c> image</c><00:07:02.030><c> data</c>

00:07:02.260 --> 00:07:02.270 align:start position:0%
visual features directly from image data
 

00:07:02.270 --> 00:07:04.360 align:start position:0%
visual features directly from image data
and<00:07:02.510><c> to</c><00:07:03.169><c> learn</c><00:07:03.350><c> a</c><00:07:03.380><c> hierarchy</c><00:07:04.100><c> of</c><00:07:04.130><c> these</c>

00:07:04.360 --> 00:07:04.370 align:start position:0%
and to learn a hierarchy of these
 

00:07:04.370 --> 00:07:06.550 align:start position:0%
and to learn a hierarchy of these
features<00:07:04.790><c> to</c><00:07:05.330><c> construct</c><00:07:05.780><c> an</c><00:07:05.990><c> internal</c>

00:07:06.550 --> 00:07:06.560 align:start position:0%
features to construct an internal
 

00:07:06.560 --> 00:07:09.220 align:start position:0%
features to construct an internal
representation<00:07:07.310><c> of</c><00:07:07.340><c> the</c><00:07:07.820><c> image</c><00:07:07.940><c> for</c><00:07:08.330><c> example</c>

00:07:09.220 --> 00:07:09.230 align:start position:0%
representation of the image for example
 

00:07:09.230 --> 00:07:11.409 align:start position:0%
representation of the image for example
if<00:07:09.800><c> we</c><00:07:10.070><c> wanted</c><00:07:10.460><c> to</c><00:07:10.520><c> be</c><00:07:10.850><c> able</c><00:07:10.970><c> to</c><00:07:11.210><c> classify</c>

00:07:11.409 --> 00:07:11.419 align:start position:0%
if we wanted to be able to classify
 

00:07:11.419 --> 00:07:14.500 align:start position:0%
if we wanted to be able to classify
images<00:07:12.110><c> of</c><00:07:12.230><c> faces</c><00:07:12.520><c> maybe</c><00:07:13.520><c> we</c><00:07:13.790><c> could</c><00:07:13.940><c> first</c>

00:07:14.500 --> 00:07:14.510 align:start position:0%
images of faces maybe we could first
 

00:07:14.510 --> 00:07:16.690 align:start position:0%
images of faces maybe we could first
learn<00:07:14.810><c> and</c><00:07:15.020><c> detect</c><00:07:15.320><c> low-level</c><00:07:15.979><c> features</c><00:07:16.340><c> like</c>

00:07:16.690 --> 00:07:16.700 align:start position:0%
learn and detect low-level features like
 

00:07:16.700 --> 00:07:19.360 align:start position:0%
learn and detect low-level features like
edges<00:07:17.120><c> and</c><00:07:17.240><c> dark</c><00:07:17.450><c> spots</c><00:07:17.750><c> mid</c><00:07:18.650><c> level</c><00:07:19.010><c> features</c>

00:07:19.360 --> 00:07:19.370 align:start position:0%
edges and dark spots mid level features
 

00:07:19.370 --> 00:07:22.480 align:start position:0%
edges and dark spots mid level features
eyes<00:07:19.820><c> ears</c><00:07:20.389><c> and</c><00:07:20.690><c> noses</c><00:07:20.870><c> and</c><00:07:21.260><c> then</c><00:07:21.889><c> high-level</c>

00:07:22.480 --> 00:07:22.490 align:start position:0%
eyes ears and noses and then high-level
 

00:07:22.490 --> 00:07:24.550 align:start position:0%
eyes ears and noses and then high-level
features<00:07:22.910><c> that</c><00:07:23.060><c> actually</c><00:07:23.600><c> resemble</c><00:07:24.080><c> facial</c>

00:07:24.550 --> 00:07:24.560 align:start position:0%
features that actually resemble facial
 

00:07:24.560 --> 00:07:28.180 align:start position:0%
features that actually resemble facial
structure<00:07:25.010><c> and</c><00:07:26.260><c> neural</c><00:07:27.260><c> networks</c><00:07:27.350><c> will</c><00:07:27.919><c> allow</c>

00:07:28.180 --> 00:07:28.190 align:start position:0%
structure and neural networks will allow
 

00:07:28.190 --> 00:07:31.420 align:start position:0%
structure and neural networks will allow
us<00:07:28.400><c> to</c><00:07:28.479><c> directly</c><00:07:29.479><c> learn</c><00:07:30.020><c> visual</c><00:07:30.919><c> features</c><00:07:31.280><c> in</c>

00:07:31.420 --> 00:07:31.430 align:start position:0%
us to directly learn visual features in
 

00:07:31.430 --> 00:07:33.250 align:start position:0%
us to directly learn visual features in
this<00:07:31.580><c> manner</c><00:07:31.880><c> if</c><00:07:32.150><c> we</c><00:07:32.660><c> construct</c><00:07:32.930><c> them</c>

00:07:33.250 --> 00:07:33.260 align:start position:0%
this manner if we construct them
 

00:07:33.260 --> 00:07:36.969 align:start position:0%
this manner if we construct them
cleverly<00:07:34.419><c> so</c><00:07:35.419><c> in</c><00:07:35.750><c> yesterday's</c><00:07:36.290><c> first</c><00:07:36.590><c> lecture</c>

00:07:36.969 --> 00:07:36.979 align:start position:0%
cleverly so in yesterday's first lecture
 

00:07:36.979 --> 00:07:38.740 align:start position:0%
cleverly so in yesterday's first lecture
we<00:07:37.160><c> learned</c><00:07:37.370><c> about</c><00:07:37.550><c> fully</c><00:07:37.970><c> connected</c><00:07:38.479><c> neural</c>

00:07:38.740 --> 00:07:38.750 align:start position:0%
we learned about fully connected neural
 

00:07:38.750 --> 00:07:40.659 align:start position:0%
we learned about fully connected neural
networks<00:07:39.110><c> where</c><00:07:39.979><c> you</c><00:07:40.130><c> can</c><00:07:40.280><c> have</c><00:07:40.460><c> multiple</c>

00:07:40.659 --> 00:07:40.669 align:start position:0%
networks where you can have multiple
 

00:07:40.669 --> 00:07:43.360 align:start position:0%
networks where you can have multiple
hidden<00:07:41.120><c> layers</c><00:07:41.270><c> and</c><00:07:41.570><c> where</c><00:07:42.290><c> each</c><00:07:42.560><c> neuron</c><00:07:42.919><c> in</c><00:07:43.280><c> a</c>

00:07:43.360 --> 00:07:43.370 align:start position:0%
hidden layers and where each neuron in a
 

00:07:43.370 --> 00:07:45.550 align:start position:0%
hidden layers and where each neuron in a
given<00:07:43.639><c> layer</c><00:07:43.820><c> is</c><00:07:44.090><c> connected</c><00:07:44.930><c> to</c><00:07:45.080><c> every</c><00:07:45.380><c> neuron</c>

00:07:45.550 --> 00:07:45.560 align:start position:0%
given layer is connected to every neuron
 

00:07:45.560 --> 00:07:48.010 align:start position:0%
given layer is connected to every neuron
in<00:07:45.680><c> the</c><00:07:45.800><c> subsequent</c><00:07:46.220><c> layer</c><00:07:46.630><c> let's</c><00:07:47.630><c> say</c><00:07:47.810><c> we</c>

00:07:48.010 --> 00:07:48.020 align:start position:0%
in the subsequent layer let's say we
 

00:07:48.020 --> 00:07:49.870 align:start position:0%
in the subsequent layer let's say we
wanted<00:07:48.410><c> to</c><00:07:48.470><c> use</c><00:07:48.860><c> a</c><00:07:48.890><c> fully</c><00:07:49.250><c> connected</c><00:07:49.580><c> and</c>

00:07:49.870 --> 00:07:49.880 align:start position:0%
wanted to use a fully connected and
 

00:07:49.880 --> 00:07:51.580 align:start position:0%
wanted to use a fully connected and
neural<00:07:50.090><c> network</c><00:07:50.419><c> like</c><00:07:50.720><c> the</c><00:07:50.840><c> one</c><00:07:50.990><c> you</c><00:07:51.140><c> see</c><00:07:51.320><c> here</c>

00:07:51.580 --> 00:07:51.590 align:start position:0%
neural network like the one you see here
 

00:07:51.590 --> 00:07:54.790 align:start position:0%
neural network like the one you see here
for<00:07:51.950><c> image</c><00:07:52.220><c> classification</c><00:07:52.460><c> in</c><00:07:53.530><c> this</c><00:07:54.530><c> case</c>

00:07:54.790 --> 00:07:54.800 align:start position:0%
for image classification in this case
 

00:07:54.800 --> 00:07:56.860 align:start position:0%
for image classification in this case
our<00:07:55.010><c> input</c><00:07:55.520><c> image</c><00:07:55.820><c> would</c><00:07:56.060><c> be</c><00:07:56.210><c> transformed</c>

00:07:56.860 --> 00:07:56.870 align:start position:0%
our input image would be transformed
 

00:07:56.870 --> 00:07:59.590 align:start position:0%
our input image would be transformed
into<00:07:57.080><c> a</c><00:07:57.229><c> vector</c><00:07:57.530><c> of</c><00:07:57.830><c> pixel</c><00:07:58.220><c> values</c><00:07:58.610><c> fed</c><00:07:59.300><c> into</c>

00:07:59.590 --> 00:07:59.600 align:start position:0%
into a vector of pixel values fed into
 

00:07:59.600 --> 00:08:01.719 align:start position:0%
into a vector of pixel values fed into
the<00:07:59.720><c> network</c><00:08:00.080><c> and</c><00:08:00.350><c> each</c><00:08:01.070><c> neuron</c><00:08:01.340><c> in</c><00:08:01.640><c> the</c>

00:08:01.719 --> 00:08:01.729 align:start position:0%
the network and each neuron in the
 

00:08:01.729 --> 00:08:03.880 align:start position:0%
the network and each neuron in the
hidden<00:08:02.000><c> layer</c><00:08:02.150><c> would</c><00:08:02.450><c> be</c><00:08:02.570><c> connected</c><00:08:02.930><c> to</c><00:08:03.740><c> all</c>

00:08:03.880 --> 00:08:03.890 align:start position:0%
hidden layer would be connected to all
 

00:08:03.890 --> 00:08:06.940 align:start position:0%
hidden layer would be connected to all
neurons<00:08:04.160><c> in</c><00:08:04.580><c> the</c><00:08:04.729><c> input</c><00:08:05.090><c> layer</c><00:08:05.240><c> I</c><00:08:05.800><c> hope</c><00:08:06.800><c> you</c>

00:08:06.940 --> 00:08:06.950 align:start position:0%
neurons in the input layer I hope you
 

00:08:06.950 --> 00:08:09.190 align:start position:0%
neurons in the input layer I hope you
can<00:08:07.100><c> appreciate</c><00:08:07.400><c> that</c><00:08:07.640><c> all</c><00:08:08.030><c> spatial</c><00:08:08.390><c> spatial</c>

00:08:09.190 --> 00:08:09.200 align:start position:0%
can appreciate that all spatial spatial
 

00:08:09.200 --> 00:08:11.770 align:start position:0%
can appreciate that all spatial spatial
information<00:08:09.500><c> from</c><00:08:10.130><c> our</c><00:08:10.370><c> 2d</c><00:08:10.880><c> array</c><00:08:11.180><c> would</c><00:08:11.600><c> be</c>

00:08:11.770 --> 00:08:11.780 align:start position:0%
information from our 2d array would be
 

00:08:11.780 --> 00:08:13.170 align:start position:0%
information from our 2d array would be
completely<00:08:12.410><c> lost</c>

00:08:13.170 --> 00:08:13.180 align:start position:0%
completely lost
 

00:08:13.180 --> 00:08:16.000 align:start position:0%
completely lost
additionally<00:08:14.180><c> additionally</c><00:08:14.930><c> we'd</c><00:08:15.410><c> have</c><00:08:15.620><c> many</c>

00:08:16.000 --> 00:08:16.010 align:start position:0%
additionally additionally we'd have many
 

00:08:16.010 --> 00:08:18.219 align:start position:0%
additionally additionally we'd have many
many<00:08:16.280><c> parameters</c><00:08:16.940><c> because</c><00:08:17.600><c> this</c><00:08:17.810><c> is</c><00:08:17.930><c> a</c><00:08:17.960><c> fully</c>

00:08:18.219 --> 00:08:18.229 align:start position:0%
many parameters because this is a fully
 

00:08:18.229 --> 00:08:20.830 align:start position:0%
many parameters because this is a fully
connected<00:08:18.530><c> network</c><00:08:19.039><c> so</c><00:08:19.760><c> it's</c><00:08:20.180><c> not</c><00:08:20.419><c> going</c><00:08:20.720><c> to</c>

00:08:20.830 --> 00:08:20.840 align:start position:0%
connected network so it's not going to
 

00:08:20.840 --> 00:08:22.900 align:start position:0%
connected network so it's not going to
be<00:08:20.960><c> really</c><00:08:21.380><c> feasible</c><00:08:21.800><c> to</c><00:08:21.950><c> implement</c><00:08:21.979><c> such</c><00:08:22.880><c> a</c>

00:08:22.900 --> 00:08:22.910 align:start position:0%
be really feasible to implement such a
 

00:08:22.910 --> 00:08:27.340 align:start position:0%
be really feasible to implement such a
network<00:08:23.120><c> in</c><00:08:23.570><c> practice</c><00:08:25.389><c> so</c><00:08:26.389><c> how</c><00:08:26.960><c> can</c><00:08:27.020><c> we</c><00:08:27.320><c> use</c>

00:08:27.340 --> 00:08:27.350 align:start position:0%
network in practice so how can we use
 

00:08:27.350 --> 00:08:29.860 align:start position:0%
network in practice so how can we use
the<00:08:27.860><c> spatial</c><00:08:28.310><c> structure</c><00:08:28.580><c> that's</c><00:08:28.940><c> inherent</c><00:08:29.660><c> in</c>

00:08:29.860 --> 00:08:29.870 align:start position:0%
the spatial structure that's inherent in
 

00:08:29.870 --> 00:08:32.050 align:start position:0%
the spatial structure that's inherent in
the<00:08:29.990><c> input</c><00:08:30.380><c> to</c><00:08:30.650><c> inform</c><00:08:31.100><c> the</c><00:08:31.220><c> architecture</c><00:08:31.850><c> of</c>

00:08:32.050 --> 00:08:32.060 align:start position:0%
the input to inform the architecture of
 

00:08:32.060 --> 00:08:36.850 align:start position:0%
the input to inform the architecture of
our<00:08:32.719><c> network</c><00:08:34.240><c> the</c><00:08:35.240><c> key</c><00:08:35.479><c> insight</c><00:08:35.750><c> in</c><00:08:36.200><c> how</c><00:08:36.260><c> we</c>

00:08:36.850 --> 00:08:36.860 align:start position:0%
our network the key insight in how we
 

00:08:36.860 --> 00:08:40.269 align:start position:0%
our network the key insight in how we
can<00:08:37.130><c> do</c><00:08:37.279><c> this</c><00:08:37.459><c> is</c><00:08:37.760><c> to</c><00:08:38.330><c> connect</c><00:08:39.229><c> patches</c><00:08:40.070><c> of</c><00:08:40.250><c> the</c>

00:08:40.269 --> 00:08:40.279 align:start position:0%
can do this is to connect patches of the
 

00:08:40.279 --> 00:08:43.449 align:start position:0%
can do this is to connect patches of the
input<00:08:40.930><c> represented</c><00:08:41.930><c> as</c><00:08:42.080><c> a</c><00:08:42.110><c> 2d</c><00:08:42.560><c> array</c><00:08:42.830><c> to</c>

00:08:43.449 --> 00:08:43.459 align:start position:0%
input represented as a 2d array to
 

00:08:43.459 --> 00:08:46.240 align:start position:0%
input represented as a 2d array to
neurons<00:08:43.789><c> in</c><00:08:44.089><c> hidden</c><00:08:44.390><c> layers</c><00:08:44.680><c> this</c><00:08:45.680><c> is</c><00:08:45.890><c> to</c><00:08:46.040><c> say</c>

00:08:46.240 --> 00:08:46.250 align:start position:0%
neurons in hidden layers this is to say
 

00:08:46.250 --> 00:08:48.100 align:start position:0%
neurons in hidden layers this is to say
that<00:08:46.490><c> each</c><00:08:46.670><c> neuron</c><00:08:47.060><c> in</c><00:08:47.300><c> a</c><00:08:47.390><c> hidden</c><00:08:47.660><c> layer</c><00:08:47.810><c> only</c>

00:08:48.100 --> 00:08:48.110 align:start position:0%
that each neuron in a hidden layer only
 

00:08:48.110 --> 00:08:50.829 align:start position:0%
that each neuron in a hidden layer only
sees<00:08:48.830><c> a</c><00:08:49.130><c> particular</c><00:08:49.400><c> region</c><00:08:49.940><c> of</c><00:08:50.270><c> what</c><00:08:50.690><c> the</c>

00:08:50.829 --> 00:08:50.839 align:start position:0%
sees a particular region of what the
 

00:08:50.839 --> 00:08:53.530 align:start position:0%
sees a particular region of what the
input<00:08:51.170><c> to</c><00:08:51.290><c> that</c><00:08:51.470><c> layer</c><00:08:51.650><c> is</c><00:08:52.209><c> this</c><00:08:53.209><c> will</c><00:08:53.390><c> not</c>

00:08:53.530 --> 00:08:53.540 align:start position:0%
input to that layer is this will not
 

00:08:53.540 --> 00:08:54.830 align:start position:0%
input to that layer is this will not
only<00:08:53.720><c> reduce</c><00:08:54.050><c> the</c><00:08:54.320><c> number</c><00:08:54.560><c> of</c>

00:08:54.830 --> 00:08:54.840 align:start position:0%
only reduce the number of
 

00:08:54.840 --> 00:08:57.140 align:start position:0%
only reduce the number of
in<00:08:55.080><c> our</c><00:08:55.170><c> network</c><00:08:55.560><c> but</c><00:08:55.800><c> also</c><00:08:56.370><c> allow</c><00:08:56.910><c> us</c><00:08:56.940><c> to</c>

00:08:57.140 --> 00:08:57.150 align:start position:0%
in our network but also allow us to
 

00:08:57.150 --> 00:08:59.450 align:start position:0%
in our network but also allow us to
leverage<00:08:57.810><c> the</c><00:08:58.440><c> fact</c><00:08:58.650><c> that</c><00:08:58.740><c> in</c><00:08:58.980><c> an</c><00:08:59.160><c> image</c>

00:08:59.450 --> 00:08:59.460 align:start position:0%
leverage the fact that in an image
 

00:08:59.460 --> 00:09:01.610 align:start position:0%
leverage the fact that in an image
pixels<00:09:00.120><c> that</c><00:09:00.300><c> are</c><00:09:00.450><c> near</c><00:09:01.020><c> each</c><00:09:01.050><c> other</c><00:09:01.320><c> are</c>

00:09:01.610 --> 00:09:01.620 align:start position:0%
pixels that are near each other are
 

00:09:01.620 --> 00:09:05.510 align:start position:0%
pixels that are near each other are
probably<00:09:02.310><c> somehow</c><00:09:02.640><c> related</c><00:09:04.190><c> to</c><00:09:05.190><c> define</c>

00:09:05.510 --> 00:09:05.520 align:start position:0%
probably somehow related to define
 

00:09:05.520 --> 00:09:08.060 align:start position:0%
probably somehow related to define
connections<00:09:06.090><c> across</c><00:09:06.420><c> the</c><00:09:06.690><c> whole</c><00:09:06.900><c> input</c><00:09:07.170><c> we</c>

00:09:08.060 --> 00:09:08.070 align:start position:0%
connections across the whole input we
 

00:09:08.070 --> 00:09:10.790 align:start position:0%
connections across the whole input we
can<00:09:08.220><c> apply</c><00:09:08.520><c> this</c><00:09:08.580><c> same</c><00:09:09.030><c> principle</c><00:09:09.510><c> by</c><00:09:10.320><c> sliding</c>

00:09:10.790 --> 00:09:10.800 align:start position:0%
can apply this same principle by sliding
 

00:09:10.800 --> 00:09:13.520 align:start position:0%
can apply this same principle by sliding
up<00:09:11.130><c> the</c><00:09:11.190><c> patch</c><00:09:11.910><c> window</c><00:09:12.420><c> across</c><00:09:12.900><c> the</c><00:09:13.170><c> entirety</c>

00:09:13.520 --> 00:09:13.530 align:start position:0%
up the patch window across the entirety
 

00:09:13.530 --> 00:09:16.100 align:start position:0%
up the patch window across the entirety
of<00:09:13.710><c> the</c><00:09:13.890><c> input</c><00:09:14.340><c> image</c><00:09:14.490><c> in</c><00:09:14.910><c> this</c><00:09:15.510><c> case</c><00:09:15.570><c> by</c><00:09:16.050><c> two</c>

00:09:16.100 --> 00:09:16.110 align:start position:0%
of the input image in this case by two
 

00:09:16.110 --> 00:09:18.980 align:start position:0%
of the input image in this case by two
units<00:09:16.680><c> in</c><00:09:17.030><c> this</c><00:09:18.030><c> way</c><00:09:18.240><c> we'll</c><00:09:18.570><c> take</c><00:09:18.810><c> into</c>

00:09:18.980 --> 00:09:18.990 align:start position:0%
units in this way we'll take into
 

00:09:18.990 --> 00:09:20.720 align:start position:0%
units in this way we'll take into
account<00:09:19.350><c> the</c><00:09:19.710><c> spatial</c><00:09:19.920><c> structure</c><00:09:20.400><c> that's</c>

00:09:20.720 --> 00:09:20.730 align:start position:0%
account the spatial structure that's
 

00:09:20.730 --> 00:09:24.620 align:start position:0%
account the spatial structure that's
present<00:09:22.160><c> but</c><00:09:23.160><c> remember</c><00:09:23.610><c> our</c><00:09:23.850><c> ultimate</c><00:09:24.420><c> task</c>

00:09:24.620 --> 00:09:24.630 align:start position:0%
present but remember our ultimate task
 

00:09:24.630 --> 00:09:27.860 align:start position:0%
present but remember our ultimate task
is<00:09:24.960><c> to</c><00:09:25.230><c> learn</c><00:09:25.440><c> visual</c><00:09:25.920><c> features</c><00:09:26.340><c> and</c><00:09:26.700><c> we</c><00:09:27.660><c> can</c>

00:09:27.860 --> 00:09:27.870 align:start position:0%
is to learn visual features and we can
 

00:09:27.870 --> 00:09:29.840 align:start position:0%
is to learn visual features and we can
do<00:09:28.020><c> this</c><00:09:28.200><c> by</c><00:09:28.440><c> smartly</c><00:09:29.220><c> weighting</c><00:09:29.730><c> the</c>

00:09:29.840 --> 00:09:29.850 align:start position:0%
do this by smartly weighting the
 

00:09:29.850 --> 00:09:33.280 align:start position:0%
do this by smartly weighting the
connections<00:09:30.450><c> between</c><00:09:30.900><c> a</c><00:09:31.440><c> patch</c><00:09:32.220><c> of</c><00:09:32.580><c> our</c><00:09:32.700><c> input</c>

00:09:33.280 --> 00:09:33.290 align:start position:0%
connections between a patch of our input
 

00:09:33.290 --> 00:09:36.080 align:start position:0%
connections between a patch of our input
to<00:09:34.290><c> the</c><00:09:34.620><c> neuron</c><00:09:34.860><c> it's</c><00:09:35.250><c> connected</c><00:09:35.730><c> to</c><00:09:35.760><c> in</c><00:09:36.000><c> the</c>

00:09:36.080 --> 00:09:36.090 align:start position:0%
to the neuron it's connected to in the
 

00:09:36.090 --> 00:09:38.240 align:start position:0%
to the neuron it's connected to in the
next<00:09:36.390><c> hidden</c><00:09:36.540><c> layer</c><00:09:36.660><c> so</c><00:09:37.530><c> as</c><00:09:37.650><c> to</c><00:09:37.830><c> detect</c>

00:09:38.240 --> 00:09:38.250 align:start position:0%
next hidden layer so as to detect
 

00:09:38.250 --> 00:09:40.700 align:start position:0%
next hidden layer so as to detect
particular<00:09:39.210><c> features</c><00:09:39.990><c> present</c><00:09:40.260><c> in</c><00:09:40.530><c> that</c>

00:09:40.700 --> 00:09:40.710 align:start position:0%
particular features present in that
 

00:09:40.710 --> 00:09:44.750 align:start position:0%
particular features present in that
input<00:09:40.890><c> and</c><00:09:42.440><c> essentially</c><00:09:43.440><c> what</c><00:09:44.130><c> this</c><00:09:44.280><c> amounts</c>

00:09:44.750 --> 00:09:44.760 align:start position:0%
input and essentially what this amounts
 

00:09:44.760 --> 00:09:47.300 align:start position:0%
input and essentially what this amounts
to<00:09:44.970><c> is</c><00:09:45.120><c> applying</c><00:09:45.930><c> a</c><00:09:46.080><c> filter</c><00:09:46.290><c> a</c><00:09:46.650><c> set</c><00:09:46.950><c> of</c><00:09:47.070><c> weights</c>

00:09:47.300 --> 00:09:47.310 align:start position:0%
to is applying a filter a set of weights
 

00:09:47.310 --> 00:09:52.010 align:start position:0%
to is applying a filter a set of weights
to<00:09:47.790><c> extract</c><00:09:48.240><c> local</c><00:09:48.630><c> features</c><00:09:50.750><c> and</c><00:09:51.750><c> it</c><00:09:51.900><c> would</c>

00:09:52.010 --> 00:09:52.020 align:start position:0%
to extract local features and it would
 

00:09:52.020 --> 00:09:53.780 align:start position:0%
to extract local features and it would
be<00:09:52.110><c> useful</c><00:09:52.350><c> for</c><00:09:52.560><c> us</c><00:09:52.770><c> in</c><00:09:53.160><c> our</c><00:09:53.340><c> classification</c>

00:09:53.780 --> 00:09:53.790 align:start position:0%
be useful for us in our classification
 

00:09:53.790 --> 00:09:56.540 align:start position:0%
be useful for us in our classification
pipeline<00:09:54.630><c> to</c><00:09:55.230><c> have</c><00:09:55.410><c> many</c><00:09:55.800><c> different</c><00:09:56.250><c> features</c>

00:09:56.540 --> 00:09:56.550 align:start position:0%
pipeline to have many different features
 

00:09:56.550 --> 00:09:58.640 align:start position:0%
pipeline to have many different features
to<00:09:56.700><c> work</c><00:09:56.910><c> with</c><00:09:57.120><c> and</c><00:09:57.390><c> we</c><00:09:57.870><c> can</c><00:09:58.020><c> do</c><00:09:58.170><c> this</c><00:09:58.350><c> by</c><00:09:58.590><c> using</c>

00:09:58.640 --> 00:09:58.650 align:start position:0%
to work with and we can do this by using
 

00:09:58.650 --> 00:10:01.700 align:start position:0%
to work with and we can do this by using
multiple<00:09:59.520><c> filters</c><00:10:00.210><c> multiple</c><00:10:01.140><c> sets</c><00:10:01.530><c> of</c>

00:10:01.700 --> 00:10:01.710 align:start position:0%
multiple filters multiple sets of
 

00:10:01.710 --> 00:10:05.090 align:start position:0%
multiple filters multiple sets of
weights<00:10:02.040><c> and</c><00:10:03.350><c> finally</c><00:10:04.350><c> we</c><00:10:04.530><c> want</c><00:10:04.770><c> to</c><00:10:04.860><c> be</c><00:10:05.010><c> able</c>

00:10:05.090 --> 00:10:05.100 align:start position:0%
weights and finally we want to be able
 

00:10:05.100 --> 00:10:07.130 align:start position:0%
weights and finally we want to be able
to<00:10:05.250><c> share</c><00:10:05.640><c> the</c><00:10:05.910><c> parameters</c><00:10:06.420><c> of</c><00:10:06.630><c> each</c><00:10:06.810><c> filter</c>

00:10:07.130 --> 00:10:07.140 align:start position:0%
to share the parameters of each filter
 

00:10:07.140 --> 00:10:09.740 align:start position:0%
to share the parameters of each filter
across<00:10:08.010><c> all</c><00:10:08.340><c> the</c><00:10:08.700><c> connections</c><00:10:09.210><c> between</c><00:10:09.420><c> the</c>

00:10:09.740 --> 00:10:09.750 align:start position:0%
across all the connections between the
 

00:10:09.750 --> 00:10:12.290 align:start position:0%
across all the connections between the
input<00:10:10.080><c> layer</c><00:10:10.260><c> and</c><00:10:10.530><c> the</c><00:10:10.800><c> next</c><00:10:11.010><c> layer</c><00:10:11.300><c> because</c>

00:10:12.290 --> 00:10:12.300 align:start position:0%
input layer and the next layer because
 

00:10:12.300 --> 00:10:15.020 align:start position:0%
input layer and the next layer because
the<00:10:12.570><c> features</c><00:10:12.960><c> that</c><00:10:12.990><c> matter</c><00:10:13.680><c> in</c><00:10:14.010><c> one</c><00:10:14.610><c> part</c><00:10:14.910><c> of</c>

00:10:15.020 --> 00:10:15.030 align:start position:0%
the features that matter in one part of
 

00:10:15.030 --> 00:10:16.970 align:start position:0%
the features that matter in one part of
the<00:10:15.090><c> input</c><00:10:15.420><c> should</c><00:10:15.840><c> matter</c><00:10:16.110><c> elsewhere</c><00:10:16.650><c> this</c>

00:10:16.970 --> 00:10:16.980 align:start position:0%
the input should matter elsewhere this
 

00:10:16.980 --> 00:10:20.030 align:start position:0%
the input should matter elsewhere this
is<00:10:17.160><c> the</c><00:10:17.280><c> same</c><00:10:17.520><c> concept</c><00:10:18.090><c> of</c><00:10:19.040><c> spatial</c>

00:10:20.030 --> 00:10:20.040 align:start position:0%
is the same concept of spatial
 

00:10:20.040 --> 00:10:23.680 align:start position:0%
is the same concept of spatial
invariants<00:10:20.550><c> that</c><00:10:20.700><c> I</c><00:10:20.730><c> alluded</c><00:10:21.060><c> to</c><00:10:21.270><c> earlier</c><00:10:22.280><c> in</c>

00:10:23.680 --> 00:10:23.690 align:start position:0%
invariants that I alluded to earlier in
 

00:10:23.690 --> 00:10:26.870 align:start position:0%
invariants that I alluded to earlier in
practice<00:10:24.690><c> this</c><00:10:25.410><c> amounts</c><00:10:26.040><c> to</c><00:10:26.070><c> a</c><00:10:26.280><c> patchy</c>

00:10:26.870 --> 00:10:26.880 align:start position:0%
practice this amounts to a patchy
 

00:10:26.880 --> 00:10:30.620 align:start position:0%
practice this amounts to a patchy
operation<00:10:27.660><c> known</c><00:10:28.320><c> as</c><00:10:28.400><c> convolution</c><00:10:29.630><c> let's</c>

00:10:30.620 --> 00:10:30.630 align:start position:0%
operation known as convolution let's
 

00:10:30.630 --> 00:10:32.200 align:start position:0%
operation known as convolution let's
first<00:10:30.870><c> think</c><00:10:30.930><c> about</c><00:10:31.140><c> this</c><00:10:31.500><c> at</c><00:10:31.710><c> a</c><00:10:31.770><c> high</c><00:10:32.010><c> level</c>

00:10:32.200 --> 00:10:32.210 align:start position:0%
first think about this at a high level
 

00:10:32.210 --> 00:10:34.750 align:start position:0%
first think about this at a high level
suppose<00:10:33.210><c> we</c><00:10:33.390><c> have</c><00:10:33.570><c> a</c><00:10:33.600><c> four</c><00:10:33.900><c> by</c><00:10:34.050><c> four</c><00:10:34.080><c> filter</c>

00:10:34.750 --> 00:10:34.760 align:start position:0%
suppose we have a four by four filter
 

00:10:34.760 --> 00:10:37.370 align:start position:0%
suppose we have a four by four filter
which<00:10:35.760><c> means</c><00:10:36.030><c> we</c><00:10:36.240><c> have</c><00:10:36.390><c> 16</c><00:10:36.870><c> different</c><00:10:37.020><c> weights</c>

00:10:37.370 --> 00:10:37.380 align:start position:0%
which means we have 16 different weights
 

00:10:37.380 --> 00:10:39.920 align:start position:0%
which means we have 16 different weights
and<00:10:37.680><c> we're</c><00:10:38.160><c> going</c><00:10:38.370><c> to</c><00:10:38.430><c> apply</c><00:10:38.580><c> the</c><00:10:38.790><c> same</c><00:10:39.180><c> filter</c>

00:10:39.920 --> 00:10:39.930 align:start position:0%
and we're going to apply the same filter
 

00:10:39.930 --> 00:10:42.610 align:start position:0%
and we're going to apply the same filter
to<00:10:40.500><c> four</c><00:10:40.740><c> by</c><00:10:40.920><c> four</c><00:10:41.160><c> patches</c><00:10:41.430><c> in</c><00:10:41.730><c> the</c><00:10:41.760><c> input</c><00:10:42.180><c> and</c>

00:10:42.610 --> 00:10:42.620 align:start position:0%
to four by four patches in the input and
 

00:10:42.620 --> 00:10:45.530 align:start position:0%
to four by four patches in the input and
use<00:10:43.620><c> the</c><00:10:43.800><c> result</c><00:10:44.130><c> of</c><00:10:44.280><c> that</c><00:10:44.400><c> operation</c><00:10:45.000><c> to</c>

00:10:45.530 --> 00:10:45.540 align:start position:0%
use the result of that operation to
 

00:10:45.540 --> 00:10:47.810 align:start position:0%
use the result of that operation to
somehow<00:10:46.050><c> influence</c><00:10:46.620><c> the</c><00:10:46.830><c> state</c><00:10:47.040><c> of</c><00:10:47.250><c> the</c>

00:10:47.810 --> 00:10:47.820 align:start position:0%
somehow influence the state of the
 

00:10:47.820 --> 00:10:49.670 align:start position:0%
somehow influence the state of the
neuron<00:10:48.090><c> in</c><00:10:48.300><c> the</c><00:10:48.330><c> next</c><00:10:48.660><c> layer</c><00:10:48.840><c> that</c><00:10:49.290><c> this</c><00:10:49.470><c> patch</c>

00:10:49.670 --> 00:10:49.680 align:start position:0%
neuron in the next layer that this patch
 

00:10:49.680 --> 00:10:52.490 align:start position:0%
neuron in the next layer that this patch
is<00:10:49.710><c> connected</c><00:10:50.430><c> to</c><00:10:50.810><c> then</c><00:10:51.810><c> we're</c><00:10:52.230><c> going</c><00:10:52.440><c> to</c>

00:10:52.490 --> 00:10:52.500 align:start position:0%
is connected to then we're going to
 

00:10:52.500 --> 00:10:56.480 align:start position:0%
is connected to then we're going to
shift<00:10:52.830><c> our</c><00:10:53.100><c> filter</c><00:10:53.550><c> by</c><00:10:53.700><c> two</c><00:10:53.760><c> pixels</c><00:10:54.390><c> as</c><00:10:55.490><c> for</c>

00:10:56.480 --> 00:10:56.490 align:start position:0%
shift our filter by two pixels as for
 

00:10:56.490 --> 00:10:58.670 align:start position:0%
shift our filter by two pixels as for
example<00:10:56.910><c> and</c><00:10:57.090><c> grab</c><00:10:57.540><c> the</c><00:10:57.780><c> next</c><00:10:57.990><c> patch</c><00:10:58.230><c> of</c><00:10:58.620><c> the</c>

00:10:58.670 --> 00:10:58.680 align:start position:0%
example and grab the next patch of the
 

00:10:58.680 --> 00:11:02.480 align:start position:0%
example and grab the next patch of the
input<00:10:59.600><c> so</c><00:11:00.650><c> in</c><00:11:01.650><c> this</c><00:11:01.770><c> way</c><00:11:01.950><c> we</c><00:11:02.130><c> can</c><00:11:02.250><c> start</c>

00:11:02.480 --> 00:11:02.490 align:start position:0%
input so in this way we can start
 

00:11:02.490 --> 00:11:04.340 align:start position:0%
input so in this way we can start
thinking<00:11:02.700><c> about</c><00:11:02.970><c> convolution</c><00:11:03.840><c> at</c><00:11:04.020><c> a</c><00:11:04.080><c> very</c>

00:11:04.340 --> 00:11:04.350 align:start position:0%
thinking about convolution at a very
 

00:11:04.350 --> 00:11:07.250 align:start position:0%
thinking about convolution at a very
high<00:11:04.560><c> level</c><00:11:05.180><c> but</c><00:11:06.180><c> you're</c><00:11:06.360><c> probably</c><00:11:06.660><c> wondering</c>

00:11:07.250 --> 00:11:07.260 align:start position:0%
high level but you're probably wondering
 

00:11:07.260 --> 00:11:09.120 align:start position:0%
high level but you're probably wondering
how<00:11:07.740><c> does</c><00:11:07.800><c> this</c><00:11:08.100><c> actually</c><00:11:08.340><c> work</c>

00:11:09.120 --> 00:11:09.130 align:start position:0%
how does this actually work
 

00:11:09.130 --> 00:11:11.610 align:start position:0%
how does this actually work
what<00:11:09.580><c> do</c><00:11:09.820><c> we</c><00:11:09.940><c> mean</c><00:11:10.150><c> by</c><00:11:10.180><c> features</c><00:11:10.720><c> and</c><00:11:11.020><c> how</c><00:11:11.560><c> does</c>

00:11:11.610 --> 00:11:11.620 align:start position:0%
what do we mean by features and how does
 

00:11:11.620 --> 00:11:13.860 align:start position:0%
what do we mean by features and how does
this<00:11:11.920><c> convolution</c><00:11:12.430><c> operation</c><00:11:12.700><c> allow</c><00:11:13.510><c> us</c><00:11:13.690><c> to</c>

00:11:13.860 --> 00:11:13.870 align:start position:0%
this convolution operation allow us to
 

00:11:13.870 --> 00:11:16.590 align:start position:0%
this convolution operation allow us to
extract<00:11:14.260><c> them</c><00:11:15.090><c> hopefully</c><00:11:16.090><c> we</c><00:11:16.270><c> can</c><00:11:16.420><c> make</c><00:11:16.570><c> this</c>

00:11:16.590 --> 00:11:16.600 align:start position:0%
extract them hopefully we can make this
 

00:11:16.600 --> 00:11:18.660 align:start position:0%
extract them hopefully we can make this
concrete<00:11:17.230><c> by</c><00:11:17.440><c> walking</c><00:11:17.860><c> through</c><00:11:18.100><c> a</c><00:11:18.130><c> couple</c><00:11:18.460><c> of</c>

00:11:18.660 --> 00:11:18.670 align:start position:0%
concrete by walking through a couple of
 

00:11:18.670 --> 00:11:22.650 align:start position:0%
concrete by walking through a couple of
examples<00:11:19.860><c> suppose</c><00:11:20.860><c> we</c><00:11:21.130><c> want</c><00:11:21.370><c> to</c><00:11:21.460><c> classify</c><00:11:21.850><c> X's</c>

00:11:22.650 --> 00:11:22.660 align:start position:0%
examples suppose we want to classify X's
 

00:11:22.660 --> 00:11:25.080 align:start position:0%
examples suppose we want to classify X's
from<00:11:23.170><c> a</c><00:11:23.320><c> set</c><00:11:23.560><c> of</c><00:11:23.740><c> black</c><00:11:23.980><c> and</c><00:11:24.220><c> white</c><00:11:24.250><c> images</c><00:11:24.820><c> of</c>

00:11:25.080 --> 00:11:25.090 align:start position:0%
from a set of black and white images of
 

00:11:25.090 --> 00:11:27.450 align:start position:0%
from a set of black and white images of
letters<00:11:25.420><c> where</c><00:11:26.110><c> black</c><00:11:26.410><c> is</c><00:11:26.620><c> equal</c><00:11:26.920><c> to</c><00:11:27.040><c> negative</c>

00:11:27.450 --> 00:11:27.460 align:start position:0%
letters where black is equal to negative
 

00:11:27.460 --> 00:11:30.600 align:start position:0%
letters where black is equal to negative
1<00:11:27.820><c> and</c><00:11:28.060><c> y</c><00:11:28.540><c> is</c><00:11:29.080><c> represented</c><00:11:29.770><c> by</c><00:11:29.800><c> a</c><00:11:29.920><c> value</c><00:11:30.280><c> of</c><00:11:30.370><c> 1</c>

00:11:30.600 --> 00:11:30.610 align:start position:0%
1 and y is represented by a value of 1
 

00:11:30.610 --> 00:11:33.930 align:start position:0%
1 and y is represented by a value of 1
for<00:11:31.470><c> classification</c><00:11:32.470><c> it</c><00:11:33.040><c> would</c><00:11:33.460><c> clearly</c><00:11:33.790><c> not</c>

00:11:33.930 --> 00:11:33.940 align:start position:0%
for classification it would clearly not
 

00:11:33.940 --> 00:11:36.240 align:start position:0%
for classification it would clearly not
be<00:11:34.090><c> possible</c><00:11:34.480><c> to</c><00:11:34.810><c> simply</c><00:11:35.320><c> compare</c><00:11:35.890><c> the</c><00:11:36.100><c> two</c>

00:11:36.240 --> 00:11:36.250 align:start position:0%
be possible to simply compare the two
 

00:11:36.250 --> 00:11:39.750 align:start position:0%
be possible to simply compare the two
matrices<00:11:36.640><c> to</c><00:11:37.360><c> see</c><00:11:37.540><c> if</c><00:11:37.630><c> they're</c><00:11:37.840><c> equal</c><00:11:38.520><c> we</c><00:11:39.520><c> want</c>

00:11:39.750 --> 00:11:39.760 align:start position:0%
matrices to see if they're equal we want
 

00:11:39.760 --> 00:11:43.350 align:start position:0%
matrices to see if they're equal we want
to<00:11:39.910><c> be</c><00:11:40.000><c> able</c><00:11:40.210><c> to</c><00:11:40.540><c> classify</c><00:11:40.930><c> an</c><00:11:41.350><c> X</c><00:11:41.590><c> as</c><00:11:41.950><c> an</c><00:11:42.640><c> X</c><00:11:42.850><c> even</c>

00:11:43.350 --> 00:11:43.360 align:start position:0%
to be able to classify an X as an X even
 

00:11:43.360 --> 00:11:47.040 align:start position:0%
to be able to classify an X as an X even
if<00:11:43.480><c> it's</c><00:11:43.660><c> shifted</c><00:11:44.520><c> shrunk</c><00:11:45.520><c> rotated</c><00:11:46.480><c> deformed</c>

00:11:47.040 --> 00:11:47.050 align:start position:0%
if it's shifted shrunk rotated deformed
 

00:11:47.050 --> 00:11:50.970 align:start position:0%
if it's shifted shrunk rotated deformed
transformed<00:11:47.710><c> in</c><00:11:47.860><c> some</c><00:11:48.100><c> way</c><00:11:49.620><c> we</c><00:11:50.620><c> want</c><00:11:50.860><c> our</c>

00:11:50.970 --> 00:11:50.980 align:start position:0%
transformed in some way we want our
 

00:11:50.980 --> 00:11:52.980 align:start position:0%
transformed in some way we want our
model<00:11:51.340><c> to</c><00:11:51.520><c> compare</c><00:11:51.910><c> the</c><00:11:52.090><c> images</c><00:11:52.510><c> of</c><00:11:52.690><c> an</c><00:11:52.780><c> X</c>

00:11:52.980 --> 00:11:52.990 align:start position:0%
model to compare the images of an X
 

00:11:52.990 --> 00:11:55.500 align:start position:0%
model to compare the images of an X
piece<00:11:53.380><c> by</c><00:11:53.620><c> piece</c><00:11:53.650><c> and</c><00:11:54.220><c> look</c><00:11:55.090><c> for</c><00:11:55.390><c> these</c>

00:11:55.500 --> 00:11:55.510 align:start position:0%
piece by piece and look for these
 

00:11:55.510 --> 00:11:59.250 align:start position:0%
piece by piece and look for these
important<00:11:56.080><c> pieces</c><00:11:56.470><c> that</c><00:11:56.980><c> define</c><00:11:57.370><c> an</c><00:11:57.820><c> X</c><00:11:58.780><c> as</c><00:11:59.020><c> an</c>

00:11:59.250 --> 00:11:59.260 align:start position:0%
important pieces that define an X as an
 

00:11:59.260 --> 00:12:02.130 align:start position:0%
important pieces that define an X as an
X<00:11:59.470><c> those</c><00:12:00.220><c> are</c><00:12:00.400><c> the</c><00:12:00.520><c> features</c><00:12:00.880><c> and</c><00:12:01.210><c> if</c><00:12:01.960><c> our</c>

00:12:02.130 --> 00:12:02.140 align:start position:0%
X those are the features and if our
 

00:12:02.140 --> 00:12:04.290 align:start position:0%
X those are the features and if our
model<00:12:02.320><c> can</c><00:12:02.680><c> find</c><00:12:02.950><c> rough</c><00:12:03.220><c> feature</c><00:12:03.520><c> matches</c><00:12:04.090><c> in</c>

00:12:04.290 --> 00:12:04.300 align:start position:0%
model can find rough feature matches in
 

00:12:04.300 --> 00:12:06.300 align:start position:0%
model can find rough feature matches in
roughly<00:12:04.900><c> the</c><00:12:04.990><c> same</c><00:12:05.290><c> positions</c><00:12:05.890><c> in</c><00:12:06.100><c> two</c>

00:12:06.300 --> 00:12:06.310 align:start position:0%
roughly the same positions in two
 

00:12:06.310 --> 00:12:08.910 align:start position:0%
roughly the same positions in two
different<00:12:06.520><c> images</c><00:12:07.150><c> it</c><00:12:07.510><c> can</c><00:12:08.020><c> get</c><00:12:08.350><c> a</c><00:12:08.500><c> lot</c><00:12:08.740><c> better</c>

00:12:08.910 --> 00:12:08.920 align:start position:0%
different images it can get a lot better
 

00:12:08.920 --> 00:12:10.950 align:start position:0%
different images it can get a lot better
at<00:12:09.250><c> seeing</c><00:12:09.580><c> the</c><00:12:09.930><c> similarity</c><00:12:10.930><c> between</c>

00:12:10.950 --> 00:12:10.960 align:start position:0%
at seeing the similarity between
 

00:12:10.960 --> 00:12:15.390 align:start position:0%
at seeing the similarity between
different<00:12:11.860><c> examples</c><00:12:12.550><c> of</c><00:12:12.640><c> X's</c><00:12:13.030><c> you</c><00:12:14.190><c> can</c><00:12:15.190><c> think</c>

00:12:15.390 --> 00:12:15.400 align:start position:0%
different examples of X's you can think
 

00:12:15.400 --> 00:12:18.140 align:start position:0%
different examples of X's you can think
of<00:12:15.490><c> each</c><00:12:15.700><c> feature</c><00:12:16.030><c> as</c><00:12:16.360><c> a</c><00:12:16.780><c> mini</c><00:12:17.110><c> image</c><00:12:17.410><c> a</c><00:12:17.560><c> small</c>

00:12:18.140 --> 00:12:18.150 align:start position:0%
of each feature as a mini image a small
 

00:12:18.150 --> 00:12:20.190 align:start position:0%
of each feature as a mini image a small
two-dimensional<00:12:19.150><c> array</c><00:12:19.420><c> of</c><00:12:19.570><c> values</c><00:12:19.930><c> and</c>

00:12:20.190 --> 00:12:20.200 align:start position:0%
two-dimensional array of values and
 

00:12:20.200 --> 00:12:22.290 align:start position:0%
two-dimensional array of values and
we're<00:12:20.500><c> going</c><00:12:20.710><c> to</c><00:12:20.770><c> use</c><00:12:21.040><c> filters</c><00:12:21.490><c> to</c><00:12:21.520><c> pick</c><00:12:21.970><c> up</c><00:12:22.150><c> on</c>

00:12:22.290 --> 00:12:22.300 align:start position:0%
we're going to use filters to pick up on
 

00:12:22.300 --> 00:12:25.800 align:start position:0%
we're going to use filters to pick up on
the<00:12:22.420><c> features</c><00:12:22.750><c> common</c><00:12:23.560><c> to</c><00:12:23.680><c> X's</c><00:12:24.070><c> in</c><00:12:24.630><c> the</c><00:12:25.630><c> case</c>

00:12:25.800 --> 00:12:25.810 align:start position:0%
the features common to X's in the case
 

00:12:25.810 --> 00:12:28.200 align:start position:0%
the features common to X's in the case
of<00:12:26.050><c> an</c><00:12:26.140><c> X</c><00:12:26.320><c> filters</c><00:12:26.980><c> that</c><00:12:27.220><c> can</c><00:12:27.670><c> pick</c><00:12:27.850><c> up</c><00:12:28.000><c> on</c>

00:12:28.200 --> 00:12:28.210 align:start position:0%
of an X filters that can pick up on
 

00:12:28.210 --> 00:12:30.630 align:start position:0%
of an X filters that can pick up on
diagonal<00:12:28.840><c> lines</c><00:12:29.110><c> and</c><00:12:29.500><c> a</c><00:12:29.800><c> crossing</c><00:12:30.430><c> will</c>

00:12:30.630 --> 00:12:30.640 align:start position:0%
diagonal lines and a crossing will
 

00:12:30.640 --> 00:12:32.840 align:start position:0%
diagonal lines and a crossing will
probably<00:12:31.180><c> capture</c><00:12:31.750><c> all</c><00:12:31.930><c> the</c><00:12:32.170><c> important</c>

00:12:32.840 --> 00:12:32.850 align:start position:0%
probably capture all the important
 

00:12:32.850 --> 00:12:36.420 align:start position:0%
probably capture all the important
characteristics<00:12:33.850><c> of</c><00:12:34.030><c> most</c><00:12:34.270><c> X's</c><00:12:34.860><c> so</c><00:12:35.860><c> know</c><00:12:36.100><c> here</c>

00:12:36.420 --> 00:12:36.430 align:start position:0%
characteristics of most X's so know here
 

00:12:36.430 --> 00:12:38.700 align:start position:0%
characteristics of most X's so know here
that<00:12:36.640><c> these</c><00:12:36.760><c> smaller</c><00:12:37.150><c> matrices</c><00:12:37.990><c> are</c><00:12:38.440><c> the</c>

00:12:38.700 --> 00:12:38.710 align:start position:0%
that these smaller matrices are the
 

00:12:38.710 --> 00:12:41.490 align:start position:0%
that these smaller matrices are the
filters<00:12:39.220><c> of</c><00:12:39.400><c> weights</c><00:12:39.700><c> that</c><00:12:40.210><c> we'll</c><00:12:40.390><c> use</c><00:12:40.420><c> in</c><00:12:40.990><c> our</c>

00:12:41.490 --> 00:12:41.500 align:start position:0%
filters of weights that we'll use in our
 

00:12:41.500 --> 00:12:43.770 align:start position:0%
filters of weights that we'll use in our
convolution<00:12:42.130><c> operation</c><00:12:42.220><c> to</c><00:12:43.180><c> detect</c><00:12:43.570><c> the</c>

00:12:43.770 --> 00:12:43.780 align:start position:0%
convolution operation to detect the
 

00:12:43.780 --> 00:12:46.520 align:start position:0%
convolution operation to detect the
corresponding<00:12:44.440><c> features</c><00:12:44.830><c> in</c><00:12:45.070><c> an</c><00:12:45.670><c> input</c><00:12:45.970><c> image</c>

00:12:46.520 --> 00:12:46.530 align:start position:0%
corresponding features in an input image
 

00:12:46.530 --> 00:12:49.680 align:start position:0%
corresponding features in an input image
so<00:12:47.530><c> now</c><00:12:47.680><c> all</c><00:12:48.010><c> that's</c><00:12:48.250><c> left</c><00:12:48.430><c> is</c><00:12:48.880><c> to</c><00:12:48.940><c> define</c><00:12:49.540><c> this</c>

00:12:49.680 --> 00:12:49.690 align:start position:0%
so now all that's left is to define this
 

00:12:49.690 --> 00:12:52.410 align:start position:0%
so now all that's left is to define this
operation<00:12:50.320><c> that</c><00:12:50.860><c> we'll</c><00:12:51.070><c> pick</c><00:12:51.340><c> up</c><00:12:51.370><c> on</c><00:12:51.700><c> when</c>

00:12:52.410 --> 00:12:52.420 align:start position:0%
operation that we'll pick up on when
 

00:12:52.420 --> 00:12:54.920 align:start position:0%
operation that we'll pick up on when
these<00:12:52.600><c> features</c><00:12:52.990><c> pop</c><00:12:53.230><c> up</c><00:12:53.440><c> in</c><00:12:53.650><c> our</c><00:12:54.130><c> image</c><00:12:54.430><c> and</c>

00:12:54.920 --> 00:12:54.930 align:start position:0%
these features pop up in our image and
 

00:12:54.930 --> 00:12:57.800 align:start position:0%
these features pop up in our image and
that<00:12:55.930><c> operation</c><00:12:56.500><c> is</c><00:12:56.650><c> convolution</c>

00:12:57.800 --> 00:12:57.810 align:start position:0%
that operation is convolution
 

00:12:57.810 --> 00:13:00.270 align:start position:0%
that operation is convolution
convolution<00:12:58.810><c> preserves</c><00:12:59.350><c> the</c><00:12:59.830><c> spatial</c>

00:13:00.270 --> 00:13:00.280 align:start position:0%
convolution preserves the spatial
 

00:13:00.280 --> 00:13:03.840 align:start position:0%
convolution preserves the spatial
relationship<00:13:01.240><c> between</c><00:13:02.110><c> pixels</c><00:13:02.650><c> by</c><00:13:03.550><c> learning</c>

00:13:03.840 --> 00:13:03.850 align:start position:0%
relationship between pixels by learning
 

00:13:03.850 --> 00:13:06.390 align:start position:0%
relationship between pixels by learning
image<00:13:04.300><c> features</c><00:13:04.660><c> in</c><00:13:04.870><c> small</c><00:13:05.380><c> squares</c><00:13:06.070><c> of</c><00:13:06.310><c> the</c>

00:13:06.390 --> 00:13:06.400 align:start position:0%
image features in small squares of the
 

00:13:06.400 --> 00:13:09.540 align:start position:0%
image features in small squares of the
input<00:13:07.410><c> to</c><00:13:08.410><c> do</c><00:13:08.530><c> this</c><00:13:08.680><c> we</c><00:13:08.950><c> perform</c><00:13:09.190><c> an</c>

00:13:09.540 --> 00:13:09.550 align:start position:0%
input to do this we perform an
 

00:13:09.550 --> 00:13:12.270 align:start position:0%
input to do this we perform an
element-wise<00:13:10.060><c> multiplication</c><00:13:10.890><c> between</c><00:13:11.890><c> the</c>

00:13:12.270 --> 00:13:12.280 align:start position:0%
element-wise multiplication between the
 

00:13:12.280 --> 00:13:14.850 align:start position:0%
element-wise multiplication between the
filter<00:13:12.490><c> matrix</c><00:13:13.060><c> and</c><00:13:13.330><c> a</c><00:13:13.840><c> patch</c><00:13:14.050><c> of</c><00:13:14.380><c> the</c><00:13:14.440><c> input</c>

00:13:14.850 --> 00:13:14.860 align:start position:0%
filter matrix and a patch of the input
 

00:13:14.860 --> 00:13:17.190 align:start position:0%
filter matrix and a patch of the input
image<00:13:15.010><c> that's</c><00:13:15.490><c> of</c><00:13:15.760><c> the</c><00:13:15.940><c> same</c><00:13:16.150><c> dimensions</c><00:13:17.050><c> as</c>

00:13:17.190 --> 00:13:17.200 align:start position:0%
image that's of the same dimensions as
 

00:13:17.200 --> 00:13:20.310 align:start position:0%
image that's of the same dimensions as
the<00:13:17.230><c> filter</c><00:13:18.000><c> this</c><00:13:19.000><c> results</c><00:13:19.540><c> in</c><00:13:19.660><c> a</c><00:13:19.750><c> 3</c><00:13:19.990><c> by</c><00:13:20.110><c> 3</c>

00:13:20.310 --> 00:13:20.320 align:start position:0%
the filter this results in a 3 by 3
 

00:13:20.320 --> 00:13:22.009 align:start position:0%
the filter this results in a 3 by 3
matrix<00:13:20.800><c> in</c>

00:13:22.009 --> 00:13:22.019 align:start position:0%
matrix in
 

00:13:22.019 --> 00:13:24.139 align:start position:0%
matrix in
example<00:13:22.470><c> you</c><00:13:22.499><c> see</c><00:13:22.889><c> here</c><00:13:23.220><c> all</c><00:13:23.399><c> entries</c><00:13:23.910><c> in</c><00:13:24.059><c> this</c>

00:13:24.139 --> 00:13:24.149 align:start position:0%
example you see here all entries in this
 

00:13:24.149 --> 00:13:26.479 align:start position:0%
example you see here all entries in this
matrix<00:13:24.209><c> are</c><00:13:24.869><c> won</c><00:13:25.110><c> and</c><00:13:25.350><c> that's</c><00:13:26.069><c> because</c>

00:13:26.479 --> 00:13:26.489 align:start position:0%
matrix are won and that's because
 

00:13:26.489 --> 00:13:28.369 align:start position:0%
matrix are won and that's because
there's<00:13:26.699><c> a</c><00:13:26.759><c> perfect</c><00:13:27.179><c> correspondence</c><00:13:28.139><c> between</c>

00:13:28.369 --> 00:13:28.379 align:start position:0%
there's a perfect correspondence between
 

00:13:28.379 --> 00:13:31.280 align:start position:0%
there's a perfect correspondence between
our<00:13:28.889><c> filter</c><00:13:29.339><c> and</c><00:13:29.550><c> the</c><00:13:30.209><c> region</c><00:13:30.629><c> of</c><00:13:30.809><c> the</c><00:13:30.929><c> input</c>

00:13:31.280 --> 00:13:31.290 align:start position:0%
our filter and the region of the input
 

00:13:31.290 --> 00:13:34.160 align:start position:0%
our filter and the region of the input
that<00:13:31.439><c> we're</c><00:13:31.589><c> convolving</c><00:13:32.100><c> it</c><00:13:32.249><c> with</c><00:13:32.929><c> finally</c><00:13:33.929><c> we</c>

00:13:34.160 --> 00:13:34.170 align:start position:0%
that we're convolving it with finally we
 

00:13:34.170 --> 00:13:37.309 align:start position:0%
that we're convolving it with finally we
sum<00:13:34.439><c> all</c><00:13:34.709><c> the</c><00:13:35.069><c> entries</c><00:13:35.610><c> of</c><00:13:35.790><c> this</c><00:13:35.879><c> matrix</c><00:13:36.050><c> get</c><00:13:37.050><c> 9</c>

00:13:37.309 --> 00:13:37.319 align:start position:0%
sum all the entries of this matrix get 9
 

00:13:37.319 --> 00:13:39.919 align:start position:0%
sum all the entries of this matrix get 9
and<00:13:37.559><c> that's</c><00:13:38.429><c> the</c><00:13:38.699><c> result</c><00:13:39.029><c> of</c><00:13:39.239><c> our</c><00:13:39.329><c> convolution</c>

00:13:39.919 --> 00:13:39.929 align:start position:0%
and that's the result of our convolution
 

00:13:39.929 --> 00:13:44.239 align:start position:0%
and that's the result of our convolution
operation<00:13:42.619><c> let's</c><00:13:43.619><c> consider</c><00:13:43.920><c> one</c><00:13:44.220><c> more</c>

00:13:44.239 --> 00:13:44.249 align:start position:0%
operation let's consider one more
 

00:13:44.249 --> 00:13:47.479 align:start position:0%
operation let's consider one more
example<00:13:45.139><c> suppose</c><00:13:46.139><c> now</c><00:13:46.379><c> we</c><00:13:46.439><c> want</c><00:13:46.949><c> to</c><00:13:47.129><c> compute</c>

00:13:47.479 --> 00:13:47.489 align:start position:0%
example suppose now we want to compute
 

00:13:47.489 --> 00:13:49.789 align:start position:0%
example suppose now we want to compute
the<00:13:47.579><c> convolution</c><00:13:47.999><c> of</c><00:13:48.329><c> a</c><00:13:48.689><c> five</c><00:13:48.899><c> by</c><00:13:49.079><c> five</c><00:13:49.110><c> image</c>

00:13:49.789 --> 00:13:49.799 align:start position:0%
the convolution of a five by five image
 

00:13:49.799 --> 00:13:52.759 align:start position:0%
the convolution of a five by five image
and<00:13:50.040><c> a</c><00:13:50.459><c> three</c><00:13:50.670><c> by</c><00:13:50.699><c> three</c><00:13:50.850><c> filter</c><00:13:51.449><c> to</c><00:13:52.379><c> do</c><00:13:52.559><c> this</c>

00:13:52.759 --> 00:13:52.769 align:start position:0%
and a three by three filter to do this
 

00:13:52.769 --> 00:13:55.249 align:start position:0%
and a three by three filter to do this
we<00:13:53.009><c> need</c><00:13:53.189><c> to</c><00:13:53.339><c> cover</c><00:13:54.059><c> the</c><00:13:54.329><c> entirety</c><00:13:54.959><c> of</c><00:13:54.989><c> the</c>

00:13:55.249 --> 00:13:55.259 align:start position:0%
we need to cover the entirety of the
 

00:13:55.259 --> 00:13:58.400 align:start position:0%
we need to cover the entirety of the
input<00:13:55.589><c> image</c><00:13:55.739><c> by</c><00:13:56.579><c> sliding</c><00:13:56.999><c> the</c><00:13:57.299><c> filter</c><00:13:57.660><c> over</c>

00:13:58.400 --> 00:13:58.410 align:start position:0%
input image by sliding the filter over
 

00:13:58.410 --> 00:14:01.160 align:start position:0%
input image by sliding the filter over
the<00:13:58.889><c> image</c><00:13:59.189><c> performing</c><00:14:00.179><c> the</c><00:14:00.299><c> same</c><00:14:00.569><c> element</c>

00:14:01.160 --> 00:14:01.170 align:start position:0%
the image performing the same element
 

00:14:01.170 --> 00:14:04.280 align:start position:0%
the image performing the same element
wise<00:14:01.379><c> multiplication</c><00:14:01.709><c> and</c><00:14:02.610><c> addition</c><00:14:03.290><c> let's</c>

00:14:04.280 --> 00:14:04.290 align:start position:0%
wise multiplication and addition let's
 

00:14:04.290 --> 00:14:06.590 align:start position:0%
wise multiplication and addition let's
see<00:14:04.470><c> what</c><00:14:04.619><c> this</c><00:14:04.769><c> looks</c><00:14:04.980><c> like</c><00:14:05.329><c> we'll</c><00:14:06.329><c> first</c>

00:14:06.590 --> 00:14:06.600 align:start position:0%
see what this looks like we'll first
 

00:14:06.600 --> 00:14:08.559 align:start position:0%
see what this looks like we'll first
start<00:14:06.869><c> off</c><00:14:06.989><c> in</c><00:14:07.290><c> the</c><00:14:07.410><c> upper</c><00:14:07.559><c> left</c><00:14:07.949><c> corner</c>

00:14:08.559 --> 00:14:08.569 align:start position:0%
start off in the upper left corner
 

00:14:08.569 --> 00:14:11.840 align:start position:0%
start off in the upper left corner
element<00:14:09.569><c> wise</c><00:14:09.779><c> multiply</c><00:14:10.679><c> this</c><00:14:10.739><c> 3</c><00:14:11.220><c> by</c><00:14:11.339><c> 3</c><00:14:11.549><c> patch</c>

00:14:11.840 --> 00:14:11.850 align:start position:0%
element wise multiply this 3 by 3 patch
 

00:14:11.850 --> 00:14:14.479 align:start position:0%
element wise multiply this 3 by 3 patch
with<00:14:12.749><c> the</c><00:14:12.779><c> entries</c><00:14:13.259><c> of</c><00:14:13.410><c> the</c><00:14:13.499><c> filter</c><00:14:13.860><c> and</c><00:14:14.069><c> then</c>

00:14:14.479 --> 00:14:14.489 align:start position:0%
with the entries of the filter and then
 

00:14:14.489 --> 00:14:17.359 align:start position:0%
with the entries of the filter and then
add<00:14:14.749><c> this</c><00:14:15.749><c> results</c><00:14:16.319><c> in</c><00:14:16.439><c> the</c><00:14:16.559><c> first</c><00:14:16.769><c> entry</c><00:14:17.040><c> in</c>

00:14:17.359 --> 00:14:17.369 align:start position:0%
add this results in the first entry in
 

00:14:17.369 --> 00:14:19.429 align:start position:0%
add this results in the first entry in
our<00:14:17.549><c> output</c><00:14:17.939><c> matrix</c><00:14:18.449><c> called</c>

00:14:19.429 --> 00:14:19.439 align:start position:0%
our output matrix called
 

00:14:19.439 --> 00:14:23.840 align:start position:0%
our output matrix called
the<00:14:19.739><c> feature</c><00:14:19.949><c> map</c><00:14:21.709><c> we'll</c><00:14:22.709><c> next</c><00:14:23.100><c> slide</c><00:14:23.399><c> the</c><00:14:23.610><c> 3</c>

00:14:23.840 --> 00:14:23.850 align:start position:0%
the feature map we'll next slide the 3
 

00:14:23.850 --> 00:14:27.769 align:start position:0%
the feature map we'll next slide the 3
by<00:14:23.970><c> 3</c><00:14:23.999><c> filter</c><00:14:24.899><c> over</c><00:14:25.679><c> by</c><00:14:25.860><c> 1</c><00:14:26.100><c> to</c><00:14:27.089><c> grab</c><00:14:27.299><c> the</c><00:14:27.480><c> next</c>

00:14:27.769 --> 00:14:27.779 align:start position:0%
by 3 filter over by 1 to grab the next
 

00:14:27.779 --> 00:14:30.139 align:start position:0%
by 3 filter over by 1 to grab the next
patch<00:14:27.959><c> and</c><00:14:28.290><c> repeat</c><00:14:28.860><c> the</c><00:14:29.069><c> same</c><00:14:29.369><c> operation</c>

00:14:30.139 --> 00:14:30.149 align:start position:0%
patch and repeat the same operation
 

00:14:30.149 --> 00:14:33.109 align:start position:0%
patch and repeat the same operation
amande<00:14:30.959><c> wise</c><00:14:31.079><c> multiplication</c><00:14:31.459><c> addition</c><00:14:32.459><c> this</c>

00:14:33.109 --> 00:14:33.119 align:start position:0%
amande wise multiplication addition this
 

00:14:33.119 --> 00:14:35.600 align:start position:0%
amande wise multiplication addition this
results<00:14:33.629><c> in</c><00:14:33.749><c> the</c><00:14:33.869><c> second</c><00:14:34.199><c> entry</c><00:14:34.350><c> and</c><00:14:34.769><c> we</c>

00:14:35.600 --> 00:14:35.610 align:start position:0%
results in the second entry and we
 

00:14:35.610 --> 00:14:37.699 align:start position:0%
results in the second entry and we
continue<00:14:36.149><c> in</c><00:14:36.299><c> this</c><00:14:36.389><c> way</c><00:14:36.449><c> until</c><00:14:36.629><c> we</c><00:14:37.499><c> have</c>

00:14:37.699 --> 00:14:37.709 align:start position:0%
continue in this way until we have
 

00:14:37.709 --> 00:14:42.470 align:start position:0%
continue in this way until we have
covered<00:14:38.220><c> the</c><00:14:39.059><c> entirety</c><00:14:39.779><c> of</c><00:14:40.100><c> our</c><00:14:41.100><c> 5x5</c><00:14:42.089><c> image</c>

00:14:42.470 --> 00:14:42.480 align:start position:0%
covered the entirety of our 5x5 image
 

00:14:42.480 --> 00:14:46.429 align:start position:0%
covered the entirety of our 5x5 image
and<00:14:42.980><c> that's</c><00:14:43.980><c> in</c><00:14:44.329><c> the</c><00:14:45.329><c> feature</c><00:14:45.569><c> map</c><00:14:45.899><c> reflects</c>

00:14:46.429 --> 00:14:46.439 align:start position:0%
and that's in the feature map reflects
 

00:14:46.439 --> 00:14:49.100 align:start position:0%
and that's in the feature map reflects
where<00:14:46.769><c> in</c><00:14:46.980><c> the</c><00:14:47.129><c> input</c><00:14:47.490><c> was</c><00:14:48.179><c> activated</c><00:14:48.899><c> by</c><00:14:49.049><c> this</c>

00:14:49.100 --> 00:14:49.110 align:start position:0%
where in the input was activated by this
 

00:14:49.110 --> 00:14:52.489 align:start position:0%
where in the input was activated by this
filter<00:14:49.679><c> that</c><00:14:49.829><c> we</c><00:14:49.949><c> applied</c><00:14:51.110><c> now</c><00:14:52.110><c> that</c><00:14:52.350><c> we've</c>

00:14:52.489 --> 00:14:52.499 align:start position:0%
filter that we applied now that we've
 

00:14:52.499 --> 00:14:54.139 align:start position:0%
filter that we applied now that we've
gone<00:14:52.679><c> through</c><00:14:52.949><c> the</c><00:14:53.100><c> mechanism</c><00:14:53.670><c> of</c><00:14:53.850><c> the</c>

00:14:54.139 --> 00:14:54.149 align:start position:0%
gone through the mechanism of the
 

00:14:54.149 --> 00:14:57.470 align:start position:0%
gone through the mechanism of the
convolution<00:14:54.749><c> operation</c><00:14:56.089><c> let's</c><00:14:57.089><c> see</c><00:14:57.269><c> how</c>

00:14:57.470 --> 00:14:57.480 align:start position:0%
convolution operation let's see how
 

00:14:57.480 --> 00:14:59.809 align:start position:0%
convolution operation let's see how
different<00:14:57.720><c> filters</c><00:14:58.290><c> can</c><00:14:58.589><c> be</c><00:14:58.769><c> used</c><00:14:59.339><c> to</c><00:14:59.610><c> produce</c>

00:14:59.809 --> 00:14:59.819 align:start position:0%
different filters can be used to produce
 

00:14:59.819 --> 00:15:02.629 align:start position:0%
different filters can be used to produce
different<00:15:00.269><c> feature</c><00:15:00.660><c> Maps</c><00:15:01.160><c> so</c><00:15:02.160><c> on</c><00:15:02.309><c> the</c><00:15:02.429><c> Left</c>

00:15:02.629 --> 00:15:02.639 align:start position:0%
different feature Maps so on the Left
 

00:15:02.639 --> 00:15:04.280 align:start position:0%
different feature Maps so on the Left
you'll<00:15:02.910><c> see</c><00:15:02.970><c> a</c><00:15:03.119><c> picture</c><00:15:03.360><c> of</c><00:15:03.509><c> a</c><00:15:03.779><c> woman's</c><00:15:04.079><c> face</c>

00:15:04.280 --> 00:15:04.290 align:start position:0%
you'll see a picture of a woman's face
 

00:15:04.290 --> 00:15:07.879 align:start position:0%
you'll see a picture of a woman's face
and<00:15:05.119><c> next</c><00:15:06.119><c> to</c><00:15:06.240><c> it</c><00:15:06.360><c> the</c><00:15:06.779><c> output</c><00:15:06.959><c> of</c><00:15:07.410><c> applying</c>

00:15:07.879 --> 00:15:07.889 align:start position:0%
and next to it the output of applying
 

00:15:07.889 --> 00:15:10.639 align:start position:0%
and next to it the output of applying
three<00:15:08.519><c> different</c><00:15:08.970><c> convolutional</c><00:15:09.689><c> filters</c><00:15:10.079><c> to</c>

00:15:10.639 --> 00:15:10.649 align:start position:0%
three different convolutional filters to
 

00:15:10.649 --> 00:15:13.519 align:start position:0%
three different convolutional filters to
that<00:15:10.799><c> same</c><00:15:11.100><c> image</c><00:15:11.519><c> and</c><00:15:11.899><c> you</c><00:15:12.899><c> can</c><00:15:12.929><c> appreciate</c>

00:15:13.519 --> 00:15:13.529 align:start position:0%
that same image and you can appreciate
 

00:15:13.529 --> 00:15:15.829 align:start position:0%
that same image and you can appreciate
that<00:15:13.740><c> by</c><00:15:13.860><c> simply</c><00:15:14.189><c> changing</c><00:15:15.119><c> the</c><00:15:15.389><c> weights</c><00:15:15.629><c> of</c>

00:15:15.829 --> 00:15:15.839 align:start position:0%
that by simply changing the weights of
 

00:15:15.839 --> 00:15:18.229 align:start position:0%
that by simply changing the weights of
the<00:15:15.869><c> filters</c><00:15:16.319><c> we</c><00:15:17.069><c> can</c><00:15:17.100><c> detect</c><00:15:17.610><c> different</c>

00:15:18.229 --> 00:15:18.239 align:start position:0%
the filters we can detect different
 

00:15:18.239 --> 00:15:22.850 align:start position:0%
the filters we can detect different
features<00:15:18.569><c> from</c><00:15:19.019><c> the</c><00:15:19.139><c> image</c><00:15:20.989><c> so</c><00:15:21.989><c> I</c><00:15:22.319><c> hope</c><00:15:22.709><c> you</c>

00:15:22.850 --> 00:15:22.860 align:start position:0%
features from the image so I hope you
 

00:15:22.860 --> 00:15:25.909 align:start position:0%
features from the image so I hope you
can<00:15:23.100><c> now</c><00:15:23.279><c> see</c><00:15:23.610><c> how</c><00:15:24.019><c> convolution</c><00:15:25.019><c> allows</c><00:15:25.350><c> us</c><00:15:25.619><c> to</c>

00:15:25.909 --> 00:15:25.919 align:start position:0%
can now see how convolution allows us to
 

00:15:25.919 --> 00:15:27.789 align:start position:0%
can now see how convolution allows us to
capitalize<00:15:26.249><c> on</c><00:15:26.579><c> the</c><00:15:26.999><c> spatial</c><00:15:27.419><c> structure</c>

00:15:27.789 --> 00:15:27.799 align:start position:0%
capitalize on the spatial structure
 

00:15:27.799 --> 00:15:31.489 align:start position:0%
capitalize on the spatial structure
inherent<00:15:28.799><c> to</c><00:15:28.889><c> image</c><00:15:29.189><c> data</c><00:15:29.429><c> and</c><00:15:29.929><c> use</c><00:15:30.929><c> sets</c><00:15:31.290><c> of</c>

00:15:31.489 --> 00:15:31.499 align:start position:0%
inherent to image data and use sets of
 

00:15:31.499 --> 00:15:34.460 align:start position:0%
inherent to image data and use sets of
weights<00:15:31.709><c> to</c><00:15:32.160><c> extract</c><00:15:32.579><c> local</c><00:15:32.999><c> features</c><00:15:33.360><c> and</c><00:15:34.290><c> to</c>

00:15:34.460 --> 00:15:34.470 align:start position:0%
weights to extract local features and to
 

00:15:34.470 --> 00:15:35.870 align:start position:0%
weights to extract local features and to
very<00:15:34.709><c> easily</c><00:15:34.949><c> be</c><00:15:35.309><c> able</c><00:15:35.429><c> to</c>

00:15:35.870 --> 00:15:35.880 align:start position:0%
very easily be able to
 

00:15:35.880 --> 00:15:38.060 align:start position:0%
very easily be able to
affect<00:15:36.090><c> different</c><00:15:36.660><c> types</c><00:15:36.900><c> of</c><00:15:37.110><c> features</c><00:15:37.290><c> by</c>

00:15:38.060 --> 00:15:38.070 align:start position:0%
affect different types of features by
 

00:15:38.070 --> 00:15:41.270 align:start position:0%
affect different types of features by
simply<00:15:38.730><c> using</c><00:15:38.760><c> different</c><00:15:39.510><c> filters</c><00:15:40.280><c> these</c>

00:15:41.270 --> 00:15:41.280 align:start position:0%
simply using different filters these
 

00:15:41.280 --> 00:15:44.030 align:start position:0%
simply using different filters these
concepts<00:15:41.910><c> of</c><00:15:42.120><c> spatial</c><00:15:43.050><c> structure</c><00:15:43.470><c> and</c><00:15:43.650><c> local</c>

00:15:44.030 --> 00:15:44.040 align:start position:0%
concepts of spatial structure and local
 

00:15:44.040 --> 00:15:46.460 align:start position:0%
concepts of spatial structure and local
feature<00:15:44.250><c> extraction</c><00:15:44.940><c> using</c><00:15:45.660><c> convolution</c><00:15:45.870><c> are</c>

00:15:46.460 --> 00:15:46.470 align:start position:0%
feature extraction using convolution are
 

00:15:46.470 --> 00:15:49.550 align:start position:0%
feature extraction using convolution are
at<00:15:47.310><c> the</c><00:15:47.550><c> core</c><00:15:47.850><c> of</c><00:15:47.880><c> the</c><00:15:48.180><c> neural</c><00:15:48.450><c> networks</c><00:15:48.840><c> used</c>

00:15:49.550 --> 00:15:49.560 align:start position:0%
at the core of the neural networks used
 

00:15:49.560 --> 00:15:52.880 align:start position:0%
at the core of the neural networks used
for<00:15:49.920><c> computer</c><00:15:50.340><c> vision</c><00:15:50.370><c> tasks</c><00:15:51.090><c> which</c><00:15:52.050><c> are</c><00:15:52.320><c> very</c>

00:15:52.880 --> 00:15:52.890 align:start position:0%
for computer vision tasks which are very
 

00:15:52.890 --> 00:15:55.040 align:start position:0%
for computer vision tasks which are very
appropriately<00:15:53.610><c> named</c><00:15:53.880><c> convolutional</c><00:15:54.780><c> neural</c>

00:15:55.040 --> 00:15:55.050 align:start position:0%
appropriately named convolutional neural
 

00:15:55.050 --> 00:15:59.030 align:start position:0%
appropriately named convolutional neural
networks<00:15:55.530><c> or</c><00:15:55.920><c> CN</c><00:15:56.490><c> NS</c><00:15:57.350><c> so</c><00:15:58.350><c> first</c><00:15:58.620><c> we'll</c><00:15:58.830><c> take</c><00:15:58.980><c> a</c>

00:15:59.030 --> 00:15:59.040 align:start position:0%
networks or CN NS so first we'll take a
 

00:15:59.040 --> 00:16:01.340 align:start position:0%
networks or CN NS so first we'll take a
look<00:15:59.160><c> at</c><00:15:59.490><c> a</c><00:15:59.550><c> CN</c><00:15:59.970><c> n</c><00:16:00.150><c> architecture</c><00:16:00.870><c> that's</c>

00:16:01.340 --> 00:16:01.350 align:start position:0%
look at a CN n architecture that's
 

00:16:01.350 --> 00:16:04.400 align:start position:0%
look at a CN n architecture that's
designed<00:16:01.800><c> for</c><00:16:01.950><c> image</c><00:16:02.400><c> classification</c><00:16:03.410><c> now</c>

00:16:04.400 --> 00:16:04.410 align:start position:0%
designed for image classification now
 

00:16:04.410 --> 00:16:07.040 align:start position:0%
designed for image classification now
there<00:16:04.650><c> are</c><00:16:04.740><c> three</c><00:16:05.010><c> main</c><00:16:05.340><c> operations</c><00:16:06.120><c> to</c><00:16:06.480><c> a</c><00:16:06.690><c> CNN</c>

00:16:07.040 --> 00:16:07.050 align:start position:0%
there are three main operations to a CNN
 

00:16:07.050 --> 00:16:10.880 align:start position:0%
there are three main operations to a CNN
first<00:16:08.040><c> convolutions</c><00:16:09.000><c> which</c><00:16:09.630><c> as</c><00:16:09.840><c> we</c><00:16:10.080><c> saw</c><00:16:10.530><c> can</c>

00:16:10.880 --> 00:16:10.890 align:start position:0%
first convolutions which as we saw can
 

00:16:10.890 --> 00:16:14.320 align:start position:0%
first convolutions which as we saw can
be<00:16:10.920><c> used</c><00:16:11.370><c> to</c><00:16:11.580><c> generate</c><00:16:11.730><c> feature</c><00:16:12.300><c> Maps</c><00:16:13.220><c> second</c>

00:16:14.320 --> 00:16:14.330 align:start position:0%
be used to generate feature Maps second
 

00:16:14.330 --> 00:16:16.880 align:start position:0%
be used to generate feature Maps second
non-linearity<00:16:15.330><c> which</c><00:16:15.930><c> we</c><00:16:16.140><c> learned</c><00:16:16.380><c> in</c><00:16:16.740><c> the</c>

00:16:16.880 --> 00:16:16.890 align:start position:0%
non-linearity which we learned in the
 

00:16:16.890 --> 00:16:20.090 align:start position:0%
non-linearity which we learned in the
first<00:16:16.920><c> lecture</c><00:16:17.600><c> yesterday</c><00:16:18.600><c> because</c><00:16:19.100><c> image</c>

00:16:20.090 --> 00:16:20.100 align:start position:0%
first lecture yesterday because image
 

00:16:20.100 --> 00:16:23.300 align:start position:0%
first lecture yesterday because image
data<00:16:20.340><c> is</c><00:16:20.550><c> highly</c><00:16:20.940><c> nonlinear</c><00:16:21.510><c> and</c><00:16:22.310><c> finally</c>

00:16:23.300 --> 00:16:23.310 align:start position:0%
data is highly nonlinear and finally
 

00:16:23.310 --> 00:16:25.280 align:start position:0%
data is highly nonlinear and finally
pooling<00:16:23.790><c> which</c><00:16:24.060><c> is</c><00:16:24.270><c> a</c><00:16:24.510><c> down</c><00:16:24.750><c> sampling</c>

00:16:25.280 --> 00:16:25.290 align:start position:0%
pooling which is a down sampling
 

00:16:25.290 --> 00:16:28.880 align:start position:0%
pooling which is a down sampling
operation<00:16:25.980><c> in</c><00:16:26.900><c> training</c><00:16:27.900><c> we</c><00:16:28.080><c> train</c><00:16:28.380><c> our</c><00:16:28.530><c> model</c>

00:16:28.880 --> 00:16:28.890 align:start position:0%
operation in training we train our model
 

00:16:28.890 --> 00:16:32.030 align:start position:0%
operation in training we train our model
our<00:16:29.160><c> CNN</c><00:16:29.670><c> model</c><00:16:30.030><c> on</c><00:16:30.180><c> a</c><00:16:30.210><c> set</c><00:16:30.480><c> of</c><00:16:30.600><c> images</c><00:16:30.780><c> and</c><00:16:31.290><c> in</c>

00:16:32.030 --> 00:16:32.040 align:start position:0%
our CNN model on a set of images and in
 

00:16:32.040 --> 00:16:33.610 align:start position:0%
our CNN model on a set of images and in
training<00:16:32.430><c> we</c><00:16:32.550><c> learn</c><00:16:32.820><c> the</c><00:16:33.030><c> weights</c><00:16:33.210><c> of</c><00:16:33.510><c> the</c>

00:16:33.610 --> 00:16:33.620 align:start position:0%
training we learn the weights of the
 

00:16:33.620 --> 00:16:37.460 align:start position:0%
training we learn the weights of the
convolutional<00:16:34.620><c> filters</c><00:16:35.420><c> that</c><00:16:36.420><c> correspond</c><00:16:37.110><c> to</c>

00:16:37.460 --> 00:16:37.470 align:start position:0%
convolutional filters that correspond to
 

00:16:37.470 --> 00:16:40.360 align:start position:0%
convolutional filters that correspond to
future<00:16:38.100><c> maps</c><00:16:38.370><c> in</c><00:16:38.670><c> convolutional</c><00:16:39.450><c> layers</c><00:16:39.720><c> and</c>

00:16:40.360 --> 00:16:40.370 align:start position:0%
future maps in convolutional layers and
 

00:16:40.370 --> 00:16:43.460 align:start position:0%
future maps in convolutional layers and
in<00:16:41.370><c> the</c><00:16:41.490><c> case</c><00:16:41.700><c> of</c><00:16:41.940><c> classification</c><00:16:42.720><c> we</c><00:16:43.260><c> can</c>

00:16:43.460 --> 00:16:43.470 align:start position:0%
in the case of classification we can
 

00:16:43.470 --> 00:16:45.770 align:start position:0%
in the case of classification we can
feed<00:16:43.740><c> the</c><00:16:44.250><c> output</c><00:16:44.280><c> of</c><00:16:44.820><c> these</c><00:16:45.090><c> convolutional</c>

00:16:45.770 --> 00:16:45.780 align:start position:0%
feed the output of these convolutional
 

00:16:45.780 --> 00:16:48.230 align:start position:0%
feed the output of these convolutional
layers<00:16:46.020><c> into</c><00:16:46.710><c> a</c><00:16:46.740><c> fully</c><00:16:47.010><c> connected</c><00:16:47.490><c> layer</c><00:16:47.730><c> to</c>

00:16:48.230 --> 00:16:48.240 align:start position:0%
layers into a fully connected layer to
 

00:16:48.240 --> 00:16:51.260 align:start position:0%
layers into a fully connected layer to
perform<00:16:48.630><c> classification</c><00:16:49.850><c> now</c><00:16:50.850><c> we'll</c><00:16:51.120><c> go</c>

00:16:51.260 --> 00:16:51.270 align:start position:0%
perform classification now we'll go
 

00:16:51.270 --> 00:16:52.790 align:start position:0%
perform classification now we'll go
through<00:16:51.510><c> each</c><00:16:51.690><c> of</c><00:16:51.870><c> these</c><00:16:51.960><c> operations</c><00:16:52.350><c> to</c>

00:16:52.790 --> 00:16:52.800 align:start position:0%
through each of these operations to
 

00:16:52.800 --> 00:16:55.610 align:start position:0%
through each of these operations to
break<00:16:53.010><c> down</c><00:16:53.450><c> the</c><00:16:54.450><c> basic</c><00:16:54.810><c> architecture</c><00:16:55.380><c> of</c><00:16:55.410><c> a</c>

00:16:55.610 --> 00:16:55.620 align:start position:0%
break down the basic architecture of a
 

00:16:55.620 --> 00:16:59.510 align:start position:0%
break down the basic architecture of a
CNN<00:16:56.570><c> first</c><00:16:57.570><c> let's</c><00:16:58.410><c> consider</c><00:16:58.650><c> the</c><00:16:58.950><c> convolution</c>

00:16:59.510 --> 00:16:59.520 align:start position:0%
CNN first let's consider the convolution
 

00:16:59.520 --> 00:17:04.550 align:start position:0%
CNN first let's consider the convolution
operation<00:17:00.770><c> as</c><00:17:01.770><c> we</c><00:17:01.980><c> saw</c><00:17:02.160><c> yesterday</c><00:17:03.560><c> each</c>

00:17:04.550 --> 00:17:04.560 align:start position:0%
operation as we saw yesterday each
 

00:17:04.560 --> 00:17:06.860 align:start position:0%
operation as we saw yesterday each
neuron<00:17:05.220><c> and</c><00:17:05.490><c> hidden</c><00:17:05.819><c> layer</c><00:17:06.030><c> will</c><00:17:06.329><c> compute</c><00:17:06.750><c> a</c>

00:17:06.860 --> 00:17:06.870 align:start position:0%
neuron and hidden layer will compute a
 

00:17:06.870 --> 00:17:09.380 align:start position:0%
neuron and hidden layer will compute a
weighted<00:17:06.900><c> sum</c><00:17:07.439><c> of</c><00:17:07.470><c> its</c><00:17:07.800><c> inputs</c><00:17:08.189><c> apply</c><00:17:09.030><c> bias</c>

00:17:09.380 --> 00:17:09.390 align:start position:0%
weighted sum of its inputs apply bias
 

00:17:09.390 --> 00:17:11.900 align:start position:0%
weighted sum of its inputs apply bias
and<00:17:09.810><c> eventually</c><00:17:10.530><c> activate</c><00:17:11.189><c> with</c><00:17:11.880><c> a</c>

00:17:11.900 --> 00:17:11.910 align:start position:0%
and eventually activate with a
 

00:17:11.910 --> 00:17:14.809 align:start position:0%
and eventually activate with a
non-linearity<00:17:12.680><c> what's</c><00:17:13.680><c> special</c><00:17:14.130><c> and</c><00:17:14.339><c> CNN's</c>

00:17:14.809 --> 00:17:14.819 align:start position:0%
non-linearity what's special and CNN's
 

00:17:14.819 --> 00:17:17.929 align:start position:0%
non-linearity what's special and CNN's
is<00:17:15.060><c> this</c><00:17:15.270><c> idea</c><00:17:15.689><c> of</c><00:17:15.959><c> local</c><00:17:16.140><c> connectivity</c><00:17:16.939><c> each</c>

00:17:17.929 --> 00:17:17.939 align:start position:0%
is this idea of local connectivity each
 

00:17:17.939 --> 00:17:20.150 align:start position:0%
is this idea of local connectivity each
neuron<00:17:18.329><c> and</c><00:17:18.600><c> hidden</c><00:17:19.199><c> layer</c><00:17:19.410><c> only</c><00:17:19.650><c> sees</c><00:17:20.130><c> a</c>

00:17:20.150 --> 00:17:20.160 align:start position:0%
neuron and hidden layer only sees a
 

00:17:20.160 --> 00:17:23.809 align:start position:0%
neuron and hidden layer only sees a
patch<00:17:20.550><c> of</c><00:17:20.579><c> its</c><00:17:20.970><c> inputs</c><00:17:22.310><c> we</c><00:17:23.310><c> can</c><00:17:23.459><c> now</c><00:17:23.579><c> define</c>

00:17:23.809 --> 00:17:23.819 align:start position:0%
patch of its inputs we can now define
 

00:17:23.819 --> 00:17:26.929 align:start position:0%
patch of its inputs we can now define
the<00:17:24.270><c> actual</c><00:17:24.750><c> computation</c><00:17:25.370><c> for</c><00:17:26.370><c> a</c><00:17:26.430><c> neuron</c><00:17:26.790><c> and</c>

00:17:26.929 --> 00:17:26.939 align:start position:0%
the actual computation for a neuron and
 

00:17:26.939 --> 00:17:29.150 align:start position:0%
the actual computation for a neuron and
a<00:17:27.030><c> hidden</c><00:17:27.300><c> layer</c><00:17:27.449><c> its</c><00:17:28.199><c> inputs</c><00:17:28.680><c> are</c><00:17:28.890><c> those</c>

00:17:29.150 --> 00:17:29.160 align:start position:0%
a hidden layer its inputs are those
 

00:17:29.160 --> 00:17:31.640 align:start position:0%
a hidden layer its inputs are those
neurons<00:17:29.940><c> in</c><00:17:30.180><c> the</c><00:17:30.390><c> patch</c><00:17:30.630><c> of</c><00:17:30.990><c> the</c><00:17:31.170><c> input</c><00:17:31.500><c> layer</c>

00:17:31.640 --> 00:17:31.650 align:start position:0%
neurons in the patch of the input layer
 

00:17:31.650 --> 00:17:34.610 align:start position:0%
neurons in the patch of the input layer
that<00:17:32.040><c> it's</c><00:17:32.220><c> connected</c><00:17:32.700><c> to</c><00:17:32.750><c> we</c><00:17:33.750><c> apply</c><00:17:34.080><c> a</c><00:17:34.110><c> matrix</c>

00:17:34.610 --> 00:17:34.620 align:start position:0%
that it's connected to we apply a matrix
 

00:17:34.620 --> 00:17:38.060 align:start position:0%
that it's connected to we apply a matrix
of<00:17:34.770><c> weights</c><00:17:34.980><c> our</c><00:17:35.450><c> convolutional</c><00:17:36.450><c> filter</c><00:17:37.070><c> 4x4</c>

00:17:38.060 --> 00:17:38.070 align:start position:0%
of weights our convolutional filter 4x4
 

00:17:38.070 --> 00:17:40.250 align:start position:0%
of weights our convolutional filter 4x4
in<00:17:38.280><c> this</c><00:17:38.430><c> example</c><00:17:38.790><c> do</c><00:17:39.660><c> an</c><00:17:39.780><c> element-wise</c>

00:17:40.250 --> 00:17:40.260 align:start position:0%
in this example do an element-wise
 

00:17:40.260 --> 00:17:43.070 align:start position:0%
in this example do an element-wise
multiplication<00:17:40.490><c> add</c><00:17:41.490><c> the</c><00:17:42.150><c> outputs</c><00:17:42.570><c> and</c><00:17:42.810><c> apply</c>

00:17:43.070 --> 00:17:43.080 align:start position:0%
multiplication add the outputs and apply
 

00:17:43.080 --> 00:17:48.440 align:start position:0%
multiplication add the outputs and apply
bias<00:17:45.170><c> so</c><00:17:46.170><c> this</c><00:17:46.650><c> defines</c><00:17:46.830><c> how</c><00:17:47.700><c> neurons</c><00:17:48.030><c> in</c>

00:17:48.440 --> 00:17:48.450 align:start position:0%
bias so this defines how neurons in
 

00:17:48.450 --> 00:17:49.320 align:start position:0%
bias so this defines how neurons in
convolutional

00:17:49.320 --> 00:17:49.330 align:start position:0%
convolutional
 

00:17:49.330 --> 00:17:52.590 align:start position:0%
convolutional
are<00:17:49.690><c> connected</c><00:17:50.700><c> but</c><00:17:51.700><c> within</c><00:17:52.149><c> a</c><00:17:52.390><c> single</c>

00:17:52.590 --> 00:17:52.600 align:start position:0%
are connected but within a single
 

00:17:52.600 --> 00:17:55.169 align:start position:0%
are connected but within a single
convolutional<00:17:53.409><c> layer</c><00:17:53.649><c> we</c><00:17:54.159><c> can</c><00:17:54.370><c> have</c><00:17:54.610><c> multiple</c>

00:17:55.169 --> 00:17:55.179 align:start position:0%
convolutional layer we can have multiple
 

00:17:55.179 --> 00:17:57.870 align:start position:0%
convolutional layer we can have multiple
different<00:17:55.750><c> filters</c><00:17:56.409><c> that</c><00:17:57.159><c> we</c><00:17:57.340><c> are</c><00:17:57.610><c> learning</c>

00:17:57.870 --> 00:17:57.880 align:start position:0%
different filters that we are learning
 

00:17:57.880 --> 00:17:59.880 align:start position:0%
different filters that we are learning
to<00:17:58.210><c> be</c><00:17:58.330><c> able</c><00:17:58.570><c> to</c><00:17:58.659><c> extract</c><00:17:59.080><c> different</c><00:17:59.559><c> features</c>

00:17:59.880 --> 00:17:59.890 align:start position:0%
to be able to extract different features
 

00:17:59.890 --> 00:18:03.060 align:start position:0%
to be able to extract different features
and<00:18:00.250><c> what</c><00:18:00.909><c> this</c><00:18:01.179><c> means</c><00:18:01.450><c> that</c><00:18:01.779><c> is</c><00:18:02.110><c> that</c><00:18:02.950><c> the</c>

00:18:03.060 --> 00:18:03.070 align:start position:0%
and what this means that is that the
 

00:18:03.070 --> 00:18:05.669 align:start position:0%
and what this means that is that the
output<00:18:03.250><c> of</c><00:18:03.669><c> a</c><00:18:04.120><c> convolutional</c><00:18:05.110><c> layer</c><00:18:05.320><c> has</c><00:18:05.649><c> a</c>

00:18:05.669 --> 00:18:05.679 align:start position:0%
output of a convolutional layer has a
 

00:18:05.679 --> 00:18:08.220 align:start position:0%
output of a convolutional layer has a
volume<00:18:06.399><c> where</c><00:18:07.299><c> the</c><00:18:07.450><c> height</c><00:18:07.690><c> and</c><00:18:07.960><c> the</c><00:18:08.049><c> width</c>

00:18:08.220 --> 00:18:08.230 align:start position:0%
volume where the height and the width
 

00:18:08.230 --> 00:18:10.740 align:start position:0%
volume where the height and the width
are<00:18:08.529><c> spatial</c><00:18:09.279><c> dimensions</c><00:18:09.789><c> and</c><00:18:10.090><c> these</c><00:18:10.480><c> spatial</c>

00:18:10.740 --> 00:18:10.750 align:start position:0%
are spatial dimensions and these spatial
 

00:18:10.750 --> 00:18:13.950 align:start position:0%
are spatial dimensions and these spatial
dimensions<00:18:11.409><c> are</c><00:18:11.649><c> dependent</c><00:18:12.580><c> on</c><00:18:12.960><c> the</c>

00:18:13.950 --> 00:18:13.960 align:start position:0%
dimensions are dependent on the
 

00:18:13.960 --> 00:18:16.529 align:start position:0%
dimensions are dependent on the
dimensions<00:18:14.500><c> of</c><00:18:14.710><c> the</c><00:18:14.890><c> input</c><00:18:15.250><c> layer</c><00:18:15.539><c> the</c>

00:18:16.529 --> 00:18:16.539 align:start position:0%
dimensions of the input layer the
 

00:18:16.539 --> 00:18:19.560 align:start position:0%
dimensions of the input layer the
dimensions<00:18:17.049><c> of</c><00:18:17.169><c> our</c><00:18:17.289><c> filter</c><00:18:17.710><c> and</c><00:18:18.269><c> how</c><00:18:19.269><c> we're</c>

00:18:19.560 --> 00:18:19.570 align:start position:0%
dimensions of our filter and how we're
 

00:18:19.570 --> 00:18:21.899 align:start position:0%
dimensions of our filter and how we're
sliding<00:18:19.929><c> our</c><00:18:20.230><c> filter</c><00:18:20.559><c> over</c><00:18:20.950><c> the</c><00:18:21.070><c> input</c><00:18:21.429><c> the</c>

00:18:21.899 --> 00:18:21.909 align:start position:0%
sliding our filter over the input the
 

00:18:21.909 --> 00:18:25.740 align:start position:0%
sliding our filter over the input the
stride<00:18:22.710><c> the</c><00:18:23.710><c> depth</c><00:18:23.950><c> of</c><00:18:24.220><c> a</c><00:18:24.610><c> this</c><00:18:25.149><c> output</c><00:18:25.600><c> volume</c>

00:18:25.740 --> 00:18:25.750 align:start position:0%
stride the depth of a this output volume
 

00:18:25.750 --> 00:18:28.019 align:start position:0%
stride the depth of a this output volume
is<00:18:26.080><c> then</c><00:18:26.350><c> given</c><00:18:26.559><c> by</c><00:18:26.830><c> the</c><00:18:26.889><c> number</c><00:18:27.429><c> of</c><00:18:27.460><c> different</c>

00:18:28.019 --> 00:18:28.029 align:start position:0%
is then given by the number of different
 

00:18:28.029 --> 00:18:31.799 align:start position:0%
is then given by the number of different
filters<00:18:28.210><c> we</c><00:18:28.840><c> apply</c><00:18:29.200><c> in</c><00:18:29.529><c> that</c><00:18:29.710><c> layer</c><00:18:30.690><c> we</c><00:18:31.690><c> can</c>

00:18:31.799 --> 00:18:31.809 align:start position:0%
filters we apply in that layer we can
 

00:18:31.809 --> 00:18:33.930 align:start position:0%
filters we apply in that layer we can
also<00:18:32.049><c> think</c><00:18:32.440><c> of</c><00:18:32.590><c> how</c><00:18:32.980><c> neurons</c><00:18:33.760><c> and</c>

00:18:33.930 --> 00:18:33.940 align:start position:0%
also think of how neurons and
 

00:18:33.940 --> 00:18:35.700 align:start position:0%
also think of how neurons and
convolutional<00:18:34.600><c> layers</c><00:18:34.809><c> are</c><00:18:35.049><c> connected</c><00:18:35.529><c> in</c>

00:18:35.700 --> 00:18:35.710 align:start position:0%
convolutional layers are connected in
 

00:18:35.710 --> 00:18:38.639 align:start position:0%
convolutional layers are connected in
terms<00:18:36.309><c> of</c><00:18:36.580><c> their</c><00:18:37.000><c> receptive</c><00:18:37.510><c> field</c><00:18:37.840><c> the</c>

00:18:38.639 --> 00:18:38.649 align:start position:0%
terms of their receptive field the
 

00:18:38.649 --> 00:18:41.190 align:start position:0%
terms of their receptive field the
locations<00:18:39.250><c> in</c><00:18:39.490><c> the</c><00:18:39.610><c> original</c><00:18:40.210><c> input</c><00:18:40.600><c> image</c>

00:18:41.190 --> 00:18:41.200 align:start position:0%
locations in the original input image
 

00:18:41.200 --> 00:18:44.310 align:start position:0%
locations in the original input image
that<00:18:41.710><c> a</c><00:18:41.740><c> node</c><00:18:42.039><c> is</c><00:18:42.220><c> path</c><00:18:42.490><c> connected</c><00:18:43.000><c> to</c><00:18:43.320><c> these</c>

00:18:44.310 --> 00:18:44.320 align:start position:0%
that a node is path connected to these
 

00:18:44.320 --> 00:18:46.049 align:start position:0%
that a node is path connected to these
parameters<00:18:44.919><c> define</c><00:18:45.429><c> the</c><00:18:45.669><c> spatial</c>

00:18:46.049 --> 00:18:46.059 align:start position:0%
parameters define the spatial
 

00:18:46.059 --> 00:18:47.759 align:start position:0%
parameters define the spatial
arrangement<00:18:46.570><c> of</c><00:18:46.720><c> the</c><00:18:47.139><c> output</c><00:18:47.529><c> of</c><00:18:47.559><c> a</c>

00:18:47.759 --> 00:18:47.769 align:start position:0%
arrangement of the output of a
 

00:18:47.769 --> 00:18:51.659 align:start position:0%
arrangement of the output of a
convolutional<00:18:48.429><c> layer</c><00:18:49.710><c> the</c><00:18:50.710><c> next</c><00:18:51.039><c> step</c><00:18:51.220><c> is</c><00:18:51.429><c> to</c>

00:18:51.659 --> 00:18:51.669 align:start position:0%
convolutional layer the next step is to
 

00:18:51.669 --> 00:18:53.610 align:start position:0%
convolutional layer the next step is to
apply<00:18:51.820><c> a</c><00:18:52.029><c> non-linearity</c><00:18:52.720><c> to</c><00:18:53.200><c> this</c><00:18:53.350><c> output</c>

00:18:53.610 --> 00:18:53.620 align:start position:0%
apply a non-linearity to this output
 

00:18:53.620 --> 00:18:57.060 align:start position:0%
apply a non-linearity to this output
volume<00:18:54.220><c> as</c><00:18:54.510><c> was</c><00:18:55.510><c> introduced</c><00:18:56.019><c> in</c><00:18:56.350><c> yesterday's</c>

00:18:57.060 --> 00:18:57.070 align:start position:0%
volume as was introduced in yesterday's
 

00:18:57.070 --> 00:18:59.460 align:start position:0%
volume as was introduced in yesterday's
lecture<00:18:57.309><c> we</c><00:18:58.029><c> do</c><00:18:58.210><c> this</c><00:18:58.360><c> because</c><00:18:58.570><c> data</c><00:18:59.230><c> is</c>

00:18:59.460 --> 00:18:59.470 align:start position:0%
lecture we do this because data is
 

00:18:59.470 --> 00:19:02.070 align:start position:0%
lecture we do this because data is
highly<00:18:59.860><c> nonlinear</c><00:19:00.399><c> and</c><00:19:00.700><c> in</c><00:19:01.240><c> cnn's</c><00:19:01.750><c> it's</c>

00:19:02.070 --> 00:19:02.080 align:start position:0%
highly nonlinear and in cnn's it's
 

00:19:02.080 --> 00:19:04.289 align:start position:0%
highly nonlinear and in cnn's it's
common<00:19:02.470><c> practice</c><00:19:02.919><c> to</c><00:19:02.950><c> apply</c><00:19:03.399><c> non-linearity</c>

00:19:04.289 --> 00:19:04.299 align:start position:0%
common practice to apply non-linearity
 

00:19:04.299 --> 00:19:07.590 align:start position:0%
common practice to apply non-linearity
after<00:19:04.929><c> every</c><00:19:05.320><c> convolution</c><00:19:05.919><c> operation</c><00:19:06.039><c> and</c><00:19:06.789><c> a</c>

00:19:07.590 --> 00:19:07.600 align:start position:0%
after every convolution operation and a
 

00:19:07.600 --> 00:19:09.750 align:start position:0%
after every convolution operation and a
common<00:19:07.929><c> activation</c><00:19:08.529><c> function</c><00:19:09.220><c> that's</c><00:19:09.399><c> used</c>

00:19:09.750 --> 00:19:09.760 align:start position:0%
common activation function that's used
 

00:19:09.760 --> 00:19:12.230 align:start position:0%
common activation function that's used
is<00:19:10.029><c> the</c><00:19:10.179><c> relu</c><00:19:10.570><c> which</c><00:19:11.380><c> is</c><00:19:11.529><c> a</c><00:19:11.559><c> pixel-by-pixel</c>

00:19:12.230 --> 00:19:12.240 align:start position:0%
is the relu which is a pixel-by-pixel
 

00:19:12.240 --> 00:19:15.240 align:start position:0%
is the relu which is a pixel-by-pixel
operation<00:19:13.240><c> that</c><00:19:13.929><c> will</c><00:19:14.080><c> replace</c><00:19:14.440><c> all</c><00:19:14.710><c> negative</c>

00:19:15.240 --> 00:19:15.250 align:start position:0%
operation that will replace all negative
 

00:19:15.250 --> 00:19:18.649 align:start position:0%
operation that will replace all negative
values<00:19:15.639><c> following</c><00:19:16.620><c> convolution</c><00:19:17.620><c> with</c><00:19:17.980><c> a</c><00:19:18.010><c> zero</c>

00:19:18.649 --> 00:19:18.659 align:start position:0%
values following convolution with a zero
 

00:19:18.659 --> 00:19:22.830 align:start position:0%
values following convolution with a zero
and<00:19:19.830><c> the</c><00:19:20.830><c> last</c><00:19:21.039><c> key</c><00:19:21.279><c> operation</c><00:19:21.880><c> to</c><00:19:22.090><c> a</c><00:19:22.120><c> CNN</c><00:19:22.450><c> is</c>

00:19:22.830 --> 00:19:22.840 align:start position:0%
and the last key operation to a CNN is
 

00:19:22.840 --> 00:19:25.799 align:start position:0%
and the last key operation to a CNN is
pooling<00:19:23.289><c> and</c><00:19:23.789><c> pooling</c><00:19:24.789><c> is</c><00:19:24.940><c> used</c><00:19:25.269><c> to</c><00:19:25.539><c> reduce</c>

00:19:25.799 --> 00:19:25.809 align:start position:0%
pooling and pooling is used to reduce
 

00:19:25.809 --> 00:19:28.490 align:start position:0%
pooling and pooling is used to reduce
dimension<00:19:26.620><c> dimension</c><00:19:27.039><c> yeah</c><00:19:27.490><c> excuse</c><00:19:28.269><c> me</c>

00:19:28.490 --> 00:19:28.500 align:start position:0%
dimension dimension yeah excuse me
 

00:19:28.500 --> 00:19:31.350 align:start position:0%
dimension dimension yeah excuse me
dimensionality<00:19:29.500><c> and</c><00:19:29.769><c> to</c><00:19:30.250><c> preserve</c><00:19:30.460><c> spatial</c>

00:19:31.350 --> 00:19:31.360 align:start position:0%
dimensionality and to preserve spatial
 

00:19:31.360 --> 00:19:34.289 align:start position:0%
dimensionality and to preserve spatial
invariants<00:19:31.990><c> a</c><00:19:32.200><c> common</c><00:19:33.159><c> text</c><00:19:33.370><c> technique</c><00:19:34.059><c> which</c>

00:19:34.289 --> 00:19:34.299 align:start position:0%
invariants a common text technique which
 

00:19:34.299 --> 00:19:37.200 align:start position:0%
invariants a common text technique which
you'll<00:19:34.539><c> very</c><00:19:35.139><c> often</c><00:19:35.500><c> see</c><00:19:35.710><c> is</c><00:19:35.950><c> max</c><00:19:36.580><c> pooling</c><00:19:36.880><c> as</c>

00:19:37.200 --> 00:19:37.210 align:start position:0%
you'll very often see is max pooling as
 

00:19:37.210 --> 00:19:39.779 align:start position:0%
you'll very often see is max pooling as
shown<00:19:37.779><c> in</c><00:19:37.809><c> this</c><00:19:38.049><c> example</c><00:19:38.110><c> and</c><00:19:38.830><c> it's</c><00:19:39.340><c> exactly</c>

00:19:39.779 --> 00:19:39.789 align:start position:0%
shown in this example and it's exactly
 

00:19:39.789 --> 00:19:42.180 align:start position:0%
shown in this example and it's exactly
what<00:19:40.149><c> it</c><00:19:40.240><c> sounds</c><00:19:40.419><c> like</c><00:19:40.630><c> we</c><00:19:41.380><c> simply</c><00:19:41.740><c> take</c><00:19:41.950><c> the</c>

00:19:42.180 --> 00:19:42.190 align:start position:0%
what it sounds like we simply take the
 

00:19:42.190 --> 00:19:44.730 align:start position:0%
what it sounds like we simply take the
maximum<00:19:42.549><c> value</c><00:19:42.700><c> in</c><00:19:43.179><c> a</c><00:19:43.269><c> patch</c><00:19:43.480><c> of</c><00:19:43.659><c> the</c><00:19:43.690><c> input</c><00:19:44.080><c> in</c>

00:19:44.730 --> 00:19:44.740 align:start position:0%
maximum value in a patch of the input in
 

00:19:44.740 --> 00:19:47.970 align:start position:0%
maximum value in a patch of the input in
this<00:19:45.700><c> case</c><00:19:45.760><c> a</c><00:19:45.970><c> two-by-two</c><00:19:46.510><c> patch</c><00:19:46.899><c> and</c><00:19:47.230><c> that</c>

00:19:47.970 --> 00:19:47.980 align:start position:0%
this case a two-by-two patch and that
 

00:19:47.980 --> 00:19:49.860 align:start position:0%
this case a two-by-two patch and that
determines<00:19:48.789><c> the</c><00:19:48.940><c> output</c><00:19:49.090><c> of</c><00:19:49.419><c> the</c><00:19:49.659><c> pooling</c>

00:19:49.860 --> 00:19:49.870 align:start position:0%
determines the output of the pooling
 

00:19:49.870 --> 00:19:53.100 align:start position:0%
determines the output of the pooling
operation<00:19:50.559><c> and</c><00:19:50.860><c> I</c><00:19:51.730><c> encourage</c><00:19:52.210><c> you</c><00:19:52.389><c> to</c><00:19:52.600><c> kind</c><00:19:52.990><c> of</c>

00:19:53.100 --> 00:19:53.110 align:start position:0%
operation and I encourage you to kind of
 

00:19:53.110 --> 00:19:56.310 align:start position:0%
operation and I encourage you to kind of
meditate<00:19:53.289><c> on</c><00:19:53.769><c> other</c><00:19:54.179><c> ways</c><00:19:55.179><c> in</c><00:19:55.510><c> which</c><00:19:55.539><c> we</c><00:19:55.750><c> could</c>

00:19:56.310 --> 00:19:56.320 align:start position:0%
meditate on other ways in which we could
 

00:19:56.320 --> 00:19:58.560 align:start position:0%
meditate on other ways in which we could
perform<00:19:56.950><c> this</c><00:19:57.250><c> sort</c><00:19:57.850><c> of</c><00:19:57.940><c> down</c><00:19:58.090><c> sampling</c>

00:19:58.560 --> 00:19:58.570 align:start position:0%
perform this sort of down sampling
 

00:19:58.570 --> 00:20:01.790 align:start position:0%
perform this sort of down sampling
operation

00:20:01.790 --> 00:20:01.800 align:start position:0%
 
 

00:20:01.800 --> 00:20:05.700 align:start position:0%
 
so<00:20:02.800><c> these</c><00:20:03.160><c> are</c><00:20:03.340><c> the</c><00:20:03.550><c> key</c><00:20:04.000><c> operations</c><00:20:04.660><c> of</c><00:20:04.900><c> a</c><00:20:05.230><c> CNN</c>

00:20:05.700 --> 00:20:05.710 align:start position:0%
so these are the key operations of a CNN
 

00:20:05.710 --> 00:20:07.710 align:start position:0%
so these are the key operations of a CNN
and<00:20:05.920><c> we're</c><00:20:06.400><c> now</c><00:20:06.520><c> ready</c><00:20:06.790><c> to</c><00:20:06.940><c> put</c><00:20:07.360><c> them</c><00:20:07.510><c> together</c>

00:20:07.710 --> 00:20:07.720 align:start position:0%
and we're now ready to put them together
 

00:20:07.720 --> 00:20:10.680 align:start position:0%
and we're now ready to put them together
to<00:20:08.440><c> actually</c><00:20:08.620><c> construct</c><00:20:09.070><c> our</c><00:20:09.370><c> network</c><00:20:09.730><c> we</c><00:20:10.660><c> can</c>

00:20:10.680 --> 00:20:10.690 align:start position:0%
to actually construct our network we can
 

00:20:10.690 --> 00:20:12.780 align:start position:0%
to actually construct our network we can
layer<00:20:11.050><c> these</c><00:20:11.320><c> operations</c><00:20:12.040><c> to</c><00:20:12.310><c> learn</c><00:20:12.550><c> a</c>

00:20:12.780 --> 00:20:12.790 align:start position:0%
layer these operations to learn a
 

00:20:12.790 --> 00:20:14.850 align:start position:0%
layer these operations to learn a
hierarchy<00:20:13.540><c> of</c><00:20:13.570><c> features</c><00:20:14.020><c> present</c><00:20:14.410><c> in</c><00:20:14.710><c> the</c>

00:20:14.850 --> 00:20:14.860 align:start position:0%
hierarchy of features present in the
 

00:20:14.860 --> 00:20:15.660 align:start position:0%
hierarchy of features present in the
image<00:20:15.130><c> data</c>

00:20:15.660 --> 00:20:15.670 align:start position:0%
image data
 

00:20:15.670 --> 00:20:20.430 align:start position:0%
image data
a<00:20:17.400><c> CNN</c><00:20:18.400><c> built</c><00:20:18.850><c> for</c><00:20:19.090><c> image</c><00:20:19.360><c> classification</c><00:20:20.110><c> can</c>

00:20:20.430 --> 00:20:20.440 align:start position:0%
a CNN built for image classification can
 

00:20:20.440 --> 00:20:22.560 align:start position:0%
a CNN built for image classification can
roughly<00:20:21.040><c> be</c><00:20:21.340><c> broken</c><00:20:21.730><c> down</c><00:20:21.910><c> into</c><00:20:22.240><c> two</c><00:20:22.540><c> parts</c>

00:20:22.560 --> 00:20:22.570 align:start position:0%
roughly be broken down into two parts
 

00:20:22.570 --> 00:20:24.810 align:start position:0%
roughly be broken down into two parts
the<00:20:23.140><c> first</c><00:20:23.590><c> is</c><00:20:24.130><c> the</c><00:20:24.280><c> feature</c><00:20:24.490><c> learning</c>

00:20:24.810 --> 00:20:24.820 align:start position:0%
the first is the feature learning
 

00:20:24.820 --> 00:20:27.210 align:start position:0%
the first is the feature learning
pipeline<00:20:25.420><c> where</c><00:20:26.140><c> we</c><00:20:26.320><c> learn</c><00:20:26.560><c> features</c><00:20:27.010><c> in</c>

00:20:27.210 --> 00:20:27.220 align:start position:0%
pipeline where we learn features in
 

00:20:27.220 --> 00:20:30.060 align:start position:0%
pipeline where we learn features in
input<00:20:27.520><c> images</c><00:20:27.970><c> through</c><00:20:28.540><c> convolution</c><00:20:29.070><c> through</c>

00:20:30.060 --> 00:20:30.070 align:start position:0%
input images through convolution through
 

00:20:30.070 --> 00:20:32.210 align:start position:0%
input images through convolution through
the<00:20:30.160><c> introduction</c><00:20:30.730><c> of</c><00:20:30.850><c> non-linearity</c><00:20:31.420><c> and</c>

00:20:32.210 --> 00:20:32.220 align:start position:0%
the introduction of non-linearity and
 

00:20:32.220 --> 00:20:36.090 align:start position:0%
the introduction of non-linearity and
the<00:20:33.220><c> pooling</c><00:20:33.670><c> operation</c><00:20:34.390><c> and</c><00:20:34.770><c> the</c><00:20:35.770><c> second</c>

00:20:36.090 --> 00:20:36.100 align:start position:0%
the pooling operation and the second
 

00:20:36.100 --> 00:20:39.510 align:start position:0%
the pooling operation and the second
part<00:20:36.310><c> is</c><00:20:37.350><c> how</c><00:20:38.350><c> we're</c><00:20:38.590><c> actually</c><00:20:38.980><c> performing</c>

00:20:39.510 --> 00:20:39.520 align:start position:0%
part is how we're actually performing
 

00:20:39.520 --> 00:20:42.660 align:start position:0%
part is how we're actually performing
the<00:20:39.580><c> classification</c><00:20:40.710><c> the</c><00:20:41.710><c> convolutional</c><00:20:42.430><c> and</c>

00:20:42.660 --> 00:20:42.670 align:start position:0%
the classification the convolutional and
 

00:20:42.670 --> 00:20:44.640 align:start position:0%
the classification the convolutional and
pooling<00:20:43.060><c> layers</c><00:20:43.270><c> output</c><00:20:43.810><c> high-level</c>

00:20:44.640 --> 00:20:44.650 align:start position:0%
pooling layers output high-level
 

00:20:44.650 --> 00:20:46.860 align:start position:0%
pooling layers output high-level
features<00:20:44.980><c> of</c><00:20:45.190><c> the</c><00:20:45.250><c> input</c><00:20:45.580><c> data</c><00:20:45.790><c> and</c><00:20:46.060><c> we</c><00:20:46.720><c> can</c>

00:20:46.860 --> 00:20:46.870 align:start position:0%
features of the input data and we can
 

00:20:46.870 --> 00:20:49.110 align:start position:0%
features of the input data and we can
feed<00:20:47.140><c> these</c><00:20:47.380><c> into</c><00:20:47.950><c> fully</c><00:20:48.370><c> connected</c><00:20:48.880><c> layers</c>

00:20:49.110 --> 00:20:49.120 align:start position:0%
feed these into fully connected layers
 

00:20:49.120 --> 00:20:52.460 align:start position:0%
feed these into fully connected layers
to<00:20:49.690><c> perform</c><00:20:50.140><c> the</c><00:20:50.320><c> actual</c><00:20:50.590><c> classification</c><00:20:51.430><c> and</c>

00:20:52.460 --> 00:20:52.470 align:start position:0%
to perform the actual classification and
 

00:20:52.470 --> 00:20:54.960 align:start position:0%
to perform the actual classification and
the<00:20:53.470><c> output</c><00:20:53.860><c> of</c><00:20:53.890><c> the</c><00:20:54.070><c> fully</c><00:20:54.280><c> connected</c><00:20:54.730><c> layers</c>

00:20:54.960 --> 00:20:54.970 align:start position:0%
the output of the fully connected layers
 

00:20:54.970 --> 00:20:56.970 align:start position:0%
the output of the fully connected layers
in<00:20:55.360><c> practice</c><00:20:56.080><c> is</c><00:20:56.320><c> a</c><00:20:56.350><c> probability</c>

00:20:56.970 --> 00:20:56.980 align:start position:0%
in practice is a probability
 

00:20:56.980 --> 00:21:00.090 align:start position:0%
in practice is a probability
distribution<00:20:57.330><c> for</c><00:20:58.330><c> an</c><00:20:58.600><c> input</c><00:20:59.590><c> images</c>

00:21:00.090 --> 00:21:00.100 align:start position:0%
distribution for an input images
 

00:21:00.100 --> 00:21:02.850 align:start position:0%
distribution for an input images
membership<00:21:01.090><c> over</c><00:21:01.720><c> set</c><00:21:01.960><c> of</c><00:21:02.170><c> possible</c><00:21:02.650><c> classes</c>

00:21:02.850 --> 00:21:02.860 align:start position:0%
membership over set of possible classes
 

00:21:02.860 --> 00:21:06.600 align:start position:0%
membership over set of possible classes
and<00:21:03.610><c> a</c><00:21:04.350><c> common</c><00:21:05.350><c> way</c><00:21:05.500><c> to</c><00:21:05.530><c> do</c><00:21:05.800><c> this</c><00:21:05.920><c> is</c><00:21:06.190><c> using</c><00:21:06.400><c> a</c>

00:21:06.600 --> 00:21:06.610 align:start position:0%
and a common way to do this is using a
 

00:21:06.610 --> 00:21:09.660 align:start position:0%
and a common way to do this is using a
function<00:21:07.000><c> called</c><00:21:07.030><c> softmax</c><00:21:07.890><c> where</c><00:21:08.890><c> the</c><00:21:09.250><c> output</c>

00:21:09.660 --> 00:21:09.670 align:start position:0%
function called softmax where the output
 

00:21:09.670 --> 00:21:11.940 align:start position:0%
function called softmax where the output
represents<00:21:10.270><c> this</c><00:21:10.450><c> categorical</c><00:21:11.020><c> probability</c>

00:21:11.940 --> 00:21:11.950 align:start position:0%
represents this categorical probability
 

00:21:11.950 --> 00:21:16.020 align:start position:0%
represents this categorical probability
distribution<00:21:14.430><c> now</c><00:21:15.430><c> that</c><00:21:15.640><c> we've</c><00:21:15.790><c> gone</c><00:21:15.970><c> through</c>

00:21:16.020 --> 00:21:16.030 align:start position:0%
distribution now that we've gone through
 

00:21:16.030 --> 00:21:18.690 align:start position:0%
distribution now that we've gone through
all<00:21:16.330><c> the</c><00:21:16.690><c> components</c><00:21:17.230><c> of</c><00:21:17.350><c> a</c><00:21:17.410><c> CN</c><00:21:17.680><c> n</c><00:21:17.800><c> all</c><00:21:18.040><c> that's</c>

00:21:18.690 --> 00:21:18.700 align:start position:0%
all the components of a CN n all that's
 

00:21:18.700 --> 00:21:21.690 align:start position:0%
all the components of a CN n all that's
left<00:21:18.970><c> is</c><00:21:19.180><c> to</c><00:21:19.210><c> discuss</c><00:21:19.750><c> how</c><00:21:19.780><c> to</c><00:21:20.050><c> train</c><00:21:20.440><c> it</c><00:21:20.700><c> again</c>

00:21:21.690 --> 00:21:21.700 align:start position:0%
left is to discuss how to train it again
 

00:21:21.700 --> 00:21:23.970 align:start position:0%
left is to discuss how to train it again
we'll<00:21:22.000><c> go</c><00:21:22.300><c> back</c><00:21:22.480><c> to</c><00:21:22.720><c> a</c><00:21:22.750><c> CNN</c><00:21:23.260><c> for</c><00:21:23.710><c> image</c>

00:21:23.970 --> 00:21:23.980 align:start position:0%
we'll go back to a CNN for image
 

00:21:23.980 --> 00:21:26.970 align:start position:0%
we'll go back to a CNN for image
classification<00:21:24.870><c> during</c><00:21:25.870><c> training</c><00:21:26.470><c> we</c><00:21:26.680><c> learn</c>

00:21:26.970 --> 00:21:26.980 align:start position:0%
classification during training we learn
 

00:21:26.980 --> 00:21:29.100 align:start position:0%
classification during training we learn
the<00:21:27.220><c> weights</c><00:21:27.370><c> of</c><00:21:27.670><c> the</c><00:21:27.910><c> convolutional</c><00:21:28.690><c> filters</c>

00:21:29.100 --> 00:21:29.110 align:start position:0%
the weights of the convolutional filters
 

00:21:29.110 --> 00:21:31.560 align:start position:0%
the weights of the convolutional filters
what<00:21:29.800><c> features</c><00:21:30.280><c> the</c><00:21:30.550><c> network</c><00:21:30.880><c> is</c><00:21:31.030><c> detecting</c>

00:21:31.560 --> 00:21:31.570 align:start position:0%
what features the network is detecting
 

00:21:31.570 --> 00:21:33.570 align:start position:0%
what features the network is detecting
and<00:21:31.750><c> in</c><00:21:32.440><c> this</c><00:21:32.590><c> case</c><00:21:32.830><c> we'll</c><00:21:33.040><c> also</c><00:21:33.220><c> learn</c><00:21:33.550><c> the</c>

00:21:33.570 --> 00:21:33.580 align:start position:0%
and in this case we'll also learn the
 

00:21:33.580 --> 00:21:35.570 align:start position:0%
and in this case we'll also learn the
weights<00:21:33.880><c> of</c><00:21:34.180><c> the</c><00:21:34.210><c> fully</c><00:21:34.570><c> connected</c><00:21:34.990><c> layers</c>

00:21:35.570 --> 00:21:35.580 align:start position:0%
weights of the fully connected layers
 

00:21:35.580 --> 00:21:38.610 align:start position:0%
weights of the fully connected layers
since<00:21:36.580><c> the</c><00:21:36.940><c> output</c><00:21:37.120><c> for</c><00:21:37.630><c> classification</c><00:21:38.410><c> is</c><00:21:38.590><c> a</c>

00:21:38.610 --> 00:21:38.620 align:start position:0%
since the output for classification is a
 

00:21:38.620 --> 00:21:41.370 align:start position:0%
since the output for classification is a
probability<00:21:39.430><c> distribution</c><00:21:39.990><c> we</c><00:21:40.990><c> can</c><00:21:41.020><c> use</c>

00:21:41.370 --> 00:21:41.380 align:start position:0%
probability distribution we can use
 

00:21:41.380 --> 00:21:44.160 align:start position:0%
probability distribution we can use
cross<00:21:41.680><c> entropy</c><00:21:41.950><c> loss</c><00:21:42.430><c> for</c><00:21:43.150><c> optimization</c><00:21:43.840><c> with</c>

00:21:44.160 --> 00:21:44.170 align:start position:0%
cross entropy loss for optimization with
 

00:21:44.170 --> 00:21:48.930 align:start position:0%
cross entropy loss for optimization with
back<00:21:44.500><c> prop</c><00:21:46.620><c> okay</c><00:21:47.620><c> so</c><00:21:47.680><c> I'd</c><00:21:48.370><c> like</c><00:21:48.550><c> to</c><00:21:48.610><c> take</c><00:21:48.910><c> a</c>

00:21:48.930 --> 00:21:48.940 align:start position:0%
back prop okay so I'd like to take a
 

00:21:48.940 --> 00:21:51.720 align:start position:0%
back prop okay so I'd like to take a
closer<00:21:49.870><c> look</c><00:21:50.200><c> at</c><00:21:50.440><c> CN</c><00:21:50.950><c> NS</c><00:21:51.100><c> for</c><00:21:51.310><c> classification</c>

00:21:51.720 --> 00:21:51.730 align:start position:0%
closer look at CN NS for classification
 

00:21:51.730 --> 00:21:55.020 align:start position:0%
closer look at CN NS for classification
and<00:21:52.560><c> discuss</c><00:21:53.560><c> what</c><00:21:53.800><c> is</c><00:21:53.920><c> arguably</c><00:21:54.610><c> the</c><00:21:54.700><c> most</c>

00:21:55.020 --> 00:21:55.030 align:start position:0%
and discuss what is arguably the most
 

00:21:55.030 --> 00:22:00.600 align:start position:0%
and discuss what is arguably the most
famous<00:21:55.480><c> example</c><00:21:56.200><c> of</c><00:21:56.410><c> a</c><00:21:57.070><c> CNN</c><00:21:57.540><c> the</c><00:21:59.370><c> the</c><00:22:00.370><c> ones</c>

00:22:00.600 --> 00:22:00.610 align:start position:0%
famous example of a CNN the the ones
 

00:22:00.610 --> 00:22:03.480 align:start position:0%
famous example of a CNN the the ones
trained<00:22:01.150><c> and</c><00:22:01.480><c> tested</c><00:22:02.080><c> on</c><00:22:02.500><c> the</c><00:22:02.830><c> image</c><00:22:03.250><c> net</c>

00:22:03.480 --> 00:22:03.490 align:start position:0%
trained and tested on the image net
 

00:22:03.490 --> 00:22:06.720 align:start position:0%
trained and tested on the image net
dataset<00:22:04.500><c> imagenet</c><00:22:05.500><c> is</c><00:22:05.710><c> a</c><00:22:05.950><c> massive</c><00:22:06.460><c> dataset</c>

00:22:06.720 --> 00:22:06.730 align:start position:0%
dataset imagenet is a massive dataset
 

00:22:06.730 --> 00:22:09.890 align:start position:0%
dataset imagenet is a massive dataset
with<00:22:07.150><c> over</c><00:22:07.510><c> 14</c><00:22:08.080><c> million</c><00:22:08.380><c> images</c><00:22:08.830><c> spanning</c>

00:22:09.890 --> 00:22:09.900 align:start position:0%
with over 14 million images spanning
 

00:22:09.900 --> 00:22:13.350 align:start position:0%
with over 14 million images spanning
20,000<00:22:10.900><c> different</c><00:22:11.290><c> categories</c><00:22:11.880><c> for</c><00:22:12.880><c> example</c>

00:22:13.350 --> 00:22:13.360 align:start position:0%
20,000 different categories for example
 

00:22:13.360 --> 00:22:15.240 align:start position:0%
20,000 different categories for example
there<00:22:13.720><c> are</c>

00:22:15.240 --> 00:22:15.250 align:start position:0%
there are
 

00:22:15.250 --> 00:22:17.490 align:start position:0%
there are
nine<00:22:15.580><c> different</c><00:22:16.240><c> pictures</c><00:22:16.600><c> of</c><00:22:16.840><c> bananas</c><00:22:17.320><c> in</c>

00:22:17.490 --> 00:22:17.500 align:start position:0%
nine different pictures of bananas in
 

00:22:17.500 --> 00:22:20.850 align:start position:0%
nine different pictures of bananas in
this<00:22:18.100><c> dataset</c><00:22:18.430><c> alone</c><00:22:19.170><c> even</c><00:22:20.170><c> better</c>

00:22:20.850 --> 00:22:20.860 align:start position:0%
this dataset alone even better
 

00:22:20.860 --> 00:22:23.540 align:start position:0%
this dataset alone even better
bananas<00:22:21.520><c> are</c><00:22:21.700><c> succinctly</c><00:22:22.300><c> described</c><00:22:22.900><c> as</c><00:22:23.140><c> an</c>

00:22:23.540 --> 00:22:23.550 align:start position:0%
bananas are succinctly described as an
 

00:22:23.550 --> 00:22:26.670 align:start position:0%
bananas are succinctly described as an
elongated<00:22:24.550><c> crescent-shaped</c><00:22:25.420><c> yellow</c><00:22:26.320><c> fruit</c>

00:22:26.670 --> 00:22:26.680 align:start position:0%
elongated crescent-shaped yellow fruit
 

00:22:26.680 --> 00:22:30.360 align:start position:0%
elongated crescent-shaped yellow fruit
with<00:22:27.520><c> soft</c><00:22:27.850><c> sweet</c><00:22:28.390><c> flesh</c><00:22:28.750><c> which</c><00:22:29.500><c> both</c><00:22:29.800><c> gives</c><00:22:30.220><c> a</c>

00:22:30.360 --> 00:22:30.370 align:start position:0%
with soft sweet flesh which both gives a
 

00:22:30.370 --> 00:22:32.790 align:start position:0%
with soft sweet flesh which both gives a
pretty<00:22:30.580><c> good</c><00:22:30.700><c> descriptive</c><00:22:31.540><c> value</c><00:22:31.960><c> of</c><00:22:32.200><c> what</c>

00:22:32.790 --> 00:22:32.800 align:start position:0%
pretty good descriptive value of what
 

00:22:32.800 --> 00:22:35.160 align:start position:0%
pretty good descriptive value of what
about<00:22:33.040><c> banana</c><00:22:33.850><c> is</c><00:22:34.090><c> and</c><00:22:34.420><c> speaks</c><00:22:34.750><c> to</c><00:22:35.020><c> its</c>

00:22:35.160 --> 00:22:35.170 align:start position:0%
about banana is and speaks to its
 

00:22:35.170 --> 00:22:37.250 align:start position:0%
about banana is and speaks to its
deliciousness

00:22:37.250 --> 00:22:37.260 align:start position:0%
deliciousness
 

00:22:37.260 --> 00:22:40.470 align:start position:0%
deliciousness
so<00:22:38.260><c> the</c><00:22:38.890><c> creators</c><00:22:39.280><c> of</c><00:22:39.310><c> the</c><00:22:39.550><c> image</c><00:22:39.790><c> net</c><00:22:40.000><c> dataset</c>

00:22:40.470 --> 00:22:40.480 align:start position:0%
so the creators of the image net dataset
 

00:22:40.480 --> 00:22:42.720 align:start position:0%
so the creators of the image net dataset
also<00:22:40.960><c> have</c><00:22:41.230><c> created</c><00:22:41.740><c> a</c><00:22:41.890><c> set</c><00:22:42.160><c> of</c><00:22:42.340><c> visual</c>

00:22:42.720 --> 00:22:42.730 align:start position:0%
also have created a set of visual
 

00:22:42.730 --> 00:22:45.270 align:start position:0%
also have created a set of visual
recognition<00:22:43.170><c> challenges</c><00:22:44.170><c> beyond</c><00:22:44.740><c> this</c><00:22:45.040><c> data</c>

00:22:45.270 --> 00:22:45.280 align:start position:0%
recognition challenges beyond this data
 

00:22:45.280 --> 00:22:48.390 align:start position:0%
recognition challenges beyond this data
set<00:22:45.550><c> and</c><00:22:45.900><c> what's</c><00:22:46.900><c> most</c><00:22:47.320><c> famous</c><00:22:47.770><c> is</c><00:22:47.950><c> the</c><00:22:47.980><c> image</c>

00:22:48.390 --> 00:22:48.400 align:start position:0%
set and what's most famous is the image
 

00:22:48.400 --> 00:22:52.220 align:start position:0%
set and what's most famous is the image
net<00:22:48.610><c> classification</c><00:22:49.390><c> task</c><00:22:49.950><c> where</c><00:22:50.950><c> users</c><00:22:51.640><c> are</c>

00:22:52.220 --> 00:22:52.230 align:start position:0%
net classification task where users are
 

00:22:52.230 --> 00:22:54.390 align:start position:0%
net classification task where users are
challengers<00:22:53.230><c> are</c><00:22:53.440><c> simply</c><00:22:53.800><c> tasked</c><00:22:54.370><c> with</c>

00:22:54.390 --> 00:22:54.400 align:start position:0%
challengers are simply tasked with
 

00:22:54.400 --> 00:22:57.150 align:start position:0%
challengers are simply tasked with
producing<00:22:55.150><c> a</c><00:22:55.810><c> list</c><00:22:56.380><c> of</c><00:22:56.560><c> the</c><00:22:56.740><c> object</c>

00:22:57.150 --> 00:22:57.160 align:start position:0%
producing a list of the object
 

00:22:57.160 --> 00:22:59.430 align:start position:0%
producing a list of the object
categories<00:22:57.760><c> present</c><00:22:58.270><c> in</c><00:22:58.450><c> a</c><00:22:58.900><c> given</c><00:22:58.930><c> image</c>

00:22:59.430 --> 00:22:59.440 align:start position:0%
categories present in a given image
 

00:22:59.440 --> 00:23:04.020 align:start position:0%
categories present in a given image
across<00:23:00.250><c> 1000</c><00:23:01.240><c> different</c><00:23:01.360><c> categories</c><00:23:02.020><c> and</c><00:23:03.030><c> the</c>

00:23:04.020 --> 00:23:04.030 align:start position:0%
across 1000 different categories and the
 

00:23:04.030 --> 00:23:06.570 align:start position:0%
across 1000 different categories and the
results<00:23:04.450><c> of</c><00:23:04.600><c> the</c><00:23:05.470><c> neural</c><00:23:05.800><c> networks</c><00:23:06.130><c> have</c><00:23:06.370><c> had</c>

00:23:06.570 --> 00:23:06.580 align:start position:0%
results of the neural networks have had
 

00:23:06.580 --> 00:23:08.760 align:start position:0%
results of the neural networks have had
on<00:23:06.880><c> this</c><00:23:07.120><c> classification</c><00:23:07.510><c> tasks</c><00:23:08.170><c> are</c><00:23:08.470><c> pretty</c>

00:23:08.760 --> 00:23:08.770 align:start position:0%
on this classification tasks are pretty
 

00:23:08.770 --> 00:23:12.630 align:start position:0%
on this classification tasks are pretty
remarkable<00:23:09.960><c> 2012</c><00:23:10.960><c> was</c><00:23:11.230><c> the</c><00:23:11.260><c> first</c><00:23:11.680><c> time</c><00:23:11.980><c> a</c><00:23:12.160><c> CNN</c>

00:23:12.630 --> 00:23:12.640 align:start position:0%
remarkable 2012 was the first time a CNN
 

00:23:12.640 --> 00:23:16.350 align:start position:0%
remarkable 2012 was the first time a CNN
won<00:23:13.270><c> this</c><00:23:13.450><c> challenge</c><00:23:13.750><c> with</c><00:23:14.350><c> the</c><00:23:14.650><c> famous</c><00:23:15.360><c> Alex</c>

00:23:16.350 --> 00:23:16.360 align:start position:0%
won this challenge with the famous Alex
 

00:23:16.360 --> 00:23:19.140 align:start position:0%
won this challenge with the famous Alex
net<00:23:16.570><c> CNN</c><00:23:17.110><c> and</c><00:23:17.350><c> since</c><00:23:18.070><c> then</c><00:23:18.280><c> neural</c><00:23:18.790><c> networks</c>

00:23:19.140 --> 00:23:19.150 align:start position:0%
net CNN and since then neural networks
 

00:23:19.150 --> 00:23:21.780 align:start position:0%
net CNN and since then neural networks
have<00:23:19.420><c> dominated</c><00:23:20.290><c> the</c><00:23:20.440><c> competition</c><00:23:20.590><c> and</c><00:23:21.220><c> the</c>

00:23:21.780 --> 00:23:21.790 align:start position:0%
have dominated the competition and the
 

00:23:21.790 --> 00:23:24.840 align:start position:0%
have dominated the competition and the
error<00:23:21.970><c> has</c><00:23:22.300><c> kept</c><00:23:22.660><c> decreasing</c><00:23:23.850><c> surpassing</c>

00:23:24.840 --> 00:23:24.850 align:start position:0%
error has kept decreasing surpassing
 

00:23:24.850 --> 00:23:28.530 align:start position:0%
error has kept decreasing surpassing
human<00:23:25.510><c> error</c><00:23:25.690><c> in</c><00:23:25.990><c> 2015</c><00:23:26.980><c> with</c><00:23:27.640><c> the</c><00:23:27.670><c> resonant</c>

00:23:28.530 --> 00:23:28.540 align:start position:0%
human error in 2015 with the resonant
 

00:23:28.540 --> 00:23:32.580 align:start position:0%
human error in 2015 with the resonant
architecture<00:23:29.820><c> but</c><00:23:30.820><c> with</c><00:23:31.540><c> improved</c><00:23:32.050><c> accuracy</c>

00:23:32.580 --> 00:23:32.590 align:start position:0%
architecture but with improved accuracy
 

00:23:32.590 --> 00:23:35.070 align:start position:0%
architecture but with improved accuracy
the<00:23:33.460><c> number</c><00:23:33.820><c> of</c><00:23:34.030><c> layers</c><00:23:34.300><c> in</c><00:23:34.660><c> these</c><00:23:34.840><c> networks</c>

00:23:35.070 --> 00:23:35.080 align:start position:0%
the number of layers in these networks
 

00:23:35.080 --> 00:23:39.240 align:start position:0%
the number of layers in these networks
has<00:23:35.710><c> been</c><00:23:37.290><c> increasing</c><00:23:38.290><c> quite</c><00:23:38.560><c> dramatically</c>

00:23:39.240 --> 00:23:39.250 align:start position:0%
has been increasing quite dramatically
 

00:23:39.250 --> 00:23:41.610 align:start position:0%
has been increasing quite dramatically
so<00:23:39.730><c> there's</c><00:23:40.180><c> a</c><00:23:40.270><c> trade-off</c><00:23:40.510><c> here</c><00:23:40.660><c> right</c><00:23:41.080><c> build</c>

00:23:41.610 --> 00:23:41.620 align:start position:0%
so there's a trade-off here right build
 

00:23:41.620 --> 00:23:46.580 align:start position:0%
so there's a trade-off here right build
your<00:23:41.800><c> network</c><00:23:42.130><c> deeper</c><00:23:43.050><c> how</c><00:23:44.050><c> deep</c><00:23:44.380><c> can</c><00:23:44.590><c> you</c><00:23:44.710><c> go</c>

00:23:46.580 --> 00:23:46.590 align:start position:0%
your network deeper how deep can you go
 

00:23:46.590 --> 00:23:49.710 align:start position:0%
your network deeper how deep can you go
so<00:23:47.590><c> so</c><00:23:47.860><c> far</c><00:23:48.070><c> we've</c><00:23:48.280><c> talked</c><00:23:48.640><c> only</c><00:23:48.910><c> about</c><00:23:49.060><c> using</c>

00:23:49.710 --> 00:23:49.720 align:start position:0%
so so far we've talked only about using
 

00:23:49.720 --> 00:23:52.860 align:start position:0%
so so far we've talked only about using
CNN's<00:23:50.170><c> for</c><00:23:50.500><c> image</c><00:23:50.830><c> classification</c><00:23:51.660><c> but</c><00:23:52.660><c> in</c>

00:23:52.860 --> 00:23:52.870 align:start position:0%
CNN's for image classification but in
 

00:23:52.870 --> 00:23:55.200 align:start position:0%
CNN's for image classification but in
reality<00:23:53.350><c> this</c><00:23:53.560><c> idea</c><00:23:53.950><c> of</c><00:23:54.160><c> using</c><00:23:54.400><c> convolutional</c>

00:23:55.200 --> 00:23:55.210 align:start position:0%
reality this idea of using convolutional
 

00:23:55.210 --> 00:23:58.650 align:start position:0%
reality this idea of using convolutional
layers<00:23:55.750><c> to</c><00:23:56.320><c> extract</c><00:23:57.250><c> features</c><00:23:57.460><c> can</c><00:23:57.910><c> extend</c><00:23:58.330><c> to</c>

00:23:58.650 --> 00:23:58.660 align:start position:0%
layers to extract features can extend to
 

00:23:58.660 --> 00:24:02.460 align:start position:0%
layers to extract features can extend to
a<00:23:58.690><c> number</c><00:23:59.200><c> of</c><00:23:59.380><c> different</c><00:23:59.770><c> applications</c><00:24:01.470><c> when</c>

00:24:02.460 --> 00:24:02.470 align:start position:0%
a number of different applications when
 

00:24:02.470 --> 00:24:04.650 align:start position:0%
a number of different applications when
we<00:24:02.590><c> consider</c><00:24:02.950><c> to</c><00:24:03.130><c> CNN</c><00:24:03.550><c> for</c><00:24:03.790><c> classification</c><00:24:04.210><c> we</c>

00:24:04.650 --> 00:24:04.660 align:start position:0%
we consider to CNN for classification we
 

00:24:04.660 --> 00:24:06.270 align:start position:0%
we consider to CNN for classification we
saw<00:24:04.900><c> that</c><00:24:05.170><c> we</c><00:24:05.290><c> could</c><00:24:05.470><c> think</c><00:24:05.620><c> of</c><00:24:05.740><c> it</c><00:24:05.860><c> in</c><00:24:06.010><c> two</c>

00:24:06.270 --> 00:24:06.280 align:start position:0%
saw that we could think of it in two
 

00:24:06.280 --> 00:24:08.310 align:start position:0%
saw that we could think of it in two
parts<00:24:06.490><c> feature</c><00:24:07.300><c> learning</c><00:24:07.630><c> and</c><00:24:08.020><c> the</c>

00:24:08.310 --> 00:24:08.320 align:start position:0%
parts feature learning and the
 

00:24:08.320 --> 00:24:13.200 align:start position:0%
parts feature learning and the
classification<00:24:11.220><c> what</c><00:24:12.220><c> is</c><00:24:12.610><c> at</c><00:24:12.820><c> the</c><00:24:12.850><c> core</c><00:24:13.180><c> of</c>

00:24:13.200 --> 00:24:13.210 align:start position:0%
classification what is at the core of
 

00:24:13.210 --> 00:24:16.380 align:start position:0%
classification what is at the core of
the<00:24:14.070><c> convolutional</c><00:24:15.070><c> neural</c><00:24:15.280><c> networks</c><00:24:15.790><c> is</c><00:24:16.060><c> is</c>

00:24:16.380 --> 00:24:16.390 align:start position:0%
the convolutional neural networks is is
 

00:24:16.390 --> 00:24:19.170 align:start position:0%
the convolutional neural networks is is
the<00:24:16.870><c> feature</c><00:24:17.080><c> learning</c><00:24:17.380><c> pipeline</c><00:24:18.010><c> after</c><00:24:18.970><c> that</c>

00:24:19.170 --> 00:24:19.180 align:start position:0%
the feature learning pipeline after that
 

00:24:19.180 --> 00:24:21.560 align:start position:0%
the feature learning pipeline after that
we<00:24:19.510><c> can</c><00:24:19.720><c> really</c><00:24:20.080><c> change</c><00:24:20.170><c> what</c><00:24:20.620><c> follows</c><00:24:21.280><c> to</c>

00:24:21.560 --> 00:24:21.570 align:start position:0%
we can really change what follows to
 

00:24:21.570 --> 00:24:25.740 align:start position:0%
we can really change what follows to
suit<00:24:22.570><c> the</c><00:24:23.530><c> application</c><00:24:24.190><c> that</c><00:24:24.370><c> we</c><00:24:24.490><c> desire</c><00:24:24.850><c> for</c>

00:24:25.740 --> 00:24:25.750 align:start position:0%
suit the application that we desire for
 

00:24:25.750 --> 00:24:28.509 align:start position:0%
suit the application that we desire for
example<00:24:26.200><c> the</c><00:24:26.679><c> portion</c><00:24:27.040><c> following</c><00:24:27.820><c> the</c><00:24:28.000><c> Khan</c>

00:24:28.509 --> 00:24:28.519 align:start position:0%
example the portion following the Khan
 

00:24:28.519 --> 00:24:30.399 align:start position:0%
example the portion following the Khan
lucien<00:24:28.789><c> layers</c><00:24:29.119><c> may</c><00:24:29.599><c> look</c><00:24:29.809><c> different</c><00:24:30.049><c> for</c>

00:24:30.399 --> 00:24:30.409 align:start position:0%
lucien layers may look different for
 

00:24:30.409 --> 00:24:32.199 align:start position:0%
lucien layers may look different for
different<00:24:30.440><c> image</c><00:24:31.009><c> classification</c><00:24:31.279><c> domains</c>

00:24:32.199 --> 00:24:32.209 align:start position:0%
different image classification domains
 

00:24:32.209 --> 00:24:35.529 align:start position:0%
different image classification domains
we<00:24:33.109><c> can</c><00:24:33.289><c> also</c><00:24:33.499><c> introduce</c><00:24:34.190><c> new</c><00:24:34.820><c> architectures</c>

00:24:35.529 --> 00:24:35.539 align:start position:0%
we can also introduce new architectures
 

00:24:35.539 --> 00:24:38.079 align:start position:0%
we can also introduce new architectures
beyond<00:24:35.989><c> fully</c><00:24:36.349><c> connected</c><00:24:36.830><c> layers</c><00:24:37.070><c> for</c><00:24:37.729><c> tasks</c>

00:24:38.079 --> 00:24:38.089 align:start position:0%
beyond fully connected layers for tasks
 

00:24:38.089 --> 00:24:40.180 align:start position:0%
beyond fully connected layers for tasks
such<00:24:38.119><c> as</c><00:24:38.320><c> segmentation</c><00:24:39.320><c> and</c><00:24:39.529><c> image</c>

00:24:40.180 --> 00:24:40.190 align:start position:0%
such as segmentation and image
 

00:24:40.190 --> 00:24:43.899 align:start position:0%
such as segmentation and image
captioning<00:24:41.919><c> so</c><00:24:42.919><c> today</c><00:24:43.219><c> I'd</c><00:24:43.429><c> like</c><00:24:43.609><c> to</c><00:24:43.759><c> go</c>

00:24:43.899 --> 00:24:43.909 align:start position:0%
captioning so today I'd like to go
 

00:24:43.909 --> 00:24:46.149 align:start position:0%
captioning so today I'd like to go
through<00:24:44.149><c> three</c><00:24:44.719><c> different</c><00:24:44.989><c> applications</c><00:24:45.440><c> of</c>

00:24:46.149 --> 00:24:46.159 align:start position:0%
through three different applications of
 

00:24:46.159 --> 00:24:48.810 align:start position:0%
through three different applications of
CNN's<00:24:46.549><c> beyond</c><00:24:47.059><c> image</c><00:24:47.479><c> classification</c>

00:24:48.810 --> 00:24:48.820 align:start position:0%
CNN's beyond image classification
 

00:24:48.820 --> 00:24:51.549 align:start position:0%
CNN's beyond image classification
semantic<00:24:49.820><c> segmentation</c><00:24:50.539><c> where</c><00:24:50.869><c> the</c><00:24:51.019><c> task</c><00:24:51.259><c> is</c>

00:24:51.549 --> 00:24:51.559 align:start position:0%
semantic segmentation where the task is
 

00:24:51.559 --> 00:24:54.549 align:start position:0%
semantic segmentation where the task is
to<00:24:51.589><c> assign</c><00:24:52.129><c> each</c><00:24:52.729><c> pixel</c><00:24:53.359><c> in</c><00:24:53.570><c> an</c><00:24:53.959><c> image</c><00:24:54.109><c> an</c>

00:24:54.549 --> 00:24:54.559 align:start position:0%
to assign each pixel in an image an
 

00:24:54.559 --> 00:24:57.820 align:start position:0%
to assign each pixel in an image an
object<00:24:55.489><c> class</c><00:24:55.700><c> to</c><00:24:56.629><c> produce</c><00:24:56.929><c> a</c><00:24:57.080><c> segmentation</c>

00:24:57.820 --> 00:24:57.830 align:start position:0%
object class to produce a segmentation
 

00:24:57.830 --> 00:25:01.029 align:start position:0%
object class to produce a segmentation
of<00:24:57.919><c> that</c><00:24:58.099><c> image</c><00:24:58.690><c> object</c><00:24:59.690><c> detection</c><00:25:00.019><c> where</c><00:25:00.769><c> we</c>

00:25:01.029 --> 00:25:01.039 align:start position:0%
of that image object detection where we
 

00:25:01.039 --> 00:25:03.669 align:start position:0%
of that image object detection where we
are<00:25:01.399><c> tasked</c><00:25:01.849><c> with</c><00:25:01.999><c> detecting</c><00:25:02.659><c> instances</c><00:25:03.529><c> of</c>

00:25:03.669 --> 00:25:03.679 align:start position:0%
are tasked with detecting instances of
 

00:25:03.679 --> 00:25:06.699 align:start position:0%
are tasked with detecting instances of
semantic<00:25:04.129><c> objects</c><00:25:04.700><c> in</c><00:25:04.940><c> an</c><00:25:05.479><c> image</c><00:25:05.779><c> and</c><00:25:05.959><c> image</c>

00:25:06.699 --> 00:25:06.709 align:start position:0%
semantic objects in an image and image
 

00:25:06.709 --> 00:25:09.339 align:start position:0%
semantic objects in an image and image
captioning<00:25:07.459><c> where</c><00:25:07.999><c> the</c><00:25:08.119><c> task</c><00:25:08.329><c> is</c><00:25:08.629><c> to</c><00:25:08.899><c> generate</c>

00:25:09.339 --> 00:25:09.349 align:start position:0%
captioning where the task is to generate
 

00:25:09.349 --> 00:25:11.589 align:start position:0%
captioning where the task is to generate
a<00:25:09.589><c> short</c><00:25:10.099><c> description</c><00:25:10.549><c> of</c><00:25:10.879><c> the</c><00:25:10.940><c> image</c><00:25:11.149><c> that</c>

00:25:11.589 --> 00:25:11.599 align:start position:0%
a short description of the image that
 

00:25:11.599 --> 00:25:15.549 align:start position:0%
a short description of the image that
captures<00:25:12.200><c> its</c><00:25:12.649><c> semantic</c><00:25:13.129><c> content</c><00:25:14.289><c> so</c><00:25:15.289><c> first</c>

00:25:15.549 --> 00:25:15.559 align:start position:0%
captures its semantic content so first
 

00:25:15.559 --> 00:25:17.589 align:start position:0%
captures its semantic content so first
let's<00:25:15.829><c> talk</c><00:25:16.039><c> about</c><00:25:16.099><c> semantic</c><00:25:16.820><c> segmentation</c>

00:25:17.589 --> 00:25:17.599 align:start position:0%
let's talk about semantic segmentation
 

00:25:17.599 --> 00:25:20.139 align:start position:0%
let's talk about semantic segmentation
with<00:25:18.169><c> fully</c><00:25:18.709><c> convolutional</c><00:25:19.459><c> net</c><00:25:19.669><c> works</c><00:25:19.849><c> or</c>

00:25:20.139 --> 00:25:20.149 align:start position:0%
with fully convolutional net works or
 

00:25:20.149 --> 00:25:23.829 align:start position:0%
with fully convolutional net works or
fcns<00:25:21.219><c> here</c><00:25:22.219><c> the</c><00:25:22.459><c> network</c><00:25:22.820><c> again</c><00:25:23.329><c> takes</c><00:25:23.629><c> an</c>

00:25:23.829 --> 00:25:23.839 align:start position:0%
fcns here the network again takes an
 

00:25:23.839 --> 00:25:26.769 align:start position:0%
fcns here the network again takes an
image<00:25:24.229><c> input</c><00:25:24.589><c> of</c><00:25:24.919><c> arbitrary</c><00:25:25.489><c> size</c><00:25:25.779><c> but</c>

00:25:26.769 --> 00:25:26.779 align:start position:0%
image input of arbitrary size but
 

00:25:26.779 --> 00:25:28.389 align:start position:0%
image input of arbitrary size but
instead<00:25:27.259><c> it</c><00:25:27.469><c> has</c><00:25:27.649><c> to</c><00:25:27.829><c> produce</c><00:25:28.039><c> a</c>

00:25:28.389 --> 00:25:28.399 align:start position:0%
instead it has to produce a
 

00:25:28.399 --> 00:25:31.389 align:start position:0%
instead it has to produce a
correspondingly<00:25:29.359><c> sized</c><00:25:29.749><c> output</c><00:25:30.469><c> where</c><00:25:31.129><c> each</c>

00:25:31.389 --> 00:25:31.399 align:start position:0%
correspondingly sized output where each
 

00:25:31.399 --> 00:25:34.060 align:start position:0%
correspondingly sized output where each
pixel<00:25:31.639><c> has</c><00:25:32.329><c> been</c><00:25:32.570><c> assigned</c><00:25:32.869><c> a</c><00:25:33.259><c> class</c><00:25:33.769><c> label</c>

00:25:34.060 --> 00:25:34.070 align:start position:0%
pixel has been assigned a class label
 

00:25:34.070 --> 00:25:36.339 align:start position:0%
pixel has been assigned a class label
which<00:25:34.999><c> we</c><00:25:35.179><c> can</c><00:25:35.329><c> then</c><00:25:35.359><c> visualize</c><00:25:35.779><c> as</c><00:25:36.320><c> a</c>

00:25:36.339 --> 00:25:36.349 align:start position:0%
which we can then visualize as a
 

00:25:36.349 --> 00:25:40.299 align:start position:0%
which we can then visualize as a
segmentation<00:25:36.979><c> as</c><00:25:37.719><c> we</c><00:25:38.719><c> saw</c><00:25:38.899><c> before</c><00:25:39.109><c> with</c><00:25:39.679><c> CNN</c><00:25:40.219><c> s</c>

00:25:40.299 --> 00:25:40.309 align:start position:0%
segmentation as we saw before with CNN s
 

00:25:40.309 --> 00:25:43.149 align:start position:0%
segmentation as we saw before with CNN s
for<00:25:40.519><c> image</c><00:25:40.909><c> classification</c><00:25:41.779><c> we</c><00:25:42.649><c> first</c><00:25:42.919><c> have</c><00:25:43.129><c> a</c>

00:25:43.149 --> 00:25:43.159 align:start position:0%
for image classification we first have a
 

00:25:43.159 --> 00:25:45.190 align:start position:0%
for image classification we first have a
series<00:25:43.549><c> of</c><00:25:43.579><c> convolutional</c><00:25:44.389><c> layers</c><00:25:44.629><c> that</c><00:25:45.079><c> are</c>

00:25:45.190 --> 00:25:45.200 align:start position:0%
series of convolutional layers that are
 

00:25:45.200 --> 00:25:47.139 align:start position:0%
series of convolutional layers that are
down<00:25:45.440><c> sampling</c><00:25:45.950><c> operations</c><00:25:46.609><c> for</c><00:25:46.879><c> future</c>

00:25:47.139 --> 00:25:47.149 align:start position:0%
down sampling operations for future
 

00:25:47.149 --> 00:25:49.539 align:start position:0%
down sampling operations for future
extraction<00:25:47.749><c> and</c><00:25:48.019><c> this</c><00:25:48.919><c> results</c><00:25:49.369><c> in</c><00:25:49.459><c> a</c>

00:25:49.539 --> 00:25:49.549 align:start position:0%
extraction and this results in a
 

00:25:49.549 --> 00:25:52.089 align:start position:0%
extraction and this results in a
hierarchy<00:25:50.149><c> of</c><00:25:50.179><c> learned</c><00:25:50.479><c> features</c><00:25:51.039><c> now</c><00:25:52.039><c> the</c>

00:25:52.089 --> 00:25:52.099 align:start position:0%
hierarchy of learned features now the
 

00:25:52.099 --> 00:25:55.180 align:start position:0%
hierarchy of learned features now the
difference<00:25:52.729><c> is</c><00:25:52.999><c> that</c><00:25:53.440><c> we</c><00:25:54.440><c> have</c><00:25:54.709><c> a</c><00:25:54.739><c> series</c><00:25:55.129><c> of</c>

00:25:55.180 --> 00:25:55.190 align:start position:0%
difference is that we have a series of
 

00:25:55.190 --> 00:25:58.569 align:start position:0%
difference is that we have a series of
up<00:25:55.549><c> sampling</c><00:25:56.059><c> operations</c><00:25:56.779><c> to</c><00:25:57.469><c> increase</c><00:25:58.190><c> the</c>

00:25:58.569 --> 00:25:58.579 align:start position:0%
up sampling operations to increase the
 

00:25:58.579 --> 00:26:01.149 align:start position:0%
up sampling operations to increase the
resolution<00:25:58.909><c> of</c><00:25:59.209><c> our</c><00:25:59.959><c> output</c><00:26:00.440><c> from</c><00:26:00.799><c> the</c><00:26:00.950><c> Foley</c>

00:26:01.149 --> 00:26:01.159 align:start position:0%
resolution of our output from the Foley
 

00:26:01.159 --> 00:26:06.009 align:start position:0%
resolution of our output from the Foley
from<00:26:01.909><c> the</c><00:26:02.029><c> convolutional</c><00:26:02.779><c> layers</c><00:26:03.909><c> and</c><00:26:05.019><c> to</c>

00:26:06.009 --> 00:26:06.019 align:start position:0%
from the convolutional layers and to
 

00:26:06.019 --> 00:26:08.229 align:start position:0%
from the convolutional layers and to
then<00:26:06.169><c> combine</c><00:26:06.859><c> this</c><00:26:07.129><c> output</c><00:26:07.789><c> of</c><00:26:07.820><c> the</c><00:26:08.059><c> up</c>

00:26:08.229 --> 00:26:08.239 align:start position:0%
then combine this output of the up
 

00:26:08.239 --> 00:26:10.869 align:start position:0%
then combine this output of the up
sampling<00:26:08.719><c> operations</c><00:26:09.379><c> with</c><00:26:10.129><c> those</c><00:26:10.399><c> from</c><00:26:10.759><c> our</c>

00:26:10.869 --> 00:26:10.879 align:start position:0%
sampling operations with those from our
 

00:26:10.879 --> 00:26:12.909 align:start position:0%
sampling operations with those from our
down<00:26:11.119><c> sampling</c><00:26:11.599><c> path</c><00:26:11.809><c> to</c><00:26:12.320><c> produce</c><00:26:12.709><c> a</c>

00:26:12.909 --> 00:26:12.919 align:start position:0%
down sampling path to produce a
 

00:26:12.919 --> 00:26:16.599 align:start position:0%
down sampling path to produce a
segmentation<00:26:14.619><c> one</c><00:26:15.619><c> application</c><00:26:16.339><c> of</c><00:26:16.429><c> this</c>

00:26:16.599 --> 00:26:16.609 align:start position:0%
segmentation one application of this
 

00:26:16.609 --> 00:26:18.930 align:start position:0%
segmentation one application of this
sort<00:26:16.849><c> of</c><00:26:16.940><c> architecture</c><00:26:17.299><c> is</c><00:26:17.899><c> to</c><00:26:18.200><c> the</c><00:26:18.320><c> real-time</c>

00:26:18.930 --> 00:26:18.940 align:start position:0%
sort of architecture is to the real-time
 

00:26:18.940 --> 00:26:22.209 align:start position:0%
sort of architecture is to the real-time
segmentation<00:26:19.940><c> of</c><00:26:20.149><c> the</c><00:26:20.570><c> driving</c><00:26:20.809><c> scene</c><00:26:21.219><c> here</c>

00:26:22.209 --> 00:26:22.219 align:start position:0%
segmentation of the driving scene here
 

00:26:22.219 --> 00:26:24.599 align:start position:0%
segmentation of the driving scene here
the<00:26:22.549><c> network</c><00:26:22.879><c> has</c><00:26:23.119><c> this</c><00:26:23.329><c> encoder</c><00:26:23.869><c> decoder</c>

00:26:24.599 --> 00:26:24.609 align:start position:0%
the network has this encoder decoder
 

00:26:24.609 --> 00:26:27.879 align:start position:0%
the network has this encoder decoder
architecture<00:26:25.609><c> for</c><00:26:26.479><c> encoding</c><00:26:27.109><c> the</c>

00:26:27.879 --> 00:26:27.889 align:start position:0%
architecture for encoding the
 

00:26:27.889 --> 00:26:30.039 align:start position:0%
architecture for encoding the
architecture<00:26:28.519><c> is</c><00:26:28.700><c> very</c><00:26:29.149><c> similar</c><00:26:29.570><c> to</c><00:26:29.779><c> what</c><00:26:29.929><c> we</c>

00:26:30.039 --> 00:26:30.049 align:start position:0%
architecture is very similar to what we
 

00:26:30.049 --> 00:26:32.649 align:start position:0%
architecture is very similar to what we
discussed<00:26:30.440><c> earlier</c><00:26:30.679><c> earlier</c><00:26:31.659><c> convolutional</c>

00:26:32.649 --> 00:26:32.659 align:start position:0%
discussed earlier earlier convolutional
 

00:26:32.659 --> 00:26:34.869 align:start position:0%
discussed earlier earlier convolutional
layers<00:26:32.929><c> to</c><00:26:33.259><c> learn</c><00:26:33.440><c> a</c><00:26:33.679><c> hierarchy</c><00:26:34.279><c> of</c><00:26:34.489><c> future</c>

00:26:34.869 --> 00:26:34.879 align:start position:0%
layers to learn a hierarchy of future
 

00:26:34.879 --> 00:26:37.539 align:start position:0%
layers to learn a hierarchy of future
maps<00:26:35.089><c> and</c><00:26:35.419><c> then</c><00:26:36.200><c> the</c><00:26:36.349><c> decoder</c><00:26:36.859><c> portion</c><00:26:37.369><c> of</c><00:26:37.460><c> the</c>

00:26:37.539 --> 00:26:37.549 align:start position:0%
maps and then the decoder portion of the
 

00:26:37.549 --> 00:26:39.880 align:start position:0%
maps and then the decoder portion of the
network<00:26:37.909><c> actually</c><00:26:38.210><c> uses</c><00:26:39.049><c> the</c><00:26:39.589><c> end</c>

00:26:39.880 --> 00:26:39.890 align:start position:0%
network actually uses the end
 

00:26:39.890 --> 00:26:43.720 align:start position:0%
network actually uses the end
from<00:26:40.490><c> pooling</c><00:26:40.880><c> operations</c><00:26:41.740><c> to</c><00:26:42.740><c> up</c><00:26:43.100><c> sample</c>

00:26:43.720 --> 00:26:43.730 align:start position:0%
from pooling operations to up sample
 

00:26:43.730 --> 00:26:45.790 align:start position:0%
from pooling operations to up sample
from<00:26:43.940><c> these</c><00:26:44.180><c> feature</c><00:26:44.420><c> maps</c><00:26:44.750><c> and</c><00:26:45.050><c> output</c><00:26:45.620><c> a</c>

00:26:45.790 --> 00:26:45.800 align:start position:0%
from these feature maps and output a
 

00:26:45.800 --> 00:26:50.260 align:start position:0%
from these feature maps and output a
segmentation<00:26:47.950><c> another</c><00:26:48.950><c> way</c><00:26:49.190><c> CNN's</c><00:26:49.790><c> have</c><00:26:50.030><c> been</c>

00:26:50.260 --> 00:26:50.270 align:start position:0%
segmentation another way CNN's have been
 

00:26:50.270 --> 00:26:53.020 align:start position:0%
segmentation another way CNN's have been
extended<00:26:51.140><c> and</c><00:26:51.320><c> applied</c><00:26:51.860><c> is</c><00:26:52.160><c> for</c><00:26:52.490><c> object</c>

00:26:53.020 --> 00:26:53.030 align:start position:0%
extended and applied is for object
 

00:26:53.030 --> 00:26:55.480 align:start position:0%
extended and applied is for object
detection<00:26:53.590><c> where</c><00:26:54.590><c> we're</c><00:26:54.860><c> trying</c><00:26:55.160><c> to</c><00:26:55.310><c> learn</c>

00:26:55.480 --> 00:26:55.490 align:start position:0%
detection where we're trying to learn
 

00:26:55.490 --> 00:26:57.370 align:start position:0%
detection where we're trying to learn
features<00:26:55.820><c> that</c><00:26:56.270><c> characterize</c><00:26:56.510><c> particular</c>

00:26:57.370 --> 00:26:57.380 align:start position:0%
features that characterize particular
 

00:26:57.380 --> 00:27:00.400 align:start position:0%
features that characterize particular
regions<00:26:57.860><c> of</c><00:26:58.040><c> the</c><00:26:58.160><c> input</c><00:26:58.520><c> and</c><00:26:58.750><c> then</c><00:26:59.750><c> classify</c>

00:27:00.400 --> 00:27:00.410 align:start position:0%
regions of the input and then classify
 

00:27:00.410 --> 00:27:02.980 align:start position:0%
regions of the input and then classify
those<00:27:00.620><c> regions</c><00:27:01.100><c> the</c><00:27:01.940><c> original</c><00:27:02.390><c> pipeline</c><00:27:02.810><c> for</c>

00:27:02.980 --> 00:27:02.990 align:start position:0%
those regions the original pipeline for
 

00:27:02.990 --> 00:27:05.230 align:start position:0%
those regions the original pipeline for
doing<00:27:03.260><c> this</c><00:27:03.380><c> called</c><00:27:03.830><c> our</c><00:27:04.100><c> CNN</c><00:27:04.520><c> is</c><00:27:04.730><c> pretty</c>

00:27:05.230 --> 00:27:05.240 align:start position:0%
doing this called our CNN is pretty
 

00:27:05.240 --> 00:27:06.010 align:start position:0%
doing this called our CNN is pretty
straightforward

00:27:06.010 --> 00:27:06.020 align:start position:0%
straightforward
 

00:27:06.020 --> 00:27:08.860 align:start position:0%
straightforward
given<00:27:07.010><c> an</c><00:27:07.190><c> input</c><00:27:07.550><c> image</c><00:27:07.700><c> the</c><00:27:08.180><c> algorithm</c>

00:27:08.860 --> 00:27:08.870 align:start position:0%
given an input image the algorithm
 

00:27:08.870 --> 00:27:11.340 align:start position:0%
given an input image the algorithm
extracts<00:27:09.470><c> region</c><00:27:10.130><c> proposals</c><00:27:10.640><c> bottom-up</c>

00:27:11.340 --> 00:27:11.350 align:start position:0%
extracts region proposals bottom-up
 

00:27:11.350 --> 00:27:13.750 align:start position:0%
extracts region proposals bottom-up
computes<00:27:12.350><c> features</c><00:27:12.890><c> for</c><00:27:13.160><c> each</c><00:27:13.370><c> of</c><00:27:13.640><c> these</c>

00:27:13.750 --> 00:27:13.760 align:start position:0%
computes features for each of these
 

00:27:13.760 --> 00:27:16.390 align:start position:0%
computes features for each of these
proposals<00:27:14.140><c> using</c><00:27:15.140><c> convolutional</c><00:27:15.830><c> layers</c><00:27:16.070><c> and</c>

00:27:16.390 --> 00:27:16.400 align:start position:0%
proposals using convolutional layers and
 

00:27:16.400 --> 00:27:20.920 align:start position:0%
proposals using convolutional layers and
then<00:27:16.910><c> classifies</c><00:27:17.540><c> each</c><00:27:17.900><c> region</c><00:27:18.290><c> proposal</c><00:27:19.930><c> but</c>

00:27:20.920 --> 00:27:20.930 align:start position:0%
then classifies each region proposal but
 

00:27:20.930 --> 00:27:23.770 align:start position:0%
then classifies each region proposal but
there's<00:27:21.320><c> a</c><00:27:21.380><c> huge</c><00:27:21.740><c> downside</c><00:27:21.980><c> to</c><00:27:22.430><c> this</c><00:27:22.570><c> so</c><00:27:23.570><c> in</c>

00:27:23.770 --> 00:27:23.780 align:start position:0%
there's a huge downside to this so in
 

00:27:23.780 --> 00:27:26.230 align:start position:0%
there's a huge downside to this so in
their<00:27:23.960><c> in</c><00:27:24.290><c> their</c><00:27:24.560><c> original</c><00:27:24.770><c> pipeline</c><00:27:25.460><c> this</c>

00:27:26.230 --> 00:27:26.240 align:start position:0%
their in their original pipeline this
 

00:27:26.240 --> 00:27:29.290 align:start position:0%
their in their original pipeline this
group<00:27:27.070><c> extracted</c><00:27:28.070><c> about</c><00:27:28.310><c> 2000</c><00:27:29.120><c> different</c>

00:27:29.290 --> 00:27:29.300 align:start position:0%
group extracted about 2000 different
 

00:27:29.300 --> 00:27:31.780 align:start position:0%
group extracted about 2000 different
region<00:27:29.900><c> proposals</c><00:27:30.470><c> which</c><00:27:31.190><c> meant</c><00:27:31.490><c> that</c><00:27:31.550><c> they</c>

00:27:31.780 --> 00:27:31.790 align:start position:0%
region proposals which meant that they
 

00:27:31.790 --> 00:27:33.850 align:start position:0%
region proposals which meant that they
had<00:27:31.820><c> to</c><00:27:32.090><c> run</c><00:27:32.270><c> two</c><00:27:32.570><c> thousand</c><00:27:33.080><c> cnn's</c><00:27:33.590><c> for</c>

00:27:33.850 --> 00:27:33.860 align:start position:0%
had to run two thousand cnn's for
 

00:27:33.860 --> 00:27:36.610 align:start position:0%
had to run two thousand cnn's for
feature<00:27:34.070><c> extraction</c><00:27:34.870><c> so</c><00:27:35.870><c> since</c><00:27:36.140><c> then</c><00:27:36.350><c> they're</c>

00:27:36.610 --> 00:27:36.620 align:start position:0%
feature extraction so since then they're
 

00:27:36.620 --> 00:27:39.570 align:start position:0%
feature extraction so since then they're
and<00:27:36.860><c> that</c><00:27:36.890><c> takes</c><00:27:37.640><c> a</c><00:27:37.820><c> really</c><00:27:38.150><c> really</c><00:27:38.600><c> long</c><00:27:38.810><c> time</c>

00:27:39.570 --> 00:27:39.580 align:start position:0%
and that takes a really really long time
 

00:27:39.580 --> 00:27:42.130 align:start position:0%
and that takes a really really long time
so<00:27:40.580><c> since</c><00:27:40.790><c> then</c><00:27:40.970><c> there</c><00:27:41.180><c> have</c><00:27:41.330><c> been</c><00:27:41.360><c> extensions</c>

00:27:42.130 --> 00:27:42.140 align:start position:0%
so since then there have been extensions
 

00:27:42.140 --> 00:27:45.190 align:start position:0%
so since then there have been extensions
of<00:27:42.410><c> this</c><00:27:42.530><c> basic</c><00:27:42.920><c> idea</c><00:27:43.240><c> one</c><00:27:44.240><c> being</c><00:27:44.510><c> to</c><00:27:44.840><c> first</c>

00:27:45.190 --> 00:27:45.200 align:start position:0%
of this basic idea one being to first
 

00:27:45.200 --> 00:27:48.070 align:start position:0%
of this basic idea one being to first
run<00:27:45.650><c> the</c><00:27:45.830><c> CNN</c><00:27:46.190><c> on</c><00:27:46.430><c> the</c><00:27:46.550><c> input</c><00:27:46.850><c> image</c><00:27:47.150><c> to</c><00:27:47.510><c> first</c>

00:27:48.070 --> 00:27:48.080 align:start position:0%
run the CNN on the input image to first
 

00:27:48.080 --> 00:27:50.920 align:start position:0%
run the CNN on the input image to first
extract<00:27:48.440><c> features</c><00:27:48.740><c> and</c><00:27:49.250><c> then</c><00:27:49.940><c> get</c><00:27:50.540><c> region</c>

00:27:50.920 --> 00:27:50.930 align:start position:0%
extract features and then get region
 

00:27:50.930 --> 00:27:55.150 align:start position:0%
extract features and then get region
proposals<00:27:51.530><c> from</c><00:27:51.950><c> the</c><00:27:52.040><c> future</c><00:27:52.490><c> Maps</c><00:27:54.160><c> finally</c>

00:27:55.150 --> 00:27:55.160 align:start position:0%
proposals from the future Maps finally
 

00:27:55.160 --> 00:27:57.370 align:start position:0%
proposals from the future Maps finally
let's<00:27:55.400><c> consider</c><00:27:55.520><c> image</c><00:27:56.270><c> captioning</c><00:27:56.840><c> so</c>

00:27:57.370 --> 00:27:57.380 align:start position:0%
let's consider image captioning so
 

00:27:57.380 --> 00:27:59.800 align:start position:0%
let's consider image captioning so
suppose<00:27:57.920><c> we're</c><00:27:58.130><c> given</c><00:27:58.460><c> an</c><00:27:58.610><c> image</c><00:27:59.210><c> of</c><00:27:59.480><c> a</c><00:27:59.570><c> cat</c>

00:27:59.800 --> 00:27:59.810 align:start position:0%
suppose we're given an image of a cat
 

00:27:59.810 --> 00:28:02.680 align:start position:0%
suppose we're given an image of a cat
riding<00:28:00.470><c> a</c><00:28:00.710><c> skateboard</c><00:28:00.920><c> in</c><00:28:01.690><c> classification</c>

00:28:02.680 --> 00:28:02.690 align:start position:0%
riding a skateboard in classification
 

00:28:02.690 --> 00:28:05.290 align:start position:0%
riding a skateboard in classification
our<00:28:03.230><c> task</c><00:28:03.500><c> is</c><00:28:03.740><c> to</c><00:28:03.770><c> output</c><00:28:04.070><c> a</c><00:28:04.340><c> class</c><00:28:04.640><c> label</c><00:28:04.910><c> for</c>

00:28:05.290 --> 00:28:05.300 align:start position:0%
our task is to output a class label for
 

00:28:05.300 --> 00:28:09.040 align:start position:0%
our task is to output a class label for
this<00:28:05.480><c> image</c><00:28:05.810><c> cat</c><00:28:06.440><c> and</c><00:28:06.910><c> as</c><00:28:07.910><c> we've</c><00:28:08.240><c> probably</c>

00:28:09.040 --> 00:28:09.050 align:start position:0%
this image cat and as we've probably
 

00:28:09.050 --> 00:28:11.800 align:start position:0%
this image cat and as we've probably
hammered<00:28:09.650><c> home</c><00:28:09.800><c> by</c><00:28:10.250><c> now</c><00:28:10.430><c> this</c><00:28:11.150><c> is</c><00:28:11.330><c> done</c><00:28:11.540><c> by</c>

00:28:11.800 --> 00:28:11.810 align:start position:0%
hammered home by now this is done by
 

00:28:11.810 --> 00:28:13.660 align:start position:0%
hammered home by now this is done by
feeding<00:28:12.230><c> our</c><00:28:12.350><c> input</c><00:28:12.650><c> image</c><00:28:12.950><c> through</c><00:28:13.280><c> a</c><00:28:13.310><c> set</c><00:28:13.640><c> of</c>

00:28:13.660 --> 00:28:13.670 align:start position:0%
feeding our input image through a set of
 

00:28:13.670 --> 00:28:16.450 align:start position:0%
feeding our input image through a set of
convolutional<00:28:14.540><c> layers</c><00:28:15.100><c> extracting</c><00:28:16.100><c> features</c>

00:28:16.450 --> 00:28:16.460 align:start position:0%
convolutional layers extracting features
 

00:28:16.460 --> 00:28:18.850 align:start position:0%
convolutional layers extracting features
and<00:28:16.790><c> then</c><00:28:17.330><c> passing</c><00:28:17.870><c> these</c><00:28:17.990><c> features</c><00:28:18.380><c> on</c><00:28:18.620><c> to</c>

00:28:18.850 --> 00:28:18.860 align:start position:0%
and then passing these features on to
 

00:28:18.860 --> 00:28:21.370 align:start position:0%
and then passing these features on to
fully<00:28:19.100><c> connected</c><00:28:19.610><c> layers</c><00:28:19.850><c> to</c><00:28:20.570><c> predict</c><00:28:21.170><c> a</c>

00:28:21.370 --> 00:28:21.380 align:start position:0%
fully connected layers to predict a
 

00:28:21.380 --> 00:28:25.000 align:start position:0%
fully connected layers to predict a
label<00:28:21.800><c> in</c><00:28:22.870><c> image</c><00:28:23.870><c> captioning</c><00:28:24.410><c> we</c><00:28:24.560><c> want</c><00:28:24.800><c> to</c>

00:28:25.000 --> 00:28:25.010 align:start position:0%
label in image captioning we want to
 

00:28:25.010 --> 00:28:27.490 align:start position:0%
label in image captioning we want to
generate<00:28:25.430><c> a</c><00:28:25.490><c> sentence</c><00:28:26.060><c> that</c><00:28:26.300><c> describes</c><00:28:26.840><c> the</c>

00:28:27.490 --> 00:28:27.500 align:start position:0%
generate a sentence that describes the
 

00:28:27.500 --> 00:28:30.580 align:start position:0%
generate a sentence that describes the
semantic<00:28:28.040><c> content</c><00:28:28.250><c> of</c><00:28:28.700><c> the</c><00:28:28.910><c> same</c><00:28:29.150><c> image</c><00:28:29.590><c> so</c>

00:28:30.580 --> 00:28:30.590 align:start position:0%
semantic content of the same image so
 

00:28:30.590 --> 00:28:32.710 align:start position:0%
semantic content of the same image so
let's<00:28:31.160><c> take</c><00:28:31.400><c> the</c><00:28:31.550><c> same</c><00:28:31.730><c> network</c><00:28:32.180><c> from</c><00:28:32.330><c> before</c>

00:28:32.710 --> 00:28:32.720 align:start position:0%
let's take the same network from before
 

00:28:32.720 --> 00:28:35.110 align:start position:0%
let's take the same network from before
and<00:28:32.960><c> remove</c><00:28:33.740><c> the</c><00:28:33.950><c> fully</c><00:28:34.160><c> connected</c><00:28:34.640><c> layers</c><00:28:34.880><c> at</c>

00:28:35.110 --> 00:28:35.120 align:start position:0%
and remove the fully connected layers at
 

00:28:35.120 --> 00:28:38.230 align:start position:0%
and remove the fully connected layers at
the<00:28:35.150><c> end</c><00:28:35.530><c> now</c><00:28:36.530><c> we</c><00:28:36.590><c> only</c><00:28:37.100><c> have</c><00:28:37.250><c> convolutional</c>

00:28:38.230 --> 00:28:38.240 align:start position:0%
the end now we only have convolutional
 

00:28:38.240 --> 00:28:40.840 align:start position:0%
the end now we only have convolutional
layers<00:28:38.540><c> to</c><00:28:38.780><c> extract</c><00:28:39.170><c> features</c><00:28:39.590><c> and</c><00:28:39.950><c> will</c>

00:28:40.840 --> 00:28:40.850 align:start position:0%
layers to extract features and will
 

00:28:40.850 --> 00:28:43.420 align:start position:0%
layers to extract features and will
replace<00:28:41.180><c> the</c><00:28:41.570><c> fully</c><00:28:41.780><c> connected</c><00:28:42.260><c> layers</c><00:28:42.530><c> with</c>

00:28:43.420 --> 00:28:43.430 align:start position:0%
replace the fully connected layers with
 

00:28:43.430 --> 00:28:46.480 align:start position:0%
replace the fully connected layers with
a<00:28:43.640><c> recurrent</c><00:28:44.180><c> neural</c><00:28:44.330><c> network</c><00:28:44.780><c> the</c><00:28:45.370><c> output</c><00:28:46.370><c> of</c>

00:28:46.480 --> 00:28:46.490 align:start position:0%
a recurrent neural network the output of
 

00:28:46.490 --> 00:28:48.280 align:start position:0%
a recurrent neural network the output of
the<00:28:46.610><c> convolutional</c><00:28:47.300><c> layers</c><00:28:47.540><c> gives</c><00:28:47.960><c> us</c><00:28:48.140><c> a</c>

00:28:48.280 --> 00:28:48.290 align:start position:0%
the convolutional layers gives us a
 

00:28:48.290 --> 00:28:51.610 align:start position:0%
the convolutional layers gives us a
fixed<00:28:48.770><c> length</c><00:28:48.950><c> encoding</c><00:28:49.460><c> of</c><00:28:50.260><c> the</c><00:28:51.260><c> features</c>

00:28:51.610 --> 00:28:51.620 align:start position:0%
fixed length encoding of the features
 

00:28:51.620 --> 00:28:53.190 align:start position:0%
fixed length encoding of the features
present<00:28:51.950><c> in</c><00:28:52.190><c> our</c><00:28:52.340><c> input</c><00:28:52.640><c> image</c>

00:28:53.190 --> 00:28:53.200 align:start position:0%
present in our input image
 

00:28:53.200 --> 00:28:55.620 align:start position:0%
present in our input image
which<00:28:53.530><c> we</c><00:28:53.740><c> can</c><00:28:53.860><c> use</c><00:28:54.130><c> to</c><00:28:54.370><c> initialize</c><00:28:55.000><c> an</c><00:28:55.240><c> RNN</c>

00:28:55.620 --> 00:28:55.630 align:start position:0%
which we can use to initialize an RNN
 

00:28:55.630 --> 00:28:58.770 align:start position:0%
which we can use to initialize an RNN
that<00:28:56.260><c> we</c><00:28:56.350><c> can</c><00:28:56.500><c> then</c><00:28:56.680><c> train</c><00:28:57.010><c> to</c><00:28:57.310><c> predict</c><00:28:57.790><c> the</c>

00:28:58.770 --> 00:28:58.780 align:start position:0%
that we can then train to predict the
 

00:28:58.780 --> 00:29:03.210 align:start position:0%
that we can then train to predict the
words<00:28:58.990><c> that</c><00:28:59.290><c> describe</c><00:28:59.710><c> this</c><00:28:59.980><c> image</c><00:29:02.220><c> using</c>

00:29:03.210 --> 00:29:03.220 align:start position:0%
words that describe this image using
 

00:29:03.220 --> 00:29:07.800 align:start position:0%
words that describe this image using
using<00:29:03.880><c> the</c><00:29:04.000><c> Arnon</c><00:29:05.730><c> so</c><00:29:06.730><c> now</c><00:29:07.180><c> that</c><00:29:07.390><c> we've</c><00:29:07.510><c> talked</c>

00:29:07.800 --> 00:29:07.810 align:start position:0%
using the Arnon so now that we've talked
 

00:29:07.810 --> 00:29:09.540 align:start position:0%
using the Arnon so now that we've talked
about<00:29:07.960><c> convolutional</c><00:29:08.890><c> neural</c><00:29:09.070><c> networks</c>

00:29:09.540 --> 00:29:09.550 align:start position:0%
about convolutional neural networks
 

00:29:09.550 --> 00:29:12.780 align:start position:0%
about convolutional neural networks
their<00:29:10.090><c> applications</c><00:29:10.920><c> we</c><00:29:11.920><c> can</c><00:29:11.950><c> introduce</c><00:29:12.370><c> some</c>

00:29:12.780 --> 00:29:12.790 align:start position:0%
their applications we can introduce some
 

00:29:12.790 --> 00:29:14.970 align:start position:0%
their applications we can introduce some
tools<00:29:13.120><c> that</c><00:29:13.450><c> have</c><00:29:13.840><c> recently</c><00:29:14.230><c> been</c><00:29:14.440><c> been</c>

00:29:14.970 --> 00:29:14.980 align:start position:0%
tools that have recently been been
 

00:29:14.980 --> 00:29:17.490 align:start position:0%
tools that have recently been been
designed<00:29:15.400><c> to</c><00:29:15.940><c> probe</c><00:29:16.300><c> and</c><00:29:16.600><c> visualize</c><00:29:17.140><c> the</c>

00:29:17.490 --> 00:29:17.500 align:start position:0%
designed to probe and visualize the
 

00:29:17.500 --> 00:29:20.310 align:start position:0%
designed to probe and visualize the
inner<00:29:17.830><c> workings</c><00:29:18.280><c> of</c><00:29:18.490><c> a</c><00:29:19.150><c> CN</c><00:29:19.420><c> n</c><00:29:19.540><c> to</c><00:29:19.810><c> get</c><00:29:19.990><c> at</c><00:29:20.170><c> this</c>

00:29:20.310 --> 00:29:20.320 align:start position:0%
inner workings of a CN n to get at this
 

00:29:20.320 --> 00:29:22.320 align:start position:0%
inner workings of a CN n to get at this
question<00:29:20.710><c> of</c><00:29:20.860><c> what</c><00:29:21.340><c> is</c><00:29:21.400><c> the</c><00:29:21.700><c> network</c><00:29:21.880><c> actually</c>

00:29:22.320 --> 00:29:22.330 align:start position:0%
question of what is the network actually
 

00:29:22.330 --> 00:29:25.920 align:start position:0%
question of what is the network actually
seeing<00:29:23.610><c> so</c><00:29:24.610><c> first</c><00:29:24.910><c> off</c><00:29:25.090><c> a</c><00:29:25.120><c> few</c><00:29:25.570><c> years</c><00:29:25.780><c> ago</c>

00:29:25.920 --> 00:29:25.930 align:start position:0%
seeing so first off a few years ago
 

00:29:25.930 --> 00:29:27.980 align:start position:0%
seeing so first off a few years ago
there<00:29:26.290><c> was</c><00:29:26.530><c> a</c><00:29:26.590><c> paper</c><00:29:26.980><c> that</c><00:29:27.310><c> published</c><00:29:27.790><c> an</c>

00:29:27.980 --> 00:29:27.990 align:start position:0%
there was a paper that published an
 

00:29:27.990 --> 00:29:31.470 align:start position:0%
there was a paper that published an
interactive<00:29:28.990><c> visualization</c><00:29:29.620><c> tool</c><00:29:30.490><c> of</c><00:29:30.700><c> a</c>

00:29:31.470 --> 00:29:31.480 align:start position:0%
interactive visualization tool of a
 

00:29:31.480 --> 00:29:34.170 align:start position:0%
interactive visualization tool of a
convolutional<00:29:32.320><c> neural</c><00:29:32.800><c> network</c><00:29:33.220><c> trained</c><00:29:33.940><c> on</c>

00:29:34.170 --> 00:29:34.180 align:start position:0%
convolutional neural network trained on
 

00:29:34.180 --> 00:29:36.720 align:start position:0%
convolutional neural network trained on
a<00:29:34.240><c> dataset</c><00:29:34.930><c> of</c><00:29:35.080><c> handwritten</c><00:29:35.830><c> digits</c><00:29:36.340><c> a</c><00:29:36.520><c> very</c>

00:29:36.720 --> 00:29:36.730 align:start position:0%
a dataset of handwritten digits a very
 

00:29:36.730 --> 00:29:39.240 align:start position:0%
a dataset of handwritten digits a very
famous<00:29:36.910><c> data</c><00:29:37.240><c> set</c><00:29:37.510><c> called</c><00:29:37.720><c> M</c><00:29:37.990><c> nest</c><00:29:38.230><c> and</c><00:29:38.590><c> you</c>

00:29:39.240 --> 00:29:39.250 align:start position:0%
famous data set called M nest and you
 

00:29:39.250 --> 00:29:40.740 align:start position:0%
famous data set called M nest and you
can<00:29:39.400><c> play</c><00:29:39.610><c> around</c><00:29:39.730><c> with</c><00:29:40.060><c> this</c><00:29:40.210><c> tool</c><00:29:40.510><c> to</c>

00:29:40.740 --> 00:29:40.750 align:start position:0%
can play around with this tool to
 

00:29:40.750 --> 00:29:42.840 align:start position:0%
can play around with this tool to
visualize<00:29:41.020><c> the</c><00:29:41.920><c> behavior</c><00:29:42.460><c> of</c><00:29:42.490><c> the</c><00:29:42.700><c> network</c>

00:29:42.840 --> 00:29:42.850 align:start position:0%
visualize the behavior of the network
 

00:29:42.850 --> 00:29:45.540 align:start position:0%
visualize the behavior of the network
given<00:29:43.660><c> a</c><00:29:43.810><c> number</c><00:29:44.140><c> that</c><00:29:44.320><c> you</c><00:29:44.380><c> yourself</c><00:29:45.010><c> draw</c><00:29:45.280><c> in</c>

00:29:45.540 --> 00:29:45.550 align:start position:0%
given a number that you yourself draw in
 

00:29:45.550 --> 00:29:48.030 align:start position:0%
given a number that you yourself draw in
and<00:29:45.910><c> what</c><00:29:46.690><c> you're</c><00:29:46.840><c> seeing</c><00:29:47.080><c> here</c><00:29:47.380><c> is</c><00:29:47.440><c> the</c>

00:29:48.030 --> 00:29:48.040 align:start position:0%
and what you're seeing here is the
 

00:29:48.040 --> 00:29:50.790 align:start position:0%
and what you're seeing here is the
feature<00:29:48.280><c> maps</c><00:29:48.640><c> for</c><00:29:49.240><c> this</c><00:29:49.450><c> seven</c><00:29:49.870><c> that</c><00:29:50.200><c> someone</c>

00:29:50.790 --> 00:29:50.800 align:start position:0%
feature maps for this seven that someone
 

00:29:50.800 --> 00:29:54.120 align:start position:0%
feature maps for this seven that someone
has<00:29:50.980><c> drawn</c><00:29:51.220><c> in</c><00:29:51.400><c> and</c><00:29:52.200><c> as</c><00:29:53.200><c> we</c><00:29:53.230><c> can</c><00:29:53.500><c> see</c><00:29:53.710><c> in</c><00:29:53.950><c> the</c>

00:29:54.120 --> 00:29:54.130 align:start position:0%
has drawn in and as we can see in the
 

00:29:54.130 --> 00:29:56.160 align:start position:0%
has drawn in and as we can see in the
first<00:29:54.310><c> layer</c><00:29:54.580><c> the</c><00:29:54.940><c> first</c><00:29:55.120><c> six</c><00:29:55.450><c> filters</c><00:29:55.870><c> are</c>

00:29:56.160 --> 00:29:56.170 align:start position:0%
first layer the first six filters are
 

00:29:56.170 --> 00:29:59.490 align:start position:0%
first layer the first six filters are
showing<00:29:57.100><c> primarily</c><00:29:57.790><c> edged</c><00:29:58.600><c> detection</c><00:29:59.260><c> and</c>

00:29:59.490 --> 00:29:59.500 align:start position:0%
showing primarily edged detection and
 

00:29:59.500 --> 00:30:01.590 align:start position:0%
showing primarily edged detection and
deeper<00:30:00.250><c> layers</c><00:30:00.460><c> will</c><00:30:00.790><c> start</c><00:30:01.030><c> to</c><00:30:01.150><c> pick</c><00:30:01.270><c> up</c><00:30:01.390><c> on</c>

00:30:01.590 --> 00:30:01.600 align:start position:0%
deeper layers will start to pick up on
 

00:30:01.600 --> 00:30:04.680 align:start position:0%
deeper layers will start to pick up on
corners<00:30:02.080><c> crosses</c><00:30:02.920><c> curves</c><00:30:03.340><c> more</c><00:30:04.180><c> complex</c>

00:30:04.680 --> 00:30:04.690 align:start position:0%
corners crosses curves more complex
 

00:30:04.690 --> 00:30:08.250 align:start position:0%
corners crosses curves more complex
features<00:30:05.550><c> the</c><00:30:06.550><c> exact</c><00:30:06.880><c> hierarchy</c><00:30:07.510><c> that</c><00:30:07.540><c> that</c>

00:30:08.250 --> 00:30:08.260 align:start position:0%
features the exact hierarchy that that
 

00:30:08.260 --> 00:30:10.710 align:start position:0%
features the exact hierarchy that that
we<00:30:08.590><c> introduced</c><00:30:09.130><c> in</c><00:30:09.940><c> the</c><00:30:10.120><c> beginning</c><00:30:10.450><c> of</c><00:30:10.570><c> this</c>

00:30:10.710 --> 00:30:10.720 align:start position:0%
we introduced in the beginning of this
 

00:30:10.720 --> 00:30:14.550 align:start position:0%
we introduced in the beginning of this
lecture<00:30:10.930><c> a</c><00:30:12.240><c> second</c><00:30:13.240><c> method</c><00:30:13.360><c> which</c><00:30:13.630><c> you'll</c><00:30:13.990><c> you</c>

00:30:14.550 --> 00:30:14.560 align:start position:0%
lecture a second method which you'll you
 

00:30:14.560 --> 00:30:16.380 align:start position:0%
lecture a second method which you'll you
yourself<00:30:15.040><c> will</c><00:30:15.250><c> have</c><00:30:15.400><c> the</c><00:30:15.550><c> chance</c><00:30:15.760><c> to</c><00:30:15.970><c> play</c>

00:30:16.380 --> 00:30:16.390 align:start position:0%
yourself will have the chance to play
 

00:30:16.390 --> 00:30:18.690 align:start position:0%
yourself will have the chance to play
around<00:30:16.690><c> with</c><00:30:16.930><c> in</c><00:30:17.140><c> the</c><00:30:17.560><c> second</c><00:30:17.920><c> lap</c><00:30:18.040><c> is</c><00:30:18.280><c> called</c>

00:30:18.690 --> 00:30:18.700 align:start position:0%
around with in the second lap is called
 

00:30:18.700 --> 00:30:22.290 align:start position:0%
around with in the second lap is called
class<00:30:19.030><c> activation</c><00:30:19.750><c> Maps</c><00:30:19.960><c> or</c><00:30:20.230><c> cams</c><00:30:21.300><c> cams</c>

00:30:22.290 --> 00:30:22.300 align:start position:0%
class activation Maps or cams cams
 

00:30:22.300 --> 00:30:25.050 align:start position:0%
class activation Maps or cams cams
generate<00:30:22.930><c> a</c><00:30:23.080><c> heat</c><00:30:23.560><c> map</c><00:30:23.770><c> that</c><00:30:24.100><c> indicates</c><00:30:24.670><c> the</c>

00:30:25.050 --> 00:30:25.060 align:start position:0%
generate a heat map that indicates the
 

00:30:25.060 --> 00:30:28.920 align:start position:0%
generate a heat map that indicates the
regions<00:30:25.570><c> of</c><00:30:25.780><c> an</c><00:30:26.560><c> image</c><00:30:26.830><c> -</c><00:30:27.100><c> which</c><00:30:27.250><c> are</c><00:30:27.480><c> CNN</c><00:30:28.480><c> for</c>

00:30:28.920 --> 00:30:28.930 align:start position:0%
regions of an image - which are CNN for
 

00:30:28.930 --> 00:30:31.740 align:start position:0%
regions of an image - which are CNN for
classification<00:30:29.380><c> attends</c><00:30:30.280><c> to</c><00:30:30.580><c> in</c><00:30:30.820><c> its</c><00:30:31.390><c> final</c>

00:30:31.740 --> 00:30:31.750 align:start position:0%
classification attends to in its final
 

00:30:31.750 --> 00:30:35.070 align:start position:0%
classification attends to in its final
layers<00:30:31.960><c> and</c><00:30:32.700><c> the</c><00:30:33.700><c> way</c><00:30:33.880><c> that</c><00:30:34.120><c> this</c><00:30:34.390><c> is</c><00:30:34.570><c> computed</c>

00:30:35.070 --> 00:30:35.080 align:start position:0%
layers and the way that this is computed
 

00:30:35.080 --> 00:30:38.370 align:start position:0%
layers and the way that this is computed
is<00:30:35.290><c> is</c><00:30:36.100><c> the</c><00:30:36.340><c> following</c><00:30:36.700><c> we</c><00:30:37.480><c> choose</c><00:30:37.810><c> an</c><00:30:38.080><c> output</c>

00:30:38.370 --> 00:30:38.380 align:start position:0%
is is the following we choose an output
 

00:30:38.380 --> 00:30:41.130 align:start position:0%
is is the following we choose an output
class<00:30:38.860><c> that</c><00:30:39.130><c> we</c><00:30:39.250><c> want</c><00:30:39.460><c> to</c><00:30:39.550><c> visualize</c><00:30:39.960><c> we</c><00:30:40.960><c> can</c>

00:30:41.130 --> 00:30:41.140 align:start position:0%
class that we want to visualize we can
 

00:30:41.140 --> 00:30:43.200 align:start position:0%
class that we want to visualize we can
then<00:30:41.320><c> obtain</c><00:30:41.560><c> the</c><00:30:41.830><c> weights</c><00:30:42.190><c> from</c><00:30:42.490><c> the</c><00:30:42.610><c> last</c>

00:30:43.200 --> 00:30:43.210 align:start position:0%
then obtain the weights from the last
 

00:30:43.210 --> 00:30:45.570 align:start position:0%
then obtain the weights from the last
fully<00:30:43.570><c> connected</c><00:30:44.080><c> layer</c><00:30:44.290><c> because</c><00:30:45.220><c> these</c>

00:30:45.570 --> 00:30:45.580 align:start position:0%
fully connected layer because these
 

00:30:45.580 --> 00:30:47.760 align:start position:0%
fully connected layer because these
represent<00:30:46.300><c> the</c><00:30:46.360><c> importance</c><00:30:47.020><c> of</c><00:30:47.290><c> each</c><00:30:47.470><c> of</c><00:30:47.620><c> the</c>

00:30:47.760 --> 00:30:47.770 align:start position:0%
represent the importance of each of the
 

00:30:47.770 --> 00:30:50.280 align:start position:0%
represent the importance of each of the
final<00:30:48.130><c> feature</c><00:30:48.310><c> maps</c><00:30:48.670><c> in</c><00:30:48.960><c> outputting</c><00:30:49.960><c> that</c>

00:30:50.280 --> 00:30:50.290 align:start position:0%
final feature maps in outputting that
 

00:30:50.290 --> 00:30:53.400 align:start position:0%
final feature maps in outputting that
class<00:30:51.030><c> we</c><00:30:52.030><c> can</c><00:30:52.180><c> compute</c><00:30:52.600><c> our</c><00:30:52.750><c> heat</c><00:30:52.990><c> map</c><00:30:53.200><c> as</c>

00:30:53.400 --> 00:30:53.410 align:start position:0%
class we can compute our heat map as
 

00:30:53.410 --> 00:30:56.550 align:start position:0%
class we can compute our heat map as
simply<00:30:54.220><c> a</c><00:30:54.250><c> weighted</c><00:30:54.790><c> combination</c><00:30:55.060><c> of</c><00:30:55.720><c> each</c><00:30:56.350><c> of</c>

00:30:56.550 --> 00:30:56.560 align:start position:0%
simply a weighted combination of each of
 

00:30:56.560 --> 00:30:58.650 align:start position:0%
simply a weighted combination of each of
the<00:30:56.680><c> final</c><00:30:57.100><c> convolutional</c><00:30:58.030><c> feature</c><00:30:58.300><c> maps</c>

00:30:58.650 --> 00:30:58.660 align:start position:0%
the final convolutional feature maps
 

00:30:58.660 --> 00:31:01.050 align:start position:0%
the final convolutional feature maps
using<00:30:59.530><c> the</c><00:30:59.800><c> weights</c><00:31:00.010><c> from</c><00:31:00.220><c> the</c><00:31:00.310><c> last</c><00:31:00.640><c> fully</c>

00:31:01.050 --> 00:31:01.060 align:start position:0%
using the weights from the last fully
 

00:31:01.060 --> 00:31:05.370 align:start position:0%
using the weights from the last fully
connected<00:31:01.540><c> layer</c><00:31:03.390><c> we</c><00:31:04.390><c> can</c><00:31:04.510><c> apply</c><00:31:04.810><c> cams</c><00:31:05.140><c> to</c>

00:31:05.370 --> 00:31:05.380 align:start position:0%
connected layer we can apply cams to
 

00:31:05.380 --> 00:31:06.790 align:start position:0%
connected layer we can apply cams to
visualize<00:31:05.830><c> both</c><00:31:06.310><c> the</c>

00:31:06.790 --> 00:31:06.800 align:start position:0%
visualize both the
 

00:31:06.800 --> 00:31:09.190 align:start position:0%
visualize both the
activation<00:31:07.160><c> maps</c><00:31:07.640><c> for</c><00:31:08.090><c> the</c><00:31:08.630><c> most</c><00:31:08.930><c> likely</c>

00:31:09.190 --> 00:31:09.200 align:start position:0%
activation maps for the most likely
 

00:31:09.200 --> 00:31:12.490 align:start position:0%
activation maps for the most likely
predictions<00:31:09.980><c> of</c><00:31:10.130><c> an</c><00:31:10.430><c> object</c><00:31:11.210><c> class</c><00:31:11.390><c> for</c><00:31:11.840><c> one</c>

00:31:12.490 --> 00:31:12.500 align:start position:0%
predictions of an object class for one
 

00:31:12.500 --> 00:31:15.700 align:start position:0%
predictions of an object class for one
image<00:31:12.680><c> as</c><00:31:13.070><c> you</c><00:31:13.580><c> see</c><00:31:13.970><c> on</c><00:31:14.090><c> the</c><00:31:14.210><c> left</c><00:31:14.420><c> and</c><00:31:14.780><c> also</c><00:31:15.470><c> to</c>

00:31:15.700 --> 00:31:15.710 align:start position:0%
image as you see on the left and also to
 

00:31:15.710 --> 00:31:19.420 align:start position:0%
image as you see on the left and also to
visualize<00:31:16.370><c> the</c><00:31:16.820><c> image</c><00:31:17.150><c> regions</c><00:31:17.930><c> used</c><00:31:18.500><c> by</c><00:31:18.770><c> the</c>

00:31:19.420 --> 00:31:19.430 align:start position:0%
visualize the image regions used by the
 

00:31:19.430 --> 00:31:22.000 align:start position:0%
visualize the image regions used by the
CNN<00:31:19.850><c> to</c><00:31:20.060><c> identify</c><00:31:20.720><c> in</c><00:31:21.020><c> different</c><00:31:21.650><c> instances</c>

00:31:22.000 --> 00:31:22.010 align:start position:0%
CNN to identify in different instances
 

00:31:22.010 --> 00:31:25.630 align:start position:0%
CNN to identify in different instances
of<00:31:22.460><c> one</c><00:31:22.970><c> object</c><00:31:23.210><c> class</c><00:31:23.690><c> as</c><00:31:23.990><c> you</c><00:31:24.440><c> see</c><00:31:24.620><c> on</c><00:31:25.190><c> the</c>

00:31:25.630 --> 00:31:25.640 align:start position:0%
of one object class as you see on the
 

00:31:25.640 --> 00:31:30.220 align:start position:0%
of one object class as you see on the
right<00:31:27.130><c> so</c><00:31:28.130><c> to</c><00:31:28.910><c> conclude</c><00:31:29.360><c> this</c><00:31:29.720><c> talk</c><00:31:29.960><c> I'd</c><00:31:30.200><c> like</c>

00:31:30.220 --> 00:31:30.230 align:start position:0%
right so to conclude this talk I'd like
 

00:31:30.230 --> 00:31:34.390 align:start position:0%
right so to conclude this talk I'd like
to<00:31:31.180><c> take</c><00:31:32.180><c> a</c><00:31:32.210><c> brief</c><00:31:32.420><c> consideration</c><00:31:33.200><c> of</c><00:31:33.440><c> how</c>

00:31:34.390 --> 00:31:34.400 align:start position:0%
to take a brief consideration of how
 

00:31:34.400 --> 00:31:36.490 align:start position:0%
to take a brief consideration of how
deep<00:31:35.030><c> learning</c><00:31:35.420><c> for</c><00:31:35.600><c> computer</c><00:31:35.960><c> vision</c><00:31:35.990><c> has</c>

00:31:36.490 --> 00:31:36.500 align:start position:0%
deep learning for computer vision has
 

00:31:36.500 --> 00:31:39.220 align:start position:0%
deep learning for computer vision has
really<00:31:36.890><c> made</c><00:31:37.100><c> an</c><00:31:37.310><c> impact</c><00:31:38.090><c> over</c><00:31:38.450><c> the</c><00:31:38.900><c> past</c>

00:31:39.220 --> 00:31:39.230 align:start position:0%
really made an impact over the past
 

00:31:39.230 --> 00:31:42.400 align:start position:0%
really made an impact over the past
several<00:31:39.890><c> years</c><00:31:40.040><c> and</c><00:31:40.630><c> the</c><00:31:41.630><c> advances</c><00:31:42.200><c> that</c><00:31:42.350><c> have</c>

00:31:42.400 --> 00:31:42.410 align:start position:0%
several years and the advances that have
 

00:31:42.410 --> 00:31:44.290 align:start position:0%
several years and the advances that have
been<00:31:42.680><c> made</c><00:31:42.920><c> in</c><00:31:42.950><c> deep</c><00:31:43.370><c> learning</c><00:31:43.580><c> for</c><00:31:43.940><c> computer</c>

00:31:44.290 --> 00:31:44.300 align:start position:0%
been made in deep learning for computer
 

00:31:44.300 --> 00:31:46.180 align:start position:0%
been made in deep learning for computer
vision<00:31:44.330><c> would</c><00:31:45.200><c> really</c><00:31:45.530><c> not</c><00:31:45.740><c> be</c><00:31:45.800><c> possible</c>

00:31:46.180 --> 00:31:46.190 align:start position:0%
vision would really not be possible
 

00:31:46.190 --> 00:31:49.240 align:start position:0%
vision would really not be possible
without<00:31:46.700><c> the</c><00:31:47.210><c> available</c><00:31:48.010><c> availability</c><00:31:49.010><c> of</c>

00:31:49.240 --> 00:31:49.250 align:start position:0%
without the available availability of
 

00:31:49.250 --> 00:31:52.110 align:start position:0%
without the available availability of
large<00:31:49.550><c> and</c><00:31:49.970><c> well</c><00:31:50.570><c> annotated</c><00:31:50.990><c> image</c><00:31:51.620><c> datasets</c>

00:31:52.110 --> 00:31:52.120 align:start position:0%
large and well annotated image datasets
 

00:31:52.120 --> 00:31:56.020 align:start position:0%
large and well annotated image datasets
and<00:31:53.120><c> this</c><00:31:53.570><c> has</c><00:31:53.750><c> really</c><00:31:54.230><c> facilitated</c><00:31:55.100><c> the</c>

00:31:56.020 --> 00:31:56.030 align:start position:0%
and this has really facilitated the
 

00:31:56.030 --> 00:31:58.150 align:start position:0%
and this has really facilitated the
progress<00:31:56.390><c> that's</c><00:31:56.570><c> been</c><00:31:56.780><c> made</c><00:31:56.840><c> some</c><00:31:57.830><c> datasets</c>

00:31:58.150 --> 00:31:58.160 align:start position:0%
progress that's been made some datasets
 

00:31:58.160 --> 00:32:01.570 align:start position:0%
progress that's been made some datasets
include<00:31:58.970><c> image</c><00:31:59.330><c> net</c><00:31:59.540><c> which</c><00:31:59.840><c> we</c><00:32:00.680><c> discussed</c><00:32:01.100><c> and</c>

00:32:01.570 --> 00:32:01.580 align:start position:0%
include image net which we discussed and
 

00:32:01.580 --> 00:32:05.530 align:start position:0%
include image net which we discussed and
this<00:32:02.420><c> a</c><00:32:02.780><c> data</c><00:32:03.620><c> set</c><00:32:03.860><c> of</c><00:32:03.890><c> handwritten</c><00:32:04.580><c> digits</c>

00:32:05.530 --> 00:32:05.540 align:start position:0%
this a data set of handwritten digits
 

00:32:05.540 --> 00:32:07.690 align:start position:0%
this a data set of handwritten digits
which<00:32:05.810><c> was</c><00:32:05.990><c> used</c><00:32:06.260><c> in</c><00:32:06.440><c> some</c><00:32:06.650><c> of</c><00:32:06.710><c> the</c><00:32:06.920><c> first</c><00:32:07.310><c> big</c>

00:32:07.690 --> 00:32:07.700 align:start position:0%
which was used in some of the first big
 

00:32:07.700 --> 00:32:11.650 align:start position:0%
which was used in some of the first big
CNN<00:32:08.570><c> papers</c><00:32:09.340><c> places</c><00:32:10.340><c> a</c><00:32:10.370><c> database</c><00:32:11.090><c> from</c><00:32:11.420><c> here</c>

00:32:11.650 --> 00:32:11.660 align:start position:0%
CNN papers places a database from here
 

00:32:11.660 --> 00:32:14.110 align:start position:0%
CNN papers places a database from here
at<00:32:11.780><c> MIT</c><00:32:12.020><c> of</c><00:32:12.530><c> scenes</c><00:32:13.100><c> and</c><00:32:13.370><c> landscapes</c><00:32:13.880><c> and</c>

00:32:14.110 --> 00:32:14.120 align:start position:0%
at MIT of scenes and landscapes and
 

00:32:14.120 --> 00:32:17.860 align:start position:0%
at MIT of scenes and landscapes and
cypher<00:32:15.050><c> 10</c><00:32:15.290><c> which</c><00:32:15.860><c> contains</c><00:32:16.340><c> images</c><00:32:16.790><c> from</c><00:32:17.450><c> ten</c>

00:32:17.860 --> 00:32:17.870 align:start position:0%
cypher 10 which contains images from ten
 

00:32:17.870 --> 00:32:21.340 align:start position:0%
cypher 10 which contains images from ten
different<00:32:18.200><c> categories</c><00:32:18.370><c> listed</c><00:32:19.370><c> here</c><00:32:20.350><c> the</c>

00:32:21.340 --> 00:32:21.350 align:start position:0%
different categories listed here the
 

00:32:21.350 --> 00:32:25.110 align:start position:0%
different categories listed here the
impact<00:32:21.770><c> has</c><00:32:22.100><c> been</c><00:32:22.130><c> broad</c><00:32:22.730><c> and</c><00:32:23.090><c> deep</c><00:32:23.890><c> and</c>

00:32:25.110 --> 00:32:25.120 align:start position:0%
impact has been broad and deep and
 

00:32:25.120 --> 00:32:27.760 align:start position:0%
impact has been broad and deep and
spanning<00:32:26.120><c> a</c><00:32:26.210><c> variety</c><00:32:26.750><c> of</c><00:32:26.780><c> different</c><00:32:27.470><c> fields</c>

00:32:27.760 --> 00:32:27.770 align:start position:0%
spanning a variety of different fields
 

00:32:27.770 --> 00:32:32.380 align:start position:0%
spanning a variety of different fields
everything<00:32:28.550><c> from</c><00:32:28.820><c> medicine</c><00:32:29.390><c> to</c><00:32:31.390><c> self-driving</c>

00:32:32.380 --> 00:32:32.390 align:start position:0%
everything from medicine to self-driving
 

00:32:32.390 --> 00:32:36.790 align:start position:0%
everything from medicine to self-driving
cars<00:32:32.780><c> to</c><00:32:33.370><c> security</c><00:32:34.900><c> one</c><00:32:35.900><c> area</c><00:32:36.260><c> in</c><00:32:36.590><c> which</c>

00:32:36.790 --> 00:32:36.800 align:start position:0%
cars to security one area in which
 

00:32:36.800 --> 00:32:39.130 align:start position:0%
cars to security one area in which
convolutional<00:32:37.790><c> neural</c><00:32:37.970><c> networks</c><00:32:38.450><c> really</c>

00:32:39.130 --> 00:32:39.140 align:start position:0%
convolutional neural networks really
 

00:32:39.140 --> 00:32:41.560 align:start position:0%
convolutional neural networks really
made<00:32:39.290><c> a</c><00:32:39.320><c> big</c><00:32:39.500><c> impact</c><00:32:39.710><c> early</c><00:32:40.190><c> on</c><00:32:40.490><c> was</c><00:32:40.820><c> in</c><00:32:41.030><c> facial</c>

00:32:41.560 --> 00:32:41.570 align:start position:0%
made a big impact early on was in facial
 

00:32:41.570 --> 00:32:43.570 align:start position:0%
made a big impact early on was in facial
recognition<00:32:41.780><c> software</c><00:32:42.290><c> and</c><00:32:42.740><c> if</c><00:32:43.250><c> you</c><00:32:43.370><c> think</c>

00:32:43.570 --> 00:32:43.580 align:start position:0%
recognition software and if you think
 

00:32:43.580 --> 00:32:45.430 align:start position:0%
recognition software and if you think
about<00:32:43.670><c> it</c><00:32:43.970><c> nowadays</c><00:32:44.240><c> this</c><00:32:44.750><c> software</c><00:32:45.230><c> is</c>

00:32:45.430 --> 00:32:45.440 align:start position:0%
about it nowadays this software is
 

00:32:45.440 --> 00:32:47.770 align:start position:0%
about it nowadays this software is
pretty<00:32:45.710><c> much</c><00:32:45.860><c> ubiquitous</c><00:32:46.460><c> from</c><00:32:47.060><c> social</c><00:32:47.750><c> media</c>

00:32:47.770 --> 00:32:47.780 align:start position:0%
pretty much ubiquitous from social media
 

00:32:47.780 --> 00:32:51.790 align:start position:0%
pretty much ubiquitous from social media
to<00:32:48.260><c> security</c><00:32:49.010><c> systems</c><00:32:50.140><c> another</c><00:32:51.140><c> area</c><00:32:51.620><c> that's</c>

00:32:51.790 --> 00:32:51.800 align:start position:0%
to security systems another area that's
 

00:32:51.800 --> 00:32:53.890 align:start position:0%
to security systems another area that's
generated<00:32:52.400><c> generated</c><00:32:53.300><c> a</c><00:32:53.390><c> lot</c><00:32:53.570><c> of</c><00:32:53.600><c> excitement</c>

00:32:53.890 --> 00:32:53.900 align:start position:0%
generated generated a lot of excitement
 

00:32:53.900 --> 00:32:57.190 align:start position:0%
generated generated a lot of excitement
as<00:32:54.650><c> of</c><00:32:54.950><c> late</c><00:32:55.130><c> is</c><00:32:55.370><c> in</c><00:32:55.880><c> autonomous</c><00:32:56.510><c> vehicles</c><00:32:56.990><c> and</c>

00:32:57.190 --> 00:32:57.200 align:start position:0%
as of late is in autonomous vehicles and
 

00:32:57.200 --> 00:32:59.830 align:start position:0%
as of late is in autonomous vehicles and
self-driving<00:32:57.800><c> cars</c><00:32:58.130><c> so</c><00:32:58.940><c> NVIDIA</c><00:32:59.510><c> has</c><00:32:59.570><c> a</c>

00:32:59.830 --> 00:32:59.840 align:start position:0%
self-driving cars so NVIDIA has a
 

00:32:59.840 --> 00:33:02.830 align:start position:0%
self-driving cars so NVIDIA has a
research<00:33:00.440><c> team</c><00:33:00.770><c> working</c><00:33:01.520><c> on</c><00:33:01.850><c> a</c><00:33:01.910><c> CNN</c><00:33:02.540><c> based</c>

00:33:02.830 --> 00:33:02.840 align:start position:0%
research team working on a CNN based
 

00:33:02.840 --> 00:33:04.900 align:start position:0%
research team working on a CNN based
system<00:33:03.350><c> for</c><00:33:03.650><c> end-to-end</c><00:33:04.010><c> control</c><00:33:04.460><c> of</c>

00:33:04.900 --> 00:33:04.910 align:start position:0%
system for end-to-end control of
 

00:33:04.910 --> 00:33:09.250 align:start position:0%
system for end-to-end control of
self-driving<00:33:05.300><c> cars</c><00:33:05.890><c> their</c><00:33:06.890><c> pipeline</c><00:33:08.170><c> feeds</c><00:33:09.170><c> a</c>

00:33:09.250 --> 00:33:09.260 align:start position:0%
self-driving cars their pipeline feeds a
 

00:33:09.260 --> 00:33:11.680 align:start position:0%
self-driving cars their pipeline feeds a
single<00:33:09.650><c> image</c><00:33:10.280><c> from</c><00:33:10.490><c> a</c><00:33:10.610><c> camera</c><00:33:10.940><c> on</c><00:33:11.060><c> the</c><00:33:11.210><c> car</c><00:33:11.390><c> to</c>

00:33:11.680 --> 00:33:11.690 align:start position:0%
single image from a camera on the car to
 

00:33:11.690 --> 00:33:14.710 align:start position:0%
single image from a camera on the car to
a<00:33:11.720><c> CNN</c><00:33:12.050><c> that</c><00:33:12.860><c> directly</c><00:33:13.490><c> outputs</c><00:33:14.060><c> a</c><00:33:14.210><c> single</c>

00:33:14.710 --> 00:33:14.720 align:start position:0%
a CNN that directly outputs a single
 

00:33:14.720 --> 00:33:17.020 align:start position:0%
a CNN that directly outputs a single
number<00:33:15.050><c> which</c><00:33:15.680><c> is</c><00:33:15.950><c> the</c><00:33:16.250><c> steering</c><00:33:16.580><c> is</c><00:33:16.760><c> the</c>

00:33:17.020 --> 00:33:17.030 align:start position:0%
number which is the steering is the
 

00:33:17.030 --> 00:33:19.570 align:start position:0%
number which is the steering is the
predicted<00:33:17.480><c> steering</c><00:33:17.900><c> wheel</c><00:33:18.080><c> angle</c><00:33:18.590><c> and</c><00:33:18.800><c> the</c>

00:33:19.570 --> 00:33:19.580 align:start position:0%
predicted steering wheel angle and the
 

00:33:19.580 --> 00:33:19.869 align:start position:0%
predicted steering wheel angle and the
man

00:33:19.869 --> 00:33:19.879 align:start position:0%
man
 

00:33:19.879 --> 00:33:21.879 align:start position:0%
man
you<00:33:19.999><c> see</c><00:33:20.149><c> in</c><00:33:20.269><c> this</c><00:33:20.389><c> video</c><00:33:20.629><c> is</c><00:33:21.019><c> actually</c><00:33:21.619><c> one</c><00:33:21.859><c> of</c>

00:33:21.879 --> 00:33:21.889 align:start position:0%
you see in this video is actually one of
 

00:33:21.889 --> 00:33:26.349 align:start position:0%
you see in this video is actually one of
our<00:33:22.309><c> guest</c><00:33:22.789><c> lectures</c><00:33:23.259><c> and</c><00:33:24.519><c> on</c><00:33:25.519><c> Thursday</c><00:33:26.089><c> we'll</c>

00:33:26.349 --> 00:33:26.359 align:start position:0%
our guest lectures and on Thursday we'll
 

00:33:26.359 --> 00:33:29.199 align:start position:0%
our guest lectures and on Thursday we'll
hear<00:33:26.690><c> about</c><00:33:26.719><c> how</c><00:33:27.169><c> his</c><00:33:27.619><c> team</c><00:33:27.649><c> is</c><00:33:27.919><c> is</c><00:33:28.459><c> developing</c>

00:33:29.199 --> 00:33:29.209 align:start position:0%
hear about how his team is is developing
 

00:33:29.209 --> 00:33:33.189 align:start position:0%
hear about how his team is is developing
this<00:33:29.389><c> platform</c><00:33:31.239><c> finally</c><00:33:32.239><c> we've</c><00:33:32.569><c> also</c><00:33:32.779><c> seen</c><00:33:33.139><c> a</c>

00:33:33.189 --> 00:33:33.199 align:start position:0%
this platform finally we've also seen a
 

00:33:33.199 --> 00:33:35.439 align:start position:0%
this platform finally we've also seen a
significant<00:33:34.099><c> impact</c><00:33:34.489><c> in</c><00:33:34.669><c> the</c><00:33:34.789><c> medical</c><00:33:35.179><c> field</c>

00:33:35.439 --> 00:33:35.449 align:start position:0%
significant impact in the medical field
 

00:33:35.449 --> 00:33:37.509 align:start position:0%
significant impact in the medical field
where<00:33:35.839><c> deep</c><00:33:36.349><c> learning</c><00:33:36.559><c> models</c><00:33:37.099><c> have</c><00:33:37.369><c> been</c>

00:33:37.509 --> 00:33:37.519 align:start position:0%
where deep learning models have been
 

00:33:37.519 --> 00:33:39.609 align:start position:0%
where deep learning models have been
applied<00:33:37.879><c> to</c><00:33:37.909><c> the</c><00:33:38.269><c> analysis</c><00:33:38.779><c> of</c><00:33:38.809><c> a</c><00:33:39.079><c> whole</c><00:33:39.349><c> host</c>

00:33:39.609 --> 00:33:39.619 align:start position:0%
applied to the analysis of a whole host
 

00:33:39.619 --> 00:33:41.799 align:start position:0%
applied to the analysis of a whole host
of<00:33:39.799><c> different</c><00:33:40.399><c> types</c><00:33:40.819><c> of</c><00:33:40.999><c> medical</c><00:33:41.359><c> images</c>

00:33:41.799 --> 00:33:41.809 align:start position:0%
of different types of medical images
 

00:33:41.809 --> 00:33:44.859 align:start position:0%
of different types of medical images
just<00:33:42.769><c> this</c><00:33:42.919><c> past</c><00:33:43.190><c> year</c><00:33:43.459><c> a</c><00:33:43.699><c> team</c><00:33:44.179><c> from</c><00:33:44.329><c> Stanford</c>

00:33:44.859 --> 00:33:44.869 align:start position:0%
just this past year a team from Stanford
 

00:33:44.869 --> 00:33:47.219 align:start position:0%
just this past year a team from Stanford
developed<00:33:45.409><c> a</c><00:33:45.589><c> CNN</c><00:33:46.309><c> that</c><00:33:46.579><c> could</c><00:33:46.909><c> achieve</c>

00:33:47.219 --> 00:33:47.229 align:start position:0%
developed a CNN that could achieve
 

00:33:47.229 --> 00:33:49.569 align:start position:0%
developed a CNN that could achieve
dermatologists<00:33:48.229><c> level</c><00:33:48.559><c> classification</c><00:33:48.979><c> of</c>

00:33:49.569 --> 00:33:49.579 align:start position:0%
dermatologists level classification of
 

00:33:49.579 --> 00:33:52.930 align:start position:0%
dermatologists level classification of
skin<00:33:49.879><c> lesions</c><00:33:50.289><c> so</c><00:33:51.289><c> you</c><00:33:51.379><c> can</c><00:33:51.709><c> imagine</c><00:33:51.949><c> and</c><00:33:52.429><c> this</c>

00:33:52.930 --> 00:33:52.940 align:start position:0%
skin lesions so you can imagine and this
 

00:33:52.940 --> 00:33:55.059 align:start position:0%
skin lesions so you can imagine and this
is<00:33:53.089><c> what</c><00:33:53.239><c> they</c><00:33:53.359><c> actually</c><00:33:53.509><c> did</c><00:33:53.869><c> having</c><00:33:54.739><c> an</c><00:33:54.859><c> app</c>

00:33:55.059 --> 00:33:55.069 align:start position:0%
is what they actually did having an app
 

00:33:55.069 --> 00:33:57.269 align:start position:0%
is what they actually did having an app
on<00:33:55.309><c> your</c><00:33:55.369><c> phone</c><00:33:55.759><c> where</c><00:33:56.089><c> you</c><00:33:56.119><c> take</c><00:33:56.509><c> a</c><00:33:56.539><c> picture</c>

00:33:57.269 --> 00:33:57.279 align:start position:0%
on your phone where you take a picture
 

00:33:57.279 --> 00:33:59.979 align:start position:0%
on your phone where you take a picture
upload<00:33:58.279><c> that</c><00:33:58.399><c> picture</c><00:33:58.789><c> to</c><00:33:58.940><c> the</c><00:33:59.029><c> app</c><00:33:59.179><c> it's</c><00:33:59.719><c> fed</c>

00:33:59.979 --> 00:33:59.989 align:start position:0%
upload that picture to the app it's fed
 

00:33:59.989 --> 00:34:03.399 align:start position:0%
upload that picture to the app it's fed
into<00:34:00.289><c> a</c><00:34:00.319><c> CNN</c><00:34:00.799><c> that</c><00:34:01.339><c> gen</c><00:34:01.849><c> that</c><00:34:02.419><c> then</c><00:34:02.839><c> generates</c>

00:34:03.399 --> 00:34:03.409 align:start position:0%
into a CNN that gen that then generates
 

00:34:03.409 --> 00:34:05.409 align:start position:0%
into a CNN that gen that then generates
a<00:34:03.559><c> prediction</c><00:34:04.159><c> of</c><00:34:04.279><c> whether</c><00:34:04.879><c> or</c><00:34:05.029><c> not</c><00:34:05.119><c> that</c>

00:34:05.409 --> 00:34:05.419 align:start position:0%
a prediction of whether or not that
 

00:34:05.419 --> 00:34:10.059 align:start position:0%
a prediction of whether or not that
lesion<00:34:05.749><c> is</c><00:34:06.109><c> reason</c><00:34:06.799><c> for</c><00:34:06.829><c> concern</c><00:34:08.589><c> so</c><00:34:09.589><c> to</c>

00:34:10.059 --> 00:34:10.069 align:start position:0%
lesion is reason for concern so to
 

00:34:10.069 --> 00:34:12.039 align:start position:0%
lesion is reason for concern so to
summarize<00:34:10.460><c> what</c><00:34:11.210><c> we've</c><00:34:11.359><c> covered</c><00:34:11.599><c> in</c><00:34:11.869><c> today's</c>

00:34:12.039 --> 00:34:12.049 align:start position:0%
summarize what we've covered in today's
 

00:34:12.049 --> 00:34:15.460 align:start position:0%
summarize what we've covered in today's
lecture<00:34:12.619><c> we</c><00:34:13.099><c> first</c><00:34:13.519><c> consider</c><00:34:13.970><c> the</c><00:34:14.389><c> origins</c><00:34:15.289><c> of</c>

00:34:15.460 --> 00:34:15.470 align:start position:0%
lecture we first consider the origins of
 

00:34:15.470 --> 00:34:17.409 align:start position:0%
lecture we first consider the origins of
the<00:34:15.559><c> computer</c><00:34:15.919><c> vision</c><00:34:16.250><c> problem</c><00:34:16.700><c> how</c><00:34:17.059><c> we</c><00:34:17.119><c> can</c>

00:34:17.409 --> 00:34:17.419 align:start position:0%
the computer vision problem how we can
 

00:34:17.419 --> 00:34:20.230 align:start position:0%
the computer vision problem how we can
represent<00:34:17.629><c> images</c><00:34:18.470><c> as</c><00:34:19.069><c> arrays</c><00:34:19.579><c> of</c><00:34:19.849><c> pixel</c>

00:34:20.230 --> 00:34:20.240 align:start position:0%
represent images as arrays of pixel
 

00:34:20.240 --> 00:34:22.749 align:start position:0%
represent images as arrays of pixel
values<00:34:20.629><c> and</c><00:34:20.899><c> what</c><00:34:21.470><c> convolutions</c><00:34:22.099><c> are</c><00:34:22.369><c> how</c>

00:34:22.749 --> 00:34:22.759 align:start position:0%
values and what convolutions are how
 

00:34:22.759 --> 00:34:25.269 align:start position:0%
values and what convolutions are how
they<00:34:22.970><c> work</c><00:34:23.000><c> we</c><00:34:23.960><c> then</c><00:34:24.200><c> discuss</c><00:34:24.679><c> the</c><00:34:24.889><c> basic</c>

00:34:25.269 --> 00:34:25.279 align:start position:0%
they work we then discuss the basic
 

00:34:25.279 --> 00:34:27.549 align:start position:0%
they work we then discuss the basic
architecture<00:34:25.490><c> of</c><00:34:26.409><c> cnn's</c>

00:34:27.549 --> 00:34:27.559 align:start position:0%
architecture of cnn's
 

00:34:27.559 --> 00:34:30.309 align:start position:0%
architecture of cnn's
and<00:34:27.710><c> finally</c><00:34:28.579><c> we</c><00:34:28.879><c> extended</c><00:34:29.389><c> this</c><00:34:29.539><c> to</c><00:34:29.869><c> consider</c>

00:34:30.309 --> 00:34:30.319 align:start position:0%
and finally we extended this to consider
 

00:34:30.319 --> 00:34:33.089 align:start position:0%
and finally we extended this to consider
some<00:34:30.619><c> different</c><00:34:30.950><c> applications</c><00:34:31.659><c> of</c>

00:34:33.089 --> 00:34:33.099 align:start position:0%
some different applications of
 

00:34:33.099 --> 00:34:36.039 align:start position:0%
some different applications of
convolutional<00:34:34.099><c> neural</c><00:34:34.279><c> networks</c><00:34:34.789><c> and</c><00:34:35.059><c> also</c>

00:34:36.039 --> 00:34:36.049 align:start position:0%
convolutional neural networks and also
 

00:34:36.049 --> 00:34:37.990 align:start position:0%
convolutional neural networks and also
talked<00:34:36.259><c> a</c><00:34:36.379><c> bit</c><00:34:36.529><c> about</c><00:34:36.649><c> how</c><00:34:37.039><c> we</c><00:34:37.099><c> can</c><00:34:37.490><c> visualize</c>

00:34:37.990 --> 00:34:38.000 align:start position:0%
talked a bit about how we can visualize
 

00:34:38.000 --> 00:34:41.470 align:start position:0%
talked a bit about how we can visualize
their<00:34:38.299><c> behavior</c><00:34:38.779><c> so</c><00:34:39.639><c> with</c><00:34:40.639><c> that</c><00:34:40.819><c> I'd</c><00:34:41.089><c> like</c><00:34:41.329><c> to</c>

00:34:41.470 --> 00:34:41.480 align:start position:0%
their behavior so with that I'd like to
 

00:34:41.480 --> 00:34:44.369 align:start position:0%
their behavior so with that I'd like to
conclude<00:34:41.690><c> I'm</c><00:34:42.679><c> happy</c><00:34:43.460><c> to</c><00:34:43.609><c> take</c><00:34:43.819><c> questions</c>

00:34:44.369 --> 00:34:44.379 align:start position:0%
conclude I'm happy to take questions
 

00:34:44.379 --> 00:34:48.249 align:start position:0%
conclude I'm happy to take questions
after<00:34:45.379><c> the</c><00:34:45.889><c> lecture</c><00:34:46.220><c> portion</c><00:34:46.399><c> is</c><00:34:46.730><c> over</c><00:34:47.259><c> it's</c>

00:34:48.249 --> 00:34:48.259 align:start position:0%
after the lecture portion is over it's
 

00:34:48.259 --> 00:34:50.470 align:start position:0%
after the lecture portion is over it's
now<00:34:48.500><c> my</c><00:34:48.740><c> pleasure</c><00:34:48.799><c> to</c><00:34:49.460><c> introduce</c><00:34:49.730><c> our</c><00:34:50.389><c> next</c>

00:34:50.470 --> 00:34:50.480 align:start position:0%
now my pleasure to introduce our next
 

00:34:50.480 --> 00:34:53.829 align:start position:0%
now my pleasure to introduce our next
speaker<00:34:51.289><c> a</c><00:34:51.529><c> special</c><00:34:52.129><c> guest</c><00:34:52.309><c> professor</c><00:34:53.269><c> Aaron</c>

00:34:53.829 --> 00:34:53.839 align:start position:0%
speaker a special guest professor Aaron
 

00:34:53.839 --> 00:34:55.240 align:start position:0%
speaker a special guest professor Aaron
Kerrville<00:34:54.230><c> from</c><00:34:54.500><c> the</c><00:34:54.619><c> University</c><00:34:55.220><c> of</c>

00:34:55.240 --> 00:34:55.250 align:start position:0%
Kerrville from the University of
 

00:34:55.250 --> 00:34:56.039 align:start position:0%
Kerrville from the University of
Montreal

00:34:56.039 --> 00:34:56.049 align:start position:0%
Montreal
 

00:34:56.049 --> 00:34:58.299 align:start position:0%
Montreal
professor<00:34:57.049><c> Kerrville</c><00:34:57.680><c> is</c><00:34:57.799><c> one</c><00:34:57.980><c> of</c><00:34:58.130><c> the</c>

00:34:58.299 --> 00:34:58.309 align:start position:0%
professor Kerrville is one of the
 

00:34:58.309 --> 00:35:00.700 align:start position:0%
professor Kerrville is one of the
creators<00:34:58.789><c> of</c><00:34:59.150><c> generative</c><00:34:59.750><c> adversarial</c>

00:35:00.700 --> 00:35:00.710 align:start position:0%
creators of generative adversarial
 

00:35:00.710 --> 00:35:02.470 align:start position:0%
creators of generative adversarial
networks<00:35:01.069><c> and</c><00:35:01.430><c> he'll</c><00:35:01.759><c> be</c><00:35:01.910><c> talking</c><00:35:02.089><c> to</c><00:35:02.329><c> us</c>

00:35:02.470 --> 00:35:02.480 align:start position:0%
networks and he'll be talking to us
 

00:35:02.480 --> 00:35:04.990 align:start position:0%
networks and he'll be talking to us
about<00:35:02.779><c> deep</c><00:35:03.500><c> generative</c><00:35:04.039><c> models</c><00:35:04.430><c> and</c><00:35:04.670><c> their</c>

00:35:04.990 --> 00:35:05.000 align:start position:0%
about deep generative models and their
 

00:35:05.000 --> 00:35:07.150 align:start position:0%
about deep generative models and their
applications<00:35:05.630><c> so</c><00:35:05.900><c> please</c><00:35:06.710><c> join</c><00:35:07.009><c> me</c><00:35:07.130><c> in</c>

00:35:07.150 --> 00:35:07.160 align:start position:0%
applications so please join me in
 

00:35:07.160 --> 00:35:07.950 align:start position:0%
applications so please join me in
welcoming<00:35:07.339><c> him</c>

00:35:07.950 --> 00:35:07.960 align:start position:0%
welcoming him
 

00:35:07.960 --> 00:35:11.340 align:start position:0%
welcoming him
[Applause]

