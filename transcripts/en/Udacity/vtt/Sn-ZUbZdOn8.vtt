WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.000
Let's look at a very simple piece of code that implements this planning algorithm.

00:00:04.000 --> 00:00:09.000
We have a grid here as before with 0s and 1s. You're familiar with it.

00:00:09.000 --> 00:00:13.000
The start location on the top left, the goal location on the bottom right.

00:00:13.000 --> 00:00:19.000
We can set up arbitrary obstacles like a wall over here and a wall over here

00:00:19.000 --> 00:00:23.000
that forces the robot into kind of an S-curve around the corner.

00:00:23.000 --> 00:00:29.000
If I run this code with this table, what I get is a table that looks like this.

00:00:29.000 --> 00:00:33.000
This for each of the states that is a non-wall state.

00:00:33.000 --> 00:00:35.000
It tells me what's the optimal thing to do.

00:00:35.000 --> 00:00:40.000
So over here it says go south. Here is says go right. Here is says go up.

00:00:40.000 --> 00:00:43.000
Here is says go right again and go south.

00:00:43.000 --> 00:00:46.000
Realize that even states I'm not likely to every reach,

00:00:46.000 --> 00:00:49.000
like the one over here and the on over here, have an optimal policy

00:00:49.000 --> 00:00:53.000
and action associated, because there is really no start state.

00:00:53.000 --> 00:00:55.000
There is just a goal state over here.

00:00:55.000 --> 00:01:00.000
The specification of the initial state has no bearing on this result.

00:01:00.000 --> 00:01:04.000
How can we compute this efficiently?

00:01:04.000 --> 00:01:09.000
Let me make a simple example of a world like this with an obstacle over here.

00:01:09.000 --> 00:01:12.000
Say our goal state is the one in the corner over here.

00:01:12.000 --> 00:01:15.000
Rather than telling you how to compute the optimal policy,

00:01:15.000 --> 00:01:18.000
which assigns an action to each of these cells,

00:01:18.000 --> 00:01:20.000
let me instead teach you about "value."

00:01:20.000 --> 00:01:28.000
A "value function" associates to each grid cell the length of the shortest path to the goal.

00:01:28.000 --> 00:01:31.000
For the goal, obviously, it is 0.

00:01:31.000 --> 00:01:34.000
For each adjacent cell to the goal, it's obviously 1.

00:01:34.000 --> 00:01:44.000
For the guys over here, 2, 3, 4, 5, 6, and 7.

00:01:44.000 --> 00:01:51.000
This is recursively calculated by taking the optimal neighbor x-prime, y-prime,

00:01:51.000 --> 00:01:56.000
considering its value, and by adding the costs it takes to get there,

00:01:56.000 --> 00:01:59.000
which in our example will be plus 1.

00:01:59.000 --> 00:02:02.000
By applying this update equation recursively,

00:02:02.000 --> 00:02:04.000
we can attain this value function over here.

00:02:04.000 --> 00:02:08.000
Once we have this value function, we find that the optimal control action

00:02:08.000 --> 00:02:15.000
is obtained by minimizing the value, which is a hill-climbing type of action.

00:02:15.000 --> 00:02:18.000
Let me give you a quiz.

00:02:18.000 --> 00:02:22.000
In this world here with the goal location over here,

00:02:22.000 --> 09:59:59.000
I'd like to understand what is the value of the cell in the bottom right.

