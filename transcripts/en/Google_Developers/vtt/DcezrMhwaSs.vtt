WEBVTT
Kind: captions
Language: en

00:00:01.155 --> 00:00:04.110
JACQUELINE FULLER: Well, good
morning, everyone and welcome.

00:00:04.110 --> 00:00:06.670
I'm Jacqueline Fuller
and I lead google.org,

00:00:06.670 --> 00:00:08.370
which is Google's philanthropy.

00:00:08.370 --> 00:00:09.830
And welcome to
today's panel, where

00:00:09.830 --> 00:00:12.330
we're going to be talking to
some of the folks who are doing

00:00:12.330 --> 00:00:14.690
the most innovative
work around the world,

00:00:14.690 --> 00:00:17.724
using technology to take on
the world's biggest challenges.

00:00:17.724 --> 00:00:20.140
Let me start by giving you
just a little bit of background

00:00:20.140 --> 00:00:21.550
on google.org.

00:00:21.550 --> 00:00:25.940
So across Google, we provide
about $100 million each year

00:00:25.940 --> 00:00:29.590
to non-profits and schools and
universities around the world.

00:00:29.590 --> 00:00:32.770
We also have our Googlers
volunteering more than about

00:00:32.770 --> 00:00:36.720
100,000 hours and we give
about a billion dollars

00:00:36.720 --> 00:00:40.070
worth of free products
to non-profits.

00:00:40.070 --> 00:00:41.830
Two quick examples
of some of the things

00:00:41.830 --> 00:00:45.180
we're working on-- one that
we launched here at I/O

00:00:45.180 --> 00:00:48.580
is a global effort
to use technology

00:00:48.580 --> 00:00:50.360
to help people living
with disabilities.

00:00:50.360 --> 00:00:54.450
So around the world, there's
more than a billion people

00:00:54.450 --> 00:00:56.130
who are living with
either a cognitive

00:00:56.130 --> 00:00:58.030
or a physical disability.

00:00:58.030 --> 00:01:00.720
And so we launched a
$20 million open call.

00:01:00.720 --> 00:01:05.019
So if you have a technology-- an
idea for how to use technology

00:01:05.019 --> 00:01:07.640
to help people with disability,
we'd love to hear from you.

00:01:07.640 --> 00:01:11.670
And out in the sandbox, we have
some of our current partners

00:01:11.670 --> 00:01:15.570
who are doing things like
3D printing of prosthetics

00:01:15.570 --> 00:01:20.330
or haptic vests that can help
the deaf community potentially

00:01:20.330 --> 00:01:24.020
hear through sensory input.

00:01:24.020 --> 00:01:26.250
Another-- and if you want
to talk more about that,

00:01:26.250 --> 00:01:27.265
Raquel Romano is here.

00:01:27.265 --> 00:01:28.390
She's a Google [INAUDIBLE].

00:01:28.390 --> 00:01:30.240
Happy to talk to
you more about that.

00:01:30.240 --> 00:01:33.210
Raquel's raising her hand.

00:01:33.210 --> 00:01:35.170
Another example of an
effort google.org's

00:01:35.170 --> 00:01:37.400
involved in is we
want to see more women

00:01:37.400 --> 00:01:39.636
and underrepresented
minorities in computer science.

00:01:39.636 --> 00:01:41.260
So we've invested
more than $50 million

00:01:41.260 --> 00:01:43.870
in partners who are helping
to make that happen.

00:01:43.870 --> 00:01:46.450
And if you want to talk
to us more about that,

00:01:46.450 --> 00:01:48.700
Ammaji Miller is here
and can talk to you.

00:01:48.700 --> 00:01:49.200
All right.

00:01:49.200 --> 00:01:51.260
So let's go to our panel now.

00:01:51.260 --> 00:01:53.730
Thanks so much for joining
us here this morning.

00:01:53.730 --> 00:01:55.480
So what I'm going
to do is I'm going

00:01:55.480 --> 00:01:58.480
to ask each of our panelists
to just briefly introduce

00:01:58.480 --> 00:02:02.520
yourself and tell us the
problem that you're working on,

00:02:02.520 --> 00:02:06.560
your big idea to take that
on, and how you're going

00:02:06.560 --> 00:02:08.100
to know if you're successful.

00:02:08.100 --> 00:02:10.130
And Jen-- if you could
start, Jen Pahlka,

00:02:10.130 --> 00:02:13.540
just giving us also a sentence
or two about yourself, who you

00:02:13.540 --> 00:02:14.040
are.

00:02:14.040 --> 00:02:14.873
JENNIFER PAHLKA: OK.

00:02:14.873 --> 00:02:15.570
Hi.

00:02:15.570 --> 00:02:16.410
I'm Jennifer Pahlka.

00:02:16.410 --> 00:02:20.532
I'm the founder and executive
director of Code for America.

00:02:20.532 --> 00:02:22.365
I just-- well, it's not
that recent anymore,

00:02:22.365 --> 00:02:23.906
but I've been saying
that recently, I

00:02:23.906 --> 00:02:26.050
came back from a year
stint in the White House.

00:02:26.050 --> 00:02:27.800
So I've had some
experience in government,

00:02:27.800 --> 00:02:28.960
which was relevant to--

00:02:28.960 --> 00:02:30.520
JACQUELINE FULLER:
Very relevant--

00:02:30.520 --> 00:02:32.436
JENNIFER PAHLKA: The big
idea, the big problem

00:02:32.436 --> 00:02:34.572
that we're trying to
solve at Code for America.

00:02:34.572 --> 00:02:36.030
The way I like to
talk about it now

00:02:36.030 --> 00:02:42.290
is that if government can't do
digital well, it can't govern.

00:02:42.290 --> 00:02:44.540
And we've seen some
big examples of that

00:02:44.540 --> 00:02:47.630
in the past year and a half,
but there are many, many more

00:02:47.630 --> 00:02:49.910
examples that you don't see.

00:02:49.910 --> 00:02:52.070
And we see them and we'll
talk about that, I think,

00:02:52.070 --> 00:02:54.130
as the panel goes on.

00:02:54.130 --> 00:02:56.290
And so our big
idea to fix that is

00:02:56.290 --> 00:03:00.110
to bring practices,
products-- technology

00:03:00.110 --> 00:03:03.190
products-- and people
to the equation that

00:03:03.190 --> 00:03:05.710
have a fundamentally different
approach, an iterative,

00:03:05.710 --> 00:03:08.630
data-driven, and user-centered
approach to solving problems

00:03:08.630 --> 00:03:09.700
with technology.

00:03:09.700 --> 00:03:12.690
So we've been doing that
for about five years now.

00:03:12.690 --> 00:03:16.480
And what we think is if we can
continue to help government

00:03:16.480 --> 00:03:20.130
do government in
the 21st century,

00:03:20.130 --> 00:03:21.880
then the world will
look pretty different,

00:03:21.880 --> 00:03:25.610
because not only will we have
government services that work,

00:03:25.610 --> 00:03:29.060
but I think that is how
fundamentally we build trust

00:03:29.060 --> 00:03:32.470
in this major institution that
we forget how much we really,

00:03:32.470 --> 00:03:35.680
really do rely on and we
need it to work for us.

00:03:35.680 --> 00:03:37.950
So I think getting
government to work

00:03:37.950 --> 00:03:39.820
is how we build that trust back.

00:03:39.820 --> 00:03:40.310
JACQUELINE FULLER:
You're setting

00:03:40.310 --> 00:03:41.510
your metrics pretty high there.

00:03:41.510 --> 00:03:42.325
JENNIFER PAHLKA: A little, yeah.

00:03:42.325 --> 00:03:43.783
JACQUELINE FULLER:
Pretty high bar.

00:03:43.783 --> 00:03:44.800
All right, Nithya.

00:03:44.800 --> 00:03:46.675
NITHYA RAMANATHAN: Hi,
I'm Nithya Ramanathan.

00:03:46.675 --> 00:03:50.446
I'm the co-founder and
president of Nexleaf Analytics.

00:03:50.446 --> 00:03:53.630
And what we noticed
early on was that there

00:03:53.630 --> 00:03:55.255
is a lack of data in
the public sector.

00:03:55.255 --> 00:03:57.421
JACQUELINE FULLER: Can I--
I just want to interrupt.

00:03:57.421 --> 00:03:59.787
Also, start by just giving
them one sentence on who

00:03:59.787 --> 00:04:01.370
you are, like what
your background is,

00:04:01.370 --> 00:04:03.930
how you came to this, because I
think we've got a lot of folks

00:04:03.930 --> 00:04:05.750
who are coming from different
backgrounds who might find that

00:04:05.750 --> 00:04:06.249
interesting.

00:04:06.249 --> 00:04:08.220
NITHYA RAMANATHAN: Yeah,
so I'm a technologist.

00:04:08.220 --> 00:04:09.420
So I came from academia.

00:04:09.420 --> 00:04:12.320
I was a faculty researcher
in computer science.

00:04:12.320 --> 00:04:14.070
And actually, it
was there that we

00:04:14.070 --> 00:04:17.089
started noticing that
there was this lack of data

00:04:17.089 --> 00:04:20.390
in the public sector, plaguing
problems in the public sector,

00:04:20.390 --> 00:04:22.250
particularly in global health.

00:04:22.250 --> 00:04:25.320
And so take vaccines,
for example,

00:04:25.320 --> 00:04:29.770
vaccines are a lifesaving
measure, especially when

00:04:29.770 --> 00:04:31.210
administered to children.

00:04:31.210 --> 00:04:34.160
And yet too often,
at an alarming rate,

00:04:34.160 --> 00:04:36.290
vaccines are deteriorating
and going bad

00:04:36.290 --> 00:04:39.790
because they are kept
too cold or too hot.

00:04:39.790 --> 00:04:42.860
And so for many years,
many smart people

00:04:42.860 --> 00:04:44.330
at the World Health
Organization,

00:04:44.330 --> 00:04:45.790
governments,
ministries of health,

00:04:45.790 --> 00:04:48.330
NGOs, have been
trying to address

00:04:48.330 --> 00:04:50.270
the problems in
the refrigeration

00:04:50.270 --> 00:04:52.070
to keep vaccines cold.

00:04:52.070 --> 00:04:55.520
But nobody had any
data, so nobody really

00:04:55.520 --> 00:04:57.810
knew what was going on,
what were the problems,

00:04:57.810 --> 00:05:00.440
and what were the types
of problems and solutions.

00:05:00.440 --> 00:05:03.390
And so when I left
academia to help start

00:05:03.390 --> 00:05:05.900
Nexleaf Analytics, one of
the problems that we took on

00:05:05.900 --> 00:05:10.540
was developing a sensor
platform that could first

00:05:10.540 --> 00:05:12.660
of all, monitor how
vaccines are stored

00:05:12.660 --> 00:05:15.950
and send out text messages
to nurses so that they could,

00:05:15.950 --> 00:05:19.240
even the most remote areas
in Mozambique, Kenya,

00:05:19.240 --> 00:05:21.700
and a number of countries
around the world,

00:05:21.700 --> 00:05:24.930
could start taking
actions to fix problems.

00:05:24.930 --> 00:05:28.202
But just as importantly
and where we really come in

00:05:28.202 --> 00:05:31.630
is in thinking about the
analytics and the data that

00:05:31.630 --> 00:05:35.380
can be then used across
thousands of sites in a country

00:05:35.380 --> 00:05:37.570
to provide a government
with visibility

00:05:37.570 --> 00:05:39.350
into their weakest links.

00:05:39.350 --> 00:05:41.780
And so where we come
in is that we're

00:05:41.780 --> 00:05:45.130
part of a small but now
growing group of technologists

00:05:45.130 --> 00:05:48.370
and computer scientists who
are not just building a sensor

00:05:48.370 --> 00:05:51.760
and selling it to a country,
but spending a lot of time

00:05:51.760 --> 00:05:54.210
listening to nurses,
listening to governments,

00:05:54.210 --> 00:05:57.350
listening to the WHO and UNICEF
to try to understand what

00:05:57.350 --> 00:06:00.490
are the actual problems that
people are trying to solve,

00:06:00.490 --> 00:06:03.030
and then get the
data to help answer

00:06:03.030 --> 00:06:05.520
those specific questions so
that we're actually building

00:06:05.520 --> 00:06:08.190
technologies that countries
are now wanting and demanding

00:06:08.190 --> 00:06:09.890
and are now using.

00:06:09.890 --> 00:06:12.270
And so what we see
success looking like

00:06:12.270 --> 00:06:15.650
is now refrigerators
around the world

00:06:15.650 --> 00:06:19.050
all instrumented with sensors
that can actually then issue

00:06:19.050 --> 00:06:20.610
a call for help
when a vaccine is

00:06:20.610 --> 00:06:24.860
about to go bad so that no child
has to receive a vaccine that

00:06:24.860 --> 00:06:26.222
is spoiled or ineffective.

00:06:26.222 --> 00:06:27.430
JACQUELINE FULLER: All right.

00:06:27.430 --> 00:06:28.080
Thanks, Nithya.

00:06:28.080 --> 00:06:29.222
Robert.

00:06:29.222 --> 00:06:31.650
ROBERT LEE: Hi, I'm Robert
from Charity Water, director

00:06:31.650 --> 00:06:33.030
of special programs there.

00:06:33.030 --> 00:06:38.700
And the problem-- oh, my
background is pretty varied.

00:06:38.700 --> 00:06:43.350
So by education, I have a
degree in computer science.

00:06:43.350 --> 00:06:46.770
But then, I went into
management consulting,

00:06:46.770 --> 00:06:49.450
then did a whole bunch of other
stuff, entrepreneurial stuff.

00:06:49.450 --> 00:06:52.750
And then I worked for
the State Department

00:06:52.750 --> 00:06:56.350
in the foreign service as a
foreign service specialist

00:06:56.350 --> 00:07:00.850
working in tech
R&amp;D, then got my MBA

00:07:00.850 --> 00:07:05.940
and was coming back to
work in the Bay Area,

00:07:05.940 --> 00:07:08.990
but then got introduced
to this nonprofit and got

00:07:08.990 --> 00:07:12.190
pulled over to New York and
started working on this really

00:07:12.190 --> 00:07:13.590
challenging project.

00:07:13.590 --> 00:07:16.260
And the challenge was
basically to combat

00:07:16.260 --> 00:07:21.020
the hurdles of keeping a
water point sustainable.

00:07:21.020 --> 00:07:23.975
So all across Africa
and South Asia

00:07:23.975 --> 00:07:27.110
and really around the world
where access to clean water

00:07:27.110 --> 00:07:29.570
is difficult, there's
these wells that are built

00:07:29.570 --> 00:07:33.269
and other forms and
they're breaking.

00:07:33.269 --> 00:07:35.310
There's millions of them
broken around the world.

00:07:35.310 --> 00:07:38.690
No one really knows exactly
how many of them are broken,

00:07:38.690 --> 00:07:39.860
but it's a lot.

00:07:39.860 --> 00:07:43.140
And there hasn't really
been a really good way

00:07:43.140 --> 00:07:47.720
to keep them well-maintained
and in working order.

00:07:47.720 --> 00:07:53.040
So we do this by doing
innovative approaches

00:07:53.040 --> 00:07:55.100
to different
maintenance programs.

00:07:55.100 --> 00:07:57.700
But in order for them to
make sense from a cost

00:07:57.700 --> 00:08:02.140
logistical perspective is
we had to design and develop

00:08:02.140 --> 00:08:04.980
a sensor to put in these wells.

00:08:04.980 --> 00:08:07.070
And these sensors
can report back to us

00:08:07.070 --> 00:08:10.650
how much water is flowing
and when it goes down

00:08:10.650 --> 00:08:14.080
and there's no
flow going through.

00:08:14.080 --> 00:08:17.500
So our purpose obviously
is to keep water

00:08:17.500 --> 00:08:20.000
flowing all the time.

00:08:20.000 --> 00:08:21.840
But we're hoping to
do even more than that

00:08:21.840 --> 00:08:24.400
by setting a new precedence
in the sector that

00:08:24.400 --> 00:08:27.330
no longer is it OK to just build
a well and not take care of it.

00:08:27.330 --> 00:08:31.920
We have to commit to
giving clean water ongoing

00:08:31.920 --> 00:08:34.825
beyond that initial moment.

00:08:34.825 --> 00:08:36.200
JACQUELINE FULLER:
Robert, you're

00:08:36.200 --> 00:08:37.741
really trying to
establish a standard

00:08:37.741 --> 00:08:40.460
not just for clean water, but
across the entire development

00:08:40.460 --> 00:08:41.260
sector.

00:08:41.260 --> 00:08:43.426
ROBERT LEE: Right, the
transparency, accountability,

00:08:43.426 --> 00:08:46.340
and the standards.

00:08:46.340 --> 00:08:49.560
JACQUELINE FULLER: And success?

00:08:49.560 --> 00:08:52.860
ROBERT LEE: Success will come
when we get all the data back.

00:08:52.860 --> 00:08:55.670
And I'll go into more
about the challenges

00:08:55.670 --> 00:08:58.810
of what to do with all that
data and how to utilize that

00:08:58.810 --> 00:09:05.850
and how it can continue to keep
providing a lot of benefits,

00:09:05.850 --> 00:09:09.190
and where this year,
we're putting out

00:09:09.190 --> 00:09:13.779
4,000 to 6,000 sensors
worldwide to start.

00:09:13.779 --> 00:09:14.820
JACQUELINE FULLER: Great.

00:09:14.820 --> 00:09:15.485
Rose.

00:09:15.485 --> 00:09:16.600
ROSE BROOME: Hi, everyone.

00:09:16.600 --> 00:09:22.470
My name is Rose Broome and I'm
the founder and CEO of HandUp.

00:09:22.470 --> 00:09:27.330
My background is, like so
many people on this panel,

00:09:27.330 --> 00:09:30.290
very mixed, but I
started in research,

00:09:30.290 --> 00:09:32.310
in behavioral research.

00:09:32.310 --> 00:09:35.780
And then, I ended up working
on the first Obama campaign

00:09:35.780 --> 00:09:38.920
managing data, and
through that experience

00:09:38.920 --> 00:09:42.810
was really inspired by
the power of technology

00:09:42.810 --> 00:09:44.400
to create social change.

00:09:44.400 --> 00:09:47.080
And so I decided to
leave the research path

00:09:47.080 --> 00:09:49.370
and go into startups.

00:09:49.370 --> 00:09:52.410
So I worked for an education
technology startup and then

00:09:52.410 --> 00:09:56.390
a health startup before
I had an experience that

00:09:56.390 --> 00:09:58.210
led me to found HandUp.

00:09:58.210 --> 00:10:01.060
And that experience was I
was walking down the street

00:10:01.060 --> 00:10:03.780
here in San Francisco and
I saw a woman sleeping

00:10:03.780 --> 00:10:04.880
on the sidewalk.

00:10:04.880 --> 00:10:06.390
It was really cold out.

00:10:06.390 --> 00:10:07.720
And I asked myself why.

00:10:07.720 --> 00:10:11.690
Why are people still sleeping
on the street at night?

00:10:11.690 --> 00:10:14.340
And I told my friend about this.

00:10:14.340 --> 00:10:17.500
My friend is an engineer
and we started brainstorming

00:10:17.500 --> 00:10:20.056
and came up with the idea.

00:10:20.056 --> 00:10:21.430
What if you could
donate directly

00:10:21.430 --> 00:10:24.030
to a homeless person
using technology?

00:10:24.030 --> 00:10:29.190
And we started working on that
and then we launched HandUp.

00:10:29.190 --> 00:10:32.830
So you can donate directly
to specific homeless people

00:10:32.830 --> 00:10:34.300
through our website
at handup.org.

00:10:37.570 --> 00:10:39.790
Everyone can see homelessness
here on the streets,

00:10:39.790 --> 00:10:41.710
but you may not know
that across the country,

00:10:41.710 --> 00:10:44.520
3 and 1/2 million Americans
experience homelessness

00:10:44.520 --> 00:10:49.490
each year and 46 million are
living below the poverty line.

00:10:49.490 --> 00:10:53.770
So through donations but also
through human connection,

00:10:53.770 --> 00:10:56.000
actually being able to
send messages to people

00:10:56.000 --> 00:10:59.160
and hear back from them and
hear updates of their life,

00:10:59.160 --> 00:11:02.670
we're trying to help
connect the community.

00:11:02.670 --> 00:11:07.330
And success to us-- we're
one piece of the puzzle.

00:11:07.330 --> 00:11:10.500
Poverty is obviously
a gigantic problem.

00:11:10.500 --> 00:11:13.930
But success to us is
making sure that everyone

00:11:13.930 --> 00:11:17.990
has a place to sleep
at night, food to eat,

00:11:17.990 --> 00:11:23.539
and the opportunity to live
a healthy, meaningful life.

00:11:23.539 --> 00:11:26.080
JACQUELINE FULLER: So let's dive
into one of our first topics

00:11:26.080 --> 00:11:28.140
and that's going to be risk.

00:11:28.140 --> 00:11:31.710
So in the tech industry, we all
know that taking informed risk

00:11:31.710 --> 00:11:32.750
is part of success.

00:11:32.750 --> 00:11:35.510
But as you get into
different cultures, academia

00:11:35.510 --> 00:11:38.590
or the non-profit
sector or government,

00:11:38.590 --> 00:11:40.100
this whole idea,
I think, is just

00:11:40.100 --> 00:11:43.105
really scary and not as common.

00:11:43.105 --> 00:11:46.570
So I just wanted to ask
about both the risks

00:11:46.570 --> 00:11:48.250
that you're encountering
in projects.

00:11:48.250 --> 00:11:51.690
And also a lot of you, as we
hear about your life plan,

00:11:51.690 --> 00:11:54.730
took a lot of personal risk
in terms of making changes

00:11:54.730 --> 00:11:55.870
along your career path.

00:11:55.870 --> 00:11:59.390
So wondering, Nithya, if you
want to start us off and just

00:11:59.390 --> 00:12:02.970
talk about how you handled risk.

00:12:02.970 --> 00:12:03.970
NITHYA RAMANATHAN: Yeah.

00:12:03.970 --> 00:12:08.590
So personally, there was a lot
of risk when I left academia.

00:12:08.590 --> 00:12:11.820
I was in a job in
computer science

00:12:11.820 --> 00:12:17.030
and as a researcher there and
leaving to start a startup

00:12:17.030 --> 00:12:18.580
was kind of that initial risk.

00:12:18.580 --> 00:12:22.030
But I think when
we first started,

00:12:22.030 --> 00:12:23.910
we needed to create a
new model, because we

00:12:23.910 --> 00:12:26.170
were a non-profit
technology startup.

00:12:26.170 --> 00:12:28.700
And six years ago when we
started, not a lot of people

00:12:28.700 --> 00:12:31.110
understood even
those words together.

00:12:31.110 --> 00:12:32.710
And so we really--

00:12:32.710 --> 00:12:34.001
JACQUELINE FULLER: Still don't.

00:12:34.001 --> 00:12:34.541
[LAUGHTER]

00:12:35.850 --> 00:12:37.350
NITHYA RAMANATHAN:
But we really had

00:12:37.350 --> 00:12:42.500
to borrow from aspects of
NGOs, academia, and of course,

00:12:42.500 --> 00:12:44.620
as well, being a
technology startup.

00:12:44.620 --> 00:12:49.450
And as a lot of people that
have seen large-scale growth

00:12:49.450 --> 00:12:51.200
in the private
sector for technology

00:12:51.200 --> 00:12:55.760
know that success comes from
providing a technology that

00:12:55.760 --> 00:12:57.470
makes people's
lives better, that

00:12:57.470 --> 00:13:00.310
integrates into people's lives,
and that has a sustainable

00:13:00.310 --> 00:13:01.190
business model.

00:13:01.190 --> 00:13:03.030
Those are a lot of key elements.

00:13:03.030 --> 00:13:06.360
And for us working exclusively
in developing countries,

00:13:06.360 --> 00:13:07.800
that looks different.

00:13:07.800 --> 00:13:14.080
And so for example, we have
had to take donor funding

00:13:14.080 --> 00:13:17.620
to do our R&amp;D so that we
had the time and place

00:13:17.620 --> 00:13:22.064
to first spend years listening
to countries to really

00:13:22.064 --> 00:13:23.730
understand, what are
the challenges that

00:13:23.730 --> 00:13:25.750
are being faced in
order to get vaccines

00:13:25.750 --> 00:13:27.360
to be delivered safely?

00:13:27.360 --> 00:13:29.880
So spending years
listening to nurses,

00:13:29.880 --> 00:13:32.650
listening to
district supervisors,

00:13:32.650 --> 00:13:36.060
listening at the national
level, to WHO, to UNICEF,

00:13:36.060 --> 00:13:37.610
to all of the
partners who all come

00:13:37.610 --> 00:13:41.830
together to ensure that every
child on earth gets a vaccine.

00:13:41.830 --> 00:13:45.190
And so not only did we have to
really understand the problems,

00:13:45.190 --> 00:13:47.420
but then continuously
iterate-- of course,

00:13:47.420 --> 00:13:50.259
we all talk about
that-- and then one

00:13:50.259 --> 00:13:51.800
of the key elements
that we had to do

00:13:51.800 --> 00:13:54.830
was go back and then
actually evaluate the impact.

00:13:54.830 --> 00:13:57.980
And so for example, in
randomized controlled trials

00:13:57.980 --> 00:14:00.190
in one of the poorest
health systems in the world,

00:14:00.190 --> 00:14:03.940
in Mozambique, we showed
a 10x reduction in damages

00:14:03.940 --> 00:14:04.780
to vaccines.

00:14:04.780 --> 00:14:05.020
JACQUELINE FULLER: Wow.

00:14:05.020 --> 00:14:06.720
NITHYA RAMANATHAN:
And that was critical.

00:14:06.720 --> 00:14:07.770
That's really changed
the conversation.

00:14:07.770 --> 00:14:09.420
JACQUELINE FULLER: And that's
saving both lives and money.

00:14:09.420 --> 00:14:10.860
NITHYA RAMANATHAN: That's right.

00:14:10.860 --> 00:14:13.505
That's right and that's totally
changing the conversation

00:14:13.505 --> 00:14:15.090
at all levels of
the health system.

00:14:15.090 --> 00:14:16.756
And actually, you
bring up a good point,

00:14:16.756 --> 00:14:20.790
because at the nurse level,
they care about saving lives.

00:14:20.790 --> 00:14:22.706
Of course, everybody
cares about saving lives,

00:14:22.706 --> 00:14:25.200
but at the nurse level, that
really resonates with them.

00:14:25.200 --> 00:14:27.617
And then at the national
level, at higher levels,

00:14:27.617 --> 00:14:29.200
they care about
saving lives but money

00:14:29.200 --> 00:14:30.680
has to be part of the equation.

00:14:30.680 --> 00:14:32.096
It's how they're
making decisions.

00:14:32.096 --> 00:14:33.954
And so absolutely,
these types of results--

00:14:33.954 --> 00:14:36.120
JACQUELINE FULLER: Probably
helps with the adoption,

00:14:36.120 --> 00:14:38.340
though, if you can say,
this is going to actually--

00:14:38.340 --> 00:14:40.340
these systems, these
improvements could actually

00:14:40.340 --> 00:14:41.040
pay for themselves.

00:14:41.040 --> 00:14:41.350
NITHYA RAMANATHAN: That's right.

00:14:41.350 --> 00:14:42.990
Of That's right
and it's completely

00:14:42.990 --> 00:14:43.320
changed the conversation.

00:14:43.320 --> 00:14:45.110
JACQUELINE FULLER: Tends to
open up a few more doors.

00:14:45.110 --> 00:14:45.990
NITHYA RAMANATHAN: That's right.

00:14:45.990 --> 00:14:46.680
JACQUELINE FULLER:
Rose, how about you

00:14:46.680 --> 00:14:47.720
on the subject of risk?

00:14:47.720 --> 00:14:50.350
ROSE BROOME: Yeah,
risk-- any time

00:14:50.350 --> 00:14:53.300
that you're trying to tackle
a big problem out there that

00:14:53.300 --> 00:14:55.820
hasn't been solve-- we
don't know how to do it--

00:14:55.820 --> 00:14:57.590
you have to take risks.

00:14:57.590 --> 00:15:00.170
There is no clear path.

00:15:00.170 --> 00:15:03.830
So risk is a very
important ingredient

00:15:03.830 --> 00:15:10.540
in innovation across all
different innovation sectors.

00:15:10.540 --> 00:15:14.030
Personally, a big risk
for me and my co-founder

00:15:14.030 --> 00:15:17.500
was the moment where we
realized our side project was

00:15:17.500 --> 00:15:19.000
getting a lot of traction.

00:15:19.000 --> 00:15:21.490
And we had non-profits
who wanted it,

00:15:21.490 --> 00:15:23.540
the homeless community
was excited about it,

00:15:23.540 --> 00:15:26.470
and we were invited
into an incubator.

00:15:26.470 --> 00:15:30.450
And we had this kind of moment
that every innovator faces

00:15:30.450 --> 00:15:33.820
where it's like, OK, am I going
to take this leap of faith

00:15:33.820 --> 00:15:39.180
and make this my life and
pour everything into this?

00:15:39.180 --> 00:15:41.310
And we decided,
yes, even though we

00:15:41.310 --> 00:15:43.965
didn't know how we were
going to pay our rent here

00:15:43.965 --> 00:15:48.140
in San Francisco and how
it was going to work.

00:15:48.140 --> 00:15:50.000
But we're so glad we did.

00:15:50.000 --> 00:15:52.490
Another risk that I wanted
to share with you guys.

00:15:52.490 --> 00:15:57.590
Along this topic of non-profit
technology organizations

00:15:57.590 --> 00:15:59.840
and how many people
haven't heard of that,

00:15:59.840 --> 00:16:04.354
well we've tried a
different approach here.

00:16:04.354 --> 00:16:06.270
And so I don't know how
many legal buffs there

00:16:06.270 --> 00:16:08.750
are in the room, but we
actually incorporated HandUp

00:16:08.750 --> 00:16:11.890
as a new kind of legal entity
called a "public benefit

00:16:11.890 --> 00:16:13.100
corporation."

00:16:13.100 --> 00:16:15.660
That is a for-profit
that also has

00:16:15.660 --> 00:16:19.920
a social mission embedded into
the foundation of the company.

00:16:19.920 --> 00:16:24.270
And this was a risk for us,
because it's something new.

00:16:24.270 --> 00:16:27.710
And for us, it gave us the
best of both worlds, to give us

00:16:27.710 --> 00:16:29.570
the freedom to
innovate and the power

00:16:29.570 --> 00:16:31.930
to innovate with the
for-profit model,

00:16:31.930 --> 00:16:34.710
but also ensuring that
our values were embedded

00:16:34.710 --> 00:16:38.130
permanently into the company.

00:16:38.130 --> 00:16:41.050
But if you're doing
something new,

00:16:41.050 --> 00:16:44.292
you have to take risks
and that's very important.

00:16:44.292 --> 00:16:45.750
JACQUELINE FULLER:
Well, if there's

00:16:45.750 --> 00:16:47.970
anything that scares people
more than taking risk,

00:16:47.970 --> 00:16:51.240
it's probably failure and
the possibility of failure.

00:16:51.240 --> 00:16:52.847
Now, again, in
the tech industry,

00:16:52.847 --> 00:16:54.930
that's something we're
much more comfortable with.

00:16:54.930 --> 00:16:57.180
In fact, when I did my
last review with our CFO

00:16:57.180 --> 00:17:01.550
at Google about our google.org
portfolio, his feedback to me

00:17:01.550 --> 00:17:04.210
was, because I went
in saying, and we're

00:17:04.210 --> 00:17:07.089
expecting across our portfolio
to have about a 10% failure

00:17:07.089 --> 00:17:07.710
rate.

00:17:07.710 --> 00:17:09.819
And he said, you need
to increase that,

00:17:09.819 --> 00:17:12.210
because if you're not
failing more often,

00:17:12.210 --> 00:17:14.270
then you're not really
funding innovation.

00:17:14.270 --> 00:17:16.055
And philanthropy needs
to be risk capital.

00:17:16.055 --> 00:17:17.680
And we need to be
really-- we're Google

00:17:17.680 --> 00:17:18.808
so we can take those risks.

00:17:18.808 --> 00:17:19.599
We understand that.

00:17:19.599 --> 00:17:23.437
So go ahead and fund the
things that essentially

00:17:23.437 --> 00:17:24.520
might have some more risk.

00:17:24.520 --> 00:17:27.940
But let's look at
this topic of failure.

00:17:27.940 --> 00:17:29.189
JENNIFER PAHLKA: Why me?

00:17:29.189 --> 00:17:30.730
JACQUELINE FULLER:
Jen notices that I

00:17:30.730 --> 00:17:32.597
look to my friend
on my left and say,

00:17:32.597 --> 00:17:34.180
is there anyone who
might want to talk

00:17:34.180 --> 00:17:36.735
about the subject of failure?

00:17:36.735 --> 00:17:38.110
JENNIFER PAHLKA:
Well, let's see,

00:17:38.110 --> 00:17:41.160
maybe because I was in the
White House doing heathcare.gov,

00:17:41.160 --> 00:17:45.980
maybe because we talk
about the need to fail.

00:17:45.980 --> 00:17:49.430
And I think that one of the
things that I firmly believe is

00:17:49.430 --> 00:17:54.410
that the comfort level with
experimentation, which imply--

00:17:54.410 --> 00:17:55.916
you cannot experiment
and not fail.

00:17:55.916 --> 00:17:57.290
Otherwise, you
didn't experiment.

00:17:57.290 --> 00:17:59.270
You just did something
you knew would work.

00:17:59.270 --> 00:18:02.010
That comfort level
with failure is

00:18:02.010 --> 00:18:04.680
why it's so
important that people

00:18:04.680 --> 00:18:07.929
from this community-- what
my old boss, Todd Park,

00:18:07.929 --> 00:18:09.720
used to call "metaphysical
Silicon Valley."

00:18:09.720 --> 00:18:11.097
You may live
anywhere, but you're

00:18:11.097 --> 00:18:12.680
part of a community
that's comfortable

00:18:12.680 --> 00:18:15.890
with it-- must come
to bring their skills

00:18:15.890 --> 00:18:20.010
and your mindset to bear on
the problems that all of us

00:18:20.010 --> 00:18:22.790
are trying to solve.

00:18:22.790 --> 00:18:26.050
So government is sort of
famously failure-adverse.

00:18:26.050 --> 00:18:29.132
And strangely, that
aversion results in,

00:18:29.132 --> 00:18:31.590
at least at the federal level--
and this is just one study.

00:18:31.590 --> 00:18:37.974
They think that 94% of
federal IT projects fail.

00:18:37.974 --> 00:18:38.890
And it's interesting--

00:18:38.890 --> 00:18:39.800
JACQUELINE FULLER: Is that 94?

00:18:39.800 --> 00:18:40.070
JENNIFER PAHLKA: 94.

00:18:40.070 --> 00:18:41.820
And it's interesting
the definition of it.

00:18:41.820 --> 00:18:45.330
So you can say, well,
it never gets delivered,

00:18:45.330 --> 00:18:47.940
gets delivered very, very
late or significantly late

00:18:47.940 --> 00:18:49.487
or significantly overbudget.

00:18:49.487 --> 00:18:50.945
So you could say,
OK, some of those

00:18:50.945 --> 00:18:53.069
are just late or overbudget
and lots of things are.

00:18:53.069 --> 00:18:57.720
But overbudget has a
different implication

00:18:57.720 --> 00:19:00.860
or a different sort of scale at
the federal level than we do.

00:19:00.860 --> 00:19:01.680
JACQUELINE FULLER:
Way more zeroes?

00:19:01.680 --> 00:19:02.596
JENNIFER PAHLKA: Yeah.

00:19:02.596 --> 00:19:05.760
So healthcare.gov--
before October 1,

00:19:05.760 --> 00:19:09.420
they sort of pegged it
at about $600 million.

00:19:09.420 --> 00:19:12.080
But it was really
more than that.

00:19:12.080 --> 00:19:15.610
There was this piece of
the California court system

00:19:15.610 --> 00:19:18.710
here for years that was
famous for spending $2 billion

00:19:18.710 --> 00:19:25.250
to do what I think was trying to
basically replicate Google Docs

00:19:25.250 --> 00:19:27.490
and then was scratched.

00:19:27.490 --> 00:19:31.120
And so in the interest
of reducing failures,

00:19:31.120 --> 00:19:33.520
we end up with a very,
very high failure rate.

00:19:33.520 --> 00:19:36.060
And the line that explains
that to me the most is

00:19:36.060 --> 00:19:38.740
what Clay Shirky said after
the failure of healthcare.gov

00:19:38.740 --> 00:19:41.240
after he and I'd spent some
time talking about what we think

00:19:41.240 --> 00:19:41.740
went wrong.

00:19:41.740 --> 00:19:43.480
And he said, "The
waterfall method

00:19:43.480 --> 00:19:45.560
amounts to a pledge
by all parties not

00:19:45.560 --> 00:19:48.020
to learn anything
while doing the work."

00:19:48.020 --> 00:19:51.630
And it's this extremely
front-loaded planning process.

00:19:51.630 --> 00:19:54.930
I'm a fan of planning, but
you can take it too far.

00:19:54.930 --> 00:20:00.060
We really find that what we
bring to the local governments

00:20:00.060 --> 00:20:02.281
that we work with,
it's not just hey,

00:20:02.281 --> 00:20:03.780
you need to be on
a different stack.

00:20:03.780 --> 00:20:06.660
Hey, you need to do things
in a more lightweight way.

00:20:06.660 --> 00:20:10.200
But we are going to show you
that we are comfortable trying

00:20:10.200 --> 00:20:11.910
some things that don't work.

00:20:11.910 --> 00:20:16.600
So in the realm of folks who
need assistance in our lives--

00:20:16.600 --> 00:20:18.570
we worked with the
City of San Francisco

00:20:18.570 --> 00:20:23.730
a couple years ago on a
problem with their food stamps,

00:20:23.730 --> 00:20:26.860
so supplemental nutrition
assistance benefits,

00:20:26.860 --> 00:20:29.390
food stamps, EBT cards.

00:20:29.390 --> 00:20:30.770
You can sign up
for this program,

00:20:30.770 --> 00:20:31.920
but it's very,
very likely you'll

00:20:31.920 --> 00:20:33.044
get kicked right off of it.

00:20:33.044 --> 00:20:34.920
The churn in the program
is very, very high

00:20:34.920 --> 00:20:37.160
because of all the very
crazy administrative things

00:20:37.160 --> 00:20:39.610
that have to happen for
you to stay on the program.

00:20:39.610 --> 00:20:41.990
And they wanted us to
look at tech solutions

00:20:41.990 --> 00:20:43.370
for solving that problem.

00:20:43.370 --> 00:20:45.940
Really, it's a communications
problem, not a tech problem.

00:20:45.940 --> 00:20:48.620
And the first things that
they did to solve this problem

00:20:48.620 --> 00:20:50.885
were to create websites
and get information

00:20:50.885 --> 00:20:52.010
to people at the same time.

00:20:52.010 --> 00:20:54.040
And they tried about
four different things

00:20:54.040 --> 00:20:56.000
that just didn't work.

00:20:56.000 --> 00:21:00.540
The good thing is-- and this
is critical to failure--

00:21:00.540 --> 00:21:02.700
is that they knew
that they didn't work.

00:21:02.700 --> 00:21:06.180
And I think we have all
experienced this culture where

00:21:06.180 --> 00:21:08.330
it's great that
you haven't failed,

00:21:08.330 --> 00:21:11.337
because there's no data to show
whether you've failed or not.

00:21:11.337 --> 00:21:12.920
But if you have that
data and you say,

00:21:12.920 --> 00:21:15.050
now, I'm going to move on
to the next thing, that

00:21:15.050 --> 00:21:16.800
gives you the power to iterate.

00:21:16.800 --> 00:21:19.070
You have to have
something to iterate on.

00:21:19.070 --> 00:21:21.270
Over time, that team
found out that they

00:21:21.270 --> 00:21:26.800
could collect phone numbers from
the recipients, text message

00:21:26.800 --> 00:21:29.820
these folks when they were
about to fall off the rolls,

00:21:29.820 --> 00:21:32.810
and then get them to call the
office, which I know sounds

00:21:32.810 --> 00:21:35.537
like the least sexy tech
project in the universe.

00:21:35.537 --> 00:21:36.870
JACQUELINE FULLER: But it works.

00:21:36.870 --> 00:21:38.710
JENNIFER PAHLKA:
It works and if you

00:21:38.710 --> 00:21:41.810
want to get a sense
of the kind of work

00:21:41.810 --> 00:21:44.364
that the Fellows are doing,
because they're continuing

00:21:44.364 --> 00:21:45.780
to iterate on this
problem now two

00:21:45.780 --> 00:21:47.920
and three years later,
I guarantee you,

00:21:47.920 --> 00:21:50.710
you will be amazed at the
kind of work that they do.

00:21:50.710 --> 00:21:53.120
The technology isn't
as hard, but the work

00:21:53.120 --> 00:21:54.920
is really, really
fascinating, because they

00:21:54.920 --> 00:21:57.840
know every day that they're
helping people go home

00:21:57.840 --> 00:21:59.940
from the grocery
store with groceries

00:21:59.940 --> 00:22:01.230
and feed hungry families.

00:22:01.230 --> 00:22:03.262
And that is really,
really powerful.

00:22:03.262 --> 00:22:04.470
JACQUELINE FULLER: All right.

00:22:04.470 --> 00:22:06.975
So Robert, I know we've had
many, many conversations

00:22:06.975 --> 00:22:08.640
about--

00:22:08.640 --> 00:22:11.940
ROBERT LEE: Oh, failure,
too many, too many.

00:22:11.940 --> 00:22:16.800
Yeah, I think-- so I'll give an
example of one of the failures

00:22:16.800 --> 00:22:17.960
that we went through.

00:22:20.600 --> 00:22:22.770
So we tested in the lab.

00:22:22.770 --> 00:22:23.530
We coded it.

00:22:23.530 --> 00:22:24.590
We built the hardware.

00:22:24.590 --> 00:22:26.470
We're like, OK, this seems good.

00:22:26.470 --> 00:22:27.630
Everything seems working.

00:22:27.630 --> 00:22:30.200
Let's take it out
into the field.

00:22:30.200 --> 00:22:35.390
We take it out to Ethiopia
and install 30 or 40 or so

00:22:35.390 --> 00:22:39.450
and none of them
are communicating.

00:22:39.450 --> 00:22:41.960
Or actually, I think only
one of them communicated,

00:22:41.960 --> 00:22:42.937
one out of 30.

00:22:42.937 --> 00:22:44.520
And we're like, wait,
what's going on?

00:22:44.520 --> 00:22:45.940
So we looked back
at our firmware.

00:22:45.940 --> 00:22:48.481
We looked back at everything
and we're like, what's going on?

00:22:48.481 --> 00:22:49.800
Everything seems to be working.

00:22:49.800 --> 00:22:51.740
And we find out
that it's actually

00:22:51.740 --> 00:22:55.080
a problem with the
Ethiopian telecom network

00:22:55.080 --> 00:22:58.480
and how they've configured
their backbone system.

00:22:58.480 --> 00:23:02.310
So then, it led to
these conference calls

00:23:02.310 --> 00:23:07.250
with our firmware guys in San
Francisco, me in New York,

00:23:07.250 --> 00:23:09.580
Ethio Telecom, their
background network

00:23:09.580 --> 00:23:14.100
in Italy with Italia
Sparkle, and our GSM chip

00:23:14.100 --> 00:23:16.535
supplier in Hong Kong.

00:23:16.535 --> 00:23:18.410
JACQUELINE FULLER: Those
are fun to schedule.

00:23:18.410 --> 00:23:20.540
[LAUGHTER]

00:23:20.540 --> 00:23:23.600
ROBERT LEE: Yeah, I think
it was 3:00 AM for me.

00:23:23.600 --> 00:23:27.430
So we're doing these calls
and we just can't get it done.

00:23:27.430 --> 00:23:29.370
So finally, it's like, OK.

00:23:29.370 --> 00:23:32.830
So we have the Hong Kong telecom
expert fly out to Ethiopia.

00:23:32.830 --> 00:23:36.300
We had to work with the politics
of the Ethiopian government,

00:23:36.300 --> 00:23:39.330
because it's a
state-owned enterprise

00:23:39.330 --> 00:23:41.740
to get them permissions
to get on to their system.

00:23:41.740 --> 00:23:44.990
It gets on their system,
starts reconfiguring it,

00:23:44.990 --> 00:23:46.650
gets our sensor transmitting.

00:23:46.650 --> 00:23:47.860
And then, while he's
there, they're like,

00:23:47.860 --> 00:23:49.693
oh, well, we also have
all these other bugs.

00:23:49.693 --> 00:23:51.126
Can you fix these, too?

00:23:51.126 --> 00:23:53.250
So he starts fixing those
bugs and all of a sudden,

00:23:53.250 --> 00:23:55.380
I'm getting text messages
for the first time

00:23:55.380 --> 00:23:57.340
on my AT&amp;T phone in Ethiopia.

00:23:57.340 --> 00:23:57.964
I'm like, yeah!

00:24:00.560 --> 00:24:06.200
But it's hard because you don't
expect some of these problems

00:24:06.200 --> 00:24:07.250
that you have.

00:24:07.250 --> 00:24:10.710
And then, some things
seem out of your control

00:24:10.710 --> 00:24:12.530
but you just kind of
have to go for it.

00:24:12.530 --> 00:24:17.590
And so that's been--
communication in Ethiopia

00:24:17.590 --> 00:24:20.450
is difficult, let
alone most of Africa,

00:24:20.450 --> 00:24:22.670
where we want to
work in rural areas.

00:24:22.670 --> 00:24:24.860
So now, we know like,
OK, we need to get ahead.

00:24:24.860 --> 00:24:26.520
So we got right on Malawi.

00:24:26.520 --> 00:24:30.020
We got on Uganda, Kenya,
Liberia, to make sure,

00:24:30.020 --> 00:24:32.810
because with Ethiopia, the
contracts were all in place

00:24:32.810 --> 00:24:37.640
and they actually had a whole
list of roaming M2M data

00:24:37.640 --> 00:24:39.490
contracts in place,
but none of them

00:24:39.490 --> 00:24:41.459
worked for the same reasons.

00:24:41.459 --> 00:24:44.000
But we were the only ones that
actually had devices out there

00:24:44.000 --> 00:24:46.350
for the first time, so
no one said anything.

00:24:46.350 --> 00:24:48.807
And so we got this fixed.

00:24:48.807 --> 00:24:49.890
We started moving forward.

00:24:49.890 --> 00:24:53.760
I actually just went
to Ethiopia last week

00:24:53.760 --> 00:24:57.760
to put out another-- we have
100 more sensors that we

00:24:57.760 --> 00:25:01.370
are configuring to test the
next iteration of the firmware.

00:25:01.370 --> 00:25:06.354
So then, I do that and
then again, one out of 30

00:25:06.354 --> 00:25:07.020
is transmitting.

00:25:07.020 --> 00:25:08.940
So I'm like, wait,
what's going on?

00:25:08.940 --> 00:25:10.830
And I talk to Ethio
Telecom and find out

00:25:10.830 --> 00:25:13.260
that they just upgraded
all their hardware

00:25:13.260 --> 00:25:14.560
to get ready for 4G.

00:25:14.560 --> 00:25:16.470
And then, they put on
the old configuration

00:25:16.470 --> 00:25:20.265
before we fixed it, so now
I have to get the Hong Kong

00:25:20.265 --> 00:25:21.390
guy to fly back out there--

00:25:21.390 --> 00:25:23.848
JACQUELINE FULLER: You've got
to make the call again, too--

00:25:23.848 --> 00:25:26.720
ROBERT LEE: And reconfigure
their system one more time.

00:25:26.720 --> 00:25:29.090
Yeah, and that's
just one example

00:25:29.090 --> 00:25:32.290
of the many ways,
the many hurdles,

00:25:32.290 --> 00:25:33.370
the unexpected hurdles.

00:25:33.370 --> 00:25:34.930
And I always tell
people-- and I'm

00:25:34.930 --> 00:25:37.970
sure there's a lot of product
managers here and developers.

00:25:37.970 --> 00:25:40.490
And I always say,
well, you spend

00:25:40.490 --> 00:25:45.380
10% of your time preparing for
90% of what's going to happen,

00:25:45.380 --> 00:25:45.990
likely case.

00:25:45.990 --> 00:25:49.460
And then, it's 90% of your
time planning for all the

00:25:49.460 --> 00:25:52.434
10% of things that
can go wrong, right?

00:25:52.434 --> 00:25:54.600
But you have to, because
you have to make this work.

00:25:54.600 --> 00:25:57.790
It has to be near
perfect, especially

00:25:57.790 --> 00:26:00.070
in this kind of environment.

00:26:00.070 --> 00:26:02.770
JACQUELINE FULLER: Well, this
is offering kind of a glimpse.

00:26:02.770 --> 00:26:05.220
Sometimes, I get from
people like, oh, yeah,

00:26:05.220 --> 00:26:08.460
I'm thinking about-- I'd like to
use my tech background to help

00:26:08.460 --> 00:26:09.720
the world to do better.

00:26:09.720 --> 00:26:11.980
But I'll do that when I'm
ready for retirement or I'm

00:26:11.980 --> 00:26:13.750
slowing down or I
want something easier.

00:26:13.750 --> 00:26:16.810
And there's, I think, sometimes
this sort of subtle notion

00:26:16.810 --> 00:26:19.070
that, oh, because you're
helping people or doing good,

00:26:19.070 --> 00:26:23.000
it'll not be as challenging
as some of the products

00:26:23.000 --> 00:26:24.000
that they're working on.

00:26:24.000 --> 00:26:26.530
So maybe if you guys
are willing to share,

00:26:26.530 --> 00:26:27.930
what's the most
challenging thing

00:26:27.930 --> 00:26:29.940
that you're coming across?

00:26:29.940 --> 00:26:30.690
Rose, do you have?

00:26:30.690 --> 00:26:34.180
ROSE BROOME: Yeah, I would
say so many technologists want

00:26:34.180 --> 00:26:37.850
to work on very hard
technical challenges.

00:26:37.850 --> 00:26:40.360
And when you think about
it, the biggest problems

00:26:40.360 --> 00:26:45.230
that we face as a civilization
are poverty, energy,

00:26:45.230 --> 00:26:47.390
environment, governance.

00:26:47.390 --> 00:26:51.230
And if you can apply
that same mindset

00:26:51.230 --> 00:26:53.820
to solving some of the
world's largest challenges,

00:26:53.820 --> 00:26:55.710
you can have a very
fulfilling career.

00:26:55.710 --> 00:26:58.050
And we're just at the
beginning of really seeing

00:26:58.050 --> 00:27:01.780
technology, whatever phrase
you like, disrupt, augment,

00:27:01.780 --> 00:27:05.010
support these industries.

00:27:05.010 --> 00:27:09.340
And the water is very warm
and I encourage all of you

00:27:09.340 --> 00:27:10.299
to jump in and join in.

00:27:10.299 --> 00:27:12.589
JACQUELINE FULLER: Come on,
the water's warm, she says.

00:27:12.589 --> 00:27:13.180
All right.

00:27:13.180 --> 00:27:15.770
Nithya, do you have maybe a
specific challenge you want?

00:27:15.770 --> 00:27:17.020
NITHYA RAMANATHAN: Yeah, sure.

00:27:17.020 --> 00:27:18.930
So one of the
challenges that we face

00:27:18.930 --> 00:27:22.900
is being able to rapidly iterate
and troubleshoot remotely.

00:27:22.900 --> 00:27:26.440
So a little bit of what
Robert's talking about, but also

00:27:26.440 --> 00:27:28.760
kind of-- take refrigerators.

00:27:28.760 --> 00:27:31.820
These things are designed
with lots of money in the R&amp;D.

00:27:31.820 --> 00:27:33.910
They're tested in
very high-tech labs.

00:27:33.910 --> 00:27:36.580
They get to remote
environments, lack of power,

00:27:36.580 --> 00:27:38.490
lack of a lot of
other resources,

00:27:38.490 --> 00:27:40.260
and they fail immediately.

00:27:40.260 --> 00:27:42.180
So similarly with
remote sensors,

00:27:42.180 --> 00:27:44.990
it's sort of this issue
of being able to test

00:27:44.990 --> 00:27:46.980
as much as we can but
then be able to remotely

00:27:46.980 --> 00:27:48.570
diagnose and
troubleshoot, as well.

00:27:48.570 --> 00:27:51.940
And so trying to kind of
figure out that balance

00:27:51.940 --> 00:27:55.410
and set up the environments
that we're still

00:27:55.410 --> 00:27:58.480
able to kind of test as much as
we can rigorously in the lab,

00:27:58.480 --> 00:28:00.820
but then also be able
to put in the components

00:28:00.820 --> 00:28:03.170
that we can remotely
sufficiently instrument

00:28:03.170 --> 00:28:06.660
our systems that we can remotely
troubleshoot and diagnose.

00:28:06.660 --> 00:28:09.820
And then, I think finally
just dealing with the data.

00:28:09.820 --> 00:28:12.180
We're going to be getting
data from tens of thousands

00:28:12.180 --> 00:28:14.490
of sites in the next year or so.

00:28:14.490 --> 00:28:18.200
And so just kind of thinking
about not necessarily

00:28:18.200 --> 00:28:21.070
the actual volume of data--
not a lot of data, especially

00:28:21.070 --> 00:28:23.060
being at Google-- but
just really thinking

00:28:23.060 --> 00:28:24.250
about meaningful analytics.

00:28:24.250 --> 00:28:26.760
What is really going to drive
change action in government

00:28:26.760 --> 00:28:28.710
so that ultimately, we
can save more vaccines,

00:28:28.710 --> 00:28:31.280
get more vaccines to kids?

00:28:31.280 --> 00:28:34.085
One of those ongoing,
interesting challenges for us.

00:28:34.085 --> 00:28:35.355
JENNIFER PAHLKA: Can I speak
to the challenge thing?

00:28:35.355 --> 00:28:36.710
JACQUELINE FULLER: Yeah, please.

00:28:36.710 --> 00:28:38.990
JENNIFER PAHLKA: Just you
mentioned this perception

00:28:38.990 --> 00:28:40.840
that doing good
sometimes is like,

00:28:40.840 --> 00:28:44.030
oh, maybe a little bit of
an easier job than say,

00:28:44.030 --> 00:28:46.440
serving the millions and
millions of people that

00:28:46.440 --> 00:28:48.622
would go to Google every day.

00:28:48.622 --> 00:28:50.080
Just two quick
stories about that--

00:28:50.080 --> 00:28:52.121
when we did our first Code
for America Fellowship

00:28:52.121 --> 00:28:56.720
back in 2011, we had people come
from the tech industry, work

00:28:56.720 --> 00:28:58.810
with city governments
for a year,

00:28:58.810 --> 00:29:01.616
and we hoped that
afterward, they would all,

00:29:01.616 --> 00:29:02.990
much like Teach
for America does,

00:29:02.990 --> 00:29:06.270
would graduate into working
in government technology.

00:29:06.270 --> 00:29:07.939
And we had 19 Fellows that year.

00:29:07.939 --> 00:29:09.980
And at the end of the
year, we were going, OK, so

00:29:09.980 --> 00:29:11.410
who's going to take a job?

00:29:11.410 --> 00:29:14.715
And they pretty much all said,
we're so exhausted from this.

00:29:14.715 --> 00:29:16.840
JACQUELINE FULLER: I'm
going back to my free lunch.

00:29:16.840 --> 00:29:17.756
JENNIFER PAHLKA: Yeah.

00:29:17.756 --> 00:29:19.560
Well, the best was
Scott Silverman,

00:29:19.560 --> 00:29:20.720
who had been at Apple.

00:29:20.720 --> 00:29:24.200
And he ended up,
he said, I am going

00:29:24.200 --> 00:29:25.950
to go back and
work at government.

00:29:25.950 --> 00:29:27.985
I just need to relax
for a year at a startup.

00:29:27.985 --> 00:29:30.880
I'm going to Airbnb.

00:29:30.880 --> 00:29:33.340
This was 2011.

00:29:33.340 --> 00:29:37.390
And the other story--
negative sells, I think,

00:29:37.390 --> 00:29:38.670
work here, right?

00:29:38.670 --> 00:29:39.795
It is incredibly hard work.

00:29:39.795 --> 00:29:41.503
JACQUELINE FULLER:
This crowd looks to me

00:29:41.503 --> 00:29:42.977
like a negative sell crowd.

00:29:42.977 --> 00:29:44.810
JENNIFER PAHLKA: It is
incredibly hard work.

00:29:44.810 --> 00:29:48.720
And the other story I would
tell is Googler Mikey Dickerson

00:29:48.720 --> 00:29:50.980
came to the White
House when our--

00:29:50.980 --> 00:29:52.480
JACQUELINE FULLER:
Our hero, Mikey--

00:29:52.480 --> 00:29:54.590
JENNIFER PAHLKA:
Yes, go, Mikey-- and

00:29:54.590 --> 00:29:57.970
was truly instrumental
in saving healthcare.gov

00:29:57.970 --> 00:30:01.740
and worked-- we
added up his hours.

00:30:01.740 --> 00:30:04.260
At one point, I think he had
worked 100 days straight,

00:30:04.260 --> 00:30:06.314
20 hours a day.

00:30:06.314 --> 00:30:07.355
So that's pretty intense.

00:30:07.355 --> 00:30:12.260
But when he was very kindly
asking other Google SREs

00:30:12.260 --> 00:30:15.290
to come help spell
other folks, since we're

00:30:15.290 --> 00:30:17.414
building this team
that did really

00:30:17.414 --> 00:30:19.080
do this amazing work
of getting the site

00:30:19.080 --> 00:30:23.030
to get through that first
open enrollment period,

00:30:23.030 --> 00:30:25.030
he would tell the
SREs that you will

00:30:25.030 --> 00:30:27.140
learn more in a day
on healthcare.gov

00:30:27.140 --> 00:30:31.530
than you will in a year,
because more things go wrong.

00:30:31.530 --> 00:30:34.015
And I think that was a very
effective recruiting technique.

00:30:34.015 --> 00:30:35.911
JACQUELINE FULLER: There you go.

00:30:35.911 --> 00:30:36.410
All right.

00:30:36.410 --> 00:30:38.900
Well, why don't we switch
gears and talk about advice

00:30:38.900 --> 00:30:40.120
and lessons, all right?

00:30:40.120 --> 00:30:42.680
So we have a crowd of
folks, a self-selected crowd

00:30:42.680 --> 00:30:46.180
here and probably 50,000
or more on the live stream.

00:30:46.180 --> 00:30:48.700
And these are folks who were
attracted to this notion of,

00:30:48.700 --> 00:30:51.350
how do we use tech
to take on and solve

00:30:51.350 --> 00:30:52.710
some of these big problems?

00:30:52.710 --> 00:30:56.210
So more generally,
lessons you've

00:30:56.210 --> 00:30:59.160
learned-- what do you wish
you could've told yourself

00:30:59.160 --> 00:31:03.140
five years ago before this all
started or in the early days?

00:31:03.140 --> 00:31:06.359
Or advice that you might have
for folks who are interested.

00:31:06.359 --> 00:31:06.900
Just open it.

00:31:06.900 --> 00:31:07.044
ROSE BROOME: Sure.

00:31:07.044 --> 00:31:08.002
I can open up for this.

00:31:08.002 --> 00:31:09.860
And the advice I
give to people who

00:31:09.860 --> 00:31:12.500
want to become social
entrepreneurs is the same as I

00:31:12.500 --> 00:31:16.560
give to any entrepreneur,
is think big but start

00:31:16.560 --> 00:31:19.970
small, very, very small,
smaller than you would even

00:31:19.970 --> 00:31:21.840
think at first.

00:31:21.840 --> 00:31:23.760
And that's one of the
biggest challenges

00:31:23.760 --> 00:31:26.270
that any early-stage
entrepreneur will face

00:31:26.270 --> 00:31:28.760
is keeping your
scope under control

00:31:28.760 --> 00:31:33.010
and staying very focused.

00:31:33.010 --> 00:31:37.380
And I was saying
earlier about how many

00:31:37.380 --> 00:31:43.700
incredible opportunities there
are in the social tech space.

00:31:43.700 --> 00:31:46.990
I'll add to it, you don't
have to leave your company

00:31:46.990 --> 00:31:48.350
to join in.

00:31:48.350 --> 00:31:51.220
Every city has Tech
for Good meet-ups

00:31:51.220 --> 00:31:53.690
that you can join and
become part of the community

00:31:53.690 --> 00:31:55.690
and contribute in small ways.

00:31:55.690 --> 00:31:58.520
And if your community
doesn't have one,

00:31:58.520 --> 00:32:02.030
that's a perfect opportunity
to join in and start your own

00:32:02.030 --> 00:32:06.500
and start bringing the social
entrepreneurial community

00:32:06.500 --> 00:32:09.527
and the tech
communities together.

00:32:09.527 --> 00:32:10.610
JACQUELINE FULLER: Robert?

00:32:10.610 --> 00:32:11.318
ROBERT LEE: Yeah.

00:32:13.930 --> 00:32:17.165
So I feel like I've
worked a broad spectrum

00:32:17.165 --> 00:32:20.400
of private government,
like non-profit.

00:32:20.400 --> 00:32:24.530
And I've been in non-profit
for two years now.

00:32:24.530 --> 00:32:26.132
And the thing that
really bothers me--

00:32:26.132 --> 00:32:27.840
it's kind of related
to what we were just

00:32:27.840 --> 00:32:29.673
talking about in the
challenges is that when

00:32:29.673 --> 00:32:32.390
people are like, oh, that's
really good, for a non-profit.

00:32:32.390 --> 00:32:35.430
I'm like, why add
that disclaimer?

00:32:35.430 --> 00:32:37.290
Why can't it just be good?

00:32:37.290 --> 00:32:38.590
Why can't it be great?

00:32:38.590 --> 00:32:40.920
And it is this
underlying thing of like,

00:32:40.920 --> 00:32:42.870
oh, there's a lower
level of effort

00:32:42.870 --> 00:32:44.890
or the standards are lower.

00:32:44.890 --> 00:32:48.260
But actually, the
challenges are a lot harder.

00:32:48.260 --> 00:32:50.990
And I'd love to say
the reason I came

00:32:50.990 --> 00:32:53.550
to Charity Water-- of
course, it was a part of it,

00:32:53.550 --> 00:32:55.650
but the biggest reason
was because there's

00:32:55.650 --> 00:32:58.299
this altruistic motive.

00:32:58.299 --> 00:32:59.340
I want to help the world.

00:32:59.340 --> 00:33:01.390
Sure, that's part of it, right?

00:33:01.390 --> 00:33:03.050
But that's not the main reason.

00:33:03.050 --> 00:33:06.960
The main reason is because I saw
how big of a challenge it was

00:33:06.960 --> 00:33:08.980
and I love taking on challenges.

00:33:08.980 --> 00:33:13.150
Even in my other jobs, I used
to do turnaround strategies.

00:33:13.150 --> 00:33:16.260
And I would ask them,
give me your worst group,

00:33:16.260 --> 00:33:18.229
the one that's failing
the most, and let

00:33:18.229 --> 00:33:19.270
me try to turn it around.

00:33:19.270 --> 00:33:20.960
And I just jumped
doing turnarounds

00:33:20.960 --> 00:33:22.460
and this was kind
of the same thing.

00:33:22.460 --> 00:33:23.780
It's like, OK, there's
this challenge that's

00:33:23.780 --> 00:33:25.480
been around for three decades.

00:33:25.480 --> 00:33:26.950
Let's figure it out.

00:33:26.950 --> 00:33:32.970
And an example is, how
many people are satisfied

00:33:32.970 --> 00:33:34.484
with their cell phone battery?

00:33:34.484 --> 00:33:36.650
Of course, we're not talking
about the Nexus, right?

00:33:36.650 --> 00:33:38.066
JACQUELINE FULLER:
No, we are not.

00:33:38.066 --> 00:33:38.880
No.

00:33:38.880 --> 00:33:40.630
ROBERT LEE: But other
cellphones out there

00:33:40.630 --> 00:33:45.850
and how do you feel about after
a year or two years, right?

00:33:45.850 --> 00:33:49.120
And when it does drain, every
day, when you charge it,

00:33:49.120 --> 00:33:53.060
you just walk a few feet and
plug it into the wall, right?

00:33:53.060 --> 00:33:55.460
Well, imagine a world where
you have to put out a device

00:33:55.460 --> 00:33:59.340
and the closest electrical
power source is a two-day hike

00:33:59.340 --> 00:34:02.430
to the local market.

00:34:02.430 --> 00:34:05.310
And there's no
local market where

00:34:05.310 --> 00:34:07.880
you can even on the phones where
you can replace the battery

00:34:07.880 --> 00:34:11.429
or you can just buy a new
battery and plug it in.

00:34:11.429 --> 00:34:14.335
So we're building this sensor
that has to be out there

00:34:14.335 --> 00:34:16.909
and last for 10 years,
because if we have

00:34:16.909 --> 00:34:19.794
to go out and constantly replace
batteries for the sensor,

00:34:19.794 --> 00:34:21.710
it kind of defeats the
purpose of saving money

00:34:21.710 --> 00:34:25.110
by not having to go out
there to check on the well.

00:34:25.110 --> 00:34:27.630
And so when we
built the sensors,

00:34:27.630 --> 00:34:30.530
we had to look through
all these batteries

00:34:30.530 --> 00:34:33.520
and figure out which one
works the best in that kind

00:34:33.520 --> 00:34:37.110
of hot, humid environment.

00:34:37.110 --> 00:34:40.763
We had to figure out-- we
implemented some AI techniques

00:34:40.763 --> 00:34:42.679
and learning algorithms
so that the sensor can

00:34:42.679 --> 00:34:44.550
be smarter and
only transmit when

00:34:44.550 --> 00:34:48.330
it needs to at the most critical
moments and otherwise just

00:34:48.330 --> 00:34:50.130
send us periodic things.

00:34:50.130 --> 00:34:51.800
So it actually learns
its own behavior

00:34:51.800 --> 00:34:54.354
of the well of what's normal
and looks at deviations.

00:34:57.090 --> 00:35:00.490
And when we look at how to
approach all these things,

00:35:00.490 --> 00:35:05.130
I'm not talking to people
that have never done it.

00:35:05.130 --> 00:35:08.580
No, I'm talking to CTOs
of the major fitness-ware,

00:35:08.580 --> 00:35:10.960
because they're thinking
of low-energy harvesting.

00:35:10.960 --> 00:35:13.480
They're thinking of
how can we optimize

00:35:13.480 --> 00:35:14.820
the power of management?

00:35:14.820 --> 00:35:17.510
And when we build these
products, it's not just-- sure,

00:35:17.510 --> 00:35:18.910
we start out in
the garage maybe,

00:35:18.910 --> 00:35:20.280
but we're putting it
through everything.

00:35:20.280 --> 00:35:21.710
We're manufacturing in China.

00:35:21.710 --> 00:35:24.500
We're putting it through UV
chamber tests, heat chamber

00:35:24.500 --> 00:35:25.020
tests.

00:35:25.020 --> 00:35:30.840
We're shooting steel balls at
it to measure the durability.

00:35:30.840 --> 00:35:31.990
We put it underwater.

00:35:31.990 --> 00:35:34.380
We put it in humidity chambers.

00:35:34.380 --> 00:35:39.660
There's no reason why we can't
build products that are just

00:35:39.660 --> 00:35:42.900
as good, if not better, than
products built for the consumer

00:35:42.900 --> 00:35:44.390
market in America.

00:35:44.390 --> 00:35:47.550
And so my challenge, too, is
this attitude of even when we

00:35:47.550 --> 00:35:50.890
do get people from the
private sector to help,

00:35:50.890 --> 00:35:53.370
is they think it's
like, OK, I'll

00:35:53.370 --> 00:35:55.425
just do this an hour
before I go to sleep.

00:35:57.949 --> 00:35:59.240
this room is filled with some--

00:35:59.240 --> 00:36:00.040
JACQUELINE FULLER: It's
going to take the best of who

00:36:00.040 --> 00:36:01.082
you are, not your scraps.

00:36:01.082 --> 00:36:01.831
ROBERT LEE: Right.

00:36:01.831 --> 00:36:02.940
JACQUELINE FULLER: Yeah.

00:36:02.940 --> 00:36:04.860
ROBERT LEE: And I
got to the point

00:36:04.860 --> 00:36:07.432
where we're Charity Water.

00:36:07.432 --> 00:36:08.890
We have a good
brand in non-profit,

00:36:08.890 --> 00:36:11.260
so we get offers for
discounts all the time.

00:36:11.260 --> 00:36:12.900
I got to the point
when I was like,

00:36:12.900 --> 00:36:14.950
I don't want you to
discount, because you give me

00:36:14.950 --> 00:36:17.470
a 10% discount, then
what do I get, right?

00:36:17.470 --> 00:36:18.750
I want to pay full price.

00:36:18.750 --> 00:36:20.740
In fact, I want to pay
more than full price.

00:36:20.740 --> 00:36:24.070
I want to pay top-market dollar
because I want your best.

00:36:24.070 --> 00:36:27.210
And so when you
guys get involved--

00:36:27.210 --> 00:36:32.000
you guys are the top talent--
really take it seriously.

00:36:32.000 --> 00:36:35.630
It is a real challenge and
there's so many benefits.

00:36:35.630 --> 00:36:37.420
Then, there is that
altruistic benefit

00:36:37.420 --> 00:36:41.770
that you're getting
and not only that,

00:36:41.770 --> 00:36:44.960
but again, if you like
facing challenges, which--

00:36:44.960 --> 00:36:46.710
JACQUELINE FULLER: If
you like challenges,

00:36:46.710 --> 00:36:48.437
do we have challenges for you?

00:36:48.437 --> 00:36:50.520
NITHYA RAMANATHAN: I want
to jump in really quick,

00:36:50.520 --> 00:36:53.940
actually, which is just that,
what if the killer apps are

00:36:53.940 --> 00:36:57.330
actually in the public sector,
in these problems that we're

00:36:57.330 --> 00:36:59.440
trying to solve, and
actually products

00:36:59.440 --> 00:37:01.590
were designed-- rather
than designing products

00:37:01.590 --> 00:37:04.630
in a first-world environment and
then trying to move them over,

00:37:04.630 --> 00:37:06.810
what if it turns out that
actually in the developing

00:37:06.810 --> 00:37:08.814
countries, that's where
the killer apps are

00:37:08.814 --> 00:37:10.230
and then we bring
those solutions?

00:37:10.230 --> 00:37:12.480
JACQUELINE FULLER: Well,
don't we all want this sensor

00:37:12.480 --> 00:37:14.200
that Robert has
in a ball-bearing

00:37:14.200 --> 00:37:16.600
chamber with the
humidity and-- if it

00:37:16.600 --> 00:37:18.230
works in Eritrea
for six months, it's

00:37:18.230 --> 00:37:20.761
going to be great
on my wrist, right?

00:37:20.761 --> 00:37:21.760
No, that's a good point.

00:37:21.760 --> 00:37:25.510
So as we're wrapping up any
last thoughts, Nithya and Jen

00:37:25.510 --> 00:37:28.375
about advice that
you would give?

00:37:28.375 --> 00:37:30.000
NITHYA RAMANATHAN:
I think I would just

00:37:30.000 --> 00:37:32.070
go back to what I
said, really thinking

00:37:32.070 --> 00:37:34.520
about how actually the
problems that we're

00:37:34.520 --> 00:37:36.810
trying to solve in
Mozambique, in Kenya,

00:37:36.810 --> 00:37:39.250
in Ethiopia around keeping
vaccines cold-- actually,

00:37:39.250 --> 00:37:41.290
it turns out those problems
exist here, as well,

00:37:41.290 --> 00:37:43.960
but aren't necessarily
maybe thought about.

00:37:43.960 --> 00:37:45.720
But as we design
solutions in Mozambique,

00:37:45.720 --> 00:37:48.024
we're actually finding
that partners in the US

00:37:48.024 --> 00:37:48.940
are saying, hey, wait.

00:37:48.940 --> 00:37:49.890
We want that.

00:37:49.890 --> 00:37:51.860
Actually, our vaccines
are going bad here.

00:37:51.860 --> 00:37:53.070
Our fridges are failing here.

00:37:53.070 --> 00:37:54.903
And so we're working
with Planned Parenthood

00:37:54.903 --> 00:37:57.802
and other partners and starting
to explore how we can actually

00:37:57.802 --> 00:37:58.510
start scaling up.

00:37:58.510 --> 00:37:59.130
JACQUELINE FULLER:
So they're saying,

00:37:59.130 --> 00:38:01.550
hey, that thing you got
working in Mozambique,

00:38:01.550 --> 00:38:02.420
bring it out here.

00:38:02.420 --> 00:38:03.402
We need it in Brooklyn.

00:38:03.402 --> 00:38:05.110
JENNIFER PAHLKA: I
see that all the time.

00:38:05.110 --> 00:38:06.400
JACQUELINE FULLER: Advice, Jen?

00:38:06.400 --> 00:38:08.150
JENNIFER PAHLKA: I
would take the question

00:38:08.150 --> 00:38:10.270
as, what advice if you
really want to have impact?

00:38:10.270 --> 00:38:13.350
And I'm going to make a
particular pitch there

00:38:13.350 --> 00:38:15.830
around impact.

00:38:15.830 --> 00:38:18.990
Think about any cause
that you care about.

00:38:18.990 --> 00:38:21.180
It could be animal rights.

00:38:21.180 --> 00:38:22.960
It could be feeding
the homeless.

00:38:22.960 --> 00:38:25.960
I guarantee you the
government spends

00:38:25.960 --> 00:38:29.800
100x what charitable
organizations are spending.

00:38:29.800 --> 00:38:32.520
I'm not saying that
they're having 100x impact.

00:38:32.520 --> 00:38:34.300
I think Rose's
work, for instance,

00:38:34.300 --> 00:38:37.710
is having a huge impact
with very little investment.

00:38:37.710 --> 00:38:40.490
But what if that enormous
investment that's

00:38:40.490 --> 00:38:43.810
coming out of government
was spent better

00:38:43.810 --> 00:38:46.600
and how could you impact that?

00:38:46.600 --> 00:38:49.820
The numbers on-- the
size of government

00:38:49.820 --> 00:38:52.680
is it's 15%, 20%
percent of our GDP.

00:38:52.680 --> 00:38:54.810
There's more people
working in government

00:38:54.810 --> 00:38:57.350
than in the entire companies
of the Dow-Jones combined.

00:38:57.350 --> 00:38:58.160
JACQUELINE FULLER: Let
me just check the room.

00:38:58.160 --> 00:39:00.770
Raise your hand if you've
ever worked for any government

00:39:00.770 --> 00:39:02.539
around the world.

00:39:02.539 --> 00:39:03.580
JENNIFER PAHLKA: Awesome.

00:39:03.580 --> 00:39:06.644
JACQUELINE FULLER: So
that was what, 10%, 8%?

00:39:06.644 --> 00:39:07.560
JENNIFER PAHLKA: Yeah.

00:39:07.560 --> 00:39:11.025
And I think that it's a
small amount considering

00:39:11.025 --> 00:39:12.400
what a big portion
of our economy

00:39:12.400 --> 00:39:13.280
it is and what a big portion--

00:39:13.280 --> 00:39:14.410
JACQUELINE FULLER: Considering
that what governments

00:39:14.410 --> 00:39:16.770
need most is probably
the skill set that's

00:39:16.770 --> 00:39:18.910
represented in this room
and on the live stream.

00:39:18.910 --> 00:39:20.368
JENNIFER PAHLKA:
And I think what's

00:39:20.368 --> 00:39:22.910
changed, if some of you--
that was a couple of years ago

00:39:22.910 --> 00:39:25.510
for you, even five
or 10 years ago-- it

00:39:25.510 --> 00:39:26.930
used to be pretty different.

00:39:26.930 --> 00:39:29.620
I think we used to have
this notion in government

00:39:29.620 --> 00:39:32.590
that digital is
something you buy.

00:39:32.590 --> 00:39:34.210
And we're trying
to teach government

00:39:34.210 --> 00:39:35.250
it's something you do.

00:39:35.250 --> 00:39:37.320
It's actually how we govern.

00:39:37.320 --> 00:39:39.790
And so I think the
big shift has been--

00:39:39.790 --> 00:39:41.960
and I think healthcare.gov
will always mark that.

00:39:41.960 --> 00:39:45.260
I think we'll look
back at it as it's now

00:39:45.260 --> 00:39:49.020
a time when digital people have
a seat at the strategic table.

00:39:49.020 --> 00:39:51.890
They're able to say,
no, no, you can't just

00:39:51.890 --> 00:39:54.660
consult me later and tell me
that we bought this thing.

00:39:54.660 --> 00:39:55.620
JACQUELINE FULLER: Go
implement this thing

00:39:55.620 --> 00:39:57.420
when the entire strategy
blueprint is off.

00:39:57.420 --> 00:39:58.380
JENNIFER PAHLKA: Exactly.

00:39:58.380 --> 00:39:59.530
JACQUELINE FULLER: Yeah,
we need tech at the table.

00:39:59.530 --> 00:40:02.110
JENNIFER PAHLKA: You have to
be there from the beginning

00:40:02.110 --> 00:40:06.280
and you have to be
in charge-- actually,

00:40:06.280 --> 00:40:08.520
there has to be digital
competence within government,

00:40:08.520 --> 00:40:10.750
not just in the
contracting ecosystem.

00:40:10.750 --> 00:40:11.958
JACQUELINE FULLER: All right.

00:40:11.958 --> 00:40:15.010
So we've got to wrap but I
would love for each of you,

00:40:15.010 --> 00:40:16.555
for just 20 seconds
each, I think

00:40:16.555 --> 00:40:18.180
you have a lot of
motivated people here

00:40:18.180 --> 00:40:19.140
who might be wondering.

00:40:19.140 --> 00:40:19.800
What's next?

00:40:19.800 --> 00:40:22.029
How can I help you
in what you're doing?

00:40:22.029 --> 00:40:23.320
So 20 seconds from each of you.

00:40:23.320 --> 00:40:24.580
How can people help?

00:40:24.580 --> 00:40:25.546
What's the next step?

00:40:25.546 --> 00:40:26.920
ROSE BROOME: I'd
say, of course--

00:40:26.920 --> 00:40:27.060
JACQUELINE FULLER: Go ahead.

00:40:27.060 --> 00:40:28.340
Go ahead.

00:40:28.340 --> 00:40:31.250
ROSE BROOME: I'd say go
to handup.org and help

00:40:31.250 --> 00:40:33.100
a homeless person
in your community.

00:40:33.100 --> 00:40:33.940
JACQUELINE FULLER: Handup.org

00:40:33.940 --> 00:40:35.314
ROSE BROOME: And
of course, we're

00:40:35.314 --> 00:40:40.940
hiring product designers,
engineers, project managers,

00:40:40.940 --> 00:40:44.330
as are all these companies and
all of google.org's portfolio

00:40:44.330 --> 00:40:44.830
companies.

00:40:44.830 --> 00:40:49.049
So check it out if you're
looking for your next move.

00:40:49.049 --> 00:40:50.590
ROBERT LEE: Yeah,
I would say contact

00:40:50.590 --> 00:40:53.080
me, as well,
robert@charitywater.org,

00:40:53.080 --> 00:40:54.350
pretty simple.

00:40:54.350 --> 00:40:57.820
And with this product,
it's not only this project

00:40:57.820 --> 00:40:58.870
that we're doing here.

00:40:58.870 --> 00:41:02.310
But I actually have been-- and
as I speak in other places,

00:41:02.310 --> 00:41:04.770
I've been pinged by
a lot of other people

00:41:04.770 --> 00:41:09.210
that want to build tech for
non-profits and social impact.

00:41:09.210 --> 00:41:10.860
And there's a lot
of different things

00:41:10.860 --> 00:41:12.986
out there, a lot of different
ways to get involved.

00:41:12.986 --> 00:41:14.360
It's pretty endless,
whether it's

00:41:14.360 --> 00:41:17.607
hardware, software,
integration, whatever it may be.

00:41:17.607 --> 00:41:19.690
So if you're interested
in getting more involved--

00:41:19.690 --> 00:41:21.523
JACQUELINE FULLER: Brave
man, gave his email

00:41:21.523 --> 00:41:22.860
to 50,000 people.

00:41:22.860 --> 00:41:23.759
Nithya?

00:41:23.759 --> 00:41:25.050
NITHYA RAMANATHAN: Same for us.

00:41:25.050 --> 00:41:26.508
We're always hiring,
always looking

00:41:26.508 --> 00:41:29.050
for really strong
technological talent,

00:41:29.050 --> 00:41:32.480
and also, just hoping that
people who want to get

00:41:32.480 --> 00:41:34.940
involved, whether it's with
Nexleaf or with others,

00:41:34.940 --> 00:41:36.760
is really spending
time to understand

00:41:36.760 --> 00:41:39.070
what are the problems and
how do we connect technology

00:41:39.070 --> 00:41:40.220
to impact?

00:41:40.220 --> 00:41:42.940
Ultimately, it's not
actually about the technology

00:41:42.940 --> 00:41:45.414
but how we're actually
getting to that impact.

00:41:45.414 --> 00:41:46.830
JACQUELINE FULLER:
Last word, Jen.

00:41:46.830 --> 00:41:48.371
JENNIFER PAHLKA:
OK, so we want to be

00:41:48.371 --> 00:41:51.020
able to show you that you can
have an impact on government.

00:41:51.020 --> 00:41:54.580
If you have an hour, you can
use our Civic Tech Issue Finder

00:41:54.580 --> 00:41:56.480
and just code on something
that needs coding

00:41:56.480 --> 00:41:58.890
and that's at
codeforamerica.org/helpout.

00:41:58.890 --> 00:42:01.000
If you want to come
once a week, there's

00:42:01.000 --> 00:42:03.750
probably a brigade in your
city if you don't live here.

00:42:03.750 --> 00:42:07.240
And that's at
codeforamerica.org/brigade.

00:42:07.240 --> 00:42:11.029
So that's your citizen volunteer
groups that help City Hall.

00:42:11.029 --> 00:42:12.570
If you've got a
year, we're currently

00:42:12.570 --> 00:42:15.030
recruiting for our
2016 class of Code

00:42:15.030 --> 00:42:17.830
for America Fellows, the
toughest job you'll ever love.

00:42:17.830 --> 00:42:21.920
You'll get addicted, I promise,
and that's at /Fellows.

00:42:21.920 --> 00:42:25.550
And if you're interested maybe
in actually looking at a job

00:42:25.550 --> 00:42:27.917
in local, state, or
federal government,

00:42:27.917 --> 00:42:29.000
codeforamerica.org/talent.

00:42:29.000 --> 00:42:32.720
That Let us know that.

00:42:32.720 --> 00:42:36.560
There's a bunch of really
interesting open positions,

00:42:36.560 --> 00:42:40.121
particularly at the USDS in 18F
that would give you a chance

00:42:40.121 --> 00:42:41.245
to be near Mikey Dickerson.

00:42:41.245 --> 00:42:41.954
He's such a hero.

00:42:41.954 --> 00:42:43.578
JACQUELINE FULLER:
All right, everyone.

00:42:43.578 --> 00:42:46.014
Well, join me in thanking
our panelists for the work

00:42:46.014 --> 00:42:47.930
that they're doing and
our conversation today.

00:42:47.930 --> 00:42:49.391
[APPLAUSE]

00:42:49.391 --> 00:42:50.365
Thanks, you guys.

00:42:55.115 --> 00:42:57.670
All right, thanks, everyone.

