WEBVTT
Kind: captions
Language: en

00:00:00.220 --> 00:00:04.848
So coming back into what we were
doing with discrete distributions.

00:00:04.848 --> 00:00:08.164
And I wanted to mention one common.

00:00:11.289 --> 00:00:16.930
Kind of the most common or
fundamental common mistake in probability.

00:00:16.930 --> 00:00:20.950
Because a couple of the TFs mentioned
that this is coming up on the homework,

00:00:20.950 --> 00:00:24.910
which I'm not surprised about
because this is very fundamental.

00:00:24.910 --> 00:00:27.640
It's trying to understand the difference
between the distribution and

00:00:27.640 --> 00:00:30.850
a random variable, right that's
what we've been talking about.

00:00:30.850 --> 00:00:35.190
But it's subtle at first, so
you have to keep practicing that.

00:00:35.190 --> 00:00:38.497
So this is what I call sympathetic magic.

00:00:41.815 --> 00:00:46.746
Sympathetic magic is what I
call the mistake of confusing

00:00:46.746 --> 00:00:50.453
a random variable with its distribution.

00:00:54.229 --> 00:00:56.038
So an example of that.

00:01:00.337 --> 00:01:07.070
That has come up on some of the homeworks,
is you have a sum of two random variables.

00:01:08.110 --> 00:01:12.490
And kind of just blindly saying,

00:01:12.490 --> 00:01:17.500
the PMF of the sum is the sum of the PMFs,
or something like that.

00:01:17.500 --> 00:01:20.170
That is, you're supposed to be
adding random variables, and

00:01:20.170 --> 00:01:25.860
if you instead add PMFs it
just makes no sense at all.

00:01:25.860 --> 00:01:30.840
So that's confusing adding random
variables is not the same as adding PMFs.

00:01:30.840 --> 00:01:33.190
And in fact, for that PMF case,

00:01:33.190 --> 00:01:38.650
it really makes no sense at
all because if you did P(X=x).

00:01:38.650 --> 00:01:42.030
Let's say I had x and y, we're gonna talk

00:01:42.030 --> 00:01:45.640
a lot more later in the course about how
do we deal with sums of random variables.

00:01:45.640 --> 00:01:47.340
But we've already talked
about to some extent,

00:01:47.340 --> 00:01:50.790
remember we spent a lot of time
talking about the sum of binomials.

00:01:50.790 --> 00:01:53.590
The sum of independence
binomials is binomial

00:01:53.590 --> 00:01:57.260
if they are the same p we prove
that in about three different ways.

00:01:57.260 --> 00:02:00.600
And we talked about dealing with
x plus y by conditioning on x.

00:02:00.600 --> 00:02:03.710
So we have talked a little bit about that,
we're gonna do more later.

00:02:05.260 --> 00:02:09.418
So we had different methods for dealing
with that, but we never said, well take

00:02:09.418 --> 00:02:14.520
P(X=x) + P(Y=y), right,
that's the PMF of x plus the PMF of y.

00:02:15.780 --> 00:02:18.970
And somehow this is
supposed to relate to x+y.

00:02:18.970 --> 00:02:24.350
Well, first of all,
if you add up two probabilities

00:02:24.350 --> 00:02:29.300
there's no reason from what I wrote or
think about still be less than 1, right?

00:02:29.300 --> 00:02:30.510
Or less than or equal to 1.

00:02:30.510 --> 00:02:32.950
That could easily exceed 1, first of all.

00:02:32.950 --> 00:02:38.070
And secondly, this thing is a function
of x and y, little x and little y.

00:02:38.070 --> 00:02:42.480
But if we were,
if we want the PMF of x + y,

00:02:42.480 --> 00:02:44.880
then we're thinking of X plus
Y as our random variable.

00:02:44.880 --> 00:02:46.240
So we should have a function of, you know,

00:02:46.240 --> 00:02:49.970
what's the probability that x + y
= little t, something like that.

00:02:49.970 --> 00:02:52.110
So you can't do things like that.

00:02:52.110 --> 00:02:56.910
Similarly, you know, if you, later we're
going to deal a lot with what happens when

00:02:56.910 --> 00:02:58.770
you transform random variables.

00:02:58.770 --> 00:03:00.320
So if I have a random variable x and

00:03:00.320 --> 00:03:04.500
I want to cube it, it doesn't mean
that somehow cube its distribution,

00:03:04.500 --> 00:03:09.600
or it's pmf, or somethings like that,
you can't do things like that.

00:03:09.600 --> 00:03:15.661
So, there's a famous saying in semantics,
that the word is not the thing,

00:03:15.661 --> 00:03:21.455
the map is not the territory, and
that's exactly what this mistake is.

00:03:25.445 --> 00:03:28.620
So just to push this analogy
a little bit further.

00:03:29.800 --> 00:03:36.560
The map is not the territory, this is
like really an obvious piece of advice.

00:03:36.560 --> 00:03:44.680
It's very rare that I've seen someone
confuse a map with the territory, right?

00:03:44.680 --> 00:03:48.430
You don't usually like put
the map on the floor and

00:03:48.430 --> 00:03:52.230
start walking around the map thinking
that you're exploring the territory.

00:03:52.230 --> 00:03:55.360
That's the map, that's the territory,
we don't make that mistake.

00:03:57.320 --> 00:03:59.540
Here though, when we're talking
about random variables and

00:03:59.540 --> 00:04:02.140
distributions, it's more mathematical.

00:04:02.140 --> 00:04:06.440
It's a more abstract thing, and do people
do make that mistake all the time.

00:04:06.440 --> 00:04:08.510
So the mistake is completely analogous,

00:04:08.510 --> 00:04:13.160
it's just in this context,
the human mind easily does that.

00:04:13.160 --> 00:04:19.050
And in the map territory context,
it's not such a big deal.

00:04:19.050 --> 00:04:26.434
Here's an analogy along these
lines that I like even more.

00:04:26.434 --> 00:04:31.575
Think of the random variable is a house

00:04:31.575 --> 00:04:38.150
rv corresponds to house and

00:04:38.150 --> 00:04:45.260
the distribution is the blueprint for
the house, okay?

00:04:45.260 --> 00:04:49.810
So mostly you probably don't try to
live inside the blueprint, right?

00:04:51.560 --> 00:04:52.790
It's the same thing.

00:04:52.790 --> 00:04:56.560
And I like this analogy even more
because now we can think of,

00:04:56.560 --> 00:05:01.590
if you have one blueprint you could build
many houses from the same blueprint right?

00:05:01.590 --> 00:05:04.490
Just use the blueprint and
build in different locations.

00:05:04.490 --> 00:05:07.160
So you can have as many
random variables as you want,

00:05:07.160 --> 00:05:08.580
all with the same distribution.

00:05:08.580 --> 00:05:12.650
They could be iid which would mean they're
independent with the same distribution or

00:05:12.650 --> 00:05:13.940
they could be dependent.

00:05:13.940 --> 00:05:16.050
But they could have all
the same distribution,

00:05:16.050 --> 00:05:19.200
there's no problem with doing that.

00:05:19.200 --> 00:05:21.580
So let's actually call
this a random house.

00:05:24.332 --> 00:05:27.960
The distribution is a blueprint for
building a random house.

00:05:27.960 --> 00:05:31.830
The random variable is one
of those random houses.

00:05:31.830 --> 00:05:36.670
So the blueprint means, it's not
just saying put this door here and

00:05:36.670 --> 00:05:37.660
put this wall here.

00:05:37.660 --> 00:05:41.560
It's saying you randomly choose
whether you have a blue door or

00:05:41.560 --> 00:05:44.350
a red door with certain probabilities.

00:05:44.350 --> 00:05:46.970
This is specifying all of
the probabilities you need for

00:05:46.970 --> 00:05:49.330
different choices for
how to build the house.

00:05:49.330 --> 00:05:52.180
Random variable is the house.

00:05:52.180 --> 00:05:56.230
Okay, so that's just a very,
very common mistake

00:05:56.230 --> 00:06:01.250
that's sort of like a meta mistake or
a class of mistakes.

00:06:01.250 --> 00:06:05.950
Because there are many, many individual
mistakes that I attribute to this.

00:06:05.950 --> 00:06:10.840
So, that's why I'm suggesting
to be careful about that.

00:06:10.840 --> 00:06:13.440
All right, so now coming back
to our discrete distributions.

00:06:14.510 --> 00:06:18.950
We only have one more famous discrete
distribution that we need for

00:06:18.950 --> 00:06:23.320
this entire semester, and
that's called the Poisson distribution.

00:06:23.320 --> 00:06:25.100
So that's the main topic for today.

00:06:26.140 --> 00:06:28.650
I want to show you first of all,
what is it?

00:06:28.650 --> 00:06:31.580
Secondly, why is it important?

00:06:31.580 --> 00:06:35.090
Arguably the Poisson is the most
important discrete distribution

00:06:35.090 --> 00:06:36.370
in all of statistics.

00:06:37.850 --> 00:06:39.910
Depends on how you define importance, but

00:06:39.910 --> 00:06:41.910
I think you can make a pretty
good argument for that.

00:06:43.960 --> 00:06:51.210
It's named after Poisson who was a famous
brilliant French mathematician who

00:06:51.210 --> 00:06:56.278
was the first or one of the first people
to start working with this distribution.

00:06:56.278 --> 00:07:00.069
In the 1830s, he was doing this.

00:07:01.330 --> 00:07:08.770
And so first let's write down the PMF and
then see what's so great about this.

00:07:08.770 --> 00:07:09.330
Okay so here's the PMF.

00:07:09.330 --> 00:07:14.680
We want to

00:07:14.680 --> 00:07:18.680
know what's the probability that X = k,
k is a non-negative integer here.

00:07:18.680 --> 00:07:22.218
So unlike the binomial which
is bounded between 0 and n,

00:07:22.218 --> 00:07:27.505
Poisson could take any
nonnegative integer value and

00:07:27.505 --> 00:07:34.640
the PMF is just e to the minus lambda,
lambda to the k over k factorial.

00:07:34.640 --> 00:07:39.050
Where k, as I said,
is a non-negative integer, 0 otherwise.

00:07:40.390 --> 00:07:46.310
And lambda is the parameter and

00:07:46.310 --> 00:07:52.410
I call it a rate parameter for reasons
that will become clear at some point.

00:07:52.410 --> 00:07:57.470
But for now just think of it as this is a
one parameter distribution with parameter

00:07:57.470 --> 00:08:01.890
Traditionally called lambda, but
you could call it whatever you want.

00:08:01.890 --> 00:08:03.700
That's just the parameter
of the distribution.

00:08:03.700 --> 00:08:06.910
But lambda is the most common name for
that particular parameter.

00:08:06.910 --> 00:08:11.295
So lambda is a positive constant.

00:08:11.295 --> 00:08:14.480
And Lambda could be any
positive real number.

00:08:14.480 --> 00:08:18.212
That's the parameter of
the Poisson distribution.

00:08:18.212 --> 00:08:21.573
All right, so
I mean I just wrote this thing down, but

00:08:21.573 --> 00:08:24.880
that doesn't mean that it's useful for
anything.

00:08:24.880 --> 00:08:26.968
So why do I say that this thing is so
important?

00:08:26.968 --> 00:08:31.510
Well, before we can do that, first,
let's check that this is a valid PMF.

00:08:36.798 --> 00:08:40.713
That is is this actually a PMF?

00:08:40.713 --> 00:08:43.070
Well, they're non-negative so

00:08:43.070 --> 00:08:47.793
the only thing that we need to check
is that these numbers add up to 1.

00:08:47.793 --> 00:08:52.737
And that's very easy, because if
you add up e to the minus lambda,

00:08:52.737 --> 00:09:00.490
lambda to the k over k!,
e to the minus lambda is a constant.

00:09:00.490 --> 00:09:03.950
So that just comes out,
the sum of lambda to k over k!.

00:09:03.950 --> 00:09:07.290
Hopefully, by now, everyone recognizes
that that's the Taylor series for

00:09:07.290 --> 00:09:08.500
e to the lambda.

00:09:08.500 --> 00:09:11.105
So this is e to the minus lambda,
e to the lambda equals 1.

00:09:12.400 --> 00:09:18.475
All right, so, basically all we did was
take one term from the Taylor series for

00:09:18.475 --> 00:09:24.020
e to the lambda, and then put a constant
in front so that they add up to 1.

00:09:24.020 --> 00:09:27.204
So that is a PMF.

00:09:27.204 --> 00:09:29.805
And while we're doing
a calculation like this,

00:09:29.805 --> 00:09:33.340
we may as well compute
the expected value also.

00:09:33.340 --> 00:09:37.880
So let's find the mean, E(X).

00:09:37.880 --> 00:09:41.840
And as notation,
we would write X as Poisson (lambda).

00:09:41.840 --> 00:09:46.419
And we'll usually just
abbreviate that to Pois, okay?

00:09:46.419 --> 00:09:49.410
So let's find the expected value.

00:09:49.410 --> 00:09:54.280
Well, remember the expected
value is the sum

00:09:54.280 --> 00:09:59.030
of the value times the probability
of the value, right?

00:09:59.030 --> 00:10:02.320
So I'll take out the e to the minus
lambda, cuz that's just a constant.

00:10:02.320 --> 00:10:04.840
So what we're doing is just adding up k,

00:10:06.260 --> 00:10:10.640
that's the value,
times the probability of the value,

00:10:10.640 --> 00:10:14.480
which is e to the minus lambda,
lambda to the k over k!,

00:10:14.480 --> 00:10:19.710
from 0 to infinity.

00:10:19.710 --> 00:10:24.400
But when k is 0, this is 0 anyway.

00:10:24.400 --> 00:10:27.015
So we may as well start the sum at 1.

00:10:29.797 --> 00:10:31.380
So let's go from 1 to infinity.

00:10:33.627 --> 00:10:35.450
And notice we have k over k!.

00:10:36.720 --> 00:10:41.050
Well, k factorial is k times k-1,
k-2, blah blah blah.

00:10:41.050 --> 00:10:44.647
So the k cancels one of the ks,
k cancels the k here.

00:10:44.647 --> 00:10:48.240
And we're left with k-1
factorial in the denominator.

00:10:48.240 --> 00:10:55.477
So that's really just lambda
to the k over (k-1)!.

00:10:55.477 --> 00:10:58.710
Now it's not a calculus problem.

00:10:58.710 --> 00:11:01.730
It's just a pattern recognition
problem at this point.

00:11:01.730 --> 00:11:05.668
When we see this thing,
it should remind us.

00:11:05.668 --> 00:11:08.400
It should remind us of what
we just did basically, right?

00:11:08.400 --> 00:11:10.750
That's the Taylor series for
e to the lambda.

00:11:10.750 --> 00:11:13.170
This series looks a lot like this series.

00:11:13.170 --> 00:11:19.328
The only difference is
that we have a (k-1)!,

00:11:19.328 --> 00:11:23.290
not a k!, and
we're starting at 1 rather than 0.

00:11:23.290 --> 00:11:28.250
So to make this match up better,
let's just take out one of the lambdas.

00:11:29.620 --> 00:11:32.190
So I make this lambda to the k-1.

00:11:32.190 --> 00:11:37.690
Lambda's just a constant, so you can
take out, put back lambdas all you want.

00:11:37.690 --> 00:11:42.852
I put it that way so
that now this k-1 matches this (k-1).

00:11:42.852 --> 00:11:47.188
[COUGH] Now if we want, we can just
let j equal k-1 at this point, or

00:11:47.188 --> 00:11:51.911
just write out the first few terms and
just directly see this is exactly,

00:11:51.911 --> 00:11:55.719
it's exactly the Taylor series for
e to the lambda again.

00:11:55.719 --> 00:11:58.342
So, therefore, this is just lambda,

00:11:58.342 --> 00:12:01.923
e to the minus lambda,
e to the lambda equals lambda.

00:12:04.086 --> 00:12:06.791
So that's a very useful,
easy to remember result,

00:12:06.791 --> 00:12:09.500
that a Poisson lambda has
expected value of lambda.

00:12:10.630 --> 00:12:15.990
Okay, so memorizing formulas is
not that important in this class.

00:12:15.990 --> 00:12:19.400
But this one is useful and
very easy to remember, the mean is lambda.

00:12:20.520 --> 00:12:23.720
Okay, so
that's the expected value of a Poisson.

00:12:23.720 --> 00:12:25.900
Now why do we care about the Poisson now?

00:12:27.190 --> 00:12:34.000
So let me just mention a few
examples where the Poisson is used.

00:12:34.000 --> 00:12:40.150
It's the most widely, in practice, it's
the single most-widely used distribution

00:12:40.150 --> 00:12:45.460
as a model for
discrete data in the real world.

00:12:45.460 --> 00:12:50.295
So, often used for applications like,

00:12:53.312 --> 00:12:55.930
Well, let me just say what
the general application is.

00:12:58.540 --> 00:13:03.603
Applications where we wanna
count something if it's used for

00:13:03.603 --> 00:13:10.531
counting, cuz it's non-negative integers,
or counting a number of something.

00:13:15.878 --> 00:13:19.240
Let's say a number of successes.

00:13:19.240 --> 00:13:21.114
But, again, just like in the Binomial.

00:13:23.708 --> 00:13:26.237
Just like in the Binomial,
we can define success and

00:13:26.237 --> 00:13:28.110
failure like in a very general way.

00:13:28.110 --> 00:13:29.586
So I'll just put successes in quote,

00:13:29.586 --> 00:13:34.030
cuz we could define that in many
different ways, just have a word.

00:13:34.030 --> 00:13:42.493
We're counting the number of successes
where we have a large number of trials.

00:13:42.493 --> 00:13:49.340
So there's a large number of things, each
of which could lead to success or failure.

00:13:49.340 --> 00:13:53.340
But the probability of success for
each one is small, right?

00:13:53.340 --> 00:13:56.283
So let's say we had 10,000 trials, but

00:13:56.283 --> 00:14:00.220
each one only has probably
one over 10,000 of success.

00:14:00.220 --> 00:14:03.530
Well, we immediately know that
the expected number of successes is one,

00:14:03.530 --> 00:14:07.440
just using linearity and
indicator random variables.

00:14:07.440 --> 00:14:11.922
But it's that kind of thing where
we have a large number of trials,

00:14:11.922 --> 00:14:14.657
small probability for each one, okay.

00:14:14.657 --> 00:14:21.626
That's the general set-up and each one
with a small probability of success.

00:14:29.410 --> 00:14:35.771
So some examples would be like number of

00:14:35.771 --> 00:14:40.793
emails that you get in an hour.

00:14:44.603 --> 00:14:46.990
I'm not claiming this is exactly Poisson.

00:14:46.990 --> 00:14:52.180
I'm saying that as an initial, right, to
check whether the number of emails you got

00:14:52.180 --> 00:14:58.560
in an hour is a Poisson or not, you'd
have to go collect data and try to see.

00:14:58.560 --> 00:15:00.080
That's an empirical question.

00:15:00.080 --> 00:15:03.160
That's not a mathematical question, okay?

00:15:03.160 --> 00:15:05.700
But the claim is that this would be

00:15:05.700 --> 00:15:08.760
a reasonable starting point as a model for
that.

00:15:08.760 --> 00:15:10.570
In some cases, it may be very good.

00:15:10.570 --> 00:15:12.010
In other cases, it may be bad.

00:15:12.010 --> 00:15:14.480
But this would be a reasonable
first approximation.

00:15:14.480 --> 00:15:17.100
That's what I'm saying,
not as an exact distribution.

00:15:19.677 --> 00:15:24.520
So why would the number of, just
intuitively, why would that be Poisson?

00:15:24.520 --> 00:15:30.170
Well, imagine that there
are a large number of people

00:15:30.170 --> 00:15:35.840
who potentially could email you in
that one hour block of time, okay?

00:15:35.840 --> 00:15:39.980
But for any individual person, unless you
have someone who's constantly emailing you

00:15:39.980 --> 00:15:44.120
like every few seconds all day,
in that particular block of time,

00:15:44.120 --> 00:15:48.010
it's fairly unlikely that any
specific person will email you.

00:15:48.010 --> 00:15:50.521
But if there are a lot of
people who could email you,

00:15:50.521 --> 00:15:52.517
then that's balancing it out, right?

00:15:52.517 --> 00:15:55.640
So we're defining success to be For
each person, whether or

00:15:55.640 --> 00:15:59.200
not they email you in that hour, there are
a lot of people who could email you, but

00:15:59.200 --> 00:16:02.440
each one is fairly unlikely,
that's the set up.

00:16:02.440 --> 00:16:05.140
Change email to phone calls,
or whatever you want here,

00:16:05.140 --> 00:16:07.050
you can make up as many
examples as you want.

00:16:08.120 --> 00:16:11.960
Another famous example is the number
of chocolate chips in a chocolate

00:16:11.960 --> 00:16:12.630
chip cookie.

00:16:13.730 --> 00:16:18.310
Number of chips in chocolate chip cookie,

00:16:19.350 --> 00:16:23.150
now let's think about how
do we interpret that one.

00:16:23.150 --> 00:16:30.170
Well it's like a cookie is made up of
bunches of, What do you call them?

00:16:31.320 --> 00:16:33.630
It's like stuff in the cookie, right?

00:16:33.630 --> 00:16:37.202
And most of the stuff is not chips, right?

00:16:37.202 --> 00:16:40.830
I guess start with some cookie dough and
there's some number of chips you put in.

00:16:40.830 --> 00:16:46.840
Now, each little bit of dough is
probably not a chocolate chip, right?

00:16:46.840 --> 00:16:50.760
It's very sad, but
there are a lot of them, right?

00:16:52.440 --> 00:16:56.740
In each little segment of the cookie,

00:16:56.740 --> 00:17:00.600
maybe a chip there, maybe a chip there,
maybe a chip there, most of them are not,

00:17:00.600 --> 00:17:04.250
but there are a lot of possible locations
on the cookie where there could a chip.

00:17:04.250 --> 00:17:08.450
So usually you at least get a few,
not enough but you get a few, okay?

00:17:08.450 --> 00:17:13.670
So that's why that's gonna be
approximately Poisson and so and so on and

00:17:13.670 --> 00:17:20.750
it can go on as many examples as you want.

00:17:20.750 --> 00:17:26.400
So this is, more serious examples
would be number of earthquakes,

00:17:28.790 --> 00:17:32.290
let's say in a year in some region.

00:17:33.760 --> 00:17:35.760
Again, I'm not saying
that's exactly Poisson, but

00:17:35.760 --> 00:17:40.720
that would be a reasonable first
choice of a distribution because

00:17:40.720 --> 00:17:43.800
on each particular day it's not
likely that there's an earthquake.

00:17:43.800 --> 00:17:49.503
But there are a lot of days in a year, and
so there may be a few earthquakes, right?

00:17:49.503 --> 00:17:51.405
So that kind of thing would
be approximately Poisson.

00:17:51.405 --> 00:17:52.948
We haven't proven that yet.

00:17:52.948 --> 00:17:56.351
We're gonna show,
not in complete generality, but

00:17:56.351 --> 00:17:59.828
we're gonna show a result
along these lines that shows

00:17:59.828 --> 00:18:03.775
where this distribution comes
from mathematically, okay?

00:18:03.775 --> 00:18:11.470
But right now this is just intuition and
how is this used.

00:18:11.470 --> 00:18:15.320
These examples, again I'm not saying
there are gonna be exactly Poisson.

00:18:15.320 --> 00:18:20.120
And in fact, if you look at a lot
of cases where the Poisson is used,

00:18:20.120 --> 00:18:24.120
actually that there's
some obvious upper bound,

00:18:24.120 --> 00:18:28.430
whereas here we're going up to
infinity that there's no upper limit.

00:18:28.430 --> 00:18:32.930
So most cases it's clear that the example
will not be exactly Poisson, but

00:18:32.930 --> 00:18:36.540
it's an extremely useful
approximation in a lot of problems.

00:18:36.540 --> 00:18:39.020
And that's called the Poisson paradigm.

00:18:43.635 --> 00:18:48.367
Or just more simply just call
it the Poisson approximation,

00:18:48.367 --> 00:18:52.280
Poisson paradigm, but
just to say that in words.

00:18:54.170 --> 00:18:55.867
Let's say we have events.

00:19:00.693 --> 00:19:05.807
A1, A2, blah, blah, blah, through A n.

00:19:05.807 --> 00:19:08.654
Suppose we have a lot of events, right?

00:19:08.654 --> 00:19:13.284
And they are with

00:19:13.284 --> 00:19:17.590
P(Aj) = Pj.

00:19:17.590 --> 00:19:19.900
So we have n is large.

00:19:21.760 --> 00:19:25.210
So we have a large number of
things that could happen, but

00:19:25.210 --> 00:19:26.610
each one is unlikely, right?

00:19:26.610 --> 00:19:27.530
That's what I was talking.

00:19:27.530 --> 00:19:32.300
I'm just trying to write this a little
more mathematically right now.

00:19:32.300 --> 00:19:37.375
And so the Pj's, they're all small, okay?

00:19:37.375 --> 00:19:40.129
And now we have to say
something about independence.

00:19:42.020 --> 00:19:49.591
So the events are either, the nicest
case is where they're independent.

00:19:49.591 --> 00:19:56.272
But actually this result is even more
general so they could be weakly dependent.

00:19:56.272 --> 00:19:59.230
We have a mathematical
definition of independence.

00:19:59.230 --> 00:20:03.010
It's very difficult to mathematically
define weakly dependent.

00:20:05.600 --> 00:20:11.846
The intuition is just that independence
comes in degrees to some extent.

00:20:11.846 --> 00:20:14.320
And independence says that.

00:20:14.320 --> 00:20:19.405
Learning, for example, learning A1
through A3 whether they happened or

00:20:19.405 --> 00:20:21.322
not gives us independence.

00:20:21.322 --> 00:20:25.860
let's say, it gives us no information
whatsoever about A4, A5, A6, and so

00:20:25.860 --> 00:20:27.217
on whether they occur.

00:20:27.217 --> 00:20:28.886
That's what independence would mean.

00:20:28.886 --> 00:20:33.300
Weak dependence would mean we could
get a little bit of information.

00:20:33.300 --> 00:20:36.250
We have to quantify what does it mean
to have a little bit of information.

00:20:36.250 --> 00:20:40.020
Well the easiest way to think of it would
just be that, let's say I knew that, for

00:20:40.020 --> 00:20:42.960
example, if knowing that A1 happened,

00:20:42.960 --> 00:20:47.650
suppose that tells us nothing
about whether A3 happened.

00:20:47.650 --> 00:20:49.960
But if we know that both A1 and
A2 happened,

00:20:49.960 --> 00:20:54.170
maybe it makes it a tiny bit more
likely or less likely that A3 happened.

00:20:54.170 --> 00:20:57.490
So it's slightly,
slight deviations from independent.

00:20:57.490 --> 00:20:58.620
That's what we're talking about.

00:21:01.400 --> 00:21:06.257
Then the claim is that
the number of events that occur,

00:21:06.257 --> 00:21:09.118
number of the Aj's that occur,

00:21:12.523 --> 00:21:17.015
Is approximately Poisson and
we can figure out right

00:21:17.015 --> 00:21:21.720
now what lambda would have to be for
that to make sense.

00:21:23.390 --> 00:21:28.010
We just showed that lambda is the expected
value of the Poisson, so it makes sense

00:21:28.010 --> 00:21:32.470
that lambda should be the expected number
of how many of these events occur.

00:21:32.470 --> 00:21:34.980
But by linearity,
even if they're dependent,

00:21:34.980 --> 00:21:39.760
by linearity, the expected number of
events that occur is the sum of the Pj's.

00:21:39.760 --> 00:21:41.609
So lambda should be the sum of the Pj's.

00:21:47.001 --> 00:21:49.480
Okay, this is approximate.

00:21:50.650 --> 00:21:53.910
So this is also called
the Poisson approximation.

00:21:53.910 --> 00:21:57.276
That's the same thing
as a Poisson paradigm.

00:22:00.761 --> 00:22:05.492
So one case that we've looked at a lot
is the case where all the events

00:22:05.492 --> 00:22:11.142
are independent and all the Pj's are the
same, they're all equal to the same P.

00:22:11.142 --> 00:22:14.120
In that case, we just have
Bernoulli trials with the same P.

00:22:14.120 --> 00:22:17.010
In that case, we know that
the number of events that are occur

00:22:17.010 --> 00:22:18.510
is exactly binomial in P.

00:22:19.600 --> 00:22:25.240
Okay, and in the minute we're gonna proof
that in fact the binomial NP does converge

00:22:25.240 --> 00:22:30.780
to a Poisson when we get N get large and
P gets small in a certain way, okay?

00:22:30.780 --> 00:22:33.420
But this is much much more general
than just for the binomial.

00:22:33.420 --> 00:22:37.102
Because this is saying that Poisson
is gonna work well even if the Ps

00:22:37.102 --> 00:22:38.497
are different, right?

00:22:38.497 --> 00:22:42.064
Remember for the binomial we had
independent trials with the same P.

00:22:42.064 --> 00:22:47.253
Here it says the Ps can be different and
they can be at least a little bit

00:22:47.253 --> 00:22:52.520
dependent and it's still gonna
be a good approximation, okay?

00:22:52.520 --> 00:22:57.536
So all right, so now let's explore
the connection with the binomial more.

00:22:59.684 --> 00:23:03.660
So what we wanna show is that
the binomial converges to the Poisson.

00:23:04.670 --> 00:23:09.344
So if we have, let's say X is Bin (n,p).

00:23:13.288 --> 00:23:18.583
And I wanna show how does this relate
to the Poisson cuz that formula for

00:23:18.583 --> 00:23:23.175
the Poisson, well it reminds us
of a Taylor series for eudx.

00:23:23.175 --> 00:23:25.738
But what does that have
to do with probability?

00:23:25.738 --> 00:23:29.490
Whereas, we know the binomial
is something very fundamental.

00:23:29.490 --> 00:23:36.420
So we start with the Bin (n,p) and
we're gonna let n go to infinity.

00:23:37.600 --> 00:23:40.840
That's just the mathematical way
of capturing the fact that we want

00:23:40.840 --> 00:23:42.810
a large number of events.

00:23:42.810 --> 00:23:44.940
So we wanna use this as an approximation,

00:23:44.940 --> 00:23:48.370
while in practice n is still
gonna be some finite number.

00:23:50.040 --> 00:23:53.590
And our hope is that here, mathematically
willing and go to an infinity.

00:23:53.590 --> 00:23:58.866
And as an approximation what we're hoping
is that n is Even though it's finite and

00:23:58.866 --> 00:24:04.220
is large enough so that this limits
kind of kick in and work well.

00:24:04.220 --> 00:24:08.310
So we're gonna let n go to infinity,
we're gonna let p go to 0.

00:24:08.310 --> 00:24:10.714
Now we're taking two limits here,

00:24:10.714 --> 00:24:15.213
that's the way to do something
that's mathematically precise.

00:24:15.213 --> 00:24:20.052
And in general this could
happen in different rates

00:24:20.052 --> 00:24:22.873
that we'd have to deal with.

00:24:22.873 --> 00:24:28.862
But the case we're most interested
in right now is where the product,

00:24:28.862 --> 00:24:32.933
let's call it lambda = np,
is held constant.

00:24:35.717 --> 00:24:38.259
And we can do more general
version than this, but

00:24:38.259 --> 00:24:40.123
this is a natural one to start with.

00:24:40.123 --> 00:24:43.141
Because remember the expected
value of binomial (n,

00:24:43.141 --> 00:24:47.613
p) is n times p, the expected value of
a Poisson lambda we just showed is lambda.

00:24:47.613 --> 00:24:51.858
So it makes sense to set lambda equal
to np to try to explore the connection

00:24:51.858 --> 00:24:52.839
between those.

00:24:52.839 --> 00:24:57.748
So this is telling us that p is going
to 0 at the same rate as n is going

00:24:57.748 --> 00:25:02.890
to infinity cuz we're holding
the product at constant.

00:25:02.890 --> 00:25:07.206
All right, so
now we wanna show that the PMF converges.

00:25:07.206 --> 00:25:11.596
So find what happens to the PMF.

00:25:17.710 --> 00:25:22.729
What happens to the P(X = k) which

00:25:22.729 --> 00:25:30.604
we keep seeing is (n k) p to
the k (1- p) to the n- k.

00:25:30.604 --> 00:25:33.472
So I wanna know what
happens to this thing?

00:25:33.472 --> 00:25:35.814
Well we're treating k as fixed.

00:25:38.996 --> 00:25:43.834
That is where we're letting n get very
large, p get very small, but then we wanna

00:25:43.834 --> 00:25:48.475
look at one specific value, for the PMF,
so we're letting k stay constant.

00:25:48.475 --> 00:25:49.840
All right, so that's what we want to do.

00:25:50.890 --> 00:25:55.815
And then at this point,
We just need to just

00:25:55.815 --> 00:25:59.210
do some algebra and
a little bit of calculus.

00:25:59.210 --> 00:26:03.264
So let's try to, right now this is
written as a function of p and n.

00:26:03.264 --> 00:26:06.800
It's gonna be easier to deal with,
if we get everything in terms of n.

00:26:06.800 --> 00:26:11.970
And so
conveniently we have p = lambda over n.

00:26:11.970 --> 00:26:14.484
So we can write everything in terms of n.

00:26:14.484 --> 00:26:17.188
And let me also write n
choose k a different way.

00:26:17.188 --> 00:26:21.892
n choose k carries the number of
ways to chose k people out of n

00:26:21.892 --> 00:26:24.345
where order doesn't matter.

00:26:24.345 --> 00:26:31.076
So that's the same thing as n.(n- 1)...,
(n- k + 1).

00:26:31.076 --> 00:26:36.308
That would be choosing a committee in
a certain order where order matters

00:26:36.308 --> 00:26:41.897
then we divide by K factorial to reflect
the fact that order doesn't matter.

00:26:41.897 --> 00:26:46.524
So that's just exactly the same thing as
n choose k, according to the story, or

00:26:46.524 --> 00:26:49.439
you can check that easily
just using factorials.

00:26:49.439 --> 00:26:56.240
I'll plug in p equals lambda over n.

00:26:56.240 --> 00:26:59.074
So that's lambda to the k over n to the k,
and

00:26:59.074 --> 00:27:01.770
I'll just stick the n to the k over there.

00:27:03.513 --> 00:27:07.419
And, then we need to deal with this thing,

00:27:07.419 --> 00:27:12.858
(1- p) which is 1 minus lambda over n,
to the n minus k.

00:27:12.858 --> 00:27:18.066
And, this kind of minus k thing is,
I'll just separate it out so

00:27:18.066 --> 00:27:21.790
we can deal with that later,
cuz k is fixed.

00:27:21.790 --> 00:27:23.520
It looks nicer to me this way.

00:27:23.520 --> 00:27:25.560
All right, so
that's just a little bit of algebra.

00:27:26.620 --> 00:27:28.467
And now let's take the limit of this.

00:27:35.402 --> 00:27:39.209
K is fixed, so k factorial here
just stays as k factorial.

00:27:42.904 --> 00:27:45.750
Now let's look at, and
lambda to the k is also fixed.

00:27:45.750 --> 00:27:48.445
So we have a lambda to
the k over k factorial.

00:27:50.590 --> 00:27:53.864
Which is pretty encouraging cuz
we want that for our Poisson.

00:27:55.505 --> 00:28:01.299
[COUGH] Now,
that's lambda to the k over k factorial.

00:28:01.299 --> 00:28:02.970
Let's look at this stuff with the ns.

00:28:04.080 --> 00:28:07.470
On top we have k terms.

00:28:07.470 --> 00:28:12.359
I've written a product of k terms
where it decreases by one each time.

00:28:12.359 --> 00:28:15.970
And the bottom is n
times n times n k times.

00:28:15.970 --> 00:28:17.590
So let's just match up terms.

00:28:17.590 --> 00:28:20.900
Take the first n from here,
match that with that n.

00:28:20.900 --> 00:28:23.380
The second one,
match it with the n minus 1,

00:28:23.380 --> 00:28:25.677
the third one with the n minus 2 and
so on.

00:28:25.677 --> 00:28:27.854
Each of those terms goes to one.

00:28:27.854 --> 00:28:32.144
Just for example,
one of them might be n minus seven over n.

00:28:32.144 --> 00:28:37.018
But I'm letting n go to infinity so
that the seven is completely negligible.

00:28:37.018 --> 00:28:40.830
So this stuff with
the n's just goes to one.

00:28:40.830 --> 00:28:47.863
Lastly, well, and in this part, n goes
to infinity so this is going to one.

00:28:47.863 --> 00:28:51.124
One to the negative k is one so
the exponent is fixed so

00:28:51.124 --> 00:28:52.764
this part just goes to 1.

00:28:52.764 --> 00:28:54.765
Lastly, we just need
to consider this part.

00:28:57.659 --> 00:29:03.000
That part I just have to remind
you of a general useful limit.

00:29:03.000 --> 00:29:08.520
At (1 + x /n) to the n,
goes to e to the x.

00:29:08.520 --> 00:29:12.878
Sometimes, this is even taken
as the definition of e to the x.

00:29:12.878 --> 00:29:16.216
This is probably the most
important limit for e to the x.

00:29:16.216 --> 00:29:19.055
I think of this as
the compound interest formula.

00:29:19.055 --> 00:29:23.890
Because if you have like money in a bank
that's being compounded a certain

00:29:23.890 --> 00:29:28.730
number of times per year, and if you
let the rate of compounding increase.

00:29:28.730 --> 00:29:32.352
In the limit, you have continuous
compounding of exponential growth.

00:29:32.352 --> 00:29:35.268
So if you haven't seen that limit before,

00:29:35.268 --> 00:29:40.055
I think it's on the math review, or
well Bow did it in his math review.

00:29:40.055 --> 00:29:42.902
Or just use Lopi take the log,
use Lopital's rule so

00:29:42.902 --> 00:29:44.660
that's a famous useful limit.

00:29:44.660 --> 00:29:47.221
So that goes to e to the x.

00:29:47.221 --> 00:29:51.401
So therefore this thing goes
to e to the minus lambda.

00:29:56.271 --> 00:29:58.329
And that's just the Poisson PMF.

00:30:03.604 --> 00:30:07.060
Evaluated at k, okay?

00:30:07.060 --> 00:30:14.753
So that shows that Binomials converge
to Poisson when we do it in this way.

00:30:18.744 --> 00:30:23.278
Another example that I like to think
of just as more intuition on this

00:30:23.278 --> 00:30:27.910
is counting the number of raindrops
that fall in some region.

00:30:27.910 --> 00:30:31.245
I don't know why I like this example, but
I just find it very, very intuitive for

00:30:31.245 --> 00:30:33.861
understanding the connection
between binomial and Poisson.

00:30:33.861 --> 00:30:36.110
So I'm just going to draw a quick picture.

00:30:36.110 --> 00:30:37.890
So this is the raindrop example.

00:30:40.188 --> 00:30:45.064
So imagine we have a piece of paper, which
I'll represent as just this rectangle.

00:30:45.064 --> 00:30:49.120
And imagine that well,
the board is vertical.

00:30:49.120 --> 00:30:51.382
But imagine we turn this horizontal.

00:30:51.382 --> 00:30:54.290
And then imagine we're outside and
it's raining on this.

00:30:54.290 --> 00:31:00.451
We wanna know how many raindrops hit this
piece of paper in say one minute, okay?

00:31:00.451 --> 00:31:05.921
Well, one way to think about that problem
would be to take this piece of paper and

00:31:05.921 --> 00:31:08.810
break it up into lots of little squares.

00:31:08.810 --> 00:31:14.288
So draw as many as you want, but
I'm just gonna draw like a grid.

00:31:14.288 --> 00:31:18.875
And imagine that we've broken
it up into like millions and

00:31:18.875 --> 00:31:22.780
millions of tiny little squares, okay?

00:31:22.780 --> 00:31:24.855
Now, for each individual,

00:31:24.855 --> 00:31:29.527
and I wanna count the number of
raindrops in some time interval.

00:31:29.527 --> 00:31:32.824
For each individual square,

00:31:36.374 --> 00:31:42.521
If we break this up into tiny enough
pieces, each individual square is unlikely

00:31:42.521 --> 00:31:48.800
to get a raindrop hitting exactly in that,
if we make them small enough, right.

00:31:48.800 --> 00:31:50.370
So each one is kind of unlikely, but

00:31:50.370 --> 00:31:52.990
we have a huge number of little squares,
okay?

00:31:52.990 --> 00:31:58.070
So we are gonna get some raindrops
hitting this piece of paper probably.

00:31:58.070 --> 00:31:59.240
All right?

00:31:59.240 --> 00:32:06.650
And lambda is gonna be a measure of the
intensity of how hard the rain is coming.

00:32:06.650 --> 00:32:12.705
And, so let's think, should we use
a binomial distribution for this?

00:32:12.705 --> 00:32:18.650
Well, if we assumed that everyone of these
squares whether it's get a drop of rain or

00:32:18.650 --> 00:32:21.790
not, is independent of all the others.

00:32:21.790 --> 00:32:25.130
And if we assume that they all
have the same probability P,

00:32:25.130 --> 00:32:28.450
then it would be exactly binomial, okay?

00:32:28.450 --> 00:32:32.040
But, I mean, I don't know enough
about rain to answer this or

00:32:32.040 --> 00:32:36.150
not, but I'm guessing it's not
really exactly independent.

00:32:36.150 --> 00:32:39.370
But it seems like a reasonable
approximation to treat them as

00:32:39.370 --> 00:32:39.960
independent.

00:32:41.050 --> 00:32:44.750
One other complication with the binomial,
with the binomial we would be assuming

00:32:44.750 --> 00:32:49.460
that each one of these squares
could only get zero or one.

00:32:49.460 --> 00:32:53.000
Now there's some tiny
chance that two raindrops

00:32:53.000 --> 00:32:55.450
could fall into one of these squares.

00:32:55.450 --> 00:33:01.010
So it's not gonna be exactly binomial,
but even if it were definitely binomial

00:33:01.010 --> 00:33:05.180
we had binomial of like a trillion,

00:33:05.180 --> 00:33:09.600
and then some tiny number that's
very very hard to work with.

00:33:09.600 --> 00:33:11.720
Certainly very hard to work with by hand.

00:33:11.720 --> 00:33:17.400
But even on a computers are gonna
have a lot of trouble handling

00:33:19.010 --> 00:33:25.230
even a thousand factorial you're gonna
run into some computational difficulties.

00:33:25.230 --> 00:33:28.430
Binomial has a lot of complications
to it where the poisson is much

00:33:28.430 --> 00:33:29.898
simpler to deal with.

00:33:29.898 --> 00:33:34.140
So a poisson seems reasonable here because
we have a huge number of little squares,

00:33:34.140 --> 00:33:35.080
each one is very unlikely.

00:33:36.510 --> 00:33:43.487
All right, so let's do one example, like
with the birthday problem type of setup.

00:33:46.311 --> 00:33:51.500
And you'll have one on the next homework,
homework five related to this too.

00:33:51.500 --> 00:33:57.020
Let's do the problem of
triple birthday matches,

00:33:57.020 --> 00:34:01.880
as we worked out the exact answer for
the birthday problem.

00:34:01.880 --> 00:34:05.260
What was the probability that at
least two people have the same

00:34:05.260 --> 00:34:08.160
birthday when you have a group of people,
okay?

00:34:08.160 --> 00:34:11.718
But now suppose we have, so

00:34:11.718 --> 00:34:17.303
the example is, let's say we have n people

00:34:20.246 --> 00:34:26.919
And we wanna know the probability that
there are 3 people with the same birthday.

00:34:30.015 --> 00:34:33.270
And we only need to do it approximately.

00:34:33.270 --> 00:34:38.520
In general, in this course,
assume that every answer you

00:34:38.520 --> 00:34:44.030
give needs to be exact unless I say,
you know give an approximation.

00:34:44.030 --> 00:34:47.772
In this case,
I'm saying find an approximation,

00:34:47.772 --> 00:34:52.655
of find the approximate probability
that there are three people.

00:34:55.643 --> 00:34:57.990
That is if you can find something,
maybe there's more than three.

00:34:57.990 --> 00:34:59.850
Maybe there's four people
of the same birthday but

00:34:59.850 --> 00:35:03.710
you can find some group of three people
who have the same birthday, okay?

00:35:03.710 --> 00:35:05.034
That's the problem.

00:35:07.872 --> 00:35:13.126
So if you try to do this in a way
that's how we solve the original

00:35:13.126 --> 00:35:18.484
basic birthday problem,
this is actually pretty difficult,

00:35:18.484 --> 00:35:24.160
very difficult you can try it,
but it's not gonna be nice, okay?

00:35:25.290 --> 00:35:30.300
But with the poisson
aproximation this is actually,

00:35:30.300 --> 00:35:33.719
should be pretty easy so
let's go through how to do that.

00:35:35.170 --> 00:35:39.210
So we want three people to
all have the same birthday,

00:35:42.273 --> 00:35:45.040
Let's just think about
what does that mean.

00:35:45.040 --> 00:35:46.900
And first of all,

00:35:48.010 --> 00:35:53.320
does this Poisson paradigm seem
appropriate for this problem?

00:35:54.435 --> 00:35:56.050
Okay, so lets think about that.

00:35:56.050 --> 00:36:02.020
Well, first I didn't say what n is,
so if n is 2 then it's 0.

00:36:02.020 --> 00:36:06.320
And if n is 3 then that's a very small
number that's not going to work so well.

00:36:06.320 --> 00:36:11.550
So assume n is reasonably
large,but n does not have to be.

00:36:12.750 --> 00:36:17.070
A key point here is that n does
not have to be very very large.

00:36:18.500 --> 00:36:21.590
Just like in the birthday problem,
part of the intuition,

00:36:21.590 --> 00:36:25.910
why do you only need 23 people
when there's 265 days in the year.

00:36:25.910 --> 00:36:30.110
Well 23 is a pretty small number,
but 23 choose 2,

00:36:30.110 --> 00:36:35.040
is 253 is reasonably large, right?

00:36:35.040 --> 00:36:38.700
So the more relevant
quantity is n choose 3.

00:36:38.700 --> 00:36:46.190
So even if n is, n should at least
be 10 or 20 or something like that.

00:36:46.190 --> 00:36:48.850
N does not have to be in the hundreds for
this to work well.

00:36:48.850 --> 00:36:52.350
Because even with some double digit n

00:36:52.350 --> 00:36:55.770
when you do n choose 3 it's
gonna be pretty large, right?

00:36:55.770 --> 00:36:59.246
Okay, so there are n choose
three triplets of people.

00:37:02.621 --> 00:37:03.442
All right?

00:37:03.442 --> 00:37:07.130
Assuming that the people, as usual,
we assume the people are labelled

00:37:07.130 --> 00:37:10.895
as 1 through n, take any subset of
three people, I call that a triplet.

00:37:10.895 --> 00:37:14.004
For each triplet you want can
ask the question does that

00:37:14.004 --> 00:37:18.680
triplet have the property all three of
them have the same birthday or not, right?

00:37:18.680 --> 00:37:22.201
So we would create an indicator
random variable for each one.

00:37:28.712 --> 00:37:34.301
Let's call that I sub ijk,
where if you have people ijk,

00:37:34.301 --> 00:37:37.580
where I is less than j less than k.

00:37:37.580 --> 00:37:40.720
So ijk or any numbers between 1 and

00:37:40.720 --> 00:37:46.050
n in increasing order just to avoid
repeating the same thing multiple times.

00:37:47.580 --> 00:37:51.420
Then we define this indicator we want if
they all have the same birthday, okay?

00:37:51.420 --> 00:37:53.610
So actually I didn't write it here but

00:37:53.610 --> 00:37:58.277
at this point we know that the expected
value we know the expected value exactly.

00:37:58.277 --> 00:38:03.274
The expected value of
the number of triple matches,

00:38:06.051 --> 00:38:10.701
We get that immediately by indicating
random variables linearly and symmetry.

00:38:10.701 --> 00:38:15.357
That is, it's n choose 3 of these
indicators, and for each one,

00:38:15.357 --> 00:38:21.060
now I'm imagining that I have the first
3 people, just for concreteness.

00:38:21.060 --> 00:38:23.210
First person can have whatever birthday,
and

00:38:23.210 --> 00:38:28.720
the second person has to match the first
person, that has probability 1 over 365.

00:38:28.720 --> 00:38:31.460
And then the third person
also has to match, so

00:38:31.460 --> 00:38:34.170
it's 1 over 365 squared, that's exact.

00:38:36.260 --> 00:38:38.840
So, that's the exact
number of triple matches.

00:38:38.840 --> 00:38:42.780
Notice though, that if there is a group of
four people who have the same birthday,

00:38:42.780 --> 00:38:46.860
we are counting that as,
for each set of three.

00:38:46.860 --> 00:38:50.400
So there are four choices of of
three people out of the four, and

00:38:50.400 --> 00:38:52.230
we're counting that as four matches.

00:38:54.010 --> 00:38:58.050
Okay, so that's the exact answer for
the expected value, but I said find

00:38:58.050 --> 00:39:04.000
the approximate probability, I didn't
say find the exact expected value, okay?

00:39:05.400 --> 00:39:10.355
So now we're gonna use
the Poisson approximation, and

00:39:10.355 --> 00:39:14.579
so let's let x equal
number of triple matches.

00:39:17.532 --> 00:39:21.878
There's no possible way that x
could b exactly poisson because

00:39:21.878 --> 00:39:26.142
the number of triple matches
couldn't be more than the number

00:39:26.142 --> 00:39:30.600
of triplets where the poisson
has no upper bound, okay?

00:39:30.600 --> 00:39:34.917
But we're claiming that
it's approximately Poisson.

00:39:37.195 --> 00:39:41.681
And so let's talk a little bit more
about why is that approximation valid.

00:39:41.681 --> 00:39:45.356
And lambda is what we just computed,
it's the expected value.

00:39:48.729 --> 00:39:52.360
So let's talk about why is that
a reasonable approximation.

00:39:52.360 --> 00:39:56.040
Again I'm assuming n is reasonably large,
it doesn't have to be very large.

00:39:58.150 --> 00:39:59.110
So why is that valid?

00:39:59.110 --> 00:40:03.594
Well, even if n is like 15 or 20,

00:40:03.594 --> 00:40:08.666
n choose 3 is pretty a big number, okay?

00:40:08.666 --> 00:40:13.400
So the number of possible,
the number of trials is fairly large.

00:40:13.400 --> 00:40:15.910
The probability of success for each trial,

00:40:15.910 --> 00:40:20.020
each trial is a group of three people,
is very small, right?

00:40:20.020 --> 00:40:21.690
Very, very unlikely, okay?

00:40:21.690 --> 00:40:24.730
So we have a large number of
possible triple birthday matches.

00:40:24.730 --> 00:40:26.490
Each one's unlikely.

00:40:26.490 --> 00:40:29.580
And then the other
question is independence.

00:40:29.580 --> 00:40:35.743
Well, we don't exactly have
independence here because,

00:40:35.743 --> 00:40:43.150
for example, I123 and
I124 are not independent, for example.

00:40:43.150 --> 00:40:47.962
Because if this event occurs, that is,

00:40:47.962 --> 00:40:52.349
if this equals 1, it means the first

00:40:52.349 --> 00:40:57.310
three people all have the same birthday.

00:40:57.310 --> 00:41:00.848
And now I want to know whether people
124 have the same birthday, well,

00:41:00.848 --> 00:41:02.600
we just kinda got a head start there.

00:41:02.600 --> 00:41:07.500
We already knew, if we know that this
event, if this indicator equals 1,

00:41:07.500 --> 00:41:09.950
then we're already kinda part way there.

00:41:09.950 --> 00:41:12.825
So they're not completely independent.

00:41:12.825 --> 00:41:17.550
But this will be an example of
weak dependence in the sense that

00:41:19.870 --> 00:41:23.320
the probabilities are small anyway,
first of all.

00:41:23.320 --> 00:41:29.020
And secondly, that's a head start, but
we still need person number four to match.

00:41:29.020 --> 00:41:33.310
And that's just like for a specific case
where I define these with some overlap.

00:41:33.310 --> 00:41:37.900
But, for example, I123 and
I456, those are independent.

00:41:37.900 --> 00:41:41.270
So there's not that much dependence there.

00:41:41.270 --> 00:41:44.320
So this seems like
a reasonable approximation.

00:41:44.320 --> 00:41:48.006
And then, to finish the calculation,
well, we would just say,

00:41:48.006 --> 00:41:51.907
we want to know what's the probability
of at least one triple match?

00:41:53.828 --> 00:41:56.656
And as if often the case,
it's easier to do the complements.

00:41:56.656 --> 00:42:01.850
So that's 1 minus the probability
that x equals zero.

00:42:01.850 --> 00:42:06.830
And as an approximation,
that's gonna be 1 minus

00:42:06.830 --> 00:42:11.320
e to the minus lambda,
lambda to the zero over zero factorial.

00:42:11.320 --> 00:42:15.558
Which, of course,
it just 1 minus e to the minus lambda.

00:42:15.558 --> 00:42:20.970
So it's 1 minus e to the minus lambda,
where lambda is this.

00:42:20.970 --> 00:42:24.000
So notice that this is something
that you could calculate

00:42:24.000 --> 00:42:27.840
easily using a calculator or a computer.

00:42:27.840 --> 00:42:30.260
You don't have to do like
some complicated sum and

00:42:30.260 --> 00:42:32.950
evaluate a lot of binomial
coefficients and all that stuff.

00:42:32.950 --> 00:42:38.400
So this is very, very useful for
getting nice quick approximations.

00:42:38.400 --> 00:42:40.240
All right, so
that's the Poisson distribution,

00:42:40.240 --> 00:42:41.980
that's our last discrete distribution.

00:42:41.980 --> 00:42:43.760
So congratulations on that.

00:42:43.760 --> 00:42:45.380
And I'll see you next time.

