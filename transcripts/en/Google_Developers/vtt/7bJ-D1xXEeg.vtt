WEBVTT
Kind: captions
Language: en

00:00:05.040 --> 00:00:06.450
Thank you, everyone,
for coming today.

00:00:06.450 --> 00:00:07.880
My name is Colt McAnlis.

00:00:07.880 --> 00:00:11.090
I'm a Developer Advocate at
Google focusing on gaming and

00:00:11.090 --> 00:00:13.030
web technologies in general.

00:00:13.030 --> 00:00:16.079
You see my email address here,
colton@google.com.

00:00:16.079 --> 00:00:17.800
And we are here today to
talk about a very,

00:00:17.800 --> 00:00:19.560
very important topic.

00:00:19.560 --> 00:00:21.370
We're here to talk about
texture compression.

00:00:21.370 --> 00:00:24.070
Now before we get too started,
I'm going to need to see a

00:00:24.070 --> 00:00:24.700
show of hands.

00:00:24.700 --> 00:00:27.570
How many of you in here have
actually shipped a product, a

00:00:27.570 --> 00:00:29.290
video game, on a console?

00:00:29.290 --> 00:00:33.680
So Xbox 360, PS3, Nintendo
DS for that matter?

00:00:33.680 --> 00:00:34.180
OK, good.

00:00:34.180 --> 00:00:35.670
So out of those hands--

00:00:35.670 --> 00:00:37.110
you can put them down and
we'll get a new set--

00:00:37.110 --> 00:00:39.890
out of those hands, how many of
you have ever fought with

00:00:39.890 --> 00:00:41.976
texture footprint in memory?

00:00:41.976 --> 00:00:42.890
OK, Good.

00:00:42.890 --> 00:00:43.510
Good, good, good.

00:00:43.510 --> 00:00:47.100
So obviously, you know that
compressing your textures is a

00:00:47.100 --> 00:00:49.700
big problem and that, most
importantly, you spend a lot

00:00:49.700 --> 00:00:53.100
of time trying to fight your
memory constraints against

00:00:53.100 --> 00:00:54.150
what your artists
are producing.

00:00:54.150 --> 00:00:55.860
Well today, we're here to talk
about that specifically.

00:00:55.860 --> 00:00:58.440
And what we want to talk about
is DXT is not enough.

00:00:58.440 --> 00:01:00.820
Now I know there's hardware
architecture differences with

00:01:00.820 --> 00:01:03.540
compressed textures and
PVR and ECT and

00:01:03.540 --> 00:01:04.190
all these other things.

00:01:04.190 --> 00:01:06.950
So I hope you take what we're
talking about today and how

00:01:06.950 --> 00:01:08.940
it's being applied
to DXT texturing.

00:01:08.940 --> 00:01:10.660
And I hope you apply it to some
of these other things

00:01:10.660 --> 00:01:12.260
that may be platform-specific.

00:01:12.260 --> 00:01:14.350
So all right, let's
get started then.

00:01:14.350 --> 00:01:17.590
So texture footprints matters
for games, as we've already

00:01:17.590 --> 00:01:18.280
talked about this.

00:01:18.280 --> 00:01:21.130
How many times have you been
down to the wire or down to

00:01:21.130 --> 00:01:24.260
memory and needed an extra five
megabytes for animations

00:01:24.260 --> 00:01:25.760
or 10 megabytes for sound?

00:01:25.760 --> 00:01:28.810
And you go back and you do your
statistic gathering and

00:01:28.810 --> 00:01:32.110
find that your losing 50% or
60% of your entire memory

00:01:32.110 --> 00:01:33.880
footprint just for
texture assets.

00:01:33.880 --> 00:01:36.260
And there's tons of different
reasons for this.

00:01:36.260 --> 00:01:38.480
They could need to
be high-res.

00:01:38.480 --> 00:01:40.390
Or there could be too
many MIP chains.

00:01:40.390 --> 00:01:43.540
Or the artists author them at
2048's when they only occupy a

00:01:43.540 --> 00:01:46.200
64x64 pixel set on the screen.

00:01:46.200 --> 00:01:47.480
Whatever the reasons, right?

00:01:47.480 --> 00:01:51.010
For us, this is a big problem
because, as a console

00:01:51.010 --> 00:01:53.590
developer, you spend a lot
of time fighting this.

00:01:53.590 --> 00:01:56.070
Retail doesn't really care
a lot about this, though.

00:01:56.070 --> 00:01:57.110
There's a great article--

00:01:57.110 --> 00:01:59.580
I forget which site posted
it-- where John Carmack

00:01:59.580 --> 00:02:03.000
actually went to the head of
their publisher at the time

00:02:03.000 --> 00:02:05.670
and said, hey, I've got an
amazing new technology.

00:02:05.670 --> 00:02:08.160
It's called MegaTexture.

00:02:08.160 --> 00:02:09.030
And it's awesome.

00:02:09.030 --> 00:02:10.630
You can stream in
infinite detail.

00:02:10.630 --> 00:02:11.610
And it's fantastic.

00:02:11.610 --> 00:02:15.590
The problem is that we're now
looking at 17 gigabytes of

00:02:15.590 --> 00:02:17.560
data just for one game.

00:02:17.560 --> 00:02:20.070
And in the article, it actually
says that the head of

00:02:20.070 --> 00:02:21.990
the publisher said, hey,
that's fantastic.

00:02:21.990 --> 00:02:22.490
I love it.

00:02:22.490 --> 00:02:25.790
This will push our Blu-ray
sales for video games.

00:02:25.790 --> 00:02:30.170
The point of this is that, for
retail developers, that's

00:02:30.170 --> 00:02:31.180
good, right?

00:02:31.180 --> 00:02:33.540
They're giving out this physical
medium and pushing

00:02:33.540 --> 00:02:36.920
this physical medium to consoles
and to PC developers

00:02:36.920 --> 00:02:39.020
the 17-gig Blu-rays.

00:02:39.020 --> 00:02:40.210
This is great.

00:02:40.210 --> 00:02:45.790
But for us digital distribution
outlets, as well

00:02:45.790 --> 00:02:48.680
as consumers in general, this
isn't a good thing.

00:02:48.680 --> 00:02:51.440
Waiting for 17 to 40 gigabytes
worth of data to play your

00:02:51.440 --> 00:02:53.740
game is not good at all.

00:02:53.740 --> 00:02:54.630
It takes time.

00:02:54.630 --> 00:02:58.260
It creates a lag between what
the user's expectation is and

00:02:58.260 --> 00:03:00.130
pay rate is to when
they actually

00:03:00.130 --> 00:03:01.140
experience the content.

00:03:01.140 --> 00:03:04.550
And there's a large gap there
that is directly correlated to

00:03:04.550 --> 00:03:07.150
how many users buy your game,
how many users continue to

00:03:07.150 --> 00:03:09.480
play your game, and how many
users just get stopped at the

00:03:09.480 --> 00:03:10.040
front door.

00:03:10.040 --> 00:03:12.310
And as we all know, especially
the Indies that may be in the

00:03:12.310 --> 00:03:14.180
audience today, that's
huge, right?

00:03:14.180 --> 00:03:16.200
You want these people
giving you money.

00:03:16.200 --> 00:03:19.800
And creating barriers to their
experience is a bad thing.

00:03:19.800 --> 00:03:22.920
So we talk about texture
compression systems because we

00:03:22.920 --> 00:03:25.940
want to avoid 17 gigabytes,
right?

00:03:25.940 --> 00:03:27.500
We want to move down.

00:03:27.500 --> 00:03:29.550
You need to look at
it as a triangle.

00:03:29.550 --> 00:03:32.520
For instance, this is a nice
little understanding here.

00:03:32.520 --> 00:03:34.610
And again, we're talking about
texture decompression, so

00:03:34.610 --> 00:03:36.620
we're talking about the runtime

00:03:36.620 --> 00:03:38.400
decompressing of textures.

00:03:38.400 --> 00:03:41.150
It's kind of a performance
triangle where you need to

00:03:41.150 --> 00:03:44.400
optimize, based upon your game
and your title, between the

00:03:44.400 --> 00:03:47.380
quality of the decompressed
image, the size and the

00:03:47.380 --> 00:03:49.930
footprint of the in-memory
representation and then how

00:03:49.930 --> 00:03:52.980
fast you can actually decompress
that data.

00:03:52.980 --> 00:03:56.330
And depending on your game,
you'll find a different point

00:03:56.330 --> 00:03:58.080
on this triangle.

00:03:58.080 --> 00:04:03.160
So for some of Id stuff,
you may be more towards

00:04:03.160 --> 00:04:05.600
decompression speed and
size than quality.

00:04:05.600 --> 00:04:09.680
For FarmVille, obviously, you
would want quality and size as

00:04:09.680 --> 00:04:10.550
a better metric.

00:04:10.550 --> 00:04:12.300
Maybe speed is not as
big of an issue.

00:04:12.300 --> 00:04:13.200
Who knows?

00:04:13.200 --> 00:04:15.860
But you really need to take
this triangle into thought

00:04:15.860 --> 00:04:17.610
when you're talking about
decompression and algorithms

00:04:17.610 --> 00:04:18.070
in general.

00:04:18.070 --> 00:04:19.450
And hopefully, you'll think
about this while we're talking

00:04:19.450 --> 00:04:21.910
about our stuff today too.

00:04:21.910 --> 00:04:23.930
So the state-of-the-art for this
process, let's talk about

00:04:23.930 --> 00:04:25.330
what you all are
already doing.

00:04:25.330 --> 00:04:26.120
So you should all know this.

00:04:26.120 --> 00:04:29.070
You should all be familiar with
this as we start forward.

00:04:29.070 --> 00:04:32.230
So the way that people handle
textures today is the artist

00:04:32.230 --> 00:04:35.290
usually generates some PSD
file in Photoshop.

00:04:35.290 --> 00:04:39.300
This PSD file is converted to a
DXT texture, either through

00:04:39.300 --> 00:04:41.350
Microsoft's tool, or
NVIDIA tools, or

00:04:41.350 --> 00:04:44.460
Compressonator from AMD, ATI.

00:04:44.460 --> 00:04:47.550
And then that data is usually
just thrown, batch process

00:04:47.550 --> 00:04:50.170
ad-hoc, into a zip file or
some sort of compressed,

00:04:50.170 --> 00:04:52.210
binary archive.

00:04:52.210 --> 00:04:55.640
And then that archive is then
put on disk or put into

00:04:55.640 --> 00:04:57.420
memory, shipped out
with the product.

00:04:57.420 --> 00:05:00.270
And when a level load or the
game loads, usually what

00:05:00.270 --> 00:05:04.450
happens is the zip
file is opened up

00:05:04.450 --> 00:05:06.400
in memory at runtime.

00:05:06.400 --> 00:05:08.850
The entire zip archive is either
loaded into memory or

00:05:08.850 --> 00:05:10.710
kept on disk and then
streamed from.

00:05:10.710 --> 00:05:13.290
And that data is copied right
from the memory, or right from

00:05:13.290 --> 00:05:15.560
the compressed version,
right to the GPU.

00:05:15.560 --> 00:05:18.180
So the DXT data that's in the
archive and in memory just

00:05:18.180 --> 00:05:20.970
gets, effectively,
mem copied over.

00:05:20.970 --> 00:05:22.590
And this is usually fine.

00:05:22.590 --> 00:05:26.340
But the problem here is that you
don't want to always keep

00:05:26.340 --> 00:05:29.410
this process going because,
when you open your zip

00:05:29.410 --> 00:05:31.730
archive, you either keep the
entire thing in memory or you

00:05:31.730 --> 00:05:35.060
keep it all on disc and
stream from it.

00:05:35.060 --> 00:05:37.150
And the point here is that you
should not keep your entire

00:05:37.150 --> 00:05:37.970
zip in archive.

00:05:37.970 --> 00:05:41.920
I remember, in my previous life,
we were putting out a

00:05:41.920 --> 00:05:43.920
console title on the Xbox 360.

00:05:43.920 --> 00:05:47.060
And we had archives
around four gigs.

00:05:47.060 --> 00:05:49.900
Well, the problem was the
hardware we were working on

00:05:49.900 --> 00:05:52.550
only had 512 megs of RAM that
we could actually work with.

00:05:52.550 --> 00:05:55.160
So obviously, keeping the entire
thing in memory is not

00:05:55.160 --> 00:05:56.330
a good idea.

00:05:56.330 --> 00:05:59.260
On the other side of this,
actually hitting the optical

00:05:59.260 --> 00:06:03.450
disk to stream in your data is
not always available as well.

00:06:03.450 --> 00:06:06.130
In addition to that, some of the
data that exists in your

00:06:06.130 --> 00:06:08.680
archive is not actually
needed.

00:06:08.680 --> 00:06:11.100
So for instance, let's say
you're loading a level, right?

00:06:11.100 --> 00:06:13.680
When you load a level, there's
going to be lots of data.

00:06:13.680 --> 00:06:16.270
There's going to be bounding
volume hierarchies, quad

00:06:16.270 --> 00:06:17.740
trees, navigational meshes.

00:06:17.740 --> 00:06:20.000
There's going to be animations
for characters.

00:06:20.000 --> 00:06:22.015
Some of this stuff can actually
be loaded once and

00:06:22.015 --> 00:06:24.810
then thrown away, while
other items and other

00:06:24.810 --> 00:06:27.420
representations need to be
loaded and continuously used.

00:06:27.420 --> 00:06:31.010
And this is where you get the
divide between load and forget

00:06:31.010 --> 00:06:32.380
and then load and
continue using

00:06:32.380 --> 00:06:36.070
between data and textures.

00:06:36.070 --> 00:06:39.050
So you kind of go, OK, well, if
I have a zip archive that

00:06:39.050 --> 00:06:43.790
represents level seven and I
only need to use the first 25%

00:06:43.790 --> 00:06:46.470
during load time, and then the
rest of it I need at runtime

00:06:46.470 --> 00:06:49.220
for textures or whatever,
then it makes sense to

00:06:49.220 --> 00:06:50.240
segment that out.

00:06:50.240 --> 00:06:52.480
And say, OK, well, here's
the pre-load

00:06:52.480 --> 00:06:53.870
stuff for level seven.

00:06:53.870 --> 00:06:56.490
And then, here's the runtime
stuff for level seven.

00:06:56.490 --> 00:06:58.240
And then there's sort of a
rabbit hole that, once you

00:06:58.240 --> 00:07:00.060
start thinking about this, you
go down this rabbit hole.

00:07:00.060 --> 00:07:01.790
And you end up in
utter madness.

00:07:01.790 --> 00:07:04.130
But you don't get the free tea
or the cool hat or the rabbit

00:07:04.130 --> 00:07:04.590
running around.

00:07:04.590 --> 00:07:08.320
But you get the craziness of
the experience because what

00:07:08.320 --> 00:07:11.230
happens is you start to bin-sort
all of your content

00:07:11.230 --> 00:07:13.950
around and figure out these
optimal solutions.

00:07:13.950 --> 00:07:15.340
That's really just madness.

00:07:15.340 --> 00:07:18.590
Really, what you should be doing
is you should find a way

00:07:18.590 --> 00:07:21.530
to hyper compress your textures
so that they turn

00:07:21.530 --> 00:07:24.370
into a load-once solution.

00:07:24.370 --> 00:07:26.860
And the rest of this talk is
going to be focused explicitly

00:07:26.860 --> 00:07:27.830
on this sort of technique.

00:07:27.830 --> 00:07:30.290
The idea here is that you're
going to want to hyper

00:07:30.290 --> 00:07:32.860
compress your textures in some
way that you can load them

00:07:32.860 --> 00:07:35.910
into main memory and keep them
in main memory for the entire

00:07:35.910 --> 00:07:38.130
duration of the game, or the
level, or what you need.

00:07:38.130 --> 00:07:41.600
And then, on-demand, decompress
them from whatever

00:07:41.600 --> 00:07:44.170
format they're in
to a GPU format.

00:07:44.170 --> 00:07:46.020
And then throw them
away, if you need.

00:07:46.020 --> 00:07:49.240
And this is actually a very
powerful instance, right?

00:07:49.240 --> 00:07:52.500
IDtech5 has made waves
with this.

00:07:52.500 --> 00:07:55.210
I've been hearing about
MegaTexture for almost six

00:07:55.210 --> 00:07:57.490
years, I think, seven years
has been the big thing.

00:07:57.490 --> 00:08:00.130
And RAGE has finally shipped.

00:08:00.130 --> 00:08:05.090
And you can go play the game
and see the RAGE of IDtech5

00:08:05.090 --> 00:08:06.380
and what happens.

00:08:06.380 --> 00:08:08.220
Basically, they realized
that this was a

00:08:08.220 --> 00:08:09.220
problem early on, right?

00:08:09.220 --> 00:08:11.070
They had tons and tons
of texture data.

00:08:11.070 --> 00:08:14.110
And the way that they handled
this was they stored the

00:08:14.110 --> 00:08:16.240
textures in this hyper
compressed format.

00:08:16.240 --> 00:08:20.200
Basically, some of their early
papers by Van Waveren--

00:08:20.200 --> 00:08:22.570
you can find them on the
Intel Developer site--

00:08:22.570 --> 00:08:26.060
actually talk about creating a
variant of JPEG, which is a

00:08:26.060 --> 00:08:30.610
DCT block-based format, and
actually compressing them to

00:08:30.610 --> 00:08:32.380
DXT at runtime.

00:08:32.380 --> 00:08:35.120
So basically, offline, they
would compress them into this

00:08:35.120 --> 00:08:36.929
lossy-style format, DCT.

00:08:36.929 --> 00:08:39.330
And then, at runtime, they'd
take that DCT data and then

00:08:39.330 --> 00:08:41.970
convert it over to DXT, which
was then mem copied to the

00:08:41.970 --> 00:08:43.360
GPU, right?

00:08:43.360 --> 00:08:45.830
The cool thing about this is,
according to their research,

00:08:45.830 --> 00:08:48.860
they were getting about 112
megapixels or megatexles a

00:08:48.860 --> 00:08:50.670
second on dual core.

00:08:50.670 --> 00:08:53.730
That's a lot of megapixels,
megatexles a second.

00:08:53.730 --> 00:08:56.670
That's a lot of texture data to
be effectively streaming,

00:08:56.670 --> 00:08:59.210
converting and moving around.

00:08:59.210 --> 00:09:01.320
Some of the down-sides of this,
though, aren't really

00:09:01.320 --> 00:09:01.980
that published.

00:09:01.980 --> 00:09:04.030
But when you actually start
implementing that algorithm

00:09:04.030 --> 00:09:06.970
and realizing what's going on
here, or if you've spend a lot

00:09:06.970 --> 00:09:09.650
of time with texture
compression, you first realize

00:09:09.650 --> 00:09:11.620
that this is very processor
intensive.

00:09:11.620 --> 00:09:14.550
You're spending a lot of
CPU time redoing the

00:09:14.550 --> 00:09:15.820
same sort of work.

00:09:15.820 --> 00:09:19.440
You're taking a DCT texture,
converting it to DXT.

00:09:19.440 --> 00:09:21.730
In addition to that, this
actually converts and

00:09:21.730 --> 00:09:24.640
introduces 2 times the
amount of noise.

00:09:24.640 --> 00:09:28.630
DCT actually is a lossy codec.

00:09:28.630 --> 00:09:29.610
It's a lossy form.

00:09:29.610 --> 00:09:32.070
So what happens is, when you
compress your texture, it's

00:09:32.070 --> 00:09:34.590
going to introduce noise
into the texture.

00:09:34.590 --> 00:09:38.130
And then, when you decompress
the texture to compress it to

00:09:38.130 --> 00:09:41.240
DXT, DXT is also
a lossy format.

00:09:41.240 --> 00:09:44.170
So what's happening is you're
introducing error, reverting

00:09:44.170 --> 00:09:46.470
that error, but keeping it
around to some respect and

00:09:46.470 --> 00:09:48.050
then introducing new
error again.

00:09:48.050 --> 00:09:50.990
So you're actually getting 2
times the amount of error in

00:09:50.990 --> 00:09:53.350
your image during the
compression process.

00:09:53.350 --> 00:09:56.040
In addition to that, because
of the fact that the DXT

00:09:56.040 --> 00:09:59.440
conversion is happening at
runtime, it means that you

00:09:59.440 --> 00:10:02.840
can't spend a ton of time
optimizing for how your DXT

00:10:02.840 --> 00:10:04.440
blocks should be considered,
right?

00:10:04.440 --> 00:10:07.430
Basically, you have to resort
to a box fit or a best fit

00:10:07.430 --> 00:10:10.480
algorithm, which might not
actually produce the best

00:10:10.480 --> 00:10:12.200
color correlation ratio.

00:10:12.200 --> 00:10:13.820
There's tons of algorithms
out there.

00:10:13.820 --> 00:10:16.960
We're going to talk about some
today that allow you to get a

00:10:16.960 --> 00:10:20.030
better quality, if you're
working at it offline.

00:10:20.030 --> 00:10:20.730
So here's the idea.

00:10:20.730 --> 00:10:22.470
Here the whole point of this
talk that I'd like to talk

00:10:22.470 --> 00:10:23.210
about today.

00:10:23.210 --> 00:10:25.410
So what if we approach this
from a different angle?

00:10:25.410 --> 00:10:30.330
What if, instead of taking an
existing compressed form,

00:10:30.330 --> 00:10:33.330
decompressing it and translating
it into DXT at

00:10:33.330 --> 00:10:36.920
runtime, what if we started with
the DXT data initially?

00:10:36.920 --> 00:10:40.030
And we took that DXT data, and
we hyper compressed it?

00:10:40.030 --> 00:10:43.270
So we hyper compress the DXT
data more, such that, at

00:10:43.270 --> 00:10:47.140
runtime, all we're doing is a
decompression step to get us

00:10:47.140 --> 00:10:49.250
our original DXT input.

00:10:49.250 --> 00:10:50.840
So this is the idea.

00:10:50.840 --> 00:10:53.020
The idea is that we don't
introduce any new error data

00:10:53.020 --> 00:10:55.300
with this because it's just
going from something

00:10:55.300 --> 00:10:57.320
compressed back to DXT.

00:10:57.320 --> 00:10:59.610
And then we can actually store
this in memory, hyper

00:10:59.610 --> 00:11:00.160
compressed.

00:11:00.160 --> 00:11:03.090
So we load it from our zip file,
keep it in some section

00:11:03.090 --> 00:11:05.080
of memory that we're not
using all the time.

00:11:05.080 --> 00:11:07.500
And then we decompress directly
to DXT whenever our

00:11:07.500 --> 00:11:11.070
view frustum changes, or we have
a cache invalidation, or

00:11:11.070 --> 00:11:14.600
we do a flush, or something
along those lines.

00:11:14.600 --> 00:11:19.200
So in order to test if this is
the right direction to go, we

00:11:19.200 --> 00:11:23.010
need some sort of test images
to prove both decompression

00:11:23.010 --> 00:11:25.550
time and size and speeds
and whatnot like that.

00:11:25.550 --> 00:11:30.240
So anytime you're doing image
compression block tests, you

00:11:30.240 --> 00:11:32.980
have to produce some sort of
conical representation

00:11:32.980 --> 00:11:33.650
to take a look at.

00:11:33.650 --> 00:11:35.800
So what I've created is
actually a random

00:11:35.800 --> 00:11:36.730
collection of images.

00:11:36.730 --> 00:11:38.640
All images behave differently.

00:11:38.640 --> 00:11:42.150
I think it would be
irresponsible on my side to

00:11:42.150 --> 00:11:43.650
just use one type of image.

00:11:43.650 --> 00:11:46.900
So I've actually included a
bunch of images in my set from

00:11:46.900 --> 00:11:47.320
different things.

00:11:47.320 --> 00:11:48.220
Some are from games--

00:11:48.220 --> 00:11:51.200
the actual PSD, the source
images, the TGAs uncompressed.

00:11:51.200 --> 00:11:53.010
So there's no noise
being added there.

00:11:53.010 --> 00:11:54.370
Some are public.

00:11:54.370 --> 00:11:56.880
For example, the famous Lena
image, which, if you don't

00:11:56.880 --> 00:12:00.500
know who Lena is in respect to
compression, please go take a

00:12:00.500 --> 00:12:01.540
look at it.

00:12:01.540 --> 00:12:03.060
And some are from
image libraries.

00:12:03.060 --> 00:12:05.100
The Kodak Image Library
is actually fantastic.

00:12:05.100 --> 00:12:07.720
It's a lot of high-resolution
images that are great to test

00:12:07.720 --> 00:12:10.230
against your own compression
algorithms.

00:12:10.230 --> 00:12:12.320
And then all of the numbers I'm
going to be showing today

00:12:12.320 --> 00:12:14.480
for these compression processes
actually include the

00:12:14.480 --> 00:12:16.860
DDS headers, which is
about 128 bytes.

00:12:16.860 --> 00:12:20.050
So this is going to skew
the results slightly.

00:12:20.050 --> 00:12:22.770
But if you take that into
account, you have to have the

00:12:22.770 --> 00:12:25.240
header of the texture, how big
it is, how many MIPs it is,

00:12:25.240 --> 00:12:27.510
what the format is, all
these other things.

00:12:27.510 --> 00:12:28.690
So take that in.

00:12:28.690 --> 00:12:32.070
And then, any percentage values
that I produce on my

00:12:32.070 --> 00:12:35.770
slides are actually in
amount of reduction.

00:12:35.770 --> 00:12:39.100
This is basically saying, how
much has been reduced from the

00:12:39.100 --> 00:12:43.020
original format so that you're
seeing this large delta to see

00:12:43.020 --> 00:12:45.340
what we're saving as all of
these things come together.

00:12:48.580 --> 00:12:50.190
So before we move on,
let's take a look at

00:12:50.190 --> 00:12:51.410
a typical DXT block.

00:12:51.410 --> 00:12:54.100
And for the sake of this talk,
I'm going to focus on DXT1

00:12:54.100 --> 00:12:56.240
because, I think, understanding
it from here

00:12:56.240 --> 00:12:59.640
allows us to launch off the
different algorithms, DXT3, 5,

00:12:59.640 --> 00:13:01.590
DXTN, all these other
sorts of things.

00:13:01.590 --> 00:13:03.930
So a DXT block is constructed
likewise.

00:13:03.930 --> 00:13:06.720
First off, it starts by storing
a high color and a low

00:13:06.720 --> 00:13:08.560
color in a 5-6-5 bit poly.

00:13:08.560 --> 00:13:11.660
That's red, green, blue
5-bit, 6-bits, 5 bits.

00:13:11.660 --> 00:13:14.160
What these two colors represent
in space is

00:13:14.160 --> 00:13:15.780
effectively the endpoints
of a line.

00:13:15.780 --> 00:13:20.260
You can see that, these two
large circles here.

00:13:20.260 --> 00:13:22.520
Then what you end up with is,
with these two high- and

00:13:22.520 --> 00:13:25.710
low-colors, you end up with
a 4x4 block of pixels.

00:13:25.710 --> 00:13:28.030
These 4x4 block effectively
only contains

00:13:28.030 --> 00:13:30.290
two bits per cell.

00:13:30.290 --> 00:13:33.930
And the two bits actually
represent a discrete step

00:13:33.930 --> 00:13:37.580
between these two endpoints
in color space, right?

00:13:37.580 --> 00:13:40.190
So you can see on the image
here, we've got some random

00:13:40.190 --> 00:13:42.460
assortment of colors that
are off the line.

00:13:42.460 --> 00:13:44.440
And effectively, they
get snapped to one

00:13:44.440 --> 00:13:46.040
of these four values.

00:13:46.040 --> 00:13:48.450
This is the heart of
DXT compression.

00:13:48.450 --> 00:13:52.060
It allows for a really fast
load, really fast filtering.

00:13:52.060 --> 00:13:54.485
It allows you to keep a lot of
blocks in a mem cache line on

00:13:54.485 --> 00:13:56.840
the GPU, which is really
important.

00:13:56.840 --> 00:13:58.590
And in general, it doesn't
actually reduce

00:13:58.590 --> 00:13:59.480
the quality too much.

00:13:59.480 --> 00:14:03.610
But it is lossy, so it does
introduce some error.

00:14:03.610 --> 00:14:06.940
So let's look at the initial
data set for DXT.

00:14:06.940 --> 00:14:10.040
The uncompressed images
are around 37 megs.

00:14:10.040 --> 00:14:12.540
This is just the source image,
non-compressed, right?

00:14:12.540 --> 00:14:15.820
And I could have chosen a
bigger data set size.

00:14:15.820 --> 00:14:19.190
But every time I would run
this set, it takes 15, 20

00:14:19.190 --> 00:14:20.700
minutes to actually run
and get the results.

00:14:20.700 --> 00:14:22.870
And I've got stuff to do today,
so I can't have a

00:14:22.870 --> 00:14:24.470
200-megabyte set.

00:14:24.470 --> 00:14:26.680
DXT1, if I just take the source
data and compress it

00:14:26.680 --> 00:14:30.800
into DXT1, it gets it down to
about 7.6 megs, which is good.

00:14:30.800 --> 00:14:31.970
That's fantastic.

00:14:31.970 --> 00:14:34.170
That's exactly the type of
compression size you want to

00:14:34.170 --> 00:14:35.720
see out of DXT.

00:14:35.720 --> 00:14:38.480
If we zip that data, so we do
whatever everyone in the room

00:14:38.480 --> 00:14:39.240
is already doing.

00:14:39.240 --> 00:14:40.750
We just take that DXT
data, throw into an

00:14:40.750 --> 00:14:41.900
archive, and zip it.

00:14:41.900 --> 00:14:46.490
That ends up at about 4.82
megs, or about 36%, 37%

00:14:46.490 --> 00:14:48.080
savings, which is good.

00:14:48.080 --> 00:14:49.460
It means we're getting
that in addition

00:14:49.460 --> 00:14:51.430
to what we're getting.

00:14:51.430 --> 00:14:54.540
Now as a form of comparison,
just to look at things a

00:14:54.540 --> 00:14:58.050
different way, if we actually
compressed each of the DXT

00:14:58.050 --> 00:15:02.590
images individually by zipping
them, we get about 5.1 megs.

00:15:02.590 --> 00:15:04.230
Now notice this is higher.

00:15:04.230 --> 00:15:07.120
The reason for this is because
we're adding all of the

00:15:07.120 --> 00:15:08.650
additional zip data overhead.

00:15:08.650 --> 00:15:12.090
So we're adding the headers,
the span chunks, all these

00:15:12.090 --> 00:15:13.610
other fun things.

00:15:13.610 --> 00:15:16.130
And there's some other
interesting nuances of why

00:15:16.130 --> 00:15:18.780
that footprint is higher that
I'll get to a little bit later

00:15:18.780 --> 00:15:19.770
in the talk.

00:15:19.770 --> 00:15:22.540
The goal for this talk, though,
is to beat this.

00:15:22.540 --> 00:15:26.900
We want our compression
algorithm to match what zip is

00:15:26.900 --> 00:15:30.130
creating, so that our in memory
version is the same

00:15:30.130 --> 00:15:32.160
footprint size as what
we would expect

00:15:32.160 --> 00:15:33.190
our of our zip file.

00:15:33.190 --> 00:15:34.740
So the goal is can
we get to that?

00:15:34.740 --> 00:15:38.680
Can we beat 36.83%
for our hyper

00:15:38.680 --> 00:15:40.750
compressed format in memory?

00:15:40.750 --> 00:15:42.640
To do this, I'm going to present
you a bag of tricks.

00:15:42.640 --> 00:15:45.830
And these are all separate
items. I'm going to talk about

00:15:45.830 --> 00:15:49.020
how each one of these items
changes the footprint size,

00:15:49.020 --> 00:15:51.260
the memory size, and then
present them individually so

00:15:51.260 --> 00:15:53.910
that you can mix and match them,
depending on your input.

00:15:53.910 --> 00:15:56.230
So for lossless algorithms, so
these are algorithms that do

00:15:56.230 --> 00:15:59.100
not introduce any new errors
into the image.

00:15:59.100 --> 00:16:01.190
The first one we're going to
cover is de-interleaving.

00:16:01.190 --> 00:16:02.320
I'll talk about that
in a minute.

00:16:02.320 --> 00:16:04.120
The next one, of course,
is Huffman compression.

00:16:04.120 --> 00:16:06.150
Everyone in here, if you have
a basic computer science

00:16:06.150 --> 00:16:08.280
degree, you should have written
a Huffman before.

00:16:08.280 --> 00:16:09.970
The next one is delta
encoding.

00:16:09.970 --> 00:16:12.210
This is sort of a tricky
thing that not a lot of

00:16:12.210 --> 00:16:13.230
people are aware of.

00:16:13.230 --> 00:16:14.890
And then, finally, we're going
to talk about codebooks and

00:16:14.890 --> 00:16:17.000
what codebooks are and
how you can use them.

00:16:17.000 --> 00:16:20.060
Now to really get some amazing
results, we're going to also

00:16:20.060 --> 00:16:23.000
talk about one specific
lossy technique.

00:16:23.000 --> 00:16:25.840
And this is actually going to
be an expanding blocks, aka

00:16:25.840 --> 00:16:28.490
region of interest-based
finding pattern.

00:16:28.490 --> 00:16:30.450
And I think you'll love what
we come out of that.

00:16:30.450 --> 00:16:31.755
But we're going to save
until the end because

00:16:31.755 --> 00:16:33.480
it's the big review.

00:16:33.480 --> 00:16:35.310
So let's start the top
with de-interleaving.

00:16:35.310 --> 00:16:38.120
Again, as mentioned before, here
is a standard DXT block.

00:16:38.120 --> 00:16:40.720
You've got your high-color and
your low-color and your 4x4

00:16:40.720 --> 00:16:42.070
set of selector bits.

00:16:42.070 --> 00:16:46.150
Now in memory, in GPU memory,
CPU memory and on-disk in a

00:16:46.150 --> 00:16:49.540
DDS format, effectively,
you concatenate

00:16:49.540 --> 00:16:50.810
these blocks in memory.

00:16:50.810 --> 00:16:53.330
So you've got your high-color,
low-color selector bits,

00:16:53.330 --> 00:16:55.540
high-color, low-color selector
bits, high-color, low-color

00:16:55.540 --> 00:16:56.540
selector bits.

00:16:56.540 --> 00:16:59.010
The ideal with de-interleaving
is what if we actually

00:16:59.010 --> 00:17:00.130
separated those out?

00:17:00.130 --> 00:17:03.160
What if we put all of our high
and low colors together, and

00:17:03.160 --> 00:17:05.990
then put all of the selector
bits together?

00:17:05.990 --> 00:17:10.099
The intent here is that, for
zip compression, which is

00:17:10.099 --> 00:17:15.140
basically a modified version
of LZW, what they create is

00:17:15.140 --> 00:17:16.400
actually a window.

00:17:16.400 --> 00:17:18.280
And they say, we're
going to analyze.

00:17:18.280 --> 00:17:20.660
And we're going to deflate--

00:17:20.660 --> 00:17:22.160
it's another compression
algorithm-- we're going to

00:17:22.160 --> 00:17:24.750
deflate everything within this
window, and then move on to

00:17:24.750 --> 00:17:27.119
the next window and deflate
everything in that window.

00:17:27.119 --> 00:17:29.590
So by de-interleaving our
data, putting all of our

00:17:29.590 --> 00:17:31.720
colors and then all of our
selector bits in, effectively,

00:17:31.720 --> 00:17:35.700
two separate bins, what we're
trying to do here is optimize

00:17:35.700 --> 00:17:38.240
for the zip compression
format.

00:17:38.240 --> 00:17:41.160
So we're trying to pack as much
duplicate data into each

00:17:41.160 --> 00:17:44.320
one of those sliding windows
as we can to, hopefully,

00:17:44.320 --> 00:17:46.570
increase the amount of
compression we get.

00:17:46.570 --> 00:17:48.840
So let's check out what
that looks like.

00:17:48.840 --> 00:17:51.990
So with DXTi, I've got each
one of these techniques

00:17:51.990 --> 00:17:55.050
segmented out with a different
enunciation at the top.

00:17:55.050 --> 00:17:58.440
So de-interleaving, so again
the DXT1 original footprint

00:17:58.440 --> 00:18:00.350
was 7.63 megs.

00:18:00.350 --> 00:18:03.500
The interleaved version
is 7.63 megs.

00:18:03.500 --> 00:18:05.470
Now again, no compression
has occurred here.

00:18:05.470 --> 00:18:09.260
All we've done here is just
moved around the selector bits

00:18:09.260 --> 00:18:12.380
and color bits so that they're
homogeneous in

00:18:12.380 --> 00:18:13.800
their sorting here.

00:18:13.800 --> 00:18:15.910
Now the zip here is fantastic.

00:18:15.910 --> 00:18:18.250
We've actually went
to 43% savings.

00:18:18.250 --> 00:18:21.070
Now if you remember, this is
basically about a 10%, I

00:18:21.070 --> 00:18:21.850
think, savings here.

00:18:21.850 --> 00:18:22.640
This is fantastic.

00:18:22.640 --> 00:18:23.780
This is the end result.

00:18:23.780 --> 00:18:28.110
So simply by de-interleaving
your data, you get about 10%

00:18:28.110 --> 00:18:31.040
savings there, which is good.

00:18:31.040 --> 00:18:33.760
A 10%, 15% savings there,
which is fantastic.

00:18:33.760 --> 00:18:36.010
And this is optimized, so
this is exactly what

00:18:36.010 --> 00:18:37.400
we wanted to see.

00:18:37.400 --> 00:18:38.080
So that's good.

00:18:38.080 --> 00:18:40.480
The next question is, can we
reduce the amount of unique

00:18:40.480 --> 00:18:43.010
data in the sliding window
even further?

00:18:43.010 --> 00:18:46.040
Such that zip can compress
it even further?

00:18:46.040 --> 00:18:48.140
To do that, we're going to look
at a common compression

00:18:48.140 --> 00:18:49.850
technique called Huffman.

00:18:49.850 --> 00:18:52.830
Now Huffman is effectively a
dictionary creation system.

00:18:52.830 --> 00:18:56.530
So dictionary systems all work
in a very specific way.

00:18:56.530 --> 00:19:00.260
Effectively, they create a
dictionary symbol of unique

00:19:00.260 --> 00:19:02.200
values that exist
in the stream.

00:19:02.200 --> 00:19:05.210
And then they replace the
instance of that symbol in the

00:19:05.210 --> 00:19:08.430
data stream with a
minimal bit-code

00:19:08.430 --> 00:19:09.740
representation of that.

00:19:09.740 --> 00:19:12.410
So for instance, Morse code is
a perfect example of that.

00:19:12.410 --> 00:19:14.570
They analyzed the English
language.

00:19:14.570 --> 00:19:16.600
And they said, E is the most
common character in the

00:19:16.600 --> 00:19:17.570
English language.

00:19:17.570 --> 00:19:19.140
Therefore, we're going
to represent E

00:19:19.140 --> 00:19:20.390
with a single beep.

00:19:20.390 --> 00:19:23.190
So then, any time you're doing
scans and whatnot, E is

00:19:23.190 --> 00:19:25.740
represented in the least
value possible.

00:19:25.740 --> 00:19:29.070
For an example of this, let's
say we've got this string,

00:19:29.070 --> 00:19:32.840
four A's, two B's and a C, which
is, with 8-bit ASCII is

00:19:32.840 --> 00:19:35.970
about 56 bits, right?

00:19:35.970 --> 00:19:39.870
In Huffman, if you just looked
at the compression stream, A

00:19:39.870 --> 00:19:41.110
is going to be the most
common symbol.

00:19:41.110 --> 00:19:44.480
So it's only going to
get the zero bit.

00:19:44.480 --> 00:19:46.020
B is the second most symbol,
so it's going

00:19:46.020 --> 00:19:46.720
to get the one bits.

00:19:46.720 --> 00:19:48.480
And then C is the third
symbol here, so it's

00:19:48.480 --> 00:19:49.460
going to get two bits.

00:19:49.460 --> 00:19:51.240
So we go from 56
bits to 8 bits.

00:19:51.240 --> 00:19:53.010
And I there's some of you in the
audience who are saying,

00:19:53.010 --> 00:19:54.670
actually, that should be
a little bit different,

00:19:54.670 --> 00:19:56.960
depending on which Huffman
version you use.

00:19:56.960 --> 00:19:58.310
We're going to use this
is an illustration.

00:19:58.310 --> 00:20:01.980
Please consult Wikipedia for
all the different versions

00:20:01.980 --> 00:20:03.010
that you can find
about Huffman.

00:20:03.010 --> 00:20:05.750
It's actually staggering how
many different versions of

00:20:05.750 --> 00:20:06.470
Huffman are out there.

00:20:06.470 --> 00:20:09.970
So make sure you're tuning
that as you see fit.

00:20:09.970 --> 00:20:11.230
So let's look at what
this did to us.

00:20:11.230 --> 00:20:12.830
So we've got DXTih.

00:20:12.830 --> 00:20:16.210
So this is de-interleaving
plus Huffman.

00:20:16.210 --> 00:20:18.640
And also, I want to point out
at the top here, I'm noting

00:20:18.640 --> 00:20:22.970
that Huffman and the usage of
Huffman requires you to define

00:20:22.970 --> 00:20:24.170
a symbol size.

00:20:24.170 --> 00:20:29.150
Now for this test here, I've
actually chosen 16-bit symbols

00:20:29.150 --> 00:20:32.800
for my colors and 8-bit symbols
for my selectors.

00:20:32.800 --> 00:20:35.520
And this has a big bearing on
how things work, right?

00:20:35.520 --> 00:20:38.650
If I were to use 16-bit for
selectors, I get less value.

00:20:38.650 --> 00:20:41.450
If I use 8-bits for colors,
I get less compression, et

00:20:41.450 --> 00:20:41.960
cetera, et cetera.

00:20:41.960 --> 00:20:43.280
So let's take a look at this.

00:20:43.280 --> 00:20:46.330
So the standard again,
7.63 megs for DXT.

00:20:46.330 --> 00:20:50.140
DXTdih, so de-interleaved
with Huffman

00:20:50.140 --> 00:20:51.500
encoding, is about 40%.

00:20:51.500 --> 00:20:53.680
So this is not as good
as we saw before.

00:20:53.680 --> 00:20:57.490
We're introducing a little bit
of noise into the stream here.

00:20:57.490 --> 00:20:59.640
When you add in the
zip, though,

00:20:59.640 --> 00:21:02.690
you get back to about--

00:21:02.690 --> 00:21:05.290
we only get about 1% savings
by adding the Huffman.

00:21:05.290 --> 00:21:07.670
So we didn't get much benefit.

00:21:07.670 --> 00:21:10.340
One important note here is
though, that again, this is

00:21:10.340 --> 00:21:12.760
optimized for your
colors up top.

00:21:12.760 --> 00:21:16.110
And basically, what
I was doing was

00:21:16.110 --> 00:21:17.640
optimizing for my set.

00:21:17.640 --> 00:21:19.840
Now if you take that into
account, what you should

00:21:19.840 --> 00:21:25.090
really be doing is finding
the optimal bit encoding.

00:21:25.090 --> 00:21:26.980
For what I did this test,
I just used a

00:21:26.980 --> 00:21:28.225
8-bit row, 4x4 block.

00:21:28.225 --> 00:21:32.680
So I just took the 4x4 and just
made that one symbol.

00:21:32.680 --> 00:21:34.930
In reality, you should scan your
data input to determine

00:21:34.930 --> 00:21:37.130
what the optimal combination
provides.

00:21:37.130 --> 00:21:38.230
So the blog, [? Seibey ?]

00:21:38.230 --> 00:21:39.020
Live--

00:21:39.020 --> 00:21:41.160
you can see the link at the
bottom of the slide there--

00:21:41.160 --> 00:21:44.720
did a similar test where they
basically figured out what the

00:21:44.720 --> 00:21:46.760
multiple pairings were to
determine the best symbol

00:21:46.760 --> 00:21:49.880
orientation for this exact
type of compression.

00:21:49.880 --> 00:21:51.730
In their particular data set
that they were using, they

00:21:51.730 --> 00:21:54.650
found that 2x2 blocks of
selector bits produced the

00:21:54.650 --> 00:21:55.670
highest number of duplicate
symbols.

00:21:55.670 --> 00:22:00.010
You can see that as the high
value there in the frequency.

00:22:00.010 --> 00:22:01.780
In reality, before you actually
compress your data,

00:22:01.780 --> 00:22:02.930
you should be doing this.

00:22:02.930 --> 00:22:05.060
You should be going through, per
texture, finding out what

00:22:05.060 --> 00:22:07.590
the optimal solution is
and compressing that.

00:22:07.590 --> 00:22:10.430
And in reality, you should all
have build farms in here.

00:22:10.430 --> 00:22:12.340
This is the modern gaming age.

00:22:12.340 --> 00:22:14.390
You should have some ability to
kick off a nightly build to

00:22:14.390 --> 00:22:16.280
a farm of servers that
bake all your data.

00:22:16.280 --> 00:22:19.300
So the processing involved
with generating this data

00:22:19.300 --> 00:22:22.490
should not be that
important to you.

00:22:22.490 --> 00:22:24.550
So Huffman provided great
reduction for the base stream,

00:22:24.550 --> 00:22:26.850
but didn't provide too much
help with our zip

00:22:26.850 --> 00:22:27.540
compression, right?

00:22:27.540 --> 00:22:29.980
So we got our base data
compressed very nicely.

00:22:29.980 --> 00:22:32.730
But when you put zip on top
of it, we didn't see

00:22:32.730 --> 00:22:33.410
too much of a help.

00:22:33.410 --> 00:22:34.260
Now this is OK.

00:22:34.260 --> 00:22:35.580
This is what we want, right?

00:22:35.580 --> 00:22:38.790
We want that base, non-zipped,
compressed data to start

00:22:38.790 --> 00:22:41.960
getting lower so
it matches zip.

00:22:41.960 --> 00:22:44.140
So delta encoding, let's
talk about this.

00:22:44.140 --> 00:22:46.830
Delta encoding will effectively
encode a stream of

00:22:46.830 --> 00:22:50.010
data by replacing each element
in the stream with a value

00:22:50.010 --> 00:22:52.720
that represents the data
from the previous sum.

00:22:52.720 --> 00:22:56.590
The goal here is to reduce the
dynamic range of symbols that

00:22:56.590 --> 00:22:58.650
could exist in the
stream, right?

00:22:58.650 --> 00:23:01.850
And so Huffman encoding will
basically replace the symbols

00:23:01.850 --> 00:23:04.780
in the stream, based upon
frequency, with minimal bit

00:23:04.780 --> 00:23:05.900
representations.

00:23:05.900 --> 00:23:09.210
Delta encoding will scan through
the string before that

00:23:09.210 --> 00:23:13.640
and, basically, truncate,
concatenate and quantize to

00:23:13.640 --> 00:23:17.340
create more of the low-band,
unique signals.

00:23:17.340 --> 00:23:18.680
A perfect example
of this this.

00:23:18.680 --> 00:23:22.520
So we've got this string here,
or this set of numbers, 155,

00:23:22.520 --> 00:23:24.050
156, yada, yada, yada.

00:23:24.050 --> 00:23:27.420
Now if we just threw this at
Huffman, the only symbols that

00:23:27.420 --> 00:23:29.250
we would find here as duplicates
are basically, the

00:23:29.250 --> 00:23:31.630
157's right there.

00:23:31.630 --> 00:23:33.870
Huffman would say that's the
most frequent symbol, that

00:23:33.870 --> 00:23:35.310
would get the lowest bit-set.

00:23:35.310 --> 00:23:38.140
Now if we run this through delta
encoding first, what

00:23:38.140 --> 00:23:43.180
happens is we create 155,
1, 1, 0, 0, 64, 1, 3.

00:23:43.180 --> 00:23:45.840
So basically, what you're seeing
here is that what we do

00:23:45.840 --> 00:23:47.610
is we say, the first
symbol is 155.

00:23:47.610 --> 00:23:51.010
The next symbol is add one.

00:23:51.010 --> 00:23:53.610
The next symbol to that is
add one to that value.

00:23:53.610 --> 00:23:56.660
The next symbol to that is add
zero, add zero, add 64, add

00:23:56.660 --> 00:23:57.490
one, add three.

00:23:57.490 --> 00:24:00.470
What we've done here,
effectively, is increased the

00:24:00.470 --> 00:24:05.500
number of duplicate symbols in
the stream by encoding the

00:24:05.500 --> 00:24:08.230
delta between the values, as
opposed to the actual values

00:24:08.230 --> 00:24:08.560
themselves.

00:24:08.560 --> 00:24:12.470
So this decreases the dynamic
range of the stream.

00:24:12.470 --> 00:24:14.150
So let's see how this
did for us.

00:24:14.150 --> 00:24:18.090
So DXTihd with delta encoding.

00:24:18.090 --> 00:24:22.660
Base, again, 7.63 megs,
non-zipped.

00:24:22.660 --> 00:24:25.410
So just taking the DXT data
itself about 41%.

00:24:25.410 --> 00:24:27.720
That's good, right?

00:24:27.720 --> 00:24:29.940
And about 45% reduction for
the zipped version.

00:24:29.940 --> 00:24:31.590
And that's great, right?

00:24:31.590 --> 00:24:33.790
The problem here is is we're
starting to see this plateau.

00:24:33.790 --> 00:24:35.710
We're not getting
huge results by

00:24:35.710 --> 00:24:38.930
squeezing this stream itself.

00:24:38.930 --> 00:24:41.220
It might be worth looking at
a different direction.

00:24:41.220 --> 00:24:45.070
And with that, we take
a look at codebooks.

00:24:45.070 --> 00:24:48.380
Effectively, we create a
codebook by scanning the DXT

00:24:48.380 --> 00:24:51.370
image and create a codebook
that lists all the unique

00:24:51.370 --> 00:24:53.440
symbols, sort of like the
dictionary method.

00:24:53.440 --> 00:24:57.540
And then we delta encode
those unique values.

00:24:57.540 --> 00:24:59.370
Then in the DST block stream--

00:24:59.370 --> 00:25:00.750
so basically, we create
all the this

00:25:00.750 --> 00:25:01.640
unique stream of colors.

00:25:01.640 --> 00:25:02.440
We delta encode that.

00:25:02.440 --> 00:25:05.680
And then, in the block stream,
we go back and we list a

00:25:05.680 --> 00:25:09.780
256-bit index into
this codebook.

00:25:09.780 --> 00:25:12.620
Now, of course, some of you in
the audience should say, hey,

00:25:12.620 --> 00:25:13.020
wait a minute.

00:25:13.020 --> 00:25:14.820
What if you've got more
than 256 colors?

00:25:14.820 --> 00:25:15.760
That's OK.

00:25:15.760 --> 00:25:19.990
What we actually do is we
create a sliding window

00:25:19.990 --> 00:25:22.870
approach to ensure that you'll
always have a 256-bit index.

00:25:22.870 --> 00:25:26.090
So basically, what we do is, for
the entire list of colors

00:25:26.090 --> 00:25:28.290
in your codebook, you basically
say, I'm going to

00:25:28.290 --> 00:25:31.570
create a window of the
first 256 values.

00:25:31.570 --> 00:25:34.995
Those will be numbered zero to
255, then the next set of 256,

00:25:34.995 --> 00:25:38.050
and then the next set of 256,
and the next set of 256.

00:25:38.050 --> 00:25:41.880
And then, in your actual index
data stream, you're going to

00:25:41.880 --> 00:25:45.050
make a reference into the proper
bin of 256 values.

00:25:45.050 --> 00:25:46.680
And there's some interesting
ways to make

00:25:46.680 --> 00:25:47.960
sure that this works.

00:25:47.960 --> 00:25:49.240
There's lots of stuff
on Wikipedia.

00:25:49.240 --> 00:25:51.490
I would highly recommend you go
take a look at that because

00:25:51.490 --> 00:25:55.660
I don't have time today to
explain the specific nuances.

00:25:55.660 --> 00:25:59.690
So with that, we actually got
some different results.

00:25:59.690 --> 00:26:00.560
It gave some really good ones.

00:26:00.560 --> 00:26:04.220
So first off, we're at 46%
without zip compression--

00:26:04.220 --> 00:26:05.750
that's pretty amazing--

00:26:05.750 --> 00:26:08.010
and 49% with zip compression.

00:26:08.010 --> 00:26:11.520
So we're still sort of chasing
the dragon of zip compression

00:26:11.520 --> 00:26:13.570
and trying to track that down.

00:26:13.570 --> 00:26:16.280
But again, we're not seeing the
huge jumps in savings that

00:26:16.280 --> 00:26:16.880
we're expecting.

00:26:16.880 --> 00:26:18.360
Or we haven't broken that 50%.

00:26:18.360 --> 00:26:22.110
And there's got to be a
better way to do that.

00:26:22.110 --> 00:26:24.350
And with that, we're going to
talk about expanding blocks.

00:26:24.350 --> 00:26:27.010
Now this is a lossy technique,
but I think you're going to

00:26:27.010 --> 00:26:27.720
like the results.

00:26:27.720 --> 00:26:29.470
And so that's why I want
to bring it up today.

00:26:29.470 --> 00:26:33.620
So the point here is that
adjacent DXT cells often share

00:26:33.620 --> 00:26:35.060
color profiles.

00:26:35.060 --> 00:26:38.840
So 4x4 and the block next to it
and the block below it and

00:26:38.840 --> 00:26:41.660
block in an adjacent direction
to it, they're--

00:26:41.660 --> 00:26:44.980
the way that artists and
pictures work, the number of

00:26:44.980 --> 00:26:47.260
pixels we have on a screen,
chances are, those colors are

00:26:47.260 --> 00:26:48.130
all correlated.

00:26:48.130 --> 00:26:51.210
So what if we used an 8x8
cell instead of a 4x4?

00:26:51.210 --> 00:26:52.210
Just hypothetically?

00:26:52.210 --> 00:26:54.440
We stored one high- and
low-color per 8x8.

00:26:54.440 --> 00:26:58.610
And then we stored six
64 two-bit selectors.

00:26:58.610 --> 00:27:00.050
What would this do
to our image?

00:27:00.050 --> 00:27:02.190
What would the results be?

00:27:02.190 --> 00:27:04.010
Well, let's take
a look at that.

00:27:04.010 --> 00:27:06.260
What you're seeing on the screen
here is two images.

00:27:06.260 --> 00:27:08.270
The left is the original.

00:27:08.270 --> 00:27:13.920
The right has been compressed
with 8x8 blocks instead of 4x4

00:27:13.920 --> 00:27:14.710
DXT blocks.

00:27:14.710 --> 00:27:17.090
From this distance, you
really shouldn't

00:27:17.090 --> 00:27:18.980
see much of a problem.

00:27:18.980 --> 00:27:22.240
You can see a little bit of
banding, a little bit

00:27:22.240 --> 00:27:25.270
quantization, maybe some blocks
for the full image.

00:27:25.270 --> 00:27:27.620
But in general, you shouldn't
see anything wrong.

00:27:27.620 --> 00:27:29.770
It's not until we zoom in that
you actually start seeing

00:27:29.770 --> 00:27:31.530
problems. And even here, it's
pretty hard to notice.

00:27:31.530 --> 00:27:33.200
I want to point out where
the errors are here.

00:27:33.200 --> 00:27:35.300
If you look at the top of the
bird's beak, you can see some

00:27:35.300 --> 00:27:36.920
blocking artifacts there.

00:27:36.920 --> 00:27:40.250
You can see some graininess at
the bottom of the background.

00:27:40.250 --> 00:27:42.630
You see the quantization of
the steps of colors there.

00:27:42.630 --> 00:27:44.480
But in general, this
image performs

00:27:44.480 --> 00:27:47.000
very well at 8x8 blocks.

00:27:47.000 --> 00:27:48.710
Here's another example.

00:27:48.710 --> 00:27:50.980
This is from the Kodak
suite as well.

00:27:50.980 --> 00:27:52.670
At this distance, you
really shouldn't

00:27:52.670 --> 00:27:54.520
see much of a problem.

00:27:54.520 --> 00:27:57.700
Again, this is 8x8 block
compressed, right?

00:27:57.700 --> 00:28:00.010
We actually see less problems
than the parrot because

00:28:00.010 --> 00:28:02.170
there's a lot less smooth
gradients in this image.

00:28:02.170 --> 00:28:05.715
And there's more general
sub-noise, which helps hide

00:28:05.715 --> 00:28:07.150
the DXT artifacts.

00:28:07.150 --> 00:28:09.230
Now when we zoom in on her
chin, you actually start

00:28:09.230 --> 00:28:11.850
seeing the problems
pretty badly.

00:28:11.850 --> 00:28:14.700
Here, you actually see the block
and the color truncation

00:28:14.700 --> 00:28:16.430
on her necklace as well as some

00:28:16.430 --> 00:28:18.430
clipping on her chin there.

00:28:18.430 --> 00:28:20.250
You can actually see these
problems. But again, you have

00:28:20.250 --> 00:28:21.860
to zoom in all the way
to look at these.

00:28:21.860 --> 00:28:24.210
Now if someone's chasing you
around with a rocket launcher

00:28:24.210 --> 00:28:26.960
at 60 frames a second, you're
probably not going to care.

00:28:26.960 --> 00:28:29.070
Here's a trick question.

00:28:29.070 --> 00:28:31.390
Which one's compressed?

00:28:31.390 --> 00:28:33.160
Now this is an in-game
texture.

00:28:33.160 --> 00:28:34.290
So this is something
you would see.

00:28:34.290 --> 00:28:36.200
This is something that artists
would generate to put in your

00:28:36.200 --> 00:28:36.830
environment.

00:28:36.830 --> 00:28:39.220
Which one of these
is compressed?

00:28:39.220 --> 00:28:41.430
Well, of course, with this
normal trend, the one on the

00:28:41.430 --> 00:28:43.850
left is the original and the
one on the right is the

00:28:43.850 --> 00:28:45.290
compressed version.

00:28:45.290 --> 00:28:47.330
But could you really tell,
especially with

00:28:47.330 --> 00:28:48.360
this type of image?

00:28:48.360 --> 00:28:51.790
Even when we zoom in, can you
tell which one of these has

00:28:51.790 --> 00:28:55.390
been quantized 8x8 blocks
and which hasn't?

00:28:55.390 --> 00:28:56.850
So let's look at the
results here.

00:28:56.850 --> 00:28:58.800
This is awesome.

00:28:58.800 --> 00:29:01.730
With 8x8 blocks explicitly--
so we're using 8x8 blocks

00:29:01.730 --> 00:29:02.340
everywhere--

00:29:02.340 --> 00:29:05.600
we actually end up with our
zipped and our non-zipped

00:29:05.600 --> 00:29:11.100
memory footprint at exactly the
same size, 67.7% savings,

00:29:11.100 --> 00:29:14.050
taking the original 37
megs down to 2.46.

00:29:14.050 --> 00:29:16.430
This is exactly what we were
looking for in this research.

00:29:16.430 --> 00:29:18.990
We were trying to find a way
to make the uncompressed

00:29:18.990 --> 00:29:21.700
version in memory match what
we get out of a zip.

00:29:21.700 --> 00:29:24.050
And we did it by combining
Huffman encoding,

00:29:24.050 --> 00:29:27.230
deinterlacing, and
doing 8x8 blocks.

00:29:27.230 --> 00:29:28.270
It's fantastic.

00:29:28.270 --> 00:29:30.420
Oh, as well as codebooks.

00:29:30.420 --> 00:29:32.280
So now let's talk about
everyone else in here.

00:29:32.280 --> 00:29:33.340
So you remember that
triangle, right?

00:29:33.340 --> 00:29:34.360
So everyone else in
here is say well,

00:29:34.360 --> 00:29:34.910
what about the triangle?

00:29:34.910 --> 00:29:38.230
We've gotten speed, and
we've gotten quality--

00:29:38.230 --> 00:29:40.130
or we've gotten compression size
and we've gotten quality.

00:29:40.130 --> 00:29:41.850
Now let's talk about speed.

00:29:41.850 --> 00:29:45.310
So for this style I just
showed you, which is

00:29:45.310 --> 00:29:47.750
de-interleaving, Huffman
and [? styles ?]

00:29:47.750 --> 00:29:53.230
delta encoding of codebooks with
8x8 blocks, using CS101

00:29:53.230 --> 00:29:54.150
Huffman and delta encoding.

00:29:54.150 --> 00:29:55.690
So I didn't do anything
special here.

00:29:55.690 --> 00:29:57.620
I didn't spend any time
optimizing my Huffman

00:29:57.620 --> 00:29:59.430
decompression or my
delta encoding.

00:29:59.430 --> 00:30:01.210
I just threw it in there.

00:30:01.210 --> 00:30:06.530
With about 67.8% savings,
I was getting about 73

00:30:06.530 --> 00:30:09.020
megapixels a second
on a single core.

00:30:09.020 --> 00:30:11.150
So this was one core doing this
compression, which is

00:30:11.150 --> 00:30:12.040
fantastic, right?

00:30:12.040 --> 00:30:14.400
If you go to two cores,
that's 114.

00:30:14.400 --> 00:30:16.610
Three cores, you start getting
into some of the numbers that

00:30:16.610 --> 00:30:18.220
you start seeing here.

00:30:18.220 --> 00:30:22.000
The cool thing here is that
that's 1.32 bits per pixel.

00:30:22.000 --> 00:30:22.670
That's fantastic.

00:30:22.670 --> 00:30:24.950
DXT is four bits per pixel.

00:30:24.950 --> 00:30:27.430
And with this technique, we
get it down to about 1.32.

00:30:27.430 --> 00:30:29.220
That's awesome.

00:30:29.220 --> 00:30:31.520
So here's the big
reveal, right?

00:30:31.520 --> 00:30:33.340
This is the entire thing.

00:30:33.340 --> 00:30:35.020
Why stop at 8x8 blocks?

00:30:35.020 --> 00:30:39.590
What if we actually modified our
image heuristics to scan,

00:30:39.590 --> 00:30:42.200
based upon the frequency of the
image, and use somewhere

00:30:42.200 --> 00:30:46.040
between four and 16x16 blocks?

00:30:46.040 --> 00:30:47.020
We do the same thing.

00:30:47.020 --> 00:30:49.220
We de-interleave, delta encode
and Huffman encode them.

00:30:49.220 --> 00:30:52.930
With this, we get about 80%
reduction at 93 megapixels a

00:30:52.930 --> 00:30:55.150
second for diffuse
textures only.

00:30:55.150 --> 00:30:58.760
And that gets us down to
0.8 bits per pixel.

00:30:58.760 --> 00:31:00.170
That is amazing.

00:31:00.170 --> 00:31:01.970
For one core, you're telling
me I can get

00:31:01.970 --> 00:31:03.210
0.8 bits per pixel.

00:31:03.210 --> 00:31:05.700
And I can decompress 93
megatexles a second.

00:31:05.700 --> 00:31:06.420
That's awesome.

00:31:06.420 --> 00:31:07.805
This is fantastic, right?

00:31:07.805 --> 00:31:10.620
Of course, you have to do all
this stuff offline and

00:31:10.620 --> 00:31:11.560
compress it properly.

00:31:11.560 --> 00:31:13.300
But that's a separate thing.

00:31:13.300 --> 00:31:15.850
So the bigger reveal is that,
once I came to all this

00:31:15.850 --> 00:31:17.560
research and got all this stuff
done a couple of days

00:31:17.560 --> 00:31:20.840
later, a very young, friendly
gentleman named Rich

00:31:20.840 --> 00:31:23.230
Geldreich, who I've worked with
at my previous companies,

00:31:23.230 --> 00:31:26.110
decided to put his
CRUNCH codec--

00:31:26.110 --> 00:31:28.360
which is his version of texture
compression-- online.

00:31:28.360 --> 00:31:30.990
It's licensed very generally.

00:31:30.990 --> 00:31:32.630
You should go check it out.

00:31:32.630 --> 00:31:34.800
Click that link there
and go check it out.

00:31:34.800 --> 00:31:38.760
He does about the same, about
between 0.8 and 1.25 bits per

00:31:38.760 --> 00:31:39.740
pixel for his texture

00:31:39.740 --> 00:31:42.580
compression for diffuse textures.

00:31:42.580 --> 00:31:45.320
For normal textures, he gets
about 1.75 to 2 bits per

00:31:45.320 --> 00:31:47.200
pixel, which is still
really amazing.

00:31:47.200 --> 00:31:49.490
This is cutting edge for game
textures, without having to go

00:31:49.490 --> 00:31:53.230
through a full JPEG
2000 stack.

00:31:53.230 --> 00:31:55.110
The thing here that is different
is that he gets

00:31:55.110 --> 00:31:59.480
about 256 megatexles a second
decompression, single core.

00:31:59.480 --> 00:32:00.820
I was only able to get 93.

00:32:00.820 --> 00:32:03.440
So he's got some dark
voodoo in there that

00:32:03.440 --> 00:32:04.930
really changes the game.

00:32:04.930 --> 00:32:08.280
Again, his stuff is up
on code.google.com.

00:32:08.280 --> 00:32:08.920
You can get it.

00:32:08.920 --> 00:32:10.520
You can use it in your game.

00:32:10.520 --> 00:32:11.990
I forget what the
licenses are.

00:32:11.990 --> 00:32:14.120
But you should definitely
go check that out.

00:32:14.120 --> 00:32:17.510
So take aways, it's actually
easy to get really simple

00:32:17.510 --> 00:32:19.740
savings with really simple
algorithms, if you just

00:32:19.740 --> 00:32:21.290
concatenate them in
the right way.

00:32:21.290 --> 00:32:24.030
Most importantly, your mileage
may vary for texture types.

00:32:24.030 --> 00:32:26.510
What works for an ambient
occlusion texture really well

00:32:26.510 --> 00:32:28.300
may not work for
a DXT texture.

00:32:28.300 --> 00:32:31.810
So it's important to optimize
and combine each of these

00:32:31.810 --> 00:32:33.870
things to put them together
to figure out what texture

00:32:33.870 --> 00:32:35.190
profile works best for you.

00:32:35.190 --> 00:32:37.700
And it's best to spend offline
doing your compression.

00:32:37.700 --> 00:32:40.170
You want to spend all of your
time online, while the game is

00:32:40.170 --> 00:32:43.360
running, while you're fighting
for that 16 millisecond frame

00:32:43.360 --> 00:32:46.230
to get you 60 Hertz, you want
all of that time to go towards

00:32:46.230 --> 00:32:46.710
what you need.

00:32:46.710 --> 00:32:48.910
You don't want to be spending
a lot of time decompressing

00:32:48.910 --> 00:32:49.460
and compressing.

00:32:49.460 --> 00:32:51.940
So do all of your compression
offline.

00:32:51.940 --> 00:32:53.680
And do your decompression
online.

00:32:53.680 --> 00:32:54.760
So that's it for me.

00:32:54.760 --> 00:32:57.190
Big thanks to Rich Geldreich,
John Brooks and Ken Adams for

00:32:57.190 --> 00:32:58.250
their help in this talk.

00:32:58.250 --> 00:32:59.500
You can see our final
table here.

00:32:59.500 --> 00:33:00.940
And feel free to contact me
any time you nee need

00:33:00.940 --> 00:33:03.180
questions, colton@google.com.

00:33:03.180 --> 00:33:03.950
Thank you all, very much.

00:33:03.950 --> 00:33:05.200
Have a good day.

