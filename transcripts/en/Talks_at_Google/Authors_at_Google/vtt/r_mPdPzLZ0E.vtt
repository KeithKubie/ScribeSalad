WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.898
[MUSIC PLAYING]

00:00:04.830 --> 00:00:07.245
[APPLAUSE]

00:00:10.160 --> 00:00:12.120
LISE A. JOHNSON: So
thanks for having us back.

00:00:12.120 --> 00:00:13.940
It's nice to get re-invited.

00:00:13.940 --> 00:00:16.680
It means we didn't do so
poorly the first time around.

00:00:16.680 --> 00:00:18.785
So my name is Lise Johnson,
and I am most recently

00:00:18.785 --> 00:00:21.590
of Rocky Vista University, which
is a very small medical school

00:00:21.590 --> 00:00:22.640
in Colorado.

00:00:22.640 --> 00:00:25.290
But I, until recently, worked
at University of Washington,

00:00:25.290 --> 00:00:27.290
so I'm very happy to
be back in the region

00:00:27.290 --> 00:00:29.520
enjoying some rain again.

00:00:29.520 --> 00:00:32.152
So today, we're going to
be talking about our book.

00:00:32.152 --> 00:00:33.860
And the first thing
we want to talk about

00:00:33.860 --> 00:00:36.470
is, why would we write
a book about worries?

00:00:36.470 --> 00:00:40.230
So this is a picture of my
daughter a couple of years ago.

00:00:40.230 --> 00:00:43.170
She's much more
advanced these days.

00:00:43.170 --> 00:00:46.430
But I think, for
me, I've always been

00:00:46.430 --> 00:00:50.188
kind of a worrier, a worrywart,
any way you want to phrase it.

00:00:50.188 --> 00:00:51.980
I've always been stressed
out about things.

00:00:51.980 --> 00:00:56.492
But this really accelerated as a
problem for me when I had kids.

00:00:56.492 --> 00:00:58.700
I don't know if anybody else
has had this experience,

00:00:58.700 --> 00:01:01.490
but I realized when I had
kids that there was, like,

00:01:01.490 --> 00:01:03.980
new categories of things
that I had never even thought

00:01:03.980 --> 00:01:06.470
about that I needed to
worry about all of a sudden.

00:01:06.470 --> 00:01:09.440
And as I would Google them,
I would get a little stressed

00:01:09.440 --> 00:01:11.960
out, because you get
1,000 hits for anything

00:01:11.960 --> 00:01:13.910
that you could potentially
be worried about.

00:01:13.910 --> 00:01:18.413
And it started to really become
kind of a problem in my life,

00:01:18.413 --> 00:01:20.330
that I was worrying about
all of these things.

00:01:20.330 --> 00:01:21.747
And my husband
said to me one day,

00:01:21.747 --> 00:01:24.320
as I was freaking out
about something trivial

00:01:24.320 --> 00:01:27.243
in retrospect, that,
as a scientist,

00:01:27.243 --> 00:01:29.660
you could probably figure out
what you need to worry about

00:01:29.660 --> 00:01:30.870
and what you don't.

00:01:30.870 --> 00:01:33.510
And I had this realization
that that was actually true.

00:01:33.510 --> 00:01:36.938
And so I started to kind of
investigate in a logical way

00:01:36.938 --> 00:01:39.230
the things that I thought
were important to worry about

00:01:39.230 --> 00:01:41.180
and the things
that I didn't think

00:01:41.180 --> 00:01:42.570
were important to worry about.

00:01:42.570 --> 00:01:45.112
And as I was in this process,
I realized lots of other people

00:01:45.112 --> 00:01:47.120
are stressed out about
lots of things, as well.

00:01:47.120 --> 00:01:48.828
And that kind of kicked
off this process.

00:01:48.828 --> 00:01:50.870
And then I worked with
Eric Chudler at the time,

00:01:50.870 --> 00:01:53.900
and he graciously agreed to
work on the project with me.

00:01:53.900 --> 00:01:55.942
And that's kind of
how we got started.

00:01:55.942 --> 00:01:57.650
Well, one of the things
that I've noticed

00:01:57.650 --> 00:02:00.350
is that this is an
extremely timely topic.

00:02:00.350 --> 00:02:02.210
Every time I tell
someone, hey, I'm

00:02:02.210 --> 00:02:05.180
writing this book about worries,
they always respond with,

00:02:05.180 --> 00:02:07.863
did you write a chapter
on, fill in the blank.

00:02:07.863 --> 00:02:10.280
Everybody has something that
they're really worried about.

00:02:10.280 --> 00:02:12.530
So we're going to talk
about some of the things

00:02:12.530 --> 00:02:15.120
that we discuss in the
book, and then we're

00:02:15.120 --> 00:02:18.320
going to give some tips on how
people can investigate worries

00:02:18.320 --> 00:02:19.430
on their own.

00:02:19.430 --> 00:02:23.420
But first we want to talk
about what our methods were.

00:02:23.420 --> 00:02:25.010
So Eric and I are
both scientists,

00:02:25.010 --> 00:02:28.350
and I would venture to say, kind
of nerdy scientists at that.

00:02:28.350 --> 00:02:29.990
And so we felt that
we needed to have

00:02:29.990 --> 00:02:32.600
a process and some
defining principles

00:02:32.600 --> 00:02:34.620
as we embarked on this project.

00:02:34.620 --> 00:02:36.920
So we thought about,
well, what sorts of things

00:02:36.920 --> 00:02:39.800
should you reasonably
be worried about?

00:02:39.800 --> 00:02:42.200
And we said there are
really three things that

00:02:42.200 --> 00:02:43.620
are important to consider.

00:02:43.620 --> 00:02:47.210
The first is how likely is
something to happen to you.

00:02:47.210 --> 00:02:49.130
The second is, how
bad is it going

00:02:49.130 --> 00:02:51.020
to be if it does in
fact happen to you.

00:02:51.020 --> 00:02:53.390
And then finally, what
can you do to prevent it

00:02:53.390 --> 00:02:54.680
from happening to you.

00:02:54.680 --> 00:02:57.140
And we decided, really,
that if something

00:02:57.140 --> 00:03:00.530
is likely to happen to you,
if it's going to be bad,

00:03:00.530 --> 00:03:02.390
and if you can do
something about it, then

00:03:02.390 --> 00:03:06.380
that sort of forms this
magic space of things

00:03:06.380 --> 00:03:07.670
that you should worry about.

00:03:07.670 --> 00:03:09.560
Everything else,
if it's unlikely,

00:03:09.560 --> 00:03:11.090
you don't need to
worry about it.

00:03:11.090 --> 00:03:12.840
If it's not really
going to be a big deal,

00:03:12.840 --> 00:03:14.120
you don't need to
worry about it.

00:03:14.120 --> 00:03:15.912
And if there's nothing
you can do about it,

00:03:15.912 --> 00:03:17.780
even if it is really
going to be a problem,

00:03:17.780 --> 00:03:20.030
then you don't really need
to worry about that either,

00:03:20.030 --> 00:03:22.440
because there's nothing you
can really do to prevent it.

00:03:22.440 --> 00:03:24.470
So it's kind of a
waste of your time.

00:03:24.470 --> 00:03:29.610
So we quantified our
worries in this way.

00:03:29.610 --> 00:03:33.343
So for each of our problems
that we investigated,

00:03:33.343 --> 00:03:35.010
each of our topics
that we investigated,

00:03:35.010 --> 00:03:37.940
we created what we
call the Worry Index.

00:03:37.940 --> 00:03:41.570
So the Worry Index is really
a very loose quantification.

00:03:41.570 --> 00:03:43.820
It's really based on,
we investigated this,

00:03:43.820 --> 00:03:47.090
we talked about it, and we
scaled each of these worries

00:03:47.090 --> 00:03:48.960
with respect to each other.

00:03:48.960 --> 00:03:53.673
So it's not something to really
take religiously to the bank.

00:03:53.673 --> 00:03:55.340
It's a good place to
start, and so we're

00:03:55.340 --> 00:04:00.252
going to talk about how we feel
about each of these topics.

00:04:00.252 --> 00:04:02.210
And hopefully, we'll have
a chance for you guys

00:04:02.210 --> 00:04:03.440
to participate, as well.

00:04:06.750 --> 00:04:08.330
ERIC CHUDLER: So as
an example, we're

00:04:08.330 --> 00:04:09.890
going to talk about
asteroid strikes.

00:04:09.890 --> 00:04:12.470
So that's one of the things
that some people worry about.

00:04:12.470 --> 00:04:15.080
In fact millions years
ago, the dinosaurs

00:04:15.080 --> 00:04:17.720
were maybe worrying about
getting struck by an asteroid

00:04:17.720 --> 00:04:21.320
if they had a large enough
brain to worry about things.

00:04:21.320 --> 00:04:24.800
But some people do worry
about, maybe all life on Earth

00:04:24.800 --> 00:04:26.180
will be destroyed again.

00:04:26.180 --> 00:04:29.020
And so I'm going to give this
as an example about the criteria

00:04:29.020 --> 00:04:31.520
that we used, and then we're
going to give you some examples

00:04:31.520 --> 00:04:33.330
to do on your own.

00:04:33.330 --> 00:04:38.330
So for an asteroid
strike, the likelihood

00:04:38.330 --> 00:04:42.560
of getting struck by a
large asteroid is minuscule.

00:04:42.560 --> 00:04:44.420
So there are a couple
of sentry systems

00:04:44.420 --> 00:04:46.460
here in the United States,
and also in Europe,

00:04:46.460 --> 00:04:50.510
that are scanning the skies
for large objects on their way

00:04:50.510 --> 00:04:51.800
to Earth.

00:04:51.800 --> 00:04:55.100
And it's estimated that
the next significant one

00:04:55.100 --> 00:05:00.890
within the next 100 years has
almost a 99.9% chance that it

00:05:00.890 --> 00:05:03.380
will miss Earth.

00:05:03.380 --> 00:05:06.980
So the likelihood of a large
asteroid hitting the earth

00:05:06.980 --> 00:05:09.230
is almost nil.

00:05:09.230 --> 00:05:13.070
So because it's unlikely,
nothing to worry about.

00:05:13.070 --> 00:05:16.820
So you should be happy
about that-- very unlikely.

00:05:16.820 --> 00:05:18.690
Can you do anything
to prevent it?

00:05:18.690 --> 00:05:20.660
So if there is an
asteroid coming to Earth,

00:05:20.660 --> 00:05:23.700
is there some way that you
can avert this disaster?

00:05:23.700 --> 00:05:28.850
So how many of you
have seen "Armageddon,"

00:05:28.850 --> 00:05:32.175
with Ben Affleck
and Bruce Willis?

00:05:32.175 --> 00:05:33.940
Yeah, most of you
have seen that one.

00:05:33.940 --> 00:05:35.810
All right, would that work?

00:05:35.810 --> 00:05:37.340
No, it's not going to work.

00:05:37.340 --> 00:05:40.310
If you put a nuclear
bomb on an asteroid,

00:05:40.310 --> 00:05:43.160
instead of one asteroid
coming down to Earth,

00:05:43.160 --> 00:05:46.640
you're going to get maybe
101, and it will still

00:05:46.640 --> 00:05:48.210
be a big problem.

00:05:48.210 --> 00:05:51.170
We do not have any
technology at this time

00:05:51.170 --> 00:05:54.920
to prevent an asteroid if
it was coming toward us.

00:05:54.920 --> 00:05:57.200
Some technologies that
are being thought of,

00:05:57.200 --> 00:05:59.510
well, maybe if you
can break it up

00:05:59.510 --> 00:06:02.360
into tiny, tiny little
pieces, maybe that would work.

00:06:02.360 --> 00:06:06.260
Because our atmosphere is being
bombarded by asteroids all

00:06:06.260 --> 00:06:08.690
the time, but
they're small enough

00:06:08.690 --> 00:06:10.790
they burn up in the atmosphere.

00:06:10.790 --> 00:06:12.590
It doesn't cause a problem.

00:06:12.590 --> 00:06:14.820
Other technologies might
be to send something up,

00:06:14.820 --> 00:06:17.450
and rather than
break it apart, just

00:06:17.450 --> 00:06:19.695
nudge it a little
bit off course.

00:06:19.695 --> 00:06:22.070
Because if you catch it soon
enough, because the Earth is

00:06:22.070 --> 00:06:24.890
moving, all you have to do
is change the trajectory

00:06:24.890 --> 00:06:29.020
a little bit, and the Earth
will move out of the way

00:06:29.020 --> 00:06:30.680
so the asteroid won't hit.

00:06:30.680 --> 00:06:33.980
Another technology would
be to put up a large object

00:06:33.980 --> 00:06:37.790
and gravitationally
pull the asteroid away,

00:06:37.790 --> 00:06:39.860
again, to divert it off course.

00:06:39.860 --> 00:06:41.570
And that would also
cause it to miss.

00:06:41.570 --> 00:06:43.160
Can we do that now?

00:06:43.160 --> 00:06:44.210
No.

00:06:44.210 --> 00:06:45.740
So we can't do
anything to prevent

00:06:45.740 --> 00:06:49.010
it-- so nothing to worry about.

00:06:49.010 --> 00:06:53.210
Now, if it did happen,
though, if it does happen,

00:06:53.210 --> 00:06:55.530
just like it happened
millions of years ago,

00:06:55.530 --> 00:06:58.340
it would wipe out a
lot of life on Earth,

00:06:58.340 --> 00:07:00.720
including all human life.

00:07:00.720 --> 00:07:02.827
So the consequence
is devastating,

00:07:02.827 --> 00:07:04.160
if something like that happened.

00:07:04.160 --> 00:07:08.670
All human life would probably
be destroyed, unfortunately.

00:07:08.670 --> 00:07:11.220
So when you add all
these up together,

00:07:11.220 --> 00:07:14.930
this is where we would place
it, on that lower left side.

00:07:14.930 --> 00:07:16.645
It's unlikely to happen.

00:07:16.645 --> 00:07:18.020
There's nothing
you can do about,

00:07:18.020 --> 00:07:20.120
although the consequence
would be bad.

00:07:20.120 --> 00:07:22.850
But because of
those two criteria,

00:07:22.850 --> 00:07:27.080
getting struck by an asteroid is
nothing for you to worry about.

00:07:27.080 --> 00:07:28.610
So those are the
methods we used.

00:07:28.610 --> 00:07:32.360
We looked at the data of the
likelihood, the consequence,

00:07:32.360 --> 00:07:36.890
and can you do
anything to prevent it.

00:07:36.890 --> 00:07:40.220
So now it's audience
participation time.

00:07:40.220 --> 00:07:43.270
So all of you should
have a piece of paper

00:07:43.270 --> 00:07:47.240
with a little graph
on it, a little plot.

00:07:47.240 --> 00:07:53.200
And your job is to
assign a little dot,

00:07:53.200 --> 00:07:56.380
and the size of the dot is
going to be the consequence.

00:07:56.380 --> 00:07:59.230
And you should be plotting
it on the two scales.

00:07:59.230 --> 00:08:01.260
So Lise's going to
introduce the first one.

00:08:04.600 --> 00:08:06.340
LISE A. JOHNSON: OK,
so the first thing

00:08:06.340 --> 00:08:07.600
we're going to talk
about is mercury.

00:08:07.600 --> 00:08:08.700
So everybody play along.

00:08:08.700 --> 00:08:10.325
It's after lunch, so
everybody wake up.

00:08:10.325 --> 00:08:12.770
Participation is key.

00:08:12.770 --> 00:08:16.450
I want you, on your
plot, to make a bubble

00:08:16.450 --> 00:08:19.930
relative to the size that you
think it should be, and then

00:08:19.930 --> 00:08:21.490
also to--

00:08:21.490 --> 00:08:22.240
oh, yeah, exactly.

00:08:22.240 --> 00:08:23.830
Eric put up this nice plot.

00:08:23.830 --> 00:08:27.100
So put it in the quadrant
that you think it belongs in,

00:08:27.100 --> 00:08:30.045
and then give it a size relative
to the magnitude of the problem

00:08:30.045 --> 00:08:30.920
that you think it is.

00:08:30.920 --> 00:08:32.659
I'll give you just
a minute to do that,

00:08:32.659 --> 00:08:35.940
and then we'll continue.

00:08:35.940 --> 00:08:36.900
Everybody has to play.

00:08:36.900 --> 00:08:37.400
Commit.

00:08:41.630 --> 00:08:44.590
OK, so raise your
hand if you think

00:08:44.590 --> 00:08:46.840
that it belongs in this
quadrant down here-- unlikely,

00:08:46.840 --> 00:08:50.150
and you can't do
anything to stop it.

00:08:50.150 --> 00:08:51.230
Excellent.

00:08:51.230 --> 00:08:54.530
How about, unlikely and you
can do something about it?

00:08:54.530 --> 00:08:55.460
Yes, excellent.

00:08:55.460 --> 00:08:56.570
Some people think so.

00:08:56.570 --> 00:08:58.778
Likely to happen, but you
can't do anything about it.

00:08:58.778 --> 00:09:00.950
This is sort of the
fatalistic category down here.

00:09:00.950 --> 00:09:01.830
A few.

00:09:01.830 --> 00:09:02.870
And how about up here?

00:09:02.870 --> 00:09:05.037
Likely to happen, and you
can do something about it,

00:09:05.037 --> 00:09:06.840
therefore you should worry.

00:09:06.840 --> 00:09:08.840
OK, some of you aren't
playing, but that's fine.

00:09:08.840 --> 00:09:11.180
You'll get into it.

00:09:11.180 --> 00:09:12.433
So here's where we put it.

00:09:12.433 --> 00:09:14.600
So we actually put it up
here in the Worry category,

00:09:14.600 --> 00:09:17.510
and it gets a
relatively large dot.

00:09:17.510 --> 00:09:18.735
So why is that?

00:09:18.735 --> 00:09:21.110
I kind of primed you in thinking
about this by giving you

00:09:21.110 --> 00:09:22.760
a picture of a
fish, so many people

00:09:22.760 --> 00:09:24.980
are thinking about
methyl mercury, which

00:09:24.980 --> 00:09:28.610
is part of our diet, and we
worry about it in that sense.

00:09:28.610 --> 00:09:29.853
And that is a problem.

00:09:29.853 --> 00:09:31.520
But there's also
another form of mercury

00:09:31.520 --> 00:09:32.937
that you might
encounter, and that

00:09:32.937 --> 00:09:34.070
would be elemental mercury.

00:09:34.070 --> 00:09:37.800
So how many people have actually
seen mercury in real life?

00:09:37.800 --> 00:09:39.740
Yeah, so it's very cool, right?

00:09:39.740 --> 00:09:42.440
My mom tells me stories about
when she was a kid, in science

00:09:42.440 --> 00:09:44.648
class, they would actually
give her a blob of mercury

00:09:44.648 --> 00:09:45.980
to roll around in her hand.

00:09:45.980 --> 00:09:49.980
And it was so exciting because
it's silvery and it's neat.

00:09:49.980 --> 00:09:51.710
Everybody thinks
it's neat, and people

00:09:51.710 --> 00:09:54.252
have thought that it's neat for
thousands of years, actually.

00:09:54.252 --> 00:09:56.605
So historically, it's been
used for medicinal purposes.

00:09:56.605 --> 00:09:58.730
It used to be a very common
treatment for syphilis,

00:09:58.730 --> 00:10:00.200
for example.

00:10:00.200 --> 00:10:02.510
It's been used for sort
of magical or ritualistic

00:10:02.510 --> 00:10:03.860
purposes, and it still is.

00:10:03.860 --> 00:10:06.200
It's used in mining,
because it forms amalgams

00:10:06.200 --> 00:10:08.180
with both gold and silver.

00:10:08.180 --> 00:10:09.980
And it's used in fillings.

00:10:09.980 --> 00:10:12.080
Does anybody have
silver fillings.

00:10:12.080 --> 00:10:14.270
You probably-- or
gold fillings also.

00:10:14.270 --> 00:10:16.010
You also have some
mercury in there,

00:10:16.010 --> 00:10:18.805
because it forms an amalgam.

00:10:18.805 --> 00:10:20.180
So the problem
with mercury that,

00:10:20.180 --> 00:10:24.110
in addition to being very cool,
it's obviously also very toxic.

00:10:24.110 --> 00:10:28.070
And mercury is most toxic, not
when you put it on your skin

00:10:28.070 --> 00:10:30.440
or even when you eat it,
but when you inhale it.

00:10:30.440 --> 00:10:33.290
So elemental mercury is most
dangerous when you inhale it.

00:10:33.290 --> 00:10:35.840
Which is strange, because
you don't think about a metal

00:10:35.840 --> 00:10:37.730
as being particularly vaporous.

00:10:37.730 --> 00:10:39.680
But mercury is a strange metal.

00:10:39.680 --> 00:10:43.850
And so it's most easily
absorbed into your body

00:10:43.850 --> 00:10:44.600
if you inhale it.

00:10:44.600 --> 00:10:46.100
And then it crosses
into your blood,

00:10:46.100 --> 00:10:47.360
and then it crosses
into your brain.

00:10:47.360 --> 00:10:49.010
And there are a number
of negative side

00:10:49.010 --> 00:10:50.670
effects associated with that.

00:10:50.670 --> 00:10:53.370
So elemental mercury is
very much to be avoided.

00:10:53.370 --> 00:10:56.510
And we don't usually encounter
it in commercial products

00:10:56.510 --> 00:10:57.340
anymore.

00:10:57.340 --> 00:10:59.480
It used to be in
thermometers, but they've

00:10:59.480 --> 00:11:01.750
been highly phased out.

00:11:01.750 --> 00:11:04.580
And you used to find it in
science classes and fishing

00:11:04.580 --> 00:11:05.972
lures and all kinds of places.

00:11:05.972 --> 00:11:08.180
The problem is that, even
though we don't manufacture

00:11:08.180 --> 00:11:10.790
those products anymore, even
though we don't distribute it

00:11:10.790 --> 00:11:12.560
to science classes
for kids to play

00:11:12.560 --> 00:11:14.227
with in the palm of
their hand, it still

00:11:14.227 --> 00:11:16.800
exists in a legacy form.

00:11:16.800 --> 00:11:19.590
And once it is distributed
in the environment--

00:11:19.590 --> 00:11:23.300
once you break a thermometer,
once you spill mercury--

00:11:23.300 --> 00:11:26.420
it's very, very difficult
and expensive to clean it up.

00:11:26.420 --> 00:11:27.710
You can't sweep it up.

00:11:27.710 --> 00:11:28.940
You can't vacuum it up.

00:11:28.940 --> 00:11:31.700
You can't just dump it down
the drain, because it persists.

00:11:31.700 --> 00:11:33.825
And there have been examples
of children finding it

00:11:33.825 --> 00:11:36.242
in their science classrooms
and taking it home and playing

00:11:36.242 --> 00:11:36.770
with it.

00:11:36.770 --> 00:11:38.187
And they have to
take their houses

00:11:38.187 --> 00:11:39.920
literally down to
the studs in order

00:11:39.920 --> 00:11:41.858
to decontaminate the house.

00:11:41.858 --> 00:11:43.400
There has been an
example of somebody

00:11:43.400 --> 00:11:47.150
who broke a mercury thermometer
20 years prior in their home,

00:11:47.150 --> 00:11:49.850
and they were still recording
measurable levels of mercury

00:11:49.850 --> 00:11:53.990
vapor in the bathroom.

00:11:53.990 --> 00:11:56.810
So that can obviously
be very problematic.

00:11:56.810 --> 00:12:01.670
Methyl mercury, which we're
familiar with in terms of fish,

00:12:01.670 --> 00:12:03.210
is also a problem.

00:12:03.210 --> 00:12:05.810
But it's hard to say how
big of a problem it is.

00:12:05.810 --> 00:12:07.520
And there's lots of confounders.

00:12:07.520 --> 00:12:12.325
So clearly having too much
methyl mercury is toxic.

00:12:12.325 --> 00:12:14.450
And we know that because
of an unfortunate incident

00:12:14.450 --> 00:12:19.680
in Minamata, Japan a
number of years ago.

00:12:19.680 --> 00:12:21.308
But it's sort of the
in-between levels

00:12:21.308 --> 00:12:22.850
are harder to
quantify, because there

00:12:22.850 --> 00:12:24.840
are so many individual
differences,

00:12:24.840 --> 00:12:28.430
and because consuming fish,
which is sort of the largest

00:12:28.430 --> 00:12:30.800
source of methyl mercury,
is also really good

00:12:30.800 --> 00:12:32.160
for you in lots of other ways.

00:12:32.160 --> 00:12:34.130
So it's this strange
balance between,

00:12:34.130 --> 00:12:37.130
fish is really good for you
and your cognitive development,

00:12:37.130 --> 00:12:38.570
but mercury is
really bad for you

00:12:38.570 --> 00:12:39.870
and your cognitive development.

00:12:39.870 --> 00:12:42.800
So how do we find
the happy medium

00:12:42.800 --> 00:12:44.180
between those two things?

00:12:44.180 --> 00:12:46.430
And I think it's fair to say
that scientists are still

00:12:46.430 --> 00:12:47.820
arguing about that.

00:12:47.820 --> 00:12:50.690
But one thing we can
say is that, fish

00:12:50.690 --> 00:12:53.630
that are higher up on the food
chain, which usually means

00:12:53.630 --> 00:12:56.333
the big ones that
eat the littler ones,

00:12:56.333 --> 00:12:58.250
are going to have more
concentration of methyl

00:12:58.250 --> 00:13:01.110
mercury than ones that are
lower on the food chain.

00:13:01.110 --> 00:13:03.820
Which means if you
eat sardines, you're

00:13:03.820 --> 00:13:06.590
going to get less mercury
than if you're eating shark

00:13:06.590 --> 00:13:08.310
or swordfish, for example.

00:13:08.310 --> 00:13:11.090
So most experts recommend now
that you should try and eat

00:13:11.090 --> 00:13:12.570
fish, but you
should eat fish that

00:13:12.570 --> 00:13:14.540
are lower on the food
chain and fish that are

00:13:14.540 --> 00:13:16.070
higher in omega fatty acids.

00:13:20.425 --> 00:13:22.300
ERIC CHUDLER: So this
one gets a frowny face.

00:13:22.300 --> 00:13:22.820
Yeah.

00:13:22.820 --> 00:13:23.830
AUDIENCE: Question--
how do you think

00:13:23.830 --> 00:13:25.080
about the scale on the x-axis?

00:13:25.080 --> 00:13:27.177
Is this a percentage
chance of encountering it

00:13:27.177 --> 00:13:29.033
in your lifetime?

00:13:29.033 --> 00:13:30.450
ERIC CHUDLER:
Repeat the question.

00:13:30.450 --> 00:13:32.575
LISE A. JOHNSON: Oh, to
repeat the question, sorry.

00:13:32.575 --> 00:13:34.420
So how do we scale
it on the x-axis?

00:13:34.420 --> 00:13:36.250
Is it sort of a lifetime risk?

00:13:36.250 --> 00:13:39.170
I tend to think of it as more
of like a snapshot in time.

00:13:39.170 --> 00:13:41.620
So at this point
in time, if you,

00:13:41.620 --> 00:13:43.600
assuming that you have
any exposure at all--

00:13:43.600 --> 00:13:47.440
like you are eating fish, or
you are living in a world where

00:13:47.440 --> 00:13:49.450
there is elemental mercury--

00:13:49.450 --> 00:13:50.530
what is your likelihood?

00:13:50.530 --> 00:13:53.770
So it's a little
bit personalized.

00:13:53.770 --> 00:13:56.330
Not everybody's going to have
the same level of exposure,

00:13:56.330 --> 00:13:59.320
which is one of the reasons why
we ask people not to read too

00:13:59.320 --> 00:14:01.810
much into our Worry
Index, but really

00:14:01.810 --> 00:14:06.000
to think about how this
might apply to you.

00:14:06.000 --> 00:14:07.950
ERIC CHUDLER: All
right, your next worry

00:14:07.950 --> 00:14:11.760
is, in honor of Brain Awareness
Week, which is actually

00:14:11.760 --> 00:14:14.070
an international event
going on next week,

00:14:14.070 --> 00:14:18.410
is your brain-eating amoeba.

00:14:18.410 --> 00:14:21.350
And this was actually something
that was in the newspaper

00:14:21.350 --> 00:14:23.030
just very recently.

00:14:23.030 --> 00:14:25.880
There was a case in Washington.

00:14:25.880 --> 00:14:28.130
And so you may have seen the
headlines in "The Seattle

00:14:28.130 --> 00:14:29.670
Times" and on the news.

00:14:29.670 --> 00:14:33.410
So again on your
sheets of paper,

00:14:33.410 --> 00:14:36.020
where would this worry be.

00:14:36.020 --> 00:14:37.940
Pick which quadrant
it should be.

00:14:37.940 --> 00:14:41.020
And again, just to
help you out, it

00:14:41.020 --> 00:14:43.970
looks like most
people are finished.

00:14:43.970 --> 00:14:48.160
So raise your hand if this
is a concern of yours,

00:14:48.160 --> 00:14:50.800
that it's unlikely to
happen, and there's

00:14:50.800 --> 00:14:53.590
nothing you can do to stop it.

00:14:53.590 --> 00:14:56.350
Unlikely to happen, but there's
nothing you can do to stop it.

00:14:56.350 --> 00:15:00.340
How about above that--
it's unlikely to happen,

00:15:00.340 --> 00:15:03.560
but there is something
you can do about it.

00:15:03.560 --> 00:15:05.240
Maybe a few more people.

00:15:05.240 --> 00:15:11.320
How about, it's
likely to happen,

00:15:11.320 --> 00:15:14.970
and you can't do
anything about it.

00:15:14.970 --> 00:15:18.680
So likely to happen, at
the bottom right-hand side.

00:15:18.680 --> 00:15:20.520
And how about just
above that-- this

00:15:20.520 --> 00:15:22.228
is something that you
should worry about.

00:15:24.780 --> 00:15:27.000
This is where we placed it.

00:15:27.000 --> 00:15:30.060
It's something that is
very unlikely to happen.

00:15:30.060 --> 00:15:32.930
There is something that
you can do about it.

00:15:32.930 --> 00:15:36.620
But it has a large circle,
because if it happens to you,

00:15:36.620 --> 00:15:38.880
it's 97% fatal.

00:15:38.880 --> 00:15:41.850
OK, so what is a
brain-eating amoeba?

00:15:41.850 --> 00:15:45.270
It's a small single-celled
organism that

00:15:45.270 --> 00:15:48.660
is found in warm, fresh water.

00:15:48.660 --> 00:15:52.170
If it's inhaled
through your nose--

00:15:52.170 --> 00:15:54.570
not in many cases,
in a few cases

00:15:54.570 --> 00:15:56.670
it gets into the upper
part of your nose,

00:15:56.670 --> 00:15:59.610
where it then crawls--

00:15:59.610 --> 00:16:01.830
I guess, if an amoeba crawls--

00:16:01.830 --> 00:16:05.250
through the sinuses
up to your brain

00:16:05.250 --> 00:16:07.110
where it starts
eating brain tissue.

00:16:07.110 --> 00:16:09.900
And that's bad, as
you might imagine.

00:16:09.900 --> 00:16:11.680
But it's very
unlikely to happen.

00:16:11.680 --> 00:16:15.960
Many waters are contaminated
with this amoeba,

00:16:15.960 --> 00:16:18.010
but very few people
come down with it.

00:16:18.010 --> 00:16:20.280
In fact, for the last
about 30 years or so,

00:16:20.280 --> 00:16:23.370
there's only been
about 150 cases or so.

00:16:23.370 --> 00:16:26.670
So it's very unlikely to happen,
even though a lot of water

00:16:26.670 --> 00:16:28.470
is contaminated.

00:16:28.470 --> 00:16:32.580
You could also not
contract the disease

00:16:32.580 --> 00:16:34.840
by swallowing the water.

00:16:34.840 --> 00:16:36.820
It must go into
your nasal passages.

00:16:36.820 --> 00:16:38.560
So just because you
swallow the water,

00:16:38.560 --> 00:16:42.610
the amoeba's killed in
your digestive system.

00:16:42.610 --> 00:16:45.540
The other way you can get it, if
you're not in this fresh water,

00:16:45.540 --> 00:16:47.910
in like a spa or
something like that,

00:16:47.910 --> 00:16:50.940
there's a device
called a neti pot.

00:16:50.940 --> 00:16:54.180
So some people use a neti pot
to clean out their sinuses.

00:16:54.180 --> 00:16:58.320
You're supposed to use
purified or sterilized water.

00:16:58.320 --> 00:17:00.510
And then you tip the
pot into your nose

00:17:00.510 --> 00:17:01.530
to clean your sinuses.

00:17:01.530 --> 00:17:03.750
Well, if that water
is contaminated,

00:17:03.750 --> 00:17:05.940
then the amoeba can
get up in your nose,

00:17:05.940 --> 00:17:08.010
and it can then crawl
into your brain,

00:17:08.010 --> 00:17:11.520
and I'm sorry to say, kill you.

00:17:11.520 --> 00:17:14.819
So again, not something
to worry about,

00:17:14.819 --> 00:17:18.030
but the headlines make
it as if it is something

00:17:18.030 --> 00:17:19.349
that you should worry about.

00:17:19.349 --> 00:17:22.260
"The Seattle Times" had a big
headline, brain-eating amoeba,

00:17:22.260 --> 00:17:25.020
doctor says it
can happen to you.

00:17:25.020 --> 00:17:27.690
But very, very unlikely,
even though it's

00:17:27.690 --> 00:17:29.850
a somewhat common bacteria.

00:17:29.850 --> 00:17:32.460
So nothing to worry about.

00:17:32.460 --> 00:17:33.457
The next one--

00:17:33.457 --> 00:17:35.790
LISE A. JOHNSON: OK, so flame
retardants-- first of all,

00:17:35.790 --> 00:17:37.290
does everybody know
what I mean when

00:17:37.290 --> 00:17:38.820
I talk about flame retardants?

00:17:38.820 --> 00:17:41.430
These are chemicals that
are added a lot of times

00:17:41.430 --> 00:17:43.470
to furniture, most
commonly furniture,

00:17:43.470 --> 00:17:49.230
also baby toys and bouncers
and stuff, and carpets and car

00:17:49.230 --> 00:17:51.810
seats, in order,
obviously, to stop them

00:17:51.810 --> 00:17:53.170
from catching on fire.

00:17:53.170 --> 00:17:55.043
So now that you
know what they are,

00:17:55.043 --> 00:17:56.460
write down quickly
what you think.

00:17:56.460 --> 00:17:57.240
Yeah.

00:17:57.240 --> 00:17:58.490
AUDIENCE: Does asbestos count?

00:17:58.490 --> 00:17:59.790
LISE A. JOHNSON: No.

00:17:59.790 --> 00:18:00.600
ERIC CHUDLER: That's
a separate chapter.

00:18:00.600 --> 00:18:01.555
LISE A. JOHNSON:
Different chapter, yeah.

00:18:01.555 --> 00:18:02.741
AUDIENCE: Are we
talking about the risk

00:18:02.741 --> 00:18:04.283
of being injured by
flame retardants,

00:18:04.283 --> 00:18:06.372
or injured by the lack
of flame retardants?

00:18:06.372 --> 00:18:10.380
LISE A. JOHNSON: Of being
injured by flame retardants.

00:18:10.380 --> 00:18:11.880
Just to remind you,
here's our plot,

00:18:11.880 --> 00:18:15.653
to put it in one
of these quadrants.

00:18:15.653 --> 00:18:17.320
Although that's an
interesting question,

00:18:17.320 --> 00:18:20.290
about whether it's being
injured by or the lack of.

00:18:20.290 --> 00:18:21.950
Both, obviously,
could be a problem.

00:18:21.950 --> 00:18:25.670
But in this case, we're
talking about the chemicals.

00:18:25.670 --> 00:18:33.560
OK, so who thinks we belong
down here, not really a problem.

00:18:33.560 --> 00:18:34.170
OK.

00:18:34.170 --> 00:18:35.280
How about up here?

00:18:38.150 --> 00:18:38.845
A few as well.

00:18:38.845 --> 00:18:40.220
So that would be
in the unlikely,

00:18:40.220 --> 00:18:41.637
and you can do
something about it.

00:18:41.637 --> 00:18:44.930
So it's highly preventable,
but not highly likely.

00:18:44.930 --> 00:18:46.470
How about, it's
likely to happen,

00:18:46.470 --> 00:18:48.220
but you can't really
do anything about it,

00:18:48.220 --> 00:18:50.428
so don't worry about it.

00:18:50.428 --> 00:18:52.470
And how many think it's
something to worry about?

00:18:55.100 --> 00:18:55.670
OK.

00:18:55.670 --> 00:18:58.610
So I'll tell you that
this is the topic that

00:18:58.610 --> 00:19:00.708
sent me kind of over the edge.

00:19:00.708 --> 00:19:02.750
I was worried about flame
retardants in our sofa,

00:19:02.750 --> 00:19:05.180
and I actually made my
husband take our sofa out

00:19:05.180 --> 00:19:07.610
to the garage, and we
lived without a sofa

00:19:07.610 --> 00:19:09.260
for several months.

00:19:09.260 --> 00:19:10.740
And I'll tell you why.

00:19:10.740 --> 00:19:13.102
So before I do, let me show
you where we assigned it

00:19:13.102 --> 00:19:14.810
and explain a little
bit about this plot.

00:19:14.810 --> 00:19:17.787
OK, so first of all,
this circle right here,

00:19:17.787 --> 00:19:19.620
it looks like it's
slightly off to the left,

00:19:19.620 --> 00:19:22.550
but it's meant to be centered
right on the 50% line.

00:19:22.550 --> 00:19:25.340
And it's about 50% size.

00:19:25.340 --> 00:19:27.320
And that means, actually,
when I give something

00:19:27.320 --> 00:19:30.800
a score of 50-50, it
means I have no idea.

00:19:30.800 --> 00:19:33.110
OK, so this is what
I like to call,

00:19:33.110 --> 00:19:35.720
in the zone of uncertainty.

00:19:35.720 --> 00:19:37.610
And the reason for
that is because, when

00:19:37.610 --> 00:19:39.540
we're talking about flame
retardant chemicals,

00:19:39.540 --> 00:19:42.230
there are thousands
of different chemicals

00:19:42.230 --> 00:19:45.210
that are out there in the
wild that are being used.

00:19:45.210 --> 00:19:46.790
And most of them
have actually not

00:19:46.790 --> 00:19:51.060
been rigorously studied for
their effects on human health.

00:19:51.060 --> 00:19:54.050
So a little bit of history
on flame retardants--

00:19:54.050 --> 00:19:57.050
they were originally
added to furniture by law

00:19:57.050 --> 00:19:58.730
in the state of
California around--

00:19:58.730 --> 00:20:00.230
oh, I'm going to
get my date wrong--

00:20:00.230 --> 00:20:02.480
I think it was 1975.

00:20:02.480 --> 00:20:05.180
Because there was a problem
with people smoking.

00:20:05.180 --> 00:20:08.630
And when they fall asleep with
a cigarette in their mouth

00:20:08.630 --> 00:20:12.770
or close by, they would light
their furniture on fire.

00:20:12.770 --> 00:20:15.590
And in order to get
around this problem,

00:20:15.590 --> 00:20:18.380
the state of California
mandated that furniture

00:20:18.380 --> 00:20:22.220
meet a certain
flame-resistant standard.

00:20:22.220 --> 00:20:24.860
And the way that manufacturers
achieved the standard

00:20:24.860 --> 00:20:27.950
was by adding flame
retardant chemicals.

00:20:27.950 --> 00:20:30.770
And because California
is such a huge market,

00:20:30.770 --> 00:20:33.278
most manufacturers
didn't make furniture

00:20:33.278 --> 00:20:35.570
for the rest of the world
and furniture for California.

00:20:35.570 --> 00:20:39.110
They just added flame retardant
chemicals to everything.

00:20:39.110 --> 00:20:41.450
So the problem is that
many of these chemicals

00:20:41.450 --> 00:20:44.120
are associated with all
kinds of health problems-- so

00:20:44.120 --> 00:20:47.990
endocrine problems, and
developmental problems, growth

00:20:47.990 --> 00:20:48.830
problems.

00:20:48.830 --> 00:20:51.230
Cancer, obviously, is
always on the list.

00:20:51.230 --> 00:20:54.470
But they haven't really been--

00:20:54.470 --> 00:20:57.960
the whole sum of them haven't
been rigorously tested.

00:20:57.960 --> 00:21:00.330
And so we still don't really
know how big of a problem

00:21:00.330 --> 00:21:00.830
they are.

00:21:00.830 --> 00:21:04.110
We know that they are persistent
environmental pollutants.

00:21:04.110 --> 00:21:07.350
So you can isolate them in
the fat of arctic polar bears,

00:21:07.350 --> 00:21:09.110
which turns out to
be the gold standard

00:21:09.110 --> 00:21:11.450
for a persistent
environmental pollutant.

00:21:11.450 --> 00:21:15.120
If you can find it in a polar
bear, then we have a problem.

00:21:18.143 --> 00:21:20.310
And they're basically
everywhere in the environment,

00:21:20.310 --> 00:21:24.140
which is why we get a pretty
low preventability score.

00:21:24.140 --> 00:21:25.070
They're in your car.

00:21:25.070 --> 00:21:26.840
They're in your house.

00:21:26.840 --> 00:21:30.320
Any place that has
foam, in particular,

00:21:30.320 --> 00:21:33.380
is often treated with
flame retardant chemicals.

00:21:33.380 --> 00:21:37.370
So recently, about 2013,
the state of California

00:21:37.370 --> 00:21:40.370
reversed its technical bulletin.

00:21:40.370 --> 00:21:42.320
It's called technical
bulletin 117,

00:21:42.320 --> 00:21:44.990
and now there's an
amendment in 2013

00:21:44.990 --> 00:21:49.310
saying you don't have to
have the same sort of flame

00:21:49.310 --> 00:21:51.260
retardant properties
in your furniture.

00:21:51.260 --> 00:21:54.560
And so now it is legal to
make furniture that doesn't

00:21:54.560 --> 00:21:55.760
have added flame retardants.

00:21:55.760 --> 00:21:57.920
And some manufacturers do.

00:21:57.920 --> 00:21:59.960
So you can find a sofa--

00:21:59.960 --> 00:22:03.500
Ikea makes them, which is where
we got non-flame retardant

00:22:03.500 --> 00:22:05.300
sofa is from Ikea.

00:22:05.300 --> 00:22:07.580
And you can buy baby
products that also don't have

00:22:07.580 --> 00:22:10.790
flame retardants added to them.

00:22:10.790 --> 00:22:13.200
It's probably worth
doing, in my opinion,

00:22:13.200 --> 00:22:15.680
just because of the
level of uncertainty.

00:22:15.680 --> 00:22:18.110
But I can't say
for sure that this

00:22:18.110 --> 00:22:20.180
is going to hurt
you or not hurt you,

00:22:20.180 --> 00:22:23.460
because the data is
really not in on that.

00:22:23.460 --> 00:22:25.400
Does that sort of
make sense, as to why

00:22:25.400 --> 00:22:29.047
it ends up in this particular
zone of uncertainty?

00:22:29.047 --> 00:22:31.130
The other thing that sort
of surrounds this issue,

00:22:31.130 --> 00:22:33.342
and it gets back to his
question over here about

00:22:33.342 --> 00:22:35.300
whether we should be
concerned about not having

00:22:35.300 --> 00:22:38.630
flame retardants-- so is there
a problem with your furniture

00:22:38.630 --> 00:22:39.830
catching on fire?

00:22:39.830 --> 00:22:41.557
And the answer is probably yes.

00:22:41.557 --> 00:22:43.640
Like we build our furniture
and many of the things

00:22:43.640 --> 00:22:47.390
that are in our house out of
incredibly flammable materials.

00:22:47.390 --> 00:22:50.540
And foam, in particular,
goes up really fast.

00:22:50.540 --> 00:22:52.280
It burns really
hot, and it produces

00:22:52.280 --> 00:22:54.710
lots of really toxic smoke.

00:22:54.710 --> 00:22:58.440
So that's obviously not ideal.

00:22:58.440 --> 00:23:02.990
It turns out that in tests done
by the Consumer Product Safety

00:23:02.990 --> 00:23:07.730
Division, sofas that were
treated with flame retardants

00:23:07.730 --> 00:23:11.690
didn't actually burn any
less quickly, any less hot,

00:23:11.690 --> 00:23:14.120
and they were a little bit
more toxic because of the added

00:23:14.120 --> 00:23:15.830
flame retardants.

00:23:15.830 --> 00:23:18.470
So it's not that we don't have
a problem with our furniture

00:23:18.470 --> 00:23:19.320
catching on fire.

00:23:19.320 --> 00:23:21.320
It's less of a problem
now, because fewer people

00:23:21.320 --> 00:23:24.540
smoke in bed or in their
couches or whatever.

00:23:24.540 --> 00:23:26.730
But we do have incredibly
flammable furniture.

00:23:26.730 --> 00:23:28.730
It's just that the flame
retardants don't really

00:23:28.730 --> 00:23:30.660
help with that problem.

00:23:30.660 --> 00:23:33.830
So good news and bad news there.

00:23:33.830 --> 00:23:36.580
So now, as I promised you, we're
going to talk about some tips

00:23:36.580 --> 00:23:38.560
for doing it yourself.

00:23:38.560 --> 00:23:40.900
So the first thing
I'll say is that, when

00:23:40.900 --> 00:23:43.420
we started on this
project, at least

00:23:43.420 --> 00:23:46.518
I thought that it was going
to be much easier to do

00:23:46.518 --> 00:23:47.560
than it turned out to be.

00:23:47.560 --> 00:23:49.660
Because I thought,
I have a huge PhD,

00:23:49.660 --> 00:23:51.460
and I know how to read a paper.

00:23:51.460 --> 00:23:53.800
I'll just go to the literature,
and I will read papers,

00:23:53.800 --> 00:23:55.660
and I will decide.

00:23:55.660 --> 00:23:57.710
And it turned out to
be really hard to do,

00:23:57.710 --> 00:24:00.040
partly because it's
really hard to identify

00:24:00.040 --> 00:24:04.840
good sources in a field
that is not your own field.

00:24:04.840 --> 00:24:08.500
So we had to spend some time
thinking about how we really

00:24:08.500 --> 00:24:11.890
were going to identify good
sources and credible sources,

00:24:11.890 --> 00:24:14.290
and sort of condense
the information.

00:24:14.290 --> 00:24:17.410
So we came up with a tip sheet,
which we've actually included

00:24:17.410 --> 00:24:18.860
in the appendix of the book.

00:24:18.860 --> 00:24:20.777
And I'm going to give
it to you for free right

00:24:20.777 --> 00:24:22.720
here, all my secrets, right now.

00:24:22.720 --> 00:24:24.390
So the first thing
that I would suggest

00:24:24.390 --> 00:24:27.930
is first to identify your
priorities, the things that you

00:24:27.930 --> 00:24:29.470
would like to worry about.

00:24:29.470 --> 00:24:32.917
And that's because we've
created this Worry Index,

00:24:32.917 --> 00:24:35.250
but it's based on what we
think is worth worrying about,

00:24:35.250 --> 00:24:37.520
not necessarily what
everybody else thinks

00:24:37.520 --> 00:24:38.520
is worth worrying about.

00:24:38.520 --> 00:24:39.680
And everybody is different.

00:24:39.680 --> 00:24:41.138
Everybody's
individual, so you have

00:24:41.138 --> 00:24:44.760
to make your own
sort of priorities

00:24:44.760 --> 00:24:45.685
before you even start.

00:24:45.685 --> 00:24:47.310
Because otherwise,
you can dig yourself

00:24:47.310 --> 00:24:50.700
into a very deep
hole very quickly.

00:24:50.700 --> 00:24:53.082
The second point is to
use credible sources.

00:24:53.082 --> 00:24:54.540
And this is the
part where I really

00:24:54.540 --> 00:24:56.640
see people going wrong quickly.

00:24:56.640 --> 00:25:01.050
I have seen many, many
poorly-informed people

00:25:01.050 --> 00:25:03.760
on the internet
spreading their ideas.

00:25:03.760 --> 00:25:07.260
And it's not always intentional.

00:25:07.260 --> 00:25:09.900
There's a lot of
misunderstanding.

00:25:09.900 --> 00:25:12.870
But you don't want to take
somebody's misunderstanding

00:25:12.870 --> 00:25:16.170
and use it as the
guide for your life.

00:25:16.170 --> 00:25:18.900
And that means that you
need to seek out expertise.

00:25:18.900 --> 00:25:21.480
And we live in a time right
now where expertise is perhaps

00:25:21.480 --> 00:25:25.210
a little undervalued, and
I think that's unfortunate.

00:25:25.210 --> 00:25:28.800
I think that when people spend
all of their time and effort

00:25:28.800 --> 00:25:30.630
learning about
something, they probably

00:25:30.630 --> 00:25:32.790
have something useful
to say about it.

00:25:32.790 --> 00:25:34.770
And we should seek out
those people when we're

00:25:34.770 --> 00:25:36.540
trying to make our decisions.

00:25:36.540 --> 00:25:39.720
Anybody can have an opinion,
but not everybody's opinion

00:25:39.720 --> 00:25:43.092
is equally valid.

00:25:43.092 --> 00:25:44.550
The other thing
that I would say is

00:25:44.550 --> 00:25:46.742
to read laterally, which
is a new term to me,

00:25:46.742 --> 00:25:47.700
so I'll just define it.

00:25:47.700 --> 00:25:49.930
You might already be
completely familiar with it.

00:25:49.930 --> 00:25:52.020
So the idea is that,
especially on the internet,

00:25:52.020 --> 00:25:55.230
if you're reading vertically,
you're starting at the top,

00:25:55.230 --> 00:25:57.420
and you're reading
down to the bottom.

00:25:57.420 --> 00:26:00.270
And that doesn't always
give you as much information

00:26:00.270 --> 00:26:03.515
about that source
as you would like.

00:26:03.515 --> 00:26:04.890
So to read laterally,
what you do

00:26:04.890 --> 00:26:07.230
is, you actually open
a new tab, and then

00:26:07.230 --> 00:26:09.870
you search for the source
that you're looking at

00:26:09.870 --> 00:26:12.480
and see what other people
have to say about it.

00:26:12.480 --> 00:26:14.532
Because the internet
being what it is,

00:26:14.532 --> 00:26:15.990
there will probably
be other people

00:26:15.990 --> 00:26:18.532
who have thought about what that
source is and whether or not

00:26:18.532 --> 00:26:19.770
it's a reliable source.

00:26:19.770 --> 00:26:23.490
And so whenever you're deciding
whether you should believe what

00:26:23.490 --> 00:26:26.040
you read, you should
think about who

00:26:26.040 --> 00:26:29.010
it is that's telling you that,
why they're telling you that,

00:26:29.010 --> 00:26:32.040
what their credentials are
for telling you that, what

00:26:32.040 --> 00:26:34.380
their biases are behind that.

00:26:34.380 --> 00:26:37.660
We usually recommend that people
start with government agencies.

00:26:37.660 --> 00:26:40.230
And people don't like to hear,
start with the government

00:26:40.230 --> 00:26:41.790
agency.

00:26:41.790 --> 00:26:44.520
But let me just say this--

00:26:44.520 --> 00:26:47.280
the government funds
science out of public funds,

00:26:47.280 --> 00:26:49.980
which means that the
requirements for transparency

00:26:49.980 --> 00:26:52.440
are very high.

00:26:52.440 --> 00:26:54.960
Grants are reviewed
by other scientists.

00:26:54.960 --> 00:26:58.590
Papers are reviewed
by other scientists.

00:26:58.590 --> 00:27:01.320
Government labs that do
research have to post their data

00:27:01.320 --> 00:27:03.360
and make their
methods available.

00:27:03.360 --> 00:27:06.420
That is not the case
for private think tanks,

00:27:06.420 --> 00:27:09.300
and it may not be clear where
their money is coming from,

00:27:09.300 --> 00:27:11.680
why they're giving
you that information.

00:27:11.680 --> 00:27:16.350
So think what you may
about government agencies,

00:27:16.350 --> 00:27:18.240
but it's at least a very
good place to start.

00:27:18.240 --> 00:27:21.050
So I always like the National
Institutes of Health.

00:27:21.050 --> 00:27:22.778
There are multiple
institutes of health.

00:27:22.778 --> 00:27:24.320
They often produce
fact sheets, which

00:27:24.320 --> 00:27:27.243
are very helpful, the
Centers for Disease Control.

00:27:27.243 --> 00:27:28.910
The World Health
Organization, while not

00:27:28.910 --> 00:27:30.560
being truly a
governmental agency,

00:27:30.560 --> 00:27:35.840
is also a very reliable
source, and the FDA, as well.

00:27:35.840 --> 00:27:37.910
Seek out expertise,
and then look

00:27:37.910 --> 00:27:40.340
for consensus in the field.

00:27:40.340 --> 00:27:43.550
So I say this because I actually
teach at a medical school,

00:27:43.550 --> 00:27:46.660
and I have my students review
scientific papers for me.

00:27:46.660 --> 00:27:50.690
And I always have a student
that reviews a paper

00:27:50.690 --> 00:27:53.630
about the dangers of vaccines.

00:27:53.630 --> 00:27:56.270
And here's my point about that.

00:27:56.270 --> 00:27:58.630
Vaccines aren't covered
in the book, first of all.

00:27:58.630 --> 00:28:00.980
So my point about
that is, yes, you

00:28:00.980 --> 00:28:04.130
can always find some
scientific literature that

00:28:04.130 --> 00:28:05.600
will review the risks.

00:28:05.600 --> 00:28:07.550
And it's not that
there aren't any risks,

00:28:07.550 --> 00:28:10.070
and it's not that those things
aren't worth investigating,

00:28:10.070 --> 00:28:13.580
but you need to look at the
entire body of literature

00:28:13.580 --> 00:28:16.970
surrounding any topic
before you make a decision.

00:28:16.970 --> 00:28:19.160
There are lots of different
opinions in science,

00:28:19.160 --> 00:28:20.000
and that's good.

00:28:20.000 --> 00:28:21.950
There should be lots
of different opinions.

00:28:21.950 --> 00:28:24.680
But when you're
trying to figure out

00:28:24.680 --> 00:28:30.110
the global truth about a
topic, what everybody thinks

00:28:30.110 --> 00:28:32.180
is going to settle
closer to the truth

00:28:32.180 --> 00:28:34.340
than what any individual thinks.

00:28:34.340 --> 00:28:37.440
So it's important to think
about what everybody is saying,

00:28:37.440 --> 00:28:39.110
not just what one paper says.

00:28:39.110 --> 00:28:45.360
One paper is never enough to
make a decision about anything.

00:28:45.360 --> 00:28:48.760
And then finally-- or
not finally, I have more.

00:28:48.760 --> 00:28:51.230
Next, I would say understand
probabilities, at least

00:28:51.230 --> 00:28:52.370
a little bit.

00:28:52.370 --> 00:28:55.200
Because scientists like
to speak in probabilities,

00:28:55.200 --> 00:28:57.230
which can be very
frustrating for people.

00:28:57.230 --> 00:29:00.800
They want to know, will
this give me cancer or not?

00:29:00.800 --> 00:29:02.330
But that's not how life works.

00:29:02.330 --> 00:29:04.080
Because everything
is multifactorial.

00:29:04.080 --> 00:29:07.970
So everything has a probability
or a chance of impacting

00:29:07.970 --> 00:29:09.800
anything happening to you.

00:29:09.800 --> 00:29:13.520
And a scientist will very
rarely say, if you eat this,

00:29:13.520 --> 00:29:15.530
you will die of cancer.

00:29:15.530 --> 00:29:17.180
They will say,
this will increase

00:29:17.180 --> 00:29:19.610
your risk of dying of cancer.

00:29:19.610 --> 00:29:22.850
So think about what
that means really,

00:29:22.850 --> 00:29:26.240
how it applies to you, how it
applies to your particular risk

00:29:26.240 --> 00:29:27.680
category.

00:29:27.680 --> 00:29:29.360
Because not everything
is going to apply

00:29:29.360 --> 00:29:31.880
to everybody in the same way.

00:29:31.880 --> 00:29:33.740
I would say, be skeptical.

00:29:33.740 --> 00:29:37.790
And this is a skill that we
maybe have lost a little bit.

00:29:37.790 --> 00:29:41.840
It's important for
somebody to convince you

00:29:41.840 --> 00:29:44.780
of why what they're
saying is true.

00:29:44.780 --> 00:29:48.120
First, when you hear something,
when somebody says to you,

00:29:48.120 --> 00:29:50.570
you should definitely
panic about this right now,

00:29:50.570 --> 00:29:52.850
your first instinct
should be, why?

00:29:52.850 --> 00:29:54.570
Why should I panic
about that right now?

00:29:54.570 --> 00:29:56.153
And if you give me
a good reason, then

00:29:56.153 --> 00:29:59.240
maybe I will panic about it.

00:29:59.240 --> 00:30:00.770
And then actually,
my last point is,

00:30:00.770 --> 00:30:02.270
I would say don't
panic about it.

00:30:02.270 --> 00:30:04.372
And that's easier said
than done, even for me.

00:30:04.372 --> 00:30:06.330
I can stand here and say,
don't panic about it.

00:30:06.330 --> 00:30:08.580
And then I'll go home and
have a little minor freakout

00:30:08.580 --> 00:30:09.830
about something.

00:30:09.830 --> 00:30:12.860
But for me, and I
think for everybody,

00:30:12.860 --> 00:30:15.470
it can help to
make an action plan

00:30:15.470 --> 00:30:17.600
and do something
about the things

00:30:17.600 --> 00:30:19.790
that you can do
something about and stop

00:30:19.790 --> 00:30:22.340
worrying about the things that
you can't do anything about.

00:30:22.340 --> 00:30:24.270
I'm not the first person
to have said that.

00:30:24.270 --> 00:30:26.060
I'm just suggesting
that you apply

00:30:26.060 --> 00:30:28.820
a logical and rigorous method
to thinking about the things

00:30:28.820 --> 00:30:30.028
that you want to worry about.

00:30:31.747 --> 00:30:33.580
Would you like to add
anything, Dr. Chudler?

00:30:33.580 --> 00:30:34.340
ERIC CHUDLER: No.

00:30:34.340 --> 00:30:35.420
No.

00:30:35.420 --> 00:30:36.693
LISE A. JOHNSON: OK.

00:30:36.693 --> 00:30:38.610
So at this point, we
have a couple of minutes.

00:30:38.610 --> 00:30:40.443
So we can go through a
couple more examples,

00:30:40.443 --> 00:30:42.484
or we could go to questions.

00:30:42.484 --> 00:30:43.370
AUDIENCE: Hi.

00:30:43.370 --> 00:30:44.912
So I'm wondering if
you have anything

00:30:44.912 --> 00:30:49.072
to say about the disconnect
between what you may logically

00:30:49.072 --> 00:30:50.780
understand is something
not to be worried

00:30:50.780 --> 00:30:55.460
about but subconsciously
or against your logic,

00:30:55.460 --> 00:30:56.888
you are worrying
about it anyway.

00:30:56.888 --> 00:30:58.430
And I think for a
lot of people, it's

00:30:58.430 --> 00:31:00.483
probably the size
of the bubble is

00:31:00.483 --> 00:31:02.900
a indicator of how much they'll
worry about it, regardless

00:31:02.900 --> 00:31:04.067
of where it is on that plot.

00:31:04.067 --> 00:31:06.080
So do you have any
techniques for how

00:31:06.080 --> 00:31:08.767
to let your logical thinking
start to overcome the worry?

00:31:08.767 --> 00:31:11.100
LISE A. JOHNSON: Well, I can
tell you what works for me,

00:31:11.100 --> 00:31:13.070
but I don't know that that
would work for everybody.

00:31:13.070 --> 00:31:14.960
And Eric probably has a
comment on that, as well.

00:31:14.960 --> 00:31:16.793
Although I don't think
he worries in general

00:31:16.793 --> 00:31:19.050
as much as I worry about things.

00:31:19.050 --> 00:31:22.280
So for me, this
process actually really

00:31:22.280 --> 00:31:25.640
helps, to go through and
look at what the evidence is,

00:31:25.640 --> 00:31:27.223
and then to make a
conscious decision.

00:31:27.223 --> 00:31:28.973
And then remind myself
every now and then,

00:31:28.973 --> 00:31:30.530
like I already
decided I'm not going

00:31:30.530 --> 00:31:32.120
to worry about flame
retardants, because I can't

00:31:32.120 --> 00:31:33.410
do anything more about it.

00:31:36.140 --> 00:31:39.510
But that's not always
enough for everybody,

00:31:39.510 --> 00:31:42.020
especially if you tend to
be a higher anxiety person.

00:31:42.020 --> 00:31:43.820
And there are other
techniques, obviously,

00:31:43.820 --> 00:31:45.080
for just dealing with stress.

00:31:45.080 --> 00:31:48.080
Like exercise is really good at
reducing people's stress level,

00:31:48.080 --> 00:31:52.980
or meditating, or praying,
or a lot of other techniques.

00:31:52.980 --> 00:31:54.980
The world is a stressful
place, and like there's

00:31:54.980 --> 00:31:57.080
no way that you're
going to eliminate

00:31:57.080 --> 00:31:59.540
all of the things
that could potentially

00:31:59.540 --> 00:32:01.400
kill you or hurt you.

00:32:01.400 --> 00:32:04.760
So in some sense, you have
to come to grips with that

00:32:04.760 --> 00:32:07.642
in whatever way you can.

00:32:07.642 --> 00:32:10.100
So this can help to some extent,
but maybe not all the way,

00:32:10.100 --> 00:32:11.193
is what I would say.

00:32:11.193 --> 00:32:13.110
ERIC CHUDLER: And you
might think to yourself,

00:32:13.110 --> 00:32:17.450
why would someone have worry
about a particular thing

00:32:17.450 --> 00:32:20.803
if they're really logic
doesn't point that direction.

00:32:20.803 --> 00:32:22.220
And I think part
of that reason is

00:32:22.220 --> 00:32:24.470
because some of these
things that happen rarely--

00:32:24.470 --> 00:32:26.970
for example, shark attacks.

00:32:26.970 --> 00:32:30.050
Some people are afraid of
getting eaten by a shark.

00:32:30.050 --> 00:32:35.810
How many people do you think
a year get killed by sharks?

00:32:35.810 --> 00:32:37.160
Just throw out some numbers.

00:32:37.160 --> 00:32:39.130
[INTERPOSING VOICES]

00:32:39.130 --> 00:32:40.096
ERIC CHUDLER: 100.

00:32:40.096 --> 00:32:42.770
Five in the entire world.

00:32:42.770 --> 00:32:45.800
How many people do you think get
attacked, an unprovoked shark

00:32:45.800 --> 00:32:47.510
attack in the world?

00:32:47.510 --> 00:32:48.430
What number?

00:32:48.430 --> 00:32:50.960
Not killed, just attacked,
bitten by a shark.

00:32:50.960 --> 00:32:51.970
How many?

00:32:51.970 --> 00:32:52.970
Throw out some numbers.

00:32:52.970 --> 00:32:54.040
AUDIENCE: 50.

00:32:54.040 --> 00:32:54.700
AUDIENCE: 10.

00:32:54.700 --> 00:32:56.000
ERIC CHUDLER: 50, 10, yeah.

00:32:56.000 --> 00:32:58.070
So now you're kind
of going down.

00:32:58.070 --> 00:33:02.450
But it's less than 100
in the entire world.

00:33:02.450 --> 00:33:03.320
And think of all the

00:33:03.320 --> 00:33:06.710
Millions of people
that swim in the ocean.

00:33:06.710 --> 00:33:09.830
Yet people are afraid
of shark attack.

00:33:09.830 --> 00:33:12.230
Well, it's because when a
shark attack does happen,

00:33:12.230 --> 00:33:14.410
it's horrific.

00:33:14.410 --> 00:33:18.050
So every time a
surfer's surfboard

00:33:18.050 --> 00:33:21.050
gets chopped by a
shark, it's in the news.

00:33:21.050 --> 00:33:22.940
And so the media
plays up these things,

00:33:22.940 --> 00:33:25.250
and plays on people's
fears, and so that

00:33:25.250 --> 00:33:29.595
might cause some illogical
fears and worries in people.

00:33:29.595 --> 00:33:31.970
But when you look at the real
data, a lot of these things

00:33:31.970 --> 00:33:33.380
are nothing to worry about.

00:33:36.278 --> 00:33:37.017
AUDIENCE: Hi.

00:33:37.017 --> 00:33:37.850
LISE A. JOHNSON: Hi.

00:33:37.850 --> 00:33:40.058
ERIC CHUDLER: You mentioned
that, when I do research,

00:33:40.058 --> 00:33:43.000
I should look at the entire
body of scientific literature,

00:33:43.000 --> 00:33:44.620
not just read one piece.

00:33:44.620 --> 00:33:46.520
But I think historically,
a lot of things

00:33:46.520 --> 00:33:48.920
have been breakthroughs
because they were

00:33:48.920 --> 00:33:50.340
in the minority or unique--

00:33:50.340 --> 00:33:53.032
for example, Darwin, Galileo.

00:33:53.032 --> 00:33:54.740
And these people have
historically always

00:33:54.740 --> 00:33:56.180
been persecuted.

00:33:56.180 --> 00:33:58.820
So I guess my question is--

00:33:58.820 --> 00:34:03.110
like nowadays, the people who
are into anti-vax probably

00:34:03.110 --> 00:34:04.660
feel that way.

00:34:04.660 --> 00:34:07.940
And I can't convince
them they're wrong.

00:34:07.940 --> 00:34:11.510
And I'm wondering, how
do I interact with people

00:34:11.510 --> 00:34:12.920
who feel this way?

00:34:12.920 --> 00:34:14.420
LISE A. JOHNSON: I
feel like we have

00:34:14.420 --> 00:34:16.003
multiple questions
in there, so let me

00:34:16.003 --> 00:34:17.699
see if I can unpack
them a little bit.

00:34:17.699 --> 00:34:21.489
So first is the point about,
science has always been,

00:34:21.489 --> 00:34:23.460
the minority is
pushing it forward.

00:34:23.460 --> 00:34:27.210
I think that that's actually an
exception rather than a rule.

00:34:27.210 --> 00:34:30.650
So it's notable when it happens,
because it happens rarely,

00:34:30.650 --> 00:34:32.810
not all the time.

00:34:32.810 --> 00:34:34.860
So yes, it does
sometimes happen.

00:34:34.860 --> 00:34:37.219
So you have to acknowledge
that sometimes everybody

00:34:37.219 --> 00:34:40.310
thinks there's an
ether and there's not.

00:34:40.310 --> 00:34:44.480
But often, science moves forward
through lots of small steps

00:34:44.480 --> 00:34:47.576
and a group of people
pushing something forward.

00:34:47.576 --> 00:34:48.659
So that's the first thing.

00:34:48.659 --> 00:34:50.179
The second thing
is, you're right.

00:34:50.179 --> 00:34:53.239
You will never convince
somebody against their will.

00:34:56.270 --> 00:34:59.270
People don't want to be wrong.

00:34:59.270 --> 00:35:02.520
So there's only so
much you can do.

00:35:02.520 --> 00:35:04.970
And I think in
some ways, you have

00:35:04.970 --> 00:35:07.390
to just be at peace with that.

00:35:07.390 --> 00:35:09.475
Like you can't convince
everybody all the time.

00:35:09.475 --> 00:35:11.600
But you can only do what
you can do about yourself,

00:35:11.600 --> 00:35:12.980
and make choices for
yourself, and make

00:35:12.980 --> 00:35:14.100
choices for your family.

00:35:14.100 --> 00:35:19.580
And that's at least my opinion
on how to interact with people.

00:35:19.580 --> 00:35:20.560
I still try.

00:35:20.560 --> 00:35:22.940
I'm a huge sort of
science advocate,

00:35:22.940 --> 00:35:28.220
an advocate for logical thinking
and for looking at data.

00:35:28.220 --> 00:35:30.170
The day that our
book was released,

00:35:30.170 --> 00:35:33.680
I got an angry email from
somebody about fluoride.

00:35:36.440 --> 00:35:38.330
I can't change
that person's mind.

00:35:38.330 --> 00:35:41.090
It doesn't matter how
much data I show them,

00:35:41.090 --> 00:35:43.190
how much data people research.

00:35:43.190 --> 00:35:45.637
If you don't want to
believe it, you don't.

00:35:45.637 --> 00:35:47.470
AUDIENCE: Quick followup
if you don't mind--

00:35:47.470 --> 00:35:49.480
you said you can't change
how other people are,

00:35:49.480 --> 00:35:53.440
but you also said that you
advocate scientific thinking.

00:35:53.440 --> 00:35:55.970
I imagine you want to pass
this onto your children,

00:35:55.970 --> 00:35:56.960
your spouse.

00:35:56.960 --> 00:36:01.570
And my question is, how do I
help my partner worry less.

00:36:01.570 --> 00:36:02.740
And I'm asking for a friend.

00:36:02.740 --> 00:36:03.946
[LAUGHTER]

00:36:04.750 --> 00:36:08.970
LISE A. JOHNSON: So
your friend's partner--

00:36:08.970 --> 00:36:12.720
it depends on your
friend's partner, really.

00:36:12.720 --> 00:36:18.610
So I guess I would try and
stay away from being judgy.

00:36:18.610 --> 00:36:22.973
So if you come to somebody with
an open mindset and an openness

00:36:22.973 --> 00:36:24.390
to talk about
things, you're going

00:36:24.390 --> 00:36:29.240
to get a lot farther than if you
come out of the gate swinging,

00:36:29.240 --> 00:36:30.415
is what I would say.

00:36:30.415 --> 00:36:31.610
Maybe you have some--

00:36:31.610 --> 00:36:32.318
ERIC CHUDLER: No.

00:36:32.318 --> 00:36:35.560
No, and I think it's a lost
skill that perhaps we may not

00:36:35.560 --> 00:36:37.810
be able to change
adults' thinking,

00:36:37.810 --> 00:36:41.120
but trying to culture
critical thinking skills

00:36:41.120 --> 00:36:45.950
in children and young
students, perhaps that's

00:36:45.950 --> 00:36:47.890
one way we can
combat some of this,

00:36:47.890 --> 00:36:49.958
these falsehoods
that we know about.

00:36:49.958 --> 00:36:52.250
AUDIENCE: Thank you.

00:36:52.250 --> 00:36:54.000
AUDIENCE: Thanks for speaking.

00:36:54.000 --> 00:36:57.550
I was wondering how
preventability interacts

00:36:57.550 --> 00:37:00.610
with things that are
as a group preventable,

00:37:00.610 --> 00:37:03.010
but as an individual you
can do very little about.

00:37:03.010 --> 00:37:04.670
Like climate change
is an example,

00:37:04.670 --> 00:37:07.090
that individually we
can almost do nothing,

00:37:07.090 --> 00:37:11.790
but would be addressable if
there were a group action.

00:37:11.790 --> 00:37:14.210
LISE A. JOHNSON: So
in the book, we really

00:37:14.210 --> 00:37:18.110
only thought about things you
can do personally to prevent.

00:37:18.110 --> 00:37:21.020
Obviously, corporate action is
a much more difficult problem.

00:37:24.140 --> 00:37:26.240
I don't have any great
answers for that.

00:37:26.240 --> 00:37:30.980
I think, like, vote is obviously
a big win, but not perfect,

00:37:30.980 --> 00:37:35.318
and community organization
and a number of other things.

00:37:35.318 --> 00:37:36.110
That's a tough nut.

00:37:36.110 --> 00:37:37.520
I don't know if I have anything.

00:37:37.520 --> 00:37:38.750
ERIC CHUDLER: Yeah, that's the
only thing that I thought of,

00:37:38.750 --> 00:37:41.150
is voting for the
types of things

00:37:41.150 --> 00:37:43.620
that you think people
should be doing.

00:37:43.620 --> 00:37:44.120
Vote.

00:37:46.940 --> 00:37:49.950
AUDIENCE: So my question
might be a little bit related.

00:37:49.950 --> 00:37:52.880
I've noticed there are certain
cases where our worries, even

00:37:52.880 --> 00:37:56.420
though they are on the
unlikely site, the fact

00:37:56.420 --> 00:38:00.220
that we worry changes
it, makes it unlikely.

00:38:00.220 --> 00:38:01.770
Let me give you an example.

00:38:01.770 --> 00:38:05.690
Let's say turning America
into dictatorship theocracy,

00:38:05.690 --> 00:38:08.180
anything else that
it's not today--

00:38:08.180 --> 00:38:09.860
it's not happening,
and the priority

00:38:09.860 --> 00:38:13.940
is low, because we worry about
it all the time, because we

00:38:13.940 --> 00:38:15.990
can do something about it.

00:38:15.990 --> 00:38:19.700
So it's low.

00:38:19.700 --> 00:38:21.410
If you look at the
probability, it's low.

00:38:21.410 --> 00:38:24.582
The impact this high, but it's
low because of our worries.

00:38:24.582 --> 00:38:25.790
What do you think about that?

00:38:28.700 --> 00:38:30.545
LISE A. JOHNSON: Yeah,
when I see students

00:38:30.545 --> 00:38:32.420
that are about to defend
their dissertations,

00:38:32.420 --> 00:38:33.830
and they're really stressed
out about it, I'm like,

00:38:33.830 --> 00:38:34.970
I'm not worried about
you, because you're

00:38:34.970 --> 00:38:35.720
worried about it.

00:38:35.720 --> 00:38:38.780
And if you weren't worried about
it, then I would be worried.

00:38:38.780 --> 00:38:42.110
And I think that's sort
of a similar situation.

00:38:42.110 --> 00:38:43.760
But I actually think
that that means

00:38:43.760 --> 00:38:46.820
that it is high on the
preventability scale, that it

00:38:46.820 --> 00:38:48.620
doesn't happen because
it is preventable,

00:38:48.620 --> 00:38:51.470
and we take action against
it, and that prevents it

00:38:51.470 --> 00:38:52.640
from happening.

00:38:52.640 --> 00:38:54.803
AUDIENCE: Yes, but it's
low on the probability.

00:38:54.803 --> 00:38:56.720
LISE A. JOHNSON: Well,
it's unlikely to happen

00:38:56.720 --> 00:38:59.933
because it's highly preventable,
I guess, is what I would say.

00:38:59.933 --> 00:39:01.850
ERIC CHUDLER: And that's
actually interesting.

00:39:01.850 --> 00:39:04.190
Perhaps that's a new
chapter for the book.

00:39:04.190 --> 00:39:08.190
And that would be, should
you worry about worrying?

00:39:08.190 --> 00:39:11.870
And the answer is probably
yes, because worrying does

00:39:11.870 --> 00:39:13.760
have physiological effects.

00:39:13.760 --> 00:39:14.930
It could reduce your sleep.

00:39:14.930 --> 00:39:18.410
It can cause stress, which cause
other physiological problems.

00:39:18.410 --> 00:39:20.990
So worrying about worrying
is probably something

00:39:20.990 --> 00:39:23.930
that we should worry about.

00:39:23.930 --> 00:39:24.850
AUDIENCE: Thank you.

00:39:24.850 --> 00:39:28.160
LISE A. JOHNSON: Or do
something about, yeah.

00:39:28.160 --> 00:39:30.800
AUDIENCE: You mention about
government organizations,

00:39:30.800 --> 00:39:33.920
but we have some examples,
like Flint, crisis

00:39:33.920 --> 00:39:35.540
where they were untruthful.

00:39:35.540 --> 00:39:40.492
So how do you look critically
at government sources, as well?

00:39:40.492 --> 00:39:42.200
LISE A. JOHNSON: So
I guess first of all,

00:39:42.200 --> 00:39:44.090
I would distinguish
between different kinds

00:39:44.090 --> 00:39:45.355
of government organizations.

00:39:45.355 --> 00:39:47.480
So when I'm talking about
government organizations,

00:39:47.480 --> 00:39:51.950
I'm talking about research-based
government organizations, not

00:39:51.950 --> 00:39:55.490
necessarily city or
government officials that

00:39:55.490 --> 00:39:56.670
are enacting policy.

00:39:56.670 --> 00:40:00.170
I think that's
slightly different.

00:40:00.170 --> 00:40:04.430
But again, so I think
government fact sheets

00:40:04.430 --> 00:40:06.260
are good places to start.

00:40:06.260 --> 00:40:08.650
And in some cases, I think
they're good places to finish,

00:40:08.650 --> 00:40:09.150
as well.

00:40:09.150 --> 00:40:11.233
But if it's something that
you're concerned about,

00:40:11.233 --> 00:40:13.280
then there are other resources.

00:40:13.280 --> 00:40:18.080
And there's other governments
with other fact sheets,

00:40:18.080 --> 00:40:19.170
so that works, as well.

00:40:19.170 --> 00:40:21.500
So the National Health
Service in the UK

00:40:21.500 --> 00:40:24.230
produces their own set of fact
sheets and you can compare.

00:40:24.230 --> 00:40:29.030
But then you can also go
to the primary literature.

00:40:29.030 --> 00:40:30.920
I try and caution people
when they do that,

00:40:30.920 --> 00:40:33.770
because the chances
of misinterpretation

00:40:33.770 --> 00:40:36.560
if you're not part of that
field can be pretty high.

00:40:36.560 --> 00:40:39.540
But that's always an
option that's out there.

00:40:39.540 --> 00:40:40.670
There are online databases.

00:40:40.670 --> 00:40:42.810
So Google Scholar is the
one that I always use.

00:40:42.810 --> 00:40:46.280
But there's also PubMed,
and you can look there

00:40:46.280 --> 00:40:48.920
for primary information
and seek out those sources

00:40:48.920 --> 00:40:51.250
and read them for yourself.

00:40:51.250 --> 00:40:53.800
AUDIENCE: How do you guys
factor in return on investment

00:40:53.800 --> 00:40:54.730
for preventability?

00:40:54.730 --> 00:40:56.260
Say something's
very preventable,

00:40:56.260 --> 00:40:58.480
but I'd have to sell my
home and move to Alaska.

00:41:03.538 --> 00:41:05.080
LISE A. JOHNSON:
That's why it's like

00:41:05.080 --> 00:41:06.880
a three-dimensional problem.

00:41:06.880 --> 00:41:09.550
So it's very preventable,
but it's very unlikely.

00:41:09.550 --> 00:41:13.000
Probably don't sell your
house and move to Alaska.

00:41:13.000 --> 00:41:15.853
You're increasing your odds of
getting eaten by a polar bear,

00:41:15.853 --> 00:41:17.770
so you have to really
factor that in as you're

00:41:17.770 --> 00:41:19.180
making that decision.

00:41:19.180 --> 00:41:22.870
And also the size of the
risk, and just prioritizing,

00:41:22.870 --> 00:41:24.760
is it really that
important to me

00:41:24.760 --> 00:41:28.270
that it's worth making a
major lifestyle change?

00:41:28.270 --> 00:41:31.030
And I think that's kind of
what it comes down to is,

00:41:31.030 --> 00:41:32.630
get all of the
data, and then say,

00:41:32.630 --> 00:41:34.248
well, what's it worth to me?

00:41:34.248 --> 00:41:35.290
And is it worth the cost?

00:41:35.290 --> 00:41:37.248
Is it really just a very
small marginal return,

00:41:37.248 --> 00:41:39.070
or is it going to
make a huge difference

00:41:39.070 --> 00:41:40.660
in my quality of life?

00:41:40.660 --> 00:41:42.700
And nobody can make
that decision for you.

00:41:42.700 --> 00:41:44.563
You've got to do your own thing.

00:41:44.563 --> 00:41:46.480
AUDIENCE: Well, thank
you very much for coming

00:41:46.480 --> 00:41:47.620
to talk to us today.

00:41:47.620 --> 00:41:50.670
[APPLAUSE]

