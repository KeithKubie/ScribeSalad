WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.720
hello and welcome back in this week you

00:00:02.720 --> 00:00:02.730
hello and welcome back in this week you
 

00:00:02.730 --> 00:00:04.760
hello and welcome back in this week you
learn about optimization algorithms that

00:00:04.760 --> 00:00:04.770
learn about optimization algorithms that
 

00:00:04.770 --> 00:00:06.170
learn about optimization algorithms that
will enable you to train in your

00:00:06.170 --> 00:00:06.180
will enable you to train in your
 

00:00:06.180 --> 00:00:08.780
will enable you to train in your
networks much faster you've heard me say

00:00:08.780 --> 00:00:08.790
networks much faster you've heard me say
 

00:00:08.790 --> 00:00:10.700
networks much faster you've heard me say
before that apply machine learning is a

00:00:10.700 --> 00:00:10.710
before that apply machine learning is a
 

00:00:10.710 --> 00:00:12.860
before that apply machine learning is a
highly empirical process is highly

00:00:12.860 --> 00:00:12.870
highly empirical process is highly
 

00:00:12.870 --> 00:00:15.079
highly empirical process is highly
intuitive process it which you just have

00:00:15.079 --> 00:00:15.089
intuitive process it which you just have
 

00:00:15.089 --> 00:00:16.910
intuitive process it which you just have
to train a lot of models to find one

00:00:16.910 --> 00:00:16.920
to train a lot of models to find one
 

00:00:16.920 --> 00:00:18.740
to train a lot of models to find one
that works really well so it really

00:00:18.740 --> 00:00:18.750
that works really well so it really
 

00:00:18.750 --> 00:00:21.320
that works really well so it really
helps to really train models quickly one

00:00:21.320 --> 00:00:21.330
helps to really train models quickly one
 

00:00:21.330 --> 00:00:22.939
helps to really train models quickly one
thing that makes it more difficult is

00:00:22.939 --> 00:00:22.949
thing that makes it more difficult is
 

00:00:22.949 --> 00:00:24.470
thing that makes it more difficult is
that deep learning which is the work

00:00:24.470 --> 00:00:24.480
that deep learning which is the work
 

00:00:24.480 --> 00:00:26.750
that deep learning which is the work
best in the regime of Big Data when

00:00:26.750 --> 00:00:26.760
best in the regime of Big Data when
 

00:00:26.760 --> 00:00:27.920
best in the regime of Big Data when
you're able to train your near network

00:00:27.920 --> 00:00:27.930
you're able to train your near network
 

00:00:27.930 --> 00:00:30.980
you're able to train your near network
on a huge data set and training on large

00:00:30.980 --> 00:00:30.990
on a huge data set and training on large
 

00:00:30.990 --> 00:00:33.979
on a huge data set and training on large
data sets is just slow so what you find

00:00:33.979 --> 00:00:33.989
data sets is just slow so what you find
 

00:00:33.989 --> 00:00:36.260
data sets is just slow so what you find
is that having fast optimization

00:00:36.260 --> 00:00:36.270
is that having fast optimization
 

00:00:36.270 --> 00:00:37.520
is that having fast optimization
algorithms having good optimization

00:00:37.520 --> 00:00:37.530
algorithms having good optimization
 

00:00:37.530 --> 00:00:39.619
algorithms having good optimization
algorithms can really speed up the

00:00:39.619 --> 00:00:39.629
algorithms can really speed up the
 

00:00:39.629 --> 00:00:42.049
algorithms can really speed up the
efficiency of you and your team so let's

00:00:42.049 --> 00:00:42.059
efficiency of you and your team so let's
 

00:00:42.059 --> 00:00:44.479
efficiency of you and your team so let's
get started by talking about mini-batch

00:00:44.479 --> 00:00:44.489
get started by talking about mini-batch
 

00:00:44.489 --> 00:00:46.610
get started by talking about mini-batch
gradient descent you've learned

00:00:46.610 --> 00:00:46.620
gradient descent you've learned
 

00:00:46.620 --> 00:00:48.830
gradient descent you've learned
previously the vectorization allows you

00:00:48.830 --> 00:00:48.840
previously the vectorization allows you
 

00:00:48.840 --> 00:00:51.049
previously the vectorization allows you
to efficiently compute on all M examples

00:00:51.049 --> 00:00:51.059
to efficiently compute on all M examples
 

00:00:51.059 --> 00:00:53.930
to efficiently compute on all M examples
that allows you to process your whole

00:00:53.930 --> 00:00:53.940
that allows you to process your whole
 

00:00:53.940 --> 00:00:55.970
that allows you to process your whole
training set without an explicit for

00:00:55.970 --> 00:00:55.980
training set without an explicit for
 

00:00:55.980 --> 00:00:58.580
training set without an explicit for
loop so that's why we would take our

00:00:58.580 --> 00:00:58.590
loop so that's why we would take our
 

00:00:58.590 --> 00:01:00.770
loop so that's why we would take our
training examples and stack them into

00:01:00.770 --> 00:01:00.780
training examples and stack them into
 

00:01:00.780 --> 00:01:05.420
training examples and stack them into
this huge matrix capital X so 6 1 X 2 X

00:01:05.420 --> 00:01:05.430
this huge matrix capital X so 6 1 X 2 X
 

00:01:05.430 --> 00:01:08.539
this huge matrix capital X so 6 1 X 2 X
3 you know and then um eventually it

00:01:08.539 --> 00:01:08.549
3 you know and then um eventually it
 

00:01:08.549 --> 00:01:11.960
3 you know and then um eventually it
goes up to X M they give M training

00:01:11.960 --> 00:01:11.970
goes up to X M they give M training
 

00:01:11.970 --> 00:01:15.170
goes up to X M they give M training
examples and similarly for y this is y 1

00:01:15.170 --> 00:01:15.180
examples and similarly for y this is y 1
 

00:01:15.180 --> 00:01:23.330
examples and similarly for y this is y 1
y 2 y 3 and so on up to Y M so the

00:01:23.330 --> 00:01:23.340
y 2 y 3 and so on up to Y M so the
 

00:01:23.340 --> 00:01:27.109
y 2 y 3 and so on up to Y M so the
dimension of X was n X by M and this is

00:01:27.109 --> 00:01:27.119
dimension of X was n X by M and this is
 

00:01:27.119 --> 00:01:30.649
dimension of X was n X by M and this is
1 by M vectorization allows you to

00:01:30.649 --> 00:01:30.659
1 by M vectorization allows you to
 

00:01:30.659 --> 00:01:32.840
1 by M vectorization allows you to
process our M examples quickly

00:01:32.840 --> 00:01:32.850
process our M examples quickly
 

00:01:32.850 --> 00:01:35.840
process our M examples quickly
relatively quickly if M is very large

00:01:35.840 --> 00:01:35.850
relatively quickly if M is very large
 

00:01:35.850 --> 00:01:38.600
relatively quickly if M is very large
then it can still be slow so for example

00:01:38.600 --> 00:01:38.610
then it can still be slow so for example
 

00:01:38.610 --> 00:01:42.170
then it can still be slow so for example
what if M was 5 million you know 50

00:01:42.170 --> 00:01:42.180
what if M was 5 million you know 50
 

00:01:42.180 --> 00:01:45.260
what if M was 5 million you know 50
million or even bigger with the

00:01:45.260 --> 00:01:45.270
million or even bigger with the
 

00:01:45.270 --> 00:01:47.330
million or even bigger with the
implementation of gradient sent on your

00:01:47.330 --> 00:01:47.340
implementation of gradient sent on your
 

00:01:47.340 --> 00:01:49.550
implementation of gradient sent on your
training set what you have to do is you

00:01:49.550 --> 00:01:49.560
training set what you have to do is you
 

00:01:49.560 --> 00:01:51.560
training set what you have to do is you
have to process your entire training set

00:01:51.560 --> 00:01:51.570
have to process your entire training set
 

00:01:51.570 --> 00:01:53.389
have to process your entire training set
before you take you know one little step

00:01:53.389 --> 00:01:53.399
before you take you know one little step
 

00:01:53.399 --> 00:01:55.280
before you take you know one little step
for gradient descent and then you have

00:01:55.280 --> 00:01:55.290
for gradient descent and then you have
 

00:01:55.290 --> 00:01:56.780
for gradient descent and then you have
to process your entire training set of

00:01:56.780 --> 00:01:56.790
to process your entire training set of
 

00:01:56.790 --> 00:01:58.340
to process your entire training set of
five million training examples again

00:01:58.340 --> 00:01:58.350
five million training examples again
 

00:01:58.350 --> 00:01:59.840
five million training examples again
before you take another little step of

00:01:59.840 --> 00:01:59.850
before you take another little step of
 

00:01:59.850 --> 00:02:01.700
before you take another little step of
gradient descent so it turns out that

00:02:01.700 --> 00:02:01.710
gradient descent so it turns out that
 

00:02:01.710 --> 00:02:04.340
gradient descent so it turns out that
you can get a faster algorithm if you

00:02:04.340 --> 00:02:04.350
you can get a faster algorithm if you
 

00:02:04.350 --> 00:02:05.539
you can get a faster algorithm if you
get straightened descent

00:02:05.539 --> 00:02:05.549
get straightened descent
 

00:02:05.549 --> 00:02:07.639
get straightened descent
start to make some progress even before

00:02:07.639 --> 00:02:07.649
start to make some progress even before
 

00:02:07.649 --> 00:02:10.279
start to make some progress even before
you finish processing your entire your

00:02:10.279 --> 00:02:10.289
you finish processing your entire your
 

00:02:10.289 --> 00:02:11.120
you finish processing your entire your
giant tree

00:02:11.120 --> 00:02:11.130
giant tree
 

00:02:11.130 --> 00:02:14.720
giant tree
in size of five million examples in

00:02:14.720 --> 00:02:14.730
in size of five million examples in
 

00:02:14.730 --> 00:02:16.910
in size of five million examples in
particular here's what you can do let's

00:02:16.910 --> 00:02:16.920
particular here's what you can do let's
 

00:02:16.920 --> 00:02:18.470
particular here's what you can do let's
say that you split up your training set

00:02:18.470 --> 00:02:18.480
say that you split up your training set
 

00:02:18.480 --> 00:02:20.990
say that you split up your training set
into smaller your little baby training

00:02:20.990 --> 00:02:21.000
into smaller your little baby training
 

00:02:21.000 --> 00:02:23.690
into smaller your little baby training
sets and these baby training sets are

00:02:23.690 --> 00:02:23.700
sets and these baby training sets are
 

00:02:23.700 --> 00:02:24.050
sets and these baby training sets are
called

00:02:24.050 --> 00:02:24.060
called
 

00:02:24.060 --> 00:02:30.200
called
mini batches and let's say each of your

00:02:30.200 --> 00:02:30.210
mini batches and let's say each of your
 

00:02:30.210 --> 00:02:33.140
mini batches and let's say each of your
baby training sets have just 1000

00:02:33.140 --> 00:02:33.150
baby training sets have just 1000
 

00:02:33.150 --> 00:02:38.200
baby training sets have just 1000
examples each so you take X 1 through X

00:02:38.200 --> 00:02:38.210
examples each so you take X 1 through X
 

00:02:38.210 --> 00:02:41.300
examples each so you take X 1 through X
1000 and you call that your first little

00:02:41.300 --> 00:02:41.310
1000 and you call that your first little
 

00:02:41.310 --> 00:02:43.010
1000 and you call that your first little
baby training session also called a mini

00:02:43.010 --> 00:02:43.020
baby training session also called a mini
 

00:02:43.020 --> 00:02:45.800
baby training session also called a mini
batch and then you take home the next

00:02:45.800 --> 00:02:45.810
batch and then you take home the next
 

00:02:45.810 --> 00:02:52.400
batch and then you take home the next
1000 examples X 1000 1 through X 2000

00:02:52.400 --> 00:02:52.410
1000 examples X 1000 1 through X 2000
 

00:02:52.410 --> 00:02:54.080
1000 examples X 1000 1 through X 2000
that's the next thousand examples and

00:02:54.080 --> 00:02:54.090
that's the next thousand examples and
 

00:02:54.090 --> 00:02:57.110
that's the next thousand examples and
call the next one and so on and I'm

00:02:57.110 --> 00:02:57.120
call the next one and so on and I'm
 

00:02:57.120 --> 00:02:58.700
call the next one and so on and I'm
going to introduce a new notation I'm

00:02:58.700 --> 00:02:58.710
going to introduce a new notation I'm
 

00:02:58.710 --> 00:03:02.390
going to introduce a new notation I'm
going to call this X superscript with

00:03:02.390 --> 00:03:02.400
going to call this X superscript with
 

00:03:02.400 --> 00:03:07.310
going to call this X superscript with
curly braces 1 and I want to call this X

00:03:07.310 --> 00:03:07.320
curly braces 1 and I want to call this X
 

00:03:07.320 --> 00:03:12.380
curly braces 1 and I want to call this X
superscript with curly braces too now if

00:03:12.380 --> 00:03:12.390
superscript with curly braces too now if
 

00:03:12.390 --> 00:03:14.600
superscript with curly braces too now if
you have five million training examples

00:03:14.600 --> 00:03:14.610
you have five million training examples
 

00:03:14.610 --> 00:03:16.220
you have five million training examples
total and each of these little mini

00:03:16.220 --> 00:03:16.230
total and each of these little mini
 

00:03:16.230 --> 00:03:18.470
total and each of these little mini
batches as a thousand examples that

00:03:18.470 --> 00:03:18.480
batches as a thousand examples that
 

00:03:18.480 --> 00:03:21.050
batches as a thousand examples that
means you have 5000 of these videos you

00:03:21.050 --> 00:03:21.060
means you have 5000 of these videos you
 

00:03:21.060 --> 00:03:24.590
means you have 5000 of these videos you
know 5000 times 1000 equals 5 million so

00:03:24.590 --> 00:03:24.600
know 5000 times 1000 equals 5 million so
 

00:03:24.600 --> 00:03:28.400
know 5000 times 1000 equals 5 million so
altogether you would have 5000 of these

00:03:28.400 --> 00:03:28.410
altogether you would have 5000 of these
 

00:03:28.410 --> 00:03:31.940
altogether you would have 5000 of these
um mini batches so the ends of X

00:03:31.940 --> 00:03:31.950
um mini batches so the ends of X
 

00:03:31.950 --> 00:03:34.880
um mini batches so the ends of X
superscript curly braces 5000 and then

00:03:34.880 --> 00:03:34.890
superscript curly braces 5000 and then
 

00:03:34.890 --> 00:03:36.980
superscript curly braces 5000 and then
similarly you do the same thing for y

00:03:36.980 --> 00:03:36.990
similarly you do the same thing for y
 

00:03:36.990 --> 00:03:39.470
similarly you do the same thing for y
you'd also split up your training data

00:03:39.470 --> 00:03:39.480
you'd also split up your training data
 

00:03:39.480 --> 00:03:43.910
you'd also split up your training data
for Y accordingly so you call that y1

00:03:43.910 --> 00:03:43.920
for Y accordingly so you call that y1
 

00:03:43.920 --> 00:03:51.050
for Y accordingly so you call that y1
and then this is y 1001 3y 2000 this

00:03:51.050 --> 00:03:51.060
and then this is y 1001 3y 2000 this
 

00:03:51.060 --> 00:03:56.840
and then this is y 1001 3y 2000 this
becomes called y2 and so on until you

00:03:56.840 --> 00:03:56.850
becomes called y2 and so on until you
 

00:03:56.850 --> 00:04:04.720
becomes called y2 and so on until you
have y 5000 so now we

00:04:04.720 --> 00:04:04.730
have y 5000 so now we
 

00:04:04.730 --> 00:04:07.619
have y 5000 so now we
- number T is going to be comprised of X

00:04:07.619 --> 00:04:07.629
- number T is going to be comprised of X
 

00:04:07.629 --> 00:04:14.680
- number T is going to be comprised of X
T and Y T and that is a thousand

00:04:14.680 --> 00:04:14.690
T and Y T and that is a thousand
 

00:04:14.690 --> 00:04:16.420
T and Y T and that is a thousand
training examples so the corresponding

00:04:16.420 --> 00:04:16.430
training examples so the corresponding
 

00:04:16.430 --> 00:04:19.390
training examples so the corresponding
input output pairs before moving on just

00:04:19.390 --> 00:04:19.400
input output pairs before moving on just
 

00:04:19.400 --> 00:04:20.349
input output pairs before moving on just
to make sure

00:04:20.349 --> 00:04:20.359
to make sure
 

00:04:20.359 --> 00:04:22.990
to make sure
notation is clear we have previously

00:04:22.990 --> 00:04:23.000
notation is clear we have previously
 

00:04:23.000 --> 00:04:25.480
notation is clear we have previously
used superscript round brackets I to

00:04:25.480 --> 00:04:25.490
used superscript round brackets I to
 

00:04:25.490 --> 00:04:27.969
used superscript round brackets I to
index on the training set so X is d I've

00:04:27.969 --> 00:04:27.979
index on the training set so X is d I've
 

00:04:27.979 --> 00:04:30.280
index on the training set so X is d I've
trained example we use superscript

00:04:30.280 --> 00:04:30.290
trained example we use superscript
 

00:04:30.290 --> 00:04:33.490
trained example we use superscript
square brackets L to index into the

00:04:33.490 --> 00:04:33.500
square brackets L to index into the
 

00:04:33.500 --> 00:04:35.140
square brackets L to index into the
different layers of a neural network so

00:04:35.140 --> 00:04:35.150
different layers of a neural network so
 

00:04:35.150 --> 00:04:39.550
different layers of a neural network so
VL comes from the Z values for the elf

00:04:39.550 --> 00:04:39.560
VL comes from the Z values for the elf
 

00:04:39.560 --> 00:04:42.070
VL comes from the Z values for the elf
layer of in your network and here we're

00:04:42.070 --> 00:04:42.080
layer of in your network and here we're
 

00:04:42.080 --> 00:04:45.909
layer of in your network and here we're
introducing the curly brackets T to

00:04:45.909 --> 00:04:45.919
introducing the curly brackets T to
 

00:04:45.919 --> 00:04:48.010
introducing the curly brackets T to
index into different mini batches so you

00:04:48.010 --> 00:04:48.020
index into different mini batches so you
 

00:04:48.020 --> 00:04:53.140
index into different mini batches so you
have X T Y T and to check your

00:04:53.140 --> 00:04:53.150
have X T Y T and to check your
 

00:04:53.150 --> 00:04:56.230
have X T Y T and to check your
understanding of these um or what's the

00:04:56.230 --> 00:04:56.240
understanding of these um or what's the
 

00:04:56.240 --> 00:05:02.800
understanding of these um or what's the
dimension right of XT and YT well X is

00:05:02.800 --> 00:05:02.810
dimension right of XT and YT well X is
 

00:05:02.810 --> 00:05:07.120
dimension right of XT and YT well X is
NX by M so if x1 is a thousand training

00:05:07.120 --> 00:05:07.130
NX by M so if x1 is a thousand training
 

00:05:07.130 --> 00:05:09.190
NX by M so if x1 is a thousand training
examples or the X values for a thousand

00:05:09.190 --> 00:05:09.200
examples or the X values for a thousand
 

00:05:09.200 --> 00:05:11.290
examples or the X values for a thousand
examples then this dimension should be

00:05:11.290 --> 00:05:11.300
examples then this dimension should be
 

00:05:11.300 --> 00:05:16.690
examples then this dimension should be
MX by 1,000 and x2 should also be an X

00:05:16.690 --> 00:05:16.700
MX by 1,000 and x2 should also be an X
 

00:05:16.700 --> 00:05:20.020
MX by 1,000 and x2 should also be an X
by 1000 and so on so all of these should

00:05:20.020 --> 00:05:20.030
by 1000 and so on so all of these should
 

00:05:20.030 --> 00:05:23.080
by 1000 and so on so all of these should
have to mention NX / 1000 and these

00:05:23.080 --> 00:05:23.090
have to mention NX / 1000 and these
 

00:05:23.090 --> 00:05:30.990
have to mention NX / 1000 and these
should have to mention 1 by 1000 right

00:05:30.990 --> 00:05:31.000
 

00:05:31.000 --> 00:05:32.490
2

00:05:32.490 --> 00:05:32.500
2
 

00:05:32.500 --> 00:05:35.580
2
the name of this algorithm - gradient

00:05:35.580 --> 00:05:35.590
the name of this algorithm - gradient
 

00:05:35.590 --> 00:05:38.190
the name of this algorithm - gradient
descent refers to the gradient descent

00:05:38.190 --> 00:05:38.200
descent refers to the gradient descent
 

00:05:38.200 --> 00:05:39.600
descent refers to the gradient descent
algorithm we've been talking about

00:05:39.600 --> 00:05:39.610
algorithm we've been talking about
 

00:05:39.610 --> 00:05:41.280
algorithm we've been talking about
previously where you process your entire

00:05:41.280 --> 00:05:41.290
previously where you process your entire
 

00:05:41.290 --> 00:05:43.200
previously where you process your entire
training set all at the same time and

00:05:43.200 --> 00:05:43.210
training set all at the same time and
 

00:05:43.210 --> 00:05:46.320
training set all at the same time and
the name comes from viewing that as

00:05:46.320 --> 00:05:46.330
the name comes from viewing that as
 

00:05:46.330 --> 00:05:48.780
the name comes from viewing that as
processing your entire batch of training

00:05:48.780 --> 00:05:48.790
processing your entire batch of training
 

00:05:48.790 --> 00:05:50.670
processing your entire batch of training
examples all at the same time I'm not

00:05:50.670 --> 00:05:50.680
examples all at the same time I'm not
 

00:05:50.680 --> 00:05:52.410
examples all at the same time I'm not
such a great name but that's just what

00:05:52.410 --> 00:05:52.420
such a great name but that's just what
 

00:05:52.420 --> 00:05:54.570
such a great name but that's just what
is called mini batch period descent in

00:05:54.570 --> 00:05:54.580
is called mini batch period descent in
 

00:05:54.580 --> 00:05:57.630
is called mini batch period descent in
contrast refers to the algorithm which

00:05:57.630 --> 00:05:57.640
contrast refers to the algorithm which
 

00:05:57.640 --> 00:05:59.220
contrast refers to the algorithm which
we'll talk about on the next slide and

00:05:59.220 --> 00:05:59.230
we'll talk about on the next slide and
 

00:05:59.230 --> 00:06:02.310
we'll talk about on the next slide and
which you process is single mini batch X

00:06:02.310 --> 00:06:02.320
which you process is single mini batch X
 

00:06:02.320 --> 00:06:04.890
which you process is single mini batch X
T YT at the same time rather than

00:06:04.890 --> 00:06:04.900
T YT at the same time rather than
 

00:06:04.900 --> 00:06:07.380
T YT at the same time rather than
processing your entire training set X Y

00:06:07.380 --> 00:06:07.390
processing your entire training set X Y
 

00:06:07.390 --> 00:06:10.140
processing your entire training set X Y
at the same time so let's see how many

00:06:10.140 --> 00:06:10.150
at the same time so let's see how many
 

00:06:10.150 --> 00:06:12.570
at the same time so let's see how many
batch gradient descent works to run

00:06:12.570 --> 00:06:12.580
batch gradient descent works to run
 

00:06:12.580 --> 00:06:14.340
batch gradient descent works to run
mini-batch gradient descent on your

00:06:14.340 --> 00:06:14.350
mini-batch gradient descent on your
 

00:06:14.350 --> 00:06:17.610
mini-batch gradient descent on your
training sets you would run for t equals

00:06:17.610 --> 00:06:17.620
training sets you would run for t equals
 

00:06:17.620 --> 00:06:22.050
training sets you would run for t equals
1 to 5000 because we had 5000 mini

00:06:22.050 --> 00:06:22.060
1 to 5000 because we had 5000 mini
 

00:06:22.060 --> 00:06:25.170
1 to 5000 because we had 5000 mini
batches of size 1,000 each and what

00:06:25.170 --> 00:06:25.180
batches of size 1,000 each and what
 

00:06:25.180 --> 00:06:26.550
batches of size 1,000 each and what
you're going to do inside the for loop

00:06:26.550 --> 00:06:26.560
you're going to do inside the for loop
 

00:06:26.560 --> 00:06:29.520
you're going to do inside the for loop
is basically implement one step of

00:06:29.520 --> 00:06:29.530
is basically implement one step of
 

00:06:29.530 --> 00:06:40.010
is basically implement one step of
gradient descent using X G comma Y T and

00:06:40.010 --> 00:06:40.020
gradient descent using X G comma Y T and
 

00:06:40.020 --> 00:06:43.290
gradient descent using X G comma Y T and
it's as if you had a training set of

00:06:43.290 --> 00:06:43.300
it's as if you had a training set of
 

00:06:43.300 --> 00:06:48.780
it's as if you had a training set of
size 1,000 examples and it was as if you

00:06:48.780 --> 00:06:48.790
size 1,000 examples and it was as if you
 

00:06:48.790 --> 00:06:51.030
size 1,000 examples and it was as if you
were to implement the algorithm you're

00:06:51.030 --> 00:06:51.040
were to implement the algorithm you're
 

00:06:51.040 --> 00:06:52.740
were to implement the algorithm you're
already familiar with but just on this

00:06:52.740 --> 00:06:52.750
already familiar with but just on this
 

00:06:52.750 --> 00:06:55.409
already familiar with but just on this
you know little training set size of M

00:06:55.409 --> 00:06:55.419
you know little training set size of M
 

00:06:55.419 --> 00:06:58.380
you know little training set size of M
equals 1000 rather than having explicit

00:06:58.380 --> 00:06:58.390
equals 1000 rather than having explicit
 

00:06:58.390 --> 00:07:01.080
equals 1000 rather than having explicit
for loop over all 1000 examples you

00:07:01.080 --> 00:07:01.090
for loop over all 1000 examples you
 

00:07:01.090 --> 00:07:03.180
for loop over all 1000 examples you
would use vectorization to process all

00:07:03.180 --> 00:07:03.190
would use vectorization to process all
 

00:07:03.190 --> 00:07:05.370
would use vectorization to process all
1,000 examples sort of all at the same

00:07:05.370 --> 00:07:05.380
1,000 examples sort of all at the same
 

00:07:05.380 --> 00:07:08.969
1,000 examples sort of all at the same
time so let's write this out first you

00:07:08.969 --> 00:07:08.979
time so let's write this out first you
 

00:07:08.979 --> 00:07:15.300
time so let's write this out first you
implement forward prop on the inputs so

00:07:15.300 --> 00:07:15.310
implement forward prop on the inputs so
 

00:07:15.310 --> 00:07:18.600
implement forward prop on the inputs so
just on XP and you do that by

00:07:18.600 --> 00:07:18.610
just on XP and you do that by
 

00:07:18.610 --> 00:07:24.390
just on XP and you do that by
implementing you know Z 1 equals W 1 now

00:07:24.390 --> 00:07:24.400
implementing you know Z 1 equals W 1 now
 

00:07:24.400 --> 00:07:27.420
implementing you know Z 1 equals W 1 now
previously we just have X there right

00:07:27.420 --> 00:07:27.430
previously we just have X there right
 

00:07:27.430 --> 00:07:29.400
previously we just have X there right
but now you're on process the entire

00:07:29.400 --> 00:07:29.410
but now you're on process the entire
 

00:07:29.410 --> 00:07:30.659
but now you're on process the entire
training set and you're just processing

00:07:30.659 --> 00:07:30.669
training set and you're just processing
 

00:07:30.669 --> 00:07:33.649
training set and you're just processing
the first mini batch so this becomes X

00:07:33.649 --> 00:07:33.659
the first mini batch so this becomes X
 

00:07:33.659 --> 00:07:37.399
the first mini batch so this becomes X
tea when you processing mini-batch tea

00:07:37.399 --> 00:07:37.409
tea when you processing mini-batch tea
 

00:07:37.409 --> 00:07:43.189
tea when you processing mini-batch tea
and then you would have a1 equals G 1 of

00:07:43.189 --> 00:07:43.199
and then you would have a1 equals G 1 of
 

00:07:43.199 --> 00:07:48.390
and then you would have a1 equals G 1 of
Z 1 District Capital Z since we're this

00:07:48.390 --> 00:07:48.400
Z 1 District Capital Z since we're this
 

00:07:48.400 --> 00:07:49.980
Z 1 District Capital Z since we're this
is actually a vectorized implementation

00:07:49.980 --> 00:07:49.990
is actually a vectorized implementation
 

00:07:49.990 --> 00:07:57.929
is actually a vectorized implementation
and so on until you end up with a l you

00:07:57.929 --> 00:07:57.939
and so on until you end up with a l you
 

00:07:57.939 --> 00:08:02.459
and so on until you end up with a l you
know as I guess GL of VL and then this

00:08:02.459 --> 00:08:02.469
know as I guess GL of VL and then this
 

00:08:02.469 --> 00:08:04.619
know as I guess GL of VL and then this
is your prediction and you notice that

00:08:04.619 --> 00:08:04.629
is your prediction and you notice that
 

00:08:04.629 --> 00:08:06.619
is your prediction and you notice that
here you should use a vectorized

00:08:06.619 --> 00:08:06.629
here you should use a vectorized
 

00:08:06.629 --> 00:08:11.339
here you should use a vectorized
implementation it's just that this

00:08:11.339 --> 00:08:11.349
implementation it's just that this
 

00:08:11.349 --> 00:08:13.860
implementation it's just that this
vectorized implementation processes

00:08:13.860 --> 00:08:13.870
vectorized implementation processes
 

00:08:13.870 --> 00:08:17.279
vectorized implementation processes
1,000 examples at a time rather than 5

00:08:17.279 --> 00:08:17.289
1,000 examples at a time rather than 5
 

00:08:17.289 --> 00:08:20.189
1,000 examples at a time rather than 5
million examples mixed you compute the

00:08:20.189 --> 00:08:20.199
million examples mixed you compute the
 

00:08:20.199 --> 00:08:25.679
million examples mixed you compute the
cost function J which I'm going to write

00:08:25.679 --> 00:08:25.689
cost function J which I'm going to write
 

00:08:25.689 --> 00:08:30.540
cost function J which I'm going to write
as 1 over 1000 since 301 thousands the

00:08:30.540 --> 00:08:30.550
as 1 over 1000 since 301 thousands the
 

00:08:30.550 --> 00:08:33.149
as 1 over 1000 since 301 thousands the
size of your little training set sum

00:08:33.149 --> 00:08:33.159
size of your little training set sum
 

00:08:33.159 --> 00:08:36.269
size of your little training set sum
from I equals 1 through L of really the

00:08:36.269 --> 00:08:36.279
from I equals 1 through L of really the
 

00:08:36.279 --> 00:08:43.829
from I equals 1 through L of really the
you know loss of Y hat I Y I and this

00:08:43.829 --> 00:08:43.839
you know loss of Y hat I Y I and this
 

00:08:43.839 --> 00:08:46.650
you know loss of Y hat I Y I and this
notation for clarity refers to examples

00:08:46.650 --> 00:08:46.660
notation for clarity refers to examples
 

00:08:46.660 --> 00:08:54.060
notation for clarity refers to examples
from the mini-batch XT YT and then if

00:08:54.060 --> 00:08:54.070
from the mini-batch XT YT and then if
 

00:08:54.070 --> 00:08:55.710
from the mini-batch XT YT and then if
you're using regularization you can also

00:08:55.710 --> 00:08:55.720
you're using regularization you can also
 

00:08:55.720 --> 00:08:59.880
you're using regularization you can also
have this regularization term just move

00:08:59.880 --> 00:08:59.890
have this regularization term just move
 

00:08:59.890 --> 00:09:05.240
have this regularization term just move
over to the denominator time sum over L

00:09:05.240 --> 00:09:05.250
 

00:09:05.250 --> 00:09:07.139
Frobenius norm the way measures a

00:09:07.139 --> 00:09:07.149
Frobenius norm the way measures a
 

00:09:07.149 --> 00:09:09.269
Frobenius norm the way measures a
squared so because this is really the

00:09:09.269 --> 00:09:09.279
squared so because this is really the
 

00:09:09.279 --> 00:09:13.259
squared so because this is really the
cost on just one rainy batch and then I

00:09:13.259 --> 00:09:13.269
cost on just one rainy batch and then I
 

00:09:13.269 --> 00:09:17.040
cost on just one rainy batch and then I
index this cost J with a superscript T

00:09:17.040 --> 00:09:17.050
index this cost J with a superscript T
 

00:09:17.050 --> 00:09:19.740
index this cost J with a superscript T
in curly braces so you notice that

00:09:19.740 --> 00:09:19.750
in curly braces so you notice that
 

00:09:19.750 --> 00:09:22.500
in curly braces so you notice that
everything we're doing is exactly the

00:09:22.500 --> 00:09:22.510
everything we're doing is exactly the
 

00:09:22.510 --> 00:09:24.630
everything we're doing is exactly the
same as when we were previously

00:09:24.630 --> 00:09:24.640
same as when we were previously
 

00:09:24.640 --> 00:09:26.759
same as when we were previously
implementing gradient descent except

00:09:26.759 --> 00:09:26.769
implementing gradient descent except
 

00:09:26.769 --> 00:09:29.610
implementing gradient descent except
that instead of doing it on X Y you're

00:09:29.610 --> 00:09:29.620
that instead of doing it on X Y you're
 

00:09:29.620 --> 00:09:32.759
that instead of doing it on X Y you're
not doing it on X T YT next you'd

00:09:32.759 --> 00:09:32.769
not doing it on X T YT next you'd
 

00:09:32.769 --> 00:09:38.020
not doing it on X T YT next you'd
implement back prop to compute

00:09:38.020 --> 00:09:38.030
implement back prop to compute
 

00:09:38.030 --> 00:09:42.830
implement back prop to compute
gradients with respect to really respect

00:09:42.830 --> 00:09:42.840
gradients with respect to really respect
 

00:09:42.840 --> 00:09:46.880
gradients with respect to really respect
to this JT so you're still using only X

00:09:46.880 --> 00:09:46.890
to this JT so you're still using only X
 

00:09:46.890 --> 00:09:54.170
to this JT so you're still using only X
T YT and then you update the weights you

00:09:54.170 --> 00:09:54.180
T YT and then you update the weights you
 

00:09:54.180 --> 00:09:59.650
T YT and then you update the weights you
know wre every WL gets updated as WL

00:09:59.650 --> 00:09:59.660
know wre every WL gets updated as WL
 

00:09:59.660 --> 00:10:11.620
know wre every WL gets updated as WL
minus alpha D WL and similarly for B and

00:10:11.620 --> 00:10:11.630
minus alpha D WL and similarly for B and
 

00:10:11.630 --> 00:10:15.290
minus alpha D WL and similarly for B and
so this is one pass through your

00:10:15.290 --> 00:10:15.300
so this is one pass through your
 

00:10:15.300 --> 00:10:17.840
so this is one pass through your
training set using mini-batch gradient

00:10:17.840 --> 00:10:17.850
training set using mini-batch gradient
 

00:10:17.850 --> 00:10:20.360
training set using mini-batch gradient
descent the code i've written down here

00:10:20.360 --> 00:10:20.370
descent the code i've written down here
 

00:10:20.370 --> 00:10:23.770
descent the code i've written down here
is also called doing one epoch of

00:10:23.770 --> 00:10:23.780
is also called doing one epoch of
 

00:10:23.780 --> 00:10:27.650
is also called doing one epoch of
training and epoch is a word that just

00:10:27.650 --> 00:10:27.660
training and epoch is a word that just
 

00:10:27.660 --> 00:10:31.430
training and epoch is a word that just
means a single pass through the training

00:10:31.430 --> 00:10:31.440
means a single pass through the training
 

00:10:31.440 --> 00:10:38.570
means a single pass through the training
set so whereas with batch gradient

00:10:38.570 --> 00:10:38.580
set so whereas with batch gradient
 

00:10:38.580 --> 00:10:41.030
set so whereas with batch gradient
descent a single pass through the

00:10:41.030 --> 00:10:41.040
descent a single pass through the
 

00:10:41.040 --> 00:10:43.040
descent a single pass through the
training set allows you to take only one

00:10:43.040 --> 00:10:43.050
training set allows you to take only one
 

00:10:43.050 --> 00:10:45.320
training set allows you to take only one
gradient descent step with really batch

00:10:45.320 --> 00:10:45.330
gradient descent step with really batch
 

00:10:45.330 --> 00:10:47.360
gradient descent step with really batch
gradient descent a single pass through

00:10:47.360 --> 00:10:47.370
gradient descent a single pass through
 

00:10:47.370 --> 00:10:49.100
gradient descent a single pass through
the training set that is one epoch

00:10:49.100 --> 00:10:49.110
the training set that is one epoch
 

00:10:49.110 --> 00:10:52.160
the training set that is one epoch
allows you to take 5000 gradient descent

00:10:52.160 --> 00:10:52.170
allows you to take 5000 gradient descent
 

00:10:52.170 --> 00:10:54.950
allows you to take 5000 gradient descent
steps now of course you want to take

00:10:54.950 --> 00:10:54.960
steps now of course you want to take
 

00:10:54.960 --> 00:10:56.540
steps now of course you want to take
multiple passes through the training

00:10:56.540 --> 00:10:56.550
multiple passes through the training
 

00:10:56.550 --> 00:10:58.850
multiple passes through the training
sets which you usually want to you might

00:10:58.850 --> 00:10:58.860
sets which you usually want to you might
 

00:10:58.860 --> 00:11:01.220
sets which you usually want to you might
want another for loop or another your

00:11:01.220 --> 00:11:01.230
want another for loop or another your
 

00:11:01.230 --> 00:11:03.440
want another for loop or another your
while loop out there so you keep taking

00:11:03.440 --> 00:11:03.450
while loop out there so you keep taking
 

00:11:03.450 --> 00:11:05.540
while loop out there so you keep taking
process through the training set until

00:11:05.540 --> 00:11:05.550
process through the training set until
 

00:11:05.550 --> 00:11:07.520
process through the training set until
hopefully you converge or it

00:11:07.520 --> 00:11:07.530
hopefully you converge or it
 

00:11:07.530 --> 00:11:09.470
hopefully you converge or it
approximately converged when you have a

00:11:09.470 --> 00:11:09.480
approximately converged when you have a
 

00:11:09.480 --> 00:11:11.480
approximately converged when you have a
lost training set meaning batch gradient

00:11:11.480 --> 00:11:11.490
lost training set meaning batch gradient
 

00:11:11.490 --> 00:11:14.000
lost training set meaning batch gradient
descent runs much faster than batch

00:11:14.000 --> 00:11:14.010
descent runs much faster than batch
 

00:11:14.010 --> 00:11:15.950
descent runs much faster than batch
gradient descent and it's pretty much

00:11:15.950 --> 00:11:15.960
gradient descent and it's pretty much
 

00:11:15.960 --> 00:11:17.900
gradient descent and it's pretty much
what everyone in deep learning will use

00:11:17.900 --> 00:11:17.910
what everyone in deep learning will use
 

00:11:17.910 --> 00:11:19.700
what everyone in deep learning will use
when you're training on a large dataset

00:11:19.700 --> 00:11:19.710
when you're training on a large dataset
 

00:11:19.710 --> 00:11:22.100
when you're training on a large dataset
in the next video let's delve deeper

00:11:22.100 --> 00:11:22.110
in the next video let's delve deeper
 

00:11:22.110 --> 00:11:24.440
in the next video let's delve deeper
into mini batch goodness and so you can

00:11:24.440 --> 00:11:24.450
into mini batch goodness and so you can
 

00:11:24.450 --> 00:11:26.360
into mini batch goodness and so you can
get a better understanding of what is

00:11:26.360 --> 00:11:26.370
get a better understanding of what is
 

00:11:26.370 --> 00:11:30.050
get a better understanding of what is
doing and why it works so well

