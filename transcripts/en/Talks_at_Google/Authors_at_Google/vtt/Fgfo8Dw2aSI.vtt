WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.458
[MUSIC PLAYING]

00:00:06.364 --> 00:00:07.780
CRAIG: Welcome to
Talks at Google.

00:00:07.780 --> 00:00:11.380
I'm very pleased
to offer discussion

00:00:11.380 --> 00:00:14.230
on the book "Meltdown."

00:00:14.230 --> 00:00:16.840
Has any complex system that
you work with ever failed?

00:00:20.080 --> 00:00:25.900
This discussion today by Chris
Clearfield and Andras Tilcsik

00:00:25.900 --> 00:00:29.960
will tell you a little bit about
why these complex systems fail.

00:00:29.960 --> 00:00:32.710
And they're going to offer some
suggestions on how to fix it.

00:00:32.710 --> 00:00:34.720
I'd like to welcome our authors.

00:00:34.720 --> 00:00:37.770
Chris is a former
derivatives trader,

00:00:37.770 --> 00:00:39.520
licensed commercial
pilot, and he tells me

00:00:39.520 --> 00:00:42.580
he's also a science geek.

00:00:42.580 --> 00:00:47.525
And Andras has the
Canada Research Chair

00:00:47.525 --> 00:00:50.530
in Strategy,
Organizations, and Society

00:00:50.530 --> 00:00:54.070
at the University of Toronto's
Rotman School of Management.

00:00:54.070 --> 00:00:55.370
They've written a great book.

00:00:55.370 --> 00:00:57.080
I've read it, and
I hope you do, too.

00:00:57.080 --> 00:00:59.794
Welcome, Chris and Andras.

00:00:59.794 --> 00:01:03.070
[APPLAUSE]

00:01:03.070 --> 00:01:05.680
CHRIS CLEARFIELD:
Thanks very much, Craig.

00:01:05.680 --> 00:01:08.992
So before we dive
into the book, we

00:01:08.992 --> 00:01:11.200
like to talk a little bit
about how we got interested

00:01:11.200 --> 00:01:12.290
in this topic.

00:01:12.290 --> 00:01:16.270
But also, wanted
to thank you all.

00:01:16.270 --> 00:01:18.610
Although, probably not
everyone here works

00:01:18.610 --> 00:01:20.200
in the Google Apps suite.

00:01:20.200 --> 00:01:23.200
This book would not be written
if it were not for Google Docs.

00:01:23.200 --> 00:01:26.110
So you can pass that
on to your colleagues.

00:01:26.110 --> 00:01:27.650
Andras is in Toronto.

00:01:27.650 --> 00:01:29.020
I'm in Seattle.

00:01:29.020 --> 00:01:31.720
And we had many, many a Google
Hangout with the Google Doc

00:01:31.720 --> 00:01:33.660
open where we were
kind of pounding away.

00:01:33.660 --> 00:01:39.220
So thank you for that because
we wouldn't be here otherwise.

00:01:39.220 --> 00:01:42.220
So as Craig said, this
book is about failure.

00:01:42.220 --> 00:01:45.600
And it's about how failure
happens in complex systems.

00:01:45.600 --> 00:01:47.440
And we just wanted
to talk a little bit

00:01:47.440 --> 00:01:50.590
about how we got onto
this topic and how

00:01:50.590 --> 00:01:52.720
we started thinking about it.

00:01:52.720 --> 00:01:54.230
And then, go through--

00:01:54.230 --> 00:01:56.170
kind of just skim
the surface of what

00:01:56.170 --> 00:01:57.670
a little bit of
our research is, so

00:01:57.670 --> 00:01:59.378
that we can have a
conversation about it.

00:01:59.378 --> 00:02:01.940
Because I think you
guys more than many,

00:02:01.940 --> 00:02:05.560
many organizations really think
carefully about this stuff

00:02:05.560 --> 00:02:08.350
and are already working
with complex systems.

00:02:08.350 --> 00:02:11.140
And many times, working with
them in a really effective way.

00:02:11.140 --> 00:02:13.570
So our hope is that this is--

00:02:13.570 --> 00:02:15.400
what we're contributing
is a perspective.

00:02:15.400 --> 00:02:18.430
And that we can use that
to start a conversation.

00:02:18.430 --> 00:02:21.010
So Andras and I got
into this research kind

00:02:21.010 --> 00:02:22.120
of from different areas.

00:02:22.120 --> 00:02:23.890
As Craig said, I was
a derivatives trader.

00:02:23.890 --> 00:02:26.410
I traded during the
financial crisis.

00:02:26.410 --> 00:02:29.500
And so I saw different
organizations

00:02:29.500 --> 00:02:32.140
that managed that
process very well

00:02:32.140 --> 00:02:34.960
and some that managed it very
poorly and went bankrupt.

00:02:34.960 --> 00:02:37.440
And a lot of people saw
the impacts of that.

00:02:37.440 --> 00:02:41.380
And so at the same time,
I was learning how to fly.

00:02:41.380 --> 00:02:44.290
I was starting the process
of becoming a pilot,

00:02:44.290 --> 00:02:46.210
and really interested
in the lessons

00:02:46.210 --> 00:02:48.190
that aviation kind
of had learned

00:02:48.190 --> 00:02:51.280
about how to manage these
complex and interconnected

00:02:51.280 --> 00:02:52.300
systems.

00:02:52.300 --> 00:02:56.710
And Andras in the meantime, was
finishing his PhD in sociology,

00:02:56.710 --> 00:02:59.430
and looking at how
organizations made decisions.

00:02:59.430 --> 00:03:03.882
And it was really after the
BP Deepwater Horizon oil spill

00:03:03.882 --> 00:03:05.590
that we kind of came
together and started

00:03:05.590 --> 00:03:08.350
thinking about this a little
bit more systematically.

00:03:08.350 --> 00:03:11.080
And just started to try
to identify kind of two--

00:03:11.080 --> 00:03:12.700
answer two big questions.

00:03:12.700 --> 00:03:15.190
Which is 1, why are
these kind of failures

00:03:15.190 --> 00:03:16.760
happening more and more?

00:03:16.760 --> 00:03:21.340
And 2, what are capable
organizations doing about it?

00:03:21.340 --> 00:03:23.930
And kind of, what can
we learn from that?

00:03:23.930 --> 00:03:27.424
So when we're talking
about big failures,

00:03:27.424 --> 00:03:28.840
today we're going
to start in what

00:03:28.840 --> 00:03:33.280
seems like an unlikely place,
which is the festive holiday

00:03:33.280 --> 00:03:34.915
season.

00:03:34.915 --> 00:03:37.540
So this example, it's one of our
favorite examples in the book.

00:03:37.540 --> 00:03:42.040
It's a social media campaign
that Starbucks was running.

00:03:42.040 --> 00:03:45.460
They had this hashtag,
#spreadthecheer, right?

00:03:45.460 --> 00:03:47.500
And I mean, it seems
so warm and fuzzy.

00:03:47.500 --> 00:03:49.720
They wanted tweets
kind of like this,

00:03:49.720 --> 00:03:54.200
"I love Starbucks gingerbread
lattes #spreadthecheer."

00:03:54.200 --> 00:03:55.630
And as part of
this campaign, they

00:03:55.630 --> 00:03:59.200
had sponsored a big projection
screen at an ice skating

00:03:59.200 --> 00:04:02.090
rink in a museum in London.

00:04:02.090 --> 00:04:04.850
And so the idea was they would
have this #spreadthecheer.

00:04:04.850 --> 00:04:07.090
And then, those tweets would
come up on this screen.

00:04:07.090 --> 00:04:09.375
Yeah, you guys see
where this is going.

00:04:09.375 --> 00:04:10.840
People would enjoy it.

00:04:10.840 --> 00:04:13.330
And listen, Starbucks is a
sophisticated company, right?

00:04:13.330 --> 00:04:17.320
I mean, they think very, very
much about their marketing.

00:04:17.320 --> 00:04:21.240
And they had a content
filter for these tweets,

00:04:21.240 --> 00:04:23.840
but the content filter broke.

00:04:23.840 --> 00:04:26.590
And so early on, you started
getting stuff like this,

00:04:26.590 --> 00:04:29.560
"I like buying coffee that
tastes nice from a shop that

00:04:29.560 --> 00:04:30.880
pays its tax.

00:04:30.880 --> 00:04:35.750
So I avoid @starbucks
#spreadthecheer."

00:04:35.750 --> 00:04:38.770
And Starbucks was enmeshed in
this kind of tax controversy

00:04:38.770 --> 00:04:40.840
in the UK at the time.

00:04:40.840 --> 00:04:41.480
Sorry.

00:04:41.480 --> 00:04:46.636
Then, you saw, #spreadthecheer,
pay your tax bills, parasites."

00:04:46.636 --> 00:04:48.010
And then, a couple
of people that

00:04:48.010 --> 00:04:52.030
were a little less
subtle in their critique.

00:04:52.030 --> 00:04:55.150
And what happened
was that people

00:04:55.150 --> 00:04:58.070
started tweeting about the
fact that this was happening.

00:04:58.070 --> 00:05:00.550
And then, we saw this
kind of snowball, right?

00:05:00.550 --> 00:05:03.610
So we saw this avalanche effect
that we see in social media

00:05:03.610 --> 00:05:06.880
where tweet after tweet after
tweet started happening.

00:05:06.880 --> 00:05:10.300
And what's interesting is even
after they fixed the moderation

00:05:10.300 --> 00:05:13.070
filter, I mean, the cat
was out of the bag, right?

00:05:13.070 --> 00:05:14.671
The problem was
already happening.

00:05:14.671 --> 00:05:16.420
And you know, Twitter
was abuzz with this.

00:05:16.420 --> 00:05:18.086
It also jumped to the
traditional media.

00:05:18.086 --> 00:05:19.030
Papers covered it.

00:05:19.030 --> 00:05:21.430
So it was a PR meltdown.

00:05:21.430 --> 00:05:24.480
And nobody died in it,
but it's a really kind

00:05:24.480 --> 00:05:26.320
of interesting
place for our story

00:05:26.320 --> 00:05:31.180
to start because this kind of
meltdown is not unique anymore.

00:05:31.180 --> 00:05:34.060
We have the story of Knight
Capital, a pretty well-known,

00:05:34.060 --> 00:05:35.770
at least in Wall
Street circles, trader,

00:05:35.770 --> 00:05:38.820
who because of a-- basically,
because of a DevOps error,

00:05:38.820 --> 00:05:41.380
they accidentally didn't
roll out a piece of software

00:05:41.380 --> 00:05:43.390
on one of their eight servers.

00:05:43.390 --> 00:05:47.260
They lost $500 million
in half an hour.

00:05:47.260 --> 00:05:52.001
And that's a lot,
even for Wall Street.

00:05:52.001 --> 00:05:53.500
A lot of us have
had this experience

00:05:53.500 --> 00:05:56.707
where we've seen or read about
or even been in a situation

00:05:56.707 --> 00:05:57.790
where flights are delayed.

00:05:57.790 --> 00:06:00.610
Not because there's a problem
with the planes or the pilots,

00:06:00.610 --> 00:06:03.026
but because there's a problem
with the airline reservation

00:06:03.026 --> 00:06:03.602
system.

00:06:03.602 --> 00:06:05.560
There's been a fire in
a data center somewhere.

00:06:05.560 --> 00:06:07.510
Or in this case,
somebody accidentally

00:06:07.510 --> 00:06:10.780
pulled the power cord out at
BA, and then the whole network

00:06:10.780 --> 00:06:12.330
goes down.

00:06:12.330 --> 00:06:13.510
I'm from Seattle.

00:06:13.510 --> 00:06:15.910
In Washington state, there
was this amazing example

00:06:15.910 --> 00:06:18.730
where in the state's
prisoner management system,

00:06:18.730 --> 00:06:21.340
the Department of Corrections
was releasing thousands

00:06:21.340 --> 00:06:24.310
of felons early for
a decade because they

00:06:24.310 --> 00:06:27.520
had a bug in their code
that miscalculated sentences

00:06:27.520 --> 00:06:29.016
and they didn't know about it.

00:06:29.016 --> 00:06:30.640
And even after they
were told about it,

00:06:30.640 --> 00:06:34.450
it took them three
years to fix it.

00:06:34.450 --> 00:06:38.740
We all saw this happen in
Hawaii, where a pretty bad user

00:06:38.740 --> 00:06:42.820
interface meant that this alert
that a ballistic missile was

00:06:42.820 --> 00:06:43.960
inbound happened.

00:06:43.960 --> 00:06:46.680
And then beyond that, they
hadn't designed for failure,

00:06:46.680 --> 00:06:47.180
right?

00:06:47.180 --> 00:06:48.920
So they couldn't pull
back the message.

00:06:48.920 --> 00:06:51.654
They didn't have the
infrastructure to do that.

00:06:51.654 --> 00:06:54.070
And so what all these things
have in common on the surface

00:06:54.070 --> 00:06:58.180
is that they are all examples
of small problems snowballing

00:06:58.180 --> 00:07:01.630
into these really big failures.

00:07:01.630 --> 00:07:03.550
And that's what we see
over and over again.

00:07:03.550 --> 00:07:05.050
And that's kind of
one of the things

00:07:05.050 --> 00:07:08.230
that unifies these
sorts of failures

00:07:08.230 --> 00:07:09.730
that we looked at in the book.

00:07:09.730 --> 00:07:13.570
And when we dug past the
headlines, what we found

00:07:13.570 --> 00:07:16.480
was that there's actually some
pretty compelling research that

00:07:16.480 --> 00:07:19.780
describes kind of not
the precise failures that

00:07:19.780 --> 00:07:20.840
are going to happen.

00:07:20.840 --> 00:07:22.240
But in general,
the dynamics that

00:07:22.240 --> 00:07:23.770
lead to these kinds of failures.

00:07:23.770 --> 00:07:25.186
So these kind of
failures are more

00:07:25.186 --> 00:07:27.580
likely to happen in
systems that are complex

00:07:27.580 --> 00:07:28.960
and tightly coupled.

00:07:28.960 --> 00:07:31.430
And I'm going to talk a little
bit about what those mean.

00:07:31.430 --> 00:07:33.790
But when a system is
complex and tightly coupled,

00:07:33.790 --> 00:07:35.870
it's in what we think
of as the danger zone.

00:07:35.870 --> 00:07:37.619
It's in this area where
these failures are

00:07:37.619 --> 00:07:38.840
more likely to happen.

00:07:38.840 --> 00:07:42.040
And when they happen, they
are more likely to spiral out

00:07:42.040 --> 00:07:43.100
of control.

00:07:43.100 --> 00:07:45.490
So from our definition
of complexity,

00:07:45.490 --> 00:07:47.870
complexity means that
the system is connected.

00:07:47.870 --> 00:07:49.120
There's a lot of connectivity.

00:07:49.120 --> 00:07:50.700
It looks a lot like a web.

00:07:50.700 --> 00:07:53.050
And it's hard to understand
what's going on inside it.

00:07:53.050 --> 00:07:56.819
So if you think about that could
be because of computer code

00:07:56.819 --> 00:07:58.360
or it could be
because it's something

00:07:58.360 --> 00:08:02.410
like Deepwater Horizon, where
the action, if you will,

00:08:02.410 --> 00:08:05.860
is happening miles under the
surface of the ocean miles

00:08:05.860 --> 00:08:08.560
under the surface of the earth.

00:08:08.560 --> 00:08:13.289
And tight coupling is this idea
that once the failure starts

00:08:13.289 --> 00:08:14.830
to happen, there's
not a lot of slack

00:08:14.830 --> 00:08:17.560
in our system to correct it.

00:08:17.560 --> 00:08:20.290
So it moves faster
than we can move.

00:08:20.290 --> 00:08:22.540
And we can't kind of put the
genie back in the bottle.

00:08:22.540 --> 00:08:25.900
Once the error starts happening,
it's hard to recover from.

00:08:25.900 --> 00:08:28.750
And when we have a system that's
complex and tightly coupled,

00:08:28.750 --> 00:08:31.630
we tend to be in
the danger zone.

00:08:31.630 --> 00:08:34.809
Now, this is research that
some of you might recognize.

00:08:34.809 --> 00:08:38.350
It was done by a guy called
[? Chick Perot ?] in the 1980s.

00:08:38.350 --> 00:08:40.210
And [? Chick Perot's ?]
research really

00:08:40.210 --> 00:08:43.690
stemmed out of the Three
Mile Island nuclear meltdown.

00:08:43.690 --> 00:08:46.304
And what he saw when he
did this research initially

00:08:46.304 --> 00:08:48.220
is that there really
weren't that many systems

00:08:48.220 --> 00:08:49.270
in this danger zone.

00:08:49.270 --> 00:08:50.800
It was kind of
like nuclear power

00:08:50.800 --> 00:08:54.650
and like big kind of
space/aerospace systems.

00:08:54.650 --> 00:08:57.190
But what we have seen in
the research for our book

00:08:57.190 --> 00:09:00.970
is that so many more systems
are in this danger zone today.

00:09:00.970 --> 00:09:02.470
Whether we're talking
about finance,

00:09:02.470 --> 00:09:05.200
or health care, or
transportation, or even

00:09:05.200 --> 00:09:07.619
the kind of gadgets that
we bring into our lives.

00:09:07.619 --> 00:09:09.160
Whether we're talking
about our homes

00:09:09.160 --> 00:09:12.320
or internet-connected cars.

00:09:12.320 --> 00:09:14.320
And so that's kind of the setup.

00:09:14.320 --> 00:09:15.940
And that's sort of the bad news.

00:09:15.940 --> 00:09:18.520
But the book is actually
deeply optimistic.

00:09:18.520 --> 00:09:20.650
And we're going to have
time to talk about a couple

00:09:20.650 --> 00:09:21.740
of solutions today.

00:09:21.740 --> 00:09:24.630
And then, kick off
a conversation.

00:09:24.630 --> 00:09:27.360
So Andras is going to start
with our first solution.

00:09:27.360 --> 00:09:28.860
ANDRAS TILCSIK:
Yeah, thanks, Chris.

00:09:28.860 --> 00:09:30.550
And as Chris said,
we don't have time

00:09:30.550 --> 00:09:33.430
to go into all the solutions
we discussed in this book,

00:09:33.430 --> 00:09:36.130
but we do have time to
give you a few tidbits.

00:09:36.130 --> 00:09:39.040
So let me start with this thing.

00:09:39.040 --> 00:09:43.180
These sentences were written
on a blog by a [INAUDIBLE]

00:09:43.180 --> 00:09:46.760
gentleman about five
or six years ago.

00:09:46.760 --> 00:09:49.960
He says, "She's unbelievably
beautiful to look at.

00:09:49.960 --> 00:09:51.430
She stands extremely tall.

00:09:51.430 --> 00:09:52.770
She's the most beautiful."

00:09:52.770 --> 00:09:54.520
You look at these, you
think he's probably

00:09:54.520 --> 00:09:56.980
writing about a
supermodel who is also

00:09:56.980 --> 00:09:59.720
very tall and very pretty.

00:09:59.720 --> 00:10:02.860
But it turns out he's actually
writing about an airplane.

00:10:02.860 --> 00:10:08.510
And he's a KLM pilot, and he
really loves the Airbus 330.

00:10:08.510 --> 00:10:10.380
If you read the whole
blog post, there

00:10:10.380 --> 00:10:12.440
are like these 10
points in there.

00:10:12.440 --> 00:10:15.450
It almost reads like a
love letter to the plane.

00:10:15.450 --> 00:10:17.630
And what he loves
about it the most

00:10:17.630 --> 00:10:22.010
is that it has this very sleek,
elegant, streamlined design,

00:10:22.010 --> 00:10:27.710
both on the outside and on
the inside in the cockpit.

00:10:27.710 --> 00:10:29.960
If you look at the
cockpit design here,

00:10:29.960 --> 00:10:32.300
one of the things that
this pilot loved the most

00:10:32.300 --> 00:10:34.880
is these small, little
side-stick controllers.

00:10:34.880 --> 00:10:37.760
They are like little
joysticks that the pilots

00:10:37.760 --> 00:10:40.010
used to control the plane.

00:10:40.010 --> 00:10:44.351
And he loves them because
they are small, elegant.

00:10:44.351 --> 00:10:45.350
They are out of the way.

00:10:45.350 --> 00:10:48.140
They even leave space
for a tray table.

00:10:48.140 --> 00:10:50.630
It's a French plane, so
lunch is very important.

00:10:50.630 --> 00:10:52.804
The pilots get to pull it out.

00:10:52.804 --> 00:10:53.720
They have their lunch.

00:10:53.720 --> 00:10:55.340
It's very neat.

00:10:55.340 --> 00:10:58.150
It doesn't obscure
the instrument panel.

00:10:58.150 --> 00:10:59.060
It's set aside.

00:10:59.060 --> 00:11:01.760
And they are also both
fully computerized.

00:11:01.760 --> 00:11:04.230
So it looks like great design.

00:11:04.230 --> 00:11:09.740
And if you compare that to
the design of a Boeing 737.

00:11:09.740 --> 00:11:11.540
Similar plane in
many other ways,

00:11:11.540 --> 00:11:14.150
but the setup of the
controls is very different.

00:11:14.150 --> 00:11:17.390
Here, instead of the
elegant, little controls,

00:11:17.390 --> 00:11:20.730
we have these big
W-shaped control yokes.

00:11:20.730 --> 00:11:24.560
They stand on these
control columns

00:11:24.560 --> 00:11:26.720
that are about 3 feet tall.

00:11:26.720 --> 00:11:31.610
In fact, these things are so
big and bulky that the seats

00:11:31.610 --> 00:11:33.860
of the pilots need to
be cut out in the front.

00:11:33.860 --> 00:11:37.460
They are sort of split over
here to make space for them.

00:11:37.460 --> 00:11:39.560
And these things are
not fully computerized.

00:11:39.560 --> 00:11:42.470
In fact, the two controls
are mechanically physically

00:11:42.470 --> 00:11:43.380
connected.

00:11:43.380 --> 00:11:45.470
So if I pull mine
back, it's also

00:11:45.470 --> 00:11:47.450
going to go back
on Chris's side.

00:11:47.450 --> 00:11:50.780
And in fact, if I pull
back too hard and too fast,

00:11:50.780 --> 00:11:54.530
it probably hits him in the knee
and makes him spill his lunch

00:11:54.530 --> 00:11:56.630
on his shirt, which
is actually something

00:11:56.630 --> 00:11:58.920
that pilots complain
about all the time.

00:11:58.920 --> 00:12:01.430
So if you look at
this, it's big.

00:12:01.430 --> 00:12:02.270
It's bulky.

00:12:02.270 --> 00:12:04.520
It's oversized.

00:12:04.520 --> 00:12:07.550
And it makes you spill
your lunch on your shirt.

00:12:07.550 --> 00:12:10.250
It looks like terrible design.

00:12:10.250 --> 00:12:13.820
But it turns out that there's
something really helpful

00:12:13.820 --> 00:12:17.882
and something really beautiful
about these ugly designs.

00:12:17.882 --> 00:12:19.940
Or at least this
particular ugly design.

00:12:19.940 --> 00:12:23.420
And it's that it makes
everything the other pilot does

00:12:23.420 --> 00:12:24.950
immediately visible.

00:12:24.950 --> 00:12:26.700
It's literally in your face.

00:12:26.700 --> 00:12:30.470
So if I'm pulling back
or pushing forward,

00:12:30.470 --> 00:12:33.080
it's very easy for Chris
to see what's happening

00:12:33.080 --> 00:12:34.640
and what I'm doing.

00:12:34.640 --> 00:12:39.780
And in fact, with the
side-stick controls,

00:12:39.780 --> 00:12:42.020
we've already seen a
number of accidents--

00:12:42.020 --> 00:12:44.210
two or three in just
the past few years--

00:12:44.210 --> 00:12:48.410
where one of the pilots on
one of those Airbus planes got

00:12:48.410 --> 00:12:50.060
confused in the
heat of the moment.

00:12:50.060 --> 00:12:51.710
They were trying
to manage a crisis.

00:12:51.710 --> 00:12:56.300
Instead of pushing
forward to avoid a stall,

00:12:56.300 --> 00:12:58.880
they were pulling
back on the control.

00:12:58.880 --> 00:13:01.310
And the other pilot, the
more experienced pilot,

00:13:01.310 --> 00:13:05.370
could have caught that error
and could have intervened,

00:13:05.370 --> 00:13:08.760
but they just didn't see it
because the side-stick control

00:13:08.760 --> 00:13:09.920
is to the side.

00:13:09.920 --> 00:13:12.950
And that would never
happen with these giant,

00:13:12.950 --> 00:13:17.570
ugly, bulky controls, which
are literally in your face

00:13:17.570 --> 00:13:21.980
and probably hitting you
in the knee and the belly.

00:13:21.980 --> 00:13:26.210
And we actually see
this sort of principle

00:13:26.210 --> 00:13:29.120
over and over again in
our research for the book.

00:13:29.120 --> 00:13:31.730
This idea that we
often privilege

00:13:31.730 --> 00:13:35.870
elegance and love the
sleek and shiny designs.

00:13:35.870 --> 00:13:39.240
But often, those designs also
make things less transparent.

00:13:39.240 --> 00:13:41.360
And sometimes, that
kind of transparency

00:13:41.360 --> 00:13:42.830
is really valuable.

00:13:42.830 --> 00:13:45.014
And what we found is--

00:13:45.014 --> 00:13:46.430
what's very
interesting is that we

00:13:46.430 --> 00:13:49.820
see this principle not just in
the design of physical systems.

00:13:49.820 --> 00:13:52.970
We see it with airplanes
and car design.

00:13:52.970 --> 00:13:57.470
But we see it even in the design
of financial or accounting

00:13:57.470 --> 00:13:58.250
systems.

00:13:58.250 --> 00:14:01.570
Or to give you one more example
that's not technological.

00:14:01.570 --> 00:14:04.210
People might remember
this from last year,

00:14:04.210 --> 00:14:10.100
the Oscars mix up in 2017 when
the wrong film was announced.

00:14:10.100 --> 00:14:11.959
CHRIS CLEARFIELD: "La La Land."

00:14:11.959 --> 00:14:14.000
ANDRAS TILCSIK: It's not
"La La Land," I'm sorry.

00:14:14.000 --> 00:14:14.570
CHRIS CLEARFIELD: "Moonlight."

00:14:14.570 --> 00:14:16.445
ANDRAS TILCSIK: It's
"Moonlight," I am sorry.

00:14:16.445 --> 00:14:17.640
Yes.

00:14:17.640 --> 00:14:21.974
So part of that confusion
came from the design

00:14:21.974 --> 00:14:22.640
of the envelope.

00:14:22.640 --> 00:14:23.514
I mean, look at this.

00:14:23.514 --> 00:14:24.930
It's pretty elegant.

00:14:24.930 --> 00:14:27.304
It's this artful
red, gold lettering

00:14:27.304 --> 00:14:28.970
that's supposed to
be the category name,

00:14:28.970 --> 00:14:31.130
but we can't really read it.

00:14:31.130 --> 00:14:33.590
And especially if you
are standing backstage,

00:14:33.590 --> 00:14:34.790
it's really hard.

00:14:34.790 --> 00:14:36.330
And if I grab the
wrong envelope,

00:14:36.330 --> 00:14:40.610
it's very hard for anyone to see
that a mistake is being made.

00:14:40.610 --> 00:14:44.060
And now, take a look at
what they did this year.

00:14:44.060 --> 00:14:44.900
They learned, right?

00:14:44.900 --> 00:14:47.840
I mean, this is not
a pretty design.

00:14:47.840 --> 00:14:49.929
I mean, I am pretty
terrible at these things,

00:14:49.929 --> 00:14:51.470
but this is the kind
of thing I would

00:14:51.470 --> 00:14:52.910
design if they told me to.

00:14:52.910 --> 00:14:55.290
It says "Best Picture"
with huge font.

00:14:55.290 --> 00:14:58.130
And it says "Best Picture"
again in small font.

00:14:58.130 --> 00:15:00.470
And it's not pretty.

00:15:00.470 --> 00:15:02.170
It's ugly, but it's
very transparent.

00:15:02.170 --> 00:15:03.770
You make it so
transparent that when

00:15:03.770 --> 00:15:06.320
you're sitting at
home watching on TV,

00:15:06.320 --> 00:15:08.710
you can probably tell
if there is a mistake.

00:15:08.710 --> 00:15:10.660
And that's really the
same kind of principle

00:15:10.660 --> 00:15:13.630
we see with the airplanes, where
the other pilot, the monitoring

00:15:13.630 --> 00:15:16.710
pilot, can intervene.

00:15:16.710 --> 00:15:19.030
And with that, I'll
turn it over to Chris,

00:15:19.030 --> 00:15:21.970
who I think will talk
about another way

00:15:21.970 --> 00:15:23.922
to get transparency.

00:15:23.922 --> 00:15:24.880
CHRIS CLEARFIELD: Yeah.

00:15:24.880 --> 00:15:26.463
So I'm just talking
about transparency

00:15:26.463 --> 00:15:29.260
through thinking about
how we design our systems.

00:15:29.260 --> 00:15:31.300
But another big way
to get transparency

00:15:31.300 --> 00:15:34.840
is by learning
about our systems.

00:15:34.840 --> 00:15:37.630
And that's because in
particular, complex systems--

00:15:37.630 --> 00:15:39.940
we can't sort of sit
down ahead of time

00:15:39.940 --> 00:15:42.577
and write down all of the
ways that our systems are

00:15:42.577 --> 00:15:43.160
going to fail.

00:15:43.160 --> 00:15:45.700
We can't sit down ahead of
time and kind of just list ab

00:15:45.700 --> 00:15:47.680
initio, these are the
things that can go wrong,

00:15:47.680 --> 00:15:48.596
because we don't know.

00:15:48.596 --> 00:15:51.030
These failures come
from these interactions.

00:15:51.030 --> 00:15:55.180
But what we can do is we
can increase transparency

00:15:55.180 --> 00:15:57.340
by learning from the
failures that do happen

00:15:57.340 --> 00:16:01.600
and preventing them from kind
of spiraling out of control.

00:16:01.600 --> 00:16:04.000
And so it turns out that
there's one big obstacle

00:16:04.000 --> 00:16:06.850
to learning about our systems.

00:16:06.850 --> 00:16:11.200
And that is actually its bosses.

00:16:11.200 --> 00:16:14.050
Now, bosses.

00:16:14.050 --> 00:16:17.020
As a boss, bosses see
themselves as somebody

00:16:17.020 --> 00:16:18.410
who is very friendly.

00:16:18.410 --> 00:16:20.590
They have an open door policy.

00:16:20.590 --> 00:16:24.580
You know, they encourage
people to speak up.

00:16:24.580 --> 00:16:28.219
But what the research shows is
that people that are managed,

00:16:28.219 --> 00:16:30.010
they often see their
bosses more like this,

00:16:30.010 --> 00:16:35.140
as these kind of sort of scary,
bear-like, shadowy creatures,

00:16:35.140 --> 00:16:36.100
right?

00:16:36.100 --> 00:16:38.260
And this is something I
think you guys probably

00:16:38.260 --> 00:16:40.920
have a more
forward-thinking discussion

00:16:40.920 --> 00:16:41.920
than a lot of companies.

00:16:41.920 --> 00:16:43.985
Because you know,
Project Aristotle, one

00:16:43.985 --> 00:16:45.610
of the big things
that came out of that

00:16:45.610 --> 00:16:48.670
was how psychological safety
helps with performance.

00:16:48.670 --> 00:16:50.930
And that's really what
we're talking about here.

00:16:50.930 --> 00:16:52.820
We're not talking about it
in the performance context,

00:16:52.820 --> 00:16:54.528
but we're talking
about it in the context

00:16:54.528 --> 00:16:57.280
of getting information
about the things that

00:16:57.280 --> 00:17:00.910
are going wrong kind of up and
distributed to decision makers

00:17:00.910 --> 00:17:02.930
as broadly as possible.

00:17:02.930 --> 00:17:05.117
And so there's a lot of
really interesting research

00:17:05.117 --> 00:17:05.950
that goes into this.

00:17:05.950 --> 00:17:09.010
We're going to talk about
of two threads of that.

00:17:09.010 --> 00:17:11.790
So if you take a
look at these two--

00:17:11.790 --> 00:17:12.290
this graph.

00:17:12.290 --> 00:17:15.970
So this is a graph of two kind
of groups of different teams

00:17:15.970 --> 00:17:17.440
solving a complex problem.

00:17:17.440 --> 00:17:19.569
Everybody has different
pieces of information,

00:17:19.569 --> 00:17:21.760
and they have to bring
that information together

00:17:21.760 --> 00:17:23.240
to solve the problem.

00:17:23.240 --> 00:17:25.129
So the blue team
and the red team.

00:17:25.129 --> 00:17:26.920
Or sorry, the blue
group and the red group,

00:17:26.920 --> 00:17:29.560
each of which is comprised
of different teams.

00:17:29.560 --> 00:17:30.580
A number of them.

00:17:30.580 --> 00:17:33.460
So if you look, the blue
group proposes five solutions.

00:17:33.460 --> 00:17:36.700
The red group proposes
closer to seven.

00:17:36.700 --> 00:17:41.020
The blue group shares about half
as many facts before a decision

00:17:41.020 --> 00:17:43.150
is reached.

00:17:43.150 --> 00:17:46.680
And that's really
interesting, right?

00:17:46.680 --> 00:17:49.280
I mean, the solutions are
significantly different.

00:17:49.280 --> 00:17:50.380
The number of solutions.

00:17:50.380 --> 00:17:52.420
But the facts are
just wildly different.

00:17:52.420 --> 00:17:57.460
So the red groups have a
much more thorough discussion

00:17:57.460 --> 00:18:00.220
before a solution is reached.

00:18:00.220 --> 00:18:01.480
And you might ask, well, why?

00:18:01.480 --> 00:18:04.840
What's kind of behind this?

00:18:04.840 --> 00:18:07.480
Is there something intrinsic
about these managers

00:18:07.480 --> 00:18:08.890
of these groups as leaders?

00:18:08.890 --> 00:18:10.630
Or kind of what is it?

00:18:10.630 --> 00:18:11.820
Do they have MBAs?

00:18:11.820 --> 00:18:14.900
Do they have kind of more
leadership experience?

00:18:14.900 --> 00:18:16.630
And it turns out it's
nothing like that.

00:18:16.630 --> 00:18:18.370
It turns out that
the intervention

00:18:18.370 --> 00:18:19.810
is very, very small.

00:18:19.810 --> 00:18:21.580
The leaders of the
blue groups are

00:18:21.580 --> 00:18:24.400
taught to start their
discussions with this phrase--

00:18:24.400 --> 00:18:27.190
the most important thing is that
we all agree on our decision.

00:18:27.190 --> 00:18:30.045
Now, here's what I
think could be done.

00:18:30.045 --> 00:18:32.170
The leaders of these red
groups, on the other hand,

00:18:32.170 --> 00:18:35.020
of these red teams, are
taught to start by saying--

00:18:35.020 --> 00:18:38.170
the most important thing is that
we air all possible viewpoints

00:18:38.170 --> 00:18:39.760
to reach a good decision.

00:18:39.760 --> 00:18:43.840
Now, what does each of
you think should be done?

00:18:43.840 --> 00:18:45.190
That's pretty dramatic, right?

00:18:45.190 --> 00:18:47.470
I mean, this very
small intervention

00:18:47.470 --> 00:18:50.680
yields dramatically different
discussions and number

00:18:50.680 --> 00:18:52.960
of solutions that are proposed.

00:18:52.960 --> 00:18:56.230
Now, one critique of this
study is you might look at it

00:18:56.230 --> 00:18:58.300
and say, well, this is
just a psych study, right?

00:18:58.300 --> 00:19:00.740
So how does this actually
apply in the real world?

00:19:00.740 --> 00:19:02.625
And I think actually, you guys--

00:19:02.625 --> 00:19:04.000
again, through
Project Aristotle,

00:19:04.000 --> 00:19:06.790
you guys actually have some
of the most exciting data

00:19:06.790 --> 00:19:10.150
about how this can
apply in the real world.

00:19:10.150 --> 00:19:13.060
But we also have a
really nice context

00:19:13.060 --> 00:19:16.960
where we have a lot of good data
about how this kind of thing

00:19:16.960 --> 00:19:18.250
applies.

00:19:18.250 --> 00:19:20.560
And that comes actually
from commercial aviation.

00:19:20.560 --> 00:19:23.680
So if you look at the accident
rate over the last four decades

00:19:23.680 --> 00:19:27.100
in commercial aviation, it
has plummeted dramatically.

00:19:27.100 --> 00:19:30.310
And that is despite the
increasing complexity

00:19:30.310 --> 00:19:33.400
of the system and the kind of
increasing technology in it.

00:19:33.400 --> 00:19:34.990
And commercial
aviation has come up

00:19:34.990 --> 00:19:38.020
with a number of things,
a number of strategies

00:19:38.020 --> 00:19:42.010
and lessons that make them
more effective at dealing

00:19:42.010 --> 00:19:46.150
with this kind of big
system that they run.

00:19:46.150 --> 00:19:49.720
But one of them is called
crew resource management.

00:19:49.720 --> 00:19:52.810
And crew resource management
is, among other things,

00:19:52.810 --> 00:19:56.320
kind of a script for
how flight crews can

00:19:56.320 --> 00:19:59.020
talk about the decisions
that they need to make,

00:19:59.020 --> 00:20:01.090
and how they can surface issues.

00:20:01.090 --> 00:20:03.880
But also, how the captains
can listen to those issues

00:20:03.880 --> 00:20:06.390
and encourage those
voices of concern.

00:20:06.390 --> 00:20:08.940
And one of the things that
crew resource management does

00:20:08.940 --> 00:20:10.200
is it's a script.

00:20:10.200 --> 00:20:13.290
It's a script for how first
officers can share concerns

00:20:13.290 --> 00:20:15.660
and how captains should
listen to those concerns.

00:20:15.660 --> 00:20:18.720
And this isn't a lot
different than a kind

00:20:18.720 --> 00:20:20.532
of boss-employee relationship.

00:20:20.532 --> 00:20:21.990
There's a hierarchy
in the cockpit.

00:20:21.990 --> 00:20:23.970
The captain is usually
more experienced

00:20:23.970 --> 00:20:25.500
than the first officer.

00:20:25.500 --> 00:20:29.042
But the first step for the
first officer to raise a concern

00:20:29.042 --> 00:20:31.000
is to start by getting
the captain's attention.

00:20:31.000 --> 00:20:33.371
So you might say, hey, Andras.

00:20:33.371 --> 00:20:34.870
ANDRAS TILCSIK: I'm
the captain now?

00:20:34.870 --> 00:20:35.630
CHRIS CLEARFIELD:
Andras is the captain.

00:20:35.630 --> 00:20:36.150
ANDRAS TILCSIK: That's awesome.

00:20:36.150 --> 00:20:36.810
All right.

00:20:36.810 --> 00:20:37.650
Hey, Chris.

00:20:37.650 --> 00:20:38.900
CHRIS CLEARFIELD: Hey, Andras.

00:20:38.900 --> 00:20:41.066
I'm worried that we're going
to be late for our talk

00:20:41.066 --> 00:20:43.650
at Google, because I've heard
that traffic in the Bay Area

00:20:43.650 --> 00:20:46.140
is pretty notorious.

00:20:46.140 --> 00:20:48.450
I'm going to state the problem.

00:20:48.450 --> 00:20:50.394
I'm going to propose a solution.

00:20:50.394 --> 00:20:52.560
I think we should leave
really early from our hotel,

00:20:52.560 --> 00:20:54.570
so we make sure we
get there on time.

00:20:54.570 --> 00:20:56.070
And then, we get
explicit agreement.

00:20:56.070 --> 00:20:57.385
How does that sound to you?

00:20:57.385 --> 00:20:58.593
ANDRAS TILCSIK: Sounds great.

00:20:58.593 --> 00:20:59.940
I'd hate to miss that talk.

00:20:59.940 --> 00:21:00.939
CHRIS CLEARFIELD: Right?

00:21:00.939 --> 00:21:06.420
So what's fascinating about
this is not its sophistication,

00:21:06.420 --> 00:21:08.492
because it's not.

00:21:08.492 --> 00:21:09.450
I have a five-year-old.

00:21:09.450 --> 00:21:12.930
This is how I talk with
my five-year-old about how

00:21:12.930 --> 00:21:15.700
he can raise concerns,
how he can ask for help.

00:21:15.700 --> 00:21:17.700
But what's really interesting
about this is this

00:21:17.700 --> 00:21:21.562
had a dramatic effect
on the accident rates

00:21:21.562 --> 00:21:22.520
in commercial aviation.

00:21:22.520 --> 00:21:25.187
In particular, when the
captain was the pilot flying.

00:21:25.187 --> 00:21:27.020
That's what's really
interesting about this.

00:21:27.020 --> 00:21:30.192
So before, first officers
wouldn't challenge the captain.

00:21:30.192 --> 00:21:31.650
It was more likely
that the captain

00:21:31.650 --> 00:21:34.830
was the one flying during
these crew-caused mistakes.

00:21:34.830 --> 00:21:36.900
After crew resource
management and this language

00:21:36.900 --> 00:21:38.580
of dissent was kind of--

00:21:38.580 --> 00:21:41.100
the language of dissent was
taught, but also learned

00:21:41.100 --> 00:21:42.980
by the captain, by the bosses.

00:21:42.980 --> 00:21:44.800
That changed dramatically.

00:21:44.800 --> 00:21:46.950
And I think there's
a bigger lesson here

00:21:46.950 --> 00:21:49.740
because we don't all
fly in airline cockpits.

00:21:49.740 --> 00:21:53.580
But what it teaches us is
that speaking up and listening

00:21:53.580 --> 00:21:55.800
are teachable skills.

00:21:55.800 --> 00:21:57.690
And that's a really
powerful thing.

00:21:57.690 --> 00:22:00.660
I think that's a really
powerful takeaway.

00:22:00.660 --> 00:22:04.170
And what that
means is that there

00:22:04.170 --> 00:22:06.390
are techniques that
we can use kind

00:22:06.390 --> 00:22:10.140
of in our day-to-day
lives, in our organizations

00:22:10.140 --> 00:22:11.150
to help encourage that.

00:22:11.150 --> 00:22:13.950
And that turns out to be
one of the keys to learning

00:22:13.950 --> 00:22:16.710
from our systems, and adding
transparency, and catching

00:22:16.710 --> 00:22:19.290
those small errors before
they turn into the big ones.

00:22:19.290 --> 00:22:20.373
ANDRAS TILCSIK: All right.

00:22:20.373 --> 00:22:22.830
And I think we have time
to talk about one more

00:22:22.830 --> 00:22:23.670
set of solutions.

00:22:23.670 --> 00:22:26.010
And this is something
that really surprised us.

00:22:26.010 --> 00:22:28.950
This really wasn't
in our book proposal.

00:22:28.950 --> 00:22:31.920
This was not something
we thought about earlier,

00:22:31.920 --> 00:22:33.690
but it's something
that we kept running

00:22:33.690 --> 00:22:39.930
into as we were studying cases
of failure and resilience.

00:22:39.930 --> 00:22:44.550
And let me introduce this
with an example as well.

00:22:44.550 --> 00:22:46.710
Take a look at
these two students,

00:22:46.710 --> 00:22:47.790
hypothetical students.

00:22:47.790 --> 00:22:52.650
Let's call them Student
A and Student B.

00:22:52.650 --> 00:22:55.680
And imagine that they
are both applying

00:22:55.680 --> 00:22:58.920
to an elite,
academically-rigorous

00:22:58.920 --> 00:23:00.450
university.

00:23:00.450 --> 00:23:02.880
Which of them do you
think would be a stronger

00:23:02.880 --> 00:23:05.310
candidate for admission?

00:23:05.310 --> 00:23:08.880
If you asked people that
question individually,

00:23:08.880 --> 00:23:13.602
something like 95% or 98% of
people will say it's Student A.

00:23:13.602 --> 00:23:15.060
And it makes a lot
of sense, right?

00:23:15.060 --> 00:23:19.020
Student A has a higher
GPA, better SAT scores,

00:23:19.020 --> 00:23:24.000
and their activities don't seem
to be any inferior to those

00:23:24.000 --> 00:23:27.640
of Student B, unless you think
that drama club is really

00:23:27.640 --> 00:23:29.470
that important.

00:23:29.470 --> 00:23:33.240
But it really seems like the
right answer is Student A.

00:23:33.240 --> 00:23:36.450
But of course, this is when
we asked people the question

00:23:36.450 --> 00:23:37.900
on an individual basis.

00:23:37.900 --> 00:23:40.830
But as humans, we
are funny creatures

00:23:40.830 --> 00:23:45.060
when we are in our
group in a social setup.

00:23:45.060 --> 00:23:48.674
And one interesting
experiment that

00:23:48.674 --> 00:23:50.340
was done actually
just a couple of years

00:23:50.340 --> 00:23:56.460
ago by researchers at MIT and
a couple of other places, what

00:23:56.460 --> 00:23:59.209
they did is they put
people in a room.

00:23:59.209 --> 00:24:00.750
They gave them the
exact same problem

00:24:00.750 --> 00:24:03.120
that I just gave you,
Student A versus Student B,

00:24:03.120 --> 00:24:05.430
and a number of other problems.

00:24:05.430 --> 00:24:08.070
And they setup the room so
that each room would only

00:24:08.070 --> 00:24:09.990
have one actual subject.

00:24:09.990 --> 00:24:12.270
The other three
people were actors

00:24:12.270 --> 00:24:14.850
who were working
with the researcher.

00:24:14.850 --> 00:24:17.160
And the researchers
instructed the actors

00:24:17.160 --> 00:24:18.870
to give the wrong answer.

00:24:18.870 --> 00:24:22.740
To say Student B when the
problem was presented.

00:24:22.740 --> 00:24:27.580
So they would say Student
B, Student B, Student B.

00:24:27.580 --> 00:24:29.400
And in a very large
number of cases--

00:24:29.400 --> 00:24:30.870
and you might
recognize this setup

00:24:30.870 --> 00:24:33.650
from the famous Asch
conformity experiments that

00:24:33.650 --> 00:24:36.090
were done a few decades ago.

00:24:36.090 --> 00:24:38.940
The actual subject would
also say Student B,

00:24:38.940 --> 00:24:41.610
even though we know that when
we asked people individually,

00:24:41.610 --> 00:24:43.560
a very small
percentage of people

00:24:43.560 --> 00:24:45.630
would think that Student
B is the right answer.

00:24:45.630 --> 00:24:48.690
And a vast majority would agree
that it's actually Student A.

00:24:48.690 --> 00:24:51.330
Yet here, after hearing
these three strangers say

00:24:51.330 --> 00:24:53.295
Student B, Student
B, Student B, we

00:24:53.295 --> 00:24:55.950
are much more likely to
say it's Student B. Much

00:24:55.950 --> 00:24:58.860
more likely to give
the wrong answer.

00:24:58.860 --> 00:25:00.910
What's really interesting
about this though,

00:25:00.910 --> 00:25:04.690
is what happens when you
rerun the same experiment

00:25:04.690 --> 00:25:05.960
with diverse groups.

00:25:05.960 --> 00:25:08.410
In this case,
racially-diverse groups.

00:25:08.410 --> 00:25:09.830
So still the same setup.

00:25:09.830 --> 00:25:13.630
We have three actors, the same
problem, one actual subject.

00:25:13.630 --> 00:25:17.450
We introduce some ethnic or
racial diversity in this case,

00:25:17.450 --> 00:25:19.190
but its setup is the same.

00:25:19.190 --> 00:25:21.380
The actors say
Student B, Student B,

00:25:21.380 --> 00:25:25.130
Student B. But in these
cases, the actual subject is

00:25:25.130 --> 00:25:29.810
much less likely to fall victim
to this kind of conformity

00:25:29.810 --> 00:25:35.340
and much more likely to give
the right answer, Student A.

00:25:35.340 --> 00:25:38.720
And this is something we see
across a very large number

00:25:38.720 --> 00:25:40.880
of studies done in the
past five or so years,

00:25:40.880 --> 00:25:45.830
this effect that diversity
makes everyone more skeptical.

00:25:45.830 --> 00:25:48.260
And that's a very
different way of thinking

00:25:48.260 --> 00:25:51.380
about the facts or
benefits of diversity

00:25:51.380 --> 00:25:53.120
than what we usually do.

00:25:53.120 --> 00:25:57.080
Usually, we think diversity
doesn't really do anything.

00:25:57.080 --> 00:25:59.750
Or if it does something,
it's because people bring

00:25:59.750 --> 00:26:01.500
different ideas to the table.

00:26:01.500 --> 00:26:04.820
It's this sort of beautiful
group process where

00:26:04.820 --> 00:26:06.904
we each bring something
unique, and then we

00:26:06.904 --> 00:26:07.820
learn from each other.

00:26:07.820 --> 00:26:09.200
And there's this
great discussion

00:26:09.200 --> 00:26:10.730
and it's this beautiful thing.

00:26:10.730 --> 00:26:13.610
Well, what this research
shows is that in many cases,

00:26:13.610 --> 00:26:17.550
diversity is much more
painful than beautiful.

00:26:17.550 --> 00:26:20.780
It's almost like a
speed bump effect.

00:26:20.780 --> 00:26:23.990
It makes things less
smooth, less pleasant.

00:26:23.990 --> 00:26:27.510
It makes things harder,
but it wakes us up.

00:26:27.510 --> 00:26:30.320
And we see this kind of
effect not just in the lab.

00:26:30.320 --> 00:26:33.770
We see it in field
studies, lab studies,

00:26:33.770 --> 00:26:37.351
across all kinds of domains,
and across types of diversity

00:26:37.351 --> 00:26:37.850
as well.

00:26:37.850 --> 00:26:41.720
Whether it's diversity
in surface-level things.

00:26:41.720 --> 00:26:44.270
Things like race
and gender and age.

00:26:44.270 --> 00:26:47.180
But also, in things like
professional background.

00:26:47.180 --> 00:26:49.320
Things that are
not on the surface,

00:26:49.320 --> 00:26:51.540
but they seem to trigger
the same kind of effect.

00:26:51.540 --> 00:26:53.370
And we see this in finance.

00:26:53.370 --> 00:26:55.310
We see it in juries.

00:26:55.310 --> 00:26:58.350
We see it in a corporate
boardroom very,

00:26:58.350 --> 00:27:00.530
very consistently.

00:27:00.530 --> 00:27:02.155
CHRIS CLEARFIELD: So
I'm going to bring

00:27:02.155 --> 00:27:05.670
this home with kind of a summary
of what we've just looked at.

00:27:05.670 --> 00:27:07.820
So we started by
talking about systems,

00:27:07.820 --> 00:27:11.540
and how we can use design
to inject some transparency

00:27:11.540 --> 00:27:12.390
into our systems.

00:27:12.390 --> 00:27:16.090
But design isn't the only thing
that can help our systems.

00:27:16.090 --> 00:27:18.510
There's a number
of other things.

00:27:18.510 --> 00:27:20.210
One example that
we really like is

00:27:20.210 --> 00:27:24.290
you can use this idea of
complexity and coupling,

00:27:24.290 --> 00:27:27.237
and you can sort of use that
as a heuristic to figure out

00:27:27.237 --> 00:27:29.570
which of your systems you
should be paying attention to,

00:27:29.570 --> 00:27:32.930
where these kind of
failures are more likely.

00:27:32.930 --> 00:27:34.340
We talked about learning.

00:27:34.340 --> 00:27:36.530
Learning by hearing
voices of concern

00:27:36.530 --> 00:27:37.910
from inside your organization.

00:27:37.910 --> 00:27:39.770
But there's other ways of
learning about our systems,

00:27:39.770 --> 00:27:40.270
too.

00:27:40.270 --> 00:27:43.580
We can learn by obsessing about
small failures that happen.

00:27:43.580 --> 00:27:48.390
And really, using them as tools
to increase our understanding

00:27:48.390 --> 00:27:51.432
of how our system might fail.

00:27:51.432 --> 00:27:53.390
And finally, we talked
about, broadly speaking,

00:27:53.390 --> 00:27:54.056
decision-making.

00:27:54.056 --> 00:27:56.060
We talked about
how diversity can

00:27:56.060 --> 00:27:59.330
be a boon to how we can
make decisions in groups.

00:27:59.330 --> 00:28:02.060
But there's other ways we can
improve our decision-making

00:28:02.060 --> 00:28:04.790
too, from introducing
outsiders--

00:28:04.790 --> 00:28:08.090
something that groups like
NASA's Jet Propulsion Labs

00:28:08.090 --> 00:28:08.950
does--

00:28:08.950 --> 00:28:13.369
to using kind of a set of
predetermined criteria to score

00:28:13.369 --> 00:28:15.410
options that we have when
we're making a decision

00:28:15.410 --> 00:28:17.650
between a couple of things.

00:28:17.650 --> 00:28:19.880
And so I think broadly
speaking, the solutions

00:28:19.880 --> 00:28:23.976
that we have researched fall
roughly into these categories.

00:28:23.976 --> 00:28:25.850
But there's a lot of
nuance and there's a lot

00:28:25.850 --> 00:28:27.780
of different texture to them.

00:28:27.780 --> 00:28:30.660
And I mean, one thing that
you might be thinking--

00:28:30.660 --> 00:28:32.300
and that we thought
a lot as we were

00:28:32.300 --> 00:28:35.169
reading the book-- is that this
stuff isn't rocket science.

00:28:35.169 --> 00:28:36.710
I mean, you don't
have to be a genius

00:28:36.710 --> 00:28:41.210
to introduce an outsider into
your decision-making process.

00:28:41.210 --> 00:28:42.830
And what we kind
of came down to was

00:28:42.830 --> 00:28:45.320
that these solutions
are simple, but that

00:28:45.320 --> 00:28:46.790
doesn't mean that they're easy.

00:28:46.790 --> 00:28:48.590
And what we see
over and over again

00:28:48.590 --> 00:28:52.040
is examples where instead of
going for transparent design,

00:28:52.040 --> 00:28:54.950
we choose sort of
sleek, shiny design.

00:28:54.950 --> 00:28:57.890
Instead of listening
to voices of concern,

00:28:57.890 --> 00:28:59.349
we ignore them
and suppress them.

00:28:59.349 --> 00:29:00.890
And you know, what
happened in Flint,

00:29:00.890 --> 00:29:02.570
Michigan with the
lead poisoning crisis

00:29:02.570 --> 00:29:04.400
is a great example of that.

00:29:04.400 --> 00:29:07.210
Or rather, a tragic
example of that.

00:29:07.210 --> 00:29:09.170
And even though we know
the value of diversity

00:29:09.170 --> 00:29:12.080
and how it actually helps
groups make better decisions,

00:29:12.080 --> 00:29:14.360
a shocking number
of groups making

00:29:14.360 --> 00:29:16.070
some of our most
important decisions

00:29:16.070 --> 00:29:18.630
are strikingly homogeneous.

00:29:18.630 --> 00:29:22.610
And so really, what our
kind of broad thesis is,

00:29:22.610 --> 00:29:24.740
is that the world has changed.

00:29:24.740 --> 00:29:27.120
A lot more of these systems
are in this danger zone.

00:29:27.120 --> 00:29:28.620
They're complex and
tightly coupled.

00:29:28.620 --> 00:29:31.970
And they lead to these
kind of idiosyncratic.

00:29:31.970 --> 00:29:35.690
And yet, somehow connected
shared sets of failures

00:29:35.690 --> 00:29:37.640
caused by a shared
set of things.

00:29:37.640 --> 00:29:39.620
And many of our
organizations, many

00:29:39.620 --> 00:29:41.344
of our most important
organizations,

00:29:41.344 --> 00:29:43.760
their decision-making, the way
they approach these things,

00:29:43.760 --> 00:29:45.320
hasn't yet caught up.

00:29:45.320 --> 00:29:48.290
And so our hope with this
book is to really just start

00:29:48.290 --> 00:29:50.390
a conversation about that.

00:29:50.390 --> 00:29:53.030
And so that's where
we'll end for today.

00:29:53.030 --> 00:29:55.520
And rather, that's where
the conversation will begin.

00:29:55.520 --> 00:29:57.350
So thank you for your
attention, and we'd

00:29:57.350 --> 00:30:00.170
love to have a discussion
if anyone has any questions

00:30:00.170 --> 00:30:01.169
or thoughts.

00:30:01.169 --> 00:30:08.877
[APPLAUSE]

00:30:08.877 --> 00:30:10.460
CRAIG: There's a
microphone over here.

00:30:10.460 --> 00:30:13.537
If you have questions,
please step up to the mike.

00:30:13.537 --> 00:30:16.876
And the authors have said
they'll take as many questions

00:30:16.876 --> 00:30:17.970
as we have time for.

00:30:17.970 --> 00:30:19.720
CHRIS CLEARFIELD: I
have a flight at 6:00.

00:30:19.720 --> 00:30:21.300
But until then, I'm good.

00:30:21.300 --> 00:30:23.560
CRAIG: So please, step up.

00:30:23.560 --> 00:30:26.470
Have your shot at them.

00:30:26.470 --> 00:30:28.562
CHRIS CLEARFIELD:
Somebody has to be first.

00:30:28.562 --> 00:30:29.770
AUDIENCE: I'll be first then.

00:30:29.770 --> 00:30:30.645
CHRIS CLEARFIELD: OK.

00:30:32.277 --> 00:30:34.860
AUDIENCE: I've not had a chance
to read through the whole book

00:30:34.860 --> 00:30:36.880
yet, but I wanted
to ask your opinion

00:30:36.880 --> 00:30:38.100
of artificial intelligence.

00:30:38.100 --> 00:30:39.420
Where does this
apply to the system

00:30:39.420 --> 00:30:40.794
and the complexity
of the systems

00:30:40.794 --> 00:30:42.511
that we're seeing today?

00:30:42.511 --> 00:30:44.510
CHRIS CLEARFIELD: Yeah,
that's a great question.

00:30:44.510 --> 00:30:48.700
So where does this apply
to artificial intelligence?

00:30:48.700 --> 00:30:53.560
I mean, I think artificial
intelligence in many ways is--

00:30:53.560 --> 00:30:55.360
I mean, I think
it's a big question,

00:30:55.360 --> 00:30:57.260
so there's a lot of answers.

00:30:57.260 --> 00:31:00.040
I mean, on the one hand,
artificial intelligence

00:31:00.040 --> 00:31:02.530
is in some ways the
ultimate black box, right?

00:31:02.530 --> 00:31:08.320
So it sort of takes transparency
and entirely removes it

00:31:08.320 --> 00:31:11.380
and replaces it with
a set of parameters

00:31:11.380 --> 00:31:14.560
that nobody but the
model itself understands.

00:31:14.560 --> 00:31:17.560
And so I think from that
perspective, that I would say

00:31:17.560 --> 00:31:20.460
is a little bit concerning.

00:31:20.460 --> 00:31:21.795
Do you have anything to add?

00:31:21.795 --> 00:31:22.670
ANDRAS TILCSIK: Yeah.

00:31:22.670 --> 00:31:24.470
No, I would say that.

00:31:24.470 --> 00:31:27.160
I think there's
potentially an upside, too,

00:31:27.160 --> 00:31:28.580
in terms of these issues.

00:31:28.580 --> 00:31:34.450
And I think that
this notion that AI,

00:31:34.450 --> 00:31:37.120
if used correctly
and well-calibrated,

00:31:37.120 --> 00:31:40.450
will allow us to
predict things in a more

00:31:40.450 --> 00:31:43.840
effective way, especially in
these very complex systems.

00:31:43.840 --> 00:31:46.320
So it's this interesting
tension between,

00:31:46.320 --> 00:31:50.320
are these things going to be the
ultimate black box or are they

00:31:50.320 --> 00:31:54.160
going to be our most effective
prediction machines that we

00:31:54.160 --> 00:31:58.720
can use to prevent, or at least
foresee some of these failures?

00:31:58.720 --> 00:32:01.390
Or at least learn better from
the enormous amounts of data

00:32:01.390 --> 00:32:03.390
that we are generating
as we are trying to learn

00:32:03.390 --> 00:32:04.880
about these small failures.

00:32:04.880 --> 00:32:06.760
And I think at
Google, either way

00:32:06.760 --> 00:32:10.070
I think the challenge is to
sort of manage that trade-off

00:32:10.070 --> 00:32:12.660
and end up on the positive path.

00:32:12.660 --> 00:32:14.410
CHRIS CLEARFIELD: And
I think I would also

00:32:14.410 --> 00:32:16.270
kind of push the
answer back a level

00:32:16.270 --> 00:32:19.200
and say that, I
think whether or not

00:32:19.200 --> 00:32:22.150
AI is able to help
or hurt depends

00:32:22.150 --> 00:32:24.610
on the organizations that
are building the AI, too.

00:32:24.610 --> 00:32:29.350
And whether or not they are able
to surface voices of concern

00:32:29.350 --> 00:32:32.050
or pay attention to these
things that aren't quite right.

00:32:32.050 --> 00:32:34.360
Because I think the other
side of the AI question

00:32:34.360 --> 00:32:37.870
is that we're building AI
into more and more important

00:32:37.870 --> 00:32:39.694
systems.

00:32:39.694 --> 00:32:41.110
We can talk more
about this, but I

00:32:41.110 --> 00:32:44.110
think driverless cars are
an excellent example of both

00:32:44.110 --> 00:32:46.910
the upside and the
challenge of that.

00:32:50.312 --> 00:32:51.770
Thanks.

00:32:51.770 --> 00:32:52.830
Do you have a question?

00:32:56.494 --> 00:32:59.790
AUDIENCE: So I was thinking
about the airplane example.

00:32:59.790 --> 00:33:02.390
And you know, there's
this funny thing

00:33:02.390 --> 00:33:04.529
that when you talk
to users, sometimes

00:33:04.529 --> 00:33:05.820
they don't know what they want.

00:33:05.820 --> 00:33:08.590
Or they think they know they
want, but really they're wrong.

00:33:11.190 --> 00:33:13.804
And maybe this is
just UX design, which

00:33:13.804 --> 00:33:14.970
I don't know anything about.

00:33:14.970 --> 00:33:18.240
But I wonder if just as there
are techniques for encouraging

00:33:18.240 --> 00:33:21.480
dissent in a
[INAUDIBLE],, are there

00:33:21.480 --> 00:33:25.200
techniques for getting users
to be-- like the people who

00:33:25.200 --> 00:33:28.911
designed the new cockpit style
must have talked to the pilots.

00:33:28.911 --> 00:33:30.660
And I guess they must
have heard, oh, it's

00:33:30.660 --> 00:33:34.290
a pain in the butt that we
knock each other's lunch

00:33:34.290 --> 00:33:35.310
on their shirts.

00:33:35.310 --> 00:33:38.669
And they weren't made
to think of the things

00:33:38.669 --> 00:33:40.710
that they were really
using about that interface.

00:33:40.710 --> 00:33:41.762
Are there techniques?

00:33:41.762 --> 00:33:43.470
Just like for the
[INAUDIBLE],, are there

00:33:43.470 --> 00:33:46.590
techniques of getting
users to better understand

00:33:46.590 --> 00:33:49.260
what are the mechanisms that are
making their systems not work?

00:33:49.260 --> 00:33:51.260
ANDRAS TILCSIK: I think
that's a great question.

00:33:51.260 --> 00:33:53.460
And I think what we've
seen in the aviation

00:33:53.460 --> 00:33:56.170
case is that it's not so much--

00:33:56.170 --> 00:33:58.020
I think it's a huge
challenge to get people

00:33:58.020 --> 00:34:00.000
to describe what they
want and how-- especially

00:34:00.000 --> 00:34:04.710
in the cockpit design, where
it's really hard for a pilot

00:34:04.710 --> 00:34:07.530
to describe how they
might be interacting

00:34:07.530 --> 00:34:11.010
with this interface
in a crisis situation.

00:34:11.010 --> 00:34:13.100
People can explain,
oh, on a normal day,

00:34:13.100 --> 00:34:14.840
it will be very nice
to have my lunch.

00:34:14.840 --> 00:34:17.150
And it's going to
be very comfortable.

00:34:17.150 --> 00:34:20.100
But it's really hard to
kind of think through that.

00:34:20.100 --> 00:34:22.630
And I don't think that's
what aviation has done.

00:34:22.630 --> 00:34:24.102
So I think with--

00:34:24.102 --> 00:34:25.560
well, maybe in the
Airbus case they

00:34:25.560 --> 00:34:26.851
sort of talked to their pilots.

00:34:26.851 --> 00:34:28.830
And maybe that's
part of the problem

00:34:28.830 --> 00:34:32.790
rather than watching what
the pilots actually do.

00:34:32.790 --> 00:34:36.185
The reason we are learning about
these issues, and the reason I

00:34:36.185 --> 00:34:37.560
could show you
the slide and talk

00:34:37.560 --> 00:34:42.540
about the benefits of that
ugly but transparent design

00:34:42.540 --> 00:34:45.150
is that we are seeing accidents.

00:34:45.150 --> 00:34:48.030
Or actually, some near misses
where we see people in action.

00:34:48.030 --> 00:34:50.670
And of course, aviation
has a huge advantage

00:34:50.670 --> 00:34:54.389
in that we have cockpit
recordings and black boxes.

00:34:54.389 --> 00:34:57.540
And we know exactly
what was happening

00:34:57.540 --> 00:35:00.940
and who was pulling back
and who was pushing forward.

00:35:00.940 --> 00:35:05.026
But I think the broader
lesson there is to--

00:35:05.026 --> 00:35:06.900
I mean, it's really
about the basic principle

00:35:06.900 --> 00:35:10.260
of design thinking, of
watching people interact

00:35:10.260 --> 00:35:14.250
with these systems across a
number of different situations

00:35:14.250 --> 00:35:16.630
rather than necessarily
listening to them

00:35:16.630 --> 00:35:18.786
as they are explaining it.

00:35:18.786 --> 00:35:20.910
CHRIS CLEARFIELD: I think
there's an element of it,

00:35:20.910 --> 00:35:22.750
too, that Andras
kind of touched on,

00:35:22.750 --> 00:35:27.600
which is not designing for
the blue sky day, right?

00:35:27.600 --> 00:35:31.400
Sort of pushing yourself
and your design team

00:35:31.400 --> 00:35:34.170
to look for all-- when you
go off the happy path, right?

00:35:34.170 --> 00:35:37.860
And I think that
you could imagine

00:35:37.860 --> 00:35:39.420
the Airbus is having
a design where

00:35:39.420 --> 00:35:40.889
they have those
lovely side sticks,

00:35:40.889 --> 00:35:42.180
but they are physically linked.

00:35:42.180 --> 00:35:43.620
They are physically connected.

00:35:43.620 --> 00:35:48.730
And for us, that's
kind of this--

00:35:48.730 --> 00:35:54.330
I think what we see is that
the users can take you so far,

00:35:54.330 --> 00:35:58.140
but then you also have to
think in terms of this--

00:35:58.140 --> 00:36:00.810
in terms if you think about
transparency, complexity,

00:36:00.810 --> 00:36:04.290
and coupling as variables when
you're designing the system,

00:36:04.290 --> 00:36:08.820
then that can give you some
really powerful insights to how

00:36:08.820 --> 00:36:13.830
to make sure that your
sphere of design is broader,

00:36:13.830 --> 00:36:15.060
I guess I would say.

00:36:15.060 --> 00:36:16.260
ANDRAS TILCSIK: Yeah.

00:36:16.260 --> 00:36:17.910
We talked about this
idea of learning

00:36:17.910 --> 00:36:20.201
from these small
failures as they happen.

00:36:20.201 --> 00:36:22.200
I mean, one of the hallmarks
of a complex system

00:36:22.200 --> 00:36:25.200
is that you can't sit
down and just predict

00:36:25.200 --> 00:36:28.560
how things might fail, but
you can learn something

00:36:28.560 --> 00:36:32.470
about how things might fail as
people are using the system.

00:36:32.470 --> 00:36:36.090
So another example is the
design of gear shifters

00:36:36.090 --> 00:36:37.800
in cars, where we
are actually seeing

00:36:37.800 --> 00:36:42.630
the exact same kind of
process where there have been

00:36:42.630 --> 00:36:44.130
a number of designs
in recent years

00:36:44.130 --> 00:36:45.730
that are very
sleek, very elegant.

00:36:45.730 --> 00:36:47.670
They are low dials
and things like that.

00:36:47.670 --> 00:36:49.860
Things that are
mostly computerized,

00:36:49.860 --> 00:36:53.370
don't have a lot of
physical feedback.

00:36:53.370 --> 00:36:54.840
And car companies
introduced them

00:36:54.840 --> 00:36:58.410
because they could and they
look nice and they were pretty,

00:36:58.410 --> 00:37:00.690
but we started seeing
a number of accidents

00:37:00.690 --> 00:37:03.310
where people didn't know
what gear their car was in.

00:37:03.310 --> 00:37:04.560
They would get out of the car.

00:37:04.560 --> 00:37:08.727
They would get injured or
the car would get damaged.

00:37:08.727 --> 00:37:11.310
And there was data about that,
but the companies didn't really

00:37:11.310 --> 00:37:12.490
pay attention to that.

00:37:12.490 --> 00:37:15.390
And often, that kind
of data is much more

00:37:15.390 --> 00:37:18.390
valuable than asking
a user, would you

00:37:18.390 --> 00:37:19.440
like this little dial?

00:37:19.440 --> 00:37:21.870
People are pretty
terrible at knowing

00:37:21.870 --> 00:37:23.420
what they'll actually use.

00:37:27.570 --> 00:37:29.676
CHRIS CLEARFIELD:
Other questions?

00:37:29.676 --> 00:37:30.910
Could you use the microphone?

00:37:30.910 --> 00:37:31.410
Thanks.

00:37:34.650 --> 00:37:37.480
AUDIENCE: So going back
to the Starbucks example,

00:37:37.480 --> 00:37:40.540
it sounded like these
[INAUDIBLE] systems

00:37:40.540 --> 00:37:42.830
and there's a lot of
human factor involved.

00:37:42.830 --> 00:37:46.690
This is not something new
that the trolling is pretty

00:37:46.690 --> 00:37:49.310
common on the Twitter world.

00:37:49.310 --> 00:37:52.370
So looking back to your
framework, what [INAUDIBLE]

00:37:52.370 --> 00:37:56.650
how could have Starbucks
done things differently?

00:37:56.650 --> 00:37:59.682
I want to understand how
these principles apply?

00:37:59.682 --> 00:38:00.640
CHRIS CLEARFIELD: Yeah.

00:38:00.640 --> 00:38:02.260
Well, I would start
even a step back

00:38:02.260 --> 00:38:07.000
where you're right that this
is not an unfamiliar dynamic

00:38:07.000 --> 00:38:10.310
for-- trolling is not an
unfamiliar dynamic for Twitter.

00:38:10.310 --> 00:38:14.870
But I think it's also easy
to forget how new Twitter is.

00:38:14.870 --> 00:38:17.740
So if you think about
Starbucks and they're

00:38:17.740 --> 00:38:20.740
kind of building up their
experience marketing.

00:38:20.740 --> 00:38:23.149
Most of that experience
or a lot of it,

00:38:23.149 --> 00:38:24.940
especially at the time
where this was done,

00:38:24.940 --> 00:38:27.530
was built up in the
non-Twitter world, right?

00:38:27.530 --> 00:38:29.380
It was built up in the--

00:38:29.380 --> 00:38:32.050
we come up with a campaign, we
push it out to people, people

00:38:32.050 --> 00:38:35.020
like it or don't like it.

00:38:35.020 --> 00:38:37.450
As you said, this is
an example where they

00:38:37.450 --> 00:38:41.720
have this kind of feedback.

00:38:41.720 --> 00:38:44.950
What I think, I guess, is--

00:38:44.950 --> 00:38:49.930
I mean, one thing might
be, be a little bit more

00:38:49.930 --> 00:38:53.470
skeptical of how you
are engaging users.

00:38:53.470 --> 00:38:57.110
Again, they designed for going
off the happy path a little bit

00:38:57.110 --> 00:38:58.960
in that they had
this content filter.

00:38:58.960 --> 00:39:01.690
But the fact that the
content filter broke.

00:39:01.690 --> 00:39:05.380
I mean, I think on the face of
it means that they didn't put

00:39:05.380 --> 00:39:10.030
the engineering talent or they
didn't take the process as

00:39:10.030 --> 00:39:13.330
seriously as they could have,
as they maybe needed to.

00:39:13.330 --> 00:39:16.270
And I think that is
actually a surprising--

00:39:16.270 --> 00:39:19.780
that is actually something that
companies have to learn from.

00:39:19.780 --> 00:39:23.440
That just like United
Airlines is seeing with--

00:39:23.440 --> 00:39:26.140
I mean, they just had PR
disaster after PR disaster,

00:39:26.140 --> 00:39:26.980
right?

00:39:26.980 --> 00:39:30.520
It used to be that that would
stay on the plane, right?

00:39:30.520 --> 00:39:32.440
And maybe other
customers on that

00:39:32.440 --> 00:39:35.230
flight would be aghast that
somebody was pulled off

00:39:35.230 --> 00:39:36.290
in that way.

00:39:36.290 --> 00:39:37.600
But now, it goes global.

00:39:37.600 --> 00:39:39.280
And it goes global immediately.

00:39:39.280 --> 00:39:44.260
And I think that the way that
these controversies can spread

00:39:44.260 --> 00:39:47.200
mean that these
companies have to take

00:39:47.200 --> 00:39:49.420
a very different
approach to managing

00:39:49.420 --> 00:39:52.090
not just their marketing,
but also their kind of--

00:39:52.090 --> 00:39:54.130
their whole approach
to this kind of thing.

00:39:54.130 --> 00:39:55.880
ANDRAS TILCSIK: I'll
add two other things.

00:39:55.880 --> 00:39:58.990
One is while
researching the book,

00:39:58.990 --> 00:40:02.290
we have come across a number of
these tools that teams can use.

00:40:02.290 --> 00:40:06.610
Little exercises that
allow them to think

00:40:06.610 --> 00:40:09.760
about these sort of
less expected risks

00:40:09.760 --> 00:40:13.570
more effectively than
they normally do.

00:40:13.570 --> 00:40:15.830
You'll see a number
of examples of that.

00:40:15.830 --> 00:40:17.860
The other thing
I'll add is relating

00:40:17.860 --> 00:40:20.500
to our point about diversity.

00:40:20.500 --> 00:40:25.840
So the Starbucks controversy
around wages and taxes

00:40:25.840 --> 00:40:31.600
at the time was really big and
really prominent in the UK.

00:40:31.600 --> 00:40:33.680
But this was a global campaign.

00:40:33.680 --> 00:40:35.560
And I think people
rolling out the campaign

00:40:35.560 --> 00:40:38.200
might not have
been aware of just

00:40:38.200 --> 00:40:41.940
how controversial
and problematic

00:40:41.940 --> 00:40:44.210
this could be
right at that time.

00:40:44.210 --> 00:40:51.250
And I think it's interesting
when you end up with this--

00:40:51.250 --> 00:40:53.250
you are trying to run
something globally,

00:40:53.250 --> 00:40:54.910
yet the team is so local.

00:40:54.910 --> 00:40:56.860
Or the decision-makers
are so local,

00:40:56.860 --> 00:40:59.134
but the consequences are global.

00:40:59.134 --> 00:41:01.550
CHRIS CLEARFIELD: I think I'll
add one more thing to that,

00:41:01.550 --> 00:41:03.841
which is it seems like what
Starbucks did-- and we also

00:41:03.841 --> 00:41:07.010
saw those with-- do you guys
remember Tay, Microsoft's Tay

00:41:07.010 --> 00:41:08.620
bot?

00:41:08.620 --> 00:41:10.520
So what happened in
both of these cases--

00:41:10.520 --> 00:41:13.180
and I think there's kind of
a common lesson to them--

00:41:13.180 --> 00:41:16.690
is they made this product and
they put it out in the world.

00:41:16.690 --> 00:41:18.910
And they just kind of
threw it over the wall.

00:41:18.910 --> 00:41:23.140
And I mean, I can't believe that
Tay ran for as long as it did.

00:41:23.140 --> 00:41:25.750
I mean, to me that's
really surprising.

00:41:25.750 --> 00:41:27.460
If you're going to
build an AI or you're

00:41:27.460 --> 00:41:30.100
going to build a system that
has all of this feedback in it,

00:41:30.100 --> 00:41:31.640
then I think you need
to be watching it much,

00:41:31.640 --> 00:41:32.410
much more closely.

00:41:32.410 --> 00:41:35.200
And I think that's a
lesson that Starbuck--

00:41:35.200 --> 00:41:38.860
they could have had somebody
monitoring it at some level.

00:41:38.860 --> 00:41:42.430
And I mean, you guys know how to
build tools to monitor things,

00:41:42.430 --> 00:41:42.940
right?

00:41:42.940 --> 00:41:45.610
They could have done that
and taken that aspect

00:41:45.610 --> 00:41:47.560
of the process more seriously.

00:41:50.700 --> 00:41:52.210
ANDRAS TILCSIK: Other questions?

00:41:52.210 --> 00:41:53.168
CHRIS CLEARFIELD: Yeah.

00:41:58.240 --> 00:42:01.940
AUDIENCE: So my question is
related to the failure itself.

00:42:01.940 --> 00:42:04.930
So I'm wondering
like, so there's

00:42:04.930 --> 00:42:12.820
a book from [INAUDIBLE]
about [INAUDIBLE]..

00:42:12.820 --> 00:42:16.485
But my question would be
about, like maybe failure

00:42:16.485 --> 00:42:17.570
is good for the system?

00:42:17.570 --> 00:42:21.060
Like small-scale failures,
you know, large failures.

00:42:21.060 --> 00:42:24.370
So it kind of
strengthens the system.

00:42:24.370 --> 00:42:27.480
So in your framework, what
is the scale that failure

00:42:27.480 --> 00:42:30.810
is good for the whole system?

00:42:30.810 --> 00:42:33.633
How do you measure the scale
of the failure, if it's big

00:42:33.633 --> 00:42:34.215
or not?

00:42:34.215 --> 00:42:36.215
CHRIS CLEARFIELD: Yeah,
that's a great question.

00:42:39.731 --> 00:42:41.660
I think we start
with the premise

00:42:41.660 --> 00:42:45.110
that failure is a natural
consequence of building

00:42:45.110 --> 00:42:46.410
any system.

00:42:46.410 --> 00:42:48.410
And so what we
have focused on is

00:42:48.410 --> 00:42:51.080
how you obsess about
failures so that you

00:42:51.080 --> 00:42:53.750
learn from them before
they become the big ones.

00:42:53.750 --> 00:42:57.500
And also, how you
recognize when your system

00:42:57.500 --> 00:42:59.390
is in a fundamentally
different regime.

00:42:59.390 --> 00:43:01.850
Because I think that's
another big thing.

00:43:01.850 --> 00:43:04.400
Some systems, you can
afford to have failures

00:43:04.400 --> 00:43:07.130
in because they're local,
and they're not tragic,

00:43:07.130 --> 00:43:08.600
and they don't propagate.

00:43:08.600 --> 00:43:13.760
But if you think about like
BP's Deepwater Horizon oil rig.

00:43:13.760 --> 00:43:17.220
You know, the consequences
of that failure.

00:43:17.220 --> 00:43:20.241
A, it was more likely
to happen because of all

00:43:20.241 --> 00:43:22.490
of the factors that obscured
the engineering that they

00:43:22.490 --> 00:43:23.240
were doing, right?

00:43:23.240 --> 00:43:24.260
It was under the ocean.

00:43:24.260 --> 00:43:25.730
It was under the rock.

00:43:25.730 --> 00:43:27.162
They couldn't
directly observe it.

00:43:27.162 --> 00:43:29.120
They had to use simulations
for a lot of stuff.

00:43:29.120 --> 00:43:32.180
And they really relied on
their gut and their intuition

00:43:32.180 --> 00:43:36.410
to decide on how to kind of
manage the drilling process.

00:43:36.410 --> 00:43:39.230
And what we saw is that,
A, that didn't work.

00:43:39.230 --> 00:43:41.810
And B, the consequences
were not only tragic

00:43:41.810 --> 00:43:44.270
and that people died,
but were expensive.

00:43:44.270 --> 00:43:47.030
I mean, $50 billion.

00:43:47.030 --> 00:43:48.190
And long-lasting.

00:43:48.190 --> 00:43:50.030
And the environmental
damage is still

00:43:50.030 --> 00:43:53.930
something that is present
and being managed.

00:43:53.930 --> 00:43:57.040
And so I think our
perspective is that--

00:43:57.040 --> 00:43:58.790
it's interesting you
mentioned [INAUDIBLE]

00:43:58.790 --> 00:44:00.990
because the black
swan comes up a lot.

00:44:00.990 --> 00:44:06.050
And I think there probably are
true black swans out there,

00:44:06.050 --> 00:44:09.140
but what we kind of think
is that most of the time,

00:44:09.140 --> 00:44:11.581
you can find feathers long
before the black swan happens,

00:44:11.581 --> 00:44:12.080
right?

00:44:12.080 --> 00:44:14.390
So you can see these
black feathers.

00:44:14.390 --> 00:44:17.120
And that the chance
that your system

00:44:17.120 --> 00:44:19.190
isn't telling you
any information

00:44:19.190 --> 00:44:21.279
that there might be a
problem, that's pretty rare.

00:44:21.279 --> 00:44:23.570
What's much more common is
that your organization isn't

00:44:23.570 --> 00:44:25.330
learning from that information.

00:44:25.330 --> 00:44:26.205
ANDRAS TILCSIK: Yeah.

00:44:26.205 --> 00:44:29.299
So in a sense, it's almost
like the issue isn't scale.

00:44:29.299 --> 00:44:31.340
So where you have a good
failure or a bad failure

00:44:31.340 --> 00:44:32.360
isn't simply scale.

00:44:32.360 --> 00:44:34.550
It's sort of what you do
with that failure, right?

00:44:34.550 --> 00:44:37.790
So obviously, Deepwater
Horizon is a terrible failure.

00:44:37.790 --> 00:44:39.680
It could have still
been a good failure

00:44:39.680 --> 00:44:43.122
in a particular sense had
the industry learned from it.

00:44:43.122 --> 00:44:44.830
And that's what we
have seen in aviation.

00:44:44.830 --> 00:44:47.810
They had some terrible
accidents in the '70s and '80s,

00:44:47.810 --> 00:44:49.640
which then triggered
all this learning.

00:44:49.640 --> 00:44:52.700
I don't think we are seeing
that in offshore drilling.

00:44:52.700 --> 00:44:54.672
And it's really
what you do with it.

00:44:54.672 --> 00:44:55.630
CHRIS CLEARFIELD: Yeah.

00:44:55.630 --> 00:44:57.650
And I think even in the
Deepwater Horizon case

00:44:57.650 --> 00:44:59.907
in particular, there were
lots of small failures

00:44:59.907 --> 00:45:01.490
that preceded that
big failure, right?

00:45:01.490 --> 00:45:03.150
There were warning
signs that were ignored,

00:45:03.150 --> 00:45:04.110
and so on and so forth.

00:45:04.110 --> 00:45:07.160
And I think the dynamics of that
accident are really interesting

00:45:07.160 --> 00:45:09.342
and can be really
instructive for thinking

00:45:09.342 --> 00:45:11.800
about your question, which is
a really thoughtful question.

00:45:11.800 --> 00:45:14.520
Thank you.

00:45:14.520 --> 00:45:17.090
CRAIG: Other questions?

00:45:17.090 --> 00:45:18.077
I have one.

00:45:18.077 --> 00:45:19.076
CHRIS CLEARFIELD: Great.

00:45:22.190 --> 00:45:25.050
CRAIG: I didn't hear two
words very often in your talk.

00:45:25.050 --> 00:45:26.480
One, testing.

00:45:26.480 --> 00:45:28.600
And the other, redundancy.

00:45:28.600 --> 00:45:30.900
And I think [INAUDIBLE].

00:45:30.900 --> 00:45:33.270
Would you have a
comment on that?

00:45:33.270 --> 00:45:35.490
CHRIS CLEARFIELD: Yeah.

00:45:35.490 --> 00:45:38.004
Testing is good.

00:45:38.004 --> 00:45:39.420
I think part of
why we didn't talk

00:45:39.420 --> 00:45:42.510
about it is we know that that's
something that you guys already

00:45:42.510 --> 00:45:44.730
do a ton of.

00:45:44.730 --> 00:45:47.910
You guys call it the Disaster
Recovery Team, the DRT team.

00:45:47.910 --> 00:45:48.780
Is that right?

00:45:48.780 --> 00:45:51.330
So I mean, I think
as an organization,

00:45:51.330 --> 00:45:52.747
you all take a
fantastic approach.

00:45:52.747 --> 00:45:54.830
Actually, it looks a lot
like Jet Propulsion Labs,

00:45:54.830 --> 00:45:56.890
where you've kind of
incorporated these outsiders

00:45:56.890 --> 00:46:01.900
into how teams are designing
and engineering things.

00:46:01.900 --> 00:46:02.400
Yeah.

00:46:02.400 --> 00:46:04.560
I mean, testing is
super-important.

00:46:08.190 --> 00:46:10.380
If you think about
learning from failures

00:46:10.380 --> 00:46:12.000
that you see as
this step, testing

00:46:12.000 --> 00:46:13.260
is almost the previous step.

00:46:13.260 --> 00:46:16.404
It's like-- sorry.

00:46:16.404 --> 00:46:17.820
Setting up situations
where you're

00:46:17.820 --> 00:46:19.194
trying to cause
these failures so

00:46:19.194 --> 00:46:21.360
you can preemptively
learn from them.

00:46:21.360 --> 00:46:23.470
I mean, one of the things
that we do see though,

00:46:23.470 --> 00:46:28.980
is that testing isn't
the whole enchilada.

00:46:28.980 --> 00:46:33.810
It's not enough because you
can never really reproduce

00:46:33.810 --> 00:46:34.980
the real world.

00:46:34.980 --> 00:46:37.710
And there's a really
interesting example--

00:46:37.710 --> 00:46:39.030
the Facebook IPO.

00:46:39.030 --> 00:46:42.120
We go into kind of how
NASDAQ tested for that.

00:46:42.120 --> 00:46:45.660
And also, how that
failure still happened.

00:46:45.660 --> 00:46:47.670
So testing is awesome.

00:46:47.670 --> 00:46:49.770
And I think we kind
of [INAUDIBLE] it

00:46:49.770 --> 00:46:52.470
a little bit because we think
that you guys are probably

00:46:52.470 --> 00:46:54.930
test evangelists.

00:46:54.930 --> 00:46:56.430
Redundancy is
really interesting.

00:46:56.430 --> 00:46:58.680
I mean, redundancy
has a huge upside.

00:46:58.680 --> 00:47:00.840
But the other thing
redundancy does

00:47:00.840 --> 00:47:04.030
is it adds a lot of
complexity to the system.

00:47:04.030 --> 00:47:08.190
And so you see kind of
failure after failure

00:47:08.190 --> 00:47:14.460
where redundancy is the
cause or exacerbates

00:47:14.460 --> 00:47:16.390
the issue in the first place.

00:47:16.390 --> 00:47:19.410
And so yes, you need
to have redundancy.

00:47:19.410 --> 00:47:25.500
But I think you need to
have redundancy in, as--

00:47:25.500 --> 00:47:31.410
I would say, almost like as
redundancy can't be an add-on.

00:47:31.410 --> 00:47:34.200
It has to be kind of an
inherent part of the setup.

00:47:34.200 --> 00:47:36.390
And if you have
that, it's much more

00:47:36.390 --> 00:47:39.300
likely to be an upside
rather than a downside.

00:47:39.300 --> 00:47:40.830
ANDRAS TILCSIK:
I'll also say that I

00:47:40.830 --> 00:47:43.830
think one problem we see
in a lot of these cases

00:47:43.830 --> 00:47:48.600
is that redundancy has these
social and psychological

00:47:48.600 --> 00:47:49.980
effects on people.

00:47:49.980 --> 00:47:52.710
So one thing we
know is that when

00:47:52.710 --> 00:47:54.420
there is some redundancy
in the system,

00:47:54.420 --> 00:47:56.880
even if it's not
perfect, people often

00:47:56.880 --> 00:48:00.570
tend to believe that it is
perfect or at least very

00:48:00.570 --> 00:48:04.350
robust, which then leads
them to take more risks.

00:48:04.350 --> 00:48:06.540
So we talked about
Deepwater Horizon just now.

00:48:06.540 --> 00:48:09.720
I think that's an
amazing example of that.

00:48:09.720 --> 00:48:12.540
There is a point
in that story where

00:48:12.540 --> 00:48:18.360
one of the engineers
on board says,

00:48:18.360 --> 00:48:22.110
I'm really not happy
with what we are doing.

00:48:22.110 --> 00:48:25.140
I'm very reluctant to
go forward with this,

00:48:25.140 --> 00:48:27.840
but I know we have
this redundant system.

00:48:27.840 --> 00:48:29.490
We have this blowout preventer.

00:48:29.490 --> 00:48:31.260
That if everything
else falls apart,

00:48:31.260 --> 00:48:33.180
it will just cut things off.

00:48:33.180 --> 00:48:36.120
And so he says, I guess that's
what we have that system for,

00:48:36.120 --> 00:48:37.410
so let's do it.

00:48:37.410 --> 00:48:41.100
And of course, when
the explosion happens

00:48:41.100 --> 00:48:44.580
and the failure happens, it
takes out that redundant system

00:48:44.580 --> 00:48:45.880
as well.

00:48:45.880 --> 00:48:48.990
And I think-- in fact, I'm
pretty convinced that had they

00:48:48.990 --> 00:48:51.240
not had that redundant
system, they probably

00:48:51.240 --> 00:48:55.860
would not have taken such
a risky series of steps.

00:48:58.690 --> 00:49:00.689
CRAIG: Any other questions?

00:49:00.689 --> 00:49:02.480
I'm going to take the
moderator's privilege

00:49:02.480 --> 00:49:03.530
and ask one more then.

00:49:03.530 --> 00:49:04.840
CHRIS CLEARFIELD: Yes.

00:49:04.840 --> 00:49:05.840
CRAIG: I read your book.

00:49:05.840 --> 00:49:07.840
And one of the things
that was fascinating to me

00:49:07.840 --> 00:49:10.090
is the discussion
of bank failures

00:49:10.090 --> 00:49:11.900
and the diversity
of the boards there.

00:49:11.900 --> 00:49:14.595
I wonder if you could
briefly say something

00:49:14.595 --> 00:49:16.220
about that, because
I think that's very

00:49:16.220 --> 00:49:19.572
relevant to decision-making
at a place like Google.

00:49:19.572 --> 00:49:21.530
CHRIS CLEARFIELD: Yeah,
this is a fascinating--

00:49:21.530 --> 00:49:24.230
totally fascinating study
where these researchers

00:49:24.230 --> 00:49:26.570
looked at the failure
of community banks

00:49:26.570 --> 00:49:28.470
through the financial crisis.

00:49:28.470 --> 00:49:30.675
And what they
found is that banks

00:49:30.675 --> 00:49:32.960
that had more bankers
on their board

00:49:32.960 --> 00:49:35.772
were actually more
likely to fail,

00:49:35.772 --> 00:49:36.980
which is a little surprising.

00:49:36.980 --> 00:49:38.479
Because you would
think that bankers

00:49:38.479 --> 00:49:41.510
should be pretty good
at managing a bank.

00:49:41.510 --> 00:49:44.229
And it turned out it's very
similar to this diversity

00:49:44.229 --> 00:49:45.020
result that we saw.

00:49:45.020 --> 00:49:48.050
That when the board had
lawyers, or nonprofit people,

00:49:48.050 --> 00:49:50.330
or doctors, or
accountants on it,

00:49:50.330 --> 00:49:53.660
there was this freeing
up to ask questions.

00:49:53.660 --> 00:49:55.730
And to say, hey, I
don't understand this.

00:49:55.730 --> 00:49:56.990
Can you all explain this more?

00:49:56.990 --> 00:49:59.540
Or like no, we're not going
to go with this new product

00:49:59.540 --> 00:50:03.290
because we don't
understand it yet.

00:50:03.290 --> 00:50:05.300
It's not like a risk effect.

00:50:05.300 --> 00:50:07.340
It's not that the
banks with more bankers

00:50:07.340 --> 00:50:09.800
were making more money because
they were taking more risks.

00:50:09.800 --> 00:50:11.549
They were making the
same amount of money,

00:50:11.549 --> 00:50:12.672
but just failing more.

00:50:12.672 --> 00:50:13.880
So they had the same returns.

00:50:13.880 --> 00:50:16.160
And I mean, to
your point, I think

00:50:16.160 --> 00:50:18.680
that's an example of two things.

00:50:22.612 --> 00:50:25.070
And I think I can speak to this
because I sort of straddled

00:50:25.070 --> 00:50:26.361
these worlds through my career.

00:50:26.361 --> 00:50:29.900
Like normal people think
differently than engineers

00:50:29.900 --> 00:50:31.260
some of the time.

00:50:31.260 --> 00:50:34.400
And so having a group that
has a mix of backgrounds

00:50:34.400 --> 00:50:37.280
can be really powerful to
getting that kind of-- that

00:50:37.280 --> 00:50:39.980
questioning and that
diverse perspective.

00:50:39.980 --> 00:50:41.780
But also, we should
be thinking about how

00:50:41.780 --> 00:50:45.730
to bring outsiders into some of
these most important decisions.

00:50:45.730 --> 00:50:47.990
I mean, the example
I think about

00:50:47.990 --> 00:50:51.700
is Apple slowed
their iPhone down

00:50:51.700 --> 00:50:53.360
with the new operating system.

00:50:53.360 --> 00:50:55.735
And that was probably a really
good engineering decision.

00:50:55.735 --> 00:50:56.693
It probably made sense.

00:50:56.693 --> 00:50:59.120
They could kind of make the
performance more predictable.

00:50:59.120 --> 00:51:00.380
The phones didn't turn off.

00:51:00.380 --> 00:51:02.570
But that's not how the
public viewed that.

00:51:02.570 --> 00:51:06.864
And I think if Apple had pulled
in somebody off the street--

00:51:06.864 --> 00:51:08.780
well, that's hard in
this area because there's

00:51:08.780 --> 00:51:09.530
so many engineers.

00:51:09.530 --> 00:51:13.070
But pulled in a non-engineer,
they might have said,

00:51:13.070 --> 00:51:13.770
this is crazy.

00:51:13.770 --> 00:51:15.200
This is not a good solution.

00:51:15.200 --> 00:51:17.420
And so I think that
there's an element of that.

00:51:17.420 --> 00:51:20.877
And it's interesting that
surface-level diversity

00:51:20.877 --> 00:51:22.460
and the kind of
professional diversity

00:51:22.460 --> 00:51:25.220
have such a similar
and striking impact.

00:51:28.370 --> 00:51:31.824
CRAIG: We have time
for more questions.

00:51:31.824 --> 00:51:33.990
CHRIS CLEARFIELD: We'll
also be up here for a minute

00:51:33.990 --> 00:51:35.850
if anybody has anything
they want to chat about.

00:51:35.850 --> 00:51:36.558
CRAIG: All right.

00:51:36.558 --> 00:51:38.300
Well, let's thank our speakers.

00:51:38.300 --> 00:51:40.020
And I encourage you
to read the book.

00:51:40.020 --> 00:51:41.120
It is fascinating.

00:51:41.120 --> 00:51:42.030
Thank you again.

00:51:42.030 --> 00:51:42.680
ANDRAS TILCSIK: Thank you.

00:51:42.680 --> 00:51:44.430
CHRIS CLEARFIELD: Thank
you all very much.

00:51:44.430 --> 00:51:46.330
[APPLAUSE]

