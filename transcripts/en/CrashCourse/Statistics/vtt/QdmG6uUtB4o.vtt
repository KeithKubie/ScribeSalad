WEBVTT
Kind: captions
Language: en

00:00:03.060 --> 00:00:06.520
Hi, I’m Adriene Hill, and welcome back to
Crash Course Statistics.

00:00:06.520 --> 00:00:11.560
General Linear Models -- like Regression and ANOVA -- let us create a statistical analysis

00:00:11.570 --> 00:00:13.780
of data for our specific needs.

00:00:13.790 --> 00:00:16.660
Fitting the right model to our experiments
is kind of like Tetris…

00:00:16.660 --> 00:00:19.240
GLMS are in this analogy tetriminos.

00:00:19.240 --> 00:00:23.320
Sometimes you need the skinny-long bric, called the straight sometimes you need the square

00:00:23.320 --> 00:00:25.460
sometimes you need the left snake.

00:00:25.460 --> 00:00:30.260
In stats, its similar sometimes you need regression sometimes ANOVA but there’s also ANCOVA

00:00:30.269 --> 00:00:32.640
---The Analysis of Covariance.

00:00:32.640 --> 00:00:34.839
And the Repeated Measures ANOVA.

00:00:34.839 --> 00:00:37.210
Today we’ll look at the shape of those models.

00:00:37.210 --> 00:00:38.740
And how they might help us level-up!

00:00:38.740 --> 00:00:48.049
INTRO

00:00:48.049 --> 00:00:53.129
As a quick review, in a few of our past episodes we covered the fact that ANOVAs and regressions

00:00:53.129 --> 00:00:55.469
are both General Linear Models.

00:00:55.469 --> 00:01:01.799
ANOVAs allow us to analyze the effect of variables with two or more groups on continuous variables.

00:01:01.800 --> 00:01:05.580
And regressions allow us to analyze two continuous variables.

00:01:05.580 --> 00:01:10.480
General Linear Models explain the data we
observe by building a model to predict that

00:01:10.480 --> 00:01:13.979
data, and then keeping track of how close
the prediction is.

00:01:13.979 --> 00:01:17.530
And both regressions and ANOVAs use a similar model setup.

00:01:17.530 --> 00:01:21.950
It looks just like the equation for a line
that you may have seen if you’ve taken Algebra.

00:01:21.950 --> 00:01:25.440
The fact that they’re set up the exact same
way is helpful for two reasons.

00:01:25.440 --> 00:01:30.250
One, it means we only have to remember one general mode , and two it allows us to combine

00:01:30.250 --> 00:01:34.590
these two powerful models to give us the even more flexible ANCOVA.

00:01:34.590 --> 00:01:38.070
For example, we might want to look at the
amount of general anesthesia needed to put

00:01:38.070 --> 00:01:39.299
a patient under.

00:01:39.299 --> 00:01:44.679
There have been studies that suggest that
redheads require more anesthesia than non-redheads

00:01:44.679 --> 00:01:48.969
because the gene mutation that causes red
hair, also affects pain receptors.

00:01:48.969 --> 00:01:51.359
So we have two groups: redheads and non-redheads.

00:01:51.359 --> 00:01:53.179
Those are categorical variables.

00:01:53.179 --> 00:01:57.499
But, we also think that weight will have a
meaningful impact on the amount of this specific

00:01:57.500 --> 00:01:59.929
anesthetic that’s needed for surgery.

00:01:59.929 --> 00:02:01.780
Weight is a continuous variable.

00:02:01.789 --> 00:02:06.200
To make sure things are relatively equal,
we look at only one kind of simple, routine

00:02:06.210 --> 00:02:08.350
surgery: appendix removal.

00:02:08.350 --> 00:02:12.310
Working with a hospital, we collect data on
100 randomly selected patients.

00:02:12.310 --> 00:02:14.989
50 redheads, and 50 non-redheads.

00:02:14.989 --> 00:02:18.360
We record their weight, natural hair color,
and the amount of anesthesia needed during

00:02:18.360 --> 00:02:19.519
their surgery.

00:02:19.519 --> 00:02:24.959
We can now build a model to predict milliliters of anesthesia based on hair color and weight.

00:02:24.959 --> 00:02:30.180
Just like its friends, regression and ANOVA,
the ANCOVA looks at the overall variation

00:02:30.180 --> 00:02:35.050
in the data, and uses different variables,
like hair color and weight, to explain it.

00:02:35.050 --> 00:02:39.970
The overall variation is, as always, measured by the sum of the squared distances between

00:02:39.970 --> 00:02:45.510
the overall mean amount of anesthesia used, and each dose of anesthesia that was administered.

00:02:45.510 --> 00:02:48.080
This variation is called the Sums of Squares total.

00:02:48.080 --> 00:02:52.769
So now we can calculate an ANOVA table that shows us the sums of squares and F-tests for

00:02:52.769 --> 00:02:54.490
each of our effects.

00:02:54.490 --> 00:02:59.489
Even though this is an ANCOVA model, we still usually refer to these as ANOVA tables.

00:02:59.489 --> 00:03:05.670
And even though this table has both continuous regression factors and categorical ANOVA factors,

00:03:05.670 --> 00:03:08.209
we read it just like it’s a regular ANOVA
table.

00:03:08.209 --> 00:03:12.400
Here we can see that weight is a significant
predictor of how much anesthesia you’ll

00:03:12.400 --> 00:03:17.650
need, but hair color isn’t .it’s really
tempting to call hair color “nearly significant”

00:03:17.650 --> 00:03:20.079
because it’s SO close to 0.05.

00:03:20.079 --> 00:03:21.180
But our cutoff is strict.

00:03:21.180 --> 00:03:23.310
It has to be less than 0.05.

00:03:23.310 --> 00:03:28.230
We now have a tool that allows us to combine categorical and continuous variables into

00:03:28.230 --> 00:03:30.680
one General Linear Model.

00:03:30.680 --> 00:03:33.240
The world as they say is our oyster.

00:03:33.240 --> 00:03:36.439
We can predict all kinds of things with all
kinds of variables.

00:03:36.440 --> 00:03:40.300
We can also use our new ANCOVA models to make stronger inferences.

00:03:40.300 --> 00:03:45.560
In our example,we were interested, mainly,
in whether being a redhead significantly increased

00:03:45.560 --> 00:03:47.500
the dose of a new anesthetic.

00:03:47.500 --> 00:03:51.480
But we also included weight in the model,
since we knew that weight plays a pretty big

00:03:51.480 --> 00:03:53.730
role in how much anesthetic you need.

00:03:53.730 --> 00:03:56.069
Weight accounted for a lot of the variation
in the model.

00:03:56.069 --> 00:04:01.909
Its eta squared is 0.353, which means that
it accounts for about 35% of the variation

00:04:01.909 --> 00:04:02.909
in our data.

00:04:02.909 --> 00:04:04.319
That’s pretty high.

00:04:04.319 --> 00:04:09.730
And since it “soaked up” all of that variation, our Sums of Squares Error is now smaller.

00:04:09.730 --> 00:04:14.280
If we had run a simple ANOVA with JUST hair color, the differences between anesthetic

00:04:14.280 --> 00:04:19.630
doses due to weight would have just been chalked up to “random variation”, or error because

00:04:19.630 --> 00:04:21.780
it’s source--weight--wasn’t in our model.

00:04:21.780 --> 00:04:24.960
For both of these models, the simple case
where we ONLY look at hair color, and the

00:04:24.960 --> 00:04:29.220
more complex case where we look at both hair color and weight, the total variation in the

00:04:29.220 --> 00:04:30.900
data is the same.

00:04:30.900 --> 00:04:32.460
Because it’s the same data.

00:04:32.460 --> 00:04:37.130
Total variation looks only at our outcome
variable--like milliliters of anesthetic.

00:04:37.130 --> 00:04:42.040
So, when we build our models, we’re partitioning the same amount of variation into groups.

00:04:42.040 --> 00:04:47.550
Our simple ANOVA model JUST looks at how much of this total variation is due to being or

00:04:47.550 --> 00:04:49.250
not being a redhead.

00:04:49.250 --> 00:04:53.360
The rest is counted as error, just because
“error” refers to variation that our model

00:04:53.360 --> 00:04:54.470
doesn’t account for.

00:04:54.470 --> 00:04:59.400
When we use the bigger model that includes both hair color and weight, we take some of

00:04:59.400 --> 00:05:03.270
that variation that was attributed to error,
and attribute it to weight instead.

00:05:03.270 --> 00:05:06.310
This makes our pile of error variation smaller.

00:05:06.310 --> 00:05:11.190
For this reason, many researchers will add
covariates--continuous variables that are

00:05:11.190 --> 00:05:16.040
used to explain our outcome variable--not
only for inference, but also to reduce the

00:05:16.040 --> 00:05:17.420
amount of error variation.

00:05:17.420 --> 00:05:19.090
Let’s take another example.

00:05:19.090 --> 00:05:22.880
Say we want to look at the effect of a new
brand of formula on the weight of infants.

00:05:22.880 --> 00:05:27.260
We have two randomly assigned groups of infants: those with our new formula and those who get

00:05:27.260 --> 00:05:29.480
an established brand of formula.

00:05:29.480 --> 00:05:34.660
But infants grow very quickly, so we want
to account for any variation due to age, so

00:05:34.660 --> 00:05:37.030
we include age in days in our model.

00:05:37.030 --> 00:05:43.020
If we just ran a model that included formula type, our Sums of Squares for Error is pretty big.

00:05:43.020 --> 00:05:45.800
And formula doesn’t have a significant effect on infants’ weight.

00:05:45.800 --> 00:05:50.580
But we know that infants weights are strongly
correlated with how old they are, so when

00:05:50.580 --> 00:05:56.060
we include that in a new ANCOVA model, it
takes some of the variation that was error

00:05:56.060 --> 00:06:00.420
variation in our simple model, and accounts for it using age in days.

00:06:00.420 --> 00:06:06.020
As you can see from this ANOVA table, adding age as a covariate allowed us to explain some

00:06:06.020 --> 00:06:10.790
of the variation, while making it easier for
us to detect the fact that there is actually

00:06:10.790 --> 00:06:14.040
a significant effect of formula type on babys’ weights.

00:06:14.040 --> 00:06:16.350
And we’re not limited to just one covariate.

00:06:16.350 --> 00:06:17.530
We can add many, if we want.

00:06:17.530 --> 00:06:22.620
We could add mother’s weight to this ANCOVA, or even another categorical variable, like ethnicity.

00:06:22.660 --> 00:06:26.320
Our models are limited only by our ability
to collect data.

00:06:26.320 --> 00:06:29.540
But we have to be careful when we're using
covariates to do inference.

00:06:29.540 --> 00:06:33.200
There are cases when it makes sense to have a bunch of covariates.

00:06:33.200 --> 00:06:37.550
But if someone is adding a bunch of them just to make their p-values significant, that could

00:06:37.550 --> 00:06:38.880
be considered p-hacking

00:06:38.880 --> 00:06:43.320
And we can continue to customize our model even further so that we’re partitioning

00:06:43.320 --> 00:06:45.480
our variation more accurately.

00:06:45.480 --> 00:06:49.410
Previously, we noted that it’s difficult
to do a statistical test on whether there

00:06:49.410 --> 00:06:53.460
was a significant difference between the mean ratings of two coffee shops.

00:06:53.460 --> 00:06:58.730
That’s because people’s individual coffee
preferences add extra variation to our data.

00:06:58.730 --> 00:07:02.020
People who hate coffee will always rate it
relatively low, and people who love coffee

00:07:02.020 --> 00:07:03.880
will always rate it pretty high.

00:07:03.880 --> 00:07:09.310
In that simple case, we did a matched pairs
t-test in order to “subtract” the variation

00:07:09.310 --> 00:07:12.340
due to people’s different levels of coffee
affinity.

00:07:12.340 --> 00:07:15.760
Essentially, what we were doing was allowing each person to have their own “baseline”

00:07:15.760 --> 00:07:16.990
coffee preference.

00:07:16.990 --> 00:07:20.420
This allowed us to see whether there was a
pattern of one coffee shop getting higher

00:07:20.420 --> 00:07:25.910
ratings than the other, regardless of whether the people who rated it loved, tolerated,

00:07:25.910 --> 00:07:27.270
or hated coffee.

00:07:27.270 --> 00:07:30.470
And we can do that with more than 2 groups as well, using something called a Repeated

00:07:30.470 --> 00:07:31.680
Measures ANOVA.

00:07:31.680 --> 00:07:36.640
A Repeated Measures ANOVA asks whether there’s a significant difference between 2 or more

00:07:36.640 --> 00:07:37.830
groups or conditions.

00:07:37.830 --> 00:07:41.790
The key to an Repeated Measures ANOVA is that the same experimental unit, whether it’s

00:07:41.790 --> 00:07:45.280
a cell, a person, or an animal, is measured
multiple times.

00:07:45.280 --> 00:07:46.280
Hence “Repeated”.

00:07:46.280 --> 00:07:51.070
And in practice, it works pretty similarly
to the match pairs t-test, except it allows

00:07:51.070 --> 00:07:53.330
you to look at more than 2 groups.

00:07:53.330 --> 00:07:57.760
A repeated measures ANOVA lets each experimental unit have its own “baseline”.

00:07:57.760 --> 00:08:02.420
So we could ask whether there’s a significant difference between 10 different coffee shops,

00:08:02.420 --> 00:08:06.940
or whether there’s a significant effect
of slow, medium, and fast tempoed music on

00:08:06.940 --> 00:08:08.350
the speed we run.

00:08:08.350 --> 00:08:09.970
Everyone has a different baseline running
speed.

00:08:09.970 --> 00:08:13.880
Maybe your friend who injured their knee runs pretty slowly, but your cousin can run a 6

00:08:13.880 --> 00:08:14.880
minute mile.

00:08:14.880 --> 00:08:19.360
But it’s still possible to say that on average,
people run faster when a bear is chasing them--whether

00:08:19.360 --> 00:08:20.410
they’re fast or slow.

00:08:20.410 --> 00:08:24.890
We’re looking at data from 150 people, and
we record how fast they can run a mile listening

00:08:24.890 --> 00:08:27.890
to slow, medium and fast tempoed songs.

00:08:27.890 --> 00:08:31.080
We measure them on different days so that
they don’t get too tired after all that

00:08:31.080 --> 00:08:33.149
running (that could affect our data).

00:08:33.149 --> 00:08:38.510
And we make sure to randomize the order of the music so that not everyone gets slow first,

00:08:38.510 --> 00:08:39.860
or medium last.

00:08:39.860 --> 00:08:44.399
If we simple looked at an ANOVA that used
music tempo to predict mile pace there’s

00:08:44.399 --> 00:08:45.500
a lot of variation.

00:08:45.500 --> 00:08:49.639
And when we ran this simple model, the effect of music tempo is non significant.

00:08:49.639 --> 00:08:53.899
That may be due in part to the fact that the
difference between how fast individual people

00:08:53.899 --> 00:08:58.600
normally run is counted in the Error Sums
of Squares, making it a lot bigger.

00:08:58.600 --> 00:09:00.810
(That might not be the only reason, though.)

00:09:00.810 --> 00:09:04.660
So, we tell our model which measurements belong to the same person.

00:09:04.660 --> 00:09:09.839
And then, we tell our model to let each individual person have their own baseline mile time,

00:09:09.839 --> 00:09:15.360
and we’ll just look at how much music tempo affects the changes from people’s baseline

00:09:15.360 --> 00:09:16.709
running speeds.

00:09:16.709 --> 00:09:21.970
So whether you normally run a 5 or 15 minute mile, an increase in 1 minute will be counted

00:09:21.970 --> 00:09:22.970
the same.

00:09:22.970 --> 00:09:27.060
Theoretically, it’s sorta like centering
everyone on their own mean running speed.

00:09:27.060 --> 00:09:30.949
If you normally run a 6 minute mile, that
becomes your 0 baseline.

00:09:30.949 --> 00:09:33.300
Same thing if you normally run a 12 minute
mile.

00:09:33.300 --> 00:09:37.709
Since the math of these models--sometimes called Random Effect Models--can get a little

00:09:37.709 --> 00:09:42.430
intense, we’re just going to focus on how
to read the ANOVA table output from a Repeated

00:09:42.430 --> 00:09:43.430
Measures ANOVA.

00:09:43.430 --> 00:09:48.290
Here, our output shows us that there is actually a significant effect of the music tempo on

00:09:48.290 --> 00:09:49.980
running time.

00:09:49.980 --> 00:09:54.610
Because we allowed everyone to have their
own “baseline” speed, we in essence took

00:09:54.610 --> 00:09:57.750
that variation away, and made our error term smaller.

00:09:57.750 --> 00:10:00.620
We now have the shapes we need to fit all
kinds of situations…

00:10:00.620 --> 00:10:04.910
We can combine categorical and continuous factors, and we know how to handle data where

00:10:04.910 --> 00:10:08.259
the same subject is measured multiple times.

00:10:08.259 --> 00:10:11.160
We can slide these pieces together in all
sorts of ways.

00:10:11.160 --> 00:10:14.410
We can build a model that looks at how the
number of hours of Tetris we play affects

00:10:14.410 --> 00:10:19.360
how far we go in each game and if expertise level effects how long someone plays.

00:10:19.360 --> 00:10:24.620
Or we could add statistical rigour to the
decade long arguments over which Tetris shapes

00:10:24.630 --> 00:10:28.430
are the best (it’s the straight) and the
worst to get.

00:10:28.430 --> 00:10:30.660
Thanks for watching, I’ll see you next time.

