WEBVTT
Kind: captions
Language: en

00:00:02.570 --> 00:00:05.760
- And everyone, hi.

00:00:05.760 --> 00:00:08.460
As Ben just said,
I'm your warm up act.

00:00:08.460 --> 00:00:09.560
Hi.

00:00:09.560 --> 00:00:10.530
I'm Alyssa Goodman.

00:00:10.530 --> 00:00:12.900
I'm one of the co-directors
at Radcliffe Institute

00:00:12.900 --> 00:00:14.120
for Science.

00:00:14.120 --> 00:00:16.140
I'm also a professor in
the Astronomy Department

00:00:16.140 --> 00:00:19.170
and on the Steering Committee
of the Science Initiative

00:00:19.170 --> 00:00:20.520
here at Harvard.

00:00:20.520 --> 00:00:22.830
And those organizations--
not counting

00:00:22.830 --> 00:00:25.710
the astronomy department--
are bringing you along

00:00:25.710 --> 00:00:28.200
with the Institute for Advanced
Computational Science, two

00:00:28.200 --> 00:00:31.410
talks by Ben Schneiderman here
at Harvard, today and tomorrow.

00:00:31.410 --> 00:00:32.630
So you're at the first one.

00:00:32.630 --> 00:00:34.572
Hopefully you're
at the right talk.

00:00:34.572 --> 00:00:36.030
And tomorrow you're
welcome to join

00:00:36.030 --> 00:00:37.710
for the second
one, which I think

00:00:37.710 --> 00:00:40.170
is in the geological classroom.

00:00:40.170 --> 00:00:44.110
The information is right up
there, as well as the topic.

00:00:44.110 --> 00:00:45.642
But what I'm going
to do is first--

00:00:45.642 --> 00:00:46.350
I'm just curious.

00:00:46.350 --> 00:00:48.120
Before you tell me
anything about Ben,

00:00:48.120 --> 00:00:50.700
Ben and I are both
curious who's here.

00:00:50.700 --> 00:00:55.020
So if you are considering
yourself a computer scientist

00:00:55.020 --> 00:00:57.500
or that your first field or
interest is computer science,

00:00:57.500 --> 00:01:00.040
can you raise your hand?

00:01:00.040 --> 00:01:01.140
Thank you.

00:01:01.140 --> 00:01:04.090
If you're from industry,
can you raise your hand?

00:01:04.090 --> 00:01:04.870
A few people.

00:01:04.870 --> 00:01:05.370
Thank you.

00:01:05.370 --> 00:01:08.170
If you're from Harvard,
can you raise your hand?

00:01:08.170 --> 00:01:08.670
OK.

00:01:08.670 --> 00:01:11.520
If you're from not Harvard,
can you raise your hand?

00:01:11.520 --> 00:01:12.030
OK, Ben.

00:01:12.030 --> 00:01:13.320
You know their favorite color?

00:01:13.320 --> 00:01:14.278
What else want to know?

00:01:14.278 --> 00:01:16.470
- I want to know [INAUDIBLE].

00:01:16.470 --> 00:01:17.700
- Ah.

00:01:17.700 --> 00:01:21.870
If you're very interested
in design, raise your hand.

00:01:21.870 --> 00:01:23.080
OK, excellent.

00:01:23.080 --> 00:01:24.690
And what about the humanities?

00:01:24.690 --> 00:01:26.780
Any actual humanists?

00:01:26.780 --> 00:01:28.620
Wonderful, OK.

00:01:28.620 --> 00:01:31.485
Library people?

00:01:31.485 --> 00:01:33.390
Ah, there we go.

00:01:33.390 --> 00:01:34.410
Thank you, James.

00:01:34.410 --> 00:01:35.760
I know his name.

00:01:35.760 --> 00:01:40.511
So he'd like social
scientists next.

00:01:40.511 --> 00:01:41.010
OK.

00:01:41.010 --> 00:01:41.509
Wait.

00:01:41.509 --> 00:01:43.360
Why are you so hesitant
to raise your hand

00:01:43.360 --> 00:01:45.060
when you're a social scientist?

00:01:45.060 --> 00:01:49.530
It's like ooh, it's our fault
if this AI stuff goes wrong.

00:01:49.530 --> 00:01:51.960
Anyway, so let me just tell
you a little bit about Ben--

00:01:51.960 --> 00:01:54.641
unless we really do need to
know their favorite color.

00:01:54.641 --> 00:01:55.140
No?

00:01:55.140 --> 00:01:55.800
No, Ben?

00:01:55.800 --> 00:01:57.570
OK.

00:01:57.570 --> 00:02:02.280
So probably most of you have
heard of Ben, or Ben's work.

00:02:02.280 --> 00:02:04.980
And you should be
very grateful to him

00:02:04.980 --> 00:02:09.000
every time you use your iPhone
or tag someone in a photo.

00:02:09.000 --> 00:02:10.830
And maybe he'll explain
some of his work

00:02:10.830 --> 00:02:14.280
today in the more
design usability

00:02:14.280 --> 00:02:17.560
aspects of the interaction
between humans and computers,

00:02:17.560 --> 00:02:20.400
which is what Ben has been
interested in pretty much

00:02:20.400 --> 00:02:23.050
for most of his career.

00:02:23.050 --> 00:02:24.930
He actually started
out-- right now

00:02:24.930 --> 00:02:28.830
he's a distinguished university
professor at the University

00:02:28.830 --> 00:02:29.490
of Maryland.

00:02:29.490 --> 00:02:32.250
And he started something there--

00:02:32.250 --> 00:02:36.430
the Human Computer
Interaction Lab

00:02:36.430 --> 00:02:38.210
there at the mercy of
Maryland-- something

00:02:38.210 --> 00:02:40.380
like 40 some odd years ago.

00:02:40.380 --> 00:02:42.370
35 years ago.

00:02:42.370 --> 00:02:44.970
But before that, he actually
was trained originally

00:02:44.970 --> 00:02:47.286
as an undergraduate in physics.

00:02:47.286 --> 00:02:49.410
I love the fact that he
also went to Bronx Science,

00:02:49.410 --> 00:02:51.330
because I'm originally
From New York, too.

00:02:51.330 --> 00:02:53.230
So for anybody here,
I should have said

00:02:53.230 --> 00:02:55.080
who here went to Bronx Science?

00:02:55.080 --> 00:02:56.820
Anybody?

00:02:56.820 --> 00:02:58.140
Oh, come on.

00:02:58.140 --> 00:02:59.400
There's usually at least one.

00:02:59.400 --> 00:02:59.970
OK.

00:02:59.970 --> 00:03:03.520
But he did go to City College,
where my dad also went.

00:03:03.520 --> 00:03:05.130
OK, we got a City
College person.

00:03:05.130 --> 00:03:05.770
Excellent.

00:03:05.770 --> 00:03:06.270
OK.

00:03:06.270 --> 00:03:10.920
So we went to City College and
then he went to Stony Brook.

00:03:10.920 --> 00:03:13.800
After getting his bachelor's
in physics at City College

00:03:13.800 --> 00:03:16.350
he got a PhD in
computer science.

00:03:16.350 --> 00:03:19.710
And then he says he
sort of became 20%

00:03:19.710 --> 00:03:23.010
a psychologist, because
he became more interested

00:03:23.010 --> 00:03:24.870
in not just what
computers could do

00:03:24.870 --> 00:03:27.630
but what humans and
computers do together, where

00:03:27.630 --> 00:03:29.355
humans use computers as tools.

00:03:29.355 --> 00:03:29.980
- There you go.

00:03:29.980 --> 00:03:30.480
Right.

00:03:30.480 --> 00:03:33.090
- Don't say that humans
and computers are

00:03:33.090 --> 00:03:34.920
partners when you're with Ben.

00:03:34.920 --> 00:03:36.990
This is just a bad idea.

00:03:36.990 --> 00:03:39.280
If you try it in the
questions, you'll see why.

00:03:39.280 --> 00:03:39.780
OK.

00:03:39.780 --> 00:03:43.860
So anyway, so Ben has done
many, many things in his career

00:03:43.860 --> 00:03:46.920
that led to important
technologies

00:03:46.920 --> 00:03:48.820
that you use in
your everyday life,

00:03:48.820 --> 00:03:53.250
including hypertext, touch
screen computing, tree maps,

00:03:53.250 --> 00:03:55.290
and other kinds
of visualization--

00:03:55.290 --> 00:03:57.630
the names of which
you might not know.

00:03:57.630 --> 00:04:00.150
But Ben and his
colleagues are responsible

00:04:00.150 --> 00:04:04.530
for them making your lives
navigating lots of information

00:04:04.530 --> 00:04:07.745
through computers much easier.

00:04:07.745 --> 00:04:09.870
Tomorrow he's going to talk
about event flow, which

00:04:09.870 --> 00:04:12.750
is one of the technologies
he's worked on more recently.

00:04:12.750 --> 00:04:15.270
And some of you may
know about NodeXL, which

00:04:15.270 --> 00:04:17.525
is a graph visualization tool.

00:04:17.525 --> 00:04:19.649
But today he's not going
to talk about any of that,

00:04:19.649 --> 00:04:21.269
I don't think, or a little bit.

00:04:21.269 --> 00:04:23.269
And so I'm just got to
say one more thing, which

00:04:23.269 --> 00:04:26.100
is Ben has these rules about
human and computer interaction

00:04:26.100 --> 00:04:28.787
usually applied to
visualization-- information

00:04:28.787 --> 00:04:29.370
visualization.

00:04:29.370 --> 00:04:31.260
And I just picked
out one that I want

00:04:31.260 --> 00:04:33.000
to read to you so
that you can keep it

00:04:33.000 --> 00:04:34.560
in mind as you hear Ben speak.

00:04:34.560 --> 00:04:37.140
And you'll see the
connection between design

00:04:37.140 --> 00:04:40.890
and human computer interaction
design and his thoughts

00:04:40.890 --> 00:04:43.810
about artificial intelligence
and machine learning.

00:04:43.810 --> 00:04:46.650
And so rule number seven from
his famous book, "Designing

00:04:46.650 --> 00:04:49.620
the User Interface"," which
first came out in 1986 and is

00:04:49.620 --> 00:04:53.790
in its sixth edition now,
says, "Support internal locus

00:04:53.790 --> 00:04:56.280
of control," and
then that's explained

00:04:56.280 --> 00:04:58.950
as experienced
operators strongly

00:04:58.950 --> 00:05:02.520
desire the sense they are
in charge of the system,

00:05:02.520 --> 00:05:05.745
and that the system
responds to their actions.

00:05:05.745 --> 00:05:09.600
Design the system to make
users the initiators of actions

00:05:09.600 --> 00:05:11.610
rather than the responders.

00:05:11.610 --> 00:05:13.020
So I think we'll
learn today what

00:05:13.020 --> 00:05:15.690
that means in the context
of artificial intelligence,

00:05:15.690 --> 00:05:18.990
and we'll also learn why
an important quote from Ben

00:05:18.990 --> 00:05:21.390
is that machine learning
without visualization

00:05:21.390 --> 00:05:22.914
should be illegal.

00:05:22.914 --> 00:05:24.330
So we'll find out
what that means.

00:05:24.330 --> 00:05:26.000
And thank you very
much coming, Ben.

00:05:26.000 --> 00:05:26.647
- Thank you.

00:05:26.647 --> 00:05:27.146
Thank you.

00:05:27.146 --> 00:05:29.461
[APPLAUSE]

00:05:32.240 --> 00:05:34.130
Thank you for that
nice introduction.

00:05:34.130 --> 00:05:38.570
And you gave half my talk,
so we can move it along.

00:05:38.570 --> 00:05:42.590
I'm very pleased
to be here and once

00:05:42.590 --> 00:05:47.480
we get my slides we will be--

00:05:47.480 --> 00:05:50.920
that's not what we want.

00:05:50.920 --> 00:05:51.670
Yup.

00:05:51.670 --> 00:05:53.161
You want to try that again?

00:05:53.161 --> 00:05:53.660
All right.

00:05:53.660 --> 00:05:54.535
I'll try it that way.

00:05:54.535 --> 00:05:55.160
OK.

00:05:55.160 --> 00:05:58.040
No slide viewer,
straight up slides.

00:05:58.040 --> 00:05:59.024
Good afternoon.

00:05:59.024 --> 00:06:00.190
I'm very pleased to be here.

00:06:00.190 --> 00:06:03.770
I've had an
energizing day talking

00:06:03.770 --> 00:06:06.360
with students and
Radcliffe Fellows

00:06:06.360 --> 00:06:08.240
and visiting around the campus.

00:06:08.240 --> 00:06:13.710
And I'm ready now for a spirited
discussion with you here today.

00:06:13.710 --> 00:06:16.940
So Alissa's already
said I'm happy to be

00:06:16.940 --> 00:06:19.650
from the University of
Maryland and represent

00:06:19.650 --> 00:06:22.670
the 35-year-old Human Computer
Interaction Lab, which

00:06:22.670 --> 00:06:24.770
is a [INAUDIBLE] bunch
between computer science

00:06:24.770 --> 00:06:28.340
and the College of Information
Studies, rapidly growing

00:06:28.340 --> 00:06:29.570
known as the iSchool.

00:06:29.570 --> 00:06:32.160
But also we work with
other groups on campus,

00:06:32.160 --> 00:06:35.120
including the wonderfully
titled Maryland Institute

00:06:35.120 --> 00:06:37.850
for Technology and
the Humanities, MITH.

00:06:37.850 --> 00:06:39.710
There are about
1,000 tech reports

00:06:39.710 --> 00:06:43.830
on our web site, 200 videos,
200 project reports and so on.

00:06:43.830 --> 00:06:45.930
So you can take a
look about that group.

00:06:45.930 --> 00:06:47.620
Alissa's already
mentioned the book,

00:06:47.620 --> 00:06:50.120
"Designing the User Interface."
that's been my calling card.

00:06:50.120 --> 00:06:52.610
I hope some of you have
seen it or used it.

00:06:52.610 --> 00:06:55.850
And it covers some of the key
things and direct manipulation

00:06:55.850 --> 00:06:59.690
theories and the way to
design screens and menus

00:06:59.690 --> 00:07:02.750
and search systems, social
media visualization.

00:07:02.750 --> 00:07:07.460
And Alissa was kind enough to
mention the eight golden rules.

00:07:07.460 --> 00:07:10.490
I was pressured
to reduce the 500

00:07:10.490 --> 00:07:13.770
rules that were in these
large books and handbooks.

00:07:13.770 --> 00:07:16.190
And it was useful 'cause
it forced me to think

00:07:16.190 --> 00:07:17.540
at a little higher level.

00:07:17.540 --> 00:07:20.390
And so there were things
like strive for consistency,

00:07:20.390 --> 00:07:23.790
and prevent errors,
and the one you said,

00:07:23.790 --> 00:07:26.720
which was from an earlier
version of that book--

00:07:26.720 --> 00:07:30.560
which was asserting
internal locus of control.

00:07:30.560 --> 00:07:34.920
I sort of updated it in the
latest version to just simply--

00:07:34.920 --> 00:07:37.850
the locus of control
phrase confused people,

00:07:37.850 --> 00:07:40.460
and so I just focused about
assuring the users in control.

00:07:40.460 --> 00:07:42.710
And you'll see that's going
to come back in the story,

00:07:42.710 --> 00:07:43.490
as well.

00:07:43.490 --> 00:07:45.680
Just where I come
from, as Alissa

00:07:45.680 --> 00:07:48.530
described, from background
doing database and file design

00:07:48.530 --> 00:07:52.610
optimizations and working
my way towards psychology.

00:07:52.610 --> 00:07:57.410
In 1980 I wrote the book
called "Software Psychology"

00:07:57.410 --> 00:08:01.720
and tried to bring these
disciplines closer together.

00:08:01.720 --> 00:08:04.220
And along the way the kind
of-- some design things

00:08:04.220 --> 00:08:05.750
travel really well.

00:08:05.750 --> 00:08:07.560
The idea of links,
that certain words that

00:08:07.560 --> 00:08:09.980
are highlighted in
the sentence-- and we

00:08:09.980 --> 00:08:12.890
chose light blue because
we studied a dozen versions

00:08:12.890 --> 00:08:14.660
and we had all
kinds of research.

00:08:14.660 --> 00:08:17.270
And bright red was
more visible, but it

00:08:17.270 --> 00:08:19.760
undermined people's
comprehension and retention

00:08:19.760 --> 00:08:21.230
of the text they were reading.

00:08:21.230 --> 00:08:23.090
So blue turned out to be good.

00:08:23.090 --> 00:08:24.410
Tim Berners-Lee said oh, yeah.

00:08:24.410 --> 00:08:25.160
That looks good.

00:08:25.160 --> 00:08:27.860
And he cited our
work in his manifesto

00:08:27.860 --> 00:08:29.750
for the web in spring '89.

00:08:29.750 --> 00:08:32.309
So those things
traveled really well.

00:08:32.309 --> 00:08:34.250
The touch screen
keyboard-- early on we

00:08:34.250 --> 00:08:39.980
had done a nine inch wide
keyboard inch square buttons.

00:08:39.980 --> 00:08:44.010
And we did seven and five and
three, and we studied them.

00:08:44.010 --> 00:08:46.910
And yes, it does take a
little longer on a three inch

00:08:46.910 --> 00:08:48.980
wide keyboard, but
people could do it.

00:08:48.980 --> 00:08:52.100
Now the reviewers of the
paper didn't believe it.

00:08:52.100 --> 00:08:54.074
And it took a video
to convince them

00:08:54.074 --> 00:08:55.240
that it was really possible.

00:08:55.240 --> 00:08:57.900
And the strategy was to put
your finger on the screen,

00:08:57.900 --> 00:08:59.932
give a little cursor
just above your finger,

00:08:59.932 --> 00:09:01.390
and then be able
to move it around.

00:09:01.390 --> 00:09:04.640
And that's still what
resides on your iPhone.

00:09:04.640 --> 00:09:08.020
We call that the lift off
strategy, and Steve Jobs--

00:09:08.020 --> 00:09:11.180
I was a consultant for Apple,
and Steve Jobs visited our lab.

00:09:11.180 --> 00:09:14.870
So there again, I see the path
for those things to happen.

00:09:14.870 --> 00:09:16.670
And things like photo
tagging, we did.

00:09:16.670 --> 00:09:19.890
And we got a patent for that,
and that's traveled well also.

00:09:19.890 --> 00:09:22.800
So those are easy stories
to tell a cocktail parties.

00:09:22.800 --> 00:09:24.500
Some of the more
difficult things,

00:09:24.500 --> 00:09:27.920
which I'm proud of as well--
a Spotfire tool that came from

00:09:27.920 --> 00:09:31.850
our work, which had the early
ideas of multiple windows,

00:09:31.850 --> 00:09:34.700
dynamic query sliders
to control ranges,

00:09:34.700 --> 00:09:40.010
rapid incremental and reversible
updates, brushing and linking--

00:09:40.010 --> 00:09:43.100
a lot of those ideas came from
the early developments there.

00:09:43.100 --> 00:09:47.480
There's the Treemaps in a
financial , tool FinViz,

00:09:47.480 --> 00:09:53.535
a commercial one where it shows
all the stocks in this S&amp;P 500.

00:09:53.535 --> 00:09:57.410
The green stocks are rising,
the red stocks are falling.

00:09:57.410 --> 00:10:00.350
The area indicates the
market capitalization.

00:10:00.350 --> 00:10:03.650
And so you can see Apple
is the biggest one.

00:10:03.650 --> 00:10:07.280
And there's Google and
Facebook and ExxonMobil

00:10:07.280 --> 00:10:10.570
and whatever-- all these
other different groups.

00:10:10.570 --> 00:10:14.390
And it's a pretty good
day in this example here,

00:10:14.390 --> 00:10:15.990
but there are some red ones.

00:10:15.990 --> 00:10:21.320
And you can see as a field the
utility sector is doing poorly.

00:10:21.320 --> 00:10:24.710
And often that sector travels
in the opposite direction

00:10:24.710 --> 00:10:26.670
of the general stock market.

00:10:26.670 --> 00:10:30.080
NodeXL was a network
visualization tool

00:10:30.080 --> 00:10:32.040
with 600,000 downloads.

00:10:32.040 --> 00:10:36.660
And we're working on the
second edition of that book.

00:10:36.660 --> 00:10:38.230
And this was an early--

00:10:38.230 --> 00:10:42.060
on the first visualization of
the polarization in Congress.

00:10:42.060 --> 00:10:45.210
These were the red Republicans.

00:10:45.210 --> 00:10:46.350
I've lost my cursor.

00:10:46.350 --> 00:10:47.840
Come on.

00:10:47.840 --> 00:10:48.787
Can you see that?

00:10:48.787 --> 00:10:49.620
Yes, you can see it.

00:10:49.620 --> 00:10:51.060
I'll try this way.

00:10:51.060 --> 00:10:53.714
The red Republicans
and the blue Democrats,

00:10:53.714 --> 00:10:54.630
the two Independents--

00:10:54.630 --> 00:10:55.620
Sanders and Lieberman.

00:10:55.620 --> 00:10:57.600
This was about 2004.

00:10:57.600 --> 00:11:01.620
And then the three crossovers
are Specter, Collins and Snow.

00:11:01.620 --> 00:11:04.980
And that proved to be
predictive, if you will--

00:11:04.980 --> 00:11:08.130
that's a word near and dear
to Alissa-- but predictive

00:11:08.130 --> 00:11:11.490
of the three Republicans
who would vote for Obama's

00:11:11.490 --> 00:11:14.250
reconstruction rebuilding plan.

00:11:14.250 --> 00:11:18.090
So that's the kind of
data sets we were after.

00:11:18.090 --> 00:11:20.340
It's still available
in a free version,

00:11:20.340 --> 00:11:22.620
but also a paid version.

00:11:22.620 --> 00:11:25.050
And currently EventFlow,
and that's tomorrow's talk,

00:11:25.050 --> 00:11:27.817
is about for looking at
electronic health records.

00:11:27.817 --> 00:11:29.400
If you have a million
patients and you

00:11:29.400 --> 00:11:31.680
want to look at the common
patterns of treatments

00:11:31.680 --> 00:11:35.640
and outcomes, this is the
way we hope you'll do it.

00:11:35.640 --> 00:11:36.740
So take a look.

00:11:36.740 --> 00:11:40.110
And Megan Monroe is
one of the key workers

00:11:40.110 --> 00:11:44.910
on developing EventFlow into
the powerful tool that it is.

00:11:44.910 --> 00:11:48.920
Take a look at her videos
on the EventFlow web site.

00:11:48.920 --> 00:11:52.260
My favorite one is the one about
her analysis of a basketball

00:11:52.260 --> 00:11:55.350
game, of every shot
and pass turnover.

00:11:55.350 --> 00:11:58.860
And she captures in a
charming and brilliant way

00:11:58.860 --> 00:12:01.920
how the game turned out
in a way that was not

00:12:01.920 --> 00:12:04.320
visible from other statistics.

00:12:04.320 --> 00:12:06.740
So this talk is about big data.

00:12:06.740 --> 00:12:07.620
OK?

00:12:07.620 --> 00:12:13.410
And big data is often described
as a flood, as a torrent,

00:12:13.410 --> 00:12:17.670
as a tidal wave that's
drowning in data.

00:12:17.670 --> 00:12:24.300
All these negative views of the
way data is overwhelming us.

00:12:24.300 --> 00:12:26.580
For me though, data
is really good.

00:12:26.580 --> 00:12:29.010
It's the sweet
irrigation from which

00:12:29.010 --> 00:12:33.760
we will forge the new ideas that
we need to make our world still

00:12:33.760 --> 00:12:34.260
better.

00:12:34.260 --> 00:12:36.710
So I'm here to
change the metaphors

00:12:36.710 --> 00:12:40.080
and to push us down a
positive road that says

00:12:40.080 --> 00:12:42.490
big data is great opportunity.

00:12:42.490 --> 00:12:45.440
And now we have to figure
out how to put it to work--

00:12:45.440 --> 00:12:48.000
how to put it to work
responsibly and safely.

00:12:48.000 --> 00:12:49.900
And those are going
to be the key themes.

00:12:49.900 --> 00:12:52.620
So I go back to the
original big data initiative

00:12:52.620 --> 00:12:53.800
that Obama launched.

00:12:53.800 --> 00:12:57.550
It's now 2012--
so six years ago.

00:12:57.550 --> 00:12:59.730
In this three page
White House announcement

00:12:59.730 --> 00:13:03.370
there were two challenges--
developing scalable algorithms

00:13:03.370 --> 00:13:06.120
for processing imperfect data
in distributed data spores.

00:13:06.120 --> 00:13:06.850
Quite nice.

00:13:06.850 --> 00:13:09.930
That's the machine learning
and algorithmic approach

00:13:09.930 --> 00:13:11.320
and doing all those things.

00:13:11.320 --> 00:13:14.070
However, number two
or equally the other

00:13:14.070 --> 00:13:17.070
challenge-- creating effective
human computer interaction

00:13:17.070 --> 00:13:18.390
tools.

00:13:18.390 --> 00:13:18.930
Pretty good.

00:13:18.930 --> 00:13:20.680
I had some influence
on this document

00:13:20.680 --> 00:13:22.320
so my words get in there, too.

00:13:22.320 --> 00:13:24.780
For facilitating
rapidly customizable--

00:13:24.780 --> 00:13:26.340
and I like this
phrase, this was not

00:13:26.340 --> 00:13:29.640
mine-- but visual reasoning
for diverse missions.

00:13:29.640 --> 00:13:32.670
So I want to remind
you that while we

00:13:32.670 --> 00:13:35.040
may be talking about
technology solutions here,

00:13:35.040 --> 00:13:38.560
we also have to at the same
time talk about human solutions.

00:13:38.560 --> 00:13:40.290
That's where we're going.

00:13:40.290 --> 00:13:43.020
The White House repeated
the big data strategies

00:13:43.020 --> 00:13:44.222
a couple of years ago.

00:13:44.222 --> 00:13:45.930
There's too much to
read here, but I want

00:13:45.930 --> 00:13:48.390
to focus on one piece of this--

00:13:48.390 --> 00:13:51.990
support R&amp;D trustworthiness
of data and the resulting

00:13:51.990 --> 00:13:52.740
decisions.

00:13:52.740 --> 00:13:53.490
OK?

00:13:53.490 --> 00:13:55.890
And they use this
phrasing, which I like--

00:13:55.890 --> 00:13:58.650
to promote transparency,
including tools--

00:13:58.650 --> 00:13:59.910
they got it again--

00:13:59.910 --> 00:14:02.160
that provide detailed audits--

00:14:02.160 --> 00:14:07.620
I like that-- to show the steps
that led to a specific action.

00:14:07.620 --> 00:14:08.160
OK?

00:14:08.160 --> 00:14:12.210
It's pretty clear and
straightforward about what

00:14:12.210 --> 00:14:16.290
the White House was asking
for in terms of building

00:14:16.290 --> 00:14:18.310
the right kinds of technology.

00:14:18.310 --> 00:14:21.330
Now I had addressed this issue,
and again thanks to Alyssa

00:14:21.330 --> 00:14:22.830
for mentioning the book.

00:14:22.830 --> 00:14:25.237
Originally in 1986,
in there, there

00:14:25.237 --> 00:14:31.080
was a section, which was titled
"Balancing Automation and Human

00:14:31.080 --> 00:14:32.200
Control".

00:14:32.200 --> 00:14:35.190
And it was built on Tom
Sheridan, MIT researcher who

00:14:35.190 --> 00:14:40.320
had cataloged 10 levels of the
movement from human control

00:14:40.320 --> 00:14:43.530
to full automation that
had machine control.

00:14:43.530 --> 00:14:47.160
And that vision has
remained the stable part.

00:14:47.160 --> 00:14:48.960
It's changed in some ways.

00:14:48.960 --> 00:14:50.730
In fact in the
driverless car world

00:14:50.730 --> 00:14:53.160
there are five levels
of automation now.

00:14:53.160 --> 00:14:57.300
And so that way of thinking
continues to dominate.

00:14:57.300 --> 00:15:00.510
However, I began to
diverge from that.

00:15:00.510 --> 00:15:04.380
And I just didn't like
the idea of partnership--

00:15:04.380 --> 00:15:07.590
of working together
or a balance.

00:15:07.590 --> 00:15:09.900
So in the sixth
edition, that section--

00:15:09.900 --> 00:15:12.690
you can read that and
get the full story.

00:15:12.690 --> 00:15:18.480
It says ensuring human control
while increasing automation.

00:15:18.480 --> 00:15:21.850
Now that may sound like a bit
of a mystery, like a [INAUDIBLE]

00:15:21.850 --> 00:15:25.260
on some kind that you
need to meditate on.

00:15:25.260 --> 00:15:29.040
But that's where I've come
to stand quite strongly.

00:15:29.040 --> 00:15:31.140
I think we're going
you want to be

00:15:31.140 --> 00:15:34.410
clearer and clearer that
the humans are in control

00:15:34.410 --> 00:15:36.420
and therefore the word
responsibility will

00:15:36.420 --> 00:15:37.560
come into play.

00:15:37.560 --> 00:15:39.420
And the way we get
it is by getting

00:15:39.420 --> 00:15:43.150
increased, but the right
kinds, of automation.

00:15:43.150 --> 00:15:44.010
OK.

00:15:44.010 --> 00:15:45.426
So that's where
we're going to go.

00:15:45.426 --> 00:15:47.610
Now these ideas have some merit.

00:15:47.610 --> 00:15:52.350
And I just want to remind
you that the notions of user

00:15:52.350 --> 00:15:52.870
control--

00:15:52.870 --> 00:15:54.870
and thank you for mentioning
that was originally

00:15:54.870 --> 00:15:56.730
in my list--

00:15:56.730 --> 00:15:59.640
but Apple and the
Microsoft guidelines

00:15:59.640 --> 00:16:02.010
still say the same thing.

00:16:02.010 --> 00:16:07.170
That they stipulate-- this is
Apple design for their products

00:16:07.170 --> 00:16:13.020
and the iOS apps-- people,
not apps, are in control.

00:16:13.020 --> 00:16:13.800
OK?

00:16:13.800 --> 00:16:15.830
Flexibility-- whoops.

00:16:15.830 --> 00:16:17.370
Come back here.

00:16:17.370 --> 00:16:21.980
Flexibility-- users
complete-- flexibility--

00:16:21.980 --> 00:16:25.413
the users complete fine-grained
control over their work.

00:16:25.413 --> 00:16:26.260
OK?

00:16:26.260 --> 00:16:28.590
That's also a pretty
clear statement.

00:16:28.590 --> 00:16:32.490
So this is quite at odds
what you see, and much

00:16:32.490 --> 00:16:34.680
of the literature and
descriptions about AI

00:16:34.680 --> 00:16:38.250
and machine learnings and
machines are taking our jobs

00:16:38.250 --> 00:16:42.190
and the rise of the robots
in a variety of other ways.

00:16:42.190 --> 00:16:44.410
And so that's where I
want to take you here.

00:16:44.410 --> 00:16:47.040
I hope to convince
you by the end

00:16:47.040 --> 00:16:50.100
that this is a fresh and
important way to think,

00:16:50.100 --> 00:16:54.030
and we need to change the
language of discussion

00:16:54.030 --> 00:16:57.630
and the way we go about
building new technologies if we

00:16:57.630 --> 00:16:59.270
want to build safety.

00:16:59.270 --> 00:17:01.260
That's really our goal.

00:17:01.260 --> 00:17:05.160
Again there's other people,
other voices that I join with.

00:17:05.160 --> 00:17:09.839
Cathy O'Neil's excellent book,
"Weapons of Math Destruction",

00:17:09.839 --> 00:17:14.210
describes the dangers of
opacity, of black boxes,

00:17:14.210 --> 00:17:18.960
of when algorithms are used at
scale and they produce damage.

00:17:18.960 --> 00:17:22.319
These algorithms slam doors in
the face of millions of people,

00:17:22.319 --> 00:17:26.220
often for the flimsiest of
reasons, and offer no appeal.

00:17:26.220 --> 00:17:28.170
And she has chapter
after chapter

00:17:28.170 --> 00:17:30.480
that talks about how
teachers were fired,

00:17:30.480 --> 00:17:32.880
how loan applicants
were rejected,

00:17:32.880 --> 00:17:38.070
how prisoners requesting
parole were rejected from that,

00:17:38.070 --> 00:17:38.940
and on and on.

00:17:38.940 --> 00:17:40.680
Medical applications and so on.

00:17:40.680 --> 00:17:43.410
So she has written
this in a very--

00:17:43.410 --> 00:17:47.030
she's a Harvard
grad, by the way,

00:17:47.030 --> 00:17:50.340
and worked on Wall Street
and PhD mathematician.

00:17:50.340 --> 00:17:54.300
But she's written a
wonderfully compelling book

00:17:54.300 --> 00:17:56.070
that makes the case.

00:17:56.070 --> 00:17:58.980
And her point is
that algorithms can

00:17:58.980 --> 00:18:01.870
be biased, harmful and deadly.

00:18:01.870 --> 00:18:02.370
OK?

00:18:02.370 --> 00:18:04.512
I guess this has become
common knowledge.

00:18:04.512 --> 00:18:06.720
I hope everybody in the room
has seen these arguments

00:18:06.720 --> 00:18:10.050
and appreciate that
there is tension.

00:18:10.050 --> 00:18:11.760
In preparing for
this talk I went out

00:18:11.760 --> 00:18:15.360
to captured the covers
from recent reports.

00:18:15.360 --> 00:18:17.920
My favorite, if you're going
to read one, is this one--

00:18:17.920 --> 00:18:19.080
"Ethically Aligned Design".

00:18:19.080 --> 00:18:20.760
This one, I really like.

00:18:20.760 --> 00:18:25.060
It's about 200 authors from
20 committees within IEEE.

00:18:25.060 --> 00:18:28.420
It presents fresh language,
fresh thinking, practical ways

00:18:28.420 --> 00:18:28.920
forward.

00:18:28.920 --> 00:18:30.210
That's the one I love.

00:18:30.210 --> 00:18:33.870
I wrote a blog for IEEE, a
sort of love letter for them.

00:18:33.870 --> 00:18:36.690
The bad guys are over here.

00:18:36.690 --> 00:18:39.840
Now this report, which
is a little older now,

00:18:39.840 --> 00:18:42.660
does suggest there's nothing
different from people

00:18:42.660 --> 00:18:44.010
and machines.

00:18:44.010 --> 00:18:47.520
It's just a degree of
the speed of performance.

00:18:47.520 --> 00:18:49.740
Human minds and computers
are quite the same.

00:18:49.740 --> 00:18:53.160
So I didn't quite like that one.

00:18:53.160 --> 00:18:57.030
This is an interesting one that
focuses on malicious use of AI

00:18:57.030 --> 00:18:59.940
and forecasting prevention
and mitigation from people

00:18:59.940 --> 00:19:03.420
that we all respect like
Electronic Frontier Foundation,

00:19:03.420 --> 00:19:05.790
Cambridge and Oxford
Universities, so on,

00:19:05.790 --> 00:19:06.930
Open AI Group.

00:19:06.930 --> 00:19:10.330
And this one from the world
wide web foundation on algorithm

00:19:10.330 --> 00:19:12.660
accountability-- a
nicely written report

00:19:12.660 --> 00:19:14.160
that focuses on the web.

00:19:14.160 --> 00:19:15.640
And there's more.

00:19:15.640 --> 00:19:17.310
This is a recent one--

00:19:17.310 --> 00:19:20.790
European Group on ethics in
science and new technologies,

00:19:20.790 --> 00:19:22.410
artificial
intelligence, robotics

00:19:22.410 --> 00:19:24.000
and autonomous systems.

00:19:24.000 --> 00:19:26.070
And this also just
a couple of weeks

00:19:26.070 --> 00:19:29.250
ago upturns a Washington
based think tank

00:19:29.250 --> 00:19:31.350
and supported by
[INAUDIBLE] Network

00:19:31.350 --> 00:19:34.290
and then the New
York based AINOW NYU.

00:19:34.290 --> 00:19:36.450
Kate Crawford and
Meredith Whitaker

00:19:36.450 --> 00:19:39.300
are key players
in here, but they

00:19:39.300 --> 00:19:42.585
follow ideas that are
close to what I'm saying.

00:19:42.585 --> 00:19:45.630
And so that's one
I'm sympathetic to,

00:19:45.630 --> 00:19:47.400
and it's just
recently released--

00:19:47.400 --> 00:19:50.070
practical framework for
public agency accountability.

00:19:50.070 --> 00:19:52.969
So this is a topic
that's in the air.

00:19:52.969 --> 00:19:55.260
And I enjoyed the discussion
with the Radcliffe Fellows

00:19:55.260 --> 00:19:56.430
this afternoon.

00:19:56.430 --> 00:20:00.240
And I hope many of you
are aware of these issues.

00:20:00.240 --> 00:20:04.920
My pitch is responsibility
is the key to understanding,

00:20:04.920 --> 00:20:08.760
and particularly
responsibility for failures.

00:20:08.760 --> 00:20:10.940
So that's the model
I'm going to build on,

00:20:10.940 --> 00:20:14.110
and there's going to be two
other components to it as well.

00:20:14.110 --> 00:20:18.020
Now an important player
for this report--

00:20:18.020 --> 00:20:19.230
don't read all this--

00:20:19.230 --> 00:20:23.700
but is the two page report from
the USACM, which I'm a member

00:20:23.700 --> 00:20:26.280
and I was part of
this committee, which

00:20:26.280 --> 00:20:29.250
is the public policy group of
the Association for Computing

00:20:29.250 --> 00:20:32.680
Machinery, the main professional
group with 100,000 members.

00:20:32.680 --> 00:20:34.990
And they got
together and prepared

00:20:34.990 --> 00:20:38.600
this report, which was then
endorsed by the European branch

00:20:38.600 --> 00:20:39.100
as well.

00:20:39.100 --> 00:20:41.683
And it has seven principles, and
we're going to look at those.

00:20:41.683 --> 00:20:42.490
OK?

00:20:42.490 --> 00:20:45.260
And I would say
this is a good one.

00:20:45.260 --> 00:20:46.210
It's good to read.

00:20:46.210 --> 00:20:50.630
It's just a page and a half,
so it's a very readable thing.

00:20:50.630 --> 00:20:53.200
And it has seven principles.

00:20:53.200 --> 00:20:56.140
My criticism-- and
in the short time

00:20:56.140 --> 00:20:58.450
we had to push this
report forward,

00:20:58.450 --> 00:21:01.780
I couldn't get the
specificity that I wanted.

00:21:01.780 --> 00:21:04.220
But they're on the right track.

00:21:04.220 --> 00:21:07.720
So awareness-- owners,
designers, builders and users

00:21:07.720 --> 00:21:10.270
of stakeholders should be
aware of the possible biases

00:21:10.270 --> 00:21:11.200
and harms.

00:21:11.200 --> 00:21:14.690
There actually should
be a zero level rule,

00:21:14.690 --> 00:21:16.360
which is the
awareness that there

00:21:16.360 --> 00:21:20.290
is a system, an AI
system being used

00:21:20.290 --> 00:21:21.740
in some of these environments.

00:21:21.740 --> 00:21:24.880
In fact, Nick
Diakopoulos has begun

00:21:24.880 --> 00:21:26.410
to collect-- there's
a website which

00:21:26.410 --> 00:21:29.800
he lists all government,
as much as he can find,

00:21:29.800 --> 00:21:34.150
government systems which have
AI systems built into them,

00:21:34.150 --> 00:21:36.230
which have potentials for bias.

00:21:36.230 --> 00:21:39.820
So we should be aware that
things are being done,

00:21:39.820 --> 00:21:44.560
and then we should be aware of
the possible biases and harms.

00:21:44.560 --> 00:21:48.570
But I was critical of shoulds.

00:21:48.570 --> 00:21:51.930
When you see shoulds in a
statement-- should be aware--

00:21:51.930 --> 00:21:54.630
how do you know if
someone's aware?

00:21:54.630 --> 00:21:55.740
And who should be aware?

00:21:55.740 --> 00:21:56.880
Who certifies this?

00:21:56.880 --> 00:21:58.260
How do you validate that?

00:21:58.260 --> 00:22:00.450
Similarly, access and redress.

00:22:00.450 --> 00:22:04.020
Regulators should
adopt mechanisms that

00:22:04.020 --> 00:22:06.480
enable questioning and redress.

00:22:06.480 --> 00:22:07.980
Which regulators?

00:22:07.980 --> 00:22:11.010
How do you get a question asked?

00:22:11.010 --> 00:22:12.480
What's the redress?

00:22:12.480 --> 00:22:13.770
What's going on here?

00:22:13.770 --> 00:22:14.460
OK?

00:22:14.460 --> 00:22:17.280
A good principle, but
it's a bit high level.

00:22:17.280 --> 00:22:20.220
And we need to make
it more specific.

00:22:20.220 --> 00:22:22.020
Accountability.

00:22:22.020 --> 00:22:25.770
Institutions should
be responsible,

00:22:25.770 --> 00:22:28.020
even if it is not
feasible to explain

00:22:28.020 --> 00:22:30.300
how algorithms
produce the result.

00:22:30.300 --> 00:22:32.880
So who holds them responsible?

00:22:32.880 --> 00:22:35.700
And we're well past the time--

00:22:35.700 --> 00:22:40.290
and this proposal is a year old.

00:22:40.290 --> 00:22:43.440
And I think the movement
has gone well past--

00:22:43.440 --> 00:22:45.090
everybody says
yes, this is right.

00:22:45.090 --> 00:22:46.600
But now how do we do it?

00:22:46.600 --> 00:22:48.630
And so that's really the
substance of discussion

00:22:48.630 --> 00:22:50.640
that I hope to contribute to.

00:22:50.640 --> 00:22:53.580
Four, explanation--
another hot topic.

00:22:53.580 --> 00:22:56.760
Systems and institutions
that use algorithmic decision

00:22:56.760 --> 00:22:59.760
making are encouraged--

00:22:59.760 --> 00:23:01.620
are encouraged--
oh, sure they're

00:23:01.620 --> 00:23:04.950
encouraged to produce
explanations of the procedures

00:23:04.950 --> 00:23:06.850
and decisions.

00:23:06.850 --> 00:23:08.940
How do they do that?

00:23:08.940 --> 00:23:10.440
And when do they do that?

00:23:10.440 --> 00:23:12.210
And where do they publish that?

00:23:12.210 --> 00:23:14.400
And who can see it?

00:23:14.400 --> 00:23:18.129
So I think there's a lot that
needs to be resolved here.

00:23:18.129 --> 00:23:19.920
There are industry
committees or government

00:23:19.920 --> 00:23:22.080
agencies around
the world and are

00:23:22.080 --> 00:23:24.360
pushing towards this happening.

00:23:24.360 --> 00:23:28.060
Explanation is a particularly
key issue in Europe.

00:23:28.060 --> 00:23:32.910
The GDPR, General
Data Protection Board,

00:23:32.910 --> 00:23:35.670
will start on May 25th--

00:23:35.670 --> 00:23:39.960
will require that companies
give explanations to people

00:23:39.960 --> 00:23:41.710
who feel they've been harmed.

00:23:41.710 --> 00:23:43.800
Now it was well-intentioned.

00:23:43.800 --> 00:23:44.850
It's a good idea.

00:23:44.850 --> 00:23:46.560
I support the GDPR.

00:23:46.560 --> 00:23:49.830
However, it was
pretty vague about how

00:23:49.830 --> 00:23:50.890
that's going to happen.

00:23:50.890 --> 00:23:55.320
So the appearance of that
will change things in Europe,

00:23:55.320 --> 00:23:58.330
and I hope eventually
move to this country.

00:23:58.330 --> 00:24:00.650
Fifth, data provenance.

00:24:00.650 --> 00:24:04.830
Algorithm builders should
maintain a description of how

00:24:04.830 --> 00:24:07.350
draining data was collected.

00:24:07.350 --> 00:24:09.420
Where do they keep
this information?

00:24:09.420 --> 00:24:11.040
Who gets to see it?

00:24:11.040 --> 00:24:12.967
Is it open?

00:24:12.967 --> 00:24:13.800
We don't quite know.

00:24:13.800 --> 00:24:18.660
What's the format
of this information?

00:24:18.660 --> 00:24:20.070
We don't yet know.

00:24:20.070 --> 00:24:23.071
Again, I repeat, I'm
happy with this report

00:24:23.071 --> 00:24:25.320
but there's still work to
be done on any one of these.

00:24:25.320 --> 00:24:27.810
If you're working in these
areas and you contribute

00:24:27.810 --> 00:24:29.490
to any one of
these seven, you'll

00:24:29.490 --> 00:24:31.910
be doing something valuable.

00:24:31.910 --> 00:24:36.630
Auditibility-- Models,
algorithms, data and decisions

00:24:36.630 --> 00:24:39.580
should be recorded so
they can be audited.

00:24:39.580 --> 00:24:41.220
I think you know what
I'm going to say.

00:24:41.220 --> 00:24:43.470
I mean, what do
we mean by should?

00:24:43.470 --> 00:24:44.880
Where do they get recorded?

00:24:44.880 --> 00:24:46.620
What form do they
get recorded in?

00:24:46.620 --> 00:24:48.120
Is there a standard way?

00:24:48.120 --> 00:24:50.850
Is it a machine readable form?

00:24:50.850 --> 00:24:54.030
And what if we
don't like the way

00:24:54.030 --> 00:24:56.700
the audit report
has been prepared?

00:24:56.700 --> 00:24:58.560
So those are the questions.

00:24:58.560 --> 00:25:00.250
Validation and testing.

00:25:00.250 --> 00:25:03.420
Institutions should
use rigorous methods

00:25:03.420 --> 00:25:04.830
to validate their models.

00:25:04.830 --> 00:25:06.600
Which rigorous methods?

00:25:06.600 --> 00:25:08.370
How is rigorous enough?

00:25:08.370 --> 00:25:09.150
OK?

00:25:09.150 --> 00:25:13.050
And I will tell you that while
there are those who say we

00:25:13.050 --> 00:25:16.290
don't want to have any
regulations that will limit

00:25:16.290 --> 00:25:20.400
innovation-- it's sort of a
current mantra of many people--

00:25:20.400 --> 00:25:23.340
the fact is many of the
people in the companies

00:25:23.340 --> 00:25:26.940
I speak to actually
want a clear message.

00:25:26.940 --> 00:25:29.010
They want to know,
what should I be

00:25:29.010 --> 00:25:33.460
doing as a responsible company
in order to do the right thing.

00:25:33.460 --> 00:25:34.710
How far do I have to go?

00:25:34.710 --> 00:25:39.660
What will make me be seen as
a responsible corporation,

00:25:39.660 --> 00:25:43.590
and that when things go bad
the public and the insurance

00:25:43.590 --> 00:25:46.500
companies will say you did OK?

00:25:46.500 --> 00:25:51.880
And will not fault them
for being insufficient.

00:25:51.880 --> 00:25:55.500
So I think the insurance
companies will be our friends

00:25:55.500 --> 00:25:58.800
in this domain, as they have
been in many other areas

00:25:58.800 --> 00:26:03.750
like the structure of
buildings and housing,

00:26:03.750 --> 00:26:07.350
that they'll provide insurance
if you have flood protection,

00:26:07.350 --> 00:26:09.870
if you've used the right kind
of construction materials,

00:26:09.870 --> 00:26:10.850
and so on.

00:26:10.850 --> 00:26:11.550
OK.

00:26:11.550 --> 00:26:13.920
So I come back and
just remind you

00:26:13.920 --> 00:26:16.260
the sort of mantra I
want to wash over you,

00:26:16.260 --> 00:26:18.240
because changing minds is hard.

00:26:18.240 --> 00:26:22.410
I just want you to remember,
we want to ensure human control

00:26:22.410 --> 00:26:24.460
while increasing automation.

00:26:24.460 --> 00:26:25.580
OK.

00:26:25.580 --> 00:26:28.470
And my mechanism for
going about doing this

00:26:28.470 --> 00:26:31.950
is a well practiced
method, which

00:26:31.950 --> 00:26:35.040
we will have to adapt
to new situation

00:26:35.040 --> 00:26:36.840
and its independent oversight.

00:26:36.840 --> 00:26:39.930
I became involved and
studied 40 different systems

00:26:39.930 --> 00:26:44.370
of independent oversight, and
developed a little taxonomy,

00:26:44.370 --> 00:26:47.340
which is what I
want to get to here,

00:26:47.340 --> 00:26:52.320
and see how then we can apply
this in the case of algorithm

00:26:52.320 --> 00:26:54.000
design.

00:26:54.000 --> 00:26:57.030
So there are independent
oversight mechanisms.

00:26:57.030 --> 00:27:00.030
Every company that registers
with the Securities

00:27:00.030 --> 00:27:03.780
and Exchange Commission
must have an internal audit

00:27:03.780 --> 00:27:08.610
committee and advisory
boards and external audits.

00:27:08.610 --> 00:27:10.740
And we kind of accept that.

00:27:10.740 --> 00:27:13.110
There's more than
10,000 companies

00:27:13.110 --> 00:27:14.910
which go through this
process every year

00:27:14.910 --> 00:27:18.270
and file their reports with
the Securities and Exchange

00:27:18.270 --> 00:27:19.380
Commission.

00:27:19.380 --> 00:27:22.860
And there are rules about how
these external audits get done.

00:27:22.860 --> 00:27:26.220
The big accounting
companies make a good amount

00:27:26.220 --> 00:27:29.190
of money on this, but
hopefully they provide value

00:27:29.190 --> 00:27:31.680
because they give the
kind of stamp of approval

00:27:31.680 --> 00:27:34.680
that says you're
doing the right thing.

00:27:34.680 --> 00:27:38.220
And that makes it better
hopefully for all of us.

00:27:38.220 --> 00:27:41.100
Now there are certain rules
that have been practiced

00:27:41.100 --> 00:27:44.220
over the year that says
look, the external auditor

00:27:44.220 --> 00:27:47.340
has the right to see
this and this data

00:27:47.340 --> 00:27:48.940
and not to see that data.

00:27:48.940 --> 00:27:51.660
There's also criteria
about what happens

00:27:51.660 --> 00:27:53.850
to the report of the
external auditor.

00:27:53.850 --> 00:27:56.850
It's not very useful if you
have a external auditor writes

00:27:56.850 --> 00:27:59.040
a report and nothing happens.

00:27:59.040 --> 00:28:01.860
So there's an expectation
that within a year

00:28:01.860 --> 00:28:05.550
these companies will follow
up on the recommendations

00:28:05.550 --> 00:28:10.110
of that report so that when
next year's report is done

00:28:10.110 --> 00:28:13.260
there can be some feedback
and validation that this

00:28:13.260 --> 00:28:14.540
is going OK.

00:28:14.540 --> 00:28:16.260
All right?

00:28:16.260 --> 00:28:20.700
University accreditation,
NSF, and British EPSRC

00:28:20.700 --> 00:28:24.540
advisory boards are all other
forms of independent oversight.

00:28:24.540 --> 00:28:28.950
And of course, tenure committees
are another form of oversight.

00:28:28.950 --> 00:28:32.070
And the right level of
independence in each of these

00:28:32.070 --> 00:28:33.259
is important.

00:28:33.259 --> 00:28:34.800
For committees there
should be people

00:28:34.800 --> 00:28:37.650
who know the discipline
well enough, but not

00:28:37.650 --> 00:28:40.830
so close that they're completely
submerged in it, or not

00:28:40.830 --> 00:28:41.790
so close.

00:28:41.790 --> 00:28:46.530
Same thing when the Federal
Reserve has oversight of banks.

00:28:46.530 --> 00:28:48.810
They want people who
know the banking industry

00:28:48.810 --> 00:28:51.450
and maybe know the company
fairly well, but not

00:28:51.450 --> 00:28:53.520
so close that they're
friends with the people who

00:28:53.520 --> 00:28:55.500
are the leaders of the company.

00:28:55.500 --> 00:28:59.490
There's rules about subpoena
power and investigation

00:28:59.490 --> 00:29:02.130
and reporting and what
can be made public.

00:29:02.130 --> 00:29:06.180
So these are all well exercised,
including many of these other

00:29:06.180 --> 00:29:09.690
technologically centered
organizations like NASA,

00:29:09.690 --> 00:29:11.850
the FAA, Food and
Drug Administration,

00:29:11.850 --> 00:29:14.550
Department of Home-- all
of them have some forms

00:29:14.550 --> 00:29:16.530
of independent oversight.

00:29:16.530 --> 00:29:20.790
And these committees
evolve their strategies

00:29:20.790 --> 00:29:23.650
for doing their investigations.

00:29:23.650 --> 00:29:26.770
So the National
Transportation Safety Board's

00:29:26.770 --> 00:29:31.530
may be the most I think,
respected and effective

00:29:31.530 --> 00:29:32.280
of these.

00:29:32.280 --> 00:29:34.080
And we all know about them.

00:29:34.080 --> 00:29:38.820
And I'm talking about a national
algorithms safety board.

00:29:38.820 --> 00:29:39.390
OK?

00:29:39.390 --> 00:29:42.330
So the Transportation
Safety Board

00:29:42.330 --> 00:29:46.810
did come in for the Tesla
car crash a year ago,

00:29:46.810 --> 00:29:48.660
which killed the driver.

00:29:48.660 --> 00:29:52.380
And if you read their 60 page
report, it's quite nicely done.

00:29:52.380 --> 00:29:54.300
I was quite impressed, actually.

00:29:54.300 --> 00:29:56.970
And it describes all
the circumstances

00:29:56.970 --> 00:29:59.820
and then has a set of
conclusions and then

00:29:59.820 --> 00:30:00.880
recommendations.

00:30:00.880 --> 00:30:02.670
Now among their
recommendations was

00:30:02.670 --> 00:30:05.370
one that's near and
dear to my heart, which

00:30:05.370 --> 00:30:09.270
was careful logging
of what happens.

00:30:09.270 --> 00:30:12.120
And it turns out
that Tesla did not

00:30:12.120 --> 00:30:17.430
have the kind of logs
that would enable

00:30:17.430 --> 00:30:21.610
the outside investigators to
understand what's going on.

00:30:21.610 --> 00:30:22.110
OK?

00:30:22.110 --> 00:30:27.060
Now the model again, is
the flight safety recorder,

00:30:27.060 --> 00:30:29.220
flight data recorder
on airplanes.

00:30:29.220 --> 00:30:31.570
When a triple 7
crosses the Atlantic,

00:30:31.570 --> 00:30:34.880
there's about a petabyte
of data generated.

00:30:34.880 --> 00:30:37.020
And that's a huge
amount of data.

00:30:37.020 --> 00:30:40.590
But there's been an
industry agreement about how

00:30:40.590 --> 00:30:42.360
to structure that data.

00:30:42.360 --> 00:30:44.490
And there are
software tools that

00:30:44.490 --> 00:30:48.160
let the analysts
understand what's going on.

00:30:48.160 --> 00:30:51.636
And I think the development
of those two strategies

00:30:51.636 --> 00:30:53.010
will be one of
the things we want

00:30:53.010 --> 00:30:57.840
to see for AI robotic
systems of the future--

00:30:57.840 --> 00:31:02.520
that we need to have a careful
logging that's been decided on.

00:31:02.520 --> 00:31:04.350
Maybe it's different
from medical devices.

00:31:04.350 --> 00:31:05.970
Maybe it's different
for aircraft.

00:31:05.970 --> 00:31:08.850
Maybe it's different for
financial transactions.

00:31:08.850 --> 00:31:12.060
Maybe it's different
for business decision

00:31:12.060 --> 00:31:13.910
making systems.

00:31:13.910 --> 00:31:17.040
And there'll be
increasing levels of care

00:31:17.040 --> 00:31:20.130
when we moved from discretionary
systems, lightweight things

00:31:20.130 --> 00:31:23.370
like recommenders,
to consequential

00:31:23.370 --> 00:31:28.860
things like mortgage financing,
hiring of teachers, parole

00:31:28.860 --> 00:31:32.520
and so on or financial
trading, and then things that

00:31:32.520 --> 00:31:35.650
are life critical applications
like transportation,

00:31:35.650 --> 00:31:37.390
medical devices and so on.

00:31:37.390 --> 00:31:41.610
So we're going to need to get
more nuanced in understanding

00:31:41.610 --> 00:31:44.710
what to capture in
each of these cases.

00:31:44.710 --> 00:31:48.510
And while the NTSB
does it fine, I

00:31:48.510 --> 00:31:50.790
think, because of
their experience

00:31:50.790 --> 00:31:55.100
with the Tesla crash and the
recent Uber pedestrian death--

00:31:55.100 --> 00:31:58.140
they're still investigating
that of course.

00:31:58.140 --> 00:32:03.060
I think broadening it out to
algorithm safety board would

00:32:03.060 --> 00:32:10.590
get a cadre of investigators
who come to know what to look

00:32:10.590 --> 00:32:12.330
for when disaster occurs.

00:32:15.650 --> 00:32:19.970
So I'm proposing three
kinds of oversight.

00:32:19.970 --> 00:32:22.520
And the first one is
planning oversight.

00:32:22.520 --> 00:32:24.170
That's in advance.

00:32:24.170 --> 00:32:28.250
If you're going to launch
a robotic or AI system,

00:32:28.250 --> 00:32:32.740
and then like a zoning
board where you come forward

00:32:32.740 --> 00:32:34.770
and you say I want
to build a house,

00:32:34.770 --> 00:32:37.490
and here's my plan for building
this house or this store

00:32:37.490 --> 00:32:40.350
or office building--

00:32:40.350 --> 00:32:44.900
and the zoning board reviews
this and then gives you

00:32:44.900 --> 00:32:46.320
approval to go build it.

00:32:46.320 --> 00:32:50.360
And then when you're done the
zoning inspector comes back,

00:32:50.360 --> 00:32:52.610
checks out that you
did it, and gives you

00:32:52.610 --> 00:32:54.500
a certificate of occupancy.

00:32:54.500 --> 00:32:59.120
And over time that process
of discussion in advance

00:32:59.120 --> 00:33:03.050
leads to a wise set of
guidelines about fire rules,

00:33:03.050 --> 00:33:05.480
that you can't use
asbestos, that you

00:33:05.480 --> 00:33:07.730
have to use certain kind
of construction materials,

00:33:07.730 --> 00:33:10.100
you need structurally
strong, depending

00:33:10.100 --> 00:33:12.800
if you may need a two story
or a 12 story building.

00:33:12.800 --> 00:33:15.910
And all those things
become open discussions

00:33:15.910 --> 00:33:19.040
where everybody knows
what the code is

00:33:19.040 --> 00:33:22.520
and what the code will be
for these robotic systems

00:33:22.520 --> 00:33:23.180
will emerge.

00:33:23.180 --> 00:33:25.550
Again, I think
industry specific.

00:33:25.550 --> 00:33:29.750
And I think that's a nice
kind of adversarial approach

00:33:29.750 --> 00:33:31.440
where people discuss--
someone says,

00:33:31.440 --> 00:33:32.750
I want to build this building.

00:33:32.750 --> 00:33:35.749
And someone may say well,
we're your neighbors.

00:33:35.749 --> 00:33:37.790
And we don't think you
should build this building

00:33:37.790 --> 00:33:43.230
because it's going to cause
subsidence or floods or so on.

00:33:43.230 --> 00:33:46.090
It's going to block the
light of our building.

00:33:46.090 --> 00:33:49.940
Or others who say we're
speaking for the fire department

00:33:49.940 --> 00:33:53.990
and we just can't feel that
we can have our fire fighters

00:33:53.990 --> 00:33:55.820
go into this building
because it's not

00:33:55.820 --> 00:33:58.610
built according to the
strength and safety we want.

00:33:58.610 --> 00:34:01.490
Or these days, earthquake
resistance, flood resistance

00:34:01.490 --> 00:34:02.740
or other issues.

00:34:02.740 --> 00:34:04.250
And we'll come to
understand where

00:34:04.250 --> 00:34:08.020
the dangers are in different
biases of machine learning

00:34:08.020 --> 00:34:09.409
systems.

00:34:09.409 --> 00:34:13.350
Second approach-- and this
is the more expensive one--

00:34:13.350 --> 00:34:15.889
so I'm less pushing on this one.

00:34:15.889 --> 00:34:17.960
But this is the
Federal Reserve Board.

00:34:17.960 --> 00:34:20.210
And they do
continuous monitoring

00:34:20.210 --> 00:34:22.100
as does the Food and
Drug Administration

00:34:22.100 --> 00:34:24.800
for pharmaceuticals
and meat packing

00:34:24.800 --> 00:34:27.889
and food packing
and distribution

00:34:27.889 --> 00:34:28.940
and other circuits.

00:34:28.940 --> 00:34:33.110
These require someone to be
there and watch what's going on

00:34:33.110 --> 00:34:36.310
and ask the right questions
and keep it going.

00:34:36.310 --> 00:34:36.889
OK?

00:34:36.889 --> 00:34:39.830
So continuous
monitoring is expensive

00:34:39.830 --> 00:34:43.100
and it results in things
like the Federal Reserve

00:34:43.100 --> 00:34:48.139
coming one day and saying
looks like you denied loans

00:34:48.139 --> 00:34:50.330
to 27 black women yesterday.

00:34:50.330 --> 00:34:51.739
What's going on?

00:34:51.739 --> 00:34:53.060
And is there a reason for it?

00:34:53.060 --> 00:34:55.940
And then the bank says you
know, we had no idea about that.

00:34:55.940 --> 00:34:57.180
Why did that happen?

00:34:57.180 --> 00:34:58.440
Let's go check it out.

00:34:58.440 --> 00:34:59.910
We don't want to do that.

00:34:59.910 --> 00:35:05.300
And so the monitors can be
an asset for the companies,

00:35:05.300 --> 00:35:06.170
as well.

00:35:06.170 --> 00:35:08.540
Doesn't always work
as wonderfully,

00:35:08.540 --> 00:35:11.390
and sometimes things go
bad even with monitors.

00:35:11.390 --> 00:35:14.120
But the evidence is very
strong that this kind

00:35:14.120 --> 00:35:16.770
of continuous monitoring
is very effective.

00:35:16.770 --> 00:35:19.520
Again, very expensive
but very effect.

00:35:19.520 --> 00:35:23.780
The third is probably
the clearest case,

00:35:23.780 --> 00:35:26.300
is the retrospective
analysis idea.

00:35:26.300 --> 00:35:31.070
When something bad goes on--
when a Tesla car kills somebody

00:35:31.070 --> 00:35:35.540
or Uber runs over a pedestrian--

00:35:35.540 --> 00:35:38.870
somebody shows up who
knows how to investigate

00:35:38.870 --> 00:35:41.450
and has access to the data.

00:35:41.450 --> 00:35:43.640
And this again, has
to be independent.

00:35:43.640 --> 00:35:48.000
It can't be that Tesla or
Uber say we'll check it out.

00:35:48.000 --> 00:35:49.040
We'll do our own study.

00:35:49.040 --> 00:35:49.800
No.

00:35:49.800 --> 00:35:53.900
I mean, I'm advocating the idea
that independent oversight has

00:35:53.900 --> 00:35:58.130
value and you want to
create a certain tension.

00:35:58.130 --> 00:36:00.080
And you want to
develop the strategies

00:36:00.080 --> 00:36:04.844
by which the
analysts can come in

00:36:04.844 --> 00:36:06.260
and where the
company says no, you

00:36:06.260 --> 00:36:08.000
can't have that part of our--

00:36:08.000 --> 00:36:10.610
that's our protected
company information.

00:36:10.610 --> 00:36:12.170
But you can have
this information.

00:36:12.170 --> 00:36:13.880
That's what we've all agreed to.

00:36:13.880 --> 00:36:16.970
So here's my little model of it.

00:36:16.970 --> 00:36:18.620
And this really gets
me to most of what

00:36:18.620 --> 00:36:20.330
I want to accomplish here--

00:36:20.330 --> 00:36:24.140
the idea that there
are social strategies

00:36:24.140 --> 00:36:29.030
that can produce the kind
of technical solutions.

00:36:29.030 --> 00:36:31.280
Now the technical solutions,
I've suggested already.

00:36:31.280 --> 00:36:34.340
The first thing I
want is appropriate

00:36:34.340 --> 00:36:37.280
monitoring and logging
of what happens.

00:36:37.280 --> 00:36:38.000
OK?

00:36:38.000 --> 00:36:41.510
The second thing I want
is a proper modular design

00:36:41.510 --> 00:36:45.530
of algorithms so that analysts
can understand what's going on.

00:36:45.530 --> 00:36:49.250
I would say the model for
this is debugging tools, where

00:36:49.250 --> 00:36:51.770
you don't just print
out everything,

00:36:51.770 --> 00:36:55.160
but you have certain
breakpoints in the program.

00:36:55.160 --> 00:36:56.870
The program is
modularly designed

00:36:56.870 --> 00:37:00.260
so that you can track where
the problem is and you

00:37:00.260 --> 00:37:02.210
can say with authority--

00:37:02.210 --> 00:37:05.420
you can say with
authority we fixed it.

00:37:05.420 --> 00:37:08.611
That problem won't happen again.

00:37:08.611 --> 00:37:09.110
OK?

00:37:09.110 --> 00:37:12.740
And that's how I think
airline systems have become

00:37:12.740 --> 00:37:15.300
such safe and good
models of what

00:37:15.300 --> 00:37:18.960
we want to build for
manufacturing robotic systems.

00:37:18.960 --> 00:37:23.190
Apparently there are 33 deaths
by robot manufacturing systems

00:37:23.190 --> 00:37:23.770
in the US.

00:37:23.770 --> 00:37:26.280
And from what I understand,
only two of them

00:37:26.280 --> 00:37:29.610
are fully understood
as to what went wrong.

00:37:29.610 --> 00:37:31.680
And that seems
frightening to me.

00:37:31.680 --> 00:37:37.650
So I'm here to raise awareness
and promote an approach, which

00:37:37.650 --> 00:37:40.680
will provide the social,
political, or maybe

00:37:40.680 --> 00:37:46.406
legal mechanisms that will drive
the right technical solutions.

00:37:46.406 --> 00:37:47.280
OK?

00:37:47.280 --> 00:37:51.360
And these issues about
degree of independence,

00:37:51.360 --> 00:37:53.400
subpoena power I
mentioned, and the power

00:37:53.400 --> 00:37:55.650
to enforce
recommendations-- now,

00:37:55.650 --> 00:37:58.080
you can get my short
paper about this

00:37:58.080 --> 00:38:00.960
in the proceedings of the
National Academy of Sciences,

00:38:00.960 --> 00:38:03.060
from which this figure is taken.

00:38:03.060 --> 00:38:05.860
So I come back
one more time here

00:38:05.860 --> 00:38:10.620
to say ensuring human
control while increasing

00:38:10.620 --> 00:38:14.100
the level of automation.

00:38:14.100 --> 00:38:18.780
We have many ways in which we
have highly automated systems,

00:38:18.780 --> 00:38:19.980
but we feel in control.

00:38:19.980 --> 00:38:22.252
When you step into an
elevator and you press seven,

00:38:22.252 --> 00:38:25.350
the light goes on that says
you're going to seventh floor.

00:38:25.350 --> 00:38:27.150
And you see are on
the third floor now,

00:38:27.150 --> 00:38:29.340
and you get to
the seventh floor.

00:38:29.340 --> 00:38:32.190
And the systems are
comprehensible at the level

00:38:32.190 --> 00:38:32.870
you want.

00:38:32.870 --> 00:38:34.500
They are predictable.

00:38:34.500 --> 00:38:37.310
You press seven, you get
to the seventh floor.

00:38:37.310 --> 00:38:40.600
And they're comprehensible,
predictable and controllable.

00:38:40.600 --> 00:38:41.100
OK?

00:38:41.100 --> 00:38:42.840
You can get where
you want to get.

00:38:42.840 --> 00:38:46.060
Now they're not
completely in control.

00:38:46.060 --> 00:38:48.060
Is there a limitation
of what you can do?

00:38:48.060 --> 00:38:52.020
But within the cognitive model
of the users of an elevator

00:38:52.020 --> 00:38:54.780
they have a sense of
comprehensible, predictable and

00:38:54.780 --> 00:38:55.830
controllable.

00:38:55.830 --> 00:38:59.010
And sometimes you see new
elevators, which don't quite

00:38:59.010 --> 00:39:00.300
have all these features.

00:39:00.300 --> 00:39:03.259
There are ones that don't
display the floor you're on

00:39:03.259 --> 00:39:04.050
or you're going to.

00:39:04.050 --> 00:39:08.230
They're kind of scary and
you feel out of control.

00:39:08.230 --> 00:39:10.080
And I would say this
is the phenomena that

00:39:10.080 --> 00:39:12.990
happens with
airplanes, which have

00:39:12.990 --> 00:39:15.960
excessive degrees of
automation in the auto pilot.

00:39:15.960 --> 00:39:17.970
The multiple modes
that the planes

00:39:17.970 --> 00:39:21.600
can be in confuse the
pilots and the stories

00:39:21.600 --> 00:39:23.760
that I've read carefully
one after another

00:39:23.760 --> 00:39:28.950
are kind of frightening,
that the heading of the plane

00:39:28.950 --> 00:39:31.770
is not shown in a apparent way.

00:39:31.770 --> 00:39:34.740
The state of them the
autopilot does not

00:39:34.740 --> 00:39:38.010
show that it's in
landing mode two and then

00:39:38.010 --> 00:39:40.490
switches to aborting
the landing.

00:39:40.490 --> 00:39:45.420
In one Airbus crash
in Thailand, the plane

00:39:45.420 --> 00:39:49.320
decided it was going to
abort and began to turn out.

00:39:49.320 --> 00:39:52.470
The pilot pushed the
stick to go further down.

00:39:52.470 --> 00:39:54.480
The plane turned
to go further up.

00:39:54.480 --> 00:39:56.490
The pilot pushed the
stick to go down.

00:39:56.490 --> 00:39:58.290
The plane went
further up, stalled,

00:39:58.290 --> 00:39:59.960
and everyone was killed.

00:39:59.960 --> 00:40:00.570
OK?

00:40:00.570 --> 00:40:04.260
Completely unnecessary
kind of outcome.

00:40:04.260 --> 00:40:06.420
If the engineers
and designers had

00:40:06.420 --> 00:40:09.720
been more thoughtful
about giving information

00:40:09.720 --> 00:40:12.240
and gave the status
to the pilots,

00:40:12.240 --> 00:40:13.860
they might have avoided that.

00:40:13.860 --> 00:40:17.610
So the solution that I advocate
is National Algorithm Safety

00:40:17.610 --> 00:40:20.290
Board, and I just close by--

00:40:20.290 --> 00:40:22.710
and happy to have questions--

00:40:22.710 --> 00:40:27.300
to design for safety the goal
is to clarify responsibility.

00:40:27.300 --> 00:40:30.030
It's especially clear in
clarifying responsibility

00:40:30.030 --> 00:40:31.350
for failures.

00:40:31.350 --> 00:40:33.810
And my approaches are
independent oversight

00:40:33.810 --> 00:40:36.720
where there's open and
adversarial reviews,

00:40:36.720 --> 00:40:39.060
transparency-- it
opens the black box,

00:40:39.060 --> 00:40:43.110
accountability for open failure
reporting and liability.

00:40:43.110 --> 00:40:48.060
So currently many systems
have hold harmless contracts.

00:40:48.060 --> 00:40:53.360
Medical systems, for example,
that in most software systems

00:40:53.360 --> 00:40:56.520
the authors of
those software tools

00:40:56.520 --> 00:40:58.290
do not accept responsibility.

00:40:58.290 --> 00:41:02.160
They say software is a
difficult and unpredictable art,

00:41:02.160 --> 00:41:05.100
and if the software
has a bug or goes wrong

00:41:05.100 --> 00:41:08.100
we take no no
responsibility for it.

00:41:08.100 --> 00:41:10.110
That's kind of an
adolescent approach.

00:41:10.110 --> 00:41:11.940
And I think it's time
to move the computing

00:41:11.940 --> 00:41:14.970
field beyond its
adolescent stage

00:41:14.970 --> 00:41:18.420
and for it to accept serious
forms of responsibility.

00:41:18.420 --> 00:41:21.380
So I close with-- you may
have you may have seen this

00:41:21.380 --> 00:41:23.760
television commercial,
hotel Trivago--

00:41:23.760 --> 00:41:26.670
this paired associate,
hotel with Trivago.

00:41:26.670 --> 00:41:28.710
So what I want you to
take away from this

00:41:28.710 --> 00:41:34.380
is safety, responsibility,
safety, responsibility, safety,

00:41:34.380 --> 00:41:35.650
responsibility.

00:41:35.650 --> 00:41:37.325
Those are the ways I
think we'll get it.

00:41:37.325 --> 00:41:40.810
If you want safety,
which I think we all do,

00:41:40.810 --> 00:41:42.660
then you're going
to have to address

00:41:42.660 --> 00:41:44.070
the fact of responsibility.

00:41:44.070 --> 00:41:47.310
And it troubles me
that in many situations

00:41:47.310 --> 00:41:51.120
that is unresolved-- especially
the driverless car situation.

00:41:51.120 --> 00:41:53.130
Who pays for damages?

00:41:53.130 --> 00:41:55.050
And I think the
early clarification

00:41:55.050 --> 00:41:58.290
of that will
accelerate the movement

00:41:58.290 --> 00:42:02.760
towards driverless cars, which
I see as a virtuous possibility.

00:42:02.760 --> 00:42:04.030
And I want to see it happen.

00:42:04.030 --> 00:42:06.750
I want to see it happen in a
responsible and positive way.

00:42:06.750 --> 00:42:08.130
So there we are again--

00:42:08.130 --> 00:42:10.890
safety, responsibility.

00:42:10.890 --> 00:42:12.570
OK, thank you.

00:42:12.570 --> 00:42:15.450
[APPLAUSE]

00:42:19.770 --> 00:42:21.240
- Thank you so much, Ben.

00:42:21.240 --> 00:42:25.840
So we have time for questions
and we can get started.

00:42:25.840 --> 00:42:27.420
We have people with
microphones, so

00:42:27.420 --> 00:42:28.878
if you would just
put your hand way

00:42:28.878 --> 00:42:30.050
up so they can see you and--

00:42:30.050 --> 00:42:31.466
gentlemen the
yellow jacket there?

00:42:36.170 --> 00:42:40.314
- It seems that you assume that
you can derive an explanation

00:42:40.314 --> 00:42:41.730
either by building
a system that's

00:42:41.730 --> 00:42:44.130
going to have
explanation built into it

00:42:44.130 --> 00:42:47.520
or that you can do the
forensics after the fact.

00:42:47.520 --> 00:42:50.640
But it seems that we're
at a point in design

00:42:50.640 --> 00:42:54.180
based on large amounts of data
where you will no longer be

00:42:54.180 --> 00:42:56.970
able to do the forensics.

00:42:56.970 --> 00:43:01.290
Why did the driverless car not
interpret the bicycle walking

00:43:01.290 --> 00:43:02.990
route across the street?

00:43:02.990 --> 00:43:05.640
And so I think there's a
problem with the assumption

00:43:05.640 --> 00:43:09.130
that you'll be able to do
the NTSB kind of analysis.

00:43:09.130 --> 00:43:11.550
And I think you're going to
have to go to a direction

00:43:11.550 --> 00:43:14.400
where there's going to
be a authority who'll

00:43:14.400 --> 00:43:19.370
validate an algorithm, held
out data, cross validated

00:43:19.370 --> 00:43:20.580
and certified in some way.

00:43:20.580 --> 00:43:23.760
Because I don't know how you'll
do the forensics when there's

00:43:23.760 --> 00:43:26.550
a black box with 100,000
parameters and some machine

00:43:26.550 --> 00:43:31.200
learning system that says
the answer was black.

00:43:31.200 --> 00:43:33.540
- I would say if it's
that much out of control,

00:43:33.540 --> 00:43:37.710
if it's that poorly
designed, shut it down.

00:43:37.710 --> 00:43:38.310
OK?

00:43:38.310 --> 00:43:40.890
Now I know the counter
is, well would you rather

00:43:40.890 --> 00:43:45.090
have a safe system or one that
you have an explanation that's

00:43:45.090 --> 00:43:45.880
not as safe?

00:43:45.880 --> 00:43:47.550
I don't think that's a choice.

00:43:47.550 --> 00:43:50.340
I think the choice is to
get a safe system, which

00:43:50.340 --> 00:43:51.180
is explainable.

00:43:51.180 --> 00:43:53.160
I think the engineers
are up to it.

00:43:53.160 --> 00:43:59.160
My optimism is guided by
the FAT ML conference--

00:43:59.160 --> 00:44:02.220
fair, accountable and
transparent machine

00:44:02.220 --> 00:44:04.590
learning-- four years
of pretty interesting

00:44:04.590 --> 00:44:06.120
work in that community.

00:44:06.120 --> 00:44:10.890
I'm encouraged by the DRPA
XAI, Explainable AI program.

00:44:10.890 --> 00:44:13.770
I think what we
need to point out

00:44:13.770 --> 00:44:18.000
is that the past history
of some workers in AI

00:44:18.000 --> 00:44:20.710
have been driven by
game-like applications,

00:44:20.710 --> 00:44:23.700
whether it's go or chess
or other kinds of things,

00:44:23.700 --> 00:44:25.731
or things that were
not consequential.

00:44:25.731 --> 00:44:26.230
OK?

00:44:26.230 --> 00:44:28.620
They're valuable-- natural
language translation,

00:44:28.620 --> 00:44:29.700
speech recognition.

00:44:29.700 --> 00:44:33.030
But when we move towards
consequential things

00:44:33.030 --> 00:44:36.540
I think we should have higher
expectations from the engineers

00:44:36.540 --> 00:44:38.350
who built these systems.

00:44:38.350 --> 00:44:39.840
I think that's
where we're going.

00:44:39.840 --> 00:44:43.560
If they want to be
contributors to these important

00:44:43.560 --> 00:44:45.410
consequential
applications they've

00:44:45.410 --> 00:44:48.909
got to get their act
together or shut them down.

00:44:48.909 --> 00:44:50.700
- And some sort of
government certification

00:44:50.700 --> 00:44:51.866
that these driverless cars--

00:44:51.866 --> 00:44:54.570
- Well, I was not too pushing
hard about government.

00:44:54.570 --> 00:44:57.060
These days a government
certification--

00:44:57.060 --> 00:44:59.910
there is I repeat
strong industry support.

00:44:59.910 --> 00:45:03.840
The partnership for AI is the
leading five top companies

00:45:03.840 --> 00:45:07.890
who are pushing for some
form that will develop,

00:45:07.890 --> 00:45:10.260
that would give them
the assurance they're

00:45:10.260 --> 00:45:11.400
doing the right thing.

00:45:11.400 --> 00:45:13.640
And I would say
I don't buy this.

00:45:13.640 --> 00:45:16.270
And I get angry-- excuse
me-- of the argument

00:45:16.270 --> 00:45:17.520
from some of the AI community.

00:45:17.520 --> 00:45:22.260
100,000 features, parameters,
100 level neural networks.

00:45:22.260 --> 00:45:24.210
How can I possibly explain it?

00:45:24.210 --> 00:45:27.510
If you can't have a way of
saying on the next time when

00:45:27.510 --> 00:45:32.455
I see the same data I'm going
to get it right, shut it down.

00:45:32.455 --> 00:45:34.080
I mean, I think that
would be the test.

00:45:34.080 --> 00:45:38.160
If when something goes bad and
you've captured the log of what

00:45:38.160 --> 00:45:40.950
happened, fix the program.

00:45:40.950 --> 00:45:44.290
Replay it and show me that the
same thing does not reoccur.

00:45:44.290 --> 00:45:44.790
OK?

00:45:44.790 --> 00:45:47.070
I think that's the kind of test.

00:45:47.070 --> 00:45:47.957
Thank you.

00:45:47.957 --> 00:45:49.270
- We have a question over here.

00:45:49.270 --> 00:45:49.910
- OK.

00:45:49.910 --> 00:45:50.120
- OK.

00:45:50.120 --> 00:45:50.750
Yeah.

00:45:50.750 --> 00:45:51.297
- Hi.

00:45:51.297 --> 00:45:53.130
I think my question is
actually thematically

00:45:53.130 --> 00:45:55.680
similar to the
previous question.

00:45:55.680 --> 00:45:58.290
But for your recommended
interventions

00:45:58.290 --> 00:46:02.400
here for governance, you lean
on different mental models

00:46:02.400 --> 00:46:06.840
of regulation and oversight like
monitoring and data sharing,

00:46:06.840 --> 00:46:09.660
like post-hoc
analysis and so on.

00:46:09.660 --> 00:46:13.440
Do you think the issue of
algorithmic accountability

00:46:13.440 --> 00:46:17.100
can be regulated
using these existing

00:46:17.100 --> 00:46:20.100
mental models for this
kind of monitoring,

00:46:20.100 --> 00:46:22.020
or do we need new ones?

00:46:22.020 --> 00:46:26.100
In part because of phenomena
like complex interactions

00:46:26.100 --> 00:46:32.220
and like the difficulty
with getting input A,

00:46:32.220 --> 00:46:34.650
output B every single
time in these systems?

00:46:34.650 --> 00:46:37.200
Do we need a mental
model that accommodates

00:46:37.200 --> 00:46:40.980
fuzzier kinds of outputs or
a probabilistic reasoning

00:46:40.980 --> 00:46:43.500
or something that looks
at interaction effects?

00:46:43.500 --> 00:46:45.690
- I'm getting closer
to what you want.

00:46:45.690 --> 00:46:48.670
I'm not sure exactly what
you're meaning by mental models.

00:46:48.670 --> 00:46:52.680
But I think mental models are
a useful way to describe it.

00:46:52.680 --> 00:46:56.185
My mental model in an elevator
is I press the seventh floor

00:46:56.185 --> 00:46:57.060
and it gets me there.

00:46:57.060 --> 00:47:00.490
I don't understand
all that's going on,

00:47:00.490 --> 00:47:03.330
but I trust that there's
a reliable system that

00:47:03.330 --> 00:47:04.350
will get me there.

00:47:04.350 --> 00:47:06.360
I also see in the
elevator that it

00:47:06.360 --> 00:47:08.670
has an inspection certificate--

00:47:08.670 --> 00:47:11.970
that someone's checked to see
that this elevator functions

00:47:11.970 --> 00:47:12.720
as promised.

00:47:12.720 --> 00:47:16.050
I don't expect every
consumer or citizen

00:47:16.050 --> 00:47:18.270
to be completely
knowledgeable about it.

00:47:18.270 --> 00:47:21.630
But I think the mechanisms by
which various intermediaries,

00:47:21.630 --> 00:47:24.900
consumer groups, public
interest groups and so on

00:47:24.900 --> 00:47:26.550
can act in that way.

00:47:26.550 --> 00:47:29.010
There's a role for
Underwriters Laboratory,

00:47:29.010 --> 00:47:32.610
for Consumer Reports, for
better Business Bureau--

00:47:32.610 --> 00:47:35.460
all these traditional
groups that helped

00:47:35.460 --> 00:47:38.820
accelerate commerce
by building trust.

00:47:38.820 --> 00:47:42.140
We need to invoke them
in new ways, maybe.

00:47:42.140 --> 00:47:45.950
And they will guide us, I
think, to get the right idea.

00:47:45.950 --> 00:47:48.660
I think, again, some
of the AI people

00:47:48.660 --> 00:47:51.420
say the only way to
control AI is of more AI.

00:47:51.420 --> 00:47:53.020
And I don't agree.

00:47:53.020 --> 00:47:54.780
I think the only
way to control AI

00:47:54.780 --> 00:47:58.140
is to have a social mechanism,
a political mechanism by which

00:47:58.140 --> 00:48:01.680
there is an open discussion
and which failures

00:48:01.680 --> 00:48:03.790
can be investigated openly.

00:48:03.790 --> 00:48:04.290
All right?

00:48:04.290 --> 00:48:05.180
Thank you.

00:48:05.180 --> 00:48:05.880
- So over here.

00:48:05.880 --> 00:48:07.130
- Yes, thank you.

00:48:07.130 --> 00:48:07.660
- Thank you.

00:48:07.660 --> 00:48:12.090
So safety and responsibility
true to shared logging,

00:48:12.090 --> 00:48:14.001
and then also better
tools for the analysts.

00:48:14.001 --> 00:48:14.500
Right?

00:48:14.500 --> 00:48:15.180
- Yes.

00:48:15.180 --> 00:48:18.570
- My question is related
to especially thinking

00:48:18.570 --> 00:48:21.610
about autonomous vehicles.

00:48:21.610 --> 00:48:25.710
What is a model to incorporate
the community level

00:48:25.710 --> 00:48:26.940
knowledge that is needed?

00:48:26.940 --> 00:48:29.940
Because I would imagine
the driving behavior

00:48:29.940 --> 00:48:32.130
should be different in
different parts of the world

00:48:32.130 --> 00:48:33.900
and different neighborhoods.

00:48:33.900 --> 00:48:38.430
So what is the framework to
incorporate community data

00:48:38.430 --> 00:48:41.550
in a way like this
kind of local knowledge

00:48:41.550 --> 00:48:44.160
that maybe doesn't come
from the analysts or NTSB?

00:48:44.160 --> 00:48:46.890
- I mean, this is discussed in
the driverless community, that

00:48:46.890 --> 00:48:48.930
may be different cultures,
different countries

00:48:48.930 --> 00:48:51.450
will actually have different
forms of regulation,

00:48:51.450 --> 00:48:53.940
different values or
ethics introduced there.

00:48:53.940 --> 00:48:58.780
I think the model for that would
be the local planning boards.

00:48:58.780 --> 00:49:01.680
The local planning boards
have some decision making.

00:49:01.680 --> 00:49:05.040
Now, you want to have
automation at every local point,

00:49:05.040 --> 00:49:07.620
but maybe at the
state level in the US.

00:49:07.620 --> 00:49:09.810
And again, maybe
at industry level

00:49:09.810 --> 00:49:12.290
would be another way
to partition the space.

00:49:12.290 --> 00:49:16.560
Then I think you might have
a planning or oversight

00:49:16.560 --> 00:49:19.350
board based on medical devices.

00:49:19.350 --> 00:49:21.900
And the experts in
that field, which

00:49:21.900 --> 00:49:24.690
some of what FDA does
already, would be

00:49:24.690 --> 00:49:26.830
the ones that you would invoke.

00:49:26.830 --> 00:49:29.550
Just as for the
Transportation Safety Board,

00:49:29.550 --> 00:49:32.760
you have that for
automobiles, ships and trains.

00:49:32.760 --> 00:49:35.040
So I think there's
examples to build.

00:49:35.040 --> 00:49:37.200
And again, it's just to open--

00:49:37.200 --> 00:49:39.650
it was a challenge
for me to open my mind

00:49:39.650 --> 00:49:42.570
and to think outside
the technology solutions

00:49:42.570 --> 00:49:45.210
and to think about
how you can create

00:49:45.210 --> 00:49:47.985
the right incentives that will
build the right technology

00:49:47.985 --> 00:49:48.940
solutions.

00:49:48.940 --> 00:49:50.330
I see the other hands.

00:49:50.330 --> 00:49:51.170
Yeah.

00:49:51.170 --> 00:49:51.280
- Oh, wait.

00:49:51.280 --> 00:49:52.190
We have a question over here.

00:49:52.190 --> 00:49:52.440
- Yes.

00:49:52.440 --> 00:49:53.070
- Thank you.

00:49:53.070 --> 00:49:55.140
- Two questions are
somewhat related.

00:49:55.140 --> 00:49:56.250
So first, a practical one.

00:49:56.250 --> 00:49:57.874
That you say there
is a planning phase,

00:49:57.874 --> 00:50:00.540
but when do you envision
this planning phase?

00:50:00.540 --> 00:50:04.260
Because you don't want to shut
down innovation in a research

00:50:04.260 --> 00:50:04.880
setting.

00:50:04.880 --> 00:50:06.990
And so I was
thinking, for example,

00:50:06.990 --> 00:50:11.340
for drug discovery there
is a clinical trial phase.

00:50:11.340 --> 00:50:13.669
Could there be some
kind of testing phase--

00:50:13.669 --> 00:50:14.210
- Absolutely.

00:50:14.210 --> 00:50:15.779
- --is based on evidence?

00:50:15.779 --> 00:50:16.320
- Absolutely.

00:50:16.320 --> 00:50:19.500
I hope I made clear,
research should proceed.

00:50:19.500 --> 00:50:22.320
You don't need to have a
review document for research,

00:50:22.320 --> 00:50:24.570
and for many things
you should be able to.

00:50:24.570 --> 00:50:25.850
It's only when--

00:50:25.850 --> 00:50:27.870
Kathy O'Neil-- when
you're at scale.

00:50:27.870 --> 00:50:29.900
When you're going to field--

00:50:29.900 --> 00:50:33.000
if a major bank
with 3,000 branches

00:50:33.000 --> 00:50:37.980
is going to install a new
system for allocating loans,

00:50:37.980 --> 00:50:39.250
that's the moment.

00:50:39.250 --> 00:50:41.850
And they had planning documents.

00:50:41.850 --> 00:50:44.880
They planned to do that for
whatever-- a year or two years.

00:50:44.880 --> 00:50:46.770
And at some point
during that process,

00:50:46.770 --> 00:50:49.380
just as they would if they
were going to build a building,

00:50:49.380 --> 00:50:52.360
they would have to
go for permission.

00:50:52.360 --> 00:50:54.240
And I think we
should come to see

00:50:54.240 --> 00:50:58.710
the construction of major
scalable and consequential

00:50:58.710 --> 00:51:02.520
tools that should have a
review process, whatever-- six

00:51:02.520 --> 00:51:04.830
months in advance
of implementation

00:51:04.830 --> 00:51:09.360
and then a review process before
the button to launch it goes.

00:51:09.360 --> 00:51:11.220
And the development
also, we didn't

00:51:11.220 --> 00:51:14.400
talk about the set
of test cases that

00:51:14.400 --> 00:51:16.890
would verify or help validate.

00:51:16.890 --> 00:51:19.320
It was part of the USACM list.

00:51:19.320 --> 00:51:21.990
But there would be an
agreement that these

00:51:21.990 --> 00:51:26.580
are the 27, or 2,700 test
cases that we should run

00:51:26.580 --> 00:51:30.315
before your algorithm can go.

00:51:30.315 --> 00:51:33.320
- So a follow up question--
during these test cases

00:51:33.320 --> 00:51:34.600
or the planning phase--

00:51:34.600 --> 00:51:36.810
so in the cases that
you focus on safety

00:51:36.810 --> 00:51:40.980
I see more clear there
could be a yes or no answer.

00:51:40.980 --> 00:51:43.530
But there are some
parts of AI that

00:51:43.530 --> 00:51:45.420
are much more
subtle, maybe that's

00:51:45.420 --> 00:51:46.800
related to these mental models.

00:51:46.800 --> 00:51:49.920
But for example, if
you apply an algorithm

00:51:49.920 --> 00:51:53.850
to help guide how
do you do education,

00:51:53.850 --> 00:51:58.560
or something that in part
could be much farther than--

00:51:58.560 --> 00:52:00.480
I mean, there is
not a safety risk,

00:52:00.480 --> 00:52:02.740
an immediate risk that you'll
get from the algorithm.

00:52:02.740 --> 00:52:05.760
But there might be an
impact in society over time.

00:52:05.760 --> 00:52:06.570
- Sure.

00:52:06.570 --> 00:52:08.190
- How do you evaluate those?

00:52:08.190 --> 00:52:08.960
- You're right.

00:52:08.960 --> 00:52:11.220
I mean, there will
be subtle cases

00:52:11.220 --> 00:52:13.590
where you're saying the
impact could be far off.

00:52:13.590 --> 00:52:17.370
You have a AI system
that grades your homework

00:52:17.370 --> 00:52:20.820
and you fail a course, and
the consequences are quite far

00:52:20.820 --> 00:52:22.350
but the algorithm was mistaken.

00:52:22.350 --> 00:52:22.850
OK?

00:52:22.850 --> 00:52:25.980
We had the recent case
of the FMRI data--

00:52:25.980 --> 00:52:28.680
40,000 papers, I think
it was, were invalid

00:52:28.680 --> 00:52:32.570
because the algorithm in the
FMRI system was incorrect.

00:52:32.570 --> 00:52:33.070
OK?

00:52:33.070 --> 00:52:34.110
And those things happen.

00:52:34.110 --> 00:52:36.840
So we have actual
bugs in systems,

00:52:36.840 --> 00:52:39.520
but we have poorly
designed systems as well.

00:52:39.520 --> 00:52:43.300
And I think we need to develop
strategies to make that happen.

00:52:43.300 --> 00:52:44.220
Yeah.

00:52:44.220 --> 00:52:48.840
- The case for better controls
on important systems certainly

00:52:48.840 --> 00:52:49.800
is a strong one.

00:52:49.800 --> 00:52:53.160
But I think we should
see it as an opportunity

00:52:53.160 --> 00:52:56.520
and be more optimistic that
the automated systems are

00:52:56.520 --> 00:53:00.420
more, not less susceptible
to the controls.

00:53:00.420 --> 00:53:03.830
What are the controls over
unfettered human discretion

00:53:03.830 --> 00:53:05.210
in mortgages?

00:53:05.210 --> 00:53:08.730
What is the
reconstruction mechanism

00:53:08.730 --> 00:53:11.170
for why a human
driver had a crash?

00:53:11.170 --> 00:53:11.670
- Yes.

00:53:11.670 --> 00:53:14.630
- So I think it's a very
positive kind of thing,

00:53:14.630 --> 00:53:18.030
that as our systems are more
and more knowledge based,

00:53:18.030 --> 00:53:20.340
they're better able
to be inspected.

00:53:20.340 --> 00:53:22.630
Human based systems are
not very inspect able.

00:53:22.630 --> 00:53:25.800
- That's a little
harsh, I would say.

00:53:25.800 --> 00:53:29.610
Accidents are investigated
where humans are part of,

00:53:29.610 --> 00:53:32.400
and loans and
banks are monitored

00:53:32.400 --> 00:53:33.784
by the Federal Reserve.

00:53:33.784 --> 00:53:34.950
Now, you don't believe that.

00:53:34.950 --> 00:53:36.380
- I don't believe
it [INAUDIBLE]..

00:53:36.380 --> 00:53:37.770
- It may be ineffective.

00:53:37.770 --> 00:53:39.630
It may fail at times.

00:53:39.630 --> 00:53:42.330
So maybe we need to way
to strengthen those.

00:53:42.330 --> 00:53:46.500
But I do think there's the
need to integrate a higher

00:53:46.500 --> 00:53:48.930
level and not technical--

00:53:48.930 --> 00:53:51.920
that's beyond the developers.

00:53:51.920 --> 00:53:55.920
When the developers say trust
me it's going to be fine,

00:53:55.920 --> 00:53:57.900
don't win the day with me.

00:53:57.900 --> 00:53:58.650
OK?

00:53:58.650 --> 00:54:01.830
If they want to present
it to others and say

00:54:01.830 --> 00:54:05.260
I've adhered to
these 27 guidelines

00:54:05.260 --> 00:54:07.800
and here is the system
that's out there.

00:54:07.800 --> 00:54:10.430
We're going to have a pilot
trial for three months.

00:54:10.430 --> 00:54:14.340
We'll validate it during that
just as any new aircraft.

00:54:14.340 --> 00:54:16.030
New aircraft-- there's
a good example--

00:54:16.030 --> 00:54:17.670
gets certified by the FAA.

00:54:17.670 --> 00:54:18.870
OK?

00:54:18.870 --> 00:54:22.500
There's a testing process
by which it's certified.

00:54:22.500 --> 00:54:24.090
And then they get
a certificate that

00:54:24.090 --> 00:54:28.560
allows them to sell
and have passengers

00:54:28.560 --> 00:54:29.870
travel on those planes.

00:54:29.870 --> 00:54:32.640
And I think we can
find those mechanisms.

00:54:32.640 --> 00:54:34.060
They will be imperfect.

00:54:34.060 --> 00:54:35.940
I know that.

00:54:35.940 --> 00:54:37.920
Current human systems
are imperfect.

00:54:37.920 --> 00:54:39.960
We're always striving
to be better.

00:54:39.960 --> 00:54:42.240
I think that's also
something-- somehow

00:54:42.240 --> 00:54:45.060
the AI and often the tech
community and computer

00:54:45.060 --> 00:54:47.460
science and my colleagues
assume the world can

00:54:47.460 --> 00:54:51.120
be made in a fully rational
way, and that everything

00:54:51.120 --> 00:54:53.190
can be algorithmic,
and that if we

00:54:53.190 --> 00:54:56.740
had only more rational discourse
that everything would be fine.

00:54:56.740 --> 00:54:58.710
But life is a little
more complicated,

00:54:58.710 --> 00:55:00.120
I would like to say.

00:55:00.120 --> 00:55:03.090
And so I think we
have to deal honestly

00:55:03.090 --> 00:55:06.450
with the complexities
of real life.

00:55:06.450 --> 00:55:07.562
- Go ahead.

00:55:07.562 --> 00:55:09.210
Did you have a question?

00:55:09.210 --> 00:55:11.341
- Another question?

00:55:11.341 --> 00:55:11.840
- OK.

00:55:11.840 --> 00:55:12.930
Oh, lots of hands.

00:55:12.930 --> 00:55:14.290
Thank you.

00:55:14.290 --> 00:55:14.790
- Hi.

00:55:14.790 --> 00:55:15.415
Good afternoon.

00:55:15.415 --> 00:55:18.430
Thank you for this good talk.

00:55:18.430 --> 00:55:23.700
So my questions maybe touch on
a couple things you just said.

00:55:23.700 --> 00:55:25.890
I'm curious about
what your thoughts are

00:55:25.890 --> 00:55:30.360
on the culture of programming
and where that might go.

00:55:30.360 --> 00:55:32.970
I mean, we're here at
a teaching institution.

00:55:32.970 --> 00:55:34.560
My own experience
with programming

00:55:34.560 --> 00:55:39.160
is that the culture tends
to be fairly Wild West.

00:55:39.160 --> 00:55:39.840
- Fairly what?

00:55:39.840 --> 00:55:40.930
- Wild West--

00:55:40.930 --> 00:55:41.680
- Wild West, OK.

00:55:41.680 --> 00:55:47.430
- --in terms of standards
and also just how it goes.

00:55:47.430 --> 00:55:50.730
And part of that question
there is really about

00:55:50.730 --> 00:55:54.820
how we do with complexity
and hyper-specialization.

00:55:54.820 --> 00:55:58.350
And then separate
from that maybe is I'm

00:55:58.350 --> 00:56:01.980
wondering what your thoughts
are around feedback loops.

00:56:01.980 --> 00:56:06.330
As these AI algorithms become
more complex the feedback

00:56:06.330 --> 00:56:11.320
loop into how society works
becomes more profound, I think.

00:56:11.320 --> 00:56:11.820
- Yeah.

00:56:11.820 --> 00:56:16.650
Those are rather larger
things, I would say.

00:56:16.650 --> 00:56:19.680
The Wild West nature of
the programming community

00:56:19.680 --> 00:56:22.320
has been faulted by
on many occasions

00:56:22.320 --> 00:56:26.960
and I don't think I want
to go to far with that.

00:56:26.960 --> 00:56:30.670
But I think Wild West
is a safe enough one.

00:56:30.670 --> 00:56:31.920
But the many problems--

00:56:31.920 --> 00:56:33.660
I think the culture
of programming--

00:56:33.660 --> 00:56:38.410
I think courses in ethics in
programming should be required.

00:56:38.410 --> 00:56:40.830
I think they're part of
the required curriculum,

00:56:40.830 --> 00:56:43.800
but many universities don't
include such a course.

00:56:43.800 --> 00:56:47.580
The ACM has a code of ethics
currently being revised,

00:56:47.580 --> 00:56:51.040
and that's another
good source to look at.

00:56:51.040 --> 00:56:53.800
So there are ways
we could make--

00:56:53.800 --> 00:56:56.250
and as I say, it's an
adolescent culture maybe,

00:56:56.250 --> 00:56:58.170
is another way to
say it, which I'm

00:56:58.170 --> 00:57:01.170
looking for it to see maturity.

00:57:01.170 --> 00:57:04.830
And maturity means taking
responsibility for what you do.

00:57:04.830 --> 00:57:08.580
And there may be things like
those hold harmless clauses

00:57:08.580 --> 00:57:11.010
that I think could be changed.

00:57:11.010 --> 00:57:13.800
And that when companies
take responsibility,

00:57:13.800 --> 00:57:16.940
I believe good things-- more
good things will happen.

00:57:16.940 --> 00:57:19.177
And your second point was--

00:57:19.177 --> 00:57:20.370
- The feedback loops.

00:57:20.370 --> 00:57:21.551
- The feedback loop.

00:57:21.551 --> 00:57:22.050
Yeah.

00:57:22.050 --> 00:57:23.140
I mean, yes.

00:57:23.140 --> 00:57:24.588
I'm not sure that's
controversial.

00:57:24.588 --> 00:57:26.004
- The challenge
there is how do we

00:57:26.004 --> 00:57:31.710
know when the perspective,
our perspective--

00:57:31.710 --> 00:57:33.840
how do we know when
our perspective has

00:57:33.840 --> 00:57:35.270
shifted to a point where--

00:57:35.270 --> 00:57:36.800
- Who is the our?

00:57:36.800 --> 00:57:38.200
- Societal perspective.

00:57:38.200 --> 00:57:38.700
Right?

00:57:38.700 --> 00:57:43.040
So if we're looking at
keeping track of algorithms

00:57:43.040 --> 00:57:46.250
and saying well, this algorithm
isn't behaving correctly--

00:57:46.250 --> 00:57:48.380
but that's our
subjective opinion

00:57:48.380 --> 00:57:50.630
and that's subject to change,
but is also subject to--

00:57:50.630 --> 00:57:52.730
- You raise another issue
which came up earlier

00:57:52.730 --> 00:57:55.670
in the day, of the
role that we might

00:57:55.670 --> 00:57:58.880
have as academics in
creating public discourse

00:57:58.880 --> 00:58:01.940
and informativeness about this.

00:58:01.940 --> 00:58:04.280
And I think that's
a valuable thing.

00:58:04.280 --> 00:58:06.650
I have sort of struggled
with the battle

00:58:06.650 --> 00:58:08.244
of the rise of the
robots, the robots

00:58:08.244 --> 00:58:10.160
are going to take our
jobs away, there's going

00:58:10.160 --> 00:58:12.090
to be widespread unemployment.

00:58:12.090 --> 00:58:13.790
I think that's preposterous.

00:58:13.790 --> 00:58:16.250
I just think that's a
failed understanding

00:58:16.250 --> 00:58:18.350
of the nature of technology.

00:58:18.350 --> 00:58:21.080
I see the effect of
technology is producing

00:58:21.080 --> 00:58:22.640
what I call techno-genesis.

00:58:22.640 --> 00:58:26.030
That is new
technologies generate

00:58:26.030 --> 00:58:27.960
a lot more than they destroy.

00:58:27.960 --> 00:58:32.540
Yes Gutenberg's presses put
the scribes out of work.

00:58:32.540 --> 00:58:34.640
You want to count how
many unemployed scribes?

00:58:34.640 --> 00:58:35.780
OK, we can count those.

00:58:35.780 --> 00:58:41.240
But it created a worldwide
literacy, a readership,

00:58:41.240 --> 00:58:44.630
publishing industry,
and most importantly

00:58:44.630 --> 00:58:48.410
it amplified the potential
for creativity of writing.

00:58:48.410 --> 00:58:50.600
Writing was not
a common process.

00:58:50.600 --> 00:58:52.940
People did not write books.

00:58:52.940 --> 00:58:57.330
And every technology--
whether photos, photography--

00:58:57.330 --> 00:59:01.110
1939 Louis Daguerre
makes photography.

00:59:01.110 --> 00:59:05.900
The French painter
Delaroche says from today,

00:59:05.900 --> 00:59:08.877
painting is dead.

00:59:08.877 --> 00:59:10.460
Well, I don't think
he got that right.

00:59:10.460 --> 00:59:14.090
And photography opens up a
world where now everybody

00:59:14.090 --> 00:59:17.510
has 100 selfies on their phone
and everybody's a photographer.

00:59:17.510 --> 00:59:21.590
You create a world
of opportunity.

00:59:21.590 --> 00:59:26.480
Gramophones, music players,
iPods and your phones.

00:59:26.480 --> 00:59:27.410
OK?

00:59:27.410 --> 00:59:30.110
Did that wipe out the
performance of music?

00:59:30.110 --> 00:59:30.680
No.

00:59:30.680 --> 00:59:32.240
Did it change it
and challenge it?

00:59:32.240 --> 00:59:33.260
Yes.

00:59:33.260 --> 00:59:34.730
OK?

00:59:34.730 --> 00:59:37.730
I really wonder--
are there more people

00:59:37.730 --> 00:59:42.937
making music and composing music
than there were 100 years ago?

00:59:42.937 --> 00:59:44.270
Kind of an interesting question.

00:59:44.270 --> 00:59:45.540
I tried to find that number.

00:59:45.540 --> 00:59:48.030
If anyone can help me with
that, I'd love to see it.

00:59:48.030 --> 00:59:50.270
But I would say
that technologies,

00:59:50.270 --> 00:59:55.640
which would seem to wipe out
the jobs of a certain category,

00:59:55.640 --> 00:59:57.110
create technogenesis.

00:59:57.110 --> 01:00:00.530
They generate huge industries.

01:00:00.530 --> 01:00:02.780
Similarly, I mean
I've got about 20

01:00:02.780 --> 01:00:04.260
of these I've been writing down.

01:00:04.260 --> 01:00:05.760
I think you get the idea.

01:00:05.760 --> 01:00:07.290
I'll give you more
if you want them.

01:00:07.290 --> 01:00:09.010
- We have like,
three more questions.

01:00:09.010 --> 01:00:10.880
We have one, two and three.

01:00:10.880 --> 01:00:12.860
- So I wanted to ask--

01:00:12.860 --> 01:00:17.330
I love to talk about
accountability of algorithms.

01:00:17.330 --> 01:00:19.620
But I ask about the
partner of algorithms.

01:00:19.620 --> 01:00:23.820
So algorithms are pretty
innocuous by themselves.

01:00:23.820 --> 01:00:27.240
It's the data that
gets them into trouble.

01:00:27.240 --> 01:00:30.830
So I'm wondering what you think
about data accountability?

01:00:30.830 --> 01:00:33.470
The data that forms the
algorithms and the data that

01:00:33.470 --> 01:00:34.970
algorithms operate on.

01:00:34.970 --> 01:00:35.540
- Thank you.

01:00:35.540 --> 01:00:39.740
Yeah, absolutely.

01:00:39.740 --> 01:00:43.500
I run into this because I'm
a proponent of visualization

01:00:43.500 --> 01:00:44.000
tools.

01:00:44.000 --> 01:00:48.380
And I might say take a look
at Fernando Viagens' work

01:00:48.380 --> 01:00:50.630
at Google and use
the visualization

01:00:50.630 --> 01:00:54.410
to understand machine
learning and neural networks.

01:00:54.410 --> 01:00:57.230
And it's a whole
effort at Google

01:00:57.230 --> 01:00:59.990
that's been transformational--
another one of those that

01:00:59.990 --> 01:01:01.700
encouraged me to
believe that you could

01:01:01.700 --> 01:01:04.830
have these kind of things.

01:01:04.830 --> 01:01:08.540
So the data though,
has a few problems.

01:01:08.540 --> 01:01:13.430
The one I run into most
is the data is poor.

01:01:13.430 --> 01:01:14.870
The quality of the data--

01:01:14.870 --> 01:01:17.124
it's incomplete,
it's contradictory.

01:01:17.124 --> 01:01:18.290
There's all kinds of things.

01:01:18.290 --> 01:01:20.310
I could tell you
lots of stories where

01:01:20.310 --> 01:01:23.990
the visualization tools that
we developed revealed problems.

01:01:23.990 --> 01:01:27.260
6,300 emergency room
admissions and they're

01:01:27.260 --> 01:01:28.970
asking the average
age of the men

01:01:28.970 --> 01:01:30.710
and the women and
those discharged.

01:01:30.710 --> 01:01:33.140
And they had no
idea after running

01:01:33.140 --> 01:01:35.060
all kinds of stats--
simple stats,

01:01:35.060 --> 01:01:36.540
not machine learning--
but they had

01:01:36.540 --> 01:01:43.100
no idea that eight of those
patients were 999 years old.

01:01:43.100 --> 01:01:45.290
Kind of a simple thing
to miss, isn't it?

01:01:45.290 --> 01:01:50.570
They had no idea that about 60
of were under one years old--

01:01:50.570 --> 01:01:52.490
a very unusually large number.

01:01:52.490 --> 01:01:55.190
And that was because in
part of the hospital the way

01:01:55.190 --> 01:01:56.840
they enter the
age of the patient

01:01:56.840 --> 01:01:58.760
was they enter the birth year.

01:01:58.760 --> 01:02:02.100
And they made consistent errors
of entering the current year.

01:02:02.100 --> 01:02:04.460
So you get lots of
patients under one-year-old

01:02:04.460 --> 01:02:05.810
who have heart attacks.

01:02:05.810 --> 01:02:08.480
OK?

01:02:08.480 --> 01:02:10.900
This goes on, of
duplicated data.

01:02:10.900 --> 01:02:14.420
A year worth of sales data--

01:02:14.420 --> 01:02:19.105
they're making a predictive
model to plan the next year.

01:02:19.105 --> 01:02:21.500
They had no idea that
April was missing.

01:02:21.500 --> 01:02:26.540
All the spring sales of April
were not in this data set.

01:02:26.540 --> 01:02:30.230
For me, the first problem is the
data quality and completeness,

01:02:30.230 --> 01:02:31.560
and so on.

01:02:31.560 --> 01:02:32.760
I could go on with stories.

01:02:32.760 --> 01:02:35.150
But the other part is the bias.

01:02:35.150 --> 01:02:37.700
And here I would say
the technical progress

01:02:37.700 --> 01:02:38.750
has been the most strong.

01:02:38.750 --> 01:02:42.710
The FAT ML community, I think
has done a fair job of looking

01:02:42.710 --> 01:02:45.830
at what would be biasing
data for particular--

01:02:45.830 --> 01:02:50.300
so loan applications, since we
have legal restrictions on bias

01:02:50.300 --> 01:02:57.050
for gender and age and race and
so on, we know that there are

01:02:57.050 --> 01:02:57.770
certain--

01:02:57.770 --> 01:03:00.890
you don't want to have
those fields in the data.

01:03:00.890 --> 01:03:06.240
And then proxies such as zip
code may be a problem, as well.

01:03:06.240 --> 01:03:08.960
And these will be specific
to particular industries.

01:03:08.960 --> 01:03:12.560
And so I think
that probably shows

01:03:12.560 --> 01:03:14.390
opportunities for fairness.

01:03:14.390 --> 01:03:17.270
There will always
be emergent bias.

01:03:17.270 --> 01:03:20.150
Helen Nissenbaum and
Dr. Friedman long ago

01:03:20.150 --> 01:03:22.550
wrote a very thoughtful paper
about the different kinds

01:03:22.550 --> 01:03:23.640
of bias.

01:03:23.640 --> 01:03:26.750
So that gives us a model
about what to look for

01:03:26.750 --> 01:03:28.400
and how the context changes.

01:03:28.400 --> 01:03:31.280
What wasn't biased
three years ago now

01:03:31.280 --> 01:03:34.430
is biased because of some
change in the context.

01:03:34.430 --> 01:03:37.220
And that's part of what went
wrong with the Google Flu

01:03:37.220 --> 01:03:41.090
Trends-- another dream of
AI way of predicting where

01:03:41.090 --> 01:03:45.920
flu outbreaks would be greatest
in the US by the search queries

01:03:45.920 --> 01:03:50.190
that people searched on of
flu or tissues or other things

01:03:50.190 --> 01:03:51.470
or medications.

01:03:51.470 --> 01:03:55.280
And it turned out that went so
bad that Apple shut that down

01:03:55.280 --> 01:03:56.690
and that website is gone.

01:03:56.690 --> 01:03:57.240
- Google.

01:03:57.240 --> 01:03:57.740
- Google.

01:03:57.740 --> 01:03:58.170
I'm sorry.

01:03:58.170 --> 01:03:58.670
Google.

01:03:58.670 --> 01:04:00.410
Thank you.

01:04:00.410 --> 01:04:01.790
So Google shut that down.

01:04:01.790 --> 01:04:05.530
There's a wonderful three
page paper from 2013

01:04:05.530 --> 01:04:08.015
in AAAS Science by David Lazer.

01:04:08.015 --> 01:04:09.050
He's the first author--

01:04:09.050 --> 01:04:13.500
L-A-Z-E-R-- who has a wonderful
analysis of what went wrong.

01:04:13.500 --> 01:04:16.880
I think studying the things that
have gone wrong with algorithms

01:04:16.880 --> 01:04:17.735
will help us a lot.

01:04:17.735 --> 01:04:18.235
Yeah.

01:04:18.235 --> 01:04:19.380
I think you said two more.

01:04:19.380 --> 01:04:20.341
- David?

01:04:20.341 --> 01:04:20.840
- Yeah.

01:04:20.840 --> 01:04:22.400
So I wanted.

01:04:22.400 --> 01:04:24.920
To start by thanking
you for being here

01:04:24.920 --> 01:04:27.410
and for leading this
discourse I think

01:04:27.410 --> 01:04:30.080
you can tell from how
animated everybody is

01:04:30.080 --> 01:04:31.500
that you've really engaged us.

01:04:31.500 --> 01:04:33.175
So thank you for that.

01:04:33.175 --> 01:04:34.550
And I also want
to agree with you

01:04:34.550 --> 01:04:37.580
that I think the retrospective
is the easiest one.

01:04:37.580 --> 01:04:41.240
I think that one, that
makes complete sense to me.

01:04:41.240 --> 01:04:44.450
I do want to express some
skepticism about the planning

01:04:44.450 --> 01:04:46.370
committee viewpoints.

01:04:46.370 --> 01:04:49.730
And also, if I may make a quick
comment about deep learning,

01:04:49.730 --> 01:04:51.540
as well.

01:04:51.540 --> 01:04:55.230
I really resonate with the
point that [INAUDIBLE] made,

01:04:55.230 --> 01:04:58.380
which is that I think that
a different way to say it

01:04:58.380 --> 01:05:01.580
is that a planning
commission for a building

01:05:01.580 --> 01:05:04.430
has a much easier
task ahead of it

01:05:04.430 --> 01:05:09.030
than a planning commission for
a computer technical system.

01:05:09.030 --> 01:05:13.970
The proximate concerns in
regards to a building, I think,

01:05:13.970 --> 01:05:16.550
are perhaps easier
to understand.

01:05:16.550 --> 01:05:17.860
You described it.

01:05:17.860 --> 01:05:21.430
Is the light of the building
falling on my neighbor?

01:05:21.430 --> 01:05:25.870
Is the building environmentally
appropriately built?

01:05:25.870 --> 01:05:27.920
Is the energy system safe?

01:05:27.920 --> 01:05:32.480
The proximate kind of
potential side effects

01:05:32.480 --> 01:05:35.150
of an AI, algorithmic
system, especially

01:05:35.150 --> 01:05:38.530
as AI begins to
essentially do everything--

01:05:38.530 --> 01:05:40.160
it seems to me that
your planning board

01:05:40.160 --> 01:05:44.510
would have to form a viewpoint
on what's right for society

01:05:44.510 --> 01:05:46.230
and what's not
right for society.

01:05:46.230 --> 01:05:50.130
So I wanted to make that comment
and invite you to respond.

01:05:50.130 --> 01:05:50.630
- Yeah.

01:05:50.630 --> 01:05:51.020
Let's--

01:05:51.020 --> 01:05:51.490
- You want to take
that one first?

01:05:51.490 --> 01:05:52.390
- Let's stay with
one piece at a time.

01:05:52.390 --> 01:05:52.890
Yeah.

01:05:52.890 --> 01:05:54.282
And I'm happy to engage.

01:05:54.282 --> 01:05:56.240
I know some people have
to leave, so thank you.

01:05:56.240 --> 01:05:58.400
But well, I don't know.

01:05:58.400 --> 01:06:00.980
I've looked at only a little
bit of building codes.

01:06:00.980 --> 01:06:02.690
But they're pretty complicated.

01:06:02.690 --> 01:06:04.460
So I don't think it's as simple.

01:06:04.460 --> 01:06:08.330
We've had 100 years of
experience in developing them,

01:06:08.330 --> 01:06:10.340
and so maybe they seem apparent.

01:06:10.340 --> 01:06:14.540
I don't think that it should
be so impossible to build

01:06:14.540 --> 01:06:20.780
a set of codes, or rules for the
way AI systems should be built.

01:06:20.780 --> 01:06:22.500
And I think we're
going to get there.

01:06:22.500 --> 01:06:25.250
And I think we should
value and respect

01:06:25.250 --> 01:06:28.370
the AI researchers and
engineering communities,

01:06:28.370 --> 01:06:29.450
that they can do this.

01:06:29.450 --> 01:06:31.580
We just have to set
the public expectation,

01:06:31.580 --> 01:06:32.870
they should do this.

01:06:32.870 --> 01:06:33.590
OK?

01:06:33.590 --> 01:06:36.380
And you made one
passing comment there--

01:06:36.380 --> 01:06:40.700
as AI systems get
into everything.

01:06:40.700 --> 01:06:42.660
I don't know about that.

01:06:42.660 --> 01:06:47.870
I don't think AI systems are as
prevalent as the hype suggests.

01:06:47.870 --> 01:06:51.450
When I do email I'm
not using much AI.

01:06:51.450 --> 01:06:55.190
A good design of systems
and screens and web sites

01:06:55.190 --> 01:06:57.330
are really what drive things.

01:06:57.330 --> 01:06:59.360
You begin to see AI.

01:06:59.360 --> 01:07:01.610
Recommender systems
are there and we

01:07:01.610 --> 01:07:03.980
have other-- machine
translation, natural language

01:07:03.980 --> 01:07:04.830
translation.

01:07:04.830 --> 01:07:08.780
But I would say AI is a
small part of the world.

01:07:08.780 --> 01:07:12.980
To me, the larger issue
is the design of systems

01:07:12.980 --> 01:07:15.200
and of the user interface.

01:07:15.200 --> 01:07:19.460
So I think we should
be clear that AI is not

01:07:19.460 --> 01:07:21.980
so pervasive as is
said, and we should be

01:07:21.980 --> 01:07:23.630
careful about what we call AI.

01:07:23.630 --> 01:07:26.180
There's a lot of language
now where suddenly everything

01:07:26.180 --> 01:07:27.950
is AI.

01:07:27.950 --> 01:07:30.250
Every algorithm is
an AI algorithm.

01:07:30.250 --> 01:07:31.810
I don't see it that way.

01:07:31.810 --> 01:07:32.420
OK?

01:07:32.420 --> 01:07:35.270
So we ought to get better
about these clarifications.

01:07:35.270 --> 01:07:36.350
OK.

01:07:36.350 --> 01:07:37.880
- OK.

01:07:37.880 --> 01:07:39.890
I think there's one
to debate there.

01:07:39.890 --> 01:07:43.020
But definitely it's an
inflection point where

01:07:43.020 --> 01:07:44.750
AI will be doing more and more.

01:07:44.750 --> 01:07:48.260
And also, we are
also in a situation

01:07:48.260 --> 01:07:51.980
where a lot of things
that have been AI

01:07:51.980 --> 01:07:53.690
are no longer
thought about as AI.

01:07:53.690 --> 01:07:56.780
So that goes in the
other direction, as well.

01:07:56.780 --> 01:07:58.200
Let me get to my second point.

01:07:58.200 --> 01:08:03.530
That pause at a automated
vehicle system that

01:08:03.530 --> 01:08:10.610
is using a black box, deep
learning technology, and that

01:08:10.610 --> 01:08:17.450
through its deployment saves
hundreds, maybe hundreds

01:08:17.450 --> 01:08:20.420
of thousands or more lives.

01:08:20.420 --> 01:08:25.250
And yet now and again, it just
something really, really stupid

01:08:25.250 --> 01:08:30.257
and it kills somebody in a way
that a human would never do.

01:08:30.257 --> 01:08:31.840
And again, this is
a black box system.

01:08:31.840 --> 01:08:32.760
This is a deep net.

01:08:32.760 --> 01:08:35.569
This is a system that you said
that you would shut it down.

01:08:35.569 --> 01:08:36.990
Would you shut that system down?

01:08:36.990 --> 01:08:37.700
- All right.

01:08:37.700 --> 01:08:39.450
So I'll give you an example.

01:08:39.450 --> 01:08:40.790
Let's see if we can--

01:08:40.790 --> 01:08:41.790
airbags.

01:08:41.790 --> 01:08:44.510
The early deployments
of airbags--

01:08:44.510 --> 01:08:48.516
is airbag system
deployment AI or not?

01:08:48.516 --> 01:08:49.599
- I tossed the microphone.

01:08:49.599 --> 01:08:50.990
I would say probably not.

01:08:50.990 --> 01:08:52.160
- Probably not, OK.

01:08:52.160 --> 01:08:53.180
But it's an algorithm.

01:08:53.180 --> 01:08:55.130
That's why I chose to
focus on algorithms,

01:08:55.130 --> 01:08:57.490
not particularly not only AI.

01:08:57.490 --> 01:09:00.260
Early airbag
systems, the estimate

01:09:00.260 --> 01:09:05.566
was it saved about
2,500 lives a year.

01:09:05.566 --> 01:09:07.399
On the other hand, the,
early airbag systems

01:09:07.399 --> 01:09:09.740
killed about 100 kids--

01:09:09.740 --> 01:09:13.090
mostly in the passenger
seat-- a year, inadvertently.

01:09:13.090 --> 01:09:14.870
Because their bags
were triggered

01:09:14.870 --> 01:09:17.510
and the explosive
force of the airbag

01:09:17.510 --> 01:09:20.140
was such that it
killed the child.

01:09:20.140 --> 01:09:22.069
OK?

01:09:22.069 --> 01:09:23.979
That should have been avoidable.

01:09:23.979 --> 01:09:26.960
That should have been something
that was taken into account.

01:09:26.960 --> 01:09:29.540
The engineers did
not get it together

01:09:29.540 --> 01:09:31.500
to anticipate that problem.

01:09:31.500 --> 01:09:34.850
Now OK, maybe I'm
being harsh on them.

01:09:34.850 --> 01:09:36.260
But there was a fix.

01:09:36.260 --> 01:09:39.439
And so you have to
fix that detects

01:09:39.439 --> 01:09:41.840
whether the passenger is
seated, and you can control

01:09:41.840 --> 01:09:43.233
the air bag a little better.

01:09:43.233 --> 01:09:45.149
You can't control
everything about the airbag,

01:09:45.149 --> 01:09:48.529
and because the rapid deployment
in 200 milliseconds means

01:09:48.529 --> 01:09:52.229
it has to be an automated
system, but I can control it.

01:09:52.229 --> 01:09:54.800
So I feel safe about
putting my child there.

01:09:54.800 --> 01:09:58.610
And I know that I put my
child in a rear facing seat.

01:09:58.610 --> 01:10:01.970
So we come to learn how
to build those systems.

01:10:01.970 --> 01:10:06.770
And I think making the AI
community aware of the dangers

01:10:06.770 --> 01:10:10.070
there will lead them
to a path will be more

01:10:10.070 --> 01:10:12.090
responsible in taking care.

01:10:12.090 --> 01:10:14.300
And then the feedback
that you asked for--

01:10:14.300 --> 01:10:19.400
I mean, good news that the
Highway Transportation Safety

01:10:19.400 --> 01:10:24.360
Board was collecting the
evidence about inadvertent air

01:10:24.360 --> 01:10:26.570
bag deployments that killed.

01:10:26.570 --> 01:10:30.350
If we didn't have that feedback,
then we'd never get that fix.

01:10:30.350 --> 01:10:34.400
And so this mindless approach
that says don't bother me,

01:10:34.400 --> 01:10:35.150
I'm just doing it.

01:10:35.150 --> 01:10:36.390
My system will be great.

01:10:36.390 --> 01:10:37.940
Everything will be safe--

01:10:37.940 --> 01:10:38.630
doesn't work.

01:10:38.630 --> 01:10:41.510
And again, another positive
thing-- the airline safety

01:10:41.510 --> 01:10:43.670
record is exceptional.

01:10:43.670 --> 01:10:46.640
And I think that's due to
the kind of feedback systems

01:10:46.640 --> 01:10:48.740
that are built in, and
responsible engineering.

01:10:48.740 --> 01:10:48.920
- All right.

01:10:48.920 --> 01:10:49.490
- We have one more?

01:10:49.490 --> 01:10:50.323
- One last question.

01:10:50.323 --> 01:10:52.560
I'm sorry to those of
you in the back who--

01:10:52.560 --> 01:10:54.060
you can come talk
to Ben afterwards.

01:10:54.060 --> 01:10:54.770
Sir?

01:10:54.770 --> 01:10:56.690
- Thanks for a great talk.

01:10:56.690 --> 01:11:02.210
The most widely used system that
probably bear no responsibility

01:11:02.210 --> 01:11:04.040
is Google Maps.

01:11:04.040 --> 01:11:06.950
What changes would
Google Maps need

01:11:06.950 --> 01:11:11.660
to make so that it avoids
doing things like accidentally

01:11:11.660 --> 01:11:14.660
sending people into fires
in Southern California,

01:11:14.660 --> 01:11:16.880
to cite one recent example.

01:11:16.880 --> 01:11:19.160
Is it that it needs to be
like, by the way, if you

01:11:19.160 --> 01:11:22.589
see a fire don't keep driving.

01:11:22.589 --> 01:11:25.130
What fundamental changes would
it need to make to avoid that?

01:11:25.130 --> 01:11:26.830
- Well, let's go back here.

01:11:26.830 --> 01:11:29.750
I'm still resonating
over your question.

01:11:29.750 --> 01:11:32.320
Is Google Maps an AI system?

01:11:32.320 --> 01:11:34.111
- It currently has AI.

01:11:34.111 --> 01:11:34.610
- It does?

01:11:34.610 --> 01:11:36.046
How so?

01:11:36.046 --> 01:11:37.898
- Because it takes a
large amount of data--

01:11:37.898 --> 01:11:38.648
- Wait one second.

01:11:38.648 --> 01:11:39.410
Sorry.

01:11:39.410 --> 01:11:41.860
- Sorry.

01:11:41.860 --> 01:11:45.020
Well, to cite that
example, it sees

01:11:45.020 --> 01:11:48.210
how cars are moving in Southern
California along the highways.

01:11:48.210 --> 01:11:53.180
It sees that it doesn't see any
traffic along say, the five.

01:11:53.180 --> 01:11:55.850
And it says oh,
go take the five.

01:11:55.850 --> 01:11:57.754
That'll get you home fastest.

01:11:57.754 --> 01:11:59.420
And then people get
on the five and they

01:11:59.420 --> 01:12:01.790
realize there are
no cars on the five

01:12:01.790 --> 01:12:03.800
because it turns out
there's a big fire.

01:12:03.800 --> 01:12:05.420
That would be one example.

01:12:05.420 --> 01:12:07.640
- And that's AI?

01:12:07.640 --> 01:12:09.090
- I would argue that that is AI.

01:12:09.090 --> 01:12:10.381
- How do you know when it's AI?

01:12:10.381 --> 01:12:12.320
You touched on this also.

01:12:12.320 --> 01:12:16.430
I mean, Google Maps, as it
started, did not have AI.

01:12:16.430 --> 01:12:18.509
It was very good
routing algorithms,

01:12:18.509 --> 01:12:19.550
and that's how it worked.

01:12:19.550 --> 01:12:23.240
Now it's come along with
some more refined points,

01:12:23.240 --> 01:12:28.730
that it amazingly integrates
current traffic data to give

01:12:28.730 --> 01:12:30.080
more refined judgment.

01:12:30.080 --> 01:12:33.920
But I'm not sure I would
give it the notion it's AI.

01:12:33.920 --> 01:12:37.230
You alluded to get the idea
that once AI systems get

01:12:37.230 --> 01:12:39.900
mature they don't
call it AI anymore.

01:12:39.900 --> 01:12:41.730
Well, I would say
I have another,

01:12:41.730 --> 01:12:44.520
maybe more cynical view.

01:12:44.520 --> 01:12:46.680
I'll risk it.

01:12:46.680 --> 01:12:49.320
Actually, they were
trying AI methods

01:12:49.320 --> 01:12:51.420
but the methods didn't work.

01:12:51.420 --> 01:12:53.820
And they cleaned them up and
they did good engineering.

01:12:53.820 --> 01:12:56.910
And it was clear,
comprehensible and predictable,

01:12:56.910 --> 01:12:58.750
and then it worked.

01:12:58.750 --> 01:13:01.591
So it's no longer an AI system,
because it is no longer--

01:13:01.591 --> 01:13:03.090
It was no longer
called an AI system

01:13:03.090 --> 01:13:04.420
because it's not an AI system.

01:13:04.420 --> 01:13:04.920
- Wow.

01:13:04.920 --> 01:13:05.420
OK.

01:13:05.420 --> 01:13:07.370
And on that very
uncontroversial note,

01:13:07.370 --> 01:13:08.740
let's just thank Ben again.

01:13:08.740 --> 01:13:09.340
- Thank you!

01:13:09.340 --> 01:13:12.690
[APPLAUSE]

