WEBVTT
Kind: captions
Language: en

00:00:00.360 --> 00:00:03.200
Continuous integration makes sure
that the software builds and

00:00:03.200 --> 00:00:06.000
runs successfully and
then you do the testing.

00:00:06.000 --> 00:00:09.290
There are still steps between this
point and the software release.

00:00:09.290 --> 00:00:12.040
Automating the release pipeline
down to the software release is

00:00:12.040 --> 00:00:13.850
called continuous delivery.

00:00:13.850 --> 00:00:16.010
But since some of QA
is a manual process,

00:00:16.010 --> 00:00:18.830
you cannot run the next step
completely automatically.

00:00:18.830 --> 00:00:22.630
You need a manual trigger in the
pipeline to deploy to the next stage,

00:00:22.630 --> 00:00:24.770
which is usually a staging server.

00:00:24.770 --> 00:00:27.800
For example,
testing against a real database.

00:00:27.800 --> 00:00:31.100
You can run more automated and
manual tests on staging.

00:00:31.100 --> 00:00:34.270
Once successful you can manually
trigger the deployment.

00:00:34.270 --> 00:00:37.790
It's important to note that a manual
trigger does not equal a manual build

00:00:37.790 --> 00:00:38.990
or deployment.

00:00:38.990 --> 00:00:40.710
Build and deployment is automatic,

00:00:40.710 --> 00:00:43.680
it's just the trigger that
starts the process is manual.

00:00:43.680 --> 00:00:46.680
There are integrations with
communication software that allows these

00:00:46.680 --> 00:00:49.780
manual triggers to be launched
in a more publicly visible and

00:00:49.780 --> 00:00:53.440
convenient way than logging into
a particular software program.

00:00:53.440 --> 00:00:55.120
For example, there are chatbots for

00:00:55.120 --> 00:00:58.960
popular team communication
platforms like Slack and HipChat

00:00:58.960 --> 00:01:03.410
that can be integrated with Jenkins,
CircleCI, and other CI solutions.

00:01:03.410 --> 00:01:07.260
These chat-bots can send build test
information to a public chat room.

00:01:07.260 --> 00:01:09.640
And react to commands
written into the chat.

00:01:09.640 --> 00:01:12.620
That makes it easy and
fast to coordinate within the team.

00:01:12.620 --> 00:01:17.120
If the CI build and test passes,
using chat, you can poll all involved

00:01:17.120 --> 00:01:19.900
developers with something like,
my change is good to go.

00:01:19.900 --> 00:01:23.030
And then issue that
command into the chat-bot.

00:01:23.030 --> 00:01:26.490
This makes for a fast and
transparent release process.

00:01:26.490 --> 00:01:30.120
We will not be setting up these chat
services, but see instructor notes for

00:01:30.120 --> 00:01:31.980
more information.

00:01:31.980 --> 00:01:35.090
&gt;&gt; If you have a service that handles
thousands of user transactions

00:01:35.090 --> 00:01:39.910
every second, over dozens or hundreds
of servers, you're not going to sit

00:01:39.910 --> 00:01:43.410
there and watch the log files
to see if it's working right.

00:01:43.410 --> 00:01:48.370
You, as a human being, literally cannot
read thousands of messages per second

00:01:48.370 --> 00:01:50.470
in hundreds of log files.

00:01:50.470 --> 00:01:53.020
You can't react the difference
between an acceptable and

00:01:53.020 --> 00:01:55.450
an unacceptable level of errors.

00:01:55.450 --> 00:01:59.460
Without monitoring systems to be your
eyes and ears you literally do not have

00:01:59.460 --> 00:02:03.890
the ability to notice or care about
something happening that fast.

00:02:03.890 --> 00:02:07.700
There are a few different ways that we
can gather data about our services.

00:02:07.700 --> 00:02:10.430
Not all of them apply to all services,
though.

00:02:10.430 --> 00:02:13.170
One is external probing,
or sending test queries.

00:02:13.170 --> 00:02:15.810
Some outside system will send
queries in to our system and

00:02:15.810 --> 00:02:18.408
see if it performs as expected and
record the results.

00:02:18.408 --> 00:02:22.380
Another is application-level metrics
that are exported by monitoring

00:02:22.380 --> 00:02:24.260
interfaces of our serving processes.

00:02:24.260 --> 00:02:27.030
This might include things like the
number of queries they're handling per

00:02:27.030 --> 00:02:29.330
second or what the latency is.

00:02:29.330 --> 00:02:32.010
Those can be pushed into
a monitoring system or

00:02:32.010 --> 00:02:34.230
pulled into it from the server.

00:02:34.230 --> 00:02:37.680
Another thing to monitor are stats that
are exported by our runtime environment,

00:02:37.680 --> 00:02:40.110
for instance,
the memory profile in a Java JVM.

00:02:40.110 --> 00:02:43.570
And then there's also information about
the computers that we're actually

00:02:43.570 --> 00:02:47.320
running on such as the load average or
the number of disc errors.

00:02:47.320 --> 00:02:50.590
You can actually very typically
discover that a disc is about to fail

00:02:50.590 --> 00:02:53.440
by monitoring the number
of errors it's having.

00:02:53.440 --> 00:02:57.750
Other sources of monitoring data can
include logs analysis, query profiling

00:02:57.750 --> 00:03:02.370
such as Google App engines app stats,
and external analytic services.

00:03:02.370 --> 00:03:06.800
There are also a number of data products
that a monitoring system can provide.

00:03:06.800 --> 00:03:08.880
The one everyone knows
about is alerting.

00:03:08.880 --> 00:03:11.560
Getting a page saying
the system is too slow.

00:03:11.560 --> 00:03:14.480
But your monitoring system can
also provide charts and graphs for

00:03:14.480 --> 00:03:16.220
performance analysis.

00:03:16.220 --> 00:03:20.200
Say things like this release is 10
percent faster than the last one.

00:03:20.200 --> 00:03:22.030
And also for capacity prediction.

00:03:22.030 --> 00:03:22.850
For instance,

00:03:22.850 --> 00:03:26.770
if it takes ten instances to support
1,000 transactions per second.

00:03:26.770 --> 00:03:29.970
Then if we scale to 2,000
transactions per second,

00:03:29.970 --> 00:03:32.710
we should expect to need
ten more instances.

00:03:32.710 --> 00:03:34.460
Also for growth measurement.

00:03:34.460 --> 00:03:36.350
You can plot a graph out of
your monitoring system and

00:03:36.350 --> 00:03:39.570
see that your service is doubling
traffic every three months.

00:03:39.570 --> 00:03:42.820
You can combine that with a capacity
prediction to get capacity needs

00:03:42.820 --> 00:03:43.420
forecasting.

00:03:43.420 --> 00:03:46.510
Tell you how many computers you're
going to need next quarter.

00:03:46.510 --> 00:03:47.650
And last but not least,

00:03:47.650 --> 00:03:51.490
your monitoring system can also provide
great metrics for debugging problems.

00:03:51.490 --> 00:03:55.480
For instance, suppose you're trying to
diagnose why your servers are crashing.

00:03:55.480 --> 00:03:59.090
If you notice from your monitoring
that each instance is using more and

00:03:59.090 --> 00:04:01.610
more memory proportional to
the number of user queries,

00:04:01.610 --> 00:04:03.710
that might indicate to look for
a memory leak.

