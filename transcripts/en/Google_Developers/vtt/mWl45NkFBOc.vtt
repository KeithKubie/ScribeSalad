WEBVTT
Kind: captions
Language: en

00:00:00.869 --> 00:00:02.410
ZAK STONE: It's been
just over a year

00:00:02.410 --> 00:00:03.910
since we open-sourced
TensorFlow,

00:00:03.910 --> 00:00:07.240
and we've been thrilled to see
the adoption by the community,

00:00:07.240 --> 00:00:09.910
and the pace of development
both here at Google and all

00:00:09.910 --> 00:00:11.010
around the world.

00:00:11.010 --> 00:00:13.040
JEFF DEAN: TensorFlow is
really the primary tool

00:00:13.040 --> 00:00:15.029
that we're using for
a lot of our machine

00:00:15.029 --> 00:00:16.570
learning work in
all of our products.

00:00:16.570 --> 00:00:18.340
Towards the end of
last year, we actually

00:00:18.340 --> 00:00:21.100
rolled out a completely
new translation system

00:00:21.100 --> 00:00:22.850
that was based on
deep neural nets.

00:00:22.850 --> 00:00:26.154
In Gmail, we were actually able
to roll out a TensorFlow model

00:00:26.154 --> 00:00:28.570
that, by understanding the
context of the message you just

00:00:28.570 --> 00:00:30.670
received, we can
predict likely replies,

00:00:30.670 --> 00:00:33.280
and this is a feature
we call Smart Reply.

00:00:33.280 --> 00:00:35.980
LILY PENG: Diabetic retinopathy
is the fastest growing cause

00:00:35.980 --> 00:00:36.700
of blindness.

00:00:36.700 --> 00:00:38.710
It's a complication of diabetes.

00:00:38.710 --> 00:00:40.750
We gathered a very
large data set

00:00:40.750 --> 00:00:43.120
and had doctors
grade the images,

00:00:43.120 --> 00:00:45.570
and then we, using TensorFlow,
trained a neural net

00:00:45.570 --> 00:00:48.370
that does a pretty good job
of predicting whether or not

00:00:48.370 --> 00:00:51.610
there is a diabetic
retinopathy in the image.

00:00:51.610 --> 00:00:54.447
DOUG ECK: Can we use something
like TensorFlow to make music,

00:00:54.447 --> 00:00:56.530
to make art, and to allow
us to communicate better

00:00:56.530 --> 00:00:57.790
with each other?

00:00:57.790 --> 00:00:59.890
With TensorFlow, we're
able to think abstractly,

00:00:59.890 --> 00:01:02.481
almost at a level of, like,
improvisation with machine

00:01:02.481 --> 00:01:02.980
learning.

00:01:02.980 --> 00:01:05.379
We're able to try new
things, to chunk models

00:01:05.379 --> 00:01:07.450
together in ways that
were impossible before we

00:01:07.450 --> 00:01:09.037
had that kind of expressivity.

00:01:14.770 --> 00:01:17.710
AMANDA HODGSON: Dugongs are classed
as vulnerable to extinction

00:01:17.710 --> 00:01:18.830
globally.

00:01:18.830 --> 00:01:21.600
So we do a lot of aerial
surveys using drones.

00:01:21.600 --> 00:01:25.620
Then once you've done a
survey of a really large area,

00:01:25.620 --> 00:01:28.360
you end up with tens, if
not hundreds of thousands

00:01:28.360 --> 00:01:29.950
of photos.

00:01:29.950 --> 00:01:33.980
The goal was to find a way to
automate that whole process.

00:01:33.980 --> 00:01:36.365
And that's where we've
been using TensorFlow.

00:01:36.365 --> 00:01:37.990
ZAK STONE: One of
the things that we've

00:01:37.990 --> 00:01:41.140
been focusing on this year
with TensorFlow is performance.

00:01:41.140 --> 00:01:43.630
We've been especially
excited to release support

00:01:43.630 --> 00:01:45.210
for distributed training.

00:01:45.210 --> 00:01:47.710
MEGAN KACHOLIA: We want to make
it easier for people to use,

00:01:47.710 --> 00:01:49.600
so they don't have
to necessarily know

00:01:49.600 --> 00:01:51.880
all of the underlying
internals in order

00:01:51.880 --> 00:01:54.430
to get the distributed
performance the best it can be.

00:01:54.430 --> 00:01:57.057
[INAUDIBLE] is something that
can compile down TensorFlow.

00:01:57.057 --> 00:01:59.140
Maybe you want to compile
your graph ahead of time

00:01:59.140 --> 00:02:01.360
and then get it down to
something much more compact,

00:02:01.360 --> 00:02:02.820
in terms of memory size.

00:02:02.820 --> 00:02:05.320
So that that way, you can
easily load it and execute it

00:02:05.320 --> 00:02:08.169
on something that might not
have as much storage space,

00:02:08.169 --> 00:02:11.419
like a mobile phone or some
other portable smaller device.

00:02:11.419 --> 00:02:13.960
RICK MAULE: When we introduced
the hexagon vector extensions,

00:02:13.960 --> 00:02:18.280
what we had in mind was
enhancing user experiences

00:02:18.280 --> 00:02:19.889
with imaging features.

00:02:19.889 --> 00:02:21.430
ERICH PLONDKE: So
the TensorFlow team

00:02:21.430 --> 00:02:23.800
said that you only needed
low precision multiplies

00:02:23.800 --> 00:02:26.570
to be able to execute these
neural networks efficiently.

00:02:26.570 --> 00:02:29.710
So we did some tests and on
the same graph, Inception_v3,

00:02:29.710 --> 00:02:32.170
we were eight times faster
and four times lower power

00:02:32.170 --> 00:02:34.240
than running on the CPUs.

00:02:34.240 --> 00:02:36.160
RICK MAULE: TensorFlow
is great to work

00:02:36.160 --> 00:02:39.130
with, easy to work with,
lots of capability.

00:02:39.130 --> 00:02:41.590
So our engineering teams
and their engineering teams

00:02:41.590 --> 00:02:45.154
working together, we were able
to do something very exciting.

00:02:45.154 --> 00:02:46.570
This is just the
beginning of what

00:02:46.570 --> 00:02:49.060
will end up being a long
evolution of some great things

00:02:49.060 --> 00:02:52.232
we can do with machine
learning and image processing.

00:02:52.232 --> 00:02:54.190
JOSH GORDON: In addition
to sharing TensorFlow,

00:02:54.190 --> 00:02:56.410
Google has also
shared a ecosystem

00:02:56.410 --> 00:02:58.540
of tools, which
contains everything

00:02:58.540 --> 00:03:01.540
you need to go all the way
from research to production.

00:03:01.540 --> 00:03:03.820
One such tool is
TensorFlow serving,

00:03:03.820 --> 00:03:06.670
and this is a open-source,
high-performance serving

00:03:06.670 --> 00:03:07.511
solution.

00:03:07.511 --> 00:03:09.760
Another great tool, which
is actually quite beautiful,

00:03:09.760 --> 00:03:11.650
is the embedding visualizer.

00:03:11.650 --> 00:03:13.390
And you can use the
embedding visualizer

00:03:13.390 --> 00:03:17.350
to interactively explore
high-dimensional data sets.

00:03:17.350 --> 00:03:19.140
On the education
side, General Assembly

00:03:19.140 --> 00:03:21.210
has done great work
teaching TensorFlow.

00:03:21.210 --> 00:03:23.500
NEHEMIAH LOURY: For
my final project,

00:03:23.500 --> 00:03:27.430
I was really interested in
doing lyrics generation.

00:03:27.430 --> 00:03:30.760
And TensorFlow was a
really great match for that

00:03:30.760 --> 00:03:34.510
because it allowed me to build
out and utilize the models

00:03:34.510 --> 00:03:36.172
that I needed to be successful.

00:03:36.172 --> 00:03:37.630
ZAK STONE: The
TensorFlow community

00:03:37.630 --> 00:03:40.330
is thriving around
the world, and we're

00:03:40.330 --> 00:03:43.910
excited about as many people
as possible being part of it.

00:03:43.910 --> 00:03:47.027
TensorFlow is an open-source
project for everyone.

00:03:47.027 --> 00:03:48.610
We're looking forward
to building this

00:03:48.610 --> 00:03:52.030
into something even better and
more useful and more powerful,

00:03:52.030 --> 00:03:55.680
in collaboration with the
whole worldwide community.

