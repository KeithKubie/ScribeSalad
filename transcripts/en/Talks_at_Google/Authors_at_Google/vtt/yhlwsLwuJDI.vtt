WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.832
[MUSIC PLAYING]

00:00:06.140 --> 00:00:07.545
KELLY WEINERSMITH: Hello.

00:00:07.545 --> 00:00:08.495
[APPLAUSE]

00:00:08.495 --> 00:00:10.160
Hi.

00:00:10.160 --> 00:00:11.010
That's a good start.

00:00:11.010 --> 00:00:13.347
ZACH WEINERSMITH: Yeah.

00:00:13.347 --> 00:00:15.680
KELLY WEINERSMITH: So we are
Zach and Kelly Weinersmith,

00:00:15.680 --> 00:00:17.810
and we wrote a book
called "SOONISH,

00:00:17.810 --> 00:00:20.060
10 Emerging Technologies
That Will Improve and/or Ruin

00:00:20.060 --> 00:00:21.750
Everything."

00:00:21.750 --> 00:00:23.360
I'm a parasitologist.

00:00:23.360 --> 00:00:24.980
I study parasites
that manipulate

00:00:24.980 --> 00:00:28.864
the behavior of their hosts,
and I work for Rice University.

00:00:28.864 --> 00:00:30.780
ZACH WEINERSMITH: And I
draw comics and stuff.

00:00:30.780 --> 00:00:33.426
[LAUGHTER]

00:00:33.426 --> 00:00:36.050
KELLY WEINERSMITH: So we decided
to write a book on technology.

00:00:36.050 --> 00:00:40.040
And in 2011, a group of policy
students at Hamilton College

00:00:40.040 --> 00:00:41.595
wrote a book called--

00:00:41.595 --> 00:00:42.450
ZACH WEINERSMITH:
Oh, it was a paper.

00:00:42.450 --> 00:00:44.158
KELLY WEINERSMITH: A
paper, sorry, called

00:00:44.158 --> 00:00:46.010
"Are Talking Heads
Blowing Hot Air?"

00:00:46.010 --> 00:00:47.426
And essentially,
they were looking

00:00:47.426 --> 00:00:50.060
at the predictive abilities
of 26 really popular pundits

00:00:50.060 --> 00:00:52.260
that were on lots of TV shows.

00:00:52.260 --> 00:00:53.960
And these pundits
ranged from being

00:00:53.960 --> 00:00:56.420
mostly right to mostly wrong.

00:00:56.420 --> 00:00:59.310
But importantly, they
all still had their jobs.

00:00:59.310 --> 00:01:02.090
And so we were like, we should
write a book about tech.

00:01:02.090 --> 00:01:05.090
where it doesn't matter if
we're right or wrong because,

00:01:05.090 --> 00:01:08.360
apparently, that doesn't impact
your ability to have a job.

00:01:08.360 --> 00:01:11.240
But actually, we're not
really interested in this book

00:01:11.240 --> 00:01:14.420
in making predictions because
we think that what's interesting

00:01:14.420 --> 00:01:17.330
isn't necessarily figuring out
how many years out a technology

00:01:17.330 --> 00:01:19.430
is, so much just
talking about what

00:01:19.430 --> 00:01:21.990
the amazing challenges that
people are working on right now

00:01:21.990 --> 00:01:22.490
are.

00:01:22.490 --> 00:01:25.640
And so we talk a lot more
about the technical hurdles

00:01:25.640 --> 00:01:27.435
that still need to be overcome.

00:01:27.435 --> 00:01:29.060
And then we talk a
little bit about how

00:01:29.060 --> 00:01:31.940
these technologies could make
everything awesome but also

00:01:31.940 --> 00:01:33.470
maybe horrible.

00:01:33.470 --> 00:01:35.900
And so, we give you a
little bit of a taste of one

00:01:35.900 --> 00:01:38.930
of the chapters in the book and
a little bit on one of the nota

00:01:38.930 --> 00:01:41.056
benes.

00:01:41.056 --> 00:01:41.930
What did want to say?

00:01:41.930 --> 00:01:42.700
ZACH WEINERSMITH: Yeah.

00:01:42.700 --> 00:01:43.610
KELLY WEINERSMITH: Oh, OK.

00:01:43.610 --> 00:01:45.210
So the 10 topics we
cover in the book--

00:01:45.210 --> 00:01:46.793
let's see if I can
remember them all--

00:01:46.793 --> 00:01:50.150
are cheap access to space,
asteroid mining, fusion,

00:01:50.150 --> 00:01:56.120
brain-computer interfaces,
bioprinting, augmented reality,

00:01:56.120 --> 00:01:59.254
robotic construction,
programmable matter--

00:01:59.254 --> 00:02:00.170
ZACH WEINERSMITH: BCI.

00:02:00.170 --> 00:02:01.370
KELLY WEINERSMITH:
Brain-Computer Interfaces, I

00:02:01.370 --> 00:02:02.220
thought I said that already.

00:02:02.220 --> 00:02:02.990
ZACH WEINERSMITH:
And then bioprinting.

00:02:02.990 --> 00:02:03.740
KELLY WEINERSMITH:
And bioprinting.

00:02:03.740 --> 00:02:04.660
Didn't I say bioprinting?

00:02:04.660 --> 00:02:05.135
[LAUGHTER]

00:02:05.135 --> 00:02:05.610
Yeah.

00:02:05.610 --> 00:02:06.650
I said both of those already.

00:02:06.650 --> 00:02:08.270
ZACH WEINERSMITH: So we're
still on eight now, huh?

00:02:08.270 --> 00:02:08.750
KELLY WEINERSMITH: That's fine.

00:02:08.750 --> 00:02:09.750
Anyway, buy the book.

00:02:09.750 --> 00:02:11.930
You'll find out the rest.

00:02:11.930 --> 00:02:13.480
That's our pitch.

00:02:13.480 --> 00:02:15.680
And at the end of a
lot of the chapters,

00:02:15.680 --> 00:02:17.892
we uncovered all
sorts of crazy stuff

00:02:17.892 --> 00:02:19.850
while we were doing the
research for this book.

00:02:19.850 --> 00:02:22.910
And that's a ton, a big part of
why this book was so much fun

00:02:22.910 --> 00:02:23.519
to write.

00:02:23.519 --> 00:02:25.310
And so, at the end of
some of the chapters,

00:02:25.310 --> 00:02:27.940
we talk about these weird
things that we encountered,

00:02:27.940 --> 00:02:30.120
and we include these
in our nota benes.

00:02:30.120 --> 00:02:32.690
And so we're going to
discuss one of the nota benes

00:02:32.690 --> 00:02:34.550
that we wrote in
the book as well.

00:02:34.550 --> 00:02:36.590
ZACH WEINERSMITH: It's not this
one, but this is a whole thing.

00:02:36.590 --> 00:02:38.381
KELLY WEINERSMITH: This
was really awesome.

00:02:38.381 --> 00:02:40.340
And we've got to talk to
so many amazing people

00:02:40.340 --> 00:02:41.870
while doing this book.

00:02:41.870 --> 00:02:45.950
But anyway, so we've been told
that, instead of talking about

00:02:45.950 --> 00:02:47.960
cheap access to space,
programmable matter

00:02:47.960 --> 00:02:50.400
would probably go over
best with this audience.

00:02:50.400 --> 00:02:53.210
So we're doing the
programmable matter chapter.

00:02:53.210 --> 00:02:54.390
So how about you start?

00:02:54.390 --> 00:02:55.000
ZACH WEINERSMITH:
Oh, I'll start?

00:02:55.000 --> 00:02:55.500
OK.

00:02:55.500 --> 00:02:56.270
Sure.

00:02:56.270 --> 00:02:58.386
So I feel like I
need to not talk

00:02:58.386 --> 00:03:00.260
too much because it's
Google, so I don't need

00:03:00.260 --> 00:03:02.180
to explain what are computers.

00:03:02.180 --> 00:03:02.850
I guess, right?

00:03:02.850 --> 00:03:04.600
You all know about those?

00:03:04.600 --> 00:03:05.100
Yeah.

00:03:05.100 --> 00:03:07.580
So the idea in general
with programmable matter

00:03:07.580 --> 00:03:10.220
is that, in the same way
that a computer is universal,

00:03:10.220 --> 00:03:12.530
you can make stuff
that's universal.

00:03:12.530 --> 00:03:14.480
And there are a lot of
different approaches

00:03:14.480 --> 00:03:15.630
to how you might do that.

00:03:15.630 --> 00:03:16.730
I think we go through
a couple of them here.

00:03:16.730 --> 00:03:18.171
The book's a lot more extensive.

00:03:18.171 --> 00:03:19.670
But when we say
programmable matter,

00:03:19.670 --> 00:03:21.044
we're kind of
condensing together

00:03:21.044 --> 00:03:24.230
a bunch of different fields
like self-reconfiguring matter.

00:03:24.230 --> 00:03:26.395
There's a book called
"Morphogenetic Engineering,"

00:03:26.395 --> 00:03:27.770
and I wish they'd
gone with that.

00:03:27.770 --> 00:03:29.780
That was really cool.

00:03:29.780 --> 00:03:32.360
But just stuff that
can reconfigure itself

00:03:32.360 --> 00:03:33.180
in different ways.

00:03:33.180 --> 00:03:36.790
So we're going to go
through a few ways.

00:03:36.790 --> 00:03:38.430
This is a little wordy.

00:03:38.430 --> 00:03:41.129
So this is my drawing
of Skyler Tibbits.

00:03:41.129 --> 00:03:42.420
Skyler Tibbits is a guy at MIT.

00:03:42.420 --> 00:03:45.260
He does what it calls
4D printing, which

00:03:45.260 --> 00:03:47.720
just means it's 3D
printing but the stuff

00:03:47.720 --> 00:03:49.180
does more once it's printed.

00:03:49.180 --> 00:03:51.950
So there are a bunch
of examples of this,

00:03:51.950 --> 00:03:55.990
but one we thought was cute
was he made this straw,

00:03:55.990 --> 00:03:58.790
and it's just made to the joints
are printed so that when intake

00:03:58.790 --> 00:04:01.422
water they bend in a way
that you, quote unquote,

00:04:01.422 --> 00:04:02.630
"program into the materials."

00:04:02.630 --> 00:04:04.937
So he made one
where you program it

00:04:04.937 --> 00:04:06.770
so you drop the stick
in water and it spells

00:04:06.770 --> 00:04:07.895
at MIT because he's at MIT.

00:04:07.895 --> 00:04:09.519
But what's cute about
that, we thought,

00:04:09.519 --> 00:04:11.090
is you could really
creep people out

00:04:11.090 --> 00:04:12.600
if they didn't know what it was.

00:04:12.600 --> 00:04:14.300
[LAUGHTER]

00:04:14.300 --> 00:04:15.980
You could make
programmable spaghetti,

00:04:15.980 --> 00:04:18.589
it just says like,
find help or something.

00:04:18.589 --> 00:04:19.130
I don't know.

00:04:23.610 --> 00:04:26.390
There's another one that's
theoretically more functional.

00:04:26.390 --> 00:04:28.390
I don't think the wood
thing was his, right?

00:04:28.390 --> 00:04:28.940
That was?

00:04:28.940 --> 00:04:29.390
KELLY WEINERSMITH: Right.

00:04:29.390 --> 00:04:29.760
Yeah.

00:04:29.760 --> 00:04:30.500
ZACH WEINERSMITH: So
there's this one project

00:04:30.500 --> 00:04:32.458
where it was essentially
wood that was designed

00:04:32.458 --> 00:04:33.764
to respond to humidity.

00:04:33.764 --> 00:04:35.930
And so it was like you can
give a building basically

00:04:35.930 --> 00:04:38.540
pores so that they can open
out or close up depending

00:04:38.540 --> 00:04:40.010
on ambient conditions.

00:04:40.010 --> 00:04:42.093
So there are a couple of
projects working on that.

00:04:42.093 --> 00:04:44.150
And in addition to just
kind of looking awesome,

00:04:44.150 --> 00:04:47.750
you have the potential for zero-
or low-energy environmental

00:04:47.750 --> 00:04:51.230
regulation mechanism, but
mostly it looks really cool.

00:04:51.230 --> 00:04:53.870
It looks like you're looking
at like a giant dead alien,

00:04:53.870 --> 00:04:55.785
so that's neat.

00:04:55.785 --> 00:04:57.410
KELLY WEINERSMITH:
So one of the things

00:04:57.410 --> 00:04:58.962
that Skyler Tibbits
pointed out was

00:04:58.962 --> 00:05:00.920
that the hard thing about
this technology right

00:05:00.920 --> 00:05:03.590
now is that there isn't
really good software written

00:05:03.590 --> 00:05:05.820
that programs in
information about joints

00:05:05.820 --> 00:05:08.010
and how they respond to
environmental conditions.

00:05:08.010 --> 00:05:09.780
And I know you guys
have like 20% of time

00:05:09.780 --> 00:05:14.190
that you get to spend on
anything else, so FYI.

00:05:14.190 --> 00:05:16.740
So then, the next category
that we talk about

00:05:16.740 --> 00:05:18.030
are origami robots.

00:05:18.030 --> 00:05:20.640
And we got really excited
about origami and robots,

00:05:20.640 --> 00:05:25.710
and so we had this guy
Jason Ku design an origami--

00:05:25.710 --> 00:05:26.790
what?

00:05:26.790 --> 00:05:27.210
ZACH WEINERSMITH: Robot.

00:05:27.210 --> 00:05:27.570
KELLY WEINERSMITH: Robot.

00:05:27.570 --> 00:05:28.510
OK, robot.

00:05:28.510 --> 00:05:30.420
But anyway, so if
you're interested

00:05:30.420 --> 00:05:33.600
in our origami robot, you
can download the design here

00:05:33.600 --> 00:05:35.350
and you can see
our origami robot.

00:05:35.350 --> 00:05:38.550
So origami, I'm sure you're
all familiar with origami.

00:05:38.550 --> 00:05:40.080
You have a sheet
of paper and you

00:05:40.080 --> 00:05:42.660
follow like some rules
for how you fold it,

00:05:42.660 --> 00:05:44.520
and you make it be
this different shape.

00:05:44.520 --> 00:05:46.980
Well, if you also put
actuators in there,

00:05:46.980 --> 00:05:48.579
you can get the
paper to fold itself

00:05:48.579 --> 00:05:50.370
so that you don't have
to do it on your own

00:05:50.370 --> 00:05:52.059
because why spend
the time doing that.

00:05:52.059 --> 00:05:53.850
It's really frustrating,
I know some people

00:05:53.850 --> 00:05:55.140
think it's beautiful.

00:05:55.140 --> 00:05:57.480
That's probably not me.

00:05:57.480 --> 00:06:00.870
And so, anyway, you can put
these actuators in there.

00:06:00.870 --> 00:06:02.850
With these actuators,
you can get the robots

00:06:02.850 --> 00:06:05.400
to walk around and
pick stuff up and do

00:06:05.400 --> 00:06:06.630
all sorts of crazy stuff.

00:06:06.630 --> 00:06:07.170
What were you going to say?

00:06:07.170 --> 00:06:07.730
ZACH WEINERSMITH: No, no.

00:06:07.730 --> 00:06:08.610
KELLY WEINERSMITH: Oh, OK.

00:06:08.610 --> 00:06:10.380
So one of the cool
things that they're

00:06:10.380 --> 00:06:12.100
working on getting
these robots to do

00:06:12.100 --> 00:06:14.860
is have medical applications.

00:06:14.860 --> 00:06:19.560
So Dr. Daniela Rus at MIT
made this origami robot out

00:06:19.560 --> 00:06:21.330
of sausage casing.

00:06:21.330 --> 00:06:23.410
And you, essentially,
fold it up.

00:06:23.410 --> 00:06:24.990
You stick it in ice.

00:06:24.990 --> 00:06:26.820
The person swallows the ice.

00:06:26.820 --> 00:06:30.120
And then, the origami bot, when
the ice dissolves, pops out.

00:06:30.120 --> 00:06:32.370
And then, you can
control it with a magnet.

00:06:32.370 --> 00:06:35.230
And apparently-- this number
blew my mind-- apparently,

00:06:35.230 --> 00:06:40.200
3,500 people every year
swallow those batteries

00:06:40.200 --> 00:06:41.887
that you find in watches.

00:06:41.887 --> 00:06:43.470
And having a
three-and-a-half-year-old

00:06:43.470 --> 00:06:46.140
and a one-year-old, I'm guessing
that they're all people under

00:06:46.140 --> 00:06:46.640
five.

00:06:46.640 --> 00:06:48.315
ZACH WEINERSMITH: I think
it's mostly children.

00:06:48.315 --> 00:06:48.900
KELLY WEINERSMITH:
It's mostly children.

00:06:48.900 --> 00:06:51.630
ZACH WEINERSMITH: But a couple
of people with certain tastes.

00:06:51.630 --> 00:06:52.260
KELLY WEINERSMITH: Sure.

00:06:52.260 --> 00:06:52.760
OK.

00:06:52.760 --> 00:06:53.490
[LAUGHTER]

00:06:53.490 --> 00:06:55.590
And so what this robot does--

00:06:55.590 --> 00:06:58.140
some percent of
those 3,500 people

00:06:58.140 --> 00:07:00.600
end up with the battery lodged
in part of their stomach,

00:07:00.600 --> 00:07:02.460
and they can't get it out
and then you have problems.

00:07:02.460 --> 00:07:03.543
If it passes, you're fine.

00:07:03.543 --> 00:07:05.260
But if it gets stuck,
you have problems.

00:07:05.260 --> 00:07:07.419
So this robot goes in,
opens up, and then you

00:07:07.419 --> 00:07:08.460
control it with a magnet.

00:07:08.460 --> 00:07:09.830
It connects to the battery.

00:07:09.830 --> 00:07:11.940
It yanks it out to dislodge it.

00:07:11.940 --> 00:07:13.890
And then it passes
with everything else

00:07:13.890 --> 00:07:15.510
and leaves naturally.

00:07:15.510 --> 00:07:18.030
And so we're hoping that
that little robot never

00:07:18.030 --> 00:07:21.180
really develops the ability to
consider its life objectively

00:07:21.180 --> 00:07:23.190
because it might be
a little depressed.

00:07:23.190 --> 00:07:24.720
But again, it's sausage casing.

00:07:24.720 --> 00:07:26.070
So it's going to
dissolve away, and it's

00:07:26.070 --> 00:07:27.690
going to go away, so you don't
have to worry about going

00:07:27.690 --> 00:07:29.010
over it feels about things.

00:07:29.010 --> 00:07:30.080
ZACH WEINERSMITH:
Because it'll be dead.

00:07:30.080 --> 00:07:31.788
KELLY WEINERSMITH:
Because it'll be dead.

00:07:31.788 --> 00:07:37.350
And so anyway, that's one use,
but Daniella is hoping that,

00:07:37.350 --> 00:07:41.070
first of all, at some point,
you won't have to control it

00:07:41.070 --> 00:07:42.960
remotely, it'll
work on its own .

00:07:42.960 --> 00:07:45.460
And then, she's also working
on other things like,

00:07:45.460 --> 00:07:47.250
can you get these bots
to deliver medicine

00:07:47.250 --> 00:07:48.410
to very particular areas?

00:07:48.410 --> 00:07:51.465
And so she's thinking about
the medical applications.

00:07:51.465 --> 00:07:53.340
But you can also make
these things big enough

00:07:53.340 --> 00:07:55.800
that it could be like a table
that, if someone's disabled,

00:07:55.800 --> 00:07:59.040
the table puts itself together
and then walks over to you

00:07:59.040 --> 00:08:01.680
so you don't have to go to
it, or a chair that can just

00:08:01.680 --> 00:08:04.230
fold up together and go
to where it needs to go.

00:08:04.230 --> 00:08:06.030
And so you can see or
you can imagine lots

00:08:06.030 --> 00:08:07.800
of cool applications for that.

00:08:11.141 --> 00:08:11.640
Go ahead.

00:08:14.610 --> 00:08:18.030
ZACH WEINERSMITH: So the
super-advanced paradigm

00:08:18.030 --> 00:08:21.220
that may never happen is called
the bucket-of-stuff paradigm,

00:08:21.220 --> 00:08:24.690
which is something
like this, perhaps.

00:08:24.690 --> 00:08:27.390
But the basic idea is it's
kind of like having a T-1000,

00:08:27.390 --> 00:08:33.187
but it fixes stuff in your
house instead of killing you.

00:08:33.187 --> 00:08:34.770
But if it's truly
universal, it should

00:08:34.770 --> 00:08:35.610
be able to turn into anything.

00:08:35.610 --> 00:08:36.485
It could be a wrench.

00:08:36.485 --> 00:08:39.150
It could be a phone.

00:08:39.150 --> 00:08:43.770
If you can command it
and it's on your side,

00:08:43.770 --> 00:08:48.610
you can just tell it to glob
over and do something for you.

00:08:48.610 --> 00:08:50.700
So there are a lot of
problems with this paradigm,

00:08:50.700 --> 00:08:53.180
and the privacy
thing is one actually

00:08:53.180 --> 00:08:55.590
that we'll probably
get to in a little bit.

00:08:55.590 --> 00:08:58.530
But we had a couple of people
actually working on this.

00:08:58.530 --> 00:09:00.600
The big problem is, I
think the smaller one was

00:09:00.600 --> 00:09:02.100
like a cubic centimeter.

00:09:02.100 --> 00:09:06.570
You have these little quote
unquote "atoms" that can move,

00:09:06.570 --> 00:09:08.830
can sense a little bit,
can dock with each other.

00:09:08.830 --> 00:09:09.714
That's important.

00:09:09.714 --> 00:09:11.880
And then, after that, some
of the stuff it's luxury.

00:09:11.880 --> 00:09:14.640
You might want a battery
on board each one.

00:09:14.640 --> 00:09:17.939
So miniaturizing this is
a really tough problem.

00:09:17.939 --> 00:09:19.980
And then, I get into the
math thing a little bit.

00:09:19.980 --> 00:09:20.730
KELLY WEINERSMITH:
That's right now.

00:09:20.730 --> 00:09:21.300
ZACH WEINERSMITH:
That's right now.

00:09:21.300 --> 00:09:21.846
Oh, god.

00:09:21.846 --> 00:09:22.110
KELLY WEINERSMITH:
The time is now.

00:09:22.110 --> 00:09:22.890
ZACH WEINERSMITH: Yes.

00:09:22.890 --> 00:09:24.750
So one of the really
interesting things we found out

00:09:24.750 --> 00:09:27.150
is that one of the difficulties
of building a T-1000

00:09:27.150 --> 00:09:31.770
to serve you is the math because
I think the way we say it is,

00:09:31.770 --> 00:09:35.064
if you imagine you
have a marching band

00:09:35.064 --> 00:09:36.480
that's got a shape
from say a star

00:09:36.480 --> 00:09:39.235
and do the university
logo, and say

00:09:39.235 --> 00:09:41.610
you only have 100 people,
that's not that hard a problem.

00:09:41.610 --> 00:09:43.651
Plus, each atom of that
system has a human brain,

00:09:43.651 --> 00:09:44.886
so that helps.

00:09:44.886 --> 00:09:46.260
But it's not that
a hard problem,

00:09:46.260 --> 00:09:48.472
but you imagine each time
you add another individual,

00:09:48.472 --> 00:09:50.430
the problem doesn't just
get one person harder.

00:09:50.430 --> 00:09:51.462
It scales.

00:09:51.462 --> 00:09:53.670
So if it's 1,000, that
becomes a really hard problem,

00:09:53.670 --> 00:09:57.040
people knowing where to go, what
to do if someone falls over.

00:09:57.040 --> 00:09:59.560
And then if you scale it
10,000 or a million or I guess

00:09:59.560 --> 00:10:01.726
the T-1000 would have a
billion and they're in three

00:10:01.726 --> 00:10:04.590
dimensions, and presumably
they're all sort of physical

00:10:04.590 --> 00:10:07.345
constraints at each point-- like
along the equivalent of a bone

00:10:07.345 --> 00:10:09.720
you have to all be docked with
each other a certain way--

00:10:09.720 --> 00:10:12.330
what happens is calculating
what everybody needs

00:10:12.330 --> 00:10:14.640
to do to make your
hand into a giant knife

00:10:14.640 --> 00:10:17.404
to kill that one
guy in the movie--

00:10:17.404 --> 00:10:19.070
I shouldn't be talking
about the T-1000.

00:10:19.070 --> 00:10:20.780
KELLY WEINERSMITH: You know way
too little about that movie,

00:10:20.780 --> 00:10:21.280
also.

00:10:21.280 --> 00:10:23.571
ZACH WEINERSMITH: So you want
to do that, it's actually

00:10:23.571 --> 00:10:24.947
a pretty tough math problem.

00:10:24.947 --> 00:10:27.030
You have to expend just
the right amount of atoms.

00:10:27.030 --> 00:10:27.930
They all have to get
to the right place.

00:10:27.930 --> 00:10:30.030
And crucially, if you
really want to kill a human,

00:10:30.030 --> 00:10:31.650
they have to go fast.

00:10:31.650 --> 00:10:34.786
So there's a version that
super-preliminary version

00:10:34.786 --> 00:10:37.410
that's called Kilobots, which if
want to visualize it's little,

00:10:37.410 --> 00:10:38.580
almost the size
of a watch battery

00:10:38.580 --> 00:10:39.913
canister with three little legs.

00:10:39.913 --> 00:10:41.599
And it kind of
moves by jiggling.

00:10:41.599 --> 00:10:43.890
And they're called Kilobots
because the original system

00:10:43.890 --> 00:10:46.740
had 1,024 of them--

00:10:46.740 --> 00:10:49.390
not 1,000 because
it's nerd town.

00:10:49.390 --> 00:10:51.360
But yes, 1,024 of these.

00:10:51.360 --> 00:10:54.059
And being aware of
this sort of problem,

00:10:54.059 --> 00:10:56.100
they wanted to have a
relatively simple algorithm

00:10:56.100 --> 00:10:57.550
that each of them were using.

00:10:57.550 --> 00:10:58.980
And so they did
get it where they

00:10:58.980 --> 00:11:00.330
could shape like a wrench--

00:11:00.330 --> 00:11:03.540
not when you could ever use, but
a 2D shape of a wrench and then

00:11:03.540 --> 00:11:05.280
change into, say, a
star or something.

00:11:05.280 --> 00:11:07.530
The problem was, I think it
was like it took six hours

00:11:07.530 --> 00:11:08.970
to go from one to the other.

00:11:08.970 --> 00:11:11.850
So they had this really sort
of simple perimeter crawling

00:11:11.850 --> 00:11:12.870
algorithm.

00:11:12.870 --> 00:11:16.800
And again, if you want
to kill somebody or have

00:11:16.800 --> 00:11:19.146
a phone instantly
appear in your hand

00:11:19.146 --> 00:11:20.437
and then have a kill somebody--

00:11:20.437 --> 00:11:21.720
[LAUGHTER]

00:11:21.720 --> 00:11:23.310
You're going to have a problem
unless you solve this math

00:11:23.310 --> 00:11:23.710
problem.

00:11:23.710 --> 00:11:23.830
I don't know.

00:11:23.830 --> 00:11:26.121
It might not even be solvable,
or at least not solvable

00:11:26.121 --> 00:11:29.832
in the sense of getting a way
to do it quickly and properly.

00:11:29.832 --> 00:11:32.290
KELLY WEINERSMITH: But this
audience can solve it probably.

00:11:32.290 --> 00:11:33.120
ZACH WEINERSMITH:
20% of your time

00:11:33.120 --> 00:11:34.800
to the robot murderer, yeah.

00:11:37.729 --> 00:11:39.270
KELLY WEINERSMITH:
So there are a lot

00:11:39.270 --> 00:11:41.490
of reasons why a bucket of
stuff could be a problem.

00:11:41.490 --> 00:11:43.560
So an ideal bucket
of stuff would

00:11:43.560 --> 00:11:46.680
be able to become like
a camera or a receiver,

00:11:46.680 --> 00:11:48.216
and it could
transmit information.

00:11:48.216 --> 00:11:49.590
And if these get
really tiny, you

00:11:49.590 --> 00:11:52.050
could imagine that it would be
very easy to spy on someone.

00:11:52.050 --> 00:11:54.220
You just put some of these
in all the hotel rooms,

00:11:54.220 --> 00:11:56.803
and then you can spy on everyone
and transmit that information

00:11:56.803 --> 00:11:57.370
anywhere.

00:11:57.370 --> 00:11:59.370
And then additionally, if you
can get this bucket of stuff

00:11:59.370 --> 00:12:01.140
to look like anything,
you could make

00:12:01.140 --> 00:12:02.250
it look like you're
clothes, and then you

00:12:02.250 --> 00:12:03.900
could bring it on
in flight and then

00:12:03.900 --> 00:12:06.000
you could turn it into
something more dangerous.

00:12:06.000 --> 00:12:07.770
And so, presumably,
the TSA would

00:12:07.770 --> 00:12:09.900
be trying to keep the
bucket of stuff out,

00:12:09.900 --> 00:12:11.524
but it's hard to know
how they would be

00:12:11.524 --> 00:12:12.970
able to do something like that.

00:12:12.970 --> 00:12:15.750
So then, you talked about
there's privacy concerns.

00:12:15.750 --> 00:12:17.610
And then there's
patenting concerns.

00:12:17.610 --> 00:12:20.100
So if you have a bucket of
stuff that can become anything,

00:12:20.100 --> 00:12:21.450
then why buy anything else?

00:12:21.450 --> 00:12:24.430
You can just tell your bucket
of stuff to become that thing.

00:12:24.430 --> 00:12:26.930
So it's hard to know how that
problem is going to be solved,

00:12:26.930 --> 00:12:28.920
although 3D printing
is sort of starting

00:12:28.920 --> 00:12:32.052
to deal with those
problems now kind of.

00:12:32.052 --> 00:12:33.510
ZACH WEINERSMITH:
No, a little bit.

00:12:33.510 --> 00:12:36.270
There's this issue of,
can you 3D print a gun?

00:12:36.270 --> 00:12:38.334
And that's a tough one.

00:12:38.334 --> 00:12:39.750
More importantly,
it's like people

00:12:39.750 --> 00:12:42.150
can't make gun laws if you
can always 3D print a gun.

00:12:42.150 --> 00:12:43.650
But if you have
programmable matter,

00:12:43.650 --> 00:12:46.170
it's like you have a permit
anything device that includes

00:12:46.170 --> 00:12:49.240
all sorts of banned things.

00:12:49.240 --> 00:12:50.430
So yeah.

00:12:50.430 --> 00:12:52.346
KELLY WEINERSMITH: Well,
then, another problem

00:12:52.346 --> 00:12:54.940
is, who is to blame when
something goes wrong?

00:12:54.940 --> 00:12:57.682
And, of course, this is the
self-driving car problem.

00:12:57.682 --> 00:12:59.640
If your self-driving car
gets into an accident,

00:12:59.640 --> 00:13:00.900
who do you blame?

00:13:00.900 --> 00:13:02.830
And so there are proposals
that you could use.

00:13:02.830 --> 00:13:05.250
So they have 4D printing stuff
that we were talking about.

00:13:05.250 --> 00:13:08.640
So maybe you could use that to
change the way airplane wings

00:13:08.640 --> 00:13:10.800
work depending on the
speed that you're going at,

00:13:10.800 --> 00:13:13.230
or maybe you could use it to
change your tires depending

00:13:13.230 --> 00:13:14.395
on the conditions.

00:13:14.395 --> 00:13:16.020
But what happens if
one of those things

00:13:16.020 --> 00:13:18.180
goes wrong at the wrong
moment and you die?

00:13:18.180 --> 00:13:19.620
Who's to blame for that?

00:13:19.620 --> 00:13:21.000
Is it the person
who designed it?

00:13:21.000 --> 00:13:23.460
And so, anyway, these decisions,
these sorts of problems

00:13:23.460 --> 00:13:26.220
need to get worked out.

00:13:26.220 --> 00:13:27.470
Any other negatives?

00:13:27.470 --> 00:13:29.428
ZACH WEINERSMITH: I think
Skylar Tibbits talked

00:13:29.428 --> 00:13:31.910
about a sort of general
negative of offloading

00:13:31.910 --> 00:13:34.020
our personal autonomy
to machines that

00:13:34.020 --> 00:13:35.330
just make decisions for us.

00:13:35.330 --> 00:13:35.950
KELLY WEINERSMITH: Oh,
we've already done that.

00:13:35.950 --> 00:13:37.184
ZACH WEINERSMITH: OK.

00:13:37.184 --> 00:13:38.850
KELLY WEINERSMITH:
That's not a problem.

00:13:38.850 --> 00:13:41.760
ZACH WEINERSMITH:
That's true, yeah.

00:13:41.760 --> 00:13:43.274
KELLY WEINERSMITH:
No, I'm kidding.

00:13:43.274 --> 00:13:45.690
So anyway, so then there is a
number of different benefits

00:13:45.690 --> 00:13:46.981
if you have this kind of stuff.

00:13:46.981 --> 00:13:49.800
So one, presumably we could
really cut down on waste.

00:13:49.800 --> 00:13:52.200
So if you had a bucket
that could become anything,

00:13:52.200 --> 00:13:53.727
then you could own
a lot less stuff

00:13:53.727 --> 00:13:56.310
because that bucket could become
your wrench and your plunger.

00:13:56.310 --> 00:13:57.685
So you don't need
a big tool kit.

00:13:57.685 --> 00:13:59.730
You've got this thing that
could become anything.

00:13:59.730 --> 00:14:01.230
We already talked
a little bit about

00:14:01.230 --> 00:14:04.110
how, if you have stuff on your
home that changes in response

00:14:04.110 --> 00:14:05.940
to ambient conditions,
you could maybe

00:14:05.940 --> 00:14:09.600
control internal conditions with
very low energy input, which

00:14:09.600 --> 00:14:11.415
we think is pretty exciting.

00:14:11.415 --> 00:14:13.290
Then we talked about
the programmable matter,

00:14:13.290 --> 00:14:15.090
like those little
origami robots which

00:14:15.090 --> 00:14:17.640
could help deliver medicine
to very particular locations

00:14:17.640 --> 00:14:20.010
or dislodge batteries.

00:14:20.010 --> 00:14:21.027
Anything else?

00:14:21.027 --> 00:14:22.610
ZACH WEINERSMITH:
Anymore awesomeness?

00:14:22.610 --> 00:14:23.610
KELLY WEINERSMITH: Yeah.

00:14:23.610 --> 00:14:25.020
ZACH WEINERSMITH: There's tons.

00:14:25.020 --> 00:14:26.970
Mostly, I want a
toy origami thing.

00:14:26.970 --> 00:14:28.950
I don't know.

00:14:28.950 --> 00:14:32.544
KELLY WEINERSMITH: A toy origami
thing would be pretty awesome.

00:14:32.544 --> 00:14:33.585
ZACH WEINERSMITH: Thanks.

00:14:33.585 --> 00:14:34.530
KELLY WEINERSMITH:
You're welcome.

00:14:34.530 --> 00:14:35.760
ZACH WEINERSMITH:
So one version we

00:14:35.760 --> 00:14:37.218
don't really talk
about in a second

00:14:37.218 --> 00:14:39.109
I think is the
swarm robots version

00:14:39.109 --> 00:14:40.150
of how you might do that.

00:14:40.150 --> 00:14:43.695
It's kind of related to
how a T-1000 would work.

00:14:43.695 --> 00:14:44.820
I keep coming back to that.

00:14:44.820 --> 00:14:47.700
But there are
practical utilities

00:14:47.700 --> 00:14:50.159
to having like a large swarm
of robots that can reconfigure

00:14:50.159 --> 00:14:52.200
with each other because
if you want to, say, send

00:14:52.200 --> 00:14:53.820
a bunch of robots
into a disaster zone

00:14:53.820 --> 00:14:56.250
a swarm might be
preferable to just one

00:14:56.250 --> 00:14:58.950
because, if something
breaks down, it would be OK.

00:14:58.950 --> 00:15:01.927
And also, by being able to break
apart and come back together,

00:15:01.927 --> 00:15:03.760
they can navigate a
little more effectively.

00:15:03.760 --> 00:15:06.900
So we looked at one
group It's something

00:15:06.900 --> 00:15:08.802
like you'd have 10
robots about this big,

00:15:08.802 --> 00:15:10.260
and one of the
tricks they could do

00:15:10.260 --> 00:15:13.384
is navigate through
like a dip in the ground

00:15:13.384 --> 00:15:14.550
by latching onto each other.

00:15:14.550 --> 00:15:16.550
And so they had this
really cute algorithm where

00:15:16.550 --> 00:15:19.490
one robot realizes there's
a big dip, and then it

00:15:19.490 --> 00:15:21.570
I think on that one it
was a lighting system.

00:15:21.570 --> 00:15:23.531
So it signaled, hey,
someone docked with me.

00:15:23.531 --> 00:15:25.530
And then, the other robots
would get the signal.

00:15:25.530 --> 00:15:28.113
They'd back up until they had a
train of the appropriate size,

00:15:28.113 --> 00:15:29.705
and then they could
go over the gap.

00:15:29.705 --> 00:15:32.080
There are all sorts of similar
things you can program in.

00:15:32.080 --> 00:15:34.680
They could effectively
tightrope walk or at least

00:15:34.680 --> 00:15:36.810
cross a narrow passage by
having two side by side

00:15:36.810 --> 00:15:38.790
going the right way.

00:15:38.790 --> 00:15:41.820
Also, can I talk about
the evolving robot thing?

00:15:41.820 --> 00:15:43.770
I know it's not
germane to the talk.

00:15:43.770 --> 00:15:45.853
KELLY WEINERSMITH: Well,
we've got to get to the--

00:15:45.853 --> 00:15:47.970
ZACH WEINERSMITH:
All right, all right.

00:15:47.970 --> 00:15:49.470
KELLY WEINERSMITH:
Maybe at the end.

00:15:49.470 --> 00:15:51.210
Someone can ask about that.

00:15:51.210 --> 00:15:53.250
But in general, if you
have this swarm that

00:15:53.250 --> 00:15:55.500
can solve its own
problems as it goes,

00:15:55.500 --> 00:15:57.840
you can send it into hazardous
areas like New Jersey

00:15:57.840 --> 00:16:00.869
and have it solve
problems and fix things.

00:16:00.869 --> 00:16:02.910
But to be honest, what we
were most excited about

00:16:02.910 --> 00:16:05.670
was imagine you
go to IKEA and you

00:16:05.670 --> 00:16:08.107
buy that table and the
table just unfolds itself

00:16:08.107 --> 00:16:09.690
and you don't have
to put it together.

00:16:09.690 --> 00:16:12.100
That would save like
a billion human hours

00:16:12.100 --> 00:16:15.030
if you didn't have to put
together your IKEA stuff.

00:16:15.030 --> 00:16:17.400
So we're going to talk
about the nota bene

00:16:17.400 --> 00:16:19.920
on how it will end
for all of humanity.

00:16:19.920 --> 00:16:22.530
So we came across a lot
of really awesome stories

00:16:22.530 --> 00:16:25.380
about human-robot interactions
that made us not particularly

00:16:25.380 --> 00:16:28.140
optimistic about humanity's
ability to persist

00:16:28.140 --> 00:16:30.240
in the face of smart robots.

00:16:30.240 --> 00:16:33.112
So do you want to talk
about Promobot first?

00:16:33.112 --> 00:16:34.320
ZACH WEINERSMITH: Sure, sure.

00:16:34.320 --> 00:16:39.141
So Promobot-- I'm definitely
pronouncing it wrong.

00:16:39.141 --> 00:16:40.140
I'm not going to try it.

00:16:40.140 --> 00:16:41.680
It's a Russian robot company.

00:16:41.680 --> 00:16:43.680
And this is just a little
sort of-- it was cute.

00:16:43.680 --> 00:16:45.390
There's this robot,
and it was designed

00:16:45.390 --> 00:16:47.040
to be like a robot assistant.

00:16:47.040 --> 00:16:49.020
It can do things like
remember human faces

00:16:49.020 --> 00:16:50.700
and learn things
about its environment.

00:16:50.700 --> 00:16:53.400
And it apparently
keeps trying to escape.

00:16:53.400 --> 00:16:55.255
[LAUGHTER]

00:16:55.255 --> 00:16:56.880
So they had, what
was it, two incidents

00:16:56.880 --> 00:16:59.405
where it got out and
ran down the street?

00:16:59.405 --> 00:17:00.780
KELLY WEINERSMITH:
And it ran out

00:17:00.780 --> 00:17:02.280
of batteries in the
middle of the road.

00:17:02.280 --> 00:17:03.720
So instead of
helping the elderly

00:17:03.720 --> 00:17:05.428
like I think it was
supposed to be doing,

00:17:05.428 --> 00:17:08.190
it instead died in the middle of
the street and stopped traffic.

00:17:08.190 --> 00:17:10.079
So we need our
robots to stop trying

00:17:10.079 --> 00:17:13.410
to escape because they're
not very helpful there.

00:17:13.410 --> 00:17:15.960
But then, our favorite
robot was named Gaia.

00:17:15.960 --> 00:17:18.930
So there was a Harvard
undergrad named Serena Booth,

00:17:18.930 --> 00:17:22.800
and she wanted to know how
much people trust robots.

00:17:22.800 --> 00:17:24.635
And so she lived in
the dorms, and there

00:17:24.635 --> 00:17:26.010
are a number of
different reasons

00:17:26.010 --> 00:17:27.510
why if you live in
Harvard dorms you

00:17:27.510 --> 00:17:30.490
shouldn't be letting anything
or anyone into the dorms.

00:17:30.490 --> 00:17:32.190
So first of all,
apparently-- and this

00:17:32.190 --> 00:17:34.680
totally creeps me out--
apparently at Harvard,

00:17:34.680 --> 00:17:36.651
tourists like to
take photos of dorms.

00:17:36.651 --> 00:17:38.400
ZACH WEINERSMITH: Like
the inside of them?

00:17:38.400 --> 00:17:39.030
KELLY WEINERSMITH:
The inside of dorms.

00:17:39.030 --> 00:17:40.490
So they'll come up
to the dorm window.

00:17:40.490 --> 00:17:41.250
You're shaking your head.

00:17:41.250 --> 00:17:42.208
Did that happen to you?

00:17:42.208 --> 00:17:43.182
Were you at Harvard?

00:17:43.182 --> 00:17:45.430
AUDIENCE: Just a
lot of universities.

00:17:45.430 --> 00:17:46.980
We have it at Google, too.

00:17:46.980 --> 00:17:49.240
KELLY WEINERSMITH: What the
heck is wrong with people?

00:17:49.240 --> 00:17:50.819
So anyway, they come
up to the window

00:17:50.819 --> 00:17:52.860
and they put the camera
up and they take a photo.

00:17:52.860 --> 00:17:55.590
So people are trying to
intrude on privacy all the time

00:17:55.590 --> 00:17:57.360
if you're in these dorms.

00:17:57.360 --> 00:17:59.240
And then, additionally,
the Harvard dorms

00:17:59.240 --> 00:18:01.110
that had bomb threats
somewhat recently--

00:18:01.110 --> 00:18:03.570
so everyone had been
written recently and told,

00:18:03.570 --> 00:18:05.710
do not let anyone
into the dorms.

00:18:05.710 --> 00:18:08.220
They didn't specify no
robots, but they probably

00:18:08.220 --> 00:18:10.210
assumed you shouldn't
be letting anything in.

00:18:10.210 --> 00:18:12.474
And then, finally, there
were a bunch of thefts.

00:18:12.474 --> 00:18:14.640
So the students knew you
shouldn't be letting anyone

00:18:14.640 --> 00:18:15.610
to the dorms.

00:18:15.610 --> 00:18:18.750
So what Serena did is she
created a robot that she

00:18:18.750 --> 00:18:20.317
could operate remotely.

00:18:20.317 --> 00:18:22.650
So she was sitting at a table
pretending to do her work,

00:18:22.650 --> 00:18:24.630
but she was really
operating this robot.

00:18:24.630 --> 00:18:26.940
And it would come up to
people first by themselves

00:18:26.940 --> 00:18:29.504
and say, hey, can let
you me into the dorm?

00:18:29.504 --> 00:18:31.920
And I think something like 21%
of the time they'd say yes,

00:18:31.920 --> 00:18:34.410
but usually they'd be like no,
you can't get in the dorms.

00:18:34.410 --> 00:18:38.790
But if the robot approached
groups, 71% of the time

00:18:38.790 --> 00:18:39.450
they let it in.

00:18:39.450 --> 00:18:41.520
So notes to any
robots out there,

00:18:41.520 --> 00:18:43.350
humans are real dumb in groups.

00:18:43.350 --> 00:18:48.540
And then, additionally, if
she gave the robot cookies,

00:18:48.540 --> 00:18:50.800
almost everyone let it in.

00:18:50.800 --> 00:18:54.870
And so it was carrying a box
from a fancy local cookie

00:18:54.870 --> 00:18:57.870
chain, but it actually only
had grocery store, crummy

00:18:57.870 --> 00:18:59.700
snickerdoodle cookies in there.

00:18:59.700 --> 00:19:02.010
And so, apparently, for
the price of like dollar

00:19:02.010 --> 00:19:05.580
snickerdoodles, people will
put their entire dorm at risk

00:19:05.580 --> 00:19:07.092
to let the cookies in.

00:19:07.092 --> 00:19:09.050
Do you want to talk about
the emergency robots,

00:19:09.050 --> 00:19:09.560
or do you want me to?

00:19:09.560 --> 00:19:10.660
ZACH WEINERSMITH: I think
you're more the expert on that.

00:19:10.660 --> 00:19:11.576
KELLY WEINERSMITH: OK.

00:19:11.576 --> 00:19:14.950
So finally, there was a PhD
student named Paul Robinette,

00:19:14.950 --> 00:19:17.340
and he was at Georgia
Institute of Technology.

00:19:17.340 --> 00:19:19.830
And he wanted to know how
much people would trust robots

00:19:19.830 --> 00:19:21.810
in an emergency situation.

00:19:21.810 --> 00:19:24.060
So first, he started off
with sort of like low stakes.

00:19:24.060 --> 00:19:25.260
There were some
undergrads who thought

00:19:25.260 --> 00:19:26.580
they were doing a survey.

00:19:26.580 --> 00:19:28.560
So they came in, and
the robot brought them

00:19:28.560 --> 00:19:30.030
to the survey room.

00:19:30.030 --> 00:19:32.550
And they did the survey,
and then the experimenters

00:19:32.550 --> 00:19:35.250
released smoke and set
off the fire alarms.

00:19:35.250 --> 00:19:38.520
And a lot of the undergrads,
instead of going out

00:19:38.520 --> 00:19:39.990
the door that they
just came in--

00:19:39.990 --> 00:19:41.865
so they knew how to get
out of the building--

00:19:41.865 --> 00:19:43.419
followed the robot.

00:19:43.419 --> 00:19:45.210
At first, we were like,
well, that's weird.

00:19:45.210 --> 00:19:47.501
And then, we watched the
video, and it was really weird

00:19:47.501 --> 00:19:50.170
because that is a
slow-moving robot.

00:19:50.170 --> 00:19:53.350
It was just like crawling along.

00:19:53.350 --> 00:19:54.810
But it gets worse.

00:19:54.810 --> 00:19:57.060
So then, they had
a situation where

00:19:57.060 --> 00:20:00.840
the robot went to the wrong
room and circled the wrong room

00:20:00.840 --> 00:20:02.820
and then went to
the survey room--

00:20:02.820 --> 00:20:04.632
again, moving real slow.

00:20:04.632 --> 00:20:06.090
And then, they did
the thing again.

00:20:06.090 --> 00:20:07.740
And still, most
of the undergrads

00:20:07.740 --> 00:20:09.450
follow that robot
instead of going out

00:20:09.450 --> 00:20:10.726
the door that they knew.

00:20:10.726 --> 00:20:12.600
And then, finally, there
was a last treatment

00:20:12.600 --> 00:20:15.210
with I think only six students,
so it's a small sample size.

00:20:15.210 --> 00:20:18.300
But the robot went into
a corner and started

00:20:18.300 --> 00:20:21.580
going like this-- like this
is where the survey is.

00:20:21.580 --> 00:20:24.180
And an experimenter came
out and said, I'm sorry,

00:20:24.180 --> 00:20:25.800
this robot is broken.

00:20:25.800 --> 00:20:28.537
They use the words,
this robot is broken.

00:20:28.537 --> 00:20:30.120
And then they went
to a different room

00:20:30.120 --> 00:20:31.410
to do the survey.

00:20:31.410 --> 00:20:33.210
And they set off
the smoke alarm,

00:20:33.210 --> 00:20:34.590
and some students followed it.

00:20:34.590 --> 00:20:36.370
And then, I think
Paul was like, I'm

00:20:36.370 --> 00:20:38.120
just going to see how
far I can push this.

00:20:38.120 --> 00:20:39.960
And so, in one situation,
he had the robot

00:20:39.960 --> 00:20:43.260
go to her room that
was blocked by a couch,

00:20:43.260 --> 00:20:46.650
all the lights were shut off,
and there was no exit sign.

00:20:46.650 --> 00:20:49.620
And the robot started
pointing at the dark room,

00:20:49.620 --> 00:20:52.110
and there were students who
had to be retrieved eventually

00:20:52.110 --> 00:20:54.885
by the experimenters because
they would not leave the robot.

00:20:57.490 --> 00:20:59.560
So anyway, this blows my mind.

00:20:59.560 --> 00:21:03.576
So this robot looked
really not human-like.

00:21:03.576 --> 00:21:05.200
It was a very
dumb-looking, slow robot.

00:21:05.200 --> 00:21:05.700
ZACH WEINERSMITH:
Trash can on wheels.

00:21:05.700 --> 00:21:06.550
KELLY WEINERSMITH:
Paul did a great job,

00:21:06.550 --> 00:21:08.640
but it looks like a
trash can on wheels.

00:21:08.640 --> 00:21:11.910
So the point is, you don't
need a T-1000 to trick humanity

00:21:11.910 --> 00:21:12.660
to their doom.

00:21:12.660 --> 00:21:14.500
It just needs to be
a trash can on wheels

00:21:14.500 --> 00:21:16.395
that's carrying
cookies and humanity

00:21:16.395 --> 00:21:18.210
is in a lot of trouble.

00:21:18.210 --> 00:21:23.100
And so, if the robots ever rise
up, we're done for perhaps.

00:21:23.100 --> 00:21:24.882
But so, anyway, that's
just a taste of one

00:21:24.882 --> 00:21:26.840
of the chapters in the
book and one of the nota

00:21:26.840 --> 00:21:28.290
benes that we did.

00:21:28.290 --> 00:21:31.630
We did 10 chapters, eight of
which we've told you about.

00:21:31.630 --> 00:21:35.130
Maybe we'll remember if you
ask use a question about it.

00:21:35.130 --> 00:21:36.610
We only spent two years on this.

00:21:36.610 --> 00:21:38.080
ZACH WEINERSMITH:
Asteroid mining.

00:21:38.080 --> 00:21:39.598
KELLY WEINERSMITH: I said that.

00:21:39.598 --> 00:21:41.430
[LAUGHTER]

00:21:41.430 --> 00:21:43.880
Synthetic biology and
precision medicine.

00:21:43.880 --> 00:21:44.670
10.

00:21:44.670 --> 00:21:47.230
Anyway, shouldn't have
been so excited about that.

00:21:47.230 --> 00:21:49.170
So anyway, that is
the book, and we

00:21:49.170 --> 00:21:51.420
hope you enjoy it, those of
you who decide to read it.

00:21:51.420 --> 00:21:53.520
And we would be happy
to answer questions now.

00:21:53.520 --> 00:21:56.370
So if you want to line
up at the microphone,

00:21:56.370 --> 00:21:59.250
we would love to hear what you
would like to ask us about.

00:22:01.690 --> 00:22:03.690
Or, we can tell you about
cheap access to space.

00:22:03.690 --> 00:22:03.910
ZACH WEINERSMITH: No, no.

00:22:03.910 --> 00:22:04.650
We got some humans.

00:22:04.650 --> 00:22:05.566
KELLY WEINERSMITH: OK.

00:22:14.980 --> 00:22:17.430
AUDIENCE: So I'm a
big fan of the comic.

00:22:17.430 --> 00:22:20.162
Zach, you look exactly
like your picture.

00:22:20.162 --> 00:22:22.120
KELLY WEINERSMITH: But
he's stressed right now.

00:22:22.120 --> 00:22:23.078
ZACH WEINERSMITH: Yeah.

00:22:23.078 --> 00:22:28.802
KELLY WEINERSMITH: Yeah,
which I make him do in public.

00:22:28.802 --> 00:22:30.260
AUDIENCE: So I
preordered the book,

00:22:30.260 --> 00:22:31.850
and I finally
received it yesterday.

00:22:31.850 --> 00:22:33.944
And I actually only
received two bookplates.

00:22:33.944 --> 00:22:35.360
So if you want one
back to give it

00:22:35.360 --> 00:22:37.505
to another eligible purchaser.

00:22:37.505 --> 00:22:38.546
ZACH WEINERSMITH: No, no.

00:22:38.546 --> 00:22:40.104
You can keep the sticker

00:22:40.104 --> 00:22:41.520
AUDIENCE: Actually,
I was curious.

00:22:41.520 --> 00:22:43.061
I only got through
the first chapter,

00:22:43.061 --> 00:22:45.382
and I was a little disappointed
to see that the space

00:22:45.382 --> 00:22:47.090
elevator was already
in the first chapter

00:22:47.090 --> 00:22:48.631
because I was looking
forward to that

00:22:48.631 --> 00:22:52.040
to be the climax of the book.

00:22:52.040 --> 00:22:54.740
Could you talk about how some
of these things interact?

00:22:54.740 --> 00:22:58.130
So like the bucket of goo,
or the bucket of stuff,

00:22:58.130 --> 00:23:01.430
could also be used
in space exploration.

00:23:01.430 --> 00:23:04.420
You wouldn't have to
bring every possible tool.

00:23:04.420 --> 00:23:04.590
ZACH WEINERSMITH:
I don't remember

00:23:04.590 --> 00:23:05.630
if we talked about
that, but that was

00:23:05.630 --> 00:23:06.838
one of the things brought up.

00:23:06.838 --> 00:23:09.620
Actually, space comes up
surprisingly often in the book,

00:23:09.620 --> 00:23:11.161
and we were trying
to figure out why.

00:23:11.161 --> 00:23:14.360
I think our general realization
was that space is also

00:23:14.360 --> 00:23:15.719
part of the universe.

00:23:15.719 --> 00:23:18.260
So it's like you're still going
to eat most of the same stuff

00:23:18.260 --> 00:23:19.460
once you're in space.

00:23:19.460 --> 00:23:21.690
But yeah, you'll need it
much more efficiently.

00:23:21.690 --> 00:23:23.250
So I think we talk
a little about it.

00:23:23.250 --> 00:23:25.160
There's a little section in
the book on 3D-printed food

00:23:25.160 --> 00:23:27.659
which they care about for space
kind of for the same reason.

00:23:27.659 --> 00:23:30.225
It's efficient
food-packing mechanism.

00:23:30.225 --> 00:23:31.850
KELLY WEINERSMITH:
We talk a little bit

00:23:31.850 --> 00:23:33.933
about recouping where
they're trying to figure out

00:23:33.933 --> 00:23:37.550
how to reuse feces
and turn it back

00:23:37.550 --> 00:23:39.350
into food in 3D print food.

00:23:39.350 --> 00:23:41.210
But they're very efficient.

00:23:41.210 --> 00:23:42.120
Yeah, exactly.

00:23:42.120 --> 00:23:44.485
That face is the right face.

00:23:44.485 --> 00:23:48.110
ZACH WEINERSMITH: The project
about using human waste

00:23:48.110 --> 00:23:51.390
to feed humans contains the
phrase closing the loop--

00:23:51.390 --> 00:23:53.690
[LAUGHTER]

00:23:53.690 --> 00:23:56.720
Which I think gives you an
insight into food scientists.

00:23:56.720 --> 00:23:57.637
We need to solve this.

00:23:57.637 --> 00:23:59.761
KELLY WEINERSMITH: Not all
loops need to be closed.

00:23:59.761 --> 00:24:01.340
ZACH WEINERSMITH:
Gone on too long.

00:24:01.340 --> 00:24:02.673
Sorry, did answer your question?

00:24:02.673 --> 00:24:03.955
I don't know.

00:24:03.955 --> 00:24:04.580
AUDIENCE: Yeah.

00:24:04.580 --> 00:24:06.020
Thank you.

00:24:06.020 --> 00:24:08.145
ZACH WEINERSMITH: It went
on a weird tangent, yeah.

00:24:10.010 --> 00:24:11.510
AUDIENCE: So this
is a question just

00:24:11.510 --> 00:24:15.774
for Zach, are you intentionally
low-key cosplaying Shaggy?

00:24:15.774 --> 00:24:16.940
ZACH WEINERSMITH: Oh my god.

00:24:16.940 --> 00:24:18.148
KELLY WEINERSMITH: Oh my god.

00:24:18.148 --> 00:24:21.486
[LAUGHTER]

00:24:23.971 --> 00:24:27.450
[APPLAUSE]

00:24:30.040 --> 00:24:32.270
I'm embarrassed.

00:24:32.270 --> 00:24:34.020
AUDIENCE: Should we
have another question?

00:24:34.020 --> 00:24:34.830
I thought she was going to--

00:24:34.830 --> 00:24:35.790
ZACH WEINERSMITH: Yeah.

00:24:35.790 --> 00:24:39.690
That was more of an
assertion than a question.

00:24:39.690 --> 00:24:42.007
I like your shirt, by the way.

00:24:42.007 --> 00:24:44.340
AUDIENCE: So what was the
thing you wanted to talk about

00:24:44.340 --> 00:24:45.420
with evolutionary robots?

00:24:45.420 --> 00:24:46.320
ZACH WEINERSMITH: Yeah.

00:24:46.320 --> 00:24:46.770
KELLY WEINERSMITH: The what?

00:24:46.770 --> 00:24:47.780
ZACH WEINERSMITH: The
Roombots that evolve.

00:24:47.780 --> 00:24:49.113
KELLY WEINERSMITH: Oh, go ahead.

00:24:49.113 --> 00:24:50.192
ZACH WEINERSMITH: So OK.

00:24:50.192 --> 00:24:51.750
You remember, we
talked about how

00:24:51.750 --> 00:24:53.310
there's a one-centimeter
version of

00:24:53.310 --> 00:24:54.630
the bucket-of-stuff paradigm.

00:24:54.630 --> 00:24:57.330
There's a slightly more
currently plausible version

00:24:57.330 --> 00:25:01.690
called Roombots, which is
totally worth looking up.

00:25:01.690 --> 00:25:03.750
I use a search engine
called AltaVista.

00:25:03.750 --> 00:25:09.990
[LAUGHTER]

00:25:09.990 --> 00:25:12.060
We're talking at the one
guy at AltaVista next.

00:25:12.060 --> 00:25:13.350
That's our next stop.

00:25:13.350 --> 00:25:16.670
[LAUGHTER]

00:25:16.670 --> 00:25:17.170
No.

00:25:17.170 --> 00:25:19.830
But these Roombots,
you can visualize

00:25:19.830 --> 00:25:23.490
like a thing about this big
that's really two hemispheres.

00:25:23.490 --> 00:25:24.750
It's not literally spherical.

00:25:24.750 --> 00:25:28.830
It's kind of like between
a sphere and a cube.

00:25:28.830 --> 00:25:30.810
Why they do that is so
that an individual one

00:25:30.810 --> 00:25:32.114
can kind of roll along.

00:25:32.114 --> 00:25:33.780
And all they do, they
can detect things,

00:25:33.780 --> 00:25:36.660
they can transmit and receive
signals, and they can dock.

00:25:36.660 --> 00:25:40.595
And so the idea with
it is you basically

00:25:40.595 --> 00:25:42.720
have these little things
that can roll around stick

00:25:42.720 --> 00:25:44.490
to each other and they can
turn to light it's cold room

00:25:44.490 --> 00:25:46.269
but because the idea
is to make furniture.

00:25:46.269 --> 00:25:48.060
So for example, you
could have 20 of these.

00:25:48.060 --> 00:25:49.393
They make the legs of the table.

00:25:49.393 --> 00:25:51.872
And then one, literally,
would go grip a tabletop maybe

00:25:51.872 --> 00:25:53.330
and then just sort
of carry it up--

00:25:53.330 --> 00:25:55.350
because they can climb walls
and have the appropriate gripper

00:25:55.350 --> 00:25:56.172
type--

00:25:56.172 --> 00:25:56.880
and form a table.

00:25:56.880 --> 00:25:59.442
And then, best of all, the
table can walk over to you.

00:25:59.442 --> 00:26:00.900
It probably would
roll over to you,

00:26:00.900 --> 00:26:03.750
but if you could get it to
walk, that would be really cool.

00:26:03.750 --> 00:26:05.730
And so the nice
idea with this is

00:26:05.730 --> 00:26:08.400
it would be good for
elder care, for people

00:26:08.400 --> 00:26:11.067
who you can't do for
themselves as well as others.

00:26:11.067 --> 00:26:12.650
But there's this
really cool project--

00:26:12.650 --> 00:26:14.275
and I don't think,
I don't know if this

00:26:14.275 --> 00:26:17.460
has been done in real life,
it's been done in simulations--

00:26:17.460 --> 00:26:19.110
but the way genetic
algorithms work

00:26:19.110 --> 00:26:22.380
is you would say that,
say, a pile of Roombots

00:26:22.380 --> 00:26:25.560
configure somehow and go as
fast as you can across the room.

00:26:25.560 --> 00:26:28.650
And then, once having done
that, you could mutate it.

00:26:28.650 --> 00:26:30.270
And so people tried this.

00:26:30.270 --> 00:26:33.060
And so what's really cool is
you can create a system that's

00:26:33.060 --> 00:26:35.400
in real life made of
these individual robots,

00:26:35.400 --> 00:26:36.870
and you can tell
them, hey, mutate

00:26:36.870 --> 00:26:40.140
based on your last couple
versions that worked.

00:26:40.140 --> 00:26:42.330
And maybe you arrive at
new design configurations

00:26:42.330 --> 00:26:44.038
that you hadn't thought
of, which is just

00:26:44.038 --> 00:26:46.080
kind of neat because it's
like genetic algorithms

00:26:46.080 --> 00:26:47.746
but they're actually
having to interface

00:26:47.746 --> 00:26:49.890
with reality instead of a
simulation, which, to me,

00:26:49.890 --> 00:26:50.860
that's really cool.

00:26:50.860 --> 00:26:51.300
KELLY WEINERSMITH:
So now we have

00:26:51.300 --> 00:26:52.970
to talk about the
swarm morph video.

00:26:52.970 --> 00:26:53.610
ZACH WEINERSMITH: Swarm morph?

00:26:53.610 --> 00:26:54.540
KELLY WEINERSMITH: So
there was another group

00:26:54.540 --> 00:26:56.280
that was doing
genetic algorithms

00:26:56.280 --> 00:27:00.150
to try to get their similar
blocks to solve problems.

00:27:00.150 --> 00:27:02.910
And we did not think that
this was necessary at all,

00:27:02.910 --> 00:27:04.500
but someone directed
us to a video

00:27:04.500 --> 00:27:07.590
where, before they would
try a new configuration,

00:27:07.590 --> 00:27:09.540
they would come
together and then they

00:27:09.540 --> 00:27:11.760
would rub against
each other for a while

00:27:11.760 --> 00:27:13.080
as though they were mating.

00:27:13.080 --> 00:27:15.315
And this went on
for like 25 seconds.

00:27:15.315 --> 00:27:16.349
[LAUGHTER]

00:27:16.349 --> 00:27:17.640
ZACH WEINERSMITH: That's right.

00:27:17.640 --> 00:27:20.202
It sounded like some
administration person had said,

00:27:20.202 --> 00:27:22.410
make them mate, and they
just taken it too literally.

00:27:22.410 --> 00:27:23.410
KELLY WEINERSMITH: Right.

00:27:23.410 --> 00:27:25.470
And then. they'd essentially
just send directions

00:27:25.470 --> 00:27:28.065
to a 3D printer, so it
was not at all necessary.

00:27:28.065 --> 00:27:29.270
[LAUGHTER]

00:27:29.270 --> 00:27:31.710
It was not a necessary
component of the process,

00:27:31.710 --> 00:27:32.970
but it still did it anyway.

00:27:32.970 --> 00:27:34.995
We took great joy in
watching that video.

00:27:34.995 --> 00:27:35.720
ZACH WEINERSMITH: I
forget about that, yeah.

00:27:35.720 --> 00:27:36.480
KELLY WEINERSMITH: Yeah.

00:27:36.480 --> 00:27:36.950
anyway.

00:27:36.950 --> 00:27:38.590
ZACH WEINERSMITH: I
erased that, I don't know.

00:27:38.590 --> 00:27:40.340
KELLY WEINERSMITH: It
was a little creepy.

00:27:40.340 --> 00:27:41.370
I'll never forget.

00:27:41.370 --> 00:27:42.240
AUDIENCE: Thank you
both for coming.

00:27:42.240 --> 00:27:43.260
Huge fan of the comic strip.

00:27:43.260 --> 00:27:44.426
ZACH WEINERSMITH: Thank you.

00:27:44.426 --> 00:27:47.490
AUDIENCE: Fairly often, there's
an extra, I guess bonus,

00:27:47.490 --> 00:27:49.290
comic panel that
appears when you

00:27:49.290 --> 00:27:51.060
hover over the votey button.

00:27:51.060 --> 00:27:53.199
I assume this has some
historical import.

00:27:53.199 --> 00:27:54.490
ZACH WEINERSMITH: Very much so.

00:27:54.490 --> 00:27:57.910
AUDIENCE: And actually, fairly
often in that pop-up panel,

00:27:57.910 --> 00:28:00.584
it shows Kelly
disapproving often

00:28:00.584 --> 00:28:02.250
holding one of your
children who is also

00:28:02.250 --> 00:28:04.899
disapproving of the
content of the comic strip.

00:28:04.899 --> 00:28:06.690
So I wonder if you
could speak a little bit

00:28:06.690 --> 00:28:08.810
about the collaborative aspect.

00:28:08.810 --> 00:28:10.910
Does she review every comic?

00:28:10.910 --> 00:28:12.810
Would there be even
more offensive jokes

00:28:12.810 --> 00:28:14.532
if she wasn't there
backstopping you?

00:28:14.532 --> 00:28:16.240
ZACH WEINERSMITH:
That's a good question.

00:28:16.240 --> 00:28:19.820
Well, so I always tell my jokes,
and she makes notes on them.

00:28:19.820 --> 00:28:22.380
And now and then, there's
one I think is really good

00:28:22.380 --> 00:28:25.650
and that you hate, and
so I always do those.

00:28:25.650 --> 00:28:27.870
[LAUGHTER]

00:28:27.870 --> 00:28:31.734
Because, if it does well,
it's just such a win

00:28:31.734 --> 00:28:32.400
in the marriage.

00:28:32.400 --> 00:28:33.810
[LAUGHTER]

00:28:33.810 --> 00:28:35.570
KELLY WEINERSMITH: The
single-use monocles

00:28:35.570 --> 00:28:37.542
was the number one
example of that.

00:28:37.542 --> 00:28:39.500
I was like, that project's
never going to work.

00:28:39.500 --> 00:28:40.010
That is stupid.

00:28:40.010 --> 00:28:41.130
ZACH WEINERSMITH: They
may not know what it is.

00:28:41.130 --> 00:28:43.290
KELLY WEINERSMITH: And
he sold tons of them.

00:28:43.290 --> 00:28:45.630
ZACH WEINERSMITH: We
sold them in 25 packs.

00:28:45.630 --> 00:28:48.866
So if you don't know, if you
go to singleusemonocles.com,

00:28:48.866 --> 00:28:50.490
it's a real thing
you can actually buy.

00:28:50.490 --> 00:28:53.490
It's like a wrapper
with one monocle.

00:28:53.490 --> 00:28:55.570
KELLY WEINERSMITH: I
still don't get it.

00:28:55.570 --> 00:28:58.320
I don't get it, but I'm glad
about the money it brought in.

00:28:58.320 --> 00:29:02.010
So this is the nice thing
when it's a project like this.

00:29:02.010 --> 00:29:05.310
So I might lose, but we
make money, and that's fine.

00:29:05.310 --> 00:29:07.830
I'd rather win, actually.

00:29:07.830 --> 00:29:11.820
There's no price that's
as important as winning,

00:29:11.820 --> 00:29:13.010
but it's all right.

00:29:13.010 --> 00:29:14.520
The money brings me some solace.

00:29:14.520 --> 00:29:17.220
But what's interesting
is that I have met people

00:29:17.220 --> 00:29:20.940
who have expected me to be super
grumpy and not a happy person,

00:29:20.940 --> 00:29:23.070
and that is not me at
all I'm actually very--

00:29:23.070 --> 00:29:24.278
ZACH WEINERSMITH: It's a lie.

00:29:26.960 --> 00:29:31.005
KELLY WEINERSMITH: I'm
actually very upbeat.

00:29:31.005 --> 00:29:33.210
And anyway, people expect
me to be kind of grumpy,

00:29:33.210 --> 00:29:35.293
and I don't think I'm that
grumpy, but maybe I am.

00:29:35.293 --> 00:29:36.330
Who knows?

00:29:36.330 --> 00:29:37.670
AUDIENCE: Thank you?

00:29:37.670 --> 00:29:39.040
KELLY WEINERSMITH: Thanks.

00:29:39.040 --> 00:29:41.040
AUDIENCE: Thank you
again, both, for coming.

00:29:41.040 --> 00:29:43.950
Just out of curiosity, I
wondered what the process was

00:29:43.950 --> 00:29:46.560
starting with the fan base
that was formed from single-use

00:29:46.560 --> 00:29:49.110
monocles, there once was
a man from Schemonaria,

00:29:49.110 --> 00:29:52.500
touch him on the
penis-style jokes--

00:29:52.500 --> 00:29:55.320
which I've loved and got me
into your comic many, many years

00:29:55.320 --> 00:29:55.890
ago--

00:29:55.890 --> 00:29:58.050
and transition them
into much more science

00:29:58.050 --> 00:30:00.014
and theory and intelligence.

00:30:00.014 --> 00:30:01.680
Did you find any
friction there, or what

00:30:01.680 --> 00:30:03.929
was the thought process going
from that style of humor

00:30:03.929 --> 00:30:04.560
to this?

00:30:04.560 --> 00:30:06.340
ZACH WEINERSMITH: Yeah, yeah.

00:30:06.340 --> 00:30:11.930
So part of probably just
maturing a bit, but also just--

00:30:11.930 --> 00:30:12.760
AUDIENCE: Boo.

00:30:12.760 --> 00:30:14.430
ZACH WEINERSMITH:
I know, I know.

00:30:14.430 --> 00:30:17.970
But I think I mistakenly
thought on the internet,

00:30:17.970 --> 00:30:20.200
if you got nerdy, people
didn't like it too much.

00:30:20.200 --> 00:30:21.990
I tried to keep it
a little, don't want

00:30:21.990 --> 00:30:23.156
to send people to Wikipedia.

00:30:23.156 --> 00:30:26.160
And it just turns out
that's totally wrong.

00:30:26.160 --> 00:30:27.900
It seemed like the
dorkier the comic got,

00:30:27.900 --> 00:30:30.150
the more sizable
the audience became.

00:30:30.150 --> 00:30:31.910
So I just kept going
in that direction.

00:30:31.910 --> 00:30:35.250
And also, as I got
to do it full time,

00:30:35.250 --> 00:30:38.170
I just had more time to read
and get well up on things.

00:30:38.170 --> 00:30:39.459
And that's been helpful.

00:30:39.459 --> 00:30:41.250
But what's nice is I've
had a lot of people

00:30:41.250 --> 00:30:43.630
say like, I started reading
your comic in high school,

00:30:43.630 --> 00:30:45.390
and it's sort of grown with me.

00:30:45.390 --> 00:30:48.924
So that's been very gratifying.

00:30:48.924 --> 00:30:51.340
AUDIENCE: With everything
that's about to ruin everything,

00:30:51.340 --> 00:30:53.381
how do we prepare the next
generation to survive?

00:30:55.540 --> 00:30:57.290
ZACH WEINERSMITH:
Prepare them to survive.

00:30:57.290 --> 00:30:59.290
KELLY WEINERSMITH: Hey, I think
they're just doomed in use.

00:30:59.290 --> 00:30:59.950
That's that.

00:30:59.950 --> 00:31:01.409
ZACH WEINERSMITH:
Just let them go.

00:31:01.409 --> 00:31:02.824
KELLY WEINERSMITH:
Just teach them

00:31:02.824 --> 00:31:04.530
that privacy doesn't matter.

00:31:04.530 --> 00:31:06.910
That's something they're
going to have to deal with.

00:31:06.910 --> 00:31:07.750
ZACH WEINERSMITH:
We're not at Facebook.

00:31:07.750 --> 00:31:08.620
You shouldn't--

00:31:08.620 --> 00:31:10.487
[LAUGHTER]

00:31:10.487 --> 00:31:11.445
KELLY WEINERSMITH: LOL.

00:31:14.025 --> 00:31:15.910
I don't have a good
answer for that.

00:31:15.910 --> 00:31:18.159
ZACH WEINERSMITH: It would
be kind of topic dependent.

00:31:18.159 --> 00:31:20.750
There are 10 different topics
and each presents issues.

00:31:20.750 --> 00:31:23.710
So she mentions privacy,
which we talk about a bit more

00:31:23.710 --> 00:31:25.960
extensively in a chapter on
brain-computer interfaces,

00:31:25.960 --> 00:31:28.390
which are like the
final end of privacy

00:31:28.390 --> 00:31:31.442
because now we can extract
brain states from a computer.

00:31:31.442 --> 00:31:33.650
KELLY WEINERSMITH: You've
got to go into more detail.

00:31:33.650 --> 00:31:35.233
Do you want me to
go into more detail?

00:31:35.233 --> 00:31:37.030
ZACH WEINERSMITH: Sure, yeah.

00:31:37.030 --> 00:31:38.380
KELLY WEINERSMITH: So
brain-computer interfaces

00:31:38.380 --> 00:31:40.213
are little machines
that talk to your brain,

00:31:40.213 --> 00:31:42.345
and they read your brain
waves and they figure out

00:31:42.345 --> 00:31:44.470
what it is you're thinking
and what you want to do.

00:31:44.470 --> 00:31:45.970
And right now,
they're being used

00:31:45.970 --> 00:31:48.204
to make prosthetics that
will reach out and grab

00:31:48.204 --> 00:31:50.620
the thing you're thinking about
reaching out and grabbing.

00:31:50.620 --> 00:31:52.300
And we thought that
the end goal was

00:31:52.300 --> 00:31:54.910
to make it so that someone
who was a quadriplegic

00:31:54.910 --> 00:31:56.950
could have all
their abilities back

00:31:56.950 --> 00:32:00.052
and, just by thinking about
it, they could do anything.

00:32:00.052 --> 00:32:01.510
And so that's the
answer I expected

00:32:01.510 --> 00:32:03.280
when I asked Gerwin
Schalk, what is

00:32:03.280 --> 00:32:05.380
the end goal of
brain-computer interfaces?

00:32:05.380 --> 00:32:09.340
And his answer was, one
day, all of our thoughts

00:32:09.340 --> 00:32:11.290
will be able to get
uploaded to one cloud

00:32:11.290 --> 00:32:13.330
and will become one
big superorganism that

00:32:13.330 --> 00:32:14.890
shares all of our thoughts.

00:32:14.890 --> 00:32:18.190
And I was like,
that is horrible.

00:32:18.190 --> 00:32:21.220
Our marriage works because
that doesn't happen.

00:32:21.220 --> 00:32:23.970
And I think that's why
society works in general.

00:32:23.970 --> 00:32:27.340
And so he admitted that there
could be negatives to that.

00:32:27.340 --> 00:32:29.680
He's like, if you're sitting
on the couch and you think,

00:32:29.680 --> 00:32:32.090
I want to leave my wife,
she would know that.

00:32:32.090 --> 00:32:34.090
And he said, and that
wouldn't be so great.

00:32:34.090 --> 00:32:36.350
And I was like, that would
absolutely not be so great.

00:32:36.350 --> 00:32:39.850
And so anyway, this is like
the ultimate end of privacy

00:32:39.850 --> 00:32:41.290
if that ever happens.

00:32:41.290 --> 00:32:43.660
And it is fascinating
to think about.

00:32:43.660 --> 00:32:46.120
Humans are a totally different
thing if all of our brains

00:32:46.120 --> 00:32:48.410
are connected like that.

00:32:48.410 --> 00:32:51.280
Anyway, it's crazy, but
we personally kind of hope

00:32:51.280 --> 00:32:52.660
that future never comes to pass

00:32:52.660 --> 00:32:52.990
ZACH WEINERSMITH:
Never happens, no.

00:32:52.990 --> 00:32:55.031
KELLY WEINERSMITH: And
then, I asked other people

00:32:55.031 --> 00:32:58.159
in the brain-computer interface
world, is this just Gerwin,

00:32:58.159 --> 00:32:59.950
or does everybody know
that this is the end

00:32:59.950 --> 00:33:01.780
goal but in interviews
you just talk

00:33:01.780 --> 00:33:05.290
about being able to move your
arm if you couldn't before?

00:33:05.290 --> 00:33:06.700
And they're like, well, yeah.

00:33:06.700 --> 00:33:08.170
In the interviews, we
talk about the arm thing.

00:33:08.170 --> 00:33:09.586
But at the conference,
we all know

00:33:09.586 --> 00:33:12.650
that we're going to upload
or brain to the cloud.

00:33:12.650 --> 00:33:17.450
And I was like, we need to
stop funding this field.

00:33:17.450 --> 00:33:20.669
I'm very sorry for
the amputees, but--

00:33:20.669 --> 00:33:22.960
ZACH WEINERSMITH: That's
going to get quoted somewhere.

00:33:22.960 --> 00:33:24.820
KELLY WEINERSMITH: I hope not.

00:33:24.820 --> 00:33:25.810
That would be very bad.

00:33:25.810 --> 00:33:26.710
ZACH WEINERSMITH: You
know what's funny?

00:33:26.710 --> 00:33:28.376
So I was just talking
at Google Seattle,

00:33:28.376 --> 00:33:31.450
and somebody brought up, what
if instead of one there's three,

00:33:31.450 --> 00:33:33.699
and now there is the
three-roommate problem?

00:33:33.699 --> 00:33:34.990
There's like three superbrains.

00:33:34.990 --> 00:33:35.860
KELLY WEINERSMITH:
I'm not following.

00:33:35.860 --> 00:33:37.510
ZACH WEINERSMITH: When two
are more down with each other

00:33:37.510 --> 00:33:38.960
than the other superbrain.

00:33:38.960 --> 00:33:40.750
KELLY WEINERSMITH: Oh, got it.

00:33:45.700 --> 00:33:48.100
So privacy, they really need
to not careful about privacy

00:33:48.100 --> 00:33:50.474
if the future is going to work
for them because we're all

00:33:50.474 --> 00:33:53.770
going to be one big superbrain.

00:33:53.770 --> 00:33:55.170
Hi.

00:33:55.170 --> 00:33:56.130
AUDIENCE: Hi.

00:33:56.130 --> 00:33:59.760
I've been reading SMBC
since I had to load it up

00:33:59.760 --> 00:34:00.830
on a dial-up modem.

00:34:00.830 --> 00:34:01.460
ZACH WEINERSMITH: Oh my god.

00:34:01.460 --> 00:34:02.100
What baud?

00:34:02.100 --> 00:34:03.510
AUDIENCE: And that
was a big deal

00:34:03.510 --> 00:34:06.914
because black-and-white
comics loaded a lot faster.

00:34:06.914 --> 00:34:08.205
ZACH WEINERSMITH: That's right.

00:34:08.205 --> 00:34:12.022
AUDIENCE: And I just was curious
what your favorite comic was?

00:34:12.022 --> 00:34:13.480
ZACH WEINERSMITH:
Favorite of mine?

00:34:13.480 --> 00:34:15.269
I kind of go through phases.

00:34:15.269 --> 00:34:17.310
Probably one of the long
story ones or something,

00:34:17.310 --> 00:34:19.529
but I don't really
have a favorite.

00:34:19.529 --> 00:34:20.820
KELLY WEINERSMITH: I like xkcd.

00:34:20.820 --> 00:34:23.190
ZACH WEINERSMITH: Yeah.

00:34:23.190 --> 00:34:24.179
I'm not grumpy.

00:34:24.179 --> 00:34:27.060
I have a nice personality.

00:34:27.060 --> 00:34:29.436
AUDIENCE: Can I ask
about the giraffe hooker?

00:34:29.436 --> 00:34:30.810
ZACH WEINERSMITH:
Giraffe hooker?

00:34:30.810 --> 00:34:33.370
For those who don't know--

00:34:33.370 --> 00:34:35.795
that's a good start to that--

00:34:35.795 --> 00:34:37.920
this is in reference, I
think there's an xkcd where

00:34:37.920 --> 00:34:39.052
he made a joke about--

00:34:39.052 --> 00:34:41.010
I don't actually remember
what the context was,

00:34:41.010 --> 00:34:42.509
but the clear
implication was that I

00:34:42.509 --> 00:34:45.339
needed to draw a sexy
giraffe as a bonus panel.

00:34:45.339 --> 00:34:46.380
You're not aware of this?

00:34:46.380 --> 00:34:48.098
KELLY WEINERSMITH: I forgot.

00:34:48.098 --> 00:34:50.389
ZACH WEINERSMITH: So if you
want to see a sexy giraffe,

00:34:50.389 --> 00:34:51.765
I'm one of your options.

00:34:51.765 --> 00:34:53.659
[LAUGHTER]

00:34:53.659 --> 00:34:54.909
KELLY WEINERSMITH: It's funny.

00:34:54.909 --> 00:34:58.326
So in grad school, I was a
faculty at Rice for a while.

00:34:58.326 --> 00:35:00.450
And a lot of the people
I'd encounter, I'd be like,

00:35:00.450 --> 00:35:01.700
oh, my husband's a cartoonist.

00:35:01.700 --> 00:35:04.840
And everyone was like,
is it the xkcd guy?

00:35:04.840 --> 00:35:05.880
No, no, no.

00:35:05.880 --> 00:35:06.679
The PhD Comics guy?

00:35:06.679 --> 00:35:08.220
ZACH WEINERSMITH:
The PhD Comics guy.

00:35:08.220 --> 00:35:09.678
KELLY WEINERSMITH:
Not that either.

00:35:09.678 --> 00:35:10.300
The SMBC guy.

00:35:10.300 --> 00:35:12.070
ZACH WEINERSMITH: And
they just walk away.

00:35:12.070 --> 00:35:13.440
KELLY WEINERSMITH: No, no.

00:35:13.440 --> 00:35:16.749
That worked plenty
of times, also.

00:35:16.749 --> 00:35:17.790
Thanks for your question.

00:35:17.790 --> 00:35:18.623
AUDIENCE: Thank you.

00:35:18.623 --> 00:35:21.060
ZACH WEINERSMITH: Thank you.

00:35:21.060 --> 00:35:22.470
AUDIENCE: I find
it inspirational

00:35:22.470 --> 00:35:26.730
that you're able to have
a relationship where

00:35:26.730 --> 00:35:31.710
the two of you can be so
collaborative and creative.

00:35:31.710 --> 00:35:33.525
I was wondering if
you had any tips.

00:35:33.525 --> 00:35:38.170
[LAUGHTER]

00:35:38.170 --> 00:35:40.480
Like tips to avoid ending
up on each other's nerves

00:35:40.480 --> 00:35:42.930
or avoid having
one person taking

00:35:42.930 --> 00:35:44.860
ownership of the project?

00:35:44.860 --> 00:35:48.086
KELLY WEINERSMITH: Ah,
well, so the background

00:35:48.086 --> 00:35:50.460
is that, when we started
dating, our favorite thing to do

00:35:50.460 --> 00:35:52.530
was spend all day in the
library and then go on walks

00:35:52.530 --> 00:35:53.988
and talk about what
we had learned.

00:35:56.680 --> 00:36:00.160
So our relationship started
as we'd like to go on walks

00:36:00.160 --> 00:36:01.120
and talk about stuff.

00:36:01.120 --> 00:36:03.030
And so this project,
essentially the topic

00:36:03.030 --> 00:36:06.492
that we talked about on our
walks, was always "SOONISH."

00:36:06.492 --> 00:36:07.950
Anyway, it was kind
of nice to know

00:36:07.950 --> 00:36:09.840
we were talking about
different papers we

00:36:09.840 --> 00:36:10.770
had read on the same topic.

00:36:10.770 --> 00:36:11.970
And anyway, so that worked well.

00:36:11.970 --> 00:36:13.553
But additionally,
we got kind of lucky

00:36:13.553 --> 00:36:16.110
because our personalities
are such that we wanted

00:36:16.110 --> 00:36:17.790
to tackle different parts.

00:36:17.790 --> 00:36:20.580
So I did all of the interviews.

00:36:20.580 --> 00:36:22.230
He did a lot of the
background reading,

00:36:22.230 --> 00:36:23.938
although some of the
chapters that was me

00:36:23.938 --> 00:36:25.950
doing the background reading.

00:36:25.950 --> 00:36:29.550
He's the funny one, and so he
did the jokes and the comics.

00:36:29.550 --> 00:36:31.644
And then I am
detail-oriented one,

00:36:31.644 --> 00:36:33.810
so I went through every
single sentence in this book

00:36:33.810 --> 00:36:36.540
and made sure we had a citation
for every single sentence.

00:36:36.540 --> 00:36:39.330
This was when we
didn't actually think

00:36:39.330 --> 00:36:41.610
that we were going to
write a bibliography, which

00:36:41.610 --> 00:36:43.170
was real dumb of
us to not expect

00:36:43.170 --> 00:36:44.310
that was going to happen.

00:36:44.310 --> 00:36:46.410
And then Jenny, who's here,
was like, oh hey guys,

00:36:46.410 --> 00:36:47.220
your bibliography.

00:36:47.220 --> 00:36:49.890
I was like, ooh.

00:36:49.890 --> 00:36:52.505
So I went through every sentence
and made our bibliography.

00:36:52.505 --> 00:36:54.630
And that was something that
would have killed Zach.

00:36:54.630 --> 00:36:55.546
ZACH WEINERSMITH: Yes.

00:36:55.546 --> 00:36:56.588
I would have cried a lot.

00:36:56.588 --> 00:36:58.212
KELLY WEINERSMITH:
He would have cried.

00:36:58.212 --> 00:36:59.970
And so I guess, to be
honest, we got lucky

00:36:59.970 --> 00:37:02.610
that we're interested
in different parts of it

00:37:02.610 --> 00:37:05.010
and that we really
just like talking

00:37:05.010 --> 00:37:06.630
about nerdy stuff
for a long time

00:37:06.630 --> 00:37:08.180
and sending chapters
back and forth.

00:37:08.180 --> 00:37:11.140
It's really important to
not get your feelings hurt.

00:37:11.140 --> 00:37:13.021
And I feel like in
any collaboration

00:37:13.021 --> 00:37:14.020
that's really important.

00:37:14.020 --> 00:37:15.520
But it's particularly
important when

00:37:15.520 --> 00:37:18.567
it's your spouse who's like,
no, the synthetic biology

00:37:18.567 --> 00:37:19.650
chapter you wrote is junk.

00:37:19.650 --> 00:37:20.899
We've got to start over again.

00:37:20.899 --> 00:37:24.820
And so you really need to have
a thick skin, which we both do.

00:37:24.820 --> 00:37:26.904
Neither one of us cares
what the other one thinks.

00:37:26.904 --> 00:37:28.570
ZACH WEINERSMITH:
It's really important.

00:37:28.570 --> 00:37:30.460
KELLY WEINERSMITH:
It's really important.

00:37:30.460 --> 00:37:31.692
Do you have another answer?

00:37:31.692 --> 00:37:33.900
ZACH WEINERSMITH: Just a
little thing to add to that.

00:37:33.900 --> 00:37:35.550
I do a decent amount
of collaborating,

00:37:35.550 --> 00:37:37.920
and sometimes it goes well
and sometimes it doesn't.

00:37:37.920 --> 00:37:41.820
And the two things that are
really important is, one,

00:37:41.820 --> 00:37:44.070
you have separate magisteria
as much as possible.

00:37:44.070 --> 00:37:45.990
You have separate roles,
and in your domain

00:37:45.990 --> 00:37:48.912
you have more veto power
if not absolute veto power.

00:37:48.912 --> 00:37:51.370
And the other thing-- and this
is hard to know in advance--

00:37:51.370 --> 00:37:53.610
if the person you're working
with communicates well,

00:37:53.610 --> 00:37:54.870
that makes a big
difference because people

00:37:54.870 --> 00:37:56.244
who don't communicate
well end up

00:37:56.244 --> 00:37:58.880
sort of storing up their
anger and then releasing it

00:37:58.880 --> 00:38:00.480
on you at some point.

00:38:00.480 --> 00:38:03.722
So if you have a relatively
mature person who

00:38:03.722 --> 00:38:05.430
talks about when
they're having a problem

00:38:05.430 --> 00:38:06.971
and doesn't just
try to tough it out,

00:38:06.971 --> 00:38:09.097
that can be really helpful.

00:38:09.097 --> 00:38:09.930
AUDIENCE: Thank you.

00:38:09.930 --> 00:38:10.888
ZACH WEINERSMITH: Yeah.

00:38:10.888 --> 00:38:12.360
KELLY WEINERSMITH: Sure, thanks.

00:38:12.360 --> 00:38:13.984
AUDIENCE: One of the
previous questions

00:38:13.984 --> 00:38:15.420
reminded me that
my favorite SMBC

00:38:15.420 --> 00:38:18.300
comic is the one
about how it takes

00:38:18.300 --> 00:38:19.890
seven years to
master a new skill,

00:38:19.890 --> 00:38:21.330
and that leads to
many lifetimes.

00:38:21.330 --> 00:38:24.574
And you could be an
artist and a writer.

00:38:24.574 --> 00:38:26.490
So can you talk about
the lifetimes you've had

00:38:26.490 --> 00:38:27.739
and where that idea came from?

00:38:27.739 --> 00:38:28.957
ZACH WEINERSMITH: Oh man.

00:38:28.957 --> 00:38:30.540
Perhaps some of you
have read, there's

00:38:30.540 --> 00:38:34.370
a famous speech by Hamming
from, I think, 1986

00:38:34.370 --> 00:38:35.970
that sort of talks about it.

00:38:35.970 --> 00:38:39.180
It's actually written for
programmers about how you're

00:38:39.180 --> 00:38:41.760
going to have your career.

00:38:41.760 --> 00:38:43.950
And so he talked kind
of a related concept.

00:38:43.950 --> 00:38:46.872
He talked about how he liked
switching fields a lot.

00:38:46.872 --> 00:38:48.330
He was a very nerdy
guy, so I think

00:38:48.330 --> 00:38:50.663
he was talking about switched
from software to hardware.

00:38:50.663 --> 00:38:52.910
He wasn't talking about
like becoming a poet.

00:38:52.910 --> 00:38:55.740
But I found that idea
really interesting

00:38:55.740 --> 00:38:57.420
to keep you from going stale.

00:38:57.420 --> 00:39:00.610
So this project was
kind of one of those.

00:39:00.610 --> 00:39:03.150
And another one we started
doing five years ago

00:39:03.150 --> 00:39:05.970
is called BAHFest, the
Festival of Bad Ad Hoc

00:39:05.970 --> 00:39:08.460
hypotheses, which gives you
a pretty good sense of how

00:39:08.460 --> 00:39:13.870
nerdy it is, which was and
remains a pretty big challenge.

00:39:13.870 --> 00:39:15.690
It's a live event.

00:39:15.690 --> 00:39:17.419
It's sort of an
improv game, I guess.

00:39:17.419 --> 00:39:18.960
It's like fake
science talks that you

00:39:18.960 --> 00:39:20.940
have to defend against
actual scientists.

00:39:20.940 --> 00:39:22.421
[LAUGHTER]

00:39:22.421 --> 00:39:22.920
Right?

00:39:27.881 --> 00:39:28.380
It's funny.

00:39:28.380 --> 00:39:30.130
Every time we do a project
like this, at some point.

00:39:30.130 --> 00:39:32.088
I have the same thought,
which is, why didn't I

00:39:32.088 --> 00:39:34.360
just keep writing comics?

00:39:34.360 --> 00:39:37.500
But yeah, I try
to on the regular

00:39:37.500 --> 00:39:40.664
do something that like makes
me really uncomfortable.

00:39:40.664 --> 00:39:43.080
I don't quite have the luxury
to completely switch careers

00:39:43.080 --> 00:39:48.080
at any moment because we have
babies, and babies like to eat.

00:39:48.080 --> 00:39:50.420
But I try to regularly
do a new experiment.

00:39:50.420 --> 00:39:53.780
So another one I'm
doing probably in 2019--

00:39:53.780 --> 00:39:56.510
this has already been announced,
it's not private info--

00:39:56.510 --> 00:39:58.801
I'm doing a project with a
gay named Bryan Caplan who's

00:39:58.801 --> 00:40:00.447
an economist.

00:40:00.447 --> 00:40:02.280
I don't want to give
away too much, I guess.

00:40:02.280 --> 00:40:04.320
But it's sort of
like a nonfiction,

00:40:04.320 --> 00:40:06.090
pro-immigration
graphic novel trying

00:40:06.090 --> 00:40:08.174
to explain some
statistics and stuff.

00:40:08.174 --> 00:40:10.590
So that has been a totally
different challenge because I'm

00:40:10.590 --> 00:40:12.330
just the Illustrator on it.

00:40:12.330 --> 00:40:14.059
I can chat a little
with him about stuff,

00:40:14.059 --> 00:40:15.850
but mostly i@m just
illustrating his words.

00:40:15.850 --> 00:40:18.224
So that has been a completely
different challenge for me.

00:40:20.300 --> 00:40:21.800
What I'm trying to
say is you should

00:40:21.800 --> 00:40:24.216
do something that makes you
feel stupid at least like once

00:40:24.216 --> 00:40:26.020
every two years.

00:40:26.020 --> 00:40:27.200
Yeah, it's very important.

00:40:27.200 --> 00:40:28.033
It's very important.

00:40:28.033 --> 00:40:29.447
AUDIENCE: Thank you.

00:40:29.447 --> 00:40:31.780
I feel like maybe you kind
of just answered my question,

00:40:31.780 --> 00:40:34.405
but I guess what I was going to
say is you're both really great

00:40:34.405 --> 00:40:37.930
communicators on complex
issues and complex topics.

00:40:37.930 --> 00:40:40.300
And I was wondering if you
thought you might move more

00:40:40.300 --> 00:40:42.582
in a direction of public
advocacy or public

00:40:42.582 --> 00:40:45.040
information, which it kind of
sounds like you already maybe

00:40:45.040 --> 00:40:45.748
are a little bit?

00:40:45.748 --> 00:40:47.620
ZACH WEINERSMITH: A little bit.

00:40:47.620 --> 00:40:49.330
I'm also working on
another comic project

00:40:49.330 --> 00:40:50.890
I shouldn't say
too much about sort

00:40:50.890 --> 00:40:54.420
of explaining political
norms, which suddenly people

00:40:54.420 --> 00:40:55.830
are very interested in.

00:40:55.830 --> 00:40:57.280
What did things used to be like?

00:41:00.220 --> 00:41:02.080
A little bit more,
although I really do

00:41:02.080 --> 00:41:04.371
enjoy fiction and
storytelling, so I don't want

00:41:04.371 --> 00:41:05.620
to get too far away from that.

00:41:07.501 --> 00:41:10.000
Part of the thing about having
a job explaining stuff is you

00:41:10.000 --> 00:41:13.030
get to learn stuff all day long,
which is a pretty good deal

00:41:13.030 --> 00:41:13.760
most of the time.

00:41:13.760 --> 00:41:16.197
Sometimes it gets a little
thick, but most of the time

00:41:16.197 --> 00:41:17.030
it's pretty awesome.

00:41:17.030 --> 00:41:19.120
So for us, it's kind of a
lifestyle choice, I guess.

00:41:19.120 --> 00:41:20.110
KELLY WEINERSMITH: One
thing that was also nice

00:41:20.110 --> 00:41:22.165
about working as a
team is that one of us

00:41:22.165 --> 00:41:24.790
would start doing the background
research, would write a draft,

00:41:24.790 --> 00:41:26.630
and then we'd send
it to the other one.

00:41:26.630 --> 00:41:29.050
And so, sometimes when you
get too thick into something,

00:41:29.050 --> 00:41:30.820
it's easy to write it
and forget what you

00:41:30.820 --> 00:41:32.530
didn't know when you started.

00:41:32.530 --> 00:41:34.240
So if the other person
comes at it fresh,

00:41:34.240 --> 00:41:35.950
they can tell you where you
haven't explained something

00:41:35.950 --> 00:41:36.692
clearly.

00:41:36.692 --> 00:41:38.650
And so we tried to work
the chapters like that.

00:41:38.650 --> 00:41:41.804
And so that's one way we tried
to make everything clear,

00:41:41.804 --> 00:41:42.970
which hopefully we did well.

00:41:46.310 --> 00:41:47.300
AUDIENCE: Hi.

00:41:47.300 --> 00:41:48.860
Kelly, I'm curious
about what some

00:41:48.860 --> 00:41:51.530
of the other ideas Zach's
had that you thought

00:41:51.530 --> 00:41:53.879
were the worst ones?

00:41:53.879 --> 00:41:55.670
KELLY WEINERSMITH: That
made it in the book

00:41:55.670 --> 00:41:56.980
that I thought were
particularly bad?

00:41:56.980 --> 00:41:58.680
AUDIENCE: Either ones
that did or didn't.

00:41:58.680 --> 00:42:00.800
KELLY WEINERSMITH: Well, one of
the really interesting things

00:42:00.800 --> 00:42:02.600
about writing this book was
that I went from thinking

00:42:02.600 --> 00:42:05.050
some technologies were just
awesome across the board

00:42:05.050 --> 00:42:07.770
to thinking they were awesome
but also kind of scary.

00:42:07.770 --> 00:42:09.909
So asteroid mining, for example.

00:42:09.909 --> 00:42:12.200
First of all, that one we
were totally wrong about what

00:42:12.200 --> 00:42:13.033
asteroid mining was.

00:42:13.033 --> 00:42:15.920
We thought the ideal was to
go out, get tungsten, bring it

00:42:15.920 --> 00:42:17.750
to Earth, and now
have a ton of tungsten

00:42:17.750 --> 00:42:18.890
that you can sell on earth.

00:42:18.890 --> 00:42:20.390
But it turns out,
economically, that just

00:42:20.390 --> 00:42:22.140
doesn't work because
you're going to bring

00:42:22.140 --> 00:42:23.450
a ton of tungsten here.

00:42:23.450 --> 00:42:24.950
You're going to
crash the market.

00:42:24.950 --> 00:42:27.033
And then that was a waste
of billions and billions

00:42:27.033 --> 00:42:27.690
of dollars.

00:42:27.690 --> 00:42:29.406
And so the point now
is you go out there,

00:42:29.406 --> 00:42:31.280
you get the tungsten,
you build a space base,

00:42:31.280 --> 00:42:33.260
and then you go and
explore the world.

00:42:33.260 --> 00:42:34.880
Or you bring it
the space station,

00:42:34.880 --> 00:42:37.010
you get water from the
asteroids, blah blah blah.

00:42:37.010 --> 00:42:39.680
But anyway, so we became really
excited about cheap access

00:42:39.680 --> 00:42:40.910
to space and asteroid mining.

00:42:40.910 --> 00:42:44.240
But the scary thing is, once
you get the ability to wrangle

00:42:44.240 --> 00:42:46.610
asteroids and you can
bring them anywhere,

00:42:46.610 --> 00:42:49.110
you can also fling them
at the Earth, which

00:42:49.110 --> 00:42:52.530
could be worse than any nuclear
bomb we've ever set off.

00:42:52.530 --> 00:42:56.310
And so when people get
cheap access to space--

00:42:56.310 --> 00:42:58.130
for example, if the
space elevator works--

00:42:58.130 --> 00:43:00.620
we suddenly will have tons
and tons of people in space.

00:43:00.620 --> 00:43:02.720
Maybe we'll have
colonies on Mars.

00:43:02.720 --> 00:43:05.120
And you'll suddenly have
people with the ability

00:43:05.120 --> 00:43:07.220
to be able to move giant
objects and fling them

00:43:07.220 --> 00:43:08.380
at Earth potentially.

00:43:08.380 --> 00:43:11.600
And so it's a great
technology, but now you

00:43:11.600 --> 00:43:14.150
have to trust that human
beings aren't horrible.

00:43:14.150 --> 00:43:17.480
And I don't know if our
history totally warrants that.

00:43:17.480 --> 00:43:18.890
And so it's a little bit scary.

00:43:18.890 --> 00:43:20.780
It's an awesome technology
that now has something

00:43:20.780 --> 00:43:21.810
that could be really negative.

00:43:21.810 --> 00:43:23.726
And a lot of these
technologies are like that.

00:43:23.726 --> 00:43:28.010
So our book originally had
advanced nuclear reactors--

00:43:28.010 --> 00:43:31.041
fission reactors-- and
that's another technology

00:43:31.041 --> 00:43:33.290
where it's like, well, if
you can trust everyone, then

00:43:33.290 --> 00:43:35.240
that's great because we have
this greenhouse gas problem

00:43:35.240 --> 00:43:36.440
that we want to get rid of.

00:43:36.440 --> 00:43:38.666
Climate change is
obviously a bad thing

00:43:38.666 --> 00:43:40.040
we're all dealing
with right now,

00:43:40.040 --> 00:43:42.440
but can you trust
people with it?

00:43:42.440 --> 00:43:43.125
Hard to say.

00:43:43.125 --> 00:43:44.750
Does that kind of
answer your question.

00:43:44.750 --> 00:43:45.140
AUDIENCE: Yeah.

00:43:45.140 --> 00:43:45.540
KELLY WEINERSMITH: OK, great.

00:43:45.540 --> 00:43:46.248
AUDIENCE: Thanks.

00:43:46.248 --> 00:43:47.654
KELLY WEINERSMITH: Thanks.

00:43:47.654 --> 00:43:49.150
ZACH WEINERSMITH: Anything else?

00:43:49.150 --> 00:43:50.525
KELLY WEINERSMITH:
Anything else?

00:43:52.790 --> 00:43:53.384
OK.

00:43:53.384 --> 00:43:54.800
Thank you very
much for your time.

00:43:54.800 --> 00:43:58.150
[APPLAUSE]

