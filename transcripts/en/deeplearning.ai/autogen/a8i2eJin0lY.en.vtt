WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.240
in the last video we described what is

00:00:02.240 --> 00:00:02.250
in the last video we described what is
 

00:00:02.250 --> 00:00:04.370
in the last video we described what is
the deep l-larry neural network and also

00:00:04.370 --> 00:00:04.380
the deep l-larry neural network and also
 

00:00:04.380 --> 00:00:06.140
the deep l-larry neural network and also
talked about the notation we use to

00:00:06.140 --> 00:00:06.150
talked about the notation we use to
 

00:00:06.150 --> 00:00:08.540
talked about the notation we use to
describe such networks in this video you

00:00:08.540 --> 00:00:08.550
describe such networks in this video you
 

00:00:08.550 --> 00:00:10.820
describe such networks in this video you
see how you can perform fold propagation

00:00:10.820 --> 00:00:10.830
see how you can perform fold propagation
 

00:00:10.830 --> 00:00:13.759
see how you can perform fold propagation
in a deep network as usual let's first

00:00:13.759 --> 00:00:13.769
in a deep network as usual let's first
 

00:00:13.769 --> 00:00:16.430
in a deep network as usual let's first
go over what forward propagation will

00:00:16.430 --> 00:00:16.440
go over what forward propagation will
 

00:00:16.440 --> 00:00:18.650
go over what forward propagation will
look like for a single training example

00:00:18.650 --> 00:00:18.660
look like for a single training example
 

00:00:18.660 --> 00:00:21.349
look like for a single training example
X and then later on we'll talk about the

00:00:21.349 --> 00:00:21.359
X and then later on we'll talk about the
 

00:00:21.359 --> 00:00:22.910
X and then later on we'll talk about the
vectorized version where you want to

00:00:22.910 --> 00:00:22.920
vectorized version where you want to
 

00:00:22.920 --> 00:00:24.800
vectorized version where you want to
carry out forward propagation on the

00:00:24.800 --> 00:00:24.810
carry out forward propagation on the
 

00:00:24.810 --> 00:00:26.810
carry out forward propagation on the
entire training set at the same time but

00:00:26.810 --> 00:00:26.820
entire training set at the same time but
 

00:00:26.820 --> 00:00:29.650
entire training set at the same time but
um given a single training example X

00:00:29.650 --> 00:00:29.660
um given a single training example X
 

00:00:29.660 --> 00:00:32.569
um given a single training example X
here's how you compute the activations

00:00:32.569 --> 00:00:32.579
here's how you compute the activations
 

00:00:32.579 --> 00:00:34.790
here's how you compute the activations
of the first layer so for this first

00:00:34.790 --> 00:00:34.800
of the first layer so for this first
 

00:00:34.800 --> 00:00:42.639
of the first layer so for this first
layer you compute v1 equals W 1 times X

00:00:42.639 --> 00:00:42.649
layer you compute v1 equals W 1 times X
 

00:00:42.649 --> 00:00:48.229
layer you compute v1 equals W 1 times X
plus b1 so W 1 and B 1 that parameters

00:00:48.229 --> 00:00:48.239
plus b1 so W 1 and B 1 that parameters
 

00:00:48.239 --> 00:00:51.110
plus b1 so W 1 and B 1 that parameters
that affect the activations in layer 1

00:00:51.110 --> 00:00:51.120
that affect the activations in layer 1
 

00:00:51.120 --> 00:00:53.869
that affect the activations in layer 1
right where this is layer 1 of the

00:00:53.869 --> 00:00:53.879
right where this is layer 1 of the
 

00:00:53.879 --> 00:00:57.319
right where this is layer 1 of the
neural network and then you compute the

00:00:57.319 --> 00:00:57.329
neural network and then you compute the
 

00:00:57.329 --> 00:00:59.270
neural network and then you compute the
activations for that layer to be equal

00:00:59.270 --> 00:00:59.280
activations for that layer to be equal
 

00:00:59.280 --> 00:01:04.369
activations for that layer to be equal
to G of Z 1 and the deactivation

00:01:04.369 --> 00:01:04.379
to G of Z 1 and the deactivation
 

00:01:04.379 --> 00:01:06.320
to G of Z 1 and the deactivation
function G depends on what layer you're

00:01:06.320 --> 00:01:06.330
function G depends on what layer you're
 

00:01:06.330 --> 00:01:09.080
function G depends on what layer you're
at and maybe index AB has the activation

00:01:09.080 --> 00:01:09.090
at and maybe index AB has the activation
 

00:01:09.090 --> 00:01:11.000
at and maybe index AB has the activation
function from layer 1 so if you do that

00:01:11.000 --> 00:01:11.010
function from layer 1 so if you do that
 

00:01:11.010 --> 00:01:12.770
function from layer 1 so if you do that
you've now computed the activations from

00:01:12.770 --> 00:01:12.780
you've now computed the activations from
 

00:01:12.780 --> 00:01:13.310
you've now computed the activations from
there 1

00:01:13.310 --> 00:01:13.320
there 1
 

00:01:13.320 --> 00:01:18.350
there 1
how about layer to say that there well

00:01:18.350 --> 00:01:18.360
how about layer to say that there well
 

00:01:18.360 --> 00:01:24.880
how about layer to say that there well
you would then compute v2 equals W to a

00:01:24.880 --> 00:01:24.890
you would then compute v2 equals W to a
 

00:01:24.890 --> 00:01:31.760
you would then compute v2 equals W to a
1 plus B 2 and then so the observation

00:01:31.760 --> 00:01:31.770
1 plus B 2 and then so the observation
 

00:01:31.770 --> 00:01:34.940
1 plus B 2 and then so the observation
of layer 2 is the way matrix times the

00:01:34.940 --> 00:01:34.950
of layer 2 is the way matrix times the
 

00:01:34.950 --> 00:01:39.140
of layer 2 is the way matrix times the
outputs of layer 1 so that value plus

00:01:39.140 --> 00:01:39.150
outputs of layer 1 so that value plus
 

00:01:39.150 --> 00:01:44.260
outputs of layer 1 so that value plus
the bias vector for layer 2 and then a2

00:01:44.260 --> 00:01:44.270
the bias vector for layer 2 and then a2
 

00:01:44.270 --> 00:01:49.569
the bias vector for layer 2 and then a2
equals the activation function apply to

00:01:49.569 --> 00:01:49.579
equals the activation function apply to
 

00:01:49.579 --> 00:01:55.670
equals the activation function apply to
z2 ok so that's it for layer 2 and so on

00:01:55.670 --> 00:01:55.680
z2 ok so that's it for layer 2 and so on
 

00:01:55.680 --> 00:01:57.980
z2 ok so that's it for layer 2 and so on
and so forth until you get to the output

00:01:57.980 --> 00:01:57.990
and so forth until you get to the output
 

00:01:57.990 --> 00:02:00.289
and so forth until you get to the output
layer that's layer 4 where you would

00:02:00.289 --> 00:02:00.299
layer that's layer 4 where you would
 

00:02:00.299 --> 00:02:06.230
layer that's layer 4 where you would
have that V 4 is equal to the parameters

00:02:06.230 --> 00:02:06.240
have that V 4 is equal to the parameters
 

00:02:06.240 --> 00:02:09.949
have that V 4 is equal to the parameters
for that layer times the activations

00:02:09.949 --> 00:02:09.959
for that layer times the activations
 

00:02:09.959 --> 00:02:11.820
for that layer times the activations
from the previous layer

00:02:11.820 --> 00:02:11.830
from the previous layer
 

00:02:11.830 --> 00:02:15.740
from the previous layer
Plus that bias vector and then similarly

00:02:15.740 --> 00:02:15.750
Plus that bias vector and then similarly
 

00:02:15.750 --> 00:02:24.330
Plus that bias vector and then similarly
a four equals G of v4 and so that's how

00:02:24.330 --> 00:02:24.340
a four equals G of v4 and so that's how
 

00:02:24.340 --> 00:02:26.730
a four equals G of v4 and so that's how
you you know compute your estimated

00:02:26.730 --> 00:02:26.740
you you know compute your estimated
 

00:02:26.740 --> 00:02:29.880
you you know compute your estimated
output Y hat so just one thing to notice

00:02:29.880 --> 00:02:29.890
output Y hat so just one thing to notice
 

00:02:29.890 --> 00:02:35.300
output Y hat so just one thing to notice
X here is also equal to a zero because

00:02:35.300 --> 00:02:35.310
X here is also equal to a zero because
 

00:02:35.310 --> 00:02:38.280
X here is also equal to a zero because
the input feature vector X is also the

00:02:38.280 --> 00:02:38.290
the input feature vector X is also the
 

00:02:38.290 --> 00:02:41.190
the input feature vector X is also the
activations of layer 0 so we scratch out

00:02:41.190 --> 00:02:41.200
activations of layer 0 so we scratch out
 

00:02:41.200 --> 00:02:43.980
activations of layer 0 so we scratch out
X I'm going to cross out X and put a 0

00:02:43.980 --> 00:02:43.990
X I'm going to cross out X and put a 0
 

00:02:43.990 --> 00:02:46.980
X I'm going to cross out X and put a 0
here then you know all of these

00:02:46.980 --> 00:02:46.990
here then you know all of these
 

00:02:46.990 --> 00:02:49.470
here then you know all of these
equations basically look the same right

00:02:49.470 --> 00:02:49.480
equations basically look the same right
 

00:02:49.480 --> 00:02:53.960
equations basically look the same right
the general rule is that ZL is equal to

00:02:53.960 --> 00:02:53.970
the general rule is that ZL is equal to
 

00:02:53.970 --> 00:03:02.760
the general rule is that ZL is equal to
WL times a of L minus 1 plus B L 1 there

00:03:02.760 --> 00:03:02.770
WL times a of L minus 1 plus B L 1 there
 

00:03:02.770 --> 00:03:05.730
WL times a of L minus 1 plus B L 1 there
and then the activations for that layer

00:03:05.730 --> 00:03:05.740
and then the activations for that layer
 

00:03:05.740 --> 00:03:10.610
and then the activations for that layer
is the activation function applied to

00:03:10.610 --> 00:03:10.620
is the activation function applied to
 

00:03:10.620 --> 00:03:16.830
is the activation function applied to
the values Z so that's the general for

00:03:16.830 --> 00:03:16.840
the values Z so that's the general for
 

00:03:16.840 --> 00:03:20.280
the values Z so that's the general for
propagation equation so we've done all

00:03:20.280 --> 00:03:20.290
propagation equation so we've done all
 

00:03:20.290 --> 00:03:23.550
propagation equation so we've done all
this for a single training example how

00:03:23.550 --> 00:03:23.560
this for a single training example how
 

00:03:23.560 --> 00:03:26.310
this for a single training example how
about for doing it in a vectorized way

00:03:26.310 --> 00:03:26.320
about for doing it in a vectorized way
 

00:03:26.320 --> 00:03:29.640
about for doing it in a vectorized way
for the whole training set at the same

00:03:29.640 --> 00:03:29.650
for the whole training set at the same
 

00:03:29.650 --> 00:03:32.699
for the whole training set at the same
time the equations look quite similar as

00:03:32.699 --> 00:03:32.709
time the equations look quite similar as
 

00:03:32.709 --> 00:03:34.979
time the equations look quite similar as
before for the first layer you would

00:03:34.979 --> 00:03:34.989
before for the first layer you would
 

00:03:34.989 --> 00:03:40.070
before for the first layer you would
have Capital Z 1 equals W 1 times

00:03:40.070 --> 00:03:40.080
have Capital Z 1 equals W 1 times
 

00:03:40.080 --> 00:03:48.390
have Capital Z 1 equals W 1 times
capital X plus B 1 and then a 1 equals G

00:03:48.390 --> 00:03:48.400
capital X plus B 1 and then a 1 equals G
 

00:03:48.400 --> 00:03:54.960
capital X plus B 1 and then a 1 equals G
of Z 1 right and bear in mind that X is

00:03:54.960 --> 00:03:54.970
of Z 1 right and bear in mind that X is
 

00:03:54.970 --> 00:03:58.590
of Z 1 right and bear in mind that X is
equal to a 0 these are just Neil the

00:03:58.590 --> 00:03:58.600
equal to a 0 these are just Neil the
 

00:03:58.600 --> 00:04:00.240
equal to a 0 these are just Neil the
training examples stacked in different

00:04:00.240 --> 00:04:00.250
training examples stacked in different
 

00:04:00.250 --> 00:04:02.400
training examples stacked in different
columns you could take this let me

00:04:02.400 --> 00:04:02.410
columns you could take this let me
 

00:04:02.410 --> 00:04:05.940
columns you could take this let me
scratch out X we can put a 0 there and

00:04:05.940 --> 00:04:05.950
scratch out X we can put a 0 there and
 

00:04:05.950 --> 00:04:08.670
scratch out X we can put a 0 there and
then for the next layer a little similar

00:04:08.670 --> 00:04:08.680
then for the next layer a little similar
 

00:04:08.680 --> 00:04:16.699
then for the next layer a little similar
Z 2 equals W 2 A 1 plus B 2 and a 2

00:04:16.699 --> 00:04:16.709
Z 2 equals W 2 A 1 plus B 2 and a 2
 

00:04:16.709 --> 00:04:23.170
Z 2 equals W 2 A 1 plus B 2 and a 2
equals G of Z 2 right we just take

00:04:23.170 --> 00:04:23.180
equals G of Z 2 right we just take
 

00:04:23.180 --> 00:04:26.230
equals G of Z 2 right we just take
these vector Z or a and so on and

00:04:26.230 --> 00:04:26.240
these vector Z or a and so on and
 

00:04:26.240 --> 00:04:29.320
these vector Z or a and so on and
stacking them up so this is V vector for

00:04:29.320 --> 00:04:29.330
stacking them up so this is V vector for
 

00:04:29.330 --> 00:04:32.620
stacking them up so this is V vector for
the first training example V vector for

00:04:32.620 --> 00:04:32.630
the first training example V vector for
 

00:04:32.630 --> 00:04:35.740
the first training example V vector for
the second training example and so on

00:04:35.740 --> 00:04:35.750
the second training example and so on
 

00:04:35.750 --> 00:04:38.050
the second training example and so on
down to the M training example and

00:04:38.050 --> 00:04:38.060
down to the M training example and
 

00:04:38.060 --> 00:04:40.930
down to the M training example and
stacking these and columns and calling

00:04:40.930 --> 00:04:40.940
stacking these and columns and calling
 

00:04:40.940 --> 00:04:44.820
stacking these and columns and calling
this capital V alright and similarly for

00:04:44.820 --> 00:04:44.830
this capital V alright and similarly for
 

00:04:44.830 --> 00:04:48.430
this capital V alright and similarly for
capital A just as capital X all the

00:04:48.430 --> 00:04:48.440
capital A just as capital X all the
 

00:04:48.440 --> 00:04:50.020
capital A just as capital X all the
training examples are column vectors

00:04:50.020 --> 00:04:50.030
training examples are column vectors
 

00:04:50.030 --> 00:04:52.300
training examples are column vectors
snacks left to right and then they then

00:04:52.300 --> 00:04:52.310
snacks left to right and then they then
 

00:04:52.310 --> 00:04:54.189
snacks left to right and then they then
end of this process you end up with y

00:04:54.189 --> 00:04:54.199
end of this process you end up with y
 

00:04:54.199 --> 00:05:00.010
end of this process you end up with y
hat which is equal to G of Z 400 this is

00:05:00.010 --> 00:05:00.020
hat which is equal to G of Z 400 this is
 

00:05:00.020 --> 00:05:03.189
hat which is equal to G of Z 400 this is
also equal to a 4 and that's the

00:05:03.189 --> 00:05:03.199
also equal to a 4 and that's the
 

00:05:03.199 --> 00:05:04.719
also equal to a 4 and that's the
predictions on all the new training

00:05:04.719 --> 00:05:04.729
predictions on all the new training
 

00:05:04.729 --> 00:05:07.990
predictions on all the new training
examples of stand horizontally so just

00:05:07.990 --> 00:05:08.000
examples of stand horizontally so just
 

00:05:08.000 --> 00:05:09.999
examples of stand horizontally so just
to summarize our notation I'm going to

00:05:09.999 --> 00:05:10.009
to summarize our notation I'm going to
 

00:05:10.009 --> 00:05:12.730
to summarize our notation I'm going to
modify this up here our notation allows

00:05:12.730 --> 00:05:12.740
modify this up here our notation allows
 

00:05:12.740 --> 00:05:17.710
modify this up here our notation allows
us to replace lowercase Z and a with the

00:05:17.710 --> 00:05:17.720
us to replace lowercase Z and a with the
 

00:05:17.720 --> 00:05:19.930
us to replace lowercase Z and a with the
uppercase counterparts it already looks

00:05:19.930 --> 00:05:19.940
uppercase counterparts it already looks
 

00:05:19.940 --> 00:05:22.390
uppercase counterparts it already looks
like a capital D and that gives you the

00:05:22.390 --> 00:05:22.400
like a capital D and that gives you the
 

00:05:22.400 --> 00:05:23.860
like a capital D and that gives you the
vectorized version of forward

00:05:23.860 --> 00:05:23.870
vectorized version of forward
 

00:05:23.870 --> 00:05:25.810
vectorized version of forward
propagation that you carry out on the

00:05:25.810 --> 00:05:25.820
propagation that you carry out on the
 

00:05:25.820 --> 00:05:29.620
propagation that you carry out on the
entire training set at a time where a 0

00:05:29.620 --> 00:05:29.630
entire training set at a time where a 0
 

00:05:29.630 --> 00:05:32.980
entire training set at a time where a 0
is X now if you look at this

00:05:32.980 --> 00:05:32.990
is X now if you look at this
 

00:05:32.990 --> 00:05:35.230
is X now if you look at this
implementation of vectorization it looks

00:05:35.230 --> 00:05:35.240
implementation of vectorization it looks
 

00:05:35.240 --> 00:05:37.659
implementation of vectorization it looks
like that there is going to be a for

00:05:37.659 --> 00:05:37.669
like that there is going to be a for
 

00:05:37.669 --> 00:05:40.360
like that there is going to be a for
loop here where it says left for l

00:05:40.360 --> 00:05:40.370
loop here where it says left for l
 

00:05:40.370 --> 00:05:44.350
loop here where it says left for l
equals 1 to 4 for l equals 1 through

00:05:44.350 --> 00:05:44.360
equals 1 to 4 for l equals 1 through
 

00:05:44.360 --> 00:05:46.990
equals 1 to 4 for l equals 1 through
capital l then you have to compute the

00:05:46.990 --> 00:05:47.000
capital l then you have to compute the
 

00:05:47.000 --> 00:05:48.939
capital l then you have to compute the
activations for layer 1 and the layer 2

00:05:48.939 --> 00:05:48.949
activations for layer 1 and the layer 2
 

00:05:48.949 --> 00:05:50.710
activations for layer 1 and the layer 2
then for layer 3 and they're gentle

00:05:50.710 --> 00:05:50.720
then for layer 3 and they're gentle
 

00:05:50.720 --> 00:05:53.200
then for layer 3 and they're gentle
therefore so since that there is a for

00:05:53.200 --> 00:05:53.210
therefore so since that there is a for
 

00:05:53.210 --> 00:05:55.960
therefore so since that there is a for
loop here and I know that when

00:05:55.960 --> 00:05:55.970
loop here and I know that when
 

00:05:55.970 --> 00:05:57.700
loop here and I know that when
implementing your networks we usually

00:05:57.700 --> 00:05:57.710
implementing your networks we usually
 

00:05:57.710 --> 00:05:59.469
implementing your networks we usually
want to get rid of explicit for loops

00:05:59.469 --> 00:05:59.479
want to get rid of explicit for loops
 

00:05:59.479 --> 00:06:02.110
want to get rid of explicit for loops
but this is one place where I don't

00:06:02.110 --> 00:06:02.120
but this is one place where I don't
 

00:06:02.120 --> 00:06:04.060
but this is one place where I don't
think there's any way to implement this

00:06:04.060 --> 00:06:04.070
think there's any way to implement this
 

00:06:04.070 --> 00:06:05.890
think there's any way to implement this
other than explicit for loop so we're

00:06:05.890 --> 00:06:05.900
other than explicit for loop so we're
 

00:06:05.900 --> 00:06:07.570
other than explicit for loop so we're
implementing for propagation it is

00:06:07.570 --> 00:06:07.580
implementing for propagation it is
 

00:06:07.580 --> 00:06:09.730
implementing for propagation it is
perfectly okay to have a for loop that

00:06:09.730 --> 00:06:09.740
perfectly okay to have a for loop that
 

00:06:09.740 --> 00:06:11.830
perfectly okay to have a for loop that
compute the activations for layer 1 then

00:06:11.830 --> 00:06:11.840
compute the activations for layer 1 then
 

00:06:11.840 --> 00:06:13.330
compute the activations for layer 1 then
there are 2 then they are threes and

00:06:13.330 --> 00:06:13.340
there are 2 then they are threes and
 

00:06:13.340 --> 00:06:15.760
there are 2 then they are threes and
therefore no one knows and I don't think

00:06:15.760 --> 00:06:15.770
therefore no one knows and I don't think
 

00:06:15.770 --> 00:06:18.730
therefore no one knows and I don't think
there is this any way to do this without

00:06:18.730 --> 00:06:18.740
there is this any way to do this without
 

00:06:18.740 --> 00:06:22.270
there is this any way to do this without
a for loop that goes from 1 to capital L

00:06:22.270 --> 00:06:22.280
a for loop that goes from 1 to capital L
 

00:06:22.280 --> 00:06:23.920
a for loop that goes from 1 to capital L
from 1 through the total number of

00:06:23.920 --> 00:06:23.930
from 1 through the total number of
 

00:06:23.930 --> 00:06:26.830
from 1 through the total number of
layers and in your network so this place

00:06:26.830 --> 00:06:26.840
layers and in your network so this place
 

00:06:26.840 --> 00:06:28.629
layers and in your network so this place
is perfectly okay to have an explicit

00:06:28.629 --> 00:06:28.639
is perfectly okay to have an explicit
 

00:06:28.639 --> 00:06:30.960
is perfectly okay to have an explicit
folder so

00:06:30.960 --> 00:06:30.970
folder so
 

00:06:30.970 --> 00:06:32.670
folder so
that's it for the notation for deep

00:06:32.670 --> 00:06:32.680
that's it for the notation for deep
 

00:06:32.680 --> 00:06:35.070
that's it for the notation for deep
neural networks as well as how to do

00:06:35.070 --> 00:06:35.080
neural networks as well as how to do
 

00:06:35.080 --> 00:06:37.380
neural networks as well as how to do
forward propagation in these networks if

00:06:37.380 --> 00:06:37.390
forward propagation in these networks if
 

00:06:37.390 --> 00:06:39.510
forward propagation in these networks if
the pieces we've seen so far looks a

00:06:39.510 --> 00:06:39.520
the pieces we've seen so far looks a
 

00:06:39.520 --> 00:06:40.950
the pieces we've seen so far looks a
little bit familiar to you that's

00:06:40.950 --> 00:06:40.960
little bit familiar to you that's
 

00:06:40.960 --> 00:06:43.290
little bit familiar to you that's
because what we're seeing is taking a

00:06:43.290 --> 00:06:43.300
because what we're seeing is taking a
 

00:06:43.300 --> 00:06:45.420
because what we're seeing is taking a
piece very similar to what you see in in

00:06:45.420 --> 00:06:45.430
piece very similar to what you see in in
 

00:06:45.430 --> 00:06:47.730
piece very similar to what you see in in
the neural network with a single hidden

00:06:47.730 --> 00:06:47.740
the neural network with a single hidden
 

00:06:47.740 --> 00:06:50.760
the neural network with a single hidden
layer and just repeating that more times

00:06:50.760 --> 00:06:50.770
layer and just repeating that more times
 

00:06:50.770 --> 00:06:53.070
layer and just repeating that more times
now it turns out that we implemented

00:06:53.070 --> 00:06:53.080
now it turns out that we implemented
 

00:06:53.080 --> 00:06:55.490
now it turns out that we implemented
deep neural network one of the ways to

00:06:55.490 --> 00:06:55.500
deep neural network one of the ways to
 

00:06:55.500 --> 00:06:57.810
deep neural network one of the ways to
increase your odds of having a bug-free

00:06:57.810 --> 00:06:57.820
increase your odds of having a bug-free
 

00:06:57.820 --> 00:06:59.490
increase your odds of having a bug-free
implementation is to think very

00:06:59.490 --> 00:06:59.500
implementation is to think very
 

00:06:59.500 --> 00:07:01.560
implementation is to think very
systematic and carefully about the

00:07:01.560 --> 00:07:01.570
systematic and carefully about the
 

00:07:01.570 --> 00:07:03.480
systematic and carefully about the
matrix dimensions you're working work so

00:07:03.480 --> 00:07:03.490
matrix dimensions you're working work so
 

00:07:03.490 --> 00:07:05.130
matrix dimensions you're working work so
when I'm trying to develop my own code

00:07:05.130 --> 00:07:05.140
when I'm trying to develop my own code
 

00:07:05.140 --> 00:07:07.050
when I'm trying to develop my own code
I'll often pull a piece of paper and

00:07:07.050 --> 00:07:07.060
I'll often pull a piece of paper and
 

00:07:07.060 --> 00:07:08.970
I'll often pull a piece of paper and
just think carefully through so the

00:07:08.970 --> 00:07:08.980
just think carefully through so the
 

00:07:08.980 --> 00:07:11.490
just think carefully through so the
dimensions of the matrix I'm working

00:07:11.490 --> 00:07:11.500
dimensions of the matrix I'm working
 

00:07:11.500 --> 00:07:13.920
dimensions of the matrix I'm working
with let's see how you could do that in

00:07:13.920 --> 00:07:13.930
with let's see how you could do that in
 

00:07:13.930 --> 00:07:16.560
with let's see how you could do that in
the next video

