WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.390
[MUSIC PLAYING]

00:00:05.934 --> 00:00:07.100
WOLFF DOBSON: Hi, everybody.

00:00:07.100 --> 00:00:09.460
I'm Wolff Dobson,
and I apparently

00:00:09.460 --> 00:00:10.450
can't walk up stairs.

00:00:10.450 --> 00:00:11.160
No.

00:00:11.160 --> 00:00:11.890
I'm Wolff Dobson.

00:00:11.890 --> 00:00:14.290
I'd like to talk to you
today about the frontiers

00:00:14.290 --> 00:00:16.360
of TensorFlow.

00:00:16.360 --> 00:00:17.652
So this talk's got three parts.

00:00:17.652 --> 00:00:19.151
We're going to talk
about the latest

00:00:19.151 --> 00:00:20.400
developments in TensorFlow.

00:00:20.400 --> 00:00:24.167
We're going to talk about
Cloud TPUs and TPU pods.

00:00:24.167 --> 00:00:26.500
And finally, Zak's going to
tell us about the TensorFlow

00:00:26.500 --> 00:00:29.620
research cloud.

00:00:29.620 --> 00:00:30.250
OK.

00:00:30.250 --> 00:00:33.670
So TensorFlow-- it's machine
learning for everyone.

00:00:33.670 --> 00:00:37.750
We like to say that it
is fast, it is flexible,

00:00:37.750 --> 00:00:39.427
and it is production ready.

00:00:39.427 --> 00:00:41.260
When we say "production
ready," what we mean

00:00:41.260 --> 00:00:44.230
is the code that you mess
around with on your laptop,

00:00:44.230 --> 00:00:46.750
that you try out on
your local machine--

00:00:46.750 --> 00:00:51.940
it's the same code that you push
out to run on giant clusters.

00:00:51.940 --> 00:00:53.500
It's all the same
thing, all the way

00:00:53.500 --> 00:00:55.270
down-- no rewriting
in a new language

00:00:55.270 --> 00:00:58.460
after you figured it out.

00:00:58.460 --> 00:01:02.120
So TensorFlow is
this language that

00:01:02.120 --> 00:01:05.810
allows you to write
computational graphs

00:01:05.810 --> 00:01:09.410
and use these computational
graphs to do, usually,

00:01:09.410 --> 00:01:12.510
machine learning and
often deep learning.

00:01:12.510 --> 00:01:16.490
So here's an example of a
basic deep learning graph.

00:01:16.490 --> 00:01:19.160
Here we have a picture
of a cat or a dog

00:01:19.160 --> 00:01:22.140
that we feed in as pixels
into the convolutional layer

00:01:22.140 --> 00:01:22.887
at the bottom.

00:01:22.887 --> 00:01:24.470
And then it convolves
all the way down

00:01:24.470 --> 00:01:26.720
to give us a prediction
of some kind.

00:01:26.720 --> 00:01:29.660
Either it's a cat,
or it's a dog.

00:01:29.660 --> 00:01:31.950
Convolutional errors
are one kind of thing.

00:01:31.950 --> 00:01:33.200
This is another kind of thing.

00:01:33.200 --> 00:01:37.010
This is a collection
of connected layers

00:01:37.010 --> 00:01:39.950
where we have each node
on each layer connects

00:01:39.950 --> 00:01:42.170
to every other node
on the next layer.

00:01:42.170 --> 00:01:44.840
The arrows between
those are all weights.

00:01:44.840 --> 00:01:47.690
Those weights are-- we live
to call them parameters.

00:01:47.690 --> 00:01:51.760
And they get changed over time.

00:01:51.760 --> 00:01:57.750
This is a very deep network
where this is inception V3.

00:01:57.750 --> 00:02:00.610
Inception V3 has
about 17 layers,

00:02:00.610 --> 00:02:01.910
depending on how you count.

00:02:01.910 --> 00:02:04.770
And it has about 25
million parameters.

00:02:04.770 --> 00:02:06.400
So it can take a while to train.

00:02:08.932 --> 00:02:11.140
And so what do you do when
you're writing TensorFlow?

00:02:11.140 --> 00:02:13.629
You're writing probably
Python, and you're going

00:02:13.629 --> 00:02:14.920
to knock out some Python there.

00:02:14.920 --> 00:02:17.120
And it's going to generate
this graph for you.

00:02:17.120 --> 00:02:20.397
And then that graph, you're
going to execute later.

00:02:20.397 --> 00:02:22.230
And you're going can
execute it in two ways.

00:02:22.230 --> 00:02:24.560
One, you're going to
be doing training,

00:02:24.560 --> 00:02:28.730
which means that you're going to
be showing examples and, based

00:02:28.730 --> 00:02:31.370
on the results you get,
modifying those parameters.

00:02:31.370 --> 00:02:36.380
And then you can run it
again later on new data.

00:02:36.380 --> 00:02:37.571
We call that inference.

00:02:37.571 --> 00:02:40.070
And we're hoping to get some
kind of statistical insight out

00:02:40.070 --> 00:02:44.130
of all the data that we
have onto this new data.

00:02:44.130 --> 00:02:47.390
TensorFlow runs on all
kinds of platforms--

00:02:47.390 --> 00:02:48.910
CPU, GPU.

00:02:48.910 --> 00:02:50.920
Obviously, we're going
to talk about TPUs later.

00:02:50.920 --> 00:02:52.690
They're pretty neat.

00:02:52.690 --> 00:02:55.390
And also on mobile,
iOS and Android.

00:02:55.390 --> 00:02:58.240
And actually, also
on Raspberry Pi.

00:02:58.240 --> 00:02:59.110
Raspberry Pi is fun.

00:02:59.110 --> 00:03:03.550
You can actually compile all
of TensorFlow on Raspberry Pi.

00:03:03.550 --> 00:03:06.370
And we support all
kinds of languages.

00:03:06.370 --> 00:03:10.270
Google supports Python,
C++, Java, and Go.

00:03:10.270 --> 00:03:11.937
These others are all
community projects.

00:03:11.937 --> 00:03:14.186
We're really excited about
the community picking these

00:03:14.186 --> 00:03:16.780
up and using our low-level C
bindings to create new language

00:03:16.780 --> 00:03:19.450
wrappers.

00:03:19.450 --> 00:03:22.540
And TensorFlow comes with a lot
of different kinds of tools.

00:03:22.540 --> 00:03:24.690
This tool is TensorBoard.

00:03:24.690 --> 00:03:27.580
TensorBoard is sort of like
x-ray for your training

00:03:27.580 --> 00:03:28.120
process.

00:03:28.120 --> 00:03:30.645
It allows you to look
inside your training

00:03:30.645 --> 00:03:32.770
and see what's actually
going on inside your graph,

00:03:32.770 --> 00:03:34.030
let's you inspect your graph.

00:03:34.030 --> 00:03:36.280
And in this case, we're even
showing off an embedding.

00:03:36.280 --> 00:03:39.070
That's MNIST, for those
of you who recognize it.

00:03:39.070 --> 00:03:41.010
It's handwriting data.

00:03:41.010 --> 00:03:43.570
And you can see we're kind of
grouping all the fives as gray

00:03:43.570 --> 00:03:45.949
and all the nines as red.

00:03:45.949 --> 00:03:47.740
It's really neat to be
able to look and see

00:03:47.740 --> 00:03:51.190
what's actually going on.

00:03:51.190 --> 00:03:54.220
And once you've got
this all trained up

00:03:54.220 --> 00:03:56.050
and you want to push
it into production,

00:03:56.050 --> 00:03:57.530
we have TensorFlow Serving.

00:03:57.530 --> 00:04:00.130
TensorFlow Serving is a
separate open source project,

00:04:00.130 --> 00:04:03.760
also developed by Google,
that lets you kind of manage

00:04:03.760 --> 00:04:07.460
the process of serving
a model in production.

00:04:07.460 --> 00:04:09.910
And unless you do things
like serve multiple models

00:04:09.910 --> 00:04:12.550
in production, use it with
some kind of container engine,

00:04:12.550 --> 00:04:14.770
like Kubernetes, it
allows you to manage

00:04:14.770 --> 00:04:17.950
the process of setting up one
of these big services that

00:04:17.950 --> 00:04:21.940
will allow you to do
insight for your customers.

00:04:21.940 --> 00:04:23.440
There's a great talk about this.

00:04:23.440 --> 00:04:24.280
I encourage you.

00:04:24.280 --> 00:04:28.620
Noah Fidel is going to be
speaking about it on Friday.

00:04:28.620 --> 00:04:31.860
So inside Google,
TensorFlow is very popular.

00:04:31.860 --> 00:04:35.400
This is a graph of
directories inside Google

00:04:35.400 --> 00:04:37.897
with model description files
in it, which is to say,

00:04:37.897 --> 00:04:39.480
this is like a little
piece of machine

00:04:39.480 --> 00:04:42.060
learning that we have inside
our main Google repository.

00:04:42.060 --> 00:04:47.010
And you can see TensorFlow
launched in Q3 2014,

00:04:47.010 --> 00:04:48.159
internally.

00:04:48.159 --> 00:04:49.950
We actually had another
system before this.

00:04:49.950 --> 00:04:52.550
It was called DistBelief.

00:04:52.550 --> 00:04:53.610
DistBelief was great.

00:04:53.610 --> 00:04:56.010
It wasn't quite flexible enough
for the kinds of projects

00:04:56.010 --> 00:04:57.400
that we wanted to do.

00:04:57.400 --> 00:05:01.380
And you can see after we
launched TensorFlow internally,

00:05:01.380 --> 00:05:03.360
there was this kind
of explosion of trying

00:05:03.360 --> 00:05:07.770
to apply ML to every kind
of problem at Google--

00:05:07.770 --> 00:05:10.920
Search, Gmail, Translate, Maps--

00:05:10.920 --> 00:05:11.750
all kinds of stuff.

00:05:14.950 --> 00:05:18.820
And in November 2015, we
launched TensorFlow in public.

00:05:21.360 --> 00:05:24.970
It quickly became one
of the most popular--

00:05:24.970 --> 00:05:27.510
actually, the most popular--
machine learning GitHub

00:05:27.510 --> 00:05:28.570
project.

00:05:28.570 --> 00:05:32.820
And when we say "most popular,"
we mean really most popular.

00:05:32.820 --> 00:05:37.390
This is a graph of all
of our stars over time.

00:05:37.390 --> 00:05:39.070
We're really happy
about community.

00:05:39.070 --> 00:05:43.800
And speaking of community,
since November 2015,

00:05:43.800 --> 00:05:47.940
we've had 17,000 commits
in public, many of those

00:05:47.940 --> 00:05:51.340
from external contributors.

00:05:51.340 --> 00:05:54.970
We have almost 500
non-Googlers contributors

00:05:54.970 --> 00:06:02.080
And for TensorFlow version
1.0, which we cut in February.

00:06:02.080 --> 00:06:04.840
And we've actually had some very
significant external commits.

00:06:04.840 --> 00:06:07.630
One that comes to mind is
the Windows GPU support

00:06:07.630 --> 00:06:09.040
was actually external.

00:06:09.040 --> 00:06:12.880
And it was committed in,
and we absolutely loved it.

00:06:12.880 --> 00:06:15.050
And if you hunt
around on GitHub,

00:06:15.050 --> 00:06:18.160
you'll find thousands
of other repositories

00:06:18.160 --> 00:06:20.400
that have TensorFlow
in the title.

00:06:20.400 --> 00:06:22.900
And along with that
comes blogs and books

00:06:22.900 --> 00:06:25.024
and all kinds of other
materials that, again,

00:06:25.024 --> 00:06:26.440
the community is
putting together.

00:06:26.440 --> 00:06:29.800
And we're so happy
that they are.

00:06:29.800 --> 00:06:34.600
We also take supporting
TensorFlow seriously.

00:06:34.600 --> 00:06:37.690
One of the great things about
working on the TensorFlow team

00:06:37.690 --> 00:06:41.770
is our engineering team
actually takes rotations

00:06:41.770 --> 00:06:48.640
through Stack Overflow
and also issues and PRs.

00:06:48.640 --> 00:06:53.110
So we've had over 5,000 Stack
Overflow questions answered

00:06:53.110 --> 00:06:56.350
and, again, about 5,000
GitHub issues filed

00:06:56.350 --> 00:06:59.500
and triaged in some useful way.

00:06:59.500 --> 00:07:01.460
And we have a lot of activity.

00:07:01.460 --> 00:07:05.630
We're getting almost
160 new issues a week.

00:07:05.630 --> 00:07:07.580
So where are we today?

00:07:07.580 --> 00:07:10.020
Last night-- actually,
yesterday afternoon--

00:07:10.020 --> 00:07:12.800
we branched TensorFlow 1.2.

00:07:12.800 --> 00:07:14.660
There's a lot of new
stuff in Tensor 1.2

00:07:14.660 --> 00:07:18.060
around supporting TPUs.

00:07:18.060 --> 00:07:19.560
Brennan is going
to talk about that.

00:07:19.560 --> 00:07:24.080
But something that's
come into TensorFlow

00:07:24.080 --> 00:07:28.190
recently is XLS, which
is TensorFlow's compiler

00:07:28.190 --> 00:07:30.590
for Accelerated Linear Algebra--

00:07:30.590 --> 00:07:31.820
XLA.

00:07:31.820 --> 00:07:36.980
And basic idea is
TensorFlow graphs go in,

00:07:36.980 --> 00:07:41.082
and optimized assembly
comes out-- machine code

00:07:41.082 --> 00:07:42.040
that will actually run.

00:07:42.040 --> 00:07:44.300
It's not like an optimized
version of the graph.

00:07:44.300 --> 00:07:46.341
It's like-- it is optimized
version of the graph,

00:07:46.341 --> 00:07:48.719
but it's actually
running in machine code.

00:07:48.719 --> 00:07:50.510
Super important to
support custom hardware.

00:07:50.510 --> 00:07:54.790
And again, we'll talk more
about this with the TPU section.

00:07:54.790 --> 00:07:57.890
TensorFlow is a distributed
execution engine

00:07:57.890 --> 00:08:02.610
that runs on top of all these
other kernels for iOS, Android,

00:08:02.610 --> 00:08:04.730
GPUs, CPUs-- all
that kind of stuff.

00:08:04.730 --> 00:08:08.010
On top of that, we have
the Python frontend,

00:08:08.010 --> 00:08:09.560
which is how you program it.

00:08:09.560 --> 00:08:13.280
We have a C++ frontend as well
and all these other languages.

00:08:13.280 --> 00:08:16.970
Recently in TensorFlow,
we've added Layers API.

00:08:16.970 --> 00:08:20.060
And Layers API gives you
sort of big building blocks

00:08:20.060 --> 00:08:22.880
that are the right shape for
you to do basic machine learning

00:08:22.880 --> 00:08:25.850
tasks in the kinds of ways we've
done-- convolutional layers,

00:08:25.850 --> 00:08:28.340
fully connected layers--
all that kind of thing.

00:08:28.340 --> 00:08:33.830
And on top of that, we have
both Keras and Estimators.

00:08:33.830 --> 00:08:37.049
Keras is an API that actually
didn't originate at Google.

00:08:37.049 --> 00:08:41.530
But we liked it so much, it's
actually in contrib right now.

00:08:41.530 --> 00:08:46.760
Keras is a high-level API that
gives you even sort of easier

00:08:46.760 --> 00:08:51.660
powerful ways to fit
all these different--

00:08:51.660 --> 00:08:54.830
all your graphs together.

00:08:54.830 --> 00:08:57.745
And it's actually built on
top of our Layers API as well.

00:08:57.745 --> 00:08:59.120
And then we also
have Estimators.

00:08:59.120 --> 00:09:01.790
And Estimators are
a way to package up

00:09:01.790 --> 00:09:04.610
an abstract away sort of the
concepts of machine learning.

00:09:04.610 --> 00:09:09.470
It has fit and evaluate and
all the rest of that stuff.

00:09:09.470 --> 00:09:12.890
Again, Estimators are really
important for parallelization.

00:09:12.890 --> 00:09:16.880
And with Estimators, we've even
come with canned Estimators

00:09:16.880 --> 00:09:18.530
that are going to
help you just--

00:09:18.530 --> 00:09:21.920
if you just have some data and
you want to do a simple linear

00:09:21.920 --> 00:09:25.970
regression or linear classifier
or a bunch of fully connected

00:09:25.970 --> 00:09:26.730
layers--

00:09:26.730 --> 00:09:29.210
whatever it is that
you want, we might

00:09:29.210 --> 00:09:31.040
be able to give you a
canned estimator that

00:09:31.040 --> 00:09:32.870
allows you to do no
graph construction

00:09:32.870 --> 00:09:35.300
and just drop your data and go.

00:09:38.430 --> 00:09:40.200
The effect of all
these high-level APIs,

00:09:40.200 --> 00:09:42.990
like Keras and
canned Estimators--

00:09:42.990 --> 00:09:46.800
one of the things you're going
to see is code gets smaller.

00:09:46.800 --> 00:09:49.635
But beyond that, like I said,
a big thing about Estimators

00:09:49.635 --> 00:09:52.470
is they help you parallelize
your computation.

00:09:52.470 --> 00:09:55.350
And we're going to talk
about that more, because TPUs

00:09:55.350 --> 00:09:58.340
are super parallel.

00:09:58.340 --> 00:10:01.520
Speaking of performance,
let's talk about performance.

00:10:01.520 --> 00:10:05.642
About a week and a half ago, we
released a bunch of benchmarks.

00:10:05.642 --> 00:10:07.100
They're actually
on TensorFlow.org.

00:10:07.100 --> 00:10:09.690
There's a link down there.

00:10:09.690 --> 00:10:12.030
This is one of the
graphs on that page--

00:10:12.030 --> 00:10:15.120
NVIDIA DGX-1 running
on synthetic data

00:10:15.120 --> 00:10:18.544
on one, two, four,
or eight GPUs.

00:10:18.544 --> 00:10:20.210
And there's lot of
raw throughput there.

00:10:20.210 --> 00:10:23.240
You can see a whole bunch
of images per second.

00:10:23.240 --> 00:10:25.710
Images per second--
super important.

00:10:25.710 --> 00:10:28.220
Another thing that's really
important in machine learning

00:10:28.220 --> 00:10:30.740
is that you get good scale-up.

00:10:30.740 --> 00:10:33.630
What that means is
if I add another GPU,

00:10:33.630 --> 00:10:37.310
do I get another GPU's worth
of performance out of that?

00:10:37.310 --> 00:10:41.530
So in this graph, if you
go from four to eight GPUs,

00:10:41.530 --> 00:10:43.335
do you actually see
double the performance?

00:10:43.335 --> 00:10:45.210
It's a little bit hard
to read on this graph,

00:10:45.210 --> 00:10:48.010
so we made it a
little bit easier.

00:10:48.010 --> 00:10:50.290
An ideal scaling line
on this graph-- and this

00:10:50.290 --> 00:10:53.670
is GPUs on one access and
speed-up on the other.

00:10:53.670 --> 00:10:56.330
And you'd want, with one
GPU, you have 1x speed up.

00:10:56.330 --> 00:10:59.390
And eight GPUs, you
have 8x speed up.

00:10:59.390 --> 00:11:01.910
And you want a straight
line all the way down there.

00:11:01.910 --> 00:11:05.120
And on our test data, which
are common models in machine

00:11:05.120 --> 00:11:08.690
learning-- inception,
resonate 50--

00:11:08.690 --> 00:11:11.090
you can see we're
getting pretty close.

00:11:11.090 --> 00:11:12.404
On the left is synthetic data.

00:11:12.404 --> 00:11:13.820
Synthetic data is
when you're kind

00:11:13.820 --> 00:11:16.010
of holding your inputs
more or less constant.

00:11:16.010 --> 00:11:18.320
And you're just putting
the engine on cinder blocks

00:11:18.320 --> 00:11:21.670
and just revving the engine
as fast as you possibly can.

00:11:21.670 --> 00:11:25.100
At Google, we think it's
kind of important to try it

00:11:25.100 --> 00:11:26.640
with real data as well.

00:11:26.640 --> 00:11:28.580
So on the right, we
have the same test

00:11:28.580 --> 00:11:30.680
except with real data.

00:11:30.680 --> 00:11:32.850
And you can see it's
not quite as ideal.

00:11:32.850 --> 00:11:34.760
It's never going to be
ideal, because there's

00:11:34.760 --> 00:11:37.580
going to be overhead for doing
the parallel computation.

00:11:37.580 --> 00:11:40.590
But we're getting pretty close.

00:11:40.590 --> 00:11:42.630
And if you want to scale
out to lots of GPUs,

00:11:42.630 --> 00:11:44.550
we have benchmarks
for that as well.

00:11:44.550 --> 00:11:51.570
This goes out to 64 GPUs, this
time on K80s in the cloud.

00:11:51.570 --> 00:11:53.610
Again, you're looking for that--

00:11:53.610 --> 00:11:58.920
from 32 to 64, are you
getting double the throughput?

00:11:58.920 --> 00:12:00.300
But don't take my word for it.

00:12:00.300 --> 00:12:01.770
Try it yourself.

00:12:01.770 --> 00:12:04.380
Not only did we release the page
that has all these benchmarks

00:12:04.380 --> 00:12:06.759
and graphs on it, we
also released the code.

00:12:06.759 --> 00:12:09.300
So you can download it and run
it yourself on your own setup.

00:12:09.300 --> 00:12:13.290
And we have benchmarks on
NVIDIA DGX-1, Google Compute

00:12:13.290 --> 00:12:16.010
Engine, Amazon EC2.

00:12:16.010 --> 00:12:19.500
But please, try it on your own
setup and tell us how it goes.

00:12:19.500 --> 00:12:22.260
Also, that code is
tuned for speed.

00:12:22.260 --> 00:12:26.662
And so it's a good place to look
that if you're having issues

00:12:26.662 --> 00:12:28.620
with scale up, you can
take a look at our stuff

00:12:28.620 --> 00:12:31.140
and see how we did it and
see if that can help you.

00:12:33.720 --> 00:12:37.740
But 64 GPUs is a lot.

00:12:37.740 --> 00:12:40.380
But there's more than 64 GPUs.

00:12:40.380 --> 00:12:46.470
And with very large
models, you're

00:12:46.470 --> 00:12:50.310
going to need a lot more
computation to train them.

00:12:50.310 --> 00:12:56.300
And that kind of computation
is more powerful chips,

00:12:56.300 --> 00:12:59.520
faster communication
among accelerators,

00:12:59.520 --> 00:13:02.820
faster memory, and a fully
optimized software stack.

00:13:02.820 --> 00:13:06.150
So to tell you about
that-- to tell you

00:13:06.150 --> 00:13:08.760
about TPUs, which are
all of those things,

00:13:08.760 --> 00:13:11.029
I'm going to hand it off to Zak.

00:13:11.029 --> 00:13:12.070
ZAK STONE: Thanks, Wolff.

00:13:15.510 --> 00:13:16.719
I'm super excited about TPUs.

00:13:16.719 --> 00:13:19.260
And I'd like to share the story
of their development with you

00:13:19.260 --> 00:13:21.510
today.

00:13:21.510 --> 00:13:24.760
Some of you may remember
this image here.

00:13:24.760 --> 00:13:26.830
This is an image of
Google's first generation

00:13:26.830 --> 00:13:30.900
TPU, which Sundar revealed
at Google I/O last year.

00:13:30.900 --> 00:13:33.840
Google actually considered
building custom ASICs as early

00:13:33.840 --> 00:13:35.340
as 2006.

00:13:35.340 --> 00:13:38.620
But the situation got
more urgent in 2013

00:13:38.620 --> 00:13:41.070
when one of our star engineers
realized if all Android

00:13:41.070 --> 00:13:43.740
users spoke to their phones
for just three minutes a day,

00:13:43.740 --> 00:13:45.570
that might force
Google to double

00:13:45.570 --> 00:13:47.520
its number of data centers.

00:13:47.520 --> 00:13:52.410
So that drove this crash project
to develop our first TPU,

00:13:52.410 --> 00:13:56.430
which, in just 15 months
of building and deploying

00:13:56.430 --> 00:13:58.320
these in the data
centers, brought us

00:13:58.320 --> 00:14:00.480
to some pretty significant
performance gains

00:14:00.480 --> 00:14:06.030
across Search Ranking, Google
Photos, Street View, Google

00:14:06.030 --> 00:14:08.760
Translate, and a variety of
our other large scale machine

00:14:08.760 --> 00:14:11.080
learning applications.

00:14:11.080 --> 00:14:14.310
In fact, compared to
contemporary CPUs and TPUs,

00:14:14.310 --> 00:14:16.980
this first generation
TPU was 15 to 30x

00:14:16.980 --> 00:14:19.230
faster on these
internal workloads

00:14:19.230 --> 00:14:22.500
while being 30 to 80x
more power-efficient.

00:14:22.500 --> 00:14:25.140
But there was an important
limitation, namely

00:14:25.140 --> 00:14:29.280
that this first generation TPU
was designed for inference, not

00:14:29.280 --> 00:14:30.300
training.

00:14:30.300 --> 00:14:32.190
As Wolff mentioned
earlier, training

00:14:32.190 --> 00:14:34.050
is this enormously
complicated process

00:14:34.050 --> 00:14:36.810
of setting all those weights
in these giant machine learning

00:14:36.810 --> 00:14:37.500
models.

00:14:37.500 --> 00:14:39.960
And then inference is the
process of running the models.

00:14:39.960 --> 00:14:42.840
And so while the first
generation TPU was fantastic

00:14:42.840 --> 00:14:45.180
for running models that
had already been trained,

00:14:45.180 --> 00:14:47.730
Google still needed to
use other hardware--

00:14:47.730 --> 00:14:49.890
huge clusters of CPUs or GPUs--

00:14:49.890 --> 00:14:51.720
to train these models.

00:14:51.720 --> 00:14:54.760
That inspired the development
of our second generation

00:14:54.760 --> 00:14:59.340
TPU, which is what Sundar
announced yesterday.

00:14:59.340 --> 00:15:03.410
This device delivers
up to 180 teraflops

00:15:03.410 --> 00:15:06.870
of floating-point performance
and contains 64 gigabytes

00:15:06.870 --> 00:15:09.240
of ultra-high-bandwidth memory.

00:15:09.240 --> 00:15:12.960
The most important thing about
this new second generation TPU

00:15:12.960 --> 00:15:15.240
is that it's designed
to support both training

00:15:15.240 --> 00:15:17.700
and inference on one platform.

00:15:17.700 --> 00:15:19.710
And that turns out to
be really convenient,

00:15:19.710 --> 00:15:22.710
because it allows you
to develop your model

00:15:22.710 --> 00:15:25.230
and then run it in
exactly the same way.

00:15:25.230 --> 00:15:28.020
You don't have this
barrier of friction

00:15:28.020 --> 00:15:30.120
of having to
quantize the weights

00:15:30.120 --> 00:15:33.990
or figure out how to deploy
it on a separate platform.

00:15:33.990 --> 00:15:35.580
But for some of
Google's largest scale

00:15:35.580 --> 00:15:39.810
machine learning problems, even
180 teraflops isn't enough.

00:15:39.810 --> 00:15:43.200
And so from the beginning,
the second generation TPUs

00:15:43.200 --> 00:15:45.660
were designed to be
connected together

00:15:45.660 --> 00:15:49.590
with an ultra fast
custom network.

00:15:49.590 --> 00:15:52.890
We call these collections
of TPUs, pods.

00:15:52.890 --> 00:15:55.200
And the TPU pods
that you see here

00:15:55.200 --> 00:16:00.870
contain 64 of these new
second generation TPUs.

00:16:00.870 --> 00:16:05.400
If you add all that up, that
gives you up to 11.5 petaflops

00:16:05.400 --> 00:16:07.290
of machine learning
acceleration that you

00:16:07.290 --> 00:16:10.530
can apply to a single machine
learning training problem,

00:16:10.530 --> 00:16:13.080
or you can split up to support
many different training

00:16:13.080 --> 00:16:15.090
problems in parallel.

00:16:15.090 --> 00:16:17.790
This whole system has
four terabytes of memory.

00:16:17.790 --> 00:16:21.000
And the TPUs are connected in
a toroidal mesh network which

00:16:21.000 --> 00:16:23.700
facilitates the ultra fast,
ultra dense communication

00:16:23.700 --> 00:16:26.596
that you need, especially to
solve these machine learning

00:16:26.596 --> 00:16:27.345
training problems.

00:16:30.740 --> 00:16:32.510
We're already seeing
interesting results

00:16:32.510 --> 00:16:34.430
on some of the important
internal workloads

00:16:34.430 --> 00:16:35.720
that matter to us.

00:16:35.720 --> 00:16:39.260
For example, on one of our
new large-scale neural machine

00:16:39.260 --> 00:16:41.390
translation models,
it used to take us

00:16:41.390 --> 00:16:45.980
24 hours to train this model
on 32 of the best commercially

00:16:45.980 --> 00:16:47.510
available GPUs.

00:16:47.510 --> 00:16:50.900
Now in just six hours, we can
train to the same accuracy

00:16:50.900 --> 00:16:52.790
on 1/8 of a TPU pod.

00:16:55.910 --> 00:16:58.839
These new devices-- these
second generation TPUs--

00:16:58.839 --> 00:17:00.380
we're thrilled to
announce are coming

00:17:00.380 --> 00:17:03.560
to Google Cloud as cloud TPUs.

00:17:03.560 --> 00:17:05.450
Our goal is for Google
Cloud to be the best

00:17:05.450 --> 00:17:07.310
cloud for machine learning.

00:17:07.310 --> 00:17:10.220
So we offer customers the
choice of the best hardware

00:17:10.220 --> 00:17:12.260
that you can find
anywhere, whether that's

00:17:12.260 --> 00:17:15.050
Skylake CPUs, NVIDIA's GPUs.

00:17:15.050 --> 00:17:17.089
We're planning to
introduce Volta and now

00:17:17.089 --> 00:17:19.760
this new member of the
family, clouds TPUs,

00:17:19.760 --> 00:17:22.970
so customers can build the
best machine learning systems

00:17:22.970 --> 00:17:25.040
for their specific workloads.

00:17:25.040 --> 00:17:28.406
To tell you a little bit more
about how cloud TPUs are going

00:17:28.406 --> 00:17:29.780
to integrate with
TensorFlow, I'd

00:17:29.780 --> 00:17:31.071
like to hand it off to Brennan.

00:17:34.060 --> 00:17:36.080
BRENNAN SAETA: Thank you, Zak.

00:17:36.080 --> 00:17:40.040
Today, I want to talk
to you about cloud TPUs.

00:17:40.040 --> 00:17:42.800
Now as we've
designed cloud TPUs,

00:17:42.800 --> 00:17:45.860
we wanted to ensure that
it would meet your needs,

00:17:45.860 --> 00:17:47.930
whether you are a
researcher or whether you're

00:17:47.930 --> 00:17:50.990
designing a high-skill,
high-performance machine

00:17:50.990 --> 00:17:52.440
learning product.

00:17:52.440 --> 00:17:54.590
And so for
researchers, we wanted

00:17:54.590 --> 00:17:56.960
to ensure that it was
interactive and low-level.

00:17:56.960 --> 00:18:00.050
You can play around with
your models and debug them.

00:18:00.050 --> 00:18:03.110
And for high-performance,
high-scale platforms

00:18:03.110 --> 00:18:05.090
and products, we
wanted to ensure

00:18:05.090 --> 00:18:08.900
that it would integrate with the
rest of Google Cloud Platform.

00:18:08.900 --> 00:18:12.000
But I think that instead of
me just telling you about it,

00:18:12.000 --> 00:18:15.030
it's more exciting
if I show you.

00:18:15.030 --> 00:18:19.820
Now before I show you, I want
to say an important caveat.

00:18:19.820 --> 00:18:23.329
As we bring TPUs
to Google Cloud,

00:18:23.329 --> 00:18:24.620
there's a lot of moving pieces.

00:18:24.620 --> 00:18:27.380
And so some of those pieces are
still under active development.

00:18:27.380 --> 00:18:29.300
So things may change.

00:18:29.300 --> 00:18:33.090
That said, let's get started.

00:18:33.090 --> 00:18:34.850
The simplest model
that I think I can show

00:18:34.850 --> 00:18:39.590
you is alpha times x plus y.

00:18:39.590 --> 00:18:42.740
So alpha is the constant 2.

00:18:42.740 --> 00:18:47.770
x is a vector, or we call
it tensor, of three 1's.

00:18:47.770 --> 00:18:49.200
y is also three 1's.

00:18:49.200 --> 00:18:51.230
And we hope to, at the
end of this computation,

00:18:51.230 --> 00:18:54.410
get three 3's.

00:18:54.410 --> 00:18:56.210
In order to use
Google Cloud TPUs,

00:18:56.210 --> 00:18:57.980
you've got to first create one.

00:18:57.980 --> 00:19:00.980
When you create one,
this is your cloud TPU.

00:19:00.980 --> 00:19:03.050
And so in order to
access it, however,

00:19:03.050 --> 00:19:06.200
you need to access it via
Google Compute Engine.

00:19:06.200 --> 00:19:10.610
So we first create a virtual
machine in Google's Compute

00:19:10.610 --> 00:19:11.960
Engine product.

00:19:11.960 --> 00:19:14.090
And we're going to
name it Demo VM.

00:19:14.090 --> 00:19:16.250
Additionally, we're
going to create a TPU.

00:19:16.250 --> 00:19:19.900
And we're going to
call that Demo TPU.

00:19:19.900 --> 00:19:23.240
If we then log in
to our demo VM,

00:19:23.240 --> 00:19:26.221
this is what we could see
after we install TensorFlow.

00:19:30.640 --> 00:19:34.540
So we're going to run the
Python interactive interpreter.

00:19:34.540 --> 00:19:38.380
And we're going to import
TensorFlow just like normal.

00:19:38.380 --> 00:19:41.320
After that, we're going to
connect to our cloud TPU

00:19:41.320 --> 00:19:46.390
using GRPC, Google's open
source RPC framework.

00:19:46.390 --> 00:19:50.440
After that, you use TensorFlow
just like you normally would,

00:19:50.440 --> 00:19:52.930
whether on a CPU or a GPU.

00:19:52.930 --> 00:19:55.060
The only difference,
however, is that you

00:19:55.060 --> 00:19:58.390
define your computations that
you want to run on the TPU

00:19:58.390 --> 00:20:01.630
in this with TF.deviceblock.

00:20:01.630 --> 00:20:04.780
In doing so, you tell TensorFlow
to run the computation

00:20:04.780 --> 00:20:06.640
on the TPU.

00:20:06.640 --> 00:20:09.040
After that, you can
just run it, again,

00:20:09.040 --> 00:20:10.840
just like normal TensorFlow.

00:20:10.840 --> 00:20:12.119
And it just works.

00:20:12.119 --> 00:20:12.910
It's very exciting.

00:20:16.310 --> 00:20:19.000
So to review what
just happened, we

00:20:19.000 --> 00:20:21.760
logged in to our Google
Compute Engine VM.

00:20:21.760 --> 00:20:24.940
We ran our TensorFlow and our
Python interactive interpreter.

00:20:24.940 --> 00:20:27.610
We connected to the cloud
TPU and ran our computation

00:20:27.610 --> 00:20:30.580
on the cloud TPU and
fetched the results back

00:20:30.580 --> 00:20:33.880
into our Google
Compute Engine VM.

00:20:33.880 --> 00:20:36.760
But this one VM
and one cloud TPU

00:20:36.760 --> 00:20:39.230
is really only the beginning.

00:20:39.230 --> 00:20:41.890
We've designed cloud
TPUs to integrate

00:20:41.890 --> 00:20:43.540
with all the rest
of the hardware

00:20:43.540 --> 00:20:48.550
and compute platforms available
in Google Cloud Platform.

00:20:48.550 --> 00:20:52.520
So if you have a model that
part of the model runs really,

00:20:52.520 --> 00:20:56.020
really well on brand new
GPUs and part of your model

00:20:56.020 --> 00:20:58.810
runs really, really
well on cloud TPUs,

00:20:58.810 --> 00:21:02.770
you can create your own
esoteric heterogeneous cluster

00:21:02.770 --> 00:21:06.070
and place your model so that the
right parts of the computation

00:21:06.070 --> 00:21:08.005
run on the right compute nodes.

00:21:10.600 --> 00:21:12.850
But one thing we've
learned at Google

00:21:12.850 --> 00:21:14.560
is that as you
push the boundaries

00:21:14.560 --> 00:21:19.450
of computational power--
as you push extreme scale,

00:21:19.450 --> 00:21:24.100
you need to optimize the
entire system from end to end.

00:21:24.100 --> 00:21:26.770
And what I mean is, I mean you
need to take into account not

00:21:26.770 --> 00:21:28.390
just the amount
of disks that you

00:21:28.390 --> 00:21:31.990
use to feed your data quickly
into these accelerator pods

00:21:31.990 --> 00:21:34.450
and not just the
network that connects

00:21:34.450 --> 00:21:37.570
all of these distributed
machines together,

00:21:37.570 --> 00:21:41.210
but you also need a highly
tuned software platform.

00:21:41.210 --> 00:21:42.730
And to tell you
about that, I'd like

00:21:42.730 --> 00:21:44.771
to tell you about some of
the latest changes that

00:21:44.771 --> 00:21:47.260
are happening that are still
under active development

00:21:47.260 --> 00:21:48.847
in Core TensorFlow.

00:21:52.190 --> 00:21:54.860
Now previously in
the presentation,

00:21:54.860 --> 00:21:58.440
we had a picture of the
Core TensorFlow platform.

00:21:58.440 --> 00:22:02.500
And I've redrawn that,
adding a few extra boxes.

00:22:02.500 --> 00:22:06.770
And I'm going to talk to you
today about XLA, Estimators,

00:22:06.770 --> 00:22:08.930
and Datasets.

00:22:08.930 --> 00:22:11.390
Let's start with closest
to the bare metal.

00:22:11.390 --> 00:22:12.310
Let's talk about XLA.

00:22:15.630 --> 00:22:19.090
TensorFlow is designed to go
from research to production.

00:22:19.090 --> 00:22:22.290
And production means not just
servers in a data center,

00:22:22.290 --> 00:22:24.630
but it means going all
the way from phones

00:22:24.630 --> 00:22:29.280
to these exotic hardware
compute devices like TPUs.

00:22:29.280 --> 00:22:31.470
And in order to run
efficiently and to get

00:22:31.470 --> 00:22:35.730
the extreme performance, you
need a highly-tuned software

00:22:35.730 --> 00:22:38.010
stack.

00:22:38.010 --> 00:22:42.480
As it turns out, in order
to get optimized code,

00:22:42.480 --> 00:22:45.080
you need to bring
your own compiler.

00:22:45.080 --> 00:22:48.330
XLA is TensorFlow's compiler.

00:22:48.330 --> 00:22:52.350
XLA stands for Accelerated
Linear Algebra.

00:22:52.350 --> 00:22:54.420
And what it does--

00:22:54.420 --> 00:22:55.980
when you use
TensorFlow, you define

00:22:55.980 --> 00:22:57.750
your computational graph.

00:22:57.750 --> 00:23:00.540
And XLA will take a
subset of that graph,

00:23:00.540 --> 00:23:02.660
or maybe even the whole graph.

00:23:02.660 --> 00:23:06.420
It will run a huge number of
optimization passes over it.

00:23:06.420 --> 00:23:09.990
And it will output
exactly the machine binary

00:23:09.990 --> 00:23:13.590
to run on the hardware
that you're targeting.

00:23:13.590 --> 00:23:18.360
In this example, we're taking
the inception algorithm

00:23:18.360 --> 00:23:21.780
that does extreme performance
on image recognition.

00:23:21.780 --> 00:23:25.020
And XLA will take a
subset of that graph.

00:23:25.020 --> 00:23:27.150
And it will reorder
the computations

00:23:27.150 --> 00:23:30.270
and output the
assembly in binary form

00:23:30.270 --> 00:23:34.730
to run efficiently
on CPUs or GPUs.

00:23:34.730 --> 00:23:40.290
But XLA was designed for the
explosion in exotic hardware

00:23:40.290 --> 00:23:42.840
that we see in machine learning.

00:23:42.840 --> 00:23:46.440
XLA has a pluggable backend
so that you can target

00:23:46.440 --> 00:23:48.420
your own hardware elements.

00:23:51.140 --> 00:23:55.040
TPUs are just a custom
backend for XLA.

00:23:55.040 --> 00:23:56.570
And in doing so,
we take advantage

00:23:56.570 --> 00:24:00.250
of all the optimizations
that we do for CPUs and GPUs,

00:24:00.250 --> 00:24:02.520
and we're able to run
them also on TPUs.

00:24:05.150 --> 00:24:07.460
But XLA runs sort of
underneath the hood.

00:24:07.460 --> 00:24:09.650
And so it's sometimes
a little hard to see.

00:24:09.650 --> 00:24:11.930
So if you'd like to
see XLA in action,

00:24:11.930 --> 00:24:14.630
you can turn on Tracing
when you run your TensorFlow

00:24:14.630 --> 00:24:16.010
computation.

00:24:16.010 --> 00:24:18.500
And if you print out the
trace information that's

00:24:18.500 --> 00:24:21.050
stored in that Run
Metadata object,

00:24:21.050 --> 00:24:23.900
you might see
something like this.

00:24:23.900 --> 00:24:27.500
In one of the node stats, so we
see this funny underscore XLA

00:24:27.500 --> 00:24:28.910
Launch operation.

00:24:28.910 --> 00:24:31.520
If you're not using
XLA, the node stats

00:24:31.520 --> 00:24:33.940
will be your
TensorFlow graph nodes.

00:24:33.940 --> 00:24:38.030
But what happens when you
use XLA is the XLA compiler

00:24:38.030 --> 00:24:40.760
generates that optimized binary.

00:24:40.760 --> 00:24:44.360
And the XLA Launch operation
is the time spent actually

00:24:44.360 --> 00:24:48.330
running that binary on
your compute devices.

00:24:48.330 --> 00:24:52.040
But one thing that's
interesting is the node

00:24:52.040 --> 00:24:55.130
names don't
correspond necessarily

00:24:55.130 --> 00:24:57.140
with your TensorFlow graph.

00:24:57.140 --> 00:25:01.250
And the reason for that is
that XLA performs whole program

00:25:01.250 --> 00:25:02.810
optimization.

00:25:02.810 --> 00:25:04.910
If you've ever used
an optimizing compiler

00:25:04.910 --> 00:25:07.762
and looked at the assembly
that's output afterwards,

00:25:07.762 --> 00:25:09.470
you'll notice that it
doesn't necessarily

00:25:09.470 --> 00:25:11.570
match up with your
statements that you write in,

00:25:11.570 --> 00:25:13.860
for example, C.

00:25:13.860 --> 00:25:15.912
That's because the
optimizing compiler

00:25:15.912 --> 00:25:17.870
is doing a number of
tricks underneath the hood

00:25:17.870 --> 00:25:20.480
to get optimal
machine performance.

00:25:20.480 --> 00:25:22.910
And XLA is doing that
not just at the function

00:25:22.910 --> 00:25:26.630
or the operation level, but
it's scheduling whole programs

00:25:26.630 --> 00:25:28.050
to run efficiently.

00:25:30.870 --> 00:25:32.820
So we've talked
about XLA or how you

00:25:32.820 --> 00:25:35.760
run on these new
hardware platforms

00:25:35.760 --> 00:25:39.600
all the way across the
range of devices we support.

00:25:39.600 --> 00:25:41.880
But I now want to talk
about how you actually

00:25:41.880 --> 00:25:44.520
write your algorithms
so that they can

00:25:44.520 --> 00:25:46.390
run across all these platforms.

00:25:46.390 --> 00:25:50.120
And for that, we
have Estimators.

00:25:50.120 --> 00:25:51.530
Now this code
snippet I'm showing

00:25:51.530 --> 00:25:55.020
you is copy pasted directly
from our source repository

00:25:55.020 --> 00:25:57.900
that it runs on CPUs and use.

00:25:57.900 --> 00:26:02.660
And I'm going to show you how
we can modify it to run on TPUs.

00:26:02.660 --> 00:26:05.360
When you use Estimators,
you define your machine

00:26:05.360 --> 00:26:07.970
learning algorithm
using a model function.

00:26:07.970 --> 00:26:10.610
And this is the
model definition for

00:26:10.610 --> 00:26:12.980
a simple convolutional
neural network

00:26:12.980 --> 00:26:14.910
to recognize
hand-written digits.

00:26:14.910 --> 00:26:19.220
This is the MNIST data set, for
those of you who are familiar.

00:26:19.220 --> 00:26:22.250
I want to draw your attention
to the final three lines.

00:26:22.250 --> 00:26:24.440
And in order to see them
a little bit better,

00:26:24.440 --> 00:26:27.440
we've made them a
little bit bigger.

00:26:27.440 --> 00:26:30.020
Normally when you train your
machine learning algorithm,

00:26:30.020 --> 00:26:33.110
you use a gradient descent
optimizer or similar

00:26:33.110 --> 00:26:34.490
to optimize the weights.

00:26:34.490 --> 00:26:37.730
This is the training step
of your machine learning

00:26:37.730 --> 00:26:38.900
algorithm.

00:26:38.900 --> 00:26:41.180
In order to get
it to run on TPUs,

00:26:41.180 --> 00:26:44.120
you just wrap your
gradient descent optimizer

00:26:44.120 --> 00:26:46.990
with a TPU
cross-shard optimizer.

00:26:46.990 --> 00:26:50.840
TPUs have a number of
parallel compute elements

00:26:50.840 --> 00:26:52.640
that all work together.

00:26:52.640 --> 00:26:55.280
And in order to aggregate
the learned weights,

00:26:55.280 --> 00:26:58.610
you need to use this
cross-shard optimizer to relearn

00:26:58.610 --> 00:27:00.590
the entire algorithm.

00:27:00.590 --> 00:27:02.990
That is the only
change required when

00:27:02.990 --> 00:27:08.210
you're using Estimators to
run on CPUs, GPUs, and TPUs

00:27:08.210 --> 00:27:10.939
for this example.

00:27:10.939 --> 00:27:12.230
Now how do you actually run it?

00:27:12.230 --> 00:27:13.896
You've got to define
your main function.

00:27:13.896 --> 00:27:17.164
And using Estimators,
it's very simple.

00:27:17.164 --> 00:27:18.830
Normally, you just
define your Estimator

00:27:18.830 --> 00:27:21.530
based on your model
function, and you call Train.

00:27:21.530 --> 00:27:25.370
And to get us to work on TPUs,
you just use a TPU Estimator

00:27:25.370 --> 00:27:28.320
and use a TPU run config.

00:27:28.320 --> 00:27:29.360
That's it.

00:27:29.360 --> 00:27:31.700
That is all the changes
required for this model

00:27:31.700 --> 00:27:36.140
to work on CPUs, GPUs, and TPUs.

00:27:36.140 --> 00:27:39.020
Some more complicated models
may require additional code

00:27:39.020 --> 00:27:39.800
changes.

00:27:39.800 --> 00:27:42.080
But the TensorFlow
team is constantly

00:27:42.080 --> 00:27:45.680
working to reduce the amount
of code changes required

00:27:45.680 --> 00:27:48.200
as you go all the way
to production on all

00:27:48.200 --> 00:27:49.730
these different platforms.

00:27:49.730 --> 00:27:52.610
If you'd like to hear more
about these high-level APIs,

00:27:52.610 --> 00:27:54.320
I encourage you to
check out the talk

00:27:54.320 --> 00:27:57.719
in the next hour
about TensorFlow

00:27:57.719 --> 00:27:58.760
with the high-level APIs.

00:28:02.070 --> 00:28:04.445
So we've talked about how to
define your machine learning

00:28:04.445 --> 00:28:06.236
algorithms, and we've
talked about how they

00:28:06.236 --> 00:28:07.590
execute on hardware devices.

00:28:07.590 --> 00:28:12.660
But we're missing one key thing.

00:28:12.660 --> 00:28:14.910
The machine learning
triad that has resulted

00:28:14.910 --> 00:28:17.400
in this explosion
of progress has been

00:28:17.400 --> 00:28:19.800
fueled by three components--

00:28:19.800 --> 00:28:22.530
high-performance
computation, advances

00:28:22.530 --> 00:28:26.880
in algorithms, and
also, input data.

00:28:26.880 --> 00:28:30.450
Without input data, all the
fancy acceleration you want

00:28:30.450 --> 00:28:34.650
is not going to generate any
more than random numbers.

00:28:34.650 --> 00:28:37.650
Clouds are designed to integrate
with the rest of Google Cloud

00:28:37.650 --> 00:28:42.630
Platform to make it easy to load
data in a high-performance way.

00:28:42.630 --> 00:28:45.420
You can load data from
your persistent disk

00:28:45.420 --> 00:28:50.130
or your ephemeral local SSD
on Google Compute Engine.

00:28:50.130 --> 00:28:52.260
But you can also
stream data in directly

00:28:52.260 --> 00:28:56.190
from Google Cloud Storage
into your cloud TPUs.

00:28:56.190 --> 00:28:58.680
And for those of you doing
advanced reinforcement learning

00:28:58.680 --> 00:29:01.020
algorithms or other
sorts of things,

00:29:01.020 --> 00:29:03.750
you can run simulators
on Google Compute Engine

00:29:03.750 --> 00:29:07.466
and integrate that with
your machine learning model

00:29:07.466 --> 00:29:08.590
running on your cloud TPUs.

00:29:11.960 --> 00:29:13.974
But as it turns
out, we at Google,

00:29:13.974 --> 00:29:16.390
we've been deploying these
high-performance accelerators--

00:29:16.390 --> 00:29:19.320
CPUs and GPUs-- for a while.

00:29:19.320 --> 00:29:21.030
And we've found
that when you really

00:29:21.030 --> 00:29:24.150
put them into production,
you find new bottlenecks.

00:29:24.150 --> 00:29:27.730
And I think this is best
explained with an example.

00:29:27.730 --> 00:29:30.900
If you've optimized
your software platform,

00:29:30.900 --> 00:29:33.120
you'll often use
pipelining techniques

00:29:33.120 --> 00:29:34.840
to overlap computation.

00:29:34.840 --> 00:29:37.860
And that's what we've done here.

00:29:37.860 --> 00:29:40.470
For this image model, we
run some input processing

00:29:40.470 --> 00:29:43.200
on the CPU for the
next training step

00:29:43.200 --> 00:29:45.810
while our accelerator
is running the model

00:29:45.810 --> 00:29:49.510
and learning on the
current training step.

00:29:49.510 --> 00:29:52.290
But if you all the sudden just
take this and port this over

00:29:52.290 --> 00:29:54.930
to an accelerator
that's 10 times faster,

00:29:54.930 --> 00:29:56.610
your training step
time is not going

00:29:56.610 --> 00:30:00.990
to decrease anywhere
close to what you'd hope.

00:30:00.990 --> 00:30:05.370
To help you feed data in in a
flexible but high-performance

00:30:05.370 --> 00:30:08.040
manner, we're
introducing a new API

00:30:08.040 --> 00:30:11.530
in TensorFlow 1.2
called Datasets.

00:30:11.530 --> 00:30:14.160
TensorFlow 1.2, the
first release candidate,

00:30:14.160 --> 00:30:16.890
is coming out as we speak.

00:30:16.890 --> 00:30:19.620
And we encourage you all
to try out these new APIs,

00:30:19.620 --> 00:30:20.460
kick the tires.

00:30:20.460 --> 00:30:22.540
Although they're still
a work in progress,

00:30:22.540 --> 00:30:23.880
we think they're very exciting.

00:30:23.880 --> 00:30:25.880
And I'd like to show you
a little bit about what

00:30:25.880 --> 00:30:28.950
this Dataset API looks like.

00:30:28.950 --> 00:30:31.800
Here is a code snippet
where we load a set of data

00:30:31.800 --> 00:30:35.040
from a TF record file on disk.

00:30:35.040 --> 00:30:39.030
We then repeat that data set
as we train our algorithm.

00:30:39.030 --> 00:30:42.690
We perform some parsing
and distortions to this,

00:30:42.690 --> 00:30:44.310
using the Parser function.

00:30:44.310 --> 00:30:48.660
But we can do this in parallel
using the Parallel Num Threads,

00:30:48.660 --> 00:30:50.320
so we do a parallel map.

00:30:50.320 --> 00:30:53.160
Finally, we batch this
up into training bunches

00:30:53.160 --> 00:30:55.380
that we then feed
to our accelerator.

00:30:55.380 --> 00:30:59.130
This new Datasets API borrows
from functional programming

00:30:59.130 --> 00:31:01.290
and other advanced
concepts to make

00:31:01.290 --> 00:31:04.950
it easy to define
high-performance input

00:31:04.950 --> 00:31:09.510
pipelines that run well.

00:31:09.510 --> 00:31:13.780
We've talked about now XLA,
Estimators, and Datasets.

00:31:13.780 --> 00:31:15.900
And these are just some
of the new changes that

00:31:15.900 --> 00:31:18.600
are coming in
TensorFlow 1.2 that help

00:31:18.600 --> 00:31:22.110
make these high-performance
accelerators, be they CPUs

00:31:22.110 --> 00:31:25.580
or GPUs or TPUs, really sing.

00:31:25.580 --> 00:31:28.920
And on top of this
base of awesome compute

00:31:28.920 --> 00:31:32.940
and infrastructure, we see
some really phenomenal learning

00:31:32.940 --> 00:31:34.240
and research work.

00:31:34.240 --> 00:31:37.590
And for that, I'd like to
turn it back over to Zak Stone

00:31:37.590 --> 00:31:40.350
to talk about research
on the frontier.

00:31:40.350 --> 00:31:42.720
ZAK STONE: Thanks, Brennan.

00:31:42.720 --> 00:31:45.540
We've witnessed extraordinary
breakthroughs in machine

00:31:45.540 --> 00:31:48.250
learning research over
the past several years.

00:31:48.250 --> 00:31:50.689
And while there are many
types of machine learning,

00:31:50.689 --> 00:31:52.230
many of these
breakthroughs have been

00:31:52.230 --> 00:31:54.330
concentrated at
the edges of what's

00:31:54.330 --> 00:31:56.620
computationally possible.

00:31:56.620 --> 00:32:01.530
And we believe that much
larger amounts of computation

00:32:01.530 --> 00:32:03.450
are going to unlock
new discoveries

00:32:03.450 --> 00:32:07.460
and new breakthroughs with
larger and more complex models

00:32:07.460 --> 00:32:10.920
than researchers can practically
experiment with today.

00:32:10.920 --> 00:32:12.850
In the photo behind
me, Wolff's daughter

00:32:12.850 --> 00:32:15.240
represents the machine
learning research community.

00:32:15.240 --> 00:32:17.700
The mountain represents
the breakthroughs yet

00:32:17.700 --> 00:32:18.540
to be discovered.

00:32:18.540 --> 00:32:21.060
And that gate is the
computational limits that

00:32:21.060 --> 00:32:23.190
are holding researchers back.

00:32:23.190 --> 00:32:26.190
Our goal is to break
open that gate and help

00:32:26.190 --> 00:32:30.390
researchers climb that mountain
to unlock new discoveries.

00:32:30.390 --> 00:32:33.000
That's why I'm thrilled that
Sundar announced the TensorFlow

00:32:33.000 --> 00:32:35.850
Research Cloud
yesterday in order

00:32:35.850 --> 00:32:39.840
to support open machine
learning research worldwide

00:32:39.840 --> 00:32:42.150
and help researchers, who
currently don't have access

00:32:42.150 --> 00:32:44.430
to enough computation,
experiment with much

00:32:44.430 --> 00:32:48.150
larger and more complex models.

00:32:48.150 --> 00:32:52.230
We've dedicated 1,000
of these new cloud TPUs

00:32:52.230 --> 00:32:55.440
to accelerate open
machine learning research.

00:32:55.440 --> 00:32:57.810
We're going to set up
an application process

00:32:57.810 --> 00:32:59.640
to let people from
all backgrounds

00:32:59.640 --> 00:33:01.740
and all fields of
expertise propose

00:33:01.740 --> 00:33:04.980
projects to take advantage
of this extraordinary amount

00:33:04.980 --> 00:33:06.120
of computing power.

00:33:06.120 --> 00:33:10.890
This is a 180 petaflops
of compute all together.

00:33:10.890 --> 00:33:13.410
And we can't wait to
see what new discoveries

00:33:13.410 --> 00:33:16.080
people are going to do
with this level of compute.

00:33:16.080 --> 00:33:17.320
What might those be?

00:33:17.320 --> 00:33:20.190
Well, we're looking
forward to finding out.

00:33:20.190 --> 00:33:22.890
But just to highlight one
example that Sundar also

00:33:22.890 --> 00:33:26.460
mentioned yesterday, I'd
like to mention AutoML,

00:33:26.460 --> 00:33:29.250
which is a family of techniques
for using machine learning

00:33:29.250 --> 00:33:33.240
models to develop new
machine learning models.

00:33:33.240 --> 00:33:34.920
I won't go through
the details here.

00:33:34.920 --> 00:33:38.730
But these are just two examples
from a linguistic problem

00:33:38.730 --> 00:33:41.550
and an image recognition
problem where

00:33:41.550 --> 00:33:43.410
we've used the
machine learning model

00:33:43.410 --> 00:33:47.040
to generate the architectures
that you see here

00:33:47.040 --> 00:33:48.210
on the screen.

00:33:48.210 --> 00:33:51.180
And the interesting thing about
it is that both of these models

00:33:51.180 --> 00:33:53.520
look kind of
complex and organic.

00:33:53.520 --> 00:33:55.470
They don't necessarily
look like the thing

00:33:55.470 --> 00:33:58.470
that a mathematician might
have come up with by hand.

00:33:58.470 --> 00:34:01.560
Now this kind of experimentation
requires immense amounts

00:34:01.560 --> 00:34:02.430
of compute.

00:34:02.430 --> 00:34:06.330
But fortunately, with this
step function to cloud TPUs,

00:34:06.330 --> 00:34:08.790
and eventually to
TPU pods, we want

00:34:08.790 --> 00:34:11.489
to shift the whole research
mindset from scarcity

00:34:11.489 --> 00:34:13.590
to abundance so you
can start contemplating

00:34:13.590 --> 00:34:15.750
these kinds of techniques
to search for new model

00:34:15.750 --> 00:34:20.560
architectures and open
up new applications.

00:34:20.560 --> 00:34:24.010
So just to be clear, we
have one underlying product

00:34:24.010 --> 00:34:27.540
in Google Cloud, which is these
new cloud TPUs, on the Google

00:34:27.540 --> 00:34:28.889
Compute Engine.

00:34:28.889 --> 00:34:33.030
But we're offering the community
access to them in two ways.

00:34:33.030 --> 00:34:36.989
For general use, we're setting
up a Cloud Alpha program that

00:34:36.989 --> 00:34:42.300
lets businesses, start-ups,
individuals, students, people,

00:34:42.300 --> 00:34:42.989
artists--

00:34:42.989 --> 00:34:46.409
anyone who's interested in this
frontier of machine learning--

00:34:46.409 --> 00:34:49.650
to sign up and try and get early
access to these limited numbers

00:34:49.650 --> 00:34:52.350
of cloud TPUs to figure
out how to adapt them

00:34:52.350 --> 00:34:54.270
for their applications.

00:34:54.270 --> 00:34:56.100
But for those of
you out there who

00:34:56.100 --> 00:34:58.830
are doing cutting edge
machine learning research,

00:34:58.830 --> 00:35:00.960
who are extending
this frontier, we

00:35:00.960 --> 00:35:03.360
set up the TensorFlow
Research Cloud.

00:35:03.360 --> 00:35:06.210
And we're going to open up
this application process

00:35:06.210 --> 00:35:09.410
to invite you in to take
advantage of these 1,000 cloud

00:35:09.410 --> 00:35:13.680
TPUs to do things that
just aren't feasible today.

00:35:13.680 --> 00:35:15.930
If you're interested in
either of these programs,

00:35:15.930 --> 00:35:20.490
we encourage you to sign
up now at G.go/TPUsignup.

00:35:20.490 --> 00:35:22.890
Optionally, tell us more
about your compute needs.

00:35:22.890 --> 00:35:25.230
And we'll notify you as
soon as more information is

00:35:25.230 --> 00:35:29.790
available about cloud TPUs and
the TensorFlow Research Cloud.

00:35:29.790 --> 00:35:31.980
Now before I go, I'd
like to encourage

00:35:31.980 --> 00:35:34.950
you to either stick around
for the next session, which

00:35:34.950 --> 00:35:36.810
will cover TensorFlow
for non-experts

00:35:36.810 --> 00:35:39.030
and tell you more about
the high-level APIs

00:35:39.030 --> 00:35:42.700
that we described earlier, or
join us in the Office Hours

00:35:42.700 --> 00:35:43.875
section for office hours.

00:35:43.875 --> 00:35:46.500
And if you're worried, how do I
choose between the next session

00:35:46.500 --> 00:35:47.940
and Office Hours, don't worry.

00:35:47.940 --> 00:35:51.580
We've added a next Office
Hour after the next session

00:35:51.580 --> 00:35:53.310
so you can do both.

00:35:53.310 --> 00:35:54.750
Thanks again for your attention.

00:35:54.750 --> 00:35:57.330
We're really excited about
TensorFlow and cloud TPUs.

00:35:57.330 --> 00:35:59.705
And we look forward to speaking
with you in Office Hours.

00:35:59.705 --> 00:36:00.510
[APPLAUSE]

00:36:00.510 --> 00:36:03.560
[MUSIC PLAYING]

