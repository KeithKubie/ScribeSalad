WEBVTT
Kind: captions
Language: en

00:00:00.220 --> 00:00:03.390
So last time we were talking
about standard normal, right?

00:00:03.390 --> 00:00:04.830
Normal zero one.

00:00:04.830 --> 00:00:08.948
So just a few quick facts
that we proved last time.

00:00:10.588 --> 00:00:15.410
So our notation is,
traditionally it's often called Z, but

00:00:15.410 --> 00:00:18.864
I'm not saying Z has
to be standard normal.

00:00:18.864 --> 00:00:23.080
Or you have to call standard normal Z,
just we often use letter Z for that.

00:00:23.080 --> 00:00:27.550
If Z is standard normal, then first
of all, we found its PDF, right?

00:00:27.550 --> 00:00:31.053
We figured out the normalizing
constant,it's CDF.

00:00:31.053 --> 00:00:34.460
It's CDF you can't actually
do in closed form,

00:00:34.460 --> 00:00:36.500
so therefore it's just called capital Phi.

00:00:36.500 --> 00:00:39.630
That's just the standard notation for
the CDF.

00:00:39.630 --> 00:00:42.710
We computed the mean and
the variance last time.

00:00:42.710 --> 00:00:47.530
Remember, the mean E of Z = 0.

00:00:47.530 --> 00:00:49.700
That's just immediate by symmetry.

00:00:51.720 --> 00:00:53.355
Then we also did the variance.

00:00:56.822 --> 00:01:02.651
Variance equals in this case
the variance is E of Z squared equals 1.

00:01:02.651 --> 00:01:06.070
Cuz variance of E of Z squared
minus E of Z squared the other way,

00:01:06.070 --> 00:01:08.620
but that's 0, so that's one.

00:01:08.620 --> 00:01:12.730
That we computed last time
using integration by parts, so

00:01:12.730 --> 00:01:13.800
we did that last time.

00:01:13.800 --> 00:01:20.570
And if we wanted, this is by the way it's
called the first moment, second moment.

00:01:20.570 --> 00:01:23.965
If we wanted E of Z cubed,
this we didn't talk about last time.

00:01:27.062 --> 00:01:28.580
That's gonna be 0 again.

00:01:29.610 --> 00:01:33.940
Because, I'll just write
down what it would be.

00:01:33.940 --> 00:01:39.770
By LOTUS, we would have the integral minus
infinity, infinity 1 over root 2 pi,

00:01:39.770 --> 00:01:44.640
E to the minus Z squared over 2 dz.

00:01:44.640 --> 00:01:47.790
This integrates 1,
that would be just integrating the PDF.

00:01:47.790 --> 00:01:51.580
And LOTUS says if we want E of Z cubed,
we just stick in a Z cubed here.

00:01:53.000 --> 00:01:55.610
If we just wanted to do E of Z,
we'd put Z, if we want E of Z cubed,

00:01:55.610 --> 00:01:58.270
we'd put Z cubed, that's LOTUS.

00:01:58.270 --> 00:02:02.580
But this is just equal to 0,
because this is an odd function, again.

00:02:03.890 --> 00:02:08.410
So we talked about that in this case, but
the same argument would apply here for

00:02:08.410 --> 00:02:09.080
Z cubed.

00:02:10.710 --> 00:02:17.557
Similarly, for any odd power here, 5,
7, and so on, we'll immediately get 0.

00:02:19.978 --> 00:02:21.710
So this is called the third moment.

00:02:22.930 --> 00:02:26.490
At some point later in the semester we
can talk about where does the word moment

00:02:26.490 --> 00:02:27.150
come from.

00:02:27.150 --> 00:02:31.173
But that's just that's just terminology
for that that's called the third moment.

00:02:31.173 --> 00:02:35.095
E of Z cubed that would be called the
second moment first moment then and so on.

00:02:35.095 --> 00:02:37.150
Okay so in other words,

00:02:37.150 --> 00:02:41.321
by symmetry we already know that all
the odd moments of the normal are 0.

00:02:41.321 --> 00:02:44.230
The even moments well,

00:02:44.230 --> 00:02:47.863
we have this the second one if
we wanted E of Z to the fourth.

00:02:47.863 --> 00:02:52.470
Well it's going to be the integral except
put Z to the fourth instead of Z cubed,

00:02:52.470 --> 00:02:55.850
then that's not such an easy
integral anymore,okay?

00:02:55.850 --> 00:02:59.150
And it's not an integral that you need
to know how to do it at this point,

00:02:59.150 --> 00:03:03.180
we'll probably come back to how
to do things like that later,

00:03:04.230 --> 00:03:05.780
not before the midterm though.

00:03:05.780 --> 00:03:08.920
But at least you should immediately
know LOTUS that you could

00:03:08.920 --> 00:03:11.840
write down the integral for
E of Z to the fourth, it's just that

00:03:11.840 --> 00:03:14.580
happens to be an integral that I don't
expect that anyone could do right now.

00:03:14.580 --> 00:03:17.850
But at least you could write
down the integral, okay?

00:03:17.850 --> 00:03:24.920
Odd moments though, you just immediately
get 0 by symmetry, no integrals needed.

00:03:24.920 --> 00:03:30.340
Okay so I was talking about symmetry,
let me just mention symmetry one other way

00:03:30.340 --> 00:03:35.160
which is that minus Z is
also standard normal.

00:03:35.160 --> 00:03:37.800
And that's just another way to
express the symmetry of it.

00:03:40.360 --> 00:03:46.630
That is, the PDF is this bell
curve that's symmetrical about 0.

00:03:46.630 --> 00:03:51.120
So if you flip,
this flips between plus and minus, right.

00:03:51.120 --> 00:03:55.690
Just flipping the sign,
that changes the random variable,

00:03:55.690 --> 00:03:58.830
it makes a positive into negative,
makes negative into positive.

00:03:58.830 --> 00:04:02.580
But it does not change the distribution,
that's what the symmetry says.

00:04:02.580 --> 00:04:06.720
So you can either just see this by
symmetry or you could compute the PDF of

00:04:06.720 --> 00:04:11.420
this by first find the CDF, then find
the PDF, and you'll see that that's true.

00:04:11.420 --> 00:04:14.779
That's a very useful fact.,
it's always useful looking for symmetries.

00:04:16.030 --> 00:04:19.640
Okay, so this is just stuff
about the standard normal.

00:04:21.050 --> 00:04:25.650
But now we wanna introduce
what happens with normal

00:04:25.650 --> 00:04:27.350
where this is not necessarily 0, 1, okay?

00:04:27.350 --> 00:04:29.220
So this is the general normal.

00:04:30.470 --> 00:04:37.960
We say that X,
if we let X equal mu plus sigma Z

00:04:40.170 --> 00:04:45.030
where mu is any real number and

00:04:45.030 --> 00:04:47.620
we would call that the mean cuz
that's going to be the mean.

00:04:47.620 --> 00:04:49.378
But we would also call that the location.

00:04:53.202 --> 00:04:57.450
Because we're just adding a constant,
it means a shift in location.

00:04:57.450 --> 00:05:01.600
We're not changing what the density
looks like by adding mu,

00:05:01.600 --> 00:05:03.290
we're just moving it around left and
right.

00:05:04.520 --> 00:05:08.620
And sigma is any positive number,
mu could be negative,

00:05:08.620 --> 00:05:12.780
sigma has to be positive, and
that's called the standard deviation.

00:05:12.780 --> 00:05:16.130
Remember standard deviation we defined
as the square root of variance.

00:05:16.130 --> 00:05:19.440
So sigma is the standard deviation but
we also call that the scale

00:05:20.670 --> 00:05:24.130
because we're just rescaling everything
by multiplying by a constant.

00:05:24.130 --> 00:05:28.910
So that's gonna effect if
you draw one of the density,

00:05:28.910 --> 00:05:33.400
it's gonna effect how wide or
how narrow that curve is.

00:05:33.400 --> 00:05:37.720
It still has to integrate to 1, so you
can't just make it really big and wide and

00:05:37.720 --> 00:05:39.490
suddenly you made the area blow up.

00:05:39.490 --> 00:05:42.910
You also have to make sure that you
multiply by a normalizing constant so

00:05:42.910 --> 00:05:46.780
it still integrates to 1, but you can
still make it more wide or more narrow.

00:05:48.130 --> 00:05:53.864
Okay then we say Then we say X is normal

00:05:53.864 --> 00:05:58.870
with mean mu and variance sigma squared.

00:05:58.870 --> 00:06:01.130
So those are the two parameters.

00:06:02.310 --> 00:06:07.720
So the reason most books would do
this a little bit differently and

00:06:07.720 --> 00:06:11.290
start by writing down the PDF of this.

00:06:11.290 --> 00:06:14.710
But this is a more useful and
more insightful way to think about it,

00:06:14.710 --> 00:06:19.230
where we're saying there's just one
fundamental basic normal distribution.

00:06:19.230 --> 00:06:20.970
That's what we call the standard normal.

00:06:20.970 --> 00:06:24.100
Once we understand the standard normal
we can easily get any other normal

00:06:24.100 --> 00:06:27.860
distribution we want just by multiplying
by a constant adding a constant.

00:06:27.860 --> 00:06:31.210
So it's reducing everything back
down to the standard normal.

00:06:31.210 --> 00:06:34.500
That's really useful to always
keep that in mind instead of just

00:06:34.500 --> 00:06:37.480
looking at ugly formulas, okay?

00:06:37.480 --> 00:06:41.740
So let's actually check that this
has the desired mean and variance.

00:06:42.950 --> 00:06:48.110
So obviously the expected value of
X just by linearity with mu plus

00:06:48.110 --> 00:06:53.600
sigma expected value of Z is 0, so
that's just mu, just immediate from this.

00:06:54.610 --> 00:06:59.257
For the variance, Then we need to talk

00:06:59.257 --> 00:07:02.770
a little bit more about what happens,
what are the properties of variance.

00:07:02.770 --> 00:07:05.090
So I'll come back to this in a minute.

00:07:05.090 --> 00:07:08.020
First, let's just talk a little bit
more in general about variance.

00:07:08.020 --> 00:07:11.020
We did a quick introduction
to variance before but

00:07:11.020 --> 00:07:12.210
we should go a little bit further.

00:07:13.750 --> 00:07:17.430
So remember,
there's two ways to write variance.

00:07:18.550 --> 00:07:23.070
The definition is to subtract
off the mean, square it,

00:07:24.190 --> 00:07:28.510
the average distance
squared of X from its mean.

00:07:28.510 --> 00:07:32.812
But we also showed that can also be
written as E(X) squared, this way,

00:07:32.812 --> 00:07:36.980
minus E(X) squared the other way, okay?

00:07:36.980 --> 00:07:43.427
Now in particular, if we had the variance
of X plus a constant, intuitively,

00:07:43.427 --> 00:07:50.230
if we just add a constant we're not
changing how variable X is, right?

00:07:50.230 --> 00:07:53.980
So intuitively that should be
the same as the variance of X.

00:07:53.980 --> 00:07:58.210
And you can see that immediately from this
first formula because You replace by x by

00:07:58.210 --> 00:08:03.300
x + c, and the mean also shifts by c by
linearity, you get the exact same thing.

00:08:03.300 --> 00:08:09.320
So that's immediate from this, so adding
a constant has no effect on the variance.

00:08:09.320 --> 00:08:15.077
Now if we multiply by a constant,
then from either of these formulas,

00:08:15.077 --> 00:08:18.800
just imagine sticking in a c here and
a c here.

00:08:18.800 --> 00:08:23.600
But the c comes out because of linearity
again, but it's squared, then.

00:08:23.600 --> 00:08:27.410
So the variance of a c times x is
c squared times the variance of x.

00:08:28.530 --> 00:08:30.720
And a common mistake is to
forget the square here, but

00:08:30.720 --> 00:08:36.182
that really messes things up, so
variance is coming out with the square.

00:08:36.182 --> 00:08:41.990
And an easy way to see that is,
if c is negative, this is still valid.

00:08:41.990 --> 00:08:45.755
But if you forgot to write the square
here, you would get a negative variance.

00:08:45.755 --> 00:08:49.178
If you ever get a negative variance,
that's very, very bad,

00:08:49.178 --> 00:08:50.870
variance cannot be negative.

00:08:51.920 --> 00:08:55.550
So anytime you compute a variance,
the first thing you should check is,

00:08:56.560 --> 00:09:01.170
is the thing I wrote down
at least non-negative?

00:09:01.170 --> 00:09:05.600
And the only case where it could
be 0 is if it's a constant,

00:09:06.960 --> 00:09:10.470
so it's always greater than or equal to 0.

00:09:10.470 --> 00:09:13.425
And variance of X = 0 if and

00:09:13.425 --> 00:09:18.189
only if X is a constant
with probability 1.

00:09:20.370 --> 00:09:23.921
P of X = a = 1 for some a, that is,

00:09:23.921 --> 00:09:29.350
with probability 0,
something bad could happen.

00:09:29.350 --> 00:09:33.080
But with probability 1,
it always equals this constant a.

00:09:33.080 --> 00:09:36.010
So that would have variance
0 because the stuff

00:09:36.010 --> 00:09:41.140
with probability 0 doesn't affect
anything, so essentially it's a constant.

00:09:41.140 --> 00:09:43.880
If it's not a constant,
the variance will be strictly positive.

00:09:45.710 --> 00:09:50.172
Okay, so that's the variance of a constant
times x, and then just one other factor

00:09:50.172 --> 00:09:53.770
about, we'll do a lot more with
variance like after the midterm.

00:09:53.770 --> 00:09:58.711
But only one other thing to point out for
now is that variance,

00:09:58.711 --> 00:10:02.811
unlike expected values,
variance is not linear.

00:10:02.811 --> 00:10:08.820
So variance of x + y is not equal
to variance x plus variance of y.

00:10:08.820 --> 00:10:14.430
In general, it may be equal, but
it's not necessarily equal, so

00:10:14.430 --> 00:10:16.840
actually, it violates both
of the linearity properties.

00:10:16.840 --> 00:10:20.326
If it were linear, we would want
constants to come out as themselves, and

00:10:20.326 --> 00:10:21.629
here it comes out squared.

00:10:21.629 --> 00:10:24.990
And we can't say the variance of
the sum is the sum of the variances.

00:10:24.990 --> 00:10:28.380
It is equal,
we're not gonna show this until later,

00:10:28.380 --> 00:10:31.320
we'll show this at sometime
after the midterm.

00:10:31.320 --> 00:10:35.220
It is equal if x and
y are independent, but remember,

00:10:35.220 --> 00:10:40.540
linearity holds regardless of whether the
random variables are independent or not.

00:10:40.540 --> 00:10:44.890
So if they're independent, it will
be equal, we'll show that later, but

00:10:44.890 --> 00:10:46.700
in general, they're not equal.

00:10:46.700 --> 00:10:52.975
And one quick example of that would be,
what if we look at the variance of x + x?

00:10:52.975 --> 00:10:56.630
All right, that's an extreme case
of dependence, that's when x,

00:10:56.630 --> 00:10:59.410
it's actually the same thing, right?

00:10:59.410 --> 00:11:02.780
Well, the variance of x +
x Is the variance of 2x,

00:11:02.780 --> 00:11:05.610
which we just said is 4
times the variance of x.

00:11:06.670 --> 00:11:10.285
So if this were true, if this were equal,
we would get 2 times the variance of x.

00:11:10.285 --> 00:11:14.256
And this says we get 4 times the
variability, not 2 times the variability,

00:11:14.256 --> 00:11:16.469
but that's just a simple example of that.

00:11:16.469 --> 00:11:21.100
But that's also a common mistake
that I've seen before when students

00:11:21.100 --> 00:11:26.045
are dealing with, in the past I've
asked questions either on homeworks or

00:11:26.045 --> 00:11:29.290
exams where we have something like 2x.

00:11:29.290 --> 00:11:33.749
And a lot of students took
the approach of, well, 2x is x + x.

00:11:33.749 --> 00:11:37.476
Of course, that's valid,
but then at that point,

00:11:37.476 --> 00:11:42.409
they made the mistake of replacing
x + x by, let's say, x1 + x2.

00:11:42.409 --> 00:11:46.910
Where those are IID,
with the same distribution as x.

00:11:46.910 --> 00:11:52.150
That's completely wrong because
x is not IID with itself.

00:11:52.150 --> 00:11:56.180
It's extremely dependent and then somehow
replacing it by independent copies,

00:11:56.180 --> 00:11:57.576
then it doesn't work.

00:11:57.576 --> 00:11:59.860
So I'm telling you to be careful of this,

00:11:59.860 --> 00:12:02.760
just keeping track of
dependents versus independents.

00:12:02.760 --> 00:12:07.100
Here they're extremely dependent, and
so that's why we got this 4 here.

00:12:07.100 --> 00:12:11.408
And I think, intuitively,
that should make some sense, right?

00:12:11.408 --> 00:12:13.750
If this was like x1 and x2 and

00:12:13.750 --> 00:12:17.180
they're independent,
then the variabilities just add.

00:12:17.180 --> 00:12:20.140
Here, they're exactly the same, so

00:12:20.140 --> 00:12:25.680
that magnifies the variability, okay.

00:12:25.680 --> 00:12:30.400
So that's a few quick
notes about variance, so

00:12:30.400 --> 00:12:33.980
now coming back to this for
the normal case.

00:12:33.980 --> 00:12:39.080
We just saw that adding mu does nothing
to the variance, multiplying by sigma.

00:12:39.080 --> 00:12:43.501
Then it comes out as sigma squared, that's
sigma squared times the variance of z.

00:12:43.501 --> 00:12:48.891
Well, that's just sigma squared,
okay, so that confirms that

00:12:48.891 --> 00:12:54.120
when we write this, this is the mean and
this is the variance.

00:12:54.120 --> 00:12:58.070
So those are the two parameters
of the normal distribution.

00:12:59.120 --> 00:13:00.730
Ane whenever you have
a normal distribution,

00:13:00.730 --> 00:13:04.650
you should always think about
reducing it back to standard normal.

00:13:05.660 --> 00:13:09.430
So we could also go the other way around,
and I don't need much space for this.

00:13:09.430 --> 00:13:13.208
Because this is just, I'm just
gonna solve this equation for z, so

00:13:13.208 --> 00:13:15.343
if we do it the other way, solve for z.

00:13:15.343 --> 00:13:20.290
z equals x minus mu over sigma,
very easy algebra,

00:13:20.290 --> 00:13:23.752
that's called standardization.

00:13:30.837 --> 00:13:33.999
So standardization says,
I'm just going the other direction here.

00:13:33.999 --> 00:13:36.700
I was starting with the standard normal,
and

00:13:36.700 --> 00:13:39.780
we can construct a general
normal this way.

00:13:39.780 --> 00:13:42.160
Now what if we wanted to go the other way,
we started with x,

00:13:42.160 --> 00:13:44.220
which is normal mu sigma squared.

00:13:44.220 --> 00:13:46.920
Subtract the mean divided
by the standard deviation,

00:13:46.920 --> 00:13:49.600
and that will always give
us a standard normal.

00:13:49.600 --> 00:13:53.451
So that process is called standardization,
it's very, very useful, it's simple,

00:13:53.451 --> 00:13:56.431
right, just subtract the mean
divided by the standard deviation.

00:13:56.431 --> 00:13:59.073
And yet sometimes students
get confused about it, or

00:13:59.073 --> 00:14:02.712
divide by the variance instead of
dividing by the standard deviation, or

00:14:02.712 --> 00:14:04.960
just don't think to do
it in the first place.

00:14:04.960 --> 00:14:09.300
So that's why I'm emphasizing that,
it's a simple but useful transformation.

00:14:10.800 --> 00:14:13.480
Okay, so
as a quick example of how we use that,

00:14:13.480 --> 00:14:21.020
let's derive the PDF
of the general normal.

00:14:22.190 --> 00:14:25.991
Find PDF of normal mu sigma squared, well,

00:14:25.991 --> 00:14:29.597
one way to find it is to
look it up in a book.

00:14:29.597 --> 00:14:33.588
But that doesn't tell you anything,
that's just like a formula in a book.

00:14:33.588 --> 00:14:38.009
So what we want to understand is,
assuming that we already know the PDF of

00:14:38.009 --> 00:14:42.510
the standard normal, how can we get
the PDF of the non-standard normal?

00:14:44.700 --> 00:14:50.514
In a way, that's easy,
without having to memorize stuff, okay,

00:14:50.514 --> 00:14:55.523
so let's call this x again,
so let's find the CDF first.

00:14:58.342 --> 00:15:01.860
So by definition,
this is just good practice with CDFs.

00:15:01.860 --> 00:15:07.084
Everyone here should make sure that
you're good at CDFs and PDFs and PMFs.

00:15:07.084 --> 00:15:11.320
And that just takes practice, so this
is just some simple practice with that.

00:15:11.320 --> 00:15:13.900
By definition, the CDF is this, and

00:15:13.900 --> 00:15:18.570
now I just told you that a useful trick is
to standardize, so let's standardize this.

00:15:19.600 --> 00:15:24.190
It's the same thing as saying X
minus mu over sigma is less than or

00:15:24.190 --> 00:15:27.770
equal to lowercase x minus mu over sigma,
right.

00:15:29.200 --> 00:15:32.424
Sigma is positive, so it doesn't
flip the inequality to do that, so

00:15:32.424 --> 00:15:33.379
I standardized it.

00:15:33.379 --> 00:15:37.101
The reason I standardized
it was because now,

00:15:37.101 --> 00:15:40.550
this thing on the left is standard normal.

00:15:40.550 --> 00:15:44.898
So by definition, this is just the CDF
of the standard normal evaluated here.

00:15:44.898 --> 00:15:49.743
So by definition,
we immediately know that's just

00:15:49.743 --> 00:15:54.929
capital phi of x minus mu over sigma,
now to get the PDF,

00:15:58.390 --> 00:16:00.970
To get the PDF we just have to
take the derivative of the CDF.

00:16:00.970 --> 00:16:06.623
That's just the chain rule right, because
this capital phi is the outer function and

00:16:06.623 --> 00:16:09.214
then we have this inner function here so

00:16:09.214 --> 00:16:13.020
it's just the chain rule
from basic calculus.

00:16:13.020 --> 00:16:17.510
It's the derivative of the outer
function evaluated here,

00:16:17.510 --> 00:16:19.720
times the derivative
of the inner function.

00:16:19.720 --> 00:16:23.152
The derivative of this inner function
is just 1 over sigma, right,

00:16:23.152 --> 00:16:24.460
cuz 1 over sigma times x.

00:16:24.460 --> 00:16:29.325
So we are gonna get a 1 over sigma, and
then we are gonna get the derivative of

00:16:29.325 --> 00:16:34.213
this the derivative of capital phi is
just the standard normal PDF, right?

00:16:34.213 --> 00:16:41.440
And it says evaluated here, so I'm just
gonna write down the standard normal PDF,

00:16:41.440 --> 00:16:45.580
and I'm gonna evaluate
it at x- mu over sigma.

00:16:49.050 --> 00:16:50.030
And that's it, we're done.

00:16:51.940 --> 00:16:53.340
So it should be a very,

00:16:53.340 --> 00:16:55.520
very quick calculation in
order to be able to do that.

00:16:59.330 --> 00:17:01.344
And as another quick example.

00:17:01.344 --> 00:17:05.852
Let's say over here in the corner,
we said what happens,

00:17:05.852 --> 00:17:09.290
z is standard normal, what happens to -z?

00:17:09.290 --> 00:17:12.150
Let's also ask the question
of what happens to -x?

00:17:14.260 --> 00:17:16.842
Well, you could work through
a similar calculation, but

00:17:16.842 --> 00:17:20.280
I think the neatest way to think of it is,
we're thinking of x as mu + sigma z.

00:17:20.280 --> 00:17:28.270
So -x- mu + sigma times -z.

00:17:28.270 --> 00:17:30.130
But -z is standard normal.

00:17:30.130 --> 00:17:33.140
So this is just of the form some

00:17:33.140 --> 00:17:37.070
location constant plus sigma
times the standard normal.

00:17:37.070 --> 00:17:41.440
So we immediately know that's
normal -mu sigma squared.

00:17:42.510 --> 00:17:46.980
Which again, makes sense intuitively,
because we put a minus sign, so

00:17:46.980 --> 00:17:48.760
we put a minus sign on the mean.

00:17:48.760 --> 00:17:50.720
We do not put a minus
sign on the variance,

00:17:50.720 --> 00:17:53.490
because variance can't be negative,
so the variants stay sigma squared.

00:17:55.700 --> 00:17:57.453
So you could do a calculation for
this, but

00:17:57.453 --> 00:18:00.492
this is just immediate from thinking
of x in terms of the standard normal.

00:18:00.492 --> 00:18:05.240
So this is the easiest way to do this,
okay?

00:18:05.240 --> 00:18:12.910
And a useful fact just to know, but
we'll prove this much later in the course.

00:18:12.910 --> 00:18:18.380
Later we'll show that if x1,

00:18:18.380 --> 00:18:23.630
let's say xj is normal mu j,

00:18:23.630 --> 00:18:27.110
sigma j squared, and they're independent.

00:18:29.690 --> 00:18:32.710
Let's say for j equals 1 to 2.

00:18:32.710 --> 00:18:37.681
Then, x1 + x2 is normal, mu1 + mu2

00:18:37.681 --> 00:18:42.372
sigma 1 squared + sigma 2 squared.

00:18:42.372 --> 00:18:47.840
So that's something we need to prove,
and we'll do that much later.

00:18:47.840 --> 00:18:51.490
The sum of independent normals is normal,
but the reason I'm mentioning it now is

00:18:51.490 --> 00:18:53.870
just let's think about what
happens to the mean and variance.

00:18:53.870 --> 00:18:59.240
By linearity, we know that the mean
would have to be mu1 + mu2.

00:18:59.240 --> 00:19:02.680
Variance, this is something
else we'll prove later.

00:19:02.680 --> 00:19:05.205
In the independent case we can
just add up the variances, so

00:19:05.205 --> 00:19:07.290
it's juts sigma1 squared + sigma2 squared.

00:19:08.920 --> 00:19:11.160
Now what if we looked at x1- x2?

00:19:12.670 --> 00:19:14.176
I'm mentioning this now,

00:19:14.176 --> 00:19:18.959
because I can't even count the number of
times when I've seen the mean is mu1- mu2.

00:19:18.959 --> 00:19:20.530
That's just linearity again.

00:19:20.530 --> 00:19:22.930
I can't even count the number
of times I've seen

00:19:22.930 --> 00:19:26.222
students write that the variance
is sigma1 squared- sigma2 squared.

00:19:27.250 --> 00:19:31.380
Well, first of all that could be negative,
so that doesn't make any sense.

00:19:31.380 --> 00:19:35.486
And secondly, any time you see
a subtraction you can really think of that

00:19:35.486 --> 00:19:38.123
as adding the negative of something,
right?

00:19:38.123 --> 00:19:40.714
So this is + of -x2.

00:19:40.714 --> 00:19:46.930
And -x2 still has variance sigma2 squared,
so the variance is still add.

00:19:51.374 --> 00:19:54.910
That's just a useful fact to keep in mind,
we'll prove it later.

00:19:54.910 --> 00:19:57.570
But I'm mainly talking about right
now just in terms of what happens to

00:19:57.570 --> 00:19:58.570
the mean and variance.

00:19:58.570 --> 00:20:01.020
Later we'll see why are they still normal.

00:20:01.020 --> 00:20:04.120
That's just one very useful
property of the normal.

00:20:04.120 --> 00:20:08.786
So let's just do a lot of things without
leaving the realm of normality, right?

00:20:08.786 --> 00:20:13.120
If you added two of them and then it
somehow becomes some completely different

00:20:13.120 --> 00:20:15.930
distribution, it's gonna
be hard to work with.

00:20:15.930 --> 00:20:18.160
So that's a very nice
property of the normal.

00:20:20.330 --> 00:20:25.140
Okay, one other fact about the normal

00:20:25.140 --> 00:20:28.370
that's just like a rule of thumb for
the normal.

00:20:32.998 --> 00:20:37.616
Because of the fact that you can't
actually compute this function,

00:20:37.616 --> 00:20:41.050
capital phi other than by
having a table of values.

00:20:41.050 --> 00:20:42.278
Or a computer that, or

00:20:42.278 --> 00:20:45.720
a calculator that specifically
knows how to do that function.

00:20:45.720 --> 00:20:49.984
You can't do it in terms
of other functions,

00:20:49.984 --> 00:20:55.145
it's useful to just have a few
quick rules of thumb, so

00:20:55.145 --> 00:21:01.226
there's something called
the 68- 95-99.7% rule.

00:21:01.226 --> 00:21:06.148
And I don't know who named it that,
but at the first time I heard of this

00:21:06.148 --> 00:21:10.730
that's the stupidest name for
a rule that I have ever heard of.

00:21:10.730 --> 00:21:13.622
However, then I always remember that,
so actually it works very well.

00:21:13.622 --> 00:21:18.686
It simply says it's just the three
simple numbers telling us

00:21:18.686 --> 00:21:23.848
how likely is it that a normal
random variable will be a certain

00:21:23.848 --> 00:21:29.328
distance from its mean measured
in terms of standard deviation.

00:21:29.328 --> 00:21:33.968
So this says that, if x is normal,

00:21:33.968 --> 00:21:39.730
then the statement is that the probability

00:21:39.730 --> 00:21:47.270
that x is more than 1 standard
deviation from its mean.

00:21:47.270 --> 00:21:49.990
So notationally we would
just write it like that.

00:21:49.990 --> 00:21:54.209
But intuitively, that's just saying what's
the chance that it falls more than 1

00:21:54.209 --> 00:21:55.786
standard deviation, right?

00:21:55.786 --> 00:21:57.310
That's 1 standard deviation.

00:21:57.310 --> 00:22:02.965
This would say the distance is more than
1 standard deviation away from the mean.

00:22:02.965 --> 00:22:05.070
Well, I was actually right the other way.

00:22:08.320 --> 00:22:12.316
The probability that x is
within 1 standard deviation of

00:22:12.316 --> 00:22:14.180
its mean is about 68%.

00:22:14.180 --> 00:22:22.569
The chance that x is within 2 standard
deviations of its mean is about 95%.

00:22:22.569 --> 00:22:28.720
And the chance that it's same with 3
standard deviations is about 99.7%.

00:22:28.720 --> 00:22:33.116
So, in other words, it's very common for

00:22:33.116 --> 00:22:39.719
people in practice to add and
subtract 2 standard deviations.

00:22:39.719 --> 00:22:46.352
What that's saying is for the normal,
that's gonna have 95% chance of so,

00:22:46.352 --> 00:22:53.890
let's say you got a bunch of observations
from this distribution independently.

00:22:53.890 --> 00:22:58.202
We would expect about 95% of them
are gonna be within 2 standard

00:22:58.202 --> 00:23:01.380
deviations of the mean, 99.7% within 3.

00:23:01.380 --> 00:23:05.440
So you can convert these statements into
statements about capital phi which is good

00:23:05.440 --> 00:23:08.816
practice while just making sure you
understand what capital phi is.

00:23:08.816 --> 00:23:12.958
But basically, this is just a few values
of capital phi just written in kind of

00:23:12.958 --> 00:23:14.180
a more intuitive way.

00:23:15.600 --> 00:23:18.880
Okay, so that's all for
the normal distribution.

00:23:20.810 --> 00:23:25.050
So the main thing left to
talk more about is LOTUS, and

00:23:25.050 --> 00:23:29.300
a couple examples of LOTUS and
using LOTUS to compute variances.

00:23:29.300 --> 00:23:30.460
For example,

00:23:30.460 --> 00:23:35.040
we proved that the variance of
the Poisson is Poisson lambda has, sorry.

00:23:35.040 --> 00:23:38.110
We proved that the mean of
a Poisson lambda is lambda.

00:23:38.110 --> 00:23:42.030
We have not yet
derived the variance of a Poisson lambda.

00:23:42.030 --> 00:23:44.610
So that's definitely
something we should do.

00:23:46.230 --> 00:23:47.290
So, okay.

00:23:47.290 --> 00:23:48.637
So let's do the variance of the Poisson.

00:23:54.941 --> 00:23:57.793
And that will also give us
a change to understand more,

00:23:57.793 --> 00:23:59.722
what's really going on with LOTUS?

00:23:59.722 --> 00:24:00.222
Why does LOTUS really work?

00:24:04.320 --> 00:24:08.100
So suppose we had a random
variable such as the Poisson, but

00:24:08.100 --> 00:24:09.970
right now I'm just thinking in general.

00:24:09.970 --> 00:24:15.810
A random variable who's possible values
are zero, one, two, three, and so on.

00:24:15.810 --> 00:24:17.580
So let's call our random variable x.

00:24:20.640 --> 00:24:25.040
And x can be 0, 1,

00:24:25.040 --> 00:24:29.810
2, 3, etc, okay?

00:24:29.810 --> 00:24:33.310
And suppose that its pmf.

00:24:33.310 --> 00:24:36.990
To say what the pmf is I just need
to say what's the probability of,

00:24:36.990 --> 00:24:41.720
0 let's call that P0 probability of 1,
P1, P2, P3.

00:24:43.640 --> 00:24:45.570
So all I did here was write out the pmf,

00:24:45.570 --> 00:24:48.300
just stringing it out as a sequence,
right?

00:24:48.300 --> 00:24:50.690
But that's just specifying the pmf and

00:24:50.690 --> 00:24:53.650
I'm calling them pj is
the probability that x equals j.

00:24:54.870 --> 00:25:00.920
Now to figure out variance we
need to study x-squared, right?

00:25:00.920 --> 00:25:02.490
So let's look at x squared.

00:25:04.690 --> 00:25:09.413
So 0-squared is 0,
1-squared is 1, 2-squared is 4,

00:25:09.413 --> 00:25:13.080
3-squared is 9, and
we keep going like that.

00:25:17.881 --> 00:25:25.090
From this point of view, it should
be easy to see what we should do.

00:25:25.090 --> 00:25:27.420
Because E(x), remember for

00:25:27.420 --> 00:25:30.670
a discrete random variable E(x)
is the sum of x times the pmf.

00:25:31.900 --> 00:25:37.210
Now here we want E(x-squared),

00:25:37.210 --> 00:25:42.330
but notice that the probability
that x-squared equals say 3-squared

00:25:42.330 --> 00:25:46.680
is just the probability P3 of
being in this column here, right?

00:25:46.680 --> 00:25:48.310
So the probabilities didn't change, and

00:25:48.310 --> 00:25:54.160
we could just still use x-squared times
the probability that x = x, right?

00:25:54.160 --> 00:25:57.390
Because when an x-squared
takes on these possible values

00:25:57.390 --> 00:25:59.900
with these same probabilities.

00:25:59.900 --> 00:26:04.730
That's what LOTUS is saying, so
it's pretty intuitive in that sense.

00:26:04.730 --> 00:26:08.780
The case that you have to think more about
is the case where this function is not

00:26:08.780 --> 00:26:09.680
1 to 1.

00:26:09.680 --> 00:26:13.050
So now squaring is not 1 to 1 in general.

00:26:13.050 --> 00:26:18.100
If I had had negative numbers,
then you would have duplicates here and

00:26:18.100 --> 00:26:19.990
you would have to sort that out.

00:26:19.990 --> 00:26:24.030
What LOTUS says is even when you have
those duplications, this still works.

00:26:24.030 --> 00:26:25.670
That I think is a little less obvious,

00:26:25.670 --> 00:26:28.990
if you think about it you can see why it's
true, but it's not completely obvious.

00:26:28.990 --> 00:26:31.850
In this case,
because we're not non-negative anyway,

00:26:31.850 --> 00:26:36.072
this is one to one and
then it just immediately true, okay?

00:26:36.072 --> 00:26:39.650
But LOTUS this is saying, no matter
how complicated your function is,

00:26:39.650 --> 00:26:42.950
something kind of this flavor still works,

00:26:42.950 --> 00:26:44.530
regardless of whether
you have duplications.

00:26:46.250 --> 00:26:49.030
So now we're ready to get
the Poisson variance.

00:26:50.320 --> 00:26:52.820
So this is just in general if you

00:26:52.820 --> 00:26:55.490
have a random variable
non-negative integer values.

00:26:55.490 --> 00:26:58.080
Now let's look at the specific
case of Poisson lambda and

00:26:58.080 --> 00:27:00.790
we want to find E(x) squared.

00:27:01.940 --> 00:27:07.080
And according to LOTUS we can just
write that as the sum k = 0 to

00:27:07.080 --> 00:27:11.520
infinity k-squared E to the minus lambda,

00:27:11.520 --> 00:27:16.640
lambda to the k over k factorial,
that's the pmf.

00:27:18.930 --> 00:27:20.470
So we have to figure
out how to do this sum,

00:27:20.470 --> 00:27:24.580
and this looks like
a pretty unfamiliar sum.

00:27:24.580 --> 00:27:28.490
I mean my first thought when I see this
would be, well this is k times k and

00:27:28.490 --> 00:27:31.365
we can cancel and
get a k minus one factorial here.

00:27:31.365 --> 00:27:33.190
And there's nothing wrong
with doing that but

00:27:33.190 --> 00:27:37.110
it's still kind of annoying because
we still have k-squared up here.

00:27:37.110 --> 00:27:39.880
When we were just planning the mean,
then we just had a k and

00:27:39.880 --> 00:27:42.080
we cancelled it and things are nice.

00:27:42.080 --> 00:27:45.340
But now we have a k-squared,
it's more annoying, okay?

00:27:45.340 --> 00:27:48.200
So here's another method for
dealing with something like that.

00:27:49.980 --> 00:27:53.570
The general method is start
with what we know, right?

00:27:53.570 --> 00:27:58.680
So what we know how to do is
the Taylor series for e to the x.

00:27:58.680 --> 00:28:02.320
Hopefully you all know that by now,
we keep using it over and over again.

00:28:02.320 --> 00:28:04.360
The sum, I'll write it in terms of lambda.

00:28:06.120 --> 00:28:10.305
The sum of lambda to
the k over k factorial.

00:28:13.886 --> 00:28:17.829
Is e to the lambda, and this is valid for
all real lambda, even for

00:28:17.829 --> 00:28:22.810
imaginary numbers, complex numbers,
this is always true, always converges.

00:28:24.190 --> 00:28:26.380
Now if I wanna get a k in front,

00:28:26.380 --> 00:28:30.940
then a natural strategy would be to
take the derivative of both sides.

00:28:32.460 --> 00:28:33.620
Well that's pretty nice right,

00:28:33.620 --> 00:28:36.770
because the derivative e to
the lambda is e to the lambda.

00:28:36.770 --> 00:28:38.650
The derivative of the left-hand side,

00:28:39.940 --> 00:28:43.710
I'll start the sum at 1
now because at 0 it's 0.

00:28:43.710 --> 00:28:49.930
So we have k lambda to
the k -1 over k factorial.

00:28:49.930 --> 00:28:51.990
I just took the derivative of both sides.

00:28:51.990 --> 00:28:54.330
I exchanged the derivative and the sum,

00:28:54.330 --> 00:28:58.730
which is valid under some
mild technical conditions.

00:29:01.470 --> 00:29:05.700
Now we're getting closer, but we still
only have a k, not a k squared, okay?

00:29:05.700 --> 00:29:08.570
So my first impulse would be,
take a derivative again,

00:29:08.570 --> 00:29:14.350
that's slightly annoying cuz then I'd get
a k-1 coming down, I want a k, not a k-1.

00:29:14.350 --> 00:29:19.830
So to fix that, all we have to do is
multiply both sides by lambda, okay?

00:29:19.830 --> 00:29:22.250
So, just put lambda on both sides.

00:29:24.150 --> 00:29:26.260
So I call that replenishing the lambdas.

00:29:26.260 --> 00:29:29.640
We just replenish it,
that we have a lambda there.

00:29:29.640 --> 00:29:32.290
I'll write it again,
k equals one to infinity.

00:29:34.020 --> 00:29:39.390
K, lambda to the k over k factorial
equals lambda e to the lambda.

00:29:40.530 --> 00:29:42.540
We've replenished our supply of lambda's,

00:29:42.540 --> 00:29:45.758
now we can take the derivative again and
we have what we want.

00:29:45.758 --> 00:29:50.910
Okay, so
I take the derivative a second time and

00:29:53.640 --> 00:29:59.530
k = 1 to infinity, take the derivative
again, now it's k-squared.

00:29:59.530 --> 00:30:03.840
Lambda to the k- 1 over k factorial.

00:30:03.840 --> 00:30:08.010
Well now we have to use the product rule,
the derivative of lambda, e to the lambda

00:30:08.010 --> 00:30:13.700
is lambda e to the lambda plus e
to the lambda by the product rule.

00:30:13.700 --> 00:30:19.206
Which we can factor out as e to
the lambda times lambda + 1.

00:30:21.625 --> 00:30:25.580
Okay, well that's exactly
the sum that we needed.

00:30:25.580 --> 00:30:29.328
Cuz this e to the minus lambda comes out,
so

00:30:29.328 --> 00:30:34.652
this is e to the minus lambda,
e to the lambda, lambda + 1.

00:30:38.994 --> 00:30:41.840
I'm missing some,
is there another lambda somewhere?

00:30:41.840 --> 00:30:43.860
Lets see, we have to replenish it again.

00:30:45.080 --> 00:30:46.800
Just put a lambda.

00:30:46.800 --> 00:30:50.970
Okay, so here we have lambda to the k- 1,
there we want lambda to the k.

00:30:50.970 --> 00:30:55.370
So its replenish again,
then there is another lambda there, okay.

00:30:55.370 --> 00:30:59.550
I'm just bringing this k- 1 back
to being lambda to the k, right?

00:31:00.770 --> 00:31:06.780
So that's just lambda squared + lambda.

00:31:08.740 --> 00:31:09.680
And now we have the variance.

00:31:11.610 --> 00:31:17.660
So the variance of X equals this thing,

00:31:17.660 --> 00:31:23.190
lambda squared plus lambda
minus the square of the mean,

00:31:23.190 --> 00:31:25.590
which is lambda squared equals lambda.

00:31:28.120 --> 00:31:30.850
So this course is not really
about memorizing formulas, but

00:31:30.850 --> 00:31:33.680
that's one that's very easy and
useful to remember.

00:31:33.680 --> 00:31:37.051
The Poisson lambda has mean lambda,
and has variance lambda.

00:31:40.572 --> 00:31:43.990
So that's kind of a strange
property if you think about it.

00:31:43.990 --> 00:31:48.900
That the mean equals the variance,
it's a little bit,

00:31:48.900 --> 00:31:52.160
maybe it would seem more natural if
the mean equal the standard deviation or

00:31:52.160 --> 00:31:55.980
something like that, because then
those are kind of in the same scale.

00:31:55.980 --> 00:31:58.250
But Poisson,
it doesn't actually have units.

00:31:58.250 --> 00:32:01.430
Poisson is just counting
numbers of things, so

00:32:01.430 --> 00:32:04.280
it doesn't have that some
dimensional interpretation.

00:32:05.610 --> 00:32:09.790
So, yeah, I wanted to also mention
that about standardization as well.

00:32:10.860 --> 00:32:15.737
Another reason this thing is really
nice to work with in the normal is if

00:32:15.737 --> 00:32:20.448
you think of normal as being
a continuous measurement in some unit,

00:32:20.448 --> 00:32:24.269
it could be a unit of length,
time, mass, whatever.

00:32:24.269 --> 00:32:27.528
If x is measured in
whatever unit you want,

00:32:27.528 --> 00:32:32.508
let's say it's time measured in seconds,
then that's seconds

00:32:32.508 --> 00:32:37.490
minus seconds divided by seconds,
the seconds cancel out.

00:32:37.490 --> 00:32:41.225
That means this is a dimensionless
quantity, which is part of what's making

00:32:41.225 --> 00:32:46.050
this standardization, it's kind of
making it more directly interpretable

00:32:46.050 --> 00:32:51.010
instead of having to worry about whether
you measured it in seconds or years.

00:32:51.010 --> 00:32:54.530
So if we started with one measurement in
seconds and one measurement in years and

00:32:54.530 --> 00:32:57.030
standardized both of them,
we get the same thing.

00:32:57.030 --> 00:32:58.820
The same measurement in different units.

00:32:58.820 --> 00:33:00.400
So that's a nice property of that.

00:33:02.120 --> 00:33:05.160
Okay, so
that's the variance of the Poisson.

00:33:05.160 --> 00:33:08.670
We haven't yet gotten the variance of
the binomial, so I'd like to do that.

00:33:14.268 --> 00:33:17.490
There's an easy way and a hard way.

00:33:17.490 --> 00:33:20.590
Well, except the hard way I don't think,
actually sorry,

00:33:20.590 --> 00:33:22.580
there's three ways to do it.

00:33:22.580 --> 00:33:26.000
There's a really easy
way that we can't do yet

00:33:26.000 --> 00:33:28.300
because we haven't proven
the necessary fact.

00:33:28.300 --> 00:33:31.850
There's an easy way that we can do,
so that's what I'm gonna do.

00:33:31.850 --> 00:33:35.070
And then there's an annoying way,
which we're not gonna do.

00:33:35.070 --> 00:33:38.917
The annoying but direct is we
want the variance of a binomial.

00:33:41.833 --> 00:33:43.010
We wanna find the variance.

00:33:53.130 --> 00:33:58.124
The most direct obvious way to do this
would be to use lotus to get E(x squared)

00:33:58.124 --> 00:34:02.362
which would mean you would have to
write down something like this,

00:34:02.362 --> 00:34:04.800
except here we wrote the Poisson PMF.

00:34:04.800 --> 00:34:07.430
Instead you'd have to write n choose k,
p to the k, whatever,

00:34:07.430 --> 00:34:08.360
the binomial PMF, right.

00:34:08.360 --> 00:34:10.100
And then you'd have to do that sum.

00:34:11.360 --> 00:34:14.586
And you can do it, but
that's pretty tedious.

00:34:14.586 --> 00:34:18.400
And you have to figure out how to do
that sum and do a lot of algebra.

00:34:18.400 --> 00:34:21.540
Okay, so
that's the way I don't wanna do it.

00:34:21.540 --> 00:34:25.574
The easiest way to do it would
be using this fact here.

00:34:28.118 --> 00:34:32.876
Which is that the variance of a sum
of independent things is the sum of

00:34:32.876 --> 00:34:36.480
the variance,
if they're independent, right.

00:34:38.460 --> 00:34:39.210
That's if, okay.

00:34:40.696 --> 00:34:43.470
So the easiest one,
we haven't proven this yet so

00:34:43.470 --> 00:34:47.910
this is not valid to do it at this way
right now but just kinda foreshadowing.

00:34:47.910 --> 00:34:49.172
We can think of the binomial,

00:34:49.172 --> 00:34:52.360
we've emphasized the fact that we can
think of a binomial as the sum of n

00:34:52.360 --> 00:34:54.700
independent Bernoulli p.

00:34:54.700 --> 00:34:57.230
So once we prove this fact,
that's applicable.

00:34:57.230 --> 00:35:00.020
So all we have to do is get
the variance of Bernoulli p,

00:35:00.020 --> 00:35:03.386
which is a really easy calculation
cuz the Bernoulli is just zero one,

00:35:03.386 --> 00:35:05.410
so that's a very very easy calculation.

00:35:05.410 --> 00:35:08.570
To get the variance of a Bernoulli p and
multiply by n,

00:35:08.570 --> 00:35:11.170
that's the neatest way to do it.

00:35:11.170 --> 00:35:14.990
You can do it that way in your head
once we get to that point, okay.

00:35:14.990 --> 00:35:19.380
Now here's kind of the compromise method
which is also just good practice with

00:35:19.380 --> 00:35:22.530
other concepts we've done,
especially indicator random variables.

00:35:22.530 --> 00:35:25.954
So I'm still going to use the same idea of

00:35:25.954 --> 00:35:29.590
representing x as a sum
of Iid Bernoulli p.

00:35:31.586 --> 00:35:34.544
So I'll write them as I1 plus blah,
blah, blah, plus In,

00:35:34.544 --> 00:35:38.690
just to emphasize the fact that
they're indicators, I for indicator.

00:35:38.690 --> 00:35:46.826
Where Ijs are Iid Bournulli p, right.

00:35:46.826 --> 00:35:48.950
So we've been doing this
many times already.

00:35:48.950 --> 00:35:53.820
That's just an indicator of success
on the jth trial, add up those and

00:35:53.820 --> 00:35:55.460
we get a binomial.

00:35:55.460 --> 00:35:58.273
Okay, so
now if we want the expected value of x

00:35:58.273 --> 00:36:02.940
squared, Let's just square this thing.

00:36:05.858 --> 00:36:08.630
Let's actually not do
the expected value yet.

00:36:08.630 --> 00:36:11.870
We'll just square it then
take the expected value.

00:36:11.870 --> 00:36:13.250
So just square this thing.

00:36:13.250 --> 00:36:17.400
Well you know you do i1 squared and
just square all the things, right.

00:36:17.400 --> 00:36:23.107
So it's i1 squared plus blah
blah blah plus In squared plus,

00:36:23.107 --> 00:36:28.030
but as you know we get a lot
of cross terms, right.

00:36:28.030 --> 00:36:34.707
Your imagining this big thing times
itself, so every possible cross term,

00:36:34.707 --> 00:36:39.836
each one twice, you have 2I 1I 2 and
2I 1I 3 and so on.

00:36:41.687 --> 00:36:44.980
All possible cross terms and
each cross term has 2 in front.

00:36:44.980 --> 00:36:48.610
Just like when you square x+y,
you get x squared + y squared + 2xy.

00:36:48.610 --> 00:36:50.000
We get all these cross terms.

00:36:50.000 --> 00:36:52.440
It doesn't matter what
order we write them in.

00:36:52.440 --> 00:36:54.570
Maybe we've ordered them in this way.

00:36:54.570 --> 00:36:55.480
So that's the last one.

00:36:55.480 --> 00:36:56.600
It doesn't matter the order.

00:36:56.600 --> 00:36:58.510
Okay, so it's all the cross terms.

00:36:58.510 --> 00:36:59.790
That looks pretty complicated.

00:36:59.790 --> 00:37:02.620
But it's actually much
simpler than it looks.

00:37:04.080 --> 00:37:07.930
Now let's take the expected value
of both sides, use linearity.

00:37:10.220 --> 00:37:12.980
Of the same,
this is a good review example as well.

00:37:12.980 --> 00:37:16.307
We're using the same tricks, symmetry,
indicator, random variables, and

00:37:16.307 --> 00:37:16.862
linearity.

00:37:19.402 --> 00:37:21.080
Each of these, these are Iid.

00:37:21.080 --> 00:37:25.305
So by symmetry,
this is just n times anyone of them.

00:37:25.305 --> 00:37:28.978
So let's just say nE(1 squared).

00:37:28.978 --> 00:37:31.100
That's just immediate by symmetry, right.

00:37:31.100 --> 00:37:34.290
So we don't have to write that big sum,
just n times one of them.

00:37:34.290 --> 00:37:36.230
And how let's just count
how many of these,

00:37:36.230 --> 00:37:38.340
well there's n choose two cross terms,
right.

00:37:38.340 --> 00:37:41.760
Because for any pair of
subscripts we have a cross term.

00:37:41.760 --> 00:37:46.030
So it's really just 2(n choose 2),
and then just take one of them for

00:37:46.030 --> 00:37:51.910
concreteness, let's say E(I1I2).

00:37:51.910 --> 00:37:56.130
Now this is even nicer,
well it definitely is looking better.

00:37:56.130 --> 00:38:02.780
But this is even better than it looks
because I1 is either just 1 or 0.

00:38:02.780 --> 00:38:06.690
If you square one you got one,
if you square zero you got zero.

00:38:06.690 --> 00:38:08.870
So I1 squared is just I1.

00:38:10.852 --> 00:38:15.520
So E(I1), that's just
the expectorate of Bernoulli p is p.

00:38:15.520 --> 00:38:20.060
So that's just np+n

00:38:20.060 --> 00:38:24.290
choose 2 is n times n-1 over 2,
so the 2s cancel.

00:38:24.290 --> 00:38:27.302
So this is really just n(n-1).

00:38:27.302 --> 00:38:30.570
Now let's think about this
indicator random variable.

00:38:30.570 --> 00:38:32.550
Well I called it an indicator
random variable,

00:38:32.550 --> 00:38:35.910
well actually it's a product
of indicator random variables.

00:38:35.910 --> 00:38:40.540
But actually a product of indicator random
variables is an indicator random variable.

00:38:40.540 --> 00:38:45.500
This thing here is the indicator
of success on both the first and

00:38:45.500 --> 00:38:46.528
the second trial, right.

00:38:47.700 --> 00:38:50.940
Because if you think of multiplying
two numbers that are zero and one, you

00:38:50.940 --> 00:38:55.740
get zero if at least one of these is zero,
you would get one if they're both one.

00:38:55.740 --> 00:38:59.623
So that's the indicator
of success on both.

00:39:05.289 --> 00:39:07.680
So it's a product but
it's actually just one indicator.

00:39:07.680 --> 00:39:11.729
Success on both trials, number 1 and 2.

00:39:16.362 --> 00:39:20.250
So its expected value is just
the probability of that happening.

00:39:20.250 --> 00:39:23.750
That probability of success
on both the first trial and

00:39:23.750 --> 00:39:26.890
the second trial, because the trials
are independent, is just p squared.

00:39:29.680 --> 00:39:32.340
Okay, so that's just, so

00:39:32.340 --> 00:39:36.920
what we just computed is
the second moment of the binomial.

00:39:36.920 --> 00:39:42.098
That's np+, if we multiply that

00:39:42.098 --> 00:39:49.251
np+n squared p squared-np squared, right.

00:39:49.251 --> 00:39:57.770
Now to get the variance all we have to do
is subtract, The square of the mean, okay.

00:39:57.770 --> 00:40:02.640
So we showed before that
a binomial np has mean n times p.

00:40:02.640 --> 00:40:06.970
So if we square that, that's this term
n squared p squared, so that cancels.

00:40:06.970 --> 00:40:12.259
So we're just canceling
out this middle term and

00:40:12.259 --> 00:40:17.166
we just have np- np squared = np (1- p).

00:40:17.166 --> 00:40:21.913
Which we would often write

00:40:21.913 --> 00:40:26.030
as npq with q = 1- p.

00:40:26.030 --> 00:40:28.550
So binomial variance is npq.

00:40:31.540 --> 00:40:37.090
So that's just a good review of indicator
of random variables and all of that stuff.

00:40:37.090 --> 00:40:39.550
So now we know the variance
of the Poisson,

00:40:39.550 --> 00:40:44.170
the normal, the uniform, the binomial.

00:40:44.170 --> 00:40:49.080
For the geometric, it's kind

00:40:49.080 --> 00:40:54.460
of a similar calculation, we did the mean
of the geometric in two different ways.

00:40:54.460 --> 00:40:58.724
The flavor of the calculation is similar
to this except we have a geometric series

00:40:58.724 --> 00:41:00.840
instead of the Taylor series for
e to the x.

00:41:00.840 --> 00:41:03.320
So I don't think it's
worth doing that in class.

00:41:04.870 --> 00:41:09.518
So in general hypergeometric, let's
talk a little bit about hypergeometric,

00:41:09.518 --> 00:41:13.010
that's pretty nasty.

00:41:13.010 --> 00:41:16.460
In the sense that in the hypergeometric,

00:41:16.460 --> 00:41:19.750
we could write it as a sum of
indicator random variables.

00:41:19.750 --> 00:41:24.580
We're imagining we're drawing
balls one at a time and, or

00:41:24.580 --> 00:41:30.110
picking elk one at a time and
success is getting a tagged elk.

00:41:30.110 --> 00:41:32.150
But the problem is that
they're not independent.

00:41:32.150 --> 00:41:34.550
So as far as the mean is
concerned we still use linearity.

00:41:34.550 --> 00:41:36.790
For the variance it's more complicated.

00:41:36.790 --> 00:41:41.650
So we'll worry about the variance of
a hypergeometric after the midterm.

00:41:41.650 --> 00:41:42.600
That's more complicated.

00:41:42.600 --> 00:41:47.616
But for the binomial this is really,
well, actually we could still.

00:41:47.616 --> 00:41:50.636
Here I didn't actually use the factor
there independent cuz I was just using

00:41:50.636 --> 00:41:51.164
linearity.

00:41:51.164 --> 00:41:55.422
So you could use a similar approach, so
actually you could do it this way, but

00:41:55.422 --> 00:41:58.845
it would be too tedious to do it
like on a midterm or something.

00:41:58.845 --> 00:42:03.108
But you could square it, if these
are dependent well, you can still work out

00:42:03.108 --> 00:42:06.980
the probability that the first two
elk that you pick are both tagged.

00:42:06.980 --> 00:42:09.590
You could do that without
too much trouble.

00:42:09.590 --> 00:42:11.010
But it's pretty messy looking.

00:42:12.300 --> 00:42:17.150
All right, so that's variance,
and I guess the last thing

00:42:17.150 --> 00:42:22.450
to do is just to explain more
about why is LOTUS true?

00:42:22.450 --> 00:42:26.994
And the basic proof of
that is actually kind of

00:42:26.994 --> 00:42:32.145
conceptually similar to
how we proved linearity.

00:42:37.139 --> 00:42:44.251
So we're trying to prove LOTUS, and
I'm only gonna prove it for a discrete.

00:42:47.794 --> 00:42:50.502
Let's say discrete sample space.

00:42:50.502 --> 00:42:54.180
That's the case where I'm
imagining finitely many pebbles.

00:42:54.180 --> 00:42:57.510
In the general case the ideas
are not essentially different.

00:42:57.510 --> 00:43:02.020
It's just that we kind of need to
write down some fancier integrals and

00:43:02.020 --> 00:43:07.830
use more kind of more technical math,
but the concept is similar.

00:43:07.830 --> 00:43:10.350
So this is enough to give you the idea.

00:43:10.350 --> 00:43:15.743
So for discrete sample space,
so the statement is that

00:43:15.743 --> 00:43:21.253
the expected value,
that's all we are trying to show,

00:43:21.253 --> 00:43:28.189
is that the E(g(x)) can be written
as the sum of g(x) P(X=x).

00:43:28.189 --> 00:43:33.085
So right, we can use the PMF of
x we do not have to first work

00:43:33.085 --> 00:43:36.990
on figuring out the distribution of g(x).

00:43:36.990 --> 00:43:41.390
That's all we are trying to do,
so let's think about it.

00:43:46.194 --> 00:43:52.615
Let's think about it as a sum of,
sum over the other.

00:43:52.615 --> 00:43:59.112
We could sum the other way around, sorry.

00:43:59.112 --> 00:44:01.256
Let me say this a different way,

00:44:01.256 --> 00:44:06.490
let me remind you of the identity
that we use for proving linearity.

00:44:06.490 --> 00:44:08.760
That was this group versus ungroup thing.

00:44:09.820 --> 00:44:14.510
So what we have is two different
ways to write a certain sum.

00:44:14.510 --> 00:44:20.320
We could either write this thing,
g(x)P(X=x) or we could write

00:44:22.050 --> 00:44:26.530
it the other way,
which is a sum over all s.

00:44:26.530 --> 00:44:31.440
Each s, we're thinking of that
as s in the sample space S.

00:44:31.440 --> 00:44:33.309
So each little s is a pebble.

00:44:34.390 --> 00:44:37.670
And if we're summing it
up pebble by pebble,

00:44:37.670 --> 00:44:40.808
then what we're doing is remember
random variables are functions.

00:44:40.808 --> 00:44:45.760
So, and g(x) just means we apply
the function x then apply the function g.

00:44:45.760 --> 00:44:48.446
So we're just computing g(x(s)),

00:44:48.446 --> 00:44:52.403
that's just the definition
times the mass of that pebble.

00:44:56.852 --> 00:44:57.785
So.

00:44:59.744 --> 00:45:02.405
If you stare at this equation long enough,
and

00:45:02.405 --> 00:45:06.990
we have five minutes left to stare at
that equation, so that's plenty of time.

00:45:06.990 --> 00:45:10.350
This is why LOTUS is true.

00:45:10.350 --> 00:45:12.510
It's just a matter of
understanding this equation.

00:45:12.510 --> 00:45:16.434
So I'm gonna talk a little more about,
how do you make sense of this equation?

00:45:18.350 --> 00:45:21.110
This is the grouped case.

00:45:22.220 --> 00:45:23.960
This is the ungrouped case.

00:45:23.960 --> 00:45:27.280
Remember I talked about pebbles and
super pebbles, ungrouped.

00:45:31.260 --> 00:45:35.140
This says take each pebble,
compute this function,

00:45:35.140 --> 00:45:38.080
g of x of s, and
you take a weighted average.

00:45:38.080 --> 00:45:39.750
Those are the weights.

00:45:39.750 --> 00:45:43.780
This says,
first combine all of the pebbles that have

00:45:43.780 --> 00:45:47.444
the same value of x into you know,
super-pebbles.

00:45:47.444 --> 00:45:51.890
A super-pebble means we grouped together
all pebbles with the same x value,

00:45:51.890 --> 00:45:54.428
not the same g(x) value, the same x value.

00:45:54.428 --> 00:45:58.310
Group those together then average,
you get the same thing.

00:46:00.270 --> 00:46:04.530
So if I want to write that out
in a little bit more detail.

00:46:04.530 --> 00:46:09.320
One way to think of it is as a double sum,
right?

00:46:09.320 --> 00:46:13.232
Because we could imagine
first summing over x.

00:46:17.783 --> 00:46:21.648
I'm gonna break this sum up.

00:46:21.648 --> 00:46:25.610
What I just explained to you was the
intuition for why this is equal to this.

00:46:25.610 --> 00:46:28.440
Because we're just grouping them
together in different ways so

00:46:28.440 --> 00:46:31.460
we changed the weights around, but as long
as we changed the weights appropriately we

00:46:31.460 --> 00:46:32.770
should get the same average.

00:46:32.770 --> 00:46:33.990
That's the intuition.

00:46:33.990 --> 00:46:38.610
But for any of you who wanna see more of
an algebraic reason, justification for

00:46:38.610 --> 00:46:41.342
that, the way to think of
it is as a double sum.

00:46:41.342 --> 00:46:43.290
So the double sum would be,

00:46:43.290 --> 00:46:48.055
I mean to rewrite this says
sum overall pebbles right?

00:46:48.055 --> 00:46:53.280
But one way to think of that would be
first sum over values of little x.

00:46:53.280 --> 00:46:58.390
And then for each value of little x,
sum over all pebbles,

00:46:58.390 --> 00:47:02.840
s such that x(s) = x.

00:47:02.840 --> 00:47:06.730
Because this is just a sum
of a bunch of a numbers.

00:47:06.730 --> 00:47:09.480
We can sum them in any order we want.

00:47:09.480 --> 00:47:11.040
So I can rearrange them,

00:47:11.040 --> 00:47:15.680
in this particular order where I'm saying
first sum over the little x values, and

00:47:15.680 --> 00:47:19.810
then group together, and sum over all
the pebbles that have that value.

00:47:19.810 --> 00:47:22.590
It's the exact same thing,
I just reordered the terms.

00:47:23.880 --> 00:47:29.509
So that's g(x(s)) times P(s).

00:47:32.926 --> 00:47:35.410
Now let's just simplify this double sum.

00:47:35.410 --> 00:47:40.130
The reason I wanted to write it as
a double sum like this is that within this

00:47:40.130 --> 00:47:46.470
inner summation X(s)=, so
this thing is just g(x).

00:47:46.470 --> 00:47:52.393
The cool thing is that g(x) does
not depend on s so that comes out.

00:47:52.393 --> 00:47:57.393
So we actually have the sum
over x of g(x) times

00:47:57.393 --> 00:48:01.145
the sum of what ever is left p(s).

00:48:03.670 --> 00:48:09.540
And now so that's summed over
all s such that s(x) = x.

00:48:09.540 --> 00:48:13.490
And now we're done with
the proof because this

00:48:13.490 --> 00:48:18.280
sum here is just saying add up
the masses of all the pebbles labeled x.

00:48:18.280 --> 00:48:20.500
In other words,
that's what I called a super pebble.

00:48:20.500 --> 00:48:23.390
The super pebble,
the mass is the sum of all

00:48:23.390 --> 00:48:26.330
the masses of the little pebbles
that form the super pebble.

00:48:26.330 --> 00:48:28.500
That's p of, this is just practice,

00:48:28.500 --> 00:48:32.420
this is going back to the very beginning
of, events and what's a random variable.

00:48:32.420 --> 00:48:35.893
That's just the event X = x.

00:48:35.893 --> 00:48:39.500
We talked, what does it mean for
big X to equal little x, right?

00:48:39.500 --> 00:48:40.810
What does that equation mean?

00:48:40.810 --> 00:48:42.400
That's an event.

00:48:42.400 --> 00:48:45.550
That's this event that we have here.

00:48:45.550 --> 00:48:47.593
Okay, so that's why that's true.

00:48:47.593 --> 00:48:49.000
So that's why LOTUS is true.

00:48:49.000 --> 00:48:51.390
Anyway that's all for
now and Friday we'll review,

00:48:51.390 --> 00:48:54.610
let me know if you have any suggestions
for things to do on Friday.

