WEBVTT
Kind: captions
Language: en

00:00:04.040 --> 00:00:13.110
We can have no guarantee that a super intelligent
AI is going to do what we want.

00:00:13.110 --> 00:00:17.800
Once we're creating something ten, a hundred,
a thousand, a million times more intelligent

00:00:17.800 --> 00:00:23.320
than we are it would be insane to think that
we could really like rigorously control what

00:00:23.320 --> 00:00:24.320
it does.

00:00:24.320 --> 00:00:30.940
It may discover aspects of the universe that
we don't even imagine at this point.

00:00:30.940 --> 00:00:40.000
However, my best intuition and educated guess
is that much like raising a human child, if

00:00:40.000 --> 00:00:49.090
we raise the young AGI in a way that's imbued
with compassion, love and understanding and

00:00:49.090 --> 00:00:56.329
if we raise the young AGI to fully understand
human values and human culture then we're

00:00:56.329 --> 00:01:03.060
maximizing the odds that as this AGI gets
beyond our rigorous control at least it's

00:01:03.060 --> 00:01:09.970
own self-modification and evolution is imbued
with human values and culture and with compassion

00:01:09.970 --> 00:01:11.220
and connection.

00:01:11.220 --> 00:01:17.110
So I would rather have an AGI that understood
human values and culture become super intelligent

00:01:17.110 --> 00:01:19.220
than one that doesn't understand even what
we're about.

00:01:19.220 --> 00:01:27.770
And I would rather have an AGI that was doing
good works like advancing science and medicine

00:01:27.770 --> 00:01:32.930
and doing elder care and education becomes
super intelligent than an AGI that was being,

00:01:32.930 --> 00:01:45.270
for example, a spy system, a killer drone
coordination system or an advertising agency.

00:01:45.270 --> 00:01:51.600
So even when you don't have a full guarantee
I think we can do things that commonsensically

00:01:51.600 --> 00:01:55.990
will bias the odds in a positive way.

00:01:55.990 --> 00:02:05.860
Now, in terms of nearer-term risks regarding
AI, I think we now have a somewhat unpleasant

00:02:05.860 --> 00:02:13.040
situation where much of the world's data,
including personal data about all of us and

00:02:13.040 --> 00:02:20.630
our bodies and our minds and our relationships
and our tastes, much of the world's data and

00:02:20.630 --> 00:02:28.230
much of the world's AI fire power are held
by a few large corporations, which are acting

00:02:28.230 --> 00:02:33.129
in close concert with a few large governments.

00:02:33.129 --> 00:02:40.150
In China the connection between big tech and
the government apparatus is very clear, but

00:02:40.150 --> 00:02:41.900
in the U.S. as well.

00:02:41.900 --> 00:02:47.489
I mean there was a big noise about Amazon's
new office, well 25,000 Amazon employees are

00:02:47.489 --> 00:02:52.950
going in Crystal City Virginia right next-door
to the Pentagon; there could be a nice big

00:02:52.950 --> 00:02:56.099
data pipe there if they want.

00:02:56.099 --> 00:03:01.959
We in the U.S. as well have very close connections
between big tech and government.

00:03:01.959 --> 00:03:06.470
Anyone can Google Eric Schmidt verses NSA
as well.

00:03:06.470 --> 00:03:11.610
So there's a few big companies with close
government connections hoarding everyone's

00:03:11.610 --> 00:03:19.110
data, developing AI processing power, hiring
most of the AI PhDs and it's not hard to see

00:03:19.110 --> 00:03:25.890
that this can bring up some ethical issues
in the near-term, even before we get to superhuman

00:03:25.890 --> 00:03:31.200
super intelligences potentially turning the
universe into paper clips.

00:03:31.200 --> 00:03:40.609
And decentralization of AI can serve to counteract
these nearer-term risks in a pretty palpable

00:03:40.609 --> 00:03:41.609
way.

00:03:41.609 --> 00:03:52.089
So as a very concrete example, one of our
largest AI development offices for SingularityNET,

00:03:52.089 --> 00:03:58.310
and for Hanson Robotics the robotics company
I'm also involved with, is in Addis Ababa

00:03:58.310 --> 00:03:59.310
Ethiopia.

00:03:59.310 --> 00:04:04.650
We have 25 AI developers and 40 or 50 interns
there.

00:04:04.650 --> 00:04:10.019
I mean these young Ethiopians aren't going
to get a job for Google, Facebook, Tencent

00:04:10.019 --> 00:04:16.389
or Baidu except in very rare cases when they
managed to get a work visa to go to one of

00:04:16.389 --> 00:04:18.359
these countries somehow.

00:04:18.359 --> 00:04:28.010
And many of the AI applications of acute interest
in those countries, say AI for analyzing agriculture

00:04:28.010 --> 00:04:33.800
and preventing agricultural disease or AI
for credit scoring for the unbank to enable

00:04:33.800 --> 00:04:40.840
micro finance, AI problems of specific interest
in sub-Saharan Africa don't get a heck of

00:04:40.840 --> 00:04:43.160
a lot of attention these days.

00:04:43.160 --> 00:04:48.290
AI wizardry from young developers there doesn't
have a heck of a lot of market these days

00:04:48.290 --> 00:04:52.760
so you've got a both a lot of the market and
a lot of the developer community that's sort

00:04:52.760 --> 00:04:59.340
of shut out by the siloing of AI inside a
few large tech companies and military organizations.

00:04:59.340 --> 00:05:05.200
And this is both a humanitarian ethical problem
because there's a lot of value being left

00:05:05.200 --> 00:05:10.460
on the table and a lot of value not being
delivered, but it also could become a different

00:05:10.460 --> 00:05:15.590
sort of crisis because if you have a whole
bunch of brilliant young hackers throughout

00:05:15.590 --> 00:05:20.790
the developing world who aren't able to fully
enter into the world economy there's a lot

00:05:20.790 --> 00:05:27.360
of other less pleasant things than work for
Google or Tencent that these young hackers

00:05:27.360 --> 00:05:30.280
could choose to spend their time on.

00:05:30.280 --> 00:05:37.200
So I think getting the whole world fully pulled
into the AI economy in terms of developers

00:05:37.200 --> 00:05:42.480
being able to monetize their code and application
developers having an easy way to apply AI

00:05:42.480 --> 00:05:48.090
to the problems of local interest to them,
I mean this is ethically positive right now

00:05:48.090 --> 00:05:54.490
in terms of doing good and in terms of diverting
effort away from people doing bad things out

00:05:54.490 --> 00:05:55.530
of frustration.

