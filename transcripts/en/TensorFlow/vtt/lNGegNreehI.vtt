WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.353
[MUSIC PLAYING]

00:00:06.987 --> 00:00:09.320
NATHAN SILBERMAN: Imagine
you're in a traumatic accident

00:00:09.320 --> 00:00:11.280
and end up in the hospital.

00:00:11.280 --> 00:00:13.670
One of the first things an
emergency room doctor may do

00:00:13.670 --> 00:00:15.170
is take an ultrasound
device and use

00:00:15.170 --> 00:00:18.770
it to inspect your heart,
your lungs, your kidney.

00:00:18.770 --> 00:00:20.780
Ultrasound has become
an indispensable window

00:00:20.780 --> 00:00:22.250
into the human
body for clinicians

00:00:22.250 --> 00:00:25.090
across areas of medicine.

00:00:25.090 --> 00:00:27.190
As powerful as
ultrasound is, access

00:00:27.190 --> 00:00:30.310
to ultrasound is still
limited by a very high price,

00:00:30.310 --> 00:00:30.977
form factor--

00:00:30.977 --> 00:00:33.310
it's usually a large cart-based
system they have to push

00:00:33.310 --> 00:00:34.197
around--

00:00:34.197 --> 00:00:36.280
and then, finally, years
of education and training

00:00:36.280 --> 00:00:38.350
required to use it effectively.

00:00:38.350 --> 00:00:40.360
As a consequence,
2/3 of the world

00:00:40.360 --> 00:00:43.510
has no access to medical
imaging of any kind,

00:00:43.510 --> 00:00:47.050
5,000 children die every day
from pediatric pneumonia,

00:00:47.050 --> 00:00:48.970
and over 800 women
die every single day

00:00:48.970 --> 00:00:51.520
from totally preventable
complications relating

00:00:51.520 --> 00:00:52.570
to maternal health.

00:00:52.570 --> 00:00:53.740
We need to do better.

00:00:56.203 --> 00:00:57.870
To address this,
Butterfly has developed

00:00:57.870 --> 00:00:59.610
a handheld,
pocket-sized ultrasound

00:00:59.610 --> 00:01:02.250
device that connects
right to your smartphone.

00:01:02.250 --> 00:01:04.379
At $2,000, a
hundredth of the price

00:01:04.379 --> 00:01:06.510
of a conventional
ultrasound system,

00:01:06.510 --> 00:01:09.610
the Butterfly iQ is a
personal ultrasound device,

00:01:09.610 --> 00:01:13.530
a true visual stethoscope.

00:01:13.530 --> 00:01:17.150
Butterfly's ambition is
to democratize ultrasound.

00:01:17.150 --> 00:01:20.180
The price and size of the device
have solved the cost and form

00:01:20.180 --> 00:01:21.080
factor problems.

00:01:21.080 --> 00:01:23.850
But to truly make ultrasound
universally accessible,

00:01:23.850 --> 00:01:26.310
we need to solve two
additional problems.

00:01:26.310 --> 00:01:28.130
The first is guidance--

00:01:28.130 --> 00:01:30.530
where exactly to place
the ultrasound probe.

00:01:30.530 --> 00:01:33.050
That's typically performed
by a stenographer.

00:01:33.050 --> 00:01:35.018
And then, interpretation--
understanding

00:01:35.018 --> 00:01:36.560
the content of the
image is typically

00:01:36.560 --> 00:01:38.610
performed by a radiologist.

00:01:38.610 --> 00:01:40.615
Now, in order to solve
the access problem,

00:01:40.615 --> 00:01:41.990
we can't just
scale up education.

00:01:41.990 --> 00:01:43.760
It doesn't scale fast enough.

00:01:43.760 --> 00:01:45.125
We need to use machine learning.

00:01:45.125 --> 00:01:47.500
What you're about to see is
our machine learning solution

00:01:47.500 --> 00:01:50.030
to guidance called,
acquisition assistance.

00:01:50.030 --> 00:01:54.380
After indicating in the app
what the user wants to image,

00:01:54.380 --> 00:01:56.433
the app shows the
user a split screen.

00:01:56.433 --> 00:01:59.100
On the bottom, what you're going
to see is the ultrasound image.

00:01:59.100 --> 00:02:02.390
And on the top is an
augmented reality interface

00:02:02.390 --> 00:02:06.350
that shows the user
turn-by-turn visual directions

00:02:06.350 --> 00:02:08.570
that indicates how exactly
to move the ultrasound

00:02:08.570 --> 00:02:10.970
device in order to acquire
a diagnostic image.

00:02:14.510 --> 00:02:17.790
After acquiring a diagnostic
image, we need to interpret it.

00:02:17.790 --> 00:02:19.860
One interpretation model
that we've developed

00:02:19.860 --> 00:02:22.170
is for ejection fraction,
an essential measurement

00:02:22.170 --> 00:02:23.670
of cardiac health.

00:02:23.670 --> 00:02:26.340
Ejection fraction captures
the ratio of blood volume

00:02:26.340 --> 00:02:28.590
entering and exiting the
left ventricle each time

00:02:28.590 --> 00:02:30.210
your heart beats.

00:02:30.210 --> 00:02:32.640
This is currently measured
by clinicians by hand

00:02:32.640 --> 00:02:35.470
tracing the edges of
the left ventricle,

00:02:35.470 --> 00:02:38.970
and then evaluating
that change over time.

00:02:38.970 --> 00:02:40.720
Unfortunately, there's
great disagreement,

00:02:40.720 --> 00:02:42.580
even among expert
clinicians, with regard

00:02:42.580 --> 00:02:46.480
to exactly where to place those
tracings or even about which

00:02:46.480 --> 00:02:48.430
frames that are of
sufficient quality

00:02:48.430 --> 00:02:50.650
for reliable evaluation.

00:02:50.650 --> 00:02:53.350
This chart shows a handful
of frames and the responses

00:02:53.350 --> 00:02:56.710
we obtained from six different
expert stenographers regarding

00:02:56.710 --> 00:02:59.470
whether a frame has
sufficient quality.

00:02:59.470 --> 00:03:02.555
This kind of disagreement
introduces a real problem.

00:03:02.555 --> 00:03:04.180
How do you train and,
more importantly,

00:03:04.180 --> 00:03:06.070
evaluate a model when
you don't have access

00:03:06.070 --> 00:03:09.860
to a single unambiguous
ground truth.

00:03:09.860 --> 00:03:11.450
In our approach to
evaluation, rather

00:03:11.450 --> 00:03:13.340
than comparing to a
single ground truth,

00:03:13.340 --> 00:03:16.695
we seek statistical
indistinguishability.

00:03:16.695 --> 00:03:18.070
Intuitively, this
means that if I

00:03:18.070 --> 00:03:20.070
were to show you a bunch
of different estimates,

00:03:20.070 --> 00:03:22.180
some from a machine
and some from humans,

00:03:22.180 --> 00:03:24.330
you wouldn't be able
to tell the difference.

00:03:24.330 --> 00:03:26.330
So, for example, on the
left, what you're seeing

00:03:26.330 --> 00:03:28.600
is our model in red,
which is distinguishable.

00:03:28.600 --> 00:03:30.220
It has a very clear upward bias.

00:03:30.220 --> 00:03:32.793
Whereas on the right, if I
were to remove the colors,

00:03:32.793 --> 00:03:34.210
you wouldn't be
able to tell which

00:03:34.210 --> 00:03:36.010
estimates come from
the algorithm and which

00:03:36.010 --> 00:03:39.780
from the clinician.

00:03:39.780 --> 00:03:41.450
So how do we actually
train a model?

00:03:41.450 --> 00:03:44.120
We use a conventional
encoder-decoder architecture.

00:03:44.120 --> 00:03:46.430
For each frame, we predict
whether the frame quality

00:03:46.430 --> 00:03:49.400
is sufficiently clear
for reliable assessment

00:03:49.400 --> 00:03:51.530
and also produce a
per-pixel segmentation

00:03:51.530 --> 00:03:54.283
of the cardiac chambers.

00:03:54.283 --> 00:03:56.450
Now that each frame in the
video has been segmented,

00:03:56.450 --> 00:04:01.010
we select a single heart cycle
that is of the highest quality.

00:04:01.010 --> 00:04:02.760
Finally, we estimate
the ejection fraction

00:04:02.760 --> 00:04:05.700
using the largest and smallest
areas produced by our model

00:04:05.700 --> 00:04:08.440
in that heart cycle.

00:04:08.440 --> 00:04:10.270
Quantitatively, our
model is statistically

00:04:10.270 --> 00:04:12.790
indistinguishable
from human experts.

00:04:12.790 --> 00:04:14.590
More specifically,
it produces estimates

00:04:14.590 --> 00:04:17.380
that are closer to the average
over all the clinicians

00:04:17.380 --> 00:04:21.560
than any of the individual
experts are to that average.

00:04:21.560 --> 00:04:23.780
Underlying all this
infrastructure is TensorFlow.

00:04:23.780 --> 00:04:27.505
Everything that we do runs
in real time on the device.

00:04:27.505 --> 00:04:28.880
This needs to work
whether you're

00:04:28.880 --> 00:04:32.090
in a subbasement of a hospital
or in a remote jungle.

00:04:32.090 --> 00:04:34.370
We train everything with
TensorFlow and compile

00:04:34.370 --> 00:04:35.930
TensorFlow right
into the app using

00:04:35.930 --> 00:04:37.700
a bunch of custom operations.

00:04:37.700 --> 00:04:40.250
Finally, we also used TF
Serving to improve our labeling

00:04:40.250 --> 00:04:41.420
and monitoring pipelines.

00:04:44.440 --> 00:04:47.082
To summarize, Butterfly has
developed a handheld ultrasound

00:04:47.082 --> 00:04:48.790
device that has put
a high-end ultrasound

00:04:48.790 --> 00:04:52.420
cart, a stenographer, and a
radiologist into your pocket.

00:04:52.420 --> 00:04:55.390
This is already being
used by expert clinicians.

00:04:55.390 --> 00:04:57.370
And by solving the
access problem,

00:04:57.370 --> 00:04:59.050
our use of real-time
machine learning

00:04:59.050 --> 00:05:01.120
is making the
democratization of ultrasound

00:05:01.120 --> 00:05:03.220
a reality around the world.

00:05:03.220 --> 00:05:04.000
Thank you.

00:05:04.000 --> 00:05:05.200
[APPLAUSE]

00:05:05.200 --> 00:05:07.300
[MUSIC PLAYING]

00:05:10.900 --> 00:05:12.150
a

