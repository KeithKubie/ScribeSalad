WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.986
[MUSIC PLAYING]

00:00:03.486 --> 00:00:06.010
ANNA MIYAJIMA: Good
afternoon, everyone.

00:00:06.010 --> 00:00:09.100
Thank you so much for
joining us here today.

00:00:09.100 --> 00:00:11.400
My name is Anna, and
I'm a product manager

00:00:11.400 --> 00:00:13.500
working AR Core.

00:00:13.500 --> 00:00:15.270
In this session,
we'll be talking

00:00:15.270 --> 00:00:18.000
about our faces and
images features,

00:00:18.000 --> 00:00:20.130
walking through
some use case ideas,

00:00:20.130 --> 00:00:24.640
and then showing you
how you can get started.

00:00:24.640 --> 00:00:27.770
AR is all about blending
the physical world

00:00:27.770 --> 00:00:29.630
with the virtual.

00:00:29.630 --> 00:00:32.750
Last year, we
launched AR Core 1.0,

00:00:32.750 --> 00:00:34.340
aiming to help you
build apps that

00:00:34.340 --> 00:00:37.520
can understand the
environment and place objects

00:00:37.520 --> 00:00:39.960
and information in it.

00:00:39.960 --> 00:00:44.070
But to truly blend the
physical and the virtual,

00:00:44.070 --> 00:00:47.410
your phone needs to be able
to augment people, places,

00:00:47.410 --> 00:00:50.630
and things in a realistic way.

00:00:50.630 --> 00:00:52.520
What we're covering
here today is

00:00:52.520 --> 00:00:55.940
a set of capabilities AR
Core provides so you can

00:00:55.940 --> 00:00:59.670
get started doing just that.

00:00:59.670 --> 00:01:02.050
Let's dive right in with faces.

00:01:02.050 --> 00:01:04.790
I'll walk us through why
we built Augmented Faces

00:01:04.790 --> 00:01:06.740
and what exactly it is.

00:01:06.740 --> 00:01:09.320
And then Ranjith, a
tech lead on the team,

00:01:09.320 --> 00:01:11.960
will show you how you can
get started with our APIs.

00:01:14.820 --> 00:01:19.410
So imagine that you come up with
a brilliant idea for a feature.

00:01:19.410 --> 00:01:21.900
And this feature
involves rendering assets

00:01:21.900 --> 00:01:24.330
on top of faces.

00:01:24.330 --> 00:01:27.360
Building out the algorithms in
order to do this from scratch

00:01:27.360 --> 00:01:31.740
is pretty difficult. So you
look around for some options.

00:01:31.740 --> 00:01:35.310
You want to make sure that your
feature works at scale on a lot

00:01:35.310 --> 00:01:38.030
of different types of phones.

00:01:38.030 --> 00:01:40.280
You try out a couple
of the options.

00:01:40.280 --> 00:01:43.310
You're not sure if they work
quite the way you need them to.

00:01:43.310 --> 00:01:48.220
And you also find that
many cost money to use.

00:01:48.220 --> 00:01:52.930
That's why we built
Augmented Faces into AR Core.

00:01:52.930 --> 00:01:56.440
Augmented Faces allows you
to have face capabilities

00:01:56.440 --> 00:02:02.590
in your own app, works at scale,
and is available at no charge.

00:02:02.590 --> 00:02:05.810
And with the improvements
we're announcing here at I/O,

00:02:05.810 --> 00:02:08.785
it will work on over
one billion devices.

00:02:11.870 --> 00:02:16.370
So what exactly is
Augmented Faces?

00:02:16.370 --> 00:02:21.270
Augmented Faces is an API that
provides you with three things.

00:02:21.270 --> 00:02:24.380
First, a center pose
for a face, which

00:02:24.380 --> 00:02:26.900
marks the middle of a head
and is useful when you want

00:02:26.900 --> 00:02:29.530
to do things like render a hat.

00:02:29.530 --> 00:02:32.530
Second, region poses,
which are useful when

00:02:32.530 --> 00:02:36.350
you want to render assets on
a nose or around the ears.

00:02:36.350 --> 00:02:41.110
And last but not
least, a 468 point

00:02:41.110 --> 00:02:44.140
dense 3D face mesh
that allows you

00:02:44.140 --> 00:02:50.160
to paint detailed textures
that accurately follow a face.

00:02:50.160 --> 00:02:54.240
We do all of this,
even the 3D face mesh

00:02:54.240 --> 00:02:59.160
without specialized hardware
by using machine learning.

00:02:59.160 --> 00:03:01.320
This enables Augmented
Faces to work

00:03:01.320 --> 00:03:03.990
at scale, which was
a key requirement we

00:03:03.990 --> 00:03:07.340
heard from you, our developers.

00:03:07.340 --> 00:03:09.860
We're really excited
about this, especially

00:03:09.860 --> 00:03:14.210
because face-based AR unlocks a
wide array of use cases ranging

00:03:14.210 --> 00:03:18.170
from beauty and accessory
try-ons to fun face effects

00:03:18.170 --> 00:03:22.380
that your users can
enjoy with their friends.

00:03:22.380 --> 00:03:25.800
A great example of a developer
that's already started

00:03:25.800 --> 00:03:28.070
using Augmented Faces is Meitu.

00:03:30.800 --> 00:03:34.130
They're known for their widely
popular photo editing apps that

00:03:34.130 --> 00:03:37.000
offer advanced beauty effects.

00:03:37.000 --> 00:03:40.610
They've used our 3D face
mesh to create even more

00:03:40.610 --> 00:03:45.090
innovative effects, and to speed
up their development process.

00:03:45.090 --> 00:03:47.720
The two face effects
that you see here

00:03:47.720 --> 00:03:50.390
will be available
next week on Android

00:03:50.390 --> 00:03:52.010
in their Beauty Plus app.

00:03:54.960 --> 00:03:57.440
If you're physically
here with us at I/O,

00:03:57.440 --> 00:04:00.290
another place that you can
try out Augmented Faces

00:04:00.290 --> 00:04:02.210
is that our Sandbox.

00:04:02.210 --> 00:04:04.760
We've built out this fun
photo booth experience

00:04:04.760 --> 00:04:06.110
that you see here.

00:04:06.110 --> 00:04:08.480
And what you see is
that we're able to trace

00:04:08.480 --> 00:04:11.690
facial features in detail
because of the quality

00:04:11.690 --> 00:04:14.800
of our face mesh.

00:04:14.800 --> 00:04:18.430
Today we're also excited to
announce that we're bringing

00:04:18.430 --> 00:04:22.660
that same face mesh to iOS.

00:04:22.660 --> 00:04:27.160
Since the mesh data structure is
the same across both platforms,

00:04:27.160 --> 00:04:31.870
you'll be able to create one
asset that runs across both iOS

00:04:31.870 --> 00:04:32.620
and Android.

00:04:35.330 --> 00:04:39.590
Just like our Android solution,
Augmented Faces on iOS

00:04:39.590 --> 00:04:41.960
does not require a depth sensor.

00:04:41.960 --> 00:04:46.210
So it will work on
all AR Kit devices.

00:04:46.210 --> 00:04:49.570
Combined with our AR
Core certified devices,

00:04:49.570 --> 00:04:51.880
Augmented Faces
will run on a total

00:04:51.880 --> 00:04:55.510
of over one billion devices.

00:04:55.510 --> 00:04:58.240
Ranjith will be joining
me now so we can show you

00:04:58.240 --> 00:05:01.180
how this looks on an
iPhone 6S, which is

00:05:01.180 --> 00:05:04.820
nearly a four-year-old device.

00:05:04.820 --> 00:05:08.950
So this is an effect that Meitu
has built for their Beauty Plus

00:05:08.950 --> 00:05:11.560
app using our mesh on iOS.

00:05:11.560 --> 00:05:13.150
So what you're
seeing right now is

00:05:13.150 --> 00:05:17.800
that as Ranjith turns his
head, the 3D party hat asset is

00:05:17.800 --> 00:05:20.080
following his head very well.

00:05:20.080 --> 00:05:27.140
And now when Ranjith opens
his mouth, what we get

00:05:27.140 --> 00:05:30.620
is a fun little
birthday surprise.

00:05:30.620 --> 00:05:34.370
This is just one example of
the delightful experiences

00:05:34.370 --> 00:05:36.680
that you can create with AR.

00:05:36.680 --> 00:05:40.350
Our iOS APIs will be
available later this summer.

00:05:40.350 --> 00:05:43.250
And we can't wait to
see what you build.

00:05:43.250 --> 00:05:45.430
Now I'll hand it
over to Ranjith,

00:05:45.430 --> 00:05:47.560
who will be diving more
into the technical details.

00:05:50.772 --> 00:05:52.720
RANJITH KAGATHI
ANANDA: Thank you Anna.

00:05:52.720 --> 00:05:54.170
Hello everyone.

00:05:54.170 --> 00:05:55.540
I'm Ranjith Kagathi.

00:05:55.540 --> 00:05:57.430
I'm a tech leader
in the AR Corps,

00:05:57.430 --> 00:06:01.210
mainly focusing on Augmented
Face as part of the SDK.

00:06:01.210 --> 00:06:04.390
In this section,
I'll explain you

00:06:04.390 --> 00:06:07.680
what are the building blocks
required to create an Android

00:06:07.680 --> 00:06:10.120
app with face effects.

00:06:10.120 --> 00:06:14.320
I'll also give you a sneak
peek under the hood, so what

00:06:14.320 --> 00:06:19.060
goes on in ARCore to enable
this feature for our developers.

00:06:19.060 --> 00:06:21.520
I'll also walk you
through some code snippets

00:06:21.520 --> 00:06:25.150
of a sample application that
implements this face effects.

00:06:29.330 --> 00:06:32.870
We'll mainly discuss
about two face effects.

00:06:32.870 --> 00:06:37.040
One on the left there
is a couple of fox ears

00:06:37.040 --> 00:06:40.710
and a nose attached somewhat
rigidly to the face.

00:06:40.710 --> 00:06:43.610
So when the head
moves, turns, rotates,

00:06:43.610 --> 00:06:46.550
you'll see that the virtual
content moves along with it.

00:06:46.550 --> 00:06:49.650
On the right side
there is a face paint

00:06:49.650 --> 00:06:52.610
overlayed on top of the face.

00:06:52.610 --> 00:06:54.980
And this not only moves
when the head moves,

00:06:54.980 --> 00:06:57.290
but it also
stretches and expands

00:06:57.290 --> 00:07:01.173
as the person opens the mouth,
makes some facial expressions

00:07:01.173 --> 00:07:01.715
and gestures.

00:07:05.240 --> 00:07:08.230
What are all the
building blocks required?

00:07:08.230 --> 00:07:13.470
We need virtual container,
we need real images

00:07:13.470 --> 00:07:18.000
with human faces, and we
need metadata about the face,

00:07:18.000 --> 00:07:21.533
as in where the face is
looking at, how is it oriented?

00:07:21.533 --> 00:07:22.950
And what are the
attachment points

00:07:22.950 --> 00:07:26.560
on the face where we can
attach this virtual content?

00:07:26.560 --> 00:07:29.670
We also need a way to
blend all these together

00:07:29.670 --> 00:07:33.390
into a composite image that
you can show it to the user.

00:07:33.390 --> 00:07:34.200
But don't worry.

00:07:34.200 --> 00:07:37.590
You don't have to build
all these blocks yourself.

00:07:37.590 --> 00:07:39.960
We will provide SDKs
to make it easier

00:07:39.960 --> 00:07:41.410
for you to develop such an app.

00:07:44.360 --> 00:07:46.520
As an app developer,
you'll be mostly

00:07:46.520 --> 00:07:50.330
responsible on creating
the experiences to create

00:07:50.330 --> 00:07:53.750
the virtual content.

00:07:53.750 --> 00:07:58.430
In ARCore we
configure and control

00:07:58.430 --> 00:08:03.590
camera to provide good quality
images for this use case.

00:08:03.590 --> 00:08:06.590
The ARCore also
processes these images

00:08:06.590 --> 00:08:10.100
to output metadata information.

00:08:10.100 --> 00:08:13.010
In this case a region
pose, one on the nose

00:08:13.010 --> 00:08:15.500
and two on the-- either
side of the forehead.

00:08:15.500 --> 00:08:19.870
And also we provide a center
pose, a [INAUDIBLE] information

00:08:19.870 --> 00:08:24.370
in the form of pose.

00:08:24.370 --> 00:08:28.610
We also provide you
a scene from SDK.

00:08:28.610 --> 00:08:32.270
This makes it straightforward
to render realistic 3D scenes

00:08:32.270 --> 00:08:36.080
in both AR and non-AR
cases without having

00:08:36.080 --> 00:08:39.740
to go through the complex
case of low-level open GL API.

00:08:42.460 --> 00:08:47.790
This is one rendering approach
to render these effects.

00:08:47.790 --> 00:08:52.470
You could as well use
Unity or Unreal engines

00:08:52.470 --> 00:08:54.120
if you are interested in so.

00:08:54.120 --> 00:08:59.720
Augmented faces supports both
Unity and Unreal on Android.

00:08:59.720 --> 00:09:03.410
Similarly, if you are
interested in developing a face

00:09:03.410 --> 00:09:09.020
mask or a face tattoo we
provide you a dense 3D face

00:09:09.020 --> 00:09:14.130
mesh onto which you can overlay
the texture information.

00:09:14.130 --> 00:09:16.040
The face mesh is
designed in such a way

00:09:16.040 --> 00:09:20.030
that more vertices are assigned
to the parts of the face which

00:09:20.030 --> 00:09:24.710
had a feature-rich like lips,
eyes, and just off the nose.

00:09:24.710 --> 00:09:27.280
And vertices are
sparsely distributed

00:09:27.280 --> 00:09:33.320
on less feature-rich areas
like foreheads and chin.

00:09:33.320 --> 00:09:35.240
So each of the
vertex will provide

00:09:35.240 --> 00:09:38.090
a x, y, z, the coordinates,
and also give you

00:09:38.090 --> 00:09:42.230
a u and v texture coordinate
for associating a texture.

00:09:42.230 --> 00:09:45.110
We also give you a
three-axis normals

00:09:45.110 --> 00:09:48.140
that you could use it
to apply any lighting

00:09:48.140 --> 00:09:53.710
effects on the face, or even
to smooth out the face effect.

00:09:53.710 --> 00:10:00.280
In summary the ARCore provides
region poses and face mesh.

00:10:00.280 --> 00:10:03.310
App developer creates
the virtual content.

00:10:03.310 --> 00:10:07.270
And we have scene form Unity
and Unreal to combine all these

00:10:07.270 --> 00:10:08.920
into a composite image.

00:10:08.920 --> 00:10:10.450
How does it all work?

00:10:10.450 --> 00:10:15.730
How is ARCore able to provide
a 3D face mesh from a 2D image

00:10:15.730 --> 00:10:18.850
without any depth hardware?

00:10:18.850 --> 00:10:21.430
We use machine
learning models that

00:10:21.430 --> 00:10:26.000
are built on top of TensorFlow
Lite platform to achieve this.

00:10:26.000 --> 00:10:29.020
And the crossing
pipeline is optimized

00:10:29.020 --> 00:10:31.130
to run on device in real time.

00:10:34.040 --> 00:10:35.930
We use a technique
called transfer

00:10:35.930 --> 00:10:41.780
learning wherein we train
the neural network for two

00:10:41.780 --> 00:10:44.900
objectives, one, to
predict 3D vertices

00:10:44.900 --> 00:10:47.840
and to predict 2D contours.

00:10:47.840 --> 00:10:53.450
To predict 3D vertices, we train
it with synthetic 3D data set,

00:10:53.450 --> 00:10:56.180
and use this neural
network as a starting point

00:10:56.180 --> 00:10:58.070
for the next stage of training.

00:10:58.070 --> 00:11:04.040
In this next stage, we
use annotated data set,

00:11:04.040 --> 00:11:09.710
annotated real world data set to
train the model for 2D contour

00:11:09.710 --> 00:11:11.470
prediction.

00:11:11.470 --> 00:11:15.920
The resulting network not
only predicts 3D vertices

00:11:15.920 --> 00:11:18.680
from a synthetic data
set, but can also

00:11:18.680 --> 00:11:20.820
perform well from 2D images.

00:11:24.880 --> 00:11:29.500
We want to make sure the
solution works for everyone.

00:11:29.500 --> 00:11:33.640
So we train the network with
geographically diverse data

00:11:33.640 --> 00:11:35.880
sets so that it works
for all types of faces,

00:11:35.880 --> 00:11:38.970
wider faces, taller faces,
and all types of skin colors.

00:11:41.930 --> 00:11:45.280
So we have 3D face mesh.

00:11:45.280 --> 00:11:48.860
But where do we really need
to really 3D face mesh?

00:11:48.860 --> 00:11:52.220
Take a look at this picture
of my colleague Lewis.

00:11:52.220 --> 00:11:56.360
What's special about this image
is Lewis does not wear glasses.

00:11:56.360 --> 00:11:59.660
So what you see there
is virtual glasses.

00:11:59.660 --> 00:12:02.570
And see how accurately
it tracks the face?

00:12:02.570 --> 00:12:06.800
Even the sides of
the glasses frame

00:12:06.800 --> 00:12:09.590
tracks the head
movement very well.

00:12:09.590 --> 00:12:15.260
It looks so realistic, I have to
cycle through different colors

00:12:15.260 --> 00:12:19.445
to convey that it is indeed
virtual glass and not real.

00:12:19.445 --> 00:12:24.520
Now let me freeze the video when
the face is an extreme angle.

00:12:24.520 --> 00:12:30.945
Notice how a part of the virtual
glasses is behind the nose.

00:12:30.945 --> 00:12:32.320
To achieve this,
you need to have

00:12:32.320 --> 00:12:35.800
3D information about the face,
that very space between nose

00:12:35.800 --> 00:12:37.580
and the cheek where
you can actually

00:12:37.580 --> 00:12:40.710
render the virtual image.

00:12:40.710 --> 00:12:45.370
So this is what ARCore provides.

00:12:45.370 --> 00:12:54.170
And to enable these complex
algorithms on mobile devices,

00:12:54.170 --> 00:12:57.530
we have multiple
adaptive algorithms

00:12:57.530 --> 00:12:59.150
built into the ARCore.

00:12:59.150 --> 00:13:03.650
These algorithms sense
dynamically how much time

00:13:03.650 --> 00:13:06.110
it has taken to process
previous images,

00:13:06.110 --> 00:13:11.210
and adjust accordingly various
parameters of the pipeline.

00:13:11.210 --> 00:13:13.880
We use multiple ML
models, one optimized

00:13:13.880 --> 00:13:17.360
for higher quality
and one optimized

00:13:17.360 --> 00:13:21.560
for higher performance when
the compute resources are

00:13:21.560 --> 00:13:22.980
really challenging.

00:13:22.980 --> 00:13:25.700
We also adjust
pipeline parameters

00:13:25.700 --> 00:13:32.160
such as inference rates so
that we skip a few images,

00:13:32.160 --> 00:13:36.800
and instead replace that
with interpolation data.

00:13:36.800 --> 00:13:38.480
With all these
techniques, what you get

00:13:38.480 --> 00:13:41.570
is a full frame rate
experience for your user.

00:13:41.570 --> 00:13:45.260
So we provide face mesh and
region poses at full camera

00:13:45.260 --> 00:13:48.690
frame rate, while handling
all these techniques internal

00:13:48.690 --> 00:13:51.250
to the ARCore.

00:13:51.250 --> 00:13:53.290
With this background, we
are now ready to build

00:13:53.290 --> 00:13:55.430
the app in three steps.

00:13:55.430 --> 00:13:59.050
Step one, create
virtual content,

00:13:59.050 --> 00:14:05.620
step two, configure ARCore,
step three, configure scene form

00:14:05.620 --> 00:14:09.580
or other rendering
engines to build your app.

00:14:09.580 --> 00:14:14.050
To create virtual content, we
provide economical face mesh

00:14:14.050 --> 00:14:17.860
in an FBX file
format in our SDK.

00:14:17.860 --> 00:14:21.130
So you don't load the
FBX file, import it

00:14:21.130 --> 00:14:24.550
into your 3D
software, and create

00:14:24.550 --> 00:14:26.200
your own virtual content.

00:14:26.200 --> 00:14:29.830
Or you could even reuse
your existing content

00:14:29.830 --> 00:14:33.760
and attach it to the
canonical face mesh.

00:14:33.760 --> 00:14:37.180
So this is the step that
decides which parts of the face

00:14:37.180 --> 00:14:41.650
should control
what virtual asset.

00:14:41.650 --> 00:14:46.330
Similarly, to build
2D texture content,

00:14:46.330 --> 00:14:48.940
we provide a template texture.

00:14:48.940 --> 00:14:50.830
We listened to you developers.

00:14:50.830 --> 00:14:53.350
Recently when, we
launched Augmented Faces,

00:14:53.350 --> 00:14:55.720
developers expressed
interest that they

00:14:55.720 --> 00:14:58.330
want to know which vertices
corresponds to what

00:14:58.330 --> 00:14:59.660
parts of the face.

00:14:59.660 --> 00:15:02.350
So in our latest
SDKs, we are going

00:15:02.350 --> 00:15:04.810
to include this
template texture, which

00:15:04.810 --> 00:15:07.330
is a flattened version
of the face mesh.

00:15:07.330 --> 00:15:10.090
And various regions
of the face is being

00:15:10.090 --> 00:15:13.570
marked with these guidelines.

00:15:13.570 --> 00:15:19.010
You import this template
texture as a layer in your image

00:15:19.010 --> 00:15:22.880
editing software and apply
your own face paint effect

00:15:22.880 --> 00:15:25.140
as another layer on top of it.

00:15:25.140 --> 00:15:29.390
You can adjust where the tattoo
or a face paint should appear.

00:15:29.390 --> 00:15:33.350
And you export it as a PNG file
by hiding the template texture

00:15:33.350 --> 00:15:36.360
and only exporting the
content you created.

00:15:36.360 --> 00:15:38.650
Now that we have
virtual content,

00:15:38.650 --> 00:15:46.010
it's time to write code and
use that virtual content.

00:15:46.010 --> 00:15:47.900
First, we will
configure the ARCore.

00:15:47.900 --> 00:15:50.420
The ARCore by
default is configured

00:15:50.420 --> 00:15:55.580
to run plane finding on
a rear-facing cameras.

00:15:55.580 --> 00:15:57.350
We will [INAUDIBLE]
the configuration

00:15:57.350 --> 00:16:00.530
using this step to configure
it for front camera

00:16:00.530 --> 00:16:04.980
and to output 3D face mesh.

00:16:04.980 --> 00:16:06.560
This is an indication
to the ARCore

00:16:06.560 --> 00:16:11.180
that it should run the
model inferences to generate

00:16:11.180 --> 00:16:13.370
face mesh every frame.

00:16:16.200 --> 00:16:19.590
Now let's move to
the step three.

00:16:19.590 --> 00:16:25.160
In your main app activity
in onCreate method,

00:16:25.160 --> 00:16:27.990
import the virtual
content you created.

00:16:27.990 --> 00:16:38.400
This is a scene from construct
to load 3D Contain into scene

00:16:38.400 --> 00:16:39.690
form.

00:16:39.690 --> 00:16:43.380
I'm making an assumption here
that you already imported this

00:16:43.380 --> 00:16:44.730
into Android Studio.

00:16:44.730 --> 00:16:48.720
And the resources available
in resources folder.

00:16:48.720 --> 00:16:53.310
Similarly, you can import
the 2D texture you design

00:16:53.310 --> 00:16:55.980
into the same form using
the texture builder.

00:16:59.380 --> 00:17:02.760
Now let's create a listener
that listens to the updates

00:17:02.760 --> 00:17:05.490
from ARCore every frame.

00:17:05.490 --> 00:17:08.040
As I mentioned,
Augmented Faces provides

00:17:08.040 --> 00:17:09.720
information about vertices.

00:17:09.720 --> 00:17:13.829
Vertices include texture
coordinates, x, y,

00:17:13.829 --> 00:17:16.829
z components, and normals,
as well as region poses.

00:17:16.829 --> 00:17:20.609
But you don't have to work with
all these components yourself.

00:17:20.609 --> 00:17:23.310
This is where the new
additions to scene form

00:17:23.310 --> 00:17:29.160
comes from, where it supports
natively Augmented Face node.

00:17:29.160 --> 00:17:31.320
So you'll create the
Augmented Face node

00:17:31.320 --> 00:17:37.590
off scene form using the
ARCore's Augmented Faces API.

00:17:37.590 --> 00:17:40.870
Now the number of lines
of coding you are to do

00:17:40.870 --> 00:17:42.570
is very few.

00:17:42.570 --> 00:17:47.897
On such a face node, you set
3D asset under 2D texture.

00:17:47.897 --> 00:17:48.480
And that's it.

00:17:48.480 --> 00:17:50.940
With three simple steps, ta-da!

00:17:50.940 --> 00:17:56.070
You have a Android
app with face effects.

00:17:56.070 --> 00:18:01.260
So everything we discuss now
is already available on Android

00:18:01.260 --> 00:18:02.340
on our developer page.

00:18:02.340 --> 00:18:05.800
You can download the
SDK and build your apps.

00:18:05.800 --> 00:18:07.890
This was about Augmented Faces.

00:18:07.890 --> 00:18:09.930
Now I will hand
it over to Ashish

00:18:09.930 --> 00:18:12.211
to talk about Augmented Images.

00:18:12.211 --> 00:18:15.097
ASHISH SHAH: Thank you, Ranjith.

00:18:15.097 --> 00:18:17.983
[APPLAUSE]

00:18:20.870 --> 00:18:22.150
Thank you, Ranjith.

00:18:22.150 --> 00:18:24.020
Good afternoon, everyone.

00:18:24.020 --> 00:18:27.500
I'm glad so many of you
could join us today.

00:18:27.500 --> 00:18:28.330
My name is Ashish.

00:18:28.330 --> 00:18:30.400
I'm a product manager at Google.

00:18:30.400 --> 00:18:32.740
And I work on a ARCore.

00:18:32.740 --> 00:18:37.530
I'll be specifically talking
about Augmented Images.

00:18:37.530 --> 00:18:40.530
For the next 15 to
20 minutes, we're

00:18:40.530 --> 00:18:42.460
going to go through four topics.

00:18:42.460 --> 00:18:45.870
The first one is the rationale
behind Augmented Images.

00:18:45.870 --> 00:18:47.980
Why did we design
Augmented Images?

00:18:47.980 --> 00:18:49.740
What's it good for?

00:18:49.740 --> 00:18:52.850
Number two, we will talk about
how Augmented Images works.

00:18:52.850 --> 00:18:54.600
And we're going to
specifically talk about

00:18:54.600 --> 00:18:58.410
this in the context of two apps
that we are showcasing here

00:18:58.410 --> 00:19:01.643
at I/O. Number three, we
will talk about a few updates

00:19:01.643 --> 00:19:03.810
that we are doing, a few
significant updates that we

00:19:03.810 --> 00:19:07.340
are doing to ARCore as
part of the ARCore launch

00:19:07.340 --> 00:19:10.300
to Augmented Images as
part of the ARCore launch.

00:19:10.300 --> 00:19:14.320
And lastly, we to talk about
how do you, as developers, take

00:19:14.320 --> 00:19:17.250
all of this and put Augmented
Images into your own app?

00:19:20.450 --> 00:19:21.970
So let's start.

00:19:21.970 --> 00:19:25.960
Why Augmented Images
in the first place?

00:19:25.960 --> 00:19:30.620
So today, people frequently
interact with physical images,

00:19:30.620 --> 00:19:32.390
whether they are
at a store, they're

00:19:32.390 --> 00:19:35.120
looking at some product,
reading the product signage

00:19:35.120 --> 00:19:37.640
to understand more
about the product.

00:19:37.640 --> 00:19:39.410
Or they could be
at a movie theater

00:19:39.410 --> 00:19:41.202
and they could be
looking at a movie poster

00:19:41.202 --> 00:19:43.010
to learn more about the movie.

00:19:43.010 --> 00:19:45.740
Or they could be at a
venue like Google I/O

00:19:45.740 --> 00:19:48.080
where it's actually loud
and they're kind of lost.

00:19:48.080 --> 00:19:50.390
And they're looking at
a map, a physical map,

00:19:50.390 --> 00:19:52.670
to figure out where they are.

00:19:52.670 --> 00:19:55.317
Our goal with
Augmented Images is

00:19:55.317 --> 00:19:56.900
to take these
interactions that people

00:19:56.900 --> 00:20:00.950
have with physical images and
make them a lot more engaging

00:20:00.950 --> 00:20:05.970
and make them a lot more useful
to the users and context.

00:20:05.970 --> 00:20:08.180
We're showcasing one
such example here

00:20:08.180 --> 00:20:11.810
at the Sandbox right
across this corner.

00:20:11.810 --> 00:20:14.840
Here, users would
be able to scan

00:20:14.840 --> 00:20:18.570
product signage, in this
case, of an espresso machine.

00:20:18.570 --> 00:20:21.320
And as soon as they scan
the product signage,

00:20:21.320 --> 00:20:24.980
you would see 3D overlays
of product features

00:20:24.980 --> 00:20:27.750
right on top of the object.

00:20:27.750 --> 00:20:31.310
This is a super powerful
way for manufacturers

00:20:31.310 --> 00:20:34.340
of these appliances to convey
their product benefits,

00:20:34.340 --> 00:20:36.260
to convey their product
features in a way

00:20:36.260 --> 00:20:40.170
that they could otherwise not
do with just plain 2D text or 2D

00:20:40.170 --> 00:20:42.240
signage.

00:20:42.240 --> 00:20:45.300
And as I had mentioned
earlier, the most powerfully

00:20:45.300 --> 00:20:50.370
our experiences are the ones
that are able to take the real

00:20:50.370 --> 00:20:53.520
and take the physical and blend
them together realistically.

00:20:53.520 --> 00:20:56.490
This is an example where you're
able to take virtual content

00:20:56.490 --> 00:20:58.380
and blend it on
top of a real world

00:20:58.380 --> 00:21:00.290
object in the right context.

00:21:03.040 --> 00:21:05.380
So how does Augmented
Images work?

00:21:05.380 --> 00:21:07.260
Let's take a look.

00:21:07.260 --> 00:21:10.510
At a high level, Augmented
Images does three things.

00:21:10.510 --> 00:21:14.190
The first one is it's able
to recognize a pre-registered

00:21:14.190 --> 00:21:16.180
image.

00:21:16.180 --> 00:21:18.900
So in this case of the espresso
machine demo that we just

00:21:18.900 --> 00:21:23.460
talked about, the app developer
has registered the signage

00:21:23.460 --> 00:21:27.550
as an augmented
image inside the app.

00:21:27.550 --> 00:21:32.340
Second, when the users
scan the augmented image,

00:21:32.340 --> 00:21:34.560
Augmented Images will
give you a six degrees

00:21:34.560 --> 00:21:40.230
of freedom pose of the image
with respect to the image.

00:21:40.230 --> 00:21:42.100
What does that mean?

00:21:42.100 --> 00:21:45.540
So for example, if this was
the image and this is a device,

00:21:45.540 --> 00:21:47.460
once I'm done scanning
my image, what

00:21:47.460 --> 00:21:50.790
it tells me is the position
of my device with respect

00:21:50.790 --> 00:21:56.340
to the image in
x, in y, and in z.

00:21:56.340 --> 00:22:01.500
It also tells me the orientation
with respect to the image

00:22:01.500 --> 00:22:04.920
in pitch, yaw and roll.

00:22:04.920 --> 00:22:08.310
So with these six numbers, the
x, y, and z, and the pitch,

00:22:08.310 --> 00:22:10.170
yaw, roll, you are
able to describe

00:22:10.170 --> 00:22:13.590
the position and orientation
of the device or the image

00:22:13.590 --> 00:22:14.580
accurately.

00:22:14.580 --> 00:22:17.310
And this is what allows you
to go to the third step, which

00:22:17.310 --> 00:22:21.420
is to take this information
and augment 3D content on top

00:22:21.420 --> 00:22:22.170
of it.

00:22:22.170 --> 00:22:24.420
And when you augment 3D
content on top of the image

00:22:24.420 --> 00:22:26.520
or offset with the
image, the content

00:22:26.520 --> 00:22:30.930
would stay there, regardless of
how you move it on your device.

00:22:30.930 --> 00:22:33.870
And that creates this illusion
of the virtual content

00:22:33.870 --> 00:22:35.390
being stuck to the real world.

00:22:38.540 --> 00:22:42.090
So as you might have heard
from Sundar during the keynote,

00:22:42.090 --> 00:22:45.110
we've launched this experience
called Explore I/O as part

00:22:45.110 --> 00:22:47.210
of the I/O 2019 app.

00:22:47.210 --> 00:22:52.580
This allows the attendees to
go and scan physical signage

00:22:52.580 --> 00:22:54.050
that you might have seen around.

00:22:54.050 --> 00:22:55.790
And as soon as you
scan physical signage,

00:22:55.790 --> 00:22:58.610
you are able to see what's
around the signage in terms

00:22:58.610 --> 00:22:59.510
of landmarks.

00:22:59.510 --> 00:23:04.290
So where is the nearest Sandbox
for demos or where are the code

00:23:04.290 --> 00:23:04.790
labs?

00:23:04.790 --> 00:23:07.550
You should be able to just
pan your camera on your phone

00:23:07.550 --> 00:23:09.460
and be able to see that.

00:23:09.460 --> 00:23:11.540
Let's deconstruct
this experience

00:23:11.540 --> 00:23:13.490
to figure out how
Augmented Images is

00:23:13.490 --> 00:23:16.180
being leveraged in here
to power this experience.

00:23:19.680 --> 00:23:21.210
The first thing
we have is that we

00:23:21.210 --> 00:23:24.180
have 50 such unique
signposts which

00:23:24.180 --> 00:23:28.500
are placed at very, very precise
locations all across the Google

00:23:28.500 --> 00:23:30.550
I/O venue.

00:23:30.550 --> 00:23:33.310
And you have these
landmarks-- just because we've

00:23:33.310 --> 00:23:37.300
designed a venue, we know where
exactly each of these landmarks

00:23:37.300 --> 00:23:39.430
are with respect
to the signposts.

00:23:39.430 --> 00:23:44.770
For example, I know that there
are signposts right outside.

00:23:44.770 --> 00:23:47.950
And the session stage
number six is right here.

00:23:47.950 --> 00:23:50.080
So the signposts
and the landmarks

00:23:50.080 --> 00:23:52.104
maintain a common
frame of reference.

00:23:54.710 --> 00:23:59.380
Now when a user goes ahead
and scans a signpost,

00:23:59.380 --> 00:24:02.230
what Augmented Images
does it provide the six

00:24:02.230 --> 00:24:05.380
degrees of freedom pose
of the device with respect

00:24:05.380 --> 00:24:06.850
to the augmented image.

00:24:06.850 --> 00:24:10.570
So now you combine the position
of the device with respect

00:24:10.570 --> 00:24:14.590
to the image and the
information about the landmarks

00:24:14.590 --> 00:24:16.510
with respect to
the image, and what

00:24:16.510 --> 00:24:20.650
you have is the information
about where the landmarks are

00:24:20.650 --> 00:24:22.100
with respect to the device.

00:24:22.100 --> 00:24:25.960
This is what allows you to
overlay content right on top

00:24:25.960 --> 00:24:27.200
of the physical world.

00:24:27.200 --> 00:24:28.680
And as you move
your device around,

00:24:28.680 --> 00:24:31.300
you are able to see this
content stay in place.

00:24:36.520 --> 00:24:40.050
We are also excited to announce
several significant updates

00:24:40.050 --> 00:24:44.270
to Augmented Images as part
of the ARCore release I/O.

00:24:44.270 --> 00:24:47.250
And a lot of this has been
in response to feedback

00:24:47.250 --> 00:24:49.467
that we heard from
you developers.

00:24:53.222 --> 00:24:55.180
If you had a couple of
algorithmic improvements

00:24:55.180 --> 00:24:57.880
under the hood, which lead
to much higher tracking

00:24:57.880 --> 00:25:00.700
precision and much
higher detection recall.

00:25:00.700 --> 00:25:03.440
What does that mean
for developers?

00:25:03.440 --> 00:25:05.980
So when you start
anchoring virtual content

00:25:05.980 --> 00:25:08.530
on top of real physical
objects, that's

00:25:08.530 --> 00:25:11.350
when your margin of error,
in terms of how much can

00:25:11.350 --> 00:25:14.170
the content shift around
while you are tracking.

00:25:14.170 --> 00:25:16.910
The margin of error that
is actually super small.

00:25:16.910 --> 00:25:19.030
And so the improvements
that we made

00:25:19.030 --> 00:25:20.590
in the past several
quarters will

00:25:20.590 --> 00:25:22.840
allow you to now start
annotating real world

00:25:22.840 --> 00:25:26.290
physical objects, and give your
users an illusion that it's

00:25:26.290 --> 00:25:27.940
actually there.

00:25:27.940 --> 00:25:30.600
We also now allow you
to track multiple images

00:25:30.600 --> 00:25:32.620
in the scene, not
just one image,

00:25:32.620 --> 00:25:34.600
but multiple images
in the scene.

00:25:34.600 --> 00:25:37.330
And lastly-- this part
I'm most excited about--

00:25:37.330 --> 00:25:40.600
is the ability to track images
that are not static, but can

00:25:40.600 --> 00:25:41.660
move around.

00:25:41.660 --> 00:25:42.910
And we continue to track them.

00:25:46.620 --> 00:25:48.780
Let's look at that for a bit.

00:25:48.780 --> 00:25:51.390
In previously ARCore
versions, ARCore

00:25:51.390 --> 00:25:55.440
used to give you back a
pose of the image at 2

00:25:55.440 --> 00:25:59.250
Hertz, so roughly every
two times a second.

00:25:59.250 --> 00:26:02.280
And that means that if the
image was to move around

00:26:02.280 --> 00:26:04.530
while the session is
going on, it is possible

00:26:04.530 --> 00:26:08.520
that the virtual content would
lag because the images are not

00:26:08.520 --> 00:26:10.620
being tracked continuously.

00:26:10.620 --> 00:26:13.590
So what we have done
is now the image poses

00:26:13.590 --> 00:26:16.050
are being given at 30 Hertz.

00:26:16.050 --> 00:26:18.570
So we've increased the
frequency at which we give you

00:26:18.570 --> 00:26:21.990
updates by 15x, but
at the same time,

00:26:21.990 --> 00:26:24.270
we made sure that the power
consumption does not rise.

00:26:24.270 --> 00:26:27.210
So the overall power consumption
rises by less than 3%

00:26:27.210 --> 00:26:30.450
for a 15x increase in the
frequency of the poses that

00:26:30.450 --> 00:26:31.500
deployed.

00:26:31.500 --> 00:26:34.680
And we also realize that
as your images move around,

00:26:34.680 --> 00:26:37.040
they might go out of your
camera field of view.

00:26:37.040 --> 00:26:40.490
And with the higher
detection recall,

00:26:40.490 --> 00:26:44.110
we're able to help
with that as well.

00:26:44.110 --> 00:26:46.740
And we're excited with what
some of our early developers

00:26:46.740 --> 00:26:50.550
are doing with moving
augmented images.

00:26:50.550 --> 00:26:52.950
JD.com is an e-commerce company.

00:26:52.950 --> 00:26:56.010
And they've come up with
an app called JD AR Books.

00:26:56.010 --> 00:26:58.530
This is an educational app
for really young children

00:26:58.530 --> 00:27:02.140
that teaches them how to
spell simple English words.

00:27:02.140 --> 00:27:05.070
So as you scan a letter C, you
would see an augmentation come

00:27:05.070 --> 00:27:09.138
up on top of letter
C. And as you move C,

00:27:09.138 --> 00:27:10.680
the augmentation
moves along with it.

00:27:10.680 --> 00:27:14.020
And it's only when you place
all the three letters together

00:27:14.020 --> 00:27:16.020
that an achievement is
unlocked and you actually

00:27:16.020 --> 00:27:17.860
see a virtual car in this case.

00:27:17.860 --> 00:27:19.360
If you were to
misplace the letters,

00:27:19.360 --> 00:27:23.173
you would actually not
see that happening.

00:27:23.173 --> 00:27:24.840
As you've seen in
most of these examples

00:27:24.840 --> 00:27:27.510
so far, what we have
done in ARCore core

00:27:27.510 --> 00:27:32.910
is take the real world and add
more information on top of it.

00:27:32.910 --> 00:27:35.370
But ARCore can
also do more things

00:27:35.370 --> 00:27:37.680
in terms of being able
to make you visualize

00:27:37.680 --> 00:27:41.670
the world in a different
way, not just add new stuff,

00:27:41.670 --> 00:27:44.590
but be able to visualize
it in a different way.

00:27:44.590 --> 00:27:47.220
So if you know US
currency traditionally

00:27:47.220 --> 00:27:50.700
does not feature faces of women.

00:27:50.700 --> 00:27:57.250
Most of the faces on US currency
is today of prominent men.

00:27:57.250 --> 00:27:59.720
A former US
Treasurer, Rosie Rios

00:27:59.720 --> 00:28:03.920
has partnered with the
Google Creative Lab,

00:28:03.920 --> 00:28:07.610
they've created this AR
experience call Notable Women.

00:28:07.610 --> 00:28:10.700
This allows users
to take US currency

00:28:10.700 --> 00:28:13.640
and be able to replace
the faces on US currency

00:28:13.640 --> 00:28:19.160
with the faces of women who have
made a tremendous impact on US.

00:28:19.160 --> 00:28:24.760
And this helps people learn more
about these remarkable women.

00:28:28.100 --> 00:28:30.440
Hopefully, all of these
examples have sparked some ideas

00:28:30.440 --> 00:28:34.130
in terms of what all you can
do with Augmented Images.

00:28:34.130 --> 00:28:35.630
Now we'll spend the
next few minutes

00:28:35.630 --> 00:28:38.720
talking about how do you
actually take Augmented Images

00:28:38.720 --> 00:28:40.290
and put it as part
of your own app.

00:28:44.600 --> 00:28:46.450
There are three
steps at a high level

00:28:46.450 --> 00:28:48.550
to create an
Augmented Images app.

00:28:48.550 --> 00:28:52.900
The first one is to create a
database of images that will be

00:28:52.900 --> 00:28:55.630
used to trigger the experience.

00:28:55.630 --> 00:28:59.830
Number two, is to create and
configure an ARCore session

00:28:59.830 --> 00:29:02.160
to use Augmented Images.

00:29:02.160 --> 00:29:05.840
And finally, at runtime,
you query the camera

00:29:05.840 --> 00:29:08.900
at every frame at 30
Hertz and figure out

00:29:08.900 --> 00:29:11.240
what is a pose of the
image that you are tracking

00:29:11.240 --> 00:29:13.188
and render content on top of it.

00:29:13.188 --> 00:29:14.730
Let's go through
each of these steps.

00:29:17.970 --> 00:29:21.210
Here in the first step, you'll
identify all the set of images

00:29:21.210 --> 00:29:24.060
that you would want to
trigger experiences of.

00:29:24.060 --> 00:29:28.380
Today we allow you to create a
database of up to 1,000 images,

00:29:28.380 --> 00:29:32.400
1,000 unique images that your
users can scan and trigger

00:29:32.400 --> 00:29:33.600
your experience.

00:29:33.600 --> 00:29:35.880
We provide you an
offline tool that you can

00:29:35.880 --> 00:29:37.980
use to generate this database.

00:29:37.980 --> 00:29:41.130
We also provide you a way to
do this at runtime in your app

00:29:41.130 --> 00:29:41.880
as well.

00:29:41.880 --> 00:29:44.300
For the purpose of
the code snippets that

00:29:44.300 --> 00:29:46.350
are going to follow,
we're going to assume

00:29:46.350 --> 00:29:48.630
that an Augmented Images
database has already

00:29:48.630 --> 00:29:49.750
been created offline.

00:29:52.590 --> 00:29:56.040
The second thing
with any ARCore app

00:29:56.040 --> 00:29:58.920
is to create and configure
any ARCore session

00:29:58.920 --> 00:30:00.240
to do a particular thing.

00:30:00.240 --> 00:30:03.220
As Ranjith did so in the
case of Augmented Faces,

00:30:03.220 --> 00:30:05.580
we'll do so now in the
case of Augmented Images.

00:30:05.580 --> 00:30:07.440
You create an ARCore session.

00:30:07.440 --> 00:30:09.540
And then you take this
Augmented Image database

00:30:09.540 --> 00:30:11.190
that you had created earlier.

00:30:11.190 --> 00:30:14.160
You decode it,
and you pass along

00:30:14.160 --> 00:30:16.860
the decoded database
as a parameter

00:30:16.860 --> 00:30:19.270
and do ARCore config session.

00:30:19.270 --> 00:30:21.660
With these simple
code lines, you

00:30:21.660 --> 00:30:26.490
should be able to create and
configure an ARCore session.

00:30:26.490 --> 00:30:28.080
Then you move along
to at runtime.

00:30:28.080 --> 00:30:30.690
What you do is
you start creating

00:30:30.690 --> 00:30:33.390
the pose of all these
images that you are

00:30:33.390 --> 00:30:36.000
tracking at every camera frame.

00:30:36.000 --> 00:30:38.010
And what ARCore
would return back

00:30:38.010 --> 00:30:40.470
is the pose of
each of the images

00:30:40.470 --> 00:30:43.170
and the tracking status
of each of these images,

00:30:43.170 --> 00:30:44.940
whether or not it's
tracking or not

00:30:44.940 --> 00:30:47.280
tracking the image might
have gone out of view.

00:30:47.280 --> 00:30:49.170
So it will give you
a status update.

00:30:49.170 --> 00:30:52.080
And as long as the
images are tracking,

00:30:52.080 --> 00:30:55.410
you can go ahead and render
physical content right

00:30:55.410 --> 00:30:57.030
on top of these images.

00:30:57.030 --> 00:31:00.000
So in these three
simple steps, you

00:31:00.000 --> 00:31:04.560
should be able to create a
basic Augmented Images app.

00:31:04.560 --> 00:31:06.480
But we'd like you
to go a bit further.

00:31:06.480 --> 00:31:08.355
There are certain things
that we have learned

00:31:08.355 --> 00:31:10.050
from experimenting with this.

00:31:10.050 --> 00:31:13.470
And we call them
our best practices.

00:31:13.470 --> 00:31:16.110
As you know, the
Augmented Images database

00:31:16.110 --> 00:31:18.720
lives and dies by the
quality of the images

00:31:18.720 --> 00:31:21.460
that you'll be
tracking or scanning.

00:31:21.460 --> 00:31:23.460
It is very important that
you choose images that

00:31:23.460 --> 00:31:24.897
are great for your experience.

00:31:24.897 --> 00:31:26.730
And we provide you a
tool that will tell you

00:31:26.730 --> 00:31:29.040
if your image is
good or bad in terms

00:31:29.040 --> 00:31:32.650
of a score for an Augmented
Images experience.

00:31:32.650 --> 00:31:34.950
So what are the images
that qualify as good images

00:31:34.950 --> 00:31:36.750
and will give you a great score?

00:31:36.750 --> 00:31:39.930
The first one is images that
avoid heavy compression.

00:31:39.930 --> 00:31:42.850
We don't want heavily
pixilated images.

00:31:42.850 --> 00:31:46.050
You also want to choose images
that have at least a 300

00:31:46.050 --> 00:31:48.108
by 300 resolution.

00:31:48.108 --> 00:31:49.650
And lastly, you want
to choose images

00:31:49.650 --> 00:31:51.750
which have distinct
non-repetitive features.

00:31:51.750 --> 00:31:56.610
The last point is a bit subtle
and it's lost a lot of times.

00:31:56.610 --> 00:31:59.100
We've got a couple
of examples in here.

00:31:59.100 --> 00:32:01.090
Here you see on
the left hand side,

00:32:01.090 --> 00:32:03.210
you see this image of a globe.

00:32:03.210 --> 00:32:04.800
If you see this
globe, there are lots

00:32:04.800 --> 00:32:08.740
of distinct high, contrast
patterns on this image.

00:32:08.740 --> 00:32:11.470
And there's nothing that
repeats across the image.

00:32:11.470 --> 00:32:14.730
So it's a really good image
to track and to recognize

00:32:14.730 --> 00:32:16.620
as an augmented image.

00:32:16.620 --> 00:32:19.630
But the image on the right hand
side, the one of the building,

00:32:19.630 --> 00:32:22.740
it has lots of features.

00:32:22.740 --> 00:32:25.410
But once you actually
start scanning the image,

00:32:25.410 --> 00:32:28.650
you understand that there are
these lots of repeating windows

00:32:28.650 --> 00:32:30.400
that go across the image.

00:32:30.400 --> 00:32:33.030
And so it is very easy for
the algorithms under the hood

00:32:33.030 --> 00:32:35.340
to get confused as to
what they are looking at.

00:32:35.340 --> 00:32:37.620
And so such an image
would not be idealy suited

00:32:37.620 --> 00:32:39.660
for an Augmented
Images experience.

00:32:39.660 --> 00:32:42.270
And our tool will tell you
that the score of this image

00:32:42.270 --> 00:32:43.080
is super low.

00:32:43.080 --> 00:32:45.330
So if we would highly encourage
that you choose images

00:32:45.330 --> 00:32:52.300
that are closer to the ones
that give you a score of 100.

00:32:52.300 --> 00:32:55.880
Also another important step
is how do you onboard users?

00:32:55.880 --> 00:32:59.050
AR is a very, very new
interactive experience

00:32:59.050 --> 00:33:00.640
for most people.

00:33:00.640 --> 00:33:02.620
And it is important
that you teach users

00:33:02.620 --> 00:33:05.440
how to be successful
in the experience.

00:33:05.440 --> 00:33:08.890
The first step in an
Augmented Images experience

00:33:08.890 --> 00:33:10.580
is for the user
to scan an image.

00:33:10.580 --> 00:33:13.360
So how do you tell
a user that hey, you

00:33:13.360 --> 00:33:15.700
are supposed to scan an image
you start the experience.

00:33:15.700 --> 00:33:18.340
A couple of visual
cues will help.

00:33:18.340 --> 00:33:21.142
In our case, we tend
to use these brackets.

00:33:21.142 --> 00:33:22.600
These virtual
brackets that show up

00:33:22.600 --> 00:33:24.190
on your camera field of view.

00:33:24.190 --> 00:33:27.550
And these steer the users
towards scanning the image.

00:33:27.550 --> 00:33:29.708
A simple text can also help.

00:33:29.708 --> 00:33:31.750
We've seen that if you do
either of these things,

00:33:31.750 --> 00:33:35.560
we've seen a remarkable
uptick in the success ratio

00:33:35.560 --> 00:33:38.830
in terms of how users
perceive your app.

00:33:38.830 --> 00:33:41.740
You also want to make sure
that the users are not just

00:33:41.740 --> 00:33:42.730
scanning tiny images.

00:33:42.730 --> 00:33:44.920
They get closer if
the images are small.

00:33:44.920 --> 00:33:47.320
So make sure you size the
brackets appropriately so

00:33:47.320 --> 00:33:50.380
users are putting in
the images at least 25%

00:33:50.380 --> 00:33:52.170
of the screen space.

00:33:52.170 --> 00:33:54.850
With these code lines
and best practices,

00:33:54.850 --> 00:33:58.180
you should be able to create
a really good Augmented Images

00:33:58.180 --> 00:33:58.870
app.

00:33:58.870 --> 00:34:02.920
And we'd love to know what
you'd be up to with this.

00:34:02.920 --> 00:34:05.458
And we are here to help.

00:34:05.458 --> 00:34:10.000
As part of I/O, we
have several codelabs.

00:34:10.000 --> 00:34:12.520
Augmented Faces has
its own codelab.

00:34:12.520 --> 00:34:14.800
So does Augmented Images.

00:34:14.800 --> 00:34:16.690
In less than 30
to 45 minutes, you

00:34:16.690 --> 00:34:19.780
should be able to create
your first Augmented Image

00:34:19.780 --> 00:34:22.278
or Augmented Face app.

00:34:22.278 --> 00:34:24.820
There's also a Sandbox demo if
you're looking for inspiration

00:34:24.820 --> 00:34:27.219
on what could AR do.

00:34:27.219 --> 00:34:30.500
There's a Sandbox, which is
right across the street here.

00:34:30.500 --> 00:34:35.960
And or dev site,
developers.google.com/AR has

00:34:35.960 --> 00:34:39.159
latest and the most up to
date information on our APIs

00:34:39.159 --> 00:34:40.630
and on our SDK.

00:34:40.630 --> 00:34:43.780
And as for us, myself,
Ranjith, and Anna,

00:34:43.780 --> 00:34:45.969
we'll stick around after
the presentation is done.

00:34:45.969 --> 00:34:49.659
If you folks have any questions
on the code that we presented,

00:34:49.659 --> 00:34:51.257
if you have questions
on use cases,

00:34:51.257 --> 00:34:52.840
or if you're looking
for more features

00:34:52.840 --> 00:34:56.030
and have questions on that,
we'd love to hear from you.

00:34:56.030 --> 00:34:56.800
So please stop by.

00:34:56.800 --> 00:34:58.330
We'll hang around right here.

00:34:58.330 --> 00:35:00.800
Thank you again
all for joining us.

00:35:00.800 --> 00:35:02.680
We really appreciate
it, and hope you all

00:35:02.680 --> 00:35:03.890
have productive I/O.

00:35:03.890 --> 00:35:07.540
[MUSIC PLAYING]

