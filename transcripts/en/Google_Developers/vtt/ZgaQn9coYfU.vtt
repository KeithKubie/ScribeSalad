WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.472
[MUSIC PLAYING]

00:00:08.928 --> 00:00:11.844
[APPLAUSE]

00:00:11.844 --> 00:00:13.260
DIRK PRIMBS: Two
days of learning,

00:00:13.260 --> 00:00:14.880
two days conversation--

00:00:14.880 --> 00:00:16.079
I can't believe it's over.

00:00:16.079 --> 00:00:16.860
Can you?

00:00:16.860 --> 00:00:19.680
It's like it just started.

00:00:19.680 --> 00:00:23.070
And we aspire to create a
conference for all of you

00:00:23.070 --> 00:00:25.740
that is more than just
a technology show.

00:00:25.740 --> 00:00:29.310
We aspired it be European,
to be a place of learning,

00:00:29.310 --> 00:00:32.310
and to be an inclusive place
where you can get inspired

00:00:32.310 --> 00:00:35.940
and have conversations and
just love being a developer,

00:00:35.940 --> 00:00:37.872
around a developer crowd.

00:00:37.872 --> 00:00:39.960
So when you design
something like this,

00:00:39.960 --> 00:00:43.350
there are a couple of things
that you try to get right.

00:00:43.350 --> 00:00:45.760
And then, you wonder if
you really got it right.

00:00:45.760 --> 00:00:49.440
So being a place of learning--
kind of easy, right?

00:00:49.440 --> 00:00:52.320
You ask the audience-- in
that case, you developers--

00:00:52.320 --> 00:00:54.240
hey, what would
you like to hear?

00:00:54.240 --> 00:00:56.130
You invite experts
that give session.

00:00:56.130 --> 00:00:57.990
You put on an agenda.

00:00:57.990 --> 00:01:00.180
Find a place-- check.

00:01:00.180 --> 00:01:02.190
And after the event,
we ask you how we did

00:01:02.190 --> 00:01:05.129
and how we can improve.

00:01:05.129 --> 00:01:07.500
Being an European conference--

00:01:07.500 --> 00:01:09.190
also kind of easy--

00:01:09.190 --> 00:01:11.850
so in this room, we are
actually more than European.

00:01:11.850 --> 00:01:13.170
We are a global conference.

00:01:13.170 --> 00:01:16.980
We have 71 nations sitting in
that room, and more than 2/3

00:01:16.980 --> 00:01:20.220
of you come from
countries outside Poland.

00:01:20.220 --> 00:01:22.774
So this is truly an
international European

00:01:22.774 --> 00:01:24.690
conference, and that was
what I heard from you

00:01:24.690 --> 00:01:27.540
all over the place, that
among good conversations,

00:01:27.540 --> 00:01:30.960
you really like the exchange
with other perspectives,

00:01:30.960 --> 00:01:34.640
other nations, people
from other countries.

00:01:34.640 --> 00:01:39.690
The inclusive, inspiring part is
a tricky piece, because, well,

00:01:39.690 --> 00:01:40.740
how do you test for that?

00:01:40.740 --> 00:01:44.070
There are a million little
things you can do and try,

00:01:44.070 --> 00:01:46.200
and it's not easy to test.

00:01:46.200 --> 00:01:50.950
It's more-- some would say
it's more a soft thing.

00:01:50.950 --> 00:01:52.170
But wait a minute.

00:01:52.170 --> 00:01:54.660
We can ask, right?

00:01:54.660 --> 00:01:56.820
So I can run a
little experiment.

00:01:56.820 --> 00:01:59.130
Maybe you do it with me.

00:01:59.130 --> 00:02:01.640
I am going to raise
my hand, and everybody

00:02:01.640 --> 00:02:05.760
who feels like inspired,
learn something new,

00:02:05.760 --> 00:02:07.590
and being part of an
inclusive conference,

00:02:07.590 --> 00:02:08.639
just starts cheering.

00:02:08.639 --> 00:02:09.360
How about that?

00:02:09.360 --> 00:02:10.299
Let's try it.

00:02:10.299 --> 00:02:13.971
[CHEERING]

00:02:15.350 --> 00:02:17.552
That was like
medium-sized cheer.

00:02:17.552 --> 00:02:20.330
[LAUGHTER]

00:02:20.330 --> 00:02:23.250
And it was kind of expected.

00:02:23.250 --> 00:02:25.550
So if you look
around, and you were

00:02:25.550 --> 00:02:28.850
some in the cheering crowd, you
might have noticed some of you

00:02:28.850 --> 00:02:30.460
didn't cheer.

00:02:30.460 --> 00:02:32.210
Because actually, you
know, we Europeans

00:02:32.210 --> 00:02:33.290
don't cheer that easily.

00:02:33.290 --> 00:02:36.035
Just because some clown on stage
says, cheer, we don't cheer.

00:02:39.710 --> 00:02:44.990
And that's a
well-known stereotype.

00:02:44.990 --> 00:02:47.540
Actually, we as Europeans even
subscribe to that stereotype.

00:02:47.540 --> 00:02:49.640
So non-European think
that Europeans are not

00:02:49.640 --> 00:02:52.310
that easy in cheering,
and Europeans kind of do

00:02:52.310 --> 00:02:54.920
it and believe it themselves.

00:02:54.920 --> 00:02:57.400
And these kind of
stereotypes we know.

00:02:57.400 --> 00:03:00.290
But there are a
million other things

00:03:00.290 --> 00:03:03.770
that are unconsciously
baked into how we

00:03:03.770 --> 00:03:05.150
behave, how we make decisions.

00:03:05.150 --> 00:03:07.640
Now, we are engineers,
so it's like a decision

00:03:07.640 --> 00:03:09.200
tree or a filter, if you will.

00:03:09.200 --> 00:03:11.030
Our brains are
remarkably good at that,

00:03:11.030 --> 00:03:14.300
because that helps us navigating
the world much faster than we

00:03:14.300 --> 00:03:18.300
would without these
heuristics, if you will.

00:03:18.300 --> 00:03:21.890
So these unconscious
stereotypes, these unconscious

00:03:21.890 --> 00:03:25.670
biases, they influence
how you make products,

00:03:25.670 --> 00:03:27.405
how you make decisions.

00:03:27.405 --> 00:03:29.030
A conference is a
product, if you will.

00:03:29.030 --> 00:03:30.740
Software is a product; hardware.

00:03:30.740 --> 00:03:33.320
You build products
out of these elements.

00:03:33.320 --> 00:03:36.260
And your stereotypes
influence your decisions.

00:03:36.260 --> 00:03:38.300
Your conscious and
unconscious biases

00:03:38.300 --> 00:03:39.570
influence your decisions.

00:03:39.570 --> 00:03:43.790
By extension, sometimes they
influence what kind of product

00:03:43.790 --> 00:03:47.040
you built.

00:03:47.040 --> 00:03:51.030
We at Google, we build products
that have a global scale.

00:03:51.030 --> 00:03:54.480
We want everyone to be
part of this technology

00:03:54.480 --> 00:03:58.280
build that we are all
creating together.

00:03:58.280 --> 00:04:00.380
How do we build processes?

00:04:00.380 --> 00:04:02.130
How do we design software?

00:04:02.130 --> 00:04:05.030
How do we design
systems that are--

00:04:05.030 --> 00:04:07.610
without realizing,
without wanting it--

00:04:07.610 --> 00:04:08.840
maybe biased, as well?

00:04:08.840 --> 00:04:10.480
Our biases creep in.

00:04:10.480 --> 00:04:13.220
And if you take the
example of this conference,

00:04:13.220 --> 00:04:14.840
this was an important
question, when

00:04:14.840 --> 00:04:17.810
we started building
up the project team

00:04:17.810 --> 00:04:21.459
and started working towards
these two amazing days we just

00:04:21.459 --> 00:04:23.240
had.

00:04:23.240 --> 00:04:26.330
So we thought in
this last session

00:04:26.330 --> 00:04:29.600
of this two-days
conference, we'd

00:04:29.600 --> 00:04:33.020
talk a little bit about how
Google tries to build software

00:04:33.020 --> 00:04:35.060
in a way that's more inclusive.

00:04:35.060 --> 00:04:36.420
What are the processes?

00:04:36.420 --> 00:04:37.920
What are the practices?

00:04:37.920 --> 00:04:40.700
What are the things that
we try to do in order

00:04:40.700 --> 00:04:43.700
to overcome that challenge?

00:04:43.700 --> 00:04:47.660
So please welcome to
the stage engineering

00:04:47.660 --> 00:04:50.364
lead, YouTube,
Sowmya Subramanian.

00:04:50.364 --> 00:04:52.730
[APPLAUSE]

00:04:52.730 --> 00:04:54.230
SOWMYA SUBRAMANIAN:
Thank you, Dirk.

00:04:54.230 --> 00:04:57.060
I'm super excited to be here.

00:04:57.060 --> 00:05:00.290
I still remember my first
technical conference

00:05:00.290 --> 00:05:03.650
presentation,
almost 17 years ago

00:05:03.650 --> 00:05:07.730
in Paris, where I, the speaker,
was pretty much the only

00:05:07.730 --> 00:05:11.030
female engineer
in the whole room.

00:05:11.030 --> 00:05:14.150
To see so many women
engineer at this conference

00:05:14.150 --> 00:05:16.140
has been amazing.

00:05:16.140 --> 00:05:19.220
We've come a really long way,
and let's give a big round

00:05:19.220 --> 00:05:20.589
of applause to that.

00:05:20.589 --> 00:05:24.082
[APPLAUSE AND CHEERING]

00:05:31.580 --> 00:05:34.190
As Dirk mentioned, I am
an engineering director

00:05:34.190 --> 00:05:35.570
at YouTube.

00:05:35.570 --> 00:05:38.780
And I will be showing
a lot of YouTube videos

00:05:38.780 --> 00:05:40.730
in my presentation.

00:05:40.730 --> 00:05:48.104
So lets start off with a video
of a favorite clip of mine.

00:05:48.104 --> 00:05:48.770
[VIDEO PLAYBACK]

00:05:48.770 --> 00:05:50.580
- Me love rewind.

00:05:50.580 --> 00:05:52.173
Um num num num.

00:05:52.173 --> 00:05:52.798
[MUSIC PLAYING]

00:05:52.798 --> 00:05:55.510
- (RAPPING) Trust me, on
my I-N-D-E-P-E-N-D-E-N-T,

00:05:55.510 --> 00:05:59.947
hustling, chasing dreams since
I was 14 with a four-track,

00:05:59.947 --> 00:06:00.933
bussin.

00:06:00.933 --> 00:06:03.391
That validation comes from
giving it back to the people

00:06:03.391 --> 00:06:03.891
now.

00:06:03.891 --> 00:06:05.863
Sing this song,
and it goes like--

00:06:05.863 --> 00:06:08.321
- Ding ding ding ding ding
ding ding ding, gading da ding

00:06:08.321 --> 00:06:08.821
ding ding.

00:06:08.821 --> 00:06:09.807
- DJ, get away.

00:06:09.807 --> 00:06:11.286
DJ, get away.

00:06:11.286 --> 00:06:12.765
- And it goes like--

00:06:12.765 --> 00:06:14.244
- Pop, pop, pop, pop, pop, pop.

00:06:14.244 --> 00:06:16.216
- Pop, pop, pop, pop, pop.

00:06:16.216 --> 00:06:17.695
Pop, pop, pop, pop, pop.

00:06:17.695 --> 00:06:19.220
- What the fox say--

00:06:19.220 --> 00:06:21.128
- You want to change the world?

00:06:21.128 --> 00:06:22.550
You're a goat!

00:06:22.550 --> 00:06:24.135
You can do that!

00:06:24.135 --> 00:06:24.530
[MUSIC - MACKLEMORE &amp; RYAN
LEWIS, "CAN'T HOLD US"]

00:06:24.530 --> 00:06:26.226
- (SINGING) Can we go back?

00:06:26.226 --> 00:06:27.690
This is the moment.

00:06:27.690 --> 00:06:29.076
Tonight is the night.

00:06:29.076 --> 00:06:30.930
We'll fight till it's over.

00:06:30.930 --> 00:06:35.208
So we put our hands up like
the ceiling can't hold us,

00:06:35.208 --> 00:06:38.136
like the ceiling can't hold us.

00:06:38.136 --> 00:06:39.600
Can we go back?

00:06:39.600 --> 00:06:41.064
This is the moment.

00:06:41.064 --> 00:06:42.040
Tonight is the night.

00:06:42.040 --> 00:06:43.992
We'll fight till it's over.

00:06:43.992 --> 00:06:47.896
So we put our hands up like
the ceiling can't hold us.

00:06:47.896 --> 00:06:48.872
[END PLAYBACK]

00:06:48.872 --> 00:06:52.800
[APPLAUSE]

00:06:52.800 --> 00:06:54.470
SOWMYA SUBRAMANIAN:
So this is a clip

00:06:54.470 --> 00:06:57.620
from my all-time favorite
YouTube Rewind videos,

00:06:57.620 --> 00:07:00.920
from 2015, because
it really captures

00:07:00.920 --> 00:07:03.650
the essence of YouTube.

00:07:03.650 --> 00:07:06.620
YouTube started out with
a very simple mission--

00:07:06.620 --> 00:07:08.510
broadcast yourself.

00:07:08.510 --> 00:07:14.390
And it has grown into a global
platform with global reach.

00:07:14.390 --> 00:07:20.000
We have over 1 and 1/2 billion
users coming to YouTube

00:07:20.000 --> 00:07:24.170
every month, and
over 80% of our views

00:07:24.170 --> 00:07:28.310
come from outside
the United States.

00:07:28.310 --> 00:07:31.160
We are an open and
democratic platform,

00:07:31.160 --> 00:07:34.280
with over 400 hours
of videos being

00:07:34.280 --> 00:07:39.140
uploaded every minute by
creators all around the world--

00:07:39.140 --> 00:07:44.460
making YouTube the platform
with the most diverse content.

00:07:44.460 --> 00:07:47.660
In the time that it took
me to say this sentence

00:07:47.660 --> 00:07:50.870
or for us to debug
that problem earlier,

00:07:50.870 --> 00:07:53.690
almost a full day's
worth of videos

00:07:53.690 --> 00:07:55.115
have been uploaded to YouTube.

00:07:57.640 --> 00:08:01.420
What makes YouTube
special is that,

00:08:01.420 --> 00:08:05.350
unlike traditional
media or television,

00:08:05.350 --> 00:08:08.950
we have no gatekeepers.

00:08:08.950 --> 00:08:14.130
Anyone can have a voice and
reach and build an audience.

00:08:14.130 --> 00:08:17.670
While this is still very,
very true about YouTube,

00:08:17.670 --> 00:08:19.620
a few years ago,
when I was looking

00:08:19.620 --> 00:08:23.880
at some usage data on
YouTube, I realized

00:08:23.880 --> 00:08:28.260
that as our platform
and usage have grown,

00:08:28.260 --> 00:08:31.840
human dynamics and
unconscious biases--

00:08:31.840 --> 00:08:34.440
like what Dirk was
talking about--

00:08:34.440 --> 00:08:36.549
are creeping in.

00:08:36.549 --> 00:08:40.799
And on YouTube, too, we
see biases and gender gaps,

00:08:40.799 --> 00:08:44.250
for instance, like
traditional media.

00:08:44.250 --> 00:08:47.680
On YouTube, too, we see a
lot of our makeup videos,

00:08:47.680 --> 00:08:51.030
for instance, are created
by female creators.

00:08:51.030 --> 00:08:53.880
And a lot of the science and
engineering and technology

00:08:53.880 --> 00:08:58.560
videos are made
by male creators.

00:08:58.560 --> 00:09:02.790
With this insight, I started
a conversation and a pitch,

00:09:02.790 --> 00:09:06.840
within YouTube and across
Google at-large, whose mission

00:09:06.840 --> 00:09:10.650
is to build products
for all, about diversity

00:09:10.650 --> 00:09:13.260
in the context of
product design.

00:09:13.260 --> 00:09:19.350
And how does unconscious bias
play into product engineering?

00:09:19.350 --> 00:09:23.580
What if, in YouTube, in
addition to optimizing

00:09:23.580 --> 00:09:27.630
for the growth of watch time,
we became more intentional

00:09:27.630 --> 00:09:32.160
about our demographic
goals, such as gender reach

00:09:32.160 --> 00:09:34.680
or ethnicity reach?

00:09:34.680 --> 00:09:38.580
That would unlock opportunities
for deeper engagement

00:09:38.580 --> 00:09:44.100
and onboard more users,
thereby driving growth.

00:09:44.100 --> 00:09:48.780
So for the first time in Google,
we expanded our dialogues

00:09:48.780 --> 00:09:53.550
around diversity at Google, from
being about hiring and building

00:09:53.550 --> 00:10:00.980
balanced teams, to about
diversity becoming a core piece

00:10:00.980 --> 00:10:03.420
to defining the growth
strategies for all

00:10:03.420 --> 00:10:05.700
our products.

00:10:05.700 --> 00:10:09.660
And that brings me to today's
topic of inclusive design

00:10:09.660 --> 00:10:12.510
as a growth accelerator.

00:10:12.510 --> 00:10:15.810
So what is inclusive design?

00:10:15.810 --> 00:10:19.500
It's about engineering
products for all your target

00:10:19.500 --> 00:10:23.360
users across all demographics.

00:10:23.360 --> 00:10:26.240
When you broaden your
demographic reach,

00:10:26.240 --> 00:10:29.000
you're increasing
your user funnel

00:10:29.000 --> 00:10:33.360
and thereby driving
growth for your products.

00:10:33.360 --> 00:10:36.330
Inclusive design is not
just about user design

00:10:36.330 --> 00:10:38.040
or visual design.

00:10:38.040 --> 00:10:42.510
It is also about machine
learning and algorithms,

00:10:42.510 --> 00:10:45.690
about the training
data, about testing,

00:10:45.690 --> 00:10:48.940
about how you launch the
product, the branding,

00:10:48.940 --> 00:10:51.850
marketing, and more.

00:10:51.850 --> 00:10:56.710
Inclusive design is about asking
which target user segments

00:10:56.710 --> 00:10:59.110
could you be doing more for?

00:10:59.110 --> 00:11:01.270
It could be gender-based.

00:11:01.270 --> 00:11:03.700
It could also be that
your products were great

00:11:03.700 --> 00:11:07.690
when you have good connectivity,
but then doesn't work that

00:11:07.690 --> 00:11:10.210
well in the developing world.

00:11:10.210 --> 00:11:12.760
It could be about
optimizing your products

00:11:12.760 --> 00:11:16.640
for certain ethnicity groups.

00:11:16.640 --> 00:11:20.420
As an industry, we all have been
doing inclusive design for lots

00:11:20.420 --> 00:11:24.170
of years now in the form
of accessibility work,

00:11:24.170 --> 00:11:30.180
and this is about expanding
that across other dimensions.

00:11:30.180 --> 00:11:34.290
For instance, when
airbags first came out,

00:11:34.290 --> 00:11:39.920
there were more deaths and
injuries in women and kids.

00:11:39.920 --> 00:11:41.150
Why?

00:11:41.150 --> 00:11:45.650
Because airbags were
tested with only tall,

00:11:45.650 --> 00:11:49.690
male crash test dummies.

00:11:49.690 --> 00:11:53.960
Were the engineers who
designed this sexist?

00:11:53.960 --> 00:11:54.640
No.

00:11:54.640 --> 00:11:56.680
It's because of the
unconscious biases

00:11:56.680 --> 00:11:58.990
that informed their process.

00:11:58.990 --> 00:12:02.420
Female drivers tend to
have a smaller build

00:12:02.420 --> 00:12:06.670
and in real crashes,
have 47% higher chance

00:12:06.670 --> 00:12:10.760
of severe injuries than men.

00:12:10.760 --> 00:12:14.380
So once the industry included
women in their design

00:12:14.380 --> 00:12:16.720
and started using
female crash test

00:12:16.720 --> 00:12:21.130
dummies, the safety of
airbags went up significantly

00:12:21.130 --> 00:12:22.390
for women--

00:12:22.390 --> 00:12:27.690
and not just for women, but for
anyone with a smaller build.

00:12:27.690 --> 00:12:32.400
So this is an example of
gender-based inclusive design.

00:12:32.400 --> 00:12:35.310
Airbags still don't
work well for kids,

00:12:35.310 --> 00:12:39.560
and that is an
opportunity of growth.

00:12:39.560 --> 00:12:42.200
For this next two example
that I want to show you,

00:12:42.200 --> 00:12:43.360
I'll start with a video.

00:12:45.954 --> 00:12:46.620
[VIDEO PLAYBACK]

00:12:46.620 --> 00:12:48.560
- My coworker Juan and
I are sitting in front

00:12:48.560 --> 00:12:51.360
of an HP media smart computer.

00:12:51.360 --> 00:12:54.120
It's supposed to
follow me as I move.

00:12:54.120 --> 00:12:56.070
I'm black.

00:12:56.070 --> 00:12:58.590
I think my blackness is
interfering with the computer's

00:12:58.590 --> 00:13:00.720
ability to follow me.

00:13:00.720 --> 00:13:04.584
- So she moved this way,
and a camera followed her.

00:13:04.584 --> 00:13:06.000
And then he'd get
into the screen,

00:13:06.000 --> 00:13:07.830
and it would be
completely stable.

00:13:07.830 --> 00:13:10.080
- No face recognition
any more, buddy.

00:13:10.080 --> 00:13:11.123
My

00:13:11.123 --> 00:13:11.706
[END PLAYBACK]

00:13:11.706 --> 00:13:13.354
[LAUGHTER]

00:13:13.354 --> 00:13:15.770
SOWMYA SUBRAMANIAN: So, anyone
know what's going on there?

00:13:18.490 --> 00:13:22.810
For this, we need to go back
to the 1950s, when Kodak

00:13:22.810 --> 00:13:25.540
was dominating
color photography,

00:13:25.540 --> 00:13:28.240
and they've introduced
the Shirley color

00:13:28.240 --> 00:13:32.510
card, which has become the
standard for all photography.

00:13:32.510 --> 00:13:36.910
But it works better
with lighter skin tones.

00:13:36.910 --> 00:13:41.710
So what this means is you
often have exposure issues

00:13:41.710 --> 00:13:45.580
when taking multiracial photos.

00:13:45.580 --> 00:13:52.540
This problem was recognized as
a big problem in the 1970s, when

00:13:52.540 --> 00:13:56.470
chocolate makers and
wood manufacturers

00:13:56.470 --> 00:13:59.650
were having a hard time
creating advertising

00:13:59.650 --> 00:14:03.220
material for their products,
because they could not

00:14:03.220 --> 00:14:07.430
capture in photos the
different shades of brown--

00:14:07.430 --> 00:14:10.480
so a funny way for this
problem to get surfaced.

00:14:10.480 --> 00:14:13.150
It also was becoming
an increasing problem

00:14:13.150 --> 00:14:15.850
when the media and
television world was starting

00:14:15.850 --> 00:14:18.520
to become more diverse.

00:14:18.520 --> 00:14:24.760
Finally, 25 years later, in
1995, a group of engineers

00:14:24.760 --> 00:14:27.340
took an inclusive
design approach

00:14:27.340 --> 00:14:29.350
to help bridge
this gap, and they

00:14:29.350 --> 00:14:34.630
launched multiracial color
cards, which have significantly

00:14:34.630 --> 00:14:38.320
improved camera and
photo technology

00:14:38.320 --> 00:14:41.890
and helped unlock a lot
of advertising revenue

00:14:41.890 --> 00:14:43.660
and new business opportunities.

00:14:46.880 --> 00:14:48.620
While things have
improved, there's

00:14:48.620 --> 00:14:51.230
still a slight light
skin bias, which

00:14:51.230 --> 00:14:54.530
is what you saw in
the video earlier.

00:14:54.530 --> 00:14:57.290
The black color skin
is not recognized

00:14:57.290 --> 00:15:01.190
by the camera, which is why,
when the black person was

00:15:01.190 --> 00:15:04.880
moving, it seemed so stable,
but the face recognition

00:15:04.880 --> 00:15:09.240
was able to follow along with
the white-skinned person.

00:15:09.240 --> 00:15:11.750
I'm really proud
that, at Google, we've

00:15:11.750 --> 00:15:14.960
taken a proactive,
inclusive design approach

00:15:14.960 --> 00:15:16.700
to helping bridge this gap.

00:15:16.700 --> 00:15:20.570
And the Google camera team is
helping solve this problem.

00:15:20.570 --> 00:15:24.914
Let's watch a video on what
the camera team has done.

00:15:24.914 --> 00:15:25.580
[VIDEO PLAYBACK]

00:15:25.580 --> 00:15:27.560
- Pictures tell stories.

00:15:27.560 --> 00:15:29.600
It doesn't really
matter if it's a selfie

00:15:29.600 --> 00:15:31.250
or if it's a portrait
that somebody

00:15:31.250 --> 00:15:33.410
is taking of another person.

00:15:33.410 --> 00:15:35.120
You are sharing in
that human moment.

00:15:37.760 --> 00:15:39.770
- Let's take a
picture of you two--

00:15:39.770 --> 00:15:43.100
one, two, three.

00:15:43.100 --> 00:15:44.995
No, he's too dark.

00:15:48.680 --> 00:15:50.880
- So this is where the
discussion started.

00:15:50.880 --> 00:15:52.510
- Here, just use my phone.

00:15:52.510 --> 00:15:54.472
It's got a better sensor.

00:15:54.472 --> 00:15:56.170
- A better sensor?

00:15:56.170 --> 00:15:57.380
What does that even mean?

00:15:57.380 --> 00:16:00.620
- Color tuning is an
extremely complicated process.

00:16:00.620 --> 00:16:02.860
When we look at images,
what we try to do

00:16:02.860 --> 00:16:04.840
is figure out how
much of a difference

00:16:04.840 --> 00:16:07.570
there is between, let's
say, a reference image

00:16:07.570 --> 00:16:12.230
and the image that we actually
are trying to quantify.

00:16:12.230 --> 00:16:14.680
We were running some
tests on a product,

00:16:14.680 --> 00:16:17.250
and it was the proximity
sensor we were testing.

00:16:17.250 --> 00:16:20.140
And we said, oh, it looks
like it does 60 centimeters.

00:16:20.140 --> 00:16:22.780
Then we looked at each other,
and we go, we're both white.

00:16:22.780 --> 00:16:27.400
The technology itself, as many
people will say, is not racist.

00:16:27.400 --> 00:16:29.890
It's just that it
wasn't tested properly

00:16:29.890 --> 00:16:34.930
to make sure that the designers
weren't unconsciously biased.

00:16:34.930 --> 00:16:36.670
Hey, there's an entire
world out there,

00:16:36.670 --> 00:16:38.767
and we want to make sure
this works for everybody.

00:16:41.450 --> 00:16:44.360
I love the fact that we
get to influence cameras,

00:16:44.360 --> 00:16:46.820
that we get to share
those experiences of life

00:16:46.820 --> 00:16:47.510
and emotion.

00:16:47.510 --> 00:16:49.790
To me, that is what
imaging is about.

00:16:49.790 --> 00:16:53.960
It as a vehicle to
express humanity.

00:16:53.960 --> 00:16:56.630
- I'll take a
Picture of you two--

00:16:56.630 --> 00:16:58.970
one, two, three.

00:16:58.970 --> 00:17:02.421
[MUSIC PLAYING]

00:17:03.900 --> 00:17:04.886
[END PLAYBACK]

00:17:04.886 --> 00:17:08.337
[APPLAUSE]

00:17:12.780 --> 00:17:15.579
SOWMYA SUBRAMANIAN: All
right, for this next example,

00:17:15.579 --> 00:17:17.125
how many of you know of Cheetos?

00:17:19.819 --> 00:17:20.930
A few.

00:17:20.930 --> 00:17:25.130
Cheese Puffs or
the cheesy snacks--

00:17:25.130 --> 00:17:28.550
it's a very American
snack called Cheetos.

00:17:28.550 --> 00:17:31.280
And when it first came
out, it was only available

00:17:31.280 --> 00:17:32.720
in one flavor--

00:17:32.720 --> 00:17:35.660
the cheesy flavor,
cheese flavor--

00:17:35.660 --> 00:17:40.250
until a janitor who
worked for the company

00:17:40.250 --> 00:17:44.240
started adding chili and lime
to his packets of Cheetos,

00:17:44.240 --> 00:17:46.910
to make it more
flavorful for him.

00:17:46.910 --> 00:17:50.390
He was Latino and
loved chili and lime.

00:17:50.390 --> 00:17:54.170
His family and friends
also liked the taste,

00:17:54.170 --> 00:17:57.140
so he decided to
pitch this new flavor

00:17:57.140 --> 00:18:01.520
idea to the president of the
company, who actually listened

00:18:01.520 --> 00:18:04.700
to this janitor and
decided to experiment

00:18:04.700 --> 00:18:07.040
with this new flavor.

00:18:07.040 --> 00:18:10.730
And that unlocked huge
opportunities and markets

00:18:10.730 --> 00:18:12.650
for Cheetos.

00:18:12.650 --> 00:18:15.770
I'm happy to say
the janitor is now

00:18:15.770 --> 00:18:18.380
an executive at the company.

00:18:18.380 --> 00:18:20.930
It's a true
rags-to-riches story,

00:18:20.930 --> 00:18:23.150
and more importantly,
it's a story

00:18:23.150 --> 00:18:26.635
that shows cultural,
inclusive design thinking.

00:18:29.870 --> 00:18:33.310
This next example
is from YouTube.

00:18:33.310 --> 00:18:37.640
A few years ago, we wanted to
increase engagement with kids

00:18:37.640 --> 00:18:40.570
and families on YouTube.

00:18:40.570 --> 00:18:43.270
So we brought kids
into our UX Lab

00:18:43.270 --> 00:18:47.960
to see, how do they use
and interact with YouTube?

00:18:47.960 --> 00:18:51.910
This is what we see, yeah?

00:18:51.910 --> 00:18:54.190
Everyone uses YouTube here?

00:18:54.190 --> 00:18:56.170
Yes, many.

00:18:56.170 --> 00:18:57.580
This is what we see.

00:18:57.580 --> 00:19:00.170
But the kids, when they
saw the same thing,

00:19:00.170 --> 00:19:01.857
this is how they saw it.

00:19:01.857 --> 00:19:04.660
[LAUGHTER]

00:19:04.660 --> 00:19:08.050
They didn't care about anything
else on the screen other

00:19:08.050 --> 00:19:11.950
than the video content
that was there.

00:19:11.950 --> 00:19:16.450
That is when we realized, if we
wanted to increase engagement

00:19:16.450 --> 00:19:21.100
with kids and families, we
had to reimagine YouTube

00:19:21.100 --> 00:19:24.550
from the ground up
and build a new app.

00:19:24.550 --> 00:19:28.330
And that's when YouTube
Kids App was born.

00:19:28.330 --> 00:19:31.930
The YouTube Kids App
provides easy-to-use

00:19:31.930 --> 00:19:36.820
interactivity and safe,
enriching, and engaging

00:19:36.820 --> 00:19:40.100
content for kids and families.

00:19:40.100 --> 00:19:42.970
And we've done this
by improving our user

00:19:42.970 --> 00:19:47.620
design and visual design and
user interactions, and also,

00:19:47.620 --> 00:19:51.430
by making a lot of
changes to our algorithms,

00:19:51.430 --> 00:19:56.400
the backend systems, and our
content classification systems.

00:19:56.400 --> 00:20:03.440
YouTube Kids now has over 11
million weekly active users,

00:20:03.440 --> 00:20:07.160
and we see the engagement
with kids and family content

00:20:07.160 --> 00:20:10.430
on the YouTube Kids
App to be significantly

00:20:10.430 --> 00:20:14.130
more than on Main YouTube.

00:20:14.130 --> 00:20:18.020
This is a great example of
age-based inclusive design

00:20:18.020 --> 00:20:20.220
driving growth.

00:20:20.220 --> 00:20:24.290
It also demonstrates
clearly that sometimes,

00:20:24.290 --> 00:20:27.980
for you to meet the needs
of specific target user

00:20:27.980 --> 00:20:30.590
demographics, you
might have to build

00:20:30.590 --> 00:20:33.600
a specialized product for them.

00:20:33.600 --> 00:20:38.130
How many of you have used
the YouTube Kids App?

00:20:38.130 --> 00:20:40.590
Hardly any, OK-- because
we haven't launched

00:20:40.590 --> 00:20:42.480
in all the markets yet.

00:20:42.480 --> 00:20:46.560
So I'm going to play the first
launch video of YouTube Kids

00:20:46.560 --> 00:20:51.270
App, for you to get an idea
of what I'm talking about.

00:20:51.270 --> 00:20:54.630
[VIDEO PLAYBACK]

00:20:55.590 --> 00:20:58.092
- OK, so do you want
to tell a story?

00:20:58.092 --> 00:21:00.410
- Uh, sure, yeah.

00:21:00.410 --> 00:21:03.840
Oh my gosh, now everything
is popping into my brain.

00:21:03.840 --> 00:21:05.420
OK.

00:21:05.420 --> 00:21:10.080
- Once upon a time, there
was a boy named Alex,

00:21:10.080 --> 00:21:12.230
- Three sisters,

00:21:12.230 --> 00:21:13.470
- A bunny,

00:21:13.470 --> 00:21:14.880
- Two trains.

00:21:14.880 --> 00:21:20.240
He got caught in a bubble, and
then he was floating around.

00:21:20.240 --> 00:21:22.770
He went to France.

00:21:22.770 --> 00:21:29.480
- And then he sees Dracula and
a few ghosts, and Frankenstein.

00:21:29.480 --> 00:21:32.240
- And then the robot came
and ate the whole town.

00:21:35.030 --> 00:21:38.390
- And everyone went like crazy.

00:21:38.390 --> 00:21:40.950
- And then they had
to destroy the robet.

00:21:40.950 --> 00:21:41.950
That was the only thing.

00:21:41.950 --> 00:21:46.030
So they put it on
the sun, like so.

00:21:46.030 --> 00:21:48.460
- Because since the
sun is made of gas,

00:21:48.460 --> 00:21:50.730
it couldn't eat
anything thing there.

00:21:50.730 --> 00:21:54.270
- He got surprised,
and then he jumped back

00:21:54.270 --> 00:21:55.200
and popped the bubble.

00:21:55.200 --> 00:21:55.830
And he fell.

00:21:55.830 --> 00:21:56.640
The end.

00:21:56.640 --> 00:21:57.948
That's really weird.

00:21:57.948 --> 00:21:58.944
[LAUGHS]

00:21:58.944 --> 00:22:02.430
[MUSIC PLAYING]

00:22:10.398 --> 00:22:11.394
[VIDEO PLAYBACK]

00:22:11.394 --> 00:22:15.378
[APPLAUSE]

00:22:17.880 --> 00:22:21.700
SOWMYA SUBRAMANIAN: So how
many of you use emojis?

00:22:21.700 --> 00:22:22.450
Yeah.

00:22:22.450 --> 00:22:23.560
You're not alone.

00:22:23.560 --> 00:22:30.900
90% of the world's online
population also uses emojis.

00:22:30.900 --> 00:22:34.140
While there are many
emojis to choose from,

00:22:34.140 --> 00:22:36.390
they're fairly stereotypical.

00:22:36.390 --> 00:22:39.930
The boys are portrayed this
way, as doctors, and police,

00:22:39.930 --> 00:22:42.480
and others, and the
girls are portrayed

00:22:42.480 --> 00:22:46.750
as queens and princesses and
giving haircuts to people.

00:22:46.750 --> 00:22:49.110
So it's fairly stereotypical.

00:22:49.110 --> 00:22:54.000
To bridge that gap and to also
be inspiring for young girls,

00:22:54.000 --> 00:22:58.590
Google has added a
whole new set of emojis

00:22:58.590 --> 00:23:02.300
to represent women and
men in diverse roles

00:23:02.300 --> 00:23:07.170
and in a mix of hair and skin
colors, to be more inclusive.

00:23:07.170 --> 00:23:10.110
This is driving
industry-wide change,

00:23:10.110 --> 00:23:13.950
with iOS, Facebook,
Twitter, and others

00:23:13.950 --> 00:23:17.670
embracing this new set of
emojis in their products

00:23:17.670 --> 00:23:19.660
and platforms.

00:23:19.660 --> 00:23:22.170
This is a great case
of gender-based and

00:23:22.170 --> 00:23:24.573
ethnicity-based
inclusive design.

00:23:24.573 --> 00:23:28.024
[APPLAUSE]

00:23:32.470 --> 00:23:35.590
This next example is
again from YouTube.

00:23:35.590 --> 00:23:39.190
How many of you have
heard of women harassment

00:23:39.190 --> 00:23:44.270
in online communities,
like the gaming community,

00:23:44.270 --> 00:23:48.400
through hashtags and
online commenting tools?

00:23:48.400 --> 00:23:49.780
Some of you, yeah.

00:23:49.780 --> 00:23:51.870
Many of you might have
heard of Gamergate,

00:23:51.870 --> 00:23:55.840
where women gamers were even
threatened for their lives,

00:23:55.840 --> 00:23:59.470
using these online
commenting tools.

00:23:59.470 --> 00:24:03.160
A few years ago, in YouTube,
we organized a Women

00:24:03.160 --> 00:24:05.710
at YouTube Hackathon
around the theme

00:24:05.710 --> 00:24:09.820
of bridging the gender gap,
to drive grassroots momentum

00:24:09.820 --> 00:24:12.340
around inclusive design.

00:24:12.340 --> 00:24:15.520
One of the salient
projects in this hackathon

00:24:15.520 --> 00:24:18.880
focused on improving
YouTube comments

00:24:18.880 --> 00:24:25.150
to help women combat harassment
and feel safer on YouTube.

00:24:25.150 --> 00:24:27.540
As a result of this
hackathon project,

00:24:27.540 --> 00:24:29.890
we've launched
several enhancements

00:24:29.890 --> 00:24:33.010
to YouTube comments on
our moderation tools,

00:24:33.010 --> 00:24:36.280
such as making it
easier for creators

00:24:36.280 --> 00:24:40.840
to deal with inappropriate
comments, blacklist words,

00:24:40.840 --> 00:24:43.750
and also limit
commenters who are making

00:24:43.750 --> 00:24:46.420
those inappropriate comments.

00:24:46.420 --> 00:24:50.800
This is a great example of
gender-based, inclusive design

00:24:50.800 --> 00:24:54.100
thinking to further
deepen our engagement

00:24:54.100 --> 00:24:56.560
with our female creators.

00:24:56.560 --> 00:25:00.520
It also shows that, many
times, grassroots ideas

00:25:00.520 --> 00:25:04.249
like hackathons can bring about
change in product direction

00:25:04.249 --> 00:25:04.790
and strategy.

00:25:07.910 --> 00:25:10.280
So now I've gone
through several examples

00:25:10.280 --> 00:25:12.740
of different kinds
of inclusive design,

00:25:12.740 --> 00:25:15.380
some from the
non-technology world

00:25:15.380 --> 00:25:17.150
and many from the
technology world,

00:25:17.150 --> 00:25:19.910
and many from
Google and YouTube.

00:25:19.910 --> 00:25:21.620
And now I want to
talk about, how

00:25:21.620 --> 00:25:24.140
do you do inclusive
design at scale,

00:25:24.140 --> 00:25:26.510
and how can you
become more proactive

00:25:26.510 --> 00:25:28.760
in incorporating
inclusive design,

00:25:28.760 --> 00:25:31.790
rather than being reactive?

00:25:31.790 --> 00:25:35.990
So this is the typical stages of
product development at Google.

00:25:35.990 --> 00:25:37.890
They're not necessarily linear.

00:25:37.890 --> 00:25:40.070
We do do it in an iterative way.

00:25:40.070 --> 00:25:42.530
But at a high level, these
are the different stages

00:25:42.530 --> 00:25:46.910
we go through during product
design and development.

00:25:46.910 --> 00:25:49.790
In Google, we are
starting to become

00:25:49.790 --> 00:25:53.000
more intentional about
setting demographic goals

00:25:53.000 --> 00:25:56.490
at every stage of
product development.

00:25:56.490 --> 00:26:00.680
So when we are defining target
users, instead of just saying

00:26:00.680 --> 00:26:05.660
18 to 34 age group, we are
also picking a demographic goal

00:26:05.660 --> 00:26:07.250
that we would like to meet.

00:26:07.250 --> 00:26:12.800
So we're saying 18 to 34 age
group, with equal engagement

00:26:12.800 --> 00:26:18.070
from male and female,
or 18 to 34 age group,

00:26:18.070 --> 00:26:21.230
with this level of
ethnic reach that we

00:26:21.230 --> 00:26:23.840
want to get on the platform.

00:26:23.840 --> 00:26:26.060
Once we've set that
demographic goal,

00:26:26.060 --> 00:26:29.120
we propagate it through
the rest of the stages

00:26:29.120 --> 00:26:30.930
of product development.

00:26:30.930 --> 00:26:33.260
So when we're doing
testing, for instance,

00:26:33.260 --> 00:26:36.740
we are very careful in
picking our test user

00:26:36.740 --> 00:26:39.170
groups, to ensure that
they are reflecting

00:26:39.170 --> 00:26:43.010
and mirroring the demographic
goals that we want to meet,

00:26:43.010 --> 00:26:45.920
whether it is with users
research and user testing,

00:26:45.920 --> 00:26:49.130
or training data for
machine learning,

00:26:49.130 --> 00:26:53.190
or market research, or more.

00:26:53.190 --> 00:26:55.830
Over the last two
days, you've heard

00:26:55.830 --> 00:26:59.310
that Google is a lot about
algorithms and machine

00:26:59.310 --> 00:27:00.600
learning.

00:27:00.600 --> 00:27:04.530
And you could ask, machines--
why are they biased?

00:27:04.530 --> 00:27:06.810
They should all be neutral.

00:27:06.810 --> 00:27:08.470
And that's not the case.

00:27:08.470 --> 00:27:09.900
There are lots of
different kinds

00:27:09.900 --> 00:27:13.230
of biases that can come into
your machine learning systems,

00:27:13.230 --> 00:27:14.160
too.

00:27:14.160 --> 00:27:16.770
And I want to roll a
video now to show you

00:27:16.770 --> 00:27:19.110
what those different
biases are, and how

00:27:19.110 --> 00:27:21.720
we are tackling it at Google.

00:27:21.720 --> 00:27:23.474
Let's roll the video.

00:27:23.474 --> 00:27:24.140
[VIDEO PLAYBACK]

00:27:24.140 --> 00:27:25.750
- Let's play a game.

00:27:25.750 --> 00:27:29.080
Close your eyes
and picture a shoe.

00:27:29.080 --> 00:27:31.734
OK, did anyone picture this?

00:27:31.734 --> 00:27:33.590
This?

00:27:33.590 --> 00:27:35.720
How about this?

00:27:35.720 --> 00:27:38.000
We may not even know
why, but each of us

00:27:38.000 --> 00:27:41.150
is biased toward one
shoe over the others.

00:27:41.150 --> 00:27:43.640
Now imagine that you're
trying to teach a computer

00:27:43.640 --> 00:27:45.200
to recognize a shoe.

00:27:45.200 --> 00:27:47.990
You may end up exposing
it to your own bias.

00:27:47.990 --> 00:27:50.990
That's how bias happens
in machine learning.

00:27:50.990 --> 00:27:53.660
But first, what is
machine learning?

00:27:53.660 --> 00:27:57.620
Well, it's used in a lot
of technology we use today.

00:27:57.620 --> 00:28:00.590
Machine learning helps us
get from place to place,

00:28:00.590 --> 00:28:04.190
gives us suggestions,
translates stuff, even

00:28:04.190 --> 00:28:06.450
understands what you say to it.

00:28:06.450 --> 00:28:07.730
How does it work?

00:28:07.730 --> 00:28:10.730
With traditional programming,
people hand code the solution

00:28:10.730 --> 00:28:13.340
to a problem step by step.

00:28:13.340 --> 00:28:16.220
With machine learning,
computers learn the solution

00:28:16.220 --> 00:28:18.440
by finding patterns in data.

00:28:18.440 --> 00:28:21.095
So it's easy to think there's
no human bias in that.

00:28:21.095 --> 00:28:23.660
But just because
something is based on data

00:28:23.660 --> 00:28:26.270
doesn't automatically
make it neutral.

00:28:26.270 --> 00:28:28.790
Even with good intentions,
it's impossible to separate

00:28:28.790 --> 00:28:31.640
ourselves from our
own human biases.

00:28:31.640 --> 00:28:34.430
So our human biases become
part of the technology we

00:28:34.430 --> 00:28:37.380
create in many different ways.

00:28:37.380 --> 00:28:41.180
There is interaction bias,
like this recent game,

00:28:41.180 --> 00:28:43.940
where people were asked to
draw shoes for the computer.

00:28:43.940 --> 00:28:46.020
Most people drew ones like this.

00:28:46.020 --> 00:28:48.080
So as more people
interacted with the game,

00:28:48.080 --> 00:28:51.440
the computer didn't
even recognize these.

00:28:51.440 --> 00:28:54.950
Latent bias-- for example, if
you were training a computer

00:28:54.950 --> 00:28:57.890
on what a physicist looks
like, and you're using pictures

00:28:57.890 --> 00:28:59.900
of past physicists,
your algorithm

00:28:59.900 --> 00:29:04.230
will end up with a latent
bias, skewing towards men.

00:29:04.230 --> 00:29:06.830
And selection bias-- say
you're training a model

00:29:06.830 --> 00:29:08.692
to recognize faces.

00:29:08.692 --> 00:29:11.150
Whether you grab images from
the internet or your own photo

00:29:11.150 --> 00:29:13.190
library, are you
making sure to select

00:29:13.190 --> 00:29:15.530
photos that represent everyone?

00:29:15.530 --> 00:29:18.800
Since some of our most advanced
products use machine learning,

00:29:18.800 --> 00:29:20.750
we've been working to
prevent that technology

00:29:20.750 --> 00:29:22.675
from perpetuating
negative human bias--

00:29:25.340 --> 00:29:27.704
from tackling offensive
or clearly misleading

00:29:27.704 --> 00:29:29.870
information from appearing
at the top of your search

00:29:29.870 --> 00:29:34.470
results page, to adding a
feedback tool in the search bar

00:29:34.470 --> 00:29:36.830
so people can flag
hateful or inappropriate

00:29:36.830 --> 00:29:38.090
autocomplete suggestions.

00:29:40.700 --> 00:29:43.220
It's a complex issue, and
there is no magic bullet.

00:29:43.220 --> 00:29:45.380
But it starts with all
of us being aware of it

00:29:45.380 --> 00:29:47.780
so we can all be part
of the conversation.

00:29:47.780 --> 00:29:49.850
Because technology
should work for everyone.

00:29:49.850 --> 00:29:53.308
[MUSIC PLAYING]

00:29:56.272 --> 00:29:57.754
[END PLAYBACK]

00:29:57.754 --> 00:30:01.212
[APPLAUSE]

00:30:04.690 --> 00:30:06.610
So once you've
designed the product,

00:30:06.610 --> 00:30:09.770
and you're ready to launch,
it's really important

00:30:09.770 --> 00:30:13.780
that your marketing
branding and support systems

00:30:13.780 --> 00:30:17.590
all are making sure to echo
the demographic goals that you

00:30:17.590 --> 00:30:20.390
set out with.

00:30:20.390 --> 00:30:23.190
Now, for inclusive
design to be a success,

00:30:23.190 --> 00:30:26.090
in addition to everything
that we talked about just now,

00:30:26.090 --> 00:30:29.660
it's really important to still
have diverse perspectives

00:30:29.660 --> 00:30:33.020
and diverse teams and to
increase the representation

00:30:33.020 --> 00:30:36.210
of diversity in technology.

00:30:36.210 --> 00:30:39.500
So to this end, Google is
investing, through our computer

00:30:39.500 --> 00:30:42.170
science education
and media efforts,

00:30:42.170 --> 00:30:44.900
to increase the access
of technology education

00:30:44.900 --> 00:30:48.050
around the world
to a diverse group

00:30:48.050 --> 00:30:53.210
and to help increase awareness
of unconscious biases,

00:30:53.210 --> 00:30:56.780
and also partnering with
Hollywood and other media

00:30:56.780 --> 00:31:01.670
producers to change how
scientists and engineers

00:31:01.670 --> 00:31:05.360
and computer scientists are
portrayed in the big screen--

00:31:05.360 --> 00:31:08.810
because that can be
a huge influencer;

00:31:08.810 --> 00:31:12.650
so another video, the last
one in this presentation,

00:31:12.650 --> 00:31:17.090
showing what we're doing
with the media work.

00:31:17.090 --> 00:31:18.174
Let's roll the video.

00:31:18.174 --> 00:31:18.840
[VIDEO PLAYBACK]

00:31:18.840 --> 00:31:22.200
[MUSIC PLAYING]

00:31:26.520 --> 00:31:29.375
- When I think of the lack
of diversity in Hollywood

00:31:29.375 --> 00:31:31.560
and the lack of
inclusion, I figure,

00:31:31.560 --> 00:31:34.670
if I'm not part of the solution,
then I'm part of the problem.

00:31:34.670 --> 00:31:38.670
[MUSIC PLAYING]

00:31:38.670 --> 00:31:40.605
- When I meet students
who look like me,

00:31:40.605 --> 00:31:42.240
I see their eyes
light up when they

00:31:42.240 --> 00:31:43.365
learn that I'm an engineer.

00:31:43.365 --> 00:31:47.245
[MUSIC PLAYING]

00:31:49.680 --> 00:31:51.879
- The most rewarding
aspect of this work

00:31:51.879 --> 00:31:53.420
is showing girls
that they don't have

00:31:53.420 --> 00:31:56.660
to choose between computer
science and their passions.

00:31:56.660 --> 00:32:00.486
Computer science is really
just a tool for what they love.

00:32:00.486 --> 00:32:02.950
- Computer code can do anything.

00:32:02.950 --> 00:32:07.360
This one searches for signs
of life in outer space.

00:32:07.360 --> 00:32:10.070
- Growing up, I was so
good at math and science.

00:32:10.070 --> 00:32:12.067
But I didn't feel like
I could pursue a career

00:32:12.067 --> 00:32:14.650
in [INAUDIBLE],, because I didn't
see anyone who would like me

00:32:14.650 --> 00:32:17.450
doing it, especially not on TV.

00:32:17.450 --> 00:32:20.470
- A Google-funded Geena Davis
inclusion quotient shows that,

00:32:20.470 --> 00:32:23.160
even when women
are leads in film,

00:32:23.160 --> 00:32:26.250
they still receive three times
less screen and speaker time

00:32:26.250 --> 00:32:27.690
than their male counterparts.

00:32:27.690 --> 00:32:31.030
[MUSIC PLAYING]

00:32:31.530 --> 00:32:34.140
- The only way to find a
source address is to access

00:32:34.140 --> 00:32:37.210
and analyze the data [INAUDIBLE]
packets in this broadcast.

00:32:37.210 --> 00:32:39.370
- We want technology
to look fun,

00:32:39.370 --> 00:32:43.000
accessible, and common as
something to do for girls--

00:32:43.000 --> 00:32:47.140
or, as some would say,
normal, because it is.

00:32:47.140 --> 00:32:50.620
- I'm here because I wanted
the billions of women and girls

00:32:50.620 --> 00:32:53.560
all over the world to know
that you can be literally

00:32:53.560 --> 00:32:54.990
anything you want to be.

00:32:54.990 --> 00:32:57.220
- Our CSM Media team
strives to change

00:32:57.220 --> 00:32:59.710
the narrative of computer
science for people of color,

00:32:59.710 --> 00:33:03.490
the LGBTQIA plus community,
people with disabilities,

00:33:03.490 --> 00:33:05.600
all aspiring youth, so
they can finally see

00:33:05.600 --> 00:33:08.230
their reflections in the media.

00:33:08.230 --> 00:33:10.000
- As a woman of
color and a mom, I

00:33:10.000 --> 00:33:12.910
want my kids to know they can
pursue any career, regardless

00:33:12.910 --> 00:33:14.230
of gender or race.

00:33:14.230 --> 00:33:16.780
- Yes, absolutely, if the
platform doesn't exist,

00:33:16.780 --> 00:33:18.580
we need to create it.

00:33:18.580 --> 00:33:21.130
- Google is in a unique
position to work with partners,

00:33:21.130 --> 00:33:23.260
to provide students with
access to technology,

00:33:23.260 --> 00:33:25.300
and empower them to be creators.

00:33:25.300 --> 00:33:28.916
- It is so cool to see their
minds fill with possibilities.

00:33:28.916 --> 00:33:32.402
[MUSIC PLAYING]

00:33:37.880 --> 00:33:38.876
[END PLAYBACK]

00:33:38.876 --> 00:33:42.362
[APPLAUSE]

00:33:46.870 --> 00:33:49.330
SOWMYA SUBRAMANIAN: So
now it comes to you.

00:33:49.330 --> 00:33:53.170
I hope I've shown you today
the value of inclusive design

00:33:53.170 --> 00:33:56.860
and how you can drive growth
using inclusive design.

00:33:56.860 --> 00:34:00.190
I hope also that I've shown
you the ways in which Google

00:34:00.190 --> 00:34:04.120
is doing inclusive design,
and what are some of the steps

00:34:04.120 --> 00:34:07.510
that we are taking by
becoming more intentional.

00:34:07.510 --> 00:34:11.290
Now, you, when you leave this
room, go back to your offices,

00:34:11.290 --> 00:34:15.400
can follow the same, similar
approach that Google is doing.

00:34:15.400 --> 00:34:18.340
I would also like to leave
you with a cheat sheet.

00:34:18.340 --> 00:34:22.030
All right, the first thing
is, in your current products,

00:34:22.030 --> 00:34:25.120
it would be great if you can
identify demographic gaps

00:34:25.120 --> 00:34:28.400
that you have and the
magnitude of these gaps

00:34:28.400 --> 00:34:31.110
and, if you were to bridge
them, what kind of impact

00:34:31.110 --> 00:34:33.159
they could be driving.

00:34:33.159 --> 00:34:37.373
Once you have that, prioritize
which gap, the one gap,

00:34:37.373 --> 00:34:38.290
that you want to fill.

00:34:38.290 --> 00:34:40.659
Because we all are
resource constrained

00:34:40.659 --> 00:34:42.310
and can't solve everything.

00:34:42.310 --> 00:34:44.739
So pick one or two
gaps that drives

00:34:44.739 --> 00:34:47.449
the biggest impact for you.

00:34:47.449 --> 00:34:49.760
As you bridge those
gaps, make sure you

00:34:49.760 --> 00:34:54.150
have metrics and logging and
data to measure progress.

00:34:54.150 --> 00:34:58.490
And when you drive big
impact, celebrate your wins.

00:34:58.490 --> 00:35:02.150
Also, it's really important
to foster an inclusive culture

00:35:02.150 --> 00:35:04.040
in your organizations.

00:35:04.040 --> 00:35:07.730
You can do hackathons around
inclusive design teams

00:35:07.730 --> 00:35:10.340
to drive more
energy and momentum

00:35:10.340 --> 00:35:12.420
across your entire team.

00:35:12.420 --> 00:35:14.960
You can also do
unconscious bias trainings

00:35:14.960 --> 00:35:19.410
to make more people aware
in your whole organization.

00:35:19.410 --> 00:35:23.210
So with that, now I want
you to take a moment

00:35:23.210 --> 00:35:25.190
and think about the
one thing you're

00:35:25.190 --> 00:35:28.550
going to do differently
when you leave this room.

00:35:28.550 --> 00:35:33.530
And if you want to be bold,
you can tweet it @googledevs.

00:35:33.530 --> 00:35:35.870
And make sure to mark
it with the hashtag

00:35:35.870 --> 00:35:40.340
GDDEurope and womentechmakers.

00:35:40.340 --> 00:35:43.460
Anyone wants to shout
out an idea that they

00:35:43.460 --> 00:35:47.290
may be doing differently?

00:35:47.290 --> 00:35:47.850
It's OK.

00:35:47.850 --> 00:35:48.350
Anyone?

00:35:51.370 --> 00:35:52.770
All right, OK.

00:35:52.770 --> 00:35:53.580
So with that--

00:35:53.580 --> 00:35:56.842
[APPLAUSE]

00:35:58.240 --> 00:36:02.680
OK, so with that, I'm going to
invite Dirk back to wrap up,

00:36:02.680 --> 00:36:04.709
and I hope you
enjoyed the session.

00:36:04.709 --> 00:36:07.703
[APPLAUSE AND CHEERING]

00:36:07.703 --> 00:36:11.196
[MUSIC PLAYING]

00:36:15.742 --> 00:36:16.700
DIRK PRIMBS: Thank you.

00:36:16.700 --> 00:36:19.550
And I believe I need to do this.

00:36:19.550 --> 00:36:23.090
There have been a few people
having quite an adrenaline rush

00:36:23.090 --> 00:36:23.970
back there.

00:36:23.970 --> 00:36:26.420
And Sowmya certainly
had the same, I think.

00:36:26.420 --> 00:36:29.510
Even Europeans can give
them an extra cheer, right?

00:36:29.510 --> 00:36:30.397
Come on.

00:36:30.397 --> 00:36:34.373
[APPLAUSE AND CHEERING]

00:36:37.355 --> 00:36:39.730
You know, that's like
presenter's nightmare,

00:36:39.730 --> 00:36:44.260
so I was really
feeling with her.

00:36:44.260 --> 00:36:46.810
The last step on our
two days' journey

00:36:46.810 --> 00:36:50.010
for me is now to say
a very warm thank you.

00:36:50.010 --> 00:36:53.270
It has been an
incredible two days.

00:36:53.270 --> 00:36:55.150
And we have a few
parting wishes.

00:36:55.150 --> 00:36:56.350
You heard Sowmya's wish.

00:36:56.350 --> 00:36:59.470
I have an additional--
please, don't be a stranger.

00:36:59.470 --> 00:37:02.740
There are several ways to
stay in touch with each other

00:37:02.740 --> 00:37:03.250
and with us.

00:37:03.250 --> 00:37:05.770
Continue the conversation
that you take with you

00:37:05.770 --> 00:37:07.290
out of this conference.

00:37:07.290 --> 00:37:09.809
The Women Techmakers, the GDGs--

00:37:09.809 --> 00:37:11.350
you name it, there
are multiple ways.

00:37:11.350 --> 00:37:13.390
And it's kind of
hard, if you have

00:37:13.390 --> 00:37:14.890
walked these halls
these past days,

00:37:14.890 --> 00:37:16.800
not to have bumped into them.

00:37:16.800 --> 00:37:18.635
And maybe you're
a member already,

00:37:18.635 --> 00:37:21.040
or maybe you're interested.

00:37:21.040 --> 00:37:22.420
Stay in touch.

00:37:22.420 --> 00:37:25.840
On your way out, there
are on the entrance,

00:37:25.840 --> 00:37:27.310
I've been told, a
couple of things

00:37:27.310 --> 00:37:29.080
you might consider grabbing.

00:37:29.080 --> 00:37:30.760
Apparently there's
some schwag left.

00:37:30.760 --> 00:37:33.220
So if you keep
your eyes open, it

00:37:33.220 --> 00:37:35.350
might be interesting
what you see.

00:37:35.350 --> 00:37:37.360
And with that,
please travel safely.

00:37:37.360 --> 00:37:40.870
Thank you for being
with us today,

00:37:40.870 --> 00:37:43.150
and yeah, maybe next time.

00:37:43.150 --> 00:37:43.960
Thank you.

00:37:43.960 --> 00:37:46.060
[APPLAUSE AND CHEERING]

00:37:46.060 --> 00:37:49.410
[MUSIC PLAYING]

