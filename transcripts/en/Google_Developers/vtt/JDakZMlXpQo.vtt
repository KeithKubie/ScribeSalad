WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.892
[MUSIC PLAYING]

00:00:07.405 --> 00:00:08.780
SABA ZAIDI: My
husband and I have

00:00:08.780 --> 00:00:12.800
been in a somewhat long-distance
relationship for a few years

00:00:12.800 --> 00:00:14.360
now.

00:00:14.360 --> 00:00:17.360
We talk on the phone
and message a lot.

00:00:17.360 --> 00:00:19.370
But perhaps what's
most enjoyable

00:00:19.370 --> 00:00:21.650
for me is when I visit him.

00:00:21.650 --> 00:00:24.890
I leave little notes
all over his apartment.

00:00:24.890 --> 00:00:26.910
And sometimes when
I'm home alone,

00:00:26.910 --> 00:00:29.030
he'll start changing the
color of the smart lights

00:00:29.030 --> 00:00:33.220
in my living room to let me
know that he's thinking of me.

00:00:33.220 --> 00:00:37.220
Our human conversations take
a lot of different forms.

00:00:37.220 --> 00:00:39.220
We don't just talk verbally.

00:00:39.220 --> 00:00:42.160
Depending on where we are
and what we're trying to say,

00:00:42.160 --> 00:00:45.610
we might write, hold
hands, or even change

00:00:45.610 --> 00:00:48.250
color of smart lights.

00:00:48.250 --> 00:00:50.470
It's these human
conversations that

00:00:50.470 --> 00:00:55.100
have inspired us to create
conversations with technology.

00:00:55.100 --> 00:00:57.800
It's no surprise then
that we're starting

00:00:57.800 --> 00:01:01.010
to incorporate more modalities
into our digital interactions

00:01:01.010 --> 00:01:03.450
as well.

00:01:03.450 --> 00:01:06.510
But with this increased number
of devices and complexity

00:01:06.510 --> 00:01:09.900
of interactions, it might
feel overwhelming to design

00:01:09.900 --> 00:01:11.330
for the Google Assistant.

00:01:15.170 --> 00:01:17.570
If you look at the
Google Assistant today,

00:01:17.570 --> 00:01:20.840
I can talk to it not
just through the speaker

00:01:20.840 --> 00:01:23.270
in my living room, but
also through my car

00:01:23.270 --> 00:01:24.590
or my headphones.

00:01:24.590 --> 00:01:28.090
I can tap on my
phone or my watch.

00:01:28.090 --> 00:01:30.700
But how do we design
for an increased number

00:01:30.700 --> 00:01:33.750
of complexity of devices?

00:01:33.750 --> 00:01:36.270
I'm Saba Zaidi, and I'm
an interaction designer

00:01:36.270 --> 00:01:38.230
on the Google Assistant team.

00:01:38.230 --> 00:01:40.330
I'll be talking about how
you can design actions

00:01:40.330 --> 00:01:43.870
across surfaces and give you
some frameworks and some tips.

00:01:43.870 --> 00:01:45.340
ULAS KIRAZCI: And hi, I'm Ulas.

00:01:45.340 --> 00:01:47.842
I'm an engineer on
Actions on Google.

00:01:47.842 --> 00:01:49.300
And later on, I'm
going to tell you

00:01:49.300 --> 00:01:51.610
how to build an action
using the design

00:01:51.610 --> 00:01:53.885
principle Saba will talk about.

00:01:53.885 --> 00:01:54.510
SABA ZAIDI: OK.

00:01:54.510 --> 00:01:57.510
So before we can get
started to understand

00:01:57.510 --> 00:02:00.090
how to design actions
across surfaces,

00:02:00.090 --> 00:02:01.740
we need to get a
better understanding

00:02:01.740 --> 00:02:04.770
of what those experiences
can look like.

00:02:04.770 --> 00:02:09.580
Let's start with a journey
through a user's day.

00:02:09.580 --> 00:02:12.410
You wake up in the morning
to the sound of an alarm

00:02:12.410 --> 00:02:14.570
ringing on your Google Home.

00:02:14.570 --> 00:02:16.910
Without even getting out
your blanket, you can say,

00:02:16.910 --> 00:02:19.420
hey, Google, stop.

00:02:19.420 --> 00:02:20.620
You get up, get ready.

00:02:20.620 --> 00:02:22.810
And as you're about
to head out, you

00:02:22.810 --> 00:02:25.450
want to make sure you
don't need an umbrella.

00:02:25.450 --> 00:02:28.180
So you turn to the smart
display in your hallway

00:02:28.180 --> 00:02:31.720
and you ask, hey, Google,
what's the weather today?

00:02:31.720 --> 00:02:34.240
You're able to hear a
summary, and also at a glance

00:02:34.240 --> 00:02:37.660
see the hourly forecast.

00:02:37.660 --> 00:02:40.630
As you're walking to your
car, you take out your phone

00:02:40.630 --> 00:02:43.930
and tap on it to ask the
Google Assistant to order

00:02:43.930 --> 00:02:46.547
your favorite from Starbucks.

00:02:46.547 --> 00:02:48.130
It's able to connect
you to Starbucks,

00:02:48.130 --> 00:02:49.754
where you can quickly
place your order.

00:02:52.560 --> 00:02:55.830
As you're driving, you want to
listen to your favorite podcast

00:02:55.830 --> 00:02:57.290
or the news.

00:02:57.290 --> 00:03:00.480
And you can ask Google Assistant
for help in a hands-free way.

00:03:00.480 --> 00:03:05.420
And it can connect you, for
example, to NPR News update.

00:03:05.420 --> 00:03:06.860
You go about your day.

00:03:06.860 --> 00:03:08.510
And when you come
back home, it's

00:03:08.510 --> 00:03:10.836
time to make dinner
for your family.

00:03:10.836 --> 00:03:12.710
You turn to the smart
display in your kitchen

00:03:12.710 --> 00:03:16.130
and you ask, for example,
Tasty for help with a recipe,

00:03:16.130 --> 00:03:18.990
like in this case, pizza bombs.

00:03:18.990 --> 00:03:21.830
Tasty comes back, and
you can hear and see

00:03:21.830 --> 00:03:23.960
on the screen
step-by-step instructions

00:03:23.960 --> 00:03:28.030
and the ingredients.

00:03:28.030 --> 00:03:30.810
After dinner, it's time
to unwind with your family

00:03:30.810 --> 00:03:31.800
in the living room.

00:03:31.800 --> 00:03:35.190
You decide to play a
Buzzfeed personality quiz.

00:03:35.190 --> 00:03:36.690
And it's a great
way for your family

00:03:36.690 --> 00:03:39.540
to get together around a shared
device and have some fun.

00:03:42.500 --> 00:03:44.040
And then you head to bed.

00:03:44.040 --> 00:03:46.530
You say, hey,
Google, good night.

00:03:46.530 --> 00:03:49.170
It's able to start your
custom bedtime routine, which

00:03:49.170 --> 00:03:51.390
includes, for example,
setting your alarms

00:03:51.390 --> 00:03:54.910
or telling you about
your day tomorrow.

00:03:54.910 --> 00:03:56.830
So as you can see, users
were able to interact

00:03:56.830 --> 00:04:00.060
with the Google Assistant
similar to human interactions

00:04:00.060 --> 00:04:04.050
in that there were a lot of
different ways and contexts.

00:04:04.050 --> 00:04:07.030
And although there were a
lot of different devices,

00:04:07.030 --> 00:04:08.710
there are some
overarching principles.

00:04:08.710 --> 00:04:11.440
So let's take a look at them.

00:04:11.440 --> 00:04:13.840
First, the experience
was familiar.

00:04:13.840 --> 00:04:16.570
Whether it was morning,
evening, or commute,

00:04:16.570 --> 00:04:19.570
users can access their favorite
Google Assistant actions

00:04:19.570 --> 00:04:23.065
whenever, wherever
they need them.

00:04:23.065 --> 00:04:25.750
The Assistant was available
in different contexts.

00:04:25.750 --> 00:04:28.930
You saw it being used
on the go and at home,

00:04:28.930 --> 00:04:30.900
up close and from
a distance, and

00:04:30.900 --> 00:04:33.432
in shared and private settings.

00:04:33.432 --> 00:04:35.140
So as you're thinking
about your actions,

00:04:35.140 --> 00:04:39.494
think about all the different
contexts it may be used in.

00:04:39.494 --> 00:04:42.240
And lastly, different
devices lend themselves

00:04:42.240 --> 00:04:44.510
to different modes
of interaction.

00:04:44.510 --> 00:04:46.770
Some are voice only,
some are visual,

00:04:46.770 --> 00:04:48.111
and some were a mix of both.

00:04:48.111 --> 00:04:50.610
And we'll talk a bit more about
the strengths and weaknesses

00:04:50.610 --> 00:04:53.580
of each of these
modalities in a bit.

00:04:53.580 --> 00:04:56.360
First, let's take a deeper look
at a couple of those devices

00:04:56.360 --> 00:04:59.850
and see how these
principles apply.

00:04:59.850 --> 00:05:01.800
You've already heard
about smart displays now.

00:05:01.800 --> 00:05:04.000
They were announced
earlier this year.

00:05:04.000 --> 00:05:05.790
And even though
it's a new device,

00:05:05.790 --> 00:05:08.850
users can expect it
to feel familiar.

00:05:08.850 --> 00:05:12.270
It's essentially like a
Google Home with a screen.

00:05:12.270 --> 00:05:15.720
It's designed to be used
at home from a distance

00:05:15.720 --> 00:05:17.890
and as a shared device.

00:05:17.890 --> 00:05:21.812
And as you can see in this
inspiration action example,

00:05:21.812 --> 00:05:23.520
even though there's
a screen, users still

00:05:23.520 --> 00:05:25.830
interact with the
device through voice.

00:05:25.830 --> 00:05:29.370
They don't have to tap through
complex app navigation.

00:05:29.370 --> 00:05:30.870
Instead, the
visuals are designed

00:05:30.870 --> 00:05:33.450
to be seen at a distance.

00:05:33.450 --> 00:05:35.670
And of course, the user
can walk up to the screen

00:05:35.670 --> 00:05:36.840
and touch it if they want.

00:05:39.350 --> 00:05:42.710
Next, let's take a
quick look at phones.

00:05:42.710 --> 00:05:44.330
One thing to note
here is that we're

00:05:44.330 --> 00:05:46.790
making phones more
visually assistive as well,

00:05:46.790 --> 00:05:49.460
similar to smart displays,
allowing for a greater

00:05:49.460 --> 00:05:52.120
focus on the content.

00:05:52.120 --> 00:05:55.840
These devices, as you know, are
great for use cases, on the go,

00:05:55.840 --> 00:05:59.699
up close, and in a
private environment.

00:05:59.699 --> 00:06:02.240
And users can interact with the
Google Assistant on the phone

00:06:02.240 --> 00:06:05.610
through both voice and visuals.

00:06:05.610 --> 00:06:07.320
So hopefully that gave
you a better sense

00:06:07.320 --> 00:06:09.510
of what the experiences
on Google Assistant

00:06:09.510 --> 00:06:12.780
look like across devices.

00:06:12.780 --> 00:06:15.420
Now, in order to design
for so many devices,

00:06:15.420 --> 00:06:20.240
it helps to have a vocabulary
to help categorize them.

00:06:20.240 --> 00:06:22.130
At Google, we use
this design framework

00:06:22.130 --> 00:06:24.530
called the multimodal spectrum.

00:06:24.530 --> 00:06:27.470
It helps us categorize devices
based on their interaction

00:06:27.470 --> 00:06:28.800
types.

00:06:28.800 --> 00:06:31.670
So on one end, you have
voice-only devices,

00:06:31.670 --> 00:06:33.980
like the Google Home and
other smart displays,

00:06:33.980 --> 00:06:37.080
that you have to
hear or talk to.

00:06:37.080 --> 00:06:39.630
On the other end, you
have visual-only devices,

00:06:39.630 --> 00:06:41.850
like a phone or a
Chromebook, that's

00:06:41.850 --> 00:06:43.860
on mute, and most watches.

00:06:43.860 --> 00:06:47.250
So you have to look at
these devices or touch them.

00:06:47.250 --> 00:06:50.330
And in the middle, you have
what we call multimodal devices

00:06:50.330 --> 00:06:53.600
in that they're a mix of both.

00:06:53.600 --> 00:06:56.870
Cars and smart displays that
rely primarily on voice,

00:06:56.870 --> 00:07:02.690
but have optional visuals, are
known as voice-forward devices.

00:07:02.690 --> 00:07:04.970
Phones and Chromebooks
with audio on,

00:07:04.970 --> 00:07:08.420
which can use a mix of
both voice and visuals,

00:07:08.420 --> 00:07:12.230
are known as intermodal devices.

00:07:12.230 --> 00:07:15.500
So now, we have a vocabulary
for categorizing these devices.

00:07:15.500 --> 00:07:17.970
But before we can start
designing for them,

00:07:17.970 --> 00:07:20.780
it helps to understand the
strengths and weaknesses

00:07:20.780 --> 00:07:23.880
of each of these modalities.

00:07:23.880 --> 00:07:27.440
Let's talk about voice first.

00:07:27.440 --> 00:07:29.920
Voice is great
for natural input.

00:07:29.920 --> 00:07:31.920
We've been using
it for millennia.

00:07:31.920 --> 00:07:34.800
Whether you're a kid,
a senior, or someone

00:07:34.800 --> 00:07:39.280
who's not tech savvy, it's
still really intuitive.

00:07:39.280 --> 00:07:42.160
It's great for hands-free,
far-field use cases,

00:07:42.160 --> 00:07:47.010
like setting a timer in the
kitchen while you're cooking.

00:07:47.010 --> 00:07:49.330
And it helps reduce
task navigation.

00:07:49.330 --> 00:07:51.960
So for example, if
you were out on a run,

00:07:51.960 --> 00:07:54.990
you could ask your favorite
Google action that's

00:07:54.990 --> 00:07:58.020
about fitness and ask
it about your workouts

00:07:58.020 --> 00:08:00.090
instead of having to
pull out your phone,

00:08:00.090 --> 00:08:03.786
navigate to that app, and
search for that answer.

00:08:03.786 --> 00:08:05.660
Similarly, you could
ask the Google Assistant

00:08:05.660 --> 00:08:07.487
to play the next
song without having

00:08:07.487 --> 00:08:08.570
to mess with any controls.

00:08:11.890 --> 00:08:13.840
So voice has a lot of benefits.

00:08:13.840 --> 00:08:14.700
It's great.

00:08:14.700 --> 00:08:16.686
But it does have
some limitations.

00:08:16.686 --> 00:08:18.060
And that's where
visuals come in.

00:08:20.790 --> 00:08:23.820
Think about the last
time you were at a cafe.

00:08:23.820 --> 00:08:26.910
You probably walked past all the
pastries, looked at the menu,

00:08:26.910 --> 00:08:30.120
and made eye contact
with the cashier.

00:08:30.120 --> 00:08:32.640
Think about how difficult
that interaction would be

00:08:32.640 --> 00:08:35.039
if it was just through voice.

00:08:35.039 --> 00:08:37.210
Have a listen at what
the menu would sound like

00:08:37.210 --> 00:08:38.460
if it was through voice alone.

00:08:41.314 --> 00:08:41.980
[AUDIO PLAYBACK]

00:08:41.980 --> 00:08:45.990
- Espresso, latte, vanilla
latte, cappuccino, mocha,

00:08:45.990 --> 00:08:49.500
Americano, flat white, hot
chocolate, black coffee,

00:08:49.500 --> 00:08:50.280
and tea.

00:08:50.280 --> 00:08:51.450
[END PLAYBACK]

00:08:51.450 --> 00:08:53.730
SABA ZAIDI: That feels
pretty overwhelming, right?

00:08:53.730 --> 00:08:55.480
It's like watching
all the options go by

00:08:55.480 --> 00:08:57.420
and catching the right one.

00:08:57.420 --> 00:09:00.420
It's like looking at a ticker.

00:09:00.420 --> 00:09:02.590
Voice is ephemeral and linear.

00:09:02.590 --> 00:09:05.710
And that makes it very difficult
to hold a lot of information

00:09:05.710 --> 00:09:07.920
in your head.

00:09:07.920 --> 00:09:09.690
By contrast, the
menu is a lot easier

00:09:09.690 --> 00:09:13.490
to scan if it looks like this.

00:09:13.490 --> 00:09:15.010
And you can imagine
that the problem

00:09:15.010 --> 00:09:19.660
gets compounded if you also had
to compare prices in calories.

00:09:19.660 --> 00:09:24.030
So visuals are great for
scanning and comparing.

00:09:24.030 --> 00:09:27.630
We also use them a lot to
reference objects in the world.

00:09:27.630 --> 00:09:30.570
So I can look at all the baked
goods and then point to the one

00:09:30.570 --> 00:09:31.800
that I want.

00:09:31.800 --> 00:09:34.530
Instead of, for example,
having to hear or say out

00:09:34.530 --> 00:09:38.261
loud something like, that small,
sugar cookie with a chocolate

00:09:38.261 --> 00:09:38.760
drizzle.

00:09:41.350 --> 00:09:44.490
So voice and visuals
both have their benefits.

00:09:44.490 --> 00:09:48.216
And it often is
useful to use both.

00:09:48.216 --> 00:09:51.740
So in this example, we usually
prefer to look at the menu

00:09:51.740 --> 00:09:56.340
but then talk to the cashier
in order to check out and pay.

00:09:56.340 --> 00:09:59.070
Similar benefits to using
both voice and visuals

00:09:59.070 --> 00:10:02.280
exist in the real world as
well or in the digital world

00:10:02.280 --> 00:10:03.630
as well.

00:10:03.630 --> 00:10:07.110
And that's what makes
multimodal devices, like phones

00:10:07.110 --> 00:10:11.550
and smart displays, such
a unique opportunity.

00:10:11.550 --> 00:10:14.430
By leveraging the best of
both voice and visuals,

00:10:14.430 --> 00:10:19.070
they're able to provide
really rich interactions.

00:10:19.070 --> 00:10:20.840
But how do we design
for them along

00:10:20.840 --> 00:10:24.820
with designing for speakers?

00:10:24.820 --> 00:10:26.350
One thing to keep
in mind, again,

00:10:26.350 --> 00:10:30.220
is to start with a
human conversation.

00:10:30.220 --> 00:10:33.700
You might have an app already,
but avoid the temptation

00:10:33.700 --> 00:10:35.770
to duplicate it.

00:10:35.770 --> 00:10:39.070
Instead, try to observe
a relevant conversation

00:10:39.070 --> 00:10:42.370
in the real world or role
play it with a colleague

00:10:42.370 --> 00:10:44.670
and write down that dialogue.

00:10:44.670 --> 00:10:47.360
You'll realize that not
everything that's in your app

00:10:47.360 --> 00:10:51.170
does well as conversation
or vice versa.

00:10:51.170 --> 00:10:53.720
Instead, think of your
action as a companion

00:10:53.720 --> 00:10:57.570
to your app that's faster
in certain use cases.

00:10:57.570 --> 00:11:00.220
I won't go into detail into
how to write good dialogue

00:11:00.220 --> 00:11:02.610
or create persona,
but I highly recommend

00:11:02.610 --> 00:11:04.780
you check out our brand
new conversation design

00:11:04.780 --> 00:11:06.540
website at that link there.

00:11:06.540 --> 00:11:10.720
It goes into great detail
into how to get started.

00:11:10.720 --> 00:11:14.740
So we've learned that we need
to create spoken dialogue

00:11:14.740 --> 00:11:16.670
and then add visuals to it.

00:11:16.670 --> 00:11:19.870
So let's take a look at an
example of how to do that

00:11:19.870 --> 00:11:22.850
and how that helps us scale.

00:11:22.850 --> 00:11:24.530
So if you haven't
already, I'd encourage

00:11:24.530 --> 00:11:27.050
you to check out
the Google I/O 2018

00:11:27.050 --> 00:11:30.710
action that helps you learn
more about this event.

00:11:30.710 --> 00:11:32.990
We started by writing
a spoken dialogue

00:11:32.990 --> 00:11:36.350
as if it was for a voice-only
device, like a Google Home.

00:11:36.350 --> 00:11:38.000
And it includes
terms like this one.

00:11:38.000 --> 00:11:40.760
So a user can say,
browse sessions.

00:11:40.760 --> 00:11:42.800
And we respond with
a spoken response

00:11:42.800 --> 00:11:44.750
like, here are some
of the topics left

00:11:44.750 --> 00:11:48.120
for today and so on.

00:11:48.120 --> 00:11:51.020
Now, in order to take this
dialogue and scale it,

00:11:51.020 --> 00:11:53.690
we need to take
every turn like this

00:11:53.690 --> 00:11:56.090
and think about all the
ways we can incorporate

00:11:56.090 --> 00:11:57.560
visual components to it.

00:11:57.560 --> 00:12:00.290
So this would include, for
example, display prompts,

00:12:00.290 --> 00:12:03.830
cards, and suggestion chips.

00:12:03.830 --> 00:12:06.710
In our example, we can
accompany that spoken prompt

00:12:06.710 --> 00:12:09.620
with a display prompt
like, "Which topic are you

00:12:09.620 --> 00:12:11.060
interested in?"

00:12:11.060 --> 00:12:15.230
This helps carry the written
conversation on a screen.

00:12:15.230 --> 00:12:18.350
We can add a list of
sessions as a card.

00:12:18.350 --> 00:12:20.930
A user could tap on
that, for example.

00:12:20.930 --> 00:12:24.530
And we could have a suggestion
chip like, "None of these."

00:12:24.530 --> 00:12:26.930
And this helps the user know
how to pivot or follow up

00:12:26.930 --> 00:12:29.320
a conversation.

00:12:29.320 --> 00:12:32.660
Once we've constructed our
response to have spoken

00:12:32.660 --> 00:12:36.080
and visual elements, we
can then map that response

00:12:36.080 --> 00:12:39.390
to the multimodal
spectrum from earlier.

00:12:39.390 --> 00:12:41.000
So depending on
whether the device

00:12:41.000 --> 00:12:44.060
has visual or
audio capabilities,

00:12:44.060 --> 00:12:48.096
or have important voices, we
can choose the right components.

00:12:48.096 --> 00:12:50.540
So you already saw
what our response would

00:12:50.540 --> 00:12:51.950
look like on a Google Home.

00:12:51.950 --> 00:12:55.167
We would simply have
the spoken prompt.

00:12:55.167 --> 00:12:56.750
Let's take a look
at what the response

00:12:56.750 --> 00:12:58.791
would look like across
some of the other devices.

00:13:01.250 --> 00:13:03.710
A smart display is a
voice-forward device.

00:13:03.710 --> 00:13:06.500
So we still need to show
all the spoken prompt

00:13:06.500 --> 00:13:10.440
and make it carry the
whole conversation.

00:13:10.440 --> 00:13:12.930
We don't really need a display
prompt anymore, especially

00:13:12.930 --> 00:13:14.490
if we're going to
have better visuals

00:13:14.490 --> 00:13:17.620
like the list and the chips.

00:13:17.620 --> 00:13:20.520
A phone, on the other
hand, is intermodal device.

00:13:20.520 --> 00:13:22.270
And we need to
have both the voice

00:13:22.270 --> 00:13:25.280
and the visuals carry
the conversation.

00:13:25.280 --> 00:13:27.280
In this case, you might
notice that we shortened

00:13:27.280 --> 00:13:29.830
the spoken prompt because we
can direct the user to look

00:13:29.830 --> 00:13:31.690
at the screen for more details.

00:13:31.690 --> 00:13:34.640
And Ulas will talk more
about how you can do that.

00:13:34.640 --> 00:13:36.640
And of course, the rest
of the visual components

00:13:36.640 --> 00:13:39.180
are there as well.

00:13:39.180 --> 00:13:41.870
And finally, if your
phone was on silent,

00:13:41.870 --> 00:13:44.120
we would simply ignore
the spoken prompt,

00:13:44.120 --> 00:13:46.610
and the visual components
are able to carry

00:13:46.610 --> 00:13:49.550
the complete conversation.

00:13:49.550 --> 00:13:52.990
So now, we learned that in
order to scale our dialogue,

00:13:52.990 --> 00:13:55.910
we need to write spoken prompts
and then add visuals to them.

00:13:55.910 --> 00:13:59.480
And that helps us
map across devices.

00:13:59.480 --> 00:14:03.600
But how do we know what
kinds of visuals to add?

00:14:03.600 --> 00:14:05.490
I'd like to leave you
with five tips for how

00:14:05.490 --> 00:14:09.690
you can incorporate
visuals into your dialogue.

00:14:09.690 --> 00:14:11.880
For that, let's
take this made up

00:14:11.880 --> 00:14:13.770
Assistant action called
the National Anthem

00:14:13.770 --> 00:14:15.870
Player on a smart display.

00:14:15.870 --> 00:14:18.780
As the name suggests, a
user can ask for a country,

00:14:18.780 --> 00:14:20.910
and it will come back
with the national anthem

00:14:20.910 --> 00:14:23.040
for that country.

00:14:23.040 --> 00:14:26.040
When you invoke this action,
it gives you a welcome message

00:14:26.040 --> 00:14:27.150
that sounds like this.

00:14:30.454 --> 00:14:31.120
[AUDIO PLAYBACK]

00:14:31.120 --> 00:14:33.220
- Welcome to National
Anthem Player.

00:14:33.220 --> 00:14:36.530
I can play the national anthems
from 20 different countries,

00:14:36.530 --> 00:14:40.150
including United States,
Canada, and the United Kingdom.

00:14:40.150 --> 00:14:41.500
Which would you like to hear?

00:14:41.500 --> 00:14:42.670
[END PLAYBACK]

00:14:42.670 --> 00:14:44.860
SABA ZAIDI: So as you
can see, the device

00:14:44.860 --> 00:14:46.570
is currently writing
on the screen

00:14:46.570 --> 00:14:48.030
what it's saying out loud.

00:14:48.030 --> 00:14:50.620
And this is really a missed
opportunity, especially

00:14:50.620 --> 00:14:52.540
given that by now, we've
learned that visuals

00:14:52.540 --> 00:14:55.870
have some strength over voice,
and that smart displays are

00:14:55.870 --> 00:15:00.270
great for showing rich,
immersive visuals.

00:15:00.270 --> 00:15:03.770
So tip number one is to consider
cards rather than just display

00:15:03.770 --> 00:15:05.130
prompts.

00:15:05.130 --> 00:15:09.030
In this case, we've swapped
out the words for a carousel.

00:15:09.030 --> 00:15:10.890
Users can quickly
browse through the list

00:15:10.890 --> 00:15:13.750
and select the country
that they want.

00:15:13.750 --> 00:15:16.520
You'll notice this is
similar to the menu example

00:15:16.520 --> 00:15:20.020
we looked at in the cafe, where
visuals are helping someone

00:15:20.020 --> 00:15:23.310
scan and compare options.

00:15:23.310 --> 00:15:26.420
Additionally, things like
maps, charts, and images

00:15:26.420 --> 00:15:28.430
are also great on
visual devices,

00:15:28.430 --> 00:15:30.710
because they are difficult
to describe through voice,

00:15:30.710 --> 00:15:34.090
similar to the cookie.

00:15:34.090 --> 00:15:37.990
Second, consider varying your
spoken and your display prompt.

00:15:37.990 --> 00:15:41.110
This is particularly useful for
devices that are intermodal,

00:15:41.110 --> 00:15:44.290
that might have a display
prompt next to a card.

00:15:44.290 --> 00:15:47.180
And some of that information
might be redundant.

00:15:47.180 --> 00:15:48.910
So in this case,
we're stripping out

00:15:48.910 --> 00:15:51.520
the examples for the
countries in the display

00:15:51.520 --> 00:15:54.425
prompt because the card already
shows a lot of examples.

00:15:56.970 --> 00:16:00.470
Third, consider visuals
for suggestions.

00:16:00.470 --> 00:16:03.450
Here, we know that the
user is a repeat user.

00:16:03.450 --> 00:16:06.470
So we're reordering the list
so that their most frequently

00:16:06.470 --> 00:16:09.230
visited countries show up first.

00:16:09.230 --> 00:16:11.960
We're also using suggestion
chips to allude to the user

00:16:11.960 --> 00:16:15.620
how they can follow up
or pivot a conversation.

00:16:15.620 --> 00:16:18.170
This kind of discovery can be
quite difficult through voice

00:16:18.170 --> 00:16:18.669
alone.

00:16:21.710 --> 00:16:25.250
Next, you can use visuals to
increase your brand expression.

00:16:25.250 --> 00:16:29.476
We used to allow you to change
your voice and choose a logo,

00:16:29.476 --> 00:16:31.100
but now we're also
going to be allowing

00:16:31.100 --> 00:16:34.550
you to choose a font and
the background image.

00:16:34.550 --> 00:16:37.310
And Ulas will talk more
about how you can do that.

00:16:37.310 --> 00:16:39.080
As you can see
here, the experience

00:16:39.080 --> 00:16:43.520
looks a lot more
custom and immersive.

00:16:43.520 --> 00:16:46.370
And lastly, visual devices
are great for carrying

00:16:46.370 --> 00:16:50.030
conversations that started
on a voice-only device.

00:16:50.030 --> 00:16:53.690
So for example, if I use
this national anthem action

00:16:53.690 --> 00:16:57.270
on a Google Home, and I
wanted to see the lyrics,

00:16:57.270 --> 00:17:00.440
the action can send a
notification after a few steps

00:17:00.440 --> 00:17:01.100
to my phone.

00:17:01.100 --> 00:17:05.170
And I can take out my
phone and read them there.

00:17:05.170 --> 00:17:07.240
So hopefully those
five tips will help you

00:17:07.240 --> 00:17:11.650
in incorporating more
visuals into your dialogue.

00:17:11.650 --> 00:17:13.410
Let's summarize what
we've learned so far

00:17:13.410 --> 00:17:16.829
on how to design actions
for the Assistant.

00:17:16.829 --> 00:17:19.290
First, users interact
with the Google Assistant

00:17:19.290 --> 00:17:22.000
in a variety of different
ways and contexts.

00:17:22.000 --> 00:17:24.780
So this could include
at home or on the go,

00:17:24.780 --> 00:17:28.420
up close, at a distance, or
through voice or visuals.

00:17:28.420 --> 00:17:30.970
In order to design across
so many modalities,

00:17:30.970 --> 00:17:33.550
it helps to keep in mind
the multimodal spectrum

00:17:33.550 --> 00:17:37.180
and think of your responses
as having visual components as

00:17:37.180 --> 00:17:39.740
well as spoken components.

00:17:39.740 --> 00:17:42.830
And lastly, learn and
leverage the strengths of each

00:17:42.830 --> 00:17:44.499
of these modalities.

00:17:44.499 --> 00:17:46.040
We learned, for
example, that visuals

00:17:46.040 --> 00:17:50.600
are great at scanning, brand
expression, and discovery.

00:17:50.600 --> 00:17:52.910
So instead of just
showing on the screen what

00:17:52.910 --> 00:17:57.630
you're saying out loud,
try to use cards instead.

00:17:57.630 --> 00:17:58.130
All right.

00:17:58.130 --> 00:17:59.630
Now, I'm going to hand
it over to my colleague,

00:17:59.630 --> 00:18:01.088
Ulas, who's going
to talk about how

00:18:01.088 --> 00:18:02.617
you can develop these actions.

00:18:02.617 --> 00:18:03.867
ULAS KIRAZCI: Thank you, Saba.

00:18:03.867 --> 00:18:05.815
[APPLAUSE]

00:18:10.690 --> 00:18:11.590
ULAS KIRAZCI: Hi.

00:18:11.590 --> 00:18:15.670
So we said that the Assistant
runs on many types of devices.

00:18:15.670 --> 00:18:18.520
And in the future, it
will run on many others.

00:18:18.520 --> 00:18:21.340
Luckily, we make it easy for you
to build your action in a way

00:18:21.340 --> 00:18:24.700
that it will run well on
all of these devices today,

00:18:24.700 --> 00:18:26.570
as well as devices
in the future.

00:18:26.570 --> 00:18:29.860
So let's go through an example.

00:18:29.860 --> 00:18:31.840
To walk you through
this, I'm going

00:18:31.840 --> 00:18:34.860
to use a test action I
created called California Surf

00:18:34.860 --> 00:18:38.450
Report that gives wave height
and weather information

00:18:38.450 --> 00:18:41.500
and beaches in
California for surfers.

00:18:41.500 --> 00:18:45.730
So currently, I only have spoken
responses, no visuals yet.

00:18:45.730 --> 00:18:49.780
So let's see what this sounds
like on a voice-only device

00:18:49.780 --> 00:18:50.930
like the Google Home.

00:18:54.014 --> 00:18:54.680
[AUDIO PLAYBACK]

00:18:54.680 --> 00:18:56.890
- --looks fair
for most of today.

00:18:56.890 --> 00:18:59.290
Waves will be from two to
three feet in the morning,

00:18:59.290 --> 00:19:01.900
to three to four feet
in the afternoon.

00:19:01.900 --> 00:19:05.920
Expect waist-high swell in the
morning with northwest winds.

00:19:05.920 --> 00:19:08.950
Shoulder-high surf in the
afternoon with southwest winds.

00:19:08.950 --> 00:19:10.521
[END PLAYBACK]

00:19:10.521 --> 00:19:11.520
ULAS KIRAZCI: OK, great.

00:19:11.520 --> 00:19:12.820
Pretty informative.

00:19:12.820 --> 00:19:15.480
So now, let's take
a look at what

00:19:15.480 --> 00:19:19.500
this sounds like and looks
like on a voice-forward device,

00:19:19.500 --> 00:19:21.480
like a smart display.

00:19:21.480 --> 00:19:22.520
Switch to demo, please.

00:19:26.960 --> 00:19:28.430
OK, Google.

00:19:28.430 --> 00:19:29.760
Talk to California Surf Report.

00:19:32.875 --> 00:19:33.750
GOOGLE ASSISTANT: OK.

00:19:33.750 --> 00:19:37.440
Let's get the test version
of California Surf Report.

00:19:37.440 --> 00:19:40.622
GOOGLE ASSISTANT: Welcome
to California Surf Report.

00:19:40.622 --> 00:19:42.830
ULAS KIRAZCI: Tell me the
surf report for Santa Cruz.

00:19:45.461 --> 00:19:47.210
GOOGLE ASSISTANT: Surf
in Santa Cruz Beach

00:19:47.210 --> 00:19:49.130
looks fair for most of today.

00:19:49.130 --> 00:19:51.550
Waves will be from two to
three feet in the morning,

00:19:51.550 --> 00:19:53.642
to three to four feet
in the afternoon.

00:19:53.642 --> 00:19:54.350
ULAS KIRAZCI: OK.

00:19:54.350 --> 00:19:57.680
I cut it short because I
think we all get the idea.

00:19:57.680 --> 00:19:59.480
So as you can see,
spoken responses

00:19:59.480 --> 00:20:01.790
are a good way to get
started and working well

00:20:01.790 --> 00:20:02.555
on many devices.

00:20:05.240 --> 00:20:08.330
But when we have a screen,
we can make it a lot better.

00:20:08.330 --> 00:20:10.320
So let's see how.

00:20:10.320 --> 00:20:13.750
One of the best visuals we can
add and the easiest ones to add

00:20:13.750 --> 00:20:15.280
is a BasicCard.

00:20:15.280 --> 00:20:17.800
And here's an example
from the Node.js client

00:20:17.800 --> 00:20:21.490
library of how to add a
BasicCard to your responses.

00:20:21.490 --> 00:20:24.460
So we start with a
spoken prompt as usual

00:20:24.460 --> 00:20:28.370
and the second statement
of ask as a BasicCard.

00:20:28.370 --> 00:20:32.020
A BasicCard can have a
title, subtitle, body text,

00:20:32.020 --> 00:20:34.370
and optional image.

00:20:34.370 --> 00:20:37.210
So let's see what this looks
like on our smart display

00:20:37.210 --> 00:20:37.805
again.

00:20:37.805 --> 00:20:39.180
Let's switch to
the demo, please.

00:20:43.540 --> 00:20:45.000
OK, Google.

00:20:45.000 --> 00:20:47.350
Show me the surf
report for Santa Cruz.

00:20:50.241 --> 00:20:51.990
GOOGLE ASSISTANT: Surf
in Santa Cruz Beach

00:20:51.990 --> 00:20:53.910
looks fair for most of today.

00:20:53.910 --> 00:20:56.310
Waves will be from two to
three feet in the morning,

00:20:56.310 --> 00:20:58.920
to three to four feet
in the afternoon.

00:20:58.920 --> 00:21:02.940
Expect waist-high swell in the
morning with northwest winds,

00:21:02.940 --> 00:21:05.940
shoulder-high surf in the
afternoon with southwest winds.

00:21:08.880 --> 00:21:11.830
ULAS KIRAZCI: OK, great.

00:21:11.830 --> 00:21:13.580
Now, it looks much better.

00:21:13.580 --> 00:21:17.650
We have a nice visual for it
rather than huge lines of text.

00:21:21.010 --> 00:21:23.170
And here are some other
kinds of cards you can use.

00:21:23.170 --> 00:21:25.450
There's a carousel
on this card that

00:21:25.450 --> 00:21:28.090
allows you to display
a set of things

00:21:28.090 --> 00:21:29.660
that the user can choose from.

00:21:29.660 --> 00:21:32.490
There's also a newly
introduced table card.

00:21:32.490 --> 00:21:35.140
Another great way to add
visuals to your action

00:21:35.140 --> 00:21:36.850
is to use suggestion chips.

00:21:36.850 --> 00:21:39.970
Suggestion chips allow
the user to understand

00:21:39.970 --> 00:21:42.820
what they can do in this
turn in the conversation.

00:21:42.820 --> 00:21:45.310
And also, they
simplify user input.

00:21:45.310 --> 00:21:47.560
You can learn more about
responses at the link.

00:21:51.490 --> 00:21:54.290
And by the way, all of
this, like I promised,

00:21:54.290 --> 00:21:57.050
works equally well on
an intermodal device,

00:21:57.050 --> 00:21:58.430
like a phone.

00:21:58.430 --> 00:22:01.280
As you can see,
we have formatted

00:22:01.280 --> 00:22:08.880
the font sizes and the layout to
fit the intermodal form factor.

00:22:08.880 --> 00:22:10.140
OK, great.

00:22:10.140 --> 00:22:12.090
So next, maybe
what we want to do

00:22:12.090 --> 00:22:13.796
is shorten the
spoken response a bit

00:22:13.796 --> 00:22:16.170
because it's a bit repetitive
with the information that's

00:22:16.170 --> 00:22:17.580
already on the card.

00:22:17.580 --> 00:22:19.860
Users can just look at
the display for these.

00:22:23.090 --> 00:22:24.340
So how do we do this?

00:22:24.340 --> 00:22:27.940
We have a feature in the
API called capabilities.

00:22:27.940 --> 00:22:30.430
So instead of thinking,
if Google Home,

00:22:30.430 --> 00:22:33.470
do this, else if
smart display do that,

00:22:33.470 --> 00:22:36.970
think about what capabilities
the surface that the user is

00:22:36.970 --> 00:22:39.070
interacting with you on have.

00:22:39.070 --> 00:22:40.330
Does it have a screen?

00:22:40.330 --> 00:22:42.910
Can it output audio?

00:22:42.910 --> 00:22:46.680
The capabilities of the
device are reported to you

00:22:46.680 --> 00:22:48.510
in every webhook
call, so you get

00:22:48.510 --> 00:22:51.570
to know what this is on
every conversation turn.

00:22:51.570 --> 00:22:54.600
And here's a sample list of
capabilities that we support.

00:22:54.600 --> 00:22:59.230
And you can learn
more at the link.

00:22:59.230 --> 00:23:01.300
So in our use case,
what we're looking for

00:23:01.300 --> 00:23:03.490
is the screen output capability.

00:23:03.490 --> 00:23:06.040
This indicates that the
user device has a screen.

00:23:06.040 --> 00:23:08.290
So we can show them a card.

00:23:08.290 --> 00:23:10.810
Oh, and by the way, if you
don't want your responses

00:23:10.810 --> 00:23:13.960
to differentiate between
ones with displays and ones

00:23:13.960 --> 00:23:16.690
that are voice only, you
can always add a card,

00:23:16.690 --> 00:23:19.660
and we'll just trip it
out for you silently.

00:23:19.660 --> 00:23:21.850
So this makes it easy
for you to build if you

00:23:21.850 --> 00:23:25.450
don't want to differentiate.

00:23:25.450 --> 00:23:27.940
And here's, again, the
Node.js client library

00:23:27.940 --> 00:23:29.960
snippet that shows
how to use this.

00:23:29.960 --> 00:23:32.170
So in the first if
statement, we determined

00:23:32.170 --> 00:23:35.170
that the user's device
does not have a screen.

00:23:35.170 --> 00:23:39.220
So we have the full content
in the spoken response.

00:23:39.220 --> 00:23:43.210
In the else clause, we know
that there is a screen.

00:23:43.210 --> 00:23:45.890
So we shorten the
spoken response

00:23:45.890 --> 00:23:49.480
and end it with a phrase
like, "Here's the report"

00:23:49.480 --> 00:23:51.400
to lead the user to the screen.

00:23:51.400 --> 00:23:55.510
And then we append the
BasicCard to the response.

00:23:55.510 --> 00:23:59.210
So let's see how this looks
like on our smart display now.

00:23:59.210 --> 00:24:00.410
Switch to demo, please.

00:24:04.540 --> 00:24:06.400
OK, Google.

00:24:06.400 --> 00:24:09.730
Show me the shortened surf
report for Santa Cruz.

00:24:12.420 --> 00:24:14.170
GOOGLE ASSISTANT: Surf
in Santa Cruz Beach

00:24:14.170 --> 00:24:16.870
looks fair for most of
today, with two to three foot

00:24:16.870 --> 00:24:18.940
waves in the morning
and three to four foot

00:24:18.940 --> 00:24:20.960
waves in the afternoon.

00:24:20.960 --> 00:24:21.730
Here's the report.

00:24:24.330 --> 00:24:26.300
ULAS KIRAZCI: So that
sounds a lot more concise

00:24:26.300 --> 00:24:27.050
and user friendly.

00:24:32.560 --> 00:24:34.350
Another way you can
use capabilities

00:24:34.350 --> 00:24:36.790
is to require that
your action only

00:24:36.790 --> 00:24:38.890
run on devices that
have the capability.

00:24:38.890 --> 00:24:41.192
This is what we call
static capabilities.

00:24:41.192 --> 00:24:43.150
And you can configure
these through the Actions

00:24:43.150 --> 00:24:45.910
on Google console,
as you can see here.

00:24:45.910 --> 00:24:49.570
But only use this if your
action absolutely makes no sense

00:24:49.570 --> 00:24:51.200
without that capability.

00:24:51.200 --> 00:24:55.150
So for example, the National
Anthem Player action

00:24:55.150 --> 00:24:59.020
that Saba talked about would
not make sense on a device

00:24:59.020 --> 00:25:00.380
without audio.

00:25:00.380 --> 00:25:02.660
So this would be a
good place to use that.

00:25:02.660 --> 00:25:05.110
However, for the
surf report app,

00:25:05.110 --> 00:25:08.290
it equally works well on
voice-only and display-only

00:25:08.290 --> 00:25:09.150
devices.

00:25:09.150 --> 00:25:11.320
So it wouldn't be a
good place to use this.

00:25:11.320 --> 00:25:14.920
You can configure all this using
the Actions on developer Google

00:25:14.920 --> 00:25:15.420
console.

00:25:18.710 --> 00:25:22.430
Another high quality and easy
way to target multiple surfaces

00:25:22.430 --> 00:25:26.100
is to use Google
libraries we call helpers.

00:25:26.100 --> 00:25:30.830
So I've been asking California
Surf Report the surf

00:25:30.830 --> 00:25:32.450
report with the beach name.

00:25:32.450 --> 00:25:35.480
But if I don't say the
beach name, I get a prompt,

00:25:35.480 --> 00:25:37.150
"Which beach?"

00:25:37.150 --> 00:25:39.920
Now, this doesn't tell
me what I can say.

00:25:39.920 --> 00:25:43.910
It doesn't tell me which beaches
this action actually supports.

00:25:43.910 --> 00:25:47.720
So we can fix that with a
helper called askWithCarousel.

00:25:47.720 --> 00:25:52.370
What askWithCarousel does
is it presents the user

00:25:52.370 --> 00:25:54.710
with a list of
options to pick from

00:25:54.710 --> 00:25:57.860
and associate visuals
with each item.

00:25:57.860 --> 00:26:02.440
In addition, when the
user utters the query

00:26:02.440 --> 00:26:06.380
to select an item, Google
does the matching of the query

00:26:06.380 --> 00:26:06.980
to the item.

00:26:06.980 --> 00:26:09.110
So we can deal with
variations in how

00:26:09.110 --> 00:26:12.970
people pick things much better.

00:26:12.970 --> 00:26:14.980
So let's make our
prompt better with

00:26:14.980 --> 00:26:18.460
the askWithCarousel helper.

00:26:18.460 --> 00:26:21.670
And again, the Node.js
library snippet here.

00:26:21.670 --> 00:26:25.150
We start with the spoken
response with the prompt.

00:26:25.150 --> 00:26:28.900
And we add a carousel to it.

00:26:28.900 --> 00:26:31.150
And the carousel is
made up of items.

00:26:31.150 --> 00:26:33.310
And each item has
a list of phrases

00:26:33.310 --> 00:26:36.970
that you think the user
might say to match this item

00:26:36.970 --> 00:26:39.520
and visuals associated
with each item

00:26:39.520 --> 00:26:43.770
so the user can understand
what they're about to tap on.

00:26:43.770 --> 00:26:46.290
So let's switch to the demo
and see what this looks like.

00:26:52.100 --> 00:26:53.170
OK, Google.

00:26:53.170 --> 00:26:56.920
Show me the surf report.

00:26:56.920 --> 00:27:00.904
GOOGLE ASSISTANT: Which beach do
you want to see the report for?

00:27:00.904 --> 00:27:02.570
ULAS KIRAZCI: OK, so
this is the example

00:27:02.570 --> 00:27:05.990
where I'm a little
confused as a user.

00:27:05.990 --> 00:27:07.540
OK, Google.

00:27:07.540 --> 00:27:08.880
Show me the beach carousel.

00:27:10.482 --> 00:27:12.940
GOOGLE ASSISTANT: Which beach
would you like to know about?

00:27:15.370 --> 00:27:16.370
ULAS KIRAZCI: OK, great.

00:27:16.370 --> 00:27:18.380
Now it's much easier
for me to understand

00:27:18.380 --> 00:27:20.990
what the possible options
are, and even tap on one

00:27:20.990 --> 00:27:23.550
if I want to go with that.

00:27:27.970 --> 00:27:30.700
And we always continuously
improve the experience

00:27:30.700 --> 00:27:32.270
with these helpers.

00:27:32.270 --> 00:27:35.800
So this is one of the
advantages of helpers,

00:27:35.800 --> 00:27:38.834
is that we continue to
modify them to optimize them

00:27:38.834 --> 00:27:39.375
for surfaces.

00:27:41.910 --> 00:27:47.010
Now, since we launched
the current API last year,

00:27:47.010 --> 00:27:49.700
we've since come up
with smart displays.

00:27:49.700 --> 00:27:52.050
As you noticed on
smart displays,

00:27:52.050 --> 00:27:55.260
each conversation turn
takes up the entire screen.

00:27:55.260 --> 00:27:59.490
So given this fact, maybe we can
make our visuals more branded

00:27:59.490 --> 00:28:01.750
and give them a
little bit more flair.

00:28:01.750 --> 00:28:06.110
So we're introducing
styling options this year.

00:28:06.110 --> 00:28:07.905
So here's how it works.

00:28:07.905 --> 00:28:09.280
Let's switch to
the demo, please.

00:28:13.860 --> 00:28:17.250
So here is a new
tab in the Actions

00:28:17.250 --> 00:28:20.430
on Google console called
theme customization.

00:28:20.430 --> 00:28:23.470
You can modify the background
color, primary color--

00:28:23.470 --> 00:28:25.860
so that's like the font
color of the text--

00:28:25.860 --> 00:28:29.020
and the typography, and
even set a background image.

00:28:29.020 --> 00:28:30.580
So let's try a few things here.

00:28:30.580 --> 00:28:35.520
Let's say we want to
make this cursive.

00:28:35.520 --> 00:28:37.170
Let's add a background image.

00:28:45.834 --> 00:28:51.500
OK, so this is the landscape
aspect ratio image.

00:28:51.500 --> 00:28:55.820
And then we want to add
a portrait image as well.

00:29:04.997 --> 00:29:06.510
All right, all set.

00:29:06.510 --> 00:29:08.280
Now, all we have to do is save.

00:29:12.510 --> 00:29:15.480
And then we click
Test right here

00:29:15.480 --> 00:29:17.330
to update our test version.

00:29:22.930 --> 00:29:26.020
All right, now, let's see what
this looks like on the demo.

00:29:28.880 --> 00:29:30.340
OK, Google.

00:29:30.340 --> 00:29:35.211
Show me the surf
report for Santa Cruz.

00:29:35.211 --> 00:29:36.960
GOOGLE ASSISTANT: Surf
in Santa Cruz Beach

00:29:36.960 --> 00:29:39.630
looks fair for most of
today, with two to three foot

00:29:39.630 --> 00:29:41.730
waves in the morning
and three to four foot

00:29:41.730 --> 00:29:43.531
waves in the afternoon.

00:29:43.531 --> 00:29:44.280
Here's the report.

00:29:46.890 --> 00:29:49.139
ULAS KIRAZCI: I think now
that looks really beautiful.

00:29:55.990 --> 00:29:59.390
Smart displays are coming
out later this year.

00:29:59.390 --> 00:30:01.270
However, you can start
building your action

00:30:01.270 --> 00:30:05.200
against these visuals today,
using the updated simulator.

00:30:05.200 --> 00:30:10.000
So we've added a new simulator
device type for smart displays,

00:30:10.000 --> 00:30:11.960
as you can see here.

00:30:11.960 --> 00:30:14.200
And we've also
added a display tab,

00:30:14.200 --> 00:30:16.510
which shows you the
full-screen version of what you

00:30:16.510 --> 00:30:18.490
would get on a smart display.

00:30:18.490 --> 00:30:21.520
You can also use
this with a phone.

00:30:21.520 --> 00:30:23.410
And on the left
side as usual, you

00:30:23.410 --> 00:30:26.500
have the spoken responses,
as well as an input box

00:30:26.500 --> 00:30:28.420
where you can put user queries.

00:30:33.470 --> 00:30:37.260
One last thing, we said that
Assistant is in many places.

00:30:37.260 --> 00:30:39.710
So if the user's
interacting with your action

00:30:39.710 --> 00:30:42.530
using a voice-only
device, maybe they also

00:30:42.530 --> 00:30:47.210
have a device that has a display
on it, for example, a phone.

00:30:47.210 --> 00:30:51.110
So what if, in your current
turn in the conversation,

00:30:51.110 --> 00:30:54.760
you really want to have your
response display something?

00:30:54.760 --> 00:30:58.940
So for example, in the
surf report action,

00:30:58.940 --> 00:31:01.890
the user might ask us
for the full report.

00:31:01.890 --> 00:31:05.515
And we want to return the hour
by hour wave height graph.

00:31:05.515 --> 00:31:06.390
So how do we do that?

00:31:10.380 --> 00:31:12.420
There's a feature
in the API called

00:31:12.420 --> 00:31:14.220
multi-surface conversations.

00:31:14.220 --> 00:31:17.570
And here's how it works.

00:31:17.570 --> 00:31:19.920
In each API call
to your webhook,

00:31:19.920 --> 00:31:22.830
we report not only
the capabilities

00:31:22.830 --> 00:31:26.850
of the device that the
user's currently using,

00:31:26.850 --> 00:31:30.060
but the union of the
capabilities of all the devices

00:31:30.060 --> 00:31:32.010
the user owns.

00:31:32.010 --> 00:31:35.790
So in this example, what you
see is that the current user

00:31:35.790 --> 00:31:40.290
device only has a voice output
capability and has no screen.

00:31:40.290 --> 00:31:42.870
But in available surfaces,
we can see the screen output

00:31:42.870 --> 00:31:43.770
capability.

00:31:43.770 --> 00:31:45.810
So the user seems to
have another device

00:31:45.810 --> 00:31:48.050
with a screen on it.

00:31:48.050 --> 00:31:49.100
So how do we use this?

00:31:52.580 --> 00:31:54.980
Again, in the client
library we have a function

00:31:54.980 --> 00:32:00.110
to help you to inspect if the
user has a certain capability

00:32:00.110 --> 00:32:02.040
among their devices.

00:32:02.040 --> 00:32:03.770
Now, we determine
that the user has

00:32:03.770 --> 00:32:05.470
a device with this capability.

00:32:05.470 --> 00:32:07.470
How do we transfer the
user to the other device?

00:32:10.040 --> 00:32:13.790
We have a function, ask for
new surface, that does this.

00:32:13.790 --> 00:32:16.430
And you can give it
a notification that

00:32:16.430 --> 00:32:18.710
will appear on
the target device,

00:32:18.710 --> 00:32:21.470
in addition to the
list of capabilities

00:32:21.470 --> 00:32:26.840
you require for continuing
your conversation.

00:32:26.840 --> 00:32:29.850
I'm not going to demo this,
but here's what it looks like.

00:32:29.850 --> 00:32:32.170
So let's say the user said,
show me the full report.

00:32:32.170 --> 00:32:34.003
And they're talking to
you on a Google Home.

00:32:36.230 --> 00:32:40.250
So you would call the
ask new surface function

00:32:40.250 --> 00:32:42.860
that I showed you earlier.

00:32:42.860 --> 00:32:48.380
And we ask the user permission
to send the conversation over

00:32:48.380 --> 00:32:49.460
to the user's phone.

00:32:49.460 --> 00:32:51.980
And if the user
accepts, then there's

00:32:51.980 --> 00:32:55.220
a notification sent
to the new device.

00:32:55.220 --> 00:32:58.650
The conversation ends
on the current device.

00:32:58.650 --> 00:33:01.370
And when the user
taps the notification,

00:33:01.370 --> 00:33:03.650
then they resume
the conversation

00:33:03.650 --> 00:33:07.110
from where you left
off, like this.

00:33:07.110 --> 00:33:09.950
Note that this is not
just for single responses.

00:33:09.950 --> 00:33:12.800
This works equally
well when you want

00:33:12.800 --> 00:33:14.210
to continue the conversation.

00:33:14.210 --> 00:33:16.751
So we bring the full context
over so you can continue

00:33:16.751 --> 00:33:17.750
from where you left off.

00:33:22.030 --> 00:33:26.790
So to sum up, we've built a
lot of features in the API

00:33:26.790 --> 00:33:29.040
for you to add visuals
to your responses.

00:33:29.040 --> 00:33:30.840
So please use them.

00:33:30.840 --> 00:33:34.560
And we make it such that
we take your responses

00:33:34.560 --> 00:33:37.490
and optimize them as best
as possible to all these

00:33:37.490 --> 00:33:39.810
surfaces, and
surfaces in the future

00:33:39.810 --> 00:33:42.240
without extra work from you.

00:33:42.240 --> 00:33:44.670
And if you wanted to
customize your responses,

00:33:44.670 --> 00:33:48.660
always think of capabilities
and not individual device types.

00:33:48.660 --> 00:33:54.030
This way, we can run your
action on new devices

00:33:54.030 --> 00:33:55.680
without any extra work from you.

00:34:01.833 --> 00:34:02.624
SABA ZAIDI: Thanks.

00:34:02.624 --> 00:34:04.608
[APPLAUSE]

00:34:07.088 --> 00:34:08.100
Thank you.

00:34:08.100 --> 00:34:11.639
So I'd just like to end
with an invitation for you.

00:34:11.639 --> 00:34:14.730
Next time you order
coffee at a cafe

00:34:14.730 --> 00:34:17.250
or do a presentation
like this one,

00:34:17.250 --> 00:34:20.580
start to notice all the
different modes of interactions

00:34:20.580 --> 00:34:22.620
you use every day.

00:34:22.620 --> 00:34:25.710
And let the richness of
those human conversations

00:34:25.710 --> 00:34:29.250
inspire how you design
actions for your users.

00:34:29.250 --> 00:34:32.429
And help us evolve what it
means to have conversations

00:34:32.429 --> 00:34:35.110
with technology.

00:34:35.110 --> 00:34:37.280
Here's some links to
resources we mentioned

00:34:37.280 --> 00:34:39.300
and how to give feedback.

00:34:39.300 --> 00:34:41.310
So also come just talk to us.

00:34:41.310 --> 00:34:43.550
We'll be with our team
at the Assistant office

00:34:43.550 --> 00:34:46.639
hours and Sandboxes, ready
to answer your questions

00:34:46.639 --> 00:34:48.370
and showing off
some of the devices.

00:34:48.370 --> 00:34:49.825
So thank you and good luck.

00:34:49.825 --> 00:34:50.825
ULAS KIRAZCI: Thank you.

00:34:50.825 --> 00:34:53.896
[MUSIC PLAYING]

