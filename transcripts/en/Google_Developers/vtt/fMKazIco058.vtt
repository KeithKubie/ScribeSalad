WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.720
[MUSIC PLAYING]

00:00:06.349 --> 00:00:07.890
JACQUELINE FULLER:
Welcome, everyone.

00:00:07.890 --> 00:00:11.250
Thanks so much for joining us.

00:00:11.250 --> 00:00:12.950
I'm Jaclyn Jacqueline
Fuller, and I'm

00:00:12.950 --> 00:00:15.320
the President of Google.org.

00:00:15.320 --> 00:00:17.690
We're Google's
philanthropy, and we've

00:00:17.690 --> 00:00:22.160
focused on exactly the topic
of today's secession-- building

00:00:22.160 --> 00:00:25.280
for everyone using tech
to change the world.

00:00:25.280 --> 00:00:29.270
Google.org invests
money as well as

00:00:29.270 --> 00:00:31.850
our Googlers, the
time of our Googlers,

00:00:31.850 --> 00:00:33.740
especially our tech Googlers.

00:00:33.740 --> 00:00:37.010
You might have heard
Sundar mention recently

00:00:37.010 --> 00:00:40.970
that we're investing a billion
dollars over the next five

00:00:40.970 --> 00:00:45.620
years on Google.org so
we can help find and fund

00:00:45.620 --> 00:00:48.620
some of the best innovators
around the world who

00:00:48.620 --> 00:00:51.740
are using technology
in really clever ways

00:00:51.740 --> 00:00:53.490
to change the world.

00:00:53.490 --> 00:01:00.380
And today, we're going to meet
a few of our partners, grantees,

00:01:00.380 --> 00:01:02.760
who are doing exactly that.

00:01:02.760 --> 00:01:04.610
So why don't we
start by just hearing

00:01:04.610 --> 00:01:08.120
a little bit about each one of
you, and telling your story.

00:01:08.120 --> 00:01:10.550
Maybe Jess, we can
start with you.

00:01:10.550 --> 00:01:13.670
And if you could give
us just maybe a sentence

00:01:13.670 --> 00:01:17.840
on who you are personally,
how you came to this issue.

00:01:17.840 --> 00:01:21.170
And maybe a sentence about the
problem, and then a sentence

00:01:21.170 --> 00:01:23.300
about what the
solution looks like.

00:01:23.300 --> 00:01:24.350
JESS LADD: Great.

00:01:24.350 --> 00:01:28.100
Hi, my name is Jess Lad,
and I'm the founder and CEO

00:01:28.100 --> 00:01:29.650
of Callisto.

00:01:29.650 --> 00:01:34.730
Callisto is a online sexual
misconduct reporting platform

00:01:34.730 --> 00:01:37.340
that helps find
serial offenders,

00:01:37.340 --> 00:01:39.410
so that if a victim
of sexual misconduct

00:01:39.410 --> 00:01:42.590
doesn't want to come forward
unless they know that there's

00:01:42.590 --> 00:01:45.410
another person, we connect
them with other victims,

00:01:45.410 --> 00:01:47.420
and with their options
for taking action

00:01:47.420 --> 00:01:49.410
to protect their community.

00:01:49.410 --> 00:01:52.070
And I came to this work
because I was sexually

00:01:52.070 --> 00:01:56.570
assaulted in college, and I went
through the reporting process.

00:01:56.570 --> 00:02:00.080
And I ended up finding
the act of reporting

00:02:00.080 --> 00:02:02.670
to be more traumatic
than the assault.

00:02:02.670 --> 00:02:05.490
So trying to figure out
how, when somebody has faced

00:02:05.490 --> 00:02:08.690
something like this, they can
use a system that actually

00:02:08.690 --> 00:02:11.780
rebuilds their sense of
agency and empowerment,

00:02:11.780 --> 00:02:14.120
rather than taking it
away all over again.

00:02:14.120 --> 00:02:16.940
And ultimately,
helps find an end

00:02:16.940 --> 00:02:20.990
stop particularly serial
offenders, but all offenders

00:02:20.990 --> 00:02:25.250
and create a real deterrent
towards this type of behavior

00:02:25.250 --> 00:02:27.230
towards other human beings.

00:02:27.230 --> 00:02:31.490
And we launched Callisto three
years ago on college campuses,

00:02:31.490 --> 00:02:33.590
and this summer,
we're expanding it

00:02:33.590 --> 00:02:39.710
to address sexual assault and
coercion in the tech industry.

00:02:39.710 --> 00:02:42.480
JACQUELINE FULLER: All
right, thanks, [INAUDIBLE]..

00:02:42.480 --> 00:02:43.380
Right?

00:02:43.380 --> 00:02:45.160
Right?

00:02:45.160 --> 00:02:47.510
OLIVER HURST HILLER: Hi,
I'm Oliver Hurst Hiller.

00:02:47.510 --> 00:02:50.420
I run engineering and
product at DonorsChoose.org.

00:02:50.420 --> 00:02:53.150
Thank you to Jacqueline
and Google.org

00:02:53.150 --> 00:02:55.042
for hosting us today.

00:02:55.042 --> 00:02:57.500
Honored to be up here with
these other social entrepreneurs

00:02:57.500 --> 00:02:59.380
and change makers.

00:02:59.380 --> 00:03:00.290
DonorsChoose.org.

00:03:00.290 --> 00:03:02.704
We are like Kickstarter
or GoFundMe,

00:03:02.704 --> 00:03:04.370
except that we're a
nonprofit, and we're

00:03:04.370 --> 00:03:08.090
focused exclusively on public
schools here in the US.

00:03:08.090 --> 00:03:10.497
Teachers bring their best
ideas to our website for things

00:03:10.497 --> 00:03:12.830
they want to do with their
classroom for their classroom

00:03:12.830 --> 00:03:14.600
that their schools
can't otherwise afford,

00:03:14.600 --> 00:03:17.540
and we help bring the
dollars to those projects

00:03:17.540 --> 00:03:19.580
to help bring them to life.

00:03:19.580 --> 00:03:22.670
At 80% of the schools
in the US, there's

00:03:22.670 --> 00:03:24.279
a teacher who's
used our web site.

00:03:24.279 --> 00:03:25.820
We're really excited
about that reach

00:03:25.820 --> 00:03:29.270
because there's about 100,000
public schools in the US.

00:03:29.270 --> 00:03:31.070
More than a million
classroom projects

00:03:31.070 --> 00:03:34.070
have been funded on the
site, delivering resources

00:03:34.070 --> 00:03:37.040
to about 28 million
public school students.

00:03:37.040 --> 00:03:41.330
And the dollars have come from
more than 3 million donors,

00:03:41.330 --> 00:03:43.970
who, in total, have
given almost $700 billion

00:03:43.970 --> 00:03:46.442
to classrooms through
our organization.

00:03:50.870 --> 00:03:52.840
JACQUELINE FULLER: All right.

00:03:52.840 --> 00:03:54.262
ALEX BARNADOTTE:
Hello, everyone.

00:03:54.262 --> 00:03:55.720
My name is Alex
Barnadotte, and I'm

00:03:55.720 --> 00:03:59.620
the founder and CEO of an
organization called Beyond 12.

00:03:59.620 --> 00:04:02.800
We are a high tech, high
touch coaching platform,

00:04:02.800 --> 00:04:05.200
and our mission is to
significantly increase

00:04:05.200 --> 00:04:07.420
the number of students
from low income communities

00:04:07.420 --> 00:04:11.147
who graduate from our nation's
colleges and universities.

00:04:11.147 --> 00:04:13.480
And like I said, we do that
through a coaching platform,

00:04:13.480 --> 00:04:15.500
both a human, a
virtual human coach.

00:04:15.500 --> 00:04:17.923
So we hire recent
college graduates

00:04:17.923 --> 00:04:19.839
who themselves are the
first in their families

00:04:19.839 --> 00:04:22.316
to go to college so they
understand firsthand,

00:04:22.316 --> 00:04:24.690
the challenges that our students
are facing on their road

00:04:24.690 --> 00:04:28.540
to a college graduation, on
their road to a college degree.

00:04:28.540 --> 00:04:31.630
And we pair our human
coaches with a mobile app

00:04:31.630 --> 00:04:33.370
called My Coach,
where we've downloaded

00:04:33.370 --> 00:04:37.120
the academic and financial
aid calendars of the colleges

00:04:37.120 --> 00:04:39.250
and universities that
students are attending.

00:04:39.250 --> 00:04:43.420
We synthesize the data into
an interactive comprehensive

00:04:43.420 --> 00:04:45.400
to do list for them
that we augment

00:04:45.400 --> 00:04:49.120
with customized evidence
based push notifications

00:04:49.120 --> 00:04:51.520
and nudges to remind
them of the activities,

00:04:51.520 --> 00:04:53.500
deadlines, and behaviors
that lead to success.

00:04:53.500 --> 00:04:55.540
And on the back end, we
have an analytics engine

00:04:55.540 --> 00:04:57.580
that's powered by
machine learning that

00:04:57.580 --> 00:05:00.040
allows us to predict which
students need our help

00:05:00.040 --> 00:05:02.480
and when, and prescribe
the right type of support,

00:05:02.480 --> 00:05:06.287
whether it be human,
digital, or both.

00:05:06.287 --> 00:05:08.620
We're B to B, so we work with
high schools and colleges.

00:05:08.620 --> 00:05:10.870
We're currently serving
about 50,000 students,

00:05:10.870 --> 00:05:15.370
and on our way to serving
a million students by 2025.

00:05:15.370 --> 00:05:17.830
I started Beyond 12 in large
part because of the challenges

00:05:17.830 --> 00:05:19.830
I faced as a first
generation college goer.

00:05:19.830 --> 00:05:21.670
So I was born in Port
Au Prince, Haiti.

00:05:21.670 --> 00:05:23.140
I grew up in inner city Boston.

00:05:23.140 --> 00:05:26.300
I was the first person in
my family to go to college.

00:05:26.300 --> 00:05:28.390
We traveled to
Hanover, New Hampshire,

00:05:28.390 --> 00:05:30.400
because I landed at
Dartmouth, thinking

00:05:30.400 --> 00:05:32.840
that the most difficult part
of the journey was behind us,

00:05:32.840 --> 00:05:34.090
and we thought we had made it.

00:05:34.090 --> 00:05:35.548
When we got that
acceptance letter,

00:05:35.548 --> 00:05:38.527
we thought we won the
lottery, this is it.

00:05:38.527 --> 00:05:40.610
As you can imagine, because
of the work that I do,

00:05:40.610 --> 00:05:41.530
it wasn't it.

00:05:41.530 --> 00:05:44.620
I completely bombed
my freshman year--

00:05:44.620 --> 00:05:46.510
academically,
socially, emotionally,

00:05:46.510 --> 00:05:49.900
and discovered that getting
in was only the first step.

00:05:49.900 --> 00:05:51.890
Luckily, I was able
to turn things around.

00:05:51.890 --> 00:05:54.400
I did get my degree,
went to grad school out

00:05:54.400 --> 00:05:55.300
here at Stanford.

00:05:55.300 --> 00:05:58.210
But each year, about
700,000 students

00:05:58.210 --> 00:06:00.550
with backgrounds and stories
similar to mine embark

00:06:00.550 --> 00:06:02.650
on their college journeys
believing that they are

00:06:02.650 --> 00:06:04.360
prepared for the road ahead.

00:06:04.360 --> 00:06:06.666
But the statistics
tell us that only 9%

00:06:06.666 --> 00:06:08.290
of students from low
income communities

00:06:08.290 --> 00:06:10.870
can expect to earn a bachelor's
degree by their mid 20s

00:06:10.870 --> 00:06:13.570
versus 77% of their
higher income peers.

00:06:13.570 --> 00:06:16.450
So we are hoping to
change those statistics

00:06:16.450 --> 00:06:21.008
through a combination of
human and technology solution.

00:06:21.008 --> 00:06:22.216
JACQUELINE FULLER: All right.

00:06:22.216 --> 00:06:26.410
[APPLAUSE]

00:06:27.342 --> 00:06:28.247
Phil.

00:06:28.247 --> 00:06:30.080
PHILLIP ATIBA GOFF: I
am Phillip Atiba Goff.

00:06:30.080 --> 00:06:32.960
I am the co-founder and
President of the Center

00:06:32.960 --> 00:06:35.060
for Policing Equity.

00:06:35.060 --> 00:06:39.230
My story and CPE came about, I
was born at a very young age.

00:06:39.230 --> 00:06:42.517
We should probably skip
ahead a little bit.

00:06:42.517 --> 00:06:43.850
JACQUELINE FULLER: One sentence.

00:06:43.850 --> 00:06:45.225
PHILLIP ATIBA
GOFF: One sentence.

00:06:45.225 --> 00:06:48.180
We started that, but I'll
go with the president here.

00:06:48.180 --> 00:06:50.240
We work inside of
police departments

00:06:50.240 --> 00:06:52.910
to help them be less
racist, and we do it

00:06:52.910 --> 00:06:54.990
at the behest of
law enforcement.

00:06:54.990 --> 00:06:57.534
I know that might sound strange,
but it's absolutely the case

00:06:57.534 --> 00:06:59.450
that many folks in law
enforcement from around

00:06:59.450 --> 00:07:02.120
the country are asking
for help to reduce

00:07:02.120 --> 00:07:04.730
the racial disparities
in what they're doing.

00:07:04.730 --> 00:07:08.250
That becomes really hard when
you don't have data on it.

00:07:08.250 --> 00:07:11.750
So we started in 2012, the
national justice database.

00:07:11.750 --> 00:07:14.420
That is the first and the
largest standardized database

00:07:14.420 --> 00:07:15.402
of police behavior.

00:07:15.402 --> 00:07:17.360
We now cover about a
third of the United States

00:07:17.360 --> 00:07:18.302
by population.

00:07:18.302 --> 00:07:19.760
And here's the
amazing thing-- when

00:07:19.760 --> 00:07:21.620
we get numbers is
we can hold people

00:07:21.620 --> 00:07:23.660
accountable to actual values.

00:07:23.660 --> 00:07:26.980
The hard part now is getting
that data to be cleaned,

00:07:26.980 --> 00:07:28.430
and getting it standardized.

00:07:28.430 --> 00:07:29.510
That takes a lot of time.

00:07:29.510 --> 00:07:32.300
It takes human beings,
every single department,

00:07:32.300 --> 00:07:36.380
about nine to 12 months to
standardize it, and audit it,

00:07:36.380 --> 00:07:37.530
and analyze it.

00:07:37.530 --> 00:07:39.500
But with the help of
our Google engineers,

00:07:39.500 --> 00:07:42.232
we've reduced that to about nine
to 12 days in the last year.

00:07:42.232 --> 00:07:43.940
We're trying to get
to nine to 12 minutes

00:07:43.940 --> 00:07:45.170
by the end of this year.

00:07:45.170 --> 00:07:46.760
And when we've got
when we get there,

00:07:46.760 --> 00:07:48.710
we can measure not
just disparities,

00:07:48.710 --> 00:07:50.182
but actual bias, right?

00:07:50.182 --> 00:07:52.640
The part of the disparities
that belongs to law enforcement

00:07:52.640 --> 00:07:54.262
policy and individuals.

00:07:54.262 --> 00:07:56.720
And with that, just like they
do with all their other stats

00:07:56.720 --> 00:07:59.210
on crime, all their other
stats on public welfare,

00:07:59.210 --> 00:08:01.010
they can hold themselves
accountable to it,

00:08:01.010 --> 00:08:02.990
and reduce the
disparities and the burden

00:08:02.990 --> 00:08:05.210
from this on each of
those communities.

00:08:05.210 --> 00:08:08.360
So that's the ways in which we
can get behavioral science--

00:08:08.360 --> 00:08:09.950
we call us the justice nerds--

00:08:09.950 --> 00:08:12.470
to combine with tech-- that
would be the justice geeks--

00:08:12.470 --> 00:08:13.835
and we can do great cool things.

00:08:13.835 --> 00:08:14.960
JACQUELINE FULLER: Awesome.

00:08:14.960 --> 00:08:20.042
[APPLAUSE]

00:08:20.042 --> 00:08:23.600
Well, I think you can
see that we've got heroes

00:08:23.600 --> 00:08:27.687
amongst us, who are day in
and day out using technology

00:08:27.687 --> 00:08:28.520
to change the world.

00:08:28.520 --> 00:08:30.680
And you know what, they
need people like you.

00:08:30.680 --> 00:08:33.110
So we're going to have
opportunities for interaction

00:08:33.110 --> 00:08:34.880
with them after
this session if you

00:08:34.880 --> 00:08:37.400
would like to get to know
them better, know their work,

00:08:37.400 --> 00:08:39.260
maybe even volunteer with them.

00:08:39.260 --> 00:08:40.909
But why don't we
dig in a little bit

00:08:40.909 --> 00:08:43.860
and go back to the
technology itself?

00:08:43.860 --> 00:08:47.840
And if you'd be willing just
to speak a bit about exactly

00:08:47.840 --> 00:08:50.090
what you're doing from
a technology perspective

00:08:50.090 --> 00:08:53.550
that you think might be
interesting to this audience.

00:08:53.550 --> 00:08:55.330
Phil kind of kicked us off.

00:08:55.330 --> 00:08:58.184
Alex, do you want to tell
us what you all are doing?

00:08:58.184 --> 00:08:59.600
ALEX BARNADOTTE:
Yeah, absolutely.

00:08:59.600 --> 00:09:02.400
Just to follow up on
sort of our solution.

00:09:02.400 --> 00:09:06.050
So we're trying to solve the
issue of college success.

00:09:06.050 --> 00:09:09.710
And we believe that there is
both a human and a technology

00:09:09.710 --> 00:09:10.820
solution for it, right?

00:09:10.820 --> 00:09:13.034
And I know that I
might be in this room

00:09:13.034 --> 00:09:15.200
where we believe that there's
an app for everything.

00:09:15.200 --> 00:09:19.530
And we believe that
there is also an app.

00:09:19.530 --> 00:09:23.204
But for us, technology is the
amplifier of our humans, right?

00:09:23.204 --> 00:09:24.620
So we had human
coaches, and we're

00:09:24.620 --> 00:09:28.760
trying to figure out how do you
amplify the support that they

00:09:28.760 --> 00:09:30.890
are providing to
students, and what's

00:09:30.890 --> 00:09:33.030
the best way for us to do that.

00:09:33.030 --> 00:09:35.560
And so we built a platform
that allows us to do that.

00:09:35.560 --> 00:09:38.270
So we haven't completely
removed our humans,

00:09:38.270 --> 00:09:40.610
but we are making our
humans most effective,

00:09:40.610 --> 00:09:42.770
and really trying
to figure out, what

00:09:42.770 --> 00:09:45.590
are the aspects
of student support

00:09:45.590 --> 00:09:47.810
that can be automated
and digitized.

00:09:47.810 --> 00:09:50.210
The navigational and
the transactional

00:09:50.210 --> 00:09:52.310
can be digitized and automated.

00:09:52.310 --> 00:09:54.860
And what are the aspects
of student support

00:09:54.860 --> 00:09:56.712
that are uniquely human.

00:09:56.712 --> 00:09:58.170
And what we've
heard from students,

00:09:58.170 --> 00:09:59.990
those are the
inspirational, right?

00:09:59.990 --> 00:10:03.260
And so, do I belong here,
am I college material,

00:10:03.260 --> 00:10:06.470
I'm having challenges
and struggles.

00:10:06.470 --> 00:10:08.450
So we've built a
solution that allows

00:10:08.450 --> 00:10:13.370
us to distinguish between
those kinds of supports,

00:10:13.370 --> 00:10:16.560
and offer a just in time
solution for students.

00:10:16.560 --> 00:10:19.190
And on the back end, this is
the really important piece--

00:10:19.190 --> 00:10:21.565
that we are going to get to
the point where we're serving

00:10:21.565 --> 00:10:24.020
millions of students annually--

00:10:24.020 --> 00:10:25.430
we needed a solution
that allowed

00:10:25.430 --> 00:10:29.120
us to predict which students
needed our help and when,

00:10:29.120 --> 00:10:31.747
and then prescribe the
right type of support.

00:10:31.747 --> 00:10:33.830
So that's how we're
incorporating machine learning

00:10:33.830 --> 00:10:37.610
into our solution to do a
little bit of the predictive

00:10:37.610 --> 00:10:40.550
and the prescriptive modeling
that allows us to scale

00:10:40.550 --> 00:10:43.160
our efforts, so that
we can serve students,

00:10:43.160 --> 00:10:44.780
but also, so that
we can provide them

00:10:44.780 --> 00:10:48.680
with the exact type of support
that they need at the moment

00:10:48.680 --> 00:10:54.029
that they need it to prevent
them from dropping out.

00:10:54.029 --> 00:10:55.070
JACQUELINE FULLER: Great.

00:10:55.070 --> 00:10:56.504
Oliver, how about you?

00:10:56.504 --> 00:10:58.670
OLIVER HURST HILLER: Yeah,
I'll share a cool example

00:10:58.670 --> 00:11:00.230
of how we're applying
technology also

00:11:00.230 --> 00:11:01.970
in the machine learning arena.

00:11:01.970 --> 00:11:04.730
But first, some things
about DonorsChoose.org

00:11:04.730 --> 00:11:06.380
that some folks may
not know and that

00:11:06.380 --> 00:11:08.960
are not typical in the
crowd funding world.

00:11:08.960 --> 00:11:12.380
First, on the fulfillment side,
when a project on our web site

00:11:12.380 --> 00:11:15.430
reaches its goal, all
the funding it needed--

00:11:15.430 --> 00:11:18.950
$600, $700-- we don't send
cash, we don't send a check.

00:11:18.950 --> 00:11:21.770
We actually buy the stuff, and
we ship it to the classroom.

00:11:21.770 --> 00:11:23.930
Or if it's a field trip,
we'll pay the bus company.

00:11:23.930 --> 00:11:27.287
And this adds significant
operational complexity,

00:11:27.287 --> 00:11:29.870
but it's core to what we do, and
we're going to keep doing it.

00:11:29.870 --> 00:11:33.050
And similarly, every project
that goes live on our site

00:11:33.050 --> 00:11:35.960
doesn't just go up there
like say, a GoFundMe.

00:11:35.960 --> 00:11:38.340
We have a human being
review every single project.

00:11:38.340 --> 00:11:41.000
And the good news is, it's
done by people who are

00:11:41.000 --> 00:11:42.670
much more qualified than I am.

00:11:42.670 --> 00:11:44.674
We have a core of a
couple hundred teachers

00:11:44.674 --> 00:11:46.340
who have been successful
on the website,

00:11:46.340 --> 00:11:48.350
and successful on the
classroom, and they're

00:11:48.350 --> 00:11:51.440
helping to publish live
about 1,000 classroom project

00:11:51.440 --> 00:11:54.600
requests every day, just to
give you a sense for the volume.

00:11:54.600 --> 00:11:56.687
So they're doing a killer job.

00:11:56.687 --> 00:11:58.520
But one of the challenges
that we anticipate

00:11:58.520 --> 00:12:02.600
is that as we grow and grow,
or we get spikes in volume,

00:12:02.600 --> 00:12:04.580
we're not sure how
well this scales.

00:12:04.580 --> 00:12:08.840
So we connected our
data science team and--

00:12:08.840 --> 00:12:10.789
team might be overstating
it, because we

00:12:10.789 --> 00:12:11.830
have two data scientists.

00:12:11.830 --> 00:12:14.240
We connected our data
science team of two

00:12:14.240 --> 00:12:16.190
with the good folks at Kaggle.

00:12:16.190 --> 00:12:18.020
And if you're not
familiar with Kaggle,

00:12:18.020 --> 00:12:21.420
Kaggle is Google's crowdsourced
data science community.

00:12:21.420 --> 00:12:23.930
So folks with data
chops organize

00:12:23.930 --> 00:12:25.490
around data challenges.

00:12:25.490 --> 00:12:28.010
And together with Kaggle,
we launched this challenge

00:12:28.010 --> 00:12:31.390
to see if we could
automate this vetting.

00:12:31.390 --> 00:12:34.190
So could we have
machines and algorithms

00:12:34.190 --> 00:12:36.590
apply the same sort
of rigor and feedback

00:12:36.590 --> 00:12:38.312
for the teachers
submitting projects

00:12:38.312 --> 00:12:39.770
that today is done
by humans, maybe

00:12:39.770 --> 00:12:43.730
to offset some of that
volume that we're seeing.

00:12:43.730 --> 00:12:45.590
And the good news
as those of you

00:12:45.590 --> 00:12:47.690
familiar with AI and
machine learning,

00:12:47.690 --> 00:12:49.820
is that the training
data set that we're

00:12:49.820 --> 00:12:52.375
able to give to this
community was pretty big.

00:12:52.375 --> 00:12:53.750
So we gave them
a couple of years

00:12:53.750 --> 00:12:57.050
worth of screening information,
a couple of 100,000

00:12:57.050 --> 00:12:58.820
projects submitted to
our site, and either

00:12:58.820 --> 00:13:01.850
approved, or rejected, and
iterated, and then approved.

00:13:01.850 --> 00:13:04.470
And each project in that
data set was pretty rich.

00:13:04.470 --> 00:13:06.560
So it has an essay written
by the teacher, which

00:13:06.560 --> 00:13:07.850
is a couple hundred words.

00:13:07.850 --> 00:13:09.530
It's got every single
resource that they

00:13:09.530 --> 00:13:10.960
requested for their classrooms.

00:13:10.960 --> 00:13:14.450
So books, or technology,
or the field trip details.

00:13:14.450 --> 00:13:16.640
And then we've got the
meta data around the school

00:13:16.640 --> 00:13:19.750
or around the classroom, like
the subject, or the grade

00:13:19.750 --> 00:13:21.340
level, or the poverty level.

00:13:21.340 --> 00:13:22.990
So it's a pretty rich data set.

00:13:22.990 --> 00:13:26.170
And we ran this contest
with Kaggle for two months.

00:13:26.170 --> 00:13:28.040
It just ended a few weeks ago.

00:13:28.040 --> 00:13:31.010
600 data scientists
participated.

00:13:31.010 --> 00:13:33.250
6,000 algorithms were submitted.

00:13:33.250 --> 00:13:34.780
And it just ended.

00:13:34.780 --> 00:13:37.420
We're still digging through
the details and the results,

00:13:37.420 --> 00:13:39.340
but the good news is
that we can already

00:13:39.340 --> 00:13:43.000
see that the top submissions
way outperformed anything

00:13:43.000 --> 00:13:44.900
we were able to do on our own.

00:13:44.900 --> 00:13:46.750
And so this is pretty
exciting, right?

00:13:46.750 --> 00:13:52.120
This is an operational scary
scaling problem that we had,

00:13:52.120 --> 00:13:54.460
and data science and
machine learning community

00:13:54.460 --> 00:13:55.330
was able to step up.

00:13:55.330 --> 00:13:57.190
And hopefully, the
results mean that we

00:13:57.190 --> 00:14:00.634
may be able to more effectively
help even more classrooms.

00:14:00.634 --> 00:14:02.050
JACQUELINE FULLER:
That's amazing.

00:14:02.050 --> 00:14:03.730
Maybe even some people
in this audience

00:14:03.730 --> 00:14:07.690
were part of the teams
that responded to that.

00:14:07.690 --> 00:14:10.570
So shifting gears a little
bit, here in Silicon Valley,

00:14:10.570 --> 00:14:13.430
we're very
comfortable with risk.

00:14:13.430 --> 00:14:15.010
But in the world
of philanthropy,

00:14:15.010 --> 00:14:17.870
in the world of nonprofits,
not so much, right?

00:14:17.870 --> 00:14:20.280
So I want to just
ask you about risk,

00:14:20.280 --> 00:14:23.470
and maybe risk that you've
had to take either personally

00:14:23.470 --> 00:14:24.620
or professionally.

00:14:24.620 --> 00:14:28.140
How that's played out, how
that goes in the culture.

00:14:28.140 --> 00:14:30.040
Would you like to start us?

00:14:30.040 --> 00:14:31.660
PHILLIP ATIBA GOFF: Sure.

00:14:31.660 --> 00:14:36.380
I'm from West Philadelphia,
literally born and raised.

00:14:36.380 --> 00:14:37.700
I lived in Bel Air for a time.

00:14:37.700 --> 00:14:41.350
Go ahead, sing the song
in your head, it's fine.

00:14:41.350 --> 00:14:44.290
But going into a
police department,

00:14:44.290 --> 00:14:46.450
and doing the work
of figuring out

00:14:46.450 --> 00:14:50.200
how to clean their
excruciatingly ugly data--

00:14:50.200 --> 00:14:52.630
it wasn't just scary as
a black person walking

00:14:52.630 --> 00:14:57.530
into a law enforcement space,
it was scary as an academic.

00:14:57.530 --> 00:14:59.530
You don't get any credit
for doing something

00:14:59.530 --> 00:15:01.960
that shifts the needle
on public policy

00:15:01.960 --> 00:15:04.596
when your job is to get
tenure and to create

00:15:04.596 --> 00:15:06.970
new knowledge, which is the
job description for a faculty

00:15:06.970 --> 00:15:07.550
member.

00:15:07.550 --> 00:15:09.890
So as a faculty
member, that was scary.

00:15:09.890 --> 00:15:12.160
But the scarier part
I think right now

00:15:12.160 --> 00:15:14.650
is building the organization
that's necessary,

00:15:14.650 --> 00:15:16.796
the infrastructure
that's necessary to keep

00:15:16.796 --> 00:15:17.670
all this stuff going.

00:15:17.670 --> 00:15:20.003
What you're seeing up here
is a bunch of folks who are--

00:15:20.003 --> 00:15:22.150
I mean, social
entrepreneurship requires

00:15:22.150 --> 00:15:24.580
that you build something
kind of with your hands,

00:15:24.580 --> 00:15:26.290
and hold it together
until there's

00:15:26.290 --> 00:15:29.680
enough revenue coming in that
it can hold itself together.

00:15:29.680 --> 00:15:31.420
I don't have a degree in that.

00:15:31.420 --> 00:15:34.690
I have a degree in many, many
other things, but not in that.

00:15:34.690 --> 00:15:37.760
So figuring it out and building
the boat as we're going along,

00:15:37.760 --> 00:15:40.300
and then now moving into
a space where technology

00:15:40.300 --> 00:15:42.220
can solve so many
of these problems,

00:15:42.220 --> 00:15:45.220
can automate this, and put
this in the hands of the 18,000

00:15:45.220 --> 00:15:47.410
police departments across
the country, each of whom

00:15:47.410 --> 00:15:51.280
have a homebrewed data
management process--

00:15:51.280 --> 00:15:53.290
it is part of what is
keeping me up at night.

00:15:53.290 --> 00:15:54.290
JACQUELINE FULLER: Yeah.

00:15:54.290 --> 00:15:58.420
I mean, it's amazing to think
that you're having these police

00:15:58.420 --> 00:15:59.620
agencies come to us.

00:15:59.620 --> 00:16:01.110
They want to work together.

00:16:01.110 --> 00:16:03.580
They're giving their
data voluntarily.

00:16:03.580 --> 00:16:06.340
I mean, you're right on
the cutting edge of some

00:16:06.340 --> 00:16:09.040
of our biggest racial
issues as a country,

00:16:09.040 --> 00:16:12.620
and it's exciting to see both
sides coming together to say,

00:16:12.620 --> 00:16:15.430
let's let the data and
the evidence really

00:16:15.430 --> 00:16:16.625
drive the discussion here.

00:16:16.625 --> 00:16:18.000
PHILLIP ATIBA
GOFF: Yeah, 1,000%.

00:16:18.000 --> 00:16:20.154
It was shocking to
me the first thing

00:16:20.154 --> 00:16:21.570
that anybody asked
when they say I

00:16:21.570 --> 00:16:24.090
do this, like, oh, so you must
sue the police departments.

00:16:24.090 --> 00:16:24.716
No.

00:16:24.716 --> 00:16:26.590
So you must get it from
the public data sets.

00:16:26.590 --> 00:16:27.030
I said, no.

00:16:27.030 --> 00:16:28.446
Well, where do you
get their data.

00:16:28.446 --> 00:16:30.850
They come to me, they say,
please, Dr. Blackenstein,

00:16:30.850 --> 00:16:33.190
would you please tell
us how racist we are.

00:16:33.190 --> 00:16:34.900
And believe it or not--

00:16:34.900 --> 00:16:38.230
I get that kind of regularly,
that's my other code name--

00:16:38.230 --> 00:16:41.560
and it's because they're used
to using data to hold themselves

00:16:41.560 --> 00:16:43.450
accountable to crime.

00:16:43.450 --> 00:16:47.290
The DNA of any police department
is a thing called comstat.

00:16:47.290 --> 00:16:49.990
And comsat, they go and they
say what time the crime is

00:16:49.990 --> 00:16:51.610
occurring, where
it is on the map,

00:16:51.610 --> 00:16:54.460
and then they do this thing
called putting cops on dops.

00:16:54.460 --> 00:16:56.295
And that helps to
reduce the crime.

00:16:56.295 --> 00:16:58.045
The problem has been
on racial disparities

00:16:58.045 --> 00:17:01.782
is that disparities could
be because of crime rates.

00:17:01.782 --> 00:17:03.490
It could be because
of poverty, something

00:17:03.490 --> 00:17:06.319
that the police department
not really accountable for.

00:17:06.319 --> 00:17:08.859
So what we've done is
we've used census data,

00:17:08.859 --> 00:17:11.714
we've used survey data from
the officers, and survey data

00:17:11.714 --> 00:17:13.630
from the residents, with
their behavioral data

00:17:13.630 --> 00:17:16.069
that they do not share
with anybody else.

00:17:16.069 --> 00:17:18.040
And we said, this
is how we give you

00:17:18.040 --> 00:17:21.064
the comsat for justice
in your department.

00:17:21.064 --> 00:17:22.480
JACQUELINE FULLER:
That's amazing.

00:17:22.480 --> 00:17:23.349
All right.

00:17:23.349 --> 00:17:26.230
So Jess, equally
another topic right

00:17:26.230 --> 00:17:29.470
in the crosshairs of
American society right now,

00:17:29.470 --> 00:17:32.260
couldn't be any more
relevant or topical.

00:17:32.260 --> 00:17:34.180
I'm sure you have
encountered some

00:17:34.180 --> 00:17:37.680
personal, and professional,
and organizational risk.

00:17:37.680 --> 00:17:39.070
JESS LADD: Sure.

00:17:39.070 --> 00:17:44.410
So personally, I had to decide
about five years ago about

00:17:44.410 --> 00:17:46.900
whether or not I
wanted to talk publicly

00:17:46.900 --> 00:17:49.300
about my sexual
assault. And I think

00:17:49.300 --> 00:17:51.250
for people who
want to make change

00:17:51.250 --> 00:17:54.580
on the highly stigmatized
areas, and if you

00:17:54.580 --> 00:17:56.480
want to found a
company in that area,

00:17:56.480 --> 00:17:59.260
it's really hard to do
without talking about why

00:17:59.260 --> 00:18:01.610
you care about that issue.

00:18:01.610 --> 00:18:04.477
And a lot of these issues
are very hard to talk about,

00:18:04.477 --> 00:18:06.310
and people aren't used
to talking about them

00:18:06.310 --> 00:18:08.220
with their parents,
the people they're

00:18:08.220 --> 00:18:10.270
dating, with their friends.

00:18:10.270 --> 00:18:13.840
And so I had to decide whether
or not to kind of like out

00:18:13.840 --> 00:18:17.470
myself in order to
work in this area.

00:18:17.470 --> 00:18:20.570
And even now, I think what's
an organizational challenge is

00:18:20.570 --> 00:18:24.380
that if there is a program
officer at a foundation who

00:18:24.380 --> 00:18:27.620
gets what we're doing on a deep
level because they faced it,

00:18:27.620 --> 00:18:29.690
that's very hard for them
to come to their boss

00:18:29.690 --> 00:18:33.690
and say, no, I know this is
a good idea, and here's why.

00:18:33.690 --> 00:18:35.330
If somebody donates
to us, and now

00:18:35.330 --> 00:18:38.060
they want to get their friends
on board or throw a fundraiser,

00:18:38.060 --> 00:18:41.180
they have to say why they
care about this organization,

00:18:41.180 --> 00:18:43.550
and again, are they having
to out themselves too.

00:18:43.550 --> 00:18:47.307
So in stigmatized areas
where we haven't quite

00:18:47.307 --> 00:18:49.640
gotten to the point where
people are comfortable telling

00:18:49.640 --> 00:18:52.860
their own personal narratives
related to the topic,

00:18:52.860 --> 00:18:55.220
it's really hard to
fundraise, actually,

00:18:55.220 --> 00:18:57.514
and it's a little
bit challenging

00:18:57.514 --> 00:18:58.430
to get off the ground.

00:18:58.430 --> 00:19:01.790
Because while everybody feels
the problem at a high level,

00:19:01.790 --> 00:19:03.560
it's hard to relate
to it personally

00:19:03.560 --> 00:19:07.160
if we can't talk about it and
share about it with each other.

00:19:07.160 --> 00:19:11.150
I think the second kind of
big challenge for us is what

00:19:11.150 --> 00:19:12.770
we're doing is really risky.

00:19:12.770 --> 00:19:18.350
Like, we're trying to create
a safe place for victims

00:19:18.350 --> 00:19:21.170
of sexual assault
and sexual harassment

00:19:21.170 --> 00:19:24.950
to share with us the deepest
secrets of their lives.

00:19:24.950 --> 00:19:27.410
And then we need
to protect those.

00:19:27.410 --> 00:19:29.850
We need to protect
those from lawyers,

00:19:29.850 --> 00:19:32.180
we need to protect
those from hackers,

00:19:32.180 --> 00:19:38.030
we need to take a tremendous
amount of care with the data

00:19:38.030 --> 00:19:40.010
that people aren't
trusting us with.

00:19:40.010 --> 00:19:43.790
Because what we're asking of
people is to create an account,

00:19:43.790 --> 00:19:47.000
and to save what happened
to them time stamped.

00:19:47.000 --> 00:19:50.330
Now why this is important
is that on average college

00:19:50.330 --> 00:19:54.650
survivors take 11 months to make
their report of sexual assault,

00:19:54.650 --> 00:19:56.600
by which time it's
really hard for anyone

00:19:56.600 --> 00:19:58.130
to do an investigation.

00:19:58.130 --> 00:20:00.860
It's really hard for them
to remember it clearly.

00:20:00.860 --> 00:20:02.850
And it's really hard
for action to be taken.

00:20:02.850 --> 00:20:06.230
So if you can allow somebody
the ability to process what

00:20:06.230 --> 00:20:09.770
happened to them closer to
the event, to write it down,

00:20:09.770 --> 00:20:12.560
to document it in a way
that they feel safe,

00:20:12.560 --> 00:20:15.020
and that you're not prompting
them to write down things

00:20:15.020 --> 00:20:18.890
they're not sure of, because
then it'll be used to,

00:20:18.890 --> 00:20:21.410
say, that's
conflicting testimony

00:20:21.410 --> 00:20:23.570
later if they come on,
it's really important

00:20:23.570 --> 00:20:25.910
that you're prompting
them to do that correctly,

00:20:25.910 --> 00:20:28.340
and that you're
securing those data.

00:20:28.340 --> 00:20:30.320
And if they want
to use our system

00:20:30.320 --> 00:20:33.200
to figure out if
there's other victims

00:20:33.200 --> 00:20:35.030
of the same
perpetrator, then we're

00:20:35.030 --> 00:20:37.910
asking them to put in
the name, and a series

00:20:37.910 --> 00:20:41.930
of unique identifiers, like
cell phone number, or Facebook

00:20:41.930 --> 00:20:44.180
URL about their perpetrator.

00:20:44.180 --> 00:20:47.390
And in the event of a
match, meaning somebody else

00:20:47.390 --> 00:20:49.490
naming the same predator.

00:20:49.490 --> 00:20:52.490
We connect them with a
Callisto options counselor

00:20:52.490 --> 00:20:55.040
who, like you, we
provide services combined

00:20:55.040 --> 00:20:57.320
with tech, who's an attorney
or under the supervision

00:20:57.320 --> 00:20:59.720
of an attorney, who talks
to each victim separately

00:20:59.720 --> 00:21:01.940
to get the sense of
their case, gives them

00:21:01.940 --> 00:21:03.120
a sense of their options.

00:21:03.120 --> 00:21:05.360
And if appropriate,
connects them together

00:21:05.360 --> 00:21:06.782
to decide what to do next.

00:21:06.782 --> 00:21:08.990
Including everything from
confronting the perpetrator

00:21:08.990 --> 00:21:12.200
about their behavior, to
going to HR, to going public,

00:21:12.200 --> 00:21:15.680
to going to the police, to
finding a lawyer, et cetera.

00:21:15.680 --> 00:21:19.250
This means though that we
are not only again holding

00:21:19.250 --> 00:21:23.230
really sensitive data, but
if we're doing our job,

00:21:23.230 --> 00:21:26.390
We're bringing down some very
influential very powerful,

00:21:26.390 --> 00:21:29.660
very well networked,
very rich people.

00:21:29.660 --> 00:21:31.250
We're building a
system to bring down

00:21:31.250 --> 00:21:32.840
the Weinsteins of the
world, to bring down

00:21:32.840 --> 00:21:34.631
the politicians of the
world, to bring down

00:21:34.631 --> 00:21:36.080
the CEOs of companies.

00:21:36.080 --> 00:21:38.690
What we're trying to
do is highly likely

00:21:38.690 --> 00:21:41.840
to be attacked from lots
of different directions,

00:21:41.840 --> 00:21:44.640
and we need to take that
charge very seriously.

00:21:44.640 --> 00:21:47.180
We need to make sure that
we're not launching anything

00:21:47.180 --> 00:21:50.240
that we're not sure of which,
means we have to build slowly,

00:21:50.240 --> 00:21:52.730
and carefully in a lot
of ways, whereas you just

00:21:52.730 --> 00:21:55.369
want to go fast as a start up.

00:21:55.369 --> 00:21:57.410
But you can't fail fast
with something like this.

00:21:57.410 --> 00:21:59.430
You can't fail
something like this.

00:22:07.294 --> 00:22:09.710
JACQUELINE FULLER: Let me just
open this up to all of you.

00:22:09.710 --> 00:22:12.350
So you have this room here
of self-selected folks

00:22:12.350 --> 00:22:14.660
who I think are passionately
interested in this topic

00:22:14.660 --> 00:22:16.970
of technology for social impact.

00:22:16.970 --> 00:22:21.680
I want you to think back to
some aha moments and insights

00:22:21.680 --> 00:22:23.630
that you've had
along your journey.

00:22:23.630 --> 00:22:26.990
And think, what do
you know now that you

00:22:26.990 --> 00:22:30.500
wish you had known in the
beginning of your journey

00:22:30.500 --> 00:22:33.320
and advice that
you have for people

00:22:33.320 --> 00:22:35.930
who are interested in
applying technology

00:22:35.930 --> 00:22:38.780
for humanitarian impact.

00:22:38.780 --> 00:22:41.240
Anyone want to
start us off there?

00:22:44.910 --> 00:22:46.410
OLIVER HURST HILLER:
My advice would

00:22:46.410 --> 00:22:50.460
be to leverage peer
groups for advice

00:22:50.460 --> 00:22:52.010
and for continual learning.

00:22:52.010 --> 00:22:54.540
The example I'll
share is called CTOs

00:22:54.540 --> 00:22:58.050
for Good, which is a group that
I co-founded some years back.

00:22:58.050 --> 00:23:00.480
And it's the people
who run engineering

00:23:00.480 --> 00:23:03.900
at what I consider some of the
most tech forward non-profits

00:23:03.900 --> 00:23:05.100
on the planet.

00:23:05.100 --> 00:23:07.800
We've got about 20
orgs participating now,

00:23:07.800 --> 00:23:12.150
including Callisto, Kiva,
Code for America, Wikipedia,

00:23:12.150 --> 00:23:16.380
Khan Academy, code.org, Charity
Water, Global Citizen, Crisis

00:23:16.380 --> 00:23:17.880
Text Line, and others.

00:23:17.880 --> 00:23:19.140
It's an amazing group.

00:23:19.140 --> 00:23:21.000
And we meet annually.

00:23:21.000 --> 00:23:22.890
We have a two day
mini conference

00:23:22.890 --> 00:23:24.930
with a very active email list.

00:23:24.930 --> 00:23:26.860
And what we're doing
is helping each other

00:23:26.860 --> 00:23:29.340
in a safe space, trusted space.

00:23:29.340 --> 00:23:32.190
We talk about technical
challenges, vendors

00:23:32.190 --> 00:23:35.490
to avoid, hiring challenges,
all sorts of things.

00:23:35.490 --> 00:23:38.970
And now there are similar peer
groups in other disciplines

00:23:38.970 --> 00:23:40.950
like marketing and operations.

00:23:40.950 --> 00:23:43.950
And also up and
down the tech stack.

00:23:43.950 --> 00:23:46.890
So we've got DevOps for good,
and IT for good, and data

00:23:46.890 --> 00:23:48.180
science for good.

00:23:48.180 --> 00:23:53.100
And look, the nonprofit
world has there's challenges.

00:23:53.100 --> 00:23:54.780
We typically have
fewer resources

00:23:54.780 --> 00:23:59.370
and tighter budgets than
our for profit compatriots

00:23:59.370 --> 00:24:00.540
in those sectors.

00:24:00.540 --> 00:24:03.030
But one of our advantages
is that we usually

00:24:03.030 --> 00:24:06.030
can share more
openly, and share more

00:24:06.030 --> 00:24:07.677
freely than other
industries, which

00:24:07.677 --> 00:24:08.760
have to be more secretive.

00:24:08.760 --> 00:24:10.860
So that's my advice--

00:24:10.860 --> 00:24:15.459
is that ability to
share to your advantage.

00:24:15.459 --> 00:24:16.500
ALEX BARNADOTTE: Awesome.

00:24:16.500 --> 00:24:18.940
I think two pieces
of advice for me.

00:24:18.940 --> 00:24:21.120
The first is I do
a lot of coaching

00:24:21.120 --> 00:24:24.420
of young social
entrepreneurs, and I often

00:24:24.420 --> 00:24:28.380
find that folks sort of have
created a solution that is

00:24:28.380 --> 00:24:30.120
in search of a problem, right?

00:24:30.120 --> 00:24:33.120
Like, I have this great
app, I have this great tool,

00:24:33.120 --> 00:24:35.220
and I'm trying to figure
out ways to apply it

00:24:35.220 --> 00:24:37.090
to a social problem.

00:24:37.090 --> 00:24:39.640
And my advice would be to
switch to the paradigm, right?

00:24:39.640 --> 00:24:42.660
Like what is the social
challenge about which you

00:24:42.660 --> 00:24:44.790
are most passionate.

00:24:44.790 --> 00:24:48.240
About which you sort of
want to spend or for which

00:24:48.240 --> 00:24:50.910
you want to spend a lot of time.

00:24:50.910 --> 00:24:54.150
And then think about what
that solution would be.

00:24:54.150 --> 00:24:57.510
And the reminder that
technology is a tool--

00:24:57.510 --> 00:24:59.010
it is not the solution.

00:24:59.010 --> 00:25:00.390
It is a tool.

00:25:00.390 --> 00:25:05.820
And so really to reframe the
sort of challenge and solution

00:25:05.820 --> 00:25:08.070
pipeline--

00:25:08.070 --> 00:25:10.070
that's probably a
little bit different

00:25:10.070 --> 00:25:13.680
than we think about it when
we are in rooms like this.

00:25:13.680 --> 00:25:16.080
The second piece
is the importance

00:25:16.080 --> 00:25:22.860
of engaging your clients
and users as designers right

00:25:22.860 --> 00:25:25.920
I think often time
in the communities

00:25:25.920 --> 00:25:27.990
that we're operating
in folks who

00:25:27.990 --> 00:25:31.260
live in the communities have a
lot of ideas for the solutions.

00:25:31.260 --> 00:25:33.240
What's missing is
the opportunity

00:25:33.240 --> 00:25:36.180
to create organizations the
way that we create them.

00:25:36.180 --> 00:25:38.290
So as you are thinking
about solutions,

00:25:38.290 --> 00:25:40.050
and as you are
thinking about what

00:25:40.050 --> 00:25:42.930
are the challenges,
really, really important

00:25:42.930 --> 00:25:45.670
to get users as designers.

00:25:45.670 --> 00:25:48.480
Not as consultants, not
participating in focus groups

00:25:48.480 --> 00:25:50.610
after you've already
come up with a solution.

00:25:50.610 --> 00:25:55.110
But can you engage folks
in that design process

00:25:55.110 --> 00:25:58.440
so that you are creating
a solution that actually

00:25:58.440 --> 00:26:00.720
is applicable and relevant
to the communities

00:26:00.720 --> 00:26:03.720
that you wish to serve.

00:26:03.720 --> 00:26:04.812
JACQUELINE FULLER: Advice?

00:26:04.812 --> 00:26:07.020
PHILLIP ATIBA GOFF: I get
uncomfortable giving advice

00:26:07.020 --> 00:26:10.170
because I have to live with
the life that I've chosen.

00:26:10.170 --> 00:26:12.270
So when asked to give
advice, I usually

00:26:12.270 --> 00:26:15.300
give what my dad has told me.

00:26:15.300 --> 00:26:18.965
If you want to be a philosopher,
go where philosophers go.

00:26:18.965 --> 00:26:20.340
He stole that from
William James,

00:26:20.340 --> 00:26:22.381
and also, no one should
want to be a philosopher.

00:26:22.381 --> 00:26:25.890
My dad was one, that's difficult
at the dinner table early on.

00:26:25.890 --> 00:26:29.310
But we took that advice when the
Google immersion team came in.

00:26:29.310 --> 00:26:31.165
They did ride alongs
with law enforcement.

00:26:31.165 --> 00:26:32.790
JACQUELINE FULLER:
Just as a side note.

00:26:32.790 --> 00:26:36.160
The Google immersion team is
when we send our engineers,

00:26:36.160 --> 00:26:40.020
product managers, UX designers,
volunteers from Google

00:26:40.020 --> 00:26:42.426
to work side by side.

00:26:42.426 --> 00:26:43.800
PHILLIP ATIBA
GOFF: That's right.

00:26:43.800 --> 00:26:46.360
And before we got
started writing code,

00:26:46.360 --> 00:26:48.360
the first thing they did
was do the ride alongs.

00:26:48.360 --> 00:26:50.370
They talked with folks, they
went to training at NYPD.

00:26:50.370 --> 00:26:51.550
JACQUELINE FULLER: Ride
alongs with the police.

00:26:51.550 --> 00:26:53.758
PHILLIP ATIBA GOFF: Ride
along with law enforcement .

00:26:53.758 --> 00:26:56.370
And before I ever started
doing this work, when I just

00:26:56.370 --> 00:26:59.820
had my regular degree in
psychological science,

00:26:59.820 --> 00:27:02.610
I went and spent a month
embedded in the Denver Police

00:27:02.610 --> 00:27:03.750
Department.

00:27:03.750 --> 00:27:06.960
And the violence that I
thought I was going experience

00:27:06.960 --> 00:27:08.640
was really different violence.

00:27:08.640 --> 00:27:10.800
It was more close up,
it was more intimate.

00:27:10.800 --> 00:27:12.960
It didn't take the shape
that I thought it would.

00:27:12.960 --> 00:27:15.690
The racism that I thought I was
going to see, which I figured

00:27:15.690 --> 00:27:19.260
would like old school racism,
it was really, really different.

00:27:19.260 --> 00:27:21.065
And so being where
the people are

00:27:21.065 --> 00:27:22.440
when making these
decisions, this

00:27:22.440 --> 00:27:24.780
is the users as designers--

00:27:24.780 --> 00:27:28.620
being in the ecosystem
where the problem is

00:27:28.620 --> 00:27:31.260
allows for the solution to
become the thing that they

00:27:31.260 --> 00:27:33.022
always knew that
they needed, but they

00:27:33.022 --> 00:27:34.980
couldn't see because they
were too close to it.

00:27:34.980 --> 00:27:36.480
If you don't get
close enough to it,

00:27:36.480 --> 00:27:38.280
then you're going to
able to see any of it.

00:27:38.280 --> 00:27:38.790
JACQUELINE FULLER: Right.

00:27:38.790 --> 00:27:40.260
As Brian Stevenson
says, you've got

00:27:40.260 --> 00:27:44.220
to get approximate, both on
the problem and the solutions.

00:27:44.220 --> 00:27:45.540
Jess.

00:27:45.540 --> 00:27:47.380
JESS LADD: I think one
of my big learnings

00:27:47.380 --> 00:27:51.810
was realizing that my job
is not to build a tech

00:27:51.810 --> 00:27:54.216
product for victims.

00:27:54.216 --> 00:27:56.340
It is to build a company,
but outside of that, even

00:27:56.340 --> 00:27:59.790
the larger offering of
what we're providing,

00:27:59.790 --> 00:28:02.940
a tech product doesn't work
if people don't know about it.

00:28:02.940 --> 00:28:05.970
People won't know about it if
you don't figure out how you're

00:28:05.970 --> 00:28:07.950
going to market it to them.

00:28:07.950 --> 00:28:11.550
Chances are, your customer
is not the same as your user

00:28:11.550 --> 00:28:13.630
if you're building a
lot of tech for good.

00:28:13.630 --> 00:28:15.780
And so if you get so
focused on building

00:28:15.780 --> 00:28:18.960
the product for your
end user, you'll

00:28:18.960 --> 00:28:21.370
lose a lot of the different
partnerships that you need.

00:28:21.370 --> 00:28:24.000
So I think it's good to
map out the whole system.

00:28:24.000 --> 00:28:27.030
Like, who is it, who needs
to buy your product, what

00:28:27.030 --> 00:28:29.940
do they care about, embed
with them in addition

00:28:29.940 --> 00:28:30.720
to the end users.

00:28:30.720 --> 00:28:31.650
Know what they want.

00:28:31.650 --> 00:28:34.500
How is your end user
going to learn about it.

00:28:34.500 --> 00:28:36.210
What's the best way
to get it to them.

00:28:36.210 --> 00:28:38.040
Get a sense of
their space, and get

00:28:38.040 --> 00:28:40.170
a sense of what they
want when they get there.

00:28:40.170 --> 00:28:43.247
But I think I got so
focused on the end results,

00:28:43.247 --> 00:28:44.830
that I didn't
understand the full path

00:28:44.830 --> 00:28:46.365
that the whole system.

00:28:46.365 --> 00:28:48.240
JACQUELINE FULLER: So
it sounds like you guys

00:28:48.240 --> 00:28:52.560
have everything in hand and
everything's going really well.

00:28:52.560 --> 00:28:55.770
But maybe there's
one or two things

00:28:55.770 --> 00:28:58.040
that keep you up at night.

00:28:58.040 --> 00:29:02.620
Maybe just something that
if this thing goes sideways,

00:29:02.620 --> 00:29:04.830
it could be the undoing.

00:29:04.830 --> 00:29:09.150
Anyone want to share about
what's keeping you up at night?

00:29:09.150 --> 00:29:13.320
JESS LADD: I mean, I'm paranoid
about security for sure.

00:29:13.320 --> 00:29:16.137
And the certainty that
we're going to get sued.

00:29:16.137 --> 00:29:17.720
It's less of a worry
that you're going

00:29:17.720 --> 00:29:19.950
to get sued when you know
it's going to happen.

00:29:19.950 --> 00:29:22.835
It's more just like, how are
we going to address this well.

00:29:22.835 --> 00:29:24.210
And things like,
how are we going

00:29:24.210 --> 00:29:26.460
to create attorney-client
privilege electronically.

00:29:26.460 --> 00:29:28.530
Like, there's not a lot
of precedent for that.

00:29:28.530 --> 00:29:30.587
How do we match on
data that we can't

00:29:30.587 --> 00:29:32.670
see, because we know it's
going to get subpoenaed,

00:29:32.670 --> 00:29:34.711
so we need to make sure
that we're matching this.

00:29:34.711 --> 00:29:36.210
We're using linear
secret sharing,

00:29:36.210 --> 00:29:37.920
and we're federating
our servers.

00:29:37.920 --> 00:29:40.890
And we're doing a lot of
just fairly complicated

00:29:40.890 --> 00:29:42.420
cutting edge technology.

00:29:42.420 --> 00:29:45.586
Whereas normally, if you're
building tech for good,

00:29:45.586 --> 00:29:48.210
you don't want to be building in
that stuff unless you need it.

00:29:48.210 --> 00:29:51.430
Like, often, a Google
form will do the trick

00:29:51.430 --> 00:29:52.470
to a lot of issues.

00:29:52.470 --> 00:29:54.400
But in our case, we do.

00:29:54.400 --> 00:29:58.720
And so it is sort of
setting new precedent

00:29:58.720 --> 00:30:02.020
in a lot of different areas,
which is a little scary.

00:30:02.020 --> 00:30:04.020
And at the moment, we're
hiring a lot of people.

00:30:04.020 --> 00:30:06.000
So finding the right
people for the team

00:30:06.000 --> 00:30:07.930
at a really critical
stage and growth,

00:30:07.930 --> 00:30:11.430
including senior leadership
is always a little scary.

00:30:11.430 --> 00:30:14.040
Because the future of your
company is your people.

00:30:16.894 --> 00:30:18.810
PHILLIP ATIBA GOFF: The
thing that keeps me up

00:30:18.810 --> 00:30:21.810
at night honestly, the thing
that I can't find a way

00:30:21.810 --> 00:30:24.060
to make jokes about,
is that people

00:30:24.060 --> 00:30:26.580
will get bored with the
suffering of the communities

00:30:26.580 --> 00:30:29.570
that we deal with.

00:30:29.570 --> 00:30:33.060
In 2014, when Ferguson
caught on fire, and Baltimore

00:30:33.060 --> 00:30:34.890
shortly thereafter,
we had been doing

00:30:34.890 --> 00:30:38.490
this work for almost
10 years prior to that,

00:30:38.490 --> 00:30:40.800
and nobody cared.

00:30:40.800 --> 00:30:45.420
And what I've seen since then is
both heartening and depressing.

00:30:45.420 --> 00:30:48.150
So this past year, we estimated
that philanthropic giving

00:30:48.150 --> 00:30:50.820
nationally to criminal justice
reform across the United States

00:30:50.820 --> 00:30:52.260
was about $220 million.

00:30:52.260 --> 00:30:54.930
That's way more
than it used to be.

00:30:54.930 --> 00:30:56.980
But of that budget,
about $10 million

00:30:56.980 --> 00:30:59.655
for re-entry and
reintegration, $200 million

00:30:59.655 --> 00:31:04.490
is for decarceration, and
$10 million is for policing.

00:31:04.490 --> 00:31:07.470
That's scary for me not just
because the resources are

00:31:07.470 --> 00:31:11.430
so small to get this work
done, but because what

00:31:11.430 --> 00:31:14.640
we do every day is we go
between the communities

00:31:14.640 --> 00:31:17.580
where there are kids
who refer to themselves

00:31:17.580 --> 00:31:20.730
as the throwaway people.

00:31:20.730 --> 00:31:23.220
To the departments
that hear that,

00:31:23.220 --> 00:31:25.720
and don't know how to do better.

00:31:25.720 --> 00:31:28.710
And so what we find ourselves
doing in the times that are not

00:31:28.710 --> 00:31:30.780
the good times,
is begging people

00:31:30.780 --> 00:31:34.380
to care enough that
the relationships

00:31:34.380 --> 00:31:36.660
between the
departments that care,

00:31:36.660 --> 00:31:38.910
and the people that need
them can be strengthened

00:31:38.910 --> 00:31:40.880
the way that they should be.

00:31:43.960 --> 00:31:45.960
JACQUELINE FULLER: Well,
in our last few minutes

00:31:45.960 --> 00:31:48.030
here-- and I'll
say, for those who

00:31:48.030 --> 00:31:51.390
are interested in having
a deeper conversation,

00:31:51.390 --> 00:31:55.650
making a personal connection
with our heroes here

00:31:55.650 --> 00:31:59.050
and, the organizations they
represent, after this talk,

00:31:59.050 --> 00:32:03.450
we're going to be out in the
grassy area by the totem pole.

00:32:03.450 --> 00:32:06.610
Just up a little bit from
there on the platform,

00:32:06.610 --> 00:32:09.600
we've got that area
reserved for conversation

00:32:09.600 --> 00:32:14.400
if folks want to come in and
have some one on one Q&amp;A.

00:32:14.400 --> 00:32:18.030
But in general, you've got
this self-selected group,

00:32:18.030 --> 00:32:20.880
this room full of
technologists who

00:32:20.880 --> 00:32:24.570
are obviously very curious about
this, want to get involved.

00:32:24.570 --> 00:32:26.560
So what's your call to action?

00:32:26.560 --> 00:32:33.390
How can these folks get involved
and help carry the load?

00:32:33.390 --> 00:32:35.120
JESS LADD: We're
hiring engineers.

00:32:35.120 --> 00:32:36.670
Our code is open source.

00:32:36.670 --> 00:32:39.690
It's up on GitHub, check
it out, contribute.

00:32:39.690 --> 00:32:43.070
And if you think that
your employer, or school,

00:32:43.070 --> 00:32:45.680
or conference, or whoever
should adopt Callisto

00:32:45.680 --> 00:32:48.040
so that all of their
members can get the access,

00:32:48.040 --> 00:32:50.810
suggests them online
at ProjectCallisto.org.

00:32:53.390 --> 00:32:55.970
OLIVER HURST HILLER: So our
first collaboration with Kaggle

00:32:55.970 --> 00:32:58.310
to automate the screening
of classroom projects

00:32:58.310 --> 00:33:00.650
that I told you about was
so successful that we're

00:33:00.650 --> 00:33:02.570
doing another Kaggle challenge.

00:33:02.570 --> 00:33:06.290
The focus is around reengaging
donors to return to the site

00:33:06.290 --> 00:33:07.920
and support more classrooms.

00:33:07.920 --> 00:33:10.910
So for example, we have 40,
50, 60,000 classroom project

00:33:10.910 --> 00:33:13.490
requests live on the
site at any one time.

00:33:13.490 --> 00:33:15.347
Could we recommend
just a couple that

00:33:15.347 --> 00:33:17.930
would resonate with a specific
donor that would make them more

00:33:17.930 --> 00:33:19.820
likely to open their wallet.

00:33:19.820 --> 00:33:23.690
And fundraising and
reengaging donors

00:33:23.690 --> 00:33:26.130
are critically important to
any org that does fundraising,

00:33:26.130 --> 00:33:29.360
but this for us is an
especially hairy challenge

00:33:29.360 --> 00:33:32.260
because many of our donors
only give once or twice.

00:33:32.260 --> 00:33:35.100
So we don't have a lot of
signal there to work with.

00:33:35.100 --> 00:33:37.970
So if you've got data
chops, and you're curious,

00:33:37.970 --> 00:33:43.880
please go check out
kaggle.com/donorschoose/io,

00:33:43.880 --> 00:33:46.910
and you can learn more there.

00:33:46.910 --> 00:33:47.990
There it is.

00:33:47.990 --> 00:33:52.680
Also, for those of you that have
kids or teachers in the family,

00:33:52.680 --> 00:33:55.640
you may know that this week is
national Teacher Appreciation

00:33:55.640 --> 00:33:56.500
Week.

00:33:56.500 --> 00:33:59.570
All week, Google is
generously doubling donations

00:33:59.570 --> 00:34:01.100
to many projects.

00:34:01.100 --> 00:34:04.490
And the way it works is that
each day, the donations,

00:34:04.490 --> 00:34:06.980
the match is focused
on different project

00:34:06.980 --> 00:34:09.320
categories that are super
important to teachers.

00:34:09.320 --> 00:34:11.226
So today, the focus is music.

00:34:11.226 --> 00:34:13.100
The idea is, let's
support those students who

00:34:13.100 --> 00:34:15.141
want to learn music, the
teachers who are helping

00:34:15.141 --> 00:34:16.520
them develop their talent.

00:34:16.520 --> 00:34:20.239
So you can go to
DonorsChoose.org/googlemusic.

00:34:20.239 --> 00:34:21.830
Pick a classroom to support.

00:34:21.830 --> 00:34:23.881
Google will double
your donation.

00:34:23.881 --> 00:34:25.630
So thank you to Google
for that generosity

00:34:25.630 --> 00:34:27.409
and thank you to
all the teachers

00:34:27.409 --> 00:34:30.020
out there who are doing the
hard work to bring the best out

00:34:30.020 --> 00:34:30.560
of our kids.

00:34:30.560 --> 00:34:32.810
JACQUELINE FULLER: All
right, thanks, Oliver.

00:34:32.810 --> 00:34:33.851
ALEX BARNADOTTE: Awesome.

00:34:33.851 --> 00:34:35.159
OK, I have a couple of things.

00:34:35.159 --> 00:34:37.889
The first one is really
simple and really easy.

00:34:37.889 --> 00:34:41.139
We are increasing our
social media presence.

00:34:41.139 --> 00:34:47.850
So if you could just go
and follow @Beyond12--

00:34:47.850 --> 00:34:49.190
is it coming up anywhere?

00:34:49.190 --> 00:34:49.760
PHILLIP ATIBA GOFF: Yes.

00:34:49.760 --> 00:34:51.719
ALEX BARNADOTTE: OK,
it's coming up somewhere,

00:34:51.719 --> 00:34:55.010
but it's @B-E-Y-O-N-D-1-2.

00:34:55.010 --> 00:34:55.929
@Beyond12.

00:34:55.929 --> 00:34:59.810
We're going to tweet lots
of interesting things.

00:34:59.810 --> 00:35:01.670
There is a TEDx talk
that we just did

00:35:01.670 --> 00:35:03.180
that we would love to amplify.

00:35:03.180 --> 00:35:05.210
So if you watch it, you
feel inspired, please

00:35:05.210 --> 00:35:06.290
feel free to amplify IT.

00:35:06.290 --> 00:35:10.130
Just follow us as we're sort of
like amping up our social media

00:35:10.130 --> 00:35:11.030
efforts.

00:35:11.030 --> 00:35:13.380
The second one is we too
are hiring engineers.

00:35:13.380 --> 00:35:16.280
So we have been outsourcing
our development,

00:35:16.280 --> 00:35:19.580
and we just hired a CTO.

00:35:19.580 --> 00:35:21.530
And we're looking
for engineering help

00:35:21.530 --> 00:35:22.910
in whatever form it comes in.

00:35:22.910 --> 00:35:24.852
So definitely
tweet at us and let

00:35:24.852 --> 00:35:26.060
us know if you're interested.

00:35:26.060 --> 00:35:30.110
Particularly looking for
folks who have ML experience.

00:35:30.110 --> 00:35:32.510
And then the third one
is much more personal.

00:35:32.510 --> 00:35:35.090
so we work with students who
are graduating from college

00:35:35.090 --> 00:35:38.870
who are also thinking about
their first jobs, who have not

00:35:38.870 --> 00:35:41.180
had the opportunities that
a lot of us who are sitting

00:35:41.180 --> 00:35:42.710
on the stage have had.

00:35:42.710 --> 00:35:45.290
So when you think about
your own personal career,

00:35:45.290 --> 00:35:48.870
and as you are climbing
your own ladders of success,

00:35:48.870 --> 00:35:51.290
I just want to ask you
to take a step back

00:35:51.290 --> 00:35:53.420
to think about who
are you pulling along

00:35:53.420 --> 00:35:56.360
with you right who may have
different experiences that you

00:35:56.360 --> 00:35:57.150
have.

00:35:57.150 --> 00:36:00.380
So when you think about who
you're mentoring at Google,

00:36:00.380 --> 00:36:03.200
when you think about who
you're hiring at Google,

00:36:03.200 --> 00:36:06.050
and when you think about
your own personal philosophy,

00:36:06.050 --> 00:36:08.180
how can you sort of
embed the principles

00:36:08.180 --> 00:36:11.780
that you've heard from all
of us in your own personal

00:36:11.780 --> 00:36:13.820
and professional
journey so that you

00:36:13.820 --> 00:36:17.330
can increase
opportunity and insure

00:36:17.330 --> 00:36:19.940
that a child's zip code
no longer determines

00:36:19.940 --> 00:36:22.544
his or her destiny

00:36:22.544 --> 00:36:23.671
JACQUELINE FULLER: Phil.

00:36:23.671 --> 00:36:25.170
PHILLIP ATIBA GOFF:
So three things.

00:36:25.170 --> 00:36:28.150
First, please do follow
us on social media.

00:36:28.150 --> 00:36:31.806
I'm @ DrPhilGoff-- that's
the other Dr. Phil.

00:36:31.806 --> 00:36:34.210
You can also follow
@policingequity.

00:36:34.210 --> 00:36:36.170
You can also sign up
to be a #justicenerd,

00:36:36.170 --> 00:36:38.000
I brought the t-shirt.

00:36:38.000 --> 00:36:40.250
So we just had our first
group of young justice nerds,

00:36:40.250 --> 00:36:41.660
so you can hashtag that.

00:36:41.660 --> 00:36:44.192
But amplify the
messages that way.

00:36:44.192 --> 00:36:45.900
A second, apparently,
it's going around--

00:36:45.900 --> 00:36:50.090
we are in the process
of pre-hiring a CTO,

00:36:50.090 --> 00:36:54.061
and we also have a group of
volunteer software engineers.

00:36:54.061 --> 00:36:56.060
So if folks are interested,
you can get in touch

00:36:56.060 --> 00:36:57.350
with us via social media.

00:36:57.350 --> 00:36:59.240
We're at policingequity.org.

00:36:59.240 --> 00:37:02.930
And the third one, this is a way
to get involved more generally

00:37:02.930 --> 00:37:05.390
when we're talking about
police accountability.

00:37:05.390 --> 00:37:08.780
In every city that you live
in in the United States,

00:37:08.780 --> 00:37:10.850
you can call your
police department

00:37:10.850 --> 00:37:14.590
not to report something,
but to request a ride along.

00:37:14.590 --> 00:37:16.766
That ride along is
just get in a car,

00:37:16.766 --> 00:37:18.140
you spend a couple
of hours going

00:37:18.140 --> 00:37:20.090
and see what law
enforcement does.

00:37:20.090 --> 00:37:22.520
And I have never had
it happen that someone

00:37:22.520 --> 00:37:24.920
goes through a ride long,
and has that experience,

00:37:24.920 --> 00:37:28.730
and doesn't want to do
something to reward folks just

00:37:28.730 --> 00:37:32.090
to make that space a
different, more vital, safer,

00:37:32.090 --> 00:37:33.380
and more fair space.

00:37:33.380 --> 00:37:36.234
It's a two to three hour
commitment, a phone call,

00:37:36.234 --> 00:37:37.400
a little bit of awkwardness.

00:37:37.400 --> 00:37:39.872
You don't have to wear the
vest if you don't want to.

00:37:39.872 --> 00:37:41.330
But I highly
encourage folks do it.

00:37:41.330 --> 00:37:43.400
There's almost nothing
more eye opening

00:37:43.400 --> 00:37:45.740
in terms of how you
relate to public safety

00:37:45.740 --> 00:37:48.073
than the ability to go and
take one of those I encourage

00:37:48.073 --> 00:37:49.130
everybody to do that.

00:37:49.130 --> 00:37:51.140
JACQUELINE FULLER: So that's
the most specific call to action

00:37:51.140 --> 00:37:52.280
I think I've ever heard.

00:37:52.280 --> 00:37:54.620
All right, in our last
just a couple minutes

00:37:54.620 --> 00:38:00.050
here, why don't we end in a
moment of inspiration and hope.

00:38:00.050 --> 00:38:04.430
If you guys want to share just
one thing that gives you hope.

00:38:04.430 --> 00:38:05.650
I'll start.

00:38:05.650 --> 00:38:09.380
It gives me hope that
this session sold out,

00:38:09.380 --> 00:38:12.200
and they had to put us in
the largest room available,

00:38:12.200 --> 00:38:15.740
because all of you are so
interested in this topic,

00:38:15.740 --> 00:38:19.640
and you all bring a skill set
that is desperately needed.

00:38:19.640 --> 00:38:20.670
And that gives me hope.

00:38:29.630 --> 00:38:31.430
PHILLIP ATIBA GOFF:
When Ferguson blew up,

00:38:31.430 --> 00:38:36.050
and we saw Baltimore get ugly,
and the rest of the country

00:38:36.050 --> 00:38:38.000
was consumed with
the issues of police

00:38:38.000 --> 00:38:42.140
violence for several
years, the head of the FBI

00:38:42.140 --> 00:38:44.030
commented that he
was embarrassed

00:38:44.030 --> 00:38:46.850
because we had no idea
how many people got shot

00:38:46.850 --> 00:38:48.710
by law enforcement every year.

00:38:48.710 --> 00:38:51.260
The federal government
didn't have any idea.

00:38:51.260 --> 00:38:55.280
By the time that we next to
elect a president in 2020,

00:38:55.280 --> 00:38:57.980
we will have enough data to
give a national snapshot,

00:38:57.980 --> 00:39:01.820
not just of disparities in
police use of deadly force,

00:39:01.820 --> 00:39:04.280
or just disparities in
police use of force,

00:39:04.280 --> 00:39:07.790
but on racial bias on
all police behaviors.

00:39:07.790 --> 00:39:09.994
That's because of
technologists like all of you,

00:39:09.994 --> 00:39:11.660
because of behavioral
scientists like me

00:39:11.660 --> 00:39:13.880
and my justice nerd
friends, and because of what

00:39:13.880 --> 00:39:16.190
law enforcement
communities are requiring.

00:39:16.190 --> 00:39:18.800
And that to me is one of
the best good news stories

00:39:18.800 --> 00:39:19.310
on racism.

00:39:19.310 --> 00:39:20.893
JACQUELINE FULLER:
That is great news.

00:39:20.893 --> 00:39:26.372
[APPLAUSE]

00:39:26.372 --> 00:39:28.330
ALEX BARNADOTTE: So I
have the incredible honor

00:39:28.330 --> 00:39:33.154
of working with amazing young
people, college students,

00:39:33.154 --> 00:39:35.320
high school students or
high school students who are

00:39:35.320 --> 00:39:37.150
about to transition to college.

00:39:37.150 --> 00:39:41.290
And what gives me hope is what
I have seen in this generation,

00:39:41.290 --> 00:39:44.080
from the students in Parkland,
to Oakland, to Chicago,

00:39:44.080 --> 00:39:49.360
to Dorchester, who are fighting
for their rights to be safe,

00:39:49.360 --> 00:39:54.970
and who are demanding that we
adults stop and pay attention.

00:39:54.970 --> 00:39:58.810
And I am incredibly inspired by
the spirit of intersectionality

00:39:58.810 --> 00:40:01.780
that these young people are
operating with, and operating

00:40:01.780 --> 00:40:04.510
with the creed that none of
us is free until all of us

00:40:04.510 --> 00:40:05.400
are free.

00:40:05.400 --> 00:40:09.470
And for me, that gives me hope.

00:40:09.470 --> 00:40:12.070
I'm on Twitter, I'll
read a Facebook post,

00:40:12.070 --> 00:40:15.520
and I'll read the latest
incident that is happening,

00:40:15.520 --> 00:40:17.350
I remember those young people.

00:40:17.350 --> 00:40:19.540
I think back to the way
that they have touched,

00:40:19.540 --> 00:40:24.890
and moved, and started a
movement, and I'm inspired.

00:40:24.890 --> 00:40:27.432
And think that we're
going to be OK.

00:40:27.432 --> 00:40:28.390
We're going to be good.

00:40:31.141 --> 00:40:33.640
OLIVER HURST HILLER: Whether
it's the response to our Kaggle

00:40:33.640 --> 00:40:35.830
challenge, or the
participation of CTOs for Good,

00:40:35.830 --> 00:40:39.000
or like Jacqueline said,
all of you being here,

00:40:39.000 --> 00:40:41.140
that makes me
hopeful and grateful

00:40:41.140 --> 00:40:45.072
because I see the tech community
rising to the occasion,

00:40:45.072 --> 00:40:46.780
and wanting to give
back, wanting to make

00:40:46.780 --> 00:40:49.390
a social impact, whether it's
full time or on a volunteering

00:40:49.390 --> 00:40:49.970
basis.

00:40:49.970 --> 00:40:52.200
And I think with
your superpowers,

00:40:52.200 --> 00:40:54.550
we can really change things.

00:40:54.550 --> 00:40:56.880
JACQUELINE FULLER: All right.

00:40:56.880 --> 00:40:59.160
And Jess, why don't
you wrap us up?

00:40:59.160 --> 00:41:02.140
JESS LADD: I'm going
to cheat and do two.

00:41:02.140 --> 00:41:06.880
So the first is that about a
month ago, I was in New York

00:41:06.880 --> 00:41:10.900
and I was visiting the Upright
Citizens Brigade Theater, which

00:41:10.900 --> 00:41:15.940
is a comedy troupe and training
school for young comedians.

00:41:15.940 --> 00:41:19.420
And I was talking to some of the
actors there, and one of them

00:41:19.420 --> 00:41:23.590
said to me, she was like,
there were always people

00:41:23.590 --> 00:41:24.910
who we kind of knew about.

00:41:24.910 --> 00:41:28.480
It was sort of an open secret,
but nothing really happened.

00:41:28.480 --> 00:41:31.690
And within two months after
Callisto had launched,

00:41:31.690 --> 00:41:35.330
those people were reported
and they weren't here anymore.

00:41:35.330 --> 00:41:36.460
JACQUELINE FULLER: Wow.

00:41:36.460 --> 00:41:38.126
JESS LADD: So to her,
to have that sense

00:41:38.126 --> 00:41:42.280
that that had happened and
that quickly in her community

00:41:42.280 --> 00:41:43.480
gave me a lot of hope.

00:41:43.480 --> 00:41:49.000
And the second is that
back in June and July,

00:41:49.000 --> 00:41:51.300
when Justin Caldbeck
had first happened,

00:41:51.300 --> 00:41:53.630
and all of these different
venture capitalists

00:41:53.630 --> 00:41:57.010
were getting outed, I started
looking into that space,

00:41:57.010 --> 00:42:00.130
and I had, based on
those news articles,

00:42:00.130 --> 00:42:05.050
had a very negative impression
of venture capitalists.

00:42:05.050 --> 00:42:07.774
And then recently, over
the last three months,

00:42:07.774 --> 00:42:09.190
I've been talking
to a lot of them

00:42:09.190 --> 00:42:11.273
because we've actually
been fundraising from them.

00:42:11.273 --> 00:42:13.540
And I've been going into
firms, and saying, hey,

00:42:13.540 --> 00:42:15.850
does your VC firm
want to donate.

00:42:15.850 --> 00:42:17.606
There is no financial ROI.

00:42:17.606 --> 00:42:19.230
VC firms don't normally
do this, right?

00:42:19.230 --> 00:42:20.710
This is not their thing.

00:42:20.710 --> 00:42:24.370
Would you like to donate to
allow us to create a reporting

00:42:24.370 --> 00:42:28.210
system to help founders report
sexual misconduct committed

00:42:28.210 --> 00:42:30.670
by venture capitalists?

00:42:30.670 --> 00:42:32.050
No idea if that would work.

00:42:32.050 --> 00:42:37.360
Seems like they have every
incentive not to do that.

00:42:37.360 --> 00:42:40.540
But about one in three
firms have said yes,

00:42:40.540 --> 00:42:42.349
and now we have seven,
including Greylock,

00:42:42.349 --> 00:42:44.140
including First Round,
a lot of great firms

00:42:44.140 --> 00:42:46.180
who are donating to
support this work,

00:42:46.180 --> 00:42:49.070
and actually want accountability
in their industry.

00:42:49.070 --> 00:42:50.740
And so I think there
are a lot of people

00:42:50.740 --> 00:42:53.560
who do care about this
issue, and you can't just

00:42:53.560 --> 00:42:56.127
assume like, oh, because
most of you are a man,

00:42:56.127 --> 00:42:57.460
or whatever, you're not in this.

00:42:57.460 --> 00:43:00.790
Some of our best
allies have been men.

00:43:00.790 --> 00:43:02.290
Some of our best
allies are the ones

00:43:02.290 --> 00:43:03.370
that you'd expect the least.

00:43:03.370 --> 00:43:04.870
And I think that
this is a problem

00:43:04.870 --> 00:43:06.301
that so many people care about.

00:43:06.301 --> 00:43:08.800
And if you just give them an
opportunity to be a part of it,

00:43:08.800 --> 00:43:10.060
they will be.

00:43:10.060 --> 00:43:12.850
JACQUELINE FULLER: Well,
thank you for everything

00:43:12.850 --> 00:43:16.510
that you guys are doing to
give all of us so much hope.

00:43:16.510 --> 00:43:18.230
So let's hear it
for these heroes.

00:43:18.230 --> 00:43:18.730
Thank you.

00:43:18.730 --> 00:43:20.646
[APPLAUSE]

00:43:20.646 --> 00:43:27.860
[MUSIC PLAYING]

