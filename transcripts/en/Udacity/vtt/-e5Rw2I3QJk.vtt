WEBVTT
Kind: captions
Language: en

00:00:01.150 --> 00:00:03.469
We'll now talk about TCP congestion

00:00:03.469 --> 00:00:07.560
control in the context of modern datacenters.

00:00:07.560 --> 00:00:10.610
And we'll talk about a particular TCP

00:00:10.610 --> 00:00:13.600
throughput collapse problem called the TCP incast

00:00:13.600 --> 00:00:21.100
problem. A typical data center consists of a set of server racks. Each holding

00:00:21.100 --> 00:00:26.880
a large number of servers, the switches that connect those racks of servers, and

00:00:26.880 --> 00:00:30.690
the connecting links that connect those switches

00:00:30.690 --> 00:00:32.920
to other parts of the topology. So the

00:00:32.920 --> 00:00:36.900
network architecture is typically made up of some

00:00:36.900 --> 00:00:40.990
sort of tree. And, switching elements that progressively

00:00:40.990 --> 00:00:43.130
are more specialized and expensive as we move

00:00:43.130 --> 00:00:45.940
up the network hierarchy. Some of the characteristics

00:00:45.940 --> 00:00:52.180
of a data center network include a high fan in, there is a very high amount

00:00:52.180 --> 00:00:58.270
of fan in between the leaves of the tree and the top of the root. Workloads

00:00:58.270 --> 00:01:01.600
are high bandwidth and low latency, and many

00:01:01.600 --> 00:01:04.640
clients issue requests in parallel, each with a

00:01:04.640 --> 00:01:08.430
relatively small amount of data per request. The

00:01:08.430 --> 00:01:10.890
other constraint that we face, is that the

00:01:10.890 --> 00:01:13.690
buffers in these switches can be quite small.

00:01:13.690 --> 00:01:17.420
So when we combine the requirements of high

00:01:17.420 --> 00:01:20.800
bandwidth and low latency for the applications,

00:01:20.800 --> 00:01:24.010
the presence of many parallel requests coming

00:01:24.010 --> 00:01:26.350
from these servers. And the fact that

00:01:26.350 --> 00:01:30.160
the switches have relatively small buffers, we

00:01:30.160 --> 00:01:31.580
can see that potentially there will be

00:01:31.580 --> 00:01:35.180
a problem. The throughput collapse that results

00:01:35.180 --> 00:01:39.180
from this phenomenon is called the TCP

00:01:39.180 --> 00:01:42.910
Incast problem. Incast is a drastic reduction

00:01:42.910 --> 00:01:45.850
in application throughput that results when

00:01:45.850 --> 00:01:49.910
servers using TCP. All simultaneously request

00:01:49.910 --> 00:01:52.820
data, leading to a gross underutilization

00:01:52.820 --> 00:01:56.390
of network capacity in many-to-one communication networks

00:01:56.390 --> 00:02:01.570
like a datacenter. The filling up of the buffers here at the switches

00:02:01.570 --> 00:02:05.680
result in bursty retransmissions that overfill

00:02:05.680 --> 00:02:08.930
the switch buffers. And these bursting retransmissions

00:02:08.930 --> 00:02:12.600
are cause by TCP timeouts. The TCP

00:02:12.600 --> 00:02:15.570
timeouts can lasts hundreds of milliseconds. But the

00:02:15.570 --> 00:02:17.660
roundtrip time in a data center network, is

00:02:17.660 --> 00:02:20.840
typically less than a millisecond. Often just hundreds

00:02:20.840 --> 00:02:23.790
of microseconds. Because the roundtrip times are so

00:02:23.790 --> 00:02:27.030
much less than TCP timeouts. The centers will

00:02:27.030 --> 00:02:30.580
have to wait for the TCP timeout. Before

00:02:30.580 --> 00:02:34.180
they retransmit. An application throughput can be reduced

00:02:34.180 --> 00:02:37.730
by as much as 90% as a result of link idle time.

