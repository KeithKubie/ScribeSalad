WEBVTT
Kind: captions
Language: en

00:00:00.330 --> 00:00:02.220
So the next from that we
need to worry about is,

00:00:02.220 --> 00:00:04.140
the problem of state estimation.

00:00:04.140 --> 00:00:07.450
And as we were just talking
about a couple moments ago.

00:00:07.450 --> 00:00:12.250
We can make a POM D.P. into a M.D.P.,
by expanding out the state space.

00:00:12.250 --> 00:00:16.540
What we're going to do is,
consider what we call belief states.

00:00:16.540 --> 00:00:19.330
So, the state that the decision
maker is going to use in

00:00:19.330 --> 00:00:22.090
the context of a POM DP is,
going to be a belief state b.

00:00:22.090 --> 00:00:27.170
And what a belief state b gives you is,
that if you tell me a state like s.

00:00:27.170 --> 00:00:30.610
Belief state b(s) tells us the
probability that we're actually in state

00:00:30.610 --> 00:00:32.580
s at the current moment in time.

00:00:32.580 --> 00:00:35.470
&gt;&gt; So b is a sort of like a state,
well it's a belief state.

00:00:36.480 --> 00:00:38.280
So it's like what we did
in the last example.

00:00:38.280 --> 00:00:39.430
It's kind of a vector.

00:00:39.430 --> 00:00:42.690
It's a distribution over states.

00:00:42.690 --> 00:00:43.530
&gt;&gt; Yes, exactly.

00:00:43.530 --> 00:00:45.780
It's a probability
distribution over states.

00:00:45.780 --> 00:00:48.368
Yes, and so and that's going to
that's going to encode for us.

00:00:48.368 --> 00:00:51.830
It's going to retain for us
the information about the history that

00:00:51.830 --> 00:00:53.670
might be important for decision making.

00:00:53.670 --> 00:00:55.130
&gt;&gt; Okay, so gets us all
the way back to the beginning.

00:00:55.130 --> 00:01:00.060
Okay, so b(s) then, so b is just
a vector and it's indexed by s and s,

00:01:00.060 --> 00:01:02.320
instead of being an integer
is just a state.

00:01:02.320 --> 00:01:03.240
&gt;&gt; Right.
&gt;&gt; Okay, that works.

00:01:03.240 --> 00:01:04.129
It's a probability distribution.

00:01:04.129 --> 00:01:04.910
All right, that makes sense.

00:01:04.910 --> 00:01:07.420
Okay.
&gt;&gt; So now what we need to do is,

00:01:07.420 --> 00:01:12.380
figure out how this belief state gets
updated as we take actions in the world.

00:01:12.380 --> 00:01:13.550
&gt;&gt; Okay.
&gt;&gt; So the way we're going to think about

00:01:13.550 --> 00:01:16.910
that is, we're in some belief state,
we choose an action.

00:01:16.910 --> 00:01:19.680
The world gives us back
an observation and we need now,

00:01:19.680 --> 00:01:22.180
a new belief state to
be formed from that.

00:01:22.180 --> 00:01:26.694
&gt;&gt; Okay, so that's exactly parallel to
the, we're in a state we took an action,

00:01:26.694 --> 00:01:28.280
then we got into another state.

00:01:28.280 --> 00:01:30.910
&gt;&gt; Right, so
what we really are doing here is,

00:01:30.910 --> 00:01:32.929
we're building up
a notion of a belief MDP.

00:01:32.929 --> 00:01:37.820
So, we're going to turn
the POMDP into a kind of MDP,

00:01:37.820 --> 00:01:39.140
specifically the belief MDP.

00:01:39.140 --> 00:01:43.070
Where the states of the belief MDP are
probability distributions over states of

00:01:43.070 --> 00:01:45.630
the underlying MDP in the POMDP.

00:01:45.630 --> 00:01:46.650
&gt;&gt; Okay, sounds good.

00:01:46.650 --> 00:01:51.110
So, an aside Michael,
is if reward is a function of state.

00:01:51.110 --> 00:01:53.820
Which it sometimes is, so

00:01:53.820 --> 00:01:57.590
you should be able to use that
to update your belief state.

00:01:57.590 --> 00:01:58.170
&gt;&gt; Right, so

00:01:58.170 --> 00:02:02.050
in what we're going to be talking about,
the reward is essentially not observed.

00:02:03.640 --> 00:02:08.139
But of course in reality, if you have
a learner that's in some environment and

00:02:08.139 --> 00:02:10.110
it's making observations.

00:02:10.110 --> 00:02:12.150
And getting some reward back,
it can actually,

00:02:12.150 --> 00:02:14.890
and if it knows something about how
that reward is being generated.

00:02:14.890 --> 00:02:22.350
It can use that as a kind of observation
and so, we can essentially assume that

00:02:22.350 --> 00:02:27.250
the observation has whatever information
might be in the reward in it.

00:02:27.250 --> 00:02:28.050
&gt;&gt; Right, so in fact,

00:02:28.050 --> 00:02:32.210
the observation is whatever sub teachers
you get to see plus the reward.

00:02:32.210 --> 00:02:34.030
&gt;&gt; Yeah,
I've seen some people write it this way,

00:02:34.030 --> 00:02:37.710
where you say that the reward is
actually something about the observable.

00:02:37.710 --> 00:02:42.380
You know, all the reward you get,
is extracted from what you observe and

00:02:42.380 --> 00:02:44.750
so, anything that you
model in the POMDP.

00:02:44.750 --> 00:02:47.910
As being reward relevant, has to
actually show up in the observation.

00:02:47.910 --> 00:02:49.160
&gt;&gt; Right, that seems reasonable.

00:02:49.160 --> 00:02:51.640
&gt;&gt; Right, otherwise you can't
learn if you can't observe it.

00:02:51.640 --> 00:02:53.410
&gt;&gt; Right, that makes perfect sense.

00:02:53.410 --> 00:02:57.410
&gt;&gt; All right, so we need to do is,
derive how we get this probability

00:02:57.410 --> 00:03:02.140
distribution over states b prime
from the old belief state.

00:03:02.140 --> 00:03:03.680
The action, the observation, and

00:03:03.680 --> 00:03:06.122
whatever quantities in
the POMDP that we have.

00:03:06.122 --> 00:03:09.870
So, what we need to figure out is,
in this new belief state,

00:03:09.870 --> 00:03:11.910
after we've taken action a.

00:03:11.910 --> 00:03:16.520
And seen observation z, what is the
probability that we're in state S prime?

00:03:16.520 --> 00:03:18.890
And so, we're going to have to
do some kind of probabilistic,

00:03:19.920 --> 00:03:22.950
I guess inference or
derivation to work that out.

00:03:22.950 --> 00:03:26.270
&gt;&gt; Sure,
we ought to be able to do that right,

00:03:26.270 --> 00:03:30.140
because we have all the quantities
that we need in the POMDP, I think.

00:03:30.140 --> 00:03:33.470
&gt;&gt; Well that's the hope, so we want
the probability of the next state,

00:03:33.470 --> 00:03:34.860
given that we're in a belief state.

00:03:34.860 --> 00:03:38.140
Took an action and made an observation.

00:03:38.140 --> 00:03:42.590
So, can we manipulate this expression
using the laws of probability

00:03:42.590 --> 00:03:43.880
to make sense out of it?

00:03:43.880 --> 00:03:45.210
Sure.
The good answer.

