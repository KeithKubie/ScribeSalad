WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.659
in the third cause of this sequence of

00:00:02.659 --> 00:00:02.669
in the third cause of this sequence of
 

00:00:02.669 --> 00:00:05.360
in the third cause of this sequence of
five causes you saw how error analysis

00:00:05.360 --> 00:00:05.370
five causes you saw how error analysis
 

00:00:05.370 --> 00:00:07.909
five causes you saw how error analysis
can help you focus your time on doing

00:00:07.909 --> 00:00:07.919
can help you focus your time on doing
 

00:00:07.919 --> 00:00:10.490
can help you focus your time on doing
the most useful work for your project

00:00:10.490 --> 00:00:10.500
the most useful work for your project
 

00:00:10.500 --> 00:00:13.370
the most useful work for your project
now beam search is an approximate search

00:00:13.370 --> 00:00:13.380
now beam search is an approximate search
 

00:00:13.380 --> 00:00:15.620
now beam search is an approximate search
algorithm also called a heuristic search

00:00:15.620 --> 00:00:15.630
algorithm also called a heuristic search
 

00:00:15.630 --> 00:00:17.480
algorithm also called a heuristic search
algorithm and so it doesn't always

00:00:17.480 --> 00:00:17.490
algorithm and so it doesn't always
 

00:00:17.490 --> 00:00:20.660
algorithm and so it doesn't always
output the most likely sentence as only

00:00:20.660 --> 00:00:20.670
output the most likely sentence as only
 

00:00:20.670 --> 00:00:24.050
output the most likely sentence as only
keeping track of B equals 3 or 10 or 100

00:00:24.050 --> 00:00:24.060
keeping track of B equals 3 or 10 or 100
 

00:00:24.060 --> 00:00:27.500
keeping track of B equals 3 or 10 or 100
top possibilities so what a beam search

00:00:27.500 --> 00:00:27.510
top possibilities so what a beam search
 

00:00:27.510 --> 00:00:29.839
top possibilities so what a beam search
makes a mistake in this video you

00:00:29.839 --> 00:00:29.849
makes a mistake in this video you
 

00:00:29.849 --> 00:00:32.600
makes a mistake in this video you
learned how error analysis interacts

00:00:32.600 --> 00:00:32.610
learned how error analysis interacts
 

00:00:32.610 --> 00:00:35.240
learned how error analysis interacts
with beam search and how you can figure

00:00:35.240 --> 00:00:35.250
with beam search and how you can figure
 

00:00:35.250 --> 00:00:37.790
with beam search and how you can figure
out whether it is the beam search

00:00:37.790 --> 00:00:37.800
out whether it is the beam search
 

00:00:37.800 --> 00:00:39.590
out whether it is the beam search
algorithm that's causing problems and

00:00:39.590 --> 00:00:39.600
algorithm that's causing problems and
 

00:00:39.600 --> 00:00:41.479
algorithm that's causing problems and
worth spending time on or whether it

00:00:41.479 --> 00:00:41.489
worth spending time on or whether it
 

00:00:41.489 --> 00:00:44.090
worth spending time on or whether it
might be your RNN model that is causing

00:00:44.090 --> 00:00:44.100
might be your RNN model that is causing
 

00:00:44.100 --> 00:00:45.770
might be your RNN model that is causing
problems that were spending time on

00:00:45.770 --> 00:00:45.780
problems that were spending time on
 

00:00:45.780 --> 00:00:47.840
problems that were spending time on
let's take a look at how to do error

00:00:47.840 --> 00:00:47.850
let's take a look at how to do error
 

00:00:47.850 --> 00:00:52.069
let's take a look at how to do error
analysis wood beam search let's use this

00:00:52.069 --> 00:00:52.079
analysis wood beam search let's use this
 

00:00:52.079 --> 00:00:56.660
analysis wood beam search let's use this
example of Jane Vizio free concept on so

00:00:56.660 --> 00:00:56.670
example of Jane Vizio free concept on so
 

00:00:56.670 --> 00:00:59.299
example of Jane Vizio free concept on so
let's say that in your machine

00:00:59.299 --> 00:00:59.309
let's say that in your machine
 

00:00:59.309 --> 00:01:01.430
let's say that in your machine
translation Jeff said your development

00:01:01.430 --> 00:01:01.440
translation Jeff said your development
 

00:01:01.440 --> 00:01:03.860
translation Jeff said your development
said the human provided this translation

00:01:03.860 --> 00:01:03.870
said the human provided this translation
 

00:01:03.870 --> 00:01:05.929
said the human provided this translation
and Jane visits Africa in September and

00:01:05.929 --> 00:01:05.939
and Jane visits Africa in September and
 

00:01:05.939 --> 00:01:08.810
and Jane visits Africa in September and
I'm going to call this y star so there's

00:01:08.810 --> 00:01:08.820
I'm going to call this y star so there's
 

00:01:08.820 --> 00:01:10.880
I'm going to call this y star so there's
a pretty good translation written by a

00:01:10.880 --> 00:01:10.890
a pretty good translation written by a
 

00:01:10.890 --> 00:01:14.090
a pretty good translation written by a
human but let's say that when you run

00:01:14.090 --> 00:01:14.100
human but let's say that when you run
 

00:01:14.100 --> 00:01:17.630
human but let's say that when you run
beam search on your learned RNN model on

00:01:17.630 --> 00:01:17.640
beam search on your learned RNN model on
 

00:01:17.640 --> 00:01:20.060
beam search on your learned RNN model on
your learn translation model it ends up

00:01:20.060 --> 00:01:20.070
your learn translation model it ends up
 

00:01:20.070 --> 00:01:22.580
your learn translation model it ends up
with this translation who thought why

00:01:22.580 --> 00:01:22.590
with this translation who thought why
 

00:01:22.590 --> 00:01:24.920
with this translation who thought why
hat Jane this is Africa lost September

00:01:24.920 --> 00:01:24.930
hat Jane this is Africa lost September
 

00:01:24.930 --> 00:01:27.320
hat Jane this is Africa lost September
which is a much worse translation of the

00:01:27.320 --> 00:01:27.330
which is a much worse translation of the
 

00:01:27.330 --> 00:01:29.090
which is a much worse translation of the
French sentence that she changes the

00:01:29.090 --> 00:01:29.100
French sentence that she changes the
 

00:01:29.100 --> 00:01:31.990
French sentence that she changes the
meaning this is not a Greek translation

00:01:31.990 --> 00:01:32.000
meaning this is not a Greek translation
 

00:01:32.000 --> 00:01:35.179
meaning this is not a Greek translation
now your model has two main components

00:01:35.179 --> 00:01:35.189
now your model has two main components
 

00:01:35.189 --> 00:01:37.999
now your model has two main components
there is a neural network model the

00:01:37.999 --> 00:01:38.009
there is a neural network model the
 

00:01:38.009 --> 00:01:40.999
there is a neural network model the
sequence the sequence model which

00:01:40.999 --> 00:01:41.009
sequence the sequence model which
 

00:01:41.009 --> 00:01:43.670
sequence the sequence model which
numbers is called that's your RNN model

00:01:43.670 --> 00:01:43.680
numbers is called that's your RNN model
 

00:01:43.680 --> 00:01:46.010
numbers is called that's your RNN model
it's really an encoder and a decoder and

00:01:46.010 --> 00:01:46.020
it's really an encoder and a decoder and
 

00:01:46.020 --> 00:01:49.190
it's really an encoder and a decoder and
you have your beam search offer from

00:01:49.190 --> 00:01:49.200
you have your beam search offer from
 

00:01:49.200 --> 00:01:52.130
you have your beam search offer from
which you're running with some B with B

00:01:52.130 --> 00:01:52.140
which you're running with some B with B
 

00:01:52.140 --> 00:01:55.520
which you're running with some B with B
and wanna be nice if we could attribute

00:01:55.520 --> 00:01:55.530
and wanna be nice if we could attribute
 

00:01:55.530 --> 00:01:57.709
and wanna be nice if we could attribute
this error there's not very good

00:01:57.709 --> 00:01:57.719
this error there's not very good
 

00:01:57.719 --> 00:01:59.420
this error there's not very good
translation to one of these two

00:01:59.420 --> 00:01:59.430
translation to one of these two
 

00:01:59.430 --> 00:02:02.510
translation to one of these two
components was it the RNN or really the

00:02:02.510 --> 00:02:02.520
components was it the RNN or really the
 

00:02:02.520 --> 00:02:05.060
components was it the RNN or really the
neural network that is more to blame or

00:02:05.060 --> 00:02:05.070
neural network that is more to blame or
 

00:02:05.070 --> 00:02:07.429
neural network that is more to blame or
is it the beam search offer that is more

00:02:07.429 --> 00:02:07.439
is it the beam search offer that is more
 

00:02:07.439 --> 00:02:10.969
is it the beam search offer that is more
to blame and what you saw in the third

00:02:10.969 --> 00:02:10.979
to blame and what you saw in the third
 

00:02:10.979 --> 00:02:13.040
to blame and what you saw in the third
cause of the sequence is that is always

00:02:13.040 --> 00:02:13.050
cause of the sequence is that is always
 

00:02:13.050 --> 00:02:13.610
cause of the sequence is that is always
10th

00:02:13.610 --> 00:02:13.620
10th
 

00:02:13.620 --> 00:02:15.890
10th
to collect more training days er that

00:02:15.890 --> 00:02:15.900
to collect more training days er that
 

00:02:15.900 --> 00:02:18.350
to collect more training days er that
never hurts so similar way is always

00:02:18.350 --> 00:02:18.360
never hurts so similar way is always
 

00:02:18.360 --> 00:02:20.869
never hurts so similar way is always
tempting to increase the beam work you

00:02:20.869 --> 00:02:20.879
tempting to increase the beam work you
 

00:02:20.879 --> 00:02:22.729
tempting to increase the beam work you
know that never hurts so it pretty much

00:02:22.729 --> 00:02:22.739
know that never hurts so it pretty much
 

00:02:22.739 --> 00:02:23.830
know that never hurts so it pretty much
never hurts

00:02:23.830 --> 00:02:23.840
never hurts
 

00:02:23.840 --> 00:02:26.690
never hurts
but just as getting more training data

00:02:26.690 --> 00:02:26.700
but just as getting more training data
 

00:02:26.700 --> 00:02:29.479
but just as getting more training data
by itself might not to get you to the

00:02:29.479 --> 00:02:29.489
by itself might not to get you to the
 

00:02:29.489 --> 00:02:32.000
by itself might not to get you to the
level performance you want in the same

00:02:32.000 --> 00:02:32.010
level performance you want in the same
 

00:02:32.010 --> 00:02:34.250
level performance you want in the same
way increasing the beam width by itself

00:02:34.250 --> 00:02:34.260
way increasing the beam width by itself
 

00:02:34.260 --> 00:02:37.190
way increasing the beam width by itself
might not get you to where you want to

00:02:37.190 --> 00:02:37.200
might not get you to where you want to
 

00:02:37.200 --> 00:02:39.680
might not get you to where you want to
go and so but how do you decide whether

00:02:39.680 --> 00:02:39.690
go and so but how do you decide whether
 

00:02:39.690 --> 00:02:42.259
go and so but how do you decide whether
or not improving the search algorithm is

00:02:42.259 --> 00:02:42.269
or not improving the search algorithm is
 

00:02:42.269 --> 00:02:44.990
or not improving the search algorithm is
a good use of your time so kids now you

00:02:44.990 --> 00:02:45.000
a good use of your time so kids now you
 

00:02:45.000 --> 00:02:47.119
a good use of your time so kids now you
can break the problem down and figure

00:02:47.119 --> 00:02:47.129
can break the problem down and figure
 

00:02:47.129 --> 00:02:48.830
can break the problem down and figure
out what's actually a good use of your

00:02:48.830 --> 00:02:48.840
out what's actually a good use of your
 

00:02:48.840 --> 00:02:49.610
out what's actually a good use of your
time

00:02:49.610 --> 00:02:49.620
time
 

00:02:49.620 --> 00:02:52.670
time
now the RNN via the neural network

00:02:52.670 --> 00:02:52.680
now the RNN via the neural network
 

00:02:52.680 --> 00:02:54.710
now the RNN via the neural network
opposed to call RNN breathing is the

00:02:54.710 --> 00:02:54.720
opposed to call RNN breathing is the
 

00:02:54.720 --> 00:03:00.170
opposed to call RNN breathing is the
encoder and the decoder it computes P of

00:03:00.170 --> 00:03:00.180
encoder and the decoder it computes P of
 

00:03:00.180 --> 00:03:04.009
encoder and the decoder it computes P of
Y given X so for example for a sentence

00:03:04.009 --> 00:03:04.019
Y given X so for example for a sentence
 

00:03:04.019 --> 00:03:07.190
Y given X so for example for a sentence
Jane visits Africa in September you plug

00:03:07.190 --> 00:03:07.200
Jane visits Africa in September you plug
 

00:03:07.200 --> 00:03:12.619
Jane visits Africa in September you plug
in Jane visits Africa again I'm ignoring

00:03:12.619 --> 00:03:12.629
in Jane visits Africa again I'm ignoring
 

00:03:12.629 --> 00:03:15.170
in Jane visits Africa again I'm ignoring
upper versus lower case for now right

00:03:15.170 --> 00:03:15.180
upper versus lower case for now right
 

00:03:15.180 --> 00:03:17.119
upper versus lower case for now right
and so on and this computes P of Y given

00:03:17.119 --> 00:03:17.129
and so on and this computes P of Y given
 

00:03:17.129 --> 00:03:22.729
and so on and this computes P of Y given
X so it turns out that the most useful

00:03:22.729 --> 00:03:22.739
X so it turns out that the most useful
 

00:03:22.739 --> 00:03:24.199
X so it turns out that the most useful
thing for you to do at this point this

00:03:24.199 --> 00:03:24.209
thing for you to do at this point this
 

00:03:24.209 --> 00:03:28.190
thing for you to do at this point this
is compute using this model to compute P

00:03:28.190 --> 00:03:28.200
is compute using this model to compute P
 

00:03:28.200 --> 00:03:32.270
is compute using this model to compute P
of Y star given X as well as to compute

00:03:32.270 --> 00:03:32.280
of Y star given X as well as to compute
 

00:03:32.280 --> 00:03:35.900
of Y star given X as well as to compute
P of Y hat given X using your own hand

00:03:35.900 --> 00:03:35.910
P of Y hat given X using your own hand
 

00:03:35.910 --> 00:03:38.449
P of Y hat given X using your own hand
model and then to see which of these two

00:03:38.449 --> 00:03:38.459
model and then to see which of these two
 

00:03:38.459 --> 00:03:40.729
model and then to see which of these two
is bigger so it's possible that the left

00:03:40.729 --> 00:03:40.739
is bigger so it's possible that the left
 

00:03:40.739 --> 00:03:42.199
is bigger so it's possible that the left
side is bigger than the right hand side

00:03:42.199 --> 00:03:42.209
side is bigger than the right hand side
 

00:03:42.209 --> 00:03:45.920
side is bigger than the right hand side
it's also possible that P of Y star is

00:03:45.920 --> 00:03:45.930
it's also possible that P of Y star is
 

00:03:45.930 --> 00:03:47.750
it's also possible that P of Y star is
less than P my have actually less than

00:03:47.750 --> 00:03:47.760
less than P my have actually less than
 

00:03:47.760 --> 00:03:49.819
less than P my have actually less than
an equal zero depending on which of

00:03:49.819 --> 00:03:49.829
an equal zero depending on which of
 

00:03:49.829 --> 00:03:52.039
an equal zero depending on which of
these two cases hold true you'd be able

00:03:52.039 --> 00:03:52.049
these two cases hold true you'd be able
 

00:03:52.049 --> 00:03:55.220
these two cases hold true you'd be able
to more clearly ascribe this particular

00:03:55.220 --> 00:03:55.230
to more clearly ascribe this particular
 

00:03:55.230 --> 00:03:58.189
to more clearly ascribe this particular
error disprove the back translation to

00:03:58.189 --> 00:03:58.199
error disprove the back translation to
 

00:03:58.199 --> 00:04:00.860
error disprove the back translation to
one of the RNN or the beam search

00:04:00.860 --> 00:04:00.870
one of the RNN or the beam search
 

00:04:00.870 --> 00:04:04.129
one of the RNN or the beam search
algorithm being at greater fault so

00:04:04.129 --> 00:04:04.139
algorithm being at greater fault so
 

00:04:04.139 --> 00:04:06.470
algorithm being at greater fault so
let's figure out the logic behind this

00:04:06.470 --> 00:04:06.480
let's figure out the logic behind this
 

00:04:06.480 --> 00:04:08.689
let's figure out the logic behind this
here are the two sentences from the

00:04:08.689 --> 00:04:08.699
here are the two sentences from the
 

00:04:08.699 --> 00:04:10.400
here are the two sentences from the
previous slide and remember we're going

00:04:10.400 --> 00:04:10.410
previous slide and remember we're going
 

00:04:10.410 --> 00:04:14.180
previous slide and remember we're going
to compute P of Y star given X and P of

00:04:14.180 --> 00:04:14.190
to compute P of Y star given X and P of
 

00:04:14.190 --> 00:04:16.640
to compute P of Y star given X and P of
Y hat give an X and see which of these

00:04:16.640 --> 00:04:16.650
Y hat give an X and see which of these
 

00:04:16.650 --> 00:04:19.039
Y hat give an X and see which of these
two and see which of these two is bigger

00:04:19.039 --> 00:04:19.049
two and see which of these two is bigger
 

00:04:19.049 --> 00:04:21.620
two and see which of these two is bigger
so they're going to be two cases in case

00:04:21.620 --> 00:04:21.630
so they're going to be two cases in case
 

00:04:21.630 --> 00:04:25.250
so they're going to be two cases in case
one P of Y star given X as output by the

00:04:25.250 --> 00:04:25.260
one P of Y star given X as output by the
 

00:04:25.260 --> 00:04:27.230
one P of Y star given X as output by the
RNN model

00:04:27.230 --> 00:04:27.240
RNN model
 

00:04:27.240 --> 00:04:30.650
RNN model
this greater than P of Y hat given X

00:04:30.650 --> 00:04:30.660
this greater than P of Y hat given X
 

00:04:30.660 --> 00:04:33.920
this greater than P of Y hat given X
what does this mean well the beam search

00:04:33.920 --> 00:04:33.930
what does this mean well the beam search
 

00:04:33.930 --> 00:04:38.180
what does this mean well the beam search
algorithm chose Y hat right the way you

00:04:38.180 --> 00:04:38.190
algorithm chose Y hat right the way you
 

00:04:38.190 --> 00:04:40.820
algorithm chose Y hat right the way you
got Y hat was you had an RNN there was

00:04:40.820 --> 00:04:40.830
got Y hat was you had an RNN there was
 

00:04:40.830 --> 00:04:45.140
got Y hat was you had an RNN there was
computing P of Y given X and beam says

00:04:45.140 --> 00:04:45.150
computing P of Y given X and beam says
 

00:04:45.150 --> 00:04:48.110
computing P of Y given X and beam says
job was to try to find the value of y

00:04:48.110 --> 00:04:48.120
job was to try to find the value of y
 

00:04:48.120 --> 00:04:50.589
job was to try to find the value of y
that gives that Outbacks

00:04:50.589 --> 00:04:50.599
that gives that Outbacks
 

00:04:50.599 --> 00:04:55.550
that gives that Outbacks
but in this case y star actually attains

00:04:55.550 --> 00:04:55.560
but in this case y star actually attains
 

00:04:55.560 --> 00:04:59.180
but in this case y star actually attains
a higher value for P of Y given X dent

00:04:59.180 --> 00:04:59.190
a higher value for P of Y given X dent
 

00:04:59.190 --> 00:05:01.790
a higher value for P of Y given X dent
in Y hat so what this allows you to

00:05:01.790 --> 00:05:01.800
in Y hat so what this allows you to
 

00:05:01.800 --> 00:05:04.550
in Y hat so what this allows you to
conclude is beam search is failing to

00:05:04.550 --> 00:05:04.560
conclude is beam search is failing to
 

00:05:04.560 --> 00:05:06.770
conclude is beam search is failing to
actually give you the value of y that

00:05:06.770 --> 00:05:06.780
actually give you the value of y that
 

00:05:06.780 --> 00:05:10.370
actually give you the value of y that
maximizes P of Y given X because you

00:05:10.370 --> 00:05:10.380
maximizes P of Y given X because you
 

00:05:10.380 --> 00:05:12.110
maximizes P of Y given X because you
know the one job that beam search had

00:05:12.110 --> 00:05:12.120
know the one job that beam search had
 

00:05:12.120 --> 00:05:14.689
know the one job that beam search had
was to find the value of y that makes us

00:05:14.689 --> 00:05:14.699
was to find the value of y that makes us
 

00:05:14.699 --> 00:05:17.990
was to find the value of y that makes us
really big but it shows y hat but y star

00:05:17.990 --> 00:05:18.000
really big but it shows y hat but y star
 

00:05:18.000 --> 00:05:19.999
really big but it shows y hat but y star
actually gets a much bigger value so in

00:05:19.999 --> 00:05:20.009
actually gets a much bigger value so in
 

00:05:20.009 --> 00:05:22.100
actually gets a much bigger value so in
this case you could conclude that beam

00:05:22.100 --> 00:05:22.110
this case you could conclude that beam
 

00:05:22.110 --> 00:05:25.640
this case you could conclude that beam
search is at fault now how about the

00:05:25.640 --> 00:05:25.650
search is at fault now how about the
 

00:05:25.650 --> 00:05:30.050
search is at fault now how about the
other case in case two P of Y star given

00:05:30.050 --> 00:05:30.060
other case in case two P of Y star given
 

00:05:30.060 --> 00:05:32.930
other case in case two P of Y star given
X is less than or equal to P or Y hat

00:05:32.930 --> 00:05:32.940
X is less than or equal to P or Y hat
 

00:05:32.940 --> 00:05:35.959
X is less than or equal to P or Y hat
given X and then you know either this or

00:05:35.959 --> 00:05:35.969
given X and then you know either this or
 

00:05:35.969 --> 00:05:38.120
given X and then you know either this or
this has got to be true so either case

00:05:38.120 --> 00:05:38.130
this has got to be true so either case
 

00:05:38.130 --> 00:05:41.240
this has got to be true so either case
one or case two has a whole troupe what

00:05:41.240 --> 00:05:41.250
one or case two has a whole troupe what
 

00:05:41.250 --> 00:05:43.310
one or case two has a whole troupe what
you conclude under case two

00:05:43.310 --> 00:05:43.320
you conclude under case two
 

00:05:43.320 --> 00:05:48.800
you conclude under case two
well in our example y star is a better

00:05:48.800 --> 00:05:48.810
well in our example y star is a better
 

00:05:48.810 --> 00:05:52.999
well in our example y star is a better
translation than Y hat but according to

00:05:52.999 --> 00:05:53.009
translation than Y hat but according to
 

00:05:53.009 --> 00:05:56.959
translation than Y hat but according to
the RNN P of Y star is less than pure Y

00:05:56.959 --> 00:05:56.969
the RNN P of Y star is less than pure Y
 

00:05:56.969 --> 00:05:59.600
the RNN P of Y star is less than pure Y
hat so saying that Y stars are less

00:05:59.600 --> 00:05:59.610
hat so saying that Y stars are less
 

00:05:59.610 --> 00:06:03.290
hat so saying that Y stars are less
likely output than Y hat so in this case

00:06:03.290 --> 00:06:03.300
likely output than Y hat so in this case
 

00:06:03.300 --> 00:06:08.270
likely output than Y hat so in this case
it seems that the RNN model is at fault

00:06:08.270 --> 00:06:08.280
it seems that the RNN model is at fault
 

00:06:08.280 --> 00:06:10.520
it seems that the RNN model is at fault
and it might be worth spending more time

00:06:10.520 --> 00:06:10.530
and it might be worth spending more time
 

00:06:10.530 --> 00:06:14.180
and it might be worth spending more time
working on the RNN there's some

00:06:14.180 --> 00:06:14.190
working on the RNN there's some
 

00:06:14.190 --> 00:06:15.140
working on the RNN there's some
subtleties here

00:06:15.140 --> 00:06:15.150
subtleties here
 

00:06:15.150 --> 00:06:17.629
subtleties here
pertaining to length normalizations and

00:06:17.629 --> 00:06:17.639
pertaining to length normalizations and
 

00:06:17.639 --> 00:06:20.209
pertaining to length normalizations and
I'm glossing over if there's some

00:06:20.209 --> 00:06:20.219
I'm glossing over if there's some
 

00:06:20.219 --> 00:06:21.589
I'm glossing over if there's some
subtleties pertaining to length

00:06:21.589 --> 00:06:21.599
subtleties pertaining to length
 

00:06:21.599 --> 00:06:24.920
subtleties pertaining to length
normalization I'm glossing over and if

00:06:24.920 --> 00:06:24.930
normalization I'm glossing over and if
 

00:06:24.930 --> 00:06:27.080
normalization I'm glossing over and if
you are using some sort of length

00:06:27.080 --> 00:06:27.090
you are using some sort of length
 

00:06:27.090 --> 00:06:29.209
you are using some sort of length
normalization instead of evaluating

00:06:29.209 --> 00:06:29.219
normalization instead of evaluating
 

00:06:29.219 --> 00:06:31.219
normalization instead of evaluating
these probabilities you should be

00:06:31.219 --> 00:06:31.229
these probabilities you should be
 

00:06:31.229 --> 00:06:32.899
these probabilities you should be
evaluating the optimization objective

00:06:32.899 --> 00:06:32.909
evaluating the optimization objective
 

00:06:32.909 --> 00:06:34.939
evaluating the optimization objective
that takes an account length

00:06:34.939 --> 00:06:34.949
that takes an account length
 

00:06:34.949 --> 00:06:35.839
that takes an account length
normalization

00:06:35.839 --> 00:06:35.849
normalization
 

00:06:35.849 --> 00:06:38.239
normalization
but ignoring that complication for now

00:06:38.239 --> 00:06:38.249
but ignoring that complication for now
 

00:06:38.249 --> 00:06:41.439
but ignoring that complication for now
in this case was it tells you is that

00:06:41.439 --> 00:06:41.449
in this case was it tells you is that
 

00:06:41.449 --> 00:06:44.869
in this case was it tells you is that
even though Y star is a better

00:06:44.869 --> 00:06:44.879
even though Y star is a better
 

00:06:44.879 --> 00:06:49.159
even though Y star is a better
translation the RNN astride y star a

00:06:49.159 --> 00:06:49.169
translation the RNN astride y star a
 

00:06:49.169 --> 00:06:52.219
translation the RNN astride y star a
lower probability than the inferior

00:06:52.219 --> 00:06:52.229
lower probability than the inferior
 

00:06:52.229 --> 00:06:54.290
lower probability than the inferior
translation so in this case I would say

00:06:54.290 --> 00:06:54.300
translation so in this case I would say
 

00:06:54.300 --> 00:06:56.749
translation so in this case I would say
the RNN model is at fault

00:06:56.749 --> 00:06:56.759
the RNN model is at fault
 

00:06:56.759 --> 00:07:00.469
the RNN model is at fault
so the error analysis process looks as

00:07:00.469 --> 00:07:00.479
so the error analysis process looks as
 

00:07:00.479 --> 00:07:02.689
so the error analysis process looks as
follows you'd go through the development

00:07:02.689 --> 00:07:02.699
follows you'd go through the development
 

00:07:02.699 --> 00:07:05.570
follows you'd go through the development
set and find the mistakes that the avram

00:07:05.570 --> 00:07:05.580
set and find the mistakes that the avram
 

00:07:05.580 --> 00:07:09.230
set and find the mistakes that the avram
aid in development set and so in this

00:07:09.230 --> 00:07:09.240
aid in development set and so in this
 

00:07:09.240 --> 00:07:13.999
aid in development set and so in this
example let's say that P of Y given X

00:07:13.999 --> 00:07:14.009
example let's say that P of Y given X
 

00:07:14.009 --> 00:07:18.139
example let's say that P of Y given X
was 2 by 10 to the minus 10 whereas pier

00:07:18.139 --> 00:07:18.149
was 2 by 10 to the minus 10 whereas pier
 

00:07:18.149 --> 00:07:20.779
was 2 by 10 to the minus 10 whereas pier
y had given X was 1 by 10 to the minus

00:07:20.779 --> 00:07:20.789
y had given X was 1 by 10 to the minus
 

00:07:20.789 --> 00:07:23.959
y had given X was 1 by 10 to the minus
10 using the logic from the previous

00:07:23.959 --> 00:07:23.969
10 using the logic from the previous
 

00:07:23.969 --> 00:07:27.919
10 using the logic from the previous
slide in this case we see that beam

00:07:27.919 --> 00:07:27.929
slide in this case we see that beam
 

00:07:27.929 --> 00:07:30.529
slide in this case we see that beam
search actually chose Y hat which has a

00:07:30.529 --> 00:07:30.539
search actually chose Y hat which has a
 

00:07:30.539 --> 00:07:33.019
search actually chose Y hat which has a
lower probability than y star so I would

00:07:33.019 --> 00:07:33.029
lower probability than y star so I would
 

00:07:33.029 --> 00:07:35.480
lower probability than y star so I would
say beam search is at fault so that

00:07:35.480 --> 00:07:35.490
say beam search is at fault so that
 

00:07:35.490 --> 00:07:37.850
say beam search is at fault so that
really that be and then you go through a

00:07:37.850 --> 00:07:37.860
really that be and then you go through a
 

00:07:37.860 --> 00:07:40.519
really that be and then you go through a
second mistake or second Battal point by

00:07:40.519 --> 00:07:40.529
second mistake or second Battal point by
 

00:07:40.529 --> 00:07:41.239
second mistake or second Battal point by
the algorithm

00:07:41.239 --> 00:07:41.249
the algorithm
 

00:07:41.249 --> 00:07:43.670
the algorithm
look at these probabilities and maybe

00:07:43.670 --> 00:07:43.680
look at these probabilities and maybe
 

00:07:43.680 --> 00:07:45.170
look at these probabilities and maybe
for the second example

00:07:45.170 --> 00:07:45.180
for the second example
 

00:07:45.180 --> 00:07:47.719
for the second example
you think the model is that for Eevee

00:07:47.719 --> 00:07:47.729
you think the model is that for Eevee
 

00:07:47.729 --> 00:07:51.319
you think the model is that for Eevee
the RNN model with R and you go through

00:07:51.319 --> 00:07:51.329
the RNN model with R and you go through
 

00:07:51.329 --> 00:07:53.749
the RNN model with R and you go through
more examples and sometimes the beam

00:07:53.749 --> 00:07:53.759
more examples and sometimes the beam
 

00:07:53.759 --> 00:07:55.549
more examples and sometimes the beam
searches that fault sometimes even

00:07:55.549 --> 00:07:55.559
searches that fault sometimes even
 

00:07:55.559 --> 00:07:59.179
searches that fault sometimes even
models at fault and so on and through

00:07:59.179 --> 00:07:59.189
models at fault and so on and through
 

00:07:59.189 --> 00:08:02.149
models at fault and so on and through
this process you can then carry out

00:08:02.149 --> 00:08:02.159
this process you can then carry out
 

00:08:02.159 --> 00:08:04.669
this process you can then carry out
error analysis to figure out what

00:08:04.669 --> 00:08:04.679
error analysis to figure out what
 

00:08:04.679 --> 00:08:06.679
error analysis to figure out what
fraction of errors are due to beam

00:08:06.679 --> 00:08:06.689
fraction of errors are due to beam
 

00:08:06.689 --> 00:08:10.969
fraction of errors are due to beam
search versus the RNN model and with an

00:08:10.969 --> 00:08:10.979
search versus the RNN model and with an
 

00:08:10.979 --> 00:08:13.609
search versus the RNN model and with an
error analysis process like this for

00:08:13.609 --> 00:08:13.619
error analysis process like this for
 

00:08:13.619 --> 00:08:16.790
error analysis process like this for
every example in your depth set where

00:08:16.790 --> 00:08:16.800
every example in your depth set where
 

00:08:16.800 --> 00:08:18.739
every example in your depth set where
the algorithm gives a much worse example

00:08:18.739 --> 00:08:18.749
the algorithm gives a much worse example
 

00:08:18.749 --> 00:08:20.689
the algorithm gives a much worse example
where the algorithm gives a much worse

00:08:20.689 --> 00:08:20.699
where the algorithm gives a much worse
 

00:08:20.699 --> 00:08:23.509
where the algorithm gives a much worse
output than the human translation you

00:08:23.509 --> 00:08:23.519
output than the human translation you
 

00:08:23.519 --> 00:08:25.969
output than the human translation you
can try to describe the error to either

00:08:25.969 --> 00:08:25.979
can try to describe the error to either
 

00:08:25.979 --> 00:08:29.959
can try to describe the error to either
the search algorithm or to the objective

00:08:29.959 --> 00:08:29.969
the search algorithm or to the objective
 

00:08:29.969 --> 00:08:31.819
the search algorithm or to the objective
function or to the RNN model that

00:08:31.819 --> 00:08:31.829
function or to the RNN model that
 

00:08:31.829 --> 00:08:33.679
function or to the RNN model that
generates the objective function that

00:08:33.679 --> 00:08:33.689
generates the objective function that
 

00:08:33.689 --> 00:08:36.379
generates the objective function that
beam search is supposed to be maximizing

00:08:36.379 --> 00:08:36.389
beam search is supposed to be maximizing
 

00:08:36.389 --> 00:08:39.110
beam search is supposed to be maximizing
and through this you can try to figure

00:08:39.110 --> 00:08:39.120
and through this you can try to figure
 

00:08:39.120 --> 00:08:41.209
and through this you can try to figure
out which of these components is

00:08:41.209 --> 00:08:41.219
out which of these components is
 

00:08:41.219 --> 00:08:44.179
out which of these components is
responsible for more errors and only if

00:08:44.179 --> 00:08:44.189
responsible for more errors and only if
 

00:08:44.189 --> 00:08:46.009
responsible for more errors and only if
you find that beam search is responsible

00:08:46.009 --> 00:08:46.019
you find that beam search is responsible
 

00:08:46.019 --> 00:08:47.780
you find that beam search is responsible
for a lot of errors then maybe it's

00:08:47.780 --> 00:08:47.790
for a lot of errors then maybe it's
 

00:08:47.790 --> 00:08:49.420
for a lot of errors then maybe it's
worth working hard to

00:08:49.420 --> 00:08:49.430
worth working hard to
 

00:08:49.430 --> 00:08:52.329
worth working hard to
is to be with whereas in contrast if you

00:08:52.329 --> 00:08:52.339
is to be with whereas in contrast if you
 

00:08:52.339 --> 00:08:55.480
is to be with whereas in contrast if you
find that the RNN model is at fault then

00:08:55.480 --> 00:08:55.490
find that the RNN model is at fault then
 

00:08:55.490 --> 00:08:57.490
find that the RNN model is at fault then
you could do a deeper layer of analysis

00:08:57.490 --> 00:08:57.500
you could do a deeper layer of analysis
 

00:08:57.500 --> 00:08:59.710
you could do a deeper layer of analysis
to try to figure out if you want to add

00:08:59.710 --> 00:08:59.720
to try to figure out if you want to add
 

00:08:59.720 --> 00:09:01.810
to try to figure out if you want to add
regularization or get more training data

00:09:01.810 --> 00:09:01.820
regularization or get more training data
 

00:09:01.820 --> 00:09:04.360
regularization or get more training data
or try different network architecture or

00:09:04.360 --> 00:09:04.370
or try different network architecture or
 

00:09:04.370 --> 00:09:06.940
or try different network architecture or
something else and so a lot of the

00:09:06.940 --> 00:09:06.950
something else and so a lot of the
 

00:09:06.950 --> 00:09:09.040
something else and so a lot of the
techniques that you saw in the third

00:09:09.040 --> 00:09:09.050
techniques that you saw in the third
 

00:09:09.050 --> 00:09:11.260
techniques that you saw in the third
course in the third course in this

00:09:11.260 --> 00:09:11.270
course in the third course in this
 

00:09:11.270 --> 00:09:13.780
course in the third course in this
sequence would be applicable there so

00:09:13.780 --> 00:09:13.790
sequence would be applicable there so
 

00:09:13.790 --> 00:09:16.960
sequence would be applicable there so
that's it for error analysis using beam

00:09:16.960 --> 00:09:16.970
that's it for error analysis using beam
 

00:09:16.970 --> 00:09:18.579
that's it for error analysis using beam
search I found this for clear error

00:09:18.579 --> 00:09:18.589
search I found this for clear error
 

00:09:18.589 --> 00:09:21.519
search I found this for clear error
analysis process very useful whenever

00:09:21.519 --> 00:09:21.529
analysis process very useful whenever
 

00:09:21.529 --> 00:09:23.650
analysis process very useful whenever
you have a approximate optimization

00:09:23.650 --> 00:09:23.660
you have a approximate optimization
 

00:09:23.660 --> 00:09:25.990
you have a approximate optimization
algorithm such as beam search that is

00:09:25.990 --> 00:09:26.000
algorithm such as beam search that is
 

00:09:26.000 --> 00:09:27.730
algorithm such as beam search that is
working to optimize some sort of

00:09:27.730 --> 00:09:27.740
working to optimize some sort of
 

00:09:27.740 --> 00:09:29.350
working to optimize some sort of
objective some sort of cost function

00:09:29.350 --> 00:09:29.360
objective some sort of cost function
 

00:09:29.360 --> 00:09:31.240
objective some sort of cost function
that is output by a learning algorithm

00:09:31.240 --> 00:09:31.250
that is output by a learning algorithm
 

00:09:31.250 --> 00:09:33.160
that is output by a learning algorithm
such as the sequence the sequence model

00:09:33.160 --> 00:09:33.170
such as the sequence the sequence model
 

00:09:33.170 --> 00:09:35.260
such as the sequence the sequence model
see one sequence RNN that we've been

00:09:35.260 --> 00:09:35.270
see one sequence RNN that we've been
 

00:09:35.270 --> 00:09:37.420
see one sequence RNN that we've been
discussing in these lectures so with

00:09:37.420 --> 00:09:37.430
discussing in these lectures so with
 

00:09:37.430 --> 00:09:39.910
discussing in these lectures so with
that I hope that you'd be more efficient

00:09:39.910 --> 00:09:39.920
that I hope that you'd be more efficient
 

00:09:39.920 --> 00:09:41.949
that I hope that you'd be more efficient
at making these types of models work

00:09:41.949 --> 00:09:41.959
at making these types of models work
 

00:09:41.959 --> 00:09:45.220
at making these types of models work
well for your applications

