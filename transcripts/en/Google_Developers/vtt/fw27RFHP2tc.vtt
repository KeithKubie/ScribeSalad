WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.413
[MUSIC]

00:00:05.299 --> 00:00:06.840
ADRIANA OLMOS:
Welcome to our session

00:00:06.840 --> 00:00:10.200
in multimodal interactions.

00:00:10.200 --> 00:00:11.810
So I would like to
start introducing

00:00:11.810 --> 00:00:14.150
my colleague Jared Strawderman.

00:00:14.150 --> 00:00:17.076
He's part of the
conversational design team,

00:00:17.076 --> 00:00:18.890
and he's also a lead
product designer

00:00:18.890 --> 00:00:21.944
for the Google Assistant.

00:00:21.944 --> 00:00:24.110
JARED STRAWDERMAN: And this
is my esteemed colleague

00:00:24.110 --> 00:00:25.520
Adriana Olmos.

00:00:25.520 --> 00:00:28.730
She is also a product designer
on Google Assistant's user

00:00:28.730 --> 00:00:30.770
experience team,
where she is in charge

00:00:30.770 --> 00:00:35.990
of the design effort for third
party multimodal experiences.

00:00:35.990 --> 00:00:37.580
Before we dive
right in, I wanted

00:00:37.580 --> 00:00:41.060
to share a personal
story, a story that I feel

00:00:41.060 --> 00:00:43.730
really captures a perfect
real world example

00:00:43.730 --> 00:00:45.710
of a multimodal experience.

00:00:45.710 --> 00:00:49.520
I was traveling to Asheville,
North Carolina, in October

00:00:49.520 --> 00:00:52.580
to attend a wedding
for two of my friends,

00:00:52.580 --> 00:00:54.270
and I'm originally
from West Virginia,

00:00:54.270 --> 00:00:56.460
so North Carolina is
not that far away,

00:00:56.460 --> 00:00:58.580
and I had been to
Asheville one or two times,

00:00:58.580 --> 00:01:00.200
but didn't know
the town very well

00:01:00.200 --> 00:01:02.990
and didn't have a good
idea where to get dinner.

00:01:02.990 --> 00:01:06.120
Luckily, my hotel
had a concierge desk,

00:01:06.120 --> 00:01:10.010
and so I talked to the
concierge about what I like,

00:01:10.010 --> 00:01:10.880
what I didn't like.

00:01:10.880 --> 00:01:11.960
We went back and forth.

00:01:11.960 --> 00:01:15.570
He was asking me questions about
how far I was willing to go,

00:01:15.570 --> 00:01:19.430
whether I had a car, and after
he talked to me for a while,

00:01:19.430 --> 00:01:21.420
he did something
very interesting.

00:01:21.420 --> 00:01:23.930
He narrowed it down
to a few restaurants,

00:01:23.930 --> 00:01:27.050
and then he handed me over the
menus to those restaurants.

00:01:27.050 --> 00:01:30.080
I took a moment to peruse those
menus, looked at the things

00:01:30.080 --> 00:01:34.940
that we were looking to
eat, and I made a decision,

00:01:34.940 --> 00:01:37.910
and I told the concierge
that we had decided

00:01:37.910 --> 00:01:41.060
to go to this southern
restaurant, at which time

00:01:41.060 --> 00:01:43.696
he actually resumed the
dialogue between me and him,

00:01:43.696 --> 00:01:45.320
and he said, well,
when you want to go?

00:01:45.320 --> 00:01:46.850
How many people are going?

00:01:46.850 --> 00:01:48.770
And he made the
reservation for me.

00:01:48.770 --> 00:01:51.050
So in this mutlimodal
interaction,

00:01:51.050 --> 00:01:54.170
the voice part is represented
by the dialogue between me

00:01:54.170 --> 00:01:56.630
and the concierge,
and the visual part

00:01:56.630 --> 00:02:01.040
is represented by the menus, and
to me, this kind of epitomizes

00:02:01.040 --> 00:02:03.560
what a mutlimodal experience
looks like in the wild.

00:02:06.090 --> 00:02:10.470
Designing mutlimodal experiences
is an interdisciplinary effort.

00:02:10.470 --> 00:02:13.380
My background is
in voice design,

00:02:13.380 --> 00:02:15.510
so I've been designing
conversational interfaces

00:02:15.510 --> 00:02:17.170
for several years now.

00:02:17.170 --> 00:02:20.190
So anything that involves
spoken input and the things

00:02:20.190 --> 00:02:23.580
that you hear, that's
my area of expertise.

00:02:23.580 --> 00:02:27.000
Adriana comes from a more
traditional visual interaction

00:02:27.000 --> 00:02:30.150
design background-- web
sites, mobile applications,

00:02:30.150 --> 00:02:31.230
and so forth.

00:02:31.230 --> 00:02:35.460
But it takes combining these
two efforts, along with things

00:02:35.460 --> 00:02:40.050
like motion design, UX writing,
user research, visual design,

00:02:40.050 --> 00:02:44.220
to create a really
cohesive monolithic user

00:02:44.220 --> 00:02:46.410
experience from multimodality.

00:02:46.410 --> 00:02:49.110
But before we go any further,
let's take a quick step back

00:02:49.110 --> 00:02:51.900
and talk about how
we, as human beings,

00:02:51.900 --> 00:02:55.316
experience things
in the real world.

00:02:55.316 --> 00:02:56.940
ADRIANA OLMOS: So
imagine the last time

00:02:56.940 --> 00:02:59.520
you were walking on the beach.

00:02:59.520 --> 00:03:01.575
You could literally
hear the ocean,

00:03:01.575 --> 00:03:05.310
smell the salinity of
the water and touch it,

00:03:05.310 --> 00:03:08.580
and it's all these sensors
triggered at you simultaneously

00:03:08.580 --> 00:03:11.810
that makes a
compelling experience.

00:03:11.810 --> 00:03:14.360
By contrast, today
our technology

00:03:14.360 --> 00:03:21.150
is mapping to mostly three
main senses, sight, hearing,

00:03:21.150 --> 00:03:22.100
and touch.

00:03:22.100 --> 00:03:24.860
And actually, the way
that we consume content

00:03:24.860 --> 00:03:28.122
is basically based
on sight and hearing.

00:03:28.122 --> 00:03:29.580
So for the sake of
this talk, we're

00:03:29.580 --> 00:03:31.350
going to be focusing
in these two.

00:03:34.520 --> 00:03:36.170
JARED STRAWDERMAN:
So in order to have

00:03:36.170 --> 00:03:39.920
any chance at all of designing
a compelling mutlimodal

00:03:39.920 --> 00:03:43.520
experience, it's an absolute
prerequisite for you

00:03:43.520 --> 00:03:47.750
to know what each modality does
well and what it does poorly.

00:03:47.750 --> 00:03:50.070
So for example, voice
does some things really,

00:03:50.070 --> 00:03:52.100
really extremely
well, and other things

00:03:52.100 --> 00:03:55.350
it's just abjectly terrible at.

00:03:55.350 --> 00:03:57.350
So we're going to talk a
little bit about those,

00:03:57.350 --> 00:03:58.820
and it's really
important for you

00:03:58.820 --> 00:04:00.920
to recognize what those
are, so you can leverage

00:04:00.920 --> 00:04:03.710
those liabilities and avoid--

00:04:03.710 --> 00:04:07.747
leverage the strengths, rather,
and avoid the liabilities.

00:04:07.747 --> 00:04:08.830
So let's start with voice.

00:04:11.700 --> 00:04:15.830
So voice is very, very good
at flattening a menu structure

00:04:15.830 --> 00:04:18.779
and providing direct
access to what you need.

00:04:18.779 --> 00:04:20.880
Before I started
using Google Home,

00:04:20.880 --> 00:04:23.340
I used to think that
my mobile device was

00:04:23.340 --> 00:04:25.120
the epitome of convenience.

00:04:25.120 --> 00:04:27.180
It's always right
here in my pocket,

00:04:27.180 --> 00:04:30.390
and I have really quick
access to information,

00:04:30.390 --> 00:04:33.870
and every facet of my life
is on this beautiful device.

00:04:33.870 --> 00:04:35.850
But then, when I
started interacting

00:04:35.850 --> 00:04:38.340
with Google Home
across the room,

00:04:38.340 --> 00:04:39.870
in kind of a far
field way, I was

00:04:39.870 --> 00:04:43.050
able to ask it across the room,
what's the score of the West

00:04:43.050 --> 00:04:45.030
Virginia Gonzaga
basketball game,

00:04:45.030 --> 00:04:46.980
and it would give me
that score immediately.

00:04:46.980 --> 00:04:49.530
Compare that to pulling
the phone out of my pocket,

00:04:49.530 --> 00:04:52.220
unlocking it, going
to the sports app,

00:04:52.220 --> 00:04:55.500
going to the college basketball
section, looking up the score,

00:04:55.500 --> 00:04:57.120
and there's the score.

00:04:57.120 --> 00:05:00.060
When you do that with the
far field interaction,

00:05:00.060 --> 00:05:02.430
it makes pulling the
phone out of your pocket

00:05:02.430 --> 00:05:04.740
seem too inconvenient,
and in many ways,

00:05:04.740 --> 00:05:07.720
that's changed the game.

00:05:07.720 --> 00:05:12.060
So as compelling and exciting
as the benefits of voice

00:05:12.060 --> 00:05:14.310
are, and there are
many, it doesn't

00:05:14.310 --> 00:05:16.410
come without it's
drawbacks, and one

00:05:16.410 --> 00:05:19.500
of the biggest drawbacks of
using Voice as an interface

00:05:19.500 --> 00:05:21.420
is its ephemeral nature.

00:05:21.420 --> 00:05:24.960
So users can only retain a
certain amount of content,

00:05:24.960 --> 00:05:27.720
a finite amount of content,
in their short-term memories,

00:05:27.720 --> 00:05:29.700
and so as designers,
you have to manage

00:05:29.700 --> 00:05:31.650
that if they have
any hope of getting

00:05:31.650 --> 00:05:33.420
through the interaction at all.

00:05:33.420 --> 00:05:36.727
So for those of you who may
have a visual design background,

00:05:36.727 --> 00:05:38.310
or for those of you
who have developed

00:05:38.310 --> 00:05:40.530
web sites and
mobile applications,

00:05:40.530 --> 00:05:43.350
imagine if the only
visual affordance that you

00:05:43.350 --> 00:05:46.140
had to present
content to your users

00:05:46.140 --> 00:05:49.590
was a scrolling ticker that
goes across the screen,

00:05:49.590 --> 00:05:54.450
and the user can only remember
that text that just evaporated

00:05:54.450 --> 00:05:56.310
on the edge of the
screen to help them

00:05:56.310 --> 00:05:57.970
get through the interaction.

00:05:57.970 --> 00:06:01.200
This is what voice designers
grapple with every day,

00:06:01.200 --> 00:06:05.004
and this is what we, as
designers, have to manage.

00:06:05.004 --> 00:06:06.670
And I'm not even sure
if it had a chance

00:06:06.670 --> 00:06:08.336
to cycle all the way
through, but that's

00:06:08.336 --> 00:06:12.070
what the ticker said,
so obviously, content--

00:06:12.070 --> 00:06:14.580
big chunks of content
are better presented,

00:06:14.580 --> 00:06:18.040
and it's a lot easier for users
to absorb, on a static screen

00:06:18.040 --> 00:06:19.040
like this.

00:06:19.040 --> 00:06:20.810
Let me give a really
quick example.

00:06:20.810 --> 00:06:23.980
Let's say I want to ask for the
hours for a restaurant in San

00:06:23.980 --> 00:06:24.760
Jose.

00:06:24.760 --> 00:06:26.620
I would just ask
Google Assistant,

00:06:26.620 --> 00:06:28.720
when is Black Sheep
Brasserie open?

00:06:28.720 --> 00:06:31.499
Here's one approach to
answering that question.

00:06:31.499 --> 00:06:32.165
[AUDIO PLAYBACK]

00:06:32.165 --> 00:06:35.020
- Black Sheep Brasserie
is open today from 5:00

00:06:35.020 --> 00:06:38.680
to 9:30 PM, and tomorrow
from 5:00 to 10:00 PM,

00:06:38.680 --> 00:06:42.340
on Saturday from 5:00 to 10:00
PM, on Sunday from 10:00 AM

00:06:42.340 --> 00:06:45.392
to 2:00 PM, and again
from 5:00 to 9:00 PM,

00:06:45.392 --> 00:06:46.600
and they're closed on Monday.

00:06:46.600 --> 00:06:47.532
[END PLAYBACK]

00:06:47.532 --> 00:06:50.550
JARED STRAWDERMAN: All right, so
while it's usually a good idea

00:06:50.550 --> 00:06:54.580
to anticipate users needs and
over deliver, when you present

00:06:54.580 --> 00:06:57.690
so much content that the
user can't grok any of it,

00:06:57.690 --> 00:06:59.280
it becomes meaningless.

00:06:59.280 --> 00:07:01.401
Here's another approach to that.

00:07:01.401 --> 00:07:02.067
[AUDIO PLAYBACK]

00:07:02.067 --> 00:07:06.087
- Black Sheep Brasserie is open
today from 5:00 to 9:30 PM.

00:07:06.087 --> 00:07:07.920
Here are the hours for
the rest of the week.

00:07:07.920 --> 00:07:08.370
[END PLAYBACK]

00:07:08.370 --> 00:07:09.720
JARED STRAWDERMAN: So
you give the salient--

00:07:09.720 --> 00:07:11.670
the most salient
pieces of information

00:07:11.670 --> 00:07:14.040
upfront, i.e. the
hours for today,

00:07:14.040 --> 00:07:16.860
and then you defer to the
screen for more dense content

00:07:16.860 --> 00:07:18.700
that the user can
peruse at their leisure.

00:07:22.340 --> 00:07:25.720
And then finally, one of
the drawbacks of using voice

00:07:25.720 --> 00:07:30.800
as an interface is when I
talk to Google Assistant,

00:07:30.800 --> 00:07:32.150
people can hear me.

00:07:32.150 --> 00:07:34.280
That's kind of how
talking works, right?

00:07:34.280 --> 00:07:38.090
So imagine if the next-- the
person next to you, right here,

00:07:38.090 --> 00:07:40.850
in the middle of Adriana
about to say something really

00:07:40.850 --> 00:07:44.540
profound and amazing decides
to pull out their Pixel phone

00:07:44.540 --> 00:07:47.060
and ask Google Assistant what
the score of the Warriors game

00:07:47.060 --> 00:07:48.170
was on Tuesday.

00:07:48.170 --> 00:07:50.120
That would be really
obnoxious, right?

00:07:50.120 --> 00:07:51.990
And of course it is.

00:07:51.990 --> 00:07:55.130
But there are some
social taboos associated

00:07:55.130 --> 00:07:58.800
with using voice interactions
in a public setting like this,

00:07:58.800 --> 00:08:00.950
and even though the
device is capable of it,

00:08:00.950 --> 00:08:04.820
it really limits the range and
access of voice interactions

00:08:04.820 --> 00:08:06.590
in public settings like this.

00:08:06.590 --> 00:08:09.350
By the way, if you're really
interested in this topic,

00:08:09.350 --> 00:08:11.240
there's an incredible
session happening

00:08:11.240 --> 00:08:15.510
at 3:30 today on stage five
given by Daniel Padgett.

00:08:15.510 --> 00:08:18.140
He's going to be talking
about the right use cases.

00:08:18.140 --> 00:08:20.390
The right voice interactions
for your application.

00:08:20.390 --> 00:08:23.775
I highly recommend it.

00:08:23.775 --> 00:08:26.350
ADRIANA OLMOS: By
contrast, visual interfaces

00:08:26.350 --> 00:08:30.400
can be as permanent
or complex and dynamic

00:08:30.400 --> 00:08:31.670
as we want it to be.

00:08:31.670 --> 00:08:33.909
And in fact, in just a
matter of milliseconds,

00:08:33.909 --> 00:08:36.600
there's a lot of information
that can be delivered.

00:08:36.600 --> 00:08:39.010
Take, for instance, just as
simple as a traffic light,

00:08:39.010 --> 00:08:41.860
and we can control
the timing of the red

00:08:41.860 --> 00:08:44.370
to green in order to
convey some meaning.

00:08:44.370 --> 00:08:47.710
Or it can be as sophisticated
as the YouTube app, where

00:08:47.710 --> 00:08:51.670
you can consume content, also
follow up what is happening,

00:08:51.670 --> 00:08:53.830
what is trending, and
[INAUDIBLE] what did

00:08:53.830 --> 00:08:55.570
you like it or not, that video.

00:08:55.570 --> 00:08:59.990
There's a lot of things that
you can do just in one screen.

00:08:59.990 --> 00:09:02.050
Now the question
is, how are we going

00:09:02.050 --> 00:09:06.230
to be combining voice and
visual information in a way that

00:09:06.230 --> 00:09:10.880
is beyond passive consumption,
that is actually for building

00:09:10.880 --> 00:09:12.950
interactive experiences?

00:09:12.950 --> 00:09:16.190
And one approach is
through conversation.

00:09:16.190 --> 00:09:18.920
In a natural conversation
that we have with people,

00:09:18.920 --> 00:09:22.110
we point at things,
we describe things,

00:09:22.110 --> 00:09:24.521
and this is nice back and
forth among human beings,

00:09:24.521 --> 00:09:25.520
and just happen natural.

00:09:28.300 --> 00:09:32.520
And the thing is when we start
thinking about applications

00:09:32.520 --> 00:09:37.520
and happening on mobile,
there's a lot of information

00:09:37.520 --> 00:09:40.440
that we convey right away,
but we have to be careful.

00:09:40.440 --> 00:09:43.920
For instance, let's
think of this mobile app

00:09:43.920 --> 00:09:45.650
for ordering food.

00:09:45.650 --> 00:09:49.420
From here, there's a lot of
things that I can quickly do.

00:09:49.420 --> 00:09:52.020
But if you remember the last
time you called or you walked

00:09:52.020 --> 00:09:54.720
into the store, you're
actually having a conversation

00:09:54.720 --> 00:09:58.020
with the person, and there's
information or just information

00:09:58.020 --> 00:10:00.430
that deliver back and forth.

00:10:00.430 --> 00:10:02.130
So if you put
these two together,

00:10:02.130 --> 00:10:05.130
they look really different.

00:10:05.130 --> 00:10:07.290
And in fact, probably
it's more closer

00:10:07.290 --> 00:10:09.880
where we're seeing
today with chatbots

00:10:09.880 --> 00:10:14.140
and what we're starting to
experience with the assistant.

00:10:14.140 --> 00:10:16.590
So one important
piece of information

00:10:16.590 --> 00:10:18.990
to take into account
is that not because you

00:10:18.990 --> 00:10:23.080
have an app on your phone, that
should be your starting point.

00:10:23.080 --> 00:10:25.500
It should be more a
conversation with a human,

00:10:25.500 --> 00:10:28.590
where should be inspired
in order to start building

00:10:28.590 --> 00:10:31.440
your applications that is
going to live in the Google

00:10:31.440 --> 00:10:34.420
Assistant.

00:10:34.420 --> 00:10:38.460
The multimodal interaction
is not something new at all,

00:10:38.460 --> 00:10:41.890
and in fact, you have
experienced this in the past.

00:10:41.890 --> 00:10:44.150
You probably tried
a karaoke machine.

00:10:44.150 --> 00:10:46.520
There is sound in
the background,

00:10:46.520 --> 00:10:50.770
and then you have the visuals,
and then you're singing along.

00:10:50.770 --> 00:10:52.370
So there's a lot of
things happening,

00:10:52.370 --> 00:10:54.980
and that's a mutlimodal
experience as well.

00:10:54.980 --> 00:10:58.810
We also experience
asking Google questions,

00:10:58.810 --> 00:11:03.010
and then Google will present
the search results in a list.

00:11:03.010 --> 00:11:06.070
But what we have to
think is how we're

00:11:06.070 --> 00:11:10.930
going to be combining these two
modalities in a way that is not

00:11:10.930 --> 00:11:13.190
overwhelming to the user.

00:11:13.190 --> 00:11:17.140
So Jared and I are very
familiar with the strengths

00:11:17.140 --> 00:11:23.080
and weaknesses of these
modalities, voice and visual,

00:11:23.080 --> 00:11:25.750
but the problem is, is
that it's easy to fall

00:11:25.750 --> 00:11:29.290
into the trap of because
we design for one surface,

00:11:29.290 --> 00:11:31.240
we know how to
design it for all.

00:11:31.240 --> 00:11:33.880
And in fact, that was the
inspiration of this talk.

00:11:33.880 --> 00:11:36.340
It's not that simple, and
there's a lot of factors

00:11:36.340 --> 00:11:40.130
that we need to take
into consideration.

00:11:40.130 --> 00:11:43.000
So we're going to be
talking about these factors

00:11:43.000 --> 00:11:45.880
that we wanted to take
into consideration when

00:11:45.880 --> 00:11:48.190
we think of all these
platforms that we're

00:11:48.190 --> 00:11:50.494
going to be designing for.

00:11:50.494 --> 00:11:52.660
JARED STRAWDERMAN: So we
wanted to talk a little bit

00:11:52.660 --> 00:11:54.770
about these factors
that we've identified

00:11:54.770 --> 00:11:58.900
to help you think about how
your experiences will manifest

00:11:58.900 --> 00:12:01.720
as Google Assistant
comes to more surfaces,

00:12:01.720 --> 00:12:07.540
and these factors are,
is your user in motion?

00:12:07.540 --> 00:12:09.610
Is the device
designed to be used

00:12:09.610 --> 00:12:12.310
as the user is walking,
running, driving,

00:12:12.310 --> 00:12:15.490
or some other situation that
makes the screen otherwise

00:12:15.490 --> 00:12:16.750
inaccessible?

00:12:16.750 --> 00:12:20.590
Think of your phone
as opposed to your TV.

00:12:20.590 --> 00:12:22.360
The next is the environment.

00:12:22.360 --> 00:12:26.230
Is the device designed to be
used in private, or anywhere?

00:12:26.230 --> 00:12:28.270
And is there a
one-to-one relationship

00:12:28.270 --> 00:12:31.180
between the user and the
device, like your phone,

00:12:31.180 --> 00:12:34.690
or is it designed to be
shared among a group of users,

00:12:34.690 --> 00:12:37.000
like a Google Home?

00:12:37.000 --> 00:12:38.860
The next is proximity.

00:12:38.860 --> 00:12:43.030
So are you close enough to tap--
to quickly tap on the device

00:12:43.030 --> 00:12:44.300
to interact with it?

00:12:44.300 --> 00:12:48.910
Think of a wearable, as opposed
to something like Google Home

00:12:48.910 --> 00:12:53.220
or your TV that's optimized
for far field interactions.

00:12:53.220 --> 00:12:55.770
The next is audio capability.

00:12:55.770 --> 00:12:57.930
Does your device just
have a very small mic

00:12:57.930 --> 00:13:00.750
that can capture speech
from a few feet away,

00:13:00.750 --> 00:13:03.630
or does it have an
entire array of mics

00:13:03.630 --> 00:13:07.600
that is designed to capture
speech from across the room?

00:13:07.600 --> 00:13:09.670
And of course,
visual capability.

00:13:09.670 --> 00:13:13.330
Do you have a full ergonomically
compatible 4D keyboard

00:13:13.330 --> 00:13:16.900
like a laptop, or is it a
smaller one like on your phone?

00:13:16.900 --> 00:13:19.750
Or are you dealing with
something even more primitive,

00:13:19.750 --> 00:13:22.150
like a D-pad on a TV remote?

00:13:22.150 --> 00:13:25.150
And then finally,
visual output, and that

00:13:25.150 --> 00:13:29.800
basically just really
boils down to screen size.

00:13:29.800 --> 00:13:33.520
So these are the factors that
we want you to consider and keep

00:13:33.520 --> 00:13:36.190
in mind, and we're going to
go through a few of these

00:13:36.190 --> 00:13:38.740
on some surfaces where
Google Assistant has already

00:13:38.740 --> 00:13:42.040
been deployed and where we
anticipate that it may show up.

00:13:42.040 --> 00:13:43.420
So keep in mind
that this is kind

00:13:43.420 --> 00:13:47.830
of a future-looking, a
forward-looking presentation

00:13:47.830 --> 00:13:49.960
to give you some
guidelines by which

00:13:49.960 --> 00:13:55.490
to anticipate how to deploy
your actions on other surfaces.

00:13:55.490 --> 00:13:58.390
So let's talk about
Google Home first,

00:13:58.390 --> 00:14:00.730
since it was such
prominently presented

00:14:00.730 --> 00:14:05.080
in yesterday's keynote, and
it's getting a lot of traction

00:14:05.080 --> 00:14:06.680
in the marketplace.

00:14:06.680 --> 00:14:09.500
So I mentioned before
about how voice

00:14:09.500 --> 00:14:12.430
is really incredible at
flattening the menu structure,

00:14:12.430 --> 00:14:14.170
and it's really,
really convenient,

00:14:14.170 --> 00:14:16.150
but there's another
upside of speech

00:14:16.150 --> 00:14:18.310
that I think is really
important to present

00:14:18.310 --> 00:14:21.640
in the context of Google
Home, and it's that speech--

00:14:21.640 --> 00:14:25.030
what I'm doing right now,
that is the interface.

00:14:25.030 --> 00:14:27.400
You're able to interact
with Google Home in a way

00:14:27.400 --> 00:14:30.160
that you've been doing since
you were two years old, by just

00:14:30.160 --> 00:14:31.450
speaking to it.

00:14:31.450 --> 00:14:34.510
So there aren't any manuals,
there's no tutorials,

00:14:34.510 --> 00:14:36.070
there's no learning curve.

00:14:36.070 --> 00:14:39.580
All you have to do is know
what kinds of features

00:14:39.580 --> 00:14:41.590
Google Assistant
generally supports,

00:14:41.590 --> 00:14:45.250
like weather and
sports, and you just

00:14:45.250 --> 00:14:48.010
ask for those things the way
you would ask another person.

00:14:50.720 --> 00:14:52.540
So if we look at
some of the factors

00:14:52.540 --> 00:14:55.240
that I outlined earlier,
if we look about

00:14:55.240 --> 00:14:57.610
whether the user is in
motion, they're not, right?

00:14:57.610 --> 00:14:59.980
You put your Google Home
on a kitchen counter,

00:14:59.980 --> 00:15:02.800
or your nightstand, and
it's usually planted there

00:15:02.800 --> 00:15:05.380
for quite a while.

00:15:05.380 --> 00:15:09.120
Google Home is deployed in a
private setting, your home,

00:15:09.120 --> 00:15:11.030
but it's shared among
a group of users.

00:15:11.030 --> 00:15:13.617
So it's somewhat
private, and you

00:15:13.617 --> 00:15:15.700
don't have to be close
enough to interact with it.

00:15:15.700 --> 00:15:18.190
It's optimized for far
field interactions.

00:15:18.190 --> 00:15:21.090
And if we look at
input-output capabilities,

00:15:21.090 --> 00:15:23.410
it's got really, really
strong capabilities

00:15:23.410 --> 00:15:25.880
for audio, both
input and output,

00:15:25.880 --> 00:15:27.970
but very little on
the visual side.

00:15:27.970 --> 00:15:33.010
So we've distilled three really
kind of overarching guidelines

00:15:33.010 --> 00:15:38.890
for how to design for
actions on Google Home,

00:15:38.890 --> 00:15:43.860
and the first one is
don't read, listen.

00:15:43.860 --> 00:15:46.950
And what I mean by that
is as you build actions,

00:15:46.950 --> 00:15:50.100
and as you develop
experiences on Google Home,

00:15:50.100 --> 00:15:53.340
you may be pulling content
from some online source

00:15:53.340 --> 00:15:56.040
or something, and a
lot of these sources

00:15:56.040 --> 00:15:59.000
are optimized for
written content, right?

00:15:59.000 --> 00:16:01.210
They're designed to
be read with the eye,

00:16:01.210 --> 00:16:03.170
not the ear, so
you may think-- it

00:16:03.170 --> 00:16:04.920
may be tempting to
think that you can just

00:16:04.920 --> 00:16:08.130
take these sources, run them
through a text to speech

00:16:08.130 --> 00:16:11.260
engine, and voila, you
have your voice interface.

00:16:11.260 --> 00:16:12.850
It's not that simple.

00:16:12.850 --> 00:16:14.260
Let's take an example.

00:16:14.260 --> 00:16:16.650
So in the context of
a weather forecast,

00:16:16.650 --> 00:16:20.340
for example, this string of
text makes perfect sense to you,

00:16:20.340 --> 00:16:20.940
right?

00:16:20.940 --> 00:16:23.880
If I were to translate
this string of text

00:16:23.880 --> 00:16:25.590
into something
that's appropriate

00:16:25.590 --> 00:16:29.340
for spoken language, I
would say, in Mountain View,

00:16:29.340 --> 00:16:32.100
it's sunny with a high
of 77 degrees with winds

00:16:32.100 --> 00:16:35.610
out of the north to northwest
at 10 to 15 miles an hour.

00:16:35.610 --> 00:16:37.680
But listen to how this--

00:16:37.680 --> 00:16:39.570
this is how ridiculous
this is when

00:16:39.570 --> 00:16:41.800
you run it through a
text to speech engine.

00:16:41.800 --> 00:16:42.466
[AUDIO PLAYBACK]

00:16:42.466 --> 00:16:43.310
- Sunny.

00:16:43.310 --> 00:16:47.045
High 77 F, winds at
NW at 10 to 15 MPH.

00:16:47.045 --> 00:16:47.795
[END PLAYBACK]

00:16:47.795 --> 00:16:50.420
JARED STRAWDERMAN: Totally
incomprehensible, right?

00:16:50.420 --> 00:16:54.980
So just-- when you get your data
source and you run it through,

00:16:54.980 --> 00:16:58.160
just make sure that it's
appropriate for spoken content.

00:16:58.160 --> 00:16:58.970
Don't read.

00:16:58.970 --> 00:17:01.700
Just take a few samples,
run them through your text

00:17:01.700 --> 00:17:04.490
to speech engine, and
don't look at the text,

00:17:04.490 --> 00:17:07.849
listen to it to see if
you can understand it.

00:17:07.849 --> 00:17:10.910
The next one is avoid
information overload.

00:17:10.910 --> 00:17:13.970
I've already kind of lamented
about the ephemeral nature

00:17:13.970 --> 00:17:16.280
of speech, but I think
it bears repeating.

00:17:16.280 --> 00:17:20.369
Just be careful how much
content you present to the user.

00:17:20.369 --> 00:17:24.140
So if I ask Google
Assistant on Home

00:17:24.140 --> 00:17:25.849
what movies are out
right now, or you

00:17:25.849 --> 00:17:28.219
want to develop an
experience like this,

00:17:28.219 --> 00:17:30.484
this is one approach.

00:17:30.484 --> 00:17:31.150
[AUDIO PLAYBACK]

00:17:31.150 --> 00:17:33.470
- Here's what's playing
at your favorite theater,

00:17:33.470 --> 00:17:36.900
"Alien: Covenant", "Diary of
a Wimpy Kid", "The Commune",

00:17:36.900 --> 00:17:38.970
"Champion",
"Wakefield", "Guardians

00:17:38.970 --> 00:17:41.610
of the Galaxy: Volume
2", "Snatched",

00:17:41.610 --> 00:17:44.220
"The Fate of the
Furious", "The Boss Baby",

00:17:44.220 --> 00:17:47.040
"Smurfs: the Lost Village",
"Gifted", and "The Circle".

00:17:47.040 --> 00:17:48.167
Do any of these sound good?

00:17:48.167 --> 00:17:48.750
[END PLAYBACK]

00:17:48.750 --> 00:17:50.770
JARED STRAWDERMAN: So
not only does it over--

00:17:50.770 --> 00:17:52.830
does it bombard
you with 12 movies,

00:17:52.830 --> 00:17:54.760
it forces you to make
a decision afterward.

00:17:54.760 --> 00:17:57.330
So it's kind of a
stressful experience.

00:17:57.330 --> 00:17:58.775
Compare that to this.

00:17:58.775 --> 00:17:59.441
[AUDIO PLAYBACK]

00:17:59.441 --> 00:18:02.460
- There are 12 movies playing
at your favorite theater.

00:18:02.460 --> 00:18:04.080
Here's what's new
this week, "Alien:

00:18:04.080 --> 00:18:07.500
Covenant", "Diary of a Wimpy
Kid", and "The Commune".

00:18:07.500 --> 00:18:09.662
Should I tell you about
any of them, or keep going?

00:18:09.662 --> 00:18:10.245
[END PLAYBACK]

00:18:10.245 --> 00:18:13.000
JARED: So that presents it in
much more manageable chunks

00:18:13.000 --> 00:18:16.500
that's easier for
the user to manage.

00:18:16.500 --> 00:18:18.780
And then finally,
answer the question.

00:18:18.780 --> 00:18:20.620
That sounds like a
pretty vague statement,

00:18:20.620 --> 00:18:22.380
so here's what I mean.

00:18:22.380 --> 00:18:24.480
If you have
something like a map,

00:18:24.480 --> 00:18:27.630
or the user's asking for
directions or navigation,

00:18:27.630 --> 00:18:31.200
a map really is efficient
at communicating information

00:18:31.200 --> 00:18:34.320
in this really nice compact
rectangular shape, right?

00:18:34.320 --> 00:18:37.560
It conveys how far you are
from your destination, what

00:18:37.560 --> 00:18:41.760
the preferred route is, what
some alternate routes are,

00:18:41.760 --> 00:18:44.710
how-- what traffic looks
like, all that kind of stuff.

00:18:44.710 --> 00:18:46.570
So it may be
tempting to just punt

00:18:46.570 --> 00:18:49.570
to the screen on
interactions like this.

00:18:49.570 --> 00:18:52.230
So when Google Home
was first deployed

00:18:52.230 --> 00:18:53.850
and people were
asking for questions

00:18:53.850 --> 00:18:57.648
about directions and navigation,
this is how we handled it.

00:18:57.648 --> 00:18:58.314
[AUDIO PLAYBACK]

00:18:58.314 --> 00:19:01.352
- Sorry, I don't have a screen,
so I can't do that for you.

00:19:01.352 --> 00:19:01.935
[END PLAYBACK]

00:19:01.935 --> 00:19:04.240
JARED STRAWDERMAN: So pretty
disappointing and unhelpful,

00:19:04.240 --> 00:19:04.740
right?

00:19:04.740 --> 00:19:05.970
So we learned our lesson.

00:19:05.970 --> 00:19:08.610
We thought about why people
are asking Google Home

00:19:08.610 --> 00:19:13.020
these questions, and we kind
of distilled a few salient

00:19:13.020 --> 00:19:15.600
points about what people
are asking for, right?

00:19:15.600 --> 00:19:17.880
In the Bay Area, the
quintessential question

00:19:17.880 --> 00:19:20.280
is 280 or 101?

00:19:20.280 --> 00:19:21.510
What's traffic look like?

00:19:21.510 --> 00:19:23.384
How long is it going to
take me to get there?

00:19:23.384 --> 00:19:25.374
So we took this
approach instead.

00:19:25.374 --> 00:19:26.040
[AUDIO PLAYBACK]

00:19:26.040 --> 00:19:29.570
- The best way to get to work
by car is through 87 and 101

00:19:29.570 --> 00:19:32.162
North, and will take about
19 minutes in light traffic.

00:19:32.162 --> 00:19:32.745
[END PLAYBACK]

00:19:32.745 --> 00:19:36.060
JARED STRAWDERMAN: So even
though an image of a map

00:19:36.060 --> 00:19:40.290
is much more efficient and can
convey a lot more information

00:19:40.290 --> 00:19:42.540
to the user, there's
no reason why

00:19:42.540 --> 00:19:45.210
you can't distill the most
salient pieces of information

00:19:45.210 --> 00:19:47.910
into a neat little
verbal summary.

00:19:47.910 --> 00:19:51.060
Now let's talk
about smart phones.

00:19:51.060 --> 00:19:53.740
ADRIANA OLMOS: So mobile
phone is a wonderful machine

00:19:53.740 --> 00:19:57.200
that we're using
everywhere in our lives.

00:19:57.200 --> 00:20:01.520
And the thing is, there's no one
way in which we use our phones.

00:20:01.520 --> 00:20:04.060
The volume would
be up one minute,

00:20:04.060 --> 00:20:06.050
and next time it's
going to be down.

00:20:06.050 --> 00:20:08.660
Probably you will be
running to a session,

00:20:08.660 --> 00:20:12.880
or then you will be playing
with the headphones.

00:20:12.880 --> 00:20:16.820
Take, for instance,
this example.

00:20:16.820 --> 00:20:20.230
I used to live in Canada, and I
was up there at the ski slopes,

00:20:20.230 --> 00:20:24.160
and we knew that there was
going to be a snowstorm,

00:20:24.160 --> 00:20:26.800
and we needed to know what
time we needed to get down

00:20:26.800 --> 00:20:28.330
from the hills,
because otherwise we

00:20:28.330 --> 00:20:30.610
were going to get caught,
and it was at that time

00:20:30.610 --> 00:20:33.010
where we wished we could
just ask the phone,

00:20:33.010 --> 00:20:36.610
and say, hey, when is the
snowstorm going to start?

00:20:36.610 --> 00:20:39.670
And because the last
thing we wanted to do

00:20:39.670 --> 00:20:43.210
is take out our mittens when
we were up there, and then

00:20:43.210 --> 00:20:45.016
our phone flying down.

00:20:45.016 --> 00:20:46.390
So this is a time
where we wanted

00:20:46.390 --> 00:20:50.530
to-- we could wish that we could
use our phone as a little home

00:20:50.530 --> 00:20:53.140
that we're carrying
in out pocket.

00:20:53.140 --> 00:20:55.615
There were other times where
we were running and navigating

00:20:55.615 --> 00:20:58.090
through this in
traffic, and you wish

00:20:58.090 --> 00:21:00.820
you could speak to
your phone and quickly

00:21:00.820 --> 00:21:03.430
get an answer to your
question, but in that example,

00:21:03.430 --> 00:21:07.330
the last thing you want to do
is have all this information be

00:21:07.330 --> 00:21:11.200
blurted at you, and you just
want to glance at the screen.

00:21:11.200 --> 00:21:13.600
So the needs and the
capacities of these devices

00:21:13.600 --> 00:21:15.880
are really vast and very broad.

00:21:15.880 --> 00:21:18.550
We can be static
in one position,

00:21:18.550 --> 00:21:22.000
and then other times
we're running to places.

00:21:22.000 --> 00:21:24.820
We use them in a very
private or public context,

00:21:24.820 --> 00:21:27.250
and what is wonderful
about them is that we can

00:21:27.250 --> 00:21:31.060
have rich interactions to them.

00:21:31.060 --> 00:21:33.370
What is even more
compelling as well,

00:21:33.370 --> 00:21:37.180
is that there's a lot of
output and input capabilities.

00:21:37.180 --> 00:21:39.370
At the keynote we saw
this exciting demo

00:21:39.370 --> 00:21:45.690
where we can even use the
cameras as a form of input.

00:21:45.690 --> 00:21:48.330
So based on this, we came
up with three guidelines

00:21:48.330 --> 00:21:50.820
that we have been observing
while we're drafting

00:21:50.820 --> 00:21:53.220
our experiences at Google.

00:21:53.220 --> 00:21:56.950
If one word goes away, the
other one should take over.

00:21:56.950 --> 00:21:58.760
Let's see another example.

00:21:58.760 --> 00:22:03.360
Let's say I'm designing a
third party application called

00:22:03.360 --> 00:22:06.040
GeekNum, and what
GeekNum does is tells me

00:22:06.040 --> 00:22:07.660
facts about numbers.

00:22:07.660 --> 00:22:09.090
It's very simple.

00:22:09.090 --> 00:22:11.691
I pull out the phone, and
then GeekNum greets me,

00:22:11.691 --> 00:22:13.440
and there's a lot of
cues that it tells me

00:22:13.440 --> 00:22:16.830
that GeekNum is there and
enter the conversation.

00:22:16.830 --> 00:22:19.440
And also it gives me
suggestions of what I could say,

00:22:19.440 --> 00:22:23.250
or I can say my own number.

00:22:23.250 --> 00:22:25.830
But here's kind of looks like
if I was not really paying

00:22:25.830 --> 00:22:28.337
attention to the screen.

00:22:28.337 --> 00:22:29.003
[AUDIO PLAYBACK]

00:22:29.003 --> 00:22:31.010
- Howdy, this is GeekNum.

00:22:31.010 --> 00:22:33.650
I can tell you facts and
trivia about almost any number,

00:22:33.650 --> 00:22:34.419
like 42.

00:22:34.419 --> 00:22:35.960
What number would
like to know about?

00:22:35.960 --> 00:22:37.256
[END PLAYBACK]

00:22:37.256 --> 00:22:40.700
ADRIANA OLMOS: So this is what
you heard in their response.

00:22:40.700 --> 00:22:42.120
But in fact, if
you pay attention,

00:22:42.120 --> 00:22:43.890
it's a little bit
different than what

00:22:43.890 --> 00:22:45.330
you see in the chat bubble.

00:22:45.330 --> 00:22:47.360
We introduce little
things like this

00:22:47.360 --> 00:22:49.501
is GeekNum as a way
to introduce himself

00:22:49.501 --> 00:22:51.000
when entering into
the conversation,

00:22:51.000 --> 00:22:53.330
because we didn't have
enough visual cues in order

00:22:53.330 --> 00:22:54.920
to prompt that.

00:22:54.920 --> 00:22:58.890
We also add little suggestions
as part of the dialogue

00:22:58.890 --> 00:23:02.280
to prompt the user of what
is what they could say,

00:23:02.280 --> 00:23:04.220
and give them
ideas, but we didn't

00:23:04.220 --> 00:23:06.050
include that in the
chat bubble in order

00:23:06.050 --> 00:23:09.360
not to make it super cumbersome
and visually complex,

00:23:09.360 --> 00:23:11.570
because you already have
the suggestion there

00:23:11.570 --> 00:23:14.874
below the chat bubble.

00:23:14.874 --> 00:23:18.340
So it's very interesting that
we optimize for the strongest

00:23:18.340 --> 00:23:20.540
mode, but allow for both.

00:23:20.540 --> 00:23:22.760
So let's see another example.

00:23:22.760 --> 00:23:25.170
It's basketball
time, and let's think

00:23:25.170 --> 00:23:28.160
that Samuel wanted to go at
the beginning of this month

00:23:28.160 --> 00:23:33.450
to see the Warriors, and
he can ask Ticketmaster

00:23:33.450 --> 00:23:38.440
through the Google
Assistant for the next place

00:23:38.440 --> 00:23:41.410
that he could go
and buy tickets for.

00:23:41.410 --> 00:23:44.520
And it's very simple and
normal to see a list.

00:23:44.520 --> 00:23:46.660
You see that in every
web site, and that's

00:23:46.660 --> 00:23:49.150
the way in which we can
consume this type of content,

00:23:49.150 --> 00:23:52.659
quickly because in a list
we can quickly scan things.

00:23:52.659 --> 00:23:54.200
Then the question
is like, how are we

00:23:54.200 --> 00:23:58.110
going to be presenting this
through auditory content?

00:24:00.579 --> 00:24:01.245
[AUDIO PLAYBACK]

00:24:01.245 --> 00:24:04.680
- OK, the Golden State Warriors
have a few games coming up.

00:24:04.680 --> 00:24:07.324
The next one is against
the Jazz on May 2,

00:24:07.324 --> 00:24:08.740
Which one do you
want tickets for?

00:24:08.740 --> 00:24:09.976
[END PLAYBACK]

00:24:09.976 --> 00:24:14.250
ADRIANA OLMOS: So if you
look at their response,

00:24:14.250 --> 00:24:17.680
we say small things
like OK, so in order

00:24:17.680 --> 00:24:20.100
to acknowledge that we're
having this conversation

00:24:20.100 --> 00:24:22.410
and making it sound
more conversational.

00:24:22.410 --> 00:24:28.610
Also, we included
the first play,

00:24:28.610 --> 00:24:32.100
that is the next one
with the Jazz team

00:24:32.100 --> 00:24:34.580
that is coming on May
2, and this is in order

00:24:34.580 --> 00:24:37.420
to give a bit of
information to the user

00:24:37.420 --> 00:24:39.520
also through auditory content.

00:24:39.520 --> 00:24:42.210
Notice as well, we didn't
list every single game,

00:24:42.210 --> 00:24:43.950
because it would
be overwhelming,

00:24:43.950 --> 00:24:47.150
and we didn't include
that in the chat bubble.

00:24:47.150 --> 00:24:51.540
And what is very interesting
is that this type of approach

00:24:51.540 --> 00:24:54.150
lets you consume the content
when you have the two

00:24:54.150 --> 00:24:56.910
types of modalities,
but also it's

00:24:56.910 --> 00:24:59.760
a [INAUDIBLE]
fallback when the one

00:24:59.760 --> 00:25:02.065
is the opposite of the other.

00:25:02.065 --> 00:25:03.690
So it's very important
that we leverage

00:25:03.690 --> 00:25:06.330
the strengths to be small
and avoid redundancy

00:25:06.330 --> 00:25:07.890
across all these modalities.

00:25:12.570 --> 00:25:15.480
JARED STRAWDERMAN: So we've
talked about the surfaces

00:25:15.480 --> 00:25:18.360
that Google Assistant already
is deployed on, right?

00:25:18.360 --> 00:25:21.390
Google Home and Android phones.

00:25:21.390 --> 00:25:23.520
But we wanted to talk a
little bit about where

00:25:23.520 --> 00:25:26.220
it's going to show up
in just a little bit,

00:25:26.220 --> 00:25:30.990
and I want to start by a surface
that was prominently mentioned

00:25:30.990 --> 00:25:34.580
in yesterday's keynote, the TV.

00:25:34.580 --> 00:25:37.910
So if we quickly look at
some of the conditions

00:25:37.910 --> 00:25:41.030
around how the TV
studio is deployed,

00:25:41.030 --> 00:25:42.060
it is a static device.

00:25:42.060 --> 00:25:45.750
It's literally anchored to
your wall, and it's not moving,

00:25:45.750 --> 00:25:47.900
and even though
the user may move,

00:25:47.900 --> 00:25:49.910
it's not going to move with it.

00:25:49.910 --> 00:25:52.130
It is designed to
be used in kind

00:25:52.130 --> 00:25:54.860
of a private setting
among a group of users,

00:25:54.860 --> 00:25:57.590
much like Google Home,
and it's too far away

00:25:57.590 --> 00:25:59.990
to touch to interact with it.

00:25:59.990 --> 00:26:03.020
Now this is the
interesting thing about TV.

00:26:03.020 --> 00:26:05.180
Look at where this is
on the output scale

00:26:05.180 --> 00:26:06.800
compared to the input scale.

00:26:06.800 --> 00:26:11.450
Very, very rich audio and visual
output capabilities, but quite

00:26:11.450 --> 00:26:14.450
moderate or limited
input capabilities.

00:26:14.450 --> 00:26:15.930
What does that tell us?

00:26:15.930 --> 00:26:19.910
That tells us that TV is mainly
a consumption device and not

00:26:19.910 --> 00:26:23.390
really an intense
interaction device.

00:26:23.390 --> 00:26:27.230
And I think that the reason
that we're bringing this up

00:26:27.230 --> 00:26:30.290
is as you anticipate
your actions coming

00:26:30.290 --> 00:26:33.830
to life through Google
Assistant on TV, just

00:26:33.830 --> 00:26:38.630
know that we have to minimize
it onto a banner on about

00:26:38.630 --> 00:26:42.290
the bottom third of the
screen to not disrupt

00:26:42.290 --> 00:26:46.520
active programing going on while
the TV is being played, right?

00:26:46.520 --> 00:26:49.490
So we don't-- because it's
primarily a consumption device,

00:26:49.490 --> 00:26:53.390
people are watching programs,
we want to leverage just that

00:26:53.390 --> 00:26:55.050
bottom third real
estate for that.

00:26:55.050 --> 00:26:58.280
So know that when your
experiences come to life on TV,

00:26:58.280 --> 00:27:01.490
that's how they will manifest.

00:27:01.490 --> 00:27:04.250
And because we have a few car
people in the audience today,

00:27:04.250 --> 00:27:07.710
I wanted to touch a little bit
on cars and Google Assistant

00:27:07.710 --> 00:27:09.020
on cars.

00:27:09.020 --> 00:27:11.330
Android Auto is an
existing solution

00:27:11.330 --> 00:27:16.640
which does a beautiful job
of moving audio-rich content

00:27:16.640 --> 00:27:18.800
by way of projection
from your phone

00:27:18.800 --> 00:27:20.840
onto your car's interface.

00:27:20.840 --> 00:27:23.720
But let's talk a little bit
about where the car shows up

00:27:23.720 --> 00:27:25.970
in our multimodal matrix.

00:27:25.970 --> 00:27:29.360
So it is not static, right?

00:27:29.360 --> 00:27:31.010
It is moving around.

00:27:31.010 --> 00:27:33.630
It's carrying you around
at 80 miles an hour,

00:27:33.630 --> 00:27:37.610
so drivers really shouldn't be
looking at the screen a lot.

00:27:37.610 --> 00:27:39.500
And even though cars
are out and about

00:27:39.500 --> 00:27:43.100
in the public setting, the
cabin, the interior of the car

00:27:43.100 --> 00:27:46.610
is ostensibly kind
of a private setting.

00:27:46.610 --> 00:27:51.110
And users generally have access
to quick-touch interactions

00:27:51.110 --> 00:27:54.170
on their car to adjust
their temperature,

00:27:54.170 --> 00:27:56.660
or change a station,
or something like that,

00:27:56.660 --> 00:28:00.950
but you don't want the driver
interacting with things a lot.

00:28:00.950 --> 00:28:03.490
They should be driving.

00:28:03.490 --> 00:28:05.770
In terms of
input-output capability,

00:28:05.770 --> 00:28:08.380
cars have these really
robust stereo systems,

00:28:08.380 --> 00:28:10.870
so there's nothing wrong
with just presenting

00:28:10.870 --> 00:28:14.410
a lot of spoken output, and
they have pretty decent mics,

00:28:14.410 --> 00:28:16.750
although sometimes recognition
is a challenge with all

00:28:16.750 --> 00:28:18.930
that ambient noise.

00:28:18.930 --> 00:28:20.690
But here's what's interesting.

00:28:20.690 --> 00:28:26.380
If we overlay Google
Home on top of the car,

00:28:26.380 --> 00:28:28.450
they're strikingly
similar in terms of where

00:28:28.450 --> 00:28:30.320
they fall on this matrix.

00:28:30.320 --> 00:28:31.910
So what does that tell us?

00:28:31.910 --> 00:28:35.770
That tells us that Google Home
is a very voice-forward, almost

00:28:35.770 --> 00:28:38.410
voice-only device,
and that tells us

00:28:38.410 --> 00:28:40.911
that the car probably should
be somewhere in that range

00:28:40.911 --> 00:28:41.410
as well.

00:28:44.130 --> 00:28:49.260
So a few take-aways as we
kind of summarize here.

00:28:49.260 --> 00:28:52.680
If you don't take anything
away from today's talk,

00:28:52.680 --> 00:28:54.330
please take this away.

00:28:54.330 --> 00:28:56.970
Know the strengths and
weaknesses of voice

00:28:56.970 --> 00:28:58.440
as opposed to visuals.

00:28:58.440 --> 00:29:00.870
Leverage what speech
does well, leverage

00:29:00.870 --> 00:29:02.640
what the screen
does really well,

00:29:02.640 --> 00:29:06.790
and avoid the liabilities
that we've talked about.

00:29:06.790 --> 00:29:09.820
Optimize for the strongest
mode, but allow both.

00:29:09.820 --> 00:29:12.100
If the screen does
something better,

00:29:12.100 --> 00:29:13.930
point the user to the
screen, but still let

00:29:13.930 --> 00:29:17.160
them do it in voice.

00:29:17.160 --> 00:29:19.590
And even though
these interactions

00:29:19.590 --> 00:29:24.060
involve both visual
and auditory modes,

00:29:24.060 --> 00:29:27.150
they're usually invoked
by a spoken input,

00:29:27.150 --> 00:29:31.470
so make sure that the first bit
of content that you're hearing

00:29:31.470 --> 00:29:35.170
is appropriate for
spoken language.

00:29:35.170 --> 00:29:37.240
And one of the
overarching principles

00:29:37.240 --> 00:29:38.920
of using Google
Assistant, and the thing

00:29:38.920 --> 00:29:40.930
that we strive
for at Google when

00:29:40.930 --> 00:29:44.440
it comes to bringing these
solutions to life for you is we

00:29:44.440 --> 00:29:47.110
want them to be efficient.

00:29:47.110 --> 00:29:50.110
And each turn in a
conversational dialogue

00:29:50.110 --> 00:29:53.770
should be really short and
sweet and easy for the user

00:29:53.770 --> 00:29:55.600
to absorb and
consume, and help them

00:29:55.600 --> 00:29:59.550
get through their interaction.

00:29:59.550 --> 00:30:01.500
ADRIANA OLMOS: So we
talked a lot today

00:30:01.500 --> 00:30:03.990
about how to
package our response

00:30:03.990 --> 00:30:10.560
and making sure that all these
two modalities cohesively make

00:30:10.560 --> 00:30:13.410
a compelling response, and
also that they're there

00:30:13.410 --> 00:30:17.070
to serve in case one is absent.

00:30:17.070 --> 00:30:19.800
And we also talked about
conversational design,

00:30:19.800 --> 00:30:21.930
and how important it is to
get inspired from having

00:30:21.930 --> 00:30:23.820
a conversation with
a human before we

00:30:23.820 --> 00:30:27.700
jump into diving into the
designing the application.

00:30:27.700 --> 00:30:29.350
But the work is not over.

00:30:29.350 --> 00:30:32.970
There's a lot at Google
we need to get moving,

00:30:32.970 --> 00:30:37.420
and one of them is that once
we have [INAUDIBLE] response,

00:30:37.420 --> 00:30:38.680
how does that--

00:30:38.680 --> 00:30:42.060
we are going to be presenting
this response automatically--

00:30:42.060 --> 00:30:45.180
that is taking into account the
context in which the user is

00:30:45.180 --> 00:30:48.810
at, whether they're
running, how we will present

00:30:48.810 --> 00:30:51.030
that information, or if
they're just passive,

00:30:51.030 --> 00:30:53.280
sitting on a couch.

00:30:53.280 --> 00:30:55.440
Also, how is it we
more and more can

00:30:55.440 --> 00:30:59.130
do this crossing between
one type of servicing

00:30:59.130 --> 00:31:02.070
to another in a manner
that is almost magical,

00:31:02.070 --> 00:31:05.210
and the user doesn't
have to ask for it.

00:31:05.210 --> 00:31:07.440
And even more
further, now they're

00:31:07.440 --> 00:31:10.050
launching our
multimodal interactions

00:31:10.050 --> 00:31:15.090
on the phone, how we can make
these interactions like lists

00:31:15.090 --> 00:31:18.150
and cars, and all these things
that people can interact

00:31:18.150 --> 00:31:24.180
with more dynamic, to allow
for more complex interactions

00:31:24.180 --> 00:31:27.970
in a way that is more
reaching and easier to use.

00:31:27.970 --> 00:31:30.240
So we're very
excited that you guys

00:31:30.240 --> 00:31:33.060
are joining the journey
and start building

00:31:33.060 --> 00:31:34.990
your applications with us.

00:31:34.990 --> 00:31:37.410
There's going to be a lot
of other talks happening

00:31:37.410 --> 00:31:38.490
today and tomorrow.

00:31:38.490 --> 00:31:40.950
You're most welcome
to join in case

00:31:40.950 --> 00:31:45.390
you're interested in the related
talk for the Google Assistant,

00:31:45.390 --> 00:31:49.870
and [INAUDIBLE] as well
that there is a challenge,

00:31:49.870 --> 00:31:51.870
and we can't wait to see
all the things that you

00:31:51.870 --> 00:31:53.740
will be submitting to it.

00:31:53.740 --> 00:31:55.516
Thank you very much.

00:31:55.516 --> 00:31:58.185
[MUSIC]

