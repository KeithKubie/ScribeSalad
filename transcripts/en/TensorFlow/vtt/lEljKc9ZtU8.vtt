WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.444
[MUSIC PLAYING]

00:00:04.855 --> 00:00:06.980
JOSH GORDAN: OK, so we have
a bunch of cool topics,

00:00:06.980 --> 00:00:08.930
and we'll definitely
hit the Getting Started

00:00:08.930 --> 00:00:11.450
resources in a
moment, but because we

00:00:11.450 --> 00:00:13.550
talk about mnist a lot,
and very basic problems

00:00:13.550 --> 00:00:15.740
in computer vision a lot,
I wanted to actually start

00:00:15.740 --> 00:00:18.635
the talk by sharing a couple
of my favorite recent examples,

00:00:18.635 --> 00:00:22.590
all of which have complete
code in TensorFlow 2.0.

00:00:22.590 --> 00:00:24.220
So even if you're
new to TensorFlow,

00:00:24.220 --> 00:00:26.060
and these concepts can
take a long time to learn--

00:00:26.060 --> 00:00:27.977
like to talk about neural
machine translation,

00:00:27.977 --> 00:00:29.400
you would need to take a class--

00:00:29.400 --> 00:00:30.620
you can still try the code.

00:00:30.620 --> 00:00:33.800
And most of them are
50 lines or less.

00:00:33.800 --> 00:00:35.248
And they run very quickly.

00:00:35.248 --> 00:00:36.290
So anyway, they're great.

00:00:36.290 --> 00:00:39.200
Anyway, TensorFlow 2.0
is currently in alpha.

00:00:39.200 --> 00:00:41.570
It is all about ease of
use, which is the number one

00:00:41.570 --> 00:00:42.680
thing I care about.

00:00:42.680 --> 00:00:44.730
And it's ease of
use at all levels.

00:00:44.730 --> 00:00:47.330
So both for novices that
are totally starting out,

00:00:47.330 --> 00:00:48.980
as well as PhD
students that want

00:00:48.980 --> 00:00:51.590
to be able to do their research
in a slightly easier way.

00:00:51.590 --> 00:00:53.400
I'm not going to spend too
much time on this slide.

00:00:53.400 --> 00:00:55.483
I just wanted to call out
the most important thing

00:00:55.483 --> 00:00:58.850
about TensorFlow is the user
and the contributor community.

00:00:58.850 --> 00:01:01.040
And as of now we've
had over 1,800 people

00:01:01.040 --> 00:01:03.260
contribute code
to the code base,

00:01:03.260 --> 00:01:05.720
which is huge if you think
about how many seats that would

00:01:05.720 --> 00:01:06.330
fill up here.

00:01:06.330 --> 00:01:09.050
So thank you very much to
everyone who has contributed,

00:01:09.050 --> 00:01:10.640
and the many more
who have done docs

00:01:10.640 --> 00:01:13.530
and have done teaching and
meetups and stuff like that.

00:01:13.530 --> 00:01:15.290
So it's super cool.

00:01:15.290 --> 00:01:18.338
Anyway, the alpha of TensorFlow
2 is available today.

00:01:18.338 --> 00:01:19.880
Also going to fly
through this slide.

00:01:19.880 --> 00:01:22.340
And what you should know is,
every single example linked

00:01:22.340 --> 00:01:24.980
from the following
slides will automatically

00:01:24.980 --> 00:01:28.460
install the correct version
of TensorFlow at the very top.

00:01:28.460 --> 00:01:31.100
And they'll all run in
Colaboratory, which is the best

00:01:31.100 --> 00:01:32.780
thing since the microwave.

00:01:32.780 --> 00:01:34.532
And if you run it
in Colab, there's

00:01:34.532 --> 00:01:36.240
nothing to install on
your local machine.

00:01:36.240 --> 00:01:38.657
So if you want to try out the
latest version of TensorFlow

00:01:38.657 --> 00:01:41.120
2.0, you literally
can follow these links

00:01:41.120 --> 00:01:43.280
and with a single click
you'll be in Colab

00:01:43.280 --> 00:01:44.670
and you're good to go.

00:01:44.670 --> 00:01:46.190
So before we get
into Hello World,

00:01:46.190 --> 00:01:49.340
I wanted to very quickly talk
about things like Deep Dream.

00:01:49.340 --> 00:01:52.868
So what deep learning is, is
really representation learning.

00:01:52.868 --> 00:01:54.410
And we don't have
as much time as I'd

00:01:54.410 --> 00:01:56.270
like to go into that
in great detail.

00:01:56.270 --> 00:01:59.060
But there's been a couple
pieces of really amazing work

00:01:59.060 --> 00:02:00.510
in the last few years.

00:02:00.510 --> 00:02:03.110
And on the left we're seeing
a psychedelic image generated

00:02:03.110 --> 00:02:04.448
by a program called Deep Dream.

00:02:04.448 --> 00:02:05.990
And what's really
interesting, I just

00:02:05.990 --> 00:02:07.760
wanted to say that
the goal of Deep Dream

00:02:07.760 --> 00:02:10.289
was not to generate
psychedelic images.

00:02:10.289 --> 00:02:13.580
It was to investigate how
convolutional neural networks

00:02:13.580 --> 00:02:16.230
are so effective at
classifying images.

00:02:16.230 --> 00:02:20.282
And it was discovered that every
neuron in every layer of a CNN

00:02:20.282 --> 00:02:22.490
learns to detect different
features when you train it

00:02:22.490 --> 00:02:24.450
on a large corpus
of training data.

00:02:24.450 --> 00:02:26.900
And you can use TensorFlow
to write a loss function

00:02:26.900 --> 00:02:30.530
to try and maximally excite
one of those neurons.

00:02:30.530 --> 00:02:34.110
So anyway, the TLDR is, the
fact that we can take an image

00:02:34.110 --> 00:02:35.960
and supersaturate
it with dogs is

00:02:35.960 --> 00:02:38.450
possible as an artifact
of training a CNN

00:02:38.450 --> 00:02:39.873
on a large database of images.

00:02:39.873 --> 00:02:42.290
Anyway, there's a notebook
there where you can try it out.

00:02:42.290 --> 00:02:43.820
It runs very quickly in Colab.

00:02:43.820 --> 00:02:46.470
And like less than
50 lines of code.

00:02:46.470 --> 00:02:47.570
It's surprisingly short.

00:02:47.570 --> 00:02:50.450
Once you know the idea, the
implementation in TensorFlow 2

00:02:50.450 --> 00:02:51.470
is super easy.

00:02:51.470 --> 00:02:52.940
On the right is
an example called

00:02:52.940 --> 00:02:55.230
Style Transfer, which
I'm going to fly through.

00:02:55.230 --> 00:02:57.500
But it comes from
exactly the same idea.

00:02:57.500 --> 00:02:59.750
It's given that we've
trained an image classifier

00:02:59.750 --> 00:03:01.340
on a very large amount of data.

00:03:01.340 --> 00:03:02.870
What else can we do with it?

00:03:02.870 --> 00:03:06.050
And it exploits very
similar ideas, Deep Dream.

00:03:06.050 --> 00:03:07.508
Another really,
really cool example

00:03:07.508 --> 00:03:09.508
that I wanted to share
with you is for something

00:03:09.508 --> 00:03:11.190
called neural
machine translation.

00:03:11.190 --> 00:03:12.620
And this is like
50 lines of code,

00:03:12.620 --> 00:03:14.270
give or take, that
out of the box

00:03:14.270 --> 00:03:16.940
will train you an English
to Spanish translator.

00:03:16.940 --> 00:03:18.892
And the only input you
need to provide-- this

00:03:18.892 --> 00:03:19.850
is totally open source.

00:03:19.850 --> 00:03:22.190
We're not hitting the
Google Translate API.

00:03:22.190 --> 00:03:24.530
The reason I like
this example, it's

00:03:24.530 --> 00:03:27.080
all the code you need to
implement a mini version

00:03:27.080 --> 00:03:27.920
of Google Translate.

00:03:27.920 --> 00:03:30.230
The simplest possible
Hello World version

00:03:30.230 --> 00:03:32.450
of Google Translate can
be done in 50 lines.

00:03:32.450 --> 00:03:35.180
And the one thing I wanted
to mention very briefly

00:03:35.180 --> 00:03:38.540
is I'm a little bit
envious of people that

00:03:38.540 --> 00:03:39.890
are bilingual and trilingual.

00:03:39.890 --> 00:03:42.510
I have English and barely
high school Spanish.

00:03:42.510 --> 00:03:45.280
And so translation has
always interested in me.

00:03:45.280 --> 00:03:48.320
And the way that the
translation example works,

00:03:48.320 --> 00:03:51.140
it uses something called a
sequence to sequence model.

00:03:51.140 --> 00:03:53.960
Briefly, what that does is it
takes a sentence in English

00:03:53.960 --> 00:03:56.210
or the source language,
and it maps it down

00:03:56.210 --> 00:03:58.670
to a vector, which you can
actually see and print out

00:03:58.670 --> 00:03:59.910
in the code and explore.

00:03:59.910 --> 00:04:02.690
And that vector is just
an array of numbers.

00:04:02.690 --> 00:04:04.530
That's called an encoder.

00:04:04.530 --> 00:04:06.140
There's a second
neural network--

00:04:06.140 --> 00:04:08.450
it's two natural networks
working parallel--

00:04:08.450 --> 00:04:10.940
there's a second network
called a decoder.

00:04:10.940 --> 00:04:13.700
The only input to the
decoder is that vector,

00:04:13.700 --> 00:04:15.950
and the decoder
produces a sentence

00:04:15.950 --> 00:04:17.670
in the target language.

00:04:17.670 --> 00:04:20.570
And what this means is that
the encoder takes a sentence

00:04:20.570 --> 00:04:22.130
and it compresses it.

00:04:22.130 --> 00:04:24.230
So before, we saw
that deep learning can

00:04:24.230 --> 00:04:26.030
be seen as
representation learning

00:04:26.030 --> 00:04:27.490
because as an
artifact of training

00:04:27.490 --> 00:04:30.320
a network on a large
corpus of images

00:04:30.320 --> 00:04:33.473
to train an image
classifier, we find neurons

00:04:33.473 --> 00:04:35.640
that learn to recognize
different shapes and objects

00:04:35.640 --> 00:04:36.232
and textures.

00:04:36.232 --> 00:04:37.940
And here we can think
about deep learning

00:04:37.940 --> 00:04:40.490
as compression, because we
can take a sentence which

00:04:40.490 --> 00:04:43.160
contains a huge amount of
information and map it down

00:04:43.160 --> 00:04:44.790
to a short list of numbers.

00:04:44.790 --> 00:04:47.568
And the reason I wanted to
mention this trilingual aspect

00:04:47.568 --> 00:04:49.610
is you can actually come
up with something called

00:04:49.610 --> 00:04:53.350
an interlingual representation,
just by modifying this example.

00:04:53.350 --> 00:04:56.480
Say we wanted to translate from
English to Spanish and also

00:04:56.480 --> 00:04:58.100
from French to Spanish.

00:04:58.100 --> 00:05:00.520
Just by modifying
this code to include

00:05:00.520 --> 00:05:04.010
a small number of French to
Spanish sentences in the input

00:05:04.010 --> 00:05:06.260
data that you
provide, that vector

00:05:06.260 --> 00:05:09.230
that comes out of the encoder
will encode a sentence

00:05:09.230 --> 00:05:12.770
either in English or in French
into the same representation.

00:05:12.770 --> 00:05:15.290
And what that means is,
you're finding a way

00:05:15.290 --> 00:05:17.747
to represent concepts
independently of language.

00:05:17.747 --> 00:05:19.580
And I know this is a
little bit out of scope

00:05:19.580 --> 00:05:22.670
for Hello TensorFlow World,
which we'll get to in a sec.

00:05:22.670 --> 00:05:24.890
But the reason I want to
mention things like this

00:05:24.890 --> 00:05:26.570
is there's incredible
opportunities

00:05:26.570 --> 00:05:29.250
at the intersection of deep
learning and other fields.

00:05:29.250 --> 00:05:31.562
So for example, if
you were a linguist,

00:05:31.562 --> 00:05:33.020
perhaps you're not
super interested

00:05:33.020 --> 00:05:35.210
in the details of the
TensorFlow implementation,

00:05:35.210 --> 00:05:37.370
but what you could do is
look at this interlingual

00:05:37.370 --> 00:05:39.890
representation that we
get almost for free,

00:05:39.890 --> 00:05:41.400
and investigate it.

00:05:41.400 --> 00:05:45.080
And right there, that would be
a killer PhD paper or thesis

00:05:45.080 --> 00:05:46.372
or something like that.

00:05:46.372 --> 00:05:48.080
And so when we put
these fields together,

00:05:48.080 --> 00:05:50.900
we get super, super cool things.

00:05:50.900 --> 00:05:52.080
I'm taking way too long.

00:05:52.080 --> 00:05:53.420
I'm going to skip this entirely.

00:05:53.420 --> 00:05:56.630
I just wanted to mention
we can also take an image

00:05:56.630 --> 00:05:59.300
and using the same encoder
decoder architecture,

00:05:59.300 --> 00:06:02.957
map the image into a vector
and reusing the decoder, which

00:06:02.957 --> 00:06:05.540
maps from vectors to sentences
that we have in the translation

00:06:05.540 --> 00:06:08.150
tutorial, almost
copying and pasting it,

00:06:08.150 --> 00:06:10.445
we can learn to
caption an image.

00:06:10.445 --> 00:06:12.320
And this tutorial has
a little bit more code,

00:06:12.320 --> 00:06:13.470
but it's the same idea.

00:06:13.470 --> 00:06:15.020
It's absolutely mind-blowing.

00:06:15.020 --> 00:06:17.840
So there's a lot of value in
learning about deep learning,

00:06:17.840 --> 00:06:20.810
and it has a lot of
potential impact.

00:06:20.810 --> 00:06:23.960
Also we have two really
excellent resources

00:06:23.960 --> 00:06:25.160
for total beginners.

00:06:25.160 --> 00:06:27.620
The first is linear regression,
finding the best fit

00:06:27.620 --> 00:06:29.870
line, which is probably
the most boring thing ever,

00:06:29.870 --> 00:06:33.410
but it's a perfect way to
learn about gradient descent

00:06:33.410 --> 00:06:35.500
and even back propagation
if you wanted to.

00:06:35.500 --> 00:06:37.250
And what's awesome is
the gradient descent

00:06:37.250 --> 00:06:39.333
and back propagation
concepts that you learn about

00:06:39.333 --> 00:06:41.750
in linear regression are
exactly the same concepts

00:06:41.750 --> 00:06:44.540
that apply to every other
model you see in TensorFlow.

00:06:44.540 --> 00:06:47.870
And then, as always, we have a
great collection of Hello World

00:06:47.870 --> 00:06:48.980
examples on the website.

00:06:48.980 --> 00:06:51.865
And we just launched a Udacity
course and a Coursera course

00:06:51.865 --> 00:06:53.240
that will go into
the Hello World

00:06:53.240 --> 00:06:56.960
examples in much more depth
than we have time for here.

00:06:56.960 --> 00:06:59.880
So Paige-- who I've learned
a lot from, by the way--

00:06:59.880 --> 00:07:02.540
is going to tell us about the
differences between TensorFlow

00:07:02.540 --> 00:07:03.988
1 and TensorFlow 2.

00:07:03.988 --> 00:07:05.030
PAIGE BAILEY: Absolutely.

00:07:05.030 --> 00:07:06.650
Thank you, Josh.

00:07:06.650 --> 00:07:07.568
So--

00:07:07.568 --> 00:07:11.780
[APPLAUSE]

00:07:11.780 --> 00:07:12.440
Excellent.

00:07:12.440 --> 00:07:18.290
So who here tried to use
TensorFlow around 2015, 2016?

00:07:18.290 --> 00:07:20.160
Show of hands.

00:07:20.160 --> 00:07:21.850
So a few of you.

00:07:21.850 --> 00:07:23.780
And if you tried using
it then, you probably

00:07:23.780 --> 00:07:26.690
saw something very similar
to what you see on the screen

00:07:26.690 --> 00:07:28.100
there.

00:07:28.100 --> 00:07:30.380
And if you're coming from
the scikit learn world,

00:07:30.380 --> 00:07:33.260
this doesn't feel very
Pythonic at all, right?

00:07:33.260 --> 00:07:35.180
So you're defining
some variables.

00:07:35.180 --> 00:07:37.280
You're assigning
some values to them,

00:07:37.280 --> 00:07:39.380
but you can't
immediately use them.

00:07:39.380 --> 00:07:41.330
You have to initialize
those variables.

00:07:41.330 --> 00:07:44.530
You have to start these weird
things called queue runners,

00:07:44.530 --> 00:07:48.290
and you have to do it all
from this sess.run statement.

00:07:48.290 --> 00:07:51.450
So creating sessions,
defining variables,

00:07:51.450 --> 00:07:53.930
but not being able to
use them immediately,

00:07:53.930 --> 00:07:56.580
this wasn't straightforward.

00:07:56.580 --> 00:07:58.640
And the user
experience, we found,

00:07:58.640 --> 00:08:00.500
with the original
version of TensorFlow

00:08:00.500 --> 00:08:03.300
left a lot to be desired.

00:08:03.300 --> 00:08:06.980
So we've learned a lot
since TensorFlow 2.0,

00:08:06.980 --> 00:08:09.830
or since TensorFlow
1.0, I should say.

00:08:09.830 --> 00:08:13.620
We've learned that there's a lot
to be gained through usability.

00:08:13.620 --> 00:08:17.660
So we've adopted Keras as
the default higher-level API.

00:08:17.660 --> 00:08:21.240
We've decided to pursue
eager execution by default.

00:08:21.240 --> 00:08:23.660
And this means that you
can do crazy things,

00:08:23.660 --> 00:08:28.520
like add two numbers together
and immediately get a response.

00:08:31.080 --> 00:08:34.309
We've also made a big
push towards clarity

00:08:34.309 --> 00:08:36.500
by removing duplicate
functionality,

00:08:36.500 --> 00:08:41.240
by making consistent intuitive
syntax across all of our APIs.

00:08:41.240 --> 00:08:44.270
So instead of having thousands
of endpoints, some of which

00:08:44.270 --> 00:08:46.550
do very similar things,
and none of which

00:08:46.550 --> 00:08:49.310
have standardized
conventions, now everything

00:08:49.310 --> 00:08:51.710
feels a little bit
more consistent.

00:08:51.710 --> 00:08:55.940
And instead of having a
large series of tf.foo,

00:08:55.940 --> 00:09:01.220
we now have things like
tf.signal.foo or tf.math.foo.

00:09:01.220 --> 00:09:04.220
We've also decided to
improve the compatibility

00:09:04.220 --> 00:09:07.140
throughout the entire
TensorFlow ecosystem.

00:09:07.140 --> 00:09:09.830
So instead of having
multiple ways to save models,

00:09:09.830 --> 00:09:12.590
we've standardized on
something called saved model.

00:09:12.590 --> 00:09:14.690
And I think we'll see an
architecture slide later

00:09:14.690 --> 00:09:18.110
in the presentation
speaking to that.

00:09:18.110 --> 00:09:21.560
We've also made a huge
push towards flexibility.

00:09:21.560 --> 00:09:23.840
We have a full
lower-level API, so if you

00:09:23.840 --> 00:09:27.260
like using the lower
level ops you're

00:09:27.260 --> 00:09:30.890
still having that capability
we've made everything

00:09:30.890 --> 00:09:33.590
accessible in tf.rawops.

00:09:33.590 --> 00:09:36.850
And we've had inheritable
interfaces added for variables,

00:09:36.850 --> 00:09:39.200
check points, layers, and more.

00:09:39.200 --> 00:09:42.388
So if you like staying at a
high level, we support that.

00:09:42.388 --> 00:09:43.930
If you want to go
a little bit lower,

00:09:43.930 --> 00:09:45.890
you can do subclassing
with Keras,

00:09:45.890 --> 00:09:48.110
and if you want
to go even lower,

00:09:48.110 --> 00:09:50.210
you have access to
the full capabilities

00:09:50.210 --> 00:09:51.650
with TensorFlow Raw Ops.

00:09:54.680 --> 00:09:57.140
This is all with one API.

00:09:57.140 --> 00:09:59.840
And I think to talk a
little bit more about it,

00:09:59.840 --> 00:10:03.200
Josh is going to mention
how much we love Keras,

00:10:03.200 --> 00:10:04.783
and love its subclassing
capabilities.

00:10:04.783 --> 00:10:06.367
JOSH GORDAN: Which
is completely true.

00:10:06.367 --> 00:10:07.190
Thank you so much.

00:10:07.190 --> 00:10:10.430
OK, so I know I'm talking fast,
but we have a lot of content

00:10:10.430 --> 00:10:11.540
to cover.

00:10:11.540 --> 00:10:15.020
Basically one of the huge
changes of TensorFlow-- well,

00:10:15.020 --> 00:10:17.030
we did this technically
in TensorFlow 1.x,

00:10:17.030 --> 00:10:19.080
but this is the
standard for 2.0.

00:10:19.080 --> 00:10:21.110
So we've standardized
on the Keras API,

00:10:21.110 --> 00:10:22.700
and we've extended it.

00:10:22.700 --> 00:10:24.830
Briefly, because we might
not get to the slide,

00:10:24.830 --> 00:10:28.940
if you go to keras.io, that is
the reference implementation

00:10:28.940 --> 00:10:31.990
for an open source
deep learning API spec.

00:10:31.990 --> 00:10:34.890
Keras is basically an API
without an implementation.

00:10:34.890 --> 00:10:36.500
It's a set of layers
that describes

00:10:36.500 --> 00:10:39.530
a very clear way to implement
your neural networks.

00:10:39.530 --> 00:10:42.560
But traditionally, Keras runs
on top of other frameworks.

00:10:42.560 --> 00:10:45.890
So if you do PIP install Keras,
you get Keras with TensorFlow

00:10:45.890 --> 00:10:48.110
behind the scenes, and
you never see TensorFlow.

00:10:48.110 --> 00:10:49.880
And this is a perfectly
fine way to get

00:10:49.880 --> 00:10:51.480
started with machine learning.

00:10:51.480 --> 00:10:53.310
In fact, you can
do like 90% of what

00:10:53.310 --> 00:10:54.560
you need to do just with that.

00:10:54.560 --> 00:10:56.100
It's phenomenally good.

00:10:56.100 --> 00:10:58.670
In TensorFlow, if you do
PIP install TensorFlow,

00:10:58.670 --> 00:11:01.292
you get the complete Keras
API and some additional stuff

00:11:01.292 --> 00:11:02.000
that we've added.

00:11:02.000 --> 00:11:03.950
There's no need to
install Keras separately.

00:11:03.950 --> 00:11:06.200
Briefly, I just want
to show you two APIs.

00:11:06.200 --> 00:11:08.120
And it says for
beginners and experts,

00:11:08.120 --> 00:11:10.275
but you can do 95%
of ML, including

00:11:10.275 --> 00:11:11.900
some of the cool
examples I showed you,

00:11:11.900 --> 00:11:13.410
with the beginner's API.

00:11:13.410 --> 00:11:15.500
The beginner's API, this
is called Sequential.

00:11:15.500 --> 00:11:17.875
And we're defining a neural
network as a stack of layers.

00:11:17.875 --> 00:11:19.292
And I know people
that I've worked

00:11:19.292 --> 00:11:21.860
with deep learning before have
almost certainly seen this.

00:11:21.860 --> 00:11:24.200
There's a couple
important points.

00:11:24.200 --> 00:11:25.635
You might not
realize it, but what

00:11:25.635 --> 00:11:28.010
you're doing here when you're
defining a sequential model

00:11:28.010 --> 00:11:30.200
is you're defining
a data structure.

00:11:30.200 --> 00:11:32.490
Because your model
is a stack of layers.

00:11:32.490 --> 00:11:34.460
Keras or TensorFlow,
depending on which way

00:11:34.460 --> 00:11:36.380
you're running this,
can look at your layers

00:11:36.380 --> 00:11:37.970
and make sure
they're compatible.

00:11:37.970 --> 00:11:39.950
So it can help you debug.

00:11:39.950 --> 00:11:42.680
And what this means is if
you define a model this way,

00:11:42.680 --> 00:11:46.130
you're not likely to have errors
in the model definition itself.

00:11:46.130 --> 00:11:48.080
Your errors are going
to be conceptual errors.

00:11:48.080 --> 00:11:49.520
They're not going to
be programming errors

00:11:49.520 --> 00:11:50.645
when you define your model.

00:11:50.645 --> 00:11:52.050
And that's very valuable.

00:11:52.050 --> 00:11:54.080
Here's how that looked
in TensorFlow 1,

00:11:54.080 --> 00:11:56.232
and here's how that
looked in TensorFlow 2.

00:11:56.232 --> 00:11:57.440
Or here's how that looks now.

00:11:57.440 --> 00:11:59.160
So this hasn't changed at all.

00:11:59.160 --> 00:12:01.610
And if you're familiar
with it, great.

00:12:01.610 --> 00:12:03.590
We've added a second
style, and this

00:12:03.590 --> 00:12:05.090
is called model subclassing.

00:12:05.090 --> 00:12:07.980
And I love this, but
it's very different.

00:12:07.980 --> 00:12:10.790
So this basically feels
like object oriented

00:12:10.790 --> 00:12:12.230
NumPy development.

00:12:12.230 --> 00:12:14.690
So many libraries do
something similar to this.

00:12:14.690 --> 00:12:16.910
The idea came from
Chainer a few years back.

00:12:16.910 --> 00:12:19.580
And what we're doing is we're
extending a class provided

00:12:19.580 --> 00:12:20.450
by the library.

00:12:20.450 --> 00:12:22.105
Here we call it model.

00:12:22.105 --> 00:12:24.230
In the constructor-- so if
you're coming from Java,

00:12:24.230 --> 00:12:25.855
this will be great--
in the constructor

00:12:25.855 --> 00:12:27.660
we define our layers.

00:12:27.660 --> 00:12:31.470
And in the call method we define
the forward pass of our model.

00:12:31.470 --> 00:12:34.160
And what's nice is the call
method is, in TensorFlow 2,

00:12:34.160 --> 00:12:36.080
this is just regular
imperative Python,

00:12:36.080 --> 00:12:37.680
exactly how you would
always write it,

00:12:37.680 --> 00:12:39.097
and it works the
way you'd expect.

00:12:39.097 --> 00:12:40.940
This makes it super
easy to debug,

00:12:40.940 --> 00:12:44.390
and my friend Sarah, she works
on custom activation functions.

00:12:44.390 --> 00:12:46.700
If Sarah wants to quickly
try her activation function,

00:12:46.700 --> 00:12:48.450
you can write it as
you would expect.

00:12:48.450 --> 00:12:50.450
And this is a huge
difference from TensorFlow 1.

00:12:50.450 --> 00:12:51.677
This is phenomenal.

00:12:54.712 --> 00:12:56.170
For people reading
at home, you can

00:12:56.170 --> 00:12:57.753
look at the slide
and the article link

00:12:57.753 --> 00:13:00.080
from that to learn a
lot more about that.

00:13:00.080 --> 00:13:02.920
Anyway, both types of models,
if you're familiar with Keras,

00:13:02.920 --> 00:13:06.010
can be trained using
model.fit, as you always would.

00:13:06.010 --> 00:13:08.290
Or if you would
like, you can use

00:13:08.290 --> 00:13:10.120
what we call the gradient tape.

00:13:10.120 --> 00:13:13.960
And so this is a perfect
way, if you're doing research

00:13:13.960 --> 00:13:15.670
or if you're a
student and you want

00:13:15.670 --> 00:13:17.680
to learn about back prop
and gradient descent,

00:13:17.680 --> 00:13:20.020
if you'd like to know what the
gradients of your loss function

00:13:20.020 --> 00:13:22.745
are with respect to the weights,
you can simply print them out.

00:13:22.745 --> 00:13:24.120
If you print out
grads there, you

00:13:24.120 --> 00:13:26.830
will just get a list of showing
all the gradients, which

00:13:26.830 --> 00:13:30.130
makes them extremely easy
to modify and log and debug.

00:13:30.130 --> 00:13:30.940
It's great.

00:13:30.940 --> 00:13:34.700
So this style of code gives
you complete flexibility,

00:13:34.700 --> 00:13:35.900
which is awesome.

00:13:35.900 --> 00:13:38.530
But you're much more likely
to have programming errors.

00:13:38.530 --> 00:13:39.963
So basically, if
you do model.fit,

00:13:39.963 --> 00:13:42.130
it works out of the box,
it's fast, It's performing,

00:13:42.130 --> 00:13:42.970
you don't have to
think about it.

00:13:42.970 --> 00:13:44.800
You can focus on the
problems that you care about.

00:13:44.800 --> 00:13:46.240
If you wanted to research
and write from scratch,

00:13:46.240 --> 00:13:47.890
you can, but there's a cost.

00:13:47.890 --> 00:13:49.570
The other cost that
I wanted to mention

00:13:49.570 --> 00:13:52.238
is actually tech debt,
which is not something

00:13:52.238 --> 00:13:53.530
you might think of off the bat.

00:13:53.530 --> 00:13:56.770
So deep learning aside, if
you implement your model

00:13:56.770 --> 00:13:58.750
using the sequential
API, I can look

00:13:58.750 --> 00:14:00.560
at any code written that way.

00:14:00.560 --> 00:14:03.400
For instance, if I'm helping a
student debug, and immediately

00:14:03.400 --> 00:14:05.230
see what the bug
is because there's

00:14:05.230 --> 00:14:07.420
a standard, conventional
way to write it.

00:14:07.420 --> 00:14:09.370
If I have students that
write code this way

00:14:09.370 --> 00:14:10.828
and they come to
me with a problem,

00:14:10.828 --> 00:14:13.132
it can take me 15
minutes to find it.

00:14:13.132 --> 00:14:14.590
And if you think
about what happens

00:14:14.590 --> 00:14:16.150
to code that you
write in a company,

00:14:16.150 --> 00:14:17.608
if you have deep
learning code that

00:14:17.608 --> 00:14:19.810
lives for five years
worked on by 50 engineers,

00:14:19.810 --> 00:14:21.670
there's a huge
cost to this style.

00:14:21.670 --> 00:14:24.730
And so I know this is obvious,
but basically software

00:14:24.730 --> 00:14:26.980
engineering best practices
apply to deep learning too.

00:14:26.980 --> 00:14:29.590
So we have the style, but
use it when you need it.

00:14:29.590 --> 00:14:33.100
More details on Keras
versus TensorFlow.

00:14:33.100 --> 00:14:34.910
Briefly, another
really awesome thing

00:14:34.910 --> 00:14:38.110
about Keras API in TensorFlow
is distributed training.

00:14:38.110 --> 00:14:40.940
So most of the work that I care
about happens on one machine.

00:14:40.940 --> 00:14:42.490
Like, what I'm
personally excited by

00:14:42.490 --> 00:14:46.212
is like a really clear
implementation of GaN.

00:14:46.212 --> 00:14:47.920
A friend of mine is
working on cycle GaN.

00:14:47.920 --> 00:14:49.850
We'll have a tutorial
available soon.

00:14:49.850 --> 00:14:51.850
But for people training
large-scale models

00:14:51.850 --> 00:14:54.130
in production, we've greatly
simplified distributed

00:14:54.130 --> 00:14:55.600
training in TensorFlow too.

00:14:55.600 --> 00:14:59.350
So here's a Keras model, and
these are the lines of code

00:14:59.350 --> 00:15:03.070
that you need to run that model
using data parallelism on one

00:15:03.070 --> 00:15:04.780
machine with many GPUs.

00:15:04.780 --> 00:15:05.800
And that's it.

00:15:05.800 --> 00:15:08.410
So assuming you have a
performing input pipeline,

00:15:08.410 --> 00:15:10.485
which, to be honest,
takes time to write

00:15:10.485 --> 00:15:11.860
and is an engineering
discipline,

00:15:11.860 --> 00:15:14.260
but once you have that done,
distributing your model

00:15:14.260 --> 00:15:14.998
is very easy.

00:15:14.998 --> 00:15:17.290
And we're working on additional
distribution strategies

00:15:17.290 --> 00:15:19.060
for different machine and
network configurations

00:15:19.060 --> 00:15:20.602
just to encapsulate
this logic so you

00:15:20.602 --> 00:15:22.930
can train your models quickly
and focus on the problems

00:15:22.930 --> 00:15:23.973
that you care about.

00:15:23.973 --> 00:15:26.140
Another thing that's really
special about TensorFlow

00:15:26.140 --> 00:15:28.280
is I want to call out
the documentation.

00:15:28.280 --> 00:15:30.070
So if you visit
tensorflow.org, this

00:15:30.070 --> 00:15:32.625
is just a screenshot of
one of our tutorials.

00:15:32.625 --> 00:15:34.000
You'll see most
of the editorials

00:15:34.000 --> 00:15:35.250
have these buttons at the top.

00:15:35.250 --> 00:15:37.500
One is view on GitHub, which
will give you the Jupyter

00:15:37.500 --> 00:15:38.020
notebook.

00:15:38.020 --> 00:15:39.670
The other is run in Colab.

00:15:39.670 --> 00:15:44.590
And all of the tutorials for the
alpha version of TensorFlow 2

00:15:44.590 --> 00:15:47.753
run end to end out of the
box with no code changes.

00:15:47.753 --> 00:15:50.170
And this is important because
it means they're easy to use

00:15:50.170 --> 00:15:51.030
and they're reproducible.

00:15:51.030 --> 00:15:53.072
So what they do is they
install the right version

00:15:53.072 --> 00:15:53.710
of TensorFlow.

00:15:53.710 --> 00:15:55.850
They download any
data sets you need.

00:15:55.850 --> 00:15:58.645
So in this GaN example,
they'll download--

00:15:58.645 --> 00:16:00.520
I actually forget the
name of the university.

00:16:00.520 --> 00:16:02.145
The paper, I believe,
is from Berkeley,

00:16:02.145 --> 00:16:04.810
but I'm not sure that's
where the data set is hosted.

00:16:04.810 --> 00:16:06.820
Anyway, we thank
them in the tutorial.

00:16:06.820 --> 00:16:08.170
They'll download the data set.

00:16:08.170 --> 00:16:09.280
They'll train a model.

00:16:09.280 --> 00:16:11.008
They'll display the
results that you see,

00:16:11.008 --> 00:16:13.300
and from there you have a
great starting point that you

00:16:13.300 --> 00:16:15.630
can modify and hack on.

00:16:15.630 --> 00:16:16.200
OK.

00:16:16.200 --> 00:16:19.900
And now Paige will tell you
more about TensorFlow 2.

00:16:19.900 --> 00:16:21.490
PAIGE BAILEY: Excellent.

00:16:21.490 --> 00:16:22.270
Thanks, Josh.

00:16:22.270 --> 00:16:24.010
So as we mentioned
before, we've tried

00:16:24.010 --> 00:16:26.920
to standardize and to
provide compatibility

00:16:26.920 --> 00:16:30.077
throughout the entire
TensorFlow ecosystem.

00:16:30.077 --> 00:16:31.660
If you were here a
little bit earlier,

00:16:31.660 --> 00:16:34.960
you saw a really cool demo
from the TensorFlow Lite team,

00:16:34.960 --> 00:16:38.260
where you had
object segmentation

00:16:38.260 --> 00:16:41.110
and you were able to
have a human being dance

00:16:41.110 --> 00:16:45.160
and immediately have their
body shape sort of transposed

00:16:45.160 --> 00:16:48.010
and make it look
like I could dance,

00:16:48.010 --> 00:16:53.510
even though that's usually
not so much the case.

00:16:53.510 --> 00:16:54.910
The way that this
is standardized

00:16:54.910 --> 00:16:56.830
is through something
called saved model.

00:16:56.830 --> 00:16:59.310
So with historic TensorFlow,
so TensorFlow 1.x,

00:16:59.310 --> 00:17:03.130
there were a variety of
ways to save your models.

00:17:03.130 --> 00:17:05.230
And it made it very,
very difficult to port it

00:17:05.230 --> 00:17:07.819
to different locations where
you might want to use them.

00:17:07.819 --> 00:17:11.589
So for example, mobile or
embedded devices or servers.

00:17:11.589 --> 00:17:15.790
Now, as part of the
standardization with TensorFlow

00:17:15.790 --> 00:17:18.010
2.0, you can take
your saved model

00:17:18.010 --> 00:17:20.530
and deploy it to TensorFlow
Serving, TensorFlow

00:17:20.530 --> 00:17:25.300
Lite for mobile and [INAUDIBLE]
embedded devices, TensorFlow JS

00:17:25.300 --> 00:17:27.910
for deep learning in the
browser or on a server,

00:17:27.910 --> 00:17:29.950
and then also for other
language bindings.

00:17:29.950 --> 00:17:36.120
We offer support for Rust, for
Java, for Scala, and many more.

00:17:36.120 --> 00:17:39.190
So as I mentioned, you can
use TensorFlow on servers,

00:17:39.190 --> 00:17:42.040
for edge devices and browsers.

00:17:42.040 --> 00:17:45.080
We have an entire training
workflow pipeline,

00:17:45.080 --> 00:17:48.160
including data ingestion and
transformation with TF data

00:17:48.160 --> 00:17:49.600
and feature columns.

00:17:49.600 --> 00:17:52.550
From model building with
Keras, premade estimators,

00:17:52.550 --> 00:17:55.550
if you would still like to
use those, and then also

00:17:55.550 --> 00:17:59.330
custom estimators or
custom Keras models.

00:17:59.330 --> 00:18:02.150
For training we've defaulted
with eager execution,

00:18:02.150 --> 00:18:04.790
so you'll be able to get
your responses immediately

00:18:04.790 --> 00:18:06.980
instead of doing that
frustrating thing

00:18:06.980 --> 00:18:09.380
with initializing variables
and starting queue runners,

00:18:09.380 --> 00:18:10.950
as I mentioned before.

00:18:10.950 --> 00:18:13.130
You can visualize all
of it with TensorBoard

00:18:13.130 --> 00:18:17.570
and then export again,
as with the saved model.

00:18:17.570 --> 00:18:19.770
If you haven't already seen
it-- and this is really,

00:18:19.770 --> 00:18:20.570
really cool--

00:18:20.570 --> 00:18:24.600
TensorBoard is now offered
fully supported in Google Colab

00:18:24.600 --> 00:18:26.660
so you're able to
start it to run

00:18:26.660 --> 00:18:31.020
it to inspect and to
visualize your models,

00:18:31.020 --> 00:18:34.220
all without those sort
of frustrating local host

00:18:34.220 --> 00:18:35.210
commands.

00:18:35.210 --> 00:18:38.030
This support is also additive
for Jupyter notebooks,

00:18:38.030 --> 00:18:40.760
so if you want to use this
with Google Cloud instances

00:18:40.760 --> 00:18:45.420
or with Jupyter notebooks
locally, you absolutely can.

00:18:45.420 --> 00:18:48.110
We've also included built-in
performance profiling

00:18:48.110 --> 00:18:48.770
for Colab.

00:18:48.770 --> 00:18:52.070
This is for GPUs
and also for TPUs.

00:18:52.070 --> 00:18:55.070
So you're able to understand
how your models are interacting

00:18:55.070 --> 00:18:57.710
with hardware and then
also ways that you

00:18:57.710 --> 00:18:59.300
can improve your performance.

00:19:02.060 --> 00:19:04.610
We heard you loud and clear
that the documentation could

00:19:04.610 --> 00:19:07.910
be improved, so a
big push for 2.0

00:19:07.910 --> 00:19:10.100
has been improving
documentation,

00:19:10.100 --> 00:19:12.320
adding API reference docs.

00:19:12.320 --> 00:19:15.350
We'll also be having a global
docs sprint in collaboration

00:19:15.350 --> 00:19:17.930
with our Google developer
experts and GDC groups

00:19:17.930 --> 00:19:19.590
later this year.

00:19:19.590 --> 00:19:21.890
And if you're interested
in collaborating

00:19:21.890 --> 00:19:24.830
for documentation, we
would love to have it.

00:19:24.830 --> 00:19:28.880
Please submit it as a pull
request to TensorFlow/examples

00:19:28.880 --> 00:19:32.000
on GitHub.

00:19:32.000 --> 00:19:34.880
We also understand that
developers and researchers need

00:19:34.880 --> 00:19:37.100
great performance,
and we did not

00:19:37.100 --> 00:19:40.620
want to sacrifice that as
part of TensorFlow 2.0.

00:19:40.620 --> 00:19:44.030
So since last year, we've
seen a 1.8 training speed up

00:19:44.030 --> 00:19:45.980
on NVIDIA Teslas.

00:19:45.980 --> 00:19:49.550
That's almost twice as fast
as some earlier versions

00:19:49.550 --> 00:19:51.110
of TensorFlow.

00:19:51.110 --> 00:19:54.470
We've seen increased
performance with Cloud TPUs,

00:19:54.470 --> 00:19:56.900
and then also great
performance in collaboration

00:19:56.900 --> 00:19:59.120
with our partners at Intel.

00:19:59.120 --> 00:20:00.770
Not just for training, though.

00:20:00.770 --> 00:20:03.440
If you're interested in
inferencing for TensorFlow

00:20:03.440 --> 00:20:07.160
Lite, we've brought down
the speed for edge TPUs

00:20:07.160 --> 00:20:10.540
to just two milliseconds
for quantized models.

00:20:10.540 --> 00:20:13.490
So underneath the hood,
TensorFlow Lite and TensorFlow

00:20:13.490 --> 00:20:17.580
are all about performance.

00:20:17.580 --> 00:20:21.030
We've also extended the
ecosystem in a number of ways.

00:20:21.030 --> 00:20:24.090
When TensorFlow was first
open sourced in 2015,

00:20:24.090 --> 00:20:27.360
it was a single repository
for numerical computing.

00:20:27.360 --> 00:20:30.400
And now it's grown into
an entire ecosystem.

00:20:30.400 --> 00:20:33.840
So if you're interested
in Bayesian modeling,

00:20:33.840 --> 00:20:36.332
you can use something like
TensorFlow Probability.

00:20:36.332 --> 00:20:38.290
If you're interested in
reinforcement learning,

00:20:38.290 --> 00:20:40.500
you can use TensorFlow Agents.

00:20:40.500 --> 00:20:43.890
If you're interested in text
processing you can use TF Text.

00:20:43.890 --> 00:20:46.560
If you're interested in
privacy or insecure computing

00:20:46.560 --> 00:20:48.990
you can use
Federated or Privacy.

00:20:48.990 --> 00:20:51.870
We also have a variety
of other projects--

00:20:51.870 --> 00:20:53.930
about 80 right now.

00:20:53.930 --> 00:20:55.710
And if you're interested
in any of them,

00:20:55.710 --> 00:20:57.990
I strongly suggest
you go and take a look

00:20:57.990 --> 00:20:59.190
at the TensorFlow GitHub.

00:21:01.643 --> 00:21:03.310
And now the question
I'm sure all of you

00:21:03.310 --> 00:21:06.730
are very, very interested
in is how do I upgrade?

00:21:06.730 --> 00:21:08.650
All of this sounds great.

00:21:08.650 --> 00:21:13.870
How do I make sure that all
of my historic legacy models

00:21:13.870 --> 00:21:16.780
continue to run
with TensorFlow 2.0?

00:21:16.780 --> 00:21:18.370
And the answer is,
we've tried to make

00:21:18.370 --> 00:21:20.590
it as easy as we possibly can.

00:21:20.590 --> 00:21:22.930
We have an escape to
backwards compatibility mode

00:21:22.930 --> 00:21:25.960
as part of tf.compat.v1.

00:21:25.960 --> 00:21:28.610
We have migration guides and
best practices that have all

00:21:28.610 --> 00:21:31.610
been placed on
tensorflow.org/alpha.

00:21:31.610 --> 00:21:35.860
We've also created something
called tf_upgrade_v2.

00:21:35.860 --> 00:21:38.110
It's a conversion
utility that you

00:21:38.110 --> 00:21:39.740
can run from the command line.

00:21:39.740 --> 00:21:44.290
And it takes your existing model
and imports the code to 2.0.

00:21:44.290 --> 00:21:46.570
Not to 2.0 syntax,
but it makes all

00:21:46.570 --> 00:21:49.150
of the changes that would
be required in order for it

00:21:49.150 --> 00:21:51.560
to run compatible with 2.0.

00:21:51.560 --> 00:21:54.280
So what does that look like?

00:21:54.280 --> 00:21:56.410
All you would have to
do is take your model,

00:21:56.410 --> 00:22:00.880
export it as a Jupyter
Notebook or as a Python file,

00:22:00.880 --> 00:22:04.000
and then run it from the
command line with tf_upgrade_v2,

00:22:04.000 --> 00:22:06.250
the input file name, and
then what you want the output

00:22:06.250 --> 00:22:07.600
file name to be.

00:22:07.600 --> 00:22:11.140
You can do this even for
a directory of files.

00:22:11.140 --> 00:22:13.060
The files then cycle
through and you

00:22:13.060 --> 00:22:15.980
get something that looks
a little bit like this.

00:22:15.980 --> 00:22:18.430
It's a report.txt
file that tells you

00:22:18.430 --> 00:22:21.730
all of the API endpoints
that have been renamed,

00:22:21.730 --> 00:22:25.030
all of the keywords
that have been added,

00:22:25.030 --> 00:22:26.830
and then all of
the instances that

00:22:26.830 --> 00:22:29.950
need to have this escape to
backwards compatibility mode.

00:22:29.950 --> 00:22:33.310
So prefixed with tf.compat.v1.

00:22:33.310 --> 00:22:34.870
Once all of that's
done, you should

00:22:34.870 --> 00:22:36.970
be able to run your
model as expected

00:22:36.970 --> 00:22:38.920
and see no performance
regressions.

00:22:38.920 --> 00:22:40.990
And if you do,
please file an issue.

00:22:40.990 --> 00:22:45.970
That's a bug, and we would
be delighted to resolve it.

00:22:45.970 --> 00:22:48.030
I'd also like to highlight
one of the projects

00:22:48.030 --> 00:22:49.620
that our Google
developer experts

00:22:49.620 --> 00:22:52.920
has created called tf2up.ml.

00:22:52.920 --> 00:22:55.050
So if you don't want to
run the upgrade utility

00:22:55.050 --> 00:22:59.605
from your command line, you're
free to take a GitHub URL,

00:22:59.605 --> 00:23:03.990
use tf2up, and see it
displayed with the changes

00:23:03.990 --> 00:23:05.910
in line in a browser window.

00:23:05.910 --> 00:23:07.080
This is really, really cool.

00:23:07.080 --> 00:23:10.910
Strongly suggest taking a look.

00:23:10.910 --> 00:23:13.630
So our timeline
for TensorFlow 2.0.

00:23:13.630 --> 00:23:15.400
Right now we're in alpha.

00:23:15.400 --> 00:23:19.210
We expect to launch an
RC release very soon,

00:23:19.210 --> 00:23:21.010
and we should have
an official release

00:23:21.010 --> 00:23:24.410
before the end of the year.

00:23:24.410 --> 00:23:27.320
You can also track all of this
progress publicly on GitHub.

00:23:27.320 --> 00:23:32.090
We've tried to increase
the transparency, and also

00:23:32.090 --> 00:23:34.130
the collaboration that
we see from the community

00:23:34.130 --> 00:23:37.490
as much as we possibly can,
because TensorFlow 2.0 really

00:23:37.490 --> 00:23:38.880
is all about community.

00:23:38.880 --> 00:23:41.300
And if you have questions
about any of the projects

00:23:41.300 --> 00:23:43.845
or about any of the issues
that you care about,

00:23:43.845 --> 00:23:44.970
we would love to hear them.

00:23:44.970 --> 00:23:49.680
And we would love to
prioritize them appropriately.

00:23:49.680 --> 00:23:53.115
And now to talk a little bit
about under-the-hood activities

00:23:53.115 --> 00:23:53.740
for TensorFlow.

00:23:53.740 --> 00:23:55.250
JOSH GORDAN: Thanks, Paige.

00:23:55.250 --> 00:23:58.310
Due to talking fast, we might
now actually have some time.

00:23:58.310 --> 00:24:01.070
So I will ask--
this is very basic,

00:24:01.070 --> 00:24:02.900
but I think a question
that a lot of people

00:24:02.900 --> 00:24:05.780
ask is, what exactly
is TensorFlow?

00:24:05.780 --> 00:24:08.660
And if I asked you that, what
I would have said when I first

00:24:08.660 --> 00:24:09.710
heard about it is,
like, right, that's

00:24:09.710 --> 00:24:11.377
an open source machine
learning library.

00:24:11.377 --> 00:24:12.770
Great, but what is it really?

00:24:12.770 --> 00:24:13.978
What does the code look like?

00:24:13.978 --> 00:24:16.603
How is it implemented, and what
problems is it actually solving

00:24:16.603 --> 00:24:17.450
that we care about?

00:24:17.450 --> 00:24:19.367
And rather than give you
the next slide, which

00:24:19.367 --> 00:24:21.338
is a lot of text and
the answer, the way

00:24:21.338 --> 00:24:22.880
to start thinking
about this question

00:24:22.880 --> 00:24:24.560
is actually with Python.

00:24:24.560 --> 00:24:26.870
So if you think about scientific
computing in general--

00:24:26.870 --> 00:24:29.878
and this is the whole field,
not just machine learning.

00:24:29.878 --> 00:24:31.670
Let's say you're doing
weather forecasting,

00:24:31.670 --> 00:24:33.380
and as part of
weather forecasting,

00:24:33.380 --> 00:24:35.900
you need to multiply a
whole bunch of matrices.

00:24:35.900 --> 00:24:38.030
Probably you're writing
your code in Python.

00:24:38.030 --> 00:24:41.690
But if you think about it, how
much slower is Python than C

00:24:41.690 --> 00:24:43.610
for multiplying matrices?

00:24:43.610 --> 00:24:44.710
Ballpark?

00:24:44.710 --> 00:24:47.420
And like a horribly
non-reproducible rough

00:24:47.420 --> 00:24:51.193
benchmark would be Python's
about 100 times slower than C.

00:24:51.193 --> 00:24:53.360
And that's the difference
between six seconds and 10

00:24:53.360 --> 00:24:56.600
minutes, which is also the
difference between running

00:24:56.600 --> 00:24:58.310
on a treadmill or
having a drink,

00:24:58.310 --> 00:25:00.920
sitting in an airplane
flying to California.

00:25:00.920 --> 00:25:04.670
So Python is horribly slow,
and yet it's extremely popular

00:25:04.670 --> 00:25:07.070
for performance-intensive tasks.

00:25:07.070 --> 00:25:11.060
One of the huge
reasons why is NumPy.

00:25:11.060 --> 00:25:15.370
And what NumPy is, it's a matrix
multiplier implemented in C

00:25:15.370 --> 00:25:17.162
that you can call from Python.

00:25:17.162 --> 00:25:18.620
And this gives you
this combination

00:25:18.620 --> 00:25:21.440
of Python ease of use
but C performance.

00:25:21.440 --> 00:25:24.740
And most deep learning libraries
have the same inspiration.

00:25:24.740 --> 00:25:27.680
So TensorFlow is a C++ back end.

00:25:27.680 --> 00:25:29.660
But usually-- not
always, but usually, we

00:25:29.660 --> 00:25:31.400
write our code in Python.

00:25:31.400 --> 00:25:35.030
On top of NumPy, what TensorFlow
and other deep learning

00:25:35.030 --> 00:25:38.000
libraries add is the
ability to run on GPUs.

00:25:38.000 --> 00:25:40.580
So in addition to being in C,
you can multiply your matrices

00:25:40.580 --> 00:25:41.660
on GPUs.

00:25:41.660 --> 00:25:43.323
And if you take a
deep learning class

00:25:43.323 --> 00:25:45.740
you'll end up learning that
the forward and backward paths

00:25:45.740 --> 00:25:48.570
in neural networks are both
matrix multiplications.

00:25:48.570 --> 00:25:50.455
So we care about this a lot.

00:25:50.455 --> 00:25:51.830
All deep learning
libraries, they

00:25:51.830 --> 00:25:53.192
add automatic differentiation.

00:25:53.192 --> 00:25:54.650
So you get the
gradient so you know

00:25:54.650 --> 00:25:56.720
how to update the
variables of your model.

00:25:56.720 --> 00:25:58.430
And TensorFlow adds
something special.

00:25:58.430 --> 00:26:01.220
And there's a lot of text on
the slide in details, whatever,

00:26:01.220 --> 00:26:02.640
you can look at later.

00:26:02.640 --> 00:26:05.798
But when you write a program
in TensorFlow in Python,

00:26:05.798 --> 00:26:07.340
one thing we care
a lot about-- and I

00:26:07.340 --> 00:26:08.660
know there are a lot of
mobile developers here--

00:26:08.660 --> 00:26:11.000
is we want to run it on devices
that don't have a Python

00:26:11.000 --> 00:26:12.170
interpreter.

00:26:12.170 --> 00:26:15.480
Examples of that would be
a web browser or a phone.

00:26:15.480 --> 00:26:18.050
And so what TensorFlow does
is your TensorFlow program

00:26:18.050 --> 00:26:20.360
is compiled to what
we call a graph.

00:26:20.360 --> 00:26:22.130
And it's a data flow graph.

00:26:22.130 --> 00:26:25.440
The word TensorFlow, tensor
is a fancy word for an array.

00:26:25.440 --> 00:26:28.220
Scalar is a tensor, an array's
a tensor, matrix is a tensor,

00:26:28.220 --> 00:26:31.010
a cube of numbers is a tensor,
it's an n-dimensional array.

00:26:31.010 --> 00:26:33.140
Flow means data flow graph.

00:26:33.140 --> 00:26:35.030
Your TensorFlow
program gets compiled

00:26:35.030 --> 00:26:36.760
behind the scenes
in TensorFlow 2--

00:26:36.760 --> 00:26:38.510
you don't need to know
the details of this

00:26:38.510 --> 00:26:41.090
unless you're
interested-- into a graph.

00:26:41.090 --> 00:26:43.890
And that graph can be
run on different devices.

00:26:43.890 --> 00:26:47.060
And what's interesting
is it can be accelerated.

00:26:47.060 --> 00:26:48.560
So just so you
know, because I've

00:26:48.560 --> 00:26:51.110
been talking about NumPy
a lot, TensorFlow 2,

00:26:51.110 --> 00:26:53.280
if you feel like it, works
basically the same way

00:26:53.280 --> 00:26:54.185
as NumPy.

00:26:54.185 --> 00:26:55.560
The syntax is
slightly different,

00:26:55.560 --> 00:26:56.685
but the ideas are the same.

00:26:56.685 --> 00:26:59.195
So here I'm creating some data,
a tensor instead of a NumPy

00:26:59.195 --> 00:27:01.362
array, and I'm multiplying
it by itself or whatever,

00:27:01.362 --> 00:27:05.060
and it just works
like regular NumPy.

00:27:05.060 --> 00:27:08.720
I should mention that instead of
being NumPy and [? D ?] arrays,

00:27:08.720 --> 00:27:11.000
TensorFlow tensors are
different data type,

00:27:11.000 --> 00:27:13.230
but they all have this
really nice NumPy method.

00:27:13.230 --> 00:27:14.630
So for any reason you're
tired of TensorFlow,

00:27:14.630 --> 00:27:16.172
you just call a
NumPy and you're back

00:27:16.172 --> 00:27:18.290
in NumPy land, which is awesome.

00:27:18.290 --> 00:27:22.222
But TensorFlow can do something
really special and cool,

00:27:22.222 --> 00:27:24.680
thanks to a really amazing
group of compiler engineers that

00:27:24.680 --> 00:27:25.790
are working on it.

00:27:25.790 --> 00:27:28.040
So here's just some random
code I wrote in TensorFlow,

00:27:28.040 --> 00:27:30.890
and I'm taking some layer and
I'm calling it on some data.

00:27:30.890 --> 00:27:33.680
And I have this horrible
non-reproducible benchmark

00:27:33.680 --> 00:27:35.810
that I ran at the bottom
there using timeit,

00:27:35.810 --> 00:27:39.140
and staring into the sun, I
think this took about 3/100

00:27:39.140 --> 00:27:40.730
of a second to run.

00:27:40.730 --> 00:27:44.420
And in TensorFlow 2.0, we can
accelerate this code further.

00:27:44.420 --> 00:27:46.160
And the way this
works is there's

00:27:46.160 --> 00:27:48.090
only one line of code change.

00:27:48.090 --> 00:27:52.040
So if you look closely here,
I've just added an annotation

00:27:52.040 --> 00:27:54.090
and I've added a
second benchmark.

00:27:54.090 --> 00:27:56.900
So if we go back one
slide, that's before.

00:27:56.900 --> 00:27:57.800
Whoops.

00:27:57.800 --> 00:27:59.960
Let's go-- perfect.

00:27:59.960 --> 00:28:02.750
So here's before
and here's after.

00:28:02.750 --> 00:28:04.580
And with just that
annotation, this

00:28:04.580 --> 00:28:06.980
is running something like
eight or nine times faster.

00:28:06.980 --> 00:28:09.747
And your mileage will vary
depending on the type of code

00:28:09.747 --> 00:28:10.580
that you accelerate.

00:28:10.580 --> 00:28:12.650
You won't always get
a performance benefit,

00:28:12.650 --> 00:28:16.520
but the only non-standard
Python syntax

00:28:16.520 --> 00:28:18.950
that you need to be aware of
optionally in TensorFlow 2

00:28:18.950 --> 00:28:20.263
is just the sanitation.

00:28:20.263 --> 00:28:21.680
So you don't need
to worry about--

00:28:21.680 --> 00:28:23.060
if you learned
TensorFlow 1, this

00:28:23.060 --> 00:28:24.950
is really valuable
knowledge, because there's so

00:28:24.950 --> 00:28:27.492
many papers with really awesome
implementations in TensorFlow

00:28:27.492 --> 00:28:28.100
1.

00:28:28.100 --> 00:28:29.475
But the only
nonstand-- you don't

00:28:29.475 --> 00:28:32.570
need to worry anymore about like
sessions, place holders, feed

00:28:32.570 --> 00:28:34.382
dictionaries, that's
just not necessary.

00:28:34.382 --> 00:28:35.340
This is the only thing.

00:28:35.340 --> 00:28:37.160
Otherwise it's regular Python.

00:28:37.160 --> 00:28:39.770
And the way this works
behind the scenes--

00:28:39.770 --> 00:28:41.330
I'm showing you
this just for fun.

00:28:41.330 --> 00:28:42.200
You can totally ignore it.

00:28:42.200 --> 00:28:44.158
I personally never look
at this because I'm not

00:28:44.158 --> 00:28:45.140
a compilers person.

00:28:45.140 --> 00:28:47.150
But it works with
something called AutoGraph.

00:28:47.150 --> 00:28:51.930
And AutoGraph is a
source-to-source Python

00:28:51.930 --> 00:28:52.860
transformer.

00:28:52.860 --> 00:28:54.660
And basically it
generates a version

00:28:54.660 --> 00:28:57.720
of this code which can be
accelerated to a greater extent

00:28:57.720 --> 00:28:58.902
by the back end.

00:28:58.902 --> 00:29:00.610
And we can actually
see what's generated.

00:29:00.610 --> 00:29:01.812
We can print out AutoGraph.

00:29:01.812 --> 00:29:03.270
And basically what
we're looking at

00:29:03.270 --> 00:29:05.670
is more or less assembly
code for that function

00:29:05.670 --> 00:29:06.540
we just wrote.

00:29:06.540 --> 00:29:09.510
But in TensorFlow 2, you get
this for free, which I think

00:29:09.510 --> 00:29:11.010
is a really awesome thing.

00:29:11.010 --> 00:29:12.570
And it's super cool.

00:29:12.570 --> 00:29:17.400
That said, the fact that we need
a compilers engineer to write

00:29:17.400 --> 00:29:20.405
something as powerful and
amazing as this points us--

00:29:20.405 --> 00:29:21.780
I mean, you might
want to think--

00:29:21.780 --> 00:29:24.750
is we're really starting to
run into the wall with Python.

00:29:24.750 --> 00:29:27.482
And Python is by far my
favorite language of all time.

00:29:27.482 --> 00:29:29.940
It's going to be probably the
standard for machine learning

00:29:29.940 --> 00:29:31.055
for many years to come.

00:29:31.055 --> 00:29:33.180
But there's a huge amount
of value in investigating

00:29:33.180 --> 00:29:35.400
compiled languages like Swift.

00:29:35.400 --> 00:29:37.500
And if you're a Swift
developer, there's

00:29:37.500 --> 00:29:39.990
an excellent implementation
of TensorFlow in Swift,

00:29:39.990 --> 00:29:42.990
which you can learn and all the
same concepts that you learned

00:29:42.990 --> 00:29:44.580
about in Python
will apply in Swift.

00:29:44.580 --> 00:29:47.235
Likewise, TensorFlow
JS is incredible.

00:29:47.235 --> 00:29:48.610
If you're a
JavaScript developer,

00:29:48.610 --> 00:29:49.650
there's no need to learn Python.

00:29:49.650 --> 00:29:51.275
You can just go
directly in JavaScript.

00:29:51.275 --> 00:29:54.720
And this is something really
special about TensorFlow.

00:29:54.720 --> 00:29:57.630
OK, due to, as I said earlier,
talking way too fast because I

00:29:57.630 --> 00:29:59.760
thought this talk was
longer than it was,

00:29:59.760 --> 00:30:03.870
here is the best way to
learn about TensorFlow 2.

00:30:03.870 --> 00:30:05.970
Right now all of our
tutorials are hosted

00:30:05.970 --> 00:30:07.950
on tensorflow.org/alpha.

00:30:07.950 --> 00:30:10.630
If you're new to TensorFlow
and you're a Python developer,

00:30:10.630 --> 00:30:12.985
you should ignore everything
else on the website.

00:30:12.985 --> 00:30:14.610
Just go to Alpha and
this will give you

00:30:14.610 --> 00:30:16.080
the latest stuff for 2.

00:30:16.080 --> 00:30:19.020
Everything else is outdated
and totally not necessary.

00:30:19.020 --> 00:30:22.060
Because this is complicated,
I put some keywords there.

00:30:22.060 --> 00:30:25.140
And if you see any of those
keywords in a book or a video

00:30:25.140 --> 00:30:26.550
or tutorial, just skip it.

00:30:26.550 --> 00:30:29.960
It's outdated, and we should
be finished with TensorFlow 2

00:30:29.960 --> 00:30:32.820
in a few months, at which point
we will roll the entire website

00:30:32.820 --> 00:30:33.990
over to the new stuff.

00:30:33.990 --> 00:30:35.550
So that's absolutely key.

00:30:35.550 --> 00:30:37.730
Speaking of things that
are coming up soon,

00:30:37.730 --> 00:30:39.780
here are two awesome books.

00:30:39.780 --> 00:30:42.340
And I want to mention, just so
you know what you're getting,

00:30:42.340 --> 00:30:43.350
I'm not going
pronounce the names.

00:30:43.350 --> 00:30:45.267
I mentioned earlier I'm
terrible at languages.

00:30:45.267 --> 00:30:46.810
I can't pronounce French at all.

00:30:46.810 --> 00:30:49.180
They're both brilliant,
awesome people.

00:30:49.180 --> 00:30:52.230
The first is the second
edition of the Hands-On Machine

00:30:52.230 --> 00:30:53.250
Learning book.

00:30:53.250 --> 00:30:55.230
The first uses
TensorFlow 1, so I'd

00:30:55.230 --> 00:30:56.525
recommend skipping that one.

00:30:56.525 --> 00:30:58.150
This one is coming
out in a few months.

00:30:58.150 --> 00:30:59.880
The GitHub repo
is available now.

00:30:59.880 --> 00:31:02.010
It's excellent, so you
can start with this.

00:31:02.010 --> 00:31:04.440
The second book, I'll
pronounce this one.

00:31:04.440 --> 00:31:05.755
It's Francois Chollet.

00:31:05.755 --> 00:31:07.260
Hope I got it right.

00:31:07.260 --> 00:31:09.840
And then Aurelien Geron,
which I probably got wrong.

00:31:09.840 --> 00:31:12.180
The second book doesn't
use TensorFlow at all.

00:31:12.180 --> 00:31:14.700
It uses the Keras
reference implementation.

00:31:14.700 --> 00:31:17.970
Every concept you learn in that
book will work in TensorFlow 2

00:31:17.970 --> 00:31:19.480
just by changing the import.

00:31:19.480 --> 00:31:20.980
So you don't waste
your time at all.

00:31:20.980 --> 00:31:23.230
It's an outstanding reference.

00:31:23.230 --> 00:31:23.730
And yeah.

00:31:23.730 --> 00:31:25.710
We'll be around after if
we can answer any questions

00:31:25.710 --> 00:31:26.627
or help with anything.

00:31:26.627 --> 00:31:27.990
So thank you very much.

00:31:27.990 --> 00:31:31.340
[MUSIC PLAYING]

