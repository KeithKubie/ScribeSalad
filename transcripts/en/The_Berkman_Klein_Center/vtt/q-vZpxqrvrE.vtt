WEBVTT
Kind: captions
Language: en

00:00:06.160 --> 00:00:11.900
In a world of conflicting values, it's going
to be difficult to develop values for artificial

00:00:11.900 --> 00:00:15.450
intelligence that are not the lowest common
denominator.

00:00:15.450 --> 00:00:20.510
In Asia particularly, we have a lot of countries
that believe that governments are the best

00:00:20.510 --> 00:00:24.910
way to make decisions for people, and that
individual human rights can be voiced only

00:00:24.910 --> 00:00:26.160
by the state.

00:00:26.160 --> 00:00:32.800
In that context, if we are trying to come
up with artificial intelligence that recognizes

00:00:32.800 --> 00:00:38.350
individual human rights, and that looks to
empower citizens and users, and to create

00:00:38.350 --> 00:00:43.170
transparency, it's going to be really challenging
to come up with an international-coordinated

00:00:43.170 --> 00:00:44.710
regime that does this.

00:00:44.710 --> 00:00:48.989
People have developed AI that is predictive,
people are researching ways to make sure that

00:00:48.989 --> 00:00:53.610
AI is able to target advertisements at people
depending on their preferences, the devices

00:00:53.610 --> 00:00:55.980
they use, the routes they take.

00:00:55.980 --> 00:01:01.410
Now, that kind of predictive AI can very easily
be used for surveillance.

00:01:01.410 --> 00:01:07.310
And it's a fact in Asia that the states in
Asia, including India, are investing very

00:01:07.310 --> 00:01:13.800
heavily in mass surveillance and they are creating
large, centralized databases that they haven't

00:01:13.800 --> 00:01:17.460
fully worked out how to sweep as of yet.

00:01:17.460 --> 00:01:24.300
In India, we've also got news of the state
using drones to monitor assemblies of people

00:01:24.300 --> 00:01:27.470
in public places to make sure that nothing
goes wrong.

00:01:27.470 --> 00:01:32.330
We've got news that the government is developing
social media labs that are supposed to watch

00:01:32.330 --> 00:01:37.240
the online social media to see what people
are saying and what kinds of subjects are

00:01:37.240 --> 00:01:38.240
trending.

00:01:38.240 --> 00:01:43.160
And in that context, again, the question we're
asking ourselves is when the state chooses

00:01:43.160 --> 00:01:49.110
to use its resources to get AI to
do these things, how far is AI going to be

00:01:49.110 --> 00:01:53.780
used to control and monitor the citizen, as
opposed to enabling the citizen?

00:01:53.780 --> 00:01:58.259
Because in democracies like ours, the balance
of power between the citizen and the state

00:01:58.259 --> 00:02:00.259
is really delicate.

00:02:00.259 --> 00:02:05.970
And there is a great potential for AI to tip
that balance of power in favor of the state.

00:02:05.970 --> 00:02:10.989
While it's important to make sure that we
don't chill innovation, it's also important

00:02:10.989 --> 00:02:16.960
to be cautious, and to make sure that technology
doesn't drag us down a dark path.

00:02:16.960 --> 00:02:22.690
We've got examples from history like the Manhattan
Project, like the way in which technology

00:02:22.690 --> 00:02:29.110
was used during the Holocaust, to remind us
that if we're not careful about what we do

00:02:29.110 --> 00:02:34.980
with technology, it can be abused in ways
that we will come to deeply regret.

00:02:34.980 --> 00:02:40.780
So it's necessary to make sure not just that
we have human rights, political theory, but

00:02:40.780 --> 00:02:45.570
also the other disciplines that understand
what it means to be human and how to engage

00:02:45.570 --> 00:02:48.760
with humans involved in the designing of AI.

00:02:48.760 --> 00:02:54.310
If we don't work out a way in which citizens
are able to ask the right questions about

00:02:54.310 --> 00:02:59.620
AI to ensure accountability every time AI
is created and used, we might be heading towards

00:02:59.620 --> 00:03:01.180
a world that Orwell predicted.

00:03:01.180 --> 00:03:06.050
and that would be really unfortunate, because
new technology should lead to a better world,

00:03:06.050 --> 00:03:09.870
and not a more controlled world or an unequal
world.

00:03:09.870 --> 00:03:14.120
As you know, technology is easily sold to
people, and it moves very quickly around the

00:03:14.120 --> 00:03:19.870
world, and so, it's really important to intervene
in Asia at the stage of design.

00:03:19.870 --> 00:03:26.430
People sometimes have the best of intentions,
but because of the way in which they're educated

00:03:26.430 --> 00:03:31.100
or they're taught to think, the way in which they design
technology can end up being really damaging to

00:03:31.100 --> 00:03:32.100
the world.

00:03:32.100 --> 00:03:36.319
Conversely, it could end up being really beautiful
as well, and that's why it's really important

00:03:36.319 --> 00:03:42.940
that we get into AI right now and help the
people that are designing it to think of it

00:03:42.940 --> 00:03:44.260
in a way that imagines a better world.

