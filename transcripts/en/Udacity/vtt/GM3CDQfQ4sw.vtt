WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:05.250
Boosting is a fairly simple variation
on bagging that strives to improve

00:00:05.250 --> 00:00:09.170
the learners by focusing on areas where
the system is not performing well.

00:00:09.170 --> 00:00:14.060
One of the most well-known algorithms
in this area is called ada boost.

00:00:14.060 --> 00:00:18.460
And I believe it's ada,
not ata because ada stands for adaptive.

00:00:18.460 --> 00:00:20.680
Here's how ada boost works.

00:00:20.680 --> 00:00:23.420
We build our first bag of
data in the usual way.

00:00:23.420 --> 00:00:26.670
We select randomly
from our training data.

00:00:26.670 --> 00:00:29.880
We then train a model in a usual way.

00:00:29.880 --> 00:00:32.430
The next thing we do, and
this is something different,

00:00:32.430 --> 00:00:37.150
we take all our training data and
use it to test the model

00:00:37.150 --> 00:00:40.975
in order to discover that
some of the points in here,

00:00:40.975 --> 00:00:46.060
our x's and our y's,
are not well predicted.

00:00:46.060 --> 00:00:48.500
So there's going to be
some points in here for

00:00:48.500 --> 00:00:52.503
which there is significant error.

00:00:52.503 --> 00:00:56.660
Now, when we go to build our
next bag of data, again,

00:00:56.660 --> 00:00:59.550
we choose randomly
from our original data.

00:00:59.550 --> 00:01:04.300
But each instance is weighted
according to this error.

00:01:04.300 --> 00:01:08.850
So, these points that had significant
error, are more likely to get picked and

00:01:08.850 --> 00:01:12.832
to go into this bag than any
other individual instance.

00:01:12.832 --> 00:01:16.900
So as you see, we ended up with
a few of those points in here and

00:01:16.900 --> 00:01:19.220
a smattering of all
the other ones as well.

00:01:19.220 --> 00:01:22.980
We build a model from this data and
then we test it.

00:01:22.980 --> 00:01:25.460
Now we test our system altogether.

00:01:25.460 --> 00:01:28.760
In other words, we've got a sort
of miniature ensemble here,

00:01:28.760 --> 00:01:30.560
just two learners.

00:01:30.560 --> 00:01:32.420
And we test both of them.

00:01:32.420 --> 00:01:37.360
We test them by inputting
again this in-sample data.

00:01:37.360 --> 00:01:41.430
We test on each instance and
we combine their outputs.

00:01:41.430 --> 00:01:44.720
And again we measure error
across all this data.

00:01:44.720 --> 00:01:47.780
Maybe this time these points
got modeled better, but

00:01:47.780 --> 00:01:49.820
there were some other ones up
here that weren't as good.

00:01:50.840 --> 00:01:53.760
And thus we build our next bag and
our next model.

00:01:53.760 --> 00:01:55.750
And we just continue this over,
and over and

00:01:55.750 --> 00:02:01.090
over again up until m or
the total number of bags we'll be using.

00:02:01.090 --> 00:02:04.720
So to recap, bagging,
when we build one of these instances,

00:02:04.720 --> 00:02:09.770
is simply choosing some subset of
the data at random with replacement,

00:02:09.770 --> 00:02:12.490
and we create each bag in the same way.

00:02:12.490 --> 00:02:18.870
Boosting is an add-on to this idea where
in subsequent bags we choose those

00:02:18.870 --> 00:02:23.280
data instances that had been modeled
poorly in the overall system before.

