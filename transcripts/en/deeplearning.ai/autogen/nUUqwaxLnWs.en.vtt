WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.929
so why does that song work just one

00:00:02.929 --> 00:00:02.939
so why does that song work just one
 

00:00:02.939 --> 00:00:05.660
so why does that song work just one
reason you've seen how normalizing the

00:00:05.660 --> 00:00:05.670
reason you've seen how normalizing the
 

00:00:05.670 --> 00:00:08.390
reason you've seen how normalizing the
input features the X's to mean 0 and

00:00:08.390 --> 00:00:08.400
input features the X's to mean 0 and
 

00:00:08.400 --> 00:00:10.220
input features the X's to mean 0 and
variance 1 how that can speed up

00:00:10.220 --> 00:00:10.230
variance 1 how that can speed up
 

00:00:10.230 --> 00:00:12.140
variance 1 how that can speed up
learning so rather than having some

00:00:12.140 --> 00:00:12.150
learning so rather than having some
 

00:00:12.150 --> 00:00:14.330
learning so rather than having some
features they range from 0 to 1 and some

00:00:14.330 --> 00:00:14.340
features they range from 0 to 1 and some
 

00:00:14.340 --> 00:00:16.430
features they range from 0 to 1 and some
from one to a thousand by normalizing

00:00:16.430 --> 00:00:16.440
from one to a thousand by normalizing
 

00:00:16.440 --> 00:00:19.040
from one to a thousand by normalizing
all the features input features X to

00:00:19.040 --> 00:00:19.050
all the features input features X to
 

00:00:19.050 --> 00:00:20.990
all the features input features X to
take on a similar range of values that

00:00:20.990 --> 00:00:21.000
take on a similar range of values that
 

00:00:21.000 --> 00:00:23.900
take on a similar range of values that
can speed up learning so one intuition

00:00:23.900 --> 00:00:23.910
can speed up learning so one intuition
 

00:00:23.910 --> 00:00:26.000
can speed up learning so one intuition
behind why passional works is this is

00:00:26.000 --> 00:00:26.010
behind why passional works is this is
 

00:00:26.010 --> 00:00:29.150
behind why passional works is this is
doing a similar thing but for the values

00:00:29.150 --> 00:00:29.160
doing a similar thing but for the values
 

00:00:29.160 --> 00:00:31.160
doing a similar thing but for the values
in your hidden unions and not just for

00:00:31.160 --> 00:00:31.170
in your hidden unions and not just for
 

00:00:31.170 --> 00:00:34.100
in your hidden unions and not just for
your input layer now this is just a

00:00:34.100 --> 00:00:34.110
your input layer now this is just a
 

00:00:34.110 --> 00:00:36.979
your input layer now this is just a
partial picture for what - norm is doing

00:00:36.979 --> 00:00:36.989
partial picture for what - norm is doing
 

00:00:36.989 --> 00:00:38.959
partial picture for what - norm is doing
there are a couple further intuitions

00:00:38.959 --> 00:00:38.969
there are a couple further intuitions
 

00:00:38.969 --> 00:00:40.729
there are a couple further intuitions
that will help you gain a deeper

00:00:40.729 --> 00:00:40.739
that will help you gain a deeper
 

00:00:40.739 --> 00:00:42.830
that will help you gain a deeper
understanding of what batch tom is doing

00:00:42.830 --> 00:00:42.840
understanding of what batch tom is doing
 

00:00:42.840 --> 00:00:45.290
understanding of what batch tom is doing
let's take a look at those in this video

00:00:45.290 --> 00:00:45.300
let's take a look at those in this video
 

00:00:45.300 --> 00:00:48.470
let's take a look at those in this video
a second reason why batch norm works is

00:00:48.470 --> 00:00:48.480
a second reason why batch norm works is
 

00:00:48.480 --> 00:00:51.860
a second reason why batch norm works is
it makes wait later or deeper than your

00:00:51.860 --> 00:00:51.870
it makes wait later or deeper than your
 

00:00:51.870 --> 00:00:54.529
it makes wait later or deeper than your
network say the way so layer 10 more

00:00:54.529 --> 00:00:54.539
network say the way so layer 10 more
 

00:00:54.539 --> 00:00:57.740
network say the way so layer 10 more
robust to changes to ways in earlier

00:00:57.740 --> 00:00:57.750
robust to changes to ways in earlier
 

00:00:57.750 --> 00:00:59.360
robust to changes to ways in earlier
layers of the neural network say in

00:00:59.360 --> 00:00:59.370
layers of the neural network say in
 

00:00:59.370 --> 00:01:02.180
layers of the neural network say in
their one to explain what I mean let's

00:01:02.180 --> 00:01:02.190
their one to explain what I mean let's
 

00:01:02.190 --> 00:01:05.240
their one to explain what I mean let's
look at this motivating example let's

00:01:05.240 --> 00:01:05.250
look at this motivating example let's
 

00:01:05.250 --> 00:01:06.710
look at this motivating example let's
say you're training a network maybe a

00:01:06.710 --> 00:01:06.720
say you're training a network maybe a
 

00:01:06.720 --> 00:01:09.830
say you're training a network maybe a
shallow Network like legit regression or

00:01:09.830 --> 00:01:09.840
shallow Network like legit regression or
 

00:01:09.840 --> 00:01:13.370
shallow Network like legit regression or
maybe a neural network maybe run maybe a

00:01:13.370 --> 00:01:13.380
maybe a neural network maybe run maybe a
 

00:01:13.380 --> 00:01:15.499
maybe a neural network maybe run maybe a
shallow Network languages Russian or

00:01:15.499 --> 00:01:15.509
shallow Network languages Russian or
 

00:01:15.509 --> 00:01:18.950
shallow Network languages Russian or
maybe a deeper network on our famous cat

00:01:18.950 --> 00:01:18.960
maybe a deeper network on our famous cat
 

00:01:18.960 --> 00:01:22.100
maybe a deeper network on our famous cat
detection sauce but let's say that

00:01:22.100 --> 00:01:22.110
detection sauce but let's say that
 

00:01:22.110 --> 00:01:24.109
detection sauce but let's say that
you've trained your datasets on all

00:01:24.109 --> 00:01:24.119
you've trained your datasets on all
 

00:01:24.119 --> 00:01:27.649
you've trained your datasets on all
images of black cats if you now try to

00:01:27.649 --> 00:01:27.659
images of black cats if you now try to
 

00:01:27.659 --> 00:01:32.450
images of black cats if you now try to
apply this network to data with colored

00:01:32.450 --> 00:01:32.460
apply this network to data with colored
 

00:01:32.460 --> 00:01:34.460
apply this network to data with colored
cats where the positive examples are not

00:01:34.460 --> 00:01:34.470
cats where the positive examples are not
 

00:01:34.470 --> 00:01:38.450
cats where the positive examples are not
just black cats like on the left but the

00:01:38.450 --> 00:01:38.460
just black cats like on the left but the
 

00:01:38.460 --> 00:01:41.060
just black cats like on the left but the
colored cats like on the right then your

00:01:41.060 --> 00:01:41.070
colored cats like on the right then your
 

00:01:41.070 --> 00:01:41.600
colored cats like on the right then your
Casas

00:01:41.600 --> 00:01:41.610
Casas
 

00:01:41.610 --> 00:01:44.719
Casas
might not do very well so in pictures if

00:01:44.719 --> 00:01:44.729
might not do very well so in pictures if
 

00:01:44.729 --> 00:01:48.080
might not do very well so in pictures if
your training set look like this where

00:01:48.080 --> 00:01:48.090
your training set look like this where
 

00:01:48.090 --> 00:01:50.030
your training set look like this where
you have positive examples here and

00:01:50.030 --> 00:01:50.040
you have positive examples here and
 

00:01:50.040 --> 00:01:53.179
you have positive examples here and
negative examples here but you were to

00:01:53.179 --> 00:01:53.189
negative examples here but you were to
 

00:01:53.189 --> 00:01:56.149
negative examples here but you were to
try to generalize it to data set where

00:01:56.149 --> 00:01:56.159
try to generalize it to data set where
 

00:01:56.159 --> 00:02:00.139
try to generalize it to data set where
we will posit examples are here and the

00:02:00.139 --> 00:02:00.149
we will posit examples are here and the
 

00:02:00.149 --> 00:02:02.810
we will posit examples are here and the
negative examples are here then you

00:02:02.810 --> 00:02:02.820
negative examples are here then you
 

00:02:02.820 --> 00:02:05.480
negative examples are here then you
might not expect a model trained on the

00:02:05.480 --> 00:02:05.490
might not expect a model trained on the
 

00:02:05.490 --> 00:02:07.940
might not expect a model trained on the
data on the left to do very well on the

00:02:07.940 --> 00:02:07.950
data on the left to do very well on the
 

00:02:07.950 --> 00:02:10.309
data on the left to do very well on the
data on the right even though you know

00:02:10.309 --> 00:02:10.319
data on the right even though you know
 

00:02:10.319 --> 00:02:11.980
data on the right even though you know
there might be the same function

00:02:11.980 --> 00:02:11.990
there might be the same function
 

00:02:11.990 --> 00:02:14.620
there might be the same function
it actually works well but you wouldn't

00:02:14.620 --> 00:02:14.630
it actually works well but you wouldn't
 

00:02:14.630 --> 00:02:17.170
it actually works well but you wouldn't
expect your learning algorithm to

00:02:17.170 --> 00:02:17.180
expect your learning algorithm to
 

00:02:17.180 --> 00:02:19.090
expect your learning algorithm to
discover that green decision boundaries

00:02:19.090 --> 00:02:19.100
discover that green decision boundaries
 

00:02:19.100 --> 00:02:21.670
discover that green decision boundaries
just looking at the data on the left so

00:02:21.670 --> 00:02:21.680
just looking at the data on the left so
 

00:02:21.680 --> 00:02:24.390
just looking at the data on the left so
this idea of your data distribution

00:02:24.390 --> 00:02:24.400
this idea of your data distribution
 

00:02:24.400 --> 00:02:28.860
this idea of your data distribution
changing goes by the somewhat fancy name

00:02:28.860 --> 00:02:28.870
changing goes by the somewhat fancy name
 

00:02:28.870 --> 00:02:33.790
changing goes by the somewhat fancy name
covariant shift and the idea is that if

00:02:33.790 --> 00:02:33.800
covariant shift and the idea is that if
 

00:02:33.800 --> 00:02:36.370
covariant shift and the idea is that if
you learn some XY mapping if the

00:02:36.370 --> 00:02:36.380
you learn some XY mapping if the
 

00:02:36.380 --> 00:02:39.280
you learn some XY mapping if the
distribution of X changes then you might

00:02:39.280 --> 00:02:39.290
distribution of X changes then you might
 

00:02:39.290 --> 00:02:41.170
distribution of X changes then you might
need to retrain your learning algorithm

00:02:41.170 --> 00:02:41.180
need to retrain your learning algorithm
 

00:02:41.180 --> 00:02:43.480
need to retrain your learning algorithm
and this is true even if the function

00:02:43.480 --> 00:02:43.490
and this is true even if the function
 

00:02:43.490 --> 00:02:46.000
and this is true even if the function
the ground true function mapping from X

00:02:46.000 --> 00:02:46.010
the ground true function mapping from X
 

00:02:46.010 --> 00:02:48.640
the ground true function mapping from X
to Y remains unchanged which it is in

00:02:48.640 --> 00:02:48.650
to Y remains unchanged which it is in
 

00:02:48.650 --> 00:02:50.680
to Y remains unchanged which it is in
this example because the ground root

00:02:50.680 --> 00:02:50.690
this example because the ground root
 

00:02:50.690 --> 00:02:53.140
this example because the ground root
function is is this picture of cattle

00:02:53.140 --> 00:02:53.150
function is is this picture of cattle
 

00:02:53.150 --> 00:02:55.330
function is is this picture of cattle
not and they need to retrain your

00:02:55.330 --> 00:02:55.340
not and they need to retrain your
 

00:02:55.340 --> 00:02:58.650
not and they need to retrain your
function becomes even more acute or

00:02:58.650 --> 00:02:58.660
function becomes even more acute or
 

00:02:58.660 --> 00:03:01.270
function becomes even more acute or
becomes even worse if the ground true

00:03:01.270 --> 00:03:01.280
becomes even worse if the ground true
 

00:03:01.280 --> 00:03:04.870
becomes even worse if the ground true
function shifts as well so how does this

00:03:04.870 --> 00:03:04.880
function shifts as well so how does this
 

00:03:04.880 --> 00:03:07.660
function shifts as well so how does this
problem of covariant apply to a neural

00:03:07.660 --> 00:03:07.670
problem of covariant apply to a neural
 

00:03:07.670 --> 00:03:10.330
problem of covariant apply to a neural
network consider a deep network like

00:03:10.330 --> 00:03:10.340
network consider a deep network like
 

00:03:10.340 --> 00:03:12.130
network consider a deep network like
this and let's look at the learning

00:03:12.130 --> 00:03:12.140
this and let's look at the learning
 

00:03:12.140 --> 00:03:14.800
this and let's look at the learning
process from the perspective of this

00:03:14.800 --> 00:03:14.810
process from the perspective of this
 

00:03:14.810 --> 00:03:17.230
process from the perspective of this
hidden layer the third hidden layer so

00:03:17.230 --> 00:03:17.240
hidden layer the third hidden layer so
 

00:03:17.240 --> 00:03:19.270
hidden layer the third hidden layer so
this network has to learn the parameters

00:03:19.270 --> 00:03:19.280
this network has to learn the parameters
 

00:03:19.280 --> 00:03:23.440
this network has to learn the parameters
W 3 and B 3 and from the perspective of

00:03:23.440 --> 00:03:23.450
W 3 and B 3 and from the perspective of
 

00:03:23.450 --> 00:03:25.780
W 3 and B 3 and from the perspective of
the third hidden layer it gets some set

00:03:25.780 --> 00:03:25.790
the third hidden layer it gets some set
 

00:03:25.790 --> 00:03:27.640
the third hidden layer it gets some set
of values from the earlier leaders and

00:03:27.640 --> 00:03:27.650
of values from the earlier leaders and
 

00:03:27.650 --> 00:03:29.290
of values from the earlier leaders and
then it has to do some stuff to

00:03:29.290 --> 00:03:29.300
then it has to do some stuff to
 

00:03:29.300 --> 00:03:31.930
then it has to do some stuff to
hopefully make the output Y hat close to

00:03:31.930 --> 00:03:31.940
hopefully make the output Y hat close to
 

00:03:31.940 --> 00:03:35.500
hopefully make the output Y hat close to
the ground true value Y so let me cover

00:03:35.500 --> 00:03:35.510
the ground true value Y so let me cover
 

00:03:35.510 --> 00:03:38.620
the ground true value Y so let me cover
up the nodes on the left for a second so

00:03:38.620 --> 00:03:38.630
up the nodes on the left for a second so
 

00:03:38.630 --> 00:03:41.650
up the nodes on the left for a second so
from the perspective of this third thin

00:03:41.650 --> 00:03:41.660
from the perspective of this third thin
 

00:03:41.660 --> 00:03:44.830
from the perspective of this third thin
layer it gets some values let's call

00:03:44.830 --> 00:03:44.840
layer it gets some values let's call
 

00:03:44.840 --> 00:03:54.850
layer it gets some values let's call
them a 2 1 a 2 2 a 2 3 and a 2 4 but

00:03:54.850 --> 00:03:54.860
them a 2 1 a 2 2 a 2 3 and a 2 4 but
 

00:03:54.860 --> 00:03:56.740
them a 2 1 a 2 2 a 2 3 and a 2 4 but
these values might as well be features

00:03:56.740 --> 00:03:56.750
these values might as well be features
 

00:03:56.750 --> 00:04:01.630
these values might as well be features
x1 x2 x3 x4 and the job of the 13 layer

00:04:01.630 --> 00:04:01.640
x1 x2 x3 x4 and the job of the 13 layer
 

00:04:01.640 --> 00:04:05.650
x1 x2 x3 x4 and the job of the 13 layer
is to take these values and find a way

00:04:05.650 --> 00:04:05.660
is to take these values and find a way
 

00:04:05.660 --> 00:04:09.330
is to take these values and find a way
to map them to my hat so you can imagine

00:04:09.330 --> 00:04:09.340
to map them to my hat so you can imagine
 

00:04:09.340 --> 00:04:11.680
to map them to my hat so you can imagine
doing gradient descent so that these

00:04:11.680 --> 00:04:11.690
doing gradient descent so that these
 

00:04:11.690 --> 00:04:15.520
doing gradient descent so that these
parameters W 3 P 3 as well as maybe W 4

00:04:15.520 --> 00:04:15.530
parameters W 3 P 3 as well as maybe W 4
 

00:04:15.530 --> 00:04:20.740
parameters W 3 P 3 as well as maybe W 4
B 4 and even w 5 B 5 maybe trying to

00:04:20.740 --> 00:04:20.750
B 4 and even w 5 B 5 maybe trying to
 

00:04:20.750 --> 00:04:22.420
B 4 and even w 5 B 5 maybe trying to
learn those parameters so the network

00:04:22.420 --> 00:04:22.430
learn those parameters so the network
 

00:04:22.430 --> 00:04:23.910
learn those parameters so the network
does a good job not being

00:04:23.910 --> 00:04:23.920
does a good job not being
 

00:04:23.920 --> 00:04:26.460
does a good job not being
from the values I drew in black on the

00:04:26.460 --> 00:04:26.470
from the values I drew in black on the
 

00:04:26.470 --> 00:04:29.640
from the values I drew in black on the
left to the output values why I had but

00:04:29.640 --> 00:04:29.650
left to the output values why I had but
 

00:04:29.650 --> 00:04:32.100
left to the output values why I had but
now let's uncover the left of the

00:04:32.100 --> 00:04:32.110
now let's uncover the left of the
 

00:04:32.110 --> 00:04:34.680
now let's uncover the left of the
network again the network is also

00:04:34.680 --> 00:04:34.690
network again the network is also
 

00:04:34.690 --> 00:04:42.120
network again the network is also
adapting parameters W 2 B 2 and W 1 B 1

00:04:42.120 --> 00:04:42.130
adapting parameters W 2 B 2 and W 1 B 1
 

00:04:42.130 --> 00:04:45.600
adapting parameters W 2 B 2 and W 1 B 1
and so as these parameters change these

00:04:45.600 --> 00:04:45.610
and so as these parameters change these
 

00:04:45.610 --> 00:04:50.610
and so as these parameters change these
values a 2 will also change so from the

00:04:50.610 --> 00:04:50.620
values a 2 will also change so from the
 

00:04:50.620 --> 00:04:52.530
values a 2 will also change so from the
perspective of the third hidden layer

00:04:52.530 --> 00:04:52.540
perspective of the third hidden layer
 

00:04:52.540 --> 00:04:55.620
perspective of the third hidden layer
these hidden unit values are changing

00:04:55.620 --> 00:04:55.630
these hidden unit values are changing
 

00:04:55.630 --> 00:04:57.840
these hidden unit values are changing
all the time and so is suffering from

00:04:57.840 --> 00:04:57.850
all the time and so is suffering from
 

00:04:57.850 --> 00:05:00.570
all the time and so is suffering from
the problem of covariant shift that we

00:05:00.570 --> 00:05:00.580
the problem of covariant shift that we
 

00:05:00.580 --> 00:05:02.610
the problem of covariant shift that we
talked about on the previous line so

00:05:02.610 --> 00:05:02.620
talked about on the previous line so
 

00:05:02.620 --> 00:05:05.460
talked about on the previous line so
what that Norm does is it reduces the

00:05:05.460 --> 00:05:05.470
what that Norm does is it reduces the
 

00:05:05.470 --> 00:05:07.860
what that Norm does is it reduces the
amount that the distribution of these

00:05:07.860 --> 00:05:07.870
amount that the distribution of these
 

00:05:07.870 --> 00:05:11.220
amount that the distribution of these
hidden unit values shifts around and if

00:05:11.220 --> 00:05:11.230
hidden unit values shifts around and if
 

00:05:11.230 --> 00:05:13.440
hidden unit values shifts around and if
it were to plot the distribution of

00:05:13.440 --> 00:05:13.450
it were to plot the distribution of
 

00:05:13.450 --> 00:05:15.980
it were to plot the distribution of
these hidden unit values maybe this is

00:05:15.980 --> 00:05:15.990
these hidden unit values maybe this is
 

00:05:15.990 --> 00:05:18.600
these hidden unit values maybe this is
technically renormalized as V so this is

00:05:18.600 --> 00:05:18.610
technically renormalized as V so this is
 

00:05:18.610 --> 00:05:25.890
technically renormalized as V so this is
actually V 2 1 and V 2 2 and we're going

00:05:25.890 --> 00:05:25.900
actually V 2 1 and V 2 2 and we're going
 

00:05:25.900 --> 00:05:28.170
actually V 2 1 and V 2 2 and we're going
to values into the full values so we can

00:05:28.170 --> 00:05:28.180
to values into the full values so we can
 

00:05:28.180 --> 00:05:31.170
to values into the full values so we can
visualize in 2d what - mom is saying is

00:05:31.170 --> 00:05:31.180
visualize in 2d what - mom is saying is
 

00:05:31.180 --> 00:05:35.010
visualize in 2d what - mom is saying is
that the values of V 2 1 and V 2 2 can

00:05:35.010 --> 00:05:35.020
that the values of V 2 1 and V 2 2 can
 

00:05:35.020 --> 00:05:37.260
that the values of V 2 1 and V 2 2 can
change and indeed they won't change when

00:05:37.260 --> 00:05:37.270
change and indeed they won't change when
 

00:05:37.270 --> 00:05:38.940
change and indeed they won't change when
the neural network updates the

00:05:38.940 --> 00:05:38.950
the neural network updates the
 

00:05:38.950 --> 00:05:41.520
the neural network updates the
parameters in the earlier layers but

00:05:41.520 --> 00:05:41.530
parameters in the earlier layers but
 

00:05:41.530 --> 00:05:43.500
parameters in the earlier layers but
what - column ensures is that no matter

00:05:43.500 --> 00:05:43.510
what - column ensures is that no matter
 

00:05:43.510 --> 00:05:46.730
what - column ensures is that no matter
how it changes the mean and variance of

00:05:46.730 --> 00:05:46.740
how it changes the mean and variance of
 

00:05:46.740 --> 00:05:54.600
how it changes the mean and variance of
Z 2 1 and V 2 2 will remain the same so

00:05:54.600 --> 00:05:54.610
Z 2 1 and V 2 2 will remain the same so
 

00:05:54.610 --> 00:05:58.020
Z 2 1 and V 2 2 will remain the same so
so even with the exact values of V 2 1

00:05:58.020 --> 00:05:58.030
so even with the exact values of V 2 1
 

00:05:58.030 --> 00:06:00.810
so even with the exact values of V 2 1
and V 2 to change their mean and

00:06:00.810 --> 00:06:00.820
and V 2 to change their mean and
 

00:06:00.820 --> 00:06:04.680
and V 2 to change their mean and
variance while these states say mean 0

00:06:04.680 --> 00:06:04.690
variance while these states say mean 0
 

00:06:04.690 --> 00:06:08.850
variance while these states say mean 0
and variance 1 or not necessarily mean 0

00:06:08.850 --> 00:06:08.860
and variance 1 or not necessarily mean 0
 

00:06:08.860 --> 00:06:11.220
and variance 1 or not necessarily mean 0
and variance 1 but whatever value is

00:06:11.220 --> 00:06:11.230
and variance 1 but whatever value is
 

00:06:11.230 --> 00:06:17.970
and variance 1 but whatever value is
governed by beta 2 and gamma 2 which is

00:06:17.970 --> 00:06:17.980
governed by beta 2 and gamma 2 which is
 

00:06:17.980 --> 00:06:19.980
governed by beta 2 and gamma 2 which is
in your networks choosers can force it

00:06:19.980 --> 00:06:19.990
in your networks choosers can force it
 

00:06:19.990 --> 00:06:22.710
in your networks choosers can force it
to be mean 0 and variance 1 or really

00:06:22.710 --> 00:06:22.720
to be mean 0 and variance 1 or really
 

00:06:22.720 --> 00:06:25.230
to be mean 0 and variance 1 or really
any other news experience but what this

00:06:25.230 --> 00:06:25.240
any other news experience but what this
 

00:06:25.240 --> 00:06:28.710
any other news experience but what this
does is it limits the amount to which

00:06:28.710 --> 00:06:28.720
does is it limits the amount to which
 

00:06:28.720 --> 00:06:31.020
does is it limits the amount to which
updating the parameters in the earlier

00:06:31.020 --> 00:06:31.030
updating the parameters in the earlier
 

00:06:31.030 --> 00:06:33.780
updating the parameters in the earlier
layers can affect the distribution of

00:06:33.780 --> 00:06:33.790
layers can affect the distribution of
 

00:06:33.790 --> 00:06:37.010
layers can affect the distribution of
values that the 3rd layer now sees

00:06:37.010 --> 00:06:37.020
values that the 3rd layer now sees
 

00:06:37.020 --> 00:06:39.140
values that the 3rd layer now sees
therefore that has to learn on and so

00:06:39.140 --> 00:06:39.150
therefore that has to learn on and so
 

00:06:39.150 --> 00:06:42.470
therefore that has to learn on and so
bashed on reduces the problem of the

00:06:42.470 --> 00:06:42.480
bashed on reduces the problem of the
 

00:06:42.480 --> 00:06:45.320
bashed on reduces the problem of the
input values changing it really causes

00:06:45.320 --> 00:06:45.330
input values changing it really causes
 

00:06:45.330 --> 00:06:49.100
input values changing it really causes
these values to become more stable so

00:06:49.100 --> 00:06:49.110
these values to become more stable so
 

00:06:49.110 --> 00:06:51.500
these values to become more stable so
that the later layers of the neural

00:06:51.500 --> 00:06:51.510
that the later layers of the neural
 

00:06:51.510 --> 00:06:54.590
that the later layers of the neural
network has more firm ground to stand on

00:06:54.590 --> 00:06:54.600
network has more firm ground to stand on
 

00:06:54.600 --> 00:06:56.840
network has more firm ground to stand on
and even though the input distribution

00:06:56.840 --> 00:06:56.850
and even though the input distribution
 

00:06:56.850 --> 00:06:59.450
and even though the input distribution
changes a bit it changes less and what

00:06:59.450 --> 00:06:59.460
changes a bit it changes less and what
 

00:06:59.460 --> 00:07:02.060
changes a bit it changes less and what
this does is even as the earlier layers

00:07:02.060 --> 00:07:02.070
this does is even as the earlier layers
 

00:07:02.070 --> 00:07:04.670
this does is even as the earlier layers
keep learning the amount that this

00:07:04.670 --> 00:07:04.680
keep learning the amount that this
 

00:07:04.680 --> 00:07:07.250
keep learning the amount that this
forces the later layers to adapt to the

00:07:07.250 --> 00:07:07.260
forces the later layers to adapt to the
 

00:07:07.260 --> 00:07:10.220
forces the later layers to adapt to the
earliest layers changes is reduced or if

00:07:10.220 --> 00:07:10.230
earliest layers changes is reduced or if
 

00:07:10.230 --> 00:07:12.400
earliest layers changes is reduced or if
you will it weakens the coupling between

00:07:12.400 --> 00:07:12.410
you will it weakens the coupling between
 

00:07:12.410 --> 00:07:14.960
you will it weakens the coupling between
what the earlier layers parameters have

00:07:14.960 --> 00:07:14.970
what the earlier layers parameters have
 

00:07:14.970 --> 00:07:16.490
what the earlier layers parameters have
to do and what the later layers

00:07:16.490 --> 00:07:16.500
to do and what the later layers
 

00:07:16.500 --> 00:07:18.740
to do and what the later layers
parameters have to do and so it allows

00:07:18.740 --> 00:07:18.750
parameters have to do and so it allows
 

00:07:18.750 --> 00:07:21.470
parameters have to do and so it allows
each layer of the network to learn by

00:07:21.470 --> 00:07:21.480
each layer of the network to learn by
 

00:07:21.480 --> 00:07:23.030
each layer of the network to learn by
itself you know a little bit more

00:07:23.030 --> 00:07:23.040
itself you know a little bit more
 

00:07:23.040 --> 00:07:25.670
itself you know a little bit more
independently of other layers and this

00:07:25.670 --> 00:07:25.680
independently of other layers and this
 

00:07:25.680 --> 00:07:27.710
independently of other layers and this
is the effect of speeding up learning in

00:07:27.710 --> 00:07:27.720
is the effect of speeding up learning in
 

00:07:27.720 --> 00:07:30.350
is the effect of speeding up learning in
the whole network so I hope this gives

00:07:30.350 --> 00:07:30.360
the whole network so I hope this gives
 

00:07:30.360 --> 00:07:32.840
the whole network so I hope this gives
some better intuition but the takeaway

00:07:32.840 --> 00:07:32.850
some better intuition but the takeaway
 

00:07:32.850 --> 00:07:36.110
some better intuition but the takeaway
is that - norm means that especially

00:07:36.110 --> 00:07:36.120
is that - norm means that especially
 

00:07:36.120 --> 00:07:37.790
is that - norm means that especially
from the perspective of one of the later

00:07:37.790 --> 00:07:37.800
from the perspective of one of the later
 

00:07:37.800 --> 00:07:40.340
from the perspective of one of the later
layers of the neural network the earlier

00:07:40.340 --> 00:07:40.350
layers of the neural network the earlier
 

00:07:40.350 --> 00:07:42.560
layers of the neural network the earlier
layers don't get to shift around as much

00:07:42.560 --> 00:07:42.570
layers don't get to shift around as much
 

00:07:42.570 --> 00:07:44.720
layers don't get to shift around as much
because they're constrained to have the

00:07:44.720 --> 00:07:44.730
because they're constrained to have the
 

00:07:44.730 --> 00:07:47.090
because they're constrained to have the
same mean and variance and so this makes

00:07:47.090 --> 00:07:47.100
same mean and variance and so this makes
 

00:07:47.100 --> 00:07:48.950
same mean and variance and so this makes
the job of learning the later layers

00:07:48.950 --> 00:07:48.960
the job of learning the later layers
 

00:07:48.960 --> 00:07:51.470
the job of learning the later layers
easier in terms of batch table has a

00:07:51.470 --> 00:07:51.480
easier in terms of batch table has a
 

00:07:51.480 --> 00:07:54.500
easier in terms of batch table has a
second effect as a slight regularization

00:07:54.500 --> 00:07:54.510
second effect as a slight regularization
 

00:07:54.510 --> 00:07:57.680
second effect as a slight regularization
impact so one known into the thing about

00:07:57.680 --> 00:07:57.690
impact so one known into the thing about
 

00:07:57.690 --> 00:08:00.140
impact so one known into the thing about
passional is that each mini batch will

00:08:00.140 --> 00:08:00.150
passional is that each mini batch will
 

00:08:00.150 --> 00:08:04.940
passional is that each mini batch will
say mini batch xt has the values VT has

00:08:04.940 --> 00:08:04.950
say mini batch xt has the values VT has
 

00:08:04.950 --> 00:08:09.710
say mini batch xt has the values VT has
the values VL scaled by the meaning

00:08:09.710 --> 00:08:09.720
the values VL scaled by the meaning
 

00:08:09.720 --> 00:08:12.320
the values VL scaled by the meaning
variance computed on just that one mini

00:08:12.320 --> 00:08:12.330
variance computed on just that one mini
 

00:08:12.330 --> 00:08:14.840
variance computed on just that one mini
batch now because the mean and variance

00:08:14.840 --> 00:08:14.850
batch now because the mean and variance
 

00:08:14.850 --> 00:08:17.090
batch now because the mean and variance
computed on just that mini batch as

00:08:17.090 --> 00:08:17.100
computed on just that mini batch as
 

00:08:17.100 --> 00:08:19.250
computed on just that mini batch as
opposed to computed on the entire

00:08:19.250 --> 00:08:19.260
opposed to computed on the entire
 

00:08:19.260 --> 00:08:21.800
opposed to computed on the entire
dataset that mean and variance has a

00:08:21.800 --> 00:08:21.810
dataset that mean and variance has a
 

00:08:21.810 --> 00:08:23.360
dataset that mean and variance has a
little bit of noise in it because this

00:08:23.360 --> 00:08:23.370
little bit of noise in it because this
 

00:08:23.370 --> 00:08:25.810
little bit of noise in it because this
computer just on your mini batch of say

00:08:25.810 --> 00:08:25.820
computer just on your mini batch of say
 

00:08:25.820 --> 00:08:30.740
computer just on your mini batch of say
64 or 128 or maybe 256 or larger

00:08:30.740 --> 00:08:30.750
64 or 128 or maybe 256 or larger
 

00:08:30.750 --> 00:08:31.880
64 or 128 or maybe 256 or larger
training examples

00:08:31.880 --> 00:08:31.890
training examples
 

00:08:31.890 --> 00:08:33.980
training examples
so because the mean and variance has

00:08:33.980 --> 00:08:33.990
so because the mean and variance has
 

00:08:33.990 --> 00:08:35.570
so because the mean and variance has
will be noisy because it's estimated

00:08:35.570 --> 00:08:35.580
will be noisy because it's estimated
 

00:08:35.580 --> 00:08:37.840
will be noisy because it's estimated
with just a relatively small sample data

00:08:37.840 --> 00:08:37.850
with just a relatively small sample data
 

00:08:37.850 --> 00:08:42.320
with just a relatively small sample data
the scaling process going from VL to z

00:08:42.320 --> 00:08:42.330
the scaling process going from VL to z
 

00:08:42.330 --> 00:08:45.650
the scaling process going from VL to z
to their L that process is broken noisy

00:08:45.650 --> 00:08:45.660
to their L that process is broken noisy
 

00:08:45.660 --> 00:08:48.020
to their L that process is broken noisy
as well because it's computed using a

00:08:48.020 --> 00:08:48.030
as well because it's computed using a
 

00:08:48.030 --> 00:08:49.940
as well because it's computed using a
slightly noisy mean

00:08:49.940 --> 00:08:49.950
slightly noisy mean
 

00:08:49.950 --> 00:08:55.100
slightly noisy mean
Darian's so similar to drop out and add

00:08:55.100 --> 00:08:55.110
Darian's so similar to drop out and add
 

00:08:55.110 --> 00:08:56.660
Darian's so similar to drop out and add
some noise to in there's

00:08:56.660 --> 00:08:56.670
some noise to in there's
 

00:08:56.670 --> 00:08:59.810
some noise to in there's
activations the way drop out as noises

00:08:59.810 --> 00:08:59.820
activations the way drop out as noises
 

00:08:59.820 --> 00:09:01.550
activations the way drop out as noises
it takes a hidden unit and it multiplies

00:09:01.550 --> 00:09:01.560
it takes a hidden unit and it multiplies
 

00:09:01.560 --> 00:09:04.100
it takes a hidden unit and it multiplies
it by zero with some probability and

00:09:04.100 --> 00:09:04.110
it by zero with some probability and
 

00:09:04.110 --> 00:09:05.810
it by zero with some probability and
multiplies it by one with some

00:09:05.810 --> 00:09:05.820
multiplies it by one with some
 

00:09:05.820 --> 00:09:07.910
multiplies it by one with some
probability and so dropout has

00:09:07.910 --> 00:09:07.920
probability and so dropout has
 

00:09:07.920 --> 00:09:09.830
probability and so dropout has
multiplicative noise because it's

00:09:09.830 --> 00:09:09.840
multiplicative noise because it's
 

00:09:09.840 --> 00:09:14.540
multiplicative noise because it's
multiplying by 0 1 whereas - norm has

00:09:14.540 --> 00:09:14.550
multiplying by 0 1 whereas - norm has
 

00:09:14.550 --> 00:09:17.000
multiplying by 0 1 whereas - norm has
multiple if noise because of scaling by

00:09:17.000 --> 00:09:17.010
multiple if noise because of scaling by
 

00:09:17.010 --> 00:09:18.920
multiple if noise because of scaling by
the standard deviation as well as

00:09:18.920 --> 00:09:18.930
the standard deviation as well as
 

00:09:18.930 --> 00:09:20.810
the standard deviation as well as
hazardous noise because it's subtracting

00:09:20.810 --> 00:09:20.820
hazardous noise because it's subtracting
 

00:09:20.820 --> 00:09:23.300
hazardous noise because it's subtracting
the mean we're here the estimates of the

00:09:23.300 --> 00:09:23.310
the mean we're here the estimates of the
 

00:09:23.310 --> 00:09:25.460
the mean we're here the estimates of the
mean and the standard deviation on noisy

00:09:25.460 --> 00:09:25.470
mean and the standard deviation on noisy
 

00:09:25.470 --> 00:09:30.200
mean and the standard deviation on noisy
and so similar to drop out bashed on

00:09:30.200 --> 00:09:30.210
and so similar to drop out bashed on
 

00:09:30.210 --> 00:09:31.850
and so similar to drop out bashed on
therefore has a slight regularization

00:09:31.850 --> 00:09:31.860
therefore has a slight regularization
 

00:09:31.860 --> 00:09:34.610
therefore has a slight regularization
effect because by adding noise to the

00:09:34.610 --> 00:09:34.620
effect because by adding noise to the
 

00:09:34.620 --> 00:09:37.610
effect because by adding noise to the
hidden unit is forcing the downstream

00:09:37.610 --> 00:09:37.620
hidden unit is forcing the downstream
 

00:09:37.620 --> 00:09:39.740
hidden unit is forcing the downstream
pending units not to rely too much on

00:09:39.740 --> 00:09:39.750
pending units not to rely too much on
 

00:09:39.750 --> 00:09:42.350
pending units not to rely too much on
any one hidden unit and so similar to

00:09:42.350 --> 00:09:42.360
any one hidden unit and so similar to
 

00:09:42.360 --> 00:09:44.630
any one hidden unit and so similar to
drop out this as noise of hidden layers

00:09:44.630 --> 00:09:44.640
drop out this as noise of hidden layers
 

00:09:44.640 --> 00:09:46.370
drop out this as noise of hidden layers
and therefore has a very slight

00:09:46.370 --> 00:09:46.380
and therefore has a very slight
 

00:09:46.380 --> 00:09:48.650
and therefore has a very slight
regularization effect because the noise

00:09:48.650 --> 00:09:48.660
regularization effect because the noise
 

00:09:48.660 --> 00:09:51.290
regularization effect because the noise
added is quite small this is not a huge

00:09:51.290 --> 00:09:51.300
added is quite small this is not a huge
 

00:09:51.300 --> 00:09:53.030
added is quite small this is not a huge
regularization effect and you might

00:09:53.030 --> 00:09:53.040
regularization effect and you might
 

00:09:53.040 --> 00:09:55.490
regularization effect and you might
choose to use national together with

00:09:55.490 --> 00:09:55.500
choose to use national together with
 

00:09:55.500 --> 00:09:58.340
choose to use national together with
dropouts and you might use bash norm

00:09:58.340 --> 00:09:58.350
dropouts and you might use bash norm
 

00:09:58.350 --> 00:10:00.440
dropouts and you might use bash norm
together with dropouts if you want the

00:10:00.440 --> 00:10:00.450
together with dropouts if you want the
 

00:10:00.450 --> 00:10:02.240
together with dropouts if you want the
more powerful regularization effective

00:10:02.240 --> 00:10:02.250
more powerful regularization effective
 

00:10:02.250 --> 00:10:04.430
more powerful regularization effective
dropout and maybe one other slightly

00:10:04.430 --> 00:10:04.440
dropout and maybe one other slightly
 

00:10:04.440 --> 00:10:06.590
dropout and maybe one other slightly
non-intuitive effect is that if you use

00:10:06.590 --> 00:10:06.600
non-intuitive effect is that if you use
 

00:10:06.600 --> 00:10:09.170
non-intuitive effect is that if you use
a bigger mini-batch size right so if you

00:10:09.170 --> 00:10:09.180
a bigger mini-batch size right so if you
 

00:10:09.180 --> 00:10:12.500
a bigger mini-batch size right so if you
use a mini batch size of say 512 instead

00:10:12.500 --> 00:10:12.510
use a mini batch size of say 512 instead
 

00:10:12.510 --> 00:10:15.710
use a mini batch size of say 512 instead
of 64 by using a larger new batch size

00:10:15.710 --> 00:10:15.720
of 64 by using a larger new batch size
 

00:10:15.720 --> 00:10:17.900
of 64 by using a larger new batch size
you're reducing this noise and therefore

00:10:17.900 --> 00:10:17.910
you're reducing this noise and therefore
 

00:10:17.910 --> 00:10:20.510
you're reducing this noise and therefore
also reducing this regularization effect

00:10:20.510 --> 00:10:20.520
also reducing this regularization effect
 

00:10:20.520 --> 00:10:23.090
also reducing this regularization effect
so that's one strange property of

00:10:23.090 --> 00:10:23.100
so that's one strange property of
 

00:10:23.100 --> 00:10:25.970
so that's one strange property of
dropout which is that by using a bigger

00:10:25.970 --> 00:10:25.980
dropout which is that by using a bigger
 

00:10:25.980 --> 00:10:28.340
dropout which is that by using a bigger
mini batch size you reduce the

00:10:28.340 --> 00:10:28.350
mini batch size you reduce the
 

00:10:28.350 --> 00:10:30.860
mini batch size you reduce the
regularization effect having said this I

00:10:30.860 --> 00:10:30.870
regularization effect having said this I
 

00:10:30.870 --> 00:10:32.720
regularization effect having said this I
wouldn't really use bash norm as a

00:10:32.720 --> 00:10:32.730
wouldn't really use bash norm as a
 

00:10:32.730 --> 00:10:33.680
wouldn't really use bash norm as a
regularizer

00:10:33.680 --> 00:10:33.690
regularizer
 

00:10:33.690 --> 00:10:36.110
regularizer
that's really not the intent of vaginal

00:10:36.110 --> 00:10:36.120
that's really not the intent of vaginal
 

00:10:36.120 --> 00:10:40.880
that's really not the intent of vaginal
but sometimes it has this extra intended

00:10:40.880 --> 00:10:40.890
but sometimes it has this extra intended
 

00:10:40.890 --> 00:10:43.100
but sometimes it has this extra intended
or unintended effect on your learning

00:10:43.100 --> 00:10:43.110
or unintended effect on your learning
 

00:10:43.110 --> 00:10:46.010
or unintended effect on your learning
algorithm but really don't turn to bash

00:10:46.010 --> 00:10:46.020
algorithm but really don't turn to bash
 

00:10:46.020 --> 00:10:47.660
algorithm but really don't turn to bash
norm as a regularization

00:10:47.660 --> 00:10:47.670
norm as a regularization
 

00:10:47.670 --> 00:10:50.510
norm as a regularization
use it as a way to normalize your hidden

00:10:50.510 --> 00:10:50.520
use it as a way to normalize your hidden
 

00:10:50.520 --> 00:10:52.850
use it as a way to normalize your hidden
units and additions in there for speed

00:10:52.850 --> 00:10:52.860
units and additions in there for speed
 

00:10:52.860 --> 00:10:54.320
units and additions in there for speed
of learning and I think the

00:10:54.320 --> 00:10:54.330
of learning and I think the
 

00:10:54.330 --> 00:10:56.090
of learning and I think the
regularization is an almost zero

00:10:56.090 --> 00:10:56.100
regularization is an almost zero
 

00:10:56.100 --> 00:10:56.910
regularization is an almost zero
unintended

00:10:56.910 --> 00:10:56.920
unintended
 

00:10:56.920 --> 00:10:59.069
unintended
side effect so I hope that gives you

00:10:59.069 --> 00:10:59.079
side effect so I hope that gives you
 

00:10:59.079 --> 00:11:01.560
side effect so I hope that gives you
better intuition about what batch norm

00:11:01.560 --> 00:11:01.570
better intuition about what batch norm
 

00:11:01.570 --> 00:11:03.420
better intuition about what batch norm
is doing before we wrap up the

00:11:03.420 --> 00:11:03.430
is doing before we wrap up the
 

00:11:03.430 --> 00:11:05.009
is doing before we wrap up the
discussion on batch alarm there's one

00:11:05.009 --> 00:11:05.019
discussion on batch alarm there's one
 

00:11:05.019 --> 00:11:06.720
discussion on batch alarm there's one
more detail I want to make sure you know

00:11:06.720 --> 00:11:06.730
more detail I want to make sure you know
 

00:11:06.730 --> 00:11:09.509
more detail I want to make sure you know
which is that batch norm handles data

00:11:09.509 --> 00:11:09.519
which is that batch norm handles data
 

00:11:09.519 --> 00:11:11.730
which is that batch norm handles data
one mini batch at a time it computes

00:11:11.730 --> 00:11:11.740
one mini batch at a time it computes
 

00:11:11.740 --> 00:11:14.970
one mini batch at a time it computes
mean and variances on mini batches so at

00:11:14.970 --> 00:11:14.980
mean and variances on mini batches so at
 

00:11:14.980 --> 00:11:16.170
mean and variances on mini batches so at
test time we're trying to make

00:11:16.170 --> 00:11:16.180
test time we're trying to make
 

00:11:16.180 --> 00:11:17.550
test time we're trying to make
predictions johnny valued in your

00:11:17.550 --> 00:11:17.560
predictions johnny valued in your
 

00:11:17.560 --> 00:11:19.620
predictions johnny valued in your
network you might not have a mini batch

00:11:19.620 --> 00:11:19.630
network you might not have a mini batch
 

00:11:19.630 --> 00:11:21.930
network you might not have a mini batch
of examples you might be processing one

00:11:21.930 --> 00:11:21.940
of examples you might be processing one
 

00:11:21.940 --> 00:11:24.810
of examples you might be processing one
single example at a time so at test time

00:11:24.810 --> 00:11:24.820
single example at a time so at test time
 

00:11:24.820 --> 00:11:26.699
single example at a time so at test time
you need to do something slightly

00:11:26.699 --> 00:11:26.709
you need to do something slightly
 

00:11:26.709 --> 00:11:28.019
you need to do something slightly
differently to make sure your

00:11:28.019 --> 00:11:28.029
differently to make sure your
 

00:11:28.029 --> 00:11:30.329
differently to make sure your
predictions make sense let in the next

00:11:30.329 --> 00:11:30.339
predictions make sense let in the next
 

00:11:30.339 --> 00:11:32.460
predictions make sense let in the next
and final video on vegetable let's talk

00:11:32.460 --> 00:11:32.470
and final video on vegetable let's talk
 

00:11:32.470 --> 00:11:34.290
and final video on vegetable let's talk
over the details of what you need to do

00:11:34.290 --> 00:11:34.300
over the details of what you need to do
 

00:11:34.300 --> 00:11:36.210
over the details of what you need to do
in order to taste you in your network

00:11:36.210 --> 00:11:36.220
in order to taste you in your network
 

00:11:36.220 --> 00:11:40.860
in order to taste you in your network
train using national to make predictions

