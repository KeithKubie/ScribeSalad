WEBVTT

1
00:00:00.150 --> 00:00:00.890
Now,

2
00:00:00.890 --> 00:00:05.890
the overwhelming narrative is that the Russians were very much invested in

3
00:00:07.951 --> 00:00:10.310
having Trump win.
Right.

4
00:00:10.370 --> 00:00:15.050
And if they were very much invested in having Trump win was the reason why they

5
00:00:15.051 --> 00:00:18.650
focus so heavily on the African American community because the half African

6
00:00:18.650 --> 00:00:21.320
American community traditionally seems to vote Democrat.

7
00:00:21.650 --> 00:00:26.450
So they were trying to do something to break that up or try to do something to

8
00:00:26.720 --> 00:00:29.720
weaken the position of the,
the,
you know,

9
00:00:29.721 --> 00:00:33.800
the incumbent or a Hilary Clinton and maybe put some emphasis on Jill Stein or

10
00:00:33.801 --> 00:00:35.240
some alternative candidates.

11
00:00:35.760 --> 00:00:40.290
<v 1>Yeah.
So the,
the way that the political campaign,</v>

12
00:00:40.291 --> 00:00:42.570
the political aspect of it played out.
So they establish,

13
00:00:42.630 --> 00:00:46.680
they started building these relationships in 2015 and yeah,
they're doing this,

14
00:00:46.681 --> 00:00:49.650
this tribal thing.
We've got our ingroup are part of this community.

15
00:00:50.010 --> 00:00:54.000
And then what you start to see them do is,
um,
early,
they're actually,

16
00:00:54.001 --> 00:00:54.900
there was a tiny,

17
00:00:54.901 --> 00:00:59.901
tiny cluster in the early primaries where they were supporting rand Paul and

18
00:01:00.690 --> 00:01:04.260
then they pivot to Trump pretty quickly and use and probably ran.

19
00:01:04.261 --> 00:01:08.010
Paul just didn't pull well and they were like,
there's no way to get a lift here.

20
00:01:08.011 --> 00:01:12.990
But,
but maybe Trump was getting,
you know,
some actual left in the media.
And so,

21
00:01:13.320 --> 00:01:16.860
so you see them move into supporting Trump and then for the remainder of the

22
00:01:16.861 --> 00:01:20.910
Dataset for from 2015 through the end,
which was mid 2017 or so,

23
00:01:20.911 --> 00:01:25.200
is when this thing ends.
Um,
it's,
it's adamantly pro Trump on the right.

24
00:01:25.710 --> 00:01:28.350
And on the right you see not only pro Trump,

25
00:01:28.351 --> 00:01:33.351
but you see them really working to like a road support for mainstream or

26
00:01:33.810 --> 00:01:36.390
traditional Republicans,
traditional conservatives.

27
00:01:36.391 --> 00:01:38.850
You see a lot of the memes about like,
um,

28
00:01:38.910 --> 00:01:41.820
are you with the Conservatives or the Konservatives in there,
you know,

29
00:01:41.821 --> 00:01:43.860
and so the Conservatives of course are like,

30
00:01:44.160 --> 00:01:47.040
they've got pictures of Lake Lindsey Graham and John Mccain,

31
00:01:47.041 --> 00:01:51.700
the hate John Mccain.
John Mccain shows up a million times.
Um,
and,
uh,

32
00:01:51.810 --> 00:01:55.260
Marco Rubio,
Ted Cruz.
Well,
I think that they,
you know,

33
00:01:55.800 --> 00:02:00.040
one of the theories is,
um,
and,
and I believe this is probably true.
They,

34
00:02:00.041 --> 00:02:04.200
they really strongly disliked Hillary Clinton because there was concerned that

35
00:02:04.201 --> 00:02:04.861
she would,
you know,

36
00:02:04.861 --> 00:02:07.740
things that she was saying about increasing freedoms in Russia were very

37
00:02:07.741 --> 00:02:12.450
threatening.
Uh,
they thought the best bet to get sanctions removed was Trump.
Um,

38
00:02:12.451 --> 00:02:17.390
so they had specific outcomes that they were hoping for.
And that was one of,

39
00:02:17.410 --> 00:02:19.350
you know,
so there's always like a political motivation.

40
00:02:19.560 --> 00:02:23.040
So there is this narrative around they just want to kind of like screw the

41
00:02:23.041 --> 00:02:25.890
American society,
create divisions,
imply divisions.

42
00:02:26.100 --> 00:02:29.010
When you look at the political content,
the clear and,

43
00:02:29.310 --> 00:02:33.540
and sustained support for Trump.
And even more than that,

44
00:02:33.541 --> 00:02:38.040
the clear disdain for Hillary Clinton.
There is not on Facebook and Instagram,

45
00:02:38.041 --> 00:02:41.550
there was not one single pro Hillary post.

46
00:02:42.000 --> 00:02:45.000
There were some anti Trump posts.
Because if you're running an LGBT page,

47
00:02:45.001 --> 00:02:48.430
of course they're going to say negative things about Trump,
you know,
and,

48
00:02:48.431 --> 00:02:52.710
and they're saying it.
So you should vote for Jill Stein.
There was early support,

49
00:02:52.820 --> 00:02:55.410
um,
and some of the left landing pages for Bernie Sanders,

50
00:02:55.620 --> 00:03:00.010
but you actually see the support for Bernie Sanders come in more after

51
00:03:00.010 --> 00:03:02.020
<v 0>it becomes clear that he's not going to win,</v>

52
00:03:02.620 --> 00:03:06.850
because then they're using Bernie Sanders as a way to say this was stolen from

53
00:03:06.851 --> 00:03:11.350
him by the evil Clintons.
Um,
or Jill Stein.
You know,
here's a true,

54
00:03:11.410 --> 00:03:13.810
a true independent,
real liberal,
uh,

55
00:03:13.840 --> 00:03:16.030
we should be voting for her if we want to support a woman.

56
00:03:16.031 --> 00:03:19.510
So there are these feminism pages really pushing this narrative of Jill Stein.

57
00:03:19.840 --> 00:03:23.350
So you have the left leaning pages,
totally anti Clinton,

58
00:03:23.560 --> 00:03:26.620
and then you have the right landing pages,
staunchly pro Trump,

59
00:03:26.621 --> 00:03:31.510
and also strongly anti cruise,
Anti Rubio,
a anti Lindsey Graham,

60
00:03:31.511 --> 00:03:35.800
basically ante every now what's called establishment Republican.

61
00:03:36.280 --> 00:03:38.260
Um,
and there's this,
uh,

62
00:03:38.320 --> 00:03:43.240
kind of pushing of people two opposite opposite ends of the political spectrum.

63
00:03:43.450 --> 00:03:47.290
So this is where you get at the conversation around facilitating polarization.

64
00:03:47.650 --> 00:03:50.920
So not just,
um,
it,

65
00:03:51.020 --> 00:03:53.380
it wasn't enough to just support Donald Trump.

66
00:03:53.381 --> 00:03:58.360
It was also necessary to strongly disparage,
um,

67
00:03:58.510 --> 00:04:02.110
the kind of traditional conservative,

68
00:04:02.111 --> 00:04:06.730
moderate center right in the course of amplifying the Trump candidacy.

69
00:04:07.300 --> 00:04:10.390
Does that make sense?
Yes.
And it was a lot of stuff.
Yeah.
I use a lot of stuff,

70
00:04:10.391 --> 00:04:11.530
but it does make sense.

71
00:04:11.531 --> 00:04:15.580
And one of the things that was really bizarre to me watching the election and I

72
00:04:15.581 --> 00:04:20.140
was trying to figure out is this because Trump is so bombastic and he's so

73
00:04:20.141 --> 00:04:24.220
outrageous and he's just a different person that the way I was describing on

74
00:04:24.221 --> 00:04:28.630
stage was that like finally the assholes have a king because they never had a

75
00:04:28.631 --> 00:04:29.291
king before.

76
00:04:29.291 --> 00:04:34.291
Like everyone who was running for president was at least mostly dignified.

77
00:04:35.500 --> 00:04:39.400
I mean basically it's really difficult to go back in time and find someone who

78
00:04:39.401 --> 00:04:41.710
isn't,
find someone who,

79
00:04:42.190 --> 00:04:44.770
there's no one who insults people like he does.

80
00:04:45.160 --> 00:04:48.850
I mean he insult people's appearances.
He calls them losers.

81
00:04:49.180 --> 00:04:53.350
He called Stormy Daniels horse face.
I mean he said some outrageous shit.

82
00:04:53.560 --> 00:04:57.490
So part of it was me think of like,
wow,
maybe he's just ignited and emboldened.

83
00:04:57.550 --> 00:05:00.810
I actually had this conversation with my wife today.
She was like a I,

84
00:05:00.811 --> 00:05:04.780
it feels like racism is more prevalent.
Like it's more,

85
00:05:04.900 --> 00:05:09.820
it's more accepted.
People feel more emboldened because they're in their mind,

86
00:05:09.821 --> 00:05:13.870
they think he is racist.
I can get away with more things.
Trump is president.

87
00:05:14.140 --> 00:05:17.830
Like there's actually videos of people saying racist Shit and saying,
hey,

88
00:05:17.831 --> 00:05:21.430
Trump's president,
now we can do this.
So I was thinking that,

89
00:05:21.431 --> 00:05:22.460
well maybe that's what it was.

90
00:05:22.461 --> 00:05:27.310
It just sort of like some rare flower that only blooms under the right

91
00:05:27.311 --> 00:05:29.470
conditions.
Poof,
it's back.
Right?

92
00:05:29.980 --> 00:05:34.980
But when you think about the influence that these pages have had in establishing

93
00:05:36.131 --> 00:05:39.880
communities and this long game that they're playing,

94
00:05:40.240 --> 00:05:43.900
like the LGBT pages,
even though they're shitting on Trump,

95
00:05:44.290 --> 00:05:47.620
they really want to support Jill Stein because they know that'll actually help

96
00:05:47.621 --> 00:05:51.280
Trump because it'll take votes away from Hillary Clinton that they,

97
00:05:52.000 --> 00:05:57.000
it seems different like political discourse discussions online and social media,

98
00:05:58.821 --> 00:06:00.620
the way social media reacted.
I mean,

99
00:06:00.621 --> 00:06:04.220
there was a lot of people that were anti Obama,
uh,
before,
you know,

100
00:06:04.490 --> 00:06:08.780
either either of his elections that he won,
but it seemed different.

101
00:06:08.970 --> 00:06:12.230
It seemed different to me than this one.
This one seemed like,

102
00:06:13.160 --> 00:06:17.120
like we had moved into another level of hostility that never experienced before

103
00:06:17.121 --> 00:06:21.650
and another level of division between the right and the left that I'd never

104
00:06:21.651 --> 00:06:23.690
experienced before.
And,
uh,

105
00:06:23.691 --> 00:06:28.691
up like a willingness to engage with really harsh,

106
00:06:30.260 --> 00:06:34.190
nasty comments and just to dive into it,

107
00:06:34.191 --> 00:06:36.180
you would see it all day.
I mean there's,

108
00:06:36.181 --> 00:06:39.680
there were certain Twitter followers that I think they're pretty much human

109
00:06:39.681 --> 00:06:40.340
beings,

110
00:06:40.340 --> 00:06:45.340
but I would follow them and they would just be engaged with people all day long

111
00:06:45.801 --> 00:06:50.180
just shitting on people and criticizing this and insulting that.

112
00:06:50.550 --> 00:06:53.960
And it seemed like it seemed dangerous.

113
00:06:54.140 --> 00:06:57.860
It seemed like things had moved into a much more aggressive,

114
00:06:58.640 --> 00:07:03.640
much more hostile and confrontational sort of chapter in American history.

115
00:07:04.370 --> 00:07:09.020
If this is all done at the same time that is happening.

116
00:07:09.230 --> 00:07:14.230
How much of an influence do you think this IRA agency had on all this stuff?

117
00:07:16.750 --> 00:07:17.590
<v 1>That's the,
uh,</v>

118
00:07:17.591 --> 00:07:21.100
that's the question that we would all like the answer to and unfortunately can't

119
00:07:21.101 --> 00:07:25.630
give it.
And so in your mind though,
yeah,
let me,
let me,
let me Kinda caveat that.

120
00:07:25.660 --> 00:07:26.493
Um,

121
00:07:27.130 --> 00:07:32.020
the thing that we don't have that nobody who looks at this on the outside has,

122
00:07:32.021 --> 00:07:35.470
is we can't see what people said in response to this stuff.

123
00:07:35.860 --> 00:07:40.420
So I've looked at now almost 200,000 of these posts,

124
00:07:40.570 --> 00:07:44.320
um,
as what I spent most of last year doing was,
was this,
was this research,

125
00:07:44.980 --> 00:07:49.150
and we can see that they have thousands of engagements,

126
00:07:49.180 --> 00:07:51.430
thousands of comments,
thousands of shares.

127
00:07:51.790 --> 00:07:56.050
We have no idea what happened afterwards.
And that's the problem.

128
00:07:56.051 --> 00:07:58.750
So when once the stuff comes down,

129
00:07:58.751 --> 00:08:00.970
it's really hard to go back and piece it together.

130
00:08:01.240 --> 00:08:05.860
So I can see that there are some,
uh,
per your point,
the really,

131
00:08:05.861 --> 00:08:09.250
really just fucking horrible troll accounts that they ran.

132
00:08:09.670 --> 00:08:12.880
They didn't necessarily have a lot of followers,
but you see them in there,

133
00:08:12.881 --> 00:08:15.730
like adding people,
so they're,
you know,
at,

134
00:08:15.731 --> 00:08:18.610
and then the name of a reporter at the name of a prominent person.

135
00:08:18.611 --> 00:08:21.610
And so they're in their kind of like draft on the popularity of,
you know,

136
00:08:21.611 --> 00:08:23.890
famous people basically.
Um,

137
00:08:24.580 --> 00:08:28.480
and they're just saying like horrible shit and it's,

138
00:08:28.481 --> 00:08:30.100
the tone is so spot on.

139
00:08:30.101 --> 00:08:32.350
And one thing that was interesting with a couple of them is like if you go and

140
00:08:32.351 --> 00:08:35.500
you look at their profile information,
which was also made public,

141
00:08:35.860 --> 00:08:40.030
they would have like,
um,
they would have a,
uh,
like a gab account in their,

142
00:08:40.210 --> 00:08:42.310
in their profile that was like,
so they would,
um,

143
00:08:42.550 --> 00:08:45.940
so it was a remarkable piece of,
of,

144
00:08:46.270 --> 00:08:48.130
of kind of the culture in which you see that.

145
00:08:48.131 --> 00:08:50.680
Like they're actually sitting on Gab too,
right?

146
00:08:50.690 --> 00:08:54.130
And so they can also go and they can draw on there.
In reddit,
there's,
you know,

147
00:08:54.520 --> 00:08:57.900
900 something a troll accounts were found on reddit there on tumbler.

148
00:08:58.170 --> 00:09:03.170
And so they're just picking the most divisive content and they're pushing it out

149
00:09:03.900 --> 00:09:07.830
into communities.
And at the same time we can see that they're doing it,

150
00:09:07.831 --> 00:09:11.010
but we can't see what people do in return.
We can't say,
did they just block?

151
00:09:11.070 --> 00:09:14.460
Did they have the fight back?
Did,
was there a huge,
you know,

152
00:09:14.820 --> 00:09:17.480
when this happens on a Facebook page,
um,

153
00:09:18.360 --> 00:09:21.600
and they're doing something like telling black people not to vote,
um,

154
00:09:21.630 --> 00:09:26.280
as black people,
we shouldn't vote.
Um,
what do people say in response?

155
00:09:26.281 --> 00:09:29.220
And that's the piece that we don't have.
So when we talk about impact,

156
00:09:29.250 --> 00:09:34.250
a lot of the impact conversation is really focused on did this swing the

157
00:09:34.621 --> 00:09:38.400
election?
We don't have nothing that I've seen has the answer to that question.

158
00:09:38.820 --> 00:09:41.760
Um,
the other thing is,
but the second question,

159
00:09:41.761 --> 00:09:44.580
the thing when I think about impact,
I think from,
from,

160
00:09:44.610 --> 00:09:46.710
I think you and I agree on this,
um,

161
00:09:47.280 --> 00:09:49.980
it also matters how does this change how people relate to each other.

162
00:09:50.760 --> 00:09:55.530
And we have no real evidence of that with no information on that either.

163
00:09:55.531 --> 00:09:58.320
This is the kind of thing that lives in some,
you know,
Facebook has it,

164
00:09:58.410 --> 00:10:02.190
the rest of us haven't seen it.
No.
Are Most of these people,

165
00:10:02.191 --> 00:10:06.150
is this mostly Facebook?
Is it mostly Twitter or where,
what does,

166
00:10:06.151 --> 00:10:09.990
how does it break down?
Yeah,
so there were,
um,
here at my like little stats here,

167
00:10:09.991 --> 00:10:11.250
cause that will give you the wrong data.

168
00:10:11.640 --> 00:10:16.640
There were 10 half million tweets of which about 6 million were original content

169
00:10:16.710 --> 00:10:20.250
created by about 3,800 accounts.
Um,

170
00:10:20.790 --> 00:10:25.550
there were about 133,
I mean,
just read it.

171
00:10:25.551 --> 00:10:30.551
133 Instagram accounts with about 116,000 posts and then 81 Facebook pages and

172
00:10:32.971 --> 00:10:36.990
17 youtube channels with about 1100 videos.

173
00:10:37.290 --> 00:10:42.290
And so they got about 200 million engagements on Instagram and about another 75

174
00:10:42.300 --> 00:10:46.890
million or so on Facebook.
Um,
engagements are like,
likes,
shares,
comments,

175
00:10:46.920 --> 00:10:50.070
reactions,
you know,
um,
so

176
00:10:51.630 --> 00:10:55.200
it's hard to contextualize,
but what we think happened,
you know,

177
00:10:55.201 --> 00:10:57.660
you can go and you can try to look at,
um,

178
00:10:58.800 --> 00:11:03.060
how well did this content perform relative to other real authentic media

179
00:11:03.061 --> 00:11:04.350
targeting these communities.

180
00:11:04.890 --> 00:11:09.120
And what you see with the black community in particular is their Instagram game

181
00:11:09.121 --> 00:11:13.740
was really good.
Um,
they're,
so they're on their Instagram accounts,

182
00:11:13.770 --> 00:11:17.340
the,
you know,
the top five,
three of them targeted the black community and got,

183
00:11:17.490 --> 00:11:21.600
you know,
um,
tends to hundred millions of engagements.

184
00:11:21.750 --> 00:11:26.430
So I'd have to pull up the exact numbers off the top of my head.
Yeah.
It's,

185
00:11:26.431 --> 00:11:30.630
um,
it's on Instagram.
It's all memes.
And then you know,

186
00:11:30.750 --> 00:11:33.600
so we have the memes and then we have the text on Instagram.

187
00:11:33.600 --> 00:11:34.890
You can't really share.

188
00:11:34.920 --> 00:11:37.890
So it's amazing that they got the kind of engagement that they did.

189
00:11:38.910 --> 00:11:40.560
Even without the sharing function.

190
00:11:41.070 --> 00:11:44.860
One of the things you can do is if you know the names of the accounts in there,

191
00:11:45.330 --> 00:11:47.610
a lot of them are out there publicly now.
Um,

192
00:11:47.670 --> 00:11:51.720
you can actually see them in regram apps.
So,
uh,

193
00:11:52.080 --> 00:11:55.870
people were regramming the content.
So Facebook says about 20 million people,

194
00:11:56.590 --> 00:11:58.300
excuse me,
engaged with the Instagram content,

195
00:11:59.020 --> 00:12:03.100
but what isn't included in that is all of the re grams of the content that were

196
00:12:03.101 --> 00:12:04.600
shared by other accounts.

197
00:12:05.200 --> 00:12:09.430
So the spread in the dispersion of this,

198
00:12:09.460 --> 00:12:11.770
it's an interesting,
uh,

199
00:12:11.830 --> 00:12:15.520
an interesting thing to try to quantify because we have engagement data,

200
00:12:15.880 --> 00:12:18.520
but we don't know,
did it change hearts and minds?

201
00:12:18.521 --> 00:12:22.810
We don't know if it influenced people to go follow other accounts.

202
00:12:22.811 --> 00:12:26.080
We don't know if it influenced people to not vote.
Uh,

203
00:12:26.081 --> 00:12:29.290
there's just so much more,
I think still to,

204
00:12:29.490 --> 00:12:32.140
to understand about how these operations work.
Kind of,

205
00:12:32.530 --> 00:12:35.530
we can assume that it had some impact,
right?
I mean,
yes,
as,

206
00:12:35.540 --> 00:12:36.640
as you were saying earlier,

207
00:12:36.641 --> 00:12:40.660
when a new person enters into a conversation that it changes the tone of it.

208
00:12:40.930 --> 00:12:45.280
How much of what they did was their own original posts and how much of it was

209
00:12:45.281 --> 00:12:47.170
commenting on other people's posts.

210
00:12:47.440 --> 00:12:52.200
So I thought you were actually going to ask a different thing there.
Yeah.

211
00:12:52.390 --> 00:12:54.700
How much of it was them repurposing our own posts,
right.

212
00:12:54.701 --> 00:12:58.690
Repurposing real American content.
Um,
did they do that as well?
Yeah,

213
00:12:58.691 --> 00:13:01.240
tons of times.
But let me,
let me,
uh,

214
00:13:01.810 --> 00:13:05.650
so they created a lot of their own stuff,
particularly in the early days.

215
00:13:05.710 --> 00:13:07.630
And so you can actually read the Dataset.

216
00:13:07.690 --> 00:13:10.990
And one of the things when we started finding these posts,

217
00:13:11.050 --> 00:13:16.050
I was struck by how sometimes it read like Esl and then sometimes it red lake,

218
00:13:16.420 --> 00:13:20.380
perfect,
flawless,
um,
professional English.

219
00:13:20.710 --> 00:13:23.080
And then other times it read like normal English vernacular.

220
00:13:23.081 --> 00:13:24.400
Just the way that we would talk to each other.

221
00:13:25.030 --> 00:13:28.420
And I started digging into what that was.

222
00:13:29.020 --> 00:13:32.070
So when it was vernacular English,
when it was,
when it,

223
00:13:32.110 --> 00:13:35.620
when it read like fluent in American American English,
it was,
um,

224
00:13:35.710 --> 00:13:37.150
usually cribbed from somewhere else.

225
00:13:37.210 --> 00:13:40.330
So they would go and they would find a local news story from some obscure local

226
00:13:40.331 --> 00:13:44.200
paper and they would crib and then they would paste that.
And then,

227
00:13:44.230 --> 00:13:48.220
so the Facebook post would be that cribbed a sentence from that article and then

228
00:13:48.221 --> 00:13:51.430
their meme and maybe they would add a sentence underneath it to give it some

229
00:13:51.431 --> 00:13:55.360
kind of context or angle.
When they would write their own stuff,

230
00:13:55.420 --> 00:13:58.330
you would see the sloppiness.
That's where you could see,
um,

231
00:13:58.360 --> 00:14:02.230
subject verb agreements,
not quite there.
The,
you know,

232
00:14:02.620 --> 00:14:06.940
ways in which like Russian possessives are different than American possessives

233
00:14:06.941 --> 00:14:11.560
the slips there.
Um,
and then the other thing was the really funny stuff,

234
00:14:11.561 --> 00:14:13.530
which was,
um,
you know,

235
00:14:13.550 --> 00:14:16.930
a post that's supposed to supposedly written by Texas secessionist.
Right?

236
00:14:16.931 --> 00:14:20.050
So you can probably have an image of a Texas secessionists in your mind as I say

237
00:14:20.051 --> 00:14:22.240
this.
And it would be things like,
um,

238
00:14:23.440 --> 00:14:27.520
Hillary Clinton is a terrible individual and as a,
uh,

239
00:14:27.540 --> 00:14:28.720
as a terrible individual,

240
00:14:28.721 --> 00:14:32.770
it's completely impossible for us to back her in her candidacy for the American

241
00:14:32.771 --> 00:14:36.850
presidency.
Furthermore,
you know what I'm doing?
It's like furthermore air go,

242
00:14:36.890 --> 00:14:39.340
yeah,
it is clear that,
and I'm like,

243
00:14:39.550 --> 00:14:42.790
it reads like member you're like in English in college or something.

244
00:14:42.791 --> 00:14:47.250
You've got to like write a formal essay.
I was like,
okay,
come on.
Right.

245
00:14:47.260 --> 00:14:50.350
So nobody actually talks like this,
especially not,
you know,
your gear,

246
00:14:51.800 --> 00:14:54.920
Texas is action.
So it was funny seeing these,
um,

247
00:14:55.310 --> 00:15:00.230
incongruities and that's unfortunately one of the best ways to tell what you're

248
00:15:00.231 --> 00:15:04.300
dealing with is actually to kind of look for those incongruities now and see,
uh,

249
00:15:04.910 --> 00:15:09.350
as you read communications online,
like,
can,
you know,

250
00:15:09.351 --> 00:15:13.430
does this,
does this read like an American,

251
00:15:13.431 --> 00:15:15.440
does this read like a communication?

252
00:15:15.440 --> 00:15:20.440
And what we started to see was one way to not get caught for your lousy English

253
00:15:23.001 --> 00:15:26.810
or your,
you know,
yeah.
Your cultural,
um,

254
00:15:27.590 --> 00:15:30.350
lack of,
uh,
kind of native,
native,
um,

255
00:15:31.100 --> 00:15:34.730
abilities is to just repurpose other people's stuff.

256
00:15:34.970 --> 00:15:38.910
And so that's where you would see memes getting shared from,
uh,

257
00:15:38.960 --> 00:15:40.550
on both the right and the left.
You know,

258
00:15:40.551 --> 00:15:43.790
you'd see a lot of these like turning point USA memes that they were repurposing

259
00:15:43.791 --> 00:15:46.170
and pushing out,
or you would see,
um,

260
00:15:46.250 --> 00:15:48.860
occupied Democrats or the other 98%.

261
00:15:48.980 --> 00:15:53.000
So memes from real American pages,

262
00:15:53.001 --> 00:15:54.230
real American culture,

263
00:15:54.620 --> 00:15:58.160
and they would just sometimes slap a new logo on and just repost it as if it was

264
00:15:58.161 --> 00:16:02.420
theirs.
So it does in those instances,
read just like,
you know,

265
00:16:02.480 --> 00:16:05.930
authentic American content.
And in many ways it is authentic American content.

