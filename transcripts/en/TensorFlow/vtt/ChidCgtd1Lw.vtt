WEBVTT
Kind: captions
Language: en

00:00:06.435 --> 00:00:09.596
KARMEL ALLISON: Hi and
welcome to Coding TensorFlow.

00:00:09.596 --> 00:00:11.470
I'm Karmel Allison, and
I'm here to guide you

00:00:11.470 --> 00:00:15.130
through a scenario using
TensorFlow's high-level APIs.

00:00:15.130 --> 00:00:17.230
This video is the
third and final part

00:00:17.230 --> 00:00:19.150
of a three-part series.

00:00:19.150 --> 00:00:20.830
In the first video,
we looked at data

00:00:20.830 --> 00:00:23.420
and how to prepare your
data for machine learning.

00:00:23.420 --> 00:00:26.080
We then moved on to optimizing
your data for machine learning

00:00:26.080 --> 00:00:28.120
with TensorFlow
and keras including

00:00:28.120 --> 00:00:30.280
building a simple model.

00:00:30.280 --> 00:00:32.200
And here's the model we defined.

00:00:32.200 --> 00:00:34.660
We will start with a simple
sequential model, which

00:00:34.660 --> 00:00:36.460
strings together
the modular keras

00:00:36.460 --> 00:00:38.560
layers, hooking
the output of each

00:00:38.560 --> 00:00:40.570
into the input of the next.

00:00:40.570 --> 00:00:42.400
Our first layer will
do all of the data

00:00:42.400 --> 00:00:44.690
transformations
we just discussed,

00:00:44.690 --> 00:00:47.230
and then we go through some
standard densely connected

00:00:47.230 --> 00:00:48.220
layers.

00:00:48.220 --> 00:00:51.190
Our final layer here will
output the class predictions

00:00:51.190 --> 00:00:53.110
for each of the four
wilderness areas

00:00:53.110 --> 00:00:55.074
that we are interested in.

00:00:55.074 --> 00:00:57.490
Notice here that we are just
establishing the architecture

00:00:57.490 --> 00:00:59.530
of our model, and
we still haven't yet

00:00:59.530 --> 00:01:01.600
hooked it up to any data.

00:01:01.600 --> 00:01:04.209
Once we have the layer
architecture established,

00:01:04.209 --> 00:01:07.660
we can compile the model,
which adds the optimizer, loss,

00:01:07.660 --> 00:01:09.820
and metrics we
are interested in.

00:01:09.820 --> 00:01:13.480
TensorFlow provides a number
of optimizers and lost choices,

00:01:13.480 --> 00:01:16.120
which we could explore
if we wanted to.

00:01:16.120 --> 00:01:18.590
And finally the
rubber meets the road.

00:01:18.590 --> 00:01:22.330
We pass our data set into
our model, and we train.

00:01:22.330 --> 00:01:25.420
Now in a real world situation
with large data sets,

00:01:25.420 --> 00:01:27.850
we would likely want to
leverage hardware accelerators

00:01:27.850 --> 00:01:29.830
like GPUs or TPUs.

00:01:29.830 --> 00:01:31.750
And we may even want
to distribute training

00:01:31.750 --> 00:01:34.300
across multiple GPUs or nodes.

00:01:34.300 --> 00:01:36.940
You can find out more about
using distribution strategies

00:01:36.940 --> 00:01:39.890
for this in the links included
in the description below.

00:01:39.890 --> 00:01:42.550
So for now I will just point
out that the same code will

00:01:42.550 --> 00:01:44.980
work in the distributed
settings and when

00:01:44.980 --> 00:01:47.024
eager execution is disabled.

00:01:47.024 --> 00:01:48.940
For now, we will just
assume that we will wait

00:01:48.940 --> 00:01:50.890
for this to finish training.

00:01:50.890 --> 00:01:53.440
Once it's done training,
you can test it.

00:01:53.440 --> 00:01:55.460
And while that's
pretty straightforward,

00:01:55.460 --> 00:01:58.240
we first need to load
in our validation data.

00:01:58.240 --> 00:02:01.270
It's important here that we use
the same processing procedure

00:02:01.270 --> 00:02:04.370
for our test data that we
did for our training data.

00:02:04.370 --> 00:02:07.690
So maybe we'll define a function
that we can use in both cases

00:02:07.690 --> 00:02:09.850
to ensure repeatability.

00:02:09.850 --> 00:02:12.130
We call the evaluate
method of our model

00:02:12.130 --> 00:02:15.550
with the validation data, which
returns the loss and accuracy

00:02:15.550 --> 00:02:17.800
that we get on our test data.

00:02:17.800 --> 00:02:19.870
Note here that because
we took care of our data

00:02:19.870 --> 00:02:21.940
transformations using
feature columns,

00:02:21.940 --> 00:02:24.820
we know that the transformation
of our input validation data

00:02:24.820 --> 00:02:27.190
will happen in the same way
as it did for our training

00:02:27.190 --> 00:02:31.340
data, which is critical to
ensuring repeatable results.

00:02:31.340 --> 00:02:33.610
So now we validated our
model on independent

00:02:33.610 --> 00:02:37.060
held out data that was processed
in the same way as our training

00:02:37.060 --> 00:02:37.805
data.

00:02:37.805 --> 00:02:39.430
And we can pretend
for a minute that we

00:02:39.430 --> 00:02:41.050
are happy with the
results, and we

00:02:41.050 --> 00:02:43.180
are ready to deploy this model.

00:02:43.180 --> 00:02:44.950
There is a lot of
tooling that is required

00:02:44.950 --> 00:02:48.820
for real world deployment, which
the library TFX makes possible.

00:02:48.820 --> 00:02:51.810
I put a link to this in
the description below.

00:02:51.810 --> 00:02:54.670
TensorFlow provides
a model saving format

00:02:54.670 --> 00:02:56.870
that works with the suite
of TensorFlow products,

00:02:56.870 --> 00:03:00.480
including TensorFlow
Serving and TensorFlow.js.

00:03:00.480 --> 00:03:03.610
The TensorFlow saved model
includes a checkpoint with all

00:03:03.610 --> 00:03:06.610
of the weights and variables,
and it also includes the graph

00:03:06.610 --> 00:03:10.100
that we built for training,
evaluating, and predicting.

00:03:10.100 --> 00:03:12.630
Keras now natively
exports to TensorFlow

00:03:12.630 --> 00:03:14.920
saved model format for serving.

00:03:14.920 --> 00:03:17.740
This saved model is a fully
contained serialization

00:03:17.740 --> 00:03:20.620
of your model, so you can
load it back into Python later

00:03:20.620 --> 00:03:24.250
if you want to retrain
or reuse your model.

00:03:24.250 --> 00:03:26.500
And now we've gone through
all of the critical stages

00:03:26.500 --> 00:03:29.170
to build a model for
our data in TensorFlow.

00:03:29.170 --> 00:03:30.370
And maybe we're done.

00:03:30.370 --> 00:03:33.312
It could be that this is the
perfect model, and we're happy.

00:03:33.312 --> 00:03:35.270
But then we'd all have
to go and find new jobs,

00:03:35.270 --> 00:03:37.436
so let's assume that we
want to improve the accuracy

00:03:37.436 --> 00:03:39.460
of the model we have built.

00:03:39.460 --> 00:03:42.220
There are lots of places we
might decide to make changes.

00:03:42.220 --> 00:03:44.260
We could go and
collect more data.

00:03:44.260 --> 00:03:47.050
We could change the way we
process and parse the data.

00:03:47.050 --> 00:03:48.760
We could change the
model architecture,

00:03:48.760 --> 00:03:50.200
add or remove layers.

00:03:50.200 --> 00:03:52.540
We could change the
optimizer or the loss.

00:03:52.540 --> 00:03:55.210
We could try different
hyper parameters and so on.

00:03:55.210 --> 00:03:56.860
What I will show
you today is how

00:03:56.860 --> 00:03:59.530
to use the same data
and features to try out

00:03:59.530 --> 00:04:01.870
one of TensorFlow's
canned estimators, which

00:04:01.870 --> 00:04:04.150
are built in
implementations of some more

00:04:04.150 --> 00:04:07.540
complex models including
those that don't fit nicely

00:04:07.540 --> 00:04:09.911
into a layer-based architecture.

00:04:09.911 --> 00:04:11.410
So what if we wanted
to shake things

00:04:11.410 --> 00:04:13.090
up and try a different model?

00:04:13.090 --> 00:04:15.010
To rewind, this is
the model we had,

00:04:15.010 --> 00:04:17.200
a densely connected
neural network.

00:04:17.200 --> 00:04:19.120
Let's try changing it.

00:04:19.120 --> 00:04:21.200
Here we are using the
same feature columns,

00:04:21.200 --> 00:04:24.670
but we're configuring one of the
TensorFlow canned estimators.

00:04:24.670 --> 00:04:27.920
We are using the DNN
linear combined classifier,

00:04:27.920 --> 00:04:30.570
which is also known as
the wide and deep model.

00:04:30.570 --> 00:04:32.140
And that gives us
an architecture

00:04:32.140 --> 00:04:34.060
that looks something
like this, allowing

00:04:34.060 --> 00:04:36.340
us to trivially leverage
all the research that

00:04:36.340 --> 00:04:39.070
went into developing
this model structure.

00:04:39.070 --> 00:04:41.200
This model combines
traditional linear learning

00:04:41.200 --> 00:04:42.880
with deep learning,
and so we can

00:04:42.880 --> 00:04:46.210
feed in our categorical data
directly to the linear half

00:04:46.210 --> 00:04:48.820
and then configure a DNN
with two dense layers

00:04:48.820 --> 00:04:50.800
for the numerical data.

00:04:50.800 --> 00:04:52.690
We can then train the
wide and deep model

00:04:52.690 --> 00:04:54.880
just as we did with
our other model.

00:04:54.880 --> 00:04:56.440
Notice here that
the caned estimator

00:04:56.440 --> 00:05:00.610
expects the input function
rather than data set directly.

00:05:00.610 --> 00:05:03.160
Estimators control their
own sessions and graphs

00:05:03.160 --> 00:05:05.500
so that at the time of
distribution they can build

00:05:05.500 --> 00:05:07.810
and replicate
graphs as necessary.

00:05:07.810 --> 00:05:10.120
So our input function
here gives the estimate

00:05:10.120 --> 00:05:12.880
of the instructions for
getting our data set

00:05:12.880 --> 00:05:15.160
and producing the
tensors that we want,

00:05:15.160 --> 00:05:18.160
and the estimates will then call
this function in its own graph

00:05:18.160 --> 00:05:20.470
and session when necessary.

00:05:20.470 --> 00:05:22.450
Here we wrap the same
data loading function

00:05:22.450 --> 00:05:24.970
that we used in our
previous model in a lambda

00:05:24.970 --> 00:05:27.340
with the correct file
names preconfigured

00:05:27.340 --> 00:05:30.040
so that the estimator can
call this function at runtime

00:05:30.040 --> 00:05:32.830
to get the appropriate
features and labels.

00:05:32.830 --> 00:05:36.820
We can use the same strategy
to evaluate using test data.

00:05:36.820 --> 00:05:39.880
And lo and behold, if we
run this for 20 epochs,

00:05:39.880 --> 00:05:43.510
we have another train model that
we can compare to the first.

00:05:43.510 --> 00:05:45.970
Note that this is just one of
a number of canned estimates

00:05:45.970 --> 00:05:47.620
that TensorFlow offers.

00:05:47.620 --> 00:05:50.670
We have boost to trees, model
for time series analysis, r

00:05:50.670 --> 00:05:53.660
and n's, walls,
and more as well.

00:05:53.660 --> 00:05:55.210
Note that for
estimators, we first

00:05:55.210 --> 00:05:57.550
have to tell the model what
shape and type of tensor

00:05:57.550 --> 00:05:59.680
to expect at inference time.

00:05:59.680 --> 00:06:03.340
For that, we have to define
an input receiver function.

00:06:03.340 --> 00:06:04.339
It sounds confusing.

00:06:04.339 --> 00:06:05.380
And I'm not going to lie.

00:06:05.380 --> 00:06:07.120
It's a little confusing.

00:06:07.120 --> 00:06:09.850
We want a function that
builds the tensor shapes

00:06:09.850 --> 00:06:12.130
that we expect at serving time.

00:06:12.130 --> 00:06:14.630
Luckily, we can use this
convenience function,

00:06:14.630 --> 00:06:16.600
which just needs the
shapes of the tensors we

00:06:16.600 --> 00:06:18.850
will want to run inference on.

00:06:18.850 --> 00:06:20.980
Because we are in
eager mode, we can just

00:06:20.980 --> 00:06:23.470
grab a row from our
data set and use

00:06:23.470 --> 00:06:26.670
that to tell the convenience
function what to expect.

00:06:26.670 --> 00:06:29.500
Here we grab the features
from the first row

00:06:29.500 --> 00:06:31.780
and don't do any
additional processing

00:06:31.780 --> 00:06:34.150
because we are assuming at
inference time the data will

00:06:34.150 --> 00:06:36.550
look the same, just
without labels.

00:06:36.550 --> 00:06:39.130
In real world scenarios,
your inference data

00:06:39.130 --> 00:06:41.500
may need additional
processing, such as parsing

00:06:41.500 --> 00:06:43.230
from a live request string.

00:06:43.230 --> 00:06:45.250
And the input receiver
function is where

00:06:45.250 --> 00:06:47.230
you would encode that logic.

00:06:47.230 --> 00:06:49.030
We can then use the
function returned

00:06:49.030 --> 00:06:52.630
by build raw serving input
receiver function to generate

00:06:52.630 --> 00:06:55.690
a saved model that can
be used in TF serving, TF

00:06:55.690 --> 00:06:59.290
hub, and elsewhere just
as we did with keras.

00:06:59.290 --> 00:07:01.177
So now we've made a full loop.

00:07:01.177 --> 00:07:03.010
And if we had more time,
we could keep going

00:07:03.010 --> 00:07:06.169
and try out some
more of these paths.

00:07:06.169 --> 00:07:07.710
I'll leave you to
explore that, and I

00:07:07.710 --> 00:07:09.840
hope that this series
has been as fun for you

00:07:09.840 --> 00:07:11.220
as it was for me.

00:07:11.220 --> 00:07:13.210
Remember if you
have any questions,

00:07:13.210 --> 00:07:15.280
please leave them in
the comments below

00:07:15.280 --> 00:07:18.410
and don't forget to hit
that subscribe button.

