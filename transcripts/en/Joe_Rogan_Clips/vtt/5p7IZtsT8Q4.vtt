WEBVTT

1
00:00:00.990 --> 00:00:04.860
The Joe Rogan experience?
No.
Zero.
I don't know anything about coding,

2
00:00:04.861 --> 00:00:09.330
but do you have like is there a spell check for coding?
Yeah.

3
00:00:09.331 --> 00:00:14.331
So it's kind of called debugging is trying to find bugs and it's a software

4
00:00:14.701 --> 00:00:16.940
that's doing this.
Yeah.
Software.
So there's,

5
00:00:16.980 --> 00:00:21.870
depending on the programming language and everybody should,
uh,
if you,
uh,

6
00:00:22.050 --> 00:00:25.080
if you haven't tried programming,
you should try it.
It's cool.

7
00:00:25.350 --> 00:00:28.860
It's the future should learn to program.
Okay.
That's my plug.

8
00:00:28.960 --> 00:00:31.720
These supposed to learn to code.
You can get started.

9
00:00:31.870 --> 00:00:35.660
What did that at [inaudible] apartment?
Scared of it.
I,

10
00:00:35.860 --> 00:00:37.110
it's a problematic term.

11
00:00:37.111 --> 00:00:40.500
I don't actually know why it's the dumbest fucking problematic code of all time

12
00:00:40.680 --> 00:00:45.680
because someone ridiculously was suggesting that coal miners could maybe learn

13
00:00:47.370 --> 00:00:51.750
how to code computer code and like get a different job.
They can be trained.

14
00:00:52.380 --> 00:00:57.180
And so as someone,
the way people were looking at it like that,

15
00:00:57.181 --> 00:00:58.620
that was a,
uh,

16
00:00:58.710 --> 00:01:03.630
like a frivolous suggestion and that it was ridiculous to try to get someone who

17
00:01:03.631 --> 00:01:04.440
is 50 years old.

18
00:01:04.440 --> 00:01:09.440
It doesn't have any education in computers at all to change their job from being

19
00:01:09.481 --> 00:01:11.500
a coal miner to learning how to code.

20
00:01:11.790 --> 00:01:14.670
So they started saying it to politicians and people mocking it,

21
00:01:15.150 --> 00:01:20.150
but then what Twitter alleged was that what was going on was it was being

22
00:01:20.161 --> 00:01:25.161
connected to white supremacy and antisemitism and a bunch of different things.

23
00:01:25.891 --> 00:01:27.300
Like people were saying,
learn to code,

24
00:01:27.510 --> 00:01:32.130
and they were putting in a bunch of these other phrases in my suggestion would

25
00:01:32.131 --> 00:01:36.270
be,
well that's a different fucking thing.
Like now you have like you look,

26
00:01:36.271 --> 00:01:39.150
you have a problem with Nazis and white supremacists,

27
00:01:39.151 --> 00:01:42.450
but that's the promise with Nazis and white supremacists when someone is just

28
00:01:42.451 --> 00:01:44.220
saying,
learn to code,

29
00:01:44.340 --> 00:01:49.200
mocking this ridiculous idea that you're going to teach,
you know,

30
00:01:49.201 --> 00:01:49.471
that's a,

31
00:01:49.471 --> 00:01:52.980
that's a legitimate criticism of someone's perspective that you're going to get

32
00:01:53.370 --> 00:01:56.580
a coal miner to learn how to fucking do computer coding.
It's crazy.

33
00:01:56.650 --> 00:02:01.020
It's so people getting banned for that.
Rightly so.

34
00:02:01.021 --> 00:02:02.070
People were furious.

35
00:02:02.610 --> 00:02:07.290
The Way Google described to me and Tim Pool and we were discussing it,

36
00:02:07.350 --> 00:02:09.690
was that Google,
I mean,
excuse me,
Twitter,

37
00:02:10.350 --> 00:02:13.320
the way Twitter described it was that essentially we're dealing with something

38
00:02:13.380 --> 00:02:16.860
where they were trying to censor things at scale.
There was,

39
00:02:17.040 --> 00:02:20.940
there was so many people and there's so much going on that it's very difficult

40
00:02:20.941 --> 00:02:24.600
to get it right and that they've made mistakes.
I think that's a fast,

41
00:02:24.660 --> 00:02:29.340
one of the most fascinating applications of AI actually is filtering,

42
00:02:29.640 --> 00:02:31.170
trying to manage pewter learning.

43
00:02:31.670 --> 00:02:35.280
[inaudible] so using machine learning to manage this huge conversation.

44
00:02:35.281 --> 00:02:38.850
You're talking about 500,
I believe it's 500 million tweets a day,

45
00:02:38.851 --> 00:02:42.060
something like that.
And he,
Jamie makes at least three,

46
00:02:43.100 --> 00:02:47.610
<v 1>three 91
I was going to say with this conversation,</v>

47
00:02:47.620 --> 00:02:51.870
I saw this recently,
I don't know who did the data on this,
but there's a,
uh,

48
00:02:52.520 --> 00:02:56.130
a statement someone put on Twitter that said that of,
um,

49
00:02:56.160 --> 00:03:00.100
let me see if I can word it correctly.
It was 22% of adult Americans

50
00:03:00.100 --> 00:03:03.220
<v 2>are on Twitter.
Whoa.
All right.
So that's like,
that's like a fact,</v>

51
00:03:03.221 --> 00:03:08.221
one of that 10% make up 80% of the tweets created by adult Americans.

52
00:03:12.370 --> 00:03:17.290
2% of the people on Twitter make up 80% of the tweets.
That makes sense.
Yeah.

53
00:03:18.120 --> 00:03:20.980
A lot of people arguing aggressively and the,

54
00:03:21.010 --> 00:03:23.590
and the question of how to manage that and you can't manage that,

55
00:03:23.591 --> 00:03:25.960
but just a manual,

56
00:03:26.740 --> 00:03:29.210
a review of [inaudible]

57
00:03:29.410 --> 00:03:34.300
<v 0>yeah.
You'd have to have so many employees.
Yeah.
That's I think more likely.</v>

58
00:03:34.540 --> 00:03:37.090
I don't think Jack is lying,
but um,

59
00:03:37.150 --> 00:03:41.620
nor is Vigia but I do think that they have a clear bias against conservatives

60
00:03:41.621 --> 00:03:42.454
and that's being shown.

61
00:03:42.770 --> 00:03:46.940
<v 2>So that's an interesting question.
I have a your friend,
my friend and mentor,</v>

62
00:03:46.941 --> 00:03:50.870
Eric Weinstein.
Yes.
Talk to me.
I disagreed with him a little bit on this.

63
00:03:51.760 --> 00:03:56.060
I think,
uh,
he basically believes so there's a bias.

64
00:03:56.250 --> 00:04:00.350
It boils down to the conversation that Jack is having at the,
at the top level,

65
00:04:00.351 --> 00:04:05.170
inside Twitter.
What,
what is that conversation like?
Uh,

66
00:04:05.390 --> 00:04:08.540
I think,
I tend to believe,
again,

67
00:04:08.570 --> 00:04:13.310
this might be my naive nature is that they have,

68
00:04:13.370 --> 00:04:15.290
they don't have bias and they have just,

69
00:04:15.380 --> 00:04:19.550
they're trying to manage this huge flood of,
um,

70
00:04:20.180 --> 00:04:23.900
of tweets and what they're trying to do is not buy,

71
00:04:24.230 --> 00:04:28.070
is not to remove sort of conservative as the liberals and so on.

72
00:04:28.400 --> 00:04:30.470
They're trying to,
uh,

73
00:04:30.710 --> 00:04:35.330
remove people that lead to,
uh,

74
00:04:35.390 --> 00:04:37.150
others leaving the conversation.

75
00:04:37.151 --> 00:04:40.580
So they want more people to be in the conversation.
I think

76
00:04:40.810 --> 00:04:41.990
<v 0>that's true as well,</v>

77
00:04:42.170 --> 00:04:45.860
but I think they definitely are biased against conservative people.
There was a,

78
00:04:45.861 --> 00:04:49.730
an Alexander x,
Alexandra AOC.

79
00:04:50.540 --> 00:04:53.420
Octavia,
how's it?
AOC is good.

80
00:04:54.420 --> 00:04:59.060
Cortez is the last one.
Is it Octavia?
Ocasio that's right.
Okay.

81
00:04:59.061 --> 00:05:03.640
I'm sorry.
Alexandra AOC.
Sorry,
I'm just,
I'm thinking I was,

82
00:05:03.660 --> 00:05:05.870
there wasn't planning on talking about her,
but,
um,

83
00:05:05.900 --> 00:05:09.290
there was a parody account and someone was running his parody account,

84
00:05:09.291 --> 00:05:12.440
which was very mild,
just humorous parody account.

85
00:05:12.441 --> 00:05:15.470
They were banned permanently for running it and then their own account was

86
00:05:15.471 --> 00:05:18.950
banned as well.
Whereas,
um,
you know,

87
00:05:18.980 --> 00:05:23.360
there's some progressive people are liberal people that post all sorts of crazy

88
00:05:23.361 --> 00:05:26.300
shit.
And they don't,
they don't get banned at the same brain.

89
00:05:26.330 --> 00:05:30.350
It's really clear that someone in the company,

90
00:05:30.740 --> 00:05:32.990
whether it's up for manual review,

91
00:05:32.991 --> 00:05:35.510
whether it's at the discretion of the people that are employees,

92
00:05:35.750 --> 00:05:38.330
when you're thinking about a company that's a silicone valley company,

93
00:05:38.331 --> 00:05:41.030
you are in without doubt.

94
00:05:41.330 --> 00:05:44.270
You're dealing with people that are leaning left.

95
00:05:44.780 --> 00:05:48.860
There's so many that lean left in silicon valley.

96
00:05:48.861 --> 00:05:52.580
The idea that that company was secretly run by Republicans is ridiculous.

97
00:05:52.820 --> 00:05:56.780
They're,
they're almost all run by Democrats or progressive people.

98
00:05:57.080 --> 00:06:01.540
So that the leadership level,
there's,
there's a narrow mindedness that,
that,
that,

99
00:06:01.850 --> 00:06:04.550
that permeates all silicon valley saying.
Well,
the question,

100
00:06:04.760 --> 00:06:08.780
I think there's a leaning left that permeate silicone valley.

101
00:06:08.781 --> 00:06:11.750
I think that's undeniable.
I think it's undeniable.
I mean,

102
00:06:11.900 --> 00:06:12.950
I think if you had a poll,

103
00:06:12.951 --> 00:06:15.800
the people that work in silicone valley where their political leanings are,

104
00:06:15.801 --> 00:06:20.450
I think it would be by far left.
I think it would be the vast majority.

105
00:06:20.990 --> 00:06:24.650
Um,
does that mean that affects their decisions?
Well,
what's the evidence?
Well,

106
00:06:24.740 --> 00:06:26.540
it's kind of shows us does,
you know,
it's,

107
00:06:26.650 --> 00:06:31.650
they're not treating it with 100% clarity and you know,

108
00:06:31.970 --> 00:06:36.140
across the board accuracy or,
um,
fairness rather.

109
00:06:36.530 --> 00:06:40.760
I think that there's absolutely people that work there that lean and there's

110
00:06:40.761 --> 00:06:43.190
been videos where they've captured people,
uh,

111
00:06:43.191 --> 00:06:46.670
that were Twitter employees talking about it,
talking about how you do that,

112
00:06:46.700 --> 00:06:48.860
how you,
uh,
make their,
you know,

113
00:06:48.861 --> 00:06:52.730
fuck you find someone who's a using Trump talk or,
you know,

114
00:06:52.731 --> 00:06:55.820
saying sad at the end of things and someone's talking,
he's gonna,
you know,

115
00:06:55.821 --> 00:06:59.390
that certain characteristics they look for and there's been videos of what does

116
00:06:59.391 --> 00:07:01.920
that project Veritas with that guy,
uh,

117
00:07:02.000 --> 00:07:06.230
got his employees got undercover footage of Twitter employees talking about that

118
00:07:06.231 --> 00:07:09.050
kind of stuff.
The question is how much power do those individuals have?

119
00:07:09.051 --> 00:07:13.760
How many individuals are there like that I are that are those people

120
00:07:13.761 --> 00:07:17.390
exaggerating their ability and what they do at work or they,

121
00:07:17.420 --> 00:07:21.080
are they talking about something that used to go on but doesn't go on anymore?

122
00:07:21.290 --> 00:07:24.180
I don't know.
I don't work there.
I think it boils down to I,

123
00:07:24.320 --> 00:07:28.760
I'm one of those people that believes it's bows out to the leadership and people

124
00:07:28.761 --> 00:07:31.760
at the top set the culture and the culture has to be,

125
00:07:32.870 --> 00:07:37.370
it cannot be this kind of silicon valley narrow minded,
uh,

126
00:07:37.790 --> 00:07:41.960
sort of left leaning thinking even if you believe even if you're a hardcore

127
00:07:41.961 --> 00:07:45.530
liberal,
you cannot,
when you operate the car,

128
00:07:45.590 --> 00:07:48.560
when you drive and manage a conversation in the entire world,

129
00:07:48.561 --> 00:07:51.080
you have to think about Middle America.
You have to think about,

130
00:07:51.170 --> 00:07:54.680
you have to have fundamental respect for human beings who voted for Trump.

131
00:07:55.100 --> 00:08:00.100
It is a concerning thing for me to see just a narrow mindedness of an all forms.

132
00:08:00.830 --> 00:08:05.010
One of the reasons I enjoy listening to this podcast is you're pretty open

133
00:08:05.011 --> 00:08:05.844
minded.

134
00:08:05.900 --> 00:08:10.900
That open mindedness is essential for leaders of Facebook and Twitter.

135
00:08:11.660 --> 00:08:15.130
People who are managing conversations.
I think so too.
I think it's,

136
00:08:15.140 --> 00:08:20.140
I think it's a s the thought of being open minded and acting in that ethic is

137
00:08:23.540 --> 00:08:27.080
probably one of the most important things that we could go forward with right

138
00:08:27.081 --> 00:08:31.850
now because things are getting so greasy.
It's so slippery on,
on both sides.

139
00:08:32.180 --> 00:08:37.180
And we're in this weird position that I don't recall ever in my life there being

140
00:08:38.031 --> 00:08:41.900
such a divide between the right and the left in this country.
I know it's more,

141
00:08:42.920 --> 00:08:46.070
more vicious,
more angry,
more hateful.

142
00:08:46.340 --> 00:08:48.980
It's different than at any other time in my life.

143
00:08:49.310 --> 00:08:54.310
And I think a lot of our ideas are based on these narratives that or may not

144
00:08:55.861 --> 00:09:00.780
even be accurate.
And then we support them and we reinforce them on either side.

145
00:09:00.810 --> 00:09:02.040
We reinforce them on the left,

146
00:09:02.050 --> 00:09:06.330
we reinforce them on the right where if you're looking at reality itself and you

147
00:09:06.331 --> 00:09:07.570
don't have these,
uh,

148
00:09:07.740 --> 00:09:12.510
clear parameters and these clear ideologies,
I think we're way,

149
00:09:12.511 --> 00:09:15.990
most of us are way more in the middle than we think we are.
Most of us are.

150
00:09:16.020 --> 00:09:18.030
We just don't want racist run in the country.

151
00:09:18.150 --> 00:09:20.850
We don't want socialists given all our money away.

152
00:09:20.970 --> 00:09:24.030
We don't want to pay too much in taxes to a shitty government.

153
00:09:24.120 --> 00:09:27.270
We don't want schools getting underfunded.
We,
we all,
you know,

154
00:09:27.450 --> 00:09:28.920
and then we decide what,

155
00:09:28.980 --> 00:09:32.550
what does my team like the team that I,

156
00:09:32.551 --> 00:09:35.220
the shit that I like is that this team,
well,
not everything,

157
00:09:35.221 --> 00:09:36.750
but they've got a lot of things.
So I'll go with them.

158
00:09:36.930 --> 00:09:38.160
Maybe I'm not a religious nut,

159
00:09:38.161 --> 00:09:41.310
but I'm fiscally conservative and I don't like with Democrats like to spend

160
00:09:41.311 --> 00:09:45.720
money.
I'm going to go with the Republicans.
You know,
maybe,
maybe I'm more,

161
00:09:46.890 --> 00:09:51.480
I'm more concerned with the state of the economy and the way we trade with the

162
00:09:51.481 --> 00:09:54.450
world than I am with certain social issues that the Democrats embrace.

163
00:09:54.451 --> 00:09:55.710
So I'll lean that way.

164
00:09:55.890 --> 00:09:59.190
Even though I do support gay rights and I do support this and I do support all

165
00:09:59.191 --> 00:10:03.210
these other progressive ideas this way more of us in that boat.

166
00:10:03.420 --> 00:10:06.770
There's way more of us that are in this middle of the whole thing

167
00:10:06.800 --> 00:10:11.690
<v 2>for sure.
But there it goes up and down.
So all of us,
so I'm open,
I believe.</v>

168
00:10:11.750 --> 00:10:16.070
I hope I am open minded most of the time.
But you have different moods.

169
00:10:16.610 --> 00:10:20.360
Oh,
for sure.
Yeah.
And the question is,
this is where the role of AI comes in.

170
00:10:20.920 --> 00:10:25.920
Does the AI that recommends what tweets I should see what Facebook messenger

171
00:10:26.090 --> 00:10:31.090
that's just see is that encouraging the darker parts of me or the the Steven

172
00:10:32.421 --> 00:10:36.440
pinker better angels of our nature though cause it,
what stuff is it showing me?

173
00:10:36.560 --> 00:10:38.750
Because if it shows me,
uh,

174
00:10:39.110 --> 00:10:44.110
stuff that if the AI trains purely unclicked,

175
00:10:44.450 --> 00:10:48.920
it may start to learn when I'm in a bad mood and point me to things that might

176
00:10:48.921 --> 00:10:49.970
be upsetting to me.

177
00:10:50.390 --> 00:10:55.390
And so escalating that division and escalating this viral thing that can be

178
00:10:57.081 --> 00:11:00.530
solved most likely with people training a little more Jujitsu or something.

179
00:11:00.990 --> 00:11:03.350
Well it's the size Facebook

180
00:11:03.380 --> 00:11:07.640
<v 0>algorithm that encourages people to be outraged because accidentally,</v>

181
00:11:07.641 --> 00:11:10.850
not even on purpose,
but this is what engages people.

182
00:11:10.851 --> 00:11:12.890
Well this is what gets clicks.
So they find out,

183
00:11:12.891 --> 00:11:16.310
oh well he clicks on things when he finds out that people are anti vaccination

184
00:11:16.520 --> 00:11:20.480
or he clicks on things when he finds out,
you know what,

185
00:11:20.490 --> 00:11:22.280
it fill in the blank with whatever the subject is.

186
00:11:22.370 --> 00:11:25.130
And then you get these mother fuckers,
you know,

187
00:11:25.160 --> 00:11:28.040
this is the reason why measles is spreading and you start getting an angry,

188
00:11:28.160 --> 00:11:30.890
I mean the anti vaccs arguments on Facebook,

189
00:11:30.891 --> 00:11:34.490
I don't know if you ever dip into those waters for a few minutes and watch

190
00:11:34.491 --> 00:11:39.080
people fight back and forth and in fury and anger,
you know,
it's a,

191
00:11:39.380 --> 00:11:43.730
it's another one of those things that becomes a extremely lucrative,

192
00:11:44.030 --> 00:11:48.980
uh,
subject for any social media empire.

193
00:11:49.280 --> 00:11:49.611
If you're,

194
00:11:49.611 --> 00:11:54.130
if you're all about getting people to engage and that's where the money is in he

195
00:11:54.210 --> 00:11:56.470
to getting people to Click on the page and the ads are on those pages,

196
00:11:56.471 --> 00:11:59.680
you get those clips,
get that money if that's how the system is set up.

197
00:11:59.681 --> 00:12:02.260
And I'm not exactly sure how it is cause I don't really use Facebook,

198
00:12:02.590 --> 00:12:05.530
but that's what it benefits.
I mean that's what,

199
00:12:05.531 --> 00:12:07.990
that's what it gravitates towards.
Gravitates towards controversy.

200
00:12:08.540 --> 00:12:09.373
<v 2>So,</v>

201
00:12:09.750 --> 00:12:13.770
and when we think about concern for AI systems to talk about sort of terminate

202
00:12:13.780 --> 00:12:17.210
or I'm sure we'll,
we'll touch on it,
but I think of Twitter as a whole,

203
00:12:17.211 --> 00:12:18.230
as one organism.

204
00:12:19.070 --> 00:12:23.510
That is the thing that worries me the most is the artificial intelligence that

205
00:12:23.511 --> 00:12:25.760
is very kind of dumb and simple,

206
00:12:25.761 --> 00:12:30.430
simple algorithms that are driving the behavior of millions of people and at

207
00:12:30.440 --> 00:12:34.280
together the kind of chaos that we can achieve.
I mean,

208
00:12:34.281 --> 00:12:38.860
that algorithm has incredible influence in all society.
Twitter are,

209
00:12:38.910 --> 00:12:43.790
our current president is on Twitter so much.
Oh yeah.
All Day,

210
00:12:43.791 --> 00:12:47.780
all night.
The,
the,
I mean,
it's scary to think about.

211
00:12:48.380 --> 00:12:52.130
We talk about autonomous vehicles leading to fate to l one fatality to

212
00:12:52.131 --> 00:12:55.260
fatalities is scary.
To think about what the difference,

213
00:12:55.510 --> 00:12:59.680
a small change in the f in the Twitter algorithm.
I mean,
I,

214
00:12:59.830 --> 00:13:04.610
it could start wars.
It really could.
And that if you think about the long term,

215
00:13:04.611 --> 00:13:09.140
if you think about is one AI organism that is a super intelligent organism that

216
00:13:09.141 --> 00:13:13.970
will have no control over.
And I think it all boils down honestly,

217
00:13:14.120 --> 00:13:17.460
to the leadership,
to Jack,
uh,
to,

218
00:13:17.461 --> 00:13:21.650
to two and other folks like him making sure that he's open minded.

219
00:13:21.800 --> 00:13:24.140
He goes hunting,
then he goes,
uh,

220
00:13:24.170 --> 00:13:28.460
does some Jujitsu that he eats some meat and sometimes goes Vegan.
Right?

