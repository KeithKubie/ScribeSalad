WEBVTT
Kind: captions
Language: en

00:00:00.291 --> 00:00:03.166
Picking up right where
we left off last time,

00:00:03.166 --> 00:00:07.037
we were deriving the variance
of a hypergeometric, right?

00:00:07.037 --> 00:00:09.763
So I was just wanting to
quickly recap that and

00:00:09.763 --> 00:00:11.860
make a few more comments about it.

00:00:11.860 --> 00:00:14.823
We basically did the calculation last
time, just didn't simplify the algebra.

00:00:14.823 --> 00:00:18.328
But I wanna say a few more things
about that and remind you.

00:00:18.328 --> 00:00:23.400
So we were doing the variance
of the hypergeometric,

00:00:27.099 --> 00:00:29.625
And we have parameters w, b, n.

00:00:29.625 --> 00:00:35.300
Which you think of it as w,
white balls, b, black balls,

00:00:35.300 --> 00:00:40.190
and we're taking a sample of
size n without replacement.

00:00:41.270 --> 00:00:45.930
And then we wanna study the variance of
the number of white balls in the sample.

00:00:45.930 --> 00:00:50.170
And I'll just remind you what we
did at the very end, last time.

00:00:50.170 --> 00:00:55.620
And let's actually make up a little
bit more notation to make this

00:00:55.620 --> 00:00:56.960
a little bit nicer.

00:00:56.960 --> 00:01:04.510
So let's let p equal w over w + b, that's
a natural quantity to look at, right?

00:01:04.510 --> 00:01:10.027
And that is between 0 and 1, it's just a
fraction of white balls in the population.

00:01:10.027 --> 00:01:13.352
And it's also kind of
convenient to let w + b = N.

00:01:13.352 --> 00:01:17.019
That's not a random variable,
but that's sort of a traditional

00:01:17.019 --> 00:01:21.230
statistics notation sometimes, for
the population size, is capital N.

00:01:21.230 --> 00:01:25.498
Sample size is lowercase n.

00:01:25.498 --> 00:01:31.875
Okay, and then what we did last
time was derive the variance,

00:01:34.778 --> 00:01:39.976
Of x, we decomposed x as a sum of
indicator random variables where

00:01:39.976 --> 00:01:46.100
xj is just the indicator of the jth
ball that you draw being white.

00:01:46.100 --> 00:01:49.055
And then using the stuff we
did last time for variance.

00:01:49.055 --> 00:01:52.818
The variance of the sum is the sum
of the variances, then plus,

00:01:52.818 --> 00:01:54.760
we have all these covariances.

00:01:54.760 --> 00:01:57.123
If they're independent, you don't
have to worry about the covariances.

00:01:57.123 --> 00:02:01.110
But in this case they're not independent,
so we need the covariance terms.

00:02:01.110 --> 00:02:05.329
So really this is just gonna
be a Var(X1) plus blah,

00:02:05.329 --> 00:02:10.205
blah, blah, plus Var(Xn), and
then all the covariances.

00:02:10.205 --> 00:02:14.424
And sorry, I'll put 2,
because I'm grouping covariance of X1 and

00:02:14.424 --> 00:02:18.320
X2 with covariance of X2 and
X1, group them together.

00:02:18.320 --> 00:02:24.750
So 2 times the sum over all i &lt; j Cov(Xi,
Xj).

00:02:28.491 --> 00:02:32.990
Okay, now here's where this would be,
it looks like a complicated sum.

00:02:32.990 --> 00:02:36.580
But we take advantage of symmetry,
like I was doing quickly the last time.

00:02:36.580 --> 00:02:39.970
But you should make sure that you
see the symmetry in this problem.

00:02:42.690 --> 00:02:46.430
Any of these Xj, the jth ball,
before you draw any balls,

00:02:46.430 --> 00:02:48.400
that's just equally likely
to be any of the balls.

00:02:48.400 --> 00:02:51.360
So we're not,
these are not conditional variances,

00:02:51.360 --> 00:02:53.510
these are the unconditional variances.

00:02:53.510 --> 00:02:55.661
By symmetry, they're all the same, so

00:02:55.661 --> 00:02:58.271
it's just n times the variance
of the first one.

00:02:58.271 --> 00:03:04.470
So n times Var(X1), well,
X1 is just Bernoulli p, right?

00:03:04.470 --> 00:03:06.366
So this is just n p (1-p).

00:03:08.611 --> 00:03:12.967
And then we need all these
covariance terms, and

00:03:12.967 --> 00:03:18.120
choose, by symmetry they're all the same,
again.

00:03:18.120 --> 00:03:21.120
So it's 2,
there's n choose 2 terms in this sum.

00:03:21.120 --> 00:03:24.580
So I'm just gonna write 2 (n choose 2),
then I don't have to do a sum anymore.

00:03:24.580 --> 00:03:31.990
So I'll go 2 (n choose 2), and
we want the covariance between X1 and X2.

00:03:31.990 --> 00:03:33.430
And we did this quickly last time, but

00:03:33.430 --> 00:03:36.428
it's important enough
to write it down again.

00:03:36.428 --> 00:03:42.130
Cov(X1, X2) =,
just to remind you of the definition,

00:03:42.130 --> 00:03:47.343
or the equivalent of the definition,
E(X1 X2)- (EX1)(EX2).

00:03:47.343 --> 00:03:51.390
So once we do, that's just how you
get covariance in general, I mean,

00:03:51.390 --> 00:03:52.680
that's always true.

00:03:52.680 --> 00:03:57.010
But once we have this, that will tell
us immediately what to put in here.

00:03:57.010 --> 00:03:58.324
At least immediately,

00:03:58.324 --> 00:04:02.538
once we think hard enough about indicator
random variables actually mean.

00:04:02.538 --> 00:04:06.810
Okay, so this part is just easy,
it's just (EX1)(EX2).

00:04:06.810 --> 00:04:11.970
And we already know that marginally,
these are just Bernoulli p's.

00:04:11.970 --> 00:04:14.830
They're not independent, but this is
just saying look at them separately,

00:04:14.830 --> 00:04:18.530
so those are just Bernoulli p's.

00:04:18.530 --> 00:04:23.970
So, that's very easy,
that's just gonna be p squared, that term.

00:04:23.970 --> 00:04:27.010
Now E(X1 X2), as I pointed at last time.

00:04:27.010 --> 00:04:29.352
If you multiply two
indicator random variables,

00:04:29.352 --> 00:04:33.060
that's just an indicator random
variable of the intersection.

00:04:33.060 --> 00:04:36.270
So this is just the event that
the first ball is white and

00:04:36.270 --> 00:04:37.740
the second ball is white.

00:04:37.740 --> 00:04:44.131
So first ball is w over w + b,
that's just p again,

00:04:44.131 --> 00:04:50.390
times w- 1 over w + b- 1,- p squared,
okay.

00:04:50.390 --> 00:04:51.881
So that looks messy.

00:04:51.881 --> 00:04:56.871
If you multiply everything and
then simplify and do the algebra,

00:04:56.871 --> 00:05:01.150
it actually comes out to
something surprisingly nice.

00:05:01.150 --> 00:05:06.340
So this is just algebra at this point,

00:05:06.340 --> 00:05:13.000
if you simplify it,
what you get is this factor.

00:05:13.000 --> 00:05:21.862
N- n over N- 1, times something
that looks familiar, n p (1-p).

00:05:21.862 --> 00:05:27.250
This part looks very familiar, right?

00:05:27.250 --> 00:05:28.950
That's just the variance of a binomial np.

00:05:29.980 --> 00:05:34.680
This factor in front, in statistics, is
called the finite population correction.

00:05:38.550 --> 00:05:43.153
And this answer, it's really neat
that it works out to something so

00:05:43.153 --> 00:05:45.670
simple and similar to the binomial.

00:05:45.670 --> 00:05:47.180
It looks like the binomial variance,

00:05:47.180 --> 00:05:51.550
it's we just need this extra
correction factor in front.

00:05:51.550 --> 00:05:54.753
And let's just check this in a couple
simple and extreme cases, right,

00:05:54.753 --> 00:05:57.132
I always recommend look at simple and
extreme cases.

00:05:57.132 --> 00:06:00.980
So one extreme case would
be of little n equals 1.

00:06:00.980 --> 00:06:07.079
Then this goes away, right, and
we just get the variance of a Bernoulli p.

00:06:07.079 --> 00:06:09.620
Well it had to be that way, right?

00:06:09.620 --> 00:06:12.450
Because if you're only picking one ball,
what difference does it make if it's with

00:06:12.450 --> 00:06:15.500
replacement or without replacement,
there's only one ball.

00:06:15.500 --> 00:06:16.495
So that makes sense, when n is 1.

00:06:16.495 --> 00:06:19.939
And now let's consider
another extreme case, so

00:06:19.939 --> 00:06:23.900
I'll just write that down, extreme cases.

00:06:23.900 --> 00:06:26.791
So one extreme case is N = 1.

00:06:26.791 --> 00:06:31.860
And the other extreme case is
if N is much, much, much larger,

00:06:31.860 --> 00:06:38.190
I'll just write much, much, not much,
much, much, larger than little n.

00:06:42.610 --> 00:06:48.400
Little n is say, 20, big N is 100,000.

00:06:49.553 --> 00:06:53.030
If that's the case,
this is extremely close to 1.

00:06:53.030 --> 00:06:57.332
Which says we're getting something
extremely close to the binomial variance,

00:06:57.332 --> 00:06:59.300
and that should make perfect sense.

00:06:59.300 --> 00:07:03.640
Because if the sample is so minuscule
compared to the population, it's very,

00:07:03.640 --> 00:07:08.063
very unlikely that you would sample
the same individual more than once, right?

00:07:08.063 --> 00:07:11.664
You're not doing replacement,
but what difference does it make,

00:07:11.664 --> 00:07:15.530
cuz it's unlikely to get the same
person twice anyway, in your sample.

00:07:15.530 --> 00:07:19.943
Okay, so it's gonna be close to
a binomial if this thing is close to 1,

00:07:19.943 --> 00:07:22.305
so that should make intuitive sense.

00:07:22.305 --> 00:07:26.102
All right, so
that's the variance of the hypergeometric.

00:07:26.102 --> 00:07:30.438
Okay, so I think we're ready for
a change of variables now,

00:07:30.438 --> 00:07:33.800
change of topic to change of variables.

00:07:33.800 --> 00:07:38.600
So change of variables is
synonymous with transformations.

00:07:40.822 --> 00:07:46.198
This is something we've done
before via other methods,

00:07:46.198 --> 00:07:50.190
but not as a topic in its own right.

00:07:50.190 --> 00:07:55.490
But hopefully, the method that we're gonna
write down, everything should look kind of

00:07:55.490 --> 00:07:57.880
natural, because we've already
done some similar stuff.

00:07:57.880 --> 00:08:00.490
So, and we've already been talking a lot

00:08:02.070 --> 00:08:05.350
About what happens when you have
a function of a random variable.

00:08:05.350 --> 00:08:07.880
A function of a random
variable is a random variable.

00:08:07.880 --> 00:08:11.840
And we use LOTUS a lot to get
its expected value, okay?

00:08:11.840 --> 00:08:14.350
But LOTUS is great but

00:08:14.350 --> 00:08:18.720
LOTUS only gives you the expected value
of that transformed random variable.

00:08:18.720 --> 00:08:20.530
It doesn't give you
the whole distribution.

00:08:20.530 --> 00:08:23.730
So a lot of times you don't just want the
mean or you just don't want the mean and

00:08:23.730 --> 00:08:30.180
the variance, you want the entire
distribution, well, how do you do that?

00:08:30.180 --> 00:08:36.640
So, let's state it as a theorem and
then, then do examples.

00:08:36.640 --> 00:08:43.520
So, it's more interesting
in the continuous case.

00:08:43.520 --> 00:08:46.390
So, I'm going to state this for
continuous random variables.

00:08:46.390 --> 00:08:49.346
So let X be a continuous random variable.

00:08:55.652 --> 00:09:01.186
With PDF, let's say,

00:09:01.186 --> 00:09:07.845
f sub x and let Y equals g of X.

00:09:07.845 --> 00:09:11.546
So transforming from X to Y by
multiplying some function g.

00:09:11.546 --> 00:09:14.615
We need to make some assumptions on g,

00:09:14.615 --> 00:09:19.660
if g is a really nasty function then
this may not work out very well.

00:09:19.660 --> 00:09:25.400
LOTUS will still be true, but
that doesn't give us the distribution.

00:09:25.400 --> 00:09:27.796
So let's assume to start with,

00:09:27.796 --> 00:09:31.806
let's assume that first of
all g is differentiable.

00:09:33.859 --> 00:09:38.146
So in particular it's continuous,
but it's stronger than that, we want

00:09:38.146 --> 00:09:42.784
the derivative of g to exist everywhere,
or at least everywhere of interest to us

00:09:44.930 --> 00:09:48.924
And let's assume that g
is strictly increasing,

00:09:56.028 --> 00:10:02.724
Okay, and then the question is
how do we get the PDF of Y And

00:10:02.724 --> 00:10:08.150
the answer, Is given by,

00:10:12.227 --> 00:10:16.180
fY of y equals, well,
we start with the PDF of X.

00:10:20.824 --> 00:10:24.250
And then we multiply by dx dy.

00:10:24.250 --> 00:10:29.400
And I just have to explain
the notation a little bit.

00:10:29.400 --> 00:10:32.770
Here we transformed
capital X to capital Y.

00:10:32.770 --> 00:10:37.200
So a natural thing to do is to mirror
that notation with the lowercase letters.

00:10:37.200 --> 00:10:44.170
So we're defining it to be true
that little y equals g of little x.

00:10:44.170 --> 00:10:46.420
So we're doing the same transformation.

00:10:48.090 --> 00:10:51.187
Now his looks a little
bit strange because,

00:10:53.563 --> 00:10:56.210
This is a function of y and
this is a function of x.

00:10:56.210 --> 00:10:57.330
And if I ask you for

00:10:57.330 --> 00:11:00.510
the PDF of y, I'm hoping you'll
give me a function of little y.

00:11:00.510 --> 00:11:03.030
And if you just write this down,
you have a function of little x.

00:11:03.030 --> 00:11:05.825
So the interpretation of this,

00:11:05.825 --> 00:11:11.216
it is that everything is then
written in terms of y, little y.

00:11:14.181 --> 00:11:22.579
It looks uglier if I write
it that way right now.

00:11:22.579 --> 00:11:26.183
Well, because I made these
assumptions that g is nice enough,

00:11:26.183 --> 00:11:30.580
g will have an inverse so we could
also write x equals g inverse of y.

00:11:30.580 --> 00:11:34.380
So all I'm saying to do is
plug in g inverse of y here,

00:11:34.380 --> 00:11:36.000
then it's a function of y.

00:11:36.000 --> 00:11:39.700
Dx dy is the derivative of x with
respect to y viewed as a function of y.

00:11:41.420 --> 00:11:44.900
And there are several variations on this.

00:11:44.900 --> 00:11:51.260
In particular,
you can also do dx dy is the same.

00:11:51.260 --> 00:11:55.590
This is just intro of calculus again,
but it's useful to point out.

00:11:56.720 --> 00:12:00.666
dx dy is the reciprocal of dy dx.

00:12:00.666 --> 00:12:03.305
That's just the Chain Rule.

00:12:03.305 --> 00:12:05.815
Remember from calculus
these look like fractions.

00:12:05.815 --> 00:12:09.565
They're not actually fractions but the
Chain Rule says they act like fractions.

00:12:09.565 --> 00:12:11.315
That's just the Chain Rule.

00:12:11.315 --> 00:12:16.240
So that says we have a choice in doing
this we can decide which is easier.

00:12:16.240 --> 00:12:20.770
Either we could do dx dy directly,
or we could take dy dx and flip it.

00:12:20.770 --> 00:12:24.610
And then we just have to remember to
write it as a function of y, either way.

00:12:24.610 --> 00:12:28.140
So there are a couple of choices for
how to use this, and

00:12:28.140 --> 00:12:31.760
you should think first about which
one's gonna be easier rather

00:12:31.760 --> 00:12:35.970
than just blindly jumping in
without actually thinking about it.

00:12:35.970 --> 00:12:39.260
Also, make sure to check the assumptions.

00:12:39.260 --> 00:12:41.280
Strictly increasing.

00:12:41.280 --> 00:12:45.750
So a common mistake on this kind of
problem would be to just try to blindly

00:12:45.750 --> 00:12:50.420
plug into this formula for
a function like g of X equals X squared.

00:12:50.420 --> 00:12:53.160
Now if g of X equals X squared is a very,
very nice function.

00:12:53.160 --> 00:12:55.970
I'm not saying that's not a nice function,
it's a parabola.

00:12:55.970 --> 00:12:58.190
But it's a u shape,
it goes down and then it goes up.

00:12:58.190 --> 00:13:02.830
So it's not strictly increasing so
you couldn't apply this.

00:13:02.830 --> 00:13:04.590
It doesn't mean we can't
solve the problem,

00:13:04.590 --> 00:13:08.460
it just means you have to go back
to first principles in that case.

00:13:08.460 --> 00:13:09.700
This would work, though,

00:13:09.700 --> 00:13:14.320
with g of X equals X squared if we're
dealing with positive random variables.

00:13:14.320 --> 00:13:17.520
Because then the negative
side doesn't come into play.

00:13:17.520 --> 00:13:20.250
But if you're dealing with both
negative and positive values and

00:13:20.250 --> 00:13:22.500
you're squaring it, that's not increasing.

00:13:22.500 --> 00:13:24.610
Okay, so you have to be careful
about things like that.

00:13:24.610 --> 00:13:27.400
Don't just plug into the formula all
without checking the assumptions.

00:13:28.480 --> 00:13:29.780
All right, so let's prove this.

00:13:29.780 --> 00:13:33.450
And the proof should be pretty easy,

00:13:33.450 --> 00:13:37.200
just based on kind of similar
calculations we did before.

00:13:37.200 --> 00:13:41.002
Like, in a sense,
this is easier than Universality, or

00:13:41.002 --> 00:13:44.025
things like that, that we've done before.

00:13:47.067 --> 00:13:50.313
Well, our proof is just
gonna be let's find the CDF,

00:13:50.313 --> 00:13:52.740
take the derivative to get the PDF.

00:13:52.740 --> 00:13:55.650
So it doesn't require any
great leaps of thought.

00:13:55.650 --> 00:13:57.470
We're just gonna find the CDF.

00:13:57.470 --> 00:13:59.600
We're gonna take the derivative,
that's it.

00:13:59.600 --> 00:14:01.095
So let's do that pretty quickly.

00:14:01.095 --> 00:14:08.073
The CDF of Y, Probability Y less than or

00:14:08.073 --> 00:14:13.088
equal to little y, equals,
I'm just gonna plug in the definition,

00:14:13.088 --> 00:14:18.541
g of X less than or
equal to, Little y, right?

00:14:20.090 --> 00:14:26.790
The derivative of the CDF is the PDF.

00:14:26.790 --> 00:14:32.810
So if we want we can write
this as P of X less than or

00:14:32.810 --> 00:14:39.530
equal to g inverse of y, because I

00:14:39.530 --> 00:14:43.340
am assuming this function has an inverse,
this event is equivalent to this event.

00:14:43.340 --> 00:14:46.550
That is you can get from here to here and
from here back, back to here.

00:14:46.550 --> 00:14:49.220
It's just the same event
written in a different way.

00:14:49.220 --> 00:14:55.810
But notice that this is just
the CDF of X evaluated here right.

00:14:55.810 --> 00:14:58.510
It's just the definition of CDF so
hopefully this is very,

00:14:58.510 --> 00:14:59.810
very familiar by now.

00:14:59.810 --> 00:15:05.010
That's just the CDF of x let's call
that F sub X of g inverse of y.

00:15:05.010 --> 00:15:10.750
And just to make the notation a little
bit nicer, really that's just FX of x.

00:15:10.750 --> 00:15:11.890
Cuz I defined it.

00:15:11.890 --> 00:15:14.000
I wrote over there x is g inverse of y.

00:15:14.000 --> 00:15:15.740
So that's an easier way to write it.

00:15:15.740 --> 00:15:17.670
So basically what that says is that for

00:15:17.670 --> 00:15:20.820
the CDF you don't really
have to do much of anything.

00:15:20.820 --> 00:15:24.990
But for the PDF you can't
just say this equals this,

00:15:24.990 --> 00:15:27.350
that you have this
derivative that comes up.

00:15:27.350 --> 00:15:28.860
That's from the Chain Rule.

00:15:28.860 --> 00:15:32.690
So now let's just take the derivative
of both sides, fy of y.

00:15:32.690 --> 00:15:37.900
Equals, I'm differentiating
with respect to Y.

00:15:37.900 --> 00:15:42.340
So chain rules says I can differentiate
first with respect to X and

00:15:42.340 --> 00:15:44.050
get the PDF of X.

00:15:44.050 --> 00:15:49.040
And then we have the correction
dx dy to correct for

00:15:49.040 --> 00:15:52.560
the fact that that we differentiate
both sides with respect to Y.

00:15:52.560 --> 00:15:56.980
Whereas to get to big FX to little fx
are differentiated with respect to X.

00:15:56.980 --> 00:16:00.217
So it's just the chain rule,
nothing else to it.

00:16:03.660 --> 00:16:05.990
All right, so
that's what we wanted to show.

00:16:07.540 --> 00:16:08.890
So let's do an example.

00:16:19.280 --> 00:16:25.073
So, here's a famous example
one of the most widely used

00:16:25.073 --> 00:16:30.629
distributions in practice
is called the log normal.

00:16:30.629 --> 00:16:38.090
And let's let Y =, log normal does
not mean the log of a normal.

00:16:38.090 --> 00:16:40.215
You can't take the log of
a normal random variable,

00:16:40.215 --> 00:16:42.397
because you can't take
the log of a negative number.

00:16:42.397 --> 00:16:46.860
Log normal means, so
this is log normal example.

00:16:46.860 --> 00:16:53.010
Log normal means that the log is normal,
not log of the normal.

00:16:53.010 --> 00:16:56.140
So if you take, z is standard normal here.

00:16:56.140 --> 00:16:59.760
More generally you could let
z be normal mu sigma squared.

00:16:59.760 --> 00:17:02.600
But let's just do the standard
normal case first.

00:17:02.600 --> 00:17:06.020
So if I take the log of this,
I'll get z which is normal.

00:17:06.020 --> 00:17:09.307
So that's why it's called
log normal log is normal.

00:17:09.307 --> 00:17:14.857
So we actually had a homework
problem about this before, right.

00:17:14.857 --> 00:17:19.968
Where if you did the problem,
what you did was to use the MGF

00:17:19.968 --> 00:17:25.690
of the normal to find moments
of the log normal, okay?

00:17:25.690 --> 00:17:29.460
But that's just moments,
right now we want the entire PDF, okay?

00:17:29.460 --> 00:17:30.580
We want the distribution.

00:17:32.020 --> 00:17:35.590
So this is an increasing,
this transformation,

00:17:35.590 --> 00:17:37.971
this is an increasing function.

00:17:37.971 --> 00:17:40.220
It's infinitely differentiable, right?

00:17:40.220 --> 00:17:41.720
It's a very, very nice function,

00:17:41.720 --> 00:17:45.290
so there's no problem with
applying that result.

00:17:45.290 --> 00:17:49.002
And we can just immediately therefore,

00:17:49.002 --> 00:17:54.034
write down the PDF,
fy(y) = let's do it here.

00:17:54.034 --> 00:17:58.800
Fy(y) =, so I'm just gonna write down

00:17:58.800 --> 00:18:03.735
the standard normal x
is z in this example.

00:18:03.735 --> 00:18:10.888
1 over root 2 pi,
e to the minus z squared over 2,

00:18:10.888 --> 00:18:17.830
except that I said that we have to
express it as a function of Y, right?

00:18:17.830 --> 00:18:22.945
So z is log y, so
instead of writing z squared over 2,

00:18:22.945 --> 00:18:26.718
I'm gonna write log y squared over 2.

00:18:29.340 --> 00:18:35.607
So this would be the normal density,
except plugging in log y in for z, and

00:18:35.607 --> 00:18:41.385
then it says according to this we
also have to multiply it by dz/dy.

00:18:41.385 --> 00:18:46.828
So over here,
let's just compute the derivative,

00:18:46.828 --> 00:18:51.766
dy/dz, =,

00:18:51.766 --> 00:18:55.353
just the derivative is dy/dz equals

00:18:55.353 --> 00:18:59.990
the derivative of e to the z,
z to the z, right?

00:18:59.990 --> 00:19:04.223
But I wanna write that
in terms of y instead,

00:19:04.223 --> 00:19:09.611
e to the z in terms of y is y so
that wasn't too difficult.

00:19:09.611 --> 00:19:15.141
And then we just need to be careful
about do we multiply it by dy/dz here or

00:19:15.141 --> 00:19:16.509
dz/dy, right?

00:19:16.509 --> 00:19:18.740
But that says dx/dy.

00:19:18.740 --> 00:19:22.362
So it's the reciprocal of this,
so we're gonna just put a 1/y.

00:19:22.362 --> 00:19:23.996
And this is for y &gt; 0.

00:19:25.820 --> 00:19:26.772
So that's gonna be the PDF.

00:19:30.320 --> 00:19:33.389
By the way if you ever forget
whether this is dx/dy, or

00:19:33.389 --> 00:19:36.807
dy/dx well, I mean it shouldn't
take long to rederive it.

00:19:36.807 --> 00:19:41.737
But kind of mnemonic is kind of pretend
that the dy is over there then it looks

00:19:41.737 --> 00:19:48.130
really nice and symmetrical f(y)/dy =
f(x)/dx, is a handy way to remember it.

00:19:48.130 --> 00:19:53.185
And you may have been taught that it's
not ok to separate the dx from the dy,

00:19:53.185 --> 00:19:58.092
but when you go further in math then
people start separating them again.

00:19:58.092 --> 00:20:02.394
And as long as you're careful about what
things mean it is possible to do that if

00:20:02.394 --> 00:20:06.060
you interpret it correctly, but
I just think of that as an amonic.

00:20:06.060 --> 00:20:11.120
So, if I every write f(y)dx equals
f(x)dx just think of that as a notation,

00:20:11.120 --> 00:20:12.440
that means exactly this.

00:20:14.078 --> 00:20:17.758
All right, so
the proof was pretty short for this, and

00:20:17.758 --> 00:20:20.010
an example just use the formula.

00:20:20.010 --> 00:20:23.328
Pretty straight forward as
long as the conditions apply.

00:20:23.328 --> 00:20:28.462
But while we're on this topic,
let's do the multidimensional version,

00:20:28.462 --> 00:20:32.710
which looks uglier, but
it's conceptually the same thing.

00:20:35.361 --> 00:20:42.210
So now we're gonna have
transformations in n dimensions.

00:20:42.210 --> 00:20:49.198
So transformations again, but
now we have the multidimensional version.

00:20:54.725 --> 00:20:59.819
Okay, so now we think of Y and X as,

00:21:01.850 --> 00:21:06.501
Random vectors, so Y = g(x),

00:21:06.501 --> 00:21:10.487
where g is a mapping from Rn to

00:21:10.487 --> 00:21:15.857
Rn Random vector just think of

00:21:15.857 --> 00:21:22.030
that as a list of random variable so
this Y is really just Y1 through Yn.

00:21:22.030 --> 00:21:23.983
So it's not really a new concept.

00:21:23.983 --> 00:21:26.775
It just means we took our
n random variables and

00:21:26.775 --> 00:21:29.430
listed them together as one vector, okay.

00:21:29.430 --> 00:21:35.460
And so we have a mapping from Rn to
itself that we're doing a transformation.

00:21:35.460 --> 00:21:39.814
And then the problem is, and

00:21:39.814 --> 00:21:46.346
let's assume that the X = x1 through xn,

00:21:46.346 --> 00:21:50.708
that vector is continuous.

00:21:50.708 --> 00:21:53.320
That is, it's a continuous random vector.

00:21:53.320 --> 00:21:56.221
In other words, we just have
some joint PDF, right, cuz we've

00:21:56.221 --> 00:21:59.737
been talking about joint PDFs, so
that's a familiar concept at this point.

00:21:59.737 --> 00:22:04.139
So we have this joint PDF, and we do
this thing, and then the question is,

00:22:04.139 --> 00:22:06.270
what's the joint PDF of Y, right?

00:22:06.270 --> 00:22:09.726
So it's completely analogous,
just higher dimensional.

00:22:09.726 --> 00:22:11.442
So I'm not gonna prove
that the analog holds,

00:22:11.442 --> 00:22:14.171
because that's just basically
an exercise in multivariable calculus,

00:22:14.171 --> 00:22:16.120
which is not really that relevent for
our purposes.

00:22:16.120 --> 00:22:20.131
It's completely analogous,
so depending on how much

00:22:20.131 --> 00:22:24.928
multivariable calculus you've
done you could either prove it or

00:22:24.928 --> 00:22:29.218
just accept it as analogous,
cuz it is analogous, okay?

00:22:29.218 --> 00:22:38.060
So we want the joint PDF of Y,
In terms of the joint PDF of X, right?

00:22:39.150 --> 00:22:46.039
So I'm just gonna write down the analogous
equation to that, f sub Y(Y),

00:22:49.410 --> 00:22:55.711
I'm just using that as notation for
the join PDF, = the joint PDF of X.

00:22:59.369 --> 00:23:02.140
Times dx/dy.

00:23:08.240 --> 00:23:10.468
The only problem with this is,

00:23:10.468 --> 00:23:16.138
how do we interpret the derivative of
this vector with respect to that vector?

00:23:16.138 --> 00:23:20.848
What does that actually mean?

00:23:20.848 --> 00:23:25.350
Well this thing, and actually we wanna
put absolute value symbols around it.

00:23:26.620 --> 00:23:30.943
By the way if this function were strictly
decreasing we could do the same thing

00:23:30.943 --> 00:23:33.347
just by sticking absolute values in here.

00:23:33.347 --> 00:23:34.927
If we forgot the absolute values,

00:23:34.927 --> 00:23:37.380
we're gonna get a negative
PDF which makes no sense.

00:23:38.720 --> 00:23:40.993
All right, so I just have to
tell you what this thing means.

00:23:40.993 --> 00:23:44.250
Well this thing is called the Jacobian.

00:23:44.250 --> 00:23:46.088
And I'm sure some of you have seen it but

00:23:46.088 --> 00:23:48.940
I'm not necessarily assuming
that you've seen it before.

00:23:48.940 --> 00:23:51.710
I mean it's standard
multivariable calculus thing.

00:23:51.710 --> 00:23:54.340
And all the Jacobean,
if you haven't seen it before,

00:23:54.340 --> 00:24:00.410
all the Jacobian is it's the matrix
of all possible partial derivatives.

00:24:00.410 --> 00:24:03.510
And as I said on the first day of class,
if you know how to an ordinary derivative,

00:24:03.510 --> 00:24:06.346
you know how to do a partial derivative,
you just hold everything else constant.

00:24:06.346 --> 00:24:13.480
So dx dy equals just to write it out.

00:24:13.480 --> 00:24:18.510
It's a matrix of all possible
partial derivatives.

00:24:18.510 --> 00:24:21.920
So, what we do is we take, X is a vector.

00:24:21.920 --> 00:24:23.730
We take the first coordinate of X,

00:24:23.730 --> 00:24:28.520
we differentiate it with respect
to all the coordinates of Y.

00:24:28.520 --> 00:24:33.160
So we go dxd1 dy1,
dxd1 dy2, blah, blah, blah,

00:24:33.160 --> 00:24:39.110
dx1 dyn And then we would take x2 for
the second row,

00:24:39.110 --> 00:24:43.878
do the same thing, and we keep going till
we've done all the partial derivatives.

00:24:43.878 --> 00:24:49.290
dxn dy1 blah blah blah dxn dyn.

00:24:52.886 --> 00:24:55.629
So it's just a matrix of all
possible partial derivatives.

00:24:56.660 --> 00:25:02.880
Now it doesn't make much sense to stick
in a matrix here, so actually, these

00:25:02.880 --> 00:25:08.475
absolute value symbols actually mean take
the absolute value of the determinant.

00:25:09.880 --> 00:25:13.881
So we're taking this Jacobian matrix and
we take the determinant of it.

00:25:14.930 --> 00:25:15.970
Take the absolute value.

00:25:21.766 --> 00:25:24.622
That's the analog of this
formula we just checking up,

00:25:24.622 --> 00:25:28.510
we have this matrix somehow we need to
compress the matrix down to a number and

00:25:28.510 --> 00:25:31.580
it turns out the right way to
do that is using a determinant.

00:25:33.000 --> 00:25:37.050
And like in the other
case we could also do,

00:25:37.050 --> 00:25:41.540
we could choose to do it this way or
we could have done dy dx.

00:25:45.460 --> 00:25:48.080
This says dude you're
copying the other way around,

00:25:48.080 --> 00:25:51.300
right take all the partials
of y with respect to x.

00:25:51.300 --> 00:25:54.630
Okay, we could have done that and
then take another reciprocal, and

00:25:54.630 --> 00:25:55.510
it would be the same thing.

00:25:57.070 --> 00:26:01.320
So sometimes one of these two methods
is much easier than the other, so

00:26:01.320 --> 00:26:04.730
you wanna think first about which
direction to do the transformation in.

00:26:04.730 --> 00:26:09.390
A lot of books just write
the Jacobian as J, and

00:26:09.390 --> 00:26:13.859
I like the letter J a lot, but
I don't like that notation here because

00:26:15.110 --> 00:26:18.740
it doesn't tell you which way are you
going, from x to y or y to x.

00:26:18.740 --> 00:26:20.610
So all right.
This way then it's very obvious that just

00:26:20.610 --> 00:26:25.090
says take derivatives of the x's with
respect to the y's, that has to be this.

00:26:25.090 --> 00:26:27.100
And this one would be
the other way around.

00:26:27.100 --> 00:26:29.610
You can do either way as long
as you're careful about whether

00:26:29.610 --> 00:26:31.190
to do the reciprocal here or not.

00:26:31.190 --> 00:26:37.740
All right, and so, that's the Jacobian.

00:26:37.740 --> 00:26:41.060
One other calculus thing

00:26:42.990 --> 00:26:46.930
that we should discuss
briefly is convolution.

00:26:49.040 --> 00:26:51.840
Convolution is something
we've done already,

00:26:52.960 --> 00:26:55.040
just like we've already
done some transformations.

00:26:55.040 --> 00:26:58.780
But I just wanted to mention
it as its own topic briefly.

00:26:58.780 --> 00:27:02.630
Convolution is just the fancy word for
sums.

00:27:02.630 --> 00:27:05.760
That is we want the distribution
of a sum of random variables.

00:27:06.780 --> 00:27:10.550
Remember for the binomial we
did a convolution of binomials

00:27:10.550 --> 00:27:14.130
using story proof as long
as they have the same P.

00:27:14.130 --> 00:27:16.950
And for Poissons and
Normals, we used the MGF and

00:27:16.950 --> 00:27:19.090
all of those are pretty easy calculations.

00:27:19.090 --> 00:27:22.150
But sometimes you can't find
a story that will help you and

00:27:22.150 --> 00:27:26.110
the MGF may not exist or
you may not know how to work with the MGF.

00:27:26.110 --> 00:27:28.850
Sometimes we need a more direct method.

00:27:28.850 --> 00:27:33.790
So in the discrete case,
we've already done calculations like this.

00:27:33.790 --> 00:27:37.717
So we want, so let's let T equal X plus Y.

00:27:37.717 --> 00:27:43.170
And we want to know the distribution of T
assuming we know the distribution of X and

00:27:43.170 --> 00:27:45.010
the distribution of Y that's
called the convolution.

00:27:47.170 --> 00:27:53.010
So, in the discrete case, we can just
immediately write down a formula.

00:27:53.010 --> 00:27:56.640
It maybe a messy formula it may or may
not be something we can actually do but

00:27:56.640 --> 00:27:58.260
at least we have an expression.

00:27:58.260 --> 00:28:01.770
So in the discreet case we can immediately
just well what's the probability

00:28:01.770 --> 00:28:03.013
that T equals t?

00:28:03.013 --> 00:28:11.410
Well, that's just the sum over,
how can I get a total equal to T?

00:28:11.410 --> 00:28:16.220
Well, X has to be something and
Y has to be whatever makes that up to T.

00:28:16.220 --> 00:28:20.330
You can think of this as
just conditioning on X.

00:28:20.330 --> 00:28:25.900
But I'll write it just as you're
using the actions of probability,

00:28:25.900 --> 00:28:28.070
breaking this up into disjoint cases.

00:28:28.070 --> 00:28:32.150
So I'm just gonna sum over,
always I can make the total equal x.

00:28:32.150 --> 00:28:33.450
So I can immediately just write it down.

00:28:33.450 --> 00:28:37.440
This is P of X equals little x.

00:28:37.440 --> 00:28:39.550
We're assuming that X and
Y are independent here.

00:28:39.550 --> 00:28:41.868
It's much nastier if they're dependent.

00:28:41.868 --> 00:28:47.720
Probability Y equals t minus x.

00:28:47.720 --> 00:28:50.980
We're summing over all x
such that this is positive.

00:28:53.100 --> 00:28:55.090
So we don't need a separate proof for
this.

00:28:55.090 --> 00:28:58.820
This just says to get the total equal P,
X has to be something and

00:28:58.820 --> 00:29:01.390
Y has to be whatever makes the total P.

00:29:01.390 --> 00:29:02.850
It has to be that way.

00:29:02.850 --> 00:29:06.920
And because I assumed independence I
split it up into two probabilities.

00:29:06.920 --> 00:29:10.090
So that's true for the discreet case.

00:29:10.090 --> 00:29:13.200
Now lets write down something
analagous in the continuance case.

00:29:17.256 --> 00:29:19.440
So now we want the PDF instead.

00:29:19.440 --> 00:29:25.010
And I'm gonna write down something
that looks completely analagous.

00:29:28.106 --> 00:29:31.680
That is, instead of doing the,
this is the PMF.

00:29:31.680 --> 00:29:35.163
So cuz it's continuous,
I'm gonna replace the PMF by the PDF.

00:29:37.385 --> 00:29:39.540
Let's go from minus infinity to infinity.

00:29:40.740 --> 00:29:44.290
This is the PMF of y,
evaluated at t minus x.

00:29:44.290 --> 00:29:49.566
I'll replace that by the PDF
evaluated at t minus x dx.

00:29:52.962 --> 00:29:55.083
This is true.

00:29:57.801 --> 00:30:02.148
And the easiest way to remember this
result is by thinking by analogy,

00:30:02.148 --> 00:30:02.890
with this.

00:30:03.990 --> 00:30:07.220
However, that's not a proof.

00:30:07.220 --> 00:30:08.890
That's just an analogy.

00:30:08.890 --> 00:30:13.100
And on the new homework, you'll see an
example where if you try to reason kind of

00:30:13.100 --> 00:30:19.540
an analogous way for a product, instead
of a sum, well, you'll see what happens.

00:30:19.540 --> 00:30:22.300
This requires more justification.

00:30:22.300 --> 00:30:25.240
There are several ways to justify this.

00:30:25.240 --> 00:30:29.873
Probably the simplest way
would be to take the CDF.

00:30:37.979 --> 00:30:40.540
Let's do the CDF take the derivative and
get the PDF.

00:30:41.840 --> 00:30:43.180
So what's the CDF?

00:30:43.180 --> 00:30:48.030
Well, for the cdf let's use
the continuous probability.

00:30:48.030 --> 00:30:53.902
So we're integrating the probability
that X plus Y is less than or

00:30:53.902 --> 00:30:58.120
equal to little t given
x times the PDF of x.

00:30:58.120 --> 00:31:01.310
This is one way to do it there
are other ways to do this calculation.

00:31:01.310 --> 00:31:02.760
But I like this one.

00:31:02.760 --> 00:31:05.400
That's just the continuous
law of probability.

00:31:07.180 --> 00:31:09.214
Now we plug in X equals x.

00:31:12.323 --> 00:31:16.820
And once we've plugged in X equals x,
we can drop the condition because X and

00:31:16.820 --> 00:31:18.590
Y are independent.

00:31:18.590 --> 00:31:23.260
And so
then all we have is the integral of,

00:31:23.260 --> 00:31:27.690
notice what's left here, just in your
mind replace big X by little x and

00:31:27.690 --> 00:31:29.700
move it over to that
side of the inequality.

00:31:29.700 --> 00:31:32.758
So it says Y less than equal t minus x,

00:31:32.758 --> 00:31:36.718
that's just the CDF of Y
evaluated at t minus x.

00:31:46.557 --> 00:31:49.720
Now take the derivative of
both sides of this equation.

00:31:51.110 --> 00:31:53.800
Derivative with respect to t,

00:31:53.800 --> 00:31:58.030
and then there's a theorem that says you
can swap the derivative and the integral.

00:31:58.030 --> 00:32:01.700
Derivative of this CDF Is the PDF so

00:32:01.700 --> 00:32:05.250
it would get that way, so
it requires some justification.

00:32:05.250 --> 00:32:09.858
Usually I would like to avoid doing
convolution integrals like this but

00:32:09.858 --> 00:32:11.944
sometimes you can't avoid it.

00:32:11.944 --> 00:32:14.509
But if possible try to use a story or
an MGF or

00:32:14.509 --> 00:32:18.799
one of the other things we've done but
sometimes you need that, yeah?

00:32:18.799 --> 00:32:22.695
&gt;&gt; [INAUDIBLE]
&gt;&gt; This is capital F, this-

00:32:22.695 --> 00:32:24.735
&gt;&gt; [INAUDIBLE]

00:32:25.990 --> 00:32:26.760
&gt;&gt; Yeah, sorry,

00:32:26.760 --> 00:32:29.200
this is F sub t(t), thank you.

00:32:29.200 --> 00:32:33.325
That's the CDF of capital T, thanks.

00:32:33.325 --> 00:32:38.790
Okay, so
my favorite thing about statistics

00:32:38.790 --> 00:32:42.890
is that you can do things that
are beautiful and useful.

00:32:42.890 --> 00:32:47.880
And Jacobeans are,
it's an extremely useful technical tool,

00:32:47.880 --> 00:32:51.176
but I've never heard anyone
describe Jacobeans as beautiful.

00:32:51.176 --> 00:32:56.239
So to kind of rebalance our
beauty quotient for today,

00:32:56.239 --> 00:33:03.407
let's do something completely different
that involves no calculus at all.

00:33:08.542 --> 00:33:14.875
Something that,
you can see whether you agree with me or

00:33:14.875 --> 00:33:23.364
not but, this is something that beauty
is not really an adequate word for,

00:33:23.364 --> 00:33:30.280
this is not something I would
consider existential, okay?

00:33:30.280 --> 00:33:32.919
So, here's an idea.

00:33:38.633 --> 00:33:45.040
The idea is you can use probability
to prove existence of up, okay?

00:33:45.040 --> 00:33:49.604
So we're gonna prove existence,
what does that mean?

00:33:52.737 --> 00:33:57.903
We're gonna prove existence of
objects with desired properties.

00:34:04.292 --> 00:34:05.737
Using probability.

00:34:07.419 --> 00:34:09.060
Properties using probability.

00:34:09.060 --> 00:34:12.230
So that's a very general idea.

00:34:12.230 --> 00:34:15.120
So let me just tell you
mathematically what's the idea.

00:34:17.220 --> 00:34:21.997
The idea is, I wanna show

00:34:21.997 --> 00:34:26.440
that an object with
a certain property exists.

00:34:27.560 --> 00:34:33.109
One way to show that
would be to show that,

00:34:33.109 --> 00:34:37.400
let's say desired property A.

00:34:38.830 --> 00:34:40.660
That is A is some property, okay?

00:34:40.660 --> 00:34:43.880
I'm saying this very generally but
we'll do an example.

00:34:43.880 --> 00:34:46.630
So we want to show there's
an object with a certain property,

00:34:46.630 --> 00:34:49.710
that sounds like it has nothing whatsoever
to do with probability and statistics.

00:34:49.710 --> 00:34:52.510
That's just like if you searched
everywhere in the universe,

00:34:52.510 --> 00:34:56.060
either mathematically or whatever,
could you ever find this thing?

00:34:56.060 --> 00:34:59.030
I didn't say anything about randomness or
uncertainty, okay?

00:34:59.030 --> 00:35:05.000
Here's the strategy,
so this is a strategy.

00:35:05.000 --> 00:35:09.590
We're gonna show that P(a) is
greater than 0, for a random object.

00:35:11.910 --> 00:35:17.010
We get to choose how to define random,
that is we just have this universe

00:35:17.010 --> 00:35:22.660
of objects and we decide on some method
for randomly selecting an object.

00:35:22.660 --> 00:35:25.290
So if it is a finite set,
the most obvious thing to do is just

00:35:25.290 --> 00:35:27.740
pick one at random where they're
all equally likely, right?

00:35:27.740 --> 00:35:30.820
If I have a million objects there's
no probability anywhere but

00:35:30.820 --> 00:35:34.260
I say well just pick one at
random equally likely, okay?

00:35:34.260 --> 00:35:39.950
And then let A be the event that the
randomly chosen object has the property.

00:35:39.950 --> 00:35:48.680
Well, is it clear that if the probability
is non 0 then there must exist one?

00:35:48.680 --> 00:35:51.820
Well, of course, if it didn't exist,
the probability would be 0, so

00:35:51.820 --> 00:35:55.040
if the probability is positive,
it must exist.

00:35:55.040 --> 00:35:58.740
So if we can show this,
we've shown that it exists.

00:35:58.740 --> 00:36:00.936
And that sounds like, so this is true,

00:36:00.936 --> 00:36:03.477
I mean I don't need to write a proof for
that.

00:36:03.477 --> 00:36:08.311
But that sounds like a very,
very wishful thinking strategy,

00:36:08.311 --> 00:36:13.326
that if we can't even exhibit
existence of even one such object,

00:36:13.326 --> 00:36:16.988
how are we ever gonna
compute the probability?

00:36:16.988 --> 00:36:20.223
We can't even find one, but
we're gonna compute its probability,

00:36:20.223 --> 00:36:21.350
that's pretty weird.

00:36:22.450 --> 00:36:26.250
Notice that we don't actually
have to compute P(A) exactly,

00:36:26.250 --> 00:36:30.260
we only need a bound that shows
that it's greater than 0, okay?

00:36:30.260 --> 00:36:33.068
WE don't need to know exactly P(A),
just that it's positive.

00:36:33.068 --> 00:36:39.080
That's method 1,
let's extend this a little bit.

00:36:39.080 --> 00:36:44.930
Suppose each object has
a number associated with it,

00:36:44.930 --> 00:36:46.350
let's think of a score.

00:36:51.380 --> 00:36:54.868
So we have this universe of objects,
no probability yet,

00:36:54.868 --> 00:36:58.725
each object has a number attached to it,
so some kind of a score.

00:37:01.553 --> 00:37:06.330
We wanna show there exists
an object with a good score.

00:37:06.330 --> 00:37:09.750
To say what does good mean,
I will talk a little bit about that.

00:37:09.750 --> 00:37:13.310
We wanna show there is an object
with a good score, but

00:37:13.310 --> 00:37:16.830
suppose it's really hard to actually
find one that has a good score.

00:37:18.850 --> 00:37:23.610
show there is an object with a good score,
I had to say what good means.

00:37:29.230 --> 00:37:30.660
Well here's the strategy.

00:37:32.860 --> 00:37:35.220
You may guess this has something
to do with probability.

00:37:35.220 --> 00:37:38.710
Pick a random object again,
look at its score.

00:37:38.710 --> 00:37:41.828
So, in other words, what's
the average score of a random object?

00:37:43.430 --> 00:37:45.941
Now, here's the theorem.

00:37:45.941 --> 00:37:51.732
There is an object, Whose

00:37:51.732 --> 00:37:57.362
score is at least the average, right?

00:38:02.165 --> 00:38:06.670
Let's just call it E(X), where this
is the score of a random object.

00:38:06.670 --> 00:38:10.561
So, we're defining a random
variable by taking a random object,

00:38:10.561 --> 00:38:12.651
find its score, take the average.

00:38:16.086 --> 00:38:17.508
Well obviously,

00:38:17.508 --> 00:38:23.410
there must be at least one object
that at least is the average, right?

00:38:23.410 --> 00:38:27.510
They can't all be below average,
that would make no sense, right?

00:38:27.510 --> 00:38:31.390
So therefore, if now of course,
E of X may be pretty lousy.

00:38:31.390 --> 00:38:34.010
But if E of X is actually pretty good then

00:38:34.010 --> 00:38:37.180
we've shown that there exists a good
one without actually exhibiting it.

00:38:38.330 --> 00:38:44.825
So again that sounds like a group
of people, and at least one person

00:38:44.825 --> 00:38:48.285
has to have at least the average salary of
the people in the room, things like that.

00:38:48.285 --> 00:38:50.065
That's an extremely crude statement.

00:38:50.065 --> 00:38:52.345
Is that ever gonna be useful for anything?

00:38:52.345 --> 00:38:56.235
I think this is a neat idea,
but is it actually useful?

00:38:56.235 --> 00:39:00.565
Well, what I consider one
of the most beautiful and

00:39:00.565 --> 00:39:04.880
useful results of the 20th
century was Shannon's theorem.

00:39:04.880 --> 00:39:08.400
Claude Shannon is the father
of information theory.

00:39:08.400 --> 00:39:11.250
Also the father of this
modern communications theory.

00:39:11.250 --> 00:39:14.730
So anytime you use the cell phone,that's
all based on communication and

00:39:14.730 --> 00:39:18.390
coding theory that goes
back to Shannon's work.

00:39:18.390 --> 00:39:20.240
So you can thank Shannon for this.

00:39:20.240 --> 00:39:23.430
Let me just tell you,
this is not an information theory course.

00:39:23.430 --> 00:39:27.780
It's a really amazing idea that you
can quantify information, though.

00:39:27.780 --> 00:39:32.862
But let me tell you very briefly
what one of Shannon's theorems was.

00:39:32.862 --> 00:39:37.210
Shannon theorem, was that he showed that
if you're trying to communicate over

00:39:37.210 --> 00:39:41.810
a noisy channel, so you're trying to send
messages from one place to another, but

00:39:41.810 --> 00:39:45.900
bits get corrupted, things, there's a lot
of noise and interference, or whatever.

00:39:47.630 --> 00:39:50.910
He showed that there's something
called a capacity of the channel, and

00:39:50.910 --> 00:39:54.430
you can communicate at rates
arbitrarily close to the capacity,

00:39:54.430 --> 00:39:58.730
with arbitrarily small chance of error
That is even if you have a very,

00:39:58.730 --> 00:40:03.900
very noisy channel, you can make
the air probability very, very low.

00:40:03.900 --> 00:40:06.560
That sounds like a very difficult theorem.

00:40:06.560 --> 00:40:09.000
And no one out he proved this in 1948.

00:40:09.000 --> 00:40:13.070
No one else was even close to
thinking of that as far as I know.

00:40:14.620 --> 00:40:20.300
The way he proved it, that there
exists what he called a good code,

00:40:20.300 --> 00:40:22.880
right, a good code is gonna
be one that works well for

00:40:22.880 --> 00:40:24.920
sending messages across
this noisy channel.

00:40:26.350 --> 00:40:31.130
The way he showed that a good code
exists was to pick a random code.

00:40:31.130 --> 00:40:34.110
And that's like kinda the most
daring thing you can imagine,

00:40:34.110 --> 00:40:39.290
he probably spent months trying to
actually find one couldn't find one so

00:40:39.290 --> 00:40:40.420
he picked a random one.

00:40:40.420 --> 00:40:44.560
And to think that a random one is actually
gonna do well is kinda unbelievable, and

00:40:44.560 --> 00:40:46.100
it turns out to be true.

00:40:46.100 --> 00:40:47.310
It was only 30 or

00:40:47.310 --> 00:40:52.490
40 years later that people actually
explicitly could write down a good code.

00:40:52.490 --> 00:40:55.780
Until then Shannon showed that
they exist because a random

00:40:55.780 --> 00:40:57.430
one has the right properties.

00:40:57.430 --> 00:41:02.650
Even though you can't actually write down
a specific one, without a lot of work.

00:41:02.650 --> 00:41:07.180
That's one of the most amazing results,
just mathematically extremely beautiful,

00:41:07.180 --> 00:41:11.090
but it underlies all of modern
communication and information theory.

00:41:11.090 --> 00:41:15.750
All right, so I'm not going try to
prove Shannon's theorem in ten, or

00:41:15.750 --> 00:41:19.240
five minutes, but
I am going to do one quick example.

00:41:19.240 --> 00:41:20.250
Along these lines so

00:41:20.250 --> 00:41:24.090
I just made up a simple example
just to illustrate this idea.

00:41:25.300 --> 00:41:29.826
So the idea is, and here is the problem.

00:41:34.443 --> 00:41:35.660
So suppose we have.

00:41:35.660 --> 00:41:41.610
100 people, I just made up some numbers,

00:41:41.610 --> 00:41:46.100
just so that we can actually do something
reasonably concrete and simple,

00:41:46.100 --> 00:41:49.540
just to show you how this idea
would work in a small example.

00:41:50.760 --> 00:41:54.726
Okay so there are 100 people and

00:41:54.726 --> 00:41:58.540
those people form committees.

00:41:58.540 --> 00:42:01.200
Now one person can be on
more than committee, so

00:42:01.200 --> 00:42:05.460
let's assume that there are,
how many committees do I want?

00:42:05.460 --> 00:42:09.740
I made up some numbers last night
I think I wanted 15 committees.

00:42:09.740 --> 00:42:13.110
I just made up some numbers where it works
out nicely but we can try this something

00:42:13.110 --> 00:42:16.010
more general like M and N and
whatever, but I made up some numbers.

00:42:16.010 --> 00:42:20.260
15 committees of 20,
that is each committee has 20 people.

00:42:21.260 --> 00:42:25.580
So I chose these numbers such that
15 times 20 is 300 which means that

00:42:25.580 --> 00:42:28.180
if everyone is on the same
number committees,

00:42:28.180 --> 00:42:31.100
than that means each person
is on three committees.

00:42:31.100 --> 00:42:33.960
You can generalize this to cases where
different people can be on different

00:42:33.960 --> 00:42:34.850
numbers of committees.

00:42:34.850 --> 00:42:38.638
But well, for simplicity, let's assume
each person is on three committees.

00:42:42.682 --> 00:42:44.970
No probability yet so far, right?

00:42:44.970 --> 00:42:48.300
That's just okay,
there's different way to do it.

00:42:48.300 --> 00:42:49.840
You can think of it as a counting problem,

00:42:49.840 --> 00:42:54.000
how many ways are there to do it,
there's some vast number of possibilities.

00:42:55.050 --> 00:42:56.365
Okay, now here's the problem.

00:42:58.897 --> 00:43:03.552
The problem is to show
that there exist two

00:43:03.552 --> 00:43:08.870
committees whos overlap is at least three.

00:43:08.870 --> 00:43:10.550
So I can find two committees, or

00:43:10.550 --> 00:43:15.480
there exist two committees, where a group
of three people is on both committees.

00:43:15.480 --> 00:43:19.621
So show there exists, two committees

00:43:22.875 --> 00:43:26.085
With overlap greater than or equal to 3.

00:43:29.780 --> 00:43:33.928
All right, so clearly the way to solve
this is not gonna be like write down

00:43:33.928 --> 00:43:37.668
every possible [INAUDIBLE] committees and
then search through and

00:43:37.668 --> 00:43:41.748
find over the computer all the overall
laps, all the intersections and

00:43:41.748 --> 00:43:43.780
go through everything right?

00:43:43.780 --> 00:43:45.230
That would be a nightmare.

00:43:48.000 --> 00:43:50.790
So we're going to use this idea and
we're gonna prove existence.

00:43:50.790 --> 00:43:52.490
This is an existence problem.

00:43:52.490 --> 00:43:56.200
We're gonna prove existence
just by computing the average.

00:43:57.320 --> 00:44:05.610
So the idea is find the average
intersection I said average.

00:44:05.610 --> 00:44:06.810
That involves probability.

00:44:06.810 --> 00:44:08.420
We didn't have any probability yet.

00:44:08.420 --> 00:44:11.690
We introduce our own probability
structure by just saying

00:44:11.690 --> 00:44:14.270
let's just choose two random committees.

00:44:15.490 --> 00:44:20.718
So find average overlap
of two random committees

00:44:24.982 --> 00:44:27.960
All right, so
hopefully we can do that quickly.

00:44:27.960 --> 00:44:31.720
So, I'll just write E,
you can make up some fancy notation and

00:44:31.720 --> 00:44:34.180
stuff, but we're just picking two,

00:44:34.180 --> 00:44:38.030
we're assuming that we have this
fixed assignment of who's on what.

00:44:38.030 --> 00:44:40.610
We have specific people with names.

00:44:40.610 --> 00:44:44.000
The so and so is on this committee and
so and so is on this committee, and so

00:44:44.000 --> 00:44:45.580
on that's not random.

00:44:45.580 --> 00:44:48.830
Our randomness is because we're
choosing two random committees.

00:44:48.830 --> 00:44:52.750
Okay, and we want the expected
overlap of those two committees.

00:44:52.750 --> 00:44:53.820
So how do we do that?

00:44:55.080 --> 00:44:56.970
Indicator random variables.

00:44:56.970 --> 00:44:59.980
We create an indicator random variable for
each person.

00:44:59.980 --> 00:45:04.380
There's 100 people, so I'm not gonna
write all the indicator random variables,

00:45:04.380 --> 00:45:06.640
because this should be familiar by now.

00:45:06.640 --> 00:45:11.100
We have 100 people, so we create an
indicator for each person, use linearity.

00:45:11.100 --> 00:45:14.790
So it's gonna be 100 times and
over here all we need to

00:45:14.790 --> 00:45:19.750
do by the fundamental bridge, all we need
to do is write down the probability that,

00:45:19.750 --> 00:45:25.550
let's say person number one is on both
of those random committees, right?

00:45:25.550 --> 00:45:26.650
So now we're looking, okay,

00:45:26.650 --> 00:45:31.610
person number one what's the probability
that that person is on both of

00:45:31.610 --> 00:45:36.710
those randomly chosen committees well,
you can think of that as a hypergeometric.

00:45:36.710 --> 00:45:38.490
You don't have to let's just
think about it directly.

00:45:39.690 --> 00:45:43.480
I'm assuming I chose
two random committees.

00:45:43.480 --> 00:45:45.410
So it's 100, choose two possibilities.

00:45:45.410 --> 00:45:46.820
Naive definition applies,

00:45:46.820 --> 00:45:52.699
because I'm assuming equally likely that
we chose any two with equal probabilities.

00:45:53.890 --> 00:45:56.420
Then the numerator, sorry,

00:45:56.420 --> 00:45:59.770
this is number of committee,
how many committees are there?

00:45:59.770 --> 00:46:03.750
15 committees, choose two out of the 15
committees and then the numerator.

00:46:04.940 --> 00:46:09.000
Person number one is on three committees
so choose two out of the three committees.

00:46:09.000 --> 00:46:14.010
So this is three choose two, three choose
two is three, so that's 300 over 1500

00:46:14.010 --> 00:46:18.760
choose two is 15 times 14 divided by 2.

00:46:18.760 --> 00:46:25.400
300 divided by 15 is 20,
the 2 comes up so it's 40 over 14.

00:46:25.400 --> 00:46:31.450
Which we can simplify as 20 over 7.

00:46:31.450 --> 00:46:39.970
If I did the arithmetic correctly that
looks like we came a little bit short.

00:46:39.970 --> 00:46:43.850
It's like almost good enough
cuz we wanted at least 3.

00:46:43.850 --> 00:46:49.020
And we only have 20 over 7 and
if only it were 21 over 7.

00:46:49.020 --> 00:46:50.535
Then we'd be so happy.

00:46:52.316 --> 00:46:53.760
But here's the idea.

00:46:53.760 --> 00:46:58.390
According to that, there must be,
so the average is 20/7.

00:46:58.390 --> 00:47:03.813
That implies that there
exists a pair of committees.

00:47:06.897 --> 00:47:08.153
With at least.

00:47:10.313 --> 00:47:12.705
An overlap of 20/ 7.

00:47:14.625 --> 00:47:19.755
Now there's no way that two
committees can have an overlap equal

00:47:19.755 --> 00:47:25.660
to 20/7 if the overlap were only
2 that would not be good enough.

00:47:25.660 --> 00:47:30.930
So we get to round this up to the next
integer because the overlap is an integer

00:47:30.930 --> 00:47:35.560
so that means we can have
overlap of at least 3.

00:47:35.560 --> 00:47:36.580
Than means we have proven that.

00:47:39.240 --> 00:47:44.540
So we prove that it exist,
we ran out of time so have a good weekend.

