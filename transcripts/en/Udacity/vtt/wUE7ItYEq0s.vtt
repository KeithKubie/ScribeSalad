WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:03.550
We didn't go into enough detail
about what it is we're trying to

00:00:03.550 --> 00:00:04.689
optimize here.

00:00:04.689 --> 00:00:09.390
I just said something vague like we
want to maximize the sum of our reward.

00:00:09.390 --> 00:00:14.340
Well, it's not so simple, in fact,
here's a great story to illustrate that.

00:00:14.340 --> 00:00:18.560
There's a great Russian comedian,
Yakov Smirnoff,

00:00:18.560 --> 00:00:22.250
you may remember him or not, but he
told this joke once that I really loved.

00:00:22.250 --> 00:00:26.420
He said,
have you heard about the Soviet lottery,

00:00:26.420 --> 00:00:31.070
it's a million rubles if you win,
one ruble a year for a million years.

00:00:32.500 --> 00:00:38.260
So the point is, and if you recall
from one of our earlier lessons,

00:00:38.260 --> 00:00:45.770
that one dollar or one ruble delivered
to us a million years in the future

00:00:45.770 --> 00:00:51.190
is really not as valuable as a dollar or
ruble that we get now.

00:00:51.190 --> 00:00:55.230
And so, for instance, if we think
about a robot living forever,

00:00:55.230 --> 00:00:59.370
it might do something just mundane
to gather a dollar a year.

00:00:59.370 --> 00:01:02.340
That's an infinite amount of money, but

00:01:02.340 --> 00:01:05.090
in practice it doesn't
really work that well.

00:01:05.090 --> 00:01:07.800
So to consider that,
and to illustrate that,

00:01:07.800 --> 00:01:10.460
I'm going to show you a little maze
problem here, and we'll think about

00:01:10.460 --> 00:01:14.920
what the robot ought to do that
would be optimal in this maze.

00:01:14.920 --> 00:01:18.850
So here's our robot, and
here's the challenge for our robot.

00:01:18.850 --> 00:01:23.830
We have a reward here of $1 and
a reward over here of $1 million.

00:01:24.850 --> 00:01:30.230
So if the robot comes over here and
gets this $1, it's special in that each

00:01:30.230 --> 00:01:36.140
time he touches it, he gets $1 and
it goes away but then it comes back.

00:01:36.140 --> 00:01:39.320
So the robot could come here go back and

00:01:39.320 --> 00:01:42.650
forth and
get a dollar each time it moves here.

00:01:42.650 --> 00:01:46.970
This one, once the robot tags it,
it's gone.

00:01:46.970 --> 00:01:50.260
But clearly it's worthwhile to
come over here and grab it.

00:01:50.260 --> 00:01:53.420
Now this red area is obstacle,
it can't go there.

00:01:53.420 --> 00:01:57.700
And here I wrote some
rewards that the robot,

00:01:57.700 --> 00:02:00.580
in fact negative one is a penalty.

00:02:00.580 --> 00:02:05.780
But the penalties the robot would
get as it went this way, and

00:02:05.780 --> 00:02:06.846
zero penalty that way.

00:02:06.846 --> 00:02:11.710
Now, if we say that what we want to
optimize is the sum of all future

00:02:11.710 --> 00:02:16.540
rewards, then it doesn't matter
whether we go this way and

00:02:16.540 --> 00:02:18.990
just get that dollar over and
over and over again.

00:02:18.990 --> 00:02:21.800
Or if we go this way,
get the million dollars,

00:02:21.800 --> 00:02:24.870
come back and get that $1 over and
over and over again.

00:02:24.870 --> 00:02:30.060
Now there's no difference because
they both sum to infinity over time.

00:02:30.060 --> 00:02:37.430
Now what if we say, okay, I want to
optimize my reward over three moves.

00:02:37.430 --> 00:02:40.690
So I've got a finite horizon.

00:02:40.690 --> 00:02:44.740
Let's consider the rewards we get
with a finite horizon of three

00:02:44.740 --> 00:02:46.680
if we go this way versus this way.

00:02:46.680 --> 00:02:52.706
So if we go this way,
we're going to get rewards of -1,

00:02:52.706 --> 00:02:57.815
-1, -1, and
if we go this way we get zero,

00:02:57.815 --> 00:03:04.900
$1, and then we have to move down here,
and get another zero.

00:03:06.620 --> 00:03:08.230
So clearly, starting here,

00:03:08.230 --> 00:03:12.700
with a finite horizon of three,
the best thing to do is go up there.

00:03:12.700 --> 00:03:17.450
Now, if we extend the horizon a little
bit further, say out to eight,

00:03:18.790 --> 00:03:22.150
we would find that this
is the best thing to do.

00:03:22.150 --> 00:03:23.540
So if we go this way,

00:03:23.540 --> 00:03:26.850
we get -1, -1, -1, until we hit
the jackpot here and get $1M.

00:03:26.850 --> 00:03:31.110
Clearly if you sum this up,
it's a pretty good prize.

00:03:31.110 --> 00:03:36.520
If we go this way and touch that $1
over and over again, we get this.

00:03:36.520 --> 00:03:43.830
So clearly as we expand our finite
horizon trivially up to say eight steps,

00:03:43.830 --> 00:03:46.430
going this way and tagging at one
million is the best thing to do.

00:03:46.430 --> 00:03:50.510
If we carried it even further, we'd
discover that then we should come back

00:03:50.510 --> 00:03:55.140
this way and go to that dollar and
tag it over and over and over again.

00:03:55.140 --> 00:03:57.060
Let me formalize these a little bit.

00:03:57.060 --> 00:04:02.820
With the infinite horizon what
we're trying to maximize is the sum

00:04:02.820 --> 00:04:05.810
of all rewards over all of the future.

00:04:05.810 --> 00:04:11.050
So it's the sum of each of these
rewards for i equals one to infinity.

00:04:11.050 --> 00:04:15.130
The finite horizon is very similar,
it's just we don't go to infinity.

00:04:15.130 --> 00:04:20.959
So for optimizing over horizon
of four steps, n would be four.

00:04:20.959 --> 00:04:25.770
We're just trying to maximize the sum
of the reward for the next four steps.

00:04:25.770 --> 00:04:31.380
Now, there is yet
another formulation that if you think

00:04:31.380 --> 00:04:36.520
back to that lecture a while back about
what's the value of a future dollar.

00:04:37.780 --> 00:04:39.220
We can dig that up and

00:04:39.220 --> 00:04:42.270
it makes a lot of sense in terms
of reinforcement learning.

00:04:42.270 --> 00:04:47.910
So remember that if it takes us say,
four years to get a dollar,

00:04:47.910 --> 00:04:51.080
that dollar is less valuable
than say if it takes one year.

00:04:51.080 --> 00:04:56.130
And the same way, if it takes,
say, eight steps to make a dollar,

00:04:56.130 --> 00:05:00.830
that dollar is less valuable than
a dollar I can get just in one step.

00:05:00.830 --> 00:05:04.810
And the way we represent
that is very simple.

00:05:04.810 --> 00:05:08.640
Just like we represented
the sum of future dividends and

00:05:08.640 --> 00:05:12.920
it looks like this,
it's called discounted reward.

00:05:12.920 --> 00:05:18.470
So instead of just summing up the r
sub i is we multiply it by this factor

00:05:18.470 --> 00:05:25.130
gamma to the i minus 1, such that our
immediate reward, the very next one

00:05:25.130 --> 00:05:30.160
we get, whatever gamma is when it gets
raised to the zero power is just one.

00:05:30.160 --> 00:05:34.560
So that means for
the very next step we get r.

00:05:34.560 --> 00:05:38.460
But for the step after it,
it's gamma to the one.

00:05:39.680 --> 00:05:43.910
So it devalues that reward a little bit.

00:05:43.910 --> 00:05:47.140
Gamma is a value between zero and one,

00:05:48.170 --> 00:05:53.730
the closer it is to one,
the more we value rewards in the future.

00:05:53.730 --> 00:05:57.700
The closer it is to zero,
the less we value rewards in the future.

00:05:57.700 --> 00:06:00.180
In fact, if gamma is set equal to one,

00:06:00.180 --> 00:06:03.390
this is exactly the same
as the infinite horizon.

00:06:03.390 --> 00:06:07.660
But gamma relates very strongly
to interest rates if you recall.

00:06:07.660 --> 00:06:14.310
So, if say, gamma were 0.95 it
means each step in the future is

00:06:14.310 --> 00:06:20.210
worth about 5% less than the immediate
reward if we got it right away.

00:06:20.210 --> 00:06:24.760
This is the method that
we use in q learning.

00:06:24.760 --> 00:06:29.020
One reason is that the math
turns out to be very handy, and

00:06:29.020 --> 00:06:31.280
it provides nice conversion properties.

