WEBVTT
Kind: captions
Language: en

00:00:07.910 --> 00:00:09.260
JU-KAY KWEK: OK.

00:00:09.260 --> 00:00:12.560
If there's one thing that you
guys take away from today's

00:00:12.560 --> 00:00:15.050
session, it's on this slide.

00:00:15.050 --> 00:00:15.870
Right?

00:00:15.870 --> 00:00:17.710
It's really that simple.

00:00:17.710 --> 00:00:20.270
What we're going to be talking
about today is big data.

00:00:20.270 --> 00:00:24.050
Quantifying big data, talking
about some real-life use cases

00:00:24.050 --> 00:00:26.380
for big data, right?

00:00:26.380 --> 00:00:27.740
It's not very easy.

00:00:27.740 --> 00:00:30.790
In fact, it can be a pain,
especially if your business is

00:00:30.790 --> 00:00:33.520
actually solving some other
business problem.

00:00:33.520 --> 00:00:37.390
We want to suggest that there
is a better way of doing big

00:00:37.390 --> 00:00:39.420
data than some of the
methods that we've

00:00:39.420 --> 00:00:41.450
seen out there today.

00:00:41.450 --> 00:00:43.960
And what we want to show, with
the help of some of our

00:00:43.960 --> 00:00:46.830
friends here, is how you can
do that with Google Cloud

00:00:46.830 --> 00:00:48.700
Services and be very
successful building

00:00:48.700 --> 00:00:51.331
applications on big data.

00:00:51.331 --> 00:00:52.450
All right?

00:00:52.450 --> 00:00:54.610
So, we're going to be handing
off a little bit.

00:00:54.610 --> 00:00:57.940
I'm going to kick us off by
starting and talking about

00:00:57.940 --> 00:00:59.370
what is big data.

00:00:59.370 --> 00:01:02.460
We're not going to spend a whole
lot of time on this, but

00:01:02.460 --> 00:01:05.850
I thought it would be fun to go
through a couple of quotes

00:01:05.850 --> 00:01:08.540
from a few of our customers
and the businesses that we

00:01:08.540 --> 00:01:10.700
work with who have to
deal with large

00:01:10.700 --> 00:01:11.560
amounts of data, right?

00:01:11.560 --> 00:01:12.870
So the first quote--

00:01:12.870 --> 00:01:14.930
and Richard here will be talking
about them later on--

00:01:14.930 --> 00:01:15.870
is from CenterParcs.

00:01:15.870 --> 00:01:20.410
They're a hospitality company, a
pretty large one, in Europe.

00:01:20.410 --> 00:01:24.430
How are hotel reservations for
Spain from New York compared

00:01:24.430 --> 00:01:26.680
with this time last year?

00:01:26.680 --> 00:01:30.700
That's a pretty precise slice
of a lot of business data.

00:01:30.700 --> 00:01:33.630
Do we need to adjust our
marketing campaign and where?

00:01:33.630 --> 00:01:37.880
That's a completely different
set of data that may come from

00:01:37.880 --> 00:01:39.870
AdWords, DoubleClick, right?

00:01:39.870 --> 00:01:42.290
Campaign data, and
then slicing that

00:01:42.290 --> 00:01:43.550
to a specific region.

00:01:43.550 --> 00:01:45.890
If you think about the nature
of this question, it's a

00:01:45.890 --> 00:01:48.980
pretty precise one to ask of--
what we'll see later on-- is a

00:01:48.980 --> 00:01:50.230
lot of data.

00:01:52.830 --> 00:01:56.070
Which users who signed up last
quarter also advanced at least

00:01:56.070 --> 00:01:59.610
three levels and purchased an
item worth more than $5?

00:01:59.610 --> 00:02:02.820
Now, this is actually
a real question.

00:02:02.820 --> 00:02:06.440
This comes from another of
our early customers.

00:02:06.440 --> 00:02:08.190
They were actually in the
Sandbox yesterday.

00:02:08.190 --> 00:02:09.020
You might have seen them.

00:02:09.020 --> 00:02:10.410
Their name is Claritics.

00:02:10.410 --> 00:02:12.890
They do user analytics
for mobile and social

00:02:12.890 --> 00:02:13.480
applications.

00:02:13.480 --> 00:02:16.120
So they deal with tens of
millions of users, hundreds of

00:02:16.120 --> 00:02:17.800
millions of events a day.

00:02:17.800 --> 00:02:22.300
And what they do, using Google
Cloud Services as a platform,

00:02:22.300 --> 00:02:27.280
is they help their customers to
understand their own user

00:02:27.280 --> 00:02:30.760
sets, with very, very detailed
questions like this, that help

00:02:30.760 --> 00:02:33.620
them find cohorts of users
and target them

00:02:33.620 --> 00:02:36.580
for specific actions.

00:02:36.580 --> 00:02:37.830
One more.

00:02:39.580 --> 00:02:41.430
By the time we prepared the
data, we'd forgotten what the

00:02:41.430 --> 00:02:43.100
question was.

00:02:43.100 --> 00:02:44.860
Anyone ever come up against
this before when you're

00:02:44.860 --> 00:02:46.520
working with data?

00:02:46.520 --> 00:02:49.130
This is a quote that literally
comes from one of Google's

00:02:49.130 --> 00:02:51.850
largest AdWords customers.

00:02:51.850 --> 00:02:56.630
These guys deal with hundreds
of gigabytes of data a day

00:02:56.630 --> 00:03:00.050
from the logs that get
served from the ads.

00:03:00.050 --> 00:03:01.870
And the reason this is really
important to them-- and

00:03:01.870 --> 00:03:06.680
they've invested a lot in
infrastructure that helps them

00:03:06.680 --> 00:03:08.810
in house, but it takes
a long time.

00:03:08.810 --> 00:03:09.770
The reason this is really
important for them is they

00:03:09.770 --> 00:03:11.840
need to calculate
things like ROI.

00:03:11.840 --> 00:03:14.620
How much are we spending
in a given region?

00:03:14.620 --> 00:03:17.860
And do we need to change that to
help the business increase?

00:03:17.860 --> 00:03:21.430
So these are all really
important business questions

00:03:21.430 --> 00:03:25.800
that are grounded on the basis
of really understanding,

00:03:25.800 --> 00:03:28.720
getting insight from, large
amounts of data.

00:03:28.720 --> 00:03:31.940
I pulled these up just to give
everyone here a level set.

00:03:31.940 --> 00:03:33.100
What are we talking about?

00:03:33.100 --> 00:03:34.880
This isn't an academic talk.

00:03:34.880 --> 00:03:37.750
This is about solving real
business problems by getting

00:03:37.750 --> 00:03:39.000
insights into data.

00:03:41.860 --> 00:03:44.300
So I just want to go through
this quickly.

00:03:44.300 --> 00:03:47.310
But again, in the spirit of
getting some terms and

00:03:47.310 --> 00:03:49.550
quantifying things, what are
some of these trends?

00:03:49.550 --> 00:03:51.760
As we step back from those
questions, what's going on in

00:03:51.760 --> 00:03:53.470
business today?

00:03:53.470 --> 00:03:56.920
As we've talked to a lot of
these customers and worked

00:03:56.920 --> 00:03:59.990
with them, these are some of
the things that are common

00:03:59.990 --> 00:04:02.070
themes that emerge.

00:04:02.070 --> 00:04:03.980
They see opportunities.

00:04:03.980 --> 00:04:07.410
Data is increasingly seen as
a core business asset.

00:04:07.410 --> 00:04:09.590
It's not something that you
just manage with IT.

00:04:09.590 --> 00:04:11.970
This is something that
marketing, your data analytics

00:04:11.970 --> 00:04:16.040
team, your sales team need
to do their job well.

00:04:16.040 --> 00:04:18.660
Increasingly, a lot of critical
business data is out

00:04:18.660 --> 00:04:19.810
in the cloud.

00:04:19.810 --> 00:04:23.580
Anybody use Salesforce here?

00:04:23.580 --> 00:04:25.230
Lots of people.

00:04:25.230 --> 00:04:25.850
That just gets going.

00:04:25.850 --> 00:04:29.320
Think about social and so
on, Google Analytics.

00:04:29.320 --> 00:04:31.850
New things are possible in
the cloud because of

00:04:31.850 --> 00:04:34.040
infrastructure, algorithms.

00:04:34.040 --> 00:04:35.770
I don't have to go into this.

00:04:35.770 --> 00:04:41.050
But all of the things that you
saw from the keynote, many of

00:04:41.050 --> 00:04:44.640
these things are only available
by using the cloud.

00:04:44.640 --> 00:04:47.230
Many businesses recognize
that too.

00:04:47.230 --> 00:04:49.900
There are use cases in terms
of sharing of data,

00:04:49.900 --> 00:04:53.710
controlling access to data,
speed of decision making.

00:04:53.710 --> 00:04:56.530
As you saw with this laptop
crash example, that's just a

00:04:56.530 --> 00:04:59.990
small one, but the speed with
which you can move data around

00:04:59.990 --> 00:05:02.960
really affects how people can
actually make use of data to

00:05:02.960 --> 00:05:05.280
make good business decisions.

00:05:05.280 --> 00:05:08.600
But there are a couple of
challenges as well.

00:05:08.600 --> 00:05:10.860
Information growing
faster than the

00:05:10.860 --> 00:05:12.660
ability to leverage it.

00:05:12.660 --> 00:05:13.630
As you heard in [INAUDIBLE]

00:05:13.630 --> 00:05:16.190
keynote this morning, and as
you saw from that chart of

00:05:16.190 --> 00:05:19.310
Chrome adoption, since 2004--

00:05:19.310 --> 00:05:22.990
if you see that spike-- it's
more users, more applications,

00:05:22.990 --> 00:05:27.990
on more devices all
generating data.

00:05:27.990 --> 00:05:31.210
It's tough for enterprises
to capture all that data.

00:05:31.210 --> 00:05:34.880
Even more challenging is for
the many businesses that do

00:05:34.880 --> 00:05:38.460
have BI and data warehousing
infrastructure, the way of

00:05:38.460 --> 00:05:41.460
doing things before with star
schemas, lots of indexing,

00:05:41.460 --> 00:05:44.240
proprietary hardware, what
we'll see is that it's

00:05:44.240 --> 00:05:46.820
actually very hard for a lot
of these installations to

00:05:46.820 --> 00:05:48.520
actually keep up with this
torrent of data.

00:05:48.520 --> 00:05:51.160
So what do you do?

00:05:51.160 --> 00:05:52.880
And finally, skills.

00:05:52.880 --> 00:05:54.410
It's not just about money.

00:05:54.410 --> 00:05:56.140
It's not just about
technology.

00:05:56.140 --> 00:05:57.580
It's about building the teams.

00:05:57.580 --> 00:06:01.840
It's about changing your
organization to adapt to these

00:06:01.840 --> 00:06:04.740
new tools and skills.

00:06:04.740 --> 00:06:09.070
As anyone here who's tried
to hire for, say, Hadoop

00:06:09.070 --> 00:06:12.880
engineers, you know skills
are hard to come by.

00:06:12.880 --> 00:06:15.910
So these are some of both the
opportunities as well as the

00:06:15.910 --> 00:06:18.410
challenges that, as we've
talked to many of these

00:06:18.410 --> 00:06:22.330
customers, are recurring themes
as we hear customers

00:06:22.330 --> 00:06:25.000
thinking about big data.

00:06:25.000 --> 00:06:27.050
So let's get concrete
about it.

00:06:27.050 --> 00:06:28.480
What does big data actually
look like?

00:06:28.480 --> 00:06:34.710
Some of the common
characteristics comes in all

00:06:34.710 --> 00:06:35.310
different kinds--

00:06:35.310 --> 00:06:39.530
structured, semi-structured,
unstructured data, emails,

00:06:39.530 --> 00:06:41.040
tweets, so on.

00:06:41.040 --> 00:06:43.160
We're often talking about
millions, if not

00:06:43.160 --> 00:06:45.410
billions, of rows.

00:06:45.410 --> 00:06:48.210
And what kind of
frequency, too?

00:06:48.210 --> 00:06:50.680
Often, it's too large a process
on a single machine.

00:06:50.680 --> 00:06:53.800
Sometimes it's even too large to
store on a single machine.

00:06:53.800 --> 00:06:55.050
What do you do then?

00:06:57.250 --> 00:07:00.120
It's got a high rate of growth,
and often, it's

00:07:00.120 --> 00:07:01.370
growing every day.

00:07:03.490 --> 00:07:05.610
I won't read off the list of
industries, but this is

00:07:05.610 --> 00:07:09.330
representative of sets of
customers that we've talked to

00:07:09.330 --> 00:07:11.455
who experience a lot of these
common challenges.

00:07:15.690 --> 00:07:21.870
So let's look at a real-life
example of big data.

00:07:21.870 --> 00:07:26.220
This is just a snapshot, just
to give you an idea, but if

00:07:26.220 --> 00:07:29.510
anyone here has worked with
DoubleClick, this is a

00:07:29.510 --> 00:07:31.835
snapshot of some of the
ad serving logs.

00:07:31.835 --> 00:07:34.310
A couple of people do.

00:07:34.310 --> 00:07:38.640
And as you can see, a single
row-- and I just took whatever

00:07:38.640 --> 00:07:41.180
fit on my screen for this-- but
you actually get a lot of

00:07:41.180 --> 00:07:42.750
data in DoubleClick.

00:07:42.750 --> 00:07:46.820
It's everything from advertiser,
campaign, time

00:07:46.820 --> 00:07:50.310
stamp, what was the impression,
where it was.

00:07:53.530 --> 00:07:55.760
This is a snapshot
from Google's own

00:07:55.760 --> 00:07:58.020
global marketing campaigns.

00:07:58.020 --> 00:08:00.540
We scrubbed it and anonymized
it just to show you.

00:08:00.540 --> 00:08:03.660
But just to give you some of the
numbers, because Google is

00:08:03.660 --> 00:08:08.390
very representative of a large
advertiser or an agency.

00:08:08.390 --> 00:08:10.110
This is a very common problem.

00:08:10.110 --> 00:08:13.150
This is just from one week.

00:08:13.150 --> 00:08:18.170
We have 300 gigabytes of data
generated from these logs.

00:08:18.170 --> 00:08:22.730
They average about 6 billion
rows, and almost a billion

00:08:22.730 --> 00:08:25.490
rows coming in every day.

00:08:25.490 --> 00:08:27.250
So that's a lot of data
to deal with.

00:08:27.250 --> 00:08:30.700
And for the reasons that I
mentioned before, the stakes

00:08:30.700 --> 00:08:33.650
are pretty high in terms of
understanding what's going on

00:08:33.650 --> 00:08:38.250
in terms of all those ads
and impressions served.

00:08:38.250 --> 00:08:40.299
What if you had to query it?

00:08:40.299 --> 00:08:42.970
How would you do that?

00:08:42.970 --> 00:08:45.360
Fortunately--

00:08:45.360 --> 00:08:48.560
and we'll just do a fun
little demo here--

00:08:48.560 --> 00:08:52.160
I'm going to show you how we
can do this on BigQuery.

00:08:52.160 --> 00:08:54.360
So this is the BigQuery
user interface.

00:08:54.360 --> 00:08:57.900
It's just a simple utilitarian
interface meant for letting

00:08:57.900 --> 00:09:01.490
analysts and developers query
against large data sets.

00:09:01.490 --> 00:09:02.860
Some of you might have played
with this before.

00:09:02.860 --> 00:09:06.910
What we're looking at here,
if you can see, is a daily

00:09:06.910 --> 00:09:09.200
snapshot of that data.

00:09:09.200 --> 00:09:11.540
And you can see the
schema here.

00:09:11.540 --> 00:09:13.370
It's a pretty complex schema.

00:09:13.370 --> 00:09:17.490
If we look at the details, you
can see a single day's data is

00:09:17.490 --> 00:09:19.760
pretty large.

00:09:19.760 --> 00:09:23.185
So what I'm going to do is
I'm just going to pull.

00:09:23.185 --> 00:09:26.060
And we're going to take that
one day of data, and we're

00:09:26.060 --> 00:09:27.310
just going to do
a simple count.

00:09:29.870 --> 00:09:35.050
How many rows is in
this 30 gigabytes?

00:09:35.050 --> 00:09:37.330
It's a little hard to read, but
that's just a little over

00:09:37.330 --> 00:09:39.580
a billion rows.

00:09:39.580 --> 00:09:42.520
It's pretty nice, though, that
we managed to count that in,

00:09:42.520 --> 00:09:45.480
what, less than six seconds.

00:09:45.480 --> 00:09:48.390
We'll do one more, and
then we'll move on.

00:09:48.390 --> 00:09:50.740
What I'm doing here is I'm
actually taking, now, that

00:09:50.740 --> 00:09:54.060
full week's worth of data that
you saw from the snapshot.

00:09:54.060 --> 00:09:57.990
And what this query is doing is
basically it's saying, find

00:09:57.990 --> 00:09:59.240
all the countries.

00:10:01.290 --> 00:10:03.470
Count all of the impressions.

00:10:03.470 --> 00:10:05.700
And then here's the interesting
one, count

00:10:05.700 --> 00:10:09.390
distinct for user IDs to find
the unique impressions.

00:10:09.390 --> 00:10:12.030
So it's not just all of the
impressions that were served.

00:10:12.030 --> 00:10:15.370
It's find all those impressions
for the exact

00:10:15.370 --> 00:10:19.450
number of people that actually
clicked on one of these

00:10:19.450 --> 00:10:21.380
impressions.

00:10:21.380 --> 00:10:25.320
And you can see there we just
processed about 177 gigabytes

00:10:25.320 --> 00:10:27.210
of data to find this question.

00:10:27.210 --> 00:10:28.220
Remember, 6 billion rows.

00:10:28.220 --> 00:10:30.940
We just went through this for
a fairly arbitrary question

00:10:30.940 --> 00:10:34.860
with no indexes or caching
to find this answer.

00:10:34.860 --> 00:10:36.080
We got a nice list here.

00:10:36.080 --> 00:10:41.840
We can see, for example, in the
United States, that's 200

00:10:41.840 --> 00:10:45.220
something million impressions,
2 billion impressions

00:10:45.220 --> 00:10:48.500
actually, 150 million
unique impressions.

00:10:48.500 --> 00:10:52.180
So that's just one example of
how you can quickly get

00:10:52.180 --> 00:10:53.580
insights from a very
large amount of

00:10:53.580 --> 00:10:56.160
data using Cloud tools.

00:10:56.160 --> 00:11:00.060
But this kind of begs the
question, if it's so easy, why

00:11:00.060 --> 00:11:03.670
aren't more people doing this?

00:11:03.670 --> 00:11:08.980
So let's take one more step
back, and let's look at what's

00:11:08.980 --> 00:11:10.860
going on out there.

00:11:10.860 --> 00:11:14.650
This is just one quote from one
analyst, but it sounds a

00:11:14.650 --> 00:11:16.110
little optimistic I think.

00:11:16.110 --> 00:11:22.950
But $3 billion to $16 billion
in five years is a little

00:11:22.950 --> 00:11:23.380
optimistic.

00:11:23.380 --> 00:11:26.710
But it just gives you a sense
of how much investment and

00:11:26.710 --> 00:11:30.470
interest there is in big data
solutions out there.

00:11:30.470 --> 00:11:32.410
And I just took a couple of
the keywords that you

00:11:32.410 --> 00:11:35.360
typically read when you
come across big data.

00:11:35.360 --> 00:11:39.910
So MapReduce, NoSQL, Hadoop,
what do you do with all of

00:11:39.910 --> 00:11:41.280
this stuff?

00:11:41.280 --> 00:11:43.425
So what we see is there's a lot
of questions being asked

00:11:43.425 --> 00:11:47.530
about how exactly do you go
about, from the problem of

00:11:47.530 --> 00:11:49.490
having a lot of data and
knowing you need to get

00:11:49.490 --> 00:11:53.490
business insight, to actually
running that really quick

00:11:53.490 --> 00:11:55.420
query and interrogating data?

00:11:55.420 --> 00:11:57.300
Finding the valuable needle
in the haystack?

00:12:03.240 --> 00:12:08.806
What we see is there are some
patterns starting to evolve.

00:12:08.806 --> 00:12:11.290
If you think about the tools
that people are assembling to

00:12:11.290 --> 00:12:13.730
deal with big data, you can
think of it as kind of a

00:12:13.730 --> 00:12:16.170
software stack, an architectural
stack.

00:12:16.170 --> 00:12:18.350
On the bottom, there's storage,
the ability to store

00:12:18.350 --> 00:12:20.320
tremendous amounts of data.

00:12:20.320 --> 00:12:22.950
Once you have the data, you need
to be able to transform

00:12:22.950 --> 00:12:25.930
it, do things like ETL, prepare
it in the right

00:12:25.930 --> 00:12:28.110
format, so that it's actually
useful and can

00:12:28.110 --> 00:12:29.960
be loaded into tools.

00:12:29.960 --> 00:12:33.570
As we just saw with BigQuery
there's query and analysis.

00:12:33.570 --> 00:12:36.050
And then once you have
those insights,

00:12:36.050 --> 00:12:36.810
what do you with them?

00:12:36.810 --> 00:12:39.020
You have to serve them in the
form of an application or a

00:12:39.020 --> 00:12:40.260
dashboard or a report.

00:12:40.260 --> 00:12:42.120
So serving the results.

00:12:42.120 --> 00:12:48.670
And if you see the two columns
here, on the right basically,

00:12:48.670 --> 00:12:52.370
if any of you have a couple
million dollars in your budget

00:12:52.370 --> 00:12:56.420
to spare, you can go out and
get a very fully featured,

00:12:56.420 --> 00:12:59.370
very capable, on-premises,
proprietary

00:12:59.370 --> 00:13:01.610
data warehouse appliance.

00:13:01.610 --> 00:13:04.840
Lots of cores and everything
to do this work for you.

00:13:04.840 --> 00:13:09.130
But if you have a couple million
spare in your budget,

00:13:09.130 --> 00:13:12.310
there may be better things
to do with it.

00:13:12.310 --> 00:13:15.915
On the other hand, we see a lot
of really good work being

00:13:15.915 --> 00:13:18.950
done in the open source world.

00:13:18.950 --> 00:13:22.690
So many of these terms
that you see here--

00:13:22.690 --> 00:13:27.820
NoSQL databases, HDFS, Hadoop,
all the different modules--

00:13:27.820 --> 00:13:29.820
there's a lot of great work
being done by the open source

00:13:29.820 --> 00:13:34.720
community to enable you to set
up your own software stack.

00:13:34.720 --> 00:13:38.050
The challenge there is it's
really, really complex.

00:13:38.050 --> 00:13:42.790
If you've ever tried to set up
an HDFS cluster, run different

00:13:42.790 --> 00:13:45.980
Hadoop nodes, and manage all of
that, the task of actually

00:13:45.980 --> 00:13:49.530
managing all of that software
stack greatly exceeds the

00:13:49.530 --> 00:13:53.020
amount of time you actually
spend doing analysis.

00:13:53.020 --> 00:13:55.530
So to go back to that customer,
by the time we

00:13:55.530 --> 00:13:57.260
actually got the data prepared,
we'd forgotten what

00:13:57.260 --> 00:13:58.460
the question was.

00:13:58.460 --> 00:14:00.620
Directly applies to this.

00:14:00.620 --> 00:14:04.910
So what we want to suggest is
maybe there's a better way of

00:14:04.910 --> 00:14:06.320
doing this.

00:14:06.320 --> 00:14:09.090
And to talk about that, I'm
going to ask my colleague,

00:14:09.090 --> 00:14:10.210
Navneet, to come up.

00:14:10.210 --> 00:14:13.610
Navneet is the product manager
for Google Cloud Storage,

00:14:13.610 --> 00:14:14.770
close associate of mine.

00:14:14.770 --> 00:14:17.770
And he's going to walk us
through some of the tools that

00:14:17.770 --> 00:14:19.370
you can use.

00:14:19.370 --> 00:14:21.900
NAVNEET JONEJA: Thank
you, Ju-kay.

00:14:21.900 --> 00:14:25.840
So we'd like to propose that a
better way to do big data is

00:14:25.840 --> 00:14:30.290
really to leverage the cloud
and to use what we call

00:14:30.290 --> 00:14:31.670
Composable Cloud Services.

00:14:31.670 --> 00:14:34.310
So Cloud Services with very
specific functionality that

00:14:34.310 --> 00:14:38.000
you can put together to solve
various problems so you have a

00:14:38.000 --> 00:14:41.250
complete stack that solves your
entire big data problem.

00:14:41.250 --> 00:14:43.330
Focus on the solution rather
than the infrastructure.

00:14:43.330 --> 00:14:46.630
So you're not worried about
deploying a new stack,

00:14:46.630 --> 00:14:50.800
learning new technology,
learning, frankly, a whole new

00:14:50.800 --> 00:14:51.790
way of doing things.

00:14:51.790 --> 00:14:53.720
But instead, you're focused
on the solution

00:14:53.720 --> 00:14:55.750
you're trying to build.

00:14:55.750 --> 00:14:58.080
And then to actually do things
you couldn't do before,

00:14:58.080 --> 00:15:01.000
because you have access to
technology built by people

00:15:01.000 --> 00:15:02.620
who've been doing this
for a while.

00:15:02.620 --> 00:15:04.990
And then finally, to make it
cost effective, because you

00:15:04.990 --> 00:15:06.100
pay only for what you use.

00:15:06.100 --> 00:15:08.290
So rather than spending a few
million dollars on an

00:15:08.290 --> 00:15:11.280
appliance, you store as
much data as you need.

00:15:11.280 --> 00:15:13.590
You process it when
you need to.

00:15:13.590 --> 00:15:15.570
And when you think that
answering a question is worth

00:15:15.570 --> 00:15:18.195
your time and money, you
use the service.

00:15:21.830 --> 00:15:26.480
So I want to kind of call out
what we internally often call

00:15:26.480 --> 00:15:27.630
the virtuous cycle of data.

00:15:27.630 --> 00:15:29.950
This is something which comes
back very often from our

00:15:29.950 --> 00:15:32.040
customers in different forms
when they talk about what they

00:15:32.040 --> 00:15:34.020
want to do with the
data they have.

00:15:34.020 --> 00:15:36.010
So at a very fundamental level,
they want to build

00:15:36.010 --> 00:15:37.450
great applications.

00:15:37.450 --> 00:15:39.850
They want to learn about how
the application is serving

00:15:39.850 --> 00:15:40.480
their users.

00:15:40.480 --> 00:15:42.060
Collect a bunch of data.

00:15:42.060 --> 00:15:44.720
Process that data, so they
actually have access to it in

00:15:44.720 --> 00:15:46.790
a form where they can
make sense of it.

00:15:46.790 --> 00:15:48.600
Analyze the data and
then iterate.

00:15:48.600 --> 00:15:50.650
They want to improve the
application, generate more

00:15:50.650 --> 00:15:53.880
data, improve the application,
and so on and so forth.

00:15:53.880 --> 00:15:56.400
So we think that's a really
great way to think about

00:15:56.400 --> 00:15:56.980
applications.

00:15:56.980 --> 00:15:58.830
And so a lot of what we're going
to talk about today, in

00:15:58.830 --> 00:16:01.920
terms of the cloud solution, is
going to be focused on how

00:16:01.920 --> 00:16:04.900
you can make this
happen for you.

00:16:04.900 --> 00:16:09.310
So let me take a step back and
say, why is Google doing this?

00:16:09.310 --> 00:16:12.340
If you think about it, searching
the web is at its

00:16:12.340 --> 00:16:15.450
heart a big data problem.

00:16:15.450 --> 00:16:18.200
Google had to learn very quickly
how to store data very

00:16:18.200 --> 00:16:21.680
reliably, very effectively,
and to go from

00:16:21.680 --> 00:16:24.350
that data to meaning.

00:16:24.350 --> 00:16:28.930
In other words, to understand
the core insights

00:16:28.930 --> 00:16:30.710
hidden in that data.

00:16:30.710 --> 00:16:33.680
And to be able to share it with
the world when we solve a

00:16:33.680 --> 00:16:37.750
search query, or to share it
with our internal developers,

00:16:37.750 --> 00:16:39.360
engineers, operations folks.

00:16:39.360 --> 00:16:41.130
We were trying to understand
what's going on with the

00:16:41.130 --> 00:16:43.690
system and how we want
to improve it.

00:16:43.690 --> 00:16:44.715
And we learned to do
this at an enormous

00:16:44.715 --> 00:16:46.390
scale and very quickly.

00:16:46.390 --> 00:16:49.100
And so really, over the years,
we've been building our

00:16:49.100 --> 00:16:51.930
technology that it turns out is
really big data technology.

00:16:51.930 --> 00:16:54.260
We released a number
of white papers--

00:16:54.260 --> 00:17:00.580
MapReduce, the Google File
System, FlumeJava, Dremel, all

00:17:00.580 --> 00:17:01.220
talking about the

00:17:01.220 --> 00:17:02.290
infrastructure we were building.

00:17:02.290 --> 00:17:04.700
And it turns out that those same
papers have turned around

00:17:04.700 --> 00:17:05.839
and become open-source projects

00:17:05.839 --> 00:17:08.010
solving data problems.

00:17:08.010 --> 00:17:13.020
Hadoop, for example, is based
on our MapReduce technology.

00:17:13.020 --> 00:17:16.829
So we thought, well, if we could
actually give access to

00:17:16.829 --> 00:17:19.390
our expertise to the world,
could we make it easier for

00:17:19.390 --> 00:17:21.130
people to build great
applications and

00:17:21.130 --> 00:17:22.660
do more with data?

00:17:22.660 --> 00:17:25.400
And so I want to propose that
the combination of four

00:17:25.400 --> 00:17:27.750
services, three of which have
been in the market for a

00:17:27.750 --> 00:17:30.380
little while and one that we
announced today, is actually a

00:17:30.380 --> 00:17:31.860
great way to solve
this problem.

00:17:31.860 --> 00:17:36.240
You can store your data
effectively, reliably, and

00:17:36.240 --> 00:17:38.840
with high performance
using Cloud Storage.

00:17:38.840 --> 00:17:42.660
You can build applications and
transform data using App

00:17:42.660 --> 00:17:44.850
Engine or Compute Engine,
depending on

00:17:44.850 --> 00:17:46.200
what your needs are.

00:17:46.200 --> 00:17:48.580
And then you can analyze that
data very quickly, as Ju-kay

00:17:48.580 --> 00:17:52.210
showed you a little bit earlier,
using BigQuery.

00:17:52.210 --> 00:17:55.350
And so, to put that in the
context of the virtuous cycle,

00:17:55.350 --> 00:17:59.240
the tools that you have are
really App Engine and Compute

00:17:59.240 --> 00:18:01.340
Engine to build applications.

00:18:01.340 --> 00:18:03.650
You can collect data using
a number of different

00:18:03.650 --> 00:18:07.850
mechanisms, from Google
Cloud Storage to Log

00:18:07.850 --> 00:18:10.200
Store in App Engine.

00:18:10.200 --> 00:18:12.660
You can actually process data
very quickly and effectively

00:18:12.660 --> 00:18:15.330
using App Engine or Google
Compute Engine.

00:18:15.330 --> 00:18:17.000
Those of you who were in the
Compute Engine session this

00:18:17.000 --> 00:18:19.640
morning saw a great
demonstration of, actually,

00:18:19.640 --> 00:18:21.720
MapReduce running in the
cloud and doing an

00:18:21.720 --> 00:18:24.130
amazingly fast sort.

00:18:24.130 --> 00:18:25.580
And then you can analyze the
data very quickly and

00:18:25.580 --> 00:18:26.830
effectively using BigQuery.

00:18:29.010 --> 00:18:33.080
So I'd like to pause here and
give you an idea what that

00:18:33.080 --> 00:18:35.870
looks like in production.

00:18:35.870 --> 00:18:39.240
And to do that, I'd like to
introduce Richard Verhoeff

00:18:39.240 --> 00:18:41.450
from Crystalloids,
a BI company.

00:18:41.450 --> 00:18:42.770
And he's going to show you a
great application that they

00:18:42.770 --> 00:18:44.050
built using the Googlestack.

00:18:50.010 --> 00:18:53.030
RICHARD VERHOEFF: Thank
you, Navneet.

00:18:53.030 --> 00:18:54.540
So my name is Richard
Verhoeff.

00:18:54.540 --> 00:18:57.350
I'm from Crystalloids
Innovations, and I'm the

00:18:57.350 --> 00:18:58.970
co-founder of Crystalloids
Innovations.

00:18:58.970 --> 00:19:02.600
We developed an application
called Inside OS, and we have

00:19:02.600 --> 00:19:07.150
built it on the stack you've
just been shown.

00:19:07.150 --> 00:19:12.190
The first customer we did this
for was a hospitality

00:19:12.190 --> 00:19:13.370
organization.

00:19:13.370 --> 00:19:14.210
It's called CenterParcs.

00:19:14.210 --> 00:19:18.440
I'm not sure if there are any
Europeans in the audience.

00:19:18.440 --> 00:19:20.560
Some, so some may know.

00:19:20.560 --> 00:19:22.950
And then for the American, you
can compare it a little bit to

00:19:22.950 --> 00:19:24.430
a cruise liner.

00:19:24.430 --> 00:19:27.070
So it's a company.

00:19:27.070 --> 00:19:28.670
Hence the picture
of the children.

00:19:28.670 --> 00:19:29.940
It's real for fun.

00:19:29.940 --> 00:19:32.060
It's not a business-oriented
business.

00:19:32.060 --> 00:19:35.290
It's a consumer-oriented
business.

00:19:35.290 --> 00:19:40.250
And they manage around
10,000 beds.

00:19:40.250 --> 00:19:44.720
So every day, they try
to fill 10,000 beds.

00:19:44.720 --> 00:19:47.890
And they operate restaurants
on their premises, hence it

00:19:47.890 --> 00:19:49.980
looks a little bit like
a cruise liner.

00:19:49.980 --> 00:19:52.330
So they have a great
captive audience.

00:19:52.330 --> 00:19:54.550
And there's one thing,
they're about 45

00:19:54.550 --> 00:19:56.000
years in business now.

00:19:56.000 --> 00:19:59.680
They've created a huge European
customer database.

00:19:59.680 --> 00:20:02.630
And from this, they accumulate
a lot of

00:20:02.630 --> 00:20:04.440
knowledge about their customers.

00:20:04.440 --> 00:20:09.760
And they were doing a lot
of data mining, a lot of

00:20:09.760 --> 00:20:11.760
forecasting on customers
and who to

00:20:11.760 --> 00:20:13.070
address in certain mailings.

00:20:13.070 --> 00:20:17.740
So they were used to having
a lot of databases.

00:20:20.350 --> 00:20:21.650
What happened with them?

00:20:21.650 --> 00:20:24.850
So at a certain point, we were
involved in this project.

00:20:24.850 --> 00:20:27.270
And they had a fairly good
overview of what they were

00:20:27.270 --> 00:20:28.260
trying to achieve.

00:20:28.260 --> 00:20:33.740
So they had a really done proper
design of what kind of

00:20:33.740 --> 00:20:36.160
application they were
trying to achieve.

00:20:36.160 --> 00:20:38.680
They had a lot of
internal tools.

00:20:38.680 --> 00:20:40.380
They had a proper
data warehouse.

00:20:40.380 --> 00:20:42.780
They had proper staging areas.

00:20:42.780 --> 00:20:45.840
They even had business
objects run properly.

00:20:45.840 --> 00:20:48.680
And the standard report
was very well.

00:20:48.680 --> 00:20:51.380
So that was not a problem.

00:20:51.380 --> 00:20:54.220
And then they started thinking
about doing slightly

00:20:54.220 --> 00:20:56.710
different, combining a
lot of the reports.

00:20:56.710 --> 00:20:59.340
They were scattered around
across all kinds of

00:20:59.340 --> 00:21:01.240
departments.

00:21:01.240 --> 00:21:04.020
Sales is concerned about
revenue, marketing about

00:21:04.020 --> 00:21:09.950
customers, and the operators on
the villages about clients.

00:21:09.950 --> 00:21:12.940
Others were concerned
about margins.

00:21:12.940 --> 00:21:16.330
And what you see in large
companies in this scattered

00:21:16.330 --> 00:21:19.660
world, they all have a slightly

00:21:19.660 --> 00:21:21.430
different view on reality.

00:21:21.430 --> 00:21:24.000
And a lot of discussions and
debates in that company.

00:21:24.000 --> 00:21:27.180
So commercial guy said let's
bring this all together.

00:21:27.180 --> 00:21:30.430
And by the way, if we've got so
much data and we determine

00:21:30.430 --> 00:21:35.620
and predict on customers, why
don't we do this on revenue?

00:21:35.620 --> 00:21:37.520
So you see here in the
first designs--

00:21:37.520 --> 00:21:40.420
I think this is five,
six, years ago--

00:21:40.420 --> 00:21:42.646
you saw a first design.

00:21:45.390 --> 00:21:46.780
So what happened?

00:21:46.780 --> 00:21:49.540
What you see in these large
companies, they start having

00:21:49.540 --> 00:21:51.430
an architectural picture.

00:21:51.430 --> 00:21:55.561
And of course, they want to
use proven technology--

00:21:55.561 --> 00:22:00.040
so Netezza Data Warehouse,
Business Objects--

00:22:00.040 --> 00:22:01.860
and let all the data flow.

00:22:01.860 --> 00:22:06.910
Well, I was there at the time
and, I think, tried five or

00:22:06.910 --> 00:22:11.865
six different solutions from
Microsoft Analytic Server to

00:22:11.865 --> 00:22:14.720
Business Object itself.

00:22:14.720 --> 00:22:17.110
It's too much to name.

00:22:17.110 --> 00:22:21.060
And they had all one
thing in common.

00:22:21.060 --> 00:22:23.370
They all failed.

00:22:23.370 --> 00:22:24.500
Everything failed.

00:22:24.500 --> 00:22:28.270
Everything that we tried,
everything failed.

00:22:28.270 --> 00:22:29.670
And why did it fail?

00:22:29.670 --> 00:22:31.490
Sometimes it was
too expensive.

00:22:31.490 --> 00:22:32.990
It was a new insight.

00:22:32.990 --> 00:22:36.770
Not something which you could
put there and it could earn

00:22:36.770 --> 00:22:39.860
money, it was just insight.

00:22:39.860 --> 00:22:43.220
I like to phrase, the benefit
of insight is insight.

00:22:43.220 --> 00:22:44.980
You still need to do something
with the insight.

00:22:44.980 --> 00:22:46.390
And if you don't do
anything with the

00:22:46.390 --> 00:22:47.690
insight it's just insight.

00:22:47.690 --> 00:22:50.980
So some of the vendors came,
yeah we can solve this problem

00:22:50.980 --> 00:22:51.600
with your forecasting.

00:22:51.600 --> 00:22:55.980
It will cost you 1 million euro
to just buy the software.

00:22:55.980 --> 00:22:58.030
So that failed on the expense.

00:22:58.030 --> 00:23:03.320
Another one was like, the
machine just stopped because

00:23:03.320 --> 00:23:05.160
it didn't work.

00:23:05.160 --> 00:23:08.810
And the other one, you increased
so much systems, you

00:23:08.810 --> 00:23:11.840
increased system management so
much that the IT guys were

00:23:11.840 --> 00:23:13.240
like, OK, sorry.

00:23:13.240 --> 00:23:15.640
That's not acceptable.

00:23:15.640 --> 00:23:18.330
So that all failed.

00:23:18.330 --> 00:23:21.480
So we thought, let's try
another approach.

00:23:21.480 --> 00:23:22.770
Completely different approach.

00:23:22.770 --> 00:23:24.840
We almost abandoned the
project with them.

00:23:24.840 --> 00:23:25.295
Just stopped.

00:23:25.295 --> 00:23:27.570
I said, well, OK, let's
do it differently.

00:23:27.570 --> 00:23:29.750
So what did we do?

00:23:29.750 --> 00:23:34.760
First we thought, let's
do it differently.

00:23:34.760 --> 00:23:37.090
So instead of having analysts
looking at the problem, we

00:23:37.090 --> 00:23:40.130
took a developer to look
at the problem.

00:23:40.130 --> 00:23:41.580
And we gave him one rule.

00:23:41.580 --> 00:23:44.670
You're only allowed to use
Google technology.

00:23:44.670 --> 00:23:45.720
And why did we do that?

00:23:45.720 --> 00:23:50.350
In that company, they had some
experience with a transaction

00:23:50.350 --> 00:23:51.380
application on Google.

00:23:51.380 --> 00:23:54.180
And what was liked about it,
it was in the cloud, of

00:23:54.180 --> 00:23:56.110
course, on the first machines.

00:23:56.110 --> 00:23:57.960
And we learned about BigTable.

00:23:57.960 --> 00:24:00.570
And we thought, well, if Google
can make such quick

00:24:00.570 --> 00:24:02.100
applications, why not we?

00:24:02.100 --> 00:24:05.380
Because they apparently seemed
to solve the solution.

00:24:05.380 --> 00:24:08.180
So we started learning about
column-based databases.

00:24:08.180 --> 00:24:12.100
And we thought, well, we'll
put all data in BigTable.

00:24:12.100 --> 00:24:13.580
That didn't work either.

00:24:13.580 --> 00:24:14.820
And then we were lucky.

00:24:14.820 --> 00:24:15.990
Sometimes you have a break.

00:24:15.990 --> 00:24:18.490
We learned about BigQuery.

00:24:18.490 --> 00:24:22.170
And it looks like that
BigQuery will do.

00:24:22.170 --> 00:24:25.560
But then there's always an issue
with a European company.

00:24:25.560 --> 00:24:27.610
Google didn't allow
us to use BigQuery

00:24:27.610 --> 00:24:29.696
because we were overseas.

00:24:29.696 --> 00:24:33.640
So we had to persuade a little
bit more, get some more phone

00:24:33.640 --> 00:24:36.700
calls, get the Google in
Europe working for us.

00:24:36.700 --> 00:24:38.600
And finally, we were made
a trusted tester.

00:24:38.600 --> 00:24:42.400
So we were, two years ago,
trusted tester on BigQuery.

00:24:42.400 --> 00:24:45.285
And we set ourselves the rule,
only use Google tools.

00:24:45.285 --> 00:24:48.620
So App Engine, Google Web
Toolkit, we found out that

00:24:48.620 --> 00:24:53.180
there is a great chart
in APIs there.

00:24:53.180 --> 00:24:55.260
And I still remember the
presentation from one of the

00:24:55.260 --> 00:24:56.620
Google I/Os a few years ago.

00:24:56.620 --> 00:24:58.220
There is an API for it.

00:24:58.220 --> 00:25:01.200
So we thought, if we can't
fix it, let's limit

00:25:01.200 --> 00:25:02.960
ourselves to Google.

00:25:02.960 --> 00:25:04.210
So we did.

00:25:08.550 --> 00:25:10.020
And we succeeded.

00:25:10.020 --> 00:25:12.580
And I will show you, in a
minute, the demo of the

00:25:12.580 --> 00:25:13.090
application.

00:25:13.090 --> 00:25:14.130
The application Is there.

00:25:14.130 --> 00:25:17.760
So it's an application which
is now also in other

00:25:17.760 --> 00:25:20.500
hospitality services.

00:25:20.500 --> 00:25:22.090
It's an application which
is being used by

00:25:22.090 --> 00:25:23.080
the real end user.

00:25:23.080 --> 00:25:26.410
So it's not a tool which we
implement for business user.

00:25:26.410 --> 00:25:30.180
And I think what happened is
that the flow of all the data,

00:25:30.180 --> 00:25:33.410
from all the back offices, is
now into cloud storages.

00:25:33.410 --> 00:25:36.160
Some is being uploaded
into BigQueries.

00:25:36.160 --> 00:25:38.300
Other stays in BigTable.

00:25:38.300 --> 00:25:42.590
And suddenly, you get the
feeling like Google does with

00:25:42.590 --> 00:25:45.950
customization or
consumerization.

00:25:45.950 --> 00:25:49.620
And suddenly a BI became really
in the hand of, let's

00:25:49.620 --> 00:25:52.100
say, the internal end user,
so not the consumer.

00:25:52.100 --> 00:25:57.790
But you suddenly see how this
works for real people who have

00:25:57.790 --> 00:26:01.100
not a technical background, not
even in computer science

00:26:01.100 --> 00:26:04.950
or in more technical marketing
departments.

00:26:04.950 --> 00:26:08.510
And they suddenly can go
through all the data.

00:26:08.510 --> 00:26:14.345
So let me show you
now the demo.

00:26:14.345 --> 00:26:16.934
We have to switch, I think.

00:26:20.590 --> 00:26:25.350
So bear one second with me.

00:26:25.350 --> 00:26:27.380
So I take, now, one viewer.

00:26:27.380 --> 00:26:28.600
And what is this viewer?

00:26:28.600 --> 00:26:31.810
If you're in travel, people book
today for a holiday in

00:26:31.810 --> 00:26:32.850
six months, of course.

00:26:32.850 --> 00:26:36.770
So what you see on the
screen is how--

00:26:36.770 --> 00:26:38.990
and this is on demo data--

00:26:38.990 --> 00:26:44.680
how the bookings of last week
are for the future.

00:26:44.680 --> 00:26:46.030
So we had this idea.

00:26:46.030 --> 00:26:48.950
So we would like to look
backwards, of course.

00:26:48.950 --> 00:26:50.080
So what did happen?

00:26:50.080 --> 00:26:52.060
What did we [? brought in ?]
in sales?

00:26:52.060 --> 00:26:53.800
And where will it land?

00:26:53.800 --> 00:27:00.230
So what you instantly see here
is that on this we put

00:27:00.230 --> 00:27:01.900
together a few years of data.

00:27:01.900 --> 00:27:05.520
And we compared July last year
with July the year before, and

00:27:05.520 --> 00:27:07.560
it instantly showed
up the graphs.

00:27:11.780 --> 00:27:13.590
I don't have that much time.

00:27:13.590 --> 00:27:14.610
So what I will do--

00:27:14.610 --> 00:27:17.520
you saw that July was behind,
or I will tell you that July

00:27:17.520 --> 00:27:18.150
was behind.

00:27:18.150 --> 00:27:19.960
And I will move to
a different view.

00:27:22.780 --> 00:27:29.550
I'm going to select now the
month of July and apply this.

00:27:29.550 --> 00:27:32.020
And what Ju-kay just did
with the BigQuery,

00:27:32.020 --> 00:27:33.770
this is BigQuery work.

00:27:33.770 --> 00:27:38.190
So picking July in the
background, it generates, of

00:27:38.190 --> 00:27:42.200
course, a BigQuery query, and it
just picked on the month of

00:27:42.200 --> 00:27:43.740
July of the last three years.

00:27:43.740 --> 00:27:46.970
This is for normal business
users who use Excel.

00:27:46.970 --> 00:27:50.270
Very difficult to do, and
especially when you see, well,

00:27:50.270 --> 00:27:52.140
I thought July was behind.

00:27:52.140 --> 00:27:54.090
But apparently it's
not behind.

00:27:54.090 --> 00:27:55.610
So how come?

00:27:55.610 --> 00:27:57.830
Well, maybe there's a
difference in weeks.

00:27:57.830 --> 00:28:01.220
So here I apply the
last week of July.

00:28:01.220 --> 00:28:04.880
And it compares to the same
week of July last year.

00:28:04.880 --> 00:28:06.520
And these are actually--

00:28:06.520 --> 00:28:10.770
every time I hit apply, it goes
back to the back end.

00:28:10.770 --> 00:28:14.570
And it comes up with the data.

00:28:14.570 --> 00:28:17.610
There is no data here, which
I don't understand.

00:28:17.610 --> 00:28:18.860
So let's do it again.

00:28:22.454 --> 00:28:25.460
Yeah, so it's not
the second week.

00:28:25.460 --> 00:28:28.500
So maybe it's the first week.

00:28:28.500 --> 00:28:31.780
And this is the way end users
want to understand their data.

00:28:31.780 --> 00:28:36.100
So I've just done a lot
of data queries.

00:28:36.100 --> 00:28:41.610
And then you see July is behind
in the first week, but

00:28:41.610 --> 00:28:42.790
I saw a bigger difference.

00:28:42.790 --> 00:28:45.710
Well, where's the difference
coming from?

00:28:45.710 --> 00:28:48.650
If you operate in Europe, and it
must be the same in United

00:28:48.650 --> 00:28:49.600
States, you've got States.

00:28:49.600 --> 00:28:51.350
In Europe you've
got countries.

00:28:51.350 --> 00:28:53.690
So lets interrogate
some countries.

00:28:53.690 --> 00:28:54.730
Well, there are a lot
of countries.

00:28:54.730 --> 00:28:59.640
This company has from these
countries and areas

00:28:59.640 --> 00:29:00.850
[INAUDIBLE].

00:29:00.850 --> 00:29:02.110
So I will drill it down.

00:29:02.110 --> 00:29:05.350
And what you saw in Ju-kay's
demonstration is now the

00:29:05.350 --> 00:29:07.140
effects from the back end.

00:29:07.140 --> 00:29:09.800
So this is real fetching it.

00:29:09.800 --> 00:29:12.490
So wow, there's a lot
of countries here.

00:29:12.490 --> 00:29:14.750
Going to make it slightly
easier, because I

00:29:14.750 --> 00:29:15.500
want to zoom in.

00:29:15.500 --> 00:29:18.025
I know already I want to zoom
in on a certain region.

00:29:18.025 --> 00:29:20.960
And I thought, well, maybe it's
Germany that's causing

00:29:20.960 --> 00:29:22.210
the problem.

00:29:25.680 --> 00:29:29.350
So, yeah, Germany was
causing the problem.

00:29:29.350 --> 00:29:32.890
So here, suddenly, the end users
making now queries to

00:29:32.890 --> 00:29:34.860
the back end and having
an overview in

00:29:34.860 --> 00:29:37.920
a very simple view.

00:29:37.920 --> 00:29:41.900
Then you wonder, well,
what's caused this?

00:29:41.900 --> 00:29:44.990
Let me see, is there
a certain region

00:29:44.990 --> 00:29:47.880
which causes this problem?

00:29:47.880 --> 00:29:50.600
And I know, because I prepared
this, of course, that there is

00:29:50.600 --> 00:29:53.410
a certain region that's
causing this problem.

00:29:53.410 --> 00:29:55.550
And now, in the hand of the
end users, they are using

00:29:55.550 --> 00:29:59.440
BigQuery without having any
knowledge about SQL,

00:29:59.440 --> 00:30:02.840
[INAUDIBLE], tooling,
cross tabulations.

00:30:02.840 --> 00:30:07.250
And it's so quick for them
that sometimes--

00:30:07.250 --> 00:30:09.850
when we first had this
application running, I still

00:30:09.850 --> 00:30:13.160
remember that one of the Germans
said to me, when it

00:30:13.160 --> 00:30:18.780
was customer implemented,
[SPEAKING GERMAN], which

00:30:18.780 --> 00:30:22.220
means, in good English, what
system do you look in?

00:30:22.220 --> 00:30:24.660
Because I don't know where
you get the data from.

00:30:24.660 --> 00:30:26.970
Because they were looking at the
old reports, the green and

00:30:26.970 --> 00:30:30.050
red reports from Business
Objects, and they couldn't

00:30:30.050 --> 00:30:33.680
figure out where I got
the data from.

00:30:33.680 --> 00:30:36.150
We had this conference call,
and during the meetings, I

00:30:36.150 --> 00:30:39.990
could look through their data
and tell them from what

00:30:39.990 --> 00:30:45.880
agents, for instance, in the
countries or what actions,

00:30:45.880 --> 00:30:48.260
like was the booking made
on the internet.

00:30:48.260 --> 00:30:49.830
Was the booking made
in a call center?

00:30:49.830 --> 00:30:53.380
Was the booking made via
the travel agents?

00:30:53.380 --> 00:30:58.440
What kind of actions revolved
around making this graph?

00:30:58.440 --> 00:31:02.790
And if an end was thinking,
OK this is about Germany.

00:31:02.790 --> 00:31:04.540
But what about Belgium?

00:31:04.540 --> 00:31:06.230
Well, in the old days, we
would have gone to the

00:31:06.230 --> 00:31:07.810
business analyst guy.

00:31:07.810 --> 00:31:12.460
Had to ask him, can you do
this again for Belgium?

00:31:12.460 --> 00:31:15.800
And then he would look at me
like, OK, so you just asked me

00:31:15.800 --> 00:31:18.310
to do everything on
this on Germany?

00:31:18.310 --> 00:31:20.800
And now you want me-- yeah,
and can you have it today?

00:31:20.800 --> 00:31:24.400
Well what's here now is
really you do it--

00:31:24.400 --> 00:31:24.760
[? is it OK? ?]

00:31:24.760 --> 00:31:27.516
OK, you just push this
one and you've got

00:31:27.516 --> 00:31:28.766
Belgium and Germany together.

00:31:33.510 --> 00:31:38.150
So instead of doing minutes
or hours or a no answer on

00:31:38.150 --> 00:31:42.790
traditional data warehousing
tooling, instead of waiting

00:31:42.790 --> 00:31:47.740
days on schedule reports,
you can interrogate.

00:31:47.740 --> 00:31:51.380
And I have to say, there is
about 40 million rows

00:31:51.380 --> 00:31:53.390
underneath this demo
in BigQuery.

00:31:53.390 --> 00:31:55.540
So it's not a Google set.

00:31:55.540 --> 00:31:57.260
But it's still, for a company,
a large set.

00:32:00.790 --> 00:32:02.680
The real end user, who's
using the tool, and not

00:32:02.680 --> 00:32:04.910
the business analyst--

00:32:04.910 --> 00:32:07.190
and I made some calculations.

00:32:07.190 --> 00:32:10.090
Not buying all the new software,
not buying all the

00:32:10.090 --> 00:32:13.360
hardware, saved about $800,000
in a project.

00:32:13.360 --> 00:32:16.910
So they are very pleased
with the tool.

00:32:16.910 --> 00:32:18.160
Thank you.

00:32:23.810 --> 00:32:24.370
NAVNEET JONEJA: Thank
you Richard.

00:32:24.370 --> 00:32:28.120
So I mean, you could imagine,
that taking data and turning

00:32:28.120 --> 00:32:31.640
it from a turnaround time of
hours to a turnaround time of

00:32:31.640 --> 00:32:35.050
seconds fundamentally changes
what you think you can do with

00:32:35.050 --> 00:32:36.890
your business, because you
can react to end users

00:32:36.890 --> 00:32:38.120
almost in real time.

00:32:38.120 --> 00:32:40.290
You can adjust your business
to deal with what you're

00:32:40.290 --> 00:32:43.970
hearing and seeing from
the market now.

00:32:43.970 --> 00:32:45.210
So let me quickly--

00:32:45.210 --> 00:32:46.530
very quickly--

00:32:46.530 --> 00:32:49.290
give you an overview of the
services we talked about.

00:32:49.290 --> 00:32:51.740
The data was stored in
Google Cloud Storage.

00:32:51.740 --> 00:32:53.990
Multiple layers of redundancy.

00:32:53.990 --> 00:32:56.360
Like all the services, powered
by the Google network, which

00:32:56.360 --> 00:32:59.180
is extremely reliable and high
performance, built to serve

00:32:59.180 --> 00:33:01.180
our own sites over several
years, and we

00:33:01.180 --> 00:33:02.420
think it really shows.

00:33:02.420 --> 00:33:04.000
You can store as much
data as you need.

00:33:04.000 --> 00:33:06.100
So as your data needs grow, and
as you decide you want to

00:33:06.100 --> 00:33:08.390
bring more data to bear, you can
do that without worrying

00:33:08.390 --> 00:33:11.110
about capacity and provisioning
and over capacity

00:33:11.110 --> 00:33:13.230
and overspend and underspend.

00:33:13.230 --> 00:33:16.340
And you can share data quickly
and effectively.

00:33:16.340 --> 00:33:19.800
You can use App Engines, various
tools, to both build

00:33:19.800 --> 00:33:22.790
applications as well as
transform your data.

00:33:22.790 --> 00:33:27.050
You can actually even make use
of what's by far the world's

00:33:27.050 --> 00:33:30.240
largest hosted NoSQL database,
the App Engine Datastore,

00:33:30.240 --> 00:33:32.060
which effectively gives you
the ability to store

00:33:32.060 --> 00:33:34.870
structured data, and query it
effectively and quickly,

00:33:34.870 --> 00:33:36.700
without having to worry
about schemas.

00:33:36.700 --> 00:33:39.530
You can have atomic
transactions, but you can get

00:33:39.530 --> 00:33:41.800
that at the expense of
giving up the ability

00:33:41.800 --> 00:33:43.670
to join across tables.

00:33:43.670 --> 00:33:45.380
But the good news is you could
actually do joins in the

00:33:45.380 --> 00:33:50.050
cloud, especially with large
data using either App Engine

00:33:50.050 --> 00:33:53.660
or using Google Compute
Engine.

00:33:53.660 --> 00:33:57.885
If you already have invested
time, money, technology, in

00:33:57.885 --> 00:34:00.130
the Hadoop ecosystem, you can
bring that to the cloud now

00:34:00.130 --> 00:34:03.450
and write in the Google cloud
close to your data and go

00:34:03.450 --> 00:34:06.520
from, again, your raw data to
your processed data so you can

00:34:06.520 --> 00:34:11.080
now actually make sense of
your data very quickly.

00:34:11.080 --> 00:34:14.530
And then finally, we talked
a lot about BigQuery.

00:34:14.530 --> 00:34:16.810
But the bottom line is, that's
where you actually get the

00:34:16.810 --> 00:34:18.830
insight from.

00:34:18.830 --> 00:34:21.510
So putting that all into the
form of an analytics pipeline,

00:34:21.510 --> 00:34:24.460
you store your data in any
number of places, depending on

00:34:24.460 --> 00:34:25.489
the kind of data you have.

00:34:25.489 --> 00:34:27.610
If it's structured, it
ends up in Datastore.

00:34:27.610 --> 00:34:30.260
If you're trying to gather
application logs from App

00:34:30.260 --> 00:34:32.400
Engine, you can store
it in Logstore.

00:34:32.400 --> 00:34:34.409
Or you could use Google Cloud
Storage for storing any

00:34:34.409 --> 00:34:36.090
generic data you have.

00:34:36.090 --> 00:34:41.230
You can then transform and
process it using either App

00:34:41.230 --> 00:34:44.920
Engine or Google
Compute Engine.

00:34:44.920 --> 00:34:48.070
And then you can create it using
BigQuery so that you can

00:34:48.070 --> 00:34:50.940
build your applications.

00:34:50.940 --> 00:34:55.130
So as Ju-kay spoke a few
minutes ago, there are

00:34:55.130 --> 00:34:56.270
multiple ways to do this.

00:34:56.270 --> 00:34:56.830
There's open source.

00:34:56.830 --> 00:34:58.250
There's proprietary
technology.

00:34:58.250 --> 00:35:02.950
We feel that both of them
have their drawbacks.

00:35:02.950 --> 00:35:04.200
Or you can just use
the Googlestack.

00:35:06.580 --> 00:35:10.630
So all of the technology you saw
today looked like a very

00:35:10.630 --> 00:35:12.850
well-integrated suite of
products, because it is.

00:35:12.850 --> 00:35:14.430
But it turns out, you don't
have to use all of these

00:35:14.430 --> 00:35:14.980
products together.

00:35:14.980 --> 00:35:16.300
You can use them individually.

00:35:16.300 --> 00:35:18.400
Each of them has
their own API.

00:35:18.400 --> 00:35:22.690
And to talk more about how you
can actually use an API to

00:35:22.690 --> 00:35:24.940
build a great application, I'd
like to invite one of our

00:35:24.940 --> 00:35:28.230
partners, Nicolas Raspal,
to talk more.

00:35:28.230 --> 00:35:29.930
NICOLAS RASPAL: Thank you.

00:35:29.930 --> 00:35:32.070
So my name is Nicolas.

00:35:32.070 --> 00:35:37.460
I'm the CTO of a company called
Bime, and our mission

00:35:37.460 --> 00:35:41.020
is to deliver business
intelligence via the cloud.

00:35:41.020 --> 00:35:46.650
So really, our goal is to
improve our business users to

00:35:46.650 --> 00:35:50.120
analyze the data at any scale
without having to think about

00:35:50.120 --> 00:35:51.070
technology.

00:35:51.070 --> 00:35:56.520
So we do that through
a very intuitive and

00:35:56.520 --> 00:35:58.100
powerful user interface.

00:35:58.100 --> 00:36:01.650
And I will show you that
in a couple of minutes.

00:36:01.650 --> 00:36:03.280
So we connect to a lot
of data sources.

00:36:03.280 --> 00:36:04.720
We connect to relational
databases.

00:36:04.720 --> 00:36:07.800
We connect to App Engine, Google
spreadsheet, Google

00:36:07.800 --> 00:36:13.400
Analytics, so we try to really
embrace where IT is today.

00:36:13.400 --> 00:36:16.680
And we were in a quest.

00:36:16.680 --> 00:36:19.470
We were in a quest to find
something like BigQuery.

00:36:19.470 --> 00:36:21.840
So it's kind of a
bold statement.

00:36:21.840 --> 00:36:25.160
We really think that BigQuery
is the revolution.

00:36:25.160 --> 00:36:26.680
Why?

00:36:26.680 --> 00:36:29.870
Because, as I said, we
were in a quest.

00:36:29.870 --> 00:36:34.180
We tried a lot of different
technologies to be able to do

00:36:34.180 --> 00:36:34.740
BI queries.

00:36:34.740 --> 00:36:40.500
So BI queries, if you think
about it, it's a large range

00:36:40.500 --> 00:36:43.020
query that aggregates
a lot of data.

00:36:43.020 --> 00:36:47.410
So typical queries give
me the sum of profit

00:36:47.410 --> 00:36:49.330
over time, for example.

00:36:49.330 --> 00:36:51.900
The second characteristic
is it has to be

00:36:51.900 --> 00:36:53.710
really, really fast.

00:36:53.710 --> 00:36:59.340
The problem is most of the
technology that was available

00:36:59.340 --> 00:37:00.730
is kind of expensive.

00:37:00.730 --> 00:37:04.390
As Ju-kay mentioned, it can be
expensive or complex if it's

00:37:04.390 --> 00:37:05.370
open source.

00:37:05.370 --> 00:37:11.020
With BigQuery, basically, it
removed all those obstacles.

00:37:11.020 --> 00:37:14.830
You get very scalable
back end to do all

00:37:14.830 --> 00:37:17.040
your kind of queries.

00:37:17.040 --> 00:37:18.300
So, seeing is believing.

00:37:18.300 --> 00:37:23.440
So we tried to show to you a
really quick, very quick, demo

00:37:23.440 --> 00:37:26.230
of what we can do on
top of BigQuery.

00:37:29.830 --> 00:37:33.600
If you can help me, Ju-kay.

00:37:33.600 --> 00:37:35.290
OK, wonderful.

00:37:35.290 --> 00:37:36.850
So we are in a web
browser here.

00:37:36.850 --> 00:37:39.820
And I would just pick
up one connection

00:37:39.820 --> 00:37:42.390
available in this account.

00:37:42.390 --> 00:37:45.950
And what you get on the left
here is all the column

00:37:45.950 --> 00:37:48.650
available in the BigQuery
database.

00:37:48.650 --> 00:37:50.800
And the way you do query,
it's really simple.

00:37:50.800 --> 00:37:56.480
You can just grab a column, and
you drop it to the layout.

00:37:56.480 --> 00:37:58.320
And by doing that, you
do your first query.

00:37:58.320 --> 00:38:02.110
So here, I just sent all
the quantity available.

00:38:02.110 --> 00:38:05.680
So we are in a retail
data set.

00:38:05.680 --> 00:38:11.050
So if I switch the aggregator,
so here, we are [? Houston, ?]

00:38:11.050 --> 00:38:13.130
but if I switch [? to counts, ?]
we would

00:38:13.130 --> 00:38:15.410
count the number of rows
available in this data set.

00:38:15.410 --> 00:38:19.200
So we are sitting on top
of a billion rows here.

00:38:19.200 --> 00:38:20.740
So as you can see,
it's really fast.

00:38:20.740 --> 00:38:21.480
It's live.

00:38:21.480 --> 00:38:22.200
No trick.

00:38:22.200 --> 00:38:23.780
No prior aggregation.

00:38:23.780 --> 00:38:26.650
Everything is done on the fly.

00:38:26.650 --> 00:38:30.470
So as I said at the beginning,
we really tried to improve our

00:38:30.470 --> 00:38:31.200
business users.

00:38:31.200 --> 00:38:34.560
So for a long time we'll forget
that we are sitting on

00:38:34.560 --> 00:38:36.110
a really large data set.

00:38:36.110 --> 00:38:39.970
It's not gigantic, but for a
live demo it's kind of-- well,

00:38:39.970 --> 00:38:43.320
that's the first thing that
BigQuery can give you.

00:38:43.320 --> 00:38:46.810
You can do demo on very
large data sets.

00:38:46.810 --> 00:38:50.190
And the first thing you want to
do when you think about a

00:38:50.190 --> 00:38:54.490
data set like this is, what is
the time frame we are on?

00:38:54.490 --> 00:38:59.600
So by just dragging another
column to the mix, I will

00:38:59.600 --> 00:39:02.460
expose all the quantity
by year.

00:39:02.460 --> 00:39:04.200
So this is it.

00:39:04.200 --> 00:39:07.020
And I will switch to a different
visualization.

00:39:07.020 --> 00:39:09.060
We are ready for queues
on that visualization.

00:39:09.060 --> 00:39:12.920
We have 16 type of visualization
with hundreds of

00:39:12.920 --> 00:39:13.790
customization points.

00:39:13.790 --> 00:39:15.670
So you can do a map.

00:39:15.670 --> 00:39:17.380
You can do relationship
on any of these.

00:39:17.380 --> 00:39:18.970
You can do a tree map.

00:39:18.970 --> 00:39:21.910
But for the sake of simplicity,
I will stick with

00:39:21.910 --> 00:39:22.950
a simple column chart.

00:39:22.950 --> 00:39:26.430
So these are the exact
same results but now

00:39:26.430 --> 00:39:27.610
through column charts.

00:39:27.610 --> 00:39:32.380
We see that in 2012, we don't
have a lot of data because

00:39:32.380 --> 00:39:33.020
it's not done.

00:39:33.020 --> 00:39:38.575
So let's break down into months
to try to picture the

00:39:38.575 --> 00:39:39.825
trend of the data.

00:39:42.070 --> 00:39:45.830
Here, we are working on sales
data set, but it can work with

00:39:45.830 --> 00:39:46.620
pretty much anything.

00:39:46.620 --> 00:39:48.240
It can be [INAUDIBLE] data.

00:39:48.240 --> 00:39:51.250
It can be AdWords data.

00:39:51.250 --> 00:39:54.050
It can be really anything and
you will get the same kind of

00:39:54.050 --> 00:39:55.150
flexibility.

00:39:55.150 --> 00:39:56.870
But let's go back
to this example.

00:39:56.870 --> 00:40:01.380
We see that the other quantity
are pretty steady, except that

00:40:01.380 --> 00:40:04.890
in April 2012 here we
have an increase.

00:40:04.890 --> 00:40:09.310
So the goal of the demo is
to try to understand why

00:40:09.310 --> 00:40:11.180
we have this peak.

00:40:11.180 --> 00:40:12.590
So I can interact
with the chart.

00:40:12.590 --> 00:40:16.360
I can really use the
visualization to interact with

00:40:16.360 --> 00:40:18.310
the data that is stored
in BigQuery.

00:40:18.310 --> 00:40:22.440
And each time I do something
like, well, I will decompose

00:40:22.440 --> 00:40:22.850
this point.

00:40:22.850 --> 00:40:26.030
I will go behind, literally
behind, this point and

00:40:26.030 --> 00:40:29.350
decompose that by, let's
say, sales channel.

00:40:29.350 --> 00:40:31.650
Each time I do something like
that, I'm [INAUDIBLE]

00:40:31.650 --> 00:40:34.760
BigQuery, and I'm processing
something like eight

00:40:34.760 --> 00:40:37.780
gigabytes, nine gigabytes
of data on the fly.

00:40:37.780 --> 00:40:39.990
And wow, it's amazingly fast.

00:40:39.990 --> 00:40:44.290
If you try to have this kind of
capability in house, and if

00:40:44.290 --> 00:40:46.640
you try to do that for several
customers on the same

00:40:46.640 --> 00:40:50.290
infrastructure, you have a
lot of pain to do that.

00:40:50.290 --> 00:40:54.660
And this is what you
get out of the box.

00:40:54.660 --> 00:40:56.900
Well, if you look at this chart,
we have the beginning

00:40:56.900 --> 00:40:58.390
of an answer.

00:40:58.390 --> 00:41:03.800
One of the sales channel is
holding most of the data.

00:41:03.800 --> 00:41:08.630
So if I just switch to another
visualization, it's 90% for

00:41:08.630 --> 00:41:10.010
one of the sales channels.

00:41:10.010 --> 00:41:13.180
But, well, still we don't have
the complete answer.

00:41:15.930 --> 00:41:18.010
We know that we have one
sales channel that is

00:41:18.010 --> 00:41:20.300
holding a lot of data.

00:41:20.300 --> 00:41:25.830
Maybe we can look at how it
compares with previous months.

00:41:25.830 --> 00:41:29.975
So to do that, I can just grab
months here, move that to

00:41:29.975 --> 00:41:32.590
another place in the
user interface.

00:41:32.590 --> 00:41:35.880
And I'm going to use a
feature that we have

00:41:35.880 --> 00:41:37.010
that is called Explosion.

00:41:37.010 --> 00:41:40.100
And basically, it will generate
one chart for each

00:41:40.100 --> 00:41:42.520
element in the explosion.

00:41:42.520 --> 00:41:47.010
So here I'm going to select
several months.

00:41:47.010 --> 00:41:51.110
And this will generate
one chart per month.

00:41:51.110 --> 00:41:51.780
Here we go.

00:41:51.780 --> 00:41:55.218
So I will just select
four months here.

00:41:58.430 --> 00:42:01.220
And why am I doing that?

00:42:01.220 --> 00:42:05.090
It's because you will see that
we will be able to compare

00:42:05.090 --> 00:42:08.065
each month with each
other very easily

00:42:08.065 --> 00:42:11.140
through different charts.

00:42:11.140 --> 00:42:14.530
So again, step back.

00:42:14.530 --> 00:42:15.845
We are [INAUDIBLE]

00:42:15.845 --> 00:42:17.480
half a billion rows
on the fly.

00:42:22.380 --> 00:42:25.110
We don't have to care about
the infrastructure.

00:42:25.110 --> 00:42:27.590
We used this data set a
couple of days to go

00:42:27.590 --> 00:42:28.470
to do a blog post.

00:42:28.470 --> 00:42:31.870
I will show you that
at the end.

00:42:31.870 --> 00:42:35.555
And we were pushing a dashboard
on the Google blog

00:42:35.555 --> 00:42:41.740
post to potentially millions
and millions of users.

00:42:41.740 --> 00:42:43.600
You can't do that in house.

00:42:43.600 --> 00:42:47.060
You have to rely on someone like
Google to do this kind of

00:42:47.060 --> 00:42:48.200
thing for you.

00:42:48.200 --> 00:42:50.010
So here we go.

00:42:50.010 --> 00:42:51.940
We have our four charts.

00:42:51.940 --> 00:42:54.940
So let's [INAUDIBLE]
that a bit.

00:42:57.560 --> 00:42:59.750
We have the answer
on this chart.

00:42:59.750 --> 00:43:03.110
So the resolution is low.

00:43:03.110 --> 00:43:05.190
I'm just switching
to another view.

00:43:05.190 --> 00:43:05.930
This is January.

00:43:05.930 --> 00:43:08.870
If you have a look at that,
the [? private-only ?]

00:43:08.870 --> 00:43:11.750
shops that somehow doesn't
exist in January.

00:43:11.750 --> 00:43:14.560
So I will move to
another month.

00:43:14.560 --> 00:43:17.420
February, still doesn't exist.

00:43:17.420 --> 00:43:19.480
March, still doesn't exist.

00:43:19.480 --> 00:43:24.030
Wow, in April, where we have a
peak in terms of sales, we

00:43:24.030 --> 00:43:25.290
have a new sales channel.

00:43:25.290 --> 00:43:26.790
So that's kind of
the explanation.

00:43:26.790 --> 00:43:28.625
Maybe our companion
have introduced

00:43:28.625 --> 00:43:29.580
and used that channel.

00:43:29.580 --> 00:43:31.080
That is working very well.

00:43:31.080 --> 00:43:33.790
And that's the explanation.

00:43:33.790 --> 00:43:37.260
Well, we have a beginning
of an explanation.

00:43:37.260 --> 00:43:40.260
And my goal was just to show
you that you can really

00:43:40.260 --> 00:43:43.050
interact with massive
amounts of data.

00:43:43.050 --> 00:43:44.470
No prior aggregation.

00:43:44.470 --> 00:43:46.640
No cache.

00:43:46.640 --> 00:43:49.830
All the time we think about it
in terms of business, not in

00:43:49.830 --> 00:43:51.143
terms of technology.

00:43:51.143 --> 00:43:54.650
And a couple of months ago, it
was not doable to do this kind

00:43:54.650 --> 00:43:56.090
of thing in the cloud.

00:43:56.090 --> 00:44:00.500
And now I believe it's kind
of the tipping point.

00:44:00.500 --> 00:44:03.710
Business intelligence is not
only doable in the cloud, but

00:44:03.710 --> 00:44:06.380
it's a fun, better, faster,
and cheaper

00:44:06.380 --> 00:44:08.910
than on-premise option.

00:44:13.630 --> 00:44:14.880
Thank you.

00:44:20.830 --> 00:44:24.760
JU-KAY KWEK: So I'm just going
to summarize very quickly.

00:44:24.760 --> 00:44:29.560
But what Nicolas was showing, he
actually made it look easy.

00:44:29.560 --> 00:44:31.820
I'm glad he pointed it out,
but that was a very

00:44:31.820 --> 00:44:33.920
large set of data.

00:44:33.920 --> 00:44:37.540
Using the set of tools from
Bime, or Richard showed

00:44:37.540 --> 00:44:40.120
before, was a custom application
with a very

00:44:40.120 --> 00:44:42.060
specific company and
domain focus.

00:44:42.060 --> 00:44:45.690
It becomes, actually, quite easy
to actually transform and

00:44:45.690 --> 00:44:48.850
shift our thinking from it's no
longer a technology problem

00:44:48.850 --> 00:44:50.070
dealing with data.

00:44:50.070 --> 00:44:52.400
We can really shift our focus to
where it ought to be, which

00:44:52.400 --> 00:44:55.390
is solving a business problem.

00:44:55.390 --> 00:44:58.420
We talked about the technologies
that Navneet

00:44:58.420 --> 00:45:00.090
brought us through.

00:45:00.090 --> 00:45:02.160
Nicolas demonstrated how to
build on top of APIs.

00:45:02.160 --> 00:45:05.260
And what we also realized is
it's very important for

00:45:05.260 --> 00:45:09.080
businesses that there's a strong
ecosystem to support

00:45:09.080 --> 00:45:11.150
them as well with other
kinds of tools.

00:45:11.150 --> 00:45:14.640
So I just wanted to highlight
this, that we're also working

00:45:14.640 --> 00:45:18.710
with a fairly broad array of
partners in the data storage

00:45:18.710 --> 00:45:22.900
space for things like NAS
replacement or online backup.

00:45:22.900 --> 00:45:23.730
In the ETLs--

00:45:23.730 --> 00:45:26.490
Extract, Transform, Load-- some
of the leading vendors in

00:45:26.490 --> 00:45:32.120
that space, who are ready to
announce connectors into

00:45:32.120 --> 00:45:34.925
systems like Cloud Storage
or BigQuery.

00:45:34.925 --> 00:45:37.610
Take your data from on premises,
or in other parts of

00:45:37.610 --> 00:45:39.250
the cloud, into our systems.

00:45:39.250 --> 00:45:41.390
Make that really easy
and automatable.

00:45:41.390 --> 00:45:45.130
And then as you just saw with
Nicolas, we have partners in

00:45:45.130 --> 00:45:48.640
the visualization space as well
to make it really easy

00:45:48.640 --> 00:45:51.460
with out-of-the-box, to slice
and dice really, really big

00:45:51.460 --> 00:45:55.410
sets of data, quickly get
to those insights.

00:45:55.410 --> 00:45:56.660
So just a quick summary.

00:46:00.540 --> 00:46:03.110
What we went through was we
talked about some really big

00:46:03.110 --> 00:46:04.960
data problems.

00:46:04.960 --> 00:46:07.500
Hopefully one of the takeaways
that you have is that it's

00:46:07.500 --> 00:46:09.770
really about focusing on solving
business problems

00:46:09.770 --> 00:46:11.480
rather than building
technology to

00:46:11.480 --> 00:46:13.490
just analyze data.

00:46:13.490 --> 00:46:17.100
For the line of business, it's
important to remember that

00:46:17.100 --> 00:46:19.460
using Google Cloud services,
you can actually start

00:46:19.460 --> 00:46:20.350
relatively small.

00:46:20.350 --> 00:46:22.180
You don't have to build
out a big data

00:46:22.180 --> 00:46:23.820
infrastructure from day one.

00:46:23.820 --> 00:46:26.940
Google Cloud services lets you
take an incremental approach.

00:46:26.940 --> 00:46:28.360
And we'll scale with you.

00:46:28.360 --> 00:46:31.280
That's true from everything from
Cloud Storage, BigQuery,

00:46:31.280 --> 00:46:33.270
App Engine, Compute Engine.

00:46:33.270 --> 00:46:36.900
And you get really, really
big applications as well.

00:46:36.900 --> 00:46:39.270
Really important, the more
questions that you can ask,

00:46:39.270 --> 00:46:40.580
the smarter those questions
become.

00:46:40.580 --> 00:46:43.000
This is kind of turning
the eye on its head.

00:46:43.000 --> 00:46:45.360
It used to be that you had to
kind of know the questions

00:46:45.360 --> 00:46:48.510
ahead of hand before you could
create the indexes and the

00:46:48.510 --> 00:46:51.940
cubes and then serve that up
to the business users.

00:46:51.940 --> 00:46:53.910
By putting data in the
cloud, that actually

00:46:53.910 --> 00:46:55.700
turns it on its head.

00:46:55.700 --> 00:46:57.470
This is something that we do
at Google all the time.

00:46:57.470 --> 00:47:00.370
We start out not really knowing,
sometimes, what is

00:47:00.370 --> 00:47:01.210
interesting in the data.

00:47:01.210 --> 00:47:04.050
And it's only by asking lots of
small questions on really

00:47:04.050 --> 00:47:07.610
big sets of data that we can
understand what's important.

00:47:07.610 --> 00:47:09.650
And through these tools, we're
allowing you to do the same

00:47:09.650 --> 00:47:12.580
thing with your data as well.

00:47:12.580 --> 00:47:17.610
If you're in the technology
side, IT and development, hey,

00:47:17.610 --> 00:47:19.410
this is new-school BI.

00:47:19.410 --> 00:47:21.160
Store all the data.

00:47:21.160 --> 00:47:23.080
Worry about joining
it, processing it,

00:47:23.080 --> 00:47:24.710
analyzing it, later on.

00:47:24.710 --> 00:47:26.720
That's really the name of the
game now, especially when you

00:47:26.720 --> 00:47:31.100
have more things like social
media, firehose, you've got

00:47:31.100 --> 00:47:33.330
mobile devices.

00:47:33.330 --> 00:47:36.050
The name of the game now is just
finding a place to store

00:47:36.050 --> 00:47:38.710
it all, and we provide you very
scalable systems to do

00:47:38.710 --> 00:47:42.450
that with things like the
Datastore, Cloud Storage.

00:47:42.450 --> 00:47:45.100
The cloud enables you to set up
pipelines to then process

00:47:45.100 --> 00:47:48.640
that on an automated basis and
to do it very rapidly, build

00:47:48.640 --> 00:47:49.670
these things very rapidly.

00:47:49.670 --> 00:47:52.997
Finally, you can actually build
apps on your data to set

00:47:52.997 --> 00:47:54.310
the data free.

00:47:54.310 --> 00:47:56.980
Again, this is something
that's quite new.

00:47:56.980 --> 00:48:01.330
It used to be that data was
trapped in traditional data

00:48:01.330 --> 00:48:04.960
warehouse systems with a team
of analysts or IT people who

00:48:04.960 --> 00:48:07.100
were kind of the gatekeepers
to it.

00:48:07.100 --> 00:48:11.230
What these tools allow you to do
is actually open up access

00:48:11.230 --> 00:48:14.670
in a secure way, in a scalable
way, to the rest of your

00:48:14.670 --> 00:48:17.770
organization and actually
develop the specific

00:48:17.770 --> 00:48:21.790
applications needed to solve
those business problems.

00:48:21.790 --> 00:48:23.480
So hopefully this session
was useful today.

00:48:23.480 --> 00:48:27.300
I'd just like to thank, again,
Bime and Crystalloids for

00:48:27.300 --> 00:48:28.730
joining us today.

00:48:28.730 --> 00:48:29.910
And thank you for your time.

00:48:29.910 --> 00:48:31.495
And we're happy to take
any questions.

00:48:38.120 --> 00:48:39.820
I did forget to make a plug.

00:48:39.820 --> 00:48:43.450
We have some other great
sessions about big data.

00:48:43.450 --> 00:48:46.040
We did, yesterday, have a
session on doing MapReduce

00:48:46.040 --> 00:48:49.120
pipelines in App Engine.

00:48:49.120 --> 00:48:51.650
The video should be encoded
and posted onto YouTube.

00:48:51.650 --> 00:48:55.440
Tomorrow we have a deep dive
into BigQuery internals.

00:48:55.440 --> 00:48:57.660
We have some of our engineering
team here to talk

00:48:57.660 --> 00:49:00.460
you through some best practices
for doing really,

00:49:00.460 --> 00:49:02.470
really, big, interesting
queries.

00:49:02.470 --> 00:49:05.170
We've got a session on cloud
storage as well.

00:49:05.170 --> 00:49:08.620
And we've also announced today
some really interesting

00:49:08.620 --> 00:49:14.450
sessions on Google Compute
Engine, which is the new VMs.

00:49:14.450 --> 00:49:16.400
If there are any questions,
please come up to the mics.

00:49:16.400 --> 00:49:17.340
Thanks.

00:49:17.340 --> 00:49:21.785
AUDIENCE: I think the question
is related to the querying of

00:49:21.785 --> 00:49:24.520
App Engine logs, which was
announced yesterday in the App

00:49:24.520 --> 00:49:27.110
Engine overview session.

00:49:27.110 --> 00:49:31.080
Logs are very generic in nature,
but if you want to

00:49:31.080 --> 00:49:34.630
analyze more specifically based
on the domain data that

00:49:34.630 --> 00:49:38.640
you might have, based on some
business data, would you have

00:49:38.640 --> 00:49:44.670
to write a specific set of API
to big data to have your own

00:49:44.670 --> 00:49:45.730
data structure?

00:49:45.730 --> 00:49:48.670
And if so, what are the
patterns possible?

00:49:48.670 --> 00:49:51.380
Because BigQuery doesn't allow
throughput beyond three or

00:49:51.380 --> 00:49:52.870
four requests per minute.

00:49:52.870 --> 00:49:55.850
So do you have any patterns that
you'd want just relating

00:49:55.850 --> 00:49:58.140
to something like mem
cache and writing it

00:49:58.140 --> 00:50:00.590
dyssynchronously or something
like that?

00:50:00.590 --> 00:50:03.560
JU-KAY KWEK: So the question
is about, I think, what are

00:50:03.560 --> 00:50:07.000
some of the patterns for getting
logs data, which might

00:50:07.000 --> 00:50:10.960
in a domain-specific format,
into BigQuery?

00:50:10.960 --> 00:50:12.860
AUDIENCE: That's the first
part, and the second part

00:50:12.860 --> 00:50:16.880
being the technical know how
of writing to the BigQuery,

00:50:16.880 --> 00:50:21.600
because the throughput to write
is lesser in the case of

00:50:21.600 --> 00:50:23.480
BigQuery per minute.

00:50:23.480 --> 00:50:25.880
JU-KAY KWEK: So maybe part of
the second question is there

00:50:25.880 --> 00:50:30.410
was a significant ingestion
throughput, which basically,

00:50:30.410 --> 00:50:34.520
the way it works is you send
us an ingestion job.

00:50:34.520 --> 00:50:36.420
And we'll put that on a queue.

00:50:36.420 --> 00:50:38.490
And we'll process that when
free cycles come by.

00:50:38.490 --> 00:50:41.560
It is subject to certain rate
limits, but through a

00:50:41.560 --> 00:50:45.760
combination of compression and
parallel import jobs, you can

00:50:45.760 --> 00:50:48.750
actually be importing
significant amounts of data,

00:50:48.750 --> 00:50:50.660
hundreds of gigabytes
of data, at a time.

00:50:50.660 --> 00:50:52.470
So give that a try and see.

00:50:52.470 --> 00:50:56.170
To your question about preparing
data, I actually

00:50:56.170 --> 00:50:59.380
highly encourage you, if you
didn't, to look at the

00:50:59.380 --> 00:51:02.080
MapReduce pipelines session.

00:51:02.080 --> 00:51:06.720
One of our developer of programs
engineers actually

00:51:06.720 --> 00:51:12.460
walks through how you do some
basic preparation of data from

00:51:12.460 --> 00:51:14.800
the Logstore and do
some massaging.

00:51:14.800 --> 00:51:17.465
And I think the example is just
massaging some of the

00:51:17.465 --> 00:51:18.000
date times.

00:51:18.000 --> 00:51:21.180
But it's representative of
reformatting the data, putting

00:51:21.180 --> 00:51:23.780
that into a MapReduce pipeline,
and adjusting it

00:51:23.780 --> 00:51:24.480
into BigQuery.

00:51:24.480 --> 00:51:28.130
AUDIENCE: That's the 27th?

00:51:28.130 --> 00:51:30.830
JU-KAY KWEK: Building Into
Pipelines at Google Scale.

00:51:30.830 --> 00:51:32.200
So check that one
out on YouTube.

00:51:36.490 --> 00:51:40.350
AUDIENCE: Can you reveal more
detail about the Compute

00:51:40.350 --> 00:51:44.370
Engine and this BigQuery,
how they work together?

00:51:44.370 --> 00:51:49.280
JU-KAY KWEK: So if you look at
the updated sessions that were

00:51:49.280 --> 00:51:52.810
posted today, there are a
significant number of sessions

00:51:52.810 --> 00:51:55.700
on Compute Engine itself.

00:51:55.700 --> 00:51:58.770
I forget which one, but it's
later today, there's actually

00:51:58.770 --> 00:52:02.180
one where we actually have a
demonstration of running a

00:52:02.180 --> 00:52:07.200
Hadoop job within Compute Engine
to extract a bunch of,

00:52:07.200 --> 00:52:09.230
I think it's in a Wikipedia
revision data which is a

00:52:09.230 --> 00:52:12.610
pretty large data set, and then
ingest that into BigQuery

00:52:12.610 --> 00:52:14.170
as a practical use case.

00:52:14.170 --> 00:52:17.220
So if you're interested in
Compute Engine for doing data

00:52:17.220 --> 00:52:19.070
processing jobs like that,
I recommend that

00:52:19.070 --> 00:52:20.470
you go for that session.

00:52:20.470 --> 00:52:21.816
NAVNEET JONEJA: I just
put up the session

00:52:21.816 --> 00:52:23.600
names on the screen.

00:52:23.600 --> 00:52:24.880
If you see the bottom-right
corner, those would be

00:52:24.880 --> 00:52:26.130
interesting sessions for you.

00:52:31.380 --> 00:52:33.250
JU-KAY KWEK: If no other
questions, thanks for your

00:52:33.250 --> 00:52:34.270
attention today.

00:52:34.270 --> 00:52:35.520
NAVNEET JONEJA: Thank you.

