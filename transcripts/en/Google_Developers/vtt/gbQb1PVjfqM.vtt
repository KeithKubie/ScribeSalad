WEBVTT
Kind: captions
Language: en

00:00:01.000 --> 00:00:04.260
&gt;&gt;&gt; Ladies and gentlemen, would you please
welcome Jeff Sharkey.

00:00:04.260 --> 00:00:09.240
[ Applause ]
&gt;&gt;Jeff Sharkey: Hi everyone, my name is Jeff.

00:00:09.240 --> 00:00:12.370
I'm a software engineer on the Android team.

00:00:12.370 --> 00:00:16.130
Today we're going to talk about doing more
with less.

00:00:16.130 --> 00:00:20.570
And along the way, we're going to build an
app that's worth a billion dollars.

00:00:20.570 --> 00:00:21.960
So where do we get started?

00:00:21.960 --> 00:00:26.960
First, I think we all can agree that we want
to build a great user experience, an experience

00:00:26.960 --> 00:00:30.380
that is fast when we are interacting with
the app in the foreground.

00:00:30.380 --> 00:00:34.010
But also lightweight when we are running in
the background.

00:00:34.010 --> 00:00:37.460
Really, this all comes down to resources.

00:00:37.460 --> 00:00:43.920
Resources like CPU and memory and disk I/O.
On a mobile device, these resources are constrained

00:00:43.920 --> 00:00:47.440
and they are shared amongst all the apps running
on the device.

00:00:47.440 --> 00:00:51.379
If we want to build that great user experience
we just described, we're going to need to

00:00:51.379 --> 00:00:52.879
work together.

00:00:52.879 --> 00:00:57.681
So today as we build an app, I'll show you
how we can work together and how we can do

00:00:57.681 --> 00:00:59.990
more with less.

00:00:59.990 --> 00:01:03.960
So let's start by building a simple photo
gallery app made of two activities.

00:01:03.960 --> 00:01:09.280
The activity on the left is a simple overview
of the photos the user has taken on the device.

00:01:09.280 --> 00:01:13.810
There's a second activity on the right we
can drill down and see a detailed zoomed in

00:01:13.810 --> 00:01:16.600
view of a specific photo.

00:01:16.600 --> 00:01:20.240
So to build this today, we will be using the
media store contented divider on the Android

00:01:20.240 --> 00:01:22.000
platform.

00:01:22.000 --> 00:01:26.479
We can query the images.media.external.

00:01:26.479 --> 00:01:33.020
We get back a list of the photos the user
has taken on the device.

00:01:33.020 --> 00:01:40.740
One of the columns is the underscore ID column,
that.

00:01:40.740 --> 00:01:46.380
With one of those IDs, we can use the thumbnails.get
thumbnail API to get a small scaled down version

00:01:46.380 --> 00:01:49.530
of one of the photos the usuals has taken.

00:01:49.530 --> 00:01:55.590
With the underscore ID we can build I/O a
full resolution of the photo to show the to

00:01:55.590 --> 00:01:56.590
the user.

00:01:56.590 --> 00:02:05.560
A moat today, I'll be digging through code
snippets.

00:02:05.560 --> 00:02:11.700
I will be using the loader pattern to perform
that query and get that cursor.

00:02:11.700 --> 00:02:16.000
That cursor comes back, I will hook it up
into a simple cursor adapter.

00:02:16.000 --> 00:02:20.950
So each of the views that I will be showing
in the grid layout, grid view is going to

00:02:20.950 --> 00:02:24.790
be an image view for each of the photos in
the list.

00:02:24.790 --> 00:02:28.480
When I go to bind the view, I'm actually going
to kick off an async task to go load that

00:02:28.480 --> 00:02:29.870
thumbnail on a background thread.

00:02:29.870 --> 00:02:32.230
Because loading thumbnails, we are going to
go to disk.

00:02:32.230 --> 00:02:36.500
Disk I/O can potentially block the main thread
of the application.

00:02:36.500 --> 00:02:42.360
So hopefully everyone here is familiar with
async task and that pattern on the platform.

00:02:42.360 --> 00:02:45.170
So here's the actual thumbnail async task,
what I'm doing.

00:02:45.170 --> 00:02:48.010
The actual argument coming in is that photo
ID.

00:02:48.010 --> 00:02:51.430
The bitmap is that small thumbnail.

00:02:51.430 --> 00:02:54.760
Here in the middle in that background thread,
I'm using thumbnail, dot get thumbnail.

00:02:54.760 --> 00:02:55.760
Load the bit map.

00:02:55.760 --> 00:02:59.959
Pass it back to the main thread and assign
it into that image to show to the user.

00:02:59.959 --> 00:03:01.731
Let's go ahead and take a look at this on
the device today.

00:03:01.731 --> 00:03:08.769
Switch to the device.

00:03:08.769 --> 00:03:10.640
So here's some photos have been taking around
the conference.

00:03:10.640 --> 00:03:12.410
I can scroll back and forth.

00:03:12.410 --> 00:03:15.349
Notice this scrolling is nice and smooth.

00:03:15.349 --> 00:03:19.040
It stays smooth because we are doing that
disk I/O on a background thread.

00:03:19.040 --> 00:03:24.110
But at the same time, if I scroll back and
forth, you will notice we end up loading the

00:03:24.110 --> 00:03:28.599
photos multiple times, photos that we loaded
moments earlier, we are now actually having

00:03:28.599 --> 00:03:29.810
to go to disk again.

00:03:29.810 --> 00:03:35.590
We are kicking off a number of those thumbnail
async tasks in doing a bunch of duplicated

00:03:35.590 --> 00:03:36.590
work.

00:03:36.590 --> 00:03:38.459
We would like to avoid that, if possible.

00:03:38.459 --> 00:03:40.510
One way to avoid that is caching.

00:03:40.510 --> 00:03:43.959
There's a couple different ways of writing
caches on Android.

00:03:43.959 --> 00:03:46.480
One way would be to use soft references.

00:03:46.480 --> 00:03:50.840
Soft references don't give us much control
over when and how that memory is cleared by

00:03:50.840 --> 00:03:51.840
the VM.

00:03:51.840 --> 00:03:54.970
Instead, we are going to use a LRU cache.

00:03:54.970 --> 00:03:59.319
Let's go back to the slides.

00:03:59.319 --> 00:04:03.370
So an LRU cache, LRU stands for least recently
used.

00:04:03.370 --> 00:04:05.040
A quick overview.

00:04:05.040 --> 00:04:08.230
Another U cache is a bounded data structure,
it has a bounded size.

00:04:08.230 --> 00:04:13.959
It keeps track of how recently items in the
cache -- how recently we have accessed them.

00:04:13.959 --> 00:04:17.609
Thinking of our thumbnail example today, photos
that the user may have just recently looked

00:04:17.609 --> 00:04:20.449
at are kept fresh in the cache near the front.

00:04:20.449 --> 00:04:25.470
But thumbnails that maybe are off in the periphery
are closer toward the end of the cache.

00:04:25.470 --> 00:04:30.240
Think as the cache needs to make more room,
it can evict the oldest items in the cache.

00:04:30.240 --> 00:04:33.630
So Android actually offers a LRU cache implementation.

00:04:33.630 --> 00:04:35.759
It's been in the platform since API level
12.

00:04:35.759 --> 00:04:38.360
It's also in the support-v4 library.

00:04:38.360 --> 00:04:41.860
You can use it on a wide range of devices.

00:04:41.860 --> 00:04:45.139
Let's use it here today to build our thumbnail
cache.

00:04:45.139 --> 00:04:49.460
LRU cache is a genericized class that takes
two types.

00:04:49.460 --> 00:04:51.240
A key and a value.

00:04:51.240 --> 00:04:54.389
For our purposes, we will map from a long
to a bitmap.

00:04:54.389 --> 00:04:57.861
The long is the photo ID, the key of the cache.

00:04:57.861 --> 00:05:01.770
Bitmap is if we have loaded one of the thumbnails
in the past.

00:05:01.770 --> 00:05:07.160
One thing that's nice about LRU cache, it
offers us a size of method that we can override.

00:05:07.160 --> 00:05:11.340
This gives us an opportunity to communicate
size of a specific item we are storing in

00:05:11.340 --> 00:05:16.120
the cache, how much we would like it to count
against the maximum size or bounded size of

00:05:16.120 --> 00:05:18.060
our cache.

00:05:18.060 --> 00:05:21.350
In this case, I'm just going to return the
number of bytes that the bitmap object actually

00:05:21.350 --> 00:05:22.960
represents in memory.

00:05:22.960 --> 00:05:28.729
Just as an example, typical 320 by 240 bitmap
uncompressed in memory is about 307K, just

00:05:28.729 --> 00:05:30.830
a ball park example.

00:05:30.830 --> 00:05:34.690
You'd expect the two last methods here, just
a put and get you'd expect from a typical

00:05:34.690 --> 00:05:35.690
caching API.

00:05:35.690 --> 00:05:36.690
We can put items in.

00:05:36.690 --> 00:05:39.139
We can attempt to see if an item is in the
cache with a get.

00:05:39.139 --> 00:05:44.160
It will return null if it is cache miss, if
the item isn't in the cache.

00:05:44.160 --> 00:05:48.210
Let's go ahead and hook this up into our application,
the one we have been writing today.

00:05:48.210 --> 00:05:51.450
The first question that we need to answer,
how large do we make it cache?

00:05:51.450 --> 00:05:53.350
It is a bounded size.

00:05:53.350 --> 00:05:56.419
If we make the cache too small, it's not going
to be effective.

00:05:56.419 --> 00:06:01.919
We will end up going to disk quite frequently
to load data if it's not in the cache.

00:06:01.919 --> 00:06:06.460
But if we make our cache too large, we actually
risk exposing our application to an out-of-memory

00:06:06.460 --> 00:06:07.960
error.

00:06:07.960 --> 00:06:11.830
Whatever we are allocating inside out LRU
cache, we need to keep enough memory for the

00:06:11.830 --> 00:06:16.699
other parts of our application like our activities
and layout that we are inflating.

00:06:16.699 --> 00:06:23.050
So a good place to start for a cache size
is the get memory class on activity manager.

00:06:23.050 --> 00:06:28.539
This returns a size in megabytes of how hard
of a memory limit we should impose in our

00:06:28.539 --> 00:06:31.380
application to let the system work its best.

00:06:31.380 --> 00:06:37.080
So some typical examples for this are about
48 megabytes on a Nexus S device and 64 megabytes

00:06:37.080 --> 00:06:38.380
on a Galaxy Nexus.

00:06:38.380 --> 00:06:43.000
The reason there is Galaxy Nexus has a gigabyte
of RAM so it has more space that applications

00:06:43.000 --> 00:06:45.430
can take advantage of.

00:06:45.430 --> 00:06:50.440
Okay, so we have -- in this case, I'm getting
the memory class, turning it into bytes, converting

00:06:50.440 --> 00:06:54.340
from megabytes to bytes, and then I'm creating
the thumbnail cache.

00:06:54.340 --> 00:07:00.229
A good place to start for the cache size is
dividing that memory class by 8.

00:07:00.229 --> 00:07:04.610
So keep in mind that we have to keep enough
head room for the rest of our application

00:07:04.610 --> 00:07:05.820
activities and layouts.

00:07:05.820 --> 00:07:07.320
Dividing by 8 is a good starting point.

00:07:07.320 --> 00:07:12.380
We will definitely want to look at our application
and tweak that value over time.

00:07:12.380 --> 00:07:16.639
Keeping in mind those two trade-offs if we
make it too small or too large.

00:07:16.639 --> 00:07:20.790
Now we have our cache, let's hook it up in
the two classes we were looking at earlier.

00:07:20.790 --> 00:07:24.940
First in the async task, once we have load
the bitmap from the thumbnail API, we will

00:07:24.940 --> 00:07:31.319
store it in the cache.

00:07:31.319 --> 00:07:34.690
Then over in the cursor adapter, when we are
binding one of the views, we will check the

00:07:34.690 --> 00:07:38.639
cache first just to see if we get a cache
hit, if that image has already been loaded

00:07:38.639 --> 00:07:40.080
and it is still around for us to use.

00:07:40.080 --> 00:07:43.960
If it is, we set it immediately and return
instantly.

00:07:43.960 --> 00:07:49.479
If it is not in the cache, a cache miss, we
will need to kick off the async task to bootstrap

00:07:49.479 --> 00:07:52.389
to load the thumbnail the first time.

00:07:52.389 --> 00:07:56.509
Let's go ahead and take a peek at this on
the device.

00:07:56.509 --> 00:08:01.270
Switch the device.

00:08:01.270 --> 00:08:04.720
In this case today it's the same app and I
just have a menu item where I can enable the

00:08:04.720 --> 00:08:05.900
cache.

00:08:05.900 --> 00:08:09.759
The first thing you will notice is down in
the bottom right, I'm surfacing some statistics

00:08:09.759 --> 00:08:11.569
about the cache, how effective it is.

00:08:11.569 --> 00:08:15.720
These statistics are automatically tracked
by LRU cache, the class on the platform.

00:08:15.720 --> 00:08:18.629
I'm surfacing here to see how effective our
cache is.

00:08:18.629 --> 00:08:22.550
They are available, if you're interested,
for your own apps.

00:08:22.550 --> 00:08:24.910
So now as we scroll through our list.

00:08:24.910 --> 00:08:26.280
You can see that we still have to bootstrap.

00:08:26.280 --> 00:08:28.389
We still have to go load the items.

00:08:28.389 --> 00:08:32.020
Now as we scroll back and forth, those photos
are coming in instantly.

00:08:32.020 --> 00:08:37.180
Scroll all the way down, back up, and through
the list.

00:08:37.180 --> 00:08:40.409
A lot of them are in the cache and now they
are filling in perfectly.

00:08:40.409 --> 00:08:45.900
Smooth UI, at the same time, I'm avoiding
the duplicated disk I/O. I'm taking advantage

00:08:45.900 --> 00:08:48.880
of the room on the device to make my app faster.

00:08:48.880 --> 00:08:52.290
We can see how much of a difference that's
actually making in the statistics.

00:08:52.290 --> 00:08:56.410
The number of cache hits we have had down
in the bottom right there is 449.

00:08:56.410 --> 00:08:59.710
So just in this quarterbacking example of
flinging back and forth in the list, we've

00:08:59.710 --> 00:09:04.480
avoided having to go to disk and load data
over 400 times.

00:09:04.480 --> 00:09:09.399
Caching is a great example of how we can do
more with less on the device.

00:09:09.399 --> 00:09:13.500
But now you might ask the question, how much
memory are we actually using?

00:09:13.500 --> 00:09:15.950
There's a great utility that can help us answer
this question.

00:09:15.950 --> 00:09:21.880
So let's go back to the slides.

00:09:21.880 --> 00:09:25.720
So today there's a great utility called procrank.

00:09:25.720 --> 00:09:30.330
Let's use that on the device today.

00:09:30.330 --> 00:09:38.370
Procrank is a list of all of the processes
that are running on the device sorted by how

00:09:38.370 --> 00:09:39.600
much memory they are using.

00:09:39.600 --> 00:09:45.490
In Linux there is many different ways of slicing
and dicing memory usage.

00:09:45.490 --> 00:09:47.140
That's what these various columns are.

00:09:47.140 --> 00:09:52.360
The number we are most interested in today
is USS, the unique set size of our application.

00:09:52.360 --> 00:09:54.700
I will come back to that in a moment.

00:09:54.700 --> 00:09:59.820
But first, Android at a high level, is a multitasking
operating system.

00:09:59.820 --> 00:10:04.570
We want to enable users to quickly switch
between multiple applications.

00:10:04.570 --> 00:10:09.630
Android does this by caching applications
in the background.

00:10:09.630 --> 00:10:14.310
When you switch between an application, between
apps, it keeps your process running including

00:10:14.310 --> 00:10:17.060
any memory that you have allocated.

00:10:17.060 --> 00:10:22.190
But at the same time, if a user is in a foreground
app and that foreground app needs memory and

00:10:22.190 --> 00:10:26.529
the system has used up all of the physical
memory on the device, the system needs to

00:10:26.529 --> 00:10:28.500
come up with that memory somewhere.

00:10:28.500 --> 00:10:32.380
So it begins looking through this list of
cached background applications, looking for

00:10:32.380 --> 00:10:37.480
apps that can kill to reclaim that memory
to be used by the foreground process.

00:10:37.480 --> 00:10:39.950
And that's where that USS metric comes into
play.

00:10:39.950 --> 00:10:46.190
USS is a rough metric of the memory the system
would reclaim if it killed that process.

00:10:46.190 --> 00:10:49.810
So if we look at this list, our application,
this gallery app that we have just written

00:10:49.810 --> 00:10:54.490
today, now that's it's caching the data, it's
at the top of that list.

00:10:54.490 --> 00:10:58.850
If some other process on the system comes
to the foreground and needs memory, our app

00:10:58.850 --> 00:11:02.610
is actually first in line to be killed on
the device.

00:11:02.610 --> 00:11:04.149
This will end up giving us a bad user experience.

00:11:04.149 --> 00:11:08.050
If our app is killed and the user does want
to come back, we have to start from scratch.

00:11:08.050 --> 00:11:09.980
We have to fork this IGOT.

00:11:09.980 --> 00:11:13.019
We have to inflate our activities and any
layouts we were showing to the user.

00:11:13.019 --> 00:11:16.470
And so, if possible, we would like to stay
cached.

00:11:16.470 --> 00:11:18.360
And there's a way to do this.

00:11:18.360 --> 00:11:20.450
Android offers -- it doesn't have to stay
this way.

00:11:20.450 --> 00:11:22.700
We don't have to keep using all this memory.

00:11:22.700 --> 00:11:26.100
Android offers a set of callbacks where we
can help release some of this memory back

00:11:26.100 --> 00:11:27.680
to the system.

00:11:27.680 --> 00:11:29.470
The first callback is on low memory.

00:11:29.470 --> 00:11:32.040
Which has been around since 1.0 in the platform.

00:11:32.040 --> 00:11:37.850
The downside of that is that method, that
callback is only called after all of the background

00:11:37.850 --> 00:11:40.050
apps have already been killed on the device.

00:11:40.050 --> 00:11:43.180
So if we are running in the background, it's
probably not very useful to us.

00:11:43.180 --> 00:11:44.700
We would never receive this callback.

00:11:44.700 --> 00:11:46.670
We would be killed.

00:11:46.670 --> 00:11:51.910
So starting in Ice Cream Sandwich, in API
14, there is a new API, a new callback called

00:11:51.910 --> 00:11:57.180
on trim memory which gives us a much more
granular view of the system state of how much

00:11:57.180 --> 00:11:59.960
memory the system would like us to release
back.

00:11:59.960 --> 00:12:04.920
And that level that it provides is the argument,
is a value between zero and a hundred indicating

00:12:04.920 --> 00:12:07.910
how aggressively we should return memory back
to the system.

00:12:07.910 --> 00:12:11.930
We will get into that a little bit more in
a second.

00:12:11.930 --> 00:12:17.260
So both of these methods, both on memory and
on trim memory, are ready to override today

00:12:17.260 --> 00:12:21.030
on activity fragment service content provider
and application.

00:12:21.030 --> 00:12:25.170
If you find yourself outside of one of those
life cycles, you can also implement the interface

00:12:25.170 --> 00:12:31.500
and call register component callbacks to use
if you are outside one of those life cycles.

00:12:31.500 --> 00:12:38.190
So let's actually hook this up in the app
we have been writing today.

00:12:38.190 --> 00:12:42.180
We talked about how there's a trim level that
comes in between zero and a hundred.

00:12:42.180 --> 00:12:45.279
There's actually a handful of values that
are well defined.

00:12:45.279 --> 00:12:49.130
Trim memory complete, moderate, background,
and UI hidden.

00:12:49.130 --> 00:12:51.320
Those have been defined in Ice Cream Sandwich,
in API 14.

00:12:51.320 --> 00:12:53.079
Those are well defined.

00:12:53.079 --> 00:12:57.480
We can check against those levels, check against
those constants, but we don't want to do a

00:12:57.480 --> 00:13:02.769
direct equality check because new levels could
be introduced into the future.

00:13:02.769 --> 00:13:07.890
Here in the code sample, we will check against
the most aggressive trim level that we are

00:13:07.890 --> 00:13:08.890
interested in.

00:13:08.890 --> 00:13:12.639
So in this code example, we are checking if
the level is greater than trim memory moderate

00:13:12.639 --> 00:13:15.610
which has a value of 60 under the hood.

00:13:15.610 --> 00:13:21.230
This trim memory moderate indicates that our
application is nearing the middle of the list

00:13:21.230 --> 00:13:25.899
of cached back applications, so the user hasn't
interacted with our app in quite a while.

00:13:25.899 --> 00:13:28.920
They are off doing some other tasks on their
device.

00:13:28.920 --> 00:13:32.680
The user make come back to our app but probably
not.

00:13:32.680 --> 00:13:36.110
So this -- we can use this as a good hint
to release the memory that we have been using

00:13:36.110 --> 00:13:37.540
for our thumbnail cache.

00:13:37.540 --> 00:13:38.540
That's what we are doing here.

00:13:38.540 --> 00:13:42.980
We are going to the cache and calling evict
all which will return all of that memory.

00:13:42.980 --> 00:13:45.400
Next we are checking for slightly lower level.

00:13:45.400 --> 00:13:48.850
Trim memory background which has a value of
40.

00:13:48.850 --> 00:13:53.200
This is a hint from the system that our application
is just entering the list of cached background

00:13:53.200 --> 00:13:57.290
apps, so the user was using us recently and
they may have switched away to other tasks.

00:13:57.290 --> 00:14:02.519
We are going to use this, this is a place
where the fact that we are using a LRU cache

00:14:02.519 --> 00:14:04.420
becomes valuable.

00:14:04.420 --> 00:14:09.940
So remember earlier that LRU cache keeps track
of which items we've used recently and also

00:14:09.940 --> 00:14:12.850
which ones are the oldest items in the cache.

00:14:12.850 --> 00:14:17.649
So we can use the trim to size method to actually
evict or release the memory for the oldest

00:14:17.649 --> 00:14:20.050
half of the items in our cache.

00:14:20.050 --> 00:14:23.570
So if the user does decide to come back to
our application, we still have some of the

00:14:23.570 --> 00:14:27.959
photos around the area the user was just looking
so it's fast and responsive there.

00:14:27.959 --> 00:14:30.920
Photos that are further off in the periphery,
we have been able to release the memory back

00:14:30.920 --> 00:14:32.700
to the system.

00:14:32.700 --> 00:14:39.290
Just a side note, trim to size is actually
a private method on that LRU cache class,

00:14:39.290 --> 00:14:43.180
but you can make a copy of the class and make
it public if you need to use it.

00:14:43.180 --> 00:14:47.860
Another thing to note is that these callbacks
are purely advisory.

00:14:47.860 --> 00:14:49.990
No guarantee that the system will call them.

00:14:49.990 --> 00:14:54.430
In fact, if the system is under extreme memory
pressure, it may end up killing our application

00:14:54.430 --> 00:14:57.180
before it has a chance to deliver the callbacks.

00:14:57.180 --> 00:15:03.160
But if we release memory through these callbacks,
the system may be able to keep our app cached

00:15:03.160 --> 00:15:05.149
longer, which is a better user experience.

00:15:05.149 --> 00:15:08.150
We are able to launch our app faster if the
use does come back.

00:15:08.150 --> 00:15:13.910
All right, so now our app, we have our gallery
app, it's running fast.

00:15:13.910 --> 00:15:18.900
We're caching items, releasing some of the
memory back to the system.

00:15:18.900 --> 00:15:21.079
So let's go ahead and add another feature.

00:15:21.079 --> 00:15:23.510
Let's add the ability to star photos.

00:15:23.510 --> 00:15:31.139
He can we can doing this using an action mode,
something introduced in Honeycomb.

00:15:31.139 --> 00:15:34.540
Set choice mode, choice mode multiple modal.

00:15:34.540 --> 00:15:38.690
When we do that, when the user long presses
on one of the photos in the list, the action

00:15:38.690 --> 00:15:44.190
ball will be swapped and replaced with an
action mode where the user can select very

00:15:44.190 --> 00:15:47.839
various items, various photos from the list
and then apply an action to them, like starring

00:15:47.839 --> 00:15:50.330
or unstarring them.

00:15:50.330 --> 00:15:56.019
One thing I want to look at today is conceptually
how that set of items is represented.

00:15:56.019 --> 00:15:58.639
The items, photos the user selected.

00:15:58.639 --> 00:16:04.300
So conceptually it's a map from an int to
a Boolean, where int is the position of the

00:16:04.300 --> 00:16:07.930
photo of the list and Boolean is an indication
of whether the user has selected the item

00:16:07.930 --> 00:16:09.490
or not.

00:16:09.490 --> 00:16:15.620
We could sort this as a hash map from an integer
to a Boolean, but instead gridview and listview

00:16:15.620 --> 00:16:20.750
on the platform return these values as a sparse
Boolean array which is an Android specific

00:16:20.750 --> 00:16:22.779
data structure.

00:16:22.779 --> 00:16:26.140
The reason for this is Autoboxing.

00:16:26.140 --> 00:16:32.149
Autoboxing is a language feature which transparently
converts from primitives, like int into instances

00:16:32.149 --> 00:16:36.180
of objects like java.lang.integer.

00:16:36.180 --> 00:16:42.410
Autoboxing is powerful because it allows us
to mix primitives with genericized data structures

00:16:42.410 --> 00:16:45.540
like hash map but it comes at a cost.

00:16:45.540 --> 00:16:50.720
Each of those box perimeters, java.lang.integer
that we are forced to allocate, adds load

00:16:50.720 --> 00:16:52.740
to the garbage collector on the system.

00:16:52.740 --> 00:16:55.720
It's an actual object allocation.

00:16:55.720 --> 00:16:59.610
Sometimes Autoboxing can sneak in if we are
not careful, if we are not aware of it.

00:16:59.610 --> 00:17:00.680
So here's two code snippets.

00:17:00.680 --> 00:17:04.730
The one at the top, we are actually looping
from zero to a hundred and adding up the total

00:17:04.730 --> 00:17:06.770
of the numbers in side.

00:17:06.770 --> 00:17:10.650
This actually allocates zero objects because
it is working with primitives the whole time.

00:17:10.650 --> 00:17:15.470
But just that one simple change from int to
integer, that same loop that produces the

00:17:15.470 --> 00:17:20.669
same resulting output value ends up allocating
over 96 objects.

00:17:20.669 --> 00:17:25.170
The reason for that is that at each point
in the loop, we end up having to unbox that

00:17:25.170 --> 00:17:31.010
total value, add to the primitive, current
I, and then rebox it back into total.

00:17:31.010 --> 00:17:37.140
So this is where using Autoboxing like this
can add overhead to your app.

00:17:37.140 --> 00:17:39.840
This is where sparse Boolean array comes into
play.

00:17:39.840 --> 00:17:45.760
Sparse Boolean array allows us to map between
primitives while still avoiding that overhead

00:17:45.760 --> 00:17:47.870
that comes with Autoboxing.

00:17:47.870 --> 00:17:53.030
So here's an example at storing a hundred
key value pairs in various data structures.

00:17:53.030 --> 00:17:56.370
The first one at the top is the one that we
were just looking at, mapping from an integer

00:17:56.370 --> 00:17:57.370
to a Boolean.

00:17:57.370 --> 00:18:03.730
If we stored that is a hash map, we end up
allocating over 200 objects under the hood.

00:18:03.730 --> 00:18:09.049
This is because hash map internally allocates
a hash map entry for each pair that we are

00:18:09.049 --> 00:18:10.049
storing.

00:18:10.049 --> 00:18:13.420
But if we were to store that same data, the
same hundred key value pairs, in a sparse

00:18:13.420 --> 00:18:16.120
Boolean array it takes many fewer objects.

00:18:16.120 --> 00:18:20.049
In fact, the majority of the data is stored
in int array and Boolean array.

00:18:20.049 --> 00:18:24.130
Which, from the perspective of the garbage
collector, are just a single object.

00:18:24.130 --> 00:18:27.600
There are many other places -- data structures.

00:18:27.600 --> 00:18:31.460
If you find yourself using one of these data
structures on the left that's mixing generics

00:18:31.460 --> 00:18:35.929
and primitives, try looking at one of the
Android utilities instead.

00:18:35.929 --> 00:18:39.830
It offers the same functionality but is much
lighter weight if you are looking to store

00:18:39.830 --> 00:18:41.460
that data.

00:18:41.460 --> 00:18:46.600
Okay, so now we have our set of stored photos
that the user is interested in.

00:18:46.600 --> 00:18:47.919
Let's go persist it.

00:18:47.919 --> 00:18:49.179
Let's use SQLite.

00:18:49.179 --> 00:18:53.680
SQLite is powerful because changes that we
make are ACID.

00:18:53.680 --> 00:18:58.190
Atomic, consistent, isolated and durable.

00:18:58.190 --> 00:19:06.140
The selected photo IDs, for each of them we
are putting that ID and the start value into

00:19:06.140 --> 00:19:07.140
content values.

00:19:07.140 --> 00:19:13.380
Then we are going to be calling SQLite database,
then insert.

00:19:13.380 --> 00:19:14.380
This code works.

00:19:14.380 --> 00:19:16.740
It successful stores the data.

00:19:16.740 --> 00:19:23.270
But dp.insert in this example guaranties,
makes those guaranties, those ACID guaranties

00:19:23.270 --> 00:19:25.309
each time we call insert.

00:19:25.309 --> 00:19:29.770
This overhead, the overhead of making those
guaranties, can add up very quickly.

00:19:29.770 --> 00:19:35.130
So instead we can manage that overhead ourselves
by wrapping those mutations, those changes

00:19:35.130 --> 00:19:36.530
inside of a transaction.

00:19:36.530 --> 00:19:41.680
So it's the same core code here, the same
four loop, but before we call into that, we

00:19:41.680 --> 00:19:43.320
are calling a begin transaction.

00:19:43.320 --> 00:19:47.860
We are opening a single atomic transaction,
performing those mutations, then setting our

00:19:47.860 --> 00:19:50.590
transaction successful and then ending it.

00:19:50.590 --> 00:19:53.700
This ends up having a big impact.

00:19:53.700 --> 00:19:58.210
So here's an example along the X axis of storing
a number of those values.

00:19:58.210 --> 00:20:03.360
So from 20, 30, up to 100 different values
increase along the X axis.

00:20:03.360 --> 00:20:07.610
Along the Y axis, we are looking at the actual
number of bytes that we ended up writing out

00:20:07.610 --> 00:20:11.970
to disk under the hood to actually persist
that data.

00:20:11.970 --> 00:20:15.990
So with the original code that we were looking
at, we can quickly see that that overhead

00:20:15.990 --> 00:20:17.740
climbs up and to the right.

00:20:17.740 --> 00:20:21.400
Because of that overhead, with each of the
DB inserts, we are making the ACID guaranties.

00:20:21.400 --> 00:20:28.049
By wrapping, we reducing a lot of that overhead
because we are performing it as a single atomic

00:20:28.049 --> 00:20:29.049
transaction.

00:20:29.049 --> 00:20:35.970
Then you might ask the question, why not wrap
all of my mutations inside one big giant transaction?

00:20:35.970 --> 00:20:38.200
This probably isn't a great idea either.

00:20:38.200 --> 00:20:43.039
The reason is, in SQLite, be default transactions
are exclusive.

00:20:43.039 --> 00:20:47.600
While someone is holding a transaction on
the database, no one else can read or write

00:20:47.600 --> 00:20:49.740
to that database.

00:20:49.740 --> 00:20:54.700
This is particularly visible if you have something
like a sync adapter in a background thread

00:20:54.700 --> 00:20:58.640
that's syncing data down from the web and
you also have UI that you are showing to the

00:20:58.640 --> 00:21:00.740
user, showing data to the user.

00:21:00.740 --> 00:21:06.220
The UI may want to query and update data to
show to the user, but it may end getting stuck

00:21:06.220 --> 00:21:10.060
situating for a transaction from the sync
adapter to finish.

00:21:10.060 --> 00:21:11.200
So we can solve this.

00:21:11.200 --> 00:21:15.610
We can solve this by inserting yield points
into our transaction.

00:21:15.610 --> 00:21:17.120
So here's example of this.

00:21:17.120 --> 00:21:20.820
We're doing our db.insert that we have been
using during the last couple examples, but

00:21:20.820 --> 00:21:24.549
now we are inserting this method yield if
contended safely.

00:21:24.549 --> 00:21:30.750
What this does, when we call into it, if the
database is contended, if someone else is

00:21:30.750 --> 00:21:35.960
interested in performing a read or write on
the database, when we call in here, it will

00:21:35.960 --> 00:21:40.290
end our current transaction that we are executing,
letting that other query, that other read

00:21:40.290 --> 00:21:42.289
or write, proceed.

00:21:42.289 --> 00:21:47.390
Then when that's finished, it will begin another
transaction before returning control back

00:21:47.390 --> 00:21:49.030
into our loop.

00:21:49.030 --> 00:21:52.360
The nice thing is, if the database isn't contended,
it will just return quickly.

00:21:52.360 --> 00:21:57.039
We will still be able to perform a lot of
our data changes inside of a single transaction

00:21:57.039 --> 00:21:59.220
if there's no one else waiting on the database.

00:21:59.220 --> 00:22:01.049
But if there is, now we have an opportunity.

00:22:01.049 --> 00:22:06.100
We're still using a transaction to apply the
changes quickly, but we are also -- our UI

00:22:06.100 --> 00:22:10.960
can now be more responsive instead of potentially
showing a loading spinner for several seconds,

00:22:10.960 --> 00:22:15.840
where those queries can come in and read out
some of the data to show to the user.

00:22:15.840 --> 00:22:17.299
Cool.

00:22:17.299 --> 00:22:18.400
So now I have my gallery app.

00:22:18.400 --> 00:22:19.400
It's fast.

00:22:19.400 --> 00:22:21.060
I'm starring photos to show to the user.

00:22:21.060 --> 00:22:26.020
I don't know, but I have been taking these
photos around the conference and they are

00:22:26.020 --> 00:22:27.020
just a little boring.

00:22:27.020 --> 00:22:30.500
I guess if I had a way to make them more interesting.

00:22:30.500 --> 00:22:34.890
Maybe if I had a SLR camera with some color
filters, I could spice things up a little

00:22:34.890 --> 00:22:35.970
bit.

00:22:35.970 --> 00:22:40.780
But carrying around equipment like this can
potentially get a little unwieldy, a little

00:22:40.780 --> 00:22:42.470
heavy to carry.

00:22:42.470 --> 00:22:45.940
So I guess we could write some code to do
it.

00:22:45.940 --> 00:22:49.710
So if we are going to be writing image processing
code on Android, we want to make sure it is

00:22:49.710 --> 00:22:52.840
fast and efficient so we are not wasting resources.

00:22:52.840 --> 00:22:56.720
There's a couple different ways of writing
fast code on Android.

00:22:56.720 --> 00:22:59.470
One option would be to use the NDK.

00:22:59.470 --> 00:23:03.059
There's another way to write C code called
Renderscript.

00:23:03.059 --> 00:23:04.809
That's what we will use today.

00:23:04.809 --> 00:23:09.721
First, before I jump in, I will talk at a
high level about three benefits that Renderscript

00:23:09.721 --> 00:23:13.830
offers us, three ways it let's us do more
with less.

00:23:13.830 --> 00:23:18.150
So first, Renderscript automatically scales
to whatever hardware we are running on top

00:23:18.150 --> 00:23:19.810
of.

00:23:19.810 --> 00:23:24.020
Renderscript does this by taking our input,
which could be a bitmap, and splitting it

00:23:24.020 --> 00:23:30.230
into small pieces likes pixels and then running
code on each of those pieces.

00:23:30.230 --> 00:23:34.289
So in this example, if I have a photo up here
on the left, it's going to split up into pixels

00:23:34.289 --> 00:23:38.830
and run an algorithm on each of those pixels.

00:23:38.830 --> 00:23:44.700
Because it splits the work into pieces, Renderscript
can parallelize this work by creating a separate

00:23:44.700 --> 00:23:48.860
worker thread for the CPU core in the device.

00:23:48.860 --> 00:23:57.549
And it targets all architectures, including
ARM, X86, MIPS, and beyond.

00:23:57.549 --> 00:24:01.370
And Renderscript does this by splitting compilation
into two stages.

00:24:01.370 --> 00:24:07.340
First, it compiles our Renderscript code,
our effect.rs file in this example, into an

00:24:07.340 --> 00:24:11.620
intermediate bit code.

00:24:11.620 --> 00:24:15.080
Then it packages that bit code inside of our
APK.

00:24:15.080 --> 00:24:20.679
Then when the app is installed on the device,
a second backend compiler takes the bit code

00:24:20.679 --> 00:24:25.140
and compiles it into optimized native code
for whatever architecture and optimizations

00:24:25.140 --> 00:24:27.540
that device may have.

00:24:27.540 --> 00:24:31.539
Finally, Renderscript generates JNI glue code
for us.

00:24:31.539 --> 00:24:34.740
If you've written code with the NDK in the
past, you've probably had to maintain JNI

00:24:34.740 --> 00:24:38.929
glue code to jump between Dalvik down into
your native code.

00:24:38.929 --> 00:24:43.490
But Renderscript generates a Java class and
methods to represent your script making it

00:24:43.490 --> 00:24:46.309
easier to jump between those two worlds.

00:24:46.309 --> 00:24:48.200
So let's dig into an example today.

00:24:48.200 --> 00:24:52.450
Let's say we want to build a simple duo tone
style color effect.

00:24:52.450 --> 00:24:56.669
Here's an example starting in the top left
with a single pixel.

00:24:56.669 --> 00:25:00.280
Maybe that's like a red, a lighter red.

00:25:00.280 --> 00:25:04.429
The first thing we will do is take the intensity
or the brightness of that color.

00:25:04.429 --> 00:25:07.150
This gives us a lighter gray.

00:25:07.150 --> 00:25:11.789
We're going to use that to map on to a gradiant,
a color gradiant between two values.

00:25:11.789 --> 00:25:13.370
So between two values.

00:25:13.370 --> 00:25:18.549
In this case, if the brightness of the -- intensity
of the color is darker, it will pick more

00:25:18.549 --> 00:25:21.270
of the greenish or bluish value.

00:25:21.270 --> 00:25:24.510
But if the color is brighter, it will pick
more toward the yellow.

00:25:24.510 --> 00:25:30.190
And in this case, this color ended up giving
this greenish value.

00:25:30.190 --> 00:25:34.320
And then we will combine together, mix together
those two values so we make the original red

00:25:34.320 --> 00:25:38.299
with our green and this gives us more of this
olive color as an output.

00:25:38.299 --> 00:25:41.640
So let's go ahead and write this code to make
it work.

00:25:41.640 --> 00:25:43.470
So here's some Renderscript code.

00:25:43.470 --> 00:25:48.480
We are defining a handful of pragmas to talk
with Renderscript to define what we are looking

00:25:48.480 --> 00:25:50.670
to do.

00:25:50.670 --> 00:25:53.820
The next thing, we are defining a constant
that we will look at in at in a moment.

00:25:53.820 --> 00:25:58.200
The thing I want to focus on first is this
root method where we are defining it.

00:25:58.200 --> 00:26:00.320
The root method takes two arguments.

00:26:00.320 --> 00:26:02.929
Input pixel and an output pixel.

00:26:02.929 --> 00:26:09.210
The data types of these arguments is, in this
case, an UR4, which is a vector type of four

00:26:09.210 --> 00:26:13.720
unsigned characters similar to a byte.

00:26:13.720 --> 00:26:17.850
You can think of this as four bytes in a single
type.

00:26:17.850 --> 00:26:21.620
And bytes, in this case the four bytes are
the four color channels.

00:26:21.620 --> 00:26:26.240
Alpha, red, green, and blue.

00:26:26.240 --> 00:26:31.070
The first thing we do is convert incoming
pixel into a float four.

00:26:31.070 --> 00:26:35.750
This is another vector type representing four
floating type numbers.

00:26:35.750 --> 00:26:41.830
The next thing we will do, we will take the
dot product of that with another static float

00:26:41.830 --> 00:26:44.590
four that we have outside of our method, a
global.

00:26:44.590 --> 00:26:47.980
We take the dot product between those two
values, it's going to give us the intensity

00:26:47.980 --> 00:26:50.309
or the brightness of that pixel.

00:26:50.309 --> 00:26:55.190
I'll divide to give us a range between zero
and one.

00:26:55.190 --> 00:26:58.330
So next I'm going to add a couple global variables.

00:26:58.330 --> 00:27:00.650
Dark color, light coloring and strength.

00:27:00.650 --> 00:27:04.800
Dark color and light color are float fours
that represent the two ends of the gradient

00:27:04.800 --> 00:27:06.110
that we were looking at.

00:27:06.110 --> 00:27:10.950
Finally, strength, which is going to indicate
how strong we want the effect to be.

00:27:10.950 --> 00:27:14.940
So if we have a strength -- we will see how
to use that in a moment.

00:27:14.940 --> 00:27:18.720
The next thing down in our root method that
we will add, we are going to use a method

00:27:18.720 --> 00:27:19.950
called mix.

00:27:19.950 --> 00:27:24.900
So Renderscript offers us a library of methods
that are useful for performing mathematics.

00:27:24.900 --> 00:27:25.900
There's mix.

00:27:25.900 --> 00:27:30.960
There's also other trig functions you might
be used to like sign and co-sign.

00:27:30.960 --> 00:27:37.140
But we will use mix today because that offers
us the ability to map between the dark color

00:27:37.140 --> 00:27:38.360
and light color.

00:27:38.360 --> 00:27:43.250
So you will mix between dark and light using
intensity as the key so that if our value

00:27:43.250 --> 00:27:46.480
is zero, it will pick dark color and so on
along that spectrum.

00:27:46.480 --> 00:27:50.140
This is going to give us a tone color.

00:27:50.140 --> 00:27:55.899
We are going to multiply that by intensity
will return some of the original brightness

00:27:55.899 --> 00:27:57.710
to the current pixel we are working on.

00:27:57.710 --> 00:28:04.909
We will use the red and final toned color
along a gradiant.

00:28:04.909 --> 00:28:06.740
We will use strength as our value.

00:28:06.740 --> 00:28:10.220
So if our strength is zero, we will use the
original input untouched.

00:28:10.220 --> 00:28:12.330
If it's one, we will use completely the tone
color.

00:28:12.330 --> 00:28:14.750
We will convert that back into UR4.

00:28:14.750 --> 00:28:16.539
That's our return value from this method.

00:28:16.539 --> 00:29:12.370
This is all in C.
Let's jump over to Java and hook this up.

00:29:12.370 --> 00:29:17.750
Under the hood, it's creating a two-dimensional
allocation of uchar4s to represent our bitmap,

00:29:17.750 --> 00:29:21.419
and then it's copying that data from the info
bitmap into the allocation.

00:29:21.419 --> 00:29:26.830
And, finally, I'm using allocation created
type to make a mirrored output allocation

00:29:26.830 --> 00:29:30.760
which we're going to use for the output of
our effect when we're done and we want to

00:29:30.760 --> 00:29:32.220
show it to the user.

00:29:32.220 --> 00:29:34.390
Okay, let's actually hook up the effect.

00:29:34.390 --> 00:29:38.320
So remember a couple of slides ago I talked
about how renderscript generates GNI glue

00:29:38.320 --> 00:29:39.740
code for us.

00:29:39.740 --> 00:29:46.320
So when we defined our effect.rs file in the
SDK, and we compiled, it generated a Java

00:29:46.320 --> 00:29:52.299
class called scriptc_effect which we can instantiate
and use over here in Dalvik.

00:29:52.299 --> 00:29:53.299
So that's what I'm doing at the top.

00:29:53.299 --> 00:29:59.130
I instantiate that script, and it also automatically
created a handful of those methods, that said

00:29:59.130 --> 00:30:03.640
dark color, light color and strength, and
I'm able to call those nice and fast.

00:30:03.640 --> 00:30:08.970
And finally, I'm calling script.foreachroot.

00:30:08.970 --> 00:30:13.060
And this is what is going to take our input
allocation, our two-dimensional bitmap, split

00:30:13.060 --> 00:30:17.030
it up into those small pieces and run our
root method on each of those small pieces,

00:30:17.030 --> 00:30:18.110
on each of those pixels.

00:30:18.110 --> 00:30:23.240
Finally, I'm going to copy the allocation
back from renderscript into Java -- or into

00:30:23.240 --> 00:30:27.170
Dalvik and then show that bitmap in an image
view.

00:30:27.170 --> 00:30:31.860
So let's take a peek at this on the device.

00:30:31.860 --> 00:30:36.010
Switch to the device.

00:30:36.010 --> 00:30:37.799
So here's the gallery map that we were using.

00:30:37.799 --> 00:30:39.610
Let's go ahead and pick a photo.

00:30:39.610 --> 00:30:42.370
Let's say one from the after hours last night.

00:30:42.370 --> 00:30:45.890
So I can go apply the duotone effect.

00:30:45.890 --> 00:30:50.330
So here's that strength, and as I drag back
and forth, it's updating that global variable

00:30:50.330 --> 00:30:53.990
and pushing it down into renderscript and
then kicking off a for each, so each time

00:30:53.990 --> 00:30:55.510
I'm changing the value.

00:30:55.510 --> 00:31:00.720
So if I pull the strength all the way to the
top, now I'm picking along the side of -- I'm

00:31:00.720 --> 00:31:05.179
picking along -- just applying the duotone
color effect, I'm not using any of the original

00:31:05.179 --> 00:31:09.150
colors and then when I pull it all the way
down using all the original pixels, so if

00:31:09.150 --> 00:31:14.910
I crank it up I can change like the dark hue
down to like red, or maybe I want something

00:31:14.910 --> 00:31:21.320
like green, light hue, maybe like red, let
me find a sweet spot right in the middle.

00:31:21.320 --> 00:31:22.320
Cool.

00:31:22.320 --> 00:31:24.429
Now my photos are looking a whole lot better.

00:31:24.429 --> 00:31:29.809
And I'm doing a great job with renderscript
of being able to apply them in a couple different

00:31:29.809 --> 00:31:30.809
ways.

00:31:30.809 --> 00:31:32.100
So let's jump back to the slides.

00:31:32.100 --> 00:31:37.710
So remember earlier I mentioned there were
some benefits that renderscript offered us.

00:31:37.710 --> 00:31:41.860
But really we've only scratched the surface.

00:31:41.860 --> 00:31:46.179
Today in our root method, we only looked at
a single pixel at a time.

00:31:46.179 --> 00:31:51.280
But in reality, we had access to the full
allocation, the full allocation in each of

00:31:51.280 --> 00:31:55.260
the -- each time we were running through the
root method.

00:31:55.260 --> 00:31:59.030
So this could enable us to write effects maybe
like a blur effect where we needed to look

00:31:59.030 --> 00:32:04.600
at neighboring pixels or like an image filter
or like an image detection or a feature detection

00:32:04.600 --> 00:32:06.830
algorithm.

00:32:06.830 --> 00:32:10.600
Another thing to note is that allocations
are much richer than just -- today we just

00:32:10.600 --> 00:32:16.200
looked at using a 2-D allocation of uchar4s
which represent bitmaps, but there are many

00:32:16.200 --> 00:32:18.280
other types of allocations we can create.

00:32:18.280 --> 00:32:23.340
For example, a one-dimensional allocation
of floats which can represent float audio

00:32:23.340 --> 00:32:25.570
data.

00:32:25.570 --> 00:32:29.400
So jumping back to those original things that
I mentioned, so renderscript allows us to

00:32:29.400 --> 00:32:33.179
scale automatically to the hardware we're
running on top of.

00:32:33.179 --> 00:32:37.149
So here's an example of how long it takes
to process a five-megapixel image, so identical

00:32:37.149 --> 00:32:38.809
image on various hardware.

00:32:38.809 --> 00:32:42.730
On the Nexus S it takes about 1200 milliseconds.

00:32:42.730 --> 00:32:47.179
On Galaxy Nexus, which is a dual core device,
to process that same image, it only takes

00:32:47.179 --> 00:32:48.720
400 milliseconds.

00:32:48.720 --> 00:32:52.460
And the reason for this is under the hood,
renderscript is creating those two worker

00:32:52.460 --> 00:32:56.020
threads, one for each of the cores on the
device and it's splitting up that work equally

00:32:56.020 --> 00:32:57.340
for us.

00:32:57.340 --> 00:32:59.090
So this is with the same code.

00:32:59.090 --> 00:33:03.710
We didn't make any code changes and it's scaling
it to the hardware we're running on top of.

00:33:03.710 --> 00:33:07.730
And also on the Nexus 7 that we announced
yesterday which is a quad core device, it

00:33:07.730 --> 00:33:09.150
runs even faster, 240 milliseconds.

00:33:09.150 --> 00:33:14.571
And again, because it's creating four of those
worker threads in the background and there

00:33:14.571 --> 00:33:16.429
were no code changes on our part.

00:33:16.429 --> 00:33:21.500
So today, on most of Android devices we're
just scaling on the CPU, but renderscript

00:33:21.500 --> 00:33:26.230
is designed in such a way that we can scale
and run computations like this on the GPU

00:33:26.230 --> 00:33:28.450
in the future.

00:33:28.450 --> 00:33:33.130
Another thing, like I mentioned earlier, is
renderscript targets all architectures including

00:33:33.130 --> 00:33:39.820
SMID style optimizations, so if you're familiar
with Neon on ARM or the SSE family of instructions

00:33:39.820 --> 00:33:45.809
on x86, renderscript automatically emits these
optimize instructions when they're available.

00:33:45.809 --> 00:33:49.420
If we had written this code with the NDK,
we would have had to write separate versions

00:33:49.420 --> 00:33:55.280
of our algorithms with separate compiler intrinsics
with each of those instruction sets, but because

00:33:55.280 --> 00:33:59.159
we wrote it with renderscript, we wrote our
algorithm once and that back-end compiler

00:33:59.159 --> 00:34:02.490
took care of emitting those optimize instructions
for us.

00:34:02.490 --> 00:34:07.690
Finally, renderscript isn't designed to replace
the NDK on the device.

00:34:07.690 --> 00:34:11.790
It doesn't offer access to bionic libc, for
example, but renderscript is really designed

00:34:11.790 --> 00:34:19.879
to be best at compute-intensive work and that's
what we used it for today in our example.

00:34:19.879 --> 00:34:23.349
So renderscript is a great example of doing
more with less.

00:34:23.349 --> 00:34:27.840
We've been able to scale automatically to
the hardware we're running on top of and to

00:34:27.840 --> 00:34:32.040
use optimized instructions with much less
coding work on our part.

00:34:32.040 --> 00:34:34.679
Okay, so we have this visual effect.

00:34:34.679 --> 00:34:38.740
Let's say we want to apply it automatically,
and we can do this on Android.

00:34:38.740 --> 00:34:43.679
There's a new powerful broadcast intent that
was introduced in API 14 that kicked off that

00:34:43.679 --> 00:34:47.010
the system sends whenever a new photo is taken.

00:34:47.010 --> 00:34:48.530
It's the new picture in 10.

00:34:48.530 --> 00:34:54.100
So here in our Android manifest, we can listen
for it using a photo receiver and that's the

00:34:54.100 --> 00:34:56.349
top definition here.

00:34:56.349 --> 00:35:00.849
And then down below is the Java code corresponding
to that where we define our broadcast receiver.

00:35:00.849 --> 00:35:04.520
And when we receive the broadcast we will
check to see -- we'll check a user setting.

00:35:04.520 --> 00:35:07.130
If the user has enabled or disabled this effect.

00:35:07.130 --> 00:35:11.510
If they want it applied, if we want the duotone
effect that we just applied automatically,

00:35:11.510 --> 00:35:14.730
then we're going to go ahead and kick off
that work in an intent service.

00:35:14.730 --> 00:35:18.870
The reason is if we did it right here in this
broadcast receiver, we would potentially expose

00:35:18.870 --> 00:35:22.690
our app to an ANR, and because we're going
to do that disk I/O and the compute-intensive

00:35:22.690 --> 00:35:26.849
work, we can to kick that off into an intent
service.

00:35:26.849 --> 00:35:27.849
So this code works.

00:35:27.849 --> 00:35:29.920
This gets the job done, it works.

00:35:29.920 --> 00:35:34.440
But the down side is if the user has never
enabled this feature in our app, this system

00:35:34.440 --> 00:35:38.099
is still going to wake us up, it will still
deliver the broadcast.

00:35:38.099 --> 00:35:42.930
And we'll run our code, we'll come in and
check that play, and if it's disabled we're

00:35:42.930 --> 00:35:44.290
just going to return.

00:35:44.290 --> 00:35:46.240
So we're actually wasting system resources.

00:35:46.240 --> 00:35:49.960
The system is waking us up and we're not doing
anything, we're just returning.

00:35:49.960 --> 00:35:52.640
But it doesn't have to stay this way.

00:35:52.640 --> 00:35:57.950
So there's a great way that we can use -- there's
a great API we can use on package manager

00:35:57.950 --> 00:36:00.710
called set component enabled setting.

00:36:00.710 --> 00:36:05.300
And we can use that today to turn on and off
our broadcast receiver dynamically.

00:36:05.300 --> 00:36:09.210
So the one change we're going to make at the
top in our manifest is we're going to make

00:36:09.210 --> 00:36:11.900
our broadcast receiver disabled by default.

00:36:11.900 --> 00:36:16.200
So we can use that with the unable equal to
false flag here.

00:36:16.200 --> 00:36:20.550
Then later, at runtime, when the user is interacting
with their app, we can dynamically -- we can

00:36:20.550 --> 00:36:25.170
programmatically enable and disable that broadcast
receiver, so when the user turns on that feature

00:36:25.170 --> 00:36:29.280
and wants the effect applied automatically,
we can turn it on and we begin receiving these

00:36:29.280 --> 00:36:31.650
broadcasts in the background from the system.

00:36:31.650 --> 00:36:35.410
And then if the user does decide to turn it
off, we can just as easily turn that broadcast

00:36:35.410 --> 00:36:38.510
receiver back off to avoid the extra overhead.

00:36:38.510 --> 00:36:42.450
So today we just used it for the new picture
broadcast, but there are many other broadcasts

00:36:42.450 --> 00:36:44.220
that are maybe more interesting.

00:36:44.220 --> 00:36:49.170
And it's especially important for common broadcasts
across the system, like connectivity change

00:36:49.170 --> 00:36:50.900
or package added.

00:36:50.900 --> 00:36:55.290
So say, for example, today in our app, if
we had written a photo upload queue, we were

00:36:55.290 --> 00:37:00.350
uploading photos to the cloud that the user
had selected, if that queue is empty and we

00:37:00.350 --> 00:37:05.050
listen for the connectivity chain event, our
app would be woken up and we wouldn't have

00:37:05.050 --> 00:37:06.480
anything to do, our queue would be empty.

00:37:06.480 --> 00:37:10.829
So that would be a great opportunity for us
to use this API, to turn off our broadcast

00:37:10.829 --> 00:37:14.900
receiver when we don't have any work to do,
when there's no work pending.

00:37:14.900 --> 00:37:16.680
Cool.

00:37:16.680 --> 00:37:20.660
So today we talked about -- just a summary
of what we talked about, we built a gallery

00:37:20.660 --> 00:37:24.000
app, we made
It fast by using caching.

00:37:24.000 --> 00:37:28.079
Then we listened to trim callbacks to reduce
our memory usage, returned it back to the

00:37:28.079 --> 00:37:31.460
system, to keep the system running nice and
smooth.

00:37:31.460 --> 00:37:35.630
Then we looked at the overhead of autoboxing
and how we can avoid it using utilities like

00:37:35.630 --> 00:37:37.480
sparse Boolean array.

00:37:37.480 --> 00:37:43.619
Then we looked at SQLite and transactions
and how they can help us save disk I/Os, we're

00:37:43.619 --> 00:37:47.170
saving to disk, persisting date.

00:37:47.170 --> 00:37:51.060
Then we looked at using renderscript as a
great way of writing visual effects that scale

00:37:51.060 --> 00:37:55.410
automatically to the hardware we're running
on top of with less coding work on our part.

00:37:55.410 --> 00:37:59.820
And then finally we looked at broadcast receivers
and using them efficiently to enable powerful

00:37:59.820 --> 00:38:04.050
effects without overhead on the system.

00:38:04.050 --> 00:38:07.180
So today we built that user experience that
we really set up to build.

00:38:07.180 --> 00:38:11.839
Our app is fast in the foreground, but our
app is also lightweight when it's in the background.

00:38:11.839 --> 00:38:16.180
And because we work together with the system,
we built a great experience, not just for

00:38:16.180 --> 00:38:18.839
our app, but for the entire system.

00:38:18.839 --> 00:38:25.079
So hopefully today I've convinced you that
on Android you truly can do more with less.

00:38:25.079 --> 00:38:28.089
Thank you.

00:38:28.089 --> 00:38:33.880
[ Applause ]
&gt;&gt;Jeff Sharkey: So I think there's two mics,

00:38:33.880 --> 00:38:37.570
if anyone has questions, there's two mics,
one in the front and the back of the room.

00:38:37.570 --> 00:38:39.760
Feel free to jump up and ask questions.

00:38:39.760 --> 00:38:42.900
If you're interested in the app that we built
today, the source code is available, there's

00:38:42.900 --> 00:38:48.190
a link up there, if you're interested, it's
open sourced under Apache 2.0.

00:38:48.190 --> 00:38:51.400
Yeah, go ahead in the front.

00:38:51.400 --> 00:38:52.400
&gt;&gt;&gt; Okay.

00:38:52.400 --> 00:38:53.400
Hi.

00:38:53.400 --> 00:38:56.170
Can you compare API shaders with renderscript
in terms of performance?

00:38:56.170 --> 00:38:58.630
&gt;&gt;Jeff Sharkey: Sure.

00:38:58.630 --> 00:39:03.660
So in this case shaders -- so there's two
parts to renderscript.

00:39:03.660 --> 00:39:07.500
There's an open GL graphics part and then
there's also a compute part.

00:39:07.500 --> 00:39:11.670
And I don't know if GL shaders, which part
they would fit under.

00:39:11.670 --> 00:39:13.100
Do you know offhand?

00:39:13.100 --> 00:39:21.310
&gt;&gt;&gt; Well, depends on which processing so what
you have done can be done with GL shaders,

00:39:21.310 --> 00:39:22.310
(indiscernible).

00:39:22.310 --> 00:39:30.560
We also do GL shaders because they're enabled
in 2.3, but if renderscript can perform better

00:39:30.560 --> 00:39:35.040
-- so is there value in like trying to support
renderscripts in Honeycomb + or just GL shaders?

00:39:35.040 --> 00:39:36.330
&gt;&gt;Jeff Sharkey: That's a great question.

00:39:36.330 --> 00:39:39.400
I actually don't know the answer off the top
of my head.

00:39:39.400 --> 00:39:43.200
If you come up and catch me afterwards on
-- there's somebody around that probably we

00:39:43.200 --> 00:39:44.830
can find an answer to your question.

00:39:44.830 --> 00:39:45.830
&gt;&gt;&gt; Thank you.

00:39:45.830 --> 00:39:46.830
&gt;&gt;Jeff Sharkey: Thanks.

00:39:46.830 --> 00:39:50.170
I'll take one from the back, if anyone's there,
or from the front, either way.

00:39:50.170 --> 00:39:51.170
What's up?

00:39:51.170 --> 00:39:52.170
&gt;&gt;&gt; Hey.

00:39:52.170 --> 00:39:57.410
Just want to ask about the autobox feature
that Java has.

00:39:57.410 --> 00:40:07.650
I read on (indiscernible) that internally
an example in the, well, Oracle JVM, they

00:40:07.650 --> 00:40:12.089
handle some sort of pool to prevent these
creational objects.

00:40:12.089 --> 00:40:16.089
I was wondering, does this feature inside
Dalvik or --

00:40:16.089 --> 00:40:19.680
&gt;&gt;Jeff Sharkey: So I can't speak to the details
of other VMs.

00:40:19.680 --> 00:40:24.300
The one thing I know about -- so, for example,
integer in Dalvik, the implementation that's

00:40:24.300 --> 00:40:28.950
there, it does actually create a handful,
a small pool, of integers for like small values,

00:40:28.950 --> 00:40:32.119
like from negative 127 to like positive 127.

00:40:32.119 --> 00:40:35.880
So it can share some of those values, but
not the entire range.

00:40:35.880 --> 00:40:36.900
&gt;&gt;&gt; Right.

00:40:36.900 --> 00:40:51.270
So my second question would be like do you
manage to use the (indiscernible) for certain

00:40:51.270 --> 00:40:57.760
things in order to prevent the creation of
serial objects but once you are going to use

00:40:57.760 --> 00:40:59.609
a key and a hash mark you don't use it.

00:40:59.609 --> 00:41:00.609
Like what's going on there?

00:41:00.609 --> 00:41:02.230
Like, I mean, do you put it along as a key
-- you're creating several objects there or

00:41:02.230 --> 00:41:03.230
also hash marks helped you in that way?

00:41:03.230 --> 00:41:05.010
&gt;&gt;Jeff Sharkey: So you're talking if I use
different types of primitives?

00:41:05.010 --> 00:41:11.870
&gt;&gt;&gt; No, when you use, an example, a long as
a key in a hash map.

00:41:11.870 --> 00:41:12.910
&gt;&gt;Jeff Sharkey: In the LRU cache?

00:41:12.910 --> 00:41:13.910
&gt;&gt;&gt; Yeah.

00:41:13.910 --> 00:41:14.910
&gt;&gt;Jeff Sharkey: Right.

00:41:14.910 --> 00:41:18.650
So LRU cache I still had that overhead of
auto boxing in that example.

00:41:18.650 --> 00:41:22.770
We could write a version of LRU cache where
we changed the key just be whatever primitive

00:41:22.770 --> 00:41:23.770
we needed.

00:41:23.770 --> 00:41:26.230
But we would have to write a different class
for that.

00:41:26.230 --> 00:41:30.850
Again, generics are powerful, but it's something
to be aware of.

00:41:30.850 --> 00:41:35.200
So if we needed our LRU cache to be really
fast, we may write a specialized version that

00:41:35.200 --> 00:41:36.890
avoids autoboxing.

00:41:36.890 --> 00:41:38.570
&gt;&gt;&gt; Thank you.

00:41:38.570 --> 00:41:41.090
&gt;&gt;Jeff Sharkey: Yeah.

00:41:41.090 --> 00:41:42.090
Next.

00:41:42.090 --> 00:41:45.950
&gt;&gt;&gt; I have a question on the LRU cache.

00:41:45.950 --> 00:41:50.430
You said it started with API 12 and backward
compatible with the support core library.

00:41:50.430 --> 00:41:55.609
What is the order of using the support library?

00:41:55.609 --> 00:42:01.050
&gt;&gt;Jeff Sharkey: It's very -- it's small, so
-- the support library's going to pull in

00:42:01.050 --> 00:42:03.730
Java -- like class files that -- into your
application.

00:42:03.730 --> 00:42:05.290
It's a jar library that you attach on.

00:42:05.290 --> 00:42:09.770
If you use something like Proguard, it can
strip out all of the other parts of support

00:42:09.770 --> 00:42:12.250
V4 library that you're not using.

00:42:12.250 --> 00:42:13.260
So that can help reduce the overhead.

00:42:13.260 --> 00:42:15.329
It will just pull in the pieces that you're
not using.

00:42:15.329 --> 00:42:16.400
&gt;&gt;&gt; So we have an APK right now.

00:42:16.400 --> 00:42:22.250
So you think it would be worthwhile, just
for the sake of the LRU cache, since we're

00:42:22.250 --> 00:42:26.690
supporting versions 11 and up, just for the
sake of the LRU cache, to -- we don't the

00:42:26.690 --> 00:42:35.270
support library right now -- to go ahead and
use the support library.

00:42:35.270 --> 00:42:40.730
&gt;&gt;Jeff Sharkey: I would say go ahead and pull
it in if you also apply the Proguard because

00:42:40.730 --> 00:42:42.350
it will help strip out the resources.

00:42:42.350 --> 00:42:46.619
The worst case -- the source code is available,
it's licensed under Apache 2.0, so you could

00:42:46.619 --> 00:42:49.810
copy the class into your app, just that single
class.

00:42:49.810 --> 00:42:53.760
&gt;&gt;&gt; And what's the backward compatibility
allowed for renderscript?

00:42:53.760 --> 00:42:55.140
&gt;&gt;Jeff Sharkey: Sure.

00:42:55.140 --> 00:42:56.741
So renderscript's -- I forgot to mention that.

00:42:56.741 --> 00:43:02.140
It was originally introduced with Honeycomb,
so that's the API, the level that it was originally

00:43:02.140 --> 00:43:03.140
introduced with.

00:43:03.140 --> 00:43:06.130
&gt;&gt;&gt; It has backward compatibility layer also
with it?

00:43:06.130 --> 00:43:11.390
&gt;&gt;Jeff Sharkey: So it's API level 11 and up,
which is increasing, there's more devices,

00:43:11.390 --> 00:43:14.300
like the devices we handed out today have
the higher API levels.

00:43:14.300 --> 00:43:15.300
&gt;&gt;&gt; Thank you.

00:43:15.300 --> 00:43:16.300
&gt;&gt;Jeff Sharkey: Yeah.

00:43:16.300 --> 00:43:17.300
Next?

00:43:17.300 --> 00:43:19.170
&gt;&gt;&gt; I have just two quick questions.

00:43:19.170 --> 00:43:25.020
Actually, the follow-up on that, you said
it's available in a later version of the API,

00:43:25.020 --> 00:43:26.430
but is it available from the support library?

00:43:26.430 --> 00:43:29.829
&gt;&gt;Jeff Sharkey: No, renderscript isn't because
-- and part of it is that back-end compiler

00:43:29.829 --> 00:43:33.569
that we saw, that needs to live on -- it needs
to be tailored for whatever device it's running

00:43:33.569 --> 00:43:34.569
on.

00:43:34.569 --> 00:43:38.070
So it really needs to be targeted to the device.

00:43:38.070 --> 00:43:43.990
&gt;&gt;&gt; The second quick question is: Is renderscript
capable of doing more complex image-processing

00:43:43.990 --> 00:43:47.960
algorithms where you need to actually look
at neighboring pixels to compute your pixels?

00:43:47.960 --> 00:43:48.960
&gt;&gt;Jeff Sharkey: Yes, yes.

00:43:48.960 --> 00:43:52.300
So when you were in that root script today,
we just looked at a single pixel at a time.

00:43:52.300 --> 00:43:56.990
We had full access to get neighboring pixels
or across the entire allocation so we had

00:43:56.990 --> 00:43:59.280
access to all that data if we needed it.

00:43:59.280 --> 00:44:00.380
&gt;&gt;&gt; Thank you.

00:44:00.380 --> 00:44:02.030
&gt;&gt;&gt;Jeff Sharkey: Yeah.

00:44:02.030 --> 00:44:03.030
Next.

00:44:03.030 --> 00:44:04.030
&gt;&gt;&gt; Hi.

00:44:04.030 --> 00:44:08.250
Is there a different in performances or optimizations
between async task and async task loaders,

00:44:08.250 --> 00:44:10.970
can we use the loaders instead of the async
loaders?

00:44:10.970 --> 00:44:16.099
&gt;&gt;Jeff Sharkey: So the one thing that loaders
offer you is that if a result has been loaded

00:44:16.099 --> 00:44:19.690
and the configuration changes, so like you
rotate the device, the loader will take care

00:44:19.690 --> 00:44:22.160
of sending that across to the new activity.

00:44:22.160 --> 00:44:25.590
If you did it with an async task, you would
have to manage and bring that along with it.

00:44:25.590 --> 00:44:29.020
So an async task loader, it brings the power
of both worlds, you get the benefits of loaders

00:44:29.020 --> 00:44:31.180
and the async task.

00:44:31.180 --> 00:44:32.180
&gt;&gt;&gt; Okay.

00:44:32.180 --> 00:44:35.500
So there isn't any performance issues with
the loaders?

00:44:35.500 --> 00:44:39.670
&gt;&gt;Jeff Sharkey: No, in fact, cursor loader,
if I remember correctly, actually extends

00:44:39.670 --> 00:44:42.829
async task loader, so it's using it under
the hood.

00:44:42.829 --> 00:44:43.829
&gt;&gt;&gt; Thank you.

00:44:43.829 --> 00:44:44.829
&gt;&gt;Jeff Sharkey: Yeah.

00:44:44.829 --> 00:44:45.829
Next?

00:44:45.829 --> 00:44:46.829
&gt;&gt;&gt; Thank you very much.

00:44:46.829 --> 00:44:48.220
I had a question about services.

00:44:48.220 --> 00:44:52.290
You talked mostly about foreground end write
apps and best practices.

00:44:52.290 --> 00:44:59.410
Is there a talk or have you given a talk about
how to use service -- make sure your service

00:44:59.410 --> 00:45:03.150
is not abusing -- being a good citizen and
can use proc write for that?

00:45:03.150 --> 00:45:06.790
&gt;&gt;Jeff Sharkey: So proc write is just going
to surface the memory usage of your process,

00:45:06.790 --> 00:45:07.859
of your application.

00:45:07.859 --> 00:45:12.540
And it doesn't offer any details into which
parts of your application are using that memory.

00:45:12.540 --> 00:45:16.200
So one good strategy, you're talking about
services, intent services is one of my favorite

00:45:16.200 --> 00:45:20.690
utilities because it helps manage and stop
-- as soon as you've finished processing the

00:45:20.690 --> 00:45:23.660
intent, you've sent it, it takes care of shutting
down the service.

00:45:23.660 --> 00:45:26.190
So that's probably one of the best practices
for services.

00:45:26.190 --> 00:45:28.150
&gt;&gt;&gt; No, but I mean when you write your own
service.

00:45:28.150 --> 00:45:35.849
Let's say I'm listening to the, you know,
GPS, I have to write a service or a C2DM service

00:45:35.849 --> 00:45:39.320
and I'm abusing the system.

00:45:39.320 --> 00:45:43.750
How do I know, you know --
&gt;&gt;Jeff Sharkey: Oh, that's a great question.

00:45:43.750 --> 00:45:47.460
So one way that you could potentially see
that is by looking -- so in the settings UI

00:45:47.460 --> 00:45:51.760
and you go into apps, there's a list of running
applications on the device, so you see if

00:45:51.760 --> 00:45:54.750
your service has been running for a long time
on the device.

00:45:54.750 --> 00:45:57.869
So that can be a signal like if you don't
expect your service to be running.

00:45:57.869 --> 00:46:01.970
In addition, like -- these are ways of tracking
down, I mean, you could also look at like

00:46:01.970 --> 00:46:05.720
the battery stats on the device to see if
it's been using a lot of resources.

00:46:05.720 --> 00:46:07.349
Are you asking more like strategies on how
to --

00:46:07.349 --> 00:46:15.270
&gt;&gt;&gt; Yeah, best practices for debugging services
or writing services, or do you give a talk

00:46:15.270 --> 00:46:16.270
on that, too, or do you plan to?

00:46:16.270 --> 00:46:18.040
&gt;&gt;Jeff Sharkey: I haven't -- I don't know
-- it sounds like a great topic.

00:46:18.040 --> 00:46:20.350
I know there's probably a lot of interest.

00:46:20.350 --> 00:46:22.470
Let's actually catch up offline.

00:46:22.470 --> 00:46:26.690
Because, again, a lot of it is going to be
dependent on the specific application.

00:46:26.690 --> 00:46:30.079
Some apps -- maybe it depends on what you're
doing in the service.

00:46:30.079 --> 00:46:31.079
Next.

00:46:31.079 --> 00:46:33.609
&gt;&gt;&gt; My question relates to the service as
well.

00:46:33.609 --> 00:46:39.994
I found a lot of apps, the user long run services,
yeah, like they use a lot of memory, right?

00:46:39.994 --> 00:46:47.510
So right now there's -- I use CODM and also
I use right now the cloud service (indiscernible).

00:46:47.510 --> 00:46:51.300
So actually consult a lot of problems, right?

00:46:51.300 --> 00:46:55.920
I think that Android should forbid long run
service, like otherwise, you've only got 10

00:46:55.920 --> 00:46:59.460
minutes, 10 minutes to do -- in the background.

00:46:59.460 --> 00:47:05.250
&gt;&gt;Jeff Sharkey: I think it's one of those
things where if you actually truly have a

00:47:05.250 --> 00:47:08.609
service running long, like we want to enable
the innovation, like somebody could have an

00:47:08.609 --> 00:47:12.760
important use for a long-running service,
so I don't think we want to artificially --

00:47:12.760 --> 00:47:16.670
&gt;&gt;&gt; Special permission, it's like basically
destroying (indiscernible).

00:47:16.670 --> 00:47:20.520
&gt;&gt;Jeff Sharkey: I mean, and that's where it
comes back, we give the UI to the user.

00:47:20.520 --> 00:47:23.770
If they notice that a service they're not
happy with running a long time, we give the

00:47:23.770 --> 00:47:24.880
user control.

00:47:24.880 --> 00:47:26.980
&gt;&gt;&gt; Now (indiscernible) memory.

00:47:26.980 --> 00:47:31.690
So right now do we need to expressly call
recycle for bitmaps?

00:47:31.690 --> 00:47:33.650
&gt;&gt;Jeff Sharkey: That's a great question.

00:47:33.650 --> 00:47:38.890
The behavior internally of bitmaps has changed
slightly over API versions between Gingerbread.

00:47:38.890 --> 00:47:43.000
The one thing to be careful of when you recycle
is that you need to make sure that it's not

00:47:43.000 --> 00:47:47.329
actively being used in your UI anywhere, because
if you recycle it and it's still being used

00:47:47.329 --> 00:47:48.329
--
&gt;&gt;&gt; Destroyed.

00:47:48.329 --> 00:47:49.660
&gt;&gt;Jeff Sharkey: Yeah.

00:47:49.660 --> 00:47:52.730
So recycle under the hood, if I remember right,
the implementation released --

00:47:52.730 --> 00:47:53.730
&gt;&gt;&gt; Native.

00:47:53.730 --> 00:47:55.589
&gt;&gt;Jeff Sharkey: Native allocation, right.

00:47:55.589 --> 00:48:00.060
And so at some point we moved that allocation
to occur on the Dalvik keep.

00:48:00.060 --> 00:48:02.849
So I don't know what recycle does in the newest
version.

00:48:02.849 --> 00:48:05.350
It's a great question.

00:48:05.350 --> 00:48:08.589
So if we meet up afterwards, we can look at
the source code and see what it does.

00:48:08.589 --> 00:48:09.589
&gt;&gt;&gt; Okay.

00:48:09.589 --> 00:48:15.359
So for the screen rotation, right, should
we always allow system to handle the rotation

00:48:15.359 --> 00:48:20.450
or just -- just handle the (indiscernible)
ourselves, that will avoid or create (indiscernible)?

00:48:20.450 --> 00:48:21.940
&gt;&gt;Jeff Sharkey: That's a great question.

00:48:21.940 --> 00:48:26.630
So you're talking about like the handles or
handles configurations or configuration changes

00:48:26.630 --> 00:48:27.630
in the manifest?

00:48:27.630 --> 00:48:28.630
&gt;&gt;&gt; Yes.

00:48:28.630 --> 00:48:32.040
&gt;&gt;Jeff Sharkey: My advice would be let the
system handle all those for you.

00:48:32.040 --> 00:48:36.369
And the reason being like even if you tell
it that you want to handle orientation changes

00:48:36.369 --> 00:48:42.130
on your own, so it may make your app fast,
but there's still the case where another configuration

00:48:42.130 --> 00:48:44.650
could change and your app is still going to
be exposed and it may be slow in that case.

00:48:44.650 --> 00:48:48.890
&gt;&gt;&gt; If you have a lot of async tasks, you
have to manually handle those async tasks,

00:48:48.890 --> 00:48:49.890
right?

00:48:49.890 --> 00:48:52.829
&gt;&gt;Jeff Sharkey: Yeah, and that's part of why
the loader pattern was introduced because

00:48:52.829 --> 00:48:54.410
that helps solve those issues.

00:48:54.410 --> 00:48:56.780
It will keep running asynchronously.

00:48:56.780 --> 00:49:01.040
And then if the result is delivered either
before or after the configuration change it

00:49:01.040 --> 00:49:02.680
won't redo that work for you.

00:49:02.680 --> 00:49:04.300
&gt;&gt;&gt; What are they called again?

00:49:04.300 --> 00:49:05.690
&gt;&gt;Jeff Sharkey: The loader pattern.

00:49:05.690 --> 00:49:06.690
&gt;&gt;&gt; Loader pattern.

00:49:06.690 --> 00:49:07.809
&gt;&gt;Jeff Sharkey: Yeah.

00:49:07.809 --> 00:49:12.450
So if you -- on activity you can do -- get
loader manager and there's several APIs on

00:49:12.450 --> 00:49:13.450
there.

00:49:13.450 --> 00:49:14.450
Yeah.

00:49:14.450 --> 00:49:20.560
&gt;&gt;&gt; Does the LRU cache handle data structures
that might change size while they're cached?

00:49:20.560 --> 00:49:25.890
&gt;&gt;Jeff Sharkey: No, so the data structure
-- that's why we overrode the method today.

00:49:25.890 --> 00:49:30.420
The only mechanism it has for counting it
against the maximum size is whatever we report.

00:49:30.420 --> 00:49:34.309
So it's up to us to return how large the size
in the LRU cache is.

00:49:34.309 --> 00:49:35.309
&gt;&gt;&gt; Thank you.

00:49:35.309 --> 00:49:36.510
&gt;&gt;Jeff Sharkey: Yeah.

00:49:36.510 --> 00:49:37.510
Next?

00:49:37.510 --> 00:49:40.320
&gt;&gt;&gt; So the renderscript example will go out
like a pixel shader but I'm curious if it

00:49:40.320 --> 00:49:46.990
can be used to do geometry and rasterized
triangles and stuff like that.

00:49:46.990 --> 00:49:48.760
&gt;&gt;Jeff Sharkey: That's a great question.

00:49:48.760 --> 00:49:54.000
So are -- you're talking about like taking
an input of like something to draw and then

00:49:54.000 --> 00:49:55.730
outputting pixels, like rendering?

00:49:55.730 --> 00:49:56.730
&gt;&gt;&gt; Right.

00:49:56.730 --> 00:49:58.170
&gt;&gt;Jeff Sharkey: That's a great question.

00:49:58.170 --> 00:49:59.800
I don't know the answer offhand.

00:49:59.800 --> 00:50:01.480
Catch us afterwards.

00:50:01.480 --> 00:50:04.350
There's somebody here that probably has the
answer.

00:50:04.350 --> 00:50:05.920
Yeah?

00:50:05.920 --> 00:50:12.120
&gt;&gt;&gt; If you have a multipass algorithm and
you're splitting up the work across several

00:50:12.120 --> 00:50:16.260
cores, how do you do synchronous --
&gt;&gt;Jeff Sharkey: Yeah, so you're talking about

00:50:16.260 --> 00:50:19.930
like you may have multiple stages of an algorithm
that you want to apply to an allocation.

00:50:19.930 --> 00:50:23.270
So the output of one allocation may be the
input to another.

00:50:23.270 --> 00:50:28.119
One way would be to merge those algorithms
-- so it's C, you could call out to methods,

00:50:28.119 --> 00:50:33.809
so you could call like method A, and then
method B and actually pass the data that way.

00:50:33.809 --> 00:50:37.420
But are you saying like those -- the stages
are fully dependent, maybe just not at the

00:50:37.420 --> 00:50:38.420
pixel level?

00:50:38.420 --> 00:50:47.369
&gt;&gt;&gt; Well, even to take your example, if you
were to -- if you have a filter so you now

00:50:47.369 --> 00:50:49.310
have one intermediate output.

00:50:49.310 --> 00:50:57.640
Now you want to apply something else, and
you want to access neighboring pixels so you

00:50:57.640 --> 00:51:05.240
cannot just split the task up completely and
just combine at the final stage, you want

00:51:05.240 --> 00:51:07.559
to have intermediate basic asynchronization.

00:51:07.559 --> 00:51:09.880
&gt;&gt;Jeff Sharkey: I think I see what you're
saying.

00:51:09.880 --> 00:51:14.940
So in that case, you could do a single for
each method where you invoke to say process

00:51:14.940 --> 00:51:16.130
this allocation.

00:51:16.130 --> 00:51:22.240
Internally it could process the first allocation,
it would take like the output of one and input

00:51:22.240 --> 00:51:23.900
to the other.

00:51:23.900 --> 00:51:27.740
As far as like helping pipeline those things,
I don't know what renderscript offers under

00:51:27.740 --> 00:51:29.349
the hood.

00:51:29.349 --> 00:51:34.190
I think the worst-case scenario, you could
run the for each, do an RS.finish to make

00:51:34.190 --> 00:51:38.160
sure that that processing is done, and then
take the output allocation and pipe it into

00:51:38.160 --> 00:51:40.520
an input of another for each.

00:51:40.520 --> 00:51:43.440
Worst case.

00:51:43.440 --> 00:51:44.900
Thanks.

00:51:44.900 --> 00:51:46.619
Go ahead.

00:51:46.619 --> 00:51:53.540
&gt;&gt;&gt; You discussed, in talking about the multicore
capability of renderscript that in the future,

00:51:53.540 --> 00:51:55.900
Google would support or the API would support
GPU.

00:51:55.900 --> 00:51:56.900
&gt;&gt;Jeff Sharkey: Yeah.

00:51:56.900 --> 00:51:57.900
&gt;&gt;&gt; Is there -- what are the options right
now if you want to break up either a computation-intensive

00:51:57.900 --> 00:52:08.950
task and be parallelized or, you know, 3-D
rendering or something and utilize the GPUs?

00:52:08.950 --> 00:52:14.830
&gt;&gt;Jeff Sharkey: So at the moment, right, so
renderscript, the implementations that are

00:52:14.830 --> 00:52:17.430
shipping today, just use the CPU side of things.

00:52:17.430 --> 00:52:22.790
In the future, we're working on enabling it
on GPUs, but there's nothing that splits up

00:52:22.790 --> 00:52:24.300
that work today in that way.

00:52:24.300 --> 00:52:28.790
I've seen it --
&gt;&gt;&gt; Is there any other mechanism?

00:52:28.790 --> 00:52:31.710
&gt;&gt;Jeff Sharkey: Not that I know of.

00:52:31.710 --> 00:52:32.930
Let's sync up afterwards.

00:52:32.930 --> 00:52:33.960
&gt;&gt;&gt; Thank you.

00:52:33.960 --> 00:52:35.500
&gt;&gt;Jeff Sharkey: Yeah.

00:52:35.500 --> 00:52:36.500
Yeah, go ahead.

00:52:36.500 --> 00:52:43.559
&gt;&gt;&gt; What is -- given that you break the code
into bit code, what's the difference between

00:52:43.559 --> 00:52:49.760
the renderscript engine and Dalvik, why couldn't
this have been Java, for example, if we were

00:52:49.760 --> 00:52:55.280
making -- we have bit code, byte code and
it's being chewed on beforehand, why isn't

00:52:55.280 --> 00:52:58.920
this part of Dalvik, why is it different?

00:52:58.920 --> 00:53:03.650
&gt;&gt;Jeff Sharkey: So renderscript was designed
-- it's designed mostly for the compute-intensive

00:53:03.650 --> 00:53:04.650
side of things.

00:53:04.650 --> 00:53:08.990
So it's very -- it's specific in nature, it's
designed to like crunch a lot of number -- like

00:53:08.990 --> 00:53:09.990
number-crunching.

00:53:09.990 --> 00:53:12.309
Dalvik is very general purpose.

00:53:12.309 --> 00:53:15.440
So it's maybe a design goal that may be different.

00:53:15.440 --> 00:53:16.440
&gt;&gt;&gt; Cool, thanks.

00:53:16.440 --> 00:53:17.720
&gt;&gt;Jeff Sharkey: Yeah.

00:53:17.720 --> 00:53:18.720
All right.

00:53:18.720 --> 00:53:19.859
Looks like we don't have any other questions.

00:53:19.859 --> 00:53:21.310
All right, thanks, everyone, for your time
today.

00:53:21.310 --> 00:53:21.349
[ Applause ]

