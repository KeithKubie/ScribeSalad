WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.000
In class we studied parsing using memoization,

00:00:04.000 --> 00:00:09.000
but I've heard in the real world many tools use techniques such as LALR(1).

00:00:09.000 --> 00:00:11.000
Why didn't we learn those?

00:00:11.000 --> 00:00:16.000
That is a good question, and actually there are a number of answers to it.

00:00:16.000 --> 00:00:18.000
I'll enumerate maybe five of them.

00:00:18.000 --> 00:00:25.000
One of the first is that this memoization or chart parsing technique that we learned

00:00:25.000 --> 00:00:30.000
is actually very similar to--almost isomorphic to-- some of the more advanced techniques

00:00:30.000 --> 00:00:32.000
used in practice.

00:00:32.000 --> 00:00:37.000
You can read up on or you may have heard of GLR--generalized LR parsing.

00:00:37.000 --> 00:00:41.000
That's commonly used in tools such as Oink--the real name for a software project,

00:00:41.000 --> 00:00:46.000
go check it out--which can be used to parse complicated languages like C++.

00:00:46.000 --> 00:00:53.000
It turns out that C++'s grammar doesn't really easily fit into these

00:00:53.000 --> 00:00:58.000
restrictive little cubbyholes that some of the faster tools prefer.

00:00:58.000 --> 00:01:01.000
It can really be a contortion that distorts meaning

00:01:01.000 --> 00:01:04.000
to try to get C++ to fit into one of these other tools.

00:01:04.000 --> 00:01:10.000
By teaching you a very general technique earl on that can handle any context-free grammar

00:01:10.000 --> 00:01:13.000
you know how parsing works even in these obscure corner cases,

00:01:13.000 --> 00:01:18.000
and I'm teaching you the approach that used in the more advanced tools.

00:01:18.000 --> 00:01:25.000
Another reason related to this is that all of these tools have the same interface.

00:01:25.000 --> 00:01:29.000
If you want to go try Oink out later on, it accepts this same sort of context-free grammar format

00:01:29.000 --> 00:01:31.000
that we're used to.

00:01:31.000 --> 00:01:40.000
So does Yacc. So does Bison. So does Ocamlyacc. So does Java CUP. So does RE Yacc, Ruby Yacc.

00:01:40.000 --> 00:01:42.000
I've taught you the way to phrase it,

00:01:42.000 --> 00:01:47.000
and then the particular tool under the hood can use whatever parsing algorithm it prefers.

00:01:47.000 --> 00:01:51.000
In the same way that we write regular expressions--ab+--

00:01:51.000 --> 00:01:54.000
and then depending on whether we're writing a PERL program or a Python program

00:01:54.000 --> 00:02:00.000
or some other program, it might convert that regular expression into a finite state machine

00:02:00.000 --> 00:02:03.000
slightly differently, but we know what the final output is going to be.

00:02:03.000 --> 00:02:05.000
We know what strings are going to be matched.

00:02:05.000 --> 00:02:10.000
Similarly here, when you write your grammar, regardless of what parser-generating algorithm

00:02:10.000 --> 00:02:14.000
is eventually used, we know what the behavior is going to be.

00:02:14.000 --> 00:02:17.000
That's what we've covered in this class. There's a related argument.

00:02:17.000 --> 00:02:22.000
You might say, well, why are we using Python in this class

00:02:22.000 --> 00:02:27.000
when the real world uses C or Java or C++

00:02:27.000 --> 00:02:31.000
or whichever language you think is commonly used in the real world--perhaps C#.

00:02:31.000 --> 00:02:33.000
That's the same argument here.

00:02:33.000 --> 00:02:37.000
Why are we using this chart parsing, when the real world uses these slightly different tools.

00:02:37.000 --> 00:02:41.000
This is a big question in education and pedagogy.

00:02:41.000 --> 00:02:44.000
I honestly believe as a practitioner and a researcher

00:02:44.000 --> 00:02:48.000
that there is a key difference between knowing how to program

00:02:48.000 --> 00:02:51.000
and knowing how to program in a particular language.

00:02:51.000 --> 00:02:53.000
The former is much more important.

00:02:53.000 --> 00:02:56.000
If you know how to program in Java, if you survived this class,

00:02:56.000 --> 00:03:03.000
you can pick up another language like Ruby, Java, Python, and do the same thing--

00:03:03.000 --> 00:03:08.000
knowing how to program, thinking about recursion, thinking about laying out data,

00:03:08.000 --> 00:03:10.000
thinking about algorithms.

00:03:10.000 --> 00:03:15.000
That's much more important than knowing the particular syntax for any language.

00:03:15.000 --> 00:03:20.000
Similarly here, I believe that really understanding context-free grammars and parsing

00:03:20.000 --> 00:03:25.000
is more important than knowing which particular parser generator tool implementation

00:03:25.000 --> 00:03:27.000
is being used under the hood.

00:03:27.000 --> 00:03:30.000
You know one particular algorithm for creating parsers,

00:03:30.000 --> 00:03:34.000
and that may as well be the one that's used in practice.

00:03:34.000 --> 00:03:39.000
Another reason that I focused on this algorithm is that it is simple.

00:03:39.000 --> 00:03:42.000
It relates grammars to parsing, which is one of the concepts in this class.

00:03:42.000 --> 00:03:45.000
We lay out context-free grammars, but now we need to be able to tell

00:03:45.000 --> 00:03:49.000
if a string or an utterance is in the language of that grammar and build up its parse tree.

00:03:49.000 --> 00:03:55.000
It's only a few lines long, and it allows us to introduce lovely concepts like memoization

00:03:55.000 --> 00:03:57.000
or list comprehensions.

00:03:57.000 --> 00:04:01.000
A number of you might wonder, if we were to write our parser in another way,

00:04:01.000 --> 00:04:06.000
maybe you've heard of techniques like recursive dissent or even using LALR(1) grammars.

00:04:06.000 --> 00:04:08.000
Would it be just as convenient?

00:04:08.000 --> 00:04:11.000
I have done them all, and the answer is no.

00:04:11.000 --> 00:04:17.000
By no means is a recursive dissent parser or an LALR(1) grammar

00:04:17.000 --> 00:04:20.000
for the language fragment that we're considering here,

00:04:20.000 --> 00:04:22.000
as easy as the parsing algorithm that I showed you.

00:04:22.000 --> 00:04:26.000
Now, this is in fact the most elegant approach.

00:04:26.000 --> 00:04:31.000
Then finally a number of students were concerned about the time taken by our parser,

00:04:31.000 --> 00:04:39.000
which is cubic or runs in time proportional to the size times the size times the size of the input program,

00:04:39.000 --> 00:04:41.000
in the worst case for an ambiguous grammar.

00:04:41.000 --> 00:04:44.000
Now, this is bit beyond the scope of this course,

00:04:44.000 --> 00:04:48.000
but it's worth pointing out that these other tools that claim to be faster,

00:04:48.000 --> 00:04:50.000
it's not actually a fair comparison.

00:04:50.000 --> 00:04:53.000
The parsing algorithm I taught you handles any grammar.

00:04:53.000 --> 00:04:56.000
The tools that claim to be faster don't.

00:04:56.000 --> 00:04:59.000
They only handle special restricted grammars.

00:04:59.000 --> 00:05:02.000
An interesting note that I didn't cover in this course,

00:05:02.000 --> 00:05:05.000
but that's nonetheless true, is that the particular parsing algorithm

00:05:05.000 --> 00:05:09.000
that we covered also runs in linear time,

00:05:09.000 --> 00:05:16.000
is very fast for the same small class of grammars--LRK grammars they're called--

00:05:16.000 --> 00:05:18.000
that are accepted by these faster tools.

00:05:18.000 --> 00:05:22.000
If you were the sort of person who would take all the work and time required

00:05:22.000 --> 00:05:27.000
to shoehorn your language's description into the particular straight jacket format

00:05:27.000 --> 00:05:29.000
favored by these other faster tools.

00:05:29.000 --> 00:05:34.000
The faster tools are actually just as slow or just as fast--your choice--

00:05:34.000 --> 00:05:38.000
as the algorithm I taught you in the limit. Their asymptotic complexity is the same.

00:05:38.000 --> 00:05:42.000
They're both linear time. You don't loose anything in terms of time either.

00:05:42.000 --> 00:05:46.000
Again, getting back to the first point, I think it's more of an historical accident

00:05:46.000 --> 00:05:53.000
that most of these tools happen to use LALR(1) or other hacks for speed in practice.

00:05:53.000 --> 00:05:57.000
In the last few years--the last 10 years, say--research has focused increasingly

00:05:57.000 --> 00:06:01.000
on GLR parsing or this sort of chart parsing that I've shown you

00:06:01.000 --> 00:06:06.000
as a way of allowing people to write the grammars that they want,

00:06:06.000 --> 00:06:10.000
get linear time, super fast performance most of the time,

00:06:10.000 --> 00:06:14.000
pay a little bit when things are ambiguous, and then get on with their lives.

00:06:14.000 --> 00:06:18.000
If foresee that languages like C++ or Java or whatnot--

00:06:18.000 --> 00:06:23.000
more complicated languages--will be analyzed using tools like that in the future.

00:06:23.000 --> 00:06:27.000
It's my honest believe as an educator that I'm teaching you the right approach,

00:06:27.000 --> 00:06:32.000
teaching something like LALR(1)--that information is really not necessary in the modern world,

00:06:32.000 --> 00:06:36.000
and it's log--those lectures feel like they go on forever.

00:06:36.000 --> 00:06:40.000
By contrast, this chart-based approach is elegant, simple,

00:06:40.000 --> 00:06:43.000
allows us to introduce fun concepts, handles all grammars,

00:06:43.000 --> 00:06:46.000
which the other approaches do not,

00:06:46.000 --> 00:06:53.000
fits in just a few slides, and is just as fast in the limit. What's not to like?

