WEBVTT
Kind: captions
Language: en

00:00:00.680 --> 00:00:04.630
&gt;&gt; From the Library of
Congress in Washington, D.C.,

00:00:23.040 --> 00:00:24.340
&gt;&gt; Good afternoon.

00:00:24.340 --> 00:00:26.900
I'm John Haskell, Director
of the Kluge Center.

00:00:26.900 --> 00:00:29.760
On behalf of the center's staff,
I want to welcome those of you

00:00:29.760 --> 00:00:32.810
who are not library employees
to The Library of Congress.

00:00:32.810 --> 00:00:34.620
This event is part of
the Center's effort

00:00:34.620 --> 00:00:36.970
to bring cutting edge
research to policymakers,

00:00:36.970 --> 00:00:38.770
and the interested public.

00:00:38.770 --> 00:00:41.260
This is in keeping with
the Kluge Center's mission

00:00:41.260 --> 00:00:43.800
since its inception in 2000.

00:00:43.800 --> 00:00:49.020
The notion, according to John
Kluge, the late John W. Kluge,

00:00:49.020 --> 00:00:51.170
is to "have a space at
the library to bring doers

00:00:51.170 --> 00:00:53.130
and thinkers together."

00:00:53.130 --> 00:00:54.900
Today, we have one
of those thinkers,

00:00:54.900 --> 00:00:59.730
a scholar in residence, Digital
Studies fellow, Dr. Todd Belt.

00:00:59.730 --> 00:01:03.110
He is here to answer questions
for me, and a little later

00:01:03.110 --> 00:01:04.630
from you, about the impact

00:01:04.630 --> 00:01:07.610
of social media on
American politics.

00:01:07.610 --> 00:01:08.910
Let me tell you a little bit

00:01:08.910 --> 00:01:10.670
about Todd before
we get started.

00:01:10.670 --> 00:01:12.240
Todd joined us at
the Kluge Center

00:01:12.240 --> 00:01:16.410
from his home institution the
University of Hawaii at Hilo.

00:01:16.410 --> 00:01:20.040
He will be going to
Colorado now, for the break,

00:01:20.040 --> 00:01:23.250
and then to get really cold
weather, before he goes back

00:01:23.250 --> 00:01:25.390
to work in the spring semester.

00:01:25.390 --> 00:01:27.280
He is a professor of
Political Science there.

00:01:27.280 --> 00:01:31.000
His research and writing focuses
on the Presidency, mass media,

00:01:31.000 --> 00:01:33.900
public opinion, and
campaigns and elections.

00:01:33.900 --> 00:01:37.630
He is co-author of four books,
over two dozen articles,

00:01:37.630 --> 00:01:40.640
and over a dozen chapters
and edited volumes.

00:01:40.640 --> 00:01:41.940
His most recent publication,

00:01:41.940 --> 00:01:43.280
which was completed
this year here

00:01:43.280 --> 00:01:45.410
at the Kluge Center is entitled,

00:01:45.410 --> 00:01:48.110
Can We At Least All
Laugh Together Now?

00:01:48.110 --> 00:01:52.350
Twitter and Online Political
Humor During the 2016 Election.

00:01:52.350 --> 00:01:54.760
It appears in the book,
The Role of Twitter

00:01:54.760 --> 00:01:58.590
in the 2016 U.S. Election, which
was just published this month.

00:01:58.590 --> 00:02:01.370
Of course, afterward Todd can
tell us more about that book,

00:02:01.370 --> 00:02:04.050
and Todd, let me start with kind

00:02:04.050 --> 00:02:07.220
of just a softball question
before I get tough on you.

00:02:07.220 --> 00:02:08.520
&gt;&gt; All right.

00:02:08.520 --> 00:02:09.820
&gt;&gt; As a student of American
politics, how did you get

00:02:09.820 --> 00:02:12.150
into the study of social media?

00:02:12.150 --> 00:02:16.430
&gt;&gt; Well, I wrote my doctoral
dissertation on the influence

00:02:16.430 --> 00:02:20.430
of text, visual and
audiovisual messages,

00:02:20.430 --> 00:02:21.730
and I was really interested

00:02:21.730 --> 00:02:25.320
in how those had a differential
impact on persuading people how

00:02:25.320 --> 00:02:28.060
to vote, and how they thought
about political issues,

00:02:28.060 --> 00:02:31.030
and during that time the
net revolution happened

00:02:31.030 --> 00:02:33.540
in the mid-1990s,
to date myself.

00:02:33.540 --> 00:02:37.330
And I was really
influenced by how

00:02:37.330 --> 00:02:39.080
so much more media
was available.

00:02:39.080 --> 00:02:41.950
Then of course, we had the
social medial revolution

00:02:41.950 --> 00:02:44.890
in the early 2000s, and
now anybody can create

00:02:44.890 --> 00:02:47.230
and disseminate all this
type of information,

00:02:47.230 --> 00:02:49.770
which has really changed
our information environment,

00:02:49.770 --> 00:02:51.180
in which we're doing governance.

00:02:51.180 --> 00:02:54.370
And I think it is a very
important question for us

00:02:54.370 --> 00:02:57.980
to address as we forge ahead
in our great experiment

00:02:57.980 --> 00:02:59.430
in popular government.

00:02:59.430 --> 00:03:02.390
&gt;&gt; So you must have been on
kind of-- you were the vanguard,

00:03:02.390 --> 00:03:04.210
part of the vanguard of
scholars, is that right?

00:03:04.210 --> 00:03:05.510
In this area?

00:03:05.510 --> 00:03:06.810
&gt;&gt; I guess [laughs].

00:03:06.810 --> 00:03:08.710
There's a bunch of people who--

00:03:08.710 --> 00:03:10.900
&gt;&gt; Couldn't have
any in the 90s...

00:03:10.900 --> 00:03:12.630
&gt;&gt; There's a bunch of people
who have been working on this,

00:03:12.630 --> 00:03:15.100
and I'm among them, but
there's a lot of people

00:03:15.100 --> 00:03:16.830
who have done quite
a lot of research

00:03:16.830 --> 00:03:18.920
that I'll talk about today.

00:03:18.920 --> 00:03:22.080
&gt;&gt; Okay. While you've been
here at the library this year,

00:03:22.080 --> 00:03:23.670
what exactly have
you been doing?

00:03:23.670 --> 00:03:26.530
You know, how did you make
use of library collections,

00:03:26.530 --> 00:03:29.090
you know, what kind of research
were you conducting here?

00:03:29.090 --> 00:03:30.950
&gt;&gt; Yeah, my research project was

00:03:30.950 --> 00:03:34.400
on how the digital revolution
has changed how we conceive

00:03:34.400 --> 00:03:38.310
of Presidential candidates, and
looking at a longitudinal study

00:03:38.310 --> 00:03:42.320
from the modern Presidential
campaign from 1960 onward

00:03:42.320 --> 00:03:46.030
to current, and comparing how
people express themselves,

00:03:46.030 --> 00:03:49.300
and how they interacted with
the information environment,

00:03:49.300 --> 00:03:51.210
both before and after
we have social media.

00:03:51.210 --> 00:03:54.180
&gt;&gt; Okay. So now we're going
to get to the substance

00:03:54.180 --> 00:03:58.450
of the matter, and I think
it's good, it's always good

00:03:58.450 --> 00:04:01.580
in an academic setting to make
sure we know what we're talking

00:04:01.580 --> 00:04:03.380
about, kind of definitionally.

00:04:03.380 --> 00:04:04.680
&gt;&gt; Sure.

00:04:04.680 --> 00:04:07.150
&gt;&gt; Let's just make sure we're
all-- what is the distinction,

00:04:07.150 --> 00:04:10.700
what is social media as
compared to regular media?

00:04:10.700 --> 00:04:12.060
Where is the line drawn?

00:04:12.060 --> 00:04:14.430
&gt;&gt; Okay. Well as I
mentioned before,

00:04:14.430 --> 00:04:16.600
we talked about the first
internet revolution,

00:04:16.600 --> 00:04:19.840
we have what we consider sort
of three different stages,

00:04:19.840 --> 00:04:22.840
what we call legacy media, this
is the good old days, right?

00:04:22.840 --> 00:04:26.410
When we had three television
networks, newspapers, and radio

00:04:26.410 --> 00:04:28.000
and such, and then we started

00:04:28.000 --> 00:04:31.150
to get our cable
television networks.

00:04:31.150 --> 00:04:34.950
Then we have web 1.0, which is
the first internet revolution.

00:04:34.950 --> 00:04:36.590
We started getting web pages.

00:04:36.590 --> 00:04:38.360
And that was distinguished
by the fact

00:04:38.360 --> 00:04:41.090
that it was what you would
call a one to many sort

00:04:41.090 --> 00:04:43.790
of relationship, whereas you
would have certain people

00:04:43.790 --> 00:04:45.810
putting up websites, had
the ability to do so,

00:04:45.810 --> 00:04:48.300
was still pretty onerous
to do it back then.

00:04:48.300 --> 00:04:51.550
And being able to talk to
people through websites,

00:04:51.550 --> 00:04:53.300
and later on some
blogs and stuff.

00:04:53.300 --> 00:04:55.520
Now, there were some people
who were putting up websites,

00:04:55.520 --> 00:04:58.340
and communicating with other
people through Usenet groups

00:04:58.340 --> 00:05:00.970
and things, but that
was a smaller fraction.

00:05:00.970 --> 00:05:04.280
What really happened with
what we called web 2.0,

00:05:04.280 --> 00:05:08.210
the term of the millennium,
was the ease

00:05:08.210 --> 00:05:10.940
with which you could create
internet content became

00:05:10.940 --> 00:05:12.410
drastically reduced.

00:05:12.410 --> 00:05:16.270
Much cheaper, much easier, you
could put together websites.

00:05:16.270 --> 00:05:18.110
Somebody would have
something that, you know,

00:05:18.110 --> 00:05:20.750
pre-programmed website, just
put your content in there,

00:05:20.750 --> 00:05:23.680
it comes out all nice, and
people can interact with it.

00:05:23.680 --> 00:05:26.820
And out of that, sprang
what we call social media.

00:05:26.820 --> 00:05:30.040
And social media now allows
us to interact with people

00:05:30.040 --> 00:05:32.750
that we know socially and people
that we don't know socially

00:05:32.750 --> 00:05:36.010
and where there's overlaps, you
can reach and get your message

00:05:36.010 --> 00:05:38.570
to people that you
wouldn't normally talk to.

00:05:38.570 --> 00:05:41.010
They may not be on your
email distribution list.

00:05:41.010 --> 00:05:43.960
They may not just happen
by your website by chance.

00:05:43.960 --> 00:05:45.460
And so because of social media,

00:05:45.460 --> 00:05:48.710
this is how we've had this big
expansiveness, of being able

00:05:48.710 --> 00:05:50.350
to contact one another.

00:05:50.350 --> 00:05:53.280
And so now that's what we call
the many to many relationship.

00:05:53.280 --> 00:05:56.270
It has really caused
a democratization

00:05:56.270 --> 00:05:57.660
of the information environment

00:05:57.660 --> 00:06:00.530
for political discourse,
for better or worse.

00:06:00.530 --> 00:06:05.300
&gt;&gt; So, this may be an
unsophisticated question,

00:06:05.300 --> 00:06:10.110
but a blog on the Washington
Post, where does that fall

00:06:10.110 --> 00:06:12.060
on the line, is that legacy?

00:06:12.060 --> 00:06:14.160
Or is that social media?

00:06:14.160 --> 00:06:16.910
&gt;&gt; Well, now because we are in
the social media environment,

00:06:16.910 --> 00:06:21.200
it becomes more legacy that is
linked through social media.

00:06:21.200 --> 00:06:24.220
It gets recirculated through
social media, through linking,

00:06:24.220 --> 00:06:27.340
through Twitter, Twitter
accounts, and Facebook.

00:06:27.340 --> 00:06:29.480
So it's all very much
linked together now.

00:06:29.480 --> 00:06:30.780
&gt;&gt; Yeah, so it's hard to--

00:06:30.780 --> 00:06:32.080
sometimes hard to make that
distinction all that fast.

00:06:32.080 --> 00:06:33.860
&gt;&gt; Yeah, and of course
people can post comments

00:06:33.860 --> 00:06:35.160
on certain things, and--

00:06:35.160 --> 00:06:36.460
&gt;&gt; Exactly, yeah, that's kind
of what I was getting at,

00:06:36.460 --> 00:06:38.640
and even on normal articles,
it can be a blog, right?

00:06:38.640 --> 00:06:39.940
&gt;&gt; Sure.

00:06:39.940 --> 00:06:41.530
&gt;&gt; In the political context,

00:06:41.530 --> 00:06:46.670
what effects do social
media have on individuals?

00:06:46.670 --> 00:06:50.010
&gt;&gt; The main effect that social
media has on individuals

00:06:50.010 --> 00:06:54.490
that has really changed how we
go about getting information

00:06:54.490 --> 00:07:00.420
and what we consider to be
valid information, there is 81%

00:07:00.420 --> 00:07:03.870
of Americans now have some
sort of social media presence.

00:07:03.870 --> 00:07:05.170
That doesn't mean
they're using it,

00:07:05.170 --> 00:07:07.590
some people will create
something and not go back to it.

00:07:07.590 --> 00:07:11.340
But of that 81%, about
two-thirds of people say

00:07:11.340 --> 00:07:14.970
that they go online for
information about politics.

00:07:14.970 --> 00:07:17.650
So social media, when
it comes to politics,

00:07:17.650 --> 00:07:20.760
is mainly about information,
and learning

00:07:20.760 --> 00:07:23.720
about what is going
on in the world.

00:07:23.720 --> 00:07:27.400
And it has had a number of
effects on people's attitudes

00:07:27.400 --> 00:07:31.830
and behaviors, in terms of just
even their policy positions,

00:07:31.830 --> 00:07:34.770
20% of Americans have said
that through their interactions

00:07:34.770 --> 00:07:38.040
in social media, they've
changed an opinion on an issue.

00:07:38.040 --> 00:07:40.970
And 17% have said that
they changed their opinion

00:07:40.970 --> 00:07:42.740
about a political candidate.

00:07:42.740 --> 00:07:45.850
So it really does have some
important ramifications there.

00:07:45.850 --> 00:07:47.860
But of course, there is
all this information.

00:07:47.860 --> 00:07:49.720
And another thing that
we're really seeing is a lot

00:07:49.720 --> 00:07:51.020
of fatigue.

00:07:51.020 --> 00:07:55.420
Over two-thirds of people
say that they are "worn out"

00:07:55.420 --> 00:07:58.520
by the amount of
politics on social media.

00:07:58.520 --> 00:08:00.510
We really have become
this information

00:08:00.510 --> 00:08:01.950
overload environment.

00:08:01.950 --> 00:08:04.780
Many people have said
that they do not--

00:08:04.780 --> 00:08:09.450
many people say the social
media has created an environment

00:08:09.450 --> 00:08:12.610
which is now more toxic
for political discussions.

00:08:12.610 --> 00:08:15.230
More than half say
that they think

00:08:15.230 --> 00:08:19.630
that social media discussions
are more angry, less tolerant,

00:08:19.630 --> 00:08:21.040
and less informative

00:08:21.040 --> 00:08:23.820
than political discussions
they have elsewhere.

00:08:23.820 --> 00:08:26.490
And so we see a lot of
people are now blocking a lot

00:08:26.490 --> 00:08:28.880
of people, and not
following other people.

00:08:28.880 --> 00:08:30.180
A quarter of Americans say

00:08:30.180 --> 00:08:32.960
that they just won't have
any interaction with people

00:08:32.960 --> 00:08:36.090
because of something they've
posted on social media

00:08:36.090 --> 00:08:37.620
with which they disagree.

00:08:37.620 --> 00:08:41.020
And yet people still
come back to it, right?

00:08:41.020 --> 00:08:42.320
We have this addiction.

00:08:42.320 --> 00:08:44.240
Social media is a
heck of a drug.

00:08:44.240 --> 00:08:47.020
And we do keep coming
back to it.

00:08:47.020 --> 00:08:51.990
And part of that is because we
have this need and this longing

00:08:51.990 --> 00:08:55.090
to make sure that what we say
is important to other people.

00:08:55.090 --> 00:08:56.900
We get a lot of validation

00:08:56.900 --> 00:08:59.670
by having what we say be
important to other people.

00:08:59.670 --> 00:09:03.290
In fact, when you get a ring
on your phone, a text message,

00:09:03.290 --> 00:09:06.140
or even just an alert
about a media post,

00:09:06.140 --> 00:09:10.260
or someone has responded to
your post, some people say

00:09:10.260 --> 00:09:12.910
that actually releases
dopamine into your brain,

00:09:12.910 --> 00:09:14.640
and you get a little
high out of it.

00:09:14.640 --> 00:09:19.080
And so it is something that
is extremely addictive.

00:09:19.080 --> 00:09:20.740
&gt;&gt; It's better for you
than some other similar--

00:09:20.740 --> 00:09:23.910
&gt;&gt; Yeah, and there are some
behavioral effects, too, right?

00:09:23.910 --> 00:09:25.700
Protests, and in elections...

00:09:25.700 --> 00:09:29.290
&gt;&gt; So I know of a
fairly rich literature

00:09:29.290 --> 00:09:32.880
that developed some years
ago as, you know, Fox News,

00:09:32.880 --> 00:09:37.950
the advent of Fox News, and then
later on MSNBC, about whether,

00:09:37.950 --> 00:09:42.470
you know, that style of media,
that sort of ideological media,

00:09:42.470 --> 00:09:46.010
was making people, you know,
more liberal or conservative.

00:09:46.010 --> 00:09:49.260
And some of that literature
is counter-intuitive.

00:09:49.260 --> 00:09:52.960
My question for you though, in
the social media context is,

00:09:52.960 --> 00:09:58.120
so is it making, you know,
moving people to the left

00:09:58.120 --> 00:10:01.160
or right or both, you know?

00:10:01.160 --> 00:10:03.380
&gt;&gt; Yeah, there's a real
big chicken and egg problem

00:10:03.380 --> 00:10:04.920
with doing that kind
of research, right?

00:10:04.920 --> 00:10:08.270
Because people tend to
seek out what they like.

00:10:08.270 --> 00:10:11.050
One of the things that we
know is that people have,

00:10:11.050 --> 00:10:14.240
one of the things I wanted
to talk about is the fact

00:10:14.240 --> 00:10:19.830
that we do have tendency for
confirmation bias, right?

00:10:19.830 --> 00:10:23.650
That is, we will believe
information that corresponds

00:10:23.650 --> 00:10:27.020
to our pre-existing
notions, and we also engage

00:10:27.020 --> 00:10:29.230
in what is called motivated
reasoning, and behaviors

00:10:29.230 --> 00:10:32.190
where we will go out
and find information

00:10:32.190 --> 00:10:34.710
that will help us avoid
what psychologists call

00:10:34.710 --> 00:10:36.010
cognitive dissonance.

00:10:36.010 --> 00:10:39.310
That is, if I hear something bad
about the candidate that I like,

00:10:39.310 --> 00:10:41.350
I will try to find
information that will discount

00:10:41.350 --> 00:10:42.790
that information, right?

00:10:42.790 --> 00:10:45.900
And so there is some
information that does happen.

00:10:45.900 --> 00:10:47.200
But I've got some graphics

00:10:47.200 --> 00:10:50.800
that will show you how this is
really played out in the 26th--

00:10:50.800 --> 00:10:52.500
&gt;&gt; So here's my hardball
question for you.

00:10:52.500 --> 00:10:56.270
Can social media actually change
the outcome of an election?

00:10:56.270 --> 00:10:58.020
&gt;&gt; Okay, I don't want to
sound like I'm hedging

00:10:58.020 --> 00:11:02.030
on this one again [laughter],
but this is a tough one, right?

00:11:02.030 --> 00:11:05.510
To try to say to somebody
did you change your opinion

00:11:05.510 --> 00:11:07.930
because of what you
saw on social media?

00:11:07.930 --> 00:11:10.070
A lot of people are not
going to answer that question

00:11:10.070 --> 00:11:11.450
in the affirmative, right?

00:11:11.450 --> 00:11:15.480
And can we go back and look at
everybody's social media history

00:11:15.480 --> 00:11:17.490
and then find out
how they voted?

00:11:17.490 --> 00:11:18.900
Well there is also
again a chicken

00:11:18.900 --> 00:11:20.200
and egg problem there too

00:11:20.200 --> 00:11:22.530
because we know people follow
people they like, right?

00:11:22.530 --> 00:11:24.280
That they already agree with.

00:11:24.280 --> 00:11:27.040
So it is difficult for us.

00:11:27.040 --> 00:11:31.130
We are having a problem
with the computer, John?

00:11:31.130 --> 00:11:33.400
Thank you.

00:11:33.400 --> 00:11:34.790
Always updating,
right [laughter].

00:11:34.790 --> 00:11:36.090
Also--

00:11:36.090 --> 00:11:40.800
&gt;&gt; 2.0, right?

00:11:40.800 --> 00:11:42.100
&gt;&gt; [Laughs] Yeah.

00:11:42.100 --> 00:11:44.900
The chicken and egg problem
also with trying to find out,

00:11:44.900 --> 00:11:50.280
you know, where people sharing
with others, but we do know

00:11:50.280 --> 00:11:52.760
that some of what we call
instrumental variables,

00:11:52.760 --> 00:11:55.440
that is things that help
candidates get elected are

00:11:55.440 --> 00:11:57.180
impacted by social media.

00:11:57.180 --> 00:11:59.590
In fact, people more
like to volunteer,

00:11:59.590 --> 00:12:02.190
to share the candidate's
information,

00:12:02.190 --> 00:12:07.310
get out the vote efforts, seem
to be enhanced by social media.

00:12:07.310 --> 00:12:10.690
Campaign contributions are
enhanced by social media,

00:12:10.690 --> 00:12:12.470
and a number of other factors

00:12:12.470 --> 00:12:14.320
that do help get
candidates elected.

00:12:14.320 --> 00:12:16.770
We are more sure that
certainly does help.

00:12:16.770 --> 00:12:18.560
&gt;&gt; Yeah, you sort
of have that path.

00:12:18.560 --> 00:12:24.560
So this is going to be back
to basics, for those of us

00:12:24.560 --> 00:12:25.860
who are more unsophisticated

00:12:25.860 --> 00:12:28.000
about the impact
of social media.

00:12:28.000 --> 00:12:30.870
The words "fake news" out
there all the time, of course,

00:12:30.870 --> 00:12:36.440
can you tell us exactly how
social media spread fake news?

00:12:36.440 --> 00:12:39.330
Is there an easy way just to
explain that, for those of us

00:12:39.330 --> 00:12:41.440
who are less sophisticated?

00:12:41.440 --> 00:12:46.290
&gt;&gt; Two very important terms,
virality, and velocity.

00:12:46.290 --> 00:12:51.710
And if we look at what we
call virality, that has to do

00:12:51.710 --> 00:12:55.780
with sort of the logarithmic
rate of growth of sharing

00:12:55.780 --> 00:12:58.260
of information that
happens in social media.

00:12:58.260 --> 00:13:03.090
This is just something I did
back in the 2008 election,

00:13:03.090 --> 00:13:08.500
looking at the viral videos
that came out with Barack Obama,

00:13:08.500 --> 00:13:13.620
and back then it was candidate
Mitt Romney, and as you can see,

00:13:13.620 --> 00:13:18.600
some of them hit over 20
million views, whereas a lot

00:13:18.600 --> 00:13:19.900
of them are languishing

00:13:19.900 --> 00:13:23.090
in the mere 2 million
views, and what happens?

00:13:23.090 --> 00:13:25.090
What happens is, you catch fire.

00:13:25.090 --> 00:13:27.160
You get to that critical mass

00:13:27.160 --> 00:13:29.760
where people start
sharing your information,

00:13:29.760 --> 00:13:33.040
not just with people they
know, but across platforms.

00:13:33.040 --> 00:13:34.770
And that is what we
call the velocity.

00:13:34.770 --> 00:13:36.840
Velocity is movement
across platforms.

00:13:36.840 --> 00:13:39.210
You can see I did a logarithmic
transformation on it.

00:13:39.210 --> 00:13:43.030
It resembles sort of a normal
curve, once you see that,

00:13:43.030 --> 00:13:45.580
once you ramp it up, and
you can really catch fire.

00:13:45.580 --> 00:13:48.810
So that's a big part of it.

00:13:48.810 --> 00:13:51.030
So how do you get to
that critical mass?

00:13:51.030 --> 00:13:52.990
And how do you get
something to catch fire?

00:13:52.990 --> 00:13:55.350
And why is it the fake news
people have figured this

00:13:55.350 --> 00:13:57.030
out more than others?

00:13:57.030 --> 00:14:00.490
Right? A lot of it has
to deal with what we call

00:14:00.490 --> 00:14:03.210
in political science,
moral contagion.

00:14:03.210 --> 00:14:05.060
And that is something
becomes contagious

00:14:05.060 --> 00:14:07.770
when it has a moral
content to it.

00:14:07.770 --> 00:14:11.300
When you use moral
language especially language

00:14:11.300 --> 00:14:14.850
that is emotionally
charged, and moral,

00:14:14.850 --> 00:14:17.860
you can usually get a bump of
about at least 20% in the amount

00:14:17.860 --> 00:14:20.530
of people who are going to
recirculate your information.

00:14:20.530 --> 00:14:22.540
So that is a really big thing.

00:14:22.540 --> 00:14:25.790
And so of course, the people we
might call bad actors out there

00:14:25.790 --> 00:14:28.550
in the world, that are
spreading fake information,

00:14:28.550 --> 00:14:29.950
they know that too.

00:14:29.950 --> 00:14:31.270
And so you can see that a lot

00:14:31.270 --> 00:14:34.320
of the stuff has been
recirculated are not necessarily

00:14:34.320 --> 00:14:36.830
things that say vote one
way or vote another way,

00:14:36.830 --> 00:14:41.290
but really designed to drive
a wedge into the public

00:14:41.290 --> 00:14:42.820
and the united states,
really trying

00:14:42.820 --> 00:14:45.430
to divide us as a country.

00:14:45.430 --> 00:14:48.510
A lot of stuff for and
against Black Lives Matter,

00:14:48.510 --> 00:14:51.940
for and against Muslims,
this came up with the issue

00:14:51.940 --> 00:14:56.100
of President Trump re-tweeting
the videos that turned

00:14:56.100 --> 00:14:58.840
out to be fake in the UK.

00:14:58.840 --> 00:15:03.330
So that is something that, you
know, once we get ramped up,

00:15:03.330 --> 00:15:06.800
we can reach that critical
mass, and then you get

00:15:06.800 --> 00:15:13.040
to this aspect of
confirmation bias.

00:15:13.040 --> 00:15:16.570
And so people want to
believe what they see.

00:15:16.570 --> 00:15:20.320
Then you get into somehow
stimulate this feed algorithm.

00:15:20.320 --> 00:15:22.440
Right? These are
some of the elements

00:15:22.440 --> 00:15:24.100
of Facebook's feed algorithm.

00:15:24.100 --> 00:15:26.310
I'm not asking you to read all
of them or anything like that,

00:15:26.310 --> 00:15:28.780
there's no test on this, right?

00:15:28.780 --> 00:15:30.910
But you can see that
it's based upon a number

00:15:30.910 --> 00:15:32.210
of different things.

00:15:32.210 --> 00:15:36.370
And they change this on a weekly
basis as to what is important.

00:15:36.370 --> 00:15:39.820
But if you notice that it's,
are you linking to other things?

00:15:39.820 --> 00:15:42.130
Are your shares liked
by other people?

00:15:42.130 --> 00:15:46.430
It's built into this equation
that if you get something

00:15:46.430 --> 00:15:50.940
that starts to move, it's going
to snowball pretty quickly

00:15:50.940 --> 00:15:53.040
and move around the internet.

00:15:53.040 --> 00:15:57.640
So it's also helped by the fact,
fake news is helped by the fact

00:15:57.640 --> 00:16:00.480
that we have a natural
psychological human tendency

00:16:00.480 --> 00:16:04.180
to like to tell people
something new, something novel,

00:16:04.180 --> 00:16:07.330
to get the scoop on the
newest, latest information,

00:16:07.330 --> 00:16:09.430
to regale our friends
with information

00:16:09.430 --> 00:16:11.220
that might be counter intuitive.

00:16:11.220 --> 00:16:12.990
That they might not expect.

00:16:12.990 --> 00:16:14.920
Oh, hey, this is actually
the way this works,

00:16:14.920 --> 00:16:16.390
not the way you thought.

00:16:16.390 --> 00:16:18.070
And so we have a
tendency to do that.

00:16:18.070 --> 00:16:19.370
And that plays into it.

00:16:19.370 --> 00:16:22.070
So what I'm telling
you is, social media,

00:16:22.070 --> 00:16:26.130
such as it is structured, plays
into our worst human tendencies

00:16:26.130 --> 00:16:29.300
to share biased information.

00:16:29.300 --> 00:16:33.050
And so you know,
this is what we get.

00:16:33.050 --> 00:16:34.500
We have people who
want to believe,

00:16:34.500 --> 00:16:35.900
and then are sharing
the information.

00:16:35.900 --> 00:16:37.460
&gt;&gt; We have an astrobiology
share as well--

00:16:37.460 --> 00:16:38.760
&gt;&gt; We do.

00:16:38.760 --> 00:16:43.130
&gt;&gt; I think that was mistakenly
in here from her talk,

00:16:43.130 --> 00:16:45.100
I think [laughter], the--

00:16:45.100 --> 00:16:48.680
so, of course, not all
social media platforms are

00:16:48.680 --> 00:16:49.980
created equal.

00:16:49.980 --> 00:16:51.800
And we are-- I assume not.

00:16:51.800 --> 00:16:54.320
Are there differences
among them in terms

00:16:54.320 --> 00:16:56.320
of their impact on politics?

00:16:56.320 --> 00:16:59.700
&gt;&gt; Yeah, and a lot of it
has to do with their nature.

00:16:59.700 --> 00:17:04.060
Twitter famous for its 140
characters, now 280 characters.

00:17:04.060 --> 00:17:07.060
You can't do much of
a policy discussion

00:17:07.060 --> 00:17:09.070
at 140 characters right?

00:17:09.070 --> 00:17:11.110
But you can link
to stuff outside,

00:17:11.110 --> 00:17:14.750
and there's also differences
in the profiles of the people

00:17:14.750 --> 00:17:17.990
who use them, and the ways that
they're used by politicians.

00:17:17.990 --> 00:17:21.700
We find that politicians have a
tendency to use Twitter to speak

00:17:21.700 --> 00:17:26.060
to the press, whereas they use
Facebook more as a tendency

00:17:26.060 --> 00:17:29.640
to organize supporters, and
that has to do with the fact

00:17:29.640 --> 00:17:32.020
that Twitter is a little
bit more professional,

00:17:32.020 --> 00:17:35.260
a little bit more geared
toward the press, mass media.

00:17:35.260 --> 00:17:37.440
And Facebook is geared
more toward, you know,

00:17:37.440 --> 00:17:39.140
what people want to do,
and what they're doing

00:17:39.140 --> 00:17:41.840
at that particular moment.

00:17:41.840 --> 00:17:44.390
And so you know, that's also
played out in just, you know,

00:17:44.390 --> 00:17:45.890
on Facebook you friend somebody,

00:17:45.890 --> 00:17:47.740
whereas on Twitter
you follow somebody.

00:17:47.740 --> 00:17:50.810
I mean, those words are
actually kind of true.

00:17:50.810 --> 00:17:55.540
&gt;&gt; So this is a little-- still
talking about the platforms

00:17:55.540 --> 00:17:58.670
and the differences among them,
but a little different angle.

00:17:58.670 --> 00:18:03.750
What, if anything, are the
different platforms doing

00:18:03.750 --> 00:18:05.830
to tamp down the
spread of fake news?

00:18:05.830 --> 00:18:08.400
&gt;&gt; Right. Well we certainly
don't want to try to force them

00:18:08.400 --> 00:18:11.300
to do anything, having the First
Amendment being such as it is,

00:18:11.300 --> 00:18:13.750
and also social media
platforms are very popular,

00:18:13.750 --> 00:18:15.410
so it probably wouldn't
be a good way to try

00:18:15.410 --> 00:18:17.890
and start legislating
against them.

00:18:17.890 --> 00:18:19.960
But some of the things
that they're going

00:18:19.960 --> 00:18:22.080
after are what we call
the dark ads, right?

00:18:22.080 --> 00:18:23.920
These are some of
the advertisements

00:18:23.920 --> 00:18:25.220
that come from other places.

00:18:25.220 --> 00:18:28.880
You may have heard
during the 2016 election,

00:18:28.880 --> 00:18:32.720
there was $100,000 in
Rubles paid to Facebook,

00:18:32.720 --> 00:18:35.360
coming from a dot-RU
address, well you know,

00:18:35.360 --> 00:18:39.350
that's got fingerprints all over
it, so that's pretty obvious.

00:18:39.350 --> 00:18:41.450
But there are some things that
are a little bit less obvious,

00:18:41.450 --> 00:18:43.250
and there are a number
of different projects

00:18:43.250 --> 00:18:44.550
that researchers are working on,

00:18:44.550 --> 00:18:46.890
trying to identify bots
a little bit better.

00:18:46.890 --> 00:18:50.010
Right? And bots are those
automated algorithms

00:18:50.010 --> 00:18:53.180
that generate and distribute
content a little bit different

00:18:53.180 --> 00:18:57.040
than what we call cyborgs, and
cyborgs are ones that are sort

00:18:57.040 --> 00:18:59.930
of human is using it and
tweaking it a little bit to try

00:18:59.930 --> 00:19:02.510
to get better distribution.

00:19:02.510 --> 00:19:06.290
So right now, Facebook is
working with Pro Publica,

00:19:06.290 --> 00:19:08.420
in order to try to identify some

00:19:08.420 --> 00:19:10.650
of those dark ads
that are coming in.

00:19:10.650 --> 00:19:15.370
Facebook tried to create what
they called a disputed indicator

00:19:15.370 --> 00:19:18.490
tag, and they were
working with a number

00:19:18.490 --> 00:19:20.180
of the fact check groups.

00:19:20.180 --> 00:19:24.350
Factcheck.org, and such,
problem was you know,

00:19:24.350 --> 00:19:26.460
it takes about three days
to fact check something.

00:19:26.460 --> 00:19:28.280
And by that time, your virality

00:19:28.280 --> 00:19:30.710
and velocity is already
over, right?

00:19:30.710 --> 00:19:33.490
When something catches fire,
and you know, it catches an air

00:19:33.490 --> 00:19:36.120
of truth to it, so that's
a little bit difficult.

00:19:36.120 --> 00:19:38.250
Now they've got something
called a trust indicator

00:19:38.250 --> 00:19:41.250
that they're putting next to
little ads with the little I

00:19:41.250 --> 00:19:42.550
that you can put
your mouse over,

00:19:42.550 --> 00:19:44.460
and then you can get a
little bit of information,

00:19:44.460 --> 00:19:47.780
but it's not quite known
whether or not that's going

00:19:47.780 --> 00:19:49.080
to be particularly effective.

00:19:49.080 --> 00:19:52.730
It just gives you information
about who put that ad there.

00:19:52.730 --> 00:19:56.270
So it's difficult,
you know, the--

00:19:56.270 --> 00:19:59.190
we always say the hackers are
one step ahead, and it's true

00:19:59.190 --> 00:20:02.560
that they are, so it's
difficult to keep that up.

00:20:02.560 --> 00:20:05.180
In terms of more
public information,

00:20:05.180 --> 00:20:09.020
there is an Oxford project
on computational propaganda,

00:20:09.020 --> 00:20:10.440
which is just an awesome word.

00:20:10.440 --> 00:20:11.740
I love that.

00:20:11.740 --> 00:20:13.040
Computational propaganda.

00:20:13.040 --> 00:20:15.380
But they're doing a lot of
research on social media,

00:20:15.380 --> 00:20:19.240
and trying to identify
things, trends that are going,

00:20:19.240 --> 00:20:21.770
and trying to identify what
the next trend is going to be,

00:20:21.770 --> 00:20:24.400
that we can impact what we
see in here in social media.

00:20:24.400 --> 00:20:26.510
&gt;&gt; Let's switch gears
a little bit.

00:20:26.510 --> 00:20:30.140
You know, we're right across
the street from the Capitol,

00:20:30.140 --> 00:20:32.850
and all the House and Senate
office buildings, and I know one

00:20:32.850 --> 00:20:34.580
of the challenges,
you know, I've been in

00:20:34.580 --> 00:20:37.650
and around Congress for,
I guess decades now.

00:20:37.650 --> 00:20:44.480
And in terms of managing social
media, as a practical matter,

00:20:44.480 --> 00:20:47.040
how should-- maybe you can
be a little prescriptive,

00:20:47.040 --> 00:20:49.190
or at least throw
out some ideas.

00:20:49.190 --> 00:20:52.550
How should Congressional
offices handle social media

00:20:52.550 --> 00:20:54.820
so that things, for
lack of a better term,

00:20:54.820 --> 00:20:56.570
don't spiral out of control?

00:20:56.570 --> 00:20:59.880
&gt;&gt; Right. Well, politicians
really love social media.

00:20:59.880 --> 00:21:02.840
Because it helps them avoid
what they call the media filter.

00:21:02.840 --> 00:21:05.660
Right? They don't have
to go through the news,

00:21:05.660 --> 00:21:07.870
and have what they're
saying edited,

00:21:07.870 --> 00:21:12.040
or in some ways constrained.

00:21:12.040 --> 00:21:13.720
So they can get all of
their message out there,

00:21:13.720 --> 00:21:16.050
without having to
go through editors.

00:21:16.050 --> 00:21:18.300
But there is also the saying,
you know, the old saying,

00:21:18.300 --> 00:21:19.630
never pick a fight with somebody

00:21:19.630 --> 00:21:21.310
who buys ink by the
barrel, right?

00:21:21.310 --> 00:21:23.920
When talking about
the old newspapers,

00:21:23.920 --> 00:21:27.570
and Congressmen's
relationships with them,

00:21:27.570 --> 00:21:28.870
should say Congress-persons now.

00:21:28.870 --> 00:21:32.520
And...everybody has
got the ink now, right?

00:21:32.520 --> 00:21:35.950
Everybody has got a chance
to, you know, there are people

00:21:35.950 --> 00:21:38.440
on Twitter who have
millions of followers,

00:21:38.440 --> 00:21:40.520
and they're not politicians,
they're not celebrities,

00:21:40.520 --> 00:21:43.670
they're people who have just
really become the go-to people,

00:21:43.670 --> 00:21:48.740
that people look toward for
comment on what's going on.

00:21:48.740 --> 00:21:50.410
So you have to be
kind of careful.

00:21:50.410 --> 00:21:53.650
But there is some political
science information that says

00:21:53.650 --> 00:21:56.960
that in terms of speaking
with constituents,

00:21:56.960 --> 00:21:58.620
you have to realize that
there are different types

00:21:58.620 --> 00:21:59.920
of constituents.

00:21:59.920 --> 00:22:02.490
There are some who are more
informed with what is going on,

00:22:02.490 --> 00:22:04.630
and there are some who are
a little bit less informed.

00:22:04.630 --> 00:22:06.480
And the ones who are a
little bit more informed,

00:22:06.480 --> 00:22:10.080
it is usually better to
use data, and numbers,

00:22:10.080 --> 00:22:13.530
and to give them the
information they need

00:22:13.530 --> 00:22:14.840
to know about a policy.

00:22:14.840 --> 00:22:18.460
But for less informed voters,
it's better to tell a story.

00:22:18.460 --> 00:22:19.760
Using a narrative.

00:22:19.760 --> 00:22:22.980
Using emotional language, you
can actually get more traction

00:22:22.980 --> 00:22:25.780
with those people by telling
a story, kind of the way

00:22:25.780 --> 00:22:29.200
that Reagan used to, when
he used to tell his stories.

00:22:29.200 --> 00:22:32.620
And you can also
create interactivity.

00:22:32.620 --> 00:22:34.210
Many of you have
probably seen on Twitter,

00:22:34.210 --> 00:22:36.350
you have your little Twitter
poll, and stuff, right?

00:22:36.350 --> 00:22:38.070
People actually like those.

00:22:38.070 --> 00:22:40.160
They like to engage with those.

00:22:40.160 --> 00:22:45.720
So people will sometimes share
those polls, and another thing

00:22:45.720 --> 00:22:49.620
that gets a lot of
re-tweeting on Twitter is

00:22:49.620 --> 00:22:53.060
when you have contests, little
contests, who can name this?

00:22:53.060 --> 00:22:55.380
And that gets people
involved as well.

00:22:55.380 --> 00:22:58.270
And asking people questions.

00:22:58.270 --> 00:23:01.250
But that can backfire on
you spectacularly as well.

00:23:01.250 --> 00:23:03.980
So when you ask the public
a question, be aware that,

00:23:03.980 --> 00:23:06.890
you know, the other side is
going to come after you on that.

00:23:06.890 --> 00:23:09.660
I mean, I think of Jeff
Lake, last week, right?

00:23:09.660 --> 00:23:15.930
He gave a contribution to Roy
Morris' competitor, Doug Jones,

00:23:15.930 --> 00:23:18.210
and you know, he just took
a picture of it right?

00:23:18.210 --> 00:23:20.330
And he said party over country.

00:23:20.330 --> 00:23:23.570
And that just totally went
viral because, because of that.

00:23:23.570 --> 00:23:26.480
It was something that people
didn't expect, it was quick,

00:23:26.480 --> 00:23:30.800
it had an emotional sort of
appeal to it, and the last thing

00:23:30.800 --> 00:23:34.520
that people need to do to
get a lot of traction is

00:23:34.520 --> 00:23:36.880
to do what we say go external.

00:23:36.880 --> 00:23:39.200
One of the things that really
hampered Hillary Clinton's

00:23:39.200 --> 00:23:43.710
online campaign in 2016 was
that of all of her Tweets,

00:23:43.710 --> 00:23:49.200
75% of them that had external
links went back to her website

00:23:49.200 --> 00:23:51.400
or other social media platforms,

00:23:51.400 --> 00:23:55.170
whereas 75% of Donald
Trump's went elsewhere,

00:23:55.170 --> 00:23:57.260
which meant that he
was expanding his web

00:23:57.260 --> 00:23:59.540
and his networks
throughout the campaign,

00:23:59.540 --> 00:24:02.470
whereas Hillary Clinton
was constraining hers.

00:24:02.470 --> 00:24:06.430
&gt;&gt; You know, kind of from a
similar practical standpoint,

00:24:06.430 --> 00:24:10.540
at least, what do citizens need
to know to protect themselves

00:24:10.540 --> 00:24:12.820
or better inform themselves
about what is going on?

00:24:12.820 --> 00:24:15.020
Protect themselves
from fake news?

00:24:15.020 --> 00:24:16.550
&gt;&gt; Well, it's difficult, right?

00:24:16.550 --> 00:24:17.880
Because it's all over the place,

00:24:17.880 --> 00:24:20.540
and the fact checkers
are three days behind,

00:24:20.540 --> 00:24:23.270
and it's difficult to know.

00:24:23.270 --> 00:24:25.160
&gt;&gt; I'm going to interrupt
you there--

00:24:25.160 --> 00:24:26.460
&gt;&gt; Please do.

00:24:26.460 --> 00:24:28.490
&gt;&gt; That raises something,
you know,

00:24:28.490 --> 00:24:30.760
you read the Washington
Post has the fact checker,

00:24:30.760 --> 00:24:33.550
and then there is the
organization, I think from out

00:24:33.550 --> 00:24:35.110
of St. Petersburg does it.

00:24:35.110 --> 00:24:36.410
&gt;&gt; Yeah, mm-hmm.

00:24:36.410 --> 00:24:38.800
&gt;&gt; And do those have impact?

00:24:38.800 --> 00:24:40.100
&gt;&gt; Mm-hm.

00:24:40.100 --> 00:24:41.600
&gt;&gt; I mean, are they
making a difference?

00:24:41.600 --> 00:24:44.430
Or is having, you know, like
the Washington Post has the

00:24:44.430 --> 00:24:45.730
four Pinocchio's--

00:24:45.730 --> 00:24:47.030
&gt;&gt; Right.

00:24:47.030 --> 00:24:48.330
&gt;&gt; Is that too confusing
or what?

00:24:48.330 --> 00:24:49.630
I mean, you get the drift
of what I am asking.

00:24:49.630 --> 00:24:50.930
&gt;&gt; Yeah, it's pretty marginal,

00:24:50.930 --> 00:24:52.830
but what political science
research has shown is

00:24:52.830 --> 00:24:56.140
that if you can re-tweet that
information in such a way

00:24:56.140 --> 00:24:59.590
that makes it funny or
humorous, you can use humor

00:24:59.590 --> 00:25:03.580
to attract attention
to someone's falsehood.

00:25:03.580 --> 00:25:04.940
That can be more informative

00:25:04.940 --> 00:25:07.000
than just the information
itself.

00:25:07.000 --> 00:25:08.300
&gt;&gt; Just the fact.

00:25:08.300 --> 00:25:10.390
&gt;&gt; It's how you package it
that really matters, yeah.

00:25:10.390 --> 00:25:14.470
And so recognizing and knowing
what is fake is a difficult

00:25:14.470 --> 00:25:18.090
thing to do, but one of
the things that we saw,

00:25:18.090 --> 00:25:21.880
let me go to it here,
this is a crazy spider web

00:25:21.880 --> 00:25:26.050
that was created by Yochai
Benkler up at Harvard and MIT,

00:25:26.050 --> 00:25:30.220
about the 2016 election,
and [sneezes] excuse me,

00:25:30.220 --> 00:25:33.340
you will see that Breitbart
became sort of the center

00:25:33.340 --> 00:25:36.530
of the shared universe
on Facebook and Twitter,

00:25:36.530 --> 00:25:38.690
during the 2016 election.

00:25:38.690 --> 00:25:41.150
That the stories that were
there, these web links

00:25:41.150 --> 00:25:44.250
and the sizes indicate how
much content was linked

00:25:44.250 --> 00:25:46.320
to from those sources.

00:25:46.320 --> 00:25:48.690
And on the right side you
can see Washington Post,

00:25:48.690 --> 00:25:51.560
New York Times, CNN, and
a lot of those stories

00:25:51.560 --> 00:25:56.120
on Breitbart were re-written
by bots with headlines

00:25:56.120 --> 00:25:57.910
that would be very
controversial,

00:25:57.910 --> 00:26:01.490
which didn't even match
the content of the story.

00:26:01.490 --> 00:26:03.310
So one of the things you
have to do is you have

00:26:03.310 --> 00:26:04.850
to read the stories, right?

00:26:04.850 --> 00:26:08.120
There's a saying on the
internet, TLDR, right?

00:26:08.120 --> 00:26:09.640
Too long, didn't read it.

00:26:09.640 --> 00:26:13.040
Right? So a lot of people
will share stories just

00:26:13.040 --> 00:26:15.890
because they read the
headline, it corresponds

00:26:15.890 --> 00:26:19.800
with something they want to
say, and having it seem to come

00:26:19.800 --> 00:26:23.380
from ostensibly a credible
source makes them want

00:26:23.380 --> 00:26:25.790
to share it to get their
political point out.

00:26:25.790 --> 00:26:28.940
So reading things
through is very important.

00:26:28.940 --> 00:26:30.240
Recognition.

00:26:30.240 --> 00:26:33.130
There are some places on the
web that you can go to and check

00:26:33.130 --> 00:26:37.300
that are a little bit more
helpful, and there is,

00:26:37.300 --> 00:26:40.030
all right, Snopes is
a good one, as well,

00:26:40.030 --> 00:26:42.800
and Hoaxy is another one.

00:26:42.800 --> 00:26:45.130
And so Twitter and
Facebook are trying

00:26:45.130 --> 00:26:48.300
to provide more credibility
to certain people,

00:26:48.300 --> 00:26:50.780
you get your little blue
checkmark, right, on Twitter,

00:26:50.780 --> 00:26:52.630
although that has become
very controversial as to

00:26:52.630 --> 00:26:53.930
who gets that, and to

00:26:53.930 --> 00:26:55.430
who is considered an
authoritative figure.

00:26:55.430 --> 00:26:57.270
So there are certain
things you can do.

00:26:57.270 --> 00:27:00.210
But the first thing is read
the whole thing, right?

00:27:00.210 --> 00:27:01.510
See if it makes sense.

00:27:01.510 --> 00:27:02.810
Does it pass the smell test?

00:27:02.810 --> 00:27:04.110
Right?

00:27:04.110 --> 00:27:07.490
&gt;&gt; So kind of get into some
wrap-up questions, you know,

00:27:07.490 --> 00:27:10.640
in an attempt to
bring together some

00:27:10.640 --> 00:27:11.940
of the things you've
talked about,

00:27:11.940 --> 00:27:15.910
and whatever else would make
sense, from your perspective

00:27:15.910 --> 00:27:18.620
and being immersed in
cutting edge scholarship,

00:27:18.620 --> 00:27:20.740
and doing the scholarship
yourself on social media

00:27:20.740 --> 00:27:25.860
and politics, can you identify
a few of the biggest problems

00:27:25.860 --> 00:27:27.450
that you would see as problems?

00:27:27.450 --> 00:27:32.190
&gt;&gt; Yeah. The biggest
problem is, as I like to say,

00:27:32.190 --> 00:27:34.680
we have met the enemy
and he is us.

00:27:34.680 --> 00:27:38.980
Right? Social media is a tool,
and it's how we have interacted

00:27:38.980 --> 00:27:42.660
with this tool, and like I
said, it's brought out some

00:27:42.660 --> 00:27:44.620
of our naturally
occurring tendencies

00:27:44.620 --> 00:27:47.250
that are really difficult.

00:27:47.250 --> 00:27:49.050
We need to catch up
to this technology

00:27:49.050 --> 00:27:50.850
in terms of the way we use it.

00:27:50.850 --> 00:27:52.950
I mean, we sprung
into this headlong

00:27:52.950 --> 00:27:55.150
without really thinking too
much about what we're doing.

00:27:55.150 --> 00:27:58.000
It's been ready, fire,
aim, instead of ready, aim,

00:27:58.000 --> 00:28:00.060
fire, with social media.

00:28:00.060 --> 00:28:03.590
And so we need to
recognize and come to grips

00:28:03.590 --> 00:28:07.990
with what we can do
to be better citizens,

00:28:07.990 --> 00:28:10.970
to pass along information
that is more verifiable,

00:28:10.970 --> 00:28:12.390
and to use it in
a responsible way.

00:28:12.390 --> 00:28:16.690
We have to recognize that we
have a natural human tendency

00:28:16.690 --> 00:28:19.350
to do public performances
of our allegiances,

00:28:19.350 --> 00:28:22.990
that is to use social media
not to say what I think,

00:28:22.990 --> 00:28:25.580
but what I want you
to think who I am.

00:28:25.580 --> 00:28:29.760
Right? And to create this
social media platform of this,

00:28:29.760 --> 00:28:32.340
this is who I am for the
world to consume, right?

00:28:32.340 --> 00:28:34.820
This is what I want
people to think of me.

00:28:34.820 --> 00:28:37.890
Which means that you end up
sometimes sharing information

00:28:37.890 --> 00:28:41.650
that may not be exactly
what has--

00:28:41.650 --> 00:28:45.540
as honest as it could
otherwise be.

00:28:45.540 --> 00:28:47.750
We also have to come to grips
with the fact that, you know,

00:28:47.750 --> 00:28:50.280
familiarity can sometimes
breed contempt,

00:28:50.280 --> 00:28:52.600
and there are things sometimes
people will say to each other

00:28:52.600 --> 00:28:54.070
when they feel familiar
with them,

00:28:54.070 --> 00:28:57.590
and behind their
keyboards, safe and secure.

00:28:57.590 --> 00:29:02.790
And say things that are,
you know, can be hurtful.

00:29:02.790 --> 00:29:04.910
There is that shield that
the internet provides

00:29:04.910 --> 00:29:09.700
that social media can
at times bring out some

00:29:09.700 --> 00:29:11.470
of our worst tendencies.

00:29:11.470 --> 00:29:14.160
And I think the last thing that
we really need to come to grips

00:29:14.160 --> 00:29:17.670
with in terms of using
social media is our tendency

00:29:17.670 --> 00:29:21.440
to read the worst motivations
in what other people post.

00:29:21.440 --> 00:29:22.740
And we do this all the time.

00:29:22.740 --> 00:29:24.690
Specifically when it's text.

00:29:24.690 --> 00:29:26.320
We have a tendency
to think, you know,

00:29:26.320 --> 00:29:29.270
the person has the worst motive,
or they're out to get us,

00:29:29.270 --> 00:29:30.880
when they may just
be asking a question.

00:29:30.880 --> 00:29:34.310
We think it's a rhetorical
question, it's a snide remark.

00:29:34.310 --> 00:29:38.130
And so dealing with our
own issues is, I think,

00:29:38.130 --> 00:29:40.660
a really big part of that.

00:29:40.660 --> 00:29:43.940
&gt;&gt; So I don't want to be droll,
but what is the plus side?

00:29:43.940 --> 00:29:47.330
&gt;&gt; Oh, absolutely there are
many plus sides to it, right?

00:29:47.330 --> 00:29:50.600
The plus sides, obviously
for members of Congress

00:29:50.600 --> 00:29:53.000
and presidents, they can
talk directly to the people

00:29:53.000 --> 00:29:55.240
and avoid the media, for
better or worse, right?

00:29:55.240 --> 00:29:56.540
Like I said.

00:29:56.540 --> 00:30:01.150
The internet has also allowed us
to, you know, not allow people

00:30:01.150 --> 00:30:03.020
to run away from
their pasts, right?

00:30:03.020 --> 00:30:04.320
We're seeing a lot of that now,

00:30:04.320 --> 00:30:06.920
that things will come
back to catch you.

00:30:06.920 --> 00:30:09.290
And it used to be
that politicians used

00:30:09.290 --> 00:30:10.940
to have what some
people would consider

00:30:10.940 --> 00:30:12.750
to be a throwaway
line about policy.

00:30:12.750 --> 00:30:15.370
If they didn't know what to
do about a political issue,

00:30:15.370 --> 00:30:18.250
they would say well, we have
to have a national dialogue

00:30:18.250 --> 00:30:19.730
about this issue, we need

00:30:19.730 --> 00:30:22.340
to have a national
dialogue about race.

00:30:22.340 --> 00:30:25.330
We need to have a national
dialogue about sexual assault.

00:30:25.330 --> 00:30:26.840
Well, guess what?

00:30:26.840 --> 00:30:28.140
We're having those.

00:30:28.140 --> 00:30:29.650
And why are we having those?

00:30:29.650 --> 00:30:32.720
Because this tool social
media has allowed these voices

00:30:32.720 --> 00:30:35.040
that have otherwise
not been heard.

00:30:35.040 --> 00:30:36.830
A place to be expressed.

00:30:36.830 --> 00:30:40.260
And so issues of sexual
harassment, and sexual assault,

00:30:40.260 --> 00:30:42.500
the Black Lives Matter
movement, other issues

00:30:42.500 --> 00:30:44.820
that otherwise may
not have been covered

00:30:44.820 --> 00:30:48.570
in main steam press are
now pushed into the foray,

00:30:48.570 --> 00:30:49.870
so much so now that, you know,

00:30:49.870 --> 00:30:52.170
you have our Times
Person of the Year, right?

00:30:52.170 --> 00:30:54.350
They're the silence breakers.

00:30:54.350 --> 00:30:58.470
&gt;&gt; So this is the your final
jeopardy, really, hard question.

00:30:58.470 --> 00:31:03.760
On balance, does social
media help or harm democracy?

00:31:03.760 --> 00:31:05.060
&gt;&gt; Can I say the
jury's out [laughter]?

00:31:05.060 --> 00:31:07.040
I don't want to hedge
or anything like that,

00:31:07.040 --> 00:31:09.050
but certainly there
are some drawbacks,

00:31:09.050 --> 00:31:10.920
and I think I've highlighted
a lot of them that have

00:31:10.920 --> 00:31:12.550
to do with, you know,
our human psychology.

00:31:12.550 --> 00:31:15.290
But there are also a
lot of benefits, right?

00:31:15.290 --> 00:31:19.470
More information ostensibly
is good for democracy, right?

00:31:19.470 --> 00:31:21.360
But of course, we
have to be able

00:31:21.360 --> 00:31:25.350
to tell what is good information
from bad information,

00:31:25.350 --> 00:31:27.790
and we have to be able to
speak with one another, right?

00:31:27.790 --> 00:31:30.840
And social media allows us
to speak with one another.

00:31:30.840 --> 00:31:33.550
We have been yelling at one
another, instead of speaking

00:31:33.550 --> 00:31:35.140
with one another, right?

00:31:35.140 --> 00:31:36.610
We've been attacking each other,

00:31:36.610 --> 00:31:39.430
instead of having
a civil dialogue.

00:31:39.430 --> 00:31:41.480
And if we can get to that point

00:31:41.480 --> 00:31:46.150
where we can have a more
civil dialogue back and forth,

00:31:46.150 --> 00:31:49.560
then I think the benefits
will outweigh the drawbacks

00:31:49.560 --> 00:31:50.860
that we've seen so far.

00:31:50.860 --> 00:31:53.500
I mean we used to have this
system, the legacy media,

00:31:53.500 --> 00:31:55.280
where you had the three
television networks

00:31:55.280 --> 00:31:58.160
that were controlling most
of what people saw and heard,

00:31:58.160 --> 00:32:00.010
and now we have so
many perspectives.

00:32:00.010 --> 00:32:01.660
Has the pendulum swung too far?

00:32:01.660 --> 00:32:03.690
Becoming polarized?

00:32:03.690 --> 00:32:05.470
It's not so bad to
become polarized

00:32:05.470 --> 00:32:06.770
and have more perspectives,

00:32:06.770 --> 00:32:10.240
as long as we can discuss
things with civility.

00:32:10.240 --> 00:32:14.130
&gt;&gt; You know, you raise--
people my age, and you know,

00:32:14.130 --> 00:32:16.780
the baby boom, which of
course the lump in the snake,

00:32:16.780 --> 00:32:18.700
so there's more than any
other group [chuckling],

00:32:18.700 --> 00:32:22.420
the baby boom grew up
you know, as though,

00:32:22.420 --> 00:32:23.720
and I guess the sense was,

00:32:23.720 --> 00:32:25.890
it was always the three
networks, and that was it.

00:32:25.890 --> 00:32:29.520
That must be the, you
know, the load star,

00:32:29.520 --> 00:32:30.880
or whatever that expression is.

00:32:30.880 --> 00:32:32.180
&gt;&gt; Right.

00:32:32.180 --> 00:32:34.090
&gt;&gt; That must be the goal, that's
the right way, but in fact,

00:32:34.090 --> 00:32:37.760
you know, we may think the
social media age is crazy,

00:32:37.760 --> 00:32:42.730
but the anomaly was that
period for a few decades

00:32:42.730 --> 00:32:45.160
when TV existed and there
were only three networks,

00:32:45.160 --> 00:32:49.140
and you capture news from
three sources, and you know,

00:32:49.140 --> 00:32:52.210
and 63 people watch the
news hour, you know,

00:32:52.210 --> 00:32:55.430
and then before that,
it was a different kind

00:32:55.430 --> 00:32:56.740
of free-for-all wasn't it?

00:32:56.740 --> 00:32:58.040
&gt;&gt; Yeah.

00:32:58.040 --> 00:32:59.450
&gt;&gt; So it isn't like, you know,
we should think about oh,

00:32:59.450 --> 00:33:02.460
it's bad, because when
Walter Cronkite told us what

00:33:02.460 --> 00:33:05.010
to think was a better time,
because there was another period

00:33:05.010 --> 00:33:07.250
that had some serious
drawbacks, too.

00:33:07.250 --> 00:33:09.360
&gt;&gt; I'm not saying it's better,
and I'm not saying its worse.

00:33:09.360 --> 00:33:11.220
I'm saying there are pluses
and negatives involved.

00:33:11.220 --> 00:33:12.520
&gt;&gt; Kind of a perspective on it.

00:33:12.520 --> 00:33:13.820
&gt;&gt; Yeah.

00:33:13.820 --> 00:33:15.120
&gt;&gt; Well thank you, Todd.

00:33:15.120 --> 00:33:16.420
&gt;&gt; Thank you so much.

00:33:16.420 --> 00:33:17.720
&gt;&gt; Very interesting.

00:33:17.720 --> 00:33:19.020
&gt;&gt; Right.

00:33:19.020 --> 00:33:20.320
[ Applause ]

00:33:20.320 --> 00:33:22.110
&gt;&gt; We are prepared to
entertain questions.

00:33:22.110 --> 00:33:24.800
Raise your hands, and we want
to, there is a gentleman--

00:33:24.800 --> 00:33:27.100
there's a few, where
are our microphones?

00:33:33.220 --> 00:33:35.200
Got you. Emily have you got it?

00:33:35.200 --> 00:33:38.150
This gentleman here
in the front, center.

00:33:43.120 --> 00:33:45.450
&gt;&gt; I realize everything is
complex, but when you look back

00:33:45.450 --> 00:33:48.890
in history, blacks and women
struggled for decades to achieve

00:33:48.890 --> 00:33:51.110
at least some degree
of legal equality.

00:33:51.110 --> 00:33:52.410
&gt;&gt; Yes.

00:33:52.410 --> 00:33:53.710
&gt;&gt; The gay movement
moved much quicker.

00:33:53.710 --> 00:33:56.430
Did social media have
any role in that?

00:33:56.430 --> 00:34:00.270
&gt;&gt; Yes. I do believe that
social media has had a role

00:34:00.270 --> 00:34:05.330
in the interactions
with-- that people have.

00:34:05.330 --> 00:34:07.680
One of the things
that we can see

00:34:07.680 --> 00:34:11.090
about the gay rights movement,
that allowed it to swing

00:34:11.090 --> 00:34:13.560
so quickly in the
period of 8 years, right?

00:34:13.560 --> 00:34:14.860
You had two-thirds against,

00:34:14.860 --> 00:34:16.850
versus two-thirds for,
gay marriage right?

00:34:16.850 --> 00:34:18.950
In the period of eight years,
and the number one thing

00:34:18.950 --> 00:34:21.330
that influenced that was
interpersonal contact.

00:34:21.330 --> 00:34:24.540
That was people knowing a
person who was gay or lesbian

00:34:24.540 --> 00:34:25.840
or transsexual, right?

00:34:25.840 --> 00:34:28.900
And because of having those
social networks, right?

00:34:28.900 --> 00:34:32.070
We've expanded who we
know, and you know,

00:34:32.070 --> 00:34:35.990
you don't have the closetedness
as you did in the past.

00:34:35.990 --> 00:34:39.020
And so yeah, so that
interpersonal connection is

00:34:39.020 --> 00:34:41.340
made, and that was one of
the things that really,

00:34:41.340 --> 00:34:45.340
really caused that policy
to move in that direction.

00:34:45.340 --> 00:34:48.910
Caused public opinion to move
that direction on that policy.

00:34:48.910 --> 00:34:51.030
&gt;&gt; Interesting question,
great question.

00:34:51.030 --> 00:34:52.330
I think somebody was
second, right here?

00:34:52.330 --> 00:34:54.900
This lady in the second row?

00:34:57.660 --> 00:34:58.500
And then we--

00:34:58.500 --> 00:35:01.670
&gt;&gt; A lot of these things do
not sound new at all to me,

00:35:01.670 --> 00:35:04.430
you can sub-- surmise
that under sensationalism.

00:35:04.430 --> 00:35:05.730
And I remember when I was

00:35:05.730 --> 00:35:08.120
in middle school we
were educated how

00:35:08.120 --> 00:35:11.900
to identify fake news in the
Yellow Press, for instance,

00:35:11.900 --> 00:35:15.050
there were some very
easily to identify markers,

00:35:15.050 --> 00:35:16.530
where we could see
that, for instance,

00:35:16.530 --> 00:35:19.270
the way they use the
language, very emotional,

00:35:19.270 --> 00:35:21.660
that there was actually
no information, of course,

00:35:21.660 --> 00:35:23.520
no mentioning of sources or so.

00:35:23.520 --> 00:35:27.660
Can you imagine that nowadays
kids get being educated in order

00:35:27.660 --> 00:35:31.060
to identify fake news in their
social media, and do you think

00:35:31.060 --> 00:35:33.860
that is a role that
schools should play?

00:35:33.860 --> 00:35:38.830
&gt;&gt; Yes [laughs], information
literacy I think is absolutely

00:35:38.830 --> 00:35:40.140
the key to this.

00:35:40.140 --> 00:35:41.690
Being able to identify
those things.

00:35:41.690 --> 00:35:43.920
Like you said, stories
that are non-sourced.

00:35:43.920 --> 00:35:45.420
I mean, there is a
real big flag there.

00:35:45.420 --> 00:35:47.530
But we also have this
social media where,

00:35:47.530 --> 00:35:51.060
it's very compressed, right?

00:35:51.060 --> 00:35:52.920
So you don't expect
a lot of sourcing.

00:35:52.920 --> 00:35:55.750
It's difficult to find where
a meme came from, right?

00:35:55.750 --> 00:35:59.200
An image with just
information on it.

00:35:59.200 --> 00:36:05.170
So absolutely, I think media
literacy is absolutely the main

00:36:05.170 --> 00:36:06.470
thing that we need to do.

00:36:06.470 --> 00:36:08.550
But I think that part
of that media literacy

00:36:08.550 --> 00:36:11.700
as I've stressed here, has
to deal with how we interact

00:36:11.700 --> 00:36:13.310
with information, and knowing

00:36:13.310 --> 00:36:17.310
and understand how we
can bring our emotions

00:36:17.310 --> 00:36:19.020
and our pre-conceived biases

00:36:19.020 --> 00:36:22.200
and how we can be easily
fooled by things as well.

00:36:22.200 --> 00:36:23.500
So I absolutely agree.

00:36:23.500 --> 00:36:25.800
Excellent question.

00:36:30.040 --> 00:36:31.340
&gt;&gt; Thank you, Todd.

00:36:31.340 --> 00:36:32.640
Very interesting.

00:36:32.640 --> 00:36:35.700
In case anybody didn't
know, today the FCC decided

00:36:35.700 --> 00:36:37.730
to demolish net neutrality.

00:36:37.730 --> 00:36:46.230
What do you see as the impact on
social media by this decision?

00:36:46.230 --> 00:36:47.530
&gt;&gt; Great question.

00:36:47.530 --> 00:36:49.490
&gt;&gt; This is a really
good question.

00:36:49.490 --> 00:36:53.700
And the main concern with net
neutrality is the throttling

00:36:53.700 --> 00:36:55.000
issue, right?

00:36:55.000 --> 00:36:58.110
And now allowing
so much bandwidth

00:36:58.110 --> 00:37:00.730
to certain types of sites.

00:37:00.730 --> 00:37:03.890
We saw what happened with
Netflix when they got throttled,

00:37:03.890 --> 00:37:06.340
and they ponied up
extra money to be able

00:37:06.340 --> 00:37:11.570
to keep their bandwidth open
for their movie streaming.

00:37:11.570 --> 00:37:13.650
It is-- obviously Twitter

00:37:13.650 --> 00:37:16.040
and Facebook are
extraordinarily successful,

00:37:16.040 --> 00:37:18.510
and so they would be
able to ride it out,

00:37:18.510 --> 00:37:22.920
but what social media
does for democracy,

00:37:22.920 --> 00:37:25.860
it's not the 140
characters that matter,

00:37:25.860 --> 00:37:27.320
it's what you're
linking to there.

00:37:27.320 --> 00:37:32.230
Right? And so there may
be some of those links

00:37:32.230 --> 00:37:35.430
to outside sources that, you
know, you may not be able

00:37:35.430 --> 00:37:38.010
to download quite as easily,
and may be silenced from them.

00:37:38.010 --> 00:37:42.790
&gt;&gt; If I could just
comment again on that.

00:37:42.790 --> 00:37:44.090
&gt;&gt; Yeah.

00:37:44.090 --> 00:37:46.290
&gt;&gt; But in other countries,
this has happened before,

00:37:46.290 --> 00:37:51.100
and free services, such as
Facebook, Twitter, etc.,

00:37:51.100 --> 00:37:55.730
have gone to paid subscriptions,
and have gone into packaging.

00:37:55.730 --> 00:37:58.770
Internet service providers
have put together packages,

00:37:58.770 --> 00:38:00.620
like your-- like Comcast would,

00:38:00.620 --> 00:38:04.090
where you don't have the
premium channels and things.

00:38:04.090 --> 00:38:08.420
If people need to start paying
to access Twitter, Facebook

00:38:08.420 --> 00:38:11.120
and Instagram, and YouTube,
and things like that,

00:38:11.120 --> 00:38:12.740
how do you think that is going

00:38:12.740 --> 00:38:15.300
to affect social
media in general?

00:38:15.300 --> 00:38:17.170
&gt;&gt; Well obviously that's going

00:38:17.170 --> 00:38:19.380
to widen what we call the
digital divide, right?

00:38:19.380 --> 00:38:22.000
Between the haves,
and the have nots.

00:38:22.000 --> 00:38:23.550
Most people, especially
most people

00:38:23.550 --> 00:38:24.850
in foreign countries right now,

00:38:24.850 --> 00:38:26.690
use their primary
device their phone.

00:38:26.690 --> 00:38:29.860
Right? And so they get a cheap
phone, and they use things

00:38:29.860 --> 00:38:34.280
for free, so I think that will
divide us even more in terms

00:38:34.280 --> 00:38:37.420
of the haves and have nots, I
think so, and obviously, yeah,

00:38:37.420 --> 00:38:40.350
just Ethiopia two days ago
shut down their social media

00:38:40.350 --> 00:38:42.280
because they were
having some unrest.

00:38:42.280 --> 00:38:44.600
You know, hopefully we don't
do that in this country,

00:38:44.600 --> 00:38:48.330
but there are-- net
neutrality is a nefarious way

00:38:48.330 --> 00:38:52.040
to privilege some forms of
speech over others, yeah.

00:38:52.040 --> 00:38:54.690
&gt;&gt; Gentleman in the
bowtie was next.

00:38:54.690 --> 00:38:59.760
&gt;&gt; So I wanted to ask
what you think the role

00:38:59.760 --> 00:39:03.270
of platforms should be, and
monitoring, sorry, censoring,

00:39:03.270 --> 00:39:05.760
moderating the national
dialogue,

00:39:05.760 --> 00:39:09.070
especially when you look at some
political candidates we've had,

00:39:09.070 --> 00:39:12.610
and elected officials, where
their platforms border on hate.

00:39:12.610 --> 00:39:16.050
And so where, what point
should we say like, nope,

00:39:16.050 --> 00:39:18.360
that's not worthy of
your 140 characters,

00:39:18.360 --> 00:39:19.660
you're not letting that go up.

00:39:19.660 --> 00:39:20.960
&gt;&gt; Mm-hm.

00:39:20.960 --> 00:39:23.810
&gt;&gt; Or what, or how should
Facebook or Twitter say like,

00:39:23.810 --> 00:39:26.400
this really isn't a
good idea, kind of.

00:39:26.400 --> 00:39:28.800
What roles do platforms play?

00:39:28.800 --> 00:39:30.230
&gt;&gt; Okay. Thank you
for that question.

00:39:30.230 --> 00:39:32.600
Obviously these tech
companies say,

00:39:32.600 --> 00:39:35.230
"We're tech companies not
media companies," right?

00:39:35.230 --> 00:39:37.890
And they like to sort
of hide behind that

00:39:37.890 --> 00:39:39.400
and give themselves a shield.

00:39:39.400 --> 00:39:43.080
But they are our
gateway to media, right?

00:39:43.080 --> 00:39:47.050
And you know, you're talking
about hate speech and such,

00:39:47.050 --> 00:39:49.040
well, that comes under
the terms of service

00:39:49.040 --> 00:39:51.130
of each particular
platform, right?

00:39:51.130 --> 00:39:52.950
And that is up to each platform.

00:39:52.950 --> 00:39:54.470
There are terms of service

00:39:54.470 --> 00:39:56.260
and Twitter has actually
been a little bit more strict

00:39:56.260 --> 00:39:59.470
than Facebook in terms
of blocking users

00:39:59.470 --> 00:40:01.950
for violating their
terms of service.

00:40:01.950 --> 00:40:03.250
It is up to them, right?

00:40:03.250 --> 00:40:04.550
They are businesses,

00:40:04.550 --> 00:40:06.360
and we shouldn't be
telling businesses how

00:40:06.360 --> 00:40:08.210
to conduct this type of thing.

00:40:08.210 --> 00:40:09.670
But on the other side, right?

00:40:09.670 --> 00:40:12.670
They are performing a very,
very important service.

00:40:12.670 --> 00:40:16.510
But we don't have like the
FCC licensing that we can hold

00:40:16.510 --> 00:40:17.810
over their heads, right?

00:40:17.810 --> 00:40:22.230
In order to be able to
do what we want, right?

00:40:22.230 --> 00:40:27.980
The internet, it's a
complicated system of, you know,

00:40:27.980 --> 00:40:30.800
of interaction, where it's
not all government owned.

00:40:30.800 --> 00:40:33.750
It's not all owned by the
people like the airwaves are,

00:40:33.750 --> 00:40:35.840
so we can't use the
FCC to do that.

00:40:35.840 --> 00:40:37.290
So it's a bit more complicated.

00:40:37.290 --> 00:40:39.440
And so it's certainly up
to those, and you know,

00:40:39.440 --> 00:40:43.400
you can opt out of a
group that you don't--

00:40:43.400 --> 00:40:46.020
of a business for which you
don't like their policies,

00:40:46.020 --> 00:40:47.620
but then again, you know,

00:40:47.620 --> 00:40:48.930
there's only two
big ones, right?

00:40:48.930 --> 00:40:51.210
I mean, well, we've got
four up here, right?

00:40:51.210 --> 00:40:53.250
But Instagram for politics?

00:40:53.250 --> 00:40:55.810
Eh, kind of not so much, right?

00:40:55.810 --> 00:40:58.200
And YouTube?

00:40:58.200 --> 00:40:59.500
Eh, kind of not so much.

00:40:59.500 --> 00:41:01.530
But it's really Twitter
and Facebook, right?

00:41:01.530 --> 00:41:04.280
That's the big fish in the sea.

00:41:04.280 --> 00:41:08.020
&gt;&gt; Down in the back there?

00:41:08.020 --> 00:41:09.600
&gt;&gt; Hi, I'd like to
offer a little bit

00:41:09.600 --> 00:41:11.580
of historical perspective.

00:41:11.580 --> 00:41:14.990
I'm doing quality review
on historic newspapers,

00:41:14.990 --> 00:41:18.460
and in 1893, one of the
newspapers was like,

00:41:18.460 --> 00:41:20.990
what would the world
be like in 1993?

00:41:20.990 --> 00:41:24.760
And a journalist was
speculating that aside

00:41:24.760 --> 00:41:26.540
from the vast acceleration

00:41:26.540 --> 00:41:29.370
of technology not
much philosophically,

00:41:29.370 --> 00:41:32.360
and he quoted something
from the 18th Century

00:41:32.360 --> 00:41:34.660
which was saying how
horrible newspapers were

00:41:34.660 --> 00:41:38.280
with character assassination,
salaciousness, mid-18th Century,

00:41:38.280 --> 00:41:41.900
which claimed that around
the turn of the 18th Century,

00:41:41.900 --> 00:41:44.640
things were sober and
good, but that was

00:41:44.640 --> 00:41:48.080
like 60 years before the 18th
Century gentleman's time,

00:41:48.080 --> 00:41:51.650
and the person from the 1893
was looking back and saying

00:41:51.650 --> 00:41:53.780
if this person said
it was so bad then,

00:41:53.780 --> 00:41:58.570
basically things will
not have changed much

00:41:58.570 --> 00:42:02.490
in terms of human nature.

00:42:02.490 --> 00:42:06.930
The main changing factor is the
vast acceleration of technology.

00:42:06.930 --> 00:42:13.130
Could you trace cyclical
changes in philosophy of news,

00:42:13.130 --> 00:42:18.610
or is the primary differential
essentially technological?

00:42:18.610 --> 00:42:20.220
&gt;&gt; What do you mean
by philosophy of news?

00:42:20.220 --> 00:42:21.520
I'm sorry--

00:42:21.520 --> 00:42:24.810
&gt;&gt; I mean trying to report
things relatively objectively

00:42:24.810 --> 00:42:26.340
rather than going for sensation.

00:42:26.340 --> 00:42:30.460
18th Century, 19th
Century, different periods.

00:42:30.460 --> 00:42:33.140
&gt;&gt; Okay. Thank you
for your question.

00:42:33.140 --> 00:42:36.400
Obviously there's
always rosy retrospection

00:42:36.400 --> 00:42:37.700
about the old times, right?

00:42:37.700 --> 00:42:41.560
And we can always complain about
the kids these days, right,

00:42:41.560 --> 00:42:45.070
and their social media,
and what's going on,

00:42:45.070 --> 00:42:47.920
and I've tried not
to do that here,

00:42:47.920 --> 00:42:50.030
and what I've been
talking about.

00:42:50.030 --> 00:42:53.840
But I think one of the problems
that social media has created is

00:42:53.840 --> 00:42:57.630
that everyone is seen
as a purveyor of news

00:42:57.630 --> 00:43:00.790
with equal amount
of credibility.

00:43:00.790 --> 00:43:02.090
And I think that is a problem.

00:43:02.090 --> 00:43:06.600
And this is not helped
by the fact that the--

00:43:06.600 --> 00:43:09.730
that mainstream news
media is pretty reviled

00:43:09.730 --> 00:43:12.940
in this country right now, and
are losing their credibility.

00:43:12.940 --> 00:43:15.020
There are still more people
who get their information

00:43:15.020 --> 00:43:17.150
from television news
than social media,

00:43:17.150 --> 00:43:20.400
but that is on the
decline, in concordance

00:43:20.400 --> 00:43:21.850
with their credibility.

00:43:21.850 --> 00:43:24.430
So that can be a problem.

00:43:24.430 --> 00:43:27.970
But there is an important
place for legacy media,

00:43:27.970 --> 00:43:31.010
and for curated news,
for fact-checking,

00:43:31.010 --> 00:43:33.070
for double-checking
your sources.

00:43:33.070 --> 00:43:34.720
CNN got into a little
bit of problem

00:43:34.720 --> 00:43:36.280
with that last week,
didn't they, right?

00:43:36.280 --> 00:43:41.210
So I think that you know,
there's still a place for that.

00:43:41.210 --> 00:43:45.420
It is important, and the
fact that those are still,

00:43:45.420 --> 00:43:47.110
as I showed with those bubbles.

00:43:47.110 --> 00:43:49.410
The legacy media are still
some of the biggest bubbles

00:43:49.410 --> 00:43:50.710
up there that are linked to.

00:43:50.710 --> 00:43:53.890
So they play an important role
in the information environment,

00:43:53.890 --> 00:43:56.730
and that air of credibility

00:43:56.730 --> 00:43:59.070
and their news philosophy
is still very important

00:43:59.070 --> 00:44:00.370
to a lot of people.

00:44:00.370 --> 00:44:03.040
&gt;&gt; Right here.

00:44:04.440 --> 00:44:08.390
&gt;&gt; Hi, I would like you to
comment on the implications

00:44:08.390 --> 00:44:12.430
of social media for the
market place in terms

00:44:12.430 --> 00:44:15.960
of for example Bloomberg had
a piece a couple of weeks ago,

00:44:15.960 --> 00:44:19.810
or whenever Amazon
took over Whole Foods.

00:44:19.810 --> 00:44:23.050
Contrasting how many
Trader Joe's customers were

00:44:23.050 --> 00:44:27.210
at Whole Foods that day,
and you know, vice-versa,

00:44:27.210 --> 00:44:32.200
and so they knew avocados were
driving the issue, for example.

00:44:32.200 --> 00:44:36.800
So that was information
aggregated from social media.

00:44:36.800 --> 00:44:42.200
So one could assume that a fake
news story could drive a market

00:44:42.200 --> 00:44:44.510
price, for example,
a stock price.

00:44:44.510 --> 00:44:45.810
That kind of thing.

00:44:45.810 --> 00:44:47.110
&gt;&gt; Or an oil price [laughter].

00:44:47.110 --> 00:44:48.410
&gt;&gt; Yeah, an oil price.

00:44:48.410 --> 00:44:52.930
Do you have any comments on
the commercial implications?

00:44:52.930 --> 00:44:56.310
And even Bitcoin, for
example, of social media?

00:44:56.310 --> 00:44:59.330
&gt;&gt; Yeah, they are
obviously vast, right?

00:44:59.330 --> 00:45:03.050
If you can get a story that
can hook into people's emotions

00:45:03.050 --> 00:45:04.900
and can make them panic
about something, right?

00:45:04.900 --> 00:45:07.430
We can-- I'm not going to say
we're going to see, you know,

00:45:07.430 --> 00:45:09.800
a 1929 situation or
anything like that,

00:45:09.800 --> 00:45:14.780
but certainly people really,
you know, gravitate to stories

00:45:14.780 --> 00:45:18.990
that make them emotionally
anxious, afraid and worried.

00:45:18.990 --> 00:45:21.170
Those types of stories
have been shown

00:45:21.170 --> 00:45:23.110
by political science scholarship

00:45:23.110 --> 00:45:26.040
to stimulate information
seeking about those things.

00:45:26.040 --> 00:45:28.900
But on the flip side, if you're
getting that information,

00:45:28.900 --> 00:45:31.010
then you're going to be doing
more information seeking,

00:45:31.010 --> 00:45:34.430
then you can sort of find
out what's real about that.

00:45:34.430 --> 00:45:40.310
So it is possible, but I hope
that our natural human tendency

00:45:40.310 --> 00:45:43.880
in that instance is to seek out
more information and then to get

00:45:43.880 --> 00:45:45.680
to the truth, I would hope.

00:45:45.680 --> 00:45:48.440
&gt;&gt; You know, one
thing that hasn't come

00:45:48.440 --> 00:45:52.630
up directly is the
impact the President,

00:45:52.630 --> 00:45:54.330
both when he was
running for President,

00:45:54.330 --> 00:45:59.720
and now that he is President,
so what impact has his approach

00:45:59.720 --> 00:46:03.800
to social media had that
may be lasting in terms

00:46:03.800 --> 00:46:06.630
of electoral strategies,
and in terms of governing.

00:46:06.630 --> 00:46:08.530
Is it too soon to
talk about that?

00:46:08.530 --> 00:46:12.490
Or what is this-- where is
the scholarship going on that?

00:46:12.490 --> 00:46:15.730
You wrote a book about
2016, or part of it--

00:46:15.730 --> 00:46:17.030
&gt;&gt; I did about the campaign.

00:46:17.030 --> 00:46:18.720
People are writing
about governance now.

00:46:18.720 --> 00:46:20.020
&gt;&gt; Yeah, right.

00:46:20.020 --> 00:46:25.290
&gt;&gt; So that stuff is still
in process, but the things

00:46:25.290 --> 00:46:28.200
that we've seen about
Donald Trump

00:46:28.200 --> 00:46:30.340
that he's been effective
at, is remember I said

00:46:30.340 --> 00:46:34.250
that politicians use
Twitter to talk to the press?

00:46:34.250 --> 00:46:36.940
It's-- there's...it's not
a coincidence that he is

00:46:36.940 --> 00:46:38.980
up so early in the
morning Tweeting.

00:46:38.980 --> 00:46:41.250
He's trying to set the
media agenda for the day.

00:46:41.250 --> 00:46:43.140
He's trying to tell
us what, you know,

00:46:43.140 --> 00:46:44.440
we're going to be talking about.

00:46:44.440 --> 00:46:46.430
He's trying to tell the
people at the New York Times,

00:46:46.430 --> 00:46:51.200
the Washington Post, ABC News,
CNN, what he wants them to cover

00:46:51.200 --> 00:46:53.580
that night, by saying
provocative things,

00:46:53.580 --> 00:46:56.540
and getting traction on
those particular issues.

00:46:56.540 --> 00:46:59.170
I think we will probably see
other people trying to do that.

00:46:59.170 --> 00:47:02.060
I don't know if they'll have the
same sort of effect that he has.

00:47:02.060 --> 00:47:05.650
Another thing that we've seen
is the name-calling, right?

00:47:05.650 --> 00:47:09.140
We see Lying Ted, right?

00:47:09.140 --> 00:47:12.220
And Crooked Hillary.

00:47:12.220 --> 00:47:15.200
I hope that's not something
we're going to start seeing.

00:47:15.200 --> 00:47:18.150
But once you get something
like that attached to you,

00:47:18.150 --> 00:47:19.980
it becomes what political
scientists call a

00:47:19.980 --> 00:47:21.280
meta narrative.

00:47:21.280 --> 00:47:25.250
It becomes really difficult
to break out of, right?

00:47:25.250 --> 00:47:28.080
Al Gore never said he
invented the internet, right?

00:47:28.080 --> 00:47:31.620
But we have this sort of idea,
that you know, he did say that.

00:47:31.620 --> 00:47:33.590
Sarah Palin never
said "I can see Alaska

00:47:33.590 --> 00:47:35.460
from my house," right?

00:47:35.460 --> 00:47:38.890
But this Saturday Night
Live thing, you know--

00:47:38.890 --> 00:47:40.190
&gt;&gt; Russia, yeah.

00:47:40.190 --> 00:47:41.490
Russia.

00:47:41.490 --> 00:47:42.790
&gt;&gt; I can see-- what did I say?

00:47:42.790 --> 00:47:44.090
&gt;&gt; Alaska.

00:47:44.090 --> 00:47:45.390
&gt;&gt; Alaska, no.

00:47:45.390 --> 00:47:46.690
I can see Russia.

00:47:46.690 --> 00:47:47.990
&gt;&gt; She may neve have said that
either but [laughter], right?

00:47:47.990 --> 00:47:49.290
&gt;&gt; And Donald Trump
never said, and this went

00:47:49.290 --> 00:47:51.400
around during the election,
he never actually said,

00:47:51.400 --> 00:47:54.030
"If I ever ran for President
I would run as a republican

00:47:54.030 --> 00:47:55.330
because they're the
dumber party."

00:47:55.330 --> 00:47:58.160
There was a meme going
around that he said that.

00:47:58.160 --> 00:47:59.560
He actually never said that.

00:47:59.560 --> 00:48:01.280
That was debunked.

00:48:01.280 --> 00:48:03.330
But once you get these sorts
of things attached to you,

00:48:03.330 --> 00:48:06.410
it's really difficult
to get through that.

00:48:06.410 --> 00:48:10.510
And so this crooked Hillary
thing, really was difficult.

00:48:10.510 --> 00:48:13.870
And you know, we saw this in
the Virginia election, right?

00:48:13.870 --> 00:48:18.280
He tried to label, Enron
Ed, Ed Gillespie, right?

00:48:18.280 --> 00:48:21.180
So we're starting to see
some of this mimicry,

00:48:21.180 --> 00:48:24.260
going on in political campaigns.

00:48:24.260 --> 00:48:26.130
And it can be effective.

00:48:26.130 --> 00:48:28.250
And it does damage
to our discourse.

00:48:28.250 --> 00:48:29.820
We're certainly not
talking about policy

00:48:29.820 --> 00:48:31.120
when we're name-calling, right?

00:48:31.120 --> 00:48:33.540
&gt;&gt; They have a question
over here.

00:48:33.540 --> 00:48:35.510
Gentleman over here.

00:48:35.510 --> 00:48:45.040
[ Inaudible ]

00:48:45.040 --> 00:48:46.340
&gt;&gt; I'm sorry, I can't
hear you, sir.

00:48:46.340 --> 00:48:47.640
&gt;&gt; Anxious, afraid
and one other thing?

00:48:47.640 --> 00:48:48.940
&gt;&gt; Worried.

00:48:48.940 --> 00:48:50.240
Yeah. Yeah.

00:48:50.240 --> 00:48:52.530
&gt;&gt; It occurs to me in your
talk that the history really,

00:48:52.530 --> 00:48:57.140
the growth of the internet
since the early 90s,

00:48:57.140 --> 00:49:00.350
coincides with the
political polarization

00:49:00.350 --> 00:49:01.970
and gridlock in Congress.

00:49:01.970 --> 00:49:04.330
The disconnect and inability

00:49:04.330 --> 00:49:06.180
of Congressional
parties to work together.

00:49:06.180 --> 00:49:07.980
And Congress is effectively
broken

00:49:07.980 --> 00:49:09.610
and not working as it should.

00:49:09.610 --> 00:49:13.800
Do you see any correlation
between the growth

00:49:13.800 --> 00:49:17.210
of the information superhighway
as we once called it,

00:49:17.210 --> 00:49:19.260
and this new transparency
that brings in,

00:49:19.260 --> 00:49:21.460
and this breaking of our system?

00:49:21.460 --> 00:49:22.760
&gt;&gt; Thank you for that question.

00:49:22.760 --> 00:49:26.150
That is sort of the holy grail
right now of American politics,

00:49:26.150 --> 00:49:27.740
that people are trying
to work on.

00:49:27.740 --> 00:49:30.460
It is very difficult
to disentangle some

00:49:30.460 --> 00:49:32.280
of these things going
on at the same time.

00:49:32.280 --> 00:49:33.920
Let me tell you about
some of the other things

00:49:33.920 --> 00:49:35.220
that are going on right?

00:49:35.220 --> 00:49:37.350
Obviously we have
increased polarization.

00:49:37.350 --> 00:49:39.480
There are some really good
charts, that you can see people

00:49:39.480 --> 00:49:42.290
who identify strongly
republican, strongly democrat,

00:49:42.290 --> 00:49:46.330
nationwide, moving
out towards the poles.

00:49:46.330 --> 00:49:48.610
There's also good
information that shows

00:49:48.610 --> 00:49:53.020
on the DW Nominate scores
that they use to rank members

00:49:53.020 --> 00:49:55.280
of Congress as to
how they're voting.

00:49:55.280 --> 00:49:57.500
They're moving out as well.

00:49:57.500 --> 00:49:59.350
We also have the
internet, right?

00:49:59.350 --> 00:50:00.650
Social media.

00:50:00.650 --> 00:50:01.950
We talk about that today.

00:50:01.950 --> 00:50:05.210
But we also have what is
called Niche Media, now.

00:50:05.210 --> 00:50:08.170
How many channels you
got on your cable now?

00:50:08.170 --> 00:50:10.150
Can you find something
that you agree with?

00:50:10.150 --> 00:50:12.630
Does everybody have
different options

00:50:12.630 --> 00:50:14.930
that they can go
to for their news?

00:50:14.930 --> 00:50:16.230
They certainly do.

00:50:16.230 --> 00:50:18.620
And the advent of Niche
Media has also done that.

00:50:18.620 --> 00:50:20.630
And so some people
have said, you know,

00:50:20.630 --> 00:50:22.390
that's part of it as well.

00:50:22.390 --> 00:50:25.370
Another thing that is
going on that a recent book

00:50:25.370 --> 00:50:27.860
that was written called
"The Big Sort," it talks

00:50:27.860 --> 00:50:30.210
about self-sorting, right?

00:50:30.210 --> 00:50:33.010
Democrats are moving
to the cities.

00:50:33.010 --> 00:50:36.650
Right? Republicans are moving
to suburbs and to rural areas,

00:50:36.650 --> 00:50:39.870
and so our amount of
interaction with people

00:50:39.870 --> 00:50:44.040
with whom we disagree
with is becoming limited,

00:50:44.040 --> 00:50:45.930
and we can de-friend
them on social media.

00:50:45.930 --> 00:50:49.290
We don't hold so many dinner
parties anymore, right?

00:50:49.290 --> 00:50:53.960
We don't have so many social
groups that we go to, you know,

00:50:53.960 --> 00:50:56.990
Elks Lodge, and different
types of social interactions

00:50:56.990 --> 00:50:59.140
after work that people used
to have because of that,

00:50:59.140 --> 00:51:01.330
so there's a lot
of things going on.

00:51:01.330 --> 00:51:05.610
And so disentangling those, and
trying to say what's the chicken

00:51:05.610 --> 00:51:07.510
and what's the egg
here, is something

00:51:07.510 --> 00:51:09.310
that political scientists
are working on.

00:51:09.310 --> 00:51:12.320
We don't have quite the
answer yet, but yes.

00:51:12.320 --> 00:51:13.620
Stay tuned.

00:51:13.620 --> 00:51:14.920
&gt;&gt; Great question.

00:51:14.920 --> 00:51:16.220
We have time for
another question,

00:51:16.220 --> 00:51:17.520
I think we have actually
a couple over here.

00:51:17.520 --> 00:51:20.750
This gentleman here, and then we
will go over here, so two more.

00:51:20.750 --> 00:51:25.440
One, and then two,
then we're done.

00:51:25.440 --> 00:51:30.120
&gt;&gt; Hi. Would you also
comment on the volume

00:51:30.120 --> 00:51:33.900
of information people
get, you know,

00:51:33.900 --> 00:51:38.410
after 24 hours seven days
news, change the landscape

00:51:38.410 --> 00:51:42.470
of information, and now they
get, you know, 100 times more

00:51:42.470 --> 00:51:45.080
than that and how do they
actually make any sense

00:51:45.080 --> 00:51:46.380
of all that?

00:51:46.380 --> 00:51:50.320
Isn't that actually pushing
them to hold onto their hatred

00:51:50.320 --> 00:51:53.760
or their, you know, fears or
whatnot, to just, you know,

00:51:53.760 --> 00:51:57.080
find some sort of
ground in that chaos?

00:51:57.080 --> 00:51:59.650
&gt;&gt; Mm, yes it is [laughs].

00:51:59.650 --> 00:52:02.240
The fact that we have
the 24 hour news cycle,

00:52:02.240 --> 00:52:03.930
and that we have
information going on.

00:52:03.930 --> 00:52:07.830
Also causes mainstream press
right, to report things

00:52:07.830 --> 00:52:09.540
without fact-checking because
they want to stay ahead

00:52:09.540 --> 00:52:12.240
of the cycle as well that
has happened a couple times.

00:52:12.240 --> 00:52:14.040
Some things haven't been
double-checked, that they would,

00:52:14.040 --> 00:52:16.690
the fact that we
want to, you know,

00:52:16.690 --> 00:52:19.690
find things that can just
make us sleep easier, right?

00:52:19.690 --> 00:52:23.910
With this whole massive
information coming at us.

00:52:23.910 --> 00:52:26.770
And so dealing with those types

00:52:26.770 --> 00:52:30.920
of things certainly does make
us a little bit more polarized.

00:52:30.920 --> 00:52:36.210
We can even find that if you
look at when people interact

00:52:36.210 --> 00:52:39.660
with information with
which they disagree,

00:52:39.660 --> 00:52:41.110
they automatically shut off.

00:52:41.110 --> 00:52:43.440
Their brain waves are
not as active, as if,

00:52:43.440 --> 00:52:46.380
when they see information
that they agree with,

00:52:46.380 --> 00:52:47.680
they become more active.

00:52:47.680 --> 00:52:49.860
So how do we get past that?

00:52:49.860 --> 00:52:51.160
I mean, with this,

00:52:51.160 --> 00:52:54.350
this oppressive information all
the time, and then leading us

00:52:54.350 --> 00:52:56.400
to seek out the stuff
that we like or is going

00:52:56.400 --> 00:52:59.040
to make us comfortable,
and the answer actually is

00:52:59.040 --> 00:53:01.250
to find common ground
with other people.

00:53:01.250 --> 00:53:05.510
When you start with a premise
that you and somebody else agree

00:53:05.510 --> 00:53:07.410
on the same thing, right?

00:53:07.410 --> 00:53:12.810
The end goal, then you can keep
somebody in a conversation.

00:53:12.810 --> 00:53:15.390
Then you can start talking
about how we get to that goal,

00:53:15.390 --> 00:53:17.240
and what the means are.

00:53:17.240 --> 00:53:19.050
But social media, right?

00:53:19.050 --> 00:53:22.010
We're not having those long,
protracted conversations.

00:53:22.010 --> 00:53:23.820
We're having little
bits and bytes.

00:53:23.820 --> 00:53:26.440
That's something I think
we need to work on.

00:53:26.440 --> 00:53:28.960
&gt;&gt; Gentleman over here,
with the last question?

00:53:35.040 --> 00:53:36.480
&gt;&gt; Since we are at the
Library of Congress today,

00:53:36.480 --> 00:53:39.430
I'm going to finish up with
a library question for you.

00:53:39.430 --> 00:53:41.900
All right, to support your
work or scholars like you,

00:53:41.900 --> 00:53:43.600
or more importantly
future scholars like you,

00:53:43.600 --> 00:53:46.060
what do you see in the role
of archives, libraries,

00:53:46.060 --> 00:53:50.160
and memory institutions
with regard to social media?

00:53:50.160 --> 00:53:52.340
&gt;&gt; Okay, that is a
fantastic question.

00:53:52.340 --> 00:53:53.640
&gt;&gt; Did you put that question in?

00:53:53.640 --> 00:53:56.220
&gt;&gt; I did not [laughter].

00:53:56.220 --> 00:53:57.520
&gt;&gt; Appreciate it if you did.

00:53:57.520 --> 00:53:58.820
&gt;&gt; I didn't.

00:53:58.820 --> 00:54:01.410
The obviously the holdings are
extensive here at the library,

00:54:01.410 --> 00:54:06.410
and some of the holdings are,
especially the digital holdings,

00:54:06.410 --> 00:54:08.220
are held by others, right?

00:54:08.220 --> 00:54:10.220
Proquest, and such.

00:54:10.220 --> 00:54:12.060
One of the things that
the library could do

00:54:12.060 --> 00:54:15.220
to help scholars like myself is

00:54:15.220 --> 00:54:19.160
to develop an institutional
capacity

00:54:19.160 --> 00:54:22.770
for extracting and
aggregating data.

00:54:22.770 --> 00:54:24.780
One of the things I've had a
little bit of difficulty is

00:54:24.780 --> 00:54:27.730
if I would like to be able
to say okay, for my project,

00:54:27.730 --> 00:54:33.470
from 1960 to 2016, I want
every op ed page from this time

00:54:33.470 --> 00:54:36.890
to this time, and to be
able to have that extracted

00:54:36.890 --> 00:54:39.360
so I can put together a
data set, then sample off

00:54:39.360 --> 00:54:41.950
that data set, and do
that sort of thing.

00:54:41.950 --> 00:54:44.380
So I think building
some more capacity.

00:54:44.380 --> 00:54:47.920
Because what the
library has is fantastic.

00:54:47.920 --> 00:54:50.990
I think getting our fingers
into it, and getting it out in

00:54:50.990 --> 00:54:54.300
such a way, where we can
make it useful, I think,

00:54:54.300 --> 00:54:55.600
is the one place
that needs some work.

00:54:55.600 --> 00:54:58.510
&gt;&gt; Well, thank you again,
Todd, was very interesting.

00:54:58.510 --> 00:55:06.060
[ Applause ]

00:55:06.060 --> 00:55:10.110
&gt;&gt; This has been a presentation
of the Library of Congress.

00:55:10.110 --> 00:55:12.890
Visit us at LOC.gov.

