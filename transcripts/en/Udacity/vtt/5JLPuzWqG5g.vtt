WEBVTT
Kind: captions
Language: en

00:00:00.460 --> 00:00:03.016
You may ask, what if the protection domain is

00:00:03.016 --> 00:00:06.510
so large that it needs all of the hardware address

00:00:06.510 --> 00:00:09.080
space? Maybe the file system code base is so

00:00:09.080 --> 00:00:12.580
big that it needs the entire hardware address space. Cannot

00:00:12.580 --> 00:00:16.020
share it with anybody else. Similarly, the code base

00:00:16.020 --> 00:00:18.560
for the storage module is so big that it may

00:00:18.560 --> 00:00:21.740
occupy the entire hardware address space. Now, if this were

00:00:21.740 --> 00:00:25.530
the situation, that is, if the protection domains are so

00:00:25.530 --> 00:00:28.880
large that they pretty much occupy the entire hardware

00:00:28.880 --> 00:00:33.260
address space, and if the TLB does not support

00:00:33.260 --> 00:00:35.830
address space tagging, then you have to do a

00:00:35.830 --> 00:00:38.500
TLB flush on a context switch. If you go

00:00:38.500 --> 00:00:42.200
from this service to this service, you need to

00:00:42.200 --> 00:00:45.360
do a TLB flush because the memory footprint of

00:00:45.360 --> 00:00:48.240
each of these service is as big as the

00:00:48.240 --> 00:00:51.360
hardware address space that's available on the processor. Or in

00:00:51.360 --> 00:00:55.020
other words, the segments overlap and therefore you have

00:00:55.020 --> 00:00:58.390
to do a TLB flush when you do a

00:00:58.390 --> 00:01:01.500
context switch. The cost that we've been talking about

00:01:01.500 --> 00:01:04.390
so far is the explicit cost, that is, the

00:01:04.390 --> 00:01:08.800
cost that is incurred for flushing the TLB at

00:01:08.800 --> 00:01:11.540
the point of a context switch. And for small

00:01:11.540 --> 00:01:14.620
protection domains that we want to go across, we

00:01:14.620 --> 00:01:16.960
want to avoid that cost and we can do that

00:01:16.960 --> 00:01:19.620
by packing them in the linear address space

00:01:19.620 --> 00:01:22.770
provided by the hardware. But if the memory

00:01:22.770 --> 00:01:24.770
footprint of the service is so big that

00:01:24.770 --> 00:01:28.860
it occupies the hardware address space of the

00:01:28.860 --> 00:01:33.380
architecture completely, the explicit cost we're going to incur,

00:01:33.380 --> 00:01:35.840
because we have to do a TLB flush,

00:01:35.840 --> 00:01:38.380
if the TLB is not address space tagged.

00:01:38.380 --> 00:01:42.260
But that explicit cost is very insignificant compared

00:01:42.260 --> 00:01:46.540
to the implicit cost that is going to be incurred. Now what do we mean by

00:01:46.540 --> 00:01:50.540
implicit cost? We mean cache effects, that is,

00:01:50.540 --> 00:01:54.510
the loss of locality going from this service

00:01:54.510 --> 00:01:56.580
to this service, that the cache is not

00:01:56.580 --> 00:02:00.840
going to have the working set of this service,

00:02:00.840 --> 00:02:07.066
when we go from here to here. That impact is much more that the explicit cost.

00:02:07.066 --> 00:02:10.720
For example, LeKay shows that on the

00:02:10.720 --> 00:02:14.460
specific architecture in which they implemented L3, which

00:02:14.460 --> 00:02:16.750
is a Pentium architecture. It had 32

00:02:16.750 --> 00:02:20.880
entries for kernel translations and 64 entries for

00:02:20.880 --> 00:02:26.430
user translations. Even if you want to flush the entire TLB, all the entries in

00:02:26.430 --> 00:02:32.450
the TLB, it takes 864 cycles to do that. But, the loss of locality, when a

00:02:32.450 --> 00:02:36.510
service goes from this to this, in terms of cache

00:02:36.510 --> 00:02:39.690
effects is going to be much more significant because, with a

00:02:39.690 --> 00:02:44.060
large address space you expect that you're doing more work

00:02:44.060 --> 00:02:47.520
in the subsystem. And the implicit costs are going to dominate.

