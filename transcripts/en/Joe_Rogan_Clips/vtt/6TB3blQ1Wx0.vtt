WEBVTT

1
00:00:00.990 --> 00:00:04.560
The Joe Rogan experience.
Yeah,
man,

2
00:00:04.650 --> 00:00:06.600
that is interesting about scientists,
right?

3
00:00:06.601 --> 00:00:10.560
They're just concentrate on the task at hand.
Yeah.
I mean it wasn't that,

4
00:00:10.561 --> 00:00:13.590
that was like one of the big concerns about the Manhattan project,
right?

5
00:00:13.830 --> 00:00:16.170
This is the task.
The task is how to figure out how to do it.

6
00:00:16.171 --> 00:00:20.760
So they figured out how to do it,
not the eventual in yes.

7
00:00:20.780 --> 00:00:24.900
Consequences.
So when Robert Oppenheimer,
who was the lead of the,

8
00:00:24.901 --> 00:00:28.650
of the Manhattan project,
when that first bomb went off,
I mean he has his,

9
00:00:28.980 --> 00:00:32.460
his famous quote.
Yeah,
exactly.
I mean the,
the,
they,

10
00:00:32.470 --> 00:00:36.990
the English common translation was holy Shit,
what have we done?

11
00:00:37.590 --> 00:00:41.670
And,
and that's,
and this science is real,
but it's not going to be,

12
00:00:41.671 --> 00:00:43.890
it's not one person doing it.
I mean,

13
00:00:44.010 --> 00:00:47.840
that's the whole like science has been diffused,
at least with,

14
00:00:47.910 --> 00:00:52.580
with nuclear power,
is a relatively small number of people.
And it was,
uh,

15
00:00:52.670 --> 00:00:55.110
you know,
one or two states that could do it.

16
00:00:55.530 --> 00:01:00.130
Now with with precision gene editing,
I mean you get the Nobel prize for,

17
00:01:00.660 --> 00:01:02.490
for figuring out how to do,

18
00:01:02.491 --> 00:01:07.110
or you will get the Nobel prize for figuring out how to do CRISPR gene edits.

19
00:01:07.620 --> 00:01:10.770
But to apply it once the formula already exists,

20
00:01:11.400 --> 00:01:14.400
you get like an a minus in your high school biology class.

21
00:01:14.460 --> 00:01:18.510
So this technology is out there.
It's cheap,
it's accessible.

22
00:01:19.200 --> 00:01:24.200
Did you go to the 2045 conference in Manhattan a couple of years back?

23
00:01:24.300 --> 00:01:26.850
Now,
do you know about all that?
2044 that's,

24
00:01:27.390 --> 00:01:30.390
that's part of the thing with these transhumanists folks.

25
00:01:30.420 --> 00:01:34.860
They believe that with their own calculations of the exponential increase of

26
00:01:34.861 --> 00:01:38.400
technology that somewhere around 2045 singularity.
Yeah,
yeah.

27
00:01:38.580 --> 00:01:40.200
Were at the very least,

28
00:01:40.201 --> 00:01:43.560
we're going to reach this point where you're going to be able to either download

29
00:01:43.561 --> 00:01:47.280
consciousness or have some sort of an artificially intelligent,

30
00:01:47.281 --> 00:01:50.730
sentient things hanging out with you.
Yeah.
So as I'm involved,

31
00:01:50.731 --> 00:01:52.590
I'm on faculty for um,

32
00:01:52.610 --> 00:01:56.250
one of the programs of singularity university called exponential medicine.

33
00:01:56.251 --> 00:01:57.930
And so we're thinking a lot about that.

34
00:01:57.931 --> 00:02:01.890
Actually had an editorial in the New York Times a few weeks ago imagining a

35
00:02:01.891 --> 00:02:06.891
visit to a fertility clinic in the year 2045 and again because we are on this

36
00:02:06.991 --> 00:02:11.991
exponential change is it's really hard for people to to internalize,

37
00:02:12.361 --> 00:02:16.200
to kind of feel how fast these changes are coming.
I do think though,

38
00:02:16.440 --> 00:02:19.590
a ray Kurzweil who's who's a really incredible genius,

39
00:02:19.890 --> 00:02:24.030
he thinks that we are soon going to get to a point now where our artificial

40
00:02:24.031 --> 00:02:27.450
intelligence is self learning because when you think about Ai,

41
00:02:27.660 --> 00:02:30.960
if it gets to the point where it can read something,
read and comprehend,

42
00:02:31.290 --> 00:02:36.290
like in seconds it will read every book ever written in Cuban history and then

43
00:02:37.501 --> 00:02:41.280
it's says when you have all these doublings and all of this more knowledge,

44
00:02:41.281 --> 00:02:45.330
you can imagine how that would happen pretty quickly that the counter argument

45
00:02:45.331 --> 00:02:48.300
against,
and I think that it will,
but I don't think that we're,

46
00:02:48.340 --> 00:02:51.120
that our human brains are on one hand,

47
00:02:51.121 --> 00:02:54.240
they're incredibly complex and they're also kind of irrational.

48
00:02:54.270 --> 00:02:57.570
I mean we have all these different layers of our lizard brain and every decision

49
00:02:57.571 --> 00:02:59.680
that we make,
there's the rational decision,

50
00:03:00.190 --> 00:03:02.470
but then there's all the other stuff that our brains,

51
00:03:02.471 --> 00:03:04.870
that doesn't even rise to the level of our awareness that our,

52
00:03:04.900 --> 00:03:07.090
that our brains are processing.

53
00:03:07.720 --> 00:03:12.720
And right now we don't really have one really effective artificial intelligence

54
00:03:12.911 --> 00:03:14.830
algorithm,
which is for pattern recognition.

55
00:03:14.860 --> 00:03:17.560
But I think if you think of pattern recognition as a core skill of what our

56
00:03:17.561 --> 00:03:22.420
brains do,
our brain's probably have a thousand,
2000 different skills.
Um,

57
00:03:22.540 --> 00:03:27.310
but the core thing is whether we reached this singularity moment or not,

58
00:03:27.670 --> 00:03:30.550
these technologies are going to become incredibly more powerful.

59
00:03:30.560 --> 00:03:34.840
They're going to become increasingly integrated into our lives and into our

60
00:03:34.841 --> 00:03:38.530
beings.
And part of our evolutionary process there is no longer,
oh,

61
00:03:38.531 --> 00:03:42.520
we just have our biological evolution and our technological evolution.

62
00:03:42.520 --> 00:03:43.870
Those are separate things.
They're connected.

63
00:03:44.110 --> 00:03:46.210
It's going to be the weird question of whether or not,

64
00:03:46.630 --> 00:03:50.950
if an artificial intelligence is going to be able to absorb all of the writing

65
00:03:50.951 --> 00:03:53.920
that human beings have ever done and really understand us,
yeah.

66
00:03:54.010 --> 00:03:56.500
Will they really still be able to understand that just because they get all the

67
00:03:56.501 --> 00:04:00.630
writing.
So right now you would say no.
I'd say no.
Yeah.

68
00:04:00.730 --> 00:04:03.730
But 20 years from now,
50 years from now,
a hundred years from now,

69
00:04:03.760 --> 00:04:07.150
they could come up with a reasonable facsimile.
I mean,
yeah,

70
00:04:07.300 --> 00:04:10.540
they could figure out a way to get it close enough.
Yeah,

71
00:04:10.740 --> 00:04:13.480
I know where it's like her,
like the,
yeah.
Yeah.

72
00:04:13.630 --> 00:04:17.800
That's an essential point because I think when people imagine this AI future,

73
00:04:18.280 --> 00:04:22.240
they're imagining like some intimate relationship with some artificial

74
00:04:22.241 --> 00:04:24.790
intelligent,
intelligent,
that feels just like a human.

75
00:04:24.870 --> 00:04:27.650
I don't think that's going to happen because it's,
it don't,
well,
no,

76
00:04:27.651 --> 00:04:32.260
but just because AI,
it will be its own form of intelligence and it may not be,

77
00:04:32.620 --> 00:04:36.190
frankly,
you wouldn't want ais with these brains like we have that have all these

78
00:04:36.191 --> 00:04:40.030
different impulses that are kind of imagining all this,
this crazy stuff we're,

79
00:04:40.090 --> 00:04:43.150
we may want them to be more rational than,
than we are.

80
00:04:43.150 --> 00:04:47.950
So like chimpanzees or our close relatives,
they don't think just like us,

81
00:04:48.130 --> 00:04:51.340
we're not know.
We're not expecting them to think like they're their own thing.

82
00:04:51.341 --> 00:04:55.090
And I think AI's will be their own things.
Will we be interacting with them?

83
00:04:55.360 --> 00:04:59.140
Will we be having sex with them?
Yes.
But if they're there,

84
00:04:59.200 --> 00:05:01.540
it's not going to be that.
They're just like us.
We're going to,

85
00:05:01.720 --> 00:05:05.920
they're going to be these things that live within us,
live with us,

86
00:05:06.040 --> 00:05:08.710
and together we're going to evolve.
Well,

87
00:05:08.740 --> 00:05:12.680
there's certainly already better at doing certain things like playing chess and

88
00:05:12.690 --> 00:05:16.870
it took a long time for an artificial intelligence to be able to compete against

89
00:05:16.871 --> 00:05:20.980
a real chess master.
But now they swamp them.
Yeah.
So they learn quickly,

90
00:05:21.050 --> 00:05:24.400
like it's incredibly quickly they teach themselves.
Yeah.
So,

91
00:05:24.401 --> 00:05:27.130
so first we had chess in chess.
People said,
oh,

92
00:05:27.131 --> 00:05:28.780
that's what it means to be a human.

93
00:05:29.200 --> 00:05:32.950
The computers will never beat humans at chess.
Now it's like everyone says,

94
00:05:32.951 --> 00:05:35.770
well no human could ever compete.
And then they said,

95
00:05:35.771 --> 00:05:39.210
well there's this Chinese game of go,
which kind of when people here look at it,

96
00:05:39.211 --> 00:05:42.310
it looks kind of like checkers,
but it's actually way more sophisticated.

97
00:05:42.311 --> 00:05:43.840
Way More complicated than chess.

98
00:05:44.020 --> 00:05:49.020
I heard that there are more moves in go more potential than there are stars in

99
00:05:49.241 --> 00:05:50.230
the universe.
Yes.

100
00:05:50.290 --> 00:05:54.750
So so then they had Alphago that this this uh,

101
00:05:54.760 --> 00:05:57.950
company deep mind which was later by Google.

102
00:05:58.160 --> 00:06:03.160
They built this algorithm that in 2016 defeated the world champions of going

103
00:06:03.480 --> 00:06:04.191
people thought that was,

104
00:06:04.191 --> 00:06:09.191
we were decades away and then deep mind created this new program called Alpha

105
00:06:09.501 --> 00:06:12.110
Zero Alpha zero with Alphago.

106
00:06:12.410 --> 00:06:16.610
They gave it access to all of the digitized games of go.

107
00:06:16.640 --> 00:06:21.640
So it very quickly was able to learn from how everybody else had played go alpha

108
00:06:21.951 --> 00:06:22.580
zero.

109
00:06:22.580 --> 00:06:27.580
They just said here are the basic rules of go and they let Alphago just play

110
00:06:27.891 --> 00:06:32.000
against itself with no other experience other than here are the rules and play

111
00:06:32.001 --> 00:06:37.001
against in four days Alpha zero destroyed Alphago and then Alpha and then Alpha

112
00:06:41.120 --> 00:06:46.120
zero destroyed the world champions of chess and destroyed every other computer

113
00:06:47.181 --> 00:06:49.640
program that had ever played chess.
And this again,

114
00:06:49.641 --> 00:06:53.960
those computer programs had internalized all of the chess games of grandmasters.

115
00:06:53.990 --> 00:06:56.300
Alpha zero had not internalized any,

116
00:06:56.301 --> 00:06:59.330
it just played against itself for a few days.
And then Shogi,

117
00:06:59.331 --> 00:07:02.210
which is a Japanese traditional game,
kind of like chess.

118
00:07:02.211 --> 00:07:07.130
It destroyed the grandmasters of that.
So that's what I'm saying is that these,

119
00:07:07.540 --> 00:07:09.260
the world is changing.

120
00:07:09.261 --> 00:07:14.261
It's changing so much faster than we anticipate and we have to be as ready for

121
00:07:14.871 --> 00:07:15.530
that as we can.

122
00:07:15.530 --> 00:07:18.470
I think we need to come to grips with the fact that we're way stupider than we

123
00:07:18.471 --> 00:07:19.304
think we are.

124
00:07:19.520 --> 00:07:23.630
We what we think we're really intelligent and we are in comparison to everything

125
00:07:23.631 --> 00:07:26.660
else on this planet.
Yeah.
But in comparison to what is possible,

126
00:07:26.840 --> 00:07:31.220
we are really fucking dumb in comparison to what this computer can do and what

127
00:07:31.350 --> 00:07:31.680
the,

128
00:07:31.680 --> 00:07:34.940
the future of that computers and what maybe that computer's going to redesign

129
00:07:34.941 --> 00:07:38.360
another computer.
Yeah.
This is good,
but I've got some,
I've got some hiccups here.

130
00:07:38.750 --> 00:07:41.850
Yeah,
no,
it's true.
But,
and yet the technology is us,
right?
It's like this.

131
00:07:42.560 --> 00:07:45.470
Not like this technology is some alien force.
We've,
it's like this.

132
00:07:45.650 --> 00:07:49.370
It's like we create art.
We create.
We could,
yeah.
You use that.

133
00:07:49.400 --> 00:07:51.860
You mentioned cities in order,
like we create these cities,

134
00:07:51.861 --> 00:07:56.120
which are these incredible places where dreams can happen to cities like here in

135
00:07:56.121 --> 00:07:58.250
Los Angeles or New York where,
where,
where I'm from.

136
00:07:58.251 --> 00:08:03.080
So this technology is us and the challenge is how can we make sure that this

137
00:08:03.200 --> 00:08:07.910
technology serves our needs rather than undermines our needs.

138
00:08:08.300 --> 00:08:08.870
Yeah.

139
00:08:08.870 --> 00:08:13.870
And whether or not our needs supersede the needs of the human race or supersedes

140
00:08:14.331 --> 00:08:18.560
the needs of the planet.
Yeah.
It's,
we're,

141
00:08:18.590 --> 00:08:22.290
we're almost too much chimp,
right,
to contemplate these.
Yeah.

142
00:08:22.460 --> 00:08:24.510
Critical decisions in terms of like what,

143
00:08:24.620 --> 00:08:29.030
how it's going to unfold from here on out.
We really might not we,

144
00:08:29.210 --> 00:08:32.270
but the people that are actually at the tip of the spear of this stuff,

145
00:08:32.271 --> 00:08:37.271
they really might be affecting the way the planet is shaped absolutely.

146
00:08:38.110 --> 00:08:40.310
From now.
And we're doing that now.
I mean we are,

147
00:08:40.311 --> 00:08:42.110
there is an article came out the other day.

148
00:08:42.111 --> 00:08:45.500
There's a million species that are on the verge of extinction.

149
00:08:45.501 --> 00:08:49.790
We are driving all these other species to extinction where warming the planet.

150
00:08:49.791 --> 00:08:54.470
So this is humans are that the determining factor in many ways for how this

151
00:08:54.530 --> 00:08:58.720
planet plays out.
And that's why in my mind,
everything comes back to values.
We,

152
00:08:58.750 --> 00:09:02.940
you're right,
we have this,
this lizard nature,
this,

153
00:09:02.970 --> 00:09:05.790
this monkey nature.
It's,
it's who we are.
And that's,
I mean,

154
00:09:05.791 --> 00:09:08.280
you wouldn't want to take that away because that's the core of,
of,

155
00:09:08.400 --> 00:09:13.050
of what we are.
And yet we're also a species that has created philosophy.

156
00:09:13.051 --> 00:09:17.880
We've created a beautiful religions in traditions and art.
And the question is,

157
00:09:17.940 --> 00:09:22.940
which version of us is going to lead us into the future?

158
00:09:22.981 --> 00:09:27.900
If it's this,
you know,
tribal primate with these urges,

159
00:09:28.260 --> 00:09:30.850
like that's really frightening.
If we can say,
you know,

160
00:09:30.860 --> 00:09:34.530
we've done better and worse in history and we had this terrible second world war

161
00:09:34.890 --> 00:09:37.560
and yet at the end of the Second World War with American leadership,

162
00:09:37.950 --> 00:09:41.010
the world came together.
We established at United Nations,

163
00:09:41.011 --> 00:09:43.140
we establish these concepts of human rights.

164
00:09:43.141 --> 00:09:46.580
Like you can't just kill everybody in your own country and say,
hey,
it's,

165
00:09:46.581 --> 00:09:50.220
it's just my business.
So we have this capability.

166
00:09:50.580 --> 00:09:52.020
But it's always a struggle.
I mean,

167
00:09:52.021 --> 00:09:54.840
neither these forces are always at war with each other in many ways.

168
00:09:56.670 --> 00:10:00.410
It's just too much to think about.
Yeah.
Well,
we have,
I know

169
00:10:00.530 --> 00:10:01.560
<v 1>we do have to.
Um,</v>

170
00:10:01.580 --> 00:10:06.580
one of the things that's always been amusing to me is that we seem to have this

171
00:10:08.690 --> 00:10:13.290
insatiable desire to improve things.
Yeah.
And I've always wondered why I like it,

172
00:10:13.510 --> 00:10:17.660
but is that maybe because this is what human beings are here for?

173
00:10:17.990 --> 00:10:22.020
It's what we do.
It's who we are.
Right?
Yeah.
But this is a product.
It's just,
uh,

174
00:10:22.130 --> 00:10:24.080
uh,
us being intelligent,

175
00:10:24.081 --> 00:10:28.430
trying to survive against nature and predators and weather and all the,

176
00:10:28.431 --> 00:10:33.290
all the different issues that we came up that we evolved growing up and dealing

177
00:10:33.291 --> 00:10:36.290
with.
And then now we just want things to be better.

178
00:10:36.530 --> 00:10:40.460
We just want things to be more convenient,
faster,
but more data.

