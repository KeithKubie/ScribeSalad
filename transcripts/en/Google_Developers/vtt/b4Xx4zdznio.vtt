WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:10.240
&gt;&gt; CROSBY: So, I'm Scott Crosby at Rice University.
This is my research into rendering my post

00:00:10.240 --> 00:00:18.130
doc on Google Wave--on a security. So I decided
to look at the authentication the Google Wave

00:00:18.130 --> 00:00:25.470
is--Google Wave uses for its cryptography.
So, crypto Google--so Google Wave uses a lot

00:00:25.470 --> 00:00:30.510
of crypto and the generally--generally speaking,
three of the major goals of cryptography are

00:00:30.510 --> 00:00:34.359
authentication. If you get who created that
kind of message. The second goal of cryptography

00:00:34.359 --> 00:00:38.390
is integrity, dealing with defective messages
are corrupted or altered. The third reason

00:00:38.390 --> 00:00:39.449
for...
&gt;&gt; Louder.

00:00:39.449 --> 00:00:46.449
&gt;&gt; CROSBY: Oh, the third reason for cryptography
is confidentiality. And I did a benchmark

00:00:46.449 --> 00:00:52.609
for--based on the FedOne text clients. We
just basically inserted blips into Waves continuously

00:00:52.609 --> 00:00:58.989
and I got about--and I--in that benchmark
showed that it was spending about 80% of their

00:00:58.989 --> 00:01:04.239
time was spent on cryptography, in particular,
RSA signing and that's because RSA signing

00:01:04.239 --> 00:01:10.280
is about 10 milliseconds a signature. So there's
a few solutions for this problem, one is you

00:01:10.280 --> 00:01:15.930
could consider using DSA. DSA signatures are
about 10 times faster. You can also consider

00:01:15.930 --> 00:01:23.700
hash tree based designs which can offer throughputs
much, much higher. And for verifying, of course,

00:01:23.700 --> 00:01:30.030
it's a tradeoff; faster signing, slower verification;
slower signing, faster verification. RSA can

00:01:30.030 --> 00:01:35.560
verify about 3,500 messages per second and
I'm going to--it is my research topic called

00:01:35.560 --> 00:01:40.780
"The Spliced Signatures." So in this talk,
I'm going to focus on reducing the crypto

00:01:40.780 --> 00:01:47.500
costs by both appropriate choice of cryptography
primitives and also aspect of the system design.

00:01:47.500 --> 00:01:52.270
And the goal is to create a primitive that's
semantically identical to sign each message

00:01:52.270 --> 00:01:59.710
one at a time individually. I don't want to
add any new interesting semantics, a design.

00:01:59.710 --> 00:02:06.040
So why is OpenSSL not a good solution for
this? It's fast, well-implemented and widespread,

00:02:06.040 --> 00:02:12.160
and OpenSSL is also very cheap. Once you initiate
a session, you're just paying basically cryptographic

00:02:12.160 --> 00:02:18.569
hash costs as--cryptographic hash costs and
encryption costs which have extremely high

00:02:18.569 --> 00:02:25.040
throughputs and of course, framing overheads.
An OpenSSL is good and is using Google Wave

00:02:25.040 --> 00:02:30.799
but offers primarily confidentiality; no one
can read the XMPP communication. It also offers

00:02:30.799 --> 00:02:37.010
integrity; you can detect it if it gets corrected
and, of course, SSL serves--also serve extremely

00:02:37.010 --> 00:02:43.049
valuable purpose. They bind a public key to
a domain name so you can tell who is the actual

00:02:43.049 --> 00:02:51.099
public key for wave.google.com. So when the
prompts OpenSSL is it doesn't really give

00:02:51.099 --> 00:02:55.920
authentication in a useful fashion--in a system
like--federated system like Google Wave because

00:02:55.920 --> 00:03:00.879
the sender of a message, the source of a message
isn't actually the author of the message.

00:03:00.879 --> 00:03:06.650
For example, if I'm the author of a message
or an update that's gets in the home--remote

00:03:06.650 --> 00:03:11.860
home or remote serve--home server for some
Wave then gets redistributed to a client,

00:03:11.860 --> 00:03:18.900
those clients receive the message from the
home server, not me. So, therefore--and you

00:03:18.900 --> 00:03:28.290
want to prevent corrupt, broken, or otherwise--a
corrupt, broken, or rooted home server from

00:03:28.290 --> 00:03:33.769
forging messages under my identity. And other
issues are, for example, that OpenSSL only

00:03:33.769 --> 00:03:38.799
authenticates the end point of the communication,
which means it can't actually cache the information.

00:03:38.799 --> 00:03:45.139
You can't usefully cache the signature along
with the actual messaging question. And two,

00:03:45.139 --> 00:03:51.329
so therefore, Wave on authenticating messages
for signing, there's a lot--tons of signatures.

00:03:51.329 --> 00:03:57.620
Roughly each wavelet operation has to be signed
when it gets sent. And for verifying, nominally,

00:03:57.620 --> 00:04:02.549
each reader has to verify each wavelet op,
so if a wave has 10 readers in different servers;

00:04:02.549 --> 00:04:09.650
that could potentially mean 10 verifications
for each operation. So, therefore, verifying

00:04:09.650 --> 00:04:16.590
is at least as frequent as signing and possibly
much more frequent. And crypto is slow. Here

00:04:16.590 --> 00:04:23.870
are some benchmarks taken on Java using one
CPU core of a Core i7 machine in terms of

00:04:23.870 --> 00:04:28.380
the rate per second. Now FedOne, if you disable
a cryptography, gets about 300 messages per

00:04:28.380 --> 00:04:35.120
CPU second. But using the current RSA 2048-bit
signatures, performance is a lot lower, I

00:04:35.120 --> 00:04:41.820
believe, around 80 or so per second and this
is why 80% of the time is spent on the crypto.

00:04:41.820 --> 00:04:47.070
Now the first optimization you could consider
is well-known Merkle trees from about 30 years

00:04:47.070 --> 00:04:53.620
ago. Now you use to amortize one expensive
signature over as many messages. And in this

00:04:53.620 --> 00:04:59.350
case, I use SHA-256 hash operations, they're
a bottom--I need about 10 to 20 of them about

00:04:59.350 --> 00:05:05.570
a microsecond each to do this. So a Merkle
tree is--starts off the use all the messages

00:05:05.570 --> 00:05:10.130
in the leaves of a tree. Here's an example
of seven messages. Then, you then create internal

00:05:10.130 --> 00:05:15.940
nodes in a tree where the hash--the hash of
an interior node is the hash link it initially--hashes

00:05:15.940 --> 00:05:22.310
of its children. So the first one on the left
is the hash of the concatenation of the hashes

00:05:22.310 --> 00:05:27.080
of the messages X1 and X2 and so forth and
of course, you can add additional layers up

00:05:27.080 --> 00:05:31.191
to the tree creating a root. If you take that
root, you can then proceed to sign it. Here,

00:05:31.191 --> 00:05:35.960
the box represents the signature over the
root hash value, not the--doesn't actually--does

00:05:35.960 --> 00:05:42.750
not actually contain a hash value. Then, of
course, this can be--this energy can authenticate

00:05:42.750 --> 00:05:47.580
all the messages in the tree. Now if you want
to actually authenticate a specific message,

00:05:47.580 --> 00:05:52.770
it's actually pretty cheap. You just have
to send, in this case, four separate hashes

00:05:52.770 --> 00:05:58.720
because from these hashes, I can infer up
the tree where the root should be. Here's,

00:05:58.720 --> 00:06:03.180
of course, the implicit parts of the tree
that are in a sense--and it can then check

00:06:03.180 --> 00:06:08.530
is the reconstructed root hash equal to the
hash that was signed if the signature matches?

00:06:08.530 --> 00:06:15.350
Then the dark green nodes combined with verifying
the root hash can authenticate my message

00:06:15.350 --> 00:06:22.420
right there. You don't have to send, in this
case, four hashes or about 128 bytes. And

00:06:22.420 --> 00:06:29.310
Merkle trees are insanely fast. So, the actual
Merkle tree itself is about 20 microseconds

00:06:29.310 --> 00:06:36.630
a message, to act--this amortize cost of--if
you have between 10 and 100,000--10 and 10,000

00:06:36.630 --> 00:06:43.710
messages and it costs to build the tree and
also to build and serialize the [INDISTINCT]

00:06:43.710 --> 00:06:50.300
tree and that's from basically a list of blobs
to a list of signature blobs all serialized.

00:06:50.300 --> 00:06:56.780
And you only serialize one tree for each leaf.
So if I have, you know, 10,000 messages; put

00:06:56.780 --> 00:07:02.740
it on a Merkle tree or 10,000 byte arrays
put in a Merkle tree and then generate 10,000

00:07:02.740 --> 00:07:09.170
byte arrays of serialized protocol buffers.
For each of the 10,000 distinct messages is,

00:07:09.170 --> 00:07:17.210
in this case, about 200 milliseconds. You
never actually use a tree this big. So they

00:07:17.210 --> 00:07:22.290
actually use this where you need asynchronous
signing API and here's an example of one such

00:07:22.290 --> 00:07:27.430
APIs you have. An outgoing message may have
a signature call that gets called with the

00:07:27.430 --> 00:07:32.170
byte array and of course, you can also request
for where the message byte is to be signed.

00:07:32.170 --> 00:07:37.930
Now, I actually have a main implementation
code. I actually pass back a protocol buffer--builder

00:07:37.930 --> 00:07:43.961
objects is--the idea is it's within my code
base that I designed to be integrated with

00:07:43.961 --> 00:07:50.490
Google Wave. I take out the protocol buffer,
I put in appropriate fields for the Merkle

00:07:50.490 --> 00:07:58.240
tree information. That gets called back and
then the part that has--the part that signs

00:07:58.240 --> 00:08:04.000
each message individually using a certificate
as in--sets its own necessary fields and then,

00:08:04.000 --> 00:08:10.460
of course, it serializes it via the normal
processing flow. And, of course, the--since

00:08:10.460 --> 00:08:13.992
I'm not sending messages one at a time, I've
designed a batch, I have basically a sign

00:08:13.992 --> 00:08:18.310
queue where I just submit a message to the
queue. And whenever I want to, I call the

00:08:18.310 --> 00:08:25.000
run method which basically builds a Merkle
tree and processes everything. So actually

00:08:25.000 --> 00:08:32.770
benchmark this on the new API using--and this
is including the public key signature, RSA

00:08:32.770 --> 00:08:39.090
or DSA. And one CPU core can process over
30,000 messages per second potentially and

00:08:39.090 --> 00:08:47.400
that's--and you can also choose the CPU utilization.
And the idea is that if messages arrive faster,

00:08:47.400 --> 00:08:51.580
the trees get larger. If messages arrive slower,
the trees get smaller if you run the queue

00:08:51.580 --> 00:08:57.810
as fast as possible and you do that--can do
that to minimize the latency. Alternatively,

00:08:57.810 --> 00:09:03.900
if you have a latency demand that's lower,
you can run on a slower schedule. For example,

00:09:03.900 --> 00:09:12.340
run every 10 milliseconds, in which case,
CPU usage remains approximately the same.

00:09:12.340 --> 00:09:18.840
Sorry. If you run DSA at 10 milliseconds,
CPU usage will drop significantly and latency

00:09:18.840 --> 00:09:27.940
will increase because cryptographic hashes
are much fast. In practice, RSA message rates

00:09:27.940 --> 00:09:32.150
at a few thousand per second. Almost all your
time is actually spent in the RSA signatures

00:09:32.150 --> 00:09:41.950
so you have inherent overload resistance if
messages write faster, unexpectedly fast.

00:09:41.950 --> 00:09:47.880
The next part is for--this is how do you optimize
verification? Verifications are harder to

00:09:47.880 --> 00:09:54.240
optimize because in practice, the user sends
consecutives message on a Wave. The--Because

00:09:54.240 --> 00:09:57.990
of latency, they'll probably be in different
batches which means that the number of public

00:09:57.990 --> 00:10:02.130
keys that a federate server has to verify
is roughly equal to the number of messages

00:10:02.130 --> 00:10:09.700
that are sent on the Wave. Now, this isn't
quite true: a large scale provider, like a

00:10:09.700 --> 00:10:15.940
Google scale provider for federation, might
happen to notice--get--receive many messages

00:10:15.940 --> 00:10:24.890
from the same batch on the same--from the
same batch and it could receive message--sorry.

00:10:24.890 --> 00:10:32.160
A large scale provider like Google could no--could
happen to receive messages sent to--from different

00:10:32.160 --> 00:10:36.570
users on different Waves which happen to occur
in the same batch and not re-verify the root

00:10:36.570 --> 00:10:42.080
public key signature multiple times. But for
a smaller scale provider, roughly the number

00:10:42.080 --> 00:10:50.320
of public key verifies equals the number of
messages sent to that federated provider.

00:10:50.320 --> 00:10:55.180
And what that means is unlimited to the throughput
of standard public crypto algorithms which

00:10:55.180 --> 00:11:03.530
is 3,700 messages per second with RSA or about
5,000--sorry, 580 per second with DSA. But

00:11:03.530 --> 00:11:09.300
you don't actually have to verify all the
messages immediately in many situations. For

00:11:09.300 --> 00:11:14.770
example, you want to verify asynchronously.
For example, the users of a Wave--the participant--some

00:11:14.770 --> 00:11:22.180
of their participants on a Wave may in fact
be offline in which case you could, in principle,

00:11:22.180 --> 00:11:27.940
avoid verifying their signatures until the
user actually logs in, then you verify all

00:11:27.940 --> 00:11:33.810
of the queued up messages that were target
to them. And the user might be, for example,

00:11:33.810 --> 00:11:38.470
read-only. It might be just reading the Wave
without actually actively modifying it, in

00:11:38.470 --> 00:11:43.839
which case you might be willing to accept
a few seconds of latency in return for a lower--in

00:11:43.839 --> 00:11:49.860
return for reducing crypto overheads. And
finally, of course, if a participant is just

00:11:49.860 --> 00:11:54.720
added to a Wave on a new federated server,
there's a catch up in all the old messages,

00:11:54.720 --> 00:11:57.460
in which case you don't want to catch up [INDISTINCT]
version saying [INDISTINCT] every message

00:11:57.460 --> 00:12:03.010
in the entire history since the beginning.
In some other cases, being one asynchronous

00:12:03.010 --> 00:12:07.940
verifying API, something along these lines
right here. We have incoming message and a

00:12:07.940 --> 00:12:14.860
couple of get the signature, get the bytes--get
the data bytes and a callback that gets called

00:12:14.860 --> 00:12:18.610
when it gets verified. Now, when you're verifying
it, you of course consummate incoming message

00:12:18.610 --> 00:12:23.210
saying, "Here's a message right here." And
we also want to force an incoming message.

00:12:23.210 --> 00:12:30.220
This means basically I need to know this message
is valid right now. And--or you can also force

00:12:30.220 --> 00:12:36.730
everything in the queue. Reason why I do this
right here is if you want to exploit delayed

00:12:36.730 --> 00:12:41.040
messages, you need to indicate when a message
has to be verified immediately. In other words,

00:12:41.040 --> 00:12:46.070
you want to let it sit around because as soon
as you get to an asynchronous place, signatures

00:12:46.070 --> 00:12:53.350
can amortize one public key verification over
several messages. And there's actually--are

00:12:53.350 --> 00:13:01.100
a lot of locality in a message for Google
Wave. So here, I collected--so here I received

00:13:01.100 --> 00:13:08.490
from Google interarrival times for wave messages.
So here what I do is I basically look at,

00:13:08.490 --> 00:13:13.140
for a user on a specific wave, what fraction
in the messages were sent with an arrival

00:13:13.140 --> 00:13:18.010
time less than the threshold? So it's saying
that 55% of the messages the user sends on

00:13:18.010 --> 00:13:22.481
the wave are less than one second from the
proceeding message on that wave. And, of course,

00:13:22.481 --> 00:13:27.360
here the rate goes up to 98% at 20 seconds.
So if you're catching up and from--so if you're

00:13:27.360 --> 00:13:35.980
doing a catch-up, you could somehow exploit
the asynchronous verification. It could potentially

00:13:35.980 --> 00:13:40.612
reduce a large asynchronous verifications
by a rather large margin in many of these

00:13:40.612 --> 00:13:45.590
scenarios. For example, a logged-off user,
when they log in, you might be able to save

00:13:45.590 --> 00:13:50.480
98% of the signature verifications if they're
the only participant on that wave on that

00:13:50.480 --> 00:13:58.610
server. So what a spliced signature is sort
of amortizing one public key verification

00:13:58.610 --> 00:14:03.890
over a sequence of messages. So here, I assume
I have a stream of messages. I have the cooperation

00:14:03.890 --> 00:14:09.640
of the signer. The signer has to explicitly
include splice points in each signature. Splices

00:14:09.640 --> 00:14:13.340
are also transitive and this is--splice is
also transitive. I'll show you an example

00:14:13.340 --> 00:14:17.180
on the next slide. And this is a subject of
a research paper that will be going out in

00:14:17.180 --> 00:14:22.340
about a week and a half. So here's a use case.
Let's say Alice publishes to some wave, three

00:14:22.340 --> 00:14:27.320
messages; A, B and C, each which includes
a splice in the proceeding message. At this

00:14:27.320 --> 00:14:33.089
point, Bob, a user on that wave on different
federate server, logs in. At that point, he

00:14:33.089 --> 00:14:39.541
wants to verify and catch up on all the proceeding
messages. What Bob can do is balancing and

00:14:39.541 --> 00:14:44.740
verifying a signature on A, B, and C individually.
It could do that but it can notice that these

00:14:44.740 --> 00:14:53.160
splices exist and can be connected together.
So Bob can--so at this point, Bob will force

00:14:53.160 --> 00:14:58.530
the message--the message basically A and will
implicitly get--in this case, Bob can force

00:14:58.530 --> 00:15:03.770
the message C, verify the one public key signature
on C, and then get all three messages basically

00:15:03.770 --> 00:15:09.040
for free with one public key verification
using very cheap hash operations. There's

00:15:09.040 --> 00:15:14.960
a lot of bookkeeping to track all the partial
dependencies of all the splices and implementation

00:15:14.960 --> 00:15:21.380
terms. But this is--the principle of it is
it's--if you want to force a particular message,

00:15:21.380 --> 00:15:27.010
you figure out--you do one public key verification
and you get--and you get--potentially get

00:15:27.010 --> 00:15:32.920
one or many other messages for free with no
public key verifications at all. So the benefits

00:15:32.920 --> 00:15:38.709
of this are--was done on a trace of 9 million
messages derived from Google Wave data. Here,

00:15:38.709 --> 00:15:44.830
I inferred--here, I actually modeled the situation
where I actually inferred log in and log off

00:15:44.830 --> 00:15:49.269
times for users. So if some of the user was
online, then they wanted zero latency. If

00:15:49.269 --> 00:15:56.180
they were offline, I was allowed to queue
up messages until they became online. So this

00:15:56.180 --> 00:16:03.760
scenery right here, roughly half the messages
were forced, meaning I'd had to have them

00:16:03.760 --> 00:16:09.420
verified immediately. The other half, I was
able to delay verification. However, I performed

00:16:09.420 --> 00:16:16.300
fewer RSA verifications. This is because as
many times our system was overloaded faster

00:16:16.300 --> 00:16:22.040
than they can do RSA verifications 22% of
the time. In these cases right here, I was

00:16:22.040 --> 00:16:28.089
unable to verify RSA messages as fast--messages
that--I was unable to verify messages that

00:16:28.089 --> 00:16:31.960
were needed to be verified now as fast as
they were arriving. So they got buffered up

00:16:31.960 --> 00:16:36.850
and I was able to, with one verification,
get several of these messages at once. So

00:16:36.850 --> 00:16:41.620
even despite being overloaded 22% of the time,
I have latency less than 30 milliseconds,

00:16:41.620 --> 00:16:47.860
99 and a half, 0.5% of them. In worst cases,
only a couple hundred milliseconds despite

00:16:47.860 --> 00:16:52.530
being overloaded far faster; this was including
all the bookkeeping overhead as far as the

00:16:52.530 --> 00:16:59.589
signature module goes. Now, protocol changes
to incorporate this in dif. For example, the

00:16:59.589 --> 00:17:09.049
FedOne protocol this time. We need more forward
compatible representation of what a signed

00:17:09.049 --> 00:17:14.210
wavelet delta is, because you need to support,
for example, individually send messages as

00:17:14.210 --> 00:17:19.390
is now possibly in a Merkle tree approach,
or splice signatures if you decide to use

00:17:19.390 --> 00:17:26.179
them; also possibly handle RSA or DSA signatures.
Finally, there's a new cryptographic hash

00:17:26.179 --> 00:17:30.870
being worked on right now which will become
the future SHA-3. Just as AES was produced--the

00:17:30.870 --> 00:17:36.260
result of the contest in about--between 2000
and 2002. There's a currently ongoing contest

00:17:36.260 --> 00:17:42.400
to replace SHA-1 and the SHA-2 family of hash
functions. So you need it to be able to handle

00:17:42.400 --> 00:17:47.760
the future migration to SHA-3. I have a design
and I may bring it up in the protocol discussion

00:17:47.760 --> 00:17:56.890
later today. So basically, I looked at ways--so
here's possible solutions for reducing crypto

00:17:56.890 --> 00:18:02.350
overheads in FedOne which could lead to various
performance improvements in the underlying

00:18:02.350 --> 00:18:08.110
code base. First thing is you could disable
a crypto if the server is local-only. DSA

00:18:08.110 --> 00:18:12.161
signatures give you some scalability at a
fairly cheap cost. This would be if you could

00:18:12.161 --> 00:18:18.799
get and use DSA certificates for assigning
usually the deltas because DSA is about 10

00:18:18.799 --> 00:18:25.059
times faster to sign. Three times slower to
verify but 10 times faster to sign at about

00:18:25.059 --> 00:18:30.720
a thousand signs and a thousand verifies per
second. And at least from my benchmarks in

00:18:30.720 --> 00:18:35.360
applying [INDISTINCT] law, I would estimate
that would improve the FedOne throughput from

00:18:35.360 --> 00:18:41.040
about a factor of five using one CPU core.
Other traces are more implementation complexity

00:18:41.040 --> 00:18:47.670
to be RSA with Merkle trees or these implementation
terms' much more complicated splice signatures.

00:18:47.670 --> 00:18:54.000
Now don't--you don't have to be as complicated
as--you can't verify them without actually

00:18:54.000 --> 00:18:59.260
using the splices because the spliced signatures--the
splices may be used but they don't have to

00:18:59.260 --> 00:19:04.540
be used. You could always verify issue message
one at a time, in which case, it's a little

00:19:04.540 --> 00:19:14.880
more complex that doing--than doing Merkle
trees. So the code status I have here is the

00:19:14.880 --> 00:19:19.070
underlying code for the cryptography building
on the prune trees and serializing them is

00:19:19.070 --> 00:19:27.640
complete and unit tested. The queue for managing
the APIs--asynchronous APIs I described is

00:19:27.640 --> 00:19:33.040
somewhat more immature and the spliced signatures--all
the bookkeeping to track all the splices is

00:19:33.040 --> 00:19:41.040
the least matured code base. We don't need
to do--but that doesn't have to be--spliced

00:19:41.040 --> 00:19:46.350
signatures can still be used without having
that code activated. Questions? Yes.

00:19:46.350 --> 00:19:57.650
&gt;&gt; In your experiment, were you running all
this through FedOne?

00:19:57.650 --> 00:20:02.740
&gt;&gt; CROSBY: No. I was hoping to but I didn't
understand the FedOne code base enough to

00:20:02.740 --> 00:20:09.539
actually integrate in with the crypto back-ends.
&gt;&gt; So you're just generating the XMPP traffic?

00:20:09.539 --> 00:20:16.260
&gt;&gt; CROSBY: I was actually running a separate
program. I basically generated a program that

00:20:16.260 --> 00:20:19.420
would--I generally the queue--stuff which
operated those queuing the asynchronous API

00:20:19.420 --> 00:20:24.020
we described. Then sent a traffic generator
which submitted messages to that, finding

00:20:24.020 --> 00:20:31.070
the Google Wave trace data. So we submitted
information into the queue saying, "Message

00:20:31.070 --> 00:20:37.360
arrived, message arrived." And then we requested,
okay, user just log-in now first on the following

00:20:37.360 --> 00:20:40.559
messages.
&gt;&gt; Was your Merkle tree--like, so the Merkle

00:20:40.559 --> 00:20:52.940
tree is just you every time you send a Merkle
tree it was with a proper trace area?

00:20:52.940 --> 00:21:01.150
&gt;&gt; CROSBY: Yeah.
&gt;&gt; It was a list of hashes...

00:21:01.150 --> 00:21:05.309
&gt;&gt; CROSBY: Yeah.
&gt;&gt; ...and a signature of the entire trading?

00:21:05.309 --> 00:21:06.309
&gt;&gt; CROSBY: No.
&gt;&gt; And then what kind of classifications if

00:21:06.309 --> 00:21:07.309
you saw that signature again for the group
hash or the hash which is part of the trade

00:21:07.309 --> 00:21:08.309
[INDISTINCT].
&gt;&gt; CROSBY: Hold on. So what's the question?

00:21:08.309 --> 00:21:23.080
&gt;&gt; I'm trying to figure out how it actually
be included in the [INDISTINCT] so if, let's

00:21:23.080 --> 00:21:27.220
say, the--a wave of like one federating server
and the whole [INDISTINCT] server then does

00:21:27.220 --> 00:21:28.520
each op individually say, "I'm sending you
this element in the tree. Also, here are a

00:21:28.520 --> 00:21:29.520
bunch of hashes."
&gt;&gt; CROSBY: Well, no. Each op--I'm actually

00:21:29.520 --> 00:21:33.320
presuming a semantics of I have the data bug--a
byte array coming in and byte array coming

00:21:33.320 --> 00:21:34.320
out.
&gt;&gt; Yeah.

00:21:34.320 --> 00:21:39.130
&gt;&gt; CROSBY: The outgoing byte array consists
of basically the green things you see--the

00:21:39.130 --> 00:21:44.620
four green hashes along with using this current
FedOne certificate to sign the hash of the

00:21:44.620 --> 00:21:49.540
root. So it consists of the current signature
size plus four hashes plus a few bites of

00:21:49.540 --> 00:21:51.560
vertical buff above overhead.
&gt;&gt; Yup.

00:21:51.560 --> 00:21:56.450
&gt;&gt; CROSBY: That's--each one is independent
and separately sets.

00:21:56.450 --> 00:22:02.070
&gt;&gt; Okay. But does--but on the other side,
the server which we verify is, well--it'll

00:22:02.070 --> 00:22:05.720
cache the hash every now and then. In that
way, it precedes another element which is

00:22:05.720 --> 00:22:10.400
in that same tree again [INDISTINCT].
&gt;&gt; CROSBY: It could. However, in practice,

00:22:10.400 --> 00:22:17.110
I expect trees would be relatively small.
For example, 20--might be as few as 20 elements

00:22:17.110 --> 00:22:23.070
because if you're running an--RSA is the slowest
algorithm. If you--I mean, it could still

00:22:23.070 --> 00:22:28.450
run in this code base right here about 80
times a second which means it's at 20 elements

00:22:28.450 --> 00:22:33.580
per tree. You're talking about a throughput
of 1,600 messages per second which is five

00:22:33.580 --> 00:22:39.230
times faster than the one CPU throughput of
FedOne in the benchmark that I did.

00:22:39.230 --> 00:22:41.559
&gt;&gt; Yeah.
&gt;&gt; CROSBY: In which case, if you happen to

00:22:41.559 --> 00:22:45.480
encounter two in the same tree, you could
apply the optimization and you could ask--if

00:22:45.480 --> 00:22:46.940
you're Google Scale...
&gt;&gt; Yeah.

00:22:46.940 --> 00:22:50.960
&gt;&gt; CROSBY: First of all, is it worthwhile
even to detect that duplication effort. If

00:22:50.960 --> 00:22:54.240
we're not Google scaling, I mean, I think
your odds of even getting towards the same

00:22:54.240 --> 00:22:55.240
tree is pretty low.
&gt;&gt; Yeah.

00:22:55.240 --> 00:22:58.350
&gt;&gt; CROSBY: And if you're on Google's scale,
is it worthwhile detecting it?

00:22:58.350 --> 00:23:04.980
&gt;&gt; Right. Yeah. Do you also encode the overhead
of waiting until you have that many messages

00:23:04.980 --> 00:23:12.900
before you send any of them to the federation?
&gt;&gt; CROSBY: In practice--since the time to

00:23:12.900 --> 00:23:20.669
actually build the tree is so small, in one
millisecond, I can build a tree of 20 nodes.

00:23:20.669 --> 00:23:24.470
So I don't--RSA loop, it's 10 milliseconds
to do the signature, which you have to pay

00:23:24.470 --> 00:23:30.350
no matter what. And then one millisecond lets
you add 20 things under the tree so getting

00:23:30.350 --> 00:23:36.960
one millisecond under the latency in return
for up by about 18 times increase in the throughput.

00:23:36.960 --> 00:23:39.270
&gt;&gt; Okay.
&gt;&gt; CROSBY: So you are adding us--you are adding

00:23:39.270 --> 00:23:43.010
leads with such a small amount relative to
the signature size that it's--that optimization

00:23:43.010 --> 00:23:47.289
is almost--in terms of latency, it's almost
free.

00:23:47.289 --> 00:23:51.900
&gt;&gt; Yeah. Cool.
&gt;&gt; CROSBY: I do have latency graphs for the--all--half

00:23:51.900 --> 00:23:58.610
of the figure. I don't have them in the presentation.
But generally speaking, up to 20 or 30,000

00:23:58.610 --> 00:24:01.740
per second, latency is about--you give them
about two or three signatures.

00:24:01.740 --> 00:24:06.590
&gt;&gt; Yeah.
&gt;&gt; CROSBY: So you're talking 30 milliseconds,

00:24:06.590 --> 00:24:09.669
pretty much 90% of the time, less than 30
milliseconds.

00:24:09.669 --> 00:24:13.049
&gt;&gt; Okay.
&gt;&gt; CROSBY: And with the DSA, less than 10

00:24:13.049 --> 00:24:28.450
milliseconds...Yes?
&gt;&gt; You said that the problem is the remote

00:24:28.450 --> 00:24:30.620
host offline [INDISTINCT]. What will you do
if it never comes back [INDISTINCT] resending

00:24:30.620 --> 00:24:35.390
deltas and then even it never comes back so
you will never be able to complete all the

00:24:35.390 --> 00:24:51.049
data just to verify it?
&gt;&gt; CROSBY: No, sorry, do you mean the case

00:24:51.049 --> 00:24:53.490
of the spliced signature example? Or do you...?
&gt;&gt; Yes.

00:24:53.490 --> 00:24:59.940
&gt;&gt; CROSBY: Okay. Then in this case right here,
what actually--a spliced signature, I think

00:24:59.940 --> 00:25:04.950
it--oh, I'm sorry. I--If you remember the
Merkle I just had right there, a spliced signature

00:25:04.950 --> 00:25:09.150
includes additional path, additional leaves,
so it's included as part of the signature

00:25:09.150 --> 00:25:15.410
atomically. So a spliced signature right here,
this would include about a dozen or so more--10

00:25:15.410 --> 00:25:20.460
to 20 more hashes than just a straight Merkle
tree signature would include and it can be

00:25:20.460 --> 00:25:25.160
verified independently of any remote server.
So if you adjust these three blobs and no

00:25:25.160 --> 00:25:30.610
other information whatsoever, I can do the
verification. This means includes additional

00:25:30.610 --> 00:25:34.960
information and then makes it independent
of any future contact with the server. So

00:25:34.960 --> 00:25:40.220
you have the public key to use to verify the
individual message and you have the auxiliary

00:25:40.220 --> 00:25:46.430
information independent from the server to
link it with a prior message. And if it links

00:25:46.430 --> 00:25:52.720
to a prior message that you don't have, well,
you just ignore it. Or if you--or if you don't--or

00:25:52.720 --> 00:25:56.700
if, for example, the status message never
arrived at all; say, Bob--pretended B didn't

00:25:56.700 --> 00:26:03.020
exist. Here I include C as a splice to B I
don't have; in which case, I have to verify

00:26:03.020 --> 00:26:07.549
both A signature and C signature. And this
is actually--the hash is supposed to--splice

00:26:07.549 --> 00:26:14.380
will be overhead that I paid but I get nothing
for it. However, in practice, you can generate--in

00:26:14.380 --> 00:26:18.750
the case of Wave, you can generate useful
splices. If, for example, Alice is sending

00:26:18.750 --> 00:26:23.169
three messages in the Wave, you can always
include a splice to the previous message that

00:26:23.169 --> 00:26:30.010
Alice sent to the same Wave. That's an obviously
good thing to know that that means that their

00:26:30.010 --> 00:26:36.840
readers can be able to verify all three messages.
And you--so you need to make sure the splice

00:26:36.840 --> 00:26:44.320
has some minimal usefulness. If Alice is--if
Bob is willing to take--Bob's server is willing

00:26:44.320 --> 00:26:51.169
to take the latency hits either voluntarily
or involuntarily through overload. In my example,

00:26:51.169 --> 00:26:57.590
I overloaded very often in my trace simulation
and so what it does, Bob was forced to use

00:26:57.590 --> 00:27:02.910
splices even though Bob wanted to verify--my
message immediately. So I had to verify the

00:27:02.910 --> 00:27:05.960
public signature immediately; but I was on
overload, I wasn't able to keep up.

00:27:05.960 --> 00:27:32.100
&gt;&gt; Great. Thank you [INDISTINCT].

