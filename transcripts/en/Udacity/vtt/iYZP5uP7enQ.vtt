WEBVTT
Kind: captions
Language: en

00:00:00.720 --> 00:00:01.720
All right.
Welcome back.

00:00:01.720 --> 00:00:03.820
I hope you took a nice break there.

00:00:03.820 --> 00:00:07.238
As you can see, I'm, well, I'm totally,
actually, it's, it's another decade and

00:00:07.238 --> 00:00:08.240
I just kind of look the same.

00:00:09.680 --> 00:00:12.360
So what we've done is we've laid
out sort of the general idea

00:00:12.360 --> 00:00:16.730
of a Markov process, a Markov system,
and also a hidden Markov model.

00:00:16.730 --> 00:00:21.340
And what I want to do now is talk about
sort of three computational problems

00:00:21.340 --> 00:00:22.730
of hidden Markov models.

00:00:22.730 --> 00:00:27.160
And then we'll talk just about a simple
application of this to, computer vision.

00:00:28.620 --> 00:00:32.360
Also, just as a word of note,
I'm not going to go into the detail of

00:00:32.360 --> 00:00:35.480
different parts of, of lots of the math,
just one part of it and

00:00:35.480 --> 00:00:39.140
then the rest I'll, as they say,
I will leave it to the reader or

00:00:39.140 --> 00:00:43.050
the viewer, as an exercise to go get,
schooled up in it.

00:00:43.050 --> 00:00:47.550
Because, the details were less important
than knowing that it could be done and

00:00:47.550 --> 00:00:49.870
then it was applied to computer vision.

00:00:49.870 --> 00:00:54.080
All right so, we have a hidden Markov
model and we say that our hidden,

00:00:54.080 --> 00:00:57.830
hidden Markov model is a lambda,
is a triplet of A, B, and pi

00:00:57.830 --> 00:01:01.890
where A is the transition probability,
B your emission probabilities, and

00:01:01.890 --> 00:01:05.000
pi was just the distribution
where things started.

00:01:05.000 --> 00:01:08.200
Well given that, a HMM, given a machine,

00:01:08.200 --> 00:01:11.900
there are three computational
problems we can look at.

00:01:11.900 --> 00:01:13.450
And the first one is this.

00:01:13.450 --> 00:01:17.020
Basically, given some
observation sequence, so

00:01:17.020 --> 00:01:22.840
remember o1 through o cap T, are the
observed outputs, from 1 to cap T.

00:01:22.840 --> 00:01:25.020
So that's a particular sequence.

00:01:25.020 --> 00:01:30.560
You could ask what is the probability,
P of O, given that machine?

00:01:30.560 --> 00:01:34.550
That is, what's the likelihood that
that machine generating that particular

00:01:34.550 --> 00:01:35.740
sequence?

00:01:35.740 --> 00:01:40.140
And this is really the classification,
recognition, it says problem but

00:01:40.140 --> 00:01:42.020
actually how you use it for that, right?

00:01:42.020 --> 00:01:44.780
Assume I've got a couple different
h m m's, maybe three of them, and

00:01:44.780 --> 00:01:46.280
they each represent an activity.

00:01:46.280 --> 00:01:50.230
And let's assume that the prior of
which activity is going on are equal.

00:01:50.230 --> 00:01:50.980
All right?

00:01:50.980 --> 00:01:53.300
So then, once I've,
if I have the priors are equal,

00:01:53.300 --> 00:01:57.060
all I care about is the likelihood
of the data given the model.

00:01:57.060 --> 00:01:59.960
And the HMM would allow me to evaluate

00:01:59.960 --> 00:02:03.470
the likelihood of the observed
sequence for each of the given models.

00:02:03.470 --> 00:02:05.930
Whichever one was highest,
that would be the one I picked.

00:02:05.930 --> 00:02:08.788
That's a generative model
like we talked about before.

00:02:08.788 --> 00:02:12.950
So the evaluation problem is,
substance the key problem at run time.

00:02:12.950 --> 00:02:14.180
It's one we'll actually talk about.

00:02:15.686 --> 00:02:19.030
Another problem is what's
called decoding the sequence.

00:02:19.030 --> 00:02:23.380
So, remember you have
a sequence of observations, and

00:02:23.380 --> 00:02:27.100
the question is,
what is the single most likely

00:02:27.100 --> 00:02:32.100
sequence of states to have generated
that sequence of observations?

00:02:32.100 --> 00:02:36.370
So, it's not all possible ways these
observations could generate, it's just

00:02:36.370 --> 00:02:40.880
the single most likely sequence,
sometimes called the decoding problem.

00:02:40.880 --> 00:02:44.080
And sometimes you care about that,
sometimes you don't.

00:02:44.080 --> 00:02:46.190
When you do it basically
has to do with saying,

00:02:46.190 --> 00:02:49.740
well if I, if the states have
some particular meaning to me.

00:02:49.740 --> 00:02:51.310
Right?
Remember we talk about sunny, rainy,

00:02:51.310 --> 00:02:53.900
cloudy, so you know they might
have some specific meaning.

00:02:53.900 --> 00:02:59.160
I might want to say, okay here's what I
think the underlying state sequence, is,

00:02:59.160 --> 00:03:02.860
actually sometimes in,
genome work, etcetera.

00:03:02.860 --> 00:03:06.677
They actually want to know what
the underlying state sequence would be.

00:03:06.677 --> 00:03:09.290
But if you are actually just
doing it for recognition,

00:03:09.290 --> 00:03:12.180
then that's,
in some sense less important.

00:03:12.180 --> 00:03:16.680
And then the third problem, is in some
sense the most complicated problem.

00:03:16.680 --> 00:03:20.220
And that is, okay,
I give you a bunch of training examples,

00:03:20.220 --> 00:03:22.330
a bunch of these O sequences.

00:03:22.330 --> 00:03:23.430
Train model.

00:03:23.430 --> 00:03:27.920
Learn the Lambda that and the way you,
that you learn is you pick the one

00:03:27.920 --> 00:03:31.200
that maximizes the likelihood
of getting that observation.

00:03:31.200 --> 00:03:33.250
It's a maximum likelihood, method.

00:03:33.250 --> 00:03:35.810
It's actually called the EM,
Expectation Maximization,

00:03:35.810 --> 00:03:38.130
and we'll talk about
that just a little bit.

00:03:38.130 --> 00:03:40.710
We won't actually,
do the math of the, of that part.

