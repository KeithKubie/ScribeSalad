WEBVTT
Kind: captions
Language: en

00:00:00.570 --> 00:00:03.710
 
in the last video you saw how very deep

00:00:03.710 --> 00:00:03.720
in the last video you saw how very deep
 

00:00:03.720 --> 00:00:05.630
in the last video you saw how very deep
neural networks can have the problems of

00:00:05.630 --> 00:00:05.640
neural networks can have the problems of
 

00:00:05.640 --> 00:00:08.330
neural networks can have the problems of
banishing and exploding gradients it

00:00:08.330 --> 00:00:08.340
banishing and exploding gradients it
 

00:00:08.340 --> 00:00:10.669
banishing and exploding gradients it
turns out that a partial solution to

00:00:10.669 --> 00:00:10.679
turns out that a partial solution to
 

00:00:10.679 --> 00:00:12.799
turns out that a partial solution to
this doesn't solve an entirely but host

00:00:12.799 --> 00:00:12.809
this doesn't solve an entirely but host
 

00:00:12.809 --> 00:00:15.530
this doesn't solve an entirely but host
a lot is better or more careful choice

00:00:15.530 --> 00:00:15.540
a lot is better or more careful choice
 

00:00:15.540 --> 00:00:18.019
a lot is better or more careful choice
of the random initialization for your

00:00:18.019 --> 00:00:18.029
of the random initialization for your
 

00:00:18.029 --> 00:00:20.450
of the random initialization for your
neural network to understand this let's

00:00:20.450 --> 00:00:20.460
neural network to understand this let's
 

00:00:20.460 --> 00:00:22.220
neural network to understand this let's
start with the example of initializing

00:00:22.220 --> 00:00:22.230
start with the example of initializing
 

00:00:22.230 --> 00:00:24.620
start with the example of initializing
the weights for a single neuron and they

00:00:24.620 --> 00:00:24.630
the weights for a single neuron and they
 

00:00:24.630 --> 00:00:26.660
the weights for a single neuron and they
will go on to generalize this to a deep

00:00:26.660 --> 00:00:26.670
will go on to generalize this to a deep
 

00:00:26.670 --> 00:00:28.759
will go on to generalize this to a deep
network let's go through this with an

00:00:28.759 --> 00:00:28.769
network let's go through this with an
 

00:00:28.769 --> 00:00:31.279
network let's go through this with an
example with just a single neuron and

00:00:31.279 --> 00:00:31.289
example with just a single neuron and
 

00:00:31.289 --> 00:00:33.470
example with just a single neuron and
we'll talk about a deep manzanita so the

00:00:33.470 --> 00:00:33.480
we'll talk about a deep manzanita so the
 

00:00:33.480 --> 00:00:35.420
we'll talk about a deep manzanita so the
single neuron you might input for

00:00:35.420 --> 00:00:35.430
single neuron you might input for
 

00:00:35.430 --> 00:00:38.660
single neuron you might input for
features x1 3 X 4 and then you have some

00:00:38.660 --> 00:00:38.670
features x1 3 X 4 and then you have some
 

00:00:38.670 --> 00:00:42.020
features x1 3 X 4 and then you have some
a equals G of Z and then it outputs um Y

00:00:42.020 --> 00:00:42.030
a equals G of Z and then it outputs um Y
 

00:00:42.030 --> 00:00:44.990
a equals G of Z and then it outputs um Y
and later on for a definite you know

00:00:44.990 --> 00:00:45.000
and later on for a definite you know
 

00:00:45.000 --> 00:00:48.279
and later on for a definite you know
these inputs will be very some layer AOL

00:00:48.279 --> 00:00:48.289
these inputs will be very some layer AOL
 

00:00:48.289 --> 00:00:51.229
these inputs will be very some layer AOL
but for now let's just call this thanks

00:00:51.229 --> 00:00:51.239
but for now let's just call this thanks
 

00:00:51.239 --> 00:00:55.910
but for now let's just call this thanks
for now so Z is going to be equal to W 1

00:00:55.910 --> 00:00:55.920
for now so Z is going to be equal to W 1
 

00:00:55.920 --> 00:01:00.770
for now so Z is going to be equal to W 1
X 1 plus W 2 X 2 plus dot dot plus I

00:01:00.770 --> 00:01:00.780
X 1 plus W 2 X 2 plus dot dot plus I
 

00:01:00.780 --> 00:01:05.240
X 1 plus W 2 X 2 plus dot dot plus I
guess W n X n and and let's set B equals

00:01:05.240 --> 00:01:05.250
guess W n X n and and let's set B equals
 

00:01:05.250 --> 00:01:07.429
guess W n X n and and let's set B equals
0 so you know let's just ignore be for

00:01:07.429 --> 00:01:07.439
0 so you know let's just ignore be for
 

00:01:07.439 --> 00:01:11.179
0 so you know let's just ignore be for
now so in order to make V not blow up

00:01:11.179 --> 00:01:11.189
now so in order to make V not blow up
 

00:01:11.189 --> 00:01:14.719
now so in order to make V not blow up
and not become too small you notice that

00:01:14.719 --> 00:01:14.729
and not become too small you notice that
 

00:01:14.729 --> 00:01:20.510
and not become too small you notice that
the larger n is the smaller you want WI

00:01:20.510 --> 00:01:20.520
the larger n is the smaller you want WI
 

00:01:20.520 --> 00:01:24.649
the larger n is the smaller you want WI
to be right because E is a sum of WI X I

00:01:24.649 --> 00:01:24.659
to be right because E is a sum of WI X I
 

00:01:24.659 --> 00:01:27.440
to be right because E is a sum of WI X I
and so if you're adding up a lot of

00:01:27.440 --> 00:01:27.450
and so if you're adding up a lot of
 

00:01:27.450 --> 00:01:29.179
and so if you're adding up a lot of
these terms you want each of these terms

00:01:29.179 --> 00:01:29.189
these terms you want each of these terms
 

00:01:29.189 --> 00:01:32.260
these terms you want each of these terms
to be smaller one reasonable thing to do

00:01:32.260 --> 00:01:32.270
to be smaller one reasonable thing to do
 

00:01:32.270 --> 00:01:38.240
to be smaller one reasonable thing to do
would be to set the variance of WI to be

00:01:38.240 --> 00:01:38.250
would be to set the variance of WI to be
 

00:01:38.250 --> 00:01:42.350
would be to set the variance of WI to be
equal to 1 over N where n is the number

00:01:42.350 --> 00:01:42.360
equal to 1 over N where n is the number
 

00:01:42.360 --> 00:01:44.690
equal to 1 over N where n is the number
of input features as going into a neuron

00:01:44.690 --> 00:01:44.700
of input features as going into a neuron
 

00:01:44.700 --> 00:01:48.020
of input features as going into a neuron
so in practice what you can do is set

00:01:48.020 --> 00:01:48.030
so in practice what you can do is set
 

00:01:48.030 --> 00:01:50.800
so in practice what you can do is set
the weight matrix W for a certain layer

00:01:50.800 --> 00:01:50.810
the weight matrix W for a certain layer
 

00:01:50.810 --> 00:01:57.200
the weight matrix W for a certain layer
to the NP thoughts random thought Rand n

00:01:57.200 --> 00:01:57.210
to the NP thoughts random thought Rand n
 

00:01:57.210 --> 00:01:59.359
to the NP thoughts random thought Rand n
you know and then whatever the shape of

00:01:59.359 --> 00:01:59.369
you know and then whatever the shape of
 

00:01:59.369 --> 00:02:01.880
you know and then whatever the shape of
the matrix is you fold this out here um

00:02:01.880 --> 00:02:01.890
the matrix is you fold this out here um
 

00:02:01.890 --> 00:02:08.230
the matrix is you fold this out here um
and then times square root of 1 over

00:02:08.230 --> 00:02:08.240
and then times square root of 1 over
 

00:02:08.240 --> 00:02:12.370
and then times square root of 1 over
number of features set into each neuron

00:02:12.370 --> 00:02:12.380
number of features set into each neuron
 

00:02:12.380 --> 00:02:15.880
number of features set into each neuron
in their health is going to n of L minus

00:02:15.880 --> 00:02:15.890
in their health is going to n of L minus
 

00:02:15.890 --> 00:02:17.860
in their health is going to n of L minus
1 because that's the number of units

00:02:17.860 --> 00:02:17.870
1 because that's the number of units
 

00:02:17.870 --> 00:02:20.170
1 because that's the number of units
that are feeding in to each of the units

00:02:20.170 --> 00:02:20.180
that are feeding in to each of the units
 

00:02:20.180 --> 00:02:22.780
that are feeding in to each of the units
in layer now it turns out that if you

00:02:22.780 --> 00:02:22.790
in layer now it turns out that if you
 

00:02:22.790 --> 00:02:26.110
in layer now it turns out that if you
are using a rare Lu activation function

00:02:26.110 --> 00:02:26.120
are using a rare Lu activation function
 

00:02:26.120 --> 00:02:28.330
are using a rare Lu activation function
then rather than one over n it turns out

00:02:28.330 --> 00:02:28.340
then rather than one over n it turns out
 

00:02:28.340 --> 00:02:30.820
then rather than one over n it turns out
that said military institutions were a

00:02:30.820 --> 00:02:30.830
that said military institutions were a
 

00:02:30.830 --> 00:02:33.250
that said military institutions were a
little bit better so you often see that

00:02:33.250 --> 00:02:33.260
little bit better so you often see that
 

00:02:33.260 --> 00:02:35.650
little bit better so you often see that
in initialization especially using a

00:02:35.650 --> 00:02:35.660
in initialization especially using a
 

00:02:35.660 --> 00:02:38.940
in initialization especially using a
value activation function so if GL of

00:02:38.940 --> 00:02:38.950
value activation function so if GL of
 

00:02:38.950 --> 00:02:43.270
value activation function so if GL of
the years value of V o and tavella how

00:02:43.270 --> 00:02:43.280
the years value of V o and tavella how
 

00:02:43.280 --> 00:02:44.920
the years value of V o and tavella how
familiar you are with random variables

00:02:44.920 --> 00:02:44.930
familiar you are with random variables
 

00:02:44.930 --> 00:02:47.170
familiar you are with random variables
it turns out that something is Gaussian

00:02:47.170 --> 00:02:47.180
it turns out that something is Gaussian
 

00:02:47.180 --> 00:02:49.210
it turns out that something is Gaussian
random variable and then multiplying it

00:02:49.210 --> 00:02:49.220
random variable and then multiplying it
 

00:02:49.220 --> 00:02:51.580
random variable and then multiplying it
by square root of this that says the

00:02:51.580 --> 00:02:51.590
by square root of this that says the
 

00:02:51.590 --> 00:02:54.430
by square root of this that says the
variance to to be equal to this thing to

00:02:54.430 --> 00:02:54.440
variance to to be equal to this thing to
 

00:02:54.440 --> 00:02:56.500
variance to to be equal to this thing to
be 2 over n okay and the reason I went

00:02:56.500 --> 00:02:56.510
be 2 over n okay and the reason I went
 

00:02:56.510 --> 00:02:58.780
be 2 over n okay and the reason I went
from n to this n superscript L minus 1

00:02:58.780 --> 00:02:58.790
from n to this n superscript L minus 1
 

00:02:58.790 --> 00:03:01.900
from n to this n superscript L minus 1
was in this example with logistic

00:03:01.900 --> 00:03:01.910
was in this example with logistic
 

00:03:01.910 --> 00:03:03.430
was in this example with logistic
regression which is that any input

00:03:03.430 --> 00:03:03.440
regression which is that any input
 

00:03:03.440 --> 00:03:05.950
regression which is that any input
features but in more general case layer

00:03:05.950 --> 00:03:05.960
features but in more general case layer
 

00:03:05.960 --> 00:03:11.020
features but in more general case layer
L would have an L minus 1 inputs each of

00:03:11.020 --> 00:03:11.030
L would have an L minus 1 inputs each of
 

00:03:11.030 --> 00:03:13.840
L would have an L minus 1 inputs each of
the units and that layer so if the input

00:03:13.840 --> 00:03:13.850
the units and that layer so if the input
 

00:03:13.850 --> 00:03:17.080
the units and that layer so if the input
features of activations are roughly mean

00:03:17.080 --> 00:03:17.090
features of activations are roughly mean
 

00:03:17.090 --> 00:03:19.990
features of activations are roughly mean
0 and standard variants and the earns 1

00:03:19.990 --> 00:03:20.000
0 and standard variants and the earns 1
 

00:03:20.000 --> 00:03:22.900
0 and standard variants and the earns 1
then this will cause thee to also be

00:03:22.900 --> 00:03:22.910
then this will cause thee to also be
 

00:03:22.910 --> 00:03:25.720
then this will cause thee to also be
take on a similar scale and this doesn't

00:03:25.720 --> 00:03:25.730
take on a similar scale and this doesn't
 

00:03:25.730 --> 00:03:29.520
take on a similar scale and this doesn't
solve but it definitely helps reduce the

00:03:29.520 --> 00:03:29.530
solve but it definitely helps reduce the
 

00:03:29.530 --> 00:03:31.900
solve but it definitely helps reduce the
vanishing exploding gradients problem

00:03:31.900 --> 00:03:31.910
vanishing exploding gradients problem
 

00:03:31.910 --> 00:03:33.310
vanishing exploding gradients problem
because those trying to set each of the

00:03:33.310 --> 00:03:33.320
because those trying to set each of the
 

00:03:33.320 --> 00:03:35.890
because those trying to set each of the
weight matrix W you know so that is not

00:03:35.890 --> 00:03:35.900
weight matrix W you know so that is not
 

00:03:35.900 --> 00:03:37.900
weight matrix W you know so that is not
too much bigger than 1 and not too much

00:03:37.900 --> 00:03:37.910
too much bigger than 1 and not too much
 

00:03:37.910 --> 00:03:40.900
too much bigger than 1 and not too much
less than 1 so it doesn't explode or

00:03:40.900 --> 00:03:40.910
less than 1 so it doesn't explode or
 

00:03:40.910 --> 00:03:43.360
less than 1 so it doesn't explode or
vanish too quickly I'll just mention

00:03:43.360 --> 00:03:43.370
vanish too quickly I'll just mention
 

00:03:43.370 --> 00:03:45.670
vanish too quickly I'll just mention
some other variants the variation we

00:03:45.670 --> 00:03:45.680
some other variants the variation we
 

00:03:45.680 --> 00:03:48.190
some other variants the variation we
just described is assuming a rare Lu

00:03:48.190 --> 00:03:48.200
just described is assuming a rare Lu
 

00:03:48.200 --> 00:03:50.800
just described is assuming a rare Lu
activation function and it's by a paper

00:03:50.800 --> 00:03:50.810
activation function and it's by a paper
 

00:03:50.810 --> 00:03:53.830
activation function and it's by a paper
by her at all a few other variants if

00:03:53.830 --> 00:03:53.840
by her at all a few other variants if
 

00:03:53.840 --> 00:03:56.800
by her at all a few other variants if
you are using a ten-inch activation

00:03:56.800 --> 00:03:56.810
you are using a ten-inch activation
 

00:03:56.810 --> 00:03:59.650
you are using a ten-inch activation
function then there's a paper that shows

00:03:59.650 --> 00:03:59.660
function then there's a paper that shows
 

00:03:59.660 --> 00:04:01.480
function then there's a paper that shows
that instead of using the constant to

00:04:01.480 --> 00:04:01.490
that instead of using the constant to
 

00:04:01.490 --> 00:04:04.690
that instead of using the constant to
steadily use the constant 1 and so 1

00:04:04.690 --> 00:04:04.700
steadily use the constant 1 and so 1
 

00:04:04.700 --> 00:04:08.590
steadily use the constant 1 and so 1
over this instead of 2 so you multiply

00:04:08.590 --> 00:04:08.600
over this instead of 2 so you multiply
 

00:04:08.600 --> 00:04:13.030
over this instead of 2 so you multiply
by the square root of this so this

00:04:13.030 --> 00:04:13.040
by the square root of this so this
 

00:04:13.040 --> 00:04:16.699
by the square root of this so this
square root term would replace

00:04:16.699 --> 00:04:16.709
square root term would replace
 

00:04:16.709 --> 00:04:20.180
square root term would replace
this term and you use this if you're

00:04:20.180 --> 00:04:20.190
this term and you use this if you're
 

00:04:20.190 --> 00:04:23.330
this term and you use this if you're
using a damaged activation function this

00:04:23.330 --> 00:04:23.340
using a damaged activation function this
 

00:04:23.340 --> 00:04:26.499
using a damaged activation function this
is called savior initialization and

00:04:26.499 --> 00:04:26.509
is called savior initialization and
 

00:04:26.509 --> 00:04:29.320
is called savior initialization and
another version work cellobiose

00:04:29.320 --> 00:04:29.330
another version work cellobiose
 

00:04:29.330 --> 00:04:31.339
another version work cellobiose
evangelist colleagues we might see in

00:04:31.339 --> 00:04:31.349
evangelist colleagues we might see in
 

00:04:31.349 --> 00:04:35.740
evangelist colleagues we might see in
some papers but is to use this formula

00:04:35.740 --> 00:04:35.750
some papers but is to use this formula
 

00:04:35.750 --> 00:04:38.839
some papers but is to use this formula
which has some other server code

00:04:38.839 --> 00:04:38.849
which has some other server code
 

00:04:38.849 --> 00:04:41.779
which has some other server code
justification but I would say if you're

00:04:41.779 --> 00:04:41.789
justification but I would say if you're
 

00:04:41.789 --> 00:04:43.700
justification but I would say if you're
using a regular activation function

00:04:43.700 --> 00:04:43.710
using a regular activation function
 

00:04:43.710 --> 00:04:44.890
using a regular activation function
which is really the most common

00:04:44.890 --> 00:04:44.900
which is really the most common
 

00:04:44.900 --> 00:04:47.480
which is really the most common
activation function I would use this

00:04:47.480 --> 00:04:47.490
activation function I would use this
 

00:04:47.490 --> 00:04:50.360
activation function I would use this
formula if you're using tannish you

00:04:50.360 --> 00:04:50.370
formula if you're using tannish you
 

00:04:50.370 --> 00:04:52.850
formula if you're using tannish you
could try this version instead and some

00:04:52.850 --> 00:04:52.860
could try this version instead and some
 

00:04:52.860 --> 00:04:55.159
could try this version instead and some
motors will also use this but in

00:04:55.159 --> 00:04:55.169
motors will also use this but in
 

00:04:55.169 --> 00:04:57.350
motors will also use this but in
practice I think all of these formulas

00:04:57.350 --> 00:04:57.360
practice I think all of these formulas
 

00:04:57.360 --> 00:04:58.879
practice I think all of these formulas
just give you a starting point which is

00:04:58.879 --> 00:04:58.889
just give you a starting point which is
 

00:04:58.889 --> 00:05:00.529
just give you a starting point which is
your default value to use for the

00:05:00.529 --> 00:05:00.539
your default value to use for the
 

00:05:00.539 --> 00:05:02.839
your default value to use for the
variant of the initialization of your

00:05:02.839 --> 00:05:02.849
variant of the initialization of your
 

00:05:02.849 --> 00:05:06.200
variant of the initialization of your
weight matrices if you wish the variants

00:05:06.200 --> 00:05:06.210
weight matrices if you wish the variants
 

00:05:06.210 --> 00:05:08.420
weight matrices if you wish the variants
here this variance parameter could be

00:05:08.420 --> 00:05:08.430
here this variance parameter could be
 

00:05:08.430 --> 00:05:10.820
here this variance parameter could be
another thing that you could - or your

00:05:10.820 --> 00:05:10.830
another thing that you could - or your
 

00:05:10.830 --> 00:05:13.730
another thing that you could - or your
hyper parameters so you can have another

00:05:13.730 --> 00:05:13.740
hyper parameters so you can have another
 

00:05:13.740 --> 00:05:15.469
hyper parameters so you can have another
parameter that multiplies into this

00:05:15.469 --> 00:05:15.479
parameter that multiplies into this
 

00:05:15.479 --> 00:05:19.159
parameter that multiplies into this
formula and tune that multiplier as tall

00:05:19.159 --> 00:05:19.169
formula and tune that multiplier as tall
 

00:05:19.169 --> 00:05:21.620
formula and tune that multiplier as tall
your hyper parameter search sometimes

00:05:21.620 --> 00:05:21.630
your hyper parameter search sometimes
 

00:05:21.630 --> 00:05:23.680
your hyper parameter search sometimes
the tuning the hyper parameter has a

00:05:23.680 --> 00:05:23.690
the tuning the hyper parameter has a
 

00:05:23.690 --> 00:05:25.520
the tuning the hyper parameter has a
modern sized effect

00:05:25.520 --> 00:05:25.530
modern sized effect
 

00:05:25.530 --> 00:05:27.560
modern sized effect
it's not one of the first type of

00:05:27.560 --> 00:05:27.570
it's not one of the first type of
 

00:05:27.570 --> 00:05:30.140
it's not one of the first type of
parameters I would usually try to tune

00:05:30.140 --> 00:05:30.150
parameters I would usually try to tune
 

00:05:30.150 --> 00:05:32.570
parameters I would usually try to tune
but I've also seen some problems where

00:05:32.570 --> 00:05:32.580
but I've also seen some problems where
 

00:05:32.580 --> 00:05:34.370
but I've also seen some problems where
tuning this you know helps have some

00:05:34.370 --> 00:05:34.380
tuning this you know helps have some
 

00:05:34.380 --> 00:05:36.499
tuning this you know helps have some
usable amount but this is usually lower

00:05:36.499 --> 00:05:36.509
usable amount but this is usually lower
 

00:05:36.509 --> 00:05:39.409
usable amount but this is usually lower
down for me in terms of how important it

00:05:39.409 --> 00:05:39.419
down for me in terms of how important it
 

00:05:39.419 --> 00:05:41.390
down for me in terms of how important it
is relative to the other high

00:05:41.390 --> 00:05:41.400
is relative to the other high
 

00:05:41.400 --> 00:05:43.700
is relative to the other high
preferences you can tune so I hope that

00:05:43.700 --> 00:05:43.710
preferences you can tune so I hope that
 

00:05:43.710 --> 00:05:45.830
preferences you can tune so I hope that
gives you some intuition about the

00:05:45.830 --> 00:05:45.840
gives you some intuition about the
 

00:05:45.840 --> 00:05:47.390
gives you some intuition about the
problem of vanishing or exploding

00:05:47.390 --> 00:05:47.400
problem of vanishing or exploding
 

00:05:47.400 --> 00:05:49.570
problem of vanishing or exploding
gradients as well as how choosing a

00:05:49.570 --> 00:05:49.580
gradients as well as how choosing a
 

00:05:49.580 --> 00:05:51.529
gradients as well as how choosing a
reasonable scaling for how you

00:05:51.529 --> 00:05:51.539
reasonable scaling for how you
 

00:05:51.539 --> 00:05:53.420
reasonable scaling for how you
initialize the weight hopefully that

00:05:53.420 --> 00:05:53.430
initialize the weight hopefully that
 

00:05:53.430 --> 00:05:55.550
initialize the weight hopefully that
makes your weight your not explode too

00:05:55.550 --> 00:05:55.560
makes your weight your not explode too
 

00:05:55.560 --> 00:05:58.459
makes your weight your not explode too
quickly and not be k20 too quickly so

00:05:58.459 --> 00:05:58.469
quickly and not be k20 too quickly so
 

00:05:58.469 --> 00:06:01.159
quickly and not be k20 too quickly so
you can train a reasonably deep network

00:06:01.159 --> 00:06:01.169
you can train a reasonably deep network
 

00:06:01.169 --> 00:06:03.350
you can train a reasonably deep network
without the weights or the gradients

00:06:03.350 --> 00:06:03.360
without the weights or the gradients
 

00:06:03.360 --> 00:06:06.110
without the weights or the gradients
exploding or vanishing too much when you

00:06:06.110 --> 00:06:06.120
exploding or vanishing too much when you
 

00:06:06.120 --> 00:06:07.640
exploding or vanishing too much when you
train deep networks this is another

00:06:07.640 --> 00:06:07.650
train deep networks this is another
 

00:06:07.650 --> 00:06:09.770
train deep networks this is another
trick that will help you make your

00:06:09.770 --> 00:06:09.780
trick that will help you make your
 

00:06:09.780 --> 00:06:13.490
trick that will help you make your
neural networks train much more quickly

