WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:01.090
We're in a grid world.

00:00:01.090 --> 00:00:02.050
&gt;&gt; Okay.
&gt;&gt; And

00:00:02.050 --> 00:00:04.430
you have to learn a reward function.

00:00:04.430 --> 00:00:08.080
You have to learn what's good or
not good to do in this grid, and

00:00:08.080 --> 00:00:09.510
you're going to get to
learn it by watching me.

00:00:09.510 --> 00:00:11.740
So I'm going to control the agent.

00:00:11.740 --> 00:00:13.000
The agent's name is Frank.

00:00:13.000 --> 00:00:17.635
&gt;&gt; Okay.
&gt;&gt; So he's going to go like this [SOUND]

00:00:17.635 --> 00:00:21.108
and then he stays there.

00:00:21.108 --> 00:00:21.780
Okay?

00:00:21.780 --> 00:00:22.360
&gt;&gt; Okay.
&gt;&gt; So

00:00:22.360 --> 00:00:26.860
I claim that you can actually make some
very good educated guesses about how

00:00:26.860 --> 00:00:30.650
rewards work in this environment, just
based on that one little demonstration.

00:00:30.650 --> 00:00:33.140
&gt;&gt; I think that's probably true.

00:00:33.140 --> 00:00:36.640
&gt;&gt; So is there anything that
the agent finds positively rewarding?

00:00:36.640 --> 00:00:37.470
&gt;&gt; Green.
&gt;&gt; Green.

00:00:37.470 --> 00:00:38.410
Yes, good.

00:00:38.410 --> 00:00:42.340
Is there anything that
the agent finds aversive?

00:00:42.340 --> 00:00:47.210
&gt;&gt; So, I could guess that blue is
aversive because you avoided the blue,

00:00:47.210 --> 00:00:51.050
but that also happened to be one of
the shortest path to get to the green.

00:00:51.050 --> 00:00:52.600
Yeah, that's true, that's true.

00:00:52.600 --> 00:00:55.120
So it might have just been
a coincidence, but I think you have some

00:00:55.120 --> 00:00:58.060
evidence that maybe blue was
something I was trying to avoid.

00:00:58.060 --> 00:00:59.100
&gt;&gt; Right.
When I-

00:00:59.100 --> 00:01:00.080
&gt;&gt; What about red?

00:01:00.080 --> 00:01:03.330
&gt;&gt; So red is not something you're
trying to avoid because you

00:01:03.330 --> 00:01:04.440
could have avoided it.

00:01:04.440 --> 00:01:05.440
&gt;&gt; That's right.
&gt;&gt; I don't know about

00:01:05.440 --> 00:01:07.490
orange because you couldn't
avoid orange to get the green.

00:01:07.490 --> 00:01:09.360
&gt;&gt; Excellent, and that was
exactly the inferences that I was

00:01:09.360 --> 00:01:10.050
hoping you could make.

00:01:10.050 --> 00:01:13.490
So, again, you can learn a tremendous
amount about what's valuable and

00:01:13.490 --> 00:01:16.790
what's not valuable by, not just the
path that was taken, but, in a sense,

00:01:16.790 --> 00:01:18.450
by the paths that were not taken.

00:01:18.450 --> 00:01:20.610
And what the other options are or

00:01:20.610 --> 00:01:22.950
were and
what the other options weren't, right?

00:01:22.950 --> 00:01:24.250
So you couldn't learn
anything about orange.

00:01:24.250 --> 00:01:27.470
It could be that he hates orange,
not as much he likes green.

00:01:27.470 --> 00:01:30.510
So Frank was willing to go through
orange and it didn't matter where.

00:01:30.510 --> 00:01:33.560
But red, yeah, totally could've
gone around red and didn't bother.

00:01:33.560 --> 00:01:35.090
Right?
So that you can avoid blue and

00:01:35.090 --> 00:01:38.130
red just by going like that,
but he didn't do that, so

00:01:38.130 --> 00:01:40.370
he probably is indifferent to red or
something.

00:01:40.370 --> 00:01:44.190
So, yes, it's actually a really cool
idea and we can actually learn things

00:01:44.190 --> 00:01:46.930
about what people want to do and
what they don't want to do.

00:01:46.930 --> 00:01:52.030
How tolerant are they of like sudden
stops in a car or a bumpy road?

00:01:52.030 --> 00:01:53.010
That sort of thing.

00:01:53.010 --> 00:01:55.900
How do you trade that off against
how fast you're trying to

00:01:55.900 --> 00:01:56.790
get to your destination?

00:01:57.800 --> 00:01:59.630
It's hard to get people to
tell you numbers for that,

00:01:59.630 --> 00:02:02.490
but it is actually not so
hard to have them demonstrate,

00:02:02.490 --> 00:02:05.720
and then infer these kinds of
values from their behavior.

00:02:05.720 --> 00:02:07.640
So, obviously, as we're sitting here,

00:02:07.640 --> 00:02:10.520
we can't really get into
the details of the algorithm.

00:02:10.520 --> 00:02:13.630
But just to give you a kind of
a hint of the form of this,

00:02:13.630 --> 00:02:16.460
there's an algorithm
that I worked on called

00:02:16.460 --> 00:02:18.530
Maximum Likelihood Inverse
Reinforcement Learning, but

00:02:18.530 --> 00:02:20.250
there's a whole bunch of
different algorithms.

00:02:20.250 --> 00:02:25.980
This one I happen to understand
the best because I helped make it up.

00:02:25.980 --> 00:02:29.090
And the basic game is here,
guess a reward function.

00:02:29.090 --> 00:02:33.200
In this case, we were guessing reward
functions that mapped colors to values.

00:02:33.200 --> 00:02:37.560
We then compute an optimal policy
with that reward function in mind.

00:02:37.560 --> 00:02:41.040
Measure the probability of the data of
the behavioral trajectories that we saw

00:02:41.040 --> 00:02:45.830
given that policy and then compute
a gradient on how can the value of

00:02:45.830 --> 00:02:49.950
the reward change in a way that
would make the data more likely?

00:02:49.950 --> 00:02:51.690
And then we just go back and

00:02:51.690 --> 00:02:56.194
do that again until we find
something that is a local maximum.

00:02:56.194 --> 00:02:58.360
&gt;&gt; That makes sense,
how does it work in practice?

00:02:58.360 --> 00:02:59.210
&gt;&gt; It seems to work pretty well.

00:02:59.210 --> 00:03:01.620
It can be slow and
it can get stuck in local minima, but

00:03:01.620 --> 00:03:03.690
we haven't really had
significant problems with that.

00:03:03.690 --> 00:03:06.210
It actually pretty consistently comes
up with a reward function that's

00:03:06.210 --> 00:03:07.060
pretty solid.

00:03:07.060 --> 00:03:09.800
There's tricky things like
keeping in mind the fact that,

00:03:09.800 --> 00:03:14.110
when I showed you the trajectory, I went
through green and then I stayed there.

00:03:14.110 --> 00:03:15.980
So how do we represent that?

00:03:15.980 --> 00:03:17.240
Is this an absorbing state?

00:03:17.240 --> 00:03:20.790
And if so, do I just include
it once in my trajectory?

00:03:20.790 --> 00:03:23.300
In which case, it just saw one little
green, in which case it's not so

00:03:23.300 --> 00:03:24.540
excited about green.

00:03:24.540 --> 00:03:26.510
Or is it really important and
it kind of sat there forever?

00:03:26.510 --> 00:03:28.810
In which case it really didn't
see much of anything else.

00:03:28.810 --> 00:03:31.830
So getting getting those terminal
states right can be a bit of a trick.

00:03:31.830 --> 00:03:33.730
&gt;&gt; That makes sense, okay, I like that.

