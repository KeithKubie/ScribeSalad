Speaker 1:          00:00          The real question is like who gets to decide what is offensive, what is not? I'm sure you're aware of the learn to code fiasco. Uh Oh I her for saying learn to code and it was really mocking this idea that people were telling coal miners who are losing their jobs, you know, hey there's jobs in computer programming and like let this out. You should learn to code. And so people started mocking people by saying learn to code and then learn to code. Apparently according to Jack Dorsey and Vigia, it got connected to a antisemitic remarks and hate remarks and [inaudible] level of Internet. This is data, that nonsense. It's fascinating because that doesn't mean anything like learn to code is not offensive. It's like, well it's ridiculous to ask a 50 year old man as a coal miner to learn to code and it doesn't have a formal education. That is ridiculous. But I mean the fact that you get banned for life from saying that, that's actually even more ridiculous.

Speaker 2:          00:57          Yeah. But then there are cases where people go and they try to create like a, if someone, if someone who look there are antisemites out there, right? They do try to come up with like ways to indicate antisemitism to each other that other people won't detect. You know what I mean? Via, you know, slang, basically inner slang. Right. Um, and uh, at some point someone needs to be able to say, okay, wait, hold on second, this is, we figured out this 87 x slang, so we're not gonna allow you to say, you know what I mean? It's like shit, like 88 or what? Isn't that a thing where it's like, yeah,

Speaker 1:          01:27          doubt. Yeah, age, age. This is though Hitler

Speaker 2:          01:29          the problem, right? This is the, this is the exact problem that you're talking about. Um, so, but the contrary, the contradiction that all of these platforms have, right, is the early days of the Internet, remember the early days, it was like people were really concerned that people would start suing websites because of what was on the website, right? Like, um, like the pirate bay, the big torrent site, you know, like you're gonna get sued because you've got DVD screeners on their, no, hold on a second. We're just, this is just where people can upload the shit. You know, Google getting sued because they would, you know, direct someone in the DVDs, Koreans, they searched for leaks, DVD screener, you know what I mean? And so that was a big concern in the early days of the Internet, right? And so we established this precedent, they're like, no, no, no. These sites don't have responsibility for that right there just how people are connecting to things.

Speaker 2:          02:14          They're not the people doing the bad shit, right? You go up to the people doing the batch, not the people who made it possible to find the bad shit. Right now though, we are in such a place where, so all these businesses built themselves on the idea of youtube, right? We don't make anything at Youtube. We just give you a place to upload your videos, right? So at first that's fine. All right, just take down the antisemitic, you know, white supremacy videos, right? Whatever. Right? But now there's so, so many of them. Right? And also not only that, Youtube's algorithm is directing people towards them and youtube is selling ads against them and making money at them. Right? Um, and at the same time, like, you know, these, these videos exist, right? Um, uh, and uh, at the same time they're still trying to say, well, we have no responsibility for that happening.

Speaker 2:          02:56          It's like, hold on a second. You guys built a system where any kind of content is allowed and you are, you, you've also built the sentence and that's directing people to that content and you build a system that's making money off of that content. I think you guys have a little bit of responsibility. Now. I agree that the question of who polices it or whatever, that's an extremely complicated conversation. But like that's what I'm just saying about these companies trying to have it both ways. They're trying to say we have no responsibility for what's on the platform, but also we've allowed this kind of content to go up. You know, I don't know if you to profits on antisemitic videos. I don't know. Having a test, I talk about it in my show. Demonetizing aspect of a youtube that affects people whenever anything's even remotely controversial, controversial.

Speaker 2:          03:42          They've started, they've started doing that, um, up until like, there's a case to talk about in my show, um, uh, like a year, year or two ago where they were like running under Armour brand under Armour ads on like white supremacy, youtube videos, you know what I mean? Yeah. And that, that was like, and their algorithm was causing this algorithm was causing that to happen. Right? And these are videos getting a hundred thousands of hits and it was like major brands and those brands found out, oh wow. And this is the title of these video. Like what kind of get on with, they say obvious antisemitic things in these were obvious enough that anybody would be pissed off about him. Yeah. Um, and uh, so that's where the demonetization thing came from, right? Because they're like, oh shit. Now the people who actually pay us, the advertisers are pissed off, right?

Speaker 2:          04:22          Yeah. So, okay, let's put a bandaid on the problem and let's demonetize videos. Right. Here's the problem. Now they're doing that. They're just doing that algorithmically, right? They're choosing which videos to demonetize monetize algorithmically. So sometimes they demonetize stuff that they shouldn't. That's definitely happened a ton and a lot of shit is still getting through the cracks. You know? And again, they're saying, okay, we did what we did, what we had to know. You guys didn't solve the fucking problem because now people are pissed off again. Right? So that is the fine that these companies are in there based on this premise of we don't moderate, moderate anything. But when you do that, a lot of shit comes in and now you're in the position where it, sorry, it's still your house. The Shit's happened in your house. You went through the House party dude, like, like the base got broken.

Speaker 2:          05:01          It's your fault. At the end of the day, you have to take some responsibility for it. And like, well, how am I supposed to police 200 kids? I don't know. You're the one who threw the party. Yeah. You know, that's a good analogy because it's the scale that's the problem. It's probably more like 200,000 kids in a house. Totally. Because it's theirs. It's unmanageable. Yeah. When you think about it, how many different people are on youtube and how many different countries are uploading videos and you know, some of them are isis beheading videos. Yeah. Those, those, like there's a bunch of cartel videos that people have sent me to on Youtube and they stay up for a little while. Yeah. You get to watch some horrible shit for a little while before they catch on.