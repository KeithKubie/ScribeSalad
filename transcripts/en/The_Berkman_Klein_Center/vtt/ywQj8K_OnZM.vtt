WEBVTT
Kind: captions
Language: en

00:00:06.700 --> 00:00:09.220
You know increasingly we're using

00:00:09.230 --> 00:00:13.790
automated technology in ways that kind

00:00:13.790 --> 00:00:16.070
of support humans and what they're doing

00:00:16.070 --> 00:00:18.740
rather than just having algorithms work

00:00:18.740 --> 00:00:20.210
on their own because they're not smart

00:00:20.210 --> 00:00:22.130
enough to do that yet or deal with

00:00:22.130 --> 00:00:25.189
unexpected situations. So we're

00:00:25.189 --> 00:00:27.110
interacting more and more with these

00:00:27.110 --> 00:00:29.180
systems and so you need an interface

00:00:29.180 --> 00:00:31.970
that makes sense for a human to interact

00:00:31.970 --> 00:00:37.399
with. And so a lot of these systems have an interface that is personified and

00:00:37.399 --> 00:00:38.899
that will talk to you.

00:00:38.899 --> 00:00:43.850
The effect that I'm so interested in the psychological effect of us treating

00:00:43.850 --> 00:00:45.980
these systems like social actors is

00:00:45.980 --> 00:00:49.760
actually - we achieved that in the 60s.

00:00:49.760 --> 00:00:51.649
Like in the 60s we had this chatbot

00:00:51.649 --> 00:00:53.929
called Eliza that Joseph Weizenbaum

00:00:53.929 --> 00:00:56.059
made. It's a really famous example

00:00:56.059 --> 00:00:58.370
of a really simple chatbot all it did

00:00:58.370 --> 00:01:00.499
was answer everything with a question

00:01:00.499 --> 00:01:03.199
like a psychoanalyst would. So if you're

00:01:03.199 --> 00:01:05.420
like "oh, I don't like my mom" it would be

00:01:05.420 --> 00:01:07.220
like "why don't you like your mom." And

00:01:07.220 --> 00:01:09.590
people would just open up and tell it

00:01:09.590 --> 00:01:11.390
all sorts of things, even though it was

00:01:11.390 --> 00:01:14.119
very primitive and how it behaved. So

00:01:14.119 --> 00:01:17.479
it's useful that we will engage with

00:01:17.479 --> 00:01:19.430
these systems on a social level because

00:01:19.430 --> 00:01:21.740
it's engaging for people and you can get

00:01:21.740 --> 00:01:23.600
people to actually use the systems more.

00:01:23.600 --> 00:01:27.590
But I do wonder whether there's any

00:01:27.590 --> 00:01:31.070
effect on our behavior. On our long-term

00:01:31.070 --> 00:01:34.729
behavioral development. It really gets

00:01:34.729 --> 00:01:42.259
interesting when we start talking about kids. You have these systems like Siri, and Alexa that kids are interacting with and

00:01:42.259 --> 00:01:44.869
if these systems are simulating lifelike

00:01:44.869 --> 00:01:48.049
behavior or a real conversation then

00:01:48.049 --> 00:01:50.659
that could actually influence kids'

00:01:50.659 --> 00:01:53.539
behavioral development and the way that

00:01:53.539 --> 00:01:57.859
they start to converse with other people.

00:01:57.859 --> 00:02:00.320
There's some examples of this just

00:02:00.320 --> 00:02:02.869
anecdotally that we have so far like

00:02:02.869 --> 00:02:04.969
there was an article in New York Times

00:02:04.969 --> 00:02:08.869
a few years ago about this kid, this autistic

00:02:08.869 --> 00:02:10.550
boy who developed a relationship with

00:02:10.550 --> 00:02:16.700
Siri. And the mom was like "this is the best thing ever. Siri is infinitely patient

00:02:16.700 --> 00:02:19.160
will answer all of his questions. Also

00:02:19.160 --> 00:02:20.390
the voice recognition

00:02:20.390 --> 00:02:21.860
is so shitty that he had to learn to

00:02:21.860 --> 00:02:24.560
articulate his words really well and it's

00:02:24.560 --> 00:02:26.630
made him a better communicator with

00:02:26.630 --> 00:02:30.620
other people because of Siri." But then on

00:02:30.620 --> 00:02:32.060
the other hand you have stories like

00:02:32.060 --> 00:02:35.090
this one guy wrote an article a few

00:02:35.090 --> 00:02:37.400
months ago about how Alexa was turning

00:02:37.400 --> 00:02:39.860
his kids into assholes because she

00:02:39.860 --> 00:02:42.860
doesn't require any please or thank you

00:02:42.860 --> 00:02:44.720
or any of the standard politeness that

00:02:44.720 --> 00:02:46.160
you want your kids to learn in

00:02:46.160 --> 00:02:49.370
conversing with others. So you know I

00:02:49.370 --> 00:02:51.620
think it is an interesting question that

00:02:51.620 --> 00:02:53.930
the more we're able to simulate like a

00:02:53.930 --> 00:02:57.200
real conversation with these systems the

00:02:57.200 --> 00:02:58.610
more that might get muddled in our

00:02:58.610 --> 00:03:00.680
subconscious in some way.

00:03:00.680 --> 00:03:03.890
One of the things that I think is

00:03:03.890 --> 00:03:06.320
really necessary is that we need to

00:03:06.320 --> 00:03:09.680
study more what impact this can have on

00:03:09.680 --> 00:03:12.769
our behavior. So we definitely need to be

00:03:12.769 --> 00:03:15.320
studying these interactions and studying

00:03:15.320 --> 00:03:18.260
the effects of these interactions.

00:03:18.260 --> 00:03:21.590
It's a similar question to violent

00:03:21.590 --> 00:03:24.260
video games or pornography. These

00:03:24.260 --> 00:03:25.370
are questions that have come up again

00:03:25.370 --> 00:03:26.959
and again but i think that these systems

00:03:26.959 --> 00:03:29.600
bring them to a new more visceral level

00:03:29.600 --> 00:03:32.750
in our psychology. So we need more

00:03:32.750 --> 00:03:35.180
research and then depending on what the

00:03:35.180 --> 00:03:38.299
answer to that specific question is we

00:03:38.299 --> 00:03:42.290
might need to think about design and use

00:03:42.290 --> 00:03:44.840
and maybe even policy for regulating

00:03:44.840 --> 00:03:47.060
these agents. Part of the problem is also

00:03:47.060 --> 00:03:48.260
that this is such an

00:03:48.260 --> 00:03:52.040
interdisciplinary problem. All of the

00:03:52.040 --> 00:03:53.750
questions that are coming up are so

00:03:53.750 --> 00:03:56.180
interdisciplinary you need technologists

00:03:56.180 --> 00:04:00.950
you need policymakers you need users you

00:04:00.950 --> 00:04:03.590
need researchers who understand

00:04:03.590 --> 00:04:05.930
psychology, who understand technology who

00:04:05.930 --> 00:04:09.079
understand sociology like all of this is

00:04:09.079 --> 00:04:12.230
coming together. And you need

00:04:12.230 --> 00:04:14.180
people talking to each other. I would

00:04:14.180 --> 00:04:17.600
really like us to in the next few years to

00:04:17.600 --> 00:04:19.850
lean into the positive effects of the

00:04:19.850 --> 00:04:22.700
technology and develop structures the

00:04:22.700 --> 00:04:24.650
way that we have in other fields like

00:04:24.650 --> 00:04:28.729
medicine to ensure that we're using the

00:04:28.729 --> 00:04:42.930
technology and responsible ethical way.

