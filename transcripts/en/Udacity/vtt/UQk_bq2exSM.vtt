WEBVTT
Kind: captions
Language: en

00:00:00.320 --> 00:00:04.620
So this brings up the issue of what neural nets

00:00:04.620 --> 00:00:08.420
are more or less appropriate for. What is the restriction bias,

00:00:08.420 --> 00:00:14.420
and the inductive bias of this class of classifiers, and

00:00:14.420 --> 00:00:18.800
regression algorithms? So Charles, can you remind us what restriction bias is?

00:00:18.800 --> 00:00:22.290
&gt;&gt; Well, restriction bias Tells you something

00:00:22.290 --> 00:00:25.450
about the representational power of whatever data structure

00:00:25.450 --> 00:00:29.490
it is that you're using. So in this case the network of neurons.

00:00:29.490 --> 00:00:33.350
And it tells you the set of hypotheses that you're willing to consider.

00:00:33.350 --> 00:00:38.750
&gt;&gt; Right, so if, if the, if there's a great deal of restriction,

00:00:38.750 --> 00:00:41.530
then there's lots and lots of different kinds of models that we're just

00:00:41.530 --> 00:00:44.680
not even considering. We're, we're restricting our view to just a subset of

00:00:44.680 --> 00:00:49.400
those. So In the case of neural nets, what restrictions are we putting?

00:00:49.400 --> 00:00:51.150
&gt;&gt; Well,

00:00:51.150 --> 00:00:54.320
we started out with a simple perceptron unit, and that

00:00:54.320 --> 00:00:59.040
we decided was linear. So we were only considering planes. Then

00:00:59.040 --> 00:01:00.940
we move to networks, so that we could do things

00:01:00.940 --> 00:01:04.280
like exor, and that allowed us to do more. Then we

00:01:04.280 --> 00:01:09.450
started sticking Sigmoids and other arbitrary functions and to nodes

00:01:09.450 --> 00:01:12.380
so that we could represent more and more, and you mention

00:01:12.380 --> 00:01:14.100
that if you let weights get big and we have

00:01:14.100 --> 00:01:16.700
lots of layers and lots of nodes, we can be really,

00:01:16.700 --> 00:01:21.410
really complex. So, it seems to me that we

00:01:21.410 --> 00:01:23.270
are actually not doing much of a restriction at

00:01:23.270 --> 00:01:27.320
all. So let me ask you this then Michael.

00:01:27.320 --> 00:01:30.260
What kind of functions can we represent, clearly we can

00:01:30.260 --> 00:01:33.060
represent boolean functions, cause we did that. Can we

00:01:33.060 --> 00:01:37.140
represent continuous functions? That's, that's a great question to

00:01:37.140 --> 00:01:38.470
ask, that's what we should try to figure that

00:01:38.470 --> 00:01:41.970
out. So, in the case, as you said, Boolean functions,

00:01:41.970 --> 00:01:45.110
we can. If we give ourselves a complex enough

00:01:45.110 --> 00:01:48.500
network with enough units, we can basically map all the

00:01:48.500 --> 00:01:51.890
different sub components of any Boolean expression to threshold

00:01:51.890 --> 00:01:54.910
like units and basically build a circuit that can compute

00:01:54.910 --> 00:01:58.000
whatever Boolean function we want. So that one definitely

00:01:58.000 --> 00:02:01.270
can happen. So what about continuous functions? So what is

00:02:01.270 --> 00:02:04.000
it? What is a continuous function? A continuous function

00:02:04.000 --> 00:02:07.510
is one where, as the input changes the output changes

00:02:08.520 --> 00:02:11.620
somewhat smoothly, right? There's no jumps in the function like that.

00:02:11.620 --> 00:02:15.380
&gt;&gt; Well, there's no discon, there's no discontinuities, that's for sure.

00:02:15.380 --> 00:02:16.610
&gt;&gt; Alright, now if we've got a continuous

00:02:16.610 --> 00:02:19.050
function that we're trying to model with a neural

00:02:19.050 --> 00:02:22.500
network. As long as it's connected, it has

00:02:22.500 --> 00:02:24.970
no, no discontinuous jumps to any place in the

00:02:24.970 --> 00:02:29.640
space, we can do this with just a single hidden layer. As long as we have enough

00:02:29.640 --> 00:02:31.400
hidden units, as long as there's enough units in

00:02:31.400 --> 00:02:33.650
that layer. And, essentially one way to think about

00:02:33.650 --> 00:02:37.530
that is, if we have enough hidden units, each hidden

00:02:37.530 --> 00:02:39.980
unit can worry about one little patch of the function

00:02:39.980 --> 00:02:45.070
that, that it needs to model. And they, the patches

00:02:45.070 --> 00:02:47.380
get set at the hidden. And at the output layer

00:02:47.380 --> 00:02:49.410
they get stitched together. And if you just have that

00:02:49.410 --> 00:02:51.690
one layer you can make any function as long as

00:02:51.690 --> 00:02:55.710
it's continuous. If it's Arbitrary. We can still represent that

00:02:55.710 --> 00:02:58.720
in our neural network. Any mapping from inputs to outputs we

00:02:58.720 --> 00:03:01.910
can represent, even if it's discontinuous, just by adding

00:03:01.910 --> 00:03:04.970
one more hidden layer, so two total hidden layers.

00:03:04.970 --> 00:03:06.960
And that gives us the ability to not just

00:03:06.960 --> 00:03:10.240
stitch these patches at their seams, but also to have

00:03:10.240 --> 00:03:13.340
big jumps between the patches. So in fact, neural

00:03:13.340 --> 00:03:16.170
networks are not very restrictive in terms of their bias

00:03:16.170 --> 00:03:20.100
as long as you have a sufficiently complex network

00:03:20.100 --> 00:03:23.600
structure, right, so maybe multiple hidden layers and multiple units.

00:03:23.600 --> 00:03:23.910
&gt;&gt; So

00:03:23.910 --> 00:03:26.680
that worries me a little bit Michael, because it means that we're almost

00:03:26.680 --> 00:03:28.720
certainly going to overfit, right? We're

00:03:28.720 --> 00:03:31.140
going to have arbitrarily complicated neural networks and

00:03:31.140 --> 00:03:34.800
we can represent anything we want to. Including all of the noise that's

00:03:34.800 --> 00:03:38.210
represented in our training set. So, how are we going to avoid doing that?

00:03:38.210 --> 00:03:41.030
&gt;&gt; Excellent question. So, it seems like there's, there

00:03:41.030 --> 00:03:44.130
is exactly that worry. But, it is the case

00:03:44.130 --> 00:03:46.200
though, that when we train neural networks, we typically

00:03:46.200 --> 00:03:49.400
give them some bounded number of hidden units and

00:03:49.400 --> 00:03:52.520
we give them some bounded number of layers. And so, it's

00:03:52.520 --> 00:03:56.180
not like any fixed network can actually capture any arbitrary function. So

00:03:56.180 --> 00:03:58.700
any fixed network can only capture whatever it can capture, which

00:03:58.700 --> 00:04:02.320
is a smaller set. So going to neural nets in general doesn't

00:04:02.320 --> 00:04:05.730
have much restriction. but any given network architecture actually does have

00:04:05.730 --> 00:04:08.540
a bit more restriction. So that's one thing, the other is hey,

00:04:08.540 --> 00:04:11.510
well we can do with overfitting what we've done the other

00:04:11.510 --> 00:04:14.610
times we've had to deal with overfitting. And that's to use ideas

00:04:14.610 --> 00:04:19.140
like, cross validation. And we used cross validation to

00:04:19.140 --> 00:04:22.720
decide. How many hidden layers to use. We can

00:04:22.720 --> 00:04:27.790
use it to decide how many nodes to put in each layer. And we can also use it

00:04:27.790 --> 00:04:30.820
to decide when to stop training because the weights

00:04:30.820 --> 00:04:33.400
have gotten too large. So, and this is, it's

00:04:33.400 --> 00:04:35.210
probably worth pointing this out that this is kind

00:04:35.210 --> 00:04:39.970
of a different, different property from the. Other classes

00:04:39.970 --> 00:04:42.700
of supervised learning algorithms we've looked at so far. So

00:04:42.700 --> 00:04:44.980
in a decision tree, you build up the decision tree, an

00:04:44.980 --> 00:04:50.140
you may have over fit, but it is what it is.

00:04:50.140 --> 00:04:52.730
In regression, you know, you solve the regression problem, and again

00:04:52.730 --> 00:04:55.900
that may have over fit. What's interesting about neural network training

00:04:55.900 --> 00:04:58.730
is it's this iterative process that you started out running, and

00:04:58.730 --> 00:05:01.980
as it's running, it's actually Errors going down and down and

00:05:01.980 --> 00:05:05.650
down. So, in this standard kind of graph, we get the

00:05:05.650 --> 00:05:09.220
error on the training set dropping as we increase iterations. It's

00:05:09.220 --> 00:05:12.130
doing a better and better job of modeling the training data.

00:05:12.130 --> 00:05:15.250
But, in classic style, if you look at the error in

00:05:15.250 --> 00:05:18.290
the, in some kind of held-out test set, or maybe in a

00:05:18.290 --> 00:05:21.260
cross validation set, you see the error starting out kind of

00:05:21.260 --> 00:05:24.430
high and maybe dropping along with this, and at some point

00:05:24.430 --> 00:05:27.720
It actually turns around and goes the other way. So here,

00:05:27.720 --> 00:05:28.990
even though we're not changing the

00:05:28.990 --> 00:05:30.910
network structure itself, we're just continuing

00:05:30.910 --> 00:05:34.560
to improve our fit, we actually get this, this pattern that

00:05:34.560 --> 00:05:38.950
we've seen before, that the cross validation error can turn around and,

00:05:38.950 --> 00:05:41.270
and at this, you know, at this low point, you might have,

00:05:41.270 --> 00:05:43.780
you might want to just stop training your network there. The more

00:05:43.780 --> 00:05:47.860
you train it, possibly the worse you'll do. And again, that, it's

00:05:47.860 --> 00:05:50.570
reflecting this idea that the complexity of the network is not just

00:05:50.570 --> 00:05:53.280
in the nodes and the layers, but also in the magnetude of

00:05:53.280 --> 00:05:56.040
the weights. Typically what happens in this turnaround point is that some

00:05:56.040 --> 00:05:57.910
weights are actually getting larger and larger

00:05:57.910 --> 00:06:00.920
and larger. So, just wanted to highlight that

00:06:00.920 --> 00:06:03.320
difference between neural net function approximation of what

00:06:03.320 --> 00:06:04.760
we see in some of the other algorithms

