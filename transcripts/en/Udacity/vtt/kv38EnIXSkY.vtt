WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:01.830
Alright Michael. So like I said, we're going

00:00:01.830 --> 00:00:03.240
to spend all this time trying to, to

00:00:03.240 --> 00:00:05.540
unpack this particular equation. And the first thing

00:00:05.540 --> 00:00:06.939
we need to do is we need to come

00:00:06.939 --> 00:00:08.930
up with another form of it that we

00:00:08.930 --> 00:00:11.360
might have some chance of actually understanding of

00:00:11.360 --> 00:00:12.950
actually getting through. So I want to use

00:00:12.950 --> 00:00:15.490
something called Bayes' rule. Do you remember Bayes' rule?

00:00:15.490 --> 00:00:16.400
&gt;&gt; I do.

00:00:16.400 --> 00:00:17.430
&gt;&gt; Okay, what's Bayes' Rule?

00:00:17.430 --> 00:00:18.915
&gt;&gt; The man with the Bayes makes the

00:00:18.915 --> 00:00:20.560
rule. Oh wait, no, that's the golden rule.

00:00:20.560 --> 00:00:21.020
&gt;&gt; That's right, no.

00:00:21.020 --> 00:00:23.950
&gt;&gt; The Bayes Rule, is, it relates, it, I

00:00:23.950 --> 00:00:25.340
don't know. I think of it as just letting

00:00:25.340 --> 00:00:27.650
you switch which thing is on which side of the bar.

00:00:27.650 --> 00:00:28.740
&gt;&gt; Okay, so.

00:00:28.740 --> 00:00:30.990
&gt;&gt; Do you want me to give the whole expression?

00:00:30.990 --> 00:00:32.060
&gt;&gt; Yeah, give me the whole expression.

00:00:32.060 --> 00:00:38.010
&gt;&gt; So if we're going to apply Bayes' Rule to the probability of h given D. We

00:00:38.010 --> 00:00:43.630
can move, turn it around and make it equal to the probably of D given H. And it

00:00:43.630 --> 00:00:45.020
would be great if we could just stop with

00:00:45.020 --> 00:00:46.860
that, but we can't. We have to now kind

00:00:46.860 --> 00:00:48.130
of put them in the same space. So, we

00:00:48.130 --> 00:00:50.420
multiply by the probability of H, and then we

00:00:50.420 --> 00:00:52.740
divide by the probability of D. And sometimes

00:00:52.740 --> 00:00:54.290
that's just a normalization and we don't have

00:00:54.290 --> 00:00:55.960
to worry about it too much. But that's,

00:00:55.960 --> 00:00:57.730
that's the bay, that's Bayes' rule right there.

00:00:57.730 --> 00:01:00.030
&gt;&gt; So this is Bayes' rule. And it actually is

00:01:00.030 --> 00:01:03.800
really easy to derive. It falls it follows directly from

00:01:03.800 --> 00:01:05.570
the chain rule in probability theory. Do you think it's

00:01:05.570 --> 00:01:09.070
worthwhile? Showing people that or just they should just accept it.

00:01:09.070 --> 00:01:11.360
&gt;&gt; Well, I mean, you could just, you might be able to just see it. Just,

00:01:11.360 --> 00:01:12.810
the, the thing on top of the, the

00:01:12.810 --> 00:01:15.710
normalization, the probability of D given h times probability

00:01:15.710 --> 00:01:18.710
of h. That's actually the probability of D and

00:01:18.710 --> 00:01:22.690
h together. Right. So the probability of h times the

00:01:22.690 --> 00:01:24.496
probability of d over h as you say also the

00:01:24.496 --> 00:01:28.577
chain rule basically the definition of conditional probability in conjunctions

00:01:28.577 --> 00:01:30.530
and if you move the probability of d over

00:01:30.530 --> 00:01:32.350
to the left hand side you can see we're really

00:01:32.350 --> 00:01:34.510
just saying the same thing two different ways. It's just

00:01:34.510 --> 00:01:36.710
the probability of h and d. So then we're done.

00:01:36.710 --> 00:01:38.650
&gt;&gt; No, that's right. So I can write down

00:01:38.650 --> 00:01:40.750
what you just said. And use different letters just

00:01:40.750 --> 00:01:42.230
to make it more confusing, so

00:01:42.230 --> 00:01:42.550
&gt;&gt; Oh good.

00:01:42.550 --> 00:01:46.800
&gt;&gt; You can point out that the probability of A and B, by the chain rule, is

00:01:46.800 --> 00:01:50.190
just the probability of A given B, times the

00:01:50.190 --> 00:01:54.880
probability of B. But because order doesn't matter, it's

00:01:54.880 --> 00:02:00.240
also the case that the probability of A and B. Is the probability of b given a

00:02:00.240 --> 00:02:03.490
times the probability of a. And that's just the

00:02:03.490 --> 00:02:05.820
chain rule. And so if these two quantities equal

00:02:05.820 --> 00:02:08.120
to one another's exactly what you say, I could say

00:02:08.120 --> 00:02:12.690
well, the probability of a given b is just the probability

00:02:12.690 --> 00:02:16.200
of b given a times the probability a divided by the

00:02:16.200 --> 00:02:19.550
probability of b. And that's exactly what we have over here.

00:02:19.550 --> 00:02:21.560
&gt;&gt; Good. So now that we've mastered that

00:02:21.560 --> 00:02:24.063
all your Bayes are belong to us. [LAUGH]

00:02:24.063 --> 00:02:26.470
&gt;&gt; How long have you been saying that?

00:02:26.470 --> 00:02:27.870
&gt;&gt; The...just, only about 3 or 4 minutes.

00:02:27.870 --> 00:02:30.960
&gt;&gt; [LAUGH] Fair enough. Okay, so we have Bayes's rule. And what's really nice

00:02:30.960 --> 00:02:33.890
about Bayes's rule is that while it's a very simple thing, it's

00:02:33.890 --> 00:02:37.180
also true. It follows directly from probability theory. But more importantly for

00:02:37.180 --> 00:02:40.770
machine learning, it gives us a handle to talk about. What it

00:02:40.770 --> 00:02:43.090
is we're exactly trying to do when we say we're trying to

00:02:43.090 --> 00:02:46.580
find the most probable hypothesis, given the data. So let's just take

00:02:46.580 --> 00:02:49.380
a moment to think about what all these terms mean. We know

00:02:49.380 --> 00:02:53.170
what this term here means. The, it's just the probability of some

00:02:53.170 --> 00:02:56.560
hypothesis given the data. But what do all these other terms mean?

00:02:56.560 --> 00:03:00.676
I want to start with this term, the probability of

00:03:00.676 --> 00:03:05.600
the data. It's really nothing more than your prior belief of

00:03:05.600 --> 00:03:08.350
seeing some particular set of data. Now, and as you point

00:03:08.350 --> 00:03:10.520
out, Michael, often it just ends up to be a normalizing

00:03:10.520 --> 00:03:13.330
term and typically does not matter, though we'll see a couple

00:03:13.330 --> 00:03:16.930
of cases where it does matter, helps us to, to sort

00:03:16.930 --> 00:03:19.490
of think about a few things. But generally speaking, whatever it

00:03:19.490 --> 00:03:21.560
is Since the only thing that we care about is the

00:03:21.560 --> 00:03:24.440
hypothesis, we're trying to find that, the

00:03:24.440 --> 00:03:26.080
probability of the data doesn't depend on the

00:03:26.080 --> 00:03:29.870
hypothesis, so typically we ignore it, but it's nice to just be clear about what

00:03:29.870 --> 00:03:33.480
it means. The other terms are a bit more interesting. They matter a little bit

00:03:33.480 --> 00:03:36.380
more. This term here, the probability is the

00:03:36.380 --> 00:03:39.010
probability of the data given the hypothesis right?

00:03:39.010 --> 00:03:41.310
&gt;&gt; Mm. Seems like learning backwards.

00:03:41.310 --> 00:03:43.640
&gt;&gt; It does seem like learning backwards but

00:03:43.640 --> 00:03:46.700
what's really nice about this quantity is that unlike

00:03:46.700 --> 00:03:50.150
the other quantity, the probability of the hypothesis given the data, it's

00:03:50.150 --> 00:03:54.250
actually, turns out to be pretty easy to think about the likelihood that

00:03:54.250 --> 00:03:57.000
we would see some data given that we were in a world

00:03:57.000 --> 00:03:59.430
where some hypothesis, h, is true. So there is a little bit of

00:03:59.430 --> 00:04:02.510
subtlety there and I, let me, let me unpack that subtlety a

00:04:02.510 --> 00:04:05.630
little bit. So we've been talking about the data if its sort of

00:04:05.630 --> 00:04:08.520
a thing that is floating out in air, but we know that the

00:04:08.520 --> 00:04:12.500
data is actually our training data. And it's a set of inputs and

00:04:12.500 --> 00:04:14.140
let's just say for the sake of argument we are

00:04:14.140 --> 00:04:17.320
going to do classification learning, it's a set of labels that

00:04:17.320 --> 00:04:20.440
are associated with those inputs. So just to drive the

00:04:20.440 --> 00:04:24.700
point home, I'm going to call those d's, little d's. And

00:04:24.700 --> 00:04:26.170
so our data is made up of a bunch of

00:04:26.170 --> 00:04:30.170
these training examples. And these training examples are whatever input that

00:04:30.170 --> 00:04:33.620
we get coming from a teacher, coming from ourselves, coming

00:04:33.620 --> 00:04:37.690
from nature, coming from somewhere and the associated label that goes

00:04:37.690 --> 00:04:42.700
along with them. So when you talk about the probability of the data given

00:04:42.700 --> 00:04:44.820
the hypothesis, what you're talking about, well,

00:04:44.820 --> 00:04:46.940
what's the likelihood that. Given that I've

00:04:46.940 --> 00:04:51.000
got all of these Xis and given that I'm living in a world where

00:04:51.000 --> 00:04:54.490
this particular hypothesis that I would see

00:04:54.490 --> 00:04:57.320
these particular labels. Does that make sense Michael?

00:04:57.320 --> 00:04:59.760
&gt;&gt; I see. Yeah, so, so I can imagine a

00:04:59.760 --> 00:05:03.240
more complicated kind of notation where, we're, we're kind of accepting

00:05:03.240 --> 00:05:07.070
the Xs as given. But the labels is what we are

00:05:07.070 --> 00:05:13.310
actually saying is something that we want to assigned probability to.

00:05:13.310 --> 00:05:16.438
&gt;&gt; Right so its not really that the x's matter in the sense

00:05:16.438 --> 00:05:21.105
that we are trying to understand those. What really mattes re the labels

00:05:21.105 --> 00:05:25.144
that are associated with them. And we will see an example of that

00:05:25.144 --> 00:05:28.310
in a moment. But I wanted to make sure that you get this subtled.

00:05:28.310 --> 00:05:31.490
&gt;&gt; So in a sense then I guess you're saying that the probability of D given H

00:05:31.490 --> 00:05:33.928
component, or, or quantity, is really like running

00:05:33.928 --> 00:05:36.350
the hypothesis. It's like, It's like labeling the data.

00:05:36.350 --> 00:05:39.040
&gt;&gt; Okay Michael, just to make sure we get this. Let's

00:05:39.040 --> 00:05:43.700
imagine we're in a universe, where the following hypothesis is true. It

00:05:43.700 --> 00:05:48.270
returns true, in exactly the cases where some input number X, is

00:05:48.270 --> 00:05:52.370
greater than or equal to 10 And it returns false otherwise. Okay?

00:05:52.370 --> 00:05:53.160
&gt;&gt; Yup.

00:05:53.160 --> 00:05:54.100
&gt;&gt; Okay. So

00:05:54.100 --> 00:05:57.740
here's a question for you. Let's say that our data was made up of exactly one

00:05:57.740 --> 00:06:03.490
point. And that value set x equal to 7. Okay? What is

00:06:03.490 --> 00:06:09.100
the probability that the label associated with 7. Would be true.

00:06:09.100 --> 00:06:11.800
&gt;&gt; Huh. So you're saying we're in a world

00:06:11.800 --> 00:06:14.370
where h is holding and that the h, h

00:06:14.370 --> 00:06:16.810
is being used to generate labels. So it wouldn't

00:06:16.810 --> 00:06:18.980
do that right? So, the probability ought to be zero.

00:06:18.980 --> 00:06:19.320
&gt;&gt; That's

00:06:19.320 --> 00:06:21.830
exactly right and what's the probability that it would

00:06:21.830 --> 00:06:25.200
be false? 1 minus 0 [LAUGH] which we'll call 1.

00:06:25.200 --> 00:06:26.780
&gt;&gt; Which we'll call 1. That's exactly right.

00:06:26.780 --> 00:06:28.590
So it's, it's just that simple. That, the

00:06:28.590 --> 00:06:30.990
probability of the data given the hypothesis, is

00:06:30.990 --> 00:06:33.150
really about, given a set of x's, what's

00:06:33.150 --> 00:06:35.550
the probability that I would see some particular

00:06:35.550 --> 00:06:38.390
label. Now, what's nice about that is, is,

00:06:38.390 --> 00:06:40.730
as you point out, is that, it's as

00:06:40.730 --> 00:06:44.190
if we're running the hypothesis. Well, given a hypothesis,

00:06:44.190 --> 00:06:47.890
it's really easy, or at least it's easier usually, to compute

00:06:47.890 --> 00:06:50.980
the probability of us seeing some labels. So, this quantity is

00:06:50.980 --> 00:06:54.920
a lot easier to figure out than the original quantity that

00:06:54.920 --> 00:06:57.730
we're looking for. The probability of the hypothesis, given the data.

00:06:57.730 --> 00:07:01.370
&gt;&gt; Yeah, I could see that. It's sort of reminding me a little

00:07:01.370 --> 00:07:06.490
bit of the Version Space, but I can't quite crystallize what the connection is.

00:07:06.490 --> 00:07:09.310
&gt;&gt; Well that's, it's good you bring that up. Because I, I think in a

00:07:09.310 --> 00:07:11.040
couple of seconds I'll give you an example

00:07:11.040 --> 00:07:12.700
that might really help you to see that. Okay?

00:07:12.700 --> 00:07:13.190
&gt;&gt; Okay.

