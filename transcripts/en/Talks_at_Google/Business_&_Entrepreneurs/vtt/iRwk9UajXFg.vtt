WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.904
[MUSIC PLAYING]

00:00:07.747 --> 00:00:08.830
JORGE CUETO: Hi, everyone.

00:00:08.830 --> 00:00:11.070
Welcome to our Talks
at Google event.

00:00:11.070 --> 00:00:13.140
I'm going to go through
some logistics first.

00:00:13.140 --> 00:00:15.440
This is a fire side
chat style talk.

00:00:15.440 --> 00:00:19.240
So it will take about 40 minutes
to talk through some questions.

00:00:19.240 --> 00:00:22.890
And then in the last 20 minutes,
we'll open up to the audience

00:00:22.890 --> 00:00:26.250
to submit questions, both
in person here through a mic

00:00:26.250 --> 00:00:32.340
in the back of the room and also
through the Dory at go/ask-sam.

00:00:32.340 --> 00:00:34.950
And I'd like to thank
everyone who has helped out

00:00:34.950 --> 00:00:37.710
to make this talk possible,
including our facilities

00:00:37.710 --> 00:00:43.580
team and everyone on the PM
speaker series organizing team.

00:00:43.580 --> 00:00:45.920
So it's great to have Sam
Altman here with us today

00:00:45.920 --> 00:00:47.960
for Talks at Google.

00:00:47.960 --> 00:00:50.750
Sam is the President
of Y Combinator, which

00:00:50.750 --> 00:00:53.420
is widely regarded as
one of the top startup

00:00:53.420 --> 00:00:55.700
incubators in Silicon Valley.

00:00:55.700 --> 00:00:58.280
He went to Stanford and
studied computer science

00:00:58.280 --> 00:01:04.730
and was the founder and CEO of
a mobile location-based startup

00:01:04.730 --> 00:01:07.700
called Loopt, which was
funded by YC as part

00:01:07.700 --> 00:01:10.670
of its first class
of startups in 2005

00:01:10.670 --> 00:01:14.180
and acquired by a financial
services company Green

00:01:14.180 --> 00:01:15.950
Dot in 2012.

00:01:15.950 --> 00:01:19.700
In 2014, he was named
president of Y Combinator.

00:01:19.700 --> 00:01:22.550
And since then, he's worked
on a wide range of initiatives

00:01:22.550 --> 00:01:26.150
from YC Research, which is
a non-profit branch of Y

00:01:26.150 --> 00:01:29.090
Combinator that focuses
on doing pure research

00:01:29.090 --> 00:01:32.780
around moon-shot ideas,
like universal basic income.

00:01:32.780 --> 00:01:36.560
And he's also worked on OpenAI,
which is a non-profit AI

00:01:36.560 --> 00:01:39.950
research company looking
into finding ways

00:01:39.950 --> 00:01:43.280
to create safe, friendly
artificial intelligence that

00:01:43.280 --> 00:01:46.590
can actually help
all of humanity.

00:01:46.590 --> 00:01:48.360
And it's great to have you here.

00:01:48.360 --> 00:01:49.800
SAM ALTMAN: Thanks
for having me.

00:01:49.800 --> 00:01:51.300
JORGE CUETO: So
just as I mentioned,

00:01:51.300 --> 00:01:54.470
you're involved in a wide range
of things from YC to OpenAI.

00:01:54.470 --> 00:01:57.800
And most recently, you
released The United Slate

00:01:57.800 --> 00:02:01.040
So I wanted to ask you how
you think about prioritizing

00:02:01.040 --> 00:02:04.078
the projects that you work on.

00:02:04.078 --> 00:02:06.830
SAM ALTMAN: You know, I
think optimal time allocation

00:02:06.830 --> 00:02:10.130
is probably like an
AI complete problem.

00:02:10.130 --> 00:02:12.860
I think if you can get to
spending like 1% of your time

00:02:12.860 --> 00:02:14.960
perfectly, that's really good.

00:02:14.960 --> 00:02:18.440
And so I think this idea of
figuring out what to focus on

00:02:18.440 --> 00:02:24.770
and what not to focus on is
both really hard and still

00:02:24.770 --> 00:02:27.170
significantly under-invested in.

00:02:27.170 --> 00:02:29.230
The frameworks that
I have used, the sort

00:02:29.230 --> 00:02:32.330
of two big frameworks that
I've used to figure out

00:02:32.330 --> 00:02:37.480
how to allocate time, one
is impact maximization

00:02:37.480 --> 00:02:39.850
slash regret minimisation.

00:02:39.850 --> 00:02:42.370
So I try to look at those
two curves together.

00:02:42.370 --> 00:02:45.505
And I try to think about
where I can have the biggest

00:02:45.505 --> 00:02:50.680
net impact on the world, net
positive impact on the world.

00:02:50.680 --> 00:02:54.610
Easy to have a big impact.

00:02:54.610 --> 00:02:56.380
And then, also, just
regret minimization.

00:02:56.380 --> 00:02:58.222
You know, you get to live once.

00:02:58.222 --> 00:03:00.430
It's really important that
you do what you want to do

00:03:00.430 --> 00:03:02.394
and that you spend time
with the people you'd

00:03:02.394 --> 00:03:04.060
like to work with and
work on the things

00:03:04.060 --> 00:03:05.559
that you find
personally fulfilling.

00:03:05.559 --> 00:03:08.740
And so if I think I'm going to
really regret doing something

00:03:08.740 --> 00:03:11.530
or regret not doing
something, even

00:03:11.530 --> 00:03:14.620
if I think it's not the best
use of my time for a pure sort

00:03:14.620 --> 00:03:17.320
of net impact on
the world, I'm still

00:03:17.320 --> 00:03:20.230
willing to take that
really seriously.

00:03:20.230 --> 00:03:21.940
And I think that
makes me do better

00:03:21.940 --> 00:03:24.410
at the things I do
that do help the world.

00:03:24.410 --> 00:03:27.370
So you know, the
broad things that I've

00:03:27.370 --> 00:03:28.840
learned that I like to do--

00:03:28.840 --> 00:03:31.240
one is teach people.

00:03:31.240 --> 00:03:33.610
Another is create
economic growth.

00:03:33.610 --> 00:03:36.970
I really do believe that one
of the things that is most

00:03:36.970 --> 00:03:39.640
fundamentally going wrong
right now in the country

00:03:39.640 --> 00:03:41.830
is that we don't have
enough economic growth.

00:03:41.830 --> 00:03:45.020
And the little that we do is
not evenly distributed at all.

00:03:45.020 --> 00:03:48.910
And so I think in a
democracy you really

00:03:48.910 --> 00:03:50.920
want everyone's lives to
get better every year.

00:03:50.920 --> 00:03:54.070
We're basically insensitive
to the absolute quality

00:03:54.070 --> 00:03:55.720
of our lives and
extremely sensitive

00:03:55.720 --> 00:03:58.930
to relative differences year
over year to our neighbors.

00:03:58.930 --> 00:04:00.970
And it's really important
that everyone's life

00:04:00.970 --> 00:04:02.920
is getting better constantly.

00:04:02.920 --> 00:04:05.380
And I think economic growth
is important in that.

00:04:05.380 --> 00:04:09.310
I think that AI is
going to be the most

00:04:09.310 --> 00:04:11.330
important technological
trend of our lifetime.

00:04:11.330 --> 00:04:16.029
So I spend a lot
of my time on that.

00:04:16.029 --> 00:04:18.610
And you know, I try
to think about things

00:04:18.610 --> 00:04:20.500
on those two strategies.

00:04:20.500 --> 00:04:23.350
The other framework besides the
sort of impact maximization,

00:04:23.350 --> 00:04:26.230
regret minimization that
I found really useful

00:04:26.230 --> 00:04:30.670
is spend a little bit of
effort trying a lot of things,

00:04:30.670 --> 00:04:33.379
and then relentlessly prune
down and focus quickly

00:04:33.379 --> 00:04:35.170
on the ones that you
like and the ones that

00:04:35.170 --> 00:04:36.680
seem to be working.

00:04:36.680 --> 00:04:40.810
So in some sense, this is the Y
Combinator model of fund a lot

00:04:40.810 --> 00:04:43.480
of startups with a
little bit of money.

00:04:43.480 --> 00:04:45.247
And then, you know,
most don't work out,

00:04:45.247 --> 00:04:46.330
and some work really well.

00:04:46.330 --> 00:04:50.810
You spend more time and more
money on the ones that do.

00:04:50.810 --> 00:04:52.030
Hey, guys.

00:04:52.030 --> 00:04:56.620
This is the thing that I've
tried to apply to my life more

00:04:56.620 --> 00:05:00.220
generally is this idea that
I can try a lot of things

00:05:00.220 --> 00:05:01.510
with a little bit of effort.

00:05:01.510 --> 00:05:03.760
It's very hard to predict
exactly what's going to work

00:05:03.760 --> 00:05:05.050
and what hasn't.

00:05:05.050 --> 00:05:07.151
But then the hard
part about that,

00:05:07.151 --> 00:05:08.650
the thing that most
people don't do,

00:05:08.650 --> 00:05:11.050
is you really want to
relentlessly focus down

00:05:11.050 --> 00:05:12.670
on the ones that do work.

00:05:15.101 --> 00:05:17.100
And the last thing I'll
say about prioritization

00:05:17.100 --> 00:05:19.350
is the other hack
I have learned is

00:05:19.350 --> 00:05:23.370
if you can get one really
great partner with you

00:05:23.370 --> 00:05:26.820
on every project, that will
cover up a lot of the slack.

00:05:26.820 --> 00:05:29.160
Because if you try to do
multiple things at once,

00:05:29.160 --> 00:05:30.830
crises come at the same times.

00:05:30.830 --> 00:05:33.489
And that's really hard if you
have to do it all yourself.

00:05:33.489 --> 00:05:35.280
JORGE CUETO: And focusing
more specifically

00:05:35.280 --> 00:05:38.160
on productivity, what
are some life hacks maybe

00:05:38.160 --> 00:05:41.652
you apply to make your
everyday more productive?

00:05:41.652 --> 00:05:43.110
SAM ALTMAN: I mean,
I think there's

00:05:43.110 --> 00:05:45.470
like a lot of crap written
about productivity,

00:05:45.470 --> 00:05:46.470
secrets on the internet.

00:05:46.470 --> 00:05:49.080
And people sort of like
get into this thing

00:05:49.080 --> 00:05:50.970
where they spend
more time trying

00:05:50.970 --> 00:05:52.455
to be productive about
their productivity system

00:05:52.455 --> 00:05:53.871
than actually
getting things done.

00:05:59.000 --> 00:06:01.100
Well, say two, I
think, pieces of advice

00:06:01.100 --> 00:06:03.200
that aren't that obvious.

00:06:03.200 --> 00:06:06.080
One is, I think,
far more important

00:06:06.080 --> 00:06:08.360
than any particular system
is just figuring out

00:06:08.360 --> 00:06:09.584
the right things to work on.

00:06:09.584 --> 00:06:11.000
And so all of the
time that people

00:06:11.000 --> 00:06:13.970
spend with this new
productivity app or that

00:06:13.970 --> 00:06:16.130
or whatever would
be better spent

00:06:16.130 --> 00:06:19.620
like really trying to
think diligently about I

00:06:19.620 --> 00:06:22.055
have the same number of
hours as anybody else.

00:06:22.055 --> 00:06:23.430
What am I going
to spend them on?

00:06:23.430 --> 00:06:26.009
And getting that right is
more important than exactly

00:06:26.009 --> 00:06:28.050
like being perfectly
productive with those hours.

00:06:30.660 --> 00:06:33.540
A big part of that is not
doing things that waste time.

00:06:33.540 --> 00:06:36.960
I think if you can just focus
on the things that are important

00:06:36.960 --> 00:06:39.150
and not do the things
that waste time,

00:06:39.150 --> 00:06:41.390
you can be fairly sloppy
with productivity otherwise,

00:06:41.390 --> 00:06:44.070
and you'll still get far
more done than most people.

00:06:47.339 --> 00:06:48.630
It's really hard to do, though.

00:06:48.630 --> 00:06:52.100
The other thing that I think
people don't think about enough

00:06:52.100 --> 00:06:55.100
is figuring out your
own personal rhythms

00:06:55.100 --> 00:06:56.840
of productivity.

00:06:56.840 --> 00:06:58.520
And there's huge
variance, I've noticed,

00:06:58.520 --> 00:07:01.400
between people that
figured this out and don't.

00:07:01.400 --> 00:07:03.710
So for me personally,
it turns out

00:07:03.710 --> 00:07:07.970
that I am most productive if I
go to sleep late, wake up late,

00:07:07.970 --> 00:07:10.550
and then keep the first like
three or four hours of the day

00:07:10.550 --> 00:07:13.400
and don't schedule any
meetings, like work from home,

00:07:13.400 --> 00:07:15.800
get there my list of
stuff then, and then pack

00:07:15.800 --> 00:07:18.230
all my meetings when I'm kind
of less productive at just

00:07:18.230 --> 00:07:22.460
grinding stuff out or thinking
creatively in the afternoon.

00:07:22.460 --> 00:07:25.820
And it took me some number
of years to figure out,

00:07:25.820 --> 00:07:28.400
because it didn't fit
well with the work

00:07:28.400 --> 00:07:29.619
schedule I was naturally in.

00:07:29.619 --> 00:07:30.910
But then I was like, all right.

00:07:30.910 --> 00:07:34.520
If this is the thing that
makes me most productive,

00:07:34.520 --> 00:07:38.610
then I'm going to make my whole
schedule work to support that.

00:07:38.610 --> 00:07:40.950
And that was a really
important change for me.

00:07:40.950 --> 00:07:45.290
So I think figuring out your
own personal optimal times

00:07:45.290 --> 00:07:47.452
to work on what kind
of different things,

00:07:47.452 --> 00:07:49.160
people don't really
talk about that much.

00:07:49.160 --> 00:07:51.320
And at least for me,
it had a huge impact.

00:07:51.320 --> 00:07:53.028
JORGE CUETO: Shifting
gears a little bit,

00:07:53.028 --> 00:07:56.630
but still trying to get your
perspective on different things

00:07:56.630 --> 00:07:59.030
related to day to day, but
one of the key issues that

00:07:59.030 --> 00:08:02.780
has come up recently is
bias in the workplace.

00:08:02.780 --> 00:08:05.630
and both conscious
and unconscious bias.

00:08:05.630 --> 00:08:07.130
And I wanted to get
your perspective

00:08:07.130 --> 00:08:09.860
on what are some of the
strategies that you implement

00:08:09.860 --> 00:08:12.200
personally, whether in
the way you interact

00:08:12.200 --> 00:08:15.320
with people that you encounter
or in how you approach

00:08:15.320 --> 00:08:19.070
the decision making process
and using strategies

00:08:19.070 --> 00:08:22.940
to minimize your own bias.

00:08:22.940 --> 00:08:24.440
SAM ALTMAN: Look,
I think this is

00:08:24.440 --> 00:08:26.750
an important conversation
happening in Silicon Valley

00:08:26.750 --> 00:08:27.270
right now.

00:08:27.270 --> 00:08:28.790
And there's a lot of
opinions on a lot of sides.

00:08:28.790 --> 00:08:30.581
But I would just like
to say the following.

00:08:30.581 --> 00:08:37.070
I think no matter what you
believe about biology, no one

00:08:37.070 --> 00:08:40.700
that I respect does
not believe that women

00:08:40.700 --> 00:08:43.070
and racial minorities and
a number of other groups

00:08:43.070 --> 00:08:46.940
face an absolutely
unfair playing field

00:08:46.940 --> 00:08:47.850
their entire lives.

00:08:47.850 --> 00:08:50.330
I think people start getting
told directly or indirectly

00:08:50.330 --> 00:08:53.900
from the very young age this
is the kind of thing you should

00:08:53.900 --> 00:08:55.670
do or can do or whatever.

00:08:55.670 --> 00:08:58.680
And that has an effect, a
[INAUDIBLE] effect on anyone.

00:08:58.680 --> 00:09:02.240
And I think trying to counter
that is really important.

00:09:02.240 --> 00:09:07.000
And again, I think
there are things

00:09:07.000 --> 00:09:09.497
reasonable well-meaning
people can disagree on.

00:09:09.497 --> 00:09:11.830
I think, unfortunately, we
spend all of our time talking

00:09:11.830 --> 00:09:13.150
about the disagreements.

00:09:13.150 --> 00:09:15.320
And we don't focus
enough on the agreements.

00:09:15.320 --> 00:09:19.030
And I think almost all
smart, reasonable people will

00:09:19.030 --> 00:09:22.480
agree that the
society we grew up in

00:09:22.480 --> 00:09:27.580
has a hugely unfair
playing field.

00:09:27.580 --> 00:09:30.010
And I think because
of that, it is not

00:09:30.010 --> 00:09:32.020
enough to talk about
unconscious bias.

00:09:32.020 --> 00:09:34.280
I think that is a real
problem to be clear.

00:09:34.280 --> 00:09:37.300
I think we are all a product
of the society we grew up in.

00:09:37.300 --> 00:09:39.460
And we all have biases
that aren't our fault,

00:09:39.460 --> 00:09:42.760
but still have a
responsibility to counteract.

00:09:42.760 --> 00:09:46.870
But one thing I don't like
about the discussion in Silicon

00:09:46.870 --> 00:09:48.460
Valley about
unconscious bias and how

00:09:48.460 --> 00:09:51.220
that's the problem that
we need to fix is I

00:09:51.220 --> 00:09:54.020
think it is not
nearly sufficient.

00:09:54.020 --> 00:09:57.010
A good thing to fix,
but it ignores the fact

00:09:57.010 --> 00:10:01.950
that, you know, decades
or centuries of society

00:10:01.950 --> 00:10:06.120
have built up a very
uneven playing field,

00:10:06.120 --> 00:10:09.810
and that is why we do
need programs to try

00:10:09.810 --> 00:10:11.790
to proactively counter that.

00:10:11.790 --> 00:10:15.600
So you know, I think it's
really important we not

00:10:15.600 --> 00:10:16.660
lose sight of that.

00:10:16.660 --> 00:10:18.649
And that unconscious
bias training alone,

00:10:18.649 --> 00:10:20.940
although currently very
fashionable, will not fix that.

00:10:20.940 --> 00:10:24.930
That said, I do believe
unconscious bias is a problem.

00:10:24.930 --> 00:10:28.350
We try to counteract it,
A, by talking about it

00:10:28.350 --> 00:10:31.440
and doing training,
which I think does help.

00:10:31.440 --> 00:10:35.089
But B, I think one
of the things that we

00:10:35.089 --> 00:10:37.380
have done that, unfortunately,
other investors have not

00:10:37.380 --> 00:10:42.420
done as much is just have
a very diverse partnership.

00:10:42.420 --> 00:10:48.020
You know, we have six
female GPs on our team.

00:10:48.020 --> 00:10:52.570
And that's probably a large part
of the women in top investing

00:10:52.570 --> 00:10:54.000
roles in Silicon Valley.

00:10:54.000 --> 00:10:55.050
And that's really bad.

00:10:55.050 --> 00:10:58.680
We have the CEO of our
corp program is black.

00:10:58.680 --> 00:11:01.630
And I hate to play
the kind of like,

00:11:01.630 --> 00:11:03.320
who's the most
discriminated stack game.

00:11:03.320 --> 00:11:05.800
But I think black
entrepreneurs in Silicon Valley

00:11:05.800 --> 00:11:08.560
have an exceptionally hard time.

00:11:08.560 --> 00:11:11.650
And I think by having
a more diverse team,

00:11:11.650 --> 00:11:14.500
it helps us have
broader networks

00:11:14.500 --> 00:11:19.120
and also think about our own
unconscious bias all the time.

00:11:19.120 --> 00:11:22.870
JORGE CUETO: And the issue of
politics or maybe even bringing

00:11:22.870 --> 00:11:26.680
these issues of in the workplace
is sometimes seen as a taboo.

00:11:26.680 --> 00:11:29.890
And yet, you've been pretty
vocal about your views

00:11:29.890 --> 00:11:32.789
on current political events
and also other issues

00:11:32.789 --> 00:11:34.330
that are coming up
in Silicon Valley.

00:11:34.330 --> 00:11:36.790
So I wanted to ask
you how you walk

00:11:36.790 --> 00:11:39.500
that fine line between
expressing your opinions,

00:11:39.500 --> 00:11:41.510
but also minimizing
any repercussions

00:11:41.510 --> 00:11:44.770
that that could have on the
day to day business of YC.

00:11:44.770 --> 00:11:47.110
SAM ALTMAN: Well, now it's
not even controversial.

00:11:47.110 --> 00:11:49.855
You know, like now,
all the tech CEOs

00:11:49.855 --> 00:11:50.980
are talking about politics.

00:11:53.560 --> 00:11:56.560
When I started doing
it a couple of years

00:11:56.560 --> 00:11:59.770
ago kind of at the beginning
of the rise of Trump,

00:11:59.770 --> 00:12:01.680
it was controversial.

00:12:01.680 --> 00:12:02.990
Two things were going on.

00:12:02.990 --> 00:12:07.920
One is I think no one
took him seriously.

00:12:07.920 --> 00:12:09.420
So most people were
like, yeah, this

00:12:09.420 --> 00:12:11.440
is a ridiculous thing
that's going to go away.

00:12:11.440 --> 00:12:16.290
Two is that I think
in normal times,

00:12:16.290 --> 00:12:20.880
it does make sense for business
leaders of large organizations

00:12:20.880 --> 00:12:23.290
to remain apolitical.

00:12:23.290 --> 00:12:27.510
You know, I think it actually
does make a lot of sense.

00:12:27.510 --> 00:12:29.680
It's a huge distraction.

00:12:29.680 --> 00:12:31.230
It's hugely time consuming.

00:12:31.230 --> 00:12:33.630
And it has all of these,
like, weird negative effects,

00:12:33.630 --> 00:12:36.090
like you know, if you
piss off the president,

00:12:36.090 --> 00:12:37.480
he can do bad things to you.

00:12:40.030 --> 00:12:43.180
However, these are
not normal times.

00:12:43.180 --> 00:12:50.680
And I think when the future
of the republic is at risk,

00:12:50.680 --> 00:12:53.590
the duty to the
country and our values

00:12:53.590 --> 00:12:56.890
transcends the duty to
your particular company.

00:12:56.890 --> 00:12:58.870
and your stock price.

00:12:58.870 --> 00:13:01.660
And I think I started
that a little bit

00:13:01.660 --> 00:13:03.107
earlier than other people.

00:13:03.107 --> 00:13:04.940
But at this point, I'm
in very good company.

00:13:04.940 --> 00:13:07.915
And it doesn't seem to be
that controversial anymore.

00:13:07.915 --> 00:13:09.290
JORGE CUETO: What
are some things

00:13:09.290 --> 00:13:11.726
that you wish people
knew about you that they

00:13:11.726 --> 00:13:12.600
don't know about you?

00:13:15.720 --> 00:13:17.220
SAM ALTMAN: You
know, at this point,

00:13:17.220 --> 00:13:23.370
I would just like to have my own
little quiet private life back.

00:13:23.370 --> 00:13:24.810
I'll say nothing at all.

00:13:24.810 --> 00:13:25.680
JORGE CUETO: OK.

00:13:25.680 --> 00:13:26.220
Fair answer.

00:13:29.970 --> 00:13:32.160
Moving on to talking
a little bit about YC

00:13:32.160 --> 00:13:34.560
and what you look for
in companies, are there

00:13:34.560 --> 00:13:37.530
qualities in founders
that you think

00:13:37.530 --> 00:13:40.272
are overrated or underrated?

00:13:40.272 --> 00:13:40.980
SAM ALTMAN: Yeah.

00:13:40.980 --> 00:13:46.580
I think the most
underrated quality of all

00:13:46.580 --> 00:13:49.370
is being really determined.

00:13:49.370 --> 00:13:51.410
This is more important
than being smart.

00:13:51.410 --> 00:13:53.290
This is more important
than having a network.

00:13:53.290 --> 00:13:55.280
This is more important
than a great idea.

00:13:55.280 --> 00:13:58.010
The hardest thing about
starting a company

00:13:58.010 --> 00:14:03.150
is the level and the
frequency of bad stuff

00:14:03.150 --> 00:14:04.320
that happens to you.

00:14:04.320 --> 00:14:07.110
And most people that are
good in really other ways

00:14:07.110 --> 00:14:09.120
eventually just get
killed, the company

00:14:09.120 --> 00:14:12.480
gets killed, by
stuff going wrong.

00:14:12.480 --> 00:14:16.680
And you know, so much about
being a successful entrepreneur

00:14:16.680 --> 00:14:19.260
is just not giving up.

00:14:19.260 --> 00:14:21.540
When we have funded
people who have

00:14:21.540 --> 00:14:27.690
a great idea, perfect background
on paper, and a great product

00:14:27.690 --> 00:14:30.450
and still failed, it has
usually been that they're

00:14:30.450 --> 00:14:31.740
insufficiently determined.

00:14:31.740 --> 00:14:34.650
So I think this is the most
important non-obvious skill

00:14:34.650 --> 00:14:35.244
of a founder.

00:14:35.244 --> 00:14:37.410
Of course, you need a good
product and a good market

00:14:37.410 --> 00:14:38.370
and to be smart.

00:14:38.370 --> 00:14:40.500
But that's really obvious.

00:14:40.500 --> 00:14:44.670
The degree to which being
like a three or four

00:14:44.670 --> 00:14:47.250
standard deviation
outlier on determination

00:14:47.250 --> 00:14:50.190
is a required skill
of a CEO is not

00:14:50.190 --> 00:14:54.450
something that was obvious
to me when I started.

00:14:54.450 --> 00:14:56.670
That's also bad, because
it's really hard to select.

00:14:56.670 --> 00:14:58.740
It's really hard
to identify that.

00:14:58.740 --> 00:15:02.790
As we have said more publicly
how important that is,

00:15:02.790 --> 00:15:05.220
people applying to YC have
gotten better and better

00:15:05.220 --> 00:15:08.010
at telling us stories from
their past life about how

00:15:08.010 --> 00:15:12.330
they overcame these impossible
odds to get through something.

00:15:12.330 --> 00:15:17.730
And unlike intelligence, which
is very difficult to fake in a,

00:15:17.730 --> 00:15:19.890
you know, one hour
meeting, you can definitely

00:15:19.890 --> 00:15:23.280
fake determination in
a one hour meeting.

00:15:23.280 --> 00:15:26.880
So that's one thing
that really matters.

00:15:26.880 --> 00:15:30.630
Another thing that really
matters that is non-obvious

00:15:30.630 --> 00:15:32.400
is independent thought.

00:15:32.400 --> 00:15:37.290
And this I think is an
even more unusual skill

00:15:37.290 --> 00:15:38.862
than determination.

00:15:38.862 --> 00:15:41.070
I think both of these you
can make a conscious effort

00:15:41.070 --> 00:15:42.000
and build up.

00:15:42.000 --> 00:15:47.410
But I think independent thought
is one of the hardest skills

00:15:47.410 --> 00:15:51.250
to build up.

00:15:51.250 --> 00:15:56.080
Because speaking of social
pressures from birth,

00:15:56.080 --> 00:15:59.540
we are all pushed to
think like other people.

00:15:59.540 --> 00:16:01.270
And if you think
in your own lives

00:16:01.270 --> 00:16:03.910
about the number of people you
spend time with that you would

00:16:03.910 --> 00:16:07.885
say are true independent
thinkers that consistently have

00:16:07.885 --> 00:16:09.760
new ideas you haven't
heard from other people

00:16:09.760 --> 00:16:12.830
and think about the
world in different ways,

00:16:12.830 --> 00:16:15.554
it's probably a very short list.

00:16:15.554 --> 00:16:17.220
And yet, these are
the people that start

00:16:17.220 --> 00:16:18.580
all the interesting companies.

00:16:18.580 --> 00:16:20.710
The consensus idea
is everyone tries

00:16:20.710 --> 00:16:23.390
Google will eat your lunch at.

00:16:23.390 --> 00:16:27.670
And they're also not kind
of the really big trends

00:16:27.670 --> 00:16:28.420
of the future.

00:16:28.420 --> 00:16:30.590
You want startup ideas--

00:16:30.590 --> 00:16:32.620
if you picture a Venn
diagram, and here you

00:16:32.620 --> 00:16:36.130
have is a good
idea, and here you

00:16:36.130 --> 00:16:41.370
have sounds like a bad idea, you
want that tiny little overlap.

00:16:41.370 --> 00:16:44.940
And those are the kind of
ideas that are the hardest

00:16:44.940 --> 00:16:48.720
to identify and
the ones that, even

00:16:48.720 --> 00:16:51.510
if you do manage to notice them,
most people will talk you out

00:16:51.510 --> 00:16:52.110
of.

00:16:52.110 --> 00:16:54.630
So I'd say those are two
non-obvious skills that we

00:16:54.630 --> 00:16:55.579
look for.

00:16:55.579 --> 00:16:57.120
JORGE CUETO: And
what's one challenge

00:16:57.120 --> 00:17:00.885
that YC companies face
repeatedly that you notice?

00:17:04.260 --> 00:17:07.829
SAM ALTMAN: Hiring engineers,
when Google can just sort of

00:17:07.829 --> 00:17:10.126
like throw unlimited
cash at anyone they want,

00:17:10.126 --> 00:17:11.834
has become very
problematic for startups.

00:17:15.020 --> 00:17:16.849
I think the good
thing about that--

00:17:16.849 --> 00:17:20.839
I always try to find the
good in any bad situation.

00:17:20.839 --> 00:17:23.540
And the good thing
about this is it

00:17:23.540 --> 00:17:28.910
means that all of the
really good startups

00:17:28.910 --> 00:17:30.889
have really important missions.

00:17:30.889 --> 00:17:32.180
Eventually, they figure it out.

00:17:32.180 --> 00:17:34.346
They may not have it on day
one, but they eventually

00:17:34.346 --> 00:17:37.700
get to this, like,
missionary mindset.

00:17:37.700 --> 00:17:41.059
And it used to be that even
if you didn't have that,

00:17:41.059 --> 00:17:43.100
you could just sort of
get a bunch of mercenaries

00:17:43.100 --> 00:17:44.120
to come work for you.

00:17:44.120 --> 00:17:46.453
And now you can't, because
you can't [INAUDIBLE] Google.

00:17:46.453 --> 00:17:49.700
And so one positive side
effect of this thing

00:17:49.700 --> 00:17:55.790
has been that the importance
now of a startup having

00:17:55.790 --> 00:17:59.810
a really clear and really
important mission on day three

00:17:59.810 --> 00:18:00.640
has gone up a lot.

00:18:00.640 --> 00:18:02.030
And I think that's leading
to better startups.

00:18:02.030 --> 00:18:03.821
Because, otherwise,
you just can't recruit.

00:18:06.240 --> 00:18:08.220
Another common problem
that startups have--

00:18:08.220 --> 00:18:11.580
and this doesn't sound
like a great insight.

00:18:11.580 --> 00:18:14.790
But most startups still
don't ever built a product

00:18:14.790 --> 00:18:17.230
that people want.

00:18:17.230 --> 00:18:21.912
And it doesn't seem to matter
how much we talk about this.

00:18:21.912 --> 00:18:24.370
It doesn't seem to matter how
much anyone talks about this.

00:18:24.370 --> 00:18:27.370
People still keep trying
to do anything but this.

00:18:27.370 --> 00:18:31.772
And if there's one thing that
a startup has to get right,

00:18:31.772 --> 00:18:34.230
it's build a product that people
really want, not a little.

00:18:34.230 --> 00:18:35.610
If they want it a
little bit, then you

00:18:35.610 --> 00:18:36.930
won't generate enough momentum.

00:18:36.930 --> 00:18:38.880
Like, you've got
to build something

00:18:38.880 --> 00:18:41.170
that some people really love.

00:18:41.170 --> 00:18:45.390
And after failing, because
of insufficiently determined

00:18:45.390 --> 00:18:47.850
founders, this is
the number two reason

00:18:47.850 --> 00:18:51.285
that startups that seem
really good otherwise fail.

00:18:51.285 --> 00:18:52.660
JORGE CUETO: Moving
on to talking

00:18:52.660 --> 00:18:54.420
about different
technologies, what

00:18:54.420 --> 00:18:56.710
would you say is the
biggest challenge

00:18:56.710 --> 00:18:59.170
that we're facing in
terms of making progress

00:18:59.170 --> 00:19:00.790
on artificial
intelligence right now?

00:19:02.530 --> 00:19:04.780
SAM ALTMAN: Well, I don't
know if any of you saw this,

00:19:04.780 --> 00:19:11.410
but OpenAI beat the best
single player DotA players

00:19:11.410 --> 00:19:15.580
in the world last Friday.

00:19:15.580 --> 00:19:18.495
And when we started that project
at the beginning of this year,

00:19:18.495 --> 00:19:20.620
I did not think that was
going to happen this year.

00:19:20.620 --> 00:19:24.050
And I wasn't even sure it was
going to happen next year.

00:19:24.050 --> 00:19:29.170
And it was very wild
to watch that happen.

00:19:29.170 --> 00:19:33.590
Because it was almost
purely self-training.

00:19:33.590 --> 00:19:36.370
And it was this very
complex environment.

00:19:36.370 --> 00:19:38.620
And the AI was just
playing itself and getting

00:19:38.620 --> 00:19:41.150
better and better and better.

00:19:41.150 --> 00:19:45.310
In fact, the final bot that we
had that beat all the humans

00:19:45.310 --> 00:19:51.190
handily, one day later, it lost
60-40 to a one-day-more evolved

00:19:51.190 --> 00:19:53.290
version of itself,
just to give you

00:19:53.290 --> 00:19:55.010
a sense for the
rate of improvement.

00:19:59.030 --> 00:20:01.920
And so I think people are kind
of asleep at the wheel here.

00:20:01.920 --> 00:20:03.364
There are problems.

00:20:03.364 --> 00:20:05.280
But again, in the same
spirit of always trying

00:20:05.280 --> 00:20:09.374
to figure out the
good, not the bad,

00:20:09.374 --> 00:20:11.290
we are making unbelievable
progress, maybe too

00:20:11.290 --> 00:20:13.330
much progress.

00:20:13.330 --> 00:20:20.344
But I think of all the
things I worry about with AI,

00:20:20.344 --> 00:20:21.760
the technical
barriers to progress

00:20:21.760 --> 00:20:24.236
are not the top of the list.

00:20:24.236 --> 00:20:25.610
JORGE CUETO:
There's another camp

00:20:25.610 --> 00:20:28.760
that says that maybe we're
placing too much emphasis

00:20:28.760 --> 00:20:30.590
on the threat of AI.

00:20:30.590 --> 00:20:32.520
What do you think about that?

00:20:32.520 --> 00:20:35.720
SAM ALTMAN: Yeah, I think
we don't talk enough

00:20:35.720 --> 00:20:38.480
about the benefits.

00:20:38.480 --> 00:20:40.280
I think that AI
has the potential

00:20:40.280 --> 00:20:44.270
to eliminate nearly
all human suffering

00:20:44.270 --> 00:20:45.592
in the next couple of decades.

00:20:45.592 --> 00:20:47.300
I think we can have
a world of abundance.

00:20:47.300 --> 00:20:49.100
We can eliminate
poverty over time.

00:20:49.100 --> 00:20:52.100
We can probably cure a
whole lot of diseases.

00:20:52.100 --> 00:20:54.985
There all these wonderful
things that technology can do.

00:20:54.985 --> 00:20:56.360
I think we're
already seeing that

00:20:56.360 --> 00:20:58.700
in just how much better a
lot of consumer products

00:20:58.700 --> 00:21:02.720
we use everyday have gotten.

00:21:02.720 --> 00:21:05.000
People like disaster porn.

00:21:05.000 --> 00:21:06.650
People are more
interested in talking

00:21:06.650 --> 00:21:09.530
about the end of the world than
they are about life getting

00:21:09.530 --> 00:21:11.780
10% better every year and
having that compound, which

00:21:11.780 --> 00:21:13.580
gets a lot better.

00:21:13.580 --> 00:21:18.314
So you know, if
you're a journalist,

00:21:18.314 --> 00:21:19.730
you can write an
article about how

00:21:19.730 --> 00:21:22.730
AI is going to end the world
and get a lot of clicks

00:21:22.730 --> 00:21:25.250
and, you know, a page view
bonus or whatever you get.

00:21:25.250 --> 00:21:28.580
Or, you can write an article
about how AI is gradually

00:21:28.580 --> 00:21:30.650
making all these
problems 10% better.

00:21:30.650 --> 00:21:32.240
And probably no one reads it.

00:21:32.240 --> 00:21:34.250
Certainly, no one shares it.

00:21:34.250 --> 00:21:38.742
And so I think, for whatever
reason, the way we're wired

00:21:38.742 --> 00:21:40.700
is to talk much more
about the downside of this

00:21:40.700 --> 00:21:41.369
than the upside.

00:21:41.369 --> 00:21:43.160
But the upside I think
is going to be huge.

00:21:43.160 --> 00:21:46.260
It already is huge.

00:21:46.260 --> 00:21:48.920
JORGE CUETO: And what do you
think about cryptocurrencies?

00:21:48.920 --> 00:21:52.062
Do you see the same growth?

00:21:52.062 --> 00:21:52.770
SAM ALTMAN: Yeah.

00:21:52.770 --> 00:21:57.620
I mean, I think it's
going to go up and down.

00:21:57.620 --> 00:22:00.849
Like I bought my
bitcoin a long time ago.

00:22:00.849 --> 00:22:02.640
I plan to hold them
for a really long time.

00:22:02.640 --> 00:22:04.098
I try not to watch
the price ticks,

00:22:04.098 --> 00:22:07.770
but it's so addicting
that I can't help myself.

00:22:07.770 --> 00:22:14.220
I think-- this is not
investment advice.

00:22:18.144 --> 00:22:21.410
I think the only super
compelling proven use

00:22:21.410 --> 00:22:24.500
case we have seen so
far is store value.

00:22:24.500 --> 00:22:26.630
And as a replacement
for gold, I think

00:22:26.630 --> 00:22:28.310
we are seeing real
adoption there

00:22:28.310 --> 00:22:30.410
and real collective
belief that actually

00:22:30.410 --> 00:22:33.060
makes it have some value.

00:22:33.060 --> 00:22:35.340
However, if that's how
it's going to play out,

00:22:35.340 --> 00:22:38.090
then I think bitcoin
should dominate--

00:22:38.090 --> 00:22:41.360
biggest network, first biggest
brand, most collective belief,

00:22:41.360 --> 00:22:42.860
whatever.

00:22:42.860 --> 00:22:46.902
And so I have been surprised
by the continual strength

00:22:46.902 --> 00:22:47.860
on all of the altcoins.

00:22:50.700 --> 00:22:55.760
I think there are potential
other truly valuable

00:22:55.760 --> 00:22:59.349
applications of the blockchain.

00:22:59.349 --> 00:23:01.640
Filecoin is a YC thing, so
I know that one pretty well.

00:23:01.640 --> 00:23:03.120
And I can see that being big.

00:23:03.120 --> 00:23:08.150
But I think most of what's
going on, outside of bitcoin,

00:23:08.150 --> 00:23:10.400
feels like a complete
speculative bubble.

00:23:10.400 --> 00:23:13.190
And I feel bad that a lot of
people are going to get burned.

00:23:13.190 --> 00:23:15.560
So I think probably
the right thing to do,

00:23:15.560 --> 00:23:19.700
if you believe in it,
is to buy bitcoin,

00:23:19.700 --> 00:23:23.161
and then not think about
it for another five years.

00:23:23.161 --> 00:23:24.910
JORGE CUETO: Is there
any specific product

00:23:24.910 --> 00:23:27.284
area or technology that you
think people are sleeping on?

00:23:32.380 --> 00:23:34.150
SAM ALTMAN: Sure, a lot.

00:23:34.150 --> 00:23:36.730
AI, as we mentioned,
I think people

00:23:36.730 --> 00:23:39.490
are asleep at the wheel
on a really big way on.

00:23:39.490 --> 00:23:43.810
Nuclear fusion, I think,
is within some single digit

00:23:43.810 --> 00:23:46.780
number of years of working.

00:23:46.780 --> 00:23:52.717
And because it's been
so bad for so long,

00:23:52.717 --> 00:23:55.050
people are just kind of burned
out and not really taking

00:23:55.050 --> 00:23:57.576
a new look at new materials
and stronger magnets

00:23:57.576 --> 00:23:58.950
and better computer
models that's

00:23:58.950 --> 00:24:01.650
going to enable this to work.

00:24:01.650 --> 00:24:04.320
Synthetic biology-- again, it's
like people talked about it

00:24:04.320 --> 00:24:05.550
a lot a couple of years ago.

00:24:05.550 --> 00:24:08.427
It didn't quite work as
fast as people were hoping.

00:24:08.427 --> 00:24:11.010
So we have like this hype cycle,
and then it really falls off.

00:24:11.010 --> 00:24:12.660
And now, we're in the part where
I think the interesting work is

00:24:12.660 --> 00:24:13.740
happening.

00:24:13.740 --> 00:24:15.848
And people aren't
paying enough attention.

00:24:19.270 --> 00:24:22.060
I guess, that's a topic I
can go on for a long time.

00:24:22.060 --> 00:24:23.220
But I'll limit it to three.

00:24:23.220 --> 00:24:24.144
JORGE CUETO: Sure.

00:24:24.144 --> 00:24:25.810
Is there any particular
problem that you

00:24:25.810 --> 00:24:29.090
think technology can't solve?

00:24:29.090 --> 00:24:32.180
SAM ALTMAN: How to make us
nice to each other technology

00:24:32.180 --> 00:24:35.180
has clearly been
quite bad at solving.

00:24:35.180 --> 00:24:36.970
I won't say it can't.

00:24:36.970 --> 00:24:38.720
But I haven't seen it
yet, and I certainly

00:24:38.720 --> 00:24:40.880
don't think technology
can by itself.

00:24:40.880 --> 00:24:47.900
And I think we are kind
of in the situation,

00:24:47.900 --> 00:24:49.670
at least in the
developed world, where

00:24:49.670 --> 00:24:53.300
the world keeps getting better,
and we keep getting unhappier.

00:24:53.300 --> 00:24:55.910
And you know, there's like a
decent amount of data on this.

00:24:55.910 --> 00:25:03.910
And I think technology
is not entirely to blame,

00:25:03.910 --> 00:25:06.340
but it's certainly
not blameless.

00:25:06.340 --> 00:25:10.810
And I think 30 minutes
left, I couldn't even

00:25:10.810 --> 00:25:11.780
start the conversation.

00:25:11.780 --> 00:25:15.880
But I think the number
of things that technology

00:25:15.880 --> 00:25:20.920
has done to make
us more isolated,

00:25:20.920 --> 00:25:24.780
feeling relatively worse--

00:25:24.780 --> 00:25:27.560
a friend of mine
a little bit older

00:25:27.560 --> 00:25:30.680
said she never
used to be unhappy.

00:25:30.680 --> 00:25:33.290
Because she had no idea
what she was missing.

00:25:33.290 --> 00:25:35.654
And now that she
has to watch, like--

00:25:35.654 --> 00:25:37.195
I'm very bad a pop
culture, I'm going

00:25:37.195 --> 00:25:38.278
to pick a name at random--

00:25:38.278 --> 00:25:41.120
Kim Kardashian fly around
in private jets on Instagram

00:25:41.120 --> 00:25:42.710
all day, she's jealous.

00:25:42.710 --> 00:25:44.240
And it makes her very unhappy.

00:25:44.240 --> 00:25:45.448
And it didn't used to happen.

00:25:47.135 --> 00:25:48.760
And I've been thinking
about that a lot

00:25:48.760 --> 00:25:50.320
in the last couple of weeks.

00:25:50.320 --> 00:25:52.937
And I don't have a
solution to that.

00:25:52.937 --> 00:25:54.520
But I understand why
that's a problem.

00:25:54.520 --> 00:25:58.830
So I think figuring out how
to make us happier and nicer

00:25:58.830 --> 00:26:01.470
to each other in particular, I
think one thing that technology

00:26:01.470 --> 00:26:07.050
does that's really bad
is we have some probably

00:26:07.050 --> 00:26:10.790
long-standing evolutionary
pressure that even if I really

00:26:10.790 --> 00:26:12.620
don't like you, I'm
very unlikely to come

00:26:12.620 --> 00:26:14.710
stand next to you and
say, I fucking hate you.

00:26:14.710 --> 00:26:16.460
You're a jerk,
you know, whatever

00:26:16.460 --> 00:26:18.012
else people say on Twitter.

00:26:18.012 --> 00:26:19.220
JORGE CUETO: A lot of things.

00:26:19.220 --> 00:26:22.250
SAM ALTMAN: Because we're
like these pack animals.

00:26:22.250 --> 00:26:24.620
And we have to live
with each other

00:26:24.620 --> 00:26:27.680
and help each other survive.

00:26:27.680 --> 00:26:32.240
But somehow on Twitter,
that goes away.

00:26:32.240 --> 00:26:35.090
And I think I'll save my
whole rant about Twitter.

00:26:35.090 --> 00:26:38.000
But I think that is a
platform in particular

00:26:38.000 --> 00:26:43.432
that rewards saying the most
aggressive snarky things.

00:26:43.432 --> 00:26:45.390
That's how you get likes
and re-tweets and sort

00:26:45.390 --> 00:26:46.680
of value out of the platform.

00:26:46.680 --> 00:26:48.540
But a lot of
technology does this.

00:26:48.540 --> 00:26:51.430
We don't have whatever
kind of the human,

00:26:51.430 --> 00:26:55.080
being nice to each other
in person, instinct is.

00:26:55.080 --> 00:26:58.590
And we have these platforms
that reward being bad,

00:26:58.590 --> 00:27:00.610
reward being a jerk.

00:27:00.610 --> 00:27:04.872
And so I think
I'm not optimistic

00:27:04.872 --> 00:27:06.830
that technology is going
to solve that problem.

00:27:06.830 --> 00:27:10.750
I think that's going to have to
be people solving that problem.

00:27:10.750 --> 00:27:14.020
JORGE CUETO: In an interview
with "Vanity Fair" in 2015,

00:27:14.020 --> 00:27:16.910
you mentioned that you were
optimistic about the future.

00:27:16.910 --> 00:27:20.540
And that was right before all
of the election stuff happened

00:27:20.540 --> 00:27:23.080
and the situation that
we find ourselves in.

00:27:23.080 --> 00:27:25.360
So would you say
that you're still

00:27:25.360 --> 00:27:26.600
optimistic about the future?

00:27:26.600 --> 00:27:28.225
SAM ALTMAN: Yeah, of
course, of course.

00:27:28.225 --> 00:27:35.140
Like, progress, it's not a
perfectly exponential curve.

00:27:35.140 --> 00:27:38.630
It's not even a straight line.

00:27:38.630 --> 00:27:43.130
And you know, we are clearly
in a challenging period now.

00:27:43.130 --> 00:27:48.530
But I think if you look back
at the last few hundred,

00:27:48.530 --> 00:27:52.100
and then few thousand
years, if you zoom out

00:27:52.100 --> 00:27:58.230
enough, the squiggles on
the curve kind of disappear.

00:27:58.230 --> 00:27:59.690
The world's getting
so much better.

00:27:59.690 --> 00:28:02.380
And I think that's
going to keep happening.

00:28:02.380 --> 00:28:06.402
And even with everything
going on right now,

00:28:06.402 --> 00:28:07.860
I'm delighted to
be alive right now

00:28:07.860 --> 00:28:09.359
and not 100 years
ago, and certainly

00:28:09.359 --> 00:28:13.012
not 200 or 2,000 years ago.

00:28:13.012 --> 00:28:15.220
And I think we do have
technology to thank for a lot.

00:28:15.220 --> 00:28:17.230
And we have better governance.

00:28:17.230 --> 00:28:21.290
Like, the number of people
living in an democracy,

00:28:21.290 --> 00:28:24.900
you know, 200 years ago was
very low, the percentage.

00:28:24.900 --> 00:28:27.050
And as horrible as
things are, the fact

00:28:27.050 --> 00:28:30.830
that we get to stand up and
speak our minds without fear

00:28:30.830 --> 00:28:33.830
of being thrown in jail
for political opposition

00:28:33.830 --> 00:28:37.726
and the fact that we get to vote
again in 3 and 1/2 more years,

00:28:37.726 --> 00:28:39.042
I think that's amazing.

00:28:39.042 --> 00:28:41.000
And I think it's easy to
take that for granted.

00:28:41.000 --> 00:28:45.610
But yeah, I think the future
is going to be a lot better.

00:28:45.610 --> 00:28:47.242
I remain very optimistic.

00:28:47.242 --> 00:28:49.450
JORGE CUETO: What do you
see as being a big challenge

00:28:49.450 --> 00:28:51.158
that we're facing as
a society right now?

00:28:54.670 --> 00:28:58.710
SAM ALTMAN: How we
deal with a world

00:28:58.710 --> 00:29:01.530
where the natural forces are
for wealth to concentrate

00:29:01.530 --> 00:29:04.830
into the hands of a smaller
and smaller number of people.

00:29:04.830 --> 00:29:07.350
As I mentioned
earlier, I think people

00:29:07.350 --> 00:29:10.350
are more sensitive to
relative quality of life

00:29:10.350 --> 00:29:11.920
than absolute quality of life.

00:29:11.920 --> 00:29:14.100
And I think technology
is naturally a force.

00:29:14.100 --> 00:29:16.905
It's a giant lever
that tends to create

00:29:16.905 --> 00:29:19.860
way more wealth, but
really concentrated.

00:29:19.860 --> 00:29:22.110
So I think one of the most
tone deaf things people say

00:29:22.110 --> 00:29:28.550
in Silicon Valley is, you know,
poor people should be happy.

00:29:28.550 --> 00:29:30.710
They get this Android
phone for not much money.

00:29:30.710 --> 00:29:33.356
They can access
anything in the world.

00:29:33.356 --> 00:29:34.980
And they wouldn't
have that without us,

00:29:34.980 --> 00:29:36.650
so why are they complaining?

00:29:36.650 --> 00:29:39.150
And OK, like there is
some truth to that.

00:29:39.150 --> 00:29:41.240
I do think it is
cool about the world

00:29:41.240 --> 00:29:44.470
that the richest
person and someone

00:29:44.470 --> 00:29:49.660
living in an absolute poverty
carry around the same phone.

00:29:49.660 --> 00:29:52.960
There was no analogy to
that other than maybe

00:29:52.960 --> 00:29:54.940
like if you got a
really bad disease.

00:29:54.940 --> 00:29:57.240
There was no like
equality like that,

00:29:57.240 --> 00:29:59.830
you know, 400, 500 years ago.

00:29:59.830 --> 00:30:01.990
However, that point,
which is always

00:30:01.990 --> 00:30:04.150
what Silicon Valley
falls back to, I think

00:30:04.150 --> 00:30:09.550
is like maybe not the most
tone deaf possible response,

00:30:09.550 --> 00:30:10.450
but up there.

00:30:10.450 --> 00:30:12.419
Like people want to feel
like they have agency.

00:30:12.419 --> 00:30:14.710
They want to feel like they
have a voice in the future.

00:30:14.710 --> 00:30:16.757
They want to feel like
they can participate.

00:30:16.757 --> 00:30:18.340
And they want to
feel like they're not

00:30:18.340 --> 00:30:23.200
just kind of like given this
baseline, while like they

00:30:23.200 --> 00:30:25.960
toil in the provinces
and Silicon Valley just

00:30:25.960 --> 00:30:28.480
gets all the money.

00:30:28.480 --> 00:30:31.000
And I think people who don't
see that are just not thinking

00:30:31.000 --> 00:30:32.350
clearly.

00:30:32.350 --> 00:30:34.810
And so I think one of
the greatest threats--

00:30:34.810 --> 00:30:37.450
something else I don't think
technology can fix on its own--

00:30:37.450 --> 00:30:40.240
is how we get to
a more just world.

00:30:40.240 --> 00:30:42.220
I really, really
fundamentally believe

00:30:42.220 --> 00:30:45.070
that economic justice is
the most important thing

00:30:45.070 --> 00:30:46.980
you can do for social justice.

00:30:46.980 --> 00:30:48.730
And if you have all
of the resources going

00:30:48.730 --> 00:30:52.930
to a small set of people, even
if everybody else is having

00:30:52.930 --> 00:30:55.210
their absolute quality of
life raised very quickly,

00:30:55.210 --> 00:30:56.672
that's not enough.

00:30:56.672 --> 00:30:58.880
JORGE CUETO: You recently
announced The United Slate,

00:30:58.880 --> 00:31:01.850
which puts forth some
policy proposals as well as

00:31:01.850 --> 00:31:03.650
an invitation for candidates.

00:31:03.650 --> 00:31:06.235
What would you say success
looks like for that?

00:31:06.235 --> 00:31:07.610
SAM ALTMAN: You
know, if we could

00:31:07.610 --> 00:31:14.780
run five, six candidates in
the 2018 cycle on that platform

00:31:14.780 --> 00:31:19.220
and have two of them
win their elections,

00:31:19.220 --> 00:31:20.930
I think that'd be
an incredible start.

00:31:20.930 --> 00:31:22.910
And you know, maybe it
doesn't work at all.

00:31:22.910 --> 00:31:27.080
Because maybe those
issues, although I

00:31:27.080 --> 00:31:28.910
believe they're
important, are not yet

00:31:28.910 --> 00:31:32.780
ready to convince
the public at large.

00:31:32.780 --> 00:31:34.310
But I think it will
be a good start.

00:31:34.310 --> 00:31:36.830
And I think over time
it will be really good

00:31:36.830 --> 00:31:39.560
if people on the
progressive side

00:31:39.560 --> 00:31:43.520
built up a kind of
long-term organization

00:31:43.520 --> 00:31:46.580
focusing on winning elections
and shifting public perception

00:31:46.580 --> 00:31:48.047
at all levels.

00:31:48.047 --> 00:31:49.880
The right has done a
fabulous job with this.

00:31:49.880 --> 00:31:53.270
Congratulations to any of
you who are on that side.

00:31:53.270 --> 00:31:57.650
But I would like the left
to do a better job at that.

00:31:57.650 --> 00:32:01.495
I don't think we're putting
forward our best game there.

00:32:01.495 --> 00:32:03.620
JORGE CUETO: In your
invitation, you also mentioned

00:32:03.620 --> 00:32:05.661
that you would be willing
to work with Democrats,

00:32:05.661 --> 00:32:07.430
Independents, and Republicans.

00:32:07.430 --> 00:32:11.660
Is there any issue in particular
that you see anyone, regardless

00:32:11.660 --> 00:32:14.810
of political ideology, coming
together and agreeing on

00:32:14.810 --> 00:32:17.714
more so than other issues?

00:32:17.714 --> 00:32:19.630
SAM ALTMAN: I think there
are a lot of issues.

00:32:19.630 --> 00:32:23.950
Again, I think people agree on
way more than they disagree on.

00:32:23.950 --> 00:32:25.540
You know, as we went
around the state

00:32:25.540 --> 00:32:29.260
and talked to Californians,
Republicans, Democrats,

00:32:29.260 --> 00:32:34.840
San Francisco, LA, Fresno,
Shasta County, wherever you go,

00:32:34.840 --> 00:32:38.290
the price of housing is like
the biggest issue for most sort

00:32:38.290 --> 00:32:42.140
of regular people that are
not Google employees, maybe

00:32:42.140 --> 00:32:44.281
even for Google employees.

00:32:44.281 --> 00:32:45.280
It's really, really bad.

00:32:45.280 --> 00:32:49.750
I mean, this is if you could
fix I think one thing that

00:32:49.750 --> 00:32:54.250
would have hugely positive
secondary effects,

00:32:54.250 --> 00:32:57.550
it's bring the cost of housing
down by a factor of 10.

00:32:57.550 --> 00:32:59.051
It'd be transformative
for society.

00:32:59.051 --> 00:32:59.550
It's a--

00:32:59.550 --> 00:33:01.370
AUDIENCE: [INAUDIBLE]
government regulation?

00:33:01.370 --> 00:33:01.650
SAM ALTMAN: What?

00:33:01.650 --> 00:33:02.920
AUDIENCE: Eliminate
government regulations.

00:33:02.920 --> 00:33:04.794
SAM ALTMAN: Eliminate
government regulations?

00:33:04.794 --> 00:33:05.860
AUDIENCE: Five stories.

00:33:05.860 --> 00:33:06.568
SAM ALTMAN: Yeah.

00:33:06.568 --> 00:33:11.410
Like, I think many regulations
have good elements.

00:33:11.410 --> 00:33:13.552
I'm not kind of the
libertarian, anarchist,

00:33:13.552 --> 00:33:15.010
all government
regulations are bad.

00:33:15.010 --> 00:33:18.550
But I do think that the sort of
no more building, no building

00:33:18.550 --> 00:33:21.640
taller, has been
disastrous for the state.

00:33:21.640 --> 00:33:26.080
And I think it is the
worst possible allocation

00:33:26.080 --> 00:33:29.650
of resources to have people tie
up every free penny they can

00:33:29.650 --> 00:33:32.139
find into the place they live.

00:33:32.139 --> 00:33:34.180
Think about anything else
we could spend that on.

00:33:34.180 --> 00:33:35.080
Think about what
it would be like

00:33:35.080 --> 00:33:36.520
if people could live
close to where they

00:33:36.520 --> 00:33:37.936
work rather than
commuting an hour

00:33:37.936 --> 00:33:41.350
and a half to get here each day.

00:33:41.350 --> 00:33:44.320
And that is something that
Democrats, Republicans,

00:33:44.320 --> 00:33:47.190
Independents, almost
everybody agrees on,

00:33:47.190 --> 00:33:50.260
or at least there are people
in all of those camps that do.

00:33:53.380 --> 00:33:56.380
So I think like, again, easy
to talk about the divisions.

00:33:56.380 --> 00:34:01.095
It's really good that when
you talk to regular people,

00:34:01.095 --> 00:34:03.220
they all agree on what the
most important issue is.

00:34:03.220 --> 00:34:05.114
And they all want to
fix it in the state.

00:34:05.114 --> 00:34:07.530
JORGE CUETO: What do you think
the tech industries role is

00:34:07.530 --> 00:34:08.654
in government and politics?

00:34:11.760 --> 00:34:14.969
SAM ALTMAN: Look, I think we are
all citizens of this country.

00:34:14.969 --> 00:34:17.100
And we all have, like--

00:34:17.100 --> 00:34:20.429
you know, there is a
right to participate

00:34:20.429 --> 00:34:23.610
in the electoral process.

00:34:23.610 --> 00:34:26.650
And I think there's
also a duty to do that.

00:34:26.650 --> 00:34:32.639
And I think tech is
going through such a boom

00:34:32.639 --> 00:34:34.310
right now it is
easy to say, hey,

00:34:34.310 --> 00:34:38.044
I'm just going to focus
on, like, doing my thing,

00:34:38.044 --> 00:34:39.960
building products, making
money, and whatever.

00:34:39.960 --> 00:34:44.770
And I'm going to let someone
else take care of the rest.

00:34:44.770 --> 00:34:46.630
One of the kind of
disappointing things

00:34:46.630 --> 00:34:48.370
that I have learned
as I've gotten

00:34:48.370 --> 00:34:52.420
to spend more and more time with
increasingly influential people

00:34:52.420 --> 00:34:55.810
and kind of how the world
works is that there is no plan.

00:34:55.810 --> 00:34:57.980
And there is no group
figuring out everything.

00:34:57.980 --> 00:34:59.710
And it's kind of
up to all of us.

00:34:59.710 --> 00:35:00.780
Everyone hopes.

00:35:00.780 --> 00:35:02.440
Like, the reason
conspiracy theories

00:35:02.440 --> 00:35:05.140
are so appealing
is that everybody

00:35:05.140 --> 00:35:07.210
wants there to be a conspiracy.

00:35:07.210 --> 00:35:11.230
You want to think that someone
has a plan, that, you know,

00:35:11.230 --> 00:35:13.030
there's all this
stuff happening.

00:35:13.030 --> 00:35:14.680
But someone's got
a centralized plan,

00:35:14.680 --> 00:35:16.270
and it's all going to work out.

00:35:16.270 --> 00:35:20.590
And I think one of the sad
realizations of growing up

00:35:20.590 --> 00:35:22.450
is that there are no grownups.

00:35:22.450 --> 00:35:24.010
No one has this master plan.

00:35:24.010 --> 00:35:27.550
And if we don't
participate, the thing

00:35:27.550 --> 00:35:28.874
can just go off the rails.

00:35:28.874 --> 00:35:30.290
JORGE CUETO: What
do you think are

00:35:30.290 --> 00:35:33.440
some of the most effective ways
for employees at a large tech

00:35:33.440 --> 00:35:35.120
company, like Google,
to get involved

00:35:35.120 --> 00:35:38.296
in government and politics?

00:35:38.296 --> 00:35:40.670
SAM ALTMAN: Everyone wants an
answer other than this one,

00:35:40.670 --> 00:35:42.840
because it would
be more convenient.

00:35:42.840 --> 00:35:47.950
But I think one of the answers
is to just run for office.

00:35:47.950 --> 00:35:49.390
We have this process.

00:35:49.390 --> 00:35:51.220
We have this system,
set of rules,

00:35:51.220 --> 00:35:53.920
that we've all agreed to.

00:35:53.920 --> 00:35:57.070
And everyone wants some way to
act around the edges of that

00:35:57.070 --> 00:36:00.960
rather than just
participate directly.

00:36:00.960 --> 00:36:01.860
And that's OK.

00:36:01.860 --> 00:36:05.010
There are good things to do.

00:36:05.010 --> 00:36:07.145
But I think just taking
the problem head on,

00:36:07.145 --> 00:36:08.270
not enough people try that.

00:36:10.887 --> 00:36:12.720
JORGE CUETO: And there's
rumors going around

00:36:12.720 --> 00:36:14.719
that Mark Zuckerberg is
going to run for office,

00:36:14.719 --> 00:36:15.845
like, every other week.

00:36:15.845 --> 00:36:17.220
And then there's
also been rumors

00:36:17.220 --> 00:36:19.050
about you running for governor.

00:36:19.050 --> 00:36:23.220
And we've seen tech people
run for office in the past.

00:36:23.220 --> 00:36:26.730
Meg Whitman ran for governor
of California in 2010.

00:36:26.730 --> 00:36:30.120
But she lost by a significant
margin to Jerry Brown.

00:36:30.120 --> 00:36:32.280
So my question is, do you
think that it's actually

00:36:32.280 --> 00:36:35.640
feasible for someone in tech
to run for office and win?

00:36:35.640 --> 00:36:39.330
Or is there too much of this
perception of the tech industry

00:36:39.330 --> 00:36:41.330
as being elitist,
such that people

00:36:41.330 --> 00:36:43.222
won't connect with voters?

00:36:43.222 --> 00:36:44.430
SAM ALTMAN: I think there is.

00:36:44.430 --> 00:36:45.971
I think, at least,
people should try.

00:36:45.971 --> 00:36:48.600
I think the tech industry is
the most hated in the Bay Area.

00:36:48.600 --> 00:36:50.010
And if you go out
throughout the rest

00:36:50.010 --> 00:36:51.330
of the state or the
country, there's

00:36:51.330 --> 00:36:52.620
a lot of people who
think it's really cool.

00:36:52.620 --> 00:36:53.385
They aspire to it.

00:36:53.385 --> 00:36:56.580
They want to participate in it.

00:36:56.580 --> 00:37:01.410
And I think it is easy to get
too negative a view of how

00:37:01.410 --> 00:37:03.090
technology people
are perceived here.

00:37:03.090 --> 00:37:06.355
I don't know if a tech person
can win the presidency.

00:37:06.355 --> 00:37:08.470
I have a feeling we're
going to find out.

00:37:08.470 --> 00:37:11.910
But what I am confident about is
that people from the technology

00:37:11.910 --> 00:37:15.210
industry could start winning
local school board and city

00:37:15.210 --> 00:37:18.460
council and, you know,
congressional seats.

00:37:18.460 --> 00:37:20.460
And that would be a great
thing to start with.

00:37:20.460 --> 00:37:21.990
JORGE CUETO: Is there
any person currently

00:37:21.990 --> 00:37:24.000
living that you would
want to run for president?

00:37:24.000 --> 00:37:25.530
Who's your ideal candidate?

00:37:31.680 --> 00:37:32.430
SAM ALTMAN: Ideal?

00:37:32.430 --> 00:37:35.250
I can't answer that on the fly.

00:37:35.250 --> 00:37:37.180
That's a good question
to think about.

00:37:37.180 --> 00:37:38.070
When you said that,
I was like, oh, I

00:37:38.070 --> 00:37:39.861
should figure out who
that person is and go

00:37:39.861 --> 00:37:42.097
try to convince them to run.

00:37:42.097 --> 00:37:44.180
But I don't have an answer
ready to go in my head.

00:37:44.180 --> 00:37:45.450
JORGE CUETO: OK.

00:37:45.450 --> 00:37:48.750
Well, with that, I'm going to
open up the floor for audience

00:37:48.750 --> 00:37:49.611
questions.

00:37:49.611 --> 00:37:51.360
So if you have a
question, feel free to go

00:37:51.360 --> 00:37:53.180
to the mic in the
back of the room,

00:37:53.180 --> 00:37:57.580
or also submit your
question to the Dory.

00:37:57.580 --> 00:38:00.640
AUDIENCE: [INAUDIBLE] Hi.

00:38:00.640 --> 00:38:02.650
You mentioned the
anecdote about your friend

00:38:02.650 --> 00:38:05.120
being unhappy and
Kim Kardashian.

00:38:05.120 --> 00:38:06.370
And it reminded me of a quote.

00:38:06.370 --> 00:38:10.490
And it said, people
aren't unhappy,

00:38:10.490 --> 00:38:12.150
because they want to be happy.

00:38:12.150 --> 00:38:14.150
They're unhappy, because
they want to be happier

00:38:14.150 --> 00:38:15.800
than other people.

00:38:15.800 --> 00:38:18.890
And you mentioned that
technology may or may not

00:38:18.890 --> 00:38:19.850
be able to solve this.

00:38:19.850 --> 00:38:24.650
So I'd like to hear some of
your ideas or success strategies

00:38:24.650 --> 00:38:28.370
on how to solve this on
a micro and macro level,

00:38:28.370 --> 00:38:30.950
whether it be
delete Twitter to--

00:38:30.950 --> 00:38:34.107
SAM ALTMAN: That's
a good thing to do.

00:38:34.107 --> 00:38:35.190
That's a good thing to do.

00:38:35.190 --> 00:38:35.840
Yeah.

00:38:35.840 --> 00:38:39.490
I deleted all the social
apps from my phone.

00:38:39.490 --> 00:38:41.990
And yeah, I can still check
them on my computer when I want.

00:38:41.990 --> 00:38:44.870
But I don't have
that constant urge

00:38:44.870 --> 00:38:47.930
to push button for
dopamine hit here.

00:38:47.930 --> 00:38:50.930
And that's been really good.

00:38:50.930 --> 00:38:54.874
You know, I think after talking
to a bunch of these people

00:38:54.874 --> 00:38:56.540
about the things that
make people happy,

00:38:56.540 --> 00:38:58.040
there's the obvious
stuff, which is

00:38:58.040 --> 00:39:02.630
like spend time with
loved ones that everyone

00:39:02.630 --> 00:39:04.094
knows even if they don't do.

00:39:04.094 --> 00:39:05.510
But there's a bunch
of things that

00:39:05.510 --> 00:39:13.850
are less obvious to me that seem
to have huge measured effects.

00:39:13.850 --> 00:39:16.865
Like taking time to think
about the things that went

00:39:16.865 --> 00:39:18.740
well as opposed to things
you're upset about,

00:39:18.740 --> 00:39:24.230
or going for a walk outside
everyday no matter what.

00:39:24.230 --> 00:39:29.950
And I think as we get sort
of to abundance and unlimited

00:39:29.950 --> 00:39:34.100
resources, this is going to
be a more important topic.

00:39:34.100 --> 00:39:38.759
What I would encourage you to do
is to just like start reading.

00:39:38.759 --> 00:39:40.550
There's a lot of
literature on the subject.

00:39:40.550 --> 00:39:42.950
No one seems to care about
it enough to spend time.

00:39:42.950 --> 00:39:44.510
But I would say
start reading it.

00:39:44.510 --> 00:39:48.228
And hopefully, I'll finish
my blog post on it soon.

00:39:48.228 --> 00:39:51.149
Thank you.

00:39:51.149 --> 00:39:51.940
AUDIENCE: Hey, Sam.

00:39:51.940 --> 00:39:52.780
Thanks for coming.

00:39:52.780 --> 00:39:55.950
I wanted to ask a question
about the larger landscape of VC

00:39:55.950 --> 00:39:56.880
in general.

00:39:56.880 --> 00:39:59.584
I heard a rumor that YC is
looking to a growth fund.

00:39:59.584 --> 00:40:01.500
I think it broke on
TechCrunch like month ago,

00:40:01.500 --> 00:40:03.710
like potentially
raising $1 billion fund.

00:40:03.710 --> 00:40:06.000
I wanted to get your
thoughts on growth

00:40:06.000 --> 00:40:10.680
versus venture and how you see
growth evolving for the Bay

00:40:10.680 --> 00:40:11.630
Area in general.

00:40:11.630 --> 00:40:13.410
You see private
companies staying

00:40:13.410 --> 00:40:15.070
private longer and longer.

00:40:15.070 --> 00:40:17.100
SAM ALTMAN: So we already
have a growth fund.

00:40:17.100 --> 00:40:19.660
It's called YC Continuity.

00:40:19.660 --> 00:40:23.000
We raised the first one in 2015.

00:40:23.000 --> 00:40:25.140
It's about halfway invested.

00:40:25.140 --> 00:40:30.030
And it basically follows on
in capital in YC companies.

00:40:30.030 --> 00:40:32.670
One of the things that I
wanted to do after I joined

00:40:32.670 --> 00:40:37.260
YC was start to fund
hard tech companies,

00:40:37.260 --> 00:40:40.650
nuclear fusion, something like
biology quantum computing,

00:40:40.650 --> 00:40:41.940
self-driving cars-- long list.

00:40:44.950 --> 00:40:47.490
And one of the good
things that I realized

00:40:47.490 --> 00:40:50.110
is that nobody else was
funding those companies.

00:40:50.110 --> 00:40:51.879
And so we could have our pick.

00:40:51.879 --> 00:40:53.420
One of the bad things
that I realized

00:40:53.420 --> 00:40:55.874
is nobody else was
funding those companies.

00:40:55.874 --> 00:40:57.540
And so we could fund
them all we wanted,

00:40:57.540 --> 00:40:58.998
and there was no
follow on capital.

00:41:01.344 --> 00:41:03.510
So that was actually the
genesis for our growth fund

00:41:03.510 --> 00:41:05.370
is that there was this
class of companies

00:41:05.370 --> 00:41:08.040
that I think are really
important and really valuable.

00:41:08.040 --> 00:41:09.630
And they weren't getting funded.

00:41:09.630 --> 00:41:11.310
Since then, we've
expanded it, and we

00:41:11.310 --> 00:41:14.070
fund lots of other companies.

00:41:14.070 --> 00:41:17.800
I will certainly
say that there is

00:41:17.800 --> 00:41:20.200
no shortage of growth
capital for software

00:41:20.200 --> 00:41:22.402
companies in Silicon Valley.

00:41:22.402 --> 00:41:23.860
You know, it may
have gotten harder

00:41:23.860 --> 00:41:25.360
to raise a seed or an A round.

00:41:25.360 --> 00:41:27.100
It probably has somewhat.

00:41:27.100 --> 00:41:30.412
But if you have things
working, like when

00:41:30.412 --> 00:41:31.870
you want to go
raise a B or C round

00:41:31.870 --> 00:41:34.057
and you have this beautiful
exponential growth,

00:41:34.057 --> 00:41:36.640
you will not be able to keep up
with the number of term sheets

00:41:36.640 --> 00:41:37.390
you're getting.

00:41:37.390 --> 00:41:39.670
That is the stage
that I think just

00:41:39.670 --> 00:41:42.640
has a huge amount of
capital allocated to it.

00:41:42.640 --> 00:41:44.950
As companies are
staying private longer,

00:41:44.950 --> 00:41:47.770
as public market investors
have a harder and harder time

00:41:47.770 --> 00:41:51.550
finding alpha, they
are more and more

00:41:51.550 --> 00:41:52.850
willing to do stuff like this.

00:41:52.850 --> 00:41:56.650
So that part of
the market is not

00:41:56.650 --> 00:41:57.973
suffering, at least, not yet.

00:41:57.973 --> 00:41:58.880
AUDIENCE: Thanks.

00:41:58.880 --> 00:41:59.950
SAM ALTMAN: Sure.

00:41:59.950 --> 00:42:03.130
JORGE CUETO: Let me do a
question from the Dory.

00:42:03.130 --> 00:42:06.190
We have a question about
universal basic income

00:42:06.190 --> 00:42:11.640
and how Google has been
supporting GiveDirectly's

00:42:11.640 --> 00:42:14.350
research on UBI in Africa.

00:42:14.350 --> 00:42:16.780
And you're a proponent
here in the US.

00:42:16.780 --> 00:42:18.740
So what are your
thoughts on how,

00:42:18.740 --> 00:42:21.790
when, and where the universal
basic income approach

00:42:21.790 --> 00:42:23.240
might be most effective?

00:42:23.240 --> 00:42:24.940
SAM ALTMAN: Yeah.

00:42:24.940 --> 00:42:26.590
I don't think universal
basic income is

00:42:26.590 --> 00:42:28.296
the solution to all problems.

00:42:28.296 --> 00:42:29.920
I think, in fact, of
the bigger problem

00:42:29.920 --> 00:42:31.669
that we were talking
about earlier of what

00:42:31.669 --> 00:42:34.960
makes people happy and fulfilled
and feel needed, and valued,

00:42:34.960 --> 00:42:37.480
and have meaning
in their lives, it

00:42:37.480 --> 00:42:39.550
is not sufficient to
solve that problem.

00:42:39.550 --> 00:42:41.050
And I don't think
it will replace

00:42:41.050 --> 00:42:43.341
the entire other social safety
net, like the people who

00:42:43.341 --> 00:42:45.250
are like eliminate
everything else, you know,

00:42:45.250 --> 00:42:48.560
no health care, no schools,
no minimum wage, basic income.

00:42:48.560 --> 00:42:50.140
I don't believe in that either.

00:42:50.140 --> 00:42:54.540
But I do think that if
we do all of our jobs,

00:42:54.540 --> 00:42:57.140
if we Silicon Valley
do all of our jobs,

00:42:57.140 --> 00:43:00.100
we will create more wealth
than the world's ever

00:43:00.100 --> 00:43:02.350
seen before and less jobs.

00:43:02.350 --> 00:43:07.120
And in that world, I think
there is a moral obligation

00:43:07.120 --> 00:43:08.650
to eliminate poverty.

00:43:08.650 --> 00:43:11.260
I think poverty is a,
obviously, very bad thing.

00:43:11.260 --> 00:43:14.410
But it is worse even
than people realize.

00:43:14.410 --> 00:43:18.070
If you look at the studies on
the long-term psychological

00:43:18.070 --> 00:43:22.150
damage that living in
poverty does to people

00:43:22.150 --> 00:43:25.582
and how it means that you never
get a chance to really invest

00:43:25.582 --> 00:43:27.790
in your own future, because
you're always just trying

00:43:27.790 --> 00:43:29.706
to survive, I think
there's just a huge amount

00:43:29.706 --> 00:43:31.160
of wasted potential.

00:43:31.160 --> 00:43:34.902
And so you know, there are a
lot of arguments about UBI.

00:43:34.902 --> 00:43:35.860
This was another thing.

00:43:35.860 --> 00:43:38.150
Like, when we started
this couple of years ago,

00:43:38.150 --> 00:43:41.200
I did not predict
at all that this

00:43:41.200 --> 00:43:43.690
was going to ignite into
this big national debate.

00:43:43.690 --> 00:43:46.480
It was like, we were just
trying to hire a researcher.

00:43:46.480 --> 00:43:49.120
You know, like, maybe
this is a good idea.

00:43:49.120 --> 00:43:50.329
Can we just do a project?

00:43:50.329 --> 00:43:51.370
And it's just been like--

00:43:55.360 --> 00:43:57.640
so I don't think
I quite understand

00:43:57.640 --> 00:44:00.400
why it has become such a
national issue so quickly.

00:44:00.400 --> 00:44:06.100
But I do think that longer
term, if we have the ability

00:44:06.100 --> 00:44:09.700
to eliminate poverty,
we will get a lot more

00:44:09.700 --> 00:44:11.290
out of the world as a whole.

00:44:11.290 --> 00:44:14.484
JORGE CUETO: As you started
the experiment in East Bay,

00:44:14.484 --> 00:44:16.150
have you noticed
anything that maybe you

00:44:16.150 --> 00:44:19.330
didn't expect, or has it
changed your attitude towards it

00:44:19.330 --> 00:44:20.260
in any way?

00:44:20.260 --> 00:44:22.218
SAM ALTMAN: You know,
we're like seven or eight

00:44:22.218 --> 00:44:23.811
months into a five year study.

00:44:23.811 --> 00:44:24.310
So not yet.

00:44:24.310 --> 00:44:26.780
JORGE CUETO: OK.

00:44:26.780 --> 00:44:27.530
AUDIENCE: Hi, Sam.

00:44:27.530 --> 00:44:29.480
I'm thinking more in
general of founders,

00:44:29.480 --> 00:44:31.280
but this could be general, too.

00:44:31.280 --> 00:44:33.860
But what advice do
you frequently give,

00:44:33.860 --> 00:44:38.645
but you find most people never
actually follow through on?

00:44:38.645 --> 00:44:40.270
SAM ALTMAN: I mean,
no one ever listens

00:44:40.270 --> 00:44:43.120
to any advice of any sort.

00:44:43.120 --> 00:44:44.470
So, all of it.

00:44:48.857 --> 00:44:51.350
I think the thing that people--

00:44:51.350 --> 00:44:54.890
well, the piece of
advice that people later

00:44:54.890 --> 00:44:57.470
say they wish they
had listened to

00:44:57.470 --> 00:45:00.920
is that it is very easy to get
sucked into a path in life.

00:45:00.920 --> 00:45:03.815
And you can do things
like, say, oh, I want

00:45:03.815 --> 00:45:04.940
to start a startup someday.

00:45:04.940 --> 00:45:08.120
Or, oh, I want to be an
AI researcher someday.

00:45:08.120 --> 00:45:11.150
But first, I'm going to go do
this other job to, you know,

00:45:11.150 --> 00:45:13.700
make money and gain
experience and whatever.

00:45:13.700 --> 00:45:16.270
And it's so easy to
get sucked into a path

00:45:16.270 --> 00:45:18.770
where you spend your entire
life doing something that is not

00:45:18.770 --> 00:45:19.895
really what you want to do.

00:45:23.332 --> 00:45:24.790
That's probably
the piece of advice

00:45:24.790 --> 00:45:27.400
that I have given people that
they have most often come

00:45:27.400 --> 00:45:29.590
to me, like, five
years later and said,

00:45:29.590 --> 00:45:31.667
I really wish I had
listened to that.

00:45:31.667 --> 00:45:32.500
AUDIENCE: Thank you.

00:45:32.500 --> 00:45:34.509
SAM ALTMAN: Sure.

00:45:34.509 --> 00:45:36.300
AUDIENCE: I'm curious
to hear your thoughts

00:45:36.300 --> 00:45:37.860
on the relative
rates of progress

00:45:37.860 --> 00:45:40.932
in AI between China
and everywhere else.

00:45:40.932 --> 00:45:42.390
So you mentioned
nobody has a plan,

00:45:42.390 --> 00:45:45.780
but China has a plan for
state level centralized

00:45:45.780 --> 00:45:46.770
investment in AI.

00:45:46.770 --> 00:45:48.844
Whereas, Elon Musk is
calling for regulation

00:45:48.844 --> 00:45:49.760
that would slow down--

00:45:49.760 --> 00:45:50.468
SAM ALTMAN: Yeah.

00:45:50.468 --> 00:45:52.752
AUDIENCE: ----[INAUDIBLE]
everywhere else.

00:45:52.752 --> 00:45:54.460
SAM ALTMAN: China does
plan, that's true.

00:45:57.652 --> 00:45:58.610
It doesn't always work.

00:45:58.610 --> 00:46:00.844
They don't always have
global coordination.

00:46:00.844 --> 00:46:03.010
You know, like right now
we're in this kind of world

00:46:03.010 --> 00:46:05.560
where it seems like you
have the internet in China,

00:46:05.560 --> 00:46:07.870
and the internet in
the rest of the world.

00:46:07.870 --> 00:46:09.850
Maybe they have a plan
for their piece of it.

00:46:14.850 --> 00:46:19.440
I don't honestly know how
far along China is with AI.

00:46:19.440 --> 00:46:21.960
And I don't think anybody else
honestly really does either.

00:46:26.570 --> 00:46:28.780
I think it would be bad
to get into an arms race

00:46:28.780 --> 00:46:29.660
with China over AI.

00:46:29.660 --> 00:46:31.660
I think that's something
we should try to avoid.

00:46:34.180 --> 00:46:37.720
And I think there is
a real opportunity,

00:46:37.720 --> 00:46:43.070
although we may not have the
leaders in place to do it

00:46:43.070 --> 00:46:48.830
right now, to make this a joint
kind of worldwide project,

00:46:48.830 --> 00:46:52.466
rather than another space
race or nuclear arms race.

00:46:52.466 --> 00:46:55.250
Thanks.

00:46:55.250 --> 00:46:56.000
AUDIENCE: Hi, Sam.

00:46:56.000 --> 00:46:57.999
I was wondering if you
could expand a little bit

00:46:57.999 --> 00:46:59.704
on funding hard tech.

00:46:59.704 --> 00:47:02.120
Because I think there's a big
problem with private capital

00:47:02.120 --> 00:47:02.780
allocation.

00:47:02.780 --> 00:47:06.080
And you know, like, $70 billion
was put into VC last year,

00:47:06.080 --> 00:47:09.470
which is a drop in the bucket in
terms of worldwide investment.

00:47:09.470 --> 00:47:12.500
So do you see YC maybe
raising an $100 billion

00:47:12.500 --> 00:47:13.610
fund in the future?

00:47:13.610 --> 00:47:14.862
SAM ALTMAN: Yeah, maybe.

00:47:14.862 --> 00:47:16.850
AUDIENCE: Like, yeah,
how can we solve this?

00:47:16.850 --> 00:47:21.140
SAM ALTMAN: I think capital
is allocated really badly.

00:47:21.140 --> 00:47:24.410
And unfortunately,
there are huge efforts--

00:47:24.410 --> 00:47:27.470
like if you have this
really valuable thing which

00:47:27.470 --> 00:47:29.990
is you get access to
invest in great startups

00:47:29.990 --> 00:47:32.930
and most of the world doesn't,
then there will obviously

00:47:32.930 --> 00:47:34.880
be a super return there.

00:47:34.880 --> 00:47:37.340
And you're going to work
really hard to protect that.

00:47:37.340 --> 00:47:44.547
And so I think expanding
access to invest in startups--

00:47:44.547 --> 00:47:47.130
which is happening, but slowly--
is a really good thing to do.

00:47:47.130 --> 00:47:52.580
And I think, you know,
equity crowdfunding

00:47:52.580 --> 00:47:56.130
we're still in the early days
of, but is an important trend.

00:47:56.130 --> 00:48:00.560
And I think that will change
the capital allocation.

00:48:00.560 --> 00:48:03.110
I think you're seeing startups
in all these different ways

00:48:03.110 --> 00:48:06.354
go to non-traditional funders.

00:48:06.354 --> 00:48:07.520
And that's been really good.

00:48:07.520 --> 00:48:09.260
We first saw that with
consumer hardware.

00:48:09.260 --> 00:48:12.625
We're now seeing that in
a lot of other places.

00:48:12.625 --> 00:48:14.750
You know, I think the other
thing that's happening,

00:48:14.750 --> 00:48:16.040
finally, is there's just--

00:48:16.040 --> 00:48:17.560
and there's bad to this, too.

00:48:17.560 --> 00:48:20.860
But there's just a huge
amount more capital coming in

00:48:20.860 --> 00:48:22.750
to fund early stage
private companies

00:48:22.750 --> 00:48:24.670
or mid-stage private companies.

00:48:24.670 --> 00:48:29.110
And I expect that in a world
where interest rates stay

00:48:29.110 --> 00:48:31.542
even close to as
low as they are,

00:48:31.542 --> 00:48:33.500
we are in the very early
innings of that trend.

00:48:33.500 --> 00:48:38.500
So my general model is that
the world's a very complex

00:48:38.500 --> 00:48:39.520
financial system.

00:48:39.520 --> 00:48:43.090
Capital sloshes around
looking for the best return.

00:48:43.090 --> 00:48:46.570
There are periods where there's
a really outperforming return,

00:48:46.570 --> 00:48:48.362
but then more capital
gets allocated there.

00:48:48.362 --> 00:48:49.778
And I think we're
seeing that now.

00:48:49.778 --> 00:48:51.880
AUDIENCE: But do you think
that a nuclear fusion

00:48:51.880 --> 00:48:54.072
company could raise
$100 million in an ICO

00:48:54.072 --> 00:48:55.030
or something like that?

00:48:55.030 --> 00:48:55.738
SAM ALTMAN: Yeah.

00:48:55.738 --> 00:48:59.720
Or maybe I wouldn't suggest
that they do an ICO.

00:48:59.720 --> 00:49:02.200
But I do think they
can raise $100 million.

00:49:02.200 --> 00:49:04.697
And I don't think they
could have three years ago.

00:49:04.697 --> 00:49:07.030
And I think that's a really
positive trend for the world

00:49:07.030 --> 00:49:11.017
that this stuff is now at
least somewhat possible.

00:49:11.017 --> 00:49:12.100
AUDIENCE: Awesome, thanks.

00:49:12.100 --> 00:49:12.550
SAM ALTMAN: Sure.

00:49:12.550 --> 00:49:14.130
JORGE CUETO: Another
Dory question--

00:49:14.130 --> 00:49:16.810
what are the most common reasons
that qualified candidates get

00:49:16.810 --> 00:49:17.920
rejected from YC?

00:49:24.368 --> 00:49:29.400
SAM ALTMAN: Well, there
is one particular thing

00:49:29.400 --> 00:49:31.770
that happens a lot with
super talented people

00:49:31.770 --> 00:49:33.270
from large tech companies.

00:49:33.270 --> 00:49:35.730
So since I'm here,
I'll mention that one,

00:49:35.730 --> 00:49:39.270
which is you are a really great
person, really talented, really

00:49:39.270 --> 00:49:41.312
smart, really driven.

00:49:41.312 --> 00:49:43.770
And you don't have a good idea,
or you don't have any idea.

00:49:43.770 --> 00:49:46.080
So you make up a bad one.

00:49:46.080 --> 00:49:51.230
And there's this,
like, myth about

00:49:51.230 --> 00:49:54.290
startups that the idea
doesn't matter at all.

00:49:54.290 --> 00:49:58.700
And I think there's existential
proof that that's not true.

00:49:58.700 --> 00:50:02.680
And I think there is
this particular failure

00:50:02.680 --> 00:50:04.430
mode of people from
the big tech companies

00:50:04.430 --> 00:50:06.096
which is like, hey,
I'm really talented.

00:50:06.096 --> 00:50:07.250
I'm like, yes, you are.

00:50:07.250 --> 00:50:08.600
So I'm going to start a company.

00:50:08.600 --> 00:50:10.502
And I'll figure out
what to do later.

00:50:10.502 --> 00:50:11.960
And I think that
isn't usually what

00:50:11.960 --> 00:50:13.650
produces the great companies.

00:50:13.650 --> 00:50:20.970
And so we have learned
that it's much, much better

00:50:20.970 --> 00:50:25.960
if you're otherwise a qualified
candidate to have a good idea.

00:50:25.960 --> 00:50:28.500
And I think that would be the
common failure case for people

00:50:28.500 --> 00:50:29.375
coming out of Google.

00:50:31.644 --> 00:50:32.435
AUDIENCE: Hey, Sam.

00:50:32.435 --> 00:50:33.101
SAM ALTMAN: Hey.

00:50:33.101 --> 00:50:35.306
AUDIENCE: So what are your
thoughts about, you know,

00:50:35.306 --> 00:50:37.180
where you're going to
go or what you're going

00:50:37.180 --> 00:50:38.770
to do with your life after YC?

00:50:38.770 --> 00:50:39.930
Or does that come across?

00:50:39.930 --> 00:50:41.930
Like, are you going to take
a political office seriously?

00:50:41.930 --> 00:50:43.346
SAM ALTMAN: You
know, I don't even

00:50:43.346 --> 00:50:46.860
know what I'm going
to do in three weeks.

00:50:46.860 --> 00:50:52.970
I plan to run YC for
a really long time.

00:50:52.970 --> 00:50:56.340
I do plan to get continually
more involved with politics.

00:50:56.340 --> 00:50:59.975
But I'd like to support others,
not run for office myself,

00:50:59.975 --> 00:51:01.100
at least not any time soon.

00:51:04.190 --> 00:51:08.174
Like, I am a big believer in
do the things that you think

00:51:08.174 --> 00:51:09.590
about in the shower
in the morning

00:51:09.590 --> 00:51:10.460
when you can think
about anything

00:51:10.460 --> 00:51:12.440
you want and your kind
of mind is just off.

00:51:12.440 --> 00:51:14.450
And the things I keep
coming back to are

00:51:14.450 --> 00:51:16.820
how can I make YC
like 100 times bigger?

00:51:16.820 --> 00:51:20.090
How can I make sure that
the arrival of AI goes well?

00:51:20.090 --> 00:51:22.730
And in the short term, how can
I help our political system

00:51:22.730 --> 00:51:26.490
from going off the rails
or any more off the rails

00:51:26.490 --> 00:51:28.010
than it already has?

00:51:28.010 --> 00:51:31.310
So you know, I kind of plan to
keep working on those things

00:51:31.310 --> 00:51:34.972
for, hopefully, you
know, a really long time.

00:51:34.972 --> 00:51:35.930
AUDIENCE: Good to hear.

00:51:35.930 --> 00:51:37.400
Thanks.

00:51:37.400 --> 00:51:40.200
AUDIENCE: Hi, thank
you for being here.

00:51:40.200 --> 00:51:44.010
I have a question
about the general way

00:51:44.010 --> 00:51:45.600
Silicon Valley works.

00:51:45.600 --> 00:51:47.250
Because it seems
like it's a system

00:51:47.250 --> 00:51:50.730
in which you have the most
talented minds dealing

00:51:50.730 --> 00:51:52.854
with maybe not the
most important issues.

00:51:52.854 --> 00:51:54.520
Like, you have the
sharpest mind working

00:51:54.520 --> 00:51:56.610
on social apps or mobile apps.

00:51:56.610 --> 00:51:58.510
But we still don't
have a cure for cancer,

00:51:58.510 --> 00:52:02.730
or we still don't have
another way of having energy,

00:52:02.730 --> 00:52:06.370
or really important stuff
that the world should handle.

00:52:06.370 --> 00:52:06.930
And

00:52:06.930 --> 00:52:08.916
So my question is, do
you agree with that?

00:52:08.916 --> 00:52:10.290
What is your
perspective on that?

00:52:10.290 --> 00:52:13.350
And how do you think maybe
this can be addressed?

00:52:13.350 --> 00:52:14.850
SAM ALTMAN: I don't
agree with that.

00:52:14.850 --> 00:52:19.250
I think I used to view my
job as a capital allocator.

00:52:19.250 --> 00:52:21.470
And now, I view my job
as a talent allocator.

00:52:21.470 --> 00:52:24.380
So most of my day
is spent meeting

00:52:24.380 --> 00:52:26.930
with really smart people
and trying to convince them

00:52:26.930 --> 00:52:28.850
to work on important problems.

00:52:28.850 --> 00:52:34.680
And you know, I
think in 2007, what

00:52:34.680 --> 00:52:36.960
everyone said is the best
minds of our generation used

00:52:36.960 --> 00:52:39.900
to build spaceships,
and now they're

00:52:39.900 --> 00:52:41.880
like moving numbers
around on Wall Street.

00:52:41.880 --> 00:52:44.580
And in 2017, what people
say is the best minds

00:52:44.580 --> 00:52:47.190
of our generation used
to build spaceships.

00:52:47.190 --> 00:52:49.080
And now they get
me to click on ads.

00:52:49.080 --> 00:52:51.830
And you know, there's
always some truth to that.

00:52:51.830 --> 00:52:54.420
And there's always
someone to pick on there.

00:52:54.420 --> 00:52:57.790
But I think OpenAI has
some of the smartest

00:52:57.790 --> 00:53:00.220
minds in our world of
our generation working

00:53:00.220 --> 00:53:01.404
on having AI go well.

00:53:01.404 --> 00:53:03.820
I think Helion, which is that
fusion company I mentioned--

00:53:03.820 --> 00:53:05.986
some of the smartest minds
of our generation working

00:53:05.986 --> 00:53:07.580
on nuclear fusion.

00:53:07.580 --> 00:53:09.880
YC, I think, last time I
counted funded eight companies

00:53:09.880 --> 00:53:12.550
working on a cure for cancer,
incredibly talented people--

00:53:12.550 --> 00:53:14.400
or cures for cancer--

00:53:14.400 --> 00:53:16.804
cancers.

00:53:16.804 --> 00:53:18.220
I think you can
always say there's

00:53:18.220 --> 00:53:21.040
all these people like working
on bad stuff or stuff that

00:53:21.040 --> 00:53:22.049
doesn't matter.

00:53:22.049 --> 00:53:23.590
But you know, I
think Google is still

00:53:23.590 --> 00:53:25.575
like a really great
thing for the world.

00:53:25.575 --> 00:53:26.950
AUDIENCE: This is
why we're here.

00:53:26.950 --> 00:53:27.550
SAM ALTMAN: Yeah.

00:53:27.550 --> 00:53:28.674
And I think it does matter.

00:53:28.674 --> 00:53:30.910
I think it's always easy
to pick on people and say,

00:53:30.910 --> 00:53:32.830
you're not working
on a cure for cancer.

00:53:32.830 --> 00:53:34.760
You know, you're
wasting your time.

00:53:34.760 --> 00:53:37.490
But you're making
this thing better

00:53:37.490 --> 00:53:38.780
that people use every day.

00:53:38.780 --> 00:53:41.917
And their lives would be a
lot worse if it went away.

00:53:41.917 --> 00:53:43.500
And you're making
it better every day.

00:53:43.500 --> 00:53:47.510
And I think it's really
easy to pick on people

00:53:47.510 --> 00:53:51.352
and say, you know, that person's
not spending her time right.

00:53:51.352 --> 00:53:53.560
And I think it always says
more about the person that

00:53:53.560 --> 00:53:56.269
says that than the person
they're pointing to.

00:53:56.269 --> 00:53:58.810
And if you're doing something
useful for the world, if you're

00:53:58.810 --> 00:54:01.140
doing something you
enjoy, even if you're

00:54:01.140 --> 00:54:03.640
having a small impact, but on
a product that a lot of people

00:54:03.640 --> 00:54:06.530
really use and love, I think
that's really valuable.

00:54:06.530 --> 00:54:08.620
And I think the people
who say this generally

00:54:08.620 --> 00:54:10.036
have a lot of
insecurity about how

00:54:10.036 --> 00:54:11.270
they're spending their time.

00:54:11.270 --> 00:54:13.395
AUDIENCE: So you don't
think like Silicon Valley is

00:54:13.395 --> 00:54:17.300
avoiding the tough problems?

00:54:17.300 --> 00:54:19.995
SAM ALTMAN: I invite you to
come sit in my office for a day

00:54:19.995 --> 00:54:21.620
and listen to the
people coming through

00:54:21.620 --> 00:54:22.700
and what they're working on.

00:54:22.700 --> 00:54:23.910
I really don't think that, no.

00:54:23.910 --> 00:54:24.400
AUDIENCE: OK.

00:54:24.400 --> 00:54:24.900
Cool.

00:54:24.900 --> 00:54:27.260
Thank you.

00:54:27.260 --> 00:54:28.530
AUDIENCE: Hey, Sam.

00:54:28.530 --> 00:54:32.220
Seeing that you're a largely
influential public figure,

00:54:32.220 --> 00:54:33.600
could you speak
to the privileges

00:54:33.600 --> 00:54:36.870
that might have led to that and
helped you with your success

00:54:36.870 --> 00:54:42.000
and how you may be
good or bad ally?

00:54:42.000 --> 00:54:45.360
And then also, seeing that a
lot of our consumer products

00:54:45.360 --> 00:54:50.190
are largely bias, specifically
to the people who create them,

00:54:50.190 --> 00:54:53.670
which is largely the Silicon
Valley engineers like myself

00:54:53.670 --> 00:54:56.400
and others in this
room, how do you

00:54:56.400 --> 00:55:00.090
keep your ideas and thoughts
diverse in addressing

00:55:00.090 --> 00:55:02.470
intersectional needs?

00:55:02.470 --> 00:55:03.950
SAM ALTMAN: Yeah,
great question.

00:55:03.950 --> 00:55:07.970
I basically had like nearly
all of the possible privileges.

00:55:07.970 --> 00:55:10.310
I had wonderful, loving parents.

00:55:10.310 --> 00:55:12.200
I grew up in a safe house.

00:55:12.200 --> 00:55:14.360
I'm a white guy.

00:55:14.360 --> 00:55:16.790
We had enough money that
I was able to pursue

00:55:16.790 --> 00:55:21.020
the things that I was interested
in and go to great college.

00:55:21.020 --> 00:55:23.090
And it's never
lost on me how if I

00:55:23.090 --> 00:55:26.880
had been born a mile
in a different place

00:55:26.880 --> 00:55:30.630
to a different family, different
skin color, different gender,

00:55:30.630 --> 00:55:32.160
I wouldn't be where I am now.

00:55:32.160 --> 00:55:35.160
I view that as an
obligation to try

00:55:35.160 --> 00:55:38.550
to make the world more
just going forward.

00:55:38.550 --> 00:55:43.394
I think anyone who is really
successful and doesn't--

00:55:43.394 --> 00:55:45.060
I think anyone should
try to do the best

00:55:45.060 --> 00:55:46.810
they can with whatever
hand they're dealt.

00:55:46.810 --> 00:55:50.580
But if you're dealt, you know,
like four aces, and you win,

00:55:50.580 --> 00:55:52.440
then I think you have
an extra obligation

00:55:52.440 --> 00:55:55.300
to try to sort of make
the world a little better.

00:55:55.300 --> 00:55:58.530
So I try to be really thankful
of what everyone's done

00:55:58.530 --> 00:56:02.890
that has allowed me to do this.

00:56:02.890 --> 00:56:07.480
I also try to figure out
how to pay that forward.

00:56:07.480 --> 00:56:11.210
And I think anyone who
is really successful

00:56:11.210 --> 00:56:13.220
or almost anyone who
is really successful

00:56:13.220 --> 00:56:16.950
has like privilege, luck,
skill, and hard work.

00:56:16.950 --> 00:56:18.800
And I think people
who try to say

00:56:18.800 --> 00:56:22.820
it's just one or just the
other all tend to be wrong.

00:56:22.820 --> 00:56:24.390
So I'm thankful for that.

00:56:24.390 --> 00:56:26.240
And I try to pay that forward.

00:56:28.790 --> 00:56:31.610
In terms of biases
in the product,

00:56:31.610 --> 00:56:35.120
I think this is one of the
reasons that diverse teams are

00:56:35.120 --> 00:56:38.210
most important, the
moral question aside.

00:56:38.210 --> 00:56:40.760
The consumer
products, the teams,

00:56:40.760 --> 00:56:44.540
the companies that I think have
done the best job addressing

00:56:44.540 --> 00:56:47.300
this head on have had
a very diverse set

00:56:47.300 --> 00:56:48.710
of voices around the table.

00:56:48.710 --> 00:56:51.410
And I think that is always
the strategy I recommend,

00:56:51.410 --> 00:56:53.952
because that's the only one
I've seen consistently work.

00:56:53.952 --> 00:56:55.039
AUDIENCE: Cool, thank you.

00:56:55.039 --> 00:56:56.330
JORGE CUETO: We're on time now.

00:56:56.330 --> 00:56:58.901
But if you can do these last
three questions quickly,

00:56:58.901 --> 00:56:59.900
we can get through them.

00:56:59.900 --> 00:57:00.830
SAM ALTMAN: Sure.

00:57:00.830 --> 00:57:02.360
AUDIENCE: Thank you
for coming, Sam.

00:57:02.360 --> 00:57:03.800
My question is on
machine learning

00:57:03.800 --> 00:57:06.920
and artificial
intelligence companies.

00:57:06.920 --> 00:57:09.590
It seems to me that the
scarce or valuable thing

00:57:09.590 --> 00:57:12.320
is data in those areas
and less so the algorithm,

00:57:12.320 --> 00:57:13.824
because they're
mostly open source.

00:57:13.824 --> 00:57:16.490
So I was wondering how you think
about that when getting pitches

00:57:16.490 --> 00:57:17.462
from companies.

00:57:17.462 --> 00:57:18.920
SAM ALTMAN: I used
to believe that.

00:57:18.920 --> 00:57:22.330
I now believe that it's going
to be compute, not data.

00:57:22.330 --> 00:57:24.410
I think data is
important, but there

00:57:24.410 --> 00:57:26.520
will be a lot of it available.

00:57:26.520 --> 00:57:30.752
And just my own
experience with OpenAI,

00:57:30.752 --> 00:57:32.210
to really be at
the forefront here,

00:57:32.210 --> 00:57:34.910
you just need massive
amounts of compute.

00:57:34.910 --> 00:57:37.055
And so I used to ask
companies how they're

00:57:37.055 --> 00:57:38.180
going to get a lot of data.

00:57:38.180 --> 00:57:40.520
Now, I ask them how they're
going to get a lot of compute.

00:57:40.520 --> 00:57:40.940
AUDIENCE: OK.

00:57:40.940 --> 00:57:41.439
Thank you.

00:57:41.439 --> 00:57:42.299
SAM ALTMAN: Sure.

00:57:42.299 --> 00:57:43.090
AUDIENCE: Hey, Sam.

00:57:43.090 --> 00:57:43.980
Thanks for coming.

00:57:43.980 --> 00:57:45.480
I know you were
talking about people

00:57:45.480 --> 00:57:47.370
are becoming unhappier
and the world [INAUDIBLE]

00:57:47.370 --> 00:57:48.745
problem, all these
kind of things

00:57:48.745 --> 00:57:51.990
that are kind of quantifiable,
like physical needs of people.

00:57:51.990 --> 00:57:54.300
Have you or had companies
that come to you kind of try

00:57:54.300 --> 00:57:55.925
to solve the spiritual
needs of people?

00:57:55.925 --> 00:57:58.170
Because we can identify
the physical needs,

00:57:58.170 --> 00:58:00.720
but what about like spiritual
needs or research or companies?

00:58:00.720 --> 00:58:02.580
SAM ALTMAN: We have had a few.

00:58:02.580 --> 00:58:04.650
None of them have
really worked yet.

00:58:04.650 --> 00:58:06.600
But you know, one that
stuck out of memory

00:58:06.600 --> 00:58:10.610
was a company came
to us and said,

00:58:10.610 --> 00:58:14.020
churches, you know,
organized religion

00:58:14.020 --> 00:58:17.290
had this really
important effect that

00:58:17.290 --> 00:58:20.020
was totally separate
from the religion

00:58:20.020 --> 00:58:23.080
itself, which was this
tight-knit community.

00:58:23.080 --> 00:58:25.300
And how do you build
that in a world

00:58:25.300 --> 00:58:27.370
where most people or a
declining number of people

00:58:27.370 --> 00:58:29.392
believe in religion
and go to church?

00:58:29.392 --> 00:58:31.600
And so I think there are
people thinking about things

00:58:31.600 --> 00:58:34.030
like that, which are sort
of these non-obvious attacks

00:58:34.030 --> 00:58:35.600
on the problem that
are interesting,

00:58:35.600 --> 00:58:37.900
but none that I could
yet point to as here's

00:58:37.900 --> 00:58:42.000
this thing that's
really worked well.

00:58:42.000 --> 00:58:42.890
AUDIENCE: Hey, Sam.

00:58:42.890 --> 00:58:46.800
So you partnered with the
ACLU earlier this year,

00:58:46.800 --> 00:58:48.930
which you got
mixed reactions to.

00:58:48.930 --> 00:58:52.560
I'm wondering what you learned
from that whole experience,

00:58:52.560 --> 00:58:54.560
what successes you've
had, and what partnerships

00:58:54.560 --> 00:58:56.570
you're looking forward
to with other non-profits

00:58:56.570 --> 00:58:59.520
in the future?

00:58:59.520 --> 00:59:02.920
SAM ALTMAN: I was really
excited about how that went.

00:59:02.920 --> 00:59:04.950
We had never done
anything like that before.

00:59:04.950 --> 00:59:05.979
We often try new things.

00:59:05.979 --> 00:59:07.020
Usually, they don't work.

00:59:07.020 --> 00:59:08.170
Sometimes they do.

00:59:08.170 --> 00:59:10.020
That's something
we would do again.

00:59:10.020 --> 00:59:11.850
We would do something
more like that.

00:59:11.850 --> 00:59:16.260
I think there are these
really important organizations

00:59:16.260 --> 00:59:19.680
in the world that can use our
help to build better technology

00:59:19.680 --> 00:59:20.712
teams.

00:59:20.712 --> 00:59:22.420
And that was an
experiment that went well

00:59:22.420 --> 00:59:24.610
and that we'd love to try again.

00:59:24.610 --> 00:59:28.167
One of our software engineers,
[INAUDIBLE],, went there for--

00:59:28.167 --> 00:59:29.750
I think she went for
like eight weeks,

00:59:29.750 --> 00:59:32.249
almost the whole program, sat
in their offices, helped them.

00:59:32.249 --> 00:59:34.760
We got a call from them
later about how well it went,

00:59:34.760 --> 00:59:37.550
being able to help
them, you know,

00:59:37.550 --> 00:59:39.350
expand and supplement
their team.

00:59:39.350 --> 00:59:41.180
And you know, I think
that's something

00:59:41.180 --> 00:59:42.561
we'd like to try again.

00:59:42.561 --> 00:59:44.560
JORGE CUETO: And one last
question, because it's

00:59:44.560 --> 00:59:45.490
gotten so many votes.

00:59:45.490 --> 00:59:45.790
SAM ALTMAN: Sure.

00:59:45.790 --> 00:59:47.373
JORGE CUETO: Elon
Musk recently called

00:59:47.373 --> 00:59:50.260
for preemptive AI regulation
at the National Governors

00:59:50.260 --> 00:59:51.460
Association.

00:59:51.460 --> 00:59:53.830
As a chair of OpenAI and
friend of Musk's, what

00:59:53.830 --> 00:59:55.420
is your opinion on this issue?

00:59:55.420 --> 00:59:58.294
And what specific actions can
we take to minimize future risk?

00:59:58.294 --> 01:00:00.460
SAM ALTMAN: The specific
thing I would support today

01:00:00.460 --> 01:00:02.510
is just insight.

01:00:02.510 --> 01:00:05.210
I think the government
should understand

01:00:05.210 --> 01:00:09.470
where the edge of capabilities
are and how it's evolving.

01:00:09.470 --> 01:00:12.200
Because I think no one,
certainly not the government,

01:00:12.200 --> 01:00:16.070
knows what the regulation for
AI should look like today.

01:00:16.070 --> 01:00:18.675
But I'd be in favor of starting
that education process.

01:00:18.675 --> 01:00:19.550
JORGE CUETO: Awesome.

01:00:19.550 --> 01:00:21.060
So, yeah, that was
our last question.

01:00:21.060 --> 01:00:21.600
SAM ALTMAN: Thank you all.

01:00:21.600 --> 01:00:23.475
JORGE CUETO: Thank you,
everyone, for coming.

01:00:23.475 --> 01:00:26.525
[APPLAUSE]

01:00:26.525 --> 01:00:27.900
SAM ALTMAN: Thanks
for having me.

01:00:27.900 --> 01:00:29.300
JORGE CUETO: [INAUDIBLE]

