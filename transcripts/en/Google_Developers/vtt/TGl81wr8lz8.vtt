WEBVTT
Kind: captions
Language: en

00:00:00.690 --> 00:00:03.130
Let's now discuss how you
can make your App Engine

00:00:03.130 --> 00:00:08.050
application more performant and
cost less using Memcache.

00:00:08.050 --> 00:00:10.440
So far, you've learned a lot
of things about Google App

00:00:10.440 --> 00:00:13.440
Engine, including the
architecture, the services,

00:00:13.440 --> 00:00:16.239
the datastore, and
lots of APIs.

00:00:16.239 --> 00:00:18.900
If you say that those features
are like the bread and butter

00:00:18.900 --> 00:00:22.090
of App Engine, then you can
also say Memcache is like

00:00:22.090 --> 00:00:24.180
coffee for App Engine.

00:00:24.180 --> 00:00:26.490
Your App Engine application
can be very functional

00:00:26.490 --> 00:00:30.820
without, just more performant
using Memcache.

00:00:30.820 --> 00:00:34.100
That's why Memcache is such an
addictive feature for a lot of

00:00:34.100 --> 00:00:34.980
developers.

00:00:34.980 --> 00:00:37.900
Just like caffeine has side
effects, however, using

00:00:37.900 --> 00:00:40.870
Memcache has some caveats
and limitations.

00:00:40.870 --> 00:00:43.960
You must use Memcache
with care.

00:00:43.960 --> 00:00:46.900
Here are the topics we're going
to cover in this lesson.

00:00:46.900 --> 00:00:49.590
First off, I'll talk about
what Memcache is.

00:00:49.590 --> 00:00:51.630
What are the general uses?

00:00:51.630 --> 00:00:54.420
And then I'll talk about some
specific benefits of using

00:00:54.420 --> 00:00:58.800
Memcache, and to highlight
why we need it.

00:00:58.800 --> 00:01:02.360
Then I'll cover how
to use Memcache.

00:01:02.360 --> 00:01:05.540
I'll talk about the Java and
Python APIs for your

00:01:05.540 --> 00:01:07.890
application to access
Memcache.

00:01:07.890 --> 00:01:11.100
Then we'll move to some
more advanced topics.

00:01:11.100 --> 00:01:14.100
We'll talk about some caveats
with using Memcache and what

00:01:14.100 --> 00:01:16.370
the general solutions will be.

00:01:16.370 --> 00:01:18.370
What is Memcache?

00:01:18.370 --> 00:01:21.400
Simply put, Memcache
is an in-memory

00:01:21.400 --> 00:01:23.790
key-value pair datastore.

00:01:23.790 --> 00:01:26.290
The datastore is sitting
in memory.

00:01:26.290 --> 00:01:29.180
That's why it's much faster
compared with other datastores

00:01:29.180 --> 00:01:31.000
sitting on disk.

00:01:31.000 --> 00:01:33.630
It's also very easy to use.

00:01:33.630 --> 00:01:37.650
The only thing you can put in
Memcache are key-value pairs.

00:01:37.650 --> 00:01:40.140
So you put a value into the
Memcache associated with a

00:01:40.140 --> 00:01:44.540
key, and you expect to get
that value back later on.

00:01:44.540 --> 00:01:47.420
You can actually put anything
that is serializable into

00:01:47.420 --> 00:01:51.540
Memcache either as a
key or as a value.

00:01:51.540 --> 00:01:54.330
It's really kind of language
independent.

00:01:54.330 --> 00:01:58.580
In both Java and Python, you can
put in any object with its

00:01:58.580 --> 00:02:00.510
serializable interface.

00:02:00.510 --> 00:02:03.840
In Python, the object has to
be serialized for using the

00:02:03.840 --> 00:02:05.430
pickle module.

00:02:05.430 --> 00:02:08.229
Watch for some future
enhancements on the roadmap to

00:02:08.229 --> 00:02:12.590
support non-serializable objects
in the near future.

00:02:12.590 --> 00:02:15.240
You could almost put anything
into Memcache.

00:02:15.240 --> 00:02:18.990
In general, there are only two
major use cases, however.

00:02:18.990 --> 00:02:22.240
The first use case for Memcache
is to do caching.

00:02:22.240 --> 00:02:26.170
For example, you can cache in
the datastore query results,

00:02:26.170 --> 00:02:30.020
or some user authentication
tokens, or session data.

00:02:30.020 --> 00:02:33.790
One typical example
is a URL fetching.

00:02:33.790 --> 00:02:35.460
You can actually cache
the content of

00:02:35.460 --> 00:02:37.550
the page into Memcache.

00:02:37.550 --> 00:02:41.810
You can also cache some other
computational results.

00:02:41.810 --> 00:02:45.460
The second use case is to share
data across different

00:02:45.460 --> 00:02:47.330
application instances.

00:02:47.330 --> 00:02:51.930
One typical example is an
application level counter.

00:02:51.930 --> 00:02:55.590
Because multiple instances
access the same counter, you

00:02:55.590 --> 00:02:58.950
have to pay attention to race
conditions, for example.

00:02:58.950 --> 00:03:02.300
Fortunately, Memcache actually
provides the mechanism to help

00:03:02.300 --> 00:03:06.060
you to implement the counter
very easily.

00:03:06.060 --> 00:03:08.090
Why do we need Memcache?

00:03:08.090 --> 00:03:09.760
You can use datastore
to share data

00:03:09.760 --> 00:03:11.500
across different instances.

00:03:11.500 --> 00:03:14.040
You can even use datastore
to do the caching of

00:03:14.040 --> 00:03:16.000
the counter API call.

00:03:16.000 --> 00:03:19.710
So what kind of benefits will
Memcache bring to us?

00:03:19.710 --> 00:03:22.950
We can summarize this
into two points.

00:03:22.950 --> 00:03:26.630
The first is to improve
application performance.

00:03:26.630 --> 00:03:29.590
The second we don't talk enough
about, is that it can

00:03:29.590 --> 00:03:30.590
actually reduce your

00:03:30.590 --> 00:03:33.430
application's operational cost.

00:03:33.430 --> 00:03:36.740
Unlike most technologies, you
can actually obtain both

00:03:36.740 --> 00:03:39.030
benefits together.

00:03:39.030 --> 00:03:41.520
Let's look at performance
first.

00:03:41.520 --> 00:03:44.920
Notice on the left here with a
simple data store query that

00:03:44.920 --> 00:03:48.260
it takes about 100 milliseconds
in App Engine.

00:03:48.260 --> 00:03:51.600
But for a Memcache read on the
right, it takes less than 10

00:03:51.600 --> 00:03:53.310
milliseconds.

00:03:53.310 --> 00:03:55.880
Therefore, Memcache is actually
more than 10 times

00:03:55.880 --> 00:03:58.840
faster than the data
store query.

00:03:58.840 --> 00:04:01.830
If a request is already in
Memcache and finds the

00:04:01.830 --> 00:04:05.730
results, there's no need
to go to the datastore.

00:04:05.730 --> 00:04:08.280
This means the request
and the response

00:04:08.280 --> 00:04:10.650
can be 10 times faster.

00:04:10.650 --> 00:04:13.500
If you have a very good hit rate
from Memcache, you can

00:04:13.500 --> 00:04:15.860
experience a huge performance
improvement for your

00:04:15.860 --> 00:04:17.990
application.

00:04:17.990 --> 00:04:20.790
How about from a cost
point of view?

00:04:20.790 --> 00:04:21.860
Zero.

00:04:21.860 --> 00:04:22.620
That's simple.

00:04:22.620 --> 00:04:23.820
It's free.

00:04:23.820 --> 00:04:26.460
But for the datastore query,
you have to pay for it.

00:04:26.460 --> 00:04:29.390
For API calls, you have
to pay for it.

00:04:29.390 --> 00:04:32.310
For your application,
that CPU memory, you

00:04:32.310 --> 00:04:33.610
have to pay for it.

00:04:33.610 --> 00:04:35.380
So think about this--

00:04:35.380 --> 00:04:39.290
if 90% of the requests can be
handled by Memcache, and

00:04:39.290 --> 00:04:42.460
there's no need to go to the
datastore or do the API call

00:04:42.460 --> 00:04:45.830
or do the computation, that
could be a huge savings for

00:04:45.830 --> 00:04:47.690
your application.

00:04:47.690 --> 00:04:52.240
That will make your manager
very happy about it.

00:04:52.240 --> 00:04:54.165
So how do we use Memcache?

00:04:54.165 --> 00:04:56.890
There are different ways to use
Memcache depending on the

00:04:56.890 --> 00:04:58.600
language you're using.

00:04:58.600 --> 00:05:02.430
If you're using Java, you can
use a high level JCache API,

00:05:02.430 --> 00:05:05.980
which is actually a Java
standard for caching services.

00:05:05.980 --> 00:05:09.590
App Engine implements most of
the JCache API, so people who

00:05:09.590 --> 00:05:12.580
choose to use it are mainly
interested in application

00:05:12.580 --> 00:05:16.230
portability versus using a
different caching service.

00:05:16.230 --> 00:05:19.480
But App Engine is only
one caching service.

00:05:19.480 --> 00:05:22.370
For a lot of developers, they
actually choose to use the

00:05:22.370 --> 00:05:24.920
lower-level Memcache APIs.

00:05:24.920 --> 00:05:27.680
The reason is, you can access
some extra functionality

00:05:27.680 --> 00:05:31.470
provided by Memcache, for
example, the byte operation,

00:05:31.470 --> 00:05:34.330
the automatic counter
manipulation, and also the

00:05:34.330 --> 00:05:36.490
synchronized core.

00:05:36.490 --> 00:05:38.120
Some developers don't
want to deal with

00:05:38.120 --> 00:05:39.760
Memcache directly at all.

00:05:39.760 --> 00:05:42.180
But they still want the
benefit of Memcache.

00:05:42.180 --> 00:05:44.920
So they might use a third-party
library, like

00:05:44.920 --> 00:05:49.030
Objectify for datastore, which
uses Memcache internally.

00:05:49.030 --> 00:05:52.020
Similarly for Python, you can
choose to use a lower-level

00:05:52.020 --> 00:05:55.430
API, like Memcache module,
which has a very similar

00:05:55.430 --> 00:05:59.710
syntax as Open Source Project
Memcache D. You can also

00:05:59.710 --> 00:06:02.910
choose to use NDB
for datastore.

00:06:02.910 --> 00:06:06.210
I said you can use Memcache to
cache a datastore or to cash

00:06:06.210 --> 00:06:08.160
API computations.

00:06:08.160 --> 00:06:10.660
But actually, from a programming
point of view, the

00:06:10.660 --> 00:06:13.230
usage pattern is all the same.

00:06:13.230 --> 00:06:16.660
For the data read operation,
what you need to do is first

00:06:16.660 --> 00:06:18.970
check to evaluate
the Memcache.

00:06:18.970 --> 00:06:21.390
If it does contain your
data, perfect.

00:06:21.390 --> 00:06:23.230
You just return the value.

00:06:23.230 --> 00:06:24.470
If it doesn't--

00:06:24.470 --> 00:06:25.910
for example, if it's
a datastore--

00:06:25.910 --> 00:06:28.520
then you have to fetch the data
from the datastore and

00:06:28.520 --> 00:06:30.910
update the cache entry.

00:06:30.910 --> 00:06:33.780
For the write operation, you
also need to evaluate the

00:06:33.780 --> 00:06:36.190
existing value in
Memcache first.

00:06:36.190 --> 00:06:38.990
You can either evaluate the
specific entry, or you can

00:06:38.990 --> 00:06:41.910
simply evaluate the
entire Memcache.

00:06:41.910 --> 00:06:44.860
Because you want to write a new
value, then you can write

00:06:44.860 --> 00:06:47.740
it to the datastore and
optionally load the value to

00:06:47.740 --> 00:06:50.410
Memcache, as well.

00:06:50.410 --> 00:06:54.230
Here is some Java sample code
that explains the read flow.

00:06:54.230 --> 00:06:57.470
Similar to other App Engine
services, you get the Memcache

00:06:57.470 --> 00:06:58.970
service object back.

00:06:58.970 --> 00:07:01.390
Then you can start to
handle the class.

00:07:01.390 --> 00:07:03.300
You try to do the read
from the cache.

00:07:03.300 --> 00:07:05.210
And if the value is
there, perfect.

00:07:05.210 --> 00:07:07.790
Otherwise, if the value is not,
then you have to get the

00:07:07.790 --> 00:07:12.220
value from the datastore
and update the cache.

00:07:12.220 --> 00:07:15.080
Similarly, this is
the Python code.

00:07:15.080 --> 00:07:18.510
You implement the same
read workflow.

00:07:18.510 --> 00:07:22.880
Also you can implement the put
operation for the write flow.

00:07:22.880 --> 00:07:25.620
This is another feature that's
provided specifically by

00:07:25.620 --> 00:07:29.340
Memcache that's basically a
batch operation to further

00:07:29.340 --> 00:07:31.010
improve performance.

00:07:31.010 --> 00:07:34.060
I talked about Memcache being
a shared service running on

00:07:34.060 --> 00:07:35.450
some server.

00:07:35.450 --> 00:07:38.760
Every API call is actually
a network call.

00:07:38.760 --> 00:07:41.460
So if you try to read one
object, it's fine.

00:07:41.460 --> 00:07:45.550
If you read 100 objects, that's
100 network calls.

00:07:45.550 --> 00:07:47.890
The network overhead added
together could be

00:07:47.890 --> 00:07:49.640
more than you want.

00:07:49.640 --> 00:07:52.950
So what you can do is deal with
this common use case.

00:07:52.950 --> 00:07:56.910
Memcache actually provides a
mechanism to do a batch.

00:07:56.910 --> 00:07:59.690
You can batch 100 network calls
into a single network

00:07:59.690 --> 00:08:03.310
call that further improves
Memcache performance.

00:08:03.310 --> 00:08:06.100
The only limitation is the
batch size for your

00:08:06.100 --> 00:08:10.940
operational API must be
less than 32 megabits.

00:08:10.940 --> 00:08:14.040
Another important feature is
that Memcache has some generic

00:08:14.040 --> 00:08:16.870
built-in support for
atomic operations.

00:08:16.870 --> 00:08:19.600
The first are increment and
incrementAll that can

00:08:19.600 --> 00:08:23.350
automatically increment
numerical values.

00:08:23.350 --> 00:08:26.030
That's very useful feature to
implement some kind of counter

00:08:26.030 --> 00:08:27.850
in Memcache.

00:08:27.850 --> 00:08:31.590
Because Memcache is not
transactional, the API cannot

00:08:31.590 --> 00:08:33.780
participate in the
transaction.

00:08:33.780 --> 00:08:37.700
It does, however, provide some
very basic optimistic lock

00:08:37.700 --> 00:08:38.929
mechanisms.

00:08:38.929 --> 00:08:42.320
It provides the method
getidentifiable and

00:08:42.320 --> 00:08:44.510
putifUntouched.

00:08:44.510 --> 00:08:47.510
For example, you want to update
something, but worry

00:08:47.510 --> 00:08:50.010
about others updating
at the same time.

00:08:50.010 --> 00:08:55.020
What you can do is you can use
getidentifiable first.

00:08:55.020 --> 00:08:58.920
What it does is return you
an identifiable object.

00:08:58.920 --> 00:09:02.810
In this object contains the
value of what you need and

00:09:02.810 --> 00:09:05.270
also a version of this value.

00:09:05.270 --> 00:09:08.420
You can treat the version
as a time stamp.

00:09:08.420 --> 00:09:11.460
Later on, when you want to
update the value, you don't

00:09:11.460 --> 00:09:15.930
call put, but you call
putifUntouched and then pass

00:09:15.930 --> 00:09:18.890
the identifiable object
as a parameter.

00:09:18.890 --> 00:09:22.680
Memcache is guaranteed only
if this value has not been

00:09:22.680 --> 00:09:25.910
changed or modified since
the last time you called

00:09:25.910 --> 00:09:27.830
getidentifiable.

00:09:27.830 --> 00:09:29.670
You will be able to update it.

00:09:29.670 --> 00:09:31.930
If somebody updated
in between, it

00:09:31.930 --> 00:09:35.110
will not do the update.

00:09:35.110 --> 00:09:37.990
Here are some other features
provided by Memcache.

00:09:37.990 --> 00:09:40.630
One is an asynchronous call.

00:09:40.630 --> 00:09:43.880
Memcache is a shared service, so
a Memcache server could be

00:09:43.880 --> 00:09:47.260
overloaded by either your
application or someone else's.

00:09:47.260 --> 00:09:50.690
Your API can actually block
on the Memcache API.

00:09:50.690 --> 00:09:54.960
For some applications, they are
very sensitive to latency.

00:09:54.960 --> 00:09:56.780
What they can do is call
the asynchronous

00:09:56.780 --> 00:09:58.710
version of the service.

00:09:58.710 --> 00:10:00.980
What that does is the
API code is not

00:10:00.980 --> 00:10:02.820
blocked it all by Memcache.

00:10:02.820 --> 00:10:04.090
Then you can continue.

00:10:04.090 --> 00:10:06.650
And then only when you need it,
you return back to sync

00:10:06.650 --> 00:10:09.470
back with the API call.

00:10:09.470 --> 00:10:11.550
Another feature is namespace.

00:10:11.550 --> 00:10:13.720
Actually, namespace is a
generic feature that's

00:10:13.720 --> 00:10:15.970
supported by the App
Engine itself.

00:10:15.970 --> 00:10:18.640
There are many use cases
for namespace.

00:10:18.640 --> 00:10:22.130
One typical use case is to
support multi tenancy.

00:10:22.130 --> 00:10:24.870
For example, you have one
application, but you want to

00:10:24.870 --> 00:10:27.340
support a different domain.

00:10:27.340 --> 00:10:29.940
But you also want to isolate
the data between

00:10:29.940 --> 00:10:31.730
the different domains.

00:10:31.730 --> 00:10:35.100
So what you can do is use
namespace to put all the data

00:10:35.100 --> 00:10:39.760
into one domain so they cannot
access each other.

00:10:39.760 --> 00:10:42.020
We've covered the basics
about Memcache.

00:10:42.020 --> 00:10:44.750
Now, let's discuss some caveats
and some solutions to

00:10:44.750 --> 00:10:46.920
work around them.

00:10:46.920 --> 00:10:49.480
The first caveat is Memcache
is volatile

00:10:49.480 --> 00:10:51.420
compared with datastore.

00:10:51.420 --> 00:10:54.150
Basically, if you put something
in Memcache, it does

00:10:54.150 --> 00:10:57.160
not mean you can get
it back later on.

00:10:57.160 --> 00:11:01.140
The reason is either the entry
has reached expiration, or the

00:11:01.140 --> 00:11:02.860
Memcache is full.

00:11:02.860 --> 00:11:05.870
You either put too much
data into it, or old

00:11:05.870 --> 00:11:07.750
data will be evicted.

00:11:07.750 --> 00:11:10.780
The Memcache server might also
crash, so you lose all your

00:11:10.780 --> 00:11:12.580
data in Memcache.

00:11:12.580 --> 00:11:15.470
It's therefore very important
for your application to handle

00:11:15.470 --> 00:11:17.820
cache miss gracefully.

00:11:17.820 --> 00:11:20.880
Either it's in the cache return
or not, or you get an

00:11:20.880 --> 00:11:23.290
exception if the
server crashes.

00:11:23.290 --> 00:11:26.230
If you really want persistence
behavior, you can implement

00:11:26.230 --> 00:11:28.110
the write-through logic backing

00:11:28.110 --> 00:11:30.360
Memcache with datastore.

00:11:30.360 --> 00:11:33.860
Actually, that's exactly what
Objectify and NDB do, but you

00:11:33.860 --> 00:11:37.760
can implement your own version
if you want flexibility.

00:11:37.760 --> 00:11:41.160
Another caveat is Memcache
is not transactional.

00:11:41.160 --> 00:11:44.805
Consider two running instances
where the first reads an

00:11:44.805 --> 00:11:47.590
account in Memcache was $100.

00:11:47.590 --> 00:11:52.130
Instance two deducts $20, and
then the Memcache becomes $80.

00:11:52.130 --> 00:11:55.510
The first instance also deducts,
say, $30, and it

00:11:55.510 --> 00:11:58.560
writes the incorrect value
of $70 to the Memcache.

00:11:58.560 --> 00:12:01.740
I think you can see this
transactional problem.

00:12:01.740 --> 00:12:05.260
You need to pay attention not
to fall into this caveat.

00:12:05.260 --> 00:12:08.200
The mechanism provided by App
Engine, you can simply use

00:12:08.200 --> 00:12:11.540
increment, treating the value
as a counter to do that.

00:12:11.540 --> 00:12:13.840
But for other types of data,
or if you want more

00:12:13.840 --> 00:12:18.030
flexibility, for example, you
can use getidentifiable.

00:12:18.030 --> 00:12:21.330
Each instance, one and instance
two, both use

00:12:21.330 --> 00:12:23.020
getidentifiable.

00:12:23.020 --> 00:12:25.120
Then you can do a
lot of things.

00:12:25.120 --> 00:12:28.460
When you do the update, using
putifUntouched, instance two

00:12:28.460 --> 00:12:29.620
will succeed.

00:12:29.620 --> 00:12:32.950
But instance one will fail,
because during that time the

00:12:32.950 --> 00:12:36.220
value has been modified.

00:12:36.220 --> 00:12:39.430
A lot of people find that they
don't have enough Memcache.

00:12:39.430 --> 00:12:41.540
Even if you're willing to
pay for it, they ask,

00:12:41.540 --> 00:12:43.150
can you give us more?

00:12:43.150 --> 00:12:45.040
But until now, we couldn't.

00:12:45.040 --> 00:12:47.170
In the future, maybe there's
some feature,

00:12:47.170 --> 00:12:49.150
but not right now.

00:12:49.150 --> 00:12:51.410
What we like to say is that
your application should

00:12:51.410 --> 00:12:54.850
function well even without
Memcache, so that Memcache

00:12:54.850 --> 00:12:57.850
only gives a boost on
the performance.

00:12:57.850 --> 00:13:01.260
Also don't put too many items
into Memcache, because you can

00:13:01.260 --> 00:13:03.760
make it less efficient
if you do that.

00:13:03.760 --> 00:13:06.060
Think about one use case.

00:13:06.060 --> 00:13:08.100
Memcache is one size.

00:13:08.100 --> 00:13:12.030
You can only put one item into
Memcache, for example.

00:13:12.030 --> 00:13:13.860
So you put item A in.

00:13:13.860 --> 00:13:16.810
Then you try to put
item B into it.

00:13:16.810 --> 00:13:19.140
Then A gets evicted.

00:13:19.140 --> 00:13:22.670
So we need to try to read A.
Then A gets read in from the

00:13:22.670 --> 00:13:25.130
datastore and then put
into Memcache.

00:13:25.130 --> 00:13:26.830
Then B gets evicted.

00:13:26.830 --> 00:13:28.340
Then you try to read B again.

00:13:28.340 --> 00:13:31.665
And if you get that kind of
situation, Memcache doesn't do

00:13:31.665 --> 00:13:32.940
you much good.

00:13:32.940 --> 00:13:36.260
So what we suggest is your
application should put only

00:13:36.260 --> 00:13:40.150
useful and necessary information
into Memcache.

00:13:40.150 --> 00:13:42.940
More specifically, some
developers use

00:13:42.940 --> 00:13:44.910
optimization like this.

00:13:44.910 --> 00:13:47.890
They don't just blindly put
the entity read from the

00:13:47.890 --> 00:13:49.960
datastore into Memcache.

00:13:49.960 --> 00:13:53.260
Because probably only 20% of the
information is really what

00:13:53.260 --> 00:13:55.470
you need cached, they
actually create

00:13:55.470 --> 00:13:57.460
their own kind of objects.

00:13:57.460 --> 00:14:01.250
And it combines information from
multiple entities and put

00:14:01.250 --> 00:14:02.540
into the cache.

00:14:02.540 --> 00:14:04.730
They even use some compression
to make

00:14:04.730 --> 00:14:06.440
the object size smaller.

00:14:06.440 --> 00:14:09.060
Those kinds of optimizations
you can do to make Memcache

00:14:09.060 --> 00:14:10.610
more efficient.

00:14:10.610 --> 00:14:14.820
But that's the cost you have--
you're coding it more.

00:14:14.820 --> 00:14:16.900
These are some of the key
takeaways you should have

00:14:16.900 --> 00:14:18.150
learned from this lesson.

