WEBVTT
Kind: captions
Language: en

00:00:01.356 --> 00:00:03.420
KENT WALKER: So good
afternoon, everybody.

00:00:03.420 --> 00:00:05.140
Welcome to Authors At.

00:00:05.140 --> 00:00:08.320
We have Professor Josh Greene
from Harvard with us today,

00:00:08.320 --> 00:00:11.370
the author of "Moral Tribes."

00:00:11.370 --> 00:00:14.730
I am hugely excited to have
Professor Greene with us.

00:00:14.730 --> 00:00:17.390
He has been somebody whose work
I've been following for years.

00:00:17.390 --> 00:00:18.880
He has been doing
a lot of thinking

00:00:18.880 --> 00:00:25.370
about the genetic and
biological and moral bases

00:00:25.370 --> 00:00:28.870
of a lot of our approaches
to complicated philosophical

00:00:28.870 --> 00:00:30.011
questions.

00:00:30.011 --> 00:00:31.510
This is interesting
in the abstract,

00:00:31.510 --> 00:00:33.750
but it's also interesting with
regard to a lot of the things

00:00:33.750 --> 00:00:35.541
we're working on at
Google, whether that be

00:00:35.541 --> 00:00:37.700
artificial intelligence
or driverless cars,

00:00:37.700 --> 00:00:40.230
or trying to figure
out how to get people

00:00:40.230 --> 00:00:42.486
comfortable with new
technologies more generally.

00:00:42.486 --> 00:00:44.486
His book-- we have copies
in the back for people

00:00:44.486 --> 00:00:46.280
who are interested
in buying a copy,

00:00:46.280 --> 00:00:48.780
and Professor Greene has agreed
to stay around and sign them

00:00:48.780 --> 00:00:52.100
if you'd be interested--
goes through a whole variety

00:00:52.100 --> 00:00:54.440
of different trolley
car problems,

00:00:54.440 --> 00:00:56.430
and I'm sure he'll
talk more about that,

00:00:56.430 --> 00:00:59.280
and other ways of testing
our own moral intuitions

00:00:59.280 --> 00:01:03.920
and trying to figure out what
feels rational and rationally

00:01:03.920 --> 00:01:06.690
based and what are some of
the other roots that lead us

00:01:06.690 --> 00:01:09.830
to some of the moral conclusions
that we draw that we think

00:01:09.830 --> 00:01:11.470
of as common morality,

00:01:11.470 --> 00:01:13.630
but in fact may be
intention with the way

00:01:13.630 --> 00:01:16.200
we approach other kinds
of rational questions

00:01:16.200 --> 00:01:17.620
that we deal with in our lives.

00:01:17.620 --> 00:01:20.317
So with that, let me turn
it over to Professor Greene.

00:01:20.317 --> 00:01:22.400
He's probably going to
talk for about half an hour

00:01:22.400 --> 00:01:24.010
and then leave the rest of
the time open for questions,

00:01:24.010 --> 00:01:26.010
so I'd encourage you to
be thinking of questions

00:01:26.010 --> 00:01:28.125
and really turn this
into a conversation.

00:01:28.125 --> 00:01:29.945
[APPLAUSE]

00:01:29.945 --> 00:01:32.220
JOSHUA GREENE: Thanks.

00:01:32.220 --> 00:01:34.390
Well, thank you so
much for having me.

00:01:34.390 --> 00:01:37.044
Really, it's a great honor to
be here, and I'm really excited.

00:01:37.044 --> 00:01:38.960
I'm glad we're having a
Q&amp;A because I'm really

00:01:38.960 --> 00:01:41.350
excited to hear your thoughts.

00:01:41.350 --> 00:01:43.770
Often, when I give a talk--
other people, the same thing--

00:01:43.770 --> 00:01:46.230
you try to find a connection
with the audience.

00:01:46.230 --> 00:01:48.720
But somehow, I feel like saying,
"I use your search engine

00:01:48.720 --> 00:01:50.350
all the time"
doesn't really cut it

00:01:50.350 --> 00:01:52.610
in terms of connecting
with this group.

00:01:52.610 --> 00:01:55.310
But I do think that
there is a higher level

00:01:55.310 --> 00:01:57.100
connection between
what I'm up to

00:01:57.100 --> 00:01:58.920
and what Google is
about more generally.

00:01:58.920 --> 00:02:02.350
So you're trying to organize
the world's information,

00:02:02.350 --> 00:02:05.890
and that is largely a
technical enterprise,

00:02:05.890 --> 00:02:08.949
but there are important
social and moral questions

00:02:08.949 --> 00:02:10.949
that go along with that--
questions about oughts

00:02:10.949 --> 00:02:12.820
and questions about whys.

00:02:12.820 --> 00:02:15.150
And this is really where
my own thinking comes in,

00:02:15.150 --> 00:02:17.510
which is to say I'm
trying to organize

00:02:17.510 --> 00:02:19.380
our thinking about
social problems

00:02:19.380 --> 00:02:21.600
and about moral
problems in a way that

00:02:21.600 --> 00:02:25.119
can be accessible to all, in a
way that can be made universal.

00:02:25.119 --> 00:02:26.660
And that's a really
challenging thing

00:02:26.660 --> 00:02:29.560
because we have deep
felt disagreements

00:02:29.560 --> 00:02:31.210
about what's right
or what's wrong.

00:02:31.210 --> 00:02:33.900
And so the question is,
what kind of common ground

00:02:33.900 --> 00:02:36.070
can we find if we're
going to resolve

00:02:36.070 --> 00:02:37.980
the moral questions
that divide us

00:02:37.980 --> 00:02:41.770
within nations and
across nations?

00:02:41.770 --> 00:02:46.439
So to begin with this problem
of moral disagreement--

00:02:46.439 --> 00:02:48.230
and we'll talk a little
bit later, perhaps,

00:02:48.230 --> 00:02:49.350
if we have time of
some of the issues

00:02:49.350 --> 00:02:50.660
that specifically arise here.

00:02:50.660 --> 00:02:55.890
So for example, this
week, the ruling

00:02:55.890 --> 00:02:58.770
in Europe about the
right to be forgotten

00:02:58.770 --> 00:03:00.951
reverses the greater
good of free speech

00:03:00.951 --> 00:03:02.700
and freedom of
information, or the issues,

00:03:02.700 --> 00:03:06.300
as Kent said, arising
from how to program a car

00:03:06.300 --> 00:03:08.490
to deal with life and
death situations, who lives

00:03:08.490 --> 00:03:09.857
and who dies.

00:03:09.857 --> 00:03:12.440
Fascinating questions, and I'm
delighted to hear your thoughts

00:03:12.440 --> 00:03:13.030
about this.

00:03:13.030 --> 00:03:16.040
I'm going to start by taking
you back a couple of years

00:03:16.040 --> 00:03:18.160
ago, almost three
years ago, to the run

00:03:18.160 --> 00:03:20.060
up of the last
presidential election.

00:03:20.060 --> 00:03:23.960
This was one of the
Republican primary debates,

00:03:23.960 --> 00:03:29.830
and the Republican nominees
were very much, and still are,

00:03:29.830 --> 00:03:33.430
opposed to the Affordable Care
Act, opposed to Obamacare.

00:03:33.430 --> 00:03:36.270
And Wolf Blitzer of CNN, who
was moderating the debate,

00:03:36.270 --> 00:03:37.969
asked a question of Ron Paul.

00:03:37.969 --> 00:03:39.510
And it was very
interesting question.

00:03:39.510 --> 00:03:40.801
It was a very awkward question.

00:03:40.801 --> 00:03:44.947
He said, suppose there's a guy
who says, you know, I'm young,

00:03:44.947 --> 00:03:47.030
I'm healthy, I don't really
need health insurance,

00:03:47.030 --> 00:03:48.779
so he decides not to
buy health insurance.

00:03:48.779 --> 00:03:50.904
And then something
terrible happens to him,

00:03:50.904 --> 00:03:52.320
he ends up in a
coma, and he needs

00:03:52.320 --> 00:03:53.900
intensive care for six months.

00:03:53.900 --> 00:03:57.630
And he says to Ron Paul,
so who should pay for that?

00:03:57.630 --> 00:04:00.084
And Ron Paul, being
a good politician,

00:04:00.084 --> 00:04:01.500
instead of answering
that question

00:04:01.500 --> 00:04:02.749
answered a different question.

00:04:02.749 --> 00:04:05.600
He said, well, he should
have bought health insurance,

00:04:05.600 --> 00:04:07.400
but Blitzer wouldn't
let him off the hook.

00:04:07.400 --> 00:04:09.220
He said, OK, he should have
bought health insurance we all

00:04:09.220 --> 00:04:10.119
agree, but he didn't.

00:04:10.119 --> 00:04:10.910
What should happen?

00:04:10.910 --> 00:04:12.540
Should we let them die?

00:04:12.540 --> 00:04:15.210
Tough question for a politician.

00:04:15.210 --> 00:04:17.480
And while Paul was
thinking, people

00:04:17.480 --> 00:04:19.529
in the audience at the
Republican primary, some

00:04:19.529 --> 00:04:23.500
of them shouted out,
yeah, let him die.

00:04:23.500 --> 00:04:25.935
And Paul couldn't quite bring
himself to agree with them

00:04:25.935 --> 00:04:28.060
or disagree, and what he
said was very interesting.

00:04:28.060 --> 00:04:29.320
He didn't say let him die.

00:04:29.320 --> 00:04:30.790
He didn't say that
the government

00:04:30.790 --> 00:04:31.790
should take care of him.

00:04:31.790 --> 00:04:34.800
He said friends, family,
perhaps the person's church

00:04:34.800 --> 00:04:36.750
should come to that
person's rescue.

00:04:36.750 --> 00:04:39.500
And I think that illustrates
a couple of interesting things

00:04:39.500 --> 00:04:42.760
about the conservative side
of the political spectrum,

00:04:42.760 --> 00:04:44.260
because you see
two strands there.

00:04:44.260 --> 00:04:48.220
One is a kind of individualism,
that is, we're not all in this

00:04:48.220 --> 00:04:48.720
together.

00:04:48.720 --> 00:04:51.140
You make your choices you're
responsible for yourself,

00:04:51.140 --> 00:04:53.160
and if you make bad
choices, you make your bed,

00:04:53.160 --> 00:04:57.380
you lie in it-- or grave,
as the case may be.

00:04:57.380 --> 00:04:59.220
And then also a
kind of you might

00:04:59.220 --> 00:05:01.709
call tribalism, which is to
say there is a collective that

00:05:01.709 --> 00:05:04.250
has an obligation to you, but
it's not the larger government.

00:05:04.250 --> 00:05:07.770
It's your friends, it's your
family, it's your church.

00:05:07.770 --> 00:05:11.470
Now, around the same time,
bursting on the scene

00:05:11.470 --> 00:05:14.460
was my home state's
Elizabeth Warren,

00:05:14.460 --> 00:05:16.840
and she came to a lot
of people's attention

00:05:16.840 --> 00:05:19.640
when this video of her speaking
in someone's living room

00:05:19.640 --> 00:05:24.140
went viral, and it was like
the opposite of Ron Paul.

00:05:24.140 --> 00:05:27.960
She was responding
to the idea of people

00:05:27.960 --> 00:05:30.490
have a right to their fortunes
and the government shouldn't

00:05:30.490 --> 00:05:32.365
be taking things away
and so on and so forth.

00:05:32.365 --> 00:05:35.080
And she said, look, if you
built something wonderful,

00:05:35.080 --> 00:05:38.569
you made a factory, you
have a successful business,

00:05:38.569 --> 00:05:40.110
you deserve your
rewards, but there's

00:05:40.110 --> 00:05:43.022
something you have to keep in
mind, which is that you moved

00:05:43.022 --> 00:05:45.230
your goods to market on the
roads that the rest of us

00:05:45.230 --> 00:05:48.037
paid for, and you were able to
hire workers in your factory

00:05:48.037 --> 00:05:49.870
because the rest of us
paid to educate them,

00:05:49.870 --> 00:05:52.161
and you were safe in your
factory because of the police

00:05:52.161 --> 00:05:56.350
forces and firefighters that
kept your building safe.

00:05:56.350 --> 00:05:58.710
So because of all those
things that you received,

00:05:58.710 --> 00:06:00.000
which you may not be
thinking about-- you're

00:06:00.000 --> 00:06:02.520
focused on your own efforts--
but because of those things,

00:06:02.520 --> 00:06:04.066
you owe something
back to society

00:06:04.066 --> 00:06:05.690
and owe something
back for the next kid

00:06:05.690 --> 00:06:10.010
to come along with a good idea
for his or her own factory.

00:06:10.010 --> 00:06:12.190
Now, commenting
on these remarks,

00:06:12.190 --> 00:06:14.410
Rush Limbaugh said
that Elizabeth Warren

00:06:14.410 --> 00:06:17.610
is a parasite who
hates her host,

00:06:17.610 --> 00:06:22.950
and Ron Paul himself had
perhaps not quite as extreme

00:06:22.950 --> 00:06:26.080
but similarly
unfavorable responses.

00:06:26.080 --> 00:06:29.770
I think those two
moments really nicely

00:06:29.770 --> 00:06:32.970
capture the political
differences that we have here

00:06:32.970 --> 00:06:35.030
in the United States,
at least a lot of them,

00:06:35.030 --> 00:06:37.560
and I think that these are
paralleled on the global stage

00:06:37.560 --> 00:06:38.341
as well.

00:06:38.341 --> 00:06:40.590
And what I'm trying to do
is to understand, how can we

00:06:40.590 --> 00:06:42.645
find common ground between
those two points of view

00:06:42.645 --> 00:06:44.436
and other points of
view that are, perhaps,

00:06:44.436 --> 00:06:48.140
even more alien to
us here in this room?

00:06:48.140 --> 00:06:53.080
So I study morality, and I think
that what morality is really

00:06:53.080 --> 00:06:56.040
ultimately about is
about cooperation,

00:06:56.040 --> 00:06:59.680
and that what cooperation
is really about is altruism.

00:06:59.680 --> 00:07:04.250
It's being willing to
sacrifice something of your own

00:07:04.250 --> 00:07:07.610
in order to benefit somebody
else, and if what you sacrifice

00:07:07.610 --> 00:07:09.566
is smaller than what
the other person gets

00:07:09.566 --> 00:07:11.690
and someone else is willing
to do the same for you,

00:07:11.690 --> 00:07:13.710
then we can all
end up better off

00:07:13.710 --> 00:07:15.100
as a result of our cooperation.

00:07:15.100 --> 00:07:18.480
And so I think of morality as
a kind of cooperation device.

00:07:18.480 --> 00:07:21.150
For me, I think the
best illustration

00:07:21.150 --> 00:07:25.050
of the problem of cooperation
goes back to a famous paper

00:07:25.050 --> 00:07:26.872
by the ecologist
Garrett Hardin called

00:07:26.872 --> 00:07:28.080
"The Tragedy of the Commons."

00:07:28.080 --> 00:07:29.650
Anyone familiar with the
tragedy of the commons?

00:07:29.650 --> 00:07:30.604
I see nodding, yeah.

00:07:30.604 --> 00:07:32.270
So for those of you
who are uninitiated,

00:07:32.270 --> 00:07:34.103
"The Tragedy of the
Commons" goes like this.

00:07:34.103 --> 00:07:36.930
You have a bunch of herders
who share a common pasture,

00:07:36.930 --> 00:07:40.562
and these are rational,
self-interested herders,

00:07:40.562 --> 00:07:42.020
and they think to
themselves, well,

00:07:42.020 --> 00:07:43.896
should I add more
animals to my herd?

00:07:43.896 --> 00:07:45.270
And the herder
thinks, well, when

00:07:45.270 --> 00:07:46.490
I have more animals,
that's good,

00:07:46.490 --> 00:07:47.890
more to sell when
I go to market.

00:07:47.890 --> 00:07:48.870
What's the downside?

00:07:48.870 --> 00:07:49.520
Well, not much.

00:07:49.520 --> 00:07:51.080
They're just grazing
on this common pasture.

00:07:51.080 --> 00:07:53.371
And so they say, yes, I'll
add more animals to my herd,

00:07:53.371 --> 00:07:55.430
and all the herders add
more and more animals.

00:07:55.430 --> 00:07:57.630
And at some point,
there are more animals

00:07:57.630 --> 00:08:01.030
on the pasture than the pasture
can support, grass is all gone,

00:08:01.030 --> 00:08:04.110
all of them die, and everybody
ends up being worse off.

00:08:04.110 --> 00:08:05.230
That's the tragedy.

00:08:05.230 --> 00:08:08.360
And what's paradoxical about
it is that each of the herders

00:08:08.360 --> 00:08:12.089
was doing what was in
his or her best interest,

00:08:12.089 --> 00:08:13.880
regardless of what
other people were doing,

00:08:13.880 --> 00:08:16.004
and yet everybody acting
in their own best interest

00:08:16.004 --> 00:08:19.240
can collectively make
everybody worse off.

00:08:19.240 --> 00:08:20.250
So what's the solution?

00:08:20.250 --> 00:08:22.690
Well, as I said, I think
that morality is essentially

00:08:22.690 --> 00:08:23.820
the solution.

00:08:23.820 --> 00:08:26.650
The problem of cooperation
is about the tension

00:08:26.650 --> 00:08:28.490
between me and us.

00:08:28.490 --> 00:08:30.930
And what morality is
essentially saying, at its most

00:08:30.930 --> 00:08:33.860
basic level, is you
can't just care about me.

00:08:33.860 --> 00:08:35.360
You have to care
about other people.

00:08:35.360 --> 00:08:36.943
You have to care
about the broader us.

00:08:36.943 --> 00:08:38.539
It's not just about yourself.

00:08:38.539 --> 00:08:41.679
But simply saying
that doesn't resolve

00:08:41.679 --> 00:08:43.530
a lot of the most
difficult moral problems.

00:08:43.530 --> 00:08:44.880
So you say, OK, you're
not allowed to steal,

00:08:44.880 --> 00:08:46.440
you're not allowed
to lie in general,

00:08:46.440 --> 00:08:48.315
you're not allowed to
kill people in general.

00:08:48.315 --> 00:08:48.870
Fine.

00:08:48.870 --> 00:08:51.350
That's pretty
universal, but there

00:08:51.350 --> 00:08:52.990
are a lot of open questions.

00:08:52.990 --> 00:08:55.150
So here we have these
herders on the pasture.

00:08:55.150 --> 00:08:57.370
Are they going to have
collective health insurance

00:08:57.370 --> 00:09:00.170
in case their sheep get sick?

00:09:00.170 --> 00:09:01.960
Is it OK for you to
fight off somebody

00:09:01.960 --> 00:09:04.790
who's trying to take your
sheep with an assault weapon?

00:09:04.790 --> 00:09:09.020
There are all kinds of questions
about the terms of cooperation

00:09:09.020 --> 00:09:11.667
that don't get resolved simply
by saying you can't just

00:09:11.667 --> 00:09:12.500
care about yourself.

00:09:12.500 --> 00:09:15.970
You have to be moral, you have
to be social in some sense.

00:09:15.970 --> 00:09:20.070
And so this is where my sequel
to Hardin's parable comes in.

00:09:20.070 --> 00:09:25.060
So now imagine you
have this large forest,

00:09:25.060 --> 00:09:28.010
and around this forest, you
have many different tribes

00:09:28.010 --> 00:09:28.950
of herders.

00:09:28.950 --> 00:09:31.964
And these herders
are cooperative all,

00:09:31.964 --> 00:09:33.880
but they're all cooperative
in different ways.

00:09:33.880 --> 00:09:37.100
So at one extreme
end of the forest,

00:09:37.100 --> 00:09:39.010
you've got your
communist herders.

00:09:39.010 --> 00:09:42.375
And they say, not only are we
going to have a common pasture,

00:09:42.375 --> 00:09:44.000
we're just going to
have a common herd.

00:09:44.000 --> 00:09:48.290
Everything's going to be
in common-- free lunch

00:09:48.290 --> 00:09:52.790
at the Google Cafe-- and that
way, the tension between me

00:09:52.790 --> 00:09:55.605
and us is resolved.

00:09:55.605 --> 00:09:58.330
At the other extreme, you have
your free market capitalist

00:09:58.330 --> 00:10:00.295
herders who say, not
only are we are not

00:10:00.295 --> 00:10:01.670
going to have
common herds, we're

00:10:01.670 --> 00:10:02.730
not going to have
a common pasture.

00:10:02.730 --> 00:10:04.100
We're going to
privatize the pasture,

00:10:04.100 --> 00:10:05.510
we're going to divide
it up into little plots,

00:10:05.510 --> 00:10:07.380
and everybody has
ownership of their bit.

00:10:07.380 --> 00:10:11.110
Our cooperation will consist
not in sharing material goods

00:10:11.110 --> 00:10:13.960
but in respecting each
other's property rights.

00:10:13.960 --> 00:10:16.250
And you can imagine
tribes that differ

00:10:16.250 --> 00:10:17.700
in a lot of other
ways as opposed

00:10:17.700 --> 00:10:21.070
to just being more individualist
or more collectivist.

00:10:21.070 --> 00:10:22.862
Tribes are organized
by different leaders.

00:10:22.862 --> 00:10:24.570
They're organized by
different religions.

00:10:24.570 --> 00:10:27.270
This god over here, this
holy book, this leader

00:10:27.270 --> 00:10:31.110
says no black and white sheep
together in the same enclosure,

00:10:31.110 --> 00:10:33.209
and this one says it's OK
for women to be herders,

00:10:33.209 --> 00:10:34.250
and this one says is not.

00:10:34.250 --> 00:10:36.490
Is it OK to be gay
herder or whatever it is?

00:10:36.490 --> 00:10:39.620
All these tribes can differ
in terms of the basic rules

00:10:39.620 --> 00:10:43.880
that organize their societies--
more collectivist, more

00:10:43.880 --> 00:10:47.060
individualistic, more
intrusive into people's

00:10:47.060 --> 00:10:50.992
personal lives when it comes to
things like sexuality or not.

00:10:50.992 --> 00:10:53.075
So a lot of different herds
with different beliefs

00:10:53.075 --> 00:10:55.600
and different values, tribes.

00:10:55.600 --> 00:10:57.830
And now imagine one
hot, dry summer,

00:10:57.830 --> 00:11:00.590
this forest in the middle of
this ring of different tribes

00:11:00.590 --> 00:11:03.440
burns down, and
then the rains come

00:11:03.440 --> 00:11:05.910
and there's this lovely
green pasture in the middle.

00:11:05.910 --> 00:11:09.110
And all of the different tribes
look at that green pasture

00:11:09.110 --> 00:11:13.680
and think, hey, nice pasture,
and they all move in.

00:11:13.680 --> 00:11:16.180
And the question is,
what's going to happen?

00:11:16.180 --> 00:11:18.052
They're all moral in
some sense, they're all

00:11:18.052 --> 00:11:19.510
cooperative in some
sense, but they

00:11:19.510 --> 00:11:21.450
cooperate on different terms.

00:11:21.450 --> 00:11:25.840
And not only that, all
of their respective ways

00:11:25.840 --> 00:11:29.080
of being cooperative, of being a
decent person within a society,

00:11:29.080 --> 00:11:30.970
seem intuitively
obvious to them.

00:11:30.970 --> 00:11:32.970
It just seems
obvious to this tribe

00:11:32.970 --> 00:11:34.840
that women should be
allowed to own sheep,

00:11:34.840 --> 00:11:36.950
and it just seems obvious
to this tribe over here

00:11:36.950 --> 00:11:39.480
that women should not, that
that's not a thing for a woman

00:11:39.480 --> 00:11:41.690
to do, or whatever it is.

00:11:41.690 --> 00:11:43.950
They're all coming
into this common space.

00:11:43.950 --> 00:11:45.940
This is essentially
the modern world.

00:11:45.940 --> 00:11:49.110
That is, moral problems
in the modern world

00:11:49.110 --> 00:11:51.290
are not simply
about me versus us,

00:11:51.290 --> 00:11:53.760
about selfishness
versus morality.

00:11:53.760 --> 00:11:56.230
They're about us
versus them, that

00:11:56.230 --> 00:11:58.450
is, our interests
versus their interests,

00:11:58.450 --> 00:12:00.450
and our values
versus their values,

00:12:00.450 --> 00:12:03.000
our system for being
a cooperative people

00:12:03.000 --> 00:12:05.880
versus their system for
being a cooperative people.

00:12:05.880 --> 00:12:09.020
And so if we're
going to figure out

00:12:09.020 --> 00:12:11.930
how to have an organized,
universal morality,

00:12:11.930 --> 00:12:15.480
we have to deal with this
higher order problem.

00:12:15.480 --> 00:12:18.410
Borrowing from Hardin and
his "Tragedy of the Commons,"

00:12:18.410 --> 00:12:21.190
I call this the tragedy
of common sense morality.

00:12:21.190 --> 00:12:23.600
There are different versions
of moral common sense

00:12:23.600 --> 00:12:27.680
that enable different tribes
to solve their smaller tribal

00:12:27.680 --> 00:12:28.315
moral problems.

00:12:28.315 --> 00:12:30.730
But then we come together
in the modern world,

00:12:30.730 --> 00:12:32.320
and now we have
this larger problem.

00:12:32.320 --> 00:12:35.210
Just as a moral society
on a small scale

00:12:35.210 --> 00:12:37.690
is one that allows individuals
to get along and form

00:12:37.690 --> 00:12:42.235
a group, a modern morality,
a universal morality

00:12:42.235 --> 00:12:43.860
is going to be one
that's going to have

00:12:43.860 --> 00:12:46.687
to govern those new pastures
with many different tribes

00:12:46.687 --> 00:12:47.520
all living together.

00:12:47.520 --> 00:12:51.130
In some sense, this has been
the enlightenment project

00:12:51.130 --> 00:12:53.910
for moral thinking for the
last few hundred years,

00:12:53.910 --> 00:12:56.210
but it hasn't the problem
hasn't really been solved.

00:12:56.210 --> 00:12:57.960
And I'll try to say a
little bit about why

00:12:57.960 --> 00:13:01.370
I think it's not reached a
satisfactory conclusion yet.

00:13:01.370 --> 00:13:03.680
So the first idea
is the distinction

00:13:03.680 --> 00:13:06.120
between these two different
kinds of moral problems,

00:13:06.120 --> 00:13:08.370
the basic moral
problem of me versus us

00:13:08.370 --> 00:13:11.300
and the higher order moral
problem of us versus them.

00:13:11.300 --> 00:13:14.440
Now I want to turn to a
different set of ideas

00:13:14.440 --> 00:13:16.850
and get inside the head
and inside the brain

00:13:16.850 --> 00:13:19.200
and talk about two different
kinds of moral thinking,

00:13:19.200 --> 00:13:21.450
and really two different
kinds of thinking in general.

00:13:21.450 --> 00:13:23.880
A lot of you are probably
familiar with Daniel Kahneman's

00:13:23.880 --> 00:13:26.820
wonderful book,
"Thinking Fast and Slow."

00:13:26.820 --> 00:13:30.930
That basic framework, the idea
that we have, on the one hand,

00:13:30.930 --> 00:13:32.562
intuitions, gut
reactions that we

00:13:32.562 --> 00:13:34.270
can use to think about
problems, and then

00:13:34.270 --> 00:13:37.350
also have a capacity for
slow, deliberate reasoning.

00:13:37.350 --> 00:13:40.780
I think this is a central
idea, an essential idea,

00:13:40.780 --> 00:13:43.190
for understanding morality.

00:13:43.190 --> 00:13:46.195
My preferred metaphor for
the fast and slow idea

00:13:46.195 --> 00:13:48.060
is like a digital SLR camera.

00:13:48.060 --> 00:13:51.875
So on the one hand, you have
your automatic settings.

00:13:51.875 --> 00:13:54.000
You take a picture of a
mountain from far away, you

00:13:54.000 --> 00:13:55.925
put it in landscape
mode and point and shoot

00:13:55.925 --> 00:13:57.580
and you've got your picture.

00:13:57.580 --> 00:13:59.640
And your camera also
has a manual mode

00:13:59.640 --> 00:14:03.000
where you can adjust
everything by hand.

00:14:03.000 --> 00:14:05.950
Now, it's not that one of these
ways of taking photographs

00:14:05.950 --> 00:14:07.860
is inherently better or
worse than the other.

00:14:07.860 --> 00:14:10.190
It's that they're good
for different things.

00:14:10.190 --> 00:14:14.330
Your automatic settings
are good for problems

00:14:14.330 --> 00:14:16.850
that the manufacturer
has anticipated.

00:14:16.850 --> 00:14:18.762
So it does the
thinking in advance

00:14:18.762 --> 00:14:20.720
and then all you have to
do is point and shoot.

00:14:20.720 --> 00:14:21.660
It's very efficient.

00:14:21.660 --> 00:14:23.750
The problem with
automatic settings

00:14:23.750 --> 00:14:25.716
is that they're
not very flexible.

00:14:25.716 --> 00:14:28.090
They're good for what they're
good for and not much else.

00:14:28.090 --> 00:14:31.401
Manual mode is very flexible
but it's not very efficient.

00:14:31.401 --> 00:14:32.650
You have to twiddle the knobs.

00:14:32.650 --> 00:14:34.108
You have to know
what you're doing.

00:14:34.108 --> 00:14:37.530
And so the way the camera
navigates this trade off

00:14:37.530 --> 00:14:39.700
between efficiency and
flexibility, which I'm sure

00:14:39.700 --> 00:14:42.500
is something that all of those
of you who do any computer

00:14:42.500 --> 00:14:44.156
programming have
thought a lot about,

00:14:44.156 --> 00:14:45.780
is by having these
two different modes,

00:14:45.780 --> 00:14:46.720
where you've got
your point and shoot

00:14:46.720 --> 00:14:48.070
and you've got your manual mode.

00:14:48.070 --> 00:14:52.030
And the human brain really has
the same basic design strategy.

00:14:52.030 --> 00:14:54.760
We have automatic settings
that are efficient but not

00:14:54.760 --> 00:14:56.674
very flexible, good
most of the time

00:14:56.674 --> 00:14:58.340
but not good for
everything, and then we

00:14:58.340 --> 00:15:01.480
have a manual mode, which is
our capacity for deliberate

00:15:01.480 --> 00:15:04.940
reasoning that enables
us to think flexibly

00:15:04.940 --> 00:15:07.720
about novel problems.

00:15:07.720 --> 00:15:09.274
So with this
framework in mind, I

00:15:09.274 --> 00:15:11.190
want to go back to the
original moral problem,

00:15:11.190 --> 00:15:12.840
the tragedy of the commons.

00:15:12.840 --> 00:15:15.240
So the laboratory version of
the tragedy of the commons

00:15:15.240 --> 00:15:16.990
is something called
the public goods game.

00:15:16.990 --> 00:15:18.610
So you bring people
to the lab-- let's

00:15:18.610 --> 00:15:21.480
say you have four people--
you give everybody $10,

00:15:21.480 --> 00:15:24.610
and then people can either keep
their $10 or they can put some

00:15:24.610 --> 00:15:27.024
or all of it into a common pool.

00:15:27.024 --> 00:15:28.440
Whatever goes into
the common pool

00:15:28.440 --> 00:15:31.930
gets doubled by the experimenter
and then divided equally

00:15:31.930 --> 00:15:33.400
among all four people.

00:15:33.400 --> 00:15:36.840
Now, if you are completely
selfish, what do you do?

00:15:36.840 --> 00:15:40.070
You keep your $10 because then
you get your $10, plus you

00:15:40.070 --> 00:15:42.850
get your share of whatever
other people put into the pool

00:15:42.850 --> 00:15:45.770
and got subsequently doubled
by the experimenters.

00:15:45.770 --> 00:15:47.996
If you're completely us-ish,
if what you care about

00:15:47.996 --> 00:15:49.870
is the total good, the
total payoff, then you

00:15:49.870 --> 00:15:51.370
put all your money
in because that's

00:15:51.370 --> 00:15:55.120
what maximizes the amount that
gets productively doubled.

00:15:55.120 --> 00:15:58.560
So you put people in the lab and
you face them with this choice.

00:15:58.560 --> 00:16:00.430
Do you do the me
thing, keep your money,

00:16:00.430 --> 00:16:02.620
or do you do the us
thing and put your money

00:16:02.620 --> 00:16:03.560
into the common pool?

00:16:03.560 --> 00:16:06.470
What we were interested
in is what, if anything,

00:16:06.470 --> 00:16:10.000
is the role of slow
versus fast thinking

00:16:10.000 --> 00:16:13.380
in this kind of social dilemma?

00:16:13.380 --> 00:16:15.120
So you might think
on the one hand,

00:16:15.120 --> 00:16:16.456
people are intuitively selfish.

00:16:16.456 --> 00:16:18.330
You think, I want my
money, but then you stop

00:16:18.330 --> 00:16:20.440
and you think, well, but really,
I should be good to the group

00:16:20.440 --> 00:16:21.419
and put more money in.

00:16:21.419 --> 00:16:23.460
Or you might think it's
the other way, that we're

00:16:23.460 --> 00:16:26.980
intuitively good, or intuitively
cooperative, but then we think,

00:16:26.980 --> 00:16:27.570
wait a second.

00:16:27.570 --> 00:16:28.945
I don't want to
get screwed here.

00:16:28.945 --> 00:16:30.100
I'm going to keep my money.

00:16:30.100 --> 00:16:31.480
Or maybe there's
no tension at all.

00:16:31.480 --> 00:16:32.790
People just have
different preferences

00:16:32.790 --> 00:16:33.720
and they just do
what they prefer,

00:16:33.720 --> 00:16:35.590
but there's no
internal competition.

00:16:35.590 --> 00:16:38.390
So in this set of experiments
done with David Rand and Martin

00:16:38.390 --> 00:16:41.540
Nowak, we had people
make these decisions

00:16:41.540 --> 00:16:43.657
in one set of experiments
under time pressure.

00:16:43.657 --> 00:16:45.740
And the idea is putting
people under time pressure

00:16:45.740 --> 00:16:47.460
is going to favor
the fast thinking

00:16:47.460 --> 00:16:48.637
over the slow thinking.

00:16:48.637 --> 00:16:50.470
And what we found is
that when we put people

00:16:50.470 --> 00:16:53.606
under time pressure, they
contributed more money.

00:16:53.606 --> 00:16:55.480
And what we found is
that when we told people

00:16:55.480 --> 00:16:57.104
you have to stop and
think for at least

00:16:57.104 --> 00:17:00.370
10 seconds about this decision,
they put in less money.

00:17:00.370 --> 00:17:02.130
So at least in this
context, it seems

00:17:02.130 --> 00:17:04.982
like the fast thinking is
what's making us cooperative

00:17:04.982 --> 00:17:06.440
and the slow
thinking, if anything,

00:17:06.440 --> 00:17:08.430
is making us less cooperative.

00:17:08.430 --> 00:17:12.380
And in fact, this result
is consistent with a lot

00:17:12.380 --> 00:17:15.230
of different results, some from
my lab and some from others,

00:17:15.230 --> 00:17:18.970
basically telling us that
we have social instincts.

00:17:18.970 --> 00:17:20.780
We have social
emotions that enable

00:17:20.780 --> 00:17:24.220
us to solve that basic moral
problem of me versus us.

00:17:24.220 --> 00:17:25.839
And you can think
of these as falling

00:17:25.839 --> 00:17:28.260
into four categories
in a two by two matrix.

00:17:28.260 --> 00:17:31.136
So we have positive emotions
and negative emotions.

00:17:31.136 --> 00:17:33.260
You can think of those as
sort of emotional carrots

00:17:33.260 --> 00:17:35.717
and emotional sticks, and we
can apply them to ourselves

00:17:35.717 --> 00:17:37.300
and we can apply
them to other people.

00:17:37.300 --> 00:17:40.634
So an emotional carrot
applied to me, if I love you,

00:17:40.634 --> 00:17:42.050
I care about you,
you're my friend

00:17:42.050 --> 00:17:43.570
or I have goodwill
towards you, that

00:17:43.570 --> 00:17:45.880
impels me to be cooperative.

00:17:45.880 --> 00:17:47.550
I might also think,
I'll feel guilty,

00:17:47.550 --> 00:17:49.150
I'll feel ashamed,
if I keep my money

00:17:49.150 --> 00:17:50.566
and everybody puts
their money in.

00:17:50.566 --> 00:17:52.939
So it's a negative feeling
that makes me be cooperative.

00:17:52.939 --> 00:17:54.730
And then I have feelings
that apply to you.

00:17:54.730 --> 00:17:56.830
If you put your money in,
you'll have my gratitude.

00:17:56.830 --> 00:17:58.010
And if you don't
put your money in,

00:17:58.010 --> 00:17:59.930
then you'll have my
scorn and my contempt.

00:17:59.930 --> 00:18:02.620
So these different
kinds of social emotions

00:18:02.620 --> 00:18:04.979
enable us to be cooperative,
to get in that state

00:18:04.979 --> 00:18:06.770
where people are not
just thinking about me

00:18:06.770 --> 00:18:08.936
and they're putting at least
some of their resources

00:18:08.936 --> 00:18:09.700
towards us.

00:18:09.700 --> 00:18:13.040
So I think part one, lesson one,
is that fast thinking, at least

00:18:13.040 --> 00:18:15.040
in some contexts--
this is not universal,

00:18:15.040 --> 00:18:16.980
and I'll get to
this in a second--

00:18:16.980 --> 00:18:20.440
does pretty well at solving
that basic moral problem.

00:18:20.440 --> 00:18:22.450
But the basic model
problem, as I said,

00:18:22.450 --> 00:18:23.940
is not the only moral problem.

00:18:23.940 --> 00:18:25.490
That's the tragedy
of the commons.

00:18:25.490 --> 00:18:28.260
We also have the tragedy
of common sense morality.

00:18:28.260 --> 00:18:31.960
And this is where, I think,
our moral intuitions often

00:18:31.960 --> 00:18:33.527
go astray.

00:18:33.527 --> 00:18:35.860
And particularly when we're
dealing with modern contexts

00:18:35.860 --> 00:18:39.570
and we're dealing with people
who are not in our groups, who

00:18:39.570 --> 00:18:42.210
we think of as
the other, this is

00:18:42.210 --> 00:18:43.637
where things go
especially badly.

00:18:43.637 --> 00:18:45.970
I think one of the most
chilling demonstrations of this,

00:18:45.970 --> 00:18:48.960
actually, comes from your own
Seth Stephens-Davidowitz who

00:18:48.960 --> 00:18:52.700
was an economics PhD student
at Harvard until not long ago.

00:18:52.700 --> 00:19:00.290
And he's analyzed Google search
data and shown that in places

00:19:00.290 --> 00:19:04.080
where there's high search volume
for searches involving the "n"

00:19:04.080 --> 00:19:07.000
word, that those are places
where Obama did especially

00:19:07.000 --> 00:19:11.170
poorly in the 2008 election, and
I believe in the 2012 election

00:19:11.170 --> 00:19:12.200
as well.

00:19:12.200 --> 00:19:16.192
That just basic animus
towards a racial out group

00:19:16.192 --> 00:19:17.650
can have-- well,
you may think it's

00:19:17.650 --> 00:19:20.066
a good effect or a bad effect
in that particular election,

00:19:20.066 --> 00:19:23.250
but I hope you'll at least
that racial animus is not

00:19:23.250 --> 00:19:24.810
a good thing.

00:19:24.810 --> 00:19:27.320
Coming back to the
tragedy of the commons

00:19:27.320 --> 00:19:32.220
and the public goods game,
we did this in our lab.

00:19:32.220 --> 00:19:35.050
Some of this was online,
some of this was in Boston.

00:19:35.050 --> 00:19:37.620
Benedict Herman and colleagues
have done these public goods

00:19:37.620 --> 00:19:39.529
games around the world.

00:19:39.529 --> 00:19:41.820
And they did repeated versions
of the public goods game

00:19:41.820 --> 00:19:43.455
where people play,
they put their money in

00:19:43.455 --> 00:19:45.590
or they don't, and then
they have the opportunity

00:19:45.590 --> 00:19:49.600
to reward or punish the
other people with whom

00:19:49.600 --> 00:19:50.810
they're playing.

00:19:50.810 --> 00:19:53.440
So let's say you cooperated,
you put your money in,

00:19:53.440 --> 00:19:56.360
and somebody else didn't, you
can pay $1 to the experimenter

00:19:56.360 --> 00:19:58.230
and they'll take away
$3 from that person.

00:19:58.230 --> 00:20:00.400
So it's the economic
equivalent of bapping somebody

00:20:00.400 --> 00:20:02.750
on the head with a stick.

00:20:02.750 --> 00:20:05.530
And what they found is
that people play this game

00:20:05.530 --> 00:20:08.440
very differently in different
places around the world.

00:20:08.440 --> 00:20:11.350
You put a bunch of strangers
together in Copenhagen,

00:20:11.350 --> 00:20:13.350
let's say, and people
right from the beginning,

00:20:13.350 --> 00:20:17.810
they put a lot of money in,
and it stays high throughout,

00:20:17.810 --> 00:20:19.650
so they walk away
with a lot of money.

00:20:19.650 --> 00:20:21.710
There are other places
like Melbourne, Australia,

00:20:21.710 --> 00:20:24.570
and Chengdu in China, where
people put in a decent amount

00:20:24.570 --> 00:20:26.260
to begin with and
then some people

00:20:26.260 --> 00:20:27.575
punish the people who
are not cooperating

00:20:27.575 --> 00:20:29.450
and then cooperation
goes up, and by the end,

00:20:29.450 --> 00:20:31.520
it looks like lovely Copenhagen.

00:20:31.520 --> 00:20:35.170
And then there are other places,
like Athens, where people don't

00:20:35.170 --> 00:20:36.790
put very much in
from the beginning,

00:20:36.790 --> 00:20:39.250
and then over time, people
have the opportunity

00:20:39.250 --> 00:20:44.060
to punish the people,
cooperation still stays low.

00:20:44.060 --> 00:20:47.290
And they were very surprised
to see what was going on here

00:20:47.290 --> 00:20:50.780
because we think of punishment
is about the cooperators is

00:20:50.780 --> 00:20:53.650
pushing the non-cooperators,
but in places like Athens,

00:20:53.650 --> 00:20:57.720
the non-cooperators were
punishing the cooperators.

00:20:57.720 --> 00:21:00.242
You're giving us money and
I'm punishing you for it.

00:21:00.242 --> 00:21:01.200
Why would they do that?

00:21:01.200 --> 00:21:03.500
Well, they interviewed them
afterwards and they said,

00:21:03.500 --> 00:21:04.949
I don't like this game.

00:21:04.949 --> 00:21:05.990
I don't know who you are.

00:21:05.990 --> 00:21:07.534
I don't know who
these people are.

00:21:07.534 --> 00:21:09.200
I don't like this
whole thing and I just

00:21:09.200 --> 00:21:10.750
want to let everybody
know, don't mess with me.

00:21:10.750 --> 00:21:12.510
I'm not going to play
your little game.

00:21:12.510 --> 00:21:13.820
Now, I've been to Athens.

00:21:13.820 --> 00:21:15.560
The people there are very nice.

00:21:15.560 --> 00:21:17.960
It's not like these people
are jerks, so what's going on?

00:21:17.960 --> 00:21:22.550
And I think it's that for them,
cooperation is more tribal.

00:21:22.550 --> 00:21:24.090
It's about who you know.

00:21:24.090 --> 00:21:25.560
It's about personal
relationships.

00:21:25.560 --> 00:21:27.770
And the idea of coming
into some antiseptic space

00:21:27.770 --> 00:21:29.895
with some experimenter
you've never heard of before

00:21:29.895 --> 00:21:32.180
and these strangers and
laying your money on the line

00:21:32.180 --> 00:21:35.842
and trusting them, that was
very uncomfortable for them.

00:21:35.842 --> 00:21:37.300
So what's interesting
about this is

00:21:37.300 --> 00:21:39.508
you give people the exact
same opportunities in, say,

00:21:39.508 --> 00:21:41.750
Copenhagen and Athens,
not even that far apart,

00:21:41.750 --> 00:21:44.180
and they walk away with much
more money in Copenhagen

00:21:44.180 --> 00:21:47.095
than they walk away in
Athens, at least playing

00:21:47.095 --> 00:21:48.160
the game this way.

00:21:48.160 --> 00:21:50.880
And it's because of those
automatic settings that people

00:21:50.880 --> 00:21:53.250
in Copenhagen have the
gut reaction that says,

00:21:53.250 --> 00:21:55.200
I can trust you,
and other research

00:21:55.200 --> 00:21:57.280
suggests that it really
is largely about trust.

00:21:57.280 --> 00:21:59.150
You have a feeling that says,
I don't know who you are,

00:21:59.150 --> 00:22:00.000
but I can trust you.

00:22:00.000 --> 00:22:01.730
We're all in this
together in some sense.

00:22:01.730 --> 00:22:03.220
Whereas in Athens
and other places,

00:22:03.220 --> 00:22:05.260
they don't have that feeling
as much for strangers,

00:22:05.260 --> 00:22:07.009
for people who they
don't know personally.

00:22:07.009 --> 00:22:12.000
This is another way in which
our gut reactions can fail us.

00:22:12.000 --> 00:22:16.170
Different case coming from
the philosopher Peter Singer.

00:22:16.170 --> 00:22:20.000
Suppose you are walking
along and there's a pond.

00:22:20.000 --> 00:22:21.950
There's a child who's
drowning in this pond,

00:22:21.950 --> 00:22:24.210
and you could save
this child's life,

00:22:24.210 --> 00:22:26.962
but you're wearing
your fancy Italian suit

00:22:26.962 --> 00:22:29.420
that you just got and you're
going to ruin your suit if you

00:22:29.420 --> 00:22:31.947
have to wade into
this muddy pond.

00:22:31.947 --> 00:22:34.530
How many of you think it's OK
to let the child die because you

00:22:34.530 --> 00:22:36.700
don't want to ruin your suit?

00:22:36.700 --> 00:22:38.630
How many of you think
that would be terrible?

00:22:38.630 --> 00:22:40.470
A lot of hands going up.

00:22:40.470 --> 00:22:41.110
Different case.

00:22:41.110 --> 00:22:41.943
You're home one day.

00:22:41.943 --> 00:22:44.120
You get a letter
from Oxfam or UNICEF

00:22:44.120 --> 00:22:46.240
saying, please make a donation.

00:22:46.240 --> 00:22:48.824
The money that you
give us can very likely

00:22:48.824 --> 00:22:51.490
save somebody's life, a child on
the other side of the world who

00:22:51.490 --> 00:22:53.540
badly needs food or medicine.

00:22:53.540 --> 00:22:57.600
And you say, well, I'd
like to help these people,

00:22:57.600 --> 00:23:00.666
but I've got my eye
on this nice suit,

00:23:00.666 --> 00:23:02.290
so I'm going to save
my money for that.

00:23:02.290 --> 00:23:03.560
Now, how many of you
think that you're

00:23:03.560 --> 00:23:06.247
a terrible person if you spend
your money on luxuries that you

00:23:06.247 --> 00:23:08.830
don't really need but that are
nice to have when you could use

00:23:08.830 --> 00:23:11.290
that money to save
someone's life?

00:23:11.290 --> 00:23:12.200
A little bit, right?

00:23:12.200 --> 00:23:16.350
We all go, I see that, but
how many of us live that?

00:23:16.350 --> 00:23:17.040
Very few of us.

00:23:19.660 --> 00:23:20.550
What's going on here?

00:23:20.550 --> 00:23:22.466
Well, philosophers have
argued about this kind

00:23:22.466 --> 00:23:24.750
of case for a long time.

00:23:24.750 --> 00:23:26.974
Very clever people
say, well, for example,

00:23:26.974 --> 00:23:29.140
in the child who's drowning,
you're the only one who

00:23:29.140 --> 00:23:31.910
can help, whereas people on the
other side of the world, there

00:23:31.910 --> 00:23:33.285
are a lot of people
to help them.

00:23:33.285 --> 00:23:35.420
So you have a special duty
to help the person here,

00:23:35.420 --> 00:23:36.630
but those other
people over there,

00:23:36.630 --> 00:23:37.880
you don't have a duty to help.

00:23:37.880 --> 00:23:39.150
And then another
clever philosopher

00:23:39.150 --> 00:23:41.733
comes along and says, well, what
if there were a lot of people

00:23:41.733 --> 00:23:44.380
standing around this pond
wearing fancy Italian suits

00:23:44.380 --> 00:23:45.780
and they're not helping?

00:23:45.780 --> 00:23:47.706
Now is it OK for you
to let the child drown

00:23:47.706 --> 00:23:49.330
because other people
are standing there

00:23:49.330 --> 00:23:50.470
in their suits doing nothing?

00:23:50.470 --> 00:23:52.180
You say, gosh, that
doesn't sound right.

00:23:52.180 --> 00:23:55.950
And this goes on
and on for decades.

00:23:55.950 --> 00:23:58.980
What I wanted to do and what
I did with a student named

00:23:58.980 --> 00:24:03.290
Jay Musen is try to turn
this thought experiment

00:24:03.290 --> 00:24:07.010
into an actual
scientific experiment.

00:24:07.010 --> 00:24:10.850
So in our version, which comes
from a philosopher named Peter

00:24:10.850 --> 00:24:14.661
Unger, you're vacationing in
this lovely but poor country.

00:24:14.661 --> 00:24:16.660
You have your little
cottage up in the mountains

00:24:16.660 --> 00:24:20.560
overlooking the coast, and
there's a terrible typhoon that

00:24:20.560 --> 00:24:23.157
hits, and there's
devastation along the coast.

00:24:23.157 --> 00:24:24.990
It's much, unfortunately,
like the situation

00:24:24.990 --> 00:24:28.100
in the Philippines not long ago.

00:24:28.100 --> 00:24:30.249
And you can help.

00:24:30.249 --> 00:24:32.790
The best thing for you to do is
not to go down there yourself

00:24:32.790 --> 00:24:35.214
but to just make a donation
to, say, the Red Cross.

00:24:35.214 --> 00:24:37.380
And your internet still
works in your little cottage

00:24:37.380 --> 00:24:38.330
and you can make a donation.

00:24:38.330 --> 00:24:40.246
And we asked people, do
you have an obligation

00:24:40.246 --> 00:24:43.020
to give some money to help
the people down below who

00:24:43.020 --> 00:24:44.840
have been devastated
by this typhoon?

00:24:44.840 --> 00:24:47.910
And most people-- about, I
think, 68% in our sample--

00:24:47.910 --> 00:24:50.009
said you have an obligation
to help in that case.

00:24:50.009 --> 00:24:52.300
We gave a different group of
people the following case.

00:24:52.300 --> 00:24:55.450
Same situation with the typhoon,
but you're not over there.

00:24:55.450 --> 00:24:56.760
Your friend is over there.

00:24:56.760 --> 00:24:58.896
Instead, you're at
your computer at home,

00:24:58.896 --> 00:25:01.020
and your friend has a
smartphone and is showing you

00:25:01.020 --> 00:25:01.960
everything that goes on.

00:25:01.960 --> 00:25:03.459
You can see, you
can hear everything

00:25:03.459 --> 00:25:05.989
your friend sees and hears,
and you can help just as much.

00:25:05.989 --> 00:25:08.030
You could make a donation
to the Red Cross online

00:25:08.030 --> 00:25:10.520
just as much as your friend can.

00:25:10.520 --> 00:25:12.160
You're just farther away.

00:25:12.160 --> 00:25:13.716
Do you have an
obligation to help?

00:25:13.716 --> 00:25:15.340
These people didn't
see the first case.

00:25:15.340 --> 00:25:16.340
They only saw this case.

00:25:16.340 --> 00:25:18.860
About half as many people,
about 34% in our sample,

00:25:18.860 --> 00:25:21.836
said you have an
obligation to help.

00:25:21.836 --> 00:25:23.460
It's not a perfect
experiment, but it's

00:25:23.460 --> 00:25:25.290
a pretty well
controlled experiment.

00:25:25.290 --> 00:25:28.140
It seems like the difference
is this physical distance.

00:25:28.140 --> 00:25:30.731
Just imagining that it's
far away versus nearby

00:25:30.731 --> 00:25:32.230
seems to make a
very big difference.

00:25:32.230 --> 00:25:36.600
Now, morally, it's pretty
hard to defend that,

00:25:36.600 --> 00:25:38.275
but psychologically
and biologically,

00:25:38.275 --> 00:25:39.710
it makes a lot of sense.

00:25:39.710 --> 00:25:42.070
We evolved to solve the
tragedy of the commons.

00:25:42.070 --> 00:25:44.520
We evolved to solve moral
problems within a tribe

00:25:44.520 --> 00:25:46.460
where we're interacting
with people in person,

00:25:46.460 --> 00:25:47.960
and anyone you might
help is someone

00:25:47.960 --> 00:25:49.550
who might come help
you later, and anyone

00:25:49.550 --> 00:25:50.940
you might hurt or
allow to die is

00:25:50.940 --> 00:25:53.010
someone who's family
might be very upset.

00:25:53.010 --> 00:25:56.180
It makes sense that we
have moral buttons that

00:25:56.180 --> 00:25:58.750
get pushed by people who
are right in front of us.

00:25:58.750 --> 00:26:01.180
But it wouldn't make any
biological sense for us

00:26:01.180 --> 00:26:03.579
to have buttons that are
pushed from the other side

00:26:03.579 --> 00:26:04.120
of the world.

00:26:04.120 --> 00:26:06.650
We didn't evolve to
cooperate with statistically

00:26:06.650 --> 00:26:09.940
indeterminate strangers on
the other side of the world.

00:26:09.940 --> 00:26:12.560
But today, we have the capacity
to save the lives of people

00:26:12.560 --> 00:26:13.480
on the other side of the world.

00:26:13.480 --> 00:26:15.688
We also have the capacity
to ruin the lives of people

00:26:15.688 --> 00:26:17.120
on the other side of the world.

00:26:17.120 --> 00:26:21.890
And so the modern world
gives us these opportunities

00:26:21.890 --> 00:26:26.800
for good or ill that perhaps
our automatic settings are not

00:26:26.800 --> 00:26:27.443
up to.

00:26:27.443 --> 00:26:30.680
Do I have time for
one more example?

00:26:30.680 --> 00:26:33.150
So another example about
thinking about helping people.

00:26:33.150 --> 00:26:36.760
This is a brain imaging
study done by Amatai Shenhav

00:26:36.760 --> 00:26:39.330
and myself, where we asked
people questions about,

00:26:39.330 --> 00:26:41.480
say you work for
the Coast Guard,

00:26:41.480 --> 00:26:42.730
you're in this rescue boat.

00:26:42.730 --> 00:26:44.150
You're going to save
someone who's drowning

00:26:44.150 --> 00:26:45.970
and then you get a radio
signal that says, wait,

00:26:45.970 --> 00:26:47.250
there are these people
who are drowning

00:26:47.250 --> 00:26:48.360
in this other location.

00:26:48.360 --> 00:26:50.467
You can turn around
and save them instead.

00:26:50.467 --> 00:26:52.800
And this, I think, may feel
in a certain sense very much

00:26:52.800 --> 00:26:56.290
like a Google car problem.

00:26:56.290 --> 00:27:01.010
Do you change course
and go save the five?

00:27:01.010 --> 00:27:04.870
Now, you want to know,
how many lives can I save,

00:27:04.870 --> 00:27:07.110
and what are my odds
of actually saving?

00:27:07.110 --> 00:27:10.070
So it could be 10 people but
a 50% chance of saving them.

00:27:10.070 --> 00:27:12.600
Do I let the one person go in
order to save those people?

00:27:12.600 --> 00:27:13.780
You need to make
that calculation.

00:27:13.780 --> 00:27:15.570
So we had people make
these kinds of judgments

00:27:15.570 --> 00:27:17.040
while having their
brains scanned,

00:27:17.040 --> 00:27:20.672
and our main scientific interest
was in, what in the brain

00:27:20.672 --> 00:27:22.380
is keeping track of
the probability, what

00:27:22.380 --> 00:27:23.390
are my odds of saving them?

00:27:23.390 --> 00:27:24.740
What's keeping track
of the magnitude,

00:27:24.740 --> 00:27:26.050
how many lives could I save?

00:27:26.050 --> 00:27:28.133
And what's putting those
two pieces of information

00:27:28.133 --> 00:27:30.970
together in order
to make a decision?

00:27:30.970 --> 00:27:33.510
I'll spare you some of the
neurobiological details,

00:27:33.510 --> 00:27:35.650
but the gist of it is
that the system that we're

00:27:35.650 --> 00:27:38.150
using to take these variables
and put them together and make

00:27:38.150 --> 00:27:41.450
a decision really
is the same system

00:27:41.450 --> 00:27:45.710
that rats and that monkeys use
to make decisions about choice

00:27:45.710 --> 00:27:47.520
under uncertainty.

00:27:47.520 --> 00:27:49.760
If you're an animal
and you're deciding,

00:27:49.760 --> 00:27:51.772
do I forage and get the
easy leaves over here

00:27:51.772 --> 00:27:53.480
or do I go for the
nice ripe fruit that's

00:27:53.480 --> 00:27:55.940
much farther away, and much
more uncertain and much more

00:27:55.940 --> 00:28:00.705
dangerous to get
there, what do you do?

00:28:00.705 --> 00:28:02.580
Now, one interesting
thing that we've noticed

00:28:02.580 --> 00:28:03.840
and other people have
noticed about when

00:28:03.840 --> 00:28:05.214
it comes to saving
people's lives

00:28:05.214 --> 00:28:07.620
is that you get these sort
of diminishing returns

00:28:07.620 --> 00:28:08.744
in terms of the numbers.

00:28:08.744 --> 00:28:11.160
So you save one person's life,
that feels like a big deal.

00:28:11.160 --> 00:28:13.576
Two people's lives, well,
that's maybe twice as important.

00:28:13.576 --> 00:28:16.410
But once you get
50, 100, 150, 1,000,

00:28:16.410 --> 00:28:18.170
it just feels like a lot.

00:28:18.170 --> 00:28:20.440
It doesn't feel very different.

00:28:20.440 --> 00:28:23.260
Insensitivity to quantity is
the technical term for this.

00:28:23.260 --> 00:28:24.720
Now, why would that be?

00:28:24.720 --> 00:28:27.470
Well, based on what I told
you from the brain imaging

00:28:27.470 --> 00:28:29.420
experiment, it actually
makes a kind of sense.

00:28:29.420 --> 00:28:33.850
That is, if this is a system
for placing values on choices,

00:28:33.850 --> 00:28:36.650
on options, that we
inherited in modified

00:28:36.650 --> 00:28:41.080
form from our primate ancestors
and our even earlier mammalian

00:28:41.080 --> 00:28:42.597
ancestors, it
makes sense that we

00:28:42.597 --> 00:28:44.180
get these kind of
diminishing returns.

00:28:44.180 --> 00:28:47.680
If you're a rat, you
don't have a fridge.

00:28:47.680 --> 00:28:49.280
The goods in your
life, once you've

00:28:49.280 --> 00:28:50.820
eaten your lunch, that's it.

00:28:50.820 --> 00:28:53.330
Having more food isn't going
to do you that much good.

00:28:53.330 --> 00:28:55.496
You could maybe share it
with a couple of other rats

00:28:55.496 --> 00:28:58.480
or something like that,
but the value of goods

00:28:58.480 --> 00:29:01.760
falls off pretty quickly
as the quantity increases.

00:29:01.760 --> 00:29:04.910
And it would make sense that we
would have a neural valuation

00:29:04.910 --> 00:29:06.640
system that follows
that pattern.

00:29:06.640 --> 00:29:08.570
But now we're using it
to think about things

00:29:08.570 --> 00:29:10.320
like saving people's lives.

00:29:10.320 --> 00:29:12.380
It didn't evolve for
that purpose, right?

00:29:12.380 --> 00:29:14.401
And so morally, it
doesn't make sense.

00:29:14.401 --> 00:29:15.900
Why should we say
the hundredth life

00:29:15.900 --> 00:29:17.800
that you can save is worth
any less than the first life

00:29:17.800 --> 00:29:18.740
that you can save.

00:29:18.740 --> 00:29:21.330
But at least to this
system within your brain,

00:29:21.330 --> 00:29:22.650
it does make a difference.

00:29:22.650 --> 00:29:23.770
I'm morally full.

00:29:23.770 --> 00:29:26.500
I've saved enough
people, 100 lives.

00:29:26.500 --> 00:29:27.260
I'm done.

00:29:27.260 --> 00:29:29.570
It's not worth much
more after that.

00:29:29.570 --> 00:29:32.382
Now, part of you does engage
in this kind of thinking

00:29:32.382 --> 00:29:34.590
and part of you, as you're
doing, can chuckle at that

00:29:34.590 --> 00:29:37.320
and say, gosh, that's kind of
ridiculous that we do that.

00:29:37.320 --> 00:29:39.130
And that's, I think, your
fast and your slow thinking.

00:29:39.130 --> 00:29:41.340
You have an intuitive way of
thinking about these problems,

00:29:41.340 --> 00:29:42.560
and then you can
step back from that

00:29:42.560 --> 00:29:44.601
with an understanding of
what your brain is doing

00:29:44.601 --> 00:29:45.749
and say, gosh, that's dumb.

00:29:45.749 --> 00:29:47.040
That doesn't really make sense.

00:29:47.040 --> 00:29:47.998
And it's not just dumb.

00:29:47.998 --> 00:29:48.860
It's consequential.

00:29:48.860 --> 00:29:52.150
The fact that we think this way
can have important implications

00:29:52.150 --> 00:29:55.820
for our willingness and ability
to save the lives of strangers

00:29:55.820 --> 00:29:57.130
on the other side of the world.

00:29:57.130 --> 00:30:01.030
So to recap a bit of
what I've said so far,

00:30:01.030 --> 00:30:04.970
the basic moral
problem, me versus us,

00:30:04.970 --> 00:30:07.081
selfishness versus
morality, we've

00:30:07.081 --> 00:30:09.330
got pretty good intuitions
for that, for getting along

00:30:09.330 --> 00:30:10.130
in a tribe.

00:30:10.130 --> 00:30:12.497
But when it comes
to the modern world

00:30:12.497 --> 00:30:14.330
where we are trying to
get along with people

00:30:14.330 --> 00:30:16.290
from different tribes
with different values

00:30:16.290 --> 00:30:18.040
and we have
opportunities, like you

00:30:18.040 --> 00:30:19.560
can save the life
of some stranger

00:30:19.560 --> 00:30:21.268
on the other side of
the world, something

00:30:21.268 --> 00:30:25.460
that your brain never evolved
to do, our moral instincts,

00:30:25.460 --> 00:30:27.388
our intuitions,
our fast thinking

00:30:27.388 --> 00:30:28.846
seems-- at least
to me, and I think

00:30:28.846 --> 00:30:30.637
you could make a pretty
good case that this

00:30:30.637 --> 00:30:32.940
is true for a lot of
people-- to fall short.

00:30:32.940 --> 00:30:34.570
And so then the
question is, all right,

00:30:34.570 --> 00:30:36.219
how do we solve
that larger problem?

00:30:36.219 --> 00:30:38.010
And this is the problem
with which I began.

00:30:38.010 --> 00:30:40.610
How do we solve the tragedy
of common sense morality?

00:30:40.610 --> 00:30:43.240
How do we, the people
of the world, people

00:30:43.240 --> 00:30:47.470
of different political parties,
even within our own nation,

00:30:47.470 --> 00:30:49.759
reconcile our
different moral values

00:30:49.759 --> 00:30:51.550
and try to come up with
some kind of system

00:30:51.550 --> 00:30:54.370
that we can all live by?

00:30:54.370 --> 00:30:58.170
So as I said, this has been
the Enlightenment project

00:30:58.170 --> 00:31:01.135
for morality.

00:31:01.135 --> 00:31:02.510
The whole second
half of the book

00:31:02.510 --> 00:31:03.600
is trying to answer
this question.

00:31:03.600 --> 00:31:05.610
And I'm now down probably
to my last five minutes,

00:31:05.610 --> 00:31:07.660
so I'm going to try to be
quick and just give you

00:31:07.660 --> 00:31:09.493
a flavor of what I think
the answer is like.

00:31:09.493 --> 00:31:13.365
So there is a philosophy
which I'm sure some of you

00:31:13.365 --> 00:31:15.350
have heard of called
utilitarianism.

00:31:15.350 --> 00:31:20.170
Worst named philosophy
ever, but the idea behind it

00:31:20.170 --> 00:31:23.060
actually makes a lot of
sense, especially if you're

00:31:23.060 --> 00:31:25.140
thinking about the
world's problems

00:31:25.140 --> 00:31:26.860
in this global kind of way.

00:31:26.860 --> 00:31:29.197
And I'm going to first
give you an example

00:31:29.197 --> 00:31:31.780
of utilitarian progress and then
say how I think we got there.

00:31:31.780 --> 00:31:36.040
So Jeremy Bentham, who was
the original utilitarian

00:31:36.040 --> 00:31:40.270
philosopher, wrote
one of what is

00:31:40.270 --> 00:31:42.890
or may be one of the first
defenses of what we now

00:31:42.890 --> 00:31:44.570
call gay rights.

00:31:44.570 --> 00:31:47.820
This was in the 18th century
at a time when being gay

00:31:47.820 --> 00:31:50.180
was punishable by death.

00:31:50.180 --> 00:31:53.560
And in this passage
that I love, he

00:31:53.560 --> 00:31:55.250
said, I've been
tormenting myself

00:31:55.250 --> 00:32:00.460
for years to try to find a
reason why it makes sense

00:32:00.460 --> 00:32:04.420
to treat being gay in
this way, because it

00:32:04.420 --> 00:32:07.060
does seem wrong to me, like
it does to so many people

00:32:07.060 --> 00:32:08.350
around here.

00:32:08.350 --> 00:32:11.670
But upon the principle of
utility, I can find none.

00:32:11.670 --> 00:32:13.965
And so he came to the
conclusion that maybe being gay

00:32:13.965 --> 00:32:17.645
is not so bad, and this was in
the end of the 18th century.

00:32:17.645 --> 00:32:19.240
I may have said
19th century before.

00:32:19.240 --> 00:32:20.980
18th century.

00:32:20.980 --> 00:32:21.970
How did he do that?

00:32:21.970 --> 00:32:23.800
What philosophy was he applying?

00:32:23.800 --> 00:32:26.280
Basically, his
philosophy says, you

00:32:26.280 --> 00:32:28.120
should try to promote
the greater good,

00:32:28.120 --> 00:32:30.030
and this really has
two parts to it.

00:32:30.030 --> 00:32:34.940
One is that everybody's
well being counts the same.

00:32:34.940 --> 00:32:36.490
So it's not just
love thy neighbor,

00:32:36.490 --> 00:32:38.570
treat thy neighbor as
you treat yourself.

00:32:38.570 --> 00:32:42.605
It's everybody counts, and
that's a pan-tribal idea.

00:32:42.605 --> 00:32:44.730
And then the question is,
well, what's the measure?

00:32:44.730 --> 00:32:46.982
What is it that matters
for each person?

00:32:46.982 --> 00:32:48.440
And the conclusion
that he came to,

00:32:48.440 --> 00:32:51.410
and others since have affirmed,
is that what ultimately matters

00:32:51.410 --> 00:32:54.280
is the quality of
people's experience.

00:32:54.280 --> 00:32:56.550
And the argument for
this goes like this.

00:32:56.550 --> 00:32:59.030
Take anything that
you care about

00:32:59.030 --> 00:33:01.334
and keep asking, why
do you care about

00:33:01.334 --> 00:33:02.750
that until you run
out of answers.

00:33:02.750 --> 00:33:03.850
You say, you came to work today.

00:33:03.850 --> 00:33:04.730
Well, why'd you come to work?

00:33:04.730 --> 00:33:06.420
Well, I enjoy work and I
also need to make money.

00:33:06.420 --> 00:33:08.128
Well, what do you need
to make money for?

00:33:08.128 --> 00:33:09.537
Well, I need a place to live.

00:33:09.537 --> 00:33:10.870
Why do you need a place to live?

00:33:10.870 --> 00:33:12.670
Why can't you just
wander around?

00:33:12.670 --> 00:33:14.070
Well, it gets cold at night.

00:33:14.070 --> 00:33:14.790
I want a place
where I can sleep.

00:33:14.790 --> 00:33:16.050
Well, what's wrong
with it being cold?

00:33:16.050 --> 00:33:16.870
Well, it's just unpleasant.

00:33:16.870 --> 00:33:18.510
What's wrong with
it being unpleasant?

00:33:18.510 --> 00:33:19.690
It's just bad.

00:33:19.690 --> 00:33:21.265
That's where you
run out of answers.

00:33:21.265 --> 00:33:22.820
And the idea is that
there are a lot of things

00:33:22.820 --> 00:33:24.695
that we care about, but
when you keep asking,

00:33:24.695 --> 00:33:27.010
why do you care about that,
ultimately, it comes down

00:33:27.010 --> 00:33:29.590
to the quality of somebody's
experience, which you can,

00:33:29.590 --> 00:33:32.170
I think, somewhat accurately
but somewhat misleadingly,

00:33:32.170 --> 00:33:34.000
call someone's happiness.

00:33:34.000 --> 00:33:37.490
And so what Bentham said is we
should be maximizing happiness

00:33:37.490 --> 00:33:38.397
impartially.

00:33:38.397 --> 00:33:40.480
Everyone's happiness counts
the same and happiness

00:33:40.480 --> 00:33:43.420
is what ultimately matters.

00:33:43.420 --> 00:33:46.140
And that's what led him
to the conclusion to say,

00:33:46.140 --> 00:33:48.570
you know what, maybe
being gay is fine.

00:33:48.570 --> 00:33:51.460
I have this automatic
setting, this point and shoot

00:33:51.460 --> 00:33:54.954
moral reaction, like all
the other people around me--

00:33:54.954 --> 00:33:56.370
Not all the other
people, but most

00:33:56.370 --> 00:33:59.960
of the other people around
me in 18th century England

00:33:59.960 --> 00:34:02.060
that this is a
terrible thing to do,

00:34:02.060 --> 00:34:04.450
but I've got this
philosophy that I

00:34:04.450 --> 00:34:07.120
use to sort of measure
the worth of things.

00:34:07.120 --> 00:34:09.570
And that led Bentham and
others since to the conclusion

00:34:09.570 --> 00:34:13.310
that there's really actually
no problem with being gay.

00:34:13.310 --> 00:34:15.980
And in fact, Bentham and his
successor, John Stuart Mill,

00:34:15.980 --> 00:34:18.960
I think got it right about every
major moral and political issue

00:34:18.960 --> 00:34:19.580
of their day.

00:34:19.580 --> 00:34:21.850
They were among the first
opponents of slavery,

00:34:21.850 --> 00:34:23.350
they were among the
first proponents

00:34:23.350 --> 00:34:24.940
of free speech and free markets.

00:34:24.940 --> 00:34:26.440
I think the way
they were able to do

00:34:26.440 --> 00:34:29.510
this is with slow thinking,
by putting their gut

00:34:29.510 --> 00:34:34.100
reactions aside and asking, what
good or bad does this actually

00:34:34.100 --> 00:34:35.940
do for the world?

00:34:35.940 --> 00:34:38.969
Now, this is a very
controversial philosophy,

00:34:38.969 --> 00:34:40.941
and I think it's
widely misunderstood.

00:34:40.941 --> 00:34:42.690
And a lot of what's
controversial about it

00:34:42.690 --> 00:34:45.210
is that it seems to get the
wrong answers in certain cases.

00:34:45.210 --> 00:34:49.491
So a famous case that I've spent
a lot of time thinking about,

00:34:49.491 --> 00:34:52.050
you have a trolley headed
towards five people

00:34:52.050 --> 00:34:53.880
and you can save them
but the only way you

00:34:53.880 --> 00:34:56.066
can save them is by
pushing this big person off

00:34:56.066 --> 00:34:57.690
of this footbridge
that you're standing

00:34:57.690 --> 00:35:00.064
on and into the trolley's
path, and then he dies and gets

00:35:00.064 --> 00:35:02.680
killed by the trolley but
the five people will live.

00:35:02.680 --> 00:35:04.520
Is that OK?

00:35:04.520 --> 00:35:05.750
No, you can't jump yourself.

00:35:05.750 --> 00:35:07.224
Yes, this will work.

00:35:07.224 --> 00:35:08.890
Even with those
assumptions, most people

00:35:08.890 --> 00:35:11.760
still say that feels wrong.

00:35:11.760 --> 00:35:15.010
And this seems to be a powerful
argument against this idea

00:35:15.010 --> 00:35:17.500
that we should be
promoting the greater good.

00:35:17.500 --> 00:35:19.677
Sometimes it's wrong to
kill one person in order

00:35:19.677 --> 00:35:20.510
to save five people.

00:35:20.510 --> 00:35:23.470
At least that's how it feels.

00:35:23.470 --> 00:35:25.760
This is a complicated
set of questions.

00:35:25.760 --> 00:35:32.100
My sense is that
it's good that we

00:35:32.100 --> 00:35:34.870
are uncomfortable
with killing people.

00:35:34.870 --> 00:35:36.750
The world would be
a much worse place

00:35:36.750 --> 00:35:39.220
if we all felt
totally comfortable

00:35:39.220 --> 00:35:41.660
going around pushing
people off of footbridges.

00:35:41.660 --> 00:35:44.130
And so it's good that we
have that automatic setting.

00:35:44.130 --> 00:35:46.300
But then for any automatic
setting that we have,

00:35:46.300 --> 00:35:48.090
it will always be
possible to contrive

00:35:48.090 --> 00:35:50.680
a situation in which
the greater good it is

00:35:50.680 --> 00:35:53.420
in conflict with what
that gut reaction, what

00:35:53.420 --> 00:35:54.965
that automatic setting says.

00:35:57.655 --> 00:35:59.030
And I think, again,
this connects

00:35:59.030 --> 00:36:00.370
with a lot of the
kinds of problems

00:36:00.370 --> 00:36:02.360
that, for example, someone
designing a driverless car

00:36:02.360 --> 00:36:03.330
has to think about.

00:36:03.330 --> 00:36:04.788
What are the
trade-offs that you're

00:36:04.788 --> 00:36:06.680
willing to make when
it comes to safety?

00:36:06.680 --> 00:36:10.090
Would you have a car
drive into one person

00:36:10.090 --> 00:36:13.270
deliberately if it was a
way of saving more lives

00:36:13.270 --> 00:36:14.290
and so on and so forth?

00:36:14.290 --> 00:36:17.910
These are things we can talk
more about in the discussion.

00:36:17.910 --> 00:36:19.970
I want to leave you
more with a problem than

00:36:19.970 --> 00:36:23.080
with my specific solution.

00:36:23.080 --> 00:36:25.547
Once again, we
have gut reactions

00:36:25.547 --> 00:36:27.880
that do a good job of dealing
with everyday social life,

00:36:27.880 --> 00:36:30.690
of moral life within the
tribe, but the problems

00:36:30.690 --> 00:36:33.676
that we face as a nation and as
a world are more complicated.

00:36:33.676 --> 00:36:35.050
They're not the
kinds of problems

00:36:35.050 --> 00:36:38.620
for which our brains,
biologically or culturally,

00:36:38.620 --> 00:36:39.744
were necessarily designed.

00:36:39.744 --> 00:36:42.410
And so at the very least, I want
to leave you with this thought,

00:36:42.410 --> 00:36:45.160
that when it comes to the
morality of everyday life,

00:36:45.160 --> 00:36:47.380
it's probably a good
thing to think fast.

00:36:47.380 --> 00:36:49.100
Your gut reactions,
more often than not,

00:36:49.100 --> 00:36:50.600
are going to make
you a good person.

00:36:50.600 --> 00:36:52.550
But when it comes to
global morality, when

00:36:52.550 --> 00:36:54.060
it comes to the
complicated problems

00:36:54.060 --> 00:36:56.320
that we face as people
within larger nations

00:36:56.320 --> 00:36:58.210
and nations within
a larger world,

00:36:58.210 --> 00:37:00.680
our gut reactions
can't all be right.

00:37:00.680 --> 00:37:03.220
We have different gut reactions
about the problems that

00:37:03.220 --> 00:37:04.636
divide us, and
we're going to need

00:37:04.636 --> 00:37:10.280
some kind of moral system,
some kind of meta-morality,

00:37:10.280 --> 00:37:15.070
to guide us as we deal
with those problems.

00:37:15.070 --> 00:37:17.250
So looking forward to
hearing your thoughts,

00:37:17.250 --> 00:37:19.126
and thanks again.

00:37:19.126 --> 00:37:24.260
[APPLAUSE]

00:37:24.260 --> 00:37:26.750
AUDIENCE: I'll take
your last point.

00:37:26.750 --> 00:37:28.530
How do we get to
that meta-morality?

00:37:28.530 --> 00:37:31.540
And I think even that seems
like a huge concession

00:37:31.540 --> 00:37:34.110
for some people,
saying, look, obviously

00:37:34.110 --> 00:37:36.320
what you have isn't scalable.

00:37:36.320 --> 00:37:38.979
For me, I always struggle
with this in the public policy

00:37:38.979 --> 00:37:39.520
conversation.

00:37:39.520 --> 00:37:41.930
It's like, just because this
makes you uncomfortable,

00:37:41.930 --> 00:37:46.680
not sufficient for a nation
of 330 million people.

00:37:46.680 --> 00:37:48.610
How do people get
to that concession?

00:37:48.610 --> 00:37:49.910
JOSHUA GREENE: Right.

00:37:49.910 --> 00:37:51.730
I was just talking
about this with Kent.

00:37:51.730 --> 00:37:58.311
My hope is that self
knowledge can be empowering.

00:37:58.311 --> 00:38:00.560
When you just have your gut
reactions and all you know

00:38:00.560 --> 00:38:02.240
is what they're
telling you, then

00:38:02.240 --> 00:38:04.280
you're just going to
trust your gut reactions.

00:38:04.280 --> 00:38:09.180
But if you have an
understanding of where these gut

00:38:09.180 --> 00:38:11.460
reactions come from
and how they work,

00:38:11.460 --> 00:38:13.650
you can recognize
that they're useful

00:38:13.650 --> 00:38:15.890
and also recognize that
they have limitations, just

00:38:15.890 --> 00:38:17.499
like the gut reactions
on the camera.

00:38:17.499 --> 00:38:18.915
Let me take this
is an opportunity

00:38:18.915 --> 00:38:21.577
to say a little bit more
about the trolley case,

00:38:21.577 --> 00:38:23.160
where most people
think, gosh, it sure

00:38:23.160 --> 00:38:24.980
feels wrong to promote
the greater good.

00:38:24.980 --> 00:38:26.969
So one experiment we
did a few years ago,

00:38:26.969 --> 00:38:28.510
we gave some people
the version where

00:38:28.510 --> 00:38:30.176
you can push the guy
off the footbridge.

00:38:30.176 --> 00:38:32.590
In this version, about 30%
of people said that it's OK.

00:38:32.590 --> 00:38:34.290
So most people said
it's not OK to push

00:38:34.290 --> 00:38:35.415
the guy off the footbridge.

00:38:35.415 --> 00:38:38.320
We gave another group
of people this version.

00:38:38.320 --> 00:38:40.866
Instead of you're on the
footbridge next to the guy

00:38:40.866 --> 00:38:42.240
and you can push
him, you can hit

00:38:42.240 --> 00:38:44.190
a switch that will
open a trap door

00:38:44.190 --> 00:38:47.180
and drop the guy
onto the tracks,

00:38:47.180 --> 00:38:49.270
and that way save
the five people.

00:38:49.270 --> 00:38:52.370
Now about twice as many
people say that it's OK.

00:38:52.370 --> 00:38:54.245
A lot of what we're
specifically sensitive to

00:38:54.245 --> 00:38:56.620
is something like the difference
between pushing somebody

00:38:56.620 --> 00:38:58.850
with your hands versus
hitting a switch.

00:38:58.850 --> 00:39:01.190
And on the one hand,
we can recognize

00:39:01.190 --> 00:39:04.370
it's good that we're
emotionally averse to going

00:39:04.370 --> 00:39:05.860
around shoving each other.

00:39:05.860 --> 00:39:08.480
We don't want a world in which
people are cool with that.

00:39:08.480 --> 00:39:11.040
But at the same
time, we don't want

00:39:11.040 --> 00:39:14.940
that to be an obstacle to the
greater good in the long run.

00:39:14.940 --> 00:39:18.432
It's a fanciful kind of case in
the trolley case they give you,

00:39:18.432 --> 00:39:19.890
but these things
really matter when

00:39:19.890 --> 00:39:21.598
it comes to medical
ethics, when it comes

00:39:21.598 --> 00:39:26.320
to something like abortion or
physician-assisted suicide.

00:39:26.320 --> 00:39:28.110
The American Medical
Association's stance

00:39:28.110 --> 00:39:30.290
is that if it feels like
the footbridge case,

00:39:30.290 --> 00:39:34.040
essentially, if it's
intentional and it's active,

00:39:34.040 --> 00:39:36.730
then a physician is not allowed
to end someone's life even

00:39:36.730 --> 00:39:39.139
if they want to because that
feels like shoving somebody

00:39:39.139 --> 00:39:39.930
off the footbridge.

00:39:39.930 --> 00:39:42.040
Whereas you can
withhold treatment

00:39:42.040 --> 00:39:45.010
or can give somebody
pain medication

00:39:45.010 --> 00:39:47.110
to ease their pain knowing
that it will kill them,

00:39:47.110 --> 00:39:49.530
so it's a kind of side effect.

00:39:49.530 --> 00:39:51.947
These things I think actually
do matter for public policy.

00:39:51.947 --> 00:39:53.613
I'm just trying to
illustrate the answer

00:39:53.613 --> 00:39:56.010
to your question, which
is that once we understand

00:39:56.010 --> 00:39:59.570
how quirky our intuitions are
and how contingent they can

00:39:59.570 --> 00:40:02.519
be-- contingent
in the sense of I

00:40:02.519 --> 00:40:05.060
grew up in this kind of culture
versus that kind of culture--

00:40:05.060 --> 00:40:08.261
I hope that that knowledge
can help us see, again, where

00:40:08.261 --> 00:40:10.510
they're likely to be helping
us and where they're also

00:40:10.510 --> 00:40:12.660
likely to be leading us astray.

00:40:12.660 --> 00:40:14.780
AUDIENCE: But for
any individual,

00:40:14.780 --> 00:40:16.504
how do we get them there?

00:40:16.504 --> 00:40:17.270
JOSHUA GREENE: Ah.

00:40:17.270 --> 00:40:19.650
Well, this is
essentially what I try

00:40:19.650 --> 00:40:22.650
to do as a scientist
and a teacher.

00:40:22.650 --> 00:40:26.270
I teach classes, write
books, give talks like this.

00:40:26.270 --> 00:40:29.077
This audience is
probably the audience

00:40:29.077 --> 00:40:31.410
that needs to hear this kind
of thing least, in a sense,

00:40:31.410 --> 00:40:35.210
because you are so
sophisticated and systematic

00:40:35.210 --> 00:40:39.460
in your thinking, but I think
it's a long, slow process.

00:40:39.460 --> 00:40:46.730
My hope is that a kind of
experimental social science

00:40:46.730 --> 00:40:50.360
will make its way into the
basic curriculum of an educated

00:40:50.360 --> 00:40:50.860
person.

00:40:50.860 --> 00:40:53.014
When you're in high school,
you learn about physics

00:40:53.014 --> 00:40:55.430
and you learn about chemistry
and you learn about biology,

00:40:55.430 --> 00:40:57.791
and you learn about the kinds
of experiments that led us

00:40:57.791 --> 00:40:59.790
to a detailed knowledge
of how these things work

00:40:59.790 --> 00:41:01.120
in a mechanical level.

00:41:01.120 --> 00:41:03.660
And then social studies,
it's much more broad.

00:41:03.660 --> 00:41:07.590
It doesn't feel like science.

00:41:07.590 --> 00:41:09.830
My hope is that
30 years from now,

00:41:09.830 --> 00:41:11.810
maybe sooner, when
kids go to school,

00:41:11.810 --> 00:41:15.670
that human behavior
will be very much

00:41:15.670 --> 00:41:19.730
a topic of scientific study.

00:41:19.730 --> 00:41:23.360
And I think that if we grow
up with an understanding

00:41:23.360 --> 00:41:26.340
that the cognitive
processes, the neural

00:41:26.340 --> 00:41:28.020
processes that lead
to our behavior

00:41:28.020 --> 00:41:31.350
are sensible and adaptive and
intelligent in a lot of ways,

00:41:31.350 --> 00:41:34.410
but also inflexible and
limited and problematic

00:41:34.410 --> 00:41:39.150
in a lot of ways, I think if
that's just part of an educated

00:41:39.150 --> 00:41:41.750
person's common sense,
I think that would

00:41:41.750 --> 00:41:43.650
make all the difference.

00:41:43.650 --> 00:41:46.220
AUDIENCE: So you mentioned
two ways of thinking,

00:41:46.220 --> 00:41:48.500
like the default
mode of morality

00:41:48.500 --> 00:41:51.830
and the slow
understanding, but have you

00:41:51.830 --> 00:41:56.910
studied the paralysis that
comes from slow understanding

00:41:56.910 --> 00:41:58.800
or logical thinking?

00:41:58.800 --> 00:42:01.780
So for example, in
the trolley example,

00:42:01.780 --> 00:42:04.910
if I'm really trying to
understand the situation

00:42:04.910 --> 00:42:09.475
and trying to do greater good, I
need to know those five people,

00:42:09.475 --> 00:42:11.370
are they good for
the world or not?

00:42:11.370 --> 00:42:13.330
Are they fanatic
terrorists who are

00:42:13.330 --> 00:42:15.267
trying to bomb the trolley
in the first place,

00:42:15.267 --> 00:42:16.600
or is it like a fat Gandhi here?

00:42:19.860 --> 00:42:22.490
Have you studied that
paralysis that comes with that?

00:42:26.125 --> 00:42:27.750
JOSHUA GREENE: In a
very limited sense,

00:42:27.750 --> 00:42:30.083
that is, when you give people
a difficult moral dilemma,

00:42:30.083 --> 00:42:33.170
they take longer.

00:42:33.170 --> 00:42:36.445
So in a boring sense,
I have measures

00:42:36.445 --> 00:42:37.820
of what you might
call paralysis.

00:42:41.230 --> 00:42:44.370
I don't think this is an
insurmountable problem.

00:42:44.370 --> 00:42:48.524
I think that any sensible
system for making decisions,

00:42:48.524 --> 00:42:49.940
especially at the
policy level, is

00:42:49.940 --> 00:42:52.250
going to be taking into
account the consequences

00:42:52.250 --> 00:42:53.820
of the available options.

00:42:53.820 --> 00:42:55.820
And when you're dealing
with big problems

00:42:55.820 --> 00:42:58.890
and non-hypothetical
problems, the amount

00:42:58.890 --> 00:43:01.320
of time that you can spend
trying to collect evidence

00:43:01.320 --> 00:43:02.370
and worrying about
whether or not

00:43:02.370 --> 00:43:04.953
your evidence is as good as it
can be, about the consequences,

00:43:04.953 --> 00:43:06.460
is infinite.

00:43:06.460 --> 00:43:10.520
So what I would say
is this is a problem

00:43:10.520 --> 00:43:12.700
if you take this approach.

00:43:12.700 --> 00:43:15.670
I've decided to
rename utilitarianism

00:43:15.670 --> 00:43:18.030
as deep pragmatism,
because I think it actually

00:43:18.030 --> 00:43:20.880
gets at what it is in
a more transparent way.

00:43:20.880 --> 00:43:22.980
If you take this pragmatic
approach to decision

00:43:22.980 --> 00:43:24.604
making where you say
it's fundamentally

00:43:24.604 --> 00:43:28.900
about the consequences
for all concerned,

00:43:28.900 --> 00:43:30.820
there's no bright
line where you say,

00:43:30.820 --> 00:43:32.350
OK, now I have all
the information

00:43:32.350 --> 00:43:35.230
I need and I can
make my decision.

00:43:35.230 --> 00:43:37.510
But the only people who
can avoid that problem

00:43:37.510 --> 00:43:38.235
are dogmatists.

00:43:38.235 --> 00:43:40.610
The only people who can avoid
that problem at the highest

00:43:40.610 --> 00:43:41.985
level are people
who say, I don't

00:43:41.985 --> 00:43:45.490
need to know what all the
consequences are going to be.

00:43:45.490 --> 00:43:48.470
Some people think of it as
a fault of deep pragmatism,

00:43:48.470 --> 00:43:51.080
as I now call it,
that it requires

00:43:51.080 --> 00:43:53.730
this impossible information
gathering exercise,

00:43:53.730 --> 00:43:56.080
but I think it's a virtue
because it requires it

00:43:56.080 --> 00:44:01.100
explicitly, because it makes the
difficulty and the complexity

00:44:01.100 --> 00:44:04.770
of the problem in the foreground
instead of in the background.

00:44:04.770 --> 00:44:07.230
Instead of trying to have a
simple principle that you just

00:44:07.230 --> 00:44:09.657
live by but that ignores
the consequences,

00:44:09.657 --> 00:44:11.490
you say, look, these
are difficult problems.

00:44:11.490 --> 00:44:12.780
And it leads to a
kind of humility

00:44:12.780 --> 00:44:15.100
as well because you know that
no matter how hard you've

00:44:15.100 --> 00:44:16.641
studied the problem,
there are always

00:44:16.641 --> 00:44:18.930
going to be things that
you haven't accounted for.

00:44:18.930 --> 00:44:21.666
So I think it's just the
reality of a complex world that

00:44:21.666 --> 00:44:24.082
gives us that problem, and
anyone who thinks that they can

00:44:24.082 --> 00:44:26.492
do away with it is just
selling a bill of goods.

00:44:26.492 --> 00:44:27.950
AUDIENCE: Over the
centuries, we've

00:44:27.950 --> 00:44:30.415
had a gradual
progress in what I see

00:44:30.415 --> 00:44:33.040
as the expansion of the scope of
what we consider to be people.

00:44:35.570 --> 00:44:39.860
It was progress when Irishman
were allowed to be people.

00:44:39.860 --> 00:44:42.807
Black people, women,
now gays are just

00:44:42.807 --> 00:44:44.390
beginning to be
acknowledged as people

00:44:44.390 --> 00:44:48.650
worthy of treating the
same as anyone else.

00:44:48.650 --> 00:44:51.950
Does that judgement
occur in the fast system,

00:44:51.950 --> 00:44:55.610
and what promotes
people expanding

00:44:55.610 --> 00:45:00.010
their scope of who
is considered people?

00:45:00.010 --> 00:45:04.650
JOSHUA GREENE: I think it starts
out slow and ends up fast.

00:45:04.650 --> 00:45:07.450
That is, it takes someone
like Jeremy Bentham

00:45:07.450 --> 00:45:12.010
to say, why shouldn't gays have
the same rights as everybody

00:45:12.010 --> 00:45:14.021
else when it comes to
their personal lives?

00:45:14.021 --> 00:45:16.270
Or it takes someone like
John Stuart Mill or his wife,

00:45:16.270 --> 00:45:18.290
Harriet Taylor Mill, to
say, why shouldn't women

00:45:18.290 --> 00:45:20.780
have the same political freedoms
and education as everybody

00:45:20.780 --> 00:45:21.280
else?

00:45:21.280 --> 00:45:23.530
And both of them--
Bentham on gays--

00:45:23.530 --> 00:45:24.750
he kept his work private.

00:45:24.750 --> 00:45:27.050
But the people who first
come out with these ideas,

00:45:27.050 --> 00:45:28.814
they're viewed as crazy.

00:45:28.814 --> 00:45:30.980
But there are a few people
who say, you know, you've

00:45:30.980 --> 00:45:32.600
got a point there.

00:45:32.600 --> 00:45:36.020
And then as more people--
this is what historians call,

00:45:36.020 --> 00:45:38.560
the terms someone's
called is a moral cascade.

00:45:38.560 --> 00:45:40.520
It starts with a
small number of people

00:45:40.520 --> 00:45:42.930
who have this crazy
idea, and then

00:45:42.930 --> 00:45:45.630
a larger and larger and
larger circle of people

00:45:45.630 --> 00:45:46.750
take it seriously.

00:45:46.750 --> 00:45:49.140
And the more people
take it seriously,

00:45:49.140 --> 00:45:50.850
the less of a moral
adventurer you

00:45:50.850 --> 00:45:52.730
have to be to join the club.

00:45:52.730 --> 00:45:55.280
And so over time,
what happens is

00:45:55.280 --> 00:45:57.530
it starts out as a kind
of theoretical argument.

00:45:57.530 --> 00:45:59.100
Hm, why shouldn't
we be giving money

00:45:59.100 --> 00:46:01.266
to poor people on the other
side of the world, which

00:46:01.266 --> 00:46:02.990
seems crazy?

00:46:02.990 --> 00:46:08.720
And then as these ideas
gain currency and as people

00:46:08.720 --> 00:46:13.610
are immersed in them,
it becomes just part

00:46:13.610 --> 00:46:15.030
of their common sense.

00:46:15.030 --> 00:46:17.699
I remember seeing in my
own lifetime attitudes

00:46:17.699 --> 00:46:18.240
towards gays.

00:46:18.240 --> 00:46:23.330
So high school or whatever,
it was common for people

00:46:23.330 --> 00:46:24.974
to call someone a fag.

00:46:24.974 --> 00:46:26.890
It was just the general
expletive or whatever.

00:46:26.890 --> 00:46:29.840
It was not a gay
friendly environment.

00:46:29.840 --> 00:46:31.440
Now I'm over that.

00:46:31.440 --> 00:46:34.480
I shudder to think what
kind of emotional residual

00:46:34.480 --> 00:46:38.150
I may have from
that time period,

00:46:38.150 --> 00:46:40.810
but I think my son, who goes
to public school in Cambridge,

00:46:40.810 --> 00:46:44.890
Massachusetts, he
doesn't have that at all.

00:46:44.890 --> 00:46:47.272
In his kindergarten class,
they have a unit on families,

00:46:47.272 --> 00:46:48.730
and some families
have two mommies,

00:46:48.730 --> 00:46:50.188
and some families
have two daddies,

00:46:50.188 --> 00:46:52.380
and it's all completely fine.

00:46:52.380 --> 00:46:56.460
And so he just doesn't even
have the slightest feeling

00:46:56.460 --> 00:46:58.650
that there's something
wrong with this.

00:46:58.650 --> 00:47:01.520
And so it starts out as this
kind of intellectual argument,

00:47:01.520 --> 00:47:06.500
and then over time becomes
just part of common sense.

00:47:06.500 --> 00:47:08.670
Linking your question
together with your question,

00:47:08.670 --> 00:47:10.628
I have two quotes in the
beginning of the book.

00:47:10.628 --> 00:47:13.510
One is from Anton Chekhov
which says-- pardon

00:47:13.510 --> 00:47:14.930
the gendered nature
of this-- man

00:47:14.930 --> 00:47:17.100
will come better when you
show him what he's like.

00:47:17.100 --> 00:47:18.300
That's the strategy.

00:47:18.300 --> 00:47:19.810
And then the answer
to your question

00:47:19.810 --> 00:47:22.059
is the second quote, which
I got from a fortune cookie

00:47:22.059 --> 00:47:23.940
from a noodle shop in
Princeton, New Jersey,

00:47:23.940 --> 00:47:26.130
which is "the philosophy
of one century

00:47:26.130 --> 00:47:27.710
is the common
sense of the next."

00:47:27.710 --> 00:47:30.080
And that's, I think,
a good summary

00:47:30.080 --> 00:47:32.360
of the answer to
those two questions.

00:47:32.360 --> 00:47:36.140
AUDIENCE: Once you determine
that our intuitive responses

00:47:36.140 --> 00:47:38.580
might not be the
best for determining

00:47:38.580 --> 00:47:42.060
the extent of our moral
obligations to others,

00:47:42.060 --> 00:47:45.320
how do you propose we
do set about determining

00:47:45.320 --> 00:47:46.050
that boundary?

00:47:46.050 --> 00:47:48.812
Are you in favor of Peter
Singer's giving until it hurts,

00:47:48.812 --> 00:47:50.270
or are there other
philosophers who

00:47:50.270 --> 00:47:54.470
have answered the question of
the extent of how far we should

00:47:54.470 --> 00:47:56.800
go?

00:47:56.800 --> 00:48:02.480
JOSHUA GREENE: The short answer
is I agree with Peter Singer,

00:48:02.480 --> 00:48:07.975
but not so much necessarily
in giving until it hurts.

00:48:07.975 --> 00:48:09.600
I think the right
way to think about it

00:48:09.600 --> 00:48:15.130
is, what is the
best policy for us?

00:48:15.130 --> 00:48:18.110
How can we do the most
good, taking into account

00:48:18.110 --> 00:48:22.040
our natural psychological
limitations and biases?

00:48:22.040 --> 00:48:24.489
So we didn't evolve to be
perfect altruists where we're

00:48:24.489 --> 00:48:27.030
concerned about the well being
of strangers on the other side

00:48:27.030 --> 00:48:28.654
of the world as much
as we're concerned

00:48:28.654 --> 00:48:31.210
about our own well being or
the well being of our family.

00:48:31.210 --> 00:48:35.600
And so you might say, well,
instead of having a birthday

00:48:35.600 --> 00:48:40.220
party for my son or daughter,
I should just take that money

00:48:40.220 --> 00:48:42.360
and give it to UNICEF
or something like that.

00:48:42.360 --> 00:48:45.090
But I think that's going to
be exceptionally hard for me,

00:48:45.090 --> 00:48:46.810
because I really
care about my kids,

00:48:46.810 --> 00:48:49.332
and we all have our
personal commitments.

00:48:49.332 --> 00:48:50.790
The good news is
that I don't think

00:48:50.790 --> 00:48:55.090
we have to give until
it hurts all that much.

00:48:55.090 --> 00:48:58.320
That is, if all of us
in the affluent world

00:48:58.320 --> 00:49:03.190
would just give a couple
percent of our income

00:49:03.190 --> 00:49:06.080
towards effective
charities, that

00:49:06.080 --> 00:49:07.852
would make an
enormous difference.

00:49:07.852 --> 00:49:10.060
I don't want to say that
would alleviate all poverty,

00:49:10.060 --> 00:49:12.100
and I think there are a
lot of challenging problems

00:49:12.100 --> 00:49:13.933
about what's exactly
the right way to do it,

00:49:13.933 --> 00:49:17.949
but I actually don't think that
it requires that much pain.

00:49:17.949 --> 00:49:20.490
Now, you might say, well, why
not keep giving until it hurts?

00:49:20.490 --> 00:49:22.365
Well, there are two ways
to think about that.

00:49:22.365 --> 00:49:26.090
One is I can try to do
a lot of good myself,

00:49:26.090 --> 00:49:28.410
but if I make a saint
out of myself where

00:49:28.410 --> 00:49:29.980
I'm living this
impoverished life

00:49:29.980 --> 00:49:32.563
and giving all my resources to
other people, people look at me

00:49:32.563 --> 00:49:35.000
and say, wow, you're inspiring,
that's really impressive,

00:49:35.000 --> 00:49:37.300
and then not be really
inspired and not

00:49:37.300 --> 00:49:38.610
really do something themselves.

00:49:38.610 --> 00:49:42.130
Whereas if you could say, look,
I'm a person just like you

00:49:42.130 --> 00:49:45.350
and I mostly care about myself
and my friends and my family,

00:49:45.350 --> 00:49:48.010
but instead of giving
nothing or almost nothing,

00:49:48.010 --> 00:49:48.947
I give this much.

00:49:48.947 --> 00:49:50.780
And someone else could
look at that and say,

00:49:50.780 --> 00:49:52.450
you know what, I can
do the same thing.

00:49:52.450 --> 00:49:54.580
And so in the long
run, I actually

00:49:54.580 --> 00:49:59.900
think that promoting a
sustainable culture of altruism

00:49:59.900 --> 00:50:02.700
is probably a better strategy
than trying to be a hero.

00:50:02.700 --> 00:50:04.980
Being a hero, you
give more now, but it

00:50:04.980 --> 00:50:11.550
doesn't light the fire that can
get things going more broadly.

00:50:11.550 --> 00:50:13.220
To put the answer
another way, I think

00:50:13.220 --> 00:50:15.510
if you think in
the long, long run

00:50:15.510 --> 00:50:20.340
and think about how human
cultural dynamics works,

00:50:20.340 --> 00:50:22.799
giving until it really hurts,
you can do a lot of good now,

00:50:22.799 --> 00:50:25.173
but I don't think that's going
to be the long run answer.

00:50:25.173 --> 00:50:27.100
I think the long run
answer is everybody just

00:50:27.100 --> 00:50:28.625
willing to care a
little bit more.

00:50:28.625 --> 00:50:31.000
KENT WALKER: Round of applause
for Professor Josh Greene.

00:50:31.000 --> 00:50:31.900
Thank you very much.

00:50:31.900 --> 00:50:33.450
[APPLAUSE]

