WEBVTT
Kind: captions
Language: en

00:00:02.060 --> 00:00:03.879
 
thanks for having me here yeah so I'm

00:00:03.879 --> 00:00:03.889
thanks for having me here yeah so I'm
 

00:00:03.889 --> 00:00:06.130
thanks for having me here yeah so I'm
I'm based in the Cambridge office which

00:00:06.130 --> 00:00:06.140
I'm based in the Cambridge office which
 

00:00:06.140 --> 00:00:09.699
I'm based in the Cambridge office which
is like a hundred meters that way um and

00:00:09.699 --> 00:00:09.709
is like a hundred meters that way um and
 

00:00:09.709 --> 00:00:12.279
is like a hundred meters that way um and
we do a lot of stuff with deep learning

00:00:12.279 --> 00:00:12.289
we do a lot of stuff with deep learning
 

00:00:12.289 --> 00:00:14.500
we do a lot of stuff with deep learning
we've got a large group in Google brain

00:00:14.500 --> 00:00:14.510
we've got a large group in Google brain
 

00:00:14.510 --> 00:00:17.470
we've got a large group in Google brain
and other related fields so hopefully

00:00:17.470 --> 00:00:17.480
and other related fields so hopefully
 

00:00:17.480 --> 00:00:19.210
and other related fields so hopefully
that's interesting to some of you at

00:00:19.210 --> 00:00:19.220
that's interesting to some of you at
 

00:00:19.220 --> 00:00:22.570
that's interesting to some of you at
some point so I'm gonna talk for about

00:00:22.570 --> 00:00:22.580
some point so I'm gonna talk for about
 

00:00:22.580 --> 00:00:23.620
some point so I'm gonna talk for about
20 minutes or so

00:00:23.620 --> 00:00:23.630
20 minutes or so
 

00:00:23.630 --> 00:00:25.960
20 minutes or so
um this sort of image issues in image

00:00:25.960 --> 00:00:25.970
um this sort of image issues in image
 

00:00:25.970 --> 00:00:28.149
um this sort of image issues in image
classification theme I'm gonna hand it

00:00:28.149 --> 00:00:28.159
classification theme I'm gonna hand it
 

00:00:28.159 --> 00:00:30.550
classification theme I'm gonna hand it
over to my excellent colleague sunshine

00:00:30.550 --> 00:00:30.560
over to my excellent colleague sunshine
 

00:00:30.560 --> 00:00:32.170
over to my excellent colleague sunshine
Kai who's going to go through an

00:00:32.170 --> 00:00:32.180
Kai who's going to go through an
 

00:00:32.180 --> 00:00:34.750
Kai who's going to go through an
entirely different subject in using

00:00:34.750 --> 00:00:34.760
entirely different subject in using
 

00:00:34.760 --> 00:00:36.820
entirely different subject in using
tensor flow debugger and eager mode to

00:00:36.820 --> 00:00:36.830
tensor flow debugger and eager mode to
 

00:00:36.830 --> 00:00:40.110
tensor flow debugger and eager mode to
make work in tensor flow easier who's

00:00:40.110 --> 00:00:40.120
make work in tensor flow easier who's
 

00:00:40.120 --> 00:00:43.990
make work in tensor flow easier who's
maybe that would be good okay so let's

00:00:43.990 --> 00:00:44.000
maybe that would be good okay so let's
 

00:00:44.000 --> 00:00:46.360
maybe that would be good okay so let's
let's take a step back so if you guys

00:00:46.360 --> 00:00:46.370
let's take a step back so if you guys
 

00:00:46.370 --> 00:00:47.710
let's take a step back so if you guys
seen happy graphs like this before go

00:00:47.710 --> 00:00:47.720
seen happy graphs like this before go
 

00:00:47.720 --> 00:00:49.030
seen happy graphs like this before go
ahead and smile and not if you've seen

00:00:49.030 --> 00:00:49.040
ahead and smile and not if you've seen
 

00:00:49.040 --> 00:00:50.980
ahead and smile and not if you've seen
stuff like this yeah okay so this is a

00:00:50.980 --> 00:00:50.990
stuff like this yeah okay so this is a
 

00:00:50.990 --> 00:00:54.610
stuff like this yeah okay so this is a
happy graph on image net based image

00:00:54.610 --> 00:00:54.620
happy graph on image net based image
 

00:00:54.620 --> 00:00:56.740
happy graph on image net based image
classification so image net is a dataset

00:00:56.740 --> 00:00:56.750
classification so image net is a dataset
 

00:00:56.750 --> 00:01:00.520
classification so image net is a dataset
of million some odd images for this

00:01:00.520 --> 00:01:00.530
of million some odd images for this
 

00:01:00.530 --> 00:01:03.539
of million some odd images for this
challenge there were a thousand classes

00:01:03.539 --> 00:01:03.549
challenge there were a thousand classes
 

00:01:03.549 --> 00:01:08.200
challenge there were a thousand classes
and in 2011 back in the dark ages when

00:01:08.200 --> 00:01:08.210
and in 2011 back in the dark ages when
 

00:01:08.210 --> 00:01:10.959
and in 2011 back in the dark ages when
nobody knew how to do anything the state

00:01:10.959 --> 00:01:10.969
nobody knew how to do anything the state
 

00:01:10.969 --> 00:01:13.029
nobody knew how to do anything the state
of the art was something like 25% error

00:01:13.029 --> 00:01:13.039
of the art was something like 25% error
 

00:01:13.039 --> 00:01:16.239
of the art was something like 25% error
rate on this stuff and in the last call

00:01:16.239 --> 00:01:16.249
rate on this stuff and in the last call
 

00:01:16.249 --> 00:01:18.999
rate on this stuff and in the last call
it six seven years the reduction in

00:01:18.999 --> 00:01:19.009
it six seven years the reduction in
 

00:01:19.009 --> 00:01:21.489
it six seven years the reduction in
error rate has been kind of astounding

00:01:21.489 --> 00:01:21.499
error rate has been kind of astounding
 

00:01:21.499 --> 00:01:22.779
error rate has been kind of astounding
to the point where it's now been talked

00:01:22.779 --> 00:01:22.789
to the point where it's now been talked
 

00:01:22.789 --> 00:01:24.190
to the point where it's now been talked
about so much it's like no longer even

00:01:24.190 --> 00:01:24.200
about so much it's like no longer even
 

00:01:24.200 --> 00:01:25.929
about so much it's like no longer even
surprising and it was like yeah yeah we

00:01:25.929 --> 00:01:25.939
surprising and it was like yeah yeah we
 

00:01:25.939 --> 00:01:29.830
surprising and it was like yeah yeah we
see this human error rate is somewhere

00:01:29.830 --> 00:01:29.840
see this human error rate is somewhere
 

00:01:29.840 --> 00:01:32.289
see this human error rate is somewhere
between five and ten percent on this

00:01:32.289 --> 00:01:32.299
between five and ten percent on this
 

00:01:32.299 --> 00:01:36.730
between five and ten percent on this
task so the contemporary results of you

00:01:36.730 --> 00:01:36.740
task so the contemporary results of you
 

00:01:36.740 --> 00:01:39.550
task so the contemporary results of you
know 2.2 or whatever it is percent error

00:01:39.550 --> 00:01:39.560
know 2.2 or whatever it is percent error
 

00:01:39.560 --> 00:01:42.940
know 2.2 or whatever it is percent error
rate are really kind of astonishing and

00:01:42.940 --> 00:01:42.950
rate are really kind of astonishing and
 

00:01:42.950 --> 00:01:44.649
rate are really kind of astonishing and
you can look at a graph like this and

00:01:44.649 --> 00:01:44.659
you can look at a graph like this and
 

00:01:44.659 --> 00:01:47.789
you can look at a graph like this and
make reasonable claims that well

00:01:47.789 --> 00:01:47.799
make reasonable claims that well
 

00:01:47.799 --> 00:01:50.679
make reasonable claims that well
machines using deep learning are better

00:01:50.679 --> 00:01:50.689
machines using deep learning are better
 

00:01:50.689 --> 00:01:52.660
machines using deep learning are better
than humans at image classification on

00:01:52.660 --> 00:01:52.670
than humans at image classification on
 

00:01:52.670 --> 00:01:55.359
than humans at image classification on
this task that's kind of weird and kind

00:01:55.359 --> 00:01:55.369
this task that's kind of weird and kind
 

00:01:55.369 --> 00:01:57.069
this task that's kind of weird and kind
of amazing and maybe we can declare

00:01:57.069 --> 00:01:57.079
of amazing and maybe we can declare
 

00:01:57.079 --> 00:01:59.649
of amazing and maybe we can declare
victory and fill audiences full of

00:01:59.649 --> 00:01:59.659
victory and fill audiences full of
 

00:01:59.659 --> 00:02:01.209
victory and fill audiences full of
people clamoring to learn about deep

00:02:01.209 --> 00:02:01.219
people clamoring to learn about deep
 

00:02:01.219 --> 00:02:06.550
people clamoring to learn about deep
learning that's cool okay so um I'm

00:02:06.550 --> 00:02:06.560
learning that's cool okay so um I'm
 

00:02:06.560 --> 00:02:08.499
learning that's cool okay so um I'm
gonna talk not about image data itself

00:02:08.499 --> 00:02:08.509
gonna talk not about image data itself
 

00:02:08.509 --> 00:02:11.410
gonna talk not about image data itself
but about a slightly different image

00:02:11.410 --> 00:02:11.420
but about a slightly different image
 

00:02:11.420 --> 00:02:13.360
but about a slightly different image
data set basically people were like okay

00:02:13.360 --> 00:02:13.370
data set basically people were like okay
 

00:02:13.370 --> 00:02:15.220
data set basically people were like okay
obviously image net is too easy let's

00:02:15.220 --> 00:02:15.230
obviously image net is too easy let's
 

00:02:15.230 --> 00:02:15.890
obviously image net is too easy let's
make a large

00:02:15.890 --> 00:02:15.900
make a large
 

00:02:15.900 --> 00:02:17.780
make a large
more interesting data set so open images

00:02:17.780 --> 00:02:17.790
more interesting data set so open images
 

00:02:17.790 --> 00:02:20.000
more interesting data set so open images
was released I think a year or two ago

00:02:20.000 --> 00:02:20.010
was released I think a year or two ago
 

00:02:20.010 --> 00:02:21.890
was released I think a year or two ago
it's got about 9 million as opposed to 1

00:02:21.890 --> 00:02:21.900
it's got about 9 million as opposed to 1
 

00:02:21.900 --> 00:02:26.800
it's got about 9 million as opposed to 1
million images the the base dataset has

00:02:26.800 --> 00:02:26.810
million images the the base dataset has
 

00:02:26.810 --> 00:02:29.330
million images the the base dataset has
6,000 labels as opposed to 1000 labels

00:02:29.330 --> 00:02:29.340
6,000 labels as opposed to 1000 labels
 

00:02:29.340 --> 00:02:31.970
6,000 labels as opposed to 1000 labels
this is also multi labels so you get you

00:02:31.970 --> 00:02:31.980
this is also multi labels so you get you
 

00:02:31.980 --> 00:02:33.860
this is also multi labels so you get you
know if there's a person holding a rugby

00:02:33.860 --> 00:02:33.870
know if there's a person holding a rugby
 

00:02:33.870 --> 00:02:35.569
know if there's a person holding a rugby
ball you get both person and rugby ball

00:02:35.569 --> 00:02:35.579
ball you get both person and rugby ball
 

00:02:35.579 --> 00:02:38.030
ball you get both person and rugby ball
in the dataset it's got all kinds of

00:02:38.030 --> 00:02:38.040
in the dataset it's got all kinds of
 

00:02:38.040 --> 00:02:39.770
in the dataset it's got all kinds of
classes including stairs here which are

00:02:39.770 --> 00:02:39.780
classes including stairs here which are
 

00:02:39.780 --> 00:02:42.649
classes including stairs here which are
lovely Lee Illustrated and you can find

00:02:42.649 --> 00:02:42.659
lovely Lee Illustrated and you can find
 

00:02:42.659 --> 00:02:44.630
lovely Lee Illustrated and you can find
this on github it's a nice data set to

00:02:44.630 --> 00:02:44.640
this on github it's a nice data set to
 

00:02:44.640 --> 00:02:48.500
this on github it's a nice data set to
play around with so some colleagues and

00:02:48.500 --> 00:02:48.510
play around with so some colleagues and
 

00:02:48.510 --> 00:02:50.360
play around with so some colleagues and
I did some work of saying ok what

00:02:50.360 --> 00:02:50.370
I did some work of saying ok what
 

00:02:50.370 --> 00:02:52.690
I did some work of saying ok what
happens if we apply just a straight-up

00:02:52.690 --> 00:02:52.700
happens if we apply just a straight-up
 

00:02:52.700 --> 00:02:56.270
happens if we apply just a straight-up
inception based model to this data there

00:02:56.270 --> 00:02:56.280
inception based model to this data there
 

00:02:56.280 --> 00:02:57.830
inception based model to this data there
we trade it up and then we look at some

00:02:57.830 --> 00:02:57.840
we trade it up and then we look at some
 

00:02:57.840 --> 00:02:59.599
we trade it up and then we look at some
how it classifies some images that we

00:02:59.599 --> 00:02:59.609
how it classifies some images that we
 

00:02:59.609 --> 00:03:02.720
how it classifies some images that we
found on the web so here's one such

00:03:02.720 --> 00:03:02.730
found on the web so here's one such
 

00:03:02.730 --> 00:03:05.929
found on the web so here's one such
image image that we found on the web all

00:03:05.929 --> 00:03:05.939
image image that we found on the web all
 

00:03:05.939 --> 00:03:07.819
image image that we found on the web all
the images here are Creative Commons and

00:03:07.819 --> 00:03:07.829
the images here are Creative Commons and
 

00:03:07.829 --> 00:03:09.800
the images here are Creative Commons and
stuff like that so it's it's OK for us

00:03:09.800 --> 00:03:09.810
stuff like that so it's it's OK for us
 

00:03:09.810 --> 00:03:14.240
stuff like that so it's it's OK for us
to look at these and when we apply an

00:03:14.240 --> 00:03:14.250
to look at these and when we apply an
 

00:03:14.250 --> 00:03:17.449
to look at these and when we apply an
image based a image nodi kind of all to

00:03:17.449 --> 00:03:17.459
image based a image nodi kind of all to
 

00:03:17.459 --> 00:03:19.970
image based a image nodi kind of all to
this classifications we get back or kind

00:03:19.970 --> 00:03:19.980
this classifications we get back or kind
 

00:03:19.980 --> 00:03:22.099
this classifications we get back or kind
of what I personally would expect I'm

00:03:22.099 --> 00:03:22.109
of what I personally would expect I'm
 

00:03:22.109 --> 00:03:24.199
of what I personally would expect I'm
seeing things like bride dress ceremony

00:03:24.199 --> 00:03:24.209
seeing things like bride dress ceremony
 

00:03:24.209 --> 00:03:28.369
seeing things like bride dress ceremony
woman wedding all things that as an

00:03:28.369 --> 00:03:28.379
woman wedding all things that as an
 

00:03:28.379 --> 00:03:29.509
woman wedding all things that as an
American in this country at this time

00:03:29.509 --> 00:03:29.519
American in this country at this time
 

00:03:29.519 --> 00:03:31.789
American in this country at this time
I'm thinking or make makes sense for

00:03:31.789 --> 00:03:31.799
I'm thinking or make makes sense for
 

00:03:31.799 --> 00:03:34.580
I'm thinking or make makes sense for
this image cool maybe we did solve it

00:03:34.580 --> 00:03:34.590
this image cool maybe we did solve it
 

00:03:34.590 --> 00:03:38.659
this image cool maybe we did solve it
image classification so then we applied

00:03:38.659 --> 00:03:38.669
image classification so then we applied
 

00:03:38.669 --> 00:03:44.140
image classification so then we applied
it to another image also of a bride and

00:03:44.140 --> 00:03:44.150
it to another image also of a bride and
 

00:03:44.150 --> 00:03:48.199
it to another image also of a bride and
the model that we had trained up on this

00:03:48.199 --> 00:03:48.209
the model that we had trained up on this
 

00:03:48.209 --> 00:03:51.140
the model that we had trained up on this
open source image dataset returned the

00:03:51.140 --> 00:03:51.150
open source image dataset returned the
 

00:03:51.150 --> 00:03:55.119
open source image dataset returned the
following classifications clothing event

00:03:55.119 --> 00:03:55.129
following classifications clothing event
 

00:03:55.129 --> 00:04:01.280
following classifications clothing event
costume read and performance art no

00:04:01.280 --> 00:04:01.290
costume read and performance art no
 

00:04:01.290 --> 00:04:06.039
costume read and performance art no
mention of bride also no mention of

00:04:06.039 --> 00:04:06.049
mention of bride also no mention of
 

00:04:06.049 --> 00:04:11.119
mention of bride also no mention of
person miss regardless of gender so in a

00:04:11.119 --> 00:04:11.129
person miss regardless of gender so in a
 

00:04:11.129 --> 00:04:12.740
person miss regardless of gender so in a
sense this this model is sort of like

00:04:12.740 --> 00:04:12.750
sense this this model is sort of like
 

00:04:12.750 --> 00:04:14.659
sense this this model is sort of like
missed the fact that there's a human in

00:04:14.659 --> 00:04:14.669
missed the fact that there's a human in
 

00:04:14.669 --> 00:04:17.509
missed the fact that there's a human in
the picture which is maybe not awesome

00:04:17.509 --> 00:04:17.519
the picture which is maybe not awesome
 

00:04:17.519 --> 00:04:20.810
the picture which is maybe not awesome
and not really what I would think of as

00:04:20.810 --> 00:04:20.820
and not really what I would think of as
 

00:04:20.820 --> 00:04:23.060
and not really what I would think of as
great success if we're claiming that

00:04:23.060 --> 00:04:23.070
great success if we're claiming that
 

00:04:23.070 --> 00:04:26.709
great success if we're claiming that
image classification is solved

00:04:26.709 --> 00:04:26.719
image classification is solved
 

00:04:26.719 --> 00:04:30.809
image classification is solved
ok so what's going on here

00:04:30.809 --> 00:04:30.819
ok so what's going on here
 

00:04:30.819 --> 00:04:34.239
ok so what's going on here
I'm gonna argue a little bit that what's

00:04:34.239 --> 00:04:34.249
I'm gonna argue a little bit that what's
 

00:04:34.249 --> 00:04:36.730
I'm gonna argue a little bit that what's
going on is is based in to some degree

00:04:36.730 --> 00:04:36.740
going on is is based in to some degree
 

00:04:36.740 --> 00:04:42.159
going on is is based in to some degree
on the idea of stereotypes and if you're

00:04:42.159 --> 00:04:42.169
on the idea of stereotypes and if you're
 

00:04:42.169 --> 00:04:43.600
on the idea of stereotypes and if you're
if you have your laptop up I'd like you

00:04:43.600 --> 00:04:43.610
if you have your laptop up I'd like you
 

00:04:43.610 --> 00:04:44.950
if you have your laptop up I'd like you
to close your laptop for a second this

00:04:44.950 --> 00:04:44.960
to close your laptop for a second this
 

00:04:44.960 --> 00:04:46.689
to close your laptop for a second this
is the interactive portion where you can

00:04:46.689 --> 00:04:46.699
is the interactive portion where you can
 

00:04:46.699 --> 00:04:51.100
is the interactive portion where you can
interact by closing your laptop and I'd

00:04:51.100 --> 00:04:51.110
interact by closing your laptop and I'd
 

00:04:51.110 --> 00:04:53.079
interact by closing your laptop and I'd
like you to find somebody sitting next

00:04:53.079 --> 00:04:53.089
like you to find somebody sitting next
 

00:04:53.089 --> 00:04:55.359
like you to find somebody sitting next
to you and exercise your human

00:04:55.359 --> 00:04:55.369
to you and exercise your human
 

00:04:55.369 --> 00:04:58.119
to you and exercise your human
conversation skills for about one minute

00:04:58.119 --> 00:04:58.129
conversation skills for about one minute
 

00:04:58.129 --> 00:05:01.779
conversation skills for about one minute
to come up with a definition between the

00:05:01.779 --> 00:05:01.789
to come up with a definition between the
 

00:05:01.789 --> 00:05:04.420
to come up with a definition between the
two of you of what is a stereotype

00:05:04.420 --> 00:05:04.430
two of you of what is a stereotype
 

00:05:04.430 --> 00:05:06.339
two of you of what is a stereotype
keeping in mind that we're in sort of a

00:05:06.339 --> 00:05:06.349
keeping in mind that we're in sort of a
 

00:05:06.349 --> 00:05:08.679
keeping in mind that we're in sort of a
statistical setting okay so have a quick

00:05:08.679 --> 00:05:08.689
statistical setting okay so have a quick
 

00:05:08.689 --> 00:05:10.659
statistical setting okay so have a quick
one-minute conversation with the person

00:05:10.659 --> 00:05:10.669
one-minute conversation with the person
 

00:05:10.669 --> 00:05:12.219
one-minute conversation with the person
sitting next to you if there's no one

00:05:12.219 --> 00:05:12.229
sitting next to you if there's no one
 

00:05:12.229 --> 00:05:13.809
sitting next to you if there's no one
sitting next to you you may move ready

00:05:13.809 --> 00:05:13.819
sitting next to you you may move ready
 

00:05:13.819 --> 00:05:20.229
sitting next to you you may move ready
set go

00:05:20.229 --> 00:05:20.239
 
 

00:05:20.239 --> 00:05:24.979
 
three-two-one and thank you know for

00:05:24.979 --> 00:05:24.989
three-two-one and thank you know for
 

00:05:24.989 --> 00:05:26.509
three-two-one and thank you know for
having that interesting conversation

00:05:26.509 --> 00:05:26.519
having that interesting conversation
 

00:05:26.519 --> 00:05:27.979
having that interesting conversation
that easily could have lasted for much

00:05:27.979 --> 00:05:27.989
that easily could have lasted for much
 

00:05:27.989 --> 00:05:30.009
that easily could have lasted for much
more than one minute but such as life

00:05:30.009 --> 00:05:30.019
more than one minute but such as life
 

00:05:30.019 --> 00:05:33.139
more than one minute but such as life
let's hear from one or two folks let's

00:05:33.139 --> 00:05:33.149
let's hear from one or two folks let's
 

00:05:33.149 --> 00:05:34.759
let's hear from one or two folks let's
had something that they came up with it

00:05:34.759 --> 00:05:34.769
had something that they came up with it
 

00:05:34.769 --> 00:05:35.749
had something that they came up with it
was interesting

00:05:35.749 --> 00:05:35.759
was interesting
 

00:05:35.759 --> 00:05:38.569
was interesting
oh yeah go ahead your name is adept yeah

00:05:38.569 --> 00:05:38.579
oh yeah go ahead your name is adept yeah
 

00:05:38.579 --> 00:05:47.239
oh yeah go ahead your name is adept yeah
okay what did you okay so Dickie is

00:05:47.239 --> 00:05:47.249
okay what did you okay so Dickie is
 

00:05:47.249 --> 00:05:48.619
okay what did you okay so Dickie is
saying that a stereotype is a

00:05:48.619 --> 00:05:48.629
saying that a stereotype is a
 

00:05:48.629 --> 00:05:50.539
saying that a stereotype is a
generalization that you find from a

00:05:50.539 --> 00:05:50.549
generalization that you find from a
 

00:05:50.549 --> 00:05:52.100
generalization that you find from a
large group of people and you apply it

00:05:52.100 --> 00:05:52.110
large group of people and you apply it
 

00:05:52.110 --> 00:05:54.819
large group of people and you apply it
to more people okay interesting

00:05:54.819 --> 00:05:54.829
to more people okay interesting
 

00:05:54.829 --> 00:05:56.809
to more people okay interesting
certainly agree with large parts that

00:05:56.809 --> 00:05:56.819
certainly agree with large parts that
 

00:05:56.819 --> 00:06:12.619
certainly agree with large parts that
yeah okay so so I'm here the claim is

00:06:12.619 --> 00:06:12.629
yeah okay so so I'm here the claim is
 

00:06:12.629 --> 00:06:14.479
yeah okay so so I'm here the claim is
that it's a label that's based on

00:06:14.479 --> 00:06:14.489
that it's a label that's based on
 

00:06:14.489 --> 00:06:16.459
that it's a label that's based on
experience from within your training set

00:06:16.459 --> 00:06:16.469
experience from within your training set
 

00:06:16.469 --> 00:06:20.569
experience from within your training set
yeah super interesting and the

00:06:20.569 --> 00:06:20.579
yeah super interesting and the
 

00:06:20.579 --> 00:06:22.639
yeah super interesting and the
probability of label based on what's

00:06:22.639 --> 00:06:22.649
probability of label based on what's
 

00:06:22.649 --> 00:06:25.959
probability of label based on what's
your training cool maybe one more oh

00:06:25.959 --> 00:06:25.969
your training cool maybe one more oh
 

00:06:25.969 --> 00:06:36.769
your training cool maybe one more oh
yeah good okay so that there's claim

00:06:36.769 --> 00:06:36.779
yeah good okay so that there's claim
 

00:06:36.779 --> 00:06:38.089
yeah good okay so that there's claim
here that stereotype has something to do

00:06:38.089 --> 00:06:38.099
here that stereotype has something to do
 

00:06:38.099 --> 00:06:39.679
here that stereotype has something to do
with unrelated features that happen to

00:06:39.679 --> 00:06:39.689
with unrelated features that happen to
 

00:06:39.689 --> 00:06:41.179
with unrelated features that happen to
be correlated I think that's interesting

00:06:41.179 --> 00:06:41.189
be correlated I think that's interesting
 

00:06:41.189 --> 00:06:43.399
be correlated I think that's interesting
let me see if I can this was not a plant

00:06:43.399 --> 00:06:43.409
let me see if I can this was not a plant
 

00:06:43.409 --> 00:06:47.479
let me see if I can this was not a plant
sorry your name was Constantine custody

00:06:47.479 --> 00:06:47.489
sorry your name was Constantine custody
 

00:06:47.489 --> 00:06:51.619
sorry your name was Constantine custody
is not a plant but I do want to look at

00:06:51.619 --> 00:06:51.629
is not a plant but I do want to look at
 

00:06:51.629 --> 00:06:53.239
is not a plant but I do want to look at
this a little bit more in detail so

00:06:53.239 --> 00:06:53.249
this a little bit more in detail so
 

00:06:53.249 --> 00:06:56.089
this a little bit more in detail so
here's here's a data set that I'm going

00:06:56.089 --> 00:06:56.099
here's here's a data set that I'm going
 

00:06:56.099 --> 00:07:00.159
here's here's a data set that I'm going
to claim is is based on running data so

00:07:00.159 --> 00:07:00.169
to claim is is based on running data so
 

00:07:00.169 --> 00:07:03.079
to claim is is based on running data so
in the early mornings I pretend that I'm

00:07:03.079 --> 00:07:03.089
in the early mornings I pretend that I'm
 

00:07:03.089 --> 00:07:07.279
in the early mornings I pretend that I'm
an athlete and go for a run and this is

00:07:07.279 --> 00:07:07.289
an athlete and go for a run and this is
 

00:07:07.289 --> 00:07:10.069
an athlete and go for a run and this is
a data set that sort of based on risk

00:07:10.069 --> 00:07:10.079
a data set that sort of based on risk
 

00:07:10.079 --> 00:07:12.769
a data set that sort of based on risk
that someone might not finish a race

00:07:12.769 --> 00:07:12.779
that someone might not finish a race
 

00:07:12.779 --> 00:07:14.839
that someone might not finish a race
that they enter in so we've got high

00:07:14.839 --> 00:07:14.849
that they enter in so we've got high
 

00:07:14.849 --> 00:07:16.699
that they enter in so we've got high
risk people or you know they are in

00:07:16.699 --> 00:07:16.709
risk people or you know they are in
 

00:07:16.709 --> 00:07:19.629
risk people or you know they are in
yellow and lower risk people are in red

00:07:19.629 --> 00:07:19.639
yellow and lower risk people are in red
 

00:07:19.639 --> 00:07:21.829
yellow and lower risk people are in red
you look in this data it's got a couple

00:07:21.829 --> 00:07:21.839
you look in this data it's got a couple
 

00:07:21.839 --> 00:07:22.759
you look in this data it's got a couple
dimensions

00:07:22.759 --> 00:07:22.769
dimensions
 

00:07:22.769 --> 00:07:25.489
dimensions
I might fit a linear classifier it's not

00:07:25.489 --> 00:07:25.499
I might fit a linear classifier it's not
 

00:07:25.499 --> 00:07:28.850
I might fit a linear classifier it's not
quite perfect if I look a little more

00:07:28.850 --> 00:07:28.860
quite perfect if I look a little more
 

00:07:28.860 --> 00:07:30.769
quite perfect if I look a little more
closely if I've actually got some more

00:07:30.769 --> 00:07:30.779
closely if I've actually got some more
 

00:07:30.779 --> 00:07:31.890
closely if I've actually got some more
information here

00:07:31.890 --> 00:07:31.900
information here
 

00:07:31.900 --> 00:07:33.810
information here
don't just have X&amp;Y I also have this

00:07:33.810 --> 00:07:33.820
don't just have X&amp;Y I also have this
 

00:07:33.820 --> 00:07:36.720
don't just have X&amp;Y I also have this
sort of color of outline so I might have

00:07:36.720 --> 00:07:36.730
sort of color of outline so I might have
 

00:07:36.730 --> 00:07:40.830
sort of color of outline so I might have
a rule that if this data point has a

00:07:40.830 --> 00:07:40.840
a rule that if this data point has a
 

00:07:40.840 --> 00:07:42.990
a rule that if this data point has a
blue outline I'm gonna predict low-risk

00:07:42.990 --> 00:07:43.000
blue outline I'm gonna predict low-risk
 

00:07:43.000 --> 00:07:45.710
blue outline I'm gonna predict low-risk
otherwise I'm gonna predict high-risk

00:07:45.710 --> 00:07:45.720
otherwise I'm gonna predict high-risk
 

00:07:45.720 --> 00:07:48.450
otherwise I'm gonna predict high-risk
fair enough

00:07:48.450 --> 00:07:48.460
fair enough
 

00:07:48.460 --> 00:07:50.879
fair enough
now the big reveal you'll never guess

00:07:50.879 --> 00:07:50.889
now the big reveal you'll never guess
 

00:07:50.889 --> 00:07:56.280
now the big reveal you'll never guess
what the the outline feature is based on

00:07:56.280 --> 00:07:56.290
what the the outline feature is based on
 

00:07:56.290 --> 00:07:59.580
what the the outline feature is based on
shoe type the other x and y are based on

00:07:59.580 --> 00:07:59.590
shoe type the other x and y are based on
 

00:07:59.590 --> 00:08:01.140
shoe type the other x and y are based on
how long the race is and sort of what a

00:08:01.140 --> 00:08:01.150
how long the race is and sort of what a
 

00:08:01.150 --> 00:08:03.600
how long the race is and sort of what a
person's weekly training volume is but

00:08:03.600 --> 00:08:03.610
person's weekly training volume is but
 

00:08:03.610 --> 00:08:05.790
person's weekly training volume is but
whether you're foolish enough to buy

00:08:05.790 --> 00:08:05.800
whether you're foolish enough to buy
 

00:08:05.800 --> 00:08:07.110
whether you're foolish enough to buy
expensive running shoes because you

00:08:07.110 --> 00:08:07.120
expensive running shoes because you
 

00:08:07.120 --> 00:08:08.280
expensive running shoes because you
think they're going to make you faster

00:08:08.280 --> 00:08:08.290
think they're going to make you faster
 

00:08:08.290 --> 00:08:11.360
think they're going to make you faster
or whatever this is what's in the data

00:08:11.360 --> 00:08:11.370
or whatever this is what's in the data
 

00:08:11.370 --> 00:08:17.000
or whatever this is what's in the data
and in traditional machine learning

00:08:17.000 --> 00:08:17.010
and in traditional machine learning
 

00:08:17.010 --> 00:08:20.250
and in traditional machine learning
supervised machine learning we might say

00:08:20.250 --> 00:08:20.260
supervised machine learning we might say
 

00:08:20.260 --> 00:08:21.150
supervised machine learning we might say
well wait a minute

00:08:21.150 --> 00:08:21.160
well wait a minute
 

00:08:21.160 --> 00:08:23.760
well wait a minute
I'm not sure that shoe type is going to

00:08:23.760 --> 00:08:23.770
I'm not sure that shoe type is going to
 

00:08:23.770 --> 00:08:26.969
I'm not sure that shoe type is going to
be actually predictive on the other hand

00:08:26.969 --> 00:08:26.979
be actually predictive on the other hand
 

00:08:26.979 --> 00:08:29.190
be actually predictive on the other hand
it's in our training data and it does

00:08:29.190 --> 00:08:29.200
it's in our training data and it does
 

00:08:29.200 --> 00:08:31.050
it's in our training data and it does
seem to be awfully predictive on this

00:08:31.050 --> 00:08:31.060
seem to be awfully predictive on this
 

00:08:31.060 --> 00:08:32.610
seem to be awfully predictive on this
data set we have a really simple model

00:08:32.610 --> 00:08:32.620
data set we have a really simple model
 

00:08:32.620 --> 00:08:34.409
data set we have a really simple model
it's highly regularized it still gives

00:08:34.409 --> 00:08:34.419
it's highly regularized it still gives
 

00:08:34.419 --> 00:08:36.620
it's highly regularized it still gives
you no perfect or near perfect accuracy

00:08:36.620 --> 00:08:36.630
you no perfect or near perfect accuracy
 

00:08:36.630 --> 00:08:42.089
you no perfect or near perfect accuracy
maybe it's fine and the only way we can

00:08:42.089 --> 00:08:42.099
maybe it's fine and the only way we can
 

00:08:42.099 --> 00:08:45.240
maybe it's fine and the only way we can
find out if it's not I would argue is by

00:08:45.240 --> 00:08:45.250
find out if it's not I would argue is by
 

00:08:45.250 --> 00:08:48.269
find out if it's not I would argue is by
gathering some more data and I'll point

00:08:48.269 --> 00:08:48.279
gathering some more data and I'll point
 

00:08:48.279 --> 00:08:50.690
gathering some more data and I'll point
out that this data set has been

00:08:50.690 --> 00:08:50.700
out that this data set has been
 

00:08:50.700 --> 00:08:53.220
out that this data set has been
diabolically constructed so that there

00:08:53.220 --> 00:08:53.230
diabolically constructed so that there
 

00:08:53.230 --> 00:08:56.190
diabolically constructed so that there
are some points in the data space that

00:08:56.190 --> 00:08:56.200
are some points in the data space that
 

00:08:56.200 --> 00:08:57.710
are some points in the data space that
are not particularly well represented

00:08:57.710 --> 00:08:57.720
are not particularly well represented
 

00:08:57.720 --> 00:08:59.760
are not particularly well represented
and you can maybe tell yourself a story

00:08:59.760 --> 00:08:59.770
and you can maybe tell yourself a story
 

00:08:59.770 --> 00:09:01.500
and you can maybe tell yourself a story
about maybe this data was collected

00:09:01.500 --> 00:09:01.510
about maybe this data was collected
 

00:09:01.510 --> 00:09:05.820
about maybe this data was collected
after some corporate 5k or something

00:09:05.820 --> 00:09:05.830
after some corporate 5k or something
 

00:09:05.830 --> 00:09:08.010
after some corporate 5k or something
like that so if we can collect some more

00:09:08.010 --> 00:09:08.020
like that so if we can collect some more
 

00:09:08.020 --> 00:09:12.240
like that so if we can collect some more
data maybe we find that actually there's

00:09:12.240 --> 00:09:12.250
data maybe we find that actually there's
 

00:09:12.250 --> 00:09:14.370
data maybe we find that actually there's
people wearing all kinds of shoes on

00:09:14.370 --> 00:09:14.380
people wearing all kinds of shoes on
 

00:09:14.380 --> 00:09:17.430
people wearing all kinds of shoes on
both sides of our imaginary classifier

00:09:17.430 --> 00:09:17.440
both sides of our imaginary classifier
 

00:09:17.440 --> 00:09:21.240
both sides of our imaginary classifier
but that this shoe type feature is

00:09:21.240 --> 00:09:21.250
but that this shoe type feature is
 

00:09:21.250 --> 00:09:22.530
but that this shoe type feature is
really not predictive at all and this

00:09:22.530 --> 00:09:22.540
really not predictive at all and this
 

00:09:22.540 --> 00:09:25.170
really not predictive at all and this
gets back to Constantine's point that

00:09:25.170 --> 00:09:25.180
gets back to Constantine's point that
 

00:09:25.180 --> 00:09:28.019
gets back to Constantine's point that
perhaps relying on features that are

00:09:28.019 --> 00:09:28.029
perhaps relying on features that are
 

00:09:28.029 --> 00:09:30.840
perhaps relying on features that are
strongly correlated but not necessarily

00:09:30.840 --> 00:09:30.850
strongly correlated but not necessarily
 

00:09:30.850 --> 00:09:33.750
strongly correlated but not necessarily
causal may be a point at which we're

00:09:33.750 --> 00:09:33.760
causal may be a point at which we're
 

00:09:33.760 --> 00:09:36.380
causal may be a point at which we're
thinking about a stereotype in some way

00:09:36.380 --> 00:09:36.390
thinking about a stereotype in some way
 

00:09:36.390 --> 00:09:39.150
thinking about a stereotype in some way
so obviously given this data and what we

00:09:39.150 --> 00:09:39.160
so obviously given this data and what we
 

00:09:39.160 --> 00:09:41.550
so obviously given this data and what we
know now I would probably go back and

00:09:41.550 --> 00:09:41.560
know now I would probably go back and
 

00:09:41.560 --> 00:09:43.560
know now I would probably go back and
suggest a linear classifier based on

00:09:43.560 --> 00:09:43.570
suggest a linear classifier based on
 

00:09:43.570 --> 00:09:45.690
suggest a linear classifier based on
these these features of length of race

00:09:45.690 --> 00:09:45.700
these these features of length of race
 

00:09:45.700 --> 00:09:47.820
these these features of length of race
and weekly training volumes potentially

00:09:47.820 --> 00:09:47.830
and weekly training volumes potentially
 

00:09:47.830 --> 00:09:51.330
and weekly training volumes potentially
a better model so how does this happen

00:09:51.330 --> 00:09:51.340
a better model so how does this happen
 

00:09:51.340 --> 00:09:55.380
a better model so how does this happen
what's what's the issue here that's at

00:09:55.380 --> 00:09:55.390
what's what's the issue here that's at
 

00:09:55.390 --> 00:09:58.110
what's what's the issue here that's at
play one of the issues that's at play is

00:09:58.110 --> 00:09:58.120
play one of the issues that's at play is
 

00:09:58.120 --> 00:10:02.430
play one of the issues that's at play is
that in supervised machine learning we

00:10:02.430 --> 00:10:02.440
that in supervised machine learning we
 

00:10:02.440 --> 00:10:05.520
that in supervised machine learning we
often make the assumption that our

00:10:05.520 --> 00:10:05.530
often make the assumption that our
 

00:10:05.530 --> 00:10:07.380
often make the assumption that our
training distribution and our test

00:10:07.380 --> 00:10:07.390
training distribution and our test
 

00:10:07.390 --> 00:10:10.350
training distribution and our test
distribution are identical right and we

00:10:10.350 --> 00:10:10.360
distribution are identical right and we
 

00:10:10.360 --> 00:10:12.210
distribution are identical right and we
make this assumption for a really good

00:10:12.210 --> 00:10:12.220
make this assumption for a really good
 

00:10:12.220 --> 00:10:14.820
make this assumption for a really good
reason which is that if we make that

00:10:14.820 --> 00:10:14.830
reason which is that if we make that
 

00:10:14.830 --> 00:10:17.160
reason which is that if we make that
assumption then we can pretend that

00:10:17.160 --> 00:10:17.170
assumption then we can pretend that
 

00:10:17.170 --> 00:10:18.390
assumption then we can pretend that
there's no difference between

00:10:18.390 --> 00:10:18.400
there's no difference between
 

00:10:18.400 --> 00:10:20.310
there's no difference between
correlation and causation and we can use

00:10:20.310 --> 00:10:20.320
correlation and causation and we can use
 

00:10:20.320 --> 00:10:22.710
correlation and causation and we can use
all of our features whether they're what

00:10:22.710 --> 00:10:22.720
all of our features whether they're what
 

00:10:22.720 --> 00:10:23.910
all of our features whether they're what
Constantine would call you know

00:10:23.910 --> 00:10:23.920
Constantine would call you know
 

00:10:23.920 --> 00:10:26.970
Constantine would call you know
meaningful or causal or not we can throw

00:10:26.970 --> 00:10:26.980
meaningful or causal or not we can throw
 

00:10:26.980 --> 00:10:28.620
meaningful or causal or not we can throw
them in there and so long as their tests

00:10:28.620 --> 00:10:28.630
them in there and so long as their tests
 

00:10:28.630 --> 00:10:29.820
them in there and so long as their tests
and training distributions are the same

00:10:29.820 --> 00:10:29.830
and training distributions are the same
 

00:10:29.830 --> 00:10:32.670
and training distributions are the same
we're probably okay to within some some

00:10:32.670 --> 00:10:32.680
we're probably okay to within some some
 

00:10:32.680 --> 00:10:37.020
we're probably okay to within some some
degree but in the real world we don't

00:10:37.020 --> 00:10:37.030
degree but in the real world we don't
 

00:10:37.030 --> 00:10:39.510
degree but in the real world we don't
just apply models to a training or test

00:10:39.510 --> 00:10:39.520
just apply models to a training or test
 

00:10:39.520 --> 00:10:42.420
just apply models to a training or test
set we also use them to make predictions

00:10:42.420 --> 00:10:42.430
set we also use them to make predictions
 

00:10:42.430 --> 00:10:44.700
set we also use them to make predictions
that may influence the world in some way

00:10:44.700 --> 00:10:44.710
that may influence the world in some way
 

00:10:44.710 --> 00:10:47.750
that may influence the world in some way
and there I think that the right sort of

00:10:47.750 --> 00:10:47.760
and there I think that the right sort of
 

00:10:47.760 --> 00:10:51.180
and there I think that the right sort of
phrase to use isn't so much test set

00:10:51.180 --> 00:10:51.190
phrase to use isn't so much test set
 

00:10:51.190 --> 00:10:54.780
phrase to use isn't so much test set
it's more inference time performance

00:10:54.780 --> 00:10:54.790
it's more inference time performance
 

00:10:54.790 --> 00:10:57.090
it's more inference time performance
okay because that at inference time when

00:10:57.090 --> 00:10:57.100
okay because that at inference time when
 

00:10:57.100 --> 00:10:58.500
okay because that at inference time when
we're going and applying our model to

00:10:58.500 --> 00:10:58.510
we're going and applying our model to
 

00:10:58.510 --> 00:11:00.060
we're going and applying our model to
some new instance in the world we may

00:11:00.060 --> 00:11:00.070
some new instance in the world we may
 

00:11:00.070 --> 00:11:01.650
some new instance in the world we may
not actually know what they let the true

00:11:01.650 --> 00:11:01.660
not actually know what they let the true
 

00:11:01.660 --> 00:11:03.300
not actually know what they let the true
label is ever are things like that but

00:11:03.300 --> 00:11:03.310
label is ever are things like that but
 

00:11:03.310 --> 00:11:04.500
label is ever are things like that but
we still care very much about having

00:11:04.500 --> 00:11:04.510
we still care very much about having
 

00:11:04.510 --> 00:11:06.990
we still care very much about having
good performance and making sure that

00:11:06.990 --> 00:11:07.000
good performance and making sure that
 

00:11:07.000 --> 00:11:10.860
good performance and making sure that
our test that our training set matches

00:11:10.860 --> 00:11:10.870
our test that our training set matches
 

00:11:10.870 --> 00:11:13.830
our test that our training set matches
our inference distribution to some

00:11:13.830 --> 00:11:13.840
our inference distribution to some
 

00:11:13.840 --> 00:11:17.250
our inference distribution to some
degree is is like super critical so

00:11:17.250 --> 00:11:17.260
degree is is like super critical so
 

00:11:17.260 --> 00:11:18.480
degree is is like super critical so
let's go back to open images and what

00:11:18.480 --> 00:11:18.490
let's go back to open images and what
 

00:11:18.490 --> 00:11:19.920
let's go back to open images and what
was happening there

00:11:19.920 --> 00:11:19.930
was happening there
 

00:11:19.930 --> 00:11:23.040
was happening there
you'll recall that it did quite badly on

00:11:23.040 --> 00:11:23.050
you'll recall that it did quite badly on
 

00:11:23.050 --> 00:11:26.940
you'll recall that it did quite badly on
at least anecdotally on that image of a

00:11:26.940 --> 00:11:26.950
at least anecdotally on that image of a
 

00:11:26.950 --> 00:11:30.510
at least anecdotally on that image of a
bride who appeared to be from India if

00:11:30.510 --> 00:11:30.520
bride who appeared to be from India if
 

00:11:30.520 --> 00:11:32.070
bride who appeared to be from India if
we look at the geo diversity of open

00:11:32.070 --> 00:11:32.080
we look at the geo diversity of open
 

00:11:32.080 --> 00:11:33.750
we look at the geo diversity of open
images this is something where we we did

00:11:33.750 --> 00:11:33.760
images this is something where we we did
 

00:11:33.760 --> 00:11:35.130
images this is something where we we did
our best to sort of track down the

00:11:35.130 --> 00:11:35.140
our best to sort of track down the
 

00:11:35.140 --> 00:11:37.740
our best to sort of track down the
geolocation of each of the images in the

00:11:37.740 --> 00:11:37.750
geolocation of each of the images in the
 

00:11:37.750 --> 00:11:40.650
geolocation of each of the images in the
open image data set what we found was

00:11:40.650 --> 00:11:40.660
open image data set what we found was
 

00:11:40.660 --> 00:11:43.770
open image data set what we found was
that an overwhelming proportion of the

00:11:43.770 --> 00:11:43.780
that an overwhelming proportion of the
 

00:11:43.780 --> 00:11:47.040
that an overwhelming proportion of the
data in open images was from North

00:11:47.040 --> 00:11:47.050
data in open images was from North
 

00:11:47.050 --> 00:11:50.330
data in open images was from North
America and six countries in Europe

00:11:50.330 --> 00:11:50.340
America and six countries in Europe
 

00:11:50.340 --> 00:11:52.830
America and six countries in Europe
vanishingly small amounts of that data

00:11:52.830 --> 00:11:52.840
vanishingly small amounts of that data
 

00:11:52.840 --> 00:11:54.810
vanishingly small amounts of that data
were from countries such as India or

00:11:54.810 --> 00:11:54.820
were from countries such as India or
 

00:11:54.820 --> 00:11:57.000
were from countries such as India or
China or other places where I've heard

00:11:57.000 --> 00:11:57.010
China or other places where I've heard
 

00:11:57.010 --> 00:11:58.620
China or other places where I've heard
there's actually a large number of

00:11:58.620 --> 00:11:58.630
there's actually a large number of
 

00:11:58.630 --> 00:11:59.739
there's actually a large number of
people

00:11:59.739 --> 00:11:59.749
people
 

00:11:59.749 --> 00:12:04.609
people
so this is clearly not representative in

00:12:04.609 --> 00:12:04.619
so this is clearly not representative in
 

00:12:04.619 --> 00:12:07.369
so this is clearly not representative in
a meaningful way of sort of the global

00:12:07.369 --> 00:12:07.379
a meaningful way of sort of the global
 

00:12:07.379 --> 00:12:09.799
a meaningful way of sort of the global
diversity of the world how does this

00:12:09.799 --> 00:12:09.809
diversity of the world how does this
 

00:12:09.809 --> 00:12:12.379
diversity of the world how does this
happen it's not like the researchers who

00:12:12.379 --> 00:12:12.389
happen it's not like the researchers who
 

00:12:12.389 --> 00:12:13.999
happen it's not like the researchers who
put the open images data set were in any

00:12:13.999 --> 00:12:14.009
put the open images data set were in any
 

00:12:14.009 --> 00:12:15.559
put the open images data set were in any
way little intention they were working

00:12:15.559 --> 00:12:15.569
way little intention they were working
 

00:12:15.569 --> 00:12:17.840
way little intention they were working
really hard to put together what they

00:12:17.840 --> 00:12:17.850
really hard to put together what they
 

00:12:17.850 --> 00:12:19.249
really hard to put together what they
believe was a more representative data

00:12:19.249 --> 00:12:19.259
believe was a more representative data
 

00:12:19.259 --> 00:12:23.059
believe was a more representative data
set then of an image net at the very

00:12:23.059 --> 00:12:23.069
set then of an image net at the very
 

00:12:23.069 --> 00:12:23.840
set then of an image net at the very
least they don't have a hundred

00:12:23.840 --> 00:12:23.850
least they don't have a hundred
 

00:12:23.850 --> 00:12:28.159
least they don't have a hundred
categories of dogs in this one so what

00:12:28.159 --> 00:12:28.169
categories of dogs in this one so what
 

00:12:28.169 --> 00:12:29.809
categories of dogs in this one so what
happens well you could make an argument

00:12:29.809 --> 00:12:29.819
happens well you could make an argument
 

00:12:29.819 --> 00:12:31.549
happens well you could make an argument
that there's some strong correlation

00:12:31.549 --> 00:12:31.559
that there's some strong correlation
 

00:12:31.559 --> 00:12:33.889
that there's some strong correlation
with the distribution of open images

00:12:33.889 --> 00:12:33.899
with the distribution of open images
 

00:12:33.899 --> 00:12:37.009
with the distribution of open images
with the distribution of countries with

00:12:37.009 --> 00:12:37.019
with the distribution of countries with
 

00:12:37.019 --> 00:12:40.009
with the distribution of countries with
high loca high bandwidth low-cost

00:12:40.009 --> 00:12:40.019
high loca high bandwidth low-cost
 

00:12:40.019 --> 00:12:43.249
high loca high bandwidth low-cost
internet access it's not a perfect

00:12:43.249 --> 00:12:43.259
internet access it's not a perfect
 

00:12:43.259 --> 00:12:45.909
internet access it's not a perfect
correlation but it's it's pretty close

00:12:45.909 --> 00:12:45.919
correlation but it's it's pretty close
 

00:12:45.919 --> 00:12:50.569
correlation but it's it's pretty close
and that if we're doing if one might do

00:12:50.569 --> 00:12:50.579
and that if we're doing if one might do
 

00:12:50.579 --> 00:12:53.530
and that if we're doing if one might do
things like base an image classifier on

00:12:53.530 --> 00:12:53.540
things like base an image classifier on
 

00:12:53.540 --> 00:12:57.409
things like base an image classifier on
data drawn from a distribution of areas

00:12:57.409 --> 00:12:57.419
data drawn from a distribution of areas
 

00:12:57.419 --> 00:12:59.749
data drawn from a distribution of areas
that have high bandwidth low cost

00:12:59.749 --> 00:12:59.759
that have high bandwidth low cost
 

00:12:59.759 --> 00:13:02.749
that have high bandwidth low cost
internet access that may induce

00:13:02.749 --> 00:13:02.759
internet access that may induce
 

00:13:02.759 --> 00:13:04.970
internet access that may induce
differences between the training

00:13:04.970 --> 00:13:04.980
differences between the training
 

00:13:04.980 --> 00:13:06.889
differences between the training
distribution and the inference time

00:13:06.889 --> 00:13:06.899
distribution and the inference time
 

00:13:06.899 --> 00:13:10.039
distribution and the inference time
distribution none of this is like

00:13:10.039 --> 00:13:10.049
distribution none of this is like
 

00:13:10.049 --> 00:13:11.859
distribution none of this is like
something you wouldn't figure out

00:13:11.859 --> 00:13:11.869
something you wouldn't figure out
 

00:13:11.869 --> 00:13:15.259
something you wouldn't figure out
without you know if you sat down for

00:13:15.259 --> 00:13:15.269
without you know if you sat down for
 

00:13:15.269 --> 00:13:16.819
without you know if you sat down for
five minutes right this is all a super

00:13:16.819 --> 00:13:16.829
five minutes right this is all a super
 

00:13:16.829 --> 00:13:19.039
five minutes right this is all a super
basic statistics it is in fact stuff

00:13:19.039 --> 00:13:19.049
basic statistics it is in fact stuff
 

00:13:19.049 --> 00:13:20.720
basic statistics it is in fact stuff
that's this is just six people have been

00:13:20.720 --> 00:13:20.730
that's this is just six people have been
 

00:13:20.730 --> 00:13:22.699
that's this is just six people have been
sort of railing at the machine learning

00:13:22.699 --> 00:13:22.709
sort of railing at the machine learning
 

00:13:22.709 --> 00:13:23.900
sort of railing at the machine learning
community at for the last several

00:13:23.900 --> 00:13:23.910
community at for the last several
 

00:13:23.910 --> 00:13:27.949
community at for the last several
decades but as machine learning models

00:13:27.949 --> 00:13:27.959
decades but as machine learning models
 

00:13:27.959 --> 00:13:30.519
decades but as machine learning models
become sort of more ubiquitous in

00:13:30.519 --> 00:13:30.529
become sort of more ubiquitous in
 

00:13:30.529 --> 00:13:32.869
become sort of more ubiquitous in
everyday life it thinks that paying

00:13:32.869 --> 00:13:32.879
everyday life it thinks that paying
 

00:13:32.879 --> 00:13:34.009
everyday life it thinks that paying
attention to these kinds of issues

00:13:34.009 --> 00:13:34.019
attention to these kinds of issues
 

00:13:34.019 --> 00:13:37.429
attention to these kinds of issues
becomes ever more important so let's go

00:13:37.429 --> 00:13:37.439
becomes ever more important so let's go
 

00:13:37.439 --> 00:13:39.349
becomes ever more important so let's go
back to what a start a stereotype and I

00:13:39.349 --> 00:13:39.359
back to what a start a stereotype and I
 

00:13:39.359 --> 00:13:41.840
back to what a start a stereotype and I
think I agree with Constantine's idea

00:13:41.840 --> 00:13:41.850
think I agree with Constantine's idea
 

00:13:41.850 --> 00:13:43.849
think I agree with Constantine's idea
and I'm gonna add one more tweak to it

00:13:43.849 --> 00:13:43.859
and I'm gonna add one more tweak to it
 

00:13:43.859 --> 00:13:46.400
and I'm gonna add one more tweak to it
so I'm gonna say that a stereotype is a

00:13:46.400 --> 00:13:46.410
so I'm gonna say that a stereotype is a
 

00:13:46.410 --> 00:13:47.749
so I'm gonna say that a stereotype is a
statistical confounder

00:13:47.749 --> 00:13:47.759
statistical confounder
 

00:13:47.759 --> 00:13:49.009
statistical confounder
I think it's using Constantine's

00:13:49.009 --> 00:13:49.019
I think it's using Constantine's
 

00:13:49.019 --> 00:13:51.470
I think it's using Constantine's
language almost exactly that has a

00:13:51.470 --> 00:13:51.480
language almost exactly that has a
 

00:13:51.480 --> 00:13:57.799
language almost exactly that has a
societal basis so when I think about

00:13:57.799 --> 00:13:57.809
societal basis so when I think about
 

00:13:57.809 --> 00:14:01.629
societal basis so when I think about
issues of fairness if it's the case that

00:14:01.629 --> 00:14:01.639
issues of fairness if it's the case that
 

00:14:01.639 --> 00:14:04.220
issues of fairness if it's the case that
you know rainy weather is correlated

00:14:04.220 --> 00:14:04.230
you know rainy weather is correlated
 

00:14:04.230 --> 00:14:06.439
you know rainy weather is correlated
with people using umbrellas like yes

00:14:06.439 --> 00:14:06.449
with people using umbrellas like yes
 

00:14:06.449 --> 00:14:07.939
with people using umbrellas like yes
that's a confounder the umbrellas did

00:14:07.939 --> 00:14:07.949
that's a confounder the umbrellas did
 

00:14:07.949 --> 00:14:11.600
that's a confounder the umbrellas did
not cause the rain but I'm not as

00:14:11.600 --> 00:14:11.610
not cause the rain but I'm not as
 

00:14:11.610 --> 00:14:13.100
not cause the rain but I'm not as
worried

00:14:13.100 --> 00:14:13.110
worried
 

00:14:13.110 --> 00:14:15.560
worried
as a individual human about the societal

00:14:15.560 --> 00:14:15.570
as a individual human about the societal
 

00:14:15.570 --> 00:14:17.540
as a individual human about the societal
impact of models that are based on that

00:14:17.540 --> 00:14:17.550
impact of models that are based on that
 

00:14:17.550 --> 00:14:19.160
impact of models that are based on that
you know module I'm sure you could

00:14:19.160 --> 00:14:19.170
you know module I'm sure you could
 

00:14:19.170 --> 00:14:21.620
you know module I'm sure you could
imagine some crazy scary scenario where

00:14:21.620 --> 00:14:21.630
imagine some crazy scary scenario where
 

00:14:21.630 --> 00:14:24.440
imagine some crazy scary scenario where
that was the case but in general I don't

00:14:24.440 --> 00:14:24.450
that was the case but in general I don't
 

00:14:24.450 --> 00:14:25.910
that was the case but in general I don't
think that's as large an issue but when

00:14:25.910 --> 00:14:25.920
think that's as large an issue but when
 

00:14:25.920 --> 00:14:27.019
think that's as large an issue but when
we think of things like internet

00:14:27.019 --> 00:14:27.029
we think of things like internet
 

00:14:27.029 --> 00:14:29.630
we think of things like internet
connectivity or other societally based

00:14:29.630 --> 00:14:29.640
connectivity or other societally based
 

00:14:29.640 --> 00:14:32.060
connectivity or other societally based
factors I think that paying attention to

00:14:32.060 --> 00:14:32.070
factors I think that paying attention to
 

00:14:32.070 --> 00:14:34.490
factors I think that paying attention to
questions of do we have confounders in

00:14:34.490 --> 00:14:34.500
questions of do we have confounders in
 

00:14:34.500 --> 00:14:36.440
questions of do we have confounders in
our data are they being picked up by our

00:14:36.440 --> 00:14:36.450
our data are they being picked up by our
 

00:14:36.450 --> 00:14:42.170
our data are they being picked up by our
models is as incredibly important so if

00:14:42.170 --> 00:14:42.180
models is as incredibly important so if
 

00:14:42.180 --> 00:14:43.910
models is as incredibly important so if
you take away nothing else from this

00:14:43.910 --> 00:14:43.920
you take away nothing else from this
 

00:14:43.920 --> 00:14:46.840
you take away nothing else from this
short talk I hope that you take away a

00:14:46.840 --> 00:14:46.850
short talk I hope that you take away a
 

00:14:46.850 --> 00:14:49.790
short talk I hope that you take away a
caution to be aware of differences

00:14:49.790 --> 00:14:49.800
caution to be aware of differences
 

00:14:49.800 --> 00:14:50.780
caution to be aware of differences
between your training and inference

00:14:50.780 --> 00:14:50.790
between your training and inference
 

00:14:50.790 --> 00:14:54.500
between your training and inference
distributions ask the question because

00:14:54.500 --> 00:14:54.510
distributions ask the question because
 

00:14:54.510 --> 00:14:56.569
distributions ask the question because
statistically this is not a particularly

00:14:56.569 --> 00:14:56.579
statistically this is not a particularly
 

00:14:56.579 --> 00:14:58.819
statistically this is not a particularly
difficult thing to uncover if you take

00:14:58.819 --> 00:14:58.829
difficult thing to uncover if you take
 

00:14:58.829 --> 00:15:02.150
difficult thing to uncover if you take
the time to look in a world of keggle

00:15:02.150 --> 00:15:02.160
the time to look in a world of keggle
 

00:15:02.160 --> 00:15:04.819
the time to look in a world of keggle
competitions and people trying to get

00:15:04.819 --> 00:15:04.829
competitions and people trying to get
 

00:15:04.829 --> 00:15:06.949
competitions and people trying to get
high marks on deep learning classes and

00:15:06.949 --> 00:15:06.959
high marks on deep learning classes and
 

00:15:06.959 --> 00:15:08.630
high marks on deep learning classes and
things like that I think it's all too

00:15:08.630 --> 00:15:08.640
things like that I think it's all too
 

00:15:08.640 --> 00:15:10.550
things like that I think it's all too
easy for us to just take datasets as

00:15:10.550 --> 00:15:10.560
easy for us to just take datasets as
 

00:15:10.560 --> 00:15:13.790
easy for us to just take datasets as
given not think about them too much and

00:15:13.790 --> 00:15:13.800
given not think about them too much and
 

00:15:13.800 --> 00:15:15.860
given not think about them too much and
just try and get our accuracy from 99.1

00:15:15.860 --> 00:15:15.870
just try and get our accuracy from 99.1
 

00:15:15.870 --> 00:15:20.509
just try and get our accuracy from 99.1
to 99 points you and as someone who is

00:15:20.509 --> 00:15:20.519
to 99 points you and as someone who is
 

00:15:20.519 --> 00:15:21.920
to 99 points you and as someone who is
interested in people coming out of

00:15:21.920 --> 00:15:21.930
interested in people coming out of
 

00:15:21.930 --> 00:15:23.780
interested in people coming out of
programs like this being ready to do

00:15:23.780 --> 00:15:23.790
programs like this being ready to do
 

00:15:23.790 --> 00:15:26.449
programs like this being ready to do
work in the real world I would caution

00:15:26.449 --> 00:15:26.459
work in the real world I would caution
 

00:15:26.459 --> 00:15:30.680
work in the real world I would caution
that we can't only be training ourselves

00:15:30.680 --> 00:15:30.690
that we can't only be training ourselves
 

00:15:30.690 --> 00:15:35.300
that we can't only be training ourselves
to do that so with that I'm gonna leave

00:15:35.300 --> 00:15:35.310
to do that so with that I'm gonna leave
 

00:15:35.310 --> 00:15:37.100
to do that so with that I'm gonna leave
you with a set of additional resources

00:15:37.100 --> 00:15:37.110
you with a set of additional resources
 

00:15:37.110 --> 00:15:38.840
you with a set of additional resources
around machine learning fairness

00:15:38.840 --> 00:15:38.850
around machine learning fairness
 

00:15:38.850 --> 00:15:40.939
around machine learning fairness
these are super hot off the presses in

00:15:40.939 --> 00:15:40.949
these are super hot off the presses in
 

00:15:40.949 --> 00:15:42.680
these are super hot off the presses in
the sense that this particular little

00:15:42.680 --> 00:15:42.690
the sense that this particular little
 

00:15:42.690 --> 00:15:45.079
the sense that this particular little
website was launched and I think 8:30

00:15:45.079 --> 00:15:45.089
website was launched and I think 8:30
 

00:15:45.089 --> 00:15:46.790
website was launched and I think 8:30
this morning something like that so

00:15:46.790 --> 00:15:46.800
this morning something like that so
 

00:15:46.800 --> 00:15:50.240
this morning something like that so
you've you've got it first MIT leading

00:15:50.240 --> 00:15:50.250
you've you've got it first MIT leading
 

00:15:50.250 --> 00:15:55.490
you've you've got it first MIT leading
the way in on this page there are n yeah

00:15:55.490 --> 00:15:55.500
the way in on this page there are n yeah
 

00:15:55.500 --> 00:15:58.120
the way in on this page there are n yeah
you can open your laptop's again now

00:15:58.120 --> 00:15:58.130
you can open your laptop's again now
 

00:15:58.130 --> 00:16:01.430
you can open your laptop's again now
there are a number of papers that go

00:16:01.430 --> 00:16:01.440
there are a number of papers that go
 

00:16:01.440 --> 00:16:03.139
there are a number of papers that go
through this sort like a greatest hits

00:16:03.139 --> 00:16:03.149
through this sort like a greatest hits
 

00:16:03.149 --> 00:16:04.400
through this sort like a greatest hits
of the machine learning fairness

00:16:04.400 --> 00:16:04.410
of the machine learning fairness
 

00:16:04.410 --> 00:16:06.009
of the machine learning fairness
literature from the last couple years

00:16:06.009 --> 00:16:06.019
literature from the last couple years
 

00:16:06.019 --> 00:16:08.480
literature from the last couple years
really interesting papers I don't think

00:16:08.480 --> 00:16:08.490
really interesting papers I don't think
 

00:16:08.490 --> 00:16:10.100
really interesting papers I don't think
any of them are like the one final

00:16:10.100 --> 00:16:10.110
any of them are like the one final
 

00:16:10.110 --> 00:16:11.930
any of them are like the one final
solution to machine learning fairness

00:16:11.930 --> 00:16:11.940
solution to machine learning fairness
 

00:16:11.940 --> 00:16:14.090
solution to machine learning fairness
issues but they're super interesting

00:16:14.090 --> 00:16:14.100
issues but they're super interesting
 

00:16:14.100 --> 00:16:16.939
issues but they're super interesting
reads and I think help sort of paint the

00:16:16.939 --> 00:16:16.949
reads and I think help sort of paint the
 

00:16:16.949 --> 00:16:18.110
reads and I think help sort of paint the
the space in the landscape really

00:16:18.110 --> 00:16:18.120
the space in the landscape really
 

00:16:18.120 --> 00:16:20.240
the space in the landscape really
usefully they're also a couple of

00:16:20.240 --> 00:16:20.250
usefully they're also a couple of
 

00:16:20.250 --> 00:16:23.630
usefully they're also a couple of
interesting exercises there that you can

00:16:23.630 --> 00:16:23.640
interesting exercises there that you can
 

00:16:23.640 --> 00:16:26.900
interesting exercises there that you can
access by a collab and

00:16:26.900 --> 00:16:26.910
access by a collab and
 

00:16:26.910 --> 00:16:28.699
access by a collab and
if you're interested in this space

00:16:28.699 --> 00:16:28.709
if you're interested in this space
 

00:16:28.709 --> 00:16:30.379
if you're interested in this space
they're things that you can play with

00:16:30.379 --> 00:16:30.389
they're things that you can play with
 

00:16:30.389 --> 00:16:32.989
they're things that you can play with
I think they include one on adversarial

00:16:32.989 --> 00:16:32.999
I think they include one on adversarial
 

00:16:32.999 --> 00:16:35.509
I think they include one on adversarial
D biasing where because you guys all

00:16:35.509 --> 00:16:35.519
D biasing where because you guys all
 

00:16:35.519 --> 00:16:38.569
D biasing where because you guys all
love deep learning you can use a network

00:16:38.569 --> 00:16:38.579
love deep learning you can use a network
 

00:16:38.579 --> 00:16:43.309
love deep learning you can use a network
to try and become unbiased by making

00:16:43.309 --> 00:16:43.319
to try and become unbiased by making
 

00:16:43.319 --> 00:16:45.169
to try and become unbiased by making
sure that by having an extra output head

00:16:45.169 --> 00:16:45.179
sure that by having an extra output head
 

00:16:45.179 --> 00:16:48.229
sure that by having an extra output head
that predicts a characteristic that you

00:16:48.229 --> 00:16:48.239
that predicts a characteristic that you
 

00:16:48.239 --> 00:16:50.359
that predicts a characteristic that you
wish to be unbiased on and then

00:16:50.359 --> 00:16:50.369
wish to be unbiased on and then
 

00:16:50.369 --> 00:16:52.369
wish to be unbiased on and then
penalizing that model if it's good at

00:16:52.369 --> 00:16:52.379
penalizing that model if it's good at
 

00:16:52.379 --> 00:16:55.400
penalizing that model if it's good at
predicting that that characteristic and

00:16:55.400 --> 00:16:55.410
predicting that that characteristic and
 

00:16:55.410 --> 00:16:57.349
predicting that that characteristic and
so this is trying to adversary only make

00:16:57.349 --> 00:16:57.359
so this is trying to adversary only make
 

00:16:57.359 --> 00:16:59.119
so this is trying to adversary only make
sure that our internal representation in

00:16:59.119 --> 00:16:59.129
sure that our internal representation in
 

00:16:59.129 --> 00:17:01.819
sure that our internal representation in
a deep network is not picking up

00:17:01.819 --> 00:17:01.829
a deep network is not picking up
 

00:17:01.829 --> 00:17:03.469
a deep network is not picking up
unwanted correlations around water

00:17:03.469 --> 00:17:03.479
unwanted correlations around water
 

00:17:03.479 --> 00:17:06.619
unwanted correlations around water
biases so I hope that that's interesting

00:17:06.619 --> 00:17:06.629
biases so I hope that that's interesting
 

00:17:06.629 --> 00:17:09.860
biases so I hope that that's interesting
and I'll be around afterwards to take

00:17:09.860 --> 00:17:09.870
and I'll be around afterwards to take
 

00:17:09.870 --> 00:17:11.869
and I'll be around afterwards to take
questions but at this point I'd like to

00:17:11.869 --> 00:17:11.879
questions but at this point I'd like to
 

00:17:11.879 --> 00:17:13.819
questions but at this point I'd like to
make sure that sunchang has plenty of

00:17:13.819 --> 00:17:13.829
make sure that sunchang has plenty of
 

00:17:13.829 --> 00:17:15.720
make sure that sunchang has plenty of
time so thank you very much

00:17:15.720 --> 00:17:15.730
time so thank you very much
 

00:17:15.730 --> 00:17:19.759
time so thank you very much
[Applause]

