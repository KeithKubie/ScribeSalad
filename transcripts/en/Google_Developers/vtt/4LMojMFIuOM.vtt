WEBVTT
Kind: captions
Language: en

00:00:02.126 --> 00:00:03.660
SPEAKER 1: She's
Innovation Director

00:00:03.660 --> 00:00:05.790
at Microsoft
Research in Cambridge

00:00:05.790 --> 00:00:08.910
and also appears on
some BBC documentaries.

00:00:08.910 --> 00:00:12.160
[INAUDIBLE] And yeah, welcome.

00:00:20.709 --> 00:00:21.500
HAYAN ZHANG: Hello.

00:00:21.500 --> 00:00:22.740
Hi, everybody.

00:00:22.740 --> 00:00:26.300
So we're so close to lunch,
so make this snappy and quick.

00:00:26.300 --> 00:00:28.670
Hello, I'm Hayan, I'm
Director of Innovation

00:00:28.670 --> 00:00:31.700
at Microsoft Research
in Cambridge in the UK.

00:00:31.700 --> 00:00:35.930
The original Cambridge,
if there's any doubts.

00:00:35.930 --> 00:00:37.370
And I call myself--

00:00:37.370 --> 00:00:39.830
well, I'm a designer
and an engineer,

00:00:39.830 --> 00:00:42.140
so I started out
in computer science

00:00:42.140 --> 00:00:46.060
and I moved to being a designer.

00:00:46.060 --> 00:00:48.740
So I worked at a
consultancy firm called

00:00:48.740 --> 00:00:50.000
IDO for about seven years.

00:00:50.000 --> 00:00:52.530
And in the last five
years, I've been

00:00:52.530 --> 00:00:57.320
in Microsoft running innovation
teams, and Xbox, and research.

00:00:57.320 --> 00:01:00.470
And I essentially call
myself a maker of things

00:01:00.470 --> 00:01:04.099
because I think it's a
really nice fusion of design

00:01:04.099 --> 00:01:06.240
and engineering practice,
to treat technology

00:01:06.240 --> 00:01:10.770
as a design material and to
work with that from day one.

00:01:10.770 --> 00:01:14.360
So I think that a quote
that really encapsulates

00:01:14.360 --> 00:01:19.050
my philosophy about design
is from Eileen Gray, who's

00:01:19.050 --> 00:01:22.410
a self-taught architect
and furniture designer.

00:01:22.410 --> 00:01:22.910
She said.

00:01:22.910 --> 00:01:25.990
"To create, one must first
question everything."

00:01:25.990 --> 00:01:28.220
You know, this is a
really broad statement.

00:01:28.220 --> 00:01:31.370
But to me it means
whether you're

00:01:31.370 --> 00:01:37.130
designing an interface for an
iPhone app, or an Android app,

00:01:37.130 --> 00:01:41.240
or a product, you're essentially
baking into that design

00:01:41.240 --> 00:01:45.050
how you think people should go
about their day, should live,

00:01:45.050 --> 00:01:47.540
should have a lamp
in their house,

00:01:47.540 --> 00:01:49.220
the quality of that lamp.

00:01:49.220 --> 00:01:52.380
I think, to me, with every
single act of design,

00:01:52.380 --> 00:01:56.210
I think that it also
has the potential

00:01:56.210 --> 00:01:58.320
to be an action to
change the world.

00:01:58.320 --> 00:02:02.340
So I'm a big fan of knife
fights in the design process.

00:02:02.340 --> 00:02:04.700
So meet me in the
dark alley, and we'll

00:02:04.700 --> 00:02:10.280
debate over whose narrative
of the future will win.

00:02:10.280 --> 00:02:14.639
Because I think within
design, within service design,

00:02:14.639 --> 00:02:17.180
interaction design, we've been
talking about these narratives

00:02:17.180 --> 00:02:20.360
that we all share-- things
like the sharing economy,

00:02:20.360 --> 00:02:25.700
things like sustainability,
and things like open-source.

00:02:25.700 --> 00:02:28.340
I think within our culture in
Europe and in North America,

00:02:28.340 --> 00:02:32.570
these are sort of narratives
of a science fiction

00:02:32.570 --> 00:02:37.950
or a designed future that we
would like to see come to pass.

00:02:37.950 --> 00:02:41.090
But just harking back a
little bit on my childhood,

00:02:41.090 --> 00:02:42.620
that was not always the case.

00:02:42.620 --> 00:02:47.320
I was born in China in the
aftermath of the Cultural

00:02:47.320 --> 00:02:48.260
Revolution.

00:02:48.260 --> 00:02:49.750
And in my early years--

00:02:49.750 --> 00:02:52.680
this is not a picture of me.

00:02:52.680 --> 00:02:56.930
I did not look like a
boy when I was little.

00:02:56.930 --> 00:03:00.920
In my early years, I went
to primary school in China.

00:03:00.920 --> 00:03:02.580
At the time, in
the early '80s, it

00:03:02.580 --> 00:03:06.650
was very over indexed on the
Communist Party and Party

00:03:06.650 --> 00:03:08.190
philosophy.

00:03:08.190 --> 00:03:11.390
So I was in a system that
was very much focused

00:03:11.390 --> 00:03:15.980
on stripping individuals
of their personal agency,

00:03:15.980 --> 00:03:20.690
of opinions that
differed from the Party.

00:03:20.690 --> 00:03:24.530
It was kind of like an
institutionalized fake news

00:03:24.530 --> 00:03:28.010
system where every
piece of information

00:03:28.010 --> 00:03:30.230
was filtered through
this kind of philosophy.

00:03:30.230 --> 00:03:31.700
Whether or not it
was true or not,

00:03:31.700 --> 00:03:36.840
it had to be sort
of bent to its will.

00:03:36.840 --> 00:03:40.820
And then transition that to
when I moved to Australia.

00:03:40.820 --> 00:03:43.250
I moved to Australia the
year that "Star Trek--

00:03:43.250 --> 00:03:45.260
The Next Generation"
aired on TV.

00:03:49.190 --> 00:03:52.670
It was a revelation to me to
go from this kind of system

00:03:52.670 --> 00:03:59.690
where individualism was frowned
upon to this world where you

00:03:59.690 --> 00:04:02.930
might be empowered to
pursue your own interests

00:04:02.930 --> 00:04:08.120
and to achieve what you might
be great at in the world.

00:04:08.120 --> 00:04:11.840
I was a big fan of "Star
Trek" and particularly

00:04:11.840 --> 00:04:14.930
enabled, empowered by
technology, where technology

00:04:14.930 --> 00:04:19.790
would serve to enable
your personal agency

00:04:19.790 --> 00:04:22.160
to allow you the
freedom to explore

00:04:22.160 --> 00:04:24.260
what you were interested in.

00:04:24.260 --> 00:04:31.250
So I sort of went into
technology as a practice

00:04:31.250 --> 00:04:34.490
and always thinking about
sort of using technology

00:04:34.490 --> 00:04:38.390
as this kind of design
material to change the world.

00:04:38.390 --> 00:04:41.330
In recent years I've been
involved in a BBC documentary

00:04:41.330 --> 00:04:43.970
series called "Big Life Fix,"
which is bringing together

00:04:43.970 --> 00:04:49.430
seven engineers and
designers from around the UK

00:04:49.430 --> 00:04:52.010
to invent cutting edge
technology for people in need,

00:04:52.010 --> 00:04:55.730
which is really funny because
the audience on BBC Two,

00:04:55.730 --> 00:05:00.130
as we're told, are kind of
like grannies in the Midlands.

00:05:00.130 --> 00:05:01.950
So it's a real juxtaposition.

00:05:01.950 --> 00:05:06.860
So one of the projects
that came out of the show

00:05:06.860 --> 00:05:12.970
was around sort of supporting
a young lady called Emma.

00:05:12.970 --> 00:05:14.990
And Emma, when she
was 28 years old,

00:05:14.990 --> 00:05:17.510
she was diagnosed with
Parkinson's disease, which

00:05:17.510 --> 00:05:19.290
is actually a very rare event.

00:05:19.290 --> 00:05:23.060
And as a result of
her disease, she

00:05:23.060 --> 00:05:27.380
had acquired an action
tremor in her hand.

00:05:27.380 --> 00:05:29.540
Being a graphic designer
and a creative director,

00:05:29.540 --> 00:05:37.550
she was no longer able to pursue
her job because her hand shook

00:05:37.550 --> 00:05:41.270
when she was trying to draw
wire frames or write her name.

00:05:41.270 --> 00:05:43.770
And so I worked with her
to think about, well,

00:05:43.770 --> 00:05:48.410
how can we create something
that might be able to help

00:05:48.410 --> 00:05:49.790
her overcome this symptom?

00:05:49.790 --> 00:05:53.540
How can we fix this
problem through a design

00:05:53.540 --> 00:05:54.561
and an engineering lens?

00:05:54.561 --> 00:05:56.269
So I'll just quickly
show you that video.

00:05:59.263 --> 00:06:00.760
[VIDEO PLAYBACK]

00:06:00.760 --> 00:06:03.754
[MUSIC PLAYING]

00:08:36.990 --> 00:08:39.744
[LAUGHTER]

00:09:19.925 --> 00:09:21.889
[END PLAYBACK]

00:09:21.889 --> 00:09:23.362
[APPLAUSE]

00:09:27.302 --> 00:09:29.010
HAYAN ZHANG: Thanks
very much, thank you.

00:09:31.722 --> 00:09:33.180
I mean, one of the
takeaways that I

00:09:33.180 --> 00:09:39.300
have from doing this work, and
there are many, one of them

00:09:39.300 --> 00:09:42.030
is that I think you
can't quite tell

00:09:42.030 --> 00:09:44.880
if this is a design project
or an engineering project.

00:09:44.880 --> 00:09:47.830
Because design alone
couldn't solve this problem.

00:09:47.830 --> 00:09:50.855
But at the same time, I think--

00:09:50.855 --> 00:09:52.230
I mean, the clip
didn't show it--

00:09:52.230 --> 00:09:54.897
but my undertaking
was a design process.

00:09:54.897 --> 00:09:57.230
And it was the design process
that led me to the insight

00:09:57.230 --> 00:10:01.630
to start prototyping with
this kind of intervention.

00:10:01.630 --> 00:10:04.020
And so an engineering
process on its own

00:10:04.020 --> 00:10:05.580
could not achieve this either.

00:10:10.290 --> 00:10:13.410
The other takeaway, which
I will talk about in a bit,

00:10:13.410 --> 00:10:20.520
is just that there is so
much potential for technology

00:10:20.520 --> 00:10:24.670
in all its forms and in
its physical form factors

00:10:24.670 --> 00:10:28.400
to still have relevance
and make a huge impact,

00:10:28.400 --> 00:10:30.345
that we haven't explored yet.

00:10:30.345 --> 00:10:32.595
I know today that we've been
talking about algorithms.

00:10:32.595 --> 00:10:34.740
We've been talking
about smart systems.

00:10:34.740 --> 00:10:38.850
But my focus and passion, for
I guess the last 10 years,

00:10:38.850 --> 00:10:41.140
has been about
hardware and devices.

00:10:41.140 --> 00:10:43.590
And I think it's
through opening up

00:10:43.590 --> 00:10:48.600
new form factors, new inputs,
new outputs, that we're

00:10:48.600 --> 00:10:53.340
able to create a new
stage for where algorithms

00:10:53.340 --> 00:10:56.520
can come into play
beyond mobile phones

00:10:56.520 --> 00:11:00.290
and the desktop metaphor.

00:11:00.290 --> 00:11:04.350
So we are with our
Microsoft Research team.

00:11:04.350 --> 00:11:10.230
We're continuing on some of the
research into this technology.

00:11:10.230 --> 00:11:13.080
And some of those challenges
that we are now encountering

00:11:13.080 --> 00:11:16.290
are we really do
think that's smarter

00:11:16.290 --> 00:11:21.416
algorithms can make the
effect more amplified.

00:11:21.416 --> 00:11:23.040
But there's this
transitional-- there's

00:11:23.040 --> 00:11:25.781
this question between signal
processing and machine

00:11:25.781 --> 00:11:26.280
learning.

00:11:26.280 --> 00:11:29.360
So where does a
signal processing end,

00:11:29.360 --> 00:11:33.600
and where is it better to pick
up machine learning as a tool

00:11:33.600 --> 00:11:34.300
to do this?

00:11:34.300 --> 00:11:36.720
And we're not quite
sure without going

00:11:36.720 --> 00:11:39.030
through the hard work of
implementing each piece

00:11:39.030 --> 00:11:40.530
of technology to test it out.

00:11:43.110 --> 00:11:45.630
I think through some
of the discussions

00:11:45.630 --> 00:11:47.610
that we have around
machine learning,

00:11:47.610 --> 00:11:49.650
there's this kind of idea
that machine learning

00:11:49.650 --> 00:11:51.610
is the solution for everything.

00:11:51.610 --> 00:11:54.810
I think we need to really
question that and understand

00:11:54.810 --> 00:11:57.235
where the boundaries of
machine learning end,

00:11:57.235 --> 00:12:00.990
and where we can best
leverage other kinds

00:12:00.990 --> 00:12:05.750
of rule-based algorithms still.

00:12:05.750 --> 00:12:11.800
So we're continuing to
develop some of the study apps

00:12:11.800 --> 00:12:17.720
and tech to capture this
data for more people.

00:12:17.720 --> 00:12:20.950
I think one of the really
interesting things that Emma

00:12:20.950 --> 00:12:23.950
says herself, is
that she is dealing

00:12:23.950 --> 00:12:27.640
with so much in her life and
so much from this disease.

00:12:27.640 --> 00:12:30.940
And wearing this watch has
been able to give her back

00:12:30.940 --> 00:12:34.420
a little bit of that
dignity that I think

00:12:34.420 --> 00:12:36.820
the disease has taken away.

00:12:36.820 --> 00:12:40.360
I mean, it's not a cure, and
it's not fixed everything.

00:12:40.360 --> 00:12:42.670
But if the technology
can give her back

00:12:42.670 --> 00:12:45.670
a little bit of her
agency and her dignity,

00:12:45.670 --> 00:12:49.990
I think in that itself is a
really worthy value for us

00:12:49.990 --> 00:12:50.932
to pursue.

00:12:50.932 --> 00:12:52.390
And at the same
time, when we think

00:12:52.390 --> 00:12:56.280
about algorithms in the
loop, or AI in the loop,

00:12:56.280 --> 00:12:59.320
you know, by wearing
the watch, Emma

00:12:59.320 --> 00:13:03.310
is able to regain some of
the clarity in her writing.

00:13:03.310 --> 00:13:05.020
But it's not perfect.

00:13:05.020 --> 00:13:07.330
This is not an
augmentation to allow

00:13:07.330 --> 00:13:09.910
her to write like
a robot, to give

00:13:09.910 --> 00:13:11.380
her perfectly straight lines.

00:13:11.380 --> 00:13:16.120
There is still a little bit
of quiver in her writing,

00:13:16.120 --> 00:13:17.440
and she enjoys that.

00:13:17.440 --> 00:13:21.160
Because the disease
is also a fuse

00:13:21.160 --> 00:13:23.230
with parts of her personality.

00:13:23.230 --> 00:13:26.620
So she enjoys the imperfection.

00:13:26.620 --> 00:13:29.890
And the technology through
not working perfectly

00:13:29.890 --> 00:13:33.550
is actually giving her back
some of that personality

00:13:33.550 --> 00:13:35.480
that she misses.

00:13:35.480 --> 00:13:37.790
So let me transition
to another project

00:13:37.790 --> 00:13:41.270
that I did recently, last year--

00:13:41.270 --> 00:13:44.670
working with a young girl who,
when she was eight years old,

00:13:44.670 --> 00:13:47.160
she had a car accident
with her family.

00:13:47.160 --> 00:13:49.960
And she suffered some
traumatic brain injury.

00:13:49.960 --> 00:13:51.640
And she is now 12.

00:13:51.640 --> 00:13:54.200
And at the time when
she had her accident,

00:13:54.200 --> 00:13:57.320
it was quite devastating.

00:13:57.320 --> 00:13:58.380
She couldn't walk.

00:13:58.380 --> 00:13:59.180
She couldn't talk.

00:13:59.180 --> 00:14:01.190
But over time, she was
able to rehabilitate,

00:14:01.190 --> 00:14:03.346
and now she's up and
about and running around

00:14:03.346 --> 00:14:04.970
on the playground,
and going to school.

00:14:04.970 --> 00:14:11.570
But she's also sustained
sort of more long term issues

00:14:11.570 --> 00:14:13.940
with her memory.

00:14:13.940 --> 00:14:17.330
And so I went in to see if
we could create technologies

00:14:17.330 --> 00:14:19.640
to help her regain
some of that memory,

00:14:19.640 --> 00:14:21.500
to help capture
some of that memory.

00:14:21.500 --> 00:14:24.590
So through the design
process, working with her,

00:14:24.590 --> 00:14:26.420
I was able to kind
of start to map

00:14:26.420 --> 00:14:28.070
out what the challenges were.

00:14:28.070 --> 00:14:29.540
Memory is kind of like--

00:14:29.540 --> 00:14:31.970
trying to define what memory
is is kind of like sand

00:14:31.970 --> 00:14:33.470
running through your fingers.

00:14:33.470 --> 00:14:37.035
Everybody said she had
short-term memory loss.

00:14:37.035 --> 00:14:39.410
But actually, she didn't have
that much short-term memory

00:14:39.410 --> 00:14:39.920
loss.

00:14:39.920 --> 00:14:43.740
What she did have
was this inability

00:14:43.740 --> 00:14:47.700
to remember events or episodic
memory in the long term.

00:14:47.700 --> 00:14:49.770
So if she'd go on a
picnic with her family,

00:14:49.770 --> 00:14:52.490
she could remember
it for 30 minutes,

00:14:52.490 --> 00:14:56.330
but she wouldn't be able to
remember it the next weekend.

00:14:56.330 --> 00:14:58.670
If she took a class, she'd
be able to remember things

00:14:58.670 --> 00:15:02.300
in the class, but she would
forget it the next time

00:15:02.300 --> 00:15:04.500
the class came around.

00:15:04.500 --> 00:15:07.650
So I really set about sort
of trying to think, well,

00:15:07.650 --> 00:15:12.110
how can we create technology
that augments her memory,

00:15:12.110 --> 00:15:15.330
augments her brain
in some ways--

00:15:15.330 --> 00:15:18.090
not through everything, not
through everything that she's

00:15:18.090 --> 00:15:19.980
doing, but at specific moments?

00:15:19.980 --> 00:15:22.170
And these might be different
pieces of technology.

00:15:22.170 --> 00:15:25.980
So thinking about how can
we build her knowledge

00:15:25.980 --> 00:15:27.180
to complete school?

00:15:27.180 --> 00:15:30.900
How can we help her relive
her emotional moments

00:15:30.900 --> 00:15:32.010
with her family?

00:15:32.010 --> 00:15:35.160
And I would say that the
results are interesting,

00:15:35.160 --> 00:15:37.350
and they do work for her,
but at the same time,

00:15:37.350 --> 00:15:39.000
the physical form
factors I still

00:15:39.000 --> 00:15:42.930
find clunky because
I had to use tablets

00:15:42.930 --> 00:15:47.880
and off-the-shelf equipment
in order to kind of augment

00:15:47.880 --> 00:15:50.730
some of her daily routines.

00:15:50.730 --> 00:16:00.360
[VIDEO PLAYBACK]

00:16:00.360 --> 00:16:02.240
[MUSIC PLAYING]

00:17:25.079 --> 00:17:26.528
[LAUGHTER]

00:17:45.380 --> 00:17:47.754
[CHEERING]

00:17:50.586 --> 00:17:52.910
[END PLAYBACK]

00:17:52.910 --> 00:17:55.850
HAYAN ZHANG: I think part of
that solution, as you saw,

00:17:55.850 --> 00:18:00.650
was some cognitive services
where we turned the teacher's

00:18:00.650 --> 00:18:03.020
speech into text, and
she's able to rewind

00:18:03.020 --> 00:18:06.170
that in the class to see what
happened five minutes ago.

00:18:06.170 --> 00:18:09.290
And I think today and also
at other AI conferences,

00:18:09.290 --> 00:18:13.800
we talk glowingly about AI and
what it's been able to achieve.

00:18:13.800 --> 00:18:15.615
But I think when the
rubber hits the road

00:18:15.615 --> 00:18:18.240
and we're trying to really make
solutions that work for people,

00:18:18.240 --> 00:18:21.440
there's still a long way to go.

00:18:21.440 --> 00:18:23.310
This technology,
I mean, I think it

00:18:23.310 --> 00:18:25.730
was a great proof of concept,
but at the same time,

00:18:25.730 --> 00:18:27.540
how do we capture the
voice of the teacher?

00:18:27.540 --> 00:18:30.030
I had to put a lapel mic on
it because we couldn't isolate

00:18:30.030 --> 00:18:31.290
her voice in the classroom.

00:18:31.290 --> 00:18:35.110
She had a Birmingham accent.

00:18:35.110 --> 00:18:39.170
I had to train specific
models just on her accent.

00:18:39.170 --> 00:18:42.710
And then the kids in
the class had names

00:18:42.710 --> 00:18:45.560
that weren't Anglo-Saxon
names, and that

00:18:45.560 --> 00:18:47.190
was difficult to translate.

00:18:47.190 --> 00:18:49.610
So I think there's
still a long way to go.

00:18:49.610 --> 00:18:51.767
And also in terms
of technology form,

00:18:51.767 --> 00:18:53.975
the fact that she has to
have this tablet next to her

00:18:53.975 --> 00:18:55.950
in class, is not ideal.

00:18:55.950 --> 00:18:58.220
And finally I'm just
going to quickly share

00:18:58.220 --> 00:18:59.775
some of the work
that my team, and I

00:18:59.775 --> 00:19:02.150
have been doing at Microsoft
Research for the last couple

00:19:02.150 --> 00:19:05.030
of years, where we've kind
of really looked at, well,

00:19:05.030 --> 00:19:11.420
how can we open up the space of
physical digital exploration?

00:19:11.420 --> 00:19:14.990
How can we kind of
create new theaters,

00:19:14.990 --> 00:19:19.100
new spaces within which people
can interact with computers

00:19:19.100 --> 00:19:21.540
and kind of create these
new kinds of experiences?

00:19:21.540 --> 00:19:26.340
So we've won a Best
Paper Award for CHI 2018,

00:19:26.340 --> 00:19:28.970
so please come see us
if you're in Montreal.

00:19:28.970 --> 00:19:32.270
And this project really
started because we were really

00:19:32.270 --> 00:19:35.330
obsessed with expressiveness
for tangible user interfaces,

00:19:35.330 --> 00:19:40.190
and frustrated with
the history of TUIs,

00:19:40.190 --> 00:19:43.681
and how they were
fixed in place.

00:19:43.681 --> 00:19:44.930
They required a lot of set up.

00:19:44.930 --> 00:19:47.124
There was occlusion issues.

00:19:47.124 --> 00:19:49.040
And we really wanted to
set out to create kind

00:19:49.040 --> 00:19:53.030
of a more fluid TUI system.

00:19:53.030 --> 00:19:57.560
And I may be old fashioned,
but I freaking love TUIs, man.

00:19:57.560 --> 00:20:01.444
All right, so and then this is
the project that we created.

00:20:01.444 --> 00:20:02.110
[VIDEO PLAYBACK]

00:20:02.110 --> 00:20:06.086
[MUSIC PLAYING]

00:20:49.910 --> 00:20:52.910
HAYAN ZHANG: So that's
the technical preview.

00:20:52.910 --> 00:20:55.790
And we've worked with
the Microsoft HML team

00:20:55.790 --> 00:20:59.090
to do on board, some
machine learning

00:20:59.090 --> 00:21:01.220
in terms of detecting the
different gestures, hover

00:21:01.220 --> 00:21:02.090
gestures.

00:21:02.090 --> 00:21:04.670
And at the same time, we're
thinking more upstream

00:21:04.670 --> 00:21:07.220
about what are some of
the applications that

00:21:07.220 --> 00:21:09.427
can open up the expressiveness?

00:21:09.427 --> 00:21:10.093
[VIDEO PLAYBACK]

00:21:10.093 --> 00:21:10.920
ROBOT: Don't shoot.

00:21:12.785 --> 00:21:13.285
Bam!

00:21:14.467 --> 00:21:16.750
I'm hit.

00:21:16.750 --> 00:21:18.500
HAYAN ZHANG: Open up
new learning and play

00:21:18.500 --> 00:21:22.100
opportunities.

00:21:22.100 --> 00:21:26.450
And here we see much
more space for new kinds

00:21:26.450 --> 00:21:29.355
of algorithms that can enrich
these play and learning

00:21:29.355 --> 00:21:29.855
experiences.

00:21:29.855 --> 00:21:32.278
[MUSIC PLAYING]

00:22:06.571 --> 00:22:10.547
ROBOT: C-- A--

00:22:10.547 --> 00:22:13.529
R. Car.

00:22:13.529 --> 00:22:17.008
[MUSIC PLAYING]

00:22:28.387 --> 00:22:28.970
[END PLAYBACK]

00:22:28.970 --> 00:22:30.440
HAYAN ZHANG: And
so we've actually

00:22:30.440 --> 00:22:33.740
engineered the entire
platforms from scratch,

00:22:33.740 --> 00:22:36.650
because within the team working
in this multidisciplinary team,

00:22:36.650 --> 00:22:40.130
we really don't see a divide
between the design vision

00:22:40.130 --> 00:22:44.240
and the engineering
possibility of this technology.

00:22:44.240 --> 00:22:47.000
So I work with a
multidisciplinary team

00:22:47.000 --> 00:22:49.130
of hardware engineers
and designers,

00:22:49.130 --> 00:22:52.540
product designers,
computer scientists,

00:22:52.540 --> 00:22:54.620
and if you are in
Montreal, come see us.

00:22:54.620 --> 00:22:56.620
We will be giving
hands-on demos as well.

00:22:56.620 --> 00:22:57.614
Thank you.

00:22:57.614 --> 00:22:59.550
[APPLAUSE]

00:23:02.830 --> 00:23:04.830
SPEAKER 1: So we're running
a little bit behind,

00:23:04.830 --> 00:23:06.500
but if there are a
couple of questions?

00:23:10.940 --> 00:23:12.620
Otherwise, if
everybody's hungry?

00:23:12.620 --> 00:23:16.266
[LAUGHS]

00:23:16.266 --> 00:23:17.640
AUDIENCE: Thank
you for the talk.

00:23:17.640 --> 00:23:19.030
It was very interesting.

00:23:19.030 --> 00:23:23.620
What interested me about
the problem solving

00:23:23.620 --> 00:23:26.980
for Aman was there was
this step of-- like,

00:23:26.980 --> 00:23:29.440
you had the little diagram
of how memory is divided

00:23:29.440 --> 00:23:31.360
and then jumped to the solution.

00:23:31.360 --> 00:23:35.140
And I was wondering if you can
maybe tell us a little bit more

00:23:35.140 --> 00:23:37.600
how you came up with
the actual design idea

00:23:37.600 --> 00:23:41.290
that we'll relay
speech into text

00:23:41.290 --> 00:23:44.890
and then highlight it in colors
and also the one for the family

00:23:44.890 --> 00:23:48.454
memories, because that was the
very interesting part about it.

00:23:48.454 --> 00:23:50.620
HAYAN ZHANG: I think I'll
talk about family memories

00:23:50.620 --> 00:23:53.850
because that was very
interesting to me as well.

00:23:53.850 --> 00:24:00.500
So Aman, she can't remember
memories with her family.

00:24:00.500 --> 00:24:03.790
And there is some research
around memory loss

00:24:03.790 --> 00:24:06.387
and using photos to
jog someone's memory,

00:24:06.387 --> 00:24:07.720
and in her case, it didn't work.

00:24:07.720 --> 00:24:10.870
She could look at photos
and she wouldn't remember.

00:24:10.870 --> 00:24:13.480
But we did a design exercise
where we went out for the day,

00:24:13.480 --> 00:24:15.240
we had her take the photos.

00:24:15.240 --> 00:24:20.800
And she was punting on
a boat in Cambridge,

00:24:20.800 --> 00:24:24.700
and she thought the punter
guy was really attractive.

00:24:24.700 --> 00:24:27.610
He looked somebody from
One Direction or something.

00:24:27.610 --> 00:24:31.060
And so she was giggling
the entire time.

00:24:31.060 --> 00:24:32.320
She was so delighted.

00:24:32.320 --> 00:24:34.780
And then afterwards
we went to a cafe,

00:24:34.780 --> 00:24:35.950
and she was sitting there.

00:24:35.950 --> 00:24:36.970
She was look at the photos.

00:24:36.970 --> 00:24:39.400
And when it came to the picture
she'd taken of the punter,

00:24:39.400 --> 00:24:40.820
she didn't remember him at all.

00:24:40.820 --> 00:24:42.550
She was like, oh, I
don't know this guy.

00:24:42.550 --> 00:24:46.360
So she'd lost that kind of
delight from her memories.

00:24:46.360 --> 00:24:49.240
But then while she was doing
that, her mom, sitting next

00:24:49.240 --> 00:24:51.640
to her, just said,
oh Aman, do you

00:24:51.640 --> 00:24:53.020
remember this is
the guy he looks

00:24:53.020 --> 00:24:54.950
like Niall from One Direction.

00:24:54.950 --> 00:24:57.400
And then this sort of
smile came over her face,

00:24:57.400 --> 00:25:04.090
and suddenly she remembered
this event through her mom

00:25:04.090 --> 00:25:06.890
retelling that story
of how she felt.

00:25:06.890 --> 00:25:11.650
And so just, for me, through
seeing that interaction,

00:25:11.650 --> 00:25:13.720
allowed me to kind
of create this app.

00:25:13.720 --> 00:25:15.220
I mean, it's a very
simple app where

00:25:15.220 --> 00:25:17.470
her family can upload
photos and record

00:25:17.470 --> 00:25:19.570
personal memories for her.

00:25:19.570 --> 00:25:22.540
And just through this
combination of personal stories

00:25:22.540 --> 00:25:25.465
and images, she's able to recall
some of these memories, which

00:25:25.465 --> 00:25:26.215
is really amazing.

00:25:31.350 --> 00:25:33.840
AUDIENCE: And how long
does that recall last?

00:25:33.840 --> 00:25:35.910
Like does she have to
kind of almost remember,

00:25:35.910 --> 00:25:38.280
like create that memory
every time she looks at it,

00:25:38.280 --> 00:25:41.180
or is there some kind of
retention of Information?

00:25:41.180 --> 00:25:42.680
HAYAN ZHANG: I think
for me this was

00:25:42.680 --> 00:25:47.580
a kind of short kind of
project to create something.

00:25:47.580 --> 00:25:50.490
So there hasn't been a
more longer term study.

00:25:50.490 --> 00:25:53.040
I think every person
is very individualized,

00:25:53.040 --> 00:25:55.140
this kind of trauma as well.

00:25:59.280 --> 00:26:02.370
It's challenging, because
I talked to neuroscientists

00:26:02.370 --> 00:26:04.920
who do research into
memory, and they

00:26:04.920 --> 00:26:08.280
talked about how
people like Aman

00:26:08.280 --> 00:26:10.504
may never be able to
experience a memory the way

00:26:10.504 --> 00:26:11.670
that we experience a memory.

00:26:11.670 --> 00:26:13.503
So they may be able to
remember the details,

00:26:13.503 --> 00:26:16.590
but they may not be able
remember the emotional quality

00:26:16.590 --> 00:26:19.350
of the details.

00:26:19.350 --> 00:26:21.960
I feel that through
watching her,

00:26:21.960 --> 00:26:25.110
she is reliving some of
that emotional component.

00:26:25.110 --> 00:26:29.970
And perhaps through
regular retrieval,

00:26:29.970 --> 00:26:35.379
she's able to maybe build those
connections in the long term.

00:26:35.379 --> 00:26:36.420
AUDIENCE: It's very cool.

00:26:36.420 --> 00:26:37.320
Thank you very much.

00:26:37.320 --> 00:26:38.881
HAYAN ZHANG: Thanks very much.

00:26:38.881 --> 00:26:40.630
SPEAKER 1: If there
are no more questions,

00:26:40.630 --> 00:26:42.040
then we can break for lunch.

00:26:42.040 --> 00:26:48.520
Lunch is over there, and we will
be here at a quarter to 3:00--

00:26:48.520 --> 00:26:52.790
2:45 sharp-- to go
on with the program.

