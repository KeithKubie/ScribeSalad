WEBVTT
Kind: captions
Language: en

00:00:03.110 --> 00:00:06.720
Hi, I’m Adriene Hill, and welcome back to
Crash Course Statistics. We’ve covered a

00:00:06.720 --> 00:00:11.660
lot of statistical models, from the matched
pairs t-test to linear regression. And for

00:00:11.660 --> 00:00:15.710
the most part, we’ve used them to model
data that we already have so we can make inferences

00:00:15.710 --> 00:00:16.260
about it.

00:00:16.260 --> 00:00:20.420
But sometimes we want to predict future data.
A model that predicts whether someone will

00:00:20.430 --> 00:00:24.980
default on their loan could be very helpful
to a bank employee. They’re probably not

00:00:24.980 --> 00:00:30.960
writing scientific papers about why people
default on loans, but they do care about accurately

00:00:30.960 --> 00:00:32.180
predicting who will.

00:00:32.180 --> 00:00:37.640
Many types of Machine Learning (ML) do just
that: build models to predict future outcomes.

00:00:37.640 --> 00:00:42.440
And this field has exploded over the past
few decades. Supervised Machine Learning takes

00:00:42.440 --> 00:00:47.290
data that already has a correct answer, like
images that have been labeled as “cat”

00:00:47.290 --> 00:00:52.420
or “not a cat”, or the current salary
of a company’s CEO, and tries to learn how

00:00:52.420 --> 00:00:56.940
to predict it. It’s supervised because we
can tell the model what it got wrong.

00:00:56.940 --> 00:01:01.160
It’s called Machine Learning because instead
of following strict rules and instructions

00:01:01.160 --> 00:01:06.680
from humans, the computers (or machines) learn
how to do things from data.

00:01:06.680 --> 00:01:11.631
Today, we’ll briefly cover a few types of
supervised Machine Learning models, logistic

00:01:11.640 --> 00:01:16.200
regression, Linear Discriminant Analysis,
and K Nearest Neighbors.

00:01:16.200 --> 00:01:25.180
Intro

00:01:25.180 --> 00:01:29.440
Say you own a microloan company. Your goal
is to give short term, low interest loans

00:01:29.440 --> 00:01:34.220
to people around the world, so they can invest
in their small businesses. You have everyone

00:01:34.220 --> 00:01:39.799
fill out an application that asks them to
specify things like their age, sex, annual

00:01:39.799 --> 00:01:42.340
income, and the number of years they’ve
been in business.

00:01:42.340 --> 00:01:46.659
The microloan is not a donation, the recipient
is supposed to pay it back. So you need to

00:01:46.659 --> 00:01:48.840
figure out who is most likely to do that.

00:01:48.840 --> 00:01:52.740
During the early days of your company, you
reviewed each application by hand and made

00:01:52.740 --> 00:01:57.080
that decision based on personal experience
of who was likely to pay back the loan.

00:01:57.080 --> 00:02:01.430
But now you have more money and applicants
than you could possibly handle. You need a

00:02:01.430 --> 00:02:05.100
model--or algorithm--to help you make these
decisions efficiently.

00:02:05.100 --> 00:02:10.220
Logistic regression is a simple twist on linear
regression. It gets its name from the fact

00:02:10.220 --> 00:02:15.480
that it is a regression that predicts what’s
called the log odds of an event occuring.

00:02:15.480 --> 00:02:20.020
While log odds can be difficult, once we have
them, we can use some quick calculations to

00:02:20.020 --> 00:02:24.590
turn them into probabilities, which are a
lot easier to work with. We can use these

00:02:24.590 --> 00:02:28.500
probabilities to predict whether an individual
will default on their loan.

00:02:28.500 --> 00:02:33.540
Usually the cutoff is 50%. If someone is less
than 50% likely to default on their loan,

00:02:33.540 --> 00:02:37.530
we’ll predict that they’ll pay it off.
Otherwise, we’ll predict that they won’t

00:02:37.530 --> 00:02:38.540
pay off their loan.

00:02:38.540 --> 00:02:42.260
We need to be able to test whether our model
will be good at predicting data it’s never

00:02:42.260 --> 00:02:47.050
seen before. Data it doesn't have the correct
answer for. So we need to pretend that some

00:02:47.050 --> 00:02:50.840
of our data is “future” data for which
we don’t know the outcome.

00:02:50.840 --> 00:02:54.010
One simple way to do that is to split your
data into two parts.

00:02:54.010 --> 00:02:58.040
The first portion of our data, called the
training set, will be the data that we use

00:02:58.040 --> 00:03:03.819
to create--or train--our model. The other
portion, called the testing set, is the data

00:03:03.819 --> 00:03:07.900
we’re pretending is from the future. We
don’t use it to train our model.

00:03:07.900 --> 00:03:12.020
Instead, to test how well our model works,
we withhold the outcomes of the test set so

00:03:12.020 --> 00:03:16.490
that the model doesn’t know whether someone
paid off their loan or not, and ask it to

00:03:16.490 --> 00:03:17.490
make a prediction.

00:03:17.490 --> 00:03:22.129
Then, we can compare these with the real outcomes
that we ignored before.

00:03:22.129 --> 00:03:27.190
We can do this using a what’s called a Confusion
Matrix. A Confusion Matrix is a chart that

00:03:27.190 --> 00:03:32.080
tells us what actually happened--whether a
person paid back a loan--and what the model

00:03:32.080 --> 00:03:33.659
predicted would happen.

00:03:33.659 --> 00:03:37.890
The diagonals of this matrix are times when
the model got it right. Cases where the model

00:03:37.890 --> 00:03:43.160
correctly predicted that the person will default
on the loan is called a True Positive. “True”

00:03:43.160 --> 00:03:47.380
because it got it right. “Positive” because
the person defaulted on their loan.

00:03:47.380 --> 00:03:51.300
Cases where the model correctly predicted
that a person will pay back the loan are called

00:03:51.300 --> 00:03:55.769
True Negatives. Again “true” because it
made the correct prediction, and “negative”

00:03:55.769 --> 00:03:57.560
because the person did not default.

00:03:57.560 --> 00:04:02.180
Cases where the model was wrong are called
False Negatives--if the model thought that

00:04:02.190 --> 00:04:07.000
they would not default--and False Positives--if
the model thought they would default.

00:04:07.019 --> 00:04:11.980
Using current data and pretending it was future
data allows us to see how this model performed

00:04:11.980 --> 00:04:14.240
with data it had never seen before.

00:04:14.240 --> 00:04:19.390
One simple way to measure how well the model
did is to calculate its accuracy. Accuracy

00:04:19.390 --> 00:04:25.069
is the total number of correct classifications--Our
True Positives and True Negatives--divided

00:04:25.069 --> 00:04:30.040
by the total number of cases. It’s the percent
of cases our model got correct.

00:04:30.040 --> 00:04:34.920
Accuracy is important. But it’s also pretty
simplistic. It doesn’t take into account

00:04:34.920 --> 00:04:39.360
the fact that in different situations, we
might care more about some mistakes than others.

00:04:39.360 --> 00:04:43.719
We won’t touch on other methods of measuring
a model’s accuracy here, but it’s important

00:04:43.719 --> 00:04:49.599
to recognize that in many situations, we want
information above and beyond just an accuracy

00:04:49.600 --> 00:04:50.340
percentage.

00:04:50.340 --> 00:04:55.140
Logistic regression isn’t the only way predict
the future. Another common model is Linear

00:04:55.150 --> 00:05:01.040
Discriminant Analysis or LDA for short. LDA
uses Bayes’ Theorem in order to help us

00:05:01.050 --> 00:05:02.749
make predictions about data.

00:05:02.749 --> 00:05:06.889
Let’s say we wanna predict whether someone
would get into our local state college based

00:05:06.889 --> 00:05:10.379
on their high school GPA.
The red dots represent people who did not

00:05:10.379 --> 00:05:12.119
get in, green are people who did.

00:05:12.119 --> 00:05:16.389
If we make a couple of assumptions, we can
estimate the GPA distributions of people who

00:05:16.389 --> 00:05:19.219
did, and did not get their acceptance letter.

00:05:19.219 --> 00:05:22.939
If we find a new student who wants to know
if they will get in to your local state school,

00:05:22.939 --> 00:05:27.889
we use Bayes Rule and these distributions
to calculate the probability of getting in

00:05:27.889 --> 00:05:28.889
or not.

00:05:28.889 --> 00:05:35.560
LDA just asks, “Which category is more likely?”
If we draw a vertical line at their GPA, whichever

00:05:35.560 --> 00:05:40.370
distribution has a higher value at that line
is the group we’d guess.

00:05:40.370 --> 00:05:45.770
Since this student, Analisa has a 3.2 GPA,
we’d predict that she DOES get in. Since

00:05:45.770 --> 00:05:48.740
it’s more likely under the “got in”
distribution.

00:05:48.740 --> 00:05:53.219
But we all know that GPA isn’t everything.
What if we looked at SAT Scores as well.

00:05:53.219 --> 00:05:57.129
Looking at the distributions of both GPA and
SAT scores together can get a little more

00:05:57.129 --> 00:06:00.339
complicated. And this is where LDA becomes
really helpful.

00:06:00.339 --> 00:06:04.660
We want to create a score, we’ll call it
Score X, that’s a linear combination of

00:06:04.660 --> 00:06:09.369
GPA and SAT scores. Something like this:
We, or rather the computer, want to make it

00:06:09.369 --> 00:06:14.860
so that the Score X value of the admitted
students is as different as possible from

00:06:14.860 --> 00:06:18.460
the Score X value of the people who weren’t
admitted.

00:06:18.460 --> 00:06:22.710
This special way of combining variables to
make a score that maximally separates the

00:06:22.710 --> 00:06:25.770
two groups is what makes LDA really special.

00:06:25.770 --> 00:06:31.029
So, Score X is a pretty good indicator of
whether or not a student got in. AND that’s

00:06:31.029 --> 00:06:35.199
just one number that we have to keep track
of, instead of two: GPA and SAT score.

00:06:35.199 --> 00:06:38.979
For this sample, my computer told me that
this is the correct formula:

00:06:38.979 --> 00:06:43.960
Which means we can take the scatter plot of
both GPA and SAT score and change it into

00:06:43.960 --> 00:06:47.089
a one-dimensional graph of just Score X.

00:06:47.089 --> 00:06:51.990
Then we can plot the distributions and use
Bayes Rule to predict whether a new student,

00:06:51.990 --> 00:06:54.250
Brad, is going to get into this school.

00:06:54.250 --> 00:07:00.340
Brad’s Score X is 8, so we predict that
he won’t get in, since with a score X of

00:07:00.340 --> 00:07:04.319
8, it’s more likely that you won’t get
in than that you will.

00:07:04.319 --> 00:07:08.599
Creating a score like Score X can simplify
things a lot. Here, we looked at two variables,

00:07:08.599 --> 00:07:12.909
which we could have easily graphed. But, that’s
not the case if we have 100 variables for

00:07:12.909 --> 00:07:17.259
each student. Trust me, you don’t want your
college admissions counselor making admissions

00:07:17.259 --> 00:07:19.149
decisions based on a graph like that.

00:07:19.149 --> 00:07:23.950
Using fewer numbers also means that on average,
the computer can do faster calculations. So

00:07:23.950 --> 00:07:29.349
if 5 million potential students ask you to
predict whether they get in, using LDA to

00:07:29.349 --> 00:07:31.830
simplify will speed things up.

00:07:31.830 --> 00:07:36.409
Reducing the number of variables we have to
deal with is called Dimensionality Reduction,

00:07:36.409 --> 00:07:40.119
and it’s really important in the world of
“Big Data”. It makes working with millions

00:07:40.119 --> 00:07:42.819
of data points, each with thousands of variables,
possible.

00:07:42.819 --> 00:07:46.929
That’s often the kind of data that companies
like Google and Amazon have.

00:07:46.929 --> 00:07:50.729
The last machine learning model we’ll talk
about is K-Nearest Neighbors.

00:07:50.729 --> 00:07:56.749
K-Nearest Neighbors...or KNN for short...relies
on the idea that data points will be similar

00:07:56.749 --> 00:07:58.740
to other data points that are near it.

00:07:58.740 --> 00:08:02.770
For example, let’s plot the height and weight
of a group of Golden Retrievers, and a group

00:08:02.770 --> 00:08:03.770
of Huskies:

00:08:03.770 --> 00:08:07.919
If someone tells us a height and weight for
a dog--named Chase--whose breed we don’t

00:08:07.919 --> 00:08:09.599
know...we could plot it on our graph.

00:08:09.599 --> 00:08:15.050
The four points closest to Chase are Golden
Retrievers, so we would guess he’s a Golden

00:08:15.050 --> 00:08:15.640
Retriever.

00:08:15.640 --> 00:08:19.980
That’s the basic idea behind K-Nearest Neighbors!
Whichever category--in this case dog breed--has

00:08:20.000 --> 00:08:24.569
the more data points near our new data point
is the category we pick.

00:08:24.569 --> 00:08:28.720
In practice it is a tiny bit more complicated
than that. One thing we need to do is decide

00:08:28.720 --> 00:08:31.339
how many “neighboring” data points to
look at.

00:08:31.339 --> 00:08:36.779
The K in KNN is a variable representing the
number of neighbors we’ll look at for each

00:08:36.779 --> 00:08:38.719
point--or dog--we want to classify.

00:08:38.719 --> 00:08:42.620
When we wanted to know whether Chase was a
Husky or a Golden Retriever, we looked at

00:08:42.620 --> 00:08:48.550
the 4 closest data points. So K equals 4.
But we can set K to be any number.

00:08:48.550 --> 00:08:54.930
We could look at the 1 nearest neighbor. Or
15 nearest neighbors. As K changes, our classifications

00:08:54.930 --> 00:09:00.730
can change. These graphs show how points in
each area of the graph would be classified.

00:09:00.730 --> 00:09:05.540
There are many ways to choose which k to use.
One way is to split your data into two groups,

00:09:05.540 --> 00:09:10.079
a training set and a test set.
I’m going to take 20% of the data, and ignore

00:09:10.079 --> 00:09:11.079
it for now.

00:09:11.079 --> 00:09:16.560
Then I’m going to take the other 80% of
the data and use it to train a KNN classifier.

00:09:16.560 --> 00:09:21.350
A classifier basically just predicts which
group something will be in. It classifies

00:09:21.350 --> 00:09:24.529
it. We’ll build it using k equals 5.

00:09:24.529 --> 00:09:29.190
And we get this result: Where blue means Golden
Retriever. And red means Husky.

00:09:29.190 --> 00:09:33.279
As you can see, the boundaries between classes
don’t have to be one straight line. That’s

00:09:33.279 --> 00:09:36.170
one benefit of KNN. It can fit all kinds of
data.

00:09:36.170 --> 00:09:40.089
Now that we have trained our classifier using
80% of the data, we can test it using the

00:09:40.089 --> 00:09:46.190
other 20%. We’ll ask it to predict the classes
of each of the data points in this 20% test

00:09:46.190 --> 00:09:54.411
set. And again, we can calculate an accuracy
score. This model has 66.25% accuracy. But

00:09:54.411 --> 00:09:58.870
we can also try out other K’s and pick the
one that has the best accuracy.

00:09:58.870 --> 00:10:03.290
It looks like using a k of 50 hits the sweet
spot for us. Since the model with k equals

00:10:03.290 --> 00:10:08.740
50 has the highest accuracy of predicting
Husky vs. Golden Retriever. So, if we want

00:10:08.740 --> 00:10:13.680
to build a KNN classifier to predict the breed
of unknown dogs, we’d start with a K of

00:10:13.680 --> 00:10:14.680
50.

00:10:14.680 --> 00:10:18.870
Choosing model parameters--variables like
k that can be different numbers--can be done

00:10:18.870 --> 00:10:22.870
in much more complex ways than we showed here,
or could be done using information about the

00:10:22.870 --> 00:10:27.569
specific data set you’re working with . We
not going to get into alternative methods

00:10:27.569 --> 00:10:30.320
now, but if you’re ever going to build models
for real, you should look it up.

00:10:30.320 --> 00:10:34.620
Machine Learning focuses a lot on prediction.
Instead of just accurately describing our

00:10:34.629 --> 00:10:38.400
current data, we want it to pretty accurately
predict future data.

00:10:38.400 --> 00:10:43.890
And these days, data is BIG. By one estimate,
we produce 2.5 QUINTILLION bytes of data per

00:10:43.890 --> 00:10:48.170
day. And supervised machine learning can help
us harness the strength of that data.

00:10:48.170 --> 00:10:52.560
We can teach models or rather have the models
teach themselves how to best distinguish between

00:10:52.560 --> 00:10:56.959
groups like will pay off a loan and those
that won’t. Or people who will love watching

00:10:56.959 --> 00:10:59.579
the new season of The Good Place `and those
that won’t.

00:10:59.579 --> 00:11:03.000
We’re affected by these models all the time.
From online shopping, to streaming a new show

00:11:03.000 --> 00:11:07.519
on Hulu, to a new song recommendation on Spotify.
Machine learning affects our lives everyday.

00:11:07.520 --> 00:11:11.280
And it doesn’t always make it better we’ll
get to that. Thanks for watching. I'll see you next time.

