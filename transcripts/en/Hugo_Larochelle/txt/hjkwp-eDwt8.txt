Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll look at how we can compute marginals in a linear chain conditional random fields.

Speaker 1:          00:07          Yeah.

Speaker 2:          00:07          So in the previous video, we've seen how we could compute the probability of a full sequence of labels y given x, uh, and uh, uh, to do this, we just take the exponential of the, some of the urinary and pairwise factors and then we have to divide by this partition function. In a previous video, we've seen how we can actually compute this partition function efficiently, even though it involves a lot of nested sums over other potential labels at each physician by computing, uh, an Alpha table or a bed, a table, we could actually, uh, through a dynamic program, get a very efficient procedure for that is not exponential. The size of the sequence for computing the partition. A function.

Speaker 1:          00:54          Yeah.

Speaker 2:          00:54          So, uh, to do this, like I said, we had to precompute a table of a Noah that we call alpha or a pre a table that we call better for a where each essentially computes the summation of the exponentiate good luck factors that involves, um, all of the labels to either the left of a given position or to the right of a given position. And so now let's assume again that we've done this, a computation of either the alpha. I actually have both the Alpha and Beta table. What we'll see now is an expression for how to compute marginals, uh, marginal probability over the sequence. So for instance, the probability of a single label at a specific position, Kay. And, uh, what we'll see is that this expression also requires the Alpha and Beta values. And specifically we'll assume that we're using the stable implementation of the often better table where we actually have a table that contains the Lug Alpha and the lug better, uh, values. And this is again, very important to actually implement that variant of the computation or the Alpha and Beta tables to get something that's a numerically more stable.

Speaker 2:          02:10          So now here we're interested in computing. Um, uh, we'll and we'll see to the particular marginals the marginal over a specific position. So the probability of a single label in the sequence at a given position, k or the pairwise, uh, they probably have a pair of labels at a position k again, so that's the first look at this one. The marginal property over single position. Well again, we'll use the Alpha and Beta tables. So remember that the Alpha table, uh, essentially gives us the uh, value of the luck of the, some of the exponentiate good luck factors for uh, all the potential labels to the left of a given position. And the better gives us a similar computation before the log of the psalms or the lug better table's going to give us the log of the, some of the exponentiate it a urinary and pairwise potentials to the write ups of given position.

Speaker 2:          03:08          Now if we want to Compute p of y k position k given x, well that would correspond to the sum over [inaudible] and the sum over why to up to the sum over y came on this one than the sum over y k plus one up to the sum over y capital gain of p of y where why here would be in bold. That would be the whole sequence given the sequence of inputs. Okay, so that's just marginalization and operation that we know from probability theory gives us. Now the probability over just a y k at the specific position came and we've seen that the lug alpha value and the lug better value actually gives us these summations. So Lug Alpha value performs the summation over the labels from position one to uh, came on a swan and the lug bed attire value gives us the lug summation of uh, the value of the label or position.

Speaker 2:          04:22          K plus one up to a y came. So actually specifically, uh, this sum here is going to correspond to the Alpha came on this one value. So, uh, remember this index tells us, uh, from y one, two, why came on a swan that, uh, it gives a, that this bottle corresponds to the summation of why one, up to why came on this one. And then similarly, uh, this sum here with Chris Bond to a better k plus one, uh, that is the k plus one wins that we've summed from, uh, y capital k two Y K plus one. So, uh, I won't go into detail as to how we obtained this expression or leave that as an exercise, but we can show that the property of yk given, uh, the whole sequence x is just the exponential of the unitary factor at a position case. Our preference for whether we like seeing the specific value of Y K at position k plus, well the Lug Alpha values and lug better values for where we assume that, uh, the label, that position case taking the value.

Speaker 2:          05:32          Why K here and here? So these are the sons that are conditioned on, uh, the Kl label being equals to being called to y k. Uh, and same thing here and now together probably distribution. Uh, we need, uh, to make sure that this is normalized so we can just normalize the numerator. It turns out. And so we're summing over all potential values, which I'm getting to a name. Why Prime K of the numerator? So the exponential of, uh, the urinary factor for, uh, the position gay if we assume the labeled takes, why prime Kate as a value and then the summation of everything to the left, assuming why prime, assuming the KF label, it takes value of our prime gay. And in the summation to the, uh, to the left, the submission to the left submission to the right, assuming that, uh, the kids labeled takes value.

Speaker 2:          06:26          Y Prime King. So actually this you can think of as just doing the softmax non linearity on the vectors of this term here. The luck factor plus the log Alpha luck, better values. And, um, so, uh, you know, the key to understanding why this makes sense is that the log alpha and the lug better values give us the sums over the everything to the left and everything to the right, which is a sum we have to perform. If we want to marginalize out these value, uh, these, these labels and get the probability for only a single position. And similarly using the same reasoning if you want the probability of the [inaudible] label and the k plus one year label taking value, why k and why k plus one, then we'll have a, that expression be the exponential of the unitary factor for position k. Also the urinary factor for position k plus one plus the pairwise factor, uh, uh, that expresses a preference between the label position k and k plus one.

Speaker 2:          07:33          And then plus, while the summation of everything that's to the left. So from position one, two k minus one, and then everything's, that's to the right. So from position Cape Plus to a till the end until uh, uh, position capital king. And again, to have this be a valid probability, we need to divide by the summation over all the values that the label could take. And they, Cape Plus one yet labeled could take of the exponential of essentially exactly the same term. So we see the exponential term here is exactly this term. I've just replaced why gay by y Prime K, uh, which is uh, and y and y k plus one by y prime keep plus one. So again, this is also similar to a soft max operation, but this would be a soft Max especial softmax over, uh, if you want the matrix of activations for all values of a y, K and why k plus one for this, the sons of urinary a pairwise and log out the lug better terms. All right, so this is how you compute a marginals in the conditional random fields. And the reason I explained this now is that particular we'll want to use this, uh, to make a prediction, uh, that we, uh, one procedure for performing classification of each and each position is going to be to use this. And we also use both of these marginals in their linear chain conditional random fields. Uh, when we'll want to train the conditional random fields, which is going to be the topic of, uh, the, uh, videos for the following week.