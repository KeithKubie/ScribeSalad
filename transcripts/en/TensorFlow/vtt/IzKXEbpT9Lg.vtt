WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:02.200
SKYE WANDERMAN-MILNE: I'm Skye,
for those who don't know me.

00:00:02.200 --> 00:00:04.260
I've been working on
Control Flow in TensorFlow

00:00:04.260 --> 00:00:06.810
for quite some time, with
the help of [? Sarab ?]

00:00:06.810 --> 00:00:09.630
and many other
individuals on the team.

00:00:09.630 --> 00:00:12.420
And so my goal with
this talk is to tell you

00:00:12.420 --> 00:00:16.320
everything I know about
Control Flow that's important.

00:00:16.320 --> 00:00:17.620
Let's get started.

00:00:17.620 --> 00:00:20.340
I'm going to start
by going over the lay

00:00:20.340 --> 00:00:23.160
of the land with Control
Flow in TensorFlow.

00:00:23.160 --> 00:00:27.130
So starting with what I'm
going to call the Base APIs,

00:00:27.130 --> 00:00:30.220
tf dot cond and
tf dot while loop.

00:00:30.220 --> 00:00:34.260
So these are the
primitives that are

00:00:34.260 --> 00:00:38.490
exposed in the public
Python API for users

00:00:38.490 --> 00:00:39.750
to access Control Flow.

00:00:39.750 --> 00:00:42.870
So you have conditional
execution and loops.

00:00:42.870 --> 00:00:43.410
That's it.

00:00:43.410 --> 00:00:46.500
So you might be wondering, what
about all the other Control

00:00:46.500 --> 00:00:50.040
Flow functions I know and
love, like map or case?

00:00:50.040 --> 00:00:53.493
These are all built on those two
base APIs, cond and while loop.

00:00:53.493 --> 00:00:54.910
They're sort of
wrappers around it

00:00:54.910 --> 00:00:58.410
that add useful functionality.

00:00:58.410 --> 00:01:00.660
So diving down
into the stack, how

00:01:00.660 --> 00:01:04.709
are these primitives, cond and
while, actually implemented?

00:01:04.709 --> 00:01:06.700
How are they represented
in the graph?

00:01:06.700 --> 00:01:11.650
So in TensorFlow 1.x, we have
these low-level Control Flow

00:01:11.650 --> 00:01:12.150
ops.

00:01:12.150 --> 00:01:14.940
You might have heard of them,
Exit, Enter, Nextiteration,

00:01:14.940 --> 00:01:16.380
Switch, and Merge.

00:01:16.380 --> 00:01:20.250
We'll talk more
about these in a bit.

00:01:20.250 --> 00:01:23.520
There's also an
alternate representation.

00:01:23.520 --> 00:01:26.520
That's what Control Flow
version 2 is all about.

00:01:26.520 --> 00:01:28.223
These are the "functional" ops.

00:01:28.223 --> 00:01:30.390
And I put "functional" in
quotes because it's caused

00:01:30.390 --> 00:01:32.350
some confusion in the past.

00:01:32.350 --> 00:01:33.900
It's not like pure functional.

00:01:33.900 --> 00:01:36.360
In the programming sense,
they're still state.

00:01:36.360 --> 00:01:38.910
But they're higher
order functions

00:01:38.910 --> 00:01:41.410
that take functions as input.

00:01:41.410 --> 00:01:45.950
So now, the cond branches will
be represented as functions.

00:01:48.510 --> 00:01:51.000
So these sort of do the same
thing as the low-level ops,

00:01:51.000 --> 00:01:52.800
but the higher level
functionality is all

00:01:52.800 --> 00:01:55.850
wrapped up into a single op.

00:01:55.850 --> 00:01:58.670
Moving back up the
stack, you might

00:01:58.670 --> 00:02:01.670
be wondering what's going to
happen with TensorFlow 2.0.

00:02:01.670 --> 00:02:05.600
If you're using Eager
execution, you just write Python

00:02:05.600 --> 00:02:07.250
and you just use
Python Control Flow.

00:02:07.250 --> 00:02:11.928
So if statements, or loops,
or list comprehensions,

00:02:11.928 --> 00:02:12.720
that kind of thing.

00:02:12.720 --> 00:02:16.250
So there's no arrow connecting
it to this graph mode stuff.

00:02:16.250 --> 00:02:21.350
But if you use tf dot
function, maybe some people

00:02:21.350 --> 00:02:23.540
have heard of Autograph,
which is automatically

00:02:23.540 --> 00:02:25.640
included in tf dot
function, and this

00:02:25.640 --> 00:02:29.660
attempts to take your eager
style, just Python code,

00:02:29.660 --> 00:02:33.830
and convert it into new Python
code that calls the TensorFlow

00:02:33.830 --> 00:02:35.270
graph APIs.

00:02:35.270 --> 00:02:36.928
So it's going to
try to rewrite all

00:02:36.928 --> 00:02:39.470
that Python Control Flow, your
if statements and while loops,

00:02:39.470 --> 00:02:43.170
into tf dot cond and
tf dot while loop.

00:02:43.170 --> 00:02:45.110
So note that Autograph
is just dealing

00:02:45.110 --> 00:02:48.260
at this abstraction layer of
the public TensorFlow API.

00:02:48.260 --> 00:02:50.600
It doesn't have to dive
down into the low-level ops

00:02:50.600 --> 00:02:52.765
or anything like that.

00:02:52.765 --> 00:02:54.140
So that's kind of
where we're at.

00:02:54.140 --> 00:02:57.710
We have the 2.0 world where you
just write Python that maybe it

00:02:57.710 --> 00:03:01.280
can get converted into our
public Graph APIs, which

00:03:01.280 --> 00:03:08.480
in turn are producing these
various operators in the graph.

00:03:08.480 --> 00:03:10.580
And one more thing.

00:03:10.580 --> 00:03:12.980
Right now, in this
new implementation

00:03:12.980 --> 00:03:15.290
of Control Flow,
Control Flow version 2,

00:03:15.290 --> 00:03:18.710
we are still converting
the functional ops back

00:03:18.710 --> 00:03:20.360
into the low-level ops.

00:03:20.360 --> 00:03:23.270
This is basically a
performance optimization.

00:03:23.270 --> 00:03:25.290
I hope we don't have
to do it in the future.

00:03:25.290 --> 00:03:28.610
That's why it's this
faded-dash arrow.

00:03:28.610 --> 00:03:34.010
So this talk, we're gonna
focus on the base API

00:03:34.010 --> 00:03:36.757
and how it's implemented.

00:03:36.757 --> 00:03:38.840
I think there'll be another
talk about Autographs,

00:03:38.840 --> 00:03:41.210
so hopefully they can talk
about Control Flow there.

00:03:41.210 --> 00:03:43.670
Maybe there's also talk
about Eager execution

00:03:43.670 --> 00:03:46.520
and the high-level APIs
that are not so complicated.

00:03:46.520 --> 00:03:50.970
So leave that as an
exercise to the viewer.

00:03:50.970 --> 00:03:51.470
OK.

00:03:51.470 --> 00:03:55.280
So I'm going to start with
going over Control Flow

00:03:55.280 --> 00:03:59.150
v1, the original
low-level representation.

00:03:59.150 --> 00:04:00.380
You might be asking, why?

00:04:00.380 --> 00:04:01.670
Why do we care at all?

00:04:01.670 --> 00:04:03.770
So like I showed
in the diagram, we

00:04:03.770 --> 00:04:07.010
do still convert the functional
ops to this representation.

00:04:07.010 --> 00:04:11.150
So this is basically how
it's executed today, always.

00:04:11.150 --> 00:04:14.900
Furthermore, this is still
what we use in TensorFlow 1.x.

00:04:14.900 --> 00:04:18.110
So all 1.x code is
using Control Flow v1.

00:04:18.110 --> 00:04:20.060
Still very much alive.

00:04:20.060 --> 00:04:22.430
And I hope it provides a
little bit of motivation

00:04:22.430 --> 00:04:24.780
for why we wanted to
implement Control Flow using

00:04:24.780 --> 00:04:26.960
the functional ops.

00:04:26.960 --> 00:04:31.070
So I'm going to start
with these low-level ops.

00:04:31.070 --> 00:04:34.840
So up here, Switch and Merge are
used for conditional execution,

00:04:34.840 --> 00:04:36.860
this is tf dot cond.

00:04:36.860 --> 00:04:39.560
Also in while loops to determine
whether we need to keep

00:04:39.560 --> 00:04:41.360
iterating or we're done.

00:04:41.360 --> 00:04:43.100
And then Enter, Exit,
and Nextiteration

00:04:43.100 --> 00:04:46.490
are just used while loops
to manage the iterations.

00:04:46.490 --> 00:04:49.020
So let's dive in.

00:04:49.020 --> 00:04:53.022
So Switch and Merge, these
are for conditionals.

00:04:53.022 --> 00:04:54.230
Let's just start with Switch.

00:04:54.230 --> 00:04:56.450
The idea is you get your
predicate tensor in,

00:04:56.450 --> 00:04:58.880
this is a Boolean, that tells
you which conditional branch

00:04:58.880 --> 00:04:59.990
you want to take.

00:04:59.990 --> 00:05:02.550
And then it has a
single data input, so

00:05:02.550 --> 00:05:03.910
[INAUDIBLE] some tensor.

00:05:03.910 --> 00:05:06.470
And it's just going to
forward that data input to one

00:05:06.470 --> 00:05:08.960
of its two outputs
depending on the predicate.

00:05:08.960 --> 00:05:11.100
So in this picture, the
predicate must be false.

00:05:11.100 --> 00:05:15.260
And so the data's coming
out of the false output.

00:05:15.260 --> 00:05:17.100
Merge basically
does the opposite.

00:05:17.100 --> 00:05:20.450
It takes two inputs,
but it only expects data

00:05:20.450 --> 00:05:22.160
from one of its inputs.

00:05:22.160 --> 00:05:25.950
And then it just
outputs a single output.

00:05:25.950 --> 00:05:29.120
So Switch is how you start
your conditional execution,

00:05:29.120 --> 00:05:33.340
because it's going to divert
that data into one branch.

00:05:33.340 --> 00:05:35.030
And then Merge brings
it back together

00:05:35.030 --> 00:05:36.650
into your mainline execution.

00:05:36.650 --> 00:05:39.080
It's not conditional anymore.

00:05:39.080 --> 00:05:42.110
One implementation detail
I'm going to mention here

00:05:42.110 --> 00:05:44.150
is dead tensors.

00:05:44.150 --> 00:05:46.400
So you might think
that nothing is

00:05:46.400 --> 00:05:50.030
going to come out of the
true output of the Switch,

00:05:50.030 --> 00:05:53.510
but it actually does output
a special dead tensor, which

00:05:53.510 --> 00:05:55.070
is just like a sentinel value.

00:05:55.070 --> 00:05:57.580
Like a little tiny thing.

00:05:57.580 --> 00:06:01.340
And dead tensors flow
through the whole untaken

00:06:01.340 --> 00:06:02.598
conditional branch.

00:06:02.598 --> 00:06:04.640
And eventually, you're
going to get a dead tensor

00:06:04.640 --> 00:06:05.960
into this Merge.

00:06:05.960 --> 00:06:10.490
It just ignores it and outputs
whatever data tensor it gets.

00:06:10.490 --> 00:06:12.860
So dead tensors are needed
for distributed Control

00:06:12.860 --> 00:06:16.070
Flow, which I'm actually not
going to cover in this talk.

00:06:16.070 --> 00:06:17.990
Because it's kind
of technical and I

00:06:17.990 --> 00:06:21.630
haven't found it that important
to know the details of it.

00:06:21.630 --> 00:06:25.120
It's covered in Yuan's paper.

00:06:25.120 --> 00:06:27.920
But I'm mentioning dead
tensors because they do show up

00:06:27.920 --> 00:06:29.000
a lot in the execution.

00:06:29.000 --> 00:06:30.625
Like, if you look at
the executor code,

00:06:30.625 --> 00:06:32.773
there's all this special
case for dead tensors.

00:06:32.773 --> 00:06:35.190
This is what they're about,
it's for conditional execution

00:06:35.190 --> 00:06:36.305
so we can do distribution.

00:06:36.305 --> 00:06:38.240
SPEAKER 1: And retval
zero doesn't help any.

00:06:38.240 --> 00:06:38.670
SKYE WANDERMAN-MILNE: Oh, yeah.

00:06:38.670 --> 00:06:40.270
And that famous
error message I want

00:06:40.270 --> 00:06:43.222
to put on a t-shirt, retval
zero does not have a value.

00:06:43.222 --> 00:06:45.180
That means you're trying
to read a dead tensor,

00:06:45.180 --> 00:06:47.760
or it probably
means there's a bug.

00:06:47.760 --> 00:06:48.410
OK.

00:06:48.410 --> 00:06:53.490
Moving on to the low-level
ops we use for while loops.

00:06:53.490 --> 00:06:58.370
These manage
iterations, basically.

00:06:58.370 --> 00:07:03.080
The concept you need to know
about in execution is frames.

00:07:03.080 --> 00:07:07.140
So you have one
frame per execution.

00:07:07.140 --> 00:07:09.260
And this is what
allows the executor

00:07:09.260 --> 00:07:11.780
to keep track of
multiple iterations,

00:07:11.780 --> 00:07:14.690
and allows a single op to
be run multiple times as you

00:07:14.690 --> 00:07:16.530
do multiple iterations.

00:07:16.530 --> 00:07:20.240
So a frame defines a name, which
is for the whole while loop.

00:07:20.240 --> 00:07:22.740
And then it also has
an iteration number.

00:07:22.740 --> 00:07:26.810
So the Enter op, that just
establishes a new frame.

00:07:26.810 --> 00:07:29.160
It means we're starting
a new while loop.

00:07:29.160 --> 00:07:31.250
So it just forwards its input.

00:07:31.250 --> 00:07:33.740
It's like an identity,
except that output is now

00:07:33.740 --> 00:07:35.920
in this new frame.

00:07:35.920 --> 00:07:37.790
And it has an attribute
that's the frame

00:07:37.790 --> 00:07:41.050
name, starts at frame 0.

00:07:41.050 --> 00:07:42.110
Exit's the opposite.

00:07:42.110 --> 00:07:44.300
It just it's like an
identity, except it strips

00:07:44.300 --> 00:07:46.040
the frame from its input.

00:07:46.040 --> 00:07:49.298
So output is now not
in that frame anymore.

00:07:49.298 --> 00:07:50.340
And these can be stacked.

00:07:50.340 --> 00:07:52.590
So if you have a bunch of
Enters on a bunch of frames,

00:07:52.590 --> 00:07:55.240
you have a bunch of Exits, it'll
pop them off one at the time.

00:07:55.240 --> 00:07:58.160
The Nextiteration's just
the final piece in order

00:07:58.160 --> 00:08:00.588
to increment that
iteration count.

00:08:00.588 --> 00:08:02.880
This might make more sense
when we put it all together,

00:08:02.880 --> 00:08:04.700
so let's do that.

00:08:04.700 --> 00:08:07.970
Starting with tf cond again.

00:08:07.970 --> 00:08:09.600
Let's just work through this.

00:08:09.600 --> 00:08:15.860
So down here, you have the
API call that we're using.

00:08:15.860 --> 00:08:17.360
So we start, we
have this predicate.

00:08:17.360 --> 00:08:21.260
Note that the predicate isn't
actually part of the cond.

00:08:21.260 --> 00:08:23.960
It happens outside here,
but then we feed it

00:08:23.960 --> 00:08:26.660
into the Switch operators.

00:08:26.660 --> 00:08:29.270
So the Switches and
Merges mark the boundary

00:08:29.270 --> 00:08:31.620
of the conditional
execution, remember.

00:08:31.620 --> 00:08:34.640
So we'll feed this predicate
and then, the true branch

00:08:34.640 --> 00:08:36.500
is an Add.

00:08:36.500 --> 00:08:40.190
So we have a Switch
for each input,

00:08:40.190 --> 00:08:43.309
for x and z, which is
the external tensors we

00:08:43.309 --> 00:08:44.960
use in that branch.

00:08:44.960 --> 00:08:47.090
You'll note that they
are only being emitted

00:08:47.090 --> 00:08:49.340
from the true side of it.

00:08:49.340 --> 00:08:54.470
So if the false branch is taken,
nothing's connected to that.

00:08:54.470 --> 00:08:58.470
That comes out of Add, then
similarly on the other side,

00:08:58.470 --> 00:09:00.590
we're Squaring y, so we
have a Switch for the y.

00:09:00.590 --> 00:09:03.680
This time, it's going to be
emitted from the false branch

00:09:03.680 --> 00:09:06.430
into the Square.

00:09:06.430 --> 00:09:08.960
And then, we only
have one output

00:09:08.960 --> 00:09:11.140
from this cond so we
have a single Merge.

00:09:11.140 --> 00:09:13.235
Either the Square or the
Add, only one of those

00:09:13.235 --> 00:09:15.860
is going to actually have data,
and that's what will be output.

00:09:15.860 --> 00:09:18.710
So note that there is
a Switch for each input

00:09:18.710 --> 00:09:21.390
and a Merge for each output,
they don't have to match.

00:09:21.390 --> 00:09:24.230
And in this example,
the two branches

00:09:24.230 --> 00:09:26.150
are using disjoint tensors.

00:09:26.150 --> 00:09:28.778
But say, we did the
Square of x instead of y,

00:09:28.778 --> 00:09:30.320
then you would have
an edge from both

00:09:30.320 --> 00:09:33.020
the true output and the
false output, depending.

00:09:33.020 --> 00:09:34.250
Go to the Add or the Square.

00:09:36.582 --> 00:09:38.540
Let's quickly, actually,
go over the while loop

00:09:38.540 --> 00:09:42.230
API, just to make
sure we all remember.

00:09:42.230 --> 00:09:44.390
So the first argument,
is a function.

00:09:44.390 --> 00:09:46.360
That's the predicate function.

00:09:46.360 --> 00:09:48.860
The second function is the body
that we're going to execute.

00:09:48.860 --> 00:09:50.670
And this is where it's
kind of interesting.

00:09:50.670 --> 00:09:54.230
So you have some inputs, these
are called the loop variables,

00:09:54.230 --> 00:09:56.150
the input to the while loop.

00:09:56.150 --> 00:09:58.730
And then it's going to
output updated versions

00:09:58.730 --> 00:10:00.560
of those same loop variables.

00:10:00.560 --> 00:10:03.980
So the inputs of the body
match the outputs of the body.

00:10:03.980 --> 00:10:06.590
Like, same number-type shape
of tensors because they're

00:10:06.590 --> 00:10:08.195
just the updated variables.

00:10:08.195 --> 00:10:10.395
SPEAKER 2: Can't the
shape-type [INAUDIBLE]

00:10:10.395 --> 00:10:12.770
SKYE WANDERMAN-MILNE: The
shape can change, you're right.

00:10:12.770 --> 00:10:16.020
Same number and types.

00:10:16.020 --> 00:10:19.400
And then the final, we'd
provide some initial input

00:10:19.400 --> 00:10:20.140
to start it off.

00:10:20.140 --> 00:10:22.573
So that's the 0,
the final argument.

00:10:22.573 --> 00:10:23.990
And then the output
is going to be

00:10:23.990 --> 00:10:28.950
whatever the final value
of the loop variables are.

00:10:28.950 --> 00:10:33.800
And then the predicate function
takes those same loop variables

00:10:33.800 --> 00:10:36.320
as input but just
outputs a Boolean, like,

00:10:36.320 --> 00:10:39.320
do we continue execution or not?

00:10:39.320 --> 00:10:43.180
So now we'll start
with the inter-node.

00:10:43.180 --> 00:10:45.080
This, remember,
establishes the new frame.

00:10:45.080 --> 00:10:46.130
We're starting a new while loop.

00:10:46.130 --> 00:10:47.540
I guess it's called L for loop.

00:10:50.220 --> 00:10:52.645
We go through a Merge now,
kind of reversed from the cond

00:10:52.645 --> 00:10:53.520
where you start with the Switch.

00:10:53.520 --> 00:10:54.645
Now you start with a Merge.

00:10:54.645 --> 00:10:57.350
Because it's choosing is
this the initial value

00:10:57.350 --> 00:11:00.860
or is this the new, updated
value from an iteration?

00:11:00.860 --> 00:11:02.160
That feeds into the predicate.

00:11:02.160 --> 00:11:04.160
Note that the predicate
is inside the while loop

00:11:04.160 --> 00:11:07.880
now because it has to
execute multiple times.

00:11:07.880 --> 00:11:09.380
The output goes
into the Switch node

00:11:09.380 --> 00:11:13.270
to choose whether if
it's false, and we're

00:11:13.270 --> 00:11:16.430
going to exit the while
loop with that exit node.

00:11:16.430 --> 00:11:20.610
Otherwise, we go into the body,
which is an Add in this case,

00:11:20.610 --> 00:11:23.210
take the output of the body,
feed it to the next iteration.

00:11:23.210 --> 00:11:25.490
Because we have to bump
that frame count, remember?

00:11:25.490 --> 00:11:27.230
And then feed it
back into the Merge,

00:11:27.230 --> 00:11:29.660
which will forward it
back again and again,

00:11:29.660 --> 00:11:31.130
until we get to the Exit.

00:11:31.130 --> 00:11:33.150
So, hopefully, this
kind of makes sense.

00:11:33.150 --> 00:11:34.850
You can see there's
a loop in there.

00:11:34.850 --> 00:11:36.880
That's the while loop.

00:11:36.880 --> 00:11:41.260
SPEAKER 3: For sequential
ones, how does the Merge know

00:11:41.260 --> 00:11:43.920
to select the z or [INAUDIBLE]?

00:11:43.920 --> 00:11:47.240
Because wouldn't neither of them
be dead tensors at that point?

00:11:47.240 --> 00:11:48.440
SKYE WANDERMAN-MILNE: I
don't know the details

00:11:48.440 --> 00:11:49.565
of how this is implemented.

00:11:49.565 --> 00:11:52.460
But I think because
the frame is different,

00:11:52.460 --> 00:11:55.870
z only is in the first frame.

00:11:55.870 --> 00:11:59.080
Because each frame
is conceptually

00:11:59.080 --> 00:12:00.600
like you made a
copy of the body,

00:12:00.600 --> 00:12:02.475
it's going to keep track
of different pending

00:12:02.475 --> 00:12:05.860
counts for each node in
the body, or the Merge,

00:12:05.860 --> 00:12:06.863
or the Switch.

00:12:06.863 --> 00:12:07.780
So I think that's why.

00:12:10.330 --> 00:12:11.395
OK.

00:12:11.395 --> 00:12:13.270
All right, so that's
all I'm going to go over

00:12:13.270 --> 00:12:15.160
with Control Flow v1.

00:12:15.160 --> 00:12:17.170
It does have some advantages.

00:12:17.170 --> 00:12:19.090
It all, kind of,
falls out of the fact

00:12:19.090 --> 00:12:23.080
that these low-level operators
are designed to naturally fit

00:12:23.080 --> 00:12:27.310
within the dataflow model,
because data graphs are

00:12:27.310 --> 00:12:28.450
dataflow graphs.

00:12:28.450 --> 00:12:31.540
So you get nice
features like pruning,

00:12:31.540 --> 00:12:34.930
works pretty naturally,
because it's all regular nodes,

00:12:34.930 --> 00:12:36.910
sort of, for pruning.

00:12:36.910 --> 00:12:39.700
You can have parallel execution
of while loop iterations, which

00:12:39.700 --> 00:12:41.920
is actually pretty
cool, I think.

00:12:41.920 --> 00:12:45.520
Because once you add
in this frames logic,

00:12:45.520 --> 00:12:48.850
it kind of naturally keeps
track of all the pending counts.

00:12:48.850 --> 00:12:51.070
It runs just like a regular--

00:12:51.070 --> 00:12:53.920
like, if you unrolled
the loop and the data

00:12:53.920 --> 00:12:56.110
will flow through
as far as it can.

00:12:56.110 --> 00:12:59.280
Ops will be executed
as soon as they can.

00:12:59.280 --> 00:13:01.640
It just kind of works.

00:13:01.640 --> 00:13:03.970
However, there are
some disadvantages.

00:13:03.970 --> 00:13:05.020
It's very complicated.

00:13:05.020 --> 00:13:07.480
Like, you can see that
this is a bunch of nodes

00:13:07.480 --> 00:13:10.090
to express what in most
programming languages

00:13:10.090 --> 00:13:13.660
is like one line, like while.

00:13:13.660 --> 00:13:16.870
This shows up
especially in gradients

00:13:16.870 --> 00:13:18.190
and nested Control Flow.

00:13:18.190 --> 00:13:20.080
You end up with all
these crazy edge

00:13:20.080 --> 00:13:23.530
cases where you didn't hook
up the inner Merges correctly

00:13:23.530 --> 00:13:25.330
or whatever.

00:13:25.330 --> 00:13:28.180
As a result of this complexity,
higher order derivatives

00:13:28.180 --> 00:13:29.170
are not implemented.

00:13:29.170 --> 00:13:33.152
This is not like a
design problem, per se.

00:13:33.152 --> 00:13:34.860
It's just it's so
complicated and there's

00:13:34.860 --> 00:13:38.560
so many edge cases no one
has been able to do it,

00:13:38.560 --> 00:13:41.630
or has wanted to do it.

00:13:41.630 --> 00:13:44.110
Similarly to graph
construction being complicated,

00:13:44.110 --> 00:13:45.640
the runtime is complicated.

00:13:45.640 --> 00:13:47.890
Because you have to have
all this dead tensor logic,

00:13:47.890 --> 00:13:50.950
all this firm logic, and
it's very intricately baked

00:13:50.950 --> 00:13:52.870
into the executor.

00:13:52.870 --> 00:13:55.600
And this makes it hard
to read and maintain,

00:13:55.600 --> 00:13:59.980
and also, adds
performance overhead.

00:13:59.980 --> 00:14:04.150
It's hard for other
downstream things

00:14:04.150 --> 00:14:06.010
to analyze and make sense of.

00:14:06.010 --> 00:14:08.470
An example of this
is [INAUDIBLE]

00:14:08.470 --> 00:14:11.800
has been trying to do
[? auto ?] clustering for XLA,

00:14:11.800 --> 00:14:13.660
and so he has like
whole docs written

00:14:13.660 --> 00:14:16.240
on how to handle dead
tensors, because they

00:14:16.240 --> 00:14:17.950
can show up anywhere.

00:14:17.950 --> 00:14:22.180
Similarly, XLA actually
represents Control Flow

00:14:22.180 --> 00:14:24.320
in a functional way
if in while ops.

00:14:24.320 --> 00:14:27.280
So when they consume
TensorFlow graphs,

00:14:27.280 --> 00:14:31.420
they have to pattern-match
this crazy stuff back

00:14:31.420 --> 00:14:34.880
into just the while op that
originally produced it.

00:14:34.880 --> 00:14:37.830
And especially with gradients
and nested Control Flow,

00:14:37.830 --> 00:14:39.370
it gets very complicated.

00:14:39.370 --> 00:14:41.530
There is a number of edge cases.

00:14:41.530 --> 00:14:44.980
This was actually one
of the main motivations

00:14:44.980 --> 00:14:47.320
for building Control Flow v2.

00:14:47.320 --> 00:14:49.752
Because we were fixing
so many bugs and how

00:14:49.752 --> 00:14:52.210
this was represented in so many
edge cases, that it's like,

00:14:52.210 --> 00:14:55.040
we just need a simpler
representation.

00:14:55.040 --> 00:14:55.700
OK.

00:14:55.700 --> 00:14:59.900
So, hopefully, this
will be simpler.

00:14:59.900 --> 00:15:01.420
I can fit it on
one slide for both.

00:15:01.420 --> 00:15:02.960
[LAUGHTER]

00:15:02.960 --> 00:15:09.080
So tf dot cond, it's
just an if op now.

00:15:09.080 --> 00:15:12.230
You have the Boolean
predicate coming in.

00:15:12.230 --> 00:15:15.290
These arrows represent the
type signature of the op, not

00:15:15.290 --> 00:15:17.060
individual tensors per se.

00:15:17.060 --> 00:15:20.300
So then this could be
any number and type

00:15:20.300 --> 00:15:22.400
of tensors coming into input.

00:15:22.400 --> 00:15:25.730
And then similarly, any number
of type tensor is coming out.

00:15:25.730 --> 00:15:28.220
They don't have to match.

00:15:28.220 --> 00:15:30.830
Then these represent,
they're technically

00:15:30.830 --> 00:15:32.720
function attributes,
but they're basically

00:15:32.720 --> 00:15:35.565
functions attached to this op
representing the true branch

00:15:35.565 --> 00:15:36.440
and the false branch.

00:15:36.440 --> 00:15:39.740
So they're like,
little subgraphs.

00:15:39.740 --> 00:15:42.560
One thing to note that's
important with these functions

00:15:42.560 --> 00:15:45.530
is that the function
signatures have to match.

00:15:45.530 --> 00:15:48.835
So the functions have the same
inputs and the same outputs.

00:15:48.835 --> 00:15:50.210
The inputs and
outputs don't have

00:15:50.210 --> 00:15:52.793
to match, what but they have to
match across the two branches.

00:15:54.500 --> 00:15:56.910
SPEAKER 4: [INAUDIBLE]
the type, not values?

00:15:56.910 --> 00:15:57.370
SKYE WANDERMAN-MILNE: Yes.

00:15:57.370 --> 00:15:57.800
Sorry.

00:15:57.800 --> 00:15:59.717
Well, we're just talking
signatures right now.

00:15:59.717 --> 00:16:05.840
So just type and possibly
shape in some cases.

00:16:05.840 --> 00:16:08.870
Yeah, it doesn't even have
to be implemented this way,

00:16:08.870 --> 00:16:09.920
but it is.

00:16:09.920 --> 00:16:12.950
It makes somethings
simpler to think about.

00:16:12.950 --> 00:16:15.050
But keep that in mind.

00:16:15.050 --> 00:16:18.920
Similarly, tf dot while loop
just turns into a while op now.

00:16:18.920 --> 00:16:21.412
Now all our inputs and outputs
are just the loop variables.

00:16:21.412 --> 00:16:23.870
Because, remember, the predicate
takes those loop variables

00:16:23.870 --> 00:16:25.130
as inputs.

00:16:25.130 --> 00:16:29.210
So you have a cond function
or a predicate function,

00:16:29.210 --> 00:16:31.580
takes a loop verbals as
input, output, or Bool.

00:16:31.580 --> 00:16:35.990
And then the body function that
takes the loop variable inputs

00:16:35.990 --> 00:16:39.860
and outputs, the updated
version, which will eventually

00:16:39.860 --> 00:16:43.520
be-- the final value will be
updated output from the op.

00:16:43.520 --> 00:16:44.520
So does this make sense?

00:16:44.520 --> 00:16:46.260
This picture.

00:16:46.260 --> 00:16:49.880
SPEAKER 4: One thing to clarify
is, in tf cond it doesn't have,

00:16:49.880 --> 00:16:53.290
actually, any concept of
variables in the higher level

00:16:53.290 --> 00:16:53.790
API.

00:16:53.790 --> 00:16:56.120
So this is things we
capture and we take

00:16:56.120 --> 00:16:58.115
care of making sure they match.

00:16:58.115 --> 00:16:59.490
So from the user's
point of view,

00:16:59.490 --> 00:17:01.115
they don't have to
do anything special.

00:17:01.115 --> 00:17:02.330
SKYE WANDERMAN-MILNE: Right.

00:17:02.330 --> 00:17:05.569
That's, kind of, like the
while op very closely matches

00:17:05.569 --> 00:17:06.710
the TensorFlow semantics.

00:17:06.710 --> 00:17:09.380
But the if op is a
little bit different.

00:17:09.380 --> 00:17:12.920
They have to match [INAUDIBLE]
inputs at all, because we do it

00:17:12.920 --> 00:17:15.619
through closures and API.

00:17:15.619 --> 00:17:17.369
That's like, you do
it within your code.

00:17:17.369 --> 00:17:18.869
So if this is good
for everyone, I'm

00:17:18.869 --> 00:17:21.440
going to move on to
going over gradients.

00:17:21.440 --> 00:17:24.172
I'm going over how gradients
work in Control Flow v2.

00:17:24.172 --> 00:17:25.130
It is somewhat general.

00:17:25.130 --> 00:17:28.099
It's much simpler to think
about with the functional ops.

00:17:28.099 --> 00:17:29.810
So let's start at a high level.

00:17:29.810 --> 00:17:34.830
Just conceptually, what
is the gradient of a cond?

00:17:34.830 --> 00:17:37.130
It's basically,
just another cond.

00:17:37.130 --> 00:17:42.140
And you take the same
predicate, and you take

00:17:42.140 --> 00:17:43.810
the gradient of both sides.

00:17:43.810 --> 00:17:47.070
So if we took the
forward true branch,

00:17:47.070 --> 00:17:50.120
then we want to take the
gradient of the true branch

00:17:50.120 --> 00:17:51.550
on the way back.

00:17:51.550 --> 00:17:52.460
Make sense?

00:17:52.460 --> 00:17:53.460
Hopefully, this is good.

00:17:55.820 --> 00:17:59.870
While loops, a little bit
more complicated, not too bad.

00:17:59.870 --> 00:18:02.060
So say we have this
forward while loop,

00:18:02.060 --> 00:18:04.010
you have your cond
and body functions.

00:18:04.010 --> 00:18:09.220
Just assume it executes end
times for now, we just know.

00:18:09.220 --> 00:18:17.430
So now the gradient, we
have to execute the gradient

00:18:17.430 --> 00:18:20.490
of the body function N times.

00:18:20.490 --> 00:18:22.260
Like we just have
to do the reverse.

00:18:22.260 --> 00:18:26.340
Imagine an unrolled loop, we
did N invocations of the body.

00:18:26.340 --> 00:18:28.050
Now we're going to
do N invocations

00:18:28.050 --> 00:18:30.840
of the gradient of the body.

00:18:30.840 --> 00:18:37.000
And you pass in the grad y's
or cotangents or whatever.

00:18:37.000 --> 00:18:39.490
And those are your
loop variables.

00:18:39.490 --> 00:18:41.040
Then your predicate,
now, is just

00:18:41.040 --> 00:18:44.440
this counter to make
sure we execute N times.

00:18:44.440 --> 00:18:46.150
So, hopefully, this makes sense.

00:18:46.150 --> 00:18:50.160
The big question is, how
do we know what N is?

00:18:50.160 --> 00:18:52.980
The answer is that, at
least in Control Flow v2,

00:18:52.980 --> 00:18:57.120
we just add a little counter
to every a while loop.

00:18:57.120 --> 00:18:59.310
That just outputs the
total number of iterations.

00:18:59.310 --> 00:19:00.960
And we don't return
this to the user,

00:19:00.960 --> 00:19:04.650
but we can wire it through to
the gradient when we need it.

00:19:04.650 --> 00:19:06.478
Does this make sense
at a high level?

00:19:06.478 --> 00:19:08.020
We're going to dive
into the details.

00:19:08.020 --> 00:19:09.950
But this is concept.

00:19:15.050 --> 00:19:15.600
OK.

00:19:15.600 --> 00:19:18.590
So I'm about to go into
more concrete examples.

00:19:18.590 --> 00:19:21.060
And I'm also going to
discuss the tricky part

00:19:21.060 --> 00:19:25.260
about gradients, which
is intermediate values.

00:19:25.260 --> 00:19:28.710
Basically, when you
have a data dependency

00:19:28.710 --> 00:19:31.660
from the forward pass
to the backwards pass.

00:19:31.660 --> 00:19:33.960
So start with cond.

00:19:33.960 --> 00:19:36.780
Here is a similar diagram.

00:19:36.780 --> 00:19:39.245
I rearranged it to
make it fit nicer.

00:19:39.245 --> 00:19:40.620
But one important
thing to notice

00:19:40.620 --> 00:19:43.020
is that now the arrows
are actual tensors.

00:19:43.020 --> 00:19:45.570
They're not just type
signatures anymore.

00:19:45.570 --> 00:19:47.760
So the predicate is a Boolean.

00:19:47.760 --> 00:19:51.180
In this example, there's only
one input and one output,

00:19:51.180 --> 00:19:53.040
maybe they're different
types, who knows.

00:19:53.040 --> 00:19:54.640
Doesn't matter for this example.

00:19:54.640 --> 00:19:56.580
And then you have the
true and false functions

00:19:56.580 --> 00:20:01.810
with the same types.

00:20:01.810 --> 00:20:02.310
OK.

00:20:02.310 --> 00:20:04.150
So here's the gradient function.

00:20:04.150 --> 00:20:05.555
It's just another if.

00:20:05.555 --> 00:20:07.680
This time we're dealing
with the cotangents instead

00:20:07.680 --> 00:20:10.085
of the initial forward values.

00:20:10.085 --> 00:20:11.960
And we have the gradient
of the true function

00:20:11.960 --> 00:20:13.920
and the gradient of
the false function.

00:20:13.920 --> 00:20:14.800
Looks good so far.

00:20:14.800 --> 00:20:16.440
Hopefully.

00:20:16.440 --> 00:20:18.068
If there was no
data dependencies

00:20:18.068 --> 00:20:20.610
between the forward and backward
pass, like if you're doing y

00:20:20.610 --> 00:20:23.790
equals x plus 1,
this is all you need.

00:20:23.790 --> 00:20:28.040
But what if somewhere in
the forward pass, let's

00:20:28.040 --> 00:20:30.195
say the true function,
there's an op?

00:20:30.195 --> 00:20:34.290
And we need to use its
output in the backwards pass?

00:20:34.290 --> 00:20:36.660
So this is conceptually
what we need to do.

00:20:36.660 --> 00:20:40.740
We need z in the
gradient function.

00:20:40.740 --> 00:20:43.110
This is a problem,
because you can't just

00:20:43.110 --> 00:20:46.470
have an edge between two
function definitions.

00:20:46.470 --> 00:20:48.240
You need to have
inputs and outputs.

00:20:48.240 --> 00:20:50.255
Like, they need to go--

00:20:50.255 --> 00:20:54.030
The If ops need to be attached
to each other with an edge.

00:20:54.030 --> 00:20:56.190
This doesn't make
sense by itself.

00:20:56.190 --> 00:20:58.415
So we're, basically,
going to do just that.

00:20:58.415 --> 00:21:00.040
We're going to make
inputs and outputs.

00:21:00.040 --> 00:21:02.640
We're going to add
them to the if op.

00:21:02.640 --> 00:21:03.640
So let's do that.

00:21:03.640 --> 00:21:06.120
So we're going to output
z from true function.

00:21:06.120 --> 00:21:07.830
And then similarly,
add it as an output

00:21:07.830 --> 00:21:11.750
from the if op, because the if
op is calling true function.

00:21:11.750 --> 00:21:14.980
And then we're going to add
it as an input to the gradient

00:21:14.980 --> 00:21:15.990
if op.

00:21:15.990 --> 00:21:20.520
And add it as an input to
the gradient true function.

00:21:20.520 --> 00:21:24.120
OK, there's still
one problem, though.

00:21:24.120 --> 00:21:28.320
And that's that now the true
and false branches of both if op

00:21:28.320 --> 00:21:29.590
don't match anymore.

00:21:29.590 --> 00:21:32.370
We need them to have
the same signature.

00:21:32.370 --> 00:21:36.090
So let's just add some
inputs and outputs.

00:21:36.090 --> 00:21:41.000
Starting on the gradient
side, this is fine.

00:21:41.000 --> 00:21:43.280
We can just add z as an
input to the false function.

00:21:43.280 --> 00:21:46.000
It's just going to ignore
it, it's an unused input.

00:21:46.000 --> 00:21:49.990
But on the forward
pass, this is a problem.

00:21:49.990 --> 00:21:52.880
Because we need to add z as an
output to the false function,

00:21:52.880 --> 00:21:56.260
but we don't actually
have anything to output.

00:21:56.260 --> 00:22:00.180
It's like, what is
this question mark op?

00:22:00.180 --> 00:22:06.020
And it needs to be the same
type, and possibly shape,

00:22:06.020 --> 00:22:11.000
if we want to keep a strong
shape, or a fully known shape.

00:22:11.000 --> 00:22:14.260
And we might not know
the shape until runtime.

00:22:14.260 --> 00:22:16.540
So what we do?

00:22:16.540 --> 00:22:19.090
I had to think about
this for a long time

00:22:19.090 --> 00:22:20.950
and came up with many
different solutions.

00:22:20.950 --> 00:22:23.470
And I partially
implemented all of them

00:22:23.470 --> 00:22:27.640
before coming up
with using Optionals.

00:22:27.640 --> 00:22:29.415
Optionals are maybe types.

00:22:29.415 --> 00:22:30.290
You've heard of that?

00:22:30.290 --> 00:22:32.320
It's a special
kind of tensor that

00:22:32.320 --> 00:22:35.677
can hold another tensor
inside of it or not.

00:22:35.677 --> 00:22:38.260
So it's just a wrapper that may
or may not have another tensor

00:22:38.260 --> 00:22:38.912
inside of it.

00:22:38.912 --> 00:22:39.870
And it's also a tensor.

00:22:39.870 --> 00:22:41.900
It's like a variant tensor.

00:22:41.900 --> 00:22:44.110
So the true function is
going to return an Optional

00:22:44.110 --> 00:22:46.390
with the z value inside of it.

00:22:46.390 --> 00:22:48.640
The false function is
going to return an Optional

00:22:48.640 --> 00:22:50.300
with no value inside of it.

00:22:50.300 --> 00:22:50.800
OK, great.

00:22:50.800 --> 00:22:54.490
Now they're the
same type, Optional.

00:22:54.490 --> 00:22:57.580
Could have the same
thing inside them.

00:22:57.580 --> 00:22:59.440
In a gradient true
function, we can

00:22:59.440 --> 00:23:01.815
unwrap that Optional
to get the raw z value.

00:23:01.815 --> 00:23:03.190
And then the false
function still

00:23:03.190 --> 00:23:05.190
just ignores it, which
is great, because there's

00:23:05.190 --> 00:23:06.370
nothing inside of it.

00:23:06.370 --> 00:23:08.860
I didn't know how to draw
this, but that's what we do.

00:23:08.860 --> 00:23:10.690
So all the intermediate
values that

00:23:10.690 --> 00:23:13.540
are needed by the
grading computation

00:23:13.540 --> 00:23:19.030
are added as Optional
outputs of the forward pass.

00:23:19.030 --> 00:23:20.500
Does this make
sense to everyone?

00:23:20.500 --> 00:23:22.750
That's it for cond gradients.

00:23:22.750 --> 00:23:25.570
SPEAKER 3: Conceptually, what's
the difference between doing

00:23:25.570 --> 00:23:27.738
this and the dead tensor stuff?

00:23:27.738 --> 00:23:28.780
SKYE WANDERMAN-MILNE: Oh.

00:23:28.780 --> 00:23:29.080
Yeah.

00:23:29.080 --> 00:23:29.770
Great question.

00:23:29.770 --> 00:23:34.300
I meant to go over that,
so thank you for asking.

00:23:34.300 --> 00:23:36.070
At a high level,
this is just how

00:23:36.070 --> 00:23:37.780
it works in Control Flow v1.

00:23:37.780 --> 00:23:40.780
The gradient if cond
is another cond.

00:23:40.780 --> 00:23:43.090
You can express that
into low-level ops.

00:23:43.090 --> 00:23:45.110
But the dead tensors
are the big difference.

00:23:45.110 --> 00:23:49.810
So v1 was, kind of, using dead
tensors instead of Optionals.

00:23:49.810 --> 00:23:51.360
And you would just
have that edge

00:23:51.360 --> 00:23:53.110
because there's no
functions [INAUDIBLE]..

00:23:53.110 --> 00:23:54.652
You could just draw
that edge between

00:23:54.652 --> 00:23:56.080
the forward and backward pass.

00:23:56.080 --> 00:23:57.910
And if it's the
untaken branch, you'll

00:23:57.910 --> 00:24:00.845
have a dead tensor
flowing across that edge.

00:24:00.845 --> 00:24:02.470
There's none of this
matching business,

00:24:02.470 --> 00:24:04.270
you just draw the edge.

00:24:04.270 --> 00:24:06.353
SPEAKER 3: The interesting
thing with the Optional

00:24:06.353 --> 00:24:09.040
is that it tells you in the type
of it that it might be that.

00:24:09.040 --> 00:24:11.600
Where in the dead tensor you
had no such information around.

00:24:11.600 --> 00:24:11.930
SKYE WANDERMAN-MILNE: Right.

00:24:11.930 --> 00:24:13.513
SPEAKER 3: So someone
like [INAUDIBLE]

00:24:13.513 --> 00:24:15.937
doesn't have to spend as much
time reverse engineering.

00:24:15.937 --> 00:24:18.520
[INAUDIBLE] exactly what it was
meant to do complicated cases.

00:24:18.520 --> 00:24:20.150
So now what tensors
might be dead or not?

00:24:20.150 --> 00:24:21.608
SPEAKER 3: So this
is, essentially,

00:24:21.608 --> 00:24:25.170
a much more explicit way
of making it clear what it

00:24:25.170 --> 00:24:26.710
be done versus what might now.

00:24:26.710 --> 00:24:28.180
SKYE WANDERMAN-MILNE: It's
kind of like, more complicated.

00:24:28.180 --> 00:24:30.370
Like, this was actually
simpler in Control Flow v2,

00:24:30.370 --> 00:24:32.050
because you're just
like, draw the edge,

00:24:32.050 --> 00:24:35.410
and the executor will take care
of all this dead tensor stuff.

00:24:35.410 --> 00:24:38.140
Yeah, it made the whole system
more complicated as a whole

00:24:38.140 --> 00:24:39.400
to support that.

00:24:39.400 --> 00:24:44.490
OK, so let's move on
to while gradients.

00:24:44.490 --> 00:24:50.830
So again, we're dealing,
now, with concrete tensors.

00:24:50.830 --> 00:24:52.630
So input x, output y.

00:24:52.630 --> 00:24:56.950
They have the same type but
they are different values.

00:24:56.950 --> 00:25:01.360
The body function--
note that I used xi

00:25:01.360 --> 00:25:04.400
because it's run multiple times.

00:25:04.400 --> 00:25:06.280
And each time it
takes, it might be x

00:25:06.280 --> 00:25:07.780
or it might be an
intermediate value

00:25:07.780 --> 00:25:11.380
and outputs the updated
value of y of i.

00:25:11.380 --> 00:25:12.880
Then I drew the
cond function small.

00:25:12.880 --> 00:25:14.200
And I didn't draw as
inputs and outputs,

00:25:14.200 --> 00:25:16.700
because they don't really matter
that much for the gradient,

00:25:16.700 --> 00:25:18.810
but they're there.

00:25:18.810 --> 00:25:20.860
It does have them.

00:25:20.860 --> 00:25:25.630
So same thing for the gradient.

00:25:25.630 --> 00:25:28.390
Very similar to the
cond case, now we're

00:25:28.390 --> 00:25:30.200
dealing with the cotangents.

00:25:30.200 --> 00:25:31.470
Hoping this makes sense.

00:25:31.470 --> 00:25:36.310
We took the gradient of the
body and we're running N times.

00:25:36.310 --> 00:25:40.480
I forgot to draw N,
too, but it's there.

00:25:40.480 --> 00:25:41.770
Same scenario.

00:25:41.770 --> 00:25:42.820
Oh, no.

00:25:42.820 --> 00:25:44.572
What are we going to do?

00:25:44.572 --> 00:25:46.780
We can't just draw this edge
between the two function

00:25:46.780 --> 00:25:47.658
definitions.

00:25:47.658 --> 00:25:50.200
So this time, we don't have to
worry about the matching thing

00:25:50.200 --> 00:25:51.020
anymore.

00:25:51.020 --> 00:25:52.050
Thank goodness.

00:25:52.050 --> 00:25:56.710
We'll add the input
to the grad body

00:25:56.710 --> 00:25:59.150
function and the
grad cond function,

00:25:59.150 --> 00:26:01.780
but that's fine because
we can ignore inputs.

00:26:01.780 --> 00:26:04.570
But we have a new problem,
which is that there's actually

00:26:04.570 --> 00:26:06.383
multiple values of z.

00:26:06.383 --> 00:26:07.800
Because the body
function is going

00:26:07.800 --> 00:26:10.390
to execute multiple times,
there's no guarantee

00:26:10.390 --> 00:26:12.160
that this op that
outputs z is going

00:26:12.160 --> 00:26:15.290
to output the same value
on every iteration.

00:26:15.290 --> 00:26:18.400
So we actually have to
output all the values of z

00:26:18.400 --> 00:26:20.440
from the forward
pass, and we don't

00:26:20.440 --> 00:26:23.770
know how many that
will be until we run it

00:26:23.770 --> 00:26:27.490
and take them as input
to the gradient function.

00:26:27.490 --> 00:26:31.390
So we use stacks,
otherwise known

00:26:31.390 --> 00:26:34.060
as accumulators in
the code sometimes.

00:26:34.060 --> 00:26:36.800
So we're going to
start with an empty--

00:26:36.800 --> 00:26:41.380
we use tensor lists, which are
kind of like tensor arrays,

00:26:41.380 --> 00:26:43.570
but not stateful.

00:26:43.570 --> 00:26:45.640
You can see in these
little function signatures,

00:26:45.640 --> 00:26:47.470
we're going to start
with an empty tensor list

00:26:47.470 --> 00:26:48.762
that we pass through the while.

00:26:48.762 --> 00:26:50.440
And then in the
forward pass, we're

00:26:50.440 --> 00:26:55.210
going to push values onto
that stack, or that list.

00:26:55.210 --> 00:26:58.502
And since it's stateless,
you take the list

00:26:58.502 --> 00:27:00.460
in as input and the value
you want to add to it

00:27:00.460 --> 00:27:04.780
and it, conceptually, returns
you a new list that has

00:27:04.780 --> 00:27:06.790
that new element added to it.

00:27:06.790 --> 00:27:08.440
Under the hood it
doesn't actually

00:27:08.440 --> 00:27:12.180
have to make all
these copies, I hope.

00:27:12.180 --> 00:27:14.020
Similarly in the backwards.

00:27:14.020 --> 00:27:18.370
So then we're going to
keep pushing values,

00:27:18.370 --> 00:27:21.520
outputting these new lists,
and keep pushing to them

00:27:21.520 --> 00:27:24.010
until we get the full list
with all the values in it.

00:27:24.010 --> 00:27:25.640
That's output from
the while loop.

00:27:25.640 --> 00:27:27.140
Actually, I have a
picture for this.

00:27:31.553 --> 00:27:33.470
So I guess the point is
that, in the backwards

00:27:33.470 --> 00:27:36.170
pass you just pop,
opposite of push,

00:27:36.170 --> 00:27:38.780
to get the value out again.

00:27:38.780 --> 00:27:42.330
And so, this is a
little bit complicated.

00:27:42.330 --> 00:27:45.440
But you start with the
empty list as input,

00:27:45.440 --> 00:27:48.110
now these lists are
actually loop variables.

00:27:48.110 --> 00:27:52.550
So the stateless tensor list
works quite nicely with this,

00:27:52.550 --> 00:27:54.880
because the loop
variable is going

00:27:54.880 --> 00:27:57.380
to have whatever has accumulated
so far as input to the body

00:27:57.380 --> 00:27:57.880
function.

00:27:57.880 --> 00:28:02.180
And it adds the
new z and outputs

00:28:02.180 --> 00:28:03.530
that as the updated version.

00:28:03.530 --> 00:28:05.030
And so the final
list is going to be

00:28:05.030 --> 00:28:08.330
the full list, which you pass
into the gradient function.

00:28:08.330 --> 00:28:11.210
It's going to do the same
thing, except popping to pass,

00:28:11.210 --> 00:28:13.670
to get that raw value of z.

00:28:13.670 --> 00:28:16.130
And then finally, the list
should be empty at the end.

00:28:16.130 --> 00:28:17.870
And then, since it's
a loop variable,

00:28:17.870 --> 00:28:19.490
we end up outputting
an empty list,

00:28:19.490 --> 00:28:21.350
but we don't actually
need that output.

00:28:21.350 --> 00:28:22.430
That's just how it works.

00:28:22.430 --> 00:28:23.390
SPEAKER 2: I have a question.

00:28:23.390 --> 00:28:24.310
SKYE WANDERMAN-MILNE: Yeah.

00:28:24.310 --> 00:28:26.477
SPEAKER 2: Are you saying
the gradient values always

00:28:26.477 --> 00:28:27.060
[INAUDIBLE]?

00:28:31.920 --> 00:28:35.060
SKYE WANDERMAN-MILNE: It's only
when you when you need them.

00:28:35.060 --> 00:28:37.050
SPEAKER 2: It's just
always [INAUDIBLE]..

00:28:37.050 --> 00:28:38.440
OK.

00:28:38.440 --> 00:28:38.940
Thank you.

00:28:38.940 --> 00:28:39.580
SKYE WANDERMAN-MILNE: Yeah.

00:28:39.580 --> 00:28:40.280
That's a good question.

00:28:40.280 --> 00:28:41.780
SPEAKER 4: Now you
could [INAUDIBLE]

00:28:41.780 --> 00:28:43.790
in the normal TensorFlow
graph probably

00:28:43.790 --> 00:28:45.770
is able to remove them.

00:28:45.770 --> 00:28:45.990
SKYE WANDERMAN-MILNE:
Yeah, that's

00:28:45.990 --> 00:28:47.270
the way it actually used to do.

00:28:47.270 --> 00:28:49.437
Although, that's a little
weird through functions so

00:28:49.437 --> 00:28:51.003
we changed it.

00:28:51.003 --> 00:28:52.170
SPEAKER 3: Another question.

00:28:52.170 --> 00:28:54.227
Does this imply that
in your while loop,

00:28:54.227 --> 00:28:55.810
your memory consumption
is, basically,

00:28:55.810 --> 00:29:00.280
linear in the number of
variations you go through?

00:29:00.280 --> 00:29:03.600
SKYE WANDERMAN-MILNE: Yeah, if
you have a gradient like this.

00:29:03.600 --> 00:29:05.110
That's some future work.

00:29:05.110 --> 00:29:07.890
I would love to see
doing re-materialization,

00:29:07.890 --> 00:29:10.680
or check-pointing, I think
it's called in the literature.

00:29:10.680 --> 00:29:13.920
But we don't do that.

00:29:13.920 --> 00:29:16.460
SPEAKER 2: Can explain
again, in the if, why

00:29:16.460 --> 00:29:20.190
can't you draw a line
just from the original--

00:29:20.190 --> 00:29:22.500
SKYE WANDERMAN-MILNE: Oh, yeah.

00:29:22.500 --> 00:29:26.070
The blue boxes are
function definition.

00:29:26.070 --> 00:29:29.730
And then the while op is
going to call that function

00:29:29.730 --> 00:29:30.300
many times.

00:29:32.880 --> 00:29:37.052
So it's sort of like, if you're
writing two functions in Python

00:29:37.052 --> 00:29:38.510
and they're not
nested or anything,

00:29:38.510 --> 00:29:40.320
they're just side by side.

00:29:40.320 --> 00:29:42.930
You can't take an intermediate
variable from one function

00:29:42.930 --> 00:29:44.110
and use it in another one.

00:29:44.110 --> 00:29:46.152
It's going to be like, I
don't know what this is.

00:29:46.152 --> 00:29:48.510
You have to output
it then have it

00:29:48.510 --> 00:29:50.150
as input to the other function.

00:29:50.150 --> 00:29:52.650
Or at least in TensorFlow we
don't have closures or anything

00:29:52.650 --> 00:29:53.350
fancy like that.

00:29:53.350 --> 00:29:55.080
So that's how we do it.

00:29:55.080 --> 00:29:57.168
Does that make sense?

00:29:57.168 --> 00:29:59.450
SPEAKER 2: Kind of.

00:29:59.450 --> 00:30:03.770
SPEAKER 3: The value for
a particular execution

00:30:03.770 --> 00:30:05.890
of a function of particular
intermediate value

00:30:05.890 --> 00:30:07.590
of a particular
function execution

00:30:07.590 --> 00:30:10.610
doesn't have a name that
can be addressed in order--

00:30:10.610 --> 00:30:13.680
And if it had a name,
it would greatly

00:30:13.680 --> 00:30:16.803
complicate the lifetime issues.

00:30:16.803 --> 00:30:18.220
We wouldn't be
able to [INAUDIBLE]

00:30:18.220 --> 00:30:19.470
intermediate
[INAUDIBLE] functions.

00:30:19.470 --> 00:30:20.370
SKYE WANDERMAN-MILNE:
Or maybe another way

00:30:20.370 --> 00:30:21.787
is that these
function definitions

00:30:21.787 --> 00:30:23.280
aren't actually in the graph.

00:30:23.280 --> 00:30:26.850
I draw them as if they are,
but they're off to the side.

00:30:26.850 --> 00:30:28.430
All you see are the while ops.

00:30:28.430 --> 00:30:30.780
And then when you call the
function, then you see that.

00:30:30.780 --> 00:30:32.880
But you only see
it for that call.

00:30:32.880 --> 00:30:41.140
So it's like this z op in
here doesn't exist out here

00:30:41.140 --> 00:30:43.980
in the main graph where this
gradient while op can see it,

00:30:43.980 --> 00:30:46.191
or in this other
function definition.

00:30:49.280 --> 00:30:53.850
Oh, and to compare to
Control Flow v1 again,

00:30:53.850 --> 00:30:54.980
same general idea.

00:30:54.980 --> 00:30:58.820
These while ops could be the
whole mess of low-level ops

00:30:58.820 --> 00:31:03.980
and, due to some true while
loops, represent it that way.

00:31:03.980 --> 00:31:06.800
The big difference, this
time, is in the stacks.

00:31:06.800 --> 00:31:09.427
They use the old resource
back tensor arrays,

00:31:09.427 --> 00:31:10.260
which were stateful.

00:31:10.260 --> 00:31:12.680
SPEAKER 4: We actually use the
resource [INAUDIBLE] stack.

00:31:12.680 --> 00:31:14.000
SKYE WANDERMAN-MILNE:
Oh, you're right.

00:31:14.000 --> 00:31:14.542
You're right.

00:31:14.542 --> 00:31:15.820
SPEAKER 4: Separate nests.

00:31:15.820 --> 00:31:16.430
SKYE WANDERMAN-MILNE: OK, yeah.

00:31:16.430 --> 00:31:17.840
But they were
stateful, is the point.

00:31:17.840 --> 00:31:19.340
So they were
actually just inputs.

00:31:19.340 --> 00:31:20.340
They weren't outputs.

00:31:20.340 --> 00:31:24.200
And you just modify that state.

00:31:24.200 --> 00:31:26.300
One big disadvantage of
this was that you couldn't

00:31:26.300 --> 00:31:29.960
take higher-order derivatives
because you would exhaust

00:31:29.960 --> 00:31:31.630
the stack once,
and it's stateful

00:31:31.630 --> 00:31:33.560
and you can't get
it back anymore.

00:31:33.560 --> 00:31:35.485
Whereas these, it's
this full list.

00:31:35.485 --> 00:31:36.860
Because it's a
stateless thing, I

00:31:36.860 --> 00:31:38.693
can pass it to another
while op, no problem.

00:31:42.760 --> 00:31:45.790
So coming back to
Control Flow v2.

00:31:45.790 --> 00:31:48.408
Let's recap what's
good and bad about it.

00:31:48.408 --> 00:31:50.200
So now we can take
higher-order derivatives

00:31:50.200 --> 00:31:51.970
because it's very simple.

00:31:51.970 --> 00:31:54.450
The gradient code, when
it's looking at an if op,

00:31:54.450 --> 00:31:56.200
it doesn't know if
that if op was actually

00:31:56.200 --> 00:31:58.110
the first derivative
of some other if op.

00:31:58.110 --> 00:31:59.290
They're are all the same.

00:31:59.290 --> 00:32:02.440
Inputs and outputs
just are normal.

00:32:02.440 --> 00:32:05.440
It's much easier to
convert to the XLA

00:32:05.440 --> 00:32:09.970
if and while ops and
downstream TPU integration.

00:32:09.970 --> 00:32:12.630
Graph construction
logic, I hope is simpler.

00:32:12.630 --> 00:32:15.260
Take a look for yourself.

00:32:15.260 --> 00:32:17.020
So besides being
easier to maintain,

00:32:17.020 --> 00:32:20.770
this lets us give
better error messages,

00:32:20.770 --> 00:32:23.600
and hopefully there'll
be fewer bugs.

00:32:23.600 --> 00:32:24.100
OK.

00:32:24.100 --> 00:32:26.740
So now assuming that we
just run the functional

00:32:26.740 --> 00:32:30.580
ops, even though I said
we don't, assume we do.

00:32:30.580 --> 00:32:32.080
The execution could
be much simpler,

00:32:32.080 --> 00:32:33.700
because we don't
have dead tensors

00:32:33.700 --> 00:32:36.340
or because we use Optionals now.

00:32:36.340 --> 00:32:40.570
And we don't have frames because
it's managed by the while op.

00:32:40.570 --> 00:32:42.520
But the disadvantage
of running these ops

00:32:42.520 --> 00:32:46.420
is that they aren't as
performant for a number

00:32:46.420 --> 00:32:49.370
of reasons listed there.

00:32:49.370 --> 00:32:51.670
So we could fix this
with the functional ops.

00:32:51.670 --> 00:32:54.220
And it would make sense to do
this because a lot of these

00:32:54.220 --> 00:32:56.625
also apply to just regular
function calls, which

00:32:56.625 --> 00:32:59.320
are kind of a big deal now.

00:32:59.320 --> 00:33:05.620
But for now, we decided to
just take the functional op.

00:33:05.620 --> 00:33:08.150
So right before you run it--
so you've already constructed

00:33:08.150 --> 00:33:10.970
the graph, you're
ready to run it--

00:33:10.970 --> 00:33:13.850
we're going to convert it
back into the old low-level

00:33:13.850 --> 00:33:16.520
representation.

00:33:16.520 --> 00:33:18.410
So now we get rid
of the disadvantages

00:33:18.410 --> 00:33:21.630
because we're, hopefully,
just running the same thing.

00:33:21.630 --> 00:33:25.250
But we also don't get our
simpler execution because we're

00:33:25.250 --> 00:33:27.170
still running the old thing.

00:33:27.170 --> 00:33:29.270
So we call this
lowering, because they're

00:33:29.270 --> 00:33:34.280
sort of lowering to this
more low-level form.

00:33:34.280 --> 00:33:37.185
This was, basically,
a staging trick

00:33:37.185 --> 00:33:39.560
so that we can do all the
graph construction stuff, which

00:33:39.560 --> 00:33:42.110
is taking quite some time,
without having to worry

00:33:42.110 --> 00:33:44.080
about the execution as much.

00:33:44.080 --> 00:33:46.650
Because there were
still some issues.

00:33:46.650 --> 00:33:48.740
It's very similar to
function in-lining.

00:33:48.740 --> 00:33:54.060
An if op and a while op are kind
of very fancy function calls.

00:33:54.060 --> 00:33:56.270
And so this is how
you in-line them,

00:33:56.270 --> 00:34:00.950
with these low-level
level dataflow operators.

00:34:00.950 --> 00:34:04.370
And so it runs with in-lining
before anything else happens,

00:34:04.370 --> 00:34:05.870
and this is so we
can take advantage

00:34:05.870 --> 00:34:10.830
of any downstream optimization
or placement or whatever.

00:34:10.830 --> 00:34:12.330
In the case of
Control Flow, we want

00:34:12.330 --> 00:34:17.150
it to work the same as it did
before in Control Flow v1.

00:34:17.150 --> 00:34:19.489
And I think Eugene is
fixing this all up,

00:34:19.489 --> 00:34:21.409
so this is actually true now.

00:34:21.409 --> 00:34:23.790
As of, like, last week.

00:34:23.790 --> 00:34:26.852
SPEAKER 5: So this converting
will be removed eventually?

00:34:26.852 --> 00:34:29.060
SKYE WANDERMAN-MILNE: I
would love to see it removed.

00:34:29.060 --> 00:34:29.750
Oh, yeah.

00:34:29.750 --> 00:34:32.420
So right now we
in-line everything,

00:34:32.420 --> 00:34:34.850
including function calls,
because similar story

00:34:34.850 --> 00:34:37.880
for functions, it makes
a lot of things easier.

00:34:37.880 --> 00:34:41.690
I hope that we don't
depend on this forever.

00:34:41.690 --> 00:34:43.340
That we sort of
do try to make it

00:34:43.340 --> 00:34:45.710
so function calls are just
as performant and as good not

00:34:45.710 --> 00:34:46.639
in-line.

00:34:46.639 --> 00:34:50.042
Because it's the same
for Control Flow.

00:34:50.042 --> 00:34:51.750
If we always assume
everything's in-line,

00:34:51.750 --> 00:34:53.417
then we're never going
to be able to get

00:34:53.417 --> 00:34:55.747
our simpler execution and
just run the functional ops.

00:34:55.747 --> 00:34:57.830
Because they're very, very
similar function calls,

00:34:57.830 --> 00:34:59.430
they have the same problems.

00:34:59.430 --> 00:35:00.920
So if you fix it
for functions it's

00:35:00.920 --> 00:35:06.200
not a huge step to, then,
fix it for Control Flow.

00:35:06.200 --> 00:35:09.080
Where are we at with
Control Flow v2?

00:35:09.080 --> 00:35:10.580
It's still under development.

00:35:10.580 --> 00:35:13.850
There's bugs and features
that need to be implemented.

00:35:13.850 --> 00:35:19.980
But it's basically on in tf 2.0,
if you're using pure 2.0 code.

00:35:19.980 --> 00:35:23.360
So remember Eager, doing his
own thing, just use Python.

00:35:23.360 --> 00:35:26.555
And then, Control Flow v2 is
always on in tf dot functions.

00:35:26.555 --> 00:35:28.180
There's no way to
get old Control Flow.

00:35:33.390 --> 00:35:36.840
If you want to run new Control
Flow in either 1.x code

00:35:36.840 --> 00:35:39.920
or you're using a
compact dot v1 dot graph,

00:35:39.920 --> 00:35:42.450
those still use the
old Control Flow,

00:35:42.450 --> 00:35:45.520
you can use this environment
variable to turn it on.

00:35:45.520 --> 00:35:48.997
So now when people ping
me in and are like,

00:35:48.997 --> 00:35:50.580
I have this horrible
Control Flow bug.

00:35:50.580 --> 00:35:52.000
I'm like, try the
environment variable.

00:35:52.000 --> 00:35:52.980
And sometimes it fixes it.

00:35:52.980 --> 00:35:55.287
Or sometimes it at least
gives an easier to debug error

00:35:55.287 --> 00:35:55.787
message.

00:35:58.530 --> 00:36:01.860
Unfortunately, I
would love to have

00:36:01.860 --> 00:36:04.530
realized the glorious future,
where it's all new Control

00:36:04.530 --> 00:36:05.040
Flow.

00:36:05.040 --> 00:36:06.332
Old Control Flow doesn't exist.

00:36:06.332 --> 00:36:07.860
We can delete that code.

00:36:07.860 --> 00:36:10.260
I don't know if it makes
sense to do the work

00:36:10.260 --> 00:36:13.530
to make it so we can
turn it on in 1.x code

00:36:13.530 --> 00:36:15.120
because there's a
few big blockers.

00:36:15.120 --> 00:36:19.110
Namely, functions don't
work with ref variables.

00:36:19.110 --> 00:36:20.940
And so by extension,
these functional ops

00:36:20.940 --> 00:36:22.310
don't work with ref variables.

00:36:22.310 --> 00:36:25.110
That would be a lot
of work to implement.

00:36:25.110 --> 00:36:29.820
And the question that you asked
about how we add the gradient

00:36:29.820 --> 00:36:31.530
outputs when you
request a gradient,

00:36:31.530 --> 00:36:33.390
only when they're needed,
which it will only

00:36:33.390 --> 00:36:36.570
know after you build
the gradient graph

00:36:36.570 --> 00:36:38.910
and see what incoming
edges you have.

00:36:38.910 --> 00:36:40.600
This actually breaks sessions.

00:36:40.600 --> 00:36:43.950
Sessions do not like it when you
add inputs and outputs to ops.

00:36:43.950 --> 00:36:46.020
And will potentially make
your session unusable.

00:36:46.020 --> 00:36:48.210
You'll have to
make a new session.

00:36:48.210 --> 00:36:50.595
So in 2.0 we don't
have sessions, great.

00:36:50.595 --> 00:36:52.220
But in 1.x we definitely
have sessions.

00:36:55.420 --> 00:36:57.070
Another little note.

00:36:57.070 --> 00:36:59.410
In addition to Control
Flow V2, there's

00:36:59.410 --> 00:37:02.990
a new effort to
re-implement tensor arrays.

00:37:02.990 --> 00:37:06.170
And I sort of hinted
at this by incorrectly

00:37:06.170 --> 00:37:09.670
stating the old tensor array as
stacks but it's the same idea.

00:37:09.670 --> 00:37:13.077
Tensor arrays were these
resource back stateful things.

00:37:13.077 --> 00:37:14.660
Now we're going to
make tensor arrays.

00:37:14.660 --> 00:37:16.118
It's still the same
API, so nothing

00:37:16.118 --> 00:37:18.110
should change for the
user, but under the hood,

00:37:18.110 --> 00:37:20.770
we're going to use immutable
tensor lists, which are

00:37:20.770 --> 00:37:23.780
variants instead of resources.

00:37:23.780 --> 00:37:26.140
And so you get
higher-order derivatives,

00:37:26.140 --> 00:37:28.180
it's easier to reason
about something

00:37:28.180 --> 00:37:30.620
that's dataflow style instead
of stateful in our dataflow

00:37:30.620 --> 00:37:31.870
graphs.

00:37:31.870 --> 00:37:32.440
It's nicer.

00:37:35.430 --> 00:37:40.490
And then in particular, an
area of active development

00:37:40.490 --> 00:37:47.810
is that we do need to make these
new tensor arrays work in XLA.

00:37:47.810 --> 00:37:51.103
So this is kind of annoying,
because we've kept saying, oh,

00:37:51.103 --> 00:37:52.520
the new Control
Flow [INAUDIBLE],,

00:37:52.520 --> 00:37:54.090
it's going to make XLA so easy.

00:37:54.090 --> 00:37:55.090
It's just going to work.

00:37:55.090 --> 00:37:57.500
But we do have to
implement this one thing.

00:37:57.500 --> 00:37:58.830
[? Sarab's ?] working on this.

00:37:58.830 --> 00:38:00.870
I think it's almost there.

00:38:00.870 --> 00:38:01.370
We'll see.

00:38:01.370 --> 00:38:02.412
SPEAKER 4: Getting there.

00:38:02.412 --> 00:38:03.170
Question.

00:38:03.170 --> 00:38:05.570
So is it true that
TensorFlow [INAUDIBLE]

00:38:05.570 --> 00:38:07.377
where you only use
the [INAUDIBLE]??

00:38:07.377 --> 00:38:08.460
SKYE WANDERMAN-MILNE: Yes.

00:38:08.460 --> 00:38:12.207
Yeah, so it's
maybe theoretically

00:38:12.207 --> 00:38:14.540
different from Control Flow,
because it's tensor arrays.

00:38:14.540 --> 00:38:17.300
But tensor arrays are so
tightly linked to Control Flow.

00:38:17.300 --> 00:38:19.310
And we only support
the new tensor arrays

00:38:19.310 --> 00:38:20.810
in new Control Flow
because we don't

00:38:20.810 --> 00:38:23.120
want to deal with
the stateful thing.

00:38:23.120 --> 00:38:25.250
SPEAKER 2: You don't know
what tensor array is.

00:38:25.250 --> 00:38:28.350
Usually when you do
Control Flow and it models,

00:38:28.350 --> 00:38:30.920
you have something like an
RNN, that computes something

00:38:30.920 --> 00:38:31.730
for [INAUDIBLE].

00:38:31.730 --> 00:38:34.520
And you often want to
take a single tensor that

00:38:34.520 --> 00:38:37.297
represents the results of
all time steps together.

00:38:37.297 --> 00:38:38.880
And tensor array is
the data structure

00:38:38.880 --> 00:38:40.935
that lets you do that.

00:38:40.935 --> 00:38:42.060
SKYE WANDERMAN-MILNE: Yeah.

00:38:42.060 --> 00:38:44.390
I don't think there's too
much use for tensor array

00:38:44.390 --> 00:38:47.420
outside of while loops,
I'm sure I would stand

00:38:47.420 --> 00:38:50.960
corrected if I looked into it.

00:38:50.960 --> 00:38:53.848
So these are some details
on what's going on here.

00:38:56.480 --> 00:38:57.642
That's all I have.

00:38:57.642 --> 00:38:59.350
I'm going to end on
this slide so you can

00:38:59.350 --> 00:39:00.340
look at the beautiful picture.

00:39:00.340 --> 00:39:02.390
And I guess we have plenty
of time for questions.

00:39:02.390 --> 00:39:06.130
So what was your Control
Flow v1 question?

00:39:06.130 --> 00:39:10.213
SPEAKER 3: How does it work
with the branches [INAUDIBLE]??

00:39:10.213 --> 00:39:11.880
SKYE WANDERMAN-MILNE:
Oh, good question.

00:39:11.880 --> 00:39:13.505
So this is when you
have a tf dot cond,

00:39:13.505 --> 00:39:16.150
remember just takes lambdas and
captures everything by closure.

00:39:16.150 --> 00:39:17.900
So you could just not
close over anything.

00:39:17.900 --> 00:39:20.662
Like, return one or two.

00:39:20.662 --> 00:39:23.120
SPEAKER 1: Or like, it's a
sourceless op like [INAUDIBLE]..

00:39:23.120 --> 00:39:24.245
SKYE WANDERMAN-MILNE: Yeah.

00:39:26.020 --> 00:39:28.170
It uses the predicate.

00:39:28.170 --> 00:39:31.860
It wires together all the
dataflow using the predicate.

00:39:31.860 --> 00:39:35.170
And in particular,
you can also have

00:39:35.170 --> 00:39:37.390
a cond that doesn't
return anything,

00:39:37.390 --> 00:39:39.620
it just has side effects.

00:39:39.620 --> 00:39:42.340
And I think in
Control Flow v1, it

00:39:42.340 --> 00:39:44.645
will return to predicate value.

00:39:44.645 --> 00:39:46.270
I thinl it does that
in Control Flow v2

00:39:46.270 --> 00:39:49.081
because I wanted to test
the pass in both cases.

00:39:49.081 --> 00:39:50.900
But it's a little arbitrary.

00:39:50.900 --> 00:39:52.540
SPEAKER 4: So the
way to do this is

00:39:52.540 --> 00:39:55.688
you have ops that have a control
dependency on something that

00:39:55.688 --> 00:39:56.605
depends on the Switch.

00:40:01.460 --> 00:40:04.940
Because [INAUDIBLE] propagates
through [INAUDIBLE] as well.

00:40:04.940 --> 00:40:07.570
So this is how it's actually
implemented in Control Flow v1.

00:40:07.570 --> 00:40:08.695
SKYE WANDERMAN-MILNE: Yeah.

00:40:08.695 --> 00:40:11.490
SPEAKER 1: Well, it can't
depend on the Switch.

00:40:11.490 --> 00:40:12.990
It has to depend
on like one output.

00:40:12.990 --> 00:40:13.180
SPEAKER 4: Yeah.

00:40:13.180 --> 00:40:15.070
So you have a Switch
of the predicate.

00:40:15.070 --> 00:40:16.570
And on each side
of that [INAUDIBLE]

00:40:16.570 --> 00:40:18.110
that takes the predicate twice.

00:40:18.110 --> 00:40:20.440
Then you have an identity
op on each branch.

00:40:20.440 --> 00:40:23.140
And every op that's inside
one of the Switch branches

00:40:23.140 --> 00:40:26.708
has a control dependency on
that corresponding identity.

00:40:26.708 --> 00:40:28.750
So because, then, this
propagates through control

00:40:28.750 --> 00:40:31.383
edges, it makes things work.

00:40:31.383 --> 00:40:32.550
SPEAKER 1: That makes sense.

00:40:32.550 --> 00:40:33.980
SKYE WANDERMAN-MILNE: That's a
part of why we were able to do

00:40:33.980 --> 00:40:34.732
[INAUDIBLE].

00:40:37.078 --> 00:40:38.120
There's a lot of storage.

00:40:40.840 --> 00:40:41.440
Yeah?

00:40:41.440 --> 00:40:45.340
SPEAKER 2: So when you
described the graph modification

00:40:45.340 --> 00:40:50.680
for taking gradients of if, when
does this modification happen?

00:40:50.680 --> 00:40:54.220
Does it happen when
you construct the if op

00:40:54.220 --> 00:40:56.188
or when you're taking gradients?

00:40:56.188 --> 00:40:57.730
SKYE WANDERMAN-MILNE:
Great question.

00:40:57.730 --> 00:40:59.380
It happens when you
take the gradient.

00:40:59.380 --> 00:41:00.380
SPEAKER 2: The gradient.

00:41:00.380 --> 00:41:01.280
So for those--

00:41:01.280 --> 00:41:03.613
SPEAKER 3: Does that depend
on whether you're using tape

00:41:03.613 --> 00:41:05.090
gradients or tf dot gradients?

00:41:05.090 --> 00:41:06.358
SKYE WANDERMAN-MILNE: No.

00:41:06.358 --> 00:41:07.900
SPEAKER 2: We could
[INAUDIBLE] early

00:41:07.900 --> 00:41:09.192
if you're doing tape gradients.

00:41:09.192 --> 00:41:10.030
We currently do not.

00:41:10.030 --> 00:41:11.300
SKYE WANDERMAN-MILNE: Yeah.

00:41:11.300 --> 00:41:14.530
SPEAKER 4: So that means for
those function arguments,

00:41:14.530 --> 00:41:17.300
or functional attributes, you
cannot draw lines between two,

00:41:17.300 --> 00:41:19.120
but you can modify one.

00:41:19.120 --> 00:41:19.510
SKYE WANDERMAN-MILNE:
Yeah, you can

00:41:19.510 --> 00:41:21.460
modify them to add
inputs and outputs, which

00:41:21.460 --> 00:41:23.940
you're not really supposed
to do with sessions.

00:41:23.940 --> 00:41:25.030
But we do it.

00:41:25.030 --> 00:41:28.665
The reason we do it when you
request a gradient is that, a,

00:41:28.665 --> 00:41:30.040
if you never take
the gradient we

00:41:30.040 --> 00:41:34.082
don't want to add extra stuff,
although it could get pruned.

00:41:34.082 --> 00:41:35.790
SPEAKER 4: You want
to look [INAUDIBLE]..

00:41:35.790 --> 00:41:38.332
SKYE WANDERMAN-MILNE: It makes
your graph look nice at least,

00:41:38.332 --> 00:41:40.100
to not have all
the extra outputs.

00:41:40.100 --> 00:41:43.780
And also, you don't
know which intermediates

00:41:43.780 --> 00:41:46.720
you're going to need until
you build the gradient graph.

00:41:46.720 --> 00:41:48.220
So if we did it
with the tape, we

00:41:48.220 --> 00:41:50.753
could say, oh,
presumably because you're

00:41:50.753 --> 00:41:52.170
running with a
tape, you are going

00:41:52.170 --> 00:41:54.180
to want to take the
gradient at some point.

00:41:54.180 --> 00:41:55.805
SPEAKER 4: We can
actually ask the tape

00:41:55.805 --> 00:41:57.860
if the tape is
going to integrate

00:41:57.860 --> 00:41:58.960
into one of those outputs.

00:41:58.960 --> 00:41:59.980
We can't answer their questions.

00:41:59.980 --> 00:42:01.180
SKYE WANDERMAN-MILNE: So
then we could proactively

00:42:01.180 --> 00:42:02.680
create the gradient
at the same time

00:42:02.680 --> 00:42:06.250
as you create the forward pass
and add the outputs there,

00:42:06.250 --> 00:42:07.180
all at once.

00:42:07.180 --> 00:42:09.310
But since we have
the two code pass,

00:42:09.310 --> 00:42:11.073
we just do it the same
in a two code pass.

00:42:11.073 --> 00:42:12.490
Because with tf
doc gradients, you

00:42:12.490 --> 00:42:13.990
have no idea if
you're gonna call it

00:42:13.990 --> 00:42:16.210
or not until it happens.

00:42:16.210 --> 00:42:18.765
That's a good question.

00:42:18.765 --> 00:42:20.640
Functions work the same
way too, because they

00:42:20.640 --> 00:42:21.965
have like a similar--

00:42:21.965 --> 00:42:23.340
if you just have
a function call,

00:42:23.340 --> 00:42:25.200
you'll have the same
thing with intermediates

00:42:25.200 --> 00:42:26.950
and you'll have to add
inputs and outputs.

00:42:26.950 --> 00:42:28.980
So we're back in
Control Flow v1, right?

00:42:28.980 --> 00:42:32.260
This is what it looks
like, this stuff.

00:42:32.260 --> 00:42:35.850
What if you want to run your
branch functions or your body

00:42:35.850 --> 00:42:39.310
or whatever on multiple devices?

00:42:39.310 --> 00:42:42.210
So I don't totally
understand this myself.

00:42:42.210 --> 00:42:45.030
It's going to be brief.

00:42:45.030 --> 00:42:48.100
Cond, it's pretty simple.

00:42:48.100 --> 00:42:53.400
You just do it like
normal, I guess.

00:42:53.400 --> 00:42:56.070
You add the sends and
receives, dead tensors

00:42:56.070 --> 00:42:57.480
can flow through these.

00:42:57.480 --> 00:43:00.150
So this is why you
need the dead tensors.

00:43:00.150 --> 00:43:04.610
Because for the untaken
branch, you basically

00:43:04.610 --> 00:43:08.300
need to tell other
device, this isn't taken.

00:43:08.300 --> 00:43:10.060
Stop waiting for inputs on this.

00:43:10.060 --> 00:43:11.763
So you can shut
down or whatever.

00:43:11.763 --> 00:43:13.430
SPEAKER 4: Another,
we could have chosen

00:43:13.430 --> 00:43:15.150
to send the predicate instead.

00:43:15.150 --> 00:43:17.800
But was a simple modification
of the existing TensorFlow

00:43:17.800 --> 00:43:19.197
that had a huge cost.

00:43:19.197 --> 00:43:20.780
If I had chosen to
send the predicate,

00:43:20.780 --> 00:43:23.570
we wouldn't need so much of
that tensor propagation and all

00:43:23.570 --> 00:43:24.737
the bugs associated with it.

00:43:24.737 --> 00:43:26.945
SKYE WANDERMAN-MILNE: Dead
tensors are kind of crazy.

00:43:26.945 --> 00:43:29.690
In really big graphs, you will
spend time just propagating

00:43:29.690 --> 00:43:33.090
all the dead tensors, and
send data across the network,

00:43:33.090 --> 00:43:33.590
or whatever.

00:43:36.208 --> 00:43:37.250
It's one of those things.

00:43:37.250 --> 00:43:38.480
We added all this
stuff and now this

00:43:38.480 --> 00:43:39.720
is very conceptually simple.

00:43:39.720 --> 00:43:41.137
You just add the
send and receive.

00:43:41.137 --> 00:43:43.860
It just works.

00:43:43.860 --> 00:43:45.840
Can we do the same
thing for while loops?

00:43:45.840 --> 00:43:47.580
Just add the sends and receives.

00:43:47.580 --> 00:43:49.530
This time it's going
to be in a loop.

00:43:49.530 --> 00:43:51.420
Seems fine.

00:43:51.420 --> 00:43:53.560
It's not fine.

00:43:53.560 --> 00:43:57.090
The problem is that
this device doesn't

00:43:57.090 --> 00:44:01.450
know that this op is supposed
to be run multiple times.

00:44:01.450 --> 00:44:04.630
I guess we didn't forward
the frame information.

00:44:04.630 --> 00:44:07.537
SPEAKER 3: It doesn't know
how many times it should run.

00:44:07.537 --> 00:44:09.120
SKYE WANDERMAN-MILNE:
Well, it's going

00:44:09.120 --> 00:44:11.762
to run once or like 0
times, then you'll have--

00:44:11.762 --> 00:44:13.220
or maybe the dead
tensor will work.

00:44:13.220 --> 00:44:14.610
But if you run it
once, it's just

00:44:14.610 --> 00:44:17.110
going to immediately shut down
because it thinks that it has

00:44:17.110 --> 00:44:20.620
to run once, like a regular op.

00:44:20.620 --> 00:44:38.360
So the solution, you, basically,
build a tiny little while

00:44:38.360 --> 00:44:40.660
loop on the other device.

00:44:40.660 --> 00:44:43.760
And so you can see
there's no real data going

00:44:43.760 --> 00:44:45.420
through this computation.

00:44:45.420 --> 00:44:49.820
But it's just used through
carefully placed control

00:44:49.820 --> 00:44:55.153
dependencies to drive this
op as many times as you need.

00:44:55.153 --> 00:44:57.320
So this is like a whole
little while loop built just

00:44:57.320 --> 00:44:59.540
to run this op n times.

00:44:59.540 --> 00:45:03.603
This while loop is indirectly
driven by the real one.

00:45:03.603 --> 00:45:05.270
SPEAKER 3: It's driven
by the predicate.

00:45:05.270 --> 00:45:06.080
SKYE WANDERMAN-MILNE: Yeah.

00:45:06.080 --> 00:45:06.950
Right, exactly.

00:45:06.950 --> 00:45:09.117
You can see that this guy
does not have a predicate.

00:45:09.117 --> 00:45:11.617
SPEAKER 4: So we're essentially
sending the predicate around

00:45:11.617 --> 00:45:14.075
for the while loop case but
not doing it for the cond case.

00:45:14.075 --> 00:45:16.534
SKYE WANDERMAN-MILNE: And we
build a little tiny while loop

00:45:16.534 --> 00:45:17.830
to actually use that predicate.

00:45:17.830 --> 00:45:19.820
SPEAKER 4: And essentially, if
we wanted to partition into two

00:45:19.820 --> 00:45:21.080
ops, we would have
to build something

00:45:21.080 --> 00:45:23.090
like this for both the
cond and [INAUDIBLE]..

00:45:23.090 --> 00:45:25.370
Or it would at least
look simpler, I think.

00:45:25.370 --> 00:45:30.795
SPEAKER 1: Well, the control
could be centralized.

00:45:30.795 --> 00:45:32.670
SPEAKER 4: Well, you
could send the predicate

00:45:32.670 --> 00:45:35.310
to other places, yes.

00:45:35.310 --> 00:45:37.040
SPEAKER 1: [INAUDIBLE]
execution, yeah.

00:45:37.040 --> 00:45:38.060
SKYE WANDERMAN-MILNE: Yeah.

00:45:38.060 --> 00:45:39.980
SPEAKER 4: You would need a
while loop [INAUDIBLE] device,

00:45:39.980 --> 00:45:42.650
but the predicate computation
only needs to happen once.

00:45:42.650 --> 00:45:42.710
SKYE WANDERMAN-MILNE: Do we?

00:45:42.710 --> 00:45:44.450
Because we have
multi-device functions,

00:45:44.450 --> 00:45:46.760
you could just call that
multiple times, right?

00:45:46.760 --> 00:45:47.427
SPEAKER 4: Yeah.

00:45:47.427 --> 00:45:48.160
I mean, sure.

00:45:48.160 --> 00:45:50.660
SKYE WANDERMAN-MILNE: You won't
get like parallel iterations

00:45:50.660 --> 00:45:53.210
and everything.

00:45:53.210 --> 00:45:57.290
So that's distribution.

00:45:57.290 --> 00:46:01.240
SPEAKER 6: I'm glad
you speak clear.

00:46:01.240 --> 00:46:03.870
SPEAKER 3: How did the
intermediate value sharing work

00:46:03.870 --> 00:46:06.165
with distribution [INAUDIBLE]?

00:46:06.165 --> 00:46:07.540
SPEAKER 1: It
works the same way,

00:46:07.540 --> 00:46:08.915
except there's a
lot more arrows.

00:46:08.915 --> 00:46:11.160
[LAUGHTER]

00:46:11.160 --> 00:46:14.080
Conceptually, they do not
interfere with [INAUDIBLE]..

00:46:14.080 --> 00:46:17.760
But you end up with the diagram
to show both at the same time

00:46:17.760 --> 00:46:20.567
would be overwhelming.

00:46:20.567 --> 00:46:22.900
SKYE WANDERMAN-MILNE: Yeah,
that's a good point, though.

00:46:22.900 --> 00:46:25.018
I feel like it's not
immediately obvious

00:46:25.018 --> 00:46:27.060
that it works with all
the dead tensors and stuff

00:46:27.060 --> 00:46:28.917
between the forward
and backwards pass.

00:46:28.917 --> 00:46:30.750
Because now you're like
mixing [INAUDIBLE]..

00:46:30.750 --> 00:46:32.828
But it does somehow work.

00:46:32.828 --> 00:46:34.870
SPEAKER 4: You need to
think of the intermediates

00:46:34.870 --> 00:46:36.900
as happening before you
do the partitioning,

00:46:36.900 --> 00:46:38.567
and then you can see
what should happen.

00:46:41.690 --> 00:46:47.480
SKYE WANDERMAN-MILNE: I'll
go back to my pretty picture.

00:46:47.480 --> 00:46:48.515
Well, thanks, everyone.

00:46:48.515 --> 00:46:49.390
SPEAKER 6: Thank you.

00:46:49.390 --> 00:46:55.440
[APPLAUSE]

