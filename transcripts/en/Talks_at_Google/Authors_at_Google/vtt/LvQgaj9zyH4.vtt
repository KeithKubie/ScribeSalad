WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.115
[MUSIC PLAYING]

00:00:05.115 --> 00:00:07.500
TIM: Thanks, again for
coming and please join me

00:00:07.500 --> 00:00:10.458
as we give a warm
welcome to Kate O'Neill.

00:00:10.458 --> 00:00:13.811
[APPLAUSE]

00:00:13.811 --> 00:00:15.280
KATE O'NEILL: Thanks very much.

00:00:15.280 --> 00:00:16.887
Thank you, Tim.

00:00:16.887 --> 00:00:17.720
Thank you, everyone.

00:00:17.720 --> 00:00:20.620
Hello, fellow humans.

00:00:20.620 --> 00:00:23.510
Wait, actually, I guess I
should check and make sure,

00:00:23.510 --> 00:00:25.960
are there any robots here?

00:00:25.960 --> 00:00:26.710
Any robots?

00:00:26.710 --> 00:00:31.570
Raise your hand if you're
a robot in the audience.

00:00:31.570 --> 00:00:34.030
I don't see any, so I think
we're safe to proceed.

00:00:34.030 --> 00:00:35.710
But I ask this
question every once

00:00:35.710 --> 00:00:38.260
in a while, because I figure,
one of these days, that it's

00:00:38.260 --> 00:00:41.020
going to be kind of a little
spindly, mechanical arm that

00:00:41.020 --> 00:00:43.280
comes up when I
ask that question.

00:00:43.280 --> 00:00:46.450
I'm not really sure what I'm
supposed to do at that moment,

00:00:46.450 --> 00:00:49.030
if I'm supposed to invite the
robot to come up here and take

00:00:49.030 --> 00:00:52.270
my job, because that's kind
of how we talk about robots

00:00:52.270 --> 00:00:54.430
and automation and AI
and everything these days

00:00:54.430 --> 00:00:57.760
is like, with this fear,
this dread about what it's

00:00:57.760 --> 00:01:00.940
going to mean for human
jobs, for humanity,

00:01:00.940 --> 00:01:05.660
for existential
reality as we know it.

00:01:05.660 --> 00:01:09.340
So my premise has been
to think about how

00:01:09.340 --> 00:01:13.030
we can make technology
better for humanity, better

00:01:13.030 --> 00:01:15.250
for our future, and
of course, better

00:01:15.250 --> 00:01:17.474
for serving business purposes.

00:01:17.474 --> 00:01:19.390
But in doing so, I think
we have to start back

00:01:19.390 --> 00:01:22.000
from one of the squares,
one of the foundations,

00:01:22.000 --> 00:01:25.730
and that is to think about
what it is that makes us human.

00:01:25.730 --> 00:01:27.340
What is it that
makes humans human?

00:01:27.340 --> 00:01:29.530
So I'll ask you to
indulge me and just

00:01:29.530 --> 00:01:33.190
think for a moment of a
word or a characteristic

00:01:33.190 --> 00:01:37.210
that you feel really captures
the human experience.

00:01:37.210 --> 00:01:40.087
What one characteristic is it?

00:01:40.087 --> 00:01:41.920
And I won't have you
call it out or anything

00:01:41.920 --> 00:01:44.360
but just hold it in
your head for a moment.

00:01:44.360 --> 00:01:49.530
What do you feel like it is
that really makes humans human?

00:01:49.530 --> 00:01:52.310
And so let me ask, how many
of you, by a show of hands,

00:01:52.310 --> 00:01:54.755
thought of something like
creativity or problem

00:01:54.755 --> 00:01:57.540
solving or innovation
or something like that?

00:01:57.540 --> 00:01:58.310
Anyone?

00:01:58.310 --> 00:01:58.810
No one?

00:01:58.810 --> 00:02:00.590
A couple people in the room.

00:02:00.590 --> 00:02:01.700
Good.

00:02:01.700 --> 00:02:04.580
So that's a pretty
common answer.

00:02:04.580 --> 00:02:06.860
This is a more common
answer, I think,

00:02:06.860 --> 00:02:09.620
empathy or love or compassion.

00:02:09.620 --> 00:02:10.639
Anyone?

00:02:10.639 --> 00:02:12.110
A few more hands.

00:02:12.110 --> 00:02:14.810
Those are both great, I
think-- great characteristics

00:02:14.810 --> 00:02:17.690
and admirable
qualities of humans.

00:02:17.690 --> 00:02:19.610
I didn't necessarily
specify that these

00:02:19.610 --> 00:02:21.355
needed to be uniquely
human attributes.

00:02:21.355 --> 00:02:23.420
But I do think, if
we think about those,

00:02:23.420 --> 00:02:25.850
they don't feel like
they are uniquely human.

00:02:25.850 --> 00:02:27.770
We've seen creativity
and problem

00:02:27.770 --> 00:02:30.980
solving in non-human
animals, like otters bang

00:02:30.980 --> 00:02:34.400
mollusks on rocks to open
them, and ravens use tools.

00:02:34.400 --> 00:02:36.410
And we've seen
compassion and love

00:02:36.410 --> 00:02:40.230
from elephants and
dogs and other species.

00:02:40.230 --> 00:02:43.460
So we know that those are
exhibited by other animals.

00:02:43.460 --> 00:02:45.530
And I don't think
it's too far fetched

00:02:45.530 --> 00:02:47.570
to imagine that in the
not too distant future,

00:02:47.570 --> 00:02:50.300
we might see at least
superficial indications

00:02:50.300 --> 00:02:53.630
of machines exhibiting
those kinds of qualities

00:02:53.630 --> 00:02:56.930
in their behavior interactions
with humans and maybe even,

00:02:56.930 --> 00:02:59.360
eventually, other machines,
which will be very

00:02:59.360 --> 00:03:02.612
interesting at a surface level.

00:03:02.612 --> 00:03:04.070
But how many of
you, when you think

00:03:04.070 --> 00:03:07.100
about what that most
human of characteristics

00:03:07.100 --> 00:03:10.880
is, thought of checking a box?

00:03:10.880 --> 00:03:14.090
Anyone, by a show of hands?

00:03:14.090 --> 00:03:16.400
Of course, you didn't,
because it's absurd.

00:03:16.400 --> 00:03:18.440
But this is the
premise, the problem

00:03:18.440 --> 00:03:20.600
that we encounter in
technology a lot of the time

00:03:20.600 --> 00:03:22.685
is that we don't
necessarily think

00:03:22.685 --> 00:03:24.560
through this kind of
foundational experience.

00:03:24.560 --> 00:03:26.780
And we are presenting
absurdities

00:03:26.780 --> 00:03:29.450
as if they are
foundational truths.

00:03:29.450 --> 00:03:31.340
And besides which,
if we were to try

00:03:31.340 --> 00:03:34.490
to claim that this is a
uniquely human characteristic,

00:03:34.490 --> 00:03:39.140
we'd get beat out anyway
by machines that can also

00:03:39.140 --> 00:03:41.670
do this characteristic.

00:03:41.670 --> 00:03:43.490
So I don't know how
many of you have

00:03:43.490 --> 00:03:49.516
seen this little [INAUDIBLE]
guy, but he's kind of fun.

00:03:49.516 --> 00:03:53.190
[LAUGHTER]

00:03:53.190 --> 00:03:57.680
So I have come to be known
as, as Tim mentioned,

00:03:57.680 --> 00:03:58.580
the tech humanist.

00:03:58.580 --> 00:04:03.000
And I take this moniker
pretty seriously,

00:04:03.000 --> 00:04:07.070
because I feel like there
is this area around which

00:04:07.070 --> 00:04:09.230
technology does
have the capacity

00:04:09.230 --> 00:04:10.950
to solve human problems.

00:04:10.950 --> 00:04:13.730
It also has the
capacity to scale,

00:04:13.730 --> 00:04:17.029
like we are experiencing
automation and AI and all kinds

00:04:17.029 --> 00:04:19.790
of other emerging
technologies, bringing scale

00:04:19.790 --> 00:04:24.240
to the types of solutions
we create like never before.

00:04:24.240 --> 00:04:26.270
And so I think it behooves
us to really think

00:04:26.270 --> 00:04:29.657
about what the human
experience around that scale

00:04:29.657 --> 00:04:31.490
is going to be and what
the human experience

00:04:31.490 --> 00:04:34.790
around technology is going to be
and how we can make technology

00:04:34.790 --> 00:04:38.660
better solve business problems
and solve human problems

00:04:38.660 --> 00:04:40.350
at the same time.

00:04:40.350 --> 00:04:42.110
So as I talk about
being a tech humanist

00:04:42.110 --> 00:04:44.180
and as I think about
solving those challenges,

00:04:44.180 --> 00:04:46.190
I'm excited that you
all are in this room

00:04:46.190 --> 00:04:48.980
and are on the livestream and
watching on the video later,

00:04:48.980 --> 00:04:49.910
I hope.

00:04:49.910 --> 00:04:52.590
And then I want to
offer that, perhaps,

00:04:52.590 --> 00:04:56.600
that that is also you, that you
maybe also are a tech humanist.

00:04:56.600 --> 00:04:58.820
And I'd like to offer
that term to you,

00:04:58.820 --> 00:05:00.590
so that when you see my book--

00:05:00.590 --> 00:05:02.030
as Tim mentioned,
it just came out

00:05:02.030 --> 00:05:04.760
September 24, "Tech Humanist"--

00:05:04.760 --> 00:05:07.190
that you will see
that title and think,

00:05:07.190 --> 00:05:10.190
I'm describing you as well,
because that is the truth.

00:05:10.190 --> 00:05:13.250
I'd like to see us all
join hands in this movement

00:05:13.250 --> 00:05:18.350
to create more human technology
and more wide-scale human

00:05:18.350 --> 00:05:20.900
experiences that are
more meaningful and more

00:05:20.900 --> 00:05:24.290
integrated and more dimensional
with the technology we create.

00:05:24.290 --> 00:05:27.890
So the premise
there is, how can we

00:05:27.890 --> 00:05:30.530
both make technology
better for business,

00:05:30.530 --> 00:05:35.060
to solve business challenges,
and make it better for humans?

00:05:35.060 --> 00:05:37.520
And I think that that
both, and framing

00:05:37.520 --> 00:05:38.900
is the key to the whole thing.

00:05:38.900 --> 00:05:42.470
We need to understand how to
accept that these things do

00:05:42.470 --> 00:05:44.750
need to be integrated together.

00:05:44.750 --> 00:05:48.890
And I would propose that the
way to accomplish this at scale

00:05:48.890 --> 00:05:54.140
is to focus on creating more
meaningful human experiences

00:05:54.140 --> 00:05:54.960
at scale.

00:05:54.960 --> 00:05:58.040
So how do we focus on
getting that meaning

00:05:58.040 --> 00:05:59.420
into the human experiences?

00:05:59.420 --> 00:06:02.660
So the way that that looks
in the model that I propose

00:06:02.660 --> 00:06:03.690
is this.

00:06:03.690 --> 00:06:07.250
On one hand, how can we
think about scaling business

00:06:07.250 --> 00:06:10.070
meaningfully through data,
through strategic alignment

00:06:10.070 --> 00:06:10.760
and automation?

00:06:10.760 --> 00:06:13.430
How can we think about using
the tools at our disposal

00:06:13.430 --> 00:06:17.510
to make business more
effective, while also creating

00:06:17.510 --> 00:06:19.550
more meaningful
human experiences

00:06:19.550 --> 00:06:22.930
and scaling those through
data and automation?

00:06:22.930 --> 00:06:25.406
I've had the opportunity
to test this idea

00:06:25.406 --> 00:06:26.780
with a lot of
different companies

00:06:26.780 --> 00:06:28.940
that I've consulted with,
spoken with, advised,

00:06:28.940 --> 00:06:31.560
worked with on different
projects over the years.

00:06:31.560 --> 00:06:34.700
And I'm excited to say that it
works in almost every industry

00:06:34.700 --> 00:06:36.190
I've encountered.

00:06:36.190 --> 00:06:40.220
It provides great results
no matter who you are

00:06:40.220 --> 00:06:42.290
or what you're
trying to accomplish.

00:06:42.290 --> 00:06:45.650
Every company is trying
to achieve profit.

00:06:45.650 --> 00:06:48.950
Every company is trying to
achieve revenue-based metrics

00:06:48.950 --> 00:06:50.870
in what they're going about.

00:06:50.870 --> 00:06:53.240
Even if you're a
non-profit organization,

00:06:53.240 --> 00:06:56.390
you still have to be accountable
to some sort of profit and loss

00:06:56.390 --> 00:06:57.060
scenario.

00:06:57.060 --> 00:06:59.390
There's some sort of
breakdown of the financials

00:06:59.390 --> 00:07:01.040
that you need to
be accountable for.

00:07:01.040 --> 00:07:03.710
And I'm happy to tell you
that the work of creating

00:07:03.710 --> 00:07:07.580
meaningful experiences actually
does lead to increased employee

00:07:07.580 --> 00:07:10.400
retention, decreased customer
acquisition cost, increased

00:07:10.400 --> 00:07:13.460
loyalty, and all kinds of
other directional metrics

00:07:13.460 --> 00:07:15.650
that lead to more profit.

00:07:15.650 --> 00:07:17.450
Of course, it's also
the right thing to do.

00:07:17.450 --> 00:07:19.510
It also creates a better
experience for all of us.

00:07:19.510 --> 00:07:21.009
And I want everybody
to be motivated

00:07:21.009 --> 00:07:25.250
by this aesthetic of wanting
the world to be better

00:07:25.250 --> 00:07:28.760
and creating more meaningful
experiences being its own end.

00:07:28.760 --> 00:07:31.500
But if we have to be motivated
by profit, we can be,

00:07:31.500 --> 00:07:33.620
and that's all a
good thing, too.

00:07:33.620 --> 00:07:35.420
So let's unpack this
just a little bit,

00:07:35.420 --> 00:07:37.910
what I mean when I
talk about creating

00:07:37.910 --> 00:07:41.160
meaningful human
experiences at scale.

00:07:41.160 --> 00:07:42.270
What does that entail?

00:07:42.270 --> 00:07:46.010
So first, let's think about
what meaningful really is.

00:07:46.010 --> 00:07:51.200
So the example of the click the
box to confirm your humanity,

00:07:51.200 --> 00:07:53.420
I mentioned, I think, that
that's an absurd example.

00:07:53.420 --> 00:07:58.760
And I have this running
hobby of appreciating

00:07:58.760 --> 00:08:02.200
the tension between meaning
and absurdity in the world.

00:08:02.200 --> 00:08:05.150
But I feel like anywhere
there is a lack of meaning,

00:08:05.150 --> 00:08:09.080
it opens up this void into
which absurdity can flow.

00:08:09.080 --> 00:08:12.140
So where we don't create
enough meaning, where

00:08:12.140 --> 00:08:14.570
we don't describe
enough meaning,

00:08:14.570 --> 00:08:17.370
we allow absurdity to flourish.

00:08:17.370 --> 00:08:20.180
So there is enough opportunity
for that in technology

00:08:20.180 --> 00:08:22.220
as it is and business, really.

00:08:22.220 --> 00:08:24.290
I think you all probably
have this experience,

00:08:24.290 --> 00:08:28.100
I'm guessing, that
there are areas where--

00:08:28.100 --> 00:08:30.860
let's say, you talk
about work things in ways

00:08:30.860 --> 00:08:33.230
that you wouldn't talk
about with your friends

00:08:33.230 --> 00:08:34.039
outside of work.

00:08:34.039 --> 00:08:37.039
You use language or
terminology that your friends

00:08:37.039 --> 00:08:40.520
who don't work with you
would not understand.

00:08:40.520 --> 00:08:44.450
Or there are things that you
do at work that just don't--

00:08:44.450 --> 00:08:47.222
that are kind of like, that's
the way we've always done it.

00:08:47.222 --> 00:08:48.680
But any time you
think to yourself,

00:08:48.680 --> 00:08:51.440
this doesn't make sense,
that's a really big clue,

00:08:51.440 --> 00:08:54.390
because making things makes
sense is what meaning does.

00:08:54.390 --> 00:08:57.830
So we have an opportunity to
step back and assess absurdity

00:08:57.830 --> 00:09:01.190
and recognize that we can infuse
meaning into those structures

00:09:01.190 --> 00:09:04.040
and create more opportunity
to avoid meaning,

00:09:04.040 --> 00:09:05.840
to keep away from meaning.

00:09:05.840 --> 00:09:08.490
So the reason that
that works, I believe,

00:09:08.490 --> 00:09:10.990
is because humans
crave meaning more

00:09:10.990 --> 00:09:12.240
than any other characteristic.

00:09:12.240 --> 00:09:15.530
So if you were to ask me what
I think makes humans human,

00:09:15.530 --> 00:09:16.860
this is what it is--

00:09:16.860 --> 00:09:19.010
is that we seek meaning.

00:09:19.010 --> 00:09:21.770
In all areas of life, we
are compelled by meaning.

00:09:21.770 --> 00:09:25.830
If you offer us a meaningful
answer or solution,

00:09:25.830 --> 00:09:27.840
we are compelled by it.

00:09:27.840 --> 00:09:29.930
How many of you are
Douglas Adams fans?

00:09:29.930 --> 00:09:30.860
Anyone?

00:09:30.860 --> 00:09:32.000
A few.

00:09:32.000 --> 00:09:35.299
So you already know where
I'm going to go with this.

00:09:35.299 --> 00:09:37.340
In the "Hitchhiker's Guide
to the Galaxy" series,

00:09:37.340 --> 00:09:39.420
the answer to the
great question of life,

00:09:39.420 --> 00:09:41.170
the universe, and
everything was--

00:09:41.170 --> 00:09:41.925
AUDIENCE: 42.

00:09:41.925 --> 00:09:44.300
KATE O'NEILL: 42, of course.

00:09:44.300 --> 00:09:46.590
So Douglas Adams wrote
or said in interviews

00:09:46.590 --> 00:09:49.760
that he chose 42 because it was
not too high and not too low

00:09:49.760 --> 00:09:51.510
of a number and because
it was just funny.

00:09:51.510 --> 00:09:52.534
It is.

00:09:52.534 --> 00:09:53.950
But I don't know
if you know this,

00:09:53.950 --> 00:09:56.730
but on Reddit,
you can find this.

00:09:56.730 --> 00:09:59.280
And a few other places
collected around the web,

00:09:59.280 --> 00:10:03.240
there are collections of
alternate explanations

00:10:03.240 --> 00:10:06.090
for why 42 actually
kind of makes sense

00:10:06.090 --> 00:10:09.100
as the explanation of
meaning in the world.

00:10:09.100 --> 00:10:12.150
So for example, there are
42 characters in the phrase,

00:10:12.150 --> 00:10:16.080
it's the answer to life, the
universe, and everything.

00:10:16.080 --> 00:10:19.120
So you're convinced now, right?

00:10:19.120 --> 00:10:22.350
Also, there's 42 dots
on a pair of dice,

00:10:22.350 --> 00:10:26.070
so that answers everything,
which I thought was just a

00:10:26.070 --> 00:10:27.030
throw away explanation.

00:10:27.030 --> 00:10:28.410
But my husband
said, well, life is

00:10:28.410 --> 00:10:30.576
kind of like a roll of the
dice, so I thought, well,

00:10:30.576 --> 00:10:33.390
all right, fair enough.

00:10:33.390 --> 00:10:36.440
My favorite of them is this.

00:10:36.440 --> 00:10:39.990
42 is apparently the
Unicode character--

00:10:39.990 --> 00:10:42.810
Unicode value for the
asterisk character,

00:10:42.810 --> 00:10:46.410
which as you may know, is
a wild card symbol, often,

00:10:46.410 --> 00:10:49.335
in computing, which means
it can mean anything.

00:10:54.944 --> 00:10:56.360
It's obviously
just a coincidence,

00:10:56.360 --> 00:10:58.276
because Douglas Adams
didn't mean it that way.

00:10:58.276 --> 00:10:59.942
AUDIENCE: That makes
it sound better.

00:10:59.942 --> 00:11:02.690
[INAUDIBLE]

00:11:02.690 --> 00:11:04.590
KATE O'NEILL: But that's
the important point,

00:11:04.590 --> 00:11:07.290
is that even though Douglas
Adams didn't mean it

00:11:07.290 --> 00:11:09.840
that way and it is
just a coincidence,

00:11:09.840 --> 00:11:13.290
it is an absurd and poetic
and beautiful coincidence.

00:11:13.290 --> 00:11:15.000
But we always make
meaning the way

00:11:15.000 --> 00:11:17.430
we have always done
and always will,

00:11:17.430 --> 00:11:20.490
which is by ascribing
different significance

00:11:20.490 --> 00:11:23.970
to different events, based on
how and much we value them,

00:11:23.970 --> 00:11:27.850
or in other words, by
making it up as we go along.

00:11:27.850 --> 00:11:30.350
And I think that's the
encouraging thing about this,

00:11:30.350 --> 00:11:33.380
is that even though
we talk about robots

00:11:33.380 --> 00:11:37.640
and automation and AI
in the broad mainstream

00:11:37.640 --> 00:11:39.830
in a scary way,
what this suggests

00:11:39.830 --> 00:11:43.850
is that there is this open
interpretation to the future.

00:11:43.850 --> 00:11:47.000
We get to make meaning for
the future as we go along.

00:11:47.000 --> 00:11:49.580
We get to decide the
future as we go along.

00:11:49.580 --> 00:11:54.030
You get to decide the
future as you go along.

00:11:54.030 --> 00:11:56.570
And that's really incredible,
because right now,

00:11:56.570 --> 00:11:59.090
the possibilities,
the power of what's

00:11:59.090 --> 00:12:02.630
happening within technology and
within the scale of emerging

00:12:02.630 --> 00:12:06.110
technologies, means that
we have the capacity

00:12:06.110 --> 00:12:09.500
to create the best futures
for the most people.

00:12:09.500 --> 00:12:11.570
There's really this
potential and, I

00:12:11.570 --> 00:12:14.000
think, even an ethical
responsibility,

00:12:14.000 --> 00:12:20.650
to think about how solutions
can scale to that sort of level.

00:12:20.650 --> 00:12:22.290
So let's go back to
unpack and create

00:12:22.290 --> 00:12:24.100
meaningful human
experiences at scale

00:12:24.100 --> 00:12:27.180
and what is it that human
experiences really describes.

00:12:30.070 --> 00:12:32.890
We talk a lot in business
about customer experience, user

00:12:32.890 --> 00:12:34.859
experience, or depending
on your industry,

00:12:34.859 --> 00:12:36.400
it may be patient
or guest or visitor

00:12:36.400 --> 00:12:38.590
experience, student experience.

00:12:38.590 --> 00:12:41.230
We don't often, in
many industries,

00:12:41.230 --> 00:12:44.770
talk about human experience
in this integrated way,

00:12:44.770 --> 00:12:47.410
in this way that brings
all of those roles together

00:12:47.410 --> 00:12:49.270
and appreciates
the fact that there

00:12:49.270 --> 00:12:52.470
is this kind of holistic
human experience

00:12:52.470 --> 00:12:55.690
that transcends any of
those roles, that you are--

00:12:55.690 --> 00:12:59.570
we are all of those roles
at any given point in time.

00:12:59.570 --> 00:13:01.750
And so even though
you may be performing

00:13:01.750 --> 00:13:04.240
as a customer in a
customer experience,

00:13:04.240 --> 00:13:07.630
you are still a human coming
into that customer experience.

00:13:07.630 --> 00:13:10.630
The important thing about that
is the transcendent empathy

00:13:10.630 --> 00:13:13.240
that can come from
understanding the baggage,

00:13:13.240 --> 00:13:16.942
the context that someone
brings to that interaction.

00:13:16.942 --> 00:13:18.400
So there's an
opportunity to create

00:13:18.400 --> 00:13:20.500
these more dimensional
interactions, these more

00:13:20.500 --> 00:13:21.670
integrated interactions.

00:13:21.670 --> 00:13:24.340
And so to create more
meaningful human interactions,

00:13:24.340 --> 00:13:26.530
it turns out, we
need to design more

00:13:26.530 --> 00:13:28.825
integrated human experiences.

00:13:28.825 --> 00:13:31.570
We need to think about how
to blend all of those roles

00:13:31.570 --> 00:13:34.595
and understandings together and
bring an understanding of where

00:13:34.595 --> 00:13:36.970
someone has been, where you're
meeting them in the world,

00:13:36.970 --> 00:13:39.310
and how you can create
these kinds of senses

00:13:39.310 --> 00:13:42.400
of dimension and holism.

00:13:42.400 --> 00:13:46.630
So I promised a Venn
diagram to a friend earlier,

00:13:46.630 --> 00:13:51.387
and I have it, the best Venn
diagram in the entire world.

00:13:51.387 --> 00:13:52.720
I'm sure you all have seen this.

00:13:52.720 --> 00:13:55.630
If you haven't, you'll want to
rush out and get this t-shirt

00:13:55.630 --> 00:13:56.700
right away.

00:13:56.700 --> 00:13:59.500
Tenso Graphics makes
this Venn diagram.

00:13:59.500 --> 00:14:01.810
But the illustration
here, really, I

00:14:01.810 --> 00:14:03.770
think, gets at the point.

00:14:03.770 --> 00:14:06.430
When you think about
what is possible on one

00:14:06.430 --> 00:14:08.620
side of an equation,
such as the best

00:14:08.620 --> 00:14:12.327
technology or the technology
to make business better,

00:14:12.327 --> 00:14:14.410
and what's possible on the
other side of equation,

00:14:14.410 --> 00:14:18.676
like what's possible to make
technology better for humans,

00:14:18.676 --> 00:14:20.800
it's only by really thinking
about the intersection

00:14:20.800 --> 00:14:25.300
of those things that you really
come at the best solutions,

00:14:25.300 --> 00:14:27.490
like platypus keytar.

00:14:27.490 --> 00:14:29.380
You don't get platypus
keytar until you're

00:14:29.380 --> 00:14:32.830
doing some serious both, anding,
so that's the opportunity.

00:14:32.830 --> 00:14:34.330
And really, what
we're talking about

00:14:34.330 --> 00:14:38.770
is augmenting human experience
with data and context.

00:14:38.770 --> 00:14:41.530
So the broader opportunity,
in a technology sense,

00:14:41.530 --> 00:14:44.282
is to really think
about, where are you

00:14:44.282 --> 00:14:45.490
meeting someone in the world?

00:14:45.490 --> 00:14:48.070
What data do you have to
understand and appreciate

00:14:48.070 --> 00:14:50.180
where they come from, what
their preferences are,

00:14:50.180 --> 00:14:51.630
what their tastes are?

00:14:51.630 --> 00:14:55.750
And how can you create context
that addresses the objectives

00:14:55.750 --> 00:14:57.880
that you have as a
business and the objectives

00:14:57.880 --> 00:14:59.830
they have as a
human and the role

00:14:59.830 --> 00:15:02.080
that you're meeting them
in and the alignment

00:15:02.080 --> 00:15:03.250
of those objectives?

00:15:03.250 --> 00:15:06.966
How can we come at that in
a way that provides that?

00:15:06.966 --> 00:15:08.590
And so I actually
kind of think of this

00:15:08.590 --> 00:15:12.310
in a way as being meaning
as a service, in a sense.

00:15:12.310 --> 00:15:15.310
It's an opportunity to
think about offering up

00:15:15.310 --> 00:15:17.680
a meaningful
construct that aligns

00:15:17.680 --> 00:15:19.900
your objective and
their objective

00:15:19.900 --> 00:15:22.960
and providing the
hooks, in a sense,

00:15:22.960 --> 00:15:24.790
to be able to expand upon that.

00:15:24.790 --> 00:15:27.340
And I really mean any
meaning of meaning.

00:15:27.340 --> 00:15:30.400
So meaning, as we talk about
it, could be at any level.

00:15:30.400 --> 00:15:33.890
We talk about meaning as it
relates to communication.

00:15:33.890 --> 00:15:35.590
So I'm a linguist
by education, so I

00:15:35.590 --> 00:15:38.860
think about the semantic
layer, how we communicate

00:15:38.860 --> 00:15:42.250
with one another, what we
convey across our communications

00:15:42.250 --> 00:15:44.357
with one another.

00:15:44.357 --> 00:15:45.940
You probably spend
a lot of your time,

00:15:45.940 --> 00:15:48.700
if you do a lot of
development or engineering,

00:15:48.700 --> 00:15:50.200
in patterns and significance.

00:15:50.200 --> 00:15:53.270
That's probably a layer that
you spend a lot of time in.

00:15:53.270 --> 00:15:55.820
But it could be all the way out
to the existential and cosmic

00:15:55.820 --> 00:15:56.320
layer.

00:15:56.320 --> 00:15:58.690
What is it all about, Alfie?

00:15:58.690 --> 00:16:00.760
That sort of thing.

00:16:00.760 --> 00:16:02.610
I think, in a sense,
it's almost like API

00:16:02.610 --> 00:16:03.610
thinking for everything.

00:16:03.610 --> 00:16:07.480
You can really think about
how one idea integrates

00:16:07.480 --> 00:16:11.140
with another and how what
is meaningful on one side,

00:16:11.140 --> 00:16:14.620
like the business side, can be
meaningful on the human side

00:16:14.620 --> 00:16:15.470
of the equation.

00:16:15.470 --> 00:16:18.250
How do you provide
hooks and intelligence

00:16:18.250 --> 00:16:21.220
across those different
parts of the experience

00:16:21.220 --> 00:16:22.899
and make sure that
that meaning is being

00:16:22.899 --> 00:16:24.190
transferred through that layer?

00:16:27.140 --> 00:16:31.610
The integration that most
brought me to this realization

00:16:31.610 --> 00:16:35.000
is thinking about how the
design of experiences online now

00:16:35.000 --> 00:16:38.570
regularly intersects with the
design of experiences offline,

00:16:38.570 --> 00:16:41.870
that more and more as we think
about physical experiences,

00:16:41.870 --> 00:16:44.520
they come with some sort
of digital component

00:16:44.520 --> 00:16:47.240
or some sort of track
ability or traceability

00:16:47.240 --> 00:16:48.659
with that physical experience.

00:16:48.659 --> 00:16:50.450
Or when we think about
digital experiences,

00:16:50.450 --> 00:16:52.241
we have to think about
the physical context

00:16:52.241 --> 00:16:56.130
somebody might be in as they
encounter those interactions.

00:16:56.130 --> 00:16:58.994
So I wrote about this in my
last book, "Pixels and Place."

00:16:58.994 --> 00:17:01.160
So thinking about things
like the internet of things

00:17:01.160 --> 00:17:02.720
and wearables and
beacons and sensors

00:17:02.720 --> 00:17:05.720
and all kinds of
connected smart devices,

00:17:05.720 --> 00:17:09.319
and how those bring
that connective layer

00:17:09.319 --> 00:17:10.770
between those two worlds.

00:17:10.770 --> 00:17:13.490
But the important
point about that

00:17:13.490 --> 00:17:16.490
is that just about
everywhere interesting

00:17:16.490 --> 00:17:18.230
that the physical
world and digital world

00:17:18.230 --> 00:17:21.200
connect, that connection
layer happens through us,

00:17:21.200 --> 00:17:23.630
through humans, through
our human experience.

00:17:23.630 --> 00:17:25.069
It's our movements.

00:17:25.069 --> 00:17:26.720
It's our behavior.

00:17:26.720 --> 00:17:27.619
It's our patterns.

00:17:27.619 --> 00:17:31.160
It's what we want, what we do,
what we indicate that really

00:17:31.160 --> 00:17:33.940
creates that connection.

00:17:33.940 --> 00:17:35.520
So again, it comes
back to thinking

00:17:35.520 --> 00:17:38.620
about that integrated
human experience.

00:17:38.620 --> 00:17:42.570
So I proposed this model in
"Pixels and Place," which

00:17:42.570 --> 00:17:45.960
is integrated human experience
design, thinking about how

00:17:45.960 --> 00:17:49.500
to blend those online
and offline contexts,

00:17:49.500 --> 00:17:53.610
thinking about how to come
across all the different levels

00:17:53.610 --> 00:17:56.517
and roles of humanity
that you might encounter,

00:17:56.517 --> 00:17:58.350
thinking about how to
think about experience

00:17:58.350 --> 00:18:00.990
in an integrated way,
interactions and transactions

00:18:00.990 --> 00:18:03.840
across all the different touch
points that you might have.

00:18:03.840 --> 00:18:08.430
And note the way I'm defining
the word design, which

00:18:08.430 --> 00:18:11.640
is the adaptive execution
of strategic intent.

00:18:11.640 --> 00:18:14.250
So you know you
have an intention.

00:18:14.250 --> 00:18:15.660
You have a purpose
to what you're

00:18:15.660 --> 00:18:19.830
trying to do with any
given design initiative.

00:18:19.830 --> 00:18:22.590
And you know that you're
going to probably not get it

00:18:22.590 --> 00:18:25.870
exactly where you want
it to be on the first go,

00:18:25.870 --> 00:18:30.000
so we need to build an adaptive
iterative process to this.

00:18:30.000 --> 00:18:32.550
And the more we do this
around a framework of creating

00:18:32.550 --> 00:18:36.060
that meaningful interaction and
that dimensional relationship

00:18:36.060 --> 00:18:39.690
between business,
entity, and human that's

00:18:39.690 --> 00:18:41.760
consuming that
experience, the more

00:18:41.760 --> 00:18:45.700
we stand a chance of conveying
some sort of meaningful truth.

00:18:45.700 --> 00:18:48.180
So the elements of
integrated human experience

00:18:48.180 --> 00:18:50.614
design, as described
in "Pixels and Place"--

00:18:50.614 --> 00:18:52.030
I'll go through
it really quickly,

00:18:52.030 --> 00:18:55.320
because what I want to get to
is that with "Tech Humanist,"

00:18:55.320 --> 00:18:57.420
I've actually
built out upon this

00:18:57.420 --> 00:19:00.600
to a more automated
understanding of experience.

00:19:00.600 --> 00:19:03.150
But within integrated
human experience design

00:19:03.150 --> 00:19:06.990
in "Pixels and Place," we look
at integration, of course.

00:19:06.990 --> 00:19:08.410
That comes along for the ride.

00:19:08.410 --> 00:19:10.590
So we're already talking
about all these layers

00:19:10.590 --> 00:19:14.190
that are being integrated, the
online and offline contexts.

00:19:14.190 --> 00:19:15.970
We're talking about
dimensionality.

00:19:15.970 --> 00:19:17.970
So how does something
come to life

00:19:17.970 --> 00:19:21.450
across different touch
points or ways in which you

00:19:21.450 --> 00:19:24.030
interact with people?

00:19:24.030 --> 00:19:26.310
How do the metaphors and
cognitive associations

00:19:26.310 --> 00:19:26.970
come to life?

00:19:26.970 --> 00:19:28.770
What sorts of intentional
things are you

00:19:28.770 --> 00:19:31.380
communicating through all of
the choices that you're making,

00:19:31.380 --> 00:19:34.037
about the language that you
use, the iconography you use,

00:19:34.037 --> 00:19:36.120
and the cognitive assertions
you're bringing along

00:19:36.120 --> 00:19:38.880
with that-- cognitive
associations you're

00:19:38.880 --> 00:19:40.260
bringing along with that?

00:19:40.260 --> 00:19:41.790
Intentionality and
purpose, so how

00:19:41.790 --> 00:19:44.123
have you defined what it is
you're trying to accomplish?

00:19:44.123 --> 00:19:47.640
And that comes into play at
a more holistic, macro level

00:19:47.640 --> 00:19:50.080
as well, which we'll
get to in just a moment.

00:19:50.080 --> 00:19:52.020
And a value and emotional
load, where are you

00:19:52.020 --> 00:19:53.228
meeting someone in the world?

00:19:53.228 --> 00:19:55.290
How challenging is that context?

00:19:55.290 --> 00:19:58.176
If you're designing for an
encounter in a hospital,

00:19:58.176 --> 00:20:00.300
for example, it's going to
be a very different type

00:20:00.300 --> 00:20:05.050
of a value or emotional load
than if you encounter somebody

00:20:05.050 --> 00:20:08.130
at a children's museum
where they're having fun--

00:20:08.130 --> 00:20:10.340
hopefully, having fun.

00:20:10.340 --> 00:20:14.690
Alignment is, of course,
that foundational principle

00:20:14.690 --> 00:20:16.719
of understanding what
the business objective is

00:20:16.719 --> 00:20:18.260
and understanding
the human objective

00:20:18.260 --> 00:20:21.610
and making sure that they are
as tightly aligned as possible.

00:20:21.610 --> 00:20:23.750
And then adaptation
and iteration being,

00:20:23.750 --> 00:20:26.390
of course, that
process of making sure

00:20:26.390 --> 00:20:29.120
that we are building
upon what we've learned,

00:20:29.120 --> 00:20:34.190
we're using experimentation and
that mental model of building

00:20:34.190 --> 00:20:36.620
our learnings as we go.

00:20:36.620 --> 00:20:42.560
There's also this premise
that experience has,

00:20:42.560 --> 00:20:46.610
in a sense, two layers to it.

00:20:46.610 --> 00:20:52.540
If you think about human
nature as this ongoing truism,

00:20:52.540 --> 00:20:54.860
like we all have,
throughout time,

00:20:54.860 --> 00:20:57.552
needed to drink
water, for example.

00:20:57.552 --> 00:20:59.510
But then there is this
shape of that experience

00:20:59.510 --> 00:21:02.090
and how it gets packaged
up and dimensionalized.

00:21:02.090 --> 00:21:04.640
And so you can see
this bottle of water

00:21:04.640 --> 00:21:07.550
is an example of
saying, well, if I were

00:21:07.550 --> 00:21:10.490
to put that water into
a heavy glass bottle

00:21:10.490 --> 00:21:14.420
and label it with some sort
of minimalistic typeface brand

00:21:14.420 --> 00:21:17.780
and create that whole aesthetic,
and it has this hipster

00:21:17.780 --> 00:21:20.000
vibe to it, maybe
I'd feel like I'm

00:21:20.000 --> 00:21:22.460
being a more aspirational
version of myself, because I'm

00:21:22.460 --> 00:21:26.360
drinking maybe even the same
water out of this cool bottle.

00:21:26.360 --> 00:21:28.010
And I feel like a
better version of who

00:21:28.010 --> 00:21:33.570
I am than if I just drank it out
of the tap glass or whatever.

00:21:33.570 --> 00:21:38.182
So there's this ongoing
way in which shapes evolve.

00:21:38.182 --> 00:21:40.640
And it's important, I think,
to recognize that as we create

00:21:40.640 --> 00:21:44.330
these integrated experiences,
that human experiences do

00:21:44.330 --> 00:21:47.210
evolve, but the shapes will
always change more readily

00:21:47.210 --> 00:21:47.960
than the nature.

00:21:47.960 --> 00:21:52.370
And it helps us get into contact
and create this continuity

00:21:52.370 --> 00:21:54.950
across time with the
human nature that

00:21:54.950 --> 00:21:56.990
persists throughout
the experiences

00:21:56.990 --> 00:21:59.120
that we're designing
for, and yet,

00:21:59.120 --> 00:22:03.780
be ready to adapt to the
changing shape of experiences.

00:22:03.780 --> 00:22:07.220
So with that, that leads
us into this opportunity

00:22:07.220 --> 00:22:11.720
to think about how machine-led
experiences can actually

00:22:11.720 --> 00:22:12.850
be more meaningful.

00:22:12.850 --> 00:22:15.920
The more we're thinking
about automated experiences

00:22:15.920 --> 00:22:18.380
and artificially
intelligent experiences,

00:22:18.380 --> 00:22:21.170
how can we think about making
sure that the humans that

00:22:21.170 --> 00:22:23.090
interact with those
are having as much

00:22:23.090 --> 00:22:27.020
of a sense of meaning and
significance and dimension

00:22:27.020 --> 00:22:28.680
to those?

00:22:28.680 --> 00:22:30.830
So what I proposed
in "Tech Humanist"

00:22:30.830 --> 00:22:34.086
is that we don't just
automate the menial.

00:22:34.086 --> 00:22:35.210
We automate the meaningful.

00:22:35.210 --> 00:22:37.730
I'll go through each one of
these in detail, of course.

00:22:37.730 --> 00:22:41.120
That we automate empathy, that
we use human data respectfully,

00:22:41.120 --> 00:22:43.580
and that we reinvest
the gains in efficiency

00:22:43.580 --> 00:22:46.310
that we get in business
from automation

00:22:46.310 --> 00:22:48.890
back into humanity and
human experiences, at least

00:22:48.890 --> 00:22:50.250
at some level.

00:22:50.250 --> 00:22:52.290
And so we'll talk about
each one of these.

00:22:52.290 --> 00:22:53.810
I'll start with this.

00:22:53.810 --> 00:22:56.390
I think a lot of times when
we talk about automation,

00:22:56.390 --> 00:22:59.390
our base understanding is
that we should automate

00:22:59.390 --> 00:23:02.360
menial, meaningless
things, so that humans

00:23:02.360 --> 00:23:06.530
can do higher order tasks,
which is a nice enough premise,

00:23:06.530 --> 00:23:08.900
until you start thinking
about that at scale

00:23:08.900 --> 00:23:11.960
and start imagining a world in
which all kinds of functions

00:23:11.960 --> 00:23:12.800
have been automated.

00:23:12.800 --> 00:23:14.390
And most of our
world is automated

00:23:14.390 --> 00:23:17.120
and most of our interactions
are with machines,

00:23:17.120 --> 00:23:20.220
and they've all been
automated to be meaningless.

00:23:20.220 --> 00:23:23.120
So I think it's a yes,
and, a both, and scenario.

00:23:23.120 --> 00:23:25.160
We do need to think
about automating

00:23:25.160 --> 00:23:28.700
the menial, meaningless
functions to free ourselves up

00:23:28.700 --> 00:23:30.890
to think about
higher order things.

00:23:30.890 --> 00:23:33.110
But we also need to think
about, what's working?

00:23:33.110 --> 00:23:35.810
What are human
interactions that convey

00:23:35.810 --> 00:23:37.760
some level of empathy
and nuance, that

00:23:37.760 --> 00:23:40.740
create some sort of
significance and dimension?

00:23:40.740 --> 00:23:43.490
And how can we work to
automate those as well?

00:23:43.490 --> 00:23:45.860
How can we capture some
of that significance

00:23:45.860 --> 00:23:47.900
in those automations?

00:23:47.900 --> 00:23:50.870
So in this way, we're talking
about using data and technology

00:23:50.870 --> 00:23:53.254
to scale, not just for
efficiency but for meaning,

00:23:53.254 --> 00:23:54.920
to think about ways
that we can actually

00:23:54.920 --> 00:23:58.580
create a sense of dimension
in the world around us.

00:23:58.580 --> 00:24:00.530
One way that that works is--

00:24:00.530 --> 00:24:02.960
I like to think about this
model of this relationship

00:24:02.960 --> 00:24:05.720
between metaphor and metadata.

00:24:05.720 --> 00:24:08.720
And I think the
easiest way to explain

00:24:08.720 --> 00:24:12.650
this is a slide I
stole from Brian

00:24:12.650 --> 00:24:15.980
Chesky, the CEO of Airbnb,
when he was demoing

00:24:15.980 --> 00:24:18.029
a couple of years
ago, the new campaign

00:24:18.029 --> 00:24:19.820
that they were launching
at the time, which

00:24:19.820 --> 00:24:23.420
was the "Don't Go There,
Live There" campaign.

00:24:23.420 --> 00:24:24.630
Anybody familiar with that?

00:24:24.630 --> 00:24:26.520
Did you guys run
across that at all?

00:24:26.520 --> 00:24:29.840
So the idea was, even if
it's only for one night,

00:24:29.840 --> 00:24:32.570
go to every place you
visit as if you're a local,

00:24:32.570 --> 00:24:35.690
treat that city
like you're a local.

00:24:35.690 --> 00:24:37.790
And this slide was
an illustration

00:24:37.790 --> 00:24:43.950
of how you could experience
a different type of approach

00:24:43.950 --> 00:24:47.240
on TripAdvisor versus
with the Airbnb approach

00:24:47.240 --> 00:24:50.660
of trusting the local experts.

00:24:50.660 --> 00:24:52.070
So this is obviously Paris.

00:24:52.070 --> 00:24:55.592
And note that
everything on each list

00:24:55.592 --> 00:24:57.050
is different, except
for one, which

00:24:57.050 --> 00:24:59.660
is the Luxembourg Garden, which
is my favorite place in Paris,

00:24:59.660 --> 00:25:02.510
so yay, me.

00:25:02.510 --> 00:25:05.670
Each of the other things
on the TripAdvisor list

00:25:05.670 --> 00:25:09.860
is really just a brute
popularity contest.

00:25:09.860 --> 00:25:15.080
It's all just, what are the most
bucket list items that someone

00:25:15.080 --> 00:25:16.930
would associate with Paris?

00:25:16.930 --> 00:25:20.510
And on the Airbnb
side, it's who has

00:25:20.510 --> 00:25:22.190
the most significant
understanding

00:25:22.190 --> 00:25:23.990
of the city of
Paris, what do they

00:25:23.990 --> 00:25:28.130
recommend as being the places
that you must visit and must

00:25:28.130 --> 00:25:29.822
experience in Paris?

00:25:29.822 --> 00:25:31.280
And what I think
is interesting, is

00:25:31.280 --> 00:25:33.500
when you think about the
metaphor that's really

00:25:33.500 --> 00:25:37.370
underlying this, it's clear
that the TripAdvisor metaphor

00:25:37.370 --> 00:25:41.390
is much more about
this casual tourist

00:25:41.390 --> 00:25:44.780
experience of the world, this
conventional understanding.

00:25:44.780 --> 00:25:46.640
Whereas the Airbnb
thing is that,

00:25:46.640 --> 00:25:49.340
don't go there, live there,
this knowledge of the expertise.

00:25:49.340 --> 00:25:51.830
And then the metadata clearly
is-- it's like the same city.

00:25:51.830 --> 00:25:53.205
These are all the
same landmarks.

00:25:53.205 --> 00:25:54.750
They exist in either case.

00:25:54.750 --> 00:25:57.350
But one is being
rated for popularity,

00:25:57.350 --> 00:26:01.610
and one is being ranked for
this expertise or authority.

00:26:01.610 --> 00:26:04.250
So the way that these
two dimensions interact

00:26:04.250 --> 00:26:07.260
with one another creates this
more meaningful understanding

00:26:07.260 --> 00:26:10.250
of what the company
is trying to achieve

00:26:10.250 --> 00:26:12.330
and how it brings it
to dimensional life

00:26:12.330 --> 00:26:15.080
for the person that's
interacting with it.

00:26:15.080 --> 00:26:17.720
Because that meaning
informs the purpose

00:26:17.720 --> 00:26:21.320
that the company is bringing
to life in their experiences,

00:26:21.320 --> 00:26:23.540
and the purpose of the
company that they're

00:26:23.540 --> 00:26:27.050
trying to bring to life
fosters the meaning

00:26:27.050 --> 00:26:29.480
that the person is going to
experience when they interact

00:26:29.480 --> 00:26:32.150
with the touch points
that the company creates,

00:26:32.150 --> 00:26:33.770
if they've done it well.

00:26:33.770 --> 00:26:36.380
And the nice thing about this,
when you think about how this

00:26:36.380 --> 00:26:41.030
really comes to life in an
automated, machine-led way,

00:26:41.030 --> 00:26:44.120
is that humans really--

00:26:44.120 --> 00:26:47.330
I think, when you think about
what the research shows,

00:26:47.330 --> 00:26:50.990
what we most thrive on
is a sense of meaning

00:26:50.990 --> 00:26:53.270
and common goals and a sense
of fulfilling something

00:26:53.270 --> 00:26:54.740
bigger than ourselves.

00:26:54.740 --> 00:26:59.360
Whereas machines thrive on this
sense of clear instruction.

00:26:59.360 --> 00:27:01.904
And what leads to both of
those things is purpose.

00:27:01.904 --> 00:27:03.320
Now, I'm not talking
about purpose

00:27:03.320 --> 00:27:07.530
like in this touchy, feely,
spiritual sense, necessarily.

00:27:07.530 --> 00:27:11.630
I'm talking about purpose as
a set of clear instructions

00:27:11.630 --> 00:27:15.800
or a sense of clarity about what
it is you're trying to achieve.

00:27:15.800 --> 00:27:18.500
And what that does is
leads to this ability

00:27:18.500 --> 00:27:22.820
to bring all your resources to
bear in a very efficient way

00:27:22.820 --> 00:27:24.380
and to align all
those resources,

00:27:24.380 --> 00:27:26.690
to set priorities
very effectively

00:27:26.690 --> 00:27:31.310
and make sure that everybody is
rowing in the same direction.

00:27:31.310 --> 00:27:33.740
My favorite example
of this, of companies

00:27:33.740 --> 00:27:35.540
setting a strategic
purpose and really

00:27:35.540 --> 00:27:40.080
using it to operationalize
around, is Disney theme parks.

00:27:40.080 --> 00:27:42.620
And from a digital
transformation perspective,

00:27:42.620 --> 00:27:44.210
the MyMagic Band program--

00:27:44.210 --> 00:27:46.820
how many of you have been
to Disney World or one

00:27:46.820 --> 00:27:49.580
of the Disney theme parks
since they've introduced this?

00:27:49.580 --> 00:27:51.530
It is pretty magical, right?

00:27:51.530 --> 00:27:54.860
So they've articulated
their purpose statement as,

00:27:54.860 --> 00:27:56.972
create magical experiences.

00:27:56.972 --> 00:27:58.430
It's really just
those three words,

00:27:58.430 --> 00:28:01.340
create magical experiences.

00:28:01.340 --> 00:28:03.590
And so you think about
across the organization,

00:28:03.590 --> 00:28:06.050
just about anyone
in any function

00:28:06.050 --> 00:28:08.420
can understand how
they can solve problems

00:28:08.420 --> 00:28:11.330
relative to their
scope of their work

00:28:11.330 --> 00:28:13.615
as it relates to creating
more magical experiences.

00:28:13.615 --> 00:28:14.990
A problem that's
brought to them,

00:28:14.990 --> 00:28:17.270
they can just go, I
know how to solve this,

00:28:17.270 --> 00:28:20.420
as long as the company actually
gets in line behind that

00:28:20.420 --> 00:28:22.940
and allows them the autonomy
to solve the problem the way

00:28:22.940 --> 00:28:24.074
they need to.

00:28:24.074 --> 00:28:25.490
But think about
that as it relates

00:28:25.490 --> 00:28:27.590
to digital transformation
and deploying

00:28:27.590 --> 00:28:32.270
a billion dollar program, which
this was on that investment

00:28:32.270 --> 00:28:34.754
scale for the company.

00:28:34.754 --> 00:28:36.920
And you can do that with
complete confidence knowing

00:28:36.920 --> 00:28:40.010
that this Magic Band is
going to allow people

00:28:40.010 --> 00:28:42.860
to be able to go around the
park and use it as payment,

00:28:42.860 --> 00:28:47.090
use it as access,
use it as preferences

00:28:47.090 --> 00:28:50.930
on all kinds of information,
tracking that certainly gives

00:28:50.930 --> 00:28:52.880
a lot of useful
information to the company

00:28:52.880 --> 00:28:57.060
as they merchandise more
effectively and so on.

00:28:57.060 --> 00:29:01.730
But that ability to
translate the purpose

00:29:01.730 --> 00:29:04.550
into a deployment at
a billion dollar scale

00:29:04.550 --> 00:29:08.790
is very clear from that program.

00:29:08.790 --> 00:29:10.190
So we can design
experiences that

00:29:10.190 --> 00:29:12.290
are aligned with
strategic purpose,

00:29:12.290 --> 00:29:16.250
so that we can actually see that
understanding of purpose scale

00:29:16.250 --> 00:29:17.780
to massive levels.

00:29:17.780 --> 00:29:20.940
And purpose is the shape
meaning takes in business.

00:29:20.940 --> 00:29:23.450
So that's how we get
that meaning to be felt

00:29:23.450 --> 00:29:25.460
and understood at a human level.

00:29:25.460 --> 00:29:27.550
By the way, I keep
talking about scale,

00:29:27.550 --> 00:29:30.270
so I want to unpack
that a little bit, too.

00:29:30.270 --> 00:29:34.400
So when we think about
creating meaningful experiences

00:29:34.400 --> 00:29:36.410
at scale--

00:29:36.410 --> 00:29:38.920
normally when we talk
about scale in a startup

00:29:38.920 --> 00:29:42.650
or a corporate business, a
corporate growth scenario,

00:29:42.650 --> 00:29:45.470
we are talking about
removing hard limits,

00:29:45.470 --> 00:29:47.840
so that growth
opportunities can flourish.

00:29:47.840 --> 00:29:50.390
And usually, we're talking about
that in terms of multiples,

00:29:50.390 --> 00:29:53.780
let's say, like 3x
or 4x or 5x or 10x,

00:29:53.780 --> 00:29:56.810
if you're very, very lucky.

00:29:56.810 --> 00:29:59.600
But what happens
when a notion meets

00:29:59.600 --> 00:30:02.570
nearly unlimited expansion
possibility, when

00:30:02.570 --> 00:30:05.370
data can model it and
software can accelerate it

00:30:05.370 --> 00:30:09.057
and automation can amplify
it and culture can adapt it?

00:30:09.057 --> 00:30:10.640
And that's what,
really, we're talking

00:30:10.640 --> 00:30:12.350
about with machine-led
experiences.

00:30:12.350 --> 00:30:15.650
And that's why it's so important
that we think about creating

00:30:15.650 --> 00:30:17.390
these in a more meaningful way.

00:30:17.390 --> 00:30:20.310
Because if we don't create
the meaning into the system,

00:30:20.310 --> 00:30:21.060
what are we doing?

00:30:21.060 --> 00:30:23.210
We're allowing
absurdity to encroach,

00:30:23.210 --> 00:30:26.490
and we don't want
absurdity to scale.

00:30:26.490 --> 00:30:29.669
So my favorite example
of absurdity at scale

00:30:29.669 --> 00:30:30.210
is one that--

00:30:30.210 --> 00:30:33.630
I don't mean to knock the
program or the product,

00:30:33.630 --> 00:30:35.400
because I think
it's incredible--

00:30:35.400 --> 00:30:36.930
the Amazon Go store.

00:30:36.930 --> 00:30:39.390
How many of you have
experienced it in person?

00:30:39.390 --> 00:30:40.849
It's pretty cool, right?

00:30:40.849 --> 00:30:43.140
The idea that you can actually
just walk into a grocery

00:30:43.140 --> 00:30:47.250
store, you scan your
app as you go in

00:30:47.250 --> 00:30:51.330
and then just pick up whatever
you need and walk right out.

00:30:51.330 --> 00:30:54.060
And there's no
checking out process.

00:30:54.060 --> 00:30:55.530
It just knows.

00:30:55.530 --> 00:30:57.210
Through cameras and
sensors and so on,

00:30:57.210 --> 00:30:59.160
it knows what you've
picked up and what

00:30:59.160 --> 00:31:02.680
to associate with your
account, and you're good to go.

00:31:02.680 --> 00:31:06.120
So obviously, we have to
talk about cashier jobs

00:31:06.120 --> 00:31:08.640
and what that means for
the future of human work

00:31:08.640 --> 00:31:10.639
as that goes to scale.

00:31:10.639 --> 00:31:12.930
But let's leave that set
aside for just another moment,

00:31:12.930 --> 00:31:15.960
because right now, what I'm
focused on is something else.

00:31:15.960 --> 00:31:19.320
What happens when you open
the app for the first time

00:31:19.320 --> 00:31:23.130
and you get this onboarding
that explains that,

00:31:23.130 --> 00:31:25.230
as you pick things
up off the shelf,

00:31:25.230 --> 00:31:26.940
the sensors know what
you've picked up.

00:31:26.940 --> 00:31:30.330
And as you put it into
your basket or in your bag,

00:31:30.330 --> 00:31:31.800
you'll be charged for it.

00:31:31.800 --> 00:31:36.400
So it says, don't pick up
anything for anyone else, which

00:31:36.400 --> 00:31:38.380
is fine, except that you
start thinking about,

00:31:38.380 --> 00:31:39.775
I don't know about
you, but I get

00:31:39.775 --> 00:31:41.650
asked all the time to
help people in stories.

00:31:41.650 --> 00:31:42.525
You seem pretty tall.

00:31:42.525 --> 00:31:44.860
You probably get asked for that.

00:31:44.860 --> 00:31:45.580
You get asked.

00:31:45.580 --> 00:31:47.680
And now, it's like,
well, I can't really

00:31:47.680 --> 00:31:50.260
help that person to get
the thing off the shelf,

00:31:50.260 --> 00:31:52.090
because it might
charge me for it.

00:31:52.090 --> 00:31:54.210
And there's a way to
get it charged back,

00:31:54.210 --> 00:31:56.460
and Amazon might fix this
before it goes to scale.

00:31:56.460 --> 00:31:59.950
But really, 3,000
Amazon Go stores

00:31:59.950 --> 00:32:02.200
have been announced before 2021.

00:32:02.200 --> 00:32:06.250
So if this doesn't get fixed and
if it is something that we all

00:32:06.250 --> 00:32:09.220
start adjusting our
behavior and not

00:32:09.220 --> 00:32:12.130
helping other people
in the Amazon Go store,

00:32:12.130 --> 00:32:14.730
well, that's the future of
retail we're talking about.

00:32:14.730 --> 00:32:17.860
3,000 Amazon Go
stores by 2021 is

00:32:17.860 --> 00:32:20.740
going to mean that retail
environments are going

00:32:20.740 --> 00:32:24.340
to be this cashierless
environment before too long,

00:32:24.340 --> 00:32:26.770
and so we won't help
each other in any stores.

00:32:26.770 --> 00:32:29.830
And how long is it before we
don't help each other at all?

00:32:29.830 --> 00:32:32.300
I know it sounds like
hyperbole at some level.

00:32:32.300 --> 00:32:36.100
But what I mean to suggest is
that the idea that experience

00:32:36.100 --> 00:32:38.550
at scale does change culture.

00:32:38.550 --> 00:32:41.440
And I think that's important
to recognize, because really,

00:32:41.440 --> 00:32:44.370
experience at scale is culture.

00:32:44.370 --> 00:32:47.500
What we all collectively
agree to do with each other

00:32:47.500 --> 00:32:50.920
and how we agree to interact
with each other is culture.

00:32:50.920 --> 00:32:54.460
And all of the work that we
do creating human experiences

00:32:54.460 --> 00:32:57.820
sets that context and
creates that modality,

00:32:57.820 --> 00:33:01.930
so that understanding is
super, super important.

00:33:01.930 --> 00:33:03.100
So I do have a slide here.

00:33:03.100 --> 00:33:05.080
And if anybody
needs these slides,

00:33:05.080 --> 00:33:06.830
I'm happy to share them.

00:33:06.830 --> 00:33:10.120
But a slide that asks questions
and it's in the book as well.

00:33:10.120 --> 00:33:15.850
If we were to try to deprogram
the absurdity of not helping

00:33:15.850 --> 00:33:17.950
each other, we could
ask some questions

00:33:17.950 --> 00:33:19.990
to step back from
that and think,

00:33:19.990 --> 00:33:23.200
how do we not create
experience at scale

00:33:23.200 --> 00:33:24.564
that's going to be absurd?

00:33:24.564 --> 00:33:26.230
How do we make sure
that the brand isn't

00:33:26.230 --> 00:33:27.730
going to be impacted
if we create

00:33:27.730 --> 00:33:29.710
products or solutions
that might scale

00:33:29.710 --> 00:33:31.750
in ways that are unexpected?

00:33:31.750 --> 00:33:33.740
How do we pivot
to deal with that?

00:33:33.740 --> 00:33:34.820
What does that look like?

00:33:34.820 --> 00:33:38.320
So there's some questions we
can ask to anticipate that.

00:33:38.320 --> 00:33:41.040
But primarily, I
think the challenge is

00:33:41.040 --> 00:33:43.930
or the opportunity is
to think about meaning

00:33:43.930 --> 00:33:46.840
and to think about keeping
absurdity from scaling.

00:33:46.840 --> 00:33:49.210
This is a comic that
was drawn for me

00:33:49.210 --> 00:33:52.090
by my friend Rob
Cottingham to illustrate

00:33:52.090 --> 00:33:54.747
the opposite of tech humanism.

00:33:54.747 --> 00:33:56.080
I don't know if you can read it.

00:33:56.080 --> 00:33:57.746
It says, "It's getting
harder and harder

00:33:57.746 --> 00:33:58.840
to hold on to my humanity.

00:33:58.840 --> 00:34:03.166
But wow is it easy to track
my Amazon deliveries."

00:34:03.166 --> 00:34:05.800
So of course, that's absurd.

00:34:05.800 --> 00:34:08.800
But it's the idea that
we aren't thinking about,

00:34:08.800 --> 00:34:11.139
what do we really want
meaningful experiences to look

00:34:11.139 --> 00:34:11.639
like?

00:34:11.639 --> 00:34:14.420
What do we really want our
future humanity to look like?

00:34:14.420 --> 00:34:16.900
How do we create
technology solutions

00:34:16.900 --> 00:34:19.000
that amplify our
humanity and don't

00:34:19.000 --> 00:34:20.364
get in the way of humanity?

00:34:20.364 --> 00:34:24.389
I think, that's really
what we're talking about.

00:34:24.389 --> 00:34:29.219
The second premise of
machine-led meaningful human

00:34:29.219 --> 00:34:33.179
experiences is to automate
empathy, which again, may sound

00:34:33.179 --> 00:34:34.763
like it's a contradiction.

00:34:34.763 --> 00:34:36.179
But I think there's
an opportunity

00:34:36.179 --> 00:34:39.840
to think about the ways that
any kind of experience that we

00:34:39.840 --> 00:34:42.929
design creates some
kind of connection

00:34:42.929 --> 00:34:46.420
and to create as meaningful
a connection as possible.

00:34:46.420 --> 00:34:49.530
So how many of you remember
the "Seinfeld" episode

00:34:49.530 --> 00:34:52.320
where Kramer got a
new phone number,

00:34:52.320 --> 00:34:54.000
and it was one digit
off from Moviefone?

00:34:54.000 --> 00:34:56.130
Anybody remember this?

00:34:56.130 --> 00:34:59.160
So anybody remember Moviefone?

00:34:59.160 --> 00:35:03.090
I know the app just
ended as of a week ago.

00:35:03.090 --> 00:35:05.700
But in the '80s and
'90s or whatever,

00:35:05.700 --> 00:35:07.950
we all had to pick up the
phone and actually call

00:35:07.950 --> 00:35:10.590
a service to tell us
what movies were playing.

00:35:10.590 --> 00:35:13.480
And in this episode, Kramer
had gotten a new phone number.

00:35:13.480 --> 00:35:15.380
It was one digit
off from Moviefone.

00:35:15.380 --> 00:35:17.805
And it's obviously
touch tone service,

00:35:17.805 --> 00:35:20.060
so he couldn't understand
the touch tones.

00:35:20.060 --> 00:35:22.579
He decided that he was going
to impersonate Moviefone.

00:35:22.579 --> 00:35:24.370
But he couldn't understand
the touch tones,

00:35:24.370 --> 00:35:26.790
so he ends up just saying,
why don't you just tell me

00:35:26.790 --> 00:35:29.230
what movie you want to see.

00:35:29.230 --> 00:35:33.030
And I find this to be
such a prescient example

00:35:33.030 --> 00:35:36.000
of how we think about
machine-driven interactions

00:35:36.000 --> 00:35:39.120
and how we think about what
human-based interactions look

00:35:39.120 --> 00:35:41.950
like, the relationship
between those two.

00:35:41.950 --> 00:35:44.790
So I think of the
Moviefone, Kramer model

00:35:44.790 --> 00:35:49.050
as agile deployment of
emerging technology.

00:35:49.050 --> 00:35:54.360
That you can think about
the robotic interaction

00:35:54.360 --> 00:35:56.915
and the human interaction
as being somehow

00:35:56.915 --> 00:35:58.290
interchangeable
with one another,

00:35:58.290 --> 00:36:00.870
so that you can actually
use human interaction

00:36:00.870 --> 00:36:05.370
to gather patterns that
you will encode as chatbots

00:36:05.370 --> 00:36:07.700
or other types of automation.

00:36:07.700 --> 00:36:09.390
And not to suggest
that you would lie,

00:36:09.390 --> 00:36:12.510
that you would present
a human and have

00:36:12.510 --> 00:36:17.252
it be posing as Moviefone or
whatever your equivalent is,

00:36:17.252 --> 00:36:18.960
but rather, that you
would have some kind

00:36:18.960 --> 00:36:21.840
of agile human-based
interaction that

00:36:21.840 --> 00:36:23.790
gives you the insights
to be able to create

00:36:23.790 --> 00:36:27.810
scripts and create patterns
that help you develop frameworks

00:36:27.810 --> 00:36:30.077
for automation.

00:36:30.077 --> 00:36:32.410
And of course, you're starting
with if, then statements.

00:36:32.410 --> 00:36:34.660
But you're quickly trying
to work beyond the if,

00:36:34.660 --> 00:36:36.430
then to get to the nuance.

00:36:36.430 --> 00:36:38.300
So if, then is
easy to anticipate,

00:36:38.300 --> 00:36:43.660
in the kind of frequently asked
questions model of automation.

00:36:43.660 --> 00:36:46.690
If you're automating, let's
say, a chatbot for a bank,

00:36:46.690 --> 00:36:48.700
you know that a lot
of your interactions

00:36:48.700 --> 00:36:51.040
are going to be about how
to change your password,

00:36:51.040 --> 00:36:53.510
for example, or how to
set up a new account.

00:36:53.510 --> 00:36:55.960
So if someone wants to
create a new account,

00:36:55.960 --> 00:36:59.110
then here's the answer and
here's the flow diagram

00:36:59.110 --> 00:37:00.820
that you can walk them through.

00:37:00.820 --> 00:37:04.630
But the nuance beyond that is,
I need to change my password,

00:37:04.630 --> 00:37:09.040
because my ex is stalking me,
and it's a dangerous situation.

00:37:09.040 --> 00:37:11.910
And there needs to be some
human interaction there.

00:37:11.910 --> 00:37:15.400
There needs to be some human
nuance to that experience.

00:37:15.400 --> 00:37:18.040
So that's more where the
empathy gets automated

00:37:18.040 --> 00:37:21.640
into the process, is finding
those types of interactions

00:37:21.640 --> 00:37:23.680
and finding the
opportunity to build out

00:37:23.680 --> 00:37:26.700
the relationship between
the automated and the human.

00:37:26.700 --> 00:37:28.450
And also, when we're
looking for patterns,

00:37:28.450 --> 00:37:31.480
that we're not just looking
for arbitrary patterns

00:37:31.480 --> 00:37:32.950
and encoding those.

00:37:32.950 --> 00:37:35.530
Arbitrariness also
sits in opposition

00:37:35.530 --> 00:37:38.069
to meaningfulness in much
the same way that absurdity

00:37:38.069 --> 00:37:39.610
sits in opposition
to meaningfulness.

00:37:39.610 --> 00:37:41.026
So we want to make
sure that we're

00:37:41.026 --> 00:37:44.310
finding meaningful patterns
and automating those.

00:37:44.310 --> 00:37:47.520
Because in all of the
work that we do with this,

00:37:47.520 --> 00:37:50.790
we cannot leave
meaning up to machines.

00:37:50.790 --> 00:37:53.130
Machines won't do meaning.

00:37:53.130 --> 00:37:55.590
That's just not something that
machines are really equipped

00:37:55.590 --> 00:37:56.400
for.

00:37:56.400 --> 00:37:59.525
So it has to be humans that
determine what is meaningful.

00:37:59.525 --> 00:38:01.380
And I love this example.

00:38:01.380 --> 00:38:05.130
I know many of you probably work
around image recognition or AI,

00:38:05.130 --> 00:38:08.490
and so you know this
dilemma very, very well.

00:38:08.490 --> 00:38:11.700
I know a lot of algorithms
have advanced since this day.

00:38:11.700 --> 00:38:16.290
The puppy versus muffin
problem is one of my faves,

00:38:16.290 --> 00:38:17.880
and it always gets a chuckle.

00:38:17.880 --> 00:38:20.220
I see some smiles
in the audience.

00:38:20.220 --> 00:38:21.900
But it's true,
that subtle nuances

00:38:21.900 --> 00:38:25.560
aren't really where AI shines,
in many cases at this point,

00:38:25.560 --> 00:38:29.340
at least not at a meaningful
recognition level,

00:38:29.340 --> 00:38:33.060
not being able to say
that the muffins all

00:38:33.060 --> 00:38:35.760
have this certain meaningful
characteristic and the human,

00:38:35.760 --> 00:38:37.593
the puppies all have
this certain meaningful

00:38:37.593 --> 00:38:38.310
characteristic.

00:38:38.310 --> 00:38:40.350
Whereas, I believe many
of you are probably

00:38:40.350 --> 00:38:42.120
able to determine
which one's a muffin

00:38:42.120 --> 00:38:43.619
and which one's a
puppy pretty well.

00:38:43.619 --> 00:38:44.995
Here's some more, by the way.

00:38:44.995 --> 00:38:47.370
You know which one's a barn
owl and which one's an apple.

00:38:47.370 --> 00:38:49.800
You're not having any
trouble with that, I bet--

00:38:49.800 --> 00:38:52.500
which one's a croissant.

00:38:52.500 --> 00:38:54.360
But I think this
introduces an idea

00:38:54.360 --> 00:39:00.420
that there may be
opportunities for humans

00:39:00.420 --> 00:39:03.990
to work alongside
machines in ways

00:39:03.990 --> 00:39:07.740
that add nuance and
empathy and understanding

00:39:07.740 --> 00:39:11.850
to the machine-led processes,
because humans generally

00:39:11.850 --> 00:39:12.930
do nuance pretty well.

00:39:12.930 --> 00:39:14.820
That's something that
we are encoded for.

00:39:14.820 --> 00:39:15.870
We get meaning.

00:39:15.870 --> 00:39:18.330
That's what we're
about, so we're

00:39:18.330 --> 00:39:20.440
able to add that into
the value proposition.

00:39:20.440 --> 00:39:23.100
So when we think about the
relationship between machines

00:39:23.100 --> 00:39:27.720
and humans as we move
into the future of work

00:39:27.720 --> 00:39:29.940
and the future of
that economy, I

00:39:29.940 --> 00:39:33.150
think we're going to add the
most value by being human

00:39:33.150 --> 00:39:35.070
and understanding
meaning and nuance

00:39:35.070 --> 00:39:37.620
and understanding value and
understanding each other

00:39:37.620 --> 00:39:41.650
and adding that layer
to those interactions.

00:39:41.650 --> 00:39:44.250
So the third tenet of the
machine-led meaningful

00:39:44.250 --> 00:39:49.390
experience is to use
human data respectfully.

00:39:49.390 --> 00:39:51.280
It comes from this
idea that when

00:39:51.280 --> 00:39:52.880
we talk about digital
transformation,

00:39:52.880 --> 00:39:55.330
I kind of feel like that's
a little bit of a misnomer

00:39:55.330 --> 00:39:56.971
at some level,
because we already

00:39:56.971 --> 00:39:59.470
made a digital transformation
the moment we started spending

00:39:59.470 --> 00:40:01.330
all of our time in
front of screens,

00:40:01.330 --> 00:40:04.250
transacting in bits and
bytes with each other.

00:40:04.250 --> 00:40:06.160
So that's kind of a done deal.

00:40:06.160 --> 00:40:07.750
And what's really
more meaningful

00:40:07.750 --> 00:40:10.990
than that is the data
transformation, the fact

00:40:10.990 --> 00:40:14.200
that all of this is happening
with a data layer behind it,

00:40:14.200 --> 00:40:16.660
that business has all
kinds of data visibility

00:40:16.660 --> 00:40:18.610
and transparency through
the supply chain,

00:40:18.610 --> 00:40:20.200
through logistics,
through operations.

00:40:20.200 --> 00:40:24.490
And everything has this
clarity and transparency

00:40:24.490 --> 00:40:28.720
about what kind of track ability
is going on, what's measurable

00:40:28.720 --> 00:40:30.010
and all that.

00:40:30.010 --> 00:40:33.490
So it's a really interesting
layer to work with.

00:40:33.490 --> 00:40:37.300
But when we talk about digital
transformation, including

00:40:37.300 --> 00:40:39.640
automation and
digitization, all of that,

00:40:39.640 --> 00:40:42.460
all of the many nuances
of that, fundamentally

00:40:42.460 --> 00:40:45.040
what we're talking about
is agility with data.

00:40:45.040 --> 00:40:48.490
As companies become more
digitally ready and digitally

00:40:48.490 --> 00:40:51.160
transformed, they're
becoming more agile

00:40:51.160 --> 00:40:53.620
with database decisions.

00:40:53.620 --> 00:40:58.300
And that data that we're talking
about is really our data.

00:40:58.300 --> 00:40:59.230
It's human data.

00:40:59.230 --> 00:41:03.830
For the most part, business
data is largely about people.

00:41:03.830 --> 00:41:05.110
It's our purchases.

00:41:05.110 --> 00:41:06.620
It's our movements
through space.

00:41:06.620 --> 00:41:07.570
It's our preferences.

00:41:07.570 --> 00:41:11.860
It's all of our tastes and
indications that we've made.

00:41:11.860 --> 00:41:16.180
And really, what I'm saying
is analytics are people.

00:41:16.180 --> 00:41:18.230
At some level,
for the most part,

00:41:18.230 --> 00:41:21.050
when we are looking at
graphs and reports and so on,

00:41:21.050 --> 00:41:23.560
we're generally looking
at the needs and interests

00:41:23.560 --> 00:41:26.470
and motivations of real
people that are there

00:41:26.470 --> 00:41:29.230
buying from our companies
and interacting with them

00:41:29.230 --> 00:41:32.380
and driving all of
these decisions for us.

00:41:32.380 --> 00:41:35.440
And I think the flip side of
appreciating that and treating

00:41:35.440 --> 00:41:37.600
that data with respect
is understanding

00:41:37.600 --> 00:41:40.780
that what we encode
into machines

00:41:40.780 --> 00:41:44.260
is really about us, that
we are putting ourselves

00:41:44.260 --> 00:41:47.770
and our biases
into the encoding,

00:41:47.770 --> 00:41:51.470
into the algorithms and
everything that we create.

00:41:51.470 --> 00:41:53.860
So the opportunity,
I think, as we

00:41:53.860 --> 00:41:56.350
look at this tech
humanist future,

00:41:56.350 --> 00:41:58.960
is to encode the
best of ourselves,

00:41:58.960 --> 00:42:01.060
is to think about
how we can create

00:42:01.060 --> 00:42:03.670
our most egalitarian
viewpoints and our most

00:42:03.670 --> 00:42:07.270
evolved understandings
into the data we model,

00:42:07.270 --> 00:42:10.900
into the algorithms we
build, and into the automated

00:42:10.900 --> 00:42:13.990
experiences that we
design and create.

00:42:13.990 --> 00:42:21.280
So we can use our human data to
make more meaning in the world.

00:42:21.280 --> 00:42:26.050
And we can recognize that
the more we create relevance

00:42:26.050 --> 00:42:27.580
in the alignment
between business

00:42:27.580 --> 00:42:30.130
objectives and human
objectives, the more we

00:42:30.130 --> 00:42:32.680
are creating a form of respect.

00:42:32.680 --> 00:42:35.830
But the caveat to that
is that discretion

00:42:35.830 --> 00:42:38.980
is a form of respect, too,
that we're also allowing people

00:42:38.980 --> 00:42:42.760
to, say, be forgotten
by us and allowing them

00:42:42.760 --> 00:42:45.616
to take their data with them,
and that we can not make people

00:42:45.616 --> 00:42:46.990
feel like we're
creeping them out

00:42:46.990 --> 00:42:49.750
by knowing so much about them.

00:42:49.750 --> 00:42:53.290
And that we protect
human data excessively,

00:42:53.290 --> 00:42:55.330
that we make sure we're
being very, very, very

00:42:55.330 --> 00:42:58.000
careful with the data
that we collect and use

00:42:58.000 --> 00:42:59.860
in business decisions,
because we recognize

00:42:59.860 --> 00:43:02.320
that it is human data.

00:43:02.320 --> 00:43:04.330
The last point-- and
it's a quick one,

00:43:04.330 --> 00:43:08.450
because this may or may not be
within scope for many of you.

00:43:08.450 --> 00:43:11.560
But that as we think
about the gains

00:43:11.560 --> 00:43:14.170
that we make in our
businesses through automation

00:43:14.170 --> 00:43:16.990
and machine-led experiences,
that we think about reinvesting

00:43:16.990 --> 00:43:19.870
some of those gains into how
to create more meaningful

00:43:19.870 --> 00:43:22.220
human experiences at scale.

00:43:22.220 --> 00:43:24.730
And I don't think it's
really a mystery why

00:43:24.730 --> 00:43:26.350
that's so important.

00:43:26.350 --> 00:43:28.560
There was a study done,
a couple of versions

00:43:28.560 --> 00:43:31.782
of a study done on what
jobs are potentially

00:43:31.782 --> 00:43:32.740
considered automatable.

00:43:32.740 --> 00:43:37.540
And this is one visualization
of the data from that study

00:43:37.540 --> 00:43:41.020
that shows the different
cities in the United States

00:43:41.020 --> 00:43:43.510
and how likely the
jobs that are there

00:43:43.510 --> 00:43:46.060
are to be automated
over the coming years.

00:43:46.060 --> 00:43:48.820
I zoomed in on New York,
which is where I'm from,

00:43:48.820 --> 00:43:51.640
and you see 55% of
jobs are considered

00:43:51.640 --> 00:43:52.860
potentially automatable.

00:43:52.860 --> 00:43:55.790
And you have to think about the
socioeconomic impact of that.

00:43:55.790 --> 00:43:58.360
You have to think about the
psychological impact of that,

00:43:58.360 --> 00:44:02.290
that humans have had a very
deeply connected experience

00:44:02.290 --> 00:44:05.380
to work, that we've derived
a lot of our sense of meaning

00:44:05.380 --> 00:44:06.700
and identity from work.

00:44:06.700 --> 00:44:09.520
We say who we are in
terms of what we do,

00:44:09.520 --> 00:44:12.640
and we have names like butcher,
baker, tanner, carpenter,

00:44:12.640 --> 00:44:16.300
and so on, that derive
from ancestral jobs, that

00:44:16.300 --> 00:44:20.060
have been carried down through
generations of our family.

00:44:20.060 --> 00:44:21.692
And that's true across cultures.

00:44:21.692 --> 00:44:23.650
So it's a really important
thing to understand,

00:44:23.650 --> 00:44:25.785
that jobs are going to change.

00:44:25.785 --> 00:44:27.910
There's going to be job
displacement, augmentation,

00:44:27.910 --> 00:44:29.980
and replacement by automation.

00:44:29.980 --> 00:44:32.580
And we don't yet know what
that means for human meaning,

00:44:32.580 --> 00:44:35.080
and we don't yet know what
that means economically.

00:44:35.080 --> 00:44:38.934
We don't yet know what that
means sociopolitically.

00:44:38.934 --> 00:44:40.600
And so there's a huge
opportunity for us

00:44:40.600 --> 00:44:43.210
to take the gains that
we make in automation

00:44:43.210 --> 00:44:46.330
and have this ethical
contribution back into society,

00:44:46.330 --> 00:44:48.100
back to humanity
and say, what can we

00:44:48.100 --> 00:44:51.520
do to foster a sense of meaning
and a sense of community

00:44:51.520 --> 00:44:53.800
and a sense of connectedness
and a sense of more

00:44:53.800 --> 00:44:57.450
humanity with those gains?

00:44:57.450 --> 00:44:59.990
So I think we can also think
about repurposing human skills

00:44:59.990 --> 00:45:04.170
and qualities into
higher value roles.

00:45:04.170 --> 00:45:08.090
One of the executives that was
in a strategic workshop I led

00:45:08.090 --> 00:45:10.390
ran a utilities company
in South America.

00:45:10.390 --> 00:45:13.760
And he found, through
our work, an opportunity

00:45:13.760 --> 00:45:16.550
to automate a customer
service function that

00:45:16.550 --> 00:45:19.580
was their most heavily
accessed customer support

00:45:19.580 --> 00:45:21.110
question and function.

00:45:21.110 --> 00:45:22.730
And once he saw
that opportunity,

00:45:22.730 --> 00:45:25.280
he saw that there was a
way to take the humans that

00:45:25.280 --> 00:45:28.149
were working in that job and
create oversight positions

00:45:28.149 --> 00:45:29.690
for them, so that
they could continue

00:45:29.690 --> 00:45:31.314
training the algorithms
that were going

00:45:31.314 --> 00:45:33.380
to create that automation.

00:45:33.380 --> 00:45:35.420
So obviously, a very
straightforward kind

00:45:35.420 --> 00:45:38.480
of replacement, it may
not be a one-for-one.

00:45:38.480 --> 00:45:40.370
We may see job loss anyway.

00:45:40.370 --> 00:45:42.290
But some of that
reinvestment is going

00:45:42.290 --> 00:45:47.100
to offer up higher
value human roles.

00:45:47.100 --> 00:45:50.780
So here are those
four tenets again.

00:45:50.780 --> 00:45:54.292
And I think the summary of
this really comes back to,

00:45:54.292 --> 00:45:56.250
as you think about the
work you're trying to do

00:45:56.250 --> 00:45:59.241
and how to create these
more meaningful experiences

00:45:59.241 --> 00:46:01.490
through automation and through
artificial intelligence

00:46:01.490 --> 00:46:04.100
and so on, it really comes
down to this question of,

00:46:04.100 --> 00:46:07.757
what is it that you are
trying to do at scale?

00:46:07.757 --> 00:46:09.090
So that's the purpose statement.

00:46:09.090 --> 00:46:10.647
How can you
articulate what it is

00:46:10.647 --> 00:46:12.980
your company is trying to do,
your team is trying to do,

00:46:12.980 --> 00:46:15.230
you are trying to do at scale?

00:46:15.230 --> 00:46:17.270
And for me, the answer
to that question

00:46:17.270 --> 00:46:20.390
is, create more meaningful
human experiences.

00:46:20.390 --> 00:46:21.600
It's just as simple as that.

00:46:21.600 --> 00:46:23.000
But the way that
I can do that is

00:46:23.000 --> 00:46:24.980
by speaking with
groups like yourself,

00:46:24.980 --> 00:46:27.080
working with executives,
working with leaders

00:46:27.080 --> 00:46:30.097
and being able to help them
hone in on that purpose

00:46:30.097 --> 00:46:32.180
and really get clearer on
how to create those more

00:46:32.180 --> 00:46:34.940
meaningful experiences
that do align the business

00:46:34.940 --> 00:46:37.430
objectives with the human
objectives, that do bring

00:46:37.430 --> 00:46:40.340
the business results and
that create a better future

00:46:40.340 --> 00:46:42.000
for humanity.

00:46:42.000 --> 00:46:44.960
So because business will have
to scale through digitization

00:46:44.960 --> 00:46:47.690
and automation, business
won't be successful long term

00:46:47.690 --> 00:46:50.100
without it, it's table stakes.

00:46:50.100 --> 00:46:54.200
But humanity won't be
successful without meaning.

00:46:54.200 --> 00:46:57.960
So for that, I thank all of
you for the work that you do.

00:46:57.960 --> 00:47:00.272
Thank you very much.

00:47:00.272 --> 00:47:03.660
[APPLAUSE]

00:47:03.660 --> 00:47:05.460
TIM: Thanks, Kate.

00:47:05.460 --> 00:47:06.102
We have a Dory.

00:47:06.102 --> 00:47:07.560
There no questions
on it right now,

00:47:07.560 --> 00:47:09.580
so we can ask a few
local questions.

00:47:09.580 --> 00:47:14.040
And if anything shows up, then
we'll get those included, too.

00:47:14.040 --> 00:47:16.620
But I see a hand right here
and can we get a mic over--

00:47:16.620 --> 00:47:18.551
SPEAKER: I have the
mic, test, test, test.

00:47:18.551 --> 00:47:19.050
Sounds good.

00:47:23.803 --> 00:47:26.240
AUDIENCE: Thank you for
the talk, first off.

00:47:26.240 --> 00:47:27.830
So for the rhetorical
question you

00:47:27.830 --> 00:47:29.910
asked at the very
beginning of the talk,

00:47:29.910 --> 00:47:31.670
I know a lot of people,
if not most people

00:47:31.670 --> 00:47:33.650
I know, would answer, a soul.

00:47:33.650 --> 00:47:36.380
What makes humans
human is a soul.

00:47:36.380 --> 00:47:41.605
So what advice would you give
about how to automate religion?

00:47:41.605 --> 00:47:43.730
KATE O'NEILL: It's a very
interesting question.

00:47:43.730 --> 00:47:47.810
I actually have talked with
a few people about the work

00:47:47.810 --> 00:47:49.910
that they're doing
around automation

00:47:49.910 --> 00:47:52.670
and creating experiences
for people at scale

00:47:52.670 --> 00:47:54.180
around religion.

00:47:54.180 --> 00:47:58.280
And I don't feel like I'm
in a really good position

00:47:58.280 --> 00:47:59.960
to be an expert on that.

00:47:59.960 --> 00:48:01.680
It's not the work that I do.

00:48:01.680 --> 00:48:05.510
But I do think that religion is
fundamentally offering meaning,

00:48:05.510 --> 00:48:07.970
so really, we're talking
about the same principles.

00:48:07.970 --> 00:48:09.950
We're talking about being
able to offer people

00:48:09.950 --> 00:48:13.940
a lens into what is meaningful
and then helping to scale that.

00:48:13.940 --> 00:48:17.450
So if there is a solution that
someone is trying to build,

00:48:17.450 --> 00:48:22.220
that is some sort of
technology product for creating

00:48:22.220 --> 00:48:25.340
a religious experience or a
religious outlet for people

00:48:25.340 --> 00:48:27.590
or community, then I
think it really comes down

00:48:27.590 --> 00:48:28.940
to the same principles.

00:48:28.940 --> 00:48:31.670
It's just like religion is
the industry, in that sense,

00:48:31.670 --> 00:48:33.890
and we're trying
to offer meaning

00:48:33.890 --> 00:48:35.600
through those experiences.

00:48:35.600 --> 00:48:38.120
That would be, probably,
my best take on that.

00:48:38.120 --> 00:48:40.520
But I think it's a more
interesting question than that

00:48:40.520 --> 00:48:42.230
at some fundamental
layer, and it

00:48:42.230 --> 00:48:45.670
sounds like a discussion over
beers or something like that.

00:48:45.670 --> 00:48:48.480
So do we have another question?

00:48:48.480 --> 00:48:51.245
Yes, to your left.

00:48:51.245 --> 00:48:53.430
AUDIENCE: Hi,
thanks for the talk.

00:48:53.430 --> 00:48:55.910
I was really struck
by your point

00:48:55.910 --> 00:48:58.520
about shared experiences
forming culture.

00:48:58.520 --> 00:49:02.600
And obviously, we in
the technology world,

00:49:02.600 --> 00:49:07.560
have a lot of increasingly
shape shared experience.

00:49:07.560 --> 00:49:12.870
So in the Amazon Go point about
people not helping each other,

00:49:12.870 --> 00:49:14.480
that's something
we can all probably

00:49:14.480 --> 00:49:15.980
agree is a net negative.

00:49:15.980 --> 00:49:18.650
And even Amazon, I'm
sure, would agree.

00:49:18.650 --> 00:49:22.611
But there's a lot of cases
where we have the potential

00:49:22.611 --> 00:49:24.860
to shape culture, where the
answer really isn't clear,

00:49:24.860 --> 00:49:27.710
what is the right thing to
do, filter bubbles being one

00:49:27.710 --> 00:49:29.240
controversial idea,
whether they're

00:49:29.240 --> 00:49:31.820
a good thing or a bad thing.

00:49:31.820 --> 00:49:34.519
So I'm wondering, what's
your take on how we

00:49:34.519 --> 00:49:35.810
should approach these problems?

00:49:35.810 --> 00:49:39.860
What principle should we use in
deciding how to shape culture

00:49:39.860 --> 00:49:42.380
or what processes or
institutions maybe we

00:49:42.380 --> 00:49:44.816
need to make these decisions?

00:49:44.816 --> 00:49:48.020
KATE O'NEILL: Thank you, it's
a really good and big question.

00:49:48.020 --> 00:49:52.280
I'd like to cop out and say
that the entire book, "Tech

00:49:52.280 --> 00:49:53.870
Humanist," addresses it.

00:49:53.870 --> 00:49:57.560
But at some level,
what it comes down

00:49:57.560 --> 00:50:01.190
to is trying to understand
that strategic purpose,

00:50:01.190 --> 00:50:05.460
that alignment between business
objective and human objective.

00:50:05.460 --> 00:50:08.780
And I think, if you're
looking at a filter bubble

00:50:08.780 --> 00:50:12.020
type of example as one
example of something

00:50:12.020 --> 00:50:18.860
where a social platform or an
online community or a media

00:50:18.860 --> 00:50:22.580
company is fostering through
algorithmic content filtering

00:50:22.580 --> 00:50:25.370
and so on, this
sense of disparity

00:50:25.370 --> 00:50:29.150
between people's collective
understanding of what is truth,

00:50:29.150 --> 00:50:32.340
I think you can probably
come to some understanding

00:50:32.340 --> 00:50:35.780
at some level of view of that.

00:50:35.780 --> 00:50:39.080
The business objective, which
may be advertising or something

00:50:39.080 --> 00:50:43.310
along those lines, and the human
objective aren't aligned there.

00:50:43.310 --> 00:50:48.590
So I do think that there is
still a useful framework there,

00:50:48.590 --> 00:50:51.600
but I do offer some additional
ones in "Tech Humanist"

00:50:51.600 --> 00:50:53.210
as well.

00:50:53.210 --> 00:50:55.760
It is a really good
and important question.

00:50:55.760 --> 00:50:57.830
And it's an important
point for us

00:50:57.830 --> 00:50:59.510
all, I think, to
consider in the work

00:50:59.510 --> 00:51:01.430
that we're doing,
because there are

00:51:01.430 --> 00:51:05.850
so many net positives and
net goods that come out of,

00:51:05.850 --> 00:51:08.900
let's say, with social
media and the connectedness

00:51:08.900 --> 00:51:11.360
we have with each
other and the way we're

00:51:11.360 --> 00:51:16.670
able to maintain relationships
with such ease versus 20

00:51:16.670 --> 00:51:18.150
years ago.

00:51:18.150 --> 00:51:23.330
But of course, it does come with
these associated difficulties

00:51:23.330 --> 00:51:25.100
and the challenges
of making sure

00:51:25.100 --> 00:51:27.987
that we're all speaking the same
language, which at the moment,

00:51:27.987 --> 00:51:28.820
I believe we're not.

00:51:28.820 --> 00:51:32.906
We were having that
discussion beforehand.

00:51:32.906 --> 00:51:34.280
So I'm going to
leave it at that.

00:51:34.280 --> 00:51:37.050
I think there's a lot in
the book, which I'll just

00:51:37.050 --> 00:51:40.320
keep pointing back to
that, that does unpack that

00:51:40.320 --> 00:51:41.190
a little further.

00:51:41.190 --> 00:51:44.130
But I genuinely think that that
framework of understanding what

00:51:44.130 --> 00:51:46.500
it is the business is
trying to accomplish

00:51:46.500 --> 00:51:49.570
and what it is that's good for
humanity, how those things can

00:51:49.570 --> 00:51:50.070
be in line.

00:51:50.070 --> 00:51:53.710
And it doesn't have to come
down to a humanitarian purpose.

00:51:53.710 --> 00:51:55.260
It just has to
mean that we're not

00:51:55.260 --> 00:51:57.690
accelerating
something that is not

00:51:57.690 --> 00:51:59.800
ultimately good for humanity.

00:51:59.800 --> 00:52:03.932
That, I think, is where the
alignment comes back to.

00:52:03.932 --> 00:52:05.390
TIM: There's a
question on the Dory

00:52:05.390 --> 00:52:07.430
and it's similar to a
question that I had.

00:52:07.430 --> 00:52:10.740
So I'm going to try to
merge them together.

00:52:10.740 --> 00:52:14.570
The question on the
Dory starts like this.

00:52:14.570 --> 00:52:18.080
Scale tends to force humans
to reduce their variety

00:52:18.080 --> 00:52:22.200
to adapt to machines instead
of the other way around.

00:52:22.200 --> 00:52:25.310
So further, what about
ways to reduce scale that

00:52:25.310 --> 00:52:28.550
are compatible with business?

00:52:28.550 --> 00:52:33.650
You mention distributism,
decentralization, or something

00:52:33.650 --> 00:52:34.580
else.

00:52:34.580 --> 00:52:36.770
And I'll add that
I think that a lot

00:52:36.770 --> 00:52:41.330
of this technological change
is really coercive, meaning,

00:52:41.330 --> 00:52:43.070
either you get
with it or you get

00:52:43.070 --> 00:52:45.530
left out, particularly
around the job changes

00:52:45.530 --> 00:52:46.850
that you talked about.

00:52:46.850 --> 00:52:49.940
And how much is
our responsibility

00:52:49.940 --> 00:52:53.440
to bring people along,
like to offer the lifeboat,

00:52:53.440 --> 00:52:56.861
and how much do people really
need to get in the boat?

00:52:56.861 --> 00:53:01.300
KATE O'NEILL: I think that's
a really difficult thing

00:53:01.300 --> 00:53:04.660
to be able to break down
in one side or the other.

00:53:04.660 --> 00:53:07.840
I think that the change
is coming no matter what.

00:53:07.840 --> 00:53:10.660
What we find,
though, is the change

00:53:10.660 --> 00:53:15.160
is going to be
disproportionately felt.

00:53:15.160 --> 00:53:18.950
So jobs that are most
likely to be automated

00:53:18.950 --> 00:53:24.100
are jobs like truck driver,
cashier, these types of things.

00:53:24.100 --> 00:53:27.040
And what we know is
that statistically,

00:53:27.040 --> 00:53:28.840
those jobs are
disproportionately

00:53:28.840 --> 00:53:33.970
held by people of
color, so that not fair,

00:53:33.970 --> 00:53:36.940
not equal distribution
is happening.

00:53:36.940 --> 00:53:38.999
So I think we do
have an obligation.

00:53:38.999 --> 00:53:41.290
If we're trying to create
the best futures for the most

00:53:41.290 --> 00:53:43.300
people, which is
what I would say

00:53:43.300 --> 00:53:46.780
is one of the underpinning
ideas or underlying ideas

00:53:46.780 --> 00:53:49.960
of "Tech Humanist," that we
have to be thinking about how

00:53:49.960 --> 00:53:53.680
to create a more equitable
distribution of opportunity

00:53:53.680 --> 00:53:57.400
and how to make sure that
the impact of automation

00:53:57.400 --> 00:54:02.380
is not going to destroy one
set of human's potential

00:54:02.380 --> 00:54:06.400
while it increases the potential
for enrichment of another.

00:54:06.400 --> 00:54:11.050
So that inequity is going
to become even more extreme

00:54:11.050 --> 00:54:12.880
than we've already experienced.

00:54:12.880 --> 00:54:15.670
So I think it's in our
best interest as humans

00:54:15.670 --> 00:54:19.360
to think about how to shift
that or how to level that out.

00:54:19.360 --> 00:54:21.460
Not that people
can't become wealthy,

00:54:21.460 --> 00:54:25.240
but that we don't create this
even more extreme distribution

00:54:25.240 --> 00:54:26.530
than we already have.

00:54:26.530 --> 00:54:29.770
So I think to some extent,
it's imperative for anyone

00:54:29.770 --> 00:54:32.320
who's creating experiences,
which is pretty much everyone

00:54:32.320 --> 00:54:36.850
that works in technology,
that works around most fields

00:54:36.850 --> 00:54:40.030
that I've worked around, health
care, entertainment, and so

00:54:40.030 --> 00:54:43.210
on, to think about the
change that's coming

00:54:43.210 --> 00:54:45.900
and how to make
sure that it is--

00:54:45.900 --> 00:54:49.240
that we are creating as much
opportunity there as possible.

00:54:49.240 --> 00:54:53.410
But yes, I think there's also
this kind of new emerging

00:54:53.410 --> 00:54:57.130
space around opportunities
to retrain people

00:54:57.130 --> 00:55:00.580
and repurpose, get people
to understand the new skills

00:55:00.580 --> 00:55:02.720
that they might have.

00:55:02.720 --> 00:55:05.320
I shared some really great
stats in "Tech Humanist"

00:55:05.320 --> 00:55:09.310
about programs that
were taking, let's say,

00:55:09.310 --> 00:55:14.230
prisoners who had come
out of prison programs

00:55:14.230 --> 00:55:19.205
and been able to retrain them
into communities, the jobs

00:55:19.205 --> 00:55:20.080
that they could keep.

00:55:20.080 --> 00:55:25.040
And there was 0.1% recidivism
within this program,

00:55:25.040 --> 00:55:28.190
so I'd urge you to
look into that example.

00:55:28.190 --> 00:55:31.570
There's just so many ways that
I think an ecosystem of answers

00:55:31.570 --> 00:55:32.980
is really what's going on here.

00:55:32.980 --> 00:55:35.290
We have to own
the responsibility

00:55:35.290 --> 00:55:38.060
as content creators and
experience creators.

00:55:38.060 --> 00:55:40.540
And we also have to
recognize that this

00:55:40.540 --> 00:55:42.250
is going to be a
broadly distributed,

00:55:42.250 --> 00:55:47.975
broadly felt thing that is
going to have inequity to it.

00:55:47.975 --> 00:55:51.220
TIM: So to bring back a word
that Paul put in his question,

00:55:51.220 --> 00:55:55.450
decentralization, how does
maybe decentralization help

00:55:55.450 --> 00:55:59.590
with this by spreading power
around or control around?

00:55:59.590 --> 00:56:02.445
Maybe just talk about
decentralization for a bit.

00:56:02.445 --> 00:56:06.040
KATE O'NEILL: Well, I think the
idea of spreading power around

00:56:06.040 --> 00:56:08.950
and spreading control
around is interesting.

00:56:08.950 --> 00:56:14.620
Certainly, we have seen, through
user-generated content, user

00:56:14.620 --> 00:56:19.190
communities, and platforms
like Meetup, for example,

00:56:19.190 --> 00:56:21.850
we were talking about
earlier, as one way that there

00:56:21.850 --> 00:56:25.030
are tools that we can put in
the hands of people that allow

00:56:25.030 --> 00:56:27.640
people to create communities
amongst themselves,

00:56:27.640 --> 00:56:29.840
create more human connection.

00:56:29.840 --> 00:56:32.500
Those are going to be, I
think, increasingly important.

00:56:32.500 --> 00:56:36.250
And the technologies are
there to foster that and allow

00:56:36.250 --> 00:56:38.200
discovery within
those communities,

00:56:38.200 --> 00:56:43.700
allow people to find each
other and connect more deeply.

00:56:43.700 --> 00:56:46.540
But I think we just have
to be thinking mindfully

00:56:46.540 --> 00:56:53.300
about the challenge of not
amplifying those net negatives,

00:56:53.300 --> 00:56:56.870
as your question was alluding to
earlier with the filter bubble

00:56:56.870 --> 00:56:58.370
and so on.

00:56:58.370 --> 00:57:02.320
So I'd love to hear more
specifically what about

00:57:02.320 --> 00:57:04.150
decentralization might be--

00:57:04.150 --> 00:57:08.470
what's nagging on the person's
mind that's asking the question

00:57:08.470 --> 00:57:12.010
or on your mind.

00:57:12.010 --> 00:57:14.135
So whoever is asking
that, feel free to ask

00:57:14.135 --> 00:57:15.010
a secondary question.

00:57:15.010 --> 00:57:15.774
TIM: If they call.

00:57:15.774 --> 00:57:17.440
KATE O'NEILL: And if
there's any other--

00:57:17.440 --> 00:57:19.400
TIM: Or maybe for
the sake of time,

00:57:19.400 --> 00:57:22.330
we look for one more question
in the room before we wrap up.

00:57:22.330 --> 00:57:24.430
Anything else?

00:57:24.430 --> 00:57:24.930
Yes.

00:57:24.930 --> 00:57:25.790
KATE O'NEILL: Great.

00:57:25.790 --> 00:57:28.460
AUDIENCE: So I have
a thought about some

00:57:28.460 --> 00:57:33.740
of this stuff in terms of,
do you ever take your work

00:57:33.740 --> 00:57:36.290
and look at it as
a lens of looking

00:57:36.290 --> 00:57:39.290
at humanity through the
lens of what technology

00:57:39.290 --> 00:57:41.679
is revealing about people?

00:57:41.679 --> 00:57:42.470
KATE O'NEILL: Yeah.

00:57:42.470 --> 00:57:43.950
I have looked at that.

00:57:43.950 --> 00:57:46.340
But I'm very curious
as to what is occurring

00:57:46.340 --> 00:57:47.949
to you as you think about that.

00:57:47.949 --> 00:57:50.240
AUDIENCE: Well, I mean, I
used to do a lot of community

00:57:50.240 --> 00:57:55.400
management, and so I came
out of being on the BBSs back

00:57:55.400 --> 00:57:58.130
in the late '80s, early '90s.

00:57:58.130 --> 00:58:02.000
And so it's this
thing where I realized

00:58:02.000 --> 00:58:06.070
that a lot of what
happened online was just

00:58:06.070 --> 00:58:08.860
what happened offline
but at a different scale

00:58:08.860 --> 00:58:12.050
and at different localities.

00:58:12.050 --> 00:58:15.370
And that was an
aha moment for me

00:58:15.370 --> 00:58:17.770
when I was a part of these
little communities back

00:58:17.770 --> 00:58:19.580
in the BBS days.

00:58:19.580 --> 00:58:23.230
So it's just kind of like as
technology has become more

00:58:23.230 --> 00:58:26.110
and more prevalent in
our lives, it's something

00:58:26.110 --> 00:58:28.990
that I kind of look
at, almost flip

00:58:28.990 --> 00:58:31.600
the conversation a little
bit in my own head of like,

00:58:31.600 --> 00:58:32.800
what does it mean--

00:58:32.800 --> 00:58:38.224
what does it say about
people given how we're using?

00:58:38.224 --> 00:58:39.640
KATE O'NEILL: A
little bit of that

00:58:39.640 --> 00:58:41.440
points back to the
decentralization discussion,

00:58:41.440 --> 00:58:41.939
as well.

00:58:41.939 --> 00:58:44.950
But I like the aspect
you brought up.

00:58:44.950 --> 00:58:48.220
One of the aspects of
this that I have looked at

00:58:48.220 --> 00:58:52.420
is that it seems to me
that our digital selves,

00:58:52.420 --> 00:58:55.570
that aggregate set of
characteristics that gets

00:58:55.570 --> 00:58:59.980
collected through our movements,
through our connections,

00:58:59.980 --> 00:59:03.340
through our interactions
in social spheres,

00:59:03.340 --> 00:59:07.450
that digital self is really
our aspirational self mostly,

00:59:07.450 --> 00:59:10.910
and that we are saying
who we most want to be.

00:59:10.910 --> 00:59:14.200
And it seems ironic to
me that that digital self

00:59:14.200 --> 00:59:16.390
is the self, the
version of ourselves,

00:59:16.390 --> 00:59:19.030
that is most
commodified by business

00:59:19.030 --> 00:59:24.130
and most capitalized upon
and manipulated by business.

00:59:24.130 --> 00:59:26.680
I think in our physical
manifestations,

00:59:26.680 --> 00:59:30.100
we are much less prone to
that kind of manipulation

00:59:30.100 --> 00:59:32.920
and over-capitalization.

00:59:32.920 --> 00:59:36.220
Yet, this digital self, which
is our aspirational self,

00:59:36.220 --> 00:59:36.960
is prone to that.

00:59:36.960 --> 00:59:39.010
So I think this is
the opportunity for us

00:59:39.010 --> 00:59:41.680
to merge that understanding
and say, well, it

00:59:41.680 --> 00:59:46.660
is a human that we're looking
at in that digital, collected,

00:59:46.660 --> 00:59:49.630
aggregate data points,
and so we need to be

00:59:49.630 --> 00:59:51.650
respectful about that too.

00:59:51.650 --> 00:59:54.160
So I think that flipped
version is to say,

00:59:54.160 --> 00:59:55.870
there's this way that
we're interacting

00:59:55.870 --> 00:59:57.670
with each other in a
way that represents

00:59:57.670 --> 01:00:01.120
who we most want to be and
who we most feel we are.

01:00:01.120 --> 01:00:02.740
So it's all the
more reason why we

01:00:02.740 --> 01:00:04.330
need to be respectful
with the data

01:00:04.330 --> 01:00:06.730
that we collect and
monetize and use

01:00:06.730 --> 01:00:09.730
within business to inform
our intelligent decisions

01:00:09.730 --> 01:00:11.260
and our systems.

01:00:11.260 --> 01:00:13.690
So I'd say that
that's exciting to me.

01:00:13.690 --> 01:00:14.790
TIM: Thank you, Kate.

01:00:14.790 --> 01:00:18.630
And thanks, Google, for being
a great audience for Kate.

01:00:18.630 --> 01:00:21.570
And I guess we owe a
solid round of applause.

01:00:21.570 --> 01:00:22.350
So thanks.

01:00:22.350 --> 01:00:23.550
[APPLAUSE]

01:00:23.550 --> 01:00:26.650
And Kate, especially.

