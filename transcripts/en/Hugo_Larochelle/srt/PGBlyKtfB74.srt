1
00:00:00,510 --> 00:00:04,060
In this video, we'll see a simple
type of conditional random fields,

2
00:00:04,220 --> 00:00:07,240
no that as the linear chain
conditional random fields.

3
00:00:09,220 --> 00:00:13,210
So we motivated the use of conditional
random fields in the context of sequence

4
00:00:13,211 --> 00:00:18,211
classification where we want to be able
to jointly model the full sequence of

5
00:00:18,911 --> 00:00:23,380
labels associated with a
sequence of inputs. So,

6
00:00:23,410 --> 00:00:24,130
uh,

7
00:00:24,130 --> 00:00:29,130
this here is essentially another way of
writing the probability of a given value

8
00:00:30,761 --> 00:00:33,870
for y one up to y capital k,

9
00:00:33,930 --> 00:00:38,930
capital k is the number of elements in
the sequence given the first input in

10
00:00:39,761 --> 00:00:43,390
this sequence up to the
last input in the sequence.

11
00:00:45,590 --> 00:00:47,560
Now let's look at how we,
uh,

12
00:00:47,570 --> 00:00:51,950
will a particular parametrization
of this conditional distribution.

13
00:00:53,710 --> 00:00:55,270
So perhaps the most simple,

14
00:00:55,600 --> 00:01:00,600
a conditional random fields or
CRF is the linear chain a CRF.

15
00:01:01,690 --> 00:01:06,280
Now let's see, uh, what is, uh, what
we do in the context of a linear CRF,

16
00:01:07,030 --> 00:01:10,900
not in regular classification
where we don't, uh,

17
00:01:10,901 --> 00:01:15,520
assume there is any dependence in
our model between, uh, the, uh,

18
00:01:15,521 --> 00:01:19,510
the adjacent positions in a
sequence. Uh, in this context,

19
00:01:19,511 --> 00:01:22,240
it mean it means that our distribution,

20
00:01:22,241 --> 00:01:27,140
the probability that our model assigned
to a particular sequence why given a

21
00:01:27,141 --> 00:01:32,141
sequence of input x will just be the
product of the probability that our neural

22
00:01:32,801 --> 00:01:34,810
network assigned to each label.

23
00:01:34,811 --> 00:01:39,400
Why k given its associated input x k.
Now,

24
00:01:39,460 --> 00:01:39,821
uh,

25
00:01:39,821 --> 00:01:44,320
in the neural network that we remodeled
this is that we take the soft max output

26
00:01:44,350 --> 00:01:45,290
activation function.

27
00:01:45,291 --> 00:01:50,291
So we take the exponential of the pre
activation value for the label for which

28
00:01:53,020 --> 00:01:57,310
we are evaluating and probability that's
why came and then we divide that by

29
00:01:57,311 --> 00:01:59,650
normalization constant
or partition function,

30
00:01:59,860 --> 00:02:03,760
which is just the sum of all
potential value of the numerator.

31
00:02:03,820 --> 00:02:08,170
So this would just be the sum of the
exponential of DP activation function for

32
00:02:08,171 --> 00:02:09,640
any value of Waikiki.

33
00:02:10,690 --> 00:02:14,170
And now we are taking the product of
this because we are assuming our uh,

34
00:02:14,230 --> 00:02:18,580
our model does not model
dependencies between adjacent
positions in the sequence.

35
00:02:18,980 --> 00:02:22,620
And now because the product of the
exponential is the exponential of the sum,

36
00:02:22,630 --> 00:02:27,400
we can also write this as the
exponential of is some over positions,

37
00:02:27,401 --> 00:02:32,401
k of the pre activation that
the output layer for each label.

38
00:02:33,191 --> 00:02:35,500
Why K as predicted.

39
00:02:35,501 --> 00:02:38,860
So the reactivation as computed
based on the associated input,

40
00:02:38,890 --> 00:02:41,200
ex gay and a.

41
00:02:41,210 --> 00:02:44,560
And then we also divide by the
product of the partition functions or

42
00:02:44,620 --> 00:02:46,030
normalization constants here.

43
00:02:47,500 --> 00:02:52,500
Now in the linear chain CRF in the
context of sequence classification will

44
00:02:52,960 --> 00:02:57,790
essentially add a term that will also
express a preference for a particular

45
00:02:57,791 --> 00:03:02,791
sequences or of AGSM and values for a
yk and why k plus one for any position

46
00:03:04,301 --> 00:03:05,950
king.
So in other words,

47
00:03:06,010 --> 00:03:10,990
the conditional distribution
of y given some sequence x,

48
00:03:11,230 --> 00:03:13,690
it's going to be the exponential of again,

49
00:03:13,691 --> 00:03:17,380
so here and here we have
exactly the same term.

50
00:03:17,620 --> 00:03:20,530
So this is essentially
saying how we think,

51
00:03:20,531 --> 00:03:24,670
why k the particular value of why gay
that were for which we are valuing your

52
00:03:24,671 --> 00:03:27,770
probability is uh,
is,

53
00:03:27,800 --> 00:03:32,800
is likely a given the input and
given the associated input x games.

54
00:03:34,071 --> 00:03:35,560
So that's why we have ex gay here.

55
00:03:36,310 --> 00:03:40,810
Plus another term which is going to
say how much our model likes that,

56
00:03:40,930 --> 00:03:45,930
whyK is followed by why k plus one and
this so how much we like seeing why k

57
00:03:48,101 --> 00:03:53,101
followed by why k plus one is going to
be parent tries by Matrix v where it's

58
00:03:53,171 --> 00:03:56,470
entry yk Yq plus one uh,

59
00:03:56,500 --> 00:04:01,120
tells us how much we liked this
particular sequence of yk. Why K plus one.

60
00:04:01,900 --> 00:04:03,490
So just to be clear,

61
00:04:03,550 --> 00:04:07,030
why k plus one is a variable
and it has a particular value.

62
00:04:07,180 --> 00:04:10,300
If we have 10 classes, then
why k could be one, two,

63
00:04:10,301 --> 00:04:14,050
three up to 10 and similarly why
k plus one is also a variable,

64
00:04:14,051 --> 00:04:18,670
but it's the variable that represents
the label at the next step, k plus one.

65
00:04:18,671 --> 00:04:22,350
And it also takes a value
from one to 10 and so v, y,

66
00:04:22,360 --> 00:04:27,360
k y k plus one if a y k
four in this sequence here,

67
00:04:28,420 --> 00:04:32,920
if Mikey was equal to say five and
then y k plus one was equal to three,

68
00:04:33,070 --> 00:04:38,070
then this term would for this particular
value of k would look at the entry at

69
00:04:39,581 --> 00:04:43,810
the fifth row and the third
column of Matrix speed. Okay.

70
00:04:44,830 --> 00:04:48,100
So to sum up in a linear chain,
a conditional random fields,

71
00:04:48,130 --> 00:04:53,130
we keep the per position term
that expresses how much we think,

72
00:04:54,221 --> 00:04:57,710
why he should be likely given
its associated but x, k.

73
00:04:57,910 --> 00:05:02,860
But we add another term which
also expresses a preference
as to whether we like

74
00:05:02,861 --> 00:05:07,861
seeing why k followed by why k plus one
for a particular position over position

75
00:05:08,170 --> 00:05:09,003
k.

76
00:05:09,010 --> 00:05:14,010
And then notice that here also we some
from k equals one up to came and as one.

77
00:05:14,591 --> 00:05:19,480
So we add our preferences for
uh, for each position. Kay.

78
00:05:19,780 --> 00:05:23,050
So if we had the sequence of size,
say three,

79
00:05:23,380 --> 00:05:28,380
and then we had that were valuing the
probability for y one equals se two,

80
00:05:28,960 --> 00:05:32,680
and then y two equals five,

81
00:05:32,740 --> 00:05:37,510
and then wide three equals uh,
let's say three,

82
00:05:37,750 --> 00:05:42,750
then it means that this term
here would correspond to a right,

83
00:05:42,941 --> 00:05:47,941
it just here that correspond to
v two five Plus v five three.

84
00:05:51,850 --> 00:05:55,870
Okay. So if we were to compute this
expression and actually this part of the

85
00:05:55,871 --> 00:05:58,940
expression for these values
for this sequence of wise,

86
00:05:59,180 --> 00:06:01,520
then I'd have this term plus district.

87
00:06:05,590 --> 00:06:10,590
So to sum up the probability of a
sequence y given x is going to be the

88
00:06:10,960 --> 00:06:12,160
exponential of a,

89
00:06:12,161 --> 00:06:17,161
some of terms which express a preference
about the value for each element in the

90
00:06:17,211 --> 00:06:19,200
sequence. Why Gay? Uh,

91
00:06:19,210 --> 00:06:24,130
in the sequence of labels given
the associated input plus the,

92
00:06:24,180 --> 00:06:28,210
uh, some of pairwise preferences
between adjacent labels.

93
00:06:28,870 --> 00:06:32,740
And so the, probably the is going to be
proportional to the exponential of the,

94
00:06:32,750 --> 00:06:35,470
some of these, uh, two types of terms.

95
00:06:35,830 --> 00:06:39,070
And now because we want a probability
distribution that sums to zero,

96
00:06:39,071 --> 00:06:42,190
we have to divide by something
we call the partition function.

97
00:06:42,191 --> 00:06:46,900
Or sometimes people use the
name normalization constant.

98
00:06:47,200 --> 00:06:51,340
It's a partition function in
particular because, uh, it depends,

99
00:06:51,460 --> 00:06:56,290
uh, for one thing on x, but it also
depends on the parameters of the,

100
00:06:56,291 --> 00:07:00,150
uh, of the models for
different parameters. We'll
get a different value of,

101
00:07:00,160 --> 00:07:01,660
of this normalization constant.

102
00:07:02,500 --> 00:07:07,500
And what this zed is here is really just
the sum of the numerator but over all

103
00:07:09,251 --> 00:07:14,080
possible sequence values. Uh, so all
possible values of that sequence.

104
00:07:14,081 --> 00:07:18,610
Why? So there's actually a number, an
exponential number of such sequences.

105
00:07:18,910 --> 00:07:22,680
And so a naive method for computing
it would be intractable, but, uh,

106
00:07:22,720 --> 00:07:27,720
we'll see in a future videos out to
actually write a dynamic program for

107
00:07:27,941 --> 00:07:31,870
computing the partition function for
linear chain conditional random fields.

108
00:07:34,870 --> 00:07:35,040
Yeah.

109
00:07:35,040 --> 00:07:39,240
Now, visually, if you
look at the, uh, uh, the,

110
00:07:39,320 --> 00:07:41,530
the flow graph for computation.

111
00:07:41,570 --> 00:07:45,540
So before in the regular
neural net from the input,

112
00:07:45,780 --> 00:07:50,780
we'd get a computation graph up to the
pre activation layer and there would be

113
00:07:50,791 --> 00:07:55,780
no interaction between this
computation for say the uh,

114
00:07:55,920 --> 00:08:00,750
input that position came on as one and
the computation of the neural net at

115
00:08:00,751 --> 00:08:05,610
Persia at position k that we computed
the pre activation for that particular

116
00:08:05,611 --> 00:08:06,444
input.

117
00:08:07,960 --> 00:08:12,960
Now what we're adding is a matrix of
undirected connections between adjacent

118
00:08:14,141 --> 00:08:16,780
position such that the pre,
uh,

119
00:08:17,170 --> 00:08:22,170
such that essentially the probability
will assign here will also depend on the,

120
00:08:23,590 --> 00:08:28,250
uh, on what's computed here and so
on. And so this is very informal.

121
00:08:28,260 --> 00:08:31,480
There's just to illustrate
the dependencies here,

122
00:08:31,481 --> 00:08:34,630
and this is sort of a informal
graph of computations,

123
00:08:34,631 --> 00:08:37,660
but we'll formalize this a
bit more in the next videos.

124
00:08:38,670 --> 00:08:42,230
What's important to realize
that between each adjacent step,

125
00:08:42,231 --> 00:08:47,231
we always use the same Matrix v as our
set of parameters that control the,

126
00:08:48,120 --> 00:08:52,790
uh, interaction between, uh, and the
essentially control how we model,

127
00:08:53,180 --> 00:08:56,190
uh,
observations of each sent in the sequence.

128
00:08:56,340 --> 00:09:00,930
So the Matrix we hear is the same as
this matrix and so on. And similarly,

129
00:09:00,931 --> 00:09:05,610
the set of weights of my neural network.
So the matrix w here and be here.

130
00:09:05,611 --> 00:09:09,320
And also all the parameters that
would be between the input and the pre

131
00:09:09,340 --> 00:09:11,430
activation at the output layer.

132
00:09:11,700 --> 00:09:15,310
These parameters for this neural
net is the same as these, uh,

133
00:09:15,330 --> 00:09:19,410
these parameters and as these
parameters. So it's sort of the, uh, uh,

134
00:09:19,440 --> 00:09:23,160
the same copy of the same neural network,
uh,

135
00:09:23,161 --> 00:09:28,050
with a shared weight and a, so this will
have implications when, for instance,

136
00:09:28,051 --> 00:09:32,190
we compute gradients. Uh, it means that
we'll get great answers that will affect,

137
00:09:32,630 --> 00:09:34,650
uh,
told that the gradients,

138
00:09:34,830 --> 00:09:38,610
when do backpropagation there will be
gradient that will go on this matrix here,

139
00:09:38,670 --> 00:09:40,200
which is the same as this here.

140
00:09:40,201 --> 00:09:44,100
So we'll have to cumulate
gradients for each position. Okay.

141
00:09:44,101 --> 00:09:48,060
But this is just to really highlight
the fact that we using the same neural

142
00:09:48,061 --> 00:09:50,730
network for each position and
between adjacent position,

143
00:09:50,731 --> 00:09:54,270
we using the same matrix of parameter v.
All right?

144
00:09:54,271 --> 00:09:57,030
So that's the linear chain
conditional random fields.

