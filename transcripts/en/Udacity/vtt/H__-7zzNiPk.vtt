WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:02.200
So here you have goals
you're trying to accomplish,

00:00:02.200 --> 00:00:04.980
sort of positive things you're trying
to do and then you have constraints.

00:00:04.980 --> 00:00:06.615
Basically I want you to eat little dots,

00:00:06.615 --> 00:00:11.140
eat big dots and
eat big ghosts while honoring

00:00:11.140 --> 00:00:15.010
the constraint that I don't want you
to be eaten by a non-scared ghost.

00:00:15.010 --> 00:00:17.850
&gt;&gt; Okay, yeah, I mean so
now from a logic standpoint,

00:00:17.850 --> 00:00:19.102
we've covered everything.

00:00:19.102 --> 00:00:21.022
Success and
things that are defined by success and

00:00:21.022 --> 00:00:22.920
things that are defined by failure.

00:00:22.920 --> 00:00:26.320
&gt;&gt; Right, and but the failure here,
I mean well, right, in exactly that way.

00:00:26.320 --> 00:00:27.700
So why did I bring all this up?

00:00:27.700 --> 00:00:30.250
I brought it up becausem one I just
think it's kind of neat that people

00:00:30.250 --> 00:00:31.810
actually come up with these things.

00:00:31.810 --> 00:00:34.650
But the other thing that I think is
kind of neat is that it gives us a hint,

00:00:34.650 --> 00:00:36.430
it gives us two hints actually.

00:00:36.430 --> 00:00:39.580
It gives us a hint about how we might
do a better job than just simply

00:00:39.580 --> 00:00:41.870
behaving randomly in our rollout policy.

00:00:41.870 --> 00:00:43.100
So here's something you might do.

00:00:43.100 --> 00:00:47.130
You might say when I get down to here,
rather than behaving randomly.

00:00:48.370 --> 00:00:51.290
And where by the way, I sometimes will
get eaten by a ghost even though I

00:00:51.290 --> 00:00:52.760
could've easily avoided it.

00:00:52.760 --> 00:00:55.930
I'm going to try to behave
randomly while still

00:00:55.930 --> 00:00:59.250
honoring whatever constraints
I happen to have defined.

00:00:59.250 --> 00:01:05.480
So here, rather than trying to just
walk around until something bad happens.

00:01:05.480 --> 00:01:09.810
I'm going to try to stay
alive as long as possible

00:01:09.810 --> 00:01:11.700
which is what this constraint tells me.

00:01:11.700 --> 00:01:14.390
And that's a different kind of
roll-out policy, but one that I

00:01:14.390 --> 00:01:17.620
claim that gives you a sort of better
view of the goodness of this state.

00:01:17.620 --> 00:01:22.131
This state action pair is better
because you can avoid violating your

00:01:22.131 --> 00:01:27.198
constraints for longer than if you are
say in this state and take this action.

00:01:27.198 --> 00:01:28.738
And there are lots of other
things that you might do.

00:01:28.738 --> 00:01:32.402
I just sort of wanted to give you
a flavor of it which is that I can use

00:01:32.402 --> 00:01:36.334
these sort of options if they are of
a certain type in order to be smarter

00:01:36.334 --> 00:01:38.560
about the way I do my roll-out policy.

00:01:38.560 --> 00:01:40.630
There's also another way
that we can do this.

00:01:40.630 --> 00:01:46.020
I can still use options in general,
in this entire search up here before.

00:01:46.020 --> 00:01:49.940
Rather than taking primitive action and
doing my search through the tree in my

00:01:49.940 --> 00:01:57.399
expansion, I can replace this with
my actions with options as well.

00:01:58.940 --> 00:02:00.160
Ooh.
And what's the big win there?

00:02:00.160 --> 00:02:01.270
Well the big win there is,

00:02:01.270 --> 00:02:04.990
by following the options,
I'm getting deeper into the tree.

00:02:04.990 --> 00:02:08.180
&gt;&gt; Yeah,
it's the same kind of idea as in

00:02:08.180 --> 00:02:10.830
the option case that we
talked about already.

00:02:10.830 --> 00:02:14.200
Except now we're in trees instead of,
I guess, grids.

00:02:14.200 --> 00:02:16.770
Yes, but trees and grids,
it turns out they're all the same thing.

00:02:16.770 --> 00:02:18.620
So there's lots of things
you can talk about.

00:02:18.620 --> 00:02:20.520
There's been a lot of
work in this space.

00:02:20.520 --> 00:02:23.450
Some of them are available as
readings for the students.

00:02:23.450 --> 00:02:24.810
I recommend thinking about them but

00:02:24.810 --> 00:02:28.995
what's I think kind of nice about
it is that this general notion of

00:02:28.995 --> 00:02:34.420
treatingm solving an MDP not as
just I've got to solve the MDP,

00:02:34.420 --> 00:02:38.630
but as a kind of tree search problem
where we take advantage of and use

00:02:38.630 --> 00:02:42.640
the randomness is something that unifies
a lot of approaches that you might have.

00:02:42.640 --> 00:02:43.360
&gt;&gt; Okay, cool.

00:02:43.360 --> 00:02:44.790
&gt;&gt; Okay.
And I think that's pretty much.

00:02:44.790 --> 00:02:47.290
I'll say one more thing about this and
then I think we're done and

00:02:47.290 --> 00:02:49.890
that's just that there are lots
of ways of looking at this.

00:02:49.890 --> 00:02:52.320
You might ask yourself is this
even reinforcement learning,

00:02:52.320 --> 00:02:54.900
where we've just gone back to searching
and I'm going to claim it is.

00:02:54.900 --> 00:02:57.020
I'm going to claim it's
a policy search algorithm.

00:02:57.020 --> 00:02:58.150
&gt;&gt; What's your justification for that?

00:02:58.150 --> 00:03:01.850
&gt;&gt; Well, my justification for that is,
I'm basically by building this up,

00:03:01.850 --> 00:03:04.990
I am doing,
I am searching over possible policies.

00:03:06.300 --> 00:03:08.620
And what I'm doing in the inner
loop of searching for

00:03:08.620 --> 00:03:12.190
policies is I'm doing a kind
of reinforcement learning

00:03:12.190 --> 00:03:16.220
step here when I do my backups and
in fact, the way you do the backups is

00:03:16.220 --> 00:03:19.970
you use the Bellman equation as
a way of re=estimating Q values.

00:03:19.970 --> 00:03:23.470
And once I have those I now have
another policy that I'm looking at and

00:03:23.470 --> 00:03:24.750
I just keep dong that over and
over again.

00:03:24.750 --> 00:03:29.123
So I'm really searching through policy
space which is sort of what the tree

00:03:29.123 --> 00:03:30.030
expansion is.

00:03:30.030 --> 00:03:33.010
And I have an inner loop
every time I run out of

00:03:33.010 --> 00:03:35.360
my confidence about the policy.

00:03:35.360 --> 00:03:38.250
I'm doing a little bit of
reinforcement learning to figure out

00:03:38.250 --> 00:03:39.610
what the policy ought to be.

00:03:39.610 --> 00:03:43.611
So it's policy search with an inner
loop of evaluation, sort of.

00:03:43.611 --> 00:03:48.990
&gt;&gt; And
Monte Carlo policy evaluation somehow.

00:03:48.990 --> 00:03:49.870
&gt;&gt; Right, exactly.

00:03:49.870 --> 00:03:52.680
Well that's really what it is and the
way I'm doing the policy evaluation is

00:03:52.680 --> 00:03:54.880
by doing this well,
doing the Monte Carlo.

00:03:54.880 --> 00:03:56.540
&gt;&gt; So it really combines a lot of

00:03:57.660 --> 00:04:00.880
ideas that we've talked about
into kind of one algorithm.

00:04:00.880 --> 00:04:02.980
Has this been successful?

00:04:02.980 --> 00:04:04.110
&gt;&gt; It has been successful.

00:04:04.110 --> 00:04:05.720
I've actually used it in my own work.

00:04:05.720 --> 00:04:08.690
It has this really neat feature
that allows you to avoid

00:04:08.690 --> 00:04:10.520
searching over lots and
lots of the state space.

00:04:10.520 --> 00:04:11.750
It's actually really wonderful.

00:04:11.750 --> 00:04:13.920
It actually has a couple of
properties that are worth mentioning.

00:04:13.920 --> 00:04:16.779
So let me mention those properties and
then I think we're done.

