WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.905
[MUSIC PLAYING]

00:00:04.905 --> 00:00:06.780
MEGHAN MEHTA: We have
had so much fun showing

00:00:06.780 --> 00:00:09.840
you I/O. But now it's
time to hear from I/O.

00:00:09.840 --> 00:00:10.860
Let's go do some meetup.

00:00:13.520 --> 00:00:15.500
How many festivals have
you been to so far?

00:00:15.500 --> 00:00:16.460
SPEAKER 1: How many?

00:00:16.460 --> 00:00:17.270
MEGHAN MEHTA: Google I/O.

00:00:17.270 --> 00:00:18.040
SPEAKER 1: This is my second.

00:00:18.040 --> 00:00:19.790
MEGHAN MEHTA: It's
your second Google I/O?

00:00:19.790 --> 00:00:21.340
SPEAKER 2: This is my first one.

00:00:21.340 --> 00:00:22.070
SPEAKER 3: This is my fourth.

00:00:22.070 --> 00:00:23.712
MEGHAN MEHTA: It's
your fourth one?

00:00:23.712 --> 00:00:24.920
SPEAKER 4: My fourth as well.

00:00:24.920 --> 00:00:25.010
MEGHAN MEHTA: Wow.

00:00:25.010 --> 00:00:25.750
What about you guys?

00:00:25.750 --> 00:00:26.380
SPEAKER 5: You're
at the fourth I/O?

00:00:26.380 --> 00:00:26.864
SPEAKER 4: Yeah.

00:00:26.864 --> 00:00:28.316
SPEAKER 5: You're
at the fourth I/O?

00:00:28.316 --> 00:00:28.800
SPEAKER 4: Yeah.

00:00:28.800 --> 00:00:29.962
SPEAKER 5: First one for me.

00:00:29.962 --> 00:00:30.690
SPEAKER 6: First one for me too.

00:00:30.690 --> 00:00:32.867
SPEAKER 7: I have
probably seven festivals.

00:00:32.867 --> 00:00:33.700
MEGHAN MEHTA: Seven!

00:00:33.700 --> 00:00:34.300
Oh my gosh.

00:00:34.300 --> 00:00:35.260
You're a veteran.

00:00:35.260 --> 00:00:35.927
SPEAKER 7: Yeah.

00:00:37.332 --> 00:00:39.790
MEGHAN MEHTA: What was your
favorite technology that Google

00:00:39.790 --> 00:00:41.000
came out with this year?

00:00:41.000 --> 00:00:41.792
SPEAKER 8: Flutter!

00:00:41.792 --> 00:00:42.292
Flutter!

00:00:42.292 --> 00:00:43.167
SPEAKER 1: Assistant.

00:00:43.167 --> 00:00:44.720
Everything about the Assistant.

00:00:44.720 --> 00:00:46.280
The upgrades are fantastic.

00:00:46.280 --> 00:00:48.197
MEGHAN MEHTA: What's
your guys' favorite thing

00:00:48.197 --> 00:00:49.540
that you've seen so far?

00:00:49.540 --> 00:00:50.846
SPEAKER 3: Definitely the--

00:00:50.846 --> 00:00:51.800
[INTERPOSING VOICES]

00:00:51.800 --> 00:00:53.200
SPEAKER 3: Yeah, [INAUDIBLE].

00:00:53.200 --> 00:00:55.090
SPEAKER 9: What was the name
of the people that were dancing

00:00:55.090 --> 00:00:56.280
on the ice cream van here?

00:00:56.280 --> 00:00:56.690
MEGHAN MEHTA: The fungineers?

00:00:56.690 --> 00:00:58.370
SPEAKER 9: They were amazing.

00:00:58.370 --> 00:00:59.980
They were so funny.

00:00:59.980 --> 00:01:02.766
SPEAKER 2: I think the live
captions seem really, really

00:01:02.766 --> 00:01:03.266
cool.

00:01:03.266 --> 00:01:03.735
MEGHAN MEHTA: Yeah.

00:01:03.735 --> 00:01:04.735
SPEAKER 2: I don't know.

00:01:04.735 --> 00:01:07.700
I just feel like it's gonna
change a lot of people's lives.

00:01:07.700 --> 00:01:09.720
SPEAKER 7: Anything with
Google Photos as well.

00:01:09.720 --> 00:01:13.130
SPEAKER 4: For me it's the
environments or the people--

00:01:13.130 --> 00:01:16.140
or the people just working
in something creative

00:01:16.140 --> 00:01:17.180
with technology.

00:01:17.180 --> 00:01:18.680
And I think that's amazing.

00:01:22.893 --> 00:01:25.060
MEGHAN MEHTA: What is the
craziest thing you've ever

00:01:25.060 --> 00:01:28.270
asked your Google Assistant?

00:01:28.270 --> 00:01:29.590
SPEAKER 1: Am I hungry?

00:01:29.590 --> 00:01:31.637
SPEAKER 7: Will I
ever find a husband?

00:01:31.637 --> 00:01:33.220
MEGHAN MEHTA: I'm
just saying, there's

00:01:33.220 --> 00:01:35.290
a ton of men around here.

00:01:35.290 --> 00:01:36.550
SPEAKER 7: It's true.

00:01:36.550 --> 00:01:39.020
SPEAKER 10: I have a
five-year-old niece.

00:01:39.020 --> 00:01:41.480
And she loves
having conversations

00:01:41.480 --> 00:01:42.735
with the Google Assistant.

00:01:42.735 --> 00:01:44.940
She's like, what's
your favorite color?

00:01:44.940 --> 00:01:47.450
And do you want to go
to the moon with me?

00:01:50.590 --> 00:01:54.270
MEGHAN MEHTA: Next I/O, if
Google could do anything,

00:01:54.270 --> 00:01:56.497
what would you want
to see next year?

00:01:56.497 --> 00:01:57.580
SPEAKER 1: Smell o vision.

00:01:57.580 --> 00:01:58.520
MEGHAN MEHTA: I love it!

00:01:58.520 --> 00:01:59.480
I want smell o vision!

00:01:59.480 --> 00:02:00.550
SPEAKER 1: So we can see
it and we can hear it.

00:02:00.550 --> 00:02:01.900
Let's be able to smell it now.

00:02:01.900 --> 00:02:02.890
SPEAKER 9: How about
a car that hovers?

00:02:02.890 --> 00:02:03.680
MEGHAN MEHTA: I love it.

00:02:03.680 --> 00:02:05.100
SPEAKER 10: Well, it
drives itself as well.

00:02:05.100 --> 00:02:06.320
SPEAKER 9: Yeah, [INAUDIBLE]
at the same time.

00:02:06.320 --> 00:02:07.778
SPEAKER 10:
Self-driving hover car.

00:02:07.778 --> 00:02:10.630
SPEAKER 11: I would like to see
more integration towards humans

00:02:10.630 --> 00:02:12.220
and artificial intelligence.

00:02:12.220 --> 00:02:15.100
So it's not about artificial
intelligence only,

00:02:15.100 --> 00:02:18.250
but I think the future that
Google is trying to build

00:02:18.250 --> 00:02:21.180
or should build is a combination
of artificial intelligence

00:02:21.180 --> 00:02:21.680
with humans.

00:02:21.680 --> 00:02:22.880
SPEAKER 12: Google Barber.

00:02:22.880 --> 00:02:24.460
MEGHAN MEHTA: Google Barber?

00:02:24.460 --> 00:02:25.140
What is that?

00:02:25.140 --> 00:02:27.010
SPEAKER 12: So
think of it, right?

00:02:27.010 --> 00:02:29.730
Each time I go to the salon,
I want people to know exactly

00:02:29.730 --> 00:02:31.530
would haircut I want.

00:02:31.530 --> 00:02:34.540
So based on my activities and my
actions and my previous haircut

00:02:34.540 --> 00:02:37.020
history, it could basically
tell me what exactly I want.

00:02:37.020 --> 00:02:39.318
SPEAKER 13: If they could
do the Google Assistant

00:02:39.318 --> 00:02:40.818
in [INAUDIBLE],,
our local language,

00:02:40.818 --> 00:02:42.873
if they could support
Google in [INAUDIBLE],,

00:02:42.873 --> 00:02:43.790
that would be awesome.

00:02:43.790 --> 00:02:45.733
MEGHAN MEHTA: Oh, great.

00:02:45.733 --> 00:02:47.150
What do you want
to see next year?

00:02:52.982 --> 00:02:54.690
SPEAKER 14: AR dog
watching, fox to trot?

00:02:54.690 --> 00:02:57.750
MEGHAN MEHTA: AR dog walking.

00:02:57.750 --> 00:02:59.450
You heard it here first.

00:02:59.450 --> 00:03:01.067
It's our next trend.

00:03:01.067 --> 00:03:03.400
TIMOTHY JORDAN: Welcome back,
and thanks for hanging out

00:03:03.400 --> 00:03:04.450
with us on I/O Live.

00:03:04.450 --> 00:03:05.330
I'm Timothy Jordan.

00:03:05.330 --> 00:03:06.220
EMILY FORTUNA: And
I'm Emily Fortuna.

00:03:06.220 --> 00:03:08.650
Our next guest is a real
pioneer, an engineer,

00:03:08.650 --> 00:03:12.040
physician, and NASA
astronaut, Dr. Mae C. Jemison.

00:03:12.040 --> 00:03:13.370
What an honor to have you here.

00:03:13.370 --> 00:03:14.870
MAE C. JEMISON:
Thank you very much.

00:03:14.870 --> 00:03:16.610
It's exciting to be
here today as well.

00:03:16.610 --> 00:03:18.500
It's a beautiful, sunny day,

00:03:18.500 --> 00:03:20.547
EMILY FORTUNA:
Definitely, very bright.

00:03:20.547 --> 00:03:22.630
You've had an extremely
impressive career spanning

00:03:22.630 --> 00:03:24.320
so many different fields.

00:03:24.320 --> 00:03:25.990
What do you see as
the through lines

00:03:25.990 --> 00:03:27.850
throughout all of
these different fields?

00:03:27.850 --> 00:03:29.850
MAE C. JEMISON: So when
I think about the things

00:03:29.850 --> 00:03:32.230
that I've done from having
been a chemical engineer

00:03:32.230 --> 00:03:36.670
and a medical doctor, and then
an astronaut, and then as soon

00:03:36.670 --> 00:03:39.430
as I left NASA, I started an
international science camp

00:03:39.430 --> 00:03:41.650
called the Earth We Share
as an environmental studies

00:03:41.650 --> 00:03:43.720
professor, 100 Year Starship.

00:03:43.720 --> 00:03:47.350
Throughout them is the
idea that we're connected.

00:03:47.350 --> 00:03:51.400
And how do we use our
technological prowess,

00:03:51.400 --> 00:03:54.670
our resources to help
better the world writ large?

00:03:54.670 --> 00:03:56.870
And I know that sounds
kind of Pollyanna-ish,

00:03:56.870 --> 00:03:59.440
but my revelation
as I've grown up,

00:03:59.440 --> 00:04:02.560
as I've looked around the world,
as I've lived different places,

00:04:02.560 --> 00:04:07.240
it's that this planet
has incredible resources,

00:04:07.240 --> 00:04:08.630
but they're finite.

00:04:08.630 --> 00:04:12.250
The resources include people
and their talents that

00:04:12.250 --> 00:04:14.110
exist everywhere in the world.

00:04:14.110 --> 00:04:17.220
It includes our plant
life, our animal life,

00:04:17.220 --> 00:04:20.832
not just the things you can
dig up out of the ground.

00:04:20.832 --> 00:04:22.540
And we have to protect
those, and we also

00:04:22.540 --> 00:04:25.300
have to understand
that we share them.

00:04:25.300 --> 00:04:27.440
And if I go a
little bit further,

00:04:27.440 --> 00:04:29.440
it's like pushing the envelope.

00:04:29.440 --> 00:04:33.640
Not being satisfied
with this is complacent.

00:04:33.640 --> 00:04:37.285
I always have to
challenge myself.

00:04:37.285 --> 00:04:40.510
And perhaps I push
and grab on things

00:04:40.510 --> 00:04:43.960
that are really
different or difficult,

00:04:43.960 --> 00:04:47.290
but for me the energy
comes with trying something

00:04:47.290 --> 00:04:51.870
that I don't know that I can
absolutely accomplish it.

00:04:51.870 --> 00:04:54.430
That it's going to require
something new for me.

00:04:54.430 --> 00:04:56.983
That it's going to
require me to grow.

00:04:56.983 --> 00:04:58.900
TIMOTHY JORDAN: During
your presentation here,

00:04:58.900 --> 00:05:03.580
you spoke about your 100
Year Starship initiative.

00:05:03.580 --> 00:05:05.200
And to introduce
it, and I love this,

00:05:05.200 --> 00:05:08.800
you spoke about how
Mars is too close.

00:05:08.800 --> 00:05:11.568
Why is that, and why should
we set our sights further out?

00:05:11.568 --> 00:05:14.110
MAE C. JEMISON: So I'm going to
be perfectly honest with you.

00:05:14.110 --> 00:05:17.200
When I was growing up, I
assumed that by the time

00:05:17.200 --> 00:05:18.902
I became an astronaut--

00:05:18.902 --> 00:05:20.860
I actually didn't even
want to be an astronaut.

00:05:20.860 --> 00:05:22.630
I just assumed
I'd go into space.

00:05:22.630 --> 00:05:25.510
But that I'd be
going to Mars or one

00:05:25.510 --> 00:05:28.640
of the moons around Jupiter,
or Saturn or something.

00:05:28.640 --> 00:05:32.230
That's what I thought because
we had all these things that

00:05:32.230 --> 00:05:33.080
were happening.

00:05:33.080 --> 00:05:37.190
In fact, we had probes on
Mars when I was a little kid.

00:05:37.190 --> 00:05:39.490
So of course, we'll be
doing something more.

00:05:39.490 --> 00:05:42.250
The reason I talk about
100 Years Starship

00:05:42.250 --> 00:05:45.550
from the perspective
of it's more audacious

00:05:45.550 --> 00:05:50.320
than Mars is because Mars
doesn't push us hard enough.

00:05:50.320 --> 00:05:52.160
We've been on Mars
multiple times.

00:05:52.160 --> 00:05:54.700
We've got Mars'
addressed, right?

00:05:54.700 --> 00:05:57.880
We can figure out the
technological timeline

00:05:57.880 --> 00:06:01.540
to do Mars, the technologies
that are needed.

00:06:01.540 --> 00:06:03.040
The reason we
haven't gone to Mars

00:06:03.040 --> 00:06:05.080
is not so much an
engineering challenge,

00:06:05.080 --> 00:06:06.760
it's a public commitment.

00:06:06.760 --> 00:06:09.280
But what's going
to really push us

00:06:09.280 --> 00:06:11.050
is thinking about
interstellar, which

00:06:11.050 --> 00:06:12.880
also captures our imagination.

00:06:12.880 --> 00:06:15.010
That doesn't mean
that we don't do Mars.

00:06:15.010 --> 00:06:19.120
It means, let's do something
that's big and audacious

00:06:19.120 --> 00:06:21.100
and keep that in our eye sight.

00:06:21.100 --> 00:06:24.790
So 100 Year Starship
was seed funded by DARPA

00:06:24.790 --> 00:06:27.130
to make sure we have
the capabilities

00:06:27.130 --> 00:06:29.170
for human interstellar flight.

00:06:29.170 --> 00:06:33.190
But the real underlying
current there is innovation.

00:06:33.190 --> 00:06:36.280
How do we get the radical
leaps in knowledge,

00:06:36.280 --> 00:06:40.290
in technologies,
in human systems

00:06:40.290 --> 00:06:44.250
that would allow us to go
to another star system?

00:06:44.250 --> 00:06:46.800
Now, that's the real piece.

00:06:46.800 --> 00:06:51.120
Because each one of those
challenges that we meet

00:06:51.120 --> 00:06:54.300
can be applied to life
here on earth right today.

00:06:54.300 --> 00:06:56.280
You can't go to
another star system

00:06:56.280 --> 00:06:58.830
with humans using the
kind of energy systems

00:06:58.830 --> 00:07:00.877
we have today for
space exploration.

00:07:00.877 --> 00:07:03.210
You know, the chemical rockets
and propulsion and stuff.

00:07:03.210 --> 00:07:06.450
We're going to have to generate
tremendous amounts of energy

00:07:06.450 --> 00:07:08.580
through either a
fission where you break

00:07:08.580 --> 00:07:12.600
an atom apart, a fusion,
like what powers the sun,

00:07:12.600 --> 00:07:14.780
or antimatter.

00:07:14.780 --> 00:07:17.760
We do fission now, but
it's not well contained.

00:07:17.760 --> 00:07:20.500
We don't know how to
control it, store it.

00:07:20.500 --> 00:07:24.320
Imagine if we could just
learn how to store this.

00:07:24.320 --> 00:07:27.827
What tremendous value would it
have for life here on earth?

00:07:27.827 --> 00:07:29.910
When you think about going
to another star system,

00:07:29.910 --> 00:07:31.063
you have to be autonomous.

00:07:31.063 --> 00:07:32.730
So you have to carry
your food with you.

00:07:32.730 --> 00:07:36.030
Because if you get to going fast
enough-- it's a hard journey.

00:07:36.030 --> 00:07:38.040
But if you get to going
fast enough so it's

00:07:38.040 --> 00:07:40.190
reasonable within
human lifetimes,

00:07:40.190 --> 00:07:42.280
you're going to have to
carry everything with you.

00:07:42.280 --> 00:07:42.980
You're going to
have to learn how

00:07:42.980 --> 00:07:45.030
to grow your food,
your medical systems.

00:07:45.030 --> 00:07:47.310
And right now when
we're on earth,

00:07:47.310 --> 00:07:51.870
we rely on this wonderful
biodiversity of the soil,

00:07:51.870 --> 00:07:54.840
the microbiome in the soil,
and all of those things

00:07:54.840 --> 00:07:56.370
to grow food.

00:07:56.370 --> 00:07:58.525
We need to understand that.

00:07:58.525 --> 00:08:00.150
We need to know how
to keep it healthy.

00:08:00.150 --> 00:08:02.050
We need to take that with us.

00:08:02.050 --> 00:08:03.660
And so there are
all these pieces,

00:08:03.660 --> 00:08:05.880
financial
infrastructures, we have

00:08:05.880 --> 00:08:07.560
to change to be
able to do something

00:08:07.560 --> 00:08:10.070
that's that long term.

00:08:10.070 --> 00:08:12.180
And if you stop and
think about that,

00:08:12.180 --> 00:08:15.187
those are all things we need
to survive on this planet.

00:08:15.187 --> 00:08:16.770
EMILY FORTUNA: Space
exploration seems

00:08:16.770 --> 00:08:20.460
inevitable with the requisite
what you call team earthlings.

00:08:20.460 --> 00:08:23.860
But at the same time, it seems
like we could be moving faster.

00:08:23.860 --> 00:08:25.432
What do you think
is holding us back?

00:08:25.432 --> 00:08:27.390
MAE C. JEMISON: I think
it's public commitment.

00:08:27.390 --> 00:08:30.420
It's really, we haven't
included people.

00:08:30.420 --> 00:08:35.280
And so folks don't see how space
technology impacts their day

00:08:35.280 --> 00:08:37.890
to day lives even
though it does.

00:08:37.890 --> 00:08:40.200
I live in Houston, Texas.

00:08:40.200 --> 00:08:42.645
We track hurricanes.

00:08:42.645 --> 00:08:44.490
And that's what we know about.

00:08:44.490 --> 00:08:48.360
We knew about the
hole in the ozone

00:08:48.360 --> 00:08:50.670
from space information and data.

00:08:50.670 --> 00:08:53.280
So it's constantly
pervasive in our lives,

00:08:53.280 --> 00:08:54.993
but we haven't told
the story very well.

00:08:54.993 --> 00:08:56.910
And when I say we, that's
I guess the royal we

00:08:56.910 --> 00:08:58.830
about space folks.

00:08:58.830 --> 00:09:01.950
And you tell the story better
by having people involved.

00:09:01.950 --> 00:09:04.695
What was riveting
people when Mars landed?

00:09:04.695 --> 00:09:06.570
You know, the Mars
Lander, remember everybody

00:09:06.570 --> 00:09:07.970
was looking and cheering?

00:09:07.970 --> 00:09:10.920
It's because they could see
it, they could be a part of it.

00:09:10.920 --> 00:09:12.350
And I think that's the piece.

00:09:12.350 --> 00:09:14.225
And I think that's the
piece with all science

00:09:14.225 --> 00:09:14.970
and technology.

00:09:14.970 --> 00:09:17.990
When it's accessible and
when people understand

00:09:17.990 --> 00:09:19.740
that it's part of their
lives and it's not

00:09:19.740 --> 00:09:21.230
this mysterious thing.

00:09:21.230 --> 00:09:23.230
EMILY FORTUNA: And they're
energized and excited

00:09:23.230 --> 00:09:24.582
towards working
towards that goal.

00:09:24.582 --> 00:09:26.457
MAE C. JEMISON: And I
think one of the things

00:09:26.457 --> 00:09:31.380
that space offices and also
other technology innovations

00:09:31.380 --> 00:09:34.260
offer us is that we get
to do bigger things,

00:09:34.260 --> 00:09:37.450
and I think people would like--
they like an adrenaline rush.

00:09:37.450 --> 00:09:40.890
But wouldn't we like to do good
things with adrenaline rush?

00:09:40.890 --> 00:09:43.230
Right now, we
manufacture adrenaline

00:09:43.230 --> 00:09:49.530
rushes in ways that are not
necessarily so productive,

00:09:49.530 --> 00:09:51.325
there you go.

00:09:51.325 --> 00:09:52.200
TIMOTHY JORDAN: Sure.

00:09:52.200 --> 00:09:53.730
How does this
relate to your theme

00:09:53.730 --> 00:09:57.420
of Look Up and the Skyfie
app that you produced?

00:09:57.420 --> 00:10:00.300
MAE C. JEMISON: So Look Up
came from 100 Year Starship,

00:10:00.300 --> 00:10:02.640
and we wanted to connect
people with space,

00:10:02.640 --> 00:10:05.890
and we also wanted to deal with
the issue of human behavior.

00:10:05.890 --> 00:10:08.010
How do we get the
best out of us?

00:10:08.010 --> 00:10:11.100
And we thought, let's ask
people to go out and look up.

00:10:11.100 --> 00:10:13.410
Like when was the last
time you looked up?

00:10:13.410 --> 00:10:15.810
Remember as a kid
looking up at the clouds?

00:10:15.810 --> 00:10:18.420
And it just made you
feel good, right?

00:10:18.420 --> 00:10:20.220
And then I remember
as a child thinking

00:10:20.220 --> 00:10:25.140
that there are other children
on the other side of the planet

00:10:25.140 --> 00:10:26.490
who are looking up as well.

00:10:26.490 --> 00:10:28.110
And what do they see?

00:10:28.110 --> 00:10:29.610
I know they see the same moon.

00:10:29.610 --> 00:10:32.580
In some instances, they see
slightly different stars.

00:10:32.580 --> 00:10:36.100
But it was just one of
those connecting things.

00:10:36.100 --> 00:10:38.580
And so we started
to work on how do we

00:10:38.580 --> 00:10:41.560
create a campaign where
one day around the world,

00:10:41.560 --> 00:10:43.620
we'd all look up?

00:10:43.620 --> 00:10:47.140
And so of course, we had to
make an app for that, right?

00:10:47.140 --> 00:10:49.020
And that's where the
Skyfie app came in.

00:10:49.020 --> 00:10:51.150
So sky, selfie.

00:10:51.150 --> 00:10:56.220
And the idea is to take
an image, a video, a text,

00:10:56.220 --> 00:10:57.720
or whatever when you look up.

00:10:57.720 --> 00:10:59.350
What does it make you feel?

00:10:59.350 --> 00:11:02.670
Post it, and we would
put it on a globe

00:11:02.670 --> 00:11:04.560
that you can twirl
around and you can see

00:11:04.560 --> 00:11:06.250
what everyone else is doing.

00:11:06.250 --> 00:11:09.450
And it's time tagged and
data tagged to where you are,

00:11:09.450 --> 00:11:12.380
and you get to see what
other people are looking at.

00:11:12.380 --> 00:11:17.130
What we came up with is that
we want to do different events.

00:11:17.130 --> 00:11:21.610
So we started on
October 18, one day.

00:11:21.610 --> 00:11:24.435
I'm proud to say we trended
number five on Google

00:11:24.435 --> 00:11:26.980
Play, which is great.

00:11:26.980 --> 00:11:30.750
But in one day, we were able
to garner lots of enthusiasm

00:11:30.750 --> 00:11:32.070
and support around the world.

00:11:32.070 --> 00:11:34.495
Of course, it took a lot of
work to build up to that.

00:11:34.495 --> 00:11:36.120
And now we're continuing
to work on it,

00:11:36.120 --> 00:11:39.465
and we're going to do one also
around the 50th anniversary

00:11:39.465 --> 00:11:40.340
of the Lunar landing.

00:11:40.340 --> 00:11:42.330
I'd like everybody
to-- can I do this?

00:11:42.330 --> 00:11:44.570
The Skyfie app is
completely free.

00:11:44.570 --> 00:11:46.360
No in-app purchases.

00:11:46.360 --> 00:11:48.460
I know that's like,
what is wrong with her?

00:11:48.460 --> 00:11:52.300
No, it's because we want
people to connect and use it.

00:11:52.300 --> 00:11:57.550
And it's available on Google,
it's available on IOS systems.

00:11:57.550 --> 00:12:00.850
And the idea is to
really just connect us.

00:12:00.850 --> 00:12:04.090
And I think as we were
talking before, just

00:12:04.090 --> 00:12:08.360
the act of looking up from your
phone changes your perspective.

00:12:08.860 --> 00:12:11.000
You get bigger, you look out.

00:12:11.000 --> 00:12:14.970
Just the act of looking up
from your desk or your work,

00:12:14.970 --> 00:12:17.410
or just sometimes I'm
walking down the street,

00:12:17.410 --> 00:12:18.940
I'm looking at the ground.

00:12:18.940 --> 00:12:20.890
Looking up, it frees us.

00:12:20.890 --> 00:12:22.660
It gives us energy.

00:12:22.660 --> 00:12:25.060
EMILY FORTUNA: So I
have one more question--

00:12:25.060 --> 00:12:26.055
and this is on behalf--

00:12:26.055 --> 00:12:27.430
MAE C. JEMISON:
Wait, wait, wait.

00:12:27.430 --> 00:12:28.680
You're going to blame Timothy?

00:12:30.700 --> 00:12:34.830
EMILY FORTUNA: Timothy asked
me to ask this question.

00:12:34.830 --> 00:12:36.760
TIMOTHY JORDAN:
Between the two of us,

00:12:36.760 --> 00:12:38.767
the biggest Star Trek
nerd is probably me.

00:12:38.767 --> 00:12:39.850
MAE C. JEMISON: OK, shoot.

00:12:39.850 --> 00:12:42.790
EMILY FORTUNA: So I know
that Nichelle Nichols

00:12:42.790 --> 00:12:44.380
is a huge inspiration to you.

00:12:44.380 --> 00:12:50.860
And I personally believe
that representation in media

00:12:50.860 --> 00:12:53.330
is super important.

00:12:53.330 --> 00:12:56.020
And I wanted to ask
how you got your guest

00:12:56.020 --> 00:12:58.823
appearance on Star Trek?

00:12:58.823 --> 00:13:00.490
MAE C. JEMISON: So
let me just start off

00:13:00.490 --> 00:13:03.610
by saying, when I was growing
up, I loved Star Trek.

00:13:03.610 --> 00:13:07.840
I was in Los Angeles before I
was accepted into the astronaut

00:13:07.840 --> 00:13:11.320
program, and I used to
get my hair cut in LA.

00:13:11.320 --> 00:13:15.520
And the person who cut
my hair was [INAUDIBLE]

00:13:15.520 --> 00:13:18.220
who was a hair dresser
in all these movies

00:13:18.220 --> 00:13:19.130
and things like that.

00:13:19.130 --> 00:13:20.380
And he knew LeVar Burton.

00:13:21.070 --> 00:13:23.080
And [? Sterphon ?] and
I were talking one day,

00:13:23.080 --> 00:13:26.710
and he said, Mae, would you
like to be on-- why don't you

00:13:26.710 --> 00:13:27.888
be on Star Trek?

00:13:27.888 --> 00:13:28.680
And I said, really?

00:13:28.680 --> 00:13:30.140
So he said, I'll
mention it to LeVar.

00:13:30.140 --> 00:13:31.307
So he mentioned it to LeVar.

00:13:31.307 --> 00:13:32.690
So I got a call one night.

00:13:32.690 --> 00:13:36.130
I was at home, and it's like,
hey, Mae, this is LeVar Burton.

00:13:36.130 --> 00:13:38.727
Would you be interested
in being on Star Trek?

00:13:38.727 --> 00:13:40.060
That's what [? Sterphon ?] said.

00:13:40.060 --> 00:13:42.910
And I was like, yeah, sure.

00:13:42.910 --> 00:13:44.620
Like, this is when
the glasses come off.

00:13:44.620 --> 00:13:47.740
Yes, sure.

00:13:47.740 --> 00:13:49.945
And they wrote a part for
me as Lieutenant Palmer.

00:13:49.945 --> 00:13:51.010
TIMOTHY JORDAN: That's awesome.

00:13:51.010 --> 00:13:53.177
MAE C. JEMISON: And that's
what happened, basically.

00:13:53.177 --> 00:13:56.440
It was an episode
LeVar was directing,

00:13:56.440 --> 00:13:58.810
and it was really very neat.

00:13:58.810 --> 00:14:01.870
And I was able to go around have
been sort of one of the Star

00:14:01.870 --> 00:14:05.170
Trek buddies since then, and
was even able to help them

00:14:05.170 --> 00:14:09.430
when they premiered the
Star Trek Discovery back

00:14:09.430 --> 00:14:10.712
last year in New York.

00:14:10.712 --> 00:14:12.045
EMILY FORTUNA: That's wonderful.

00:14:12.045 --> 00:14:13.770
TIMOTHY JORDAN:
That's really cool.

00:14:13.770 --> 00:14:15.520
All right, before we
go, is there anything

00:14:15.520 --> 00:14:17.230
else you'd like to
tell our viewers?

00:14:17.230 --> 00:14:21.070
MAE C. JEMISON: Well, all this
discussion we're having now

00:14:21.070 --> 00:14:24.820
is really around science and
technology and some of those

00:14:24.820 --> 00:14:25.760
issues.

00:14:25.760 --> 00:14:28.780
And it's really important
that we understand

00:14:28.780 --> 00:14:33.910
that we research science
and problems based

00:14:33.910 --> 00:14:35.560
on the people who are there.

00:14:35.560 --> 00:14:38.770
Their ambitions, their dreams,
their goals, their fears.

00:14:38.770 --> 00:14:40.300
And so it's really
important that we

00:14:40.300 --> 00:14:43.750
have as many people as
possible doing that work.

00:14:43.750 --> 00:14:47.980
We design technology and tools
based on the people we are.

00:14:47.980 --> 00:14:50.950
It's important that we have
lots of people involved.

00:14:50.950 --> 00:14:56.050
And we fail to develop and
use so much of the talent

00:14:56.050 --> 00:14:57.200
in this country.

00:14:57.200 --> 00:15:00.700
So I spend a lot of my time
around science literacy,

00:15:00.700 --> 00:15:02.950
and I think it's
vitally important

00:15:02.950 --> 00:15:05.680
that every child
have an opportunity

00:15:05.680 --> 00:15:08.650
for an excellent education,
that every child is

00:15:08.650 --> 00:15:09.670
science literate.

00:15:09.670 --> 00:15:12.360
As adults, that we
don't sort of say, oh,

00:15:12.360 --> 00:15:13.570
I don't know about that.

00:15:13.570 --> 00:15:16.431
You know, that's not cute.

00:15:16.431 --> 00:15:19.270
We need to make sure
that we're fully engaged,

00:15:19.270 --> 00:15:22.390
and that's my task, to
be inclusive and bring

00:15:22.390 --> 00:15:24.100
lots of people along.

00:15:24.100 --> 00:15:25.870
EMILY FORTUNA: Here, here.

00:15:25.870 --> 00:15:28.210
Dr. Jemison, thank you
so much for joining us

00:15:28.210 --> 00:15:29.080
on this show today.

00:15:29.080 --> 00:15:30.580
MAE C. JEMISON:
You're very welcome.

00:15:30.580 --> 00:15:31.720
Thank you.

00:15:31.720 --> 00:15:33.112
I get to shake hands.

00:15:33.112 --> 00:15:34.738
TIMOTHY JORDAN: Thank you.

00:15:34.738 --> 00:15:36.280
EMILY FORTUNA: And
what could be more

00:15:36.280 --> 00:15:38.830
appropriate to follow this talk
than a demo with its own Mars

00:15:38.830 --> 00:15:39.448
Rover?

00:15:39.448 --> 00:15:40.240
Take it away, Todd.

00:15:50.822 --> 00:15:52.920
TODD KERPELMAN:
Hello I/O viewers.

00:15:52.920 --> 00:15:55.600
I am here with Amanda in
the augmented reality booth,

00:15:55.600 --> 00:15:58.510
and Amanda, you're going to show
me something cool from the NASA

00:15:58.510 --> 00:15:59.493
website.

00:15:59.493 --> 00:16:01.660
AMANDA LE: So, here you can
see on the NASA website,

00:16:01.660 --> 00:16:04.270
they're showing 3D
models of their rovers.

00:16:04.270 --> 00:16:06.340
Here, we have the
Curiosity Rover.

00:16:06.340 --> 00:16:08.830
And they're actually using
the Model VR web component

00:16:08.830 --> 00:16:10.927
to put that 3D asset
on their website.

00:16:10.927 --> 00:16:12.510
TODD KERPELMAN: Wow,
that's very cool.

00:16:12.510 --> 00:16:14.560
Now, you said it's
a web component, so

00:16:14.560 --> 00:16:17.560
do they need to NPM install
something to get it working?

00:16:17.560 --> 00:16:19.060
AMANDA LE: It's
actually mark up.

00:16:19.060 --> 00:16:21.060
So very simple to just
put on your website.

00:16:21.060 --> 00:16:22.060
TODD KERPELMAN: Got you.

00:16:22.060 --> 00:16:23.140
Well, that's very cool.

00:16:23.140 --> 00:16:26.850
And this 3D model, what
file format is that?

00:16:26.850 --> 00:16:29.470
AMANDA LE: So we
recommend using GLTF,

00:16:29.470 --> 00:16:31.748
but it also supports
USDZ and GLB.

00:16:31.748 --> 00:16:32.540
TODD KERPELMAN: OK.

00:16:32.540 --> 00:16:34.690
And is that something most
commercial 3D modeling

00:16:34.690 --> 00:16:35.770
tools will export to?

00:16:35.770 --> 00:16:37.490
AMANDA LE: You should be able
to export to that easily.

00:16:37.490 --> 00:16:39.010
TODD KERPELMAN: I see you've got
that little button down there

00:16:39.010 --> 00:16:39.970
in the bottom right.

00:16:39.970 --> 00:16:40.960
What does that do?

00:16:40.960 --> 00:16:42.877
AMANDA LE: So this
actually is the new feature

00:16:42.877 --> 00:16:44.830
that we're launching
at I/O today.

00:16:44.830 --> 00:16:48.040
And what it allows you
to do is open up what

00:16:48.040 --> 00:16:49.390
we call Scene Viewer.

00:16:49.390 --> 00:16:52.450
So Scene Viewer allows you
to take these 3D models

00:16:52.450 --> 00:16:54.640
and actually place it in
your real world space.

00:16:54.640 --> 00:16:57.240
TODD KERPELMAN: Woah, no way.

00:16:57.240 --> 00:16:59.740
AMANDA LE: So here you can see
the Curiosity Rover at scale.

00:16:59.740 --> 00:17:01.323
TODD KERPELMAN: Wow,
it's like there's

00:17:01.323 --> 00:17:05.240
a real life spaceship here
in the booth, but not really.

00:17:05.240 --> 00:17:08.380
It's just actually
very nicely rendered.

00:17:08.380 --> 00:17:09.589
It kind of fooled me.

00:17:09.589 --> 00:17:11.230
AMANDA LE: Yup, and so you
can actually manipulate

00:17:11.230 --> 00:17:12.200
this in your own space.

00:17:12.200 --> 00:17:14.050
So you'll see it at full
scale, but you can also

00:17:14.050 --> 00:17:15.633
shrink it down so
that it can actually

00:17:15.633 --> 00:17:16.710
fit in your living room.

00:17:16.710 --> 00:17:17.710
TODD KERPELMAN: OK, yes.

00:17:17.710 --> 00:17:21.040
So if I want to put the
Curiosity Rover on my coffee

00:17:21.040 --> 00:17:23.589
table, I now can just by
shrinking it down a little bit.

00:17:23.589 --> 00:17:24.250
AMANDA LE: Yup.

00:17:24.250 --> 00:17:27.609
TODD KERPELMAN: And so how did
you get the size specification?

00:17:27.609 --> 00:17:29.860
Like apparently this
was built real life,

00:17:29.860 --> 00:17:31.330
or this is showing
it in real life

00:17:31.330 --> 00:17:32.370
and you're able
to shrink it down?

00:17:32.370 --> 00:17:34.620
AMANDA LE: Yeah, so NASA
when they built the 3D model,

00:17:34.620 --> 00:17:36.307
they decided to
build it at scale.

00:17:36.307 --> 00:17:37.890
So that's the model
we're referencing.

00:17:37.890 --> 00:17:38.890
TODD KERPELMAN: Got you.

00:17:38.890 --> 00:17:39.850
Very cool.

00:17:39.850 --> 00:17:43.750
And so if I'm a developer and
I want to turn my model viewer

00:17:43.750 --> 00:17:45.940
component into this
cool scene viewer

00:17:45.940 --> 00:17:49.108
augmented reality experience,
what do I have to do?

00:17:49.108 --> 00:17:51.400
AMANDA LE: So there's no need
to build an app for this.

00:17:51.400 --> 00:17:54.750
All you actually do is
go into the mark-up,

00:17:54.750 --> 00:17:56.440
and you add the AR
attribute, which

00:17:56.440 --> 00:17:58.960
is literally the letters a
and r, so super simple to do.

00:17:58.960 --> 00:18:01.210
TODD KERPELMAN: I type two
letters and I get all that.

00:18:01.210 --> 00:18:01.660
AMANDA LE: Yes.

00:18:01.660 --> 00:18:03.702
TODD KERPELMAN: All right,
well that sounds easy.

00:18:03.702 --> 00:18:04.990
I think even I could do that.

00:18:04.990 --> 00:18:07.240
Well, I could use a little
pick me up after that demo.

00:18:07.240 --> 00:18:12.090
Let's head out over here and
see if I can find some caffeine.

00:18:12.090 --> 00:18:12.948
Hey, Ashish.

00:18:12.948 --> 00:18:14.490
ASHISH SHAH: How
are you doing, Todd?

00:18:14.490 --> 00:18:15.520
TODD KERPELMAN: I am doing well.

00:18:15.520 --> 00:18:17.050
I'm already excited
for this demo,

00:18:17.050 --> 00:18:19.330
because I see there's an
Espresso machine here.

00:18:19.330 --> 00:18:21.610
So walk me through what
you're showing us today.

00:18:21.610 --> 00:18:23.850
ASHISH SHAH: All right,
so what are doing in here

00:18:23.850 --> 00:18:27.460
is we're showing an espresso
machine coming to life in AR

00:18:27.460 --> 00:18:30.640
using this technology called
augmented images that's

00:18:30.640 --> 00:18:31.400
part of AR Core.

00:18:31.400 --> 00:18:32.650
Do you want to give it a spin?

00:18:32.650 --> 00:18:34.260
TODD KERPELMAN: Yes,
let's check it out.

00:18:34.260 --> 00:18:34.840
ASHISH SHAH: All
right, so as soon

00:18:34.840 --> 00:18:36.047
as you look at this marker--

00:18:36.047 --> 00:18:38.130
TODD KERPELMAN: So you're
showing the image there,

00:18:38.130 --> 00:18:40.190
and it's telling
you to look around.

00:18:40.190 --> 00:18:44.020
ASHISH SHAH: And now it's
annotated the machine

00:18:44.020 --> 00:18:45.520
with information
about its features.

00:18:45.520 --> 00:18:47.812
TODD KERPELMAN: I see the
little pieces coming to life.

00:18:47.812 --> 00:18:49.450
ASHISH SHAH: Yep,
it's telling you

00:18:49.450 --> 00:18:52.318
about those control grinding--

00:18:52.318 --> 00:18:54.110
TODD KERPELMAN: I need
my doses controlled.

00:18:54.110 --> 00:18:55.818
ASHISH SHAH: And the
precise temperature.

00:18:55.818 --> 00:18:57.550
What would you do without that?

00:18:57.550 --> 00:19:00.790
And you could actually see
an espresso being made here.

00:19:00.790 --> 00:19:02.110
TODD KERPELMAN: Oh, fantastic.

00:19:02.110 --> 00:19:04.810
ASHISH SHAH: This is a really
powerful way for manufacturers

00:19:04.810 --> 00:19:07.870
to show features
of their product

00:19:07.870 --> 00:19:10.510
which are otherwise
just lost in text,

00:19:10.510 --> 00:19:13.550
and this really makes
the product come to life

00:19:13.550 --> 00:19:15.280
in a really powerful way.

00:19:15.280 --> 00:19:18.700
And so this triggers the
experience once it recognizes.

00:19:18.700 --> 00:19:21.670
And now what happens once
this image is recognized,

00:19:21.670 --> 00:19:23.740
it starts establishing
a correspondence

00:19:23.740 --> 00:19:25.572
between the phone and the image.

00:19:25.572 --> 00:19:27.280
TODD KERPELMAN: So
the phone always knows

00:19:27.280 --> 00:19:28.970
exactly where it is in
response to this image.

00:19:28.970 --> 00:19:30.460
ASHISH SHAH: With
respect to the image.

00:19:30.460 --> 00:19:32.752
And when I say it establishes
a correspondence in terms

00:19:32.752 --> 00:19:35.080
of oppose, that's
position and orientation.

00:19:35.080 --> 00:19:39.980
So position in x
direction and y direction

00:19:39.980 --> 00:19:43.600
and in z direction, and also
the orientation in three degrees

00:19:43.600 --> 00:19:45.250
like pitch, yaw, and roll.

00:19:45.250 --> 00:19:47.080
So the phone knows
exactly where it

00:19:47.080 --> 00:19:48.490
is with respect to the marker.

00:19:48.490 --> 00:19:49.100
TODD KERPELMAN: Gotcha.

00:19:49.100 --> 00:19:50.600
So this kind of
works best in places

00:19:50.600 --> 00:19:53.020
where you have strict
control over the environment

00:19:53.020 --> 00:19:55.520
in which this image is going
to appear and what's around it.

00:19:55.520 --> 00:19:56.260
ASHISH SHAH: Yes.

00:19:56.260 --> 00:19:58.360
If you're trying to
augment real things,

00:19:58.360 --> 00:20:00.627
you want to make sure
that the image is

00:20:00.627 --> 00:20:03.210
rigid with respect to the thing
that you're trying to augment.

00:20:03.210 --> 00:20:03.520
TODD KERPELMAN: Gotcha.

00:20:03.520 --> 00:20:05.560
But if you just want the image
itself specifically to come

00:20:05.560 --> 00:20:06.370
to life, that's all you need.

00:20:06.370 --> 00:20:07.370
ASHISH SHAH: Absolutely.

00:20:07.370 --> 00:20:10.970
So think about a scenario
where you're at a store,

00:20:10.970 --> 00:20:13.120
and you want to see
what's inside a box.

00:20:13.120 --> 00:20:15.290
So you could use an
app to look at the box.

00:20:15.290 --> 00:20:17.513
It would recognize
the box as a marker,

00:20:17.513 --> 00:20:19.180
and you could actually
have a sneak peek

00:20:19.180 --> 00:20:21.013
at what's inside the box.

00:20:21.013 --> 00:20:23.430
TODD KERPELMAN: Oh, so the
model could pop out of the box,

00:20:23.430 --> 00:20:26.972
and you could see exactly what
my waffle iron or toaster--

00:20:26.972 --> 00:20:28.180
I need stuff for my kitchen--

00:20:28.180 --> 00:20:28.800
that's all going to look like.

00:20:28.800 --> 00:20:30.950
ASHISH SHAH: Without
even opening it up.

00:20:30.950 --> 00:20:33.640
TODD KERPELMAN: And then I won't
get yelled at by shopkeepers.

00:20:33.640 --> 00:20:35.057
Well, thank you
very much, Ashish.

00:20:35.057 --> 00:20:37.240
I learned a lot about
augmented reality today.

00:20:37.240 --> 00:20:39.180
I'm going to grab
this espresso--

00:20:39.180 --> 00:20:40.968
oh, it's not real.

00:20:40.968 --> 00:20:42.010
But the coffee beans are.

00:20:42.010 --> 00:20:43.030
Can I eat the coffee beans?

00:20:43.030 --> 00:20:43.780
ASHISH SHAH: Yes, you can.

00:20:43.780 --> 00:20:45.790
TODD KERPELMAN: I'm going to
take a few coffee beans to go.

00:20:45.790 --> 00:20:47.123
Thank you very much live stream.

00:20:47.123 --> 00:20:50.350
I'm going to go get caffeinated.

00:20:50.350 --> 00:20:53.200
EMILY FORTUNA: AR really spans
the outer limits, up to Mars

00:20:53.200 --> 00:20:55.150
and back down to a
simple cup of coffee.

00:20:55.150 --> 00:20:57.210
TIMOTHY JORDAN: Well,
one fancy cup of coffee.

00:20:57.210 --> 00:20:59.320
And you have a minute
to grab one yourself

00:20:59.320 --> 00:21:00.580
while we take a quick break.

00:21:00.580 --> 00:21:02.410
The next sessions
are about to begin.

00:21:02.410 --> 00:21:06.027
You're watching I/O live from
Mountain View, California.

00:21:30.840 --> 00:21:32.100
Welcome back to I/O Live.

00:21:32.100 --> 00:21:33.750
I'm Timothy Jordan,
and we're honored

00:21:33.750 --> 00:21:35.790
to have renowned artist
Sougwen Chung here

00:21:35.790 --> 00:21:36.990
to talk about her work.

00:21:36.990 --> 00:21:37.695
Welcome.

00:21:37.695 --> 00:21:38.445
SOUGWEN CHUNG: Hi.

00:21:38.445 --> 00:21:40.362
TIMOTHY JORDAN: Now,
I've been looking forward

00:21:40.362 --> 00:21:43.120
to your performance here at I/O,
and I'd like to talk about it.

00:21:43.120 --> 00:21:45.090
But first, how about
some background?

00:21:45.090 --> 00:21:48.378
When did you start working
with robots and why?

00:21:48.378 --> 00:21:49.920
SOUGWEN CHUNG: It's
a great question.

00:21:49.920 --> 00:21:53.010
I get asked that all the time
I started working with robots

00:21:53.010 --> 00:21:54.600
about five years ago.

00:21:54.600 --> 00:21:56.820
I've been kind of
a digital nomad.

00:21:56.820 --> 00:21:59.070
I learned to code when
I was quite young,

00:21:59.070 --> 00:22:01.800
and I got really
interested in ways

00:22:01.800 --> 00:22:06.600
to take what's present inside
the simulation of the computer

00:22:06.600 --> 00:22:07.900
into the physical world.

00:22:07.900 --> 00:22:12.570
So I started with a really
simple drawing collaboration

00:22:12.570 --> 00:22:16.852
with my first robotic arm, and
it's just taken off ever since.

00:22:16.852 --> 00:22:18.810
TIMOTHY JORDAN: How has
the experience of, say,

00:22:18.810 --> 00:22:20.730
human machine
collaboration changed

00:22:20.730 --> 00:22:22.650
your process as an artist?

00:22:22.650 --> 00:22:24.570
SOUGWEN CHUNG: I think
about that a lot,

00:22:24.570 --> 00:22:28.410
because I think it has changed
it quite significantly.

00:22:28.410 --> 00:22:33.500
I've gotten more accustomed
to chaos, to put it mildly.

00:22:33.500 --> 00:22:35.750
TIMOTHY JORDAN: Things going
not the way you expected?

00:22:35.750 --> 00:22:39.240
SOUGWEN CHUNG: Yeah, there's
a lot of erratic emergent

00:22:39.240 --> 00:22:42.540
behavior or assigned
attributed behavior

00:22:42.540 --> 00:22:46.140
to working with these machines
that really got me out

00:22:46.140 --> 00:22:50.040
of outside of my comfort zone
from just my simple drawing

00:22:50.040 --> 00:22:50.580
practice.

00:22:50.580 --> 00:22:56.700
I think there's something really
[INAUDIBLE] and the cognitive

00:22:56.700 --> 00:23:00.330
about mark making that when
you do that with a robotic

00:23:00.330 --> 00:23:02.750
articulated unit, it--

00:23:02.750 --> 00:23:04.800
there's a lot of new surprises.

00:23:04.800 --> 00:23:07.050
I like to think of working
with these machines

00:23:07.050 --> 00:23:09.780
as a creative catalyst,
so that ends up

00:23:09.780 --> 00:23:11.460
being quite inspiring for me.

00:23:11.460 --> 00:23:12.720
TIMOTHY JORDAN: Interesting.

00:23:12.720 --> 00:23:15.570
All right, let's talk about
the performance a little bit.

00:23:15.570 --> 00:23:17.160
It was beautiful and elegant.

00:23:17.160 --> 00:23:18.202
SOUGWEN CHUNG: Thank you.

00:23:18.202 --> 00:23:19.743
TIMOTHY JORDAN: I
expected that part,

00:23:19.743 --> 00:23:21.630
because I looked at
your work online.

00:23:21.630 --> 00:23:23.970
What I didn't expect was
something completely different,

00:23:23.970 --> 00:23:26.640
that your relationship
with the robots

00:23:26.640 --> 00:23:30.360
felt if I can say gentle
and familial, like they

00:23:30.360 --> 00:23:31.420
were a family.

00:23:31.420 --> 00:23:32.503
SOUGWEN CHUNG: Yeah, yeah.

00:23:32.503 --> 00:23:34.098
They pretty much
are at this point.

00:23:34.098 --> 00:23:35.890
TIMOTHY JORDAN: I mean,
tell me about that.

00:23:35.890 --> 00:23:39.240
How much of what you're
doing about the art

00:23:39.240 --> 00:23:42.825
is about that relationship
rather than the marks

00:23:42.825 --> 00:23:44.010
that you're making?

00:23:44.010 --> 00:23:45.400
SOUGWEN CHUNG: Yeah, definitely.

00:23:45.400 --> 00:23:49.025
So I implicate the tool as a
collaborator, and I think--

00:23:49.025 --> 00:23:52.230
I frame it as a
collaboration, because we

00:23:52.230 --> 00:23:57.000
want to facilitate kind,
generous collaborations

00:23:57.000 --> 00:23:59.650
in the world between
humans to humans.

00:23:59.650 --> 00:24:03.310
And I think when I regard these
machines as my collaborators,

00:24:03.310 --> 00:24:08.190
it's a way of just creating
a more empathetic model.

00:24:08.190 --> 00:24:12.150
In this context, it's
behavioral empathy.

00:24:12.150 --> 00:24:16.140
But when I'm on stage
performing with these robots,

00:24:16.140 --> 00:24:22.690
I think it's very much me and
them and this captive audience,

00:24:22.690 --> 00:24:28.710
and that mutual reliance
and co-creation ends up

00:24:28.710 --> 00:24:32.050
spreading a very emotional
connection with them.

00:24:32.050 --> 00:24:34.550
And I think that we've evolved.

00:24:34.550 --> 00:24:36.830
Like my process has
evolved significantly

00:24:36.830 --> 00:24:41.480
with this collaborative
work, and they have as well.

00:24:41.480 --> 00:24:46.490
I think it's a positive way
of slanting anthromorphism

00:24:46.490 --> 00:24:49.258
that's just been really
inspiring for the work.

00:24:49.258 --> 00:24:51.050
TIMOTHY JORDAN: Did
you have to do anything

00:24:51.050 --> 00:24:53.425
to build that relationship
like hang out with him outside

00:24:53.425 --> 00:24:55.040
of work?

00:24:55.040 --> 00:24:57.650
SOUGWEN CHUNG: Yeah, we get
a few beers every so often.

00:24:57.650 --> 00:24:58.510
Yeah, no.

00:24:58.510 --> 00:25:02.540
It's honestly, there's so much
vitality and vulnerability

00:25:02.540 --> 00:25:07.250
when doing these
performances that we've

00:25:07.250 --> 00:25:10.130
kind of been in this together
with the risk of sounding a bit

00:25:10.130 --> 00:25:11.030
cheesy.

00:25:11.030 --> 00:25:14.300
Has made it feel like a
really close connection.

00:25:14.300 --> 00:25:16.940
In addition to the
robotic arms, I've

00:25:16.940 --> 00:25:19.700
been designing a multi
robotic system that's actually

00:25:19.700 --> 00:25:22.850
a series of 20
painting robots that

00:25:22.850 --> 00:25:26.840
also is an articulation
of this robotic form

00:25:26.840 --> 00:25:28.400
in a different way.

00:25:28.400 --> 00:25:30.650
Instead of one to
one, it's one to many.

00:25:30.650 --> 00:25:32.780
And that is a totally
different relationship.

00:25:32.780 --> 00:25:36.950
It's almost like a natural swarm
that I'm really excited about.

00:25:36.950 --> 00:25:39.470
So that reframes the
relationship too.

00:25:39.470 --> 00:25:42.740
Yeah, it's embodied
kinetic sculpture.

00:25:42.740 --> 00:25:46.190
That's only possible
in today's moments,

00:25:46.190 --> 00:25:47.315
and that really excites me.

00:25:47.315 --> 00:25:48.690
TIMOTHY JORDAN:
It's really cool.

00:25:48.690 --> 00:25:49.670
SOUGWEN CHUNG: Thanks.

00:25:49.670 --> 00:25:51.530
TIMOTHY JORDAN: OK,
the themes of the work

00:25:51.530 --> 00:25:53.480
that you performed here--

00:25:53.480 --> 00:25:57.320
mimicry, memory, and
future speculations.

00:25:57.320 --> 00:25:59.060
How did you land on this?

00:25:59.060 --> 00:26:02.870
SOUGWEN CHUNG: So the mimicry
was the first generation.

00:26:02.870 --> 00:26:08.870
I call my robots Doug,
because the project is named--

00:26:08.870 --> 00:26:12.510
let's just drop the formalities
and call them what they are.

00:26:12.510 --> 00:26:15.110
No, I started the
project, and it's

00:26:15.110 --> 00:26:18.770
called Drawing Operations Unit
Generation One was focused

00:26:18.770 --> 00:26:22.770
on mimicry, and
that was DOUG One,

00:26:22.770 --> 00:26:26.390
and it's sort of
grown over time.

00:26:26.390 --> 00:26:30.350
It's a mimicry, memory,
and future speculations,

00:26:30.350 --> 00:26:34.610
because those are the three
chapters of this evolving

00:26:34.610 --> 00:26:36.350
generational project.

00:26:36.350 --> 00:26:38.720
So I'm really excited
about the third,

00:26:38.720 --> 00:26:42.110
because it shows multi
robotic collaboration

00:26:42.110 --> 00:26:45.452
alongside my human
drawing agency.

00:26:45.452 --> 00:26:46.910
And that's very
much where the work

00:26:46.910 --> 00:26:50.180
is headed, towards a
more collaborative, more

00:26:50.180 --> 00:26:51.917
collective idea.

00:26:51.917 --> 00:26:53.750
TIMOTHY JORDAN: Is there
a fourth generation

00:26:53.750 --> 00:26:54.670
you have in mind?

00:26:54.670 --> 00:26:55.920
SOUGWEN CHUNG: Yeah, there is.

00:26:55.920 --> 00:26:57.650
Well, the third
generation is actually

00:26:57.650 --> 00:27:00.620
the multi robotic system,
which is the painting ones.

00:27:00.620 --> 00:27:03.730
But there's a fourth generation
that I can't talk about yet,

00:27:03.730 --> 00:27:06.560
but it'll be coming soon.

00:27:06.560 --> 00:27:08.810
TIMOTHY JORDAN: Awesome,
well I'll look forward to it.

00:27:08.810 --> 00:27:11.450
SOUGWEN CHUNG: I'm
excited to share.

00:27:11.450 --> 00:27:14.090
TIMOTHY JORDAN: The score
for the work by Aquariun,

00:27:14.090 --> 00:27:16.560
how did that contribute
to your work?

00:27:16.560 --> 00:27:19.025
SOUGWEN CHUNG: It
contributed so much.

00:27:19.025 --> 00:27:24.680
Acquariun has been a good
friend of mine for quite a while

00:27:24.680 --> 00:27:27.560
and does some
incredible music that

00:27:27.560 --> 00:27:31.280
is actually quite brilliant.

00:27:31.280 --> 00:27:32.690
Like, I think I
was describing it

00:27:32.690 --> 00:27:37.460
earlier as it's
futuristic but also taps

00:27:37.460 --> 00:27:41.540
into this nostalgia about
jungle and drum and bass

00:27:41.540 --> 00:27:43.730
that's really,
really interesting.

00:27:43.730 --> 00:27:48.470
I sort of asked him to come
up with the score for me

00:27:48.470 --> 00:27:51.350
and discussed with
him about the themes

00:27:51.350 --> 00:27:53.040
I'd like him to interpret.

00:27:53.040 --> 00:27:57.680
So he really came up with
a beautiful melodic work

00:27:57.680 --> 00:28:00.290
that really adds so
much to the piece.

00:28:00.290 --> 00:28:04.400
We were really excited to
collaborate with and use the 40

00:28:04.400 --> 00:28:09.080
Sound System from Monome
and spatialize this work.

00:28:09.080 --> 00:28:11.880
One component of the
performance that we did,

00:28:11.880 --> 00:28:14.720
especially for I/O,
was the integration

00:28:14.720 --> 00:28:17.580
of a Contact Mic component.

00:28:17.580 --> 00:28:20.270
So in the beginning,
my drawing gestures

00:28:20.270 --> 00:28:25.530
actually form architectural
sound throughout the space.

00:28:25.530 --> 00:28:28.520
And it was really cool
to control the universe

00:28:28.520 --> 00:28:29.810
of the performance like that.

00:28:29.810 --> 00:28:32.478
So we were very excited about
it and hope to do again soon.

00:28:32.478 --> 00:28:33.770
TIMOTHY JORDAN: That's awesome.

00:28:33.770 --> 00:28:35.618
One more question.

00:28:35.618 --> 00:28:36.410
I have to ask this.

00:28:36.410 --> 00:28:39.470
What's the biggest surprise
your sort of robot partner

00:28:39.470 --> 00:28:41.465
has ever given you
in a performance?

00:28:41.465 --> 00:28:43.640
SOUGWEN CHUNG: I like
your verbiage with that.

00:28:43.640 --> 00:28:47.570
It's a very-- because I do
think about it as like giving

00:28:47.570 --> 00:28:49.700
of the artistic behavior.

00:28:49.700 --> 00:28:52.250
I think it's been
incredibly surprising.

00:28:52.250 --> 00:28:55.880
I think one of the reasons I've
been working with neural nets

00:28:55.880 --> 00:29:00.980
is its ability to and its
native ability to generate

00:29:00.980 --> 00:29:05.590
non-human unexpected behaviors.

00:29:05.590 --> 00:29:08.470
It's been a really interesting
way of looking at my own work,

00:29:08.470 --> 00:29:12.760
because I train all my neural
nets on my own drawings

00:29:12.760 --> 00:29:15.400
to kind of re-examine
the type of work I do

00:29:15.400 --> 00:29:18.740
and see how it feels to
be articulated by a unit.

00:29:18.740 --> 00:29:21.070
So I think it's
constantly surprising

00:29:21.070 --> 00:29:23.980
me, which is probably not the
answer you're looking for.

00:29:23.980 --> 00:29:26.480
TIMOTHY JORDAN: I don't know,
I think that's a great answer.

00:29:26.480 --> 00:29:28.420
SOUGWEN CHUNG: I
will say what I do

00:29:28.420 --> 00:29:33.230
find surprising about the duets
is not only the robot's output,

00:29:33.230 --> 00:29:36.940
but the response to it has
been really interesting.

00:29:36.940 --> 00:29:40.900
And it's really taken off a
lot of speculative questions

00:29:40.900 --> 00:29:44.530
about technology and
philosophy and sort

00:29:44.530 --> 00:29:48.160
of what it means to make
work as an artist today.

00:29:48.160 --> 00:29:51.370
So that's been the most
surprising connection

00:29:51.370 --> 00:29:54.430
that this performance
has created for me.

00:29:54.430 --> 00:29:56.030
TIMOTHY JORDAN:
Thank you for that.

00:29:56.030 --> 00:29:58.090
And thanks for sharing
your vision and your art

00:29:58.090 --> 00:29:59.410
with us here at I/O 19.

00:29:59.410 --> 00:30:01.252
SOUGWEN CHUNG: Thank you.

00:30:01.252 --> 00:30:02.710
TIMOTHY JORDAN:
Next up, let's join

00:30:02.710 --> 00:30:03.985
Wayne for more in the sandbox.

00:30:08.032 --> 00:30:09.740
WAYNE PIEKARSKI: Hi,
I'm Wayne Piekarski,

00:30:09.740 --> 00:30:12.170
and welcome to Google I/O 2019.

00:30:12.170 --> 00:30:14.225
We're here at
Shoreline Amphitheatre,

00:30:14.225 --> 00:30:16.850
where there's a bunch of really
cool demos and exhibits and fun

00:30:16.850 --> 00:30:17.480
things to do.

00:30:17.480 --> 00:30:18.980
So what I'm going
to do is I'm going

00:30:18.980 --> 00:30:21.105
to take you on a live guided
tour of all the really

00:30:21.105 --> 00:30:22.073
cool stuff here.

00:30:22.073 --> 00:30:23.240
So let's go and get started.

00:30:27.230 --> 00:30:27.890
Oh wow, look.

00:30:27.890 --> 00:30:30.015
You can see all of these
augmented reality flowers.

00:30:33.740 --> 00:30:34.240
Wow, look.

00:30:34.240 --> 00:30:37.450
I got augmented reality
glasses and hair.

00:30:37.450 --> 00:30:39.745
So we've got Flutter running
on iOS and Android devices

00:30:39.745 --> 00:30:40.870
and tablets and everything.

00:30:40.870 --> 00:30:41.495
It looks great.

00:30:41.495 --> 00:30:42.680
And I got a free snack.

00:30:49.880 --> 00:30:51.258
OK, let's try this one here.

00:30:51.258 --> 00:30:52.550
What's the temperature outside?

00:30:55.150 --> 00:30:56.620
A perfect day.

00:30:56.620 --> 00:30:57.490
Exactly.

00:30:57.490 --> 00:30:59.980
It's always like that at
I/O. All right, let's go.

00:30:59.980 --> 00:31:02.710
I guess I should see what the
latest news is from my bosses

00:31:02.710 --> 00:31:03.210
here.

00:31:03.210 --> 00:31:05.877
So it's really cool, because you
can use either a remote control

00:31:05.877 --> 00:31:08.140
or a game controller, and
you can control your TV

00:31:08.140 --> 00:31:12.130
to play games, watch YouTube
videos, see what's going on.

00:31:12.130 --> 00:31:14.311
Experiments with Google.

00:31:14.311 --> 00:31:15.907
So what we're
doing here is we've

00:31:15.907 --> 00:31:17.740
got a machine learning
algorithm that's been

00:31:17.740 --> 00:31:19.330
trained with animal sounds.

00:31:19.330 --> 00:31:21.220
And the more accurately
you can make them,

00:31:21.220 --> 00:31:24.390
the faster your little tractor
drives around the track here.

00:31:24.390 --> 00:31:27.620
This is so much fun.

00:31:27.620 --> 00:31:29.030
How was your experiment?

00:31:29.030 --> 00:31:31.640
Awesome.

00:31:31.640 --> 00:31:34.040
All right, so here,
we're in the gaming area,

00:31:34.040 --> 00:31:36.890
where we're seeing lots of
demos, interesting things.

00:31:36.890 --> 00:31:38.390
These are all
targeted at developers

00:31:38.390 --> 00:31:40.898
who actually make games for
Android and web and so forth.

00:31:40.898 --> 00:31:43.190
So there's a lot of really
interesting news and updates

00:31:43.190 --> 00:31:44.030
for them.

00:31:44.030 --> 00:31:46.430
But what we should
do now is let's

00:31:46.430 --> 00:31:49.890
go and check out what Stadia
has to offer for a demo.

00:31:49.890 --> 00:31:51.410
Let's check it out.

00:31:51.410 --> 00:31:52.910
So here we have
Stadia, which allows

00:31:52.910 --> 00:31:54.870
us to play games on the Cloud.

00:31:54.870 --> 00:31:56.030
So what do we have here?

00:31:56.030 --> 00:31:58.540
SPEAKER 15: This is a developer
tool demo that we have.

00:31:58.540 --> 00:32:02.180
So what you see here is
we've taken this 2D image,

00:32:02.180 --> 00:32:04.600
and we've trained a machine
learning model offline,

00:32:04.600 --> 00:32:07.530
and we can apply it directly
in the game at runtime.

00:32:07.530 --> 00:32:09.280
WAYNE PIEKARSKI: Let's
take a walk around.

00:32:09.280 --> 00:32:11.120
Can I play it?

00:32:11.120 --> 00:32:12.800
So this is using shaders, or--

00:32:12.800 --> 00:32:14.883
SPEAKER 15: It's using
machine learning technology

00:32:14.883 --> 00:32:15.530
with shaders.

00:32:15.530 --> 00:32:16.490
WAYNE PIEKARSKI: Wow,
this is really cool.

00:32:16.490 --> 00:32:17.600
Thanks very much.

00:32:17.600 --> 00:32:20.100
All right, let's get out of
here and see what else there is.

00:32:24.680 --> 00:32:27.305
You are talking to this.

00:32:27.305 --> 00:32:28.250
Oh, come on.

00:32:32.892 --> 00:32:33.725
Oh, I got something.

00:32:33.725 --> 00:32:35.653
This thing is insanely accurate.

00:32:35.653 --> 00:32:37.820
All right, you can check
out my poses on the screen.

00:32:37.820 --> 00:32:38.590
Oh, look at this.

00:32:38.590 --> 00:32:39.880
What a superstar.

00:32:39.880 --> 00:32:43.250
OK, so is the Android
auto area, where we have

00:32:43.250 --> 00:32:44.993
Android that runs in vehicles.

00:32:44.993 --> 00:32:46.910
So what we see here is
that we've got actually

00:32:46.910 --> 00:32:49.520
a head unit from a
vehicle mounted here,

00:32:49.520 --> 00:32:52.500
and then the phone is
projecting onto the screen.

00:32:52.500 --> 00:32:55.800
And so the phone can allow
us to run Google Maps,

00:32:55.800 --> 00:32:57.742
make phone calls,
play music, whatever.

00:32:57.742 --> 00:33:00.200
It's really neat, because you
can do all the same functions

00:33:00.200 --> 00:33:01.617
from your phone,
but you can do it

00:33:01.617 --> 00:33:03.300
in a nice head mounted
in your vehicle.

00:33:03.300 --> 00:33:05.480
Let's go see if we can get
a sneak peek inside one

00:33:05.480 --> 00:33:07.673
of the cars themselves.

00:33:07.673 --> 00:33:10.165
Come on, let's go.

00:33:10.165 --> 00:33:12.150
All right, here we have
and Android AutoCast.

00:33:12.150 --> 00:33:13.130
This is exciting.

00:33:13.130 --> 00:33:15.760
So we've got Google
Maps, we've our phone,

00:33:15.760 --> 00:33:17.950
we can make calls with
it, listen to our music.

00:33:17.950 --> 00:33:21.360
So let's get ready
to take a drive.

00:33:21.360 --> 00:33:23.830
So the Android Auto people
wouldn't let me take their car,

00:33:23.830 --> 00:33:26.140
so instead I thought I'd
come over here and try out

00:33:26.140 --> 00:33:27.670
this virtual moped
ride, where I get

00:33:27.670 --> 00:33:29.560
to ride through the
streets of Jakarta

00:33:29.560 --> 00:33:33.460
watching a 360 video on these
virtual reality glasses here.

00:33:33.460 --> 00:33:36.040
What's really cool is
you can do a 360 video.

00:33:36.040 --> 00:33:37.540
When you're sitting
in a motorcycle,

00:33:37.540 --> 00:33:39.516
it feels so
incredibly realistic.

00:33:43.530 --> 00:33:46.510
All right, well that was
an amazing whirlwind tour

00:33:46.510 --> 00:33:49.520
of Jakarta on a 360
degree YouTube video.

00:33:49.520 --> 00:33:51.700
And this is also a great
tour of the Sandboxes

00:33:51.700 --> 00:33:53.512
at Google I/O 2019.

00:33:53.512 --> 00:33:54.970
I hope you had as
much fun as I did

00:33:54.970 --> 00:33:56.678
watching everything
that's going on here,

00:33:56.678 --> 00:33:59.558
and I'll see you next time.

00:33:59.558 --> 00:34:01.600
TIMOTHY JORDAN: The Sandbox
is such a great place

00:34:01.600 --> 00:34:03.460
to really become
immersed in everything

00:34:03.460 --> 00:34:05.130
featured at I/O this year.

00:34:05.130 --> 00:34:07.840
And if you missed any part
of I/O live, you're in luck.

00:34:07.840 --> 00:34:11.710
You can check it all
out at g.co/io/live.

00:34:11.710 --> 00:34:13.204
We'll see you after
these sessions.

00:34:25.236 --> 00:34:27.600
NARRATOR: Learn to
architect and develop

00:34:27.600 --> 00:34:30.060
Android apps in the Kotlin
programming language

00:34:30.060 --> 00:34:33.360
using industry proven
tools and libraries.

00:34:33.360 --> 00:34:36.780
Creating Android apps in Kotlin
is a course developed by Google

00:34:36.780 --> 00:34:38.790
together with Udacity.

00:34:38.790 --> 00:34:42.000
You'll learn by building real
Android apps using industry

00:34:42.000 --> 00:34:45.360
best practices with
modern app architecture.

00:34:45.360 --> 00:34:48.750
Understand why and how to use
Andrew Jetpack components,

00:34:48.750 --> 00:34:51.389
such as room for
databases, Work Manager

00:34:51.389 --> 00:34:53.969
for background processing,
the new navigation

00:34:53.969 --> 00:34:55.800
components, and more.

00:34:55.800 --> 00:34:58.380
You'll use key Kotlin features
to write your app code

00:34:58.380 --> 00:35:01.140
more quickly and concisely.

00:35:01.140 --> 00:35:02.730
Learning to develop
on Android is

00:35:02.730 --> 00:35:05.610
much more than learning
APIs and shortcuts.

00:35:05.610 --> 00:35:09.090
It's training your brain to
think like a mobile developer.

00:35:09.090 --> 00:35:11.110
Come learn with us.

00:35:11.110 --> 00:35:14.160
For more information on this
free course and to see all

00:35:14.160 --> 00:35:16.170
of Google's courses
with Udacity,

00:35:16.170 --> 00:35:35.675
go to udacity.com/google.

00:35:35.675 --> 00:35:37.050
TIMOTHY JORDAN:
One of the things

00:35:37.050 --> 00:35:40.040
I love about I/O is
the intersection of art

00:35:40.040 --> 00:35:42.540
and technology, which is why
we're able to speak with people

00:35:42.540 --> 00:35:46.110
like Amit, the director of
Google's Arts and Culture,

00:35:46.110 --> 00:35:48.480
and Google Distinguished
Scientist, Blaise.

00:35:48.480 --> 00:35:49.655
Welcome both to the show.

00:35:49.655 --> 00:35:50.340
AMIT SOOD: Thank you.

00:35:50.340 --> 00:35:51.085
TIMOTHY JORDAN:
How are you doing?

00:35:51.085 --> 00:35:52.040
AMIT SOOD: Very well.

00:35:52.040 --> 00:35:52.670
You?

00:35:52.670 --> 00:35:54.628
TIMOTHY JORDAN: I'm doing
really, really great.

00:35:54.628 --> 00:35:59.220
So Amit, let's start with some
background on Google's Cultural

00:35:59.220 --> 00:36:01.680
Institute and Google's
arts and cultures.

00:36:01.680 --> 00:36:04.260
How did it get started,
and what's the mission?

00:36:04.260 --> 00:36:05.190
AMIT SOOD: Sure.

00:36:05.190 --> 00:36:08.530
I think most of it is
publicly available.

00:36:08.530 --> 00:36:10.710
But I think the
short version is it

00:36:10.710 --> 00:36:14.670
was a grounds up 20% initiative
by a couple of Googlers

00:36:14.670 --> 00:36:18.570
back in 2010, 2011,
to essentially do

00:36:18.570 --> 00:36:19.920
one very simple thing.

00:36:19.920 --> 00:36:24.090
To make access to art and
culture truly accessible

00:36:24.090 --> 00:36:26.190
online without any
barriers to entry

00:36:26.190 --> 00:36:29.940
by working with museums,
artists, and institutions.

00:36:29.940 --> 00:36:32.910
And since then, the
initiative is now formalized.

00:36:32.910 --> 00:36:35.610
It's a non-profit
within the company,

00:36:35.610 --> 00:36:38.070
and essentially it focuses
on telling stories,

00:36:38.070 --> 00:36:40.620
on getting cultural
content onto the internet,

00:36:40.620 --> 00:36:42.750
and essentially collaborating
with amazing artists

00:36:42.750 --> 00:36:43.783
and great engineers.

00:36:43.783 --> 00:36:46.200
TIMOTHY JORDAN: One of the
first things that I heard about

00:36:46.200 --> 00:36:50.970
was the Museum of Museums, I
think it was called one point.

00:36:50.970 --> 00:36:54.120
Being able to go online
and see these works of art

00:36:54.120 --> 00:36:55.970
in such high detail.

00:36:55.970 --> 00:36:57.720
Like you could even
see the brush strokes.

00:36:57.720 --> 00:36:59.980
And that was an
incredible start.

00:36:59.980 --> 00:37:01.500
What are some of
the other things?

00:37:01.500 --> 00:37:03.292
AMIT SOOD: Well, I
think we are very lucky.

00:37:03.292 --> 00:37:05.940
When you start a project like
this and you work at Google,

00:37:05.940 --> 00:37:07.440
you get access to teams.

00:37:07.440 --> 00:37:10.830
Like for example, Blaise's
team, or for example Geo Team,

00:37:10.830 --> 00:37:12.660
that goes and builds
cameras for you

00:37:12.660 --> 00:37:14.130
that allows you to
essentially take

00:37:14.130 --> 00:37:18.540
Street View inside a museum or
to take high resolution giga

00:37:18.540 --> 00:37:19.780
pixel images.

00:37:19.780 --> 00:37:21.540
So these were the kind
of starting points

00:37:21.540 --> 00:37:22.380
for the experience.

00:37:22.380 --> 00:37:26.160
And now we have moved into
things like art selfies,

00:37:26.160 --> 00:37:29.520
so making sure that
people have a deeper

00:37:29.520 --> 00:37:31.020
appreciation for
portraits that are

00:37:31.020 --> 00:37:32.640
lying in museums
around the world

00:37:32.640 --> 00:37:35.640
by matching them to a selfie.

00:37:35.640 --> 00:37:38.250
And this was one of our
features that crossed

00:37:38.250 --> 00:37:40.260
over 100 million
selfies have been

00:37:40.260 --> 00:37:41.910
taken, where people
are now finding

00:37:41.910 --> 00:37:43.560
a new way to interact with art.

00:37:43.560 --> 00:37:45.185
MEGHAN MEHTA: And
that's the one that's

00:37:45.185 --> 00:37:48.073
using style transfer, an
aspect of machine learning.

00:37:48.073 --> 00:37:50.490
AMIT SOOD: It is using an
aspect of machine learning, yes.

00:37:50.490 --> 00:37:55.470
And it's done on device, and
it's really one of those things

00:37:55.470 --> 00:37:57.240
that we didn't
expect to take off

00:37:57.240 --> 00:37:59.700
in the way it did, and
neither did the museums.

00:37:59.700 --> 00:38:03.060
Because they've been just
preserving these artworks

00:38:03.060 --> 00:38:04.560
without understanding
that there can

00:38:04.560 --> 00:38:06.480
be a new relationship
with the audience

00:38:06.480 --> 00:38:08.902
that's not one of education,
but one of interaction.

00:38:08.902 --> 00:38:10.360
TIMOTHY JORDAN:
That's really cool.

00:38:10.360 --> 00:38:13.170
All right, let's talk about
art and machine intelligence

00:38:13.170 --> 00:38:15.330
in a little bit more depth.

00:38:15.330 --> 00:38:18.930
I think you've called
it an emerging practice.

00:38:18.930 --> 00:38:21.540
What are we seeing, and where?

00:38:21.540 --> 00:38:22.920
Who's doing what?

00:38:22.920 --> 00:38:24.420
BLAISE AGUERA Y
ARCAS: Well, there's

00:38:24.420 --> 00:38:26.250
actually sort of a
natural flow from what

00:38:26.250 --> 00:38:30.000
Google Arts and Culture
started to do many years ago.

00:38:30.000 --> 00:38:32.700
And where we began with the
artist and machine intelligence

00:38:32.700 --> 00:38:34.740
program in 2015.

00:38:34.740 --> 00:38:38.070
The first thing that became
possible with deep learning

00:38:38.070 --> 00:38:42.150
was the analysis of these
artworks and of visual media

00:38:42.150 --> 00:38:43.830
in general, in which
you start to be

00:38:43.830 --> 00:38:45.720
able to extract the
semantics of those things

00:38:45.720 --> 00:38:49.380
and go beyond exploration,
beyond Street View type things

00:38:49.380 --> 00:38:53.410
and into search and
semantic analysis.

00:38:53.410 --> 00:38:58.440
But then, what really sort
of flipped a bit in our minds

00:38:58.440 --> 00:39:01.670
was Alex Mordvintsev's
Deep Dream work.

00:39:01.670 --> 00:39:04.470
You remember the trippy
squirrel and all that stuff.

00:39:04.470 --> 00:39:07.050
MEGHAN MEHTA: It's creepy and
beautiful all at the same time.

00:39:07.050 --> 00:39:08.850
BLAISE AGUERA Y ARCAS:
Creepy and wonderful

00:39:08.850 --> 00:39:12.400
and leaked just
shortly before we--

00:39:12.400 --> 00:39:15.890
it somewhat accelerated
our need to get out there

00:39:15.890 --> 00:39:17.670
with what this actually was.

00:39:17.670 --> 00:39:21.210
But that was the first
time that neural nets

00:39:21.210 --> 00:39:23.860
began to sort of invert.

00:39:23.860 --> 00:39:27.180
And there's been a
long running thread

00:39:27.180 --> 00:39:29.850
in machine learning of
doing synthesis and not

00:39:29.850 --> 00:39:30.730
just analysis.

00:39:30.730 --> 00:39:33.307
But in the era of
classical machine learning,

00:39:33.307 --> 00:39:34.890
that wasn't generally
very successful,

00:39:34.890 --> 00:39:37.650
because the actual
machine learning

00:39:37.650 --> 00:39:39.780
part of machine learning
prior to deep learning

00:39:39.780 --> 00:39:41.550
was very low dimensional.

00:39:41.550 --> 00:39:46.320
So everything relied
on feature engineering.

00:39:46.320 --> 00:39:49.140
You write lots of code to
reduce whatever media you're

00:39:49.140 --> 00:39:51.540
talking about to a
handful of dimensions,

00:39:51.540 --> 00:39:53.610
and then you end up
doing regression.

00:39:53.610 --> 00:39:58.050
And so inverting through
feature engineering is hard.

00:39:58.050 --> 00:40:00.643
You can do maybe generated
faces in a video game

00:40:00.643 --> 00:40:02.310
with different
proportions or something,

00:40:02.310 --> 00:40:04.470
but you're not going
to generate things

00:40:04.470 --> 00:40:08.550
that are all that realistic
or artistically interesting.

00:40:08.550 --> 00:40:11.070
But with Deep Dream,
the realization

00:40:11.070 --> 00:40:14.460
was that since neural
nets work with very, very

00:40:14.460 --> 00:40:18.660
high dimensional inputs directly
and don't rely on feature

00:40:18.660 --> 00:40:22.290
engineering, you can turn that
engine around the other way,

00:40:22.290 --> 00:40:24.420
and suddenly you
have the machinery

00:40:24.420 --> 00:40:27.660
for not only understanding
but also synthesizing media.

00:40:27.660 --> 00:40:30.280
And that raises all of these
very profound questions.

00:40:30.280 --> 00:40:32.250
So it's not just
new art practices

00:40:32.250 --> 00:40:34.780
that emerge from that, but
new philosophical questions

00:40:34.780 --> 00:40:37.300
about the relationship
of creativity

00:40:37.300 --> 00:40:40.690
and human exceptionalism
to computing.

00:40:40.690 --> 00:40:44.110
And so that's where
those additional layers

00:40:44.110 --> 00:40:45.690
of cultural
engagement came from.

00:40:45.690 --> 00:40:48.220
TIMOTHY JORDAN: Or even the
relatively simple question of

00:40:48.220 --> 00:40:51.460
are we seeing the output
of Deep Dream, is that

00:40:51.460 --> 00:40:54.683
the mind of that neural network?

00:40:54.683 --> 00:40:56.100
BLAISE AGUERA Y
ARCAS: Yeah, yeah.

00:40:56.100 --> 00:40:58.270
And in fact, in the
beginning, the reason

00:40:58.270 --> 00:41:00.370
that Alex did that
experiment was

00:41:00.370 --> 00:41:03.280
to do a kind of neuroscience
on those networks.

00:41:03.280 --> 00:41:05.860
Originally it was
visualization of what

00:41:05.860 --> 00:41:08.320
those neurons high up in
that neural net were actually

00:41:08.320 --> 00:41:09.270
sensitive to.

00:41:09.270 --> 00:41:10.835
TIMOTHY JORDAN: That's crazy.

00:41:10.835 --> 00:41:14.320
All right, so at I/O, you all
have announced the Artists

00:41:14.320 --> 00:41:17.710
and Machine Intelligence
Grants, so what's the goal

00:41:17.710 --> 00:41:19.673
and who's eligible?

00:41:19.673 --> 00:41:21.340
AMIT SOOD: I can give
a very quick thing

00:41:21.340 --> 00:41:23.500
and then let Blaise
explain more.

00:41:23.500 --> 00:41:27.940
But I think for us, partnering
with Blaise's team is natural,

00:41:27.940 --> 00:41:31.240
because they're essentially
at the forefront of what's

00:41:31.240 --> 00:41:33.190
happening in machine
learning, and we

00:41:33.190 --> 00:41:35.200
are trying to get
museums and artists

00:41:35.200 --> 00:41:39.730
to adopt those technologies by
creating beautiful prototypes

00:41:39.730 --> 00:41:40.540
or experiments.

00:41:40.540 --> 00:41:45.010
So we've launched a program
called Google Arts and Culture

00:41:45.010 --> 00:41:47.650
Experiments, and over there,
you get all different types

00:41:47.650 --> 00:41:49.990
of artists, studios,
and individuals

00:41:49.990 --> 00:41:51.895
who are partnering either
with an institution

00:41:51.895 --> 00:41:54.827
or with a technologist to
create a new experiment.

00:41:54.827 --> 00:41:57.160
And we were trying to figure
out how can we further that

00:41:57.160 --> 00:41:59.950
and how can we maybe
structure that in a way that

00:41:59.950 --> 00:42:01.210
is a bit more appealing?

00:42:01.210 --> 00:42:02.680
And that's where
Blaise's team came

00:42:02.680 --> 00:42:04.767
in with the idea of the grants.

00:42:04.767 --> 00:42:06.850
BLAISE AGUERA Y ARCAS: And
Google Arts and Culture

00:42:06.850 --> 00:42:08.808
has always been interested
in not only engaging

00:42:08.808 --> 00:42:10.790
with institutions
but moving upstream

00:42:10.790 --> 00:42:13.235
with artists, of course.

00:42:13.235 --> 00:42:16.030
So it was a very obvious
synergy from the beginning.

00:42:16.030 --> 00:42:21.550
But beyond just tool
making or building

00:42:21.550 --> 00:42:24.715
bridges between artists
and technologists,

00:42:24.715 --> 00:42:26.440
a number of those
grants are actually

00:42:26.440 --> 00:42:31.060
being given to philosophers
and theorists, people

00:42:31.060 --> 00:42:35.260
who cross over, who are
very multidisciplinary,

00:42:35.260 --> 00:42:37.330
not only between
arts and technology

00:42:37.330 --> 00:42:43.930
but also thinking, critical
theory, media analysis,

00:42:43.930 --> 00:42:46.150
philosophy, aesthetics.

00:42:46.150 --> 00:42:49.300
Because it's again,
it's not just

00:42:49.300 --> 00:42:53.480
a sort of trick for
synthesizing trippy images.

00:42:53.480 --> 00:42:59.507
It also is a tool, and a--

00:42:59.507 --> 00:43:01.840
AMIT SOOD: It's a deeper
question about where this goes.

00:43:02.017 --> 00:43:02.800
BLAISE AGUERA Y
ARCAS: Yeah, it's

00:43:02.800 --> 00:43:04.870
a tool for thinking
about very deep questions

00:43:04.870 --> 00:43:07.570
about what humans are all
about, what creativity

00:43:07.570 --> 00:43:10.990
is all about, what is the future
relationship between humans

00:43:10.990 --> 00:43:13.145
and machines and
society and so on.

00:43:13.145 --> 00:43:14.770
AMIT SOOD: So we have
very meta topics,

00:43:14.770 --> 00:43:16.300
and then we need
to find ways that

00:43:16.300 --> 00:43:20.830
can be easily explainable or
communicated to the audience.

00:43:20.830 --> 00:43:23.470
And that's where I think the
synergy of this program is--

00:43:23.470 --> 00:43:24.778
for us, it's our first attempt.

00:43:24.778 --> 00:43:26.320
It's the first time
we have announced

00:43:26.320 --> 00:43:27.580
this program together.

00:43:27.580 --> 00:43:30.640
So we are hoping the
output will be interesting,

00:43:30.640 --> 00:43:32.830
and then hopefully we can
scale it and expand it.

00:43:32.830 --> 00:43:33.700
TIMOTHY JORDAN: And
it's so important

00:43:33.700 --> 00:43:35.658
to involve artists in
those kinds of questions.

00:43:35.658 --> 00:43:36.927
AMIT SOOD: Exactly, exactly.

00:43:36.927 --> 00:43:38.510
TIMOTHY JORDAN: So
let's fast forward.

00:43:38.510 --> 00:43:40.570
I understand the program
runs from September

00:43:40.570 --> 00:43:44.530
to January of 2020.

00:43:44.530 --> 00:43:48.490
In February of 2020, how
does the world look different

00:43:48.490 --> 00:43:51.580
if this goes perfectly well?

00:43:51.580 --> 00:43:53.650
BLAISE AGUERA Y ARCAS:
Well, obviously there

00:43:53.650 --> 00:43:58.120
are works that we hope will come
out of this whole process that

00:43:58.120 --> 00:44:03.175
will be beautiful or frightening
or engage discussion, or--

00:44:03.175 --> 00:44:04.800
TIMOTHY JORDAN: Or
all of those things.

00:44:04.800 --> 00:44:06.383
BLAISE AGUERA Y
ARCAS: In various ways

00:44:06.383 --> 00:44:08.100
all of the above, perhaps.

00:44:08.100 --> 00:44:10.150
So certainly, there
are artistic outputs

00:44:10.150 --> 00:44:12.790
that we expect will happen.

00:44:12.790 --> 00:44:15.940
But a big part of this is
also about advancing dialogue,

00:44:15.940 --> 00:44:20.620
and it's not a marketing
exercise for Google.

00:44:20.620 --> 00:44:25.450
We have felt from the beginning
that the two-way dialogue,

00:44:25.450 --> 00:44:27.610
the influence of a
lot of those thinkers

00:44:27.610 --> 00:44:30.070
on what we're doing
in our other work

00:44:30.070 --> 00:44:33.220
is at least as important
from our point of view

00:44:33.220 --> 00:44:36.820
as just making tools
available to the artists.

00:44:36.820 --> 00:44:38.960
TIMOTHY JORDAN: Thank
you for being here,

00:44:38.960 --> 00:44:41.900
and if they want to
check this out online,

00:44:41.900 --> 00:44:42.970
there's a URL right?

00:44:42.970 --> 00:44:44.595
AMIT SOOD: Yeah,
they've just got to go

00:44:44.595 --> 00:44:46.300
to go.co/ArtsExperiments.

00:44:46.300 --> 00:44:48.130
And they can apply for
the grants program,

00:44:48.130 --> 00:44:50.080
they can learn more
about RMI's work,

00:44:50.080 --> 00:44:52.690
and it's a fascinating
place to be.

00:44:52.690 --> 00:44:53.980
TIMOTHY JORDAN: Gentlemen,
thank you so much for joining me

00:44:53.980 --> 00:44:54.730
on the show today.

00:44:54.730 --> 00:44:55.605
AMIT SOOD: Thank you.

00:44:55.605 --> 00:44:57.855
TIMOTHY JORDAN: We're going
to take a quick break now,

00:44:57.855 --> 00:44:59.880
and then we'll be back
with more I/O Live.

00:45:20.220 --> 00:45:22.470
EMILY FORTUNA: It has been
a jam packed I/O this year,

00:45:22.470 --> 00:45:23.850
and I'm sorry it's already over.

00:45:23.850 --> 00:45:24.210
TIMOTHY JORDAN: Right?

00:45:24.210 --> 00:45:25.570
I mean, we've covered so much.

00:45:25.570 --> 00:45:27.130
It's been an amazing three days.

00:45:27.130 --> 00:45:28.800
EMILY FORTUNA: Thank you all for
tuning in and experiencing it

00:45:28.800 --> 00:45:29.603
with us.

00:45:29.603 --> 00:45:31.020
Though we're
wrapping up this I/O,

00:45:31.020 --> 00:45:33.183
keep the conversation
going with hashtag I/O 19.

00:45:33.183 --> 00:45:35.100
TIMOTHY JORDAN: And to
watch all the keynotes,

00:45:35.100 --> 00:45:37.740
all of the sessions, and
all the I/O Live content,

00:45:37.740 --> 00:45:41.910
visit g.co/io/live, or
head directly to the Google

00:45:41.910 --> 00:45:43.390
Developers YouTube channel.

00:45:43.390 --> 00:45:44.580
EMILY FORTUNA: And to
stay connected with Google

00:45:44.580 --> 00:45:46.320
Developers, sign up
for our newsletter,

00:45:46.320 --> 00:45:48.510
at g.co/dev/newsletter.

00:45:48.510 --> 00:45:50.730
TIMOTHY JORDAN: So
long from I/O 2019.

00:45:50.730 --> 00:45:55.227
EMILY FORTUNA: See you in 2020.

00:45:55.227 --> 00:45:57.800
SYNTHESIZED VOICE: Good
morning, Shore Line.

00:45:57.800 --> 00:45:59.980
Welcome to Google I/O.

00:45:59.980 --> 00:46:03.060
SUNDAR PICHAI: Thank you
all for joining us in person

00:46:03.060 --> 00:46:06.062
and to the millions around the
world watching on Livestream.

00:46:06.062 --> 00:46:07.520
TODD KERPELMAN:
Right, experiments.

00:46:07.520 --> 00:46:09.380
Let's go check it out.

00:46:09.380 --> 00:46:14.800
SPEAKER 16: Today, there are
over 2.5 billion active Android

00:46:14.800 --> 00:46:15.908
devices.

00:46:15.908 --> 00:46:18.325
SPEAKER 17: The vibe's great,
the people here are amazing.

00:46:18.325 --> 00:46:20.810
SPEAKER 18: The sessions,
the food, the people--

00:46:20.810 --> 00:46:21.920
everything is perfect.

00:46:21.920 --> 00:46:23.270
SPEAKER 19: 10 years.

00:46:23.270 --> 00:46:26.210
10 years of our Google
developer community.

00:46:26.210 --> 00:46:28.040
Each of you do so much to help.

00:46:28.040 --> 00:46:32.130
We appreciate everything
you do for us.

00:46:32.130 --> 00:46:34.968
SPEAKER 20: Here is our friend.

00:46:34.968 --> 00:46:36.487
SPEAKER 21: It's an AR shark.

00:46:36.487 --> 00:46:37.070
It won't bite.

00:46:40.907 --> 00:46:42.240
SPEAKER 22: Oh, that's so nifty.

00:46:42.240 --> 00:46:43.860
SPEAKER 23: It's
a perfect example

00:46:43.860 --> 00:46:46.170
of what we mean
by building a more

00:46:46.170 --> 00:46:48.180
helpful Google for everyone.

00:46:48.180 --> 00:46:53.930
[MUSIC PLAYING]

