WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.305
[MUSIC PLAYING]

00:00:15.600 --> 00:00:16.329
RYAN BOYD: Hello everyone.

00:00:16.329 --> 00:00:17.720
My name is Ryan Boyd.

00:00:17.720 --> 00:00:19.030
MICHAEL MANOOCHEHRI: And I'm
Michael Manoochehri.

00:00:19.030 --> 00:00:21.400
RYAN BOYD: And this is the
Michael and Ryan Show.

00:00:21.400 --> 00:00:24.000
Today we're here to talk about
best practices for querying

00:00:24.000 --> 00:00:26.710
massive datasets using
Google BigQuery.

00:00:26.710 --> 00:00:29.770
If you have any questions that
come up while you're listening

00:00:29.770 --> 00:00:32.980
to this or watching this
broadcast, feel free to reach

00:00:32.980 --> 00:00:38.270
out to us on Stack Overflow in
the google-bigquery tag, or

00:00:38.270 --> 00:00:41.812
find us on the Google Cloud
Platform Developers+ page, or

00:00:41.812 --> 00:00:46.230
at our Google+ profiles and
Twitter handles here.

00:00:46.230 --> 00:00:49.666
We welcome to hear for
your feedback.

00:00:49.666 --> 00:00:51.970
All right, so, querying massive

00:00:51.970 --> 00:00:54.370
datasets with Google BigQuery.

00:00:54.370 --> 00:00:57.020
First of all, let's give you
a little bit of history.

00:00:57.020 --> 00:00:59.650
It used to be when people wanted
to query their data

00:00:59.650 --> 00:01:02.800
sets, they would load their
data into a relational

00:01:02.800 --> 00:01:05.400
database server that looked
something like this.

00:01:05.400 --> 00:01:09.950
That relational database server
would get basically all

00:01:09.950 --> 00:01:14.020
of their data, and it
would build indexes

00:01:14.020 --> 00:01:15.150
on top of that data.

00:01:15.150 --> 00:01:18.480
And those indexes made it quick
to query things that you

00:01:18.480 --> 00:01:21.130
knew in advance that you wanted
to be able to query.

00:01:21.130 --> 00:01:24.670
So if you wanted to do some
aggregate queries on that

00:01:24.670 --> 00:01:28.870
data, it might be OK too, based
off of those indexes,

00:01:28.870 --> 00:01:32.120
but if you want to do really ad
hoc queries to explore your

00:01:32.120 --> 00:01:35.670
data and understand your data
better, you'd often end up

00:01:35.670 --> 00:01:38.380
doing something called
a full table scan.

00:01:38.380 --> 00:01:41.190
Now, a full table scan is
basically reading every single

00:01:41.190 --> 00:01:45.760
row of your data from disk
and processing each row

00:01:45.760 --> 00:01:46.840
individually.

00:01:46.840 --> 00:01:50.520
Now, this is fine if you have
a small dataset, but as you

00:01:50.520 --> 00:01:53.490
grow your data and you have,
say, hundreds of millions of

00:01:53.490 --> 00:01:56.850
rows or billions of rows of
data, you can only scale your

00:01:56.850 --> 00:01:59.340
relational database servers
so large and

00:01:59.340 --> 00:02:01.690
in a vertical fashion.

00:02:01.690 --> 00:02:05.120
And Google really had a
challenge internally.

00:02:05.120 --> 00:02:07.360
We had a challenge that we
needed to be able to query

00:02:07.360 --> 00:02:09.930
these types of massive
datasets in

00:02:09.930 --> 00:02:11.760
an interactive style.

00:02:11.760 --> 00:02:14.710
We needed to get results
quickly, because those fast

00:02:14.710 --> 00:02:17.830
results help us iterate on our
products which are necessary

00:02:17.830 --> 00:02:21.370
for us to build our business.

00:02:21.370 --> 00:02:24.920
So Google said, well, what
if we don't use a typical

00:02:24.920 --> 00:02:26.590
relational database server.

00:02:26.590 --> 00:02:30.760
What if we don't rely upon this
concept of indexes, but

00:02:30.760 --> 00:02:34.960
instead embrace the concept of a
full table scan, embrace the

00:02:34.960 --> 00:02:37.350
idea that we are going
to query over every

00:02:37.350 --> 00:02:39.530
single row of our data.

00:02:39.530 --> 00:02:42.000
What would we need to
make that happen?

00:02:42.000 --> 00:02:45.680
Well, Google already had tens,
hundreds, thousands of servers

00:02:45.680 --> 00:02:49.900
around the world, and these
servers had thousands of disks

00:02:49.900 --> 00:02:51.100
in them as well.

00:02:51.100 --> 00:02:54.860
And disk performance is one
of the key things that's

00:02:54.860 --> 00:02:59.370
necessary in order to query
massive amounts of data.

00:02:59.370 --> 00:03:04.710
Now, you can parallelize the
queries across all of these

00:03:04.710 --> 00:03:10.000
machines using typical big data
processing techniques

00:03:10.000 --> 00:03:11.950
such as the MapReduce
algorithm,

00:03:11.950 --> 00:03:13.590
which Google invented.

00:03:13.590 --> 00:03:18.540
But the MapReduce algorithm and
other big data techniques

00:03:18.540 --> 00:03:20.590
often times are batch-based.

00:03:20.590 --> 00:03:24.510
They basically allow you to
query your data or process

00:03:24.510 --> 00:03:27.940
your data in batches, and those
batches can sometimes

00:03:27.940 --> 00:03:33.190
take minutes, or hours, or
even days to get results.

00:03:33.190 --> 00:03:36.260
Google really needed to have
that interactive query speed,

00:03:36.260 --> 00:03:39.410
so we decided to invent a
new technology, and that

00:03:39.410 --> 00:03:42.410
technology internally
is known as Dremel.

00:03:42.410 --> 00:03:46.510
And externally, we've released
this to you to use as Google

00:03:46.510 --> 00:03:47.830
BigQuery, as a service.

00:03:47.830 --> 00:03:52.070
We built an API and some
developer usability features

00:03:52.070 --> 00:03:56.010
on top of the internal
service.

00:03:56.010 --> 00:03:58.210
So here's what BigQuery
looks like.

00:03:58.210 --> 00:04:00.740
We wanted to give you an idea
of what BigQuery looks like,

00:04:00.740 --> 00:04:03.520
what the architecture is, so you
can better understand when

00:04:03.520 --> 00:04:06.320
you're running your queries
what's happening.

00:04:06.320 --> 00:04:11.650
At the bottom of this diagram
is distributed storage.

00:04:11.650 --> 00:04:14.740
So your data sits on this
distributed storage and is

00:04:14.740 --> 00:04:18.320
read off of the disks
by leaf nodes.

00:04:18.320 --> 00:04:21.880
And each leaf node only reads
a small chunk of your data.

00:04:21.880 --> 00:04:25.570
Each of these leaf nodes would
be a CPU or a machine, and

00:04:25.570 --> 00:04:28.830
they're only responsible for
reading that small chunk.

00:04:28.830 --> 00:04:30.900
And each of them processes
their small chunk

00:04:30.900 --> 00:04:32.020
individually.

00:04:32.020 --> 00:04:36.410
And as the data flows up the
tree through the mixers, the

00:04:36.410 --> 00:04:37.610
data is aggregated.

00:04:37.610 --> 00:04:40.480
So if you think about something
like account.

00:04:40.480 --> 00:04:44.080
The two leaves on the left hand
side would each count the

00:04:44.080 --> 00:04:47.510
number of records that they're
responsible for, in the data

00:04:47.510 --> 00:04:50.700
set that they're responsible
for, and then the mixer, as it

00:04:50.700 --> 00:04:53.480
goes up the tree, would actually
sum those two counts.

00:04:53.480 --> 00:04:56.400
It's a very simple operation.

00:04:56.400 --> 00:04:59.470
Now, what's important here is
that we're reading from the

00:04:59.470 --> 00:05:02.660
distributed storage only at
the bottom of the tree.

00:05:02.660 --> 00:05:05.580
We're not reading and writing
from the distributed storage

00:05:05.580 --> 00:05:08.440
throughout the process, as
is typically done in a

00:05:08.440 --> 00:05:10.460
MapReduce-based algorithm.

00:05:10.460 --> 00:05:12.380
What's also important
here is this is a

00:05:12.380 --> 00:05:14.110
long-lived serving tree.

00:05:14.110 --> 00:05:18.230
This serving tree exists for
all of your queries as a

00:05:18.230 --> 00:05:20.170
customer, and also
exists for the

00:05:20.170 --> 00:05:21.460
queries of other customers.

00:05:21.460 --> 00:05:23.510
So it sits around ready
to process your

00:05:23.510 --> 00:05:26.530
queries very quickly.

00:05:26.530 --> 00:05:30.340
The other thing that I want to
point out is we use a thing

00:05:30.340 --> 00:05:31.970
called columnar storage.

00:05:31.970 --> 00:05:35.070
Basically, we're storing
the data in columns

00:05:35.070 --> 00:05:36.710
rather than in rows.

00:05:36.710 --> 00:05:40.280
So as we read the data off of
disk, oftentimes you will only

00:05:40.280 --> 00:05:43.390
need to read very particular
columns.

00:05:43.390 --> 00:05:46.100
Usually when you're running a
query-- think of a SQL-like

00:05:46.100 --> 00:05:48.600
syntax, which is what we
use for BigQuery--

00:05:48.600 --> 00:05:51.465
you're only selecting particular
columns and using

00:05:51.465 --> 00:05:53.360
your WHERE clause on
particular columns.

00:05:53.360 --> 00:05:55.220
So we only need to read
those particular

00:05:55.220 --> 00:05:56.670
columns off of disks.

00:05:56.670 --> 00:05:59.180
This saves us a lot of our
operations, and we'll see how

00:05:59.180 --> 00:06:01.250
it does that here shortly.

00:06:01.250 --> 00:06:03.740
All right, so hopefully you
understand a little bit about

00:06:03.740 --> 00:06:07.230
how BigQuery works in the
underlying architecture.

00:06:07.230 --> 00:06:11.490
Now let's dive into some of
the types of queries that

00:06:11.490 --> 00:06:13.600
BigQuery can process.

00:06:13.600 --> 00:06:16.550
And the first one I want to
talk about here is regular

00:06:16.550 --> 00:06:17.750
expressions.

00:06:17.750 --> 00:06:21.600
So regular expressions is
something that all good geeks

00:06:21.600 --> 00:06:25.780
know and love, and have worked
with back since I started

00:06:25.780 --> 00:06:29.240
programming on Perl when I first
started development, and

00:06:29.240 --> 00:06:30.490
regular expressions
are obviously very

00:06:30.490 --> 00:06:32.350
well known in Perl.

00:06:32.350 --> 00:06:35.100
But regular expressions
are powerful ways of

00:06:35.100 --> 00:06:36.840
working with text.

00:06:36.840 --> 00:06:39.300
Now, the thing with regular
expressions, though, is you're

00:06:39.300 --> 00:06:41.260
changing them often.

00:06:41.260 --> 00:06:44.320
When you're trying to do these
exploration of your data,

00:06:44.320 --> 00:06:46.750
you're changing those regular
expressions, so you can't

00:06:46.750 --> 00:06:50.470
build typical indexes on
it like would work in a

00:06:50.470 --> 00:06:51.860
relational database.

00:06:51.860 --> 00:06:54.210
So most relational databases,
if you're running regular

00:06:54.210 --> 00:06:58.110
expressions, you're going to
be doing those and doing a

00:06:58.110 --> 00:07:01.040
full table scan when you write
those regular expressions or

00:07:01.040 --> 00:07:02.710
if you have to modify them.

00:07:02.710 --> 00:07:05.190
So this is a very powerful
feature that I

00:07:05.190 --> 00:07:06.490
want to show you.

00:07:06.490 --> 00:07:10.310
BigQuery supports regular
expressions using matches,

00:07:10.310 --> 00:07:13.480
replacements, or extractions,
supports all these new

00:07:13.480 --> 00:07:15.630
different types of regular
expressions.

00:07:15.630 --> 00:07:18.630
Let's show you where that power
really lies when you're

00:07:18.630 --> 00:07:22.830
trying to query not just, say,
tens of millions or tens of

00:07:22.830 --> 00:07:27.150
thousands of rows of data, but
if you're doing billions of

00:07:27.150 --> 00:07:29.760
rows of data.

00:07:29.760 --> 00:07:32.880
So I'm going to start running
this query over here, and this

00:07:32.880 --> 00:07:40.420
is doing an aggregate analysis
on 15.7 billion rows of data.

00:07:40.420 --> 00:07:44.140
So this is 15.7 billion rows,
and we're actually going to do

00:07:44.140 --> 00:07:48.850
a regular expression on every
single one of these rows.

00:07:48.850 --> 00:07:50.230
So we can see a regex match.

00:07:50.230 --> 00:07:52.510
This dataset basically
represents

00:07:52.510 --> 00:07:54.220
something like a log file.

00:07:54.220 --> 00:07:58.610
In this case, it's a log file
of page views on Wikipedia,

00:07:58.610 --> 00:08:03.410
but imagine in your case it
might be a log file of

00:08:03.410 --> 00:08:07.510
activity on your website or
clicks, or views on your ads

00:08:07.510 --> 00:08:08.690
that you place.

00:08:08.690 --> 00:08:13.330
And so this log file has titles
in it, and those titles

00:08:13.330 --> 00:08:16.210
represent the page
that was viewed.

00:08:16.210 --> 00:08:19.160
In this case, we want to do a
regular expression to find all

00:08:19.160 --> 00:08:22.250
of those pages that start with
a capital G and with a

00:08:22.250 --> 00:08:24.670
lowercase e, and have
two o's in between.

00:08:24.670 --> 00:08:26.830
I wonder what that spells out.

00:08:26.830 --> 00:08:30.620
So we can see that we ran
this regular expression.

00:08:30.620 --> 00:08:32.540
There's 13 tables here.

00:08:32.540 --> 00:08:37.490
It represents one terabyte of
underlying data, and we ran

00:08:37.490 --> 00:08:40.520
this regular expression
in 34 seconds.

00:08:40.520 --> 00:08:46.620
So that was 15.7 billion regular
expressions run in 34

00:08:46.620 --> 00:08:50.290
seconds across a terabyte
of data in the tables.

00:08:50.290 --> 00:08:52.660
And because of the columnar data
store, we actually only

00:08:52.660 --> 00:08:56.290
had to process about 665
gigabytes of data, but it's

00:08:56.290 --> 00:08:59.560
still a massive amount of data
to find the result that

00:08:59.560 --> 00:09:05.920
Google, over the year 2010 and
the first month of 2011, was

00:09:05.920 --> 00:09:10.520
the most popular viewed
Wikipedia page with, I think

00:09:10.520 --> 00:09:15.200
that's 1.7 million views if
I'm reading that right.

00:09:15.200 --> 00:09:17.500
And we see some other
pages here as well.

00:09:17.500 --> 00:09:20.030
But the important thing to
emphasize here is we're

00:09:20.030 --> 00:09:23.270
running that 15.7
billion regular

00:09:23.270 --> 00:09:26.320
expressions in 34 seconds.

00:09:26.320 --> 00:09:28.980
Imagine trying to do that in
your typical relational

00:09:28.980 --> 00:09:32.150
database server, or
even if using the

00:09:32.150 --> 00:09:33.520
MapReduce-type algorithm.

00:09:33.520 --> 00:09:36.740
It would require a lot of
processing power, and may not

00:09:36.740 --> 00:09:39.960
even be possible then.

00:09:39.960 --> 00:09:40.410
All right.

00:09:40.410 --> 00:09:43.170
So that gives you an idea
of the types of regular

00:09:43.170 --> 00:09:46.230
expressions and the power of
BigQuery to run queries on

00:09:46.230 --> 00:09:47.780
massive datasets.

00:09:47.780 --> 00:09:50.390
Now I'm going to throw it over
to Michael here, and Michael's

00:09:50.390 --> 00:09:52.650
going to talk about the next
section here, which is

00:09:52.650 --> 00:09:53.670
statistical queries.

00:09:53.670 --> 00:09:55.230
MICHAEL MANOOCHEHRI: Yeah.,
so statistical queries are

00:09:55.230 --> 00:09:58.120
another example of the type
of query that challenges

00:09:58.120 --> 00:10:01.240
relational databases when the
data sizes are large.

00:10:01.240 --> 00:10:03.250
And also, if you're using
something like a document

00:10:03.250 --> 00:10:06.390
store or NoSQL database or any
kind of actual database where

00:10:06.390 --> 00:10:09.880
you have a distributed system,
generally to get statistical

00:10:09.880 --> 00:10:10.780
aggregate queries--

00:10:10.780 --> 00:10:14.480
and I'm talking about things
like counts, averages, even

00:10:14.480 --> 00:10:16.890
standard deviations and things
like quantiles--

00:10:16.890 --> 00:10:18.420
generally you'll have
to write a MapReduce

00:10:18.420 --> 00:10:20.060
function of your own.

00:10:20.060 --> 00:10:22.000
What's great about BigQuery is
that you can express these

00:10:22.000 --> 00:10:25.340
types of queries using SQL
or SQL-like language that

00:10:25.340 --> 00:10:26.490
BigQuery supports.

00:10:26.490 --> 00:10:28.430
So let's take a look
at some examples.

00:10:28.430 --> 00:10:30.620
We're going to take a look at
a public dataset we have in

00:10:30.620 --> 00:10:33.310
BigQuery that you have access
to when you sign up.

00:10:33.310 --> 00:10:34.980
And that's the natality
dataset.

00:10:34.980 --> 00:10:36.890
So this is a really interesting
data set.

00:10:36.890 --> 00:10:41.400
It's a record of births in
the United States, and a

00:10:41.400 --> 00:10:44.520
collection of data with mother's
age, father's age,

00:10:44.520 --> 00:10:47.550
the baby weight, and other
statistics about the birth.

00:10:47.550 --> 00:10:50.572
And I think we have several
decades of this data.

00:10:50.572 --> 00:10:52.250
I think millions and
millions of rows of

00:10:52.250 --> 00:10:53.940
data actually, obviously.

00:10:53.940 --> 00:10:55.830
So let's take a look at some
queries using some of the

00:10:55.830 --> 00:10:58.260
statistical functions that
BigQuery supports.

00:10:58.260 --> 00:11:02.800
It can give you a sense of the
types of queries you can do.

00:11:02.800 --> 00:11:05.380
So we're going to come back
to our BigQuery UI.

00:11:05.380 --> 00:11:09.140
Now, this is a query that we
were interested in looking at,

00:11:09.140 --> 00:11:12.690
if there's some kind of variance
between the baby

00:11:12.690 --> 00:11:15.280
weights of women who
smoke and those who

00:11:15.280 --> 00:11:17.190
don't during gestation.

00:11:17.190 --> 00:11:19.910
And so here we're going to run
a query to find the average

00:11:19.910 --> 00:11:21.630
baby weight and the average
mother's age.

00:11:21.630 --> 00:11:22.700
And we just picked
a random state.

00:11:22.700 --> 00:11:25.470
We're just going to pick Ohio,
and we're going to group by if

00:11:25.470 --> 00:11:27.840
the mothers smoked cigarettes
or not.

00:11:27.840 --> 00:11:29.060
So let's run this query.

00:11:29.060 --> 00:11:32.200
This is going to give you a
sense of the aggregates over

00:11:32.200 --> 00:11:33.340
these large data sets.

00:11:33.340 --> 00:11:35.250
And that was a very
quick result.

00:11:35.250 --> 00:11:38.980
And as you can see, there is
a difference in baby weight

00:11:38.980 --> 00:11:42.130
between the fact that if
a mother smoked or not.

00:11:42.130 --> 00:11:44.580
And the joke I always talk to
Ryan about is is this of

00:11:44.580 --> 00:11:45.870
statistical significance.

00:11:45.870 --> 00:11:47.900
So if you had a statistical
package like R, you could

00:11:47.900 --> 00:11:50.120
actually do some kind
of correlation.

00:11:50.120 --> 00:11:52.810
Here I also ran a standard
deviation to give you a sense

00:11:52.810 --> 00:11:54.190
of that as well.

00:11:54.190 --> 00:11:57.360
And so this was running over
gigabytes of data, and if you

00:11:57.360 --> 00:11:58.770
were using a relational
database, this

00:11:58.770 --> 00:12:00.300
would take some time.

00:12:00.300 --> 00:12:04.090
So let's take a look at another
statistical query.

00:12:04.090 --> 00:12:06.950
In this case, we're going to do
the count of baby births by

00:12:06.950 --> 00:12:09.310
gender for large states.

00:12:09.310 --> 00:12:12.490
So I've actually picked
a number that's, I

00:12:12.490 --> 00:12:13.650
think, three million.

00:12:13.650 --> 00:12:16.240
So we're going to look at states
that have had over

00:12:16.240 --> 00:12:16.910
three million births.

00:12:16.910 --> 00:12:18.830
I think there's only going to
be California and Texas in

00:12:18.830 --> 00:12:19.920
this example.

00:12:19.920 --> 00:12:23.000
But here I'm highlighting
another interesting function,

00:12:23.000 --> 00:12:23.970
and that's HAVING.

00:12:23.970 --> 00:12:27.220
HAVING is like using WHERE in a
normal SQL clause, but here

00:12:27.220 --> 00:12:29.020
we're looking at the
aggregate result.

00:12:29.020 --> 00:12:32.590
So here I'm going to say, give
me all of the count of baby

00:12:32.590 --> 00:12:36.510
births by gender having a count
over three million.

00:12:36.510 --> 00:12:37.370
Let's run that.

00:12:37.370 --> 00:12:40.110
And here we're going to
show gender disparity

00:12:40.110 --> 00:12:42.680
between large states.

00:12:42.680 --> 00:12:44.950
And this should actually come
back pretty quickly as well.

00:12:44.950 --> 00:12:47.680
So here you can see, here's the
state of California and

00:12:47.680 --> 00:12:50.820
the state of Texas, and you
can see the difference of

00:12:50.820 --> 00:12:52.710
birth genders between
the two states.

00:12:52.710 --> 00:12:54.480
So, again, a ton of data.

00:12:54.480 --> 00:12:56.880
We're whipping through this
really fast, and it's easy to

00:12:56.880 --> 00:13:00.190
express these types of queries
in BigQuery's language.

00:13:00.190 --> 00:13:00.510
Now here's one.

00:13:00.510 --> 00:13:02.640
This is one of my favorites.

00:13:02.640 --> 00:13:04.920
I'm going to look at a standard
deviation, and here

00:13:04.920 --> 00:13:07.340
we're going to use BigQuery to
find outliers in this dataset.

00:13:07.340 --> 00:13:10.470
So I was interested to see how
large does a baby have to be

00:13:10.470 --> 00:13:13.230
to be several standard
deviations above the mean.

00:13:13.230 --> 00:13:14.830
And I just randomly
picked four.

00:13:14.830 --> 00:13:17.170
I wanted to see if there were
any babies that were four

00:13:17.170 --> 00:13:20.710
standard deviations
above the mean.

00:13:20.710 --> 00:13:24.330
So you can run that across the
entire dataset of US births,

00:13:24.330 --> 00:13:26.230
and very quickly the
result comes back.

00:13:26.230 --> 00:13:30.280
That's a baby that's about 12
pounds, almost 13 pounds, is a

00:13:30.280 --> 00:13:32.580
baby that's four standard
deviations above the mean.

00:13:32.580 --> 00:13:34.210
That's a pretty chubby baby.

00:13:34.210 --> 00:13:36.950
So I took this number, and I
wanted to do a quick ad hoc

00:13:36.950 --> 00:13:40.730
query to see what months and
which states were the

00:13:40.730 --> 00:13:42.300
chubbiest babies born in.

00:13:42.300 --> 00:13:44.810
So actually what's great about
BigQuery is you can run these

00:13:44.810 --> 00:13:47.750
ad hoc aggregate queries.

00:13:47.750 --> 00:13:50.450
So let's take that number
and put it back

00:13:50.450 --> 00:13:51.830
into another query.

00:13:51.830 --> 00:13:54.840
And here I'm just going to
group by state, year, and

00:13:54.840 --> 00:13:58.620
month, and I'm going to see
which state for which date

00:13:58.620 --> 00:14:01.050
were the fattest babies born.

00:14:01.050 --> 00:14:03.570
Or let's say the most
giant babies.

00:14:03.570 --> 00:14:08.000
And so shockingly, Maryland in
December 1990 had quite a lot

00:14:08.000 --> 00:14:09.500
of very chubby babies.

00:14:09.500 --> 00:14:11.140
Actually I went to Google
to see what was going.

00:14:11.140 --> 00:14:13.570
I couldn't find a definitive
answer, so if anyone has any

00:14:13.570 --> 00:14:17.190
idea why Maryland in December
1990 had such chubby babies,

00:14:17.190 --> 00:14:18.140
please let me know.

00:14:18.140 --> 00:14:19.970
But these are the kind of
queries you can do that are

00:14:19.970 --> 00:14:22.750
very difficult on other systems,
and are pretty easy

00:14:22.750 --> 00:14:25.800
and quick with BigQuery.

00:14:25.800 --> 00:14:28.500
So let's take a look at some
other functions that

00:14:28.500 --> 00:14:30.540
BigQuery's really good at.

00:14:30.540 --> 00:14:32.670
Earlier, Ryan talked about
some of the log data.

00:14:32.670 --> 00:14:34.880
We were looking at Wikipedia
page view data.

00:14:34.880 --> 00:14:36.590
BigQuery's excellent
for log data.

00:14:36.590 --> 00:14:38.800
It's one of the canonical
use cases for it.

00:14:38.800 --> 00:14:42.890
So it has certain specific
functions that help you format

00:14:42.890 --> 00:14:45.670
and parse time stamps
and URLs and IPs.

00:14:45.670 --> 00:14:49.310
So you can do things like take a
UTC formatted time stamp and

00:14:49.310 --> 00:14:51.650
format it into a more human
readable string.

00:14:51.650 --> 00:14:53.590
We have functions
to parse URLs.

00:14:53.590 --> 00:14:56.100
So if you're looking at domains,
grouping things by

00:14:56.100 --> 00:14:57.430
domains, that's really easy.

00:14:57.430 --> 00:14:59.500
And you can parse and format IP
addresses and convert them

00:14:59.500 --> 00:15:01.470
from integers back into
a string format,

00:15:01.470 --> 00:15:02.380
and things like that.

00:15:02.380 --> 00:15:04.580
So let's do a few quick examples
just to show you what

00:15:04.580 --> 00:15:05.600
I'm talking about.

00:15:05.600 --> 00:15:08.340
Let's go back to our BigQuery
web UI, and we're going to

00:15:08.340 --> 00:15:11.880
take a look at a quick
domain query.

00:15:11.880 --> 00:15:14.180
So this is a really interesting
public dataset we

00:15:14.180 --> 00:15:14.970
have as well.

00:15:14.970 --> 00:15:17.670
One of our colleagues Ilya,
who's on the Chrome team, he

00:15:17.670 --> 00:15:19.860
was really interested in
GitHub public data.

00:15:19.860 --> 00:15:23.470
So he loaded up a bunch of the
data from the GitHub Public

00:15:23.470 --> 00:15:27.120
Timeline, which is great stuff
like repository homepage,

00:15:27.120 --> 00:15:28.640
commit message, really
cool stuff.

00:15:28.640 --> 00:15:30.630
And we have some examples
on our website in our

00:15:30.630 --> 00:15:33.700
documentation about ways you
can query this and do some

00:15:33.700 --> 00:15:36.400
interesting, what I call the
poor man's sentiment analysis,

00:15:36.400 --> 00:15:39.140
by studying commit string
messages and things like that.

00:15:39.140 --> 00:15:40.540
You can find the most
popular projects.

00:15:40.540 --> 00:15:42.220
So in this case, what we're
doing is we're just going to

00:15:42.220 --> 00:15:44.370
look at the project
homepage domains

00:15:44.370 --> 00:15:45.580
with the most activity.

00:15:45.580 --> 00:15:47.570
So in GitHub you can
say, this is my

00:15:47.570 --> 00:15:48.860
home page for my project.

00:15:48.860 --> 00:15:50.380
And oftentimes it is GitHub.

00:15:50.380 --> 00:15:52.030
So let's just take a
quick look at this.

00:15:54.690 --> 00:15:57.830
I'm using the domain function,
and I'm grouping by domain.

00:15:57.830 --> 00:16:00.890
So here you can see people
have with the domain

00:16:00.890 --> 00:16:03.960
github.com, which is the most
common, google.com, that might

00:16:03.960 --> 00:16:07.080
even be code.google.com,
Khan Academy Mozilla.

00:16:07.080 --> 00:16:09.380
So you can do these kind of
analyses on your logs to find

00:16:09.380 --> 00:16:11.840
out, for example, referrers,
like what is the most common

00:16:11.840 --> 00:16:13.640
referrer, et cetera.

00:16:13.640 --> 00:16:18.652
So really useful stuff when
it comes to logs and IP.

00:16:18.652 --> 00:16:21.830
And let's do a quick time
stamp query as well.

00:16:21.830 --> 00:16:23.450
We're going to take a quick
look at this one.

00:16:23.450 --> 00:16:24.780
This is an interesting one.

00:16:24.780 --> 00:16:27.200
Going back to our Wikipedia
dataset, I was interested in

00:16:27.200 --> 00:16:33.380
knowing what moment in time
the most edited pages are.

00:16:33.380 --> 00:16:34.400
So here's an example.

00:16:34.400 --> 00:16:35.900
I just randomly chose Obama.

00:16:35.900 --> 00:16:39.210
I wanted to see what moment, to
the second, were the most

00:16:39.210 --> 00:16:43.080
edits made to pages containing
the name Obama.

00:16:43.080 --> 00:16:45.740
So you can actually do these
kind of queries with BigQuery.

00:16:45.740 --> 00:16:48.380
This is going to look through
gigabytes and

00:16:48.380 --> 00:16:49.050
gigabytes of data.

00:16:49.050 --> 00:16:52.920
And I found out that on
7/29/2008 at a particular

00:16:52.920 --> 00:16:55.470
time, there were five revisions
being made.

00:16:55.470 --> 00:16:58.410
And it's really interesting to
see this kind of stuff happen.

00:16:58.410 --> 00:17:01.310
You can bucket by time when
revisions are being made, and

00:17:01.310 --> 00:17:03.780
find really interesting
visualizations and interesting

00:17:03.780 --> 00:17:06.400
graphs using this
type of data.

00:17:06.400 --> 00:17:08.130
So really interesting
stuff there.

00:17:08.130 --> 00:17:10.440
This is one of the most common
questions we get are

00:17:10.440 --> 00:17:12.300
geospatial queries.

00:17:12.300 --> 00:17:14.319
Ever since we both started the
project, almost every time we

00:17:14.319 --> 00:17:15.790
meet developers they
ask us about this.

00:17:15.790 --> 00:17:18.380
So we made a special effort
to talk a little bit about

00:17:18.380 --> 00:17:21.310
geospatial queries in our Query
Cookbook documentation.

00:17:21.310 --> 00:17:23.490
So this is a really interesting
one, and I think

00:17:23.490 --> 00:17:26.050
Ryan has been playing a lot with
geospatial queries with

00:17:26.050 --> 00:17:27.720
your car data.

00:17:27.720 --> 00:17:31.680
We did some automotive data
analysis, and we can talk

00:17:31.680 --> 00:17:32.780
about that on another GL.

00:17:32.780 --> 00:17:35.720
It's on our videos page where
we talk about sensor data.

00:17:35.720 --> 00:17:38.470
So let's do a few examples
of geospatial queries.

00:17:40.990 --> 00:17:43.940
So BigQuery has a lot of
mathematical functions besides

00:17:43.940 --> 00:17:46.770
the statistical aggregate
mathematical functions.

00:17:46.770 --> 00:17:50.550
Cosine, sine, trigonometric
functions, rounding functions,

00:17:50.550 --> 00:17:51.510
all kinds of things.

00:17:51.510 --> 00:17:54.050
So you can actually use these
functions to do things like

00:17:54.050 --> 00:17:56.200
approximate a bounding
circle using

00:17:56.200 --> 00:17:57.140
some of the trig functions.

00:17:57.140 --> 00:17:59.590
And of course you can
do a range for the

00:17:59.590 --> 00:18:00.950
bounding box as well.

00:18:00.950 --> 00:18:04.400
So let's take a look at some
of those examples.

00:18:04.400 --> 00:18:06.880
We have some here.

00:18:06.880 --> 00:18:08.920
Let's take a quick look.

00:18:08.920 --> 00:18:10.920
So let's look at a geospatial
bounding box.

00:18:10.920 --> 00:18:12.300
This is a pretty simple one.

00:18:12.300 --> 00:18:14.410
We're going to take a latitude
and longitude, and we're going

00:18:14.410 --> 00:18:18.200
to find a range in between the
boundaries of this latitude

00:18:18.200 --> 00:18:20.640
and longitude grouping.

00:18:20.640 --> 00:18:22.760
And here we're going to
look at San Francisco.

00:18:22.760 --> 00:18:25.200
We both live in San Francisco,
and we're going to look at the

00:18:25.200 --> 00:18:27.820
average temperature from
weather station data.

00:18:27.820 --> 00:18:29.490
We have an interesting
dataset in BigQuery.

00:18:29.490 --> 00:18:32.170
It's the general surface
summary data.

00:18:32.170 --> 00:18:36.400
So every day, all the weather
stations that are public

00:18:36.400 --> 00:18:38.920
collect data, and this is a
dataset set of all of that

00:18:38.920 --> 00:18:41.000
data that we have available
since, I think it's

00:18:41.000 --> 00:18:42.360
like 1929 or 1930.

00:18:42.360 --> 00:18:44.390
So millions and millions
of records.

00:18:44.390 --> 00:18:45.810
Really interesting stuff.

00:18:45.810 --> 00:18:48.170
What we did was we took this
data, and we attached a

00:18:48.170 --> 00:18:50.230
latitude and longitude
to each station.

00:18:50.230 --> 00:18:52.460
And here we're able to actually
run these geospatial

00:18:52.460 --> 00:18:55.730
queries and get aggregate
results for any station that

00:18:55.730 --> 00:18:57.180
falls within a certain
geographic range.

00:18:57.180 --> 00:18:58.170
It's really interesting.

00:18:58.170 --> 00:19:01.270
So here we're going to run a
query around San Francisco,

00:19:01.270 --> 00:19:04.480
and we're going to group by
year and month, and we're

00:19:04.480 --> 00:19:07.900
going to produce a listing of
the average temperature, and

00:19:07.900 --> 00:19:10.190
also the minimum and maximum
over a range.

00:19:10.190 --> 00:19:13.260
And here that was a very quick
query, just a few seconds over

00:19:13.260 --> 00:19:14.410
gigabytes of data.

00:19:14.410 --> 00:19:16.390
And this is another kind of
query that would be great for

00:19:16.390 --> 00:19:17.180
a visualization.

00:19:17.180 --> 00:19:19.710
You can actually bucket by year
or month, produce the

00:19:19.710 --> 00:19:22.600
average, the minimum, the max,
very interesting for that

00:19:22.600 --> 00:19:23.820
specific range.

00:19:23.820 --> 00:19:25.000
So really good query there.

00:19:25.000 --> 00:19:27.680
Now let's do a bounding
circle.

00:19:27.680 --> 00:19:29.940
Now Ryan, we spent a lot of time
trying to learn about the

00:19:29.940 --> 00:19:30.500
bounding circle.

00:19:30.500 --> 00:19:33.860
Can you tell us a little bit
about how this bounding circle

00:19:33.860 --> 00:19:34.620
query works?

00:19:34.620 --> 00:19:35.350
RYAN BOYD: Yeah.

00:19:35.350 --> 00:19:39.010
Basically, the bounding circle
query is just using basic trig

00:19:39.010 --> 00:19:42.090
functions, and assuming
that the world

00:19:42.090 --> 00:19:44.670
is perfectly spherical.

00:19:44.670 --> 00:19:47.410
The world isn't perfectly
spherical, so you won't get

00:19:47.410 --> 00:19:50.390
totally accurate results if
you're near the poles, but

00:19:50.390 --> 00:19:54.450
oftentimes, this type of
bounding circle query is

00:19:54.450 --> 00:19:58.050
actually adequate for most
places in the world.

00:19:58.050 --> 00:20:00.830
So it's a pretty interesting
query to run.

00:20:00.830 --> 00:20:03.740
You can see all the trig
functions that we're using.

00:20:03.740 --> 00:20:06.180
And why don't we kickoff
that query here.

00:20:06.180 --> 00:20:08.720
And we're finding the average
temperature for all the

00:20:08.720 --> 00:20:12.070
weather stations that are around
Denver, Colorado, in

00:20:12.070 --> 00:20:13.290
this particular query.

00:20:13.290 --> 00:20:14.200
MICHAEL MANOOCHEHRI: And
this is actually for

00:20:14.200 --> 00:20:15.120
the month of January.

00:20:15.120 --> 00:20:17.460
We picked Denver because there's
a wide variation of

00:20:17.460 --> 00:20:18.870
weather between winter
and summer.

00:20:18.870 --> 00:20:22.210
So as you can see here, this
will produce the distance away

00:20:22.210 --> 00:20:24.810
from this middle of the
circle, along with the

00:20:24.810 --> 00:20:27.500
latitude and longitude of the
weather station and the

00:20:27.500 --> 00:20:29.360
temperature as well, which
is really interesting.

00:20:29.360 --> 00:20:30.250
So you can see the temperatures

00:20:30.250 --> 00:20:31.800
here are pretty cold.

00:20:31.800 --> 00:20:33.820
To just give you an example of
the cool things you can do,

00:20:33.820 --> 00:20:35.670
you can switch this up
to June or July.

00:20:35.670 --> 00:20:36.660
Let's take July.

00:20:36.660 --> 00:20:39.810
RYAN BOYD: So we're about 30
degrees or so, I believe, in

00:20:39.810 --> 00:20:41.060
Fahrenheit.

00:20:41.060 --> 00:20:44.360
So switching to July, what
do we end up with here?

00:20:44.360 --> 00:20:44.990
MICHAEL MANOOCHEHRI:
Should be warmer.

00:20:44.990 --> 00:20:45.450
RYAN BOYD: 70-ish.

00:20:45.450 --> 00:20:46.070
MICHAEL MANOOCHEHRI:
Sounds pretty good.

00:20:46.070 --> 00:20:47.560
Very mild, very great.

00:20:47.560 --> 00:20:49.790
Love to be in Denver then.

00:20:49.790 --> 00:20:50.210
Looks good.

00:20:50.210 --> 00:20:53.780
So this exact example is on
our Query Cookbook on the

00:20:53.780 --> 00:20:56.820
Google Developers page for
BigQuery, and I hope you check

00:20:56.820 --> 00:21:00.610
it out if you're interested
in this type of query.

00:21:00.610 --> 00:21:00.890
All right.

00:21:00.890 --> 00:21:03.850
So let's talk about nested
and repeated data.

00:21:03.850 --> 00:21:08.550
So to get your data into
BigQuery, you have to ingest

00:21:08.550 --> 00:21:09.140
source files.

00:21:09.140 --> 00:21:11.440
And your source files can
be in CSV data, which is

00:21:11.440 --> 00:21:14.570
rectangular, log-shaped data
fits into this very well, but

00:21:14.570 --> 00:21:17.250
you can also ingest JSON data.

00:21:17.250 --> 00:21:20.970
And JSON data can model a nested
and repeated dataset.

00:21:20.970 --> 00:21:23.370
So let me just show you a
little bit what I mean.

00:21:23.370 --> 00:21:24.580
Here's an example
I like to use.

00:21:24.580 --> 00:21:27.030
I call it the social music
store dataset.

00:21:27.030 --> 00:21:29.940
So in this case, imagine a
social music store where you

00:21:29.940 --> 00:21:32.260
have a lot of customers, and
they can either listen to or

00:21:32.260 --> 00:21:33.390
purchase music.

00:21:33.390 --> 00:21:36.910
And your logs will record
each of these events.

00:21:36.910 --> 00:21:40.240
So here's an example of, Michael
is listening to a

00:21:40.240 --> 00:21:42.640
song, and he decides to
purchase the song.

00:21:42.640 --> 00:21:45.370
The two types of data, listening
and purchasing, have

00:21:45.370 --> 00:21:47.960
different data types in that,
when you listen, it doesn't

00:21:47.960 --> 00:21:48.610
cost anything.

00:21:48.610 --> 00:21:50.320
So you're not recording
a cost.

00:21:50.320 --> 00:21:52.420
And when you purchase something,
you're actually

00:21:52.420 --> 00:21:54.820
buying it, so there's a cost
associated with that.

00:21:54.820 --> 00:21:58.810
And so you could model this in
a flat, CSV-style format.

00:21:58.810 --> 00:22:00.660
But as you can see, there's
redundancy,

00:22:00.660 --> 00:22:02.190
there's some null values.

00:22:02.190 --> 00:22:04.870
You may have to have a separate
field for listen and

00:22:04.870 --> 00:22:06.860
purchase, like a flag
that describes it.

00:22:06.860 --> 00:22:07.300
So this works.

00:22:07.300 --> 00:22:09.270
There's nothing wrong with it,
and BigQuery can actually

00:22:09.270 --> 00:22:11.550
handle this type of data, if
this is how you have it.

00:22:11.550 --> 00:22:14.070
But imagine another way, maybe
a different way to model this

00:22:14.070 --> 00:22:15.970
data to give you some
added advantages.

00:22:15.970 --> 00:22:18.020
And I'll talk about
that in a second.

00:22:18.020 --> 00:22:21.230
So you can actually take that
rectangular data and model it

00:22:21.230 --> 00:22:23.270
like this, in a nested
and repeated way.

00:22:23.270 --> 00:22:27.350
So every day, I can actually
have a top level record called

00:22:27.350 --> 00:22:28.800
Michael, and it can show
my activities.

00:22:28.800 --> 00:22:31.530
So in this case, I've
highlighted the listens, and

00:22:31.530 --> 00:22:34.500
that is a separate model, a
separate type of record, and

00:22:34.500 --> 00:22:35.420
then below it is purchases.

00:22:35.420 --> 00:22:38.380
So as you can see, there's a
cost associated with it.

00:22:38.380 --> 00:22:41.350
Basically, I can have two
different types of records

00:22:41.350 --> 00:22:43.360
within this Michael record.

00:22:43.360 --> 00:22:44.050
So what does this mean?

00:22:44.050 --> 00:22:45.180
So why is there an
advantage here?

00:22:45.180 --> 00:22:47.750
Well, there's actually
several.

00:22:47.750 --> 00:22:49.860
RYAN BOYD: I just want to say
I mean, it's actually a much

00:22:49.860 --> 00:22:53.080
more natural way to model the
data rather than flatting it

00:22:53.080 --> 00:22:55.990
out, and it's also a way
that's used by a lot of

00:22:55.990 --> 00:22:59.710
standard NoSQL databases and
other databases to model data.

00:22:59.710 --> 00:23:02.780
So that makes BigQuery really
compatible if you're trying to

00:23:02.780 --> 00:23:06.110
analyze your massive dataset
that's stored in a typical

00:23:06.110 --> 00:23:07.200
NoSQL database.

00:23:07.200 --> 00:23:09.150
You can load it into BigQuery
and run the

00:23:09.150 --> 00:23:10.360
aggregate queries there.

00:23:10.360 --> 00:23:10.780
MICHAEL MANOOCHEHRI: No doubt.

00:23:10.780 --> 00:23:12.940
As I was saying before, this
is a big advantage.

00:23:12.940 --> 00:23:14.830
And if you're using something
like Mongo, you can actually

00:23:14.830 --> 00:23:16.520
export the data as JSON.

00:23:16.520 --> 00:23:18.900
It's the native file
format for some of

00:23:18.900 --> 00:23:19.670
those documents stores.

00:23:19.670 --> 00:23:22.440
So really easy to get the data
into BigQuery that way.

00:23:22.440 --> 00:23:25.230
But actually this kind of
structure, this nested and

00:23:25.230 --> 00:23:27.510
repeated structure, supports
some additional query

00:23:27.510 --> 00:23:29.130
functionality which
is really useful.

00:23:29.130 --> 00:23:30.610
One is WITHIN RECORD.

00:23:30.610 --> 00:23:33.960
So in this case, what you can do
is actually find aggregate

00:23:33.960 --> 00:23:35.720
data within a specific record.

00:23:35.720 --> 00:23:37.570
And I'll explain how that
works in a second.

00:23:37.570 --> 00:23:39.390
The other is POSITION.

00:23:39.390 --> 00:23:42.350
If you ingest your data in a
nested way with repeated

00:23:42.350 --> 00:23:45.320
values, BigQuery actually
remembers the position of each

00:23:45.320 --> 00:23:48.190
one, so you can do cool things
like playlists and show the

00:23:48.190 --> 00:23:49.640
position of a song
in a playlist.

00:23:49.640 --> 00:23:51.420
And I'll give you an example
in a second.

00:23:51.420 --> 00:23:54.290
Just to give you an idea, this
is the kind of query.

00:23:54.290 --> 00:23:56.050
How many users have
listened to a

00:23:56.050 --> 00:23:58.100
particular artist, for example.

00:23:58.100 --> 00:24:02.950
This kind of query you can
actually say, well, the count

00:24:02.950 --> 00:24:05.150
of people that listen to this
particular artist in this

00:24:05.150 --> 00:24:07.740
playlist, you could have a full
count, an aggregate over

00:24:07.740 --> 00:24:12.880
the entire dataset, or you can
actually use WITHIN RECORD to

00:24:12.880 --> 00:24:15.540
actually pull out the average
for each individual user,

00:24:15.540 --> 00:24:17.920
which is super useful.

00:24:17.920 --> 00:24:20.370
And here's an example of what
position is a particular

00:24:20.370 --> 00:24:22.190
artist's song in a daily
user playlist.

00:24:22.190 --> 00:24:26.380
Here I'm using the POSITION
function to

00:24:26.380 --> 00:24:28.190
actually pull that out.

00:24:28.190 --> 00:24:29.680
I have some sample data.

00:24:29.680 --> 00:24:32.550
Have a fake music store, and we
can actually look at some

00:24:32.550 --> 00:24:35.800
of these queries in action.

00:24:35.800 --> 00:24:38.455
So let's start with the
WITHIN RECORD query.

00:24:40.960 --> 00:24:44.290
In my fake music store, I'm
going to run a subquery to

00:24:44.290 --> 00:24:48.090
find users who are listening
to the artist PSY, and I'm

00:24:48.090 --> 00:24:52.310
going to find the count within
record of PSY's songs.

00:24:52.310 --> 00:24:53.340
So let's see.

00:24:53.340 --> 00:24:56.330
It's a pretty small dataset, but
in this case, you can see

00:24:56.330 --> 00:24:59.680
I have apparently two PSY
songs in my playlist.

00:24:59.680 --> 00:25:01.670
I didn't know there was another
popular PSY song, but

00:25:01.670 --> 00:25:02.880
it's in there.

00:25:02.880 --> 00:25:04.660
And now let's take a
look at position.

00:25:04.660 --> 00:25:05.930
So this is very interesting.

00:25:05.930 --> 00:25:09.148
I'm going to look at
the actual position

00:25:09.148 --> 00:25:11.640
of a song in a playlist.

00:25:11.640 --> 00:25:15.620
As you saw, I may have multiple
songs by a particular

00:25:15.620 --> 00:25:16.700
artist in my playlist.

00:25:16.700 --> 00:25:19.530
Now here I can say, find
the actual position.

00:25:19.530 --> 00:25:22.430
So you can see in this case,
each of the users in our

00:25:22.430 --> 00:25:24.750
dataset has the song in
a different position.

00:25:24.750 --> 00:25:27.890
This is very useful, for
example, if you want to know

00:25:27.890 --> 00:25:28.780
things like bouncing.

00:25:28.780 --> 00:25:31.690
Like when people come to your
site, when do they bounce out?

00:25:31.690 --> 00:25:34.390
You can use playlist data
or shopping data.

00:25:34.390 --> 00:25:36.670
What kind of things are people
buying in sequence?

00:25:36.670 --> 00:25:39.720
There's a lot of applications
for this type of query, and

00:25:39.720 --> 00:25:41.820
it's actually supported by
BigQuery using nested and

00:25:41.820 --> 00:25:43.070
repeated data.

00:25:45.344 --> 00:25:45.840
So yeah.

00:25:45.840 --> 00:25:50.290
So I think that pretty much
wraps up our querying session.

00:25:50.290 --> 00:25:52.530
A lot of the queries you saw
today are available in our

00:25:52.530 --> 00:25:56.870
Query Cookbook, which is on
developers.google.com/bigquery.

00:25:56.870 --> 00:25:58.080
And of course we want
to hear from you.

00:25:58.080 --> 00:25:59.920
We want to know what kind of
things you're doing, and what

00:25:59.920 --> 00:26:01.300
kind of queries you're
running, what

00:26:01.300 --> 00:26:01.930
you'd like to see.

00:26:01.930 --> 00:26:04.080
If there's a query you're having
trouble with, of course

00:26:04.080 --> 00:26:06.060
Stack Overflow is a good
place for that.

00:26:06.060 --> 00:26:09.260
Post your question using the
google-bigquery tag, and some

00:26:09.260 --> 00:26:11.710
of our engineers are on that
site all the time and we'll

00:26:11.710 --> 00:26:13.820
get back to you as
soon as we can.

00:26:13.820 --> 00:26:15.020
RYAN BOYD: I think that
about wraps it up.

00:26:15.020 --> 00:26:16.080
I'm Ryan.

00:26:16.080 --> 00:26:16.990
MICHAEL MANOOCHEHRI:
I'm Michael.

00:26:16.990 --> 00:26:18.470
RYAN BOYD: And this is the
Michael and Ryan Show.

00:26:18.470 --> 00:26:19.720
Thank you.

00:26:25.570 --> 00:26:39.067
[MUSIC PLAYING]

