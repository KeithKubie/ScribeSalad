WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:04.350
So let's look more closely at what
is called cache-to-cache transfers,

00:00:04.350 --> 00:00:10.360
which is what happens when core one
has block B in a modified state,

00:00:10.360 --> 00:00:15.230
and another core puts a read
request on the bus, so

00:00:15.230 --> 00:00:18.228
that it wants to basically bring
the block in its own cache.

00:00:18.228 --> 00:00:23.050
At this point, C1 has to react.

00:00:23.050 --> 00:00:26.370
Because C1 is the one that
has to provide the data,

00:00:26.370 --> 00:00:28.110
if we have the block in the M state,

00:00:28.110 --> 00:00:33.420
that means that the only up to date
copy in the system is our copy.

00:00:33.420 --> 00:00:36.430
Even the memory doesn't
have an up to date copy.

00:00:36.430 --> 00:00:42.370
So if the data is going to be sent to
C2, it had better be our data, but how,

00:00:42.370 --> 00:00:46.450
and we have briefly touched on this when
we were discussing the inside protocol.

00:00:46.450 --> 00:00:48.520
There are really two solutions.

00:00:48.520 --> 00:00:51.300
The first solution is abort and retry.

00:00:51.300 --> 00:00:56.680
The idea here is that C1
somehow cancels C2's request,

00:00:56.680 --> 00:00:59.710
and for that, we need some
sort of an abort bus signal,

00:00:59.710 --> 00:01:04.510
that when our request is placed on the
bus, another core can assert the abort

00:01:04.510 --> 00:01:09.820
signal that tells the requesting core to
kind of back off and try again later.

00:01:09.820 --> 00:01:17.410
Now that C2's request has been aborted,
C1 can do a normal write back to memory.

00:01:17.410 --> 00:01:21.680
At this point,
the memory has the up to date data, and

00:01:21.680 --> 00:01:26.790
when C2 retries its read request,
it will get the data from memory.

00:01:26.790 --> 00:01:29.710
The problem with this
approach is that really,

00:01:29.710 --> 00:01:32.557
from the time C2 makes
the request If this data

00:01:32.557 --> 00:01:36.572
was normally coming from memory,
we will have a memory latency,

00:01:36.572 --> 00:01:40.700
so if this is just a read miss,
we will have a memory latency.

00:01:40.700 --> 00:01:45.170
But if it is a read miss where
another core has the data,

00:01:45.170 --> 00:01:50.110
we really have to have two memory
latency's before C2 can get the data.

00:01:50.110 --> 00:01:54.980
One for the write-back to memory
to happen on C1, and then for

00:01:54.980 --> 00:01:57.320
us to actually get the data from memory.

00:01:57.320 --> 00:02:02.240
So the read miss from C1 has a latency
that is twice the memory latency,

00:02:02.240 --> 00:02:03.290
plus some.

00:02:03.290 --> 00:02:06.020
So this is why this
approach is not ideal.

00:02:06.020 --> 00:02:10.110
The second solution is
called intervention.

00:02:10.110 --> 00:02:14.740
In this case, C1C is the read
request is being made, and

00:02:14.740 --> 00:02:19.650
it tells the memory somehow that C1 will
supply the data instead of the memory.

00:02:19.650 --> 00:02:23.440
So normally the memory would
respond to a read miss request,

00:02:23.440 --> 00:02:26.890
now C1 kind of tells memory
that I will do that, and for

00:02:26.890 --> 00:02:31.170
that we need an intervention
bus signal through which C1 by

00:02:32.270 --> 00:02:36.890
asserting the interventions signal,
signals to the memory not to respond.

00:02:36.890 --> 00:02:40.970
So the memory now doesn't
respond with the data, but

00:02:40.970 --> 00:02:45.700
C1 does, and now the memory must
pick up the data that C1 was

00:02:45.700 --> 00:02:50.940
responding with in order to put it
in the appropriate memory block.

00:02:50.940 --> 00:02:55.990
The reason the memory has to pick up
the data is that once C1 responds

00:02:55.990 --> 00:03:01.160
with the data, both C1 and C2 will
have the block in the shared state.

00:03:01.160 --> 00:03:05.580
So both of them will think that the
block is not dirty, and that means that

00:03:05.580 --> 00:03:09.780
if the memory doesn't get the data now,
it will never get the fresh data.

00:03:09.780 --> 00:03:11.230
So by picking up the data,

00:03:11.230 --> 00:03:16.160
the memory ensures that it gets the data
that will not be written back anymore.

00:03:16.160 --> 00:03:20.580
So the disadvantage of this approach,
is that it needs more complex hardware.

00:03:20.580 --> 00:03:24.990
Now our cache needs to be able to kind
of insert its data, into what would

00:03:24.990 --> 00:03:29.480
normally be the memory sending data,
and also the memory has to have enough

00:03:29.480 --> 00:03:34.160
intelligence to pick up the data when
a core is responding to an intervention

00:03:34.160 --> 00:03:39.820
request, instead of just picking up the
data when there is a write back request.

00:03:39.820 --> 00:03:42.850
In modern processors
we nowadays mostly use

00:03:42.850 --> 00:03:45.530
a variant of an intervention approach.

00:03:45.530 --> 00:03:49.870
And the reason is that, yes the hardware
is slightly more complex, but

00:03:49.870 --> 00:03:54.020
fancier snooping protocols actually
eliminate the problem of memory picking

00:03:54.020 --> 00:03:58.070
up the data, and the complexity to
respond with the data is not too large.

00:03:58.070 --> 00:04:00.810
In contrast,
here the performance suffers.

00:04:00.810 --> 00:04:05.390
So pretty much, between having
a fancier protocol and having enough

00:04:05.390 --> 00:04:10.120
transistors from Moore's law to
actually add this capability to caches,

00:04:10.120 --> 00:04:14.280
we pretty much can do this and not have
to suffer in performance like this.

