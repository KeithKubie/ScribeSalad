WEBVTT
Kind: captions
Language: en

00:00:00.470 --> 00:00:03.180
So it's important to understand these two concepts

00:00:03.180 --> 00:00:06.700
of latency and throughput. Latency is the elapsed time

00:00:06.700 --> 00:00:09.280
from an event. If it takes me one minute

00:00:09.280 --> 00:00:12.300
to walk from my office to the classroom, that's

00:00:12.300 --> 00:00:14.710
the latency that I'm going to experience for that event

00:00:14.710 --> 00:00:18.200
of walking from my office to the classroom. That's

00:00:18.200 --> 00:00:22.410
the elapsed time. Throughput is the number of events

00:00:22.410 --> 00:00:25.510
that can be executed per unit time. Bandwidth is

00:00:25.510 --> 00:00:28.090
a measure of throughput. So once again with this

00:00:28.090 --> 00:00:31.870
analogy of walking to the classroom from my office, if

00:00:31.870 --> 00:00:36.380
the hallway is wide enough to allow five, ten

00:00:36.380 --> 00:00:39.020
of us to walk in parallel side by side to

00:00:39.020 --> 00:00:42.660
the classroom, increases the throughput but it does nothing

00:00:42.660 --> 00:00:44.940
to the latency. The latency is going to be determined by

00:00:44.940 --> 00:00:46.650
how fast I can walk from my office to

00:00:46.650 --> 00:00:50.630
the classroom. So, the difference between latency and, and throughput

00:00:50.630 --> 00:00:52.870
is very important to understand. In other

00:00:52.870 --> 00:00:56.670
words, I can increase the bandwidth and that'll

00:00:56.670 --> 00:00:58.840
improve the throughput but it is not going

00:00:58.840 --> 00:01:01.160
to do anything to the latency itself. So

00:01:01.160 --> 00:01:04.030
in other words, higher bandwidth does not

00:01:04.030 --> 00:01:07.430
necessarily imply lower latency. You'll work hard to

00:01:07.430 --> 00:01:12.040
lower the latency. RPC is a basis for

00:01:12.040 --> 00:01:15.770
client server based distributed systems. And performance of

00:01:15.770 --> 00:01:24.060
RPC is crucial, specifically in the context of this lesson. Latency refers to

00:01:24.060 --> 00:01:26.120
the time it takes for an application

00:01:26.120 --> 00:01:29.070
generated message to reach it's destination. So

00:01:29.070 --> 00:01:34.310
for instance, if you're doing an RPC call from a client to the server, then the

00:01:34.310 --> 00:01:40.870
RPC call entails sending the argument from the client to the server.

00:01:40.870 --> 00:01:43.160
And there is work to be done here, work

00:01:43.160 --> 00:01:45.190
to be done in sending the message, work to

00:01:45.190 --> 00:01:48.180
be done here before the server can actually execute

00:01:48.180 --> 00:01:51.125
the server procedure. So it's the latency that we

00:01:51.125 --> 00:01:54.150
are concerned about. And what we will see is

00:01:54.150 --> 00:01:58.640
all the software components that comprise the latency for

00:01:58.640 --> 00:02:02.280
RPC based communication. And performance of RPC is very

00:02:02.280 --> 00:02:06.710
crucial in building client server systems. There are two components

00:02:06.710 --> 00:02:11.220
to the latency that is observed for message communication in a distributive

00:02:11.220 --> 00:02:15.390
system. The first component is the hardware overhead and the second component is

00:02:15.390 --> 00:02:20.430
the software overhead. The hardware overhead is really dependent on how the

00:02:20.430 --> 00:02:26.460
network is interfaced to the computer. So, typically in

00:02:26.460 --> 00:02:32.680
any computer, what you have is a network controller that interfaces

00:02:32.680 --> 00:02:36.050
the network to the CPU. And typically, the

00:02:36.050 --> 00:02:39.310
network controller operates by moving the bits of

00:02:39.310 --> 00:02:43.310
the message from the system memory of the

00:02:43.310 --> 00:02:46.980
node into it's private buffer, which is inside the

00:02:46.980 --> 00:02:49.540
network controller. And this part of it, moving

00:02:49.540 --> 00:02:52.550
the bits from the memory of the node

00:02:52.550 --> 00:02:55.230
into the internal buffer of the network controller

00:02:55.230 --> 00:02:58.380
is accomplished using what is called direct memory access.

00:02:58.380 --> 00:03:01.140
Meaning the network controller is smart enough to move the

00:03:01.140 --> 00:03:04.690
bits directly using the bus that connects the memory to

00:03:04.690 --> 00:03:07.862
the network controller without the intervention of the CPU. And

00:03:07.862 --> 00:03:11.190
this is what is called dir, direct memory access. And that's

00:03:11.190 --> 00:03:14.320
how the bits are moved from the memory of the

00:03:14.320 --> 00:03:18.000
system into the buffer of the network controller, and once it

00:03:18.000 --> 00:03:20.920
comes here, the network controller can then put the bits

00:03:20.920 --> 00:03:23.530
out on the wire, and this is where the bandwidth that

00:03:23.530 --> 00:03:27.650
you have connecting your node to the network comes

00:03:27.650 --> 00:03:30.140
into play. But there are also other types of

00:03:30.140 --> 00:03:33.820
network controllers where the CPU may actually be involved

00:03:33.820 --> 00:03:36.200
in the data movement, and in that case, the

00:03:36.200 --> 00:03:39.890
CPU does program I/O to move the bits from

00:03:39.890 --> 00:03:43.130
the memory into the buffer of the network controller,

00:03:43.130 --> 00:03:45.040
from which the network controller will then put it

00:03:45.040 --> 00:03:48.540
out on the network. But modern network controllers tend to

00:03:48.540 --> 00:03:51.874
be built using DMA technique, meaning that the

00:03:51.874 --> 00:03:55.430
network controller, once the CPU tells the network controller

00:03:55.430 --> 00:03:58.760
were in memory the messages to be sent

00:03:58.760 --> 00:04:01.200
on the wire, network controller does the rest in

00:04:01.200 --> 00:04:03.478
terms of moving the bits into it's internal

00:04:03.478 --> 00:04:05.468
buffer, and then from the buffer putting it out

00:04:05.468 --> 00:04:09.680
onto the network. The software overhead is what the

00:04:09.680 --> 00:04:14.030
operating system tax on to the hardware overhead of

00:04:14.030 --> 00:04:17.560
moving the bits out onto the network. So

00:04:17.560 --> 00:04:19.790
the latency, if you think about the latency

00:04:19.790 --> 00:04:23.560
as a whole for doing a network transmission,

00:04:23.560 --> 00:04:26.540
there is the software overhead incurred in the

00:04:26.540 --> 00:04:29.470
layers of the operating system to make the

00:04:29.470 --> 00:04:33.380
message available in the memory of the processor,

00:04:33.380 --> 00:04:36.050
ready for transmission. Once it is ready for

00:04:36.050 --> 00:04:39.110
transmission, the hardware overhead kicks in, and the

00:04:39.110 --> 00:04:42.840
hardware, the network controller in particular, moves the bits

00:04:42.840 --> 00:04:45.260
from the memory into it's buffer and then out on

00:04:45.260 --> 00:04:47.890
the wire. The focus of course being an operating

00:04:47.890 --> 00:04:54.170
system designers work, is to reduce the software overhead and

00:04:54.170 --> 00:04:56.500
take what the hardware gives you and think about

00:04:56.500 --> 00:04:58.810
how you can reduce the software overhead so that we

00:04:58.810 --> 00:05:02.224
can overall reduce the latency involved in transmission. Which

00:05:02.224 --> 00:05:05.085
is a sum of the hardware overhead and software overhead.

