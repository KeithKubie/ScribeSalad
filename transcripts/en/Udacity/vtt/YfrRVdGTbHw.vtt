WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:03.060
So let's talk about cache affinity and modern

00:00:03.060 --> 00:00:06.750
multicore processors, in modern multicore processors you have

00:00:06.750 --> 00:00:09.960
multiple cores on a single processor, and in

00:00:09.960 --> 00:00:12.100
addition to the multiple cores that are in a

00:00:12.100 --> 00:00:15.700
single processor, the processors the,selves are also hardware

00:00:15.700 --> 00:00:19.990
multithreaded. What hardware multithreading means is that. If

00:00:19.990 --> 00:00:22.150
a, if a thread that is currently running

00:00:22.150 --> 00:00:25.650
on a processor, on a core C1, is experiencing

00:00:25.650 --> 00:00:28.820
a long latency operation, for instance it misses in the

00:00:28.820 --> 00:00:30.880
cache and therefore has to go out in order to

00:00:30.880 --> 00:00:34.930
fetch the contents from memory, that's a long latency operation.

00:00:34.930 --> 00:00:38.430
In that case. The hardware may switch to one of the

00:00:38.430 --> 00:00:42.110
other threads and run those. So, in other words, it

00:00:42.110 --> 00:00:44.670
wants to keep this core busy. There's only one execution

00:00:44.670 --> 00:00:47.970
engine in this core, but it has four threads that

00:00:47.970 --> 00:00:50.710
it can run on this core. Depending on what these threads

00:00:50.710 --> 00:00:53.480
are doing, if they are involved in long latency operations,

00:00:53.480 --> 00:00:55.910
meaning they are going out, they're not switched out of

00:00:55.910 --> 00:00:59.540
the processor, in terms of operating system scheduling. It is

00:00:59.540 --> 00:01:03.130
just that these are the, the threads that have been scheduled

00:01:03.130 --> 00:01:06.660
to run on this core and the hardware is switching

00:01:06.660 --> 00:01:10.000
among these threads by itself without the intervention of the operating

00:01:10.000 --> 00:01:12.920
system. It is automatically switching among these threads depending on

00:01:12.920 --> 00:01:15.903
what these threads are doing. If this thread, does the memory

00:01:15.903 --> 00:01:19.207
access which is going outside the processor, then the hardware is

00:01:19.207 --> 00:01:22.440
going to say, well, you know this guy is going to be. Waiting

00:01:22.440 --> 00:01:24.940
for a while, so let me switch to this guy, and

00:01:24.940 --> 00:01:27.960
let him do its, do its work because it's possible that

00:01:27.960 --> 00:01:30.670
what he needs is available in the cache. And if this

00:01:30.670 --> 00:01:35.110
guy also makes a long latency operation, like a memory access,

00:01:35.110 --> 00:01:37.820
then the hardware can switch to this guy. And to this

00:01:37.820 --> 00:01:40.820
guy. So if all of these guys are waiting on memory,

00:01:40.820 --> 00:01:42.140
then of course the core is not able to

00:01:42.140 --> 00:01:44.520
going to be, going to be able to do anything useful until

00:01:44.520 --> 00:01:47.450
at least one of these memory accesses are complete. So

00:01:47.450 --> 00:01:51.510
that's the idea behind. Hardware multithreading. So it is not

00:01:51.510 --> 00:01:55.870
very unusual for modern multicore processors to employ hardware

00:01:55.870 --> 00:01:58.930
multithreading. So in this example I'm showing you, there are

00:01:58.930 --> 00:02:02.610
four cores and in each core I have four hardware

00:02:02.610 --> 00:02:07.190
threads. So it is a four way hardware multithreaded core.

00:02:07.190 --> 00:02:13.510
And I'm showing you two levels of caches, L1 and L2 cache. L1 cache is specific

00:02:13.510 --> 00:02:16.040
to this particular core, C1, shared by these

00:02:16.040 --> 00:02:20.290
threads. Similarly, L1 cache here is. Is specific to

00:02:20.290 --> 00:02:25.450
this core C2, shared by the threads that are on it. On the other hand this

00:02:25.450 --> 00:02:28.140
L2 cache, is common for all the cores.

00:02:28.140 --> 00:02:32.210
So [INAUDIBLE] and any, anyone of these L1 caches,

00:02:32.210 --> 00:02:34.740
the hope is that we were able to find it

00:02:34.740 --> 00:02:37.900
in the L2 cache. If the processor. Has only these

00:02:37.900 --> 00:02:40.980
two levels of caches, L1 cache and L2 cache. This

00:02:40.980 --> 00:02:43.330
thing in L2 cache is really bad news [LAUGH] because

00:02:43.330 --> 00:02:46.170
then, you're going all to, all to the chip. It's

00:02:46.170 --> 00:02:50.640
a long latency memory operation. And modern multiprocessors may in

00:02:50.640 --> 00:02:55.770
fact even employ even more levels of caching. In addition

00:02:55.770 --> 00:02:58.080
to L1 and L2, there may be an L3 cache.

00:02:58.080 --> 00:03:01.090
It's normal to have, modern processors having at least

00:03:01.090 --> 00:03:04.170
three levels of caches on the chip. And L1

00:03:04.170 --> 00:03:07.750
cache associated with core, and a shared L2 cache,

00:03:07.750 --> 00:03:10.860
and a shared L3 cache. So that's the structure

00:03:10.860 --> 00:03:13.120
that you might see in modern mulitprocessors. So what

00:03:13.120 --> 00:03:15.710
we have to think about now is. Eh, thinking

00:03:15.710 --> 00:03:19.440
about this cache affinity and the modern multi core

00:03:19.440 --> 00:03:23.180
processes and how the operating system should make its

00:03:23.180 --> 00:03:26.010
scheduling decisions. So here again, there's a

00:03:26.010 --> 00:03:30.210
partnership between the operating system and the hardware.

00:03:30.210 --> 00:03:34.400
The hardware is providing these hardware threads inside

00:03:34.400 --> 00:03:37.180
each core. And what the operating system is

00:03:37.180 --> 00:03:43.240
doing is picking which threads that it has in its pool of vulnerable threads and

00:03:43.240 --> 00:03:45.370
map them on to the threads that are

00:03:45.370 --> 00:03:48.400
available in the hardware. And clearly, the scheduling

00:03:48.400 --> 00:03:51.820
decision, what it tries to do is to make sure

00:03:51.820 --> 00:03:55.840
that. Most of the threads that are scheduled a particular core

00:03:55.840 --> 00:04:00.150
may find their working set in the L1 cache if possible,

00:04:00.150 --> 00:04:02.880
and similarly the threads that are scheduled on this may find

00:04:02.880 --> 00:04:05.946
its working set of [UNKNOWN] cache of C2 if possible, and

00:04:05.946 --> 00:04:08.540
so on. And also the other thing that the operating system

00:04:08.540 --> 00:04:10.612
may try to do is, if you just take the universal,

00:04:10.612 --> 00:04:13.700
all the threads That are currently scheduled by the operating system

00:04:13.700 --> 00:04:16.320
to run on all these four cores. You want to make

00:04:16.320 --> 00:04:19.990
sure that working set of all these threads are likely to

00:04:19.990 --> 00:04:23.110
be found in the L2 cache, because if you're missing the

00:04:23.110 --> 00:04:26.960
L2 cache bad news because then you're going outside the chip

00:04:26.960 --> 00:04:29.760
and that's a very long latency memory operation. And of course

00:04:29.760 --> 00:04:32.040
you can extend this idea if, if there is a third

00:04:32.040 --> 00:04:34.950
level of cache but to make things concrete let's just stick

00:04:34.950 --> 00:04:39.110
to two levels of caches L1 cache and L2 cache and.

00:04:39.110 --> 00:04:41.740
The criterion for operating system is to

00:04:41.740 --> 00:04:43.760
make sure that the threads that are currently

00:04:43.760 --> 00:04:46.810
scheduled on the processors that's available, all the

00:04:46.810 --> 00:04:48.800
cores that are available, what it wants to

00:04:48.800 --> 00:04:52.630
try to do is make sure that all the threads will find the contents in the

00:04:52.630 --> 00:04:54.500
L2 cache. Because nothing in the L2 cache

00:04:54.500 --> 00:04:56.760
is going to be elongating the memory operation.

