WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:01.490
SPEAKER 1: Cennydd?

00:00:01.490 --> 00:00:02.440
Excellent.

00:00:02.440 --> 00:00:04.790
Cennydd it is a
designer and an author

00:00:04.790 --> 00:00:09.870
and he has been looking
particularly at future ethics.

00:00:09.870 --> 00:00:14.190
He's been advising different
companies like Samsung, Twitter

00:00:14.190 --> 00:00:17.920
around the ethical
implications of technology.

00:00:17.920 --> 00:00:18.599
Thank you.

00:00:18.599 --> 00:00:19.390
CENNYDD: Thank you.

00:00:24.250 --> 00:00:27.030
All design is
interested in behavior.

00:00:27.030 --> 00:00:28.780
For interaction
design, that really

00:00:28.780 --> 00:00:30.970
points in two directions
that interest.

00:00:30.970 --> 00:00:32.920
One, we're interested
in designing

00:00:32.920 --> 00:00:36.190
the behavior of a system
responding to user input.

00:00:36.190 --> 00:00:40.240
But we're also interested in
the medium of human behavior.

00:00:40.240 --> 00:00:42.160
And for all design,
what we consider

00:00:42.160 --> 00:00:46.880
good design is often design that
directs behavior appropriately.

00:00:46.880 --> 00:00:49.360
A design is successful if
the user can react in the way

00:00:49.360 --> 00:00:52.450
that we'd planned for,
that we'd anticipated.

00:00:52.450 --> 00:00:54.190
A good interface,
for instance, is

00:00:54.190 --> 00:00:56.770
one that highlights the
right information in a system

00:00:56.770 --> 00:01:00.670
so that the user is steered
to the next step in a process.

00:01:00.670 --> 00:01:03.560
If we, say, enlarge
a button on a UI,

00:01:03.560 --> 00:01:04.810
yes, it makes it more visible.

00:01:04.810 --> 00:01:08.680
But the main point is, more
people will click that.

00:01:08.680 --> 00:01:10.660
Because it's about
behavior, then, that

00:01:10.660 --> 00:01:13.870
means design is laden
with ethical impact.

00:01:13.870 --> 00:01:17.499
Indeed, you could say that
design is applied ethics.

00:01:17.499 --> 00:01:19.540
I think it might have been
Cameron Tonkinwise who

00:01:19.540 --> 00:01:24.020
argued that design is morality
practiced with the hands.

00:01:24.020 --> 00:01:27.176
Now sometimes that relationship
is explicit and obvious.

00:01:27.176 --> 00:01:29.050
If you're designing
razor wire, for instance,

00:01:29.050 --> 00:01:31.240
you're saying that
people have a right

00:01:31.240 --> 00:01:35.230
to privacy and to property
such that actually people

00:01:35.230 --> 00:01:37.330
should be injured for
trying to contravene it.

00:01:37.330 --> 00:01:40.810
If you design speed bumps, then
you're saying that a car cannot

00:01:40.810 --> 00:01:44.560
travel down this road beyond a
certain speed without damaging

00:01:44.560 --> 00:01:47.090
the owner's suspension.

00:01:47.090 --> 00:01:50.050
In these instances, we're
delegating moral control

00:01:50.050 --> 00:01:53.680
to the artifacts
in the environment.

00:01:53.680 --> 00:01:56.510
But every act of design,
whatever the medium,

00:01:56.510 --> 00:01:59.210
is about specifying the future.

00:01:59.210 --> 00:02:00.920
It's about making
a claim for how

00:02:00.920 --> 00:02:02.720
we think the future should be.

00:02:02.720 --> 00:02:04.490
When we choose one
preferred future,

00:02:04.490 --> 00:02:08.030
we also discard thousands
of alternatives.

00:02:08.030 --> 00:02:11.490
And if design is really
around deciding what to make

00:02:11.490 --> 00:02:14.150
and if ethics is really
about deciding how to live,

00:02:14.150 --> 00:02:16.340
then there's clearly
some kind of relationship

00:02:16.340 --> 00:02:19.620
between those two things.

00:02:19.620 --> 00:02:22.430
So designers try persistently
to persuade people

00:02:22.430 --> 00:02:24.140
to act in desirable ways.

00:02:24.140 --> 00:02:26.960
And this is not a rational art.

00:02:26.960 --> 00:02:28.680
If it were, it would
be fairly simple.

00:02:28.680 --> 00:02:30.440
We'd present the
options available

00:02:30.440 --> 00:02:33.830
and we'd leave it to the user
to make the rational choice.

00:02:33.830 --> 00:02:35.510
But we know well
that other factors

00:02:35.510 --> 00:02:37.790
influence decision making--

00:02:37.790 --> 00:02:40.580
emotions, defaults, framing.

00:02:40.580 --> 00:02:42.350
So designers have
to appeal in part

00:02:42.350 --> 00:02:45.590
to the unpredictable aspects
of human nature-- in essence,

00:02:45.590 --> 00:02:47.750
to human weakness.

00:02:47.750 --> 00:02:51.370
Hence, nudge theory, which is
all about changing behavior

00:02:51.370 --> 00:02:53.680
without changing the
options available.

00:02:53.680 --> 00:02:57.530
It's all about the power
of framing and positioning.

00:02:57.530 --> 00:02:59.140
So we know the common examples.

00:02:59.140 --> 00:03:01.140
You can change defaults
to encourage opt-in

00:03:01.140 --> 00:03:03.010
for, say, organ donation.

00:03:03.010 --> 00:03:05.320
You can move the healthier
snacks by the tills

00:03:05.320 --> 00:03:08.810
to encourage people to
make healthier choices.

00:03:08.810 --> 00:03:11.410
And nudge, of course, has
been terrifically popular

00:03:11.410 --> 00:03:12.700
in the public sector.

00:03:12.700 --> 00:03:15.100
But Silicon Valley, I
think, likes it, as well.

00:03:15.100 --> 00:03:18.190
Nudge doesn't trespass on
the individual freedoms

00:03:18.190 --> 00:03:19.750
that the industry holds dear.

00:03:19.750 --> 00:03:22.120
But it's still a very potent
way to liberate people

00:03:22.120 --> 00:03:24.460
from their time and money.

00:03:24.460 --> 00:03:26.740
Nudge generally, I think,
has a positive reputation

00:03:26.740 --> 00:03:29.860
because its applications
are seen as positive.

00:03:29.860 --> 00:03:31.330
We always talk
about it in relation

00:03:31.330 --> 00:03:32.871
to things like weight
loss or smoking

00:03:32.871 --> 00:03:35.630
cessation, financial prudence.

00:03:35.630 --> 00:03:37.720
But of course, any
persuasive technique

00:03:37.720 --> 00:03:41.051
has a potentially
less healthy purpose.

00:03:41.051 --> 00:03:42.550
Designers, I think,
are particularly

00:03:42.550 --> 00:03:45.070
familiar with this dichotomy.

00:03:45.070 --> 00:03:47.770
Our industry designs
appealing cigarette packets

00:03:47.770 --> 00:03:51.900
and the calming hospital signage
for the smoker's last days.

00:03:51.900 --> 00:03:55.270
There's currently this panic
about persuasive technology

00:03:55.270 --> 00:03:57.460
becoming addictive.

00:03:57.460 --> 00:03:59.550
It's not one I'm quite
so concerned with.

00:03:59.550 --> 00:04:02.560
I think the lines between a
product that's simply so good

00:04:02.560 --> 00:04:05.140
that we want to come back to
it, it's so well-designed,

00:04:05.140 --> 00:04:07.840
and one that's explicitly
designed to capture attention--

00:04:07.840 --> 00:04:10.210
I think those lines are blurry.

00:04:10.210 --> 00:04:13.000
There's also not so much
direct evidence at the moment

00:04:13.000 --> 00:04:15.850
that technological addiction
is a separate disorder

00:04:15.850 --> 00:04:17.260
from existing ones.

00:04:17.260 --> 00:04:22.180
However, it's clear that chasing
engagement, that essentially

00:04:22.180 --> 00:04:23.830
trying to persuade
people to reuse

00:04:23.830 --> 00:04:26.080
our products on
a habitual basis,

00:04:26.080 --> 00:04:29.010
has led to some very
trashy uses of technology.

00:04:29.010 --> 00:04:34.270
This, of course, is the
fundamental goal, if you like,

00:04:34.270 --> 00:04:36.430
of Tristan Harris's
Time Well Spent movement

00:04:36.430 --> 00:04:40.500
to interrogate that
potential downside.

00:04:40.500 --> 00:04:44.220
More interesting to me, though,
is the politics of nudging.

00:04:44.220 --> 00:04:47.370
Because we find-- there's
a 2015 study that attitudes

00:04:47.370 --> 00:04:49.440
to nudging actually
vary quite a bit

00:04:49.440 --> 00:04:51.150
across the political spectrum.

00:04:51.150 --> 00:04:53.940
Libertarians particularly
are skeptical of nudge

00:04:53.940 --> 00:04:58.080
because they regard it as
the top of a slippery slope.

00:04:58.080 --> 00:05:01.639
But we find that whatever
someone's political leanings,

00:05:01.639 --> 00:05:03.180
the biggest way to
change their mind,

00:05:03.180 --> 00:05:05.310
to change their
appetite for nudging,

00:05:05.310 --> 00:05:08.335
is to show them examples that
they politically approve of,

00:05:08.335 --> 00:05:10.710
or to say that the people
trying to institute these nudge

00:05:10.710 --> 00:05:13.860
policies are politicians
that they happen to support.

00:05:13.860 --> 00:05:17.340
The same study also found that
nudge can induce antagonism.

00:05:17.340 --> 00:05:19.800
Some people-- mostly
right-leaning people--

00:05:19.800 --> 00:05:22.299
deliberately chose
the alternative option

00:05:22.299 --> 00:05:24.840
when they were told that they
were being nudged-- essentially

00:05:24.840 --> 00:05:26.900
a protest vote.

00:05:26.900 --> 00:05:31.190
Another study found that the
source of nudging matters.

00:05:31.190 --> 00:05:34.190
US and Israeli
participants in this study

00:05:34.190 --> 00:05:36.980
generally disliked nudges
that came from government

00:05:36.980 --> 00:05:37.760
and authorities.

00:05:37.760 --> 00:05:39.530
Instead, they
preferred employers

00:05:39.530 --> 00:05:40.700
to be doing the nudging.

00:05:40.700 --> 00:05:43.970
In Germany, there was
no such difference.

00:05:43.970 --> 00:05:46.410
And we rarely talk
about the politics

00:05:46.410 --> 00:05:50.240
of these persuasive angles,
even though Thaler and Sunstein,

00:05:50.240 --> 00:05:53.330
the architects of nudge
theory, went as far as to call

00:05:53.330 --> 00:05:55.100
nudge libertarian paternalism.

00:05:57.770 --> 00:05:59.840
Technologies are, of
course, great platforms

00:05:59.840 --> 00:06:02.220
for designing behavior change.

00:06:02.220 --> 00:06:04.940
They sit atop oceans of data.

00:06:04.940 --> 00:06:09.020
Online behavior, sure, but also
an increasingly datafied world

00:06:09.020 --> 00:06:13.020
typified by computer vision,
by audio capture, IoT

00:06:13.020 --> 00:06:17.390
and the smart city as
substrate for urban sensing.

00:06:17.390 --> 00:06:21.230
We all know the now banal
cliche that data is the new oil.

00:06:21.230 --> 00:06:24.190
But that contains some
interesting metaphors.

00:06:24.190 --> 00:06:27.070
We now talk about oil with
these liquid figures of speech.

00:06:27.070 --> 00:06:27.940
It's a deluge.

00:06:27.940 --> 00:06:28.930
It's a torrent.

00:06:28.930 --> 00:06:32.600
It's an uncontrollable
natural source.

00:06:32.600 --> 00:06:34.640
It's no accident,
of course, that oil

00:06:34.640 --> 00:06:37.310
is dangerous to
store at high volumes

00:06:37.310 --> 00:06:40.100
and that it has a
habit of leaking.

00:06:40.100 --> 00:06:42.230
We also now have the
computational ability

00:06:42.230 --> 00:06:44.360
to make sense of
this ocean of data.

00:06:44.360 --> 00:06:47.090
We have machine learning and
analytics tools to make sense

00:06:47.090 --> 00:06:49.370
and to act upon these systems.

00:06:49.370 --> 00:06:51.800
And this offers a
significant scale.

00:06:51.800 --> 00:06:54.650
It means we can abstract
persuasion to the technologies

00:06:54.650 --> 00:06:57.650
all around us, giving us
higher impact at lower cost.

00:07:00.540 --> 00:07:04.150
But the common examples of
technology and its persuasive

00:07:04.150 --> 00:07:06.910
potential, they're mostly
jejune and harmless.

00:07:06.910 --> 00:07:09.400
Think of wearable
fitness trackers,

00:07:09.400 --> 00:07:12.040
apps that remind you to
drink more water, and so on.

00:07:12.040 --> 00:07:15.100
But we're starting, now,
to find deeper and more

00:07:15.100 --> 00:07:18.430
eye-opening prospects.

00:07:18.430 --> 00:07:21.700
Karen Yeung, who's a legal
scholar based in the UK,

00:07:21.700 --> 00:07:25.114
says that we're entering an era
of what she calls hyper-nudge.

00:07:25.114 --> 00:07:26.530
This is a persuasive
system that's

00:07:26.530 --> 00:07:30.100
networked so that it can
learn from millions of people.

00:07:30.100 --> 00:07:32.950
It's dynamic, so it's
able to shift strategies

00:07:32.950 --> 00:07:34.950
unlike, for instance,
mass advertising.

00:07:34.950 --> 00:07:37.360
And it's personalized,
potentially down

00:07:37.360 --> 00:07:40.200
to the individual.

00:07:40.200 --> 00:07:44.410
So this is a system that knows
intimately you and your data.

00:07:44.410 --> 00:07:45.920
It can adapt to your behavior.

00:07:45.920 --> 00:07:48.890
It can change its mind if
an approach doesn't work.

00:07:48.890 --> 00:07:52.610
And it will, therefore, know
exactly which buttons to push.

00:07:52.610 --> 00:07:55.920
This could be an
irresistible manipulator.

00:07:55.920 --> 00:07:59.090
We already have crude variants
of this, already quite

00:07:59.090 --> 00:08:01.490
effective and quite scalable.

00:08:01.490 --> 00:08:04.670
Another study found 50% uplift
in Facebook ad click-throughs

00:08:04.670 --> 00:08:08.260
simply by tailoring
the copy in the image

00:08:08.260 --> 00:08:12.950
in the ad for very crude
personality types interpolated

00:08:12.950 --> 00:08:14.036
from a user's likes.

00:08:14.036 --> 00:08:15.410
This was a fairly
manual process,

00:08:15.410 --> 00:08:17.480
but it's clear how
this can be automated.

00:08:17.480 --> 00:08:19.520
50% is obviously a
very large uplift

00:08:19.520 --> 00:08:23.960
through this particularly
crude human-operated algorithm.

00:08:23.960 --> 00:08:26.660
Amazon already sent
millions of automated nudges

00:08:26.660 --> 00:08:29.290
to their affiliates.

00:08:29.290 --> 00:08:32.830
And this suggests, of course,
a significant ethical question.

00:08:32.830 --> 00:08:35.740
At what point does a
nudge become a shove?

00:08:35.740 --> 00:08:41.530
At what point does this act of
persuasion become unethical?

00:08:41.530 --> 00:08:44.840
And that's particularly
tricky to pin down.

00:08:44.840 --> 00:08:47.210
The ethics of
persuasion are complex.

00:08:47.210 --> 00:08:49.320
You always have multiple
actors, for a start.

00:08:49.320 --> 00:08:53.307
You have at least two-- a
persuader and a persuadee.

00:08:53.307 --> 00:08:55.140
Daniel Berdichevsky and
Erik Neuenschwander,

00:08:55.140 --> 00:08:59.220
early theorists of persuasive
technology, put it very nicely.

00:08:59.220 --> 00:09:03.010
"Persuaders have always stood
on uneasy ethical ground.

00:09:03.010 --> 00:09:05.440
If a serpent persuades
you to eat a fruit,

00:09:05.440 --> 00:09:10.310
does culpability fall on
you or upon the serpent?"

00:09:10.310 --> 00:09:12.320
Technologies dilute
this further.

00:09:12.320 --> 00:09:15.200
We still have the persuader
and the persuadee.

00:09:15.200 --> 00:09:17.930
But now technology is the
medium for that persuasion

00:09:17.930 --> 00:09:18.590
to take place.

00:09:18.590 --> 00:09:20.790
And tech itself has many layers.

00:09:20.790 --> 00:09:24.800
Morality seeps into the
whole technology stack.

00:09:24.800 --> 00:09:29.000
So if I have an AI that, say,
persuades me to commit fraud,

00:09:29.000 --> 00:09:30.030
who's responsible?

00:09:30.030 --> 00:09:30.920
Well, me, sure.

00:09:30.920 --> 00:09:32.720
I should bear some liability.

00:09:32.720 --> 00:09:35.390
But perhaps also the software
engineer, or the platform

00:09:35.390 --> 00:09:38.240
company that allowed the
app to exist unchecked

00:09:38.240 --> 00:09:40.950
in their app store or on
their operating system.

00:09:40.950 --> 00:09:43.145
Maybe at some point we'll
even blame the AI itself.

00:09:45.700 --> 00:09:47.525
This requires us,
of course, to admit

00:09:47.525 --> 00:09:49.150
what's been hinted
at a couple of times

00:09:49.150 --> 00:09:51.590
today, that technology
is not neutral.

00:09:51.590 --> 00:09:55.750
We don't just build mere
instruments to fulfill tasks.

00:09:55.750 --> 00:09:59.770
Back in 1980, Langdon Winner,
philosopher of technology,

00:09:59.770 --> 00:10:03.600
wrote a famous
essay now entitled,

00:10:03.600 --> 00:10:05.470
"Do Artifacts Have Politics?"

00:10:05.470 --> 00:10:08.740
His conclusion was, essentially,
well, damn right, they do.

00:10:08.740 --> 00:10:10.840
And Robert Moses was--

00:10:10.840 --> 00:10:14.010
Winner gives the example
of Robert Moses, rather,

00:10:14.010 --> 00:10:17.470
as the New York City town
planner back in maybe the '40s,

00:10:17.470 --> 00:10:18.970
something like that.

00:10:18.970 --> 00:10:20.390
And Moses was a racist.

00:10:20.390 --> 00:10:22.360
And although it's
slightly disputed,

00:10:22.360 --> 00:10:26.260
it is alleged that Moses built
intentionally low bridges

00:10:26.260 --> 00:10:29.680
covering the freeways on the
way down to Long Island beaches

00:10:29.680 --> 00:10:32.050
so that black people who
traveled mostly by bus

00:10:32.050 --> 00:10:33.580
wouldn't be able to get there.

00:10:33.580 --> 00:10:37.300
Even a bridge-- this hulking
monstrosity of hundreds of tons

00:10:37.300 --> 00:10:39.626
of concrete and steel and iron--

00:10:39.626 --> 00:10:41.990
have moral impact.

00:10:41.990 --> 00:10:45.170
It's a mistake to separate
technological capabilities

00:10:45.170 --> 00:10:46.790
from human capabilities.

00:10:46.790 --> 00:10:47.960
These things act together.

00:10:47.960 --> 00:10:51.380
We're interwoven,
hybridized actors.

00:10:51.380 --> 00:10:54.980
Things change what people
can do and how they do it.

00:10:54.980 --> 00:10:58.160
It is true that guns on
their own don't kill people.

00:10:58.160 --> 00:11:00.870
But a gunman certainly can.

00:11:00.870 --> 00:11:03.110
So we have to reject
this idea of neutrality.

00:11:03.110 --> 00:11:06.410
We have to accept the inevitable
social, political, and ethical

00:11:06.410 --> 00:11:07.490
outcomes of our work.

00:11:10.190 --> 00:11:12.160
The clear ethical risk
in all this, of course,

00:11:12.160 --> 00:11:14.890
is that we eliminate free will.

00:11:14.890 --> 00:11:17.890
That's hyper-effective
persuasion

00:11:17.890 --> 00:11:20.170
diminishes human
agency, which is

00:11:20.170 --> 00:11:22.437
something that lies
at the heart of almost

00:11:22.437 --> 00:11:23.395
every popular dystopia.

00:11:26.230 --> 00:11:28.150
And the power dynamics
of hyper-nudge

00:11:28.150 --> 00:11:30.430
strike me as
particularly worrying.

00:11:30.430 --> 00:11:32.600
Because they have the
advantage of scale,

00:11:32.600 --> 00:11:34.610
they can learn from
millions of people.

00:11:34.610 --> 00:11:37.800
Because they can shift
strategies at will,

00:11:37.800 --> 00:11:40.780
hyper-nudge algorithms are
highly potent but essentially

00:11:40.780 --> 00:11:44.230
invisible, diluted by
the technology stack

00:11:44.230 --> 00:11:46.510
using obfuscated data
flows and operating

00:11:46.510 --> 00:11:48.850
on the thresholds of consent.

00:11:48.850 --> 00:11:52.200
And this creates a
significant power imbalance.

00:11:52.200 --> 00:11:55.110
If we aren't careful, we'll
build systems in a way

00:11:55.110 --> 00:11:56.970
that we won't know
who's persuading us.

00:11:56.970 --> 00:11:58.440
We won't know their intent.

00:11:58.440 --> 00:12:00.510
And we won't know their methods.

00:12:00.510 --> 00:12:03.090
This means that people
will have no recourse

00:12:03.090 --> 00:12:06.270
to collectivize and to contest
these systems because they

00:12:06.270 --> 00:12:08.800
won't even know that
they're operating.

00:12:08.800 --> 00:12:14.050
We will become blind to our
own manipulation, which,

00:12:14.050 --> 00:12:16.720
of course, makes this a
terrifically valuable prospect

00:12:16.720 --> 00:12:18.520
for the high-stakes
games of politics.

00:12:18.520 --> 00:12:21.704
We all know far too well
how this is heading.

00:12:21.704 --> 00:12:23.620
The recent headlines
about Cambridge Analytica

00:12:23.620 --> 00:12:26.800
clearly point in that direction.

00:12:26.800 --> 00:12:29.260
The nature and structure
of connected technology--

00:12:29.260 --> 00:12:30.490
and particularly the web--

00:12:30.490 --> 00:12:33.700
already privileges
certain belief systems.

00:12:33.700 --> 00:12:36.510
Hypertext has a way
of breaking down

00:12:36.510 --> 00:12:39.070
centralized linear narratives.

00:12:39.070 --> 00:12:40.880
It encourages,
instead, apophenia,

00:12:40.880 --> 00:12:43.000
the habit of imposing
relationships

00:12:43.000 --> 00:12:44.780
on unconnected things.

00:12:44.780 --> 00:12:46.780
So you could argue that
the structure of the web

00:12:46.780 --> 00:12:49.897
already affords conspiracy.

00:12:49.897 --> 00:12:51.730
Where this gets
particularly alarming for me

00:12:51.730 --> 00:12:55.450
is if we combine that tendency
with evidence collapse,

00:12:55.450 --> 00:12:58.640
with falsifiable
audio and video.

00:12:58.640 --> 00:13:01.000
So we can have soon
an AI that knows

00:13:01.000 --> 00:13:03.130
how to manipulate
us individually,

00:13:03.130 --> 00:13:05.860
and that can generate the
evidence required on demand

00:13:05.860 --> 00:13:07.350
to support that manipulation.

00:13:07.350 --> 00:13:10.104
It's a particularly
scary prospect.

00:13:10.104 --> 00:13:11.020
Now, we might counter.

00:13:11.020 --> 00:13:13.870
We can say we can build
trust-based technologies,

00:13:13.870 --> 00:13:16.570
things like cryptography
and blockchain technologies.

00:13:16.570 --> 00:13:19.960
And you know, maybe that will
help alleviate that situation.

00:13:19.960 --> 00:13:22.187
But these are
complex technologies.

00:13:22.187 --> 00:13:24.520
We may end up, very likely,
in a situation where they're

00:13:24.520 --> 00:13:28.200
available to only the rich or
the highly digitally literate,

00:13:28.200 --> 00:13:31.850
and information anarchy
still reigns for the rest.

00:13:31.850 --> 00:13:34.700
As always, what the
superintelligent and powerful

00:13:34.700 --> 00:13:38.230
do with AI is a bigger threat
than powerful superintelligent

00:13:38.230 --> 00:13:38.730
AI.

00:13:43.120 --> 00:13:45.046
So what can we do about this?

00:13:45.046 --> 00:13:46.420
It's all very well
to acknowledge

00:13:46.420 --> 00:13:47.860
these issues, of course.

00:13:47.860 --> 00:13:49.780
But I think we have
a bit of a moral duty

00:13:49.780 --> 00:13:51.470
to try and mitigate them.

00:13:51.470 --> 00:13:53.636
And I'll suggest
just four steps here.

00:13:53.636 --> 00:13:56.260
First-- and again, I think this
is something that's been hinted

00:13:56.260 --> 00:13:58.240
at throughout the day--

00:13:58.240 --> 00:14:00.520
we need to materialize
the invisible.

00:14:00.520 --> 00:14:03.040
So many of tech's
ethical dangers

00:14:03.040 --> 00:14:06.260
stem from the invisibility
of these spaces,

00:14:06.260 --> 00:14:09.130
be that intentional
or otherwise.

00:14:09.130 --> 00:14:12.400
By shifting persuasive acts
into the visible spectrum,

00:14:12.400 --> 00:14:15.640
people can make more informed
choices about what's happening.

00:14:15.640 --> 00:14:17.740
And this is already
happening in other areas.

00:14:17.740 --> 00:14:21.310
GDPR is going to force a lot
of data flows to surface.

00:14:21.310 --> 00:14:24.670
The Explainable AI movement
will illuminate the reasoning

00:14:24.670 --> 00:14:27.970
for algorithmic decisions.

00:14:27.970 --> 00:14:32.544
People may argue that this might
make persuasion less effective.

00:14:32.544 --> 00:14:33.460
Frankly, I don't care.

00:14:33.460 --> 00:14:34.900
I think the power
of these systems

00:14:34.900 --> 00:14:39.125
is potentially too great for
us to be too concerned by that.

00:14:39.125 --> 00:14:41.500
And note when I'm talking
about materializing this stuff,

00:14:41.500 --> 00:14:43.960
I don't necessarily
mean increasing

00:14:43.960 --> 00:14:45.610
the complexity of our systems.

00:14:45.610 --> 00:14:48.520
I'm talking about making
information available rather

00:14:48.520 --> 00:14:52.330
than mandatory, creating
extra screens or interfaces

00:14:52.330 --> 00:14:54.607
or affordances that
people can interrogate

00:14:54.607 --> 00:14:55.690
if they want to know more.

00:14:58.630 --> 00:15:01.742
There's also a host of ethical
theory we can bring to bear.

00:15:01.742 --> 00:15:03.450
I've only got time,
really, for one here.

00:15:03.450 --> 00:15:06.260
This comes from John
Rawls in his book,

00:15:06.260 --> 00:15:07.770
"A Theory of Justice."

00:15:07.770 --> 00:15:10.710
Rawls talks about the veil
of ignorance, the idea

00:15:10.710 --> 00:15:12.390
that we should
design a system as

00:15:12.390 --> 00:15:16.180
if we don't know our eventual
place within that system.

00:15:16.180 --> 00:15:18.570
So if I'm designing,
say, a welfare system,

00:15:18.570 --> 00:15:22.500
I should design it such that
no matter where I end up--

00:15:22.500 --> 00:15:25.290
whether I'm an administrator
of that scheme,

00:15:25.290 --> 00:15:27.909
whether I'm a taxpayer, whether
I'm a welfare recipient--

00:15:27.909 --> 00:15:29.700
I should look at that
system and think I've

00:15:29.700 --> 00:15:32.720
got a fair end of that deal.

00:15:32.720 --> 00:15:34.870
I think designers
generally get this.

00:15:34.870 --> 00:15:38.770
This is really a forcing
function on maybe not empathy--

00:15:38.770 --> 00:15:42.507
that misused term-- but at least
getting out of one's own head

00:15:42.507 --> 00:15:44.590
and seeing a problem from
multiple points of view.

00:15:47.982 --> 00:15:49.690
I also think we should
make a distinction

00:15:49.690 --> 00:15:53.160
between an informational
and a structural nudge.

00:15:53.160 --> 00:15:55.660
This idea comes from
Luciano Floridi.

00:15:55.660 --> 00:15:57.680
And he distinguishes
the two in this way.

00:15:57.680 --> 00:15:59.770
He says that a
structural nudge is

00:15:59.770 --> 00:16:03.130
one that changes the nature of
the action available to you.

00:16:03.130 --> 00:16:06.420
So if I run a cafe and I want
people to eat more healthily,

00:16:06.420 --> 00:16:09.340
then if I change the order
in which food is presented

00:16:09.340 --> 00:16:11.530
or I move things to
the back of the store

00:16:11.530 --> 00:16:14.415
or move them high up and so
on, that's a structural change.

00:16:14.415 --> 00:16:16.810
Now, the downside of
the structural change

00:16:16.810 --> 00:16:19.360
is it can lead to almost
forced compliance.

00:16:19.360 --> 00:16:21.670
If I put the snacks
right on the top shelf,

00:16:21.670 --> 00:16:24.160
well, anyone below
6 foot is going

00:16:24.160 --> 00:16:26.150
to struggle to reach them.

00:16:26.150 --> 00:16:28.510
By contrast, an
informational nudge

00:16:28.510 --> 00:16:31.420
only changes the nature of
the information on offer, not

00:16:31.420 --> 00:16:33.255
the action itself.

00:16:33.255 --> 00:16:35.380
So the example here would
be changing the labeling,

00:16:35.380 --> 00:16:37.240
not moving it to a high shelf.

00:16:37.240 --> 00:16:40.000
That generally poses less
of a threat to free will.

00:16:40.000 --> 00:16:43.960
The user is better able
to retain their freedoms.

00:16:43.960 --> 00:16:46.970
And this aligns, generally,
with our social hunches

00:16:46.970 --> 00:16:48.180
about nudging.

00:16:48.180 --> 00:16:51.380
In 2013-- yes, another study--

00:16:51.380 --> 00:16:55.040
people found that they were
more favorable to nudges if they

00:16:55.040 --> 00:16:58.370
used conscious process--
so things like signage--

00:16:58.370 --> 00:17:01.010
than unconscious nudges
like room layouts.

00:17:01.010 --> 00:17:03.140
Again, this relates
to visibility.

00:17:03.140 --> 00:17:05.390
It was easier in those
situations for a user

00:17:05.390 --> 00:17:08.569
to see how the information
environment around them

00:17:08.569 --> 00:17:09.260
had been shaped.

00:17:13.020 --> 00:17:15.480
Finally, I think
technologists have a role

00:17:15.480 --> 00:17:19.050
to create counter-technologies
that contest,

00:17:19.050 --> 00:17:22.750
that protest, and that hamper
overreach where it occurs.

00:17:22.750 --> 00:17:25.710
And this is really analogous
to the rise in ad blockers.

00:17:25.710 --> 00:17:28.590
Maybe we'll see persuasion
blockers, browser warnings

00:17:28.590 --> 00:17:30.180
for this kind of
thing, maybe even

00:17:30.180 --> 00:17:33.480
some sort of nudge
traceroutes to expose

00:17:33.480 --> 00:17:37.484
who's trying to get you to
do what within a system.

00:17:37.484 --> 00:17:39.150
And of course, there
is a potential role

00:17:39.150 --> 00:17:40.829
here for regulation.

00:17:40.829 --> 00:17:42.870
But that will only happen
with continued pressure

00:17:42.870 --> 00:17:46.620
from the industry, from the
public, and the press alike.

00:17:46.620 --> 00:17:48.270
To see this kind
of real change, it

00:17:48.270 --> 00:17:51.570
may take internal pressure
and occasional whistleblowing

00:17:51.570 --> 00:17:54.350
and campaigning.

00:17:54.350 --> 00:17:58.590
I will say that if you
feel comfortable, safe,

00:17:58.590 --> 00:18:02.150
and respected in your
industry, in your work,

00:18:02.150 --> 00:18:03.860
you're in the perfect
position to use up

00:18:03.860 --> 00:18:06.830
some of that goodwill
for ethical change.

00:18:06.830 --> 00:18:09.540
We have to try and continue
this push for change

00:18:09.540 --> 00:18:10.210
in the industry.

00:18:10.210 --> 00:18:13.870
It's exciting to see
over the last 12 months

00:18:13.870 --> 00:18:19.109
the increased focus on
our ethical implication.

00:18:19.109 --> 00:18:20.400
But we need to keep that going.

00:18:20.400 --> 00:18:24.192
I think we're going to need
all the help that we can get.

00:18:24.192 --> 00:18:24.775
I have a book.

00:18:24.775 --> 00:18:26.480
It will be out in September.

00:18:26.480 --> 00:18:29.640
URL is there and I'll be talking
about it a lot on Twitter.

00:18:29.640 --> 00:18:31.520
And thanks for your time.

00:18:31.520 --> 00:18:35.360
[APPLAUSE]

00:18:37.677 --> 00:18:39.760
SPEAKER 2: I think we've
got time for one or two--

00:18:39.760 --> 00:18:41.230
I sort of promised
Cennydd earlier

00:18:41.230 --> 00:18:44.870
we'd skip Q&amp;A for the panel.

00:18:44.870 --> 00:18:46.015
But I'm going to--

00:18:46.015 --> 00:18:47.020
CENNYDD: I ran short.

00:18:47.020 --> 00:18:47.590
It's my own fault.

00:18:47.590 --> 00:18:49.215
SPEAKER 2: I'm going
to-- yes, exactly.

00:18:49.215 --> 00:18:53.420
My ethical responsibilities--
so one or two questions

00:18:53.420 --> 00:18:55.000
for Cennydd, and we'll move to--

00:18:55.000 --> 00:18:55.832
anybody?

00:19:03.017 --> 00:19:03.850
AUDIENCE: Thank you.

00:19:03.850 --> 00:19:07.540
I work in public policy and I
want to really congratulate you

00:19:07.540 --> 00:19:11.470
on this work that you're doing
because it's very important.

00:19:11.470 --> 00:19:14.380
I just want to say, if
I look at the evolution

00:19:14.380 --> 00:19:18.790
of digital technologies, it
went really from a great promise

00:19:18.790 --> 00:19:22.960
to today, in some areas,
an existential threat.

00:19:22.960 --> 00:19:25.540
And what I see here in
the room are incredibly

00:19:25.540 --> 00:19:26.930
well-intentioned people.

00:19:26.930 --> 00:19:29.830
So my question is, how
could it all go so wrong?

00:19:29.830 --> 00:19:33.340
And how can you build
in the community more

00:19:33.340 --> 00:19:35.860
of a sense of sort of
strategic foresight

00:19:35.860 --> 00:19:39.950
in anticipating some of
these challenges better?

00:19:39.950 --> 00:19:41.960
CENNYDD: That's a big question.

00:19:41.960 --> 00:19:42.680
What went wrong?

00:19:45.340 --> 00:19:50.990
A bit of hubris, a
significant lack of diversity.

00:19:50.990 --> 00:19:54.010
I very strongly believe that
diversity is an early warning

00:19:54.010 --> 00:19:56.080
system for ethical harm.

00:19:56.080 --> 00:19:58.000
I recognize it's a
highly politicized issue

00:19:58.000 --> 00:20:00.950
at the moment, but I make no
apologies for bringing it up.

00:20:00.950 --> 00:20:02.830
I think a team
that's homogeneous

00:20:02.830 --> 00:20:05.057
is always going to overstate
the value of their work

00:20:05.057 --> 00:20:06.640
for people like them
and to understate

00:20:06.640 --> 00:20:08.620
the value of the potential
harms of their work

00:20:08.620 --> 00:20:09.661
for people not like them.

00:20:12.424 --> 00:20:14.090
What was the other
part of the question?

00:20:14.090 --> 00:20:14.590
Sorry.

00:20:16.931 --> 00:20:17.430
Oh, yes.

00:20:17.430 --> 00:20:17.970
I can-- yes.

00:20:17.970 --> 00:20:19.620
Essentially, how can we
try and address this?

00:20:19.620 --> 00:20:20.120
Right.

00:20:20.120 --> 00:20:21.240
What are some techniques?

00:20:21.240 --> 00:20:25.320
I'm turning a lot to speculative
design on this, actually.

00:20:25.320 --> 00:20:28.620
There are tools to if
not predict the future

00:20:28.620 --> 00:20:32.040
but to open the
possibility space

00:20:32.040 --> 00:20:33.570
of potential
futures, if you like,

00:20:33.570 --> 00:20:36.510
in sort of highfalutin language.

00:20:36.510 --> 00:20:40.740
I'm fairly convinced now that
speculative and critical design

00:20:40.740 --> 00:20:43.680
is probably the next field that
interaction design is going

00:20:43.680 --> 00:20:46.060
to seize upon and partly ruin.

00:20:46.060 --> 00:20:48.707
But I think there's
something very interesting

00:20:48.707 --> 00:20:49.540
in those techniques.

00:20:49.540 --> 00:20:51.540
And a lot of the work I'm
doing with clients now

00:20:51.540 --> 00:20:55.200
is to use those structured
techniques to just

00:20:55.200 --> 00:20:57.660
scan the horizon a bit better
and to better understand

00:20:57.660 --> 00:20:59.190
unintended consequences.

00:20:59.190 --> 00:21:01.000
And once they're
exposed, then of course,

00:21:01.000 --> 00:21:04.380
you can start them to
try to mitigate them.

00:21:04.380 --> 00:21:05.600
So yeah.

00:21:05.600 --> 00:21:06.744
They're not magic answers.

00:21:06.744 --> 00:21:08.410
Codes of ethics are
a whole other thing.

00:21:08.410 --> 00:21:11.160
Maybe we'll rant about
those a bit later.

00:21:11.160 --> 00:21:14.070
But you know, we aren't the
first people on these shores.

00:21:14.070 --> 00:21:16.740
Academics, people in STS, people
in philosophy of technology

00:21:16.740 --> 00:21:18.584
have been looking
at this for decades.

00:21:18.584 --> 00:21:20.250
And it's about time
we started listening

00:21:20.250 --> 00:21:21.631
to them as a community.

00:21:28.732 --> 00:21:29.940
SPEAKER 2: Shall we panelize?

00:21:29.940 --> 00:21:31.830
SPEAKER 1: Yes, panelize.

00:21:31.830 --> 00:21:33.270
SPEAKER 2: Penalize, panelize.

00:21:33.270 --> 00:21:34.100
SPEAKER 1: Yeah.

00:21:34.100 --> 00:21:34.990
Thank you.

00:21:34.990 --> 00:21:36.683
CENNYDD: Thank you.

00:21:36.683 --> 00:21:38.010
[APPLAUSE]

00:21:38.010 --> 00:21:42.620
SPEAKER 1: So next
we're going to have

00:21:42.620 --> 00:21:50.480
a panel around the opportunities
and challenges of UX and UI,

00:21:50.480 --> 00:21:51.566
and--

