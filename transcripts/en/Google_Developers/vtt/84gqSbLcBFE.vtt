WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.844
[MUSIC PLAYING]

00:00:06.640 --> 00:00:07.447
Welcome back.

00:00:07.447 --> 00:00:09.030
We've covered a lot
of ground already,

00:00:09.030 --> 00:00:12.070
so today I want to review
and reinforce concepts.

00:00:12.070 --> 00:00:14.250
To do that, we'll
explore two things.

00:00:14.250 --> 00:00:16.090
First, we'll code
up a basic pipeline

00:00:16.090 --> 00:00:17.640
for supervised learning.

00:00:17.640 --> 00:00:19.390
I'll show you how
multiple classifiers

00:00:19.390 --> 00:00:21.280
can solve the same problem.

00:00:21.280 --> 00:00:23.200
Next, we'll build up a
little more intuition

00:00:23.200 --> 00:00:25.710
for what it means for an
algorithm to learn something

00:00:25.710 --> 00:00:29.502
from data, because that sounds
kind of magical, but it's not.

00:00:29.502 --> 00:00:31.710
To kick things off, let's
look at a common experiment

00:00:31.710 --> 00:00:33.010
you might want to do.

00:00:33.010 --> 00:00:35.210
Imagine you're building
a spam classifier.

00:00:35.210 --> 00:00:37.510
That's just a function that
labels an incoming email

00:00:37.510 --> 00:00:39.307
as spam or not spam.

00:00:39.307 --> 00:00:41.140
Now, say you've already
collected a data set

00:00:41.140 --> 00:00:42.850
and you're ready
to train a model.

00:00:42.850 --> 00:00:44.460
But before you put
it into production,

00:00:44.460 --> 00:00:46.760
there's a question you
need to answer first--

00:00:46.760 --> 00:00:49.820
how accurate will it be when you
use it to classify emails that

00:00:49.820 --> 00:00:51.740
weren't in your training data?

00:00:51.740 --> 00:00:54.850
As best we can, we want to
verify our models work well

00:00:54.850 --> 00:00:56.490
before we deploy them.

00:00:56.490 --> 00:00:59.290
And we can do an experiment
to help us figure that out.

00:00:59.290 --> 00:01:02.930
One approach is to partition
our data set into two parts.

00:01:02.930 --> 00:01:05.080
We'll call these Train and Test.

00:01:05.080 --> 00:01:07.010
We'll use Train
to train our model

00:01:07.010 --> 00:01:10.380
and Test to see how
accurate it is on new data.

00:01:10.380 --> 00:01:13.890
That's a common pattern, so
let's see how it looks in code.

00:01:13.890 --> 00:01:17.060
To kick things off, let's import
a data set into [? SyKit. ?]

00:01:17.060 --> 00:01:20.020
We'll use Iris again, because
it's handily included.

00:01:20.020 --> 00:01:21.960
Now, we already saw
Iris in episode two.

00:01:21.960 --> 00:01:23.560
But what we haven't
seen before is

00:01:23.560 --> 00:01:26.832
that I'm calling the
features x and the labels y.

00:01:26.832 --> 00:01:28.210
Why is that?

00:01:28.210 --> 00:01:30.670
Well, that's because one
way to think of a classifier

00:01:30.670 --> 00:01:32.230
is as a function.

00:01:32.230 --> 00:01:34.750
At a high level, you can
think of x as the input

00:01:34.750 --> 00:01:36.500
and y as the output.

00:01:36.500 --> 00:01:39.892
I'll talk more about that in
the second half of this episode.

00:01:39.892 --> 00:01:42.350
After we import the data set,
the first thing we want to do

00:01:42.350 --> 00:01:44.590
is partition it
into Train and Test.

00:01:44.590 --> 00:01:46.640
And to do that, we can
import a handy utility,

00:01:46.640 --> 00:01:48.530
and it makes the syntax clear.

00:01:48.530 --> 00:01:50.340
We're taking our
x's and our y's,

00:01:50.340 --> 00:01:52.930
or our features and labels,
and partitioning them

00:01:52.930 --> 00:01:54.450
into two sets.

00:01:54.450 --> 00:01:56.690
X_train and y_train are
the features and labels

00:01:56.690 --> 00:01:57.980
for the training set.

00:01:57.980 --> 00:02:00.630
And X_test and y_test are
the features and labels

00:02:00.630 --> 00:02:02.032
for the testing set.

00:02:02.032 --> 00:02:04.240
Here, I'm just saying that
I want half the data to be

00:02:04.240 --> 00:02:05.580
used for testing.

00:02:05.580 --> 00:02:09.229
So if we have 150 examples
in Iris, 75 will be in Train

00:02:09.229 --> 00:02:11.520
and 75 will be in Test.

00:02:11.520 --> 00:02:13.280
Now we'll create our classifier.

00:02:13.280 --> 00:02:14.980
I'll use two
different types here

00:02:14.980 --> 00:02:17.860
to show you how they
accomplish the same task.

00:02:17.860 --> 00:02:20.500
Let's start with the decision
tree we've already seen.

00:02:20.500 --> 00:02:22.240
Note there's only
two lines of code

00:02:22.240 --> 00:02:23.448
that are classifier-specific.

00:02:25.650 --> 00:02:28.830
Now let's train the classifier
using our training data.

00:02:28.830 --> 00:02:31.600
At this point, it's ready
to be used to classify data.

00:02:31.600 --> 00:02:33.330
And next, we'll call
the predict method

00:02:33.330 --> 00:02:35.805
and use it to classify
our testing data.

00:02:35.805 --> 00:02:37.180
If you print out
the predictions,

00:02:37.180 --> 00:02:38.970
you'll see there are
a list of numbers.

00:02:38.970 --> 00:02:40.660
These correspond
to the type of Iris

00:02:40.660 --> 00:02:44.010
the classifier predicts for
each row in the testing data.

00:02:44.010 --> 00:02:46.230
Now let's see how
accurate our classifier

00:02:46.230 --> 00:02:48.280
was on the testing set.

00:02:48.280 --> 00:02:50.840
Recall that up top, we have
the true labels for the testing

00:02:50.840 --> 00:02:51.650
data.

00:02:51.650 --> 00:02:53.460
To calculate our
accuracy, we can

00:02:53.460 --> 00:02:55.760
compare the predicted
labels to the true labels,

00:02:55.760 --> 00:02:57.349
and tally up the score.

00:02:57.349 --> 00:02:59.140
There's a convenience
method in [? Sykit ?]

00:02:59.140 --> 00:03:00.830
we can import to do that.

00:03:00.830 --> 00:03:03.505
Notice here, our
accuracy was over 90%.

00:03:03.505 --> 00:03:06.130
If you try this on your own, it
might be a little bit different

00:03:06.130 --> 00:03:08.270
because of some randomness
in how the Train/Test

00:03:08.270 --> 00:03:10.040
data is partitioned.

00:03:10.040 --> 00:03:11.880
Now, here's something
interesting.

00:03:11.880 --> 00:03:14.690
By replacing these two lines, we
can use a different classifier

00:03:14.690 --> 00:03:16.920
to accomplish the same task.

00:03:16.920 --> 00:03:18.570
Instead of using
a decision tree,

00:03:18.570 --> 00:03:20.930
we'll use one called
[? KNearestNeighbors. ?]

00:03:20.930 --> 00:03:23.340
If we run our experiment,
we'll see that the code

00:03:23.340 --> 00:03:25.354
works in exactly the same way.

00:03:25.354 --> 00:03:27.270
The accuracy may be
different when you run it,

00:03:27.270 --> 00:03:29.800
because this classifier works
a little bit differently

00:03:29.800 --> 00:03:32.440
and because of the randomness
in the Train/Test split.

00:03:32.440 --> 00:03:35.420
Likewise, if we wanted to use a
more sophisticated classifier,

00:03:35.420 --> 00:03:38.220
we could just import it
and change these two lines.

00:03:38.220 --> 00:03:40.297
Otherwise, our code is the same.

00:03:40.297 --> 00:03:42.880
The takeaway here is that while
there are many different types

00:03:42.880 --> 00:03:45.920
of classifiers, at a high level,
they have a similar interface.

00:03:49.059 --> 00:03:50.850
Now let's talk a little
bit more about what

00:03:50.850 --> 00:03:53.120
it means to learn from data.

00:03:53.120 --> 00:03:56.080
Earlier, I said we called the
features x and the labels y,

00:03:56.080 --> 00:03:58.717
because they were the input
and output of a function.

00:03:58.717 --> 00:04:00.800
Now, of course, a function
is something we already

00:04:00.800 --> 00:04:02.190
know from programming.

00:04:02.190 --> 00:04:04.900
def classify--
there's our function.

00:04:04.900 --> 00:04:06.920
As we already know in
supervised learning,

00:04:06.920 --> 00:04:09.060
we don't want to
write this ourselves.

00:04:09.060 --> 00:04:12.360
We want an algorithm to
learn it from training data.

00:04:12.360 --> 00:04:15.240
So what does it mean
to learn a function?

00:04:15.240 --> 00:04:17.120
Well, a function is just
a mapping from input

00:04:17.120 --> 00:04:18.660
to output values.

00:04:18.660 --> 00:04:20.660
Here's a function you
might have seen before-- y

00:04:20.660 --> 00:04:22.700
equals mx plus b.

00:04:22.700 --> 00:04:24.820
That's the equation
for a line, and there

00:04:24.820 --> 00:04:27.340
are two parameters-- m,
which gives the slope;

00:04:27.340 --> 00:04:29.680
and b, which gives
the y-intercept.

00:04:29.680 --> 00:04:31.110
Given these
parameters, of course,

00:04:31.110 --> 00:04:34.320
we can plot the function
for different values of x.

00:04:34.320 --> 00:04:36.610
Now, in supervised learning,
our classified function

00:04:36.610 --> 00:04:38.420
might have some
parameters as well,

00:04:38.420 --> 00:04:41.290
but the input x are the
features for an example we

00:04:41.290 --> 00:04:43.630
want to classify,
and the output y

00:04:43.630 --> 00:04:47.220
is a label, like Spam or Not
Spam, or a type of flower.

00:04:47.220 --> 00:04:49.661
So what could the body of
the function look like?

00:04:49.661 --> 00:04:51.910
Well, that's the part we
want to write algorithmically

00:04:51.910 --> 00:04:53.737
or in other words, learn.

00:04:53.737 --> 00:04:55.320
The important thing
to understand here

00:04:55.320 --> 00:04:57.130
is we're not
starting from scratch

00:04:57.130 --> 00:05:00.060
and pulling the body of the
function out of thin air.

00:05:00.060 --> 00:05:01.990
Instead, we start with a model.

00:05:01.990 --> 00:05:04.050
And you can think of a
model as the prototype for

00:05:04.050 --> 00:05:07.030
or the rules that define
the body of our function.

00:05:07.030 --> 00:05:08.540
Typically, a model
has parameters

00:05:08.540 --> 00:05:10.290
that we can adjust
with our training data.

00:05:10.290 --> 00:05:14.560
And here's a high-level example
of how this process works.

00:05:14.560 --> 00:05:17.380
Let's look at a toy data set and
think about what kind of model

00:05:17.380 --> 00:05:19.210
we could use as a classifier.

00:05:19.210 --> 00:05:20.960
Pretend we're interested
in distinguishing

00:05:20.960 --> 00:05:23.350
between red dots and
green dots, some of which

00:05:23.350 --> 00:05:25.080
I've drawn here on a graph.

00:05:25.080 --> 00:05:27.210
To do that, we'll use
just two features--

00:05:27.210 --> 00:05:29.450
the x- and
y-coordinates of a dot.

00:05:29.450 --> 00:05:32.670
Now let's think about how
we could classify this data.

00:05:32.670 --> 00:05:34.090
We want a function
that considers

00:05:34.090 --> 00:05:35.800
a new dot it's
never seen before,

00:05:35.800 --> 00:05:38.170
and classifies it
as red or green.

00:05:38.170 --> 00:05:40.990
In fact, there might be a lot
of data we want to classify.

00:05:40.990 --> 00:05:42.840
Here, I've drawn
our testing examples

00:05:42.840 --> 00:05:44.960
in light green and light red.

00:05:44.960 --> 00:05:47.210
These are dots that weren't
in our training data.

00:05:47.210 --> 00:05:49.790
The classifier has never
seen them before, so how can

00:05:49.790 --> 00:05:51.700
it predict the right label?

00:05:51.700 --> 00:05:53.820
Well, imagine if we
could somehow draw a line

00:05:53.820 --> 00:05:56.037
across the data like this.

00:05:56.037 --> 00:05:57.620
Then we could say
the dots to the left

00:05:57.620 --> 00:06:00.090
of the line are green and dots
to the right of the line are

00:06:00.090 --> 00:06:00.920
red.

00:06:00.920 --> 00:06:03.430
And this line can serve
as our classifier.

00:06:03.430 --> 00:06:05.610
So how can we learn this line?

00:06:05.610 --> 00:06:08.240
Well, one way is to use
the training data to adjust

00:06:08.240 --> 00:06:09.880
the parameters of a model.

00:06:09.880 --> 00:06:12.830
And let's say the model we
use is a simple straight line

00:06:12.830 --> 00:06:14.460
like we saw before.

00:06:14.460 --> 00:06:17.830
That means we have two
parameters to adjust-- m and b.

00:06:17.830 --> 00:06:21.050
And by changing them, we can
change where the line appears.

00:06:21.050 --> 00:06:23.500
So how could we learn
the right parameters?

00:06:23.500 --> 00:06:25.690
Well, one idea is that
we can iteratively adjust

00:06:25.690 --> 00:06:27.640
them using our training data.

00:06:27.640 --> 00:06:29.890
For example, we might
start with a random line

00:06:29.890 --> 00:06:32.810
and use it to classify the
first training example.

00:06:32.810 --> 00:06:35.370
If it gets it right, we don't
need to change our line,

00:06:35.370 --> 00:06:36.969
so we move on to the next one.

00:06:36.969 --> 00:06:38.760
But on the other hand,
if it gets it wrong,

00:06:38.760 --> 00:06:41.300
we could slightly adjust
the parameters of our model

00:06:41.300 --> 00:06:43.070
to make it more accurate.

00:06:43.070 --> 00:06:44.680
The takeaway here is this.

00:06:44.680 --> 00:06:47.490
One way to think of learning
is using training data

00:06:47.490 --> 00:06:50.980
to adjust the
parameters of a model.

00:06:50.980 --> 00:06:52.950
Now, here's something
really special.

00:06:52.950 --> 00:06:55.270
It's called
tensorflow/playground.

00:06:55.270 --> 00:06:57.370
This is a beautiful
example of a neural network

00:06:57.370 --> 00:07:00.020
you can run and experiment
with right in your browser.

00:07:00.020 --> 00:07:02.060
Now, this deserves its
own episode for sure,

00:07:02.060 --> 00:07:03.730
but for now, go ahead
and play with it.

00:07:03.730 --> 00:07:04.930
It's awesome.

00:07:04.930 --> 00:07:06.630
The playground comes
with different data

00:07:06.630 --> 00:07:08.300
sets you can try out.

00:07:08.300 --> 00:07:09.470
Some are very simple.

00:07:09.470 --> 00:07:12.620
For example, we could use our
line to classify this one.

00:07:12.620 --> 00:07:15.980
Some data sets are
much more complex.

00:07:15.980 --> 00:07:17.620
This data set is
especially hard.

00:07:17.620 --> 00:07:20.357
And see if you can build
a network to classify it.

00:07:20.357 --> 00:07:21.940
Now, you can think
of a neural network

00:07:21.940 --> 00:07:24.170
as a more sophisticated
type of classifier,

00:07:24.170 --> 00:07:26.430
like a decision tree
or a simple line.

00:07:26.430 --> 00:07:29.191
But in principle,
the idea is similar.

00:07:29.191 --> 00:07:29.690
OK.

00:07:29.690 --> 00:07:30.687
Hope that was helpful.

00:07:30.687 --> 00:07:32.520
I just created a Twitter
that you can follow

00:07:32.520 --> 00:07:33.834
to be notified of new episodes.

00:07:33.834 --> 00:07:36.000
And the next one should be
out in a couple of weeks,

00:07:36.000 --> 00:07:38.750
depending on how much work I'm
doing for Google I/O. Thanks,

00:07:38.750 --> 00:07:41.620
as always, for watching,
and I'll see you next time.

