Speaker 1:          00:00          In this video, we'll quickly look at an alternative to a conditional random fields for sequence classification, no one that does the maximum entropy Markov

Speaker 2:          00:10          model.

Speaker 1:          00:12          So we've mainly discussed conditional random fields. They have one advantage, which is that they are, uh, descriptively trained. Uh, and as we've seen before, we should expect that for problem where we have a lot of data, uh, that this is a better approach than training a model. Generatively and, uh, but we haven't really discussed whether there were other discriminative alternatives to the conditional random fields. So let's look at one in this video knowing that the, as the maximum entropy Markov model.

Speaker 2:          00:45          Okay.

Speaker 1:          00:45          So the maximum entropy Markov model or m e m m is a different from the conditioner and feel for a one main reason, which is that it is directed. Uh, so, uh, we see here and illustration of the mem as a Bayesian network, which means that now we have random variables that aren't our nodes in the network. And, uh, the edges between the nodes are actually directed and the direction of the edges, uh, essentially the straight, the generative story associated behind the model. So in, in and a m e m m d a way we assume that the data has been generated is that, uh, given the whole sequence, so an emm emm does not define the way that the input sequence was, uh, was, uh, was generated. So it's a conditional model that does not define a p of X. And a, so given some input sequence, it assumes that the sequence of labels was generated from left to right.

Speaker 1:          01:48          So the first sequence, sorry, the first label, why one was generated only from the information in x one and x two. So that's four a, a similar model then the CRF with the context window radius one. So why one was generated and then y two was generated from x one x two and x three and y one. So all of these guys were able to influence what value for the second label was generated. Uh, and then wide three was, uh, then, uh, generated and the value of y three was generated from, again, y two x two x three and x four and so on until we reached the end of the sequence.

Speaker 2:          02:32          Yeah.

Speaker 1:          02:33          So, uh, so it is a mark of, in the sense that, uh, the, uh, distribution of one label here is a dependent also on the label, uh, previously, um, and the, uh, transition. So it's a mark of model similarly to a hidden Markov model. Uh, but the difference is that in the hidden Markov model, we would have arrows going down like this from the labels to, uh, the, uh, input. We're asked for a maximum entropy Markov model. The arrows are this way instead. So we don't actually define the distribution over the inputs and we just defined it conditional distribution over the labels. And the conditional story goes from, you know, we assume that there was some generative process for the old inputs and then the sequence which generated instead of the other way around. And so more specifically, the probability of a sequence of label, why given the sequence of input x is going to be this product of conditional probabilities over why k given the previous label, why k minus one and some information in X. So, uh, in the case here, sometimes, uh, you know, for why one, it was only x one and x two that, uh, influenced the probability of y one.

Speaker 1:          03:51          But in general it could depend on the whole sequence or a, so I just use a general form here and, uh, what will be used for the conditional distribution, the way they would be factored a would be through a, uh, something very similar to either a logistic regression or a neural network. So you would essentially have a term that would depend similar to our, uh, urinary luck factors that would depend on some information in the input. And then plus something similar to a pairwise factor where, um, given the value at the previous step in the sequence came on as one, this, uh, we could express a preference for what the value we think would be most likely at Viking. And then we will normalize this. And now the normalization constant here would depend not just on x, but also on the value of Y K minus one. So the novelization constant here, that would be the sum over the numerator, uh, uh, over all the potential values for just why king. So here we don't have a partition function over the where we have a summation over all sequences. We have only a sum over the value of the label for the current position for the, the given conditional.

Speaker 1:          05:09          So this is a model that's not too complicated to train, uh, but it has one problem which is known as the label bias problem. So, uh, essentially intuitively when it means is that the observations that are far away in the sequence will not impact predictions that we would be making, uh, early in the sequence. And this could be, this can be illustrated in this example here where we actually have, and we can show that the marginal probability of the third label, given the whole sequence actually only depends, it can be reduced to the conditional probability of y three, given only the first four elements in the sequence. So I in particular, because I have a context window that goes on the up to here, well then this, um, uh, this label here and this label here, it can be marginalized out. So, uh, if I wanted, so I want to know what's the conditional probability of throwing some mink of y three given x one x two x three x four x five that to do this and we need to marginalize over these labels here and these labels here.

Speaker 1:          06:23          Now if, uh, I can, if I marginalize those out that I can't really reduce anything because these depend on these, then they will, uh, they will, uh, have an impact on why three through, why one? However, from this, because, uh, the arrows is directed like this and like that, if I marginalize over why fi, uh, then, uh, it's actually equivalent to just removing that note. Uh, and that's because if I write out the full expression for the probable the a of everything including Wifi, then this marginalizes into a conditional that only involves wifi. And, uh, this term actually needs to sum to one. So I can just, uh, some, some it out, uh, some that turnout. And then if I do the same thing with why for a, so now I have is essentially I have a simpler graphical model that does not include Wifi and then they can also integrate out why four because now it's the last in my sort of hierarchy or random variable is the last one which doesn't have any descendants now.

Speaker 1:          07:35          And so I can also integrate it out, which means that this would yield a simpler graph, my bad. And so now we see that [inaudible] is not connected anymore to y three and a, so for that reason we have that y three given x is actually just a, sorry, p of y three given this is just pure y three given x one, two x four. So I'm going fairly quickly over this. Uh, um, those that are familiar with directed graphical model might see why this is not, you can look at a machine learning textbook on directed graphical models. Uh, but that's, you know, the main takeaway message here is that maximum entropy Markov models, while they might be easier to train, and I'm just stating this, I haven't really described why, but they're actually fairly simple to train. Uh, they have a problem, which is that predictions in early positions in the sequence are actually not dependent on observations further away in the sequence. And so they're not, in a sense, fully leveraging all of the information that might be helpful for predicting a label at a given position.