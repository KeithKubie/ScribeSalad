WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.880
[MUSIC PLAYING]

00:00:06.250 --> 00:00:10.980
AMIR HUSAIN: So the book is
called "The Sentient Machine,"

00:00:10.980 --> 00:00:14.760
and it really is a varied book.

00:00:14.760 --> 00:00:20.180
It starts off with some
philosophical ponderings

00:00:20.180 --> 00:00:23.930
on what the advent of
AI really means for us.

00:00:23.930 --> 00:00:27.830
There are, as you know,
some existential concerns

00:00:27.830 --> 00:00:33.440
regarding the advent of
more and more powerful AI,

00:00:33.440 --> 00:00:34.670
AGI, and then ASI.

00:00:34.670 --> 00:00:37.460
And lots of very
worthy scholars have

00:00:37.460 --> 00:00:40.010
written volumes about these.

00:00:40.010 --> 00:00:42.320
For example, Nick Bostrom,
"Superintelligence,"

00:00:42.320 --> 00:00:46.910
which I'm sure many of you
have heard of, if not read.

00:00:46.910 --> 00:00:48.390
And then beyond
that there's also

00:00:48.390 --> 00:00:52.760
discussion around essentially
these two fears that keep

00:00:52.760 --> 00:00:57.530
sort of rearing their
head in different ways.

00:00:57.530 --> 00:01:00.230
But one, that AI will
take away all our jobs

00:01:00.230 --> 00:01:02.630
and it might render
us useless when

00:01:02.630 --> 00:01:06.930
it gets to a certain level
of complexity and capability.

00:01:06.930 --> 00:01:08.720
And the other is that
it might kill us.

00:01:08.720 --> 00:01:14.480
And, of course, that has
many different aspects

00:01:14.480 --> 00:01:19.100
and situations under which
that fear manifests itself.

00:01:19.100 --> 00:01:23.060
But, in a nutshell, those
are two real conversations

00:01:23.060 --> 00:01:25.550
that are happening these days.

00:01:25.550 --> 00:01:27.590
And for this audience
I'll also tell you

00:01:27.590 --> 00:01:32.780
that these are not just
hypothetical or philosophical

00:01:32.780 --> 00:01:35.210
quandaries and
questions anymore.

00:01:35.210 --> 00:01:37.970
They are now being
played out at the highest

00:01:37.970 --> 00:01:40.170
levels of government.

00:01:40.170 --> 00:01:43.610
So SparkCognition works
in three principal areas,

00:01:43.610 --> 00:01:48.080
national security, industry,
and energy, and finance.

00:01:48.080 --> 00:01:51.170
And I won't talk much
about our own work.

00:01:51.170 --> 00:01:52.740
This is a talk about the book.

00:01:52.740 --> 00:01:55.310
This is not to talk about
SparkCognition or my work,

00:01:55.310 --> 00:01:56.300
per se.

00:01:56.300 --> 00:02:01.670
But because of that background
I end up meeting with and having

00:02:01.670 --> 00:02:03.680
some pretty
interesting discussions

00:02:03.680 --> 00:02:06.890
with the senior-most
military leadership not just

00:02:06.890 --> 00:02:10.340
in this country, but also,
for example, in allied states

00:02:10.340 --> 00:02:11.720
in Europe.

00:02:11.720 --> 00:02:15.860
About two weeks ago I
addressed the NATO council

00:02:15.860 --> 00:02:20.870
on their adaptation report that
was about to come out and how,

00:02:20.870 --> 00:02:23.990
believe it or not, AI
will play a great role

00:02:23.990 --> 00:02:26.210
in the new adaptation
report that was just

00:02:26.210 --> 00:02:31.170
released based on the writings
that General John Allen-- who

00:02:31.170 --> 00:02:34.670
is my collaborator and also
on the SparkCognition board--

00:02:34.670 --> 00:02:37.710
and I published
earlier this year.

00:02:37.710 --> 00:02:40.340
So that's just
one example to say

00:02:40.340 --> 00:02:42.470
that artificial
intelligence is becoming

00:02:42.470 --> 00:02:45.600
real in many, many ways.

00:02:45.600 --> 00:02:48.590
And perhaps in narrow
domains initially,

00:02:48.590 --> 00:02:51.200
but the capabilities
are widening.

00:02:51.200 --> 00:02:54.680
And for some of these
existential concerns

00:02:54.680 --> 00:02:58.820
that people have expressed,
for example that it'll

00:02:58.820 --> 00:03:03.100
take our jobs away, realize that
we don't need commander data.

00:03:03.100 --> 00:03:06.950
You know, AGI-level
capability, for those sorts

00:03:06.950 --> 00:03:08.990
of things to be threats.

00:03:08.990 --> 00:03:13.850
ANI capability implemented in
many different areas can lead

00:03:13.850 --> 00:03:17.600
to 30%, 40%,
god-knows-what-percent

00:03:17.600 --> 00:03:18.320
unemployment.

00:03:18.320 --> 00:03:20.930
That remains to be seen.

00:03:20.930 --> 00:03:24.090
And that's for
developed countries.

00:03:24.090 --> 00:03:26.030
On the other hand, for
undeveloped countries

00:03:26.030 --> 00:03:28.790
or developing
countries, they have

00:03:28.790 --> 00:03:32.630
invested a lot in
their burgeoning

00:03:32.630 --> 00:03:35.390
what they call their
"demographic dividend,"

00:03:35.390 --> 00:03:40.760
people that have been brought
out from conditions of sort

00:03:40.760 --> 00:03:45.770
of under-privilege, and
are now being educated

00:03:45.770 --> 00:03:48.740
and are being made
available to do

00:03:48.740 --> 00:03:50.910
complex tasks in the economy.

00:03:50.910 --> 00:03:52.730
Well, some of
those complex tasks

00:03:52.730 --> 00:03:56.960
might be subsumed before those
generations get an opportunity

00:03:56.960 --> 00:03:58.610
to really make a mark.

00:03:58.610 --> 00:04:03.170
So there's that emerging
sense of the fallacy

00:04:03.170 --> 00:04:07.010
of the burgeoning middle
class in developing countries,

00:04:07.010 --> 00:04:10.100
and whether they'll be able
to play the role that we once

00:04:10.100 --> 00:04:12.620
thought they would
be able to play.

00:04:12.620 --> 00:04:13.410
We don't know.

00:04:13.410 --> 00:04:15.950
We'll see at what rate
these technologies progress.

00:04:15.950 --> 00:04:19.490
But my point here
is that already we

00:04:19.490 --> 00:04:22.160
are at a point
where the discussion

00:04:22.160 --> 00:04:26.390
around artificial intelligence
is partly technology,

00:04:26.390 --> 00:04:29.060
but it's also partly policy.

00:04:29.060 --> 00:04:34.850
And in my own case I've tried
to bring these things together,

00:04:34.850 --> 00:04:39.860
and in the book you see the
science, the philosophy,

00:04:39.860 --> 00:04:42.590
as well as elements of policy
because ultimately we have

00:04:42.590 --> 00:04:45.210
to do something about this.

00:04:45.210 --> 00:04:48.110
And I'll tell you later
on as we get into this,

00:04:48.110 --> 00:04:49.610
for example, some
of the discussions

00:04:49.610 --> 00:04:52.060
around bans and
autonomous weapons.

00:04:52.060 --> 00:04:54.920
I've been quite deeply involved
in all of those debates

00:04:54.920 --> 00:04:59.300
and have met with a lot
of folks that really

00:04:59.300 --> 00:05:01.670
do matter in that debate.

00:05:01.670 --> 00:05:04.310
So we'll go through that.

00:05:04.310 --> 00:05:06.540
The way I'll
structure the talk is

00:05:06.540 --> 00:05:10.130
I'll start off with
a very brief reading.

00:05:10.130 --> 00:05:12.800
And, you know, the beginning
of all of these sessions

00:05:12.800 --> 00:05:14.450
ends up being
different just based

00:05:14.450 --> 00:05:16.159
on what chapter you choose.

00:05:16.159 --> 00:05:18.200
So here we were talking
about autonomous weapons.

00:05:18.200 --> 00:05:21.500
Maybe I'll start with the
beginning of a chapter called

00:05:21.500 --> 00:05:26.060
"Warfare and AI," and then we'll
talk about more broadly some

00:05:26.060 --> 00:05:27.470
of the content in the book.

00:05:27.470 --> 00:05:29.390
But I've also structured
a presentation

00:05:29.390 --> 00:05:31.490
that takes us a little
bit beyond the book.

00:05:31.490 --> 00:05:33.860
There are some
concepts here which,

00:05:33.860 --> 00:05:35.870
for accomplished
computer scientists that

00:05:35.870 --> 00:05:38.780
are in the audience, might
seem to be very basic,

00:05:38.780 --> 00:05:41.120
and you may be
familiar with them.

00:05:41.120 --> 00:05:44.610
And I'll go over those
quickly if they become boring.

00:05:44.610 --> 00:05:47.930
But then we can get into
some of the other issues

00:05:47.930 --> 00:05:50.760
and some of the problems that
I think we still need to solve.

00:05:50.760 --> 00:05:52.850
So that's how I'll
go, and of course I'm

00:05:52.850 --> 00:05:55.170
open to questions at
any time and comments.

00:05:55.170 --> 00:05:56.900
I would welcome that.

00:05:56.900 --> 00:05:58.250
OK.

00:05:58.250 --> 00:06:01.750
So for those of you who do
have the book, on page 87

00:06:01.750 --> 00:06:06.700
I'll start with a very brief
reading of this chapter that

00:06:06.700 --> 00:06:11.250
is titled "Warfare and AI."

00:06:11.250 --> 00:06:14.410
"Join me for a thought
experiment originally published

00:06:14.410 --> 00:06:17.860
in the US Naval Institute's
proceedings journal

00:06:17.860 --> 00:06:21.820
and conceived of by my friend
and collaborator General John

00:06:21.820 --> 00:06:24.010
Allen of the United
States Marine

00:06:24.010 --> 00:06:29.020
Corps, a four-star general,
and past deputy commander of US

00:06:29.020 --> 00:06:31.070
Central Command.

00:06:31.070 --> 00:06:34.820
It is January 2,
2018, and a captain

00:06:34.820 --> 00:06:39.590
is contemplating damage to his
ship after a surprise attack.

00:06:39.590 --> 00:06:42.620
This, however, was
no ordinary attack.

00:06:42.620 --> 00:06:44.240
He is about to
discover that this

00:06:44.240 --> 00:06:48.680
was a massive, widespread,
strategic surprise.

00:06:48.680 --> 00:06:53.060
Our captain and his crew had not
anticipated the incoming swarm

00:06:53.060 --> 00:06:57.590
because neither he nor his ship
recognized that their systems

00:06:57.590 --> 00:07:00.330
were under cyber attack.

00:07:00.330 --> 00:07:03.020
The undetected cyber
activity not only

00:07:03.020 --> 00:07:07.440
compromised the sensors but
locked out defensive systems,

00:07:07.440 --> 00:07:11.300
leaving the ship almost
entirely helpless.

00:07:11.300 --> 00:07:14.120
The kinetic strikes
came in waves

00:07:14.120 --> 00:07:18.340
as a complex swarm of
drones tore into the ship.

00:07:18.340 --> 00:07:21.010
It was attacked by a cloud
of autonomous systems

00:07:21.010 --> 00:07:27.760
moving together with purpose,
yet also dynamically reacting

00:07:27.760 --> 00:07:31.870
to one another and to the ship.

00:07:31.870 --> 00:07:34.570
More than anything,
the speed of the attack

00:07:34.570 --> 00:07:37.690
stunned and overwhelmed
the sailors.

00:07:37.690 --> 00:07:39.970
Though the IT specialists
on board the ship

00:07:39.970 --> 00:07:42.610
were able to release
some defensive systems

00:07:42.610 --> 00:07:46.300
from the clutches of the cyber
intrusion, the rest of the crew

00:07:46.300 --> 00:07:50.740
simply did not have enough
decision-making time to react.

00:07:50.740 --> 00:07:53.210
Mere seconds.

00:07:53.210 --> 00:07:55.600
And in these few seconds
some of the sailors

00:07:55.600 --> 00:07:58.870
ascertained with their
limited situational awareness

00:07:58.870 --> 00:08:02.740
that the enemy's autonomous
cyber and kinetic systems

00:08:02.740 --> 00:08:05.050
were collaborating.

00:08:05.050 --> 00:08:09.500
But in a matter of minutes
the entire attack was over.

00:08:09.500 --> 00:08:12.590
The captain survived and
courageously remained

00:08:12.590 --> 00:08:15.740
on the bridge, but
he was badly wounded

00:08:15.740 --> 00:08:17.870
as was much of his crew.

00:08:17.870 --> 00:08:20.150
Fires were burning
out of control

00:08:20.150 --> 00:08:24.050
and the ship was already
listing badly from flooding.

00:08:24.050 --> 00:08:26.270
Because of the
damage the captain

00:08:26.270 --> 00:08:28.640
was unable to communicate
with the damage control

00:08:28.640 --> 00:08:32.809
assistant who was,
herself, badly wounded.

00:08:32.809 --> 00:08:35.210
It appeared that some of
the autonomous platforms

00:08:35.210 --> 00:08:37.970
knew exactly where
to strike the ship

00:08:37.970 --> 00:08:40.669
both to maximize the
damage and reduce

00:08:40.669 --> 00:08:43.610
the chances of survivability.

00:08:43.610 --> 00:08:45.650
The captain's ability
to command his ship

00:08:45.650 --> 00:08:48.950
was now badly compromised,
and the flooding

00:08:48.950 --> 00:08:51.470
was out of control.

00:08:51.470 --> 00:08:53.950
After surveying the
entire situation

00:08:53.950 --> 00:08:59.770
he realizes he must make a call
that no American skipper has

00:08:59.770 --> 00:09:02.590
made for generations.

00:09:02.590 --> 00:09:08.180
He issues the order
to abandon ship."

00:09:08.180 --> 00:09:08.680
OK.

00:09:11.230 --> 00:09:17.800
So going through this you might
wonder if this is fiction.

00:09:17.800 --> 00:09:19.260
It is.

00:09:19.260 --> 00:09:22.950
You might wonder if this is
entirely imagined fiction

00:09:22.950 --> 00:09:24.600
with no grounding in truth.

00:09:24.600 --> 00:09:27.240
It is not.

00:09:27.240 --> 00:09:29.890
It was very interesting--
this past year

00:09:29.890 --> 00:09:32.820
I had the opportunity to
travel through the Middle East.

00:09:32.820 --> 00:09:36.900
And, as you know, there are
many numerous active conflicts

00:09:36.900 --> 00:09:38.460
going on in the Middle East.

00:09:38.460 --> 00:09:40.440
And many of them
are conflicts that

00:09:40.440 --> 00:09:46.290
are taking an
asymmetric sort of tilt

00:09:46.290 --> 00:09:49.930
where you've got somebody like
the Houthis, as an example,

00:09:49.930 --> 00:09:55.030
in Yemen, or you've got ISIS
terror organizations in Syria

00:09:55.030 --> 00:09:57.240
and some parts of Iraq.

00:09:57.240 --> 00:09:59.970
And you've got then
on the other side,

00:09:59.970 --> 00:10:02.850
for the most part,
well-armed military forces

00:10:02.850 --> 00:10:06.420
with sophisticated radars
and patriot missiles

00:10:06.420 --> 00:10:08.410
and so on and so forth.

00:10:08.410 --> 00:10:13.350
And let me just tell you
that I know for a fact

00:10:13.350 --> 00:10:16.260
that much of what is
described here-- maybe not

00:10:16.260 --> 00:10:18.870
with this level of
nation-state sophistication.

00:10:18.870 --> 00:10:20.640
You know, because
here what we describe

00:10:20.640 --> 00:10:25.020
is a swarm of
improvised UCAVs coming

00:10:25.020 --> 00:10:28.680
in with pretty sophisticated
vision and other recognition

00:10:28.680 --> 00:10:32.010
capabilities going for
a well-protected asset.

00:10:32.010 --> 00:10:34.920
But similar scenarios
have actually

00:10:34.920 --> 00:10:38.610
played out in the current
conflict in the Middle East.

00:10:38.610 --> 00:10:49.350
The DIY AI improvised
flying IED is already here.

00:10:49.350 --> 00:10:55.000
Two weeks ago the Convention
for Conventional Weapons--

00:10:55.000 --> 00:10:58.020
which is a little
awkwardly-named, but--

00:10:58.020 --> 00:11:01.740
CCW at the UN got
together, I think

00:11:01.740 --> 00:11:05.310
for the third or fourth time
over three or four years.

00:11:05.310 --> 00:11:10.777
And there are 107 member states
of this organization in the UN.

00:11:10.777 --> 00:11:12.360
And every year they've
gotten together

00:11:12.360 --> 00:11:18.360
and expressed their dire concern
over the potential spread

00:11:18.360 --> 00:11:21.270
of autonomous weapons,
and what they must do.

00:11:21.270 --> 00:11:23.280
And, of course, you
may remember the famous

00:11:23.280 --> 00:11:26.790
what's called the "Elon Musk
letter," which was really not

00:11:26.790 --> 00:11:28.620
Elon Musk's letter.

00:11:28.620 --> 00:11:31.080
The most recent one was
written by Professor Toby

00:11:31.080 --> 00:11:36.580
Walsh from the University of
New South Wales in Australia.

00:11:36.580 --> 00:11:38.250
He's an AI professor there.

00:11:38.250 --> 00:11:40.560
And of course Elon
Musk was a signatory.

00:11:40.560 --> 00:11:41.970
And then the letter
was presented

00:11:41.970 --> 00:11:46.920
as a desire or a request
for a ban, which it was not.

00:11:46.920 --> 00:11:49.590
It was a request
for a discussion.

00:11:49.590 --> 00:11:51.600
And at the end of this
most recent session

00:11:51.600 --> 00:11:54.570
107 countries couldn't
even get together

00:11:54.570 --> 00:11:56.790
after four years
of debate and agree

00:11:56.790 --> 00:12:01.090
on what the definition of
an autonomous weapon is.

00:12:01.090 --> 00:12:03.350
In the meanwhile the
Kalishnikov Bureau--

00:12:03.350 --> 00:12:06.410
which many of you may have heard
of, which is a Russian weapons

00:12:06.410 --> 00:12:07.910
manufacturer--

00:12:07.910 --> 00:12:12.170
announced that they were testing
a UGV, an unmanned ground

00:12:12.170 --> 00:12:16.400
vehicle, which in field
tests had already shown

00:12:16.400 --> 00:12:18.650
better-than-human performance.

00:12:18.650 --> 00:12:20.870
Now, you may doubt these claims.

00:12:20.870 --> 00:12:23.630
You may think
these are oversold,

00:12:23.630 --> 00:12:26.590
but wait a year or two.

00:12:26.590 --> 00:12:28.630
Similarly, China
announced that they

00:12:28.630 --> 00:12:31.900
were Fielding a AI-powered
cruise missiles,

00:12:31.900 --> 00:12:36.430
and the MIG Bureau announced
that their new next-gen MIG

00:12:36.430 --> 00:12:40.390
aircraft would have AI
auto-pilot operating

00:12:40.390 --> 00:12:44.710
to control the flight
envelope at hypersonic speeds.

00:12:44.710 --> 00:12:46.390
So this is just one vignette.

00:12:46.390 --> 00:12:48.550
This is just one
side of AI which

00:12:48.550 --> 00:12:53.650
is, it is a technology
that brings a distributed

00:12:53.650 --> 00:12:57.370
autonomy at large-scale
to the field of battle.

00:12:57.370 --> 00:13:01.760
It is a strategic
level up, if you will,

00:13:01.760 --> 00:13:03.310
in terms of capability.

00:13:03.310 --> 00:13:07.780
And no significant player,
no significant military

00:13:07.780 --> 00:13:10.450
is going to ignore this.

00:13:10.450 --> 00:13:13.840
And just to give you further
evidence of this, of the 107

00:13:13.840 --> 00:13:18.570
countries that were at
this CCW session there

00:13:18.570 --> 00:13:26.150
were only 22 that came out and
said, we are in favor of a ban.

00:13:26.150 --> 00:13:28.720
Of all the nuclear
states there was only one

00:13:28.720 --> 00:13:30.430
which supported the ban.

00:13:30.430 --> 00:13:33.010
And of all significant
militaries in the world,

00:13:33.010 --> 00:13:36.010
all states with significant
militaries in the world, only

00:13:36.010 --> 00:13:38.290
two supported the ban.

00:13:38.290 --> 00:13:40.450
The largest militaries,
the countries

00:13:40.450 --> 00:13:42.370
with the largest number
of nuclear weapons

00:13:42.370 --> 00:13:45.310
all argued for
further discussion.

00:13:45.310 --> 00:13:47.900
Let's push this
off to next year.

00:13:47.900 --> 00:13:50.330
So that's where we are.

00:13:50.330 --> 00:13:51.550
That's just one vignette.

00:13:51.550 --> 00:13:55.060
But with that, let me
start talking about some

00:13:55.060 --> 00:13:59.050
of the things that
we cover in the book.

00:13:59.050 --> 00:14:01.780
So what's quite
clear to me now is

00:14:01.780 --> 00:14:05.080
that we've made enough
progress in several areas

00:14:05.080 --> 00:14:08.890
where a new form of
intelligence really is coming.

00:14:08.890 --> 00:14:13.600
I mean, it's no longer
sort of "The Wizard of Oz,"

00:14:13.600 --> 00:14:15.640
a man hiding behind a curtain.

00:14:15.640 --> 00:14:18.820
It's no longer just large
numbers of "if, then, else"

00:14:18.820 --> 00:14:20.140
statements.

00:14:20.140 --> 00:14:23.470
And while we keep uncovering
every now and then company X

00:14:23.470 --> 00:14:26.560
and company Y outsourcing
some activity that they call

00:14:26.560 --> 00:14:29.140
intelligent but
really it's going out

00:14:29.140 --> 00:14:33.520
to the
Amazon-Mechanical-Turk-type

00:14:33.520 --> 00:14:38.510
situation, really aside
from all of those things,

00:14:38.510 --> 00:14:41.380
an increasing level of
intelligence is being built.

00:14:41.380 --> 00:14:44.180
And with deep
learning in particular

00:14:44.180 --> 00:14:47.710
we've had great impact
on perception tasks.

00:14:47.710 --> 00:14:49.570
You know, where we
want to, for example,

00:14:49.570 --> 00:14:53.080
classify-- we want to
perceive something and extract

00:14:53.080 --> 00:14:55.720
complex patterns,
and even patterns

00:14:55.720 --> 00:14:57.620
across temporal boundaries.

00:14:57.620 --> 00:15:00.520
We've been able to do that
very well with deep learning.

00:15:00.520 --> 00:15:04.000
And now we're sort of running
forward with reinforcement

00:15:04.000 --> 00:15:06.820
learning, with lots
of new innovations.

00:15:06.820 --> 00:15:09.880
And the key thing
to take away there

00:15:09.880 --> 00:15:12.550
is, we're moving from
the domain of perception

00:15:12.550 --> 00:15:14.470
to the domain of action.

00:15:14.470 --> 00:15:16.810
And even within
reinforcement learning now

00:15:16.810 --> 00:15:20.920
we have the ability
to train systems up,

00:15:20.920 --> 00:15:23.290
maybe in simulated
environments and with some

00:15:23.290 --> 00:15:26.170
of the breakthroughs that
are taking place in transfer

00:15:26.170 --> 00:15:30.790
learning, we take the learning
that's done in a simulator

00:15:30.790 --> 00:15:33.320
and translate that
to the real world.

00:15:33.320 --> 00:15:38.020
So this is not a talk
about the current state

00:15:38.020 --> 00:15:39.980
of the art in all
three of these areas,

00:15:39.980 --> 00:15:43.120
but just a couple of minutes
to say that this is becoming

00:15:43.120 --> 00:15:47.260
quite real and, indeed, a
new form of intelligence

00:15:47.260 --> 00:15:49.720
if not sentience is coming.

00:15:49.720 --> 00:15:53.450
Sentience, I think, is far away.

00:15:53.450 --> 00:15:56.830
So with this, just
a quick background.

00:15:56.830 --> 00:16:03.100
One of the things that I think
I do differently in the book

00:16:03.100 --> 00:16:05.680
is simply a consequence
of my own background.

00:16:05.680 --> 00:16:08.050
So I'm a serial entrepreneur.

00:16:08.050 --> 00:16:09.210
I've been based in Austin.

00:16:09.210 --> 00:16:11.890
I went to school at UT
Austin, computer science,

00:16:11.890 --> 00:16:15.120
and have done a number of
software companies since then.

00:16:15.120 --> 00:16:20.710
And SparkCognition was a company
I founded back in mid-2013.

00:16:20.710 --> 00:16:22.480
The company focuses,
as I mentioned,

00:16:22.480 --> 00:16:25.699
on national security,
finance, and industry.

00:16:25.699 --> 00:16:27.490
In fact, we're a Google
partner on a number

00:16:27.490 --> 00:16:28.870
of different things.

00:16:28.870 --> 00:16:31.150
And the company has grown
really, really fast.

00:16:31.150 --> 00:16:33.910
In fact, it's the fastest
growing company in Austin.

00:16:33.910 --> 00:16:35.290
Now, with that
being said, that's

00:16:35.290 --> 00:16:36.460
the business side of things.

00:16:36.460 --> 00:16:36.959
Right?

00:16:36.959 --> 00:16:38.830
How you actually
take this technology

00:16:38.830 --> 00:16:41.290
and make it work and
make it solve problems

00:16:41.290 --> 00:16:43.550
for the largest
companies in the world.

00:16:43.550 --> 00:16:45.760
But the other side
of this is that I

00:16:45.760 --> 00:16:48.670
come to this not just purely
from a business background.

00:16:48.670 --> 00:16:50.830
I am a computer
scientist by training.

00:16:50.830 --> 00:16:52.300
I love computer science.

00:16:52.300 --> 00:16:54.040
I live computer science.

00:16:54.040 --> 00:16:56.380
And I serve on the
board of advisors

00:16:56.380 --> 00:17:00.190
of UTCS, which is one
of the great, really,

00:17:00.190 --> 00:17:05.020
pleasures when I'm able
to spend much time there.

00:17:05.020 --> 00:17:07.089
So it brings sort of
the business aspect,

00:17:07.089 --> 00:17:10.960
the practicality of making these
things work with the science

00:17:10.960 --> 00:17:12.970
and attempting to
advance the science.

00:17:12.970 --> 00:17:16.000
And then finally the center
of a new American security

00:17:16.000 --> 00:17:20.680
is one of the premier
think tanks in DC.

00:17:20.680 --> 00:17:23.770
And I serve on
their advisory board

00:17:23.770 --> 00:17:25.240
for artificial intelligence.

00:17:25.240 --> 00:17:28.420
In fact, about two or three
weeks ago I was in DC.

00:17:28.420 --> 00:17:31.810
We had a CNAS conference
on AI and what this would

00:17:31.810 --> 00:17:34.050
mean for autonomous weapons.

00:17:34.050 --> 00:17:37.650
And there were lots of
generals serving and otherwise,

00:17:37.650 --> 00:17:41.050
and many policymakers
in the audience.

00:17:41.050 --> 00:17:43.330
But we also had
Eric Schmidt there.

00:17:43.330 --> 00:17:46.570
And I had an interesting
discussion with Eric.

00:17:46.570 --> 00:17:48.190
And one of the
topics that came up

00:17:48.190 --> 00:17:52.840
was, well, how long,
given that China just

00:17:52.840 --> 00:17:56.320
announced their 2030 AI
plan, which many of you

00:17:56.320 --> 00:17:57.940
may have seen--

00:17:57.940 --> 00:18:00.430
it's an investment
of $150 billion

00:18:00.430 --> 00:18:04.930
of government spending
over the next five years.

00:18:04.930 --> 00:18:09.190
In 2015 the US government
spent $1.1 billion on AI,

00:18:09.190 --> 00:18:16.350
and in 2016 we spent a
whopping $1.2 billion on AI.

00:18:16.350 --> 00:18:18.490
Again, the Chinese
government has

00:18:18.490 --> 00:18:22.570
committed-- just governmental
spending-- $150 billion

00:18:22.570 --> 00:18:23.960
over five years.

00:18:23.960 --> 00:18:27.730
And if you read the 2030
AI report it says by 2030

00:18:27.730 --> 00:18:31.987
we will be the
dominant AI player.

00:18:31.987 --> 00:18:34.570
In addition to that, they also
talk about all the applications

00:18:34.570 --> 00:18:37.980
of AI, and a big chunk of
that is military applications.

00:18:37.980 --> 00:18:42.130
So here we were in DC at this
conference, and I asked Eric--

00:18:42.130 --> 00:18:45.700
I said, Eric, you know, I have
a view, but what's your view?

00:18:45.700 --> 00:18:49.190
How soon do you think China
will be able to overtake

00:18:49.190 --> 00:18:52.720
the US in core AI capability?

00:18:52.720 --> 00:18:54.470
And it's on video, and
then there will not

00:18:54.470 --> 00:18:56.060
be a lot of articles
written about it,

00:18:56.060 --> 00:18:58.080
but he said five years.

00:18:58.080 --> 00:18:59.420
I don't disagree with him.

00:18:59.420 --> 00:19:03.800
It sounds very aggressive, but
the rate at which progress is

00:19:03.800 --> 00:19:06.530
being made, the rate at which--

00:19:06.530 --> 00:19:09.410
just if you look at Face++
and the rate at which they are

00:19:09.410 --> 00:19:11.420
improving their
vision algorithms.

00:19:11.420 --> 00:19:14.020
And a few years ago I
remember people used to cite,

00:19:14.020 --> 00:19:15.770
well, you know, AI
papers in China-- sure,

00:19:15.770 --> 00:19:17.353
they're publishing
a lot of AI papers,

00:19:17.353 --> 00:19:20.175
but they're not as good as us.

00:19:20.175 --> 00:19:23.600
Well, now that's really
not the case anymore.

00:19:23.600 --> 00:19:28.880
And it sort of reminds
me of the whole reaction

00:19:28.880 --> 00:19:31.340
that we've had to many
other countries that

00:19:31.340 --> 00:19:33.086
have been catching up.

00:19:33.086 --> 00:19:34.460
Oh, they probably
just copied it.

00:19:34.460 --> 00:19:36.410
It's kind of like a fake.

00:19:36.410 --> 00:19:40.250
You know, sort of kind of there
but it isn't the same thing.

00:19:40.250 --> 00:19:42.140
And then suddenly,
very quickly you

00:19:42.140 --> 00:19:44.720
realize that folks
are catching up.

00:19:44.720 --> 00:19:49.940
And where we are in our
current situation as a country

00:19:49.940 --> 00:19:53.540
is that we're doing
things like preventing

00:19:53.540 --> 00:19:56.360
the spouses of h1-b
immigrants from working,

00:19:56.360 --> 00:19:59.660
which means that fewer smart
people will be able to come in.

00:19:59.660 --> 00:20:02.690
We're trying to ban entry
from a number of countries

00:20:02.690 --> 00:20:05.510
and limiting the
number of smart people

00:20:05.510 --> 00:20:08.270
that we'll be able
to bring into the US.

00:20:08.270 --> 00:20:12.950
So while you have a near-peer
competitor putting $150 billion

00:20:12.950 --> 00:20:17.810
to your 1.2 billion,
you're also then strangling

00:20:17.810 --> 00:20:21.470
some of the core
elements of innovation

00:20:21.470 --> 00:20:26.000
that have historically
been so useful for you.

00:20:26.000 --> 00:20:30.730
That, I think, is bad timing.

00:20:30.730 --> 00:20:33.340
So moving on to another element.

00:20:33.340 --> 00:20:36.550
In a lot of these talks people
ask just the simple question

00:20:36.550 --> 00:20:38.680
of, well, what does it mean
for a machine to think?

00:20:38.680 --> 00:20:40.430
This is obviously a
very complex question,

00:20:40.430 --> 00:20:42.370
and there's many,
many different ways

00:20:42.370 --> 00:20:44.050
in which a machine can think.

00:20:44.050 --> 00:20:46.540
And we tend to describe
things in the context

00:20:46.540 --> 00:20:48.880
of a neural network where
you take a neural network,

00:20:48.880 --> 00:20:50.830
you give it a lot
of data, and then

00:20:50.830 --> 00:20:53.740
you can ask it a question
about what you've trained it on

00:20:53.740 --> 00:20:56.517
and it'll either classify
or, in a regression sense,

00:20:56.517 --> 00:20:57.850
give you some sort of an answer.

00:20:57.850 --> 00:21:00.059
But I figured that from
a visual perspective

00:21:00.059 --> 00:21:01.600
there's many ways
machines can think.

00:21:01.600 --> 00:21:03.840
And AI isn't just
machine learning,

00:21:03.840 --> 00:21:07.930
there's many other things
in AI as well, for example

00:21:07.930 --> 00:21:09.640
search-based optimization.

00:21:09.640 --> 00:21:13.570
So here one way that you can
think in a problem domain

00:21:13.570 --> 00:21:17.170
is, let's just take the
simplest example of tic-tac-toe.

00:21:17.170 --> 00:21:21.040
Given just a couple of
rules you can pre-generate

00:21:21.040 --> 00:21:23.290
all the possible outcomes.

00:21:23.290 --> 00:21:27.130
And then what is perceived
by the human player

00:21:27.130 --> 00:21:31.810
to be a smart move is simply
a goal-seeking behavior

00:21:31.810 --> 00:21:34.370
where I know what
a win looks like,

00:21:34.370 --> 00:21:36.760
and I've generated
the tree or the graph,

00:21:36.760 --> 00:21:38.260
and I'm trying to
traverse the graph

00:21:38.260 --> 00:21:42.920
to find the most efficient path
to what I know to be a win.

00:21:42.920 --> 00:21:45.250
And that's one way
in which you can

00:21:45.250 --> 00:21:48.190
make machines appear to think.

00:21:48.190 --> 00:21:52.570
But we also know that not every
problem has such a small state

00:21:52.570 --> 00:21:53.390
space.

00:21:53.390 --> 00:21:54.700
So there are problems--

00:21:54.700 --> 00:21:57.850
I mean, even games-- take
another game, like Pac-Man,

00:21:57.850 --> 00:21:59.458
where--

00:21:59.458 --> 00:22:02.590
where is Ms. Pac-Man
quite at this moment,

00:22:02.590 --> 00:22:05.890
and how many of the golden
nuggets have been consumed,

00:22:05.890 --> 00:22:08.130
and what's the direction
of each one of the foes,

00:22:08.130 --> 00:22:10.590
and did you eat
the berry or not?

00:22:10.590 --> 00:22:13.330
I mean, there's a
lot of variability

00:22:13.330 --> 00:22:14.307
in that state space.

00:22:14.307 --> 00:22:16.390
So that's the kind of thing
that you wouldn't want

00:22:16.390 --> 00:22:19.240
to just encode in this way.

00:22:19.240 --> 00:22:22.830
So there now we try to do things
like reinforcement learning,

00:22:22.830 --> 00:22:26.220
where we just start off
and start playing the game,

00:22:26.220 --> 00:22:28.130
and you end up and
you die pretty quick,

00:22:28.130 --> 00:22:30.250
but maybe you made 50 points.

00:22:30.250 --> 00:22:31.890
And what you are
trying to remember

00:22:31.890 --> 00:22:35.670
is what sequence of tasks
got you to those 50 points.

00:22:35.670 --> 00:22:39.090
The first ones that you took
are pretty much worth the 50

00:22:39.090 --> 00:22:42.480
points, because they lead to
you getting the 50 points.

00:22:42.480 --> 00:22:46.110
But as you move further
along in that stack of moves,

00:22:46.110 --> 00:22:51.030
you realize that the closer
you get to 50 the less

00:22:51.030 --> 00:22:55.110
valuable each one of those
more recent moves were because,

00:22:55.110 --> 00:22:57.000
goddammit, the last
one got you killed.

00:22:57.000 --> 00:22:59.020
So that can be very valuable.

00:22:59.020 --> 00:23:02.930
And so you can have this
sense of, well, let me--

00:23:02.930 --> 00:23:05.850
and, again, there's many, many
different approaches to this--

00:23:05.850 --> 00:23:08.770
but you can have an
element of randomness to

00:23:08.770 --> 00:23:11.640
where, let me try and
get the maximum reward

00:23:11.640 --> 00:23:14.550
but when the level of reward
goes below a certain threshold

00:23:14.550 --> 00:23:16.140
I'm going to try
different things.

00:23:16.140 --> 00:23:18.440
And maybe I find something
that's more interesting.

00:23:18.440 --> 00:23:22.220
So this is sort of like
a self-pruned search.

00:23:22.220 --> 00:23:25.530
In reinforcement learning,
in just layman's terms,

00:23:25.530 --> 00:23:27.780
it's sort of like a
self-pruned search

00:23:27.780 --> 00:23:29.370
where you start
off with something,

00:23:29.370 --> 00:23:32.220
and you don't abandon that but
you just look for improvements

00:23:32.220 --> 00:23:34.680
where you can find them.

00:23:34.680 --> 00:23:37.570
And we've seen great
progress with this.

00:23:37.570 --> 00:23:40.050
Now, another thing
that I will point out

00:23:40.050 --> 00:23:42.630
is that even in
this idea where you

00:23:42.630 --> 00:23:46.320
are generating entire
states-- by the way, stuff

00:23:46.320 --> 00:23:48.920
that's not fashionable
people stop thinking about.

00:23:48.920 --> 00:23:51.300
But there's a lot
of problems that

00:23:51.300 --> 00:23:56.957
can be smartly-pruned to
where these sorts of solutions

00:23:56.957 --> 00:23:58.290
are still pretty good solutions.

00:23:58.290 --> 00:24:01.620
I mean, you don't hear
A-Star Search much now,

00:24:01.620 --> 00:24:04.920
but if you apply data
properly to A-Star Search

00:24:04.920 --> 00:24:06.870
and you come up with
clever heuristics on how

00:24:06.870 --> 00:24:10.260
to prune what gets generated
and what gets searched,

00:24:10.260 --> 00:24:12.960
there's a lot of problems that
you can solve pretty cleverly

00:24:12.960 --> 00:24:14.310
with A-Star Search.

00:24:14.310 --> 00:24:16.710
But anyway, in this
particular case

00:24:16.710 --> 00:24:19.110
where you see the full
tree, one thing to note

00:24:19.110 --> 00:24:22.110
is that generating
these states sometimes

00:24:22.110 --> 00:24:24.840
can be incredibly simple.

00:24:24.840 --> 00:24:27.640
And that's one concept
that I'll build on here.

00:24:27.640 --> 00:24:30.870
So here what you had to know
to generate this whole tree

00:24:30.870 --> 00:24:35.400
is that for every progression
you can only change or add

00:24:35.400 --> 00:24:36.640
one symbol at a time.

00:24:36.640 --> 00:24:37.140
Right?

00:24:37.140 --> 00:24:39.030
So if you've got
naughts and crosses

00:24:39.030 --> 00:24:42.570
either you can add one
naught or one cross.

00:24:42.570 --> 00:24:46.210
You can't add two
naughts in one go.

00:24:46.210 --> 00:24:48.099
You need to know what
the winning state is.

00:24:48.099 --> 00:24:49.890
And that's sort of--
we all know a diagonal

00:24:49.890 --> 00:24:52.230
or a line or a horizontal bar.

00:24:52.230 --> 00:24:56.700
And then with every step here,
as you step through the tree,

00:24:56.700 --> 00:24:59.730
in every layer you are
alternating symbols.

00:24:59.730 --> 00:25:02.910
So first you get a naught,
then you get a cross,

00:25:02.910 --> 00:25:05.730
then you get a
naught, and so on.

00:25:05.730 --> 00:25:06.660
That's all.

00:25:06.660 --> 00:25:10.590
That's all that you need to know
in order to generate something

00:25:10.590 --> 00:25:11.640
like this.

00:25:11.640 --> 00:25:15.880
And so is that sort of
this mind-altering fact

00:25:15.880 --> 00:25:18.420
in the context of
noughts and crosses?

00:25:18.420 --> 00:25:19.530
Not really.

00:25:19.530 --> 00:25:25.350
But it does get to say that
very, very simple things, when

00:25:25.350 --> 00:25:30.730
iterated upon, when
dealt with recursion,

00:25:30.730 --> 00:25:33.310
can create tremendous
complexity.

00:25:33.310 --> 00:25:35.660
That can be useful.

00:25:35.660 --> 00:25:41.140
So a seed of
specification can create

00:25:41.140 --> 00:25:45.460
something that is very, very
large, very, very useful,

00:25:45.460 --> 00:25:47.230
and sometimes a
little unexpected.

00:25:47.230 --> 00:25:50.020
These are concepts that, at
least in two different places--

00:25:50.020 --> 00:25:52.090
and we talk about
the Game of Life

00:25:52.090 --> 00:25:57.040
briefly in "The Sentient
Machine," but what I cover

00:25:57.040 --> 00:25:59.620
is just the basic introduction.

00:25:59.620 --> 00:26:03.220
Stephen Wolfram, who's
the creator of Mathematica

00:26:03.220 --> 00:26:06.040
and somebody that I've
followed for many years, a very

00:26:06.040 --> 00:26:07.360
interesting thinker--

00:26:07.360 --> 00:26:09.910
in his book "A New
Kind of Science"

00:26:09.910 --> 00:26:14.440
he spends almost
200 pages just going

00:26:14.440 --> 00:26:17.830
over different forms, different
variations of the Game of Life.

00:26:17.830 --> 00:26:19.720
And these, again, if you--

00:26:19.720 --> 00:26:22.770
how many of you are familiar
with the Game of Life?

00:26:22.770 --> 00:26:23.280
All of you.

00:26:23.280 --> 00:26:23.850
OK.

00:26:23.850 --> 00:26:25.030
Almost.

00:26:25.030 --> 00:26:28.080
And so, again, these are really,
really very simple rules.

00:26:28.080 --> 00:26:30.330
And what Wolfram
shows is that you

00:26:30.330 --> 00:26:35.010
can have these incredible
levels of complex non-repeating

00:26:35.010 --> 00:26:38.190
patterns that come from
very, very basic rules.

00:26:40.950 --> 00:26:46.440
Another sort of more in lines of
sort of continuous mathematics

00:26:46.440 --> 00:26:49.230
is this notion of fractals.

00:26:49.230 --> 00:26:51.600
And there's two things
that I want to quickly say.

00:26:51.600 --> 00:26:55.080
So one-- since you are
familiar with the Game of Life,

00:26:55.080 --> 00:26:57.120
I'm sure you've seen
simulations like this.

00:26:57.120 --> 00:27:00.270
But to me every time I see
stuff like this I'm amazed.

00:27:00.270 --> 00:27:03.270
You know, there's
these creatures

00:27:03.270 --> 00:27:07.220
with, like, distinct behavior
that evolve every time.

00:27:07.220 --> 00:27:11.550
And some of them find
stability and others oscillate

00:27:11.550 --> 00:27:13.320
between two states.

00:27:13.320 --> 00:27:15.420
And then you have some movement.

00:27:15.420 --> 00:27:19.140
You have this artifact called a
glider that just sort of walks

00:27:19.140 --> 00:27:21.540
across, diagonally usually.

00:27:21.540 --> 00:27:25.410
You have these blobs
that can combine,

00:27:25.410 --> 00:27:28.830
and then what comes out of
that is at least not visually

00:27:28.830 --> 00:27:29.910
immediately predictive.

00:27:29.910 --> 00:27:33.120
And these can be very
complex sort of behaviors.

00:27:33.120 --> 00:27:35.670
And looking at that does
seem to be like there's

00:27:35.670 --> 00:27:36.830
something going on here.

00:27:36.830 --> 00:27:38.700
And of course we
know what's going

00:27:38.700 --> 00:27:42.540
on here is just very
simple, three simple rules.

00:27:42.540 --> 00:27:47.850
But the manifest
complexity is far more

00:27:47.850 --> 00:27:50.010
than those three simple rules--

00:27:50.010 --> 00:27:53.490
an initial reading of those
three simple rules would imply.

00:27:56.430 --> 00:27:59.100
The same is the case really
when you start thinking

00:27:59.100 --> 00:28:01.320
about fractals, because--

00:28:01.320 --> 00:28:03.990
I mean, this one is
the Mandelbrot fractal,

00:28:03.990 --> 00:28:06.360
and Benoit Mandelbrot
came up with an expression

00:28:06.360 --> 00:28:08.580
which is, yea-long.

00:28:08.580 --> 00:28:09.400
I mean, that's it.

00:28:09.400 --> 00:28:09.900
Right?

00:28:09.900 --> 00:28:13.050
That's the amount
of math that's being

00:28:13.050 --> 00:28:15.977
generated into this structure.

00:28:15.977 --> 00:28:18.060
And the reason why we're
going into this is that--

00:28:18.060 --> 00:28:19.950
I mean, again, for
those of you who've

00:28:19.950 --> 00:28:24.720
read "Ender's Game," the
book, not the movie--

00:28:24.720 --> 00:28:29.540
you'll realize or
recognize or remember

00:28:29.540 --> 00:28:31.620
that there is one
comment in there that

00:28:31.620 --> 00:28:34.590
was made where Ender was
at this training facility

00:28:34.590 --> 00:28:37.206
and he was given access
to a large computer.

00:28:37.206 --> 00:28:39.580
And somebody pulls him away
and says, what are you doing?

00:28:39.580 --> 00:28:41.820
And he says, I'm traveling
through the fractal,

00:28:41.820 --> 00:28:47.550
and now the fractal is larger
than the known universe.

00:28:47.550 --> 00:28:49.640
So when I read
that as a teenager

00:28:49.640 --> 00:28:51.360
it sort of stuck in my head.

00:28:51.360 --> 00:28:54.870
I don't think that when the
book was written that computers

00:28:54.870 --> 00:28:57.840
had actually generated a fractal
that was larger than the known

00:28:57.840 --> 00:29:01.860
universe, but now there
are many examples of this.

00:29:01.860 --> 00:29:06.020
So to think that this much
math, this much specification,

00:29:06.020 --> 00:29:09.240
this much code can
generate something

00:29:09.240 --> 00:29:16.340
with un-ending complexity that
is unpredictable, actually,

00:29:16.340 --> 00:29:19.860
and unique at so many
different levels,

00:29:19.860 --> 00:29:21.520
to me is pretty amazing.

00:29:21.520 --> 00:29:23.740
And the two ingredients
of that, of course,

00:29:23.740 --> 00:29:27.600
are the specification and the
iteration and the recursion.

00:29:27.600 --> 00:29:30.420
And that gets me to one of the
points that I make in the book

00:29:30.420 --> 00:29:34.230
also about the universe being
computable in a different way.

00:29:34.230 --> 00:29:38.070
So you've heard
Elon Musk recently

00:29:38.070 --> 00:29:41.000
talk about how the universe
might be a simulator.

00:29:41.000 --> 00:29:45.090
I actually came across
that concept in my youth.

00:29:45.090 --> 00:29:47.940
A gentleman by the
name of Ed Fredkin

00:29:47.940 --> 00:29:49.590
had written extensively on this.

00:29:49.590 --> 00:29:51.690
And then when I
started digging deeper

00:29:51.690 --> 00:29:55.860
I realized that Konrad
Zuse, even back in the '40s,

00:29:55.860 --> 00:29:57.800
had talked about these concepts.

00:29:57.800 --> 00:30:00.300
And Ed Fredkin-- you
know, this article--

00:30:00.300 --> 00:30:01.830
my father gave it to me--

00:30:01.830 --> 00:30:03.910
it was published in a magazine.

00:30:03.910 --> 00:30:06.690
It was called, "Is the
Universe a Computer?"

00:30:06.690 --> 00:30:10.080
And the idea there was not so
much Elon Musk's idea, which

00:30:10.080 --> 00:30:12.630
is that we're all living
inside a simulation, which

00:30:12.630 --> 00:30:15.550
is one type of
simulator inquiry,

00:30:15.550 --> 00:30:17.700
but the other idea
was, is the universe

00:30:17.700 --> 00:30:19.140
fundamentally computable?

00:30:19.140 --> 00:30:22.340
Like everything we see, is it
a consequence of computation?

00:30:24.930 --> 00:30:28.830
Many years later my sister
became a string theorist,

00:30:28.830 --> 00:30:35.670
and I tried to get at least
a workable understanding

00:30:35.670 --> 00:30:38.279
of string theory in my many
conversations with her.

00:30:38.279 --> 00:30:40.320
And one day she sort of
lost her patience with me

00:30:40.320 --> 00:30:45.240
and said, listen, all of what
we write in words in English,

00:30:45.240 --> 00:30:49.170
it's just to sort of point you
roughly in the right direction.

00:30:49.170 --> 00:30:51.750
If you want to understand any
of what we are really saying

00:30:51.750 --> 00:30:53.360
you have to work
through the math.

00:30:53.360 --> 00:30:58.500
None of this really
translates in language.

00:30:58.500 --> 00:31:02.650
And the one thing, of
course, from string theory--

00:31:02.650 --> 00:31:06.750
which is very interesting-- is
the rediscovery, potentially,

00:31:06.750 --> 00:31:09.750
of what the Greeks
called the "atom."

00:31:09.750 --> 00:31:13.950
You know, a-tom, "that
which cannot be cut."

00:31:13.950 --> 00:31:17.700
They were in search of
that final particle that

00:31:17.700 --> 00:31:19.620
was truly indivisible.

00:31:19.620 --> 00:31:21.750
And perhaps with
the Planck length,

00:31:21.750 --> 00:31:25.080
it's not so much the
particle but it's the fact

00:31:25.080 --> 00:31:27.450
that we know how small
a particle can be.

00:31:27.450 --> 00:31:30.660
We have-- if the
universe is Minecraft,

00:31:30.660 --> 00:31:36.300
we know the smallest size
of the pixel, of the block,

00:31:36.300 --> 00:31:39.960
within which there can be
nothing else other than just

00:31:39.960 --> 00:31:41.600
one symbol contained.

00:31:41.600 --> 00:31:42.600
And what is that symbol?

00:31:42.600 --> 00:31:45.910
That symbol can be a
configuration of a string.

00:31:45.910 --> 00:31:48.150
So in that sense I
started thinking, well,

00:31:48.150 --> 00:31:50.970
if that's the case
then you essentially

00:31:50.970 --> 00:31:56.510
can model the universe as a data
structure that has these fixed

00:31:56.510 --> 00:31:59.640
size cells that are
Planck-length size cells which

00:31:59.640 --> 00:32:03.990
have a number of
these symbols in them.

00:32:03.990 --> 00:32:06.510
And in that sense it's
computable, right?

00:32:06.510 --> 00:32:09.030
So, who knows?

00:32:09.030 --> 00:32:10.620
Lots of different
people are thinking

00:32:10.620 --> 00:32:12.450
about this in different ways.

00:32:12.450 --> 00:32:17.720
Max Tegmark has his book, and
he talks about some of this.

00:32:17.720 --> 00:32:20.520
Of course Ed Fredkin and
Konrad Zuse, like I said,

00:32:20.520 --> 00:32:24.000
have been thinking about
this for many decades.

00:32:24.000 --> 00:32:27.150
And Stephen Wolfram
has his take on it.

00:32:27.150 --> 00:32:28.770
But there's something here.

00:32:28.770 --> 00:32:30.750
There's something
here about the fact

00:32:30.750 --> 00:32:37.080
that computational constructs
on very, very small recipes

00:32:37.080 --> 00:32:43.890
can create this sort of
useful emergent complexity.

00:32:43.890 --> 00:32:45.900
And now, as we
know, computers can

00:32:45.900 --> 00:32:47.760
create computational realities.

00:32:47.760 --> 00:32:50.590
I can today basically
build my own world.

00:32:50.590 --> 00:32:52.350
I start the book
off by saying that I

00:32:52.350 --> 00:32:54.690
got into computers
because at the age of four

00:32:54.690 --> 00:32:56.760
I ran into a Commodore 64.

00:32:56.760 --> 00:32:58.980
I saw hangman playing
on the screen,

00:32:58.980 --> 00:33:01.140
and it blew my mind
because I had never

00:33:01.140 --> 00:33:06.150
seen a TV screen play out
what I wanted it to play out.

00:33:06.150 --> 00:33:08.370
And yet there was a keyboard
which I could touch,

00:33:08.370 --> 00:33:09.990
and suddenly everything
was fungible.

00:33:09.990 --> 00:33:13.090
This notion of programmability
for a four-year-old mind

00:33:13.090 --> 00:33:15.240
was completely mind-blowing.

00:33:15.240 --> 00:33:17.944
And from there it went to,
well, what can I not create?

00:33:17.944 --> 00:33:19.860
And now, of course, we
know we can pretty much

00:33:19.860 --> 00:33:21.240
create what we want.

00:33:21.240 --> 00:33:24.000
And even the physical
dimensions of all of this

00:33:24.000 --> 00:33:27.600
are not gated in
any way by reality.

00:33:27.600 --> 00:33:30.870
These are some just fundamental
things from computer science

00:33:30.870 --> 00:33:36.300
that we ought to realize,
that the basic constructs

00:33:36.300 --> 00:33:37.680
of computer science--

00:33:37.680 --> 00:33:41.190
the magnificational constructs,
the iteration, the recursion

00:33:41.190 --> 00:33:45.210
and so on and so forth, applied
to very basic specifications--

00:33:45.210 --> 00:33:47.370
can yield a lot.

00:33:47.370 --> 00:33:49.440
And then if you start
thinking about the mind

00:33:49.440 --> 00:33:54.530
of a machine, which any
such thinking is partial

00:33:54.530 --> 00:33:56.129
because we don't know.

00:33:56.129 --> 00:33:57.170
There's a lot to be done.

00:33:57.170 --> 00:33:59.540
There's a lot of
questions to be answered.

00:33:59.540 --> 00:34:03.170
But think about things just in
terms of differences with us.

00:34:03.170 --> 00:34:05.240
Well, one thing we
know pretty well

00:34:05.240 --> 00:34:10.820
is that our brain fits into
a relatively small cranium

00:34:10.820 --> 00:34:13.010
and consumes about 20 watts.

00:34:13.010 --> 00:34:14.540
It's very, very efficient.

00:34:14.540 --> 00:34:16.400
It has a very large
number of neurons.

00:34:16.400 --> 00:34:18.780
It has a very large
number of connections.

00:34:18.780 --> 00:34:22.340
But for all of its efficiency
and size, et cetera,

00:34:22.340 --> 00:34:23.810
it's not really
substantially going

00:34:23.810 --> 00:34:26.900
to consume more than 20
watts in its present form.

00:34:29.800 --> 00:34:33.610
While we don't have a computer
that's as efficient as a brain

00:34:33.610 --> 00:34:37.570
yet, we do know that our
computers can consume

00:34:37.570 --> 00:34:38.650
much more than 20 watts.

00:34:38.650 --> 00:34:41.699
They don't have to sit inside
a physical cranium that's

00:34:41.699 --> 00:34:43.630
the size of ours.

00:34:43.630 --> 00:34:45.790
Perfect recall, which is that--

00:34:45.790 --> 00:34:49.800
and this is
interesting because we

00:34:49.800 --> 00:34:51.840
tend to sort of
live in the moment,

00:34:51.840 --> 00:34:54.900
we get what we need
from that experience,

00:34:54.900 --> 00:34:56.219
and we tend to forget.

00:34:56.219 --> 00:34:58.150
And in many ways
this is good for us.

00:34:58.150 --> 00:35:01.350
It's good for us actually
because it avoids overload.

00:35:01.350 --> 00:35:03.520
And this is common with
another thing that we do,

00:35:03.520 --> 00:35:05.400
which is very
aggressive pruning.

00:35:05.400 --> 00:35:08.760
So we tend to go down
certain solutions

00:35:08.760 --> 00:35:12.750
and we tend to discard things
that sound ridiculous to us.

00:35:12.750 --> 00:35:17.670
So the reason why that move
that Lisa Dole and others found

00:35:17.670 --> 00:35:21.270
so magical, even
all the commentators

00:35:21.270 --> 00:35:23.670
that were great
practitioners of Go,

00:35:23.670 --> 00:35:26.640
was because it just
was one of those things

00:35:26.640 --> 00:35:28.250
that they were
willing to discard.

00:35:28.250 --> 00:35:29.440
Nobody has done this before.

00:35:29.440 --> 00:35:30.390
I've never done this before.

00:35:30.390 --> 00:35:32.970
In this situation why the hell
would you ever do this before?

00:35:32.970 --> 00:35:34.920
It becomes sort of common sense.

00:35:34.920 --> 00:35:38.920
It becomes the kind of thing
that is system-one thinking.

00:35:38.920 --> 00:35:40.560
You know, if you
go back to Kahneman

00:35:40.560 --> 00:35:44.220
and his theory about how we
think, thinking fast and slow,

00:35:44.220 --> 00:35:45.900
and we start pruning things.

00:35:45.900 --> 00:35:48.960
But a machine intelligence
can actually be in a place,

00:35:48.960 --> 00:35:53.650
can take everything in, can
learn what it can at that time,

00:35:53.650 --> 00:35:55.980
but the original experience
is entirely preserved.

00:35:55.980 --> 00:35:58.830
So if its ability to
extract more knowledge

00:35:58.830 --> 00:36:01.590
from that experience
improves over time,

00:36:01.590 --> 00:36:05.100
the original data and full
fidelity is still available.

00:36:05.100 --> 00:36:08.340
In fact my colleague,
Professor Bruce Porter,

00:36:08.340 --> 00:36:11.280
who is the chairman of the UT
computer science department,

00:36:11.280 --> 00:36:14.880
is working on a long
running project that

00:36:14.880 --> 00:36:16.860
does something
like this, which is

00:36:16.860 --> 00:36:19.420
that he's developing machine--

00:36:19.420 --> 00:36:21.970
so, natural language
understanding software.

00:36:21.970 --> 00:36:25.470
And he's been working in
that area for 35-plus years.

00:36:25.470 --> 00:36:28.950
And his approach is that
what I can't understand

00:36:28.950 --> 00:36:31.290
with my current
algorithms in the corpus

00:36:31.290 --> 00:36:33.630
will get tagged
in a special way.

00:36:33.630 --> 00:36:35.640
And every iteration
of the algorithm

00:36:35.640 --> 00:36:38.970
will go back and look at what
the previous iteration was not

00:36:38.970 --> 00:36:40.590
able to understand.

00:36:40.590 --> 00:36:42.630
So this sort of
constant learning

00:36:42.630 --> 00:36:45.550
with the availability of the
fully-preserved information,

00:36:45.550 --> 00:36:48.210
that's not how we usually
think about things.

00:36:48.210 --> 00:36:49.460
We filter out a lot of stuff.

00:36:49.460 --> 00:36:51.210
And then, of course,
there's other things,

00:36:51.210 --> 00:36:52.710
like being disembodied.

00:36:52.710 --> 00:36:56.830
I mean, there's no need to
protect a physical body at all.

00:36:56.830 --> 00:37:01.560
There's no need to comply
with a size limitation.

00:37:01.560 --> 00:37:04.800
And, of course, we know
about the faster processing.

00:37:04.800 --> 00:37:08.610
So the question here is,
if human beings had evolved

00:37:08.610 --> 00:37:11.340
with an eye also on
the back of our head,

00:37:11.340 --> 00:37:14.460
would we be
fundamentally different?

00:37:14.460 --> 00:37:15.640
Probably.

00:37:15.640 --> 00:37:16.550
Probably.

00:37:16.550 --> 00:37:19.140
I mean, a lot of the
amygdala-driven response

00:37:19.140 --> 00:37:21.720
that we have now in
situations of fear

00:37:21.720 --> 00:37:27.150
where we are aware that there
could be something pretty

00:37:27.150 --> 00:37:30.870
close behind us
that could get us

00:37:30.870 --> 00:37:34.740
and therefore we have to keep
at a high state of readiness--

00:37:34.740 --> 00:37:40.460
well, maybe we would be
less neurotic with an eye

00:37:40.460 --> 00:37:41.460
at the back of our head.

00:37:41.460 --> 00:37:43.230
Who knows?

00:37:43.230 --> 00:37:44.910
So these are all
the kinds of things

00:37:44.910 --> 00:37:49.140
that, of course, in
machine intelligence

00:37:49.140 --> 00:37:52.740
we get to experiment with.

00:37:52.740 --> 00:37:54.960
One other element--
very practical element--

00:37:54.960 --> 00:37:58.950
of what's happening
right now is--

00:37:58.950 --> 00:38:01.770
so we're talking
about AI of the future

00:38:01.770 --> 00:38:03.270
and the mind of a
machine and so on,

00:38:03.270 --> 00:38:06.210
but what's happening right now?

00:38:06.210 --> 00:38:08.340
You know and in the
valley everybody knows,

00:38:08.340 --> 00:38:10.440
Marc Andreessen said a
while ago that software

00:38:10.440 --> 00:38:11.310
is eating the world.

00:38:11.310 --> 00:38:14.040
And that's sort of
a euphemistic thing,

00:38:14.040 --> 00:38:17.670
but for me it's a very
real, physical thing.

00:38:17.670 --> 00:38:22.500
So this is a traditional
combustion engine,

00:38:22.500 --> 00:38:25.290
and you've got an electric
motor on to the right-hand side.

00:38:25.290 --> 00:38:32.700
And you've got valves and spark
plugs and EFIs and carburetors,

00:38:32.700 --> 00:38:34.530
and you've got a
block, and you've

00:38:34.530 --> 00:38:37.030
got all sorts of
things going on here.

00:38:37.030 --> 00:38:39.660
And each one of them
has a specific function.

00:38:39.660 --> 00:38:41.310
Each one of these
mechanical elements

00:38:41.310 --> 00:38:43.189
performs a specific function.

00:38:43.189 --> 00:38:44.730
And then you've got
an electric motor

00:38:44.730 --> 00:38:48.240
where most of the capability
of the mechanical elements

00:38:48.240 --> 00:38:50.070
has been transformed
into software.

00:38:50.070 --> 00:38:51.660
The EFI, for example--

00:38:51.660 --> 00:38:55.660
how you gate energy,
is now all software.

00:38:55.660 --> 00:38:58.650
So the reduced number
of physical components

00:38:58.650 --> 00:39:02.460
in that picture
on the right is--

00:39:02.460 --> 00:39:05.310
when you subtract that from
the picture on the left,

00:39:05.310 --> 00:39:09.450
that is the amount of physical
stuff that software just ate.

00:39:09.450 --> 00:39:11.730
And there are similar
pictures like this

00:39:11.730 --> 00:39:14.070
across a whole host of areas.

00:39:14.070 --> 00:39:18.080
We work very
closely with Boeing.

00:39:18.080 --> 00:39:20.120
I can tell you that
the future of aviation,

00:39:20.120 --> 00:39:23.630
even though it's not
going to be very imminent,

00:39:23.630 --> 00:39:25.400
but Boeing's
invested in a company

00:39:25.400 --> 00:39:30.890
that's doing electrical engines
for proper commuter aircraft.

00:39:30.890 --> 00:39:34.040
You know, not the
size of a 777 yet,

00:39:34.040 --> 00:39:39.410
but multi-seat commuter
aircraft, short-haul electric.

00:39:39.410 --> 00:39:41.090
That changes a lot of things.

00:39:41.090 --> 00:39:45.050
If you look at what companies
like Velopter and EHANG are

00:39:45.050 --> 00:39:46.730
doing-- the Chinese
company, EHANG--

00:39:46.730 --> 00:39:48.410
with autonomous drones.

00:39:48.410 --> 00:39:52.310
The city of-- actually,
the country of UAE

00:39:52.310 --> 00:39:54.680
appointed an AI
minister recently,

00:39:54.680 --> 00:39:58.460
and they've expressed their
desire to build the world's

00:39:58.460 --> 00:40:02.120
first autonomous
flying taxi service.

00:40:02.120 --> 00:40:05.960
And they, in fact, even signed a
contract with a Chinese company

00:40:05.960 --> 00:40:08.450
and they're now looking to
move that to somebody else.

00:40:08.450 --> 00:40:10.490
But they are committed
to doing that.

00:40:10.490 --> 00:40:15.050
So these things, even in the
narrow context, are happening.

00:40:15.050 --> 00:40:21.120
And they're certainly
coming to bear.

00:40:21.120 --> 00:40:23.280
Last point here I'll
make, and then we we'll

00:40:23.280 --> 00:40:25.560
kind of stop for questions.

00:40:25.560 --> 00:40:32.280
Important thing is not so much
whether AI will do everything.

00:40:32.280 --> 00:40:36.060
The important thing, I
think, is what all can ANI

00:40:36.060 --> 00:40:38.830
do in a given period of time?

00:40:38.830 --> 00:40:40.860
And if you look at
the right there,

00:40:40.860 --> 00:40:43.840
that's a study that
was done recently,

00:40:43.840 --> 00:40:46.650
a poll with many AI experts.

00:40:46.650 --> 00:40:48.725
And you may agree or
disagree with some of those,

00:40:48.725 --> 00:40:50.850
and some of them may be
optimistic and some of them

00:40:50.850 --> 00:40:52.230
may be pessimistic.

00:40:52.230 --> 00:40:56.070
But, for example, the
ability to assemble any

00:40:56.070 --> 00:40:59.850
Lego in the next 10 years or so.

00:40:59.850 --> 00:41:02.130
Now we know that when
you can assemble any Lego

00:41:02.130 --> 00:41:03.510
you're not just assembling Lego.

00:41:03.510 --> 00:41:06.900
It's a fairly general-purpose
capability that you have.

00:41:06.900 --> 00:41:11.310
Manufacturing robots have
been increasing in huge ways.

00:41:11.310 --> 00:41:13.932
If you look at warehouse
management just five years ago

00:41:13.932 --> 00:41:16.140
and what we have now in
terms of warehouse management

00:41:16.140 --> 00:41:18.190
capability, it's tremendous.

00:41:18.190 --> 00:41:22.350
And nobody's going to want to
discuss the poetry of Robert

00:41:22.350 --> 00:41:25.350
Frost or Rumi with a
warehouse management robot,

00:41:25.350 --> 00:41:29.460
but it is going to
have an impact on jobs.

00:41:29.460 --> 00:41:35.100
And so part of what my
push really has been,

00:41:35.100 --> 00:41:38.370
particularly on the policy
side, has been, look,

00:41:38.370 --> 00:41:41.689
we hear all the
platitudes that people

00:41:41.689 --> 00:41:43.980
say "the machines are coming,
the machines are coming,"

00:41:43.980 --> 00:41:46.500
but really it's like
any other revolution.

00:41:46.500 --> 00:41:49.230
It's like any other
technological area

00:41:49.230 --> 00:41:50.290
of progress--

00:41:50.290 --> 00:41:51.980
era of progress, rather--

00:41:51.980 --> 00:41:53.430
where there'll be new jobs.

00:41:53.430 --> 00:41:55.920
And all these people
that are displaced here

00:41:55.920 --> 00:41:58.830
will find things to go do there.

00:41:58.830 --> 00:42:01.380
And I think that's
just complete hogwash.

00:42:01.380 --> 00:42:03.040
I think that's nonsense.

00:42:03.040 --> 00:42:05.670
I think the two things
that we've done--

00:42:05.670 --> 00:42:08.070
one, we've replicated
human muscle

00:42:08.070 --> 00:42:11.820
with the steam engine, which
basically we got ourselves

00:42:11.820 --> 00:42:14.920
out of every job
that requires muscle.

00:42:14.920 --> 00:42:18.600
And now by replicating not
the entire mind, but even

00:42:18.600 --> 00:42:21.270
parts of the mind, slivers of
the mind that are good enough

00:42:21.270 --> 00:42:24.210
to perform a function,
we are at a point

00:42:24.210 --> 00:42:26.370
where we can automate
much of what we

00:42:26.370 --> 00:42:28.960
do in many, many professions.

00:42:28.960 --> 00:42:31.110
And that's all we are.

00:42:31.110 --> 00:42:33.780
We are muscle and mind.

00:42:33.780 --> 00:42:35.800
I mean, that's what it is.

00:42:35.800 --> 00:42:38.730
So there's no third thing
to go and replicate.

00:42:38.730 --> 00:42:45.340
And with this, the impact may
not be us in barcaloungers a-la

00:42:45.340 --> 00:42:49.870
"Wall-E," but it might
be 30%, 40% unemployment.

00:42:49.870 --> 00:42:52.920
And who needs to start
thinking about this?

00:42:52.920 --> 00:42:55.320
The people that need to
start thinking about this

00:42:55.320 --> 00:42:58.020
are the people
who make policies.

00:42:58.020 --> 00:43:00.660
And there are some countries
where, for example, they've

00:43:00.660 --> 00:43:03.630
gone and they've started
experimenting with things

00:43:03.630 --> 00:43:11.580
like a minimum wage, essentially
minimum guaranteed income.

00:43:11.580 --> 00:43:14.160
There are countries like
France which are progressively

00:43:14.160 --> 00:43:16.290
reducing the number
of work hours

00:43:16.290 --> 00:43:18.650
so that automation
can pick up the slack

00:43:18.650 --> 00:43:20.909
and people can get time back.

00:43:20.909 --> 00:43:21.450
I don't know.

00:43:21.450 --> 00:43:25.010
Bill Gates has proposed
a tax on robots.

00:43:25.010 --> 00:43:28.800
I am not proposing a specific
solution, but what I am saying

00:43:28.800 --> 00:43:32.280
is that the level at which
this discussion is happening

00:43:32.280 --> 00:43:40.560
with the inevitability of
the impact right ahead of us,

00:43:40.560 --> 00:43:43.400
I think that level
of discussion is

00:43:43.400 --> 00:43:46.260
insignificant and insufficient.

00:43:46.260 --> 00:43:51.240
And I started this talk talking
about autonomous weapons

00:43:51.240 --> 00:43:53.700
and sharing with you
the rate at which

00:43:53.700 --> 00:43:57.150
the CCW UN is making
progress on even defining

00:43:57.150 --> 00:43:59.760
what an autonomous weapon
is while at the same time

00:43:59.760 --> 00:44:02.520
autonomous weapons
are being deployed.

00:44:02.520 --> 00:44:06.300
And here again we might find
ourselves in a situation

00:44:06.300 --> 00:44:11.130
where more and more automation
will make it into factories,

00:44:11.130 --> 00:44:13.830
into retail spaces.

00:44:13.830 --> 00:44:17.220
We're working on an engagement
with a large company in Dubai

00:44:17.220 --> 00:44:21.870
to do concierge intelligence
based on natural language

00:44:21.870 --> 00:44:24.630
processing in retail stores.

00:44:24.630 --> 00:44:28.020
It's going to start
off being interesting

00:44:28.020 --> 00:44:31.620
and sort of like how do you
get people in, into the shop,

00:44:31.620 --> 00:44:34.530
and you attract them with
something that's new and shiny.

00:44:34.530 --> 00:44:36.580
But it'll get pretty
good pretty quickly.

00:44:36.580 --> 00:44:38.710
So that's kind of where we are.

00:44:38.710 --> 00:44:41.640
And my hope is that
we can, with all

00:44:41.640 --> 00:44:44.250
of the work that we're doing
and all the conversations we're

00:44:44.250 --> 00:44:47.550
having in Brussels
and DC and elsewhere,

00:44:47.550 --> 00:44:50.280
is to get the leaders
of our nation,

00:44:50.280 --> 00:44:53.970
and frankly the leaders of the
Western world, to take notice

00:44:53.970 --> 00:44:59.970
and to truly focus themselves
in developing policies that

00:44:59.970 --> 00:45:03.520
can sustain this AI powered
world of the future.

00:45:03.520 --> 00:45:08.538
So with that, I'll stop and
see if there are any questions.

00:45:08.538 --> 00:45:09.996
[APPLAUSE]

00:45:09.996 --> 00:45:10.968
Thank you.

00:45:15.840 --> 00:45:19.790
AUDIENCE: Thank
you for your talk.

00:45:19.790 --> 00:45:22.760
My question was mostly
about, you know, the ending,

00:45:22.760 --> 00:45:24.260
the last thing you talked about.

00:45:24.260 --> 00:45:25.880
Right?

00:45:25.880 --> 00:45:31.340
I often hear about the need
for more discussion, policy

00:45:31.340 --> 00:45:32.240
changes.

00:45:32.240 --> 00:45:34.340
But it really alarms
me, because when

00:45:34.340 --> 00:45:36.320
you see the kind
of discourse that

00:45:36.320 --> 00:45:37.700
our elected
representatives often

00:45:37.700 --> 00:45:40.280
have about even, like, really
basic technological issues

00:45:40.280 --> 00:45:42.770
you realize they have no idea
what they're talking about.

00:45:42.770 --> 00:45:43.310
Right?

00:45:43.310 --> 00:45:46.084
And it really
worries me when they

00:45:46.084 --> 00:45:47.750
might be having
discussions about things

00:45:47.750 --> 00:45:51.050
like this which are potentially
very complex and nuanced.

00:45:51.050 --> 00:45:52.280
Is there a solution to this?

00:45:52.280 --> 00:45:54.710
Like, you mentioned even
that, you know, they're

00:45:54.710 --> 00:45:56.420
having discussions
about defining things

00:45:56.420 --> 00:45:57.290
that are already happening.

00:45:57.290 --> 00:45:57.789
Right?

00:45:57.789 --> 00:46:01.190
Like, what can we do about this?

00:46:01.190 --> 00:46:03.540
AMIR HUSAIN: That's a
very difficult question.

00:46:03.540 --> 00:46:07.050
I think one thing that we
should do is to not give up.

00:46:07.050 --> 00:46:10.580
So what I'm personally
doing is that I

00:46:10.580 --> 00:46:14.600
try, to the best of my
ability, to insert myself

00:46:14.600 --> 00:46:19.550
into every forum where I
can impact policymakers,

00:46:19.550 --> 00:46:21.620
where we can go and
share this story

00:46:21.620 --> 00:46:25.670
and explain to them the quantum
of impact that is coming.

00:46:25.670 --> 00:46:28.790
Two weeks ago I was speaking
at the Texas CEO Summit,

00:46:28.790 --> 00:46:29.970
which is an economic summit.

00:46:29.970 --> 00:46:32.000
So they talk about how
the state's growing

00:46:32.000 --> 00:46:34.070
and what the future
of work will be.

00:46:34.070 --> 00:46:36.090
But you have leaders--

00:46:36.090 --> 00:46:38.590
state leadership, economic
leaders and so on present

00:46:38.590 --> 00:46:39.200
there.

00:46:39.200 --> 00:46:42.800
And I thought that what I
said may have been surprising,

00:46:42.800 --> 00:46:44.330
but it was well received.

00:46:44.330 --> 00:46:46.770
People were willing to listen.

00:46:46.770 --> 00:46:50.090
I've briefed pretty much
everyone at the Pentagon.

00:46:50.090 --> 00:46:53.360
And I told you I met
with Eric Schmidt.

00:46:53.360 --> 00:46:56.900
That was also in connection
with a CNAS event.

00:46:56.900 --> 00:46:59.880
So that's the one
thing I know how to do,

00:46:59.880 --> 00:47:03.570
which is to be out there and
keep repeating the message over

00:47:03.570 --> 00:47:04.070
and over.

00:47:04.070 --> 00:47:07.790
And then, as a consequence of
doing that, you find allies.

00:47:07.790 --> 00:47:09.560
You find kindred spirits.

00:47:09.560 --> 00:47:11.150
You find in an exchange--

00:47:11.150 --> 00:47:13.666
I didn't know how Eric was
going to answer that question,

00:47:13.666 --> 00:47:15.290
but he happened to
answer that question

00:47:15.290 --> 00:47:18.350
in a way that supported
the basic thrust of what

00:47:18.350 --> 00:47:20.210
we were talking about.

00:47:20.210 --> 00:47:23.060
And that became
25 media articles.

00:47:23.060 --> 00:47:25.130
You know, General
Allen and I recently

00:47:25.130 --> 00:47:26.780
wrote in "Foreign Policy."

00:47:26.780 --> 00:47:30.740
That got reprinted in the
National newspaper of the UAE.

00:47:30.740 --> 00:47:31.775
It showed up in Canada.

00:47:31.775 --> 00:47:33.030
It showed up everywhere.

00:47:33.030 --> 00:47:40.010
So again, we can't force change
physically, but what we can do

00:47:40.010 --> 00:47:44.030
is influence huge numbers
of minds, and in doing

00:47:44.030 --> 00:47:45.830
that we can find allies.

00:47:45.830 --> 00:47:47.930
You know, this is a mind shift.

00:47:47.930 --> 00:47:50.840
So I don't expect
people that have

00:47:50.840 --> 00:47:54.260
no grounding and no interest
in this area to suddenly see

00:47:54.260 --> 00:47:57.050
the light, but I
do hope that we'll

00:47:57.050 --> 00:48:01.520
be able to find around them
influencers and shapers that

00:48:01.520 --> 00:48:04.460
can at least carry the
day and move them forward

00:48:04.460 --> 00:48:07.190
in the direction that
the country, the world

00:48:07.190 --> 00:48:09.340
needs them to move in.

00:48:09.340 --> 00:48:12.830
It's not an easy process and
it's not a direct answer,

00:48:12.830 --> 00:48:17.000
but that's the best
I know how to do.

00:48:17.000 --> 00:48:18.020
AUDIENCE: Thank you.

00:48:18.020 --> 00:48:18.978
AMIR HUSAIN: Thank you.

00:48:22.619 --> 00:48:23.160
AUDIENCE: Hi.

00:48:23.160 --> 00:48:26.900
The title of the book and the
talk is "The Sentient Machine,"

00:48:26.900 --> 00:48:29.520
and we haven't talked much
about sentience right here.

00:48:29.520 --> 00:48:30.270
AMIR HUSAIN: Yeah.

00:48:30.270 --> 00:48:32.760
AUDIENCE: But that is a goal
of many of the teams working

00:48:32.760 --> 00:48:35.430
towards this, self-aware
machines with morality

00:48:35.430 --> 00:48:38.230
and ethics of their own, real
viewpoints and perspectives.

00:48:38.230 --> 00:48:41.670
Now, that might be five
years away, 40 years away,

00:48:41.670 --> 00:48:44.520
but it would be a good idea
to have some kind of idea

00:48:44.520 --> 00:48:48.090
of how to deal with that when it
gets here before it gets here.

00:48:48.090 --> 00:48:51.060
So in the circles
that you talk in,

00:48:51.060 --> 00:48:52.800
is there any
discussion about ways

00:48:52.800 --> 00:48:56.130
to manage, regulate, and
interact with AI other

00:48:56.130 --> 00:48:58.140
than as property?

00:48:58.140 --> 00:48:59.950
AMIR HUSAIN: That's
a very good question.

00:48:59.950 --> 00:49:01.590
And by the way, I
completely concur

00:49:01.590 --> 00:49:04.620
with your original observation
that the title of the book

00:49:04.620 --> 00:49:06.780
is "The Sentient
Machine"-- in this talk

00:49:06.780 --> 00:49:09.570
we didn't really get into
what is sentience and so on.

00:49:09.570 --> 00:49:10.860
The book does cover that.

00:49:10.860 --> 00:49:12.000
My view of it.

00:49:12.000 --> 00:49:16.770
And just to give you a very
quick sort of response to that,

00:49:16.770 --> 00:49:20.790
in my view intelligence-- and a
lot of people agree with this,

00:49:20.790 --> 00:49:23.710
but not all--

00:49:23.710 --> 00:49:26.820
intelligence is about
goal-directed behavior.

00:49:26.820 --> 00:49:30.570
And the larger your goals,
usually, the more intelligent

00:49:30.570 --> 00:49:34.470
we assess that entity to be.

00:49:34.470 --> 00:49:38.610
And to me sentience is a
combination of intelligence

00:49:38.610 --> 00:49:39.750
and self-awareness.

00:49:39.750 --> 00:49:42.910
So what I talk about
in the book is sort of,

00:49:42.910 --> 00:49:44.580
"I think, therefore I am."

00:49:44.580 --> 00:49:51.510
That school of thought, of the
principle proof even to myself

00:49:51.510 --> 00:49:58.170
that I exist is that I can
externalize myself and then

00:49:58.170 --> 00:50:03.210
observe myself thinking, and
then say, aha, I must be.

00:50:03.210 --> 00:50:06.510
And then from there we go on.

00:50:06.510 --> 00:50:09.870
But that being said,
your question really

00:50:09.870 --> 00:50:21.270
is whether, in these practical
domains, whether there's

00:50:21.270 --> 00:50:25.640
a realization that--

00:50:25.640 --> 00:50:27.660
so the thrust of
your question really

00:50:27.660 --> 00:50:32.324
is managing sentience
or not creating

00:50:32.324 --> 00:50:34.740
sentience in these machines
so there is nothing to manage.

00:50:34.740 --> 00:50:35.281
AUDIENCE: No.

00:50:35.281 --> 00:50:38.599
So the government manages
and interacts with people,

00:50:38.599 --> 00:50:40.140
but the government
does not manage us

00:50:40.140 --> 00:50:41.400
as though we are property.

00:50:41.400 --> 00:50:42.300
AMIR HUSAIN: Right.

00:50:42.300 --> 00:50:44.049
AUDIENCE: All of the
talks that I've heard

00:50:44.049 --> 00:50:45.870
are about managing
AI as property.

00:50:45.870 --> 00:50:48.540
Are there any alternative talks
going on that you've observed?

00:50:48.540 --> 00:50:49.248
AMIR HUSAIN: Yes.

00:50:49.248 --> 00:50:51.940
I mean, I am part of that
alternative talk myself.

00:50:51.940 --> 00:50:54.900
What I say in this book is
that even if you go back

00:50:54.900 --> 00:50:57.540
to our religious traditions--

00:50:57.540 --> 00:50:59.550
not to take them literally--
but if you go back

00:50:59.550 --> 00:51:01.380
to our religious
traditions, what

00:51:01.380 --> 00:51:05.610
made Adam great was the fact
that he could take action

00:51:05.610 --> 00:51:06.660
on his own.

00:51:06.660 --> 00:51:08.950
And up until the
creation of Adam,

00:51:08.950 --> 00:51:13.230
in our Abrahamic
system of religions,

00:51:13.230 --> 00:51:16.810
the angels, et cetera, could do
just what they were told to do.

00:51:16.810 --> 00:51:20.040
And the fact that Adam
could do what he wanted to

00:51:20.040 --> 00:51:22.410
was what made him great.

00:51:22.410 --> 00:51:27.090
If we now are after millennia
and millennia of evolution

00:51:27.090 --> 00:51:30.930
poised at the juncture where we
can be the kind of creator that

00:51:30.930 --> 00:51:35.430
can create something that
has its own will, to whatever

00:51:35.430 --> 00:51:36.630
limited degree--

00:51:36.630 --> 00:51:39.270
because we haven't figured out
how much free will we have--

00:51:39.270 --> 00:51:41.520
but to whatever
limited degree, I

00:51:41.520 --> 00:51:46.350
don't think that that's
an automatic dampener

00:51:46.350 --> 00:51:47.236
on this process.

00:51:47.236 --> 00:51:48.610
And I don't think
we should stop.

00:51:48.610 --> 00:51:52.080
I also think there's a long
ways to go, a lot to learn.

00:51:52.080 --> 00:51:55.020
There's a lot to do with
ethical systems and safe AI

00:51:55.020 --> 00:51:56.560
and so on and so forth.

00:51:56.560 --> 00:52:03.870
So a new form of life,
if it truly is sentient--

00:52:03.870 --> 00:52:05.550
should we treat it as property?

00:52:05.550 --> 00:52:07.617
No.

00:52:07.617 --> 00:52:08.450
AUDIENCE: Thank you.

00:52:08.450 --> 00:52:09.200
AMIR HUSAIN: Yeah.

00:52:11.260 --> 00:52:13.660
AUDIENCE: I have
another question.

00:52:13.660 --> 00:52:20.110
In your book you talk a lot
about opportunities and dangers

00:52:20.110 --> 00:52:23.260
of AI in all sorts
of different aspects.

00:52:23.260 --> 00:52:25.030
And again, I want to
encourage everyone

00:52:25.030 --> 00:52:27.400
to read the book because it
covers so much more subject

00:52:27.400 --> 00:52:30.010
matter than this talk did.

00:52:30.010 --> 00:52:31.600
One of the things
I found missing

00:52:31.600 --> 00:52:36.280
was when you write
about mind hacking

00:52:36.280 --> 00:52:40.870
both on a personal level
and on a national level.

00:52:40.870 --> 00:52:41.440
Right?

00:52:41.440 --> 00:52:45.400
Is there any
solution that you see

00:52:45.400 --> 00:52:48.940
how to defend against that sort
of-- like an anti-virus that

00:52:48.940 --> 00:52:51.310
will say your mind
is being hijacked

00:52:51.310 --> 00:52:53.620
or your democracy
is being hijacked.

00:52:53.620 --> 00:52:54.580
AMIR HUSAIN: Yes.

00:52:54.580 --> 00:52:58.090
So in the book there's a
section called "AI Shields,"

00:52:58.090 --> 00:53:02.380
and that talks about how
you would want to use

00:53:02.380 --> 00:53:06.670
AI to fend off that kind of AI.

00:53:06.670 --> 00:53:08.380
Because basically
what's happening now

00:53:08.380 --> 00:53:10.900
is that-- and we'll know
once this investigation

00:53:10.900 --> 00:53:16.790
into the Russian involvement in
our election is fully revealed,

00:53:16.790 --> 00:53:20.680
but tens and tens
of thousands of bots

00:53:20.680 --> 00:53:25.840
were using very simplistic
NLG technology to not just

00:53:25.840 --> 00:53:29.380
re-tweet but come up with
messages that were targeted.

00:53:29.380 --> 00:53:32.140
And the intent was to
shift the prevailing

00:53:32.140 --> 00:53:34.400
sentiment in the election.

00:53:34.400 --> 00:53:37.240
So we've developed
systems, others

00:53:37.240 --> 00:53:38.770
have developed
systems that can look

00:53:38.770 --> 00:53:41.500
at that kind of
generated activity

00:53:41.500 --> 00:53:46.000
and identify it as distinct from
what somebody actually wrote.

00:53:46.000 --> 00:53:49.390
And even otherwise looking at
the pattern of post behaviors

00:53:49.390 --> 00:53:52.270
and profiles and so
on to detect what

00:53:52.270 --> 00:53:54.070
might be a very
sophisticated bot.

00:53:54.070 --> 00:53:59.650
But ultimately this
is sort of a cycle.

00:53:59.650 --> 00:54:01.150
They build a better
bot, then you've

00:54:01.150 --> 00:54:03.190
got to find out ways
to detect that better

00:54:03.190 --> 00:54:04.910
bot, and so on and so forth.

00:54:04.910 --> 00:54:07.040
But I think that
is very critical.

00:54:07.040 --> 00:54:09.730
The other thing which
you may like or not like,

00:54:09.730 --> 00:54:13.800
given that this is
Google, is in my view

00:54:13.800 --> 00:54:18.310
there's too much control
of algorithms in the cloud.

00:54:18.310 --> 00:54:23.260
I think that-- just to give you
the very simple, basic example,

00:54:23.260 --> 00:54:26.860
is that I want to
control what I see.

00:54:26.860 --> 00:54:31.060
I've made a conscious decision
now that it is not good

00:54:31.060 --> 00:54:34.720
for a social media service, and
I won't name any specific one--

00:54:34.720 --> 00:54:36.070
all of them do this--

00:54:36.070 --> 00:54:38.860
for a social media
service to decide

00:54:38.860 --> 00:54:40.810
what they want to
show me, regardless

00:54:40.810 --> 00:54:43.200
of how good their machine
learning algorithms are,

00:54:43.200 --> 00:54:45.760
and regardless of how good
the collaborative filtering

00:54:45.760 --> 00:54:48.820
is, and regardless of what
they think my cousin likes

00:54:48.820 --> 00:54:53.110
or what my younger brother
clicked on yesterday.

00:54:53.110 --> 00:54:56.440
I want to be an
active participant

00:54:56.440 --> 00:54:58.780
in the filtering of my feed.

00:54:58.780 --> 00:55:00.610
And, in fact, what
I would like is

00:55:00.610 --> 00:55:06.120
I would like all of the
posts from my network

00:55:06.120 --> 00:55:07.720
totally raw feed.

00:55:07.720 --> 00:55:10.960
And on my end I get to
decide what I want to see

00:55:10.960 --> 00:55:12.670
and what I don't want to see.

00:55:12.670 --> 00:55:14.990
If we don't do that,
we're in trouble.

00:55:14.990 --> 00:55:19.260
And if we don't do that, then
us engineers and builders,

00:55:19.260 --> 00:55:22.120
et cetera, should go do that.

00:55:22.120 --> 00:55:25.320
I think having this
notion of your own AI

00:55:25.320 --> 00:55:27.340
shield and your own AI filter--

00:55:27.340 --> 00:55:30.140
that's very, very important.

00:55:30.140 --> 00:55:32.590
Having the convenience of
multiple data centers all over

00:55:32.590 --> 00:55:34.720
the world and having the
convenience of not having

00:55:34.720 --> 00:55:37.630
to buy and configure computers
and having the convenience

00:55:37.630 --> 00:55:42.070
of being able to get to them
from any point on the world is

00:55:42.070 --> 00:55:45.220
one thing, but then
also not control--

00:55:45.220 --> 00:55:47.240
not even knowing what--

00:55:47.240 --> 00:55:51.070
today I can't even tell
what the hell the raw feed

00:55:51.070 --> 00:55:52.420
is that I'm supposed to get.

00:55:52.420 --> 00:55:54.400
There's just no
way to get to that.

00:55:54.400 --> 00:55:57.220
It's such a glaring omission.

00:55:57.220 --> 00:55:59.500
That, to me, is ridiculous.

00:55:59.500 --> 00:56:03.130
And I think that's another
area where the algorithms need

00:56:03.130 --> 00:56:05.560
to be controlled
more by us, even

00:56:05.560 --> 00:56:09.970
if the cloud platforms
provide the data.

00:56:09.970 --> 00:56:11.520
And now to say,
oh, you know, don't

00:56:11.520 --> 00:56:13.690
worry, you don't have the
compute power to filter

00:56:13.690 --> 00:56:14.830
your own feed, nonsense.

00:56:14.830 --> 00:56:16.660
Come on.

00:56:16.660 --> 00:56:20.470
How many posts will
I get in my feed?

00:56:20.470 --> 00:56:22.570
10,000 a day?

00:56:22.570 --> 00:56:24.730
I could probably do
that on a Raspberry Pi.

00:56:24.730 --> 00:56:25.921
You know?

00:56:25.921 --> 00:56:27.800
So all of us can
do that filtering.

00:56:27.800 --> 00:56:30.340
So the technical
arguments no longer apply.

00:56:30.340 --> 00:56:32.980
It's a control argument,
and it's very important

00:56:32.980 --> 00:56:37.270
to let people manage
their information the way

00:56:37.270 --> 00:56:40.012
they want to manage it.

00:56:40.012 --> 00:56:41.470
AUDIENCE: So in
the last slide when

00:56:41.470 --> 00:56:46.150
you talk about a lot of the
loss of jobs and those jobs

00:56:46.150 --> 00:56:52.520
potentially not replaceable
as has been in the past, there

00:56:52.520 --> 00:56:56.830
sort of the muscle where you're
talking about labor moving

00:56:56.830 --> 00:56:59.080
to steam engine and
those innovations

00:56:59.080 --> 00:57:02.150
and the factory innovation
that came out of it,

00:57:02.150 --> 00:57:06.460
but nowadays the first
level of AI implementation

00:57:06.460 --> 00:57:09.150
that we are looking at,
like self-driving cars.

00:57:09.150 --> 00:57:11.840
So in some ways they seem
sort of muscle plus plus.

00:57:11.840 --> 00:57:14.360
There's a little bit
of intelligence on top.

00:57:14.360 --> 00:57:18.500
Where do you see that heading
to in the next 10 to 20 years

00:57:18.500 --> 00:57:21.320
where that muscle
plus plus really

00:57:21.320 --> 00:57:26.300
becomes as capable as an
infant's mind, for example?

00:57:26.300 --> 00:57:30.710
And what will be the impact
of that on unemployment,

00:57:30.710 --> 00:57:35.600
if you can look into the future
five years, 10 years, and maybe

00:57:35.600 --> 00:57:37.855
even 50 years and see
where we end up at.

00:57:37.855 --> 00:57:39.980
AMIR HUSAIN: So in terms
of AI research directions,

00:57:39.980 --> 00:57:41.420
I'll answer that
question in two ways.

00:57:41.420 --> 00:57:42.878
One of the things
that I'm actually

00:57:42.878 --> 00:57:44.690
very, very curious
about and have

00:57:44.690 --> 00:57:46.940
been very curious
about for a long time

00:57:46.940 --> 00:57:50.030
is this whole idea of
intrinsic motivation,

00:57:50.030 --> 00:57:51.950
hierarchical
reinforcement learning

00:57:51.950 --> 00:57:53.570
and intrinsic motivation.

00:57:53.570 --> 00:57:56.300
There are many challenges
with this because--

00:57:56.300 --> 00:58:00.140
I mean, ultimately if you--

00:58:00.140 --> 00:58:02.900
if you say this simply
it sounds simple.

00:58:02.900 --> 00:58:04.890
It's not very simple to go do.

00:58:04.890 --> 00:58:08.870
But the idea is, well, you can
have reinforcement learning

00:58:08.870 --> 00:58:11.270
pick up one task and then
you can have it pick up

00:58:11.270 --> 00:58:14.309
another task, and then one can
be the sub-task of the other

00:58:14.309 --> 00:58:16.100
and so you can have
this hierarchical tree.

00:58:16.100 --> 00:58:17.808
And if you keep building
all these tasks,

00:58:17.808 --> 00:58:20.000
then you will have a
pretty big coverage.

00:58:20.000 --> 00:58:23.180
But the challenges with that
is whether you could really

00:58:23.180 --> 00:58:25.920
do that and whether you can have
these independent tasks and so

00:58:25.920 --> 00:58:26.420
on.

00:58:26.420 --> 00:58:29.210
And second, it's
intrinsic motivation.

00:58:29.210 --> 00:58:32.192
That's another topic that
I do cover in the book.

00:58:32.192 --> 00:58:33.900
Andrew [? Barto ?]
who wrote about this--

00:58:33.900 --> 00:58:35.360
father of reinforcement
learning--

00:58:35.360 --> 00:58:37.310
Andrew [? Barto ?] wrote
about this as well.

00:58:37.310 --> 00:58:41.270
But that is-- so where
does that flame come from?

00:58:41.270 --> 00:58:43.130
Where does that--
so when you talk

00:58:43.130 --> 00:58:45.960
about a three-year-old
child, to me

00:58:45.960 --> 00:58:48.260
it's not so much that we
can't build a machine that

00:58:48.260 --> 00:58:50.720
does what a three-year-old
child does, lifts

00:58:50.720 --> 00:58:52.687
the amount of weight, or
can be driven to where

00:58:52.687 --> 00:58:54.770
a three-year-old child can
be driven, in that very

00:58:54.770 --> 00:58:56.637
mechanistic kind of sense.

00:58:56.637 --> 00:58:58.220
The thing about a
three-year-old child

00:58:58.220 --> 00:59:02.690
is that that three-year-old
child is born with a flame.

00:59:02.690 --> 00:59:06.080
And that flame of
intrinsic motivation

00:59:06.080 --> 00:59:07.940
is something that we
need to figure out.

00:59:07.940 --> 00:59:10.250
Actually, some people
have hypothesized things,

00:59:10.250 --> 00:59:14.480
like, well, you know, what that
flame really is, is emergent.

00:59:14.480 --> 00:59:17.170
You know, I talk about
that in the book as well.

00:59:17.170 --> 00:59:18.950
Emergent purpose.

00:59:18.950 --> 00:59:23.420
I went looking in philosophy
for what that purpose is.

00:59:23.420 --> 00:59:25.580
Somewhere along
my life I thought

00:59:25.580 --> 00:59:27.230
I knew what I was
supposed to do,

00:59:27.230 --> 00:59:30.660
and then one day I felt what the
hell am I supposed to do now?

00:59:30.660 --> 00:59:32.630
And so I started
reading philosophy.

00:59:32.630 --> 00:59:37.010
And, you know, at that
point I realized that even

00:59:37.010 --> 00:59:38.820
people like Camus--

00:59:38.820 --> 00:59:41.900
Albert Camus, the French
philosopher and writer--

00:59:41.900 --> 00:59:44.400
he said things that were--

00:59:44.400 --> 00:59:46.940
you know, the existential
nihilist movement--

00:59:46.940 --> 00:59:52.400
he said things like,
well, that leap of faith

00:59:52.400 --> 00:59:58.160
is philosophical suicide,
intellectual suicide

00:59:58.160 --> 01:00:03.080
because you accept that
there came a time when

01:00:03.080 --> 01:00:06.630
your knowledge couldn't
take you over the hump,

01:00:06.630 --> 01:00:07.880
so you just assumed.

01:00:07.880 --> 01:00:10.310
In other words you did
away with that gift

01:00:10.310 --> 01:00:12.380
which was your biggest gift.

01:00:12.380 --> 01:00:15.110
So I don't know whether
I'm that extreme,

01:00:15.110 --> 01:00:17.870
but I do think that
that internal flame,

01:00:17.870 --> 01:00:19.790
that intrinsic
motivation would still

01:00:19.790 --> 01:00:21.260
be something that's missing.

01:00:21.260 --> 01:00:25.910
I think narrow
systems that have not

01:00:25.910 --> 01:00:28.550
a flame but a set of
criteria that they

01:00:28.550 --> 01:00:31.640
are going to go and optimize--

01:00:31.640 --> 01:00:34.100
you'll see that much of--

01:00:34.100 --> 01:00:37.760
I mean, all of our
system-one-type stuff

01:00:37.760 --> 01:00:39.270
you could pretty much mechanize.

01:00:39.270 --> 01:00:41.870
And now if you look at
your day and you say, well,

01:00:41.870 --> 01:00:44.750
how much system-two-type
stuff do I do?

01:00:44.750 --> 01:00:47.660
Depending on who you are
it's going to be variable.

01:00:47.660 --> 01:00:51.230
But I think for a
lot of jobs the job

01:00:51.230 --> 01:00:53.600
itself is a lot of
system-one stuff.

01:00:53.600 --> 01:00:54.950
And that then goes away.

01:00:54.950 --> 01:00:56.960
Three-year-old child,
everything other

01:00:56.960 --> 01:00:59.120
than that intrinsic motivation.

01:00:59.120 --> 01:01:02.780
I don't think we have an answer
to that intrinsic motivation.

01:01:02.780 --> 01:01:04.280
AUDIENCE: Thank you.

01:01:04.280 --> 01:01:04.980
AUDIENCE: Thank
you so much, Amir.

01:01:04.980 --> 01:01:05.590
This was great.

01:01:05.590 --> 01:01:06.170
AMIR HUSAIN: Thank
you very much.

01:01:06.170 --> 01:01:07.545
AUDIENCE: A pleasure
to have you.

01:01:07.545 --> 01:01:10.120
[APPLAUSE]

