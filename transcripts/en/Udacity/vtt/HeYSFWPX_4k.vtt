WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:00.979
Hello Charles.

00:00:00.979 --> 00:00:02.518
&gt;&gt; Hi Michael. How are you doing?

00:00:02.518 --> 00:00:03.987
&gt;&gt; I'm doing okay.

00:00:03.987 --> 00:00:04.979
&gt;&gt; Good.

00:00:04.979 --> 00:00:06.407
&gt;&gt; It's you know, exciting to

00:00:06.407 --> 00:00:08.661
be getting to talk about reinforcement learning.

00:00:08.661 --> 00:00:11.620
&gt;&gt; Mm. Reinforcement learning. It's my favorite type of learning.

00:00:11.620 --> 00:00:12.360
&gt;&gt; Is that true?

00:00:12.360 --> 00:00:13.070
&gt;&gt; It is true.

00:00:13.070 --> 00:00:14.910
&gt;&gt; Wow. I like machine learning.

00:00:14.910 --> 00:00:17.010
&gt;&gt; I like machine learning too. But of all the kinds

00:00:17.010 --> 00:00:20.240
of machine learning, reinforcement learning is my favorite kind of learning. So,

00:00:20.240 --> 00:00:23.360
let's, how bout, how bout, giving the opportunity for the students

00:00:23.360 --> 00:00:25.710
in the class to learn about reinforcement learning by having us tell

00:00:25.710 --> 00:00:26.170
them about it.

00:00:26.170 --> 00:00:27.920
&gt;&gt; Oh, let's do that. You first.

00:00:27.920 --> 00:00:31.190
&gt;&gt; Alright. We're going to build up on the stuff you told us about last time.

00:00:31.190 --> 00:00:32.990
&gt;&gt; You mean the fantastic, well defined, and

00:00:32.990 --> 00:00:34.580
well formalized stuff that we talked about last time?

00:00:34.580 --> 00:00:36.540
&gt;&gt; Yes, it was fantastic and it laid the

00:00:36.540 --> 00:00:38.180
groundwork for what I would like to talk about. So

00:00:38.180 --> 00:00:40.560
you set up Markov decision processes, and I'm going to

00:00:40.560 --> 00:00:42.690
talk about what it means to learn in that context.

00:00:42.690 --> 00:00:43.920
&gt;&gt; Excellent.

00:00:43.920 --> 00:00:45.450
&gt;&gt; I find it useful to start off by

00:00:45.450 --> 00:00:47.090
thinking about a reinforcement learning

00:00:47.090 --> 00:00:49.530
API, like application programmer interface.

00:00:50.620 --> 00:00:54.480
So what you talked about is, is this box here.

00:00:54.480 --> 00:00:56.752
The idea of being able to take a model of

00:00:56.752 --> 00:01:00.231
an MDP, which consists of a transition function, T, and

00:01:00.231 --> 00:01:03.606
a reward function R. And it goes through some code and,

00:01:03.606 --> 00:01:06.415
you know, it comes out and a policy comes out.

00:01:06.415 --> 00:01:09.386
Right? And a policy is, is like pi. It maps states

00:01:09.386 --> 00:01:13.038
to actions. And that whole activity, what would you call

00:01:13.038 --> 00:01:15.949
that? What would you call the, the, the problem of, or

00:01:15.949 --> 00:01:19.770
the approach of taking models and turning them into policies?

00:01:19.770 --> 00:01:21.030
&gt;&gt; Maybe I'd call them planning.

00:01:21.030 --> 00:01:23.112
&gt;&gt; Yeah that's what I, that's, that's what I was hoping you would say.

00:01:23.112 --> 00:01:24.130
&gt;&gt; Mmhm.

00:01:24.130 --> 00:01:25.858
&gt;&gt; Alright, now we're going to talk about

00:01:25.858 --> 00:01:27.548
a different set up here. We're still

00:01:27.548 --> 00:01:30.020
interested in spitting out policies, right? Figuring

00:01:30.020 --> 00:01:31.999
out how to behave to maximize reward. But

00:01:31.999 --> 00:01:34.194
a learner's going to do something different. Instead

00:01:34.194 --> 00:01:35.538
of taking a model as input, it's

00:01:35.538 --> 00:01:38.026
going to take transitions. It's going to take samples

00:01:38.026 --> 00:01:40.666
of, being in some state, taking some action,

00:01:40.666 --> 00:01:43.438
observing a reward and observing the state that is

00:01:43.438 --> 00:01:46.279
at the other end of that transition. Alright? And

00:01:46.279 --> 00:01:47.643
we'll, I put a little star on that to

00:01:47.643 --> 00:01:49.790
say well we're going to see a bunch of these transitions.

00:01:49.790 --> 00:01:50.140
&gt;&gt; Mm.

00:01:50.140 --> 00:01:52.930
&gt;&gt; And using that information we're going to instead

00:01:52.930 --> 00:01:54.930
of computing a policy, we're going to learn a policy.

00:01:54.930 --> 00:01:57.220
&gt;&gt; I see. So we call that learning?

00:01:57.220 --> 00:01:58.590
&gt;&gt; Yeah, or even reinforcement learning.

00:01:58.590 --> 00:02:01.600
&gt;&gt; Mm. By the way, what makes it reinforcement learning?

00:02:01.600 --> 00:02:04.577
&gt;&gt; That's a question. [LAUGH].

00:02:04.577 --> 00:02:06.120
&gt;&gt; It's not a good question,

00:02:06.120 --> 00:02:06.810
but it's a question.

00:02:06.810 --> 00:02:08.350
&gt;&gt; I was, I was going to say good question, but

00:02:08.350 --> 00:02:10.370
I'll, well maybe, maybe there, it's not that it was a

00:02:10.370 --> 00:02:12.370
bad question, it's that I don't have a particularly good answer

00:02:12.370 --> 00:02:14.640
for it. So, maybe we need another slide to discuss that.

00:02:14.640 --> 00:02:15.170
&gt;&gt; Okay.

