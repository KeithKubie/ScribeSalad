WEBVTT
Kind: captions
Language: en

00:00:04.280 --> 00:00:07.140
HAL VARIAN: Welcome, you
all, to this session,

00:00:07.140 --> 00:00:11.490
where Richard Thaler is going
to tell us about his new book.

00:00:11.490 --> 00:00:13.370
And I should tell
you, both Richard

00:00:13.370 --> 00:00:16.020
and I were on a panel in
San Francisco last night.

00:00:16.020 --> 00:00:21.400
So this is kind of a lot of
instant replay, deja vu for us.

00:00:21.400 --> 00:00:23.620
RICHARD THALER: I'm going
to change the answers, so--

00:00:23.620 --> 00:00:25.495
HAL VARIAN: It's kind
of like the distinction

00:00:25.495 --> 00:00:27.660
between microeconomics
and macroeconomics,

00:00:27.660 --> 00:00:31.000
because in micro, it's different
questions but the same answers.

00:00:31.000 --> 00:00:33.360
In macro, it's the same
question, with different.

00:00:33.360 --> 00:00:35.550
So this is kind of a
behavioral economics

00:00:35.550 --> 00:00:39.610
has in common with some
other parts of economics.

00:00:39.610 --> 00:00:44.710
But one of the good
lead-in questions

00:00:44.710 --> 00:00:47.040
that we had last night, I'm
going to run again today,

00:00:47.040 --> 00:00:49.660
is, what does this
mean, Misbehaving?

00:00:49.660 --> 00:00:50.940
What kind of title is that?

00:00:50.940 --> 00:00:52.570
Where did it come from?

00:00:52.570 --> 00:00:59.120
RICHARD THALER: So
title kind of reflects

00:00:59.120 --> 00:01:01.470
three aspects of the book.

00:01:01.470 --> 00:01:07.300
The first is that economists
have a particular view

00:01:07.300 --> 00:01:15.320
of humanity-- if we want to
call it humanity, or inhumanity.

00:01:15.320 --> 00:01:19.790
So if you read a
graduate level textbook,

00:01:19.790 --> 00:01:29.440
like Hal's-- that'll be
$1 please, for the plug--

00:01:29.440 --> 00:01:34.630
the people he's describing
in the models are not ones

00:01:34.630 --> 00:01:38.500
you'd recognize
in ordinary life.

00:01:38.500 --> 00:01:42.940
They're are as good
at math as Hal.

00:01:42.940 --> 00:01:50.350
They are as good at
self control as Gandhi.

00:01:50.350 --> 00:01:54.290
And they're complete jerks.

00:01:54.290 --> 00:01:57.760
If you left your
wallet lying around,

00:01:57.760 --> 00:02:02.230
they would take it if they were
sure they wouldn't get caught.

00:02:02.230 --> 00:02:05.560
So for the last 40
years or so, I've

00:02:05.560 --> 00:02:11.380
been studying humans-- I call
those mythical creatures econs,

00:02:11.380 --> 00:02:13.600
and I've been studying humans.

00:02:13.600 --> 00:02:19.240
And by economists'
standards, humans misbehave.

00:02:19.240 --> 00:02:24.660
And so, that's the first meaning
of misbehaving in this book,

00:02:24.660 --> 00:02:26.720
it's the people I talk about.

00:02:26.720 --> 00:02:31.510
The second meaning
is the fact that I've

00:02:31.510 --> 00:02:36.370
devoted my career to
studying that was itself

00:02:36.370 --> 00:02:39.490
an act of misbehaving,
because economists

00:02:39.490 --> 00:02:41.300
are supposed to study econs.

00:02:41.300 --> 00:02:46.032
And what are you doing
studying these other people?

00:02:46.032 --> 00:02:52.010
And maybe a third
is, and it's kind of

00:02:52.010 --> 00:02:54.220
consistent with the
second, the book

00:02:54.220 --> 00:02:58.820
is not written as a proper
book, or at least everyone

00:02:58.820 --> 00:03:01.140
in the publishing
industry told me

00:03:01.140 --> 00:03:03.760
one is not supposed to
write a book like this.

00:03:03.760 --> 00:03:10.570
So it's structured
kind of as a memoir,

00:03:10.570 --> 00:03:15.440
but it's a primer in
behavioral economics,

00:03:15.440 --> 00:03:17.900
so there's a lot of
substance in there.

00:03:17.900 --> 00:03:21.110
And it's at least
meant to be funny.

00:03:21.110 --> 00:03:25.010
And I was told that
the set of books

00:03:25.010 --> 00:03:29.790
that are substantive,
funny memoirs,

00:03:29.790 --> 00:03:36.190
and have sold as many as
100 copies, is the null set.

00:03:36.190 --> 00:03:39.750
And so you probably shouldn't
write a book like that.

00:03:39.750 --> 00:03:41.560
But that's the book I wrote.

00:03:41.560 --> 00:03:45.140
It was the only
book I could write.

00:03:45.140 --> 00:03:48.960
HAL VARIAN: You might say, also,
that the econs are misbehaving

00:03:48.960 --> 00:03:51.860
from the viewpoint
of the humans.

00:03:51.860 --> 00:03:54.170
Like for example,
humans are expected

00:03:54.170 --> 00:03:56.516
to leave tips at restaurants,
and that kind of thing.

00:03:56.516 --> 00:03:57.890
RICHARD THALER:
Ah, that's right.

00:03:57.890 --> 00:04:01.030
I mean, econs don't
leave tips at restaurants

00:04:01.030 --> 00:04:07.120
they don't plan to go back
to, because why would you?

00:04:07.120 --> 00:04:09.720
HAL VARIAN: Of course,
that's a bit of exaggeration.

00:04:09.720 --> 00:04:14.390
RICHARD THALER:
Well, only a bit.

00:04:14.390 --> 00:04:21.100
I mean, it is true that many
economists have taken seriously

00:04:21.100 --> 00:04:24.340
some of the departures.

00:04:24.340 --> 00:04:30.660
My late colleague Gary
Becker, in some ways,

00:04:30.660 --> 00:04:36.520
his career was devoted
to the opposite of mine.

00:04:36.520 --> 00:04:40.080
Although we both studied
odd kinds of behavior,

00:04:40.080 --> 00:04:45.370
his approach was
modeling everything

00:04:45.370 --> 00:04:49.210
through the lens of a
rational economic agent.

00:04:49.210 --> 00:04:51.660
So he has a rational
model of addiction.

00:04:57.410 --> 00:05:00.036
He could rationalize anything.

00:05:00.036 --> 00:05:01.410
HAL VARIAN: Gotta
do this, sorry.

00:05:01.410 --> 00:05:02.910
RICHARD THALER:
That is misbehaving.

00:05:02.910 --> 00:05:04.700
HAL VARIAN: I
know, I'm so sorry.

00:05:04.700 --> 00:05:07.800
RICHARD THALER:
That's a new chapter.

00:05:07.800 --> 00:05:10.900
Take the books back, I
have to add the chapter

00:05:10.900 --> 00:05:14.350
on cell phone misbehaving.

00:05:14.350 --> 00:05:17.060
So yeah, I mean you
could write down

00:05:17.060 --> 00:05:24.230
a model where you care about
the opinion of the waiter,

00:05:24.230 --> 00:05:27.260
and so therefore
you leave a tip.

00:05:27.260 --> 00:05:30.520
But that's kind of
a tautological model

00:05:30.520 --> 00:05:33.340
that isn't very helpful.

00:05:33.340 --> 00:05:35.860
HAL VARIAN: And I should say,
we warned people last night.

00:05:35.860 --> 00:05:39.480
The American Economics
Association meeting,

00:05:39.480 --> 00:05:43.530
annual meeting, is going to be
in San Francisco in January.

00:05:43.530 --> 00:05:46.490
And we expect all waiters
and waitresses to leave town.

00:05:46.490 --> 00:05:47.130
So--

00:05:47.130 --> 00:05:48.588
RICHARD THALER:
Yeah, it's probably

00:05:48.588 --> 00:05:50.660
going to be a really bad time.

00:05:50.660 --> 00:05:56.300
Unless they don't-- either they
don't realize that the AEA is

00:05:56.300 --> 00:06:00.600
in town in January, or they
don't know that economists are

00:06:00.600 --> 00:06:02.770
bad tippers.

00:06:02.770 --> 00:06:05.620
But yeah, otherwise
it'll be a good time

00:06:05.620 --> 00:06:07.280
to go on vacation
if you're a waiter.

00:06:07.280 --> 00:06:09.780
HAL VARIAN: I once
did a informal study

00:06:09.780 --> 00:06:13.190
of this of tipping, just
by talking to people.

00:06:13.190 --> 00:06:14.810
Unusual methodology, I know.

00:06:14.810 --> 00:06:19.510
RICHARD THALER: Yeah, you
could lose your credentials

00:06:19.510 --> 00:06:21.957
as a real economist
by doing that.

00:06:21.957 --> 00:06:24.040
HAL VARIAN: So it turns
out that, I think the best

00:06:24.040 --> 00:06:27.310
explanatory variable
for whether or not

00:06:27.310 --> 00:06:29.990
you leave a generous
tip is whether you ever

00:06:29.990 --> 00:06:31.670
worked as a waiter or waitress.

00:06:31.670 --> 00:06:34.410
RICHARD THALER: Yeah,
I think that's true.

00:06:34.410 --> 00:06:39.070
You know, I saw an interesting
op-ed about tipping,

00:06:39.070 --> 00:06:41.220
that I've kind of
taken to heart,

00:06:41.220 --> 00:06:46.340
which is that if you're
concerned about inequality

00:06:46.340 --> 00:06:48.750
of the sort that
Piketty talks about,

00:06:48.750 --> 00:06:52.780
here's one small thing you can
do-- become a better tipper.

00:06:52.780 --> 00:06:59.253
And so I've just decided
to leave bigger tips.

00:06:59.253 --> 00:07:02.020
It's not going to
change the world,

00:07:02.020 --> 00:07:04.640
but all the people
who get tips from me

00:07:04.640 --> 00:07:06.070
are much poorer than me.

00:07:06.070 --> 00:07:09.390
And I can spend a
little wealth that way.

00:07:09.390 --> 00:07:12.334
HAL VARIAN: That's
a very useful tip.

00:07:12.334 --> 00:07:14.640
RICHARD THALER: Nice one, Hal.

00:07:14.640 --> 00:07:16.400
HAL VARIAN: I couldn't resist.

00:07:16.400 --> 00:07:19.580
So tell us about some
of these anomalies.

00:07:19.580 --> 00:07:21.839
In fact, tell me how you
got started in the business.

00:07:21.839 --> 00:07:23.380
RICHARD THALER: Ah
yeah, well there's

00:07:23.380 --> 00:07:27.150
fishing for a compliment.

00:07:27.150 --> 00:07:28.950
No, deservedly so.

00:07:28.950 --> 00:07:36.340
So Hal once introduced
me to someone, and said,

00:07:36.340 --> 00:07:37.730
this is Richard Thaler.

00:07:37.730 --> 00:07:39.260
Actually, I invented him.

00:07:41.790 --> 00:07:44.950
And so here's what Hal meant.

00:07:44.950 --> 00:07:47.240
We've known each
other for longer

00:07:47.240 --> 00:07:50.880
than we would care to admit.

00:07:50.880 --> 00:07:56.740
And, let's see,
this was about 1986.

00:07:56.740 --> 00:08:00.370
We were at a
conference together.

00:08:00.370 --> 00:08:04.390
I don't remember where, we
were eating a meal together.

00:08:04.390 --> 00:08:07.330
And Hal was telling
me about a new journal

00:08:07.330 --> 00:08:10.980
the AEA was starting, called
the Journal of Economic

00:08:10.980 --> 00:08:12.630
Perspectives.

00:08:12.630 --> 00:08:17.360
And the aim of the
journal was to have

00:08:17.360 --> 00:08:20.800
articles written
in what economists

00:08:20.800 --> 00:08:24.600
would pass for plain English.

00:08:24.600 --> 00:08:27.320
So not really
articles for laymen,

00:08:27.320 --> 00:08:29.730
but articles for
non-specialists.

00:08:29.730 --> 00:08:34.919
So an article that any economist
or economics grad student

00:08:34.919 --> 00:08:36.230
could read.

00:08:36.230 --> 00:08:42.130
And so I said, oh,
that's interesting.

00:08:42.130 --> 00:08:46.450
And then he and I
dreamed up the idea

00:08:46.450 --> 00:08:50.380
for a column in that journal
that I ended up starting

00:08:50.380 --> 00:08:53.480
to write, called Anomalies.

00:08:53.480 --> 00:08:56.690
And for four years,
once a quarter,

00:08:56.690 --> 00:09:00.230
I wrote about an anomaly.

00:09:00.230 --> 00:09:06.230
And I think that--
so I owe Hal a debt,

00:09:06.230 --> 00:09:10.030
and the field of
behavioral economics

00:09:10.030 --> 00:09:12.290
owes Hal a debt for that.

00:09:12.290 --> 00:09:14.050
He convinced Joe
Stiglitz, who was

00:09:14.050 --> 00:09:17.840
the editor at the
time-- who, I think,

00:09:17.840 --> 00:09:19.280
didn't need much convincing.

00:09:19.280 --> 00:09:20.510
HAL VARIAN: No, he
was easy to convince.

00:09:20.510 --> 00:09:22.426
RICHARD THALER: Joe's a
bit of a troublemaker,

00:09:22.426 --> 00:09:27.390
so he liked the fact that I
was going to stir the pot.

00:09:27.390 --> 00:09:32.380
You know, Kuhn, the great
philosopher of science,

00:09:32.380 --> 00:09:35.420
talks about paradigm shifts.

00:09:35.420 --> 00:09:39.850
And how do you create
a paradigm shift?

00:09:39.850 --> 00:09:43.580
And the only way to
do it is to create

00:09:43.580 --> 00:09:45.970
a long list of anomalies.

00:09:45.970 --> 00:09:50.340
Because any one or two
can be explained away.

00:09:50.340 --> 00:09:53.610
OK, so people leave
tips, even at restaurants

00:09:53.610 --> 00:09:54.780
they don't go back to.

00:09:54.780 --> 00:10:03.950
But, you know, maybe--
and if the set of excuses

00:10:03.950 --> 00:10:07.790
for each anomaly
has to be different,

00:10:07.790 --> 00:10:10.540
then people start
to wonder, oh, yeah,

00:10:10.540 --> 00:10:13.920
maybe there's a more
basic problem here.

00:10:13.920 --> 00:10:18.420
So the fact that I could
write 14 Anomalies columns

00:10:18.420 --> 00:10:23.220
about different things, I think
changed some people's mind.

00:10:23.220 --> 00:10:26.550
And the fact that
the 1987 stock market

00:10:26.550 --> 00:10:29.910
crash occurred during
the very period

00:10:29.910 --> 00:10:33.630
when I was writing these
columns may have also helped.

00:10:33.630 --> 00:10:35.270
You didn't do that,
I don't think.

00:10:35.270 --> 00:10:39.950
HAL VARIAN: No, that wasn't
my fault. In the book,

00:10:39.950 --> 00:10:42.430
you said it was partly a
tutorial about behavior

00:10:42.430 --> 00:10:43.002
economics.

00:10:43.002 --> 00:10:44.460
And you go through
this long list--

00:10:44.460 --> 00:10:47.820
it's a long list of
aspects of human behavior

00:10:47.820 --> 00:10:51.330
that are not accounted for
by conventional theory.

00:10:51.330 --> 00:10:54.760
And the first one-- both
temporally and in the book--

00:10:54.760 --> 00:10:57.680
is in fact the endowment effect.

00:10:57.680 --> 00:11:00.024
So why don't you tell us
about the endowment effect.

00:11:00.024 --> 00:11:01.440
RICHARD THALER:
So let me tell you

00:11:01.440 --> 00:11:03.860
when I first discovered
what I later came

00:11:03.860 --> 00:11:06.200
to call the endowment effect.

00:11:06.200 --> 00:11:12.460
My PhD thesis was
on a topic that

00:11:12.460 --> 00:11:19.060
sounds funny-- or not funny,
but odd-- the value of a life.

00:11:19.060 --> 00:11:24.856
And this wasn't philosophical,
this was an economics problem.

00:11:24.856 --> 00:11:29.140
And it's a problem that all
governments have to deal with.

00:11:29.140 --> 00:11:31.630
We can make things safer.

00:11:31.630 --> 00:11:34.816
We can make highways safer--
we could lower the speed limit,

00:11:34.816 --> 00:11:37.920
we can do all kinds
of things to reduce

00:11:37.920 --> 00:11:39.480
the chance you're going die.

00:11:39.480 --> 00:11:42.280
How much should we be
willing to spend on that?

00:11:42.280 --> 00:11:44.690
We don't want to spend
all our money on that.

00:11:44.690 --> 00:11:46.450
So we need a number.

00:11:46.450 --> 00:11:50.390
And so that was what
my thesis was about.

00:11:50.390 --> 00:11:53.390
And it was a very
straight economics,

00:11:53.390 --> 00:11:56.240
econometrics
exercise, estimating

00:11:56.240 --> 00:12:00.960
how much you had to pay people
to get them to take risky jobs.

00:12:00.960 --> 00:12:04.480
Like in logging, or
coal mining, or window

00:12:04.480 --> 00:12:07.120
washing on skyscrapers.

00:12:07.120 --> 00:12:09.960
But while I was
working on that, sort

00:12:09.960 --> 00:12:14.700
of as a break from
writing Fortran code,

00:12:14.700 --> 00:12:16.325
I decided I'd ask a question.

00:12:19.170 --> 00:12:21.890
So I asked the following
question-- suppose,

00:12:21.890 --> 00:12:25.130
by attending, this
lecture today,

00:12:25.130 --> 00:12:30.950
you've been exposed to
a rare, fatal disease.

00:12:30.950 --> 00:12:33.560
And there's a one
in a thousand chance

00:12:33.560 --> 00:12:36.400
you're going to
drop dead next week.

00:12:36.400 --> 00:12:41.570
A quick and painless
death, not to worry.

00:12:41.570 --> 00:12:42.860
We have one cure.

00:12:42.860 --> 00:12:46.310
It's in this glass right, here.

00:12:46.310 --> 00:12:48.780
And we'll sell it to
the highest bidder.

00:12:48.780 --> 00:12:50.690
How much will you pay?

00:12:50.690 --> 00:12:52.190
That's question one.

00:12:52.190 --> 00:12:57.890
Question two is Stanford
is running some studies

00:12:57.890 --> 00:13:00.230
on that same disease,
and they need

00:13:00.230 --> 00:13:02.490
volunteers for an experiment.

00:13:02.490 --> 00:13:04.930
All you have to do
is walk into a room

00:13:04.930 --> 00:13:08.740
and expose yourself to one
in a thousand risk of death.

00:13:08.740 --> 00:13:11.060
There will be no cure available.

00:13:11.060 --> 00:13:14.055
What would you have to
be paid to participate

00:13:14.055 --> 00:13:16.000
in that experiment?

00:13:16.000 --> 00:13:18.200
Now according to
economic theory,

00:13:18.200 --> 00:13:20.150
the answers to those
questions should

00:13:20.150 --> 00:13:22.990
be approximately the same.

00:13:22.990 --> 00:13:28.550
And the responses I got
were wildly different.

00:13:28.550 --> 00:13:34.390
So someone would say, oh,
I'd pay $5,000 for that cure,

00:13:34.390 --> 00:13:38.440
but I wouldn't do that
experiment for $500,000.

00:13:38.440 --> 00:13:41.290
So orders of
magnitude difference.

00:13:41.290 --> 00:13:48.080
And then, I started
lowering the stakes.

00:13:48.080 --> 00:13:55.570
You know, suppose somebody
offers you two tickets to one

00:13:55.570 --> 00:13:58.560
of the Warriors playoffs games.

00:13:58.560 --> 00:14:02.390
And let's say the market
price for those tickets

00:14:02.390 --> 00:14:07.360
is $1,000 each.

00:14:07.360 --> 00:14:11.540
Would you be willing to pay
$1,000 to get those tickets?

00:14:11.540 --> 00:14:13.000
Probably not.

00:14:13.000 --> 00:14:15.010
Would you sell them for $1,000?

00:14:15.010 --> 00:14:16.460
Probably not.

00:14:16.460 --> 00:14:21.080
OK, well that's the
endowment effect.

00:14:21.080 --> 00:14:28.190
And what it implies is a
kind of status quo bias.

00:14:28.190 --> 00:14:32.020
That if somebody gives
us the tickets, we'll go.

00:14:32.020 --> 00:14:35.070
But if they don't give us
the tickets, we won't go.

00:14:35.070 --> 00:14:42.570
And your econ 101 quiz question,
if you've got those tickets

00:14:42.570 --> 00:14:45.490
and the market prices
is $1,000, how much does

00:14:45.490 --> 00:14:47.450
it cost you to go to the game?

00:14:47.450 --> 00:14:50.110
Answer, $1,000.

00:14:50.110 --> 00:14:53.010
But that's not the way
people think about it.

00:14:53.010 --> 00:14:59.550
And eventually, I started
having a list on my blackboard,

00:14:59.550 --> 00:15:05.810
as an assistant professor,
of weird shit people do.

00:15:05.810 --> 00:15:11.700
And that was the first
thing on my list.

00:15:11.700 --> 00:15:18.420
The second one goes
back to a dinner party

00:15:18.420 --> 00:15:21.320
I hosted as a graduate student.

00:15:21.320 --> 00:15:25.470
And while some
roast or something

00:15:25.470 --> 00:15:30.150
was cooking in the oven,
creating delightful aromas,

00:15:30.150 --> 00:15:33.470
I brought out a
bowl of cashew nuts.

00:15:33.470 --> 00:15:37.995
And we all started
munching away.

00:15:37.995 --> 00:15:45.100
And the bowl of cashew nuts and
our appetites were in danger.

00:15:45.100 --> 00:15:49.260
And so after a few
minutes, I took the bowl,

00:15:49.260 --> 00:15:52.650
and-- eating a few
more nuts on the way--

00:15:52.650 --> 00:15:57.410
went and hid it in the
kitchen, and came back.

00:15:57.410 --> 00:16:01.210
And this was a group
of econ grad students.

00:16:01.210 --> 00:16:05.100
And so we immediately
started analyzing

00:16:05.100 --> 00:16:07.990
what had just happened.

00:16:07.990 --> 00:16:09.890
There's a rule of
thumb I mentioned

00:16:09.890 --> 00:16:13.280
in Nudge, my
previous book, which

00:16:13.280 --> 00:16:18.100
is that the conversation
at a dinner party

00:16:18.100 --> 00:16:21.430
will be ruined if more
than half the guests come

00:16:21.430 --> 00:16:23.150
from the economics department.

00:16:23.150 --> 00:16:25.690
And this story is
an illustration

00:16:25.690 --> 00:16:29.120
of that, since the removal
of a bowl of cashew nuts

00:16:29.120 --> 00:16:31.440
led to a decision tree.

00:16:31.440 --> 00:16:37.380
And what economists
all know is that one

00:16:37.380 --> 00:16:41.750
can't be made better
off if one's choice

00:16:41.750 --> 00:16:44.610
set is made smaller.

00:16:44.610 --> 00:16:46.880
And that's what I just did.

00:16:46.880 --> 00:16:51.360
We previously had the choice
eat nuts or don't eat nuts.

00:16:51.360 --> 00:16:54.600
Now we didn't have that
choice, and we were happy.

00:16:54.600 --> 00:16:56.170
How could that be?

00:16:56.170 --> 00:16:58.890
OK, it goes on my list.

00:16:58.890 --> 00:17:01.950
And so for a long time,
all I had was a list.

00:17:01.950 --> 00:17:08.130
And dwindling
professional aspirations,

00:17:08.130 --> 00:17:12.410
because a list of
weird shit people do

00:17:12.410 --> 00:17:14.750
doesn't get you tenure.

00:17:14.750 --> 00:17:16.770
HAL VARIAN: I think last
night at our meeting,

00:17:16.770 --> 00:17:20.339
I cited a well known 20th
century philosopher, who said,

00:17:20.339 --> 00:17:22.790
the Lord above made
liquor for temptation

00:17:22.790 --> 00:17:25.089
to see if man could
stay away from sin.

00:17:25.089 --> 00:17:27.520
The lord above made
liquor for temptation,

00:17:27.520 --> 00:17:29.950
but with a little bit of
luck, when temptation comes,

00:17:29.950 --> 00:17:32.015
you'll give right in.

00:17:32.015 --> 00:17:34.015
And in fact, at that
point, we broke for drinks.

00:17:36.176 --> 00:17:38.300
RICHARD THALER: I don't
think you're serving drinks

00:17:38.300 --> 00:17:40.119
after this talk, right?

00:17:40.119 --> 00:17:41.160
HAL VARIAN: Just for you.

00:17:45.070 --> 00:17:46.070
So that was one of them.

00:17:46.070 --> 00:17:48.000
And by the way, the
tickets is a nice example,

00:17:48.000 --> 00:17:50.208
because tickets come up in
another place in the book,

00:17:50.208 --> 00:17:51.310
and that's on scalping.

00:17:51.310 --> 00:17:54.930
This is also a case
where economists may not

00:17:54.930 --> 00:17:58.472
like tipping so much, but
they really like scalping.

00:17:58.472 --> 00:17:59.930
RICHARD THALER:
They like scalping.

00:17:59.930 --> 00:18:10.750
And economists have no trouble
with Uber's surge pricing,

00:18:10.750 --> 00:18:14.810
no matter how high
the surge goes.

00:18:14.810 --> 00:18:20.120
And economists are
unique in that.

00:18:20.120 --> 00:18:26.390
I did a study with my good
friend and colleague Danny

00:18:26.390 --> 00:18:31.510
Kahneman in 1985.

00:18:31.510 --> 00:18:32.230
I was 12.

00:18:36.000 --> 00:18:41.710
And it was a study of basically
what pisses people off.

00:18:41.710 --> 00:18:48.830
And so we had a whole series
of questions where we would

00:18:48.830 --> 00:18:51.040
ask whether something is fair.

00:18:51.040 --> 00:18:55.690
So here's an example
from that study.

00:18:55.690 --> 00:19:00.520
A hardware has been selling
snow shovels for $15.

00:19:00.520 --> 00:19:03.510
The morning after a
blizzard, the store

00:19:03.510 --> 00:19:06.560
raises the price to $20.

00:19:06.560 --> 00:19:11.300
Rate that on a one to
four scale fairness,

00:19:11.300 --> 00:19:14.840
from completely fair
to totally unfair.

00:19:14.840 --> 00:19:16.490
People hate that.

00:19:16.490 --> 00:19:17.840
They hate it.

00:19:17.840 --> 00:19:21.700
Now I give that same
question to my MBA students.

00:19:21.700 --> 00:19:23.100
They all say, yeah.

00:19:23.100 --> 00:19:24.340
Right.

00:19:24.340 --> 00:19:28.270
Because they took a price
theory class, and in that class,

00:19:28.270 --> 00:19:30.470
this was the right
answer, right?

00:19:30.470 --> 00:19:35.020
Demand has shifted,
price goes up.

00:19:35.020 --> 00:19:41.280
So in New York City,
there was a blizzard,

00:19:41.280 --> 00:19:45.560
and Uber thought it
was a really good time

00:19:45.560 --> 00:19:49.137
to raise the price of cab
rides by a factor of 10.

00:19:52.515 --> 00:19:53.425
10, yes.

00:19:57.270 --> 00:20:01.220
Many people, including
the state attorney general

00:20:01.220 --> 00:20:05.390
decided that wasn't
really a good idea.

00:20:05.390 --> 00:20:08.940
And in fact, many
states have a law

00:20:08.940 --> 00:20:11.510
against what's called gouging.

00:20:11.510 --> 00:20:13.820
Now the literal
meaning of gouging

00:20:13.820 --> 00:20:16.850
is to poke a hole in something.

00:20:16.850 --> 00:20:21.500
And that's what most
humans feel like you

00:20:21.500 --> 00:20:27.560
are doing if you charge them 10
times the usual fare because it

00:20:27.560 --> 00:20:28.515
happens to be snowing.

00:20:31.200 --> 00:20:35.910
Uber ended up making an
agreement with the state

00:20:35.910 --> 00:20:40.800
attorney general to cap
the amount by which they

00:20:40.800 --> 00:20:44.670
would surge in an emergency.

00:20:44.670 --> 00:20:50.240
I write in the book that it's
my opinion that they should

00:20:50.240 --> 00:20:53.880
have done that unilaterally.

00:20:53.880 --> 00:20:57.930
And a similar thing
happened in Sydney.

00:20:57.930 --> 00:21:00.890
I don't remember the
details of what happened,

00:21:00.890 --> 00:21:05.120
but there was some
terrorist attack,

00:21:05.120 --> 00:21:11.330
or maybe it was thought to
be a terrorist-- something

00:21:11.330 --> 00:21:13.448
like that.

00:21:13.448 --> 00:21:16.620
And there was a surge.

00:21:16.620 --> 00:21:19.930
And as I describe
in the book, imagine

00:21:19.930 --> 00:21:23.980
had Uber been around on 9/11.

00:21:23.980 --> 00:21:30.210
And all the cabs were
snagged by investment bankers

00:21:30.210 --> 00:21:32.310
who needed a ride to Greenwich.

00:21:32.310 --> 00:21:35.905
I think that would've been the
last day Uber was in business.

00:21:38.610 --> 00:21:44.780
So the norms of fairness
say in emergencies,

00:21:44.780 --> 00:21:47.320
we help each other out.

00:21:47.320 --> 00:21:50.220
And I've actually talked
to several Uber drivers

00:21:50.220 --> 00:21:54.450
about that, and
said, what would you

00:21:54.450 --> 00:21:57.090
want to do in that situation?

00:21:57.090 --> 00:22:01.340
And most of them say,
I'd want to help.

00:22:01.340 --> 00:22:05.960
And Uber doesn't make very
much money on those surges.

00:22:05.960 --> 00:22:09.640
And I think they
could do better,

00:22:09.640 --> 00:22:15.289
and I may try to convince them
that they should do better.

00:22:15.289 --> 00:22:16.830
HAL VARIAN: So one
thing in your book

00:22:16.830 --> 00:22:22.240
that I think could use
a little more exposition

00:22:22.240 --> 00:22:26.870
is this distinction between
a psychological predilection

00:22:26.870 --> 00:22:29.740
towards something, and
a social convention.

00:22:29.740 --> 00:22:32.130
So let's take the
example like tipping.

00:22:32.130 --> 00:22:34.730
Well, in some countries,
tipping's not practiced.

00:22:34.730 --> 00:22:35.990
So of course, you don't tip.

00:22:35.990 --> 00:22:37.600
That's a service
charge or something,

00:22:37.600 --> 00:22:39.030
built into the record.

00:22:39.030 --> 00:22:41.890
And the same thing
with fairness norms.

00:22:41.890 --> 00:22:43.615
So I mentioned
scalping, originally.

00:22:43.615 --> 00:22:45.360
And scalping, some
people thought

00:22:45.360 --> 00:22:47.550
that was this terrible
thing, that you buy tickets

00:22:47.550 --> 00:22:49.520
at a low price, resell
them at a high price.

00:22:49.520 --> 00:22:51.836
But now, there are organized
markets like StubHub,

00:22:51.836 --> 00:22:53.960
everybody expects, well if
I don't use the tickets,

00:22:53.960 --> 00:22:55.810
I could go into the
secondary market.

00:22:55.810 --> 00:22:59.260
And I don't think scalping
is really considered immoral

00:22:59.260 --> 00:22:59.809
anymore.

00:22:59.809 --> 00:23:01.600
I don't know, what does
the audience think?

00:23:01.600 --> 00:23:03.980
Is scalping immoral?

00:23:03.980 --> 00:23:05.020
Nobody.

00:23:05.020 --> 00:23:07.072
But at one point, that the view.

00:23:07.072 --> 00:23:08.030
RICHARD THALER: No, no.

00:23:08.030 --> 00:23:08.710
So, look.

00:23:08.710 --> 00:23:14.290
I think these things-- so
to the specific question

00:23:14.290 --> 00:23:20.570
you're raising, which I think
is a really good one, what

00:23:20.570 --> 00:23:25.820
the distinction I would make
is, what is the cultural norm?

00:23:25.820 --> 00:23:29.410
And then, do people
adhere to it?

00:23:29.410 --> 00:23:31.760
And then, what the
cultural norms are

00:23:31.760 --> 00:23:33.940
will depend on the cultures.

00:23:33.940 --> 00:23:37.295
So one of the
reasons why Greece is

00:23:37.295 --> 00:23:41.230
in so much trouble is the
cultural norm in Greece

00:23:41.230 --> 00:23:47.500
is that if you pay your
taxes, you're a sucker.

00:23:47.500 --> 00:23:50.590
That's a problem.

00:23:50.590 --> 00:23:53.460
None of us love paying taxes.

00:23:53.460 --> 00:23:58.220
Most of us grumble about
how high our taxes are.

00:23:58.220 --> 00:24:03.940
But we think that
we have to do it.

00:24:03.940 --> 00:24:11.080
And, I mean, people may
be imaginative in thinking

00:24:11.080 --> 00:24:12.650
of deductions.

00:24:12.650 --> 00:24:16.290
But basically, in
this country, people

00:24:16.290 --> 00:24:18.860
pay most of the tax they owe.

00:24:18.860 --> 00:24:22.880
And that's not the
case in Greece.

00:24:22.880 --> 00:24:26.260
And so they have an
economic problem,

00:24:26.260 --> 00:24:29.900
but they have kind of
a social norms problem.

00:24:29.900 --> 00:24:33.070
And there's all
kinds of discussion

00:24:33.070 --> 00:24:37.380
about whether they should
have more or less austerity.

00:24:37.380 --> 00:24:41.800
The only way Greece is going to
solve their economic problems

00:24:41.800 --> 00:24:45.760
in the long run is to
change their cultural norms.

00:24:45.760 --> 00:24:51.180
And it's not that I have an
answer about how to do that.

00:24:51.180 --> 00:24:53.640
I kind of know some of the
things you'd want to do.

00:24:56.230 --> 00:25:00.840
Hal knows that I have
been doing some work

00:25:00.840 --> 00:25:03.620
for the British government
for the last five years.

00:25:03.620 --> 00:25:06.910
After I wrote, with my
friend Cass Sunstein,

00:25:06.910 --> 00:25:11.280
the book Nudge,
David Cameron created

00:25:11.280 --> 00:25:15.150
a tiny little government unit,
called the Behavioral Insights

00:25:15.150 --> 00:25:19.440
Team that everyone now
just calls the Nudge Unit.

00:25:19.440 --> 00:25:23.490
And it started out with five
people, it's now over 50.

00:25:23.490 --> 00:25:27.700
And one of our big success
stories, early on-- I

00:25:27.700 --> 00:25:30.020
say, we, because
I've been working

00:25:30.020 --> 00:25:33.920
with that team from
its conception--

00:25:33.920 --> 00:25:36.850
was exactly on collecting
money from people

00:25:36.850 --> 00:25:39.520
who owed on their taxes.

00:25:39.520 --> 00:25:43.440
And we were able to
run an experiment.

00:25:43.440 --> 00:25:45.370
So we met some guy.

00:25:45.370 --> 00:25:48.330
One of the very first
meetings I had over there

00:25:48.330 --> 00:25:52.020
was with a guy whose job it
was to collect from people

00:25:52.020 --> 00:25:53.940
who owed money on their taxes.

00:25:53.940 --> 00:25:55.850
We say, all right,
what do you do?

00:25:55.850 --> 00:25:57.580
And he says, well, we
send them a letter.

00:25:57.580 --> 00:26:04.040
Dear Mr. Varian, you owe
$15,000-- 15,000 pounds,

00:26:04.040 --> 00:26:08.220
we'll make that-- on your taxes.

00:26:08.220 --> 00:26:10.640
Here's how to pay,
and if you don't pay,

00:26:10.640 --> 00:26:12.580
we're going to be mean to you.

00:26:12.580 --> 00:26:18.490
And so we got permission
to run experiments

00:26:18.490 --> 00:26:22.910
of the sort of Google
does every minute,

00:26:22.910 --> 00:26:25.790
changing the wording
of that letter.

00:26:25.790 --> 00:26:29.380
And the winning
letter uses a trick

00:26:29.380 --> 00:26:33.585
from the Robert Cialdini bible.

00:26:36.680 --> 00:26:39.360
Robert Cialdini is a
social psychologist

00:26:39.360 --> 00:26:42.510
who wrote the famous
book Influence.

00:26:42.510 --> 00:26:46.290
And so what we told
people-- truthfully--

00:26:46.290 --> 00:26:51.670
is that the residents-- and then
we localized this, because it

00:26:51.670 --> 00:26:53.250
turns out that helps.

00:26:53.250 --> 00:26:57.550
So the residents
of Manchester-- 90%

00:26:57.550 --> 00:27:01.500
of the residents of Manchester
pay their taxes on time.

00:27:01.500 --> 00:27:05.790
You are in the minority
of those who don't.

00:27:05.790 --> 00:27:08.790
That increased
the percent to pay

00:27:08.790 --> 00:27:14.816
within the first window, which
happens to be 23 days, by 5

00:27:14.816 --> 00:27:17.490
percentage points.

00:27:17.490 --> 00:27:21.010
Now that means millions
of pounds, right?

00:27:21.010 --> 00:27:24.967
And it costs nothing to add
that sentence to that letter.

00:27:24.967 --> 00:27:26.550
You're already mailing
the letter out.

00:27:29.390 --> 00:27:32.700
So there are several
lessons from that.

00:27:32.700 --> 00:27:41.870
One is people respond
positively to social norms.

00:27:41.870 --> 00:27:43.320
You know, what do
you do when you

00:27:43.320 --> 00:27:47.000
go to another country--
going back to tipping.

00:27:47.000 --> 00:27:50.250
Those of us who like
to behave, we ask,

00:27:50.250 --> 00:27:53.240
what's the tipping
norm in this country?

00:27:53.240 --> 00:27:59.000
And then, we try to behave
the way the natives do.

00:27:59.000 --> 00:28:02.770
So if everybody pays
their taxes on time,

00:28:02.770 --> 00:28:06.650
you try to behave
that way, as well.

00:28:06.650 --> 00:28:09.110
The second lesson
from that is one

00:28:09.110 --> 00:28:16.500
that Google has largely learned,
although not completely,

00:28:16.500 --> 00:28:21.540
which is when you can, you
should run experiments.

00:28:21.540 --> 00:28:25.970
And the only way really to
learn is to run experiments.

00:28:25.970 --> 00:28:30.450
And most organizations are
really terrible at this.

00:28:30.450 --> 00:28:35.130
And Google is not
terrible, but I

00:28:35.130 --> 00:28:39.140
would argue even Google
could run more experiments.

00:28:39.140 --> 00:28:42.420
They're very good in
the domain in which it's

00:28:42.420 --> 00:28:47.030
very easy to run experiments,
like changing the order of ads

00:28:47.030 --> 00:28:49.390
and wording of various things.

00:28:49.390 --> 00:28:53.550
But probably not as good
at experimenting on what

00:28:53.550 --> 00:28:56.570
kinds of people to hire,
and what kinds of jobs

00:28:56.570 --> 00:28:57.422
to give them.

00:28:57.422 --> 00:28:58.880
HAL VARIAN: So one
thing, I'm going

00:28:58.880 --> 00:29:00.960
to say a word of
defense of the Greeks,

00:29:00.960 --> 00:29:03.220
because I think your
analysis is right.

00:29:03.220 --> 00:29:05.840
But of course, this has
developed over 100 years.

00:29:05.840 --> 00:29:08.760
They didn't really like
paying taxes to the Ottomans.

00:29:08.760 --> 00:29:10.480
And in fact, the
Americans didn't

00:29:10.480 --> 00:29:13.980
like paying taxes to the Brits,
because they weren't getting

00:29:13.980 --> 00:29:15.920
perceived value in return.

00:29:15.920 --> 00:29:18.220
But once you develop that
norm, than it's going to be--

00:29:18.220 --> 00:29:19.970
RICHARD THALER: That's
right, the Ottomans

00:29:19.970 --> 00:29:24.630
have been gone a long time--
almost as long as the Brits.

00:29:24.630 --> 00:29:27.260
We've gotten over it.

00:29:27.260 --> 00:29:30.140
HAL VARIAN: So
one thing I wanted

00:29:30.140 --> 00:29:35.770
you to talk a little
more about the savings,

00:29:35.770 --> 00:29:38.790
because I think that's an
extremely interesting part

00:29:38.790 --> 00:29:44.570
of the book, of how you can
help people increase savings

00:29:44.570 --> 00:29:47.020
in a unobtrusive way.

00:29:47.020 --> 00:29:50.200
RICHARD THALER: So
probably the domain

00:29:50.200 --> 00:29:52.070
in which behavioral
economics has

00:29:52.070 --> 00:29:56.790
had its greatest impact is
in the domain of retirement

00:29:56.790 --> 00:29:57.890
saving.

00:29:57.890 --> 00:30:06.220
And As I think most of you
know, once upon a time,

00:30:06.220 --> 00:30:08.620
there were pensions.

00:30:08.620 --> 00:30:13.290
Before many of you were
born, but dinosaurs

00:30:13.290 --> 00:30:15.460
like us remember pensions.

00:30:15.460 --> 00:30:17.640
Hal have a very nice
one at UC Berkeley.

00:30:20.420 --> 00:30:24.700
Where all you did
is work, and then

00:30:24.700 --> 00:30:28.070
when you were done working,
you got a paycheck.

00:30:28.070 --> 00:30:33.540
And you got that
paycheck until you died.

00:30:33.540 --> 00:30:39.110
Now you have to figure out-- you
have to join the 401(k) plan,

00:30:39.110 --> 00:30:42.750
you to figure out how much
to save and how to invest it.

00:30:42.750 --> 00:30:44.890
And then you're going
to, at some point,

00:30:44.890 --> 00:30:47.580
figure out what to
do with that money.

00:30:47.580 --> 00:30:51.140
So that's asking a lot of
people who don't know very

00:30:51.140 --> 00:30:54.820
much about financial markets.

00:30:54.820 --> 00:30:59.530
So the first step is just to
get people to join the plan.

00:30:59.530 --> 00:31:05.810
And there, we encouraged people
to make a very simple change,

00:31:05.810 --> 00:31:11.450
which is to change
the default. So this

00:31:11.450 --> 00:31:13.320
is called automatic enrollment.

00:31:13.320 --> 00:31:16.850
And under the old
regime, when you're first

00:31:16.850 --> 00:31:20.900
eligible for the plan, you get
a pile of papers to fill out.

00:31:20.900 --> 00:31:24.740
And if you don't fill those
out, you don't get in the plan.

00:31:24.740 --> 00:31:26.390
Under automatic
enrollment-- I assume

00:31:26.390 --> 00:31:29.140
you have automatic
enrollment at Google?

00:31:29.140 --> 00:31:30.290
HAL VARIAN: Yes.

00:31:30.290 --> 00:31:32.670
RICHARD THALER: Good.

00:31:32.670 --> 00:31:37.090
Then you're told, unless
you fill out this form,

00:31:37.090 --> 00:31:40.830
we're going to enroll you.

00:31:40.830 --> 00:31:47.850
However, what is the
saving rate at which

00:31:47.850 --> 00:31:50.180
you get started, if
you're automatically

00:31:50.180 --> 00:31:52.380
enrolled at Google?

00:31:52.380 --> 00:31:53.077
AUDIENCE: 10%.

00:31:53.077 --> 00:31:53.910
RICHARD THALER: 10%?

00:31:53.910 --> 00:31:55.800
Excellent.

00:31:55.800 --> 00:31:59.100
That is really
good, I'm impressed.

00:31:59.100 --> 00:32:04.320
I'd say 90% of companies that
use automatic enrollment enroll

00:32:04.320 --> 00:32:06.860
people at 3%.

00:32:06.860 --> 00:32:12.740
And the reason for
that is sad and funny.

00:32:12.740 --> 00:32:20.080
So back in the mid '90s,
when this idea was new,

00:32:20.080 --> 00:32:21.970
companies would
come to me and say,

00:32:21.970 --> 00:32:23.820
you know, we'd like
to do this, but we're

00:32:23.820 --> 00:32:26.850
worried about
whether it's legal,

00:32:26.850 --> 00:32:28.710
because we're going
to sign people up

00:32:28.710 --> 00:32:30.450
without their permission.

00:32:30.450 --> 00:32:33.040
So I called a friend of mine
who worked in the Treasury

00:32:33.040 --> 00:32:39.780
Department, and said, can
you get some letter written

00:32:39.780 --> 00:32:42.420
clarifying that this is legal?

00:32:42.420 --> 00:32:45.250
And he said, yeah,
I could do that.

00:32:45.250 --> 00:32:49.490
And so he and somebody from
the IRS drafted a letter.

00:32:49.490 --> 00:32:51.740
And the way those letters
tend to be written

00:32:51.740 --> 00:32:56.770
is you give a general statement,
and then you give an example.

00:32:56.770 --> 00:33:01.170
So for example, suppose
there's a company,

00:33:01.170 --> 00:33:04.450
and it signs people up
for their pension plan,

00:33:04.450 --> 00:33:09.520
at a 3% saving rate,
blah, blah, blah.

00:33:09.520 --> 00:33:14.900
And it's still now the case
that most companies sign people

00:33:14.900 --> 00:33:18.080
up at 3%.

00:33:18.080 --> 00:33:23.130
Yeah, so this is called
an unintentional anchor.

00:33:23.130 --> 00:33:26.300
And it's had the effect
of anchoring people

00:33:26.300 --> 00:33:28.940
at a very low saving
rate, which created

00:33:28.940 --> 00:33:32.830
the need for another
behavioral economics idea

00:33:32.830 --> 00:33:36.960
that a former student of
mine, Shlomo Benartzi and I,

00:33:36.960 --> 00:33:40.360
developed, that we call
save more tomorrow.

00:33:40.360 --> 00:33:44.930
And save more tomorrow
is based on the premise

00:33:44.930 --> 00:33:49.580
that we all have more self
control in the future.

00:33:49.580 --> 00:33:54.780
So I'm planning a
diet, but not tonight.

00:33:54.780 --> 00:33:58.410
And probably not this week,
or at least not until the end

00:33:58.410 --> 00:34:01.370
of this book tour.

00:34:01.370 --> 00:34:06.800
So, you know, Lord give
me strength, but not now.

00:34:06.800 --> 00:34:09.370
You know that?

00:34:09.370 --> 00:34:12.020
So the idea of
save more tomorrow

00:34:12.020 --> 00:34:15.280
is you invite people to
increase their saving

00:34:15.280 --> 00:34:19.929
rates in the future,
when they get a raise.

00:34:19.929 --> 00:34:28.690
And so modern 401(k) plans
now have automatic enrollment.

00:34:28.690 --> 00:34:31.510
And you don't need
it so much at Google

00:34:31.510 --> 00:34:33.250
if you start people at 10%.

00:34:33.250 --> 00:34:37.230
But automatic escalation
to get them up to 10%,

00:34:37.230 --> 00:34:39.250
or some number higher.

00:34:39.250 --> 00:34:44.889
And then that, plus a sensible
default investment vehicle,

00:34:44.889 --> 00:34:49.440
like a target date
fund-- my mantra when

00:34:49.440 --> 00:34:52.170
I'm in the UK with
the Nudge Unit

00:34:52.170 --> 00:34:55.929
and we're talking
to these ministers,

00:34:55.929 --> 00:34:57.720
in virtually every
meeting, I would

00:34:57.720 --> 00:35:02.270
find myself repeating the same
three words-- make it easy.

00:35:02.270 --> 00:35:06.400
If you want to get
people to do something,

00:35:06.400 --> 00:35:10.690
remove the barriers that are
preventing them from doing it.

00:35:10.690 --> 00:35:14.870
And automatic enrollment
is a good example.

00:35:14.870 --> 00:35:17.900
And there are now countries
all over the world starting

00:35:17.900 --> 00:35:19.530
these Nudge Units.

00:35:19.530 --> 00:35:25.090
And the advice I give them is,
start with low hanging fruit.

00:35:25.090 --> 00:35:28.620
And saving was
low hanging fruit,

00:35:28.620 --> 00:35:32.970
because, a, it's hard, both
cognitively to figure out

00:35:32.970 --> 00:35:35.790
what to do, and
then will power, you

00:35:35.790 --> 00:35:38.690
have to get yourself to do it.

00:35:38.690 --> 00:35:43.590
And we could solve all
of that with one click.

00:35:43.590 --> 00:35:49.200
Or zero clicks, if it's
automatic enrollment.

00:35:49.200 --> 00:35:52.490
There are other problems--
there's no one click diet.

00:35:56.268 --> 00:36:02.520
It would be good for some of us
if there were, but there isn't.

00:36:02.520 --> 00:36:03.900
Technology may help.

00:36:07.590 --> 00:36:13.090
So if we can find other domains,
where we can make it easy

00:36:13.090 --> 00:36:18.674
and help with problems, then
that's the place to start.

00:36:18.674 --> 00:36:20.090
HAL VARIAN: I'll
give you a Google

00:36:20.090 --> 00:36:21.530
example of this phenomenon.

00:36:21.530 --> 00:36:25.140
If you ask, what day of the
year are the most queries

00:36:25.140 --> 00:36:26.557
for weight loss?

00:36:26.557 --> 00:36:27.640
RICHARD THALER: January 1.

00:36:27.640 --> 00:36:28.750
HAL VARIAN: Of course.

00:36:28.750 --> 00:36:31.370
But at the same time,
what day of the year

00:36:31.370 --> 00:36:33.627
are the most queries
for hangover?

00:36:33.627 --> 00:36:34.710
RICHARD THALER: January 1.

00:36:34.710 --> 00:36:35.800
HAL VARIAN: Exactly.

00:36:35.800 --> 00:36:40.170
So it's a kind of demarcation
on both the future and the past.

00:36:40.170 --> 00:36:41.900
RICHARD THALER: Right.

00:36:41.900 --> 00:36:47.250
Notice, econs never have
hangovers, and never

00:36:47.250 --> 00:36:50.315
have to go on diets, because
they way the optimal amount.

00:36:53.160 --> 00:36:55.500
HAL VARIAN: So
I'm going to break

00:36:55.500 --> 00:36:57.030
for questions in just a minute.

00:36:57.030 --> 00:36:59.610
But I would like you to
say another word or two

00:36:59.610 --> 00:37:02.510
about the applications
of behavioral, economics,

00:37:02.510 --> 00:37:03.900
and finance.

00:37:03.900 --> 00:37:08.760
Before I do that, I want to
throw in another Google story.

00:37:08.760 --> 00:37:12.580
Back when we have
the IPO-- in 2004,

00:37:12.580 --> 00:37:18.430
I think-- we wanted
to give advice

00:37:18.430 --> 00:37:20.390
to all of the people
that were working

00:37:20.390 --> 00:37:23.930
at Google at that time of
how to do responsible money

00:37:23.930 --> 00:37:24.900
management, and so on.

00:37:24.900 --> 00:37:25.983
But we weren't allowed to.

00:37:25.983 --> 00:37:28.530
A company can't give
financial advice

00:37:28.530 --> 00:37:31.344
to its employees,
which is unfortunate,

00:37:31.344 --> 00:37:33.260
for the reasons you
describe, but you can also

00:37:33.260 --> 00:37:37.570
understand why it might make
sense to have such a rule.

00:37:37.570 --> 00:37:41.090
And my boss, at the time,
Jonathan Rosenberg, said, look,

00:37:41.090 --> 00:37:42.700
we've got to do something.

00:37:42.700 --> 00:37:46.030
Hal, will you organize a
seminar series-- sorry,

00:37:46.030 --> 00:37:47.430
a lecture series?

00:37:47.430 --> 00:37:50.300
So we brought in some of the
real luminaries of finance--

00:37:50.300 --> 00:37:53.460
Bill Sharp, and Burt
Malkiel, we got some people

00:37:53.460 --> 00:37:55.650
from real estate, and
in charitable giving,

00:37:55.650 --> 00:37:57.430
and a number of other topics.

00:37:57.430 --> 00:38:00.040
And gave these
tech talks, really,

00:38:00.040 --> 00:38:03.460
to the entire group of Googlers.

00:38:03.460 --> 00:38:05.960
And I can't tell you how
many times since then

00:38:05.960 --> 00:38:07.600
people have come up
to me and said what

00:38:07.600 --> 00:38:09.570
a great service that was to do.

00:38:09.570 --> 00:38:13.340
And I will say, it's Jonathan
who really had the idea.

00:38:13.340 --> 00:38:16.240
It was his vision, and I think
it was just a great help.

00:38:16.240 --> 00:38:18.960
And I wish more Silicon Valley
companies could do that.

00:38:18.960 --> 00:38:19.980
RICHARD THALER: Yeah.

00:38:19.980 --> 00:38:30.610
So financial markets-- the
conventional University

00:38:30.610 --> 00:38:34.380
of Chicago view of
financial markets

00:38:34.380 --> 00:38:36.945
is called the efficient
market hypothesis.

00:38:36.945 --> 00:38:42.520
It's coined by my friend
and colleague Gene Fama,

00:38:42.520 --> 00:38:45.930
and it has two components.

00:38:45.930 --> 00:38:48.670
One is that you can't
beat the market.

00:38:48.670 --> 00:38:51.510
You can't predict the
future from the past

00:38:51.510 --> 00:38:54.610
or from anything else,
because all information

00:38:54.610 --> 00:38:58.900
is impounded in today's price.

00:38:58.900 --> 00:39:02.310
And the second
component is what I

00:39:02.310 --> 00:39:05.590
call the price is
right component, which

00:39:05.590 --> 00:39:11.420
is that asset prices are equal
to their intrinsic value,

00:39:11.420 --> 00:39:13.185
whatever that is.

00:39:13.185 --> 00:39:17.280
It may be the net present
value of future cash flows,

00:39:17.280 --> 00:39:18.610
or something like that.

00:39:18.610 --> 00:39:21.160
HAL VARIAN: Just like our
optimal weight is [INAUDIBLE]

00:39:21.160 --> 00:39:21.690
value.

00:39:21.690 --> 00:39:22.880
RICHARD THALER: Right.

00:39:22.880 --> 00:39:30.650
So I think the first
part is not far off.

00:39:30.650 --> 00:39:33.410
I say that in spite
of the fact that I'm

00:39:33.410 --> 00:39:36.910
a principal in a money
management firm, located

00:39:36.910 --> 00:39:40.850
about 10 miles north
of here in San Mateo,

00:39:40.850 --> 00:39:44.790
that uses behavioral finance
to try and beat the market.

00:39:44.790 --> 00:39:49.660
And we are moderately
successful.

00:39:49.660 --> 00:39:54.870
Nevertheless, I don't advise
any of you to try to do it.

00:39:54.870 --> 00:40:00.680
And I do not own any
individual securities.

00:40:00.680 --> 00:40:02.430
I don't think I can do it.

00:40:02.430 --> 00:40:07.600
I think our guys can do it,
but they work full time on it,

00:40:07.600 --> 00:40:10.230
and they have disciplines
that we've given them,

00:40:10.230 --> 00:40:15.800
and they have access to
information that you don't.

00:40:15.800 --> 00:40:21.200
The second part of the
hypothesis-- prices

00:40:21.200 --> 00:40:23.480
are equal to intrinsic value.

00:40:23.480 --> 00:40:26.920
For a long time,
financial economists

00:40:26.920 --> 00:40:31.450
lived in the comfort of thinking
that that part of the theory

00:40:31.450 --> 00:40:33.260
was untestable.

00:40:33.260 --> 00:40:40.850
And there's no better feature
in a theory than untestability.

00:40:40.850 --> 00:40:45.490
I mean that's a really
comforting fact.

00:40:45.490 --> 00:40:51.610
But of course, everything turns
out to be testable in the end.

00:40:51.610 --> 00:40:55.590
And you need some
special circumstances

00:40:55.590 --> 00:41:00.790
to find obvious
violations of that.

00:41:00.790 --> 00:41:03.440
Let me give you a recent one.

00:41:03.440 --> 00:41:09.800
There is a closed
end mutual fund.

00:41:09.800 --> 00:41:13.430
I will give a 15 second
definition of a closed end

00:41:13.430 --> 00:41:17.610
mutual fund-- they sell
a fixed amount of money,

00:41:17.610 --> 00:41:20.770
and then the shares are traded.

00:41:20.770 --> 00:41:24.010
And you buy and sell them.

00:41:24.010 --> 00:41:27.660
And what that
means is the shares

00:41:27.660 --> 00:41:31.140
can trade at a price
different from the value

00:41:31.140 --> 00:41:35.760
of the asset they own, which
is already embarrassing

00:41:35.760 --> 00:41:38.500
to efficient market zealots.

00:41:38.500 --> 00:41:41.450
But there is a closed
end mutual fund.

00:41:41.450 --> 00:41:47.400
It happens to have the
ticker symbol C-U-B-A.

00:41:47.400 --> 00:41:54.890
Now, needless to say, it
has never, and cannot,

00:41:54.890 --> 00:41:58.995
invest in Cuba, in
spite of its name.

00:41:58.995 --> 00:42:05.240
It invests-- it's called the
Caribbean something fund.

00:42:05.240 --> 00:42:07.890
And it invests in things
like cruise lines,

00:42:07.890 --> 00:42:12.990
and companies in
Mexico, but not in Cuba.

00:42:12.990 --> 00:42:19.630
And has been trading at about
a 15% discount to its net asset

00:42:19.630 --> 00:42:22.780
value for several years.

00:42:22.780 --> 00:42:25.760
The day-- you can see
where this is going.

00:42:25.760 --> 00:42:32.030
The day that President Obama
made this announcement about

00:42:32.030 --> 00:42:42.580
relaxed terms with Cuba, the
C-U-B-A a fund jumped to a 70%,

00:42:42.580 --> 00:42:46.490
which means people were
paying $170 for $100 worth

00:42:46.490 --> 00:42:50.320
of securities that they
could have got for $85 a week

00:42:50.320 --> 00:42:52.150
earlier.

00:42:52.150 --> 00:42:54.624
That is not an efficient market.

00:42:54.624 --> 00:42:56.790
HAL VARIAN: Lucky for me,
I missed that opportunity.

00:42:56.790 --> 00:43:00.410
RICHARD THALER: Yeah,
well lucky for you

00:43:00.410 --> 00:43:02.540
that you didn't buy before.

00:43:06.260 --> 00:43:13.040
My former co-author's wife
is the ambassador to the UN.

00:43:13.040 --> 00:43:16.850
If we had gotten
a tip from her, we

00:43:16.850 --> 00:43:19.335
could afford to
pay our bills, Hal.

00:43:19.335 --> 00:43:19.960
He's suffering.

00:43:19.960 --> 00:43:24.875
HAL VARIAN: I think it's
time to open the time up

00:43:24.875 --> 00:43:26.390
for the audience questions.

00:43:26.390 --> 00:43:28.849
I'm sure there are people
who want to ask something.

00:43:28.849 --> 00:43:30.890
RICHARD THALER: So I think
you guys know the rig.

00:43:30.890 --> 00:43:37.166
Go to the mic so your questions
are preserved for posterity.

00:43:39.670 --> 00:43:40.980
HAL VARIAN: Go ahead.

00:43:40.980 --> 00:43:41.820
AUDIENCE: Hey there.

00:43:41.820 --> 00:43:45.150
So I had a question about
qualitative approaches

00:43:45.150 --> 00:43:49.140
to gathering some information
about misbehavior.

00:43:49.140 --> 00:43:52.070
User interviews, or
observational studies.

00:43:52.070 --> 00:43:54.880
Any way on how to
incorporate that with sort

00:43:54.880 --> 00:43:59.070
of the more quant data
you get from experiments.

00:43:59.070 --> 00:44:01.920
RICHARD THALER:
So, good question.

00:44:01.920 --> 00:44:03.475
I think the answer
is, it's hard.

00:44:07.550 --> 00:44:11.850
Take focus groups--
I think people

00:44:11.850 --> 00:44:15.550
who watch a focus group
think they've learned

00:44:15.550 --> 00:44:18.290
way more than they have.

00:44:18.290 --> 00:44:22.890
And, look-- it's small samples.

00:44:22.890 --> 00:44:30.340
And I use those kinds of
things to form hypotheses,

00:44:30.340 --> 00:44:34.780
and then I go test them
with large data sets.

00:44:34.780 --> 00:44:38.280
So I think it's great
to talk to people,

00:44:38.280 --> 00:44:40.110
and I think people
who are developing

00:44:40.110 --> 00:44:46.340
new software absolutely should
be watching real people use it.

00:44:46.340 --> 00:44:50.470
And seeing that what
was obvious to them

00:44:50.470 --> 00:44:55.160
wasn't obvious to somebody who
hadn't written the software.

00:44:55.160 --> 00:44:57.300
But then, you've got
to take it to scale.

00:45:00.779 --> 00:45:01.820
AUDIENCE: Hey, Professor.

00:45:01.820 --> 00:45:02.784
Good afternoon.

00:45:02.784 --> 00:45:03.950
I wanted to first thank you.

00:45:03.950 --> 00:45:05.740
I work in sales here
at Google, and I

00:45:05.740 --> 00:45:08.527
use anchoring all the time.

00:45:08.527 --> 00:45:09.610
RICHARD THALER: Good idea.

00:45:09.610 --> 00:45:13.340
AUDIENCE: My question is this--
given Google's enormous reach

00:45:13.340 --> 00:45:16.410
and how we're involved in so
many different ways in millions

00:45:16.410 --> 00:45:17.890
and billions of
people's lives, I

00:45:17.890 --> 00:45:19.306
was wondering if
were are any ways

00:45:19.306 --> 00:45:22.240
that you wish that we would
nudge people towards something

00:45:22.240 --> 00:45:22.740
at all?

00:45:25.806 --> 00:45:26.680
RICHARD THALER: Sure.

00:45:29.740 --> 00:45:33.010
Let's talk about
organ donations.

00:45:33.010 --> 00:45:36.970
And let me clear up a
misconception, first of all.

00:45:36.970 --> 00:45:41.780
So many people, even
those who've read Nudge,

00:45:41.780 --> 00:45:49.130
think that I endorse an opt
out solution to organ donation,

00:45:49.130 --> 00:45:55.740
meaning that-- this is sometimes
called presumed consent.

00:45:55.740 --> 00:45:59.550
We presume you give your
permission unless you opt out.

00:45:59.550 --> 00:46:01.360
I don't like that plan.

00:46:01.360 --> 00:46:03.140
The reason I don't
like that plan--

00:46:03.140 --> 00:46:06.070
it is the case that
almost no one opts out,

00:46:06.070 --> 00:46:08.200
so it has some appeal.

00:46:08.200 --> 00:46:12.140
The downside is that
in most countries,

00:46:12.140 --> 00:46:15.640
they don't really implement
the plan strictly.

00:46:15.640 --> 00:46:21.680
And so family members are
presented with an extremely

00:46:21.680 --> 00:46:23.510
difficult problem.

00:46:23.510 --> 00:46:26.530
A loved one has
died, often suddenly.

00:46:26.530 --> 00:46:31.280
They have no clue what
the donor's wishes were.

00:46:31.280 --> 00:46:37.700
And so I prefer what I
call prompted choice.

00:46:37.700 --> 00:46:41.160
And so, for example, in
the state of Illinois, when

00:46:41.160 --> 00:46:44.160
you renew your driver's
license, they ask you,

00:46:44.160 --> 00:46:46.430
would you like to
be an organ donor?

00:46:46.430 --> 00:46:47.990
Yes or no?

00:46:47.990 --> 00:46:55.115
And I like that better, because
now family members know--

00:46:55.115 --> 00:46:57.930
and, in fact, most
states also have

00:46:57.930 --> 00:47:01.470
a law called first
person consent, which

00:47:01.470 --> 00:47:05.180
means the donor's wishes count.

00:47:05.180 --> 00:47:11.470
So now the transplant team goes
to the family members, and say,

00:47:11.470 --> 00:47:13.760
our condolences
about your loved one,

00:47:13.760 --> 00:47:20.450
but you may be comforted
to know that your son

00:47:20.450 --> 00:47:26.120
or daughter wanted his
or her organs to be used.

00:47:26.120 --> 00:47:29.660
And it may prolong the
life of 10 other people,

00:47:29.660 --> 00:47:33.220
and they actually get
no say in the matter.

00:47:33.220 --> 00:47:37.410
If they throw a complete
fit, they usually win.

00:47:37.410 --> 00:47:40.240
But they usually don't.

00:47:40.240 --> 00:47:43.160
All right, so what does
this have to do with Google?

00:47:43.160 --> 00:47:46.325
Driver's licenses are
only one way to prompt.

00:47:50.840 --> 00:47:56.950
Hal and I both have spent
time writing columns

00:47:56.950 --> 00:47:59.280
for the New York Times.

00:47:59.280 --> 00:48:04.660
And I wrote one
on organ donation,

00:48:04.660 --> 00:48:09.640
and it was around the time that
Steve Jobs had gotten his liver

00:48:09.640 --> 00:48:11.180
transplant.

00:48:11.180 --> 00:48:16.590
And I challenged him
to make it as it easy

00:48:16.590 --> 00:48:21.700
to sign up to be an organ donor
as it was to download an app.

00:48:21.700 --> 00:48:24.750
And a week later,
there was an app.

00:48:28.390 --> 00:48:31.700
Jobs had nothing to do with
it, somebody had written it.

00:48:31.700 --> 00:48:35.110
So OK, here's where
Google comes in.

00:48:35.110 --> 00:48:40.430
Why not prompt people
to be organ donors?

00:48:40.430 --> 00:48:46.780
There is an app, and you guys
could create another one.

00:48:46.780 --> 00:48:50.220
Somebody here could
do it in a few days.

00:48:50.220 --> 00:48:54.990
All you need is a root into each
of the state-- every state has

00:48:54.990 --> 00:49:06.320
an online registry, and have a
once a year, a day-- an organ

00:49:06.320 --> 00:49:09.780
donation drive.

00:49:09.780 --> 00:49:12.970
And that could matter.

00:49:12.970 --> 00:49:18.700
If you want to really do it--
I mentioned this last night,

00:49:18.700 --> 00:49:20.360
and use the same phrase.

00:49:20.360 --> 00:49:25.640
Google the phrase,
immortal fans,

00:49:25.640 --> 00:49:30.410
and you will see an
extremely powerful video

00:49:30.410 --> 00:49:34.470
that could be used for
this organ donation drive.

00:49:34.470 --> 00:49:38.140
And if anyone in the room
takes this idea seriously,

00:49:38.140 --> 00:49:39.760
I would love to help you.

00:49:39.760 --> 00:49:41.880
Send me an email,
and we'll figure out

00:49:41.880 --> 00:49:44.245
how to make it happen.

00:49:44.245 --> 00:49:46.120
HAL VARIAN: There's
another interesting thing

00:49:46.120 --> 00:49:48.800
about organ donation,
and it also depends

00:49:48.800 --> 00:49:50.840
hugely on cultural norms.

00:49:50.840 --> 00:49:53.760
Great variation across
cultures, and not always the way

00:49:53.760 --> 00:49:54.930
you would think.

00:49:54.930 --> 00:49:56.660
It's sometimes
counter-intuitive ways.

00:49:56.660 --> 00:49:59.280
RICHARD THALER: But what's
true is, if you ask people,

00:49:59.280 --> 00:50:01.840
would you like to
be an organ donor?

00:50:01.840 --> 00:50:07.200
You at least 80% in
this country saying yes.

00:50:07.200 --> 00:50:11.320
So our goal should be to
get all the people who

00:50:11.320 --> 00:50:14.930
want to be organ donors
to be organ donors.

00:50:14.930 --> 00:50:19.780
Then the next step might-- we
maybe could get 80% up to 95%.

00:50:19.780 --> 00:50:23.620
But getting to 80% will
get most of the job done.

00:50:23.620 --> 00:50:24.495
Yeah?

00:50:24.495 --> 00:50:27.120
AUDIENCE: You mentioned briefly
inequality and Piketty earlier.

00:50:27.120 --> 00:50:28.720
And I was curious, because
one of the weird things

00:50:28.720 --> 00:50:30.570
about our current
inequality situation

00:50:30.570 --> 00:50:33.827
is, depending on how you phrase
the problem to Americans,

00:50:33.827 --> 00:50:35.410
you get very different
answers, right?

00:50:35.410 --> 00:50:36.950
A lot of Americans
don't really like

00:50:36.950 --> 00:50:38.700
the idea of redistribution,
even if they'd

00:50:38.700 --> 00:50:40.820
be the beneficiaries
of redistribution.

00:50:40.820 --> 00:50:42.930
But if you talk about
equality of opportunity,

00:50:42.930 --> 00:50:45.127
or better access to
education, or whatever,

00:50:45.127 --> 00:50:46.710
then they're kind
of excited about it.

00:50:46.710 --> 00:50:49.450
And so in a democracy,
where we're not

00:50:49.450 --> 00:50:51.529
voting for more
redistribution, do

00:50:51.529 --> 00:50:53.320
think there's partly
a behavioral economics

00:50:53.320 --> 00:50:56.090
explanation for why there's
that seeming mismatch,

00:50:56.090 --> 00:50:57.340
and what we could do about it?

00:50:57.340 --> 00:50:59.930
RICHARD THALER: You know,
politics is all about words

00:50:59.930 --> 00:51:00.990
and all about framing.

00:51:04.090 --> 00:51:12.340
The most successful
political phrase in my memory

00:51:12.340 --> 00:51:16.390
is the word, death tax.

00:51:16.390 --> 00:51:20.390
Now, there's never
been a death tax.

00:51:20.390 --> 00:51:23.490
You can die, and it
doesn't cost anything.

00:51:26.030 --> 00:51:29.570
But calling the
estate tax a death tax

00:51:29.570 --> 00:51:33.180
was extremely effective.

00:51:33.180 --> 00:51:39.010
And if you ask people, are
you in favor of a death tax?

00:51:39.010 --> 00:51:41.510
Everyone says no.

00:51:41.510 --> 00:51:44.990
And right now, you
only pay an estate tax

00:51:44.990 --> 00:51:47.620
if you have an estate,
for a married couple,

00:51:47.620 --> 00:51:49.280
of excess of $10 million.

00:51:49.280 --> 00:51:55.570
So this is not the 1%, this
is the point 0.01, or 0.001.

00:51:55.570 --> 00:52:01.240
So it's quite striking
that 90% of the people

00:52:01.240 --> 00:52:03.410
are opposed to something
that would have nothing

00:52:03.410 --> 00:52:04.610
to do with them.

00:52:04.610 --> 00:52:10.990
So politicians on whichever
side of the line you're on

00:52:10.990 --> 00:52:13.986
need to be concerned
about the words they use.

00:52:13.986 --> 00:52:15.360
Here's an
interesting-- there was

00:52:15.360 --> 00:52:19.820
something in The Economist
I saw this morning.

00:52:19.820 --> 00:52:23.490
And I may get a chance to
talk to some people in the UK

00:52:23.490 --> 00:52:25.760
about this summer.

00:52:25.760 --> 00:52:32.200
David Cameron has promised a
vote on in or out of the EU.

00:52:32.200 --> 00:52:36.660
He hasn't said what the
phrasing of that would be.

00:52:36.660 --> 00:52:40.880
And undoubtedly, the way
that question is worded

00:52:40.880 --> 00:52:44.070
will have a strong
influence on the outcome.

00:52:44.070 --> 00:52:46.610
I don't know which way
he wants it to come out,

00:52:46.610 --> 00:52:49.800
but I intend to find out.

00:52:49.800 --> 00:52:52.430
HAL VARIAN: By the way, econs
are in favor of a death tax.

00:52:52.430 --> 00:52:56.675
Because if you tax it,
there'd be less of it.

00:52:56.675 --> 00:52:58.120
RICHARD THALER: Nice.

00:52:58.120 --> 00:53:01.280
Dumb econs.

00:53:01.280 --> 00:53:03.570
AUDIENCE: What do you think
is the next breakthrough

00:53:03.570 --> 00:53:06.070
for behavioral economics
affecting policy?

00:53:06.070 --> 00:53:08.160
The next low hanging
fruit, as it were?

00:53:10.970 --> 00:53:13.300
RICHARD THALER: I'm not sure
what the next low hanging

00:53:13.300 --> 00:53:18.840
fruit is, but I can tell you
I end the book with my hope.

00:53:18.840 --> 00:53:24.950
And so my hope is that
there's a new wave

00:53:24.950 --> 00:53:28.180
of behavioral macroeconomics.

00:53:28.180 --> 00:53:33.385
And macroeconomics is the
field that needs the most work.

00:53:36.030 --> 00:53:41.930
The state of macroeconomics
is really pitiful,

00:53:41.930 --> 00:53:46.640
and we don't agree on
the most basic of things.

00:53:46.640 --> 00:53:52.460
So should Greece have increased
or decreased austerity?

00:53:55.510 --> 00:53:59.990
You'll get very strong
opinions on both sides of that.

00:53:59.990 --> 00:54:01.300
That's bad.

00:54:01.300 --> 00:54:05.550
We pretty much all agree
if you raise the price,

00:54:05.550 --> 00:54:07.160
people will buy less.

00:54:07.160 --> 00:54:08.780
That's microeconomics.

00:54:08.780 --> 00:54:11.575
Macroeconomics, we can't
agree on first principles.

00:54:14.110 --> 00:54:16.610
But of course,
macroeconomics is nothing

00:54:16.610 --> 00:54:21.120
more than microeconomics
plus summation signs.

00:54:21.120 --> 00:54:26.570
So we ought to be
summing up based

00:54:26.570 --> 00:54:31.510
on behaviorally
sound microeconomics,

00:54:31.510 --> 00:54:34.840
and I'm hoping
there are a bunch of

00:54:34.840 --> 00:54:37.800
smart young graduate
students out there

00:54:37.800 --> 00:54:40.409
that are going to do that.

00:54:40.409 --> 00:54:41.950
HAL VARIAN: And I'm
going to give you

00:54:41.950 --> 00:54:45.640
the last word on
this topic-- who

00:54:45.640 --> 00:54:49.006
is the best known behavioral
economist of the 20th century

00:54:49.006 --> 00:54:49.756
of macroeconomics?

00:54:53.030 --> 00:54:54.530
RICHARD THALER:
John Maynard Keynes.

00:54:54.530 --> 00:54:56.613
HAL VARIAN: Exactly, because
if you read the book,

00:54:56.613 --> 00:54:59.660
it's got chapter after chapter
full of astute observations

00:54:59.660 --> 00:55:01.240
about how people
actually behave.

00:55:01.240 --> 00:55:03.360
And so that's a good start.

00:55:03.360 --> 00:55:06.640
RICHARD THALER: Yeah, anyone
who this is here or watches

00:55:06.640 --> 00:55:11.950
this talk, if you want to go
be the next great behavioral

00:55:11.950 --> 00:55:16.089
macroeconomist, start
by reading Keynes.

00:55:16.089 --> 00:55:17.380
HAL VARIAN: Good place to stop.

00:55:17.380 --> 00:55:20.264
So thank you very much, and
thanks for coming, Richard.

00:55:20.264 --> 00:55:21.805
RICHARD THALER:
Thank you, thank you.

00:55:24.610 --> 00:55:28.170
We should do this every
day, we'll get good at it.

