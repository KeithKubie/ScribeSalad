WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:04.729
So what's RAID 1 reliability if we
do replace failed disks as soon as

00:00:04.729 --> 00:00:06.240
possible?

00:00:06.240 --> 00:00:11.500
We have the situation that both disks
are okay for this amount of time.

00:00:11.500 --> 00:00:16.450
Then a disk fails, and then we have
only one okay disk for the mean time to

00:00:16.450 --> 00:00:21.340
repair for one disk, so this is how long
does it take us to replace the disk?

00:00:21.340 --> 00:00:24.800
After that,
both disks are okay again, and

00:00:24.800 --> 00:00:27.820
then we again have this long
until the next failure.

00:00:27.820 --> 00:00:33.430
So what's the mean time to data loss for
RAID 1 with two disks?

00:00:33.430 --> 00:00:37.690
The question really is,
how many times can we have this

00:00:37.690 --> 00:00:41.800
until we encounter the period when
a single disk has failed, and

00:00:41.800 --> 00:00:46.800
during the time it takes to replace it,
another disk fails?

00:00:46.800 --> 00:00:49.826
Well, it's equal to
the length of this interval.

00:00:49.826 --> 00:00:55.090
This is how how long we survive
with both disks working, times,

00:00:55.090 --> 00:00:59.130
how many times can we get this until
we encounter a repair period where

00:00:59.130 --> 00:01:01.440
the second disk failed during it.

00:01:01.440 --> 00:01:06.130
So, the question is really what's the
probability of the second disk failing

00:01:06.130 --> 00:01:08.890
during the time to repair one disk.

00:01:08.890 --> 00:01:12.420
Now, normally, to calculate this
probability, we would have to compute

00:01:12.420 --> 00:01:17.380
the distribution for this function and
then see what's the probability for

00:01:17.380 --> 00:01:20.760
this distribution being under MTTR 1,
and so on.

00:01:20.760 --> 00:01:25.755
But when the time to repair
is a lot smaller then

00:01:25.755 --> 00:01:30.225
the time to failure of a single disk,
then we can approximate this

00:01:30.225 --> 00:01:34.015
probability of the second disk
failing during the MTTR for

00:01:34.015 --> 00:01:39.952
the first disk simply as
the MTTR divided by the MTTF.

00:01:39.952 --> 00:01:44.862
So, for example, if the MTTR is,
let's say, a single day, and

00:01:44.862 --> 00:01:50.002
MTTF is 100 days, then
the probability of the second failure

00:01:50.002 --> 00:01:54.902
during the repair period is
simply the length of the repair

00:01:54.902 --> 00:02:00.402
period divided by the period that
we expected this to survive.

00:02:00.402 --> 00:02:04.810
So for example, if the time to
repair is, for example, one day, and

00:02:04.810 --> 00:02:09.250
the time to failure is, let's say,
a hundred days, then we can expect that

00:02:09.250 --> 00:02:15.390
there is a one in 100 chance off of
this failing during the one day.

00:02:15.390 --> 00:02:20.760
So now how many times can we repair
this before we encounter one of these?

00:02:20.760 --> 00:02:24.100
Well, the number of
times we can try this

00:02:24.100 --> 00:02:28.550
until we encounter a failure is
going to be 1 over this probability.

00:02:28.550 --> 00:02:31.790
So what we finally get is this.

00:02:31.790 --> 00:02:37.500
The mean time to data loss is
MTTF of a single disk squared,

00:02:37.500 --> 00:02:41.410
divided by 2 times the mean
to repair for one disk.

00:02:41.410 --> 00:02:44.730
How long does it take us to replace
the disk and copy the data on it so

00:02:44.730 --> 00:02:47.430
that now we have a fully
functional RAID 1 array?

00:02:47.430 --> 00:02:50.820
And as a reminder, this is how long

00:02:50.820 --> 00:02:54.870
do we get a working array with
both these functional, and

00:02:54.870 --> 00:02:59.910
this is how many times do we do a repair
until we can expect that the second

00:02:59.910 --> 00:03:04.910
disk that is covering us during
the repair is going to fail on us.

00:03:04.910 --> 00:03:10.670
Not that because the MTTF Is
much larger than the MTTR,

00:03:10.670 --> 00:03:15.600
this is measured in years, usually,
this hopefully is hours or days.

00:03:15.600 --> 00:03:17.981
It's just how long does it
take us to replace the disk.

00:03:17.981 --> 00:03:22.530
This factor here is very,
very large, and now we get many,

00:03:22.530 --> 00:03:26.340
many times the MTTF of
a single disk over 2.

00:03:26.340 --> 00:03:30.530
This factor way larger than 2, which
means that the time to data loss with

00:03:30.530 --> 00:03:36.400
a RAID 1 array with only two disks is
way higher than the time to data loss

00:03:36.400 --> 00:03:41.290
for a single disk, which means that RAID
1 is dramatically improving reliability.

