WEBVTT
Kind: captions
Language: en

00:12:34.510 --> 00:14:58.460
 
you

00:14:58.460 --> 00:14:58.470
 
 

00:14:58.470 --> 00:15:32.150
 
you

00:15:32.150 --> 00:15:32.160
 
 

00:15:32.160 --> 00:15:45.129
 
[Music]

00:15:45.129 --> 00:15:45.139
 
 

00:15:45.139 --> 00:15:52.639
 
you

00:15:52.639 --> 00:15:52.649
 
 

00:15:52.649 --> 00:23:11.260
 
you

00:23:11.260 --> 00:23:11.270
 
 

00:23:11.270 --> 00:23:14.290
 
ladies and gentlemen Farnum jahannum

00:23:14.290 --> 00:23:14.300
ladies and gentlemen Farnum jahannum
 

00:23:14.300 --> 00:23:18.450
ladies and gentlemen Farnum jahannum
president Carnegie Mellon University

00:23:18.450 --> 00:23:18.460
president Carnegie Mellon University
 

00:23:18.460 --> 00:23:28.120
president Carnegie Mellon University
[Applause]

00:23:28.120 --> 00:23:28.130
 
 

00:23:28.130 --> 00:23:32.990
 
well good afternoon okay we're going to

00:23:32.990 --> 00:23:33.000
well good afternoon okay we're going to
 

00:23:33.000 --> 00:23:35.690
well good afternoon okay we're going to
try this again good afternoon

00:23:35.690 --> 00:23:35.700
try this again good afternoon
 

00:23:35.700 --> 00:23:39.710
try this again good afternoon
Oh much better much better it is my

00:23:39.710 --> 00:23:39.720
Oh much better much better it is my
 

00:23:39.720 --> 00:23:42.470
Oh much better much better it is my
great pleasure to extend a warm welcome

00:23:42.470 --> 00:23:42.480
great pleasure to extend a warm welcome
 

00:23:42.480 --> 00:23:46.880
great pleasure to extend a warm welcome
to all of you as we kick off this

00:23:46.880 --> 00:23:46.890
to all of you as we kick off this
 

00:23:46.890 --> 00:23:50.680
to all of you as we kick off this
inaugural CMU K&amp;L gates conference on

00:23:50.680 --> 00:23:50.690
inaugural CMU K&amp;L gates conference on
 

00:23:50.690 --> 00:23:53.260
inaugural CMU K&amp;L gates conference on
ethics and artificial intelligence

00:23:53.260 --> 00:23:53.270
ethics and artificial intelligence
 

00:23:53.270 --> 00:23:56.810
ethics and artificial intelligence
before I get started we pick this date

00:23:56.810 --> 00:23:56.820
before I get started we pick this date
 

00:23:56.820 --> 00:24:02.570
before I get started we pick this date
in April because we knew that this would

00:24:02.570 --> 00:24:02.580
in April because we knew that this would
 

00:24:02.580 --> 00:24:05.770
in April because we knew that this would
be the third week of spring and

00:24:05.770 --> 00:24:05.780
be the third week of spring and
 

00:24:05.780 --> 00:24:08.810
be the third week of spring and
especially for many of our friends and

00:24:08.810 --> 00:24:08.820
especially for many of our friends and
 

00:24:08.820 --> 00:24:11.390
especially for many of our friends and
colleagues from the Bay Area from the

00:24:11.390 --> 00:24:11.400
colleagues from the Bay Area from the
 

00:24:11.400 --> 00:24:15.290
colleagues from the Bay Area from the
west coast and from Texas we knew that

00:24:15.290 --> 00:24:15.300
west coast and from Texas we knew that
 

00:24:15.300 --> 00:24:18.650
west coast and from Texas we knew that
you wanted to enjoy spring in Pittsburgh

00:24:18.650 --> 00:24:18.660
you wanted to enjoy spring in Pittsburgh
 

00:24:18.660 --> 00:24:24.560
you wanted to enjoy spring in Pittsburgh
so you are welcome really happy to

00:24:24.560 --> 00:24:24.570
so you are welcome really happy to
 

00:24:24.570 --> 00:24:27.920
so you are welcome really happy to
welcome all of you on behalf of my

00:24:27.920 --> 00:24:27.930
welcome all of you on behalf of my
 

00:24:27.930 --> 00:24:29.570
welcome all of you on behalf of my
colleagues at Carnegie Mellon we're

00:24:29.570 --> 00:24:29.580
colleagues at Carnegie Mellon we're
 

00:24:29.580 --> 00:24:32.360
colleagues at Carnegie Mellon we're
joined here today by several of our

00:24:32.360 --> 00:24:32.370
joined here today by several of our
 

00:24:32.370 --> 00:24:34.400
joined here today by several of our
trustees and members of the university

00:24:34.400 --> 00:24:34.410
trustees and members of the university
 

00:24:34.410 --> 00:24:38.500
trustees and members of the university
leadership team and by a wide array of

00:24:38.500 --> 00:24:38.510
leadership team and by a wide array of
 

00:24:38.510 --> 00:24:42.020
leadership team and by a wide array of
distinguished CMU faculty and thought

00:24:42.020 --> 00:24:42.030
distinguished CMU faculty and thought
 

00:24:42.030 --> 00:24:44.630
distinguished CMU faculty and thought
leaders from across the country you will

00:24:44.630 --> 00:24:44.640
leaders from across the country you will
 

00:24:44.640 --> 00:24:47.270
leaders from across the country you will
hear from them throughout the day today

00:24:47.270 --> 00:24:47.280
hear from them throughout the day today
 

00:24:47.280 --> 00:24:50.900
hear from them throughout the day today
and tomorrow as one of Carnegie Mellon's

00:24:50.900 --> 00:24:50.910
and tomorrow as one of Carnegie Mellon's
 

00:24:50.910 --> 00:24:54.140
and tomorrow as one of Carnegie Mellon's
newest initiatives and annual traditions

00:24:54.140 --> 00:24:54.150
newest initiatives and annual traditions
 

00:24:54.150 --> 00:24:57.020
newest initiatives and annual traditions
this conference brings together thought

00:24:57.020 --> 00:24:57.030
this conference brings together thought
 

00:24:57.030 --> 00:24:59.240
this conference brings together thought
leaders from diverse perspectives to

00:24:59.240 --> 00:24:59.250
leaders from diverse perspectives to
 

00:24:59.250 --> 00:25:03.650
leaders from diverse perspectives to
discuss the ethical social and policy

00:25:03.650 --> 00:25:03.660
discuss the ethical social and policy
 

00:25:03.660 --> 00:25:06.980
discuss the ethical social and policy
ramifications brought on by advances in

00:25:06.980 --> 00:25:06.990
ramifications brought on by advances in
 

00:25:06.990 --> 00:25:08.330
ramifications brought on by advances in
artificial intelligence and

00:25:08.330 --> 00:25:08.340
artificial intelligence and
 

00:25:08.340 --> 00:25:11.450
artificial intelligence and
computational technologies this two-day

00:25:11.450 --> 00:25:11.460
computational technologies this two-day
 

00:25:11.460 --> 00:25:14.480
computational technologies this two-day
event has been made possible by an

00:25:14.480 --> 00:25:14.490
event has been made possible by an
 

00:25:14.490 --> 00:25:17.900
event has been made possible by an
exceptional coalition of support first I

00:25:17.900 --> 00:25:17.910
exceptional coalition of support first I
 

00:25:17.910 --> 00:25:19.670
exceptional coalition of support first I
want to take a moment to thank our

00:25:19.670 --> 00:25:19.680
want to take a moment to thank our
 

00:25:19.680 --> 00:25:23.780
want to take a moment to thank our
extraordinary partners at K&amp;L Gates ever

00:25:23.780 --> 00:25:23.790
extraordinary partners at K&amp;L Gates ever
 

00:25:23.790 --> 00:25:25.820
extraordinary partners at K&amp;L Gates ever
since we began discussing the

00:25:25.820 --> 00:25:25.830
since we began discussing the
 

00:25:25.830 --> 00:25:28.940
since we began discussing the
possibility of this new forum I've been

00:25:28.940 --> 00:25:28.950
possibility of this new forum I've been
 

00:25:28.950 --> 00:25:32.480
possibility of this new forum I've been
impressed by the thought and foresight

00:25:32.480 --> 00:25:32.490
impressed by the thought and foresight
 

00:25:32.490 --> 00:25:35.000
impressed by the thought and foresight
of our K&amp;L Gates partners

00:25:35.000 --> 00:25:35.010
of our K&amp;L Gates partners
 

00:25:35.010 --> 00:25:38.870
of our K&amp;L Gates partners
who understood the far-reaching the

00:25:38.870 --> 00:25:38.880
who understood the far-reaching the
 

00:25:38.880 --> 00:25:41.690
who understood the far-reaching the
effects of these new technologies not

00:25:41.690 --> 00:25:41.700
effects of these new technologies not
 

00:25:41.700 --> 00:25:44.390
effects of these new technologies not
only in the legal world but also

00:25:44.390 --> 00:25:44.400
only in the legal world but also
 

00:25:44.400 --> 00:25:46.610
only in the legal world but also
throughout society and they've been

00:25:46.610 --> 00:25:46.620
throughout society and they've been
 

00:25:46.620 --> 00:25:49.220
throughout society and they've been
willing to invest in work to make sure

00:25:49.220 --> 00:25:49.230
willing to invest in work to make sure
 

00:25:49.230 --> 00:25:51.440
willing to invest in work to make sure
that these technologies and these

00:25:51.440 --> 00:25:51.450
that these technologies and these
 

00:25:51.450 --> 00:25:53.450
that these technologies and these
unprecedented advances that we see

00:25:53.450 --> 00:25:53.460
unprecedented advances that we see
 

00:25:53.460 --> 00:25:56.530
unprecedented advances that we see
benefit society and benefits humanity

00:25:56.530 --> 00:25:56.540
benefit society and benefits humanity
 

00:25:56.540 --> 00:25:59.300
benefit society and benefits humanity
because of their generosity can all get

00:25:59.300 --> 00:25:59.310
because of their generosity can all get
 

00:25:59.310 --> 00:26:01.220
because of their generosity can all get
Endowment for Ethics and computational

00:26:01.220 --> 00:26:01.230
Endowment for Ethics and computational
 

00:26:01.230 --> 00:26:03.860
Endowment for Ethics and computational
technologies will advance research and

00:26:03.860 --> 00:26:03.870
technologies will advance research and
 

00:26:03.870 --> 00:26:05.950
technologies will advance research and
education in this critical domain and

00:26:05.950 --> 00:26:05.960
education in this critical domain and
 

00:26:05.960 --> 00:26:08.690
education in this critical domain and
foster dialogues like this week's

00:26:08.690 --> 00:26:08.700
foster dialogues like this week's
 

00:26:08.700 --> 00:26:10.940
foster dialogues like this week's
conference I'm delighted to welcome

00:26:10.940 --> 00:26:10.950
conference I'm delighted to welcome
 

00:26:10.950 --> 00:26:14.120
conference I'm delighted to welcome
several representatives from K&amp;L gates

00:26:14.120 --> 00:26:14.130
several representatives from K&amp;L gates
 

00:26:14.130 --> 00:26:17.570
several representatives from K&amp;L gates
this afternoon including Jim Segura

00:26:17.570 --> 00:26:17.580
this afternoon including Jim Segura
 

00:26:17.580 --> 00:26:19.970
this afternoon including Jim Segura
Dahle global managing partner who will

00:26:19.970 --> 00:26:19.980
Dahle global managing partner who will
 

00:26:19.980 --> 00:26:22.270
Dahle global managing partner who will
provide remarks in just a few minutes

00:26:22.270 --> 00:26:22.280
provide remarks in just a few minutes
 

00:26:22.280 --> 00:26:25.610
provide remarks in just a few minutes
Michael Casas chairman of the management

00:26:25.610 --> 00:26:25.620
Michael Casas chairman of the management
 

00:26:25.620 --> 00:26:28.700
Michael Casas chairman of the management
committee and david lehman partner and

00:26:28.700 --> 00:26:28.710
committee and david lehman partner and
 

00:26:28.710 --> 00:26:30.200
committee and david lehman partner and
co head of the K&amp;L gates

00:26:30.200 --> 00:26:30.210
co head of the K&amp;L gates
 

00:26:30.210 --> 00:26:32.510
co head of the K&amp;L gates
artificial intelligence initiative and i

00:26:32.510 --> 00:26:32.520
artificial intelligence initiative and i
 

00:26:32.520 --> 00:26:35.720
artificial intelligence initiative and i
should note that michael is also a proud

00:26:35.720 --> 00:26:35.730
should note that michael is also a proud
 

00:26:35.730 --> 00:26:40.310
should note that michael is also a proud
CMU alone dietrich a college class of 82

00:26:40.310 --> 00:26:40.320
CMU alone dietrich a college class of 82
 

00:26:40.320 --> 00:26:42.200
CMU alone dietrich a college class of 82
michael was 10 years old when he

00:26:42.200 --> 00:26:42.210
michael was 10 years old when he
 

00:26:42.210 --> 00:26:44.720
michael was 10 years old when he
graduated from college ladies and

00:26:44.720 --> 00:26:44.730
graduated from college ladies and
 

00:26:44.730 --> 00:26:46.070
graduated from college ladies and
gentlemen please join me in welcoming

00:26:46.070 --> 00:26:46.080
gentlemen please join me in welcoming
 

00:26:46.080 --> 00:26:48.320
gentlemen please join me in welcoming
and thanking all of our partners from

00:26:48.320 --> 00:26:48.330
and thanking all of our partners from
 

00:26:48.330 --> 00:26:49.250
and thanking all of our partners from
K&amp;L gates

00:26:49.250 --> 00:26:49.260
K&amp;L gates
 

00:26:49.260 --> 00:26:54.730
K&amp;L gates
[Applause]

00:26:54.730 --> 00:26:54.740
 
 

00:26:54.740 --> 00:26:57.919
 
I'm also police to acknowledge all of

00:26:57.919 --> 00:26:57.929
I'm also police to acknowledge all of
 

00:26:57.929 --> 00:27:00.140
I'm also police to acknowledge all of
our conference speakers and panelists

00:27:00.140 --> 00:27:00.150
our conference speakers and panelists
 

00:27:00.150 --> 00:27:02.870
our conference speakers and panelists
including my friend Eric Horvitz

00:27:02.870 --> 00:27:02.880
including my friend Eric Horvitz
 

00:27:02.880 --> 00:27:05.299
including my friend Eric Horvitz
director of Microsoft Research Lab who

00:27:05.299 --> 00:27:05.309
director of Microsoft Research Lab who
 

00:27:05.309 --> 00:27:07.669
director of Microsoft Research Lab who
is here today as our distinguished

00:27:07.669 --> 00:27:07.679
is here today as our distinguished
 

00:27:07.679 --> 00:27:10.970
is here today as our distinguished
speaker for this event welcome Eric pre

00:27:10.970 --> 00:27:10.980
speaker for this event welcome Eric pre
 

00:27:10.980 --> 00:27:13.070
speaker for this event welcome Eric pre
praying such an extraordinary agenda

00:27:13.070 --> 00:27:13.080
praying such an extraordinary agenda
 

00:27:13.080 --> 00:27:15.590
praying such an extraordinary agenda
requires exceptional leadership so I'm

00:27:15.590 --> 00:27:15.600
requires exceptional leadership so I'm
 

00:27:15.600 --> 00:27:17.450
requires exceptional leadership so I'm
grateful to the conference organizers

00:27:17.450 --> 00:27:17.460
grateful to the conference organizers
 

00:27:17.460 --> 00:27:20.260
grateful to the conference organizers
the steering committee and of course my

00:27:20.260 --> 00:27:20.270
the steering committee and of course my
 

00:27:20.270 --> 00:27:23.060
the steering committee and of course my
colleagues the co-chairs David Danks and

00:27:23.060 --> 00:27:23.070
colleagues the co-chairs David Danks and
 

00:27:23.070 --> 00:27:24.220
colleagues the co-chairs David Danks and
Eleanor Bach

00:27:24.220 --> 00:27:24.230
Eleanor Bach
 

00:27:24.230 --> 00:27:26.830
Eleanor Bach
finally this conference has been

00:27:26.830 --> 00:27:26.840
finally this conference has been
 

00:27:26.840 --> 00:27:29.390
finally this conference has been
supported by countless professionals

00:27:29.390 --> 00:27:29.400
supported by countless professionals
 

00:27:29.400 --> 00:27:32.840
supported by countless professionals
including dorothy robinson of counsel at

00:27:32.840 --> 00:27:32.850
including dorothy robinson of counsel at
 

00:27:32.850 --> 00:27:35.450
including dorothy robinson of counsel at
K&amp;L gates and the entire planning

00:27:35.450 --> 00:27:35.460
K&amp;L gates and the entire planning
 

00:27:35.460 --> 00:27:38.090
K&amp;L gates and the entire planning
committee at carnegie mellon and at kano

00:27:38.090 --> 00:27:38.100
committee at carnegie mellon and at kano
 

00:27:38.100 --> 00:27:40.940
committee at carnegie mellon and at kano
gates please join me in thanking all of

00:27:40.940 --> 00:27:40.950
gates please join me in thanking all of
 

00:27:40.950 --> 00:27:42.950
gates please join me in thanking all of
our special guests as well as our

00:27:42.950 --> 00:27:42.960
our special guests as well as our
 

00:27:42.960 --> 00:27:51.860
our special guests as well as our
conference organizers there is little

00:27:51.860 --> 00:27:51.870
conference organizers there is little
 

00:27:51.870 --> 00:27:54.260
conference organizers there is little
doubt that emerging technologies and

00:27:54.260 --> 00:27:54.270
doubt that emerging technologies and
 

00:27:54.270 --> 00:27:57.470
doubt that emerging technologies and
artificial intelligence or at the center

00:27:57.470 --> 00:27:57.480
artificial intelligence or at the center
 

00:27:57.480 --> 00:28:00.140
artificial intelligence or at the center
of an ongoing economic and societal

00:28:00.140 --> 00:28:00.150
of an ongoing economic and societal
 

00:28:00.150 --> 00:28:03.140
of an ongoing economic and societal
transformation that will no doubt will

00:28:03.140 --> 00:28:03.150
transformation that will no doubt will
 

00:28:03.150 --> 00:28:06.470
transformation that will no doubt will
continue for decades to come as we

00:28:06.470 --> 00:28:06.480
continue for decades to come as we
 

00:28:06.480 --> 00:28:08.650
continue for decades to come as we
embrace the Internet of Things

00:28:08.650 --> 00:28:08.660
embrace the Internet of Things
 

00:28:08.660 --> 00:28:11.960
embrace the Internet of Things
unprecedented access to massive amount

00:28:11.960 --> 00:28:11.970
unprecedented access to massive amount
 

00:28:11.970 --> 00:28:14.720
unprecedented access to massive amount
of data and the rise of automation and

00:28:14.720 --> 00:28:14.730
of data and the rise of automation and
 

00:28:14.730 --> 00:28:18.530
of data and the rise of automation and
robotics we're barreling toward a future

00:28:18.530 --> 00:28:18.540
robotics we're barreling toward a future
 

00:28:18.540 --> 00:28:22.190
robotics we're barreling toward a future
run by cyber enabled systems while these

00:28:22.190 --> 00:28:22.200
run by cyber enabled systems while these
 

00:28:22.200 --> 00:28:24.549
run by cyber enabled systems while these
technologies will enhance our comfort

00:28:24.549 --> 00:28:24.559
technologies will enhance our comfort
 

00:28:24.559 --> 00:28:29.330
technologies will enhance our comfort
security and quality of life for society

00:28:29.330 --> 00:28:29.340
security and quality of life for society
 

00:28:29.340 --> 00:28:32.390
security and quality of life for society
for for all of us their deployment has

00:28:32.390 --> 00:28:32.400
for for all of us their deployment has
 

00:28:32.400 --> 00:28:35.540
for for all of us their deployment has
had unprecedent consequences for our

00:28:35.540 --> 00:28:35.550
had unprecedent consequences for our
 

00:28:35.550 --> 00:28:38.290
had unprecedent consequences for our
workforce our education system

00:28:38.290 --> 00:28:38.300
workforce our education system
 

00:28:38.300 --> 00:28:40.640
workforce our education system
potentially for social justice for

00:28:40.640 --> 00:28:40.650
potentially for social justice for
 

00:28:40.650 --> 00:28:43.430
potentially for social justice for
fairness for privacy and many other

00:28:43.430 --> 00:28:43.440
fairness for privacy and many other
 

00:28:43.440 --> 00:28:46.400
fairness for privacy and many other
aspects of society the effect of

00:28:46.400 --> 00:28:46.410
aspects of society the effect of
 

00:28:46.410 --> 00:28:48.890
aspects of society the effect of
technology on society is not a brand new

00:28:48.890 --> 00:28:48.900
technology on society is not a brand new
 

00:28:48.900 --> 00:28:52.040
technology on society is not a brand new
concern but as we see these emerging

00:28:52.040 --> 00:28:52.050
concern but as we see these emerging
 

00:28:52.050 --> 00:28:55.669
concern but as we see these emerging
technologies pervade every aspect of our

00:28:55.669 --> 00:28:55.679
technologies pervade every aspect of our
 

00:28:55.679 --> 00:28:59.090
technologies pervade every aspect of our
lives and disrupt markets and disrupt

00:28:59.090 --> 00:28:59.100
lives and disrupt markets and disrupt
 

00:28:59.100 --> 00:29:02.690
lives and disrupt markets and disrupt
industries these issues have become even

00:29:02.690 --> 00:29:02.700
industries these issues have become even
 

00:29:02.700 --> 00:29:06.290
industries these issues have become even
more urgent consider for a moment if

00:29:06.290 --> 00:29:06.300
more urgent consider for a moment if
 

00:29:06.300 --> 00:29:08.300
more urgent consider for a moment if
new phenomena of tar technology driven

00:29:08.300 --> 00:29:08.310
new phenomena of tar technology driven
 

00:29:08.310 --> 00:29:12.260
new phenomena of tar technology driven
world as data has become a new currency

00:29:12.260 --> 00:29:12.270
world as data has become a new currency
 

00:29:12.270 --> 00:29:16.040
world as data has become a new currency
for business and for technology we're

00:29:16.040 --> 00:29:16.050
for business and for technology we're
 

00:29:16.050 --> 00:29:17.960
for business and for technology we're
seeing a dramatic surge in the

00:29:17.960 --> 00:29:17.970
seeing a dramatic surge in the
 

00:29:17.970 --> 00:29:22.060
seeing a dramatic surge in the
collection storage analysis and

00:29:22.060 --> 00:29:22.070
collection storage analysis and
 

00:29:22.070 --> 00:29:25.970
collection storage analysis and
monetization of our personal and often

00:29:25.970 --> 00:29:25.980
monetization of our personal and often
 

00:29:25.980 --> 00:29:30.530
monetization of our personal and often
most sensitive data meanwhile the most

00:29:30.530 --> 00:29:30.540
most sensitive data meanwhile the most
 

00:29:30.540 --> 00:29:32.660
most sensitive data meanwhile the most
cherished attributes of the internet and

00:29:32.660 --> 00:29:32.670
cherished attributes of the internet and
 

00:29:32.670 --> 00:29:36.890
cherished attributes of the internet and
the web its speed its reach its openness

00:29:36.890 --> 00:29:36.900
the web its speed its reach its openness
 

00:29:36.900 --> 00:29:40.520
the web its speed its reach its openness
and the notion of anonymity are being

00:29:40.520 --> 00:29:40.530
and the notion of anonymity are being
 

00:29:40.530 --> 00:29:43.700
and the notion of anonymity are being
used to undermine our democracy and our

00:29:43.700 --> 00:29:43.710
used to undermine our democracy and our
 

00:29:43.710 --> 00:29:47.450
used to undermine our democracy and our
civil liberties as AI enhances and

00:29:47.450 --> 00:29:47.460
civil liberties as AI enhances and
 

00:29:47.460 --> 00:29:50.810
civil liberties as AI enhances and
augments and in some cases outpaces

00:29:50.810 --> 00:29:50.820
augments and in some cases outpaces
 

00:29:50.820 --> 00:29:53.750
augments and in some cases outpaces
human capabilities human machine

00:29:53.750 --> 00:29:53.760
human capabilities human machine
 

00:29:53.760 --> 00:29:56.510
human capabilities human machine
interaction is reimagining the future of

00:29:56.510 --> 00:29:56.520
interaction is reimagining the future of
 

00:29:56.520 --> 00:30:01.210
interaction is reimagining the future of
work at the same time the very nature of

00:30:01.210 --> 00:30:01.220
work at the same time the very nature of
 

00:30:01.220 --> 00:30:05.120
work at the same time the very nature of
employment is involving skill cycles are

00:30:05.120 --> 00:30:05.130
employment is involving skill cycles are
 

00:30:05.130 --> 00:30:09.560
employment is involving skill cycles are
shorter than ever before consider for a

00:30:09.560 --> 00:30:09.570
shorter than ever before consider for a
 

00:30:09.570 --> 00:30:12.490
shorter than ever before consider for a
moment that according to a recent study

00:30:12.490 --> 00:30:12.500
moment that according to a recent study
 

00:30:12.500 --> 00:30:15.590
moment that according to a recent study
65% of the jobs that generation Z will

00:30:15.590 --> 00:30:15.600
65% of the jobs that generation Z will
 

00:30:15.600 --> 00:30:20.150
65% of the jobs that generation Z will
perform don't even exist today this will

00:30:20.150 --> 00:30:20.160
perform don't even exist today this will
 

00:30:20.160 --> 00:30:23.240
perform don't even exist today this will
have dramatic implications for educating

00:30:23.240 --> 00:30:23.250
have dramatic implications for educating
 

00:30:23.250 --> 00:30:25.490
have dramatic implications for educating
the next generation it's not just a

00:30:25.490 --> 00:30:25.500
the next generation it's not just a
 

00:30:25.500 --> 00:30:28.220
the next generation it's not just a
scale and ubiquity of these advances

00:30:28.220 --> 00:30:28.230
scale and ubiquity of these advances
 

00:30:28.230 --> 00:30:32.780
scale and ubiquity of these advances
that are remarkable the pace of advances

00:30:32.780 --> 00:30:32.790
that are remarkable the pace of advances
 

00:30:32.790 --> 00:30:35.570
that are remarkable the pace of advances
as well as the acceleration of their

00:30:35.570 --> 00:30:35.580
as well as the acceleration of their
 

00:30:35.580 --> 00:30:38.390
as well as the acceleration of their
economic impact is also unparalleled in

00:30:38.390 --> 00:30:38.400
economic impact is also unparalleled in
 

00:30:38.400 --> 00:30:41.720
economic impact is also unparalleled in
human history as an example it took

00:30:41.720 --> 00:30:41.730
human history as an example it took
 

00:30:41.730 --> 00:30:45.380
human history as an example it took
landline phones about 45 years to go

00:30:45.380 --> 00:30:45.390
landline phones about 45 years to go
 

00:30:45.390 --> 00:30:47.540
landline phones about 45 years to go
from five percent to 50 percent

00:30:47.540 --> 00:30:47.550
from five percent to 50 percent
 

00:30:47.550 --> 00:30:51.320
from five percent to 50 percent
penetration among US households by

00:30:51.320 --> 00:30:51.330
penetration among US households by
 

00:30:51.330 --> 00:30:53.660
penetration among US households by
comparison smart phones went from five

00:30:53.660 --> 00:30:53.670
comparison smart phones went from five
 

00:30:53.670 --> 00:30:56.390
comparison smart phones went from five
percent to 40 percent in about just four

00:30:56.390 --> 00:30:56.400
percent to 40 percent in about just four
 

00:30:56.400 --> 00:30:58.190
percent to 40 percent in about just four
years and that was during a recession

00:30:58.190 --> 00:30:58.200
years and that was during a recession
 

00:30:58.200 --> 00:31:01.910
years and that was during a recession
and in 2017 that number reached more

00:31:01.910 --> 00:31:01.920
and in 2017 that number reached more
 

00:31:01.920 --> 00:31:04.880
and in 2017 that number reached more
than 77 percent today we're at an

00:31:04.880 --> 00:31:04.890
than 77 percent today we're at an
 

00:31:04.890 --> 00:31:07.040
than 77 percent today we're at an
inflection point for the proliferation

00:31:07.040 --> 00:31:07.050
inflection point for the proliferation
 

00:31:07.050 --> 00:31:10.280
inflection point for the proliferation
of AI robotics and automation as this

00:31:10.280 --> 00:31:10.290
of AI robotics and automation as this
 

00:31:10.290 --> 00:31:13.010
of AI robotics and automation as this
innovation moves forward at warp speed

00:31:13.010 --> 00:31:13.020
innovation moves forward at warp speed
 

00:31:13.020 --> 00:31:15.980
innovation moves forward at warp speed
our deployment of ethics our development

00:31:15.980 --> 00:31:15.990
our deployment of ethics our development
 

00:31:15.990 --> 00:31:18.770
our deployment of ethics our development
of ethics I should say and policy must

00:31:18.770 --> 00:31:18.780
of ethics I should say and policy must
 

00:31:18.780 --> 00:31:19.940
of ethics I should say and policy must
keep up

00:31:19.940 --> 00:31:19.950
keep up
 

00:31:19.950 --> 00:31:22.430
keep up
now of course is a time for partners

00:31:22.430 --> 00:31:22.440
now of course is a time for partners
 

00:31:22.440 --> 00:31:24.350
now of course is a time for partners
from across public and private sectors

00:31:24.350 --> 00:31:24.360
from across public and private sectors
 

00:31:24.360 --> 00:31:27.590
from across public and private sectors
to come together with effective

00:31:27.590 --> 00:31:27.600
to come together with effective
 

00:31:27.600 --> 00:31:30.470
to come together with effective
communities to ensure that technology is

00:31:30.470 --> 00:31:30.480
communities to ensure that technology is
 

00:31:30.480 --> 00:31:34.010
communities to ensure that technology is
used to benefit humanity individually of

00:31:34.010 --> 00:31:34.020
used to benefit humanity individually of
 

00:31:34.020 --> 00:31:38.020
used to benefit humanity individually of
course as well as the entire society as

00:31:38.020 --> 00:31:38.030
course as well as the entire society as
 

00:31:38.030 --> 00:31:41.210
course as well as the entire society as
the university that has been intimately

00:31:41.210 --> 00:31:41.220
the university that has been intimately
 

00:31:41.220 --> 00:31:42.830
the university that has been intimately
involved in the creation and evolution

00:31:42.830 --> 00:31:42.840
involved in the creation and evolution
 

00:31:42.840 --> 00:31:45.890
involved in the creation and evolution
of artificial intelligence we've

00:31:45.890 --> 00:31:45.900
of artificial intelligence we've
 

00:31:45.900 --> 00:31:47.660
of artificial intelligence we've
definitely been responsible for some

00:31:47.660 --> 00:31:47.670
definitely been responsible for some
 

00:31:47.670 --> 00:31:50.960
definitely been responsible for some
really groundbreaking innovations but

00:31:50.960 --> 00:31:50.970
really groundbreaking innovations but
 

00:31:50.970 --> 00:31:52.250
really groundbreaking innovations but
we've also built an interdisciplinary

00:31:52.250 --> 00:31:52.260
we've also built an interdisciplinary
 

00:31:52.260 --> 00:31:54.740
we've also built an interdisciplinary
culture focused on making sure that

00:31:54.740 --> 00:31:54.750
culture focused on making sure that
 

00:31:54.750 --> 00:31:57.550
culture focused on making sure that
technological progress benefits society

00:31:57.550 --> 00:31:57.560
technological progress benefits society
 

00:31:57.560 --> 00:32:00.740
technological progress benefits society
through our partnerships we're K&amp;L gates

00:32:00.740 --> 00:32:00.750
through our partnerships we're K&amp;L gates
 

00:32:00.750 --> 00:32:02.990
through our partnerships we're K&amp;L gates
as well as our block center for

00:32:02.990 --> 00:32:03.000
as well as our block center for
 

00:32:03.000 --> 00:32:04.640
as well as our block center for
technology and society which we just

00:32:04.640 --> 00:32:04.650
technology and society which we just
 

00:32:04.650 --> 00:32:07.010
technology and society which we just
recently launched CMU and our

00:32:07.010 --> 00:32:07.020
recently launched CMU and our
 

00:32:07.020 --> 00:32:08.630
recently launched CMU and our
collaborators are helping to shape the

00:32:08.630 --> 00:32:08.640
collaborators are helping to shape the
 

00:32:08.640 --> 00:32:10.940
collaborators are helping to shape the
world in which people policy and

00:32:10.940 --> 00:32:10.950
world in which people policy and
 

00:32:10.950 --> 00:32:14.630
world in which people policy and
technology are better connected some of

00:32:14.630 --> 00:32:14.640
technology are better connected some of
 

00:32:14.640 --> 00:32:18.170
technology are better connected some of
CMU's most passionate partners have been

00:32:18.170 --> 00:32:18.180
CMU's most passionate partners have been
 

00:32:18.180 --> 00:32:20.330
CMU's most passionate partners have been
right here in Pittsburgh which has been

00:32:20.330 --> 00:32:20.340
right here in Pittsburgh which has been
 

00:32:20.340 --> 00:32:21.980
right here in Pittsburgh which has been
serving as a living laboratory for

00:32:21.980 --> 00:32:21.990
serving as a living laboratory for
 

00:32:21.990 --> 00:32:24.380
serving as a living laboratory for
technology that is integrated with

00:32:24.380 --> 00:32:24.390
technology that is integrated with
 

00:32:24.390 --> 00:32:28.100
technology that is integrated with
society our Mayor Bill Peduto has been

00:32:28.100 --> 00:32:28.110
society our Mayor Bill Peduto has been
 

00:32:28.110 --> 00:32:29.840
society our Mayor Bill Peduto has been
at the forefront of these collaborative

00:32:29.840 --> 00:32:29.850
at the forefront of these collaborative
 

00:32:29.850 --> 00:32:32.390
at the forefront of these collaborative
efforts and has established himself as a

00:32:32.390 --> 00:32:32.400
efforts and has established himself as a
 

00:32:32.400 --> 00:32:35.630
efforts and has established himself as a
national leader in progressive inclusive

00:32:35.630 --> 00:32:35.640
national leader in progressive inclusive
 

00:32:35.640 --> 00:32:38.780
national leader in progressive inclusive
and sustainable innovation of American

00:32:38.780 --> 00:32:38.790
and sustainable innovation of American
 

00:32:38.790 --> 00:32:41.570
and sustainable innovation of American
societies I'm delighted that he's able

00:32:41.570 --> 00:32:41.580
societies I'm delighted that he's able
 

00:32:41.580 --> 00:32:43.790
societies I'm delighted that he's able
to join us today to offer a few words

00:32:43.790 --> 00:32:43.800
to join us today to offer a few words
 

00:32:43.800 --> 00:32:46.460
to join us today to offer a few words
ladies and gentlemen please join me in

00:32:46.460 --> 00:32:46.470
ladies and gentlemen please join me in
 

00:32:46.470 --> 00:32:49.390
ladies and gentlemen please join me in
welcoming Pittsburgh Mayor Bill Peduto

00:32:49.390 --> 00:32:49.400
welcoming Pittsburgh Mayor Bill Peduto
 

00:32:49.400 --> 00:32:57.470
welcoming Pittsburgh Mayor Bill Peduto
[Applause]

00:32:57.470 --> 00:32:57.480
[Applause]
 

00:32:57.480 --> 00:33:01.320
[Applause]
Thank You mr. president and thank you to

00:33:01.320 --> 00:33:01.330
Thank You mr. president and thank you to
 

00:33:01.330 --> 00:33:03.920
Thank You mr. president and thank you to
Carnegie Mellon University and K&amp;L gates

00:33:03.920 --> 00:33:03.930
Carnegie Mellon University and K&amp;L gates
 

00:33:03.930 --> 00:33:07.350
Carnegie Mellon University and K&amp;L gates
for taking on this issue and being able

00:33:07.350 --> 00:33:07.360
for taking on this issue and being able
 

00:33:07.360 --> 00:33:09.960
for taking on this issue and being able
to have a discussion and I also want to

00:33:09.960 --> 00:33:09.970
to have a discussion and I also want to
 

00:33:09.970 --> 00:33:11.520
to have a discussion and I also want to
thank the person who wrote this speech

00:33:11.520 --> 00:33:11.530
thank the person who wrote this speech
 

00:33:11.530 --> 00:33:12.930
thank the person who wrote this speech
for me because it's one of the best

00:33:12.930 --> 00:33:12.940
for me because it's one of the best
 

00:33:12.940 --> 00:33:16.080
for me because it's one of the best
political speeches that I've not going

00:33:16.080 --> 00:33:16.090
political speeches that I've not going
 

00:33:16.090 --> 00:33:20.730
political speeches that I've not going
to read I always try to speak off the

00:33:20.730 --> 00:33:20.740
to read I always try to speak off the
 

00:33:20.740 --> 00:33:24.510
to read I always try to speak off the
cuff so let me explain what these next

00:33:24.510 --> 00:33:24.520
cuff so let me explain what these next
 

00:33:24.520 --> 00:33:27.270
cuff so let me explain what these next
two days mean for policy makers you know

00:33:27.270 --> 00:33:27.280
two days mean for policy makers you know
 

00:33:27.280 --> 00:33:30.830
two days mean for policy makers you know
Pittsburgh was an industry leader right

00:33:30.830 --> 00:33:30.840
Pittsburgh was an industry leader right
 

00:33:30.840 --> 00:33:34.590
Pittsburgh was an industry leader right
the industrial revolution during World

00:33:34.590 --> 00:33:34.600
the industrial revolution during World
 

00:33:34.600 --> 00:33:37.370
the industrial revolution during World
War two we produced more steel than

00:33:37.370 --> 00:33:37.380
War two we produced more steel than
 

00:33:37.380 --> 00:33:41.880
War two we produced more steel than
Germany in Japan combined and when we

00:33:41.880 --> 00:33:41.890
Germany in Japan combined and when we
 

00:33:41.890 --> 00:33:44.250
Germany in Japan combined and when we
built out this industry in this great

00:33:44.250 --> 00:33:44.260
built out this industry in this great
 

00:33:44.260 --> 00:33:47.280
built out this industry in this great
wealth that came to this region it came

00:33:47.280 --> 00:33:47.290
wealth that came to this region it came
 

00:33:47.290 --> 00:33:51.630
wealth that came to this region it came
at a very heavy cost we had air that was

00:33:51.630 --> 00:33:51.640
at a very heavy cost we had air that was
 

00:33:51.640 --> 00:33:54.720
at a very heavy cost we had air that was
dangerous to breathe we had water that

00:33:54.720 --> 00:33:54.730
dangerous to breathe we had water that
 

00:33:54.730 --> 00:33:57.540
dangerous to breathe we had water that
was poisonous to drink and we had the

00:33:57.540 --> 00:33:57.550
was poisonous to drink and we had the
 

00:33:57.550 --> 00:33:59.760
was poisonous to drink and we had the
greatest disparity between the people

00:33:59.760 --> 00:33:59.770
greatest disparity between the people
 

00:33:59.770 --> 00:34:02.070
greatest disparity between the people
who worked in those mines and mills and

00:34:02.070 --> 00:34:02.080
who worked in those mines and mills and
 

00:34:02.080 --> 00:34:05.490
who worked in those mines and mills and
those that owned and operated them the

00:34:05.490 --> 00:34:05.500
those that owned and operated them the
 

00:34:05.500 --> 00:34:07.500
those that owned and operated them the
greatest disparity in American history

00:34:07.500 --> 00:34:07.510
greatest disparity in American history
 

00:34:07.510 --> 00:34:10.470
greatest disparity in American history
and those were the costs that came with

00:34:10.470 --> 00:34:10.480
and those were the costs that came with
 

00:34:10.480 --> 00:34:13.230
and those were the costs that came with
it and it would take decades and decades

00:34:13.230 --> 00:34:13.240
it and it would take decades and decades
 

00:34:13.240 --> 00:34:16.080
it and it would take decades and decades
to be able to change that what we did

00:34:16.080 --> 00:34:16.090
to be able to change that what we did
 

00:34:16.090 --> 00:34:19.110
to be able to change that what we did
what Pittsburghers do we created the

00:34:19.110 --> 00:34:19.120
what Pittsburghers do we created the
 

00:34:19.120 --> 00:34:21.900
what Pittsburghers do we created the
first Clean Air Act in American history

00:34:21.900 --> 00:34:21.910
first Clean Air Act in American history
 

00:34:21.910 --> 00:34:25.320
first Clean Air Act in American history
and we went about to clean our air we

00:34:25.320 --> 00:34:25.330
and we went about to clean our air we
 

00:34:25.330 --> 00:34:27.960
and we went about to clean our air we
created public/private partnerships in

00:34:27.960 --> 00:34:27.970
created public/private partnerships in
 

00:34:27.970 --> 00:34:30.300
created public/private partnerships in
order to be able to clean our water and

00:34:30.300 --> 00:34:30.310
order to be able to clean our water and
 

00:34:30.310 --> 00:34:33.480
order to be able to clean our water and
we organized in those mines and in those

00:34:33.480 --> 00:34:33.490
we organized in those mines and in those
 

00:34:33.490 --> 00:34:35.970
we organized in those mines and in those
mills and in the process of building

00:34:35.970 --> 00:34:35.980
mills and in the process of building
 

00:34:35.980 --> 00:34:38.040
mills and in the process of building
America every Bridge and every

00:34:38.040 --> 00:34:38.050
America every Bridge and every
 

00:34:38.050 --> 00:34:41.600
America every Bridge and every
skyscraper we created the middle class

00:34:41.600 --> 00:34:41.610
skyscraper we created the middle class
 

00:34:41.610 --> 00:34:45.570
skyscraper we created the middle class
but they came as afterthoughts to the

00:34:45.570 --> 00:34:45.580
but they came as afterthoughts to the
 

00:34:45.580 --> 00:34:48.480
but they came as afterthoughts to the
Industrial Revolution not a component of

00:34:48.480 --> 00:34:48.490
Industrial Revolution not a component of
 

00:34:48.490 --> 00:34:50.490
Industrial Revolution not a component of
the Industrial Revolution

00:34:50.490 --> 00:34:50.500
the Industrial Revolution
 

00:34:50.500 --> 00:34:53.040
the Industrial Revolution
so as we find ourselves today in the

00:34:53.040 --> 00:34:53.050
so as we find ourselves today in the
 

00:34:53.050 --> 00:34:56.190
so as we find ourselves today in the
fourth Industrial Revolution we have to

00:34:56.190 --> 00:34:56.200
fourth Industrial Revolution we have to
 

00:34:56.200 --> 00:34:59.220
fourth Industrial Revolution we have to
look beyond simply what this technology

00:34:59.220 --> 00:34:59.230
look beyond simply what this technology
 

00:34:59.230 --> 00:35:01.800
look beyond simply what this technology
can provide the venture capitalists out

00:35:01.800 --> 00:35:01.810
can provide the venture capitalists out
 

00:35:01.810 --> 00:35:04.470
can provide the venture capitalists out
of California we have to think about

00:35:04.470 --> 00:35:04.480
of California we have to think about
 

00:35:04.480 --> 00:35:07.620
of California we have to think about
what it will provide to the people who

00:35:07.620 --> 00:35:07.630
what it will provide to the people who
 

00:35:07.630 --> 00:35:09.580
what it will provide to the people who
live in cities throughout

00:35:09.580 --> 00:35:09.590
live in cities throughout
 

00:35:09.590 --> 00:35:13.470
live in cities throughout
country we went through decades of

00:35:13.470 --> 00:35:13.480
country we went through decades of
 

00:35:13.480 --> 00:35:16.930
country we went through decades of
redlining out neighborhoods based upon

00:35:16.930 --> 00:35:16.940
redlining out neighborhoods based upon
 

00:35:16.940 --> 00:35:20.680
redlining out neighborhoods based upon
race and based upon income we cannot

00:35:20.680 --> 00:35:20.690
race and based upon income we cannot
 

00:35:20.690 --> 00:35:23.200
race and based upon income we cannot
read line out communities in this new

00:35:23.200 --> 00:35:23.210
read line out communities in this new
 

00:35:23.210 --> 00:35:23.950
read line out communities in this new
economy

00:35:23.950 --> 00:35:23.960
economy
 

00:35:23.960 --> 00:35:26.080
economy
based on whether or not you have a

00:35:26.080 --> 00:35:26.090
based on whether or not you have a
 

00:35:26.090 --> 00:35:27.100
based on whether or not you have a
cellphone

00:35:27.100 --> 00:35:27.110
cellphone
 

00:35:27.110 --> 00:35:30.280
cellphone
whether you or not you have credit we

00:35:30.280 --> 00:35:30.290
whether you or not you have credit we
 

00:35:30.290 --> 00:35:32.170
whether you or not you have credit we
have to be able to make sure everybody

00:35:32.170 --> 00:35:32.180
have to be able to make sure everybody
 

00:35:32.180 --> 00:35:35.290
have to be able to make sure everybody
is having that same opportunity and the

00:35:35.290 --> 00:35:35.300
is having that same opportunity and the
 

00:35:35.300 --> 00:35:38.500
is having that same opportunity and the
technology will help us to expand it we

00:35:38.500 --> 00:35:38.510
technology will help us to expand it we
 

00:35:38.510 --> 00:35:41.290
technology will help us to expand it we
have to look beyond just what the

00:35:41.290 --> 00:35:41.300
have to look beyond just what the
 

00:35:41.300 --> 00:35:43.420
have to look beyond just what the
technology will do in stark tech

00:35:43.420 --> 00:35:43.430
technology will do in stark tech
 

00:35:43.430 --> 00:35:46.330
technology will do in stark tech
understand how it can minimize negative

00:35:46.330 --> 00:35:46.340
understand how it can minimize negative
 

00:35:46.340 --> 00:35:48.850
understand how it can minimize negative
effects to the environment and we have

00:35:48.850 --> 00:35:48.860
effects to the environment and we have
 

00:35:48.860 --> 00:35:51.850
effects to the environment and we have
to look and see how it will enhance the

00:35:51.850 --> 00:35:51.860
to look and see how it will enhance the
 

00:35:51.860 --> 00:35:54.610
to look and see how it will enhance the
places we call home in Pittsburgh we

00:35:54.610 --> 00:35:54.620
places we call home in Pittsburgh we
 

00:35:54.620 --> 00:36:00.090
places we call home in Pittsburgh we
call it P for people planet Place and

00:36:00.090 --> 00:36:00.100
call it P for people planet Place and
 

00:36:00.100 --> 00:36:04.000
call it P for people planet Place and
performance a new metric for a 21st

00:36:04.000 --> 00:36:04.010
performance a new metric for a 21st
 

00:36:04.010 --> 00:36:07.810
performance a new metric for a 21st
century not old economics of the 19th

00:36:07.810 --> 00:36:07.820
century not old economics of the 19th
 

00:36:07.820 --> 00:36:10.960
century not old economics of the 19th
century a quadruple bottom line of being

00:36:10.960 --> 00:36:10.970
century a quadruple bottom line of being
 

00:36:10.970 --> 00:36:13.150
century a quadruple bottom line of being
able to understand how to measure

00:36:13.150 --> 00:36:13.160
able to understand how to measure
 

00:36:13.160 --> 00:36:17.050
able to understand how to measure
success and for policymakers it's

00:36:17.050 --> 00:36:17.060
success and for policymakers it's
 

00:36:17.060 --> 00:36:20.470
success and for policymakers it's
absolutely critical because when we look

00:36:20.470 --> 00:36:20.480
absolutely critical because when we look
 

00:36:20.480 --> 00:36:24.190
absolutely critical because when we look
at the ethics of where AI can take us it

00:36:24.190 --> 00:36:24.200
at the ethics of where AI can take us it
 

00:36:24.200 --> 00:36:28.630
at the ethics of where AI can take us it
goes beyond purchasing drones for a city

00:36:28.630 --> 00:36:28.640
goes beyond purchasing drones for a city
 

00:36:28.640 --> 00:36:31.720
goes beyond purchasing drones for a city
that will come up with issues of privacy

00:36:31.720 --> 00:36:31.730
that will come up with issues of privacy
 

00:36:31.730 --> 00:36:36.100
that will come up with issues of privacy
or being able to throw a drone behind a

00:36:36.100 --> 00:36:36.110
or being able to throw a drone behind a
 

00:36:36.110 --> 00:36:38.950
or being able to throw a drone behind a
wall and having somebody have already

00:36:38.950 --> 00:36:38.960
wall and having somebody have already
 

00:36:38.960 --> 00:36:41.530
wall and having somebody have already
have programmed when it takes a human

00:36:41.530 --> 00:36:41.540
have programmed when it takes a human
 

00:36:41.540 --> 00:36:44.590
have programmed when it takes a human
life at what point then do we start to

00:36:44.590 --> 00:36:44.600
life at what point then do we start to
 

00:36:44.600 --> 00:36:47.380
life at what point then do we start to
think about creating machines and

00:36:47.380 --> 00:36:47.390
think about creating machines and
 

00:36:47.390 --> 00:36:50.470
think about creating machines and
algorithms that have the ability to take

00:36:50.470 --> 00:36:50.480
algorithms that have the ability to take
 

00:36:50.480 --> 00:36:53.500
algorithms that have the ability to take
a human life we are at the forefront of

00:36:53.500 --> 00:36:53.510
a human life we are at the forefront of
 

00:36:53.510 --> 00:36:55.420
a human life we are at the forefront of
all of this coming together and it's

00:36:55.420 --> 00:36:55.430
all of this coming together and it's
 

00:36:55.430 --> 00:36:58.810
all of this coming together and it's
coming together so fast but if we don't

00:36:58.810 --> 00:36:58.820
coming together so fast but if we don't
 

00:36:58.820 --> 00:37:01.990
coming together so fast but if we don't
plan for the negative consequences in

00:37:01.990 --> 00:37:02.000
plan for the negative consequences in
 

00:37:02.000 --> 00:37:03.280
plan for the negative consequences in
the beginning

00:37:03.280 --> 00:37:03.290
the beginning
 

00:37:03.290 --> 00:37:06.190
the beginning
we'll have made the mistake that we made

00:37:06.190 --> 00:37:06.200
we'll have made the mistake that we made
 

00:37:06.200 --> 00:37:09.220
we'll have made the mistake that we made
in the industrial revolution and we will

00:37:09.220 --> 00:37:09.230
in the industrial revolution and we will
 

00:37:09.230 --> 00:37:12.100
in the industrial revolution and we will
spend decades trying to solve those

00:37:12.100 --> 00:37:12.110
spend decades trying to solve those
 

00:37:12.110 --> 00:37:14.490
spend decades trying to solve those
problems

00:37:14.490 --> 00:37:14.500
problems
 

00:37:14.500 --> 00:37:24.290
problems
[Applause]

00:37:24.290 --> 00:37:24.300
 
 

00:37:24.300 --> 00:37:26.790
 
thank you very much bill for your

00:37:26.790 --> 00:37:26.800
thank you very much bill for your
 

00:37:26.800 --> 00:37:29.339
thank you very much bill for your
leadership and for making Pittsburgh be

00:37:29.339 --> 00:37:29.349
leadership and for making Pittsburgh be
 

00:37:29.349 --> 00:37:32.490
leadership and for making Pittsburgh be
such an inclusive community I'm happy to

00:37:32.490 --> 00:37:32.500
such an inclusive community I'm happy to
 

00:37:32.500 --> 00:37:34.620
such an inclusive community I'm happy to
share with you that later this evening

00:37:34.620 --> 00:37:34.630
share with you that later this evening
 

00:37:34.630 --> 00:37:37.020
share with you that later this evening
we'll will celebrate the first two

00:37:37.020 --> 00:37:37.030
we'll will celebrate the first two
 

00:37:37.030 --> 00:37:40.680
we'll will celebrate the first two
recipients of new professorships funded

00:37:40.680 --> 00:37:40.690
recipients of new professorships funded
 

00:37:40.690 --> 00:37:43.980
recipients of new professorships funded
through their K&amp;L gates endowment these

00:37:43.980 --> 00:37:43.990
through their K&amp;L gates endowment these
 

00:37:43.990 --> 00:37:46.650
through their K&amp;L gates endowment these
recipients exemplify interdisciplinary

00:37:46.650 --> 00:37:46.660
recipients exemplify interdisciplinary
 

00:37:46.660 --> 00:37:48.480
recipients exemplify interdisciplinary
nature of these complex issues

00:37:48.480 --> 00:37:48.490
nature of these complex issues
 

00:37:48.490 --> 00:37:50.819
nature of these complex issues
the first is illenore Bock who is an

00:37:50.819 --> 00:37:50.829
the first is illenore Bock who is an
 

00:37:50.829 --> 00:37:52.880
the first is illenore Bock who is an
expert of course in our computer science

00:37:52.880 --> 00:37:52.890
expert of course in our computer science
 

00:37:52.890 --> 00:37:55.319
expert of course in our computer science
school while the other Molly write

00:37:55.319 --> 00:37:55.329
school while the other Molly write
 

00:37:55.329 --> 00:37:58.200
school while the other Molly write
Stinson investigates past and present

00:37:58.200 --> 00:37:58.210
Stinson investigates past and present
 

00:37:58.210 --> 00:38:00.690
Stinson investigates past and present
implications of AI and computation on

00:38:00.690 --> 00:38:00.700
implications of AI and computation on
 

00:38:00.700 --> 00:38:02.609
implications of AI and computation on
design and architecture with

00:38:02.609 --> 00:38:02.619
design and architecture with
 

00:38:02.619 --> 00:38:05.010
design and architecture with
appointments in our School of Design and

00:38:05.010 --> 00:38:05.020
appointments in our School of Design and
 

00:38:05.020 --> 00:38:07.109
appointments in our School of Design and
our school of architecture in addition

00:38:07.109 --> 00:38:07.119
our school of architecture in addition
 

00:38:07.119 --> 00:38:09.839
our school of architecture in addition
the K&amp;L Gates Presidential fellowship

00:38:09.839 --> 00:38:09.849
the K&amp;L Gates Presidential fellowship
 

00:38:09.849 --> 00:38:12.900
the K&amp;L Gates Presidential fellowship
endowed fund will support for

00:38:12.900 --> 00:38:12.910
endowed fund will support for
 

00:38:12.910 --> 00:38:16.040
endowed fund will support for
outstanding doctoral students veronica

00:38:16.040 --> 00:38:16.050
outstanding doctoral students veronica
 

00:38:16.050 --> 00:38:19.200
outstanding doctoral students veronica
allanté Zachary and Abigail whom you'll

00:38:19.200 --> 00:38:19.210
allanté Zachary and Abigail whom you'll
 

00:38:19.210 --> 00:38:20.750
allanté Zachary and Abigail whom you'll
meet later today whose work spans

00:38:20.750 --> 00:38:20.760
meet later today whose work spans
 

00:38:20.760 --> 00:38:23.520
meet later today whose work spans
information systems Computer Science

00:38:23.520 --> 00:38:23.530
information systems Computer Science
 

00:38:23.530 --> 00:38:27.059
information systems Computer Science
Engineering public policy and social and

00:38:27.059 --> 00:38:27.069
Engineering public policy and social and
 

00:38:27.069 --> 00:38:28.740
Engineering public policy and social and
decision science I'm delighted to

00:38:28.740 --> 00:38:28.750
decision science I'm delighted to
 

00:38:28.750 --> 00:38:30.870
decision science I'm delighted to
congratulate these pioneering scholars

00:38:30.870 --> 00:38:30.880
congratulate these pioneering scholars
 

00:38:30.880 --> 00:38:33.890
congratulate these pioneering scholars
on their prestigious professorship and

00:38:33.890 --> 00:38:33.900
on their prestigious professorship and
 

00:38:33.900 --> 00:38:35.640
on their prestigious professorship and
scholarship please join me in

00:38:35.640 --> 00:38:35.650
scholarship please join me in
 

00:38:35.650 --> 00:38:38.250
scholarship please join me in
congratulating all day awardees

00:38:38.250 --> 00:38:38.260
congratulating all day awardees
 

00:38:38.260 --> 00:38:41.039
congratulating all day awardees
[Applause]

00:38:41.039 --> 00:38:41.049
[Applause]
 

00:38:41.049 --> 00:38:44.049
[Applause]
as I mentioned earlier we are truly

00:38:44.049 --> 00:38:44.059
as I mentioned earlier we are truly
 

00:38:44.059 --> 00:38:46.000
as I mentioned earlier we are truly
fortunate to have K&amp;L gates as our

00:38:46.000 --> 00:38:46.010
fortunate to have K&amp;L gates as our
 

00:38:46.010 --> 00:38:49.000
fortunate to have K&amp;L gates as our
partner whose generous support will help

00:38:49.000 --> 00:38:49.010
partner whose generous support will help
 

00:38:49.010 --> 00:38:52.029
partner whose generous support will help
us help put us at the center of some of

00:38:52.029 --> 00:38:52.039
us help put us at the center of some of
 

00:38:52.039 --> 00:38:55.630
us help put us at the center of some of
the most pressing conversations facing

00:38:55.630 --> 00:38:55.640
the most pressing conversations facing
 

00:38:55.640 --> 00:38:57.640
the most pressing conversations facing
society at this time it is my great

00:38:57.640 --> 00:38:57.650
society at this time it is my great
 

00:38:57.650 --> 00:39:00.450
society at this time it is my great
pleasure to introduce Jim Sagar Hall as

00:39:00.450 --> 00:39:00.460
pleasure to introduce Jim Sagar Hall as
 

00:39:00.460 --> 00:39:03.069
pleasure to introduce Jim Sagar Hall as
global managing partner he serves as a

00:39:03.069 --> 00:39:03.079
global managing partner he serves as a
 

00:39:03.079 --> 00:39:04.990
global managing partner he serves as a
firm's chief executive officer and

00:39:04.990 --> 00:39:05.000
firm's chief executive officer and
 

00:39:05.000 --> 00:39:07.569
firm's chief executive officer and
native Pittsburgher so you can tell you

00:39:07.569 --> 00:39:07.579
native Pittsburgher so you can tell you
 

00:39:07.579 --> 00:39:08.380
native Pittsburgher so you can tell you
all about the weather

00:39:08.380 --> 00:39:08.390
all about the weather
 

00:39:08.390 --> 00:39:10.720
all about the weather
Jim is a board member of the Allegheny

00:39:10.720 --> 00:39:10.730
Jim is a board member of the Allegheny
 

00:39:10.730 --> 00:39:12.059
Jim is a board member of the Allegheny
Conference for Community Development

00:39:12.059 --> 00:39:12.069
Conference for Community Development
 

00:39:12.069 --> 00:39:14.740
Conference for Community Development
please join me in extending him a warm

00:39:14.740 --> 00:39:14.750
please join me in extending him a warm
 

00:39:14.750 --> 00:39:29.380
please join me in extending him a warm
welcome to Jim Saget all Thank You

00:39:29.380 --> 00:39:29.390
welcome to Jim Saget all Thank You
 

00:39:29.390 --> 00:39:31.569
welcome to Jim Saget all Thank You
president Johanna and I'm thrilled to be

00:39:31.569 --> 00:39:31.579
president Johanna and I'm thrilled to be
 

00:39:31.579 --> 00:39:34.960
president Johanna and I'm thrilled to be
here it's very exciting event just

00:39:34.960 --> 00:39:34.970
here it's very exciting event just
 

00:39:34.970 --> 00:39:37.329
here it's very exciting event just
terrific to be in the company of so many

00:39:37.329 --> 00:39:37.339
terrific to be in the company of so many
 

00:39:37.339 --> 00:39:40.750
terrific to be in the company of so many
brilliant people working on truly

00:39:40.750 --> 00:39:40.760
brilliant people working on truly
 

00:39:40.760 --> 00:39:44.430
brilliant people working on truly
important matters I am pleased and

00:39:44.430 --> 00:39:44.440
important matters I am pleased and
 

00:39:44.440 --> 00:39:46.960
important matters I am pleased and
honored to welcome all of you to the

00:39:46.960 --> 00:39:46.970
honored to welcome all of you to the
 

00:39:46.970 --> 00:39:49.210
honored to welcome all of you to the
first inaugural conference of ethics and

00:39:49.210 --> 00:39:49.220
first inaugural conference of ethics and
 

00:39:49.220 --> 00:39:51.519
first inaugural conference of ethics and
artificial intelligence which I am proud

00:39:51.519 --> 00:39:51.529
artificial intelligence which I am proud
 

00:39:51.529 --> 00:39:54.160
artificial intelligence which I am proud
to say is funded by the kano Gates

00:39:54.160 --> 00:39:54.170
to say is funded by the kano Gates
 

00:39:54.170 --> 00:39:56.799
to say is funded by the kano Gates
endowment for ethics and computational

00:39:56.799 --> 00:39:56.809
endowment for ethics and computational
 

00:39:56.809 --> 00:39:59.920
endowment for ethics and computational
technologies with this initiative we

00:39:59.920 --> 00:39:59.930
technologies with this initiative we
 

00:39:59.930 --> 00:40:02.730
technologies with this initiative we
honor not only our long-standing

00:40:02.730 --> 00:40:02.740
honor not only our long-standing
 

00:40:02.740 --> 00:40:05.740
honor not only our long-standing
relationship with CMU but also the

00:40:05.740 --> 00:40:05.750
relationship with CMU but also the
 

00:40:05.750 --> 00:40:07.990
relationship with CMU but also the
commitment of both organizations to be

00:40:07.990 --> 00:40:08.000
commitment of both organizations to be
 

00:40:08.000 --> 00:40:10.390
commitment of both organizations to be
at the forefront in furthering the

00:40:10.390 --> 00:40:10.400
at the forefront in furthering the
 

00:40:10.400 --> 00:40:12.370
at the forefront in furthering the
understanding of the opportunities and

00:40:12.370 --> 00:40:12.380
understanding of the opportunities and
 

00:40:12.380 --> 00:40:14.849
understanding of the opportunities and
the challenges presented by the

00:40:14.849 --> 00:40:14.859
the challenges presented by the
 

00:40:14.859 --> 00:40:17.710
the challenges presented by the
ever-evolving role of Technology in our

00:40:17.710 --> 00:40:17.720
ever-evolving role of Technology in our
 

00:40:17.720 --> 00:40:20.620
ever-evolving role of Technology in our
society including the role of artificial

00:40:20.620 --> 00:40:20.630
society including the role of artificial
 

00:40:20.630 --> 00:40:22.599
society including the role of artificial
intelligence machine learning and

00:40:22.599 --> 00:40:22.609
intelligence machine learning and
 

00:40:22.609 --> 00:40:25.299
intelligence machine learning and
robotics our law firm has had a long

00:40:25.299 --> 00:40:25.309
robotics our law firm has had a long
 

00:40:25.309 --> 00:40:27.789
robotics our law firm has had a long
relationship with CMU over many decades

00:40:27.789 --> 00:40:27.799
relationship with CMU over many decades
 

00:40:27.799 --> 00:40:30.490
relationship with CMU over many decades
that relationship has been a source of

00:40:30.490 --> 00:40:30.500
that relationship has been a source of
 

00:40:30.500 --> 00:40:33.460
that relationship has been a source of
tremendous pride for us although our

00:40:33.460 --> 00:40:33.470
tremendous pride for us although our
 

00:40:33.470 --> 00:40:35.380
tremendous pride for us although our
core missions are different we have

00:40:35.380 --> 00:40:35.390
core missions are different we have
 

00:40:35.390 --> 00:40:37.930
core missions are different we have
worked together shared civic involvement

00:40:37.930 --> 00:40:37.940
worked together shared civic involvement
 

00:40:37.940 --> 00:40:42.160
worked together shared civic involvement
and even shared leaders for example one

00:40:42.160 --> 00:40:42.170
and even shared leaders for example one
 

00:40:42.170 --> 00:40:43.990
and even shared leaders for example one
of my predecessors as Managing Partner

00:40:43.990 --> 00:40:44.000
of my predecessors as Managing Partner
 

00:40:44.000 --> 00:40:47.710
of my predecessors as Managing Partner
of kano Gates Chuck Queenan is a former

00:40:47.710 --> 00:40:47.720
of kano Gates Chuck Queenan is a former
 

00:40:47.720 --> 00:40:50.620
of kano Gates Chuck Queenan is a former
chair of the board at CMU he's currently

00:40:50.620 --> 00:40:50.630
chair of the board at CMU he's currently
 

00:40:50.630 --> 00:40:52.700
chair of the board at CMU he's currently
an emeritus trustee of the University

00:40:52.700 --> 00:40:52.710
an emeritus trustee of the University
 

00:40:52.710 --> 00:40:54.950
an emeritus trustee of the University
continues to provide wisdom and guidance

00:40:54.950 --> 00:40:54.960
continues to provide wisdom and guidance
 

00:40:54.960 --> 00:40:57.890
continues to provide wisdom and guidance
to both of our organizations with this

00:40:57.890 --> 00:40:57.900
to both of our organizations with this
 

00:40:57.900 --> 00:41:00.230
to both of our organizations with this
conference we have added another

00:41:00.230 --> 00:41:00.240
conference we have added another
 

00:41:00.240 --> 00:41:02.720
conference we have added another
dimension to that relationship as you

00:41:02.720 --> 00:41:02.730
dimension to that relationship as you
 

00:41:02.730 --> 00:41:04.970
dimension to that relationship as you
know it will be a biennial event and

00:41:04.970 --> 00:41:04.980
know it will be a biennial event and
 

00:41:04.980 --> 00:41:07.670
know it will be a biennial event and
will be an opportunity that we hope will

00:41:07.670 --> 00:41:07.680
will be an opportunity that we hope will
 

00:41:07.680 --> 00:41:10.579
will be an opportunity that we hope will
stimulate great discussion and important

00:41:10.579 --> 00:41:10.589
stimulate great discussion and important
 

00:41:10.589 --> 00:41:15.349
stimulate great discussion and important
dialogue you might reasonably ask why is

00:41:15.349 --> 00:41:15.359
dialogue you might reasonably ask why is
 

00:41:15.359 --> 00:41:18.140
dialogue you might reasonably ask why is
the field of ethics and intelligence

00:41:18.140 --> 00:41:18.150
the field of ethics and intelligence
 

00:41:18.150 --> 00:41:20.270
the field of ethics and intelligence
artificial intelligence important decay

00:41:20.270 --> 00:41:20.280
artificial intelligence important decay
 

00:41:20.280 --> 00:41:23.540
artificial intelligence important decay
no gates a law firm that's a reasonable

00:41:23.540 --> 00:41:23.550
no gates a law firm that's a reasonable
 

00:41:23.550 --> 00:41:26.540
no gates a law firm that's a reasonable
question our firms work and our

00:41:26.540 --> 00:41:26.550
question our firms work and our
 

00:41:26.550 --> 00:41:29.920
question our firms work and our
footprint is global as well as national

00:41:29.920 --> 00:41:29.930
footprint is global as well as national
 

00:41:29.930 --> 00:41:32.599
footprint is global as well as national
we like to think that we are a leader in

00:41:32.599 --> 00:41:32.609
we like to think that we are a leader in
 

00:41:32.609 --> 00:41:34.910
we like to think that we are a leader in
the practice of law as it relates to

00:41:34.910 --> 00:41:34.920
the practice of law as it relates to
 

00:41:34.920 --> 00:41:37.240
the practice of law as it relates to
technology innovation and development

00:41:37.240 --> 00:41:37.250
technology innovation and development
 

00:41:37.250 --> 00:41:39.890
technology innovation and development
we're fortunate to represent some of the

00:41:39.890 --> 00:41:39.900
we're fortunate to represent some of the
 

00:41:39.900 --> 00:41:41.630
we're fortunate to represent some of the
leading technology entities in the world

00:41:41.630 --> 00:41:41.640
leading technology entities in the world
 

00:41:41.640 --> 00:41:44.480
leading technology entities in the world
as well as numerous startup companies

00:41:44.480 --> 00:41:44.490
as well as numerous startup companies
 

00:41:44.490 --> 00:41:48.109
as well as numerous startup companies
that aspire to be in that sphere we care

00:41:48.109 --> 00:41:48.119
that aspire to be in that sphere we care
 

00:41:48.119 --> 00:41:49.880
that aspire to be in that sphere we care
about what our clients care about and

00:41:49.880 --> 00:41:49.890
about what our clients care about and
 

00:41:49.890 --> 00:41:52.520
about what our clients care about and
forward-looking clients everywhere care

00:41:52.520 --> 00:41:52.530
forward-looking clients everywhere care
 

00:41:52.530 --> 00:41:54.950
forward-looking clients everywhere care
about artificial intelligence and its

00:41:54.950 --> 00:41:54.960
about artificial intelligence and its
 

00:41:54.960 --> 00:41:57.530
about artificial intelligence and its
potential impact on their businesses and

00:41:57.530 --> 00:41:57.540
potential impact on their businesses and
 

00:41:57.540 --> 00:42:01.700
potential impact on their businesses and
society at large evidence of this is the

00:42:01.700 --> 00:42:01.710
society at large evidence of this is the
 

00:42:01.710 --> 00:42:04.490
society at large evidence of this is the
fact that our artificial intelligence

00:42:04.490 --> 00:42:04.500
fact that our artificial intelligence
 

00:42:04.500 --> 00:42:07.160
fact that our artificial intelligence
practice area now comprises over 80

00:42:07.160 --> 00:42:07.170
practice area now comprises over 80
 

00:42:07.170 --> 00:42:10.670
practice area now comprises over 80
lawyers the legal app the legal

00:42:10.670 --> 00:42:10.680
lawyers the legal app the legal
 

00:42:10.680 --> 00:42:13.250
lawyers the legal app the legal
implications of the application of AI on

00:42:13.250 --> 00:42:13.260
implications of the application of AI on
 

00:42:13.260 --> 00:42:15.799
implications of the application of AI on
our clients is broad deep and

00:42:15.799 --> 00:42:15.809
our clients is broad deep and
 

00:42:15.809 --> 00:42:18.289
our clients is broad deep and
interdisciplinary it will affect

00:42:18.289 --> 00:42:18.299
interdisciplinary it will affect
 

00:42:18.299 --> 00:42:20.390
interdisciplinary it will affect
numerous different practice areas for us

00:42:20.390 --> 00:42:20.400
numerous different practice areas for us
 

00:42:20.400 --> 00:42:22.460
numerous different practice areas for us
and for clients around the world

00:42:22.460 --> 00:42:22.470
and for clients around the world
 

00:42:22.470 --> 00:42:25.250
and for clients around the world
including mergers and acquisitions

00:42:25.250 --> 00:42:25.260
including mergers and acquisitions
 

00:42:25.260 --> 00:42:27.530
including mergers and acquisitions
healthcare the protection of

00:42:27.530 --> 00:42:27.540
healthcare the protection of
 

00:42:27.540 --> 00:42:30.200
healthcare the protection of
intellectual property employment law and

00:42:30.200 --> 00:42:30.210
intellectual property employment law and
 

00:42:30.210 --> 00:42:34.910
intellectual property employment law and
a host of regulatory issues further the

00:42:34.910 --> 00:42:34.920
a host of regulatory issues further the
 

00:42:34.920 --> 00:42:36.770
a host of regulatory issues further the
practice of law itself is undergoing

00:42:36.770 --> 00:42:36.780
practice of law itself is undergoing
 

00:42:36.780 --> 00:42:40.160
practice of law itself is undergoing
major shifts in AI increasingly will be

00:42:40.160 --> 00:42:40.170
major shifts in AI increasingly will be
 

00:42:40.170 --> 00:42:42.620
major shifts in AI increasingly will be
important to how legal services are

00:42:42.620 --> 00:42:42.630
important to how legal services are
 

00:42:42.630 --> 00:42:45.380
important to how legal services are
delivered to clients we approach what we

00:42:45.380 --> 00:42:45.390
delivered to clients we approach what we
 

00:42:45.390 --> 00:42:48.410
delivered to clients we approach what we
do in a forward-looking way with a

00:42:48.410 --> 00:42:48.420
do in a forward-looking way with a
 

00:42:48.420 --> 00:42:50.720
do in a forward-looking way with a
forward-looking mindset that's part of

00:42:50.720 --> 00:42:50.730
forward-looking mindset that's part of
 

00:42:50.730 --> 00:42:52.910
forward-looking mindset that's part of
our DNA and that's why we're so excited

00:42:52.910 --> 00:42:52.920
our DNA and that's why we're so excited
 

00:42:52.920 --> 00:42:54.770
our DNA and that's why we're so excited
to be part of this important discussion

00:42:54.770 --> 00:42:54.780
to be part of this important discussion
 

00:42:54.780 --> 00:42:58.640
to be part of this important discussion
and exchange of ideas all of us know

00:42:58.640 --> 00:42:58.650
and exchange of ideas all of us know
 

00:42:58.650 --> 00:43:00.440
and exchange of ideas all of us know
that advancements and computer related

00:43:00.440 --> 00:43:00.450
that advancements and computer related
 

00:43:00.450 --> 00:43:02.960
that advancements and computer related
technologies many of them born of

00:43:02.960 --> 00:43:02.970
technologies many of them born of
 

00:43:02.970 --> 00:43:04.490
technologies many of them born of
research here at Carnegie Mellon

00:43:04.490 --> 00:43:04.500
research here at Carnegie Mellon
 

00:43:04.500 --> 00:43:06.520
research here at Carnegie Mellon
University going back to her

00:43:06.520 --> 00:43:06.530
University going back to her
 

00:43:06.530 --> 00:43:08.440
University going back to her
Simon and his colleagues will

00:43:08.440 --> 00:43:08.450
Simon and his colleagues will
 

00:43:08.450 --> 00:43:10.180
Simon and his colleagues will
increasingly bring about profound

00:43:10.180 --> 00:43:10.190
increasingly bring about profound
 

00:43:10.190 --> 00:43:12.880
increasingly bring about profound
changes that affect our society and

00:43:12.880 --> 00:43:12.890
changes that affect our society and
 

00:43:12.890 --> 00:43:16.290
changes that affect our society and
humanity in many ways we can see that a

00:43:16.290 --> 00:43:16.300
humanity in many ways we can see that a
 

00:43:16.300 --> 00:43:19.270
humanity in many ways we can see that a
critical dimension to many of the

00:43:19.270 --> 00:43:19.280
critical dimension to many of the
 

00:43:19.280 --> 00:43:20.920
critical dimension to many of the
choices that will have to be made

00:43:20.920 --> 00:43:20.930
choices that will have to be made
 

00:43:20.930 --> 00:43:23.910
choices that will have to be made
whether implicitly or explicitly as

00:43:23.910 --> 00:43:23.920
whether implicitly or explicitly as
 

00:43:23.920 --> 00:43:27.820
whether implicitly or explicitly as
technology is developed is the ethical

00:43:27.820 --> 00:43:27.830
technology is developed is the ethical
 

00:43:27.830 --> 00:43:30.010
technology is developed is the ethical
dimension and this is going to be

00:43:30.010 --> 00:43:30.020
dimension and this is going to be
 

00:43:30.020 --> 00:43:33.190
dimension and this is going to be
important as our policymakers grapple

00:43:33.190 --> 00:43:33.200
important as our policymakers grapple
 

00:43:33.200 --> 00:43:35.470
important as our policymakers grapple
with difficult issue as time goes on and

00:43:35.470 --> 00:43:35.480
with difficult issue as time goes on and
 

00:43:35.480 --> 00:43:39.610
with difficult issue as time goes on and
as advances are made we all have a stake

00:43:39.610 --> 00:43:39.620
as advances are made we all have a stake
 

00:43:39.620 --> 00:43:41.440
as advances are made we all have a stake
in the outcome of these discussions

00:43:41.440 --> 00:43:41.450
in the outcome of these discussions
 

00:43:41.450 --> 00:43:44.260
in the outcome of these discussions
getting the best minds from across the

00:43:44.260 --> 00:43:44.270
getting the best minds from across the
 

00:43:44.270 --> 00:43:46.840
getting the best minds from across the
relevant fields of study from robotics

00:43:46.840 --> 00:43:46.850
relevant fields of study from robotics
 

00:43:46.850 --> 00:43:50.530
relevant fields of study from robotics
to philosophy in between and around to

00:43:50.530 --> 00:43:50.540
to philosophy in between and around to
 

00:43:50.540 --> 00:43:52.750
to philosophy in between and around to
focus on these problems is essential to

00:43:52.750 --> 00:43:52.760
focus on these problems is essential to
 

00:43:52.760 --> 00:43:55.410
focus on these problems is essential to
address in these matters efficiently and

00:43:55.410 --> 00:43:55.420
address in these matters efficiently and
 

00:43:55.420 --> 00:43:59.350
address in these matters efficiently and
ethically in making our gift to fund an

00:43:59.350 --> 00:43:59.360
ethically in making our gift to fund an
 

00:43:59.360 --> 00:44:01.330
ethically in making our gift to fund an
endowment and ethics and computational

00:44:01.330 --> 00:44:01.340
endowment and ethics and computational
 

00:44:01.340 --> 00:44:03.850
endowment and ethics and computational
technologies we made an investment for

00:44:03.850 --> 00:44:03.860
technologies we made an investment for
 

00:44:03.860 --> 00:44:06.640
technologies we made an investment for
the long term in an area important to us

00:44:06.640 --> 00:44:06.650
the long term in an area important to us
 

00:44:06.650 --> 00:44:09.790
the long term in an area important to us
as a law firm into society at large with

00:44:09.790 --> 00:44:09.800
as a law firm into society at large with
 

00:44:09.800 --> 00:44:11.410
as a law firm into society at large with
this conference we are pleased to open

00:44:11.410 --> 00:44:11.420
this conference we are pleased to open
 

00:44:11.420 --> 00:44:13.030
this conference we are pleased to open
this new forum for discussion and

00:44:13.030 --> 00:44:13.040
this new forum for discussion and
 

00:44:13.040 --> 00:44:15.250
this new forum for discussion and
development of the important work of the

00:44:15.250 --> 00:44:15.260
development of the important work of the
 

00:44:15.260 --> 00:44:18.310
development of the important work of the
speakers and participants welcome and I

00:44:18.310 --> 00:44:18.320
speakers and participants welcome and I
 

00:44:18.320 --> 00:44:19.960
speakers and participants welcome and I
hope you enjoy and benefit from the

00:44:19.960 --> 00:44:19.970
hope you enjoy and benefit from the
 

00:44:19.970 --> 00:44:21.550
hope you enjoy and benefit from the
conference

00:44:21.550 --> 00:44:21.560
conference
 

00:44:21.560 --> 00:44:36.000
conference
[Applause]

00:44:36.000 --> 00:44:36.010
 
 

00:44:36.010 --> 00:44:38.980
 
Thank You President Joe Haney and mr.

00:44:38.980 --> 00:44:38.990
Thank You President Joe Haney and mr.
 

00:44:38.990 --> 00:44:41.290
Thank You President Joe Haney and mr.
Sager doll and Mayor Peduto I'm David

00:44:41.290 --> 00:44:41.300
Sager doll and Mayor Peduto I'm David
 

00:44:41.300 --> 00:44:43.180
Sager doll and Mayor Peduto I'm David
Danks Thurston professor of philosophy

00:44:43.180 --> 00:44:43.190
Danks Thurston professor of philosophy
 

00:44:43.190 --> 00:44:47.800
Danks Thurston professor of philosophy
and psychology here at CMU and co-chair

00:44:47.800 --> 00:44:47.810
and psychology here at CMU and co-chair
 

00:44:47.810 --> 00:44:49.350
and psychology here at CMU and co-chair
of this event with Eleanor Bosch

00:44:49.350 --> 00:44:49.360
of this event with Eleanor Bosch
 

00:44:49.360 --> 00:44:51.640
of this event with Eleanor Bosch
currently professor of robotics for

00:44:51.640 --> 00:44:51.650
currently professor of robotics for
 

00:44:51.650 --> 00:44:53.920
currently professor of robotics for
about another two hours at which point

00:44:53.920 --> 00:44:53.930
about another two hours at which point
 

00:44:53.930 --> 00:44:56.050
about another two hours at which point
he becomes the inaugural K&amp;L gates

00:44:56.050 --> 00:44:56.060
he becomes the inaugural K&amp;L gates
 

00:44:56.060 --> 00:44:58.170
he becomes the inaugural K&amp;L gates
professor of ethics and computational

00:44:58.170 --> 00:44:58.180
professor of ethics and computational
 

00:44:58.180 --> 00:45:00.940
professor of ethics and computational
technologies and it is our great

00:45:00.940 --> 00:45:00.950
technologies and it is our great
 

00:45:00.950 --> 00:45:03.400
technologies and it is our great
pleasure to welcome you all to this

00:45:03.400 --> 00:45:03.410
pleasure to welcome you all to this
 

00:45:03.410 --> 00:45:05.670
pleasure to welcome you all to this
inaugural conference on ethics and AI

00:45:05.670 --> 00:45:05.680
inaugural conference on ethics and AI
 

00:45:05.680 --> 00:45:09.790
inaugural conference on ethics and AI
here in Pittsburgh I want to say a few

00:45:09.790 --> 00:45:09.800
here in Pittsburgh I want to say a few
 

00:45:09.800 --> 00:45:12.010
here in Pittsburgh I want to say a few
words about why this conference needs to

00:45:12.010 --> 00:45:12.020
words about why this conference needs to
 

00:45:12.020 --> 00:45:13.870
words about why this conference needs to
happen now and why it has to have the

00:45:13.870 --> 00:45:13.880
happen now and why it has to have the
 

00:45:13.880 --> 00:45:15.880
happen now and why it has to have the
format that it has no doubt you're all

00:45:15.880 --> 00:45:15.890
format that it has no doubt you're all
 

00:45:15.890 --> 00:45:17.170
format that it has no doubt you're all
well aware that artificial intelligence

00:45:17.170 --> 00:45:17.180
well aware that artificial intelligence
 

00:45:17.180 --> 00:45:19.840
well aware that artificial intelligence
has reached fever pitch in society today

00:45:19.840 --> 00:45:19.850
has reached fever pitch in society today
 

00:45:19.850 --> 00:45:22.210
has reached fever pitch in society today
thanks to certain companies what they've

00:45:22.210 --> 00:45:22.220
thanks to certain companies what they've
 

00:45:22.220 --> 00:45:24.310
thanks to certain companies what they've
done in terms of rushing forth with

00:45:24.310 --> 00:45:24.320
done in terms of rushing forth with
 

00:45:24.320 --> 00:45:26.890
done in terms of rushing forth with
deployments in the real world and thanks

00:45:26.890 --> 00:45:26.900
deployments in the real world and thanks
 

00:45:26.900 --> 00:45:27.940
deployments in the real world and thanks
to discourse by outstanding

00:45:27.940 --> 00:45:27.950
to discourse by outstanding
 

00:45:27.950 --> 00:45:29.590
to discourse by outstanding
investigative journalists who have

00:45:29.590 --> 00:45:29.600
investigative journalists who have
 

00:45:29.600 --> 00:45:31.060
investigative journalists who have
turned the public's attention to these

00:45:31.060 --> 00:45:31.070
turned the public's attention to these
 

00:45:31.070 --> 00:45:33.460
turned the public's attention to these
issues but the people in this room know

00:45:33.460 --> 00:45:33.470
issues but the people in this room know
 

00:45:33.470 --> 00:45:35.680
issues but the people in this room know
something else because many of our

00:45:35.680 --> 00:45:35.690
something else because many of our
 

00:45:35.690 --> 00:45:37.390
something else because many of our
friends who are here from policy makers

00:45:37.390 --> 00:45:37.400
friends who are here from policy makers
 

00:45:37.400 --> 00:45:39.880
friends who are here from policy makers
to business folks to researchers are

00:45:39.880 --> 00:45:39.890
to business folks to researchers are
 

00:45:39.890 --> 00:45:41.290
to business folks to researchers are
flying around the world at a breakneck

00:45:41.290 --> 00:45:41.300
flying around the world at a breakneck
 

00:45:41.300 --> 00:45:43.600
flying around the world at a breakneck
pace now between meetings and other

00:45:43.600 --> 00:45:43.610
pace now between meetings and other
 

00:45:43.610 --> 00:45:45.750
pace now between meetings and other
meetings discussing regulation

00:45:45.750 --> 00:45:45.760
meetings discussing regulation
 

00:45:45.760 --> 00:45:47.980
meetings discussing regulation
discussing entrepreneurship and venture

00:45:47.980 --> 00:45:47.990
discussing entrepreneurship and venture
 

00:45:47.990 --> 00:45:50.590
discussing entrepreneurship and venture
capital funding to AI discussing the

00:45:50.590 --> 00:45:50.600
capital funding to AI discussing the
 

00:45:50.600 --> 00:45:52.450
capital funding to AI discussing the
question of how we engineer ethics into

00:45:52.450 --> 00:45:52.460
question of how we engineer ethics into
 

00:45:52.460 --> 00:45:54.700
question of how we engineer ethics into
artificial intelligence systems and all

00:45:54.700 --> 00:45:54.710
artificial intelligence systems and all
 

00:45:54.710 --> 00:45:56.170
artificial intelligence systems and all
those meetings are fantastic and must

00:45:56.170 --> 00:45:56.180
those meetings are fantastic and must
 

00:45:56.180 --> 00:45:58.180
those meetings are fantastic and must
happen but this meeting has to happen

00:45:58.180 --> 00:45:58.190
happen but this meeting has to happen
 

00:45:58.190 --> 00:45:59.410
happen but this meeting has to happen
because there's a fundamental question

00:45:59.410 --> 00:45:59.420
because there's a fundamental question
 

00:45:59.420 --> 00:46:01.810
because there's a fundamental question
that is actually inherently

00:46:01.810 --> 00:46:01.820
that is actually inherently
 

00:46:01.820 --> 00:46:05.650
that is actually inherently
transdisciplinary what will artificial

00:46:05.650 --> 00:46:05.660
transdisciplinary what will artificial
 

00:46:05.660 --> 00:46:08.110
transdisciplinary what will artificial
intelligence do to our sense of human

00:46:08.110 --> 00:46:08.120
intelligence do to our sense of human
 

00:46:08.120 --> 00:46:11.410
intelligence do to our sense of human
identity what will we be in the age that

00:46:11.410 --> 00:46:11.420
identity what will we be in the age that
 

00:46:11.420 --> 00:46:13.900
identity what will we be in the age that
we can see already on the horizon that's

00:46:13.900 --> 00:46:13.910
we can see already on the horizon that's
 

00:46:13.910 --> 00:46:16.180
we can see already on the horizon that's
the fundamental question of identity and

00:46:16.180 --> 00:46:16.190
the fundamental question of identity and
 

00:46:16.190 --> 00:46:17.800
the fundamental question of identity and
the fundamental question of human

00:46:17.800 --> 00:46:17.810
the fundamental question of human
 

00:46:17.810 --> 00:46:20.200
the fundamental question of human
dignity and those are the issues that we

00:46:20.200 --> 00:46:20.210
dignity and those are the issues that we
 

00:46:20.210 --> 00:46:22.030
dignity and those are the issues that we
will grapple with during this conference

00:46:22.030 --> 00:46:22.040
will grapple with during this conference
 

00:46:22.040 --> 00:46:23.860
will grapple with during this conference
so we've designed a conference for you

00:46:23.860 --> 00:46:23.870
so we've designed a conference for you
 

00:46:23.870 --> 00:46:25.270
so we've designed a conference for you
that brings together outstanding

00:46:25.270 --> 00:46:25.280
that brings together outstanding
 

00:46:25.280 --> 00:46:27.700
that brings together outstanding
journalists and experts and sets them up

00:46:27.700 --> 00:46:27.710
journalists and experts and sets them up
 

00:46:27.710 --> 00:46:29.980
journalists and experts and sets them up
not to talk within Disciplinary confines

00:46:29.980 --> 00:46:29.990
not to talk within Disciplinary confines
 

00:46:29.990 --> 00:46:31.450
not to talk within Disciplinary confines
but rather to talk in a

00:46:31.450 --> 00:46:31.460
but rather to talk in a
 

00:46:31.460 --> 00:46:32.380
but rather to talk in a
transdisciplinary

00:46:32.380 --> 00:46:32.390
transdisciplinary
 

00:46:32.390 --> 00:46:34.540
transdisciplinary
manner about the core issues of identity

00:46:34.540 --> 00:46:34.550
manner about the core issues of identity
 

00:46:34.550 --> 00:46:37.450
manner about the core issues of identity
of humanity itself that's a conversation

00:46:37.450 --> 00:46:37.460
of humanity itself that's a conversation
 

00:46:37.460 --> 00:46:39.620
of humanity itself that's a conversation
that we think has to start now

00:46:39.620 --> 00:46:39.630
that we think has to start now
 

00:46:39.630 --> 00:46:41.690
that we think has to start now
has to repeat over and over again so

00:46:41.690 --> 00:46:41.700
has to repeat over and over again so
 

00:46:41.700 --> 00:46:44.630
has to repeat over and over again so
that's the conversation that you're part

00:46:44.630 --> 00:46:44.640
that's the conversation that you're part
 

00:46:44.640 --> 00:46:46.160
that's the conversation that you're part
of and that we're very happy to welcome

00:46:46.160 --> 00:46:46.170
of and that we're very happy to welcome
 

00:46:46.170 --> 00:46:49.970
of and that we're very happy to welcome
you to but enough about the highfalutin

00:46:49.970 --> 00:46:49.980
you to but enough about the highfalutin
 

00:46:49.980 --> 00:46:52.960
you to but enough about the highfalutin
stuff some logistical notes very quickly

00:46:52.960 --> 00:46:52.970
stuff some logistical notes very quickly
 

00:46:52.970 --> 00:46:55.340
stuff some logistical notes very quickly
you'll notice on all of the tables that

00:46:55.340 --> 00:46:55.350
you'll notice on all of the tables that
 

00:46:55.350 --> 00:46:57.830
you'll notice on all of the tables that
there are cubes that have the schedule

00:46:57.830 --> 00:46:57.840
there are cubes that have the schedule
 

00:46:57.840 --> 00:47:00.470
there are cubes that have the schedule
have information about Wi-Fi for those

00:47:00.470 --> 00:47:00.480
have information about Wi-Fi for those
 

00:47:00.480 --> 00:47:02.210
have information about Wi-Fi for those
who are interested in that Twitter

00:47:02.210 --> 00:47:02.220
who are interested in that Twitter
 

00:47:02.220 --> 00:47:05.810
who are interested in that Twitter
hashtag and so forth throughout the the

00:47:05.810 --> 00:47:05.820
hashtag and so forth throughout the the
 

00:47:05.820 --> 00:47:08.270
hashtag and so forth throughout the the
conference over the next two days there

00:47:08.270 --> 00:47:08.280
conference over the next two days there
 

00:47:08.280 --> 00:47:10.460
conference over the next two days there
will be opportunities for you as members

00:47:10.460 --> 00:47:10.470
will be opportunities for you as members
 

00:47:10.470 --> 00:47:12.920
will be opportunities for you as members
of the audience to participate in terms

00:47:12.920 --> 00:47:12.930
of the audience to participate in terms
 

00:47:12.930 --> 00:47:14.990
of the audience to participate in terms
of providing questions for panelists to

00:47:14.990 --> 00:47:15.000
of providing questions for panelists to
 

00:47:15.000 --> 00:47:17.300
of providing questions for panelists to
answer that will all be done using a

00:47:17.300 --> 00:47:17.310
answer that will all be done using a
 

00:47:17.310 --> 00:47:19.280
answer that will all be done using a
technology refer to a slide Oh an app

00:47:19.280 --> 00:47:19.290
technology refer to a slide Oh an app
 

00:47:19.290 --> 00:47:20.930
technology refer to a slide Oh an app
and there will be some more information

00:47:20.930 --> 00:47:20.940
and there will be some more information
 

00:47:20.940 --> 00:47:23.300
and there will be some more information
provided about that as we get to the

00:47:23.300 --> 00:47:23.310
provided about that as we get to the
 

00:47:23.310 --> 00:47:26.330
provided about that as we get to the
first panel the conference is being

00:47:26.330 --> 00:47:26.340
first panel the conference is being
 

00:47:26.340 --> 00:47:28.490
first panel the conference is being
live-streamed so if there are people

00:47:28.490 --> 00:47:28.500
live-streamed so if there are people
 

00:47:28.500 --> 00:47:29.990
live-streamed so if there are people
that you know wanted to participate but

00:47:29.990 --> 00:47:30.000
that you know wanted to participate but
 

00:47:30.000 --> 00:47:31.820
that you know wanted to participate but
we're unable to be here please let them

00:47:31.820 --> 00:47:31.830
we're unable to be here please let them
 

00:47:31.830 --> 00:47:32.570
we're unable to be here please let them
know about that

00:47:32.570 --> 00:47:32.580
know about that
 

00:47:32.580 --> 00:47:34.910
know about that
and the conference elements will all

00:47:34.910 --> 00:47:34.920
and the conference elements will all
 

00:47:34.920 --> 00:47:37.790
and the conference elements will all
over the coming weeks be rolled out on

00:47:37.790 --> 00:47:37.800
over the coming weeks be rolled out on
 

00:47:37.800 --> 00:47:40.160
over the coming weeks be rolled out on
the webpage so we encourage you to

00:47:40.160 --> 00:47:40.170
the webpage so we encourage you to
 

00:47:40.170 --> 00:47:43.250
the webpage so we encourage you to
continue to look back at that ok back to

00:47:43.250 --> 00:47:43.260
continue to look back at that ok back to
 

00:47:43.260 --> 00:47:46.370
continue to look back at that ok back to
the important things this conference as

00:47:46.370 --> 00:47:46.380
the important things this conference as
 

00:47:46.380 --> 00:47:48.800
the important things this conference as
ela said is about having a conversation

00:47:48.800 --> 00:47:48.810
ela said is about having a conversation
 

00:47:48.810 --> 00:47:51.650
ela said is about having a conversation
of the ways that technology particularly

00:47:51.650 --> 00:47:51.660
of the ways that technology particularly
 

00:47:51.660 --> 00:47:54.430
of the ways that technology particularly
AI and robotic technologies are

00:47:54.430 --> 00:47:54.440
AI and robotic technologies are
 

00:47:54.440 --> 00:47:56.960
AI and robotic technologies are
influencing impacting providing

00:47:56.960 --> 00:47:56.970
influencing impacting providing
 

00:47:56.970 --> 00:48:00.050
influencing impacting providing
opportunities and challenges for us

00:48:00.050 --> 00:48:00.060
opportunities and challenges for us
 

00:48:00.060 --> 00:48:03.200
opportunities and challenges for us
humans for societies for nations for

00:48:03.200 --> 00:48:03.210
humans for societies for nations for
 

00:48:03.210 --> 00:48:06.050
humans for societies for nations for
peoples and in particular we've decided

00:48:06.050 --> 00:48:06.060
peoples and in particular we've decided
 

00:48:06.060 --> 00:48:08.210
peoples and in particular we've decided
to organize this conference around four

00:48:08.210 --> 00:48:08.220
to organize this conference around four
 

00:48:08.220 --> 00:48:09.800
to organize this conference around four
different sessions each of which is

00:48:09.800 --> 00:48:09.810
different sessions each of which is
 

00:48:09.810 --> 00:48:12.680
different sessions each of which is
anchored in a particular human interest

00:48:12.680 --> 00:48:12.690
anchored in a particular human interest
 

00:48:12.690 --> 00:48:14.960
anchored in a particular human interest
or human value so rather than having a

00:48:14.960 --> 00:48:14.970
or human value so rather than having a
 

00:48:14.970 --> 00:48:17.720
or human value so rather than having a
session on AI and self-driving cars we

00:48:17.720 --> 00:48:17.730
session on AI and self-driving cars we
 

00:48:17.730 --> 00:48:20.270
session on AI and self-driving cars we
have sessions on equity of access and

00:48:20.270 --> 00:48:20.280
have sessions on equity of access and
 

00:48:20.280 --> 00:48:23.930
have sessions on equity of access and
equity of impact trust policy and

00:48:23.930 --> 00:48:23.940
equity of impact trust policy and
 

00:48:23.940 --> 00:48:26.330
equity of impact trust policy and
governance and agency and empowerment

00:48:26.330 --> 00:48:26.340
governance and agency and empowerment
 

00:48:26.340 --> 00:48:29.480
governance and agency and empowerment
which we hope will help to focus the

00:48:29.480 --> 00:48:29.490
which we hope will help to focus the
 

00:48:29.490 --> 00:48:31.430
which we hope will help to focus the
interdisciplinary and transdisciplinary

00:48:31.430 --> 00:48:31.440
interdisciplinary and transdisciplinary
 

00:48:31.440 --> 00:48:33.370
interdisciplinary and transdisciplinary
and multidisciplinary conversations

00:48:33.370 --> 00:48:33.380
and multidisciplinary conversations
 

00:48:33.380 --> 00:48:36.290
and multidisciplinary conversations
around these core human values and

00:48:36.290 --> 00:48:36.300
around these core human values and
 

00:48:36.300 --> 00:48:38.660
around these core human values and
interests so that we can start to make

00:48:38.660 --> 00:48:38.670
interests so that we can start to make
 

00:48:38.670 --> 00:48:40.940
interests so that we can start to make
real progress over the course of the

00:48:40.940 --> 00:48:40.950
real progress over the course of the
 

00:48:40.950 --> 00:48:44.090
real progress over the course of the
coming days and so with that said I'd

00:48:44.090 --> 00:48:44.100
coming days and so with that said I'd
 

00:48:44.100 --> 00:48:45.650
coming days and so with that said I'd
like to turn things over to ela to

00:48:45.650 --> 00:48:45.660
like to turn things over to ela to
 

00:48:45.660 --> 00:48:48.650
like to turn things over to ela to
introduce the first session thank you

00:48:48.650 --> 00:48:48.660
introduce the first session thank you
 

00:48:48.660 --> 00:48:49.040
introduce the first session thank you
David

00:48:49.040 --> 00:48:49.050
David
 

00:48:49.050 --> 00:48:51.110
David
session 1 we're gonna dive right into

00:48:51.110 --> 00:48:51.120
session 1 we're gonna dive right into
 

00:48:51.120 --> 00:48:53.300
session 1 we're gonna dive right into
this equity of access and they could

00:48:53.300 --> 00:48:53.310
this equity of access and they could
 

00:48:53.310 --> 00:48:55.760
this equity of access and they could
impact so let me just say 30 seconds

00:48:55.760 --> 00:48:55.770
impact so let me just say 30 seconds
 

00:48:55.770 --> 00:48:57.440
impact so let me just say 30 seconds
about that what are the interesting

00:48:57.440 --> 00:48:57.450
about that what are the interesting
 

00:48:57.450 --> 00:48:59.030
about that what are the interesting
things about artificial intelligence and

00:48:59.030 --> 00:48:59.040
things about artificial intelligence and
 

00:48:59.040 --> 00:49:01.070
things about artificial intelligence and
computational technologies writ large is

00:49:01.070 --> 00:49:01.080
computational technologies writ large is
 

00:49:01.080 --> 00:49:02.990
computational technologies writ large is
that they can increase the apparent

00:49:02.990 --> 00:49:03.000
that they can increase the apparent
 

00:49:03.000 --> 00:49:04.880
that they can increase the apparent
productivity of human society they can

00:49:04.880 --> 00:49:04.890
productivity of human society they can
 

00:49:04.890 --> 00:49:06.920
productivity of human society they can
allow us to do more they can allow us

00:49:06.920 --> 00:49:06.930
allow us to do more they can allow us
 

00:49:06.930 --> 00:49:09.350
allow us to do more they can allow us
new kinds of discoveries new spaces to

00:49:09.350 --> 00:49:09.360
new kinds of discoveries new spaces to
 

00:49:09.360 --> 00:49:11.810
new kinds of discoveries new spaces to
in which we can discover and create new

00:49:11.810 --> 00:49:11.820
in which we can discover and create new
 

00:49:11.820 --> 00:49:13.970
in which we can discover and create new
knowledge the challenge with that form

00:49:13.970 --> 00:49:13.980
knowledge the challenge with that form
 

00:49:13.980 --> 00:49:16.910
knowledge the challenge with that form
of innovation is of course stemming from

00:49:16.910 --> 00:49:16.920
of innovation is of course stemming from
 

00:49:16.920 --> 00:49:19.160
of innovation is of course stemming from
the fact that AI and information itself

00:49:19.160 --> 00:49:19.170
the fact that AI and information itself
 

00:49:19.170 --> 00:49:21.260
the fact that AI and information itself
has ownership and as a result as you

00:49:21.260 --> 00:49:21.270
has ownership and as a result as you
 

00:49:21.270 --> 00:49:24.110
has ownership and as a result as you
create new AI technologies new

00:49:24.110 --> 00:49:24.120
create new AI technologies new
 

00:49:24.120 --> 00:49:26.750
create new AI technologies new
innovations you have the danger that you

00:49:26.750 --> 00:49:26.760
innovations you have the danger that you
 

00:49:26.760 --> 00:49:28.520
innovations you have the danger that you
can limit the access to that AI

00:49:28.520 --> 00:49:28.530
can limit the access to that AI
 

00:49:28.530 --> 00:49:30.260
can limit the access to that AI
technology to the few you have the

00:49:30.260 --> 00:49:30.270
technology to the few you have the
 

00:49:30.270 --> 00:49:32.980
technology to the few you have the
further danger that you can in fact

00:49:32.980 --> 00:49:32.990
further danger that you can in fact
 

00:49:32.990 --> 00:49:35.570
further danger that you can in fact
augment the wealth of the few through

00:49:35.570 --> 00:49:35.580
augment the wealth of the few through
 

00:49:35.580 --> 00:49:37.280
augment the wealth of the few through
those very same technologies thus

00:49:37.280 --> 00:49:37.290
those very same technologies thus
 

00:49:37.290 --> 00:49:39.680
those very same technologies thus
increasing disparity between the those

00:49:39.680 --> 00:49:39.690
increasing disparity between the those
 

00:49:39.690 --> 00:49:41.870
increasing disparity between the those
who have and those who have not so

00:49:41.870 --> 00:49:41.880
who have and those who have not so
 

00:49:41.880 --> 00:49:43.460
who have and those who have not so
fundamentally we have on the one hand

00:49:43.460 --> 00:49:43.470
fundamentally we have on the one hand
 

00:49:43.470 --> 00:49:45.200
fundamentally we have on the one hand
technology that can make the world a

00:49:45.200 --> 00:49:45.210
technology that can make the world a
 

00:49:45.210 --> 00:49:45.890
technology that can make the world a
better place

00:49:45.890 --> 00:49:45.900
better place
 

00:49:45.900 --> 00:49:48.350
better place
on average the same time we have a

00:49:48.350 --> 00:49:48.360
on average the same time we have a
 

00:49:48.360 --> 00:49:50.360
on average the same time we have a
technology that while the average case

00:49:50.360 --> 00:49:50.370
technology that while the average case
 

00:49:50.370 --> 00:49:50.990
technology that while the average case
might be better

00:49:50.990 --> 00:49:51.000
might be better
 

00:49:51.000 --> 00:49:53.270
might be better
the extrema might be far worse and so

00:49:53.270 --> 00:49:53.280
the extrema might be far worse and so
 

00:49:53.280 --> 00:49:56.030
the extrema might be far worse and so
that fundamental challenge in terms of

00:49:56.030 --> 00:49:56.040
that fundamental challenge in terms of
 

00:49:56.040 --> 00:49:58.520
that fundamental challenge in terms of
both access and in terms of how the

00:49:58.520 --> 00:49:58.530
both access and in terms of how the
 

00:49:58.530 --> 00:50:00.530
both access and in terms of how the
world as a whole is impacted by AI

00:50:00.530 --> 00:50:00.540
world as a whole is impacted by AI
 

00:50:00.540 --> 00:50:02.720
world as a whole is impacted by AI
that's the focus of our first session

00:50:02.720 --> 00:50:02.730
that's the focus of our first session
 

00:50:02.730 --> 00:50:04.400
that's the focus of our first session
now each of our sessions is structured

00:50:04.400 --> 00:50:04.410
now each of our sessions is structured
 

00:50:04.410 --> 00:50:05.510
now each of our sessions is structured
differently so we think you're gonna

00:50:05.510 --> 00:50:05.520
differently so we think you're gonna
 

00:50:05.520 --> 00:50:07.190
differently so we think you're gonna
enjoy that there are spotlight speakers

00:50:07.190 --> 00:50:07.200
enjoy that there are spotlight speakers
 

00:50:07.200 --> 00:50:09.110
enjoy that there are spotlight speakers
there are videos that help to set the

00:50:09.110 --> 00:50:09.120
there are videos that help to set the
 

00:50:09.120 --> 00:50:11.300
there are videos that help to set the
stage and there are panel discussions in

00:50:11.300 --> 00:50:11.310
stage and there are panel discussions in
 

00:50:11.310 --> 00:50:13.370
stage and there are panel discussions in
this session we're gonna start with a

00:50:13.370 --> 00:50:13.380
this session we're gonna start with a
 

00:50:13.380 --> 00:50:15.290
this session we're gonna start with a
video and I'm going to ask to Q that

00:50:15.290 --> 00:50:15.300
video and I'm going to ask to Q that
 

00:50:15.300 --> 00:50:16.730
video and I'm going to ask to Q that
right away and then I'll introduce the

00:50:16.730 --> 00:50:16.740
right away and then I'll introduce the
 

00:50:16.740 --> 00:50:18.080
right away and then I'll introduce the
spotlight speaker to get things started

00:50:18.080 --> 00:50:18.090
spotlight speaker to get things started
 

00:50:18.090 --> 00:50:28.950
spotlight speaker to get things started
so let's run with the video please

00:50:28.950 --> 00:50:28.960
 
 

00:50:28.960 --> 00:50:33.490
 
we really as engineers need to take

00:50:33.490 --> 00:50:33.500
we really as engineers need to take
 

00:50:33.500 --> 00:50:35.410
we really as engineers need to take
responsibility to work on the things

00:50:35.410 --> 00:50:35.420
responsibility to work on the things
 

00:50:35.420 --> 00:50:37.000
responsibility to work on the things
that will make the world a better place

00:50:37.000 --> 00:50:37.010
that will make the world a better place
 

00:50:37.010 --> 00:50:39.970
that will make the world a better place
taking responsibility having a research

00:50:39.970 --> 00:50:39.980
taking responsibility having a research
 

00:50:39.980 --> 00:50:42.580
taking responsibility having a research
strategy instead of just going wherever

00:50:42.580 --> 00:50:42.590
strategy instead of just going wherever
 

00:50:42.590 --> 00:50:44.980
strategy instead of just going wherever
your curiosity takes you AI and

00:50:44.980 --> 00:50:44.990
your curiosity takes you AI and
 

00:50:44.990 --> 00:50:46.900
your curiosity takes you AI and
optimization have very different

00:50:46.900 --> 00:50:46.910
optimization have very different
 

00:50:46.910 --> 00:50:49.450
optimization have very different
strengths than humans do humans are

00:50:49.450 --> 00:50:49.460
strengths than humans do humans are
 

00:50:49.460 --> 00:50:51.280
strengths than humans do humans are
really good at actually figuring out

00:50:51.280 --> 00:50:51.290
really good at actually figuring out
 

00:50:51.290 --> 00:50:53.740
really good at actually figuring out
what the ins are what the value

00:50:53.740 --> 00:50:53.750
what the ins are what the value
 

00:50:53.750 --> 00:50:55.690
what the ins are what the value
judgments are in general how it should

00:50:55.690 --> 00:50:55.700
judgments are in general how it should
 

00:50:55.700 --> 00:50:57.220
judgments are in general how it should
trade-off between different things like

00:50:57.220 --> 00:50:57.230
trade-off between different things like
 

00:50:57.230 --> 00:50:59.260
trade-off between different things like
various forms of efficiency and various

00:50:59.260 --> 00:50:59.270
various forms of efficiency and various
 

00:50:59.270 --> 00:51:02.680
various forms of efficiency and various
forms of fairness but he must think of

00:51:02.680 --> 00:51:02.690
forms of fairness but he must think of
 

00:51:02.690 --> 00:51:05.650
forms of fairness but he must think of
them in the context of special cases and

00:51:05.650 --> 00:51:05.660
them in the context of special cases and
 

00:51:05.660 --> 00:51:08.440
them in the context of special cases and
humans are very bad at actually sifting

00:51:08.440 --> 00:51:08.450
humans are very bad at actually sifting
 

00:51:08.450 --> 00:51:10.480
humans are very bad at actually sifting
through all of the possible special

00:51:10.480 --> 00:51:10.490
through all of the possible special
 

00:51:10.490 --> 00:51:12.820
through all of the possible special
cases of which there are more than the

00:51:12.820 --> 00:51:12.830
cases of which there are more than the
 

00:51:12.830 --> 00:51:14.710
cases of which there are more than the
number of atoms in the universe we

00:51:14.710 --> 00:51:14.720
number of atoms in the universe we
 

00:51:14.720 --> 00:51:16.360
number of atoms in the universe we
developed this new framework which we

00:51:16.360 --> 00:51:16.370
developed this new framework which we
 

00:51:16.370 --> 00:51:19.120
developed this new framework which we
call future match where we take as input

00:51:19.120 --> 00:51:19.130
call future match where we take as input
 

00:51:19.130 --> 00:51:22.570
call future match where we take as input
to humans value judgments and then using

00:51:22.570 --> 00:51:22.580
to humans value judgments and then using
 

00:51:22.580 --> 00:51:27.760
to humans value judgments and then using
AI based simulation and past data we can

00:51:27.760 --> 00:51:27.770
AI based simulation and past data we can
 

00:51:27.770 --> 00:51:30.070
AI based simulation and past data we can
actually optimize the policy parameters

00:51:30.070 --> 00:51:30.080
actually optimize the policy parameters
 

00:51:30.080 --> 00:51:34.420
actually optimize the policy parameters
as to how do you best achieve those

00:51:34.420 --> 00:51:34.430
as to how do you best achieve those
 

00:51:34.430 --> 00:51:36.580
as to how do you best achieve those
goals so we're separating the means and

00:51:36.580 --> 00:51:36.590
goals so we're separating the means and
 

00:51:36.590 --> 00:51:38.500
goals so we're separating the means and
the ends the humans are talking about

00:51:38.500 --> 00:51:38.510
the ends the humans are talking about
 

00:51:38.510 --> 00:51:41.050
the ends the humans are talking about
ends and the AI is figuring out the

00:51:41.050 --> 00:51:41.060
ends and the AI is figuring out the
 

00:51:41.060 --> 00:51:43.030
ends and the AI is figuring out the
means and I think that this is a very

00:51:43.030 --> 00:51:43.040
means and I think that this is a very
 

00:51:43.040 --> 00:51:47.020
means and I think that this is a very
important separation for the future in

00:51:47.020 --> 00:51:47.030
important separation for the future in
 

00:51:47.030 --> 00:51:52.330
important separation for the future in
many different areas of AI that's almost

00:51:52.330 --> 00:51:52.340
many different areas of AI that's almost
 

00:51:52.340 --> 00:51:53.800
many different areas of AI that's almost
sent home our colleague right here at

00:51:53.800 --> 00:51:53.810
sent home our colleague right here at
 

00:51:53.810 --> 00:51:55.030
sent home our colleague right here at
Carnegie Mellon University and the

00:51:55.030 --> 00:51:55.040
Carnegie Mellon University and the
 

00:51:55.040 --> 00:51:56.950
Carnegie Mellon University and the
school of computer science now I'm very

00:51:56.950 --> 00:51:56.960
school of computer science now I'm very
 

00:51:56.960 --> 00:51:58.750
school of computer science now I'm very
pleased to introduce our first speaker

00:51:58.750 --> 00:51:58.760
pleased to introduce our first speaker
 

00:51:58.760 --> 00:52:00.730
pleased to introduce our first speaker
he's gonna set the stage for about 20

00:52:00.730 --> 00:52:00.740
he's gonna set the stage for about 20
 

00:52:00.740 --> 00:52:01.690
he's gonna set the stage for about 20
minutes and then we're gonna have a

00:52:01.690 --> 00:52:01.700
minutes and then we're gonna have a
 

00:52:01.700 --> 00:52:03.130
minutes and then we're gonna have a
little discussion the comfortable

00:52:03.130 --> 00:52:03.140
little discussion the comfortable
 

00:52:03.140 --> 00:52:05.230
little discussion the comfortable
armchairs in the middle our first

00:52:05.230 --> 00:52:05.240
armchairs in the middle our first
 

00:52:05.240 --> 00:52:08.140
armchairs in the middle our first
speaker is Sean dae-ho Shobha and I'm

00:52:08.140 --> 00:52:08.150
speaker is Sean dae-ho Shobha and I'm
 

00:52:08.150 --> 00:52:09.340
speaker is Sean dae-ho Shobha and I'm
happy to welcome him to the stage

00:52:09.340 --> 00:52:09.350
happy to welcome him to the stage
 

00:52:09.350 --> 00:52:11.260
happy to welcome him to the stage
what Shonda is an engineer at RAND

00:52:11.260 --> 00:52:11.270
what Shonda is an engineer at RAND
 

00:52:11.270 --> 00:52:12.850
what Shonda is an engineer at RAND
Corporation and he's a professor at the

00:52:12.850 --> 00:52:12.860
Corporation and he's a professor at the
 

00:52:12.860 --> 00:52:15.280
Corporation and he's a professor at the
party ran school his background is

00:52:15.280 --> 00:52:15.290
party ran school his background is
 

00:52:15.290 --> 00:52:16.840
party ran school his background is
perfect for this his background is in

00:52:16.840 --> 00:52:16.850
perfect for this his background is in
 

00:52:16.850 --> 00:52:18.760
perfect for this his background is in
machine learning optimization control

00:52:18.760 --> 00:52:18.770
machine learning optimization control
 

00:52:18.770 --> 00:52:21.460
machine learning optimization control
but he's also branched from machine

00:52:21.460 --> 00:52:21.470
but he's also branched from machine
 

00:52:21.470 --> 00:52:23.140
but he's also branched from machine
learning into health defense and

00:52:23.140 --> 00:52:23.150
learning into health defense and
 

00:52:23.150 --> 00:52:25.990
learning into health defense and
Technology Policy so from there he's

00:52:25.990 --> 00:52:26.000
Technology Policy so from there he's
 

00:52:26.000 --> 00:52:27.730
Technology Policy so from there he's
going to data privacy and accountability

00:52:27.730 --> 00:52:27.740
going to data privacy and accountability
 

00:52:27.740 --> 00:52:30.010
going to data privacy and accountability
so you notice I mentioned pretty much

00:52:30.010 --> 00:52:30.020
so you notice I mentioned pretty much
 

00:52:30.020 --> 00:52:32.230
so you notice I mentioned pretty much
all the key words as we look at the way

00:52:32.230 --> 00:52:32.240
all the key words as we look at the way
 

00:52:32.240 --> 00:52:34.030
all the key words as we look at the way
AI integrates into society

00:52:34.030 --> 00:52:34.040
AI integrates into society
 

00:52:34.040 --> 00:52:36.820
AI integrates into society
before joining Rand or Sunday was at USC

00:52:36.820 --> 00:52:36.830
before joining Rand or Sunday was at USC
 

00:52:36.830 --> 00:52:38.590
before joining Rand or Sunday was at USC
and indeed got his PhD there in

00:52:38.590 --> 00:52:38.600
and indeed got his PhD there in
 

00:52:38.600 --> 00:52:39.700
and indeed got his PhD there in
electrical engineering

00:52:39.700 --> 00:52:39.710
electrical engineering
 

00:52:39.710 --> 00:52:41.049
electrical engineering
and without further ado welcome ocean

00:52:41.049 --> 00:52:41.059
and without further ado welcome ocean
 

00:52:41.059 --> 00:52:50.140
and without further ado welcome ocean
day so today we're going to be talking

00:52:50.140 --> 00:52:50.150
day so today we're going to be talking
 

00:52:50.150 --> 00:52:51.790
day so today we're going to be talking
about equity of access and equity of

00:52:51.790 --> 00:52:51.800
about equity of access and equity of
 

00:52:51.800 --> 00:52:54.270
about equity of access and equity of
impact and how artificial intelligence

00:52:54.270 --> 00:52:54.280
impact and how artificial intelligence
 

00:52:54.280 --> 00:52:57.250
impact and how artificial intelligence
mediates transforms affects those two

00:52:57.250 --> 00:52:57.260
mediates transforms affects those two
 

00:52:57.260 --> 00:53:02.349
mediates transforms affects those two
things I will be suggesting that we look

00:53:02.349 --> 00:53:02.359
things I will be suggesting that we look
 

00:53:02.359 --> 00:53:05.230
things I will be suggesting that we look
to the Past look to antiquity to sort of

00:53:05.230 --> 00:53:05.240
to the Past look to antiquity to sort of
 

00:53:05.240 --> 00:53:07.420
to the Past look to antiquity to sort of
help us see what's going on in the

00:53:07.420 --> 00:53:07.430
help us see what's going on in the
 

00:53:07.430 --> 00:53:10.450
help us see what's going on in the
future I want us to focus on these two

00:53:10.450 --> 00:53:10.460
future I want us to focus on these two
 

00:53:10.460 --> 00:53:15.670
future I want us to focus on these two
individuals Plato and Socrates my

00:53:15.670 --> 00:53:15.680
individuals Plato and Socrates my
 

00:53:15.680 --> 00:53:17.980
individuals Plato and Socrates my
contention is that the interaction the

00:53:17.980 --> 00:53:17.990
contention is that the interaction the
 

00:53:17.990 --> 00:53:19.210
contention is that the interaction the
difference is in the way these two

00:53:19.210 --> 00:53:19.220
difference is in the way these two
 

00:53:19.220 --> 00:53:21.730
difference is in the way these two
people work kind of sort of pre figures

00:53:21.730 --> 00:53:21.740
people work kind of sort of pre figures
 

00:53:21.740 --> 00:53:24.400
people work kind of sort of pre figures
are complexities the complexity of

00:53:24.400 --> 00:53:24.410
are complexities the complexity of
 

00:53:24.410 --> 00:53:26.349
are complexities the complexity of
dealing with today when it comes to AI

00:53:26.349 --> 00:53:26.359
dealing with today when it comes to AI
 

00:53:26.359 --> 00:53:30.220
dealing with today when it comes to AI
and equity if there is a key takeaway

00:53:30.220 --> 00:53:30.230
and equity if there is a key takeaway
 

00:53:30.230 --> 00:53:32.799
and equity if there is a key takeaway
point from from this conversation I hope

00:53:32.799 --> 00:53:32.809
point from from this conversation I hope
 

00:53:32.809 --> 00:53:35.319
point from from this conversation I hope
it would be the idea of participatory

00:53:35.319 --> 00:53:35.329
it would be the idea of participatory
 

00:53:35.329 --> 00:53:38.020
it would be the idea of participatory
equity I will be making the contention

00:53:38.020 --> 00:53:38.030
equity I will be making the contention
 

00:53:38.030 --> 00:53:39.520
equity I will be making the contention
that that should be the foundation of

00:53:39.520 --> 00:53:39.530
that that should be the foundation of
 

00:53:39.530 --> 00:53:41.950
that that should be the foundation of
any successful infrastructure for

00:53:41.950 --> 00:53:41.960
any successful infrastructure for
 

00:53:41.960 --> 00:53:46.620
any successful infrastructure for
ethically aligned what fish or fair AI

00:53:46.620 --> 00:53:46.630
ethically aligned what fish or fair AI
 

00:53:46.630 --> 00:53:49.210
ethically aligned what fish or fair AI
now that I have given you the key point

00:53:49.210 --> 00:53:49.220
now that I have given you the key point
 

00:53:49.220 --> 00:53:52.750
now that I have given you the key point
we can all take naps basically back to

00:53:52.750 --> 00:53:52.760
we can all take naps basically back to
 

00:53:52.760 --> 00:53:55.750
we can all take naps basically back to
Plato so the mental image I have a Plato

00:53:55.750 --> 00:53:55.760
Plato so the mental image I have a Plato
 

00:53:55.760 --> 00:53:59.109
Plato so the mental image I have a Plato
is of this undoubted genius but he's

00:53:59.109 --> 00:53:59.119
is of this undoubted genius but he's
 

00:53:59.119 --> 00:54:02.470
is of this undoubted genius but he's
also this guy who's deeply deeply

00:54:02.470 --> 00:54:02.480
also this guy who's deeply deeply
 

00:54:02.480 --> 00:54:04.539
also this guy who's deeply deeply
scarred by one pivotal event in his life

00:54:04.539 --> 00:54:04.549
scarred by one pivotal event in his life
 

00:54:04.549 --> 00:54:07.210
scarred by one pivotal event in his life
and that's the state sanctioned

00:54:07.210 --> 00:54:07.220
and that's the state sanctioned
 

00:54:07.220 --> 00:54:09.490
and that's the state sanctioned
execution of his teacher and mentor of

00:54:09.490 --> 00:54:09.500
execution of his teacher and mentor of
 

00:54:09.500 --> 00:54:14.549
execution of his teacher and mentor of
Socrates Socrates Plato reacts to that

00:54:14.549 --> 00:54:14.559
Socrates Socrates Plato reacts to that
 

00:54:14.559 --> 00:54:17.799
Socrates Socrates Plato reacts to that
event by retreating from the marketplace

00:54:17.799 --> 00:54:17.809
event by retreating from the marketplace
 

00:54:17.809 --> 00:54:20.319
event by retreating from the marketplace
of ideas from the from the population

00:54:20.319 --> 00:54:20.329
of ideas from the from the population
 

00:54:20.329 --> 00:54:22.589
of ideas from the from the population
from the community and he sets up this

00:54:22.589 --> 00:54:22.599
from the community and he sets up this
 

00:54:22.599 --> 00:54:27.280
from the community and he sets up this
he sets up this archetype for the

00:54:27.280 --> 00:54:27.290
he sets up this archetype for the
 

00:54:27.290 --> 00:54:30.130
he sets up this archetype for the
solitary contemplative philosopher and

00:54:30.130 --> 00:54:30.140
solitary contemplative philosopher and
 

00:54:30.140 --> 00:54:34.020
solitary contemplative philosopher and
he he desires you to peers in which

00:54:34.020 --> 00:54:34.030
he he desires you to peers in which
 

00:54:34.030 --> 00:54:37.660
he he desires you to peers in which
basically the demos the people that he

00:54:37.660 --> 00:54:37.670
basically the demos the people that he
 

00:54:37.670 --> 00:54:41.230
basically the demos the people that he
that he kind of pulls back from sort of

00:54:41.230 --> 00:54:41.240
that he kind of pulls back from sort of
 

00:54:41.240 --> 00:54:44.650
that he kind of pulls back from sort of
always subject to some philosopher king

00:54:44.650 --> 00:54:44.660
always subject to some philosopher king
 

00:54:44.660 --> 00:54:46.660
always subject to some philosopher king
and that is the idea what a fair society

00:54:46.660 --> 00:54:46.670
and that is the idea what a fair society
 

00:54:46.670 --> 00:54:48.640
and that is the idea what a fair society
is and it does is without engaging with

00:54:48.640 --> 00:54:48.650
is and it does is without engaging with
 

00:54:48.650 --> 00:54:50.230
is and it does is without engaging with
the people at all he just designed this

00:54:50.230 --> 00:54:50.240
the people at all he just designed this
 

00:54:50.240 --> 00:54:51.040
the people at all he just designed this
from

00:54:51.040 --> 00:54:51.050
from
 

00:54:51.050 --> 00:54:53.890
from
now contrast this to how Socrates does

00:54:53.890 --> 00:54:53.900
now contrast this to how Socrates does
 

00:54:53.900 --> 00:54:58.420
now contrast this to how Socrates does
inquiry he engages deeply and very very

00:54:58.420 --> 00:54:58.430
inquiry he engages deeply and very very
 

00:54:58.430 --> 00:55:00.580
inquiry he engages deeply and very very
critically with the community he's

00:55:00.580 --> 00:55:00.590
critically with the community he's
 

00:55:00.590 --> 00:55:02.740
critically with the community he's
engaged he's around when he asked when

00:55:02.740 --> 00:55:02.750
engaged he's around when he asked when
 

00:55:02.750 --> 00:55:04.030
engaged he's around when he asked when
he has to answer the simple question

00:55:04.030 --> 00:55:04.040
he has to answer the simple question
 

00:55:04.040 --> 00:55:08.200
he has to answer the simple question
what is wisdom now he doesn't get his

00:55:08.200 --> 00:55:08.210
what is wisdom now he doesn't get his
 

00:55:08.210 --> 00:55:09.940
what is wisdom now he doesn't get his
answer he doesn't get a definite I'm

00:55:09.940 --> 00:55:09.950
answer he doesn't get a definite I'm
 

00:55:09.950 --> 00:55:11.410
answer he doesn't get a definite I'm
definitely to answer from that question

00:55:11.410 --> 00:55:11.420
definitely to answer from that question
 

00:55:11.420 --> 00:55:13.450
definitely to answer from that question
from an interaction but at least he can

00:55:13.450 --> 00:55:13.460
from an interaction but at least he can
 

00:55:13.460 --> 00:55:16.510
from an interaction but at least he can
claim some defense against the biases

00:55:16.510 --> 00:55:16.520
claim some defense against the biases
 

00:55:16.520 --> 00:55:18.640
claim some defense against the biases
that his personal perspective

00:55:18.640 --> 00:55:18.650
that his personal perspective
 

00:55:18.650 --> 00:55:21.600
that his personal perspective
necessarily brings to that conversation

00:55:21.600 --> 00:55:21.610
necessarily brings to that conversation
 

00:55:21.610 --> 00:55:24.850
necessarily brings to that conversation
think about how these two men might have

00:55:24.850 --> 00:55:24.860
think about how these two men might have
 

00:55:24.860 --> 00:55:26.440
think about how these two men might have
thought about the questions that lay

00:55:26.440 --> 00:55:26.450
thought about the questions that lay
 

00:55:26.450 --> 00:55:27.220
thought about the questions that lay
before us today

00:55:27.220 --> 00:55:27.230
before us today
 

00:55:27.230 --> 00:55:30.250
before us today
the idea of equity what is equity and

00:55:30.250 --> 00:55:30.260
the idea of equity what is equity and
 

00:55:30.260 --> 00:55:32.110
the idea of equity what is equity and
how do our giri's make decision-making

00:55:32.110 --> 00:55:32.120
how do our giri's make decision-making
 

00:55:32.120 --> 00:55:37.060
how do our giri's make decision-making
artifacts interact with equity Aristotle

00:55:37.060 --> 00:55:37.070
artifacts interact with equity Aristotle
 

00:55:37.070 --> 00:55:39.520
artifacts interact with equity Aristotle
makes an appearance in discussions of

00:55:39.520 --> 00:55:39.530
makes an appearance in discussions of
 

00:55:39.530 --> 00:55:41.290
makes an appearance in discussions of
equity also but we won't focus on him

00:55:41.290 --> 00:55:41.300
equity also but we won't focus on him
 

00:55:41.300 --> 00:55:45.850
equity also but we won't focus on him
today back in the day a few months ago I

00:55:45.850 --> 00:55:45.860
today back in the day a few months ago I
 

00:55:45.860 --> 00:55:48.040
today back in the day a few months ago I
was introduced as done as an engineer of

00:55:48.040 --> 00:55:48.050
was introduced as done as an engineer of
 

00:55:48.050 --> 00:55:51.190
was introduced as done as an engineer of
fairness that's a little bit of a

00:55:51.190 --> 00:55:51.200
fairness that's a little bit of a
 

00:55:51.200 --> 00:55:55.050
fairness that's a little bit of a
paradoxical title engineers have this

00:55:55.050 --> 00:55:55.060
paradoxical title engineers have this
 

00:55:55.060 --> 00:55:58.480
paradoxical title engineers have this
image of being objective precise

00:55:58.480 --> 00:55:58.490
image of being objective precise
 

00:55:58.490 --> 00:56:00.760
image of being objective precise
analysts while fairness is this very

00:56:00.760 --> 00:56:00.770
analysts while fairness is this very
 

00:56:00.770 --> 00:56:03.160
analysts while fairness is this very
contextual very socially defined very

00:56:03.160 --> 00:56:03.170
contextual very socially defined very
 

00:56:03.170 --> 00:56:06.540
contextual very socially defined very
fuzzy concept that slippery to define I

00:56:06.540 --> 00:56:06.550
fuzzy concept that slippery to define I
 

00:56:06.550 --> 00:56:10.270
fuzzy concept that slippery to define I
argue that any any discussion of

00:56:10.270 --> 00:56:10.280
argue that any any discussion of
 

00:56:10.280 --> 00:56:11.680
argue that any any discussion of
artificial intelligence and equity is

00:56:11.680 --> 00:56:11.690
artificial intelligence and equity is
 

00:56:11.690 --> 00:56:13.900
artificial intelligence and equity is
going to be doomed to this compress of

00:56:13.900 --> 00:56:13.910
going to be doomed to this compress of
 

00:56:13.910 --> 00:56:17.080
going to be doomed to this compress of
opposite qualities it used to be that

00:56:17.080 --> 00:56:17.090
opposite qualities it used to be that
 

00:56:17.090 --> 00:56:19.660
opposite qualities it used to be that
when we talked about AI equity people

00:56:19.660 --> 00:56:19.670
when we talked about AI equity people
 

00:56:19.670 --> 00:56:22.270
when we talked about AI equity people
sneer at us they'll say things like well

00:56:22.270 --> 00:56:22.280
sneer at us they'll say things like well
 

00:56:22.280 --> 00:56:23.980
sneer at us they'll say things like well
what is equity in the same way that

00:56:23.980 --> 00:56:23.990
what is equity in the same way that
 

00:56:23.990 --> 00:56:26.440
what is equity in the same way that
Pontius Pilate probably asked what is

00:56:26.440 --> 00:56:26.450
Pontius Pilate probably asked what is
 

00:56:26.450 --> 00:56:31.720
Pontius Pilate probably asked what is
truth and typical conversations about AI

00:56:31.720 --> 00:56:31.730
truth and typical conversations about AI
 

00:56:31.730 --> 00:56:33.520
truth and typical conversations about AI
policies they were focused on these

00:56:33.520 --> 00:56:33.530
policies they were focused on these
 

00:56:33.530 --> 00:56:36.490
policies they were focused on these
future looking scenarios this Terminator

00:56:36.490 --> 00:56:36.500
future looking scenarios this Terminator
 

00:56:36.500 --> 00:56:38.970
future looking scenarios this Terminator
scenarios these are existential crises

00:56:38.970 --> 00:56:38.980
scenarios these are existential crises
 

00:56:38.980 --> 00:56:41.800
scenarios these are existential crises
and I would argue that that that focus

00:56:41.800 --> 00:56:41.810
and I would argue that that that focus
 

00:56:41.810 --> 00:56:44.440
and I would argue that that that focus
that's a product of a privileged

00:56:44.440 --> 00:56:44.450
that's a product of a privileged
 

00:56:44.450 --> 00:56:46.480
that's a product of a privileged
perspective a perspective they cannot

00:56:46.480 --> 00:56:46.490
perspective a perspective they cannot
 

00:56:46.490 --> 00:56:50.170
perspective a perspective they cannot
imagine the problems of AI affecting us

00:56:50.170 --> 00:56:50.180
imagine the problems of AI affecting us
 

00:56:50.180 --> 00:56:51.460
imagine the problems of AI affecting us
today they have to look to the future to

00:56:51.460 --> 00:56:51.470
today they have to look to the future to
 

00:56:51.470 --> 00:56:54.010
today they have to look to the future to
see to imagine things that go wrong now

00:56:54.010 --> 00:56:54.020
see to imagine things that go wrong now
 

00:56:54.020 --> 00:56:55.990
see to imagine things that go wrong now
that is probably a lazy argument so I

00:56:55.990 --> 00:56:56.000
that is probably a lazy argument so I
 

00:56:56.000 --> 00:56:58.930
that is probably a lazy argument so I
offer a reconciliation so here we have

00:56:58.930 --> 00:56:58.940
offer a reconciliation so here we have
 

00:56:58.940 --> 00:56:59.950
offer a reconciliation so here we have
parking

00:56:59.950 --> 00:56:59.960
parking
 

00:56:59.960 --> 00:57:02.560
parking
along pushing that line of inquiry let

00:57:02.560 --> 00:57:02.570
along pushing that line of inquiry let
 

00:57:02.570 --> 00:57:04.120
along pushing that line of inquiry let
me offer a reconciliation by observing

00:57:04.120 --> 00:57:04.130
me offer a reconciliation by observing
 

00:57:04.130 --> 00:57:07.690
me offer a reconciliation by observing
that questions of AI equity and AI

00:57:07.690 --> 00:57:07.700
that questions of AI equity and AI
 

00:57:07.700 --> 00:57:09.940
that questions of AI equity and AI
safety existential safety concerns they

00:57:09.940 --> 00:57:09.950
safety existential safety concerns they
 

00:57:09.950 --> 00:57:14.050
safety existential safety concerns they
actually share common wheat for example

00:57:14.050 --> 00:57:14.060
actually share common wheat for example
 

00:57:14.060 --> 00:57:16.089
actually share common wheat for example
they are concerned with value alignment

00:57:16.089 --> 00:57:16.099
they are concerned with value alignment
 

00:57:16.099 --> 00:57:18.040
they are concerned with value alignment
by value alignment I mean how do we

00:57:18.040 --> 00:57:18.050
by value alignment I mean how do we
 

00:57:18.050 --> 00:57:20.800
by value alignment I mean how do we
design algorithms that that support

00:57:20.800 --> 00:57:20.810
design algorithms that that support
 

00:57:20.810 --> 00:57:23.980
design algorithms that that support
society defined norms they also share

00:57:23.980 --> 00:57:23.990
society defined norms they also share
 

00:57:23.990 --> 00:57:26.320
society defined norms they also share
the problem of explained ability in the

00:57:26.320 --> 00:57:26.330
the problem of explained ability in the
 

00:57:26.330 --> 00:57:29.500
the problem of explained ability in the
in the interest of X of procedural

00:57:29.500 --> 00:57:29.510
in the interest of X of procedural
 

00:57:29.510 --> 00:57:32.620
in the interest of X of procedural
transparency we would like to be able to

00:57:32.620 --> 00:57:32.630
transparency we would like to be able to
 

00:57:32.630 --> 00:57:35.020
transparency we would like to be able to
derive stable robust meaningful

00:57:35.020 --> 00:57:35.030
derive stable robust meaningful
 

00:57:35.030 --> 00:57:36.760
derive stable robust meaningful
explanations from the algorithmic

00:57:36.760 --> 00:57:36.770
explanations from the algorithmic
 

00:57:36.770 --> 00:57:40.180
explanations from the algorithmic
decision making artifacts but let's

00:57:40.180 --> 00:57:40.190
decision making artifacts but let's
 

00:57:40.190 --> 00:57:42.970
decision making artifacts but let's
focus on equity when you focus on equity

00:57:42.970 --> 00:57:42.980
focus on equity when you focus on equity
 

00:57:42.980 --> 00:57:48.900
focus on equity when you focus on equity
there are there are there are these

00:57:48.900 --> 00:57:48.910
there are there are there are these
 

00:57:48.910 --> 00:57:51.490
there are there are there are these
definitional problems there are these

00:57:51.490 --> 00:57:51.500
definitional problems there are these
 

00:57:51.500 --> 00:57:53.620
definitional problems there are these
issues these are perceived that picture

00:57:53.620 --> 00:57:53.630
issues these are perceived that picture
 

00:57:53.630 --> 00:57:56.260
issues these are perceived that picture
in the definition of equity and it's a

00:57:56.260 --> 00:57:56.270
in the definition of equity and it's a
 

00:57:56.270 --> 00:57:58.150
in the definition of equity and it's a
great thing that we now have examples

00:57:58.150 --> 00:57:58.160
great thing that we now have examples
 

00:57:58.160 --> 00:58:01.060
great thing that we now have examples
that in spite of the arbitrary in spite

00:58:01.060 --> 00:58:01.070
that in spite of the arbitrary in spite
 

00:58:01.070 --> 00:58:02.560
that in spite of the arbitrary in spite
of definition of problems they're able

00:58:02.560 --> 00:58:02.570
of definition of problems they're able
 

00:58:02.570 --> 00:58:05.370
of definition of problems they're able
to demonstrate examples of algorithms

00:58:05.370 --> 00:58:05.380
to demonstrate examples of algorithms
 

00:58:05.380 --> 00:58:08.339
to demonstrate examples of algorithms
violating socially defined equity norms

00:58:08.339 --> 00:58:08.349
violating socially defined equity norms
 

00:58:08.349 --> 00:58:12.359
violating socially defined equity norms
these examples have done as deep and and

00:58:12.359 --> 00:58:12.369
these examples have done as deep and and
 

00:58:12.369 --> 00:58:15.550
these examples have done as deep and and
huge service of kind of sort of

00:58:15.550 --> 00:58:15.560
huge service of kind of sort of
 

00:58:15.560 --> 00:58:18.940
huge service of kind of sort of
challenging that deep slumber of settle

00:58:18.940 --> 00:58:18.950
challenging that deep slumber of settle
 

00:58:18.950 --> 00:58:21.099
challenging that deep slumber of settle
opinion that algorithms are necessarily

00:58:21.099 --> 00:58:21.109
opinion that algorithms are necessarily
 

00:58:21.109 --> 00:58:23.290
opinion that algorithms are necessarily
fair and infallible in fact we have

00:58:23.290 --> 00:58:23.300
fair and infallible in fact we have
 

00:58:23.300 --> 00:58:25.690
fair and infallible in fact we have
researchers now saying that any any

00:58:25.690 --> 00:58:25.700
researchers now saying that any any
 

00:58:25.700 --> 00:58:28.240
researchers now saying that any any
technological design necessarily embed

00:58:28.240 --> 00:58:28.250
technological design necessarily embed
 

00:58:28.250 --> 00:58:30.820
technological design necessarily embed
ethical decisions necessarily embed

00:58:30.820 --> 00:58:30.830
ethical decisions necessarily embed
 

00:58:30.830 --> 00:58:33.270
ethical decisions necessarily embed
values either implicitly or explicitly

00:58:33.270 --> 00:58:33.280
values either implicitly or explicitly
 

00:58:33.280 --> 00:58:36.940
values either implicitly or explicitly
when we talk about AI equity with those

00:58:36.940 --> 00:58:36.950
when we talk about AI equity with those
 

00:58:36.950 --> 00:58:39.130
when we talk about AI equity with those
conversations turn on how those values

00:58:39.130 --> 00:58:39.140
conversations turn on how those values
 

00:58:39.140 --> 00:58:41.770
conversations turn on how those values
align with society defined expectations

00:58:41.770 --> 00:58:41.780
align with society defined expectations
 

00:58:41.780 --> 00:58:47.710
align with society defined expectations
of fairness the first is a street bump

00:58:47.710 --> 00:58:47.720
of fairness the first is a street bump
 

00:58:47.720 --> 00:58:49.390
of fairness the first is a street bump
example the second is a compensation

00:58:49.390 --> 00:58:49.400
example the second is a compensation
 

00:58:49.400 --> 00:58:51.010
example the second is a compensation
division and the third is insurance

00:58:51.010 --> 00:58:51.020
division and the third is insurance
 

00:58:51.020 --> 00:58:53.440
division and the third is insurance
price in the last two examples were sort

00:58:53.440 --> 00:58:53.450
price in the last two examples were sort
 

00:58:53.450 --> 00:58:55.630
price in the last two examples were sort
of where the result of ProPublica is

00:58:55.630 --> 00:58:55.640
of where the result of ProPublica is
 

00:58:55.640 --> 00:58:59.710
of where the result of ProPublica is
explorations let's try to examine how

00:58:59.710 --> 00:58:59.720
explorations let's try to examine how
 

00:58:59.720 --> 00:59:02.140
explorations let's try to examine how
bias is my seep into our supposedly

00:59:02.140 --> 00:59:02.150
bias is my seep into our supposedly
 

00:59:02.150 --> 00:59:06.099
bias is my seep into our supposedly
objective infallible algorithms we are

00:59:06.099 --> 00:59:06.109
objective infallible algorithms we are
 

00:59:06.109 --> 00:59:08.079
objective infallible algorithms we are
probably familiar with how a I systems

00:59:08.079 --> 00:59:08.089
probably familiar with how a I systems
 

00:59:08.089 --> 00:59:10.240
probably familiar with how a I systems
are designed this is really design phase

00:59:10.240 --> 00:59:10.250
are designed this is really design phase
 

00:59:10.250 --> 00:59:11.079
are designed this is really design phase
a programming

00:59:11.079 --> 00:59:11.089
a programming
 

00:59:11.089 --> 00:59:13.239
a programming
is a training phase and a testing phase

00:59:13.239 --> 00:59:13.249
is a training phase and a testing phase
 

00:59:13.249 --> 00:59:15.759
is a training phase and a testing phase
but right here in the design of an

00:59:15.759 --> 00:59:15.769
but right here in the design of an
 

00:59:15.769 --> 00:59:17.769
but right here in the design of an
algorithmic artifact to make a decision

00:59:17.769 --> 00:59:17.779
algorithmic artifact to make a decision
 

00:59:17.779 --> 00:59:19.900
algorithmic artifact to make a decision
we have situations where we have framing

00:59:19.900 --> 00:59:19.910
we have situations where we have framing
 

00:59:19.910 --> 00:59:22.450
we have situations where we have framing
artifacts framing effects when you

00:59:22.450 --> 00:59:22.460
artifacts framing effects when you
 

00:59:22.460 --> 00:59:24.400
artifacts framing effects when you
design a system to make decisions and

00:59:24.400 --> 00:59:24.410
design a system to make decisions and
 

00:59:24.410 --> 00:59:26.019
design a system to make decisions and
you focus any frame it as an

00:59:26.019 --> 00:59:26.029
you focus any frame it as an
 

00:59:26.029 --> 00:59:28.749
you focus any frame it as an
optimization on predictive accuracy it

00:59:28.749 --> 00:59:28.759
optimization on predictive accuracy it
 

00:59:28.759 --> 00:59:30.579
optimization on predictive accuracy it
shouldn't be surprising when you have

00:59:30.579 --> 00:59:30.589
shouldn't be surprising when you have
 

00:59:30.589 --> 00:59:32.650
shouldn't be surprising when you have
fairness fairness problems with that

00:59:32.650 --> 00:59:32.660
fairness fairness problems with that
 

00:59:32.660 --> 00:59:34.499
fairness fairness problems with that
with that design

00:59:34.499 --> 00:59:34.509
with that design
 

00:59:34.509 --> 00:59:37.209
with that design
besides the the actual framing effects

00:59:37.209 --> 00:59:37.219
besides the the actual framing effects
 

00:59:37.219 --> 00:59:39.819
besides the the actual framing effects
in the design you have inputs that are

00:59:39.819 --> 00:59:39.829
in the design you have inputs that are
 

00:59:39.829 --> 00:59:42.099
in the design you have inputs that are
used to design the systems these usually

00:59:42.099 --> 00:59:42.109
used to design the systems these usually
 

00:59:42.109 --> 00:59:44.380
used to design the systems these usually
consist of data and assumptions at least

00:59:44.380 --> 00:59:44.390
consist of data and assumptions at least
 

00:59:44.390 --> 00:59:45.670
consist of data and assumptions at least
in models that is to come machine

00:59:45.670 --> 00:59:45.680
in models that is to come machine
 

00:59:45.680 --> 00:59:48.640
in models that is to come machine
learning models you know all the AI

00:59:48.640 --> 00:59:48.650
learning models you know all the AI
 

00:59:48.650 --> 00:59:50.920
learning models you know all the AI
models they relied more on rules and

00:59:50.920 --> 00:59:50.930
models they relied more on rules and
 

00:59:50.930 --> 00:59:52.719
models they relied more on rules and
assumptions explicit rules and

00:59:52.719 --> 00:59:52.729
assumptions explicit rules and
 

00:59:52.729 --> 00:59:55.569
assumptions explicit rules and
assumptions but even in these inputs you

00:59:55.569 --> 00:59:55.579
assumptions but even in these inputs you
 

00:59:55.579 --> 00:59:57.670
assumptions but even in these inputs you
have this problem and I'm using terms

00:59:57.670 --> 00:59:57.680
have this problem and I'm using terms
 

00:59:57.680 --> 00:59:59.739
have this problem and I'm using terms
from behavioral economics to help us

00:59:59.739 --> 00:59:59.749
from behavioral economics to help us
 

00:59:59.749 --> 01:00:02.019
from behavioral economics to help us
maybe train this better we have this

01:00:02.019 --> 01:00:02.029
maybe train this better we have this
 

01:00:02.029 --> 01:00:04.479
maybe train this better we have this
Pirlo of naive realism where the model

01:00:04.479 --> 01:00:04.489
Pirlo of naive realism where the model
 

01:00:04.489 --> 01:00:07.329
Pirlo of naive realism where the model
assumes that the data it give it's it's

01:00:07.329 --> 01:00:07.339
assumes that the data it give it's it's
 

01:00:07.339 --> 01:00:10.059
assumes that the data it give it's it's
able to see is a complete contextual

01:00:10.059 --> 01:00:10.069
able to see is a complete contextual
 

01:00:10.069 --> 01:00:12.700
able to see is a complete contextual
representation of the real world so you

01:00:12.700 --> 01:00:12.710
representation of the real world so you
 

01:00:12.710 --> 01:00:16.630
representation of the real world so you
can imagine a model trained on a subset

01:00:16.630 --> 01:00:16.640
can imagine a model trained on a subset
 

01:00:16.640 --> 01:00:18.430
can imagine a model trained on a subset
of the population assuming that the

01:00:18.430 --> 01:00:18.440
of the population assuming that the
 

01:00:18.440 --> 01:00:19.749
of the population assuming that the
general population to which to going to

01:00:19.749 --> 01:00:19.759
general population to which to going to
 

01:00:19.759 --> 01:00:21.910
general population to which to going to
be applied looks like that training set

01:00:21.910 --> 01:00:21.920
be applied looks like that training set
 

01:00:21.920 --> 01:00:24.309
be applied looks like that training set
that's a form of naive realism or you

01:00:24.309 --> 01:00:24.319
that's a form of naive realism or you
 

01:00:24.319 --> 01:00:26.920
that's a form of naive realism or you
can imagine an algorithm trying to make

01:00:26.920 --> 01:00:26.930
can imagine an algorithm trying to make
 

01:00:26.930 --> 01:00:29.380
can imagine an algorithm trying to make
inferences about people's networks based

01:00:29.380 --> 01:00:29.390
inferences about people's networks based
 

01:00:29.390 --> 01:00:32.380
inferences about people's networks based
on just social media links assuming that

01:00:32.380 --> 01:00:32.390
on just social media links assuming that
 

01:00:32.390 --> 01:00:34.299
on just social media links assuming that
social media links represent the full

01:00:34.299 --> 01:00:34.309
social media links represent the full
 

01:00:34.309 --> 01:00:35.799
social media links represent the full
state of connections in the real world

01:00:35.799 --> 01:00:35.809
state of connections in the real world
 

01:00:35.809 --> 01:00:38.319
state of connections in the real world
that's a form of naive realism we also

01:00:38.319 --> 01:00:38.329
that's a form of naive realism we also
 

01:00:38.329 --> 01:00:40.109
that's a form of naive realism we also
have this issue of anchoring biases

01:00:40.109 --> 01:00:40.119
have this issue of anchoring biases
 

01:00:40.119 --> 01:00:42.039
have this issue of anchoring biases
statistical models they tend to

01:00:42.039 --> 01:00:42.049
statistical models they tend to
 

01:00:42.049 --> 01:00:44.650
statistical models they tend to
basically anchor on examples they've

01:00:44.650 --> 01:00:44.660
basically anchor on examples they've
 

01:00:44.660 --> 01:00:46.509
basically anchor on examples they've
seen in the past this is an example of a

01:00:46.509 --> 01:00:46.519
seen in the past this is an example of a
 

01:00:46.519 --> 01:00:48.940
seen in the past this is an example of a
generalization error problem and so by

01:00:48.940 --> 01:00:48.950
generalization error problem and so by
 

01:00:48.950 --> 01:00:50.499
generalization error problem and so by
anchoring on those examples in the

01:00:50.499 --> 01:00:50.509
anchoring on those examples in the
 

01:00:50.509 --> 01:00:51.819
anchoring on those examples in the
future when it's presented with things

01:00:51.819 --> 01:00:51.829
future when it's presented with things
 

01:00:51.829 --> 01:00:54.339
future when it's presented with things
it's not familiar with you have these

01:00:54.339 --> 01:00:54.349
it's not familiar with you have these
 

01:00:54.349 --> 01:00:57.099
it's not familiar with you have these
anchors that can sort of help sometimes

01:00:57.099 --> 01:00:57.109
anchors that can sort of help sometimes
 

01:00:57.109 --> 01:00:58.859
anchors that can sort of help sometimes
they help sometimes they're not so good

01:00:58.859 --> 01:00:58.869
they help sometimes they're not so good
 

01:00:58.869 --> 01:01:01.799
they help sometimes they're not so good
the last part where biases can seep in

01:01:01.799 --> 01:01:01.809
the last part where biases can seep in
 

01:01:01.809 --> 01:01:05.349
the last part where biases can seep in
isn't the outcomes so we have this issue

01:01:05.349 --> 01:01:05.359
isn't the outcomes so we have this issue
 

01:01:05.359 --> 01:01:07.930
isn't the outcomes so we have this issue
we might call outcome bias or automation

01:01:07.930 --> 01:01:07.940
we might call outcome bias or automation
 

01:01:07.940 --> 01:01:11.289
we might call outcome bias or automation
bias where we assume that the process

01:01:11.289 --> 01:01:11.299
bias where we assume that the process
 

01:01:11.299 --> 01:01:12.969
bias where we assume that the process
the outcome of our decision-making

01:01:12.969 --> 01:01:12.979
the outcome of our decision-making
 

01:01:12.979 --> 01:01:15.370
the outcome of our decision-making
artifact characterized the quality of

01:01:15.370 --> 01:01:15.380
artifact characterized the quality of
 

01:01:15.380 --> 01:01:17.499
artifact characterized the quality of
the of the decision-making artifact it's

01:01:17.499 --> 01:01:17.509
the of the decision-making artifact it's
 

01:01:17.509 --> 01:01:20.229
the of the decision-making artifact it's
just like the the key example here might

01:01:20.229 --> 01:01:20.239
just like the the key example here might
 

01:01:20.239 --> 01:01:22.190
just like the the key example here might
be saying well a broken

01:01:22.190 --> 01:01:22.200
be saying well a broken
 

01:01:22.200 --> 01:01:23.930
be saying well a broken
clock is correct twice a day if you

01:01:23.930 --> 01:01:23.940
clock is correct twice a day if you
 

01:01:23.940 --> 01:01:25.490
clock is correct twice a day if you
observe it only have those two points of

01:01:25.490 --> 01:01:25.500
observe it only have those two points of
 

01:01:25.500 --> 01:01:26.990
observe it only have those two points of
the day you think it's it's working

01:01:26.990 --> 01:01:27.000
the day you think it's it's working
 

01:01:27.000 --> 01:01:28.640
the day you think it's it's working
perfectly but you're not thinking about

01:01:28.640 --> 01:01:28.650
perfectly but you're not thinking about
 

01:01:28.650 --> 01:01:30.200
perfectly but you're not thinking about
the process by which is getting those

01:01:30.200 --> 01:01:30.210
the process by which is getting those
 

01:01:30.210 --> 01:01:32.210
the process by which is getting those
decisions you're not questioning whether

01:01:32.210 --> 01:01:32.220
decisions you're not questioning whether
 

01:01:32.220 --> 01:01:34.670
decisions you're not questioning whether
that's correct or not so this is just a

01:01:34.670 --> 01:01:34.680
that's correct or not so this is just a
 

01:01:34.680 --> 01:01:36.530
that's correct or not so this is just a
framing I might suggest for how we might

01:01:36.530 --> 01:01:36.540
framing I might suggest for how we might
 

01:01:36.540 --> 01:01:38.870
framing I might suggest for how we might
think about how biases in equity CPM to

01:01:38.870 --> 01:01:38.880
think about how biases in equity CPM to
 

01:01:38.880 --> 01:01:40.460
think about how biases in equity CPM to
grade make decision-making like

01:01:40.460 --> 01:01:40.470
grade make decision-making like
 

01:01:40.470 --> 01:01:45.440
grade make decision-making like
artifacts let's try to be I'm going to

01:01:45.440 --> 01:01:45.450
artifacts let's try to be I'm going to
 

01:01:45.450 --> 01:01:47.120
artifacts let's try to be I'm going to
try to present two examples to case

01:01:47.120 --> 01:01:47.130
try to present two examples to case
 

01:01:47.130 --> 01:01:49.609
try to present two examples to case
studies where we can examine how an

01:01:49.609 --> 01:01:49.619
studies where we can examine how an
 

01:01:49.619 --> 01:01:52.040
studies where we can examine how an
algorithm might have to deal with equity

01:01:52.040 --> 01:01:52.050
algorithm might have to deal with equity
 

01:01:52.050 --> 01:01:53.870
algorithm might have to deal with equity
concerns the first is the use of

01:01:53.870 --> 01:01:53.880
concerns the first is the use of
 

01:01:53.880 --> 01:01:56.420
concerns the first is the use of
algorithms in criminal justice example

01:01:56.420 --> 01:01:56.430
algorithms in criminal justice example
 

01:01:56.430 --> 01:01:58.280
algorithms in criminal justice example
applications of the use of algorithms as

01:01:58.280 --> 01:01:58.290
applications of the use of algorithms as
 

01:01:58.290 --> 01:02:00.470
applications of the use of algorithms as
reflected in a compass report it is the

01:02:00.470 --> 01:02:00.480
reflected in a compass report it is the
 

01:02:00.480 --> 01:02:02.270
reflected in a compass report it is the
use of algorithms for setting bills and

01:02:02.270 --> 01:02:02.280
use of algorithms for setting bills and
 

01:02:02.280 --> 01:02:04.220
use of algorithms for setting bills and
for sentencing it used to be that

01:02:04.220 --> 01:02:04.230
for sentencing it used to be that
 

01:02:04.230 --> 01:02:05.859
for sentencing it used to be that
compass was mainly for setting bail

01:02:05.859 --> 01:02:05.869
compass was mainly for setting bail
 

01:02:05.869 --> 01:02:07.760
compass was mainly for setting bail
increasingly there was a feature creep

01:02:07.760 --> 01:02:07.770
increasingly there was a feature creep
 

01:02:07.770 --> 01:02:08.990
increasingly there was a feature creep
where it was increasingly used for

01:02:08.990 --> 01:02:09.000
where it was increasingly used for
 

01:02:09.000 --> 01:02:11.900
where it was increasingly used for
sentencing now the frame I'm going to

01:02:11.900 --> 01:02:11.910
sentencing now the frame I'm going to
 

01:02:11.910 --> 01:02:13.280
sentencing now the frame I'm going to
use to talk about this case to design

01:02:13.280 --> 01:02:13.290
use to talk about this case to design
 

01:02:13.290 --> 01:02:15.079
use to talk about this case to design
exam is something I'm hoping we can take

01:02:15.079 --> 01:02:15.089
exam is something I'm hoping we can take
 

01:02:15.089 --> 01:02:16.880
exam is something I'm hoping we can take
away from this conversation every time

01:02:16.880 --> 01:02:16.890
away from this conversation every time
 

01:02:16.890 --> 01:02:18.319
away from this conversation every time
you think about algorithms in

01:02:18.319 --> 01:02:18.329
you think about algorithms in
 

01:02:18.329 --> 01:02:19.819
you think about algorithms in
decision-making which you think about

01:02:19.819 --> 01:02:19.829
decision-making which you think about
 

01:02:19.829 --> 01:02:21.470
decision-making which you think about
the problem is trying to solve in this

01:02:21.470 --> 01:02:21.480
the problem is trying to solve in this
 

01:02:21.480 --> 01:02:24.230
the problem is trying to solve in this
case bills bill setting a sentencing but

01:02:24.230 --> 01:02:24.240
case bills bill setting a sentencing but
 

01:02:24.240 --> 01:02:26.540
case bills bill setting a sentencing but
we should also be explicit about the

01:02:26.540 --> 01:02:26.550
we should also be explicit about the
 

01:02:26.550 --> 01:02:29.260
we should also be explicit about the
guiding norms the equity norms that

01:02:29.260 --> 01:02:29.270
guiding norms the equity norms that
 

01:02:29.270 --> 01:02:31.640
guiding norms the equity norms that
decision making artifact is subject to

01:02:31.640 --> 01:02:31.650
decision making artifact is subject to
 

01:02:31.650 --> 01:02:34.010
decision making artifact is subject to
in the context of criminal justice I

01:02:34.010 --> 01:02:34.020
in the context of criminal justice I
 

01:02:34.020 --> 01:02:36.109
in the context of criminal justice I
would argue that we have at least two

01:02:36.109 --> 01:02:36.119
would argue that we have at least two
 

01:02:36.119 --> 01:02:37.609
would argue that we have at least two
norms the equal protection norm

01:02:37.609 --> 01:02:37.619
norms the equal protection norm
 

01:02:37.619 --> 01:02:40.339
norms the equal protection norm
guaranteed by the US Constitution at

01:02:40.339 --> 01:02:40.349
guaranteed by the US Constitution at
 

01:02:40.349 --> 01:02:42.109
guaranteed by the US Constitution at
least in the United States and you might

01:02:42.109 --> 01:02:42.119
least in the United States and you might
 

01:02:42.119 --> 01:02:44.030
least in the United States and you might
also argue for the due process norm

01:02:44.030 --> 01:02:44.040
also argue for the due process norm
 

01:02:44.040 --> 01:02:47.470
also argue for the due process norm
guaranteed by the Fifth Amendment rights

01:02:47.470 --> 01:02:47.480
guaranteed by the Fifth Amendment rights
 

01:02:47.480 --> 01:02:50.000
guaranteed by the Fifth Amendment rights
the second example the second case study

01:02:50.000 --> 01:02:50.010
the second example the second case study
 

01:02:50.010 --> 01:02:52.069
the second example the second case study
the second domain would be algorithms

01:02:52.069 --> 01:02:52.079
the second domain would be algorithms
 

01:02:52.079 --> 01:02:55.430
the second domain would be algorithms
not insurance insurance represents this

01:02:55.430 --> 01:02:55.440
not insurance insurance represents this
 

01:02:55.440 --> 01:02:58.450
not insurance insurance represents this
it solves this social social function of

01:02:58.450 --> 01:02:58.460
it solves this social social function of
 

01:02:58.460 --> 01:03:00.980
it solves this social social function of
safeguarding people from catastrophic

01:03:00.980 --> 01:03:00.990
safeguarding people from catastrophic
 

01:03:00.990 --> 01:03:03.170
safeguarding people from catastrophic
risk and so it's not surprising that

01:03:03.170 --> 01:03:03.180
risk and so it's not surprising that
 

01:03:03.180 --> 01:03:05.030
risk and so it's not surprising that
insurers might want to use algorithms to

01:03:05.030 --> 01:03:05.040
insurers might want to use algorithms to
 

01:03:05.040 --> 01:03:07.220
insurers might want to use algorithms to
help them make their make their

01:03:07.220 --> 01:03:07.230
help them make their make their
 

01:03:07.230 --> 01:03:10.720
help them make their make their
decisions more accurate more fair

01:03:10.720 --> 01:03:10.730
decisions more accurate more fair
 

01:03:10.730 --> 01:03:12.890
decisions more accurate more fair
example applications example problems

01:03:12.890 --> 01:03:12.900
example applications example problems
 

01:03:12.900 --> 01:03:14.690
example applications example problems
here are the problems of rate setting

01:03:14.690 --> 01:03:14.700
here are the problems of rate setting
 

01:03:14.700 --> 01:03:16.819
here are the problems of rate setting
and underwriting rate certain is just

01:03:16.819 --> 01:03:16.829
and underwriting rate certain is just
 

01:03:16.829 --> 01:03:18.620
and underwriting rate certain is just
really about setting the price of an

01:03:18.620 --> 01:03:18.630
really about setting the price of an
 

01:03:18.630 --> 01:03:19.400
really about setting the price of an
insurance premium

01:03:19.400 --> 01:03:19.410
insurance premium
 

01:03:19.410 --> 01:03:21.530
insurance premium
underwriting is kind of sort of gauging

01:03:21.530 --> 01:03:21.540
underwriting is kind of sort of gauging
 

01:03:21.540 --> 01:03:24.650
underwriting is kind of sort of gauging
the risk the risk tranches for people

01:03:24.650 --> 01:03:24.660
the risk the risk tranches for people
 

01:03:24.660 --> 01:03:26.870
the risk the risk tranches for people
who are supposed to be insured now what

01:03:26.870 --> 01:03:26.880
who are supposed to be insured now what
 

01:03:26.880 --> 01:03:29.150
who are supposed to be insured now what
would be the insurance the guiding norm

01:03:29.150 --> 01:03:29.160
would be the insurance the guiding norm
 

01:03:29.160 --> 01:03:32.030
would be the insurance the guiding norm
in the situation this is a bit more

01:03:32.030 --> 01:03:32.040
in the situation this is a bit more
 

01:03:32.040 --> 01:03:35.240
in the situation this is a bit more
complicated and I'd argue that the

01:03:35.240 --> 01:03:35.250
complicated and I'd argue that the
 

01:03:35.250 --> 01:03:36.110
complicated and I'd argue that the
actuarial

01:03:36.110 --> 01:03:36.120
actuarial
 

01:03:36.120 --> 01:03:38.060
actuarial
this body of the United States says well

01:03:38.060 --> 01:03:38.070
this body of the United States says well
 

01:03:38.070 --> 01:03:40.100
this body of the United States says well
your insurance prices should be actually

01:03:40.100 --> 01:03:40.110
your insurance prices should be actually
 

01:03:40.110 --> 01:03:41.630
your insurance prices should be actually
fair ie

01:03:41.630 --> 01:03:41.640
fair ie
 

01:03:41.640 --> 01:03:43.730
fair ie
the cost of an insurance premium should

01:03:43.730 --> 01:03:43.740
the cost of an insurance premium should
 

01:03:43.740 --> 01:03:45.830
the cost of an insurance premium should
reflect on the lying risk of the person

01:03:45.830 --> 01:03:45.840
reflect on the lying risk of the person
 

01:03:45.840 --> 01:03:47.690
reflect on the lying risk of the person
but then we have these issues when we

01:03:47.690 --> 01:03:47.700
but then we have these issues when we
 

01:03:47.700 --> 01:03:49.850
but then we have these issues when we
start asking for universal health care

01:03:49.850 --> 01:03:49.860
start asking for universal health care
 

01:03:49.860 --> 01:03:51.740
start asking for universal health care
mandate universal insurance mandate and

01:03:51.740 --> 01:03:51.750
mandate universal insurance mandate and
 

01:03:51.750 --> 01:03:54.050
mandate universal insurance mandate and
have to worry about affordability being

01:03:54.050 --> 01:03:54.060
have to worry about affordability being
 

01:03:54.060 --> 01:03:57.500
have to worry about affordability being
part of the norms here now in a market

01:03:57.500 --> 01:03:57.510
part of the norms here now in a market
 

01:03:57.510 --> 01:03:59.840
part of the norms here now in a market
setting that might be a problem these

01:03:59.840 --> 01:03:59.850
setting that might be a problem these
 

01:03:59.850 --> 01:04:01.250
setting that might be a problem these
are questions we'll have to tease out as

01:04:01.250 --> 01:04:01.260
are questions we'll have to tease out as
 

01:04:01.260 --> 01:04:02.690
are questions we'll have to tease out as
we as we think about equity going

01:04:02.690 --> 01:04:02.700
we as we think about equity going
 

01:04:02.700 --> 01:04:05.780
we as we think about equity going
forward let's try to use these case

01:04:05.780 --> 01:04:05.790
forward let's try to use these case
 

01:04:05.790 --> 01:04:09.970
forward let's try to use these case
studies to tease out more general ideas

01:04:09.970 --> 01:04:09.980
studies to tease out more general ideas
 

01:04:09.980 --> 01:04:12.590
studies to tease out more general ideas
well the very first idea or the very

01:04:12.590 --> 01:04:12.600
well the very first idea or the very
 

01:04:12.600 --> 01:04:15.620
well the very first idea or the very
first theme is that equity norms and

01:04:15.620 --> 01:04:15.630
first theme is that equity norms and
 

01:04:15.630 --> 01:04:17.930
first theme is that equity norms and
necessarily context-specific that's an

01:04:17.930 --> 01:04:17.940
necessarily context-specific that's an
 

01:04:17.940 --> 01:04:21.170
necessarily context-specific that's an
almost obvious statement but that

01:04:21.170 --> 01:04:21.180
almost obvious statement but that
 

01:04:21.180 --> 01:04:24.530
almost obvious statement but that
obvious statement kind of undermines our

01:04:24.530 --> 01:04:24.540
obvious statement kind of undermines our
 

01:04:24.540 --> 01:04:26.620
obvious statement kind of undermines our
expectation that maybe there is this

01:04:26.620 --> 01:04:26.630
expectation that maybe there is this
 

01:04:26.630 --> 01:04:29.300
expectation that maybe there is this
prescriptive mathematical definition of

01:04:29.300 --> 01:04:29.310
prescriptive mathematical definition of
 

01:04:29.310 --> 01:04:31.820
prescriptive mathematical definition of
equity that we just need to impose on

01:04:31.820 --> 01:04:31.830
equity that we just need to impose on
 

01:04:31.830 --> 01:04:35.390
equity that we just need to impose on
algorithms to make them fair that's

01:04:35.390 --> 01:04:35.400
algorithms to make them fair that's
 

01:04:35.400 --> 01:04:38.330
algorithms to make them fair that's
probably not going to happen and we can

01:04:38.330 --> 01:04:38.340
probably not going to happen and we can
 

01:04:38.340 --> 01:04:40.970
probably not going to happen and we can
actually add more to we can actually add

01:04:40.970 --> 01:04:40.980
actually add more to we can actually add
 

01:04:40.980 --> 01:04:43.250
actually add more to we can actually add
more context more complexity to this to

01:04:43.250 --> 01:04:43.260
more context more complexity to this to
 

01:04:43.260 --> 01:04:45.200
more context more complexity to this to
this issue of the existence of

01:04:45.200 --> 01:04:45.210
this issue of the existence of
 

01:04:45.210 --> 01:04:48.380
this issue of the existence of
prescriptive equity norms work by I hope

01:04:48.380 --> 01:04:48.390
prescriptive equity norms work by I hope
 

01:04:48.390 --> 01:04:49.700
prescriptive equity norms work by I hope
I'm pronouncing that name properly shoe

01:04:49.700 --> 01:04:49.710
I'm pronouncing that name properly shoe
 

01:04:49.710 --> 01:04:53.630
I'm pronouncing that name properly shoe
- / and crime burg Emily Nathan they are

01:04:53.630 --> 01:04:53.640
- / and crime burg Emily Nathan they are
 

01:04:53.640 --> 01:04:55.550
- / and crime burg Emily Nathan they are
addressing they are identifying what we

01:04:55.550 --> 01:04:55.560
addressing they are identifying what we
 

01:04:55.560 --> 01:04:57.560
addressing they are identifying what we
might call weak impossibility theorems

01:04:57.560 --> 01:04:57.570
might call weak impossibility theorems
 

01:04:57.570 --> 01:05:01.510
might call weak impossibility theorems
that if you have a set of a set of

01:05:01.510 --> 01:05:01.520
that if you have a set of a set of
 

01:05:01.520 --> 01:05:04.160
that if you have a set of a set of
mathematically defined equity norms that

01:05:04.160 --> 01:05:04.170
mathematically defined equity norms that
 

01:05:04.170 --> 01:05:05.780
mathematically defined equity norms that
you would hope your decision-making

01:05:05.780 --> 01:05:05.790
you would hope your decision-making
 

01:05:05.790 --> 01:05:07.970
you would hope your decision-making
artifact respect in this case group

01:05:07.970 --> 01:05:07.980
artifact respect in this case group
 

01:05:07.980 --> 01:05:11.150
artifact respect in this case group
calibration equality of negative force

01:05:11.150 --> 01:05:11.160
calibration equality of negative force
 

01:05:11.160 --> 01:05:13.460
calibration equality of negative force
positive or false negative rate equality

01:05:13.460 --> 01:05:13.470
positive or false negative rate equality
 

01:05:13.470 --> 01:05:15.710
positive or false negative rate equality
or false positive rate there is

01:05:15.710 --> 01:05:15.720
or false positive rate there is
 

01:05:15.720 --> 01:05:18.260
or false positive rate there is
something inherently incompatible in

01:05:18.260 --> 01:05:18.270
something inherently incompatible in
 

01:05:18.270 --> 01:05:21.500
something inherently incompatible in
trying to achieve all three equity norms

01:05:21.500 --> 01:05:21.510
trying to achieve all three equity norms
 

01:05:21.510 --> 01:05:24.680
trying to achieve all three equity norms
and conversations a few a few months a

01:05:24.680 --> 01:05:24.690
and conversations a few a few months a
 

01:05:24.690 --> 01:05:26.860
and conversations a few a few months a
few weeks ago at the fat ml conference

01:05:26.860 --> 01:05:26.870
few weeks ago at the fat ml conference
 

01:05:26.870 --> 01:05:30.050
few weeks ago at the fat ml conference
extended this to any collection of

01:05:30.050 --> 01:05:30.060
extended this to any collection of
 

01:05:30.060 --> 01:05:31.700
extended this to any collection of
equity mathematically defined equity

01:05:31.700 --> 01:05:31.710
equity mathematically defined equity
 

01:05:31.710 --> 01:05:33.830
equity mathematically defined equity
norms trying to satisfy more than three

01:05:33.830 --> 01:05:33.840
norms trying to satisfy more than three
 

01:05:33.840 --> 01:05:36.110
norms trying to satisfy more than three
and three or more is income is

01:05:36.110 --> 01:05:36.120
and three or more is income is
 

01:05:36.120 --> 01:05:38.120
and three or more is income is
impossible we can call that on a weak

01:05:38.120 --> 01:05:38.130
impossible we can call that on a weak
 

01:05:38.130 --> 01:05:40.460
impossible we can call that on a weak
impossibility theorem now you might hear

01:05:40.460 --> 01:05:40.470
impossibility theorem now you might hear
 

01:05:40.470 --> 01:05:42.380
impossibility theorem now you might hear
this and think well since I can have it

01:05:42.380 --> 01:05:42.390
this and think well since I can have it
 

01:05:42.390 --> 01:05:42.770
this and think well since I can have it
all

01:05:42.770 --> 01:05:42.780
all
 

01:05:42.780 --> 01:05:45.200
all
let me just have let me have my users

01:05:45.200 --> 01:05:45.210
let me just have let me have my users
 

01:05:45.210 --> 01:05:47.480
let me just have let me have my users
vote to decide which is and which ones

01:05:47.480 --> 01:05:47.490
vote to decide which is and which ones
 

01:05:47.490 --> 01:05:48.090
vote to decide which is and which ones
are important

01:05:48.090 --> 01:05:48.100
are important
 

01:05:48.100 --> 01:05:50.490
are important
but then we run into impossibility

01:05:50.490 --> 01:05:50.500
but then we run into impossibility
 

01:05:50.500 --> 01:05:51.840
but then we run into impossibility
theorem is the stronger impossibility

01:05:51.840 --> 01:05:51.850
theorem is the stronger impossibility
 

01:05:51.850 --> 01:05:54.570
theorem is the stronger impossibility
theorems by Kenneth arrow Bieber dance

01:05:54.570 --> 01:05:54.580
theorems by Kenneth arrow Bieber dance
 

01:05:54.580 --> 01:05:56.640
theorems by Kenneth arrow Bieber dance
after tweet saying essentially it's

01:05:56.640 --> 01:05:56.650
after tweet saying essentially it's
 

01:05:56.650 --> 01:05:59.280
after tweet saying essentially it's
impossible to reasonably aggregate

01:05:59.280 --> 01:05:59.290
impossible to reasonably aggregate
 

01:05:59.290 --> 01:06:03.240
impossible to reasonably aggregate
Collective normative preferences alright

01:06:03.240 --> 01:06:03.250
Collective normative preferences alright
 

01:06:03.250 --> 01:06:06.810
Collective normative preferences alright
so that's the second theme the third

01:06:06.810 --> 01:06:06.820
so that's the second theme the third
 

01:06:06.820 --> 01:06:08.760
so that's the second theme the third
team I'm trying to have been trying to

01:06:08.760 --> 01:06:08.770
team I'm trying to have been trying to
 

01:06:08.770 --> 01:06:10.470
team I'm trying to have been trying to
develop over the past year or so or two

01:06:10.470 --> 01:06:10.480
develop over the past year or so or two
 

01:06:10.480 --> 01:06:12.750
develop over the past year or so or two
thinking about this is this framing of

01:06:12.750 --> 01:06:12.760
thinking about this is this framing of
 

01:06:12.760 --> 01:06:14.490
thinking about this is this framing of
allocation problems distribution

01:06:14.490 --> 01:06:14.500
allocation problems distribution
 

01:06:14.500 --> 01:06:17.370
allocation problems distribution
problems as entitlement programs versus

01:06:17.370 --> 01:06:17.380
problems as entitlement programs versus
 

01:06:17.380 --> 01:06:19.950
problems as entitlement programs versus
market programs we can think about it as

01:06:19.950 --> 01:06:19.960
market programs we can think about it as
 

01:06:19.960 --> 01:06:22.500
market programs we can think about it as
a spectrum entitlement distribution

01:06:22.500 --> 01:06:22.510
a spectrum entitlement distribution
 

01:06:22.510 --> 01:06:25.770
a spectrum entitlement distribution
programs the distribution problems in

01:06:25.770 --> 01:06:25.780
programs the distribution problems in
 

01:06:25.780 --> 01:06:27.870
programs the distribution problems in
which there are clear equity norms

01:06:27.870 --> 01:06:27.880
which there are clear equity norms
 

01:06:27.880 --> 01:06:30.120
which there are clear equity norms
guiding the decision process the

01:06:30.120 --> 01:06:30.130
guiding the decision process the
 

01:06:30.130 --> 01:06:31.710
guiding the decision process the
distribution distributed decision

01:06:31.710 --> 01:06:31.720
distribution distributed decision
 

01:06:31.720 --> 01:06:34.890
distribution distributed decision
process normative clarity usually means

01:06:34.890 --> 01:06:34.900
process normative clarity usually means
 

01:06:34.900 --> 01:06:36.960
process normative clarity usually means
that there is some law some regulation

01:06:36.960 --> 01:06:36.970
that there is some law some regulation
 

01:06:36.970 --> 01:06:39.630
that there is some law some regulation
and everybody accepts that determines

01:06:39.630 --> 01:06:39.640
and everybody accepts that determines
 

01:06:39.640 --> 01:06:43.200
and everybody accepts that determines
how fair our decision is you can think

01:06:43.200 --> 01:06:43.210
how fair our decision is you can think
 

01:06:43.210 --> 01:06:44.880
how fair our decision is you can think
about the example of United States

01:06:44.880 --> 01:06:44.890
about the example of United States
 

01:06:44.890 --> 01:06:47.970
about the example of United States
criminal justice you have the public

01:06:47.970 --> 01:06:47.980
criminal justice you have the public
 

01:06:47.980 --> 01:06:51.060
criminal justice you have the public
good law enforcement that's being

01:06:51.060 --> 01:06:51.070
good law enforcement that's being
 

01:06:51.070 --> 01:06:53.400
good law enforcement that's being
distributed to the public under the

01:06:53.400 --> 01:06:53.410
distributed to the public under the
 

01:06:53.410 --> 01:06:55.890
distributed to the public under the
normative guidelines of equal protection

01:06:55.890 --> 01:06:55.900
normative guidelines of equal protection
 

01:06:55.900 --> 01:07:00.000
normative guidelines of equal protection
and due process but we also have what we

01:07:00.000 --> 01:07:00.010
and due process but we also have what we
 

01:07:00.010 --> 01:07:03.090
and due process but we also have what we
might think of as market programs market

01:07:03.090 --> 01:07:03.100
might think of as market programs market
 

01:07:03.100 --> 01:07:04.470
might think of as market programs market
distribution programs are just

01:07:04.470 --> 01:07:04.480
distribution programs are just
 

01:07:04.480 --> 01:07:06.360
distribution programs are just
distribution programs where equity norms

01:07:06.360 --> 01:07:06.370
distribution programs where equity norms
 

01:07:06.370 --> 01:07:08.100
distribution programs where equity norms
play little or no role and the

01:07:08.100 --> 01:07:08.110
play little or no role and the
 

01:07:08.110 --> 01:07:10.290
play little or no role and the
distributive action is determined

01:07:10.290 --> 01:07:10.300
distributive action is determined
 

01:07:10.300 --> 01:07:12.930
distributive action is determined
largely by the intersection of demand or

01:07:12.930 --> 01:07:12.940
largely by the intersection of demand or
 

01:07:12.940 --> 01:07:17.250
largely by the intersection of demand or
supply now let's try to figure out

01:07:17.250 --> 01:07:17.260
supply now let's try to figure out
 

01:07:17.260 --> 01:07:19.590
supply now let's try to figure out
questions that come up from this in the

01:07:19.590 --> 01:07:19.600
questions that come up from this in the
 

01:07:19.600 --> 01:07:20.850
questions that come up from this in the
entitlement programs you have the

01:07:20.850 --> 01:07:20.860
entitlement programs you have the
 

01:07:20.860 --> 01:07:23.250
entitlement programs you have the
question of are the norms necessarily

01:07:23.250 --> 01:07:23.260
question of are the norms necessarily
 

01:07:23.260 --> 01:07:25.350
question of are the norms necessarily
equitable and if they are not equitable

01:07:25.350 --> 01:07:25.360
equitable and if they are not equitable
 

01:07:25.360 --> 01:07:29.040
equitable and if they are not equitable
are there remedies for it social

01:07:29.040 --> 01:07:29.050
are there remedies for it social
 

01:07:29.050 --> 01:07:30.780
are there remedies for it social
movements like black lives matter and

01:07:30.780 --> 01:07:30.790
movements like black lives matter and
 

01:07:30.790 --> 01:07:32.640
movements like black lives matter and
civil rights movements they are

01:07:32.640 --> 01:07:32.650
civil rights movements they are
 

01:07:32.650 --> 01:07:34.740
civil rights movements they are
essentially challenges to the equity

01:07:34.740 --> 01:07:34.750
essentially challenges to the equity
 

01:07:34.750 --> 01:07:36.210
essentially challenges to the equity
norms they rule in equity norms at the

01:07:36.210 --> 01:07:36.220
norms they rule in equity norms at the
 

01:07:36.220 --> 01:07:38.490
norms they rule in equity norms at the
time and we have the same essentially

01:07:38.490 --> 01:07:38.500
time and we have the same essentially
 

01:07:38.500 --> 01:07:40.410
time and we have the same essentially
that legality is not really a question

01:07:40.410 --> 01:07:40.420
that legality is not really a question
 

01:07:40.420 --> 01:07:44.160
that legality is not really a question
of justice the question of power and we

01:07:44.160 --> 01:07:44.170
of justice the question of power and we
 

01:07:44.170 --> 01:07:46.170
of justice the question of power and we
switch over to the market session you

01:07:46.170 --> 01:07:46.180
switch over to the market session you
 

01:07:46.180 --> 01:07:48.420
switch over to the market session you
have this issue of there is a trade-off

01:07:48.420 --> 01:07:48.430
have this issue of there is a trade-off
 

01:07:48.430 --> 01:07:50.940
have this issue of there is a trade-off
between the accuracy the efficiency of

01:07:50.940 --> 01:07:50.950
between the accuracy the efficiency of
 

01:07:50.950 --> 01:07:53.340
between the accuracy the efficiency of
the market and the imposition of equity

01:07:53.340 --> 01:07:53.350
the market and the imposition of equity
 

01:07:53.350 --> 01:07:56.250
the market and the imposition of equity
norms if we try to make a market fair

01:07:56.250 --> 01:07:56.260
norms if we try to make a market fair
 

01:07:56.260 --> 01:07:58.020
norms if we try to make a market fair
accordance on the equity norms you can

01:07:58.020 --> 01:07:58.030
accordance on the equity norms you can
 

01:07:58.030 --> 01:07:59.180
accordance on the equity norms you can
often impose a

01:07:59.180 --> 01:07:59.190
often impose a
 

01:07:59.190 --> 01:08:01.670
often impose a
deadweight cost what is essentially dead

01:08:01.670 --> 01:08:01.680
deadweight cost what is essentially dead
 

01:08:01.680 --> 01:08:02.330
deadweight cost what is essentially dead
wood cost

01:08:02.330 --> 01:08:02.340
wood cost
 

01:08:02.340 --> 01:08:04.400
wood cost
the question becomes who gets to bear

01:08:04.400 --> 01:08:04.410
the question becomes who gets to bear
 

01:08:04.410 --> 01:08:08.780
the question becomes who gets to bear
that burden and how big is that burden

01:08:08.780 --> 01:08:08.790
that burden and how big is that burden
 

01:08:08.790 --> 01:08:12.650
that burden and how big is that burden
that sounds theoretical highfalutin in

01:08:12.650 --> 01:08:12.660
that sounds theoretical highfalutin in
 

01:08:12.660 --> 01:08:14.570
that sounds theoretical highfalutin in
concrete terms you can think about it in

01:08:14.570 --> 01:08:14.580
concrete terms you can think about it in
 

01:08:14.580 --> 01:08:16.070
concrete terms you can think about it in
terms of the mark the insurance market

01:08:16.070 --> 01:08:16.080
terms of the mark the insurance market
 

01:08:16.080 --> 01:08:17.810
terms of the mark the insurance market
in health insurance if you have

01:08:17.810 --> 01:08:17.820
in health insurance if you have
 

01:08:17.820 --> 01:08:20.510
in health insurance if you have
universal mandate who gets to bear the

01:08:20.510 --> 01:08:20.520
universal mandate who gets to bear the
 

01:08:20.520 --> 01:08:23.120
universal mandate who gets to bear the
cost of that increasing in risk just

01:08:23.120 --> 01:08:23.130
cost of that increasing in risk just
 

01:08:23.130 --> 01:08:25.070
cost of that increasing in risk just
because you have universal mandate less

01:08:25.070 --> 01:08:25.080
because you have universal mandate less
 

01:08:25.080 --> 01:08:26.570
because you have universal mandate less
controversially initially in auto

01:08:26.570 --> 01:08:26.580
controversially initially in auto
 

01:08:26.580 --> 01:08:28.580
controversially initially in auto
insurance market who gets to bear that

01:08:28.580 --> 01:08:28.590
insurance market who gets to bear that
 

01:08:28.590 --> 01:08:30.560
insurance market who gets to bear that
cost those are questions we have to

01:08:30.560 --> 01:08:30.570
cost those are questions we have to
 

01:08:30.570 --> 01:08:32.330
cost those are questions we have to
tackle with we have to grapple with if

01:08:32.330 --> 01:08:32.340
tackle with we have to grapple with if
 

01:08:32.340 --> 01:08:35.480
tackle with we have to grapple with if
we really care about equity in the

01:08:35.480 --> 01:08:35.490
we really care about equity in the
 

01:08:35.490 --> 01:08:38.750
we really care about equity in the
decision-making systems all right I feel

01:08:38.750 --> 01:08:38.760
decision-making systems all right I feel
 

01:08:38.760 --> 01:08:42.110
decision-making systems all right I feel
like I've been a bit negative about the

01:08:42.110 --> 01:08:42.120
like I've been a bit negative about the
 

01:08:42.120 --> 01:08:45.680
like I've been a bit negative about the
prospect of algorithmic equity let's try

01:08:45.680 --> 01:08:45.690
prospect of algorithmic equity let's try
 

01:08:45.690 --> 01:08:48.740
prospect of algorithmic equity let's try
to see if I can pull out more positive

01:08:48.740 --> 01:08:48.750
to see if I can pull out more positive
 

01:08:48.750 --> 01:08:53.560
to see if I can pull out more positive
vision from the rubble

01:08:53.560 --> 01:08:53.570
 
 

01:08:53.570 --> 01:08:56.570
 
we've I've been poking holes in the idea

01:08:56.570 --> 01:08:56.580
we've I've been poking holes in the idea
 

01:08:56.580 --> 01:08:58.160
we've I've been poking holes in the idea
that you can actually define press

01:08:58.160 --> 01:08:58.170
that you can actually define press
 

01:08:58.170 --> 01:09:00.080
that you can actually define press
creative equity enormous mathematically

01:09:00.080 --> 01:09:00.090
creative equity enormous mathematically
 

01:09:00.090 --> 01:09:02.420
creative equity enormous mathematically
and impose them on your algorithms and

01:09:02.420 --> 01:09:02.430
and impose them on your algorithms and
 

01:09:02.430 --> 01:09:04.730
and impose them on your algorithms and
even if such a thing existed you will

01:09:04.730 --> 01:09:04.740
even if such a thing existed you will
 

01:09:04.740 --> 01:09:06.560
even if such a thing existed you will
still need a buy-in of your subjects of

01:09:06.560 --> 01:09:06.570
still need a buy-in of your subjects of
 

01:09:06.570 --> 01:09:09.830
still need a buy-in of your subjects of
your users that is not easy to to to

01:09:09.830 --> 01:09:09.840
your users that is not easy to to to
 

01:09:09.840 --> 01:09:12.250
your users that is not easy to to to
maintain to receive in a dictatorial

01:09:12.250 --> 01:09:12.260
maintain to receive in a dictatorial
 

01:09:12.260 --> 01:09:15.500
maintain to receive in a dictatorial
technocracy so this suggests that maybe

01:09:15.500 --> 01:09:15.510
technocracy so this suggests that maybe
 

01:09:15.510 --> 01:09:17.030
technocracy so this suggests that maybe
we should be thinking more about

01:09:17.030 --> 01:09:17.040
we should be thinking more about
 

01:09:17.040 --> 01:09:19.430
we should be thinking more about
participate reversions participatory

01:09:19.430 --> 01:09:19.440
participate reversions participatory
 

01:09:19.440 --> 01:09:24.350
participate reversions participatory
models of AI equity basically these will

01:09:24.350 --> 01:09:24.360
models of AI equity basically these will
 

01:09:24.360 --> 01:09:26.900
models of AI equity basically these will
be models in which your users which

01:09:26.900 --> 01:09:26.910
be models in which your users which
 

01:09:26.910 --> 01:09:27.740
be models in which your users which
constantly

01:09:27.740 --> 01:09:27.750
constantly
 

01:09:27.750 --> 01:09:31.450
constantly
query your users for equity concerns

01:09:31.450 --> 01:09:31.460
query your users for equity concerns
 

01:09:31.460 --> 01:09:34.310
query your users for equity concerns
when we talk about diversity in the rank

01:09:34.310 --> 01:09:34.320
when we talk about diversity in the rank
 

01:09:34.320 --> 01:09:36.590
when we talk about diversity in the rank
this is the diversity point you knew it

01:09:36.590 --> 01:09:36.600
this is the diversity point you knew it
 

01:09:36.600 --> 01:09:38.690
this is the diversity point you knew it
was coming so whatever when we talk

01:09:38.690 --> 01:09:38.700
was coming so whatever when we talk
 

01:09:38.700 --> 01:09:40.160
was coming so whatever when we talk
about diversity in the ranks of

01:09:40.160 --> 01:09:40.170
about diversity in the ranks of
 

01:09:40.170 --> 01:09:42.620
about diversity in the ranks of
algorithmic designers we are not just

01:09:42.620 --> 01:09:42.630
algorithmic designers we are not just
 

01:09:42.630 --> 01:09:45.350
algorithmic designers we are not just
pleading special circumstances it's

01:09:45.350 --> 01:09:45.360
pleading special circumstances it's
 

01:09:45.360 --> 01:09:47.510
pleading special circumstances it's
precisely because by having more design

01:09:47.510 --> 01:09:47.520
precisely because by having more design
 

01:09:47.520 --> 01:09:50.060
precisely because by having more design
more diversity in the rank of designers

01:09:50.060 --> 01:09:50.070
more diversity in the rank of designers
 

01:09:50.070 --> 01:09:51.560
more diversity in the rank of designers
you're pulling in more social

01:09:51.560 --> 01:09:51.570
you're pulling in more social
 

01:09:51.570 --> 01:09:53.420
you're pulling in more social
perspectives to inform the design

01:09:53.420 --> 01:09:53.430
perspectives to inform the design
 

01:09:53.430 --> 01:09:57.770
perspectives to inform the design
process but even diversity diversify is

01:09:57.770 --> 01:09:57.780
process but even diversity diversify is
 

01:09:57.780 --> 01:10:00.200
process but even diversity diversify is
not enough because if you think about it

01:10:00.200 --> 01:10:00.210
not enough because if you think about it
 

01:10:00.210 --> 01:10:02.450
not enough because if you think about it
algorithms are always necessarily going

01:10:02.450 --> 01:10:02.460
algorithms are always necessarily going
 

01:10:02.460 --> 01:10:05.300
algorithms are always necessarily going
to act well generally going to affect

01:10:05.300 --> 01:10:05.310
to act well generally going to affect
 

01:10:05.310 --> 01:10:07.910
to act well generally going to affect
more people than are being represented

01:10:07.910 --> 01:10:07.920
more people than are being represented
 

01:10:07.920 --> 01:10:11.100
more people than are being represented
in the designer class

01:10:11.100 --> 01:10:11.110
 
 

01:10:11.110 --> 01:10:13.050
 
so what you're going to need is

01:10:13.050 --> 01:10:13.060
so what you're going to need is
 

01:10:13.060 --> 01:10:16.050
so what you're going to need is
something that allows for for more

01:10:16.050 --> 01:10:16.060
something that allows for for more
 

01:10:16.060 --> 01:10:18.420
something that allows for for more
participation from wider groups and we

01:10:18.420 --> 01:10:18.430
participation from wider groups and we
 

01:10:18.430 --> 01:10:20.870
participation from wider groups and we
can sharpen this point by observing that

01:10:20.870 --> 01:10:20.880
can sharpen this point by observing that
 

01:10:20.880 --> 01:10:23.430
can sharpen this point by observing that
equity challenges equity concerns

01:10:23.430 --> 01:10:23.440
equity challenges equity concerns
 

01:10:23.440 --> 01:10:26.220
equity challenges equity concerns
they're distributed form of knowledge

01:10:26.220 --> 01:10:26.230
they're distributed form of knowledge
 

01:10:26.230 --> 01:10:28.770
they're distributed form of knowledge
there is no single controlling group

01:10:28.770 --> 01:10:28.780
there is no single controlling group
 

01:10:28.780 --> 01:10:30.330
there is no single controlling group
that's ever going to be able to

01:10:30.330 --> 01:10:30.340
that's ever going to be able to
 

01:10:30.340 --> 01:10:32.790
that's ever going to be able to
coordinate the transmission and

01:10:32.790 --> 01:10:32.800
coordinate the transmission and
 

01:10:32.800 --> 01:10:35.130
coordinate the transmission and
accommodation of equity challenges so we

01:10:35.130 --> 01:10:35.140
accommodation of equity challenges so we
 

01:10:35.140 --> 01:10:37.080
accommodation of equity challenges so we
need more participation basically we

01:10:37.080 --> 01:10:37.090
need more participation basically we
 

01:10:37.090 --> 01:10:39.960
need more participation basically we
need more more most Socrates and fewer

01:10:39.960 --> 01:10:39.970
need more more most Socrates and fewer
 

01:10:39.970 --> 01:10:43.500
need more more most Socrates and fewer
Plato's designing from from isolation we

01:10:43.500 --> 01:10:43.510
Plato's designing from from isolation we
 

01:10:43.510 --> 01:10:45.420
Plato's designing from from isolation we
need people who are going to go out

01:10:45.420 --> 01:10:45.430
need people who are going to go out
 

01:10:45.430 --> 01:10:47.190
need people who are going to go out
there and do the messy work of engaging

01:10:47.190 --> 01:10:47.200
there and do the messy work of engaging
 

01:10:47.200 --> 01:10:48.900
there and do the messy work of engaging
with the subject and this is where I'm

01:10:48.900 --> 01:10:48.910
with the subject and this is where I'm
 

01:10:48.910 --> 01:10:50.430
with the subject and this is where I'm
reminded again of shoulder shoulders

01:10:50.430 --> 01:10:50.440
reminded again of shoulder shoulders
 

01:10:50.440 --> 01:10:52.770
reminded again of shoulder shoulders
work on the Allegheny County child

01:10:52.770 --> 01:10:52.780
work on the Allegheny County child
 

01:10:52.780 --> 01:10:55.470
work on the Allegheny County child
welfare system it's really good work

01:10:55.470 --> 01:10:55.480
welfare system it's really good work
 

01:10:55.480 --> 01:11:00.290
welfare system it's really good work
which is why I'm talking about it now

01:11:00.290 --> 01:11:00.300
which is why I'm talking about it now
 

01:11:00.300 --> 01:11:03.690
which is why I'm talking about it now
besides the participatory model we might

01:11:03.690 --> 01:11:03.700
besides the participatory model we might
 

01:11:03.700 --> 01:11:05.490
besides the participatory model we might
want to think about instead of

01:11:05.490 --> 01:11:05.500
want to think about instead of
 

01:11:05.500 --> 01:11:07.650
want to think about instead of
prescriptive norms we might want to

01:11:07.650 --> 01:11:07.660
prescriptive norms we might want to
 

01:11:07.660 --> 01:11:11.220
prescriptive norms we might want to
think about infrastructures for imposing

01:11:11.220 --> 01:11:11.230
think about infrastructures for imposing
 

01:11:11.230 --> 01:11:14.370
think about infrastructures for imposing
for enabling and designing equity into

01:11:14.370 --> 01:11:14.380
for enabling and designing equity into
 

01:11:14.380 --> 01:11:16.800
for enabling and designing equity into
algorithmic decision-making systems this

01:11:16.800 --> 01:11:16.810
algorithmic decision-making systems this
 

01:11:16.810 --> 01:11:18.750
algorithmic decision-making systems this
is kind of sort of the gdpr route

01:11:18.750 --> 01:11:18.760
is kind of sort of the gdpr route
 

01:11:18.760 --> 01:11:21.240
is kind of sort of the gdpr route
instead of trying to impose norm as you

01:11:21.240 --> 01:11:21.250
instead of trying to impose norm as you
 

01:11:21.250 --> 01:11:23.040
instead of trying to impose norm as you
impose it like you define an

01:11:23.040 --> 01:11:23.050
impose it like you define an
 

01:11:23.050 --> 01:11:24.810
impose it like you define an
infrastructure that enables good

01:11:24.810 --> 01:11:24.820
infrastructure that enables good
 

01:11:24.820 --> 01:11:28.230
infrastructure that enables good
behavior to to happen now you might want

01:11:28.230 --> 01:11:28.240
behavior to to happen now you might want
 

01:11:28.240 --> 01:11:30.330
behavior to to happen now you might want
to ask ourselves what are the key the

01:11:30.330 --> 01:11:30.340
to ask ourselves what are the key the
 

01:11:30.340 --> 01:11:32.220
to ask ourselves what are the key the
minimal key components of such an

01:11:32.220 --> 01:11:32.230
minimal key components of such an
 

01:11:32.230 --> 01:11:34.860
minimal key components of such an
infrastructure I want to argue for three

01:11:34.860 --> 01:11:34.870
infrastructure I want to argue for three
 

01:11:34.870 --> 01:11:39.510
infrastructure I want to argue for three
or three or four fluency we need more

01:11:39.510 --> 01:11:39.520
or three or four fluency we need more
 

01:11:39.520 --> 01:11:44.790
or three or four fluency we need more
fluency in the principles of equity so

01:11:44.790 --> 01:11:44.800
fluency in the principles of equity so
 

01:11:44.800 --> 01:11:46.920
fluency in the principles of equity so
the economist Peyton Jung put it this

01:11:46.920 --> 01:11:46.930
the economist Peyton Jung put it this
 

01:11:46.930 --> 01:11:49.710
the economist Peyton Jung put it this
way equity principles these norms I've

01:11:49.710 --> 01:11:49.720
way equity principles these norms I've
 

01:11:49.720 --> 01:11:51.900
way equity principles these norms I've
been going on about and the mathematical

01:11:51.900 --> 01:11:51.910
been going on about and the mathematical
 

01:11:51.910 --> 01:11:53.490
been going on about and the mathematical
definitions of these norms they are

01:11:53.490 --> 01:11:53.500
definitions of these norms they are
 

01:11:53.500 --> 01:11:55.620
definitions of these norms they are
really just instruments they're not ends

01:11:55.620 --> 01:11:55.630
really just instruments they're not ends
 

01:11:55.630 --> 01:11:57.600
really just instruments they're not ends
in themselves they're just imposed in an

01:11:57.600 --> 01:11:57.610
in themselves they're just imposed in an
 

01:11:57.610 --> 01:11:59.550
in themselves they're just imposed in an
algorithm they are really instrument by

01:11:59.550 --> 01:11:59.560
algorithm they are really instrument by
 

01:11:59.560 --> 01:12:02.810
algorithm they are really instrument by
which society kind of sort of

01:12:02.810 --> 01:12:02.820
which society kind of sort of
 

01:12:02.820 --> 01:12:05.760
which society kind of sort of
adjudicates equity problems distributive

01:12:05.760 --> 01:12:05.770
adjudicates equity problems distributive
 

01:12:05.770 --> 01:12:08.070
adjudicates equity problems distributive
problems especially when efficient in

01:12:08.070 --> 01:12:08.080
problems especially when efficient in
 

01:12:08.080 --> 01:12:09.870
problems especially when efficient in
market efficiency fails as a as a

01:12:09.870 --> 01:12:09.880
market efficiency fails as a as a
 

01:12:09.880 --> 01:12:12.900
market efficiency fails as a as a
criterion now if we are not fluent in

01:12:12.900 --> 01:12:12.910
criterion now if we are not fluent in
 

01:12:12.910 --> 01:12:14.520
criterion now if we are not fluent in
the language of equity principles

01:12:14.520 --> 01:12:14.530
the language of equity principles
 

01:12:14.530 --> 01:12:16.680
the language of equity principles
they're going to have problems properly

01:12:16.680 --> 01:12:16.690
they're going to have problems properly
 

01:12:16.690 --> 01:12:18.870
they're going to have problems properly
adjudicated and responding to equity

01:12:18.870 --> 01:12:18.880
adjudicated and responding to equity
 

01:12:18.880 --> 01:12:21.219
adjudicated and responding to equity
concerns

01:12:21.219 --> 01:12:21.229
concerns
 

01:12:21.229 --> 01:12:23.199
concerns
the second one under which I'll combine

01:12:23.199 --> 01:12:23.209
the second one under which I'll combine
 

01:12:23.209 --> 01:12:25.419
the second one under which I'll combine
it to transparency and dissent I think

01:12:25.419 --> 01:12:25.429
it to transparency and dissent I think
 

01:12:25.429 --> 01:12:27.669
it to transparency and dissent I think
primarily it's about dissent you need

01:12:27.669 --> 01:12:27.679
primarily it's about dissent you need
 

01:12:27.679 --> 01:12:30.009
primarily it's about dissent you need
any implementation any algorithmic

01:12:30.009 --> 01:12:30.019
any implementation any algorithmic
 

01:12:30.019 --> 01:12:34.989
any implementation any algorithmic
implementation to have basically settled

01:12:34.989 --> 01:12:34.999
implementation to have basically settled
 

01:12:34.999 --> 01:12:37.120
implementation to have basically settled
avenues for for enabling dissent for

01:12:37.120 --> 01:12:37.130
avenues for for enabling dissent for
 

01:12:37.130 --> 01:12:40.719
avenues for for enabling dissent for
collecting dissent from the users part

01:12:40.719 --> 01:12:40.729
collecting dissent from the users part
 

01:12:40.729 --> 01:12:42.699
collecting dissent from the users part
of that dissent procedure will probably

01:12:42.699 --> 01:12:42.709
of that dissent procedure will probably
 

01:12:42.709 --> 01:12:44.649
of that dissent procedure will probably
require transparency at all the norms

01:12:44.649 --> 01:12:44.659
require transparency at all the norms
 

01:12:44.659 --> 01:12:45.910
require transparency at all the norms
governing the area throughout the domain

01:12:45.910 --> 01:12:45.920
governing the area throughout the domain
 

01:12:45.920 --> 01:12:48.669
governing the area throughout the domain
and thirdly I think this is the part

01:12:48.669 --> 01:12:48.679
and thirdly I think this is the part
 

01:12:48.679 --> 01:12:50.140
and thirdly I think this is the part
that feels especially in privacy

01:12:50.140 --> 01:12:50.150
that feels especially in privacy
 

01:12:50.150 --> 01:12:52.419
that feels especially in privacy
conversations you need methods for

01:12:52.419 --> 01:12:52.429
conversations you need methods for
 

01:12:52.429 --> 01:12:54.549
conversations you need methods for
accountable redress you need to be able

01:12:54.549 --> 01:12:54.559
accountable redress you need to be able
 

01:12:54.559 --> 01:12:57.870
accountable redress you need to be able
to have institutions be able to make and

01:12:57.870 --> 01:12:57.880
to have institutions be able to make and
 

01:12:57.880 --> 01:13:00.250
to have institutions be able to make and
keep promises you need incentives to

01:13:00.250 --> 01:13:00.260
keep promises you need incentives to
 

01:13:00.260 --> 01:13:02.589
keep promises you need incentives to
allow that to happen usually at least

01:13:02.589 --> 01:13:02.599
allow that to happen usually at least
 

01:13:02.599 --> 01:13:05.439
allow that to happen usually at least
according to GDP GDP GDP are basically

01:13:05.439 --> 01:13:05.449
according to GDP GDP GDP are basically
 

01:13:05.449 --> 01:13:07.750
according to GDP GDP GDP are basically
says or the way to do that is by having

01:13:07.750 --> 01:13:07.760
says or the way to do that is by having
 

01:13:07.760 --> 01:13:10.209
says or the way to do that is by having
these steep fines and I'm inclined to

01:13:10.209 --> 01:13:10.219
these steep fines and I'm inclined to
 

01:13:10.219 --> 01:13:12.699
these steep fines and I'm inclined to
agree because we have a history of data

01:13:12.699 --> 01:13:12.709
agree because we have a history of data
 

01:13:12.709 --> 01:13:14.919
agree because we have a history of data
breaches that have that has not abated

01:13:14.919 --> 01:13:14.929
breaches that have that has not abated
 

01:13:14.929 --> 01:13:17.020
breaches that have that has not abated
in spite of the small fines we've been

01:13:17.020 --> 01:13:17.030
in spite of the small fines we've been
 

01:13:17.030 --> 01:13:20.169
in spite of the small fines we've been
having so far these are the ideas I have

01:13:20.169 --> 01:13:20.179
having so far these are the ideas I have
 

01:13:20.179 --> 01:13:23.529
having so far these are the ideas I have
out there I I will admit that my

01:13:23.529 --> 01:13:23.539
out there I I will admit that my
 

01:13:23.539 --> 01:13:25.949
out there I I will admit that my
conversation has been from a very

01:13:25.949 --> 01:13:25.959
conversation has been from a very
 

01:13:25.959 --> 01:13:29.649
conversation has been from a very
allocated distribution perspective I I

01:13:29.649 --> 01:13:29.659
allocated distribution perspective I I
 

01:13:29.659 --> 01:13:31.500
allocated distribution perspective I I
want to acknowledge Kate Crawford

01:13:31.500 --> 01:13:31.510
want to acknowledge Kate Crawford
 

01:13:31.510 --> 01:13:34.299
want to acknowledge Kate Crawford
perspective that besides allocated

01:13:34.299 --> 01:13:34.309
perspective that besides allocated
 

01:13:34.309 --> 01:13:36.580
perspective that besides allocated
problems you have representational harms

01:13:36.580 --> 01:13:36.590
problems you have representational harms
 

01:13:36.590 --> 01:13:38.709
problems you have representational harms
that come from using algorithms in

01:13:38.709 --> 01:13:38.719
that come from using algorithms in
 

01:13:38.719 --> 01:13:42.250
that come from using algorithms in
society windows allocate renders

01:13:42.250 --> 01:13:42.260
society windows allocate renders
 

01:13:42.260 --> 01:13:45.040
society windows allocate renders
when algorithms that have that will

01:13:45.040 --> 01:13:45.050
when algorithms that have that will
 

01:13:45.050 --> 01:13:47.229
when algorithms that have that will
present the proper distribution and

01:13:47.229 --> 01:13:47.239
present the proper distribution and
 

01:13:47.239 --> 01:13:49.000
present the proper distribution and
society this disparate distributions of

01:13:49.000 --> 01:13:49.010
society this disparate distributions of
 

01:13:49.010 --> 01:13:52.000
society this disparate distributions of
society are now responsible for shaping

01:13:52.000 --> 01:13:52.010
society are now responsible for shaping
 

01:13:52.010 --> 01:13:53.890
society are now responsible for shaping
our preferences what does that mean long

01:13:53.890 --> 01:13:53.900
our preferences what does that mean long
 

01:13:53.900 --> 01:13:58.470
our preferences what does that mean long
term that's all I have for you

01:13:58.470 --> 01:13:58.480
term that's all I have for you
 

01:13:58.480 --> 01:14:04.220
term that's all I have for you
[Applause]

01:14:04.220 --> 01:14:04.230
[Applause]
 

01:14:04.230 --> 01:14:05.609
[Applause]
thank you

01:14:05.609 --> 01:14:05.619
thank you
 

01:14:05.619 --> 01:14:09.299
thank you
now we get to relax and talk take any

01:14:09.299 --> 01:14:09.309
now we get to relax and talk take any
 

01:14:09.309 --> 01:14:11.959
now we get to relax and talk take any
chair as long just one in the middle -

01:14:11.959 --> 01:14:11.969
chair as long just one in the middle -
 

01:14:11.969 --> 01:14:15.080
chair as long just one in the middle -
that's like a fair allocation problem

01:14:15.080 --> 01:14:15.090
that's like a fair allocation problem
 

01:14:15.090 --> 01:14:19.169
that's like a fair allocation problem
that's right so fascinating talk we have

01:14:19.169 --> 01:14:19.179
that's right so fascinating talk we have
 

01:14:19.179 --> 01:14:20.129
that's right so fascinating talk we have
a few questions we're gonna ask

01:14:20.129 --> 01:14:20.139
a few questions we're gonna ask
 

01:14:20.139 --> 01:14:21.540
a few questions we're gonna ask
everybody but I have to drill into what

01:14:21.540 --> 01:14:21.550
everybody but I have to drill into what
 

01:14:21.550 --> 01:14:24.510
everybody but I have to drill into what
you said first okay first of all you

01:14:24.510 --> 01:14:24.520
you said first okay first of all you
 

01:14:24.520 --> 01:14:25.560
you said first okay first of all you
know when you talked about weak and

01:14:25.560 --> 01:14:25.570
know when you talked about weak and
 

01:14:25.570 --> 01:14:27.330
know when you talked about weak and
strong impossibility there was a point

01:14:27.330 --> 01:14:27.340
strong impossibility there was a point
 

01:14:27.340 --> 01:14:28.680
strong impossibility there was a point
there that I think it's important to

01:14:28.680 --> 01:14:28.690
there that I think it's important to
 

01:14:28.690 --> 01:14:30.899
there that I think it's important to
reiterate for the audience so I want to

01:14:30.899 --> 01:14:30.909
reiterate for the audience so I want to
 

01:14:30.909 --> 01:14:31.890
reiterate for the audience so I want to
ask you to build out a little bit

01:14:31.890 --> 01:14:31.900
ask you to build out a little bit
 

01:14:31.900 --> 01:14:36.049
ask you to build out a little bit
because this idea that mathematically

01:14:36.049 --> 01:14:36.059
because this idea that mathematically
 

01:14:36.059 --> 01:14:38.580
because this idea that mathematically
achieving a computational way of

01:14:38.580 --> 01:14:38.590
achieving a computational way of
 

01:14:38.590 --> 01:14:40.260
achieving a computational way of
including the norms of equity such that

01:14:40.260 --> 01:14:40.270
including the norms of equity such that
 

01:14:40.270 --> 01:14:42.240
including the norms of equity such that
the computer system itself the robot the

01:14:42.240 --> 01:14:42.250
the computer system itself the robot the
 

01:14:42.250 --> 01:14:44.819
the computer system itself the robot the
AI system can execute and guarantee that

01:14:44.819 --> 01:14:44.829
AI system can execute and guarantee that
 

01:14:44.829 --> 01:14:46.620
AI system can execute and guarantee that
mm-hmm what I heard you say is that

01:14:46.620 --> 01:14:46.630
mm-hmm what I heard you say is that
 

01:14:46.630 --> 01:14:50.390
mm-hmm what I heard you say is that
that's essentially impossible yeah I

01:14:50.390 --> 01:14:50.400
that's essentially impossible yeah I
 

01:14:50.400 --> 01:14:52.740
that's essentially impossible yeah I
mean I mean we can all we can all talk

01:14:52.740 --> 01:14:52.750
mean I mean we can all we can all talk
 

01:14:52.750 --> 01:14:55.049
mean I mean we can all we can all talk
about okay we want equal we can oh wait

01:14:55.049 --> 01:14:55.059
about okay we want equal we can oh wait
 

01:14:55.059 --> 01:14:56.910
about okay we want equal we can oh wait
we can make arguments for certain equity

01:14:56.910 --> 01:14:56.920
we can make arguments for certain equity
 

01:14:56.920 --> 01:14:58.560
we can make arguments for certain equity
principles to be enforced in algorithms

01:14:58.560 --> 01:14:58.570
principles to be enforced in algorithms
 

01:14:58.570 --> 01:15:02.399
principles to be enforced in algorithms
but at some point a group is going to

01:15:02.399 --> 01:15:02.409
but at some point a group is going to
 

01:15:02.409 --> 01:15:04.799
but at some point a group is going to
argue and see actually the outcome on

01:15:04.799 --> 01:15:04.809
argue and see actually the outcome on
 

01:15:04.809 --> 01:15:06.899
argue and see actually the outcome on
our end is is violate what we think is

01:15:06.899 --> 01:15:06.909
our end is is violate what we think is
 

01:15:06.909 --> 01:15:08.430
our end is is violate what we think is
fair you're going to have to accommodate

01:15:08.430 --> 01:15:08.440
fair you're going to have to accommodate
 

01:15:08.440 --> 01:15:11.640
fair you're going to have to accommodate
that non theoretical that very practical

01:15:11.640 --> 01:15:11.650
that non theoretical that very practical
 

01:15:11.650 --> 01:15:13.919
that non theoretical that very practical
messy equity concern and that probably

01:15:13.919 --> 01:15:13.929
messy equity concern and that probably
 

01:15:13.929 --> 01:15:16.140
messy equity concern and that probably
means moving away from mathematically

01:15:16.140 --> 01:15:16.150
means moving away from mathematically
 

01:15:16.150 --> 01:15:18.479
means moving away from mathematically
precise definitions of equity and trying

01:15:18.479 --> 01:15:18.489
precise definitions of equity and trying
 

01:15:18.489 --> 01:15:22.140
precise definitions of equity and trying
to kind of engage with them I I think

01:15:22.140 --> 01:15:22.150
to kind of engage with them I I think
 

01:15:22.150 --> 01:15:25.500
to kind of engage with them I I think
the judgment hand he put it this way he

01:15:25.500 --> 01:15:25.510
the judgment hand he put it this way he
 

01:15:25.510 --> 01:15:26.850
the judgment hand he put it this way he
said equity concerns

01:15:26.850 --> 01:15:26.860
said equity concerns
 

01:15:26.860 --> 01:15:30.899
said equity concerns
well equity is really just about the the

01:15:30.899 --> 01:15:30.909
well equity is really just about the the
 

01:15:30.909 --> 01:15:33.379
well equity is really just about the the
the uncomfortable accommodation of

01:15:33.379 --> 01:15:33.389
the uncomfortable accommodation of
 

01:15:33.389 --> 01:15:35.939
the uncomfortable accommodation of
conflict to the society trying to do

01:15:35.939 --> 01:15:35.949
conflict to the society trying to do
 

01:15:35.949 --> 01:15:38.580
conflict to the society trying to do
that in a precise mathematical way might

01:15:38.580 --> 01:15:38.590
that in a precise mathematical way might
 

01:15:38.590 --> 01:15:41.250
that in a precise mathematical way might
not be a robust solution long term this

01:15:41.250 --> 01:15:41.260
not be a robust solution long term this
 

01:15:41.260 --> 01:15:43.709
not be a robust solution long term this
reminds me of talking to urban designers

01:15:43.709 --> 01:15:43.719
reminds me of talking to urban designers
 

01:15:43.719 --> 01:15:45.600
reminds me of talking to urban designers
who say there's this thing called the

01:15:45.600 --> 01:15:45.610
who say there's this thing called the
 

01:15:45.610 --> 01:15:47.790
who say there's this thing called the
wicked problem you can't optimize

01:15:47.790 --> 01:15:47.800
wicked problem you can't optimize
 

01:15:47.800 --> 01:15:49.890
wicked problem you can't optimize
because this is a social challenge

01:15:49.890 --> 01:15:49.900
because this is a social challenge
 

01:15:49.900 --> 01:15:52.709
because this is a social challenge
exactly and this also reminds me of the

01:15:52.709 --> 01:15:52.719
exactly and this also reminds me of the
 

01:15:52.719 --> 01:15:54.180
exactly and this also reminds me of the
defense argument this says we'll make

01:15:54.180 --> 01:15:54.190
defense argument this says we'll make
 

01:15:54.190 --> 01:15:56.280
defense argument this says we'll make
the perfect robot soldier argument

01:15:56.280 --> 01:15:56.290
the perfect robot soldier argument
 

01:15:56.290 --> 01:15:58.799
the perfect robot soldier argument
against which is no actually even war is

01:15:58.799 --> 01:15:58.809
against which is no actually even war is
 

01:15:58.809 --> 01:16:00.510
against which is no actually even war is
social we don't optimize it

01:16:00.510 --> 01:16:00.520
social we don't optimize it
 

01:16:00.520 --> 01:16:03.000
social we don't optimize it
quantitatively yep yep well so this is

01:16:03.000 --> 01:16:03.010
quantitatively yep yep well so this is
 

01:16:03.010 --> 01:16:04.890
quantitatively yep yep well so this is
interesting if it's the case that we

01:16:04.890 --> 01:16:04.900
interesting if it's the case that we
 

01:16:04.900 --> 01:16:07.439
interesting if it's the case that we
can't actually hope for an AI system

01:16:07.439 --> 01:16:07.449
can't actually hope for an AI system
 

01:16:07.449 --> 01:16:08.939
can't actually hope for an AI system
that will make the world equitable by

01:16:08.939 --> 01:16:08.949
that will make the world equitable by
 

01:16:08.949 --> 01:16:10.200
that will make the world equitable by
the very nature of how its programmed

01:16:10.200 --> 01:16:10.210
the very nature of how its programmed
 

01:16:10.210 --> 01:16:12.030
the very nature of how its programmed
then you talked about participatory

01:16:12.030 --> 01:16:12.040
then you talked about participatory
 

01:16:12.040 --> 01:16:15.299
then you talked about participatory
equity which I love except when I look

01:16:15.299 --> 01:16:15.309
equity which I love except when I look
 

01:16:15.309 --> 01:16:15.709
equity which I love except when I look
at this

01:16:15.709 --> 01:16:15.719
at this
 

01:16:15.719 --> 01:16:17.870
at this
like Facebook er AdWords on Google what

01:16:17.870 --> 01:16:17.880
like Facebook er AdWords on Google what
 

01:16:17.880 --> 01:16:20.779
like Facebook er AdWords on Google what
I see is participatory systems and what

01:16:20.779 --> 01:16:20.789
I see is participatory systems and what
 

01:16:20.789 --> 01:16:23.299
I see is participatory systems and what
I see in those systems is since no

01:16:23.299 --> 01:16:23.309
I see in those systems is since no
 

01:16:23.309 --> 01:16:25.520
I see in those systems is since no
system is equitably perfect so to speak

01:16:25.520 --> 01:16:25.530
system is equitably perfect so to speak
 

01:16:25.530 --> 01:16:28.250
system is equitably perfect so to speak
people find a way to hack them yep and

01:16:28.250 --> 01:16:28.260
people find a way to hack them yep and
 

01:16:28.260 --> 01:16:30.529
people find a way to hack them yep and
then use their own asymmetric levers of

01:16:30.529 --> 01:16:30.539
then use their own asymmetric levers of
 

01:16:30.539 --> 01:16:32.120
then use their own asymmetric levers of
power there to hack the very

01:16:32.120 --> 01:16:32.130
power there to hack the very
 

01:16:32.130 --> 01:16:33.649
power there to hack the very
participatory system you created to do

01:16:33.649 --> 01:16:33.659
participatory system you created to do
 

01:16:33.659 --> 01:16:35.750
participatory system you created to do
things like throw say I don't know an

01:16:35.750 --> 01:16:35.760
things like throw say I don't know an
 

01:16:35.760 --> 01:16:39.819
things like throw say I don't know an
election so that's not funny but ability

01:16:39.819 --> 01:16:39.829
election so that's not funny but ability
 

01:16:39.829 --> 01:16:44.810
election so that's not funny but ability
so I would argue that maybe the the the

01:16:44.810 --> 01:16:44.820
so I would argue that maybe the the the
 

01:16:44.820 --> 01:16:47.089
so I would argue that maybe the the the
issue is probably mechanism design

01:16:47.089 --> 01:16:47.099
issue is probably mechanism design
 

01:16:47.099 --> 01:16:50.209
issue is probably mechanism design
learning to to put in incentives and in

01:16:50.209 --> 01:16:50.219
learning to to put in incentives and in
 

01:16:50.219 --> 01:16:52.910
learning to to put in incentives and in
platforms that allow that allow for this

01:16:52.910 --> 01:16:52.920
platforms that allow that allow for this
 

01:16:52.920 --> 01:16:54.709
platforms that allow that allow for this
type of for correcting for these types

01:16:54.709 --> 01:16:54.719
type of for correcting for these types
 

01:16:54.719 --> 01:16:57.200
type of for correcting for these types
of evolutionary behaviors over time

01:16:57.200 --> 01:16:57.210
of evolutionary behaviors over time
 

01:16:57.210 --> 01:17:00.830
of evolutionary behaviors over time
trying to a priority design a perfect

01:17:00.830 --> 01:17:00.840
trying to a priority design a perfect
 

01:17:00.840 --> 01:17:03.020
trying to a priority design a perfect
system that's like you're saying it's

01:17:03.020 --> 01:17:03.030
system that's like you're saying it's
 

01:17:03.030 --> 01:17:04.850
system that's like you're saying it's
not going to happen so it's about insane

01:17:04.850 --> 01:17:04.860
not going to happen so it's about insane
 

01:17:04.860 --> 01:17:06.680
not going to happen so it's about insane
structural incentives creating an

01:17:06.680 --> 01:17:06.690
structural incentives creating an
 

01:17:06.690 --> 01:17:09.169
structural incentives creating an
infrastructure for for for modifying

01:17:09.169 --> 01:17:09.179
infrastructure for for for modifying
 

01:17:09.179 --> 01:17:13.009
infrastructure for for for modifying
them over time so then here's my fear I

01:17:13.009 --> 01:17:13.019
them over time so then here's my fear I
 

01:17:13.019 --> 01:17:14.450
them over time so then here's my fear I
totally get that we're gonna make

01:17:14.450 --> 01:17:14.460
totally get that we're gonna make
 

01:17:14.460 --> 01:17:16.129
totally get that we're gonna make
systems we'll make them as as good as we

01:17:16.129 --> 01:17:16.139
systems we'll make them as as good as we
 

01:17:16.139 --> 01:17:16.520
systems we'll make them as as good as we
can

01:17:16.520 --> 01:17:16.530
can
 

01:17:16.530 --> 01:17:18.200
can
people will misuse them they're only bad

01:17:18.200 --> 01:17:18.210
people will misuse them they're only bad
 

01:17:18.210 --> 01:17:20.330
people will misuse them they're only bad
actors and we're going to respond we're

01:17:20.330 --> 01:17:20.340
actors and we're going to respond we're
 

01:17:20.340 --> 01:17:22.790
actors and we're going to respond we're
gonna have a rapid reaction to that but

01:17:22.790 --> 01:17:22.800
gonna have a rapid reaction to that but
 

01:17:22.800 --> 01:17:25.310
gonna have a rapid reaction to that but
here's my concern the AI systems we're

01:17:25.310 --> 01:17:25.320
here's my concern the AI systems we're
 

01:17:25.320 --> 01:17:26.299
here's my concern the AI systems we're
talking about are inherently

01:17:26.299 --> 01:17:26.309
talking about are inherently
 

01:17:26.309 --> 01:17:29.029
talking about are inherently
accumulating and concentrating power and

01:17:29.029 --> 01:17:29.039
accumulating and concentrating power and
 

01:17:29.039 --> 01:17:32.120
accumulating and concentrating power and
knowledge which means the bad actors

01:17:32.120 --> 01:17:32.130
knowledge which means the bad actors
 

01:17:32.130 --> 01:17:35.629
knowledge which means the bad actors
ability to negatively influence reality

01:17:35.629 --> 01:17:35.639
ability to negatively influence reality
 

01:17:35.639 --> 01:17:39.290
ability to negatively influence reality
it's big and only bigger next year and

01:17:39.290 --> 01:17:39.300
it's big and only bigger next year and
 

01:17:39.300 --> 01:17:41.629
it's big and only bigger next year and
the year after here so it seems to me by

01:17:41.629 --> 01:17:41.639
the year after here so it seems to me by
 

01:17:41.639 --> 01:17:43.669
the year after here so it seems to me by
the time we catch them the crimes get

01:17:43.669 --> 01:17:43.679
the time we catch them the crimes get
 

01:17:43.679 --> 01:17:46.189
the time we catch them the crimes get
worse before we do the redress or the

01:17:46.189 --> 01:17:46.199
worse before we do the redress or the
 

01:17:46.199 --> 01:17:48.049
worse before we do the redress or the
things so don't we have this weird

01:17:48.049 --> 01:17:48.059
things so don't we have this weird
 

01:17:48.059 --> 01:17:50.919
things so don't we have this weird
runaway problem yes is it so my so I

01:17:50.919 --> 01:17:50.929
runaway problem yes is it so my so I
 

01:17:50.929 --> 01:17:53.509
runaway problem yes is it so my so I
this these are problems I think about

01:17:53.509 --> 01:17:53.519
this these are problems I think about
 

01:17:53.519 --> 01:17:55.850
this these are problems I think about
every day and I talk about with my with

01:17:55.850 --> 01:17:55.860
every day and I talk about with my with
 

01:17:55.860 --> 01:17:57.830
every day and I talk about with my with
fellow researchers at Rand and one of my

01:17:57.830 --> 01:17:57.840
fellow researchers at Rand and one of my
 

01:17:57.840 --> 01:17:59.330
fellow researchers at Rand and one of my
fellow research because I dislike the

01:17:59.330 --> 01:17:59.340
fellow research because I dislike the
 

01:17:59.340 --> 01:18:01.279
fellow research because I dislike the
solution right I don't see a way around

01:18:01.279 --> 01:18:01.289
solution right I don't see a way around
 

01:18:01.289 --> 01:18:03.229
solution right I don't see a way around
it one of my fellow researchers rant

01:18:03.229 --> 01:18:03.239
it one of my fellow researchers rant
 

01:18:03.239 --> 01:18:05.120
it one of my fellow researchers rant
Walsman he argues that the way to

01:18:05.120 --> 01:18:05.130
Walsman he argues that the way to
 

01:18:05.130 --> 01:18:09.200
Walsman he argues that the way to
address this is to have adversarial

01:18:09.200 --> 01:18:09.210
address this is to have adversarial
 

01:18:09.210 --> 01:18:12.080
address this is to have adversarial
reaction so the first person the bad

01:18:12.080 --> 01:18:12.090
reaction so the first person the bad
 

01:18:12.090 --> 01:18:14.419
reaction so the first person the bad
actors do something like flooding for

01:18:14.419 --> 01:18:14.429
actors do something like flooding for
 

01:18:14.429 --> 01:18:17.149
actors do something like flooding for
the net with with Reznick fake news and

01:18:17.149 --> 01:18:17.159
the net with with Reznick fake news and
 

01:18:17.159 --> 01:18:19.339
the net with with Reznick fake news and
the solution is for the good actors to

01:18:19.339 --> 01:18:19.349
the solution is for the good actors to
 

01:18:19.349 --> 01:18:22.779
the solution is for the good actors to
flood the net with resonant true-true

01:18:22.779 --> 01:18:22.789
flood the net with resonant true-true
 

01:18:22.789 --> 01:18:25.870
flood the net with resonant true-true
News and the idea is how to is basically

01:18:25.870 --> 01:18:25.880
News and the idea is how to is basically
 

01:18:25.880 --> 01:18:27.609
News and the idea is how to is basically
this back and forth between between

01:18:27.609 --> 01:18:27.619
this back and forth between between
 

01:18:27.619 --> 01:18:30.160
this back and forth between between
actors I see that as a runaway situation

01:18:30.160 --> 01:18:30.170
actors I see that as a runaway situation
 

01:18:30.170 --> 01:18:35.009
actors I see that as a runaway situation
yeah but thinking of solutions is hard

01:18:35.009 --> 01:18:35.019
yeah but thinking of solutions is hard
 

01:18:35.019 --> 01:18:37.479
yeah but thinking of solutions is hard
so that's really scares me that's a good

01:18:37.479 --> 01:18:37.489
so that's really scares me that's a good
 

01:18:37.489 --> 01:18:38.589
so that's really scares me that's a good
way to start the conference because

01:18:38.589 --> 01:18:38.599
way to start the conference because
 

01:18:38.599 --> 01:18:40.629
way to start the conference because
we're clearly into the conference so let

01:18:40.629 --> 01:18:40.639
we're clearly into the conference so let
 

01:18:40.639 --> 01:18:43.509
we're clearly into the conference so let
me jump on my extreme level of fear now

01:18:43.509 --> 01:18:43.519
me jump on my extreme level of fear now
 

01:18:43.519 --> 01:18:45.339
me jump on my extreme level of fear now
that I have to the three questions we're

01:18:45.339 --> 01:18:45.349
that I have to the three questions we're
 

01:18:45.349 --> 01:18:47.229
that I have to the three questions we're
hoping to ask everybody because this

01:18:47.229 --> 01:18:47.239
hoping to ask everybody because this
 

01:18:47.239 --> 01:18:49.000
hoping to ask everybody because this
gives us a neat thread to push

01:18:49.000 --> 01:18:49.010
gives us a neat thread to push
 

01:18:49.010 --> 01:18:50.890
gives us a neat thread to push
throughout the whole conference I think

01:18:50.890 --> 01:18:50.900
throughout the whole conference I think
 

01:18:50.900 --> 01:18:52.330
throughout the whole conference I think
you answered one of them already but I'm

01:18:52.330 --> 01:18:52.340
you answered one of them already but I'm
 

01:18:52.340 --> 01:18:53.859
you answered one of them already but I'm
getting ahead of myself

01:18:53.859 --> 01:18:53.869
getting ahead of myself
 

01:18:53.869 --> 01:18:55.899
getting ahead of myself
the first question I really want to

01:18:55.899 --> 01:18:55.909
the first question I really want to
 

01:18:55.909 --> 01:18:58.419
the first question I really want to
narrow your view and unfortunately cause

01:18:58.419 --> 01:18:58.429
narrow your view and unfortunately cause
 

01:18:58.429 --> 01:18:59.709
narrow your view and unfortunately cause
you to prognosticate a little bit into

01:18:59.709 --> 01:18:59.719
you to prognosticate a little bit into
 

01:18:59.719 --> 01:19:01.240
you to prognosticate a little bit into
the future I want to look at the next

01:19:01.240 --> 01:19:01.250
the future I want to look at the next
 

01:19:01.250 --> 01:19:03.129
the future I want to look at the next
ten years hmm and I want to ask the

01:19:03.129 --> 01:19:03.139
ten years hmm and I want to ask the
 

01:19:03.139 --> 01:19:04.779
ten years hmm and I want to ask the
question what is the thing you're most

01:19:04.779 --> 01:19:04.789
question what is the thing you're most
 

01:19:04.789 --> 01:19:07.299
question what is the thing you're most
excited about visa Vai and equity what

01:19:07.299 --> 01:19:07.309
excited about visa Vai and equity what
 

01:19:07.309 --> 01:19:08.740
excited about visa Vai and equity what
is it what is something you can imagine

01:19:08.740 --> 01:19:08.750
is it what is something you can imagine
 

01:19:08.750 --> 01:19:10.660
is it what is something you can imagine
that that's actually doable the next ten

01:19:10.660 --> 01:19:10.670
that that's actually doable the next ten
 

01:19:10.670 --> 01:19:13.540
that that's actually doable the next ten
years where AI can actually help with

01:19:13.540 --> 01:19:13.550
years where AI can actually help with
 

01:19:13.550 --> 01:19:18.490
years where AI can actually help with
questions of equity was a good one we

01:19:18.490 --> 01:19:18.500
questions of equity was a good one we
 

01:19:18.500 --> 01:19:19.600
questions of equity was a good one we
didn't share these with the speakers

01:19:19.600 --> 01:19:19.610
didn't share these with the speakers
 

01:19:19.610 --> 01:19:28.330
didn't share these with the speakers
ahead of time so I think we already seen

01:19:28.330 --> 01:19:28.340
ahead of time so I think we already seen
 

01:19:28.340 --> 01:19:30.040
ahead of time so I think we already seen
this happening maybe ten years is too

01:19:30.040 --> 01:19:30.050
this happening maybe ten years is too
 

01:19:30.050 --> 01:19:32.290
this happening maybe ten years is too
far out the issue of credit allocation

01:19:32.290 --> 01:19:32.300
far out the issue of credit allocation
 

01:19:32.300 --> 01:19:35.620
far out the issue of credit allocation
credit scoring for thin file for thin

01:19:35.620 --> 01:19:35.630
credit scoring for thin file for thin
 

01:19:35.630 --> 01:19:37.410
credit scoring for thin file for thin
flour subjects people who don't have

01:19:37.410 --> 01:19:37.420
flour subjects people who don't have
 

01:19:37.420 --> 01:19:40.660
flour subjects people who don't have
large records in large formal records in

01:19:40.660 --> 01:19:40.670
large records in large formal records in
 

01:19:40.670 --> 01:19:42.339
large records in large formal records in
the economic sectors so this would be

01:19:42.339 --> 01:19:42.349
the economic sectors so this would be
 

01:19:42.349 --> 01:19:44.169
the economic sectors so this would be
like people in developing countries poor

01:19:44.169 --> 01:19:44.179
like people in developing countries poor
 

01:19:44.179 --> 01:19:47.109
like people in developing countries poor
people the ability to use artificial

01:19:47.109 --> 01:19:47.119
people the ability to use artificial
 

01:19:47.119 --> 01:19:50.879
people the ability to use artificial
intelligence to I tried to use the term

01:19:50.879 --> 01:19:50.889
intelligence to I tried to use the term
 

01:19:50.889 --> 01:19:52.870
intelligence to I tried to use the term
algorithm intelligence as often as I can

01:19:52.870 --> 01:19:52.880
algorithm intelligence as often as I can
 

01:19:52.880 --> 01:19:54.220
algorithm intelligence as often as I can
the ability to use machine learning

01:19:54.220 --> 01:19:54.230
the ability to use machine learning
 

01:19:54.230 --> 01:19:56.830
the ability to use machine learning
methods to learn about credit score

01:19:56.830 --> 01:19:56.840
methods to learn about credit score
 

01:19:56.840 --> 01:19:58.930
methods to learn about credit score
about credit worthiness from all the

01:19:58.930 --> 01:19:58.940
about credit worthiness from all the
 

01:19:58.940 --> 01:20:02.169
about credit worthiness from all the
sources of data seems to me extremely

01:20:02.169 --> 01:20:02.179
sources of data seems to me extremely
 

01:20:02.179 --> 01:20:04.359
sources of data seems to me extremely
powerful well if if properly applied and

01:20:04.359 --> 01:20:04.369
powerful well if if properly applied and
 

01:20:04.369 --> 01:20:07.120
powerful well if if properly applied and
properly supervised because you can also

01:20:07.120 --> 01:20:07.130
properly supervised because you can also
 

01:20:07.130 --> 01:20:09.160
properly supervised because you can also
use the type of scoring mechanism for

01:20:09.160 --> 01:20:09.170
use the type of scoring mechanism for
 

01:20:09.170 --> 01:20:11.169
use the type of scoring mechanism for
things like the China the Chinese

01:20:11.169 --> 01:20:11.179
things like the China the Chinese
 

01:20:11.179 --> 01:20:13.870
things like the China the Chinese
approach of social scoring but just

01:20:13.870 --> 01:20:13.880
approach of social scoring but just
 

01:20:13.880 --> 01:20:16.660
approach of social scoring but just
enabling better credit to people in

01:20:16.660 --> 01:20:16.670
enabling better credit to people in
 

01:20:16.670 --> 01:20:19.240
enabling better credit to people in
poorer situations he's a huge

01:20:19.240 --> 01:20:19.250
poorer situations he's a huge
 

01:20:19.250 --> 01:20:22.029
poorer situations he's a huge
transformative fit I think and I love

01:20:22.029 --> 01:20:22.039
transformative fit I think and I love
 

01:20:22.039 --> 01:20:23.319
transformative fit I think and I love
that answer because it also brings into

01:20:23.319 --> 01:20:23.329
that answer because it also brings into
 

01:20:23.329 --> 01:20:24.729
that answer because it also brings into
the play the whole question of

01:20:24.729 --> 01:20:24.739
the play the whole question of
 

01:20:24.739 --> 01:20:26.109
the play the whole question of
developing world and ways in which

01:20:26.109 --> 01:20:26.119
developing world and ways in which
 

01:20:26.119 --> 01:20:27.700
developing world and ways in which
developing world may access what we're

01:20:27.700 --> 01:20:27.710
developing world may access what we're
 

01:20:27.710 --> 01:20:29.859
developing world may access what we're
talking about so the second question is

01:20:29.859 --> 01:20:29.869
talking about so the second question is
 

01:20:29.869 --> 01:20:31.629
talking about so the second question is
the exact opposite again

01:20:31.629 --> 01:20:31.639
the exact opposite again
 

01:20:31.639 --> 01:20:33.149
the exact opposite again
limiting ourselves to ten years or so

01:20:33.149 --> 01:20:33.159
limiting ourselves to ten years or so
 

01:20:33.159 --> 01:20:35.780
limiting ourselves to ten years or so
what is it that most

01:20:35.780 --> 01:20:35.790
what is it that most
 

01:20:35.790 --> 01:20:37.730
what is it that most
keeps you up at night in terms of equity

01:20:37.730 --> 01:20:37.740
keeps you up at night in terms of equity
 

01:20:37.740 --> 01:20:42.530
keeps you up at night in terms of equity
and AI mmm mine don't sleep regularly so

01:20:42.530 --> 01:20:42.540
and AI mmm mine don't sleep regularly so
 

01:20:42.540 --> 01:20:49.340
and AI mmm mine don't sleep regularly so
you know I tend to avoid projects

01:20:49.340 --> 01:20:49.350
you know I tend to avoid projects
 

01:20:49.350 --> 01:20:52.790
you know I tend to avoid projects
involve in heart defense because it

01:20:52.790 --> 01:20:52.800
involve in heart defense because it
 

01:20:52.800 --> 01:20:56.000
involve in heart defense because it
makes me really anxious and the question

01:20:56.000 --> 01:20:56.010
makes me really anxious and the question
 

01:20:56.010 --> 01:20:58.310
makes me really anxious and the question
of deterrence in a world in which we

01:20:58.310 --> 01:20:58.320
of deterrence in a world in which we
 

01:20:58.320 --> 01:21:02.840
of deterrence in a world in which we
have a I enabled weapons makes me

01:21:02.840 --> 01:21:02.850
have a I enabled weapons makes me
 

01:21:02.850 --> 01:21:06.380
have a I enabled weapons makes me
worried and worse than that it's not

01:21:06.380 --> 01:21:06.390
worried and worse than that it's not
 

01:21:06.390 --> 01:21:07.850
worried and worse than that it's not
just the existence of the weapons that

01:21:07.850 --> 01:21:07.860
just the existence of the weapons that
 

01:21:07.860 --> 01:21:09.680
just the existence of the weapons that
makes me worried its take is the

01:21:09.680 --> 01:21:09.690
makes me worried its take is the
 

01:21:09.690 --> 01:21:12.980
makes me worried its take is the
differences in the culture the cultures

01:21:12.980 --> 01:21:12.990
differences in the culture the cultures
 

01:21:12.990 --> 01:21:14.930
differences in the culture the cultures
deploying those weapons if you think

01:21:14.930 --> 01:21:14.940
deploying those weapons if you think
 

01:21:14.940 --> 01:21:16.910
deploying those weapons if you think
about the Chinese the Chinese our

01:21:16.910 --> 01:21:16.920
about the Chinese the Chinese our
 

01:21:16.920 --> 01:21:18.500
about the Chinese the Chinese our
approach to defense in the American

01:21:18.500 --> 01:21:18.510
approach to defense in the American
 

01:21:18.510 --> 01:21:20.900
approach to defense in the American
approach to defense you have basically

01:21:20.900 --> 01:21:20.910
approach to defense you have basically
 

01:21:20.910 --> 01:21:24.530
approach to defense you have basically
China seeming to be more trusting of

01:21:24.530 --> 01:21:24.540
China seeming to be more trusting of
 

01:21:24.540 --> 01:21:28.100
China seeming to be more trusting of
just wholesale fully automated weapons

01:21:28.100 --> 01:21:28.110
just wholesale fully automated weapons
 

01:21:28.110 --> 01:21:31.340
just wholesale fully automated weapons
and you have United States doctrine say

01:21:31.340 --> 01:21:31.350
and you have United States doctrine say
 

01:21:31.350 --> 01:21:32.870
and you have United States doctrine say
something to the effect that will always

01:21:32.870 --> 01:21:32.880
something to the effect that will always
 

01:21:32.880 --> 01:21:35.420
something to the effect that will always
have humans in the loop that that that

01:21:35.420 --> 01:21:35.430
have humans in the loop that that that
 

01:21:35.430 --> 01:21:37.640
have humans in the loop that that that
makes sense from that seems comfortable

01:21:37.640 --> 01:21:37.650
makes sense from that seems comfortable
 

01:21:37.650 --> 01:21:40.100
makes sense from that seems comfortable
comforting from a unilateral perspective

01:21:40.100 --> 01:21:40.110
comforting from a unilateral perspective
 

01:21:40.110 --> 01:21:42.800
comforting from a unilateral perspective
but imagine a situation in which a

01:21:42.800 --> 01:21:42.810
but imagine a situation in which a
 

01:21:42.810 --> 01:21:45.230
but imagine a situation in which a
country has human loop for all the

01:21:45.230 --> 01:21:45.240
country has human loop for all the
 

01:21:45.240 --> 01:21:47.960
country has human loop for all the
autonomous systems and the other country

01:21:47.960 --> 01:21:47.970
autonomous systems and the other country
 

01:21:47.970 --> 01:21:51.050
autonomous systems and the other country
just doesn't care that seems to have

01:21:51.050 --> 01:21:51.060
just doesn't care that seems to have
 

01:21:51.060 --> 01:21:53.060
just doesn't care that seems to have
deterrent implications that are

01:21:53.060 --> 01:21:53.070
deterrent implications that are
 

01:21:53.070 --> 01:21:54.830
deterrent implications that are
difficult to think about so lack of

01:21:54.830 --> 01:21:54.840
difficult to think about so lack of
 

01:21:54.840 --> 01:21:57.260
difficult to think about so lack of
parity may actually force a good actor

01:21:57.260 --> 01:21:57.270
parity may actually force a good actor
 

01:21:57.270 --> 01:21:59.990
parity may actually force a good actor
to become less good yep indeed last

01:21:59.990 --> 01:22:00.000
to become less good yep indeed last
 

01:22:00.000 --> 01:22:02.150
to become less good yep indeed last
question for you is an interesting

01:22:02.150 --> 01:22:02.160
question for you is an interesting
 

01:22:02.160 --> 01:22:03.740
question for you is an interesting
future in question that's kind of

01:22:03.740 --> 01:22:03.750
future in question that's kind of
 

01:22:03.750 --> 01:22:05.870
future in question that's kind of
navel-gazing about this conference so

01:22:05.870 --> 01:22:05.880
navel-gazing about this conference so
 

01:22:05.880 --> 01:22:07.580
navel-gazing about this conference so
imagine we're here 15 years from now

01:22:07.580 --> 01:22:07.590
imagine we're here 15 years from now
 

01:22:07.590 --> 01:22:09.560
imagine we're here 15 years from now
having this conference mmm how will it

01:22:09.560 --> 01:22:09.570
having this conference mmm how will it
 

01:22:09.570 --> 01:22:13.750
having this conference mmm how will it
look or feel different because of AI I

01:22:13.750 --> 01:22:13.760
look or feel different because of AI I
 

01:22:13.760 --> 01:22:15.890
look or feel different because of AI I
assume most people would not actually be

01:22:15.890 --> 01:22:15.900
assume most people would not actually be
 

01:22:15.900 --> 01:22:18.350
assume most people would not actually be
here would have bots in telepresence

01:22:18.350 --> 01:22:18.360
here would have bots in telepresence
 

01:22:18.360 --> 01:22:20.180
here would have bots in telepresence
always be talking to a set of screens

01:22:20.180 --> 01:22:20.190
always be talking to a set of screens
 

01:22:20.190 --> 01:22:25.160
always be talking to a set of screens
here yeah III like this idea and maybe I

01:22:25.160 --> 01:22:25.170
here yeah III like this idea and maybe I
 

01:22:25.170 --> 01:22:26.480
here yeah III like this idea and maybe I
mean I'm trying to imagine how that

01:22:26.480 --> 01:22:26.490
mean I'm trying to imagine how that
 

01:22:26.490 --> 01:22:28.100
mean I'm trying to imagine how that
would apply in a conference setting I

01:22:28.100 --> 01:22:28.110
would apply in a conference setting I
 

01:22:28.110 --> 01:22:30.290
would apply in a conference setting I
like this idea of our clock on traumas

01:22:30.290 --> 01:22:30.300
like this idea of our clock on traumas
 

01:22:30.300 --> 01:22:32.300
like this idea of our clock on traumas
extended mind I feel like most of us

01:22:32.300 --> 01:22:32.310
extended mind I feel like most of us
 

01:22:32.310 --> 01:22:34.430
extended mind I feel like most of us
will be more invested in the tools that

01:22:34.430 --> 01:22:34.440
will be more invested in the tools that
 

01:22:34.440 --> 01:22:36.350
will be more invested in the tools that
enable us to engage with the world and

01:22:36.350 --> 01:22:36.360
enable us to engage with the world and
 

01:22:36.360 --> 01:22:39.620
enable us to engage with the world and
so maybe we'll have augmented brains of

01:22:39.620 --> 01:22:39.630
so maybe we'll have augmented brains of
 

01:22:39.630 --> 01:22:42.350
so maybe we'll have augmented brains of
some sort that make us faster thinkers I

01:22:42.350 --> 01:22:42.360
some sort that make us faster thinkers I
 

01:22:42.360 --> 01:22:44.000
some sort that make us faster thinkers I
always think about how to think faster

01:22:44.000 --> 01:22:44.010
always think about how to think faster
 

01:22:44.010 --> 01:22:45.530
always think about how to think faster
how to think more creatively and I

01:22:45.530 --> 01:22:45.540
how to think more creatively and I
 

01:22:45.540 --> 01:22:46.610
how to think more creatively and I
imagine that artificial intelligence

01:22:46.610 --> 01:22:46.620
imagine that artificial intelligence
 

01:22:46.620 --> 01:22:47.959
imagine that artificial intelligence
will probably enabled

01:22:47.959 --> 01:22:47.969
will probably enabled
 

01:22:47.969 --> 01:22:49.700
will probably enabled
the future so cognitive orthotics for us

01:22:49.700 --> 01:22:49.710
the future so cognitive orthotics for us
 

01:22:49.710 --> 01:22:51.800
the future so cognitive orthotics for us
all maybe yeah excellent well thank you

01:22:51.800 --> 01:22:51.810
all maybe yeah excellent well thank you
 

01:22:51.810 --> 01:22:53.730
all maybe yeah excellent well thank you
so much for joining us wonderful speech

01:22:53.730 --> 01:22:53.740
so much for joining us wonderful speech
 

01:22:53.740 --> 01:23:02.220
so much for joining us wonderful speech
[Applause]

