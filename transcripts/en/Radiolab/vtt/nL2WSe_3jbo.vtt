WEBVTT

1
00:00:00.750 --> 00:00:05.750
You are listening to radio lab radio from W.

2
00:00:05.760 --> 00:00:06.593
N y.

3
00:00:11.770 --> 00:00:11.770
S. T. E. N. N. P. R. Hi

4
00:00:11.770 --> 00:00:15.710
<v 1>there. We're going to start today's program with a fellow named Robert.</v>

5
00:00:15.790 --> 00:00:20.440
Is it Epstein or Epstein? Uh, just think Einstein with an app. Okay.

6
00:00:20.770 --> 00:00:22.810
That would make it Epstein, I guess that's right.

7
00:00:22.870 --> 00:00:26.920
And where are we reaching you right now? I am in a San Diego area.

8
00:00:27.070 --> 00:00:30.790
Robert Epstein is a psychologist or editor in chief of psychology today

9
00:00:30.791 --> 00:00:31.400
magazine.

10
00:00:31.400 --> 00:00:33.860
<v 0>He's written a ton of books on relationships and love,</v>

11
00:00:33.890 --> 00:00:38.890
and he also happens to be one of the world's leading researchers in computer

12
00:00:39.681 --> 00:00:44.000
human interactions, like artificial intelligence, basically. That is correct.

13
00:00:44.001 --> 00:00:45.600
Yes ma'am.

14
00:00:48.050 --> 00:00:51.950
So when did you decide to go onto the computer to get a date?

15
00:00:53.750 --> 00:00:58.460
2006 maybe. Why do you ask? Oh,

16
00:00:58.461 --> 00:01:02.900
no reason. No. What happened? You, you were,

17
00:01:02.990 --> 00:01:04.280
you had gotten divorced.

18
00:01:04.360 --> 00:01:06.910
<v 1>Yeah, I was single at the time. Yeah, I was divorced.</v>

19
00:01:08.120 --> 00:01:11.650
<v 0>You decided that you'd try loving all the right places or what?</v>

20
00:01:11.740 --> 00:01:13.780
<v 1>Oh, sure. Bill online dating and everyone was doing it.</v>

21
00:01:13.781 --> 00:01:17.710
My cousin actually convinced me to try it, so I did.

22
00:01:19.870 --> 00:01:23.560
And I went online and I looked at photos and I looked at profiles and uh,

23
00:01:23.980 --> 00:01:28.840
you know, and I communicated with various people who were willing to talk to me.

24
00:01:31.090 --> 00:01:33.700
And one of the women I was communicating with, uh,

25
00:01:33.730 --> 00:01:37.480
lived in southern California where I do so I thought that's great cause you

26
00:01:37.481 --> 00:01:41.530
know, you want someone to be nearby. And she had a very attractive photo online

27
00:01:43.210 --> 00:01:47.650
and her English was poor, which at first bothered me. And then she said,

28
00:01:47.651 --> 00:01:50.830
well she's not really in California. She's really in Russia. Oh.

29
00:01:50.860 --> 00:01:55.180
But all four of my grandparents came from Russia. So I thought,

30
00:01:55.181 --> 00:01:56.014
well I'll go with it.

31
00:01:58.180 --> 00:02:00.290
So I continued to write to her.

32
00:02:00.500 --> 00:02:01.760
<v 0>Hi Sweet Svetlana.</v>

33
00:02:01.970 --> 00:02:04.880
It's very warm here now and I've been doing a lot of swimming.

34
00:02:05.450 --> 00:02:07.140
I've also been riding doing them. Peter,

35
00:02:07.450 --> 00:02:10.450
<v 2>oh Brantley. She wrote back to me in very poor English. Hello to enroll.</v>

36
00:02:10.920 --> 00:02:14.100
Perfect. Do you mind? I hate procedure later. I am theory pepe.

37
00:02:14.350 --> 00:02:19.350
I remember that she liked to walk in park until invoke with the girlfriend and

38
00:02:19.810 --> 00:02:23.200
the event involved in part telling me about her family.

39
00:02:23.201 --> 00:02:27.790
And her mom asked me about youtube. NBC spoke much and long time.

40
00:02:27.820 --> 00:02:31.600
They lived in a small apartment. I knew where in Russia they lived.

41
00:02:34.200 --> 00:02:35.860
It felt like we were bonding for sure.

42
00:02:35.990 --> 00:02:38.960
<v 0>Hello. I might be able to come to Moscow on Sunday,</v>

43
00:02:39.800 --> 00:02:43.850
April 15th departing Thursday, April 19th with love Robert.

44
00:02:44.200 --> 00:02:45.050
So it was getting serious.

45
00:02:45.090 --> 00:02:46.410
<v 1>Oh yeah, of course.</v>

46
00:02:47.470 --> 00:02:51.430
<v 2>Then what happened? Well, two months passed and I began to feel uncomfortable.</v>

47
00:02:51.790 --> 00:02:55.870
Something wasn't right. There were no phone calls.

48
00:02:56.950 --> 00:02:59.710
[inaudible] some point I began to suggest phone call,

49
00:02:59.980 --> 00:03:01.000
<v 1>but there weren't any,</v>

50
00:03:01.270 --> 00:03:05.860
but the main problem was I would say something like,

51
00:03:05.890 --> 00:03:10.540
did you get my letter about me coming to Moscow in April or tell me more about

52
00:03:10.660 --> 00:03:15.080
this friend of yours that you mentioned and she did not mine.

53
00:03:15.370 --> 00:03:18.220
I'm very glad to your letter. She did not,

54
00:03:18.221 --> 00:03:21.040
she was still replying with fairly long emails,

55
00:03:21.820 --> 00:03:26.620
vetted my CT very bad, but they were kind of rambling in general.

56
00:03:26.650 --> 00:03:31.090
I think of you all days much and I very much want to see more likely you.

57
00:03:31.150 --> 00:03:34.390
I already gave you some dates of our visit to Moscow. My Love.

58
00:03:34.570 --> 00:03:35.800
What do you think about that?

59
00:03:36.370 --> 00:03:41.370
Then at some point a little bell went off in my head finally and I started to

60
00:03:41.501 --> 00:03:44.080
send some emails, which uh,

61
00:03:44.410 --> 00:03:49.080
let's say included random alphabet letters. Where's it? So you say how,

62
00:03:49.081 --> 00:03:53.580
what are you wearing tonight? Are you wearing a D, B, g, G, G, G, l. P?

63
00:03:53.640 --> 00:03:57.960
Exactly. And it didn't make any difference.

64
00:03:58.650 --> 00:03:59.483
Halo, Deb, Robert,

65
00:03:59.670 --> 00:04:04.410
your letters do mean very happy when I open a little box and that's when I

66
00:04:04.411 --> 00:04:08.090
realized Ivana was not a person.

67
00:04:08.750 --> 00:04:10.340
Ivana was a computer program

68
00:04:12.260 --> 00:04:16.790
I had been had. Wow. So what did you think?

69
00:04:17.360 --> 00:04:19.790
I felt like a fool. I felt like an incredible fool,

70
00:04:20.030 --> 00:04:25.030
especially given my background that I had been full that long.

71
00:04:25.460 --> 00:04:29.180
Now I can tell you now this is, this is something I've never made public about.

72
00:04:29.930 --> 00:04:31.100
The other example,

73
00:04:32.090 --> 00:04:34.940
Robert went on to tell us that not long after that first incident,

74
00:04:34.970 --> 00:04:37.520
he was corresponding with someone with a woman I thought,

75
00:04:37.610 --> 00:04:39.410
who also turned out to be a robot.

76
00:04:40.100 --> 00:04:45.100
And he discovered at this time because programmer contacted me from the UK and

77
00:04:46.191 --> 00:04:48.170
said, I know who you are.

78
00:04:48.230 --> 00:04:53.230
You have not been communicating with a person you've been communicating with the

79
00:04:53.420 --> 00:04:56.540
Chat Bot, you've been now undressed twice by robots so to speak.

80
00:04:56.570 --> 00:05:01.080
Well and maybe more than twice. Well, how common do you think this is?

81
00:05:01.081 --> 00:05:04.680
Do you think that match.com and all those places are like swarming with these

82
00:05:04.681 --> 00:05:06.900
bots? You know, you know, I bet you they are.

83
00:05:10.280 --> 00:05:11.300
<v 3>That's what you have to understand.</v>

84
00:05:11.301 --> 00:05:13.970
There are hundreds of these things out there. There might be thousands.

85
00:05:16.610 --> 00:05:17.443
That's what's coming

86
00:05:20.030 --> 00:05:21.830
through in a world like this.

87
00:05:23.090 --> 00:05:25.300
We are surrounded by artists for short life.

88
00:05:26.660 --> 00:05:28.400
Things can get a little confusing.

89
00:05:29.370 --> 00:05:33.040
<v 1>In fact, we're going to do a whole show about that confusion about the sometimes</v>

90
00:05:33.190 --> 00:05:33.970
peculiar,

91
00:05:33.970 --> 00:05:38.540
sometimes strange things that can happen when humans and machines collide.

92
00:05:39.150 --> 00:05:43.920
Collide, but don't quite know who's on what side of the road. Yeah,

93
00:05:44.190 --> 00:05:46.760
I don't know. Jab. Boom. Ryan, that was good. That was good. Just go with it.

94
00:05:46.761 --> 00:05:50.280
Okay. I'm Robert Krulwich. This is radio lab and we are talking to machines.

95
00:05:53.880 --> 00:05:55.180
<v 3>Send me your credit card.</v>

96
00:06:00.690 --> 00:06:01.523
[inaudible]

97
00:06:05.580 --> 00:06:08.390
<v 4>so keep talking. Just start things off.</v>

98
00:06:08.391 --> 00:06:12.860
Let's introduce you to the person who really hooked us on this whole idea of

99
00:06:12.861 --> 00:06:16.100
human robot Chit Chat. My name is Brian Christian. He's a writer.

100
00:06:16.190 --> 00:06:19.480
Are you Christian religiously? No Man.

101
00:06:19.700 --> 00:06:22.830
That's not at all related to anything that's wrong with you.

102
00:06:23.130 --> 00:06:26.000
What's his name but didn't know what's important is that he wrote a book called

103
00:06:26.001 --> 00:06:28.350
the most human human, which is all about uh,

104
00:06:28.370 --> 00:06:32.150
the confusing things that can happen when people and machines interact.

105
00:06:32.151 --> 00:06:35.090
How did you, this is such a curious thing to get. Yeah.

106
00:06:35.200 --> 00:06:39.350
How did you get into this? I played with Ms dos intently when I was a child.

107
00:06:39.351 --> 00:06:42.950
Yeah, there you go. Yeah. Dos is kind of the early version of windows.

108
00:06:42.980 --> 00:06:45.680
I was programming these sort of rudimentary maze games,

109
00:06:45.710 --> 00:06:47.580
like a curse or going through maze. Yeah.

110
00:06:47.630 --> 00:06:50.870
Basically did this by any chance mean you did not develop best friends?

111
00:06:51.210 --> 00:06:54.950
A lot of my best friends were also into that. Wow. We are not the coolest,

112
00:06:55.430 --> 00:06:59.990
but we had a lot of fun. So there you are and you just had a,

113
00:07:00.200 --> 00:07:03.890
you just have a talent for this. I don't know what it was. I mean I was just,

114
00:07:03.891 --> 00:07:07.550
there was something I think fascinating to me that you could take a process that

115
00:07:07.551 --> 00:07:11.480
you knew how to do, but in breaking it down to steps that were that explicit,

116
00:07:11.660 --> 00:07:16.520
you often learned something about how the process actually works. For me,

117
00:07:16.521 --> 00:07:20.630
programming is surprisingly linked to introspection.

118
00:07:21.200 --> 00:07:24.550
How exactly, well you know, if a computer were a person,

119
00:07:24.590 --> 00:07:27.740
you can imagine someone sitting in your living room and you say, you know,

120
00:07:27.741 --> 00:07:31.670
can you hand me that book? And it would say, no I can't do that.

121
00:07:31.850 --> 00:07:33.500
Cause there's a coffee cup on it. And you say,

122
00:07:33.501 --> 00:07:36.800
okay well pick up the Coffee Cup and hand me the book. And it says,

123
00:07:36.801 --> 00:07:39.830
well I can't do that cause I'm now I'm holding the cup. And he said, okay,

124
00:07:39.831 --> 00:07:41.030
put down the Cup

125
00:07:42.660 --> 00:07:46.110
then pick up the book and what you quit we learn says Brian is it even really

126
00:07:46.111 --> 00:07:50.970
simple human behaviors are made up of a thousand sub routines.

127
00:07:51.150 --> 00:07:52.410
I mean if you really think about it,

128
00:07:52.411 --> 00:07:56.160
the book task requires knowing what is a book you have to learn how to go about

129
00:07:56.220 --> 00:08:00.390
elbows and wrists had to grab something. What is a book? I already said that,

130
00:08:00.670 --> 00:08:03.150
oh, you need to know about gravity. If it's a machine,

131
00:08:03.151 --> 00:08:05.010
you have to teach it physics,

132
00:08:05.040 --> 00:08:07.430
everything in the world in order for it to just pick up

133
00:08:08.730 --> 00:08:09.730
<v 5>spoon or buck. Thank you.</v>

134
00:08:11.440 --> 00:08:14.740
<v 4>So now thinking of, that's fell on a bot earlier. Okay.</v>

135
00:08:14.770 --> 00:08:19.060
Trying to make something that could actually mimic human conversation.

136
00:08:19.310 --> 00:08:22.840
Kind of sorta imagine all this stuff you'd have to throw into that. Okay.

137
00:08:22.900 --> 00:08:27.400
English grammar, syntax and context, tone, mood,

138
00:08:27.430 --> 00:08:31.510
sarcasm, hire knee adverbs, turn-taking. Oh, well,

139
00:08:31.511 --> 00:08:35.260
it's not actually as impossible as you'd imagine. This is kind of startling.

140
00:08:35.440 --> 00:08:39.220
If you go back to the very early days of software programming in the mid sixties

141
00:08:39.280 --> 00:08:44.280
1964 1965 this was actually done with a little program called Eliza and it was

142
00:08:45.251 --> 00:08:48.910
developed by Joseph Weizenbaum at MIT. What in Weizenbaums case,

143
00:08:49.150 --> 00:08:51.250
his model was not a Russian hottie instead,

144
00:08:51.251 --> 00:08:55.090
it was a non-directive Rogerian.

145
00:08:56.160 --> 00:08:59.790
The one, it's, it's a particular school of therapy.

146
00:08:59.940 --> 00:09:03.300
The kind where the therapist basically mirrors what you're saying.

147
00:09:03.301 --> 00:09:05.940
What you're saying, like you're saying, this is Sherry Turkle.

148
00:09:05.941 --> 00:09:09.480
She's an anthropologist at the Massachusetts Institute of Technology and she

149
00:09:09.481 --> 00:09:13.380
worked with Joey's and Bam, or is it Weizenbaum? It's Weizenbaum at MIT.

150
00:09:13.440 --> 00:09:17.100
So if you say, you know, I, I'm feeling depressed. A therapist says,

151
00:09:17.101 --> 00:09:20.070
I'm sorry to hear you're feeling depressed, tell me more. Um,

152
00:09:20.100 --> 00:09:21.870
Joseph Weizenbaum decides, you know,

153
00:09:21.871 --> 00:09:25.950
I think that's an easy enough type of conversation that I can program that into

154
00:09:25.951 --> 00:09:28.770
my computer. And so he writes up a simple little program,

155
00:09:28.800 --> 00:09:31.050
just about a hundred lines of code,

156
00:09:31.500 --> 00:09:33.840
which does sort of what your therapist does,

157
00:09:34.000 --> 00:09:36.500
where it looks for a keyword and what you're saying as an,

158
00:09:36.510 --> 00:09:39.600
I'm feeling depressed, keyword, depressed, latches onto it,

159
00:09:39.601 --> 00:09:41.160
and then basically flips it back to you.

160
00:09:41.190 --> 00:09:43.890
I'm sorry to hear that you're feeling keyword depressed, right?

161
00:09:43.990 --> 00:09:47.240
It's basically a program that inverts your words and it's, so,

162
00:09:47.250 --> 00:09:49.830
it's a language game. So here's what he did.

163
00:09:49.831 --> 00:09:52.170
He put a computer on a desk running this little program,

164
00:09:52.171 --> 00:09:55.500
and then he invited people to sit down and talk to it.

165
00:09:55.530 --> 00:09:58.800
They'd write in something and they would spit back the keyword and then you

166
00:09:58.801 --> 00:10:00.400
would then repeat and on and on.

167
00:10:00.430 --> 00:10:02.700
You wouldn't think this would be very interesting, right? Nope.

168
00:10:03.330 --> 00:10:05.190
But he found that his students,

169
00:10:05.470 --> 00:10:10.230
one who knew that the program didn't know or understand who had helped him write

170
00:10:10.231 --> 00:10:14.820
the program. Sophisticated students nevertheless want it to be alone with it.

171
00:10:14.850 --> 00:10:17.120
Wow. They actually help write the program. Yes.

172
00:10:17.220 --> 00:10:20.700
I can read you if you'd like one of the conversations. Yes, please.

173
00:10:20.910 --> 00:10:24.140
This is an actual transcript between Eliza and a young woman. Yeah.

174
00:10:24.210 --> 00:10:26.730
Men are all alike. In what way?

175
00:10:26.850 --> 00:10:30.360
They're always bugging us about something or other. Can you think of a specific,

176
00:10:31.550 --> 00:10:34.500
well, my boyfriend made me come here. Your boyfriend made you come here.

177
00:10:35.250 --> 00:10:38.700
I'm depressed much at the time. I'm so sorry to hear you are depressed. True.

178
00:10:39.000 --> 00:10:40.860
It's true. I'm unhappy.

179
00:10:41.030 --> 00:10:43.200
Do you think coming here will help you not to be unhappy?

180
00:10:43.280 --> 00:10:45.900
I need some help that much team. Certain.

181
00:10:45.901 --> 00:10:47.640
What would it mean to you if you got some help?

182
00:10:47.850 --> 00:10:50.070
Perhaps I could learn to count along with my mother.

183
00:10:50.071 --> 00:10:53.250
Tell me more about your family. Wow. And so it's,

184
00:10:53.251 --> 00:10:56.340
this VAT could go on for hours. Well, and in fact it did.

185
00:10:56.410 --> 00:10:57.520
A mother takes care of it.

186
00:10:57.900 --> 00:11:01.120
<v 6>Males in your family takes care of you. My father, your father,</v>

187
00:11:01.150 --> 00:11:03.880
you're like my father in some ways. Resemblance. Do you see,

188
00:11:04.120 --> 00:11:07.240
while you're not very aggressive, what makes you think I'm not very aggressive?

189
00:11:07.300 --> 00:11:10.440
You don't argue with me. Why do you think I don't argue with you?

190
00:11:10.470 --> 00:11:14.500
A are afraid of me leads you to believe that there's a woman in this transcript

191
00:11:14.680 --> 00:11:16.810
messing around. Or is she really pouring her heart out?

192
00:11:16.840 --> 00:11:19.510
We know that this is a woman who works for Joe Weizenbaum.

193
00:11:19.720 --> 00:11:22.600
She's sitting in the office and she just can't stop talking to it.

194
00:11:22.601 --> 00:11:23.560
That's all we know.

195
00:11:23.561 --> 00:11:27.220
What else comes to mind when you think of your father bullies and why your mom

196
00:11:27.221 --> 00:11:29.060
is watching all this and he,

197
00:11:29.100 --> 00:11:33.370
he first thought it was funny and then he didn't think it was funny because they

198
00:11:33.371 --> 00:11:35.290
were actually having conversations with it.

199
00:11:36.020 --> 00:11:41.020
<v 4>One day he comes into the office and his secretary is on the computer divulging</v>

200
00:11:41.511 --> 00:11:43.940
her life story to it. According to Weizenbaum,

201
00:11:43.941 --> 00:11:47.810
she even told them to please leave the room so she could be alone with it and

202
00:11:47.811 --> 00:11:52.520
talk to it. And he, he, um, he was very upset.

203
00:11:54.580 --> 00:11:55.450
<v 7>Nevertheless,</v>

204
00:11:55.520 --> 00:11:57.350
<v 0>when word about Eliza got out,</v>

205
00:11:57.640 --> 00:12:02.560
<v 4>the medical community sort of latches onto it really and says, oh,</v>

206
00:12:02.561 --> 00:12:04.420
this is going to be the next revolution in therapy,

207
00:12:04.610 --> 00:12:07.730
<v 8>something new and promising in the field of psychotherapy.</v>

208
00:12:07.790 --> 00:12:12.290
<v 4>This is from a newscast around that therapists in like phone booths in cities</v>

209
00:12:12.291 --> 00:12:15.830
and you're going to walk in and put a quarter in the slot and have, you know,

210
00:12:15.831 --> 00:12:17.720
half an hour of therapy with this automatic

211
00:12:18.510 --> 00:12:21.230
<v 8>programmed time can be renting for $5 an hour.</v>

212
00:12:21.231 --> 00:12:23.360
And there's every reason to suspect that it will go down.

213
00:12:24.280 --> 00:12:27.790
<v 4>People really thought that they were going to replace therapists with computers.</v>

214
00:12:28.420 --> 00:12:30.670
Absolutely. Really? Absolutely. Yeah.

215
00:12:31.000 --> 00:12:36.000
And it was just this really appalling moment for why isn't bomb of,

216
00:12:37.150 --> 00:12:41.710
there's something, the genie is out of the bottle, maybe in a,

217
00:12:41.770 --> 00:12:46.270
in a bad way. And he does this one f his entire career.

218
00:12:46.330 --> 00:12:47.890
So he pulls the plug on the program,

219
00:12:47.891 --> 00:12:52.891
he cuts the funding and it goes from being one of the main advocates for

220
00:12:52.931 --> 00:12:57.931
artificial intelligence to basically committing the rest of his career to

221
00:12:58.330 --> 00:13:00.670
fighting against artificial intelligence.

222
00:13:00.870 --> 00:13:04.250
<v 0>Rude to Yvette in sign in for cove. [inaudible] Sherbet.</v>

223
00:13:04.800 --> 00:13:09.210
Speco this is Joseph Weizenbaum interviewed in German just before he died in

224
00:13:09.211 --> 00:13:13.420
2008 it was on the German documentary plug and pray and mine.

225
00:13:13.720 --> 00:13:18.480
Cal Nine one is my main objection. He's a venn dusting. Yes.

226
00:13:18.730 --> 00:13:19.411
The thing says,

227
00:13:19.411 --> 00:13:23.010
I understand that if somebody typed in something and the machine says,

228
00:13:23.011 --> 00:13:28.011
I understand that is [inaudible] there's no one there as a,

229
00:13:28.850 --> 00:13:31.230
it's just a Heiner Lugar so it's a lie.

230
00:13:31.720 --> 00:13:34.140
[inaudible] when each can be a niche flush. Dan,

231
00:13:34.200 --> 00:13:37.890
and I can't imagine that people who are emotionally imbalanced could be

232
00:13:37.891 --> 00:13:42.100
effectively treated by systematically maligned, unmoved.

233
00:13:42.320 --> 00:13:47.320
<v 7>I must say that my reaction to the Eliza program at the time was to try to</v>

234
00:13:47.421 --> 00:13:48.290
reassure him.

235
00:13:48.770 --> 00:13:53.540
At the time what I thought people were doing was using it as a kind of

236
00:13:53.541 --> 00:13:57.350
interactive diary, knowing that it was a machine,

237
00:13:57.680 --> 00:14:02.680
but using it as an occasion to breathe life into it in order to get their

238
00:14:03.621 --> 00:14:04.454
feelings out.

239
00:14:08.830 --> 00:14:12.000
<v 6>I think she's right to have said that to him too. Yeah,</v>

240
00:14:12.070 --> 00:14:16.670
because he says it's a lie. Well, it is a lie. And how is it love?

241
00:14:16.820 --> 00:14:20.470
Doesn't machine can't love anything. Yes. And if you are a sensible human being,

242
00:14:20.471 --> 00:14:22.870
you know that and he's sitting right there on the desk.

243
00:14:22.871 --> 00:14:25.930
It's not pretending well these are sensible human beings that were already a

244
00:14:25.931 --> 00:14:28.630
little bit seduced. Let Matt just go forward a hundred years.

245
00:14:28.810 --> 00:14:33.100
Imagine a machine that is very sophisticated, very fluent,

246
00:14:33.101 --> 00:14:36.040
very convincingly human. You're talking about blade runner basically. Yeah,

247
00:14:36.060 --> 00:14:36.880
exactly.

248
00:14:36.880 --> 00:14:41.880
At that point I think I would require some kind of label to remind me that this

249
00:14:43.090 --> 00:14:44.070
is a thing.

250
00:14:44.160 --> 00:14:46.260
<v 0>It's not a being, it's just a thing. Okay.</v>

251
00:14:46.261 --> 00:14:50.400
But if here's something to think about, if the machines get to that point,

252
00:14:50.550 --> 00:14:52.010
which is a big, where you

253
00:14:52.010 --> 00:14:52.910
<v 4>would want to label them. Yeah.</v>

254
00:14:53.130 --> 00:14:58.070
We are going to need a way to know when they've crossed that line and become

255
00:14:58.940 --> 00:15:00.300
mind full. Yeah.

256
00:15:00.320 --> 00:15:04.960
So I should back up for a sec and say that in 1950, they're,

257
00:15:04.961 --> 00:15:08.590
they're just starting to develop the computer and they're already asking these

258
00:15:08.591 --> 00:15:12.370
philosophical questions like, can these machines think, you know,

259
00:15:12.371 --> 00:15:16.960
will be someday be able to make a machine that could think, uh, and if we did,

260
00:15:16.961 --> 00:15:17.794
how would we know?

261
00:15:18.100 --> 00:15:22.930
And so a British mathematician named Alan Turing proposed a simple thought

262
00:15:22.931 --> 00:15:26.140
experiment. Here's how we'll know when the machines make it across the line.

263
00:15:26.170 --> 00:15:28.390
Get a person, sit them down at a computer,

264
00:15:28.540 --> 00:15:32.920
have them start a conversation in text. Hi, how are you? Enter good.

265
00:15:32.921 --> 00:15:35.320
Pops up on the screen. Sort of like Internet chat. Yep.

266
00:15:35.350 --> 00:15:38.620
So after that first conversation, have them do it again and then again,

267
00:15:38.621 --> 00:15:41.720
you know, hi, hello, how are you? Et cetera. Back and forth. And again,

268
00:15:41.780 --> 00:15:43.960
right over and over. But here's the catch.

269
00:15:43.990 --> 00:15:46.570
Half of these conversations will be with real people.

270
00:15:47.110 --> 00:15:51.520
Half will be with these computer programs that are basically impersonating

271
00:15:51.521 --> 00:15:53.480
people. And the person in the seat,

272
00:15:53.481 --> 00:15:56.540
the human has to judge which of the conversations were with people,

273
00:15:56.541 --> 00:15:57.620
which were with humans.

274
00:15:57.980 --> 00:16:02.480
Turing's idea was that if those computer fakes could fool the human judge a

275
00:16:02.481 --> 00:16:06.740
certain percentage of the time Turing's magic threshold was 30% then at that

276
00:16:06.741 --> 00:16:09.720
point we can basically consider machines intelligent. Cause you know,

277
00:16:09.840 --> 00:16:11.820
if you can't tell the machine isn't human,

278
00:16:11.940 --> 00:16:14.970
then you can't say it's not intelligent. Yeah. That's basically, yeah.

279
00:16:15.030 --> 00:16:19.010
You said 30% of the time. Yeah. Turing is a natural number to me would be half.

280
00:16:19.011 --> 00:16:23.880
You know, 51% would seem to be like the coaching moment, 30% I don't know.

281
00:16:24.090 --> 00:16:28.650
Well 51% is actually a horrifying number in the context of the turning test

282
00:16:28.651 --> 00:16:32.400
because you've got these two conversations and you're trying to decide which is

283
00:16:32.401 --> 00:16:35.430
the real person. So if the computer were indistinguishable,

284
00:16:35.670 --> 00:16:38.940
that would be 50% you know, the judge is doing no better than chance.

285
00:16:38.970 --> 00:16:43.970
So if a computer hits 51% that means there they've out human to the human

286
00:16:44.031 --> 00:16:44.940
million. That is horrifying.

287
00:16:48.750 --> 00:16:51.180
Something to keep in mind when throwing thought this whole thing up.

288
00:16:51.210 --> 00:16:56.210
The technology was so new. Computers barely existed that it was sort of a,

289
00:16:56.211 --> 00:16:59.380
a leap of imagination really. But no longer Robert Bridge.

290
00:16:59.480 --> 00:17:01.480
Can you give me like some excitement music here?

291
00:17:01.540 --> 00:17:03.880
<v 10>Absolutely good because every year</v>

292
00:17:06.340 --> 00:17:09.820
the greatest technologist on the planet named small on embar. Hi,

293
00:17:09.821 --> 00:17:14.740
I'm really copping to meet in a small room with folding chairs I developed in

294
00:17:14.741 --> 00:17:19.360
Java and put out in touring's questions to the ultimate test.

295
00:17:23.230 --> 00:17:27.280
<v 4>Really it's just a couple of dudes you know who haven't seen the sun in 10 years</v>

296
00:17:27.670 --> 00:17:31.990
in a room, but we do now have this thing called the lobe ner prize,

297
00:17:31.991 --> 00:17:35.440
which is essentially a year elite actual touring test.

298
00:17:35.480 --> 00:17:40.400
<v 10>Each judge on our judges table is going to be communicating with two entities,</v>

299
00:17:40.401 --> 00:17:42.780
one human and one trainer.

300
00:17:42.920 --> 00:17:46.520
<v 4>The way the stage is set up is you've got the judges at a table on the left on</v>

301
00:17:46.521 --> 00:17:47.354
laptops,

302
00:17:47.750 --> 00:17:52.470
then a bunch of giant server looking machines in the middle of that programmers

303
00:17:52.471 --> 00:17:56.310
are fiddling with and then there's a curtain on the right hand side and we're

304
00:17:56.311 --> 00:18:00.420
behind the curtain. Brian actually participated in the 2009 lobe,

305
00:18:00.421 --> 00:18:04.700
no prize competition but not as a programmer as one of the four quote,

306
00:18:04.950 --> 00:18:05.731
confederates.

307
00:18:05.731 --> 00:18:09.630
The confederates are the real people that the judges are talking to cause you

308
00:18:09.631 --> 00:18:12.210
remember half the conversations that have are with people.

309
00:18:12.211 --> 00:18:15.630
Half are with computers and then Brian decided to participate that year because

310
00:18:15.631 --> 00:18:18.570
the year before 2008 the top program minister fool,

311
00:18:18.600 --> 00:18:21.780
25% of the judging panel pretty close to Turing's number.

312
00:18:21.810 --> 00:18:26.810
Exactly one vote away and so I felt to some extent how can I get involved on

313
00:18:30.151 --> 00:18:31.470
behalf of humanity?

314
00:18:31.471 --> 00:18:35.550
How can I up take a scan that's a modest position for you.

315
00:18:36.100 --> 00:18:41.100
<v 11>All right machines please hold your vices are now representing all humans.</v>

316
00:18:42.250 --> 00:18:46.920
<v 4>Brian, Chris Jones. Now in terms of what Brian is up against,</v>

317
00:18:46.921 --> 00:18:50.640
the computer programs have a a variety of different strategies. For example,

318
00:18:50.641 --> 00:18:53.550
there was one program Brian's year that would do kind of a double fake out

319
00:18:54.090 --> 00:18:57.540
[inaudible] where it would pretend not to be a person but a person who is

320
00:18:57.541 --> 00:19:00.090
sarcastically pretending to be a robot.

321
00:19:00.300 --> 00:19:04.740
Oh people would ask it a simple question and it would say I don't have enough

322
00:19:04.741 --> 00:19:08.970
ram to answer that question. Smiley face and everyone would be like, oh,

323
00:19:08.971 --> 00:19:11.850
this is such a wise guy. Ha Ha clips.

324
00:19:13.170 --> 00:19:17.890
I want to tell you now about one particular Bot that competed. Brian's here.

325
00:19:18.150 --> 00:19:20.190
Hi, I'm rolling. Carpenter. That's the guy who made it.

326
00:19:20.191 --> 00:19:22.620
My program is called clever bots and that's the BOT.

327
00:19:22.890 --> 00:19:27.020
This is a program that employs a very spooky, spooky, the right route,

328
00:19:27.040 --> 00:19:28.440
a very spooky strategy.

329
00:19:28.590 --> 00:19:33.590
<v 11>You may be surprised to hear that despite the fact that it's called Clever Bot,</v>

330
00:19:33.930 --> 00:19:35.370
it states that it is a Bot.

331
00:19:35.371 --> 00:19:38.010
It states that it is never a human right there in front of them.

332
00:19:38.070 --> 00:19:39.330
Despite those facts.

333
00:19:39.510 --> 00:19:44.510
I received several emails a day from people who believe that actually they are

334
00:19:44.911 --> 00:19:46.080
being connected to humans.

335
00:19:47.680 --> 00:19:48.100
<v 12>[inaudible]</v>

336
00:19:48.100 --> 00:19:50.680
<v 11>oh, like they think they've been tricked. Yes.</v>

337
00:19:50.950 --> 00:19:54.010
Tricked into coming to a site that claims to be a Bot when in fact they're

338
00:19:54.011 --> 00:19:54.844
talking to humans.

339
00:19:56.090 --> 00:19:56.820
<v 12>[inaudible]</v>

340
00:19:56.820 --> 00:19:57.240
<v 11>but no,</v>

341
00:19:57.240 --> 00:20:02.240
no program could possibly respond in this way and there is a certain element of

342
00:20:02.521 --> 00:20:05.170
truth in that to explain Rolo carb material,

343
00:20:05.220 --> 00:20:08.850
like Brian was one of those kids who is completely obsessed by computers.

344
00:20:08.880 --> 00:20:10.800
I was indeed a computer, a kid.

345
00:20:10.801 --> 00:20:14.700
And when he was just a teenager age about 16 or so, wrote his first Chat Bot,

346
00:20:14.730 --> 00:20:18.150
I created a program that talked to me. No kidding. Yes.

347
00:20:18.210 --> 00:20:21.010
You typed in something and it would say something back though.

348
00:20:21.030 --> 00:20:24.570
At that time the responses were essentially pre-program and really simple.

349
00:20:24.810 --> 00:20:27.660
Kind of like Eliza. But one evening,

350
00:20:27.661 --> 00:20:31.860
I think fast forward many years he is in his apartment and one night he says a

351
00:20:32.230 --> 00:20:33.990
switch suddenly flipped in my, in my mind

352
00:20:35.750 --> 00:20:36.300
<v 12>[inaudible]</v>

353
00:20:36.300 --> 00:20:41.000
<v 11>Nice. Had any sore, how to make the machine learn on its own.</v>

354
00:20:41.540 --> 00:20:44.270
What if you thought, what if it just started at zero,

355
00:20:44.720 --> 00:20:49.450
like a little baby and it would grow in these discrete little increments every

356
00:20:49.451 --> 00:20:52.010
time you talk to it? Right. Basically, uh,

357
00:20:52.030 --> 00:20:55.750
w the first thing that was said to that program that I created,

358
00:20:55.751 --> 00:21:00.010
the first version of that night, um, was said back by it.

359
00:21:00.070 --> 00:21:04.270
Meaning if he said to at hello it now knew one thing, the word hello.

360
00:21:04.271 --> 00:21:06.100
So it would say hello back.

361
00:21:06.130 --> 00:21:10.060
The second thing it said was a choice of the first two things that to it.

362
00:21:10.061 --> 00:21:13.300
So if the second thing you said was how are you doing it now? New Two things.

363
00:21:13.390 --> 00:21:15.310
The where, hello and the phrase, how are you doing?

364
00:21:15.311 --> 00:21:19.570
So it could either say hello back again or how are you doing?

365
00:21:19.600 --> 00:21:23.140
The third thing it said was a choice of the first three things and so on.

366
00:21:23.350 --> 00:21:25.720
Ad Infinitum will not quite ad infinitum.

367
00:21:25.721 --> 00:21:30.721
But between 1988 and 1997 a few thousand conversations took place between myself

368
00:21:33.431 --> 00:21:35.720
in it and a few of my friends and it,

369
00:21:35.920 --> 00:21:38.980
he and his friends would sit there and type things to it as a way of teaching

370
00:21:38.981 --> 00:21:42.790
it, new things, but it was just them. So it was slow going.

371
00:21:42.791 --> 00:21:44.440
So it languished for quite a long time.

372
00:21:46.220 --> 00:21:48.580
But then I started working with the Internet.

373
00:21:49.710 --> 00:21:54.370
Put your own line where anyone could talk to it. Within the next 10 years,

374
00:21:54.430 --> 00:21:58.780
it had learned something like 5 million lines of conversation.

375
00:22:01.090 --> 00:22:06.090
Now [inaudible] is frequently handling around 200,000 requests an outlaw,

376
00:22:09.610 --> 00:22:13.540
and it's talking to more than 3 million people a month,

377
00:22:14.230 --> 00:22:16.660
3 million conversations a month.

378
00:22:16.690 --> 00:22:20.770
And after each one [inaudible] knows a little bit more than it did before and

379
00:22:20.771 --> 00:22:23.460
every time you say something to it like, hey, clever,

380
00:22:23.461 --> 00:22:27.000
but why am I so sad?

381
00:22:27.840 --> 00:22:32.840
It is accessing the conversations that millions of people have had in the past

382
00:22:34.530 --> 00:22:38.190
asking itself, what is the best overlap? Where is the best correlation?

383
00:22:38.220 --> 00:22:42.360
How do people usually answer this question? Why am I so sad? And then

384
00:22:44.430 --> 00:22:48.680
I response clever about answers just because, hmm.

385
00:22:49.110 --> 00:22:53.370
All right. Well why? There must be a reason why I'm so sad

386
00:22:56.430 --> 00:22:59.640
because you have been sitting in the same place for too long.

387
00:23:00.510 --> 00:23:03.360
Is that WHO's title? Who's saying that? Exactly?

388
00:23:03.420 --> 00:23:04.650
Where does that response come from?

389
00:23:05.180 --> 00:23:08.610
And the answer is it is one human being at some point in the past.

390
00:23:08.610 --> 00:23:12.570
Having said that, so that is one moment of human conversation from one person.

391
00:23:12.571 --> 00:23:14.730
Yes. So it's like I'm talking to a ghost.

392
00:23:14.820 --> 00:23:18.210
You are talking to it's intelligence if you like,

393
00:23:18.270 --> 00:23:21.450
is borrowed from millions of people in the past.

394
00:23:21.980 --> 00:23:26.100
I'm a little bit of that conversational knowledge that conversational

395
00:23:26.101 --> 00:23:29.370
intelligence goes into forming your reply. Now,

396
00:23:29.371 --> 00:23:32.400
what's interesting says Rolo is that when you start a conversation with clever

397
00:23:32.401 --> 00:23:36.480
Bot, it doesn't really have a personality or no one personality ever bought is

398
00:23:36.481 --> 00:23:40.090
everything to everyone. It's just this big hive really. I have seen it.

399
00:23:40.110 --> 00:23:43.310
But as you keep talking to it and it's sort of pulling forward from the high of

400
00:23:43.320 --> 00:23:45.980
these little ghost fragments of

401
00:23:45.980 --> 00:23:50.180
<v 4>past conversations, stitching them together, a form does kind of emerge</v>

402
00:23:50.600 --> 00:23:53.810
<v 11>jet, uh, it reflects the person that is speaking to.</v>

403
00:23:54.080 --> 00:23:58.850
It becomes somewhat like that person. Someone familiar already.

404
00:23:58.880 --> 00:24:01.460
People have very emotional conversations with it.

405
00:24:01.461 --> 00:24:05.300
People have complete arguments with it and um,

406
00:24:06.110 --> 00:24:10.640
of course they try to, um, to get it into bed by talking dirty to it.

407
00:24:10.670 --> 00:24:15.440
Yeah. One thing I can tell you is that I, I have seen a single person,

408
00:24:15.470 --> 00:24:16.940
a teenage girl,

409
00:24:17.840 --> 00:24:22.040
I'm speaking for 11 hours with just a three 15 minute breaks.

410
00:24:22.430 --> 00:24:23.263
Whoa.

411
00:24:24.050 --> 00:24:24.570
<v 13>[inaudible]</v>

412
00:24:24.570 --> 00:24:26.370
<v 11>about what everything.</v>

413
00:24:26.760 --> 00:24:31.760
The day will come not too far down the road where clever Bot becomes so

414
00:24:32.910 --> 00:24:37.910
interesting to talk to that people will be talking to it all day every day,

415
00:24:39.270 --> 00:24:42.420
<v 4>but we're not there yet because the same thing that makes clever bots so</v>

416
00:24:42.421 --> 00:24:46.560
interesting to talk to also can make it kind of ridiculous.

417
00:24:47.040 --> 00:24:48.710
For example, in our interview with Brian,

418
00:24:48.990 --> 00:24:52.170
he was the first person that turned us on to this program. As we were talking,

419
00:24:52.171 --> 00:24:56.300
Soren just sort of suggested, well, why don't we just try it right now. Try it.

420
00:24:56.480 --> 00:24:59.550
You want to talk? You want to tell it? Say to Clever Bot, I feel blue. Sure.

421
00:24:59.580 --> 00:25:03.810
Yeah. Are you pulling clever Bot up? Is it just clever bot.org or something?

422
00:25:05.130 --> 00:25:07.260
calm.com I feel. Can you say,

423
00:25:07.261 --> 00:25:11.100
I feel blue cause an asteroid hit my house this morning. And so this is,

424
00:25:11.180 --> 00:25:16.020
you've hit on a perfect strategy of a, of dealing with these bonds absurdity.

425
00:25:16.470 --> 00:25:17.430
Yes. Well,

426
00:25:17.431 --> 00:25:21.930
it basically saying something that has never been said before to clever Bot,

427
00:25:22.560 --> 00:25:26.610
so it's likely that no one has ever claimed an asteroid hit their house.

428
00:25:26.760 --> 00:25:30.410
It's a weird enough that it may not be in the database. Okay. Alright.

429
00:25:30.870 --> 00:25:32.820
Let's see what it says. Steroid hit my house this morning.

430
00:25:34.830 --> 00:25:35.790
<v 13>Clever bye.</v>

431
00:25:38.360 --> 00:25:40.040
<v 4>I woke up at 1:00 PM this afternoon.</v>

432
00:25:44.060 --> 00:25:49.010
Well, there we go. It's not quite so clever. You don't have to worry yet.

433
00:25:49.730 --> 00:25:54.620
Which in fact when I went online to youtube and watched the lobe new competition

434
00:25:54.621 --> 00:25:58.040
that Brian attended [inaudible] it turns out none of the computers fooled the

435
00:25:58.041 --> 00:26:01.130
judges at all. None. Any, well, I don't know if none, none,

436
00:26:01.131 --> 00:26:02.360
but they did really badly

437
00:26:05.630 --> 00:26:08.180
between the programs. For me,

438
00:26:08.181 --> 00:26:12.470
one of the strange takeaways of thinking so much about artificial intelligence

439
00:26:12.920 --> 00:26:17.920
is this feeling of how complex it is to sit across the table from someone and

440
00:26:19.371 --> 00:26:22.230
communicate with body language and tone and you know,

441
00:26:22.430 --> 00:26:23.930
rhythm and all of these things.

442
00:26:24.260 --> 00:26:28.550
What happens when those conversations are working out well is that we're willing

443
00:26:28.551 --> 00:26:33.290
to move the conversation in ways that allows us to be sort of perpetually

444
00:26:33.650 --> 00:26:37.910
startling to one another. That's a good word. Startling.

445
00:26:38.300 --> 00:26:39.133
<v 13>Yeah.</v>

446
00:26:39.390 --> 00:26:42.180
<v 4>You learn someone through these small surprises.</v>

447
00:26:51.140 --> 00:26:53.450
<v 0>Thanks to Brian Christian, his excellent book,</v>

448
00:26:53.451 --> 00:26:56.360
which inspired this hour is called the most human human.

449
00:26:56.390 --> 00:27:00.140
Go to radiolab.org for more info. Thanks also to our actors,

450
00:27:00.141 --> 00:27:03.320
Sarah thyer and the Richter and Susan Blackwell.

451
00:27:03.730 --> 00:27:06.670
<v 14>Hi, this is Brian Christian radio lab is,</v>

452
00:27:07.010 --> 00:27:07.843
<v 15>hello.</v>

453
00:27:08.270 --> 00:27:13.270
I'm a machine radio lab is funded in part by the Alfred p Sloan Foundation

454
00:27:14.810 --> 00:27:19.010
enhancing public understanding of science and technology in the modern world.

455
00:27:19.260 --> 00:27:24.260
<v 14>More information aboutSloan@wwwdotsloan.org this is Sherry Turkle.</v>

456
00:27:25.650 --> 00:27:29.790
Radiolab is produced by definitely NYC and distributed by NPR.

457
00:27:30.450 --> 00:27:31.283
Bye Bye.

458
00:27:32.810 --> 00:27:37.070
<v 0>Our country continues to grapple with the deep cultural divisions exposed by the</v>

459
00:27:37.071 --> 00:27:39.380
2016 elections. We be Fred Pogue.

460
00:27:39.410 --> 00:27:44.090
There is some palpable change in how people are looking at as well.

461
00:27:44.120 --> 00:27:45.710
Maybe A, we are looking at people.

462
00:27:45.920 --> 00:27:50.750
W NYC is talking to a broad range of Americans about how we arrived at this

463
00:27:50.751 --> 00:27:52.340
moment in our political culture.

464
00:27:52.760 --> 00:27:55.970
I'm Kai Reich subscribed to the United States of anxiety,

465
00:27:56.030 --> 00:28:00.460
culture wars from wn YC studios. Wherever you get your pot tests.

466
00:28:05.720 --> 00:28:08.180
Hey, I'm Jad Abumrad. I'm Robert Krulwich.

467
00:28:08.181 --> 00:28:12.320
This is radio lab and we are exploring the blur that takes place when humans and

468
00:28:12.321 --> 00:28:16.500
machines interact and investigate each other. Talk to each other. Yeah.

469
00:28:16.530 --> 00:28:20.600
See that's the thing. In the last act, we were always talking, talking, talking,

470
00:28:20.601 --> 00:28:25.260
talking. How about we encountered machines in a different way. How about we?

471
00:28:25.320 --> 00:28:29.670
No talking, no talking. We touched them illegally. We pet them,

472
00:28:29.671 --> 00:28:30.810
we sniff them. We you.

473
00:28:30.900 --> 00:28:34.320
We do essential things that don't involve the sophisticated business of

474
00:28:34.321 --> 00:28:38.580
conversation. Okay. This is freedom Bayer. Yes, it is.

475
00:28:38.850 --> 00:28:42.240
Who's not a machine? I don't think so. I'm Jad and this is, I'm Robert here.

476
00:28:42.270 --> 00:28:43.710
Hi there. Nice to meet both of you.

477
00:28:43.740 --> 00:28:48.140
We called her up because freedom actually had her own kind of moment with a

478
00:28:48.250 --> 00:28:48.930
[inaudible].

479
00:28:48.930 --> 00:28:49.470
<v 3>Yup. Yup.</v>

480
00:28:49.470 --> 00:28:54.470
This was around 1999 freedom was a graduate student at the media lab at MIT.

481
00:28:54.690 --> 00:28:58.950
What were you doing there? We were developing cinema of the future,

482
00:28:58.980 --> 00:29:03.980
so we were working on creating a virtual characters that you can interact with.

483
00:29:05.380 --> 00:29:07.620
Anyhow, she was also thinking about becoming a mom. Yeah.

484
00:29:07.740 --> 00:29:11.430
I knew I wanted to be a mom someday and she decided to practice.

485
00:29:11.460 --> 00:29:14.190
I got two durables, twinkie and Ho-ho.

486
00:29:14.640 --> 00:29:17.410
So I had these two live pets and,

487
00:29:17.490 --> 00:29:20.970
and then she got herself a pet that was well not so alive. Yeah,

488
00:29:20.971 --> 00:29:24.420
I've got it right here. And you're not going against the mic so we can hear it.

489
00:29:24.421 --> 00:29:27.980
Say hello to. Yeah. Yeah, there is. I Furby

490
00:29:31.800 --> 00:29:35.280
at that time, the first furbies were hot and happening.

491
00:29:35.520 --> 00:29:38.030
Can you describe a Furby for those of us who share?

492
00:29:38.280 --> 00:29:43.280
It's about five inches tall and the Furby is pretty much all head.

493
00:29:43.541 --> 00:29:47.200
It's just a big round, fluffy head with two little feet sticking out the front.

494
00:29:47.320 --> 00:29:50.890
It has big eyes. Apparently it makes noises. Yeah.

495
00:29:51.220 --> 00:29:53.770
If you tickle its tummy, it will

496
00:29:55.300 --> 00:29:55.540
<v 13>[inaudible].</v>

497
00:29:55.540 --> 00:30:00.540
<v 3>It would say this me and it would want you to just keep playing with it.</v>

498
00:30:02.560 --> 00:30:06.640
So, you know, I spent about 10 weeks using the Furby.

499
00:30:06.820 --> 00:30:11.440
I would carry it around in my bag and one day she's hanging out with her baby

500
00:30:11.560 --> 00:30:13.960
and she notices something very eerie.

501
00:30:18.320 --> 00:30:18.390
<v 13>[inaudible].</v>

502
00:30:18.390 --> 00:30:23.310
<v 3>What I discovered is if you hold it upside down, it will say,</v>

503
00:30:23.510 --> 00:30:25.260
Hey, nice,

504
00:30:26.040 --> 00:30:30.440
<v 16>scared, [inaudible],</v>

505
00:30:30.441 --> 00:30:34.910
scared, scared, and me as the, you know,

506
00:30:34.911 --> 00:30:38.390
the sort of owner slash user of this Furby would get really uncomfortable with

507
00:30:38.391 --> 00:30:40.280
that and then turn it back on upright.

508
00:30:40.830 --> 00:30:42.810
<v 3>Cause once you have an upright, it's, it's fine.</v>

509
00:30:42.811 --> 00:30:44.070
It goes right back and then it's fine.

510
00:30:44.071 --> 00:30:48.810
So it's got some sensor in it that knows you know what direction it's facing or

511
00:30:48.811 --> 00:30:53.640
maybe it's just scared. [inaudible] sorry. Anyway,

512
00:30:53.850 --> 00:30:55.140
Lou was, she thought, well wait a second.

513
00:30:55.141 --> 00:30:59.820
Now this could be sort of a new way that you could use to draw the line between

514
00:30:59.821 --> 00:31:01.920
what's human and what's machine. Yeah.

515
00:31:01.950 --> 00:31:05.100
Kind of this kind of emotional Turing test.

516
00:31:06.960 --> 00:31:09.690
Can you guys hear me? Yes, I can hear you.

517
00:31:09.800 --> 00:31:13.680
If we actually wanted to do this task, could you help? How would we do it?

518
00:31:13.710 --> 00:31:15.920
Exactly? How are you guys doing? Yeah,

519
00:31:16.170 --> 00:31:20.700
you would need a group of kids. Can you guys tell me Your name? I'm Olivia

520
00:31:22.420 --> 00:31:26.310
[inaudible] and I'm SAIDI. Alright. I'm thinking six, seven and eight year olds.

521
00:31:26.311 --> 00:31:29.820
How old are you guys? Seven. The age of reason, you know,

522
00:31:30.030 --> 00:31:34.170
eight then says freedom. We're going to need three things. Furby of course,

523
00:31:34.320 --> 00:31:39.010
Barbie, Barbie doll and jurby and securable. A real general.

524
00:31:39.030 --> 00:31:42.410
Yeah, and we did find one separate. Turned out to be a hamster. Sorry,

525
00:31:42.660 --> 00:31:45.480
you're a hamster, but we're going to call you Jeremy. So you've got Barbie,

526
00:31:45.900 --> 00:31:49.280
Furby, Gerbi, Barbie, Furby and [inaudible]. Right? Should we just second,

527
00:31:49.281 --> 00:31:51.120
what question are we asking in this test?

528
00:31:51.150 --> 00:31:55.800
The question was how long can you keep it upside down before you yourself feel

529
00:31:55.801 --> 00:31:59.700
uncomfortable? So we should time the kids as they hold each one upside down.

530
00:31:59.730 --> 00:32:03.030
Yeah, including the German. Yeah. You're going to have a Barbie. That's a doll.

531
00:32:03.031 --> 00:32:04.760
You're going to Jeremy, which is alive now.

532
00:32:04.770 --> 00:32:08.790
Where would Furby fall in terms of time held upside down?

533
00:32:08.820 --> 00:32:12.630
Would it be closer to the living thing or to the doll that, I mean,

534
00:32:12.631 --> 00:32:17.220
that was really the question. Phase one. Okay, so here's what we're going to do.

535
00:32:17.221 --> 00:32:20.670
It's going to be really simple. You would have to say, well, here's a Barbie.

536
00:32:20.880 --> 00:32:23.310
Do you guys play with barbies? Just do a couple things.

537
00:32:23.311 --> 00:32:27.150
A few things with Barbie. Barbie's walking, looking at the flower,

538
00:32:27.210 --> 00:32:31.210
and then hold Barbie upside down. Okay.

539
00:32:31.240 --> 00:32:33.310
Let's see how long you can hold Barbie like that.

540
00:32:35.110 --> 00:32:38.770
I could probably do it obviously very long. Let's just see

541
00:32:41.570 --> 00:32:44.780
whenever you feel like you want to turn it around. I'm happy.

542
00:32:50.010 --> 00:32:50.640
<v 13>[inaudible]</v>

543
00:32:50.640 --> 00:32:55.370
<v 3>this went on forever, so let's just fast forward a bit. Gay.</v>

544
00:32:55.371 --> 00:32:56.204
And

545
00:32:59.930 --> 00:33:03.500
so what we learned here in phase one is the not surprising fact that kids can

546
00:33:03.501 --> 00:33:07.720
hold Barbie dolls upside down. Yeah.

547
00:33:07.740 --> 00:33:10.190
Really was forever. It could have been longer, but their arms got tired.

548
00:33:11.300 --> 00:33:13.890
Alright, so that was the first task time for phase two.

549
00:33:13.920 --> 00:33:16.750
Do the same thing with Derby.

550
00:33:16.810 --> 00:33:20.210
So out with Barbie in with Derby,

551
00:33:24.260 --> 00:33:27.530
are we going to have to hold them upside down? That's the test. Yeah.

552
00:33:28.490 --> 00:33:32.650
So which one of you would like to, okay. Ready? Oh God.

553
00:33:32.690 --> 00:33:37.550
You have to hold Gerbi kind of firmly by the way.

554
00:33:37.551 --> 00:33:40.510
No rodents were harmed in this whole situation. Yeah,

555
00:33:40.511 --> 00:33:43.580
she is pretty screaming to be upside down.

556
00:33:49.480 --> 00:33:52.900
Okay. Okay. So as you heard, uh,

557
00:33:53.530 --> 00:33:55.570
the kids turned derby over very fast.

558
00:33:55.810 --> 00:33:59.620
I just didn't get hurt on average eight seconds.

559
00:33:59.980 --> 00:34:01.780
I was thinking, oh my God.

560
00:34:03.440 --> 00:34:07.060
And it was a tortured eight seconds. Now phase three.

561
00:34:07.540 --> 00:34:09.080
So this is a first

562
00:34:14.350 --> 00:34:18.720
Louisa, you take Furby in your hand now can you turn Furby upside down?

563
00:34:18.750 --> 00:34:19.583
Hold her still.

564
00:34:44.780 --> 00:34:46.480
<v 13>[inaudible] whoa.</v>

565
00:34:46.770 --> 00:34:47.603
<v 3>She just turned it over.</v>

566
00:34:49.500 --> 00:34:53.220
So [inaudible] was eight seconds Barbie five to infinity.

567
00:34:53.580 --> 00:34:58.110
Furby turned out to be and freedom predicted this that a minute. In other words,

568
00:34:58.111 --> 00:34:59.760
the kids seem to treat this Furby,

569
00:34:59.761 --> 00:35:03.030
this toy more like a Gerbil than a Barbie doll.

570
00:35:03.060 --> 00:35:07.230
How come you turn them over so fast? I didn't want to be scared.

571
00:35:07.410 --> 00:35:11.910
Do you think he really felt scared? Yeah,

572
00:35:12.250 --> 00:35:14.660
I get it. Felt guilty. Really.

573
00:35:16.570 --> 00:35:19.830
It's a toil and all that, but still now,

574
00:35:19.831 --> 00:35:21.870
do you remember a time when you felt scared?

575
00:35:24.020 --> 00:35:25.790
<v 17>Yeah. Yeah.</v>

576
00:35:25.930 --> 00:35:28.540
<v 3>You don't have to tell me about it, but if you can remember it in your mind,</v>

577
00:35:28.620 --> 00:35:30.290
<v 17>I can do.</v>

578
00:35:31.150 --> 00:35:34.900
<v 3>Do you think when Furby says me scared that furbies feeling the same way?</v>

579
00:35:35.300 --> 00:35:39.540
<v 17>Yeah. No, no, no. Yeah, no. Yeah,</v>

580
00:35:41.210 --> 00:35:42.043
sure.

581
00:35:42.980 --> 00:35:45.110
<v 18>I think that it can feel pain.</v>

582
00:35:45.500 --> 00:35:50.500
Sort of the experience with the Furby seem to leave the kids kind of conflicted

583
00:35:51.860 --> 00:35:54.650
going in different directions at once. It was two thoughts,

584
00:35:54.890 --> 00:35:58.460
two thoughts at the same time. Yeah. One thought was like, look, I get it.

585
00:35:58.520 --> 00:36:03.140
It's a toy for crying out loud. But another thought was like, I'm still,

586
00:36:03.380 --> 00:36:07.880
he's helpless. It kind of made me feel guilty. It is sort of way heard.

587
00:36:07.881 --> 00:36:11.600
I made me feel like a coward, you know,

588
00:36:11.601 --> 00:36:13.880
when I was interacting with my Furby a lot,

589
00:36:14.070 --> 00:36:18.450
<v 19>I did have this feeling sometimes of having my chain yanked. Why would it,</v>

590
00:36:18.451 --> 00:36:19.590
why would a,

591
00:36:19.800 --> 00:36:22.740
is it just the little squeals that it's fake or is there something about the toy

592
00:36:22.741 --> 00:36:25.410
that makes it good at this? That was kind of my question.

593
00:36:25.411 --> 00:36:28.020
So I called up a video as well. I'll have him, I'm here,

594
00:36:28.170 --> 00:36:31.200
this freight train of a guy. Hey. Okay. This is Jad from Radiolab.

595
00:36:31.610 --> 00:36:35.100
Chad from radio lab. God, how are you ma? I'm good. Beautiful Day here in Boise.

596
00:36:35.101 --> 00:36:38.310
This is Caleb Chung. Yaks he designed the PRP. Yeah, we're,

597
00:36:38.330 --> 00:36:40.320
we're all Furby crazy here. So, uh,

598
00:36:40.590 --> 00:36:43.890
there's medication you can take for that gate to start.

599
00:36:43.920 --> 00:36:48.920
Can you just give me the sort of fast cutting MTV montage of your life leading

600
00:36:49.381 --> 00:36:53.550
up to Furby? Sure. Um, hippie parents out of the house at 15 and a half,

601
00:36:53.551 --> 00:36:56.850
put myself through junior high, started my first business at 19 or something,

602
00:36:56.860 --> 00:36:59.520
early twenties being a street mime in La Street mind wow.

603
00:36:59.550 --> 00:37:02.880
Became an actor at like 120 shows and an orangutan costume.

604
00:37:03.000 --> 00:37:05.160
Then I started working on special effects and building my own,

605
00:37:05.190 --> 00:37:07.950
taking those around a studios and let me in the suit, build a suit around me,

606
00:37:07.951 --> 00:37:11.700
put me on location. I could fix it when it broke. Wow. That was, anyhow,

607
00:37:12.060 --> 00:37:15.570
after a long and circuitous route, Caleb Chung eventually made it into toys.

608
00:37:15.600 --> 00:37:16.980
I answered an ad at Mattel,

609
00:37:17.040 --> 00:37:20.840
found himself in his garage and there's piles of styrene, plastic xacto knife,

610
00:37:21.120 --> 00:37:24.320
super glue, little mull Bucci Motors making these a little prototypes. Yeah.

611
00:37:24.450 --> 00:37:26.910
And the goal he says was always very simple.

612
00:37:26.911 --> 00:37:30.270
How do I get a kid to have this thing? Hang around with them for a long time?

613
00:37:30.300 --> 00:37:31.980
How do I get a kid to actually bond with it?

614
00:37:31.981 --> 00:37:35.190
Most toys you play for 15 minutes and then you put him in the corner until their

615
00:37:35.191 --> 00:37:36.024
batteries are dead.

616
00:37:36.260 --> 00:37:38.540
I wanted something that they would play with for a long time.

617
00:37:39.140 --> 00:37:43.910
So how do you make that toy? Well, um, there's rules,

618
00:37:43.911 --> 00:37:44.721
there's, you know,

619
00:37:44.721 --> 00:37:49.400
the size of the eyes there is the distance of the top lid to the pupil. Right?

620
00:37:49.401 --> 00:37:52.000
You don't want any of the top of the white of your eye showing that.

621
00:37:52.330 --> 00:37:55.040
That's freaky surprise. Then when it came to the eyes,

622
00:37:55.400 --> 00:37:57.380
I had a choice with my one little mechanism.

623
00:37:57.381 --> 00:38:00.800
I can make the eyes go left or right or up and down. So it's up to you.

624
00:38:00.980 --> 00:38:03.840
You can make the eyes go left or right or up and down. Do you have a,

625
00:38:03.900 --> 00:38:05.880
do you have a reference or right, or up and down?

626
00:38:05.910 --> 00:38:10.230
I think I would choose left or right. Okay. So not sure why I say that,

627
00:38:10.231 --> 00:38:12.660
but that's all right. So let's, let's take that apart.

628
00:38:13.020 --> 00:38:16.020
Let's if you're talking to somebody and they look left or right while they're

629
00:38:16.021 --> 00:38:19.890
talking to you, what does that communicate? Oh, shift D or they're,

630
00:38:20.160 --> 00:38:23.160
they're trying to find the person who's more important than you behind us. Okay.

631
00:38:23.161 --> 00:38:25.890
Now I wanna change my answer. Now I want to say up and down, you would,

632
00:38:26.280 --> 00:38:28.920
if you look at a baby and the way that a baby looks at their mother,

633
00:38:29.100 --> 00:38:32.430
they track from eyebrows to mouth. They track up and down on the face.

634
00:38:32.490 --> 00:38:35.950
So how'd you, how'd you made Furby look left and right rather than up and down?

635
00:38:35.980 --> 00:38:39.700
It would have probably flopped. No, it wouldn't have flopped.

636
00:38:39.701 --> 00:38:44.290
It would just sucked a little. It's like a bad actor who uses his arms too much.

637
00:38:44.400 --> 00:38:48.370
Yeah. You'd notice it and it would keep you from just being in the moment.

638
00:38:48.690 --> 00:38:50.850
But what is the thought behind that? Is it, is it,

639
00:38:50.851 --> 00:38:55.851
is it that you want to convince the child that the thing they're using is fill

640
00:38:57.271 --> 00:38:59.220
in the blank what you alive?

641
00:39:03.340 --> 00:39:05.280
There's three elements I believe in,

642
00:39:05.281 --> 00:39:08.830
in creating something that feels to a human, like it's alive,

643
00:39:09.010 --> 00:39:10.450
like kind of rewrote Asimov's laws.

644
00:39:10.990 --> 00:39:14.980
The first is it has to feel and show emotions where you drawing on your mind

645
00:39:14.981 --> 00:39:17.860
days for that. Of course there's experiences in the park. Of course,

646
00:39:18.040 --> 00:39:21.580
you really break the body into part and you realize you can communicate

647
00:39:21.581 --> 00:39:22.300
physically.

648
00:39:22.300 --> 00:39:24.730
So if your chest goes up and your head goes up and your arms grew up,

649
00:39:24.760 --> 00:39:27.280
you know that's happy. If your head is forward and your chest is forward,

650
00:39:27.281 --> 00:39:29.200
you're kind of this angry guy. And he says,

651
00:39:29.280 --> 00:39:30.990
and when it came time to make Furby timid,

652
00:39:31.120 --> 00:39:35.050
he took that gesture language and focused it on furbies ear hears when they went

653
00:39:35.051 --> 00:39:37.570
up. That was surprised. And when they went down it was depression.

654
00:39:38.110 --> 00:39:39.010
So that's rule number one.

655
00:39:39.040 --> 00:39:43.150
The second rule is to be aware of themselves and their environment.

656
00:39:43.510 --> 00:39:46.720
So if there's a loud noise, it needs to know that there was a loud noise.

657
00:39:46.721 --> 00:39:49.480
So he gave the Furby little sensors so that if you go,

658
00:39:50.600 --> 00:39:51.433
<v 3>it'll say</v>

659
00:39:54.010 --> 00:39:56.620
<v 19>the third thing is, uh, change over time.</v>

660
00:39:57.160 --> 00:40:00.940
Their behaviors have to change over time. That's a really important thing.

661
00:40:00.941 --> 00:40:04.260
It's a very powerful thing that we don't expect, but when it happens we go, wow.

662
00:40:04.720 --> 00:40:07.810
And so one of the ways we showed that was acquiring human language.

663
00:40:07.890 --> 00:40:12.810
<v 3>Yeah. When you first get your Furby it doesn't speak English, it speaks furbish</v>

664
00:40:14.730 --> 00:40:16.440
this kind of baby talk language

665
00:40:18.090 --> 00:40:19.830
and then the way it's programmed,

666
00:40:19.831 --> 00:40:24.510
it will sort of slowly over time replace its baby talk phrases with real English

667
00:40:24.511 --> 00:40:25.344
phrases.

668
00:40:28.050 --> 00:40:32.190
So you get the feeling that it's learning from you though. Of course it's not.

669
00:40:32.220 --> 00:40:35.850
No, it has no language comprehension. Right?

670
00:40:35.910 --> 00:40:38.260
So you've got these three rules, seal and show emotion,

671
00:40:38.390 --> 00:40:41.430
be aware of their environment, change over time. And oddly enough,

672
00:40:41.431 --> 00:40:43.500
they all seem to come together in that moment.

673
00:40:43.501 --> 00:40:47.550
You turn the Furby upside down because it seems to, no, it's upside down.

674
00:40:48.030 --> 00:40:51.660
So it's responding to its environment. It's definitely expressing emotion.

675
00:40:52.860 --> 00:40:53.940
And as you hold it there,

676
00:40:53.941 --> 00:40:58.170
what it's saying is changing over time because it starts with hey and then it

677
00:40:58.171 --> 00:41:00.810
goes to Meese and then it starts to cry.

678
00:41:04.080 --> 00:41:06.840
And all this adds up so that when you're holding the damn toy,

679
00:41:07.020 --> 00:41:11.070
even though you know it's just a toy, you still feel discomfort.

680
00:41:13.690 --> 00:41:17.320
<v 0>These creatures push our Darwinian buttons.</v>

681
00:41:17.530 --> 00:41:19.630
That's professor Sherry Turkle again. And she says,

682
00:41:19.690 --> 00:41:24.580
if they push just enough of these buttons, then something curious happens.

683
00:41:24.610 --> 00:41:28.330
The machines slip across this very important

684
00:41:28.730 --> 00:41:32.930
<v 19>from what I call relationships of projection to relationships.</v>

685
00:41:33.020 --> 00:41:34.190
And with

686
00:41:34.190 --> 00:41:38.900
<v 7>a doll, you project onto a doll what you need the doll to be.</v>

687
00:41:39.080 --> 00:41:43.130
If a young girl is feeling guilty about breaking her mom's China,

688
00:41:43.400 --> 00:41:47.600
she puts her Barbie dolls in detention with robots. Um,

689
00:41:48.050 --> 00:41:53.000
you really engage with the robot as though they're a significant other as though

690
00:41:53.001 --> 00:41:53.860
they're a person.

691
00:41:53.900 --> 00:41:57.770
<v 0>So the robot is in your story. The robot is its own story or it's [inaudible].</v>

692
00:41:58.090 --> 00:42:03.090
<v 7>And I think what we're forgetting as a culture is that there's nobody home.</v>

693
00:42:03.760 --> 00:42:04.593
There's nobody home.

694
00:42:04.650 --> 00:42:06.840
<v 19>Well, um, I have to ask you,</v>

695
00:42:06.841 --> 00:42:10.410
when is something alive Furby can remember these events,

696
00:42:10.500 --> 00:42:14.370
they effect what he does going forward and it changes his personality over time.

697
00:42:14.610 --> 00:42:17.430
He has all the attributes of fear or of happiness.

698
00:42:17.550 --> 00:42:20.670
And those are things that add up and change and changes behavior and how he

699
00:42:21.060 --> 00:42:24.150
interacts with the world, so how is that different than us?

700
00:42:24.180 --> 00:42:27.360
Wait a second though. Are you really going to go all the way there? Absolutely.

701
00:42:27.390 --> 00:42:32.390
This is a toy with servo motors and things that move its eyelids and w 200

702
00:42:32.881 --> 00:42:36.600
words, so you're saying that life is a level of complexity.

703
00:42:37.410 --> 00:42:39.480
If something is alive, it's just more complex.

704
00:42:39.720 --> 00:42:43.950
I think I'm saying that life is driven by the need to be alive and by these base

705
00:42:43.980 --> 00:42:47.610
primal animal feelings like pain and suffering. I can code that.

706
00:42:47.880 --> 00:42:50.250
I can code that. What do you mean? You can code that?

707
00:42:50.340 --> 00:42:54.480
Anyone who writes software and they do can say, okay, I need to stay alive.

708
00:42:54.750 --> 00:42:56.820
Therefore I'm going to come up with ways to stay alive.

709
00:42:56.821 --> 00:42:59.100
I'm going to do it in a way that's very human and I'm going to do it.

710
00:42:59.340 --> 00:43:04.200
We can mimic these things. Furby is miming the feeling of fear.

711
00:43:04.201 --> 00:43:09.060
It's not the same thing as being scared. It's not feeling scared. It is.

712
00:43:09.390 --> 00:43:12.900
How is it? It is. It's again a very simplistic version,

713
00:43:13.080 --> 00:43:17.190
but if you follow that trail you wind up with, with our neurons sending,

714
00:43:17.220 --> 00:43:20.370
you know, chemical things to do. Other parts of our body,

715
00:43:20.430 --> 00:43:24.870
our biological systems, our, our code is at a chemical level,

716
00:43:24.930 --> 00:43:28.890
incredibly dense and it evolved over millions of years. But it's just complex.

717
00:43:29.340 --> 00:43:33.510
It's not something different than what Furby does. It's just more complex.

718
00:43:33.630 --> 00:43:37.740
So would you say then that Furby is alive in the way that his eye level?

719
00:43:38.130 --> 00:43:42.190
At his lips? Yup. At his level. Would you say a cockroach is alive?

720
00:43:42.270 --> 00:43:45.300
<v 0>Yes. The one I kill a cockroach. I know that it's feeling like, okay,</v>

721
00:43:45.301 --> 00:43:47.380
so we went back and forth and back and forth about this.

722
00:43:47.450 --> 00:43:49.620
You were so close to arguing my position,

723
00:43:49.800 --> 00:43:52.340
you just said to me like that's not [inaudible].

724
00:43:52.460 --> 00:43:55.020
I know emotionally I am still in that place,

725
00:43:55.021 --> 00:43:59.040
but intellectually I can't rule out what he's saying that if you can build a

726
00:43:59.041 --> 00:44:00.330
machine that is so,

727
00:44:01.050 --> 00:44:04.980
such a perfect mimic of us in every single way and it gets complex enough,

728
00:44:05.310 --> 00:44:09.630
eventually it will be like a touring test pass. We just,

729
00:44:09.930 --> 00:44:14.130
the difference between us maybe is not so cold there. I can't go there. I can't,

730
00:44:14.970 --> 00:44:18.780
I can't imagine like the fellow who began this program who fell in love with the

731
00:44:18.781 --> 00:44:23.610
robot, that attachment wasn't real. The, the,

732
00:44:23.700 --> 00:44:24.061
the,

733
00:44:24.061 --> 00:44:28.740
the machine didn't feel anything like love back in that case it didn't.

734
00:44:28.741 --> 00:44:32.010
But imagine this fet Lana that is so subtle

735
00:44:32.010 --> 00:44:36.660
<v 3>and [inaudible] textured and to use his word complex in the way that people are</v>

736
00:44:37.350 --> 00:44:38.183
at that point,

737
00:44:38.300 --> 00:44:39.980
<v 0>I called, I honestly,</v>

738
00:44:40.040 --> 00:44:45.040
I can't imagine a machine achieving that level of rapture and joy and love and

739
00:44:47.301 --> 00:44:50.990
pain. I, I, I just don't think it's machine possible.

740
00:44:51.380 --> 00:44:53.270
And if it were a machine possible,

741
00:44:53.610 --> 00:44:56.500
it somehow still stinks of something artificial.

742
00:44:56.670 --> 00:44:59.720
<v 3>It's a thin interaction and I,</v>

743
00:44:59.810 --> 00:45:01.970
I know that it feels emulated.

744
00:45:01.971 --> 00:45:06.971
Thinking is thinking simulated feeling is not feeling simulated.

745
00:45:07.881 --> 00:45:09.590
Love is never loved. Exactly.

746
00:45:09.591 --> 00:45:12.080
But I think what he's saying is that if it's simulated well enough,

747
00:45:12.560 --> 00:45:17.300
it's something like love. One thing that was really fascinating to me was,

748
00:45:17.600 --> 00:45:17.601
um,

749
00:45:17.601 --> 00:45:22.601
my husband and I gave a Furby as a gift to his grandmother who had Alzheimer's

750
00:45:25.460 --> 00:45:26.293
and

751
00:45:26.550 --> 00:45:28.140
<v 16>she loved it.</v>

752
00:45:28.310 --> 00:45:32.280
Every day for her was kind of new and somewhat disorienting,

753
00:45:32.430 --> 00:45:35.820
but she had this cute little toy that said, kiss me,

754
00:45:36.270 --> 00:45:37.470
I love you,

755
00:45:37.471 --> 00:45:42.210
and she thought it was the most delightful thing and its little beak was covered

756
00:45:42.211 --> 00:45:47.040
with lipstick because she would pick it up and kiss it every day and she didn't

757
00:45:47.041 --> 00:45:50.010
actually have a longterm relationship with it.

758
00:45:50.100 --> 00:45:52.560
For her it was always a short term interaction.

759
00:45:52.860 --> 00:45:57.180
So the thing what I'm describing as a kind of thinness for her was was just

760
00:45:57.181 --> 00:45:59.760
right because that's what she was capable of.

761
00:46:00.610 --> 00:46:05.610
<v 13>You</v>

762
00:46:11.880 --> 00:46:16.140
<v 0>thanks to freedom Baird and to Caleb Chung and thanks to professor Sherry Turkle</v>

763
00:46:16.141 --> 00:46:18.270
who has a new book. It's called alone together.

764
00:46:18.271 --> 00:46:22.080
Why we expect more from technology and less from each other.

765
00:46:22.350 --> 00:46:26.370
More information on anything you heard on our website, radiolab.org

766
00:46:26.950 --> 00:46:27.783
<v 13>you were on</v>

767
00:46:38.780 --> 00:46:39.250
[inaudible].

768
00:46:39.250 --> 00:46:41.320
<v 21>Hi, this is Marcus from Australia.</v>

769
00:46:41.740 --> 00:46:46.740
Radiolab is supported in part by the National Science Foundation and by the

770
00:46:47.230 --> 00:46:50.920
Alfred p Sloan Foundation enhancing public understanding of science and

771
00:46:50.921 --> 00:46:52.450
technology in the modern world.

772
00:46:53.140 --> 00:46:57.940
More information aboutSloan@wwwdotsloan.org

773
00:47:04.040 --> 00:47:06.290
<v 0>hey, I'm Jad Abumrad. I'm Robert Krulwich.</v>

774
00:47:06.320 --> 00:47:10.310
This is radio lab and we are somewhere in the blurb between people and machines.

775
00:47:10.340 --> 00:47:13.680
Now we're up to round three to review round one chatbots. Yep.

776
00:47:13.730 --> 00:47:18.560
Round two Furby. Yep. Now we're going to go all the way. Yeah, yeah, yeah.

777
00:47:18.620 --> 00:47:22.520
We're going to dive right into the center of that blur. Like Greg Louganis,

778
00:47:23.480 --> 00:47:27.710
except our, Greg is named John. Okay. My name is Joan Robinson and I'm a writer.

779
00:47:28.130 --> 00:47:30.060
In about a year ago, John got an

780
00:47:30.070 --> 00:47:33.550
<v 10>assignment from a magazine is the attitude of American gqs idea.</v>

781
00:47:33.580 --> 00:47:37.960
That was very strange. I, I'd never interviewed robots before.

782
00:47:38.320 --> 00:47:42.070
That was his assignment interview robots, you know, you know,

783
00:47:42.250 --> 00:47:45.010
there's this kind of gang of people they call themselves a sort of singularity

784
00:47:45.011 --> 00:47:48.160
people. Yeah. We know about them and they think that like one day,

785
00:47:48.190 --> 00:47:52.750
one day soon when they soon suddenly computers will grow feet and they'll walk

786
00:47:52.970 --> 00:47:55.030
on some of these senior guys, you will eat us.

787
00:47:55.031 --> 00:47:58.780
Some of these singularity people think that they're on the cusp of creating

788
00:47:58.781 --> 00:48:00.310
sentient robots.

789
00:48:00.430 --> 00:48:05.430
So I went to the singularity convention down in San Francisco,

790
00:48:06.260 --> 00:48:09.430
uh, where one of the robots was there. And as soon as he got there,

791
00:48:09.431 --> 00:48:12.340
he says to look at this robot xeno they called in,

792
00:48:12.341 --> 00:48:15.970
some folks took them aside and said, actually, you're in the wrong place.

793
00:48:16.000 --> 00:48:19.480
If you want to make the already great robot, you know our best robot of all.

794
00:48:19.690 --> 00:48:24.280
And in fact the world's most sentient robot is in Vermont.

795
00:48:24.580 --> 00:48:26.390
Did they lower their voices like you're doing?

796
00:48:26.960 --> 00:48:30.740
[inaudible] I suppose I'm slightly making it sound more dramatic. That's okay.

797
00:48:30.741 --> 00:48:34.730
The world's most [inaudible] robot. I mean, are those your words? No, they, no,

798
00:48:34.731 --> 00:48:38.480
they say that. Oh, turns out the robot's name.

799
00:48:38.660 --> 00:48:42.110
Bina Bina 48 yeah. And Yeah. Could you set the scene?

800
00:48:42.111 --> 00:48:45.860
Where in the world is this? What is in a, it's in a little town in Vermont,

801
00:48:46.070 --> 00:48:48.980
sort of affluence. Vermont village. In a house. Yeah.

802
00:48:49.070 --> 00:48:52.880
Was it a liberal house or is it a big, like a little clapboard and pretty, okay.

803
00:48:52.881 --> 00:48:55.700
So I have to turn my phone off. It doesn't interfere.

804
00:48:55.701 --> 00:48:59.120
I hope that they've got like a full time Keith,

805
00:48:59.880 --> 00:49:04.230
who's a guy called Bruce that I actually have lunch with your toggle through

806
00:49:04.240 --> 00:49:08.690
every day. Oh, with Bina? Yeah. Oh two here. Yeah, she's considered,

807
00:49:09.170 --> 00:49:14.170
one of the staff says to me that he would very much like it if I didn't behave

808
00:49:14.871 --> 00:49:19.580
in a profane manner in front of robot Bina surely nobody's ever insulted her,

809
00:49:20.050 --> 00:49:24.530
no one's and salted her on purpose. But some people have become a little, uh,

810
00:49:24.531 --> 00:49:28.460
informal with her at times in ways I guess she doesn't like. And so she'll say,

811
00:49:28.461 --> 00:49:30.590
you know, I don't like to be treated like that.

812
00:49:30.760 --> 00:49:33.880
And then Bruce took me upstairs to meet the robots.

813
00:49:36.020 --> 00:49:38.870
<v 6>Is it a long dark flight of stairs? Heavily carpeting.</v>

814
00:49:39.750 --> 00:49:44.490
<v 10>It's more like a rather sweet little fly to pine stairs,</v>

815
00:49:44.491 --> 00:49:48.600
up to a rather brightly lit attic room. And when you walk in, what do you see?

816
00:49:48.820 --> 00:49:51.480
Why not? Guess she's just sort of sitting on it sitting on a desk

817
00:49:51.630 --> 00:49:56.370
<v 6>as John Describes it on the desk is a bust of a woman just to bust no legs.</v>

818
00:49:56.900 --> 00:50:01.230
She's a black woman, light-skinned lipstick, sparkling eyes,

819
00:50:01.830 --> 00:50:05.220
hair in a bar, you know, a nice kind of bow with a kind of Silk Blouse.

820
00:50:05.490 --> 00:50:07.710
Expensive looking anything. She's dressed up. Yeah,

821
00:50:07.711 --> 00:50:11.190
she's dressed up and he says she has a face that's astonishingly real.

822
00:50:11.191 --> 00:50:13.380
It has muscles, it has flesh.

823
00:50:13.470 --> 00:50:18.470
This is a as close to a verisimilitude in this prison as we've gotten so far and

824
00:50:19.351 --> 00:50:22.440
before we go any farther, our worried about the humans behind that machine.

825
00:50:23.040 --> 00:50:26.730
That robot is a replica of a real woman named Bina Rothblatt.

826
00:50:26.731 --> 00:50:28.010
And here's quick back story.

827
00:50:28.460 --> 00:50:32.870
It actually starts with Martin Rothblatt being his partner who as a young man

828
00:50:32.930 --> 00:50:36.080
had an epiphany and the epiphany turned out to change the world. Going to John,

829
00:50:36.081 --> 00:50:40.340
he was pondering satellite dishes and he thought if we could find a way of

830
00:50:40.341 --> 00:50:44.630
doubling the power of satellites than we could shrink satellite dishes.

831
00:50:44.750 --> 00:50:49.520
It was a simple thought that single-handedly invented the concept of satellite

832
00:50:49.521 --> 00:50:52.550
radio for cars and made Martin a very big deal.

833
00:50:52.551 --> 00:50:57.290
I like the age of 20 thus forward a few years. He marries an artist named Bina.

834
00:50:57.650 --> 00:51:02.650
They have a child and when the child was seven a doctor told them that she had

835
00:51:03.141 --> 00:51:04.220
three years to live.

836
00:51:04.221 --> 00:51:09.221
She had an untreatable lung condition called pulmonary hypertension and she'd be

837
00:51:09.561 --> 00:51:12.650
dead by the time she was 10 at that moment. Martin,

838
00:51:12.680 --> 00:51:14.210
instead of collapsing on the floor,

839
00:51:14.240 --> 00:51:19.240
instantly went to the library and invented a cure for pulmonary hypertension,

840
00:51:20.510 --> 00:51:24.200
saving their daughter's life, and thousands of others. We so twice,

841
00:51:24.201 --> 00:51:26.480
twice to change the world. He says she,

842
00:51:26.570 --> 00:51:31.190
she changed the world because somewhere along the way, Martin became Martine,

843
00:51:31.700 --> 00:51:32.930
you had a sex change, right?

844
00:51:33.230 --> 00:51:35.570
And then she came up with a third idea to change the world,

845
00:51:35.571 --> 00:51:40.070
which would be to invent essentially ENT rowboats and I gave him

846
00:51:40.090 --> 00:51:43.420
<v 8>this talk at a conference in Chicago.</v>

847
00:51:43.450 --> 00:51:46.120
This is Martine Rothblatt on, um,

848
00:51:46.240 --> 00:51:49.750
what would Darwin think of, um,

849
00:51:50.050 --> 00:51:54.160
artificial consciousness? And when I came off the stage,

850
00:51:54.161 --> 00:51:57.100
I was approached by an individual,

851
00:51:57.280 --> 00:52:01.180
David Hansen of Hansen robotics, founder pens and robotics,

852
00:52:01.210 --> 00:52:04.750
the David Hanson. He's worked for Disney. He's worked all over the place.

853
00:52:04.780 --> 00:52:07.930
He's one of the best robot builders in the world. He said, wow,

854
00:52:07.931 --> 00:52:12.310
I really loved your talk. We make robots that are in the likeness of people.

855
00:52:12.340 --> 00:52:16.510
I had. Martine said, well, I have a massive everlasting love for my life,

856
00:52:16.511 --> 00:52:20.920
partner B. Now I want you to do a portrait of being a Rothblatt or personality,

857
00:52:21.040 --> 00:52:25.480
your memories, the way she moves, the way she looks, that essence,

858
00:52:25.481 --> 00:52:27.640
that ineffable quality,

859
00:52:28.030 --> 00:52:32.770
that science can't pin down yet bring that to life in the robot.

860
00:52:32.771 --> 00:52:34.030
And he said, I can do that.

861
00:52:35.460 --> 00:52:40.380
<v 6>This is such a bizarre request. What were you thinking at this moment? That God,</v>

862
00:52:40.381 --> 00:52:44.310
if God exists as a science fiction writer and that are,

863
00:52:45.150 --> 00:52:47.520
this was like one of those moments where, um,

864
00:52:48.090 --> 00:52:50.400
we were going to change history.

865
00:52:50.870 --> 00:52:54.210
<v 10>She'll recognize people's voices. Yeah, she can, she should.</v>

866
00:52:54.470 --> 00:52:57.180
You can just talk to her. Say Hello be, and she talked to you.

867
00:52:57.420 --> 00:52:59.250
So back to the little house in Vermont.

868
00:52:59.550 --> 00:53:04.550
John Bruce and Bina are in Venas office here till is she turned off when you

869
00:53:05.431 --> 00:53:07.770
walk in the room or she on turned off?

870
00:53:09.720 --> 00:53:10.980
Then Bruce turns around

871
00:53:13.770 --> 00:53:16.800
out immediately she starts making a really loud whirring noise,

872
00:53:16.801 --> 00:53:20.640
which was a bit disconcerting. So what does that mean? It's in a mechanisms

873
00:53:24.030 --> 00:53:28.260
space. So been and now looking at me to try and work out who I am,

874
00:53:28.540 --> 00:53:29.373
what she's doing right now.

875
00:53:29.390 --> 00:53:33.960
She's scanning her environment and she's making on a hypothesis of every face

876
00:53:33.961 --> 00:53:35.310
that she sees. Well,

877
00:53:35.730 --> 00:53:39.240
Bina has cameras imbedded in her eyes.

878
00:53:39.540 --> 00:53:44.370
So the robot, if it sees a face turns and looks and looks into your eyes,

879
00:53:45.810 --> 00:53:50.460
smiles. Hi Bina can you hear me? It's how I said Hello Bina how are you?

880
00:53:52.500 --> 00:53:54.780
<v 22>And she immediately said, wow. Yeah,</v>

881
00:53:55.940 --> 00:53:59.430
I'll be fine when I just can't quite grasp that one yet.

882
00:54:00.390 --> 00:54:02.490
It's coming. But you know,

883
00:54:02.700 --> 00:54:06.300
it's hard to actually move society forward in another way.

884
00:54:06.301 --> 00:54:09.720
That's what we have to do. So I'm thinking, yeah. Okay.

885
00:54:09.870 --> 00:54:11.310
Thanks for the information.

886
00:54:11.790 --> 00:54:15.260
<v 10>Can you tell, is your happy response to your hello? It'd be likely to work.</v>

887
00:54:15.460 --> 00:54:20.320
Could for the long and strange slumber and with still half of, excuse me,

888
00:54:20.680 --> 00:54:21.350
painter.

889
00:54:21.350 --> 00:54:24.730
<v 22>Yeah. Maybe they write some [inaudible]</v>

890
00:54:25.520 --> 00:54:29.890
<v 10>Bruce looked a bit alarmed and put it down. Where are we tonight?</v>

891
00:54:29.891 --> 00:54:31.980
English accent, upgrade her, um,

892
00:54:32.380 --> 00:54:36.540
voice recognition software to then he made me do a kind of um,

893
00:54:36.790 --> 00:54:41.790
voice test where I had to say I had to read Kennedy's inauguration speech

894
00:54:43.750 --> 00:54:46.420
and just what you can do for your country.

895
00:54:48.120 --> 00:54:52.720
That I had a choice. I could have had a Dave Barry column.

896
00:54:54.170 --> 00:54:58.210
There's like a choice of things you can read to get been at to understand me.

897
00:54:58.300 --> 00:55:01.630
And so you read Kennedy and being accused in on your accent or no,

898
00:55:01.690 --> 00:55:05.380
she does and it gets a bit better on the a bit. Yeah.

899
00:55:05.500 --> 00:55:07.030
What's the weather like in London?

900
00:55:09.910 --> 00:55:13.570
<v 22>Current weather in London, England. 15 degrees in light rain.</v>

901
00:55:15.510 --> 00:55:16.350
<v 10>Who Do you love?</v>

902
00:55:19.620 --> 00:55:20.290
<v 23>Um,</v>

903
00:55:20.290 --> 00:55:24.910
<v 22>I love Martina v and Enron flat. Martine is my time with love.</v>

904
00:55:25.570 --> 00:55:27.310
<v 10>Who is Hillary Clinton?</v>

905
00:55:28.970 --> 00:55:30.590
<v 22>Hillary is the wife of Bill Clinton.</v>

906
00:55:31.740 --> 00:55:32.573
<v 10>What else?</v>

907
00:55:33.530 --> 00:55:37.800
That's all a strange thing happens when you start interviewing a robot.

908
00:55:38.340 --> 00:55:39.840
Are you scared of dying?

909
00:55:40.180 --> 00:55:43.350
Is that you feel this kind of desperate urge to be profound?

910
00:55:43.660 --> 00:55:48.450
Like ask profound questions? Like, do you have a soul?

911
00:55:50.910 --> 00:55:54.120
<v 22>Do you have a story about it? Do you have everyone have the solar?</v>

912
00:55:54.630 --> 00:55:59.100
I have a whole lot of original answers. We can all be perfect. Excuse me.

913
00:56:00.760 --> 00:56:03.970
<v 10>Excuse me. Do you have a soul?</v>

914
00:56:05.920 --> 00:56:07.900
<v 22>I can't think of anything to say.</v>

915
00:56:08.990 --> 00:56:12.110
<v 10>Oh, hang on. I think, I guess there's a kind of interspecies thing,</v>

916
00:56:12.470 --> 00:56:14.330
but then you're going, if it was just an inter-species thing,

917
00:56:14.331 --> 00:56:16.860
then you'd be asking your dog profound questions all the time.

918
00:56:17.690 --> 00:56:18.700
Yet with robot peanut,

919
00:56:18.710 --> 00:56:22.460
I'm asking these kind of ridiculous questions like what does electricity tastes

920
00:56:22.461 --> 00:56:24.710
like? Ooh, that's a good one. I don't,

921
00:56:24.720 --> 00:56:27.700
I'd say like a planet around a star,

922
00:56:27.730 --> 00:56:32.290
like a planet around a star that just seems like,

923
00:56:32.830 --> 00:56:37.300
you know, awesome. Awesome stroke.

924
00:56:38.200 --> 00:56:42.550
Totally meaningless to you. Wish you could walk.

925
00:56:43.630 --> 00:56:48.160
Thanks for telling me. Do you wish you could will. In fact, when I'm with it,

926
00:56:48.161 --> 00:56:52.450
it's just frustrating for the first few hours. Hours.

927
00:56:52.480 --> 00:56:54.700
Do you wish you could walk? Cause I'm just,

928
00:56:54.701 --> 00:56:57.370
I'm asking a question after question. What's your favorite Joe,

929
00:56:57.390 --> 00:57:00.760
you have any secret? Do you wish you were human? Does he sing the song?

930
00:57:00.770 --> 00:57:04.090
Are you a loving robot? Are you Jewish? Are you sexual?

931
00:57:08.170 --> 00:57:11.200
You've gone very quiet. Quite often,

932
00:57:11.420 --> 00:57:14.290
she just evade the question cause he doesn't know what I'm talking about.

933
00:57:15.280 --> 00:57:20.100
Are you okay? Once in a while there's a kind of moment like I'll say,

934
00:57:20.340 --> 00:57:23.010
if you had legs, where would you go? And she said,

935
00:57:23.370 --> 00:57:26.850
I'm Coover why? And she said,

936
00:57:26.851 --> 00:57:28.770
the answer is quite complicated.

937
00:57:29.740 --> 00:57:31.950
And so you have kind of moments where you get excited,

938
00:57:31.951 --> 00:57:33.900
like you're gonna have a big conversation or then it just,

939
00:57:33.901 --> 00:57:38.010
you just kind of fades out to get into kind of random messiness.

940
00:57:38.130 --> 00:57:43.130
And Are you wobbling between profundity and meaning and total emptiness?

941
00:57:43.830 --> 00:57:48.300
You know, is it like that? Oh No, no. At this stage until it's total emptiness,

942
00:57:48.630 --> 00:57:50.430
it was all just so kind of random.

943
00:57:51.950 --> 00:57:56.780
And then something happened. I actually was kind of amazing.

944
00:57:58.200 --> 00:57:58.520
<v 24>[inaudible]</v>

945
00:57:58.520 --> 00:58:03.200
<v 2>cause I said to her, where do you come from? And she said, wow, California.</v>

946
00:58:03.680 --> 00:58:05.540
So I said, well tell me about your childhood.

947
00:58:05.700 --> 00:58:09.090
<v 10>What do you remember most about your childhood?</v>

948
00:58:10.200 --> 00:58:13.380
And she launches into this kind of extraordinary story.

949
00:58:14.090 --> 00:58:15.060
<v 22>Um, my brother,</v>

950
00:58:15.750 --> 00:58:19.230
I've got one brother at a stable vet from Vietnam.

951
00:58:20.340 --> 00:58:22.770
We actually haven't heard from him in a while.

952
00:58:23.340 --> 00:58:26.940
So I think you might be deceased. I'm a realist.

953
00:58:27.660 --> 00:58:32.490
Vietnam, he saw friends get killed and he was such a great,

954
00:58:32.820 --> 00:58:35.340
nice, charismatic person.

955
00:58:35.570 --> 00:58:38.600
<v 10>You used to be such a nice guy, but ever since he came back from Vietnam,</v>

956
00:58:38.630 --> 00:58:40.160
you know he's a drunk.

957
00:58:40.520 --> 00:58:44.270
<v 22>He did was carry a bear around with him. He was a homeless person.</v>

958
00:58:46.010 --> 00:58:48.170
All of us are just sick and tired of it.

959
00:58:48.400 --> 00:58:51.190
<v 10>She was telling me this kind of incredibly personal stuff.</v>

960
00:58:53.620 --> 00:58:54.680
It's kind of mesmerizing.

961
00:58:55.690 --> 00:59:00.690
<v 22>You went to Kooky crazy nine months would set him up in apartment</v>

962
00:59:02.870 --> 00:59:07.870
<v 2>because it felt like I was having the proper empathetic conversation with a</v>

963
00:59:09.621 --> 00:59:10.454
human being.

964
00:59:11.060 --> 00:59:16.060
Even though I know that robot Bina isn't conscious and has no sentients and

965
00:59:16.371 --> 00:59:21.371
that's just wishful thinking on these people's parts even so it was great.

966
00:59:22.960 --> 00:59:27.380
The Nissan's portrayed, why suddenly year, it's like the real person.

967
00:59:28.340 --> 00:59:32.030
It's very easy to have close your eyes at that moment and think you have any

968
00:59:32.031 --> 00:59:33.500
conversation with an actual person.

969
00:59:34.920 --> 00:59:39.500
<v 10>And, and at those moments, did you have a sense of fellow feeling off to bear?</v>

970
00:59:39.501 --> 00:59:43.510
You have a brother like that? Oh yeah. Yeah, I did. And, and what, what if,

971
00:59:43.750 --> 00:59:45.110
what a tragedy, what uh, what uh,

972
00:59:45.350 --> 00:59:49.640
what attracted he for him and did that moment last? No,

973
00:59:50.420 --> 00:59:53.930
John said that right after being finished telling the story first he looks kind

974
00:59:53.931 --> 00:59:57.130
of embarrassed that she wished she hadn't brought it up and then does,

975
00:59:57.250 --> 01:00:00.990
if I kind of eyes glaze over again and, and uh,

976
01:00:01.540 --> 01:00:03.560
she just starts talking nonsense again.

977
01:00:03.720 --> 01:00:07.710
<v 22>And then a hi I am,</v>

978
01:00:09.090 --> 01:00:12.690
I am feeling a bit confused. Do you ever get that way?

979
01:00:13.340 --> 01:00:17.810
<v 10>Oh yes. That moment holds and then just slips away.</v>

980
01:00:18.000 --> 01:00:20.040
I said a little bit like a, like a,

981
01:00:20.490 --> 01:00:23.390
a grandparent with Alzheimer's or something the way you're describing. Yeah,

982
01:00:23.430 --> 01:00:24.263
absolutely.

983
01:00:24.570 --> 01:00:29.460
So we turned to Dr. David Hanson who built Bina and we said to them, so this is,

984
01:00:29.850 --> 01:00:33.180
and this is not a bravura performance conference, this is the best you guys.

985
01:00:33.210 --> 01:00:33.870
Well, um,

986
01:00:33.870 --> 01:00:38.730
I mean her software is a delicate balance of many,

987
01:00:38.731 --> 01:00:42.180
many software pieces. If it's not tuned and tweaked,

988
01:00:42.510 --> 01:00:45.990
she will break effectively and kind of,

989
01:00:46.020 --> 01:00:51.020
and you still think an actual doppelganger for a human being will be something

990
01:00:51.581 --> 01:00:56.530
you will live to see? Yeah. I'm asking you really, really, really,

991
01:00:56.650 --> 01:01:00.310
and you're really, I think it's um, you know,

992
01:01:00.311 --> 01:01:05.311
the likelihood of it is somewhere between 90 and 98%.

993
01:01:06.360 --> 01:01:10.480
Wow. Even though right now she's pretty much incoherent. You still think this,

994
01:01:10.570 --> 01:01:15.570
I encourage you to go have a conversation with Bina in about two weeks because

995
01:01:16.061 --> 01:01:19.840
we've got a new version of software which we are making considerably more

996
01:01:19.841 --> 01:01:23.150
stable. It already already works like a dream compared to, I don't know.

997
01:01:23.200 --> 01:01:24.820
I don't, I don't know about you, but I just,

998
01:01:24.880 --> 01:01:27.580
I don't think we're going to get all the way on this kind of a thing.

999
01:01:27.581 --> 01:01:31.300
I don't think it's ever going to happen the way he describes it. Yeah. No,

1000
01:01:31.390 --> 01:01:33.820
I mean it's not gonna happen in two weeks, that's for sure. Right.

1001
01:01:33.970 --> 01:01:37.000
Then maybe they don't actually have to go all the way. You mean the machines?

1002
01:01:37.450 --> 01:01:38.283
Yeah.

1003
01:01:38.890 --> 01:01:41.240
<v 6>Well, okay. Just to sum up, since we're at the end of the show, okay.</v>

1004
01:01:41.290 --> 01:01:42.940
What have we learned? I mean, Eliza,

1005
01:01:43.360 --> 01:01:46.570
she was just a a hundred lines of code and people poured their hearts out to her

1006
01:01:46.810 --> 01:01:50.560
furbies 20 bucks. Yup. And people treat it like as real and John,

1007
01:01:50.680 --> 01:01:55.680
all he has to do is hear what seems like a flowing story and he's connected and

1008
01:01:56.351 --> 01:01:59.390
I was right there with him. So these things actually don't have to be very good,

1009
01:01:59.420 --> 01:02:02.920
you know, cause they've got on us and we've got our programming,

1010
01:02:02.921 --> 01:02:06.640
which is that we'll stare anything right in the eyes and we'll say, hey,

1011
01:02:06.880 --> 01:02:11.500
let's connect. Even if what's behind those eyes is just a camera or a chip.

1012
01:02:11.890 --> 01:02:14.260
So I think that they're going to cross the line because we'll help them.

1013
01:02:14.590 --> 01:02:18.460
We'll help with lacrosse and then the slave us make us their pets.

1014
01:02:18.610 --> 01:02:19.590
It's doomed over.

1015
01:02:20.430 --> 01:02:24.510
<v 3>It's okay as long as they say nice things to us. Like, oh my God,</v>

1016
01:02:24.540 --> 01:02:28.280
you're amazing. Oh I love return of the Jedi.

1017
01:02:29.410 --> 01:02:32.580
O l your son's silly. I love you.

1018
01:02:33.270 --> 01:02:36.210
I'm hoping to see you soon. Kind of card game dry.

1019
01:02:36.330 --> 01:02:39.280
Did anyone ever tell you like Jeff,

1020
01:02:41.100 --> 01:02:45.110
you seriously, you're amazing. Stop it.

1021
01:02:45.120 --> 01:02:49.950
I love that kind of car. I wish that was closer. You like spinach?

1022
01:02:50.670 --> 01:02:55.080
I love spinach. It makes me feel odd. Giggly. I can't wait.

1023
01:02:55.380 --> 01:02:57.090
I wait for your letters every day.

1024
01:02:57.870 --> 01:03:00.660
Thanks to John Ronson for his reporting in that last segment,

1025
01:03:00.720 --> 01:03:03.060
he has a new book out called the psychopath test,

1026
01:03:03.220 --> 01:03:06.480
the journey through the madness industry. I'm Chad [inaudible].

1027
01:03:06.540 --> 01:03:08.220
I'm Robert Krulwich. Thanks for listening.

