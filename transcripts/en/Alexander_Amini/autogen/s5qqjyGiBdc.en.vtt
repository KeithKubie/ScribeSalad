WEBVTT
Kind: captions
Language: en

00:00:02.010 --> 00:00:07.040
 
all right good morning everyone my name

00:00:07.040 --> 00:00:07.050
all right good morning everyone my name
 

00:00:07.050 --> 00:00:09.140
all right good morning everyone my name
is Lex Frieden I'm a research scientist

00:00:09.140 --> 00:00:09.150
is Lex Frieden I'm a research scientist
 

00:00:09.150 --> 00:00:11.420
is Lex Frieden I'm a research scientist
here at MIT I build autonomous vehicles

00:00:11.420 --> 00:00:11.430
here at MIT I build autonomous vehicles
 

00:00:11.430 --> 00:00:12.830
here at MIT I build autonomous vehicles
and perception systems for these

00:00:12.830 --> 00:00:12.840
and perception systems for these
 

00:00:12.840 --> 00:00:15.410
and perception systems for these
vehicles and today I'd like to talk

00:00:15.410 --> 00:00:15.420
vehicles and today I'd like to talk
 

00:00:15.420 --> 00:00:18.020
vehicles and today I'd like to talk
first of all it's great to be part of

00:00:18.020 --> 00:00:18.030
first of all it's great to be part of
 

00:00:18.030 --> 00:00:20.029
first of all it's great to be part of
this amazing course and enter deep

00:00:20.029 --> 00:00:20.039
this amazing course and enter deep
 

00:00:20.039 --> 00:00:22.880
this amazing course and enter deep
learning it's it's a wonderful course

00:00:22.880 --> 00:00:22.890
learning it's it's a wonderful course
 

00:00:22.890 --> 00:00:27.439
learning it's it's a wonderful course
covering pretty quickly but deeply some

00:00:27.439 --> 00:00:27.449
covering pretty quickly but deeply some
 

00:00:27.449 --> 00:00:28.700
covering pretty quickly but deeply some
of the fundamental aspects of deep

00:00:28.700 --> 00:00:28.710
of the fundamental aspects of deep
 

00:00:28.710 --> 00:00:29.810
of the fundamental aspects of deep
learning this is awesome

00:00:29.810 --> 00:00:29.820
learning this is awesome
 

00:00:29.820 --> 00:00:32.479
learning this is awesome
so the topic that I'm perhaps most

00:00:32.479 --> 00:00:32.489
so the topic that I'm perhaps most
 

00:00:32.489 --> 00:00:34.219
so the topic that I'm perhaps most
passionate about from the perspective of

00:00:34.219 --> 00:00:34.229
passionate about from the perspective of
 

00:00:34.229 --> 00:00:36.919
passionate about from the perspective of
just being a researcher in artificial

00:00:36.919 --> 00:00:36.929
just being a researcher in artificial
 

00:00:36.929 --> 00:00:39.469
just being a researcher in artificial
intelligence is deep reinforcement

00:00:39.469 --> 00:00:39.479
intelligence is deep reinforcement
 

00:00:39.479 --> 00:00:42.739
intelligence is deep reinforcement
learning which is a set of ideas and

00:00:42.739 --> 00:00:42.749
learning which is a set of ideas and
 

00:00:42.749 --> 00:00:48.500
learning which is a set of ideas and
methods that teach agents to act in the

00:00:48.500 --> 00:00:48.510
methods that teach agents to act in the
 

00:00:48.510 --> 00:00:53.239
methods that teach agents to act in the
real world if you think about what

00:00:53.239 --> 00:00:53.249
real world if you think about what
 

00:00:53.249 --> 00:00:56.719
real world if you think about what
machine learning is it allows us to make

00:00:56.719 --> 00:00:56.729
machine learning is it allows us to make
 

00:00:56.729 --> 00:00:59.020
machine learning is it allows us to make
sense of the world but to truly have

00:00:59.020 --> 00:00:59.030
sense of the world but to truly have
 

00:00:59.030 --> 00:01:02.419
sense of the world but to truly have
impact in the physical world we need to

00:01:02.419 --> 00:01:02.429
impact in the physical world we need to
 

00:01:02.429 --> 00:01:05.240
impact in the physical world we need to
also act in that world so you bring the

00:01:05.240 --> 00:01:05.250
also act in that world so you bring the
 

00:01:05.250 --> 00:01:07.040
also act in that world so you bring the
understanding that you extract from the

00:01:07.040 --> 00:01:07.050
understanding that you extract from the
 

00:01:07.050 --> 00:01:09.020
understanding that you extract from the
world through the perceptual systems and

00:01:09.020 --> 00:01:09.030
world through the perceptual systems and
 

00:01:09.030 --> 00:01:11.750
world through the perceptual systems and
actually decide to make actions so you

00:01:11.750 --> 00:01:11.760
actually decide to make actions so you
 

00:01:11.760 --> 00:01:13.700
actually decide to make actions so you
can think of intelligent systems as this

00:01:13.700 --> 00:01:13.710
can think of intelligent systems as this
 

00:01:13.710 --> 00:01:15.470
can think of intelligent systems as this
kind of stack from the top to the bottom

00:01:15.470 --> 00:01:15.480
kind of stack from the top to the bottom
 

00:01:15.480 --> 00:01:17.660
kind of stack from the top to the bottom
so in the environment at the top the

00:01:17.660 --> 00:01:17.670
so in the environment at the top the
 

00:01:17.670 --> 00:01:19.370
so in the environment at the top the
world that the agent operates in and at

00:01:19.370 --> 00:01:19.380
world that the agent operates in and at
 

00:01:19.380 --> 00:01:20.990
world that the agent operates in and at
the bottom is the effectors that

00:01:20.990 --> 00:01:21.000
the bottom is the effectors that
 

00:01:21.000 --> 00:01:24.080
the bottom is the effectors that
actually make changes to the world by

00:01:24.080 --> 00:01:24.090
actually make changes to the world by
 

00:01:24.090 --> 00:01:27.440
actually make changes to the world by
moving the robot moving the agent in a

00:01:27.440 --> 00:01:27.450
moving the robot moving the agent in a
 

00:01:27.450 --> 00:01:29.450
moving the robot moving the agent in a
way that changes the world that acts in

00:01:29.450 --> 00:01:29.460
way that changes the world that acts in
 

00:01:29.460 --> 00:01:31.580
way that changes the world that acts in
the world and so from the environment it

00:01:31.580 --> 00:01:31.590
the world and so from the environment it
 

00:01:31.590 --> 00:01:34.970
the world and so from the environment it
goes to the sensors sensing the raw

00:01:34.970 --> 00:01:34.980
goes to the sensors sensing the raw
 

00:01:34.980 --> 00:01:37.250
goes to the sensors sensing the raw
sensory data extracting the features

00:01:37.250 --> 00:01:37.260
sensory data extracting the features
 

00:01:37.260 --> 00:01:38.870
sensory data extracting the features
making sense of the features

00:01:38.870 --> 00:01:38.880
making sense of the features
 

00:01:38.880 --> 00:01:41.030
making sense of the features
understanding forming representations

00:01:41.030 --> 00:01:41.040
understanding forming representations
 

00:01:41.040 --> 00:01:42.650
understanding forming representations
higher and higher order representations

00:01:42.650 --> 00:01:42.660
higher and higher order representations
 

00:01:42.660 --> 00:01:44.660
higher and higher order representations
from those representations we gain

00:01:44.660 --> 00:01:44.670
from those representations we gain
 

00:01:44.670 --> 00:01:46.450
from those representations we gain
knowledge that's useful actionable

00:01:46.450 --> 00:01:46.460
knowledge that's useful actionable
 

00:01:46.460 --> 00:01:49.670
knowledge that's useful actionable
finally we reason the thing that we hold

00:01:49.670 --> 00:01:49.680
finally we reason the thing that we hold
 

00:01:49.680 --> 00:01:53.090
finally we reason the thing that we hold
so dear as human beings the reasoning

00:01:53.090 --> 00:01:53.100
so dear as human beings the reasoning
 

00:01:53.100 --> 00:01:55.430
so dear as human beings the reasoning
that builds and aggregates knowledge

00:01:55.430 --> 00:01:55.440
that builds and aggregates knowledge
 

00:01:55.440 --> 00:01:57.860
that builds and aggregates knowledge
basis and using that reasoning form

00:01:57.860 --> 00:01:57.870
basis and using that reasoning form
 

00:01:57.870 --> 00:02:00.020
basis and using that reasoning form
short-term and long-term plans that

00:02:00.020 --> 00:02:00.030
short-term and long-term plans that
 

00:02:00.030 --> 00:02:02.420
short-term and long-term plans that
finally turn into action and act in that

00:02:02.420 --> 00:02:02.430
finally turn into action and act in that
 

00:02:02.430 --> 00:02:04.490
finally turn into action and act in that
world changing it so that's kind of the

00:02:04.490 --> 00:02:04.500
world changing it so that's kind of the
 

00:02:04.500 --> 00:02:06.620
world changing it so that's kind of the
stack of artificial intelligence and the

00:02:06.620 --> 00:02:06.630
stack of artificial intelligence and the
 

00:02:06.630 --> 00:02:09.619
stack of artificial intelligence and the
question is how much in the same way as

00:02:09.619 --> 00:02:09.629
question is how much in the same way as
 

00:02:09.629 --> 00:02:11.690
question is how much in the same way as
human beings we learn most of the stack

00:02:11.690 --> 00:02:11.700
human beings we learn most of the stack
 

00:02:11.700 --> 00:02:13.699
human beings we learn most of the stack
when we're born know very little and

00:02:13.699 --> 00:02:13.709
when we're born know very little and
 

00:02:13.709 --> 00:02:15.710
when we're born know very little and
would take in five sensory

00:02:15.710 --> 00:02:15.720
would take in five sensory
 

00:02:15.720 --> 00:02:18.860
would take in five sensory
sources of sensory data and make sense

00:02:18.860 --> 00:02:18.870
sources of sensory data and make sense
 

00:02:18.870 --> 00:02:20.540
sources of sensory data and make sense
of the world learn over time to act

00:02:20.540 --> 00:02:20.550
of the world learn over time to act
 

00:02:20.550 --> 00:02:22.970
of the world learn over time to act
successful in that world how much the

00:02:22.970 --> 00:02:22.980
successful in that world how much the
 

00:02:22.980 --> 00:02:24.800
successful in that world how much the
question is can we use deep learning

00:02:24.800 --> 00:02:24.810
question is can we use deep learning
 

00:02:24.810 --> 00:02:27.440
question is can we use deep learning
methods to learn parts of the stack the

00:02:27.440 --> 00:02:27.450
methods to learn parts of the stack the
 

00:02:27.450 --> 00:02:29.450
methods to learn parts of the stack the
modules of the stack or the entire stack

00:02:29.450 --> 00:02:29.460
modules of the stack or the entire stack
 

00:02:29.460 --> 00:02:32.480
modules of the stack or the entire stack
end to end let's go over them okay so

00:02:32.480 --> 00:02:32.490
end to end let's go over them okay so
 

00:02:32.490 --> 00:02:35.450
end to end let's go over them okay so
for robots that act in the world

00:02:35.450 --> 00:02:35.460
for robots that act in the world
 

00:02:35.460 --> 00:02:37.340
for robots that act in the world
autonomous vehicles humanoid robots

00:02:37.340 --> 00:02:37.350
autonomous vehicles humanoid robots
 

00:02:37.350 --> 00:02:40.220
autonomous vehicles humanoid robots
drones their sensors whether it's lidar

00:02:40.220 --> 00:02:40.230
drones their sensors whether it's lidar
 

00:02:40.230 --> 00:02:42.080
drones their sensors whether it's lidar
camera or microphone coming from the

00:02:42.080 --> 00:02:42.090
camera or microphone coming from the
 

00:02:42.090 --> 00:02:44.480
camera or microphone coming from the
audio networking for the communications

00:02:44.480 --> 00:02:44.490
audio networking for the communications
 

00:02:44.490 --> 00:02:46.490
audio networking for the communications
I am you getting the kinematics of the

00:02:46.490 --> 00:02:46.500
I am you getting the kinematics of the
 

00:02:46.500 --> 00:02:48.290
I am you getting the kinematics of the
different vehicles that's the raw data

00:02:48.290 --> 00:02:48.300
different vehicles that's the raw data
 

00:02:48.300 --> 00:02:52.730
different vehicles that's the raw data
coming in that's the eyes ears nose for

00:02:52.730 --> 00:02:52.740
coming in that's the eyes ears nose for
 

00:02:52.740 --> 00:02:55.130
coming in that's the eyes ears nose for
robots then the ones you have the

00:02:55.130 --> 00:02:55.140
robots then the ones you have the
 

00:02:55.140 --> 00:02:58.010
robots then the ones you have the
sensory data the task is to form

00:02:58.010 --> 00:02:58.020
sensory data the task is to form
 

00:02:58.020 --> 00:03:00.380
sensory data the task is to form
representations on that data you make

00:03:00.380 --> 00:03:00.390
representations on that data you make
 

00:03:00.390 --> 00:03:03.050
representations on that data you make
sense of this raw pixels or raw pieces

00:03:03.050 --> 00:03:03.060
sense of this raw pixels or raw pieces
 

00:03:03.060 --> 00:03:05.270
sense of this raw pixels or raw pieces
samples for whatever the sensor is and

00:03:05.270 --> 00:03:05.280
samples for whatever the sensor is and
 

00:03:05.280 --> 00:03:07.820
samples for whatever the sensor is and
you start to try to piece it together

00:03:07.820 --> 00:03:07.830
you start to try to piece it together
 

00:03:07.830 --> 00:03:11.150
you start to try to piece it together
into something that can be used to gain

00:03:11.150 --> 00:03:11.160
into something that can be used to gain
 

00:03:11.160 --> 00:03:13.550
into something that can be used to gain
understanding it's just numbers and

00:03:13.550 --> 00:03:13.560
understanding it's just numbers and
 

00:03:13.560 --> 00:03:15.380
understanding it's just numbers and
those numbers need to be converted into

00:03:15.380 --> 00:03:15.390
those numbers need to be converted into
 

00:03:15.390 --> 00:03:17.120
those numbers need to be converted into
something that can be reasoned with

00:03:17.120 --> 00:03:17.130
something that can be reasoned with
 

00:03:17.130 --> 00:03:19.310
something that can be reasoned with
that's forming representations and

00:03:19.310 --> 00:03:19.320
that's forming representations and
 

00:03:19.320 --> 00:03:21.710
that's forming representations and
that's where machine learning deep

00:03:21.710 --> 00:03:21.720
that's where machine learning deep
 

00:03:21.720 --> 00:03:23.780
that's where machine learning deep
learning steps in and takes this raw

00:03:23.780 --> 00:03:23.790
learning steps in and takes this raw
 

00:03:23.790 --> 00:03:26.509
learning steps in and takes this raw
sensory data the with some pre process

00:03:26.509 --> 00:03:26.519
sensory data the with some pre process
 

00:03:26.519 --> 00:03:28.570
sensory data the with some pre process
some process some initial processing and

00:03:28.570 --> 00:03:28.580
some process some initial processing and
 

00:03:28.580 --> 00:03:30.170
some process some initial processing and
forming higher and higher order

00:03:30.170 --> 00:03:30.180
forming higher and higher order
 

00:03:30.180 --> 00:03:32.449
forming higher and higher order
representations in that data that can be

00:03:32.449 --> 00:03:32.459
representations in that data that can be
 

00:03:32.459 --> 00:03:34.670
representations in that data that can be
reasoned with about in the computer

00:03:34.670 --> 00:03:34.680
reasoned with about in the computer
 

00:03:34.680 --> 00:03:38.390
reasoned with about in the computer
vision from edges to faces to entire

00:03:38.390 --> 00:03:38.400
vision from edges to faces to entire
 

00:03:38.400 --> 00:03:42.410
vision from edges to faces to entire
entities and finally the interpretation

00:03:42.410 --> 00:03:42.420
entities and finally the interpretation
 

00:03:42.420 --> 00:03:44.060
entities and finally the interpretation
the semantic interpretation of the scene

00:03:44.060 --> 00:03:44.070
the semantic interpretation of the scene
 

00:03:44.070 --> 00:03:47.390
the semantic interpretation of the scene
that's machine learning playing with the

00:03:47.390 --> 00:03:47.400
that's machine learning playing with the
 

00:03:47.400 --> 00:03:50.240
that's machine learning playing with the
representations and the reasoning part

00:03:50.240 --> 00:03:50.250
representations and the reasoning part
 

00:03:50.250 --> 00:03:53.630
representations and the reasoning part
one of the exciting fundamental open

00:03:53.630 --> 00:03:53.640
one of the exciting fundamental open
 

00:03:53.640 --> 00:03:56.570
one of the exciting fundamental open
challenges of machine learning is how

00:03:56.570 --> 00:03:56.580
challenges of machine learning is how
 

00:03:56.580 --> 00:03:58.610
challenges of machine learning is how
does this greater and greater

00:03:58.610 --> 00:03:58.620
does this greater and greater
 

00:03:58.620 --> 00:04:00.289
does this greater and greater
representation that can be formed

00:04:00.289 --> 00:04:00.299
representation that can be formed
 

00:04:00.299 --> 00:04:02.360
representation that can be formed
through deep neural networks can then

00:04:02.360 --> 00:04:02.370
through deep neural networks can then
 

00:04:02.370 --> 00:04:05.210
through deep neural networks can then
lead to reasoning to building knowledge

00:04:05.210 --> 00:04:05.220
lead to reasoning to building knowledge
 

00:04:05.220 --> 00:04:08.840
lead to reasoning to building knowledge
not just a memorization task which is

00:04:08.840 --> 00:04:08.850
not just a memorization task which is
 

00:04:08.850 --> 00:04:11.030
not just a memorization task which is
taking supervised learning memorizing

00:04:11.030 --> 00:04:11.040
taking supervised learning memorizing
 

00:04:11.040 --> 00:04:12.590
taking supervised learning memorizing
patterns and the input data based on

00:04:12.590 --> 00:04:12.600
patterns and the input data based on
 

00:04:12.600 --> 00:04:15.860
patterns and the input data based on
human annotations but also in extracting

00:04:15.860 --> 00:04:15.870
human annotations but also in extracting
 

00:04:15.870 --> 00:04:17.750
human annotations but also in extracting
those patterns but also then taking that

00:04:17.750 --> 00:04:17.760
those patterns but also then taking that
 

00:04:17.760 --> 00:04:19.640
those patterns but also then taking that
knowledge and building over time as we

00:04:19.640 --> 00:04:19.650
knowledge and building over time as we
 

00:04:19.650 --> 00:04:21.650
knowledge and building over time as we
humans do building into something that

00:04:21.650 --> 00:04:21.660
humans do building into something that
 

00:04:21.660 --> 00:04:23.090
humans do building into something that
could be called common sense into

00:04:23.090 --> 00:04:23.100
could be called common sense into
 

00:04:23.100 --> 00:04:25.640
could be called common sense into
knowledge basis in a very trivial task

00:04:25.640 --> 00:04:25.650
knowledge basis in a very trivial task
 

00:04:25.650 --> 00:04:29.360
knowledge basis in a very trivial task
this means aggregating fusing multiple

00:04:29.360 --> 00:04:29.370
this means aggregating fusing multiple
 

00:04:29.370 --> 00:04:33.320
this means aggregating fusing multiple
types of multiple types of extraction of

00:04:33.320 --> 00:04:33.330
types of multiple types of extraction of
 

00:04:33.330 --> 00:04:35.270
types of multiple types of extraction of
knowledge so from image recognition you

00:04:35.270 --> 00:04:35.280
knowledge so from image recognition you
 

00:04:35.280 --> 00:04:36.890
knowledge so from image recognition you
could think if it looks like a duck in

00:04:36.890 --> 00:04:36.900
could think if it looks like a duck in
 

00:04:36.900 --> 00:04:38.570
could think if it looks like a duck in
an image it sounds like a duck on the

00:04:38.570 --> 00:04:38.580
an image it sounds like a duck on the
 

00:04:38.580 --> 00:04:40.790
an image it sounds like a duck on the
audio and then you could do act activity

00:04:40.790 --> 00:04:40.800
audio and then you could do act activity
 

00:04:40.800 --> 00:04:43.550
audio and then you could do act activity
recognition with the video it swims like

00:04:43.550 --> 00:04:43.560
recognition with the video it swims like
 

00:04:43.560 --> 00:04:44.900
recognition with the video it swims like
a duck then it must be a duck just

00:04:44.900 --> 00:04:44.910
a duck then it must be a duck just
 

00:04:44.910 --> 00:04:47.990
a duck then it must be a duck just
aggregating this trivial different

00:04:47.990 --> 00:04:48.000
aggregating this trivial different
 

00:04:48.000 --> 00:04:50.450
aggregating this trivial different
sources of information that's reasoning

00:04:50.450 --> 00:04:50.460
sources of information that's reasoning
 

00:04:50.460 --> 00:04:53.120
sources of information that's reasoning
now I think from the human perspective

00:04:53.120 --> 00:04:53.130
now I think from the human perspective
 

00:04:53.130 --> 00:04:54.650
now I think from the human perspective
from the very biased human perspective

00:04:54.650 --> 00:04:54.660
from the very biased human perspective
 

00:04:54.660 --> 00:04:58.339
from the very biased human perspective
one of the illustrative aspects of

00:04:58.339 --> 00:04:58.349
one of the illustrative aspects of
 

00:04:58.349 --> 00:05:01.460
one of the illustrative aspects of
reasoning is theorem proving is the

00:05:01.460 --> 00:05:01.470
reasoning is theorem proving is the
 

00:05:01.470 --> 00:05:04.190
reasoning is theorem proving is the
moment of invention of creative genius

00:05:04.190 --> 00:05:04.200
moment of invention of creative genius
 

00:05:04.200 --> 00:05:07.040
moment of invention of creative genius
of this breakthrough ideas as we humans

00:05:07.040 --> 00:05:07.050
of this breakthrough ideas as we humans
 

00:05:07.050 --> 00:05:08.870
of this breakthrough ideas as we humans
come up with I mean really these aren't

00:05:08.870 --> 00:05:08.880
come up with I mean really these aren't
 

00:05:08.880 --> 00:05:10.969
come up with I mean really these aren't
new ideas whenever we come up with an

00:05:10.969 --> 00:05:10.979
new ideas whenever we come up with an
 

00:05:10.979 --> 00:05:13.070
new ideas whenever we come up with an
interesting idea they're not new we're

00:05:13.070 --> 00:05:13.080
interesting idea they're not new we're
 

00:05:13.080 --> 00:05:15.950
interesting idea they're not new we're
just collecting pieces of higher-order

00:05:15.950 --> 00:05:15.960
just collecting pieces of higher-order
 

00:05:15.960 --> 00:05:17.750
just collecting pieces of higher-order
representations of knowledge that we've

00:05:17.750 --> 00:05:17.760
representations of knowledge that we've
 

00:05:17.760 --> 00:05:20.060
representations of knowledge that we've
gained over time and then piecing them

00:05:20.060 --> 00:05:20.070
gained over time and then piecing them
 

00:05:20.070 --> 00:05:23.420
gained over time and then piecing them
together to form something some simple

00:05:23.420 --> 00:05:23.430
together to form something some simple
 

00:05:23.430 --> 00:05:26.450
together to form something some simple
beautiful distillation that is useful

00:05:26.450 --> 00:05:26.460
beautiful distillation that is useful
 

00:05:26.460 --> 00:05:29.450
beautiful distillation that is useful
for for the rest of the world and one of

00:05:29.450 --> 00:05:29.460
for for the rest of the world and one of
 

00:05:29.460 --> 00:05:33.230
for for the rest of the world and one of
my favorite sort of human stories and

00:05:33.230 --> 00:05:33.240
my favorite sort of human stories and
 

00:05:33.240 --> 00:05:36.320
my favorite sort of human stories and
discoveries in pure theorem proving is

00:05:36.320 --> 00:05:36.330
discoveries in pure theorem proving is
 

00:05:36.330 --> 00:05:37.670
discoveries in pure theorem proving is
for Moz Last Theorem

00:05:37.670 --> 00:05:37.680
for Moz Last Theorem
 

00:05:37.680 --> 00:05:41.060
for Moz Last Theorem
it stood for 350 years this is a trivial

00:05:41.060 --> 00:05:41.070
it stood for 350 years this is a trivial
 

00:05:41.070 --> 00:05:44.360
it stood for 350 years this is a trivial
thing to explain most six well eight

00:05:44.360 --> 00:05:44.370
thing to explain most six well eight
 

00:05:44.370 --> 00:05:46.730
thing to explain most six well eight
year olds can understand the definition

00:05:46.730 --> 00:05:46.740
year olds can understand the definition
 

00:05:46.740 --> 00:05:49.790
year olds can understand the definition
of the the conjecture that X to the M

00:05:49.790 --> 00:05:49.800
of the the conjecture that X to the M
 

00:05:49.800 --> 00:05:52.790
of the the conjecture that X to the M
plus y to the N equals e to the N has no

00:05:52.790 --> 00:05:52.800
plus y to the N equals e to the N has no
 

00:05:52.800 --> 00:05:55.010
plus y to the N equals e to the N has no
solution for N greater than three

00:05:55.010 --> 00:05:55.020
solution for N greater than three
 

00:05:55.020 --> 00:05:57.140
solution for N greater than three
greater than equal to three okay this

00:05:57.140 --> 00:05:57.150
greater than equal to three okay this
 

00:05:57.150 --> 00:05:59.350
greater than equal to three okay this
this has been unsolved there's been

00:05:59.350 --> 00:05:59.360
this has been unsolved there's been
 

00:05:59.360 --> 00:06:01.310
this has been unsolved there's been
hundreds of thousands of people try to

00:06:01.310 --> 00:06:01.320
hundreds of thousands of people try to
 

00:06:01.320 --> 00:06:04.969
hundreds of thousands of people try to
solve it and finally Andrew Wiles from

00:06:04.969 --> 00:06:04.979
solve it and finally Andrew Wiles from
 

00:06:04.979 --> 00:06:07.430
solve it and finally Andrew Wiles from
Oxford and Princeton had this final

00:06:07.430 --> 00:06:07.440
Oxford and Princeton had this final
 

00:06:07.440 --> 00:06:12.890
Oxford and Princeton had this final
breakthrough moment in 1994 so he first

00:06:12.890 --> 00:06:12.900
breakthrough moment in 1994 so he first
 

00:06:12.900 --> 00:06:14.839
breakthrough moment in 1994 so he first
proved in 1993 and then it was shown

00:06:14.839 --> 00:06:14.849
proved in 1993 and then it was shown
 

00:06:14.849 --> 00:06:17.149
proved in 1993 and then it was shown
that he failed and then so that's that's

00:06:17.149 --> 00:06:17.159
that he failed and then so that's that's
 

00:06:17.159 --> 00:06:19.250
that he failed and then so that's that's
the the human drama and then nightly so

00:06:19.250 --> 00:06:19.260
the the human drama and then nightly so
 

00:06:19.260 --> 00:06:21.800
the the human drama and then nightly so
he had he he spent a year trying to find

00:06:21.800 --> 00:06:21.810
he had he he spent a year trying to find
 

00:06:21.810 --> 00:06:23.450
he had he he spent a year trying to find
the solution and there's this moment

00:06:23.450 --> 00:06:23.460
the solution and there's this moment
 

00:06:23.460 --> 00:06:25.880
the solution and there's this moment
this final breakthrough three hundred

00:06:25.880 --> 00:06:25.890
this final breakthrough three hundred
 

00:06:25.890 --> 00:06:28.070
this final breakthrough three hundred
fifty eight years after this first form

00:06:28.070 --> 00:06:28.080
fifty eight years after this first form
 

00:06:28.080 --> 00:06:31.159
fifty eight years after this first form
of the by Pharma as a conjecture he he

00:06:31.159 --> 00:06:31.169
of the by Pharma as a conjecture he he
 

00:06:31.169 --> 00:06:33.620
of the by Pharma as a conjecture he he
said it was so incredibly beautiful it

00:06:33.620 --> 00:06:33.630
said it was so incredibly beautiful it
 

00:06:33.630 --> 00:06:35.780
said it was so incredibly beautiful it
was so simple so elegant I couldn't

00:06:35.780 --> 00:06:35.790
was so simple so elegant I couldn't
 

00:06:35.790 --> 00:06:37.700
was so simple so elegant I couldn't
understand how I'd missed it and I just

00:06:37.700 --> 00:06:37.710
understand how I'd missed it and I just
 

00:06:37.710 --> 00:06:39.439
understand how I'd missed it and I just
stared at it and just believed for 20

00:06:39.439 --> 00:06:39.449
stared at it and just believed for 20
 

00:06:39.449 --> 00:06:40.260
stared at it and just believed for 20
minutes

00:06:40.260 --> 00:06:40.270
minutes
 

00:06:40.270 --> 00:06:42.420
minutes
this is him finally closing the loop

00:06:42.420 --> 00:06:42.430
this is him finally closing the loop
 

00:06:42.430 --> 00:06:45.480
this is him finally closing the loop
figuring out the final proof I just

00:06:45.480 --> 00:06:45.490
figuring out the final proof I just
 

00:06:45.490 --> 00:06:47.700
figuring out the final proof I just
stared at it in bit disbelief for 20

00:06:47.700 --> 00:06:47.710
stared at it in bit disbelief for 20
 

00:06:47.710 --> 00:06:49.260
stared at it in bit disbelief for 20
minutes then during the day I walked

00:06:49.260 --> 00:06:49.270
minutes then during the day I walked
 

00:06:49.270 --> 00:06:51.210
minutes then during the day I walked
around the department and I keep coming

00:06:51.210 --> 00:06:51.220
around the department and I keep coming
 

00:06:51.220 --> 00:06:53.100
around the department and I keep coming
back to my desk looking to see if I was

00:06:53.100 --> 00:06:53.110
back to my desk looking to see if I was
 

00:06:53.110 --> 00:06:54.720
back to my desk looking to see if I was
still there it was still there

00:06:54.720 --> 00:06:54.730
still there it was still there
 

00:06:54.730 --> 00:06:56.460
still there it was still there
I couldn't contain myself I was so

00:06:56.460 --> 00:06:56.470
I couldn't contain myself I was so
 

00:06:56.470 --> 00:06:58.980
I couldn't contain myself I was so
excited it was the most important moment

00:06:58.980 --> 00:06:58.990
excited it was the most important moment
 

00:06:58.990 --> 00:07:01.590
excited it was the most important moment
of my working life nothing I ever done

00:07:01.590 --> 00:07:01.600
of my working life nothing I ever done
 

00:07:01.600 --> 00:07:03.960
of my working life nothing I ever done
nothing I ever do again will mean as

00:07:03.960 --> 00:07:03.970
nothing I ever do again will mean as
 

00:07:03.970 --> 00:07:06.240
nothing I ever do again will mean as
much so this moment a breakthrough

00:07:06.240 --> 00:07:06.250
much so this moment a breakthrough
 

00:07:06.250 --> 00:07:09.180
much so this moment a breakthrough
how do we teach neural networks how do

00:07:09.180 --> 00:07:09.190
how do we teach neural networks how do
 

00:07:09.190 --> 00:07:11.430
how do we teach neural networks how do
we learn from data to achieve this level

00:07:11.430 --> 00:07:11.440
we learn from data to achieve this level
 

00:07:11.440 --> 00:07:13.350
we learn from data to achieve this level
breakthrough that's the open question

00:07:13.350 --> 00:07:13.360
breakthrough that's the open question
 

00:07:13.360 --> 00:07:15.300
breakthrough that's the open question
that I want you to sort of walk away

00:07:15.300 --> 00:07:15.310
that I want you to sort of walk away
 

00:07:15.310 --> 00:07:18.300
that I want you to sort of walk away
from this part of the lecture thinking

00:07:18.300 --> 00:07:18.310
from this part of the lecture thinking
 

00:07:18.310 --> 00:07:21.420
from this part of the lecture thinking
about what is the future of agents that

00:07:21.420 --> 00:07:21.430
about what is the future of agents that
 

00:07:21.430 --> 00:07:23.820
about what is the future of agents that
think and Alexander will talk about the

00:07:23.820 --> 00:07:23.830
think and Alexander will talk about the
 

00:07:23.830 --> 00:07:26.580
think and Alexander will talk about the
new future challenges next but what what

00:07:26.580 --> 00:07:26.590
new future challenges next but what what
 

00:07:26.590 --> 00:07:29.040
new future challenges next but what what
can we use deep reinforcement learning

00:07:29.040 --> 00:07:29.050
can we use deep reinforcement learning
 

00:07:29.050 --> 00:07:31.770
can we use deep reinforcement learning
to extend past memorization passpack

00:07:31.770 --> 00:07:31.780
to extend past memorization passpack
 

00:07:31.780 --> 00:07:33.540
to extend past memorization passpack
pattern recognition into something like

00:07:33.540 --> 00:07:33.550
pattern recognition into something like
 

00:07:33.550 --> 00:07:34.980
pattern recognition into something like
reasoning and achieving this

00:07:34.980 --> 00:07:34.990
reasoning and achieving this
 

00:07:34.990 --> 00:07:36.870
reasoning and achieving this
breakthrough moment and at the very

00:07:36.870 --> 00:07:36.880
breakthrough moment and at the very
 

00:07:36.880 --> 00:07:41.070
breakthrough moment and at the very
least something brilliantly in 1995 and

00:07:41.070 --> 00:07:41.080
least something brilliantly in 1995 and
 

00:07:41.080 --> 00:07:44.280
least something brilliantly in 1995 and
after Andrew Wiles Homer Simpson and and

00:07:44.280 --> 00:07:44.290
after Andrew Wiles Homer Simpson and and
 

00:07:44.290 --> 00:07:46.380
after Andrew Wiles Homer Simpson and and
those are fans of The Simpsons actually

00:07:46.380 --> 00:07:46.390
those are fans of The Simpsons actually
 

00:07:46.390 --> 00:07:48.270
those are fans of The Simpsons actually
proved him wrong this is very

00:07:48.270 --> 00:07:48.280
proved him wrong this is very
 

00:07:48.280 --> 00:07:50.190
proved him wrong this is very
interesting so you found an example

00:07:50.190 --> 00:07:50.200
interesting so you found an example
 

00:07:50.200 --> 00:07:51.780
interesting so you found an example
where it does hold true the form a

00:07:51.780 --> 00:07:51.790
where it does hold true the form a
 

00:07:51.790 --> 00:07:54.150
where it does hold true the form a
theorem to a certain number of digits

00:07:54.150 --> 00:07:54.160
theorem to a certain number of digits
 

00:07:54.160 --> 00:07:58.230
theorem to a certain number of digits
after the period okay and then finally

00:07:58.230 --> 00:07:58.240
after the period okay and then finally
 

00:07:58.240 --> 00:08:00.240
after the period okay and then finally
aggregating this knowledge into action

00:08:00.240 --> 00:08:00.250
aggregating this knowledge into action
 

00:08:00.250 --> 00:08:01.440
aggregating this knowledge into action
that's what deep reinforcement learning

00:08:01.440 --> 00:08:01.450
that's what deep reinforcement learning
 

00:08:01.450 --> 00:08:04.110
that's what deep reinforcement learning
is about extracting patterns from raw

00:08:04.110 --> 00:08:04.120
is about extracting patterns from raw
 

00:08:04.120 --> 00:08:07.620
is about extracting patterns from raw
data and then finally being able to

00:08:07.620 --> 00:08:07.630
data and then finally being able to
 

00:08:07.630 --> 00:08:10.740
data and then finally being able to
estimate the state of the world around

00:08:10.740 --> 00:08:10.750
estimate the state of the world around
 

00:08:10.750 --> 00:08:13.410
estimate the state of the world around
the agent in order to make successful

00:08:13.410 --> 00:08:13.420
the agent in order to make successful
 

00:08:13.420 --> 00:08:17.480
the agent in order to make successful
action that completes at a certain goal

00:08:17.480 --> 00:08:17.490
 
 

00:08:17.490 --> 00:08:20.610
 
so and I will talk about the difference

00:08:20.610 --> 00:08:20.620
so and I will talk about the difference
 

00:08:20.620 --> 00:08:22.890
so and I will talk about the difference
between agents that are learning from

00:08:22.890 --> 00:08:22.900
between agents that are learning from
 

00:08:22.900 --> 00:08:24.630
between agents that are learning from
data and agents that are currently

00:08:24.630 --> 00:08:24.640
data and agents that are currently
 

00:08:24.640 --> 00:08:26.280
data and agents that are currently
successfully being able to operate in

00:08:26.280 --> 00:08:26.290
successfully being able to operate in
 

00:08:26.290 --> 00:08:28.320
successfully being able to operate in
this world example the agents here from

00:08:28.320 --> 00:08:28.330
this world example the agents here from
 

00:08:28.330 --> 00:08:30.510
this world example the agents here from
Boston Dynamics are ones that don't use

00:08:30.510 --> 00:08:30.520
Boston Dynamics are ones that don't use
 

00:08:30.520 --> 00:08:31.980
Boston Dynamics are ones that don't use
any deeper enforcement learning that

00:08:31.980 --> 00:08:31.990
any deeper enforcement learning that
 

00:08:31.990 --> 00:08:33.930
any deeper enforcement learning that
don't use any they don't learn from data

00:08:33.930 --> 00:08:33.940
don't use any they don't learn from data
 

00:08:33.940 --> 00:08:36.120
don't use any they don't learn from data
this is the open gap the challenge that

00:08:36.120 --> 00:08:36.130
this is the open gap the challenge that
 

00:08:36.130 --> 00:08:38.130
this is the open gap the challenge that
we have to solve how do we use

00:08:38.130 --> 00:08:38.140
we have to solve how do we use
 

00:08:38.140 --> 00:08:40.020
we have to solve how do we use
reinforcement learning methods build

00:08:40.020 --> 00:08:40.030
reinforcement learning methods build
 

00:08:40.030 --> 00:08:42.180
reinforcement learning methods build
robots agents that act in the real world

00:08:42.180 --> 00:08:42.190
robots agents that act in the real world
 

00:08:42.190 --> 00:08:45.120
robots agents that act in the real world
that learn from that world except for

00:08:45.120 --> 00:08:45.130
that learn from that world except for
 

00:08:45.130 --> 00:08:48.120
that learn from that world except for
the perception task so in this stack you

00:08:48.120 --> 00:08:48.130
the perception task so in this stack you
 

00:08:48.130 --> 00:08:49.440
the perception task so in this stack you
can think from the environment to the

00:08:49.440 --> 00:08:49.450
can think from the environment to the
 

00:08:49.450 --> 00:08:51.720
can think from the environment to the
effectors the promise the beautiful

00:08:51.720 --> 00:08:51.730
effectors the promise the beautiful
 

00:08:51.730 --> 00:08:53.730
effectors the promise the beautiful
power of deep learning is

00:08:53.730 --> 00:08:53.740
power of deep learning is
 

00:08:53.740 --> 00:08:56.100
power of deep learning is
taking the raw sensory data and being

00:08:56.100 --> 00:08:56.110
taking the raw sensory data and being
 

00:08:56.110 --> 00:08:58.200
taking the raw sensory data and being
able to an automated way do feature

00:08:58.200 --> 00:08:58.210
able to an automated way do feature
 

00:08:58.210 --> 00:09:00.930
able to an automated way do feature
learning to extract arbitrary high order

00:09:00.930 --> 00:09:00.940
learning to extract arbitrary high order
 

00:09:00.940 --> 00:09:03.180
learning to extract arbitrary high order
representations and that data so makes

00:09:03.180 --> 00:09:03.190
representations and that data so makes
 

00:09:03.190 --> 00:09:05.070
representations and that data so makes
sense of the patterns in order to be

00:09:05.070 --> 00:09:05.080
sense of the patterns in order to be
 

00:09:05.080 --> 00:09:08.640
sense of the patterns in order to be
able to learn in supervised learning to

00:09:08.640 --> 00:09:08.650
able to learn in supervised learning to
 

00:09:08.650 --> 00:09:09.980
able to learn in supervised learning to
learn the mapping of those patterns

00:09:09.980 --> 00:09:09.990
learn the mapping of those patterns
 

00:09:09.990 --> 00:09:11.160
learn the mapping of those patterns
arbitrarily

00:09:11.160 --> 00:09:11.170
arbitrarily
 

00:09:11.170 --> 00:09:13.140
arbitrarily
high order representations on those

00:09:13.140 --> 00:09:13.150
high order representations on those
 

00:09:13.150 --> 00:09:18.090
high order representations on those
patterns to to to extract actionable

00:09:18.090 --> 00:09:18.100
patterns to to to extract actionable
 

00:09:18.100 --> 00:09:20.190
patterns to to to extract actionable
useful knowledge that's in the red box

00:09:20.190 --> 00:09:20.200
useful knowledge that's in the red box
 

00:09:20.200 --> 00:09:22.440
useful knowledge that's in the red box
so the promise of deep reinforcement

00:09:22.440 --> 00:09:22.450
so the promise of deep reinforcement
 

00:09:22.450 --> 00:09:25.520
so the promise of deep reinforcement
learning why it's so exciting for

00:09:25.520 --> 00:09:25.530
learning why it's so exciting for
 

00:09:25.530 --> 00:09:28.190
learning why it's so exciting for
artificial intelligence community why it

00:09:28.190 --> 00:09:28.200
artificial intelligence community why it
 

00:09:28.200 --> 00:09:30.510
artificial intelligence community why it
captivates our imaginations about the

00:09:30.510 --> 00:09:30.520
captivates our imaginations about the
 

00:09:30.520 --> 00:09:32.760
captivates our imaginations about the
possibility towards achieving human

00:09:32.760 --> 00:09:32.770
possibility towards achieving human
 

00:09:32.770 --> 00:09:34.920
possibility towards achieving human
level general intelligence is that you

00:09:34.920 --> 00:09:34.930
level general intelligence is that you
 

00:09:34.930 --> 00:09:36.380
level general intelligence is that you
can take not just the end-to-end

00:09:36.380 --> 00:09:36.390
can take not just the end-to-end
 

00:09:36.390 --> 00:09:40.290
can take not just the end-to-end
extraction of knowledge from raw sensory

00:09:40.290 --> 00:09:40.300
extraction of knowledge from raw sensory
 

00:09:40.300 --> 00:09:45.240
extraction of knowledge from raw sensory
data you can also do end-to-end from raw

00:09:45.240 --> 00:09:45.250
data you can also do end-to-end from raw
 

00:09:45.250 --> 00:09:48.660
data you can also do end-to-end from raw
sensory data be able to produce actions

00:09:48.660 --> 00:09:48.670
sensory data be able to produce actions
 

00:09:48.670 --> 00:09:53.540
sensory data be able to produce actions
to brute force learn from the raw data

00:09:53.540 --> 00:09:53.550
to brute force learn from the raw data
 

00:09:53.550 --> 00:09:59.160
to brute force learn from the raw data
the the semantic context the meaning of

00:09:59.160 --> 00:09:59.170
the the semantic context the meaning of
 

00:09:59.170 --> 00:10:00.600
the the semantic context the meaning of
the world around you in order to

00:10:00.600 --> 00:10:00.610
the world around you in order to
 

00:10:00.610 --> 00:10:02.850
the world around you in order to
successfully act in our world and to end

00:10:02.850 --> 00:10:02.860
successfully act in our world and to end
 

00:10:02.860 --> 00:10:04.590
successfully act in our world and to end
just like we humans do that's the

00:10:04.590 --> 00:10:04.600
just like we humans do that's the
 

00:10:04.600 --> 00:10:09.300
just like we humans do that's the
promise but we're in the very early

00:10:09.300 --> 00:10:09.310
promise but we're in the very early
 

00:10:09.310 --> 00:10:16.250
promise but we're in the very early
stages of of achieving that promise so

00:10:16.250 --> 00:10:16.260
stages of of achieving that promise so
 

00:10:16.260 --> 00:10:18.840
stages of of achieving that promise so
any successful presentation must include

00:10:18.840 --> 00:10:18.850
any successful presentation must include
 

00:10:18.850 --> 00:10:21.540
any successful presentation must include
cats so supervised learning and

00:10:21.540 --> 00:10:21.550
cats so supervised learning and
 

00:10:21.550 --> 00:10:23.220
cats so supervised learning and
unsupervised learning and reinforcement

00:10:23.220 --> 00:10:23.230
unsupervised learning and reinforcement
 

00:10:23.230 --> 00:10:25.230
unsupervised learning and reinforcement
learning sits in the middle was

00:10:25.230 --> 00:10:25.240
learning sits in the middle was
 

00:10:25.240 --> 00:10:29.520
learning sits in the middle was
supervised learning most of the the data

00:10:29.520 --> 00:10:29.530
supervised learning most of the the data
 

00:10:29.530 --> 00:10:31.890
supervised learning most of the the data
has to come from the human they're the

00:10:31.890 --> 00:10:31.900
has to come from the human they're the
 

00:10:31.900 --> 00:10:34.800
has to come from the human they're the
insights about what is inside the data

00:10:34.800 --> 00:10:34.810
insights about what is inside the data
 

00:10:34.810 --> 00:10:37.050
insights about what is inside the data
has to come from human annotations and

00:10:37.050 --> 00:10:37.060
has to come from human annotations and
 

00:10:37.060 --> 00:10:38.760
has to come from human annotations and
it's the task of the machine to learn

00:10:38.760 --> 00:10:38.770
it's the task of the machine to learn
 

00:10:38.770 --> 00:10:40.740
it's the task of the machine to learn
how to generalize based on those human

00:10:40.740 --> 00:10:40.750
how to generalize based on those human
 

00:10:40.750 --> 00:10:43.290
how to generalize based on those human
annotations over future examples it

00:10:43.290 --> 00:10:43.300
annotations over future examples it
 

00:10:43.300 --> 00:10:45.270
annotations over future examples it
hasn't seen before and unsupervised

00:10:45.270 --> 00:10:45.280
hasn't seen before and unsupervised
 

00:10:45.280 --> 00:10:47.690
hasn't seen before and unsupervised
learning you have no human annotation

00:10:47.690 --> 00:10:47.700
learning you have no human annotation
 

00:10:47.700 --> 00:10:50.430
learning you have no human annotation
reinforcement learning is somewhere in

00:10:50.430 --> 00:10:50.440
reinforcement learning is somewhere in
 

00:10:50.440 --> 00:10:51.720
reinforcement learning is somewhere in
between close around supervised learning

00:10:51.720 --> 00:10:51.730
between close around supervised learning
 

00:10:51.730 --> 00:10:54.390
between close around supervised learning
where the annotations from humans the

00:10:54.390 --> 00:10:54.400
where the annotations from humans the
 

00:10:54.400 --> 00:10:56.070
where the annotations from humans the
information the knowledge from humans is

00:10:56.070 --> 00:10:56.080
information the knowledge from humans is
 

00:10:56.080 --> 00:10:58.620
information the knowledge from humans is
very sparse extremely sparse and so you

00:10:58.620 --> 00:10:58.630
very sparse extremely sparse and so you
 

00:10:58.630 --> 00:11:01.140
very sparse extremely sparse and so you
have to use the temporal dynamics the

00:11:01.140 --> 00:11:01.150
have to use the temporal dynamics the
 

00:11:01.150 --> 00:11:05.060
have to use the temporal dynamics the
fact that in time these the the

00:11:05.060 --> 00:11:05.070
fact that in time these the the
 

00:11:05.070 --> 00:11:06.990
fact that in time these the the
continuity of

00:11:06.990 --> 00:11:07.000
continuity of
 

00:11:07.000 --> 00:11:10.050
continuity of
our world through time you have to use

00:11:10.050 --> 00:11:10.060
our world through time you have to use
 

00:11:10.060 --> 00:11:12.329
our world through time you have to use
the sparse little rewards you have along

00:11:12.329 --> 00:11:12.339
the sparse little rewards you have along
 

00:11:12.339 --> 00:11:14.790
the sparse little rewards you have along
the way to extend that over the entirety

00:11:14.790 --> 00:11:14.800
the way to extend that over the entirety
 

00:11:14.800 --> 00:11:17.819
the way to extend that over the entirety
of the the temporal domain to make some

00:11:17.819 --> 00:11:17.829
of the the temporal domain to make some
 

00:11:17.829 --> 00:11:19.259
of the the temporal domain to make some
sense of the world even though the

00:11:19.259 --> 00:11:19.269
sense of the world even though the
 

00:11:19.269 --> 00:11:21.449
sense of the world even though the
rewards are really sparse those are two

00:11:21.449 --> 00:11:21.459
rewards are really sparse those are two
 

00:11:21.459 --> 00:11:25.740
rewards are really sparse those are two
cats learning Pavlov's cats if you will

00:11:25.740 --> 00:11:25.750
cats learning Pavlov's cats if you will
 

00:11:25.750 --> 00:11:29.340
cats learning Pavlov's cats if you will
learning to ring the doorbell in order

00:11:29.340 --> 00:11:29.350
learning to ring the doorbell in order
 

00:11:29.350 --> 00:11:31.379
learning to ring the doorbell in order
to get some food that's the basic for

00:11:31.379 --> 00:11:31.389
to get some food that's the basic for
 

00:11:31.389 --> 00:11:33.179
to get some food that's the basic for
them the reinforcement learning problem

00:11:33.179 --> 00:11:33.189
them the reinforcement learning problem
 

00:11:33.189 --> 00:11:35.730
them the reinforcement learning problem
so from supervised learning you can

00:11:35.730 --> 00:11:35.740
so from supervised learning you can
 

00:11:35.740 --> 00:11:37.550
so from supervised learning you can
think of those networks as memorizers

00:11:37.550 --> 00:11:37.560
think of those networks as memorizers
 

00:11:37.560 --> 00:11:39.869
think of those networks as memorizers
reinforcement learning is you can think

00:11:39.869 --> 00:11:39.879
reinforcement learning is you can think
 

00:11:39.879 --> 00:11:42.030
reinforcement learning is you can think
of them crudely so as sort of

00:11:42.030 --> 00:11:42.040
of them crudely so as sort of
 

00:11:42.040 --> 00:11:44.720
of them crudely so as sort of
brute-force reasoning trying to

00:11:44.720 --> 00:11:44.730
brute-force reasoning trying to
 

00:11:44.730 --> 00:11:47.999
brute-force reasoning trying to
propagate rewards into extending how to

00:11:47.999 --> 00:11:48.009
propagate rewards into extending how to
 

00:11:48.009 --> 00:11:49.679
propagate rewards into extending how to
make sense of the world in order to act

00:11:49.679 --> 00:11:49.689
make sense of the world in order to act
 

00:11:49.689 --> 00:11:53.249
make sense of the world in order to act
in it the pieces are simple there's an

00:11:53.249 --> 00:11:53.259
in it the pieces are simple there's an
 

00:11:53.259 --> 00:11:55.650
in it the pieces are simple there's an
environment there's an agent it takes

00:11:55.650 --> 00:11:55.660
environment there's an agent it takes
 

00:11:55.660 --> 00:11:57.629
environment there's an agent it takes
actions in that agent it senses the

00:11:57.629 --> 00:11:57.639
actions in that agent it senses the
 

00:11:57.639 --> 00:11:59.519
actions in that agent it senses the
environment so there's always a state at

00:11:59.519 --> 00:11:59.529
environment so there's always a state at
 

00:11:59.529 --> 00:12:03.329
environment so there's always a state at
the agent senses and then I always when

00:12:03.329 --> 00:12:03.339
the agent senses and then I always when
 

00:12:03.339 --> 00:12:05.069
the agent senses and then I always when
taking an action receives some kind of

00:12:05.069 --> 00:12:05.079
taking an action receives some kind of
 

00:12:05.079 --> 00:12:07.550
taking an action receives some kind of
reward or punishment from that world so

00:12:07.550 --> 00:12:07.560
reward or punishment from that world so
 

00:12:07.560 --> 00:12:09.960
reward or punishment from that world so
we can model any kind of world in this

00:12:09.960 --> 00:12:09.970
we can model any kind of world in this
 

00:12:09.970 --> 00:12:13.319
we can model any kind of world in this
way they can model an arcade game break

00:12:13.319 --> 00:12:13.329
way they can model an arcade game break
 

00:12:13.329 --> 00:12:15.660
way they can model an arcade game break
out here Atari break out the engine has

00:12:15.660 --> 00:12:15.670
out here Atari break out the engine has
 

00:12:15.670 --> 00:12:17.910
out here Atari break out the engine has
the capacity to act move the paddle it

00:12:17.910 --> 00:12:17.920
the capacity to act move the paddle it
 

00:12:17.920 --> 00:12:20.790
the capacity to act move the paddle it
has it can influence the future state of

00:12:20.790 --> 00:12:20.800
has it can influence the future state of
 

00:12:20.800 --> 00:12:22.439
has it can influence the future state of
the system by taking those actions and

00:12:22.439 --> 00:12:22.449
the system by taking those actions and
 

00:12:22.449 --> 00:12:24.480
the system by taking those actions and
it achieves the reward there's a goal to

00:12:24.480 --> 00:12:24.490
it achieves the reward there's a goal to
 

00:12:24.490 --> 00:12:26.790
it achieves the reward there's a goal to
this game the goal is to maximize future

00:12:26.790 --> 00:12:26.800
this game the goal is to maximize future
 

00:12:26.800 --> 00:12:29.879
this game the goal is to maximize future
reward you can model the cart pole

00:12:29.879 --> 00:12:29.889
reward you can model the cart pole
 

00:12:29.889 --> 00:12:32.189
reward you can model the cart pole
balance a problem where the you can

00:12:32.189 --> 00:12:32.199
balance a problem where the you can
 

00:12:32.199 --> 00:12:34.170
balance a problem where the you can
control the pole Angle angular speed you

00:12:34.170 --> 00:12:34.180
control the pole Angle angular speed you
 

00:12:34.180 --> 00:12:35.579
control the pole Angle angular speed you
can control the cart position the

00:12:35.579 --> 00:12:35.589
can control the cart position the
 

00:12:35.589 --> 00:12:38.369
can control the cart position the
horizontal velocity the actions of that

00:12:38.369 --> 00:12:38.379
horizontal velocity the actions of that
 

00:12:38.379 --> 00:12:41.759
horizontal velocity the actions of that
is the pushing the cart applying the

00:12:41.759 --> 00:12:41.769
is the pushing the cart applying the
 

00:12:41.769 --> 00:12:43.379
is the pushing the cart applying the
force to the cart and the task is to

00:12:43.379 --> 00:12:43.389
force to the cart and the task is to
 

00:12:43.389 --> 00:12:46.019
force to the cart and the task is to
balance the pole okay and the reward is

00:12:46.019 --> 00:12:46.029
balance the pole okay and the reward is
 

00:12:46.029 --> 00:12:47.850
balance the pole okay and the reward is
one at each time step the pole still

00:12:47.850 --> 00:12:47.860
one at each time step the pole still
 

00:12:47.860 --> 00:12:49.799
one at each time step the pole still
upright that's the goal so that's a

00:12:49.799 --> 00:12:49.809
upright that's the goal so that's a
 

00:12:49.809 --> 00:12:52.439
upright that's the goal so that's a
simple formulation of state action

00:12:52.439 --> 00:12:52.449
simple formulation of state action
 

00:12:52.449 --> 00:12:55.319
simple formulation of state action
reward you can play a game of doom with

00:12:55.319 --> 00:12:55.329
reward you can play a game of doom with
 

00:12:55.329 --> 00:12:57.540
reward you can play a game of doom with
a state being the raw pixels of the of

00:12:57.540 --> 00:12:57.550
a state being the raw pixels of the of
 

00:12:57.550 --> 00:13:00.299
a state being the raw pixels of the of
the game and the actions are moving the

00:13:00.299 --> 00:13:00.309
the game and the actions are moving the
 

00:13:00.309 --> 00:13:03.600
the game and the actions are moving the
the player around shooting and the

00:13:03.600 --> 00:13:03.610
the player around shooting and the
 

00:13:03.610 --> 00:13:07.259
the player around shooting and the
reward positive one at eliminating an

00:13:07.259 --> 00:13:07.269
reward positive one at eliminating an
 

00:13:07.269 --> 00:13:09.269
reward positive one at eliminating an
opponent and negative one agent is

00:13:09.269 --> 00:13:09.279
opponent and negative one agent is
 

00:13:09.279 --> 00:13:09.929
opponent and negative one agent is
eliminated

00:13:09.929 --> 00:13:09.939
eliminated
 

00:13:09.939 --> 00:13:13.470
eliminated
it's just a game industrial robotics any

00:13:13.470 --> 00:13:13.480
it's just a game industrial robotics any
 

00:13:13.480 --> 00:13:15.329
it's just a game industrial robotics any
kind of humanoid robotics when you have

00:13:15.329 --> 00:13:15.339
kind of humanoid robotics when you have
 

00:13:15.339 --> 00:13:17.129
kind of humanoid robotics when you have
to control multiple degrees of freedom

00:13:17.129 --> 00:13:17.139
to control multiple degrees of freedom
 

00:13:17.139 --> 00:13:18.689
to control multiple degrees of freedom
control the robotic arm control the

00:13:18.689 --> 00:13:18.699
control the robotic arm control the
 

00:13:18.699 --> 00:13:19.950
control the robotic arm control the
robot the

00:13:19.950 --> 00:13:19.960
robot the
 

00:13:19.960 --> 00:13:21.720
robot the
State is the raw pixels of the real

00:13:21.720 --> 00:13:21.730
State is the raw pixels of the real
 

00:13:21.730 --> 00:13:23.580
State is the raw pixels of the real
world coming into the sensors of the

00:13:23.580 --> 00:13:23.590
world coming into the sensors of the
 

00:13:23.590 --> 00:13:25.650
world coming into the sensors of the
robot the actions of the possible

00:13:25.650 --> 00:13:25.660
robot the actions of the possible
 

00:13:25.660 --> 00:13:27.000
robot the actions of the possible
actions the robot can take so

00:13:27.000 --> 00:13:27.010
actions the robot can take so
 

00:13:27.010 --> 00:13:28.560
actions the robot can take so
manipulating each of its sensors

00:13:28.560 --> 00:13:28.570
manipulating each of its sensors
 

00:13:28.570 --> 00:13:31.320
manipulating each of its sensors
actuators sorry actuators and then the

00:13:31.320 --> 00:13:31.330
actuators sorry actuators and then the
 

00:13:31.330 --> 00:13:33.300
actuators sorry actuators and then the
reward is positive when placing a device

00:13:33.300 --> 00:13:33.310
reward is positive when placing a device
 

00:13:33.310 --> 00:13:35.280
reward is positive when placing a device
successfully negative otherwise so the

00:13:35.280 --> 00:13:35.290
successfully negative otherwise so the
 

00:13:35.290 --> 00:13:36.750
successfully negative otherwise so the
task is to pick something up put it back

00:13:36.750 --> 00:13:36.760
task is to pick something up put it back
 

00:13:36.760 --> 00:13:42.380
task is to pick something up put it back
down okay so a and I'd like to sort of

00:13:42.380 --> 00:13:42.390
down okay so a and I'd like to sort of
 

00:13:42.390 --> 00:13:44.640
down okay so a and I'd like to sort of
continue this trajectory further and

00:13:44.640 --> 00:13:44.650
continue this trajectory further and
 

00:13:44.650 --> 00:13:46.860
continue this trajectory further and
further complex systems to think the

00:13:46.860 --> 00:13:46.870
further complex systems to think the
 

00:13:46.870 --> 00:13:48.810
further complex systems to think the
biggest challenge for reinforcement

00:13:48.810 --> 00:13:48.820
biggest challenge for reinforcement
 

00:13:48.820 --> 00:13:51.570
biggest challenge for reinforcement
learning is formulating the world that

00:13:51.570 --> 00:13:51.580
learning is formulating the world that
 

00:13:51.580 --> 00:13:53.460
learning is formulating the world that
we need to solve the set of goals in

00:13:53.460 --> 00:13:53.470
we need to solve the set of goals in
 

00:13:53.470 --> 00:13:55.530
we need to solve the set of goals in
such a way that can be a we can apply

00:13:55.530 --> 00:13:55.540
such a way that can be a we can apply
 

00:13:55.540 --> 00:13:56.760
such a way that can be a we can apply
these deep reinforcement reinforcement

00:13:56.760 --> 00:13:56.770
these deep reinforcement reinforcement
 

00:13:56.770 --> 00:13:59.160
these deep reinforcement reinforcement
learning methods so give you an

00:13:59.160 --> 00:13:59.170
learning methods so give you an
 

00:13:59.170 --> 00:14:01.530
learning methods so give you an
intuition about us humans it's

00:14:01.530 --> 00:14:01.540
intuition about us humans it's
 

00:14:01.540 --> 00:14:04.020
intuition about us humans it's
exceptionally difficult to formulate the

00:14:04.020 --> 00:14:04.030
exceptionally difficult to formulate the
 

00:14:04.030 --> 00:14:06.030
exceptionally difficult to formulate the
goals of life what is a survival

00:14:06.030 --> 00:14:06.040
goals of life what is a survival
 

00:14:06.040 --> 00:14:09.710
goals of life what is a survival
homeostasis happiness who knows

00:14:09.710 --> 00:14:09.720
homeostasis happiness who knows
 

00:14:09.720 --> 00:14:13.020
homeostasis happiness who knows
citations depending on who you are so

00:14:13.020 --> 00:14:13.030
citations depending on who you are so
 

00:14:13.030 --> 00:14:14.970
citations depending on who you are so
state the state sight hearing taste

00:14:14.970 --> 00:14:14.980
state the state sight hearing taste
 

00:14:14.980 --> 00:14:16.820
state the state sight hearing taste
smell touch that's the raw sensory data

00:14:16.820 --> 00:14:16.830
smell touch that's the raw sensory data
 

00:14:16.830 --> 00:14:19.860
smell touch that's the raw sensory data
actions are think move what else I don't

00:14:19.860 --> 00:14:19.870
actions are think move what else I don't
 

00:14:19.870 --> 00:14:22.320
actions are think move what else I don't
know and then the reward is just it's

00:14:22.320 --> 00:14:22.330
know and then the reward is just it's
 

00:14:22.330 --> 00:14:24.480
know and then the reward is just it's
open all these questions are open so if

00:14:24.480 --> 00:14:24.490
open all these questions are open so if
 

00:14:24.490 --> 00:14:26.460
open all these questions are open so if
we want to actually start to create more

00:14:26.460 --> 00:14:26.470
we want to actually start to create more
 

00:14:26.470 --> 00:14:28.230
we want to actually start to create more
and more intelligent systems it's hard

00:14:28.230 --> 00:14:28.240
and more intelligent systems it's hard
 

00:14:28.240 --> 00:14:30.120
and more intelligent systems it's hard
to formulate what the goals are with the

00:14:30.120 --> 00:14:30.130
to formulate what the goals are with the
 

00:14:30.130 --> 00:14:32.100
to formulate what the goals are with the
state space is with the action spaces

00:14:32.100 --> 00:14:32.110
state space is with the action spaces
 

00:14:32.110 --> 00:14:35.730
state space is with the action spaces
that's the that's if this the if you

00:14:35.730 --> 00:14:35.740
that's the that's if this the if you
 

00:14:35.740 --> 00:14:37.170
that's the that's if this the if you
take it away anything sort of in a

00:14:37.170 --> 00:14:37.180
take it away anything sort of in a
 

00:14:37.180 --> 00:14:39.960
take it away anything sort of in a
practical sense from today form from the

00:14:39.960 --> 00:14:39.970
practical sense from today form from the
 

00:14:39.970 --> 00:14:41.700
practical sense from today form from the
deeper enforcement learning part here a

00:14:41.700 --> 00:14:41.710
deeper enforcement learning part here a
 

00:14:41.710 --> 00:14:45.480
deeper enforcement learning part here a
few slides is that there's a fun part

00:14:45.480 --> 00:14:45.490
few slides is that there's a fun part
 

00:14:45.490 --> 00:14:48.210
few slides is that there's a fun part
and a hard part to all of this work so

00:14:48.210 --> 00:14:48.220
and a hard part to all of this work so
 

00:14:48.220 --> 00:14:50.160
and a hard part to all of this work so
the fun part the what this course is

00:14:50.160 --> 00:14:50.170
the fun part the what this course is
 

00:14:50.170 --> 00:14:50.460
the fun part the what this course is
about

00:14:50.460 --> 00:14:50.470
about
 

00:14:50.470 --> 00:14:53.160
about
I hope it's inspire people about the

00:14:53.160 --> 00:14:53.170
I hope it's inspire people about the
 

00:14:53.170 --> 00:14:54.930
I hope it's inspire people about the
amazing interesting fundamental

00:14:54.930 --> 00:14:54.940
amazing interesting fundamental
 

00:14:54.940 --> 00:14:57.480
amazing interesting fundamental
algorithms of deep learning that's the

00:14:57.480 --> 00:14:57.490
algorithms of deep learning that's the
 

00:14:57.490 --> 00:15:01.500
algorithms of deep learning that's the
fun part the hard part is the collecting

00:15:01.500 --> 00:15:01.510
fun part the hard part is the collecting
 

00:15:01.510 --> 00:15:02.970
fun part the hard part is the collecting
and annotating huge amount of

00:15:02.970 --> 00:15:02.980
and annotating huge amount of
 

00:15:02.980 --> 00:15:04.890
and annotating huge amount of
representative data in deep learning

00:15:04.890 --> 00:15:04.900
representative data in deep learning
 

00:15:04.900 --> 00:15:07.380
representative data in deep learning
informing higher representations data

00:15:07.380 --> 00:15:07.390
informing higher representations data
 

00:15:07.390 --> 00:15:11.130
informing higher representations data
does the hard work so data is everything

00:15:11.130 --> 00:15:11.140
does the hard work so data is everything
 

00:15:11.140 --> 00:15:12.660
does the hard work so data is everything
once you have good algorithms data is

00:15:12.660 --> 00:15:12.670
once you have good algorithms data is
 

00:15:12.670 --> 00:15:14.220
once you have good algorithms data is
everything and deep reinforcement

00:15:14.220 --> 00:15:14.230
everything and deep reinforcement
 

00:15:14.230 --> 00:15:17.370
everything and deep reinforcement
learning the heart the fun part again is

00:15:17.370 --> 00:15:17.380
learning the heart the fun part again is
 

00:15:17.380 --> 00:15:19.230
learning the heart the fun part again is
these algorithms we'll talk about them

00:15:19.230 --> 00:15:19.240
these algorithms we'll talk about them
 

00:15:19.240 --> 00:15:22.260
these algorithms we'll talk about them
today we will overview them but the hard

00:15:22.260 --> 00:15:22.270
today we will overview them but the hard
 

00:15:22.270 --> 00:15:25.050
today we will overview them but the hard
part is defining the world the action

00:15:25.050 --> 00:15:25.060
part is defining the world the action
 

00:15:25.060 --> 00:15:27.890
part is defining the world the action
space and the reward space now it's just

00:15:27.890 --> 00:15:27.900
space and the reward space now it's just
 

00:15:27.900 --> 00:15:30.360
space and the reward space now it's just
defining formalizing the problem is

00:15:30.360 --> 00:15:30.370
defining formalizing the problem is
 

00:15:30.370 --> 00:15:32.160
defining formalizing the problem is
actually exceptionally difficult when

00:15:32.160 --> 00:15:32.170
actually exceptionally difficult when
 

00:15:32.170 --> 00:15:33.400
actually exceptionally difficult when
you start to

00:15:33.400 --> 00:15:33.410
you start to
 

00:15:33.410 --> 00:15:36.879
you start to
to create an agent that operates in the

00:15:36.879 --> 00:15:36.889
to create an agent that operates in the
 

00:15:36.889 --> 00:15:38.470
to create an agent that operates in the
real world and actually operates with

00:15:38.470 --> 00:15:38.480
real world and actually operates with
 

00:15:38.480 --> 00:15:39.939
real world and actually operates with
other human beings and are actually

00:15:39.939 --> 00:15:39.949
other human beings and are actually
 

00:15:39.949 --> 00:15:42.850
other human beings and are actually
significantly helps in the world so this

00:15:42.850 --> 00:15:42.860
significantly helps in the world so this
 

00:15:42.860 --> 00:15:44.559
significantly helps in the world so this
isn't playing an arcade game where

00:15:44.559 --> 00:15:44.569
isn't playing an arcade game where
 

00:15:44.569 --> 00:15:46.389
isn't playing an arcade game where
everything is clean or playing chess go

00:15:46.389 --> 00:15:46.399
everything is clean or playing chess go
 

00:15:46.399 --> 00:15:48.460
everything is clean or playing chess go
it's when you're operating in the real

00:15:48.460 --> 00:15:48.470
it's when you're operating in the real
 

00:15:48.470 --> 00:15:50.139
it's when you're operating in the real
world everything is messy how do you

00:15:50.139 --> 00:15:50.149
world everything is messy how do you
 

00:15:50.149 --> 00:15:51.970
world everything is messy how do you
formalize that that's the hard part and

00:15:51.970 --> 00:15:51.980
formalize that that's the hard part and
 

00:15:51.980 --> 00:15:55.059
formalize that that's the hard part and
then the hardest part is getting a lot

00:15:55.059 --> 00:15:55.069
then the hardest part is getting a lot
 

00:15:55.069 --> 00:15:57.850
then the hardest part is getting a lot
of meaningful data that represents that

00:15:57.850 --> 00:15:57.860
of meaningful data that represents that
 

00:15:57.860 --> 00:15:59.710
of meaningful data that represents that
fits into that formalization that you

00:15:59.710 --> 00:15:59.720
fits into that formalization that you
 

00:15:59.720 --> 00:16:04.329
fits into that formalization that you
form okay in the the Markov decision

00:16:04.329 --> 00:16:04.339
form okay in the the Markov decision
 

00:16:04.339 --> 00:16:06.160
form okay in the the Markov decision
process that's underlying the thinking

00:16:06.160 --> 00:16:06.170
process that's underlying the thinking
 

00:16:06.170 --> 00:16:07.389
process that's underlying the thinking
of reinforcement learning there's a

00:16:07.389 --> 00:16:07.399
of reinforcement learning there's a
 

00:16:07.399 --> 00:16:09.100
of reinforcement learning there's a
state you take can actually receive a

00:16:09.100 --> 00:16:09.110
state you take can actually receive a
 

00:16:09.110 --> 00:16:11.230
state you take can actually receive a
reward and then observe the next state

00:16:11.230 --> 00:16:11.240
reward and then observe the next state
 

00:16:11.240 --> 00:16:13.389
reward and then observe the next state
so it's always state action reward state

00:16:13.389 --> 00:16:13.399
so it's always state action reward state
 

00:16:13.399 --> 00:16:15.699
so it's always state action reward state
that's the sample of data you get as an

00:16:15.699 --> 00:16:15.709
that's the sample of data you get as an
 

00:16:15.709 --> 00:16:17.769
that's the sample of data you get as an
agent acting in the world state action

00:16:17.769 --> 00:16:17.779
agent acting in the world state action
 

00:16:17.779 --> 00:16:21.490
agent acting in the world state action
reward state there's a policy where an

00:16:21.490 --> 00:16:21.500
reward state there's a policy where an
 

00:16:21.500 --> 00:16:25.059
reward state there's a policy where an
agent tries to form a a policy how to

00:16:25.059 --> 00:16:25.069
agent tries to form a a policy how to
 

00:16:25.069 --> 00:16:27.699
agent tries to form a a policy how to
act in the world so that it will never

00:16:27.699 --> 00:16:27.709
act in the world so that it will never
 

00:16:27.709 --> 00:16:32.740
act in the world so that it will never
state it's in it it has a preference to

00:16:32.740 --> 00:16:32.750
state it's in it it has a preference to
 

00:16:32.750 --> 00:16:34.629
state it's in it it has a preference to
act in a certain way in order to

00:16:34.629 --> 00:16:34.639
act in a certain way in order to
 

00:16:34.639 --> 00:16:36.309
act in a certain way in order to
optimize the reward there's a value

00:16:36.309 --> 00:16:36.319
optimize the reward there's a value
 

00:16:36.319 --> 00:16:38.710
optimize the reward there's a value
function that can estimate how good a

00:16:38.710 --> 00:16:38.720
function that can estimate how good a
 

00:16:38.720 --> 00:16:40.629
function that can estimate how good a
certain action is in a certain state and

00:16:40.629 --> 00:16:40.639
certain action is in a certain state and
 

00:16:40.639 --> 00:16:43.179
certain action is in a certain state and
there is sometimes a model that the

00:16:43.179 --> 00:16:43.189
there is sometimes a model that the
 

00:16:43.189 --> 00:16:46.120
there is sometimes a model that the
agent forms about the world a quick

00:16:46.120 --> 00:16:46.130
agent forms about the world a quick
 

00:16:46.130 --> 00:16:48.460
agent forms about the world a quick
example you can have a robot in the room

00:16:48.460 --> 00:16:48.470
example you can have a robot in the room
 

00:16:48.470 --> 00:16:50.530
example you can have a robot in the room
at the bottom left starting moving about

00:16:50.530 --> 00:16:50.540
at the bottom left starting moving about
 

00:16:50.540 --> 00:16:53.259
at the bottom left starting moving about
this room it's a three by four grid it

00:16:53.259 --> 00:16:53.269
this room it's a three by four grid it
 

00:16:53.269 --> 00:16:54.850
this room it's a three by four grid it
tries to get to the top right because

00:16:54.850 --> 00:16:54.860
tries to get to the top right because
 

00:16:54.860 --> 00:16:56.740
tries to get to the top right because
it's a plus one but because it's a

00:16:56.740 --> 00:16:56.750
it's a plus one but because it's a
 

00:16:56.750 --> 00:16:59.230
it's a plus one but because it's a
stochastic system when it goes chooses

00:16:59.230 --> 00:16:59.240
stochastic system when it goes chooses
 

00:16:59.240 --> 00:17:01.210
stochastic system when it goes chooses
to go up it sometimes goes left and

00:17:01.210 --> 00:17:01.220
to go up it sometimes goes left and
 

00:17:01.220 --> 00:17:03.340
to go up it sometimes goes left and
right ten percent of the time so in this

00:17:03.340 --> 00:17:03.350
right ten percent of the time so in this
 

00:17:03.350 --> 00:17:05.710
right ten percent of the time so in this
world it has to try to come up with a

00:17:05.710 --> 00:17:05.720
world it has to try to come up with a
 

00:17:05.720 --> 00:17:07.809
world it has to try to come up with a
policy so what's the policy is this a

00:17:07.809 --> 00:17:07.819
policy so what's the policy is this a
 

00:17:07.819 --> 00:17:10.329
policy so what's the policy is this a
solution so it's starting at the bottom

00:17:10.329 --> 00:17:10.339
solution so it's starting at the bottom
 

00:17:10.339 --> 00:17:12.909
solution so it's starting at the bottom
arrows show the actions whenever you in

00:17:12.909 --> 00:17:12.919
arrows show the actions whenever you in
 

00:17:12.919 --> 00:17:14.199
arrows show the actions whenever you in
that state that you would like to take

00:17:14.199 --> 00:17:14.209
that state that you would like to take
 

00:17:14.209 --> 00:17:16.120
that state that you would like to take
so this is this is a pretty good

00:17:16.120 --> 00:17:16.130
so this is this is a pretty good
 

00:17:16.130 --> 00:17:18.069
so this is this is a pretty good
solution to get to the plus one in a

00:17:18.069 --> 00:17:18.079
solution to get to the plus one in a
 

00:17:18.079 --> 00:17:19.840
solution to get to the plus one in a
deterministic world and it's the cast

00:17:19.840 --> 00:17:19.850
deterministic world and it's the cast
 

00:17:19.850 --> 00:17:22.600
deterministic world and it's the cast
the world when you don't when you go up

00:17:22.600 --> 00:17:22.610
the world when you don't when you go up
 

00:17:22.610 --> 00:17:25.350
the world when you don't when you go up
you don't always go up it's not a

00:17:25.350 --> 00:17:25.360
you don't always go up it's not a
 

00:17:25.360 --> 00:17:27.819
you don't always go up it's not a
optimal policy because you have to have

00:17:27.819 --> 00:17:27.829
optimal policy because you have to have
 

00:17:27.829 --> 00:17:29.830
optimal policy because you have to have
an optimal policy has to have an answer

00:17:29.830 --> 00:17:29.840
an optimal policy has to have an answer
 

00:17:29.840 --> 00:17:31.899
an optimal policy has to have an answer
for every single state you might be in

00:17:31.899 --> 00:17:31.909
for every single state you might be in
 

00:17:31.909 --> 00:17:33.880
for every single state you might be in
so this is the optimum policy would look

00:17:33.880 --> 00:17:33.890
so this is the optimum policy would look
 

00:17:33.890 --> 00:17:37.060
so this is the optimum policy would look
something like this now that's for when

00:17:37.060 --> 00:17:37.070
something like this now that's for when
 

00:17:37.070 --> 00:17:40.120
something like this now that's for when
the the cost the reward is negative

00:17:40.120 --> 00:17:40.130
the the cost the reward is negative
 

00:17:40.130 --> 00:17:44.200
the the cost the reward is negative
point zero one for taking a step now fee

00:17:44.200 --> 00:17:44.210
point zero one for taking a step now fee
 

00:17:44.210 --> 00:17:46.480
point zero one for taking a step now fee
every time I take a step it's really

00:17:46.480 --> 00:17:46.490
every time I take a step it's really
 

00:17:46.490 --> 00:17:47.200
every time I take a step it's really
painful

00:17:47.200 --> 00:17:47.210
painful
 

00:17:47.210 --> 00:17:50.500
painful
it's you get a negative to reward and so

00:17:50.500 --> 00:17:50.510
it's you get a negative to reward and so
 

00:17:50.510 --> 00:17:53.140
it's you get a negative to reward and so
there's a the optimal policy changes

00:17:53.140 --> 00:17:53.150
there's a the optimal policy changes
 

00:17:53.150 --> 00:17:54.970
there's a the optimal policy changes
there's no matter what no matter the

00:17:54.970 --> 00:17:54.980
there's no matter what no matter the
 

00:17:54.980 --> 00:17:57.010
there's no matter what no matter the
statistician a of the system the

00:17:57.010 --> 00:17:57.020
statistician a of the system the
 

00:17:57.020 --> 00:17:59.650
statistician a of the system the
randomness you want to get to the end as

00:17:59.650 --> 00:17:59.660
randomness you want to get to the end as
 

00:17:59.660 --> 00:18:01.240
randomness you want to get to the end as
fast as possible even if you go to

00:18:01.240 --> 00:18:01.250
fast as possible even if you go to
 

00:18:01.250 --> 00:18:03.220
fast as possible even if you go to
negative states you just want to get to

00:18:03.220 --> 00:18:03.230
negative states you just want to get to
 

00:18:03.230 --> 00:18:05.710
negative states you just want to get to
the plus one as quickly as possible and

00:18:05.710 --> 00:18:05.720
the plus one as quickly as possible and
 

00:18:05.720 --> 00:18:08.740
the plus one as quickly as possible and
then so the reward structure changes the

00:18:08.740 --> 00:18:08.750
then so the reward structure changes the
 

00:18:08.750 --> 00:18:10.300
then so the reward structure changes the
optimum policy if you make the reward

00:18:10.300 --> 00:18:10.310
optimum policy if you make the reward
 

00:18:10.310 --> 00:18:12.460
optimum policy if you make the reward
negative point one then there's some

00:18:12.460 --> 00:18:12.470
negative point one then there's some
 

00:18:12.470 --> 00:18:14.500
negative point one then there's some
more incentive to explore and as we

00:18:14.500 --> 00:18:14.510
more incentive to explore and as we
 

00:18:14.510 --> 00:18:17.080
more incentive to explore and as we
increase that reward or decrease the

00:18:17.080 --> 00:18:17.090
increase that reward or decrease the
 

00:18:17.090 --> 00:18:19.540
increase that reward or decrease the
punishment of taking a step more and

00:18:19.540 --> 00:18:19.550
punishment of taking a step more and
 

00:18:19.550 --> 00:18:21.790
punishment of taking a step more and
more exploration is encouraged until we

00:18:21.790 --> 00:18:21.800
more exploration is encouraged until we
 

00:18:21.800 --> 00:18:25.270
more exploration is encouraged until we
get to the kind of what I think of as

00:18:25.270 --> 00:18:25.280
get to the kind of what I think of as
 

00:18:25.280 --> 00:18:27.160
get to the kind of what I think of as
college which is you encourage

00:18:27.160 --> 00:18:27.170
college which is you encourage
 

00:18:27.170 --> 00:18:29.170
college which is you encourage
exploration by having a positive reward

00:18:29.170 --> 00:18:29.180
exploration by having a positive reward
 

00:18:29.180 --> 00:18:30.820
exploration by having a positive reward
to moving around and so you never want

00:18:30.820 --> 00:18:30.830
to moving around and so you never want
 

00:18:30.830 --> 00:18:32.680
to moving around and so you never want
to get to the end you just kind of walk

00:18:32.680 --> 00:18:32.690
to get to the end you just kind of walk
 

00:18:32.690 --> 00:18:35.140
to get to the end you just kind of walk
around the world without ever reaching

00:18:35.140 --> 00:18:35.150
around the world without ever reaching
 

00:18:35.150 --> 00:18:38.920
around the world without ever reaching
the end okay so there's a the main goal

00:18:38.920 --> 00:18:38.930
the end okay so there's a the main goal
 

00:18:38.930 --> 00:18:40.930
the end okay so there's a the main goal
is to really optimize reward in this

00:18:40.930 --> 00:18:40.940
is to really optimize reward in this
 

00:18:40.940 --> 00:18:43.000
is to really optimize reward in this
world and because the reward is

00:18:43.000 --> 00:18:43.010
world and because the reward is
 

00:18:43.010 --> 00:18:44.710
world and because the reward is
collected over time you want to have

00:18:44.710 --> 00:18:44.720
collected over time you want to have
 

00:18:44.720 --> 00:18:47.070
collected over time you want to have
some estimate of future reward and

00:18:47.070 --> 00:18:47.080
some estimate of future reward and
 

00:18:47.080 --> 00:18:48.880
some estimate of future reward and
because you don't have a perfect

00:18:48.880 --> 00:18:48.890
because you don't have a perfect
 

00:18:48.890 --> 00:18:50.800
because you don't have a perfect
estimate of the future you have to

00:18:50.800 --> 00:18:50.810
estimate of the future you have to
 

00:18:50.810 --> 00:18:53.740
estimate of the future you have to
discount that reward over time so the

00:18:53.740 --> 00:18:53.750
discount that reward over time so the
 

00:18:53.750 --> 00:18:56.080
discount that reward over time so the
goal is to maximize the discounted

00:18:56.080 --> 00:18:56.090
goal is to maximize the discounted
 

00:18:56.090 --> 00:18:58.900
goal is to maximize the discounted
reward over time and in q-learning

00:18:58.900 --> 00:18:58.910
reward over time and in q-learning
 

00:18:58.910 --> 00:19:04.060
reward over time and in q-learning
is an approach that that I'd like to

00:19:04.060 --> 00:19:04.070
is an approach that that I'd like to
 

00:19:04.070 --> 00:19:07.210
is an approach that that I'd like to
focus in on today the there's a state

00:19:07.210 --> 00:19:07.220
focus in on today the there's a state
 

00:19:07.220 --> 00:19:10.150
focus in on today the there's a state
action value queue it's a queue function

00:19:10.150 --> 00:19:10.160
action value queue it's a queue function
 

00:19:10.160 --> 00:19:11.380
action value queue it's a queue function
that takes in the state and action and

00:19:11.380 --> 00:19:11.390
that takes in the state and action and
 

00:19:11.390 --> 00:19:13.300
that takes in the state and action and
tells you the value of that state it's

00:19:13.300 --> 00:19:13.310
tells you the value of that state it's
 

00:19:13.310 --> 00:19:16.990
tells you the value of that state it's
off policy because we can learn this

00:19:16.990 --> 00:19:17.000
off policy because we can learn this
 

00:19:17.000 --> 00:19:19.840
off policy because we can learn this
function without forming a optimal

00:19:19.840 --> 00:19:19.850
function without forming a optimal
 

00:19:19.850 --> 00:19:22.180
function without forming a optimal
without keeping an optimal policy an

00:19:22.180 --> 00:19:22.190
without keeping an optimal policy an
 

00:19:22.190 --> 00:19:24.010
without keeping an optimal policy an
estimate of an optimal policy with us

00:19:24.010 --> 00:19:24.020
estimate of an optimal policy with us
 

00:19:24.020 --> 00:19:26.590
estimate of an optimal policy with us
and what it turns out with the equation

00:19:26.590 --> 00:19:26.600
and what it turns out with the equation
 

00:19:26.600 --> 00:19:27.970
and what it turns out with the equation
at the bottom or the bellman equation

00:19:27.970 --> 00:19:27.980
at the bottom or the bellman equation
 

00:19:27.980 --> 00:19:32.230
at the bottom or the bellman equation
you can estimate the you can update your

00:19:32.230 --> 00:19:32.240
you can estimate the you can update your
 

00:19:32.240 --> 00:19:34.840
you can estimate the you can update your
estimate of the Q function in such a way

00:19:34.840 --> 00:19:34.850
estimate of the Q function in such a way
 

00:19:34.850 --> 00:19:37.150
estimate of the Q function in such a way
that over time it converges to an

00:19:37.150 --> 00:19:37.160
that over time it converges to an
 

00:19:37.160 --> 00:19:39.670
that over time it converges to an
optimal policy and the update is simple

00:19:39.670 --> 00:19:39.680
optimal policy and the update is simple
 

00:19:39.680 --> 00:19:41.830
optimal policy and the update is simple
you have an estimate you start knowing

00:19:41.830 --> 00:19:41.840
you have an estimate you start knowing
 

00:19:41.840 --> 00:19:44.890
you have an estimate you start knowing
nothing the there's an old this estimate

00:19:44.890 --> 00:19:44.900
nothing the there's an old this estimate
 

00:19:44.900 --> 00:19:47.740
nothing the there's an old this estimate
of an old state Q SAT and then you take

00:19:47.740 --> 00:19:47.750
of an old state Q SAT and then you take
 

00:19:47.750 --> 00:19:49.960
of an old state Q SAT and then you take
an action you collect a reward and you

00:19:49.960 --> 00:19:49.970
an action you collect a reward and you
 

00:19:49.970 --> 00:19:51.670
an action you collect a reward and you
update your estimate based on the

00:19:51.670 --> 00:19:51.680
update your estimate based on the
 

00:19:51.680 --> 00:19:53.230
update your estimate based on the
awardee received and the difference

00:19:53.230 --> 00:19:53.240
awardee received and the difference
 

00:19:53.240 --> 00:19:54.940
awardee received and the difference
between what you've expected and what

00:19:54.940 --> 00:19:54.950
between what you've expected and what
 

00:19:54.950 --> 00:19:56.620
between what you've expected and what
you actually received and that's the

00:19:56.620 --> 00:19:56.630
you actually received and that's the
 

00:19:56.630 --> 00:19:58.330
you actually received and that's the
update usually walk around this world

00:19:58.330 --> 00:19:58.340
update usually walk around this world
 

00:19:58.340 --> 00:20:00.669
update usually walk around this world
exploring until you form better but

00:20:00.669 --> 00:20:00.679
exploring until you form better but
 

00:20:00.679 --> 00:20:02.470
exploring until you form better but
understanding of what is the good action

00:20:02.470 --> 00:20:02.480
understanding of what is the good action
 

00:20:02.480 --> 00:20:03.999
understanding of what is the good action
to take in each individual state and

00:20:03.999 --> 00:20:04.009
to take in each individual state and
 

00:20:04.009 --> 00:20:06.430
to take in each individual state and
there's always as in life and in

00:20:06.430 --> 00:20:06.440
there's always as in life and in
 

00:20:06.440 --> 00:20:08.109
there's always as in life and in
reinforcement learning in any agents

00:20:08.109 --> 00:20:08.119
reinforcement learning in any agents
 

00:20:08.119 --> 00:20:10.119
reinforcement learning in any agents
that act if there's a learning stage

00:20:10.119 --> 00:20:10.129
that act if there's a learning stage
 

00:20:10.129 --> 00:20:12.399
that act if there's a learning stage
where you have to explore exploring pays

00:20:12.399 --> 00:20:12.409
where you have to explore exploring pays
 

00:20:12.409 --> 00:20:14.889
where you have to explore exploring pays
off when you know very little the more

00:20:14.889 --> 00:20:14.899
off when you know very little the more
 

00:20:14.899 --> 00:20:17.049
off when you know very little the more
and more you learn the less and less

00:20:17.049 --> 00:20:17.059
and more you learn the less and less
 

00:20:17.059 --> 00:20:18.999
and more you learn the less and less
valuable it is to explore and you want

00:20:18.999 --> 00:20:19.009
valuable it is to explore and you want
 

00:20:19.009 --> 00:20:20.710
valuable it is to explore and you want
to exploit you want to take greedy

00:20:20.710 --> 00:20:20.720
to exploit you want to take greedy
 

00:20:20.720 --> 00:20:22.539
to exploit you want to take greedy
actions and that's always the balance

00:20:22.539 --> 00:20:22.549
actions and that's always the balance
 

00:20:22.549 --> 00:20:24.700
actions and that's always the balance
you start exploring at first but

00:20:24.700 --> 00:20:24.710
you start exploring at first but
 

00:20:24.710 --> 00:20:26.169
you start exploring at first but
eventually you want to make some money

00:20:26.169 --> 00:20:26.179
eventually you want to make some money
 

00:20:26.179 --> 00:20:28.509
eventually you want to make some money
whatever the the the metric of success

00:20:28.509 --> 00:20:28.519
whatever the the the metric of success
 

00:20:28.519 --> 00:20:30.909
whatever the the the metric of success
is and you want to focus on a policy

00:20:30.909 --> 00:20:30.919
is and you want to focus on a policy
 

00:20:30.919 --> 00:20:32.830
is and you want to focus on a policy
that you've converged towards that is

00:20:32.830 --> 00:20:32.840
that you've converged towards that is
 

00:20:32.840 --> 00:20:35.859
that you've converged towards that is
pretty good a near optimal policy in

00:20:35.859 --> 00:20:35.869
pretty good a near optimal policy in
 

00:20:35.869 --> 00:20:39.730
pretty good a near optimal policy in
order to act in a greedy way as you move

00:20:39.730 --> 00:20:39.740
order to act in a greedy way as you move
 

00:20:39.740 --> 00:20:42.039
order to act in a greedy way as you move
around with this bellman equation move

00:20:42.039 --> 00:20:42.049
around with this bellman equation move
 

00:20:42.049 --> 00:20:43.749
around with this bellman equation move
around the world taking different states

00:20:43.749 --> 00:20:43.759
around the world taking different states
 

00:20:43.759 --> 00:20:45.970
around the world taking different states
different actions you can update a you

00:20:45.970 --> 00:20:45.980
different actions you can update a you
 

00:20:45.980 --> 00:20:48.190
different actions you can update a you
can think of it as a cue table and you

00:20:48.190 --> 00:20:48.200
can think of it as a cue table and you
 

00:20:48.200 --> 00:20:50.710
can think of it as a cue table and you
can update the quality of a taking a

00:20:50.710 --> 00:20:50.720
can update the quality of a taking a
 

00:20:50.720 --> 00:20:52.840
can update the quality of a taking a
certain action in a certain state so

00:20:52.840 --> 00:20:52.850
certain action in a certain state so
 

00:20:52.850 --> 00:20:56.769
certain action in a certain state so
that's a that's a that's a picture of a

00:20:56.769 --> 00:20:56.779
that's a that's a that's a picture of a
 

00:20:56.779 --> 00:20:58.899
that's a that's a that's a picture of a
table there or in this the world with

00:20:58.899 --> 00:20:58.909
table there or in this the world with
 

00:20:58.909 --> 00:21:00.879
table there or in this the world with
four states and four actions and you can

00:21:00.879 --> 00:21:00.889
four states and four actions and you can
 

00:21:00.889 --> 00:21:02.200
four states and four actions and you can
move around using the bellman equation

00:21:02.200 --> 00:21:02.210
move around using the bellman equation
 

00:21:02.210 --> 00:21:04.690
move around using the bellman equation
updating the value of being in that

00:21:04.690 --> 00:21:04.700
updating the value of being in that
 

00:21:04.700 --> 00:21:07.060
updating the value of being in that
state the problem is when this cute

00:21:07.060 --> 00:21:07.070
state the problem is when this cute
 

00:21:07.070 --> 00:21:09.220
state the problem is when this cute
table grows exponentially in order to

00:21:09.220 --> 00:21:09.230
table grows exponentially in order to
 

00:21:09.230 --> 00:21:11.619
table grows exponentially in order to
represent raw sensory data like we

00:21:11.619 --> 00:21:11.629
represent raw sensory data like we
 

00:21:11.629 --> 00:21:14.109
represent raw sensory data like we
humans have when taking in vision or if

00:21:14.109 --> 00:21:14.119
humans have when taking in vision or if
 

00:21:14.119 --> 00:21:16.090
humans have when taking in vision or if
you take in the raw pixels of an arcade

00:21:16.090 --> 00:21:16.100
you take in the raw pixels of an arcade
 

00:21:16.100 --> 00:21:18.759
you take in the raw pixels of an arcade
game that's the number of pixels that

00:21:18.759 --> 00:21:18.769
game that's the number of pixels that
 

00:21:18.769 --> 00:21:22.600
game that's the number of pixels that
are there get is larger than is it is

00:21:22.600 --> 00:21:22.610
are there get is larger than is it is
 

00:21:22.610 --> 00:21:25.090
are there get is larger than is it is
larger than can be stored in memory is

00:21:25.090 --> 00:21:25.100
larger than can be stored in memory is
 

00:21:25.100 --> 00:21:26.710
larger than can be stored in memory is
larger that can be explored the

00:21:26.710 --> 00:21:26.720
larger that can be explored the
 

00:21:26.720 --> 00:21:29.019
larger that can be explored the
simulation it's exceptionally large and

00:21:29.019 --> 00:21:29.029
simulation it's exceptionally large and
 

00:21:29.029 --> 00:21:31.720
simulation it's exceptionally large and
if you know anything about exceptionally

00:21:31.720 --> 00:21:31.730
if you know anything about exceptionally
 

00:21:31.730 --> 00:21:34.960
if you know anything about exceptionally
large high dimensional spaces and

00:21:34.960 --> 00:21:34.970
large high dimensional spaces and
 

00:21:34.970 --> 00:21:36.700
large high dimensional spaces and
learning anything about them that's what

00:21:36.700 --> 00:21:36.710
learning anything about them that's what
 

00:21:36.710 --> 00:21:39.070
learning anything about them that's what
deep neural networks are good at forming

00:21:39.070 --> 00:21:39.080
deep neural networks are good at forming
 

00:21:39.080 --> 00:21:41.230
deep neural networks are good at forming
the approximator is forming some kind of

00:21:41.230 --> 00:21:41.240
the approximator is forming some kind of
 

00:21:41.240 --> 00:21:42.940
the approximator is forming some kind of
representation an exceptionally high

00:21:42.940 --> 00:21:42.950
representation an exceptionally high
 

00:21:42.950 --> 00:21:46.389
representation an exceptionally high
dimensional complex space so that's the

00:21:46.389 --> 00:21:46.399
dimensional complex space so that's the
 

00:21:46.399 --> 00:21:48.999
dimensional complex space so that's the
hope for deeper enforcement learning is

00:21:48.999 --> 00:21:49.009
hope for deeper enforcement learning is
 

00:21:49.009 --> 00:21:50.529
hope for deeper enforcement learning is
you take these reinforcement learning

00:21:50.529 --> 00:21:50.539
you take these reinforcement learning
 

00:21:50.539 --> 00:21:52.810
you take these reinforcement learning
ideas where an agent acts in the world

00:21:52.810 --> 00:21:52.820
ideas where an agent acts in the world
 

00:21:52.820 --> 00:21:54.369
ideas where an agent acts in the world
to learn something about that world and

00:21:54.369 --> 00:21:54.379
to learn something about that world and
 

00:21:54.379 --> 00:21:56.070
to learn something about that world and
we use a neural network as the

00:21:56.070 --> 00:21:56.080
we use a neural network as the
 

00:21:56.080 --> 00:21:58.810
we use a neural network as the
approximator as the thing that the agent

00:21:58.810 --> 00:21:58.820
approximator as the thing that the agent
 

00:21:58.820 --> 00:22:00.789
approximator as the thing that the agent
uses in order to approximate the quality

00:22:00.789 --> 00:22:00.799
uses in order to approximate the quality
 

00:22:00.799 --> 00:22:03.159
uses in order to approximate the quality
either approximate the policy or

00:22:03.159 --> 00:22:03.169
either approximate the policy or
 

00:22:03.169 --> 00:22:04.450
either approximate the policy or
approximate the quality of taking a

00:22:04.450 --> 00:22:04.460
approximate the quality of taking a
 

00:22:04.460 --> 00:22:06.310
approximate the quality of taking a
certain action in certain state and and

00:22:06.310 --> 00:22:06.320
certain action in certain state and and
 

00:22:06.320 --> 00:22:08.259
certain action in certain state and and
therefore making sense of this raw

00:22:08.259 --> 00:22:08.269
therefore making sense of this raw
 

00:22:08.269 --> 00:22:11.470
therefore making sense of this raw
information forming higher and higher

00:22:11.470 --> 00:22:11.480
information forming higher and higher
 

00:22:11.480 --> 00:22:13.779
information forming higher and higher
order representations of the raw sensory

00:22:13.779 --> 00:22:13.789
order representations of the raw sensory
 

00:22:13.789 --> 00:22:14.200
order representations of the raw sensory
and for me

00:22:14.200 --> 00:22:14.210
and for me
 

00:22:14.210 --> 00:22:17.169
and for me
in order to then as an output take an

00:22:17.169 --> 00:22:17.179
in order to then as an output take an
 

00:22:17.179 --> 00:22:21.340
in order to then as an output take an
action so the neural network is injected

00:22:21.340 --> 00:22:21.350
action so the neural network is injected
 

00:22:21.350 --> 00:22:23.470
action so the neural network is injected
as a function approximator into the

00:22:23.470 --> 00:22:23.480
as a function approximator into the
 

00:22:23.480 --> 00:22:25.419
as a function approximator into the
queue it's it's the cue function is

00:22:25.419 --> 00:22:25.429
queue it's it's the cue function is
 

00:22:25.429 --> 00:22:26.619
queue it's it's the cue function is
approximated with a neural network

00:22:26.619 --> 00:22:26.629
approximated with a neural network
 

00:22:26.629 --> 00:22:30.100
approximated with a neural network
that's DQ n that's deep Q learning so

00:22:30.100 --> 00:22:30.110
that's DQ n that's deep Q learning so
 

00:22:30.110 --> 00:22:32.529
that's DQ n that's deep Q learning so
injecting into the Q learning framework

00:22:32.529 --> 00:22:32.539
injecting into the Q learning framework
 

00:22:32.539 --> 00:22:34.600
injecting into the Q learning framework
and your own network that's what's been

00:22:34.600 --> 00:22:34.610
and your own network that's what's been
 

00:22:34.610 --> 00:22:36.100
and your own network that's what's been
the success for deep mind with the

00:22:36.100 --> 00:22:36.110
the success for deep mind with the
 

00:22:36.110 --> 00:22:38.019
the success for deep mind with the
playing the Atari games having this

00:22:38.019 --> 00:22:38.029
playing the Atari games having this
 

00:22:38.029 --> 00:22:39.639
playing the Atari games having this
neural network takes in the raw pixels

00:22:39.639 --> 00:22:39.649
neural network takes in the raw pixels
 

00:22:39.649 --> 00:22:41.980
neural network takes in the raw pixels
of the Atari game and produces actions

00:22:41.980 --> 00:22:41.990
of the Atari game and produces actions
 

00:22:41.990 --> 00:22:44.470
of the Atari game and produces actions
or values of each individual actions and

00:22:44.470 --> 00:22:44.480
or values of each individual actions and
 

00:22:44.480 --> 00:22:46.330
or values of each individual actions and
then in a greedy way picking the best

00:22:46.330 --> 00:22:46.340
then in a greedy way picking the best
 

00:22:46.340 --> 00:22:49.480
then in a greedy way picking the best
action and the learning the loss

00:22:49.480 --> 00:22:49.490
action and the learning the loss
 

00:22:49.490 --> 00:22:53.379
action and the learning the loss
function for these networks is twofold

00:22:53.379 --> 00:22:53.389
function for these networks is twofold
 

00:22:53.389 --> 00:22:55.450
function for these networks is twofold
so you have a you have a Q function an

00:22:55.450 --> 00:22:55.460
so you have a you have a Q function an
 

00:22:55.460 --> 00:22:56.950
so you have a you have a Q function an
estimate of taking a certain action

00:22:56.950 --> 00:22:56.960
estimate of taking a certain action
 

00:22:56.960 --> 00:22:59.109
estimate of taking a certain action
certain state you take that action and

00:22:59.109 --> 00:22:59.119
certain state you take that action and
 

00:22:59.119 --> 00:23:01.840
certain state you take that action and
then you observe how it's the actual

00:23:01.840 --> 00:23:01.850
then you observe how it's the actual
 

00:23:01.850 --> 00:23:03.820
then you observe how it's the actual
reward received is different so you have

00:23:03.820 --> 00:23:03.830
reward received is different so you have
 

00:23:03.830 --> 00:23:05.950
reward received is different so you have
a target you have a prediction and the

00:23:05.950 --> 00:23:05.960
a target you have a prediction and the
 

00:23:05.960 --> 00:23:08.710
a target you have a prediction and the
loss is the squared error between those

00:23:08.710 --> 00:23:08.720
loss is the squared error between those
 

00:23:08.720 --> 00:23:12.190
loss is the squared error between those
two and DQ n has uses the same network

00:23:12.190 --> 00:23:12.200
two and DQ n has uses the same network
 

00:23:12.200 --> 00:23:17.139
two and DQ n has uses the same network
the traditional DQ n the first ya DQ and

00:23:17.139 --> 00:23:17.149
the traditional DQ n the first ya DQ and
 

00:23:17.149 --> 00:23:19.509
the traditional DQ n the first ya DQ and
uses one network to estimate both Q's in

00:23:19.509 --> 00:23:19.519
uses one network to estimate both Q's in
 

00:23:19.519 --> 00:23:23.649
uses one network to estimate both Q's in
that loss function a double D Q and d DQ

00:23:23.649 --> 00:23:23.659
that loss function a double D Q and d DQ
 

00:23:23.659 --> 00:23:26.529
that loss function a double D Q and d DQ
n uses a separate network for each one

00:23:26.529 --> 00:23:26.539
n uses a separate network for each one
 

00:23:26.539 --> 00:23:31.259
n uses a separate network for each one
of those a few tricks their key

00:23:31.259 --> 00:23:31.269
of those a few tricks their key
 

00:23:31.269 --> 00:23:35.769
of those a few tricks their key
experience replay so it tricks in

00:23:35.769 --> 00:23:35.779
experience replay so it tricks in
 

00:23:35.779 --> 00:23:38.049
experience replay so it tricks in
reinforcement learning because it's the

00:23:38.049 --> 00:23:38.059
reinforcement learning because it's the
 

00:23:38.059 --> 00:23:42.009
reinforcement learning because it's the
fact that it works is incredible and as

00:23:42.009 --> 00:23:42.019
fact that it works is incredible and as
 

00:23:42.019 --> 00:23:43.840
fact that it works is incredible and as
a fundamental sort of philosophical idea

00:23:43.840 --> 00:23:43.850
a fundamental sort of philosophical idea
 

00:23:43.850 --> 00:23:45.789
a fundamental sort of philosophical idea
knowing so little and being able to make

00:23:45.789 --> 00:23:45.799
knowing so little and being able to make
 

00:23:45.799 --> 00:23:48.549
knowing so little and being able to make
sense with from such a high dimensional

00:23:48.549 --> 00:23:48.559
sense with from such a high dimensional
 

00:23:48.559 --> 00:23:51.249
sense with from such a high dimensional
space is amazing but actually these

00:23:51.249 --> 00:23:51.259
space is amazing but actually these
 

00:23:51.259 --> 00:23:53.320
space is amazing but actually these
ideas have been around for quite a long

00:23:53.320 --> 00:23:53.330
ideas have been around for quite a long
 

00:23:53.330 --> 00:23:56.409
ideas have been around for quite a long
time and a few key tricks is what made

00:23:56.409 --> 00:23:56.419
time and a few key tricks is what made
 

00:23:56.419 --> 00:23:58.720
time and a few key tricks is what made
them really work so in the first I think

00:23:58.720 --> 00:23:58.730
them really work so in the first I think
 

00:23:58.730 --> 00:24:00.310
them really work so in the first I think
the two things for D queuing is

00:24:00.310 --> 00:24:00.320
the two things for D queuing is
 

00:24:00.320 --> 00:24:03.609
the two things for D queuing is
experience replay is instead of letting

00:24:03.609 --> 00:24:03.619
experience replay is instead of letting
 

00:24:03.619 --> 00:24:07.210
experience replay is instead of letting
the agent learn for as it as it acts in

00:24:07.210 --> 00:24:07.220
the agent learn for as it as it acts in
 

00:24:07.220 --> 00:24:09.940
the agent learn for as it as it acts in
the world agent is acting in the world

00:24:09.940 --> 00:24:09.950
the world agent is acting in the world
 

00:24:09.950 --> 00:24:14.109
the world agent is acting in the world
and collecting experiences that can be

00:24:14.109 --> 00:24:14.119
and collecting experiences that can be
 

00:24:14.119 --> 00:24:15.609
and collecting experiences that can be
replayed through the learning so the

00:24:15.609 --> 00:24:15.619
replayed through the learning so the
 

00:24:15.619 --> 00:24:17.440
replayed through the learning so the
learning process jumps around through

00:24:17.440 --> 00:24:17.450
learning process jumps around through
 

00:24:17.450 --> 00:24:19.600
learning process jumps around through
memory through the experiences and

00:24:19.600 --> 00:24:19.610
memory through the experiences and
 

00:24:19.610 --> 00:24:23.109
memory through the experiences and
instead so it doesn't it doesn't learn

00:24:23.109 --> 00:24:23.119
instead so it doesn't it doesn't learn
 

00:24:23.119 --> 00:24:26.259
instead so it doesn't it doesn't learn
on the local evolution of a particular

00:24:26.259 --> 00:24:26.269
on the local evolution of a particular
 

00:24:26.269 --> 00:24:27.060
on the local evolution of a particular
simulation

00:24:27.060 --> 00:24:27.070
simulation
 

00:24:27.070 --> 00:24:28.740
simulation
that learn of the entirety of its

00:24:28.740 --> 00:24:28.750
that learn of the entirety of its
 

00:24:28.750 --> 00:24:31.350
that learn of the entirety of its
experiences then the fixed target

00:24:31.350 --> 00:24:31.360
experiences then the fixed target
 

00:24:31.360 --> 00:24:34.110
experiences then the fixed target
network as I mentioned with GG QN the

00:24:34.110 --> 00:24:34.120
network as I mentioned with GG QN the
 

00:24:34.120 --> 00:24:38.340
network as I mentioned with GG QN the
fact that the loss function includes if

00:24:38.340 --> 00:24:38.350
fact that the loss function includes if
 

00:24:38.350 --> 00:24:42.240
fact that the loss function includes if
you notice sort of two two forward

00:24:42.240 --> 00:24:42.250
you notice sort of two two forward
 

00:24:42.250 --> 00:24:44.220
you notice sort of two two forward
passes through the neural network and so

00:24:44.220 --> 00:24:44.230
passes through the neural network and so
 

00:24:44.230 --> 00:24:46.529
passes through the neural network and so
because when you know very little in the

00:24:46.529 --> 00:24:46.539
because when you know very little in the
 

00:24:46.539 --> 00:24:48.659
because when you know very little in the
beginning it's a very unstable system

00:24:48.659 --> 00:24:48.669
beginning it's a very unstable system
 

00:24:48.669 --> 00:24:51.090
beginning it's a very unstable system
and bias can I can have a significant

00:24:51.090 --> 00:24:51.100
and bias can I can have a significant
 

00:24:51.100 --> 00:24:53.129
and bias can I can have a significant
negative effect so there's some benefit

00:24:53.129 --> 00:24:53.139
negative effect so there's some benefit
 

00:24:53.139 --> 00:24:56.970
negative effect so there's some benefit
in for the target the forward pass the

00:24:56.970 --> 00:24:56.980
in for the target the forward pass the
 

00:24:56.980 --> 00:24:58.379
in for the target the forward pass the
neural network takes for the target

00:24:58.379 --> 00:24:58.389
neural network takes for the target
 

00:24:58.389 --> 00:25:02.070
neural network takes for the target
function to to be fixed and only be

00:25:02.070 --> 00:25:02.080
function to to be fixed and only be
 

00:25:02.080 --> 00:25:03.779
function to to be fixed and only be
updated then you'll now work there to be

00:25:03.779 --> 00:25:03.789
updated then you'll now work there to be
 

00:25:03.789 --> 00:25:06.180
updated then you'll now work there to be
only updated every thousand one hundred

00:25:06.180 --> 00:25:06.190
only updated every thousand one hundred
 

00:25:06.190 --> 00:25:10.259
only updated every thousand one hundred
steps and there is a few other tricks

00:25:10.259 --> 00:25:10.269
steps and there is a few other tricks
 

00:25:10.269 --> 00:25:12.600
steps and there is a few other tricks
the slides are available online there's

00:25:12.600 --> 00:25:12.610
the slides are available online there's
 

00:25:12.610 --> 00:25:15.240
the slides are available online there's
a few interesting bits throughout these

00:25:15.240 --> 00:25:15.250
a few interesting bits throughout these
 

00:25:15.250 --> 00:25:17.909
a few interesting bits throughout these
slides please check them out there's a

00:25:17.909 --> 00:25:17.919
slides please check them out there's a
 

00:25:17.919 --> 00:25:20.399
slides please check them out there's a
lot of interesting results here on this

00:25:20.399 --> 00:25:20.409
lot of interesting results here on this
 

00:25:20.409 --> 00:25:24.389
lot of interesting results here on this
slide and showing the benefit that you

00:25:24.389 --> 00:25:24.399
slide and showing the benefit that you
 

00:25:24.399 --> 00:25:26.039
slide and showing the benefit that you
get from these tricks so replay

00:25:26.039 --> 00:25:26.049
get from these tricks so replay
 

00:25:26.049 --> 00:25:28.499
get from these tricks so replay
experience replay and fixed target

00:25:28.499 --> 00:25:28.509
experience replay and fixed target
 

00:25:28.509 --> 00:25:30.509
experience replay and fixed target
network are the biggest that's the magic

00:25:30.509 --> 00:25:30.519
network are the biggest that's the magic
 

00:25:30.519 --> 00:25:31.799
network are the biggest that's the magic
that's the thing that made it work for

00:25:31.799 --> 00:25:31.809
that's the thing that made it work for
 

00:25:31.809 --> 00:25:37.649
that's the thing that made it work for
the atari games and the result was

00:25:37.649 --> 00:25:37.659
the atari games and the result was
 

00:25:37.659 --> 00:25:40.999
the atari games and the result was
achieving with deepmind achieving super

00:25:40.999 --> 00:25:41.009
achieving with deepmind achieving super
 

00:25:41.009 --> 00:25:43.409
achieving with deepmind achieving super
above human level performance on these

00:25:43.409 --> 00:25:43.419
above human level performance on these
 

00:25:43.419 --> 00:25:47.490
above human level performance on these
atari games now what's it's been very

00:25:47.490 --> 00:25:47.500
atari games now what's it's been very
 

00:25:47.500 --> 00:25:50.009
atari games now what's it's been very
successful use now with alphago and the

00:25:50.009 --> 00:25:50.019
successful use now with alphago and the
 

00:25:50.019 --> 00:25:52.440
successful use now with alphago and the
other more complex systems is policy

00:25:52.440 --> 00:25:52.450
other more complex systems is policy
 

00:25:52.450 --> 00:25:54.749
other more complex systems is policy
gradients which is a slight variation on

00:25:54.749 --> 00:25:54.759
gradients which is a slight variation on
 

00:25:54.759 --> 00:25:56.789
gradients which is a slight variation on
this idea of applying your own networks

00:25:56.789 --> 00:25:56.799
this idea of applying your own networks
 

00:25:56.799 --> 00:25:57.960
this idea of applying your own networks
in this deep reinforcement learning

00:25:57.960 --> 00:25:57.970
in this deep reinforcement learning
 

00:25:57.970 --> 00:26:02.039
in this deep reinforcement learning
space so DQ one is Q learning you'll

00:26:02.039 --> 00:26:02.049
space so DQ one is Q learning you'll
 

00:26:02.049 --> 00:26:03.330
space so DQ one is Q learning you'll
know what can the Q learning framework

00:26:03.330 --> 00:26:03.340
know what can the Q learning framework
 

00:26:03.340 --> 00:26:06.749
know what can the Q learning framework
it's off policy so it's approximating Q

00:26:06.749 --> 00:26:06.759
it's off policy so it's approximating Q
 

00:26:06.759 --> 00:26:09.779
it's off policy so it's approximating Q
and infer the optimal policy policy

00:26:09.779 --> 00:26:09.789
and infer the optimal policy policy
 

00:26:09.789 --> 00:26:12.570
and infer the optimal policy policy
gradients PG is on policy is directly

00:26:12.570 --> 00:26:12.580
gradients PG is on policy is directly
 

00:26:12.580 --> 00:26:14.700
gradients PG is on policy is directly
optimizing the policy space so the

00:26:14.700 --> 00:26:14.710
optimizing the policy space so the
 

00:26:14.710 --> 00:26:16.619
optimizing the policy space so the
neural network is estimating the

00:26:16.619 --> 00:26:16.629
neural network is estimating the
 

00:26:16.629 --> 00:26:18.600
neural network is estimating the
probability of taking a certain action

00:26:18.600 --> 00:26:18.610
probability of taking a certain action
 

00:26:18.610 --> 00:26:20.820
probability of taking a certain action
and the learning and there's a great if

00:26:20.820 --> 00:26:20.830
and the learning and there's a great if
 

00:26:20.830 --> 00:26:22.560
and the learning and there's a great if
you want the details of this from Andre

00:26:22.560 --> 00:26:22.570
you want the details of this from Andre
 

00:26:22.570 --> 00:26:24.590
you want the details of this from Andre
Botha is a great post explaining

00:26:24.590 --> 00:26:24.600
Botha is a great post explaining
 

00:26:24.600 --> 00:26:26.430
Botha is a great post explaining
illustrated in an illustrative way

00:26:26.430 --> 00:26:26.440
illustrated in an illustrative way
 

00:26:26.440 --> 00:26:28.799
illustrated in an illustrative way
deeper enforcement learning by looking

00:26:28.799 --> 00:26:28.809
deeper enforcement learning by looking
 

00:26:28.809 --> 00:26:31.889
deeper enforcement learning by looking
at pong playing pong so the training

00:26:31.889 --> 00:26:31.899
at pong playing pong so the training
 

00:26:31.899 --> 00:26:33.810
at pong playing pong so the training
process there is you look at the

00:26:33.810 --> 00:26:33.820
process there is you look at the
 

00:26:33.820 --> 00:26:35.220
process there is you look at the
evolution of the different games and

00:26:35.220 --> 00:26:35.230
evolution of the different games and
 

00:26:35.230 --> 00:26:37.980
evolution of the different games and
then reinforced also knows actor critic

00:26:37.980 --> 00:26:37.990
then reinforced also knows actor critic
 

00:26:37.990 --> 00:26:40.470
then reinforced also knows actor critic
you take the policy gradient

00:26:40.470 --> 00:26:40.480
you take the policy gradient
 

00:26:40.480 --> 00:26:42.870
you take the policy gradient
increases the probability of good action

00:26:42.870 --> 00:26:42.880
increases the probability of good action
 

00:26:42.880 --> 00:26:44.550
increases the probability of good action
and decrease the probability of bad

00:26:44.550 --> 00:26:44.560
and decrease the probability of bad
 

00:26:44.560 --> 00:26:46.500
and decrease the probability of bad
action so the policy network is the

00:26:46.500 --> 00:26:46.510
action so the policy network is the
 

00:26:46.510 --> 00:26:48.630
action so the policy network is the
actor so the neural network is the thing

00:26:48.630 --> 00:26:48.640
actor so the neural network is the thing
 

00:26:48.640 --> 00:26:50.550
actor so the neural network is the thing
that takes in the raw pixels usually a

00:26:50.550 --> 00:26:50.560
that takes in the raw pixels usually a
 

00:26:50.560 --> 00:26:52.950
that takes in the raw pixels usually a
sequence of frames and outputs a

00:26:52.950 --> 00:26:52.960
sequence of frames and outputs a
 

00:26:52.960 --> 00:26:54.510
sequence of frames and outputs a
probability of taking a certain action

00:26:54.510 --> 00:26:54.520
probability of taking a certain action
 

00:26:54.520 --> 00:26:58.080
probability of taking a certain action
so you want to reward actions that have

00:26:58.080 --> 00:26:58.090
so you want to reward actions that have
 

00:26:58.090 --> 00:27:00.630
so you want to reward actions that have
eventually led to a winning a high

00:27:00.630 --> 00:27:00.640
eventually led to a winning a high
 

00:27:00.640 --> 00:27:03.630
eventually led to a winning a high
reward and you want to punish the you

00:27:03.630 --> 00:27:03.640
reward and you want to punish the you
 

00:27:03.640 --> 00:27:05.490
reward and you want to punish the you
want to decrease you note of negative

00:27:05.490 --> 00:27:05.500
want to decrease you note of negative
 

00:27:05.500 --> 00:27:09.480
want to decrease you note of negative
gradient for actions that that led to a

00:27:09.480 --> 00:27:09.490
gradient for actions that that led to a
 

00:27:09.490 --> 00:27:12.570
gradient for actions that that led to a
negative reward so the the reward there

00:27:12.570 --> 00:27:12.580
negative reward so the the reward there
 

00:27:12.580 --> 00:27:14.280
negative reward so the the reward there
is the critic the policy network is the

00:27:14.280 --> 00:27:14.290
is the critic the policy network is the
 

00:27:14.290 --> 00:27:19.040
is the critic the policy network is the
actor the pros and cons of DQ on of

00:27:19.040 --> 00:27:19.050
actor the pros and cons of DQ on of
 

00:27:19.050 --> 00:27:22.470
actor the pros and cons of DQ on of
policy gradients was DQ n most folks now

00:27:22.470 --> 00:27:22.480
policy gradients was DQ n most folks now
 

00:27:22.480 --> 00:27:24.480
policy gradients was DQ n most folks now
the success comes from policy gradients

00:27:24.480 --> 00:27:24.490
the success comes from policy gradients
 

00:27:24.490 --> 00:27:25.920
the success comes from policy gradients
active critic methods different

00:27:25.920 --> 00:27:25.930
active critic methods different
 

00:27:25.930 --> 00:27:28.140
active critic methods different
variations of it the the pros are its

00:27:28.140 --> 00:27:28.150
variations of it the the pros are its
 

00:27:28.150 --> 00:27:29.820
variations of it the the pros are its
able to deal with more complex Q

00:27:29.820 --> 00:27:29.830
able to deal with more complex Q
 

00:27:29.830 --> 00:27:32.130
able to deal with more complex Q
function as faster convergence in most

00:27:32.130 --> 00:27:32.140
function as faster convergence in most
 

00:27:32.140 --> 00:27:36.270
function as faster convergence in most
in most cases given given you have

00:27:36.270 --> 00:27:36.280
in most cases given given you have
 

00:27:36.280 --> 00:27:38.730
in most cases given given you have
enough data that's the big con is its

00:27:38.730 --> 00:27:38.740
enough data that's the big con is its
 

00:27:38.740 --> 00:27:41.010
enough data that's the big con is its
needs a lot of data and use a lot

00:27:41.010 --> 00:27:41.020
needs a lot of data and use a lot
 

00:27:41.020 --> 00:27:43.260
needs a lot of data and use a lot
ability to simulate huge huge amounts of

00:27:43.260 --> 00:27:43.270
ability to simulate huge huge amounts of
 

00:27:43.270 --> 00:27:47.100
ability to simulate huge huge amounts of
evolutions of the system and because the

00:27:47.100 --> 00:27:47.110
evolutions of the system and because the
 

00:27:47.110 --> 00:27:48.870
evolutions of the system and because the
model probabilities the the policy of

00:27:48.870 --> 00:27:48.880
model probabilities the the policy of
 

00:27:48.880 --> 00:27:50.100
model probabilities the the policy of
grading smaller the probabilities of

00:27:50.100 --> 00:27:50.110
grading smaller the probabilities of
 

00:27:50.110 --> 00:27:53.310
grading smaller the probabilities of
action they're able to learn stochastic

00:27:53.310 --> 00:27:53.320
action they're able to learn stochastic
 

00:27:53.320 --> 00:27:56.910
action they're able to learn stochastic
policies and DQ lung cannot and that's

00:27:56.910 --> 00:27:56.920
policies and DQ lung cannot and that's
 

00:27:56.920 --> 00:28:00.570
policies and DQ lung cannot and that's
where the game of Go has received a lot

00:28:00.570 --> 00:28:00.580
where the game of Go has received a lot
 

00:28:00.580 --> 00:28:02.010
where the game of Go has received a lot
of successes with the application of

00:28:02.010 --> 00:28:02.020
of successes with the application of
 

00:28:02.020 --> 00:28:04.770
of successes with the application of
policy gradients where at first

00:28:04.770 --> 00:28:04.780
policy gradients where at first
 

00:28:04.780 --> 00:28:07.410
policy gradients where at first
alphago 2016 beat the top humans in the

00:28:07.410 --> 00:28:07.420
alphago 2016 beat the top humans in the
 

00:28:07.420 --> 00:28:09.840
alphago 2016 beat the top humans in the
world at the game of go by training an

00:28:09.840 --> 00:28:09.850
world at the game of go by training an
 

00:28:09.850 --> 00:28:13.560
world at the game of go by training an
expert games so in a supervised way

00:28:13.560 --> 00:28:13.570
expert games so in a supervised way
 

00:28:13.570 --> 00:28:16.020
expert games so in a supervised way
starting from training on those human

00:28:16.020 --> 00:28:16.030
starting from training on those human
 

00:28:16.030 --> 00:28:20.310
starting from training on those human
expert positions and alphago 0 in 2017

00:28:20.310 --> 00:28:20.320
expert positions and alphago 0 in 2017
 

00:28:20.320 --> 00:28:23.250
expert positions and alphago 0 in 2017
achieving a monumental feat and an

00:28:23.250 --> 00:28:23.260
achieving a monumental feat and an
 

00:28:23.260 --> 00:28:24.690
achieving a monumental feat and an
artificial intelligence one of the

00:28:24.690 --> 00:28:24.700
artificial intelligence one of the
 

00:28:24.700 --> 00:28:25.950
artificial intelligence one of the
greatest in my opinion in the last

00:28:25.950 --> 00:28:25.960
greatest in my opinion in the last
 

00:28:25.960 --> 00:28:29.730
greatest in my opinion in the last
decade of training on no human expert

00:28:29.730 --> 00:28:29.740
decade of training on no human expert
 

00:28:29.740 --> 00:28:32.670
decade of training on no human expert
play playing against it itself being

00:28:32.670 --> 00:28:32.680
play playing against it itself being
 

00:28:32.680 --> 00:28:35.100
play playing against it itself being
able to beat and the initial alphago and

00:28:35.100 --> 00:28:35.110
able to beat and the initial alphago and
 

00:28:35.110 --> 00:28:36.870
able to beat and the initial alphago and
beat the best human players in the world

00:28:36.870 --> 00:28:36.880
beat the best human players in the world
 

00:28:36.880 --> 00:28:38.670
beat the best human players in the world
this is an incredible achievement for

00:28:38.670 --> 00:28:38.680
this is an incredible achievement for
 

00:28:38.680 --> 00:28:40.470
this is an incredible achievement for
reinforcement learning they captivated

00:28:40.470 --> 00:28:40.480
reinforcement learning they captivated
 

00:28:40.480 --> 00:28:42.540
reinforcement learning they captivated
our imagination of what's possible with

00:28:42.540 --> 00:28:42.550
our imagination of what's possible with
 

00:28:42.550 --> 00:28:45.780
our imagination of what's possible with
these approaches but the actual approach

00:28:45.780 --> 00:28:45.790
these approaches but the actual approach
 

00:28:45.790 --> 00:28:47.430
these approaches but the actual approach
and you can look through the slides as a

00:28:47.430 --> 00:28:47.440
and you can look through the slides as a
 

00:28:47.440 --> 00:28:51.330
and you can look through the slides as a
few interesting tidbits in there but the

00:28:51.330 --> 00:28:51.340
few interesting tidbits in there but the
 

00:28:51.340 --> 00:28:53.130
few interesting tidbits in there but the
it's using the same kind of methodology

00:28:53.130 --> 00:28:53.140
it's using the same kind of methodology
 

00:28:53.140 --> 00:28:54.240
it's using the same kind of methodology
that a lot of game engines

00:28:54.240 --> 00:28:54.250
that a lot of game engines
 

00:28:54.250 --> 00:28:56.550
that a lot of game engines
I've been using and certainly go players

00:28:56.550 --> 00:28:56.560
I've been using and certainly go players
 

00:28:56.560 --> 00:28:59.280
I've been using and certainly go players
for the Monte Carlo tree search so you

00:28:59.280 --> 00:28:59.290
for the Monte Carlo tree search so you
 

00:28:59.290 --> 00:29:01.410
for the Monte Carlo tree search so you
have this incredibly huge search space

00:29:01.410 --> 00:29:01.420
have this incredibly huge search space
 

00:29:01.420 --> 00:29:04.230
have this incredibly huge search space
and you have to figure out like which

00:29:04.230 --> 00:29:04.240
and you have to figure out like which
 

00:29:04.240 --> 00:29:06.600
and you have to figure out like which
parts of it do I search in order to find

00:29:06.600 --> 00:29:06.610
parts of it do I search in order to find
 

00:29:06.610 --> 00:29:08.310
parts of it do I search in order to find
the good positions the good actions and

00:29:08.310 --> 00:29:08.320
the good positions the good actions and
 

00:29:08.320 --> 00:29:10.860
the good positions the good actions and
so there you'll now actually use to do

00:29:10.860 --> 00:29:10.870
so there you'll now actually use to do
 

00:29:10.870 --> 00:29:12.480
so there you'll now actually use to do
the estimation of what are the good

00:29:12.480 --> 00:29:12.490
the estimation of what are the good
 

00:29:12.490 --> 00:29:14.900
the estimation of what are the good
actions what are the good positions

00:29:14.900 --> 00:29:14.910
actions what are the good positions
 

00:29:14.910 --> 00:29:17.940
actions what are the good positions
again the slides have the fun the fun

00:29:17.940 --> 00:29:17.950
again the slides have the fun the fun
 

00:29:17.950 --> 00:29:20.970
again the slides have the fun the fun
details for those of you who are

00:29:20.970 --> 00:29:20.980
details for those of you who are
 

00:29:20.980 --> 00:29:22.680
details for those of you who are
gambling addicts

00:29:22.680 --> 00:29:22.690
gambling addicts
 

00:29:22.690 --> 00:29:25.410
gambling addicts
this is importantly so the the

00:29:25.410 --> 00:29:25.420
this is importantly so the the
 

00:29:25.420 --> 00:29:27.870
this is importantly so the the
stochastic element of poker at least

00:29:27.870 --> 00:29:27.880
stochastic element of poker at least
 

00:29:27.880 --> 00:29:30.710
stochastic element of poker at least
heads-up poker so one on one has been

00:29:30.710 --> 00:29:30.720
heads-up poker so one on one has been
 

00:29:30.720 --> 00:29:34.230
heads-up poker so one on one has been
for the first time ever this same exact

00:29:34.230 --> 00:29:34.240
for the first time ever this same exact
 

00:29:34.240 --> 00:29:36.030
for the first time ever this same exact
approach have been used in deep stack

00:29:36.030 --> 00:29:36.040
approach have been used in deep stack
 

00:29:36.040 --> 00:29:39.000
approach have been used in deep stack
and other agents to beat the top

00:29:39.000 --> 00:29:39.010
and other agents to beat the top
 

00:29:39.010 --> 00:29:40.680
and other agents to beat the top
professional poker players in the world

00:29:40.680 --> 00:29:40.690
professional poker players in the world
 

00:29:40.690 --> 00:29:44.010
professional poker players in the world
in 2017 the open challenge for the

00:29:44.010 --> 00:29:44.020
in 2017 the open challenge for the
 

00:29:44.020 --> 00:29:46.080
in 2017 the open challenge for the
community for maybe people in this room

00:29:46.080 --> 00:29:46.090
community for maybe people in this room
 

00:29:46.090 --> 00:29:49.470
community for maybe people in this room
in 2018 is to apply these methods to win

00:29:49.470 --> 00:29:49.480
in 2018 is to apply these methods to win
 

00:29:49.480 --> 00:29:51.330
in 2018 is to apply these methods to win
in a much more complex environment of

00:29:51.330 --> 00:29:51.340
in a much more complex environment of
 

00:29:51.340 --> 00:29:52.950
in a much more complex environment of
term and play when there's multiple

00:29:52.950 --> 00:29:52.960
term and play when there's multiple
 

00:29:52.960 --> 00:29:55.020
term and play when there's multiple
players so heads-up poker is a much

00:29:55.020 --> 00:29:55.030
players so heads-up poker is a much
 

00:29:55.030 --> 00:29:58.140
players so heads-up poker is a much
easier problem the human element is much

00:29:58.140 --> 00:29:58.150
easier problem the human element is much
 

00:29:58.150 --> 00:30:00.350
easier problem the human element is much
more formal izybelle and clear there

00:30:00.350 --> 00:30:00.360
more formal izybelle and clear there
 

00:30:00.360 --> 00:30:02.400
more formal izybelle and clear there
when there's multiple players it's

00:30:02.400 --> 00:30:02.410
when there's multiple players it's
 

00:30:02.410 --> 00:30:04.290
when there's multiple players it's
exceptionally difficult and fascinating

00:30:04.290 --> 00:30:04.300
exceptionally difficult and fascinating
 

00:30:04.300 --> 00:30:06.270
exceptionally difficult and fascinating
a fascinating problem that's perhaps

00:30:06.270 --> 00:30:06.280
a fascinating problem that's perhaps
 

00:30:06.280 --> 00:30:09.480
a fascinating problem that's perhaps
more representative of agents that have

00:30:09.480 --> 00:30:09.490
more representative of agents that have
 

00:30:09.490 --> 00:30:11.250
more representative of agents that have
to act in the real world so now the

00:30:11.250 --> 00:30:11.260
to act in the real world so now the
 

00:30:11.260 --> 00:30:14.160
to act in the real world so now the
downer part a lot of the successful

00:30:14.160 --> 00:30:14.170
downer part a lot of the successful
 

00:30:14.170 --> 00:30:16.830
downer part a lot of the successful
agents that we work with here at MIT and

00:30:16.830 --> 00:30:16.840
agents that we work with here at MIT and
 

00:30:16.840 --> 00:30:19.380
agents that we work with here at MIT and
build the robots that act in the real

00:30:19.380 --> 00:30:19.390
build the robots that act in the real
 

00:30:19.390 --> 00:30:22.500
build the robots that act in the real
world are using almost no deep

00:30:22.500 --> 00:30:22.510
world are using almost no deep
 

00:30:22.510 --> 00:30:24.240
world are using almost no deep
reinforcement learning so deeper

00:30:24.240 --> 00:30:24.250
reinforcement learning so deeper
 

00:30:24.250 --> 00:30:26.130
reinforcement learning so deeper
enforcement learning is successfully

00:30:26.130 --> 00:30:26.140
enforcement learning is successfully
 

00:30:26.140 --> 00:30:29.040
enforcement learning is successfully
applied in context of simulation in

00:30:29.040 --> 00:30:29.050
applied in context of simulation in
 

00:30:29.050 --> 00:30:31.140
applied in context of simulation in
context of game playing but in

00:30:31.140 --> 00:30:31.150
context of game playing but in
 

00:30:31.150 --> 00:30:34.020
context of game playing but in
successfully controlling humanoid

00:30:34.020 --> 00:30:34.030
successfully controlling humanoid
 

00:30:34.030 --> 00:30:37.830
successfully controlling humanoid
robotics or human robots humanoid robots

00:30:37.830 --> 00:30:37.840
robotics or human robots humanoid robots
 

00:30:37.840 --> 00:30:40.650
robotics or human robots humanoid robots
or autonomous vehicles for example the

00:30:40.650 --> 00:30:40.660
or autonomous vehicles for example the
 

00:30:40.660 --> 00:30:42.600
or autonomous vehicles for example the
deep learning methods they use primarily

00:30:42.600 --> 00:30:42.610
deep learning methods they use primarily
 

00:30:42.610 --> 00:30:44.040
deep learning methods they use primarily
for the perception tasks they're

00:30:44.040 --> 00:30:44.050
for the perception tasks they're
 

00:30:44.050 --> 00:30:45.570
for the perception tasks they're
exceptionally good at making sense of

00:30:45.570 --> 00:30:45.580
exceptionally good at making sense of
 

00:30:45.580 --> 00:30:48.570
exceptionally good at making sense of
the environment and extracting useful

00:30:48.570 --> 00:30:48.580
the environment and extracting useful
 

00:30:48.580 --> 00:30:49.830
the environment and extracting useful
knowledge from it but in terms of

00:30:49.830 --> 00:30:49.840
knowledge from it but in terms of
 

00:30:49.840 --> 00:30:51.690
knowledge from it but in terms of
forming actions that's usually done

00:30:51.690 --> 00:30:51.700
forming actions that's usually done
 

00:30:51.700 --> 00:30:54.650
forming actions that's usually done
through optimization based methods

00:30:54.650 --> 00:30:54.660
through optimization based methods
 

00:30:54.660 --> 00:30:58.110
through optimization based methods
finally a quick comment on the

00:30:58.110 --> 00:30:58.120
finally a quick comment on the
 

00:30:58.120 --> 00:31:00.900
finally a quick comment on the
unexpected local pockets that's that's

00:31:00.900 --> 00:31:00.910
unexpected local pockets that's that's
 

00:31:00.910 --> 00:31:03.060
unexpected local pockets that's that's
at the core of why these methods are not

00:31:03.060 --> 00:31:03.070
at the core of why these methods are not
 

00:31:03.070 --> 00:31:05.730
at the core of why these methods are not
used in the real world here is a game of

00:31:05.730 --> 00:31:05.740
used in the real world here is a game of
 

00:31:05.740 --> 00:31:07.590
used in the real world here is a game of
coast runners where a boat is

00:31:07.590 --> 00:31:07.600
coast runners where a boat is
 

00:31:07.600 --> 00:31:09.750
coast runners where a boat is
asked with receiving a lot of points

00:31:09.750 --> 00:31:09.760
asked with receiving a lot of points
 

00:31:09.760 --> 00:31:11.940
asked with receiving a lot of points
traditionally the game is played by

00:31:11.940 --> 00:31:11.950
traditionally the game is played by
 

00:31:11.950 --> 00:31:13.740
traditionally the game is played by
racing other boats and trying to get to

00:31:13.740 --> 00:31:13.750
racing other boats and trying to get to
 

00:31:13.750 --> 00:31:15.659
racing other boats and trying to get to
the finish as quickly as possible and

00:31:15.659 --> 00:31:15.669
the finish as quickly as possible and
 

00:31:15.669 --> 00:31:17.760
the finish as quickly as possible and
this boat figures out that it doesn't

00:31:17.760 --> 00:31:17.770
this boat figures out that it doesn't
 

00:31:17.770 --> 00:31:19.260
this boat figures out that it doesn't
need to do that in a brilliant

00:31:19.260 --> 00:31:19.270
need to do that in a brilliant
 

00:31:19.270 --> 00:31:22.529
need to do that in a brilliant
breakthrough idea it can just collect

00:31:22.529 --> 00:31:22.539
breakthrough idea it can just collect
 

00:31:22.539 --> 00:31:25.380
breakthrough idea it can just collect
the regenerating green squares that's an

00:31:25.380 --> 00:31:25.390
the regenerating green squares that's an
 

00:31:25.390 --> 00:31:28.650
the regenerating green squares that's an
unintended consequence that you can

00:31:28.650 --> 00:31:28.660
unintended consequence that you can
 

00:31:28.660 --> 00:31:30.600
unintended consequence that you can
extend to other systems perhaps

00:31:30.600 --> 00:31:30.610
extend to other systems perhaps
 

00:31:30.610 --> 00:31:33.539
extend to other systems perhaps
including you can imagine what how the

00:31:33.539 --> 00:31:33.549
including you can imagine what how the
 

00:31:33.549 --> 00:31:36.930
including you can imagine what how the
the cat system over time at the bottom

00:31:36.930 --> 00:31:36.940
the cat system over time at the bottom
 

00:31:36.940 --> 00:31:38.130
the cat system over time at the bottom
right can evolve into something

00:31:38.130 --> 00:31:38.140
right can evolve into something
 

00:31:38.140 --> 00:31:41.730
right can evolve into something
undesirable and further on in these

00:31:41.730 --> 00:31:41.740
undesirable and further on in these
 

00:31:41.740 --> 00:31:43.799
undesirable and further on in these
reinforcement learning agents when they

00:31:43.799 --> 00:31:43.809
reinforcement learning agents when they
 

00:31:43.809 --> 00:31:46.919
reinforcement learning agents when they
act in the real world the human life is

00:31:46.919 --> 00:31:46.929
act in the real world the human life is
 

00:31:46.929 --> 00:31:49.590
act in the real world the human life is
often the human factor are often

00:31:49.590 --> 00:31:49.600
often the human factor are often
 

00:31:49.600 --> 00:31:51.149
often the human factor are often
injected into the system and so

00:31:51.149 --> 00:31:51.159
injected into the system and so
 

00:31:51.159 --> 00:31:53.370
injected into the system and so
oftentimes in the reward function the

00:31:53.370 --> 00:31:53.380
oftentimes in the reward function the
 

00:31:53.380 --> 00:31:55.230
oftentimes in the reward function the
objective loss function you start

00:31:55.230 --> 00:31:55.240
objective loss function you start
 

00:31:55.240 --> 00:31:57.539
objective loss function you start
injecting concepts of risk and even

00:31:57.539 --> 00:31:57.549
injecting concepts of risk and even
 

00:31:57.549 --> 00:31:59.789
injecting concepts of risk and even
human life so what does it look like in

00:31:59.789 --> 00:31:59.799
human life so what does it look like in
 

00:31:59.799 --> 00:32:02.010
human life so what does it look like in
terms of AI safety when an agent has to

00:32:02.010 --> 00:32:02.020
terms of AI safety when an agent has to
 

00:32:02.020 --> 00:32:04.500
terms of AI safety when an agent has to
make decision based on a loss function

00:32:04.500 --> 00:32:04.510
make decision based on a loss function
 

00:32:04.510 --> 00:32:07.200
make decision based on a loss function
that includes an estimate or risk of

00:32:07.200 --> 00:32:07.210
that includes an estimate or risk of
 

00:32:07.210 --> 00:32:09.360
that includes an estimate or risk of
killing another human being this is a

00:32:09.360 --> 00:32:09.370
killing another human being this is a
 

00:32:09.370 --> 00:32:10.919
killing another human being this is a
very important thing to think about

00:32:10.919 --> 00:32:10.929
very important thing to think about
 

00:32:10.929 --> 00:32:14.399
very important thing to think about
about machines that learn from data and

00:32:14.399 --> 00:32:14.409
about machines that learn from data and
 

00:32:14.409 --> 00:32:17.690
about machines that learn from data and
finally to play around as there's a

00:32:17.690 --> 00:32:17.700
finally to play around as there's a
 

00:32:17.700 --> 00:32:19.980
finally to play around as there's a
there's a lot of ways to explore and

00:32:19.980 --> 00:32:19.990
there's a lot of ways to explore and
 

00:32:19.990 --> 00:32:22.080
there's a lot of ways to explore and
learn about deeper enforcement learning

00:32:22.080 --> 00:32:22.090
learn about deeper enforcement learning
 

00:32:22.090 --> 00:32:25.950
learn about deeper enforcement learning
and we have at the URL below there a

00:32:25.950 --> 00:32:25.960
and we have at the URL below there a
 

00:32:25.960 --> 00:32:27.870
and we have at the URL below there a
deep traffic simulation game it's a

00:32:27.870 --> 00:32:27.880
deep traffic simulation game it's a
 

00:32:27.880 --> 00:32:31.020
deep traffic simulation game it's a
competition where you get to build a car

00:32:31.020 --> 00:32:31.030
competition where you get to build a car
 

00:32:31.030 --> 00:32:33.570
competition where you get to build a car
that speeds that tries to achieve the as

00:32:33.570 --> 00:32:33.580
that speeds that tries to achieve the as
 

00:32:33.580 --> 00:32:35.310
that speeds that tries to achieve the as
close to 80 miles per hour as possible

00:32:35.310 --> 00:32:35.320
close to 80 miles per hour as possible
 

00:32:35.320 --> 00:32:39.140
close to 80 miles per hour as possible
and I encourage you to participate

00:32:39.140 --> 00:32:39.150
 
 

00:32:39.150 --> 00:32:41.760
 
participate and try to win get at a

00:32:41.760 --> 00:32:41.770
participate and try to win get at a
 

00:32:41.770 --> 00:32:43.680
participate and try to win get at a
leaderboard not enough MIT folks are at

00:32:43.680 --> 00:32:43.690
leaderboard not enough MIT folks are at
 

00:32:43.690 --> 00:32:46.110
leaderboard not enough MIT folks are at
the top 10 so with that thank you very

00:32:46.110 --> 00:32:46.120
the top 10 so with that thank you very
 

00:32:46.120 --> 00:32:47.970
the top 10 so with that thank you very
much thank you for having me

00:32:47.970 --> 00:32:47.980
much thank you for having me
 

00:32:47.980 --> 00:32:50.690
much thank you for having me
[Applause]

