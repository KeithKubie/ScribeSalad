Speaker 1:          00:00          And does, we know, we'll look at the usual preprocessing steps that we go through to take some text data in its raw form and put it in a different form, which is more amenable for a neural network processing. So, um, what is texts really? What is the raw form for a actual text? Uh, well, really it's, you should think of text in its raw form as just a long string that contains all of the characters that, uh, that are contained in a contained, in the text. So we have an example here where we have just a single sentence represented as a long string, uh, but it could be a whole webpage. Uh, so the textual content of a webpage where the whole string would be, uh, all the sentences, uh, in order that they appear in, in the, uh, in the webpage and question. So that's the raw format for our text data and that we'll see a bunch of steps, uh, preprocessing steps that we usually go through to get, uh, to represent this text in a format that's going to be easier to manipulate when we want to feed textual data.

Speaker 1:          01:13          To a neuro network. Um, so the first thing that we usually do is that we will tokenize the text. That is, we will identify essentially the words or tokens that are, uh, present in, uh, in the, in the text itself. So, um, so for instance, for this sentence here, he spending seven days in San Francisco, then we could split it, uh, like this. Um, so the first token or word would be heat. The second one would be apostrophe s then spending seven and so on. So we'd convert that into a list of shorter strings where each train is a word and, um, uh, instead of word, we often say a token. So for me, the data sets, if you download the data, set a NLP Dataset from the web, this step as often been done for you already, uh, but if you were to apply this on where you would extract by yourself some, some text data from the web point instance, you would have to go through that step, uh, for English and for French, usually just splitting words or tokens based on, uh, by essentially spinning at spaces and also separating punctuations.

Speaker 1:          02:24          So, for instance, we notice here that the a.here a is actually a separate it as its own token. And if there had been a Camo so it would be considered its own token. Uh, so if we just use these two simple rules separate at spaces and make punctuations their own token, then for English and French, that usually works fine. And we've been able like this to, uh, essentially separate everything into words. It's not as much true for other language before English and French that works well. Uh, and there are some other languages where that works well. But we have an example here where our points, maybe what we would want actually is consider San Francisco as a single word, a single token and a, if you wanted to do some more fancier preprocessing like this, you need to define some rules. Uh, where are you would identify that a space here for instance, is not something that should separate that token into two. Okay. But just keep in mind and in general, just separating by species is often sufficient. Uh, in particular for English and French.

Speaker 1:          03:30          Um, next, we often limitizing the tokens that it will put them in some sort of standard form where we remove some, uh, usually syntactic information that is contained in the words. So for instance, we could take the word he and just remove the capital letters, so uncapitalized it, or we could also take some verbs and actually put them in a more standard form. Uh, we could take numbers, it just replace them by some arbitrary token. Here I chosen the token number, uh, just to say that this was a number and a pass this step, you just don't know which number it was. Um, we could remove plurals or days converted today and so on. So the specific kind of limits ization that you might want to do for a particular problem, we'll, we'll vary. Essentially what you want to do is remove variations of words, uh, that are not relevant for the task you want to solve. So if knowing that this is seven, this is just an arbitrary number is important, then you'll want to keep seven here. Uh, in the same way, if, uh, knowing that over, uh, uh, now noun for instance, was singular or plural is important, then don't remove the plural. Just keep it in its original form, in the original text. Okay. So that's the monetization.

Speaker 1:          04:53          And then what we'll do is that, uh, will not convert ev all of these tokens, which are strings into an integer id. And the way we'll do that is that will first come up with a vocabulary, uh, which is a list of words that we're willing to recognize, advanced English. And, uh, so this vocabulary, we can think of it as a big long lists of words. And then, uh, once we've made up this list and we can associate, uh, each word in that list or each token, uh, to a unique ids offense, the first one could be id one, the second one, I need two, three, and so on. Um, so how did we come up with that list of words that are different criteria that we can use? Uh, we can just pick the most frequent words in the data set that we've collected.

Speaker 1:          05:43          Uh, so we take all the words to have a frequency of at least five, or if we want to limit the number of words, if we know the number of words we willing to, to distinguish, we could say we'll take the 100,000 most frequent words. So that's a very general, which is often used sometimes. Also we, uh, decided to ignore some quote unquote uninformative words, uh, which would define using a short list. For instance, we could remove articles like the or, uh, uh, there might be other words which are not useful to recognize in the text for the problem that we want to solve. And at Dnh then for all the words or tokens that are not in, not in our vocabulary that we've defined, uh, then all of these other words will be mapped to a special out of vocabulary id. So at the very end of our list, we can think of it as there being an extra word, Oval v four out of vocabulary. And then this one will take a, the last id in our list of, so if I had a say here, I had nine words, then this would take id, 10 points things.

Speaker 2:          06:54          Yeah.

Speaker 1:          06:55          So again, depending on the problem, the type of vocabulary we want to define the rules that we want to choose for selecting which words we want to distinguish and associated that are unique id to a is going to vary. Um, and in practice the vocabulary size can actually, uh, uh, be quite big. It can be as little as around 10,000 or as high as ton and a 50,000 or even more.

Speaker 2:          07:24          Yeah.

Speaker 1:          07:24          So here's an example of the last preprocessing step. So we have this list of tokens that have been limitizing. And then imagine I've defined this vocabulary here where the is associated with the ID one and two, two and so on until Oh v, which is associated with the ID five. Then I take that and I replace it by one, the integer one, because the has the id one and cat in the list, uh, is not present. So we'll map it to the idea of [inaudible]. So that's why we have five here than, and is going to take the ID to uh, the one again. And then dog, she's a, which has the ID three, it's going to be associated with three and so on.

Speaker 2:          08:06          Yeah.

Speaker 1:          08:07          So, um, um, for the rest of the videos, uh, w whatever we're going to refer to some word ideas often I will use this symbol w um, so we can think of w as a cartographer categorical feature that describes the original word. It's essentially a feature description of the original words. So fun since the, we can think of one as being a feature and it's a categorical feature because it corresponds to which category in many categories where each category would be a different word in the vocabulary. So to which category does the correspond to and uh, we could imagine adding other features and that's, we'll probably talk about in the following videos. Um, but at this point now we have data which is taking a, a format which is more numerical, so that's going to be easier to deal with. And, uh, still sometimes when I'm going to talk about and refer to WWE, I might say the word w, even though really it corresponds to an id, but because it's just at this point, these categories correspond Duker respond to individual words, except for o v which correspond to any other word that's not in the vocabulary.

Speaker 1:          09:28          So, um, you know, talking about WWE as a word is not such a stretch really. So, um, so that's a 40 preprocessing, and that's the kind of preprocessing we'll assume that has been done for the data, uh, for everything else that follows in the next [inaudible].