WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.490
Hello, my name is Jay Judkowitz.

00:00:02.490 --> 00:00:05.230
I'm the product manager for
Google Cloud Platform block

00:00:05.230 --> 00:00:06.410
storage products.

00:00:06.410 --> 00:00:08.680
I'm here with some
news, technical details,

00:00:08.680 --> 00:00:10.840
and best practices
about block storage

00:00:10.840 --> 00:00:12.850
in Google Cloud Platform.

00:00:12.850 --> 00:00:15.530
At Google, we're always excited
to bring new storage offerings

00:00:15.530 --> 00:00:16.500
to the cloud.

00:00:16.500 --> 00:00:19.220
That's because it's all based
on the great technology we've

00:00:19.220 --> 00:00:22.670
built in-house to run some of
the largest and most successful

00:00:22.670 --> 00:00:24.430
applications on the internet.

00:00:24.430 --> 00:00:25.825
When you use Google
Cloud, you're

00:00:25.825 --> 00:00:28.200
taking advantage of the same
great storage infrastructure

00:00:28.200 --> 00:00:31.280
as we do at Google.

00:00:31.280 --> 00:00:34.170
Block storage in the cloud
is general-purpose storage

00:00:34.170 --> 00:00:36.230
that you attach to a
virtual machine that

00:00:36.230 --> 00:00:38.580
appears as a disk
drive to the VM.

00:00:38.580 --> 00:00:40.850
You generally format it
with some file system

00:00:40.850 --> 00:00:42.980
and write files or
databases to it.

00:00:42.980 --> 00:00:46.450
It's what you use to create your
own self-managed data stores.

00:00:46.450 --> 00:00:48.680
This section will announce
two new pieces of news

00:00:48.680 --> 00:00:50.510
about block storage
in Google Cloud,

00:00:50.510 --> 00:00:53.080
and quickly describe the
three block storage options

00:00:53.080 --> 00:00:54.290
that we now have.

00:00:54.290 --> 00:00:57.040
Cloud Platform offers three
types of block storage.

00:00:57.040 --> 00:00:58.880
The key difference
between them is how many

00:00:58.880 --> 00:01:02.150
I/Os per second, or
IOPS, can be supplied

00:01:02.150 --> 00:01:04.050
by each gigabyte of storage.

00:01:04.050 --> 00:01:05.830
Higher IOPS per
gigabyte trade off

00:01:05.830 --> 00:01:08.040
against cost and flexibility.

00:01:08.040 --> 00:01:11.161
The first option, Standard
Persistent Disk, or PD,

00:01:11.161 --> 00:01:12.660
has been around
since Google Compute

00:01:12.660 --> 00:01:14.160
Engine was first released.

00:01:14.160 --> 00:01:16.370
It utilizes networked
hard drives.

00:01:16.370 --> 00:01:19.560
It's designed for security,
high availability,

00:01:19.560 --> 00:01:21.620
and consistent performance.

00:01:21.620 --> 00:01:23.970
For use cases that
need up to, say, two

00:01:23.970 --> 00:01:28.460
random read IOPS per second per
gigabyte, this option is ideal.

00:01:28.460 --> 00:01:31.530
You get steady performance
at a low price.

00:01:31.530 --> 00:01:34.205
The second option,
SSD Persistent Disk,

00:01:34.205 --> 00:01:37.440
we're pleased to announce is
in general availability now.

00:01:37.440 --> 00:01:41.190
It's like Standard PD,
but backed by network SSD.

00:01:41.190 --> 00:01:43.520
As a result, you
have a higher cost

00:01:43.520 --> 00:01:45.750
but a higher performance
per gigabyte.

00:01:45.750 --> 00:01:48.510
Use it like you would Standard
PD, but in cases where you need

00:01:48.510 --> 00:01:51.920
between two and 30 IOPS per gig.

00:01:51.920 --> 00:01:54.160
The third option is not
generally available,

00:01:54.160 --> 00:01:56.980
but is now accepting
trusted testers.

00:01:56.980 --> 00:01:58.790
It's a local SSD option.

00:01:58.790 --> 00:02:01.040
It offers the highest
possible performance,

00:02:01.040 --> 00:02:03.660
but has availability,
durability, and flexibility

00:02:03.660 --> 00:02:06.520
trade-offs I'll discuss shortly.

00:02:06.520 --> 00:02:08.782
Before we get too
deep into the numbers,

00:02:08.782 --> 00:02:10.740
let's give a quick overview
of the similarities

00:02:10.740 --> 00:02:14.760
and differences between the
two PD options, and local SSD.

00:02:14.760 --> 00:02:16.380
First, the similarities.

00:02:16.380 --> 00:02:17.880
All of our block
storage options are

00:02:17.880 --> 00:02:23.010
priced on a per-gigabyte basis,
with no I/O charges ever.

00:02:23.010 --> 00:02:26.800
This makes planning simpler,
and eliminates surprises.

00:02:26.800 --> 00:02:28.300
All of our block
storage options are

00:02:28.300 --> 00:02:30.210
designed to perform
consistently.

00:02:30.210 --> 00:02:32.280
Consistently between
similar volumes,

00:02:32.280 --> 00:02:34.900
and consistently over time.

00:02:34.900 --> 00:02:38.100
Third, all of our block storage
options are encrypted on disk.

00:02:38.100 --> 00:02:40.420
And because PD is a
network storage offering,

00:02:40.420 --> 00:02:42.350
the data is also
encrypted over the wire.

00:02:42.350 --> 00:02:45.210
This is to give you the
best possible security.

00:02:45.210 --> 00:02:48.820
Lastly, all of our block storage
options support live migration,

00:02:48.820 --> 00:02:51.390
and are therefore not affected
by planned downtimes that

00:02:51.390 --> 00:02:54.474
keep Google Cloud data centers
patched and maintained--

00:02:54.474 --> 00:02:56.140
which is what keeps
your applications as

00:02:56.140 --> 00:02:59.180
secure and available
as possible.

00:02:59.180 --> 00:03:00.230
Now for differences.

00:03:00.230 --> 00:03:02.670
Starting with reliability.

00:03:02.670 --> 00:03:04.860
PD has additional
reliability that

00:03:04.860 --> 00:03:06.840
comes from storing
its data redundant,

00:03:06.840 --> 00:03:10.040
so the volumes can survive
the sudden loss of hardware.

00:03:10.040 --> 00:03:12.076
Local SSD does not
have this redundancy.

00:03:12.076 --> 00:03:13.700
It's up to you to
protect the data that

00:03:13.700 --> 00:03:15.880
needs to survive
storage failures.

00:03:15.880 --> 00:03:17.660
PD also was built in check sums.

00:03:17.660 --> 00:03:19.250
Whenever you read
data, no matter

00:03:19.250 --> 00:03:21.780
how long ago it was
stored, PD makes sure

00:03:21.780 --> 00:03:23.830
that it returns what
was written to you,

00:03:23.830 --> 00:03:26.060
and avoids silent
data corruptions.

00:03:26.060 --> 00:03:28.850
Lastly, PD has a
snapshot functionality

00:03:28.850 --> 00:03:31.940
that makes it easier for you to
protect your data against user

00:03:31.940 --> 00:03:34.530
error and physical disasters.

00:03:34.530 --> 00:03:36.620
In terms of performance,
while all the

00:03:36.620 --> 00:03:39.980
block storage options
perform consistently,

00:03:39.980 --> 00:03:42.940
local SSD has much
higher IOPS, and is

00:03:42.940 --> 00:03:45.090
designed for
sub-millisecond latency,

00:03:45.090 --> 00:03:49.060
making it ideal for the most
performance-hungry databases.

00:03:49.060 --> 00:03:52.260
For flexibility, note
that both PD and local SSD

00:03:52.260 --> 00:03:53.500
are very flexible.

00:03:53.500 --> 00:03:55.270
PD volumes are
the more flexible,

00:03:55.270 --> 00:03:57.715
in that they can be created
in sizes ranging from as small

00:03:57.715 --> 00:04:00.820
as one gigabyte all the
way up to 10 terabytes,

00:04:00.820 --> 00:04:02.720
in one-gigabyte increments.

00:04:02.720 --> 00:04:06.100
And it could be attached and
detached from VMs on the fly.

00:04:06.100 --> 00:04:08.360
Local SSD, by contrast,
can be allocated

00:04:08.360 --> 00:04:11.150
to VMs only at creation
time, and needs

00:04:11.150 --> 00:04:15.960
to be allocated in one to four
partitions of 375 gigabytes.

00:04:15.960 --> 00:04:17.870
While this isn't
as flexible as PD,

00:04:17.870 --> 00:04:20.560
local SSD does not require
special instance types

00:04:20.560 --> 00:04:22.820
with pre-specified
amounts of SSD.

00:04:22.820 --> 00:04:29.920
This lets you right-size the
VM, CPU, memory, and storage.

00:04:29.920 --> 00:04:32.100
So given these
three options, how

00:04:32.100 --> 00:04:35.770
do you decide which option is
right for your application?

00:04:35.770 --> 00:04:37.590
Let's split it up by use case.

00:04:37.590 --> 00:04:39.990
First, boot volumes
and bulk storage

00:04:39.990 --> 00:04:43.800
are great use cases
for standard PD,

00:04:43.800 --> 00:04:47.290
because those use cases require
low IOPS and low throughput,

00:04:47.290 --> 00:04:48.670
and basically just
want to occupy

00:04:48.670 --> 00:04:52.210
the cheapest reliable
space available.

00:04:52.210 --> 00:04:54.970
Streaming I/O is a use case
that works best for Standard PD

00:04:54.970 --> 00:04:55.880
as well.

00:04:55.880 --> 00:04:58.680
While the other two options
have a much lower cost per IOPS,

00:04:58.680 --> 00:05:00.500
the cost per throughput
is generally better

00:05:00.500 --> 00:05:01.770
on Standard PD.

00:05:01.770 --> 00:05:06.070
Reading large, sequential blocks
is what disks are good at.

00:05:06.070 --> 00:05:10.680
SQL databases, like MySQL and
Postgres, and NoSQL databases,

00:05:10.680 --> 00:05:12.460
like Cassandra,
Mongo, and Redis,

00:05:12.460 --> 00:05:15.070
tend to be transactional
and IOPS-heavy.

00:05:15.070 --> 00:05:17.930
Smaller instances might be
able to be run on Standard PD,

00:05:17.930 --> 00:05:20.270
but important production
databases should usually

00:05:20.270 --> 00:05:23.800
be run on some form of
SSD-- either SSD Persistent

00:05:23.800 --> 00:05:28.800
Disk, or local SSD, depending
on how high the IOPS needs are.

00:05:28.800 --> 00:05:31.580
File servers, like
NFS Gluster or Ceph,

00:05:31.580 --> 00:05:33.920
can be more streaming
or more transactional,

00:05:33.920 --> 00:05:35.820
depending on what the
clients are doing.

00:05:35.820 --> 00:05:40.120
SSD PD is generally
a great choice here.

00:05:40.120 --> 00:05:43.550
High performance scratch disk,
where keeping the track glocal

00:05:43.550 --> 00:05:47.196
is the right architecture,
should go on local SSD.

00:05:47.196 --> 00:05:48.820
For Hadoop deployments,
where I/O needs

00:05:48.820 --> 00:05:51.870
exceed what Google Cloud
Storage Connector for Hadoop

00:05:51.870 --> 00:05:55.490
can provide-- for that case,
you should use local SSD,

00:05:55.490 --> 00:05:59.170
under Hadoop
Optimized File System.

00:05:59.170 --> 00:06:01.860
Here's another way to
visualize the same information.

00:06:01.860 --> 00:06:04.760
As you can see, depending on
the performance needs of file

00:06:04.760 --> 00:06:06.660
servers or databases,
you have options

00:06:06.660 --> 00:06:08.824
as to which block
storage product to use.

00:06:08.824 --> 00:06:10.240
To decide, you'll
want to get down

00:06:10.240 --> 00:06:12.239
to the real numbers
of space and IOPS.

00:06:12.239 --> 00:06:14.530
I'll go through some specific
examples just a bit later

00:06:14.530 --> 00:06:16.590
in the presentation.

00:06:16.590 --> 00:06:18.550
But right now, let's
double-click on Persistent

00:06:18.550 --> 00:06:20.750
Disk, and talk about
what separates the two PD

00:06:20.750 --> 00:06:23.320
options-- their performance.

00:06:23.320 --> 00:06:25.990
With both options,
Standard and SSD,

00:06:25.990 --> 00:06:27.830
for both read and
write, the more space

00:06:27.830 --> 00:06:30.170
you buy, more IOPS you get.

00:06:30.170 --> 00:06:34.010
But SSD Persistent Disk grows
in IOPS capability much faster

00:06:34.010 --> 00:06:36.640
than Standard PD, which
is why it costs more.

00:06:36.640 --> 00:06:40.670
Each gigabyte of SSD PD
gets 20 times the write IOPS

00:06:40.670 --> 00:06:43.040
of a gigabyte of
Standard PD, and a hundre

00:06:43.040 --> 00:06:46.290
times the read IOPS of a
gigabyte of Standard PD.

00:06:46.290 --> 00:06:48.160
These high IOPS
numbers are what make

00:06:48.160 --> 00:06:51.690
SSD PD ideal for
transactional workloads,

00:06:51.690 --> 00:06:53.920
like heavily used databases.

00:06:53.920 --> 00:06:56.930
Now, the SSD PD
IOPS grow so fast

00:06:56.930 --> 00:06:58.560
that they reach the
limit of the IOPS

00:06:58.560 --> 00:07:00.710
that a VM can handle at
a relatively low volume

00:07:00.710 --> 00:07:02.380
size, which is why
these graphs have

00:07:02.380 --> 00:07:05.010
this plateau at the
top of the IOPS curve.

00:07:05.010 --> 00:07:07.239
We're always working to
increase the per VM limits,

00:07:07.239 --> 00:07:09.280
and you should check on
the current documentation

00:07:09.280 --> 00:07:11.696
for the latest numbers, rather
than coming back the graph.

00:07:15.020 --> 00:07:18.010
Moving from IOPS to
throughput, same pattern here.

00:07:18.010 --> 00:07:20.740
Performance grows with
space for both options.

00:07:20.740 --> 00:07:22.530
Unlike the IOPS
grass, however, you

00:07:22.530 --> 00:07:26.150
can see that SSD PD's advantage
in throughput is less dramatic.

00:07:26.150 --> 00:07:29.860
And both Standard and SSD PD
can hit the VM throughput limits

00:07:29.860 --> 00:07:31.530
at moderate sizes.

00:07:31.530 --> 00:07:33.850
This explains why we
recommend Standard PD

00:07:33.850 --> 00:07:35.430
for throughput-oriented
applications

00:07:35.430 --> 00:07:36.690
that have low IOPS.

00:07:36.690 --> 00:07:39.820
While SSD PD has a much
better cost per IOPS,

00:07:39.820 --> 00:07:42.590
Standard PD has the better
cost for throughput.

00:07:42.590 --> 00:07:44.150
Like with the
previous slide, please

00:07:44.150 --> 00:07:46.108
check the latest
documentation, as we're always

00:07:46.108 --> 00:07:48.270
striving to raise the per
VM caps on throughput,

00:07:48.270 --> 00:07:50.890
just like with IOPS.

00:07:50.890 --> 00:07:53.330
The idea of growing IOPS
with space is pretty unique,

00:07:53.330 --> 00:07:56.042
and many customers have asked
us why we designed it this way.

00:07:56.042 --> 00:07:58.250
We made this choice because
of the following benefits

00:07:58.250 --> 00:07:59.820
that we think are
very important.

00:07:59.820 --> 00:08:03.390
First, simple and
consistent pricing.

00:08:03.390 --> 00:08:05.730
With this model, there's
no billing variations

00:08:05.730 --> 00:08:08.360
due to your app's I/O
needs changing over time.

00:08:08.360 --> 00:08:10.590
The cost of a volume
is the same from month

00:08:10.590 --> 00:08:13.620
to month with no surprises,
because all the I/O

00:08:13.620 --> 00:08:16.460
was included in the
cost of the space.

00:08:16.460 --> 00:08:19.160
Second, consistency
of performance.

00:08:19.160 --> 00:08:20.410
This is critical.

00:08:20.410 --> 00:08:23.510
By selling I/O and space
together in a safe ratio,

00:08:23.510 --> 00:08:25.660
we stabilize the
overall storage traffic,

00:08:25.660 --> 00:08:28.300
and keep volume performance
consistent over time.

00:08:28.300 --> 00:08:31.900
This means your app won't suffer
the noisy neighbor problem.

00:08:31.900 --> 00:08:34.750
Third, simplicity of in-VM
management and snapshot

00:08:34.750 --> 00:08:35.700
management.

00:08:35.700 --> 00:08:37.710
Because larger volumes
give you the performance

00:08:37.710 --> 00:08:40.350
of many small volumes, your
spared the upfront effort

00:08:40.350 --> 00:08:41.900
of striping volumes together.

00:08:41.900 --> 00:08:44.640
And because each VM needs
only one data volume,

00:08:44.640 --> 00:08:46.490
management of snapshots
becomes trivial.

00:08:46.490 --> 00:08:49.760
Each volume snapshot is a
copy of an entire VM's data

00:08:49.760 --> 00:08:51.780
at a given time.

00:08:51.780 --> 00:08:54.840
Lastly, we wanted to give you
consistency across the product

00:08:54.840 --> 00:08:55.642
line.

00:08:55.642 --> 00:08:57.600
We've been able to make
all three block storage

00:08:57.600 --> 00:08:59.037
options behave similarly.

00:08:59.037 --> 00:09:01.620
You don't need to pick between
different price and performance

00:09:01.620 --> 00:09:05.260
models, or plan I/O down to
the individual IOPS level.

00:09:05.260 --> 00:09:07.820
We've tried to simplify
everything as much as possible.

00:09:07.820 --> 00:09:09.290
You decide how much
space you need,

00:09:09.290 --> 00:09:11.790
and you pick the storage type
based on how many IOPS per gig

00:09:11.790 --> 00:09:14.210
need to come along with it.

00:09:14.210 --> 00:09:15.954
Now at this point,
I'd like to help you

00:09:15.954 --> 00:09:17.620
with the most important
decisions you're

00:09:17.620 --> 00:09:20.222
going to make in
implementing block storage.

00:09:20.222 --> 00:09:21.680
Given the three
options, how do you

00:09:21.680 --> 00:09:23.400
decide which to
use for which app,

00:09:23.400 --> 00:09:25.650
and how large do you need
to make the volumes in order

00:09:25.650 --> 00:09:27.150
to get the performance you need?

00:09:27.150 --> 00:09:29.660
While I can't give you an
exhaustive process or algorithm

00:09:29.660 --> 00:09:31.850
to answer all the
use cases, I will

00:09:31.850 --> 00:09:33.737
walk through four
separate examples

00:09:33.737 --> 00:09:35.570
to show you how you can
approach to decision

00:09:35.570 --> 00:09:37.810
for your application.

00:09:37.810 --> 00:09:39.202
In the first
example, we're going

00:09:39.202 --> 00:09:42.180
to have 300 gigabytes of space,
and a fairly modest amount

00:09:42.180 --> 00:09:45.260
of I/O. 60 random reads
per second, and 120

00:09:45.260 --> 00:09:46.910
random writes per second.

00:09:46.910 --> 00:09:50.710
Here, simply 350 gigabytes
of Standard Persistent Disk

00:09:50.710 --> 00:09:52.170
will do the trick nicely.

00:09:52.170 --> 00:09:55.270
Very economical cost, and
give you all the I/O you need,

00:09:55.270 --> 00:09:57.300
with some to spare.

00:09:57.300 --> 00:09:59.600
The second example is
actually the trickiest one.

00:09:59.600 --> 00:10:02.510
Here, we're still going to go
with 350 gigabytes of space,

00:10:02.510 --> 00:10:06.740
but this time with more I/O.
450 random reads per second,

00:10:06.740 --> 00:10:08.850
and 600 random
writes per second.

00:10:08.850 --> 00:10:12.150
We can quickly see that 350
gigabytes of Standard PD

00:10:12.150 --> 00:10:15.930
will not give a sufficient
I/O. While we could move up

00:10:15.930 --> 00:10:20.040
to Standard PD, SSD PD, we
don't absolutely need to.

00:10:20.040 --> 00:10:22.410
Because the price of
Standard PD is so low,

00:10:22.410 --> 00:10:25.720
we can actually create roughly
an 1800-gigabyte Standard PD

00:10:25.720 --> 00:10:28.160
volume, and it winds up
meeting the performance needs

00:10:28.160 --> 00:10:30.220
while still being somewhat
more cost-effective

00:10:30.220 --> 00:10:34.490
than a 350-gigabyte
SSD PD volume would be.

00:10:34.490 --> 00:10:36.770
The third example leaves
us no room for doubt.

00:10:36.770 --> 00:10:39.480
Here, we want 350
gigabytes of space,

00:10:39.480 --> 00:10:42.590
and a large amount of
I/O. 9,000 randon IOPS.

00:10:42.590 --> 00:10:48.510
Here, 300 gigabytes of SSD
PD meets the needs perfectly.

00:10:48.510 --> 00:10:50.320
In the last example,
we're going to stay

00:10:50.320 --> 00:10:53.770
with 350 gigabytes of space, but
this time, a very large amount

00:10:53.770 --> 00:10:56.530
of I/O. We're going to go
for 60,000 random IOPS.

00:10:56.530 --> 00:10:59.500
Here, you can use a single
local SSD partition of the four

00:10:59.500 --> 00:11:00.680
that you could use.

00:11:00.680 --> 00:11:03.330
A single partition will meet
both the space and the IOPS

00:11:03.330 --> 00:11:05.910
requirements.

00:11:05.910 --> 00:11:09.120
So now let's switch over
to general best practices,

00:11:09.120 --> 00:11:12.170
focusing on
Persistent Disk here.

00:11:12.170 --> 00:11:14.570
Most of the best practices
we're going to talk about

00:11:14.570 --> 00:11:16.330
are about performance.

00:11:16.330 --> 00:11:18.490
The first performance
tip comes from the fact

00:11:18.490 --> 00:11:21.680
that PD works with
four-kilobyte I/O boundaries.

00:11:21.680 --> 00:11:23.600
Any misaligned I/O
will dramatically

00:11:23.600 --> 00:11:24.965
lower the performance.

00:11:24.965 --> 00:11:28.350
All of the PD I/O should be
multiples of four kilobyte,

00:11:28.350 --> 00:11:30.990
and aligned to
four-kilobyte boundaries.

00:11:30.990 --> 00:11:32.810
Disk partitions and
their file systems

00:11:32.810 --> 00:11:35.214
must be aligned to
those 4K boundaries.

00:11:35.214 --> 00:11:36.880
The easiest way to
be sure of doing this

00:11:36.880 --> 00:11:39.300
properly is to have a
simple partitioning scheme,

00:11:39.300 --> 00:11:41.250
with a single partition
for the volume,

00:11:41.250 --> 00:11:44.370
and to format that partition
using the Google-provided Safe

00:11:44.370 --> 00:11:46.110
Format and Mount script.

00:11:46.110 --> 00:11:47.900
The operating system
and application stack

00:11:47.900 --> 00:11:50.920
should do everything
else for you.

00:11:50.920 --> 00:11:53.702
Here's a syntax for that
Safe Format and Mount script.

00:11:53.702 --> 00:11:55.660
And keep in mind, if you
don't use that script,

00:11:55.660 --> 00:11:57.610
and you format your file
systems on your own,

00:11:57.610 --> 00:12:00.000
please make sure to
align on 4K boundaries.

00:12:00.000 --> 00:12:01.970
And if you mount your
file systems on your own,

00:12:01.970 --> 00:12:04.460
please make sure to mount
with the Discard option.

00:12:04.460 --> 00:12:07.520
The Discard option makes
format times much faster,

00:12:07.520 --> 00:12:10.300
makes snapshots
smaller, faster to take,

00:12:10.300 --> 00:12:14.232
and makes storing those
snapshots less expensive.

00:12:14.232 --> 00:12:15.690
The next performance
recommendation

00:12:15.690 --> 00:12:19.650
is about tuning your application
properly for your I/O pattern.

00:12:19.650 --> 00:12:21.030
Starting with IOPS.

00:12:21.030 --> 00:12:22.880
PD attains its high
IOPS performance

00:12:22.880 --> 00:12:25.360
through the use of
parallelism on the back end.

00:12:25.360 --> 00:12:28.490
Therefore, for higher IOPS, you
may need a larger queue depth,

00:12:28.490 --> 00:12:30.320
or application thread pool.

00:12:30.320 --> 00:12:32.290
The more I/Os you
can issue at once,

00:12:32.290 --> 00:12:35.220
the faster the PD volumes
can return the data.

00:12:35.220 --> 00:12:38.415
You'll need a queue depth of
one for every 400 to 800 IOPS

00:12:38.415 --> 00:12:42.050
you require, up to a
maximum queue depth of 64.

00:12:42.050 --> 00:12:43.796
Consult the application
documentation

00:12:43.796 --> 00:12:45.170
on how to configure
this, as it's

00:12:45.170 --> 00:12:47.370
different for every application.

00:12:47.370 --> 00:12:49.430
Now, if you need higher
throughput, as opposed

00:12:49.430 --> 00:12:52.090
to higher IOPS, there are
different recommendations.

00:12:52.090 --> 00:12:54.520
When you need high throughput,
the best thing to do

00:12:54.520 --> 00:12:58.930
is to increase the I/O size
to 256 kilobytes or greater.

00:12:58.930 --> 00:13:01.490
Also, increasing the
OS read-ahead cache,

00:13:01.490 --> 00:13:04.550
using the command line here,
will improve streaming I/O--

00:13:04.550 --> 00:13:06.740
especially if you're
forced by the application,

00:13:06.740 --> 00:13:09.440
for some reason, to
use small block sizes.

00:13:09.440 --> 00:13:12.870
Please note that we intend
for the values on this slide

00:13:12.870 --> 00:13:15.730
to change over time, as we
improve Persistent Disk.

00:13:15.730 --> 00:13:18.720
The concepts described here
will stay true, but consult

00:13:18.720 --> 00:13:20.720
the Persistent Disk
documentation for the latest

00:13:20.720 --> 00:13:23.190
values.

00:13:23.190 --> 00:13:25.219
Another performance
tip is around CPU.

00:13:25.219 --> 00:13:27.010
It's important to note
that Google does not

00:13:27.010 --> 00:13:30.420
require special instance type
for getting high I/O. Larger

00:13:30.420 --> 00:13:34.460
VMs can get higher I/O by
virtue of their more CPUs.

00:13:34.460 --> 00:13:37.320
A quick rule of thumb is
to have one CPU core free

00:13:37.320 --> 00:13:40.780
for every 2,000 to 2,500 IOPS.

00:13:40.780 --> 00:13:42.780
As with the last slide,
we intend this value

00:13:42.780 --> 00:13:45.790
to change as well over time,
as make Persistent Disk better.

00:13:45.790 --> 00:13:49.800
Please consult the documentation
for the latest value.

00:13:49.800 --> 00:13:52.130
Like I mentioned before,
the larger a PD volume

00:13:52.130 --> 00:13:54.090
is, the more I/O it can do.

00:13:54.090 --> 00:13:56.430
So you never need to
create many small volumes

00:13:56.430 --> 00:13:58.590
and stripe them together
to increase performance.

00:13:58.590 --> 00:14:02.080
For example, one terabyte volume
will give the same performance

00:14:02.080 --> 00:14:05.370
as 10 100-gigabyte
volumes striped together.

00:14:05.370 --> 00:14:07.947
And because we support volumes
up to 10 terabytes in size,

00:14:07.947 --> 00:14:09.530
you don't never need
to stripe volumes

00:14:09.530 --> 00:14:11.000
together for space, either.

00:14:11.000 --> 00:14:16.080
One data volume per VM is
all you should ever need.

00:14:16.080 --> 00:14:17.440
Last performance tip.

00:14:17.440 --> 00:14:18.910
Sometimes people
mistakenly think

00:14:18.910 --> 00:14:21.590
there's a need to
pre-warm PD volumes.

00:14:21.590 --> 00:14:24.980
This is not needed, as PD
volumes perform consistently

00:14:24.980 --> 00:14:27.440
over time, and are ready
for their top performance

00:14:27.440 --> 00:14:28.930
as soon as they're created.

00:14:28.930 --> 00:14:31.140
Trying to pre-warm
the volumes will only

00:14:31.140 --> 00:14:33.570
waste time and resources.

00:14:33.570 --> 00:14:36.100
PD volumes are also consistent
relative to one another.

00:14:36.100 --> 00:14:39.432
There's never a need to create
many volumes, test their speed,

00:14:39.432 --> 00:14:40.890
throw away the ones
you don't like,

00:14:40.890 --> 00:14:42.500
and keep the ones you do like.

00:14:42.500 --> 00:14:45.370
All the volumes of the
same size and the same type

00:14:45.370 --> 00:14:48.450
will perform the same way.

00:14:48.450 --> 00:14:50.870
So now let's move from
performance to protection.

00:14:50.870 --> 00:14:55.480
As you know, mistakes can
happen, bugs can happen,

00:14:55.480 --> 00:14:58.010
and disasters will
eventually happen.

00:14:58.010 --> 00:14:59.730
All of which can
cause you lose data

00:14:59.730 --> 00:15:02.130
if you don't take the
proper precautions.

00:15:02.130 --> 00:15:05.090
When using PD, please
take snapshots frequently.

00:15:05.090 --> 00:15:07.590
This will protect you against
software and human errors that

00:15:07.590 --> 00:15:09.700
may delete data, and
will also protect you

00:15:09.700 --> 00:15:11.570
from physical disasters.

00:15:11.570 --> 00:15:14.430
Because PD has
differential snapshots,

00:15:14.430 --> 00:15:16.310
snapshot time and
storage costs will

00:15:16.310 --> 00:15:18.130
be as efficient as possible.

00:15:18.130 --> 00:15:20.850
Also, snapshots will never
perceptibly pause your VM,

00:15:20.850 --> 00:15:23.880
and will not affect
your applications.

00:15:23.880 --> 00:15:25.800
Moving away from PD
for just a second,

00:15:25.800 --> 00:15:29.020
remember that local SSD does
not have the snapshot feature.

00:15:29.020 --> 00:15:30.850
So make sure when
using local SSD,

00:15:30.850 --> 00:15:32.890
if you need to protect
some of that data,

00:15:32.890 --> 00:15:36.530
that you use some backup
mechanism supported by the OS

00:15:36.530 --> 00:15:38.800
in application.

00:15:38.800 --> 00:15:41.820
The last tip we have
is about moving data.

00:15:41.820 --> 00:15:43.960
If you want to move
Persistent Disk volumes

00:15:43.960 --> 00:15:46.960
from one zone to another
zone, the easiest thing to do

00:15:46.960 --> 00:15:50.690
is to snapshot the PD volume
into Google Cloud Storage,

00:15:50.690 --> 00:15:53.717
and then restore it
to the other zone.

00:15:53.717 --> 00:15:55.800
Now, if you need to move
individual pieces of data

00:15:55.800 --> 00:15:59.790
from a non-PD source into
a VM, into the PD volume,

00:15:59.790 --> 00:16:02.220
you can use the
gcloud push command.

00:16:02.220 --> 00:16:05.190
To do the reverse, and move
data from a PD volume and a VM

00:16:05.190 --> 00:16:08.820
to a non-PD destination, you
can use the gcloud pull command.

00:16:12.380 --> 00:16:15.340
So here's a quick summary
of what we discussed.

00:16:15.340 --> 00:16:19.850
First, Google's block storage
is unique in its combination

00:16:19.850 --> 00:16:22.870
of reliability, security
features, and performance

00:16:22.870 --> 00:16:24.400
consistency.

00:16:24.400 --> 00:16:27.570
Second, Google's
block storage options

00:16:27.570 --> 00:16:30.990
offer a unique continuum,
with all of the block storage

00:16:30.990 --> 00:16:33.610
options scaling performance
with volume size.

00:16:33.610 --> 00:16:35.530
This way, the block
storage options

00:16:35.530 --> 00:16:38.180
can be compared in an
apples to apples manner,

00:16:38.180 --> 00:16:41.090
and decisions made simply,
mostly based on the performance

00:16:41.090 --> 00:16:42.710
needs.

00:16:42.710 --> 00:16:46.520
Third, all the options have
a simple price per gigabyte,

00:16:46.520 --> 00:16:49.210
never charging extra
for I/O. You can always

00:16:49.210 --> 00:16:52.490
be assured that each option's
complete cost is understood

00:16:52.490 --> 00:16:56.160
up front, with no
surprises later on.

00:16:56.160 --> 00:16:58.860
Lastly, please refer back to
this presentation for our best

00:16:58.860 --> 00:17:01.690
practices on how to choose
between block storage types,

00:17:01.690 --> 00:17:04.795
and how to optimize I/O. If
you follow these guidelines,

00:17:04.795 --> 00:17:07.170
you should be able to get
consistently superb performance

00:17:07.170 --> 00:17:09.235
at the lowest possible price.

00:17:09.235 --> 00:17:11.359
I encourage you to try out
all of our block storage

00:17:11.359 --> 00:17:13.020
options, especially
the two new ones

00:17:13.020 --> 00:17:15.040
we're so excited
to announce here.

00:17:15.040 --> 00:17:17.200
If you'd like to
follow up on any

00:17:17.200 --> 00:17:19.839
of this presentation
for more detail,

00:17:19.839 --> 00:17:23.410
please contact me personally,
or contact the bigger teams

00:17:23.410 --> 00:17:27.130
using the forums listed here.

00:17:27.130 --> 00:17:29.830
Thank you very
much for watching.

