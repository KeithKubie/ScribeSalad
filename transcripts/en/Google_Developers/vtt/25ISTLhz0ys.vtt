WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.450
[MUSIC PLAYING]

00:00:05.717 --> 00:00:06.550
KAZ SATO: Thank you.

00:00:06.550 --> 00:00:09.590
Thank you for attending
our session, "Android Meets

00:00:09.590 --> 00:00:10.140
TensorFlow."

00:00:10.140 --> 00:00:11.860
I'm Kaz Sato.

00:00:11.860 --> 00:00:14.750
I'm a Developer Advocate for
Google Cloud Platform team,

00:00:14.750 --> 00:00:15.760
and--

00:00:15.760 --> 00:00:16.570
HAK MATSUDA: Hello.

00:00:16.570 --> 00:00:17.600
Good morning.

00:00:17.600 --> 00:00:21.460
Thanks for coming in a
very, very early morning.

00:00:21.460 --> 00:00:24.770
I'm not a morning person,
so I'm a bit sleepy now.

00:00:24.770 --> 00:00:27.410
Anyway, my name
is Hakuro Matsuda.

00:00:27.410 --> 00:00:30.690
I am a developer advocate
of Android gaming

00:00:30.690 --> 00:00:34.490
and native technology.

00:00:34.490 --> 00:00:36.100
KAZ SATO: So today,
in this session,

00:00:36.100 --> 00:00:38.780
we'd like to discuss
these topics.

00:00:38.780 --> 00:00:41.260
So the first part
of this session,

00:00:41.260 --> 00:00:44.830
I will be discussing, what
is AI, machine learning,

00:00:44.830 --> 00:00:46.510
neural network,
and deep learning,

00:00:46.510 --> 00:00:49.540
and how Google has been using
these kinds of technologies

00:00:49.540 --> 00:00:51.640
for implementing our services.

00:00:51.640 --> 00:00:53.620
And what is the TensorFlow?

00:00:53.620 --> 00:00:55.900
That is an open-source
library for building

00:00:55.900 --> 00:00:57.790
your own neural network.

00:00:57.790 --> 00:00:59.810
And then, I will pass
the stage to Hak.

00:00:59.810 --> 00:01:03.230
He'll be discussing how
you can build the Android

00:01:03.230 --> 00:01:05.050
applications powered
by TensorFlow

00:01:05.050 --> 00:01:07.030
and how you can optimize it.

00:01:07.030 --> 00:01:10.150
And lastly, I'd like
to do a sneak preview

00:01:10.150 --> 00:01:14.560
of the new technologies, such
as TensorFlow Lite and Android

00:01:14.560 --> 00:01:18.010
neural network API.

00:01:18.010 --> 00:01:20.950
So what is machine learning
and neural networks?

00:01:20.950 --> 00:01:24.610
How many people actually tried
neural networks by yourselves?

00:01:24.610 --> 00:01:25.590
How many people?

00:01:25.590 --> 00:01:27.180
Oh, so many people!

00:01:27.180 --> 00:01:29.470
Like, 20%?

00:01:29.470 --> 00:01:32.170
And how many people have
actually used a neural network

00:01:32.170 --> 00:01:34.430
on mobile devices?

00:01:34.430 --> 00:01:36.550
Oh, thank you so much.

00:01:36.550 --> 00:01:39.350
I found, like, 10 people.

00:01:39.350 --> 00:01:43.900
Today, we'd like to learn how
you can use the TensorFlow

00:01:43.900 --> 00:01:46.380
and machine learning
module running

00:01:46.380 --> 00:01:49.570
inside mobile applications.

00:01:49.570 --> 00:01:51.400
There have been
so many buzzwords

00:01:51.400 --> 00:01:55.330
such as AI, machine
learning, neural network,

00:01:55.330 --> 00:01:56.710
or deep learning.

00:01:56.710 --> 00:01:58.870
We have been hearing
about those buzzwords

00:01:58.870 --> 00:02:00.490
for the last few years.

00:02:00.490 --> 00:02:02.050
What's the difference?

00:02:02.050 --> 00:02:05.470
The AI or Artificial
Intelligence-- you can say that

00:02:05.470 --> 00:02:10.389
is a science for making
smart things like building

00:02:10.389 --> 00:02:13.225
an autonomous
driving car or having

00:02:13.225 --> 00:02:17.530
a computer drawing a beautiful
picture or composing music.

00:02:17.530 --> 00:02:21.970
And one way to realize
that vision of AI

00:02:21.970 --> 00:02:23.620
is in machine learning.

00:02:23.620 --> 00:02:26.110
Machine learning is
a technology where

00:02:26.110 --> 00:02:31.330
you can let a computer train
itself, rather than having

00:02:31.330 --> 00:02:34.690
human programmers
instructing every step,

00:02:34.690 --> 00:02:39.770
to process the data by itself.

00:02:39.770 --> 00:02:43.300
And one of the many different
algorithms in machine learning

00:02:43.300 --> 00:02:44.920
is neural network.

00:02:44.920 --> 00:02:49.270
And since around 2012, we have
been seeing a big break-through

00:02:49.270 --> 00:02:51.230
in the world of
the neural network,

00:02:51.230 --> 00:02:54.520
especially for image
recognition, voice recognition,

00:02:54.520 --> 00:02:58.720
or natural language processing
and many other applications.

00:02:58.720 --> 00:03:00.580
And at Google, we
have been focusing

00:03:00.580 --> 00:03:07.400
on developing the neural network
technologies for several years.

00:03:07.400 --> 00:03:08.980
So what is a neural network?

00:03:08.980 --> 00:03:12.250
You can think of it just like
a function in mathematics

00:03:12.250 --> 00:03:14.540
or a function in
programming language.

00:03:14.540 --> 00:03:17.500
So you can put any kind
of data as an input

00:03:17.500 --> 00:03:22.730
and do some matrix
operations or calculations

00:03:22.730 --> 00:03:24.010
inside neural networks.

00:03:24.010 --> 00:03:27.190
Then, eventually, you
would get an output vector

00:03:27.190 --> 00:03:30.580
which has the many labels
or speculative values.

00:03:30.580 --> 00:03:33.610
For example, if you
have a bunch of images,

00:03:33.610 --> 00:03:35.950
you can train the neural
network to classify

00:03:35.950 --> 00:03:39.730
which one is the image of a
cat or the image of a dog.

00:03:39.730 --> 00:03:43.270
And this is just one
example of the use

00:03:43.270 --> 00:03:44.380
cases of neural networks.

00:03:44.380 --> 00:03:48.160
You can apply the technology
to solve any kind of business

00:03:48.160 --> 00:03:49.720
problems you have.

00:03:49.720 --> 00:03:53.540
For example, if you have
a bunch of gaming servers,

00:03:53.540 --> 00:03:57.250
then you can try converting all
the user activities, the player

00:03:57.250 --> 00:04:00.290
activities, convert them
into a bunch of numbers,

00:04:00.290 --> 00:04:03.490
such as vectors, and try
training the neural networks

00:04:03.490 --> 00:04:07.840
to classify the type of
players on your gaming server.

00:04:07.840 --> 00:04:11.350
For example, if you want to
find any cheating player who

00:04:11.350 --> 00:04:13.270
will be using the
automated script to

00:04:13.270 --> 00:04:16.000
try to cheat on
your server, or you

00:04:16.000 --> 00:04:19.690
can try to use a neural network
to find the premium players who

00:04:19.690 --> 00:04:23.530
could be buying more and more
items from your gaming server.

00:04:23.530 --> 00:04:26.590
So that is just one
possible example

00:04:26.590 --> 00:04:30.070
of the possible thousands
of applications.

00:04:30.070 --> 00:04:33.190
At Google, we have been using
deep learning technologies

00:04:33.190 --> 00:04:38.170
for implementing the smart
functions on over 100

00:04:38.170 --> 00:04:42.310
production projects, including
Google Search, Android, Play,

00:04:42.310 --> 00:04:44.360
and many other applications.

00:04:44.360 --> 00:04:47.120
For example, if you are using
Google Search every day,

00:04:47.120 --> 00:04:49.780
that means you are accessing
the deep learning technologies

00:04:49.780 --> 00:04:51.850
provided by Google every day.

00:04:51.850 --> 00:04:56.230
Because in 2015, we
introduced RankBrain,

00:04:56.230 --> 00:05:00.220
which is a deep learning-based
ranking algorithm that

00:05:00.220 --> 00:05:03.110
generates the signals for
the defining and ranking

00:05:03.110 --> 00:05:06.470
of the search results.

00:05:06.470 --> 00:05:08.720
And if you take a look at
the mobile applications

00:05:08.720 --> 00:05:10.130
from Google--

00:05:10.130 --> 00:05:11.770
for example, Google
Photos is one

00:05:11.770 --> 00:05:14.410
of the most successful
mobile applications that

00:05:14.410 --> 00:05:18.120
has been using deep learning
for analyzing and understanding

00:05:18.120 --> 00:05:21.340
the content of each image
taken by your smartphones,

00:05:21.340 --> 00:05:25.000
so you don't have to put any
tags or keywords by yourself.

00:05:25.000 --> 00:05:28.280
Instead, you can just type
dog, your friend's name,

00:05:28.280 --> 00:05:34.120
or the wedding party to find the
images based on this content.

00:05:34.120 --> 00:05:39.010
Smart Reply is a feature that
shows the options to reply

00:05:39.010 --> 00:05:40.960
to each email thread.

00:05:40.960 --> 00:05:43.690
So it uses natural
language processing

00:05:43.690 --> 00:05:45.700
powered by the
neural network model

00:05:45.700 --> 00:05:50.470
to try to understand the
context of the email exchanges.

00:05:50.470 --> 00:05:54.220
And now, over 12%
of the responses

00:05:54.220 --> 00:05:57.560
are generated by the
Smart Reply features.

00:05:57.560 --> 00:06:01.080
So you can say that now, email
has been written by computers

00:06:01.080 --> 00:06:05.490
and not by humans anymore.

00:06:05.490 --> 00:06:07.680
Google Translate app
recently introduced

00:06:07.680 --> 00:06:11.340
a new neural
translation model that

00:06:11.340 --> 00:06:15.070
has improved the quality,
especially the fluency,

00:06:15.070 --> 00:06:17.620
of the translated
text significantly.

00:06:17.620 --> 00:06:22.050
So there are so many
possible use cases

00:06:22.050 --> 00:06:24.570
for the combination
between machine learning

00:06:24.570 --> 00:06:26.550
and mobile
applications, starting

00:06:26.550 --> 00:06:30.180
from image recognition,
OCR, speech-to-text

00:06:30.180 --> 00:06:34.470
and text-to-speech,
translation, and NLP.

00:06:34.470 --> 00:06:37.440
And especially, you can
apply machine learning

00:06:37.440 --> 00:06:42.560
to mobile-specific applications
such as motion detection or GPS

00:06:42.560 --> 00:06:45.840
location tracking.

00:06:45.840 --> 00:06:49.202
And why do you want
to run your machine

00:06:49.202 --> 00:06:50.660
learning model,
your neural network

00:06:50.660 --> 00:06:52.770
model inside your
mobile applications?

00:06:52.770 --> 00:06:55.710
Because by using the
machine learnings,

00:06:55.710 --> 00:06:58.890
you can reduce the
significant amount of traffic,

00:06:58.890 --> 00:07:00.900
and you can get much
faster responses

00:07:00.900 --> 00:07:02.880
from your Cloud services.

00:07:02.880 --> 00:07:07.200
Because you can extract the
meaning from the raw data.

00:07:07.200 --> 00:07:08.704
What do I mean by that?

00:07:08.704 --> 00:07:10.620
For example, if you are
using machine learning

00:07:10.620 --> 00:07:13.140
for image recognition,
the easiest way

00:07:13.140 --> 00:07:17.250
to implement that is to
send all the raw image data

00:07:17.250 --> 00:07:20.620
taken by the camera
to the server.

00:07:20.620 --> 00:07:24.000
But instead, you can have the
machine learning model running

00:07:24.000 --> 00:07:28.620
inside your mobile application
so that your mobile application

00:07:28.620 --> 00:07:32.730
can recognize what kind of
object is in each image,

00:07:32.730 --> 00:07:36.080
so that you can just send
the label, such as flower

00:07:36.080 --> 00:07:38.850
or human face, to the server.

00:07:38.850 --> 00:07:42.360
That can reduce the
traffic to 1/10 or 1/100.

00:07:42.360 --> 00:07:46.920
It's a significant amount
of savings of traffic.

00:07:46.920 --> 00:07:50.430
Another example could
be motion detection.

00:07:50.430 --> 00:07:56.280
So here, gathering all
the motion, sensory data--

00:07:56.280 --> 00:08:00.100
not sending just the raw
images directly to the server,

00:08:00.100 --> 00:08:02.720
you can use machine learning
algorithms to extract

00:08:02.720 --> 00:08:05.640
the so-called feature vectors.

00:08:05.640 --> 00:08:08.430
Feature vector is just
a bunch of numbers,

00:08:08.430 --> 00:08:14.220
like 100 or 1,000 numbers that
represent the characteristic

00:08:14.220 --> 00:08:20.430
or signature of the motions
from the motion sensors.

00:08:20.430 --> 00:08:24.270
So you can just send the
100 or a 1,000 numbers

00:08:24.270 --> 00:08:26.750
in the feature
vector to the server.

00:08:29.790 --> 00:08:34.020
And what is the
starting point to build

00:08:34.020 --> 00:08:37.490
your own mobile application that
is powered by machine learning?

00:08:37.490 --> 00:08:39.780
The starting point
could be the TensorFlow,

00:08:39.780 --> 00:08:41.850
which is the open-source
library for machine

00:08:41.850 --> 00:08:45.540
intelligence from Google.

00:08:45.540 --> 00:08:47.580
And TensorFlow is
the latest framework

00:08:47.580 --> 00:08:51.120
for building machine learning
or AI-based services developed

00:08:51.120 --> 00:08:51.950
in Google.

00:08:51.950 --> 00:08:56.130
And we open sourced
it in November 2015.

00:08:56.130 --> 00:08:59.140
And now, TensorFlow is
the most popular framework

00:08:59.140 --> 00:09:01.650
for building neural networks
or deep learning in the world.

00:09:04.410 --> 00:09:06.510
And one benefit you
could get with TensorFlow

00:09:06.510 --> 00:09:07.950
is ease of development.

00:09:07.950 --> 00:09:10.780
So it's really easy
to get started.

00:09:10.780 --> 00:09:13.200
You can just write a few
lines of Python code,

00:09:13.200 --> 00:09:16.530
or tens of lines of Python code,
to define your neural network

00:09:16.530 --> 00:09:17.580
by yourself.

00:09:17.580 --> 00:09:23.010
And actually, this technology
is very valuable for people

00:09:23.010 --> 00:09:27.240
like me, because I don't have
any sophisticated mathematical

00:09:27.240 --> 00:09:27.910
background.

00:09:27.910 --> 00:09:32.860
So when I started reading the
textbook on neural networks,

00:09:32.860 --> 00:09:37.190
I found many mathematical
equations on the textbook,

00:09:37.190 --> 00:09:39.550
like differentiation,
back propagation,

00:09:39.550 --> 00:09:40.790
and gradient descent.

00:09:40.790 --> 00:09:46.610
And I really didn't want to
implement everything by myself.

00:09:46.610 --> 00:09:49.980
Instead, now, you can
just download TensorFlow,

00:09:49.980 --> 00:09:53.060
where you can write a
single line of Python code,

00:09:53.060 --> 00:09:56.310
like GradientDescentOptimizer.

00:09:56.310 --> 00:09:58.770
That single line of
code can encapsulate

00:09:58.770 --> 00:10:02.580
all these obfuscated algorithms
such as gradient descent,

00:10:02.580 --> 00:10:06.090
back propagation, or any other
latest algorithms implemented

00:10:06.090 --> 00:10:07.800
by the Google engineers.

00:10:07.800 --> 00:10:10.260
So you yourself
don't have to have

00:10:10.260 --> 00:10:12.810
the skill set to implement
the neural network

00:10:12.810 --> 00:10:14.040
technologies from scratch.

00:10:16.590 --> 00:10:20.340
Also, the benefits
of the TensorFlow

00:10:20.340 --> 00:10:22.780
is the portability
and scalability.

00:10:22.780 --> 00:10:25.230
For example, if
you're just getting

00:10:25.230 --> 00:10:27.870
started with the technology,
you can download TensorFlow,

00:10:27.870 --> 00:10:29.970
only on Mac or
Windows, where you

00:10:29.970 --> 00:10:33.840
can play with the Hello World
kind of very simple samples.

00:10:33.840 --> 00:10:36.780
But if you are getting serious
about the technologies--

00:10:36.780 --> 00:10:39.210
for example, if you want to
render a model from scratch

00:10:39.210 --> 00:10:42.520
to recognize the image of a
cat, then you may want to use

00:10:42.520 --> 00:10:44.080
the GPU server--

00:10:44.080 --> 00:10:47.100
because GPU is,
like, 10 or 50 times

00:10:47.100 --> 00:10:50.020
faster than CPU or
Mac or Windows--

00:10:50.020 --> 00:10:52.410
to train your model.

00:10:52.410 --> 00:10:55.860
But many large companies,
like Google Earth

00:10:55.860 --> 00:10:58.290
or any other
enterprises, are using

00:10:58.290 --> 00:11:02.400
tens or sometimes hundreds
of GPUs running on the cloud

00:11:02.400 --> 00:11:05.520
because the computing
power is the largest

00:11:05.520 --> 00:11:08.630
challenge for the deep
learning technologies.

00:11:08.630 --> 00:11:12.060
But still, you don't have
to make any major changes

00:11:12.060 --> 00:11:14.490
on your TensorFlow
neural networks,

00:11:14.490 --> 00:11:17.500
because TensorFlow is
designed to be scalable.

00:11:17.500 --> 00:11:19.890
So once you have defined
your neural networks,

00:11:19.890 --> 00:11:22.800
you can learn it,
train it, and use it

00:11:22.800 --> 00:11:25.210
on a single CPU,
or multiple GPUs,

00:11:25.210 --> 00:11:29.070
or hundreds of GPUs, or TPU,
or Tensor Processing Unit,

00:11:29.070 --> 00:11:36.270
which is the ASIC or customized
ADSI designed by Google.

00:11:36.270 --> 00:11:38.910
And once you have finished
training your model,

00:11:38.910 --> 00:11:42.130
you can copy the
model, which has--

00:11:42.130 --> 00:11:44.190
for example, for
image recognition,

00:11:44.190 --> 00:11:46.950
a single model can
consist of 100 megabytes

00:11:46.950 --> 00:11:48.990
of data, the parameters.

00:11:48.990 --> 00:11:51.450
You can copy those parameters
to the mobile devices

00:11:51.450 --> 00:11:55.320
such as Android,
iOS, or Raspberry Pi.

00:11:55.320 --> 00:11:59.340
And if you go to the
TensorFlow.org website,

00:11:59.340 --> 00:12:03.390
you can find the sample code
for the embedded systems

00:12:03.390 --> 00:12:06.560
and mobile phones.

00:12:06.560 --> 00:12:09.070
And then, the benefit you
could get from TensorFlow

00:12:09.070 --> 00:12:12.070
is the community and ecosystems.

00:12:12.070 --> 00:12:16.000
So if you want to find any
practical production quality

00:12:16.000 --> 00:12:19.159
solutions, then the
TensorFlow can provide

00:12:19.159 --> 00:12:20.200
the best answer for that.

00:12:20.200 --> 00:12:23.830
Because there are so many large
enterprises and developers

00:12:23.830 --> 00:12:27.470
who are using TensorFlow
for serious development,

00:12:27.470 --> 00:12:31.780
such as Airbus, ARM, eBay,
Intel, Dropbox, and Twitter.

00:12:31.780 --> 00:12:36.310
They're all using TensorFlow.

00:12:36.310 --> 00:12:38.670
Now, I'd like to pass
the stage to Hak,

00:12:38.670 --> 00:12:41.780
who will be talking about how
you can implement the Android

00:12:41.780 --> 00:12:45.560
applications powered
by TensorFlow.

00:12:45.560 --> 00:12:46.060
[APPLAUSE]

00:12:46.060 --> 00:12:46.846
Oh, thank you.

00:12:50.950 --> 00:12:52.710
HAK MATSUDA: Thank you, Kazu.

00:12:52.710 --> 00:12:55.890
So let's move on to
the Android part.

00:12:55.890 --> 00:12:59.390
As Kazu mentioned,
we have found a lot

00:12:59.390 --> 00:13:02.550
of great use cases
of running TensorFlow

00:13:02.550 --> 00:13:05.250
inference on mobile devices.

00:13:05.250 --> 00:13:07.780
Let's take a look at
how we can integrate

00:13:07.780 --> 00:13:11.010
TensorFlow inference
on mobile devices

00:13:11.010 --> 00:13:13.410
and how we can optimize it.

00:13:16.610 --> 00:13:20.720
TensorFlow supports multiple
mobile platforms including

00:13:20.720 --> 00:13:24.470
Android, iOS, and Raspberry Pi.

00:13:24.470 --> 00:13:29.330
In this talk, we'd like to focus
on mobile devices like Android

00:13:29.330 --> 00:13:29.870
and iOS.

00:13:34.450 --> 00:13:37.770
Building a TensorFlow
shared object from scratch

00:13:37.770 --> 00:13:39.360
was a bit tricky.

00:13:39.360 --> 00:13:42.840
It required multiple
steps, starting

00:13:42.840 --> 00:13:47.980
from [INAUDIBLE] from GitHub,
installing Bazel, installing

00:13:47.980 --> 00:13:51.120
Android Studio,
Android SDK and NDK,

00:13:51.120 --> 00:13:54.360
and finally editing a
setting file, et cetera.

00:13:57.730 --> 00:14:01.120
But we have good news today.

00:14:01.120 --> 00:14:05.740
Announced this IO, we just
added a JSON integration,

00:14:05.740 --> 00:14:09.594
which makes those steps
a lot, a lot easier.

00:14:09.594 --> 00:14:11.070
[APPLAUSE]

00:14:11.070 --> 00:14:12.546
Thank you, thank you.

00:14:16.000 --> 00:14:18.830
Just add one line
to the build Gradle,

00:14:18.830 --> 00:14:22.980
and the Gradle takes care
of the rest of steps.

00:14:22.980 --> 00:14:25.620
Under the library archive,
[INAUDIBLE] TensorFlow

00:14:25.620 --> 00:14:29.680
shared object is
downloaded from JCenter,

00:14:29.680 --> 00:14:32.470
linked against the
application automatically.

00:14:35.020 --> 00:14:38.245
Also, you can fetch
prebuilt model

00:14:38.245 --> 00:14:41.830
files, such as the
inception, stylize,

00:14:41.830 --> 00:14:43.750
et cetera, from
the cloud as well.

00:14:48.230 --> 00:14:52.010
And it's easier for iOS as well.

00:14:52.010 --> 00:14:55.490
We just started a Cocoapod
integration as well.

00:14:55.490 --> 00:14:57.000
It's quite simple now.

00:15:02.310 --> 00:15:06.960
Let's take a look at how you
can use the TensorFlow API.

00:15:06.960 --> 00:15:10.200
We released Android
inference library

00:15:10.200 --> 00:15:13.812
to integrate TensorFlow
for Java application.

00:15:13.812 --> 00:15:17.500
The library is the
[INAUDIBLE] from Java

00:15:17.500 --> 00:15:19.650
to the native implementation.

00:15:19.650 --> 00:15:21.630
And the performance
impact is minimal.

00:15:24.300 --> 00:15:28.250
At first, create TensorFlow
inference interface,

00:15:28.250 --> 00:15:32.790
opening the model file
from the asset in the APK.

00:15:32.790 --> 00:15:38.760
Then, set up the input
feed using feed API.

00:15:38.760 --> 00:15:42.280
On mobile, the input
feed tends to be

00:15:42.280 --> 00:15:46.680
retrieved from various sensors
like a camera, accelerometer,

00:15:46.680 --> 00:15:48.420
et cetera.

00:15:48.420 --> 00:15:50.070
Then, run the inference.

00:15:50.070 --> 00:15:53.130
And finally, you can
fetch the results

00:15:53.130 --> 00:15:55.140
using fetch method over there.

00:15:58.600 --> 00:16:03.670
You would notice that those
calls are all blocking calls.

00:16:03.670 --> 00:16:08.440
So you'd want to run them in
a worker thread, rather than

00:16:08.440 --> 00:16:12.550
the main thread, because API
would take a long, long time,

00:16:12.550 --> 00:16:13.580
like several seconds.

00:16:16.580 --> 00:16:19.040
This one is Java API.

00:16:19.040 --> 00:16:23.240
And of course, you can use
regular C++ API as well,

00:16:23.240 --> 00:16:26.330
if you love C++ as I do.

00:16:33.841 --> 00:16:34.340
OK.

00:16:34.340 --> 00:16:36.490
Let's move on to the demo.

00:16:54.270 --> 00:16:58.050
This one is a TensorFlow
sample running on Android.

00:16:58.050 --> 00:17:00.530
The sample has three modes.

00:17:00.530 --> 00:17:03.910
The first one is running
Inception v3 that

00:17:03.910 --> 00:17:06.569
classifies the camera image.

00:17:06.569 --> 00:17:10.869
And also, we have a classified
face and the stylized photo

00:17:10.869 --> 00:17:11.690
sample.

00:17:11.690 --> 00:17:14.140
This one is a
stylized photo that

00:17:14.140 --> 00:17:22.626
is applying the artistic
filter on the camera preview.

00:17:22.626 --> 00:17:25.021
[APPLAUSE]

00:17:27.416 --> 00:17:28.380
Thank you.

00:17:28.380 --> 00:17:32.580
And one special
thing on the demo

00:17:32.580 --> 00:17:37.590
is that I tweaked the demo
a little bit using the GPU,

00:17:37.590 --> 00:17:41.100
using [INAUDIBLE]
Compute Shader.

00:17:41.100 --> 00:17:44.700
[INAUDIBLE] sample
just supports the CPU

00:17:44.700 --> 00:17:46.190
and the new optimization.

00:17:46.190 --> 00:17:50.280
But I tweaked that
using the GPU.

00:17:50.280 --> 00:17:56.180
And this one was just for
experiment and just for fun.

00:17:56.180 --> 00:17:57.480
That was pretty fun.

00:17:57.480 --> 00:18:02.990
And I learned a lot to optimize
the TensorFlow for GPU.

00:18:02.990 --> 00:18:09.280
Basically, on Android devices,
performance limiting factor

00:18:09.280 --> 00:18:12.300
is mostly coming from
the memory bandwidth

00:18:12.300 --> 00:18:16.950
rather than computing itself, so
that reducing memory bandwidth

00:18:16.950 --> 00:18:19.920
helped a lot.

00:18:19.920 --> 00:18:23.790
For instance, in some
[INAUDIBLE] to the kernel,

00:18:23.790 --> 00:18:29.940
it was fetching 32 by
32 by 32 by 4 samples,

00:18:29.940 --> 00:18:34.350
just to generate one
output value, which

00:18:34.350 --> 00:18:39.160
is a huge amount of data from
the viewpoint of the Compute

00:18:39.160 --> 00:18:40.710
Shader.

00:18:40.710 --> 00:18:45.600
So memory bandwidth is key,
crucial for the Android

00:18:45.600 --> 00:18:49.250
and mobile device optimization.

00:18:49.250 --> 00:18:52.470
Anyway, everybody can
tweak TensorFlow code

00:18:52.470 --> 00:18:54.790
because it's open-sourced.

00:18:54.790 --> 00:18:56.660
It's the beauty of
open source, right?

00:19:01.710 --> 00:19:05.430
So now, we can integrate
TensorFlow inference on mobile

00:19:05.430 --> 00:19:08.700
quite easily, as I explained.

00:19:08.700 --> 00:19:14.080
However, there will still be
challenges in performance.

00:19:14.080 --> 00:19:16.020
Even the mobile
device performance

00:19:16.020 --> 00:19:18.970
has been increased
significantly.

00:19:18.970 --> 00:19:24.940
It has less computing power than
Cloud or desktop counterpart.

00:19:24.940 --> 00:19:28.010
Also, it has limited
RAM, which is

00:19:28.010 --> 00:19:31.820
a precious resource on mobile.

00:19:31.820 --> 00:19:36.590
Let's say, if the application
takes one gigabyte of RAM,

00:19:36.590 --> 00:19:40.200
then the application
is highly likely

00:19:40.200 --> 00:19:42.420
to be killed by
the system itself

00:19:42.420 --> 00:19:45.200
when the application
goes to the background.

00:19:45.200 --> 00:19:48.270
That's not a happy
situation, right?

00:19:48.270 --> 00:19:52.140
So let's take a look at how
we can optimize TensorFlow

00:19:52.140 --> 00:19:55.350
graphics, reducing
memory footprint,

00:19:55.350 --> 00:19:58.910
increasing runtime performance,
and improving load time as

00:19:58.910 --> 00:19:59.410
well.

00:20:05.180 --> 00:20:09.600
This one is the model of
the Inception v3 model.

00:20:09.600 --> 00:20:13.750
The model takes around
91 megabytes in storage,

00:20:13.750 --> 00:20:16.190
with 25 million parameters.

00:20:16.190 --> 00:20:20.610
And the binary size
would take 12 megabytes.

00:20:20.610 --> 00:20:21.695
That is huge.

00:20:27.940 --> 00:20:31.630
And we have multiple techniques
to optimize the graph,

00:20:31.630 --> 00:20:35.950
such as freezing the graph,
using the Graph Transform

00:20:35.950 --> 00:20:40.850
Tool, quantization,
memory mapping, et cetera.

00:20:40.850 --> 00:20:41.770
Let's go through.

00:20:45.260 --> 00:20:47.920
Freezing graph is
one of the load

00:20:47.920 --> 00:20:51.740
time optimizations, which
convert variable node

00:20:51.740 --> 00:20:54.170
into constant node.

00:20:54.170 --> 00:20:55.910
What's a variable node?

00:20:55.910 --> 00:21:01.100
In TensorFlow, variable node
starts in a different file.

00:21:01.100 --> 00:21:07.680
But the constant
node is included

00:21:07.680 --> 00:21:09.940
in the graph def itself.

00:21:09.940 --> 00:21:13.610
So moving a variable
into the constant node

00:21:13.610 --> 00:21:17.665
can concatenate multiple
files into one file,

00:21:17.665 --> 00:21:20.890
like [INAUDIBLE].

00:21:20.890 --> 00:21:22.720
That would be a
slight performance

00:21:22.720 --> 00:21:27.430
win in mobile and
easier to handle.

00:21:27.430 --> 00:21:31.075
To do that, we prepare the
freeze graph dot Python script.

00:21:37.230 --> 00:21:41.290
And the graph transform
tool is your friend.

00:21:41.290 --> 00:21:43.830
The tool supports various
optimization tasks,

00:21:43.830 --> 00:21:48.270
such as stripping unused
nodes for inference.

00:21:48.270 --> 00:21:52.770
But it's used only in
the learning phrase

00:21:52.770 --> 00:21:56.380
when learning the inference
node, so it's not necessary.

00:21:59.586 --> 00:22:04.440
I think that currently, it
would require some minor steps

00:22:04.440 --> 00:22:07.920
to determine which
node is the start node

00:22:07.920 --> 00:22:10.530
and which node is
the output node.

00:22:10.530 --> 00:22:13.820
So the tool requires
both start and output

00:22:13.820 --> 00:22:15.200
points specified manually.

00:22:22.810 --> 00:22:26.450
Let's talk about quantization.

00:22:26.450 --> 00:22:29.530
Neural network operation
requires a bunch

00:22:29.530 --> 00:22:34.670
of matrix characterizations,
which means tons of multiply

00:22:34.670 --> 00:22:37.280
and add operations.

00:22:37.280 --> 00:22:41.270
Current mobile devices are
capable of doing some of them

00:22:41.270 --> 00:22:43.130
with specialized hardware.

00:22:43.130 --> 00:22:45.740
For instance,
[INAUDIBLE] instruction

00:22:45.740 --> 00:22:48.440
in CPU, general-purpose
computing

00:22:48.440 --> 00:22:52.310
in GPU, DSP, et cetera.

00:22:52.310 --> 00:22:56.480
Roughly, on mobile CPU,
it can perform around

00:22:56.480 --> 00:23:00.190
10 to 20 gigabyte
FLOPS in total.

00:23:00.190 --> 00:23:07.510
And using GPU, it would achieve
300 to 500 gigaFLOPS or more.

00:23:07.510 --> 00:23:10.560
That sounds like a great
number, but is still

00:23:10.560 --> 00:23:13.520
based on a desktop
or server environment

00:23:13.520 --> 00:23:17.144
so that we want to
perform some optimization.

00:23:22.090 --> 00:23:23.920
Quantization is one
of the techniques

00:23:23.920 --> 00:23:28.560
to reduce both memory
footprint and computer load.

00:23:28.560 --> 00:23:32.770
Usually, TensorFlow takes a
single precision floating value

00:23:32.770 --> 00:23:38.440
for input and math, and
also output as well.

00:23:38.440 --> 00:23:40.870
As you know, single
precision floating point

00:23:40.870 --> 00:23:44.620
takes 32 bits each.

00:23:44.620 --> 00:23:51.140
But we found that we can reduce
the precision to 16 bits, 8

00:23:51.140 --> 00:23:56.340
bits, or even less while
keeping a good result,

00:23:56.340 --> 00:23:59.700
just because our
learning process involves

00:23:59.700 --> 00:24:01.550
some noise by nature.

00:24:01.550 --> 00:24:06.420
And adding some extra
noises wouldn't matter much.

00:24:09.540 --> 00:24:13.080
So quantized weight
is the optimization

00:24:13.080 --> 00:24:16.150
for storage size, which
reduces the precision

00:24:16.150 --> 00:24:20.670
of the constant node
in the graph file.

00:24:20.670 --> 00:24:27.020
But please note that with the
quantized weight optimization,

00:24:27.020 --> 00:24:30.580
the value will be
expanded on memory when

00:24:30.580 --> 00:24:34.380
the graph is loaded.

00:24:34.380 --> 00:24:36.910
So we have another optimization.

00:24:36.910 --> 00:24:41.440
We can call it the
quantized calculations.

00:24:41.440 --> 00:24:44.790
With the quantized
calculations, we

00:24:44.790 --> 00:24:49.620
can reduce computing precision
by using the quantized value

00:24:49.620 --> 00:24:51.620
directory.

00:24:51.620 --> 00:24:54.810
This one is good for
first memory bandwidth,

00:24:54.810 --> 00:24:58.060
which is a limiting
factor in mobile devices.

00:24:58.060 --> 00:25:02.550
Also, hardware can handle
these precision values faster

00:25:02.550 --> 00:25:04.921
than single precision
floating values.

00:25:07.810 --> 00:25:10.540
But we still have an open issue.

00:25:10.540 --> 00:25:14.680
To do the quantized
calculation, we

00:25:14.680 --> 00:25:19.060
need the maximum value
and the minimum value

00:25:19.060 --> 00:25:24.356
that specify the range
of the quantized values.

00:25:24.356 --> 00:25:28.810
We still don't have a
great solution for that.

00:25:28.810 --> 00:25:30.400
It's still manual.

00:25:30.400 --> 00:25:36.380
But active research is going on
so that hopefully, this issue

00:25:36.380 --> 00:25:38.070
will be resolved pretty soon.

00:25:42.050 --> 00:25:45.460
This one is an example of how
quantized characterization

00:25:45.460 --> 00:25:48.330
optimization works
in TensorFlow.

00:25:48.330 --> 00:25:52.670
TensorFlow has some operations
that support quantization.

00:25:52.670 --> 00:25:56.810
For instance,
convolution to the matrix

00:25:56.810 --> 00:26:00.380
multiply [INAUDIBLE], et cetera.

00:26:00.380 --> 00:26:03.080
We think that's
good enough to cover

00:26:03.080 --> 00:26:05.460
most of inference scenarios.

00:26:05.460 --> 00:26:10.340
However, we still don't have
order of operations quantized

00:26:10.340 --> 00:26:16.850
yet, so that we need to quantize
the value and dequantize value

00:26:16.850 --> 00:26:22.800
output right before
and after each node.

00:26:22.800 --> 00:26:27.440
And the graph transform
tool analyzes parts

00:26:27.440 --> 00:26:29.320
of each graph node.

00:26:29.320 --> 00:26:33.690
And sometimes, we can reduce
unnecessary quantized and

00:26:33.690 --> 00:26:35.150
dequantized values.

00:26:41.890 --> 00:26:46.290
Memory mapping is yet another
optimization for loading time.

00:26:46.290 --> 00:26:49.110
With this optimization,
the model file

00:26:49.110 --> 00:26:52.950
is converted and can
be mapped directly

00:26:52.950 --> 00:26:57.250
using the memmap API,
which could be slightly

00:26:57.250 --> 00:27:02.260
performance optimization on
some Linux-based operating

00:27:02.260 --> 00:27:03.300
systems like Android.

00:27:09.760 --> 00:27:13.680
Another one is reducing
executable size.

00:27:13.680 --> 00:27:15.250
That's an important
topic in mobile

00:27:15.250 --> 00:27:19.890
as well, because
on mobile devices,

00:27:19.890 --> 00:27:23.980
executable package size is
limited to a specific size--

00:27:23.980 --> 00:27:29.710
for instance, 100
megabytes for Android,

00:27:29.710 --> 00:27:34.030
including binary, graphics,
and any other resources.

00:27:37.490 --> 00:27:41.780
By default, a mobile device
supports multiple selected

00:27:41.780 --> 00:27:44.420
operations that are
mostly good enough

00:27:44.420 --> 00:27:49.550
to cover inference operations,
but ops used learning process

00:27:49.550 --> 00:27:50.090
is missing.

00:27:50.090 --> 00:27:54.590
So if you want to do the
learning on a mobile device,

00:27:54.590 --> 00:27:59.480
you need to register
extra operations.

00:27:59.480 --> 00:28:02.650
Also, if your graph
wouldn't require

00:28:02.650 --> 00:28:05.360
some preregistered
operations, you

00:28:05.360 --> 00:28:08.510
would also remove some of them.

00:28:08.510 --> 00:28:13.540
To do that, you can do
selective registration.

00:28:16.320 --> 00:28:19.290
So for instance,
for Inception v3,

00:28:19.290 --> 00:28:23.180
by doing the selective
registration,

00:28:23.180 --> 00:28:27.480
the original binary
size was 12 megabytes.

00:28:27.480 --> 00:28:29.940
And after the
optimization, it can

00:28:29.940 --> 00:28:34.940
be reduced to 1.5 megabytes.

00:28:34.940 --> 00:28:39.950
Note that this optimization
requires rebuilding the shared

00:28:39.950 --> 00:28:42.040
object in your local.

00:28:42.040 --> 00:28:47.600
So you would need to construct
the build environment as well.

00:28:51.140 --> 00:28:57.100
So with these optimizations,
Inception v3 graph

00:28:57.100 --> 00:29:02.350
now becomes 23 megabytes
and 1.5 megabytes

00:29:02.350 --> 00:29:06.880
in binary size, which
is 75% smaller now.

00:29:10.100 --> 00:29:12.249
Let's get back to Kazu.

00:29:12.249 --> 00:29:13.290
KAZ SATO: Thank you, Hak.

00:29:13.290 --> 00:29:15.590
So as Hak mentioned,
there are so many--

00:29:15.590 --> 00:29:17.084
[APPLAUSE]

00:29:17.084 --> 00:29:18.080
Thank you.

00:29:23.060 --> 00:29:26.600
As Hak mentioned, there
are so many tips and tricks

00:29:26.600 --> 00:29:29.720
to optimize your
TensorFlow model

00:29:29.720 --> 00:29:33.680
to squeeze into an Android
mobile application.

00:29:33.680 --> 00:29:35.690
And that is what you
can do right now.

00:29:35.690 --> 00:29:37.700
These techniques are
available right now.

00:29:37.700 --> 00:29:40.280
But now, I'd like to
discuss a little bit

00:29:40.280 --> 00:29:43.710
about the new technologies
coming in the near future,

00:29:43.710 --> 00:29:48.980
such as TensorFlow Lite and
Android Neural Network API.

00:29:48.980 --> 00:29:51.360
What is NNAPI?

00:29:51.360 --> 00:29:54.980
It's a new API for neural
network processings

00:29:54.980 --> 00:30:00.650
inside Android, and that will be
added to the Android framework.

00:30:00.650 --> 00:30:03.880
And the purpose of
adding a new API

00:30:03.880 --> 00:30:07.610
is to encapsulate and
have an abstraction

00:30:07.610 --> 00:30:11.900
layer for the hardware
accelerators such as GPU, DSP,

00:30:11.900 --> 00:30:14.030
and ISP.

00:30:14.030 --> 00:30:17.750
And modern smartphones have such
a powerful computing resource

00:30:17.750 --> 00:30:20.915
other than CPU,
such as GPU or DSP.

00:30:20.915 --> 00:30:23.750
Especially the DSP
is designed to do

00:30:23.750 --> 00:30:27.500
a massive amount of the matrix
and vector calculations,

00:30:27.500 --> 00:30:31.550
so it's much faster to
use DSP or GPUs to do

00:30:31.550 --> 00:30:35.480
the neural networks inference
on it, rather than using a CPU.

00:30:35.480 --> 00:30:38.370
But right now, if
you want to do that,

00:30:38.370 --> 00:30:42.470
you have to go directly
into the library provided

00:30:42.470 --> 00:30:45.660
by the hardware vendors
and build some binaries

00:30:45.660 --> 00:30:46.580
by yourself.

00:30:46.580 --> 00:30:48.230
It's a tedious task.

00:30:48.230 --> 00:30:50.690
And also, it's not portable.

00:30:50.690 --> 00:30:55.010
So instead, we'll be providing a
standard API so that developers

00:30:55.010 --> 00:30:59.650
don't have to be aware of any
of the hardware accelerators

00:30:59.650 --> 00:31:01.155
from each individual vendor.

00:31:04.200 --> 00:31:06.560
On top of the
Neural Network API,

00:31:06.560 --> 00:31:08.660
we will be providing
TensorFlow Lite.

00:31:08.660 --> 00:31:10.380
That will be a new
TensorFlow runtime

00:31:10.380 --> 00:31:14.260
optimized for mobile and
embedded applications.

00:31:14.260 --> 00:31:18.870
So TensorFlow Lite is designed
for smart and compact mobile

00:31:18.870 --> 00:31:20.580
or embedded applications.

00:31:20.580 --> 00:31:23.600
And also, it is designed to
be combined with the Android

00:31:23.600 --> 00:31:25.290
NNAPI.

00:31:25.290 --> 00:31:27.570
So all you have
to do is to build

00:31:27.570 --> 00:31:31.260
your model with TensorFlow
Lite, and that's it.

00:31:31.260 --> 00:31:34.020
Eventually, you will be
getting all the benefits

00:31:34.020 --> 00:31:36.120
you could get from
the Android NNAPI,

00:31:36.120 --> 00:31:39.360
such as the hardware
acceleration.

00:31:39.360 --> 00:31:41.920
That will be coming as an open
source in the near future,

00:31:41.920 --> 00:31:43.590
so stay tuned.

00:31:43.590 --> 00:31:47.150
And if you're interested in
these new coming technologies,

00:31:47.150 --> 00:31:49.440
please take a photograph
of this QR code,

00:31:49.440 --> 00:31:56.010
so that you can join our
survey for the ML on Android,

00:31:56.010 --> 00:31:59.940
where you can give your
feedback or requests

00:31:59.940 --> 00:32:00.855
for the new products.

00:32:09.460 --> 00:32:11.610
OK, thank you.

00:32:11.610 --> 00:32:15.360
So lastly, I'd like to show
some very interesting and fun

00:32:15.360 --> 00:32:17.630
real-world applications
built with TensorFlow

00:32:17.630 --> 00:32:19.890
on mobile and embedded systems.

00:32:19.890 --> 00:32:23.820
The first application is running
on the Raspberry Pi, built

00:32:23.820 --> 00:32:27.600
by a Japanese cucumber farmer.

00:32:27.600 --> 00:32:30.840
Actually, I myself
took this photograph.

00:32:30.840 --> 00:32:34.710
I went there to the cucumber
farm and took this photograph.

00:32:34.710 --> 00:32:38.990
And they have, you can see--

00:32:38.990 --> 00:32:40.686
oh, sorry.

00:32:40.686 --> 00:32:42.660
[CHUCKLE] There's no pointer.

00:32:42.660 --> 00:32:44.940
So you have the one
person in the middle.

00:32:44.940 --> 00:32:48.490
He's [INAUDIBLE].

00:32:48.490 --> 00:32:53.160
He started helping with
cucumber farming two years ago.

00:32:53.160 --> 00:32:59.760
And he found out that sorting
cucumbers into correct classes

00:32:59.760 --> 00:33:01.620
is the most tedious task.

00:33:01.620 --> 00:33:03.960
His mother spent
eight hours a day

00:33:03.960 --> 00:33:08.820
classifying each cucumber based
on its shape, length, or color,

00:33:08.820 --> 00:33:11.820
into nine different
classes, and he really

00:33:11.820 --> 00:33:14.740
didn't want to help her.

00:33:14.740 --> 00:33:17.190
So instead, he
downloaded TensorFlow

00:33:17.190 --> 00:33:20.002
and built his own
cucumber sorter.

00:33:20.002 --> 00:33:22.457
[APPLAUSE]

00:33:27.860 --> 00:33:31.105
So what he did is he
took 9,000 photographs

00:33:31.105 --> 00:33:33.390
of the different
cucumbers, and put

00:33:33.390 --> 00:33:36.720
the labels classified
by his mother,

00:33:36.720 --> 00:33:39.450
and trained the TensorFlow
model by himself.

00:33:39.450 --> 00:33:46.440
And he built the sorter robotics
by himself, by spending $1,500.

00:33:46.440 --> 00:33:50.760
And the TensorFlow model is
running on the Raspberry Pi

00:33:50.760 --> 00:33:54.030
to detect the cucumbers
put on the plate.

00:33:54.030 --> 00:33:57.810
And it can classify cucumbers
into nine different classes,

00:33:57.810 --> 00:34:00.870
based on its shape and colors.

00:34:00.870 --> 00:34:03.390
And this is a system
diagram of the systems.

00:34:03.390 --> 00:34:05.580
So it has three parts.

00:34:05.580 --> 00:34:08.760
The Arduino Micro is used
for controlling the servers

00:34:08.760 --> 00:34:10.050
and models.

00:34:10.050 --> 00:34:14.820
And Raspberry Pi has a camera to
take a picture of the cucumbers

00:34:14.820 --> 00:34:15.900
on the plate.

00:34:15.900 --> 00:34:18.780
And it runs a very small
TensorFlow model on it.

00:34:18.780 --> 00:34:21.120
And this is actually
a very great example

00:34:21.120 --> 00:34:23.460
of how you can split
the tasks, the work

00:34:23.460 --> 00:34:28.260
load for machine learning into
an edge device and cloud part.

00:34:28.260 --> 00:34:32.300
Because he found out running
the whole set of the TensorFlow

00:34:32.300 --> 00:34:34.920
model inside the Raspberry
Pi is too heavyweight.

00:34:34.920 --> 00:34:37.770
So he decided to split
it into two tasks.

00:34:37.770 --> 00:34:41.010
So the TensorFlow model
running on Raspberry Pi only

00:34:41.010 --> 00:34:45.580
detects whether there's a
cucumber on the plate or not.

00:34:45.580 --> 00:34:48.989
And only when it detects
there's a cucumber on the plate,

00:34:48.989 --> 00:34:51.030
it sends the picture
to the server,

00:34:51.030 --> 00:34:52.900
where he has a more
powerful TensorFlow

00:34:52.900 --> 00:34:54.945
model that can classify
the cucumber into nine

00:34:54.945 --> 00:34:55.694
different classes.

00:34:58.370 --> 00:35:01.090
Let's take a look at another
interesting application

00:35:01.090 --> 00:35:05.700
running on Android and iOS,
that is the Gymnastic Exercise

00:35:05.700 --> 00:35:07.200
Scorer.

00:35:07.200 --> 00:35:08.270
What is that?

00:35:08.270 --> 00:35:12.470
Actually, every Japanese
knows the exercise

00:35:12.470 --> 00:35:16.850
called radio exercise, because
we have a national radio

00:35:16.850 --> 00:35:18.660
broadcasting network.

00:35:18.660 --> 00:35:22.340
They play the same exercise
music at the same time

00:35:22.340 --> 00:35:23.720
every morning.

00:35:23.720 --> 00:35:25.760
And tens of millions
of Japanese people

00:35:25.760 --> 00:35:29.840
are doing the same
exercise every morning.

00:35:29.840 --> 00:35:30.590
Did you know that?

00:35:30.590 --> 00:35:34.110
[LAUGHTER]

00:35:34.110 --> 00:35:37.290
This application
is designed to--

00:35:37.290 --> 00:35:38.870
we call it the
scorer-- so how well

00:35:38.870 --> 00:35:42.170
you have been doing your
exercise with the music.

00:35:42.170 --> 00:35:48.080
And to capture the motions
with the motion sensor,

00:35:48.080 --> 00:35:50.780
they have used TensorFlow.

00:35:50.780 --> 00:35:54.140
And they were able to train
the TensorFlow model to capture

00:35:54.140 --> 00:35:57.350
and extract the patterns
or features from the data

00:35:57.350 --> 00:35:59.720
from the motion
sensors, so that it

00:35:59.720 --> 00:36:04.760
is able to evaluate the
motions made by the human hand.

00:36:04.760 --> 00:36:08.750
And they also built their
own TensorFlow compiler

00:36:08.750 --> 00:36:10.880
so that they were able
to apply the techniques

00:36:10.880 --> 00:36:14.360
such as the quantization
or approximations.

00:36:14.360 --> 00:36:17.640
And they were able to
reduce the TensorFlow

00:36:17.640 --> 00:36:20.570
model from tens of megabytes
into a few megabytes.

00:36:20.570 --> 00:36:25.550
So that was the key technology
to build the production quality

00:36:25.550 --> 00:36:30.532
Android and iOS applications
with the TensorFlow power.

00:36:30.532 --> 00:36:32.990
Let's take a look at the live
demonstration of the Exercise

00:36:32.990 --> 00:36:34.640
Scorer.

00:36:34.640 --> 00:36:36.710
So can I switch to here?

00:36:47.120 --> 00:36:49.100
This is the application
where you can choose

00:36:49.100 --> 00:36:51.600
the various kinds of exercises.

00:36:51.600 --> 00:36:54.170
And I'll be playing
the most standard one.

00:36:54.170 --> 00:36:55.850
[MUSIC PLAYING]

00:36:55.850 --> 00:36:57.192
So this is the music.

00:36:57.192 --> 00:36:58.124
[LAUGHTER]

00:36:58.124 --> 00:37:00.660
[JAPANESE SPEECH FROM EXERCISE
 SCORER]

00:37:14.616 --> 00:37:18.025
[APPLAUSE]

00:37:27.280 --> 00:37:28.476
Am I doing all right?

00:37:39.765 --> 00:37:40.870
So just like that--

00:37:40.870 --> 00:37:42.067
so let's stop.

00:37:42.067 --> 00:37:44.929
[LAUGHTER AND APPLAUSE]

00:37:49.699 --> 00:37:51.571
[MUSIC STOPS ABRUPTLY]

00:37:51.571 --> 00:37:52.070
It's enough.

00:37:52.070 --> 00:37:52.750
It's enough.

00:37:52.750 --> 00:37:54.650
[LAUGHTER]

00:37:54.650 --> 00:37:57.020
Now, the TensorFlow
model is trying

00:37:57.020 --> 00:38:00.160
to evaluate how well you
have done this exercise.

00:38:00.160 --> 00:38:02.270
And you can see
the bar chart here.

00:38:02.270 --> 00:38:04.470
That is the variation
by the TensorFlow

00:38:04.470 --> 00:38:05.900
model inside this application.

00:38:05.900 --> 00:38:07.670
It's a real thing.

00:38:07.670 --> 00:38:10.370
OK?

00:38:10.370 --> 00:38:12.160
Go back to the slide.

00:38:16.570 --> 00:38:20.680
That was the thing
we wanted to show.

00:38:20.680 --> 00:38:23.510
In this session, we have
learned many things, including

00:38:23.510 --> 00:38:27.590
yet another weird
thing from Japan,

00:38:27.590 --> 00:38:29.210
and then some
optimization techniques

00:38:29.210 --> 00:38:31.620
for building
TensorFlow applications

00:38:31.620 --> 00:38:35.270
with the production quality
Android and iOS applications.

00:38:35.270 --> 00:38:38.840
And if you're interested,
please go to TensorFlow.org,

00:38:38.840 --> 00:38:41.750
where you can have lots of
getting-started materials.

00:38:41.750 --> 00:38:45.310
And there are some
of the good codelabs

00:38:45.310 --> 00:38:48.330
on the codelab website.

00:38:48.330 --> 00:38:49.221
Thank you so much.

00:38:49.221 --> 00:38:50.179
HAK MATSUDA: Thank you.

00:38:50.179 --> 00:38:53.265
[APPLAUSE]

