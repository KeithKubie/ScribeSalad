WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:01.420
So, Charles, I kind of cheated.

00:00:01.420 --> 00:00:02.630
&gt;&gt; Oh, tell me more.

00:00:02.630 --> 00:00:04.750
&gt;&gt; So, Q-learning isn't really an

00:00:04.750 --> 00:00:07.130
algorithm. Q-leaning is actually a family of

00:00:07.130 --> 00:00:10.604
algorithms. There's lots of different reinforcement learning

00:00:10.604 --> 00:00:14.432
algorithms, specific reinforcement learning algorithms that can

00:00:14.432 --> 00:00:16.940
be reasonably called Q-learning, and they

00:00:16.940 --> 00:00:20.440
vary typically along these three dimensions. How

00:00:20.440 --> 00:00:25.308
do we initialize our estimate, Q hat, how do we decay our learning rates,

00:00:25.308 --> 00:00:29.530
alpha sub-t? And how do we choose actions during learning?

00:00:29.530 --> 00:00:30.740
&gt;&gt; Hm.

00:00:30.740 --> 00:00:33.440
&gt;&gt; And different ways of making

00:00:33.440 --> 00:00:35.540
these choices actually lead to algorithms with

00:00:35.540 --> 00:00:39.990
fairly different behaviors. In particular when we use this in the context of

00:00:39.990 --> 00:00:42.660
an MDP, well let's, let me, let me ask you. So like, what

00:00:42.660 --> 00:00:47.590
do you think might matter about let's start with the last one, choosing actions?

00:00:47.590 --> 00:00:50.510
&gt;&gt; Well. It see, well there's a bunch of dumb

00:00:50.510 --> 00:00:52.544
things you could do, right? You could just, just pick an

00:00:52.544 --> 00:00:56.030
amption, action, action every single time, like the same action every single

00:00:56.030 --> 00:00:59.180
time independent of what you learned, that's kind of dumb. But,

00:00:59.180 --> 00:01:02.020
it seems like the obvious smart thing to do is say look,

00:01:02.020 --> 00:01:04.129
I'm learning, I'm getting better and better, so what I'm going to

00:01:04.129 --> 00:01:07.360
do is at this next time steps is I actually have to

00:01:07.360 --> 00:01:10.060
take an action. I'll just pick the action that my Q

00:01:10.060 --> 00:01:12.890
hat tells me is the best action to take. And I'm done.

00:01:12.890 --> 00:01:15.593
&gt;&gt; So all right, let me, let me see if I can capture some of what you

00:01:15.593 --> 00:01:18.690
just said there. So, one way to choose actions

00:01:18.690 --> 00:01:22.170
really badly is say, pick some action, call it a

00:01:22.170 --> 00:01:23.850
sub 0. And no matter what state your in.

00:01:23.850 --> 00:01:26.040
No matter what's happen so far, always choose that action.

00:01:26.040 --> 00:01:26.590
&gt;&gt; Mm-hm.

00:01:26.590 --> 00:01:27.730
&gt;&gt; Right, so this possibly can't

00:01:27.730 --> 00:01:30.340
work. It's going to violate the Q-learning convergence

00:01:30.340 --> 00:01:33.970
that says we have to visit each state action pair infinitely often, and update

00:01:33.970 --> 00:01:37.180
them to converge and you know and, and it makes perfect sense. Like if

00:01:37.180 --> 00:01:40.720
we never try something, like how do you know that you don't like something

00:01:40.720 --> 00:01:42.060
if you've never even tried it.

00:01:42.060 --> 00:01:43.014
&gt;&gt; Like spinach.

00:01:43.014 --> 00:01:46.200
&gt;&gt; Exactly. Another idea would be to choose randomly.

00:01:46.200 --> 00:01:48.160
And this seems kind of good and that we are going

00:01:48.160 --> 00:01:50.720
to visit. You know, all the states that are visitable

00:01:50.720 --> 00:01:53.980
and we will try all the actions that are actionable.

00:01:53.980 --> 00:01:58.100
And we could actually learn Q this way. But, as

00:01:58.100 --> 00:02:00.380
you pointed out, this is not a great idea because

00:02:00.380 --> 00:02:02.570
we may have learned Q, but we haven't really used

00:02:02.570 --> 00:02:06.140
it. We haven't really chosen actions using what we've learned,

00:02:06.140 --> 00:02:09.255
so it's like we're wise but we are impotent.

00:02:09.255 --> 00:02:13.020
&gt;&gt; No, I think it's more like we're wise but we're stupid. I mean.

00:02:13.020 --> 00:02:14.380
&gt;&gt; We're wise but we still. We know a

00:02:14.380 --> 00:02:16.570
lot but refuse to actually do anything about it.

00:02:16.570 --> 00:02:20.060
&gt;&gt; Right. In particular in some sense the only difference between the two

00:02:20.060 --> 00:02:24.784
is the the theorem, right? If I, if I'm just choosing randomly. Wha, what's

00:02:24.784 --> 00:02:26.500
the problem with them always choosing a

00:02:26.500 --> 00:02:28.710
sub 0. Well, you're don't going to converge

00:02:28.710 --> 00:02:31.160
but the real problem is that you don't learn anything. Or you don't take

00:02:31.160 --> 00:02:33.660
advantage of anything you learned. Choosing randomly is basically the same

00:02:33.660 --> 00:02:36.630
thing, you basically, you never take advantage of anything that you learn.

00:02:36.630 --> 00:02:39.178
What's the point in learning a Q function if you are always

00:02:39.178 --> 00:02:41.310
going to behave randomly, you've learned enough

00:02:41.310 --> 00:02:43.065
or you've learned but you [CROSSTALK].

00:02:43.065 --> 00:02:46.330
&gt;&gt; [CROSSTALK] right you've actually learned the ultra policy but you're

00:02:46.330 --> 00:02:48.840
not following it so you're not actually using what you know,

00:02:48.840 --> 00:02:51.930
so you can't you know it doesn't, it doesn't work all

00:02:51.930 --> 00:02:56.450
right. So then you had another idea. Which was to use

00:02:56.450 --> 00:02:58.200
our estimate to choose actions.

00:02:58.200 --> 00:02:59.800
&gt;&gt; Yeah.

00:02:59.800 --> 00:03:02.630
&gt;&gt; And that seems like a good idea in that

00:03:02.630 --> 00:03:05.770
we will use it. Is it possible that it won't learn?

00:03:05.770 --> 00:03:07.950
&gt;&gt; Well, it will learn something.

00:03:07.950 --> 00:03:11.330
&gt;&gt; Well, yeah. It might not learn anything all that good though.

00:03:11.330 --> 00:03:14.090
So for example, what if we do something like this. So we

00:03:14.090 --> 00:03:16.740
initialize, now we're back up to this, this first point here. We

00:03:16.740 --> 00:03:22.150
initialize the, the estimate Q hat so that for every state, a0 looks

00:03:22.150 --> 00:03:24.500
awesome and all the other actions look terrible.

00:03:24.500 --> 00:03:27.770
&gt;&gt; Wait [INAUDIBLE] is that metric awesome? Or English awesome?

00:03:27.770 --> 00:03:29.740
&gt;&gt; Oh you're right I'm sorry, I didn't

00:03:29.740 --> 00:03:32.335
put units on that. That's in Chilean dollars.

00:03:32.335 --> 00:03:32.830
&gt;&gt; [LAUGH]

00:03:32.830 --> 00:03:36.680
&gt;&gt; Oh okay, so it's pretty awesome then. Okay. So, if

00:03:36.680 --> 00:03:40.140
you do that, then well let's see what happens. It's almost

00:03:40.140 --> 00:03:43.380
like always taking a0. The only thing that would save you

00:03:43.380 --> 00:03:47.610
from taking a0 forever, is if, as you take a0, you learn

00:03:47.610 --> 00:03:50.840
that, you update your Qs and you keep getting really, really

00:03:50.840 --> 00:03:54.769
bad results. Really, really bad results, in fact, worse than terrible.

00:03:55.970 --> 00:03:58.620
&gt;&gt; yes. Well let's imagine the terrible is worse than terrible.

00:03:58.620 --> 00:03:59.130
&gt;&gt; Oh.

00:03:59.130 --> 00:04:02.060
&gt;&gt; So, but you're right, yeah you're right. So, so there's, there's

00:04:02.060 --> 00:04:05.270
at least the case that if this, if this terrible value is

00:04:05.270 --> 00:04:09.040
actually lower than the value of always choosing a0, then we'll continue

00:04:09.040 --> 00:04:13.408
to use a0 forever. This is, this is called the greedy action

00:04:13.408 --> 00:04:15.694
selection strategy.

00:04:15.694 --> 00:04:17.149
&gt;&gt; Mm-mh.

00:04:17.149 --> 00:04:20.700
&gt;&gt; And that, this is the problem that runs into, it's a kind of local min.

00:04:20.700 --> 00:04:24.650
&gt;&gt; Oh, I see. Oh and oh, okay, I see that. So, you, you didn't even have

00:04:24.650 --> 00:04:26.240
to come up with this ridiculous example. If

00:04:26.240 --> 00:04:29.010
you, if you well, not ridiculous, but you, extremely.

00:04:29.010 --> 00:04:30.900
&gt;&gt; I was going to say, I'm, I'm sorry, ridiculous how?

00:04:30.900 --> 00:04:33.370
&gt;&gt; Well, I'm sorry. It, it's a ridiculous situation

00:04:33.370 --> 00:04:37.040
to be in. It's sort of the extremely unluicky example.

00:04:37.040 --> 00:04:38.510
&gt;&gt; I think what you're saying is

00:04:38.510 --> 00:04:39.760
that you don't like people from Chile.

00:04:39.760 --> 00:04:41.682
&gt;&gt; [LAUGH] Oh no I love people from, I love

00:04:41.682 --> 00:04:45.285
Chile. Especially with you know, just some good beans and some

00:04:45.285 --> 00:04:48.490
nice meat. But the thing is that you, if you randomly

00:04:48.490 --> 00:04:51.210
set your Q hat in such a way that a bad

00:04:51.210 --> 00:04:55.310
action or let's say a suboptimal action ends up looking much

00:04:55.310 --> 00:04:58.510
better to begin with then the optimal action. You can get

00:04:58.510 --> 00:05:01.320
in a situation where you keep choosing the wrong action anyway

00:05:01.320 --> 00:05:03.610
and so you are only going to learn things that reinforce that

00:05:03.610 --> 00:05:07.300
action which might be good just not optimal. So you won't actually end up

00:05:07.300 --> 00:05:10.770
converging onto the true Q hat and that's why you get into a local min.

00:05:10.770 --> 00:05:13.410
&gt;&gt; That's right and so in that bad situation and you,

00:05:13.410 --> 00:05:17.420
I admit it's a little contrived because I've never, I've never even

00:05:17.420 --> 00:05:21.350
been to Chile is that it wont learn. It'd actually be

00:05:21.350 --> 00:05:24.020
exactly the same as this first case, always choose a sub 0.

00:05:24.020 --> 00:05:25.281
&gt;&gt; Mm-hm.

00:05:25.281 --> 00:05:28.820
&gt;&gt; So, that seems problematic too and it interacts in an interesting

00:05:28.820 --> 00:05:35.580
way with the initialization. Right. So maybe we can do this idea of using

00:05:35.580 --> 00:05:38.560
Q hat, but we have to be much more careful about how we initialize.

00:05:38.560 --> 00:05:43.870
&gt;&gt; Hm. So, you know, what I want to do is something like random restarts.

