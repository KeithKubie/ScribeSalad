WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:02.530
So now you have a small neural network.

00:00:02.530 --> 00:00:05.370
It's not particularly deep,
just two layers.

00:00:05.370 --> 00:00:07.440
You can make it bigger, more complex,

00:00:07.440 --> 00:00:10.330
by increasing the size of that
hidden layer in the middle.

00:00:10.330 --> 00:00:13.370
But it turns out that increasing
this H is not particularly

00:00:13.370 --> 00:00:15.020
efficient in general.

00:00:15.020 --> 00:00:19.170
You need to make it very, very big,
and then it gets really hard to train.

00:00:19.170 --> 00:00:22.820
This is where the central idea of
deep learning comes into play.

00:00:22.820 --> 00:00:26.360
Instead, you can also add more
layers and make your model deeper.

00:00:26.360 --> 00:00:28.550
There are lots of good
reasons to do that.

00:00:28.550 --> 00:00:30.570
One is parameter efficiency.

00:00:30.570 --> 00:00:34.320
You can typically get much more
performance with fewer parameters

00:00:34.320 --> 00:00:36.430
by going deeper rather than wider.

00:00:37.510 --> 00:00:40.340
Another one is that a lot of
natural phenomena that you might be

00:00:40.340 --> 00:00:41.350
interested in,

00:00:41.350 --> 00:00:45.920
tend to have a hierarchical structure
which deep models naturally capture.

00:00:45.920 --> 00:00:48.120
If you poke at a model for
images, for example, and

00:00:48.120 --> 00:00:50.230
visualize what the model learns,

00:00:50.230 --> 00:00:55.350
you'll often find very simple things at
the lowest layers, like lines or edges.

00:00:55.350 --> 00:00:56.450
Once you move up,

00:00:56.450 --> 00:00:59.820
you tend to see more complicated
things like geometric shapes.

00:00:59.820 --> 00:01:03.660
Go further up and you start seeing
things like objects, faces.

00:01:03.660 --> 00:01:07.095
This is very powerful because the model
structure matches the kind of

00:01:07.095 --> 00:01:10.050
abstractions that you might
expect to see in your data.

00:01:10.050 --> 00:01:12.910
And as a result the model has
an easier time learning them.

