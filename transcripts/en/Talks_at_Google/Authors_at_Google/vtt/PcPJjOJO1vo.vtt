WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.335
[MUSIC PLAYING]

00:00:05.790 --> 00:00:08.188
SPEAKER: Please join me in
welcoming Jamie Susskind.

00:00:08.188 --> 00:00:08.688
Thank you.

00:00:08.688 --> 00:00:11.058
[APPLAUSE]

00:00:12.452 --> 00:00:14.160
JAMIE SUSSKIND: Thank
you all, very much.

00:00:14.160 --> 00:00:17.340
There's a story that's told of
an encounter that took place

00:00:17.340 --> 00:00:20.970
in the 19th century between the
great Prime Minister William

00:00:20.970 --> 00:00:25.140
Gladstone and the
scientist Michael Faraday.

00:00:25.140 --> 00:00:28.020
And Faraday were
showing Gladstone

00:00:28.020 --> 00:00:29.940
the invention of
electricity, which

00:00:29.940 --> 00:00:32.225
Gladstone hadn't seen before.

00:00:32.225 --> 00:00:34.350
And the prime minister was
looking at it curiously.

00:00:34.350 --> 00:00:39.210
And he said to Faraday,
well, what does it do?

00:00:39.210 --> 00:00:40.980
What use is it?

00:00:40.980 --> 00:00:43.590
And Faraday gave an
explanation as to what

00:00:43.590 --> 00:00:45.750
he thought the scientific
implications of it were

00:00:45.750 --> 00:00:48.489
and how it marked a great
advance in that respect.

00:00:48.489 --> 00:00:49.780
But Gladstone wasn't convinced.

00:00:49.780 --> 00:00:52.320
And he kept asking,
more and more rudely,

00:00:52.320 --> 00:00:53.940
well, what use is it?

00:00:53.940 --> 00:00:55.371
What use is it?

00:00:55.371 --> 00:00:57.870
And eventually, Faraday turned
around to the prime minister,

00:00:57.870 --> 00:01:00.150
and he said, well,
sir, in due course,

00:01:00.150 --> 00:01:03.420
I'm sure you'll find
a way of taxing it.

00:01:03.420 --> 00:01:07.869
What that story
shows, to my mind,

00:01:07.869 --> 00:01:09.660
is a phenomenon that's
the same today as it

00:01:09.660 --> 00:01:13.290
was in the 19th century,
which is that there are lots

00:01:13.290 --> 00:01:16.200
of Gladstones in
the world who know

00:01:16.200 --> 00:01:19.814
a lot about politics, but
not much about technology.

00:01:19.814 --> 00:01:21.480
And equally, there
are a lot of Faradays

00:01:21.480 --> 00:01:25.030
in the world who know a lot
about science and technology,

00:01:25.030 --> 00:01:28.020
but don't immediately see
the social implications

00:01:28.020 --> 00:01:29.340
of their work.

00:01:29.340 --> 00:01:31.860
And to my mind, the
Gladstones and the Faradays

00:01:31.860 --> 00:01:33.620
are remaking the
world that we live in.

00:01:33.620 --> 00:01:36.180
They're the most important
people on the planet

00:01:36.180 --> 00:01:38.320
just now, when it
comes to politics.

00:01:38.320 --> 00:01:41.070
And I want to start, if I
may, with just four examples

00:01:41.070 --> 00:01:44.130
of simple technologies
that are emerging

00:01:44.130 --> 00:01:45.780
that we'll all have heard of.

00:01:45.780 --> 00:01:47.415
The first is a self-driving car.

00:01:47.415 --> 00:01:49.290
I want you to imagine
you're taking a journey

00:01:49.290 --> 00:01:50.910
in a self-driving car.

00:01:50.910 --> 00:01:52.920
And you ask that
vehicle to speed up,

00:01:52.920 --> 00:01:54.600
to go over the speed limit.

00:01:54.600 --> 00:01:57.030
The vehicle refuses.

00:01:57.030 --> 00:02:00.180
You ask it to park illegally
on a double-yellow line,

00:02:00.180 --> 00:02:02.820
just for a moment so you
can nip into the shops.

00:02:02.820 --> 00:02:04.680
The vehicle refuses.

00:02:04.680 --> 00:02:06.450
In due course, a
police car comes along,

00:02:06.450 --> 00:02:08.709
its sirens blaring and is
asking you to pull over.

00:02:08.709 --> 00:02:10.500
For whatever reason,
you don't want the car

00:02:10.500 --> 00:02:12.810
to pull over, at least not yet.

00:02:12.810 --> 00:02:16.410
But it does against your will.

00:02:16.410 --> 00:02:19.200
Now I want you to imagine you
are using a virtual reality

00:02:19.200 --> 00:02:22.470
system, one which
enables you to experience

00:02:22.470 --> 00:02:25.560
things which otherwise would
be inaccessible to you.

00:02:25.560 --> 00:02:29.330
And you ask that system to
let you, for whatever reason,

00:02:29.330 --> 00:02:31.440
experience what
it was like to be

00:02:31.440 --> 00:02:35.700
a Nazi executioner at Auschwitz
or to perform a particularly

00:02:35.700 --> 00:02:39.300
depraved sexual act which
society would condemn,

00:02:39.300 --> 00:02:41.620
by and large, as immoral.

00:02:41.620 --> 00:02:44.550
The system refuses.

00:02:44.550 --> 00:02:47.594
Now let's think about an
invention or a development

00:02:47.594 --> 00:02:49.260
which took place just
a couple of months

00:02:49.260 --> 00:02:53.550
ago in relation to chatbots
where a Babylon system was said

00:02:53.550 --> 00:02:56.820
to be able to pass the
Royal Society of General

00:02:56.820 --> 00:03:01.140
Practitioners' general exam
better than the average score

00:03:01.140 --> 00:03:04.830
of its human practitioners.

00:03:04.830 --> 00:03:07.680
Imagine living in a
world where chatbots

00:03:07.680 --> 00:03:10.260
are not just better at
talking about medicine

00:03:10.260 --> 00:03:12.720
and diagnosing conditions,
but are better at talking

00:03:12.720 --> 00:03:16.146
about politics than
the rest of us as well.

00:03:16.146 --> 00:03:17.520
And finally, think
of the stories

00:03:17.520 --> 00:03:21.085
that we've all heard of the soap
dispensers that won't dispense

00:03:21.085 --> 00:03:23.460
soap to people of color because
they've only been trained

00:03:23.460 --> 00:03:27.012
on white hands, the voice
recognition systems that won't

00:03:27.012 --> 00:03:29.220
hear women because they've
only been trained on men's

00:03:29.220 --> 00:03:34.340
voices, the passport recognition
system in New Zealand that

00:03:34.340 --> 00:03:37.070
declined to issue a passport
to a man of Asian extraction

00:03:37.070 --> 00:03:39.930
there because it said
that his eyes were closed

00:03:39.930 --> 00:03:42.720
in his photograph.

00:03:42.720 --> 00:03:44.280
These were previously
and, too often

00:03:44.280 --> 00:03:46.200
still are, seen as
technical problems--

00:03:46.200 --> 00:03:48.490
the ones that I've
just described.

00:03:48.490 --> 00:03:50.620
But to my mind,
they're political.

00:03:50.620 --> 00:03:53.550
The self-driving car
example is an example

00:03:53.550 --> 00:03:56.130
of power plain and simple--
a technology getting

00:03:56.130 --> 00:03:58.680
us to do something we
wouldn't otherwise do

00:03:58.680 --> 00:04:01.630
or not to do something we
would otherwise have done.

00:04:01.630 --> 00:04:04.380
The virtual reality example
goes right to the heart

00:04:04.380 --> 00:04:06.360
of the question of liberty.

00:04:06.360 --> 00:04:08.280
What is permitted in society?

00:04:08.280 --> 00:04:10.650
What should be
permitted in society?

00:04:10.650 --> 00:04:13.260
And what should be forbidden?

00:04:13.260 --> 00:04:17.100
The chatbot example goes
to the heart of democracy.

00:04:17.100 --> 00:04:20.010
In a world where deliberation
takes place increasingly

00:04:20.010 --> 00:04:22.860
by machines, or
could do, what place

00:04:22.860 --> 00:04:27.670
is there for us in the
systems that govern our lives?

00:04:27.670 --> 00:04:31.360
And finally, the examples
of the soap dispenser

00:04:31.360 --> 00:04:32.920
and of the voice
recognition system

00:04:32.920 --> 00:04:37.080
and of the passport system go
to the heart of social justice

00:04:37.080 --> 00:04:39.450
because they deal
with how we recognize

00:04:39.450 --> 00:04:43.080
each other in society, how
we rank and sort each other,

00:04:43.080 --> 00:04:45.570
and how we place each other
in the great chain of status

00:04:45.570 --> 00:04:47.280
and esteem.

00:04:47.280 --> 00:04:51.450
Power, freedom,
democracy, justice--

00:04:51.450 --> 00:04:55.370
these concepts are the
currency of politics.

00:04:55.370 --> 00:04:58.080
And increasingly, I argue
in "Future Politics,"

00:04:58.080 --> 00:05:00.120
they're the currency
of technology.

00:05:00.120 --> 00:05:03.480
And what I say is that, like
it or not, social engineers--

00:05:03.480 --> 00:05:07.320
forgive me-- software
engineers such as yourselves

00:05:07.320 --> 00:05:09.750
are increasingly becoming
social engineers.

00:05:09.750 --> 00:05:12.620
You see the two are just
the same in my mind now.

00:05:12.620 --> 00:05:16.080
I struggle even to
distinguish them.

00:05:16.080 --> 00:05:17.760
Technology, I say,
is transforming

00:05:17.760 --> 00:05:18.990
the way we live together.

00:05:18.990 --> 00:05:20.670
And what I hope to
do in my talk today

00:05:20.670 --> 00:05:24.700
is briefly sketch out how I
think it might be doing that.

00:05:24.700 --> 00:05:27.110
But the overarching
thesis is clear.

00:05:27.110 --> 00:05:28.830
The digital is political.

00:05:28.830 --> 00:05:31.470
We can no longer be blind
to the social and political

00:05:31.470 --> 00:05:33.990
implications of stuff which
previously in the past

00:05:33.990 --> 00:05:36.600
was just seen as
consumer products

00:05:36.600 --> 00:05:39.630
or as commercial or
technical matters only.

00:05:39.630 --> 00:05:42.060
I thought I'd begin
by outlining the three

00:05:42.060 --> 00:05:46.440
main trends in technology,
which lead me to the conclusions

00:05:46.440 --> 00:05:48.190
that I reach in
respect to politics.

00:05:48.190 --> 00:05:51.420
And you don't need me to
spend much time on these,

00:05:51.420 --> 00:05:53.490
but I'll just rattle
them off anyway.

00:05:53.490 --> 00:05:56.910
The first is increasingly
capable systems.

00:05:56.910 --> 00:05:59.844
In short, we are
developing systems--

00:05:59.844 --> 00:06:01.260
call them artificial
intelligence,

00:06:01.260 --> 00:06:02.550
call them what you will--

00:06:02.550 --> 00:06:05.340
they're increasingly able to
do things which we previously

00:06:05.340 --> 00:06:07.002
thought only human
beings could do.

00:06:07.002 --> 00:06:08.460
And they can do
them as well as us,

00:06:08.460 --> 00:06:10.900
and they can do them
better in some cases,

00:06:10.900 --> 00:06:13.620
whether it's lip reading,
whether it's transcription,

00:06:13.620 --> 00:06:17.850
mimicking human speech,
detecting lung cancers

00:06:17.850 --> 00:06:21.150
and diagnosing and
predicting survival periods.

00:06:21.150 --> 00:06:23.340
Almost every game
that we've invented,

00:06:23.340 --> 00:06:26.747
computers now do them better
or equal to human beings.

00:06:26.747 --> 00:06:28.830
And the simple thesis is
that progress isn't going

00:06:28.830 --> 00:06:30.300
to slow down any time soon.

00:06:30.300 --> 00:06:33.660
Some people say it's increasing
at an exponential rate--

00:06:33.660 --> 00:06:35.870
so increasingly capable systems.

00:06:35.870 --> 00:06:37.560
But the second
point of importance

00:06:37.560 --> 00:06:40.230
is not just that our systems
are increasingly capable,

00:06:40.230 --> 00:06:42.720
but they're increasingly,
they're everywhere.

00:06:42.720 --> 00:06:46.805
We live in what's being called
the era of the glass slab

00:06:46.805 --> 00:06:48.930
where, principally, our
interaction with technology

00:06:48.930 --> 00:06:51.060
takes place on computer
screens, or iPads,

00:06:51.060 --> 00:06:55.380
or phones through the
medium of a glass slab.

00:06:55.380 --> 00:06:58.950
But what's said is
that, in the future,

00:06:58.950 --> 00:07:03.090
technology will be dispersed
around us in our architecture,

00:07:03.090 --> 00:07:07.440
in our utilities at
home, in our appliances,

00:07:07.440 --> 00:07:09.480
in our public spaces,
even on our clothes

00:07:09.480 --> 00:07:10.890
and inside our bodies--

00:07:10.890 --> 00:07:14.010
the so-called internet of
things or ubiquitous computing.

00:07:14.010 --> 00:07:17.897
Which means that, increasingly,
sensors' processing power

00:07:17.897 --> 00:07:19.980
and connection to the
internet will be distributed

00:07:19.980 --> 00:07:22.800
all around us in
items and artifacts

00:07:22.800 --> 00:07:25.390
that we previously wouldn't
have seen as technology.

00:07:25.390 --> 00:07:28.750
So the idea of the glass slab
will gradually fade away.

00:07:28.750 --> 00:07:32.500
The distinction between online
and offline, real and virtual,

00:07:32.500 --> 00:07:36.840
meet space and cyberspace
will lose some of its meaning

00:07:36.840 --> 00:07:39.600
and, certainly, lose a
lot of its importance.

00:07:39.600 --> 00:07:41.400
So we got increasingly
capable systems

00:07:41.400 --> 00:07:45.502
and what I call increasingly
integrated technology.

00:07:45.502 --> 00:07:47.460
And finally, we have an
increasingly quantified

00:07:47.460 --> 00:07:48.860
society.

00:07:48.860 --> 00:07:51.330
Now, what's said is
that every two days,

00:07:51.330 --> 00:07:53.790
we generate more
data than we did

00:07:53.790 --> 00:07:57.690
from the dawn of
civilization until 2003.

00:07:57.690 --> 00:08:00.630
And it's predicted
that by 2020, there'll

00:08:00.630 --> 00:08:03.840
be about 3 million books worth
of data for every human being

00:08:03.840 --> 00:08:05.050
on the planet.

00:08:05.050 --> 00:08:07.170
This is obviously unprecedented.

00:08:07.170 --> 00:08:11.430
And what it means is that,
increasingly, what we say,

00:08:11.430 --> 00:08:14.970
what we think, how
we feel, where we go,

00:08:14.970 --> 00:08:18.430
who we associate with,
what we like and dislike--

00:08:18.430 --> 00:08:21.780
almost every aspect of
our lives, in some sense,

00:08:21.780 --> 00:08:24.390
will be captured,
recorded as data,

00:08:24.390 --> 00:08:26.910
stored in permanent or
semi-permanent form,

00:08:26.910 --> 00:08:30.030
and made available
for processing.

00:08:30.030 --> 00:08:32.460
Looking at the crowd in
this room, a lot of this

00:08:32.460 --> 00:08:35.136
may seem natural
and normal to us

00:08:35.136 --> 00:08:36.719
because it's what
we've grown up with.

00:08:36.719 --> 00:08:38.100
And all of these trends
have been increasing

00:08:38.100 --> 00:08:39.030
through our lifetime.

00:08:39.030 --> 00:08:42.840
But to my mind, it marks
a pretty substantial shift

00:08:42.840 --> 00:08:44.890
in the state of humanity.

00:08:44.890 --> 00:08:47.670
It could be as profound for us
as the scientific revolution,

00:08:47.670 --> 00:08:49.559
the agricultural revolution.

00:08:49.559 --> 00:08:50.850
Because it's only just started.

00:08:50.850 --> 00:08:53.100
We're only 5 or 10
seconds into this stuff

00:08:53.100 --> 00:08:54.480
in the historical perspective.

00:08:54.480 --> 00:08:56.730
And if you think about what
might be around the corner

00:08:56.730 --> 00:08:58.980
10 or 20 years down
the line, then it

00:08:58.980 --> 00:09:02.310
would be mad to assume that
the consequences for politics,

00:09:02.310 --> 00:09:05.520
for how we live together,
wouldn't be profound.

00:09:05.520 --> 00:09:08.820
Because we've never had to live
alongside non-human systems

00:09:08.820 --> 00:09:10.950
of extraordinary
capability before.

00:09:10.950 --> 00:09:15.270
We've never known what it's
like for digital technology

00:09:15.270 --> 00:09:19.900
to be integrated seamlessly
into the world around us.

00:09:19.900 --> 00:09:21.690
There's never been
a human civilization

00:09:21.690 --> 00:09:25.260
where every facet of its
social and private life

00:09:25.260 --> 00:09:28.920
has, in some way, been
recorded and stored as data.

00:09:28.920 --> 00:09:31.590
And our duty-- whether we're
Gladstones, or Faradays,

00:09:31.590 --> 00:09:32.530
or just citizens--

00:09:32.530 --> 00:09:34.780
is to try and understand
what the implications of that

00:09:34.780 --> 00:09:36.820
are for future of politics.

00:09:36.820 --> 00:09:40.290
And so I thought what I'd do
today is just go through four

00:09:40.290 --> 00:09:43.810
of the most basic concepts in
politics-- power, democracy,

00:09:43.810 --> 00:09:45.000
freedom, and justice--

00:09:45.000 --> 00:09:47.820
and say how I think that
the digital is political

00:09:47.820 --> 00:09:49.620
and how your work as
software engineers

00:09:49.620 --> 00:09:53.400
will increasingly make
you social engineers too.

00:09:53.400 --> 00:09:55.620
People often say that
big tech companies

00:09:55.620 --> 00:09:57.060
have a great deal of power.

00:09:57.060 --> 00:09:58.630
And it's true, they do.

00:09:58.630 --> 00:10:02.111
And that's only likely to
increase in the future.

00:10:02.111 --> 00:10:04.110
But I think there's often
a conceptual confusion

00:10:04.110 --> 00:10:05.520
that people come
across, which is

00:10:05.520 --> 00:10:07.750
that they mistake
purely economic power

00:10:07.750 --> 00:10:09.100
for political power.

00:10:09.100 --> 00:10:11.070
And I don't think the
two are the same thing.

00:10:11.070 --> 00:10:12.550
In politics and
political science,

00:10:12.550 --> 00:10:15.089
it's said that a very
basic definition of power

00:10:15.089 --> 00:10:17.380
is the ability to get people
to do things they wouldn't

00:10:17.380 --> 00:10:21.190
otherwise do or not to do things
they would otherwise have done.

00:10:21.190 --> 00:10:22.990
And let's adopt
that as our working

00:10:22.990 --> 00:10:25.060
definition for a moment.

00:10:25.060 --> 00:10:28.120
Now, I suggest that
technology, digital technology,

00:10:28.120 --> 00:10:32.540
is capable of exerting
power in one of three ways.

00:10:32.540 --> 00:10:36.220
The first is in the way that we
saw with the self-driving car

00:10:36.220 --> 00:10:38.140
example at the beginning,
which is basically

00:10:38.140 --> 00:10:40.870
that, whenever we use
a technology, whenever

00:10:40.870 --> 00:10:44.500
we interact with it, we
are subject to the dictates

00:10:44.500 --> 00:10:46.796
of the code of that technology.

00:10:46.796 --> 00:10:48.670
So when you use an online
platform or a piece

00:10:48.670 --> 00:10:52.090
of software, you can't
ask it to do something

00:10:52.090 --> 00:10:54.100
that it's not programmed to do.

00:10:54.100 --> 00:10:56.116
It can only do what
it's programmed to do.

00:10:56.116 --> 00:10:57.990
And to take another
prime ministerial example

00:10:57.990 --> 00:10:59.657
that often springs to mind--

00:10:59.657 --> 00:11:01.990
when Gordon Brown was prime
minister, he went to the US,

00:11:01.990 --> 00:11:07.389
and President Obama gave him 25
DVDs of classic American films.

00:11:07.389 --> 00:11:09.430
This was, for some reason,
seen as a great insult

00:11:09.430 --> 00:11:11.950
to the British people,
in and of itself.

00:11:11.950 --> 00:11:14.470
But if that was insulting,
what then happened

00:11:14.470 --> 00:11:17.110
when the prime minister
went and sat down at home,

00:11:17.110 --> 00:11:20.620
popcorn in hand, was that the
DVDs wouldn't play because they

00:11:20.620 --> 00:11:23.170
were coded for US DVD players.

00:11:23.170 --> 00:11:26.350
And the digital rights
management system on those DVDs

00:11:26.350 --> 00:11:27.700
simply forbade it.

00:11:27.700 --> 00:11:29.260
Now, we know about
that technology,

00:11:29.260 --> 00:11:30.760
and we understand
why it's happened.

00:11:30.760 --> 00:11:32.930
But to the untrained eye,
it looks like a glitch.

00:11:32.930 --> 00:11:33.910
But it's not a glitch.

00:11:33.910 --> 00:11:36.860
Technologies-- we
can only do with them

00:11:36.860 --> 00:11:38.610
what the code is, what
the programmers say

00:11:38.610 --> 00:11:39.443
we can do with them.

00:11:39.443 --> 00:11:41.291
It's a very simple
fact about technology.

00:11:41.291 --> 00:11:42.790
And this was
acknowledged very early

00:11:42.790 --> 00:11:44.790
on when we started using
computers and internet.

00:11:44.790 --> 00:11:48.070
And people started saying
well, that means code is law

00:11:48.070 --> 00:11:50.450
or, at least, code is like law.

00:11:50.450 --> 00:11:53.500
But things have developed
since then, quite recently.

00:11:53.500 --> 00:11:55.450
The first is that,
whereas we used

00:11:55.450 --> 00:11:59.860
to think of the code that
was inside our technology

00:11:59.860 --> 00:12:01.285
as a kind of
architecture-- people

00:12:01.285 --> 00:12:03.520
that used to talk about
software architecture.

00:12:03.520 --> 00:12:06.370
And the language we use
reflect that-- so platforms,

00:12:06.370 --> 00:12:08.590
and portals, and gateways--

00:12:08.590 --> 00:12:11.824
as if it was a metaphor
for physical architecture.

00:12:11.824 --> 00:12:13.990
That's no longer going to
be the case in the future.

00:12:13.990 --> 00:12:15.850
Increasingly capable
systems means

00:12:15.850 --> 00:12:18.250
that the code that
animates our technology

00:12:18.250 --> 00:12:19.670
is likely to be dynamic.

00:12:19.670 --> 00:12:22.240
It might be capable of learning
and changing over time.

00:12:22.240 --> 00:12:26.140
It might be remotely
changeable by its creators.

00:12:26.140 --> 00:12:28.150
Or it might do so
on its own basis.

00:12:28.150 --> 00:12:29.800
So the code that
used to control us

00:12:29.800 --> 00:12:32.560
in the early days of the
internet and on cyberspace

00:12:32.560 --> 00:12:34.450
with more of, like,
dumb architecture,

00:12:34.450 --> 00:12:37.330
but in the future, it's more
likely to be more dynamic.

00:12:37.330 --> 00:12:40.240
The second big change is that
code is no longer just power

00:12:40.240 --> 00:12:42.350
or law in cyberspace.

00:12:42.350 --> 00:12:43.825
It's in real space, too.

00:12:43.825 --> 00:12:45.700
And that's because of
increasingly integrated

00:12:45.700 --> 00:12:46.720
technology.

00:12:46.720 --> 00:12:49.030
When we go around our
daily lives and interact

00:12:49.030 --> 00:12:52.090
with technologies,
we can't shut down

00:12:52.090 --> 00:12:54.560
or log off like we might have
been able to in the past.

00:12:54.560 --> 00:12:57.610
If that distinction
between real and virtual,

00:12:57.610 --> 00:12:59.710
or online and offline,
or cyberspace and meet

00:12:59.710 --> 00:13:02.670
space, if it does dissolve, and
if people are right about that,

00:13:02.670 --> 00:13:05.095
then code is going to be
able to exert power on us.

00:13:05.095 --> 00:13:06.970
Technology is going to
be able to exert power

00:13:06.970 --> 00:13:09.260
on us all the time.

00:13:09.260 --> 00:13:11.060
And there's no way of
getting away from it.

00:13:11.060 --> 00:13:13.120
So that's the first
way that I would say,

00:13:13.120 --> 00:13:17.208
simply, technology can
be used to exert power.

00:13:17.208 --> 00:13:20.410
The second and third
ways and more subtle.

00:13:20.410 --> 00:13:22.930
The first is through scrutiny.

00:13:22.930 --> 00:13:25.290
The more you know
about someone--

00:13:25.290 --> 00:13:29.920
what they like, what they
fear, what they hate--

00:13:29.920 --> 00:13:31.960
the more easy it is
to influence them.

00:13:31.960 --> 00:13:34.780
It's the basic premise
behind all online advertising

00:13:34.780 --> 00:13:38.800
and all political
advertising as well.

00:13:38.800 --> 00:13:41.500
If it's the case that society
is becoming increasingly

00:13:41.500 --> 00:13:43.660
quantified, that all of
our thoughts and feelings

00:13:43.660 --> 00:13:46.060
and inner life is
becoming better-known

00:13:46.060 --> 00:13:49.180
to those who make and
govern technologies,

00:13:49.180 --> 00:13:50.912
then it'll be easier
to influence us

00:13:50.912 --> 00:13:52.870
because they'll have more
information about us.

00:13:52.870 --> 00:13:54.415
It's a simple point.

00:13:54.415 --> 00:13:56.290
There's a deeper and
more subtle way, though,

00:13:56.290 --> 00:13:58.040
that people gathering
information about us

00:13:58.040 --> 00:13:59.980
allows them to exert power.

00:13:59.980 --> 00:14:02.680
And it's the
disciplinary effect.

00:14:02.680 --> 00:14:05.660
When we know we're
being watched,

00:14:05.660 --> 00:14:07.090
we change our behavior.

00:14:07.090 --> 00:14:08.710
We police ourselves.

00:14:08.710 --> 00:14:11.270
We're less likely to do things
that society would think are

00:14:11.270 --> 00:14:14.080
sinful, or shameful,
or wrong, or that

00:14:14.080 --> 00:14:16.090
might land us in hot water.

00:14:16.090 --> 00:14:17.440
Google's not a bad example.

00:14:17.440 --> 00:14:20.950
Because one of the things
that Google apparently does

00:14:20.950 --> 00:14:24.880
is, if people search for things
related to child pornography,

00:14:24.880 --> 00:14:26.980
they're reported to authorities.

00:14:26.980 --> 00:14:29.260
That, in itself, the
dissemination of that fact

00:14:29.260 --> 00:14:33.620
is likely to and does change
the way that people behave.

00:14:33.620 --> 00:14:35.689
So the second way that
technology exerts power

00:14:35.689 --> 00:14:37.480
is by gathering
information about us, which

00:14:37.480 --> 00:14:40.972
can be used to influence us
or by causing us to discipline

00:14:40.972 --> 00:14:43.180
and police ourselves because
we know that information

00:14:43.180 --> 00:14:45.400
is being gathered about us.

00:14:45.400 --> 00:14:48.640
And the third is the most
subtle of all and possibly the

00:14:48.640 --> 00:14:49.600
most powerful of all.

00:14:49.600 --> 00:14:52.900
And I call it
perception control.

00:14:52.900 --> 00:14:57.880
We, all of us, rely on
other people or other things

00:14:57.880 --> 00:15:01.870
to gather information
about the world,

00:15:01.870 --> 00:15:05.230
distill it into something
sensible and comprehensible,

00:15:05.230 --> 00:15:08.050
and present it to us
in a digestible form--

00:15:08.050 --> 00:15:09.760
so like a filtering.

00:15:09.760 --> 00:15:12.435
Otherwise, all we'd
know about the world

00:15:12.435 --> 00:15:15.180
is what we immediately perceive.

00:15:15.180 --> 00:15:18.540
Now increasingly, we
rely on technologies

00:15:18.540 --> 00:15:21.690
to do the work of filtering for
us, whether it's when we go out

00:15:21.690 --> 00:15:25.212
and look for information, such
as in a search function, when

00:15:25.212 --> 00:15:26.670
information is
gathered and brought

00:15:26.670 --> 00:15:28.680
to us in a news function.

00:15:28.680 --> 00:15:32.250
Increasingly, we're subjecting
our immediate sensory

00:15:32.250 --> 00:15:35.640
perception to technologies as
well with augmented reality--

00:15:35.640 --> 00:15:40.770
over our eyes, over our ears,
over our bodies in haptic form

00:15:40.770 --> 00:15:42.640
or virtual reality too.

00:15:42.640 --> 00:15:45.450
And those who control the
flow of information in society

00:15:45.450 --> 00:15:46.949
exert a great deal of power.

00:15:46.949 --> 00:15:48.990
Because you know that the
best way to stop people

00:15:48.990 --> 00:15:50.448
from being upset
about something is

00:15:50.448 --> 00:15:52.360
to stop them knowing
about it at all.

00:15:52.360 --> 00:15:54.540
Or the best way to get
people angry about something

00:15:54.540 --> 00:15:56.670
is to tell them over and
over that it's disgusting

00:15:56.670 --> 00:15:59.670
and that it's wrong and
that it has to be punished.

00:15:59.670 --> 00:16:04.110
And the work of filtering,
presenting to each us

00:16:04.110 --> 00:16:07.380
the world beyond
our immediate gaze,

00:16:07.380 --> 00:16:09.364
is increasingly done
by technologies.

00:16:09.364 --> 00:16:11.280
And so when I say that
technology is powerful,

00:16:11.280 --> 00:16:13.110
I'm usually referring to
one of those three things--

00:16:13.110 --> 00:16:15.000
the ability to force
us through code to do

00:16:15.000 --> 00:16:17.700
something, the ability to
gather information about us,

00:16:17.700 --> 00:16:20.567
the ability to control the
way we perceive the world.

00:16:20.567 --> 00:16:22.650
And there's nothing
necessarily nefarious or wrong

00:16:22.650 --> 00:16:23.796
with any of these.

00:16:23.796 --> 00:16:25.170
It's just a helpful,
I think, way

00:16:25.170 --> 00:16:28.410
of thinking about how technology
can change the way people

00:16:28.410 --> 00:16:30.180
behave, how it can exert power.

00:16:32.700 --> 00:16:34.800
The other important
implication, however,

00:16:34.800 --> 00:16:43.800
of technology flowing on from
how it exerts power on us

00:16:43.800 --> 00:16:46.110
is how it affects our freedom.

00:16:46.110 --> 00:16:50.970
Now, the great debate that
we've all heard for 20 years

00:16:50.970 --> 00:16:54.180
is how increasing
technologies of surveillance

00:16:54.180 --> 00:16:58.450
will potentially lead to states
and maybe tech firms having

00:16:58.450 --> 00:17:00.450
too much power over people
because they watch us

00:17:00.450 --> 00:17:02.820
the whole time and are
capable of regulating us.

00:17:02.820 --> 00:17:04.060
That's an important debate.

00:17:04.060 --> 00:17:05.920
It's not the one I want to
necessarily talk about today.

00:17:05.920 --> 00:17:08.640
Because I think that the effects
of technology on our freedom

00:17:08.640 --> 00:17:11.109
are actually a little
bit more subtle.

00:17:11.109 --> 00:17:13.140
So I would ask the
people in the room

00:17:13.140 --> 00:17:16.560
to ask themselves if you've ever
streamed an episode of "Game

00:17:16.560 --> 00:17:22.230
of Thrones" illegally or you've
gone to take a second helping

00:17:22.230 --> 00:17:27.030
from the Coke machine even
though you've only paid for one

00:17:27.030 --> 00:17:30.000
or if you've dodged a bus fare,
here or abroad, by jumping

00:17:30.000 --> 00:17:33.690
on a bus and not paying for a
ticket and jumping off again?

00:17:33.690 --> 00:17:36.990
74% of British people admit
to having done these things.

00:17:36.990 --> 00:17:39.460
It's not because
they're all scoundrels.

00:17:39.460 --> 00:17:44.244
It's because there is this
hinterland in the law where

00:17:44.244 --> 00:17:46.410
people are allowed to get
away with things from time

00:17:46.410 --> 00:17:50.000
to time without being punished
as long as it's not constant,

00:17:50.000 --> 00:17:52.290
as long as it's not egregious.

00:17:52.290 --> 00:17:54.540
That's why so many people do it.

00:17:54.540 --> 00:17:57.660
I suggest, in a world of
increasingly capable systems

00:17:57.660 --> 00:17:59.800
and increasingly
integrated technology,

00:17:59.800 --> 00:18:02.580
those little bits of naughtiness
will become much more

00:18:02.580 --> 00:18:06.660
difficult. Whether it's because
your smart wallet automatically

00:18:06.660 --> 00:18:11.490
deducts the bus fare when you
jump on the bus, or the "Game

00:18:11.490 --> 00:18:14.497
of Thrones" episode, it just
becomes impossible to stream

00:18:14.497 --> 00:18:16.830
because the digital rights
management technology becomes

00:18:16.830 --> 00:18:20.010
so good, or because you need
face recognition software

00:18:20.010 --> 00:18:22.260
to get that second
helping of Coke.

00:18:22.260 --> 00:18:23.820
And if you think
that's petty, you

00:18:23.820 --> 00:18:27.120
should know that, in Beijing's
Temple of Heaven Park,

00:18:27.120 --> 00:18:28.857
facial recognition
software is already

00:18:28.857 --> 00:18:30.690
used to make sure that
people don't use more

00:18:30.690 --> 00:18:32.820
than their fair share
of toilet paper.

00:18:32.820 --> 00:18:35.060
And if that's the world
that we're moving into,

00:18:35.060 --> 00:18:37.990
then that hinterland of
naughtiness-- the ability

00:18:37.990 --> 00:18:41.130
to make little mistakes
around the edges,

00:18:41.130 --> 00:18:43.800
like getting your self-driving
car to go over the speed

00:18:43.800 --> 00:18:46.660
limit or park illegally--
becomes a lot more difficult.

00:18:46.660 --> 00:18:48.934
I think that has
implication for freedom.

00:18:48.934 --> 00:18:50.850
The more profound
implication for our freedom,

00:18:50.850 --> 00:18:53.430
though, is what I call
the privatization of it.

00:18:53.430 --> 00:18:58.410
Increasingly, we use
technologies to do the things

00:18:58.410 --> 00:19:01.950
that would traditionally be
considered freedom-making,

00:19:01.950 --> 00:19:06.450
whether it's freedom of speech--

00:19:06.450 --> 00:19:08.940
an increasing amount of
important political speech

00:19:08.940 --> 00:19:11.790
takes place online
on online platforms--

00:19:11.790 --> 00:19:13.779
whether it's freedom
of movement in the form

00:19:13.779 --> 00:19:15.570
of self-driving cars
or whatever it is that

00:19:15.570 --> 00:19:18.300
comes next, whether it's
freedom of thought, the ability

00:19:18.300 --> 00:19:20.790
to think clearly and
rationally, which is obviously,

00:19:20.790 --> 00:19:24.350
affected by the systems that
filter our information for us.

00:19:24.350 --> 00:19:26.100
The good news about
technology, obviously,

00:19:26.100 --> 00:19:30.216
is that our freedom can be
enhanced by these technologies.

00:19:30.216 --> 00:19:31.590
The interesting
point, though, is

00:19:31.590 --> 00:19:34.560
that, whereas in the past,
for most of human history,

00:19:34.560 --> 00:19:37.720
questions of freedom
were left to the states

00:19:37.720 --> 00:19:40.740
and were considered political
questions to be decided

00:19:40.740 --> 00:19:43.350
on by the whole of society.

00:19:43.350 --> 00:19:45.340
Nowadays, they're
increasingly privatized.

00:19:45.340 --> 00:19:49.050
What you can do on a
political speech platform,

00:19:49.050 --> 00:19:51.930
what you can do with
a self-driving car,

00:19:51.930 --> 00:19:56.580
how Facebook or Twitter
filters the news that you see--

00:19:56.580 --> 00:19:59.010
these aren't decisions that
you and I-- maybe you--

00:19:59.010 --> 00:20:02.010
these are decisions
that most of us take.

00:20:02.010 --> 00:20:03.270
They're done privately.

00:20:03.270 --> 00:20:06.699
And they're done by
tech firms, often,

00:20:06.699 --> 00:20:08.490
acting in what they
perceive to be the best

00:20:08.490 --> 00:20:09.670
interest of their consumers.

00:20:09.670 --> 00:20:11.820
But they're
ultimately, just now,

00:20:11.820 --> 00:20:14.580
a matter of private
decisions taken by tech

00:20:14.580 --> 00:20:16.470
firms and their lawyers.

00:20:16.470 --> 00:20:20.970
And I think we need to think
through quite carefully what

00:20:20.970 --> 00:20:23.430
the implications of this
are, just in political terms,

00:20:23.430 --> 00:20:26.850
looking at the long
run of human history.

00:20:26.850 --> 00:20:31.380
Because what it first
means is that tech firms

00:20:31.380 --> 00:20:35.010
take on quite a significant
moral burden when

00:20:35.010 --> 00:20:36.660
they decide what
we can and can't

00:20:36.660 --> 00:20:38.140
do with their technologies.

00:20:38.140 --> 00:20:40.860
That was previously a
matter of societal debate.

00:20:40.860 --> 00:20:44.420
So the VR system I
think is a good example.

00:20:44.420 --> 00:20:46.170
When you get a Virtual
Reality system that

00:20:46.170 --> 00:20:48.300
is supposed to be
customizable in some way

00:20:48.300 --> 00:20:50.220
or give you lots of
different experiences,

00:20:50.220 --> 00:20:52.560
should it be up to you,
the individual user,

00:20:52.560 --> 00:20:57.210
to decide which experiences you
want, depraved or otherwise?

00:20:57.210 --> 00:20:58.920
Should it be up
to the tech firm?

00:20:58.920 --> 00:21:00.837
Should it be up to
society as a whole?

00:21:00.837 --> 00:21:02.670
The traditional answer
given by human beings

00:21:02.670 --> 00:21:05.490
is that society, as a whole,
the limits of what is right

00:21:05.490 --> 00:21:08.430
and what is moral and
what is forbidden.

00:21:08.430 --> 00:21:11.700
Right now we don't
live in that world.

00:21:11.700 --> 00:21:13.680
The second thing
is that, obviously,

00:21:13.680 --> 00:21:17.700
through no wrongdoing,
tech firms are not

00:21:17.700 --> 00:21:19.560
answerable to the
people in the same way

00:21:19.560 --> 00:21:22.740
that the governments
that set laws are.

00:21:22.740 --> 00:21:24.930
The third difference between
a tech firm and a state

00:21:24.930 --> 00:21:27.240
is that, in the state,
the law develops

00:21:27.240 --> 00:21:30.270
over time in a public
and consistent way that

00:21:30.270 --> 00:21:32.100
applies to everyone.

00:21:32.100 --> 00:21:34.500
Whereas, tech firms
do things differently.

00:21:34.500 --> 00:21:37.800
Google might have a different
policy towards hate speech

00:21:37.800 --> 00:21:41.232
than Twitter does, a different
policy than Facebook does.

00:21:41.232 --> 00:21:43.690
And some people would say that's
a good thing-- for reasons

00:21:43.690 --> 00:21:44.856
I'll come on to in a second.

00:21:44.856 --> 00:21:47.490
And others would
say it's a challenge

00:21:47.490 --> 00:21:51.000
to the development, the overall
moral development of society,

00:21:51.000 --> 00:21:53.142
of shared values between us all.

00:21:53.142 --> 00:21:54.600
Just to take two
examples that have

00:21:54.600 --> 00:21:57.180
troubled political philosophies
since time immemorial--

00:21:57.180 --> 00:22:00.210
one is the question
of harm to self.

00:22:00.210 --> 00:22:04.140
Should we, as grown up adults,
be able to harm ourselves?

00:22:04.140 --> 00:22:07.260
So if I ask my self-driving
car to reverse over my head,

00:22:07.260 --> 00:22:10.590
should it do that because
it's my autonomous decision

00:22:10.590 --> 00:22:11.850
that I'd like it to do that?

00:22:11.850 --> 00:22:13.980
Or my automated cooking
system in my kitchen--

00:22:13.980 --> 00:22:16.675
if I want it to make a curry
for me that's so spicy that

00:22:16.675 --> 00:22:19.800
it's likely to send me to
hospital, but it's my choice--

00:22:19.800 --> 00:22:20.910
should it do it?

00:22:20.910 --> 00:22:23.940
Or should systems be
designed to protect us?

00:22:23.940 --> 00:22:26.160
The idea that systems,
beyond our control,

00:22:26.160 --> 00:22:28.380
should be designed to
protect us might seem anodyne

00:22:28.380 --> 00:22:29.190
in this room.

00:22:29.190 --> 00:22:31.620
But to John Stuart
Mill and Jeremy Bentham

00:22:31.620 --> 00:22:33.360
and other philosophers
like that on whom

00:22:33.360 --> 00:22:36.670
our legal system and its
principles are often based--

00:22:36.670 --> 00:22:39.870
that would have been anathema
to them for the same reason

00:22:39.870 --> 00:22:44.641
that suicide stopped being
illegal not so long ago.

00:22:44.641 --> 00:22:46.140
Because people are
generally thought

00:22:46.140 --> 00:22:48.810
to be able to do things which
harm themselves and should

00:22:48.810 --> 00:22:51.100
be free to do that.

00:22:51.100 --> 00:22:53.740
Even more so, the
question of immoral acts--

00:22:53.740 --> 00:22:56.880
there are very few laws left on
our statute books which stop us

00:22:56.880 --> 00:22:58.980
from doing things
which are considered

00:22:58.980 --> 00:23:00.120
immoral or disgusting.

00:23:00.120 --> 00:23:01.619
In the privacy of
your own home, you

00:23:01.619 --> 00:23:05.250
can do almost any sex
act apart from something

00:23:05.250 --> 00:23:08.984
which causes very serious
harm to yourself or to others.

00:23:08.984 --> 00:23:10.650
And so free speech--
you can anticipate,

00:23:10.650 --> 00:23:12.816
in the future, free speech
and free action campaign,

00:23:12.816 --> 00:23:14.700
again, to say, if I
want to simulate sex

00:23:14.700 --> 00:23:17.100
with a child on my
virtual reality system

00:23:17.100 --> 00:23:20.040
in circumstances where it
causes no harm to anyone else,

00:23:20.040 --> 00:23:21.790
I should be allowed to do that.

00:23:21.790 --> 00:23:23.250
And actually, a
governing principle

00:23:23.250 --> 00:23:25.350
of English law for
centuries has been,

00:23:25.350 --> 00:23:27.540
if something doesn't
harm other people,

00:23:27.540 --> 00:23:28.965
you should be free to do it.

00:23:28.965 --> 00:23:30.840
Now, there might be
disagreements in the room

00:23:30.840 --> 00:23:32.790
about whether that's
right or wrong.

00:23:32.790 --> 00:23:36.240
The interesting point for me is
that, right now, that decision

00:23:36.240 --> 00:23:38.460
is not going to be
taken by the states.

00:23:38.460 --> 00:23:41.730
It's going to be
taken by companies.

00:23:41.730 --> 00:23:44.190
And that marks quite a profound
shift, I think, in the way

00:23:44.190 --> 00:23:48.780
that politics is arranged and
the way that political theory

00:23:48.780 --> 00:23:49.691
needs to proceed.

00:23:49.691 --> 00:23:50.440
Now, in the book--

00:23:50.440 --> 00:23:51.981
I won't bore you
with this too much--

00:23:51.981 --> 00:23:57.150
I try to outline a series of
doctrines, of ways of thinking,

00:23:57.150 --> 00:24:00.840
that can help us to think
clearly and crisply about

00:24:00.840 --> 00:24:02.640
what's at stake when
we limit and don't

00:24:02.640 --> 00:24:04.170
limit people's freedom.

00:24:04.170 --> 00:24:06.180
So I've got this idea of
digital libertarianism,

00:24:06.180 --> 00:24:07.721
which some people
are going to adopt,

00:24:07.721 --> 00:24:10.020
which is the idea that,
basically, freedom is freedom

00:24:10.020 --> 00:24:11.632
from any form of technology.

00:24:11.632 --> 00:24:13.590
If I don't want to have
technology in my house,

00:24:13.590 --> 00:24:14.970
I should be free not to have it.

00:24:14.970 --> 00:24:16.970
There should be no
requirement of smart devices,

00:24:16.970 --> 00:24:18.240
small utilities.

00:24:18.240 --> 00:24:21.360
And any piece of code
that restricts my freedom

00:24:21.360 --> 00:24:22.980
is unwanted.

00:24:22.980 --> 00:24:24.650
More likely is that
people will adopt

00:24:24.650 --> 00:24:26.910
a position of what I call
digital liberalism, which

00:24:26.910 --> 00:24:29.310
is that the rules that
are coded into technology

00:24:29.310 --> 00:24:31.530
should try to maximize
the overall freedom

00:24:31.530 --> 00:24:34.930
of the community, even
if it means minimizing

00:24:34.930 --> 00:24:36.970
the freedom for some.

00:24:36.970 --> 00:24:38.490
A particular doctrine,
which I think

00:24:38.490 --> 00:24:41.415
will appeal to free marketers,
I call digital confederalism,

00:24:41.415 --> 00:24:43.290
which basically means
that any company should

00:24:43.290 --> 00:24:46.230
be able to set its own rules
so long as there's always

00:24:46.230 --> 00:24:48.700
a sufficient number
of different companies

00:24:48.700 --> 00:24:51.510
so you can switch between
them according to your choice.

00:24:51.510 --> 00:24:53.790
People will say, that's the
way to maintain freedom--

00:24:53.790 --> 00:24:56.790
lots of different
little subsets.

00:24:56.790 --> 00:24:59.640
Digital moralism-- the idea
that technology should encourage

00:24:59.640 --> 00:25:01.020
us to be better people.

00:25:01.020 --> 00:25:03.720
Digital paternalism-- the
idea that technologies

00:25:03.720 --> 00:25:06.850
should protect us from ourselves
and our own worst instincts.

00:25:06.850 --> 00:25:09.900
Or digital republicanism--
for centuries, humans

00:25:09.900 --> 00:25:13.320
have demanded that, when
power is exerted over them,

00:25:13.320 --> 00:25:15.570
that power should
not be unaccountable.

00:25:15.570 --> 00:25:18.340
That power should be
answerable in some way,

00:25:18.340 --> 00:25:21.570
even if that power is
exerted benevolently.

00:25:21.570 --> 00:25:25.860
It's why the
American and English

00:25:25.860 --> 00:25:27.972
Revolutions, to a certain
extent, both happened.

00:25:27.972 --> 00:25:29.430
It wasn't just
people's frustration

00:25:29.430 --> 00:25:31.650
that the monarch
was behaving badly.

00:25:31.650 --> 00:25:34.540
It's that they could
behave badly at any point.

00:25:34.540 --> 00:25:37.470
So a freedom which relies on
the benevolence of someone else

00:25:37.470 --> 00:25:39.529
is no kind of freedom at all.

00:25:39.529 --> 00:25:41.070
And digital
republicanism, therefore,

00:25:41.070 --> 00:25:42.780
means that, in any
technology, whenever

00:25:42.780 --> 00:25:44.640
power is exerted
over you, you should

00:25:44.640 --> 00:25:46.082
be able to have a say in it.

00:25:46.082 --> 00:25:47.540
You should be able
to customize it,

00:25:47.540 --> 00:25:50.460
to edit it according to your
principle of the good life,

00:25:50.460 --> 00:25:52.690
to your vision of
what's right for you.

00:25:52.690 --> 00:25:54.822
These are all ideas that
are new and strange,

00:25:54.822 --> 00:25:57.030
but I think we're going to
have to grapple with them,

00:25:57.030 --> 00:25:59.050
whether we're Gladstones
or whether we're Faradays,

00:25:59.050 --> 00:26:00.900
if it's right that so
many of our freedoms

00:26:00.900 --> 00:26:03.480
are now going to be in the
hands of technologies and people

00:26:03.480 --> 00:26:05.220
who make them.

00:26:05.220 --> 00:26:09.540
Democracy-- we all
know the ways in which

00:26:09.540 --> 00:26:11.400
technology has
affected democracy

00:26:11.400 --> 00:26:14.974
as we currently experience it.

00:26:14.974 --> 00:26:16.890
It's changed the
relationship between citizens

00:26:16.890 --> 00:26:18.264
and other citizens,
allowing them

00:26:18.264 --> 00:26:22.950
to organize like the MoveOn, or
the Occupy, or the Arab Spring

00:26:22.950 --> 00:26:23.866
movements.

00:26:23.866 --> 00:26:25.740
In some places, it's
changed the relationship

00:26:25.740 --> 00:26:27.270
between the citizen
and the state,

00:26:27.270 --> 00:26:29.935
enabling a more collaborative
form of government--

00:26:29.935 --> 00:26:33.907
e-petitions, online
consultations.

00:26:33.907 --> 00:26:35.490
It's definitely
transformed the nature

00:26:35.490 --> 00:26:40.080
of campaigning between
party and activist

00:26:40.080 --> 00:26:43.080
and between party and voter.

00:26:43.080 --> 00:26:46.650
Activism is obviously, almost
entirely done online now--

00:26:46.650 --> 00:26:49.380
the organization of it,
the central organization.

00:26:49.380 --> 00:26:52.290
And Cambridge Analytica,
and the Brexit,

00:26:52.290 --> 00:26:55.860
and 2016 American referendum
show that, increasingly,

00:26:55.860 --> 00:26:58.320
big data and the
technology surrounding it

00:26:58.320 --> 00:27:01.500
are used to pinpoint each of us
based on psychological profiles

00:27:01.500 --> 00:27:04.350
or profiles of what we like
in order to influence us

00:27:04.350 --> 00:27:05.590
in a particular way.

00:27:05.590 --> 00:27:07.740
Now, everyone gets very
upset about this stuff

00:27:07.740 --> 00:27:09.227
or very excited about it.

00:27:09.227 --> 00:27:10.310
And I think it's right to.

00:27:10.310 --> 00:27:12.180
But it's ultimately
an example of what

00:27:12.180 --> 00:27:14.462
I call faster-horses thinking.

00:27:14.462 --> 00:27:16.170
The reason I call it
that is because when

00:27:16.170 --> 00:27:18.180
Henry Ford, the inventor
of the automobile,

00:27:18.180 --> 00:27:22.380
was asked what did people tell
you they wanted, he replied,

00:27:22.380 --> 00:27:24.240
faster horses.

00:27:24.240 --> 00:27:26.220
It's sometimes difficult
for us to conceive,

00:27:26.220 --> 00:27:28.260
in politics, of systems
that are so radically

00:27:28.260 --> 00:27:29.550
different from our own.

00:27:29.550 --> 00:27:31.410
And instead, we just
think of technologies

00:27:31.410 --> 00:27:34.640
as augmenting or supercharging
what we already have.

00:27:34.640 --> 00:27:37.260
And so the changes that I've
just described to democracy

00:27:37.260 --> 00:27:38.070
are all profound.

00:27:38.070 --> 00:27:40.470
But they don't change the
nature of democracy itself.

00:27:40.470 --> 00:27:43.380
They work within the system
to which we are presently

00:27:43.380 --> 00:27:45.330
accustomed.

00:27:45.330 --> 00:27:47.820
And I wonder if that's going
to be sustainable or true

00:27:47.820 --> 00:27:49.601
within our lifetime.

00:27:49.601 --> 00:27:51.600
I suggest there'll be
four challenges to the way

00:27:51.600 --> 00:27:53.920
that we currently
think about democracy.

00:27:53.920 --> 00:27:56.610
The first is the one that I
described in the introduction.

00:27:56.610 --> 00:27:59.790
If bots get to this stage
where they are good enough

00:27:59.790 --> 00:28:03.270
to debate in a way that
is more rational and more

00:28:03.270 --> 00:28:06.860
persuasive than us, or
even if they don't, how--

00:28:06.860 --> 00:28:08.790
and a lot of political
speech takes place

00:28:08.790 --> 00:28:10.860
in online platforms--
how on Earth are we

00:28:10.860 --> 00:28:13.950
supposed to sustain a system
of deliberation in which you

00:28:13.950 --> 00:28:17.640
and I have meaningful say
when, every time we speak,

00:28:17.640 --> 00:28:21.384
we're shot down or presented
with 50 facts to the contrary.

00:28:21.384 --> 00:28:22.800
Now, remember that,
in the future,

00:28:22.800 --> 00:28:24.960
bots aren't going to be
disembodied lines of code.

00:28:24.960 --> 00:28:26.130
They'll have human faces.

00:28:26.130 --> 00:28:29.820
They'll be able-- if the sensors
are there to detect human

00:28:29.820 --> 00:28:30.550
emotion--

00:28:30.550 --> 00:28:33.010
they'll be persuasive
and real-seeming.

00:28:33.010 --> 00:28:35.130
So deliberation,
which has been part

00:28:35.130 --> 00:28:37.830
of our concept of
democracy since Greece,

00:28:37.830 --> 00:28:40.020
could be completely disrupted
by a technology that's

00:28:40.020 --> 00:28:43.110
already underway.

00:28:43.110 --> 00:28:44.920
No one really talks
about that that much.

00:28:44.920 --> 00:28:47.336
I think that's something that
could be a problem within 10

00:28:47.336 --> 00:28:48.360
or 15 years.

00:28:48.360 --> 00:28:49.770
And that's pretty profound.

00:28:49.770 --> 00:28:52.060
Second big challenge is,
we're now entering a time

00:28:52.060 --> 00:28:56.010
where, easily, it's
foreseeable that we could

00:28:56.010 --> 00:28:57.810
have full direct
democracy where,

00:28:57.810 --> 00:29:01.470
basically, using a smartphone
or whatever replaces it.

00:29:01.470 --> 00:29:03.720
We vote on the issues
of the day directly

00:29:03.720 --> 00:29:05.160
with no need for politicians.

00:29:05.160 --> 00:29:09.090
Or wiki democracy, where we
edit the laws ourselves--

00:29:09.090 --> 00:29:10.640
some model of it.

00:29:10.640 --> 00:29:13.560
It's absolutely not technically
infeasible in the course

00:29:13.560 --> 00:29:15.070
of our lifetimes.

00:29:15.070 --> 00:29:18.570
We need to re-have the debate
about whether that's desirable.

00:29:18.570 --> 00:29:21.330
How much democracy is
too much democracy?

00:29:21.330 --> 00:29:23.271
Why is democracy valuable
in the first place?

00:29:23.271 --> 00:29:25.020
I don't think we're
ready for that debate.

00:29:25.020 --> 00:29:26.700
I don't think it's one
we've started happening.

00:29:26.700 --> 00:29:28.800
It wouldn't surprise me at
all if the natural offshoots

00:29:28.800 --> 00:29:30.175
of the populist
movements that we

00:29:30.175 --> 00:29:36.150
see just now is a demand for
more direct accountability

00:29:36.150 --> 00:29:38.100
for political decisions--
people will vote

00:29:38.100 --> 00:29:41.040
using stuff in their pockets.

00:29:41.040 --> 00:29:45.150
Data democracy-- it's going to
become increasingly weird that

00:29:45.150 --> 00:29:48.840
we consider a system legitimate
on the basis that we put a tick

00:29:48.840 --> 00:29:51.780
in a box once every five years--

00:29:51.780 --> 00:29:55.470
an almost inconceivably
small amount of data

00:29:55.470 --> 00:29:58.470
is used to constitute the
government of the day.

00:29:58.470 --> 00:30:01.170
I think there's a theoretical
and philosophical challenge

00:30:01.170 --> 00:30:04.590
to be made about a system which
uses the abundance of data,

00:30:04.590 --> 00:30:07.704
which really reflects the
lives that we actually lead

00:30:07.704 --> 00:30:09.120
and the role that
that should play

00:30:09.120 --> 00:30:10.740
in legitimizing governments.

00:30:10.740 --> 00:30:13.380
That is to say, if a government
doesn't pay attention

00:30:13.380 --> 00:30:16.380
to the data that actually
exists about its people,

00:30:16.380 --> 00:30:18.717
how can it be really
said to represent them?

00:30:18.717 --> 00:30:19.800
It's interesting question.

00:30:19.800 --> 00:30:21.716
It's one that we haven't
currently got to yet.

00:30:21.716 --> 00:30:24.030
I suspect it will
rise in salience.

00:30:24.030 --> 00:30:26.770
And the final question is
going to be about AI democracy.

00:30:26.770 --> 00:30:30.680
It's not a tool not to consider
as we entrust Artificial

00:30:30.680 --> 00:30:33.890
Intelligence systems with
more and more valuable

00:30:33.890 --> 00:30:37.400
things-- trading on the stock
market, robots conducting

00:30:37.400 --> 00:30:40.640
operations.

00:30:40.640 --> 00:30:44.720
One was appointed to the board
of a company in Singapore--

00:30:44.720 --> 00:30:46.400
that we might ask,
what role should

00:30:46.400 --> 00:30:49.750
AI's play in the decision
of public policymaking,

00:30:49.750 --> 00:30:51.890
in the decisions made
by public policymakers?

00:30:51.890 --> 00:30:57.702
Which areas of politics
would we be better served

00:30:57.702 --> 00:30:59.660
with systems taking the
decision on our behalf,

00:30:59.660 --> 00:31:03.410
perhaps, according to principles
that are agreed democratically?

00:31:03.410 --> 00:31:05.960
Or should we each have an
AI system in our pocket

00:31:05.960 --> 00:31:08.180
which votes in our
behalf 10,000 times

00:31:08.180 --> 00:31:12.207
a day on the issues of
the day based on the data

00:31:12.207 --> 00:31:13.790
that it has about
us and what it knows

00:31:13.790 --> 00:31:17.180
about our preferences
and our lived experience?

00:31:17.180 --> 00:31:19.610
We're just at the cusp
of these questions.

00:31:19.610 --> 00:31:22.430
But the system of democracy
that we have is a very old one.

00:31:22.430 --> 00:31:24.380
And it would very
much surprise me

00:31:24.380 --> 00:31:27.890
if faster horses was all we got,
if the disruption we've already

00:31:27.890 --> 00:31:29.450
seen to democracy
was the last we

00:31:29.450 --> 00:31:31.700
saw of democratic disruption.

00:31:31.700 --> 00:31:34.010
That would seem to me to
be against the grain of how

00:31:34.010 --> 00:31:39.050
the digital really is
becoming political.

00:31:39.050 --> 00:31:42.755
Final concept-- social justice.

00:31:42.755 --> 00:31:44.880
When political theorists
talk about social justice,

00:31:44.880 --> 00:31:47.170
they tend to be
one of two things.

00:31:47.170 --> 00:31:50.220
First is distribution-- how
should benefits and burdens

00:31:50.220 --> 00:31:52.740
be distributed in society?

00:31:52.740 --> 00:31:55.470
Equally, according to
some principle of merit,

00:31:55.470 --> 00:31:59.160
to the best, disproportionately,
to the most needy?

00:31:59.160 --> 00:32:01.780
These are all arguments
that philosophers have had

00:32:01.780 --> 00:32:05.634
and politicians have
had for generations.

00:32:05.634 --> 00:32:07.050
And in the past,
they were settled

00:32:07.050 --> 00:32:11.190
by the market, which
distributed goods among us

00:32:11.190 --> 00:32:13.350
and by the state, which
kind of intervened

00:32:13.350 --> 00:32:15.684
and regulated the
distribution of those goods.

00:32:15.684 --> 00:32:17.100
Increasingly, it's
algorithms that

00:32:17.100 --> 00:32:20.100
are being used to
distribute goods in society.

00:32:20.100 --> 00:32:24.280
72% of CVs-- or resumes,
for an American audience--

00:32:24.280 --> 00:32:27.660
are never seen by human eyes.

00:32:27.660 --> 00:32:31.470
The systems that make
decisions about who gets jobs

00:32:31.470 --> 00:32:35.190
have profound distributive
consequences for who does well

00:32:35.190 --> 00:32:37.620
and who doesn't in society.

00:32:37.620 --> 00:32:42.880
Mortgages, insurance,
a whole host

00:32:42.880 --> 00:32:47.556
of other distributively
important things

00:32:47.556 --> 00:32:48.680
are affected by algorithms.

00:32:48.680 --> 00:32:50.388
For example, the fact
that algorithms now

00:32:50.388 --> 00:32:53.200
trade on the stock market
has caused a ballooning

00:32:53.200 --> 00:32:55.900
in the wealth that flows to
people who use those automated

00:32:55.900 --> 00:32:56.730
systems--

00:32:56.730 --> 00:32:58.560
mostly banks.

00:32:58.560 --> 00:33:00.950
That has distributive
consequences.

00:33:00.950 --> 00:33:03.730
So what political
philosophers typically thought

00:33:03.730 --> 00:33:07.060
of as a question of
political economy--

00:33:07.060 --> 00:33:09.212
the market and the state--

00:33:09.212 --> 00:33:11.170
that question of social
justice is increasingly

00:33:11.170 --> 00:33:13.990
entrusted to the people
who write those algorithms.

00:33:13.990 --> 00:33:16.794
That's the first way
that technology is

00:33:16.794 --> 00:33:18.085
going to affect social justice.

00:33:20.650 --> 00:33:24.280
But there's more to justice than
just the distribution of stuff.

00:33:24.280 --> 00:33:30.030
When we see the slave kneeling
at the feet of the master,

00:33:30.030 --> 00:33:32.620
or the woman cowering
before her husband,

00:33:32.620 --> 00:33:37.330
or the person from a black
or minority ethnic community

00:33:37.330 --> 00:33:40.150
having insults hurled
at them, the injustice

00:33:40.150 --> 00:33:42.610
there has nothing to do with
the distribution of stuff.

00:33:42.610 --> 00:33:45.160
It's what's called an
injustice of recognition

00:33:45.160 --> 00:33:49.180
where we fail to accord
each human being the dignity

00:33:49.180 --> 00:33:51.489
and respect that they deserve.

00:33:51.489 --> 00:33:53.530
Now, in the past, it was
really only other people

00:33:53.530 --> 00:33:55.480
who could disrespect
us in this way.

00:33:55.480 --> 00:33:59.542
In the future, as we've seen,
it can be systems, as well.

00:33:59.542 --> 00:34:01.000
If you think of
the frustration you

00:34:01.000 --> 00:34:03.257
feel when your computer
doesn't work today,

00:34:03.257 --> 00:34:05.590
imagine what it's going to
be like when one doesn't even

00:34:05.590 --> 00:34:08.279
recognize your face because
it's the wrong color

00:34:08.279 --> 00:34:09.820
or because it doesn't
hear your voice

00:34:09.820 --> 00:34:14.380
because you're the wrong gender
or because it doesn't let you

00:34:14.380 --> 00:34:16.300
into the nightclub
because your face doesn't

00:34:16.300 --> 00:34:22.510
meet the right specifications
that the club owner has set.

00:34:22.510 --> 00:34:28.179
Technology is increasingly used
in questions of recognition.

00:34:28.179 --> 00:34:34.030
And I think that's a profound
importance for social justice.

00:34:34.030 --> 00:34:36.400
The other way that
technology affects justice

00:34:36.400 --> 00:34:40.120
is that it ranks us.

00:34:40.120 --> 00:34:43.250
Today, we all know what the
currency of social status is.

00:34:43.250 --> 00:34:49.050
Increasingly, it's likes,
it's retweets, it's followers.

00:34:49.050 --> 00:34:51.060
People who, half a
century ago, would not

00:34:51.060 --> 00:34:53.310
have held high
status in society,

00:34:53.310 --> 00:34:55.242
now hold high status in society.

00:34:55.242 --> 00:34:57.450
And the reason they do is
because of a particular set

00:34:57.450 --> 00:34:59.730
of algorithms designed
by people like you

00:34:59.730 --> 00:35:04.500
have been set which decide
what the key factors are.

00:35:04.500 --> 00:35:05.610
Who's in and who's out?

00:35:05.610 --> 00:35:06.960
Who's up and who's down?

00:35:06.960 --> 00:35:09.390
Who's seen and who is unseen?

00:35:09.390 --> 00:35:12.090
Who's great and who's nothing?

00:35:12.090 --> 00:35:14.700
Now, there's nothing inherently
nefarious about this, nothing

00:35:14.700 --> 00:35:17.220
inherently wrong with it.

00:35:17.220 --> 00:35:20.250
But it used to be that only
people, and our social norms,

00:35:20.250 --> 00:35:23.400
and occasionally laws like the
Nuremberg laws or the Jim Crow

00:35:23.400 --> 00:35:27.120
laws, which specifically
discriminated against people,

00:35:27.120 --> 00:35:30.750
were the things that decided
the politics of recognition.

00:35:30.750 --> 00:35:32.570
Now that's done by technology.

00:35:32.570 --> 00:35:34.710
And it's increasingly
in the hands of people

00:35:34.710 --> 00:35:37.320
who aren't politicians and who
aren't necessarily philosophers

00:35:37.320 --> 00:35:39.490
either.

00:35:39.490 --> 00:35:40.890
So just stepping back--

00:35:40.890 --> 00:35:44.380
power, democracy,
freedom, justice--

00:35:44.380 --> 00:35:46.890
these used to be words
that just politicians

00:35:46.890 --> 00:35:51.030
and political philosophers used
in their day-to-day discourse.

00:35:51.030 --> 00:35:54.510
I say that they have to be words
that software engineers use

00:35:54.510 --> 00:35:57.000
in their day-to-day discourse
and that tech firms know

00:35:57.000 --> 00:35:59.940
and are familiar
with and understand.

00:35:59.940 --> 00:36:02.310
I'd like to close
with two quotes that

00:36:02.310 --> 00:36:05.580
have always stuck out to me.

00:36:05.580 --> 00:36:08.010
The first is this-- and
you might have heard it--

00:36:08.010 --> 00:36:10.080
"The philosophers
have only interpreted

00:36:10.080 --> 00:36:15.520
the world in various ways;
the point is to change it."

00:36:15.520 --> 00:36:16.740
The second is this--

00:36:16.740 --> 00:36:22.040
"We're not analyzing the
world; we're building it."

00:36:22.040 --> 00:36:24.290
And essentially, they
mean the same thing.

00:36:24.290 --> 00:36:26.529
What they say is you can
talk, and you can think,

00:36:26.529 --> 00:36:27.320
and you can debate.

00:36:27.320 --> 00:36:29.450
But the real people
who create change

00:36:29.450 --> 00:36:31.430
are those who go out and do it.

00:36:31.430 --> 00:36:33.140
The first quote
is from Karl Marx,

00:36:33.140 --> 00:36:37.100
it's from his "Theses
on Feuerbach" in 1845.

00:36:37.100 --> 00:36:39.080
It was a rallying cry
for Revolutionaries

00:36:39.080 --> 00:36:42.650
for more than a century
after it was published.

00:36:42.650 --> 00:36:44.780
The second quote is
from Tim Berners-Lee who

00:36:44.780 --> 00:36:47.390
couldn't be more different from
Karl Marx and his politics,

00:36:47.390 --> 00:36:50.872
his temperament, or indeed,
his choice of facial hair.

00:36:50.872 --> 00:36:52.910
But the point's the same--

00:36:52.910 --> 00:36:54.620
the digital is political.

00:36:54.620 --> 00:37:00.330
Software engineers are
increasingly social engineers.

00:37:00.330 --> 00:37:02.010
And that's the
message of my talk.

00:37:02.010 --> 00:37:03.096
Thank you very much.

00:37:03.096 --> 00:37:05.576
[APPLAUSE]

00:37:10.729 --> 00:37:12.270
SPEAKER: Thank you
very much, indeed.

00:37:12.270 --> 00:37:14.027
We do have time for questions.

00:37:17.376 --> 00:37:19.750
AUDIENCE: So what do you think
of the increasing tendency

00:37:19.750 --> 00:37:23.860
of governments to abdicate
responsibility to tech firms

00:37:23.860 --> 00:37:24.970
to make decisions?

00:37:24.970 --> 00:37:27.760
The classic example, in
the last week I think,

00:37:27.760 --> 00:37:30.760
the EU has said we want tech
firms to make the decision

00:37:30.760 --> 00:37:33.740
and take things
down within an hour.

00:37:33.740 --> 00:37:36.555
Do you think that's
a good trend?

00:37:36.555 --> 00:37:37.930
JAMIE SUSSKIND:
I'm not sure what

00:37:37.930 --> 00:37:39.870
you mean by the abdication
of responsibility.

00:37:39.870 --> 00:37:42.870
AUDIENCE: Or the
delegation, if you want--

00:37:42.870 --> 00:37:45.730
where the government could
choose to regulate, but instead

00:37:45.730 --> 00:37:47.890
choose to say you must decide.

00:37:47.890 --> 00:37:49.840
JAMIE SUSSKIND: The
message I have is this.

00:37:52.450 --> 00:37:55.490
If it's the case that
there's going to be--

00:37:55.490 --> 00:37:58.210
that tech firms are going
to be taking decisions that

00:37:58.210 --> 00:38:00.470
are of political
significance, in due course,

00:38:00.470 --> 00:38:03.306
people are going to expect to
know what those are, to demand

00:38:03.306 --> 00:38:04.930
transference, to
demand accountability,

00:38:04.930 --> 00:38:07.860
to demand regulation.

00:38:07.860 --> 00:38:10.960
Tech firms essentially
have two choices--

00:38:10.960 --> 00:38:13.130
not mutually inconsistent.

00:38:13.130 --> 00:38:18.430
They can try to get it right
themselves and articulate

00:38:18.430 --> 00:38:21.460
why they think they're trying
to get it right to set out,

00:38:21.460 --> 00:38:24.040
clearly, the way their
algorithms work insofar

00:38:24.040 --> 00:38:26.500
as is possible in
the market system

00:38:26.500 --> 00:38:28.480
to justify them by
reference to principles

00:38:28.480 --> 00:38:31.480
of justice or principles
of democracy or freedom.

00:38:31.480 --> 00:38:34.540
The more of that that is done
privately and willingly by tech

00:38:34.540 --> 00:38:36.490
firms, the less likely
it is that the state

00:38:36.490 --> 00:38:38.672
is going to come barging
in and start regulating.

00:38:38.672 --> 00:38:39.880
And we've actually seen that.

00:38:39.880 --> 00:38:43.060
I think tech firms
are increasingly

00:38:43.060 --> 00:38:45.430
becoming answerable to the
unhappiness or the perceived

00:38:45.430 --> 00:38:47.580
unhappiness of their
consumers about the way

00:38:47.580 --> 00:38:49.730
that things are working.

00:38:49.730 --> 00:38:52.077
But I think if the state
just came trundling in

00:38:52.077 --> 00:38:53.660
and started regulating,
the tech firms

00:38:53.660 --> 00:38:55.690
would say the same as any
private corporation have said

00:38:55.690 --> 00:38:57.370
since the invention
of the state, which

00:38:57.370 --> 00:39:00.310
is, I can't believe these fools
at the center of government

00:39:00.310 --> 00:39:02.830
are trampling all over matters
that they don't understand--

00:39:02.830 --> 00:39:04.330
these Gladstones.

00:39:04.330 --> 00:39:06.622
But we have to find a compromise
between the Gladstones

00:39:06.622 --> 00:39:08.913
and the Faradays-- the people
who know a lot about tech

00:39:08.913 --> 00:39:10.850
and the people who know
a lot about politics.

00:39:10.850 --> 00:39:14.140
And I think if tech firms
assume responsibility,

00:39:14.140 --> 00:39:18.160
they're less likely to face
regulation which they consider

00:39:18.160 --> 00:39:19.750
to be dumb or ill-informed.

00:39:22.570 --> 00:39:24.137
AUDIENCE: Thank you.

00:39:24.137 --> 00:39:25.640
So when you said--

00:39:25.640 --> 00:39:29.050
and I think it was in the first
quarter or half of the talk

00:39:29.050 --> 00:39:32.650
regarding the
privatization of policy

00:39:32.650 --> 00:39:35.800
through the use of
tech in these firms--

00:39:35.800 --> 00:39:38.050
where does open source fit
into this and free software

00:39:38.050 --> 00:39:39.050
and that whole movement?

00:39:39.050 --> 00:39:40.290
Because one would argue--

00:39:40.290 --> 00:39:42.040
and I think a lot of
people would probably

00:39:42.040 --> 00:39:45.550
agree with me--
that open source is

00:39:45.550 --> 00:39:48.460
a political movement of tech.

00:39:48.460 --> 00:39:52.090
So before it was really
known in the private world

00:39:52.090 --> 00:39:53.680
that tech would
become political.

00:39:53.680 --> 00:39:56.300
So where does that fit
into this whole picture,

00:39:56.300 --> 00:39:58.240
and how does it
change the equation?

00:39:58.240 --> 00:39:59.080
JAMIE SUSSKIND: It's
a great question.

00:39:59.080 --> 00:40:00.663
And the answer is
it obviously doesn't

00:40:00.663 --> 00:40:03.406
fit into the very simple
dichotomy that I gave.

00:40:03.406 --> 00:40:04.780
But I think it's
also fair to say

00:40:04.780 --> 00:40:08.290
that, although the open-source
movement has become incredibly

00:40:08.290 --> 00:40:12.354
important in many respects, most
people don't know what it is.

00:40:12.354 --> 00:40:14.020
Most people, when
they use technologies,

00:40:14.020 --> 00:40:16.480
don't have the opportunity
to customize or edit

00:40:16.480 --> 00:40:18.160
those technologies
or to understand

00:40:18.160 --> 00:40:21.550
the rules that govern them.

00:40:21.550 --> 00:40:24.700
If more tech was open
source, that would definitely

00:40:24.700 --> 00:40:27.130
resolve some of the
tension between what

00:40:27.130 --> 00:40:29.320
appears to be private
entities exercising

00:40:29.320 --> 00:40:31.570
a kind of public
power if they're

00:40:31.570 --> 00:40:35.050
using code that can be at
least seen and understood

00:40:35.050 --> 00:40:38.270
by its consumers.

00:40:38.270 --> 00:40:41.530
I just don't think
it yet characterizes

00:40:41.530 --> 00:40:45.490
a lot of the technologies
of power that I described.

00:40:45.490 --> 00:40:48.420
AUDIENCE: OK, thank you.

00:40:48.420 --> 00:40:49.780
AUDIENCE: Thank you.

00:40:49.780 --> 00:40:52.860
I'm also going back to the
issue of privatization.

00:40:52.860 --> 00:40:54.990
And I think, in
some ways, we could

00:40:54.990 --> 00:40:56.550
argue that there's
a benefit here

00:40:56.550 --> 00:41:01.860
that, with an increased number
of actors making decisions,

00:41:01.860 --> 00:41:05.830
we get pluralism, and
that's not a terrible thing.

00:41:05.830 --> 00:41:08.100
But I think that maybe--

00:41:08.100 --> 00:41:11.010
I wonder if you can reflect
on whether this claim

00:41:11.010 --> 00:41:16.530
of privatization is as
solid as you suggest.

00:41:16.530 --> 00:41:19.470
A lot of these technologies
were funded by public bodies,

00:41:19.470 --> 00:41:20.860
by the state.

00:41:20.860 --> 00:41:26.039
And I wonder if we need
to revisit the genesis

00:41:26.039 --> 00:41:27.330
of a lot of these technologies.

00:41:27.330 --> 00:41:31.320
Because we often forget that
these were funded by taxpayers,

00:41:31.320 --> 00:41:34.430
and that they're not strictly
private architectures

00:41:34.430 --> 00:41:35.961
or private systems.

00:41:35.961 --> 00:41:38.460
JAMIE SUSSKIND: I think it's a
really valuable and important

00:41:38.460 --> 00:41:39.410
point.

00:41:39.410 --> 00:41:42.190
There are two
reflections I would make.

00:41:42.190 --> 00:41:45.270
The first is the fact
that a technology

00:41:45.270 --> 00:41:49.710
derives from public
investment doesn't necessarily

00:41:49.710 --> 00:41:52.350
mean that the public
retains a degree of control

00:41:52.350 --> 00:41:53.919
or transparency
or accountability.

00:41:53.919 --> 00:41:56.210
It's the use and the application
of the technology that

00:41:56.210 --> 00:41:58.418
matters for political
purposes, for the ones that I'm

00:41:58.418 --> 00:42:02.880
describing, rather than
the genesis of them.

00:42:02.880 --> 00:42:06.090
The second point that I maybe
I didn't make strongly enough

00:42:06.090 --> 00:42:11.070
in my speech is that, a
lot of time the alternative

00:42:11.070 --> 00:42:14.630
to technologies being
controlled privately

00:42:14.630 --> 00:42:18.150
is technologies being
controlled by the state.

00:42:18.150 --> 00:42:21.660
And there are huge,
enormous risks with that.

00:42:21.660 --> 00:42:26.940
The modern state is already the
most powerful and remarkable

00:42:26.940 --> 00:42:30.600
system of control that
humans have ever invented.

00:42:30.600 --> 00:42:33.570
The idea of endowing the
state, through regulation

00:42:33.570 --> 00:42:36.750
or nationalization or whatever
it is as some people suggest,

00:42:36.750 --> 00:42:39.570
with further power in the
form of awesome technologies

00:42:39.570 --> 00:42:42.570
of surveillance, of force,
and of perception control

00:42:42.570 --> 00:42:47.370
is not something that I
would welcome inherently.

00:42:47.370 --> 00:42:49.560
So actually, the big
political tension I say,

00:42:49.560 --> 00:42:51.270
for the next half
century or so, is

00:42:51.270 --> 00:42:53.370
going to be how
much of this stuff

00:42:53.370 --> 00:42:57.000
is best left to the custodians
in the private sector acting

00:42:57.000 --> 00:42:58.120
responsibly?

00:42:58.120 --> 00:43:01.740
And how much should be brought
under the aegis of the state?

00:43:01.740 --> 00:43:05.205
But it's certainly not a kind
of state/good regulation/good

00:43:05.205 --> 00:43:07.102
privatization/bad dichotomy.

00:43:07.102 --> 00:43:09.060
I'm not just saying that
because I'm at Google.

00:43:09.060 --> 00:43:10.684
I think the argument
is often forgotten

00:43:10.684 --> 00:43:13.620
by those who
criticize tech firms,

00:43:13.620 --> 00:43:19.470
that the state can act in a
pretty heavy-handed way when

00:43:19.470 --> 00:43:21.400
it comes to technology as well.

00:43:21.400 --> 00:43:23.400
There's a balance to be struck.

00:43:23.400 --> 00:43:26.160
AUDIENCE: I guess you probably
part answered my question just

00:43:26.160 --> 00:43:27.270
now.

00:43:27.270 --> 00:43:32.700
But my question is, in a
similar sense about the-- like

00:43:32.700 --> 00:43:35.870
what option does the
regulator even have?

00:43:35.870 --> 00:43:38.980
And I'm thinking now
of a global scale.

00:43:38.980 --> 00:43:46.740
So the status quo as I see it--
and tell me if you disagree--

00:43:46.740 --> 00:43:51.040
regulation is always playing
catch-up with technology.

00:43:51.040 --> 00:43:53.460
And the question
is, if the regulator

00:43:53.460 --> 00:43:56.010
wants to turn this around, what
option would they even have?

00:43:56.010 --> 00:44:01.140
Because if one
country would start

00:44:01.140 --> 00:44:04.530
trying to invert
this and basically

00:44:04.530 --> 00:44:10.140
try to have regulation be the
default, and as a technologist,

00:44:10.140 --> 00:44:12.660
you would basically have
to seek an exception

00:44:12.660 --> 00:44:16.410
for every single thing you want
to do rather than what is now--

00:44:16.410 --> 00:44:21.000
like, technology companies
invent new paradigms

00:44:21.000 --> 00:44:24.030
that affect society, and
then regulation catches up.

00:44:24.030 --> 00:44:29.040
So obviously, if one state
started trying to invert this,

00:44:29.040 --> 00:44:33.450
tech firms would probably
move away from that country

00:44:33.450 --> 00:44:35.190
and do their
innovation elsewhere.

00:44:35.190 --> 00:44:39.660
And there would always be sort
of islands of deregulation

00:44:39.660 --> 00:44:46.570
as there are islands of tax
harbors and that kind of thing.

00:44:46.570 --> 00:44:50.730
So if you think it from
that point of view,

00:44:50.730 --> 00:44:53.830
what's your view on that?

00:44:53.830 --> 00:44:56.730
JAMIE SUSSKIND: Well, you've
identified two problems

00:44:56.730 --> 00:44:58.530
that the regulator faces.

00:44:58.530 --> 00:45:00.060
One is you're always behind.

00:45:00.060 --> 00:45:01.560
The technologies
are invented first,

00:45:01.560 --> 00:45:02.760
and then you're kind
of playing catch-up

00:45:02.760 --> 00:45:05.280
to try and understand their
implications and, if necessary,

00:45:05.280 --> 00:45:06.520
regulate them.

00:45:06.520 --> 00:45:09.320
The second is the problem of
multinational corporations.

00:45:09.320 --> 00:45:11.070
If you're just one
country, it's very hard

00:45:11.070 --> 00:45:14.610
to set a rule that
others don't follow,

00:45:14.610 --> 00:45:17.580
which might place you at
some kind of disadvantage

00:45:17.580 --> 00:45:20.325
economically or commercially and
incentivize that firm to leave.

00:45:20.325 --> 00:45:22.200
There are other problems
too like the problem

00:45:22.200 --> 00:45:24.741
that regulators sometimes
don't have the best people,

00:45:24.741 --> 00:45:26.490
the best people are
in the private sector.

00:45:26.490 --> 00:45:28.281
I know that with the
regulation of finance,

00:45:28.281 --> 00:45:30.581
for instance, that's
a consistent problem.

00:45:30.581 --> 00:45:32.580
So there's no doubt that
the task for regulators

00:45:32.580 --> 00:45:35.922
is formidable.

00:45:35.922 --> 00:45:36.880
What are their options?

00:45:36.880 --> 00:45:38.640
Well, they've got
to do their best.

00:45:38.640 --> 00:45:40.825
Tech firms, I think,
shouldn't just

00:45:40.825 --> 00:45:42.450
see it as a matter
of we'll do whatever

00:45:42.450 --> 00:45:44.035
we like until we're regulated.

00:45:44.035 --> 00:45:45.660
I think the whole
system would function

00:45:45.660 --> 00:45:50.500
better if purely commercial
considerations didn't just

00:45:50.500 --> 00:45:52.570
motivate the policy
set by tech firms.

00:45:52.570 --> 00:45:54.270
And increasingly, they don't.

00:45:54.270 --> 00:45:59.260
I wouldn't, for a second,
suggest that they always do.

00:45:59.260 --> 00:46:02.860
The problem of international
movement of capital

00:46:02.860 --> 00:46:06.920
or of competitive
advantage is a tough one.

00:46:06.920 --> 00:46:11.700
The EU is actually not a
bad counterpoint to that.

00:46:11.700 --> 00:46:13.692
The GDPR, say what
you like about it,

00:46:13.692 --> 00:46:15.400
it's a kind of
regulation, and it applies

00:46:15.400 --> 00:46:16.720
to every country in Europe.

00:46:16.720 --> 00:46:19.282
And that makes it easy for them
to act in concerted fashion.

00:46:19.282 --> 00:46:19.990
I would welcome--

00:46:22.510 --> 00:46:24.190
I see technology
like climate change

00:46:24.190 --> 00:46:25.960
as one of those
issues that benefits

00:46:25.960 --> 00:46:29.860
from international
collaboration and cooperation.

00:46:29.860 --> 00:46:32.050
Partly, the way we think
about it though is we

00:46:32.050 --> 00:46:33.790
think about it as
an economic problem.

00:46:33.790 --> 00:46:37.510
Like, the power
that tech companies

00:46:37.510 --> 00:46:40.090
or the problems that can
be caused by technology

00:46:40.090 --> 00:46:41.440
are just matters of economics.

00:46:41.440 --> 00:46:42.710
And this is actually
part of the mindset

00:46:42.710 --> 00:46:44.210
that I want to try
and change, which

00:46:44.210 --> 00:46:48.040
is, we have to start seeing
them as political problems.

00:46:48.040 --> 00:46:51.790
And I would hope and encourage,
for the part of states,

00:46:51.790 --> 00:46:57.580
that they don't
deregulate or create

00:46:57.580 --> 00:47:03.302
Wild Wests out of a desire to
attain an economic advantage.

00:47:03.302 --> 00:47:04.510
Countries do do that, though.

00:47:04.510 --> 00:47:06.120
There's just no doubt about it.

00:47:06.120 --> 00:47:08.830
So I hold my hands up, and I
say the task of the regulator

00:47:08.830 --> 00:47:10.190
is formidable.

00:47:10.190 --> 00:47:14.440
I think there's so little
regulation just now, though.

00:47:14.440 --> 00:47:17.950
And technologies are
becoming so much more

00:47:17.950 --> 00:47:19.990
persuasive and so
much more powerful,

00:47:19.990 --> 00:47:21.250
something will be done.

00:47:21.250 --> 00:47:23.860
As I said earlier, the
more that tech firms

00:47:23.860 --> 00:47:26.942
are involved in that
proactively and sensibly

00:47:26.942 --> 00:47:28.900
the better it will be
for them, for the states,

00:47:28.900 --> 00:47:30.524
and for the people
who use the systems.

00:47:32.319 --> 00:47:32.860
AUDIENCE: Hi.

00:47:32.860 --> 00:47:35.890
I have a question more about
the concentration of power

00:47:35.890 --> 00:47:40.180
and the accountability which
people demand after that.

00:47:40.180 --> 00:47:42.670
Increasingly, companies
like Google or Facebook,

00:47:42.670 --> 00:47:44.950
they've become public
utilities where

00:47:44.950 --> 00:47:49.210
we use search or a social
network on a daily basis.

00:47:49.210 --> 00:47:51.190
And that is the
concentration of power.

00:47:51.190 --> 00:47:53.350
Do you think, in 20
years from now, we

00:47:53.350 --> 00:47:56.650
will see a Google
or a Facebook that's

00:47:56.650 --> 00:47:59.620
held accountable,
maybe, inside the state,

00:47:59.620 --> 00:48:02.830
and we actually vote on
how that's regulated?

00:48:02.830 --> 00:48:04.330
JAMIE SUSSKIND:
Well, I certainly

00:48:04.330 --> 00:48:07.630
don't think nationalization,
public ownership

00:48:07.630 --> 00:48:11.740
of things like Google or
Facebook would be a good thing.

00:48:11.740 --> 00:48:14.050
I also am not sure if
public utility is--

00:48:14.050 --> 00:48:16.300
I think it's the best
word we've probably

00:48:16.300 --> 00:48:19.546
got just now to describe
the kinds of status

00:48:19.546 --> 00:48:21.670
that these companies have
within our modern economy

00:48:21.670 --> 00:48:22.940
and within our modern
society, but I don't think

00:48:22.940 --> 00:48:24.520
it accurately describes it.

00:48:24.520 --> 00:48:27.490
Most public utilities
don't exert power over us.

00:48:27.490 --> 00:48:28.210
We rely on them.

00:48:28.210 --> 00:48:30.460
We rely on the water company,
the electricity company,

00:48:30.460 --> 00:48:32.260
but they don't get us to do
things we wouldn't otherwise

00:48:32.260 --> 00:48:32.810
do.

00:48:32.810 --> 00:48:34.900
They don't influence elections.

00:48:34.900 --> 00:48:36.850
They don't determine
matters of social justice

00:48:36.850 --> 00:48:40.690
or what is and isn't permitted.

00:48:40.690 --> 00:48:44.740
So I think the public-utility
analogy is helpful only up unto

00:48:44.740 --> 00:48:46.001
a point.

00:48:46.001 --> 00:48:47.500
Do I think that,
in the future, it's

00:48:47.500 --> 00:48:49.210
possible they would
be nationalized

00:48:49.210 --> 00:48:51.990
or part of the state?

00:48:51.990 --> 00:48:52.510
I guess so.

00:48:52.510 --> 00:48:54.730
I think it would
be not sensible.

00:48:54.730 --> 00:49:00.785
But again, I think the
regulatory environment,

00:49:00.785 --> 00:49:02.410
the regulatory future,
is up for grabs.

00:49:07.800 --> 00:49:09.590
AUDIENCE: So you
talked briefly about

00:49:09.590 --> 00:49:12.590
how people still have this
mindset of faster horses when

00:49:12.590 --> 00:49:14.610
it comes to technology a lot.

00:49:14.610 --> 00:49:17.480
What are the hallmarks
or what time scale

00:49:17.480 --> 00:49:20.240
do you expect this
public mindset

00:49:20.240 --> 00:49:23.780
to shift from just
thinking of technology

00:49:23.780 --> 00:49:26.990
as like a step change
rather than like a big step

00:49:26.990 --> 00:49:30.535
forward and a revolutionary
aspect of technology?

00:49:30.535 --> 00:49:32.660
JAMIE SUSSKIND: It's a
really interesting question.

00:49:32.660 --> 00:49:34.820
And I'm not going to give
you a defined timescale

00:49:34.820 --> 00:49:37.730
because I think, again,
it's up for grabs.

00:49:37.730 --> 00:49:42.790
I think what I try to do in my
book is to sound the fog horn

00:49:42.790 --> 00:49:44.540
and say we need to
think about this stuff,

00:49:44.540 --> 00:49:47.780
not just as consumers,
but as citizens.

00:49:47.780 --> 00:49:50.090
We need to not think about
it like faster horses

00:49:50.090 --> 00:49:52.460
but to see the fundamental
revolutionary change.

00:49:52.460 --> 00:49:54.740
A, some people are going to
disagree with that thesis.

00:49:54.740 --> 00:49:57.090
B, a lot of people aren't
going to be interested in it.

00:49:57.090 --> 00:49:59.840
They're just going to be
interested in interacting

00:49:59.840 --> 00:50:02.090
with technology as consumers,
which is what most of us

00:50:02.090 --> 00:50:04.100
do most of the time--
that looks cool.

00:50:04.100 --> 00:50:06.620
This is a cool new function--
without necessarily seeing

00:50:06.620 --> 00:50:09.862
the huge broader picture.

00:50:09.862 --> 00:50:11.570
So I don't have an
answer to the question

00:50:11.570 --> 00:50:15.110
as to when I expect, if at all,
public perception of this stuff

00:50:15.110 --> 00:50:16.076
to change.

00:50:16.076 --> 00:50:17.450
I do think that
market forces are

00:50:17.450 --> 00:50:21.320
likely to result in the
transformations I described.

00:50:21.320 --> 00:50:27.009
So insofar as the political
classes paying attention,

00:50:27.009 --> 00:50:28.550
I think, easily
within our lifetimes,

00:50:28.550 --> 00:50:32.540
we're going to see the big
question of politics change

00:50:32.540 --> 00:50:34.550
from what it was in
the last century,

00:50:34.550 --> 00:50:37.469
which was, to what extent
should the state be

00:50:37.469 --> 00:50:39.260
involved in the
functioning of the economy?

00:50:39.260 --> 00:50:41.600
And to what extent should things
be left to the free market?

00:50:41.600 --> 00:50:43.490
That was, like, the
big ideological debate

00:50:43.490 --> 00:50:44.797
of the last century.

00:50:44.797 --> 00:50:46.880
I think the big ideological
debate of our lifetime

00:50:46.880 --> 00:50:50.540
is, to what extent should we
be subject to digital systems,

00:50:50.540 --> 00:50:51.950
and on what terms?

00:50:51.950 --> 00:50:54.170
And I see, over the
course of our lifetime,

00:50:54.170 --> 00:50:56.749
the debate shifting that way
because I see it as almost

00:50:56.749 --> 00:50:58.790
inevitable if the technologies
develop in the way

00:50:58.790 --> 00:51:00.040
that people predict they will.

00:51:03.370 --> 00:51:04.980
AUDIENCE: You said
a few times, you'd

00:51:04.980 --> 00:51:08.910
like to see technology
companies, technologists, get

00:51:08.910 --> 00:51:11.320
more involved in politics.

00:51:11.320 --> 00:51:13.920
In a lot of people's
heads, that's

00:51:13.920 --> 00:51:15.570
equated with
lobbying, which tends

00:51:15.570 --> 00:51:16.710
to be seen as a bad thing.

00:51:16.710 --> 00:51:18.793
Can you talk about maybe
some of the positive ways

00:51:18.793 --> 00:51:21.150
you can see technologists
or technology companies get

00:51:21.150 --> 00:51:22.080
involved in politics?

00:51:22.080 --> 00:51:25.530
JAMIE SUSSKIND: In fact, I
think that analogy perfectly

00:51:25.530 --> 00:51:29.010
demonstrates the change in
mindset I think we need.

00:51:29.010 --> 00:51:30.900
Powerful companies in
the past-- say, like,

00:51:30.900 --> 00:51:32.940
the great monopolies of
the early 19th century--

00:51:32.940 --> 00:51:34.800
had power in the
political process.

00:51:34.800 --> 00:51:38.040
But they exerted it
indirectly through lobbying

00:51:38.040 --> 00:51:39.966
and through campaign finance.

00:51:39.966 --> 00:51:41.340
What's different
about technology

00:51:41.340 --> 00:51:43.470
is that it affects us directly.

00:51:43.470 --> 00:51:45.900
If you're a tech
firm, you don't need

00:51:45.900 --> 00:51:47.640
to go through the
government in order

00:51:47.640 --> 00:51:50.580
to exert power over people or
to affect democracy or affect

00:51:50.580 --> 00:51:52.920
freedom or justice.

00:51:52.920 --> 00:51:56.560
That's what's so profoundly
different about technology.

00:51:56.560 --> 00:51:58.830
And so I say that people
who work in tech firms

00:51:58.830 --> 00:52:01.830
do work in politics
because their inventions,

00:52:01.830 --> 00:52:04.650
their algorithms, their
systems are the ones

00:52:04.650 --> 00:52:07.800
that are actually changing
the way that people live

00:52:07.800 --> 00:52:10.500
and changing the way
that we live together.

00:52:10.500 --> 00:52:12.930
So it's not Mark Zuckerberg
should run for president.

00:52:12.930 --> 00:52:15.600
It's Mark Zuckerberg
is already, in a sense,

00:52:15.600 --> 00:52:18.870
some kind of president
because he affects all of us

00:52:18.870 --> 00:52:20.970
in ways that he should
know about more.

00:52:20.970 --> 00:52:23.020
And so he should
take that power,

00:52:23.020 --> 00:52:25.860
as I'm sure he does,
responsibly and seriously.

00:52:25.860 --> 00:52:28.680
So what I don't want
people to go away thinking

00:52:28.680 --> 00:52:31.140
is I'm saying that
we need technologists

00:52:31.140 --> 00:52:33.300
to step into the
political process more,

00:52:33.300 --> 00:52:34.800
although, there
should definitely

00:52:34.800 --> 00:52:36.600
be constructive engagement.

00:52:36.600 --> 00:52:39.570
The point is that, if you work
in technology, in a sense,

00:52:39.570 --> 00:52:41.224
you already work in politics.

00:52:41.224 --> 00:52:43.544
AUDIENCE: So what's the
positive improvement

00:52:43.544 --> 00:52:45.217
that you'd like to see?

00:52:45.217 --> 00:52:47.550
JAMIE SUSSKIND: The positive
improvement I'd like to see

00:52:47.550 --> 00:52:51.510
is the Tim Berners-Lee idea
of philosophical engineers.

00:52:51.510 --> 00:52:53.910
He's the one who said,
we're not analyzing a world;

00:52:53.910 --> 00:52:56.190
we're creating it, and we're
philosophical engineers.

00:52:56.190 --> 00:52:58.060
Well, sometimes.

00:52:58.060 --> 00:53:00.610
The arc of a computer
science degree is long,

00:53:00.610 --> 00:53:03.210
but it doesn't necessarily
bend towards justice.

00:53:03.210 --> 00:53:06.300
Just like people who
know a lot about politics

00:53:06.300 --> 00:53:09.590
shouldn't be assumed to
know a lot about technology,

00:53:09.590 --> 00:53:11.490
I think that people
who work in technology

00:53:11.490 --> 00:53:15.910
should have a good grounding
in the values and principles

00:53:15.910 --> 00:53:17.580
that they are--
whether they know it

00:53:17.580 --> 00:53:20.310
or not-- embedding
in their work.

00:53:20.310 --> 00:53:22.496
And that's why I wrote
the book in many ways.

00:53:22.496 --> 00:53:23.870
It's a book about
tech for people

00:53:23.870 --> 00:53:25.330
who know a lot about politics.

00:53:25.330 --> 00:53:26.871
It's a book about
politics for people

00:53:26.871 --> 00:53:29.480
who know a lot about tech.

00:53:29.480 --> 00:53:30.540
AUDIENCE: Hi.

00:53:30.540 --> 00:53:34.050
So my question is, considering
that, in private companies,

00:53:34.050 --> 00:53:37.850
the end goal or the incentive
is usually to make their users

00:53:37.850 --> 00:53:38.600
happy--

00:53:38.600 --> 00:53:41.700
which is starkly different from
what the state cares about,

00:53:41.700 --> 00:53:46.010
which is to promote the general
well-being of their citizens--

00:53:46.010 --> 00:53:48.260
it's hard for me
to think of things

00:53:48.260 --> 00:53:51.590
like filtering content
as an exertion of power

00:53:51.590 --> 00:53:56.870
rather than an enabler of their
users to exercise their freedom

00:53:56.870 --> 00:53:58.620
as they would like.

00:53:58.620 --> 00:54:01.640
And so I guess my
question is, when you're

00:54:01.640 --> 00:54:06.140
saying that technologists should
be these social engineers,

00:54:06.140 --> 00:54:09.380
do you think that requires a
fundamental shift in what we're

00:54:09.380 --> 00:54:13.400
prioritizing in adopting this
more paternalistic approach

00:54:13.400 --> 00:54:16.400
towards, oh, we think this
would be good for our users

00:54:16.400 --> 00:54:20.464
rather than, this is what the
evidence shows our users like?

00:54:20.464 --> 00:54:22.130
JAMIE SUSSKIND: Again,
a great question.

00:54:22.130 --> 00:54:24.884
And if I may, I'll unpick it.

00:54:24.884 --> 00:54:27.050
Do I think there needs to
be a change in priorities?

00:54:27.050 --> 00:54:29.870
My first answer would be to
dodge and say I don't know.

00:54:29.870 --> 00:54:32.240
Because most of the
algorithms that you describe

00:54:32.240 --> 00:54:33.764
are not made public.

00:54:33.764 --> 00:54:35.930
And if you look at what
Jack Dorsey said to Congress

00:54:35.930 --> 00:54:37.100
the other day--

00:54:37.100 --> 00:54:40.250
and one can applaud
him for bringing it

00:54:40.250 --> 00:54:43.850
to the public's attention-- he
basically said we got it wrong.

00:54:43.850 --> 00:54:46.280
600,000 accounts,
including the accounts

00:54:46.280 --> 00:54:48.020
of some members of
Congress, were wrongly

00:54:48.020 --> 00:54:50.150
deprioritized from
political discourse

00:54:50.150 --> 00:54:53.720
at quite a sensitive time.

00:54:53.720 --> 00:54:57.630
The answer to that, to my mind,
would be a Twitter algorithm

00:54:57.630 --> 00:54:59.770
that people are capable
of understanding

00:54:59.770 --> 00:55:02.090
and that people are capable
of critiquing rather

00:55:02.090 --> 00:55:04.997
than a one-paragraph
explanation from Twitter which

00:55:04.997 --> 00:55:07.580
says what their policies are and
says "and many other factors"

00:55:07.580 --> 00:55:08.750
at the end.

00:55:08.750 --> 00:55:10.780
We, the users, are
not in a position,

00:55:10.780 --> 00:55:13.310
either to know whether
the algorithm actually

00:55:13.310 --> 00:55:17.270
embodies the values
that are stated

00:55:17.270 --> 00:55:19.700
and, to a certain extent,
what the values are.

00:55:19.700 --> 00:55:22.370
So the first thing, and one of
things I talk about in my book

00:55:22.370 --> 00:55:25.310
is, the more transparent
companies are,

00:55:25.310 --> 00:55:27.475
the more people will be
comfortable and justifiably

00:55:27.475 --> 00:55:29.600
comfortable, just as they
are with governments that

00:55:29.600 --> 00:55:33.200
become more transparent, that
the people who exercise power

00:55:33.200 --> 00:55:37.650
over them are doing it
in a responsible way,

00:55:37.650 --> 00:55:40.910
even if it's just a
small amount of power.

00:55:40.910 --> 00:55:44.090
The second thing I would
say is that you correctly

00:55:44.090 --> 00:55:47.186
identify that the
intentions of the state

00:55:47.186 --> 00:55:48.560
are different from
the intentions

00:55:48.560 --> 00:55:51.870
of a private company operating
within a market system.

00:55:51.870 --> 00:55:55.070
The difficulty with the
market-system approach to tech,

00:55:55.070 --> 00:55:57.710
to just letting the market
do its job is, first of all,

00:55:57.710 --> 00:55:59.150
you get monopolies.

00:55:59.150 --> 00:56:01.780
And so even if I
don't like Facebook,

00:56:01.780 --> 00:56:04.260
if I want to be on a
social-networking system--

00:56:04.260 --> 00:56:06.760
there's no point moving to one,
which is just me and my mum,

00:56:06.760 --> 00:56:09.706
even if it's superior
in loads of respects

00:56:09.706 --> 00:56:11.330
because there's a
network effect there,

00:56:11.330 --> 00:56:15.210
and Facebook has dominated it.

00:56:15.210 --> 00:56:17.600
The second is you'd also-- it
relates back to the first--

00:56:17.600 --> 00:56:20.510
we don't always know the
principles on which companies

00:56:20.510 --> 00:56:22.040
are competing.

00:56:22.040 --> 00:56:24.650
The difference between
the way that news

00:56:24.650 --> 00:56:27.560
is ranked in one system and
news is ranked in another system

00:56:27.560 --> 00:56:29.600
is apparent only
from what we see.

00:56:29.600 --> 00:56:31.800
But we don't always
know what we don't see.

00:56:31.800 --> 00:56:36.200
And so I think it's hard to say
that people are empowered fully

00:56:36.200 --> 00:56:40.007
to make decisions like
that if, A, they don't have

00:56:40.007 --> 00:56:41.840
a choice, because there's
a monopoly, and B,

00:56:41.840 --> 00:56:45.470
they actually aren't shown
the full basis of their choice

00:56:45.470 --> 00:56:47.600
that they have to make.

00:56:50.480 --> 00:56:54.500
You are right though that a
pluralist system where people

00:56:54.500 --> 00:56:58.700
have a choice of moving between
systems of competing values

00:56:58.700 --> 00:57:01.700
according to their
values would definitely

00:57:01.700 --> 00:57:05.810
be one solution to the problem
of what might be perceived

00:57:05.810 --> 00:57:07.490
to be too much
power concentration

00:57:07.490 --> 00:57:08.970
or too much unaccountability.

00:57:08.970 --> 00:57:09.720
That's one answer.

00:57:12.112 --> 00:57:13.570
SPEAKER: We do have
more questions,

00:57:13.570 --> 00:57:15.560
but we're unfortunately
out of time.

00:57:15.560 --> 00:57:17.650
So thank you again very
much, Jamie Susskind.

00:57:17.650 --> 00:57:21.600
[APPLAUSE]

