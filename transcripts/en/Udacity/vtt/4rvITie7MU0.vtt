WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:00.650
All right.
So there's

00:00:00.650 --> 00:00:02.740
going to be a couple key
ideas to make this work.

00:00:02.740 --> 00:00:05.180
And one of them is the simulation lemma.

00:00:05.180 --> 00:00:07.820
The idea of the simulation
lemma is that if we have

00:00:07.820 --> 00:00:12.630
a pretty good estimate of the real MDP,
then optimizing our rewards in that

00:00:12.630 --> 00:00:15.220
estimate is going to be
pretty good in the real MDP.

00:00:15.220 --> 00:00:18.640
&gt;&gt; This is a lemma about simulations,
not a simulation about a lemma.

00:00:18.640 --> 00:00:20.190
&gt;&gt; That's right.
It's not a simulated lemma.

00:00:21.240 --> 00:00:22.780
It's a lemma about simulation, right?

00:00:22.780 --> 00:00:26.720
And so the idea is that
we've got transitions and

00:00:26.720 --> 00:00:29.550
rewards that are off by alpha or
less, and

00:00:29.550 --> 00:00:33.270
we want to think about what happens
if we adopt some policy pi.

00:00:33.270 --> 00:00:33.950
Any policy pi.

00:00:33.950 --> 00:00:39.010
We want to compare the value we get for
following pi in MDP 1.

00:00:39.010 --> 00:00:42.700
That has transition function T1 and
reward function R1,

00:00:42.700 --> 00:00:47.620
to the value that we get, return that
we get for following policy pi in

00:00:47.620 --> 00:00:52.592
mdp V2 which has transition
function T2 and reward function R2.

00:00:52.592 --> 00:00:53.630
&gt;&gt; Okay.

00:00:53.630 --> 00:00:58.130
&gt;&gt; And if those are going to be near
each other given that T1 is near T2 and

00:00:58.130 --> 00:00:58.840
R1 is near R2,

00:00:58.840 --> 00:01:02.670
then that's going to give us the ability
to have accurate simulations, right.

00:01:02.670 --> 00:01:05.660
We can use our model of
the MVP to stimulate.

00:01:05.660 --> 00:01:07.570
What's going to happen in the real MVP.

00:01:07.570 --> 00:01:11.250
And just to be concrete, this is what I
mean by the transition functions, reward

00:01:11.250 --> 00:01:14.270
functions not being too different,
than we have this value alpha.

00:01:14.270 --> 00:01:19.040
And if we take the maximum over all
state action x state triples, that

00:01:19.040 --> 00:01:23.250
the probability assigned by transition
function T1 and the probability assigned

00:01:23.250 --> 00:01:27.510
by transaction function T2 are not
different by more than alpha.

00:01:27.510 --> 00:01:28.940
And, same thing with the rewards.

00:01:28.940 --> 00:01:29.960
&gt;&gt; So that's a little weird.

00:01:29.960 --> 00:01:32.840
&gt;&gt; How so?
&gt;&gt; Well, because the transition model

00:01:32.840 --> 00:01:36.170
is probability, so
that can't ever be off by more than one.

00:01:36.170 --> 00:01:36.820
&gt;&gt; Sure.

00:01:36.820 --> 00:01:40.450
&gt;&gt; And the rewards can be gigantic
numbers that can be off by billions.

00:01:40.450 --> 00:01:41.020
&gt;&gt; Sure.

00:01:41.020 --> 00:01:42.020
&gt;&gt; Billions and billions.

00:01:42.020 --> 00:01:43.300
So, it just seems a little weird.

00:01:43.300 --> 00:01:45.250
&gt;&gt; You feel like it should
be two different alphas.

00:01:45.250 --> 00:01:48.340
Yeah, alpha T and
alpha R, but maybe not.

00:01:48.340 --> 00:01:51.470
&gt;&gt; Yeah, I mean I didn't want
to proliferate variables

00:01:51.470 --> 00:01:53.460
when we didn't really need to,
but you're right.

00:01:53.460 --> 00:01:55.400
The scale of these could
be very different.

00:01:55.400 --> 00:02:00.995
A lot of times in the proofs that
people use for these kinds of MBPs.

00:02:00.995 --> 00:02:05.005
They first assume that rewards are all
in the range zero to one, also.

00:02:05.005 --> 00:02:07.955
It doesn't change the fact that you
still might want a different alpha for

00:02:07.955 --> 00:02:12.320
the two cases, but it does at least get
the scale More approximately correctly.

00:02:12.320 --> 00:02:14.100
&gt;&gt; Okay, so
then without loss of generality,

00:02:14.100 --> 00:02:16.580
assume that all your rewards
are between minus one and one.

00:02:16.580 --> 00:02:17.350
&gt;&gt; Or zero and one.

00:02:17.350 --> 00:02:18.130
&gt;&gt; Or zero and one.

00:02:18.130 --> 00:02:18.790
Why not?

00:02:18.790 --> 00:02:20.355
Because it's all the same MDP.

00:02:20.355 --> 00:02:22.970
&gt;&gt; [LAUGH] Yeah, they look more
like probabilities or something.

00:02:22.970 --> 00:02:24.140
&gt;&gt; Okay, cool.
So,

00:02:24.140 --> 00:02:26.720
without loss of generality assume that
your rewards are between zero and

00:02:26.720 --> 00:02:29.490
one and your transition probabilities
which Would have to be here between

00:02:29.490 --> 00:02:33.040
zero and one so that we can use the same
alpha and everything kind of works out.

00:02:33.040 --> 00:02:33.710
&gt;&gt; Yeah.
Again,

00:02:33.710 --> 00:02:35.340
it still might be worth
having different alphas.

00:02:35.340 --> 00:02:36.620
But yeah, you're right.

00:02:36.620 --> 00:02:38.980
Again, it puts us in
the right ball park.

00:02:38.980 --> 00:02:42.690
&gt;&gt; Okay.
So this is a ball park simulation.

00:02:42.690 --> 00:02:44.375
I'm perfectly fine with that.

00:02:44.375 --> 00:02:46.080
&gt;&gt; [LAUGH]
&gt;&gt; Now we're out of the ballpark.

00:02:46.080 --> 00:02:50.050
So that's either a vacuum cleaner or
a metal detector,

00:02:50.050 --> 00:02:53.960
or it's a gigantic baseball
being knocked out of a park.

00:02:53.960 --> 00:02:57.650
&gt;&gt; It was supposed to be a ballpark,
but it looks to me more like

00:02:57.650 --> 00:03:00.280
some kind of terrible
&gt;&gt; Cruel alien.

00:03:00.280 --> 00:03:01.890
&gt;&gt; [LAUGH]
&gt;&gt; So

00:03:01.890 --> 00:03:02.670
I
&gt;&gt; With

00:03:02.670 --> 00:03:04.340
&gt;&gt; I don't know.

00:03:04.340 --> 00:03:05.050
&gt;&gt; Head issues.
&gt;&gt; Anyway,

00:03:05.050 --> 00:03:08.780
the point is that, it's.it's alpha
close to a reasonable pictures.

00:03:08.780 --> 00:03:09.620
&gt;&gt; And that's alpha.

00:03:09.620 --> 00:03:10.120
Okay.

