WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:02.600
Welcome back to Computer Vision.

00:00:02.600 --> 00:00:07.130
Today we're going to be learning
about a technique that is important,

00:00:07.130 --> 00:00:10.430
in general, but
we sort of slid it here and

00:00:10.430 --> 00:00:14.960
we're talking about recognition under
the generative model description.

00:00:14.960 --> 00:00:18.720
So, last time we introduced which
I hope for you is the last time.

00:00:18.720 --> 00:00:19.970
Certainly for me it was the last time.

00:00:21.020 --> 00:00:22.760
They say,
you never forget your first time.

00:00:22.760 --> 00:00:24.120
Me, I can barely remember the last time.

00:00:26.190 --> 00:00:29.310
Last time we introduced generative
models for classification.

00:00:29.310 --> 00:00:31.630
And there are a couple of
points about generative models.

00:00:31.630 --> 00:00:33.060
Meghan's still giggling
in the back there.

00:00:34.720 --> 00:00:38.870
One of the points was that in general
models you build the new model for

00:00:38.870 --> 00:00:41.240
every new object category.

00:00:41.240 --> 00:00:42.830
And you don't really
think too much about

00:00:42.830 --> 00:00:44.520
the other objects that you know about.

00:00:44.520 --> 00:00:46.140
And you just you,
you build the description.

00:00:46.140 --> 00:00:49.370
And then when new things come in you
see how likely is the new thing for

00:00:49.370 --> 00:00:51.860
each of the description,
each of the models that you have.

00:00:51.860 --> 00:00:52.840
But there was another point and

00:00:52.840 --> 00:00:56.160
that was perhaps a little more subtle
because it was kind of Flew by.

00:00:56.160 --> 00:01:01.320
And that is that generative models
work best in lower dimensional spaces.

00:01:01.320 --> 00:01:05.290
And what I mean by that is since we're
going to build these probabilistic

00:01:05.290 --> 00:01:09.910
descriptions usually some sort of
probability density function, it's only

00:01:09.910 --> 00:01:14.670
plausible to estimate those, and you've
got a modest number of dimensions.

00:01:14.670 --> 00:01:16.550
If you have very high dimen,
number of dimensions, or

00:01:16.550 --> 00:01:19.910
even, even, even just a moderate
number of dimensions.

00:01:19.910 --> 00:01:24.860
To estimate a real probability density
requires lots and lots and lots of data.

00:01:24.860 --> 00:01:27.660
So today what we're going to do
is we're going to learn about one

00:01:27.660 --> 00:01:30.760
of the most common and, and, perhaps, I
don't want to sa, call it important, but

00:01:30.760 --> 00:01:34.160
it, but, just everybody would expect
that you would understand it.

00:01:34.160 --> 00:01:36.230
Method of dimensionality reduction,

00:01:36.230 --> 00:01:39.300
which referred to as
principal component analysis.

00:01:39.300 --> 00:01:42.530
You may have seen this in other
signal stuff, and maybe even in one

00:01:42.530 --> 00:01:46.380
of your algebra, but it became,
pretty significant in Computer Vision

00:01:46.380 --> 00:01:49.340
because of this particular piece
of work that I'll tell you about.

00:01:49.340 --> 00:01:51.980
And it, because of that it
gets used all the time.

