WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.423
[MUSIC PLAYING]

00:00:04.683 --> 00:00:05.850
SARAH SIRAJUDDIN: I'm Sarah.

00:00:05.850 --> 00:00:08.700
I'm the engineering lead
for TensorFlow Lite.

00:00:08.700 --> 00:00:10.830
And I'm really happy
to be back at I/O

00:00:10.830 --> 00:00:13.940
again, talking about
TensorFlow Lite again.

00:00:13.940 --> 00:00:14.770
TIM DAVIS: Woo-hoo.

00:00:14.770 --> 00:00:17.350
Thank you to all of you
for joining us here today.

00:00:17.350 --> 00:00:17.850
I'm Tim.

00:00:17.850 --> 00:00:19.838
I'm the product manager
for TensorFlow Lite.

00:00:19.838 --> 00:00:22.380
And today we're here to tell
you about doing machine learning

00:00:22.380 --> 00:00:25.913
on mobile and IoT devices.

00:00:25.913 --> 00:00:27.830
SARAH SIRAJUDDIN: So I
expect that most of you

00:00:27.830 --> 00:00:31.580
here are already familiar
with what machine learning is,

00:00:31.580 --> 00:00:33.990
so I won't be going into that.

00:00:33.990 --> 00:00:37.760
But let's talk instead
about what is TensorFlow.

00:00:37.760 --> 00:00:41.030
TensorFlow is Google's open
source, cross-platform machine

00:00:41.030 --> 00:00:42.390
learning framework.

00:00:42.390 --> 00:00:44.480
It allows you to
build your models

00:00:44.480 --> 00:00:47.990
and deploy them to servers,
browsers, and all the way

00:00:47.990 --> 00:00:51.360
to edge devices.

00:00:51.360 --> 00:00:53.910
It's a full end-to-end
ecosystem which

00:00:53.910 --> 00:00:56.880
goes all the way from
research to production,

00:00:56.880 --> 00:00:59.910
from training in the data center
to deployment and edge like,

00:00:59.910 --> 00:01:01.110
I said.

00:01:01.110 --> 00:01:03.840
TensorFlow has support
for multiple languages--

00:01:03.840 --> 00:01:05.789
Swift, JavaScript, and Python.

00:01:08.450 --> 00:01:11.950
TIM DAVIS: So what is TensorFlow
Lite, and how does it fit in?

00:01:11.950 --> 00:01:15.550
TensorFlow Lite is TensorFLow's
cross-platform framework

00:01:15.550 --> 00:01:19.540
for deploying ML on mobile
devices and embedded systems.

00:01:19.540 --> 00:01:21.670
You can take your
existing TensorFlow models

00:01:21.670 --> 00:01:24.432
and convert them over to
TensorFlow Lite easily.

00:01:24.432 --> 00:01:26.140
But I wanted to walk
you through why this

00:01:26.140 --> 00:01:29.080
is so important to TensorFlow.

00:01:29.080 --> 00:01:31.330
There has been a
global explosion

00:01:31.330 --> 00:01:35.380
in Edge ML, driven by the
need for user experiences

00:01:35.380 --> 00:01:40.400
that require low latency and
closer knit interactions.

00:01:40.400 --> 00:01:42.620
Further drivers, like
poor network connectivity

00:01:42.620 --> 00:01:45.740
in many geographical regions
around the world and user

00:01:45.740 --> 00:01:49.580
privacy requirements, have
all fueled the need for ML

00:01:49.580 --> 00:01:51.910
on device.

00:01:51.910 --> 00:01:54.820
This has led to a whole
revolution of machine learning

00:01:54.820 --> 00:01:58.270
and product innovation in nearly
every single industry vertical.

00:01:58.270 --> 00:02:01.000
We see it all over
the place, driven

00:02:01.000 --> 00:02:03.640
by on-device machine learning
running in environments that

00:02:03.640 --> 00:02:09.360
utilize small compute, that have
a constrained memory capacity,

00:02:09.360 --> 00:02:12.570
and consume low power.

00:02:12.570 --> 00:02:15.590
So that's why the TensorFlow
team decided to invest heavily

00:02:15.590 --> 00:02:19.760
in making it easy to
develop, build, and deploy ML

00:02:19.760 --> 00:02:22.160
that is cross-platform capable.

00:02:22.160 --> 00:02:25.310
TensorFlow Lite can be
deployed on Android, iOS,

00:02:25.310 --> 00:02:27.720
Linux, and other platforms.

00:02:27.720 --> 00:02:30.170
It's easier than ever
to use TensorFlow

00:02:30.170 --> 00:02:32.420
and convert your model
to TensorFlow Lite

00:02:32.420 --> 00:02:34.932
and deploy it anywhere.

00:02:34.932 --> 00:02:36.390
SARAH SIRAJUDDIN:
So at this point,

00:02:36.390 --> 00:02:39.440
you might be wondering what can
you do with TensorFlow Lite.

00:02:39.440 --> 00:02:42.740
We really want you to be able
to solve any kind of problem

00:02:42.740 --> 00:02:45.860
that you can imagine
directly on the device.

00:02:45.860 --> 00:02:47.630
And later in this
talk, we will be

00:02:47.630 --> 00:02:49.730
talking about the
many ways developers

00:02:49.730 --> 00:02:51.620
are using TensorFlow Lite.

00:02:51.620 --> 00:02:54.440
But first, I want to
show you a video of a fun

00:02:54.440 --> 00:02:57.560
demo that we built, which was
featured in yesterday's dev

00:02:57.560 --> 00:02:58.490
note--

00:02:58.490 --> 00:03:00.200
developer keynote.

00:03:00.200 --> 00:03:02.480
This highlights the
cutting edge of what is

00:03:02.480 --> 00:03:04.760
possible with TensorFlow Lite.

00:03:04.760 --> 00:03:06.920
The demo is called Dance Like.

00:03:06.920 --> 00:03:08.992
Let's roll the video.

00:03:08.992 --> 00:03:09.904
[VIDEO PLAYBACK]

00:03:09.904 --> 00:03:11.880
[MUSIC PLAYING]

00:03:11.880 --> 00:03:14.010
- Dance Like
enables you to learn

00:03:14.010 --> 00:03:17.310
how to dance on a mobile phone.

00:03:17.310 --> 00:03:20.700
- TensorFlow can take
our smartphone camera

00:03:20.700 --> 00:03:24.837
and turn it into a powerful
tool for analyzing body pose.

00:03:24.837 --> 00:03:27.420
- We had a team at Google that
had developed an advanced model

00:03:27.420 --> 00:03:29.165
for doing pose segmentation.

00:03:29.165 --> 00:03:31.040
So we were able to take
their implementation,

00:03:31.040 --> 00:03:33.030
convert it into TensorFlow Lite.

00:03:33.030 --> 00:03:36.180
Once we had it there, we
could use it directly.

00:03:36.180 --> 00:03:40.560
- To run all the AI and machine
learning models to detect body

00:03:40.560 --> 00:03:43.140
parts, it's a very
computationally expensive

00:03:43.140 --> 00:03:47.250
process where we need to
use the on-device GPU.

00:03:47.250 --> 00:03:50.040
TensorFlow Library
made it possible so

00:03:50.040 --> 00:03:51.990
that we can leverage
all these resources,

00:03:51.990 --> 00:03:56.790
the compute on the device, and
give a great user experience.

00:03:56.790 --> 00:03:59.520
- Teaching people to dance is
just the tip of the iceberg.

00:03:59.520 --> 00:04:02.910
Anything that involves movement
would be a great candidate.

00:04:02.910 --> 00:04:05.340
- So that means
people who have skills

00:04:05.340 --> 00:04:07.860
can teach other
people those skills.

00:04:07.860 --> 00:04:10.860
And AI is just this layer
that really just interfaces

00:04:10.860 --> 00:04:12.240
between the two things.

00:04:12.240 --> 00:04:14.654
When you empower
people to teach people,

00:04:14.654 --> 00:04:16.529
I think that's really
when you have something

00:04:16.529 --> 00:04:18.366
that is game-changing.

00:04:25.656 --> 00:04:27.725
[END PLAYBACK]

00:04:27.725 --> 00:04:28.850
TIM DAVIS: All right, cool.

00:04:28.850 --> 00:04:31.590
So to build Dance Like, as
I talked about yesterday,

00:04:31.590 --> 00:04:33.510
we built ourselves
this audacious goal

00:04:33.510 --> 00:04:37.800
of running five on-device tasks
in parallel, in real time,

00:04:37.800 --> 00:04:40.800
without sacrificing performance.

00:04:40.800 --> 00:04:43.050
And I want to walk you
through what they were.

00:04:43.050 --> 00:04:47.130
So we're running two body
part segmentation models.

00:04:47.130 --> 00:04:50.350
We're matching the segmentation
models in real time.

00:04:50.350 --> 00:04:52.830
We're running
dynamic time warping.

00:04:52.830 --> 00:04:54.995
We're playing a video
and encoding a video.

00:04:54.995 --> 00:04:56.370
And let me emphasize
this again--

00:04:56.370 --> 00:04:59.740
this is all running on device.

00:04:59.740 --> 00:05:02.560
And to show you, I'm actually
going to do a live demo.

00:05:02.560 --> 00:05:04.690
I spend a lot of I/O dancing.

00:05:04.690 --> 00:05:08.290
And if we just cut to
the app, what you'll see

00:05:08.290 --> 00:05:13.190
is there's a few dancers
you can choose from,

00:05:13.190 --> 00:05:14.740
so real time in slow mo.

00:05:14.740 --> 00:05:19.470
I'm going to like slow mo
because I'm a beginner dancer.

00:05:19.470 --> 00:05:21.910
And so I can fire
up some dance moves.

00:05:21.910 --> 00:05:22.990
You can see me--

00:05:22.990 --> 00:05:25.330
the pose model running.

00:05:25.330 --> 00:05:26.860
And basically
what's happening now

00:05:26.860 --> 00:05:28.735
is it's segmenting me
out from the background

00:05:28.735 --> 00:05:31.980
and identifying different
parts of my body.

00:05:31.980 --> 00:05:34.710
And then, as I follow
along with the dancer,

00:05:34.710 --> 00:05:38.090
a second segmentation model
starts running, but this time

00:05:38.090 --> 00:05:39.380
on the dancer.

00:05:39.380 --> 00:05:44.020
So now there's two segmentation
models running by the GPU.

00:05:44.020 --> 00:05:45.560
And that produces
the matching score

00:05:45.560 --> 00:05:48.523
that you see up in the
top right-hand corner.

00:05:48.523 --> 00:05:49.940
And what that's
doing is giving me

00:05:49.940 --> 00:05:55.400
some feedback on how well
I'm matching the dancer.

00:05:55.400 --> 00:05:57.590
It's pretty cool, right?

00:05:57.590 --> 00:06:00.050
But we went further.

00:06:00.050 --> 00:06:03.440
Dancing is cool, and dancing
in slow mo isn't that cool.

00:06:03.440 --> 00:06:04.910
So what we thought
we would do is

00:06:04.910 --> 00:06:07.610
we would use
dynamic time warping

00:06:07.610 --> 00:06:12.410
to sync my slow mo moves
with the real-time dancer.

00:06:12.410 --> 00:06:15.020
And so what you get
is an effect where

00:06:15.020 --> 00:06:17.330
the user, all running
on device, can

00:06:17.330 --> 00:06:20.532
output this type of content.

00:06:20.532 --> 00:06:22.490
So you can come and try
this in the AI Sandbox.

00:06:22.490 --> 00:06:23.960
You can see for yourself.

00:06:23.960 --> 00:06:24.830
You can get a video.

00:06:24.830 --> 00:06:25.842
You can share it.

00:06:25.842 --> 00:06:27.050
And it's really, really cool.

00:06:27.050 --> 00:06:30.495
And it's all because
of TensorFlow Lite.

00:06:30.495 --> 00:06:32.120
SARAH SIRAJUDDIN:
How awesome was that?

00:06:32.120 --> 00:06:35.480
And props to Tim for agreeing
to dancing on stage at I/O,

00:06:35.480 --> 00:06:38.030
and not once, but twice.

00:06:38.030 --> 00:06:40.940
So besides dancing, what
are some other use cases

00:06:40.940 --> 00:06:44.910
that developers are using
TensorFlow Lite for?

00:06:44.910 --> 00:06:47.040
The major on-device
use cases that we

00:06:47.040 --> 00:06:49.680
see are typically related
to image and speech,

00:06:49.680 --> 00:06:52.440
so things like
segmentation, object

00:06:52.440 --> 00:06:56.550
detection, image classification,
or speech recognition.

00:06:56.550 --> 00:07:00.210
But we are also seeing a lot of
new and emerging use cases come

00:07:00.210 --> 00:07:05.890
up in the areas around content
generation and text prediction.

00:07:05.890 --> 00:07:09.340
TensorFlow Lite is now on
more than 2 billion devices

00:07:09.340 --> 00:07:13.170
around the world, running
on many different apps.

00:07:13.170 --> 00:07:16.800
Many of Google's own
largest apps are using it,

00:07:16.800 --> 00:07:21.130
as are apps from many
other external companies.

00:07:21.130 --> 00:07:23.370
So this is a sampling
of some of the apps

00:07:23.370 --> 00:07:25.200
which are using
TensorFlow Lite--

00:07:25.200 --> 00:07:29.130
Google Photos, GBoard,
YouTube, Assistant,

00:07:29.130 --> 00:07:30.960
along with several
global companies,

00:07:30.960 --> 00:07:34.780
like Uber and Airbnb.

00:07:34.780 --> 00:07:37.360
TIM DAVIS: So TensorFlow
Lite also powers ML Kit,

00:07:37.360 --> 00:07:39.460
which is our out-of-the-box
solution for deploying

00:07:39.460 --> 00:07:42.370
Google's best proprietary
models on device.

00:07:42.370 --> 00:07:44.460
You would have heard
about this yesterday, too.

00:07:44.460 --> 00:07:47.882
We are also powering
that in the back end.

00:07:47.882 --> 00:07:49.340
SARAH SIRAJUDDIN:
So now let's move

00:07:49.340 --> 00:07:51.290
on to how you can get
started with using

00:07:51.290 --> 00:07:53.640
TensorFlow Lite yourself.

00:07:53.640 --> 00:07:55.535
It's fairly simple
to get started.

00:07:55.535 --> 00:07:56.910
I'm going to walk
you through how

00:07:56.910 --> 00:08:01.320
you can use an off-the-shelf
model, or retrain your model,

00:08:01.320 --> 00:08:03.150
or use a custom
model that you may

00:08:03.150 --> 00:08:07.380
have built for your own specific
use case with TensorFlow Lite.

00:08:07.380 --> 00:08:10.350
And once you've done that,
it's really about validation

00:08:10.350 --> 00:08:14.520
and optimizing your
performance for latency, size,

00:08:14.520 --> 00:08:17.260
and accuracy.

00:08:17.260 --> 00:08:20.630
So first, let's dive into
how you can get started.

00:08:20.630 --> 00:08:23.810
As a new user, the
simplest way to get started

00:08:23.810 --> 00:08:28.100
is to download a pretrained
model from our model repository

00:08:28.100 --> 00:08:30.200
on tensorflow.org.

00:08:30.200 --> 00:08:32.330
We have models for
popular use cases

00:08:32.330 --> 00:08:36.169
like image classification,
object detection, estimating

00:08:36.169 --> 00:08:38.720
poses, and smart reply.

00:08:38.720 --> 00:08:40.880
And this is an
area where we plan

00:08:40.880 --> 00:08:42.650
to keep adding more
and more models,

00:08:42.650 --> 00:08:45.860
so please check back often.

00:08:45.860 --> 00:08:48.380
These models that are
hosted there are already

00:08:48.380 --> 00:08:50.390
in the TensorFlow
Lite model format,

00:08:50.390 --> 00:08:54.575
so you can use these
directly in the app.

00:08:54.575 --> 00:08:58.520
Now if you did not find a model
which is a good use for your--

00:08:58.520 --> 00:09:02.630
which is a good fit for your use
case, you can try retraining.

00:09:02.630 --> 00:09:04.760
And this technique is
also frequently called

00:09:04.760 --> 00:09:06.300
transfer learning.

00:09:06.300 --> 00:09:09.020
And the idea here
is that you can

00:09:09.020 --> 00:09:12.170
reuse a model that was
trained for another task

00:09:12.170 --> 00:09:16.350
as a starting point for a
model for a different task.

00:09:16.350 --> 00:09:17.900
And the reason
why this is useful

00:09:17.900 --> 00:09:19.790
is that training a
model from scratch

00:09:19.790 --> 00:09:22.880
can sometimes take days.

00:09:22.880 --> 00:09:26.640
But transfer learning can
be done in short order.

00:09:26.640 --> 00:09:28.465
Note that if you
do retrain a model,

00:09:28.465 --> 00:09:30.590
you will still need to
convert that retrained model

00:09:30.590 --> 00:09:34.730
into TensorFlow Lite's format
before you use it in an app.

00:09:34.730 --> 00:09:36.590
And later in this
talk, I will show you

00:09:36.590 --> 00:09:40.010
how you do that conversion.

00:09:40.010 --> 00:09:44.450
OK, so once we have a model
in TensorFlow Lite format,

00:09:44.450 --> 00:09:46.630
how do you use it in an app?

00:09:46.630 --> 00:09:49.080
First you load the model.

00:09:49.080 --> 00:09:51.750
Then you preprocess
your data into a format

00:09:51.750 --> 00:09:53.990
that your model will accept.

00:09:53.990 --> 00:09:55.880
Then you change your
application code

00:09:55.880 --> 00:09:59.460
to invoke the TensorFlow
Lite inference library.

00:09:59.460 --> 00:10:05.070
And finally, you use the result
of the inference in your code.

00:10:05.070 --> 00:10:07.890
So let's walk through some
code which shows this.

00:10:07.890 --> 00:10:10.950
This is code which was taken
from the image classifier

00:10:10.950 --> 00:10:11.490
example.

00:10:11.490 --> 00:10:14.530
This is hosted on our website.

00:10:14.530 --> 00:10:17.520
It's in Java, written
for the Android platform.

00:10:17.520 --> 00:10:19.560
You can see that the first
thing that we do here

00:10:19.560 --> 00:10:21.930
is that we load
the model, and then

00:10:21.930 --> 00:10:26.050
we construct the TensorFlow
Lite interpreter.

00:10:26.050 --> 00:10:29.800
We then load the image
data and preprocess it.

00:10:29.800 --> 00:10:32.050
And you'll notice that
we're using a byte buffer.

00:10:32.050 --> 00:10:37.350
And the reason we're doing that
is to optimize for performance.

00:10:37.350 --> 00:10:41.430
Next step is to run inference
and classify the images.

00:10:41.430 --> 00:10:43.150
And that's it.

00:10:43.150 --> 00:10:48.060
That's all you need to do to get
an image classifier on Android.

00:10:48.060 --> 00:10:51.030
I do want to highlight that the
example that I have run through

00:10:51.030 --> 00:10:54.210
is in Java, but TensorFlow
Lite also has bindings

00:10:54.210 --> 00:10:58.380
for Objective-C, C++,
Swift, as well as Python.

00:11:01.090 --> 00:11:02.740
So the next thing I
want to move on to

00:11:02.740 --> 00:11:04.840
is how you can use
your own custom

00:11:04.840 --> 00:11:07.950
model with TensorFlow Lite.

00:11:07.950 --> 00:11:10.740
So the high-level steps here
are that you train your model

00:11:10.740 --> 00:11:11.850
with TensorFlow.

00:11:11.850 --> 00:11:14.605
You write it out into
the saved model format.

00:11:14.605 --> 00:11:16.230
And then you would
need to convert that

00:11:16.230 --> 00:11:19.560
into TensorFlow Lite format
using TensorFlow Lite's

00:11:19.560 --> 00:11:20.790
converter.

00:11:20.790 --> 00:11:23.040
And then you make your
changes in your app

00:11:23.040 --> 00:11:28.070
to use the model like I
walked you through just now.

00:11:28.070 --> 00:11:29.710
So this is a code
snippet showing

00:11:29.710 --> 00:11:32.530
how you can convert a saved
model into TensorFlow Lite

00:11:32.530 --> 00:11:33.970
model format.

00:11:33.970 --> 00:11:36.140
As you can see, it's
fairly simple to do.

00:11:36.140 --> 00:11:38.590
It's a matter of
constructing the converter

00:11:38.590 --> 00:11:40.660
with your saved model,
and then invoking

00:11:40.660 --> 00:11:42.760
the convert function on it.

00:11:42.760 --> 00:11:46.900
Please check out our website
for more documentation on this.

00:11:46.900 --> 00:11:49.750
And it also has documentation
on how you can do this

00:11:49.750 --> 00:11:52.900
with TensorFlow 2.0, which is
the newest release coming out

00:11:52.900 --> 00:11:55.970
from TensorFlow.

00:11:55.970 --> 00:11:59.390
Speaking of conversion, we've
heard feedback from our users

00:11:59.390 --> 00:12:02.600
that TensorFlow Lite
conversion is sometimes hard.

00:12:02.600 --> 00:12:04.730
Developers sometimes
run into issues

00:12:04.730 --> 00:12:07.220
that the ops that
their models are using

00:12:07.220 --> 00:12:09.650
are not supported
with TensorFlow Lite.

00:12:09.650 --> 00:12:11.400
Or they might be
using semantics,

00:12:11.400 --> 00:12:16.160
which we don't support yet,
for example, control flows.

00:12:16.160 --> 00:12:18.700
Rest assured that we've
heard this feedback,

00:12:18.700 --> 00:12:21.290
and we are actively
working to improve it.

00:12:21.290 --> 00:12:23.650
We are building a
brand new converter.

00:12:23.650 --> 00:12:27.500
This converter is based on
MLIR, which is Google's latest

00:12:27.500 --> 00:12:30.830
compiler infrastructure.

00:12:30.830 --> 00:12:33.740
And our new converter
will be a lot more

00:12:33.740 --> 00:12:36.380
extensible and easy
to use and debug.

00:12:39.730 --> 00:12:42.900
So that was all about how
you can convert and deploy

00:12:42.900 --> 00:12:44.670
your model with TensorFlow Lite.

00:12:44.670 --> 00:12:47.010
I want to walk through some
advanced techniques, which

00:12:47.010 --> 00:12:51.380
you may use if they
are useful for you.

00:12:51.380 --> 00:12:53.690
So many developers who
use TensorFlow Lite

00:12:53.690 --> 00:12:57.800
care deeply about keeping
the binary footprint small.

00:12:57.800 --> 00:13:00.110
Selective registration
is one feature

00:13:00.110 --> 00:13:01.650
that can really help here.

00:13:01.650 --> 00:13:04.460
And the idea is that you
can only link in the ops

00:13:04.460 --> 00:13:06.170
that your model is using.

00:13:06.170 --> 00:13:11.170
And this thereby makes the
size of the binary small.

00:13:11.170 --> 00:13:12.720
So let's see how
this works in code.

00:13:12.720 --> 00:13:15.540
You create a custom
op resolver, and you

00:13:15.540 --> 00:13:20.580
replace TensorFlow Lite's
built-in op resolver with it.

00:13:20.580 --> 00:13:24.150
And then, in your build
file, you specify your model

00:13:24.150 --> 00:13:27.310
and the custom op resolver
that you just created.

00:13:27.310 --> 00:13:30.460
And then TensorFlow Lite
will scan over your model,

00:13:30.460 --> 00:13:35.610
create a repository of ops
that are used in your model.

00:13:35.610 --> 00:13:37.920
And then, when you
build the interpreter,

00:13:37.920 --> 00:13:39.870
only those ops are linked.

00:13:39.870 --> 00:13:42.450
And this in turn will reduce
the size of the binary.

00:13:45.720 --> 00:13:48.090
Another advanced feature
that I want to mention

00:13:48.090 --> 00:13:50.490
is TensorFlow Select.

00:13:50.490 --> 00:13:54.510
It allows developers to access
many of TensorFlow's ops

00:13:54.510 --> 00:13:56.460
via TensorFlow Lite.

00:13:56.460 --> 00:13:58.740
The caveat, though, is
that it does increase

00:13:58.740 --> 00:14:00.690
the size of the binary.

00:14:00.690 --> 00:14:03.690
But if your use case is not
very sensitive to the size

00:14:03.690 --> 00:14:06.690
of the binary, and you're
running into issues that you

00:14:06.690 --> 00:14:09.820
need ops which TensorFlow
Lite doesn't support yet,

00:14:09.820 --> 00:14:13.540
I highly recommend that
you check this out.

00:14:13.540 --> 00:14:15.660
So this is how it works in code.

00:14:15.660 --> 00:14:18.240
It's a small modification
to how you would convert

00:14:18.240 --> 00:14:20.220
your model to TensorFlow Lite.

00:14:20.220 --> 00:14:21.870
This is pretty
much the same code

00:14:21.870 --> 00:14:24.390
that you would use
for normal conversion.

00:14:24.390 --> 00:14:27.990
The only difference here
is that in target ops,

00:14:27.990 --> 00:14:32.040
you specify the set of
TensorFlow Select ops.

00:14:32.040 --> 00:14:33.800
You can find a lot
more documentation

00:14:33.800 --> 00:14:36.050
on how to do this
on our website.

00:14:36.050 --> 00:14:39.290
It also has
information on how this

00:14:39.290 --> 00:14:41.825
is working under the hood
as well as usage examples.

00:14:45.540 --> 00:14:48.960
We get a lot of requests
to support control flows

00:14:48.960 --> 00:14:50.400
in TensorFlow Lite.

00:14:50.400 --> 00:14:53.760
These are constructs like
loops and conditionals.

00:14:53.760 --> 00:14:55.830
We're actively working
on this, and we

00:14:55.830 --> 00:14:59.030
hope to share more about this in
the coming few months with you.

00:15:01.970 --> 00:15:04.690
And the last thing that I
want to cover in this section

00:15:04.690 --> 00:15:06.670
is on-device training.

00:15:06.670 --> 00:15:08.890
This is an exciting
new area, and we

00:15:08.890 --> 00:15:11.650
believe that this will open
up many new opportunities

00:15:11.650 --> 00:15:14.350
for research and
product innovation.

00:15:14.350 --> 00:15:17.920
We are working to add training
support to TensorFlow Lite.

00:15:17.920 --> 00:15:19.780
And at this point, I
would guess that this

00:15:19.780 --> 00:15:23.140
would be available towards the
end of the year for developers

00:15:23.140 --> 00:15:23.800
to try out.

00:15:26.032 --> 00:15:26.740
TIM DAVIS: Great.

00:15:26.740 --> 00:15:28.720
Thanks, Sarah.

00:15:28.720 --> 00:15:31.060
So now that you have your
model up and running, now

00:15:31.060 --> 00:15:33.700
you need to validate it
and get it running fast.

00:15:33.700 --> 00:15:36.940
To get started, we recommend
benchmarking your model

00:15:36.940 --> 00:15:38.710
with our benchmark tooling.

00:15:38.710 --> 00:15:41.410
This will enable you to
validate your model's accuracy,

00:15:41.410 --> 00:15:44.013
size, and performance,
and make adjustments

00:15:44.013 --> 00:15:45.055
depending on the results.

00:15:48.220 --> 00:15:50.530
Before I do get
into that, I wanted

00:15:50.530 --> 00:15:53.530
to share the key performance
goal of TensorFlow Lite,

00:15:53.530 --> 00:15:57.010
and that is to make your
models run as fast as possible

00:15:57.010 --> 00:16:00.540
on CPUs, GPUs, DSPs, and NPUs.

00:16:03.170 --> 00:16:06.072
Fast is what we care about.

00:16:06.072 --> 00:16:08.030
So if you don't know what
all those terms mean,

00:16:08.030 --> 00:16:09.650
I'll explain them now.

00:16:09.650 --> 00:16:13.580
Most phones have a CPU, and
many have a GPU and a DSP.

00:16:13.580 --> 00:16:17.630
CPU is typically the best
option for simple ML models.

00:16:17.630 --> 00:16:20.540
GPUs are usually really great
for high-energy processing

00:16:20.540 --> 00:16:21.860
at fast speeds.

00:16:21.860 --> 00:16:25.730
And DSPs tend to be best for
low-powered, complex models

00:16:25.730 --> 00:16:28.460
that require very
fast execution.

00:16:28.460 --> 00:16:32.420
It depends on the use case
and experimentation with you,

00:16:32.420 --> 00:16:34.280
but the great thing
for TensorFlow Lite

00:16:34.280 --> 00:16:39.690
is that it allows you to
execute your ML on all of them.

00:16:39.690 --> 00:16:41.460
And our team has
worked incredibly

00:16:41.460 --> 00:16:44.340
hard to have optimal
performance across all

00:16:44.340 --> 00:16:46.290
these different architectures.

00:16:46.290 --> 00:16:48.360
For example,
MobileNet V1 achieves

00:16:48.360 --> 00:16:53.460
83 millisecond inference speed
on a Pixel 3 with a single CPU

00:16:53.460 --> 00:16:55.320
thread.

00:16:55.320 --> 00:16:57.420
Drop that to just
15 milliseconds

00:16:57.420 --> 00:16:59.475
when you delegate that
across to the GPU.

00:17:02.330 --> 00:17:04.700
So we have a lot more
CPU optimizations

00:17:04.700 --> 00:17:08.060
coming in our pipeline to
get even better performance

00:17:08.060 --> 00:17:13.010
across 2019, in addition to
more op support on ARM and Intel

00:17:13.010 --> 00:17:14.157
architectures.

00:17:16.900 --> 00:17:20.440
So what about the delegation API
is such an important mechanism

00:17:20.440 --> 00:17:22.020
inside TensorFlow?

00:17:22.020 --> 00:17:24.270
But you're probably thinking,
what is this magical API

00:17:24.270 --> 00:17:27.220
and how does it work?

00:17:27.220 --> 00:17:30.780
So the delegation API
delegates part of your graph

00:17:30.780 --> 00:17:33.800
to another executor at runtime.

00:17:33.800 --> 00:17:37.620
It accelerates any or all parts
of the graph If it can get

00:17:37.620 --> 00:17:42.990
better performance and falls
back to CPU when it can't.

00:17:42.990 --> 00:17:45.570
So here's a great
way to visualize it.

00:17:45.570 --> 00:17:48.420
A graph is made up of
a series of operations.

00:17:48.420 --> 00:17:51.570
And for operations supported
on a particular architecture,

00:17:51.570 --> 00:17:55.920
TensorFlow Lite will accelerate
those if you ask it to.

00:17:55.920 --> 00:17:59.490
If certain operations aren't
supported on that delegate,

00:17:59.490 --> 00:18:04.950
it will fall back to
the CPU automatically.

00:18:04.950 --> 00:18:07.340
So now that we've spoken
about the delegation API,

00:18:07.340 --> 00:18:10.310
the Android Neural
Network API uses

00:18:10.310 --> 00:18:14.000
it to standardize hardware
acceleration across the Android

00:18:14.000 --> 00:18:15.650
ecosystem.

00:18:15.650 --> 00:18:18.860
In P, it supports around
30 graph operations.

00:18:18.860 --> 00:18:21.410
And in Q, it will
have more than 100

00:18:21.410 --> 00:18:24.410
and support use cases
like image, audio, speech,

00:18:24.410 --> 00:18:26.070
and others.

00:18:26.070 --> 00:18:29.480
For example, you could
achieve 9x latency improvement

00:18:29.480 --> 00:18:35.020
on the ML Kit Face Detection
model using the NN API.

00:18:35.020 --> 00:18:36.720
And it's really easy to use.

00:18:36.720 --> 00:18:40.830
You just flip the setUseNNA
flag to true, and you're done.

00:18:40.830 --> 00:18:42.470
You will get acceleration.

00:18:42.470 --> 00:18:45.615
Your graph will accelerate
where possible using the NN API.

00:18:49.222 --> 00:18:50.680
So the other thing
we've done is we

00:18:50.680 --> 00:18:53.650
released the GPU
acceleration ops using

00:18:53.650 --> 00:18:59.260
Open GL ES 3.1 for Android
and metal shaders for iOS.

00:18:59.260 --> 00:19:02.530
So this will give you a 2
to 7x speed-up in comparison

00:19:02.530 --> 00:19:05.140
to floating point on the CPU.

00:19:05.140 --> 00:19:08.170
But it does add a tiny bit
more to your binary size.

00:19:10.835 --> 00:19:13.710
Oh, did I go--

00:19:13.710 --> 00:19:14.290
no.

00:19:14.290 --> 00:19:15.700
And so it's really--

00:19:15.700 --> 00:19:17.324
sorry, I went backwards.

00:19:20.832 --> 00:19:22.790
Anyway, we're working on
making the GPU faster,

00:19:22.790 --> 00:19:25.160
is basically what
I'm saying here.

00:19:25.160 --> 00:19:27.890
We love feedback, so please
reach out to us and let us

00:19:27.890 --> 00:19:30.830
know what's important.

00:19:30.830 --> 00:19:35.060
So Edge TPU is another example
of the delegation API working

00:19:35.060 --> 00:19:37.220
with a custom ML
accelerator that

00:19:37.220 --> 00:19:39.830
has high-performance,
low-powered acceleration

00:19:39.830 --> 00:19:41.450
at the edge.

00:19:41.450 --> 00:19:43.910
It accelerates
TensorFlow Lite models,

00:19:43.910 --> 00:19:48.970
and you can find out more in
the Edge TPU talk tomorrow.

00:19:48.970 --> 00:19:51.790
So now we've got a really
exciting announcement

00:19:51.790 --> 00:19:53.590
around DSP performance.

00:19:53.590 --> 00:19:58.360
We've partnered with Qualcomm
to enable DSP delegation

00:19:58.360 --> 00:20:02.350
through TensorFlow Lite directly
for their 600 to 800 series

00:20:02.350 --> 00:20:07.760
devices, which is hundreds
of millions of devices.

00:20:07.760 --> 00:20:11.330
While we recommend that you
use the NN API in Android Q

00:20:11.330 --> 00:20:14.300
and beyond, this
announcement just gives us

00:20:14.300 --> 00:20:19.250
another option for accelerating
your models on the DSP.

00:20:19.250 --> 00:20:22.190
You'll be able to include
a Qualcomm signed binary

00:20:22.190 --> 00:20:26.020
and rely on the delegation
API for acceleration.

00:20:26.020 --> 00:20:29.990
And we hope to have this
released later this summer.

00:20:29.990 --> 00:20:32.490
So you're probably wondering,
well, what type of performance

00:20:32.490 --> 00:20:34.850
can I get on the DSP?

00:20:34.850 --> 00:20:38.630
You can get up to an 8.3x
speed-up delegating over

00:20:38.630 --> 00:20:42.950
to the DSP on Qualcomm
devices, which is an incredible

00:20:42.950 --> 00:20:46.260
performance boost that we are
excited to bring to TensorFlow

00:20:46.260 --> 00:20:47.540
Lite.

00:20:47.540 --> 00:20:49.700
This is just another
example of how

00:20:49.700 --> 00:20:51.260
we are enabling
more accelerators

00:20:51.260 --> 00:20:55.870
to work with your ML models.

00:20:55.870 --> 00:20:57.550
Lastly, as I talked
about earlier,

00:20:57.550 --> 00:21:00.220
you want to ensure that you
are benchmarking and validating

00:21:00.220 --> 00:21:01.930
your models.

00:21:01.930 --> 00:21:04.060
And so we offer some
very simple tooling

00:21:04.060 --> 00:21:07.970
to enable this for threading
and per-op profiling.

00:21:07.970 --> 00:21:10.760
And here is a way to
execute the per-up profiling

00:21:10.760 --> 00:21:13.785
via the command line
with Bazel and ADB.

00:21:17.630 --> 00:21:19.670
So this is basically
what you get as an output

00:21:19.670 --> 00:21:21.800
when you are doing
per-op profiling.

00:21:21.800 --> 00:21:24.380
It really does enable you
to narrow down your graph

00:21:24.380 --> 00:21:30.630
execution, and then go back and
tune performance bottlenecks.

00:21:30.630 --> 00:21:34.790
Now let's talk about optimizing
your graph using the TensorFlow

00:21:34.790 --> 00:21:38.280
Model Optimization Toolkit.

00:21:38.280 --> 00:21:40.950
We offer a simple toolkit
to optimize your graphs

00:21:40.950 --> 00:21:44.850
and enable them to run
faster at a smaller size.

00:21:44.850 --> 00:21:46.700
For those that
already understand,

00:21:46.700 --> 00:21:48.630
we are adding more
techniques for during

00:21:48.630 --> 00:21:50.970
and post-training quantization.

00:21:50.970 --> 00:21:53.130
And if none of these
concepts are familiar to you,

00:21:53.130 --> 00:21:56.320
don't worry, I'll explain.

00:21:56.320 --> 00:21:58.920
So what is quantization,
you might be wondering.

00:21:58.920 --> 00:22:01.000
Quantization is really
just the reduction

00:22:01.000 --> 00:22:05.230
in precision on weights and
activations in your graph,

00:22:05.230 --> 00:22:07.600
essentially reducing
from floating point

00:22:07.600 --> 00:22:09.790
to integer-based numbers.

00:22:09.790 --> 00:22:11.860
The reason this works
so well is that we

00:22:11.860 --> 00:22:16.120
try to optimize the heaviest
computations in lower precision

00:22:16.120 --> 00:22:19.480
but preserve the most sensitive
ones with higher precision,

00:22:19.480 --> 00:22:21.838
so there is no accuracy loss.

00:22:21.838 --> 00:22:22.630
There are variants.

00:22:22.630 --> 00:22:26.740
Post-training occurs once
you have an outputted graph,

00:22:26.740 --> 00:22:29.980
and during training, which
preserves the forward pass

00:22:29.980 --> 00:22:34.970
and matches precision for
both training and inference.

00:22:34.970 --> 00:22:37.420
So now you know what
quantization is.

00:22:37.420 --> 00:22:39.190
The goal of the
toolkit is really

00:22:39.190 --> 00:22:42.910
to make your graphs run
faster and be smaller

00:22:42.910 --> 00:22:45.850
by abstracting all the
complexity involved with all

00:22:45.850 --> 00:22:48.880
these differing techniques.

00:22:48.880 --> 00:22:50.310
So we strongly
recommend that you

00:22:50.310 --> 00:22:52.740
start with post-training
quantization,

00:22:52.740 --> 00:22:55.900
as it's a simple flag
flip to utilize it,

00:22:55.900 --> 00:22:59.040
and you can see what type
of performance improvements

00:22:59.040 --> 00:22:59.790
you can achieve.

00:23:03.780 --> 00:23:07.970
We've seen up to a 4x
reduction in model size,

00:23:07.970 --> 00:23:12.320
10% to 50% faster execution
for convolutional models,

00:23:12.320 --> 00:23:16.250
and 3x improvements on fully
connected and RNN-based models

00:23:16.250 --> 00:23:19.100
on the CPU.

00:23:19.100 --> 00:23:21.040
And this is how
simple it is to do it.

00:23:21.040 --> 00:23:23.000
It really is just
a simple flag flip.

00:23:26.850 --> 00:23:29.470
So in the coming months,
we'll be improving our support

00:23:29.470 --> 00:23:32.050
for quantization
with Keras, enabling

00:23:32.050 --> 00:23:35.950
post-training quantization
with fixed-point math,

00:23:35.950 --> 00:23:38.590
and adding more advanced
techniques, like connection

00:23:38.590 --> 00:23:42.010
pruning and sparsity support.

00:23:42.010 --> 00:23:44.410
But one thing we really
want to emphasize

00:23:44.410 --> 00:23:47.140
is that post-training
quantization is really

00:23:47.140 --> 00:23:50.890
almost as good as
during-training quantization.

00:23:50.890 --> 00:23:53.200
If you're an
advanced practitioner

00:23:53.200 --> 00:23:57.170
and have access to the
model and the training data,

00:23:57.170 --> 00:24:00.640
then during-training
quantization might be for you.

00:24:00.640 --> 00:24:03.100
But for most,
post-training quantization

00:24:03.100 --> 00:24:09.500
can almost have the exact same
effect on size and accuracy.

00:24:09.500 --> 00:24:11.920
So here you can actually
see the difference--

00:24:11.920 --> 00:24:13.900
and there's really only
marginal differences--

00:24:13.900 --> 00:24:16.900
between during-training
quantization

00:24:16.900 --> 00:24:19.230
versus post-training
quantization.

00:24:19.230 --> 00:24:23.770
So, again, please try and start
with post-training quantization

00:24:23.770 --> 00:24:26.640
first.

00:24:26.640 --> 00:24:28.320
SARAH SIRAJUDDIN: Thanks, Tim.

00:24:28.320 --> 00:24:32.430
So is TensorFlow Lite
only for mobile phones?

00:24:32.430 --> 00:24:33.550
It is not.

00:24:33.550 --> 00:24:36.930
TensorFlow Lite is already being
used in many, many products

00:24:36.930 --> 00:24:38.310
which are not phones.

00:24:38.310 --> 00:24:42.300
It is being used in smart
speakers, smart mirrors, vacuum

00:24:42.300 --> 00:24:45.930
cleaners, and even small
space satellites from NASA.

00:24:48.480 --> 00:24:51.690
It's because of this demand
that we see from our developers

00:24:51.690 --> 00:24:54.030
and also the fact that
there are a huge number

00:24:54.030 --> 00:24:56.410
of microcontrollers
that are out there,

00:24:56.410 --> 00:24:58.050
we have decided to
invest in making

00:24:58.050 --> 00:25:01.230
TensorFlow Lite even
lighter and suitable for use

00:25:01.230 --> 00:25:03.834
on these platforms.

00:25:03.834 --> 00:25:06.930
OK, so let's first talk about
what is a microcontroller.

00:25:06.930 --> 00:25:10.410
They're essentially small
computers on a single circuit.

00:25:10.410 --> 00:25:12.930
They typically don't
run an operating system.

00:25:12.930 --> 00:25:15.930
They have very limited RAM
and flash, usually just

00:25:15.930 --> 00:25:18.300
tens of kilobytes of it.

00:25:18.300 --> 00:25:24.910
And they only have memory, CPUs,
and perhaps some peripherals.

00:25:24.910 --> 00:25:27.730
And the way microcontrollers
are used many times

00:25:27.730 --> 00:25:31.040
is that they are used
in a cascading fashion.

00:25:31.040 --> 00:25:32.770
So they perform
lightweight processing,

00:25:32.770 --> 00:25:34.780
and, based on the
result of that,

00:25:34.780 --> 00:25:38.800
it triggers heavier processing
on some more powerful hardware.

00:25:38.800 --> 00:25:41.410
So as an example,
a microcontroller

00:25:41.410 --> 00:25:44.260
can be checking to see
if there is any sound.

00:25:44.260 --> 00:25:47.200
And this in turn can
trigger processing

00:25:47.200 --> 00:25:49.570
on a second
microcontroller, which

00:25:49.570 --> 00:25:53.590
would check if the detected
sound was human speech.

00:25:53.590 --> 00:25:57.280
And this in turn would trigger
processing on a heavier

00:25:57.280 --> 00:26:01.220
application processor.

00:26:01.220 --> 00:26:03.100
So at a high level,
the architecture

00:26:03.100 --> 00:26:05.170
for TensorFlow Lite
for microcontrollers

00:26:05.170 --> 00:26:07.930
is the same as what we
have for TensorFlow Lite.

00:26:07.930 --> 00:26:10.990
We use the same model format,
and we use the same conversion

00:26:10.990 --> 00:26:12.200
process.

00:26:12.200 --> 00:26:15.520
The only difference here
is that the interpreter

00:26:15.520 --> 00:26:19.180
is a stripped-down, lightweight
version of TensorFlow Lite's

00:26:19.180 --> 00:26:22.060
interpreter.

00:26:22.060 --> 00:26:25.170
We have been working on getting
example models ready for you

00:26:25.170 --> 00:26:27.340
to use on microcontrollers.

00:26:27.340 --> 00:26:30.040
We have one already on
the website for speech,

00:26:30.040 --> 00:26:32.370
and there is another one
for image classification

00:26:32.370 --> 00:26:34.340
that is coming out soon.

00:26:34.340 --> 00:26:36.480
And these models have
to be pretty small, too,

00:26:36.480 --> 00:26:38.730
as you can imagine.

00:26:38.730 --> 00:26:41.160
If you go to our website,
you will find instructions

00:26:41.160 --> 00:26:43.510
on how you can use these models.

00:26:43.510 --> 00:26:45.110
And if you are here at--

00:26:45.110 --> 00:26:45.610
sorry.

00:26:45.610 --> 00:26:49.290
And it also has suggestions for
how you can procure hardware

00:26:49.290 --> 00:26:52.380
to get started.

00:26:52.380 --> 00:26:55.320
This brings me to another
exciting announcement.

00:26:55.320 --> 00:26:58.680
We are happy to announce a
closer collaboration with ARM

00:26:58.680 --> 00:27:02.280
on the development of TensorFlow
Lite for microcontrollers.

00:27:02.280 --> 00:27:05.790
ARM is a well established and
respected leader in this space,

00:27:05.790 --> 00:27:09.770
and we are very excited
to work closely with them.

00:27:09.770 --> 00:27:11.410
We will be working
closely with ARM

00:27:11.410 --> 00:27:14.950
on the development of
models, framework design,

00:27:14.950 --> 00:27:17.830
as well as performance
optimizations.

00:27:17.830 --> 00:27:20.560
The Mbed community is
the largest community

00:27:20.560 --> 00:27:22.300
of developers in this space.

00:27:22.300 --> 00:27:24.730
And we will be integrating
deeply with the tooling

00:27:24.730 --> 00:27:28.150
there to make TensorFlow
Lite easy and performant

00:27:28.150 --> 00:27:28.960
for them to use.

00:27:32.020 --> 00:27:34.600
This is a relatively
new effort, and we

00:27:34.600 --> 00:27:37.240
would love to work closely
with the TensorFlow community

00:27:37.240 --> 00:27:39.070
to make this successful.

00:27:39.070 --> 00:27:42.460
So please send us your
feedback, ideas, and also

00:27:42.460 --> 00:27:44.450
code contributions.

00:27:44.450 --> 00:27:47.500
And if you are here at I/O, you
can go to the Codelabs area,

00:27:47.500 --> 00:27:49.690
where you can try running
in TensorFlow Lite

00:27:49.690 --> 00:27:51.183
on a microcontroller yourself.

00:27:54.013 --> 00:27:55.680
TIM DAVIS: So where
can you go and learn

00:27:55.680 --> 00:27:59.900
more about TensorFlow Lite and
everything we've shared today?

00:27:59.900 --> 00:28:01.930
So the first thing is we
really listened to you

00:28:01.930 --> 00:28:04.940
and heard that our documentation
just wasn't good enough.

00:28:04.940 --> 00:28:06.760
So we've worked
really hard on making

00:28:06.760 --> 00:28:09.880
it a lot better so you
have the resources you

00:28:09.880 --> 00:28:12.880
need to develop what you want
to do with TensorFlow Lite.

00:28:15.950 --> 00:28:18.620
We have new tutorials,
better demos,

00:28:18.620 --> 00:28:24.920
and a new model repository
available and live right now.

00:28:24.920 --> 00:28:29.240
Just go to tensorflow.org/lite
to get started,

00:28:29.240 --> 00:28:32.120
and you'll see our revamped
website that makes it easy

00:28:32.120 --> 00:28:36.828
to navigate and find
what you're looking for.

00:28:36.828 --> 00:28:38.370
Now, as an open
source product, we're

00:28:38.370 --> 00:28:40.710
working hard to engage
with the community

00:28:40.710 --> 00:28:43.860
and be even more transparent
about where we're headed.

00:28:43.860 --> 00:28:46.680
That's why we've
published our 2019 roadmap

00:28:46.680 --> 00:28:50.160
on tensorflow.org/lite
so you have visibility

00:28:50.160 --> 00:28:51.780
into our priorities.

00:28:51.780 --> 00:28:56.990
So please, feel free to check
it out and give us feedback.

00:28:56.990 --> 00:28:59.470
We also have new code
samples and models

00:28:59.470 --> 00:29:01.630
on the site for common
use cases, like image

00:29:01.630 --> 00:29:04.330
classification, object
detection, and the others

00:29:04.330 --> 00:29:07.260
that you see here.

00:29:07.260 --> 00:29:10.010
So now we're excited to
show one last demo, which

00:29:10.010 --> 00:29:12.710
is very unique,
fun, and shows off

00:29:12.710 --> 00:29:14.450
the performance
of TensorFlow Lite

00:29:14.450 --> 00:29:16.680
in a non-mobile environment.

00:29:16.680 --> 00:29:19.230
So Sarah is going to be the
star of the show for this demo,

00:29:19.230 --> 00:29:22.040
which will look at the virtual
try-on of glasses and hair

00:29:22.040 --> 00:29:22.730
color.

00:29:22.730 --> 00:29:24.230
And then we'll take
a look at what's

00:29:24.230 --> 00:29:26.070
going on behind the scenes.

00:29:26.070 --> 00:29:29.740
So Sarah, come on up,
and let's do this.

00:29:29.740 --> 00:29:33.240
SARAH SIRAJUDDIN: So as you can
see, this is a smart mirror.

00:29:33.240 --> 00:29:36.630
This was built by one
of our developers.

00:29:36.630 --> 00:29:38.520
As you can see, this
is a touchless mirror,

00:29:38.520 --> 00:29:41.490
which is operated
only by hand gestures.

00:29:41.490 --> 00:29:43.860
It's running the
CareOS operating system

00:29:43.860 --> 00:29:46.350
running on Qualcomm hardware.

00:29:46.350 --> 00:29:48.660
All the machine
learning on this mirror

00:29:48.660 --> 00:29:50.835
is powered by
TensorFlow Lite, and we

00:29:50.835 --> 00:29:55.180
are accelerating it on the GPU
to get optimal performance.

00:29:55.180 --> 00:29:57.463
OK, let's give this a whirl.

00:29:57.463 --> 00:29:59.630
TIM DAVIS: All right, so
the first try-on experience

00:29:59.630 --> 00:30:02.600
we're going to show you is
realistic virtual try-on

00:30:02.600 --> 00:30:04.490
of eyeglasses.

00:30:04.490 --> 00:30:06.200
So as you can see,
Sarah doesn't need

00:30:06.200 --> 00:30:08.210
to touch the mirror
since it works

00:30:08.210 --> 00:30:10.570
through a touchless interaction.

00:30:10.570 --> 00:30:13.850
The embedded Google AI
technology runs in real time,

00:30:13.850 --> 00:30:16.760
and is driven end to end
with GPU acceleration

00:30:16.760 --> 00:30:19.130
for model inference
and rendering.

00:30:19.130 --> 00:30:20.930
Those glasses are
looking awesome.

00:30:23.810 --> 00:30:26.416
So now let's try
maybe hair recoloring.

00:30:26.416 --> 00:30:29.800
SARAH SIRAJUDDIN: Oh,
I'll go for the blue.

00:30:29.800 --> 00:30:32.240
TIM DAVIS: Oh,
that looks awesome.

00:30:32.240 --> 00:30:34.490
This is enabled by a
state-of-the-art segmentation

00:30:34.490 --> 00:30:38.450
model that predicts for every
pixel the confidence of being

00:30:38.450 --> 00:30:41.030
part of the user's hair or not.

00:30:41.030 --> 00:30:42.740
Sarah, you look great.

00:30:42.740 --> 00:30:44.480
So now, why don't we
take a look inside?

00:30:44.480 --> 00:30:45.855
What's going on
behind the scenes

00:30:45.855 --> 00:30:47.880
to achieve this experience?

00:30:47.880 --> 00:30:52.070
So for every frame, a TensorFlow
high-fidelity geometry model

00:30:52.070 --> 00:30:55.280
is being run to predict
over 400 points on the face.

00:30:55.280 --> 00:30:57.307
And it even works
for multiple people.

00:30:57.307 --> 00:30:57.890
I'll show you.

00:31:02.470 --> 00:31:06.140
So all up, this is an awesome
example of on-device ML

00:31:06.140 --> 00:31:09.380
using TensorFlow Lite
on a non-mobile device.

00:31:09.380 --> 00:31:12.140
You can check out the
segmentation models that we

00:31:12.140 --> 00:31:14.593
have available on
tensorflow.org/lite.

00:31:14.593 --> 00:31:16.760
And if you'd like to find
out more about the mirror,

00:31:16.760 --> 00:31:18.260
please come up to
us after the show,

00:31:18.260 --> 00:31:22.427
and we can direct you where
to get more information.

00:31:22.427 --> 00:31:24.260
SARAH SIRAJUDDIN: That's
all we have, folks.

00:31:24.260 --> 00:31:28.050
Please try out TF Lite if
you haven't done so already.

00:31:28.050 --> 00:31:29.660
We would not have
gotten to this point

00:31:29.660 --> 00:31:32.090
if it wasn't for our
developer community, which

00:31:32.090 --> 00:31:34.520
has helped us a
lot with feedback,

00:31:34.520 --> 00:31:36.140
as well as code contributions.

00:31:36.140 --> 00:31:37.130
So thank you a lot.

00:31:37.130 --> 00:31:39.485
We're really grateful.

00:31:39.485 --> 00:31:42.752
[APPLAUSE]

00:31:45.600 --> 00:31:48.800
A few of us will be at the AI
Sandbox today and tomorrow.

00:31:48.800 --> 00:31:52.010
Please come by to have a chat
and try out one of our demos.

00:31:52.010 --> 00:31:54.470
TIM DAVIS: We'll also be at
Office Hours today at 10:30 AM,

00:31:54.470 --> 00:31:55.770
right after this talk.

00:31:55.770 --> 00:31:57.023
Thank you very much.

00:31:57.023 --> 00:31:58.190
SARAH SIRAJUDDIN: Thank you.

00:31:58.190 --> 00:32:02.140
[MUSIC PLAYING]

