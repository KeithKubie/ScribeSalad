WEBVTT
Kind: captions
Language: en

00:00:03.110 --> 00:00:06.170
today we're going to be discussing deep

00:00:06.170 --> 00:00:06.180
today we're going to be discussing deep
 

00:00:06.180 --> 00:00:09.080
today we're going to be discussing deep
reinforcement learning which is actually

00:00:09.080 --> 00:00:09.090
reinforcement learning which is actually
 

00:00:09.090 --> 00:00:12.079
reinforcement learning which is actually
one of a combination of disciplines

00:00:12.079 --> 00:00:12.089
one of a combination of disciplines
 

00:00:12.089 --> 00:00:14.360
one of a combination of disciplines
between deep learning and the

00:00:14.360 --> 00:00:14.370
between deep learning and the
 

00:00:14.370 --> 00:00:16.730
between deep learning and the
long-standing community of reinforcement

00:00:16.730 --> 00:00:16.740
long-standing community of reinforcement
 

00:00:16.740 --> 00:00:18.620
long-standing community of reinforcement
learning which has been around for many

00:00:18.620 --> 00:00:18.630
learning which has been around for many
 

00:00:18.630 --> 00:00:22.370
learning which has been around for many
decades now in machine learning and at a

00:00:22.370 --> 00:00:22.380
decades now in machine learning and at a
 

00:00:22.380 --> 00:00:23.720
decades now in machine learning and at a
high level reinforcement learning

00:00:23.720 --> 00:00:23.730
high level reinforcement learning
 

00:00:23.730 --> 00:00:26.089
high level reinforcement learning
provides us with a set of mathematical

00:00:26.089 --> 00:00:26.099
provides us with a set of mathematical
 

00:00:26.099 --> 00:00:29.300
provides us with a set of mathematical
tools and methods for teaching agents

00:00:29.300 --> 00:00:29.310
tools and methods for teaching agents
 

00:00:29.310 --> 00:00:31.490
tools and methods for teaching agents
how to actually go from perceiving the

00:00:31.490 --> 00:00:31.500
how to actually go from perceiving the
 

00:00:31.500 --> 00:00:33.440
how to actually go from perceiving the
world which is the way we usually talk

00:00:33.440 --> 00:00:33.450
world which is the way we usually talk
 

00:00:33.450 --> 00:00:36.200
world which is the way we usually talk
about deep learning or machine learning

00:00:36.200 --> 00:00:36.210
about deep learning or machine learning
 

00:00:36.210 --> 00:00:37.549
about deep learning or machine learning
problems in the context of computer

00:00:37.549 --> 00:00:37.559
problems in the context of computer
 

00:00:37.559 --> 00:00:40.940
problems in the context of computer
vision perception to actually go beyond

00:00:40.940 --> 00:00:40.950
vision perception to actually go beyond
 

00:00:40.950 --> 00:00:43.580
vision perception to actually go beyond
this perception to actually act acting

00:00:43.580 --> 00:00:43.590
this perception to actually act acting
 

00:00:43.590 --> 00:00:45.319
this perception to actually act acting
in the world and figuring out how to

00:00:45.319 --> 00:00:45.329
in the world and figuring out how to
 

00:00:45.329 --> 00:00:49.160
in the world and figuring out how to
optimally act in that world and I'd like

00:00:49.160 --> 00:00:49.170
optimally act in that world and I'd like
 

00:00:49.170 --> 00:00:52.610
optimally act in that world and I'd like
to start by showing a rather short but

00:00:52.610 --> 00:00:52.620
to start by showing a rather short but
 

00:00:52.620 --> 00:00:54.740
to start by showing a rather short but
dramatic video of a trailer of the movie

00:00:54.740 --> 00:00:54.750
dramatic video of a trailer of the movie
 

00:00:54.750 --> 00:00:57.619
dramatic video of a trailer of the movie
based on the alpha ghost story which you

00:00:57.619 --> 00:00:57.629
based on the alpha ghost story which you
 

00:00:57.629 --> 00:00:59.660
based on the alpha ghost story which you
might have heard of just to give us an

00:00:59.660 --> 00:00:59.670
might have heard of just to give us an
 

00:00:59.670 --> 00:01:01.279
might have heard of just to give us an
introduction to the power of these

00:01:01.279 --> 00:01:01.289
introduction to the power of these
 

00:01:01.289 --> 00:01:06.770
introduction to the power of these
techniques go is the world's oldest

00:01:06.770 --> 00:01:06.780
techniques go is the world's oldest
 

00:01:06.780 --> 00:01:09.950
techniques go is the world's oldest
continuously played board game it is one

00:01:09.950 --> 00:01:09.960
continuously played board game it is one
 

00:01:09.960 --> 00:01:12.940
continuously played board game it is one
of the simplest and also most abstract

00:01:12.940 --> 00:01:12.950
of the simplest and also most abstract
 

00:01:12.950 --> 00:01:15.530
of the simplest and also most abstract
beats me a professional player it go is

00:01:15.530 --> 00:01:15.540
beats me a professional player it go is
 

00:01:15.540 --> 00:01:17.750
beats me a professional player it go is
a long-standing challenge of artificial

00:01:17.750 --> 00:01:17.760
a long-standing challenge of artificial
 

00:01:17.760 --> 00:01:19.900
a long-standing challenge of artificial
intelligence

00:01:19.900 --> 00:01:19.910
 

00:01:19.910 --> 00:01:21.160
[Music]

00:01:21.160 --> 00:01:21.170
[Music]
 

00:01:21.170 --> 00:01:23.450
[Music]
everything we've ever tried in AI just

00:01:23.450 --> 00:01:23.460
everything we've ever tried in AI just
 

00:01:23.460 --> 00:01:25.279
everything we've ever tried in AI just
falls over when you try the game of Go a

00:01:25.279 --> 00:01:25.289
falls over when you try the game of Go a
 

00:01:25.289 --> 00:01:27.410
falls over when you try the game of Go a
number of possible configurations of the

00:01:27.410 --> 00:01:27.420
number of possible configurations of the
 

00:01:27.420 --> 00:01:29.120
number of possible configurations of the
board is more than the number of atoms

00:01:29.120 --> 00:01:29.130
board is more than the number of atoms
 

00:01:29.130 --> 00:01:30.109
board is more than the number of atoms
in the universe

00:01:30.109 --> 00:01:30.119
in the universe
 

00:01:30.119 --> 00:01:32.389
in the universe
I'll forego found a way to learn how to

00:01:32.389 --> 00:01:32.399
I'll forego found a way to learn how to
 

00:01:32.399 --> 00:01:35.660
I'll forego found a way to learn how to
play go so far alphago has beaten every

00:01:35.660 --> 00:01:35.670
play go so far alphago has beaten every
 

00:01:35.670 --> 00:01:37.940
play go so far alphago has beaten every
challenge me giving it but we won't know

00:01:37.940 --> 00:01:37.950
challenge me giving it but we won't know
 

00:01:37.950 --> 00:01:40.039
challenge me giving it but we won't know
its true strength until we play somebody

00:01:40.039 --> 00:01:40.049
its true strength until we play somebody
 

00:01:40.049 --> 00:01:42.319
its true strength until we play somebody
who is at the top of the world like Lisa

00:01:42.319 --> 00:01:42.329
who is at the top of the world like Lisa
 

00:01:42.329 --> 00:01:45.589
who is at the top of the world like Lisa
doll I'm not like no other is about to

00:01:45.589 --> 00:01:45.599
doll I'm not like no other is about to
 

00:01:45.599 --> 00:01:47.480
doll I'm not like no other is about to
get underway in South Korea they said

00:01:47.480 --> 00:01:47.490
get underway in South Korea they said
 

00:01:47.490 --> 00:01:49.249
get underway in South Korea they said
all is to go what Roger Federer is to

00:01:49.249 --> 00:01:49.259
all is to go what Roger Federer is to
 

00:01:49.259 --> 00:01:50.839
all is to go what Roger Federer is to
tennis just the very thought of a

00:01:50.839 --> 00:01:50.849
tennis just the very thought of a
 

00:01:50.849 --> 00:01:52.309
tennis just the very thought of a
machine playing a human because

00:01:52.309 --> 00:01:52.319
machine playing a human because
 

00:01:52.319 --> 00:01:54.050
machine playing a human because
inherently intriguing the place is a

00:01:54.050 --> 00:01:54.060
inherently intriguing the place is a
 

00:01:54.060 --> 00:01:56.719
inherently intriguing the place is a
madhouse welcome to the deep mind

00:01:56.719 --> 00:01:56.729
madhouse welcome to the deep mind
 

00:01:56.729 --> 00:01:58.999
madhouse welcome to the deep mind
challenge for world is watching

00:01:58.999 --> 00:01:59.009
challenge for world is watching
 

00:01:59.009 --> 00:02:02.410
challenge for world is watching
can Lisa doll find alphago's weakness

00:02:02.410 --> 00:02:02.420
can Lisa doll find alphago's weakness
 

00:02:02.420 --> 00:02:07.399
can Lisa doll find alphago's weakness
whoa is there in fact a weakness the

00:02:07.399 --> 00:02:07.409
whoa is there in fact a weakness the
 

00:02:07.409 --> 00:02:10.339
whoa is there in fact a weakness the
game kind of turned on its axis right

00:02:10.339 --> 00:02:10.349
game kind of turned on its axis right
 

00:02:10.349 --> 00:02:13.280
game kind of turned on its axis right
now he's not confident thanks it's

00:02:13.280 --> 00:02:13.290
now he's not confident thanks it's
 

00:02:13.290 --> 00:02:14.809
now he's not confident thanks it's
developing into a very very dangerous

00:02:14.809 --> 00:02:14.819
developing into a very very dangerous
 

00:02:14.819 --> 00:02:17.030
developing into a very very dangerous
fight hold the phone Rena's left the

00:02:17.030 --> 00:02:17.040
fight hold the phone Rena's left the
 

00:02:17.040 --> 00:02:19.160
fight hold the phone Rena's left the
room in the end he used about the pride

00:02:19.160 --> 00:02:19.170
room in the end he used about the pride
 

00:02:19.170 --> 00:02:21.650
room in the end he used about the pride
I think something went wrong gasps man

00:02:21.650 --> 00:02:21.660
I think something went wrong gasps man
 

00:02:21.660 --> 00:02:23.750
I think something went wrong gasps man
thank you he's got a plan here these

00:02:23.750 --> 00:02:23.760
thank you he's got a plan here these
 

00:02:23.760 --> 00:02:26.509
thank you he's got a plan here these
ideas that are driving alphago are gonna

00:02:26.509 --> 00:02:26.519
ideas that are driving alphago are gonna
 

00:02:26.519 --> 00:02:32.990
ideas that are driving alphago are gonna
drive our future this is it folks so for

00:02:32.990 --> 00:02:33.000
drive our future this is it folks so for
 

00:02:33.000 --> 00:02:34.250
drive our future this is it folks so for
those of you interested that's actually

00:02:34.250 --> 00:02:34.260
those of you interested that's actually
 

00:02:34.260 --> 00:02:36.500
those of you interested that's actually
a movie that came out about a year or

00:02:36.500 --> 00:02:36.510
a movie that came out about a year or
 

00:02:36.510 --> 00:02:38.539
a movie that came out about a year or
two ago and it's available on Netflix

00:02:38.539 --> 00:02:38.549
two ago and it's available on Netflix
 

00:02:38.549 --> 00:02:41.839
two ago and it's available on Netflix
now it's a rather dramatic depiction of

00:02:41.839 --> 00:02:41.849
now it's a rather dramatic depiction of
 

00:02:41.849 --> 00:02:44.900
now it's a rather dramatic depiction of
the true story of alphago facing Lisa

00:02:44.900 --> 00:02:44.910
the true story of alphago facing Lisa
 

00:02:44.910 --> 00:02:46.940
the true story of alphago facing Lisa
Dole but it's an incredibly powerful

00:02:46.940 --> 00:02:46.950
Dole but it's an incredibly powerful
 

00:02:46.950 --> 00:02:48.559
Dole but it's an incredibly powerful
story at the same time because it really

00:02:48.559 --> 00:02:48.569
story at the same time because it really
 

00:02:48.569 --> 00:02:51.770
story at the same time because it really
shows the impact that this algorithm had

00:02:51.770 --> 00:02:51.780
shows the impact that this algorithm had
 

00:02:51.780 --> 00:02:53.390
shows the impact that this algorithm had
on the world and the press that it

00:02:53.390 --> 00:02:53.400
on the world and the press that it
 

00:02:53.400 --> 00:02:57.080
on the world and the press that it
received as a result and hopefully by

00:02:57.080 --> 00:02:57.090
received as a result and hopefully by
 

00:02:57.090 --> 00:02:58.250
received as a result and hopefully by
the end of this lecture you'll get a

00:02:58.250 --> 00:02:58.260
the end of this lecture you'll get a
 

00:02:58.260 --> 00:03:00.979
the end of this lecture you'll get a
sense of the way that this this

00:03:00.979 --> 00:03:00.989
sense of the way that this this
 

00:03:00.989 --> 00:03:02.809
sense of the way that this this
remarkable algorithm that Valve ago was

00:03:02.809 --> 00:03:02.819
remarkable algorithm that Valve ago was
 

00:03:02.819 --> 00:03:05.870
remarkable algorithm that Valve ago was
trained and kind of going beyond that

00:03:05.870 --> 00:03:05.880
trained and kind of going beyond that
 

00:03:05.880 --> 00:03:07.250
trained and kind of going beyond that
although will then give a lecture on

00:03:07.250 --> 00:03:07.260
although will then give a lecture on
 

00:03:07.260 --> 00:03:09.110
although will then give a lecture on
some of the new frontiers of deep

00:03:09.110 --> 00:03:09.120
some of the new frontiers of deep
 

00:03:09.120 --> 00:03:11.300
some of the new frontiers of deep
learning as well so let's start by

00:03:11.300 --> 00:03:11.310
learning as well so let's start by
 

00:03:11.310 --> 00:03:13.099
learning as well so let's start by
actually talking about some of the

00:03:13.099 --> 00:03:13.109
actually talking about some of the
 

00:03:13.109 --> 00:03:15.259
actually talking about some of the
classes of what we've seen in this

00:03:15.259 --> 00:03:15.269
classes of what we've seen in this
 

00:03:15.269 --> 00:03:18.140
classes of what we've seen in this
lecture so far and comparing and seeing

00:03:18.140 --> 00:03:18.150
lecture so far and comparing and seeing
 

00:03:18.150 --> 00:03:19.940
lecture so far and comparing and seeing
how reinforcement learning fits into

00:03:19.940 --> 00:03:19.950
how reinforcement learning fits into
 

00:03:19.950 --> 00:03:22.879
how reinforcement learning fits into
those classes so first of all supervised

00:03:22.879 --> 00:03:22.889
those classes so first of all supervised
 

00:03:22.889 --> 00:03:24.319
those classes so first of all supervised
learning is probably the most common

00:03:24.319 --> 00:03:24.329
learning is probably the most common
 

00:03:24.329 --> 00:03:25.550
learning is probably the most common
thing that we've been dealing with in

00:03:25.550 --> 00:03:25.560
thing that we've been dealing with in
 

00:03:25.560 --> 00:03:27.830
thing that we've been dealing with in
this class we're given data and labels

00:03:27.830 --> 00:03:27.840
this class we're given data and labels
 

00:03:27.840 --> 00:03:29.270
this class we're given data and labels
and we're trying to learn this

00:03:29.270 --> 00:03:29.280
and we're trying to learn this
 

00:03:29.280 --> 00:03:31.159
and we're trying to learn this
functional mapping to go from new data

00:03:31.159 --> 00:03:31.169
functional mapping to go from new data
 

00:03:31.169 --> 00:03:32.900
functional mapping to go from new data
to a new to

00:03:32.900 --> 00:03:32.910
to a new to
 

00:03:32.910 --> 00:03:34.610
to a new to
one of the existing labels in our

00:03:34.610 --> 00:03:34.620
one of the existing labels in our
 

00:03:34.620 --> 00:03:38.510
one of the existing labels in our
training set and for an example we can

00:03:38.510 --> 00:03:38.520
training set and for an example we can
 

00:03:38.520 --> 00:03:40.130
training set and for an example we can
do things like classification where I

00:03:40.130 --> 00:03:40.140
do things like classification where I
 

00:03:40.140 --> 00:03:42.080
do things like classification where I
give you an image of an apple and the

00:03:42.080 --> 00:03:42.090
give you an image of an apple and the
 

00:03:42.090 --> 00:03:44.270
give you an image of an apple and the
algorithm is tasked to determine that

00:03:44.270 --> 00:03:44.280
algorithm is tasked to determine that
 

00:03:44.280 --> 00:03:48.440
algorithm is tasked to determine that
this is indeed an apple unsupervised

00:03:48.440 --> 00:03:48.450
this is indeed an apple unsupervised
 

00:03:48.450 --> 00:03:50.480
this is indeed an apple unsupervised
learning is what we discussed yesterday

00:03:50.480 --> 00:03:50.490
learning is what we discussed yesterday
 

00:03:50.490 --> 00:03:52.370
learning is what we discussed yesterday
and this deals with the problem where

00:03:52.370 --> 00:03:52.380
and this deals with the problem where
 

00:03:52.380 --> 00:03:54.970
and this deals with the problem where
there's only data and no labels and

00:03:54.970 --> 00:03:54.980
there's only data and no labels and
 

00:03:54.980 --> 00:03:57.710
there's only data and no labels and
comparing our problem with the Apple

00:03:57.710 --> 00:03:57.720
comparing our problem with the Apple
 

00:03:57.720 --> 00:04:00.110
comparing our problem with the Apple
example here I give it another Apple and

00:04:00.110 --> 00:04:00.120
example here I give it another Apple and
 

00:04:00.120 --> 00:04:01.790
example here I give it another Apple and
it's able to learn that this thing is

00:04:01.790 --> 00:04:01.800
it's able to learn that this thing is
 

00:04:01.800 --> 00:04:03.590
it's able to learn that this thing is
like that other thing even though if it

00:04:03.590 --> 00:04:03.600
like that other thing even though if it
 

00:04:03.600 --> 00:04:05.000
like that other thing even though if it
doesn't know exactly that these are

00:04:05.000 --> 00:04:05.010
doesn't know exactly that these are
 

00:04:05.010 --> 00:04:06.920
doesn't know exactly that these are
apples but it's able to understand some

00:04:06.920 --> 00:04:06.930
apples but it's able to understand some
 

00:04:06.930 --> 00:04:09.260
apples but it's able to understand some
underlying structure about these two

00:04:09.260 --> 00:04:09.270
underlying structure about these two
 

00:04:09.270 --> 00:04:13.070
underlying structure about these two
objects now finally how does

00:04:13.070 --> 00:04:13.080
objects now finally how does
 

00:04:13.080 --> 00:04:14.510
objects now finally how does
reinforcement learning fit into this

00:04:14.510 --> 00:04:14.520
reinforcement learning fit into this
 

00:04:14.520 --> 00:04:16.940
reinforcement learning fit into this
paradigm reinforcement learning deals

00:04:16.940 --> 00:04:16.950
paradigm reinforcement learning deals
 

00:04:16.950 --> 00:04:19.190
paradigm reinforcement learning deals
with something that we call state action

00:04:19.190 --> 00:04:19.200
with something that we call state action
 

00:04:19.200 --> 00:04:24.020
with something that we call state action
pairs so it says pairs of both states in

00:04:24.020 --> 00:04:24.030
pairs so it says pairs of both states in
 

00:04:24.030 --> 00:04:26.270
pairs so it says pairs of both states in
the environment that an agent receives

00:04:26.270 --> 00:04:26.280
the environment that an agent receives
 

00:04:26.280 --> 00:04:29.150
the environment that an agent receives
as well as actions that it takes in that

00:04:29.150 --> 00:04:29.160
as well as actions that it takes in that
 

00:04:29.160 --> 00:04:31.790
as well as actions that it takes in that
environment to execute and observe new

00:04:31.790 --> 00:04:31.800
environment to execute and observe new
 

00:04:31.800 --> 00:04:35.300
environment to execute and observe new
States and the goal of reinforcement

00:04:35.300 --> 00:04:35.310
States and the goal of reinforcement
 

00:04:35.310 --> 00:04:36.920
States and the goal of reinforcement
learning as opposed to supervised or

00:04:36.920 --> 00:04:36.930
learning as opposed to supervised or
 

00:04:36.930 --> 00:04:39.500
learning as opposed to supervised or
unsupervised learning is to actually

00:04:39.500 --> 00:04:39.510
unsupervised learning is to actually
 

00:04:39.510 --> 00:04:42.710
unsupervised learning is to actually
maximize the future rewards that it

00:04:42.710 --> 00:04:42.720
maximize the future rewards that it
 

00:04:42.720 --> 00:04:45.020
maximize the future rewards that it
could see in any future time so to act

00:04:45.020 --> 00:04:45.030
could see in any future time so to act
 

00:04:45.030 --> 00:04:47.780
could see in any future time so to act
optimally in in this environment such I

00:04:47.780 --> 00:04:47.790
optimally in in this environment such I
 

00:04:47.790 --> 00:04:49.670
optimally in in this environment such I
can maximize all future rewards that it

00:04:49.670 --> 00:04:49.680
can maximize all future rewards that it
 

00:04:49.680 --> 00:04:53.390
can maximize all future rewards that it
sees and going back to the Apple example

00:04:53.390 --> 00:04:53.400
sees and going back to the Apple example
 

00:04:53.400 --> 00:04:55.790
sees and going back to the Apple example
if I show it this image of an apple the

00:04:55.790 --> 00:04:55.800
if I show it this image of an apple the
 

00:04:55.800 --> 00:04:57.620
if I show it this image of an apple the
agent might now respond in a

00:04:57.620 --> 00:04:57.630
agent might now respond in a
 

00:04:57.630 --> 00:04:59.600
agent might now respond in a
reinforcement learning setting by saying

00:04:59.600 --> 00:04:59.610
reinforcement learning setting by saying
 

00:04:59.610 --> 00:05:01.670
reinforcement learning setting by saying
I should eat that thing because I've

00:05:01.670 --> 00:05:01.680
I should eat that thing because I've
 

00:05:01.680 --> 00:05:03.770
I should eat that thing because I've
seen in the past that it helps me get

00:05:03.770 --> 00:05:03.780
seen in the past that it helps me get
 

00:05:03.780 --> 00:05:06.730
seen in the past that it helps me get
nutrition and it helps keep me alive so

00:05:06.730 --> 00:05:06.740
nutrition and it helps keep me alive so
 

00:05:06.740 --> 00:05:09.140
nutrition and it helps keep me alive so
again we don't know what this thing is

00:05:09.140 --> 00:05:09.150
again we don't know what this thing is
 

00:05:09.150 --> 00:05:10.520
again we don't know what this thing is
it's not a supervised learning problem

00:05:10.520 --> 00:05:10.530
it's not a supervised learning problem
 

00:05:10.530 --> 00:05:12.110
it's not a supervised learning problem
where we're explicitly telling you that

00:05:12.110 --> 00:05:12.120
where we're explicitly telling you that
 

00:05:12.120 --> 00:05:14.270
where we're explicitly telling you that
this is an apple with nutritional value

00:05:14.270 --> 00:05:14.280
this is an apple with nutritional value
 

00:05:14.280 --> 00:05:16.520
this is an apple with nutritional value
but it's learned over time that it gets

00:05:16.520 --> 00:05:16.530
but it's learned over time that it gets
 

00:05:16.530 --> 00:05:18.560
but it's learned over time that it gets
reward from eating this and that should

00:05:18.560 --> 00:05:18.570
reward from eating this and that should
 

00:05:18.570 --> 00:05:22.400
reward from eating this and that should
continue eating it in the future so our

00:05:22.400 --> 00:05:22.410
continue eating it in the future so our
 

00:05:22.410 --> 00:05:24.560
continue eating it in the future so our
focus today in this class will be on

00:05:24.560 --> 00:05:24.570
focus today in this class will be on
 

00:05:24.570 --> 00:05:27.080
focus today in this class will be on
reinforcement learning and seeing how we

00:05:27.080 --> 00:05:27.090
reinforcement learning and seeing how we
 

00:05:27.090 --> 00:05:30.140
reinforcement learning and seeing how we
can build algorithms to operate in this

00:05:30.140 --> 00:05:30.150
can build algorithms to operate in this
 

00:05:30.150 --> 00:05:33.080
can build algorithms to operate in this
state action or learning and perception

00:05:33.080 --> 00:05:33.090
state action or learning and perception
 

00:05:33.090 --> 00:05:36.830
state action or learning and perception
paradigm so I've defined a couple

00:05:36.830 --> 00:05:36.840
paradigm so I've defined a couple
 

00:05:36.840 --> 00:05:38.420
paradigm so I've defined a couple
concepts in that previous slide like an

00:05:38.420 --> 00:05:38.430
concepts in that previous slide like an
 

00:05:38.430 --> 00:05:40.760
concepts in that previous slide like an
agent and environment action rewards

00:05:40.760 --> 00:05:40.770
agent and environment action rewards
 

00:05:40.770 --> 00:05:44.240
agent and environment action rewards
that I didn't really define and I'd like

00:05:44.240 --> 00:05:44.250
that I didn't really define and I'd like
 

00:05:44.250 --> 00:05:46.250
that I didn't really define and I'd like
to now start by going through

00:05:46.250 --> 00:05:46.260
to now start by going through
 

00:05:46.260 --> 00:05:47.900
to now start by going through
simple example or a simple schematic

00:05:47.900 --> 00:05:47.910
simple example or a simple schematic
 

00:05:47.910 --> 00:05:50.030
simple example or a simple schematic
where I clearly define all of those

00:05:50.030 --> 00:05:50.040
where I clearly define all of those
 

00:05:50.040 --> 00:05:51.890
where I clearly define all of those
things so we can use them later in the

00:05:51.890 --> 00:05:51.900
things so we can use them later in the
 

00:05:51.900 --> 00:05:54.980
things so we can use them later in the
lecture and go into greater greater

00:05:54.980 --> 00:05:54.990
lecture and go into greater greater
 

00:05:54.990 --> 00:05:59.090
lecture and go into greater greater
levels of abstraction so the idea of

00:05:59.090 --> 00:05:59.100
levels of abstraction so the idea of
 

00:05:59.100 --> 00:06:01.550
levels of abstraction so the idea of
reinforcement learning deals with the

00:06:01.550 --> 00:06:01.560
reinforcement learning deals with the
 

00:06:01.560 --> 00:06:03.080
reinforcement learning deals with the
central component of reinforcement

00:06:03.080 --> 00:06:03.090
central component of reinforcement
 

00:06:03.090 --> 00:06:05.420
central component of reinforcement
learning deals with an agent so an agent

00:06:05.420 --> 00:06:05.430
learning deals with an agent so an agent
 

00:06:05.430 --> 00:06:07.730
learning deals with an agent so an agent
for example is like a drone that's

00:06:07.730 --> 00:06:07.740
for example is like a drone that's
 

00:06:07.740 --> 00:06:10.940
for example is like a drone that's
making a delivery it could be also Super

00:06:10.940 --> 00:06:10.950
making a delivery it could be also Super
 

00:06:10.950 --> 00:06:12.530
making a delivery it could be also Super
Mario that's trying to navigate a

00:06:12.530 --> 00:06:12.540
Mario that's trying to navigate a
 

00:06:12.540 --> 00:06:16.190
Mario that's trying to navigate a
videogame the algorithm is the agent and

00:06:16.190 --> 00:06:16.200
videogame the algorithm is the agent and
 

00:06:16.200 --> 00:06:19.460
videogame the algorithm is the agent and
in real life you are the agent okay so

00:06:19.460 --> 00:06:19.470
in real life you are the agent okay so
 

00:06:19.470 --> 00:06:21.530
in real life you are the agent okay so
you're trying to build an algorithm or a

00:06:21.530 --> 00:06:21.540
you're trying to build an algorithm or a
 

00:06:21.540 --> 00:06:24.200
you're trying to build an algorithm or a
machine learning work model that models

00:06:24.200 --> 00:06:24.210
machine learning work model that models
 

00:06:24.210 --> 00:06:27.140
machine learning work model that models
that agent and the agent takes actions

00:06:27.140 --> 00:06:27.150
that agent and the agent takes actions
 

00:06:27.150 --> 00:06:29.660
that agent and the agent takes actions
in some environment now the environment

00:06:29.660 --> 00:06:29.670
in some environment now the environment
 

00:06:29.670 --> 00:06:32.450
in some environment now the environment
is like I said the place where the agent

00:06:32.450 --> 00:06:32.460
is like I said the place where the agent
 

00:06:32.460 --> 00:06:36.170
is like I said the place where the agent
exists and it operates within it can

00:06:36.170 --> 00:06:36.180
exists and it operates within it can
 

00:06:36.180 --> 00:06:38.630
exists and it operates within it can
send actions to that environment an

00:06:38.630 --> 00:06:38.640
send actions to that environment an
 

00:06:38.640 --> 00:06:41.090
send actions to that environment an
action is simply just a possible move

00:06:41.090 --> 00:06:41.100
action is simply just a possible move
 

00:06:41.100 --> 00:06:43.010
action is simply just a possible move
that the agent can make in that

00:06:43.010 --> 00:06:43.020
that the agent can make in that
 

00:06:43.020 --> 00:06:46.640
that the agent can make in that
environment and your action is almost

00:06:46.640 --> 00:06:46.650
environment and your action is almost
 

00:06:46.650 --> 00:06:50.170
environment and your action is almost
self-explanatory but I should note that

00:06:50.170 --> 00:06:50.180
self-explanatory but I should note that
 

00:06:50.180 --> 00:06:53.210
self-explanatory but I should note that
the action that the agent executes at

00:06:53.210 --> 00:06:53.220
the action that the agent executes at
 

00:06:53.220 --> 00:06:55.910
the action that the agent executes at
time T here I'm Dino Nia's little a of T

00:06:55.910 --> 00:06:55.920
time T here I'm Dino Nia's little a of T
 

00:06:55.920 --> 00:06:58.610
time T here I'm Dino Nia's little a of T
can be chosen from usually a discrete

00:06:58.610 --> 00:06:58.620
can be chosen from usually a discrete
 

00:06:58.620 --> 00:07:01.010
can be chosen from usually a discrete
subset of possible actions which we'll

00:07:01.010 --> 00:07:01.020
subset of possible actions which we'll
 

00:07:01.020 --> 00:07:04.640
subset of possible actions which we'll
call capital a so in this first part of

00:07:04.640 --> 00:07:04.650
call capital a so in this first part of
 

00:07:04.650 --> 00:07:06.380
call capital a so in this first part of
the lecture I'll focus on discrete

00:07:06.380 --> 00:07:06.390
the lecture I'll focus on discrete
 

00:07:06.390 --> 00:07:08.870
the lecture I'll focus on discrete
action spaces where there's a limited

00:07:08.870 --> 00:07:08.880
action spaces where there's a limited
 

00:07:08.880 --> 00:07:10.220
action spaces where there's a limited
number of possible actions you can

00:07:10.220 --> 00:07:10.230
number of possible actions you can
 

00:07:10.230 --> 00:07:12.950
number of possible actions you can
imagine me telling it in a video game

00:07:12.950 --> 00:07:12.960
imagine me telling it in a video game
 

00:07:12.960 --> 00:07:15.140
imagine me telling it in a video game
for example that the agent can go either

00:07:15.140 --> 00:07:15.150
for example that the agent can go either
 

00:07:15.150 --> 00:07:17.690
for example that the agent can go either
left or right forwards or backwards pick

00:07:17.690 --> 00:07:17.700
left or right forwards or backwards pick
 

00:07:17.700 --> 00:07:19.550
left or right forwards or backwards pick
up the block put down the block these

00:07:19.550 --> 00:07:19.560
up the block put down the block these
 

00:07:19.560 --> 00:07:21.800
up the block put down the block these
are all possible actions that you could

00:07:21.800 --> 00:07:21.810
are all possible actions that you could
 

00:07:21.810 --> 00:07:26.240
are all possible actions that you could
take in that environment now upon taking

00:07:26.240 --> 00:07:26.250
take in that environment now upon taking
 

00:07:26.250 --> 00:07:28.730
take in that environment now upon taking
an action in the environment the agent

00:07:28.730 --> 00:07:28.740
an action in the environment the agent
 

00:07:28.740 --> 00:07:31.010
an action in the environment the agent
will then receive some observation back

00:07:31.010 --> 00:07:31.020
will then receive some observation back
 

00:07:31.020 --> 00:07:33.400
will then receive some observation back
from the environment these observations

00:07:33.400 --> 00:07:33.410
from the environment these observations
 

00:07:33.410 --> 00:07:36.710
from the environment these observations
actually define how the agent interacts

00:07:36.710 --> 00:07:36.720
actually define how the agent interacts
 

00:07:36.720 --> 00:07:38.330
actually define how the agent interacts
with the environment and it can include

00:07:38.330 --> 00:07:38.340
with the environment and it can include
 

00:07:38.340 --> 00:07:41.930
with the environment and it can include
things like how the state changes so an

00:07:41.930 --> 00:07:41.940
things like how the state changes so an
 

00:07:41.940 --> 00:07:44.030
things like how the state changes so an
example of a state is for example if

00:07:44.030 --> 00:07:44.040
example of a state is for example if
 

00:07:44.040 --> 00:07:46.070
example of a state is for example if
you're the agent the state is what you

00:07:46.070 --> 00:07:46.080
you're the agent the state is what you
 

00:07:46.080 --> 00:07:49.070
you're the agent the state is what you
see so it's the sensor inputs that you

00:07:49.070 --> 00:07:49.080
see so it's the sensor inputs that you
 

00:07:49.080 --> 00:07:51.470
see so it's the sensor inputs that you
obtain it's your vision it's your sound

00:07:51.470 --> 00:07:51.480
obtain it's your vision it's your sound
 

00:07:51.480 --> 00:07:53.720
obtain it's your vision it's your sound
your touch these are all parts of your

00:07:53.720 --> 00:07:53.730
your touch these are all parts of your
 

00:07:53.730 --> 00:07:58.210
your touch these are all parts of your
state and basically it's a concrete

00:07:58.210 --> 00:07:58.220
state and basically it's a concrete
 

00:07:58.220 --> 00:08:00.080
state and basically it's a concrete
situation that the agent

00:08:00.080 --> 00:08:00.090
situation that the agent
 

00:08:00.090 --> 00:08:02.300
situation that the agent
finds itself at any given time so when

00:08:02.300 --> 00:08:02.310
finds itself at any given time so when
 

00:08:02.310 --> 00:08:03.980
finds itself at any given time so when
it takes this action the environment

00:08:03.980 --> 00:08:03.990
it takes this action the environment
 

00:08:03.990 --> 00:08:06.920
it takes this action the environment
responds with a new state that the agent

00:08:06.920 --> 00:08:06.930
responds with a new state that the agent
 

00:08:06.930 --> 00:08:09.879
responds with a new state that the agent
is located and given that action and

00:08:09.879 --> 00:08:09.889
is located and given that action and
 

00:08:09.889 --> 00:08:12.850
is located and given that action and
finally the last part part of this

00:08:12.850 --> 00:08:12.860
finally the last part part of this
 

00:08:12.860 --> 00:08:15.320
finally the last part part of this
schematic is the reward that the

00:08:15.320 --> 00:08:15.330
schematic is the reward that the
 

00:08:15.330 --> 00:08:17.270
schematic is the reward that the
environment responds with given that

00:08:17.270 --> 00:08:17.280
environment responds with given that
 

00:08:17.280 --> 00:08:19.760
environment responds with given that
action so the reward is basically like a

00:08:19.760 --> 00:08:19.770
action so the reward is basically like a
 

00:08:19.770 --> 00:08:22.909
action so the reward is basically like a
feedback that measures the success or

00:08:22.909 --> 00:08:22.919
feedback that measures the success or
 

00:08:22.919 --> 00:08:25.400
feedback that measures the success or
failure of the given action taken in

00:08:25.400 --> 00:08:25.410
failure of the given action taken in
 

00:08:25.410 --> 00:08:28.129
failure of the given action taken in
that environment for example in a video

00:08:28.129 --> 00:08:28.139
that environment for example in a video
 

00:08:28.139 --> 00:08:30.740
that environment for example in a video
game when Mario touches a coin it gets

00:08:30.740 --> 00:08:30.750
game when Mario touches a coin it gets
 

00:08:30.750 --> 00:08:33.620
game when Mario touches a coin it gets
an eight it gets a reward from any given

00:08:33.620 --> 00:08:33.630
an eight it gets a reward from any given
 

00:08:33.630 --> 00:08:35.899
an eight it gets a reward from any given
state an agent sends an output in the

00:08:35.899 --> 00:08:35.909
state an agent sends an output in the
 

00:08:35.909 --> 00:08:37.909
state an agent sends an output in the
form of actions to the environment and

00:08:37.909 --> 00:08:37.919
form of actions to the environment and
 

00:08:37.919 --> 00:08:40.459
form of actions to the environment and
the environment returns the agents new

00:08:40.459 --> 00:08:40.469
the environment returns the agents new
 

00:08:40.469 --> 00:08:42.950
the environment returns the agents new
state and an associated reward with that

00:08:42.950 --> 00:08:42.960
state and an associated reward with that
 

00:08:42.960 --> 00:08:46.430
state and an associated reward with that
action rewards can be either immediate

00:08:46.430 --> 00:08:46.440
action rewards can be either immediate
 

00:08:46.440 --> 00:08:48.110
action rewards can be either immediate
where you take an action and you

00:08:48.110 --> 00:08:48.120
where you take an action and you
 

00:08:48.120 --> 00:08:49.760
where you take an action and you
immediately get a reward or they can be

00:08:49.760 --> 00:08:49.770
immediately get a reward or they can be
 

00:08:49.770 --> 00:08:51.769
immediately get a reward or they can be
delayed in the context of delayed

00:08:51.769 --> 00:08:51.779
delayed in the context of delayed
 

00:08:51.779 --> 00:08:53.210
delayed in the context of delayed
gratification where you may take an

00:08:53.210 --> 00:08:53.220
gratification where you may take an
 

00:08:53.220 --> 00:08:55.250
gratification where you may take an
action today that you may not benefit

00:08:55.250 --> 00:08:55.260
action today that you may not benefit
 

00:08:55.260 --> 00:08:58.190
action today that you may not benefit
until you may not benefit from until

00:08:58.190 --> 00:08:58.200
until you may not benefit from until
 

00:08:58.200 --> 00:08:59.990
until you may not benefit from until
tomorrow or the day after tomorrow or

00:08:59.990 --> 00:09:00.000
tomorrow or the day after tomorrow or
 

00:09:00.000 --> 00:09:03.019
tomorrow or the day after tomorrow or
even in some cases very long term into

00:09:03.019 --> 00:09:03.029
even in some cases very long term into
 

00:09:03.029 --> 00:09:09.410
even in some cases very long term into
the future so building on this concept

00:09:09.410 --> 00:09:09.420
the future so building on this concept
 

00:09:09.420 --> 00:09:12.019
the future so building on this concept
of rewards we can define this notion of

00:09:12.019 --> 00:09:12.029
of rewards we can define this notion of
 

00:09:12.029 --> 00:09:14.150
of rewards we can define this notion of
a total reward that the agent obtains at

00:09:14.150 --> 00:09:14.160
a total reward that the agent obtains at
 

00:09:14.160 --> 00:09:16.670
a total reward that the agent obtains at
any given time the total future reward

00:09:16.670 --> 00:09:16.680
any given time the total future reward
 

00:09:16.680 --> 00:09:19.220
any given time the total future reward
rather as just the sum of all rewards

00:09:19.220 --> 00:09:19.230
rather as just the sum of all rewards
 

00:09:19.230 --> 00:09:21.800
rather as just the sum of all rewards
from that time step into the future

00:09:21.800 --> 00:09:21.810
from that time step into the future
 

00:09:21.810 --> 00:09:24.199
from that time step into the future
so for example if we're starting at time

00:09:24.199 --> 00:09:24.209
so for example if we're starting at time
 

00:09:24.209 --> 00:09:26.449
so for example if we're starting at time
T we're looking at the reward that it

00:09:26.449 --> 00:09:26.459
T we're looking at the reward that it
 

00:09:26.459 --> 00:09:28.370
T we're looking at the reward that it
obtains at time T plus the time to

00:09:28.370 --> 00:09:28.380
obtains at time T plus the time to
 

00:09:28.380 --> 00:09:31.400
obtains at time T plus the time to
reward that obtains at time t + 1 +

00:09:31.400 --> 00:09:31.410
reward that obtains at time t + 1 +
 

00:09:31.410 --> 00:09:36.320
reward that obtains at time t + 1 +
roared at t + 2 and so on there's a

00:09:36.320 --> 00:09:36.330
roared at t + 2 and so on there's a
 

00:09:36.330 --> 00:09:38.600
roared at t + 2 and so on there's a
slight problem here and that's that if

00:09:38.600 --> 00:09:38.610
slight problem here and that's that if
 

00:09:38.610 --> 00:09:40.970
slight problem here and that's that if
you are going to infinity if this

00:09:40.970 --> 00:09:40.980
you are going to infinity if this
 

00:09:40.980 --> 00:09:42.680
you are going to infinity if this
summation is going to infinity so if

00:09:42.680 --> 00:09:42.690
summation is going to infinity so if
 

00:09:42.690 --> 00:09:44.360
summation is going to infinity so if
you're looking at infinite time horizons

00:09:44.360 --> 00:09:44.370
you're looking at infinite time horizons
 

00:09:44.370 --> 00:09:47.210
you're looking at infinite time horizons
with potentially infinite rewards this

00:09:47.210 --> 00:09:47.220
with potentially infinite rewards this
 

00:09:47.220 --> 00:09:49.850
with potentially infinite rewards this
term capital R of T could also go to

00:09:49.850 --> 00:09:49.860
term capital R of T could also go to
 

00:09:49.860 --> 00:09:52.400
term capital R of T could also go to
infinity and that's not a great property

00:09:52.400 --> 00:09:52.410
infinity and that's not a great property
 

00:09:52.410 --> 00:09:55.130
infinity and that's not a great property
of mathematical equations so what we do

00:09:55.130 --> 00:09:55.140
of mathematical equations so what we do
 

00:09:55.140 --> 00:09:57.650
of mathematical equations so what we do
is we introduce this notion of the

00:09:57.650 --> 00:09:57.660
is we introduce this notion of the
 

00:09:57.660 --> 00:09:59.900
is we introduce this notion of the
discounted reward that's essentially

00:09:59.900 --> 00:09:59.910
discounted reward that's essentially
 

00:09:59.910 --> 00:10:02.930
discounted reward that's essentially
where we discount future actions based

00:10:02.930 --> 00:10:02.940
where we discount future actions based
 

00:10:02.940 --> 00:10:05.660
where we discount future actions based
on this discounting factor gamma it's

00:10:05.660 --> 00:10:05.670
on this discounting factor gamma it's
 

00:10:05.670 --> 00:10:07.790
on this discounting factor gamma it's
just a number between zero and one where

00:10:07.790 --> 00:10:07.800
just a number between zero and one where
 

00:10:07.800 --> 00:10:09.860
just a number between zero and one where
we're placing more weight on rewards

00:10:09.860 --> 00:10:09.870
we're placing more weight on rewards
 

00:10:09.870 --> 00:10:11.540
we're placing more weight on rewards
obtained in the near-term future and

00:10:11.540 --> 00:10:11.550
obtained in the near-term future and
 

00:10:11.550 --> 00:10:13.470
obtained in the near-term future and
placing less weight on

00:10:13.470 --> 00:10:13.480
placing less weight on
 

00:10:13.480 --> 00:10:16.620
placing less weight on
it's placed in long term long term time

00:10:16.620 --> 00:10:16.630
it's placed in long term long term time
 

00:10:16.630 --> 00:10:20.000
it's placed in long term long term time
steps from the current state

00:10:20.000 --> 00:10:20.010
 

00:10:20.010 --> 00:10:23.220
so I guess before moving on any further

00:10:23.220 --> 00:10:23.230
so I guess before moving on any further
 

00:10:23.230 --> 00:10:24.960
so I guess before moving on any further
I want to make sure that everyone

00:10:24.960 --> 00:10:24.970
I want to make sure that everyone
 

00:10:24.970 --> 00:10:26.550
I want to make sure that everyone
understands all of these concepts

00:10:26.550 --> 00:10:26.560
understands all of these concepts
 

00:10:26.560 --> 00:10:28.110
understands all of these concepts
present in this schematic because we're

00:10:28.110 --> 00:10:28.120
present in this schematic because we're
 

00:10:28.120 --> 00:10:29.400
present in this schematic because we're
only going to be building on things from

00:10:29.400 --> 00:10:29.410
only going to be building on things from
 

00:10:29.410 --> 00:10:32.670
only going to be building on things from
this point on and using these using this

00:10:32.670 --> 00:10:32.680
this point on and using these using this
 

00:10:32.680 --> 00:10:34.920
this point on and using these using this
terminology to build higher and higher

00:10:34.920 --> 00:10:34.930
terminology to build higher and higher
 

00:10:34.930 --> 00:10:37.439
terminology to build higher and higher
levels of that extraction so is this all

00:10:37.439 --> 00:10:37.449
levels of that extraction so is this all
 

00:10:37.449 --> 00:10:45.060
levels of that extraction so is this all
clear to everyone yep go ahead the state

00:10:45.060 --> 00:10:45.070
clear to everyone yep go ahead the state
 

00:10:45.070 --> 00:10:47.220
clear to everyone yep go ahead the state
changes immediate so usually you take an

00:10:47.220 --> 00:10:47.230
changes immediate so usually you take an
 

00:10:47.230 --> 00:10:49.410
changes immediate so usually you take an
action and that's let's suppose in like

00:10:49.410 --> 00:10:49.420
action and that's let's suppose in like
 

00:10:49.420 --> 00:10:50.759
action and that's let's suppose in like
self-driving cars if you want to train

00:10:50.759 --> 00:10:50.769
self-driving cars if you want to train
 

00:10:50.769 --> 00:10:52.560
self-driving cars if you want to train
an agent to navigate in a self-driving

00:10:52.560 --> 00:10:52.570
an agent to navigate in a self-driving
 

00:10:52.570 --> 00:10:53.310
an agent to navigate in a self-driving
car world

00:10:53.310 --> 00:10:53.320
car world
 

00:10:53.320 --> 00:10:56.100
car world
you do it your action is a steering

00:10:56.100 --> 00:10:56.110
you do it your action is a steering
 

00:10:56.110 --> 00:10:58.829
you do it your action is a steering
wheel angle and the next state is the

00:10:58.829 --> 00:10:58.839
wheel angle and the next state is the
 

00:10:58.839 --> 00:11:04.950
wheel angle and the next state is the
next camera and put that that car sees

00:11:04.950 --> 00:11:04.960
 

00:11:04.960 --> 00:11:06.990
it does not need to be related to the

00:11:06.990 --> 00:11:07.000
it does not need to be related to the
 

00:11:07.000 --> 00:11:08.670
it does not need to be related to the
world so the rewards are basically just

00:11:08.670 --> 00:11:08.680
world so the rewards are basically just
 

00:11:08.680 --> 00:11:11.070
world so the rewards are basically just
a scalar number that come back with each

00:11:11.070 --> 00:11:11.080
a scalar number that come back with each
 

00:11:11.080 --> 00:11:13.920
a scalar number that come back with each
state but you could also imagine where

00:11:13.920 --> 00:11:13.930
state but you could also imagine where
 

00:11:13.930 --> 00:11:16.260
state but you could also imagine where
there are no rewards and at a given time

00:11:16.260 --> 00:11:16.270
there are no rewards and at a given time
 

00:11:16.270 --> 00:11:18.570
there are no rewards and at a given time
step you might not see a reward but you

00:11:18.570 --> 00:11:18.580
step you might not see a reward but you
 

00:11:18.580 --> 00:11:20.540
step you might not see a reward but you
might see a reward way into the future

00:11:20.540 --> 00:11:20.550
might see a reward way into the future
 

00:11:20.550 --> 00:11:25.620
might see a reward way into the future
so for example in in some games like for

00:11:25.620 --> 00:11:25.630
so for example in in some games like for
 

00:11:25.630 --> 00:11:27.810
so for example in in some games like for
example pong you can imagine you've

00:11:27.810 --> 00:11:27.820
example pong you can imagine you've
 

00:11:27.820 --> 00:11:29.699
example pong you can imagine you've
never get a reward until you either win

00:11:29.699 --> 00:11:29.709
never get a reward until you either win
 

00:11:29.709 --> 00:11:31.680
never get a reward until you either win
or lose the game if you win the game you

00:11:31.680 --> 00:11:31.690
or lose the game if you win the game you
 

00:11:31.690 --> 00:11:33.060
or lose the game if you win the game you
get a positive reward but if you lose

00:11:33.060 --> 00:11:33.070
get a positive reward but if you lose
 

00:11:33.070 --> 00:11:34.470
get a positive reward but if you lose
the game you get a negative reward and

00:11:34.470 --> 00:11:34.480
the game you get a negative reward and
 

00:11:34.480 --> 00:11:37.350
the game you get a negative reward and
all of the intermediate frames and

00:11:37.350 --> 00:11:37.360
all of the intermediate frames and
 

00:11:37.360 --> 00:11:39.630
all of the intermediate frames and
actions that you take never result in

00:11:39.630 --> 00:11:39.640
actions that you take never result in
 

00:11:39.640 --> 00:11:43.019
actions that you take never result in
any reward at all so now we're gonna

00:11:43.019 --> 00:11:43.029
any reward at all so now we're gonna
 

00:11:43.029 --> 00:11:46.260
any reward at all so now we're gonna
take this notion of an agent collecting

00:11:46.260 --> 00:11:46.270
take this notion of an agent collecting
 

00:11:46.270 --> 00:11:48.570
take this notion of an agent collecting
rewards in an environment and try to

00:11:48.570 --> 00:11:48.580
rewards in an environment and try to
 

00:11:48.580 --> 00:11:50.579
rewards in an environment and try to
define what we call a queue function

00:11:50.579 --> 00:11:50.589
define what we call a queue function
 

00:11:50.589 --> 00:11:53.010
define what we call a queue function
this is kind of a fundamental function

00:11:53.010 --> 00:11:53.020
this is kind of a fundamental function
 

00:11:53.020 --> 00:11:54.449
this is kind of a fundamental function
that's gonna be the building block of

00:11:54.449 --> 00:11:54.459
that's gonna be the building block of
 

00:11:54.459 --> 00:11:56.490
that's gonna be the building block of
one of the algorithms that we're gonna

00:11:56.490 --> 00:11:56.500
one of the algorithms that we're gonna
 

00:11:56.500 --> 00:11:58.710
one of the algorithms that we're gonna
use in reinforcement learning in the

00:11:58.710 --> 00:11:58.720
use in reinforcement learning in the
 

00:11:58.720 --> 00:12:01.800
use in reinforcement learning in the
first part of this lecture and just to

00:12:01.800 --> 00:12:01.810
first part of this lecture and just to
 

00:12:01.810 --> 00:12:04.650
first part of this lecture and just to
reiterate again this this equation which

00:12:04.650 --> 00:12:04.660
reiterate again this this equation which
 

00:12:04.660 --> 00:12:06.150
reiterate again this this equation which
you see on the top is the same equation

00:12:06.150 --> 00:12:06.160
you see on the top is the same equation
 

00:12:06.160 --> 00:12:08.579
you see on the top is the same equation
as before it's saying the total reward

00:12:08.579 --> 00:12:08.589
as before it's saying the total reward
 

00:12:08.589 --> 00:12:11.040
as before it's saying the total reward
that we can obtain from time T it's just

00:12:11.040 --> 00:12:11.050
that we can obtain from time T it's just
 

00:12:11.050 --> 00:12:13.199
that we can obtain from time T it's just
a summation of the discounted rewards in

00:12:13.199 --> 00:12:13.209
a summation of the discounted rewards in
 

00:12:13.209 --> 00:12:16.650
a summation of the discounted rewards in
the future ok and now we want to define

00:12:16.650 --> 00:12:16.660
the future ok and now we want to define
 

00:12:16.660 --> 00:12:18.720
the future ok and now we want to define
this dysfunction which we're going to

00:12:18.720 --> 00:12:18.730
this dysfunction which we're going to
 

00:12:18.730 --> 00:12:20.640
this dysfunction which we're going to
call the cue function and it's going to

00:12:20.640 --> 00:12:20.650
call the cue function and it's going to
 

00:12:20.650 --> 00:12:22.680
call the cue function and it's going to
take as input two things one is the

00:12:22.680 --> 00:12:22.690
take as input two things one is the
 

00:12:22.690 --> 00:12:25.290
take as input two things one is the
state and the action that the agent

00:12:25.290 --> 00:12:25.300
state and the action that the agent
 

00:12:25.300 --> 00:12:28.590
state and the action that the agent
wants to execute at that state and we

00:12:28.590 --> 00:12:28.600
wants to execute at that state and we
 

00:12:28.600 --> 00:12:31.260
wants to execute at that state and we
want that cue function to represent the

00:12:31.260 --> 00:12:31.270
want that cue function to represent the
 

00:12:31.270 --> 00:12:34.350
want that cue function to represent the
expected total discounted reward that it

00:12:34.350 --> 00:12:34.360
expected total discounted reward that it
 

00:12:34.360 --> 00:12:37.860
expected total discounted reward that it
could obtain in the future so now to

00:12:37.860 --> 00:12:37.870
could obtain in the future so now to
 

00:12:37.870 --> 00:12:39.600
could obtain in the future so now to
give an example of this let's assume

00:12:39.600 --> 00:12:39.610
give an example of this let's assume
 

00:12:39.610 --> 00:12:41.250
give an example of this let's assume
let's go back to the self-driving car

00:12:41.250 --> 00:12:41.260
let's go back to the self-driving car
 

00:12:41.260 --> 00:12:44.820
let's go back to the self-driving car
example your plate you're placing your

00:12:44.820 --> 00:12:44.830
example your plate you're placing your
 

00:12:44.830 --> 00:12:46.680
example your plate you're placing your
self-driving car on a position on the

00:12:46.680 --> 00:12:46.690
self-driving car on a position on the
 

00:12:46.690 --> 00:12:50.940
self-driving car on a position on the
road that's your state and you want to

00:12:50.940 --> 00:12:50.950
road that's your state and you want to
 

00:12:50.950 --> 00:12:53.220
road that's your state and you want to
know for any given action what is the

00:12:53.220 --> 00:12:53.230
know for any given action what is the
 

00:12:53.230 --> 00:12:55.740
know for any given action what is the
total amount of reward future reward

00:12:55.740 --> 00:12:55.750
total amount of reward future reward
 

00:12:55.750 --> 00:12:58.590
total amount of reward future reward
that that car can achieve by executing

00:12:58.590 --> 00:12:58.600
that that car can achieve by executing
 

00:12:58.600 --> 00:13:00.690
that that car can achieve by executing
that action of course some actions will

00:13:00.690 --> 00:13:00.700
that action of course some actions will
 

00:13:00.700 --> 00:13:03.600
that action of course some actions will
result in better Q values higher Q

00:13:03.600 --> 00:13:03.610
result in better Q values higher Q
 

00:13:03.610 --> 00:13:05.370
result in better Q values higher Q
values because you're going to obtain if

00:13:05.370 --> 00:13:05.380
values because you're going to obtain if
 

00:13:05.380 --> 00:13:08.730
values because you're going to obtain if
the road is straight obtaining a higher

00:13:08.730 --> 00:13:08.740
the road is straight obtaining a higher
 

00:13:08.740 --> 00:13:11.670
the road is straight obtaining a higher
Q value might occur if you are taking

00:13:11.670 --> 00:13:11.680
Q value might occur if you are taking
 

00:13:11.680 --> 00:13:13.230
Q value might occur if you are taking
actions corresponding to straight

00:13:13.230 --> 00:13:13.240
actions corresponding to straight
 

00:13:13.240 --> 00:13:15.390
actions corresponding to straight
steering bowl angles but if you try to

00:13:15.390 --> 00:13:15.400
steering bowl angles but if you try to
 

00:13:15.400 --> 00:13:17.610
steering bowl angles but if you try to
steer sharp to the right or sharp to the

00:13:17.610 --> 00:13:17.620
steer sharp to the right or sharp to the
 

00:13:17.620 --> 00:13:18.600
steer sharp to the right or sharp to the
left your Q

00:13:18.600 --> 00:13:18.610
left your Q
 

00:13:18.610 --> 00:13:21.060
left your Q
is going to sharply decrease on each end

00:13:21.060 --> 00:13:21.070
is going to sharply decrease on each end
 

00:13:21.070 --> 00:13:23.430
is going to sharply decrease on each end
because these are undesirable actions so

00:13:23.430 --> 00:13:23.440
because these are undesirable actions so
 

00:13:23.440 --> 00:13:26.220
because these are undesirable actions so
in a sense our Q value our Q function is

00:13:26.220 --> 00:13:26.230
in a sense our Q value our Q function is
 

00:13:26.230 --> 00:13:29.820
in a sense our Q value our Q function is
telling us for any given action that the

00:13:29.820 --> 00:13:29.830
telling us for any given action that the
 

00:13:29.830 --> 00:13:32.760
telling us for any given action that the
agent can make in a given state what is

00:13:32.760 --> 00:13:32.770
agent can make in a given state what is
 

00:13:32.770 --> 00:13:35.460
agent can make in a given state what is
the what is the expected reward that it

00:13:35.460 --> 00:13:35.470
the what is the expected reward that it
 

00:13:35.470 --> 00:13:41.060
the what is the expected reward that it
can obtain by executing that action okay

00:13:41.060 --> 00:13:41.070
can obtain by executing that action okay
 

00:13:41.070 --> 00:13:44.160
can obtain by executing that action okay
so the key part of this problem in

00:13:44.160 --> 00:13:44.170
so the key part of this problem in
 

00:13:44.170 --> 00:13:45.540
so the key part of this problem in
reinforcement learning is actually

00:13:45.540 --> 00:13:45.550
reinforcement learning is actually
 

00:13:45.550 --> 00:13:47.370
reinforcement learning is actually
learning this function this is the hard

00:13:47.370 --> 00:13:47.380
learning this function this is the hard
 

00:13:47.380 --> 00:13:49.170
learning this function this is the hard
thing so we want to learn this Q value

00:13:49.170 --> 00:13:49.180
thing so we want to learn this Q value
 

00:13:49.180 --> 00:13:51.150
thing so we want to learn this Q value
function so given a state and given an

00:13:51.150 --> 00:13:51.160
function so given a state and given an
 

00:13:51.160 --> 00:13:53.880
function so given a state and given an
input how can we compute that expected

00:13:53.880 --> 00:13:53.890
input how can we compute that expected
 

00:13:53.890 --> 00:13:57.390
input how can we compute that expected
return of reward but ultimately what we

00:13:57.390 --> 00:13:57.400
return of reward but ultimately what we
 

00:13:57.400 --> 00:13:59.610
return of reward but ultimately what we
need to actually act in the environment

00:13:59.610 --> 00:13:59.620
need to actually act in the environment
 

00:13:59.620 --> 00:14:01.530
need to actually act in the environment
is a new function that I haven't defined

00:14:01.530 --> 00:14:01.540
is a new function that I haven't defined
 

00:14:01.540 --> 00:14:03.300
is a new function that I haven't defined
it and that's called the policy function

00:14:03.300 --> 00:14:03.310
it and that's called the policy function
 

00:14:03.310 --> 00:14:06.480
it and that's called the policy function
so here we're calling PI of s the policy

00:14:06.480 --> 00:14:06.490
so here we're calling PI of s the policy
 

00:14:06.490 --> 00:14:09.480
so here we're calling PI of s the policy
and here the policy only takes as input

00:14:09.480 --> 00:14:09.490
and here the policy only takes as input
 

00:14:09.490 --> 00:14:11.910
and here the policy only takes as input
just the state so it doesn't care about

00:14:11.910 --> 00:14:11.920
just the state so it doesn't care about
 

00:14:11.920 --> 00:14:14.700
just the state so it doesn't care about
the action that the agent takes in fact

00:14:14.700 --> 00:14:14.710
the action that the agent takes in fact
 

00:14:14.710 --> 00:14:17.160
the action that the agent takes in fact
it wants to output the desired action

00:14:17.160 --> 00:14:17.170
it wants to output the desired action
 

00:14:17.170 --> 00:14:19.740
it wants to output the desired action
given any state so the agent obtains

00:14:19.740 --> 00:14:19.750
given any state so the agent obtains
 

00:14:19.750 --> 00:14:21.930
given any state so the agent obtains
some state it perceives the world and

00:14:21.930 --> 00:14:21.940
some state it perceives the world and
 

00:14:21.940 --> 00:14:24.330
some state it perceives the world and
ultimately you want your policy to

00:14:24.330 --> 00:14:24.340
ultimately you want your policy to
 

00:14:24.340 --> 00:14:27.180
ultimately you want your policy to
output the optimal action to take given

00:14:27.180 --> 00:14:27.190
output the optimal action to take given
 

00:14:27.190 --> 00:14:29.280
output the optimal action to take given
that state that's ultimately the goal of

00:14:29.280 --> 00:14:29.290
that state that's ultimately the goal of
 

00:14:29.290 --> 00:14:30.750
that state that's ultimately the goal of
reinforcement learning you want to see a

00:14:30.750 --> 00:14:30.760
reinforcement learning you want to see a
 

00:14:30.760 --> 00:14:32.970
reinforcement learning you want to see a
state and then know how to act in that

00:14:32.970 --> 00:14:32.980
state and then know how to act in that
 

00:14:32.980 --> 00:14:35.580
state and then know how to act in that
state now the question I want to pose

00:14:35.580 --> 00:14:35.590
state now the question I want to pose
 

00:14:35.590 --> 00:14:39.030
state now the question I want to pose
here is assuming we can learn this q

00:14:39.030 --> 00:14:39.040
here is assuming we can learn this q
 

00:14:39.040 --> 00:14:41.940
here is assuming we can learn this q
function is there a way that we can now

00:14:41.940 --> 00:14:41.950
function is there a way that we can now
 

00:14:41.950 --> 00:14:47.150
function is there a way that we can now
create or infer our policy function and

00:14:47.150 --> 00:14:47.160
create or infer our policy function and
 

00:14:47.160 --> 00:14:51.120
create or infer our policy function and
I hope it's a little obvious here that

00:14:51.120 --> 00:14:51.130
I hope it's a little obvious here that
 

00:14:51.130 --> 00:14:53.390
I hope it's a little obvious here that
the strategy we want to take is

00:14:53.390 --> 00:14:53.400
the strategy we want to take is
 

00:14:53.400 --> 00:14:55.590
the strategy we want to take is
essentially like we want to try all

00:14:55.590 --> 00:14:55.600
essentially like we want to try all
 

00:14:55.600 --> 00:14:57.990
essentially like we want to try all
possible actions that the agent can take

00:14:57.990 --> 00:14:58.000
possible actions that the agent can take
 

00:14:58.000 --> 00:15:00.390
possible actions that the agent can take
in that given state and just find the

00:15:00.390 --> 00:15:00.400
in that given state and just find the
 

00:15:00.400 --> 00:15:02.310
in that given state and just find the
maximum the one that results in the

00:15:02.310 --> 00:15:02.320
maximum the one that results in the
 

00:15:02.320 --> 00:15:05.010
maximum the one that results in the
maximum reward and the ones that results

00:15:05.010 --> 00:15:05.020
maximum reward and the ones that results
 

00:15:05.020 --> 00:15:06.540
maximum reward and the ones that results
in the maximum reward is just going to

00:15:06.540 --> 00:15:06.550
in the maximum reward is just going to
 

00:15:06.550 --> 00:15:08.870
in the maximum reward is just going to
be the one that has the maximum Q value

00:15:08.870 --> 00:15:08.880
be the one that has the maximum Q value
 

00:15:08.880 --> 00:15:11.340
be the one that has the maximum Q value
so what we're gonna define the policy

00:15:11.340 --> 00:15:11.350
so what we're gonna define the policy
 

00:15:11.350 --> 00:15:14.490
so what we're gonna define the policy
function here as it's just the Arg max

00:15:14.490 --> 00:15:14.500
function here as it's just the Arg max
 

00:15:14.500 --> 00:15:17.760
function here as it's just the Arg max
over all possible actions of that q

00:15:17.760 --> 00:15:17.770
over all possible actions of that q
 

00:15:17.770 --> 00:15:20.610
over all possible actions of that q
value function so what that means just

00:15:20.610 --> 00:15:20.620
value function so what that means just
 

00:15:20.620 --> 00:15:22.380
value function so what that means just
one more time is that we're gonna plug

00:15:22.380 --> 00:15:22.390
one more time is that we're gonna plug
 

00:15:22.390 --> 00:15:25.380
one more time is that we're gonna plug
in all possible actions given the state

00:15:25.380 --> 00:15:25.390
in all possible actions given the state
 

00:15:25.390 --> 00:15:28.680
in all possible actions given the state
into the Q value find the action that

00:15:28.680 --> 00:15:28.690
into the Q value find the action that
 

00:15:28.690 --> 00:15:30.540
into the Q value find the action that
results in the highest possible total

00:15:30.540 --> 00:15:30.550
results in the highest possible total
 

00:15:30.550 --> 00:15:31.920
results in the highest possible total
return in rewards

00:15:31.920 --> 00:15:31.930
return in rewards
 

00:15:31.930 --> 00:15:33.630
return in rewards
and that's going to be the action that

00:15:33.630 --> 00:15:33.640
and that's going to be the action that
 

00:15:33.640 --> 00:15:35.790
and that's going to be the action that
we take at that given state in deep

00:15:35.790 --> 00:15:35.800
we take at that given state in deep
 

00:15:35.800 --> 00:15:37.320
we take at that given state in deep
reinforcement learning there are two

00:15:37.320 --> 00:15:37.330
reinforcement learning there are two
 

00:15:37.330 --> 00:15:39.570
reinforcement learning there are two
main ways that we can try to learn

00:15:39.570 --> 00:15:39.580
main ways that we can try to learn
 

00:15:39.580 --> 00:15:42.120
main ways that we can try to learn
policy functions the first way is

00:15:42.120 --> 00:15:42.130
policy functions the first way is
 

00:15:42.130 --> 00:15:44.790
policy functions the first way is
actually by like I was alluding to

00:15:44.790 --> 00:15:44.800
actually by like I was alluding to
 

00:15:44.800 --> 00:15:46.889
actually by like I was alluding to
before trying to first learn the q-value

00:15:46.889 --> 00:15:46.899
before trying to first learn the q-value
 

00:15:46.899 --> 00:15:49.530
before trying to first learn the q-value
function so that's on the left hand side

00:15:49.530 --> 00:15:49.540
function so that's on the left hand side
 

00:15:49.540 --> 00:15:51.810
function so that's on the left hand side
so we try and learn this cue function

00:15:51.810 --> 00:15:51.820
so we try and learn this cue function
 

00:15:51.820 --> 00:15:53.850
so we try and learn this cue function
that goes from States and actions and

00:15:53.850 --> 00:15:53.860
that goes from States and actions and
 

00:15:53.860 --> 00:15:57.750
that goes from States and actions and
then use that to infer a deterministic

00:15:57.750 --> 00:15:57.760
then use that to infer a deterministic
 

00:15:57.760 --> 00:15:59.850
then use that to infer a deterministic
signal of which action to take given the

00:15:59.850 --> 00:15:59.860
signal of which action to take given the
 

00:15:59.860 --> 00:16:01.350
signal of which action to take given the
state that we're currently in using this

00:16:01.350 --> 00:16:01.360
state that we're currently in using this
 

00:16:01.360 --> 00:16:03.590
state that we're currently in using this
argument function our max function and

00:16:03.590 --> 00:16:03.600
argument function our max function and
 

00:16:03.600 --> 00:16:06.750
argument function our max function and
that's like what we just saw another

00:16:06.750 --> 00:16:06.760
that's like what we just saw another
 

00:16:06.760 --> 00:16:08.220
that's like what we just saw another
alternative approach that we'll discuss

00:16:08.220 --> 00:16:08.230
alternative approach that we'll discuss
 

00:16:08.230 --> 00:16:10.740
alternative approach that we'll discuss
later in the class is using what's

00:16:10.740 --> 00:16:10.750
later in the class is using what's
 

00:16:10.750 --> 00:16:13.199
later in the class is using what's
called as policy learning and here we

00:16:13.199 --> 00:16:13.209
called as policy learning and here we
 

00:16:13.209 --> 00:16:16.079
called as policy learning and here we
don't care about explicitly modeling the

00:16:16.079 --> 00:16:16.089
don't care about explicitly modeling the
 

00:16:16.089 --> 00:16:18.690
don't care about explicitly modeling the
cue function but instead we want to just

00:16:18.690 --> 00:16:18.700
cue function but instead we want to just
 

00:16:18.700 --> 00:16:21.389
cue function but instead we want to just
have our output of our model be the

00:16:21.389 --> 00:16:21.399
have our output of our model be the
 

00:16:21.399 --> 00:16:23.430
have our output of our model be the
policy that the agent should take so

00:16:23.430 --> 00:16:23.440
policy that the agent should take so
 

00:16:23.440 --> 00:16:26.269
policy that the agent should take so
here the model that we're creating is

00:16:26.269 --> 00:16:26.279
here the model that we're creating is
 

00:16:26.279 --> 00:16:29.220
here the model that we're creating is
not taking as input both the state and

00:16:29.220 --> 00:16:29.230
not taking as input both the state and
 

00:16:29.230 --> 00:16:31.019
not taking as input both the state and
the action it's only taking as input the

00:16:31.019 --> 00:16:31.029
the action it's only taking as input the
 

00:16:31.029 --> 00:16:34.470
the action it's only taking as input the
state and it's predicting a probability

00:16:34.470 --> 00:16:34.480
state and it's predicting a probability
 

00:16:34.480 --> 00:16:37.230
state and it's predicting a probability
distribution which is PI over all

00:16:37.230 --> 00:16:37.240
distribution which is PI over all
 

00:16:37.240 --> 00:16:39.510
distribution which is PI over all
possible actions probability

00:16:39.510 --> 00:16:39.520
possible actions probability
 

00:16:39.520 --> 00:16:41.400
possible actions probability
distributions sum up to one they have

00:16:41.400 --> 00:16:41.410
distributions sum up to one they have
 

00:16:41.410 --> 00:16:42.780
distributions sum up to one they have
some nice properties and then what we

00:16:42.780 --> 00:16:42.790
some nice properties and then what we
 

00:16:42.790 --> 00:16:44.370
some nice properties and then what we
can do is we can actually just sample an

00:16:44.370 --> 00:16:44.380
can do is we can actually just sample an
 

00:16:44.380 --> 00:16:46.050
can do is we can actually just sample an
action from that probability

00:16:46.050 --> 00:16:46.060
action from that probability
 

00:16:46.060 --> 00:16:47.850
action from that probability
distribution in order to act in that

00:16:47.850 --> 00:16:47.860
distribution in order to act in that
 

00:16:47.860 --> 00:16:51.390
distribution in order to act in that
state so like I said these are two

00:16:51.390 --> 00:16:51.400
state so like I said these are two
 

00:16:51.400 --> 00:16:53.430
state so like I said these are two
different approaches for reinforcement

00:16:53.430 --> 00:16:53.440
different approaches for reinforcement
 

00:16:53.440 --> 00:16:55.710
different approaches for reinforcement
learning two main approaches and the

00:16:55.710 --> 00:16:55.720
learning two main approaches and the
 

00:16:55.720 --> 00:16:57.030
learning two main approaches and the
first part of the class will focus on

00:16:57.030 --> 00:16:57.040
first part of the class will focus on
 

00:16:57.040 --> 00:16:58.530
first part of the class will focus on
value learning and then we'll come back

00:16:58.530 --> 00:16:58.540
value learning and then we'll come back
 

00:16:58.540 --> 00:17:00.540
value learning and then we'll come back
to policy learning as a more general

00:17:00.540 --> 00:17:00.550
to policy learning as a more general
 

00:17:00.550 --> 00:17:02.490
to policy learning as a more general
framework and more powerful framework

00:17:02.490 --> 00:17:02.500
framework and more powerful framework
 

00:17:02.500 --> 00:17:04.410
framework and more powerful framework
we'll see that actually this is what

00:17:04.410 --> 00:17:04.420
we'll see that actually this is what
 

00:17:04.420 --> 00:17:06.780
we'll see that actually this is what
alphago uses policy learning is what

00:17:06.780 --> 00:17:06.790
alphago uses policy learning is what
 

00:17:06.790 --> 00:17:09.090
alphago uses policy learning is what
alphago uses and that's kind of what

00:17:09.090 --> 00:17:09.100
alphago uses and that's kind of what
 

00:17:09.100 --> 00:17:12.919
alphago uses and that's kind of what
we'll end on and touch on how that works

00:17:12.919 --> 00:17:12.929
 

00:17:12.929 --> 00:17:16.530
so before we get there let's keep going

00:17:16.530 --> 00:17:16.540
so before we get there let's keep going
 

00:17:16.540 --> 00:17:19.410
so before we get there let's keep going
digger let's keep going deeper into the

00:17:19.410 --> 00:17:19.420
digger let's keep going deeper into the
 

00:17:19.420 --> 00:17:22.559
digger let's keep going deeper into the
Q function so here's an example game

00:17:22.559 --> 00:17:22.569
Q function so here's an example game
 

00:17:22.569 --> 00:17:24.449
Q function so here's an example game
that we'll consider this is the atari

00:17:24.449 --> 00:17:24.459
that we'll consider this is the atari
 

00:17:24.459 --> 00:17:26.760
that we'll consider this is the atari
breakout game and the way it works is

00:17:26.760 --> 00:17:26.770
breakout game and the way it works is
 

00:17:26.770 --> 00:17:28.380
breakout game and the way it works is
you're this agent you're this little

00:17:28.380 --> 00:17:28.390
you're this agent you're this little
 

00:17:28.390 --> 00:17:31.290
you're this agent you're this little
pedal on the bottom and you can either

00:17:31.290 --> 00:17:31.300
pedal on the bottom and you can either
 

00:17:31.300 --> 00:17:33.299
pedal on the bottom and you can either
choose to move left or right in the

00:17:33.299 --> 00:17:33.309
choose to move left or right in the
 

00:17:33.309 --> 00:17:39.270
choose to move left or right in the
world at any given frame and there is

00:17:39.270 --> 00:17:39.280
world at any given frame and there is
 

00:17:39.280 --> 00:17:40.770
world at any given frame and there is
this ball also in the world that's

00:17:40.770 --> 00:17:40.780
this ball also in the world that's
 

00:17:40.780 --> 00:17:42.630
this ball also in the world that's
coming either towards you or away from

00:17:42.630 --> 00:17:42.640
coming either towards you or away from
 

00:17:42.640 --> 00:17:45.060
coming either towards you or away from
you your job as the agent is to move

00:17:45.060 --> 00:17:45.070
you your job as the agent is to move
 

00:17:45.070 --> 00:17:45.390
you your job as the agent is to move
your

00:17:45.390 --> 00:17:45.400
your
 

00:17:45.400 --> 00:17:47.780
your
idle left and right to hit that ball and

00:17:47.780 --> 00:17:47.790
idle left and right to hit that ball and
 

00:17:47.790 --> 00:17:50.940
idle left and right to hit that ball and
reflect it so that you can try to knock

00:17:50.940 --> 00:17:50.950
reflect it so that you can try to knock
 

00:17:50.950 --> 00:17:52.650
reflect it so that you can try to knock
off a lot of these blocks on the top

00:17:52.650 --> 00:17:52.660
off a lot of these blocks on the top
 

00:17:52.660 --> 00:17:55.530
off a lot of these blocks on the top
part of the screen every time you hit a

00:17:55.530 --> 00:17:55.540
part of the screen every time you hit a
 

00:17:55.540 --> 00:17:56.940
part of the screen every time you hit a
block on the top part of the screen you

00:17:56.940 --> 00:17:56.950
block on the top part of the screen you
 

00:17:56.950 --> 00:17:58.890
block on the top part of the screen you
get a reward if you don't hit a block

00:17:58.890 --> 00:17:58.900
get a reward if you don't hit a block
 

00:17:58.900 --> 00:18:01.049
get a reward if you don't hit a block
you don't get a reward and if that ball

00:18:01.049 --> 00:18:01.059
you don't get a reward and if that ball
 

00:18:01.059 --> 00:18:03.960
you don't get a reward and if that ball
passes your pedal without you hitting it

00:18:03.960 --> 00:18:03.970
passes your pedal without you hitting it
 

00:18:03.970 --> 00:18:07.500
passes your pedal without you hitting it
you lose the game so your goal is to

00:18:07.500 --> 00:18:07.510
you lose the game so your goal is to
 

00:18:07.510 --> 00:18:09.630
you lose the game so your goal is to
keep hitting that ball back onto the top

00:18:09.630 --> 00:18:09.640
keep hitting that ball back onto the top
 

00:18:09.640 --> 00:18:12.450
keep hitting that ball back onto the top
of the board and breaking off as many as

00:18:12.450 --> 00:18:12.460
of the board and breaking off as many as
 

00:18:12.460 --> 00:18:14.820
of the board and breaking off as many as
many of these colored blocks as possible

00:18:14.820 --> 00:18:14.830
many of these colored blocks as possible
 

00:18:14.830 --> 00:18:18.440
many of these colored blocks as possible
each time getting a brand new reward and

00:18:18.440 --> 00:18:18.450
each time getting a brand new reward and
 

00:18:18.450 --> 00:18:21.590
each time getting a brand new reward and
the point I want to make here is that

00:18:21.590 --> 00:18:21.600
the point I want to make here is that
 

00:18:21.600 --> 00:18:24.600
the point I want to make here is that
understanding queue functions or

00:18:24.600 --> 00:18:24.610
understanding queue functions or
 

00:18:24.610 --> 00:18:26.760
understanding queue functions or
understanding optimal queue values is

00:18:26.760 --> 00:18:26.770
understanding optimal queue values is
 

00:18:26.770 --> 00:18:29.070
understanding optimal queue values is
actually a really tough problem and if I

00:18:29.070 --> 00:18:29.080
actually a really tough problem and if I
 

00:18:29.080 --> 00:18:31.680
actually a really tough problem and if I
show you two possible example states and

00:18:31.680 --> 00:18:31.690
show you two possible example states and
 

00:18:31.690 --> 00:18:35.669
show you two possible example states and
actions that an agent could take so for

00:18:35.669 --> 00:18:35.679
actions that an agent could take so for
 

00:18:35.679 --> 00:18:40.260
actions that an agent could take so for
example here's it makes a pedal with a

00:18:40.260 --> 00:18:40.270
example here's it makes a pedal with a
 

00:18:40.270 --> 00:18:42.299
example here's it makes a pedal with a
ball coming straight for the paddle down

00:18:42.299 --> 00:18:42.309
ball coming straight for the paddle down
 

00:18:42.309 --> 00:18:44.340
ball coming straight for the paddle down
the agent can choose to stay where it is

00:18:44.340 --> 00:18:44.350
the agent can choose to stay where it is
 

00:18:44.350 --> 00:18:46.590
the agent can choose to stay where it is
and basically just deflect that ball

00:18:46.590 --> 00:18:46.600
and basically just deflect that ball
 

00:18:46.600 --> 00:18:49.350
and basically just deflect that ball
straight back up that's one possible

00:18:49.350 --> 00:18:49.360
straight back up that's one possible
 

00:18:49.360 --> 00:18:52.440
straight back up that's one possible
state action pair another possible state

00:18:52.440 --> 00:18:52.450
state action pair another possible state
 

00:18:52.450 --> 00:18:54.060
state action pair another possible state
action pair is where the ball is coming

00:18:54.060 --> 00:18:54.070
action pair is where the ball is coming
 

00:18:54.070 --> 00:18:56.340
action pair is where the ball is coming
slightly at an angle towards the pedal

00:18:56.340 --> 00:18:56.350
slightly at an angle towards the pedal
 

00:18:56.350 --> 00:18:58.169
slightly at an angle towards the pedal
the paddle can move slightly to the

00:18:58.169 --> 00:18:58.179
the paddle can move slightly to the
 

00:18:58.179 --> 00:19:01.320
the paddle can move slightly to the
right hit that ball at an angle and just

00:19:01.320 --> 00:19:01.330
right hit that ball at an angle and just
 

00:19:01.330 --> 00:19:03.240
right hit that ball at an angle and just
barely just barely knick it and send it

00:19:03.240 --> 00:19:03.250
barely just barely knick it and send it
 

00:19:03.250 --> 00:19:05.640
barely just barely knick it and send it
ricocheting off into the screen and to

00:19:05.640 --> 00:19:05.650
ricocheting off into the screen and to
 

00:19:05.650 --> 00:19:06.480
ricocheting off into the screen and to
the side of the screen

00:19:06.480 --> 00:19:06.490
the side of the screen
 

00:19:06.490 --> 00:19:09.110
the side of the screen
rather now the question I have here is

00:19:09.110 --> 00:19:09.120
rather now the question I have here is
 

00:19:09.120 --> 00:19:13.980
rather now the question I have here is
as a human which action state action

00:19:13.980 --> 00:19:13.990
as a human which action state action
 

00:19:13.990 --> 00:19:15.930
as a human which action state action
pair do you think is more desirable to

00:19:15.930 --> 00:19:15.940
pair do you think is more desirable to
 

00:19:15.940 --> 00:19:19.169
pair do you think is more desirable to
be in in this game which of you think

00:19:19.169 --> 00:19:19.179
be in in this game which of you think
 

00:19:19.179 --> 00:19:26.370
be in in this game which of you think
it's a okay how about be interesting so

00:19:26.370 --> 00:19:26.380
it's a okay how about be interesting so
 

00:19:26.380 --> 00:19:28.560
it's a okay how about be interesting so
actually you guys are much smarter than

00:19:28.560 --> 00:19:28.570
actually you guys are much smarter than
 

00:19:28.570 --> 00:19:30.810
actually you guys are much smarter than
I anticipated or maybe you're just

00:19:30.810 --> 00:19:30.820
I anticipated or maybe you're just
 

00:19:30.820 --> 00:19:32.669
I anticipated or maybe you're just
looking at the notes because the correct

00:19:32.669 --> 00:19:32.679
looking at the notes because the correct
 

00:19:32.679 --> 00:19:34.890
looking at the notes because the correct
answer is B even in a slightly

00:19:34.890 --> 00:19:34.900
answer is B even in a slightly
 

00:19:34.900 --> 00:19:36.990
answer is B even in a slightly
stochastic setting let's suppose you

00:19:36.990 --> 00:19:37.000
stochastic setting let's suppose you
 

00:19:37.000 --> 00:19:38.669
stochastic setting let's suppose you
keep executing a and you keep hitting

00:19:38.669 --> 00:19:38.679
keep executing a and you keep hitting
 

00:19:38.679 --> 00:19:40.650
keep executing a and you keep hitting
off these blocks in the middle of the

00:19:40.650 --> 00:19:40.660
off these blocks in the middle of the
 

00:19:40.660 --> 00:19:42.630
off these blocks in the middle of the
screen you're kind of having a limited

00:19:42.630 --> 00:19:42.640
screen you're kind of having a limited
 

00:19:42.640 --> 00:19:44.520
screen you're kind of having a limited
approach because every block that you

00:19:44.520 --> 00:19:44.530
approach because every block that you
 

00:19:44.530 --> 00:19:46.710
approach because every block that you
knock off has to be this one that you

00:19:46.710 --> 00:19:46.720
knock off has to be this one that you
 

00:19:46.720 --> 00:19:49.470
knock off has to be this one that you
explicitly aim towards and hit so here's

00:19:49.470 --> 00:19:49.480
explicitly aim towards and hit so here's
 

00:19:49.480 --> 00:19:51.180
explicitly aim towards and hit so here's
an example of a policy executing

00:19:51.180 --> 00:19:51.190
an example of a policy executing
 

00:19:51.190 --> 00:19:53.010
an example of a policy executing
something like a and it's hitting a lot

00:19:53.010 --> 00:19:53.020
something like a and it's hitting a lot
 

00:19:53.020 --> 00:19:54.210
something like a and it's hitting a lot
of the blocks in the center of the

00:19:54.210 --> 00:19:54.220
of the blocks in the center of the
 

00:19:54.220 --> 00:19:56.250
of the blocks in the center of the
screen not really hitting things on the

00:19:56.250 --> 00:19:56.260
screen not really hitting things on the
 

00:19:56.260 --> 00:19:58.080
screen not really hitting things on the
side of the screen very often even

00:19:58.080 --> 00:19:58.090
side of the screen very often even
 

00:19:58.090 --> 00:19:59.280
side of the screen very often even
though it's not going directly

00:19:59.280 --> 00:19:59.290
though it's not going directly
 

00:19:59.290 --> 00:20:01.290
though it's not going directly
up and down it is targeting the center

00:20:01.290 --> 00:20:01.300
up and down it is targeting the center
 

00:20:01.300 --> 00:20:05.130
up and down it is targeting the center
more than the side okay now I want to

00:20:05.130 --> 00:20:05.140
more than the side okay now I want to
 

00:20:05.140 --> 00:20:07.710
more than the side okay now I want to
show you an alternative policy now it's

00:20:07.710 --> 00:20:07.720
show you an alternative policy now it's
 

00:20:07.720 --> 00:20:10.950
show you an alternative policy now it's
B that's explicitly trying to hit the

00:20:10.950 --> 00:20:10.960
B that's explicitly trying to hit the
 

00:20:10.960 --> 00:20:12.960
B that's explicitly trying to hit the
side of the paddle every time no matter

00:20:12.960 --> 00:20:12.970
side of the paddle every time no matter
 

00:20:12.970 --> 00:20:14.610
side of the paddle every time no matter
where the ball is is trying to move away

00:20:14.610 --> 00:20:14.620
where the ball is is trying to move away
 

00:20:14.620 --> 00:20:16.650
where the ball is is trying to move away
from the ball and then come back towards

00:20:16.650 --> 00:20:16.660
from the ball and then come back towards
 

00:20:16.660 --> 00:20:18.840
from the ball and then come back towards
it so it hits the side and just barely

00:20:18.840 --> 00:20:18.850
it so it hits the side and just barely
 

00:20:18.850 --> 00:20:20.370
it so it hits the side and just barely
hits the ball so I can send it

00:20:20.370 --> 00:20:20.380
hits the ball so I can send it
 

00:20:20.380 --> 00:20:22.860
hits the ball so I can send it
ricocheting off into the corner of the

00:20:22.860 --> 00:20:22.870
ricocheting off into the corner of the
 

00:20:22.870 --> 00:20:24.840
ricocheting off into the corner of the
screen and what you're gonna see is it's

00:20:24.840 --> 00:20:24.850
screen and what you're gonna see is it's
 

00:20:24.850 --> 00:20:26.820
screen and what you're gonna see is it's
gonna basically be trying to create

00:20:26.820 --> 00:20:26.830
gonna basically be trying to create
 

00:20:26.830 --> 00:20:29.160
gonna basically be trying to create
these gaps in the corner of the screen

00:20:29.160 --> 00:20:29.170
these gaps in the corner of the screen
 

00:20:29.170 --> 00:20:31.440
these gaps in the corner of the screen
so on both left and right side so that

00:20:31.440 --> 00:20:31.450
so on both left and right side so that
 

00:20:31.450 --> 00:20:33.750
so on both left and right side so that
the ball can get stuck in that gap and

00:20:33.750 --> 00:20:33.760
the ball can get stuck in that gap and
 

00:20:33.760 --> 00:20:35.760
the ball can get stuck in that gap and
then start killing off a whole bunch of

00:20:35.760 --> 00:20:35.770
then start killing off a whole bunch of
 

00:20:35.770 --> 00:20:37.470
then start killing off a whole bunch of
different blocks with one single action

00:20:37.470 --> 00:20:37.480
different blocks with one single action
 

00:20:37.480 --> 00:20:40.560
different blocks with one single action
so here's an example it's gonna be

00:20:40.560 --> 00:20:40.570
so here's an example it's gonna be
 

00:20:40.570 --> 00:20:42.510
so here's an example it's gonna be
trying to kill off those side blocks now

00:20:42.510 --> 00:20:42.520
trying to kill off those side blocks now
 

00:20:42.520 --> 00:20:45.060
trying to kill off those side blocks now
it's going for the left side and once it

00:20:45.060 --> 00:20:45.070
it's going for the left side and once it
 

00:20:45.070 --> 00:20:46.590
it's going for the left side and once it
breaks open now you can see it just

00:20:46.590 --> 00:20:46.600
breaks open now you can see it just
 

00:20:46.600 --> 00:20:49.020
breaks open now you can see it just
starts dominating the game because it's

00:20:49.020 --> 00:20:49.030
starts dominating the game because it's
 

00:20:49.030 --> 00:20:51.330
starts dominating the game because it's
ball is just getting stuck on that top

00:20:51.330 --> 00:20:51.340
ball is just getting stuck on that top
 

00:20:51.340 --> 00:20:53.010
ball is just getting stuck on that top
platform and it's able to succeed much

00:20:53.010 --> 00:20:53.020
platform and it's able to succeed much
 

00:20:53.020 --> 00:20:55.320
platform and it's able to succeed much
faster than the first than the first

00:20:55.320 --> 00:20:55.330
faster than the first than the first
 

00:20:55.330 --> 00:21:00.510
faster than the first than the first
agent in agent a so to me at least this

00:21:00.510 --> 00:21:00.520
agent in agent a so to me at least this
 

00:21:00.520 --> 00:21:02.850
agent in agent a so to me at least this
was not an intuitive action or an

00:21:02.850 --> 00:21:02.860
was not an intuitive action or an
 

00:21:02.860 --> 00:21:05.940
was not an intuitive action or an
intuitive Q value to learn and for me I

00:21:05.940 --> 00:21:05.950
intuitive Q value to learn and for me I
 

00:21:05.950 --> 00:21:07.320
intuitive Q value to learn and for me I
would have assumed that the safest

00:21:07.320 --> 00:21:07.330
would have assumed that the safest
 

00:21:07.330 --> 00:21:09.570
would have assumed that the safest
action to take was actually a but

00:21:09.570 --> 00:21:09.580
action to take was actually a but
 

00:21:09.580 --> 00:21:11.430
action to take was actually a but
through reinforcement learning we can

00:21:11.430 --> 00:21:11.440
through reinforcement learning we can
 

00:21:11.440 --> 00:21:13.800
through reinforcement learning we can
learn more optimal actions than what

00:21:13.800 --> 00:21:13.810
learn more optimal actions than what
 

00:21:13.810 --> 00:21:16.170
learn more optimal actions than what
might be immediately apparent to human

00:21:16.170 --> 00:21:16.180
might be immediately apparent to human
 

00:21:16.180 --> 00:21:20.250
might be immediately apparent to human
operators so now let's bring this back

00:21:20.250 --> 00:21:20.260
operators so now let's bring this back
 

00:21:20.260 --> 00:21:22.860
operators so now let's bring this back
to the context of deep learning and find

00:21:22.860 --> 00:21:22.870
to the context of deep learning and find
 

00:21:22.870 --> 00:21:25.470
to the context of deep learning and find
out how we can use deep learning to

00:21:25.470 --> 00:21:25.480
out how we can use deep learning to
 

00:21:25.480 --> 00:21:28.170
out how we can use deep learning to
actually model Q functions and estimate

00:21:28.170 --> 00:21:28.180
actually model Q functions and estimate
 

00:21:28.180 --> 00:21:32.130
actually model Q functions and estimate
Q functions using training data and we

00:21:32.130 --> 00:21:32.140
Q functions using training data and we
 

00:21:32.140 --> 00:21:33.960
Q functions using training data and we
can do this in one of two ways so the

00:21:33.960 --> 00:21:33.970
can do this in one of two ways so the
 

00:21:33.970 --> 00:21:35.910
can do this in one of two ways so the
primary way or the primary model that's

00:21:35.910 --> 00:21:35.920
primary way or the primary model that's
 

00:21:35.920 --> 00:21:39.060
primary way or the primary model that's
used is called a deep Q Network and this

00:21:39.060 --> 00:21:39.070
used is called a deep Q Network and this
 

00:21:39.070 --> 00:21:40.500
used is called a deep Q Network and this
is essentially like I said a model

00:21:40.500 --> 00:21:40.510
is essentially like I said a model
 

00:21:40.510 --> 00:21:43.190
is essentially like I said a model
that's trying to estimate a Q function

00:21:43.190 --> 00:21:43.200
that's trying to estimate a Q function
 

00:21:43.200 --> 00:21:45.780
that's trying to estimate a Q function
so in this first in this first model

00:21:45.780 --> 00:21:45.790
so in this first in this first model
 

00:21:45.790 --> 00:21:47.520
so in this first in this first model
that I'm showing here it takes as input

00:21:47.520 --> 00:21:47.530
that I'm showing here it takes as input
 

00:21:47.530 --> 00:21:49.800
that I'm showing here it takes as input
a state and a possible action that you

00:21:49.800 --> 00:21:49.810
a state and a possible action that you
 

00:21:49.810 --> 00:21:52.050
a state and a possible action that you
could execute at that state and the

00:21:52.050 --> 00:21:52.060
could execute at that state and the
 

00:21:52.060 --> 00:21:54.510
could execute at that state and the
output is just the Q value it's just a

00:21:54.510 --> 00:21:54.520
output is just the Q value it's just a
 

00:21:54.520 --> 00:21:56.970
output is just the Q value it's just a
scale or output and it's the neural

00:21:56.970 --> 00:21:56.980
scale or output and it's the neural
 

00:21:56.980 --> 00:21:59.010
scale or output and it's the neural
network is basically predicting what is

00:21:59.010 --> 00:21:59.020
network is basically predicting what is
 

00:21:59.020 --> 00:22:02.340
network is basically predicting what is
the estimated expected total reward that

00:22:02.340 --> 00:22:02.350
the estimated expected total reward that
 

00:22:02.350 --> 00:22:04.320
the estimated expected total reward that
it can obtain given the state and this

00:22:04.320 --> 00:22:04.330
it can obtain given the state and this
 

00:22:04.330 --> 00:22:06.000
it can obtain given the state and this
action then you want to train this

00:22:06.000 --> 00:22:06.010
action then you want to train this
 

00:22:06.010 --> 00:22:08.970
action then you want to train this
network using mean squared error to

00:22:08.970 --> 00:22:08.980
network using mean squared error to
 

00:22:08.980 --> 00:22:10.590
network using mean squared error to
produce the right answer given a lot of

00:22:10.590 --> 00:22:10.600
produce the right answer given a lot of
 

00:22:10.600 --> 00:22:12.150
produce the right answer given a lot of
training data at a high level that's

00:22:12.150 --> 00:22:12.160
training data at a high level that's
 

00:22:12.160 --> 00:22:13.320
training data at a high level that's
what's going on

00:22:13.320 --> 00:22:13.330
what's going on
 

00:22:13.330 --> 00:22:15.700
what's going on
the problem with this approach is that

00:22:15.700 --> 00:22:15.710
the problem with this approach is that
 

00:22:15.710 --> 00:22:18.100
the problem with this approach is that
if we want to use our policy now and we

00:22:18.100 --> 00:22:18.110
if we want to use our policy now and we
 

00:22:18.110 --> 00:22:20.650
if we want to use our policy now and we
want our agent to act in this world we

00:22:20.650 --> 00:22:20.660
want our agent to act in this world we
 

00:22:20.660 --> 00:22:23.080
want our agent to act in this world we
have to feed through the network a whole

00:22:23.080 --> 00:22:23.090
have to feed through the network a whole
 

00:22:23.090 --> 00:22:24.910
have to feed through the network a whole
bunch of different actions at every time

00:22:24.910 --> 00:22:24.920
bunch of different actions at every time
 

00:22:24.920 --> 00:22:26.710
bunch of different actions at every time
step to find the optimal queue value

00:22:26.710 --> 00:22:26.720
step to find the optimal queue value
 

00:22:26.720 --> 00:22:29.410
step to find the optimal queue value
right so we have to for every possible

00:22:29.410 --> 00:22:29.420
right so we have to for every possible
 

00:22:29.420 --> 00:22:31.210
right so we have to for every possible
action imagine we have a ton of actions

00:22:31.210 --> 00:22:31.220
action imagine we have a ton of actions
 

00:22:31.220 --> 00:22:33.040
action imagine we have a ton of actions
in the previous example it's simple

00:22:33.040 --> 00:22:33.050
in the previous example it's simple
 

00:22:33.050 --> 00:22:34.450
in the previous example it's simple
because we only have left and right as

00:22:34.450 --> 00:22:34.460
because we only have left and right as
 

00:22:34.460 --> 00:22:36.220
because we only have left and right as
possible actions but let's suppose we

00:22:36.220 --> 00:22:36.230
possible actions but let's suppose we
 

00:22:36.230 --> 00:22:38.140
possible actions but let's suppose we
have a ton of different actions for each

00:22:38.140 --> 00:22:38.150
have a ton of different actions for each
 

00:22:38.150 --> 00:22:40.060
have a ton of different actions for each
action we have to feed in the state and

00:22:40.060 --> 00:22:40.070
action we have to feed in the state and
 

00:22:40.070 --> 00:22:44.170
action we have to feed in the state and
that action compute the Q value do that

00:22:44.170 --> 00:22:44.180
that action compute the Q value do that
 

00:22:44.180 --> 00:22:45.700
that action compute the Q value do that
for all actions now we have a whole

00:22:45.700 --> 00:22:45.710
for all actions now we have a whole
 

00:22:45.710 --> 00:22:48.580
for all actions now we have a whole
bunch of Q values we take the maximum

00:22:48.580 --> 00:22:48.590
bunch of Q values we take the maximum
 

00:22:48.590 --> 00:22:53.260
bunch of Q values we take the maximum
and use that action to act okay that's

00:22:53.260 --> 00:22:53.270
and use that action to act okay that's
 

00:22:53.270 --> 00:22:55.240
and use that action to act okay that's
not great because it requires executing

00:22:55.240 --> 00:22:55.250
not great because it requires executing
 

00:22:55.250 --> 00:22:58.330
not great because it requires executing
this network in a forward pass a total

00:22:58.330 --> 00:22:58.340
this network in a forward pass a total
 

00:22:58.340 --> 00:22:59.920
this network in a forward pass a total
number of times that's equal to the

00:22:59.920 --> 00:22:59.930
number of times that's equal to the
 

00:22:59.930 --> 00:23:01.750
number of times that's equal to the
total number of actions that the agent

00:23:01.750 --> 00:23:01.760
total number of actions that the agent
 

00:23:01.760 --> 00:23:04.150
total number of actions that the agent
could take at that step another

00:23:04.150 --> 00:23:04.160
could take at that step another
 

00:23:04.160 --> 00:23:06.730
could take at that step another
alternative is slightly reaper amat

00:23:06.730 --> 00:23:06.740
alternative is slightly reaper amat
 

00:23:06.740 --> 00:23:09.010
alternative is slightly reaper amat
rising this problem still learning the Q

00:23:09.010 --> 00:23:09.020
rising this problem still learning the Q
 

00:23:09.020 --> 00:23:11.350
rising this problem still learning the Q
value but now we input just the state

00:23:11.350 --> 00:23:11.360
value but now we input just the state
 

00:23:11.360 --> 00:23:13.870
value but now we input just the state
and the network intrinsically will

00:23:13.870 --> 00:23:13.880
and the network intrinsically will
 

00:23:13.880 --> 00:23:15.970
and the network intrinsically will
compute the Q value for each of the

00:23:15.970 --> 00:23:15.980
compute the Q value for each of the
 

00:23:15.980 --> 00:23:18.430
compute the Q value for each of the
possible actions and since your action

00:23:18.430 --> 00:23:18.440
possible actions and since your action
 

00:23:18.440 --> 00:23:20.350
possible actions and since your action
space is fixed and reinforcement

00:23:20.350 --> 00:23:20.360
space is fixed and reinforcement
 

00:23:20.360 --> 00:23:23.590
space is fixed and reinforcement
learning in a lot of cases your output

00:23:23.590 --> 00:23:23.600
learning in a lot of cases your output
 

00:23:23.600 --> 00:23:25.450
learning in a lot of cases your output
of the network is also fixed which means

00:23:25.450 --> 00:23:25.460
of the network is also fixed which means
 

00:23:25.460 --> 00:23:27.910
of the network is also fixed which means
that it each time you input a state and

00:23:27.910 --> 00:23:27.920
that it each time you input a state and
 

00:23:27.920 --> 00:23:29.700
that it each time you input a state and
the network is basically outputting and

00:23:29.700 --> 00:23:29.710
the network is basically outputting and
 

00:23:29.710 --> 00:23:32.680
the network is basically outputting and
numbers where n is the dimensionality of

00:23:32.680 --> 00:23:32.690
numbers where n is the dimensionality of
 

00:23:32.690 --> 00:23:35.170
numbers where n is the dimensionality of
your action space where each output

00:23:35.170 --> 00:23:35.180
your action space where each output
 

00:23:35.180 --> 00:23:37.690
your action space where each output
corresponds to the Q value of executing

00:23:37.690 --> 00:23:37.700
corresponds to the Q value of executing
 

00:23:37.700 --> 00:23:40.660
corresponds to the Q value of executing
that action now this is great because it

00:23:40.660 --> 00:23:40.670
that action now this is great because it
 

00:23:40.670 --> 00:23:43.480
that action now this is great because it
means if we want to take an action given

00:23:43.480 --> 00:23:43.490
means if we want to take an action given
 

00:23:43.490 --> 00:23:45.490
means if we want to take an action given
a state we simply feed in our state to

00:23:45.490 --> 00:23:45.500
a state we simply feed in our state to
 

00:23:45.500 --> 00:23:47.980
a state we simply feed in our state to
the network it gives us back all these Q

00:23:47.980 --> 00:23:47.990
the network it gives us back all these Q
 

00:23:47.990 --> 00:23:50.710
the network it gives us back all these Q
values we pick the maximum Q value and

00:23:50.710 --> 00:23:50.720
values we pick the maximum Q value and
 

00:23:50.720 --> 00:23:52.450
values we pick the maximum Q value and
we use the wrote we use the action

00:23:52.450 --> 00:23:52.460
we use the wrote we use the action
 

00:23:52.460 --> 00:23:56.550
we use the wrote we use the action
associated to that maximum Q value in

00:23:56.550 --> 00:23:56.560
associated to that maximum Q value in
 

00:23:56.560 --> 00:23:59.380
associated to that maximum Q value in
both of these cases however we can

00:23:59.380 --> 00:23:59.390
both of these cases however we can
 

00:23:59.390 --> 00:24:02.050
both of these cases however we can
actually train using mean squared error

00:24:02.050 --> 00:24:02.060
actually train using mean squared error
 

00:24:02.060 --> 00:24:04.450
actually train using mean squared error
it's a fancy term a fancy version of

00:24:04.450 --> 00:24:04.460
it's a fancy term a fancy version of
 

00:24:04.460 --> 00:24:05.530
it's a fancy term a fancy version of
mean squared error that I'll just

00:24:05.530 --> 00:24:05.540
mean squared error that I'll just
 

00:24:05.540 --> 00:24:08.440
mean squared error that I'll just
quickly walk through so the right side

00:24:08.440 --> 00:24:08.450
quickly walk through so the right side
 

00:24:08.450 --> 00:24:11.170
quickly walk through so the right side
is the predicted Q value that's actually

00:24:11.170 --> 00:24:11.180
is the predicted Q value that's actually
 

00:24:11.180 --> 00:24:13.630
is the predicted Q value that's actually
the output of the neural network just to

00:24:13.630 --> 00:24:13.640
the output of the neural network just to
 

00:24:13.640 --> 00:24:15.460
the output of the neural network just to
reiterate this takes as input the state

00:24:15.460 --> 00:24:15.470
reiterate this takes as input the state
 

00:24:15.470 --> 00:24:17.260
reiterate this takes as input the state
and the action and this is what the

00:24:17.260 --> 00:24:17.270
and the action and this is what the
 

00:24:17.270 --> 00:24:19.960
and the action and this is what the
network predicts you then want to

00:24:19.960 --> 00:24:19.970
network predicts you then want to
 

00:24:19.970 --> 00:24:22.900
network predicts you then want to
minimize the error of that predicted Q

00:24:22.900 --> 00:24:22.910
minimize the error of that predicted Q
 

00:24:22.910 --> 00:24:25.390
minimize the error of that predicted Q
value compared to the true or the target

00:24:25.390 --> 00:24:25.400
value compared to the true or the target
 

00:24:25.400 --> 00:24:26.430
value compared to the true or the target
Q value

00:24:26.430 --> 00:24:26.440
Q value
 

00:24:26.440 --> 00:24:28.110
Q value
which in this case is on the right hand

00:24:28.110 --> 00:24:28.120
which in this case is on the right hand
 

00:24:28.120 --> 00:24:32.100
which in this case is on the right hand
side so the target Q value is what you

00:24:32.100 --> 00:24:32.110
side so the target Q value is what you
 

00:24:32.110 --> 00:24:33.810
side so the target Q value is what you
actually observed when you took that

00:24:33.810 --> 00:24:33.820
actually observed when you took that
 

00:24:33.820 --> 00:24:36.180
actually observed when you took that
action so when the agent takes an action

00:24:36.180 --> 00:24:36.190
action so when the agent takes an action
 

00:24:36.190 --> 00:24:38.220
action so when the agent takes an action
it gets a reward that you can just

00:24:38.220 --> 00:24:38.230
it gets a reward that you can just
 

00:24:38.230 --> 00:24:40.800
it gets a reward that you can just
record you store it in memory and you

00:24:40.800 --> 00:24:40.810
record you store it in memory and you
 

00:24:40.810 --> 00:24:43.080
record you store it in memory and you
can also record the discounted reward

00:24:43.080 --> 00:24:43.090
can also record the discounted reward
 

00:24:43.090 --> 00:24:45.090
can also record the discounted reward
that it receives in every action after

00:24:45.090 --> 00:24:45.100
that it receives in every action after
 

00:24:45.100 --> 00:24:48.000
that it receives in every action after
that so that's the target return that's

00:24:48.000 --> 00:24:48.010
that so that's the target return that's
 

00:24:48.010 --> 00:24:50.010
that so that's the target return that's
what you know it that's what you know

00:24:50.010 --> 00:24:50.020
what you know it that's what you know
 

00:24:50.020 --> 00:24:52.350
what you know it that's what you know
the agent obtained that's a reward that

00:24:52.350 --> 00:24:52.360
the agent obtained that's a reward that
 

00:24:52.360 --> 00:24:54.930
the agent obtained that's a reward that
they obtained given that action and you

00:24:54.930 --> 00:24:54.940
they obtained given that action and you
 

00:24:54.940 --> 00:24:57.150
they obtained given that action and you
can use that to now have a regression

00:24:57.150 --> 00:24:57.160
can use that to now have a regression
 

00:24:57.160 --> 00:24:59.880
can use that to now have a regression
problem over the predicted Q values and

00:24:59.880 --> 00:24:59.890
problem over the predicted Q values and
 

00:24:59.890 --> 00:25:01.650
problem over the predicted Q values and
basically over time using back

00:25:01.650 --> 00:25:01.660
basically over time using back
 

00:25:01.660 --> 00:25:03.300
basically over time using back
propagation it's just a normal

00:25:03.300 --> 00:25:03.310
propagation it's just a normal
 

00:25:03.310 --> 00:25:05.490
propagation it's just a normal
feed-forward network we can train this

00:25:05.490 --> 00:25:05.500
feed-forward network we can train this
 

00:25:05.500 --> 00:25:07.290
feed-forward network we can train this
loss function train this network

00:25:07.290 --> 00:25:07.300
loss function train this network
 

00:25:07.300 --> 00:25:10.020
loss function train this network
according to this loss function to make

00:25:10.020 --> 00:25:10.030
according to this loss function to make
 

00:25:10.030 --> 00:25:11.730
according to this loss function to make
our predicted Q value as close as

00:25:11.730 --> 00:25:11.740
our predicted Q value as close as
 

00:25:11.740 --> 00:25:13.890
our predicted Q value as close as
possible to our desired or target Q

00:25:13.890 --> 00:25:13.900
possible to our desired or target Q
 

00:25:13.900 --> 00:25:18.480
possible to our desired or target Q
values and just to show you some

00:25:18.480 --> 00:25:18.490
values and just to show you some
 

00:25:18.490 --> 00:25:20.310
values and just to show you some
exciting results so when this first came

00:25:20.310 --> 00:25:20.320
exciting results so when this first came
 

00:25:20.320 --> 00:25:23.610
exciting results so when this first came
out paper by deep mind showed that it

00:25:23.610 --> 00:25:23.620
out paper by deep mind showed that it
 

00:25:23.620 --> 00:25:26.070
out paper by deep mind showed that it
could work in the context of Atari games

00:25:26.070 --> 00:25:26.080
could work in the context of Atari games
 

00:25:26.080 --> 00:25:27.660
could work in the context of Atari games
and they wanted to present this general

00:25:27.660 --> 00:25:27.670
and they wanted to present this general
 

00:25:27.670 --> 00:25:30.990
and they wanted to present this general
deep Q network this just learning

00:25:30.990 --> 00:25:31.000
deep Q network this just learning
 

00:25:31.000 --> 00:25:32.880
deep Q network this just learning
through deep Q networks where they input

00:25:32.880 --> 00:25:32.890
through deep Q networks where they input
 

00:25:32.890 --> 00:25:35.700
through deep Q networks where they input
the state of the game on the left-hand

00:25:35.700 --> 00:25:35.710
the state of the game on the left-hand
 

00:25:35.710 --> 00:25:37.110
the state of the game on the left-hand
side pass it through a series of

00:25:37.110 --> 00:25:37.120
side pass it through a series of
 

00:25:37.120 --> 00:25:38.670
side pass it through a series of
convolutional layers followed by

00:25:38.670 --> 00:25:38.680
convolutional layers followed by
 

00:25:38.680 --> 00:25:40.920
convolutional layers followed by
nonlinear activation functions like

00:25:40.920 --> 00:25:40.930
nonlinear activation functions like
 

00:25:40.930 --> 00:25:44.070
nonlinear activation functions like
rellis and propagating this information

00:25:44.070 --> 00:25:44.080
rellis and propagating this information
 

00:25:44.080 --> 00:25:45.900
rellis and propagating this information
forward each at each time using

00:25:45.900 --> 00:25:45.910
forward each at each time using
 

00:25:45.910 --> 00:25:48.420
forward each at each time using
convolutional layer activation function

00:25:48.420 --> 00:25:48.430
convolutional layer activation function
 

00:25:48.430 --> 00:25:50.130
convolutional layer activation function
fully connected layer activation

00:25:50.130 --> 00:25:50.140
fully connected layer activation
 

00:25:50.140 --> 00:25:51.720
fully connected layer activation
function and then finally at the output

00:25:51.720 --> 00:25:51.730
function and then finally at the output
 

00:25:51.730 --> 00:25:56.280
function and then finally at the output
we have a list of n Q values where each

00:25:56.280 --> 00:25:56.290
we have a list of n Q values where each
 

00:25:56.290 --> 00:25:58.350
we have a list of n Q values where each
Q value corresponds to the possible

00:25:58.350 --> 00:25:58.360
Q value corresponds to the possible
 

00:25:58.360 --> 00:25:59.910
Q value corresponds to the possible
action that it could take and this is

00:25:59.910 --> 00:25:59.920
action that it could take and this is
 

00:25:59.920 --> 00:26:01.350
action that it could take and this is
the exact same picture as I gave you

00:26:01.350 --> 00:26:01.360
the exact same picture as I gave you
 

00:26:01.360 --> 00:26:03.780
the exact same picture as I gave you
before except now it's just for a

00:26:03.780 --> 00:26:03.790
before except now it's just for a
 

00:26:03.790 --> 00:26:07.800
before except now it's just for a
specific game in Atari and one of the

00:26:07.800 --> 00:26:07.810
specific game in Atari and one of the
 

00:26:07.810 --> 00:26:09.360
specific game in Atari and one of the
remarkable things that they showed was

00:26:09.360 --> 00:26:09.370
remarkable things that they showed was
 

00:26:09.370 --> 00:26:12.450
remarkable things that they showed was
that this is an incredibly flexible

00:26:12.450 --> 00:26:12.460
that this is an incredibly flexible
 

00:26:12.460 --> 00:26:14.820
that this is an incredibly flexible
algorithm because without changing

00:26:14.820 --> 00:26:14.830
algorithm because without changing
 

00:26:14.830 --> 00:26:16.530
algorithm because without changing
anything about this algorithm but just

00:26:16.530 --> 00:26:16.540
anything about this algorithm but just
 

00:26:16.540 --> 00:26:18.540
anything about this algorithm but just
deploying it in many different types of

00:26:18.540 --> 00:26:18.550
deploying it in many different types of
 

00:26:18.550 --> 00:26:20.640
deploying it in many different types of
games you can get this network to

00:26:20.640 --> 00:26:20.650
games you can get this network to
 

00:26:20.650 --> 00:26:24.660
games you can get this network to
perform above human level on a lot of

00:26:24.660 --> 00:26:24.670
perform above human level on a lot of
 

00:26:24.670 --> 00:26:27.120
perform above human level on a lot of
different tasks in Atari so Atari is

00:26:27.120 --> 00:26:27.130
different tasks in Atari so Atari is
 

00:26:27.130 --> 00:26:28.620
different tasks in Atari so Atari is
composed of a whole bunch of games which

00:26:28.620 --> 00:26:28.630
composed of a whole bunch of games which
 

00:26:28.630 --> 00:26:31.200
composed of a whole bunch of games which
you can see on the x-axis so each bar

00:26:31.200 --> 00:26:31.210
you can see on the x-axis so each bar
 

00:26:31.210 --> 00:26:34.310
you can see on the x-axis so each bar
here is a different game in Atari and

00:26:34.310 --> 00:26:34.320
here is a different game in Atari and
 

00:26:34.320 --> 00:26:37.020
here is a different game in Atari and
things to the left of this vertical bar

00:26:37.020 --> 00:26:37.030
things to the left of this vertical bar
 

00:26:37.030 --> 00:26:40.320
things to the left of this vertical bar
correspond to situations where the

00:26:40.320 --> 00:26:40.330
correspond to situations where the
 

00:26:40.330 --> 00:26:42.180
correspond to situations where the
deep Q Network was able to outperform

00:26:42.180 --> 00:26:42.190
deep Q Network was able to outperform
 

00:26:42.190 --> 00:26:44.940
deep Q Network was able to outperform
the level of a human operator in that

00:26:44.940 --> 00:26:44.950
the level of a human operator in that
 

00:26:44.950 --> 00:26:45.450
the level of a human operator in that
game

00:26:45.450 --> 00:26:45.460
game
 

00:26:45.460 --> 00:26:47.730
game
there are definitely situations in an

00:26:47.730 --> 00:26:47.740
there are definitely situations in an
 

00:26:47.740 --> 00:26:50.519
there are definitely situations in an
Atari where that the DQ network was not

00:26:50.519 --> 00:26:50.529
Atari where that the DQ network was not
 

00:26:50.529 --> 00:26:52.320
Atari where that the DQ network was not
able to outperform human level and

00:26:52.320 --> 00:26:52.330
able to outperform human level and
 

00:26:52.330 --> 00:26:55.769
able to outperform human level and
typically what was noticed by a lot of

00:26:55.769 --> 00:26:55.779
typically what was noticed by a lot of
 

00:26:55.779 --> 00:26:58.310
typically what was noticed by a lot of
researchers afterwards was that in

00:26:58.310 --> 00:26:58.320
researchers afterwards was that in
 

00:26:58.320 --> 00:27:01.649
researchers afterwards was that in
situations or in games where we don't

00:27:01.649 --> 00:27:01.659
situations or in games where we don't
 

00:27:01.659 --> 00:27:05.330
situations or in games where we don't
have a perfectly observable world where

00:27:05.330 --> 00:27:05.340
have a perfectly observable world where
 

00:27:05.340 --> 00:27:08.210
have a perfectly observable world where
if I give you a state you can observe

00:27:08.210 --> 00:27:08.220
if I give you a state you can observe
 

00:27:08.220 --> 00:27:12.240
if I give you a state you can observe
the like you can optimally observe the

00:27:12.240 --> 00:27:12.250
the like you can optimally observe the
 

00:27:12.250 --> 00:27:16.139
the like you can optimally observe the
correct action to take in that state not

00:27:16.139 --> 00:27:16.149
correct action to take in that state not
 

00:27:16.149 --> 00:27:17.789
correct action to take in that state not
all of these games are having that

00:27:17.789 --> 00:27:17.799
all of these games are having that
 

00:27:17.799 --> 00:27:19.230
all of these games are having that
property and that's a very nice property

00:27:19.230 --> 00:27:19.240
property and that's a very nice property
 

00:27:19.240 --> 00:27:22.860
property and that's a very nice property
to have so a lot of games have very

00:27:22.860 --> 00:27:22.870
to have so a lot of games have very
 

00:27:22.870 --> 00:27:25.049
to have so a lot of games have very
sparse rewards for example this this

00:27:25.049 --> 00:27:25.059
sparse rewards for example this this
 

00:27:25.059 --> 00:27:27.259
sparse rewards for example this this
game on the end montezuma's revenge is

00:27:27.259 --> 00:27:27.269
game on the end montezuma's revenge is
 

00:27:27.269 --> 00:27:29.759
game on the end montezuma's revenge is
notorious for having extremely sparse

00:27:29.759 --> 00:27:29.769
notorious for having extremely sparse
 

00:27:29.769 --> 00:27:31.740
notorious for having extremely sparse
rewards because it requires that the

00:27:31.740 --> 00:27:31.750
rewards because it requires that the
 

00:27:31.750 --> 00:27:34.200
rewards because it requires that the
agent go through a ladder go to a

00:27:34.200 --> 00:27:34.210
agent go through a ladder go to a
 

00:27:34.210 --> 00:27:35.909
agent go through a ladder go to a
different room and collect a key and

00:27:35.909 --> 00:27:35.919
different room and collect a key and
 

00:27:35.919 --> 00:27:38.430
different room and collect a key and
then use that key to turn the knob or

00:27:38.430 --> 00:27:38.440
then use that key to turn the knob or
 

00:27:38.440 --> 00:27:40.909
then use that key to turn the knob or
something like that and without randomly

00:27:40.909 --> 00:27:40.919
something like that and without randomly
 

00:27:40.919 --> 00:27:43.350
something like that and without randomly
exploring or possibly seeing that

00:27:43.350 --> 00:27:43.360
exploring or possibly seeing that
 

00:27:43.360 --> 00:27:47.009
exploring or possibly seeing that
sequence of states the agent would never

00:27:47.009 --> 00:27:47.019
sequence of states the agent would never
 

00:27:47.019 --> 00:27:49.049
sequence of states the agent would never
get exposed to those cue values it could

00:27:49.049 --> 00:27:49.059
get exposed to those cue values it could
 

00:27:49.059 --> 00:27:50.610
get exposed to those cue values it could
never learn that this was an optimal

00:27:50.610 --> 00:27:50.620
never learn that this was an optimal
 

00:27:50.620 --> 00:27:52.500
never learn that this was an optimal
action to take if it just randomly

00:27:52.500 --> 00:27:52.510
action to take if it just randomly
 

00:27:52.510 --> 00:27:54.360
action to take if it just randomly
explores the environment it's never

00:27:54.360 --> 00:27:54.370
explores the environment it's never
 

00:27:54.370 --> 00:27:56.190
explores the environment it's never
going to actually see that possible

00:27:56.190 --> 00:27:56.200
going to actually see that possible
 

00:27:56.200 --> 00:27:58.110
going to actually see that possible
action in that case it's never ever

00:27:58.110 --> 00:27:58.120
action in that case it's never ever
 

00:27:58.120 --> 00:28:00.899
action in that case it's never ever
going to get any context of what the

00:28:00.899 --> 00:28:00.909
going to get any context of what the
 

00:28:00.909 --> 00:28:03.539
going to get any context of what the
optimal action to take is in the context

00:28:03.539 --> 00:28:03.549
optimal action to take is in the context
 

00:28:03.549 --> 00:28:04.889
optimal action to take is in the context
of breakout which I was showing you

00:28:04.889 --> 00:28:04.899
of breakout which I was showing you
 

00:28:04.899 --> 00:28:06.299
of breakout which I was showing you
before where you have that paddle and

00:28:06.299 --> 00:28:06.309
before where you have that paddle and
 

00:28:06.309 --> 00:28:08.039
before where you have that paddle and
the ball hitting the paddle and you're

00:28:08.039 --> 00:28:08.049
the ball hitting the paddle and you're
 

00:28:08.049 --> 00:28:09.450
the ball hitting the paddle and you're
trying to break off all of these points

00:28:09.450 --> 00:28:09.460
trying to break off all of these points
 

00:28:09.460 --> 00:28:11.700
trying to break off all of these points
on the top this is an example of a game

00:28:11.700 --> 00:28:11.710
on the top this is an example of a game
 

00:28:11.710 --> 00:28:13.950
on the top this is an example of a game
where we have perfect information so if

00:28:13.950 --> 00:28:13.960
where we have perfect information so if
 

00:28:13.960 --> 00:28:15.480
where we have perfect information so if
we know the direction of the ball we

00:28:15.480 --> 00:28:15.490
we know the direction of the ball we
 

00:28:15.490 --> 00:28:17.220
we know the direction of the ball we
know the direction of what the position

00:28:17.220 --> 00:28:17.230
know the direction of what the position
 

00:28:17.230 --> 00:28:19.110
know the direction of what the position
of our paddle and we see all of the

00:28:19.110 --> 00:28:19.120
of our paddle and we see all of the
 

00:28:19.120 --> 00:28:21.269
of our paddle and we see all of the
points in the in the space we can

00:28:21.269 --> 00:28:21.279
points in the in the space we can
 

00:28:21.279 --> 00:28:24.570
points in the in the space we can
correctly predict with like we we have

00:28:24.570 --> 00:28:24.580
correctly predict with like we we have
 

00:28:24.580 --> 00:28:26.700
correctly predict with like we we have
an optimal solution at ma given state

00:28:26.700 --> 00:28:26.710
an optimal solution at ma given state
 

00:28:26.710 --> 00:28:28.889
an optimal solution at ma given state
given what we see of where to move the

00:28:28.889 --> 00:28:28.899
given what we see of where to move the
 

00:28:28.899 --> 00:28:31.680
given what we see of where to move the
paddle that kind of summarizes our topic

00:28:31.680 --> 00:28:31.690
paddle that kind of summarizes our topic
 

00:28:31.690 --> 00:28:34.950
paddle that kind of summarizes our topic
of Q learning and I want to end on some

00:28:34.950 --> 00:28:34.960
of Q learning and I want to end on some
 

00:28:34.960 --> 00:28:37.200
of Q learning and I want to end on some
of the downsides of Q learning it

00:28:37.200 --> 00:28:37.210
of the downsides of Q learning it
 

00:28:37.210 --> 00:28:39.180
of the downsides of Q learning it
surpasses human level performance on a

00:28:39.180 --> 00:28:39.190
surpasses human level performance on a
 

00:28:39.190 --> 00:28:42.210
surpasses human level performance on a
lot of simpler tasks but it also has

00:28:42.210 --> 00:28:42.220
lot of simpler tasks but it also has
 

00:28:42.220 --> 00:28:44.820
lot of simpler tasks but it also has
trouble dealing with complexity it also

00:28:44.820 --> 00:28:44.830
trouble dealing with complexity it also
 

00:28:44.830 --> 00:28:46.860
trouble dealing with complexity it also
can't handle action spaces which are

00:28:46.860 --> 00:28:46.870
can't handle action spaces which are
 

00:28:46.870 --> 00:28:49.560
can't handle action spaces which are
continuous so if you think back to the

00:28:49.560 --> 00:28:49.570
continuous so if you think back to the
 

00:28:49.570 --> 00:28:51.960
continuous so if you think back to the
way we defined the Q Network the deep Q

00:28:51.960 --> 00:28:51.970
way we defined the Q Network the deep Q
 

00:28:51.970 --> 00:28:52.500
way we defined the Q Network the deep Q
Network

00:28:52.500 --> 00:28:52.510
Network
 

00:28:52.510 --> 00:28:54.070
Network
we're outputting a

00:28:54.070 --> 00:28:54.080
we're outputting a
 

00:28:54.080 --> 00:28:55.930
we're outputting a
q-value for each possible action it

00:28:55.930 --> 00:28:55.940
q-value for each possible action it
 

00:28:55.940 --> 00:28:57.370
q-value for each possible action it
could take but imagine you're a

00:28:57.370 --> 00:28:57.380
could take but imagine you're a
 

00:28:57.380 --> 00:29:00.340
could take but imagine you're a
self-driving car and the actions that

00:29:00.340 --> 00:29:00.350
self-driving car and the actions that
 

00:29:00.350 --> 00:29:02.440
self-driving car and the actions that
you take are a continuous variable on

00:29:02.440 --> 00:29:02.450
you take are a continuous variable on
 

00:29:02.450 --> 00:29:04.419
you take are a continuous variable on
your steering wheel angle it's the angle

00:29:04.419 --> 00:29:04.429
your steering wheel angle it's the angle
 

00:29:04.429 --> 00:29:06.880
your steering wheel angle it's the angle
that the wheel should turn now you can't

00:29:06.880 --> 00:29:06.890
that the wheel should turn now you can't
 

00:29:06.890 --> 00:29:08.440
that the wheel should turn now you can't
use cue learning because it requires an

00:29:08.440 --> 00:29:08.450
use cue learning because it requires an
 

00:29:08.450 --> 00:29:10.630
use cue learning because it requires an
infinite number of outputs there are

00:29:10.630 --> 00:29:10.640
infinite number of outputs there are
 

00:29:10.640 --> 00:29:12.159
infinite number of outputs there are
tricks that you can get around this with

00:29:12.159 --> 00:29:12.169
tricks that you can get around this with
 

00:29:12.169 --> 00:29:14.440
tricks that you can get around this with
because you can just discretize your

00:29:14.440 --> 00:29:14.450
because you can just discretize your
 

00:29:14.450 --> 00:29:16.509
because you can just discretize your
action space into very small bins and

00:29:16.509 --> 00:29:16.519
action space into very small bins and
 

00:29:16.519 --> 00:29:19.120
action space into very small bins and
try to learn the Q value for each bin

00:29:19.120 --> 00:29:19.130
try to learn the Q value for each bin
 

00:29:19.130 --> 00:29:21.370
try to learn the Q value for each bin
but of course the question is well how

00:29:21.370 --> 00:29:21.380
but of course the question is well how
 

00:29:21.380 --> 00:29:22.659
but of course the question is well how
small do you want to make this the

00:29:22.659 --> 00:29:22.669
small do you want to make this the
 

00:29:22.669 --> 00:29:24.730
small do you want to make this the
smaller you make these bins the harder

00:29:24.730 --> 00:29:24.740
smaller you make these bins the harder
 

00:29:24.740 --> 00:29:27.370
smaller you make these bins the harder
learning becomes and just at its core

00:29:27.370 --> 00:29:27.380
learning becomes and just at its core
 

00:29:27.380 --> 00:29:30.039
learning becomes and just at its core
the vanilla Q learning algorithm that I

00:29:30.039 --> 00:29:30.049
the vanilla Q learning algorithm that I
 

00:29:30.049 --> 00:29:32.169
the vanilla Q learning algorithm that I
presented here is not well-suited for

00:29:32.169 --> 00:29:32.179
presented here is not well-suited for
 

00:29:32.179 --> 00:29:36.850
presented here is not well-suited for
continuous action spaces and on another

00:29:36.850 --> 00:29:36.860
continuous action spaces and on another
 

00:29:36.860 --> 00:29:39.639
continuous action spaces and on another
level they're not flexible to handle

00:29:39.639 --> 00:29:39.649
level they're not flexible to handle
 

00:29:39.649 --> 00:29:42.669
level they're not flexible to handle
stochastic policies because we're

00:29:42.669 --> 00:29:42.679
stochastic policies because we're
 

00:29:42.679 --> 00:29:44.169
stochastic policies because we're
basically sampling from this argument

00:29:44.169 --> 00:29:44.179
basically sampling from this argument
 

00:29:44.179 --> 00:29:47.049
basically sampling from this argument
function we have our Q function and we

00:29:47.049 --> 00:29:47.059
function we have our Q function and we
 

00:29:47.059 --> 00:29:48.909
function we have our Q function and we
just take the arc max to compute the

00:29:48.909 --> 00:29:48.919
just take the arc max to compute the
 

00:29:48.919 --> 00:29:50.620
just take the arc max to compute the
best action that we can execute at any

00:29:50.620 --> 00:29:50.630
best action that we can execute at any
 

00:29:50.630 --> 00:29:52.810
best action that we can execute at any
given time it does it means that we

00:29:52.810 --> 00:29:52.820
given time it does it means that we
 

00:29:52.820 --> 00:29:56.560
given time it does it means that we
can't actually learn when our when our

00:29:56.560 --> 00:29:56.570
can't actually learn when our when our
 

00:29:56.570 --> 00:29:58.990
can't actually learn when our when our
policies are stochastically computed so

00:29:58.990 --> 00:29:59.000
policies are stochastically computed so
 

00:29:59.000 --> 00:30:00.610
policies are stochastically computed so
when the next state is maybe not

00:30:00.610 --> 00:30:00.620
when the next state is maybe not
 

00:30:00.620 --> 00:30:03.370
when the next state is maybe not
deterministic but instead having some

00:30:03.370 --> 00:30:03.380
deterministic but instead having some
 

00:30:03.380 --> 00:30:08.649
deterministic but instead having some
random component to it as well right so

00:30:08.649 --> 00:30:08.659
random component to it as well right so
 

00:30:08.659 --> 00:30:10.149
random component to it as well right so
this is the point I mentioned about the

00:30:10.149 --> 00:30:10.159
this is the point I mentioned about the
 

00:30:10.159 --> 00:30:11.710
this is the point I mentioned about the
continuous action space being actually a

00:30:11.710 --> 00:30:11.720
continuous action space being actually a
 

00:30:11.720 --> 00:30:14.159
continuous action space being actually a
very important problem that might seem

00:30:14.159 --> 00:30:14.169
very important problem that might seem
 

00:30:14.169 --> 00:30:16.480
very important problem that might seem
kind of trivial to deal with by just

00:30:16.480 --> 00:30:16.490
kind of trivial to deal with by just
 

00:30:16.490 --> 00:30:18.220
kind of trivial to deal with by just
bending the solution bending the outputs

00:30:18.220 --> 00:30:18.230
bending the solution bending the outputs
 

00:30:18.230 --> 00:30:19.720
bending the solution bending the outputs
but this is actually a really big

00:30:19.720 --> 00:30:19.730
but this is actually a really big
 

00:30:19.730 --> 00:30:22.870
but this is actually a really big
problem in practice and to overcome this

00:30:22.870 --> 00:30:22.880
problem in practice and to overcome this
 

00:30:22.880 --> 00:30:24.940
problem in practice and to overcome this
we're gonna consider a new class of

00:30:24.940 --> 00:30:24.950
we're gonna consider a new class of
 

00:30:24.950 --> 00:30:27.190
we're gonna consider a new class of
reinforcement learning models called

00:30:27.190 --> 00:30:27.200
reinforcement learning models called
 

00:30:27.200 --> 00:30:29.799
reinforcement learning models called
policy gradient models for training

00:30:29.799 --> 00:30:29.809
policy gradient models for training
 

00:30:29.809 --> 00:30:34.149
policy gradient models for training
these algorithms so policy gradients is

00:30:34.149 --> 00:30:34.159
these algorithms so policy gradients is
 

00:30:34.159 --> 00:30:36.940
these algorithms so policy gradients is
a slightly different twist on Q learning

00:30:36.940 --> 00:30:36.950
a slightly different twist on Q learning
 

00:30:36.950 --> 00:30:38.649
a slightly different twist on Q learning
but at the foundation it's actually very

00:30:38.649 --> 00:30:38.659
but at the foundation it's actually very
 

00:30:38.659 --> 00:30:42.279
but at the foundation it's actually very
different so let's recall Q learning so

00:30:42.279 --> 00:30:42.289
different so let's recall Q learning so
 

00:30:42.289 --> 00:30:44.259
different so let's recall Q learning so
the deep Q network takes input the

00:30:44.259 --> 00:30:44.269
the deep Q network takes input the
 

00:30:44.269 --> 00:30:47.019
the deep Q network takes input the
states and predicts a Q value for each

00:30:47.019 --> 00:30:47.029
states and predicts a Q value for each
 

00:30:47.029 --> 00:30:48.909
states and predicts a Q value for each
possible action on the right hand side

00:30:48.909 --> 00:30:48.919
possible action on the right hand side
 

00:30:48.919 --> 00:30:52.480
possible action on the right hand side
now in policy gradients we're gonna do

00:30:52.480 --> 00:30:52.490
now in policy gradients we're gonna do
 

00:30:52.490 --> 00:30:54.549
now in policy gradients we're gonna do
something slightly different we're gonna

00:30:54.549 --> 00:30:54.559
something slightly different we're gonna
 

00:30:54.559 --> 00:30:56.649
something slightly different we're gonna
take us and put the state but now we're

00:30:56.649 --> 00:30:56.659
take us and put the state but now we're
 

00:30:56.659 --> 00:30:58.720
take us and put the state but now we're
gonna output a probability distribution

00:30:58.720 --> 00:30:58.730
gonna output a probability distribution
 

00:30:58.730 --> 00:31:02.049
gonna output a probability distribution
over all possible actions again we're

00:31:02.049 --> 00:31:02.059
over all possible actions again we're
 

00:31:02.059 --> 00:31:03.519
over all possible actions again we're
still considering the case of discrete

00:31:03.519 --> 00:31:03.529
still considering the case of discrete
 

00:31:03.529 --> 00:31:06.039
still considering the case of discrete
action spaces but we'll see how we can

00:31:06.039 --> 00:31:06.049
action spaces but we'll see how we can
 

00:31:06.049 --> 00:31:07.250
action spaces but we'll see how we can
easily extend the

00:31:07.250 --> 00:31:07.260
easily extend the
 

00:31:07.260 --> 00:31:09.200
easily extend the
in ways that we couldn't do with

00:31:09.200 --> 00:31:09.210
in ways that we couldn't do with
 

00:31:09.210 --> 00:31:11.120
in ways that we couldn't do with
q-learning to continuous action spaces

00:31:11.120 --> 00:31:11.130
q-learning to continuous action spaces
 

00:31:11.130 --> 00:31:13.130
q-learning to continuous action spaces
as well let's stick with discrete action

00:31:13.130 --> 00:31:13.140
as well let's stick with discrete action
 

00:31:13.140 --> 00:31:16.010
as well let's stick with discrete action
spaces for now just for simplicity so

00:31:16.010 --> 00:31:16.020
spaces for now just for simplicity so
 

00:31:16.020 --> 00:31:21.740
spaces for now just for simplicity so
here pie of alpha sorry PI of AI for all

00:31:21.740 --> 00:31:21.750
here pie of alpha sorry PI of AI for all
 

00:31:21.750 --> 00:31:24.620
here pie of alpha sorry PI of AI for all
I is just the probability that you

00:31:24.620 --> 00:31:24.630
I is just the probability that you
 

00:31:24.630 --> 00:31:26.990
I is just the probability that you
should execute action I given the state

00:31:26.990 --> 00:31:27.000
should execute action I given the state
 

00:31:27.000 --> 00:31:31.130
should execute action I given the state
that you see as an input and since this

00:31:31.130 --> 00:31:31.140
that you see as an input and since this
 

00:31:31.140 --> 00:31:32.690
that you see as an input and since this
is the probability distribution it means

00:31:32.690 --> 00:31:32.700
is the probability distribution it means
 

00:31:32.700 --> 00:31:34.100
is the probability distribution it means
that all of these outputs have to add up

00:31:34.100 --> 00:31:34.110
that all of these outputs have to add up
 

00:31:34.110 --> 00:31:36.740
that all of these outputs have to add up
to one we can do this using softmax

00:31:36.740 --> 00:31:36.750
to one we can do this using softmax
 

00:31:36.750 --> 00:31:38.180
to one we can do this using softmax
activation function and deep neural

00:31:38.180 --> 00:31:38.190
activation function and deep neural
 

00:31:38.190 --> 00:31:40.280
activation function and deep neural
networks it's just enforces that the

00:31:40.280 --> 00:31:40.290
networks it's just enforces that the
 

00:31:40.290 --> 00:31:45.470
networks it's just enforces that the
outputs are summing to one and again

00:31:45.470 --> 00:31:45.480
outputs are summing to one and again
 

00:31:45.480 --> 00:31:49.130
outputs are summing to one and again
just to reiterate this PI a given s is

00:31:49.130 --> 00:31:49.140
just to reiterate this PI a given s is
 

00:31:49.140 --> 00:31:51.830
just to reiterate this PI a given s is
the probability distribution of taking a

00:31:51.830 --> 00:31:51.840
the probability distribution of taking a
 

00:31:51.840 --> 00:31:53.900
the probability distribution of taking a
given action given the state that we

00:31:53.900 --> 00:31:53.910
given action given the state that we
 

00:31:53.910 --> 00:31:56.060
given action given the state that we
currently see and this is just

00:31:56.060 --> 00:31:56.070
currently see and this is just
 

00:31:56.070 --> 00:31:57.830
currently see and this is just
fundamentally different than what we

00:31:57.830 --> 00:31:57.840
fundamentally different than what we
 

00:31:57.840 --> 00:31:59.600
fundamentally different than what we
were doing before which was estimating a

00:31:59.600 --> 00:31:59.610
were doing before which was estimating a
 

00:31:59.610 --> 00:32:01.970
were doing before which was estimating a
Q value which is saying what is the

00:32:01.970 --> 00:32:01.980
Q value which is saying what is the
 

00:32:01.980 --> 00:32:04.520
Q value which is saying what is the
possible reward that I can obtain by

00:32:04.520 --> 00:32:04.530
possible reward that I can obtain by
 

00:32:04.530 --> 00:32:07.580
possible reward that I can obtain by
executing this action and then using the

00:32:07.580 --> 00:32:07.590
executing this action and then using the
 

00:32:07.590 --> 00:32:11.810
executing this action and then using the
maximum the Q value of maximum reward to

00:32:11.810 --> 00:32:11.820
maximum the Q value of maximum reward to
 

00:32:11.820 --> 00:32:14.000
maximum the Q value of maximum reward to
execute that action so now we're

00:32:14.000 --> 00:32:14.010
execute that action so now we're
 

00:32:14.010 --> 00:32:15.620
execute that action so now we're
directly learning the policy we're

00:32:15.620 --> 00:32:15.630
directly learning the policy we're
 

00:32:15.630 --> 00:32:17.540
directly learning the policy we're
directly saying what is the correct

00:32:17.540 --> 00:32:17.550
directly saying what is the correct
 

00:32:17.550 --> 00:32:18.830
directly saying what is the correct
action that I should take what is the

00:32:18.830 --> 00:32:18.840
action that I should take what is the
 

00:32:18.840 --> 00:32:21.020
action that I should take what is the
probability that that action is a 1

00:32:21.020 --> 00:32:21.030
probability that that action is a 1
 

00:32:21.030 --> 00:32:22.490
probability that that action is a 1
whereas the probability that that action

00:32:22.490 --> 00:32:22.500
whereas the probability that that action
 

00:32:22.500 --> 00:32:24.410
whereas the probability that that action
is a 2 and just execute the correct

00:32:24.410 --> 00:32:24.420
is a 2 and just execute the correct
 

00:32:24.420 --> 00:32:28.100
is a 2 and just execute the correct
action so in some sense it's skipping a

00:32:28.100 --> 00:32:28.110
action so in some sense it's skipping a
 

00:32:28.110 --> 00:32:29.900
action so in some sense it's skipping a
step from Q learning and Q learning you

00:32:29.900 --> 00:32:29.910
step from Q learning and Q learning you
 

00:32:29.910 --> 00:32:31.850
step from Q learning and Q learning you
learn the Q function use the Q function

00:32:31.850 --> 00:32:31.860
learn the Q function use the Q function
 

00:32:31.860 --> 00:32:35.690
learn the Q function use the Q function
to infer your policy and policy learning

00:32:35.690 --> 00:32:35.700
to infer your policy and policy learning
 

00:32:35.700 --> 00:32:43.790
to infer your policy and policy learning
you just learn your policy directly so

00:32:43.790 --> 00:32:43.800
you just learn your policy directly so
 

00:32:43.800 --> 00:32:47.470
you just learn your policy directly so
how do we train policy gradient learning

00:32:47.470 --> 00:32:47.480
how do we train policy gradient learning
 

00:32:47.480 --> 00:32:50.690
how do we train policy gradient learning
essentially the way it works is we run a

00:32:50.690 --> 00:32:50.700
essentially the way it works is we run a
 

00:32:50.700 --> 00:32:52.850
essentially the way it works is we run a
policy for a long time before we even

00:32:52.850 --> 00:32:52.860
policy for a long time before we even
 

00:32:52.860 --> 00:32:54.740
policy for a long time before we even
start training we run multiple episodes

00:32:54.740 --> 00:32:54.750
start training we run multiple episodes
 

00:32:54.750 --> 00:32:56.510
start training we run multiple episodes
or multiple rollouts

00:32:56.510 --> 00:32:56.520
or multiple rollouts
 

00:32:56.520 --> 00:32:58.880
or multiple rollouts
of that policy a roll out is basically

00:32:58.880 --> 00:32:58.890
of that policy a roll out is basically
 

00:32:58.890 --> 00:33:03.890
of that policy a roll out is basically
from start to end of a training session

00:33:03.890 --> 00:33:03.900
from start to end of a training session
 

00:33:03.900 --> 00:33:05.840
from start to end of a training session
so we can define a roll out as basically

00:33:05.840 --> 00:33:05.850
so we can define a roll out as basically
 

00:33:05.850 --> 00:33:09.920
so we can define a roll out as basically
from time 0 to time T where T is the end

00:33:09.920 --> 00:33:09.930
from time 0 to time T where T is the end
 

00:33:09.930 --> 00:33:14.810
from time 0 to time T where T is the end
of some definition of a episode in that

00:33:14.810 --> 00:33:14.820
of some definition of a episode in that
 

00:33:14.820 --> 00:33:17.600
of some definition of a episode in that
game so in the case of breakout capital

00:33:17.600 --> 00:33:17.610
game so in the case of breakout capital
 

00:33:17.610 --> 00:33:19.640
game so in the case of breakout capital
T would be the time at which the ball

00:33:19.640 --> 00:33:19.650
T would be the time at which the ball
 

00:33:19.650 --> 00:33:20.690
T would be the time at which the ball
passes the pad

00:33:20.690 --> 00:33:20.700
passes the pad
 

00:33:20.700 --> 00:33:22.850
passes the pad
you miss it so this is the time when the

00:33:22.850 --> 00:33:22.860
you miss it so this is the time when the
 

00:33:22.860 --> 00:33:24.650
you miss it so this is the time when the
episode ends and you miss the ball or

00:33:24.650 --> 00:33:24.660
episode ends and you miss the ball or
 

00:33:24.660 --> 00:33:26.450
episode ends and you miss the ball or
it's the time at which you kill all of

00:33:26.450 --> 00:33:26.460
it's the time at which you kill all of
 

00:33:26.460 --> 00:33:28.970
it's the time at which you kill all of
the points on the top and you have no

00:33:28.970 --> 00:33:28.980
the points on the top and you have no
 

00:33:28.980 --> 00:33:30.620
the points on the top and you have no
other points to kill so now the game is

00:33:30.620 --> 00:33:30.630
other points to kill so now the game is
 

00:33:30.630 --> 00:33:33.800
other points to kill so now the game is
over so you run your policy for a long

00:33:33.800 --> 00:33:33.810
over so you run your policy for a long
 

00:33:33.810 --> 00:33:36.680
over so you run your policy for a long
time and then you get the reward after

00:33:36.680 --> 00:33:36.690
time and then you get the reward after
 

00:33:36.690 --> 00:33:39.770
time and then you get the reward after
running that policy now in policy

00:33:39.770 --> 00:33:39.780
running that policy now in policy
 

00:33:39.780 --> 00:33:42.170
running that policy now in policy
gradients all you want to do is increase

00:33:42.170 --> 00:33:42.180
gradients all you want to do is increase
 

00:33:42.180 --> 00:33:44.000
gradients all you want to do is increase
the probability of actions that lead to

00:33:44.000 --> 00:33:44.010
the probability of actions that lead to
 

00:33:44.010 --> 00:33:46.130
the probability of actions that lead to
high rewards and decrease the

00:33:46.130 --> 00:33:46.140
high rewards and decrease the
 

00:33:46.140 --> 00:33:48.170
high rewards and decrease the
probability of actions that lead to low

00:33:48.170 --> 00:33:48.180
probability of actions that lead to low
 

00:33:48.180 --> 00:33:51.170
probability of actions that lead to low
rewards it sounds simple it is simple

00:33:51.170 --> 00:33:51.180
rewards it sounds simple it is simple
 

00:33:51.180 --> 00:33:53.210
rewards it sounds simple it is simple
let's just see how it's done by looking

00:33:53.210 --> 00:33:53.220
let's just see how it's done by looking
 

00:33:53.220 --> 00:33:55.730
let's just see how it's done by looking
at the gradient which is where this this

00:33:55.730 --> 00:33:55.740
at the gradient which is where this this
 

00:33:55.740 --> 00:33:58.310
at the gradient which is where this this
algorithm gets its name which is right

00:33:58.310 --> 00:33:58.320
algorithm gets its name which is right
 

00:33:58.320 --> 00:34:01.940
algorithm gets its name which is right
here so let's walk through this let's

00:34:01.940 --> 00:34:01.950
here so let's walk through this let's
 

00:34:01.950 --> 00:34:02.990
here so let's walk through this let's
walk through this algorithm a little

00:34:02.990 --> 00:34:03.000
walk through this algorithm a little
 

00:34:03.000 --> 00:34:07.040
walk through this algorithm a little
more detail so we do a roll out for an

00:34:07.040 --> 00:34:07.050
more detail so we do a roll out for an
 

00:34:07.050 --> 00:34:10.510
more detail so we do a roll out for an
episode given our policy so policy is

00:34:10.510 --> 00:34:10.520
episode given our policy so policy is
 

00:34:10.520 --> 00:34:13.060
episode given our policy so policy is
defined by the neural network

00:34:13.060 --> 00:34:13.070
defined by the neural network
 

00:34:13.070 --> 00:34:17.060
defined by the neural network
parametrized by the parameters theta we

00:34:17.060 --> 00:34:17.070
parametrized by the parameters theta we
 

00:34:17.070 --> 00:34:19.190
parametrized by the parameters theta we
sample a bunch of episodes from that

00:34:19.190 --> 00:34:19.200
sample a bunch of episodes from that
 

00:34:19.200 --> 00:34:21.620
sample a bunch of episodes from that
policy and each episode is basically

00:34:21.620 --> 00:34:21.630
policy and each episode is basically
 

00:34:21.630 --> 00:34:23.840
policy and each episode is basically
just a collection of state action and

00:34:23.840 --> 00:34:23.850
just a collection of state action and
 

00:34:23.850 --> 00:34:25.610
just a collection of state action and
reward pairs so we record all of those

00:34:25.610 --> 00:34:25.620
reward pairs so we record all of those
 

00:34:25.620 --> 00:34:28.370
reward pairs so we record all of those
into memory then when we're ready to

00:34:28.370 --> 00:34:28.380
into memory then when we're ready to
 

00:34:28.380 --> 00:34:30.800
into memory then when we're ready to
begin training all we do is compute this

00:34:30.800 --> 00:34:30.810
begin training all we do is compute this
 

00:34:30.810 --> 00:34:34.940
begin training all we do is compute this
gradient right here and that gradient is

00:34:34.940 --> 00:34:34.950
gradient right here and that gradient is
 

00:34:34.950 --> 00:34:37.610
gradient right here and that gradient is
the log likelihood of seeing a

00:34:37.610 --> 00:34:37.620
the log likelihood of seeing a
 

00:34:37.620 --> 00:34:39.940
the log likelihood of seeing a
particular action given the state

00:34:39.940 --> 00:34:39.950
particular action given the state
 

00:34:39.950 --> 00:34:43.310
particular action given the state
multiplied by the expected reward of

00:34:43.310 --> 00:34:43.320
multiplied by the expected reward of
 

00:34:43.320 --> 00:34:46.880
multiplied by the expected reward of
that action sorry excuse me that's the

00:34:46.880 --> 00:34:46.890
that action sorry excuse me that's the
 

00:34:46.890 --> 00:34:49.310
that action sorry excuse me that's the
expected discounted reward of that

00:34:49.310 --> 00:34:49.320
expected discounted reward of that
 

00:34:49.320 --> 00:34:52.370
expected discounted reward of that
action at that time so let's try and

00:34:52.370 --> 00:34:52.380
action at that time so let's try and
 

00:34:52.380 --> 00:34:53.870
action at that time so let's try and
parse what this means because this is

00:34:53.870 --> 00:34:53.880
parse what this means because this is
 

00:34:53.880 --> 00:34:56.090
parse what this means because this is
really the entire policy gradient

00:34:56.090 --> 00:34:56.100
really the entire policy gradient
 

00:34:56.100 --> 00:34:58.040
really the entire policy gradient
algorithm on this one line this is the

00:34:58.040 --> 00:34:58.050
algorithm on this one line this is the
 

00:34:58.050 --> 00:34:59.870
algorithm on this one line this is the
key line here so let's really try and

00:34:59.870 --> 00:34:59.880
key line here so let's really try and
 

00:34:59.880 --> 00:35:03.890
key line here so let's really try and
understand this line the green part is

00:35:03.890 --> 00:35:03.900
understand this line the green part is
 

00:35:03.900 --> 00:35:07.520
understand this line the green part is
simply the log likelihood of obtaining

00:35:07.520 --> 00:35:07.530
simply the log likelihood of obtaining
 

00:35:07.530 --> 00:35:10.520
simply the log likelihood of obtaining
or of outputting that action so let's

00:35:10.520 --> 00:35:10.530
or of outputting that action so let's
 

00:35:10.530 --> 00:35:12.650
or of outputting that action so let's
suppose our action was very desirable it

00:35:12.650 --> 00:35:12.660
suppose our action was very desirable it
 

00:35:12.660 --> 00:35:15.740
suppose our action was very desirable it
led to a good reward okay and that's

00:35:15.740 --> 00:35:15.750
led to a good reward okay and that's
 

00:35:15.750 --> 00:35:18.020
led to a good reward okay and that's
just defined by we do our roll out on

00:35:18.020 --> 00:35:18.030
just defined by we do our roll out on
 

00:35:18.030 --> 00:35:20.060
just defined by we do our roll out on
the top and we won the game at the end

00:35:20.060 --> 00:35:20.070
the top and we won the game at the end
 

00:35:20.070 --> 00:35:22.100
the top and we won the game at the end
so all of these policies all of these

00:35:22.100 --> 00:35:22.110
so all of these policies all of these
 

00:35:22.110 --> 00:35:25.190
so all of these policies all of these
actions should be enforced or reinforced

00:35:25.190 --> 00:35:25.200
actions should be enforced or reinforced
 

00:35:25.200 --> 00:35:28.610
actions should be enforced or reinforced
and in this case we did and on the

00:35:28.610 --> 00:35:28.620
and in this case we did and on the
 

00:35:28.620 --> 00:35:31.370
and in this case we did and on the
second line we did another episode which

00:35:31.370 --> 00:35:31.380
second line we did another episode which
 

00:35:31.380 --> 00:35:34.520
second line we did another episode which
resulted in a loss all of these Paul

00:35:34.520 --> 00:35:34.530
resulted in a loss all of these Paul
 

00:35:34.530 --> 00:35:37.490
resulted in a loss all of these Paul
should be discouraged in the future so

00:35:37.490 --> 00:35:37.500
should be discouraged in the future so
 

00:35:37.500 --> 00:35:39.710
should be discouraged in the future so
when things result in positive rewards

00:35:39.710 --> 00:35:39.720
when things result in positive rewards
 

00:35:39.720 --> 00:35:42.050
when things result in positive rewards
we multiply this is going to be a

00:35:42.050 --> 00:35:42.060
we multiply this is going to be a
 

00:35:42.060 --> 00:35:44.120
we multiply this is going to be a
positive number and we're going to try

00:35:44.120 --> 00:35:44.130
positive number and we're going to try
 

00:35:44.130 --> 00:35:46.550
positive number and we're going to try
and increase the log likelihood of

00:35:46.550 --> 00:35:46.560
and increase the log likelihood of
 

00:35:46.560 --> 00:35:48.530
and increase the log likelihood of
seeing those actions again in the future

00:35:48.530 --> 00:35:48.540
seeing those actions again in the future
 

00:35:48.540 --> 00:35:50.540
seeing those actions again in the future
so we want to tell the network

00:35:50.540 --> 00:35:50.550
so we want to tell the network
 

00:35:50.550 --> 00:35:53.030
so we want to tell the network
essentially to update your parameters

00:35:53.030 --> 00:35:53.040
essentially to update your parameters
 

00:35:53.040 --> 00:35:55.640
essentially to update your parameters
such that whatever you did that resulted

00:35:55.640 --> 00:35:55.650
such that whatever you did that resulted
 

00:35:55.650 --> 00:35:58.070
such that whatever you did that resulted
in a good reward is gonna happen again

00:35:58.070 --> 00:35:58.080
in a good reward is gonna happen again
 

00:35:58.080 --> 00:35:58.790
in a good reward is gonna happen again
in the future

00:35:58.790 --> 00:35:58.800
in the future
 

00:35:58.800 --> 00:36:01.520
in the future
and to even greater probability so let's

00:36:01.520 --> 00:36:01.530
and to even greater probability so let's
 

00:36:01.530 --> 00:36:02.900
and to even greater probability so let's
make sure that we definitely sample

00:36:02.900 --> 00:36:02.910
make sure that we definitely sample
 

00:36:02.910 --> 00:36:04.070
make sure that we definitely sample
those things again because we got good

00:36:04.070 --> 00:36:04.080
those things again because we got good
 

00:36:04.080 --> 00:36:07.100
those things again because we got good
rewards from them last time on the

00:36:07.100 --> 00:36:07.110
rewards from them last time on the
 

00:36:07.110 --> 00:36:10.400
rewards from them last time on the
converse side if R is is negative or if

00:36:10.400 --> 00:36:10.410
converse side if R is is negative or if
 

00:36:10.410 --> 00:36:12.320
converse side if R is is negative or if
it's zero if we didn't get any reward

00:36:12.320 --> 00:36:12.330
it's zero if we didn't get any reward
 

00:36:12.330 --> 00:36:13.850
it's zero if we didn't get any reward
from it we want to make sure that we

00:36:13.850 --> 00:36:13.860
from it we want to make sure that we
 

00:36:13.860 --> 00:36:16.550
from it we want to make sure that we
update our network now to change the

00:36:16.550 --> 00:36:16.560
update our network now to change the
 

00:36:16.560 --> 00:36:18.410
update our network now to change the
parameters and make sure that we

00:36:18.410 --> 00:36:18.420
parameters and make sure that we
 

00:36:18.420 --> 00:36:21.080
parameters and make sure that we
discourage any of the probabilities that

00:36:21.080 --> 00:36:21.090
discourage any of the probabilities that
 

00:36:21.090 --> 00:36:23.000
discourage any of the probabilities that
we output on the previous time so we

00:36:23.000 --> 00:36:23.010
we output on the previous time so we
 

00:36:23.010 --> 00:36:25.220
we output on the previous time so we
want to lower the log likelihood of

00:36:25.220 --> 00:36:25.230
want to lower the log likelihood of
 

00:36:25.230 --> 00:36:27.320
want to lower the log likelihood of
executing those actions that resulted in

00:36:27.320 --> 00:36:27.330
executing those actions that resulted in
 

00:36:27.330 --> 00:36:32.750
executing those actions that resulted in
negative rewards so now I'll talk a

00:36:32.750 --> 00:36:32.760
negative rewards so now I'll talk a
 

00:36:32.760 --> 00:36:35.180
negative rewards so now I'll talk a
little bit about the game of Go and how

00:36:35.180 --> 00:36:35.190
little bit about the game of Go and how
 

00:36:35.190 --> 00:36:36.980
little bit about the game of Go and how
we can use policy gradient learning

00:36:36.980 --> 00:36:36.990
we can use policy gradient learning
 

00:36:36.990 --> 00:36:40.850
we can use policy gradient learning
combined with some fancy tricks that

00:36:40.850 --> 00:36:40.860
combined with some fancy tricks that
 

00:36:40.860 --> 00:36:43.010
combined with some fancy tricks that
deepmind implemented in the game of

00:36:43.010 --> 00:36:43.020
deepmind implemented in the game of
 

00:36:43.020 --> 00:36:47.740
deepmind implemented in the game of
alphago in the algorithm alphago and i

00:36:47.740 --> 00:36:47.750
alphago in the algorithm alphago and i
 

00:36:47.750 --> 00:36:51.830
alphago in the algorithm alphago and i
think the core for those of you who

00:36:51.830 --> 00:36:51.840
think the core for those of you who
 

00:36:51.840 --> 00:36:53.420
think the core for those of you who
aren't familiar with the game of Go it's

00:36:53.420 --> 00:36:53.430
aren't familiar with the game of Go it's
 

00:36:53.430 --> 00:36:54.890
aren't familiar with the game of Go it's
an incredibly complex game with a

00:36:54.890 --> 00:36:54.900
an incredibly complex game with a
 

00:36:54.900 --> 00:36:56.960
an incredibly complex game with a
massive state space there are more

00:36:56.960 --> 00:36:56.970
massive state space there are more
 

00:36:56.970 --> 00:36:58.400
massive state space there are more
States than there are atoms in the

00:36:58.400 --> 00:36:58.410
States than there are atoms in the
 

00:36:58.410 --> 00:37:00.050
States than there are atoms in the
universe and that's in the full version

00:37:00.050 --> 00:37:00.060
universe and that's in the full version
 

00:37:00.060 --> 00:37:02.420
universe and that's in the full version
of the game where it's in 19 by 19 game

00:37:02.420 --> 00:37:02.430
of the game where it's in 19 by 19 game
 

00:37:02.430 --> 00:37:05.960
of the game where it's in 19 by 19 game
and the idea of go is that you have a

00:37:05.960 --> 00:37:05.970
and the idea of go is that you have a
 

00:37:05.970 --> 00:37:07.760
and the idea of go is that you have a
it's a two-player game black and white

00:37:07.760 --> 00:37:07.770
it's a two-player game black and white
 

00:37:07.770 --> 00:37:11.720
it's a two-player game black and white
and the motivation or the goal is that

00:37:11.720 --> 00:37:11.730
and the motivation or the goal is that
 

00:37:11.730 --> 00:37:13.880
and the motivation or the goal is that
you want to get more board territory

00:37:13.880 --> 00:37:13.890
you want to get more board territory
 

00:37:13.890 --> 00:37:17.120
you want to get more board territory
than your opponent the state here is

00:37:17.120 --> 00:37:17.130
than your opponent the state here is
 

00:37:17.130 --> 00:37:19.280
than your opponent the state here is
just the board of black and white cells

00:37:19.280 --> 00:37:19.290
just the board of black and white cells
 

00:37:19.290 --> 00:37:23.180
just the board of black and white cells
and the action that you want to execute

00:37:23.180 --> 00:37:23.190
and the action that you want to execute
 

00:37:23.190 --> 00:37:24.920
and the action that you want to execute
it's just a probability distribution

00:37:24.920 --> 00:37:24.930
it's just a probability distribution
 

00:37:24.930 --> 00:37:27.860
it's just a probability distribution
over each possible cell that you need to

00:37:27.860 --> 00:37:27.870
over each possible cell that you need to
 

00:37:27.870 --> 00:37:32.330
over each possible cell that you need to
put your next piece out that cell so you

00:37:32.330 --> 00:37:32.340
put your next piece out that cell so you
 

00:37:32.340 --> 00:37:34.430
put your next piece out that cell so you
can have a you can train a network like

00:37:34.430 --> 00:37:34.440
can have a you can train a network like
 

00:37:34.440 --> 00:37:36.500
can have a you can train a network like
we were defining before a policy based

00:37:36.500 --> 00:37:36.510
we were defining before a policy based
 

00:37:36.510 --> 00:37:38.930
we were defining before a policy based
net network which takes us input an

00:37:38.930 --> 00:37:38.940
net network which takes us input an
 

00:37:38.940 --> 00:37:41.750
net network which takes us input an
image of this board it's a 19 by 19

00:37:41.750 --> 00:37:41.760
image of this board it's a 19 by 19
 

00:37:41.760 --> 00:37:44.300
image of this board it's a 19 by 19
image where each pixel in that image

00:37:44.300 --> 00:37:44.310
image where each pixel in that image
 

00:37:44.310 --> 00:37:46.640
image where each pixel in that image
corresponds to a cell in the board and

00:37:46.640 --> 00:37:46.650
corresponds to a cell in the board and
 

00:37:46.650 --> 00:37:47.780
corresponds to a cell in the board and
the

00:37:47.780 --> 00:37:47.790
the
 

00:37:47.790 --> 00:37:50.000
the
part of that network is going to be a

00:37:50.000 --> 00:37:50.010
part of that network is going to be a
 

00:37:50.010 --> 00:37:52.280
part of that network is going to be a
probability distribution again it's a 19

00:37:52.280 --> 00:37:52.290
probability distribution again it's a 19
 

00:37:52.290 --> 00:37:54.230
probability distribution again it's a 19
by 19 probability distribution where

00:37:54.230 --> 00:37:54.240
by 19 probability distribution where
 

00:37:54.240 --> 00:37:56.060
by 19 probability distribution where
each cell is now the probability that

00:37:56.060 --> 00:37:56.070
each cell is now the probability that
 

00:37:56.070 --> 00:37:58.580
each cell is now the probability that
your next action should be placing a

00:37:58.580 --> 00:37:58.590
your next action should be placing a
 

00:37:58.590 --> 00:38:05.050
your next action should be placing a
token on that cell on the board right so

00:38:05.050 --> 00:38:05.060
token on that cell on the board right so
 

00:38:05.060 --> 00:38:07.700
token on that cell on the board right so
let's see how at a high level the

00:38:07.700 --> 00:38:07.710
let's see how at a high level the
 

00:38:07.710 --> 00:38:09.730
let's see how at a high level the
alphago algorithm works

00:38:09.730 --> 00:38:09.740
alphago algorithm works
 

00:38:09.740 --> 00:38:11.960
alphago algorithm works
they use a little trick here in the

00:38:11.960 --> 00:38:11.970
they use a little trick here in the
 

00:38:11.970 --> 00:38:13.880
they use a little trick here in the
beginning which is they start by

00:38:13.880 --> 00:38:13.890
beginning which is they start by
 

00:38:13.890 --> 00:38:16.310
beginning which is they start by
initializing their network by training

00:38:16.310 --> 00:38:16.320
initializing their network by training
 

00:38:16.320 --> 00:38:19.610
initializing their network by training
it on a bunch of experts from sorry

00:38:19.610 --> 00:38:19.620
it on a bunch of experts from sorry
 

00:38:19.620 --> 00:38:21.290
it on a bunch of experts from sorry
sorry they training it from a bunch of

00:38:21.290 --> 00:38:21.300
sorry they training it from a bunch of
 

00:38:21.300 --> 00:38:23.720
sorry they training it from a bunch of
human experts on playing the game of go

00:38:23.720 --> 00:38:23.730
human experts on playing the game of go
 

00:38:23.730 --> 00:38:26.000
human experts on playing the game of go
so they have humans play each other

00:38:26.000 --> 00:38:26.010
so they have humans play each other
 

00:38:26.010 --> 00:38:27.830
so they have humans play each other
these are usually professional or very

00:38:27.830 --> 00:38:27.840
these are usually professional or very
 

00:38:27.840 --> 00:38:30.890
these are usually professional or very
high quality high-level humans they

00:38:30.890 --> 00:38:30.900
high quality high-level humans they
 

00:38:30.900 --> 00:38:33.940
high quality high-level humans they
record all of that training data and

00:38:33.940 --> 00:38:33.950
record all of that training data and
 

00:38:33.950 --> 00:38:37.190
record all of that training data and
then they use that as input to train a

00:38:37.190 --> 00:38:37.200
then they use that as input to train a
 

00:38:37.200 --> 00:38:39.050
then they use that as input to train a
supervised learning problem on that

00:38:39.050 --> 00:38:39.060
supervised learning problem on that
 

00:38:39.060 --> 00:38:41.360
supervised learning problem on that
policy network so this again takes us

00:38:41.360 --> 00:38:41.370
policy network so this again takes us
 

00:38:41.370 --> 00:38:44.570
policy network so this again takes us
input in action sorry it excuse me it

00:38:44.570 --> 00:38:44.580
input in action sorry it excuse me it
 

00:38:44.580 --> 00:38:45.890
input in action sorry it excuse me it
takes us input the state of the board

00:38:45.890 --> 00:38:45.900
takes us input the state of the board
 

00:38:45.900 --> 00:38:48.980
takes us input the state of the board
and it tries to maximize the probability

00:38:48.980 --> 00:38:48.990
and it tries to maximize the probability
 

00:38:48.990 --> 00:38:51.590
and it tries to maximize the probability
of executing the action that the human

00:38:51.590 --> 00:38:51.600
of executing the action that the human
 

00:38:51.600 --> 00:38:54.320
of executing the action that the human
output or sorry executing the action

00:38:54.320 --> 00:38:54.330
output or sorry executing the action
 

00:38:54.330 --> 00:38:58.550
output or sorry executing the action
that the human executed okay so now this

00:38:58.550 --> 00:38:58.560
that the human executed okay so now this
 

00:38:58.560 --> 00:39:02.030
that the human executed okay so now this
first step here is focusing on building

00:39:02.030 --> 00:39:02.040
first step here is focusing on building
 

00:39:02.040 --> 00:39:04.490
first step here is focusing on building
a model using supervised learning to

00:39:04.490 --> 00:39:04.500
a model using supervised learning to
 

00:39:04.500 --> 00:39:08.090
a model using supervised learning to
imitate what the humans how the humans

00:39:08.090 --> 00:39:08.100
imitate what the humans how the humans
 

00:39:08.100 --> 00:39:10.280
imitate what the humans how the humans
played the game of go of course this is

00:39:10.280 --> 00:39:10.290
played the game of go of course this is
 

00:39:10.290 --> 00:39:12.200
played the game of go of course this is
not going to surpass any human level

00:39:12.200 --> 00:39:12.210
not going to surpass any human level
 

00:39:12.210 --> 00:39:14.090
not going to surpass any human level
performance because you're purely doing

00:39:14.090 --> 00:39:14.100
performance because you're purely doing
 

00:39:14.100 --> 00:39:17.210
performance because you're purely doing
imitation learning so the next step of

00:39:17.210 --> 00:39:17.220
imitation learning so the next step of
 

00:39:17.220 --> 00:39:19.040
imitation learning so the next step of
this algorithm is then to use that

00:39:19.040 --> 00:39:19.050
this algorithm is then to use that
 

00:39:19.050 --> 00:39:20.210
this algorithm is then to use that
network which you trained using

00:39:20.210 --> 00:39:20.220
network which you trained using
 

00:39:20.220 --> 00:39:23.090
network which you trained using
supervised learning and to pit it

00:39:23.090 --> 00:39:23.100
supervised learning and to pit it
 

00:39:23.100 --> 00:39:26.090
supervised learning and to pit it
against itself in games of self play so

00:39:26.090 --> 00:39:26.100
against itself in games of self play so
 

00:39:26.100 --> 00:39:27.500
against itself in games of self play so
you're gonna basically make two copies

00:39:27.500 --> 00:39:27.510
you're gonna basically make two copies
 

00:39:27.510 --> 00:39:30.080
you're gonna basically make two copies
of this network and play one network

00:39:30.080 --> 00:39:30.090
of this network and play one network
 

00:39:30.090 --> 00:39:33.350
of this network and play one network
against itself and use now reinforcement

00:39:33.350 --> 00:39:33.360
against itself and use now reinforcement
 

00:39:33.360 --> 00:39:35.320
against itself and use now reinforcement
learning to achieve superhuman

00:39:35.320 --> 00:39:35.330
learning to achieve superhuman
 

00:39:35.330 --> 00:39:37.190
learning to achieve superhuman
performance so now since its play

00:39:37.190 --> 00:39:37.200
performance so now since its play
 

00:39:37.200 --> 00:39:38.630
performance so now since its play
against itself and not it's not

00:39:38.630 --> 00:39:38.640
against itself and not it's not
 

00:39:38.640 --> 00:39:41.330
against itself and not it's not
receiving human input its able to

00:39:41.330 --> 00:39:41.340
receiving human input its able to
 

00:39:41.340 --> 00:39:43.790
receiving human input its able to
discover new possible actions that the

00:39:43.790 --> 00:39:43.800
discover new possible actions that the
 

00:39:43.800 --> 00:39:46.370
discover new possible actions that the
human may not have thought of that may

00:39:46.370 --> 00:39:46.380
human may not have thought of that may
 

00:39:46.380 --> 00:39:48.940
human may not have thought of that may
result in even higher reward than before

00:39:48.940 --> 00:39:48.950
result in even higher reward than before
 

00:39:48.950 --> 00:39:53.510
result in even higher reward than before
and finally the third step here is that

00:39:53.510 --> 00:39:53.520
and finally the third step here is that
 

00:39:53.520 --> 00:39:55.460
and finally the third step here is that
you want to build another network at

00:39:55.460 --> 00:39:55.470
you want to build another network at
 

00:39:55.470 --> 00:39:59.120
you want to build another network at
each time at each position on the board

00:39:59.120 --> 00:39:59.130
each time at each position on the board
 

00:39:59.130 --> 00:40:01.250
each time at each position on the board
so it takes now

00:40:01.250 --> 00:40:01.260
so it takes now
 

00:40:01.260 --> 00:40:03.560
so it takes now
the state of the board and tries to

00:40:03.560 --> 00:40:03.570
the state of the board and tries to
 

00:40:03.570 --> 00:40:05.270
the state of the board and tries to
learn the value function and that's

00:40:05.270 --> 00:40:05.280
learn the value function and that's
 

00:40:05.280 --> 00:40:07.070
learn the value function and that's
essentially very similar to the cue

00:40:07.070 --> 00:40:07.080
essentially very similar to the cue
 

00:40:07.080 --> 00:40:09.320
essentially very similar to the cue
function except now it's outputting one

00:40:09.320 --> 00:40:09.330
function except now it's outputting one
 

00:40:09.330 --> 00:40:11.210
function except now it's outputting one
output which is just the maximum Q

00:40:11.210 --> 00:40:11.220
output which is just the maximum Q
 

00:40:11.220 --> 00:40:14.480
output which is just the maximum Q
function so over all actions this this

00:40:14.480 --> 00:40:14.490
function so over all actions this this
 

00:40:14.490 --> 00:40:16.280
function so over all actions this this
value network is basically telling you

00:40:16.280 --> 00:40:16.290
value network is basically telling you
 

00:40:16.290 --> 00:40:18.620
value network is basically telling you
how good of a board state is this so

00:40:18.620 --> 00:40:18.630
how good of a board state is this so
 

00:40:18.630 --> 00:40:20.660
how good of a board state is this so
this gives us an intuition we're using

00:40:20.660 --> 00:40:20.670
this gives us an intuition we're using
 

00:40:20.670 --> 00:40:22.130
this gives us an intuition we're using
neural networks now to give us an

00:40:22.130 --> 00:40:22.140
neural networks now to give us an
 

00:40:22.140 --> 00:40:25.160
neural networks now to give us an
intuition as to what board states are

00:40:25.160 --> 00:40:25.170
intuition as to what board states are
 

00:40:25.170 --> 00:40:28.010
intuition as to what board states are
desirable what support states may result

00:40:28.010 --> 00:40:28.020
desirable what support states may result
 

00:40:28.020 --> 00:40:30.320
desirable what support states may result
in higher values or more probability of

00:40:30.320 --> 00:40:30.330
in higher values or more probability of
 

00:40:30.330 --> 00:40:33.410
in higher values or more probability of
winning and you might notice that this

00:40:33.410 --> 00:40:33.420
winning and you might notice that this
 

00:40:33.420 --> 00:40:36.410
winning and you might notice that this
is very closely related to the Q

00:40:36.410 --> 00:40:36.420
is very closely related to the Q
 

00:40:36.420 --> 00:40:40.430
is very closely related to the Q
function this is telling us like I said

00:40:40.430 --> 00:40:40.440
function this is telling us like I said
 

00:40:40.440 --> 00:40:42.530
function this is telling us like I said
at the core how desirable a given board

00:40:42.530 --> 00:40:42.540
at the core how desirable a given board
 

00:40:42.540 --> 00:40:45.710
at the core how desirable a given board
state is and basically just by looking

00:40:45.710 --> 00:40:45.720
state is and basically just by looking
 

00:40:45.720 --> 00:40:47.120
state is and basically just by looking
at the board state we want to determine

00:40:47.120 --> 00:40:47.130
at the board state we want to determine
 

00:40:47.130 --> 00:40:49.130
at the board state we want to determine
is this a good board state or a bad

00:40:49.130 --> 00:40:49.140
is this a good board state or a bad
 

00:40:49.140 --> 00:40:51.170
is this a good board state or a bad
sport state and the way that alphago

00:40:51.170 --> 00:40:51.180
sport state and the way that alphago
 

00:40:51.180 --> 00:40:54.410
sport state and the way that alphago
uses this is it then uses this value

00:40:54.410 --> 00:40:54.420
uses this is it then uses this value
 

00:40:54.420 --> 00:40:57.250
uses this is it then uses this value
network to understand which parts of the

00:40:57.250 --> 00:40:57.260
network to understand which parts of the
 

00:40:57.260 --> 00:41:00.620
network to understand which parts of the
game it should focus more on so if it's

00:41:00.620 --> 00:41:00.630
game it should focus more on so if it's
 

00:41:00.630 --> 00:41:03.500
game it should focus more on so if it's
at a particular state where it knows

00:41:03.500 --> 00:41:03.510
at a particular state where it knows
 

00:41:03.510 --> 00:41:06.010
at a particular state where it knows
that this is not a good state to be in

00:41:06.010 --> 00:41:06.020
that this is not a good state to be in
 

00:41:06.020 --> 00:41:08.060
that this is not a good state to be in
it knows that it shouldn't keep

00:41:08.060 --> 00:41:08.070
it knows that it shouldn't keep
 

00:41:08.070 --> 00:41:10.250
it knows that it shouldn't keep
executing trials down the state so it's

00:41:10.250 --> 00:41:10.260
executing trials down the state so it's
 

00:41:10.260 --> 00:41:12.800
executing trials down the state so it's
able to actually prune itself and have a

00:41:12.800 --> 00:41:12.810
able to actually prune itself and have a
 

00:41:12.810 --> 00:41:14.540
able to actually prune itself and have a
bit of a more intelligent algorithm of

00:41:14.540 --> 00:41:14.550
bit of a more intelligent algorithm of
 

00:41:14.550 --> 00:41:16.490
bit of a more intelligent algorithm of
learning by not executing all possible

00:41:16.490 --> 00:41:16.500
learning by not executing all possible
 

00:41:16.500 --> 00:41:19.310
learning by not executing all possible
actions at every time step but by kind

00:41:19.310 --> 00:41:19.320
actions at every time step but by kind
 

00:41:19.320 --> 00:41:21.410
actions at every time step but by kind
of building a smart tree of actions

00:41:21.410 --> 00:41:21.420
of building a smart tree of actions
 

00:41:21.420 --> 00:41:23.990
of building a smart tree of actions
based on this heuristic learned by

00:41:23.990 --> 00:41:24.000
based on this heuristic learned by
 

00:41:24.000 --> 00:41:27.800
based on this heuristic learned by
another network and the result is

00:41:27.800 --> 00:41:27.810
another network and the result is
 

00:41:27.810 --> 00:41:31.520
another network and the result is
alphago which in 2016 beat lee sedol

00:41:31.520 --> 00:41:31.530
alphago which in 2016 beat lee sedol
 

00:41:31.530 --> 00:41:35.390
alphago which in 2016 beat lee sedol
which is who's the top human player at

00:41:35.390 --> 00:41:35.400
which is who's the top human player at
 

00:41:35.400 --> 00:41:38.600
which is who's the top human player at
go and this was the first time that an

00:41:38.600 --> 00:41:38.610
go and this was the first time that an
 

00:41:38.610 --> 00:41:42.490
go and this was the first time that an
AI algorithm has beaten top human

00:41:42.490 --> 00:41:42.500
AI algorithm has beaten top human
 

00:41:42.500 --> 00:41:44.960
AI algorithm has beaten top human
performers in the game of go this was

00:41:44.960 --> 00:41:44.970
performers in the game of go this was
 

00:41:44.970 --> 00:41:47.480
performers in the game of go this was
really a groundbreaking moment it ties

00:41:47.480 --> 00:41:47.490
really a groundbreaking moment it ties
 

00:41:47.490 --> 00:41:49.820
really a groundbreaking moment it ties
us back to that trailer that we saw at

00:41:49.820 --> 00:41:49.830
us back to that trailer that we saw at
 

00:41:49.830 --> 00:41:53.090
us back to that trailer that we saw at
the beginning of class and finally just

00:41:53.090 --> 00:41:53.100
the beginning of class and finally just
 

00:41:53.100 --> 00:41:55.850
the beginning of class and finally just
to summarize some really exciting new

00:41:55.850 --> 00:41:55.860
to summarize some really exciting new
 

00:41:55.860 --> 00:41:59.540
to summarize some really exciting new
work from the same team that created

00:41:59.540 --> 00:41:59.550
work from the same team that created
 

00:41:59.550 --> 00:42:03.230
work from the same team that created
alphago now they've released a new model

00:42:03.230 --> 00:42:03.240
alphago now they've released a new model
 

00:42:03.240 --> 00:42:04.970
alphago now they've released a new model
called alpha zero which was published in

00:42:04.970 --> 00:42:04.980
called alpha zero which was published in
 

00:42:04.980 --> 00:42:08.450
called alpha zero which was published in
science about one month ago and it's a

00:42:08.450 --> 00:42:08.460
science about one month ago and it's a
 

00:42:08.460 --> 00:42:11.240
science about one month ago and it's a
general framework for learning self play

00:42:11.240 --> 00:42:11.250
general framework for learning self play
 

00:42:11.250 --> 00:42:14.240
general framework for learning self play
models of board games so they show it

00:42:14.240 --> 00:42:14.250
models of board games so they show it
 

00:42:14.250 --> 00:42:14.980
models of board games so they show it
being

00:42:14.980 --> 00:42:14.990
being
 

00:42:14.990 --> 00:42:18.280
being
to outperform top model-based approaches

00:42:18.280 --> 00:42:18.290
to outperform top model-based approaches
 

00:42:18.290 --> 00:42:21.660
to outperform top model-based approaches
and top human players in games of chess

00:42:21.660 --> 00:42:21.670
and top human players in games of chess
 

00:42:21.670 --> 00:42:26.230
and top human players in games of chess
shogi and alphago so it's actually able

00:42:26.230 --> 00:42:26.240
shogi and alphago so it's actually able
 

00:42:26.240 --> 00:42:28.690
shogi and alphago so it's actually able
to surpass the performance of alphago in

00:42:28.690 --> 00:42:28.700
to surpass the performance of alphago in
 

00:42:28.700 --> 00:42:31.750
to surpass the performance of alphago in
just 40 hours and now this network alpha

00:42:31.750 --> 00:42:31.760
just 40 hours and now this network alpha
 

00:42:31.760 --> 00:42:33.160
just 40 hours and now this network alpha
zero is called alpha zero because it

00:42:33.160 --> 00:42:33.170
zero is called alpha zero because it
 

00:42:33.170 --> 00:42:36.460
zero is called alpha zero because it
requires no prior knowledge of human

00:42:36.460 --> 00:42:36.470
requires no prior knowledge of human
 

00:42:36.470 --> 00:42:38.590
requires no prior knowledge of human
players it's learned entirely using

00:42:38.590 --> 00:42:38.600
players it's learned entirely using
 

00:42:38.600 --> 00:42:40.840
players it's learned entirely using
reinforcement learning and self play and

00:42:40.840 --> 00:42:40.850
reinforcement learning and self play and
 

00:42:40.850 --> 00:42:42.640
reinforcement learning and self play and
that's kind of the remarkable thinkers

00:42:42.640 --> 00:42:42.650
that's kind of the remarkable thinkers
 

00:42:42.650 --> 00:42:44.890
that's kind of the remarkable thinkers
that without using any prior information

00:42:44.890 --> 00:42:44.900
that without using any prior information
 

00:42:44.900 --> 00:42:48.240
that without using any prior information
at all now we've shown it's possible to

00:42:48.240 --> 00:42:48.250
at all now we've shown it's possible to
 

00:42:48.250 --> 00:42:51.970
at all now we've shown it's possible to
one learn the possible or learn the

00:42:51.970 --> 00:42:51.980
one learn the possible or learn the
 

00:42:51.980 --> 00:42:54.070
one learn the possible or learn the
likely sets of moves that human players

00:42:54.070 --> 00:42:54.080
likely sets of moves that human players
 

00:42:54.080 --> 00:42:56.440
likely sets of moves that human players
would make and then the really

00:42:56.440 --> 00:42:56.450
would make and then the really
 

00:42:56.450 --> 00:42:58.570
would make and then the really
interesting thing is that it then kind

00:42:58.570 --> 00:42:58.580
interesting thing is that it then kind
 

00:42:58.580 --> 00:43:01.000
interesting thing is that it then kind
of starts to discard those moves in

00:43:01.000 --> 00:43:01.010
of starts to discard those moves in
 

00:43:01.010 --> 00:43:03.520
of starts to discard those moves in
favor of even better moves that humans

00:43:03.520 --> 00:43:03.530
favor of even better moves that humans
 

00:43:03.530 --> 00:43:05.950
favor of even better moves that humans
never really thought of making and the

00:43:05.950 --> 00:43:05.960
never really thought of making and the
 

00:43:05.960 --> 00:43:07.540
never really thought of making and the
really interesting thing is if you

00:43:07.540 --> 00:43:07.550
really interesting thing is if you
 

00:43:07.550 --> 00:43:10.000
really interesting thing is if you
follow this this evolution of the

00:43:10.000 --> 00:43:10.010
follow this this evolution of the
 

00:43:10.010 --> 00:43:13.780
follow this this evolution of the
rewards going up in time if you stop it

00:43:13.780 --> 00:43:13.790
rewards going up in time if you stop it
 

00:43:13.790 --> 00:43:16.000
rewards going up in time if you stop it
at any intermediate time especially on

00:43:16.000 --> 00:43:16.010
at any intermediate time especially on
 

00:43:16.010 --> 00:43:18.100
at any intermediate time especially on
the early parts of so if you stop in the

00:43:18.100 --> 00:43:18.110
the early parts of so if you stop in the
 

00:43:18.110 --> 00:43:19.450
the early parts of so if you stop in the
early training parts like for example

00:43:19.450 --> 00:43:19.460
early training parts like for example
 

00:43:19.460 --> 00:43:22.090
early training parts like for example
right here and you look at the behavior

00:43:22.090 --> 00:43:22.100
right here and you look at the behavior
 

00:43:22.100 --> 00:43:25.090
right here and you look at the behavior
of these agents they behave similar to

00:43:25.090 --> 00:43:25.100
of these agents they behave similar to
 

00:43:25.100 --> 00:43:27.390
of these agents they behave similar to
top human players so they especially the

00:43:27.390 --> 00:43:27.400
top human players so they especially the
 

00:43:27.400 --> 00:43:29.890
top human players so they especially the
execute the actions that they execute in

00:43:29.890 --> 00:43:29.900
execute the actions that they execute in
 

00:43:29.900 --> 00:43:32.800
execute the actions that they execute in
the initial board states like right when

00:43:32.800 --> 00:43:32.810
the initial board states like right when
 

00:43:32.810 --> 00:43:35.190
the initial board states like right when
the game starts they behave very similar

00:43:35.190 --> 00:43:35.200
the game starts they behave very similar
 

00:43:35.200 --> 00:43:38.320
the game starts they behave very similar
to top human players but then as

00:43:38.320 --> 00:43:38.330
to top human players but then as
 

00:43:38.330 --> 00:43:41.080
to top human players but then as
training continues and the agent starts

00:43:41.080 --> 00:43:41.090
training continues and the agent starts
 

00:43:41.090 --> 00:43:44.010
training continues and the agent starts
to discover new and new more and more

00:43:44.010 --> 00:43:44.020
to discover new and new more and more
 

00:43:44.020 --> 00:43:47.290
to discover new and new more and more
different advanced ways of executors

00:43:47.290 --> 00:43:47.300
different advanced ways of executors
 

00:43:47.300 --> 00:43:50.230
different advanced ways of executors
starting the game it's actually able to

00:43:50.230 --> 00:43:50.240
starting the game it's actually able to
 

00:43:50.240 --> 00:43:54.190
starting the game it's actually able to
create new policies that humans never

00:43:54.190 --> 00:43:54.200
create new policies that humans never
 

00:43:54.200 --> 00:43:56.650
create new policies that humans never
even considered and now alpha zero is

00:43:56.650 --> 00:43:56.660
even considered and now alpha zero is
 

00:43:56.660 --> 00:43:58.030
even considered and now alpha zero is
being used as almost a learning

00:43:58.030 --> 00:43:58.040
being used as almost a learning
 

00:43:58.040 --> 00:44:00.130
being used as almost a learning
mechanism for top human performers on

00:44:00.130 --> 00:44:00.140
mechanism for top human performers on
 

00:44:00.140 --> 00:44:02.860
mechanism for top human performers on
these new policies that can improve

00:44:02.860 --> 00:44:02.870
these new policies that can improve
 

00:44:02.870 --> 00:44:06.310
these new policies that can improve
humans even more I think this is a

00:44:06.310 --> 00:44:06.320
humans even more I think this is a
 

00:44:06.320 --> 00:44:07.660
humans even more I think this is a
really powerful technique because it

00:44:07.660 --> 00:44:07.670
really powerful technique because it
 

00:44:07.670 --> 00:44:09.580
really powerful technique because it
shows reinforcement learning being used

00:44:09.580 --> 00:44:09.590
shows reinforcement learning being used
 

00:44:09.590 --> 00:44:12.540
shows reinforcement learning being used
not just to pit humans against machines

00:44:12.540 --> 00:44:12.550
not just to pit humans against machines
 

00:44:12.550 --> 00:44:15.100
not just to pit humans against machines
but also as a teaching mechanism for

00:44:15.100 --> 00:44:15.110
but also as a teaching mechanism for
 

00:44:15.110 --> 00:44:17.860
but also as a teaching mechanism for
humans to discover new ways to execute

00:44:17.860 --> 00:44:17.870
humans to discover new ways to execute
 

00:44:17.870 --> 00:44:20.440
humans to discover new ways to execute
optimal policies in some of these games

00:44:20.440 --> 00:44:20.450
optimal policies in some of these games
 

00:44:20.450 --> 00:44:22.660
optimal policies in some of these games
and even going beyond games the ultimate

00:44:22.660 --> 00:44:22.670
and even going beyond games the ultimate
 

00:44:22.670 --> 00:44:23.650
and even going beyond games the ultimate
goal of course is to create

00:44:23.650 --> 00:44:23.660
goal of course is to create
 

00:44:23.660 --> 00:44:25.920
goal of course is to create
reinforcement learning agents that can

00:44:25.920 --> 00:44:25.930
reinforcement learning agents that can
 

00:44:25.930 --> 00:44:28.450
reinforcement learning agents that can
act in the real world

00:44:28.450 --> 00:44:28.460
act in the real world
 

00:44:28.460 --> 00:44:30.430
act in the real world
robotic reinforcement learning agents

00:44:30.430 --> 00:44:30.440
robotic reinforcement learning agents
 

00:44:30.440 --> 00:44:33.190
robotic reinforcement learning agents
not just in simulated board games but in

00:44:33.190 --> 00:44:33.200
not just in simulated board games but in
 

00:44:33.200 --> 00:44:35.560
not just in simulated board games but in
the real world with humans and help us

00:44:35.560 --> 00:44:35.570
the real world with humans and help us
 

00:44:35.570 --> 00:44:39.250
the real world with humans and help us
learn in this world as well so that's

00:44:39.250 --> 00:44:39.260
learn in this world as well so that's
 

00:44:39.260 --> 00:44:41.710
learn in this world as well so that's
all for reinforcement learning happy to

00:44:41.710 --> 00:44:41.720
all for reinforcement learning happy to
 

00:44:41.720 --> 00:44:44.230
all for reinforcement learning happy to
take any questions or will hand it off

00:44:44.230 --> 00:44:44.240
take any questions or will hand it off
 

00:44:44.240 --> 00:44:47.140
take any questions or will hand it off
Dava for the new frontiers of deep

00:44:47.140 --> 00:44:47.150
Dava for the new frontiers of deep
 

00:44:47.150 --> 00:44:51.410
Dava for the new frontiers of deep
learning in the next part okay thank you

00:44:51.410 --> 00:44:51.420
learning in the next part okay thank you
 

00:44:51.420 --> 00:44:54.659
learning in the next part okay thank you
[Applause]

