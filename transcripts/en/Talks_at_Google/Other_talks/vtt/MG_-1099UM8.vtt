WEBVTT
Kind: captions
Language: en

00:00:06.734 --> 00:00:08.112
OWEN: OK.

00:00:08.112 --> 00:00:10.760
Welcome to Authors@Google.

00:00:10.760 --> 00:00:14.940
Thanks everybody for coming.

00:00:14.940 --> 00:00:17.130
This is the second
talk I've arranged,

00:00:17.130 --> 00:00:22.020
and so I'm really happy
to have you all here.

00:00:22.020 --> 00:00:24.130
For the past 15
years, David Mindell

00:00:24.130 --> 00:00:27.000
has been combining engineering
and historical research

00:00:27.000 --> 00:00:29.940
to investigate the evolution
of humans' relationships

00:00:29.940 --> 00:00:31.360
to machines.

00:00:31.360 --> 00:00:33.430
His engineering work
has been primarily

00:00:33.430 --> 00:00:38.300
in the field of deep ocean
robotics and archaeology.

00:00:38.300 --> 00:00:41.810
He's founder and director of
MIT's Deep Water Archaeology

00:00:41.810 --> 00:00:44.690
Research Group, focused
on exploring the deepest

00:00:44.690 --> 00:00:47.180
parts of the world's oceans.

00:00:47.180 --> 00:00:50.470
As a research engineer at Woods
Hole Oceanographic Institute,

00:00:50.470 --> 00:00:53.250
he developed the control
system and pilot interface

00:00:53.250 --> 00:00:56.650
for the Jason
submersible vehicle.

00:00:56.650 --> 00:00:59.910
His most recent book, which you
will find scattered around--

00:00:59.910 --> 00:01:02.850
"Digital Apollo"-- explores
how human pilots and automated

00:01:02.850 --> 00:01:05.960
systems worked together
to put a man on the moon.

00:01:05.960 --> 00:01:08.400
His current research
involves the Laboratory

00:01:08.400 --> 00:01:10.400
for Automation,
Robotics and Society,

00:01:10.400 --> 00:01:13.140
which examines
human-machine relationships

00:01:13.140 --> 00:01:15.140
in extreme
environments, including

00:01:15.140 --> 00:01:18.440
the deep sea, but also space
flight, military robotics,

00:01:18.440 --> 00:01:20.960
aviation, and surgery.

00:01:20.960 --> 00:01:24.488
Here today to talk about some of
his latest work, David Mindell.

00:01:24.488 --> 00:01:27.469
[APPLAUSE]

00:01:27.469 --> 00:01:28.760
DAVID MINDELL: Thank you, Owen.

00:01:28.760 --> 00:01:31.340
Thanks for inviting me.

00:01:31.340 --> 00:01:32.493
Hello to everyone online.

00:01:35.570 --> 00:01:37.490
As Owen mentioned,
I've been working

00:01:37.490 --> 00:01:42.570
on robotics in extreme
environments for a long time.

00:01:42.570 --> 00:01:45.530
And the talk I'm
gonna give today

00:01:45.530 --> 00:01:50.440
is observations-- things
we can learn from autonomy

00:01:50.440 --> 00:01:52.460
and robotics in extreme
environments that

00:01:52.460 --> 00:01:57.440
are helpful to more terrestrial
and everyday environments

00:01:57.440 --> 00:02:01.270
in which, as you all know,
autonomy and robotics are

00:02:01.270 --> 00:02:01.890
coming.

00:02:01.890 --> 00:02:05.700
Driverless cars, robotic
surgery, practically

00:02:05.700 --> 00:02:08.539
any place where the
human relationship

00:02:08.539 --> 00:02:16.110
to a particular task is
changed or troubled by robots.

00:02:16.110 --> 00:02:18.545
This is a sort of
precis of a book I'm

00:02:18.545 --> 00:02:20.920
working on that will be out
next year called "Our Robots,

00:02:20.920 --> 00:02:24.460
Ourselves-- How Robots and
Autonomy are Transforming

00:02:24.460 --> 00:02:27.290
Human Experience"
from Viking Penguin.

00:02:27.290 --> 00:02:33.350
So this is a kind of 20,000
foot, or 250,000 mile overview,

00:02:33.350 --> 00:02:36.730
of that book.

00:02:36.730 --> 00:02:40.970
And I'll start with a headline
from the Wall Street Journal

00:02:40.970 --> 00:02:42.460
Online.

00:02:42.460 --> 00:02:45.230
This is about April 5, so
just about a month ago.

00:02:45.230 --> 00:02:47.820
"Navy Drones with a
Mind of Their Own."

00:02:47.820 --> 00:02:50.890
"Newly unveiled
technology runs on tablets

00:02:50.890 --> 00:02:53.380
and enables unmanned
aircraft," for the picture

00:02:53.380 --> 00:02:56.680
of this aircraft called
AACUS, which was sponsored by

00:02:56.680 --> 00:02:57.600
the Navy.

00:02:57.600 --> 00:03:00.960
And I'll come back to
this later on in the talk.

00:03:00.960 --> 00:03:02.480
But I want to just
sort of highlight

00:03:02.480 --> 00:03:05.250
the headline, "Navy Drones
with a Mind of Their Own."

00:03:05.250 --> 00:03:10.070
Kind of classic press coverage
of autonomous systems,

00:03:10.070 --> 00:03:13.930
encapsulating in those few words
all of the stereotypes and kind

00:03:13.930 --> 00:03:16.690
of media imagery
around autonomy, which

00:03:16.690 --> 00:03:19.040
is very broadly held not
just by reporters, but also

00:03:19.040 --> 00:03:21.106
by the public.

00:03:21.106 --> 00:03:23.950
Then I'm going to
back up a little bit

00:03:23.950 --> 00:03:26.230
and go back into
the undersea realm

00:03:26.230 --> 00:03:28.970
and start with this family tree
of vehicles which comes out

00:03:28.970 --> 00:03:31.150
of the Woods Hole
Oceanographic Institute.

00:03:31.150 --> 00:03:34.876
And it starts down at the
bottom with manned submersibles

00:03:34.876 --> 00:03:36.750
like Alvin, which you
may well have heard of.

00:03:36.750 --> 00:03:38.470
I'll say more about
that in a minute.

00:03:38.470 --> 00:03:40.370
Goes on to the
remote vehicles that

00:03:40.370 --> 00:03:42.592
are tethered and cabled
to the ship, like Jason.

00:03:42.592 --> 00:03:44.675
Owen mentioned that I
worked on Jason for a while.

00:03:44.675 --> 00:03:46.960
Some of the more advanced
versions of that.

00:03:46.960 --> 00:03:50.480
And then into these autonomous
systems, the REMUS, ABE, SeaBED

00:03:50.480 --> 00:03:50.980
Sentry.

00:03:50.980 --> 00:03:54.430
Actually Nereus was
just lost last week

00:03:54.430 --> 00:03:57.050
at about 10,000
meters-- it imploded.

00:03:57.050 --> 00:04:00.740
One of the reasons it's good
to build autonomous vehicles.

00:04:00.740 --> 00:04:05.290
The REMUS vehicles found the
wreckage of the Air France 447.

00:04:05.290 --> 00:04:08.564
SeaBED and ABE
and now Sentry are

00:04:08.564 --> 00:04:10.230
hard at work for the
science [INAUDIBLE]

00:04:10.230 --> 00:04:12.030
looking at hydrothermal vents.

00:04:12.030 --> 00:04:14.350
And the implication
of this family tree

00:04:14.350 --> 00:04:17.729
is there's a kind of autonomous
technological progress

00:04:17.729 --> 00:04:22.700
that moves from manned to remote
to unmanned, or autonomous.

00:04:22.700 --> 00:04:27.190
And that very much represents
the point of view that I had

00:04:27.190 --> 00:04:29.460
and that everyone I worked
with had in the '80s

00:04:29.460 --> 00:04:31.715
when we started building
the first remote vehicles.

00:04:31.715 --> 00:04:34.640
These were going to be
the wave of the future.

00:04:34.640 --> 00:04:37.437
Robots were going to supersede
human visits to the sea floor.

00:04:41.520 --> 00:04:45.990
Alvin at that time had been
diving since the early '60s.

00:04:45.990 --> 00:04:50.750
So already in the late '80s
it had 25 years of experience,

00:04:50.750 --> 00:04:55.235
and brought and still brings
three people to the sea floor

00:04:55.235 --> 00:04:58.650
down to about, these
days its 6,000 meters,

00:04:58.650 --> 00:05:02.430
and puts the human eyeballs
directly on the seafloor.

00:05:02.430 --> 00:05:05.940
And so you have that-- whoops.

00:05:05.940 --> 00:05:08.540
I don't want the
[? Boston Marriot ?]

00:05:08.540 --> 00:05:09.760
Cambridge-- whatever that is.

00:05:14.410 --> 00:05:18.530
So we have this human vehicle.

00:05:18.530 --> 00:05:20.880
And the scientist
who led Alvin's use

00:05:20.880 --> 00:05:23.355
as a scientific tool during
the late '60s and '70s

00:05:23.355 --> 00:05:26.640
was this guy Robert Ballard,
who I'm sure you've heard of.

00:05:26.640 --> 00:05:28.280
And in the early
'80s, he actually

00:05:28.280 --> 00:05:34.090
came to MIT to one
of my teachers--

00:05:34.090 --> 00:05:37.320
Tom Sheridan-- although he
wasn't nearly my teacher yet--

00:05:37.320 --> 00:05:40.045
and he helped develop
this idea of telepresence

00:05:40.045 --> 00:05:41.530
on the sea floor.

00:05:41.530 --> 00:05:43.520
No longer do we
need to send people.

00:05:43.520 --> 00:05:45.690
Now we have remote
fiber optic cables.

00:05:45.690 --> 00:05:48.700
There's a sort of garage that
maybe has some sonars on it,

00:05:48.700 --> 00:05:50.330
hangs off the ship,
and now you've

00:05:50.330 --> 00:05:53.310
got a little remote robot
floating around down here.

00:05:53.310 --> 00:05:54.890
And this image was
actually published

00:05:54.890 --> 00:05:59.250
in "National
Geographic" in 1981.

00:05:59.250 --> 00:06:03.492
Ballard said manned
submersibles are obsolete.

00:06:03.492 --> 00:06:05.908
Manned submersibles are doomed,
actually, is what he said.

00:06:05.908 --> 00:06:08.116
He was quoted in the "Cape
Cod Times" as saying that.

00:06:08.116 --> 00:06:10.560
Marked a big split
from the Alvin group.

00:06:10.560 --> 00:06:15.320
He went off and started his
own lab to pursue this vision.

00:06:15.320 --> 00:06:18.767
One of the early vehicles
from that vision,

00:06:18.767 --> 00:06:20.350
which is basically
just the towed sled

00:06:20.350 --> 00:06:24.330
part without the robot, was
actually the thing in 1985

00:06:24.330 --> 00:06:27.530
that discovered the
wreck of the Titanic.

00:06:27.530 --> 00:06:30.880
But most people think Alvin
discovered the Titanic.

00:06:30.880 --> 00:06:35.710
And the reason is because
they went back to the Titanic

00:06:35.710 --> 00:06:37.370
in 1986.

00:06:37.370 --> 00:06:42.142
And the configuration
was Alvin, with

00:06:42.142 --> 00:06:44.550
this little, teeny
remote robot Jason

00:06:44.550 --> 00:06:47.330
Junior-- it was kind of a
predecessor to the Jason robot.

00:06:47.330 --> 00:06:50.500
And Alvin would sit on
the deck of the Titanic

00:06:50.500 --> 00:06:53.690
and dive-- Jason
Junior would dive,

00:06:53.690 --> 00:06:55.680
remotely controlled
by human operator

00:06:55.680 --> 00:06:58.300
with a little Sony
Watchman-- if you ever

00:06:58.300 --> 00:07:00.330
remember what
those were-- camera

00:07:00.330 --> 00:07:03.920
and actually dive down the
grand staircase of the Titanic.

00:07:03.920 --> 00:07:06.630
And I have a chapter in the
book describing that expedition,

00:07:06.630 --> 00:07:08.710
because there was
literally this tension

00:07:08.710 --> 00:07:11.132
between the remote
robot people and Alvin

00:07:11.132 --> 00:07:14.110
people who were worried
about being superseded.

00:07:14.110 --> 00:07:18.950
In fact the crew of
the ship baked a cake

00:07:18.950 --> 00:07:22.742
with Ballard's quote that said
manned submersible are doomed.

00:07:22.742 --> 00:07:24.700
And before they would
let him to dive in Alvin,

00:07:24.700 --> 00:07:27.410
they made him eat that
cake, literally forcing

00:07:27.410 --> 00:07:30.140
him to eat his words, as
though he was clawing back

00:07:30.140 --> 00:07:32.765
to use this, what they
found, very capable

00:07:32.765 --> 00:07:33.925
manned submersible.

00:07:33.925 --> 00:07:37.690
And then the remote
robot was added to that.

00:07:37.690 --> 00:07:39.800
And I put these two
images up, because they're

00:07:39.800 --> 00:07:43.110
interesting in that the "Time"
magazine image of the dive

00:07:43.110 --> 00:07:46.510
has only Alvin-- doesn't even
show the little robot Jason

00:07:46.510 --> 00:07:47.307
Junior.

00:07:47.307 --> 00:07:48.765
And the "National
Geographic" image

00:07:48.765 --> 00:07:51.925
which was illustrating an
article that Ballard wrote,

00:07:51.925 --> 00:07:54.995
has only Jason
Junior and no Alvin.

00:07:54.995 --> 00:07:57.370
So you can see there's two
very different interpretations

00:07:57.370 --> 00:07:58.735
of that event.

00:07:58.735 --> 00:08:00.690
And lest you think--
I mean, these already

00:08:00.690 --> 00:08:03.660
brought this event
to public prominence.

00:08:03.660 --> 00:08:06.500
But this scene, basically,
is the opening scene

00:08:06.500 --> 00:08:09.670
of the second most
highest-grossing film

00:08:09.670 --> 00:08:10.680
ever made.

00:08:10.680 --> 00:08:12.660
Jim Cameron's film of
"Titanic" reenacts,

00:08:12.660 --> 00:08:15.800
in a kind of Hollywood
version, this scene.

00:08:15.800 --> 00:08:18.515
So from the kind of late
'80s, early '90s you

00:08:18.515 --> 00:08:21.140
have this question being raised
sort of in the public discourse

00:08:21.140 --> 00:08:23.690
about what is the value
of remote presence

00:08:23.690 --> 00:08:27.110
versus physical presence
on the sea floor.

00:08:27.110 --> 00:08:30.505
I joined the group just a year
or two after that expedition

00:08:30.505 --> 00:08:33.230
and began working on
this vehicle, Jason,

00:08:33.230 --> 00:08:35.900
which has this light
fiber optic cable

00:08:35.900 --> 00:08:41.289
that brings electric power and
sends telemetry and video back

00:08:41.289 --> 00:08:46.000
up to the surface.

00:08:46.000 --> 00:08:48.990
And then later in the '90s, and
much more in the last 10 years,

00:08:48.990 --> 00:08:52.220
these autonomous vehicles
have come online.

00:08:52.220 --> 00:08:53.660
And what we found
through all this

00:08:53.660 --> 00:08:56.990
was that the advantages of the
remote and autonomous vehicles

00:08:56.990 --> 00:08:59.370
were not usually the
things we expected.

00:08:59.370 --> 00:09:02.940
For example, they weren't
cheaper and they weren't safer.

00:09:02.940 --> 00:09:06.460
But they did radically change
the nature of the work.

00:09:06.460 --> 00:09:07.970
With the remote
vehicle, it meant

00:09:07.970 --> 00:09:10.004
that rather than
having three people

00:09:10.004 --> 00:09:11.920
in the sphere of the
submersible-- one of whom

00:09:11.920 --> 00:09:15.210
was a pilot, two of whom
were scientist observers--

00:09:15.210 --> 00:09:17.290
they would go down
in the morning,

00:09:17.290 --> 00:09:19.160
spend all day on the
bottom, come back up,

00:09:19.160 --> 00:09:21.690
tell everybody in a
meeting what they saw.

00:09:21.690 --> 00:09:24.340
With Jason you had the
entire science party--

00:09:24.340 --> 00:09:28.090
maybe 40 people-- watching on
the surface ship in real time.

00:09:28.090 --> 00:09:30.710
And it was a very different
kind of collective scientific

00:09:30.710 --> 00:09:32.960
experience to do
that exploration,

00:09:32.960 --> 00:09:35.444
with really profound
implications.

00:09:35.444 --> 00:09:36.860
And it gave you a
tremendous sense

00:09:36.860 --> 00:09:39.270
of presence on the
seafloor, not least of it

00:09:39.270 --> 00:09:42.160
from all the other people who
you were sharing experience

00:09:42.160 --> 00:09:44.580
with, and different
scientists telling you

00:09:44.580 --> 00:09:46.540
what you were seeing.

00:09:46.540 --> 00:09:49.410
In the case of shipwrecks we've
worked with archaeologists.

00:09:49.410 --> 00:09:51.692
Similarly, the
autonomous systems--

00:09:51.692 --> 00:09:53.400
the original idea was
you'd send them out

00:09:53.400 --> 00:09:55.316
and they'd go do the
thing and then come back.

00:09:55.316 --> 00:09:57.190
But parallel with
this was the rise

00:09:57.190 --> 00:09:59.800
of the technology
of acoustic modems.

00:09:59.800 --> 00:10:04.440
And even if you could only get
low baud rate text messages,

00:10:04.440 --> 00:10:06.900
you still always wanted to
stay in touch with this thing

00:10:06.900 --> 00:10:08.480
as much as you can.

00:10:08.480 --> 00:10:11.340
But that's since developed in
a much more sophisticated sense

00:10:11.340 --> 00:10:14.410
of-- autonomous vehicles
in the ocean have,

00:10:14.410 --> 00:10:16.070
depending on where
they are, because

00:10:16.070 --> 00:10:18.020
of the nature of the
acoustic channels,

00:10:18.020 --> 00:10:21.320
you have very different levels
of bandwidth moment to moment,

00:10:21.320 --> 00:10:21.820
almost.

00:10:21.820 --> 00:10:27.114
And now Woods Hole is
developing optical modems

00:10:27.114 --> 00:10:28.530
that are almost
like streetlights.

00:10:28.530 --> 00:10:33.750
If you picture one of these
lights kind of hanging down

00:10:33.750 --> 00:10:36.370
and then any time you're within
seeing range of that light

00:10:36.370 --> 00:10:39.560
you can get 10 megabits per
second of data back and forth,

00:10:39.560 --> 00:10:41.200
and then you may swim away.

00:10:41.200 --> 00:10:43.060
And that, so you're
really always

00:10:43.060 --> 00:10:47.620
in this mode of sort of
autonomous and sort of remotely

00:10:47.620 --> 00:10:48.880
controlled.

00:10:48.880 --> 00:10:50.429
Then you zoom out
a little bit, as we

00:10:50.429 --> 00:10:51.970
had to with these
vehicles, and think

00:10:51.970 --> 00:10:53.400
about what's really going on.

00:10:53.400 --> 00:10:55.340
You have a ship out
there on the ocean,

00:10:55.340 --> 00:10:58.350
and the vehicle dives maybe
for a day and comes back,

00:10:58.350 --> 00:11:01.636
and then you take off the data,
and you recharge batteries,

00:11:01.636 --> 00:11:02.760
and send it back out again.

00:11:02.760 --> 00:11:05.360
Maybe even have two or three
of those vehicles working.

00:11:05.360 --> 00:11:08.210
And so what you really have
is a collaborative system

00:11:08.210 --> 00:11:11.247
with a manned vehicle--
i.e., a ship-- one

00:11:11.247 --> 00:11:12.830
of the oldest, most
traditional manned

00:11:12.830 --> 00:11:15.960
vehicles we've ever had--
and an autonomous vehicle.

00:11:15.960 --> 00:11:20.290
And what you call autonomy
is really fairly well bounded

00:11:20.290 --> 00:11:23.450
between these episodes
of human intervention.

00:11:23.450 --> 00:11:26.240
So that's one of
the early lessons.

00:11:26.240 --> 00:11:30.090
And this image was actually
Andy Bowen's image--

00:11:30.090 --> 00:11:32.150
who was the designer of
the Nereus vehicle, which

00:11:32.150 --> 00:11:35.560
as I mentioned was
just lost last week--

00:11:35.560 --> 00:11:40.630
captures some of that fuzzy
world where what's human,

00:11:40.630 --> 00:11:43.720
what's remote, what's autonomous
is constantly blurring.

00:11:43.720 --> 00:11:45.580
And these are not
categories that

00:11:45.580 --> 00:11:48.840
are really all that
useful to us anymore.

00:11:48.840 --> 00:11:51.540
Again, you have the
acoustics modem here doing

00:11:51.540 --> 00:11:55.465
a survey, a kind of optical
spotlight kind of modem,

00:11:55.465 --> 00:11:57.423
other kinds of modalities,
maybe there's a buoy

00:11:57.423 --> 00:11:58.600
and you exchange with light.

00:11:58.600 --> 00:12:02.440
And there's a whole very rich
period right now experimenting

00:12:02.440 --> 00:12:06.080
with all these
different combinations.

00:12:06.080 --> 00:12:09.060
So lesson number
one is just that

00:12:09.060 --> 00:12:12.660
human remote and autonomous
systems are evolving together.

00:12:12.660 --> 00:12:16.135
You don't have this kind of
replacement of human systems

00:12:16.135 --> 00:12:18.150
by autonomous systems.

00:12:18.150 --> 00:12:20.390
And they all interact
and affect each other.

00:12:20.390 --> 00:12:22.970
For example, there's a new
Alvin just came out, just

00:12:22.970 --> 00:12:24.750
finished its sea
trials actually.

00:12:24.750 --> 00:12:27.350
Fully upgraded, has a
new pressure sphere.

00:12:27.350 --> 00:12:30.010
All the electronics and the
software in the new Alvin

00:12:30.010 --> 00:12:32.940
are taken out of the
autonomous vehicles, right?

00:12:32.940 --> 00:12:35.750
So all that software,
all the navigation,

00:12:35.750 --> 00:12:39.840
all the interesting sort of slam
modeling of the sea floor that

00:12:39.840 --> 00:12:42.550
gets done now gets them
inside the sphere in Alvin.

00:12:42.550 --> 00:12:44.136
Changes what it
means to be in Alvin.

00:12:44.136 --> 00:12:46.510
Changes what it means to be
physically present on the sea

00:12:46.510 --> 00:12:48.080
floor.

00:12:48.080 --> 00:12:52.570
But how is Alvin not
a autonomous vehicle

00:12:52.570 --> 00:12:53.911
that you sit inside?

00:12:53.911 --> 00:12:54.410
OK?

00:12:54.410 --> 00:12:57.100
And that's a bigger question
for a lot of other arenas.

00:12:57.100 --> 00:13:00.880
How is a modern jet liner
not an autonomous vehicle

00:13:00.880 --> 00:13:02.044
that you sit inside?

00:13:02.044 --> 00:13:03.460
And those questions
are constantly

00:13:03.460 --> 00:13:06.160
coming up in the
aviation domain as well.

00:13:06.160 --> 00:13:10.030
So what the book does, it
starts with the undersea story,

00:13:10.030 --> 00:13:13.650
but then goes into three other
extreme environments really.

00:13:13.650 --> 00:13:18.319
One is remote warfare with the
Predator and Reaper operators

00:13:18.319 --> 00:13:20.860
who are doing things very very
similar to what we learned how

00:13:20.860 --> 00:13:22.812
to do undersea, although
the implications are

00:13:22.812 --> 00:13:23.892
quite different.

00:13:23.892 --> 00:13:25.769
Then in the aviation
domain, and what's

00:13:25.769 --> 00:13:28.185
happening in commercial aviation
with different technology

00:13:28.185 --> 00:13:30.720
in the cockpits--
heads-up displays,

00:13:30.720 --> 00:13:32.920
particularly synthetic
vision displays.

00:13:32.920 --> 00:13:36.185
How is this changing the roles
of what the pilots are doing?

00:13:36.185 --> 00:13:38.040
How might it change
in the future?

00:13:38.040 --> 00:13:39.540
What are the cautions
we've learned?

00:13:39.540 --> 00:13:41.665
Because there have been
some really scary accidents

00:13:41.665 --> 00:13:43.050
around that.

00:13:43.050 --> 00:13:45.956
And then into the remote
spaceflight realm, particularly

00:13:45.956 --> 00:13:48.521
looking at the Apollo
landings which I'll come to,

00:13:48.521 --> 00:13:50.710
which I've already
written about,

00:13:50.710 --> 00:13:53.320
and the Mars exploration
rovers where,

00:13:53.320 --> 00:13:56.670
even with a 20 minute
time delay, and relatively

00:13:56.670 --> 00:13:59.930
limited data bandwidth,
the scientists who

00:13:59.930 --> 00:14:02.800
work on that stuff still have
a tremendous sense of presence

00:14:02.800 --> 00:14:04.360
in that Martian terrain.

00:14:04.360 --> 00:14:07.540
So the sense of presence is
not exclusively dependent

00:14:07.540 --> 00:14:11.390
on Hi-Def video and
real time feedback.

00:14:11.390 --> 00:14:13.520
So the overall goal
is to move beyond some

00:14:13.520 --> 00:14:15.670
of what I'm calling these
kind of 20th century

00:14:15.670 --> 00:14:19.350
ideas of present versus
not present, manned versus

00:14:19.350 --> 00:14:23.760
unmanned, human operated
versus autonomous,

00:14:23.760 --> 00:14:27.560
but rather thinking about mixes
of all these things, new ways

00:14:27.560 --> 00:14:29.949
of being present,
and the questions I'm

00:14:29.949 --> 00:14:31.865
interested in asking is,
where are the people?

00:14:31.865 --> 00:14:33.420
What are they doing?

00:14:33.420 --> 00:14:35.790
When are they doing it-- in
many cases what you consider

00:14:35.790 --> 00:14:38.430
autonomy really has to
do with human intention

00:14:38.430 --> 00:14:41.020
and being uploaded
and programmed

00:14:41.020 --> 00:14:43.840
at an earlier point in
time before it's executed.

00:14:43.840 --> 00:14:46.120
Through what bandwidth,
and what time delays?

00:14:46.120 --> 00:14:49.012
And then the last one,
which is the kicker

00:14:49.012 --> 00:14:50.720
for a lot of the really
passionate stuff,

00:14:50.720 --> 00:14:53.530
is why does it matter and
why should anyone care?

00:14:53.530 --> 00:14:55.630
It turns out people care a lot.

00:14:55.630 --> 00:14:58.860
Even if the cognitive
experience of warfare operating

00:14:58.860 --> 00:15:01.780
through a remote
vehicle 6,000 miles away

00:15:01.780 --> 00:15:04.670
is very, very similar to
the cognitive experience

00:15:04.670 --> 00:15:07.060
of operating the
same sort of system

00:15:07.060 --> 00:15:09.470
from an aircraft at 35,000 feet.

00:15:09.470 --> 00:15:11.380
The physical
presence of the body

00:15:11.380 --> 00:15:15.750
has a huge cultural
weight around it.

00:15:15.750 --> 00:15:18.761
And for one, you get flight
time and you can receive combat

00:15:18.761 --> 00:15:20.260
medals, and for the
other you can't.

00:15:23.260 --> 00:15:25.607
And again-- and this is
sort of again extrapolating

00:15:25.607 --> 00:15:27.440
from the undersea world,
but there are many,

00:15:27.440 --> 00:15:29.380
many other examples
of it-- is that what

00:15:29.380 --> 00:15:32.940
we think of as
autonomy really needs

00:15:32.940 --> 00:15:35.190
to be rethought
of as opposed to,

00:15:35.190 --> 00:15:36.640
this is an autonomous vehicle.

00:15:36.640 --> 00:15:39.260
It's a robot in the kind
of science fiction sense

00:15:39.260 --> 00:15:41.800
and it's going to go out
there and do its own stuff.

00:15:41.800 --> 00:15:44.575
It's really-- autonomy is
something that's bounded,

00:15:44.575 --> 00:15:48.960
and it lives in the time domain
as a function of bandwidth

00:15:48.960 --> 00:15:52.040
and human context and the nature
of the task at any given point.

00:15:54.610 --> 00:15:56.490
That echoes a
report, for example,

00:15:56.490 --> 00:15:59.610
from the Defense
Science Board in 2013:

00:15:59.610 --> 00:16:02.650
"All autonomous systems are
joint human machine cognitive

00:16:02.650 --> 00:16:03.480
systems.

00:16:03.480 --> 00:16:05.470
There are no fully
autonomous systems,

00:16:05.470 --> 00:16:08.930
just as there are no fully
autonomous soldiers, sailors,

00:16:08.930 --> 00:16:09.920
airmen or marines."

00:16:09.920 --> 00:16:13.380
And then you can add to that
engineers, undersea explorers,

00:16:13.380 --> 00:16:17.220
astronauts automobile
drivers, doctors, surgeons--

00:16:17.220 --> 00:16:19.020
everybody's operating
within a network,

00:16:19.020 --> 00:16:21.680
and the key is to understand
what is that network,

00:16:21.680 --> 00:16:24.490
and how does it work?

00:16:24.490 --> 00:16:27.000
So I'll give you a little
excerpt from my previous book

00:16:27.000 --> 00:16:28.940
"Digital Apollo,"
that Owen mentioned,

00:16:28.940 --> 00:16:31.370
which illustrates some of this.

00:16:31.370 --> 00:16:34.190
And one of the real
observations from that system,

00:16:34.190 --> 00:16:36.370
although it's from many,
many others as well,

00:16:36.370 --> 00:16:40.380
is that when we think we have
a system that's autonomous

00:16:40.380 --> 00:16:43.010
they tend to work,
well, kind of notionally

00:16:43.010 --> 00:16:44.720
in a laboratory environment.

00:16:44.720 --> 00:16:46.990
R&amp;D loves autonomy.

00:16:46.990 --> 00:16:49.190
But as you get closer
to real environments,

00:16:49.190 --> 00:16:53.350
where there are real lives at
stake and resources at risk,

00:16:53.350 --> 00:16:56.630
human interventions tend to
be added at critical points.

00:16:56.630 --> 00:16:58.540
And so this is
one of the reasons

00:16:58.540 --> 00:17:00.300
I think that you
look at autonomy--

00:17:00.300 --> 00:17:04.140
and people used to say this
about AI-- sort of like, five

00:17:04.140 --> 00:17:07.660
years into the future, always
has been, always ever will.

00:17:07.660 --> 00:17:10.640
Because it's much
easier to imagine it

00:17:10.640 --> 00:17:13.220
in a demonstration
kind of setting.

00:17:13.220 --> 00:17:16.050
And this is not an
ontological argument.

00:17:16.050 --> 00:17:19.440
This is an empirical argument
from tens and tens of studies

00:17:19.440 --> 00:17:21.599
that I've done of different
autonomous systems

00:17:21.599 --> 00:17:24.710
over many decades,
which is that as lives

00:17:24.710 --> 00:17:27.359
are at stake, as the
stakes get raised,

00:17:27.359 --> 00:17:29.580
people naturally put
in different kinds

00:17:29.580 --> 00:17:30.700
of human approvals.

00:17:30.700 --> 00:17:33.130
Again what that means
is, the autonomy that we

00:17:33.130 --> 00:17:37.210
find in systems tends to be
bounded in the time domain.

00:17:37.210 --> 00:17:39.770
Not to say that things
aren't acting in certain ways

00:17:39.770 --> 00:17:44.230
automatically, but they tend to
be bounded in the time domain.

00:17:44.230 --> 00:17:47.150
Classic example is
the lunar landing

00:17:47.150 --> 00:17:49.280
this is-- the last
200 pages of my book

00:17:49.280 --> 00:17:51.537
are all about the design
of the last 10 minutes

00:17:51.537 --> 00:17:53.870
and the execution of the last
10 minutes of the landing,

00:17:53.870 --> 00:17:56.290
starting from 50,000 feet.

00:17:56.290 --> 00:17:58.590
You may well know that the
computers and the software

00:17:58.590 --> 00:18:01.650
for this system were designed
within a mile of this spot

00:18:01.650 --> 00:18:03.940
at what is now the Draper
Labs, what was then

00:18:03.940 --> 00:18:05.940
the MIT Instrumentation Lab.

00:18:05.940 --> 00:18:08.840
And when MIT first got the
contract they said, no problem!

00:18:08.840 --> 00:18:10.910
We can build a computer
to land on the moon.

00:18:10.910 --> 00:18:12.836
And NASA said,
what's the interface?

00:18:12.836 --> 00:18:14.210
And they said well
the interface,

00:18:14.210 --> 00:18:15.970
all it needs is
two buttons, right?

00:18:15.970 --> 00:18:18.100
One button is,
"Take me to moon,"

00:18:18.100 --> 00:18:20.170
and one button is,
"Take me home."

00:18:20.170 --> 00:18:20.670
OK.

00:18:20.670 --> 00:18:22.450
There again, that's
the original notion

00:18:22.450 --> 00:18:25.690
from the kind of
laboratory view out of what

00:18:25.690 --> 00:18:28.120
the autonomy of the
lunar landing could be.

00:18:28.120 --> 00:18:31.090
It turned out that that was
unacceptable for systems

00:18:31.090 --> 00:18:33.520
that had people on board.

00:18:33.520 --> 00:18:36.000
And, in the end, there
was a very, very rich set

00:18:36.000 --> 00:18:40.360
of interactions between
the human and the machine.

00:18:40.360 --> 00:18:42.260
Now interestingly,
another one of the lessons

00:18:42.260 --> 00:18:44.620
which I'll come to
appears here, which

00:18:44.620 --> 00:18:47.590
is that it was actually
technologically much

00:18:47.590 --> 00:18:50.420
easier to make a fully
autonomous landing on the moon

00:18:50.420 --> 00:18:53.840
than to make one that
carried a person in it.

00:18:53.840 --> 00:18:58.960
In fact, Apollo 12 landed
within 100 yards of Surveyor 3,

00:18:58.960 --> 00:19:01.490
which was an unmanned
spacecraft, a robot, that

00:19:01.490 --> 00:19:05.810
had landed before Neil Armstrong
landed on the moon under-- it

00:19:05.810 --> 00:19:07.780
was also sort of
remotely controlled,

00:19:07.780 --> 00:19:11.220
but it was perfectly easy
to do-- not perfectly easy--

00:19:11.220 --> 00:19:14.000
but it was, you're
perfectly capable to do

00:19:14.000 --> 00:19:18.190
a unmanned landing on the moon
and much simpler in many ways

00:19:18.190 --> 00:19:20.220
than a man landing on
the moon-- again, because

00:19:20.220 --> 00:19:23.420
of all the sort of
baggage that comes along,

00:19:23.420 --> 00:19:26.064
but also just the natural safety
and ethical considerations

00:19:26.064 --> 00:19:28.730
of what are you going to do with
a system that could potentially

00:19:28.730 --> 00:19:30.700
kill people.

00:19:30.700 --> 00:19:32.960
This image is actually
replayed on the cover

00:19:32.960 --> 00:19:36.840
of the book, although
not as clearly.

00:19:36.840 --> 00:19:39.460
And this is what the
lunar module cockpit

00:19:39.460 --> 00:19:41.950
looked like from Neil
Armstrong's point of view

00:19:41.950 --> 00:19:44.540
at the moment that he
reached up to turn off

00:19:44.540 --> 00:19:47.330
the automatic targeting
system that he had been given

00:19:47.330 --> 00:19:52.420
and land the spacecraft in a
kind of semi-automatic mode.

00:19:52.420 --> 00:19:55.700
This is the scene that opens the
book and sort of the whole book

00:19:55.700 --> 00:19:58.540
kind of orients around
why did this happen.

00:19:58.540 --> 00:20:01.050
This image is actually computer
graphic image made for me

00:20:01.050 --> 00:20:02.840
by a guy named
John Knoll, who is

00:20:02.840 --> 00:20:05.180
the chief animator at
Industrial Light and Magic.

00:20:05.180 --> 00:20:07.640
Also you may know him as
one of the two founders

00:20:07.640 --> 00:20:10.030
with his brother
of-- or creators

00:20:10.030 --> 00:20:12.990
of what became Adobe Photoshop.

00:20:12.990 --> 00:20:14.890
And it's a historical
document in that

00:20:14.890 --> 00:20:17.710
every switch and every light
and every piece of data

00:20:17.710 --> 00:20:20.740
represented there is as
accurate as we could make it.

00:20:20.740 --> 00:20:23.060
And you can see the
crater outside the moon.

00:20:23.060 --> 00:20:25.420
Armstrong said, "I
turned off the automatic

00:20:25.420 --> 00:20:29.770
targeting because the
computer was taking us

00:20:29.770 --> 00:20:34.770
into a rocky area,
and I needed to tell

00:20:34.770 --> 00:20:36.940
it to go to a smooth
spot that I could see."

00:20:36.940 --> 00:20:41.150
And that explanation, which is
echoed almost exactly verbatim

00:20:41.150 --> 00:20:43.975
on all of the following five
Apollo landings, where they all

00:20:43.975 --> 00:20:45.850
turned off the same
targeting system at right

00:20:45.850 --> 00:20:49.940
about the same spot, really
is an inadequate explanation

00:20:49.940 --> 00:20:51.870
for that, because the
automatic targeting

00:20:51.870 --> 00:20:56.180
system-- I don't
have an image of it--

00:20:56.180 --> 00:20:58.710
was designed to allow them
to interact with the computer

00:20:58.710 --> 00:21:01.800
and sort of set a landing set
point based on looking out

00:21:01.800 --> 00:21:02.720
the window.

00:21:02.720 --> 00:21:06.272
You can see these little
gradations just barely there.

00:21:06.272 --> 00:21:08.710
It was a kind of passive
heads-up display.

00:21:08.710 --> 00:21:12.470
And he could actually jog the
intended landing spot around

00:21:12.470 --> 00:21:14.852
with his joystick as
many times as he wanted.

00:21:14.852 --> 00:21:16.310
And every time he
did, the computer

00:21:16.310 --> 00:21:18.960
would recalculate a
parabolic trajectory

00:21:18.960 --> 00:21:20.370
and bring him to that spot.

00:21:20.370 --> 00:21:22.060
And that system was
designed so that it

00:21:22.060 --> 00:21:24.150
would converge on
a nice smooth area.

00:21:24.150 --> 00:21:25.990
That's the system he turned off.

00:21:25.990 --> 00:21:28.220
He ended up landing in
attitude hold mode, which

00:21:28.220 --> 00:21:29.950
had an automatic
rate of descent,

00:21:29.950 --> 00:21:31.609
so it was by no means manual.

00:21:31.609 --> 00:21:33.150
But the "New York
Times" the next day

00:21:33.150 --> 00:21:36.540
said all praise the human
intervention in the system.

00:21:36.540 --> 00:21:39.850
People still have a role to play
in the grand flow of history.

00:21:39.850 --> 00:21:42.410
Wonderful extrapolation
of the biggest thing

00:21:42.410 --> 00:21:43.730
from the smallest moment.

00:21:46.350 --> 00:21:50.190
And so here again you come to
this less than-- that greater

00:21:50.190 --> 00:21:53.470
autonomy is often an easier
problem than autonomy

00:21:53.470 --> 00:21:56.700
with rich human involvement
with a system, which requires

00:21:56.700 --> 00:21:58.490
more sophisticated technology.

00:21:58.490 --> 00:22:00.410
And a good example
of this from the '60s

00:22:00.410 --> 00:22:03.060
is the Soviet
spacecraft at the time

00:22:03.060 --> 00:22:06.410
all had analog
electronic control loops.

00:22:06.410 --> 00:22:08.520
And therefore they were
very highly automated.

00:22:08.520 --> 00:22:11.800
It was not easy to make those
control loops places where

00:22:11.800 --> 00:22:13.590
the human could
really intervene.

00:22:13.590 --> 00:22:16.520
It was a radical very
forward-looking step

00:22:16.520 --> 00:22:19.970
in the Apollo computer to
put integrated circuits

00:22:19.970 --> 00:22:22.110
and make it a general
purpose digital computer

00:22:22.110 --> 00:22:27.620
and have it software
programmable and all that.

00:22:27.620 --> 00:22:29.810
But one of the
things that enabled

00:22:29.810 --> 00:22:34.240
was a lot richer interaction
between the crews.

00:22:34.240 --> 00:22:36.700
And so I'll show you just
a little bit of one way

00:22:36.700 --> 00:22:38.845
I'm trying to learn how to
depict this graphically.

00:22:41.530 --> 00:22:47.300
And my post-doc and
Yanni Loukissas and I

00:22:47.300 --> 00:22:50.970
put together this app
which is a-- basically we

00:22:50.970 --> 00:22:53.930
got the raw downlink
from the LEM

00:22:53.930 --> 00:22:56.830
from one of the Draper engineers
who had it on microfilm

00:22:56.830 --> 00:22:59.490
in his basement, something
that NASA didn't have.

00:22:59.490 --> 00:23:02.810
We scanned it and OCR'd it, and
then reprocessed all the data,

00:23:02.810 --> 00:23:10.910
including the audio into this
sort of richer view of how

00:23:10.910 --> 00:23:13.010
the system actually worked.

00:23:13.010 --> 00:23:18.390
And what you're seeing here is
time goes from left to right,

00:23:18.390 --> 00:23:22.180
as does the distance across
the surface of the moon.

00:23:22.180 --> 00:23:26.470
And then the y-axis
is a log scale

00:23:26.470 --> 00:23:30.080
of altitude from the
surface of the moon.

00:23:30.080 --> 00:23:34.340
So the orange dots represent the
lunar module about 10 to the 4

00:23:34.340 --> 00:23:38.600
1/2 from the moon at that
50,000 foot parking orbit.

00:23:38.600 --> 00:23:43.110
The slightly less orange
dots are the Columbia,

00:23:43.110 --> 00:23:46.020
the service module, at
about 10 to the 5 1/2.

00:23:46.020 --> 00:23:49.700
And then 10 to the 9th feet
above the moon is Houston.

00:23:49.700 --> 00:23:52.210
And each of these dots
represents an utterance

00:23:52.210 --> 00:23:54.350
by the crew in one
of those spots,

00:23:54.350 --> 00:23:56.480
and then the lines
are links when

00:23:56.480 --> 00:23:59.390
they have a kind of call
and response relationship.

00:23:59.390 --> 00:24:01.910
And then, up here you
see the utterances

00:24:01.910 --> 00:24:04.480
from all the entire
ground control

00:24:04.480 --> 00:24:06.980
team, starting with the
flight director here.

00:24:06.980 --> 00:24:09.950
And so what this
little viewer allows

00:24:09.950 --> 00:24:14.230
you to do is sort
of play through

00:24:14.230 --> 00:24:17.560
and scan through-- and down at
the bottom here I should add,

00:24:17.560 --> 00:24:20.460
in fact, it's rare--
I have not yet

00:24:20.460 --> 00:24:22.890
given a talk on this where
the projector could actually

00:24:22.890 --> 00:24:26.440
display this part of the
screen, but yours happily

00:24:26.440 --> 00:24:29.480
does-- is the raw data from the
computer-- in this case it's

00:24:29.480 --> 00:24:32.460
pitch-- and these are
various other computer modes

00:24:32.460 --> 00:24:33.919
being indicated in the software.

00:24:33.919 --> 00:24:35.460
And then there's a
little interaction

00:24:35.460 --> 00:24:38.230
where you can kind of turn off
or turn on any of these pieces

00:24:38.230 --> 00:24:39.100
at any given point.

00:24:39.100 --> 00:24:40.016
[BEGIN VIDEO PLAYBACK]

00:24:40.016 --> 00:24:41.516
ASTRONAUT (ON
PLAYBACK) [INAUDIBLE].

00:24:41.516 --> 00:24:43.474
DAVID MINDELL: And you
can kinda play through--

00:24:43.474 --> 00:24:45.960
of course there's this
famous 1202 program alarm,

00:24:45.960 --> 00:24:48.580
and I won't go
through it all here,

00:24:48.580 --> 00:24:52.250
but you can play through the
interactions between the crew

00:24:52.250 --> 00:24:54.880
and the ground, and you can
see, among other things,

00:24:54.880 --> 00:24:57.060
the vehicle becomes
a lot more autonomous

00:24:57.060 --> 00:24:58.940
in the last few moments
before the landing,

00:24:58.940 --> 00:25:01.420
as the ground kind of shuts up.

00:25:01.420 --> 00:25:03.770
But Aldrin and Armstrong
were still very much talking

00:25:03.770 --> 00:25:05.720
to each other, and
the data's all still

00:25:05.720 --> 00:25:08.330
being downloaded to the
ground, and these are all

00:25:08.330 --> 00:25:12.156
their various
interpretations of the data.

00:25:12.156 --> 00:25:13.697
I'll just let this
play for a second.

00:25:13.697 --> 00:25:15.655
BUZZ ALDRIN (ON PLAYBACK):
40 feet, down 2 1/2.

00:25:15.655 --> 00:25:16.991
Picking up some dust.

00:25:16.991 --> 00:25:18.122
30 feet.

00:25:18.122 --> 00:25:21.400
2 1/2 down.

00:25:21.400 --> 00:25:22.705
Four forward.

00:25:22.705 --> 00:25:23.710
Four forward.

00:25:23.710 --> 00:25:25.045
Drifting to the right a little.

00:25:25.045 --> 00:25:27.289
[INAUDIBLE]

00:25:27.289 --> 00:25:28.768
30 second [INAUDIBLE].

00:25:35.670 --> 00:25:36.400
Contact light.

00:25:38.960 --> 00:25:39.460
OK.

00:25:39.460 --> 00:25:40.765
Engine stop.

00:25:40.765 --> 00:25:43.530
ACA out of detent.

00:25:43.530 --> 00:25:44.710
Mode control both auto.

00:25:44.710 --> 00:25:46.210
Descent engine
command override off.

00:25:46.210 --> 00:25:48.470
Engine arm off.

00:25:48.470 --> 00:25:49.793
413 is in.

00:25:53.034 --> 00:25:55.780
MISSION CONTROL (ON PLAYBACK):
We copy you down, Eagle.

00:25:55.780 --> 00:25:58.130
NEIL ARMSTRONG (ON
PLAYBACK): Houston.

00:25:58.130 --> 00:25:59.876
Tranquility Base here.

00:25:59.876 --> 00:26:01.205
The Eagle has landed.

00:26:01.205 --> 00:26:03.580
MISSION CONTROL (ON PLAYBACK):
Roger, twan-- tranquility.

00:26:03.580 --> 00:26:04.685
We copy you on the ground.

00:26:04.685 --> 00:26:06.750
You've got a bunch of
guys about to turn blue.

00:26:06.750 --> 00:26:07.955
We're breathing again!

00:26:07.955 --> 00:26:08.260
[END VIDEO PLAYBACK]

00:26:08.260 --> 00:26:08.940
DAVID MINDELL: So
that's the part

00:26:08.940 --> 00:26:10.440
that everybody's
most familiar with.

00:26:10.440 --> 00:26:14.070
But you can also see here, if
this is the pitched values here

00:26:14.070 --> 00:26:16.460
actually, this is where
Armstrong turns off

00:26:16.460 --> 00:26:18.980
the automatic
targeting, and then

00:26:18.980 --> 00:26:21.240
goes through some pretty
wild pitch gyrations--

00:26:21.240 --> 00:26:25.350
he himself called them spastic--
as he tried to kind of get

00:26:25.350 --> 00:26:27.870
control of the thing and
get over this crater.

00:26:27.870 --> 00:26:31.140
And I had never had-- actually
I produced this visualization

00:26:31.140 --> 00:26:32.164
after I wrote the book.

00:26:32.164 --> 00:26:33.830
So I didn't have it
when I did the book.

00:26:33.830 --> 00:26:36.930
But there was never really a
way before to kind of understand

00:26:36.930 --> 00:26:39.960
this data all coordinated
with the audio data

00:26:39.960 --> 00:26:41.150
and the trajectory data.

00:26:41.150 --> 00:26:43.010
And so it's just a
kind of early attempt

00:26:43.010 --> 00:26:46.060
to say, how do we
represent visually

00:26:46.060 --> 00:26:48.190
this rich sense of
autonomy, and how

00:26:48.190 --> 00:26:50.954
control is traded in
real time, back and forth

00:26:50.954 --> 00:26:51.870
through these systems.

00:26:51.870 --> 00:26:55.240
And then I'm also
interested in how

00:26:55.240 --> 00:26:57.260
does that help you think
about designing systems

00:26:57.260 --> 00:26:58.850
in the future.

00:26:58.850 --> 00:27:01.430
Also did a similar study
of heads-up displays

00:27:01.430 --> 00:27:05.180
in commercial airline
cockpits, which

00:27:05.180 --> 00:27:06.447
there's a lot of debate about.

00:27:06.447 --> 00:27:08.030
There's actually a
lot of debate about

00:27:08.030 --> 00:27:09.670
whether airlines
want to spend money

00:27:09.670 --> 00:27:12.720
on automatic landing, which
has been around since the '60s,

00:27:12.720 --> 00:27:15.710
and it's a pretty robust
system, but quite expensive,

00:27:15.710 --> 00:27:19.680
and keeps them rather
far out of the loop.

00:27:19.680 --> 00:27:22.609
These heads-up displays, they're
also not new as an invention,

00:27:22.609 --> 00:27:24.650
but they've come a long
way in the last 10 years,

00:27:24.650 --> 00:27:26.480
and they're becoming
very practical.

00:27:26.480 --> 00:27:29.130
And they give the pilots
a much better sense

00:27:29.130 --> 00:27:31.020
of being involved
with the energy

00:27:31.020 --> 00:27:33.590
state of the aircraft
at critical moments.

00:27:33.590 --> 00:27:36.870
And you can go back and look
at-- even the Air France

00:27:36.870 --> 00:27:39.480
crash-- but certainly
the Asiana crash

00:27:39.480 --> 00:27:42.380
that happened in San
Francisco last summer--

00:27:42.380 --> 00:27:46.790
there's a crash outside
of Amsterdam in 2009--

00:27:46.790 --> 00:27:48.530
any number of these
crashes-- there's

00:27:48.530 --> 00:27:50.800
a good argument that they
would have been avoided

00:27:50.800 --> 00:27:53.830
if the pilots had been looking
through the heads-up display.

00:27:53.830 --> 00:27:55.750
And the key thing there
is the head-up display

00:27:55.750 --> 00:27:58.900
is not simply a representation
of the data that's down below,

00:27:58.900 --> 00:28:01.090
but actually imposes
a flight path vector

00:28:01.090 --> 00:28:03.100
on the scene the
pilot's looking at,

00:28:03.100 --> 00:28:05.040
which is a kind of
aggregate-- actually

00:28:05.040 --> 00:28:08.690
one of the engineers who
created the code for it calls it

00:28:08.690 --> 00:28:10.620
"a manufactured
product," which I love,

00:28:10.620 --> 00:28:13.001
because it's an aggregate
of a lot of flight data

00:28:13.001 --> 00:28:14.750
that's filtered and
smoothed in such a way

00:28:14.750 --> 00:28:16.650
that a human can follow it.

00:28:16.650 --> 00:28:20.847
And if the pilot looks at
that flight path vector

00:28:20.847 --> 00:28:22.680
and puts it on the end
of the runway, that's

00:28:22.680 --> 00:28:24.810
where the airplane's
going to go, full stop.

00:28:24.810 --> 00:28:27.890
I mean, it's just
as simple as that.

00:28:27.890 --> 00:28:31.080
And amongst the pilots,
we did a whole project

00:28:31.080 --> 00:28:32.810
interviewing pilots
from European airline

00:28:32.810 --> 00:28:34.652
and American cargo airlines.

00:28:34.652 --> 00:28:36.860
There's a lot of debate
about whether that represents

00:28:36.860 --> 00:28:40.060
a different way to fly or
not, and whether it changes

00:28:40.060 --> 00:28:41.810
the nature of their
task, in what ways,

00:28:41.810 --> 00:28:45.880
and what the advantages
and disadvantages are.

00:28:45.880 --> 00:28:47.980
So I'll come back
now to this "Navy

00:28:47.980 --> 00:28:50.880
Drones with a Mind
of Their Own."

00:28:50.880 --> 00:28:53.300
And this is really
interesting to me

00:28:53.300 --> 00:28:57.220
because I was heavily
involved in this project.

00:28:57.220 --> 00:29:00.690
It's a project done by Aurora
Flight Sciences, who is just

00:29:00.690 --> 00:29:03.280
over here in the
next building, and it

00:29:03.280 --> 00:29:06.020
was a Office of Naval
Research Project.

00:29:06.020 --> 00:29:09.520
And the goal was to build a
full-size autonomous cargo

00:29:09.520 --> 00:29:10.410
helicopter.

00:29:10.410 --> 00:29:13.430
The chief of Naval
Research had, Amazon, I'll

00:29:13.430 --> 00:29:14.610
give you a two pound book.

00:29:14.610 --> 00:29:17.960
I'll give you 1,200
pounds of supplies.

00:29:17.960 --> 00:29:19.310
And that was the goal.

00:29:19.310 --> 00:29:22.700
And originally the sort
of assignment from ONR

00:29:22.700 --> 00:29:26.290
was make this thing fully
autonomous with absolutely

00:29:26.290 --> 00:29:29.540
minimal dependence on a
pilot or a human operator.

00:29:29.540 --> 00:29:34.920
And even just to get it to the
point of a demo, piece by piece

00:29:34.920 --> 00:29:37.700
that autonomy again
got punctuated

00:29:37.700 --> 00:29:39.000
with human approvals.

00:29:39.000 --> 00:29:40.430
So let me give
you an explanation

00:29:40.430 --> 00:29:43.260
of what this slide is.

00:29:43.260 --> 00:29:45.120
The vehicle's
intended to come in.

00:29:45.120 --> 00:29:48.480
It has a super duper
amazing LIDAR on it

00:29:48.480 --> 00:29:51.530
that was built for us by
Sanjiv Singh and his group

00:29:51.530 --> 00:29:52.800
at Carnegie Mellon.

00:29:52.800 --> 00:29:56.720
And basically can scan the area,
combine it with optical data,

00:29:56.720 --> 00:29:59.730
identify flat spots that
have certain characteristics

00:29:59.730 --> 00:30:03.570
conducive to the aircraft
landing, and then come in

00:30:03.570 --> 00:30:04.350
and land.

00:30:04.350 --> 00:30:09.140
And here's a low-resolution
image of the map

00:30:09.140 --> 00:30:12.820
that it might make, some
depictions of that LIDAR.

00:30:12.820 --> 00:30:16.210
And here again, to do
this fully autonomously

00:30:16.210 --> 00:30:17.670
is relatively straightforward.

00:30:17.670 --> 00:30:20.200
In fact, Sanjiv's group
at Carnegie Mellon

00:30:20.200 --> 00:30:22.920
had already done that
fully autonomously.

00:30:22.920 --> 00:30:26.040
But remember what you're looking
at in a real application.

00:30:26.040 --> 00:30:28.350
Here you've got a whole
group of people standing

00:30:28.350 --> 00:30:31.540
around, plus a whole--
maybe their little compound

00:30:31.540 --> 00:30:33.340
and their equipment--
and then you've

00:30:33.340 --> 00:30:39.290
got this 3,500, 4,500,
5,500 pound helicopter

00:30:39.290 --> 00:30:41.230
under autonomous
control screaming

00:30:41.230 --> 00:30:43.380
at them at full speed.

00:30:43.380 --> 00:30:45.780
And one of the assignments--
one of the specs

00:30:45.780 --> 00:30:49.070
was-- you couldn't come up and
sort of hover around and scan

00:30:49.070 --> 00:30:49.800
the thing.

00:30:49.800 --> 00:30:52.383
You had to come in at what they
call a tactical landing, which

00:30:52.383 --> 00:30:56.130
is basically 90 knots and a
very, very steep descent angle

00:30:56.130 --> 00:30:57.290
and very fast.

00:30:57.290 --> 00:30:59.520
Terrifying to the
people on the ground!

00:30:59.520 --> 00:31:01.630
They don't know what the
autonomy is going to do.

00:31:01.630 --> 00:31:03.796
These are all people who
have worked in Afghanistan.

00:31:03.796 --> 00:31:05.940
We did a lot of interviewing
with them in Iraq.

00:31:05.940 --> 00:31:07.700
And they've seen
autonomous vehicles

00:31:07.700 --> 00:31:10.325
flying around where they have no
idea what they're going to do.

00:31:10.325 --> 00:31:14.910
You had to give them both a set
of approvals and rejections,

00:31:14.910 --> 00:31:19.429
and also give the basic
vehicle a kind of autonomy

00:31:19.429 --> 00:31:21.970
that the people could relate to
so that they would understand

00:31:21.970 --> 00:31:23.480
what it was going to do.

00:31:23.480 --> 00:31:25.950
Down here on the left side
you see a little iPad mini

00:31:25.950 --> 00:31:29.180
interface that gives this
radically simplified sense

00:31:29.180 --> 00:31:30.860
of what the
vehicle's states are.

00:31:30.860 --> 00:31:33.680
In fact, I think there's
only three of them.

00:31:33.680 --> 00:31:37.320
And as the vehicle
comes in it has

00:31:37.320 --> 00:31:40.660
to accomplish what we call a
negotiation between the person

00:31:40.660 --> 00:31:42.400
on the ground and
the vehicle itself.

00:31:42.400 --> 00:31:45.310
I want to land-- you
ask me the land there.

00:31:45.310 --> 00:31:48.540
That site isn't OK, for one
reason or another-- the trees

00:31:48.540 --> 00:31:49.490
are too close.

00:31:49.490 --> 00:31:50.460
I want to land there.

00:31:50.460 --> 00:31:51.360
That looks good.

00:31:51.360 --> 00:31:52.910
Is that acceptable to you?

00:31:52.910 --> 00:31:54.640
Yes or no?

00:31:54.640 --> 00:31:57.700
And originally we had
very complex ideas

00:31:57.700 --> 00:31:59.820
for how that
negotiation might go

00:31:59.820 --> 00:32:01.820
in the midst of the
tactical landing.

00:32:01.820 --> 00:32:04.870
Given all the speeds
and physics and sensors,

00:32:04.870 --> 00:32:06.740
you had less than 30
seconds to accomplish

00:32:06.740 --> 00:32:08.270
that whole conversation.

00:32:08.270 --> 00:32:11.360
So that put real
limits on how rich

00:32:11.360 --> 00:32:13.160
that interaction
was going to be.

00:32:13.160 --> 00:32:15.197
But even for the
Navy test range where

00:32:15.197 --> 00:32:17.030
we did this-- we've
never demonstrated this,

00:32:17.030 --> 00:32:21.070
it was just demonstrated
in February--

00:32:21.070 --> 00:32:23.480
you had to have all kinds
of safety considerations

00:32:23.480 --> 00:32:25.610
just to do it in a
test setting, much less

00:32:25.610 --> 00:32:28.480
with real people
standing underneath it.

00:32:28.480 --> 00:32:30.580
Here's an image
of the helicopter.

00:32:30.580 --> 00:32:32.535
It's a Boeing Little
Bird that's been modified

00:32:32.535 --> 00:32:35.331
to be fully fly-by-wire
with the LIDAR on the front.

00:32:35.331 --> 00:32:36.830
And then instead
of carrying people,

00:32:36.830 --> 00:32:40.020
it carries this enormous
computer rack in the back.

00:32:40.020 --> 00:32:44.630
And here's the marine
on the ground who's

00:32:44.630 --> 00:32:47.100
supposed to be able to conduct
that conversation with 15

00:32:47.100 --> 00:32:48.990
minutes of training.

00:32:48.990 --> 00:32:51.650
And what you ended
up having to do

00:32:51.650 --> 00:32:54.400
was create a basic-- this
is really not all that clear

00:32:54.400 --> 00:32:58.550
and the details don't matter--
but the basic mission manager

00:32:58.550 --> 00:33:01.490
model, which was modeled
in MATLAB Stateflow,

00:33:01.490 --> 00:33:03.627
is the state machine
of, I'm coming in.

00:33:03.627 --> 00:33:05.710
I'm either going to land
or I'm not going to land.

00:33:05.710 --> 00:33:07.174
If I don't land,
I may come around

00:33:07.174 --> 00:33:08.965
for one more pass or
I may go off and hold,

00:33:08.965 --> 00:33:10.230
and give you this option.

00:33:10.230 --> 00:33:11.980
And that state machine
had to be something

00:33:11.980 --> 00:33:15.440
that the human could
understand easily, intuitively,

00:33:15.440 --> 00:33:16.090
and quickly.

00:33:16.090 --> 00:33:19.069
It couldn't be this
rich, multilayered thing.

00:33:19.069 --> 00:33:20.860
And we ended up creating
that state machine

00:33:20.860 --> 00:33:26.670
and then autocoding it into
the autonomy, and also then,

00:33:26.670 --> 00:33:30.590
as a crude working model
and MATLAB of the GUI,

00:33:30.590 --> 00:33:34.010
before operating the state
machine directly, and then

00:33:34.010 --> 00:33:36.550
porting that into the
kind of iPad interface.

00:33:36.550 --> 00:33:38.230
So this was a bit
of an experiment,

00:33:38.230 --> 00:33:40.590
although it worked
very well in saying,

00:33:40.590 --> 00:33:44.210
let's make sure that the
issues on the human autonomy

00:33:44.210 --> 00:33:47.280
interaction are not
interface issues per se.

00:33:47.280 --> 00:33:50.970
They're issues about how
the core autonomy behaves

00:33:50.970 --> 00:33:52.560
at its most fundamental level.

00:33:52.560 --> 00:33:54.800
And you need to be thinking
about all that, and all

00:33:54.800 --> 00:33:57.735
that interaction, in
this very early sort

00:33:57.735 --> 00:34:01.570
of pre-coding phase in a way.

00:34:01.570 --> 00:34:03.730
And I'll just
encapsulate that saying,

00:34:03.730 --> 00:34:05.910
if you want the human
to trust the system,

00:34:05.910 --> 00:34:09.350
they have to have a mental
model that's simple enough

00:34:09.350 --> 00:34:11.230
that they can relate to it.

00:34:11.230 --> 00:34:13.409
And that actually puts
limits on the complexity

00:34:13.409 --> 00:34:14.340
of the internal state.

00:34:14.340 --> 00:34:16.871
You can't have the thing
doing too much that people

00:34:16.871 --> 00:34:17.870
aren't going to predict.

00:34:17.870 --> 00:34:19.739
The marines we
talked to-- and it

00:34:19.739 --> 00:34:24.270
was pretty detailed kind of
ethnographic study of maybe 60

00:34:24.270 --> 00:34:26.860
or 70 people who had operated
these systems-- they said,

00:34:26.860 --> 00:34:30.460
the last thing I want to do is
something I don't understand.

00:34:30.460 --> 00:34:32.440
Same thing, you asked
airline pilots to say,

00:34:32.440 --> 00:34:35.310
have you ever asked of your
flight deck automation,

00:34:35.310 --> 00:34:36.949
"What's it doing now?"

00:34:36.949 --> 00:34:40.030
Every one of them will say,
yes, I've had that experience.

00:34:40.030 --> 00:34:43.614
And then they'll say, that's
only what the beginners say.

00:34:43.614 --> 00:34:48.260
The advanced users say, "Oh,
it does that sometimes."

00:34:48.260 --> 00:34:51.199
But what then they said is,
what we really want to know

00:34:51.199 --> 00:34:53.692
is why did they do it that way?

00:34:53.692 --> 00:34:56.150
I don't care even if I don't
like it or I disagree with it.

00:34:56.150 --> 00:34:59.980
If I understand something about
the reasoning behind the states

00:34:59.980 --> 00:35:02.910
of the autonomy, it's much
easier for me to relate to.

00:35:02.910 --> 00:35:04.590
And we've done some
work on actually

00:35:04.590 --> 00:35:11.020
trying to communicate that from
an engineering team to pilots.

00:35:11.020 --> 00:35:13.550
Then that gets you into
one more observation,

00:35:13.550 --> 00:35:15.890
which is, if you really want
the thing to do things that

00:35:15.890 --> 00:35:18.540
are what some people would
define as autonomy-- that

00:35:18.540 --> 00:35:21.920
is non-deterministic
decision making that's not

00:35:21.920 --> 00:35:25.470
predictable from a
set of conditions

00:35:25.470 --> 00:35:28.680
that the designers may specify--
there's all these things

00:35:28.680 --> 00:35:30.660
that you have to rethink.

00:35:30.660 --> 00:35:33.460
And I'll give you just one which
is sort of related-- safety

00:35:33.460 --> 00:35:36.690
and verification validation.

00:35:36.690 --> 00:35:38.880
Obviously, we fly in
jet liners and airliners

00:35:38.880 --> 00:35:40.860
all the time that are
heavily run by software.

00:35:40.860 --> 00:35:44.300
They're fly-by-wire, so software
runs the basic control systems.

00:35:44.300 --> 00:35:45.425
Those things are certified.

00:35:45.425 --> 00:35:49.290
They're triply, quadruply
redundant in some cases.

00:35:49.290 --> 00:35:50.340
How does that happen?

00:35:50.340 --> 00:35:54.910
Well, that software
is incredibly, highly

00:35:54.910 --> 00:35:58.580
deterministic and run through
in a very formalized set

00:35:58.580 --> 00:36:03.610
of processes all the different
possible ramifications--

00:36:03.610 --> 00:36:05.700
or as many as
people can approach.

00:36:05.700 --> 00:36:08.910
You have to prove that every
line of code has been executed,

00:36:08.910 --> 00:36:11.550
and the software has
to be-- the way you

00:36:11.550 --> 00:36:14.340
make certified software
in the avionics business

00:36:14.340 --> 00:36:18.440
is to run through these
very formalized procedures.

00:36:18.440 --> 00:36:20.570
There's not even a model
for how to do that when

00:36:20.570 --> 00:36:23.861
you have systems where
the decision making may

00:36:23.861 --> 00:36:25.360
be non-deterministic,
and you really

00:36:25.360 --> 00:36:26.920
don't know what's
going to happen.

00:36:26.920 --> 00:36:29.660
The DOD is very--
putting a lot of effort

00:36:29.660 --> 00:36:31.410
into trying to think
through that problem,

00:36:31.410 --> 00:36:34.110
but it's going to be a huge
issue for moving forward

00:36:34.110 --> 00:36:37.530
with higher levels of autonomy
in a traditional sense.

00:36:37.530 --> 00:36:40.560
And the similar issues come
around-- liability and testing

00:36:40.560 --> 00:36:41.740
and certification.

00:36:41.740 --> 00:36:45.140
And even who are your users,
and what are they doing?

00:36:45.140 --> 00:36:48.240
Another interesting example
with that autonomous helicopter

00:36:48.240 --> 00:36:50.710
is, so we've got this
fancy LIDAR on there.

00:36:50.710 --> 00:36:55.897
It'll find you a landing spot
and survey all the approaches

00:36:55.897 --> 00:36:57.480
and make sure you
can get out of there

00:36:57.480 --> 00:37:00.420
cleanly and land
you automatically.

00:37:00.420 --> 00:37:02.390
Well, if I was a
helicopter pilot,

00:37:02.390 --> 00:37:03.770
my response would
be, hey, I want

00:37:03.770 --> 00:37:06.370
one of those on my helicopter!

00:37:06.370 --> 00:37:09.500
It changes what you think
of as a manned helicopter,

00:37:09.500 --> 00:37:13.180
and it changes what you think of
as, how what you're doing there

00:37:13.180 --> 00:37:15.125
as an operator, now
that the pilot maybe

00:37:15.125 --> 00:37:16.500
doesn't have to
worry about that.

00:37:16.500 --> 00:37:18.240
There's a supervisory
control system

00:37:18.240 --> 00:37:21.230
that can give them
input into those things.

00:37:21.230 --> 00:37:25.170
But many helicopter pilots
have, as their central sense

00:37:25.170 --> 00:37:28.320
of identity, this sort of
two hands on the stick, two

00:37:28.320 --> 00:37:30.110
feet on the pedals,
come in and do

00:37:30.110 --> 00:37:35.240
the kind of detailed landing.

00:37:35.240 --> 00:37:39.510
So that's where kind of
my final observation.

00:37:39.510 --> 00:37:43.440
The big advances I think
that are required coming up

00:37:43.440 --> 00:37:47.070
are advances in these human
relationships with systems.

00:37:47.070 --> 00:37:48.920
What do we think of as a pilot?

00:37:48.920 --> 00:37:51.000
What do we think
of as an explorer?

00:37:51.000 --> 00:37:53.310
What do we think
of as a scientist?

00:37:53.310 --> 00:37:55.960
What do we think
of as a warrior?

00:37:55.960 --> 00:37:58.510
And these are not
things that are

00:37:58.510 --> 00:38:00.940
going to be cracked by
kind of clearer interfaces

00:38:00.940 --> 00:38:03.530
or better designs of
interfaces, nor solely

00:38:03.530 --> 00:38:06.750
by algorithms, but this whole
kind of system progressing

00:38:06.750 --> 00:38:10.450
together and new
kinds of operators,

00:38:10.450 --> 00:38:13.060
new kinds of vehicles,
new kinds of systems,

00:38:13.060 --> 00:38:15.960
and sort of rethinking
the whole system.

00:38:15.960 --> 00:38:17.950
A good example of this
is a current debate

00:38:17.950 --> 00:38:19.882
which is-- there's not
a day that it's not

00:38:19.882 --> 00:38:21.590
in the paper-- which
is unmanned vehicles

00:38:21.590 --> 00:38:24.240
in the national airspace system.

00:38:24.240 --> 00:38:26.440
And I think it's
perfectly possible

00:38:26.440 --> 00:38:28.750
to imagine a regulatory
framework that

00:38:28.750 --> 00:38:32.702
will allow that
to happen, but it

00:38:32.702 --> 00:38:34.410
will have to change
what human pilots are

00:38:34.410 --> 00:38:36.370
doing in manned
cockpits as well.

00:38:36.370 --> 00:38:38.840
That's already changing in
a lot of different ways,

00:38:38.840 --> 00:38:41.900
but you'll have to think
about-- differently

00:38:41.900 --> 00:38:44.000
about what it means to
operate a manned aircraft.

00:38:44.000 --> 00:38:46.310
Which is, again,
already happening.

00:38:46.310 --> 00:38:48.910
Already happening for
the last 50 years.

00:38:48.910 --> 00:38:51.865
So I'll leave it at that, and
leave some time for questions.

00:38:51.865 --> 00:38:53.885
And thanks for your attention.

00:38:53.885 --> 00:38:55.370
[APPLAUSE]

00:38:59.144 --> 00:39:01.310
AUDIENCE: I was wondering
if you could say something

00:39:01.310 --> 00:39:05.765
about the relationship
between humans and automation

00:39:05.765 --> 00:39:07.770
when the automation fails.

00:39:07.770 --> 00:39:12.200
For example, I was
flying an approach

00:39:12.200 --> 00:39:17.249
on the autopilot a few weeks
ago, and I all of a sudden I

00:39:17.249 --> 00:39:19.290
went down, and the autopilot
was not at all doing

00:39:19.290 --> 00:39:20.664
what it was supposed
to be doing,

00:39:20.664 --> 00:39:24.442
and I had to shut it
off and land it myself.

00:39:24.442 --> 00:39:25.150
So that was fine.

00:39:25.150 --> 00:39:28.035
But in a situation like an
auto landing system where

00:39:28.035 --> 00:39:31.296
the response time is
much shorter [INAUDIBLE].

00:39:31.296 --> 00:39:33.420
DAVID MINDELL: Yeah, that's
a really good question,

00:39:33.420 --> 00:39:36.960
and that's actually
a good example.

00:39:36.960 --> 00:39:38.380
And the most
extreme case of that

00:39:38.380 --> 00:39:43.530
is the Air France 447 crash,
where the pitot tubes iced up,

00:39:43.530 --> 00:39:46.320
the automation doesn't
have the data it needs.

00:39:46.320 --> 00:39:48.740
Ironically, it actually
did, and there was no reason

00:39:48.740 --> 00:39:52.060
that the system,
given that data,

00:39:52.060 --> 00:39:54.890
couldn't basically,
just using GPS guidance,

00:39:54.890 --> 00:39:57.200
kind of keep the airplane
straight and level

00:39:57.200 --> 00:39:58.870
and bring it down
to a safe altitude

00:39:58.870 --> 00:40:00.160
where it was easier to fly.

00:40:00.160 --> 00:40:01.890
But that's not how
it was programmed.

00:40:01.890 --> 00:40:04.030
Again autonomy is
the assumptions

00:40:04.030 --> 00:40:06.330
of the designer being
pushed forward in time.

00:40:06.330 --> 00:40:07.630
It was programmed to say, "Oop!

00:40:07.630 --> 00:40:08.595
Air data is iced up.

00:40:08.595 --> 00:40:09.700
I'm out!

00:40:09.700 --> 00:40:10.770
See you guys!

00:40:10.770 --> 00:40:11.540
I'm outta here!"

00:40:11.540 --> 00:40:14.040
Turns over the entire
system to the crew, who

00:40:14.040 --> 00:40:17.870
are a little fatigued, a little
distracted, in bad weather

00:40:17.870 --> 00:40:20.630
at night at very
high altitude where

00:40:20.630 --> 00:40:24.360
the angle of attack that's
survivable in a big airplane

00:40:24.360 --> 00:40:26.250
like that is very, very narrow.

00:40:26.250 --> 00:40:28.830
And they actually had control
of it for a minute or two,

00:40:28.830 --> 00:40:31.940
and then they lost
control of it.

00:40:31.940 --> 00:40:34.320
And that highlights this
case-- and actually many

00:40:34.320 --> 00:40:37.577
of these other recent crashes
do-- where automation is always

00:40:37.577 --> 00:40:38.160
going to fail.

00:40:38.160 --> 00:40:39.100
Stuff just fails.

00:40:39.100 --> 00:40:40.560
We know that.

00:40:40.560 --> 00:40:44.610
And so you cannot design
a system that keeps people

00:40:44.610 --> 00:40:48.200
so distant from it all the
time that they don't have that

00:40:48.200 --> 00:40:49.560
ability to intervene.

00:40:49.560 --> 00:40:51.999
And I've really, in the
last seven or eight years,

00:40:51.999 --> 00:40:54.290
I've changed my opinion on
this a great deal, because I

00:40:54.290 --> 00:40:57.160
was a much greater believer
in kind of full autonomy.

00:40:57.160 --> 00:40:59.410
And when I started
the Apollo project

00:40:59.410 --> 00:41:03.000
I really believed it was the
kind of full just macho-ness

00:41:03.000 --> 00:41:05.630
of the pilots that made them
want that ability to intervene

00:41:05.630 --> 00:41:07.190
and get hands on the stick.

00:41:07.190 --> 00:41:09.755
But over time, and having
talked to them about it,

00:41:09.755 --> 00:41:12.440
and looking at other
systems like the shuttle,

00:41:12.440 --> 00:41:15.740
I'm a lot more
nuanced about it now.

00:41:15.740 --> 00:41:19.040
And so any system you
have-- and again think about

00:41:19.040 --> 00:41:20.900
an automobile, or general
aviation aircraft,

00:41:20.900 --> 00:41:23.520
very similar.

00:41:23.520 --> 00:41:25.780
Actually automobiles, I
think, is a harder case,

00:41:25.780 --> 00:41:27.680
because things in
aircraft actually

00:41:27.680 --> 00:41:31.150
happen relatively slowly
compared to things in cars,

00:41:31.150 --> 00:41:33.105
because the distances
to collision

00:41:33.105 --> 00:41:34.980
are greater, even though
the speeds are high.

00:41:34.980 --> 00:41:36.640
You've got a lot more
space around you.

00:41:36.640 --> 00:41:38.440
The time constants
are just slower.

00:41:38.440 --> 00:41:41.000
So any automated
system in a car is

00:41:41.000 --> 00:41:42.510
going to have to
be designed such

00:41:42.510 --> 00:41:47.670
that if and when it checks
out, the person is going

00:41:47.670 --> 00:41:50.340
to be able to intervene in
a way that they will find

00:41:50.340 --> 00:41:54.380
it at least safe-able if
not continued drive-able.

00:41:54.380 --> 00:41:57.340
And that's a hard problem
that A-- we don't even

00:41:57.340 --> 00:41:58.850
understand very well.

00:41:58.850 --> 00:42:03.760
Again, Air France, we lost 250
people because of that problem.

00:42:03.760 --> 00:42:05.620
Clearly there's a great
interest and a lot

00:42:05.620 --> 00:42:09.320
of financial backing to
understanding that problem.

00:42:09.320 --> 00:42:11.620
And B-- we're just
beginning to think

00:42:11.620 --> 00:42:14.870
about how to do that
in a technical way.

00:42:14.870 --> 00:42:17.840
And again, the
Asiana crash seems

00:42:17.840 --> 00:42:21.670
to be very much about
pilots losing manual skills,

00:42:21.670 --> 00:42:23.290
and then, for various
reasons-- there

00:42:23.290 --> 00:42:26.910
was a crash in Buffalo in 2009,
similar story-- when they're

00:42:26.910 --> 00:42:29.540
called on to execute
those manual skills,

00:42:29.540 --> 00:42:32.350
for a variety of
reasons they atrophy

00:42:32.350 --> 00:42:35.165
and they can't
control it manually.

00:42:35.165 --> 00:42:39.650
So again, it's an easier problem
to design a system that's

00:42:39.650 --> 00:42:41.940
fully autonomous that
never has people in it.

00:42:41.940 --> 00:42:44.500
But you won't stake human
lives on that people-- on that

00:42:44.500 --> 00:42:45.850
generally.

00:42:45.850 --> 00:42:51.006
And my friends just lost their
AUV deep off of New Zealand

00:42:51.006 --> 00:42:51.505
last week.

00:42:51.505 --> 00:42:52.100
Big deal!

00:42:52.100 --> 00:42:54.500
It imploded somebody's gotta
write an insurance check.

00:42:54.500 --> 00:42:58.030
But everybody's a little
sad, but not all that sad.

00:42:58.030 --> 00:43:02.820
That's a whole different
story than-- actually,

00:43:02.820 --> 00:43:04.800
scientists originally
wanted to go

00:43:04.800 --> 00:43:06.820
to those trenches
in manned vehicles.

00:43:06.820 --> 00:43:09.260
And that would have been
a whole different thing.

00:43:09.260 --> 00:43:11.770
Or what you end up doing
is just certifying them

00:43:11.770 --> 00:43:14.370
so far that they become
more expensive and whatnot.

00:43:14.370 --> 00:43:17.140
So I don't know if that
answers your question but very

00:43:17.140 --> 00:43:20.420
much an issue for a general
aviation pilot, too.

00:43:20.420 --> 00:43:23.290
And that happens all
the time and there

00:43:23.290 --> 00:43:25.920
are concerns in GA craft also.

00:43:25.920 --> 00:43:30.900
In fact, after doing this
study with heads-up displays,

00:43:30.900 --> 00:43:33.060
I went and put a
synthetic vision system

00:43:33.060 --> 00:43:37.769
in my aircraft, because I
don't fly a lot of approaches

00:43:37.769 --> 00:43:39.810
at night in the mountains,
which is traditionally

00:43:39.810 --> 00:43:42.300
what I had thought it
was for, but it gives you

00:43:42.300 --> 00:43:43.260
a flight-path vector.

00:43:43.260 --> 00:43:44.885
It's not a heads-up
flight-path vector,

00:43:44.885 --> 00:43:47.890
but if you have this
3D display displayed,

00:43:47.890 --> 00:43:49.630
then you have a
flight-path vector,

00:43:49.630 --> 00:43:52.160
and then you're always
seeing where you're going.

00:43:52.160 --> 00:43:53.940
And it's that sort
of aggregate that

00:43:53.940 --> 00:43:56.620
helps you just
intuitively appreciate

00:43:56.620 --> 00:43:58.260
the energy state
in different ways.

00:44:05.460 --> 00:44:08.340
AUDIENCE: So the programming
codes that you are using,

00:44:08.340 --> 00:44:10.542
or that you're
allowed to use, is

00:44:10.542 --> 00:44:14.056
that still in a state where
dynamic memory application is

00:44:14.056 --> 00:44:17.340
not allowed and very
primitive languages?

00:44:17.340 --> 00:44:21.166
Or do you have more flexibility
as long as, you know,

00:44:21.166 --> 00:44:22.840
people are on board?

00:44:22.840 --> 00:44:25.560
DAVID MINDELL: So that's
an interesting question,

00:44:25.560 --> 00:44:28.270
because that was a vicious
battle in the Apollo

00:44:28.270 --> 00:44:31.880
system was, do you have
a preemptive multitasker,

00:44:31.880 --> 00:44:34.930
or do you have a fully
deterministic scheduler?

00:44:34.930 --> 00:44:38.410
And they ended up going with
a preemptive multitasker,

00:44:38.410 --> 00:44:41.010
although there were
bodies on the floor when

00:44:41.010 --> 00:44:42.980
they made that decision.

00:44:42.980 --> 00:44:45.980
And it's a-- part of the
book is a story about how

00:44:45.980 --> 00:44:47.630
the people who
programmed that feel it

00:44:47.630 --> 00:44:49.470
saved the day in the end.

00:44:49.470 --> 00:44:52.380
Interestingly,
like the 787, which

00:44:52.380 --> 00:44:54.440
is a fully digital
airplane, it has

00:44:54.440 --> 00:44:57.220
to use-- it has a fully
deterministic scheduler.

00:44:57.220 --> 00:44:59.560
The networks are all
fairly deterministic.

00:44:59.560 --> 00:45:02.840
There's no preemptive
communications

00:45:02.840 --> 00:45:04.230
on the networks at all.

00:45:04.230 --> 00:45:07.200
And so, at some level,
the current state

00:45:07.200 --> 00:45:10.340
of certified avionics
is behind where

00:45:10.340 --> 00:45:13.280
the Apollo system was in '60s.

00:45:13.280 --> 00:45:16.086
Although the last
part of your question

00:45:16.086 --> 00:45:17.460
is a very good
one, which is, one

00:45:17.460 --> 00:45:19.920
of the real benefits of unmanned
systems-- and we saw this,

00:45:19.920 --> 00:45:21.940
again, way back in
the '80s and '90s--

00:45:21.940 --> 00:45:27.220
was gee, we would
continue-- Jason would

00:45:27.220 --> 00:45:30.054
take two, two and a half hours
to go down into the sea floor,

00:45:30.054 --> 00:45:31.220
just because it's very deep.

00:45:31.220 --> 00:45:32.200
So would Alvin.

00:45:32.200 --> 00:45:34.850
And with Jason, you could
continue working on the code

00:45:34.850 --> 00:45:35.530
that whole time.

00:45:35.530 --> 00:45:36.744
That was valuable time.

00:45:36.744 --> 00:45:38.160
And then before
you hit the bottom

00:45:38.160 --> 00:45:41.700
you'd do another download
of the code and-- the code

00:45:41.700 --> 00:45:43.700
actually on the vehicle
doesn't do all that much

00:45:43.700 --> 00:45:44.658
in a vehicle like that.

00:45:44.658 --> 00:45:46.620
Most of it's top
side, but same thing.

00:45:46.620 --> 00:45:50.510
Whereas in a certified
manned system,

00:45:50.510 --> 00:45:55.150
all that code has to be done
months or years ahead of time.

00:45:55.150 --> 00:45:58.410
Alvin, to this day, has no
software in its critical loops.

00:45:58.410 --> 00:46:03.190
It's totally hardware-operated
vehicle in its native mode.

00:46:03.190 --> 00:46:05.660
So there's no soft sort
of certification at all.

00:46:05.660 --> 00:46:07.654
That means that the
unmanned systems tend

00:46:07.654 --> 00:46:09.570
to develop more quickly,
and it's a lot easier

00:46:09.570 --> 00:46:14.520
to experiment with them, and the
barriers to changing and doing

00:46:14.520 --> 00:46:15.960
weird stuff is a lot lower.

00:46:15.960 --> 00:46:19.262
And that's what's-- again one
of the things we did not expect

00:46:19.262 --> 00:46:21.470
when we started building
remote systems in the ocean.

00:46:21.470 --> 00:46:23.470
It turned out to be one
of the great advantages.

00:46:27.620 --> 00:46:29.120
AUDIENCE: Would you
say that you are

00:46:29.120 --> 00:46:31.194
forced to use tools
that are maybe not

00:46:31.194 --> 00:46:33.984
as flexible as you
wish, software-wise?

00:46:33.984 --> 00:46:35.775
Are advanced programming
languages allowed?

00:46:38.515 --> 00:46:42.612
Isn't that a problem that if you
use a more primitive language

00:46:42.612 --> 00:46:46.500
than you would like to,
that there might be a higher

00:46:46.500 --> 00:46:50.540
chance of high-level
bugs as [INAUDIBLE]?

00:46:50.540 --> 00:46:52.360
DAVID MINDELL: I'm
sure that's true.

00:46:52.360 --> 00:46:54.440
That was certainly true
with the Apollo system.

00:46:54.440 --> 00:46:56.570
Most of it was written
in assembly code

00:46:56.570 --> 00:47:00.500
and-- or in a very primitive
kind of algebraic compiler.

00:47:00.500 --> 00:47:03.210
And you can imagine what
it was like to debug that.

00:47:05.871 --> 00:47:07.120
That was a long time ago, but.

00:47:15.529 --> 00:47:16.960
AUDIENCE: Just wondering.

00:47:16.960 --> 00:47:18.576
You've touched a
little bit on cars

00:47:18.576 --> 00:47:19.950
a moment ago in
the failure case.

00:47:19.950 --> 00:47:23.704
If you can comment on autonomous
cars, or somewhat autonomous

00:47:23.704 --> 00:47:27.005
cars, as all these
principles apply to them.

00:47:27.005 --> 00:47:28.497
DAVID MINDELL: Sure.

00:47:28.497 --> 00:47:30.080
I'm sort of dancing
around that topic,

00:47:30.080 --> 00:47:35.460
because it's-- and it sort of
lurks behind the whole book

00:47:35.460 --> 00:47:38.050
in a certain sense.

00:47:38.050 --> 00:47:41.990
Again, I think-- and I actually,
all I know about the Google car

00:47:41.990 --> 00:47:43.705
is what I follow
the press basically.

00:47:43.705 --> 00:47:48.110
But to begin with,
and many people

00:47:48.110 --> 00:47:49.860
have said this,
car's already have

00:47:49.860 --> 00:47:52.700
a lot of these little autonomy
things built into them,

00:47:52.700 --> 00:47:54.610
from anti-lock breaks
to cruise control

00:47:54.610 --> 00:47:56.340
to automatic transmission.

00:47:56.340 --> 00:47:59.570
I mean, what is that
if not a shifting

00:47:59.570 --> 00:48:01.260
of the operator's usage?

00:48:01.260 --> 00:48:02.000
OK.

00:48:02.000 --> 00:48:04.040
And even there, with the
automatic transition,

00:48:04.040 --> 00:48:07.570
you still have a Low 1 and a
Low 2 and some more these days.

00:48:07.570 --> 00:48:10.330
So even that system,
which is from the '50s,

00:48:10.330 --> 00:48:13.650
has an ability for the human
to kind of intervene and push

00:48:13.650 --> 00:48:16.640
that loop out.

00:48:16.640 --> 00:48:19.860
I think where it
gets challenging is

00:48:19.860 --> 00:48:23.480
all the social dimensions of
driving, and all of the ways

00:48:23.480 --> 00:48:26.100
that people are making eye
contact with each other,

00:48:26.100 --> 00:48:31.205
and perceiving-- and you see
this in the warfare context

00:48:31.205 --> 00:48:31.705
actually.

00:48:36.830 --> 00:48:38.950
For a machine, or
even for a human

00:48:38.950 --> 00:48:41.030
looking through a
soda straw video,

00:48:41.030 --> 00:48:43.790
to discern the difference
between a wedding

00:48:43.790 --> 00:48:46.990
and a nefarious meeting
with people carrying guns

00:48:46.990 --> 00:48:48.750
is not all that easy.

00:48:48.750 --> 00:48:49.620
And why?

00:48:49.620 --> 00:48:53.599
Because that's a social judgment
that maybe some people can

00:48:53.599 --> 00:48:55.390
make, but the people
who are doing the work

00:48:55.390 --> 00:48:57.740
are not trained to make
those kind of judgments.

00:48:57.740 --> 00:49:01.760
And as you drive a car, you
make those kind of judgments

00:49:01.760 --> 00:49:03.140
all the time.

00:49:03.140 --> 00:49:05.650
So on the one hand you do
basic collision avoidance.

00:49:05.650 --> 00:49:08.270
And that's easier these
days to understand,

00:49:08.270 --> 00:49:10.470
how you would automate that.

00:49:10.470 --> 00:49:12.670
And on the other hand,
you're constantly

00:49:12.670 --> 00:49:15.296
saying, that car looks
a little ratty, has

00:49:15.296 --> 00:49:16.170
a lot of dents on it.

00:49:16.170 --> 00:49:18.080
I think I'll stay away from it.

00:49:18.080 --> 00:49:20.160
That person clearly is
not paying attention

00:49:20.160 --> 00:49:23.790
when they're standing at
the stop sign and whatnot.

00:49:23.790 --> 00:49:30.170
And how am I going to
interact with other people

00:49:30.170 --> 00:49:31.690
on the highway, and whatnot?

00:49:31.690 --> 00:49:40.670
And again I think automated
cars are imaginable,

00:49:40.670 --> 00:49:43.250
but I think the whole
system is going to change,

00:49:43.250 --> 00:49:45.630
and it's going to change
what it means to drive.

00:49:45.630 --> 00:49:50.881
And even there, people have
a lot-- it varies a lot, OK?

00:49:50.881 --> 00:49:52.380
That's why it's
hard to understand--

00:49:52.380 --> 00:49:55.390
people have a lot of
cultural connection

00:49:55.390 --> 00:49:59.230
to their identities as drivers.

00:49:59.230 --> 00:50:00.147
And some people don't.

00:50:00.147 --> 00:50:01.938
Maybe some people don't
care, and they just

00:50:01.938 --> 00:50:04.110
want to go from A to B.
That's certainly the case.

00:50:04.110 --> 00:50:05.720
Other people love their cars.

00:50:05.720 --> 00:50:07.820
They find an extension
of their identity.

00:50:07.820 --> 00:50:11.700
They like the feeling of
being in control and whatnot.

00:50:11.700 --> 00:50:14.720
And that varies as much
as individual people vary.

00:50:14.720 --> 00:50:18.340
And so how do you kind of
negotiate what people want,

00:50:18.340 --> 00:50:20.460
and what level they
want to be involved?

00:50:20.460 --> 00:50:23.380
But a good example of
this which sort of came up

00:50:23.380 --> 00:50:25.520
is automatic landing
in airliners,

00:50:25.520 --> 00:50:30.340
where automatic landing requires
a tremendously high amount

00:50:30.340 --> 00:50:32.170
of vigilance on the
part of the pilots.

00:50:32.170 --> 00:50:33.840
In fact the stress
levels-- there's

00:50:33.840 --> 00:50:36.950
been studies that show that
the stress levels on average

00:50:36.950 --> 00:50:38.760
are higher than they
are in manual landing,

00:50:38.760 --> 00:50:40.650
because it's a little
bit all or nothing.

00:50:40.650 --> 00:50:43.220
Either I have nothing to do
and it's just going to land me,

00:50:43.220 --> 00:50:46.460
or I have to intervene at
the last possible minute

00:50:46.460 --> 00:50:48.950
and bounce off the runway
and go do a go-around.

00:50:48.950 --> 00:50:54.671
And I talked to pilots, and
they all say, we like auto land.

00:50:54.671 --> 00:50:57.170
If we're tired at the end of
the day, we sort of flick it on

00:50:57.170 --> 00:50:58.400
and then it lands for us.

00:50:58.400 --> 00:51:00.450
It's not supposed to
be for that, right?

00:51:00.450 --> 00:51:04.140
It's supposed to
be-- legally, they're

00:51:04.140 --> 00:51:07.560
supposed to have the very
same level of vigilance.

00:51:07.560 --> 00:51:12.055
But over time it gets
used in a more what's

00:51:12.055 --> 00:51:15.340
the word-- complacent kind
of way, and that's dangerous.

00:51:15.340 --> 00:51:19.830
And so-- and airline
pilots, or any pilots,

00:51:19.830 --> 00:51:23.130
are highly trained, sort
of technically astute.

00:51:23.130 --> 00:51:26.090
They've had a medical exam
in the last six months.

00:51:26.090 --> 00:51:27.320
All these things.

00:51:27.320 --> 00:51:31.460
And you extend those kind of
complex automation interactions

00:51:31.460 --> 00:51:35.750
issues to the world of
millions of drivers.

00:51:35.750 --> 00:51:38.440
And to me that's a very
scary thing, actually.

00:51:38.440 --> 00:51:40.430
And you look at that
Air France crash

00:51:40.430 --> 00:51:44.100
where airline flying is
incredibly, incredibly safe--

00:51:44.100 --> 00:51:46.250
much safer already
than driving is.

00:51:46.250 --> 00:51:48.410
And yet there are these
corners of the state space

00:51:48.410 --> 00:51:52.140
where the people on the machine
get into these wacky loops

00:51:52.140 --> 00:51:55.160
and everything just falls
apart and it goes to hell.

00:51:55.160 --> 00:51:57.660
And how are we going to
learn about those corners

00:51:57.660 --> 00:52:00.790
of the state space in the
human-automated driving system?

00:52:00.790 --> 00:52:03.756
That's the question that
really interests me these days.

00:52:08.980 --> 00:52:10.479
AUDIENCE: Do you
know of anybody who

00:52:10.479 --> 00:52:13.230
is sort of doing fault injection
and testing on the way,

00:52:13.230 --> 00:52:14.740
sort of, for humans?

00:52:14.740 --> 00:52:17.470
That is, if I know an airline
flight is going to be boring,

00:52:17.470 --> 00:52:19.305
I can sort of check
out of manual pilot.

00:52:19.305 --> 00:52:22.670
But if there's a chance that
the thing's gonna drill me,

00:52:22.670 --> 00:52:28.280
or, it's gonna-- it's gonna,
keeping me interested and, I

00:52:28.280 --> 00:52:32.350
can see that being extremely--
I can see as a pilot being

00:52:32.350 --> 00:52:35.895
extremely-- feeling untrusted
if I'm put in that situation.

00:52:35.895 --> 00:52:37.795
But I can also see it
keeping me on my toes.

00:52:37.795 --> 00:52:40.170
Is there anybody who is doing
anything in that direction?

00:52:40.170 --> 00:52:43.130
DAVID MINDELL: Actually,
there are some airliners

00:52:43.130 --> 00:52:46.400
that have a very crude
form of that, where

00:52:46.400 --> 00:52:49.570
like a little warning light goes
on if nothing has been changed

00:52:49.570 --> 00:52:52.930
in the last 20 minutes or so.

00:52:52.930 --> 00:52:54.620
And that's a very simple form.

00:52:54.620 --> 00:52:57.300
I actually think that one of
the frontiers for the work

00:52:57.300 --> 00:53:02.570
is to have the systems that
run the aircraft actively

00:53:02.570 --> 00:53:05.250
manage the cognition of
the pilot a little more.

00:53:05.250 --> 00:53:07.300
Because we know that
people get fatigued,

00:53:07.300 --> 00:53:11.220
and we know that people get less
fatigued when they're engaged,

00:53:11.220 --> 00:53:13.340
and so there's no
reason the system

00:53:13.340 --> 00:53:16.670
can't begin to
engage the people.

00:53:16.670 --> 00:53:19.350
Frankly, it might be
better for your pilot

00:53:19.350 --> 00:53:23.310
to be watching a
movie than dozing off.

00:53:23.310 --> 00:53:26.940
And maybe their favorite
movie is a better way

00:53:26.940 --> 00:53:30.300
of keeping them
alert than pretending

00:53:30.300 --> 00:53:32.740
against all scientific evidence
that they can spend eight

00:53:32.740 --> 00:53:34.330
hours in front of
a bunch of dials

00:53:34.330 --> 00:53:37.640
and remain their
level of alertness.

00:53:37.640 --> 00:53:39.560
The same thing with
napping, by the way.

00:53:39.560 --> 00:53:42.620
We know it's better for people
to nap for brief periods,

00:53:42.620 --> 00:53:43.970
and they're more alert.

00:53:43.970 --> 00:53:47.670
Yet that's illegal currently
in an airline cockpit--

00:53:47.670 --> 00:53:50.940
even though 85% of pilots will
tell you that they do that.

00:53:54.050 --> 00:53:57.360
And just those two issues
begin to get around

00:53:57.360 --> 00:54:00.120
some of the cultural
baggage around what

00:54:00.120 --> 00:54:01.790
do you expect your pilot to be?

00:54:01.790 --> 00:54:06.710
You expect them to be this
kind of heroic, active flyer,

00:54:06.710 --> 00:54:09.400
and if they're napping
they're clearly checked out--

00:54:09.400 --> 00:54:13.460
even though you have
a set of research data

00:54:13.460 --> 00:54:17.110
that says people on the
average over 12 hours

00:54:17.110 --> 00:54:20.760
are much stronger if they've
napped for 10 minutes here

00:54:20.760 --> 00:54:22.080
or 20 minutes there.

00:54:22.080 --> 00:54:23.946
And so there's a
whole set of things

00:54:23.946 --> 00:54:26.570
that you can begin to imagine--
we're beginning to work on some

00:54:26.570 --> 00:54:31.200
of these things--
of actively engaging

00:54:31.200 --> 00:54:33.830
the cognitive state
of the operator--

00:54:33.830 --> 00:54:37.990
but that requires changing what
you think the operator is--

00:54:37.990 --> 00:54:40.762
to be a little bit more
mechanistic, and say,

00:54:40.762 --> 00:54:42.470
look you know-- and
this is one of things

00:54:42.470 --> 00:54:44.070
about learning to fly
an airplane anyway.

00:54:44.070 --> 00:54:46.028
You just learn all these
things about yourself,

00:54:46.028 --> 00:54:48.790
where you really are a machine
when it comes to fatigue,

00:54:48.790 --> 00:54:49.750
right?

00:54:49.750 --> 00:54:51.380
Your performance
just degrades when

00:54:51.380 --> 00:54:53.190
you're fatigued-- period, right?

00:54:53.190 --> 00:54:55.420
Your visual acuity
just degrades when

00:54:55.420 --> 00:54:57.850
you have lower oxygen
levels, period.

00:54:57.850 --> 00:55:00.450
You can't sit there and go,
I'm smart-- I can see better,

00:55:00.450 --> 00:55:01.090
right?

00:55:01.090 --> 00:55:06.096
And so there's a kind of a
cognitive extension of that,

00:55:06.096 --> 00:55:09.440
that the regulatory system
and what we expect out

00:55:09.440 --> 00:55:13.340
of our pilots is not
remotely caught up with.

00:55:13.340 --> 00:55:16.030
And so there's a lot of things
you can imagine like that.

00:55:16.030 --> 00:55:17.840
And that may be the
case in cars too,

00:55:17.840 --> 00:55:23.040
where, in fact, I think even
some of the lane departure

00:55:23.040 --> 00:55:24.590
warning sort of
things-- they only

00:55:24.590 --> 00:55:26.350
work if your hands
are on the wheel.

00:55:26.350 --> 00:55:29.230
So you can't fall asleep and
let your Mercedes S-Class drive

00:55:29.230 --> 00:55:29.790
you.

00:55:29.790 --> 00:55:31.373
You have to stay
like this, and that's

00:55:31.373 --> 00:55:34.769
one way of keeping
people vigilant.

00:55:34.769 --> 00:55:36.060
Does that answer your question?

00:55:39.642 --> 00:55:40.350
MALE SPEAKER: OK!

00:55:40.350 --> 00:55:41.080
Thank you very much!

00:55:41.080 --> 00:55:41.370
DAVID MINDELL: Great!

00:55:41.370 --> 00:55:41.970
Thanks for your attention.

00:55:41.970 --> 00:55:42.595
Good questions!

00:55:42.595 --> 00:55:44.720
[APPLAUSE]

