WEBVTT
Kind: captions
Language: en

00:00:09.875 --> 00:00:11.250
MATEI CIOCARLIE:
Before we begin,

00:00:11.250 --> 00:00:13.160
one message, I was
asked to pass along,

00:00:13.160 --> 00:00:16.180
is to, please, hold off
questions until after the talk.

00:00:16.180 --> 00:00:18.110
And with that, it's
my great pleasure

00:00:18.110 --> 00:00:20.930
to introduce Ken Goldberg
who is a distinguished

00:00:20.930 --> 00:00:24.250
professor of new media
up at UC Berkeley.

00:00:24.250 --> 00:00:27.800
Ken is a roboticist, is
a computer scientist,

00:00:27.800 --> 00:00:31.000
is really a true Renaissance
man with broad multidisciplinary

00:00:31.000 --> 00:00:32.420
interests.

00:00:32.420 --> 00:00:35.590
Undergraduate degrees
in EE and economics.

00:00:35.590 --> 00:00:38.960
So, I guess, triple E at UPenn.

00:00:38.960 --> 00:00:41.830
Then a PhD in robotics
at Carnegie Mellon.

00:00:41.830 --> 00:00:44.996
And, now, holds a primary
appointment in IOR at Berkeley

00:00:44.996 --> 00:00:46.370
but, also, secondary
appointments

00:00:46.370 --> 00:00:50.600
in EECS, Art Practice,
School of Information,

00:00:50.600 --> 00:00:52.820
and Department of
Radiation Oncology.

00:00:52.820 --> 00:00:54.860
So really broad
multidisciplinary interests.

00:00:54.860 --> 00:00:57.620
And, maybe because of that,
he was one of the people

00:00:57.620 --> 00:01:00.680
to notice, early, that this
possibility of combining

00:01:00.680 --> 00:01:03.890
two fields-- combining
robotics and cloud computing.

00:01:03.890 --> 00:01:07.490
And he's actually been working
with Google, in this direction,

00:01:07.490 --> 00:01:09.126
for a couple of years now.

00:01:09.126 --> 00:01:10.500
And I'm sure he
will tell us more

00:01:10.500 --> 00:01:13.030
about the results in that area.

00:01:13.030 --> 00:01:15.520
And before I pass it on to
Ken, just one last thing

00:01:15.520 --> 00:01:17.820
I want to mention is
that Ken once told me

00:01:17.820 --> 00:01:21.850
that he considers his job
to be to make people think.

00:01:21.850 --> 00:01:23.720
And I think he's
really good at that

00:01:23.720 --> 00:01:25.510
because he sees connections.

00:01:25.510 --> 00:01:27.977
And he sees new angles
in new directions.

00:01:27.977 --> 00:01:29.810
So we're really happy
to have him here today

00:01:29.810 --> 00:01:32.570
to inspire us to
think in new ways.

00:01:32.570 --> 00:01:33.747
So take it away, Ken.

00:01:33.747 --> 00:01:36.747
KEN GOLDBERG: Thank you.

00:01:36.747 --> 00:01:38.080
Matei, I love your introduction.

00:01:38.080 --> 00:01:40.325
Triple E, I think I'm going
to use that from now on.

00:01:40.325 --> 00:01:40.740
MATEI CIOCARLIE:
[CHUCKLES] Yeah.

00:01:40.740 --> 00:01:43.480
KEN GOLDBERG: I want to
thank you all for being here.

00:01:43.480 --> 00:01:46.000
I want to thank
Michael, and James,

00:01:46.000 --> 00:01:47.840
and all those I've
been lucky enough

00:01:47.840 --> 00:01:49.220
to work with here at Google.

00:01:49.220 --> 00:01:52.240
It's really a pleasure to be
here because right now, Google

00:01:52.240 --> 00:01:56.030
is the world mecca for robotics.

00:01:56.030 --> 00:01:58.460
So I want to give you
my perspective on what

00:01:58.460 --> 00:02:00.620
I think what's happening here.

00:02:00.620 --> 00:02:04.840
I want to take you back,
first, about 20 years

00:02:04.840 --> 00:02:06.650
to the early days of the web.

00:02:06.650 --> 00:02:08.419
Who remembers?

00:02:08.419 --> 00:02:13.220
Your first encounter
with the mosaic browsers?

00:02:13.220 --> 00:02:18.770
I had a student, a group of
students in a lab at USC.

00:02:18.770 --> 00:02:21.330
Actually, one of
them is here today.

00:02:21.330 --> 00:02:23.520
And we were
interested in what we

00:02:23.520 --> 00:02:26.260
could do with the worldwide
web in the early days.

00:02:26.260 --> 00:02:27.920
And since we were
in a robotics lab,

00:02:27.920 --> 00:02:30.060
we decided to think
about how we could

00:02:30.060 --> 00:02:32.195
attach a robot to the web.

00:02:32.195 --> 00:02:34.390
And we wanted it to do
something interesting.

00:02:34.390 --> 00:02:36.265
So we set it up as
an art installation.

00:02:36.265 --> 00:02:38.960
It was the idea, kind
of the last thing

00:02:38.960 --> 00:02:41.520
we thought people would really
want to do over the web,

00:02:41.520 --> 00:02:43.960
was to actually garden.

00:02:43.960 --> 00:02:46.170
So we made something we
called the Telegarden.

00:02:46.170 --> 00:02:48.790
And it was a very
early interface.

00:02:48.790 --> 00:02:52.100
It uses the first
version of HTML.

00:02:52.100 --> 00:02:53.660
So you would be able to come in.

00:02:53.660 --> 00:02:58.330
You could move the robot around
by clicking on this workspace.

00:02:58.330 --> 00:03:00.630
Then the camera at
the end effector,

00:03:00.630 --> 00:03:03.040
it would show you what
you were looking at.

00:03:03.040 --> 00:03:05.300
And then, that way, you
could visit the garden.

00:03:05.300 --> 00:03:08.226
But if you wanted to register,
and we would send you

00:03:08.226 --> 00:03:09.850
a password, you could
then participate.

00:03:09.850 --> 00:03:12.350
You could help us
water the garden.

00:03:12.350 --> 00:03:14.560
So there was a button
here for watering.

00:03:14.560 --> 00:03:15.810
So you can actually do things.

00:03:15.810 --> 00:03:17.720
You could interact with
the garden over the web.

00:03:17.720 --> 00:03:19.570
And then, if you watered for
a certain amount of time,

00:03:19.570 --> 00:03:21.403
we considered you a
member in good standing.

00:03:21.403 --> 00:03:23.160
And we'd grant you
your first seed.

00:03:23.160 --> 00:03:24.869
So you could plant
seeds in the garden.

00:03:24.869 --> 00:03:26.410
And, of course, the
interesting thing

00:03:26.410 --> 00:03:28.060
was that, contrary to
almost everything that

00:03:28.060 --> 00:03:30.460
happens on the internet--
where you click on something,

00:03:30.460 --> 00:03:31.990
and you get instant
gratification.

00:03:31.990 --> 00:03:34.010
That's not how the
real world works.

00:03:34.010 --> 00:03:36.000
The natural world really
hasn't evolved much

00:03:36.000 --> 00:03:37.630
in the last 100,000 years.

00:03:37.630 --> 00:03:39.380
So when you plant a
seed, nothing happens.

00:03:39.380 --> 00:03:41.380
You have to come back,
and water, and et cetera.

00:03:41.380 --> 00:03:43.750
And so it was really
a social experiment

00:03:43.750 --> 00:03:45.330
to see what would
happen, how people

00:03:45.330 --> 00:03:47.150
would interact with
something like this.

00:03:47.150 --> 00:03:50.250
And one thing we
didn't anticipate,

00:03:50.250 --> 00:03:53.310
because we're engineers,
was that if you build an 11

00:03:53.310 --> 00:03:55.740
foot by 11 foot garden
space, and you invite people

00:03:55.740 --> 00:03:59.030
from around the world to
come in and plant seeds,

00:03:59.030 --> 00:04:02.367
you get it very quickly
becomes horrendously overgrown.

00:04:02.367 --> 00:04:03.950
So you get something
that was, really,

00:04:03.950 --> 00:04:05.530
more of an exercise
in the tragedy

00:04:05.530 --> 00:04:09.391
of the common than robotics.

00:04:09.391 --> 00:04:10.890
But one other thing
that came up was

00:04:10.890 --> 00:04:14.330
the question of was it real?

00:04:14.330 --> 00:04:15.900
Actually, a student
wrote and said,

00:04:15.900 --> 00:04:17.750
how do I know there
really is a garden.

00:04:17.750 --> 00:04:19.946
Because, as you can see,
they could be simulated.

00:04:19.946 --> 00:04:22.029
And, in fact, this question
is becoming, actually,

00:04:22.029 --> 00:04:26.850
more interesting today where
the advances in graphics

00:04:26.850 --> 00:04:29.840
have evolved to the point
where it's, actually,

00:04:29.840 --> 00:04:32.120
there's many cases where
it becomes increasingly

00:04:32.120 --> 00:04:35.100
difficult to distinguish
between virtual reality--

00:04:35.100 --> 00:04:36.956
when there's a
synthetic environment--

00:04:36.956 --> 00:04:39.330
versus distal reality-- where
there's a real environment,

00:04:39.330 --> 00:04:42.165
but it may be mediated by
some technology like video

00:04:42.165 --> 00:04:43.420
over the internet.

00:04:43.420 --> 00:04:45.470
And we became really
interested in that question.

00:04:45.470 --> 00:04:48.230
What is the basis
for understanding?

00:04:48.230 --> 00:04:51.920
How do we know when
we're in this environment

00:04:51.920 --> 00:04:53.560
versus that one?

00:04:53.560 --> 00:04:56.630
And that led us to
a book that I wrote

00:04:56.630 --> 00:04:59.180
with a number of
colleagues, including

00:04:59.180 --> 00:05:00.970
Hubert Dreyfus--
who's at Berkeley--

00:05:00.970 --> 00:05:02.539
the eminent philosopher.

00:05:02.539 --> 00:05:04.330
And we ended up calling
it Telepistemology,

00:05:04.330 --> 00:05:08.610
the question of what is
knowable at a distance?

00:05:08.610 --> 00:05:11.030
And how do
technologies influence

00:05:11.030 --> 00:05:13.870
what we can know
over a distance?

00:05:13.870 --> 00:05:15.930
Now, that was 20 years ago.

00:05:15.930 --> 00:05:18.150
And a lot has
happened since then.

00:05:18.150 --> 00:05:20.490
So now, we're in 2014.

00:05:20.490 --> 00:05:23.020
The field of robotics
has evolved dramatically.

00:05:23.020 --> 00:05:27.070
And there is, really,
an inflection point

00:05:27.070 --> 00:05:30.100
in the evolution of robotics.

00:05:30.100 --> 00:05:33.150
We now have over
a million service

00:05:33.150 --> 00:05:35.070
robotics out in the home.

00:05:35.070 --> 00:05:38.730
There are defense robots,
the enormous investments

00:05:38.730 --> 00:05:39.590
in defense.

00:05:39.590 --> 00:05:41.850
And thousands of
surgical robots are

00:05:41.850 --> 00:05:44.590
being used around the world.

00:05:44.590 --> 00:05:48.250
And there's also the many
advances in technology,

00:05:48.250 --> 00:05:48.960
like sensors.

00:05:48.960 --> 00:05:50.900
This is the connect center.

00:05:50.900 --> 00:05:52.310
This has revolutionized
the field

00:05:52.310 --> 00:05:55.180
because it provides
very low cost access

00:05:55.180 --> 00:05:58.120
to three dimensional
models of the environment.

00:05:58.120 --> 00:06:00.494
There's also this development.

00:06:00.494 --> 00:06:01.160
[VIDEO PLAYBACK]

00:06:01.160 --> 00:06:03.620
-And one of my responsibilities
as Commander-in-Chief,

00:06:03.620 --> 00:06:06.572
is to keep my eye on robots.

00:06:06.572 --> 00:06:08.875
[LAUGHTER]

00:06:08.875 --> 00:06:13.650
-I'm pleased to report that
the robots you manufacture here

00:06:13.650 --> 00:06:14.650
seem peaceful.

00:06:14.650 --> 00:06:16.650
[LAUGHTER]

00:06:16.650 --> 00:06:18.845
-At least for now.

00:06:18.845 --> 00:06:21.320
To help everyone, from
factory workers to astronauts,

00:06:21.320 --> 00:06:23.795
carry out more
complicated tasks.

00:06:23.795 --> 00:06:26.930
NASA and other agencies
will support research

00:06:26.930 --> 00:06:29.667
into next-generation robotics.

00:06:29.667 --> 00:06:30.500
[END VIDEO PLAYBACK]

00:06:30.500 --> 00:06:32.291
KEN GOLDBERG: So this
is a major milestone,

00:06:32.291 --> 00:06:35.060
as well, because when the
president came out and said

00:06:35.060 --> 00:06:37.780
that he was going to start
this initiative, the National

00:06:37.780 --> 00:06:40.570
Robotics Initiative, this
has galvanized research

00:06:40.570 --> 00:06:42.620
around the country.

00:06:42.620 --> 00:06:45.640
And, of course, all of you
are familiar with what's

00:06:45.640 --> 00:06:48.335
going on here at Google.

00:06:48.335 --> 00:06:51.620
The self-driving car is one
major project that's actually

00:06:51.620 --> 00:06:54.022
very interesting and, I
think, a perfect example

00:06:54.022 --> 00:06:55.480
of the kind of
thing I want to talk

00:06:55.480 --> 00:06:58.360
about today which is a
broader approach that we

00:06:58.360 --> 00:07:01.000
call cloud robotics.

00:07:01.000 --> 00:07:02.760
And this term, I
want to give credit

00:07:02.760 --> 00:07:06.690
to James Kuffner
who's here at Google.

00:07:06.690 --> 00:07:07.467
He's away today.

00:07:07.467 --> 00:07:08.050
He's in Japan.

00:07:08.050 --> 00:07:11.090
But he coined this term in 2010.

00:07:11.090 --> 00:07:14.780
And I find this to be
exactly the right term

00:07:14.780 --> 00:07:18.005
to describe a host
of new technologies

00:07:18.005 --> 00:07:20.980
and new ideas that are coming
together and really changing

00:07:20.980 --> 00:07:23.195
the way we think about robots.

00:07:23.195 --> 00:07:24.445
So let me give you an example.

00:07:24.445 --> 00:07:26.650
I'll start with
this one which is

00:07:26.650 --> 00:07:30.230
there are five ways I think
about where robots will be

00:07:30.230 --> 00:07:32.370
affected by the
web, by the cloud.

00:07:32.370 --> 00:07:34.980
The first one is big data.

00:07:34.980 --> 00:07:38.920
So when robots are in, working,
moving around in environments,

00:07:38.920 --> 00:07:41.520
they will often encounter
things that they may not

00:07:41.520 --> 00:07:43.120
have encountered before.

00:07:43.120 --> 00:07:47.500
So they can access
this vast library

00:07:47.500 --> 00:07:50.520
of data sets, that
are available online,

00:07:50.520 --> 00:07:55.540
for information about all
kinds of objects, and scenes,

00:07:55.540 --> 00:08:00.160
and-- for example-- maps,
weather conditions, basically

00:08:00.160 --> 00:08:03.050
skills that they could
acquire from the internet.

00:08:03.050 --> 00:08:04.790
They can download on-demand.

00:08:04.790 --> 00:08:06.350
The second one is
that robots often

00:08:06.350 --> 00:08:09.170
have to do extensive
computation, for example,

00:08:09.170 --> 00:08:13.360
to do motion planning,
or statistical analysis.

00:08:13.360 --> 00:08:15.341
And these can now be
done in the cloud.

00:08:15.341 --> 00:08:16.840
In other words, a
robot doesn't have

00:08:16.840 --> 00:08:20.770
to carry around the processing
elements on its own onboard.

00:08:20.770 --> 00:08:22.955
So it can access the
cloud with a problem,

00:08:22.955 --> 00:08:24.560
with a problem description.

00:08:24.560 --> 00:08:29.350
It can be run and
process in the cloud.

00:08:29.350 --> 00:08:33.400
The third one is the idea
of people sharing resources.

00:08:33.400 --> 00:08:36.099
And this has been
another change, really,

00:08:36.099 --> 00:08:37.140
in the field of robotics.

00:08:37.140 --> 00:08:39.980
As Matei mentioned, he
played a very active role

00:08:39.980 --> 00:08:43.720
in the development of ROS
at Willow Garage, the Robot

00:08:43.720 --> 00:08:44.640
Operating System.

00:08:44.640 --> 00:08:46.410
This has dramatically
changed the way

00:08:46.410 --> 00:08:47.810
we think about robotics.

00:08:47.810 --> 00:08:49.810
There's a lot more
sharing and open source

00:08:49.810 --> 00:08:51.455
tools that are now available.

00:08:51.455 --> 00:08:53.330
And one other aspect of
this, that's related,

00:08:53.330 --> 00:08:56.515
is the idea of letting
people share ideas,

00:08:56.515 --> 00:09:00.950
so letting them work on
designs using the internet.

00:09:00.950 --> 00:09:02.250
So-- oh, I'm sorry.

00:09:02.250 --> 00:09:06.850
This is another image
of ROS where researchers

00:09:06.850 --> 00:09:10.430
are sharing data, in real-time,
doing experiments, testing

00:09:10.430 --> 00:09:15.010
algorithms on the
web using the cloud.

00:09:15.010 --> 00:09:17.440
And what I was getting to
with the idea of individuals

00:09:17.440 --> 00:09:20.220
sharing ideas was
that we can also

00:09:20.220 --> 00:09:24.970
use the cloud as a way of
gaining creative ideas.

00:09:24.970 --> 00:09:27.840
And two years ago,
I formed, I was

00:09:27.840 --> 00:09:30.340
co-founder of the
African Robotics Network.

00:09:30.340 --> 00:09:32.420
And our challenge
with that was how

00:09:32.420 --> 00:09:36.080
to develop an ultra low-cost
robot for education.

00:09:36.080 --> 00:09:39.900
It could be that students
in Africa could afford.

00:09:39.900 --> 00:09:45.070
So the idea was to aim for
a costpoint of about $10.00.

00:09:45.070 --> 00:09:47.330
And the idea was to use
the cloud as a resource,

00:09:47.330 --> 00:09:50.600
to put out a
competition for ideas.

00:09:50.600 --> 00:09:53.460
So we put this out with prizes.

00:09:53.460 --> 00:09:58.780
And we got a number of great
suggestions, of entries.

00:09:58.780 --> 00:10:01.780
And these are the
10 winning designs.

00:10:01.780 --> 00:10:03.580
And they were beautiful designs.

00:10:03.580 --> 00:10:05.860
Very, very creative
use of materials.

00:10:05.860 --> 00:10:09.610
This one is just done with
cardboard and zip ties.

00:10:09.610 --> 00:10:15.410
And they were all, they came
in around somewhat under $100.

00:10:15.410 --> 00:10:28.280
But the grand prize winner was
something called the Lollybot.

00:10:28.280 --> 00:10:30.290
And it's beautiful.

00:10:30.290 --> 00:10:31.860
It used a game controller.

00:10:31.860 --> 00:10:34.502
And turns out that the
vibratory motors, that

00:10:34.502 --> 00:10:35.960
are built into the
game controller,

00:10:35.960 --> 00:10:37.710
can be turned around.

00:10:37.710 --> 00:10:40.340
And by patching a couple
wheels on to that,

00:10:40.340 --> 00:10:41.930
they can drive these wheels.

00:10:41.930 --> 00:10:45.770
And then, the thumb switches
can actually act as sensors.

00:10:45.770 --> 00:10:49.220
And the idea is that when the
robot bumps into something.

00:10:49.220 --> 00:10:52.740
But you need a moment
arm for the sensors.

00:10:52.740 --> 00:10:55.954
So he came up with the idea
of attaching two lollipops.

00:10:55.954 --> 00:10:57.120
They're actually functional.

00:10:57.120 --> 00:10:59.120
So they, actually,
served as levers.

00:10:59.120 --> 00:11:04.250
And, of course, what
kid could resist a robot

00:11:04.250 --> 00:11:06.380
with two lollipops
attached on the top of it.

00:11:06.380 --> 00:11:08.030
What's really
remarkable about this

00:11:08.030 --> 00:11:10.970
is that you can get
these components surplus.

00:11:10.970 --> 00:11:18.030
And Tom Tilley, who is basically
a hobbyist based in Thailand,

00:11:18.030 --> 00:11:19.090
put this idea together.

00:11:19.090 --> 00:11:20.798
And he puts all the
information about how

00:11:20.798 --> 00:11:22.690
to build your own on the web.

00:11:22.690 --> 00:11:24.890
And he put the part list.

00:11:24.890 --> 00:11:27.780
And because you can
get these for $3 or $4,

00:11:27.780 --> 00:11:32.860
the entire robot costs $8.96.

00:11:32.860 --> 00:11:35.150
That's including
the two lollipops.

00:11:35.150 --> 00:11:37.690
So this is an example
of the kind of ingenuity

00:11:37.690 --> 00:11:42.050
that can be tapped in the
cloud for designing new ideas.

00:11:42.050 --> 00:11:44.730
Now, there's also
the idea of clouds

00:11:44.730 --> 00:11:47.460
being used in automation,
in factory environments,

00:11:47.460 --> 00:11:48.600
for logistics.

00:11:48.600 --> 00:11:52.980
Here's the very well-known
Kiva Systems robots.

00:11:52.980 --> 00:11:54.920
These orange things
at the bottom,

00:11:54.920 --> 00:11:58.870
they're basically moving
around to help warehouses,

00:11:58.870 --> 00:12:01.350
large warehouses like at Amazon.

00:12:01.350 --> 00:12:03.550
And the idea is that
all these robots

00:12:03.550 --> 00:12:05.490
communicate on an
internal network.

00:12:05.490 --> 00:12:07.990
So they're cloud-based.

00:12:07.990 --> 00:12:12.680
Not using the global cloud
but using an internal cloud.

00:12:12.680 --> 00:12:16.100
And the last way,
that I could think of,

00:12:16.100 --> 00:12:18.970
that robots can
benefit from the cloud

00:12:18.970 --> 00:12:22.380
is by the use of the
availability of humans

00:12:22.380 --> 00:12:23.586
when all else fails.

00:12:23.586 --> 00:12:25.210
Because we're never
going to get robots

00:12:25.210 --> 00:12:28.050
that will absolutely work
in every single possible

00:12:28.050 --> 00:12:29.050
circumstance.

00:12:29.050 --> 00:12:30.899
So when a robot gets
stuck, when it's

00:12:30.899 --> 00:12:33.440
trying to clean up your house
and it gets into a corner where

00:12:33.440 --> 00:12:35.370
it's just really
not sure what to do,

00:12:35.370 --> 00:12:37.620
the idea is it can
call a call center.

00:12:37.620 --> 00:12:39.460
And, hopefully, humans
will be standing by,

00:12:39.460 --> 00:12:41.990
be able to help diagnose and
figure out what went wrong.

00:12:41.990 --> 00:12:43.410
It's a little
different than today

00:12:43.410 --> 00:12:46.227
where you call a call
center and you get a robot.

00:12:46.227 --> 00:12:47.810
And this will be the
other way around.

00:12:47.810 --> 00:12:49.364
The robot will call
and get a human.

00:12:49.364 --> 00:12:50.780
But I do think
this is interesting

00:12:50.780 --> 00:12:54.050
that humans can also be
available as resource

00:12:54.050 --> 00:12:56.390
to help robots.

00:12:56.390 --> 00:13:00.130
Now, I also want to make a
distinction for that we're not

00:13:00.130 --> 00:13:03.680
talking about the cloud
being used in real-time to do

00:13:03.680 --> 00:13:06.210
all the computations on the fly.

00:13:06.210 --> 00:13:08.620
So, for example, there's a
lot of robotic activities

00:13:08.620 --> 00:13:11.049
that require a
very high latency.

00:13:11.049 --> 00:13:12.840
So you need to be able
to-- or low latency.

00:13:12.840 --> 00:13:14.740
And so you need to
be able to respond

00:13:14.740 --> 00:13:16.580
to things and very quickly.

00:13:16.580 --> 00:13:19.740
And we're not, we can't depend
on the cloud as you know.

00:13:19.740 --> 00:13:22.710
We can't get all kinds
of real-time response,

00:13:22.710 --> 00:13:23.966
at least not yet.

00:13:23.966 --> 00:13:26.340
So the idea is that you can
do a lots of pre-computation.

00:13:26.340 --> 00:13:27.730
And I'll be talking
about some of what

00:13:27.730 --> 00:13:29.622
some of those architectures
might look like.

00:13:29.622 --> 00:13:31.330
But the idea is that
you're pre-computing

00:13:31.330 --> 00:13:33.282
a lots of things in
advance so that you

00:13:33.282 --> 00:13:34.490
can index those in real-time.

00:13:34.490 --> 00:13:35.710
And then make use of them.

00:13:35.710 --> 00:13:39.650
So there will, also, still
be local computation.

00:13:39.650 --> 00:13:43.860
So, in summary, these
are the five benefits

00:13:43.860 --> 00:13:45.380
that the cloud offers.

00:13:45.380 --> 00:13:47.470
The first is big data
so there's access

00:13:47.470 --> 00:13:51.180
to all these resources of
images, maps, and models.

00:13:51.180 --> 00:13:54.995
The idea of cloud computing
for a variety of computations,

00:13:54.995 --> 00:13:56.810
including statistical learning.

00:13:56.810 --> 00:13:58.790
The idea of open
source so humans

00:13:58.790 --> 00:14:02.400
are able to share code,
data, and designs.

00:14:02.400 --> 00:14:04.470
The idea of robots
sharing information,

00:14:04.470 --> 00:14:07.590
so learning from each other
and all those experiences being

00:14:07.590 --> 00:14:11.720
accumulated so they can
be, basically, combined

00:14:11.720 --> 00:14:14.900
and accessed globally
when on-demand.

00:14:14.900 --> 00:14:17.390
And then, the last is
this idea of call centers.

00:14:17.390 --> 00:14:19.390
[VIDEO PLAYBACK]

00:14:19.390 --> 00:14:21.858
-You fly that thing?

00:14:21.858 --> 00:14:22.358
-Not yet.

00:14:26.286 --> 00:14:26.786
-Operator.

00:14:26.786 --> 00:14:30.714
-Kenny, can I get a pilot to
program the self-helicopter?

00:14:30.714 --> 00:14:31.214
Hurry.

00:14:40.062 --> 00:14:40.562
Let's go.

00:14:40.562 --> 00:14:42.040
[END VIDEO PLAYBACK]

00:14:42.040 --> 00:14:43.410
KEN GOLDBERG: So you remember
that scene from "Matrix,"

00:14:43.410 --> 00:14:43.590
right?

00:14:43.590 --> 00:14:45.920
This is sort of the idea
we're talking about which

00:14:45.920 --> 00:14:50.810
is that the robot, in this
case, the person doesn't need

00:14:50.810 --> 00:14:53.250
to have all that information
stored in her head.

00:14:53.250 --> 00:14:54.502
She can access it on-demand.

00:14:54.502 --> 00:14:56.960
And so this is very similar
for the idea of cloud robotics.

00:14:59.441 --> 00:14:59.940
OK.

00:14:59.940 --> 00:15:01.212
I have to do this manually.

00:15:04.906 --> 00:15:06.444
That's for the part two.

00:15:10.396 --> 00:15:11.810
And play.

00:15:11.810 --> 00:15:12.310
Good.

00:15:12.310 --> 00:15:12.870
All right.

00:15:12.870 --> 00:15:16.672
So that introduces this
idea in a nutshell.

00:15:16.672 --> 00:15:19.130
And now, I want to go into some
of the examples of research

00:15:19.130 --> 00:15:22.280
that we're doing in our
lab and in collaboration.

00:15:22.280 --> 00:15:24.430
Some of this is collaboration
with researchers

00:15:24.430 --> 00:15:25.500
here at Google.

00:15:25.500 --> 00:15:27.700
So the first one has
to do with grasping.

00:15:27.700 --> 00:15:33.375
And the truth of what it
looks like, let's say,

00:15:33.375 --> 00:15:35.080
from a robot's
point of view, let's

00:15:35.080 --> 00:15:38.730
consider something as simple as
just sitting at a dinner table

00:15:38.730 --> 00:15:40.340
and wanting to pick up a cup.

00:15:40.340 --> 00:15:42.870
Now, it's something that humans
all, we all do effortlessly.

00:15:42.870 --> 00:15:43.578
It's very simple.

00:15:43.578 --> 00:15:48.900
But to put yourself into the
position of being a robot,

00:15:48.900 --> 00:15:53.500
this is what things
look like to the robot.

00:15:53.500 --> 00:15:55.930
So everything is very noisy.

00:15:55.930 --> 00:15:58.550
Your perception is imprecise.

00:15:58.550 --> 00:15:59.580
There's dropout.

00:15:59.580 --> 00:16:02.389
There's a lot of noise.

00:16:02.389 --> 00:16:04.180
And one of the other
things to keep in mind

00:16:04.180 --> 00:16:06.970
is you, also, don't even have
very good control over your end

00:16:06.970 --> 00:16:07.920
effectors.

00:16:07.920 --> 00:16:10.390
You don't even know where your
own hands and fingers are.

00:16:10.390 --> 00:16:13.290
So it's a very big
challenge on how to do this.

00:16:13.290 --> 00:16:16.590
And what we've
been thinking about

00:16:16.590 --> 00:16:21.570
is how can the cloud
be used as a benefit?

00:16:21.570 --> 00:16:24.190
And this is one idea
that we're exploring

00:16:24.190 --> 00:16:28.556
which is using technique,
using probability

00:16:28.556 --> 00:16:30.180
distributions to
model the environment.

00:16:30.180 --> 00:16:32.790
So because we don't
know it exactly,

00:16:32.790 --> 00:16:36.520
we can put distributions over
the objects in the environment.

00:16:36.520 --> 00:16:40.530
And then we can try to compute
the best strategy given

00:16:40.530 --> 00:16:42.390
all the distributions
that are there.

00:16:42.390 --> 00:16:45.480
And the way that we do
this is by sampling.

00:16:45.480 --> 00:16:48.320
So we're going to sample
from these distributions

00:16:48.320 --> 00:16:51.080
and then run an analysis
for each one of them.

00:16:51.080 --> 00:16:54.452
And again, this can be done
in parallel, in the cloud.

00:16:54.452 --> 00:16:55.910
And then we're
going to, basically,

00:16:55.910 --> 00:16:58.130
have all these
applications report back

00:16:58.130 --> 00:17:00.240
to be able to decide
which strategy,

00:17:00.240 --> 00:17:03.400
which motion has the highest
probability of success.

00:17:03.400 --> 00:17:05.640
So let me go into a
little more example

00:17:05.640 --> 00:17:09.579
in the particular
problem of grasping.

00:17:09.579 --> 00:17:11.700
And we'll look at a
two-dimensional problem.

00:17:11.700 --> 00:17:15.140
Here, we have this is
a part that a robot

00:17:15.140 --> 00:17:16.560
might want to pick up.

00:17:16.560 --> 00:17:18.270
Now, if you look at
this, you'd say, OK,

00:17:18.270 --> 00:17:20.280
it's obvious where I
want to grasp this.

00:17:20.280 --> 00:17:24.589
But the fact is that because the
robot's sensing is imprecise,

00:17:24.589 --> 00:17:27.199
the true object may
be any one of these.

00:17:27.199 --> 00:17:30.280
There's a very large
number of options.

00:17:30.280 --> 00:17:31.990
And we don't know.

00:17:31.990 --> 00:17:34.250
The robot doesn't know what
is the real object that's

00:17:34.250 --> 00:17:35.089
in front of it.

00:17:35.089 --> 00:17:37.130
So now the question is
what is the best strategy?

00:17:37.130 --> 00:17:40.600
Where should you grasp this
part given that uncertainty?

00:17:40.600 --> 00:17:43.770
So the idea is that we
can do an analysis given

00:17:43.770 --> 00:17:46.470
any one particular
shape of the object.

00:17:46.470 --> 00:17:48.360
We can perform a
mechanical analysis

00:17:48.360 --> 00:17:51.880
to figure out the success
of a particular grasp.

00:17:51.880 --> 00:17:54.420
And there's some
nice theory that

00:17:54.420 --> 00:17:57.860
uses coefficient of friction
and the pushing directions that

00:17:57.860 --> 00:18:01.660
can tell us whether a grasp
has a chance of success.

00:18:01.660 --> 00:18:04.330
And then, that's only
from one particular grasp.

00:18:04.330 --> 00:18:07.940
What we now want
to do is consider,

00:18:07.940 --> 00:18:10.460
for each of the
possible objects--

00:18:10.460 --> 00:18:18.440
So we consider all the objects
as a probability distribution.

00:18:18.440 --> 00:18:20.700
Then, we sample from
that distribution.

00:18:20.700 --> 00:18:22.210
And we send each
of those samples,

00:18:22.210 --> 00:18:25.280
or groups of those samples,
out to nodes in the cloud.

00:18:25.280 --> 00:18:26.630
And we have those.

00:18:26.630 --> 00:18:30.150
Then, each of those try a number
of different grasp strategies

00:18:30.150 --> 00:18:33.280
to determine what is
the probability of each

00:18:33.280 --> 00:18:34.770
of those grasp strategies.

00:18:34.770 --> 00:18:36.764
And then we accumulate
all this back.

00:18:36.764 --> 00:18:38.180
And the idea is
what we want to do

00:18:38.180 --> 00:18:41.290
is compute a probability or,
in this case a lower bound,

00:18:41.290 --> 00:18:44.576
on the probability of
a successful grasp.

00:18:44.576 --> 00:18:48.130
And what you're seeing here,
with this whisker diagram,

00:18:48.130 --> 00:18:51.360
is these are all approach
directions from the gripper.

00:18:51.360 --> 00:18:54.635
And the length of the whisker
is related to the probability

00:18:54.635 --> 00:18:57.460
that that approach direction
will be successful.

00:18:57.460 --> 00:18:58.550
So you can compute this.

00:18:58.550 --> 00:19:01.010
And, again, this is
very parallizable.

00:19:01.010 --> 00:19:03.750
And then, we want to be able
to do this in the cloud.

00:19:03.750 --> 00:19:08.740
And so here the algorithm
is outlined here.

00:19:08.740 --> 00:19:10.760
This is the result of
this whisker diagram

00:19:10.760 --> 00:19:12.190
for this particular
part after we

00:19:12.190 --> 00:19:14.640
sampled these different
approach directions.

00:19:14.640 --> 00:19:20.800
And this gives you some idea of
how this could be parallelized.

00:19:20.800 --> 00:19:24.310
We've actually run this on
some multi-core machines.

00:19:24.310 --> 00:19:26.150
And on of the things
that's interesting

00:19:26.150 --> 00:19:28.890
is that the results are
not always intuitive.

00:19:28.890 --> 00:19:31.180
So that, for example,
this one-- part

00:19:31.180 --> 00:19:33.690
D-- the intuition
would be to have

00:19:33.690 --> 00:19:35.580
the robot gripper come here.

00:19:35.580 --> 00:19:40.750
And that actually would,
it turns out, not be very,

00:19:40.750 --> 00:19:43.300
the probability of
success is not that high

00:19:43.300 --> 00:19:46.240
because of the uncertainty
in the part's shape.

00:19:46.240 --> 00:19:49.880
So there's a lot of opportunity
for these two corners, here,

00:19:49.880 --> 00:19:51.850
to intersect with the
gripper if you do that.

00:19:51.850 --> 00:19:55.390
So it turns out this is actually
the optimal grasp in this case.

00:19:55.390 --> 00:19:57.870
So, again, we can't trust
our intuition completely.

00:19:57.870 --> 00:20:01.660
But the computation can
be done by sampling.

00:20:01.660 --> 00:20:03.980
And this can be done very
rapidly in the cloud.

00:20:03.980 --> 00:20:07.800
And we've done some
experiments using PiCloud,

00:20:07.800 --> 00:20:09.930
an architecture that's
available on the web.

00:20:09.930 --> 00:20:11.760
And we're getting
results like this.

00:20:11.760 --> 00:20:15.000
So we're seeing that we
are getting approximately

00:20:15.000 --> 00:20:17.630
linear speedup for
many of the cases.

00:20:17.630 --> 00:20:19.130
But not for all of them.

00:20:19.130 --> 00:20:22.365
And one of the things that
we've recently realized

00:20:22.365 --> 00:20:25.570
is that there's a problem with
dropout where some of the nodes

00:20:25.570 --> 00:20:26.800
don't return.

00:20:26.800 --> 00:20:29.630
So we have to be really
smart about the sampling.

00:20:29.630 --> 00:20:32.850
We have oversample the parts so
that we can allow for dropout.

00:20:32.850 --> 00:20:35.740
And we, also, have to really
do a-- the next idea-- is

00:20:35.740 --> 00:20:36.890
to do adaptive sampling.

00:20:36.890 --> 00:20:39.340
So we don't have to depend
on all these processors

00:20:39.340 --> 00:20:42.186
all coming back to us
within a given time.

00:20:42.186 --> 00:20:45.480
So a lot of interesting
new work to be done there.

00:20:45.480 --> 00:20:47.150
The second area,
under grasping, has

00:20:47.150 --> 00:20:49.070
to do with the object
identification.

00:20:49.070 --> 00:20:51.160
So as I mentioned earlier,
you're moving around.

00:20:51.160 --> 00:20:52.951
Robot's moving around
in a new environment.

00:20:52.951 --> 00:20:57.070
And it comes across an object
that it doesn't recognize.

00:20:57.070 --> 00:21:00.410
So this is where I started
working with James here

00:21:00.410 --> 00:21:03.730
at Google, about two
years ago, on using

00:21:03.730 --> 00:21:06.580
the Google's Recognition Engine.

00:21:06.580 --> 00:21:09.075
And as you all know,
Goggles is very effective.

00:21:09.075 --> 00:21:11.670
It's been running
for many years now.

00:21:11.670 --> 00:21:15.630
And it has built up a fairly
big library of images,

00:21:15.630 --> 00:21:17.370
and tagged images.

00:21:17.370 --> 00:21:20.430
It's using machine learning to
associate those tagged images

00:21:20.430 --> 00:21:24.980
with sets of web pages.

00:21:24.980 --> 00:21:30.320
But our idea is to say, if
we can use the same system

00:21:30.320 --> 00:21:33.540
and adapt it so that instead
of giving us web pages,

00:21:33.540 --> 00:21:35.370
we could look at an
object, take an image,

00:21:35.370 --> 00:21:37.530
and send it up to the web.

00:21:37.530 --> 00:21:43.430
And then index, into the cloud,
a variety of descriptors,

00:21:43.430 --> 00:21:44.980
of semantics for that object.

00:21:44.980 --> 00:21:48.810
So it could give us things
like the exact geometry,

00:21:48.810 --> 00:21:51.030
or a model, 3D model
of that object.

00:21:51.030 --> 00:21:53.170
We could learn about its
physical properties--

00:21:53.170 --> 00:21:57.610
it's mass, it's
friction, it's mechanics.

00:21:57.610 --> 00:22:00.080
And, importantly, also we
could pre-compute a variety

00:22:00.080 --> 00:22:02.049
of different grasping
strategies that

00:22:02.049 --> 00:22:03.590
would be appropriate
for that object.

00:22:03.590 --> 00:22:06.450
So those could all be stored in
a cloud with those image tags.

00:22:06.450 --> 00:22:08.120
So that would
allow the robot to,

00:22:08.120 --> 00:22:10.832
then, successfully
pick that object up.

00:22:10.832 --> 00:22:12.800
So we've been on the
architecture of this.

00:22:12.800 --> 00:22:15.020
We implemented a
version of this.

00:22:15.020 --> 00:22:18.100
And the idea is
that we're making

00:22:18.100 --> 00:22:21.169
use of the vast resources that
Google is employing everyday,

00:22:21.169 --> 00:22:22.460
where people are taking images.

00:22:22.460 --> 00:22:25.010
They're also tagging images.

00:22:25.010 --> 00:22:28.015
And this is what's allowing
the Google Object Recognition

00:22:28.015 --> 00:22:29.620
Engine to work so well.

00:22:29.620 --> 00:22:33.400
And then, the idea,
and this is something

00:22:33.400 --> 00:22:35.640
that we would hope
to see in the future,

00:22:35.640 --> 00:22:39.312
is that companies
and other resources

00:22:39.312 --> 00:22:40.770
would be available
that would allow

00:22:40.770 --> 00:22:44.620
us to take all kinds of this
semantic information, store

00:22:44.620 --> 00:22:47.980
that along with grasp analysis
that's done in the background

00:22:47.980 --> 00:22:49.882
online.

00:22:49.882 --> 00:22:53.260
And then-- and one of
these would be tools like

00:22:53.260 --> 00:22:57.040
Matei's-- a terrific grasp
analysis tool that we could

00:22:57.040 --> 00:22:58.790
grow on a variety of
different approaches.

00:22:58.790 --> 00:23:01.960
For example, simulated
annealing is one technique

00:23:01.960 --> 00:23:03.780
that could be used
to find good grasp

00:23:03.780 --> 00:23:07.745
but, again, is not practical
to do that in real-time.

00:23:07.745 --> 00:23:10.075
But could pre-computed offline.

00:23:10.075 --> 00:23:12.860
And then, online what
happens is the camera

00:23:12.860 --> 00:23:14.580
would take an
image of an object,

00:23:14.580 --> 00:23:17.630
send that up to the
Recognition Engine with label

00:23:17.630 --> 00:23:19.120
and identify that object.

00:23:19.120 --> 00:23:22.280
And then would send
back the CAD model.

00:23:22.280 --> 00:23:25.390
We would be able to use
our 3D sensing to adjust,

00:23:25.390 --> 00:23:27.780
to basically
transform that model

00:23:27.780 --> 00:23:29.930
to the environment
in front of us.

00:23:29.930 --> 00:23:32.120
But then, we'd also have
a set of candidate grasps,

00:23:32.120 --> 00:23:35.135
that we would select from,
based on obstructions

00:23:35.135 --> 00:23:38.599
and whatever limitations were
present in the environment.

00:23:38.599 --> 00:23:40.390
But then, what's kind
of, also, interesting

00:23:40.390 --> 00:23:44.005
is that we would choose a grasp
and, then, execute that grasp.

00:23:44.005 --> 00:23:47.170
And then , close the loop
by sending the results back

00:23:47.170 --> 00:23:48.360
into the cloud.

00:23:48.360 --> 00:23:50.830
So if the grasp was
successful, we'd

00:23:50.830 --> 00:23:52.950
be able to report that
so it accumulates,

00:23:52.950 --> 00:23:55.446
the probability should
be adjusted accordingly.

00:23:55.446 --> 00:23:57.820
Or if it's unsuccessful, which
is particularly important,

00:23:57.820 --> 00:24:00.990
you'd be able to instantly
learn from that mistake.

00:24:00.990 --> 00:24:03.700
And so that grasp would be
removed from the library.

00:24:03.700 --> 00:24:05.495
And other robots
could be notified

00:24:05.495 --> 00:24:08.770
for the next time they try
to pick up that object.

00:24:08.770 --> 00:24:12.320
So this is the paper that we
published last year on this.

00:24:12.320 --> 00:24:15.520
And we have some
more results on that.

00:24:15.520 --> 00:24:17.135
It's available online.

00:24:17.135 --> 00:24:18.890
Now, a shift, in
the last few minutes

00:24:18.890 --> 00:24:20.840
I have, to talk
about health care.

00:24:20.840 --> 00:24:24.390
And then I'll be able
to take your questions.

00:24:24.390 --> 00:24:26.276
On health care,
there's two areas

00:24:26.276 --> 00:24:27.400
that we've been looking at.

00:24:27.400 --> 00:24:29.310
One has to do with
radiation therapy.

00:24:29.310 --> 00:24:33.610
And I hope that none of you, in
the audience, ever need this.

00:24:33.610 --> 00:24:38.695
But if you have a cancer
that is in a body cavity,

00:24:38.695 --> 00:24:40.262
there are a variety
of treatments.

00:24:40.262 --> 00:24:41.970
But one of them is
something called Reiki

00:24:41.970 --> 00:24:44.350
therapy, intracavitary
Reiki therapy.

00:24:44.350 --> 00:24:46.030
And what they do
is they, basically,

00:24:46.030 --> 00:24:51.230
insert radioactive seeds
into the body using needles.

00:24:51.230 --> 00:24:56.050
And then those seeds expose
the cancer to radiation,

00:24:56.050 --> 00:25:00.030
hopefully killing the tumors
and sparing the healthy tissue.

00:25:00.030 --> 00:25:03.340
Now, these are some of the
devices, that are used today,

00:25:03.340 --> 00:25:07.450
to achieve, to guide the
radioactivity into place.

00:25:07.450 --> 00:25:11.280
And these look like medieval
torture instruments.

00:25:11.280 --> 00:25:14.630
They really haven't
changed much in many years.

00:25:14.630 --> 00:25:16.510
And what's important
to notice is

00:25:16.510 --> 00:25:18.170
that they're all standardized.

00:25:18.170 --> 00:25:21.720
So they're not customized
to the shape of the body.

00:25:21.720 --> 00:25:25.020
So our idea is to
use 3D printing.

00:25:25.020 --> 00:25:28.200
And to develop a new
version of these implants,

00:25:28.200 --> 00:25:32.190
these applicators that
are custom-designed

00:25:32.190 --> 00:25:34.620
for the anatomy
of the individual.

00:25:34.620 --> 00:25:39.180
So this is an example for a
gynecological case where we're

00:25:39.180 --> 00:25:42.150
able to scan the body, build
a three-dimensional model.

00:25:42.150 --> 00:25:45.400
And then generate
an implant that's

00:25:45.400 --> 00:25:47.730
tailored to the
shape of the cavity.

00:25:47.730 --> 00:25:49.850
But what's really
interesting is that we also

00:25:49.850 --> 00:25:53.810
plan channels, within the
cavity, using 3D printing,

00:25:53.810 --> 00:25:56.200
so that we can guide
the seeds right

00:25:56.200 --> 00:26:01.130
next to, so they can dwell
right next to the tumor zone

00:26:01.130 --> 00:26:05.600
and, then, be quickly moved
away to avoid contact,

00:26:05.600 --> 00:26:08.030
minimize radiation to
the healthy tissue.

00:26:08.030 --> 00:26:10.090
So it's, essentially, a
motion planning problem.

00:26:10.090 --> 00:26:13.500
It's how do you design
these paths, these channels

00:26:13.500 --> 00:26:18.510
through the solid material
that will achieve this desired

00:26:18.510 --> 00:26:23.460
doses to the tumors and minimize
doses to the healthy tissue?

00:26:23.460 --> 00:26:26.652
And you have, essentially, this
is a classic motion planning

00:26:26.652 --> 00:26:28.360
problem where we have
multiple paths that

00:26:28.360 --> 00:26:30.980
have to coexist
and be disjoined.

00:26:30.980 --> 00:26:34.120
So we've been looking at this
in a variety of contexts.

00:26:34.120 --> 00:26:37.350
We're working with
faculty at UCSF.

00:26:37.350 --> 00:26:39.070
And we have some
initial results.

00:26:39.070 --> 00:26:43.634
For example, we did a comparison
between a, in simulation,

00:26:43.634 --> 00:26:45.050
we did a comparison
between what's

00:26:45.050 --> 00:26:47.680
news today, which is
a standardized ring.

00:26:47.680 --> 00:26:50.580
As you can see, because of
the proximity, the distance

00:26:50.580 --> 00:26:53.520
between the dwell
points to the tumors,

00:26:53.520 --> 00:26:55.820
the performance here
is not very good.

00:26:55.820 --> 00:26:58.040
But this is what you would
achieve with, let's say,

00:26:58.040 --> 00:27:01.630
a classic technique like
drilling holes in an implant.

00:27:01.630 --> 00:27:04.271
So you have only
linear channels.

00:27:04.271 --> 00:27:06.270
And so, here, you'd get
a number of dwell points

00:27:06.270 --> 00:27:07.630
right at the tip.

00:27:07.630 --> 00:27:09.715
But because of crowding,
there's a limitation

00:27:09.715 --> 00:27:12.011
of how many dwell
points you can get.

00:27:12.011 --> 00:27:13.510
And then this is
the idea that we're

00:27:13.510 --> 00:27:17.419
exploring which is where you
have curved channels that

00:27:17.419 --> 00:27:19.210
can then be-- and the
only way to fabricate

00:27:19.210 --> 00:27:22.400
these is by 3D printing.

00:27:22.400 --> 00:27:24.980
And I should say that the idea
of this is how to computing

00:27:24.980 --> 00:27:28.510
the optimal set of channels
here is something that can also

00:27:28.510 --> 00:27:30.814
be parallelized and
done in the class.

00:27:30.814 --> 00:27:32.230
The last thing
I'll tell you about

00:27:32.230 --> 00:27:34.210
is something we're calling
superhuman surgery.

00:27:34.210 --> 00:27:35.930
And this is done
with Jur van den

00:27:35.930 --> 00:27:39.480
Berg who's now here at Google,
sitting right over there.

00:27:39.480 --> 00:27:44.420
And the idea is that--
you've, hopefully, heard

00:27:44.420 --> 00:27:47.790
about what the da Vinci
robot System that's

00:27:47.790 --> 00:27:50.740
been used for many
surgeries around the world.

00:27:50.740 --> 00:27:53.680
It's in 2,000
operating rooms today.

00:27:53.680 --> 00:27:55.940
And this is a, it's a
very effective tool.

00:27:55.940 --> 00:27:58.231
But one of the things that's
important to know about it

00:27:58.231 --> 00:28:00.860
is that it's operated
purely in master-slave mode.

00:28:00.860 --> 00:28:04.230
So it's always under complete
control of the human surgeon.

00:28:04.230 --> 00:28:08.580
The robot is just reflecting
what the surgeon's motions are.

00:28:08.580 --> 00:28:10.900
Now, what we were
interested in is can we

00:28:10.900 --> 00:28:12.280
start to relax that subject?

00:28:12.280 --> 00:28:14.310
Can we, not replace
the human entirely,

00:28:14.310 --> 00:28:19.320
but can we have certain
subtasks performed autonomously

00:28:19.320 --> 00:28:21.350
under the supervision
of the doctor?

00:28:21.350 --> 00:28:23.900
So, for example,
there's two benefits.

00:28:23.900 --> 00:28:25.510
One is to reduce fatigue.

00:28:25.510 --> 00:28:28.390
So something like
suturing can be

00:28:28.390 --> 00:28:31.705
very challenging for a doctor.

00:28:31.705 --> 00:28:32.485
It's just tedious.

00:28:32.485 --> 00:28:34.360
And they actually spend
a fair amount of time

00:28:34.360 --> 00:28:35.890
performing these sutures.

00:28:35.890 --> 00:28:38.000
So if a doctor could
specify, the surgeon

00:28:38.000 --> 00:28:41.500
could specify the
position of sutures,

00:28:41.500 --> 00:28:43.730
the robot could
perform automonously.

00:28:43.730 --> 00:28:46.140
And the other advantage
is for telesurgery

00:28:46.140 --> 00:28:47.850
so that we could
allow, let's say,

00:28:47.850 --> 00:28:51.340
a master surgeon, who
may be located very far

00:28:51.340 --> 00:28:54.555
away from a patient,
could perform surgery

00:28:54.555 --> 00:28:55.920
over a distance.

00:28:55.920 --> 00:28:58.350
The challenge, today, is that
directly operating the robot

00:28:58.350 --> 00:29:01.140
is not practical
because of time delays.

00:29:01.140 --> 00:29:04.830
But if we could automate
each of the subtasks,

00:29:04.830 --> 00:29:07.510
then the surgery might be
able to be accomplished

00:29:07.510 --> 00:29:11.130
by the surgeon in
a supervisory mode.

00:29:11.130 --> 00:29:13.950
So one of the challenges
we had was, we were facing,

00:29:13.950 --> 00:29:16.380
is how can we automate
such subtasks?

00:29:16.380 --> 00:29:21.950
And this an example
of a human operating

00:29:21.950 --> 00:29:23.820
a robot to do suturing.

00:29:23.820 --> 00:29:28.160
And suturing is extremely
subtle and complex to program.

00:29:28.160 --> 00:29:31.920
So we decided that because
this is so complicated, what

00:29:31.920 --> 00:29:35.190
we would use is a technique that
was pioneered by my colleague

00:29:35.190 --> 00:29:37.350
Pieter Abbeel, at
Berkeley, which

00:29:37.350 --> 00:29:41.160
is to use robot learning
from demonstration.

00:29:41.160 --> 00:29:44.860
So we want to have a human,
an expert human surgeon

00:29:44.860 --> 00:29:48.900
perform demonstrations
of a task like suturing.

00:29:51.770 --> 00:29:55.070
And then, what we're going to
do is learn from those examples.

00:29:55.070 --> 00:29:57.660
So to illustrate that, I'll
just use this to give you

00:29:57.660 --> 00:30:00.080
the idea of how
this would work is

00:30:00.080 --> 00:30:03.540
let's say we consider a task
like performing this figure

00:30:03.540 --> 00:30:04.940
eight motion.

00:30:04.940 --> 00:30:07.420
Now, these are examples
of what the human might,

00:30:07.420 --> 00:30:09.730
the surgeon may
actually perform if we

00:30:09.730 --> 00:30:13.432
asked him or her to
perform this trajectory.

00:30:13.432 --> 00:30:14.640
Now, these are not very good.

00:30:14.640 --> 00:30:16.350
But they're,
actually, this is just

00:30:16.350 --> 00:30:19.236
the nature of robotic
teleoperation today.

00:30:19.236 --> 00:30:21.860
It's that there's actually fair
amount of noise and imprecision

00:30:21.860 --> 00:30:23.510
in how it gets translated.

00:30:23.510 --> 00:30:26.735
So you might collect, let's say,
a dozen human demonstrations

00:30:26.735 --> 00:30:28.540
that look like this.

00:30:28.540 --> 00:30:32.237
But the idea is that they're
all attempts at some underlying,

00:30:32.237 --> 00:30:33.695
there is some
underlying trajectory

00:30:33.695 --> 00:30:35.780
that they all have in common.

00:30:35.780 --> 00:30:40.030
So we can treat that
as a latent signal

00:30:40.030 --> 00:30:44.130
that we're trying to infer from
a number of noisy observations.

00:30:44.130 --> 00:30:47.180
So we can use, first of
all, dynamic time warping,

00:30:47.180 --> 00:30:50.280
which is well-known and
well-developed from speech

00:30:50.280 --> 00:30:52.960
recognition, to time
align all of the examples

00:30:52.960 --> 00:30:55.260
that were given.

00:30:55.260 --> 00:30:58.910
And then, we basically treat
them as noisy observations.

00:30:58.910 --> 00:31:03.100
And we use a common filter
model, a linear dynamic model

00:31:03.100 --> 00:31:05.780
where we're basically
take the data

00:31:05.780 --> 00:31:09.010
and run it through this
common smoother that

00:31:09.010 --> 00:31:13.810
allows us to extract parameters
for an underlying signal that's

00:31:13.810 --> 00:31:16.960
the latent trajectory.

00:31:16.960 --> 00:31:18.650
So we take these
human demonstrations

00:31:18.650 --> 00:31:20.597
and we get something
that looks like this.

00:31:20.597 --> 00:31:22.305
Now, what's also nice
about this approach

00:31:22.305 --> 00:31:25.140
is that also results
in a smooth trajectory.

00:31:25.140 --> 00:31:30.470
So it takes out a lot of the
jaggedness, the high frequency

00:31:30.470 --> 00:31:32.380
elements out of the trajectory.

00:31:32.380 --> 00:31:35.330
And then, what we want to do is
now take this and execute this

00:31:35.330 --> 00:31:38.640
on the trajectory on the robot.

00:31:38.640 --> 00:31:42.297
We perform it, we use it
to perform the motion.

00:31:42.297 --> 00:31:43.630
And then, we observe the motion.

00:31:43.630 --> 00:31:44.870
And it's not always perfect.

00:31:44.870 --> 00:31:47.190
So then, we adjust
it using ideas

00:31:47.190 --> 00:31:49.230
called iterative learning.

00:31:49.230 --> 00:31:52.600
So what we're going to do
is observe the results,

00:31:52.600 --> 00:31:54.990
change the parameters,
observe the results again

00:31:54.990 --> 00:31:59.046
until the trajectory looks
like very close to what

00:31:59.046 --> 00:32:00.100
we have been speaking.

00:32:00.100 --> 00:32:01.710
But then, the new
idea-- and this

00:32:01.710 --> 00:32:04.500
is where it becomes
superhuman-- is that we also

00:32:04.500 --> 00:32:08.830
want to increase the speed,
increase the velocity.

00:32:08.830 --> 00:32:10.980
So now, within this
framework we now,

00:32:10.980 --> 00:32:12.550
in the iterative
learning phase, we

00:32:12.550 --> 00:32:14.980
increase, we turn up the speed.

00:32:14.980 --> 00:32:16.340
And we run it again.

00:32:16.340 --> 00:32:18.580
And it deviates from
the desired trajectory.

00:32:18.580 --> 00:32:22.030
We adjust the parameters, run
it again until it converges.

00:32:22.030 --> 00:32:24.130
And then increase
the speed again.

00:32:24.130 --> 00:32:27.830
So the idea is can we keep
increasing the speed over time

00:32:27.830 --> 00:32:31.930
so that what we'll end up with
is performance with a control

00:32:31.930 --> 00:32:34.140
signal that will, then,
give us a trajectory?

00:32:34.140 --> 00:32:38.156
This is what it would look
like at one time, four times.

00:32:38.156 --> 00:32:40.738
This is at seven times speed up.

00:32:40.738 --> 00:32:43.130
And here's at 10 times speed up.

00:32:43.130 --> 00:32:45.110
So, actually, getting
something that's

00:32:45.110 --> 00:32:48.470
very close to what we want
but faster than the samples

00:32:48.470 --> 00:32:50.060
that we actually collected.

00:32:50.060 --> 00:32:53.250
So the goal is, here, and this
is still a work-in-progress,

00:32:53.250 --> 00:32:56.270
is can we get the
robot to actually do

00:32:56.270 --> 00:32:59.230
something that is better than
what even the best surgeons can

00:32:59.230 --> 00:32:59.730
do?

00:32:59.730 --> 00:33:03.870
In other words, can it do
it faster and more precise?

00:33:03.870 --> 00:33:05.360
So we're working
now with a team.

00:33:05.360 --> 00:33:07.540
We have a National
Science Foundation grant.

00:33:07.540 --> 00:33:10.090
We're working with a team
of colleagues on this.

00:33:10.090 --> 00:33:13.660
This a master surgeon, from
UC Davis, who's helping us.

00:33:13.660 --> 00:33:16.430
And we're working
with, this is the Raven

00:33:16.430 --> 00:33:22.450
which is an open-source
surgical robot system.

00:33:22.450 --> 00:33:26.087
So I'll just close with some
future directions and some

00:33:26.087 --> 00:33:27.420
of the things I'm excited about.

00:33:27.420 --> 00:33:31.130
One is this area that
we call belief space.

00:33:31.130 --> 00:33:35.502
And you're fortunate to have
some of the world pioneers

00:33:35.502 --> 00:33:38.910
and world experts
right here at Google.

00:33:38.910 --> 00:33:40.160
Jur being one of them.

00:33:40.160 --> 00:33:43.730
And here's the idea is that
if we imagine this is a robot,

00:33:43.730 --> 00:33:46.780
here, that wants to get into
this green zone over here,

00:33:46.780 --> 00:33:48.400
passed two obstacles.

00:33:48.400 --> 00:33:52.520
And let's say there's two
light sources here and here.

00:33:52.520 --> 00:33:55.470
The classic technique is
to sort of move like this.

00:33:55.470 --> 00:34:01.380
And these circles indicate
the position uncertainty

00:34:01.380 --> 00:34:02.480
for the robot.

00:34:02.480 --> 00:34:05.220
And as you can see, there's
a fairly decent probability

00:34:05.220 --> 00:34:08.310
the robot will have a collision
with one of the two obstacles

00:34:08.310 --> 00:34:09.204
en route.

00:34:09.204 --> 00:34:11.120
But the new idea is that
with the belief space

00:34:11.120 --> 00:34:13.540
is that you actually
take this into account.

00:34:13.540 --> 00:34:15.790
And you build in a
model of uncertainty

00:34:15.790 --> 00:34:18.409
so that it actually,
the optimal path

00:34:18.409 --> 00:34:21.250
is to move like this
down into the light zone.

00:34:21.250 --> 00:34:23.440
So that where the
uncertainty can be reduced.

00:34:23.440 --> 00:34:28.060
And then, the path has a very
low probability of collision.

00:34:28.060 --> 00:34:31.219
So this analysis is
subtle and complex.

00:34:31.219 --> 00:34:34.550
And when it's Gaussian
it's somewhat tractable.

00:34:34.550 --> 00:34:37.570
When it's multimodal,
a mixture of Gaussians,

00:34:37.570 --> 00:34:39.679
it's actually very,
very challenging.

00:34:39.679 --> 00:34:42.860
But how can we do this
kind of calculations?

00:34:42.860 --> 00:34:44.699
And there's a lot
of excitement, now,

00:34:44.699 --> 00:34:47.750
that there are techniques
that can reduce the complexity

00:34:47.750 --> 00:34:49.810
and that we can perform
this in a cloud.

00:34:49.810 --> 00:34:53.210
We can do some of
this planning using

00:34:53.210 --> 00:34:55.976
the offline cluster-- sorry.

00:34:55.976 --> 00:34:58.600
A cluster is available
in the cloud.

00:34:58.600 --> 00:35:00.845
And the other area
is Google Glass.

00:35:00.845 --> 00:35:04.330
I see at least one person
in the audience wearing it.

00:35:04.330 --> 00:35:07.480
There's idea this can be,
provide augmented reality

00:35:07.480 --> 00:35:08.905
and lots of
information on-demand.

00:35:08.905 --> 00:35:11.150
But it's also
interesting in that it

00:35:11.150 --> 00:35:16.670
can be a way of collecting
vast amounts of video

00:35:16.670 --> 00:35:18.805
of human experience
through the eye,

00:35:18.805 --> 00:35:21.540
through the point of
view of the human.

00:35:21.540 --> 00:35:23.865
And we think that, actually,
may be very, very valuable

00:35:23.865 --> 00:35:26.950
because if you've
collected all this data,

00:35:26.950 --> 00:35:30.250
and you've used deep learning,
other techniques that you could

00:35:30.250 --> 00:35:35.560
have that may be able to
extract structure and features

00:35:35.560 --> 00:35:39.270
from vast data sets, from vast
libraries of data of humans

00:35:39.270 --> 00:35:41.050
manipulating objects
in the environment.

00:35:41.050 --> 00:35:44.250
Could we start to learn
structure and manipulation

00:35:44.250 --> 00:35:49.170
strategies from
all these samples?

00:35:49.170 --> 00:35:50.670
There's many other
things going on.

00:35:50.670 --> 00:35:53.620
This is a robot that
has become available.

00:35:53.620 --> 00:35:54.850
It's a very low cost.

00:35:54.850 --> 00:35:58.185
It's about $150, by a
company here in the Bay Area,

00:35:58.185 --> 00:35:59.710
called Romo.

00:35:59.710 --> 00:36:04.090
And what's nice about it is
it makes use of the cell phone

00:36:04.090 --> 00:36:07.490
and all the advances that are
happening with cell phones.

00:36:07.490 --> 00:36:10.390
So that, onboard, it has
cameras, computation,

00:36:10.390 --> 00:36:13.720
networking, speech
recognition, et cetera.

00:36:13.720 --> 00:36:17.160
And so the robot system could
be very low cost as a result.

00:36:17.160 --> 00:36:19.770
And what's nice is
that this designed

00:36:19.770 --> 00:36:22.090
to work very closely
in the cloud.

00:36:22.090 --> 00:36:24.870
So this robot can be
used for telepresence

00:36:24.870 --> 00:36:28.070
but, also, can then be
automatically updated

00:36:28.070 --> 00:36:32.130
with new software and data
as it emerges on the cloud.

00:36:32.130 --> 00:36:34.350
So that's an example where
the robot, essentially,

00:36:34.350 --> 00:36:35.920
is very low cost.

00:36:35.920 --> 00:36:38.370
But it's taking
advantage in leveraging

00:36:38.370 --> 00:36:43.515
the cloud, the voucher
resources that are on the cloud.

00:36:43.515 --> 00:36:44.890
And then, even as
some people are

00:36:44.890 --> 00:36:47.098
thinking about things like
this, the robot app store.

00:36:47.098 --> 00:36:49.990
Where you'll have something
like an app store for robots.

00:36:49.990 --> 00:36:51.870
So when a robot is
moving around and decides

00:36:51.870 --> 00:36:53.720
it needs to learn
something new, it

00:36:53.720 --> 00:36:57.320
can download an app on-demand
for solving some interesting,

00:36:57.320 --> 00:36:59.900
for some problem it
doesn't know about.

00:36:59.900 --> 00:37:02.540
This all ties into
Internet of Things.

00:37:02.540 --> 00:37:04.690
So there's been a lot of
discussion about the idea

00:37:04.690 --> 00:37:06.575
that all kinds of
objects, not just robots,

00:37:06.575 --> 00:37:07.700
will be networked together.

00:37:07.700 --> 00:37:09.620
This will be a great
benefit for robots,

00:37:09.620 --> 00:37:13.220
obviously, because they'll be
able to access and communicate

00:37:13.220 --> 00:37:16.970
with devices and sensors
in their environment.

00:37:16.970 --> 00:37:19.660
And General Electric is
talking about this context

00:37:19.660 --> 00:37:25.970
of for their industrial
systems for large, for example,

00:37:25.970 --> 00:37:28.432
airplane turbine engines
will be communicating

00:37:28.432 --> 00:37:29.640
with each other in the cloud.

00:37:29.640 --> 00:37:31.640
So they'll be able to
share operating set

00:37:31.640 --> 00:37:33.822
points across many
different engines.

00:37:33.822 --> 00:37:35.530
And there's lots of
interesting questions

00:37:35.530 --> 00:37:39.590
about proprietary data, how
do you share information

00:37:39.590 --> 00:37:44.120
without violating the
confidences between, let's say,

00:37:44.120 --> 00:37:46.500
competing companies.

00:37:46.500 --> 00:37:48.490
Now there's a lot of
things going on here.

00:37:48.490 --> 00:37:53.260
And there's a resource that
we've set up at Berkeley.

00:37:53.260 --> 00:37:56.410
It's a website if want
to learn more about this.

00:37:56.410 --> 00:37:59.095
We keep it up-to-date
with new information.

00:37:59.095 --> 00:38:02.860
And one thing I want to
mention is a special issue

00:38:02.860 --> 00:38:06.270
of this journal, the IEEE
Transactions on Automation

00:38:06.270 --> 00:38:07.572
Science and Engineering.

00:38:07.572 --> 00:38:10.030
And if you want to publish any
papers in this general area,

00:38:10.030 --> 00:38:12.896
I encourage you to submit them.

00:38:12.896 --> 00:38:16.750
This special issue will
come out next year.

00:38:16.750 --> 00:38:21.440
And I want thank some of
the organizations that

00:38:21.440 --> 00:38:24.840
have made this work possible
for my students and I.

00:38:24.840 --> 00:38:28.120
And we're at
Berkeley if you want

00:38:28.120 --> 00:38:32.650
to come up, and visit us,
and learn more about this.

00:38:32.650 --> 00:38:35.030
So I'll close with
this quick summary

00:38:35.030 --> 00:38:38.600
of what cloud
robotics has to offer.

00:38:38.600 --> 00:38:46.290
And I can't overestimate the
amount of potential I see here.

00:38:46.290 --> 00:38:47.900
This is really
changing our field.

00:38:47.900 --> 00:38:51.520
It is rare, in the
course of your career,

00:38:51.520 --> 00:38:54.990
that you see a new development
occur that really changes

00:38:54.990 --> 00:38:57.100
the basic assumptions,
the fundamental ideas

00:38:57.100 --> 00:38:57.930
around robotics.

00:38:57.930 --> 00:38:59.388
And this is exactly
what's happened

00:38:59.388 --> 00:39:00.606
in the last couple of years.

00:39:00.606 --> 00:39:04.060
So you're exactly at the right
place for this here at Google.

00:39:04.060 --> 00:39:06.120
And thank you for
your time here today.

00:39:06.120 --> 00:39:07.320
Thanks.

00:39:07.320 --> 00:39:08.870
[APPLAUSE]

