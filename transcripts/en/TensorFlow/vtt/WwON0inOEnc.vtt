WEBVTT
Kind: captions
Language: en

00:00:02.032 --> 00:00:03.490
SANDEEP GUPTA:
Thank you, Laurence.

00:00:03.490 --> 00:00:05.830
So Laurence gave us a
very nice introduction

00:00:05.830 --> 00:00:07.630
to what is machine
learning and a glimpse

00:00:07.630 --> 00:00:09.130
of some of the
applications that are

00:00:09.130 --> 00:00:10.840
possible with machine learning.

00:00:10.840 --> 00:00:13.510
So at Google, we saw
the enormous potential

00:00:13.510 --> 00:00:15.760
that machine learning
was beginning

00:00:15.760 --> 00:00:18.490
to have all around
us, and that led us

00:00:18.490 --> 00:00:21.190
to releasing TensorFlow
as an open source platform

00:00:21.190 --> 00:00:24.400
back in 2015, and the
objective of doing

00:00:24.400 --> 00:00:28.030
that was to give everyone access
to this open source library,

00:00:28.030 --> 00:00:30.100
to develop machine
learning solutions,

00:00:30.100 --> 00:00:32.259
and thereby, our aim
was to accelerate

00:00:32.259 --> 00:00:34.690
the progress and pace of
the development of machine

00:00:34.690 --> 00:00:35.810
learning.

00:00:35.810 --> 00:00:38.440
So since that time,
TensorFlow has

00:00:38.440 --> 00:00:40.380
become enormously successful.

00:00:40.380 --> 00:00:42.850
In about these
three years, it has

00:00:42.850 --> 00:00:46.150
grown to become the number
1 repository on GitHub

00:00:46.150 --> 00:00:48.220
for machine learning,
and we are very

00:00:48.220 --> 00:00:51.850
proud to have been named as
the most loved software library

00:00:51.850 --> 00:00:56.050
or framework in the 2018 Stack
Overflow Developer Survey.

00:00:56.050 --> 00:00:59.110
So all of this success,
actually, in large part

00:00:59.110 --> 00:01:04.930
is due to the very enormous
group of users and developers

00:01:04.930 --> 00:01:08.020
that we have out there who are
using TensorFlow and building

00:01:08.020 --> 00:01:10.480
lot of these interesting
applications.

00:01:10.480 --> 00:01:12.550
We are blown away by
these download numbers.

00:01:12.550 --> 00:01:15.790
Used more than
17.5 million times.

00:01:15.790 --> 00:01:17.490
The number that we
are very proud of

00:01:17.490 --> 00:01:20.020
is the more than
1,600 contributors,

00:01:20.020 --> 00:01:24.340
and vast majority of them
being non-Google contributors.

00:01:24.340 --> 00:01:26.980
And you see very active
engagement by the TensorFlow

00:01:26.980 --> 00:01:29.930
Engineering team with the
community in answering issues

00:01:29.930 --> 00:01:33.230
and fielding pull
requests and so on.

00:01:33.230 --> 00:01:35.590
So it's a very tiring--

00:01:35.590 --> 00:01:38.530
an active ecosystem
that TensorFlow

00:01:38.530 --> 00:01:41.000
has generated around it.

00:01:41.000 --> 00:01:43.480
If you look at where
our users come from,

00:01:43.480 --> 00:01:45.650
they come from all
parts of the globe.

00:01:45.650 --> 00:01:49.360
So these are self-identified
locations from our users

00:01:49.360 --> 00:01:52.330
on GitHub who have
started TensorFlow,

00:01:52.330 --> 00:01:55.000
and you'll see that they
come from every time zone

00:01:55.000 --> 00:01:56.050
on the Earth--

00:01:56.050 --> 00:01:58.330
down in the south,
right up to Antarctica,

00:01:58.330 --> 00:02:01.750
and right up in the
Arctic Circle in Norway.

00:02:01.750 --> 00:02:04.180
And I think I see a dot there
in Ireland as well, which

00:02:04.180 --> 00:02:06.490
might be Laurence, yeah.

00:02:06.490 --> 00:02:11.510
So it's used everywhere for
a wide variety of things.

00:02:11.510 --> 00:02:15.040
So let me switch gears
and talk a little bit

00:02:15.040 --> 00:02:17.680
about the architecture
of TensorFlow-- what

00:02:17.680 --> 00:02:19.680
it is, what it lets you do.

00:02:19.680 --> 00:02:23.410
So I'll talk briefly about
the APIs that it has,

00:02:23.410 --> 00:02:26.860
and then describe the
platforms and the languages

00:02:26.860 --> 00:02:29.140
that it supports, and
then take a quick look

00:02:29.140 --> 00:02:31.090
at some of the tooling
that really makes it

00:02:31.090 --> 00:02:33.250
as a very useful platform
for doing machine

00:02:33.250 --> 00:02:39.050
learning and letting you do a
very versatile set of things.

00:02:39.050 --> 00:02:41.560
So to look at the
API for TensorFlow,

00:02:41.560 --> 00:02:45.130
fundamentally it's a
computation execution engine.

00:02:45.130 --> 00:02:48.580
And by that, what we mean is
that you called your machine

00:02:48.580 --> 00:02:50.900
learning algorithm--
or your steps,

00:02:50.900 --> 00:02:53.470
as Laurence was showing
in that example--

00:02:53.470 --> 00:02:56.980
and then TensorFlow
automates all the mechanics

00:02:56.980 --> 00:02:59.800
of the training process
and of running it

00:02:59.800 --> 00:03:03.250
on your device of interest,
and lets you build that model

00:03:03.250 --> 00:03:05.960
and then use it in
practical applications.

00:03:05.960 --> 00:03:09.540
So you do this by using
these higher-level APIs

00:03:09.540 --> 00:03:11.500
as an easy way to
get started, and we

00:03:11.500 --> 00:03:14.350
have two paths of doing this.

00:03:14.350 --> 00:03:16.290
Some of you might be
familiar with Keras

00:03:16.290 --> 00:03:19.030
as a library for developing
machine learning models,

00:03:19.030 --> 00:03:22.600
and TensorFlow has full
tight integration with Keras,

00:03:22.600 --> 00:03:25.180
and in fact, Keras is the
preferred high-level way

00:03:25.180 --> 00:03:26.950
of building machine
learning models

00:03:26.950 --> 00:03:30.670
using these LEGO bricks, which
let you piece together models

00:03:30.670 --> 00:03:34.810
one layer at a time and build
pretty complex architectures.

00:03:34.810 --> 00:03:36.610
So you can use
the Keras library.

00:03:36.610 --> 00:03:39.910
In addition, we also package
some of these very commonly

00:03:39.910 --> 00:03:43.990
used models by what
we call estimators,

00:03:43.990 --> 00:03:47.480
and these packaged models are
battle tested and hardened,

00:03:47.480 --> 00:03:49.540
and they let you
do your job quickly

00:03:49.540 --> 00:03:52.799
based on architectures that have
already proven to be valuable.

00:03:52.799 --> 00:03:54.340
And there's also a
lot of flexibility

00:03:54.340 --> 00:03:56.170
in customizing things
and changing things

00:03:56.170 --> 00:04:00.560
and wiring them anyway you
need to for your application.

00:04:00.560 --> 00:04:05.150
So these APIs for model
building are fed by data,

00:04:05.150 --> 00:04:07.300
and part of what
TensorFlow offers

00:04:07.300 --> 00:04:11.710
is very flexible libraries for
building these data pipelines--

00:04:11.710 --> 00:04:13.660
for bringing in data
into your models,

00:04:13.660 --> 00:04:15.840
and then doing some of
the feature transformation

00:04:15.840 --> 00:04:18.190
or pre-processing to
prepare the data so

00:04:18.190 --> 00:04:21.880
that it's ready to be ingested
by a machine learning model.

00:04:21.880 --> 00:04:23.710
And then once you
have all this set up,

00:04:23.710 --> 00:04:26.380
then we have this
distribution layer,

00:04:26.380 --> 00:04:29.320
which basically deals with
abstracting away your model

00:04:29.320 --> 00:04:31.810
and distributing your
training job to run

00:04:31.810 --> 00:04:35.110
on a CPU or single
GPU or multiple GPUs,

00:04:35.110 --> 00:04:37.630
or even on custom
architectures such as TPUs,

00:04:37.630 --> 00:04:41.330
which we'll talk a little bit
more about as we go forward.

00:04:41.330 --> 00:04:43.870
So having trained
your model, then, you

00:04:43.870 --> 00:04:45.830
save your model as an object.

00:04:45.830 --> 00:04:47.500
And this object
basically captures

00:04:47.500 --> 00:04:49.810
the architecture of
the model, as well as

00:04:49.810 --> 00:04:53.470
the rates and the
tuning of the knobs

00:04:53.470 --> 00:04:55.400
that you did during
your training phase,

00:04:55.400 --> 00:04:59.000
and now it's ready for
use in your application.

00:04:59.000 --> 00:05:00.820
And that's where
you, again, have

00:05:00.820 --> 00:05:05.380
a very flexible choice of how
to use that trained model.

00:05:05.380 --> 00:05:08.260
You can use a library such
as tf.serving to manage

00:05:08.260 --> 00:05:09.930
the whole process of serving.

00:05:09.930 --> 00:05:14.830
You can deploy it on mobile
devices or using TensorFlow.js.

00:05:14.830 --> 00:05:16.490
You can deploy it
in the browser.

00:05:16.490 --> 00:05:19.660
And we'll talk more about
all of these later today,

00:05:19.660 --> 00:05:21.880
as well as the
dedicated talks on each

00:05:21.880 --> 00:05:26.540
of these components, where
you can learn more about them.

00:05:26.540 --> 00:05:30.220
So on the platform side,
as I was saying earlier,

00:05:30.220 --> 00:05:33.760
TensorFlow lets you run
your machine learning models

00:05:33.760 --> 00:05:36.550
on CPUs, GPUs, as well
as on these custom

00:05:36.550 --> 00:05:39.920
hardwares such as TPUs, as
well as on the mobile devices,

00:05:39.920 --> 00:05:42.520
be it Android or iOS
framework, and then

00:05:42.520 --> 00:05:48.670
going forward on lots of these
embedded IoT type devices.

00:05:48.670 --> 00:05:51.300
So talking a little bit
more about the platforms,

00:05:51.300 --> 00:05:53.890
one platform that we're
particularly excited about

00:05:53.890 --> 00:05:55.300
is Cloud TPUs.

00:05:55.300 --> 00:05:59.570
So Cloud TPUs were announced
by Google actually last year,

00:05:59.570 --> 00:06:01.410
and then this version
2 of the Cloud TPUs

00:06:01.410 --> 00:06:06.250
was made general availability
earlier this year.

00:06:06.250 --> 00:06:09.070
And these are
specially-designed pieces

00:06:09.070 --> 00:06:11.110
of hardware from
the ground-up which

00:06:11.110 --> 00:06:13.600
are really, really
optimized for machine

00:06:13.600 --> 00:06:15.750
learning type of workloads.

00:06:15.750 --> 00:06:17.290
So they're blazingly
fast for that.

00:06:17.290 --> 00:06:20.380
Some of the specs are they're
extremely high performance

00:06:20.380 --> 00:06:23.860
in terms of compute, as
well as a large amount

00:06:23.860 --> 00:06:25.780
of very high bandwidth
memory, which

00:06:25.780 --> 00:06:28.750
lets you parallelize
your training job

00:06:28.750 --> 00:06:31.540
and take full advantage of
this kind of an architecture.

00:06:31.540 --> 00:06:34.720
And the nice thing is that
TensorFlow is the programming

00:06:34.720 --> 00:06:37.570
environment for Cloud TPUs,
so TensorFlow is very tightly

00:06:37.570 --> 00:06:41.960
coupled with being able to do
this seamlessly and easily.

00:06:41.960 --> 00:06:44.890
So let's see what's possible
wirh these types of devices.

00:06:44.890 --> 00:06:46.300
Here I'm showing
you some numbers

00:06:46.300 --> 00:06:50.250
from training, what's
called the ResNet-50 model.

00:06:50.250 --> 00:06:53.590
So ResNet-50 is one of the
most commonly used models

00:06:53.590 --> 00:06:55.360
for image classification.

00:06:55.360 --> 00:06:57.610
It's a very complex
deep learning

00:06:57.610 --> 00:06:58.960
model which has 50 layers.

00:06:58.960 --> 00:07:00.820
Has been named ResNet-50.

00:07:00.820 --> 00:07:03.880
And it has more than 20
million tunable parameters,

00:07:03.880 --> 00:07:07.790
which you have to optimize
during the course of training.

00:07:07.790 --> 00:07:10.720
So this turns out to be a
very commonly used benchmark

00:07:10.720 --> 00:07:12.250
to look at
performance of machine

00:07:12.250 --> 00:07:17.980
learning tools and models to
compare how well the system is

00:07:17.980 --> 00:07:19.420
optimized.

00:07:19.420 --> 00:07:22.610
So we train the ResNet-50
model on a public data set

00:07:22.610 --> 00:07:25.450
called ImageNet, which is a
data set of tens of millions

00:07:25.450 --> 00:07:28.720
of images that are labeled
for object recognition

00:07:28.720 --> 00:07:29.920
type of tasks.

00:07:29.920 --> 00:07:34.180
And this model could be trained
on Cloud TPUs for a total cost

00:07:34.180 --> 00:07:37.780
of less than $40 with an
extremely high image throughput

00:07:37.780 --> 00:07:39.200
rate, which is mentioned there.

00:07:39.200 --> 00:07:42.530
And that lets you take the
entire ImageNet data set

00:07:42.530 --> 00:07:44.270
and train your
model, in a matter

00:07:44.270 --> 00:07:48.320
of tens of minutes, what used
to take hours, if not days,

00:07:48.320 --> 00:07:50.940
a few months or years ago.

00:07:50.940 --> 00:07:53.705
So really exciting to see
the pace of this development

00:07:53.705 --> 00:07:56.720
to do machine learning at scale.

00:07:56.720 --> 00:07:59.430
On the other end
of the spectrum,

00:07:59.430 --> 00:08:02.630
we see enormous growth
in the capabilities

00:08:02.630 --> 00:08:04.970
of these small
minicomputer devices

00:08:04.970 --> 00:08:06.950
that we carry around
in our pockets.

00:08:06.950 --> 00:08:11.390
Smartphones, smart connected
devices, IoT devices--

00:08:11.390 --> 00:08:12.650
these are exploding.

00:08:12.650 --> 00:08:15.520
And I think by some
counts, their estimates

00:08:15.520 --> 00:08:18.980
are that there will be about
30 billion such devices

00:08:18.980 --> 00:08:20.390
within the next five years.

00:08:20.390 --> 00:08:23.090
And there are a lot of
machine learning applications

00:08:23.090 --> 00:08:25.440
that are possible on
these types of devices.

00:08:25.440 --> 00:08:28.130
So we have made it a priority
to make sure that TensorFlow

00:08:28.130 --> 00:08:29.690
runs-- and runs well--

00:08:29.690 --> 00:08:31.910
on these types of
devices by releasing

00:08:31.910 --> 00:08:34.620
a library called
TensorFlow Lite.

00:08:34.620 --> 00:08:37.370
So TensorFlow Lite is
a lightweight version

00:08:37.370 --> 00:08:42.640
of TensorFlow, and this
is how the workflow works.

00:08:42.640 --> 00:08:47.270
You take a TensorFlow model,
and you train it offline--

00:08:47.270 --> 00:08:51.950
let's say on a workstation
or in distributed computing.

00:08:51.950 --> 00:08:53.800
And you create your saved model.

00:08:53.800 --> 00:08:56.570
That saved model then goes
through a converter process

00:08:56.570 --> 00:08:59.210
where we convert it into a
model that's specifically

00:08:59.210 --> 00:09:00.710
optimized for mobile devices.

00:09:00.710 --> 00:09:03.210
We call it the
TensorFlow Lite format.

00:09:03.210 --> 00:09:05.270
So this TensorFlow
Lite model can now

00:09:05.270 --> 00:09:08.960
be installed on a mobile
device where we also

00:09:08.960 --> 00:09:11.340
have a runtime for TensorFlow,
which is a TensorFlow Lite

00:09:11.340 --> 00:09:12.110
interpreter.

00:09:12.110 --> 00:09:14.360
So it takes this
model, and it runs it.

00:09:14.360 --> 00:09:17.480
It binds to the local hardware
acceleration-- custom hardware

00:09:17.480 --> 00:09:20.390
acceleration-- on that device--
for example, on Android.

00:09:20.390 --> 00:09:23.490
You have the NNAPI interface--
the Neural Network interface--

00:09:23.490 --> 00:09:26.390
which takes advantage of
whatever might be the hardware

00:09:26.390 --> 00:09:29.600
configuration, and this gives
you a model that's lightweight.

00:09:29.600 --> 00:09:32.330
It uses less memory,
less power consumption,

00:09:32.330 --> 00:09:36.330
and is fast for
user mobile devices.

00:09:36.330 --> 00:09:40.640
So here's an example of how
this might look in practice.

00:09:40.640 --> 00:09:45.770
What you're seeing here is a
image classification example

00:09:45.770 --> 00:09:48.680
running on a mobile phone, and
we are holding common office

00:09:48.680 --> 00:09:51.920
objects in front of it, and it's
classifying them in real time--

00:09:51.920 --> 00:09:57.470
scissors and Post-its, and
obviously, a TensorFlow logo.

00:09:57.470 --> 00:10:00.860
So it's very easy to build
these types of models

00:10:00.860 --> 00:10:04.230
and get these applications
up-and-running very quickly,

00:10:04.230 --> 00:10:06.500
and there are
examples and tutorials

00:10:06.500 --> 00:10:10.800
on our YouTube channel to
show you how to do this.

00:10:10.800 --> 00:10:13.100
So in addition to the
flexibility on the platform

00:10:13.100 --> 00:10:15.800
side, TensorFlow also gives
you a lot of flexibility

00:10:15.800 --> 00:10:19.550
in programming languages that
you can use to call it in.

00:10:19.550 --> 00:10:23.489
So we've always had a large
collection of languages

00:10:23.489 --> 00:10:24.530
that have been supported.

00:10:24.530 --> 00:10:27.290
Python continues to be the
mainstay of machine learning,

00:10:27.290 --> 00:10:29.390
and lot of work being
done in that area.

00:10:29.390 --> 00:10:31.040
But you can see
many more languages,

00:10:31.040 --> 00:10:33.039
and most of these, actually,
have been developed

00:10:33.039 --> 00:10:34.570
through community support.

00:10:34.570 --> 00:10:36.680
Two languages we are
particularly excited about,

00:10:36.680 --> 00:10:38.670
which we launched
earlier this year--

00:10:38.670 --> 00:10:40.500
one is support for Swift.

00:10:40.500 --> 00:10:42.710
So Swift gives some
unique advantages

00:10:42.710 --> 00:10:47.660
by combining the benefits of
a very intuitive, imperative

00:10:47.660 --> 00:10:50.790
programming style with the
benefits of a compiled language

00:10:50.790 --> 00:10:53.210
so you get all the
performance optimizations

00:10:53.210 --> 00:10:55.220
that graphs typically
bring you, and so you

00:10:55.220 --> 00:10:57.560
can have best of both worlds.

00:10:57.560 --> 00:10:59.870
Another language that's
extremely exciting

00:10:59.870 --> 00:11:02.270
is bringing machine
learning to JavaScript.

00:11:02.270 --> 00:11:05.240
There's a huge JavaScript
and web developer community

00:11:05.240 --> 00:11:09.230
out there, and we believe that
by using TensorFlow.js, which

00:11:09.230 --> 00:11:11.510
is the JavaScript
version of TensorFlow,

00:11:11.510 --> 00:11:14.390
it lets JavaScript
developers easily

00:11:14.390 --> 00:11:16.730
jump into machine learning
and develop models

00:11:16.730 --> 00:11:18.740
in the browser using
JavaScript, or run it

00:11:18.740 --> 00:11:21.320
with a node server backend.

00:11:21.320 --> 00:11:24.200
So we're beginning to see
some really cool applications

00:11:24.200 --> 00:11:27.600
of this, and I'll show you
two examples of that here.

00:11:27.600 --> 00:11:30.820
So this is a tool which you can
try out yourself on that site

00:11:30.820 --> 00:11:31.320
up there.

00:11:31.320 --> 00:11:33.620
It's called
TensorFlow Playground,

00:11:33.620 --> 00:11:36.500
and this lets you
interact with a machine

00:11:36.500 --> 00:11:40.070
learning model with a neural
network very interactively

00:11:40.070 --> 00:11:41.190
in the browser.

00:11:41.190 --> 00:11:43.430
So you can define a simple
neural network model.

00:11:43.430 --> 00:11:45.110
You can control how
many layers it has,

00:11:45.110 --> 00:11:46.850
how many nodes it
has, and then you

00:11:46.850 --> 00:11:49.250
can see the effect of
changing parameters

00:11:49.250 --> 00:11:52.340
like learning rates,
et cetera, and look

00:11:52.340 --> 00:11:55.940
at how individual
neurons are behaving

00:11:55.940 --> 00:11:59.300
and what's happening to
your model convergence.

00:11:59.300 --> 00:12:01.730
And these are the types of
interactive visualizations

00:12:01.730 --> 00:12:03.980
that are really
possible when you

00:12:03.980 --> 00:12:08.360
have a browser as a platform
for machine learning.

00:12:08.360 --> 00:12:09.830
On the TensorFlow.js
website, you

00:12:09.830 --> 00:12:11.870
can see a lot more
really fun examples.

00:12:11.870 --> 00:12:14.420
My favorite one is the one
which lets you drive a "Pac-Man"

00:12:14.420 --> 00:12:17.690
game by using hand
gestures or gestures.

00:12:17.690 --> 00:12:21.410
This one is an example of the
PoseNet model, where a machine

00:12:21.410 --> 00:12:23.690
learning model can
be fed to webcam data

00:12:23.690 --> 00:12:26.090
to identify human pose,
and then this data

00:12:26.090 --> 00:12:29.010
can be used to drive
or control actions.

00:12:29.010 --> 00:12:32.740
So again, all the source code
for this model is available,

00:12:32.740 --> 00:12:34.490
and the model is also
available in trained

00:12:34.490 --> 00:12:37.590
from that you can use.

00:12:37.590 --> 00:12:39.470
So just finally, I want
to talk a little bit

00:12:39.470 --> 00:12:42.320
about some of the tooling
that makes TensorFlow

00:12:42.320 --> 00:12:48.170
very flexible and easy to use
for a variety of applications.

00:12:48.170 --> 00:12:51.620
One example of such a
tool is TensorBoard.

00:12:51.620 --> 00:12:54.230
So TensorBoard is the
visualization suite

00:12:54.230 --> 00:12:57.470
that comes with TensorFlow,
and by using TensorBoard

00:12:57.470 --> 00:13:00.560
you can dive into the
complexities of your model,

00:13:00.560 --> 00:13:02.690
and you can look at the
architecture of the model.

00:13:02.690 --> 00:13:04.850
You can see how the
graph is laid out.

00:13:04.850 --> 00:13:06.657
You can break down your data.

00:13:06.657 --> 00:13:08.990
You can see how the data is
flowing through the network,

00:13:08.990 --> 00:13:11.990
look at some convergence
metrics and accuracy and so on.

00:13:11.990 --> 00:13:14.870
It's a very powerful way of
working with your machine

00:13:14.870 --> 00:13:15.770
learning models.

00:13:15.770 --> 00:13:17.990
What you're seeing
here is a visualization

00:13:17.990 --> 00:13:21.290
of the MNIST data set, which
is the handwritten digit

00:13:21.290 --> 00:13:22.860
recognition data set.

00:13:22.860 --> 00:13:24.830
And you can see how
your data set clusters,

00:13:24.830 --> 00:13:26.496
and whether you have
a good distribution

00:13:26.496 --> 00:13:28.760
of different types of
labels in your data,

00:13:28.760 --> 00:13:32.280
and how they are separated.

00:13:32.280 --> 00:13:35.810
So we talked a lot about machine
learning model development.

00:13:35.810 --> 00:13:37.700
It turns out that in
order to put machine

00:13:37.700 --> 00:13:40.370
learning into practice and
really using it in a production

00:13:40.370 --> 00:13:44.100
setting, it takes a lot more
than just the model itself.

00:13:44.100 --> 00:13:46.160
So you have to deal with
a lot of things related

00:13:46.160 --> 00:13:48.560
to bringing the data
in, and then dealing

00:13:48.560 --> 00:13:50.420
with model serving,
model management,

00:13:50.420 --> 00:13:52.550
and evaluating the
performance of models

00:13:52.550 --> 00:13:55.940
and keeping them
refreshed over time,

00:13:55.940 --> 00:13:58.380
and the lifecycle
management of models.

00:13:58.380 --> 00:14:01.280
So for this, we
recently open sourced

00:14:01.280 --> 00:14:04.940
an extension to TensorFlow
called TensorFlow Extended,

00:14:04.940 --> 00:14:07.700
which really deals with this
process of end-to-end machine

00:14:07.700 --> 00:14:10.160
learning and gives
you a lot of tools

00:14:10.160 --> 00:14:13.490
that you can use to deal with
these system-level aspects

00:14:13.490 --> 00:14:15.110
of end-to-end machine learning.

00:14:15.110 --> 00:14:17.960
And my colleague [? Clemens ?]
has a talk later this afternoon

00:14:17.960 --> 00:14:20.040
where he dives into more
details about what all

00:14:20.040 --> 00:14:22.470
you can do with this platform.

00:14:22.470 --> 00:14:24.860
So with this brief
introduction to TensorFlow,

00:14:24.860 --> 00:14:26.360
let me turn it back
to Laurence, who

00:14:26.360 --> 00:14:30.450
will show how we are making it
easier to use and get started.

00:14:30.450 --> 00:14:32.618
LAURENCE MORONEY:
Thanks, Sandeep.

00:14:32.618 --> 00:14:33.720
My mic still working?

00:14:33.720 --> 00:14:34.670
There we go.

00:14:34.670 --> 00:14:37.864
So Sandeep showed TensorFlow
and some of the things

00:14:37.864 --> 00:14:39.530
that we've been able
to do in TensorFlow

00:14:39.530 --> 00:14:41.520
and what it's all about.

00:14:41.520 --> 00:14:44.290
One of the themes that we've
had for this year in TensorFlow

00:14:44.290 --> 00:14:47.075
is to try and make it
easier for developers.

00:14:47.075 --> 00:14:48.950
So what I want to share
is four of the things

00:14:48.950 --> 00:14:50.720
that we've actually worked on.

00:14:50.720 --> 00:14:53.420
The first of these is Keras.

00:14:53.420 --> 00:14:55.820
Sandeep mentioned, if
you're familiar with Keras,

00:14:55.820 --> 00:14:59.090
it was an open source
community framework

00:14:59.090 --> 00:15:02.730
that it made it easy for you to
design and train neural nets.

00:15:02.730 --> 00:15:06.040
Francois, the inventor of
Keras, now actually works for us

00:15:06.040 --> 00:15:08.120
at Google, and as
a result, we've

00:15:08.120 --> 00:15:10.701
incorporated Keras
us into TensorFlow.

00:15:10.701 --> 00:15:12.450
And to be able to use
Keras in TensorFlow,

00:15:12.450 --> 00:15:16.149
it's as easy as from
tf import keras.

00:15:16.149 --> 00:15:17.940
The second thing that
we've been working on

00:15:17.940 --> 00:15:20.290
is something called
Eager Execution.

00:15:20.290 --> 00:15:24.420
And so I came to TensorFlow as
a coder, not as an AI scientist

00:15:24.420 --> 00:15:27.840
or as a data scientist,
and the first thing

00:15:27.840 --> 00:15:30.852
that I found that it was very
difficult for me to understand

00:15:30.852 --> 00:15:33.060
was how a lot of the things
that you do in TensorFlow

00:15:33.060 --> 00:15:34.905
was that you write all
your code up front,

00:15:34.905 --> 00:15:37.420
you load it into a graph,
and then you execute it.

00:15:37.420 --> 00:15:39.687
And it was hard for me, as
a developer, to understand,

00:15:39.687 --> 00:15:42.270
because I don't know about you,
I'm not a very good developer.

00:15:42.270 --> 00:15:43.860
I need to write two or
three lines of code,

00:15:43.860 --> 00:15:45.510
step through them,
make sure they work.

00:15:45.510 --> 00:15:47.490
Write another two or three lines
of code, step through them,

00:15:47.490 --> 00:15:48.532
make sure that they work.

00:15:48.532 --> 00:15:50.114
And one of the things
that we've added

00:15:50.114 --> 00:15:52.110
is what we've called
Eager Execution to make

00:15:52.110 --> 00:15:52.785
that possible.

00:15:52.785 --> 00:15:54.660
So instead of loading
everything into a graph

00:15:54.660 --> 00:15:56.340
and then executing
on a graph, you

00:15:56.340 --> 00:15:58.830
can actually do
step-by-step execution now

00:15:58.830 --> 00:16:01.290
with Eager in your
TensorFlow application.

00:16:01.290 --> 00:16:03.569
So you can debug
your data pipelines.

00:16:03.569 --> 00:16:05.860
You can debug your training
and all that kind of stuff.

00:16:05.860 --> 00:16:06.359
I love it.

00:16:06.359 --> 00:16:07.894
It makes it a lot easier.

00:16:07.894 --> 00:16:09.810
The third one is something
that we've released

00:16:09.810 --> 00:16:11.670
called TensorFlow Hub.

00:16:11.670 --> 00:16:13.560
And the idea behind
TensorFlow Hub

00:16:13.560 --> 00:16:17.340
is that it's a repository
of pre-trained models

00:16:17.340 --> 00:16:19.590
that you can then just
incorporate directly

00:16:19.590 --> 00:16:22.220
in your app with one
or two lines of code.

00:16:22.220 --> 00:16:24.880
And not only can you
incorporate the entire model,

00:16:24.880 --> 00:16:26.517
but through transfer
of learning,

00:16:26.517 --> 00:16:28.350
you can actually just
pull individual layers

00:16:28.350 --> 00:16:31.890
or individual attributes of a
model and use them in your code

00:16:31.890 --> 00:16:32.610
as well.

00:16:32.610 --> 00:16:34.985
The idea behind that is really
to get you up-and-running,

00:16:34.985 --> 00:16:37.470
get you started quickly in
common scenarios such as image

00:16:37.470 --> 00:16:39.150
classification,
text classification,

00:16:39.150 --> 00:16:40.840
that type of thing.

00:16:40.840 --> 00:16:42.990
And then the final one is--

00:16:42.990 --> 00:16:45.180
really it's been designed
to help researchers

00:16:45.180 --> 00:16:47.450
and to make it a lot
easier for researchers,

00:16:47.450 --> 00:16:49.470
and it's called Tensor2Tensor.

00:16:49.470 --> 00:16:51.750
And the idea, again,
behind Tensor2Tensor

00:16:51.750 --> 00:16:54.780
is to give you
pre-trained models

00:16:54.780 --> 00:16:58.230
and pre-trained scenarios that
you can use, you can reuse,

00:16:58.230 --> 00:17:00.300
and you can start
adapting really quickly.

00:17:00.300 --> 00:17:02.216
But let me start looking
at a couple of these.

00:17:02.216 --> 00:17:03.677
So first of all is Keras.

00:17:03.677 --> 00:17:06.260
Now, the idea is Keras-- it's a
high-level neural network API.

00:17:06.260 --> 00:17:07.973
It was built in Python.

00:17:07.973 --> 00:17:09.556
It's designed to
make it easy for you,

00:17:09.556 --> 00:17:11.800
as I've mentioned, to
build a neural network.

00:17:11.800 --> 00:17:13.260
And one of the
things that it does

00:17:13.260 --> 00:17:15.240
is it includes the
ability to access

00:17:15.240 --> 00:17:17.520
various different namespaces--

00:17:17.520 --> 00:17:20.609
sorry, it allows you to
access various different--

00:17:20.609 --> 00:17:22.650
sorry, my slides
animation are broken--

00:17:22.650 --> 00:17:24.119
access different data sets.

00:17:24.119 --> 00:17:26.650
So for example, one data
set that's built into it

00:17:26.650 --> 00:17:28.089
is called Fashion-MNIST.

00:17:28.089 --> 00:17:31.710
The graph that Sandeep showed
earlier for handwriting

00:17:31.710 --> 00:17:32.625
recognition is MNIST.

00:17:32.625 --> 00:17:34.850
This similar one,
called Fashion-MNIST,

00:17:34.850 --> 00:17:37.680
allows you to do
a classification

00:17:37.680 --> 00:17:39.145
on items of clothing.

00:17:39.145 --> 00:17:40.770
And in the next
session after this one,

00:17:40.770 --> 00:17:42.580
I'm actually going to show
all the code for that--

00:17:42.580 --> 00:17:44.550
that you can build a
classifier are using

00:17:44.550 --> 00:17:47.460
Keras or TensorFlow to
classify items of clothing

00:17:47.460 --> 00:17:49.900
in about 12 lines of code.

00:17:49.900 --> 00:17:52.440
So I'm going to show that a
little bit later on if you

00:17:52.440 --> 00:17:54.090
want to stick around for that.

00:17:54.090 --> 00:17:57.220
But very, very easy for you
to use, as you can see here.

00:17:57.220 --> 00:17:58.050
This is in Keras.

00:17:58.050 --> 00:18:00.480
If I wanted to use something
like Fashion-MNIST,

00:18:00.480 --> 00:18:02.160
I could just incorporate
the data set.

00:18:02.160 --> 00:18:03.868
And if I want to build
a neural network--

00:18:03.868 --> 00:18:06.930
this neural network is actually
what will classify the clothing

00:18:06.930 --> 00:18:09.690
that I mentioned earlier on,
so these three lines of code--

00:18:09.690 --> 00:18:10.950
these three layers--

00:18:10.950 --> 00:18:13.699
are what you'll use to
define a neural network that

00:18:13.699 --> 00:18:15.240
can be fed an item
of clothing and it

00:18:15.240 --> 00:18:16.860
will tell you what it is.

00:18:16.860 --> 00:18:19.430
So Keras-- designed to
make all of that simple.

00:18:19.430 --> 00:18:21.540
Eager Execution--
very similar thing.

00:18:21.540 --> 00:18:24.540
The idea is it's an imperative
programming environment.

00:18:24.540 --> 00:18:26.590
It evaluates
operations immediately.

00:18:26.590 --> 00:18:29.020
You don't build graphs,
and your operations

00:18:29.020 --> 00:18:30.540
will return concrete
values so you

00:18:30.540 --> 00:18:32.370
can debug them straight away.

00:18:32.370 --> 00:18:34.960
And to turn it on in TensorFlow,
all you have to do is say

00:18:34.960 --> 00:18:37.380
tf.enable_eager_execution.

00:18:37.380 --> 00:18:40.380
And if you use, for example,
an IDE like PyCharm,

00:18:40.380 --> 00:18:42.697
that then enables you to
do step-by-step debugging

00:18:42.697 --> 00:18:45.030
through the Python that you're
using to build your model

00:18:45.030 --> 00:18:47.390
and test your model.

00:18:47.390 --> 00:18:50.434
So TensorFlow Hub--
the idea is it's

00:18:50.434 --> 00:18:52.100
a library of model
components that we've

00:18:52.100 --> 00:18:54.080
designed to make it easy
for you to integrate

00:18:54.080 --> 00:18:56.210
these pre-trained models
into your application,

00:18:56.210 --> 00:19:00.680
or even pull individual layers
through transfer of learning

00:19:00.680 --> 00:19:03.110
into your application, and
it's just a single line of code

00:19:03.110 --> 00:19:04.070
to integrate them.

00:19:04.070 --> 00:19:07.160
And here's a sample piece
of code here for it,

00:19:07.160 --> 00:19:09.740
where the idea is that I'm now--

00:19:09.740 --> 00:19:11.630
the first three
lines there is I'm

00:19:11.630 --> 00:19:16.180
calling on hub.Module to
bring in this nasnet model,

00:19:16.180 --> 00:19:17.960
and then those bottom
three lines of code

00:19:17.960 --> 00:19:19.760
is I could just pull
things out of that.

00:19:19.760 --> 00:19:22.580
So for example, features
= module (my_images)

00:19:22.580 --> 00:19:25.320
is going to give me the set
of features in that model.

00:19:25.320 --> 00:19:27.260
So I can inspect them,
I can adapt them,

00:19:27.260 --> 00:19:29.540
and I can maybe build my
own models using them.

00:19:29.540 --> 00:19:31.950
And other things like the
logics and the probabilities,

00:19:31.950 --> 00:19:34.800
I can actually pull out of
that just using method calls.

00:19:34.800 --> 00:19:37.700
So if you've ever done some
simple TensorFlow programming,

00:19:37.700 --> 00:19:39.740
you might have seen--
for example, on GitHub,

00:19:39.740 --> 00:19:41.240
there were models
such as ImageNet

00:19:41.240 --> 00:19:43.100
that you could download
and start using.

00:19:43.100 --> 00:19:44.930
This brings that
to the next level.

00:19:44.930 --> 00:19:46.440
Instead of you
downloading a model,

00:19:46.440 --> 00:19:48.949
putting it somewhere,
writing code to read that in,

00:19:48.949 --> 00:19:51.240
the idea is that there's an
API for you to pull it out,

00:19:51.240 --> 00:19:53.960
and also to inspect the
properties of that model

00:19:53.960 --> 00:19:56.110
and use them yourself.

00:19:56.110 --> 00:19:58.270
And then finally,
Tensor2Tensor-- the idea

00:19:58.270 --> 00:20:01.110
is that this is similar in that
it's a library of deep learning

00:20:01.110 --> 00:20:03.074
models and data
sets we've designed

00:20:03.074 --> 00:20:04.740
that's really for
deep learning research

00:20:04.740 --> 00:20:06.280
to make it more accessible.

00:20:06.280 --> 00:20:09.530
So for example, if the latest
adaptive learning rate method,

00:20:09.530 --> 00:20:11.310
I think is what
we're showing here,

00:20:11.310 --> 00:20:13.170
then the idea is I
can actually pull that

00:20:13.170 --> 00:20:16.630
in using Tensor2Tensor,
and in this case,

00:20:16.630 --> 00:20:19.030
the code is actually--
this is for translation.

00:20:19.030 --> 00:20:22.080
So I can pull in the latest
research on translation.

00:20:22.080 --> 00:20:24.180
I believe this one
English to German.

00:20:24.180 --> 00:20:27.540
And I can use that
in my own models,

00:20:27.540 --> 00:20:29.890
or I could build my own
models wrapping them.

00:20:29.890 --> 00:20:33.404
So again, really, the whole
idea is to take existing models

00:20:33.404 --> 00:20:35.820
and to make building your own
machine learned applications

00:20:35.820 --> 00:20:37.510
using those models
a lot simpler.

00:20:37.510 --> 00:20:39.690
So those are four
of the advances

00:20:39.690 --> 00:20:41.804
that we've been working
on in this year.

00:20:41.804 --> 00:20:43.470
The whole idea, like
we said-- the theme

00:20:43.470 --> 00:20:45.080
is to make machine
learning easier.

00:20:45.080 --> 00:20:47.370
First is Keras for
building neural networks.

00:20:47.370 --> 00:20:48.670
Make it very simple.

00:20:48.670 --> 00:20:50.550
The second is Eager
Execution to make life

00:20:50.550 --> 00:20:53.970
easier for programmers so
you've got imperative execution.

00:20:53.970 --> 00:20:57.231
The third, then, is pre-trained
finished models and providing

00:20:57.231 --> 00:20:58.980
a repository for them
so that you can just

00:20:58.980 --> 00:21:00.930
pull them out using
a single line of code

00:21:00.930 --> 00:21:02.550
and not go through
all that friction.

00:21:02.550 --> 00:21:04.440
And then finally, and
more cutting-edge stuff

00:21:04.440 --> 00:21:07.110
is Tensor2Tensor, where
some of the latest research

00:21:07.110 --> 00:21:10.050
is available, and code that's
been written by our researchers

00:21:10.050 --> 00:21:12.810
is available for you to be able
to incorporate their models

00:21:12.810 --> 00:21:15.120
and their work into your own.

00:21:15.120 --> 00:21:18.490
So with that, the most important
thing for us, of course,

00:21:18.490 --> 00:21:21.090
is community and to be
able to extend TensorFlow

00:21:21.090 --> 00:21:22.869
through a community,
as Sandeep shared.

00:21:22.869 --> 00:21:24.660
And Edd, who's our
specialist in community,

00:21:24.660 --> 00:21:25.810
will share that with you.

00:21:25.810 --> 00:21:27.116
Thank you.

00:21:27.116 --> 00:21:30.610
[APPLAUSE]

00:21:30.610 --> 00:21:32.940
EDD WILDER-JAMES
Thanks, Laurence.

00:21:32.940 --> 00:21:33.570
Hey, everybody.

00:21:33.570 --> 00:21:36.420
My name's Edd Wilder-James,
and as Laurence said,

00:21:36.420 --> 00:21:38.850
I work on growing open
source collaboration

00:21:38.850 --> 00:21:41.170
around TensorFlow.

00:21:41.170 --> 00:21:42.640
The most important
thing is that we

00:21:42.640 --> 00:21:45.580
want machine learning to be
in as many hands as possible.

00:21:45.580 --> 00:21:47.884
So firstly, thank
you for being here.

00:21:47.884 --> 00:21:50.050
And there's one reason that
we've convened these two

00:21:50.050 --> 00:21:52.990
days in this conference
where you can come and learn

00:21:52.990 --> 00:21:56.910
about TensorFlow not from
people five-step-removed

00:21:56.910 --> 00:21:59.410
from the project, but from the
actual developers and product

00:21:59.410 --> 00:22:01.506
managers who are working
on all the features.

00:22:01.506 --> 00:22:03.130
So please stick with
us for this track,

00:22:03.130 --> 00:22:07.470
because you'll be getting it
absolutely from the source.

00:22:07.470 --> 00:22:11.640
Talking about source--
so TensorFlow does

00:22:11.640 --> 00:22:13.450
have a massive
worldwide community,

00:22:13.450 --> 00:22:15.820
and we'd love you to
be involved in it.

00:22:15.820 --> 00:22:19.890
There's two places you
really need to know about.

00:22:19.890 --> 00:22:23.000
The first of these is
tensorflow.org/community.

00:22:23.000 --> 00:22:26.250
Anything we talk about
today, where it's just

00:22:26.250 --> 00:22:29.910
a resource or a user
group or a mailing list,

00:22:29.910 --> 00:22:32.140
you can find from there.

00:22:32.140 --> 00:22:34.650
So if you're a
user of TensorFlow

00:22:34.650 --> 00:22:37.740
and haven't joined up, maybe,
to the discuss mailing list,

00:22:37.740 --> 00:22:40.350
or you don't use Dtack
Overflow to get your answers,

00:22:40.350 --> 00:22:43.080
or you don't use GitHub
Issues to file when you've

00:22:43.080 --> 00:22:45.480
got a really gnarly problem,
please head over and get

00:22:45.480 --> 00:22:48.000
familiar with those resources.

00:22:48.000 --> 00:22:50.550
One of the other unique
things about these resources

00:22:50.550 --> 00:22:53.740
is who's on the end
of the issue, who's

00:22:53.740 --> 00:22:55.700
on the end of the Stack
Overflow question.

00:22:55.700 --> 00:22:57.710
It's actually a member
of the TensorFlow team

00:22:57.710 --> 00:22:59.800
at Google Brian, a
lot of the time, who's

00:22:59.800 --> 00:23:01.270
going to answer your question.

00:23:01.270 --> 00:23:05.920
We really believe that we should
be connected directly to users

00:23:05.920 --> 00:23:08.360
and be learning from use
cases and helping out,

00:23:08.360 --> 00:23:10.450
so we actually have
engineers who basically

00:23:10.450 --> 00:23:13.540
do a rotation taking turns to
answer Stack Overflow questions

00:23:13.540 --> 00:23:14.507
and address issues.

00:23:14.507 --> 00:23:16.090
It's really getting
right to the heart

00:23:16.090 --> 00:23:20.470
of the team when you file this.

00:23:20.470 --> 00:23:22.600
And one of the unique
characteristics

00:23:22.600 --> 00:23:24.520
about TensorFlow as
an open source project

00:23:24.520 --> 00:23:28.330
is you are using the exact same
code we use inside Google when

00:23:28.330 --> 00:23:29.500
you're using TensorFlow.

00:23:29.500 --> 00:23:30.880
There's no two versions.

00:23:30.880 --> 00:23:32.270
It's the same code.

00:23:32.270 --> 00:23:34.220
So you're reaching
the same folks,

00:23:34.220 --> 00:23:37.030
and your issues have
a similar eye on them

00:23:37.030 --> 00:23:40.730
as any other person
inside Google's world.

00:23:40.730 --> 00:23:42.230
The other thing we
really want to do

00:23:42.230 --> 00:23:44.550
is encourage and grow
more contributors.

00:23:44.550 --> 00:23:46.430
So if you're a
developer and want

00:23:46.430 --> 00:23:48.580
to become involved
contributing to TensorFlow,

00:23:48.580 --> 00:23:50.410
there's a developer's
mailing list, again,

00:23:50.410 --> 00:23:51.822
you can find up there.

00:23:51.822 --> 00:23:53.780
And on that mailing list,
we do a lot of things

00:23:53.780 --> 00:23:55.550
like coordinating releases.

00:23:55.550 --> 00:23:58.310
We also, now, starting in
the middle of this year,

00:23:58.310 --> 00:24:00.590
publish requests for
comments around new designs.

00:24:00.590 --> 00:24:02.090
As you'll probably
hear later, we're

00:24:02.090 --> 00:24:05.990
in the middle of figuring out
what TensorFlow 2.0 is going

00:24:05.990 --> 00:24:08.510
to look like, and a
big part of that for us

00:24:08.510 --> 00:24:10.280
is public consultation
on design.

00:24:10.280 --> 00:24:12.380
So if you're a heavy
user or developer,

00:24:12.380 --> 00:24:15.330
please play a part
in that review.

00:24:15.330 --> 00:24:17.680
You can join that, again,
through the community page.

00:24:19.762 --> 00:24:21.970
One other great resource--
this is on the other side,

00:24:21.970 --> 00:24:23.770
if you're new-- is Colabs.

00:24:23.770 --> 00:24:27.410
Has anyone here
used a Colab at all?

00:24:27.410 --> 00:24:28.784
OK, a few people.

00:24:28.784 --> 00:24:30.200
So this is a really
exciting thing

00:24:30.200 --> 00:24:32.150
that we brought
online this year.

00:24:32.150 --> 00:24:36.360
Basically, you can use
compute resource free, on us,

00:24:36.360 --> 00:24:40.260
in collabs where TensorFlow is
all set up and ready to use.

00:24:40.260 --> 00:24:42.260
And one of the easiest
ways of getting into this

00:24:42.260 --> 00:24:44.450
is if you're on the
tensorflow.org website

00:24:44.450 --> 00:24:49.010
and you see code examples, you
see the Open in Colab button.

00:24:49.010 --> 00:24:55.310
And you hit that, boom, you're
in a Jupyter notebook running

00:24:55.310 --> 00:24:57.920
TensorFlow, and you get access--

00:24:57.920 --> 00:25:01.410
for a limited time, obviously--
to GPUs that we provide.

00:25:01.410 --> 00:25:04.280
So it really is an amazing
way that you don't have

00:25:04.280 --> 00:25:05.760
to worry about the resource.

00:25:05.760 --> 00:25:07.760
You don't have to
worry about installing.

00:25:07.760 --> 00:25:10.610
You just don't even have to
worry about writing the code.

00:25:10.610 --> 00:25:13.430
You can actually take an example
and then start tweaking it

00:25:13.430 --> 00:25:17.060
straight online, and it's
one of the best learning

00:25:17.060 --> 00:25:19.400
tools for playing
around with TensorFlow.

00:25:19.400 --> 00:25:21.810
And the other fun
attributes of this

00:25:21.810 --> 00:25:24.440
are you can save your work
to your Google Drive account

00:25:24.440 --> 00:25:26.510
so it's not lost,
and you can also

00:25:26.510 --> 00:25:29.880
start up a colab from
any GitHub repository.

00:25:29.880 --> 00:25:32.790
So it's a pretty powerful tool.

00:25:32.790 --> 00:25:35.690
OK, a few other things that
are good places to hang out.

00:25:35.690 --> 00:25:37.310
We have a YouTube channel.

00:25:37.310 --> 00:25:38.810
And if you enjoyed
Laurence talking,

00:25:38.810 --> 00:25:41.650
you can just literally
have hours of Laurence

00:25:41.650 --> 00:25:44.594
talking and talking
about TensorFlow.

00:25:44.594 --> 00:25:46.760
But one of the fun things
he does there, as well, is

00:25:46.760 --> 00:25:49.790
gets out into the user
base and to the developers

00:25:49.790 --> 00:25:54.530
and interviews people who
are using TensorFlow as well.

00:25:54.530 --> 00:25:58.070
So if you have a useful
and interesting application

00:25:58.070 --> 00:25:59.770
in TensorFlow, come and find us.

00:25:59.770 --> 00:26:00.970
We'd love to talk to you.

00:26:00.970 --> 00:26:03.250
We'd love to profile you,
because these things aren't

00:26:03.250 --> 00:26:04.720
just for us to broadcast out.

00:26:04.720 --> 00:26:07.570
We want to celebrate
the community.

00:26:07.570 --> 00:26:09.400
Likewise, we have
a blog, which you

00:26:09.400 --> 00:26:14.920
can find on Medium or
blog.tensorflow.org.

00:26:14.920 --> 00:26:17.440
And as well as
publishing, I think,

00:26:17.440 --> 00:26:20.380
what is a really great
resource for technical articles

00:26:20.380 --> 00:26:22.570
and important information
about TensorFlow there,

00:26:22.570 --> 00:26:25.570
we're, again, incorporating
content from the broader

00:26:25.570 --> 00:26:26.550
developer community.

00:26:26.550 --> 00:26:28.300
So if you're interested
in guest blogging,

00:26:28.300 --> 00:26:29.840
again, come talk to us.

00:26:29.840 --> 00:26:32.650
We'd love to have a lot of
content that reflects everybody

00:26:32.650 --> 00:26:33.690
using the code.

00:26:35.884 --> 00:26:37.550
And of course, there's
this little thing

00:26:37.550 --> 00:26:40.790
called Twitter, which is our
main newswire for everything

00:26:40.790 --> 00:26:44.510
that tickers over-- so releases,
important articles, and things

00:26:44.510 --> 00:26:45.990
like that.

00:26:45.990 --> 00:26:48.160
We'd love you to hook
into all those things.

00:26:48.160 --> 00:26:50.920
We'd love you to talk us over
the course of these two days.

00:26:50.920 --> 00:26:55.420
We also have a presence at the
booth over in the expo area--

00:26:55.420 --> 00:26:56.662
the [INAUDIBLE]--

00:26:56.662 --> 00:26:58.120
and we're hanging
around, made sure

00:26:58.120 --> 00:26:59.661
that we got a good
rotation of people

00:26:59.661 --> 00:27:01.160
to come that you could talk to.

00:27:01.160 --> 00:27:03.200
So even if we don't know the
right answer to your question,

00:27:03.200 --> 00:27:05.260
we'll probably be able to say,
well, so-and-so is over there.

00:27:05.260 --> 00:27:06.370
Come and talk to him.

00:27:06.370 --> 00:27:09.670
This is your best chance to
get face-to-face information

00:27:09.670 --> 00:27:11.270
from us, basically.

00:27:11.270 --> 00:27:13.700
So thank you very much
for your attention so far.

00:27:13.700 --> 00:27:16.390
I know Laurence is going to
speak shortly and guide us

00:27:16.390 --> 00:27:17.740
into the two days.

00:27:17.740 --> 00:27:19.150
Very grateful for
you being here.

00:27:19.150 --> 00:27:19.649
Thank you.

00:27:19.649 --> 00:27:22.798
[APPLAUSE]

