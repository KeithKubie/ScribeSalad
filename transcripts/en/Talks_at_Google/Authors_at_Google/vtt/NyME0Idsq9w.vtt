WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.802
[MUSIC PLAYING]

00:00:06.540 --> 00:00:10.060
SPEAKER: It is my very honor to
welcome you, Tali, to Mountain

00:00:10.060 --> 00:00:12.090
View here, to Google.

00:00:12.090 --> 00:00:15.450
And it is my very
honor and pleasure

00:00:15.450 --> 00:00:20.250
to introduce Tali to you guys
here in the room, a world

00:00:20.250 --> 00:00:22.650
renowned researcher
and scientist

00:00:22.650 --> 00:00:26.780
in the field of
cognitive neuroscience.

00:00:26.780 --> 00:00:30.210
Tali investigates how
motivation and emotion determine

00:00:30.210 --> 00:00:34.630
our expectations of the
future, our everyday decisions,

00:00:34.630 --> 00:00:37.807
our memories, and
our ability to learn.

00:00:37.807 --> 00:00:40.140
She's talking about her brand
new book, "The Influential

00:00:40.140 --> 00:00:40.950
Mind."

00:00:40.950 --> 00:00:47.690
And Tali shows us how to avoid
pitfalls in our daily struggle

00:00:47.690 --> 00:00:49.670
to affect others.

00:00:49.670 --> 00:00:52.520
Ladies and gentlemen, it is my
very honor and my very pleasure

00:00:52.520 --> 00:00:56.374
to pass on the microphone
to Dr. Tali Sharot.

00:00:56.374 --> 00:00:58.744
[CLAPPING]

00:01:03.020 --> 00:01:06.380
TALI SHAROT: Thank you so much,
[? Minu, ?] for inviting me.

00:01:06.380 --> 00:01:09.410
[? Minu ?] has been amazing
at organizing this, and also

00:01:09.410 --> 00:01:12.040
at organizing a previous
talk and a conference

00:01:12.040 --> 00:01:14.510
that he organized in Chicago.

00:01:14.510 --> 00:01:16.720
So thank you so much.

00:01:16.720 --> 00:01:19.880
And thank you all for coming
and watching wherever you are.

00:01:19.880 --> 00:01:22.820
So as [? Minu ?] said, I am
a cognitive neuroscientist.

00:01:22.820 --> 00:01:25.310
So that's basically
psychology and neuroscience

00:01:25.310 --> 00:01:26.540
mixed together.

00:01:26.540 --> 00:01:28.670
And I bring people
into my lab, and I

00:01:28.670 --> 00:01:31.280
try to understand the brain
mechanisms that give rise

00:01:31.280 --> 00:01:34.370
to how people act every day,
how they interact, how they

00:01:34.370 --> 00:01:37.190
make decisions, how they think.

00:01:37.190 --> 00:01:39.020
And "The Influential
Mind" is really about

00:01:39.020 --> 00:01:43.940
how people form beliefs, how
belief can be quite stubborn,

00:01:43.940 --> 00:01:47.100
but how change is possible if
we understand the human mind

00:01:47.100 --> 00:01:48.770
and if we understand thinking.

00:01:48.770 --> 00:01:50.840
And a lot of what I'm
going to talk about today

00:01:50.840 --> 00:01:52.920
is how people
process information,

00:01:52.920 --> 00:01:55.160
how they seek out
information, and how

00:01:55.160 --> 00:01:58.040
they process information
to form those beliefs.

00:01:58.040 --> 00:02:01.130
And so that's one
kind of point that

00:02:01.130 --> 00:02:04.820
has a lot to do with what
you guys are doing here.

00:02:04.820 --> 00:02:07.112
But I'm going to start
by exploring thinking.

00:02:07.112 --> 00:02:08.570
And the first thing
I'm going to do

00:02:08.570 --> 00:02:11.510
is I'm going to do an
experiment with you guys here.

00:02:11.510 --> 00:02:14.180
So I'm going to give
you three numbers.

00:02:14.180 --> 00:02:16.880
And I want you to figure
out the rule that I

00:02:16.880 --> 00:02:19.510
used to generate those numbers.

00:02:19.510 --> 00:02:21.260
But for now just keep
it in your mind, OK.

00:02:21.260 --> 00:02:22.700
Don't say anything.

00:02:22.700 --> 00:02:28.110
So my numbers are
two, four, six.

00:02:28.110 --> 00:02:28.806
OK.

00:02:28.806 --> 00:02:30.430
Now, the way that
we're going to do it,

00:02:30.430 --> 00:02:31.930
I'm going to give
you an opportunity

00:02:31.930 --> 00:02:33.430
to give me three numbers.

00:02:33.430 --> 00:02:36.250
And I will tell you if those
three numbers fit my rule

00:02:36.250 --> 00:02:37.690
or do not fit my rule.

00:02:37.690 --> 00:02:39.700
And then I'll let
you guess the rule.

00:02:39.700 --> 00:02:40.510
OK.

00:02:40.510 --> 00:02:41.690
So [? Minu, ?] I'm
going to start with you.

00:02:41.690 --> 00:02:43.450
Give me three numbers,
and I will tell you

00:02:43.450 --> 00:02:44.920
if they fit my rule
or not, and then I'll

00:02:44.920 --> 00:02:45.680
let you guess the rule.

00:02:45.812 --> 00:02:47.063
SPEAKER: I say it's 10 and 12.

00:02:47.063 --> 00:02:47.728
TALI SHAROT: No.

00:02:47.728 --> 00:02:48.863
three numbers you need.

00:02:48.863 --> 00:02:49.821
SPEAKER: Three numbers?

00:02:49.821 --> 00:02:51.345
TALI SHAROT: Yeah.

00:02:51.345 --> 00:02:54.910
So I have a rule which I use
to generate these numbers.

00:02:54.910 --> 00:02:57.100
What you need to do is
figure out what this rule is.

00:02:57.100 --> 00:02:58.516
And the way that
we're going to do

00:02:58.516 --> 00:03:01.270
it is I'll let you test it
by giving me three numbers,

00:03:01.270 --> 00:03:03.700
and I'll tell you it
fits my rule or not.

00:03:03.700 --> 00:03:06.010
So give me any three
numbers that you think.

00:03:06.010 --> 00:03:07.930
And I'll tell you, does
it fit my rule or not?

00:03:07.930 --> 00:03:09.721
And then I'll let
you guess the rule.

00:03:09.721 --> 00:03:10.220
So.

00:03:10.220 --> 00:03:11.482
SPEAKER: 12, 14, and 16?

00:03:11.482 --> 00:03:12.190
TALI SHAROT: Yes.

00:03:12.190 --> 00:03:12.880
If fits my rule.

00:03:12.880 --> 00:03:14.690
Do you want to guess the rule?

00:03:14.690 --> 00:03:19.754
SPEAKER: It is you're
always adding two up.

00:03:19.754 --> 00:03:20.420
TALI SHAROT: OK.

00:03:20.420 --> 00:03:20.920
No.

00:03:20.920 --> 00:03:22.070
That's not the rule.

00:03:22.070 --> 00:03:23.120
So we'll do one more.

00:03:23.120 --> 00:03:24.815
Do you want to give
me three numbers?

00:03:24.815 --> 00:03:25.850
AUDIENCE: 8, 10, 12.

00:03:25.850 --> 00:03:26.600
TALI SHAROT: Yeah.

00:03:26.600 --> 00:03:27.460
It fits my rule.

00:03:27.460 --> 00:03:29.344
Do you want to guess a rule?

00:03:29.344 --> 00:03:31.934
AUDIENCE: It starts with
a two and [INAUDIBLE] two.

00:03:31.934 --> 00:03:32.600
TALI SHAROT: No.

00:03:32.600 --> 00:03:33.100
No.

00:03:33.100 --> 00:03:34.479
That's not my rule.

00:03:34.479 --> 00:03:37.020
Now, I think you know the answer
because you're [INAUDIBLE]..

00:03:37.020 --> 00:03:38.070
So I'll let you do it.

00:03:38.070 --> 00:03:38.725
Go ahead.

00:03:38.725 --> 00:03:40.370
AUDIENCE: Even
numbers is the rule?

00:03:40.370 --> 00:03:40.527
TALI SHAROT: No.

00:03:40.527 --> 00:03:41.800
First of all, you have
to give me three numbers.

00:03:41.800 --> 00:03:43.140
AUDIENCE: Oh, I'm sorry.

00:03:43.140 --> 00:03:44.747
How about two, four--

00:03:44.747 --> 00:03:46.080
can I give you the same numbers?

00:03:46.080 --> 00:03:48.410
TALI SHAROT: Well, they
obviously fit my rule.

00:03:48.410 --> 00:03:48.950
So you can.

00:03:48.950 --> 00:03:51.450
But that--

00:03:51.450 --> 00:03:52.790
AUDIENCE: It's not even numbers?

00:03:52.790 --> 00:03:53.360
TALI SHAROT: No.

00:03:53.360 --> 00:03:53.859
OK.

00:03:53.859 --> 00:03:54.890
One last thing.

00:03:54.890 --> 00:03:57.155
Give me three numbers
and then guess the rule.

00:03:57.155 --> 00:04:00.060
Does anyone want to have a try?

00:04:00.060 --> 00:04:01.921
Go ahead.

00:04:01.921 --> 00:04:02.420
Go ahead.

00:04:02.420 --> 00:04:03.564
Yeah.

00:04:03.564 --> 00:04:06.458
AUDIENCE: I was
thinking like 8, 10, 12.

00:04:06.458 --> 00:04:09.030
And I was thinking you
add a two [INAUDIBLE]..

00:04:09.030 --> 00:04:09.780
TALI SHAROT: Yeah.

00:04:09.780 --> 00:04:10.080
OK.

00:04:10.080 --> 00:04:11.190
So it does [INAUDIBLE].

00:04:11.190 --> 00:04:11.910
OK.

00:04:11.910 --> 00:04:16.779
So my rule was simply
escalating numbers.

00:04:16.779 --> 00:04:19.050
5, 10, 100 would be a yes.

00:04:19.050 --> 00:04:21.050
Five, zero, one would be a no.

00:04:21.050 --> 00:04:21.550
OK.

00:04:21.550 --> 00:04:23.790
What's the point
of this experiment?

00:04:23.790 --> 00:04:25.830
The point of this
experiment is to show you

00:04:25.830 --> 00:04:28.290
that based on a very
limited set of data

00:04:28.290 --> 00:04:32.310
we form strong beliefs and
hypotheses in our head,

00:04:32.310 --> 00:04:35.370
and then automatically
what we try to do

00:04:35.370 --> 00:04:39.360
is we try to confirm those
beliefs by seeking out

00:04:39.360 --> 00:04:41.220
confirming evidence.

00:04:41.220 --> 00:04:44.370
So many people here thought
that my rule was escalating even

00:04:44.370 --> 00:04:45.330
numbers.

00:04:45.330 --> 00:04:47.120
And so what you
automatically do is

00:04:47.120 --> 00:04:50.510
you give me an example of
escalating even numbers.

00:04:50.510 --> 00:04:53.700
So you say 8, 10, 12.

00:04:53.700 --> 00:04:55.830
You're trying to
confirm that belief.

00:04:55.830 --> 00:04:57.810
And what people don't
do is they don't

00:04:57.810 --> 00:05:00.340
try to challenge those beliefs.

00:05:00.340 --> 00:05:02.340
So another way to get
to the truth, if you

00:05:02.340 --> 00:05:04.020
think it's escalating
even numbers,

00:05:04.020 --> 00:05:06.150
would be to say
one, three, five.

00:05:06.150 --> 00:05:08.420
And I would say,
yes, it fits my rule.

00:05:08.420 --> 00:05:10.630
Then you would know that
your belief is wrong,

00:05:10.630 --> 00:05:13.480
and you would get
to the truth faster.

00:05:13.480 --> 00:05:16.900
So that's an example of what
we call the confirmation bias.

00:05:16.900 --> 00:05:18.450
Now, this specific
example was first

00:05:18.450 --> 00:05:21.270
shown by Peter Wason in 1960.

00:05:21.270 --> 00:05:23.850
Peter was an academic in
my own department at UCL.

00:05:23.850 --> 00:05:27.760
He passed away a few years
before I joined the department.

00:05:27.760 --> 00:05:30.180
And the reason this
experiment is important

00:05:30.180 --> 00:05:32.310
is not about numbers.

00:05:32.310 --> 00:05:35.520
The reason it's important
is that we all host

00:05:35.520 --> 00:05:37.290
a range of beliefs in our head.

00:05:37.290 --> 00:05:38.800
We have beliefs about gender.

00:05:38.800 --> 00:05:42.240
We have scientific theories
that we believe in.

00:05:42.240 --> 00:05:44.700
We have beliefs about religion,
and beliefs about ourselves

00:05:44.700 --> 00:05:47.850
and how our company should work,
and about our relationships

00:05:47.850 --> 00:05:50.250
with our family
and our colleagues.

00:05:50.250 --> 00:05:53.070
And what we tend to
do without knowing it

00:05:53.070 --> 00:05:56.340
is that we go about our
day every single day trying

00:05:56.340 --> 00:05:59.240
to find evidence that will
confirm and strengthen

00:05:59.240 --> 00:06:00.510
those beliefs.

00:06:00.510 --> 00:06:04.140
And rarely do we try to
challenge those beliefs.

00:06:04.140 --> 00:06:07.560
But what happens when we do
encounter a piece of evidence

00:06:07.560 --> 00:06:11.770
or an opinion that doesn't
quite fit what we believe in?

00:06:11.770 --> 00:06:16.340
So we tested this question in
the domain of climate change.

00:06:16.340 --> 00:06:20.670
So together with my colleagues
we conducted a study

00:06:20.670 --> 00:06:22.680
where we wanted to see
whether we can change

00:06:22.680 --> 00:06:24.900
people's opinions
about climate change

00:06:24.900 --> 00:06:27.120
by giving them information.

00:06:27.120 --> 00:06:28.920
Now, I'm not only
talking about those

00:06:28.920 --> 00:06:30.930
who do not believe
in climate change,

00:06:30.930 --> 00:06:34.800
but also those that do
believe in climate change.

00:06:34.800 --> 00:06:37.960
Can we change their opinion
as well with information?

00:06:37.960 --> 00:06:40.770
So the first thing we
did is we asked everyone,

00:06:40.770 --> 00:06:43.020
do you believe in
made climate change?

00:06:43.020 --> 00:06:45.330
Do you support the
Paris agreement?

00:06:45.330 --> 00:06:47.310
And based on their
answers we divided them

00:06:47.310 --> 00:06:50.730
into the strong believers
and the weak believers.

00:06:50.730 --> 00:06:52.740
Next we asked them,
please estimate

00:06:52.740 --> 00:06:54.660
by how much do you think
the temperature would

00:06:54.660 --> 00:06:57.420
rise in the next 100 years.

00:06:57.420 --> 00:06:59.760
So, unsurprisingly,
the weak believers

00:06:59.760 --> 00:07:02.460
gave us an estimate
that was smaller

00:07:02.460 --> 00:07:04.350
than the strong believers.

00:07:04.350 --> 00:07:06.420
Then came the real test.

00:07:06.420 --> 00:07:09.030
We told half of all
our participants

00:07:09.030 --> 00:07:12.000
that the scientists have
re-evaluated the data

00:07:12.000 --> 00:07:14.100
and they now concluded
that actually things

00:07:14.100 --> 00:07:15.310
are not that bad.

00:07:15.310 --> 00:07:17.040
It's not as bad as
they thought before.

00:07:17.040 --> 00:07:20.200
And the temperature would rise
by only a tiny little bit.

00:07:20.200 --> 00:07:22.680
We told the other half
of our participants

00:07:22.680 --> 00:07:25.380
that the scientists have
re-evaluated the data,

00:07:25.380 --> 00:07:28.710
and now have concluded the
situation is much, much worse

00:07:28.710 --> 00:07:31.230
than they previously
thought, and the temperature

00:07:31.230 --> 00:07:33.840
would rise by a very
significant amount.

00:07:33.840 --> 00:07:37.350
And please, everyone give
us your new estimate.

00:07:37.350 --> 00:07:39.330
So the question
was whether people

00:07:39.330 --> 00:07:42.060
will change their estimate
based on this information

00:07:42.060 --> 00:07:43.540
that we gave them.

00:07:43.540 --> 00:07:47.340
And the answer was, yes,
as long as the information

00:07:47.340 --> 00:07:49.860
fit their original worldview.

00:07:49.860 --> 00:07:52.810
So when the weak believers heard
that the scientists are saying,

00:07:52.810 --> 00:07:56.070
actually you made a mistake, and
the situation is not that bad

00:07:56.070 --> 00:07:59.550
at all, they moved a
lot in that direction.

00:07:59.550 --> 00:08:00.984
But they didn't
budge at all when

00:08:00.984 --> 00:08:02.400
they heard that
the scientists are

00:08:02.400 --> 00:08:05.940
saying things are actually
more dire than we originally

00:08:05.940 --> 00:08:07.140
thought.

00:08:07.140 --> 00:08:10.330
Now, the strong believers
showed the opposite.

00:08:10.330 --> 00:08:12.610
So when they learned that
the scientists are saying,

00:08:12.610 --> 00:08:14.920
actually things are much
better than we thought,

00:08:14.920 --> 00:08:16.500
the situation is
not that bad, they

00:08:16.500 --> 00:08:18.052
didn't move in that direction.

00:08:18.052 --> 00:08:20.010
But when they learned
the scientists are saying

00:08:20.010 --> 00:08:21.960
things are much worse
than we thought before,

00:08:21.960 --> 00:08:25.180
they moved a lot
in that direction.

00:08:25.180 --> 00:08:26.640
So this, again,
is another example

00:08:26.640 --> 00:08:28.500
of the confirmation
bias, which is basically

00:08:28.500 --> 00:08:30.480
when you give
people data they are

00:08:30.480 --> 00:08:34.500
quick to take in the data that
fits their original worldview

00:08:34.500 --> 00:08:38.280
but assess counter evidence
with a critical eye.

00:08:38.280 --> 00:08:41.409
Now, this phenomena
is not new at all.

00:08:41.409 --> 00:08:42.570
It's an old phenomena.

00:08:42.570 --> 00:08:46.530
But now when information
is so readily accessible

00:08:46.530 --> 00:08:48.750
it's having more of an
effect on our beliefs

00:08:48.750 --> 00:08:51.900
because we can find
data, and information,

00:08:51.900 --> 00:08:54.720
and evidence on the
web to support anything

00:08:54.720 --> 00:08:56.160
that we want to believe in.

00:08:56.160 --> 00:09:00.540
And so what's happening is that
groups are moving to extremes.

00:09:00.540 --> 00:09:03.860
And so information can
actually cause polarization

00:09:03.860 --> 00:09:08.450
instead of bringing people
together because of this bias.

00:09:08.450 --> 00:09:12.740
So we wanted to know what goes
on inside the human brain when

00:09:12.740 --> 00:09:16.670
people encounter opinions
or a piece of data that

00:09:16.670 --> 00:09:19.320
doesn't really fit their view.

00:09:19.320 --> 00:09:22.580
So the way that we did that is
we brought people into our lab

00:09:22.580 --> 00:09:24.080
in pairs.

00:09:24.080 --> 00:09:27.770
And we asked them to make
financial decisions together.

00:09:27.770 --> 00:09:31.000
Specifically they had
to assess real estate.

00:09:31.000 --> 00:09:33.650
And while they were doing
that we recorded their brain

00:09:33.650 --> 00:09:37.700
activity in two MRI scanners,
but they could communicate

00:09:37.700 --> 00:09:39.950
with each other over computers.

00:09:39.950 --> 00:09:43.130
So the way that it works is
each person lies down like this.

00:09:43.130 --> 00:09:49.670
And there's a little
head coil with a mirror.

00:09:49.670 --> 00:09:51.560
And the mirror can
project anything

00:09:51.560 --> 00:09:53.210
that we want from
a computer screen.

00:09:53.210 --> 00:09:55.130
So we show people the questions.

00:09:55.130 --> 00:09:57.540
We show them the opinion
of their partner.

00:09:57.540 --> 00:10:00.830
And they put their answers
using the button box here.

00:10:00.830 --> 00:10:03.530
And while we do that
we record their brain

00:10:03.530 --> 00:10:07.520
activity using this method
that's called functional MRI.

00:10:07.520 --> 00:10:09.890
So what we found
was when two people

00:10:09.890 --> 00:10:12.770
agreed each person's
brain showed

00:10:12.770 --> 00:10:14.960
precise encoding
of the information

00:10:14.960 --> 00:10:17.190
given by the agreeing partner.

00:10:17.190 --> 00:10:19.700
So what I'm showing you here
is a slice of the brain.

00:10:19.700 --> 00:10:22.940
If I were to cut your brain
like this and then look inside,

00:10:22.940 --> 00:10:26.270
and I'm highlighting all
the regions that we found

00:10:26.270 --> 00:10:28.670
were encoding the
information coming

00:10:28.670 --> 00:10:31.190
from an agreeing partner.

00:10:31.190 --> 00:10:34.550
And what happened was when two
people agreed their confidence

00:10:34.550 --> 00:10:37.610
in their own opinion and
their own decision went up.

00:10:37.610 --> 00:10:39.350
So so far this is not
surprising, right?

00:10:39.350 --> 00:10:43.100
If another person agrees
your confidence goes up.

00:10:43.100 --> 00:10:47.000
However, when the two people
disagreed, metaphorically

00:10:47.000 --> 00:10:49.869
speaking, it looks as if
the brain was shutting down,

00:10:49.869 --> 00:10:51.410
and we couldn't find
evidence that it

00:10:51.410 --> 00:10:54.440
was encoding the information
coming from a disagreeing

00:10:54.440 --> 00:10:55.804
partner.

00:10:55.804 --> 00:10:57.470
And what happened to
people's confidence

00:10:57.470 --> 00:10:58.940
in their own decision?

00:10:58.940 --> 00:11:00.200
It didn't change much.

00:11:00.200 --> 00:11:02.510
There was only a
non-significant decrease.

00:11:02.510 --> 00:11:05.540
So people were kind
of ignoring it.

00:11:05.540 --> 00:11:09.950
Interestingly, sometimes we
blacked out the computer screen

00:11:09.950 --> 00:11:13.470
so people couldn't see
what others were thinking.

00:11:13.470 --> 00:11:16.340
Nevertheless, in those
situations people

00:11:16.340 --> 00:11:19.336
also became more confident
in their own decision.

00:11:19.336 --> 00:11:21.960
I can only assume that they were
thinking that the other person

00:11:21.960 --> 00:11:23.251
is probably agreeing with them.

00:11:23.251 --> 00:11:24.320
They just can't see it.

00:11:24.320 --> 00:11:27.440
And they became
more confident too.

00:11:27.440 --> 00:11:29.390
So many times when
people see this they

00:11:29.390 --> 00:11:32.300
ask me, well, is this
true for everyone?

00:11:32.300 --> 00:11:35.150
Is there individual differences?

00:11:35.150 --> 00:11:38.180
Well, if you see yourself
as highly analytical, which

00:11:38.180 --> 00:11:42.330
I think most people in this
room do, embrace yourself.

00:11:42.330 --> 00:11:46.190
So a study conducted by
[? Don ?] [? Kahan ?] at Yale

00:11:46.190 --> 00:11:50.900
University showed that people
with better math skills are

00:11:50.900 --> 00:11:54.150
more likely to
twist data at will.

00:11:54.150 --> 00:11:58.850
So he tested 1,000 Americans,
and the first thing he did

00:11:58.850 --> 00:12:00.950
is he gave everyone
math questions

00:12:00.950 --> 00:12:04.040
and analytical questions,
logic questions.

00:12:04.040 --> 00:12:06.320
And based on those
questions he divided them

00:12:06.320 --> 00:12:11.020
into those with high skills
and those with low skills.

00:12:11.020 --> 00:12:14.280
And then he gave them
two sets of data.

00:12:14.280 --> 00:12:16.400
The first set of
data he told them

00:12:16.400 --> 00:12:20.690
was looking at whether skin
treatment was helping rashes.

00:12:20.690 --> 00:12:22.110
And he said, OK,
look at the data.

00:12:22.110 --> 00:12:23.040
Analyze the data.

00:12:23.040 --> 00:12:26.270
And tell me if this
treatment is helping.

00:12:26.270 --> 00:12:28.910
Unsurprisingly, those
with better math skills

00:12:28.910 --> 00:12:31.610
did better at this question.

00:12:31.610 --> 00:12:33.740
Then he gave them
another set of data.

00:12:33.740 --> 00:12:35.660
And this time he
said this set of data

00:12:35.660 --> 00:12:37.130
is looking at
whether gun control

00:12:37.130 --> 00:12:40.050
laws are reducing crime.

00:12:40.050 --> 00:12:42.030
Look at the data,
analyze the data,

00:12:42.030 --> 00:12:45.320
and tell me whether
gun control rules

00:12:45.320 --> 00:12:47.240
are helping in reducing crime.

00:12:47.240 --> 00:12:49.790
Now, the difference
here is that everyone

00:12:49.790 --> 00:12:53.030
had very strong opinions
about gun control laws.

00:12:53.030 --> 00:12:56.030
Some people were for, and
some people were against.

00:12:56.030 --> 00:12:59.700
And that interfered with their
ability to analyze the data.

00:12:59.700 --> 00:13:04.310
And, in fact, those with
better math skills did worse.

00:13:04.310 --> 00:13:06.170
Because they were
using their skills

00:13:06.170 --> 00:13:09.590
not necessarily to find
the accurate conclusion,

00:13:09.590 --> 00:13:11.840
but rather to find
fault with the numbers

00:13:11.840 --> 00:13:13.420
that they were unhappy with.

00:13:13.420 --> 00:13:17.510
Right, they had the skills to
do so, while the others didn't.

00:13:17.510 --> 00:13:20.912
AUDIENCE: Did everyone get
the same report in data?

00:13:20.912 --> 00:13:21.620
TALI SHAROT: Yes.

00:13:21.620 --> 00:13:23.330
So actually it
was the same data.

00:13:23.330 --> 00:13:25.380
AUDIENCE: Then isn't it
possible that it just

00:13:25.380 --> 00:13:27.840
happens to be that the
people with high math skills

00:13:27.840 --> 00:13:30.620
happen to have the
political opinions that

00:13:30.620 --> 00:13:33.875
disagreed with that data?

00:13:33.875 --> 00:13:34.820
TALI SHAROT: No.

00:13:34.820 --> 00:13:36.230
So it was a two by two design.

00:13:36.230 --> 00:13:38.600
AUDIENCE: So there was
different sets of data.

00:13:38.600 --> 00:13:38.830
TALI SHAROT: Yeah.

00:13:38.830 --> 00:13:39.350
Yeah.

00:13:39.350 --> 00:13:43.310
It was a two by two
design, which is sometimes

00:13:43.310 --> 00:13:45.050
the data would agree
with one opinion,

00:13:45.050 --> 00:13:46.591
and sometimes with
the other opinion.

00:13:46.591 --> 00:13:48.383
And everything was
counterbalanced.

00:13:48.383 --> 00:13:49.370
Yeah.

00:13:49.370 --> 00:13:50.621
Good question, though.

00:13:50.621 --> 00:13:51.120
OK.

00:13:51.120 --> 00:13:55.490
So the next question
is why would we

00:13:55.490 --> 00:13:58.490
have a brain that has
evolved to disregard

00:13:58.490 --> 00:14:03.870
perfectly good information
when it doesn't fit our view?

00:14:03.870 --> 00:14:07.010
So the answer is that
the brain assesses

00:14:07.010 --> 00:14:10.160
a piece of new evidence
in light of the knowledge

00:14:10.160 --> 00:14:13.250
that it already stores
because on average that is,

00:14:13.250 --> 00:14:15.600
in fact, the correct approach.

00:14:15.600 --> 00:14:17.810
So, for example, if
I were to tell you

00:14:17.810 --> 00:14:20.810
that I saw a pink elephant
flying in the sky,

00:14:20.810 --> 00:14:25.520
you would assume that I am lying
or delusional, as you should.

00:14:25.520 --> 00:14:28.850
Because on average when
a piece of evidence

00:14:28.850 --> 00:14:32.570
doesn't fit a belief that
you hold strongly, on average

00:14:32.570 --> 00:14:34.970
that piece of evidence is wrong.

00:14:34.970 --> 00:14:36.950
So it's a shortcut
that the brain

00:14:36.950 --> 00:14:41.210
uses to assess evidence and
incorporate into beliefs, which

00:14:41.210 --> 00:14:42.590
it actually is quite optimal.

00:14:42.590 --> 00:14:43.550
It's a good process.

00:14:43.550 --> 00:14:47.900
However, it also means
that strongly held beliefs

00:14:47.900 --> 00:14:52.070
are very difficult to change,
even when they're wrong.

00:14:52.070 --> 00:14:54.530
There are four
factors that determine

00:14:54.530 --> 00:14:57.290
whether a piece of evidence
will change your belief.

00:14:57.290 --> 00:15:00.500
Your current belief, your
confidence in that belief,

00:15:00.500 --> 00:15:02.780
the new piece of evidence,
and the confidence

00:15:02.780 --> 00:15:04.250
in that piece of evidence.

00:15:04.250 --> 00:15:07.730
And now the further the piece
of evidence is from your belief,

00:15:07.730 --> 00:15:11.060
the less likely it
is to change it.

00:15:11.060 --> 00:15:13.280
There is one exception though.

00:15:13.280 --> 00:15:15.680
And that is when the
piece of evidence

00:15:15.680 --> 00:15:18.320
doesn't fit what
you believe, but it

00:15:18.320 --> 00:15:20.570
fits what you want to believe.

00:15:20.570 --> 00:15:22.770
So let me give you an example.

00:15:22.770 --> 00:15:26.990
So a few months before the
presidential election 1,000

00:15:26.990 --> 00:15:31.310
Americans were asked by a
group of scientists in England,

00:15:31.310 --> 00:15:33.950
who do you want to
win the election,

00:15:33.950 --> 00:15:37.110
and who do you think is
going to win the election?

00:15:37.110 --> 00:15:41.870
So back in August 2016
50% wanted Trump to win

00:15:41.870 --> 00:15:45.020
and 50% wanted Clinton to win.

00:15:45.020 --> 00:15:48.110
But back then the
Trump supporters

00:15:48.110 --> 00:15:50.240
and the Clinton
supporters believed

00:15:50.240 --> 00:15:52.410
that Clinton was going to win.

00:15:52.410 --> 00:15:54.410
Then they were given a new poll.

00:15:54.410 --> 00:15:59.570
And that poll predicted
a Trump victory.

00:15:59.570 --> 00:16:01.070
And they were asked
again, OK, now

00:16:01.070 --> 00:16:04.790
that you've seen the poll, who
do you think is going to win?

00:16:04.790 --> 00:16:07.040
Did it change their prediction?

00:16:07.040 --> 00:16:08.510
And the answer was, yes.

00:16:08.510 --> 00:16:09.830
It changed their predictions.

00:16:09.830 --> 00:16:11.570
But mostly it changed
the predictions

00:16:11.570 --> 00:16:13.370
of the Trump supporters.

00:16:13.370 --> 00:16:15.830
They were elated
by this new poll.

00:16:15.830 --> 00:16:19.280
And they were quick to move
and say, well, in that case,

00:16:19.280 --> 00:16:21.410
maybe he's going to win.

00:16:21.410 --> 00:16:23.670
The Clinton supporters,
on the other hand,

00:16:23.670 --> 00:16:25.610
didn't change their
prediction much.

00:16:25.610 --> 00:16:28.380
They said, well, we still
think Clinton is going to win.

00:16:28.380 --> 00:16:31.250
We don't really
believe this poll.

00:16:31.250 --> 00:16:34.910
So when people encounter
a piece of evidence

00:16:34.910 --> 00:16:37.550
that goes against what
they really want to believe

00:16:37.550 --> 00:16:40.910
and what they want to happen
our immediate reaction

00:16:40.910 --> 00:16:45.050
is basically this,
denial, rationalization,

00:16:45.050 --> 00:16:49.010
and trying to just distance
ourselves from the fact.

00:16:49.010 --> 00:16:52.490
And perhaps the
most efficient way

00:16:52.490 --> 00:16:54.800
to distance ourself from
the truth or from the facts

00:16:54.800 --> 00:16:57.500
is not to expose ourselves
to that information

00:16:57.500 --> 00:16:59.520
in the first place.

00:16:59.520 --> 00:17:02.790
So take, for example,
the stock market.

00:17:02.790 --> 00:17:04.819
You know when people
check their account

00:17:04.819 --> 00:17:07.520
to see how much they are worth?

00:17:07.520 --> 00:17:10.099
So not with any intention
to make a transaction.

00:17:10.099 --> 00:17:12.950
Just to have a little peek to
see have my stocks gone up?

00:17:12.950 --> 00:17:14.690
Have they gone down?

00:17:14.690 --> 00:17:18.800
So this is a study conducted
by three behavioral economists,

00:17:18.800 --> 00:17:23.859
Lowenstein, Sepi, and Carlson.

00:17:23.859 --> 00:17:26.060
And what you can
see here in black

00:17:26.060 --> 00:17:29.510
is the S&amp;P 500 over two years.

00:17:29.510 --> 00:17:32.870
And in gray is a number of
times that people logged in

00:17:32.870 --> 00:17:34.730
to check on their accounts.

00:17:34.730 --> 00:17:36.030
Now, these are not raw numbers.

00:17:36.030 --> 00:17:38.450
So they've been corrected for
all the obvious confounds,

00:17:38.450 --> 00:17:40.820
like market volume, and
willingness to transact,

00:17:40.820 --> 00:17:41.510
and all of that.

00:17:41.510 --> 00:17:42.010
OK.

00:17:42.010 --> 00:17:43.120
So what do we think?

00:17:43.120 --> 00:17:46.970
When the market is high people
tend to log in all the time.

00:17:46.970 --> 00:17:49.910
They say, well, when the market
is high my value probably

00:17:49.910 --> 00:17:50.720
has gone up.

00:17:50.720 --> 00:17:53.510
And they want to get a
sniff of the good news.

00:17:53.510 --> 00:17:55.550
But when the market
is down people

00:17:55.550 --> 00:17:58.190
are less likely to log in.

00:17:58.190 --> 00:18:00.110
They say, well,
the market is down.

00:18:00.110 --> 00:18:02.870
I probably lost money,
and I don't want to know.

00:18:02.870 --> 00:18:06.980
Now, all this is true as
long as negative information

00:18:06.980 --> 00:18:09.150
can reasonably be avoided.

00:18:09.150 --> 00:18:10.760
So what you don't
see here is what

00:18:10.760 --> 00:18:14.210
happened a few months later in
the financial collapse of 2008

00:18:14.210 --> 00:18:16.400
when the market went
drastically down,

00:18:16.400 --> 00:18:19.250
and that's when people started
logging on frantically.

00:18:19.250 --> 00:18:22.410
But it was a little
bit too late.

00:18:22.410 --> 00:18:25.820
So we know a little
bit about the biology

00:18:25.820 --> 00:18:28.520
that underlies this behavior.

00:18:28.520 --> 00:18:31.880
And a lot of what we know
actually comes from monkeys.

00:18:31.880 --> 00:18:34.870
So there is a great series of
studies by Ethan [INAUDIBLE]

00:18:34.870 --> 00:18:37.850
Martin at Columbia University.

00:18:37.850 --> 00:18:40.970
And what Ethan showed
is that monkeys too

00:18:40.970 --> 00:18:45.710
want to know about the
good stuff ahead of time.

00:18:45.710 --> 00:18:48.410
So what Ethan did
is he did a study

00:18:48.410 --> 00:18:51.050
where monkeys could
get a big reward, which

00:18:51.050 --> 00:18:53.810
was a lot of water, or they
can get a small reward, which

00:18:53.810 --> 00:18:54.740
is a little water.

00:18:54.740 --> 00:18:56.790
Now, for monkeys water
was really important.

00:18:56.790 --> 00:18:57.890
They were quite thirsty.

00:18:57.890 --> 00:19:00.370
So this was a big deal for them.

00:19:00.370 --> 00:19:03.850
It's kind of like getting
$1,000 versus $10.

00:19:03.850 --> 00:19:06.020
And he asked the
monkeys, do you want

00:19:06.020 --> 00:19:08.060
to know a few seconds in
advance whether you're

00:19:08.060 --> 00:19:10.610
going to get a lot of water
or a little bit of water?

00:19:10.610 --> 00:19:12.710
Now, the monkeys
can't talk obviously.

00:19:12.710 --> 00:19:15.380
But he trained them
over many weeks

00:19:15.380 --> 00:19:18.150
to know what different
symbols mean.

00:19:18.150 --> 00:19:20.090
So one symbol they
knew if you see

00:19:20.090 --> 00:19:23.430
the symbol I will tell you in
advance how much water you're

00:19:23.430 --> 00:19:24.170
going to get.

00:19:24.170 --> 00:19:27.320
If you see this other symbol
I will not let you know.

00:19:27.320 --> 00:19:29.930
And so the monkeys use
their eyes to indicate

00:19:29.930 --> 00:19:32.630
do I want to know or
do I not want to know?

00:19:32.630 --> 00:19:34.520
And then if they
said, I want to know,

00:19:34.520 --> 00:19:35.840
then he showed another symbol.

00:19:35.840 --> 00:19:38.750
And that symbol said, you're
about to get a lot of water.

00:19:38.750 --> 00:19:40.460
Or he showed another
symbol that said,

00:19:40.460 --> 00:19:43.100
you're going to get a
little bit of water.

00:19:43.100 --> 00:19:47.420
So first thing he found is,
yes, the monkeys want to know.

00:19:47.420 --> 00:19:49.550
Just as we want to know
if we got a lot of money,

00:19:49.550 --> 00:19:51.650
the monkeys want
to know that too.

00:19:51.650 --> 00:19:53.990
The second thing
was they were even

00:19:53.990 --> 00:19:57.770
willing to pay to know a few
seconds in advance how much

00:19:57.770 --> 00:19:59.400
water they were going to get.

00:19:59.400 --> 00:20:01.409
The way that they paid
is they paid by water.

00:20:01.409 --> 00:20:03.200
So they would give up
a little bit of water

00:20:03.200 --> 00:20:05.366
to know a few seconds in
advance how much water they

00:20:05.366 --> 00:20:06.710
were going to get.

00:20:06.710 --> 00:20:08.480
And Ethan did more than that.

00:20:08.480 --> 00:20:12.020
He recorded the brain
activity of the monkeys

00:20:12.020 --> 00:20:15.350
while they were making
these decisions.

00:20:15.350 --> 00:20:18.710
So he inserted electrodes
into the monkey's brains,

00:20:18.710 --> 00:20:22.260
specifically to a part of the
brain called the midbrain where

00:20:22.260 --> 00:20:24.380
there is dopaminergic neurons.

00:20:24.380 --> 00:20:27.530
And those neurons are
known to respond to rewards

00:20:27.530 --> 00:20:30.140
like water, food, sex.

00:20:30.140 --> 00:20:32.030
And he recorded those neurons.

00:20:32.030 --> 00:20:35.420
And what he wanted to know
is, do those neurons respond

00:20:35.420 --> 00:20:39.200
not only to your basic
rewards, like food and water,

00:20:39.200 --> 00:20:41.480
do they also respond
to the opportunity

00:20:41.480 --> 00:20:42.690
to receive information?

00:20:42.690 --> 00:20:43.190
Right.

00:20:43.190 --> 00:20:46.400
He was asking, does the
brain treat information

00:20:46.400 --> 00:20:48.889
as if it's a real reward?

00:20:48.889 --> 00:20:50.180
So let me show you his results.

00:20:50.180 --> 00:20:51.596
It's a bit of a
complicated graph,

00:20:51.596 --> 00:20:52.860
but I'll walk you through it.

00:20:52.860 --> 00:20:54.890
So what we're looking
at here is activity

00:20:54.890 --> 00:20:57.370
of these dopaminergic
neurons in the midbrain.

00:20:57.370 --> 00:21:00.860
And the first thing that you
see is the classic finding

00:21:00.860 --> 00:21:04.730
which is there is great
activity at the moment when

00:21:04.730 --> 00:21:07.220
the monkeys receive a large
reward into their mouth, lots

00:21:07.220 --> 00:21:08.240
of water.

00:21:08.240 --> 00:21:11.240
And there is a decrease when
they receive just a little bit.

00:21:11.240 --> 00:21:12.730
So they're disappointed.

00:21:12.730 --> 00:21:15.800
So basically these neurons
go up when things are good,

00:21:15.800 --> 00:21:17.600
and they go down
when things are bad.

00:21:17.600 --> 00:21:18.500
OK.

00:21:18.500 --> 00:21:20.090
Now the important thing.

00:21:20.090 --> 00:21:23.030
He found that the same
exact neurons also

00:21:23.030 --> 00:21:27.650
fire when the monkey learns that
he's about to get information.

00:21:27.650 --> 00:21:30.154
Kind of like you get an
email or you get an envelope.

00:21:30.154 --> 00:21:31.820
You don't know what's
in it yet, but you

00:21:31.820 --> 00:21:33.310
know there's information.

00:21:33.310 --> 00:21:35.990
The neurons fire
to the opportunity

00:21:35.990 --> 00:21:40.790
to receive information, just
as they do to water and food.

00:21:40.790 --> 00:21:42.410
And when the monkeys
learn I'm not

00:21:42.410 --> 00:21:46.800
going to give you information
there is a decrease in firing.

00:21:46.800 --> 00:21:48.050
So they go down.

00:21:48.050 --> 00:21:50.690
They're disappointed.

00:21:50.690 --> 00:21:56.000
So Ethan's conclusion was, yes,
the brain treats information

00:21:56.000 --> 00:22:00.140
as if information is
rewards, like water and food.

00:22:00.140 --> 00:22:02.840
But when I saw that
I asked Ethan, well,

00:22:02.840 --> 00:22:05.480
is this true for any
kind of information,

00:22:05.480 --> 00:22:07.545
or is it only true
for information

00:22:07.545 --> 00:22:08.420
about the good stuff?

00:22:08.420 --> 00:22:11.070
What happens if the
monkey's not getting water.

00:22:11.070 --> 00:22:13.329
It's getting shocks,
electric shocks.

00:22:13.329 --> 00:22:14.870
Does a monkey want
to know in advance

00:22:14.870 --> 00:22:17.290
if it's going to be a big
shock or a small shock?

00:22:17.290 --> 00:22:19.610
So Ethan said, well, I
can't shock my monkeys,

00:22:19.610 --> 00:22:24.000
because the ethics won't
allow me to do that.

00:22:24.000 --> 00:22:26.970
So he said, well, let's do
it on undergrads in my lab.

00:22:26.970 --> 00:22:29.120
[LAUGHTER]

00:22:29.120 --> 00:22:31.520
And actually we can get
ethics to shock undergrads,

00:22:31.520 --> 00:22:33.300
but we decided there's no need.

00:22:33.300 --> 00:22:36.290
And we just use a design where
they either get a lot of money

00:22:36.290 --> 00:22:39.170
or a little bit of money,
or they lose a lot of money

00:22:39.170 --> 00:22:41.360
or lose a little bit of money.

00:22:41.360 --> 00:22:45.350
And what we found
was, first of all,

00:22:45.350 --> 00:22:47.810
the undergrads looked
a lot like monkeys.

00:22:47.810 --> 00:22:49.379
So they wanted to
know in advance

00:22:49.379 --> 00:22:51.170
whether they're going
to get a lot of money

00:22:51.170 --> 00:22:52.340
or a little bit of money.

00:22:52.340 --> 00:22:56.100
They were even willing to
pay us a little bit for that.

00:22:56.100 --> 00:22:59.120
And, yes, when we recorded
the same region that Ethan

00:22:59.120 --> 00:23:00.470
was looking at in monkeys--

00:23:00.470 --> 00:23:03.500
we recorded that in
undergrads using FMRI--

00:23:03.500 --> 00:23:05.019
we found the same response.

00:23:05.019 --> 00:23:07.310
So when they learned they
were about to get information

00:23:07.310 --> 00:23:10.049
about good stuff,
these neurons went up.

00:23:10.049 --> 00:23:12.590
And when they learned we're not
going to tell you in advance,

00:23:12.590 --> 00:23:14.270
they went down.

00:23:14.270 --> 00:23:20.010
And when it came to losses we
found the opposite pattern.

00:23:20.010 --> 00:23:24.620
So when they
learned that they're

00:23:24.620 --> 00:23:28.610
going to get
information about losses

00:23:28.610 --> 00:23:32.180
there was actually a decrease
in firing, a disappointment.

00:23:32.180 --> 00:23:34.760
But when they learned
we're not going to tell you

00:23:34.760 --> 00:23:36.580
about how much
you're going to lose

00:23:36.580 --> 00:23:38.300
there was actually an
increase in firing,

00:23:38.300 --> 00:23:41.690
as if the brain was
treating ignorance

00:23:41.690 --> 00:23:45.650
as a reward when the ignorance
was about the bad stuff.

00:23:45.650 --> 00:23:50.690
And behaviorally the
undergrads, the more

00:23:50.690 --> 00:23:53.180
they were likely to win, the
more they wanted to know.

00:23:53.180 --> 00:23:54.890
But the more they
were likely to lose,

00:23:54.890 --> 00:23:57.770
the less they wanted to know.

00:23:57.770 --> 00:24:01.230
And so this kind of explains
our everyday behavior.

00:24:01.230 --> 00:24:02.890
We want to know the good stuff.

00:24:02.890 --> 00:24:03.880
Did I get the job?

00:24:03.880 --> 00:24:05.510
How much money did I get?

00:24:05.510 --> 00:24:07.460
But we kind of don't
want to go to the doctor

00:24:07.460 --> 00:24:10.670
to hear about our results.

00:24:10.670 --> 00:24:14.150
And when we expect a
bad email coming our way

00:24:14.150 --> 00:24:17.870
we don't check
our phone so much.

00:24:17.870 --> 00:24:21.140
But when we expect something
good, then we check it a lot.

00:24:21.140 --> 00:24:23.480
So if the brain
treats information

00:24:23.480 --> 00:24:26.210
about the good stuff
like food and water,

00:24:26.210 --> 00:24:29.300
and it treats information
about bad stuff like a shock,

00:24:29.300 --> 00:24:31.160
well, it's not
surprising that we

00:24:31.160 --> 00:24:33.920
try to find information
about the good stuff

00:24:33.920 --> 00:24:36.530
and try to avoid information
about the bad stuff,

00:24:36.530 --> 00:24:38.810
just as we do trying
to avoid shocks

00:24:38.810 --> 00:24:40.700
and trying to get
food and water.

00:24:40.700 --> 00:24:46.240
And so we will end up with a
view like this of ourselves.

00:24:46.240 --> 00:24:52.760
Now, our mistake is that
instead of going along

00:24:52.760 --> 00:24:57.710
with how our brain works we
often try to go against it.

00:24:57.710 --> 00:25:00.650
We often try to put a clear
mirror in front of people

00:25:00.650 --> 00:25:02.850
and say, look, this
is how it looks.

00:25:02.850 --> 00:25:04.460
This is the data.

00:25:04.460 --> 00:25:05.990
Things are going to get worse.

00:25:05.990 --> 00:25:08.540
Or you're wrong and I'm right.

00:25:08.540 --> 00:25:11.150
And it doesn't often work,
because the brain will

00:25:11.150 --> 00:25:14.060
try to frantically distort
the information until it

00:25:14.060 --> 00:25:17.330
gets the picture
that it's happy with.

00:25:17.330 --> 00:25:20.540
But what will happen if we went
along with how our brain works,

00:25:20.540 --> 00:25:21.830
not against it?

00:25:21.830 --> 00:25:23.750
And that's the idea of
"The Influential Mind."

00:25:23.750 --> 00:25:25.375
Basically what I do
in "The Influential

00:25:25.375 --> 00:25:27.110
Mind," in each chapter
I talk about one

00:25:27.110 --> 00:25:29.810
of these factors that are really
important for how people form

00:25:29.810 --> 00:25:32.750
beliefs and decisions, whether
it's a confirmation bias,

00:25:32.750 --> 00:25:35.642
or how we seek information,
or other factors.

00:25:35.642 --> 00:25:37.100
And then I say,
well, given that we

00:25:37.100 --> 00:25:42.080
know that, how can we go along
with that to make a change?

00:25:42.080 --> 00:25:43.820
And what kind of
mistakes do we do

00:25:43.820 --> 00:25:48.880
when we try to go against
this way that our brain works?

00:25:48.880 --> 00:25:51.212
So I'm just going to
give you a few examples.

00:25:51.212 --> 00:25:52.670
I'm just going to
look at that time

00:25:52.670 --> 00:25:55.170
to make sure that we have time
for questions and everything.

00:25:55.170 --> 00:25:57.956
So I'm going to give you a few
examples of how we can do that.

00:25:57.956 --> 00:25:59.330
So the first thing
I talked about

00:25:59.330 --> 00:26:03.950
was how the brain encodes
information much better when it

00:26:03.950 --> 00:26:06.710
comes from an agreeing partner.

00:26:06.710 --> 00:26:10.940
So our mistake is that usually
when we're in a disagreement

00:26:10.940 --> 00:26:13.270
we come with counter
evidence, right.

00:26:13.270 --> 00:26:14.740
It's like, you're
wrong, I'm right.

00:26:14.740 --> 00:26:16.010
Here is all the evidence.

00:26:16.010 --> 00:26:18.050
And it doesn't
often work so well.

00:26:18.050 --> 00:26:20.180
But could we go along
with how our brain works?

00:26:20.180 --> 00:26:22.610
Could we go along with
a confirmation bias

00:26:22.610 --> 00:26:25.790
by first trying to
find common grounds?

00:26:25.790 --> 00:26:28.880
So common beliefs
or a common motive

00:26:28.880 --> 00:26:31.250
so the other person is more
likely to kind of encode

00:26:31.250 --> 00:26:33.050
what we're saying.

00:26:33.050 --> 00:26:36.750
And let me give you one example
of how this has been done.

00:26:36.750 --> 00:26:39.560
So this is an example that
relates to the relationship

00:26:39.560 --> 00:26:41.840
between autism and vaccines.

00:26:41.840 --> 00:26:44.540
So some parents decide not
to vaccinate their kids

00:26:44.540 --> 00:26:47.810
because of the alleged link
between autism and vaccines.

00:26:47.810 --> 00:26:50.510
Now, the normal approach
of health professionals

00:26:50.510 --> 00:26:53.180
is to say, well, here's
all the data showing

00:26:53.180 --> 00:26:55.019
that there isn't a link.

00:26:55.019 --> 00:26:56.810
But when people have
a very strong opinion,

00:26:56.810 --> 00:26:57.980
that doesn't seem to work.

00:26:57.980 --> 00:27:00.050
And there's studies
showing that there's really

00:27:00.050 --> 00:27:03.860
no change in people's
attitudes when they learn

00:27:03.860 --> 00:27:06.680
this, or a very small change.

00:27:06.680 --> 00:27:09.360
But then a group
at UCLA said, well,

00:27:09.360 --> 00:27:11.450
can we take a different route?

00:27:11.450 --> 00:27:13.550
Can we get to the
same outcome, which

00:27:13.550 --> 00:27:16.490
is getting the parents
to vaccinate their kids,

00:27:16.490 --> 00:27:19.290
without actually talking
about what we disagree on,

00:27:19.290 --> 00:27:22.220
but rather talking
about what we agree on?

00:27:22.220 --> 00:27:23.840
And so what they
highlighted instead

00:27:23.840 --> 00:27:27.470
was that these vaccines
actually protect kids

00:27:27.470 --> 00:27:30.680
from the measles,
mumps, or rubella.

00:27:30.680 --> 00:27:33.380
This is not something that
the parents disagreed on.

00:27:33.380 --> 00:27:36.680
But it seemed to be forgotten
in the heated debate.

00:27:36.680 --> 00:27:40.340
So by highlighting, in fact,
what they agreed on, and also

00:27:40.340 --> 00:27:42.080
a common motive
that they had, which

00:27:42.080 --> 00:27:44.450
is keeping the
kids healthy, they

00:27:44.450 --> 00:27:47.390
were able to change parents'
intentions of vaccinating

00:27:47.390 --> 00:27:50.810
their kids by three-fold.

00:27:50.810 --> 00:27:53.990
So trying to think about what
we have in common in order

00:27:53.990 --> 00:27:55.820
to kind of work with
a confirmation bias

00:27:55.820 --> 00:27:57.500
instead of against it.

00:27:57.500 --> 00:28:00.320
And there's another reason
that this works so well.

00:28:00.320 --> 00:28:03.080
And that relates to
the brain's tendency

00:28:03.080 --> 00:28:05.390
to encode information
that suggests

00:28:05.390 --> 00:28:08.930
progress better than information
that suggests decline.

00:28:08.930 --> 00:28:11.570
So let me first show
you an experiment

00:28:11.570 --> 00:28:13.910
that we've done that shows
this and then how can we

00:28:13.910 --> 00:28:16.760
use that knowledge.

00:28:16.760 --> 00:28:20.480
So in one study we brought
people into the lab,

00:28:20.480 --> 00:28:22.790
and we gave them 100
different events,

00:28:22.790 --> 00:28:26.030
negative events that can happen
to them in their lifetime.

00:28:26.030 --> 00:28:27.680
For example, cancer.

00:28:27.680 --> 00:28:29.440
And we said, what
is the likelihood,

00:28:29.440 --> 00:28:31.310
do you think, that
you will encounter

00:28:31.310 --> 00:28:33.590
this event in your lifetime?

00:28:33.590 --> 00:28:35.360
Just put in the number.

00:28:35.360 --> 00:28:37.140
So each person
puts in the number.

00:28:37.140 --> 00:28:40.400
And then we say, well,
this is the likelihood

00:28:40.400 --> 00:28:43.550
that you will have
cancer based on your age,

00:28:43.550 --> 00:28:45.750
and where you live, and
the gender, and so on.

00:28:45.750 --> 00:28:48.080
Its about, for
example, 30%, which

00:28:48.080 --> 00:28:51.630
is actually close to the
number in the population.

00:28:51.630 --> 00:28:53.300
And then we asked them again.

00:28:53.300 --> 00:28:56.030
How likely are you
to have cancer,

00:28:56.030 --> 00:28:58.910
and 100 different other events.

00:28:58.910 --> 00:29:01.400
What we wanted to know is
whether the information

00:29:01.400 --> 00:29:05.250
that we gave them
changed their estimates.

00:29:05.250 --> 00:29:08.760
And what we found is,
indeed, it did, but mostly

00:29:08.760 --> 00:29:12.130
when the information was
better than expected.

00:29:12.130 --> 00:29:13.980
So, for example,
if someone said,

00:29:13.980 --> 00:29:17.550
well, I think my likelihood
of having cancer is about 50%,

00:29:17.550 --> 00:29:19.350
and we said, hey,
for someone like you

00:29:19.350 --> 00:29:23.370
it's only about 30%,
it's good news, right?

00:29:23.370 --> 00:29:26.739
So they said, well, in that
case, maybe it's only 35%.

00:29:26.739 --> 00:29:29.280
So when they got good news they
changed their estimate really

00:29:29.280 --> 00:29:31.170
quite quickly.

00:29:31.170 --> 00:29:33.060
But if someone
said, well, I think

00:29:33.060 --> 00:29:35.880
for me my likelihood
is about 10%,

00:29:35.880 --> 00:29:39.480
and we said, for you
it's actually worse.

00:29:39.480 --> 00:29:43.710
On average it's
probably around 30%.

00:29:43.710 --> 00:29:46.590
The next time around
they said, yeah,

00:29:46.590 --> 00:29:49.920
I still think it's about 11%.

00:29:49.920 --> 00:29:52.341
So it's not that they didn't
change their estimate at all.

00:29:52.341 --> 00:29:52.840
They did.

00:29:52.840 --> 00:29:54.120
They learned from bad news.

00:29:54.120 --> 00:29:58.099
But they learned from bad
news less than from good news.

00:29:58.099 --> 00:30:00.390
And it's not that they didn't
remember the information.

00:30:00.390 --> 00:30:02.520
They remembered the
information that we gave them.

00:30:02.520 --> 00:30:03.780
But they didn't
think it was related

00:30:03.780 --> 00:30:05.040
to them when it was bad news.

00:30:05.040 --> 00:30:06.414
But they did think
it was related

00:30:06.414 --> 00:30:08.370
to them when it was good news.

00:30:08.370 --> 00:30:12.030
And we did this study again
by looking at people's brain

00:30:12.030 --> 00:30:15.250
activity while they were
performing this task.

00:30:15.250 --> 00:30:18.540
And what we found was that
quite a few regions in the brain

00:30:18.540 --> 00:30:21.780
were responsive to unexpected
positive information.

00:30:21.780 --> 00:30:24.180
They were encoding this
information quite well,

00:30:24.180 --> 00:30:25.980
including this
region here, which

00:30:25.980 --> 00:30:28.040
is called the left
inferior frontal gyrus.

00:30:28.040 --> 00:30:30.036
So it sits over here.

00:30:30.036 --> 00:30:32.990
However, when we
gave people bad news

00:30:32.990 --> 00:30:35.160
another part of the
brain on the other side,

00:30:35.160 --> 00:30:37.260
the right inferior
frontal gyrus,

00:30:37.260 --> 00:30:39.380
was encoding that information.

00:30:39.380 --> 00:30:42.120
And on average it didn't
do a very good job.

00:30:42.120 --> 00:30:46.380
So it didn't encode it
as well as the good news.

00:30:46.380 --> 00:30:51.150
And as a result people were
less likely to incorporate

00:30:51.150 --> 00:30:55.240
unexpected bad news than
unexpected good news.

00:30:55.240 --> 00:30:57.600
So we were wondering,
can we change this?

00:30:57.600 --> 00:31:01.080
Can we change the way that
people process information

00:31:01.080 --> 00:31:04.800
by changing the activity
in the brain directly?

00:31:04.800 --> 00:31:07.690
And there is at least one
way that we could do that.

00:31:07.690 --> 00:31:09.000
And so this is my collaborator.

00:31:09.000 --> 00:31:13.170
And what he's doing is, he's
passing a small magnetic pulse

00:31:13.170 --> 00:31:15.270
through the scalp
of this participant.

00:31:15.270 --> 00:31:17.460
And we can direct
it specifically

00:31:17.460 --> 00:31:19.240
to part of the brain.

00:31:19.240 --> 00:31:20.820
So we can, for
example, direct it

00:31:20.820 --> 00:31:22.980
to the inferior frontal
gyrus that I showed you,

00:31:22.980 --> 00:31:24.780
either left or right.

00:31:24.780 --> 00:31:27.300
And by doing that
we are interfering

00:31:27.300 --> 00:31:29.070
with the activity of
this brain function

00:31:29.070 --> 00:31:30.960
for about half an hour.

00:31:30.960 --> 00:31:35.040
And after that everything goes
back to normal, or so we hope.

00:31:35.040 --> 00:31:36.770
So doing that--

00:31:36.770 --> 00:31:37.500
[LAUGHTER]

00:31:37.500 --> 00:31:38.960
That was a delayed response.

00:31:38.960 --> 00:31:42.060
So during that half
hour we could have

00:31:42.060 --> 00:31:44.790
the participant do our tasks.

00:31:44.790 --> 00:31:48.990
And we did this for three
groups of participants.

00:31:48.990 --> 00:31:50.830
One group was the control group.

00:31:50.830 --> 00:31:53.500
So it's as if I would
test all of you now.

00:31:53.500 --> 00:31:55.440
And I would measure how
much you learned more

00:31:55.440 --> 00:31:56.640
from good news and bad news.

00:31:56.640 --> 00:31:59.181
So this is on average how much
you would learn more from good

00:31:59.181 --> 00:32:00.420
than bad.

00:32:00.420 --> 00:32:03.390
Then we interfered with the
part of the brain that was

00:32:03.390 --> 00:32:05.340
encoding negative information.

00:32:05.340 --> 00:32:07.200
We interfered with it.

00:32:07.200 --> 00:32:10.681
And so the bias
became even larger.

00:32:10.681 --> 00:32:12.930
And when we interfered with
the part of the brain that

00:32:12.930 --> 00:32:15.900
was encoding good news,
we interfered with it,

00:32:15.900 --> 00:32:19.464
the bias disappeared altogether.

00:32:19.464 --> 00:32:20.880
So we were kind
of amazed by this,

00:32:20.880 --> 00:32:23.160
that we could change this
kind of deep rooted bias

00:32:23.160 --> 00:32:25.969
by changing activity
in the brain.

00:32:25.969 --> 00:32:27.510
But, of course,
we're not going to go

00:32:27.510 --> 00:32:29.910
around zapping people's brains.

00:32:29.910 --> 00:32:32.130
And so that means
that on average

00:32:32.130 --> 00:32:33.580
when people get bad news--

00:32:33.580 --> 00:32:34.080
Yeah.

00:32:34.080 --> 00:32:34.651
Go ahead.

00:32:34.651 --> 00:32:36.900
AUDIENCE: Did the interference
reduce people's ability

00:32:36.900 --> 00:32:42.150
to learn or increase their
ability to learn correctly?

00:32:42.150 --> 00:32:45.990
TALI SHAROT: So, first of
all, it's obviously selective.

00:32:45.990 --> 00:32:49.200
Because different parts
interfere with learning either

00:32:49.200 --> 00:32:51.302
from good or bad.

00:32:51.302 --> 00:32:53.760
And so what is the difference
between learning and learning

00:32:53.760 --> 00:32:54.260
correctly?

00:32:54.260 --> 00:32:56.800
Can you elaborate?

00:32:56.800 --> 00:32:59.760
So overall, it's not a general
interference with learning.

00:32:59.760 --> 00:33:02.080
It's valency specific.

00:33:02.080 --> 00:33:02.690
Right?

00:33:02.690 --> 00:33:03.510
AUDIENCE: Right.

00:33:03.510 --> 00:33:06.510
So if you interfere
with the part that

00:33:06.510 --> 00:33:10.199
makes them learn very
strongly from good news,

00:33:10.199 --> 00:33:12.240
then does that mean that
they learn less strongly

00:33:12.240 --> 00:33:15.960
from good news, or that
they learn equally strongly

00:33:15.960 --> 00:33:17.624
from good and bad news?

00:33:17.624 --> 00:33:19.290
Does it improve the
learning of bad news

00:33:19.290 --> 00:33:20.820
or decrease the
learning of good news?

00:33:20.820 --> 00:33:21.240
TALI SHAROT: Yeah.

00:33:21.240 --> 00:33:21.740
OK.

00:33:21.740 --> 00:33:23.060
So that's a very good question.

00:33:23.060 --> 00:33:28.920
And so it depends on which
part we were looking at.

00:33:28.920 --> 00:33:32.310
So, in fact, the story
is a little bit more

00:33:32.310 --> 00:33:36.180
complicated, which is when we
interfered with that part, what

00:33:36.180 --> 00:33:40.560
we actually also found was
an inhibition of learning

00:33:40.560 --> 00:33:42.370
from bad news.

00:33:42.370 --> 00:33:44.400
So the good news
area-- it's not really

00:33:44.400 --> 00:33:47.276
a good news area-- but that
part was not only encoding

00:33:47.276 --> 00:33:48.150
positive information.

00:33:48.150 --> 00:33:51.640
It was also inhibiting learning
from negative information.

00:33:51.640 --> 00:33:56.730
And we actually followed this
up with an experiment using DTI.

00:33:56.730 --> 00:34:00.270
So DTI basically looks at
white matter connectivity

00:34:00.270 --> 00:34:03.330
between different
neuron regions.

00:34:03.330 --> 00:34:06.660
And what we could find
was that people who

00:34:06.660 --> 00:34:10.620
had better connectivity
between these regions,

00:34:10.620 --> 00:34:12.810
the left inferior frontal
gyrus in particularly,

00:34:12.810 --> 00:34:16.469
and subcortical regions that
were important for emotion,

00:34:16.469 --> 00:34:19.620
and memory, and learning, people
who had better connections

00:34:19.620 --> 00:34:23.460
showed more of a bias
because of two things.

00:34:23.460 --> 00:34:25.889
Both they learned
better from positive.

00:34:25.889 --> 00:34:28.290
And they learned
worse from negative.

00:34:28.290 --> 00:34:31.560
So that suggests-- it's only a
suggestion-- that there could

00:34:31.560 --> 00:34:35.250
be inhibition, as
well as amplifying

00:34:35.250 --> 00:34:38.100
learning from positive, but
also inhibition from negative.

00:34:38.100 --> 00:34:40.650
So very good question.

00:34:40.650 --> 00:34:43.200
Yeah.

00:34:43.200 --> 00:34:44.040
AUDIENCE: Thanks.

00:34:44.040 --> 00:34:44.440
TALI SHAROT: Right.

00:34:44.440 --> 00:34:44.940
OK.

00:34:44.940 --> 00:34:48.760
So what this means on average--
and also just one little note.

00:34:48.760 --> 00:34:50.139
There is individual differences.

00:34:50.139 --> 00:34:54.044
So, for example, in depressed
individuals we don't find this.

00:34:54.044 --> 00:34:54.960
So they don't have it.

00:34:54.960 --> 00:34:58.740
They have the same learning
from positive and negative.

00:34:58.740 --> 00:35:00.960
And also hopefully I'll
have time in the end

00:35:00.960 --> 00:35:04.260
to show you that actually
this changes according

00:35:04.260 --> 00:35:05.610
to your own mental state.

00:35:05.610 --> 00:35:07.470
So this is not something
that's stable even

00:35:07.470 --> 00:35:09.000
within an individual.

00:35:09.000 --> 00:35:11.130
It actually changes
in response to what's

00:35:11.130 --> 00:35:13.170
going on in the environment,
and whether you're

00:35:13.170 --> 00:35:15.510
in a safe environment or
dangerous environment.

00:35:15.510 --> 00:35:17.010
So it's a very flexible system.

00:35:19.740 --> 00:35:24.240
But on average, for most people,
in relaxed circumstances,

00:35:24.240 --> 00:35:26.730
it means that we
tend to learn more

00:35:26.730 --> 00:35:28.660
from the good news
and the bad news.

00:35:28.660 --> 00:35:31.770
And so when smokers see
something like this they say,

00:35:31.770 --> 00:35:35.527
yes, smoking kills, but
mostly it kills the other guy.

00:35:35.527 --> 00:35:37.110
But when they get
the good news, which

00:35:37.110 --> 00:35:38.770
is the housing
market is going up,

00:35:38.770 --> 00:35:40.740
which it seems to be
going here, people

00:35:40.740 --> 00:35:43.350
say, oh, my house will
definitely rise in price.

00:35:45.900 --> 00:35:48.270
So what we can
take away from this

00:35:48.270 --> 00:35:52.710
is that we often need to reframe
our message to try and figure

00:35:52.710 --> 00:35:55.980
out, can I reframe it to
highlight the progress, not

00:35:55.980 --> 00:35:57.010
the decline?

00:35:57.010 --> 00:35:58.530
And let me give
you a few examples.

00:35:58.530 --> 00:36:00.810
And then I'll give you a
more empirical example.

00:36:00.810 --> 00:36:03.420
So, for example,
instead of telling a kid

00:36:03.420 --> 00:36:05.047
if you smoke you
would get cancer,

00:36:05.047 --> 00:36:06.630
you might also say,
well, if you don't

00:36:06.630 --> 00:36:08.990
smoke you're more likely to
get on the basketball team.

00:36:08.990 --> 00:36:10.890
Or instead of telling
someone if you take out

00:36:10.890 --> 00:36:14.010
A you will lose time and
money, so highlighting decline,

00:36:14.010 --> 00:36:18.190
you might say if you take route
B you will gain time and money.

00:36:18.190 --> 00:36:21.240
So highlighting what we need
to do to get to progress

00:36:21.240 --> 00:36:25.110
rather than highlighting what
has been done to decline.

00:36:25.110 --> 00:36:28.260
And let me give you one
example, empirical example.

00:36:28.260 --> 00:36:32.400
So we all know that
hand-washing is the number one

00:36:32.400 --> 00:36:34.900
way to reduce the
spread of disease.

00:36:34.900 --> 00:36:37.530
So in a hospital
in the east coast

00:36:37.530 --> 00:36:41.070
cameras were installed to see
how often medical staff will

00:36:41.070 --> 00:36:44.280
wash their hands before and
after entering a patient's

00:36:44.280 --> 00:36:46.500
room.

00:36:46.500 --> 00:36:49.080
And the medical staff knew that
the cameras were installed.

00:36:49.080 --> 00:36:51.600
It wasn't a nanny
camera situation.

00:36:51.600 --> 00:36:55.581
Nonetheless, only 10% of
medical staff, so 1 in 10,

00:36:55.581 --> 00:36:58.080
would wash their hands before
and after entering a patient's

00:36:58.080 --> 00:36:59.610
room.

00:36:59.610 --> 00:37:02.340
But then an electronic
board was introduced.

00:37:02.340 --> 00:37:06.180
And the electronic board told
the medical staff in real time

00:37:06.180 --> 00:37:08.310
how well they were doing.

00:37:08.310 --> 00:37:10.480
So every time the medical
staff washed their hands

00:37:10.480 --> 00:37:12.690
there were actually people
in India watching them.

00:37:12.690 --> 00:37:15.141
And they automatically
gave them good feedback

00:37:15.141 --> 00:37:16.140
on the electronic board.

00:37:16.140 --> 00:37:19.200
It said like, well
done, good job.

00:37:19.200 --> 00:37:22.230
And also the numbers went
up, showing the current shift

00:37:22.230 --> 00:37:25.320
rate and the weekly rate
going up immediately.

00:37:25.320 --> 00:37:30.210
And what they found was that
compliance raised to 90%

00:37:30.210 --> 00:37:31.350
almost immediately.

00:37:31.350 --> 00:37:33.270
And it stayed there.

00:37:33.270 --> 00:37:35.670
So this effect is
so large that it

00:37:35.670 --> 00:37:37.410
seems a little bit suspicious.

00:37:37.410 --> 00:37:39.870
So the research team
made sure to replicate it

00:37:39.870 --> 00:37:42.240
in another division
in a hospital.

00:37:42.240 --> 00:37:44.910
This time the medical
staff started at 30%.

00:37:44.910 --> 00:37:47.070
So one in three medical
staff washed their hands

00:37:47.070 --> 00:37:50.609
before and after entering
a patient's room, which is

00:37:50.609 --> 00:37:51.900
actually closer to the average.

00:37:51.900 --> 00:37:56.080
So the average both in hospitals
and restaurants is 38%.

00:37:56.080 --> 00:37:58.260
But then the electronic
board was introduced,

00:37:58.260 --> 00:38:01.600
and, again, went up
to 90% immediately.

00:38:01.600 --> 00:38:02.100
OK.

00:38:02.100 --> 00:38:03.900
So why does this
intervention work so well?

00:38:03.900 --> 00:38:05.550
And here is the
general message, right.

00:38:05.550 --> 00:38:07.770
It's not about hand-washing.

00:38:07.770 --> 00:38:09.260
The reason this
intervention works

00:38:09.260 --> 00:38:12.570
so well, one of the
reasons is that instead

00:38:12.570 --> 00:38:14.542
of using the normal
approach-- what

00:38:14.542 --> 00:38:15.750
would be the normal approach?

00:38:15.750 --> 00:38:17.730
The normal approach
in hospitals is

00:38:17.730 --> 00:38:20.250
to warn medical
staff of bad stuff

00:38:20.250 --> 00:38:22.920
in the future, of
decline, of disease.

00:38:22.920 --> 00:38:25.140
Instead of doing that
what they did was

00:38:25.140 --> 00:38:27.660
they highlighted the
possibility of progress,

00:38:27.660 --> 00:38:30.180
by showing the medical staff
the numbers going up every time

00:38:30.180 --> 00:38:31.690
they washed their hands.

00:38:31.690 --> 00:38:34.170
And it did one more thing.

00:38:34.170 --> 00:38:36.750
It gave the medical
staff immediate rewards.

00:38:36.750 --> 00:38:39.090
So every time they
washed their hands

00:38:39.090 --> 00:38:40.770
there was a positive feedback.

00:38:40.770 --> 00:38:43.200
And we know that positive
feedbacks activate

00:38:43.200 --> 00:38:44.820
reward regions in our brains.

00:38:44.820 --> 00:38:46.650
There is a reward
signal in our brain

00:38:46.650 --> 00:38:49.830
that reinforces the
action that caused it.

00:38:49.830 --> 00:38:51.660
And so that action
is more likely to be

00:38:51.660 --> 00:38:52.740
repeated in the future.

00:38:55.260 --> 00:38:59.730
And that's actually
our next factor

00:38:59.730 --> 00:39:02.910
that we're going to talk about,
which is immediate rewards.

00:39:02.910 --> 00:39:03.856
Yeah.

00:39:03.856 --> 00:39:06.516
AUDIENCE: You said also
that they're being watched.

00:39:06.516 --> 00:39:07.015
Yeah.

00:39:07.015 --> 00:39:08.380
Let me get a mic.

00:39:08.380 --> 00:39:11.070
It's also that
they're being watched.

00:39:11.070 --> 00:39:14.014
I mean, the message is that
they're being monitored.

00:39:14.014 --> 00:39:14.680
TALI SHAROT: No.

00:39:14.680 --> 00:39:16.570
So we know that's not the case.

00:39:16.570 --> 00:39:20.230
Because they were monitored,
and then it was 10%.

00:39:20.230 --> 00:39:21.550
So they were monitored.

00:39:21.550 --> 00:39:23.530
The first thing they
did was monitor them,

00:39:23.530 --> 00:39:25.363
telling them that they
were monitoring them,

00:39:25.363 --> 00:39:27.080
and that's where
the 10% came from.

00:39:27.080 --> 00:39:29.424
That's the first point, right.

00:39:29.424 --> 00:39:31.340
And then they introduced
the electronic board.

00:39:31.340 --> 00:39:32.281
And then it went up.

00:39:32.281 --> 00:39:34.322
AUDIENCE: Did they try
going, we're watching you,

00:39:34.322 --> 00:39:35.497
we're watching you?

00:39:35.497 --> 00:39:36.580
TALI SHAROT: I don't know.

00:39:36.580 --> 00:39:39.080
I think they just told them
they were going to be watched.

00:39:39.080 --> 00:39:42.580
So monitoring alone seemed to
not have much of an effect.

00:39:42.580 --> 00:39:45.190
But there are other reasons,
which I talk about in the book.

00:39:45.190 --> 00:39:47.680
I just don't have time to
talk about everything today.

00:39:47.680 --> 00:39:50.440
But, for example,
social norms come in.

00:39:50.440 --> 00:39:54.220
Because what they see is, oh,
look, now 65% of my colleagues

00:39:54.220 --> 00:39:55.420
wash their hands.

00:39:55.420 --> 00:39:57.400
So it's not only
these two issues.

00:39:57.400 --> 00:40:01.920
It's a very smart design because
it really has a lot of things.

00:40:01.920 --> 00:40:04.210
There are even other
things that matter here.

00:40:04.210 --> 00:40:07.720
It gives them control,
that it's their choice.

00:40:07.720 --> 00:40:08.830
So there's other things.

00:40:08.830 --> 00:40:10.900
But it's not monitoring.

00:40:10.900 --> 00:40:12.280
They knew that.

00:40:12.280 --> 00:40:15.671
AUDIENCE: So I've seen a lot
of studies around how pain--

00:40:15.671 --> 00:40:16.420
TALI SHAROT: What?

00:40:16.420 --> 00:40:20.720
AUDIENCE: Pain, or fear also
motivates more than rewards.

00:40:20.720 --> 00:40:22.480
And I haven't really
dug into any of it.

00:40:22.480 --> 00:40:23.610
But I'm curious to
hear your thoughts.

00:40:23.610 --> 00:40:23.870
TALI SHAROT: Yeah.

00:40:23.870 --> 00:40:24.970
So we're going to
talk about it now.

00:40:24.970 --> 00:40:26.770
So I'll talk about
when rewards are better

00:40:26.770 --> 00:40:29.057
and when fear is better.

00:40:29.057 --> 00:40:30.390
So we'll start with the rewards.

00:40:30.390 --> 00:40:34.060
So when we talk about
immediate rewards,

00:40:34.060 --> 00:40:36.020
both words matter a lot.

00:40:36.020 --> 00:40:38.350
So first of all,
immediate and rewards.

00:40:38.350 --> 00:40:41.740
So focus about immediate first.

00:40:41.740 --> 00:40:44.380
We find that immediate rewards
are more effective than rewards

00:40:44.380 --> 00:40:48.260
that you can get in the future
for at least two reasons.

00:40:48.260 --> 00:40:51.440
One is known as the
attribution problem.

00:40:51.440 --> 00:40:53.920
So imagine your partner
is washing the dishes

00:40:53.920 --> 00:40:56.050
and then you give them a kiss.

00:40:56.050 --> 00:40:58.696
So it's clear that the kiss is
related to washing the dishes.

00:40:58.696 --> 00:41:00.820
And so they're more likely
to wash the dishes again

00:41:00.820 --> 00:41:02.490
in the future.

00:41:02.490 --> 00:41:05.740
Now, imagine that your
partner is washing the dishes,

00:41:05.740 --> 00:41:08.570
but then you give them
a kiss the next day.

00:41:08.570 --> 00:41:11.050
It's now not clear
that the kiss is

00:41:11.050 --> 00:41:12.910
related to washing the dishes.

00:41:12.910 --> 00:41:17.230
And so it's less likely
to strengthen that action.

00:41:17.230 --> 00:41:19.880
So immediacy helps
because of that.

00:41:19.880 --> 00:41:21.400
And then at least
one other reason

00:41:21.400 --> 00:41:23.530
is called temporal discounting.

00:41:23.530 --> 00:41:26.170
So we know that
people value rewards

00:41:26.170 --> 00:41:28.210
now more than the
same rewards that they

00:41:28.210 --> 00:41:29.690
can get in the future.

00:41:29.690 --> 00:41:31.190
So, for example,
if you ask people,

00:41:31.190 --> 00:41:35.410
do you want $100 now
or $110 next week,

00:41:35.410 --> 00:41:39.100
most people will take $100 now.

00:41:39.100 --> 00:41:42.490
And at least one reason is
that if you get the money now,

00:41:42.490 --> 00:41:43.540
it's certain.

00:41:43.540 --> 00:41:44.790
It's in your hands.

00:41:44.790 --> 00:41:46.570
But $110 next
week, well, there's

00:41:46.570 --> 00:41:47.830
something uncertain about it.

00:41:47.830 --> 00:41:50.150
And people don't
want uncertainty.

00:41:50.150 --> 00:41:52.720
So that's another reason
why immediate works so well.

00:41:52.720 --> 00:41:55.760
Now the question about
rewards versus punishments.

00:41:55.760 --> 00:42:00.130
So why are rewards more
likely to induce actions

00:42:00.130 --> 00:42:03.370
than punishments, and
when do punishments work?

00:42:03.370 --> 00:42:05.500
Well, one reason that
we find that rewards

00:42:05.500 --> 00:42:08.500
are more effective
at enhancing action

00:42:08.500 --> 00:42:13.150
is because of what's known as
the approach avoidance rule.

00:42:13.150 --> 00:42:15.890
So in life to get
the good stuff,

00:42:15.890 --> 00:42:20.080
whether it's chocolate cake,
or love, or a promotion,

00:42:20.080 --> 00:42:21.151
we usually need to act.

00:42:21.151 --> 00:42:21.650
Right.

00:42:21.650 --> 00:42:22.660
We need to approach.

00:42:22.660 --> 00:42:24.680
We need to do something.

00:42:24.680 --> 00:42:29.560
And so our brain has evolved
in an environment where

00:42:29.560 --> 00:42:32.140
action is a way to get rewards.

00:42:32.140 --> 00:42:34.720
And so our reward system
is very much connected

00:42:34.720 --> 00:42:36.880
to our modus system.

00:42:36.880 --> 00:42:42.190
And I'm showing you part
of the reward system here.

00:42:42.190 --> 00:42:44.580
So down here we
have the midbrain.

00:42:44.580 --> 00:42:46.840
So these are the same
neurons that Ethan

00:42:46.840 --> 00:42:48.490
recorded from in his monkeys.

00:42:48.490 --> 00:42:51.430
So we have dopaminergic neurons
down there in the midbrain.

00:42:51.430 --> 00:42:55.090
They go up into the center
of the brain, the striatum,

00:42:55.090 --> 00:42:57.250
the nucleus accumbens,
which is what usually people

00:42:57.250 --> 00:43:00.290
talk about when they talk about
the reward center in the brain.

00:43:00.290 --> 00:43:06.220
And then you go into the
cortex and the motor cortex.

00:43:06.220 --> 00:43:11.620
And what we find is that when
you anticipate something good,

00:43:11.620 --> 00:43:15.040
a go signal is activated
from the midbrain,

00:43:15.040 --> 00:43:17.920
and it makes action more likely.

00:43:17.920 --> 00:43:21.680
However, to avoid the
bad stuff in life,

00:43:21.680 --> 00:43:25.060
whether it is poison,
or untrustworthy people,

00:43:25.060 --> 00:43:31.360
or deep waters, mostly the best
approach is not to do anything,

00:43:31.360 --> 00:43:32.590
just to stay put.

00:43:32.590 --> 00:43:33.640
Not to take an action.

00:43:33.640 --> 00:43:34.900
Not to take a risk.

00:43:34.900 --> 00:43:35.690
Not always.

00:43:35.690 --> 00:43:37.880
But often that's
the best approach.

00:43:37.880 --> 00:43:40.720
And so the brain has
adapted to that environment.

00:43:40.720 --> 00:43:43.560
And when people anticipate
a loss or something bad

00:43:43.560 --> 00:43:47.740
there is a no go signal in the
brain, and it inhibits action.

00:43:47.740 --> 00:43:50.080
So, again, coming from the
midbrain to the motor cortex

00:43:50.080 --> 00:43:53.300
and inhibiting action.

00:43:53.300 --> 00:43:55.960
A study that was
led by [INAUDIBLE]

00:43:55.960 --> 00:43:59.250
that I collaborated on
as well, what he did,

00:43:59.250 --> 00:44:02.870
he showed that if
you promise people $1

00:44:02.870 --> 00:44:05.870
for pressing a button,
they will be quicker

00:44:05.870 --> 00:44:13.640
at pressing the button
than if you tell them

00:44:13.640 --> 00:44:16.737
that you will take away $1
for them pressing the button.

00:44:16.737 --> 00:44:19.070
So if you say, I'll give you
$1 for pressing the button,

00:44:19.070 --> 00:44:21.260
they do it quicker
than if you tell them,

00:44:21.260 --> 00:44:24.560
if you press the button I won't
take away $1 from you, a loss.

00:44:24.560 --> 00:44:27.720
So they're quicker to do
actions to get a reward.

00:44:27.720 --> 00:44:31.670
The opposite happens when
they need to not act.

00:44:31.670 --> 00:44:34.460
So when we tell them,
don't press a button

00:44:34.460 --> 00:44:37.100
and we will give
you $1, or when you

00:44:37.100 --> 00:44:40.370
tell them don't press a
button, and you won't lose $1,

00:44:40.370 --> 00:44:42.110
they make less
mistakes when they

00:44:42.110 --> 00:44:45.512
don't need to press a button
in order not to lose $1.

00:44:45.512 --> 00:44:46.970
The reason, again,
is that when you

00:44:46.970 --> 00:44:49.100
tell them don't press a
button to get $1, sometimes

00:44:49.100 --> 00:44:51.530
they just immediately press
$1, because the anticipation

00:44:51.530 --> 00:44:53.660
of reward causes action.

00:44:53.660 --> 00:44:58.190
So that's kind of the connection
between rewards and action

00:44:58.190 --> 00:44:59.940
and punishments and inaction.

00:44:59.940 --> 00:45:03.260
So back to your
question, it seems

00:45:03.260 --> 00:45:05.090
that if you want
to motivate action,

00:45:05.090 --> 00:45:09.080
like you wanted to get employees
to work on their report really

00:45:09.080 --> 00:45:11.570
well, or if you want your
kid to tidy their room,

00:45:11.570 --> 00:45:15.050
perhaps promising a reward
would be better than threatening

00:45:15.050 --> 00:45:15.900
with punishment.

00:45:15.900 --> 00:45:18.350
But if you want someone
not to do something,

00:45:18.350 --> 00:45:20.750
so you want your kid
not to eat a cookie,

00:45:20.750 --> 00:45:22.520
or you want your
employees not to share

00:45:22.520 --> 00:45:25.940
privileged information, perhaps
the threat of a punishment

00:45:25.940 --> 00:45:27.660
would be better.

00:45:27.660 --> 00:45:29.960
So we have to be very
cautious from taking

00:45:29.960 --> 00:45:34.070
this basic science to more
like real world application.

00:45:34.070 --> 00:45:36.570
But if that's true, if
that connection is true,

00:45:36.570 --> 00:45:38.941
that's what this suggests.

00:45:38.941 --> 00:45:39.440
OK.

00:45:39.440 --> 00:45:44.300
So then you can see why
telling the medical staff,

00:45:44.300 --> 00:45:47.930
threatening them
that if they need

00:45:47.930 --> 00:45:51.200
to wash their hands otherwise
there'll be disease didn't work

00:45:51.200 --> 00:45:54.170
as well as promising
them a reward, which

00:45:54.170 --> 00:45:56.210
was positive feedback.

00:45:56.210 --> 00:45:59.330
And we know, positive feedback,
you can give people money.

00:45:59.330 --> 00:46:00.680
That could be a reward.

00:46:00.680 --> 00:46:02.240
But these little
likes on Facebook

00:46:02.240 --> 00:46:03.890
even work so well, right.

00:46:03.890 --> 00:46:05.870
So people go out
of their way just

00:46:05.870 --> 00:46:07.522
for a little kind
of like on Facebook,

00:46:07.522 --> 00:46:08.480
or a positive feedback.

00:46:08.480 --> 00:46:11.190
So those are actually
quite effective.

00:46:11.190 --> 00:46:12.830
Let me just make this
last point, which

00:46:12.830 --> 00:46:16.707
is a really important
thing for all of this

00:46:16.707 --> 00:46:19.290
is to consider the mental state
of the person in front of you.

00:46:19.290 --> 00:46:22.010
I already told you that things
are different under depression,

00:46:22.010 --> 00:46:24.050
for example.

00:46:24.050 --> 00:46:26.810
But they're also different
under different mental states

00:46:26.810 --> 00:46:28.520
like stress.

00:46:28.520 --> 00:46:31.850
So what we find is that
under stress the brain

00:46:31.850 --> 00:46:33.440
function changes
a lot, and the way

00:46:33.440 --> 00:46:37.400
that we process information
changes really, really fast.

00:46:37.400 --> 00:46:40.370
And people become hyper
vigilant to negative information

00:46:40.370 --> 00:46:41.070
around them.

00:46:41.070 --> 00:46:44.630
So, for example, in one study
we brought people into our lab,

00:46:44.630 --> 00:46:48.860
and we told them, I'm
going to give you a task,

00:46:48.860 --> 00:46:50.960
and then I'll give
you a surprise topic.

00:46:50.960 --> 00:46:54.200
And you'd have to give a talk
about this surprise topic

00:46:54.200 --> 00:46:55.500
in front of everyone.

00:46:55.500 --> 00:46:56.690
We're going to judge you.

00:46:56.690 --> 00:46:57.530
We're going to record you.

00:46:57.530 --> 00:46:58.904
We're going to
put it on YouTube.

00:46:58.904 --> 00:47:02.720
So basically what I'm doing
today, but I prepared.

00:47:02.720 --> 00:47:04.897
And people got really stressed.

00:47:04.897 --> 00:47:06.230
We made sure they were stressed.

00:47:06.230 --> 00:47:08.660
We looked at their
cortisol and their saliva.

00:47:08.660 --> 00:47:10.110
We looked at the
skin conductance.

00:47:10.110 --> 00:47:11.870
So when you stress you
start sweating and your skin

00:47:11.870 --> 00:47:12.710
conductance goes up.

00:47:12.710 --> 00:47:14.043
We asked them, are you stressed?

00:47:14.043 --> 00:47:15.242
Yes, they were stressed.

00:47:15.242 --> 00:47:16.700
And then we did
the same experiment

00:47:16.700 --> 00:47:21.110
that I showed you earlier where
we give you information that

00:47:21.110 --> 00:47:23.786
could be unexpectedly
good or unexpectedly bad,

00:47:23.786 --> 00:47:25.910
and we see whether you
incorporate that information

00:47:25.910 --> 00:47:27.506
into your beliefs,
like, oh, you're

00:47:27.506 --> 00:47:28.880
more likely to
get cancer, you're

00:47:28.880 --> 00:47:31.700
less likely to be a victim
of card fraud, and so on.

00:47:31.700 --> 00:47:35.060
What we found was that
under stress immediately

00:47:35.060 --> 00:47:37.460
people became more
likely to incorporate

00:47:37.460 --> 00:47:39.290
negative information
into their beliefs

00:47:39.290 --> 00:47:41.690
than they were just
a few seconds ago.

00:47:41.690 --> 00:47:44.300
And then at that point
there wasn't a bias, what

00:47:44.300 --> 00:47:45.830
we call a desirability bias.

00:47:45.830 --> 00:47:46.760
It went away.

00:47:46.760 --> 00:47:48.860
They were balanced
in how they took

00:47:48.860 --> 00:47:50.720
in the good news
and the bad news.

00:47:50.720 --> 00:47:52.640
We did the same experiment
with firefighters

00:47:52.640 --> 00:47:53.999
in the state of Colorado.

00:47:53.999 --> 00:47:55.790
So the interesting
thing about firefighters

00:47:55.790 --> 00:47:58.300
is that their day
can be quite varied.

00:47:58.300 --> 00:47:59.900
So some days they're relaxed.

00:47:59.900 --> 00:48:01.984
They're in the station.

00:48:01.984 --> 00:48:02.900
They're just relaxing.

00:48:02.900 --> 00:48:06.890
And some other days are really
life threatening events.

00:48:06.890 --> 00:48:09.710
And they did our
studies in the station.

00:48:09.710 --> 00:48:10.880
And they had different days.

00:48:10.880 --> 00:48:11.510
Some are stressed.

00:48:11.510 --> 00:48:12.120
Some are not.

00:48:12.120 --> 00:48:14.600
And what we found
was on stressful days

00:48:14.600 --> 00:48:16.197
when they were
stressed out they were

00:48:16.197 --> 00:48:17.780
more likely to take
in the information

00:48:17.780 --> 00:48:20.750
that we gave them that was
negative than on relaxed days.

00:48:20.750 --> 00:48:23.510
Although that information had
nothing to do with their job.

00:48:23.510 --> 00:48:26.900
It's this general enhancement
of taking in bad news

00:48:26.900 --> 00:48:28.140
that stress causes.

00:48:28.140 --> 00:48:30.130
It's not a specific.

00:48:30.130 --> 00:48:34.910
So you can see how in response
to stressful public events,

00:48:34.910 --> 00:48:38.240
like market collapse,
or terrorist attacks,

00:48:38.240 --> 00:48:42.020
or natural disaster,
a lot of the stuff

00:48:42.020 --> 00:48:45.800
that we had recently causes
people to be stressed.

00:48:45.800 --> 00:48:49.520
Even if the event is halfway
around the world people often,

00:48:49.520 --> 00:48:51.260
their stress levels go up.

00:48:51.260 --> 00:48:53.090
And then what happens
is that people start

00:48:53.090 --> 00:48:55.490
getting hyper-vigilant
to negative information

00:48:55.490 --> 00:48:58.010
in the media, are more
likely to take it in.

00:48:58.010 --> 00:49:00.680
That makes them more
pessimistic, sometimes overly

00:49:00.680 --> 00:49:01.750
pessimistic.

00:49:01.750 --> 00:49:04.540
And that could cause
suboptimal decisions.

00:49:04.540 --> 00:49:06.730
So, for example,
after market collapse

00:49:06.730 --> 00:49:10.030
we often find that people
decide to sell stocks

00:49:10.030 --> 00:49:12.604
where really the best
thing to do is hold on.

00:49:12.604 --> 00:49:14.020
Or after a terrorist
attack people

00:49:14.020 --> 00:49:17.271
cancel holidays or decide not to
go on a plane, but go in a car

00:49:17.271 --> 00:49:17.770
instead.

00:49:17.770 --> 00:49:20.910
Again, suboptimal decisions.

00:49:20.910 --> 00:49:23.100
And interestingly,
it changes with age.

00:49:23.100 --> 00:49:26.810
So stress changes in a very
predictable manner with age.

00:49:26.810 --> 00:49:29.680
So stress is quite low
in kids and teenagers.

00:49:29.680 --> 00:49:33.400
It goes up, up, up, reaching
peak in your midlife,

00:49:33.400 --> 00:49:35.770
and then starts
going down again.

00:49:35.770 --> 00:49:37.730
And happiness goes
the other way.

00:49:37.730 --> 00:49:41.290
So happiness is quite high
in kids and teenagers,

00:49:41.290 --> 00:49:42.100
down, down, down.

00:49:42.100 --> 00:49:45.190
Reaches the bottom in midlife.

00:49:45.190 --> 00:49:46.927
But then it starts
going up again.

00:49:46.927 --> 00:49:48.260
And that's the good news, right?

00:49:48.260 --> 00:49:51.680
So if you're in the slum
now, there's good news.

00:49:51.680 --> 00:49:53.870
[LAUGHTER]

00:49:53.870 --> 00:49:59.030
And it stays up until the
last couple of years of life.

00:49:59.030 --> 00:50:01.594
So let me just end
with this note,

00:50:01.594 --> 00:50:03.760
which is what happens if
this is the guy that you're

00:50:03.760 --> 00:50:05.557
dealing with?

00:50:05.557 --> 00:50:08.140
And actually what you want to
do is reduce stress and increase

00:50:08.140 --> 00:50:09.490
happiness?

00:50:09.490 --> 00:50:12.220
So one thing to think
about is that our studies

00:50:12.220 --> 00:50:16.720
show that the thing that
matters to happiness the most

00:50:16.720 --> 00:50:19.510
is not necessarily what's
happening to you now,

00:50:19.510 --> 00:50:22.294
but what you expect will
happen to you in the future.

00:50:22.294 --> 00:50:23.710
And I'm actually
going to give you

00:50:23.710 --> 00:50:26.770
an example from a study
conducted at Harvard where

00:50:26.770 --> 00:50:29.890
people were asked a few days
before vacation how happy they

00:50:29.890 --> 00:50:32.370
were, a few days during
vacations how happy they were,

00:50:32.370 --> 00:50:34.870
and a few days after
vacation how happy they were.

00:50:34.870 --> 00:50:37.242
So which day do you think
they were the happiest?

00:50:37.242 --> 00:50:37.950
AUDIENCE: Before.

00:50:37.950 --> 00:50:38.210
TALI SHAROT: Before.

00:50:38.210 --> 00:50:38.440
Right.

00:50:38.440 --> 00:50:39.010
Right.

00:50:39.010 --> 00:50:40.750
So the day they
were still in work,

00:50:40.750 --> 00:50:44.260
sitting in their desk, that was
a day that they were happiest,

00:50:44.260 --> 00:50:46.840
because in their mind they
were already on vacation.

00:50:46.840 --> 00:50:48.920
And in the mind it was perfect.

00:50:48.920 --> 00:50:50.050
Then they went on vacation.

00:50:50.050 --> 00:50:50.841
It was pretty nice.

00:50:50.841 --> 00:50:53.200
But it never was as good
as it was in their mind,

00:50:53.200 --> 00:50:55.420
or at least often enough.

00:50:55.420 --> 00:50:57.760
So what that means
is that an easy way

00:50:57.760 --> 00:51:00.280
to reduce stress and
increase happiness

00:51:00.280 --> 00:51:01.990
is to create
anticipatory events.

00:51:01.990 --> 00:51:05.110
Always have events that
we could look forward to.

00:51:05.110 --> 00:51:06.310
Have that vacation planned.

00:51:06.310 --> 00:51:08.050
Have whatever weekend
activities you

00:51:08.050 --> 00:51:11.080
like planned, both for
yourself, and for the family,

00:51:11.080 --> 00:51:12.470
or for the company.

00:51:12.470 --> 00:51:14.660
So employees have things
to look forward to.

00:51:14.660 --> 00:51:15.160
OK.

00:51:15.160 --> 00:51:20.170
So I think I'm just going to end
here, which is just with kind

00:51:20.170 --> 00:51:22.810
of a general note that
most people are not

00:51:22.810 --> 00:51:25.030
aware of exactly how
their brain works

00:51:25.030 --> 00:51:26.820
and all these
biases that we have.

00:51:26.820 --> 00:51:28.660
We're not aware
every day that we're

00:51:28.660 --> 00:51:30.910
trying to confirm our
beliefs all the time,

00:51:30.910 --> 00:51:32.830
or that stress will
change how you process

00:51:32.830 --> 00:51:34.750
any kind of information.

00:51:34.750 --> 00:51:36.510
But becoming aware of
it means two things.

00:51:36.510 --> 00:51:38.890
A, it means that we might
become more conscious

00:51:38.890 --> 00:51:40.330
of our own beliefs
and decisions,

00:51:40.330 --> 00:51:42.770
why we have them, why we do it.

00:51:42.770 --> 00:51:45.400
And second of all, if we know
how other brains around us

00:51:45.400 --> 00:51:48.760
work, we might be better
able to communicate advice

00:51:48.760 --> 00:51:51.260
and information to others.

00:51:51.260 --> 00:51:52.760
So I think that would be it.

00:51:52.760 --> 00:51:55.210
And we might take a
few questions before.

00:51:55.210 --> 00:51:55.920
SPEAKER: Yeah.

00:51:55.920 --> 00:51:57.211
TALI SHAROT: Thank you so much.

00:51:57.211 --> 00:51:58.990
[CLAPPING]

00:52:01.760 --> 00:52:04.280
SPEAKER: But we can
ask some questions now

00:52:04.280 --> 00:52:07.315
how we could improve decision
making, our own decision

00:52:07.315 --> 00:52:08.440
making, and that of others.

00:52:08.440 --> 00:52:11.750
So if there are any questions
now in the audience,

00:52:11.750 --> 00:52:14.450
please go ahead.

00:52:14.450 --> 00:52:17.090
AUDIENCE: From the experiment
where they should not

00:52:17.090 --> 00:52:23.940
push the button to not lose $1,
if that was more successful--

00:52:23.940 --> 00:52:26.772
but when we put something
on a cigarette pack that

00:52:26.772 --> 00:52:28.730
says they're going to
take something from them,

00:52:28.730 --> 00:52:31.050
it sounded less
successful than saying,

00:52:31.050 --> 00:52:33.050
hey, you can get on the
basketball team instead.

00:52:33.050 --> 00:52:33.470
TALI SHAROT: Right.

00:52:33.470 --> 00:52:33.950
Right.

00:52:33.950 --> 00:52:34.450
Right.

00:52:34.450 --> 00:52:36.270
That's a very, very good point.

00:52:36.270 --> 00:52:41.660
So I think that in
order to quit smoking--

00:52:41.660 --> 00:52:44.210
so those warnings are
not too bad for people

00:52:44.210 --> 00:52:47.360
who never approached smoking.

00:52:47.360 --> 00:52:50.480
The problem with those warnings
are people who already smoke.

00:52:50.480 --> 00:52:53.700
That's where we really
don't see much of an effect.

00:52:53.700 --> 00:52:55.910
And I've thought about
exactly this question.

00:52:55.910 --> 00:52:58.010
And I think that
what is happening

00:52:58.010 --> 00:53:01.650
is that in order to quit
smoking you need to take action.

00:53:01.650 --> 00:53:03.830
So you actually have to
find alternative things

00:53:03.830 --> 00:53:06.230
that you could do,
whether it's gum,

00:53:06.230 --> 00:53:08.630
or you have to find
different routines.

00:53:08.630 --> 00:53:11.820
And it actually does
include some actions.

00:53:11.820 --> 00:53:15.440
So I think that could
be the reason for that.

00:53:15.440 --> 00:53:16.580
But yes.

00:53:16.580 --> 00:53:17.850
Good observation.

00:53:17.850 --> 00:53:18.350
OK.

00:53:18.350 --> 00:53:21.250
Well, thank you
so much everyone.

00:53:21.250 --> 00:53:24.300
[CLAPPING]

