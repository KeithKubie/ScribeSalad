WEBVTT
Kind: captions
Language: en

00:00:00.090 --> 00:00:01.859
Okay Michael so I think with that little

00:00:01.859 --> 00:00:05.450
bit of discussion, I feel like we're done.

00:00:05.450 --> 00:00:08.100
&gt;&gt; Cool! Alright. Well it's been nice talking to you. I hope

00:00:08.100 --> 00:00:11.120
the course went well and, oh you mean just for this lesson?

00:00:11.120 --> 00:00:12.330
&gt;&gt; Yeah just for this lesson.

00:00:12.330 --> 00:00:12.670
&gt;&gt; Alright.

00:00:12.670 --> 00:00:12.850
&gt;&gt; So let's.

00:00:12.850 --> 00:00:13.940
&gt;&gt; Well let's wrap up this lesson then.

00:00:13.940 --> 00:00:15.460
&gt;&gt; Yeah let's see what have we learned?

00:00:15.460 --> 00:00:17.640
So remind me Michael what have we learned?

00:00:17.640 --> 00:00:20.090
&gt;&gt; Well we were talking about instance based learning.

00:00:20.090 --> 00:00:23.330
&gt;&gt; That's true. That's the first thing we learned. You will notice

00:00:23.330 --> 00:00:25.480
by the way, I never actually told you why it was called instance

00:00:25.480 --> 00:00:25.905
based learning.

00:00:25.905 --> 00:00:28.520
&gt;&gt; Charles, why is it called instance based learning?

00:00:28.520 --> 00:00:30.690
&gt;&gt; I don't know but I am willing to guess that it has to do with a

00:00:30.690 --> 00:00:33.520
fact that we look at the exact instances that

00:00:33.520 --> 00:00:35.590
we have and we base our learning on that.

00:00:35.590 --> 00:00:40.030
&gt;&gt; Alright and we brought it up in

00:00:40.030 --> 00:00:43.160
by starting off thinking about eager and lazy learning.

00:00:43.160 --> 00:00:45.950
&gt;&gt; Right. What is the difference, Michael?

00:00:45.950 --> 00:00:49.770
&gt;&gt; I will tell you when I need to tell you.

00:00:49.770 --> 00:00:51.230
&gt;&gt; [Laugh]

00:00:51.230 --> 00:00:52.210
That's exactly right.

00:00:52.210 --> 00:00:55.610
&gt;&gt; So lazy learning is about putting off the work

00:00:55.610 --> 00:00:58.600
until it's actually needed. Eager is about, as soon as

00:00:58.600 --> 00:01:01.750
the problem is posed, solve it. And then, you know,

00:01:01.750 --> 00:01:05.790
if you're lucky, the answer will eventually come in handy.

00:01:05.790 --> 00:01:07.750
&gt;&gt; Exactly right. Okay what else?

00:01:07.750 --> 00:01:11.430
&gt;&gt; So as a concrete example of a lazy

00:01:11.430 --> 00:01:15.617
learner we talked about k-nearest neighbor or k-NN.

00:01:15.617 --> 00:01:17.930
&gt;&gt; kNN. And

00:01:17.930 --> 00:01:20.750
this whole notion of nearest neighbor. Is in

00:01:20.750 --> 00:01:26.390
fact one way of talking about similarity functions.

00:01:26.390 --> 00:01:31.240
&gt;&gt; Right and the similarity functions play a really central role in all this.

00:01:31.240 --> 00:01:33.450
&gt;&gt; Right. Similarity. We talk about it as

00:01:33.450 --> 00:01:35.850
if they were distance functions. But distance is

00:01:35.850 --> 00:01:39.870
just another way of talking about, a similarity.

00:01:39.870 --> 00:01:42.770
So, this is actually keeping. K-nn was a specific,

00:01:42.770 --> 00:01:45.290
algorithm we used, and we talked about various versions of it,

00:01:45.290 --> 00:01:48.050
and, the nearest neighbor part really got us to think a

00:01:48.050 --> 00:01:51.110
little bit about similarity and distance and, and what all that

00:01:51.110 --> 00:01:56.600
means. A really important thing here is, I think, that similarity is

00:01:56.600 --> 00:02:00.395
just another way of capturing domain knowledge. And K in

00:02:00.395 --> 00:02:02.780
k-NN is another way of capturing domain knowledge. And that

00:02:02.780 --> 00:02:05.540
if we saw through the quizes and some of our discussions,

00:02:05.540 --> 00:02:08.479
that this is actually very, very important. That domain knowledge matters.

00:02:10.090 --> 00:02:13.950
Now, we also talked about KNN in the context of both regression and

00:02:13.950 --> 00:02:19.880
classification. I see what you did there, 'Knnowledge', it's got KNN in it.

00:02:19.880 --> 00:02:24.270
&gt;&gt; &gt;Okay, so yeah, classification and regression are different things. But,

00:02:24.270 --> 00:02:26.410
KNN can handle both of them. And, at the end of the

00:02:26.410 --> 00:02:30.710
day, that's all stuck in our notion of similarity, and our

00:02:30.710 --> 00:02:36.410
notion of averaging. Which we kind of took as an overall term,

00:02:36.410 --> 00:02:40.000
for a bunch of different things you might do, which some people find confusing.

00:02:40.000 --> 00:02:40.322
&gt;&gt; [LAUGH].

00:02:40.322 --> 00:02:41.950
&gt;&gt; But I think that other people

00:02:41.950 --> 00:02:43.390
would really kind of understand what we mean.

00:02:43.390 --> 00:02:45.240
&gt;&gt; I'm very tempted to take that out of

00:02:45.240 --> 00:02:47.770
Wikipedia, but that would, that would just be rude.

00:02:47.770 --> 00:02:51.960
&gt;&gt; It would be rude. Anything else? We learned one big thing.

00:02:51.960 --> 00:02:55.570
&gt;&gt; We looked at how to compose different learning algorithms

00:02:55.570 --> 00:02:59.120
together. For example in the context of locally weighted linear regression.

00:02:59.120 --> 00:02:59.268
&gt;&gt; Mm hm.

00:02:59.268 --> 00:03:01.680
&gt;&gt; We used this instance based idea

00:03:01.680 --> 00:03:05.010
along with linear regression to get something

00:03:05.010 --> 00:03:08.620
that was both locally smooth but globally bumpy.

00:03:08.620 --> 00:03:11.320
&gt;&gt; Right. So, I'm going to just say

00:03:11.320 --> 00:03:13.970
locally weighted regression. Where we can do any kind

00:03:13.970 --> 00:03:16.590
of regression we might want to. I was going to

00:03:16.590 --> 00:03:19.180
call that $X. So, stick in your favorite value.

00:03:20.510 --> 00:03:22.280
&gt;&gt; Let's see, what else? Oh, oh, a

00:03:22.280 --> 00:03:24.520
really big thing was Bellman's curse of dimensionality.

00:03:24.520 --> 00:03:25.480
&gt;&gt; Yes.

00:03:25.480 --> 00:03:27.120
&gt;&gt; And

00:03:27.120 --> 00:03:30.210
the idea there was that the more features that you

00:03:30.210 --> 00:03:34.030
include The more data you need to fill up that space.

00:03:34.030 --> 00:03:37.440
&gt;&gt; Yep, It's exponential.

00:03:37.440 --> 00:03:42.230
&gt;&gt; And in fact I even just I decided to go and play with this a little

00:03:42.230 --> 00:03:43.580
bit so that example that you were doing

00:03:43.580 --> 00:03:47.110
before where the y equals x1 squared plus x2

00:03:47.110 --> 00:03:48.450
&gt;&gt; Mm-hm.

00:03:48.450 --> 00:03:52.360
&gt;&gt; When I gave it well as we saw in the example we gave it like ten or 12

00:03:52.360 --> 00:03:56.700
examples and it did really badly. So it continued to do somewhat badly until

00:03:56.700 --> 00:03:59.800
I got to about 100,000 and then it was actually doing [LAUGH] really well.

00:03:59.800 --> 00:04:00.540
&gt;&gt; Hmm.

00:04:00.540 --> 00:04:02.140
&gt;&gt; But that seems like an awful lot of

00:04:02.140 --> 00:04:04.480
examples for what is otherwise a very simple problem.

00:04:04.480 --> 00:04:07.280
&gt;&gt; Right. Well if you think about it, the

00:04:07.280 --> 00:04:09.880
amount of data you have to see to determine

00:04:09.880 --> 00:04:13.680
the relative. Relevance of the two different dimensions is

00:04:13.680 --> 00:04:15.330
quite a bit in that particular kind of function.

00:04:15.330 --> 00:04:16.180
&gt;&gt; Hm.

00:04:16.180 --> 00:04:17.420
&gt;&gt; Yeah. That's a lot

00:04:17.420 --> 00:04:19.815
of space to cover. All possible real

00:04:19.815 --> 00:04:24.010
values [LAUGH] across a potentially infinite space.

00:04:24.010 --> 00:04:26.810
&gt;&gt; Yeah, I guess that's true.

00:04:26.810 --> 00:04:29.290
&gt;&gt; Yeah, so the cursor dimensionality is real and we

00:04:29.290 --> 00:04:32.670
just sort of can't get around it. Although As I mentioned

00:04:32.670 --> 00:04:36.000
earlier we will see in the second part of the

00:04:36.000 --> 00:04:39.250
course ways that people try to get around the curse of dimensionality.

00:04:39.250 --> 00:04:40.300
&gt;&gt; Ahah!

00:04:40.300 --> 00:04:42.120
&gt;&gt; Mm-hm. Okay.

00:04:42.120 --> 00:04:42.560
&gt;&gt; Or at least

00:04:42.560 --> 00:04:43.240
blunt it. Right?

00:04:43.240 --> 00:04:44.590
&gt;&gt; Yeah or at least blunt it because you can't

00:04:44.590 --> 00:04:47.820
actually get around the curse of dimensionality, you can only deal with it.

00:04:47.820 --> 00:04:50.870
&gt;&gt; There is no free lunch.

00:04:50.870 --> 00:04:52.570
&gt;&gt; There is no free lunch. In fact, that's a theorem.

00:04:52.570 --> 00:04:52.730
&gt;&gt; [LAUGH]

00:04:52.730 --> 00:04:53.850
&gt;&gt; Isn't that a theorem?

00:04:53.850 --> 00:04:54.480
&gt;&gt; Yeah, I think so.

00:04:54.480 --> 00:04:55.480
&gt;&gt; What's the theorem?

00:04:55.480 --> 00:05:01.650
&gt;&gt; No free lunch. That any learning algorithm that you create

00:05:01.650 --> 00:05:07.530
is going to have the property; that if you average over all possible instances,

00:05:07.530 --> 00:05:09.080
it's not doing any different than random.

00:05:09.080 --> 00:05:11.550
&gt;&gt; Right. And, and another way of thinking about

00:05:11.550 --> 00:05:13.480
that, a practical way of thinking about that is;

00:05:13.480 --> 00:05:15.070
if I don't know anything about the data that

00:05:15.070 --> 00:05:17.620
I'm going to have to learn over. Then, it doesn't

00:05:17.620 --> 00:05:20.050
really matter what I do because there's all possible

00:05:20.050 --> 00:05:22.320
kind of data sets. However, if I have domain

00:05:22.320 --> 00:05:26.030
knowledge, I can use that to choose the best

00:05:26.030 --> 00:05:28.690
learning algorithm for the problems that I'm going to encounter.

00:05:29.700 --> 00:05:32.820
&gt;&gt; So does that mean that, that all of machine learning really comes down to,

00:05:32.820 --> 00:05:34.790
you have to already know what you need to solve

00:05:34.790 --> 00:05:37.910
the problem to apply these. Techniques to solve the problem?

00:05:37.910 --> 00:05:39.930
&gt;&gt; No, but you have to know a little bit

00:05:39.930 --> 00:05:42.410
about your problem in order to decide what to do.

00:05:42.410 --> 00:05:44.020
And in fact, you could make the argument that this

00:05:44.020 --> 00:05:47.530
entire class is about exposing the students to a wide

00:05:47.530 --> 00:05:50.560
range of techniques and giving them enough practice so that

00:05:50.560 --> 00:05:52.740
they can do a pretty good job of telling, given

00:05:52.740 --> 00:05:54.890
a problem. Would it be better to use this kind

00:05:54.890 --> 00:05:57.900
of technique or this kind of technique? Is K-nn a sort of

00:05:57.900 --> 00:06:00.340
better way of approaching it? Decision tree a better way

00:06:00.340 --> 00:06:02.740
of approaching it? Um,It's a lot of what this class is

00:06:02.740 --> 00:06:05.840
about. Is helping them to get enough domain knowledge or enough

00:06:05.840 --> 00:06:08.510
knowledge anyways so that they can apply it to particular domains.

00:06:08.510 --> 00:06:11.730
&gt;&gt; Cool, so alright, that seems like a plenty useful lesson.

00:06:11.730 --> 00:06:14.770
&gt;&gt; Yes, it's a very hopeful note to end on, so let's end on that.

00:06:14.770 --> 00:06:16.650
&gt;&gt; Alright, see you next time.

00:06:16.650 --> 00:06:17.300
&gt;&gt; Alright, bye Michael.

