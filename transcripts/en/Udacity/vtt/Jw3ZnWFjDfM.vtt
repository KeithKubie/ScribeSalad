WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:02.570
Now, I actually snuck something important in here, I

00:00:02.570 --> 00:00:05.250
actually snuck two things that are important in here.

00:00:05.250 --> 00:00:08.250
The first is, the, what's called the Markovian property.

00:00:08.250 --> 00:00:10.070
Do you remember what the Markovian property is Michael?

00:00:10.070 --> 00:00:11.380
&gt;&gt; From what?

00:00:11.380 --> 00:00:15.390
&gt;&gt; From, I dunno. Actually, where does the Markovian property come from?

00:00:15.390 --> 00:00:16.740
&gt;&gt; I'm going to say Russia.

00:00:16.740 --> 00:00:19.330
&gt;&gt; Okay, yeah, from the, from the Russian.

00:00:19.330 --> 00:00:23.990
&gt;&gt; Yeah, so I have Russian ancestors, and they passed onto me this idea that.

00:00:25.210 --> 00:00:28.980
Markov means that you don't have to

00:00:28.980 --> 00:00:31.520
condition on anything past the most recent state.

00:00:31.520 --> 00:00:32.670
&gt;&gt; That's exactly right.

00:00:32.670 --> 00:00:36.290
&gt;&gt; The Markovian property is that only the present matters.

00:00:36.290 --> 00:00:39.100
&gt;&gt; And they had to pass that down to me you

00:00:39.100 --> 00:00:44.180
know, one generation at a time, because you know, Markov property.

00:00:44.180 --> 00:00:46.960
&gt;&gt; Exactly right, that was very good Michael. So what does this

00:00:46.960 --> 00:00:48.180
mean? What this means is our

00:00:48.180 --> 00:00:50.790
transition function, which shows you the probability

00:00:50.790 --> 00:00:53.570
you end up in some state as prime, given that you're in state

00:00:53.570 --> 00:00:59.400
S and took action A, only depends upon the current state S. If it

00:00:59.400 --> 00:01:03.050
also depended upon where you were 20 minutes before then, you would have

00:01:03.050 --> 00:01:06.930
to have more S's here. And then you would be violating the Markovian property.

00:01:06.930 --> 00:01:10.480
&gt;&gt; So is this like, do historians hate this?

00:01:10.480 --> 00:01:13.080
&gt;&gt; Well, you know, one never learns anything from history.

00:01:13.080 --> 00:01:16.860
&gt;&gt; No, you're supposed to learn from history, or you're doomed to,

00:01:16.860 --> 00:01:20.150
I don't know, let me make something up, repeat it.

00:01:20.150 --> 00:01:21.970
&gt;&gt; [LAUGH] Fair enough. Historians probably don't like

00:01:21.970 --> 00:01:24.130
this, but there is a way for mathematicians to

00:01:24.130 --> 00:01:25.880
convince them that they're okay with it. And

00:01:25.880 --> 00:01:27.960
the way that, you, mathematicians convince you that you're

00:01:27.960 --> 00:01:29.390
okay with this is to point out that

00:01:29.390 --> 00:01:33.420
you can turn almost anything, into a Markovian Process

00:01:33.420 --> 00:01:35.670
by simply making certain that your current state

00:01:35.670 --> 00:01:38.300
remembers everything you need to remember from the past.

00:01:38.300 --> 00:01:39.150
&gt;&gt; I see.

00:01:39.150 --> 00:01:41.940
&gt;&gt; So, in general, even if something isn't

00:01:41.940 --> 00:01:44.190
really Markovian, you need to know what you were, not only what

00:01:44.190 --> 00:01:46.900
you're doing now, but what you were doing five minutes ago. You could

00:01:46.900 --> 00:01:49.710
just turn your current state into what you're doing now and what you

00:01:49.710 --> 00:01:52.530
were doing five minutes ago. The obvious problem with that, of course, is

00:01:52.530 --> 00:01:55.580
that if you have to remember everything from the beginning of time.

00:01:55.580 --> 00:01:58.740
You're only going to see every state once and it's going to be very difficult

00:01:58.740 --> 00:02:03.260
to learn anything. But, that Markovian property turns out to be, turns out

00:02:03.260 --> 00:02:06.960
to be important and it actually allows us to solve these problems in

00:02:06.960 --> 00:02:09.660
a tractable way. But I snuck in something else, Michael.

00:02:09.660 --> 00:02:11.810
What I snuck in is this idea about the transition

00:02:11.810 --> 00:02:15.570
model, is that nothing ever changes. So this second property

00:02:15.570 --> 00:02:17.570
that matters for Markov decision processes, at least for the

00:02:17.570 --> 00:02:19.460
sets of things that we're going to be talking about in

00:02:19.460 --> 00:02:23.200
the beginning, is that things are stationary. That is these

00:02:23.200 --> 00:02:25.320
for the purposes of this discussion, it means that these

00:02:25.320 --> 00:02:30.480
rules don't change. Over time. That's one notion of stationary, okay?

00:02:30.480 --> 00:02:32.080
&gt;&gt; Does that mean that the agent can never leave

00:02:32.080 --> 00:02:33.620
the start state?

00:02:33.620 --> 00:02:35.080
&gt;&gt; No, the agent can leave the start state any

00:02:35.080 --> 00:02:37.650
time it takes the action, then it gets another start state.

00:02:37.650 --> 00:02:38.980
&gt;&gt; Then how is it, how is it stationary, then?

00:02:38.980 --> 00:02:42.480
&gt;&gt; It's not stationary, the world is stationary. The, the transition

00:02:42.480 --> 00:02:45.570
model is stationary, the physics are stationary, the rules don't change any.

00:02:45.570 --> 00:02:46.770
&gt;&gt; The rules don't change.

00:02:46.770 --> 00:02:47.190
&gt;&gt; Right.

00:02:47.190 --> 00:02:48.660
&gt;&gt; I, I, okay. I see.

00:02:48.660 --> 00:02:50.130
&gt;&gt; Then there's another notion of stationary

00:02:50.130 --> 00:02:51.169
that we'll see a little bit later.

00:02:52.500 --> 00:02:54.410
Okay, last thing to point out about

00:02:54.410 --> 00:02:56.980
the definition of a mark on decision process,

00:02:56.980 --> 00:03:02.310
is the notion of reward. So, reward is simply a

00:03:02.310 --> 00:03:05.940
scalr value, that you get for being in a state. So

00:03:05.940 --> 00:03:09.140
for example we might say, you know, this green goal

00:03:09.140 --> 00:03:11.670
is a really good goal. And so, if you get there,

00:03:11.670 --> 00:03:13.650
we're going to give you a dollar. This red dual,

00:03:13.650 --> 00:03:15.570
on the other hand, is a very, very bad state. We

00:03:15.570 --> 00:03:17.330
don't want you to end there. And so, if you end

00:03:17.330 --> 00:03:19.160
up there we're going to take away a dollar from you.

00:03:19.160 --> 00:03:20.490
&gt;&gt; What if I don't have a dollar?

00:03:20.490 --> 00:03:22.010
&gt;&gt; Someone will give you the dollar.

00:03:22.010 --> 00:03:23.140
The universe will give you the dollar.

00:03:23.140 --> 00:03:25.000
&gt;&gt; [LAUGH] And then take it away?

00:03:25.000 --> 00:03:27.580
&gt;&gt; Or, the universe will take it away. Even

00:03:27.580 --> 00:03:30.080
if you don't have it. You'll have negative dollars.

00:03:30.080 --> 00:03:30.510
&gt;&gt; Oh, man.

00:03:30.510 --> 00:03:33.450
&gt;&gt; Okay, so Reward is very important here in a of couple

00:03:33.450 --> 00:03:38.160
ways. Reward is one of the things that, as I'm always harping on

00:03:38.160 --> 00:03:41.950
encompasses our domain knowledge. So, the Rewards you get from the state tells

00:03:41.950 --> 00:03:47.020
you the usefulness of entering into that state. Now. I wrote out three

00:03:47.020 --> 00:03:50.030
different definitions of r here, because sometimes it's very useful to think

00:03:50.030 --> 00:03:52.460
about them differently. I've been talking about the reward you get for entering

00:03:52.460 --> 00:03:55.610
into the state, but there's also a notion of reward that you get

00:03:55.610 --> 00:03:59.210
for entering into a state and taking an action. There's a rewar, or,

00:03:59.210 --> 00:04:02.040
being in a state and taking an action, there's a reward that you

00:04:02.040 --> 00:04:05.220
could get for being in a state, taking an action, and then ending

00:04:05.220 --> 00:04:08.610
up in another state as prime. It turns out these are all mathematically

00:04:08.610 --> 00:04:12.390
equivalent. But often it's easier to think about one form or the other.

00:04:12.390 --> 00:04:16.760
But for the purposes of the, you know, for the rest of this discussion,

00:04:16.760 --> 00:04:19.860
really you can just focus on that one, the reward of the value entering

00:04:19.860 --> 00:04:21.959
into a state. And those four things,

00:04:21.959 --> 00:04:24.340
by themselves, along with this Markov property

00:04:24.340 --> 00:04:27.010
and non-stationarity, defines what's called the Markov

00:04:27.010 --> 00:04:29.830
Decision Process. Or an MDP. Got it?

00:04:29.830 --> 00:04:34.120
&gt;&gt; I'm a little stuck on the, how those could be mathematically equivalent.

00:04:34.120 --> 00:04:37.140
&gt;&gt; Well we'll get to that later. Would you like a little bit of intuition?

00:04:37.140 --> 00:04:38.030
&gt;&gt; Sure.

00:04:38.030 --> 00:04:40.400
&gt;&gt; Well, you can imagine that just as

00:04:40.400 --> 00:04:42.530
before we were dealing with the the notion

00:04:42.530 --> 00:04:45.730
of making a non-Markovian thing Markovian by putting

00:04:45.730 --> 00:04:47.240
a little bit of history into your state.

00:04:47.240 --> 00:04:50.810
You can always fold in the action that you took to be in a state or

00:04:50.810 --> 00:04:53.770
the action that you took to get to a state, as a part of your state.

00:04:53.770 --> 00:04:57.290
&gt;&gt; But that would be a different Markov Decision Process.

00:04:57.290 --> 00:05:00.020
&gt;&gt; It would, but they would work out to have the same solution.

00:05:00.020 --> 00:05:01.100
&gt;&gt; Oh, I see.

