WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:02.890
Let's look at one way to
learn those embeddings.

00:00:02.890 --> 00:00:04.750
It's called Word2Vec.

00:00:04.750 --> 00:00:09.150
Word2Vec is a surprisingly simple
model that works very well.

00:00:09.150 --> 00:00:12.230
Imagine you had a corpus of
text with a sentence say,

00:00:12.230 --> 00:00:15.320
the quick brown fox
jumps over the lazy dog.

00:00:15.320 --> 00:00:19.870
For each word in this sentence,
we're going to map it to an embedding.

00:00:19.870 --> 00:00:21.415
Initially a random one.

00:00:21.415 --> 00:00:24.187
And then we're going to use
that embedding to try and

00:00:24.187 --> 00:00:25.954
predict the context of the word.

00:00:25.954 --> 00:00:30.090
In this model, the context is
simply the words that are nearby.

00:00:30.090 --> 00:00:33.590
Pick a random word in a window
around the original word,

00:00:33.590 --> 00:00:35.930
and that's your target.

00:00:35.930 --> 00:00:39.690
Then train your model exactly as
if it were a supervised problem.

00:00:39.690 --> 00:00:42.930
The model you're going to use
to predict this nearby word

00:00:42.930 --> 00:00:44.760
is a simple logistic regression.

00:00:44.760 --> 00:00:47.260
Nothing deep about it,
just a simple linear model.

