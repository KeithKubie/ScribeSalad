WEBVTT
Kind: captions
Language: en

00:00:07.060 --> 00:00:12.299
&gt;&gt; KOCHER: All right. Hello, everyone. Welcome
to App Engine Nitty Gritty. We put up one

00:00:12.299 --> 00:00:17.289
of those Google moderator things, so if you
want to jump over there, a tiny world called

00:00:17.289 --> 00:00:20.570
App Engine Nitty Gritty. Welcome to submit
questions, we'll take up from the mics as

00:00:20.570 --> 00:00:27.560
well. As people are trickling in here, probably,
people are grabbing food, just going to tell

00:00:27.560 --> 00:00:33.079
you a little bit about that what we're covering
today. One is that for folks who went to Brett's

00:00:33.079 --> 00:00:38.200
session yesterday afternoon, he had a session
on scaling Apps and App Engine. There's going

00:00:38.200 --> 00:00:44.930
to be no overlap at all with that one. We'll
be covering you know, also things about scalability

00:00:44.930 --> 00:00:49.920
but, none of the same things. We definitely
recommend his talk though if you didn't catch

00:00:49.920 --> 00:00:55.120
it. So, we're going to talk about some things
that are unique about building an App Engine.

00:00:55.120 --> 00:00:59.400
How you can, you know, when you're on a system
where you expect to have some errors, how

00:00:59.400 --> 00:01:03.920
you can build something that's stable and
reliable on top of that? Knowing that, you

00:01:03.920 --> 00:01:09.240
know, some of those parts can and will at
times fail. We'll also talk about some of

00:01:09.240 --> 00:01:13.850
the lessons we learned and recommendations
about how to make your upscale well, and we'll

00:01:13.850 --> 00:01:19.710
also talk about integrating external services.
In our case, that was Amazon EC2. We'll talk

00:01:19.710 --> 00:01:22.859
about why we did that, why you might want
to do that, it could be a different service,

00:01:22.859 --> 00:01:28.740
but just looking at how you might have things
that don't fit on to App Engine that, you

00:01:28.740 --> 00:01:36.359
know, are important to your App. We are from
FrontSeat.org which is a civic software company

00:01:36.359 --> 00:01:41.939
up in Seattle. And, we were founded around
the idea that software is getting cheaper

00:01:41.939 --> 00:01:47.100
and cheaper to build, and because of that
we can start applying software to, you know,

00:01:47.100 --> 00:01:50.819
civic issues, things where in the past, it
might have been prohibitably expensive but

00:01:50.819 --> 00:01:56.789
with the falling costs, there really are some
new opportunities out there. I'm Jesse. This

00:01:56.789 --> 00:02:01.740
is Dave and Josh, and I'm the lead developer
at Front Seat. These guys each run their own

00:02:01.740 --> 00:02:05.389
consulting companies and have been working
with us for a long time, and they've been

00:02:05.389 --> 00:02:11.009
very involved in all of our work with App
Engine. I'm going to give you a couple of

00:02:11.009 --> 00:02:15.430
examples of the kind of things I'm talking
about civic software before we dive in. This

00:02:15.430 --> 00:02:21.569
is the site we did just after the election
last fall where Obama had said, "I want to

00:02:21.569 --> 00:02:26.260
appoint a chief technical officer for the
country," but had no way of finding what that

00:02:26.260 --> 00:02:30.530
role was and so we put up a site. It was really
quickly, it took us about a day to launch

00:02:30.530 --> 00:02:36.230
this and let people submit, discuss and vote
on ideas for what that position should be

00:02:36.230 --> 00:02:42.790
about, what the priorities for that position
should be, so you can see the top result here

00:02:42.790 --> 00:02:49.569
was about a combination of neutrality and
accessibility. Earlier, last summer, we did

00:02:49.569 --> 00:02:54.659
a site where students who went to school in
a state other than their home state can go

00:02:54.659 --> 00:02:58.900
and say, "You know, I'm from this state. I'm
at school on this other state," and the website

00:02:58.900 --> 00:03:02.650
we tell them where your presidential vote
will count more and then it would help them

00:03:02.650 --> 00:03:08.260
register to vote and get an absentee ballot.
So we have a whole bunch of projects. I'm

00:03:08.260 --> 00:03:11.560
going to skip over with these other ones if
you want to know more about the kind of stuff

00:03:11.560 --> 00:03:16.069
we're doing. You can check out frontseat.org,
it links to all these things; everything from

00:03:16.069 --> 00:03:22.260
utility bill designs that promote conservation
to satire about the payday loan industry.

00:03:22.260 --> 00:03:30.840
Projects we're going to talk about today is
called Walk Score, and before we dive into

00:03:30.840 --> 00:03:35.150
that, you know, the context of all of these
is that, we're a very small team, we have

00:03:35.150 --> 00:03:39.269
a lot of projects and we need to minimize
our overhead and our maintenance, and that's

00:03:39.269 --> 00:03:45.409
the, the context for our use of App Engine.
So we wanted to bring a Civic Software service

00:03:45.409 --> 00:03:51.450
to scale on a budget with a very small staff,
you know, no system staff to sit around and

00:03:51.450 --> 00:04:00.930
monitor it all the time. So, on to Walk Score--Walk
Score is a website that lets you measure the

00:04:00.930 --> 00:04:05.510
walkability of any neighborhood in terms of
access to amenities. So, it's, you know, how

00:04:05.510 --> 00:04:12.599
much can you do without getting in a car,
and it's--I'm going to give you a demo really

00:04:12.599 --> 00:04:21.760
quickly of the original Walk Score website.
It started out with Google Maps mesh ups.

00:04:21.760 --> 00:04:25.530
I'm going to search here, in search for any
address. I'm going to search for the Mascony

00:04:25.530 --> 00:04:33.550
Center address. So you can see, if our network
cooperates, my mother also seems to be a little

00:04:33.550 --> 00:04:40.210
bit confused about what size it is so, sorry
that is off-center here. So, as these searches

00:04:40.210 --> 00:04:44.080
complete, it pulls up a bunch of amenities
in the neighborhood and it calculates the

00:04:44.080 --> 00:04:48.030
score, up here at the top, from zero to a
hundred of how walkable that neighborhood

00:04:48.030 --> 00:04:51.500
is. So, there's a ton of stuff around here.
You can do a lot of things about getting in

00:04:51.500 --> 00:04:55.680
a car, so you get a very high score, 98 out
of 100. And on this side here you can see

00:04:55.680 --> 00:05:03.080
things like the restaurants, the coffee shops,
got parks and schools, all these different

00:05:03.080 --> 00:05:12.360
amenities. And, we've got integration with
Yelp. So if you look at the Verba Buena, it'll

00:05:12.360 --> 00:05:18.560
search for reviews and pops up a little picture
there, you can click through the reviews.

00:05:18.560 --> 00:05:23.919
And we also have street views so, maybe you
want to go coffee and see, you know, what's

00:05:23.919 --> 00:05:34.710
the closest thing that's not Starbucks. Jump
over here, and I think that's it. Yeah, there's

00:05:34.710 --> 00:05:43.419
that coffee shop. So, this Walk Score indicates,
you know, how much you can do without getting

00:05:43.419 --> 00:05:48.520
in a car, and it's also an indicator of the
vibrancy of the neighborhood. And, this part

00:05:48.520 --> 00:05:51.550
that I just showed you, this is great for
checking out a specific address, but what

00:05:51.550 --> 00:05:55.889
if you want to get to know a city at a larger
scale, we did some work where we've generated

00:05:55.889 --> 00:06:01.130
these heat maps that can show you in green
the most walkable areas and fading out to

00:06:01.130 --> 00:06:06.270
red, the least walkable. So this is the map
for Seattle, you can see, those who know the

00:06:06.270 --> 00:06:11.599
city a little bit, downtown, Capitol Hill,
university district, and you have these little

00:06:11.599 --> 00:06:15.389
pockets, these little neighborhoods that are
down Columbia City and West Seattle where

00:06:15.389 --> 00:06:19.669
you have some kind of little neighborhood.
And this is a great way to get a quick insight

00:06:19.669 --> 00:06:23.479
into the shape of the city in terms of, you
know, where people actually hang out on the

00:06:23.479 --> 00:06:29.509
streets and where the street life is. So why
are we doing all these work about walkability?

00:06:29.509 --> 00:06:35.330
Well, it turns out that walkability encapsulates
a whole bunch of great things into one concept;

00:06:35.330 --> 00:06:45.940
it's very easy for people to understand.

00:06:45.940 --> 00:06:50.050
So, here are those benefits very quickly:
climate when people walk more, they drive

00:06:50.050 --> 00:06:55.680
less, they emit less greenhouse gases. Health,
people weigh about seven pounds on average--seven

00:06:55.680 --> 00:07:01.039
pounds less on average in walkable neighborhoods.
These neighborhoods tend to have strong social

00:07:01.039 --> 00:07:06.690
capital, good transit options, very few auto-related
deaths. Also, a lot of economic benefits,

00:07:06.690 --> 00:07:15.389
home values tend to be higher or appreciate
faster or in this climate, fall more slowly.

00:07:15.389 --> 00:07:19.789
And, transportation costs are a big part,
about 18% of household income, much less in

00:07:19.789 --> 00:07:23.440
walkable neighborhoods. And they tend to have
very strong local economies where local businesses

00:07:23.440 --> 00:07:28.490
can thrive. It's also something that people
rank very highly in terms of choosing where

00:07:28.490 --> 00:07:32.770
to live, they say walkability or access to
amenities is one of the top two things higher

00:07:32.770 --> 00:07:38.740
than property taxes or schools. So, what we
tried to do is create demand for walkable

00:07:38.740 --> 00:07:43.690
neighborhoods by educating people about the
benefits and then, fill that demand by providing

00:07:43.690 --> 00:07:48.470
transparency about the walkability of every
property. And, we want to, you know, help

00:07:48.470 --> 00:07:55.470
people find these places and also create more
of them. So, walkability matters most in this

00:07:55.470 --> 00:07:59.069
kind of transparency, it matter most when
people are choosing a place to live and that

00:07:59.069 --> 00:08:05.470
means, we need to be on real estate sites.
So, the first thing we did for that is build

00:08:05.470 --> 00:08:10.780
this widget that's very easy to, for people
to embed, and this is basically a miniature

00:08:10.780 --> 00:08:16.310
version of what I showed you before. It's
that rectangle part, it's what can be embedded

00:08:16.310 --> 00:08:20.220
into a real estate site, and we see a lot
of adoption to that. We've got about twice

00:08:20.220 --> 00:08:27.479
as much traffic to that tile as we do to our
website, so it's been really successful. But,

00:08:27.479 --> 00:08:31.080
they often put us down in the kind of "about
the neighborhood section," it's not really

00:08:31.080 --> 00:08:37.930
the ideal placement. So we really want is
this, we want to be right in the primary information

00:08:37.930 --> 00:08:42.979
about a property: three bedrooms, two bathrooms,
walk score: 86, and we want "Search By Walk

00:08:42.979 --> 00:08:47.790
Score." And so to do those things, we need
to build an API that gives scores back to

00:08:47.790 --> 00:08:51.860
a real estate site very quickly, so they can
just pass us latitude, longitude and we give

00:08:51.860 --> 00:08:55.779
them a score, they don't want to wait for
all those local searches to happen and all

00:08:55.779 --> 00:09:02.040
the restaurants to load and all that. So we're
going to jump into the text stuff now, and

00:09:02.040 --> 00:09:06.380
we'll talk a little bit about why we brought
EC2 into the picture, why we didn't just do

00:09:06.380 --> 00:09:11.050
everything that we built for the API on App
Engine? And we'll talk a little bit about

00:09:11.050 --> 00:09:15.260
some guidance on making App Scale and some
issues that we ran into and thoughts about

00:09:15.260 --> 00:09:19.800
working within the App Engine environment.
I'm going to hand it over to Josh to take

00:09:19.800 --> 00:09:24.540
us in.
&gt;&gt; LIVNI: Thanks, Jesse. All right. So, when

00:09:24.540 --> 00:09:31.200
we first--talking about the API, this is pretty
much our ideal workflow for the entire API.

00:09:31.200 --> 00:09:36.170
All we really want to do is have a request
come in, latitude, longitude, location; goes

00:09:36.170 --> 00:09:41.990
to this magic box, spits out a score response,
that's pretty much it. And, of course, things

00:09:41.990 --> 00:09:44.490
get a little bit more complicated than that
but before we get to the complications, we

00:09:44.490 --> 00:09:48.070
had to figure where we're going to host this.
It is pretty straightforward; we could've

00:09:48.070 --> 00:09:53.990
run our own servers, and so when we're considering
where to put it, these were some of the, these

00:09:53.990 --> 00:09:56.930
are unfortunately just the negative considerations
at first glance of App Engine. But some of

00:09:56.930 --> 00:10:01.399
the things we thought about were vendor lock
in, if we write code for App Engine, are we

00:10:01.399 --> 00:10:05.600
going to be able to, is it portable a little
bit because it's similar to Jango but, we're

00:10:05.600 --> 00:10:10.079
going to have to rewrite some things in case
we had to move. The cost when we started this

00:10:10.079 --> 00:10:15.089
and launched it, there was no pricing announced,
so we didn't know what we were in for. We

00:10:15.089 --> 00:10:20.570
assumed here it would be, you know, a competitive
cost to host on App Engine. And it's a data

00:10:20.570 --> 00:10:24.740
product, so we didn't know, is it going to
potentially go down for six, eight hours,

00:10:24.740 --> 00:10:28.930
make us look bad, are we going to have various
other problems. And we decided after all of

00:10:28.930 --> 00:10:33.310
these, you know, that Google engineers probably
are going to keep things up better than we

00:10:33.310 --> 00:10:37.810
would with the series of EC2 or other virtual
machines. Cost would be good and we weren't

00:10:37.810 --> 00:10:43.019
super worried about the portability of the
codes, so we went with App Engine. And the

00:10:43.019 --> 00:10:48.260
next step then is to figure out what I considered
kind of the core functionality, and the core

00:10:48.260 --> 00:10:52.440
functionality is just this, this is really
what we need, the ability to return responses

00:10:52.440 --> 00:10:59.410
really fast. So, the criteria for us is always
on, let's go ahead and separate out this core

00:10:59.410 --> 00:11:01.740
functionality from these other pieces and
I'm going to show you some of these other

00:11:01.740 --> 00:11:06.209
pieces that we considered secondary functionality.
We put the secondary functionality on App

00:11:06.209 --> 00:11:11.320
Engine where it might conflict potentially
or cause a problem or our core functionality

00:11:11.320 --> 00:11:15.149
of returning responses would fail. Should
we put it somewhere else? How do we integrate

00:11:15.149 --> 00:11:20.180
these other pieces? I'm going to spend a little
time just talking about some of these secondary

00:11:20.180 --> 00:11:25.110
pieces that we decided not to put on App Engine
for something that we did. So, App Engine,

00:11:25.110 --> 00:11:28.700
of course, is really, really, really good
for simple operations. A request comes in;

00:11:28.700 --> 00:11:33.380
does something basic such as look at the score,
return it. Not so good for certain other things

00:11:33.380 --> 00:11:37.820
that it's just maybe not designed for. Some
of these things Jesse mentioned such as the

00:11:37.820 --> 00:11:43.639
rankings. We have a fairly complex procedure
to figure out the rankings. It's not just

00:11:43.639 --> 00:11:47.940
counting up points in a polygon from different
neighborhoods and cities. We bring in all

00:11:47.940 --> 00:11:53.690
kinds of demographic data and weight by population
with Walk Score and App Engine is not really

00:11:53.690 --> 00:11:58.889
set up for that kind of geo-capabilities.
Some folks I know have done some back and

00:11:58.889 --> 00:12:04.410
worked with this but we do all of these uploaded
on a post [INDISTINCT] database, and so that's

00:12:04.410 --> 00:12:08.339
offline. The other is, what if we just want
to look at the API usage? Who's using stock

00:12:08.339 --> 00:12:12.870
in a day or a week or a month, and where are
the queries coming in? So this is Seattle

00:12:12.870 --> 00:12:17.440
API usage over a given time, and we want to
know a little bit. Are people coming in and

00:12:17.440 --> 00:12:22.860
maybe doing a kind of a survey or an academic
study where they're going to request a couple

00:12:22.860 --> 00:12:27.529
of hundred thousand points in a specific area?
Is it people just looking at houses all over

00:12:27.529 --> 00:12:33.029
the place and when, and this helps us decide
where to pre-calculate points. We want to

00:12:33.029 --> 00:12:38.649
make sure we can respond really fast to places
so we see the API with some obvious things

00:12:38.649 --> 00:12:43.800
such as all the sort of kind places or cities,
maybe the top thousand cities will score all

00:12:43.800 --> 00:12:48.990
of the possible walk scores there. But where
else do we go next? By looking at the usage,

00:12:48.990 --> 00:12:55.120
we could see things like, oh, people query
for houses mostly within two miles of a city

00:12:55.120 --> 00:12:59.310
center. And also, when we're doing the rankings,
are going to rank over urban areas? Are we

00:12:59.310 --> 00:13:03.699
going to rank over this statistical metropolitan
stuff from the census? So, we did this offline

00:13:03.699 --> 00:13:09.230
and then precede the cache offline with places
of interest. And all of this pre-calculation

00:13:09.230 --> 00:13:14.620
stuff brings in the complication to that really,
really simple workflow which is we have to

00:13:14.620 --> 00:13:20.820
actually calculate the score to give it out.
And, so the API gets a little bit more complicated

00:13:20.820 --> 00:13:26.010
if we don't have a score, we return saying
"We don't have a score" then we have to go

00:13:26.010 --> 00:13:31.339
and do something about that, and to explains
a little bit why we decided to off load the

00:13:31.339 --> 00:13:37.570
scoring portion outside of App Engine, I'll
hand over to Dave.

00:13:37.570 --> 00:13:42.130
&gt;&gt; PECK: Okay, so, Josh showed us a number
of the reasons that we couldn't strictly build

00:13:42.130 --> 00:13:46.759
the Walk Score API on top of App Engine. And
I'm going to talk through sort of the central

00:13:46.759 --> 00:13:51.470
reason that we had to make use of Amazon EC2
and that's that, as Josh hinted that, that

00:13:51.470 --> 00:13:57.079
Walk Score Calculation takes time. Now, calculating
the Walk Score is not CPU intensive but it

00:13:57.079 --> 00:14:03.430
is rather I/O intensive. At a minimum, Walk
Score requires us to make 17 URL fetches to

00:14:03.430 --> 00:14:08.220
Google local search, and potentially talk
to other services, census services, geo-coding,

00:14:08.220 --> 00:14:13.300
et cetera. Now, if you're an individual user
and you go to our website and type in an address,

00:14:13.300 --> 00:14:17.310
your browser and our JavaScript will do most
of the work for you. But if you're a real

00:14:17.310 --> 00:14:22.009
estate company what you really want is programmatic
access and you want us to do the work of calculating

00:14:22.009 --> 00:14:27.019
Walk Score on your behalf and so that's why
we built this API. From a customer's perspective,

00:14:27.019 --> 00:14:31.540
the request response cycle as Josh mentioned
is, give us the latitude and longitude, get

00:14:31.540 --> 00:14:36.430
back a score if we've already calculated it,
otherwise, queue it up. And we've actually

00:14:36.430 --> 00:14:42.870
built a reliable message queue abstraction
on top of the App Engine data store, to hold

00:14:42.870 --> 00:14:46.459
on to the latitudes and longitudes that we
haven't yet calculated. Of course, at some

00:14:46.459 --> 00:14:50.089
point, we have to turn around and actually
service that queue. And so, we looked at a

00:14:50.089 --> 00:14:54.980
few options when we we're building our API.
The first option we looked at was App Engine

00:14:54.980 --> 00:15:01.130
Cron Jobs, and as you know, App Engine Cron
jobs allow you to regularly ping an App Engine

00:15:01.130 --> 00:15:07.220
URL handler. That URL handler is subject to
many of the same restrictions that standard

00:15:07.220 --> 00:15:13.220
URL handlers are subject to, on App Engine.
So, in our case making 17 URL fetches is probably

00:15:13.220 --> 00:15:19.329
not going to happen in the timeframe that
App Engine request response are allowed. So,

00:15:19.329 --> 00:15:23.829
unfortunately, Cron jobs didn't look like
a particularly good answer. And, of course,

00:15:23.829 --> 00:15:27.899
the second thing we looked at is whether App
Engine supported background tasks, and obviously,

00:15:27.899 --> 00:15:33.149
if you were at Brett's talk earlier, you will
know that the new task queue API is going

00:15:33.149 --> 00:15:38.610
online in a few weeks and we're very excited
about that. We believe that we can move a

00:15:38.610 --> 00:15:44.050
lot of our functionality of our Walk Score
calculation on to App Engine. But, six months

00:15:44.050 --> 00:15:49.880
ago, when we started building this API, background
tasks weren't available on App Engine, so

00:15:49.880 --> 00:15:55.790
it wasn't really an option for us. So, where
do we turn to? Well, we turn to Amazon EC2.

00:15:55.790 --> 00:16:00.199
For those who don't know EC2, it's Amazon's
API to spin up virtual servers, you have full

00:16:00.199 --> 00:16:04.959
control of the machine, you can choose whatever
operating system image you want, and for us

00:16:04.959 --> 00:16:09.269
the things that it gave us was a place to
do background processing, they build and they

00:16:09.269 --> 00:16:14.470
did an arbitrary number of URL fetches, of
course, just arbitrary I/O's since you are

00:16:14.470 --> 00:16:19.170
on the machine. And, the design of our calculator
is very parallel. We have lots of processes

00:16:19.170 --> 00:16:23.009
running, working on different latitudes and
longitudes making different network requests,

00:16:23.009 --> 00:16:27.610
we needed a place to run lots of processes
in parallel. So, in other words the Walk Score

00:16:27.610 --> 00:16:32.519
API is built with both App Engine and EC2,
and when you start to build the service with

00:16:32.519 --> 00:16:37.850
multiple cloud computing environments, there
are a few important considerations to keep

00:16:37.850 --> 00:16:42.839
in mind. So, I just wanted to show this perspective.
Here, we've got the customers code on the

00:16:42.839 --> 00:16:48.649
left, our App Engine code in the middle and
our Amazon EC2 code on the right. And here's

00:16:48.649 --> 00:16:52.529
a customer request where we've already calculated
the score, they give us the latitude and longitude,

00:16:52.529 --> 00:16:58.259
we checked to see that it is already calculated.
We package up a response and send it back

00:16:58.259 --> 00:17:01.399
to them, and here's the other type of customer
request, we haven't actually seen it. The

00:17:01.399 --> 00:17:04.819
details of what we do on the App Engine side
aren't really important and as Josh mentioned

00:17:04.819 --> 00:17:10.420
it's not very complex. But the important thing
to see here is that, during a customer request

00:17:10.420 --> 00:17:15.080
response cycle, Amazon EC2 has never been
touched. So, Amazon EC2 is simply our queue

00:17:15.080 --> 00:17:21.220
servicing code, and customer request never
get to Amazon. And that's a really important

00:17:21.220 --> 00:17:26.280
design point that I'd urge you to think about
if you are architecting an API for multiple

00:17:26.280 --> 00:17:31.610
cloud services. For us, what it means is that
our App time and our scalability is not a

00:17:31.610 --> 00:17:37.300
function of both App Engine and EC2s App side,
up time, or rather our downtime is not a function

00:17:37.300 --> 00:17:43.900
of both of them, it's only strictly tied to
App Engine for us. If Amazon EC2 goes offline,

00:17:43.900 --> 00:17:52.100
what that means is that it'll take a little
longer for us to process requests in our queue.

00:17:52.100 --> 00:17:56.520
So, we've architected this API and it's been
running for the last six months and I want

00:17:56.520 --> 00:18:03.100
to talk a little bit about the behavior that
we've seen during the last six months. So,

00:18:03.100 --> 00:18:07.680
this is sort of the, the bottom line thing
that I'd like to stress and, of course, App

00:18:07.680 --> 00:18:12.180
Engine is a beta service so some amount of
unpredictable performance is predictable.

00:18:12.180 --> 00:18:18.090
But, we saw a number of concerning things
along the way which have really smoothed out

00:18:18.090 --> 00:18:23.950
over this beta period. So, the number one
thing that we struggled with while running

00:18:23.950 --> 00:18:29.990
our API in production is high data store contention,
and by contention what I mean is when we make

00:18:29.990 --> 00:18:34.770
a fetch or a put to the data store, we see
a time out, so contention was the percentage

00:18:34.770 --> 00:18:39.820
of timeouts we saw in a given set of requests.
And in particular for us where we saw a contention

00:18:39.820 --> 00:18:43.500
was accessing our queue data structure. So,
obviously, customer request are coming in

00:18:43.500 --> 00:18:47.350
a rather rapid rate. They're adding new latitudes
and longitudes that we haven't calculated

00:18:47.350 --> 00:18:52.580
yet to the queue, at the same time our calculators
are trying to pull them off, computing them,

00:18:52.580 --> 00:18:58.530
and then pushing them back to App Engine and
removing those entries from the queue once

00:18:58.530 --> 00:19:04.260
they're calculated. So, a typical day for
us today is about a half percent failure on

00:19:04.260 --> 00:19:10.680
reading from the queue. Just two days ago
at around 1:00 a.m., I think, we saw App Engine

00:19:10.680 --> 00:19:16.710
contention rise up to about 50% or 60% for
about six hours, so a really big surprise;

00:19:16.710 --> 00:19:20.740
something that we had to architect for on
the Amazon EC2 side where we're servicing

00:19:20.740 --> 00:19:25.240
our queue. Obviously, we'd buffer up a lot
of latitudes and longitudes there so that

00:19:25.240 --> 00:19:30.880
if we do, or if we're unable to get data from
App Engine for a while, we can still continue

00:19:30.880 --> 00:19:37.820
to calculate Walk Scores. So, another thing
that could happen during this, especially

00:19:37.820 --> 00:19:41.740
during beta period is that the data store
goes offline or simply goes in to read-only

00:19:41.740 --> 00:19:45.790
mode. For those of you who have App running
application over the last six months, you'll

00:19:45.790 --> 00:19:50.810
know that this has happened from time to time.
What you see normally in this case is when

00:19:50.810 --> 00:19:58.440
you make a database, data store request, you
see a capability disabled exception. Another

00:19:58.440 --> 00:20:02.650
thing that we've seen a lot is that the App
Engine response time increases and we measured

00:20:02.650 --> 00:20:07.080
this on the EC2 side, so as our calculator
pulls new points from our App Engine queue,

00:20:07.080 --> 00:20:12.040
normally those requests take about half a
second to a second. Every once in a while

00:20:12.040 --> 00:20:18.100
the response time goes up to several seconds
so, in other words latency increases. App

00:20:18.100 --> 00:20:21.330
Engine is a bit, a bit of a black box, so
it's hard to always understand why this is

00:20:21.330 --> 00:20:25.620
happening. You know, with no changes to your
code and with no changes to your underlying

00:20:25.620 --> 00:20:29.790
data models you might see this. And this is
a very rare occurrence but has happened once

00:20:29.790 --> 00:20:38.630
or twice in the last half year. So, with those
performances issues in mind, what can you

00:20:38.630 --> 00:20:43.540
do to make sure that your application scales
as far as you need it to? And so what I'm

00:20:43.540 --> 00:20:47.850
going to talk about here are sort of the steps
that you can follow to make your applications

00:20:47.850 --> 00:20:52.340
scale as far as you'd like. These steps are
distilled from our experience building the

00:20:52.340 --> 00:20:57.490
Walk Score API but also building other APIs
on top of App Engine that we're working on.

00:20:57.490 --> 00:21:01.580
So, they may not apply in your condition,
they're just good rules of thumb to keep in

00:21:01.580 --> 00:21:09.380
mind as you're building your application.
So, scalability on any system is really about

00:21:09.380 --> 00:21:13.940
stair steps, you do a certain amount of work
you get to the next step. To get to the step

00:21:13.940 --> 00:21:19.250
higher, it might require substantially more
work on your part. And so, at the bottom rung

00:21:19.250 --> 00:21:24.200
for App Engine when you're just getting your
feet wet, here are some of the things that

00:21:24.200 --> 00:21:29.530
your code might have. So, first of all, inconsistent
model design, either your models are very

00:21:29.530 --> 00:21:33.550
large and you only access one or two properties
in any amount of time, or your models are

00:21:33.550 --> 00:21:38.190
very small and distributed, and you end up
accessing a cluster of different models in

00:21:38.190 --> 00:21:42.350
a single request. In that case, you probably
want to re-think the design of your models,

00:21:42.350 --> 00:21:47.430
shape them a little bit differently. So this
is a characteristic that I've seen in, just

00:21:47.430 --> 00:21:51.470
starting out App Engine applications especially
for people who've worked in the sequel world

00:21:51.470 --> 00:21:56.850
before and are just starting to move over.
Uneven or no memcache use, it's a lot of fun

00:21:56.850 --> 00:22:02.380
and it's a lot easier to rate your application
without thinking about memcache at first.

00:22:02.380 --> 00:22:05.830
Unfortunately, pretty much all good App Engine
applications are going to make some or very

00:22:05.830 --> 00:22:12.120
heavy use of memcache early on in their life
cycle. Well, the final thing is, for those

00:22:12.120 --> 00:22:16.890
who don't know every time you make a data
store request of fetch or put, you're effectively

00:22:16.890 --> 00:22:22.120
making an RPC request somewhere in Google's
data center. So, you're able to batch these

00:22:22.120 --> 00:22:26.850
requests but it can be difficult to design
your code that way so, a lot of early stage

00:22:26.850 --> 00:22:32.870
code that I've seen doesn't do that. So, with
this sort of naive style of coding with App

00:22:32.870 --> 00:22:38.370
Engine, we've been able to see something like
five queries a second handled which is actually

00:22:38.370 --> 00:22:42.510
really rather amazing. I should point out
that this number is in our experience depending

00:22:42.510 --> 00:22:45.050
on the type of application that you're building,
you may see something different. But this

00:22:45.050 --> 00:22:49.050
is approximately what we saw when we we're
starting out with the Walk Score API. Five

00:22:49.050 --> 00:22:53.680
queries a second is over 10 million requests
a month. It started with really rather large

00:22:53.680 --> 00:22:58.560
application and it speaks to the power of
App Engine as a platform to get you scaling

00:22:58.560 --> 00:23:04.450
very fast right out the door. So, where do
you go from there once you want to go pass

00:23:04.450 --> 00:23:13.100
that five or so queries a second? Well, the
very first thing I'd urge you to think about

00:23:13.100 --> 00:23:17.790
is starting to use memcache and the easiest
way to use memcache is just slather it on

00:23:17.790 --> 00:23:22.560
everywhere you read data from your data store.
So, the basic behavior, of course, is if you're

00:23:22.560 --> 00:23:26.310
going to read an entity from the data store,
check to see if you've cached it first. If

00:23:26.310 --> 00:23:30.980
you have, great, you're done, if you haven't,
read it, add it to the cache. Oh and be sure

00:23:30.980 --> 00:23:34.300
when you're writing back or updating that
entity in the data store to either clear a

00:23:34.300 --> 00:23:39.000
memcache or update it. And if you actually
go and look at the Google App Engine cookbooks,

00:23:39.000 --> 00:23:43.650
somebody uploaded, I think, about a month
or two ago, a really great shim for the App

00:23:43.650 --> 00:23:49.370
Engine data store that just causes memcache
on read to happen everywhere, so, as a first

00:23:49.370 --> 00:23:56.470
step, that might be something to look into.
Another thing to think about is to batch your

00:23:56.470 --> 00:24:02.580
data store request where easy, where easy
for us was anywhere we had non-nested loops.

00:24:02.580 --> 00:24:06.100
So anywhere we had a loop that we hadn't,
we we're putting single entities; we inverted

00:24:06.100 --> 00:24:12.350
that, now we call one put with lots of entities.
And I should mention that batching is a little

00:24:12.350 --> 00:24:16.870
bit tricky because you can only fetch up to
a thousand entities at a time from the data

00:24:16.870 --> 00:24:21.880
store, and puts are limited based on the size
of the entities you're putting in data store,

00:24:21.880 --> 00:24:25.990
a good rule of thumb is 50 at a time. So,
you may need to have multiple batches depending

00:24:25.990 --> 00:24:32.900
on the size of operations you're performing.
So, without work, we saw a pretty much a double

00:24:32.900 --> 00:24:36.850
in our variability to handle load which is
actually, again, really rather impressive

00:24:36.850 --> 00:24:41.590
and that's not so much engineering work on
our part. But we wanted to go a lot further,

00:24:41.590 --> 00:24:45.530
and of course, the running Walk Score API
see substantially more than 10 queries a second

00:24:45.530 --> 00:24:50.750
on any given day. So, what do we have to do
next, and the next of the tips are really

00:24:50.750 --> 00:24:55.900
a grab bag of things that you might want to
think about as you really scale out your application.

00:24:55.900 --> 00:25:01.010
The first is that I'd urge you to think long
and hard about how your data store is accessed

00:25:01.010 --> 00:25:05.990
and what type of usage pattern do you see.
So memcache is really great in particular

00:25:05.990 --> 00:25:11.910
for two types of usage pattern. One is repeated
request of the same data and the other is

00:25:11.910 --> 00:25:16.650
predictable sequential access to data. So,
for a repeated request of the same data, if

00:25:16.650 --> 00:25:20.460
you know your customer is going to hit that
same entity again and again, "cache on read,"

00:25:20.460 --> 00:25:25.250
like we discussed in the previous step is
actually really a great strategy. But if you

00:25:25.250 --> 00:25:29.970
have a predictable sequential access, for
example, in the case of the Walk Score queue,

00:25:29.970 --> 00:25:32.970
we know that we're going to pull out those
queue items in order or "Calculate our code"

00:25:32.970 --> 00:25:37.920
is going to request them in a certain order.
So our calculator talks to App Engine and

00:25:37.920 --> 00:25:42.950
it requests 50 items at a time from the queue.
But on the App Engine side, what we do is

00:25:42.950 --> 00:25:47.490
we actually pull out a full thousand items
from our queue, put that entire list in the

00:25:47.490 --> 00:25:52.070
memcache, and that obviously cuts down substantially
on a number of data store accesses we need

00:25:52.070 --> 00:25:59.080
to make. So, think about memcache usage patterns.
Batch all of your data store calls, this went

00:25:59.080 --> 00:26:05.990
a long way for us to scaling outward, and
for us what that meant is unraveling nested

00:26:05.990 --> 00:26:10.650
loops. Sometimes they were loops that were
basically a cross method boundary so we had

00:26:10.650 --> 00:26:17.540
to flatten a lot of our code up but it really
helped us a lot. As important as using memcache

00:26:17.540 --> 00:26:22.280
carefully is, sometimes it's just not to right
thing to do to use memcache depending on your

00:26:22.280 --> 00:26:27.290
access pattern. It may not, memcache may not
provide a very meaningful barrier between

00:26:27.290 --> 00:26:32.040
your users and the data store, and that's,
memcache is a limited resource. You obviously

00:26:32.040 --> 00:26:38.260
don't want to populate memcache with entities
that aren't useful to you and, and lose the

00:26:38.260 --> 00:26:43.450
entities that are. And, of course, memcache,
when you add items to memcache, you can, of

00:26:43.450 --> 00:26:47.800
course, think about how long they stay there.
Sometimes that's a very great way to keep

00:26:47.800 --> 00:26:53.520
memcache pressure low. And then, I would urge
you to load test your application. We actually

00:26:53.520 --> 00:26:58.420
built a load testing hardness on Amazon EC2.
We're able to hit, our running Walk Score

00:26:58.420 --> 00:27:04.680
instances with over a hundred queries a second
and that's real load, talking to real URL

00:27:04.680 --> 00:27:09.290
handlers that actually do real work with the
data store. So, load testing, there are third

00:27:09.290 --> 00:27:15.280
party products to help you with that too.
App Engine load tends to be surprising once

00:27:15.280 --> 00:27:21.580
you get pretty high, past 10 queries a second.
And the last thing is, of course, monitor

00:27:21.580 --> 00:27:27.450
your performance. Now the frontline of defense
for monitoring performance is the, is the

00:27:27.450 --> 00:27:31.290
App Engine Dashboard which is a great place
to go and of course, your logs to look at

00:27:31.290 --> 00:27:36.480
individual requests that, for example, it
gave you a data store time out. Also the system

00:27:36.480 --> 00:27:40.590
status if you're seeing a lot of latency is
a good place to go, just look at the overall

00:27:40.590 --> 00:27:44.670
behavior of App Engine. We actually built
our own performance dashboard for the Walk

00:27:44.670 --> 00:27:50.300
Score API which we're happy to show you if
you'd like after this talk. And it monitors

00:27:50.300 --> 00:27:56.000
specific things about the behavior of our
EC2's calculator code communication with App

00:27:56.000 --> 00:28:01.530
Engine. And just sort of one last technique,
if you're getting started with App Engine

00:28:01.530 --> 00:28:06.400
coding, that I think everybody should know
which is, if at first you don't succeed, try,

00:28:06.400 --> 00:28:11.490
try again. What we have here is basically
an attempt to write to the data store that

00:28:11.490 --> 00:28:16.720
time, if it times out, we're going to turn
around right away and write again. Just a

00:28:16.720 --> 00:28:20.610
few milliseconds later, you may not get a
time-out exception and this is something we

00:28:20.610 --> 00:28:23.890
do everywhere in our code base now, both on
the read and write side and it's actually

00:28:23.890 --> 00:28:31.170
extremely helpful. So, I've talked a little
bit about general principles for scaling out

00:28:31.170 --> 00:28:34.830
App Engine applications, and what I want to
do now is turn it back over the Josh, who's

00:28:34.830 --> 00:28:39.700
going to talk about one specific instance
where we had some difficult scalability issues

00:28:39.700 --> 00:28:45.630
that we needed to tackle and the sort of unorthodox
technique we used to solve. So, thanks.

00:28:45.630 --> 00:28:52.110
&gt;&gt; LIVNI: Thanks Dave. So, yeah, I started
out talking about kind of the core functionality

00:28:52.110 --> 00:28:56.900
and secondary functionality. Obviously, your
guy's applications are going to have different

00:28:56.900 --> 00:29:00.010
pieces of secondary functionality that may
or may not fit different places. A really

00:29:00.010 --> 00:29:06.040
common use case on App Engine is counting
stuff. I'm going to talk about specific issue

00:29:06.040 --> 00:29:12.390
we have counting our user's request. We have
a basic quota system, we have an API key.

00:29:12.390 --> 00:29:14.430
We want to know who's basically doing what
so, certain folks might get, you know, a couple

00:29:14.430 --> 00:29:20.220
of hundred thousand requests a day. Others
might get, you know, maybe a couple million

00:29:20.220 --> 00:29:25.100
a day. But we want to make sure that people
can't just run away and abuse the system and

00:29:25.100 --> 00:29:29.730
again, we understand who's doing what and
where. So counting every request that comes

00:29:29.730 --> 00:29:32.960
in, matching it to users should be pretty
straightforward and the answer that's usually

00:29:32.960 --> 00:29:37.680
given for this is I'll just use sharded counters.
Every time a request comes in, write it to

00:29:37.680 --> 00:29:42.100
one of this shards, you're good to go. But
every time a request comes in, if that request

00:29:42.100 --> 00:29:45.400
is, you know, a hundred requests a second,
that's a lot of shards and you're going to

00:29:45.400 --> 00:29:49.860
really hit a lot of data store contention.
So, we found that these sharded counters didn't

00:29:49.860 --> 00:29:54.360
scale as well as we had hoped, and what happened
in our case, this was from about six months

00:29:54.360 --> 00:29:59.920
ago and things are really bit different now.
Around 30-40 queries a second, our response

00:29:59.920 --> 00:30:04.260
time started creeping up, you know, two, four,
six plus seconds and then, along with the

00:30:04.260 --> 00:30:08.410
error rates and then all of a sudden, everything
was 503s and this is a really big problem

00:30:08.410 --> 00:30:13.190
because again, core functionality should not
be hurt by this sort of icing on the cake

00:30:13.190 --> 00:30:17.020
of counting who's doing what. So, we were
finding that just because we want to have

00:30:17.020 --> 00:30:21.080
quota system now nobody can even get a score
because all the requests are just bombing

00:30:21.080 --> 00:30:26.310
out entirely. So, the solution that we came
up with, how many of you guys were in the

00:30:26.310 --> 00:30:32.280
talk just before this, at Brett's talk next
door? So a few guys, so, one of the things

00:30:32.280 --> 00:30:38.660
I thought really interesting example of the
new task queue was the backend sort of writing

00:30:38.660 --> 00:30:43.350
to a cache. We implemented something--similar
concepts. Dave talked a little about using

00:30:43.350 --> 00:30:49.410
memcache cleverly on read and this is using
memcache on write which in the Google group

00:30:49.410 --> 00:30:52.010
discussions in other places, people are going
to say "Oh, don't do that, you know, memcache

00:30:52.010 --> 00:30:56.990
unreliable. Things might go away." But in
my opinion, there are certain cases and this

00:30:56.990 --> 00:31:03.490
is one of them for us where you don't absolutely
need to have exact accuracy. If you're writing

00:31:03.490 --> 00:31:08.190
a banking application and counting people's
pennies, you know, don't do this. But for

00:31:08.190 --> 00:31:12.260
a lot of cases, if you're just getting a general
idea of a quota, how many things are, you

00:31:12.260 --> 00:31:16.160
know, generally around at a given time. And
again, knowing things might fail, you have

00:31:16.160 --> 00:31:19.980
other processes to come in place and check
things happened accurately, do things twice

00:31:19.980 --> 00:31:23.770
and so forth. Using memcache on write, it
can be a really interesting idea. So this

00:31:23.770 --> 00:31:29.280
diagram, basically, it shows that request
comes in with the API rather than writing

00:31:29.280 --> 00:31:34.590
your sharded counter, data store counter,
we write to the sharded memcache counters.

00:31:34.590 --> 00:31:39.600
When the memcache counters fill up to 100,
to 100 whatever, only then do we write that

00:31:39.600 --> 00:31:45.650
bit of information to the data store. And
then, the task queue is a little bit nicer

00:31:45.650 --> 00:31:50.300
than this, and the talk just before this,
you know, just between the memcache and the

00:31:50.300 --> 00:31:54.790
data store, there's this task queue and it's
a lot faster to write to the upcoming task

00:31:54.790 --> 00:32:00.270
queue that we certainly weren't expecting
when we started writing this. But, you still

00:32:00.270 --> 00:32:03.960
have the same issue, writing to a task queue
or writing to the data store which is just

00:32:03.960 --> 00:32:09.050
this small time period where request can come
in, get your memcache shard, increment the

00:32:09.050 --> 00:32:12.700
count before you've written and then cleared
the memcache shard. So, you just have to sort

00:32:12.700 --> 00:32:17.330
of keep track of the data--of what you're
writing to the data store and then the amount

00:32:17.330 --> 00:32:23.210
of things that came in before you got confirmation,
same with task queue. So, as I mentioned,

00:32:23.210 --> 00:32:28.100
the solution is really nice, the advantage
is really nice, you get, instead of, you know,

00:32:28.100 --> 00:32:32.770
a hundred data store, you know, hits a second,
you get a couple of orders of magnitude less

00:32:32.770 --> 00:32:37.440
and that just scales up really, really, really
nicely. But the disadvantage is there is,

00:32:37.440 --> 00:32:42.300
I don't know where my disadvantage went? But
there's a possibility you could lose a little

00:32:42.300 --> 00:32:46.690
bit. And so for our use case, we were okay
if it was off a little bit, as long as we

00:32:46.690 --> 00:32:51.420
didn't over count. We didn't want to accidentally
set some of our quota that they didn't use

00:32:51.420 --> 00:32:54.740
and so we just really, really careful that
in case of a loss of data, it's always a subtle

00:32:54.740 --> 00:32:59.490
under count. And again, memcache has proven
really, really reliable, there's a couple

00:32:59.490 --> 00:33:05.390
of, you know, two or three hour knockouts
that we had that we just lost counters for.

00:33:05.390 --> 00:33:09.830
But, aside from that, as a general rule, you
can pretty much rely on it. So, after we implement

00:33:09.830 --> 00:33:14.020
the memcache, things went like this, and again,
this graph's from a while ago, these days,

00:33:14.020 --> 00:33:17.970
it's smoother and things stay well under a
second for the response time, but it's really

00:33:17.970 --> 00:33:23.120
a huge difference. What it means is we can
actually count stuff now at a very, very high

00:33:23.120 --> 00:33:28.170
rate per second, and not have to worry about
hitting these issues where our core functionalities

00:33:28.170 --> 00:33:34.790
compromise. So the summary here is, it's not
necessarily a bad idea to think about using

00:33:34.790 --> 00:33:40.380
memcache on write. Again, possibility of glossiness
is always there but, in many cases that you

00:33:40.380 --> 00:33:46.310
might come across, it can really save not
only the possibility of this data store contention

00:33:46.310 --> 00:33:51.120
but also some money because you charged for
the CPU time on the data store, and if you're

00:33:51.120 --> 00:33:57.020
not in the data store, you know that can add
up a few bucks a day overtime. And then, the

00:33:57.020 --> 00:34:02.820
other piece is early on decide; is this functionality
a good fit for App Engine? Certain things,

00:34:02.820 --> 00:34:08.409
for example, we could've offloaded the quota,
parsed logs every couple of hours offline,

00:34:08.409 --> 00:34:12.929
come back on. And we decided "Oh, this is
a good piece to fit on App Engine, we ought

00:34:12.929 --> 00:34:16.300
to be able to count stuff." Other things we
decided, "No, this piece, although integrated

00:34:16.300 --> 00:34:21.800
with App Engine and the API, maybe when we
box, so just deciding which pieces fit in

00:34:21.800 --> 00:34:25.639
App Engine is important. And then the final
one is, of course, when you're putting those

00:34:25.639 --> 00:34:29.760
pieces into App Engine, make sure that you
write them in such a way that should strange

00:34:29.760 --> 00:34:33.540
and weird things happen, like all of a sudden
you get popular, 80 queries a second, data

00:34:33.540 --> 00:34:37.800
store contention hits. Those features that
you wanted to have an option, don't compromise

00:34:37.800 --> 00:34:42.730
the really core functionality that your application
may have. And so with that, we'll turn it

00:34:42.730 --> 00:34:48.360
back to Jesse and talk about some of the overall
results.

00:34:48.360 --> 00:34:52.010
&gt;&gt; KOCHER: So, you've seen some of the, you
know, stair steps to scalability hotness that

00:34:52.010 --> 00:34:58.520
Dave went through, where has all that gotten
us? Well, the three of us, a very small team

00:34:58.520 --> 00:35:04.850
have built an App Engine API that handles
over 1.5 million requests a day, and peak

00:35:04.850 --> 00:35:09.910
rates up to about 80 requests per second,
it's quite a bit higher than where we started.

00:35:09.910 --> 00:35:17.800
And we've come to think of App Engine as really
being a black box. It's a place where things

00:35:17.800 --> 00:35:22.650
may shift in, you know, unexpected ways internally
and you may not always know what's going on

00:35:22.650 --> 00:35:29.510
in there. But it's also very well documented
and not just the main documentation online

00:35:29.510 --> 00:35:35.880
but also this dynamic support with IRC channel,
the Google groups, the issue tracker, and

00:35:35.880 --> 00:35:42.290
the App Engine team is really free, they frequent
all of these places. So the degree of communication

00:35:42.290 --> 00:35:48.310
is pretty amazing and it makes working inside
of a black box considerably less daunting.

00:35:48.310 --> 00:35:53.370
So, our amended statement would be, it's a
very good black box. I think looking back,

00:35:53.370 --> 00:35:57.340
we would make the same choice that we made,
you know, not knowing a lot of the things

00:35:57.340 --> 00:36:01.780
that we've learned along the way, and I can
say that having watched App Engine progress

00:36:01.780 --> 00:36:07.200
over these last few months, the work that
we have done would be considerably easier

00:36:07.200 --> 00:36:12.850
starting now than it was starting back then.
So we hope that this talk has helped you understand

00:36:12.850 --> 00:36:17.890
some of the risks and rewards of using App
Engine and some things about how you get App

00:36:17.890 --> 00:36:25.630
to scale well. We have, you know, put up our
contact info for a second here, so if you

00:36:25.630 --> 00:36:30.830
want to get in touch with us, you can jot
that down. We will also, got some time for

00:36:30.830 --> 00:36:35.840
questions here and we also will be out of
the demo spot for most of the rest of the

00:36:35.840 --> 00:36:43.960
afternoon until about 4 o'clock, so if you
want to catch us there, please do. And we'll

00:36:43.960 --> 00:36:48.260
move into Q and A now, so, we're happy to
take questions on anything. We'll take them

00:36:48.260 --> 00:36:54.840
at the mic and also, I'll flip over to the
moderator side in a few minutes. But before

00:36:54.840 --> 00:36:57.770
I do that I want to put up, these are a few
things that we didn't put in to our talk but

00:36:57.770 --> 00:37:01.540
we definitely have more to say about, so if
any of those interest you, feel free to ask

00:37:01.540 --> 00:37:06.560
about those. And then, Google sent us a note
earlier today asking us to direct you all

00:37:06.560 --> 00:37:12.320
to haveasec.com/io. If you go there, you put
in the time slot and the session and you can

00:37:12.320 --> 00:37:16.430
give feedback so we'd appreciate any feedback
you have for us. And with that, we will to

00:37:16.430 --> 00:37:20.840
move to questions so, go ahead.
&gt;&gt; Two part question about your counters.

00:37:20.840 --> 00:37:25.600
Why did you still shard them when you put
them on memcache?

00:37:25.600 --> 00:37:30.860
&gt;&gt; KOCHER: There actually can be a contention
with memcache in that, when you're hitting

00:37:30.860 --> 00:37:34.500
it very intensively.
&gt;&gt; How many request per second when you put

00:37:34.500 --> 00:37:39.480
that or…?
&gt;&gt; LIVNI: Well, we found that once we hit

00:37:39.480 --> 00:37:43.920
our, there's a couple of reasons we sharded
the memcache counter originally. One is there

00:37:43.920 --> 00:37:47.840
is that possibility of contention, and the
other is it's cheap to make lots of memcache

00:37:47.840 --> 00:37:53.140
shards, and one of the issues I mentioned
in that slide is that as you're writing the

00:37:53.140 --> 00:37:57.340
shard to the data store, you might not get
a response back for a half second or four

00:37:57.340 --> 00:38:02.220
seconds. So, all of that time, you're having
a lot more requests coming in. We wanted to

00:38:02.220 --> 00:38:08.570
minimize to some degree in order to keep the
count a little--as accurate as possible, that

00:38:08.570 --> 00:38:13.610
very, very small timeframe from when we clear
the memcache shard as, "Okay, we got a confirmation

00:38:13.610 --> 00:38:17.850
back" and then we've written it out. We don't
want 30 new requests coming in in that timeframe

00:38:17.850 --> 00:38:21.980
because there is still a little bit of latency
even writing to memcache. It's, you know,

00:38:21.980 --> 00:38:26.530
much, much faster writing to data store and
so by making a lot of shards, it's not really

00:38:26.530 --> 00:38:31.470
any overhead for the system that we can sort
of spread out the number of requests in that

00:38:31.470 --> 00:38:36.330
just a couple of milliseconds, it's less likely
we'll lose something. Does that make sense?

00:38:36.330 --> 00:38:41.250
&gt;&gt; Okay. The second part is, before this morning,
I, you know, I didn't really know how many

00:38:41.250 --> 00:38:46.980
memcache instances Google would have for my
App. But now, it sounds like I have to assume

00:38:46.980 --> 00:38:51.900
if I write to a key, it will always be the
same one memcache because, otherwise, these

00:38:51.900 --> 00:38:57.170
counters wouldn't work. You know, if I have
one instance of my Java App running in Australia…

00:38:57.170 --> 00:39:02.550
&gt;&gt; PECK: No, that's correct, memcache is a
distributed API and so, if you write on, you

00:39:02.550 --> 00:39:06.710
don't have any knowledge of what CPU your
code is executing on inside the App Engine

00:39:06.710 --> 00:39:09.490
data center.
&gt;&gt; Right. But now, it sounds like I have to

00:39:09.490 --> 00:39:15.220
guarantee that when I write to memcache, all
my 500 App instances write to the same memcache

00:39:15.220 --> 00:39:16.610
instance.
&gt;&gt; PECK: Yeah, you're effectively writing

00:39:16.610 --> 00:39:19.990
to the same store. That's correct.
&gt;&gt; Do you think that will be the future proof?

00:39:19.990 --> 00:39:23.930
&gt;&gt; PECK: I don't know they propagate that
data across, I don't know how long it takes

00:39:23.930 --> 00:39:28.240
so, if you're running a code, not that you
can specify when you're writing an App Engine

00:39:28.240 --> 00:39:30.440
application right now, but if some of your
codes are running in Australia and some of

00:39:30.440 --> 00:39:34.000
it is running here, I don't know how long
that would take to propagate. But, yes, it

00:39:34.000 --> 00:39:38.200
looks like you see a consistent view of the
memcache world regardless of where your code

00:39:38.200 --> 00:39:41.080
is.
&gt;&gt; Just surprised that that's future proof

00:39:41.080 --> 00:39:45.910
because, you know, once you get to the Facebook
scale, maybe you guys, you need multiple memcache

00:39:45.910 --> 00:39:47.170
instances.
&gt;&gt; PECK: Yeah. Yeah.

00:39:47.170 --> 00:39:49.850
&gt;&gt; For the same write.
&gt;&gt; LIVNI: Yeah, I mean, we've had good luck

00:39:49.850 --> 00:39:53.790
with it but I'd say that's a really good question
for the folks who actually built it would

00:39:53.790 --> 00:39:58.090
have the better answers than we would.
&gt;&gt; KOCHER: They also have, you know, the memcache

00:39:58.090 --> 00:40:01.960
has a certain size and, you know, the more
you put in there, the more you increase pressure

00:40:01.960 --> 00:40:06.210
on your memcache and the sooner things will
expire from it. So, there's really a balancing

00:40:06.210 --> 00:40:11.100
act about figuring out which things to put
in there and, you know, the size of your entities

00:40:11.100 --> 00:40:17.060
matters when you're putting things in. So,
choosing that, you're making a lot of decisions

00:40:17.060 --> 00:40:19.210
that will affect how available that data is
when you go back to look for it later.

00:40:19.210 --> 00:40:22.850
&gt;&gt; LIVNI: I think there's one actually a small
thing I should mention on that which is that

00:40:22.850 --> 00:40:27.640
when you write to a memcache, I believe it
gets deleted, first created first out, not

00:40:27.640 --> 00:40:34.970
last updated, out. And so, when we refresh
our memcache key, I believe we delete the

00:40:34.970 --> 00:40:40.240
key and then recreate it, rather than just
updating it. So that way, it's got a fresher

00:40:40.240 --> 00:40:48.030
date and is less likely to get migrated should
more contention happen in memcache.

00:40:48.030 --> 00:40:56.050
&gt;&gt; I have a couple of questions. First, it's
about your API. You sort of had a very simple

00:40:56.050 --> 00:41:01.830
diagram but the queuing that you do when you
don't have the latitude and longitude, does

00:41:01.830 --> 00:41:09.420
your API just are done queued up? Like, I
mean, in case, the queries for a latitude

00:41:09.420 --> 00:41:14.020
and longitude that you haven't calculated,
do you just redone queued up?

00:41:14.020 --> 00:41:18.070
&gt;&gt; PECK: Yeah, that's right. Our API, we tell
our customer try back later basically. If

00:41:18.070 --> 00:41:22.580
you think about the typical use case of embedding
in a real estate site, what that means is

00:41:22.580 --> 00:41:26.590
a customer doesn't display the Walk Score
at that time. But the next time one of their

00:41:26.590 --> 00:41:30.380
users comes and looks at that same property,
we'll probably have that Walk Score queued

00:41:30.380 --> 00:41:35.410
up and ready for them.
&gt;&gt; Okay. And the other question is about the,

00:41:35.410 --> 00:41:42.800
so, you use this metric of queries per second.
So, when you were describing the techniques

00:41:42.800 --> 00:41:46.770
to speed up like using memcache and everything.
&gt;&gt; PECK: Yeah.

00:41:46.770 --> 00:41:50.730
&gt;&gt; So, these queries per second I'm assuming
is like data store queries that you're saying,

00:41:50.730 --> 00:41:52.860
right?
&gt;&gt; PECK: Oh no, when I was showing those queries

00:41:52.860 --> 00:41:58.030
per second numbers, what I was showing is
the number of your successful URL requests

00:41:58.030 --> 00:42:02.650
that someone outside of our, of the App Engine
data center can make through our application

00:42:02.650 --> 00:42:07.640
and get running correctly. And those numbers
were rough, they won't apply to your application

00:42:07.640 --> 00:42:10.140
exactly but that's approximately what we saw
in the development of the Walk Score.

00:42:10.140 --> 00:42:11.490
&gt;&gt; LIVNI: They're also a little bit better
now.

00:42:11.490 --> 00:42:13.580
&gt;&gt; PECK: Yeah, they really are.
&gt;&gt; LIVNI: I think if you would write the same

00:42:13.580 --> 00:42:19.250
App that we did early on, you would get, I
think, a much higher queries per second today

00:42:19.250 --> 00:42:22.090
than eight months ago.
&gt;&gt; PECK: Yeah, I mean, the bottom line message

00:42:22.090 --> 00:42:26.430
is that with even with naive code, you can
actually scale how pretty far with that, that's

00:42:26.430 --> 00:42:29.870
just pretty impressive.
&gt;&gt; I have two questions. The first one is:

00:42:29.870 --> 00:42:34.740
are you now running your entire production
front end off of App Engine?

00:42:34.740 --> 00:42:39.060
&gt;&gt; PECK: Yes, we do.
&gt;&gt; So, if we go to www.walkscore.com is actually…

00:42:39.060 --> 00:42:45.640
&gt;&gt; KOCHER: No, great. No, we're only doing
the API there. We are looking at, we're considering

00:42:45.640 --> 00:42:48.930
moving other parts that, you know, either
the tile or potentially even the whole website

00:42:48.930 --> 00:42:53.440
over and you could mention quickly the other
stuff.

00:42:53.440 --> 00:42:58.410
&gt;&gt; PECK: Yeah, yesterday. So, that the Walk
Score website is also actually written in

00:42:58.410 --> 00:43:03.830
PHP and as you know JBM now on App Engine
and there's a company here called Calcio which

00:43:03.830 --> 00:43:09.320
makes a PHP implementation for the JBM. And
actually yesterday, we just, we're able to

00:43:09.320 --> 00:43:17.210
port our entire PHP website over to App Engine,
and we just started testing it but it looks

00:43:17.210 --> 00:43:22.460
like it's extremely performant which is impressive.
It's a lot faster than running Apache on a

00:43:22.460 --> 00:43:27.830
private server at some random ISP. Yeah, the
API itself, however, is all of course, Python

00:43:27.830 --> 00:43:32.430
App Engine.
&gt;&gt; And the second question is, have you run

00:43:32.430 --> 00:43:37.350
into any of the big limits of App Engine in
terms of the storage and Nexus? You've mentioned

00:43:37.350 --> 00:43:42.840
in your last slide, have there been any big
things where you end up having to be billed

00:43:42.840 --> 00:43:50.200
for a lot more than you expected?
&gt;&gt; KOCHER: We haven't. They gave us, you know,

00:43:50.200 --> 00:43:55.810
as early users of early high traffic users.
They gave us access to go beyond the, those

00:43:55.810 --> 00:44:02.800
initial free quarters, you know, before the
billing was enabled. Now that billing is enabled,

00:44:02.800 --> 00:44:07.980
there's a lot more flexibility to go, you
can use a lot, so we haven't really run into

00:44:07.980 --> 00:44:14.190
any of those limits. I think the things that
are interesting are the per request limits

00:44:14.190 --> 00:44:20.770
of, you know, how much CPU time you can use
and how much time you can use, and we have

00:44:20.770 --> 00:44:26.800
some of the EC2 to App Engine communication.
Some of those things do, they use way more

00:44:26.800 --> 00:44:30.960
CPU time than App Engine is really happy with,
but because they're are much less frequent

00:44:30.960 --> 00:44:37.180
than the public calls that come in at a much
higher rate. We can manage those and we use

00:44:37.180 --> 00:44:41.290
some backing off techniques where if EC2 has
trouble communicating, it can say, "Oh, we're

00:44:41.290 --> 00:44:44.180
going to stop talking to App Engine for a
little while and stop hitting it with this

00:44:44.180 --> 00:44:47.920
high CPU intensive request."
&gt;&gt; PECK: Do you want to take this?

00:44:47.920 --> 00:44:53.140
&gt;&gt; KOCHER: Sure.
&gt;&gt; PECK: So, considering that calculating

00:44:53.140 --> 00:44:58.510
Walk Scores requires considerable time, would
consider using web hooks? And, actually, I

00:44:58.510 --> 00:45:04.360
was in Brett's talk just previous and the
answer is yes, I think we'd love to use web

00:45:04.360 --> 00:45:09.200
hooks. We haven't used them yet, but it might
be something for us to look at. And certainly,

00:45:09.200 --> 00:45:12.560
for those who were there, I think you probably
guessed that a lot of our calculator code

00:45:12.560 --> 00:45:17.030
fits nicely with the task queue API that he
described.

00:45:17.030 --> 00:45:23.180
&gt;&gt; KOCHER: Yes, go ahead.
&gt;&gt; Could you please elaborate a little bit

00:45:23.180 --> 00:45:31.910
into how you implemented the, how you implemented
doing the queries of your App Engine and the

00:45:31.910 --> 00:45:38.400
computational intensive part of Amazon EC2?
&gt;&gt; PECK: Sure, I mean, I can dig a little

00:45:38.400 --> 00:45:42.440
more into the design of the calculator. Basically,
the calculator is currently, well, okay, we

00:45:42.440 --> 00:45:51.240
have a bunch of other stuff. So, the calculator
is, of course, completely running on at EC2.

00:45:51.240 --> 00:45:53.860
And it's actually running on a single machine
but it's lots and lots of processes.

00:45:53.860 --> 00:45:55.590
&gt;&gt; KOCHER: Do you want to jump this thing
here?

00:45:55.590 --> 00:46:00.070
&gt;&gt; PECK: Oh, actually I wanted to jump to
that one. And so, the key thing that we saw

00:46:00.070 --> 00:46:03.780
because of the unpredictable performance reading
from the queue that we've implemented on the

00:46:03.780 --> 00:46:08.660
App Engine side is that we wanted to decouple
the computation of the Walk Score from the

00:46:08.660 --> 00:46:13.450
I/O request we've made. So, what we have is
a master process that spawns up a ton of slaves.

00:46:13.450 --> 00:46:17.990
Some of those slaves are responsible for talking
to App Engine and requesting new latitudes

00:46:17.990 --> 00:46:23.710
and longitudes that we need to calculate.
And what we do is we have a rather large buffer

00:46:23.710 --> 00:46:27.460
on the Amazon EC2 site. A lot of latitudes
and longitudes coming in, so that if we can't

00:46:27.460 --> 00:46:32.850
talk to App Engine for a while, we can continue
to calculate scores, so that's one major thing.

00:46:32.850 --> 00:46:39.080
We basically decoupled our I/O from our computation
on the EC2 side and we buffered all our I/O.

00:46:39.080 --> 00:46:43.130
And sort of one last point which Jesse alluded
to you is that we dynamically respond to the

00:46:43.130 --> 00:46:48.320
changing I/O conditions, so, if App Engine
is, if contentions are very high for more

00:46:48.320 --> 00:46:52.720
than, say, 10 or 15 requests in a row, what
we do is we actually back off and stop talking

00:46:52.720 --> 00:46:58.510
to App Engine for a while. And for various
reasons based on the internal design of the

00:46:58.510 --> 00:47:02.930
data store, that's something that can be alleviate
contention if we come back 20 minutes later,

00:47:02.930 --> 00:47:07.400
and just work through the buffer, the data
that's buffered on the EC2 side. So, that's

00:47:07.400 --> 00:47:09.850
sort of the big picture of the design of the
calculator.

00:47:09.850 --> 00:47:15.420
&gt;&gt; All right. So, it was roughly one database
and you used RSync or something like that

00:47:15.420 --> 00:47:18.590
between…
&gt;&gt; PECK: Oh, so actually App Engine is our

00:47:18.590 --> 00:47:24.060
data master. All of our data is there. All
of the points that we're working on, on the

00:47:24.060 --> 00:47:29.970
EC2 side are simply held in RAM. If that process
crashes, it's not the end of the world. We

00:47:29.970 --> 00:47:36.290
basically might read you a few points. We
actually, the EC2 code is also Python. So,

00:47:36.290 --> 00:47:42.930
if we accept out of a process, we actually
just pickle out all the current data, so.

00:47:42.930 --> 00:47:46.830
&gt;&gt; Okay. Thanks.
&gt;&gt; KOCHER: Any other questions?

00:47:46.830 --> 00:47:49.560
&gt;&gt; PECK: Any other questions?

00:47:49.560 --> 00:47:54.480
&gt;&gt; KOCHER: That was the only…
&gt;&gt; LIVNI: I think that was it.

00:47:54.480 --> 00:47:54.650
&gt;&gt; PECK: Oh, thank you.
&gt;&gt; LIVNI: Thanks, guys.

00:47:54.650 --> 00:47:54.710
&gt;&gt; KOCHER: Yup. Thanks.

