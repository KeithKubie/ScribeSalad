WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:03.370
Okay Michael, so that was pretty good with the quiz. I want to

00:00:03.370 --> 00:00:05.430
do another derivation and I want you to help me with it, okay?

00:00:05.430 --> 00:00:06.100
&gt;&gt; Hm. Cool.

00:00:06.100 --> 00:00:07.740
&gt;&gt; Okay, so Michael, we have a similar

00:00:07.740 --> 00:00:09.680
setup to what we've had before. We're given a

00:00:09.680 --> 00:00:12.470
bunch of training data, XI inputs and DI

00:00:12.470 --> 00:00:15.760
outputs. But this time we're dealing with real valued

00:00:15.760 --> 00:00:18.140
functions. So the way DIs are constructed is

00:00:18.140 --> 00:00:21.590
there's some Deterministic function f, that we pass the

00:00:21.590 --> 00:00:23.880
fs through. And that gives us some value.

00:00:23.880 --> 00:00:25.690
And that's really what we're trying to figure out.

00:00:25.690 --> 00:00:28.610
What is the underlying f? But to make our job

00:00:28.610 --> 00:00:31.990
a little bit harder, we have noisy outputs. So, for every

00:00:31.990 --> 00:00:36.820
single DI that is generated, there's some small error, epsilon

00:00:36.820 --> 00:00:40.530
that is added to it. Now, this particular error term, is

00:00:40.530 --> 00:00:43.640
in fact drawn from a normal distribution with zero mean

00:00:43.640 --> 00:00:46.080
and some variance. We don't know what the variance is. It's

00:00:46.080 --> 00:00:48.830
going to turn out. It doesn't actually matter. There's some variance

00:00:48.830 --> 00:00:51.330
going on here. The important thing is that there's zero mean.

00:00:51.330 --> 00:00:51.870
So, you got it?

00:00:51.870 --> 00:00:55.720
&gt;&gt; And it's important that it's probably the same variance for all the data.

00:00:55.720 --> 00:01:00.110
&gt;&gt; That's right, in fact, each of these epsilon sub i's are drawn iid.

00:01:00.110 --> 00:01:02.280
&gt;&gt; And is that f, are we assuming it's linear?

00:01:02.280 --> 00:01:03.700
&gt;&gt; Nope, we're not assuming that it's linear.

00:01:03.700 --> 00:01:04.019
&gt;&gt; Okay.

00:01:04.019 --> 00:01:04.879
&gt;&gt; It's just some function.

00:01:04.879 --> 00:01:05.740
&gt;&gt; All right, I'm with you.

00:01:05.740 --> 00:01:07.050
&gt;&gt; Okay, so you got it?

00:01:07.050 --> 00:01:07.480
&gt;&gt; Yep.

00:01:07.480 --> 00:01:10.110
&gt;&gt; All right. So, here's my question

00:01:10.110 --> 00:01:14.050
to you. What is The maximum likelihood hypothesis.

00:01:14.050 --> 00:01:16.210
&gt;&gt; Do we know f? Can I just say f?

00:01:16.210 --> 00:01:16.690
&gt;&gt; No,

00:01:16.690 --> 00:01:20.230
we don't know f. All we see are x sub i's and d sub i's. But we

00:01:20.230 --> 00:01:22.660
know there is some underlying f. And we

00:01:22.660 --> 00:01:25.100
know that it's noisy, according to some normal distribution.

00:01:25.100 --> 00:01:27.210
&gt;&gt; I don't know how I would find that.

00:01:27.210 --> 00:01:29.850
&gt;&gt; Well let's try to walk it through. So.

00:01:29.850 --> 00:01:32.140
We know how to find the maximum likelihood hypothesis, at

00:01:32.140 --> 00:01:34.010
least we know an equation for it. The maximum

00:01:34.010 --> 00:01:39.070
likelihood hypothesis is simply the one that maximizes this expression.

00:01:39.070 --> 00:01:41.710
&gt;&gt; Right. That was when we assumed a uniform

00:01:41.710 --> 00:01:43.300
prior on the hypotheses.

00:01:43.300 --> 00:01:45.480
&gt;&gt; Exactly. And so we, this is sort of the easiest

00:01:45.480 --> 00:01:47.900
case to think about Where it turns out that finding the

00:01:47.900 --> 00:01:51.550
hypothesis that best fits the data is the same as finding

00:01:51.550 --> 00:01:54.470
a hypothesis that describes the data the best. If you make an

00:01:54.470 --> 00:01:58.400
assumption about a uniform distribution, or a uniform prior. Okay, so.

00:01:58.400 --> 00:02:00.130
This is all we have to do now is figure out

00:02:00.130 --> 00:02:04.720
what we're going to do to expand this expression. So what do

00:02:04.720 --> 00:02:06.910
you think we should do first? The probability of the data given

00:02:06.910 --> 00:02:10.889
the hypothesis. Right. So each we assumed IID.

00:02:10.889 --> 00:02:11.473
&gt;&gt; Mm-hm.

00:02:11.473 --> 00:02:13.660
&gt;&gt; You actually helpfully even wrote that down.

00:02:13.660 --> 00:02:15.820
So we can expand that into the product

00:02:15.820 --> 00:02:18.990
over all the data elements of the probablity

00:02:18.990 --> 00:02:22.700
of that data element given the hypothesis. And x.

00:02:22.700 --> 00:02:23.930
&gt;&gt; Okay, so let's do that, Michael.

00:02:23.930 --> 00:02:27.390
Let's write that out. So, finding the hypothesis

00:02:27.390 --> 00:02:32.470
that maximizes the data that we see, as you point out, is just a product

00:02:32.470 --> 00:02:35.330
over each of the independent data that we

00:02:35.330 --> 00:02:39.330
see. Or datums. So that's good. That's one

00:02:39.330 --> 00:02:41.170
nice step. So we've gone from talking about

00:02:41.170 --> 00:02:43.130
all of the data together to each of the

00:02:43.130 --> 00:02:45.390
individual training data that we see. So what

00:02:45.390 --> 00:02:47.310
do we do next? What is the probability

00:02:47.310 --> 00:02:52.350
of seeing one particular P sub i, given that we're in a world where H is true.

00:02:52.350 --> 00:02:57.850
&gt;&gt; So okay, given that H is true that means whatever the corresponding xi

00:02:57.850 --> 00:03:02.500
is, if we push that through the f function, then the di is going to

00:03:02.500 --> 00:03:08.280
be F of XI plus some error term so I guess if we took di minus

00:03:08.280 --> 00:03:10.900
F X I, that would tell us what the error term is and the we

00:03:10.900 --> 00:03:14.110
just need an expression for saying how likely it is that we get that much error.

00:03:14.110 --> 00:03:16.340
&gt;&gt; Right, so, what is the expression that tells us that?

00:03:16.340 --> 00:03:18.540
&gt;&gt; I'm guessing it's something that uses the

00:03:18.540 --> 00:03:20.600
normal distribution, it probably has an E in it.

00:03:20.600 --> 00:03:23.120
&gt;&gt; [LAUGH] I think that' s absolutely right.

00:03:23.120 --> 00:03:25.770
So, let's be particular about what you said. So, when

00:03:25.770 --> 00:03:27.930
you say that we should push it through F of X,

00:03:27.930 --> 00:03:30.600
let's be clear that that's basically what H is supposed

00:03:30.600 --> 00:03:33.250
to be. Our goal here is, given all of this training

00:03:33.250 --> 00:03:37.330
data, let's recover what the true f of x is.

00:03:37.330 --> 00:03:40.630
And that's what our H is. Each of our hypotheses a

00:03:40.630 --> 00:03:44.660
guess about what the true underlying deterministic function F is.

00:03:44.660 --> 00:03:48.210
So, if we have some particular labels, some particular value D

00:03:48.210 --> 00:03:51.340
sub I that is at variance with that. What's the probability of us

00:03:51.340 --> 00:03:53.440
seeing something that far away from the

00:03:53.440 --> 00:03:57.150
true underlying F. Well, it's completely determined

00:03:57.150 --> 00:04:00.091
by, the noise model. And the noise is a Gaussian. So, we can

00:04:00.091 --> 00:04:01.749
actually write down Gaussian. Do you remember

00:04:01.749 --> 00:04:03.985
what the equation for a Gaussian is?

00:04:03.985 --> 00:04:06.190
&gt;&gt; Yes. It's exactly something that has an E in it.

00:04:06.190 --> 00:04:09.630
&gt;&gt; That's right. So I'll see, I'm going to start writing it

00:04:09.630 --> 00:04:12.470
and you see if you remember any of what I'm writing down.

00:04:12.470 --> 00:04:13.720
&gt;&gt; E to the...

00:04:13.720 --> 00:04:14.190
&gt;&gt; No.

00:04:14.190 --> 00:04:14.772
&gt;&gt; Okay, good.

00:04:14.772 --> 00:04:16.612
&gt;&gt; It's 1 over.

00:04:16.612 --> 00:04:17.498
&gt;&gt; E to the.

00:04:17.498 --> 00:04:18.116
&gt;&gt; No.

00:04:18.116 --> 00:04:19.069
&gt;&gt; Okay.

00:04:19.069 --> 00:04:26.393
&gt;&gt; Square root of, it's, it's coming back to you now. 2 pi sigma squared.

00:04:26.393 --> 00:04:27.350
&gt;&gt; Okay.

00:04:27.350 --> 00:04:28.330
&gt;&gt; Times...

00:04:28.330 --> 00:04:29.830
&gt;&gt; I was going to put that in after.

00:04:29.830 --> 00:04:33.260
&gt;&gt; Oh, okay. So now you get your E, so E to the what?

00:04:33.260 --> 00:04:38.730
&gt;&gt; It's going to be the value, which, in our case, is, like, H of XI

00:04:38.730 --> 00:04:39.610
minus DI.

00:04:39.610 --> 00:04:44.870
&gt;&gt; Yeah. And then I feel like, we probably square that?

00:04:44.870 --> 00:04:45.570
&gt;&gt; Yep.

00:04:45.570 --> 00:04:47.810
&gt;&gt; And then we divide by sigma squared?

00:04:49.200 --> 00:04:49.490
&gt;&gt; right.

00:04:49.490 --> 00:04:50.170
&gt;&gt; Really?

00:04:50.170 --> 00:04:50.770
&gt;&gt; Yeah.

00:04:50.770 --> 00:04:51.480
&gt;&gt; Sweet!

00:04:51.480 --> 00:04:53.780
&gt;&gt; And your missing one tiny thing.

00:04:53.780 --> 00:04:55.060
&gt;&gt; There needs to be another two.

00:04:55.060 --> 00:04:57.500
&gt;&gt; Yes. And in fact it's minus one half.

00:04:57.500 --> 00:04:59.200
&gt;&gt; Got it.

00:04:59.200 --> 00:05:01.450
&gt;&gt; So, this is exactly the form of the Gaussian

00:05:01.450 --> 00:05:03.791
in the normal distribution. And what it basically says is the

00:05:03.791 --> 00:05:07.561
probability me seeing some particular point, in this case D of I. Given

00:05:07.561 --> 00:05:11.225
that the mean is H of X. Which is to say

00:05:11.225 --> 00:05:14.310
that's the underlying the function. Is exactly this expression. e to

00:05:14.310 --> 00:05:17.535
the minus one half, of the distance from the mean, squared,

00:05:17.535 --> 00:05:20.620
divided by the merits. Okay. And that's just, you either remember

00:05:20.620 --> 00:05:23.050
that or you don't. But that's just the definition of a

00:05:23.050 --> 00:05:26.590
Gaussian. So that means the probability of us seeing the data

00:05:26.590 --> 00:05:28.800
is the product of the probability of us seeing each of

00:05:28.800 --> 00:05:32.450
the data items. And that's just the product of this expression

00:05:32.450 --> 00:05:36.430
here. Good. Now, we need to simplify this. We could stop here

00:05:36.430 --> 00:05:38.910
because this is true, but we really need to simplify this and

00:05:38.910 --> 00:05:42.250
I think it's pretty... Not to hard to do. It's pretty easy.

00:05:42.250 --> 00:05:43.050
&gt;&gt; Mm..hm.

00:05:43.050 --> 00:05:45.880
&gt;&gt; What kind of trick do you think we would do here to simplify this?

00:05:45.880 --> 00:05:49.050
&gt;&gt; So, first thing I would do is, noticed that the 1 over square root 2 pi

00:05:49.050 --> 00:05:51.610
sigma square doesn't depend on i at all, and

00:05:51.610 --> 00:05:53.990
maybe move it outside the pi but then realize,

00:05:53.990 --> 00:05:57.530
well, actually since we're doing an argmax anyway, it's not going to have

00:05:57.530 --> 00:06:00.270
any impact at all. [CROSSTALK] I would just like cross that baby out.

00:06:00.270 --> 00:06:03.680
&gt;&gt; I like that. No point in keeping it. All right, now I'm

00:06:03.680 --> 00:06:07.120
hoping that the other sigma squared we can make that go away too.

00:06:07.120 --> 00:06:10.000
So I'm tempted to just cross it out, but I'd rather, I'd be

00:06:10.000 --> 00:06:13.450
much more happy if I had a good explanation for why that's okay.

00:06:13.450 --> 00:06:14.460
&gt;&gt; Well, so what's the normal trick,

00:06:14.460 --> 00:06:15.710
so we're trying to maximize the function,

00:06:15.710 --> 00:06:19.270
right? What you just said is we can get rid of this particular constant

00:06:19.270 --> 00:06:21.335
expression because it doesn't affect the max.

00:06:21.335 --> 00:06:22.550
What's making it hard for you to get

00:06:22.550 --> 00:06:24.466
rid of the sigma squared here is that

00:06:24.466 --> 00:06:26.470
it's being passed through some exponential and you

00:06:26.470 --> 00:06:27.790
can't remember off the top of your head

00:06:27.790 --> 00:06:29.750
what clever work you can do with constants

00:06:29.750 --> 00:06:31.180
inside of exponentials. So it would be nice

00:06:31.180 --> 00:06:32.580
if we could get rid of the exponential.

00:06:34.790 --> 00:06:39.170
&gt;&gt; Very good. So because log is concave.

00:06:39.170 --> 00:06:40.790
&gt;&gt; No, because it's monotonic.

00:06:40.790 --> 00:06:45.370
&gt;&gt; um-hm. We can take the log of the whole shabang. So this is going to

00:06:45.370 --> 00:06:50.390
be equal to the argmax of the sum of the log of that expression, which

00:06:50.390 --> 00:06:53.490
is going to move the thing to the outside and the log of E, so that's

00:06:53.490 --> 00:06:54.660
going to be good, so it's going to be

00:06:54.660 --> 00:06:57.439
the sum of the superscript thing, the power.

00:06:57.439 --> 00:07:00.240
&gt;&gt; Right. So let's write that down. Okay, so just

00:07:00.240 --> 00:07:02.290
to be sure that that was clear to everybody, let's just

00:07:02.290 --> 00:07:04.720
point out that we basically took the log of both, the

00:07:04.720 --> 00:07:07.020
natural log of both sides, and so we said, instead of

00:07:07.020 --> 00:07:10.260
trying to find the maximum hypothesis or the maximum likelihood hypothesis

00:07:10.260 --> 00:07:14.530
by evaluating this expression directly, we instead evaluated the log of

00:07:14.530 --> 00:07:19.160
that expression. And as you'll recall from intermediate algebra, the log

00:07:19.160 --> 00:07:21.190
of a product is the same as the sum of the

00:07:21.190 --> 00:07:25.480
logs, and the log of E to something is just that thing.

00:07:25.480 --> 00:07:26.940
&gt;&gt; As long as we do natural log.

00:07:26.940 --> 00:07:30.230
&gt;&gt; As long as we do natural log when we have E. If we were doing

00:07:30.230 --> 00:07:34.640
something to the, 2 to the power of something, we'd want to do log base 2. Okay.

00:07:34.640 --> 00:07:37.940
&gt;&gt; Got it. And you said to do it to both sides but we really didn't need to

00:07:37.940 --> 00:07:39.270
do it to both sides we just needed to

00:07:39.270 --> 00:07:41.314
do it inside the things we taking the argmax.

00:07:41.314 --> 00:07:43.340
&gt;&gt; That's correct. Okay, so we've got here. So,

00:07:43.340 --> 00:07:45.430
is there any other simplifying that we can do.

00:07:45.430 --> 00:07:46.950
&gt;&gt; Yeah, yeah now it seems much clearer

00:07:46.950 --> 00:07:50.550
so the. The negative one half divided by sigma

00:07:50.550 --> 00:07:54.410
squared all can move outside the sum cause it doesn't depend on I at all.

00:07:54.410 --> 00:07:56.270
&gt;&gt; Right. And then the sigma squared you

00:07:56.270 --> 00:07:58.402
said that before you said that that wasn't going

00:07:58.402 --> 00:08:00.815
to turn out to matter. Both sigma squares ended

00:08:00.815 --> 00:08:03.873
up, you know, getting tossed into the rubbish heap.

00:08:03.873 --> 00:08:04.356
&gt;&gt; That's right.

00:08:04.356 --> 00:08:06.721
&gt;&gt; And I want to be careful with the negative sign. Like I feel like

00:08:06.721 --> 00:08:09.345
the half can go and the sigma square can go but the negative has to stay.

00:08:09.345 --> 00:08:12.077
&gt;&gt; You're right. The half can go. And the sigma squared

00:08:12.077 --> 00:08:15.497
can go. So that leaves us with this expression. So I've taken,

00:08:15.497 --> 00:08:17.462
gotten rid of the one half, like you

00:08:17.462 --> 00:08:20.112
suggested. Got rid of the sigma squared like you

00:08:20.112 --> 00:08:22.617
suggested, and I moved the minus sign outside

00:08:22.617 --> 00:08:25.532
of the summation. And I'm left with this expression.

00:08:25.532 --> 00:08:28.497
&gt;&gt; I have a thought about getting rid of that minus sign.

00:08:28.497 --> 00:08:30.837
&gt;&gt; Well how would you get rid of a minus sign?

00:08:30.837 --> 00:08:34.238
&gt;&gt; So the max of a negative is the min. Right, so we can

00:08:34.238 --> 00:08:36.086
get rid of the minus sign by

00:08:36.086 --> 00:08:40.756
just simply minimizing instead of maximizing that expression.

00:08:40.756 --> 00:08:41.905
We end up with this expression.

00:08:41.905 --> 00:08:44.599
&gt;&gt; Nice. That's much simpler than where we started. The e is gone.

00:08:44.599 --> 00:08:46.338
&gt;&gt; It's much simpler. We got rid of a bunch

00:08:46.338 --> 00:08:47.896
of e's. We got rid of a bunch of turns

00:08:47.896 --> 00:08:50.966
out extraneous constants. We got rid of multiplication. We did

00:08:50.966 --> 00:08:53.226
a bunch of stuff, and we ended up with this.

00:08:53.226 --> 00:08:57.146
&gt;&gt; You know, we got rid of two pis. It's kind of sad I would like some pie.

00:08:57.146 --> 00:08:59.157
&gt;&gt; Mm, I wonder what kind of pie it was?

00:08:59.157 --> 00:08:59.788
&gt;&gt; Pecan pie?

00:08:59.788 --> 00:09:00.251
&gt;&gt; [LAUGH]

00:09:00.251 --> 00:09:03.054
&gt;&gt; Okay, so we got this expression, and that's kind of

00:09:03.054 --> 00:09:05.950
nice on your own you say, but actually it's even nicer

00:09:05.950 --> 00:09:06.556
than that.

00:09:06.556 --> 00:09:06.926
&gt;&gt; What?

00:09:06.926 --> 00:09:09.595
&gt;&gt; What does this expression remind you of Michael?

00:09:09.595 --> 00:09:10.689
&gt;&gt; The Sum of Squares.

00:09:10.689 --> 00:09:12.414
&gt;&gt; This is exactly it. This is, in

00:09:12.414 --> 00:09:15.352
fact. The sum of squared error, which is awesome.

00:09:15.352 --> 00:09:17.784
&gt;&gt; Yeah, whoever decided it would be a good idea

00:09:17.784 --> 00:09:20.882
to model noise as a Gaussian was really on to something.

00:09:20.882 --> 00:09:22.752
&gt;&gt; Mm-hm. Now, think about what this

00:09:22.752 --> 00:09:25.827
means, Michael. We just took, using Bayesian Learning,

00:09:25.827 --> 00:09:28.452
a very simple idea of maximizing a likelihood.

00:09:28.452 --> 00:09:30.793
We did nothing but substitution, here and there.

00:09:30.793 --> 00:09:33.260
With the noise model. We got rid of a bunch of things

00:09:33.260 --> 00:09:36.206
that we didn't have to get rid of. We cleverly used the natural

00:09:36.206 --> 00:09:38.709
log. Notice that the minus sign can be taken away with the

00:09:38.709 --> 00:09:42.062
min. And, we ended up with sum of squared error. Which suggests that

00:09:42.062 --> 00:09:45.194
all that stuff we were doing with back propagation. And, all these

00:09:45.194 --> 00:09:48.374
other kinds of things we're doing with receptrons is the right thing to

00:09:48.374 --> 00:09:51.512
do. Minimizing the sum of square error, which we've just been doing

00:09:51.512 --> 00:09:55.232
before. Is in fact the right thing to do according to Bayesian learning.

00:09:55.232 --> 00:09:55.882
&gt;&gt; Right in this

00:09:55.882 --> 00:09:58.107
case meaning meaning what a Bayesian would say.

00:09:58.107 --> 00:10:00.407
&gt;&gt; Meaning what a Bayesian would say which I believe

00:10:00.407 --> 00:10:02.117
is sort of right by definition. More importantly here it is.

00:10:02.117 --> 00:10:02.927
&gt;&gt; They certainly believe it.

00:10:02.927 --> 00:10:04.184
&gt;&gt; Well, they, they do frequently.

00:10:04.184 --> 00:10:07.432
&gt;&gt; Oh! I see what you did there.

00:10:07.432 --> 00:10:09.471
&gt;&gt; No one will get that but, but us. Anyway,

00:10:09.471 --> 00:10:11.965
the thing is this is the thing you should minimize if

00:10:11.965 --> 00:10:15.590
you're trying to find the maximum likelihood hypothesis. Now, I just

00:10:15.590 --> 00:10:21.029
want to say something. That is beautiful. Absolutely beautiful. That you

00:10:21.029 --> 00:10:24.263
do something very simple like finding the maximum likelihood

00:10:24.263 --> 00:10:27.585
hypothesis and you end up deriving sum of squared errors.

00:10:27.585 --> 00:10:28.421
&gt;&gt; So, just to make sure that

00:10:28.421 --> 00:10:30.339
I'm understanding. because I see some beauty here,

00:10:30.339 --> 00:10:34.393
but maybe not all of it. We didn't talk about what the hypothesis class here

00:10:34.393 --> 00:10:36.129
was. Right, so, if you don't know

00:10:36.129 --> 00:10:38.832
what the hypothesis class is... You're, you're kind

00:10:38.832 --> 00:10:40.457
of stalled at this point, but if we

00:10:40.457 --> 00:10:42.782
say the hypothesis class is say linear functions.

00:10:42.782 --> 00:10:43.157
&gt;&gt; Mm-hm.

00:10:43.157 --> 00:10:45.782
&gt;&gt; Then, what we're saying is we can do linear regression,

00:10:45.782 --> 00:10:48.657
because linear regression is exactly about minimizing the sum

00:10:48.657 --> 00:10:51.935
of the squares, right? So linear regression comes popping out

00:10:51.935 --> 00:10:55.037
of this kind of Bayesian perspective just like that, so

00:10:55.037 --> 00:10:57.413
is, is that part of what makes it so cool?

00:10:57.413 --> 00:10:58.568
&gt;&gt; That is part of what makes it cool, but I

00:10:58.568 --> 00:11:01.390
just think more generally about gradient descent right? The way gradient descend

00:11:01.390 --> 00:11:03.730
works is you take a derivative by stepping in this, in

00:11:03.730 --> 00:11:06.586
this space of the error function, which is sum of squared error.

00:11:06.586 --> 00:11:08.269
&gt;&gt; I see, so you get gradient descend too.

00:11:08.269 --> 00:11:10.712
&gt;&gt; Yes, you get all of the stuff that people have been doing.

00:11:10.712 --> 00:11:13.306
Now, there's a piece of beauty there, which is that we derived

00:11:13.306 --> 00:11:16.271
things like gradient descend and linear regression, all of the stuff we

00:11:16.271 --> 00:11:19.137
were talking about before and we have an argument. For why it's

00:11:19.137 --> 00:11:21.926
the right thing to do at least in a Bayesian sense. But

00:11:21.926 --> 00:11:25.412
there's an even deeper beauty here, which is tied in with ugliness,

00:11:25.412 --> 00:11:28.089
which is the reason this is the right thing to do, is

00:11:28.089 --> 00:11:31.662
because of the specific assumptions that we've made. So what were the

00:11:31.662 --> 00:11:35.750
assumptions that we made? We assumed that there was some True deterministic

00:11:35.750 --> 00:11:38.054
function that was mapping our x's to our in

00:11:38.054 --> 00:11:40.550
this case our d's and that they were corrupted

00:11:40.550 --> 00:11:43.302
say transmission error or line noise or however you

00:11:43.302 --> 00:11:46.356
want to think about it. They are corrupted by some

00:11:46.356 --> 00:11:50.930
noise that has a very particular form. Uncorrelated, independently

00:11:50.930 --> 00:11:54.730
drawn, Gaussian noise, with mean zero. So the less pretty

00:11:54.730 --> 00:11:58.113
way of thinking about it is. Whenever you're trying

00:11:58.113 --> 00:12:00.873
to minimize the sum of squared error, you are in

00:12:00.873 --> 00:12:04.461
fact assuming that the data that you have has been corrupted by

00:12:04.461 --> 00:12:07.798
Gaussian noise. And if it's corrupted by some other noise, or you're

00:12:07.798 --> 00:12:11.860
actually not trying to model determinance function, of this sort. And then

00:12:11.860 --> 00:12:15.250
you are in fact, possibly, in fact most likely doing the wrong thing.

00:12:15.250 --> 00:12:18.820
&gt;&gt; I mean are there other noise models

00:12:18.820 --> 00:12:21.440
that lead to some other kinds of learning.

00:12:21.440 --> 00:12:23.755
&gt;&gt; Sure, pick any other model in here that does't look

00:12:23.755 --> 00:12:26.420
Gaussian at all, and you would end up with something else.

00:12:26.420 --> 00:12:28.329
I don't know what you would end up with because. You

00:12:28.329 --> 00:12:30.720
know, you couldn't do all these cute tricks with natural logs but

00:12:30.720 --> 00:12:33.594
yes, you would end up with something different. And one question

00:12:33.594 --> 00:12:36.810
you might ask yourself is well, if I try to do minimizing

00:12:36.810 --> 00:12:39.480
the sum of the squared errors, or something for which this

00:12:39.480 --> 00:12:43.330
model was not the right one, what sort of bad things might

00:12:43.330 --> 00:12:48.040
happen? Here let me give you an example, let's imagine that we're

00:12:48.040 --> 00:12:52.615
looking at this here, and our X's are, I don't know measurements

00:12:52.615 --> 00:12:56.970
of people. Okay? So height and weight. Something like that.

00:12:56.970 --> 00:12:58.003
&gt;&gt; Mm-hm.

00:12:58.003 --> 00:13:00.000
&gt;&gt; And in fact let's make it, let's make it

00:13:00.000 --> 00:13:01.750
let's make it even simpler than that. Let's imagine that our

00:13:01.750 --> 00:13:05.490
x is our height. And our outputs, our d's, are

00:13:05.490 --> 00:13:07.790
say weight. And what we're trying to learn is some kind of

00:13:07.790 --> 00:13:09.976
function from height to weight. Now, this doesn't make a

00:13:09.976 --> 00:13:11.686
lot of sense to have a true [INAUDIBLE], but I'm trying

00:13:11.686 --> 00:13:14.300
to make a point here. So what we're saying here is

00:13:14.300 --> 00:13:18.430
that we, we measure our height and then we measure weight.

00:13:19.620 --> 00:13:21.670
That there's some simple relationship between

00:13:21.670 --> 00:13:24.620
them that's captured by f. But, when

00:13:24.620 --> 00:13:28.110
we measure the weight, we get a sort of noisy version of that

00:13:28.110 --> 00:13:31.630
weight. Okay? That seems reasonable. But

00:13:31.630 --> 00:13:33.730
what's not reasonable is we're saying. Our

00:13:33.730 --> 00:13:36.820
measurement of the weight is noisy, but our measurement of height is not.

00:13:38.110 --> 00:13:42.120
&gt;&gt; Because if the x's are noisy, then this is not a valid assumption.

00:13:42.120 --> 00:13:42.800
&gt;&gt; I see.

00:13:42.800 --> 00:13:45.110
&gt;&gt; So, it seems to work

00:13:45.110 --> 00:13:47.330
a lot of the time and we have an argument for

00:13:47.330 --> 00:13:50.640
when it will work, but it's not clear that this particular assumption

00:13:50.640 --> 00:13:53.210
actually makes a lot of sense in the real world. Even

00:13:53.210 --> 00:13:56.490
though in practice it seems to do just fine. Okay, got it?

00:13:56.490 --> 00:13:58.520
&gt;&gt; I think so though I feel like if the error if you put

00:13:58.520 --> 00:14:02.830
an error term inside the f along with the x and f is say linear.

00:14:02.830 --> 00:14:03.376
&gt;&gt; Mm-hm.

00:14:03.376 --> 00:14:06.451
&gt;&gt; Then maybe it pops out and it just becomes another

00:14:06.451 --> 00:14:10.200
part of the noise term and, and it all still goes through.

00:14:10.200 --> 00:14:12.670
Like I feel lines are still pretty happy even with that.

00:14:12.670 --> 00:14:14.803
&gt;&gt; No I think you're right. Lines would be happy here because

00:14:14.803 --> 00:14:17.410
linear, I mean linear functions are very nicely behaved in that way.

00:14:17.410 --> 00:14:19.500
But of course, they'd have to be the same noise model in

00:14:19.500 --> 00:14:20.850
order for it to work the way you want it to work.

00:14:20.850 --> 00:14:20.940
&gt;&gt; Yeah.

00:14:20.940 --> 00:14:23.390
&gt;&gt; They'd have to both be Gaussian. They have to both have

00:14:23.390 --> 00:14:26.360
zero mean, right? And they'd have to be independent of one another.

00:14:26.360 --> 00:14:30.910
So your measuring device that gives you an error for your height

00:14:30.910 --> 00:14:35.010
would also have to give you an independent normal error for the weight.

00:14:35.010 --> 00:14:36.010
&gt;&gt;

00:14:35.010 --> 00:14:36.960
Yeah. Though I feel like my scale and my

00:14:36.960 --> 00:14:41.410
yardstick actually are fairly independent. And they're Gaussian? .

00:14:41.410 --> 00:14:42.490
&gt;&gt; Oh mine is clearly Gaussian.

00:14:42.490 --> 00:14:42.850
&gt;&gt; Yeah.

00:14:42.850 --> 00:14:44.140
&gt;&gt; Yeah. Well at least they're normal.

00:14:44.140 --> 00:14:44.880
&gt;&gt; They're normally are.

00:14:44.880 --> 00:14:45.865
&gt;&gt; Mm-hm.

00:14:45.865 --> 00:14:49.310
&gt;&gt; Okay good. So let's move on to the next thing Michael. Let's try one

00:14:49.310 --> 00:14:52.830
more example of this and, and then I hope that means you got it, okay?

00:14:52.830 --> 00:14:53.670
&gt;&gt; Sure.

00:14:53.670 --> 00:14:54.100
&gt;&gt; Beautiful.

