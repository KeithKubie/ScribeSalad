WEBVTT
Kind: captions
Language: en

00:00:01.470 --> 00:00:04.490
MARK: Well, welcome
everyone to Talks at Google.

00:00:04.490 --> 00:00:06.130
We have a very
special guest today,

00:00:06.130 --> 00:00:08.724
which is Dr. Susan Murphy.

00:00:08.724 --> 00:00:11.993
Susan is a professor of
statistics and psychiatry

00:00:11.993 --> 00:00:14.845
at the University of Michigan.

00:00:14.845 --> 00:00:19.040
She's been doing
research on developing

00:00:19.040 --> 00:00:22.900
wearable mobile devices in
the health care industry.

00:00:22.900 --> 00:00:26.400
She's also received some
amazing awards, including

00:00:26.400 --> 00:00:33.245
a Fellow-- this one, actually,
is pretty impressive--

00:00:33.245 --> 00:00:36.916
the Fellowship as a
MacArthur Fellow in 2013,

00:00:36.916 --> 00:00:42.245
and received over $600,000 no
strings attached [INAUDIBLE]

00:00:42.245 --> 00:00:46.850
to do research, because she's
shown exceptional creativity

00:00:46.850 --> 00:00:48.150
and promised to do more.

00:00:48.150 --> 00:00:51.370
She's working on projects that
are helping a lot of people,

00:00:51.370 --> 00:00:52.884
which we'll hear about today.

00:00:52.884 --> 00:00:54.692
So thanks for joining us.

00:00:54.692 --> 00:00:56.319
I'll turn the time
over to Susan.

00:00:56.319 --> 00:00:57.443
SUSAN MURPHY: Thanks, Mark.

00:01:00.200 --> 00:01:00.700
OK.

00:01:00.700 --> 00:01:05.019
It's a pleasure to be
here with all of you.

00:01:05.019 --> 00:01:07.870
So you can see from
this first slide

00:01:07.870 --> 00:01:10.579
that this is about
mobile health,

00:01:10.579 --> 00:01:11.870
and I've been really fortunate.

00:01:11.870 --> 00:01:17.360
I'm part of multiple groups
that work in mobile health.

00:01:17.360 --> 00:01:19.620
So here I have my
little pictures,

00:01:19.620 --> 00:01:22.160
one on physical activity.

00:01:22.160 --> 00:01:25.970
We also have some work in
smoking cessation, right now

00:01:25.970 --> 00:01:31.480
in alcoholism, drug abuse,
also congestive heart failure

00:01:31.480 --> 00:01:33.180
as well.

00:01:33.180 --> 00:01:37.610
And that involves most of
these different groups.

00:01:37.610 --> 00:01:39.670
And today we're going to
spend most of our time

00:01:39.670 --> 00:01:41.600
when I talk about
an example, I'll

00:01:41.600 --> 00:01:45.590
talk about the physical
activity study.

00:01:45.590 --> 00:01:51.970
So what I'm going to do here is
even as a quantitative person,

00:01:51.970 --> 00:01:56.120
it's really important
to be careful about what

00:01:56.120 --> 00:02:01.060
are all the parts of
a project or an entity

00:02:01.060 --> 00:02:02.040
that you're building.

00:02:02.040 --> 00:02:04.434
In this case, it's a
mobile health intervention.

00:02:04.434 --> 00:02:06.350
And you have to think,
what are all the parts,

00:02:06.350 --> 00:02:08.570
so that you can
think of each part

00:02:08.570 --> 00:02:10.849
and then how to
optimize that part.

00:02:10.849 --> 00:02:12.890
And so that's what I'm
going to go through first.

00:02:12.890 --> 00:02:14.390
So it's going to
all be very verbal,

00:02:14.390 --> 00:02:16.010
but I'm talking--
because one needs

00:02:16.010 --> 00:02:19.650
to think about each one of
these parts to optimize them.

00:02:19.650 --> 00:02:21.650
So this is adaptive
interventions.

00:02:21.650 --> 00:02:25.070
Adaptive interventions--
that's a name of a design

00:02:25.070 --> 00:02:29.320
that mobile health interventions
fit within that name.

00:02:29.320 --> 00:02:32.280
And the idea is these
are interventions,

00:02:32.280 --> 00:02:35.990
where you want to tailor the
intervention to the person.

00:02:35.990 --> 00:02:39.520
One thinks that there may be
systematic reasons why people

00:02:39.520 --> 00:02:41.870
respond differently,
and to the extent

00:02:41.870 --> 00:02:44.130
that you can use those
systematic reasons

00:02:44.130 --> 00:02:47.680
to personalize treatment,
you'd like to do that.

00:02:47.680 --> 00:02:50.270
And what I'm going to do is
I'm going to show you an older

00:02:50.270 --> 00:02:52.940
example, and the reason
why I'm showing this to you

00:02:52.940 --> 00:02:55.380
is-- it's very simple.

00:02:55.380 --> 00:02:57.230
It doesn't involve
mobile health,

00:02:57.230 --> 00:03:01.160
but it helps really
communicate the ideas.

00:03:01.160 --> 00:03:04.750
And it's, for me,
very interesting is

00:03:04.750 --> 00:03:07.500
that we in the mobile
health field, we forget--

00:03:07.500 --> 00:03:10.970
in the health field in general--
we forget that a lot of what we

00:03:10.970 --> 00:03:14.780
do has a lot of relevance
for social problems

00:03:14.780 --> 00:03:17.370
and social programs.

00:03:17.370 --> 00:03:19.880
And this really gets at that.

00:03:19.880 --> 00:03:22.700
So this is an adapted
drug court program

00:03:22.700 --> 00:03:24.400
for drug abusing offenders.

00:03:24.400 --> 00:03:26.360
So what happens is
you get arrested,

00:03:26.360 --> 00:03:29.660
and you're offered
the opportunity

00:03:29.660 --> 00:03:35.482
to bypass jail and go
into a drug court program,

00:03:35.482 --> 00:03:37.440
and that's what we're
talking about individuals

00:03:37.440 --> 00:03:39.120
who had that opportunity.

00:03:39.120 --> 00:03:42.030
And what happens is these
individuals come in,

00:03:42.030 --> 00:03:47.500
and this is a study that
went on in Philly, actually.

00:03:47.500 --> 00:03:49.720
And I was a consultant
on this study,

00:03:49.720 --> 00:03:52.170
and they wanted to develop an
adaptive intervention, which

00:03:52.170 --> 00:03:54.170
they called the Adaptive
Drug Court Program.

00:03:54.170 --> 00:03:56.160
This is a big coup
actually, because this

00:03:56.160 --> 00:03:58.906
is working with the judicial
system, which is nontrivial.

00:03:58.906 --> 00:04:01.030
AUDIENCE: Are we able to
ask questions [INAUDIBLE]?

00:04:01.030 --> 00:04:02.654
SUSAN MURPHY: I think
it would be nice.

00:04:02.654 --> 00:04:04.340
Is that OK?

00:04:04.340 --> 00:04:04.840
MARK: Sure.

00:04:04.840 --> 00:04:05.631
SUSAN MURPHY: Yeah.

00:04:05.631 --> 00:04:07.280
AUDIENCE: In a
program like this,

00:04:07.280 --> 00:04:11.594
Mark had talked about your
research receiving funds.

00:04:11.594 --> 00:04:15.261
Is this funded by
a grant, or is this

00:04:15.261 --> 00:04:18.410
funded by a university that's
thinking this is interesting?

00:04:18.410 --> 00:04:19.910
Is it funded by the
judicial system?

00:04:19.910 --> 00:04:20.701
SUSAN MURPHY: Yeah.

00:04:20.701 --> 00:04:21.209
Right.

00:04:21.209 --> 00:04:22.640
This is a good question.

00:04:22.640 --> 00:04:28.280
So he asked me, how do these
types of projects get funded?

00:04:28.280 --> 00:04:30.900
And in the world I live
in, for the most part,

00:04:30.900 --> 00:04:36.620
we're always interested
in edgy kind of projects.

00:04:36.620 --> 00:04:39.922
So they tend to be funded
by NIH or foundations

00:04:39.922 --> 00:04:43.370
like the Ford Foundation
or some other foundation.

00:04:43.370 --> 00:04:48.140
Autism Speaks has funded some
of our work, for example.

00:04:48.140 --> 00:04:53.090
You also see work funded
by industry as well.

00:04:53.090 --> 00:04:54.920
Every now and then,
little bitty bits

00:04:54.920 --> 00:04:56.450
will get funded
by the university,

00:04:56.450 --> 00:04:58.557
but that's not really
where the money is.

00:04:58.557 --> 00:05:03.310
AUDIENCE: So it alleviates then
the court systems or taxpayers

00:05:03.310 --> 00:05:04.561
were paying for some of these.

00:05:04.561 --> 00:05:05.476
SUSAN MURPHY: Oh yeah.

00:05:05.476 --> 00:05:07.850
The taxpayers-- they were
paying for it, but not

00:05:07.850 --> 00:05:09.290
through the judicial system.

00:05:09.290 --> 00:05:11.040
It was via a
research project that

00:05:11.040 --> 00:05:16.020
was funded by the National
Institutes of Health.

00:05:16.020 --> 00:05:18.000
So the whole goal
was to figure out,

00:05:18.000 --> 00:05:21.260
can we produce a good program?

00:05:21.260 --> 00:05:23.240
So people were
classifying-- what you

00:05:23.240 --> 00:05:24.680
see is a decision tree here.

00:05:24.680 --> 00:05:27.640
And you can think of most
adaptive interventions as akin

00:05:27.640 --> 00:05:29.320
to decision trees.

00:05:29.320 --> 00:05:31.500
People were classified
into low risk or high risk.

00:05:31.500 --> 00:05:36.030
This is a diagnosis of
antisocial personality disorder

00:05:36.030 --> 00:05:38.020
or formal drug abuse treatment.

00:05:38.020 --> 00:05:39.270
They're brought in.

00:05:39.270 --> 00:05:42.380
They're provided standard
group-based drug abuse

00:05:42.380 --> 00:05:46.360
counseling, and they're
provided two different options

00:05:46.360 --> 00:05:48.360
of a very expensive
intervention,

00:05:48.360 --> 00:05:49.720
judicial supervision.

00:05:49.720 --> 00:05:52.120
So that's where the game
is here, the expense

00:05:52.120 --> 00:05:53.880
of judicial supervision.

00:05:53.880 --> 00:05:55.560
Can you reduce that?

00:05:55.560 --> 00:05:58.390
And can you only step
it up if you need to?

00:05:58.390 --> 00:06:02.260
Every month they're monitored,
and at the end of each month,

00:06:02.260 --> 00:06:04.190
they could be doing well.

00:06:04.190 --> 00:06:05.270
Everything's fine.

00:06:05.270 --> 00:06:06.930
Or they might be non-compliant.

00:06:06.930 --> 00:06:09.460
That means they're not following
the rules of the drug court

00:06:09.460 --> 00:06:10.380
program.

00:06:10.380 --> 00:06:13.330
They're not showing up to
enough drug abuse sessions--

00:06:13.330 --> 00:06:16.310
two or more-- or they're
not providing urine screens

00:06:16.310 --> 00:06:18.340
every week.

00:06:18.340 --> 00:06:20.890
If they're non-compliant,
the judicial supervision

00:06:20.890 --> 00:06:24.760
is stepped up at the
end of that month.

00:06:24.760 --> 00:06:26.410
If they're doing
well, nothing happens.

00:06:26.410 --> 00:06:27.785
They just stay in
the same thing.

00:06:27.785 --> 00:06:29.400
If they're
non-responsive, that's

00:06:29.400 --> 00:06:31.240
someone who's
really trying hard,

00:06:31.240 --> 00:06:35.330
but they're providing positive
urine screens-- more than two.

00:06:35.330 --> 00:06:39.200
And then you don't want to
up the judicial supervision.

00:06:39.200 --> 00:06:43.920
Instead, you want to increase
the support for drug abuse

00:06:43.920 --> 00:06:44.640
therapy.

00:06:44.640 --> 00:06:46.930
And so they move to what's
called intensive case

00:06:46.930 --> 00:06:47.580
management.

00:06:47.580 --> 00:06:51.740
This includes
individual-based therapy

00:06:51.740 --> 00:06:54.400
and a lot of different
supportive services.

00:06:54.400 --> 00:06:56.520
The whole goal of
this project was

00:06:56.520 --> 00:07:01.050
to help people graduate
from the drug program, as

00:07:01.050 --> 00:07:03.370
opposed to going back
into jail, which is

00:07:03.370 --> 00:07:04.935
a very expensive intervention.

00:07:07.620 --> 00:07:09.630
So this is an
adaptive intervention,

00:07:09.630 --> 00:07:12.490
and it has these
elements that one really

00:07:12.490 --> 00:07:15.220
needs to think about when
you think about forming

00:07:15.220 --> 00:07:16.770
one of these interventions.

00:07:16.770 --> 00:07:18.870
So there's always some
sort of distal outcome.

00:07:18.870 --> 00:07:22.000
In this case, it was
helping people graduate

00:07:22.000 --> 00:07:24.110
from the drug abuse program.

00:07:24.110 --> 00:07:26.510
You can imagine
in other settings,

00:07:26.510 --> 00:07:29.940
that distal outcome might
be a tradeoff between cost

00:07:29.940 --> 00:07:32.960
and benefit.

00:07:32.960 --> 00:07:35.200
There's always a
proximal response here.

00:07:35.200 --> 00:07:37.570
In our case, the proximal
response was whether or not

00:07:37.570 --> 00:07:41.150
you were compliant each
month, or you were responsive,

00:07:41.150 --> 00:07:44.910
whether or not you
have positive urines.

00:07:44.910 --> 00:07:47.230
So that's what we're
looking-- we're

00:07:47.230 --> 00:07:49.540
looking to see how you're
responding, compliance.

00:07:49.540 --> 00:07:50.710
You're responsive.

00:07:50.710 --> 00:07:53.050
And then the decision
tree is formed

00:07:53.050 --> 00:07:55.740
by what people in this field
call tailoring variables.

00:07:55.740 --> 00:07:58.340
Those are the variables
we use to make decisions.

00:07:58.340 --> 00:08:00.070
In our case, they
are, in fact, the same

00:08:00.070 --> 00:08:02.649
as the proximal response.

00:08:02.649 --> 00:08:04.690
The decision rules, which
is that little decision

00:08:04.690 --> 00:08:07.790
tree, kinds of treatments
we have-- We're ready.

00:08:07.790 --> 00:08:11.400
We're able to provide levels
of judicial supervision, levels

00:08:11.400 --> 00:08:13.750
of drug abuse therapy.

00:08:13.750 --> 00:08:15.440
And then you trigger--
you have to think

00:08:15.440 --> 00:08:18.680
about when you're going to
trigger the decision rules.

00:08:18.680 --> 00:08:21.320
And in a drug abuse
program, it was monthly,

00:08:21.320 --> 00:08:25.010
so at very coarse
intervals in time.

00:08:25.010 --> 00:08:29.350
And all of this has to be
thought carefully about if you

00:08:29.350 --> 00:08:32.919
want to get something that's
both cost-effective and

00:08:32.919 --> 00:08:35.280
effective.

00:08:35.280 --> 00:08:38.919
So let's go to a more
of a mobile setting.

00:08:38.919 --> 00:08:44.510
So here, people call these just
in time adaptive interventions

00:08:44.510 --> 00:08:47.290
to communicate the
fact that you're

00:08:47.290 --> 00:08:51.060
trying to intervene at the
time, in the moment, where

00:08:51.060 --> 00:08:56.150
a person really can use help
at 2:00 AM in the stairwell,

00:08:56.150 --> 00:08:58.050
delivered when needed,
wherever needed.

00:09:00.890 --> 00:09:05.570
So here's a really simple
example that I like.

00:09:05.570 --> 00:09:07.400
It's highly simplistic.

00:09:07.400 --> 00:09:10.020
Nowadays things have gotten
much more sophisticated,

00:09:10.020 --> 00:09:12.210
and this is where you
have software that's

00:09:12.210 --> 00:09:17.100
monitoring your mouse activity,
as well as your keyboard

00:09:17.100 --> 00:09:19.170
activity on the computer.

00:09:19.170 --> 00:09:20.680
You're an office worker.

00:09:20.680 --> 00:09:25.200
And if you go 30 minutes
with uninterrupted activity,

00:09:25.200 --> 00:09:28.770
you get a ping, and it suggests
that you stand up and walk

00:09:28.770 --> 00:09:29.480
around the room.

00:09:29.480 --> 00:09:32.270
Actually, there's a variety of
different kinds of messages,

00:09:32.270 --> 00:09:34.870
and they had the messages
compete with each other

00:09:34.870 --> 00:09:38.420
to see which one would be
best for you in what setting.

00:09:38.420 --> 00:09:41.111
So we're going to be talking
about this in the mobile health

00:09:41.111 --> 00:09:41.610
setting.

00:09:41.610 --> 00:09:44.390
This is not mobile,
but it is just in time.

00:09:44.390 --> 00:09:47.030
It has that connotation.

00:09:47.030 --> 00:09:50.470
So what's common about
adaptive interventions

00:09:50.470 --> 00:09:54.070
and these just in time
adaptive interventions

00:09:54.070 --> 00:09:56.490
is they're time
varying treatments.

00:09:56.490 --> 00:09:58.742
You see treatments
changing over time.

00:09:58.742 --> 00:10:00.700
The idea is you're going
to adapt the treatment

00:10:00.700 --> 00:10:01.510
to the individual.

00:10:01.510 --> 00:10:04.780
You're going to personalize
treatment in some way.

00:10:04.780 --> 00:10:08.100
However, in our just
in time intervention,

00:10:08.100 --> 00:10:09.680
it's technology-based.

00:10:09.680 --> 00:10:12.380
It's really focused,
and, indeed, often,

00:10:12.380 --> 00:10:15.900
you'd want access to wearable
sensor streams and so on.

00:10:15.900 --> 00:10:20.014
You want to be able
to collect information

00:10:20.014 --> 00:10:22.680
wherever it's needed so that you
get the right kind of tailoring

00:10:22.680 --> 00:10:24.054
variable, so you
know what to do.

00:10:24.054 --> 00:10:27.030
You know you can identify
moments of opportunity,

00:10:27.030 --> 00:10:28.141
moments of risk.

00:10:28.141 --> 00:10:30.140
And you want to be able
to deliver interventions

00:10:30.140 --> 00:10:33.780
in the real life of the person.

00:10:33.780 --> 00:10:36.520
So it's the exact
same game as before.

00:10:36.520 --> 00:10:38.500
The components are
the exact same.

00:10:38.500 --> 00:10:41.750
There's nothing
spectacularly new here.

00:10:41.750 --> 00:10:45.570
It's just that everything
becomes more in the moment.

00:10:45.570 --> 00:10:48.240
At first, I used
to say momentary,

00:10:48.240 --> 00:10:51.040
but I don't want to
give you the impression

00:10:51.040 --> 00:10:54.230
that the effects
might be fleeting.

00:10:54.230 --> 00:10:55.770
But they're in
the moment, so I'm

00:10:55.770 --> 00:10:59.020
trying to help you with
your craving right now.

00:10:59.020 --> 00:11:03.280
I'm trying to help you
be active right now.

00:11:03.280 --> 00:11:05.072
And the decision
points are real time.

00:11:05.072 --> 00:11:07.280
And what I'm going to do on
the next couple of slides

00:11:07.280 --> 00:11:09.490
is I'm going to go
through each one of these,

00:11:09.490 --> 00:11:11.550
and it turns out
right now, the state

00:11:11.550 --> 00:11:16.380
of the field in mobile
health-- it's pretty immature.

00:11:16.380 --> 00:11:19.320
And this makes it really
exciting for scientists

00:11:19.320 --> 00:11:22.900
and technology developers,
because most of this,

00:11:22.900 --> 00:11:25.960
we don't have good--
we don't really

00:11:25.960 --> 00:11:27.610
know how to optimize yet.

00:11:27.610 --> 00:11:29.190
We know that we got problems.

00:11:29.190 --> 00:11:31.900
We know where we need to go, but
we haven't gotten it straight

00:11:31.900 --> 00:11:34.580
in our head how to optimize it.

00:11:34.580 --> 00:11:38.990
So we're going to start
off by the distal outcome.

00:11:38.990 --> 00:11:42.260
So the distal outcome
is some long term goal

00:11:42.260 --> 00:11:44.220
that you want to achieve.

00:11:44.220 --> 00:11:46.330
And in the world
I live in, which

00:11:46.330 --> 00:11:49.960
is more on the research,
more research side,

00:11:49.960 --> 00:11:52.530
it usually is some sort
of clinical outcome

00:11:52.530 --> 00:11:54.620
like I'm trying
to delay the time

00:11:54.620 --> 00:11:57.110
to relapse in substance use.

00:11:57.110 --> 00:12:01.580
I'm trying to reduce the rate
at which people binge eat.

00:12:01.580 --> 00:12:05.930
I'm trying to keep people
from drinking heavily,

00:12:05.930 --> 00:12:08.040
binge drinking, and so on.

00:12:08.040 --> 00:12:11.230
And a lot of areas maintain
adherence to medications.

00:12:11.230 --> 00:12:13.910
It's very common.

00:12:13.910 --> 00:12:18.270
The whole idea here is you
formulate your intervention

00:12:18.270 --> 00:12:21.320
options, the interventions
that you might provide

00:12:21.320 --> 00:12:24.930
so that you'll affect
these proximal responses,

00:12:24.930 --> 00:12:27.360
and thus achieve
your distal outcome.

00:12:27.360 --> 00:12:30.170
So in the behavioral
science world,

00:12:30.170 --> 00:12:35.510
this fits in very well
with the way the theories

00:12:35.510 --> 00:12:37.355
with which they
guide their thinking.

00:12:40.160 --> 00:12:43.864
So proximal responses--
they're often called mediators.

00:12:43.864 --> 00:12:45.280
I don't know if
you all would have

00:12:45.280 --> 00:12:47.860
heard those words, mediators.

00:12:47.860 --> 00:12:50.390
They're used a lot in
the behavioral sciences,

00:12:50.390 --> 00:12:55.390
and if you think of A causes
B, and then B causes C,

00:12:55.390 --> 00:12:57.105
B is the mediator.

00:12:57.105 --> 00:12:59.610
A doesn't cause C directly.

00:12:59.610 --> 00:13:02.420
It causes B, and
then B causes C.

00:13:02.420 --> 00:13:05.880
So there's this causal chain
that you want to impact.

00:13:05.880 --> 00:13:07.400
So here, I've
given some examples

00:13:07.400 --> 00:13:09.110
of what the proximal
response might

00:13:09.110 --> 00:13:12.680
be-- substance use
over the near time,

00:13:12.680 --> 00:13:17.270
if you're interested in reducing
relapse, a physical activity,

00:13:17.270 --> 00:13:22.830
if you're trying to help people
who want to be less obese,

00:13:22.830 --> 00:13:26.280
medication adherence
over the next time.

00:13:26.280 --> 00:13:28.820
So this is the same as the
short term targeted behavior.

00:13:28.820 --> 00:13:30.740
The distal outcome
is a behavior.

00:13:30.740 --> 00:13:33.630
The proximal response
is a behavior.

00:13:33.630 --> 00:13:37.910
In other settings, it's some
measure of short-term risk.

00:13:37.910 --> 00:13:41.170
So for example, in
smoking cessation,

00:13:41.170 --> 00:13:44.900
one of the big determinants
of why people tend to relapse

00:13:44.900 --> 00:13:47.640
is having problems
with stress management,

00:13:47.640 --> 00:13:50.620
because smoking is used
to help you manage stress.

00:13:50.620 --> 00:13:53.800
And so stress would be a
classical kind of mediator

00:13:53.800 --> 00:13:56.810
that you might want to impact.

00:13:56.810 --> 00:13:58.570
And the last one--
this is something

00:13:58.570 --> 00:14:01.560
that we end up in the
mobile health field,

00:14:01.560 --> 00:14:06.530
we're really starting to realize
how absolutely critical it is.

00:14:06.530 --> 00:14:09.190
And I think someone
in business would've

00:14:09.190 --> 00:14:13.280
realized this maybe even
earlier-- is engagement.

00:14:13.280 --> 00:14:15.860
So if you're dealing
with a problem, which

00:14:15.860 --> 00:14:19.760
is chronic like, for
example, substance use,

00:14:19.760 --> 00:14:23.764
being physically active, eating
well, managing your stress,

00:14:23.764 --> 00:14:25.180
this is something
you have to deal

00:14:25.180 --> 00:14:27.430
with for long periods of time.

00:14:27.430 --> 00:14:29.640
If we don't keep
you engaged, when

00:14:29.640 --> 00:14:31.635
things start to go
downhill, that app

00:14:31.635 --> 00:14:33.490
will not be there for you.

00:14:33.490 --> 00:14:39.640
So engagement could be another
type of proximal response.

00:14:39.640 --> 00:14:42.180
The kinds of intervention
options-- this,

00:14:42.180 --> 00:14:45.250
too, is very-- it's
quite interesting.

00:14:45.250 --> 00:14:46.750
I didn't realize
there were so many,

00:14:46.750 --> 00:14:49.795
but it's a vast array of
different kinds of ways

00:14:49.795 --> 00:14:53.080
that we can try and
help people, ways

00:14:53.080 --> 00:14:56.780
to reframe a relapse,
alternative activities

00:14:56.780 --> 00:15:00.450
to help me deal with
craving for a cigarette.

00:15:00.450 --> 00:15:03.770
Maybe there's different
interventions,

00:15:03.770 --> 00:15:06.270
little interventions
that are on the phone,

00:15:06.270 --> 00:15:09.200
residing on my
smartphone, yet I don't

00:15:09.200 --> 00:15:11.110
I think when I
should access them,

00:15:11.110 --> 00:15:13.550
because I'm not so self-aware.

00:15:13.550 --> 00:15:14.700
I'm so stressed out.

00:15:14.700 --> 00:15:16.360
I'm not that aware right now.

00:15:16.360 --> 00:15:19.340
So it might be a nudge
to access an intervention

00:15:19.340 --> 00:15:22.700
that I can obtain on the phone.

00:15:22.700 --> 00:15:26.580
Self-monitoring-- prompt me
to record my diet or something

00:15:26.580 --> 00:15:27.750
at that time.

00:15:27.750 --> 00:15:31.040
And of course, there's the
provide nothing option, which

00:15:31.040 --> 00:15:33.930
actually is one of the
best options of all,

00:15:33.930 --> 00:15:38.140
because I don't know how you
all are, but my smartphone

00:15:38.140 --> 00:15:40.960
and me-- we stay
together all the time.

00:15:40.960 --> 00:15:44.480
When I take a shower, my
smartphone is on the sink.

00:15:44.480 --> 00:15:46.820
When I go to bed, my
smartphone is next-- I mean, me

00:15:46.820 --> 00:15:49.650
and my smartphone-- we're
together all the time.

00:15:49.650 --> 00:15:53.950
And so the issue of
burden and aggravation

00:15:53.950 --> 00:15:56.390
is profound in this domain.

00:15:56.390 --> 00:16:00.200
It's much more profound than
in any other area of health

00:16:00.200 --> 00:16:03.960
I know of in terms of just
aggravation to the person.

00:16:03.960 --> 00:16:07.910
So the provide nothing option
is, of course, a good one.

00:16:07.910 --> 00:16:09.910
So tailoring variables--
these are the variables

00:16:09.910 --> 00:16:15.340
you use to decide whether or not
to provide a push or support.

00:16:15.340 --> 00:16:19.190
Often, they are
proximal responses

00:16:19.190 --> 00:16:21.160
you've already observed.

00:16:21.160 --> 00:16:25.940
Whether or not you're
going into a risky period,

00:16:25.940 --> 00:16:29.750
like if I want to suggest that
you might go and go for a walk

00:16:29.750 --> 00:16:32.250
or somehow break your
sedentary behavior,

00:16:32.250 --> 00:16:35.500
I might want to look at
the busyness of your Google

00:16:35.500 --> 00:16:37.750
Calendar over the
next couple hours,

00:16:37.750 --> 00:16:40.239
because it might not be worth
it to suggest you go for

00:16:40.239 --> 00:16:42.280
walk if you're going
straight into a whole series

00:16:42.280 --> 00:16:44.230
of meetings, and so on.

00:16:44.230 --> 00:16:45.960
And of course,
adherence, too, might

00:16:45.960 --> 00:16:50.870
be if you're showing signs that
you're becoming more aggravated

00:16:50.870 --> 00:16:52.080
with the intervention.

00:16:52.080 --> 00:16:53.840
Maybe I need to back off.

00:16:53.840 --> 00:16:55.000
The phone should back off.

00:16:55.000 --> 00:16:57.090
The algorithm should
back off [INAUDIBLE].

00:16:57.090 --> 00:17:01.010
So these all could be
tailoring variables.

00:17:01.010 --> 00:17:02.530
Decision points--
these are the time

00:17:02.530 --> 00:17:05.170
points at which you have
to decide whether or not

00:17:05.170 --> 00:17:08.230
you might intervene.

00:17:08.230 --> 00:17:11.510
So this is complicated,
and it's something

00:17:11.510 --> 00:17:15.470
that we really don't have
quite straight in the science,

00:17:15.470 --> 00:17:18.680
because it's tailored to the
dynamics of whatever process

00:17:18.680 --> 00:17:20.109
you're trying to impact.

00:17:20.109 --> 00:17:22.730
So with stress, for
example, we don't really

00:17:22.730 --> 00:17:26.400
understand how volatile
stress might be for people

00:17:26.400 --> 00:17:28.089
who are trying to quit smoking.

00:17:28.089 --> 00:17:32.050
How volatile is it in a
dynamic way over the day?

00:17:32.050 --> 00:17:36.470
So how often might we want
to intervene during the day?

00:17:36.470 --> 00:17:37.970
That's not very clear.

00:17:37.970 --> 00:17:39.920
In the smoking
cessation study, we

00:17:39.920 --> 00:17:43.110
decided-- I'm not going to
talk about that study here,

00:17:43.110 --> 00:17:45.740
but we have decision
points at every minute,

00:17:45.740 --> 00:17:48.480
because we don't know
yet how volatile it is,

00:17:48.480 --> 00:17:50.480
so we picked the smallest
time at which we could

00:17:50.480 --> 00:17:54.290
get classifications of stress.

00:17:54.290 --> 00:17:56.210
Also, the intervention
may only have

00:17:56.210 --> 00:18:00.750
an impact over a short period
of time, and that'll inform.

00:18:00.750 --> 00:18:02.710
The decision
rules-- so these can

00:18:02.710 --> 00:18:04.650
be decision rules
like a decision tree

00:18:04.650 --> 00:18:06.420
like I showed you
before, except in more

00:18:06.420 --> 00:18:08.560
of a real-time kind of setting.

00:18:08.560 --> 00:18:11.330
They can be if/then
kind of rules.

00:18:11.330 --> 00:18:15.060
In some settings, they're
black box type rules

00:18:15.060 --> 00:18:17.355
that it's an output
of an algorithm.

00:18:21.150 --> 00:18:22.970
If you go to the
behavioral sciences,

00:18:22.970 --> 00:18:24.800
and you go to the
research field,

00:18:24.800 --> 00:18:30.030
most people hand tailor,
hand tune the decision rules.

00:18:30.030 --> 00:18:34.540
There's some indirect use of
data, but for the large part,

00:18:34.540 --> 00:18:36.380
people hand tune it.

00:18:36.380 --> 00:18:39.290
And so what this talk from
now on is going to be about

00:18:39.290 --> 00:18:41.710
is how we might
collect data so that we

00:18:41.710 --> 00:18:44.852
could use data to inform that
tuning of these decision rules.

00:18:47.667 --> 00:18:49.250
Oh, here's just an
example in the case

00:18:49.250 --> 00:18:51.695
of the sedentary computer
behavior of a decision

00:18:51.695 --> 00:18:55.970
rule in the if/then framework,
as opposed to a decision tree.

00:18:55.970 --> 00:18:57.660
Just thought I'd
give you an example.

00:18:57.660 --> 00:18:59.460
So here's my tailoring variable.

00:18:59.460 --> 00:19:00.890
Here's an intervention option.

00:19:00.890 --> 00:19:04.170
We have a threshold on
the tailoring variable.

00:19:04.170 --> 00:19:07.730
We might want to learn with data
what the best threshold might

00:19:07.730 --> 00:19:09.740
be, and that
threshold might vary

00:19:09.740 --> 00:19:14.570
by person, because my
tolerance of being pinged

00:19:14.570 --> 00:19:16.360
might be lower than
someone else's,

00:19:16.360 --> 00:19:18.520
so you might have to have
a higher threshold for me

00:19:18.520 --> 00:19:19.900
than someone else.

00:19:19.900 --> 00:19:23.950
Or in this particular setting,
whatever's going on today

00:19:23.950 --> 00:19:29.490
may mean my threshold for
being pinged is lower.

00:19:29.490 --> 00:19:31.390
So here's the summary.

00:19:31.390 --> 00:19:32.940
I'm just going to
skip this, but it's

00:19:32.940 --> 00:19:36.720
a summary of the way behavioral
scientists might think

00:19:36.720 --> 00:19:38.450
about when they
think about forming

00:19:38.450 --> 00:19:40.604
one of these interventions.

00:19:40.604 --> 00:19:42.020
So I want to go
on to Heart Steps.

00:19:42.020 --> 00:19:45.140
Heart Steps is a study
that just-- well, actually,

00:19:45.140 --> 00:19:45.760
it says set.

00:19:45.760 --> 00:19:47.710
This is one of
these studies that's

00:19:47.710 --> 00:19:50.810
funded by the National
Institutes of Health,

00:19:50.810 --> 00:19:56.620
and it's a study that involves
three different trials.

00:19:56.620 --> 00:19:59.620
In the first trial,
the last two trials

00:19:59.620 --> 00:20:02.140
would be with individuals
who had a heart attack,

00:20:02.140 --> 00:20:04.430
and we're trying to
help them stay active

00:20:04.430 --> 00:20:06.310
after the heart
attack to prevent

00:20:06.310 --> 00:20:09.050
future cardiovascular problems.

00:20:09.050 --> 00:20:11.040
The first study's just
for sedentary people,

00:20:11.040 --> 00:20:12.810
because we're trying--
if you've ever

00:20:12.810 --> 00:20:15.760
worked a lot with
technology, there's

00:20:15.760 --> 00:20:18.730
really a lot of bugs you've got
to deal with in this setting.

00:20:18.730 --> 00:20:20.450
So that's Heart
Steps, and that's

00:20:20.450 --> 00:20:22.410
what we're going
to talk about next.

00:20:22.410 --> 00:20:24.720
So Heart Steps--
what it involves

00:20:24.720 --> 00:20:30.590
is in the first of the three
studies we used as a Jawbone

00:20:30.590 --> 00:20:35.300
on the wrist to get step count
as well as sleep quality.

00:20:35.300 --> 00:20:39.730
And we built a little
app on the phone,

00:20:39.730 --> 00:20:43.780
and of course, this app has
a lot of supportive stuff

00:20:43.780 --> 00:20:46.280
that is involved in
that app, but I'm

00:20:46.280 --> 00:20:48.120
going to focus on
only certain aspects.

00:20:50.770 --> 00:20:53.320
So the distal outcome--
it was a 42-day study,

00:20:53.320 --> 00:20:55.010
and in this particular
case, we were

00:20:55.010 --> 00:20:58.530
looking to increase the
sedentary individuals'

00:20:58.530 --> 00:21:02.580
activity over the
duration of the 42 days.

00:21:02.580 --> 00:21:05.420
But however, the way we
wanted to do this is we

00:21:05.420 --> 00:21:09.660
wanted to provide them
actionable suggestions

00:21:09.660 --> 00:21:11.400
that when I say
actionable, this means

00:21:11.400 --> 00:21:16.250
you need to know what context
they're in at that moment.

00:21:16.250 --> 00:21:17.930
Actionable
suggestions they could

00:21:17.930 --> 00:21:22.140
use to be less sedentary
in that moment.

00:21:22.140 --> 00:21:24.320
And so the proximal
response is going

00:21:24.320 --> 00:21:26.490
to be some measure--
in this case, step

00:21:26.490 --> 00:21:31.000
count-- of their activity
following a decision point.

00:21:31.000 --> 00:21:34.650
And at a decision
point, we could give

00:21:34.650 --> 00:21:36.230
different kinds of messages.

00:21:36.230 --> 00:21:40.840
In fact, here, it turns
out we had three options.

00:21:40.840 --> 00:21:43.860
We wouldn't bother you at
all at a decision point,

00:21:43.860 --> 00:21:48.590
or we would give you one of
two varieties of an activity

00:21:48.590 --> 00:21:49.680
recommendation.

00:21:49.680 --> 00:21:52.575
One is an activity
recommendation

00:21:52.575 --> 00:21:56.040
that's just aimed at
reducing sedentary behavior.

00:21:56.040 --> 00:21:59.910
For example, I got one in the
office one day in the mid day,

00:21:59.910 --> 00:22:02.460
and it suggested I stand
up and roll my arms.

00:22:02.460 --> 00:22:04.040
It was actually very actionable.

00:22:04.040 --> 00:22:05.390
I liked it.

00:22:05.390 --> 00:22:08.680
But others are about going for
a walk like a 10-minute walk,

00:22:08.680 --> 00:22:10.200
so it's much more
onerous for me.

00:22:10.200 --> 00:22:12.200
I mean, I really need to
be in the right setting

00:22:12.200 --> 00:22:14.184
to be able to act on that.

00:22:14.184 --> 00:22:16.600
The decision times were every
two to two and a half hours,

00:22:16.600 --> 00:22:17.630
and you might ask why.

00:22:17.630 --> 00:22:19.600
And in fact, we
spent a lot of time

00:22:19.600 --> 00:22:21.610
trying to get that
straight in our head.

00:22:21.610 --> 00:22:24.080
We were lucky this is
part of a big team,

00:22:24.080 --> 00:22:28.700
and a group of us on
this team had access

00:22:28.700 --> 00:22:30.960
to large amounts
of Jawbone data.

00:22:30.960 --> 00:22:35.990
And they looked to see when was
the most variance in activity

00:22:35.990 --> 00:22:37.960
in people's lives.

00:22:37.960 --> 00:22:41.100
So if there's a lot of
variance at that time,

00:22:41.100 --> 00:22:43.480
there's the potential to
increase their activity,

00:22:43.480 --> 00:22:47.260
because there is times at which
they would be active sometimes

00:22:47.260 --> 00:22:48.000
at those times.

00:22:48.000 --> 00:22:51.460
And it turned out these
were five times a day

00:22:51.460 --> 00:22:54.320
right before you went to
work, pre-morning commute,

00:22:54.320 --> 00:22:57.270
around midday, mid-afternoon,
around when you're leaving

00:22:57.270 --> 00:22:58.930
work, and in the evening.

00:22:58.930 --> 00:23:01.290
And that's about every two
to two and a half hours,

00:23:01.290 --> 00:23:02.702
so five times a day.

00:23:02.702 --> 00:23:05.160
So originally, we were going
to do it much more frequently,

00:23:05.160 --> 00:23:07.400
but once we looked at the
date, it was very clear,

00:23:07.400 --> 00:23:09.805
this is when you saw that
variance, this potential

00:23:09.805 --> 00:23:10.430
to have impact.

00:23:13.700 --> 00:23:15.960
So how might a
recommendation look?

00:23:15.960 --> 00:23:19.170
And I'm going to focus on no
message versus a message here.

00:23:19.170 --> 00:23:24.110
And I got this message-- I
think this was last year.

00:23:24.110 --> 00:23:26.465
It was absolutely frigid.

00:23:26.465 --> 00:23:27.590
It was wonderful, actually.

00:23:27.590 --> 00:23:28.548
It was a beautiful day.

00:23:28.548 --> 00:23:30.290
It was sunny.

00:23:30.290 --> 00:23:33.740
March 14-- it was on
a Saturday, so it's

00:23:33.740 --> 00:23:35.400
going to only give
me a message, which

00:23:35.400 --> 00:23:36.860
makes sense on a Saturday.

00:23:36.860 --> 00:23:39.460
It won't give me a message
about work, for example,

00:23:39.460 --> 00:23:40.770
because I'm not at work.

00:23:40.770 --> 00:23:44.170
And actually, it was in
the middle of the day.

00:23:44.170 --> 00:23:47.890
So here it suggested that I
try and make a warm treat,

00:23:47.890 --> 00:23:51.180
and maybe do some jumping
jacks, move around a lot

00:23:51.180 --> 00:23:54.420
while I was at work.

00:23:54.420 --> 00:23:58.380
These suggestions are
tailored to the day

00:23:58.380 --> 00:24:01.680
of the week, the time of the
day, the weather outside.

00:24:01.680 --> 00:24:04.540
For example, if it's absolutely
nasty weather outside,

00:24:04.540 --> 00:24:09.270
we're not going to suggest
that you go for a walk outside.

00:24:09.270 --> 00:24:10.590
[INAUDIBLE]

00:24:10.590 --> 00:24:12.835
I can't remember what else.

00:24:12.835 --> 00:24:15.067
It was four different
kinds of ways they

00:24:15.067 --> 00:24:17.320
were tailored to the context.

00:24:17.320 --> 00:24:21.220
And when I get this, this is
a notification on your phone.

00:24:21.220 --> 00:24:23.190
And when I get
this notification,

00:24:23.190 --> 00:24:27.240
I have three ways
to swipe it off.

00:24:27.240 --> 00:24:29.160
I can press thumbs down.

00:24:29.160 --> 00:24:31.430
It wasn't useful for me.

00:24:31.430 --> 00:24:33.460
I can press thumbs
up, I liked it.

00:24:33.460 --> 00:24:35.190
And it goes away then.

00:24:35.190 --> 00:24:37.190
Or I can press this
button in the middle.

00:24:37.190 --> 00:24:38.840
If I press the
button in the middle,

00:24:38.840 --> 00:24:40.950
I turn off the intervention.

00:24:40.950 --> 00:24:44.506
So if I know that I'm going
to be busy, for example,

00:24:44.506 --> 00:24:46.630
I could turn off the
intervention for the next four

00:24:46.630 --> 00:24:47.430
hours.

00:24:47.430 --> 00:24:49.190
So I have control.

00:24:49.190 --> 00:24:51.250
I, the user, have control.

00:24:51.250 --> 00:24:51.830
This is nice.

00:24:54.413 --> 00:24:56.385
AUDIENCE: How does it
find all that information

00:24:56.385 --> 00:24:57.371
like the weather?

00:24:57.371 --> 00:24:58.360
Does it pull it up [INAUDIBLE]?

00:24:58.360 --> 00:24:59.193
SUSAN MURPHY: Right.

00:24:59.193 --> 00:25:02.590
So we're using Google a lot,
as you could well imagine.

00:25:02.590 --> 00:25:05.020
So we have-- so
I'm going to talk

00:25:05.020 --> 00:25:07.437
about that in the next slide.

00:25:07.437 --> 00:25:08.353
AUDIENCE: [INAUDIBLE].

00:25:08.353 --> 00:25:09.228
SUSAN MURPHY: No, no.

00:25:09.228 --> 00:25:10.990
It's perfect.

00:25:10.990 --> 00:25:17.150
So we're using the Google
activity recognition here.

00:25:17.150 --> 00:25:19.390
For example-- and
you'll see why,

00:25:19.390 --> 00:25:22.630
but we used the information
on the phone as to

00:25:22.630 --> 00:25:24.760
whether or not you're
moving really fast.

00:25:24.760 --> 00:25:26.610
You could be
driving, for example.

00:25:26.610 --> 00:25:31.290
Whether or not you're
currently walking and so on.

00:25:31.290 --> 00:25:34.240
And then we also
have the weather.

00:25:34.240 --> 00:25:36.280
I'm not sure where
we pull that from,

00:25:36.280 --> 00:25:39.280
because that's not specific
to that individual.

00:25:39.280 --> 00:25:42.350
That's just specific
to that location.

00:25:42.350 --> 00:25:44.660
And then we also look at
their Google Calendar,

00:25:44.660 --> 00:25:46.890
actually, and classify
whether or not

00:25:46.890 --> 00:25:51.460
the Google Calendar looks
busy or not, and so on.

00:25:51.460 --> 00:25:55.737
And in the Jawbone, we get the
step count from the Jawbone.

00:25:55.737 --> 00:25:57.820
I just want to talk to you
about the trial we ran,

00:25:57.820 --> 00:25:59.410
because this was the study.

00:25:59.410 --> 00:26:00.870
We wanted to design
it-- actually,

00:26:00.870 --> 00:26:03.010
it turns out there were
all kinds of decision rules

00:26:03.010 --> 00:26:04.111
we want to form.

00:26:04.111 --> 00:26:06.360
There were other interventions
we were thinking about,

00:26:06.360 --> 00:26:09.520
but I just focused on this
one activity recommendation

00:26:09.520 --> 00:26:11.480
up to five times a day.

00:26:11.480 --> 00:26:12.950
So we ran this study.

00:26:12.950 --> 00:26:17.830
We've developed this study
called a micro randomized trial

00:26:17.830 --> 00:26:21.350
to investigate whether or not
these activity recommendations

00:26:21.350 --> 00:26:24.390
are working as designed.

00:26:24.390 --> 00:26:28.220
So in these trials, what
happens is each participant

00:26:28.220 --> 00:26:31.190
is randomized at
each decision time.

00:26:31.190 --> 00:26:35.460
So in the case of Heart
Steps, it's a 42-day study.

00:26:35.460 --> 00:26:37.830
There's five times a
day, so there's 210.

00:26:37.830 --> 00:26:40.410
So each person is
randomized 210 times

00:26:40.410 --> 00:26:42.110
over the course of the study.

00:26:42.110 --> 00:26:45.870
And the smoking-- we're involved
in a smoking cessation study.

00:26:45.870 --> 00:26:49.890
There it's every
minute, 10 hours a day,

00:26:49.890 --> 00:26:55.117
10 days, so around
6,000 times or so.

00:26:55.117 --> 00:26:56.700
So you can be
randomized a great deal.

00:26:56.700 --> 00:26:58.158
Of course, most of
the time, you're

00:26:58.158 --> 00:27:01.310
going to be
randomized to nothing.

00:27:01.310 --> 00:27:07.060
So the thing I want to point out
these two points here before.

00:27:07.060 --> 00:27:09.390
In the scientific
literature, these

00:27:09.390 --> 00:27:14.210
might be-- these are akin to
full factorial trials, designs,

00:27:14.210 --> 00:27:15.390
experimental designs.

00:27:15.390 --> 00:27:20.260
And that sends a lot of
signals, and one signal it sends

00:27:20.260 --> 00:27:25.600
is that you want to collect
data to develop a strategy,

00:27:25.600 --> 00:27:27.060
a mobile health intervention.

00:27:27.060 --> 00:27:30.070
You're not collecting data to
confirm that one mobile health

00:27:30.070 --> 00:27:34.940
intervention is better, one
package is better than another.

00:27:34.940 --> 00:27:37.910
Instead, you're trying to
develop a mobile health

00:27:37.910 --> 00:27:41.240
intervention packet.

00:27:41.240 --> 00:27:43.710
And it can also be viewed--
I think some of you

00:27:43.710 --> 00:27:45.800
all will be familiar
with AB testing.

00:27:45.800 --> 00:27:48.970
It's commonly used throughout
the advertising world

00:27:48.970 --> 00:27:51.320
to decide where we're
going to place ads

00:27:51.320 --> 00:27:54.670
on websites and so on.

00:27:54.670 --> 00:27:59.500
And this generally is
between person randomization.

00:27:59.500 --> 00:28:02.040
Every time someone
goes to the website,

00:28:02.040 --> 00:28:04.210
the placements are
randomized, and we

00:28:04.210 --> 00:28:06.350
have some contextual
information on them,

00:28:06.350 --> 00:28:09.540
and we're trying to figure
out where to place an ad based

00:28:09.540 --> 00:28:11.190
on that contextual information.

00:28:11.190 --> 00:28:12.820
So it's an AB test.

00:28:12.820 --> 00:28:14.450
And the single case
designs-- these

00:28:14.450 --> 00:28:16.560
are used a lot in
the behavioral field,

00:28:16.560 --> 00:28:19.140
and this is where generally,
most of the randomization

00:28:19.140 --> 00:28:22.240
is with in-person randomization.

00:28:22.240 --> 00:28:24.350
So what happens when you
get something versus what

00:28:24.350 --> 00:28:25.820
happens when you don't get it?

00:28:25.820 --> 00:28:26.940
And this is a combination.

00:28:26.940 --> 00:28:29.020
It extends both of these.

00:28:29.020 --> 00:28:31.950
It uses them in different ways
from the way they're normally

00:28:31.950 --> 00:28:34.530
used, but the
design itself looks

00:28:34.530 --> 00:28:36.860
similar to a combination
of these two.

00:28:36.860 --> 00:28:39.310
So we're going to have
multiple people-- they'll

00:28:39.310 --> 00:28:41.570
be randomized across
people and with in person.

00:28:44.280 --> 00:28:45.980
And one other
thing I just wanted

00:28:45.980 --> 00:28:48.440
to make a statement
about-- if you're

00:28:48.440 --> 00:28:51.810
used to obtaining
large data sets,

00:28:51.810 --> 00:28:56.490
this is clearer now,
particularly in the business

00:28:56.490 --> 00:28:59.590
community, but for a
long time, a lot of us

00:28:59.590 --> 00:29:00.440
thought, well, look.

00:29:00.440 --> 00:29:04.340
Just give me tons of
data, tracking, say,

00:29:04.340 --> 00:29:06.990
how people are accessing
an intervention,

00:29:06.990 --> 00:29:09.970
and I'll use my algorithms
and tell you whether or not

00:29:09.970 --> 00:29:12.140
the intervention is working.

00:29:12.140 --> 00:29:15.260
And we now understand
better and better

00:29:15.260 --> 00:29:17.790
that this is not the way you
want-- this is not the best

00:29:17.790 --> 00:29:20.480
way to discover if an
intervention is working,

00:29:20.480 --> 00:29:24.120
because the reasons why someone
accesses an intervention

00:29:24.120 --> 00:29:28.070
may be strongly related to
how they behave following

00:29:28.070 --> 00:29:30.690
receipt of that intervention.

00:29:30.690 --> 00:29:32.760
And so you don't know if
the intervention caused

00:29:32.760 --> 00:29:34.540
that behavior or
the reasons why they

00:29:34.540 --> 00:29:37.390
accessed the intervention
are related to that behavior.

00:29:37.390 --> 00:29:40.180
So randomization breaks
that so that we can just

00:29:40.180 --> 00:29:43.020
see whether or not the
intervention causes

00:29:43.020 --> 00:29:45.370
the behavior as [INAUDIBLE].

00:29:45.370 --> 00:29:48.210
And that's why you see so
much randomization now,

00:29:48.210 --> 00:29:52.860
both in the business community
as well as in the health field.

00:29:52.860 --> 00:29:57.120
So in Heart Steps, we're
in this simplified aspect

00:29:57.120 --> 00:29:58.101
of Heart Steps.

00:29:58.101 --> 00:30:00.350
Heart Steps, we're only
looking to know whether or not

00:30:00.350 --> 00:30:02.530
we should provide
a tailored activity

00:30:02.530 --> 00:30:07.090
recommendation at these
five decision times per day,

00:30:07.090 --> 00:30:09.470
42 days, 210.

00:30:09.470 --> 00:30:13.910
And what we did was we
randomized 2/5, 3/5,

00:30:13.910 --> 00:30:15.930
and you might ask why.

00:30:15.930 --> 00:30:19.870
If you have had any
statistics in school,

00:30:19.870 --> 00:30:23.810
you would know that we would
normally use 1/2 randomization.

00:30:23.810 --> 00:30:26.660
It gives you highest power
to detect the signal.

00:30:26.660 --> 00:30:28.420
The issue here is burden.

00:30:28.420 --> 00:30:32.670
If you ping people too much
and suggest they do things,

00:30:32.670 --> 00:30:35.710
they may start to
habituate and ignore you.

00:30:35.710 --> 00:30:38.590
So we only want it on
average two suggestions

00:30:38.590 --> 00:30:42.535
per day originally, so we
used five occasions, 2/5.

00:30:47.860 --> 00:30:50.640
The first question
we wanted to ask

00:30:50.640 --> 00:30:54.660
was, do these activity
suggestions have any impact?

00:30:54.660 --> 00:30:56.160
And then we would
ask whether or not

00:30:56.160 --> 00:31:01.770
the context affected the
impact of those suggestions.

00:31:01.770 --> 00:31:04.110
So what we do is we
test for main effects.

00:31:04.110 --> 00:31:07.630
So I'm concerned, because
it's 1:07 right now,

00:31:07.630 --> 00:31:10.320
and how much time
do you think I have?

00:31:10.320 --> 00:31:12.880
We started a little bit late,
but I think we should still--

00:31:12.880 --> 00:31:14.360
MARK: We've got until
1:30 including Q&amp;A.

00:31:14.360 --> 00:31:15.180
SUSAN MURPHY: OK.

00:31:15.180 --> 00:31:16.305
I just wanted to make sure.

00:31:16.305 --> 00:31:18.270
Thanks.

00:31:18.270 --> 00:31:20.770
So what we're going
to do here is--

00:31:20.770 --> 00:31:23.520
so we're trying to figure
out how to design this study.

00:31:23.520 --> 00:31:25.600
So when you say
design, we already

00:31:25.600 --> 00:31:27.390
talked about we have
five decision points.

00:31:27.390 --> 00:31:29.140
We're going to randomize
people to getting

00:31:29.140 --> 00:31:31.290
their suggestion at each
of those decision points.

00:31:31.290 --> 00:31:32.640
That seems all kind of clear.

00:31:32.640 --> 00:31:35.400
The question is, how many
people do we have to recruit?

00:31:35.400 --> 00:31:38.120
And that has cost
considerations when

00:31:38.120 --> 00:31:40.200
you're trying to fund
a trial like this,

00:31:40.200 --> 00:31:42.770
because it takes
time to, if you're

00:31:42.770 --> 00:31:45.090
talking about human
subjects, it takes time

00:31:45.090 --> 00:31:49.660
to recruit these people,
enroll them, and monitor.

00:31:49.660 --> 00:31:52.560
So what we're going to do
is we're going to say, well,

00:31:52.560 --> 00:31:54.980
how many subjects
do we need in order

00:31:54.980 --> 00:31:58.120
to test whether or not
there's a signal at all

00:31:58.120 --> 00:32:00.130
of these activities suggestions?

00:32:00.130 --> 00:32:04.080
Does it have any impact on you?

00:32:04.080 --> 00:32:06.000
So there's two
things that come up

00:32:06.000 --> 00:32:09.080
that I think are
really cool here.

00:32:09.080 --> 00:32:11.750
And these two considerations
make these types

00:32:11.750 --> 00:32:14.830
of problems in mobile help
completely different from what

00:32:14.830 --> 00:32:17.430
you see across the
health field when you're

00:32:17.430 --> 00:32:20.370
talking about medications
or different kinds

00:32:20.370 --> 00:32:22.670
of physical therapies and so on.

00:32:22.670 --> 00:32:24.640
And here's the first,
and I'll talk to you

00:32:24.640 --> 00:32:26.640
about the second after this.

00:32:26.640 --> 00:32:30.810
So the first is that this is an
intervention that's occurring

00:32:30.810 --> 00:32:33.160
in the context of your life.

00:32:33.160 --> 00:32:35.240
It's right on your
hip that your phone

00:32:35.240 --> 00:32:38.220
can ping you and intervene.

00:32:38.220 --> 00:32:41.770
So there's this potential, as
I mentioned early, for burden.

00:32:41.770 --> 00:32:43.590
It can be very burdensome.

00:32:43.590 --> 00:32:46.930
And in particular, there's
a potential for habituation,

00:32:46.930 --> 00:32:50.310
and habituation I'm using in
a very negative sense here.

00:32:50.310 --> 00:32:54.190
That is when people habituate
to a message in a negative way,

00:32:54.190 --> 00:32:55.980
they don't even see
it after awhile.

00:32:55.980 --> 00:32:57.710
It's just not even there.

00:32:57.710 --> 00:33:00.040
You and I are habituating
to all kinds of things.

00:33:00.040 --> 00:33:01.980
It allows us to be productive.

00:33:01.980 --> 00:33:03.680
We just don't even
see these issues.

00:33:03.680 --> 00:33:06.330
You don't hear about the
noise outside and so on.

00:33:06.330 --> 00:33:09.040
And so to the extent
people start to habituate,

00:33:09.040 --> 00:33:10.410
we'll lose effect.

00:33:10.410 --> 00:33:15.080
So we can't anticipate that
the effect early in the study

00:33:15.080 --> 00:33:17.330
might be the same as the
effect later in the study,

00:33:17.330 --> 00:33:21.520
because by then, they may have
habituated to these messages.

00:33:21.520 --> 00:33:24.340
Now we do have
hundreds and hundreds

00:33:24.340 --> 00:33:26.130
of messages we can
provide, and they

00:33:26.130 --> 00:33:27.960
are tailored to the context.

00:33:27.960 --> 00:33:30.145
But nonetheless, you
may still habituate.

00:33:33.390 --> 00:33:37.590
And if you get aggravated, you
may just start ignoring them.

00:33:37.590 --> 00:33:42.540
So what we're going to do is
when we talk about the signal

00:33:42.540 --> 00:33:44.560
or what we call a
main effect-- that's

00:33:44.560 --> 00:33:47.440
the signal, the effect
of the intervention

00:33:47.440 --> 00:33:50.030
message on your
activity-- so what's

00:33:50.030 --> 00:33:52.550
my activity look like if I
get a message versus what's

00:33:52.550 --> 00:33:55.780
my activity look like if
I don't get a message?

00:33:55.780 --> 00:33:57.820
We're going to allow
that to vary with time.

00:34:01.330 --> 00:34:03.170
The other thing we
have to think about,

00:34:03.170 --> 00:34:04.480
and this is also related.

00:34:04.480 --> 00:34:08.179
This is very special
to mobile health--

00:34:08.179 --> 00:34:10.300
is this issue of availability.

00:34:10.300 --> 00:34:14.179
And this is something
only recently have we

00:34:14.179 --> 00:34:18.540
in the research community
really began to appreciate.

00:34:18.540 --> 00:34:21.520
And it's really important.

00:34:21.520 --> 00:34:24.650
I think it's going to be a
dominant issue going forward.

00:34:24.650 --> 00:34:28.469
And that is you cannot provide
an intervention to someone when

00:34:28.469 --> 00:34:30.070
they're not available.

00:34:30.070 --> 00:34:33.739
So for example, in Heart
Steps, if the Google classifier

00:34:33.739 --> 00:34:36.840
indicated that you might
be driving the car,

00:34:36.840 --> 00:34:39.320
we would not have
the phone light up,

00:34:39.320 --> 00:34:45.060
ping, and possibly interrupt
you and prevent you

00:34:45.060 --> 00:34:46.960
from paying attention
to the road.

00:34:46.960 --> 00:34:48.489
We just wouldn't do that.

00:34:48.489 --> 00:34:51.139
Also, if the Google
classifier that's on the phone

00:34:51.139 --> 00:34:53.460
indicated that you're
currently walking,

00:34:53.460 --> 00:34:56.909
we didn't want the phone to
ping-- audibly ping-- light up,

00:34:56.909 --> 00:34:59.450
and suggest you go for walk.

00:34:59.450 --> 00:35:01.840
That would just
be so aggravating.

00:35:01.840 --> 00:35:04.850
So you're not
available at that time.

00:35:04.850 --> 00:35:06.970
The other way you
might not be available

00:35:06.970 --> 00:35:09.490
is if you don't
want to be bothered,

00:35:09.490 --> 00:35:11.300
so you have turned
off the intervention.

00:35:11.300 --> 00:35:12.820
You've said, don't bother me.

00:35:12.820 --> 00:35:15.250
So for example, like
I had days where

00:35:15.250 --> 00:35:17.550
I knew I was going
to go from one--

00:35:17.550 --> 00:35:20.480
I was going to review
meetings all day.

00:35:20.480 --> 00:35:23.410
I would turn off the
intervention for the whole day.

00:35:23.410 --> 00:35:25.570
Those are times when
I'm not available.

00:35:25.570 --> 00:35:31.220
So when we talk about the signal
or what I call the main effect,

00:35:31.220 --> 00:35:33.085
and we say it's
time varying, it's

00:35:33.085 --> 00:35:34.710
not only that it's
time varying because

00:35:34.710 --> 00:35:36.750
of burden and habituation.

00:35:36.750 --> 00:35:43.990
It also-- the signal
at time 30 is only

00:35:43.990 --> 00:35:47.330
among the people who are
available at time 30.

00:35:50.170 --> 00:35:52.910
So at each point in
time, the population

00:35:52.910 --> 00:35:55.859
among whom I'm going to
ask if there's an effect,

00:35:55.859 --> 00:35:57.150
is a different group of people.

00:35:57.150 --> 00:35:59.270
It could be a slightly
different group of people.

00:35:59.270 --> 00:36:00.186
Yeah.

00:36:00.186 --> 00:36:01.102
AUDIENCE: [INAUDIBLE].

00:36:05.850 --> 00:36:08.820
So do you find, because you
can track when somebody is,

00:36:08.820 --> 00:36:11.810
say, walking, that
if you were to give

00:36:11.810 --> 00:36:15.350
a message of support, so
congrats, you're out walking,

00:36:15.350 --> 00:36:18.460
whatever that [INAUDIBLE] that
you can reduce habituation

00:36:18.460 --> 00:36:21.570
over the 42 days so that
at the end of the trial,

00:36:21.570 --> 00:36:26.360
it's less variance versus
people who started.

00:36:26.360 --> 00:36:28.080
Do you not measure [INAUDIBLE]?

00:36:28.080 --> 00:36:30.920
SUSAN MURPHY: In Heart
Steps, we're not.

00:36:30.920 --> 00:36:33.350
But right now, I told you
we were involved with an app

00:36:33.350 --> 00:36:37.060
challenge, because we're very
interested in-- so there's

00:36:37.060 --> 00:36:38.307
therapeutic interventions.

00:36:38.307 --> 00:36:39.890
That's what we're
talking about today.

00:36:39.890 --> 00:36:41.120
They're therapeutic,
because these

00:36:41.120 --> 00:36:43.070
are people who are
sedentary, and we're trying

00:36:43.070 --> 00:36:45.560
to prevent bad health outcomes.

00:36:45.560 --> 00:36:48.130
There's also engagement
interventions,

00:36:48.130 --> 00:36:49.970
and that's what
you're talking about.

00:36:49.970 --> 00:36:55.580
How can I make them more
engaged with my intervention?

00:36:55.580 --> 00:36:58.790
And we are working exactly
on those kinds of problems.

00:36:58.790 --> 00:37:01.590
How do we feed back, give
them encouraging messages?

00:37:01.590 --> 00:37:04.500
They're currently doing
something that's good for them.

00:37:04.500 --> 00:37:07.520
How do we help them feel
good about this whole--

00:37:07.520 --> 00:37:08.780
and we're working on that now.

00:37:08.780 --> 00:37:11.850
And there's a lot of interest
in having both just in time

00:37:11.850 --> 00:37:17.630
adaptive interventions that both
have engagement or reinforcers.

00:37:17.630 --> 00:37:21.130
The interventions themselves
are reinforcers, as well as

00:37:21.130 --> 00:37:23.650
therapeutic in some sense.

00:37:23.650 --> 00:37:26.190
Yeah.

00:37:26.190 --> 00:37:30.210
I mean this is a big-- and I
personally think that latter,

00:37:30.210 --> 00:37:33.510
the reinforcers are going
to be even more important.

00:37:33.510 --> 00:37:35.850
And if we can keep
you engaged long term,

00:37:35.850 --> 00:37:37.740
we have the potential
to help you long term.

00:37:37.740 --> 00:37:39.448
AUDIENCE: Yeah, I just
was just wondering

00:37:39.448 --> 00:37:42.371
the impacts on habituation and
burden on future messaging.

00:37:42.371 --> 00:37:43.870
SUSAN MURPHY: That's
the whole game.

00:37:43.870 --> 00:37:45.520
That is the whole game.

00:37:45.520 --> 00:37:50.290
So we haven't run a study where
we know what that impact is,

00:37:50.290 --> 00:37:52.360
but we hope that
it would be good.

00:37:52.360 --> 00:37:55.130
So we're designing
studies to test this.

00:37:55.130 --> 00:37:57.547
AUDIENCE: Human studies or
human nature, you would think--

00:37:57.547 --> 00:37:58.796
SUSAN MURPHY: You would think.

00:37:58.796 --> 00:37:59.860
AUDIENCE: [INAUDIBLE].

00:37:59.860 --> 00:38:00.670
SUSAN MURPHY: You've
got to be careful.

00:38:00.670 --> 00:38:01.320
Right.

00:38:01.320 --> 00:38:01.819
Right.

00:38:01.819 --> 00:38:04.100
Exactly.

00:38:04.100 --> 00:38:07.691
AUDIENCE: So at this point,
there are a lot of apps

00:38:07.691 --> 00:38:08.190
out there.

00:38:08.190 --> 00:38:10.350
Like FitBit has their
own app, and they

00:38:10.350 --> 00:38:12.337
use this engagement
model [INAUDIBLE].

00:38:12.337 --> 00:38:13.170
SUSAN MURPHY: Right.

00:38:13.170 --> 00:38:14.360
AUDIENCE: Community sharing.

00:38:14.360 --> 00:38:14.790
SUSAN MURPHY: Right.

00:38:14.790 --> 00:38:15.766
A lot of social--

00:38:15.766 --> 00:38:17.492
AUDIENCE: [INAUDIBLE].

00:38:17.492 --> 00:38:18.700
SUSAN MURPHY: --competitions.

00:38:18.700 --> 00:38:19.324
AUDIENCE: Yeah.

00:38:19.324 --> 00:38:22.310
When you think about this, and
when this research is going,

00:38:22.310 --> 00:38:25.090
how are you thinking about
it different from what's

00:38:25.090 --> 00:38:25.980
already out there?

00:38:25.980 --> 00:38:27.167
What's popular, what isn't

00:38:27.167 --> 00:38:28.000
SUSAN MURPHY: Right.

00:38:28.000 --> 00:38:29.666
AUDIENCE: So we've
got 20 wearables now.

00:38:29.666 --> 00:38:33.975
They all tap into
an app [INAUDIBLE].

00:38:38.894 --> 00:38:43.532
How do you they see all of
your studies to be different?

00:38:43.532 --> 00:38:45.450
Do you even see
it as [INAUDIBLE]?

00:38:45.450 --> 00:38:48.695
Where is it going
eventually [INAUDIBLE]?

00:38:48.695 --> 00:38:50.070
SUSAN MURPHY: Oh,
I can tell you.

00:38:50.070 --> 00:38:50.300
Yeah.

00:38:50.300 --> 00:38:50.799
OK.

00:38:50.799 --> 00:38:51.460
That's cool.

00:38:51.460 --> 00:38:57.830
So there are a lot--
there are groups

00:38:57.830 --> 00:39:01.930
that are very interested
in pushing the research

00:39:01.930 --> 00:39:06.340
boundaries, not only having a
commercially viable product,

00:39:06.340 --> 00:39:08.720
but also pushing
our understanding

00:39:08.720 --> 00:39:13.010
of behavior science, the
boundaries of behavior science.

00:39:13.010 --> 00:39:16.110
There are commercial
groups that are doing this.

00:39:16.110 --> 00:39:19.690
I think Microsoft
has it in Health Arm.

00:39:19.690 --> 00:39:22.090
Google DeepMind has
some interest in this.

00:39:22.090 --> 00:39:27.530
So there is, but by and large,
mostly in the commercial world,

00:39:27.530 --> 00:39:30.990
there is a-- you're looking
for a commercially viable app.

00:39:30.990 --> 00:39:36.050
And that may just
mean that we need

00:39:36.050 --> 00:39:39.750
people to engage for a month.

00:39:39.750 --> 00:39:42.530
And then it's
commercially viable.

00:39:42.530 --> 00:39:45.890
So in our world though,
in the world we're in,

00:39:45.890 --> 00:39:48.120
we're both interested in
pushing the boundaries

00:39:48.120 --> 00:39:53.120
of behavioral science, as well
as having a health impact.

00:39:53.120 --> 00:39:58.210
So whatever-- we really want
to take apart these black boxes

00:39:58.210 --> 00:40:00.750
and understand what
parts of the intervention

00:40:00.750 --> 00:40:04.080
really work the way we
intended them to do.

00:40:04.080 --> 00:40:08.570
And can we test the way our
understanding of behavior--

00:40:08.570 --> 00:40:12.940
can we test it and
further develop it?

00:40:12.940 --> 00:40:16.070
So habituation is a big
player in this game,

00:40:16.070 --> 00:40:19.570
and in fact, as you
may already know,

00:40:19.570 --> 00:40:22.540
most health apps that
you can download,

00:40:22.540 --> 00:40:25.910
the attrition to health
apps is profound.

00:40:25.910 --> 00:40:26.630
It's really bad.

00:40:26.630 --> 00:40:30.210
It's like within two months, 75%
of people don't even use them.

00:40:30.210 --> 00:40:31.850
It's a big problem.

00:40:31.850 --> 00:40:35.120
But commercially
that may be fine.

00:40:35.120 --> 00:40:38.550
It just depends on what your
commercial objective is.

00:40:38.550 --> 00:40:41.480
AUDIENCE: Yeah, I guess
my question was also

00:40:41.480 --> 00:40:45.012
along the lines of
since that happens,

00:40:45.012 --> 00:40:48.734
in the scope of your research,
how do you implement that?

00:40:48.734 --> 00:40:51.850
And how can you get
some research points

00:40:51.850 --> 00:40:56.220
to actually help people
continue with this [INAUDIBLE]?

00:40:56.220 --> 00:40:57.080
SUSAN MURPHY: Right.

00:40:57.080 --> 00:40:59.460
So that's why we look
at that micro-- that's

00:40:59.460 --> 00:41:02.240
the whole purpose of these
micro randomized studies.

00:41:02.240 --> 00:41:05.130
Is the intervention
working in the way

00:41:05.130 --> 00:41:06.130
in which we designed it?

00:41:06.130 --> 00:41:08.570
We designed it to have an
in-the-moment impact on you?

00:41:08.570 --> 00:41:09.570
Did it?

00:41:09.570 --> 00:41:13.220
Did the social networking
have an in-the-moment impact?

00:41:13.220 --> 00:41:15.350
Did the gaming,
like often there's

00:41:15.350 --> 00:41:19.210
interventions which have
a game-like structure.

00:41:19.210 --> 00:41:22.540
Some people at some
times, that may not work.

00:41:22.540 --> 00:41:24.770
We want to really
understand that.

00:41:24.770 --> 00:41:27.100
And I'm not saying I
actually am working

00:41:27.100 --> 00:41:29.000
with a number of
startups, and they

00:41:29.000 --> 00:41:31.610
want to answer those
questions as well.

00:41:31.610 --> 00:41:34.100
But we do too.

00:41:34.100 --> 00:41:37.340
And we want to help
the behavioral theory,

00:41:37.340 --> 00:41:39.750
develop behavioral theory
so that we can come up

00:41:39.750 --> 00:41:43.420
and think of even more
innovative interventions,

00:41:43.420 --> 00:41:46.085
both engagement as well
as therapeutic, actually.

00:41:46.085 --> 00:41:46.918
AUDIENCE: Hi, Susan.

00:41:46.918 --> 00:41:48.822
I have a question for you.

00:41:48.822 --> 00:41:51.509
What's your vision that really
motivates you to do this?

00:41:51.509 --> 00:41:52.300
SUSAN MURPHY: Yeah.

00:41:52.300 --> 00:41:53.332
This is a good question.

00:41:53.332 --> 00:41:54.790
AUDIENCE: [INAUDIBLE]
stop smoking.

00:41:54.790 --> 00:41:55.400
Or what was the--

00:41:55.400 --> 00:41:56.108
SUSAN MURPHY: No.

00:41:56.108 --> 00:41:57.670
My vision-- no.

00:41:57.670 --> 00:41:59.590
I'm a data scientist, OK?

00:41:59.590 --> 00:42:02.110
Let's be honest about
what my vision is.

00:42:02.110 --> 00:42:04.926
My vision is what
we're going to do

00:42:04.926 --> 00:42:07.300
is we're going to use-- I'm
going to really get off track

00:42:07.300 --> 00:42:07.799
here.

00:42:07.799 --> 00:42:11.479
Just please forgive me,
all those guys on the-- so

00:42:11.479 --> 00:42:13.520
we're going to use these
micro randomized trials.

00:42:13.520 --> 00:42:15.650
We tested that there's a signal.

00:42:15.650 --> 00:42:20.710
Then what we'll do is
we're developing methods

00:42:20.710 --> 00:42:26.850
to develop what we call warm
start just in time adaptive

00:42:26.850 --> 00:42:28.050
interventions.

00:42:28.050 --> 00:42:31.120
That is these are
adaptive interventions

00:42:31.120 --> 00:42:35.180
that we use that data to try and
get those decision rules pinned

00:42:35.180 --> 00:42:36.230
down.

00:42:36.230 --> 00:42:39.190
Then in our last study, for
example, in Heart Steps,

00:42:39.190 --> 00:42:42.470
and in our last study,
we'll start everyone off

00:42:42.470 --> 00:42:44.540
on the warm start.

00:42:44.540 --> 00:42:47.650
And then we're going to have
algorithms that run locally

00:42:47.650 --> 00:42:50.790
on the phone, so they
have to be computationally

00:42:50.790 --> 00:42:55.580
efficient, less so as time goes
on, but you know what I mean.

00:42:55.580 --> 00:42:59.830
But anyway, they will
personalize those decision

00:42:59.830 --> 00:43:02.580
rules to each one of us.

00:43:02.580 --> 00:43:06.990
And we want to personalize the--
so this is a field of health.

00:43:06.990 --> 00:43:08.290
We have to be really careful.

00:43:08.290 --> 00:43:11.940
This is very different
from personalizing ads.

00:43:11.940 --> 00:43:14.490
We don't want to have any--
we want to really avoid

00:43:14.490 --> 00:43:16.270
iatrogenic effects.

00:43:16.270 --> 00:43:18.744
We don't want to
disillusion you if you're

00:43:18.744 --> 00:43:20.410
trying-- say you're
someone who's trying

00:43:20.410 --> 00:43:22.320
to control your binge eating.

00:43:22.320 --> 00:43:25.190
We don't want you to
become disillusioned.

00:43:25.190 --> 00:43:30.970
I mean we really have to worry
about-- in our experimentation,

00:43:30.970 --> 00:43:35.040
we have to be concerned
about having bad effects.

00:43:35.040 --> 00:43:38.980
So when we develop
these algorithms that

00:43:38.980 --> 00:43:41.770
will personalize the
intervention to you,

00:43:41.770 --> 00:43:45.890
we have to also monitor
them the whole time.

00:43:45.890 --> 00:43:50.770
So let me just make it
a little bit more clear.

00:43:50.770 --> 00:43:53.740
Remember in the example
of that decision rule,

00:43:53.740 --> 00:43:56.300
I said that there was this
threshold for how long

00:43:56.300 --> 00:43:58.620
you've been working
on the computer,

00:43:58.620 --> 00:44:02.060
and that threshold may
differ from person to person.

00:44:02.060 --> 00:44:03.790
Mine may be 30 minutes.

00:44:03.790 --> 00:44:06.090
Mark's may be an hour,
and it may differ

00:44:06.090 --> 00:44:07.960
on what my calendar looks like.

00:44:07.960 --> 00:44:10.020
It may differ by
all kinds of things

00:44:10.020 --> 00:44:12.660
that are going on in my life.

00:44:12.660 --> 00:44:20.210
So that threshold, we can start
at the beginning of the study.

00:44:20.210 --> 00:44:21.800
It would be something
pretty simple.

00:44:21.800 --> 00:44:22.570
Mark's male.

00:44:22.570 --> 00:44:23.140
I'm female.

00:44:23.140 --> 00:44:25.850
Maybe we have different
thresholds, because one's male,

00:44:25.850 --> 00:44:26.990
one's female.

00:44:26.990 --> 00:44:29.970
He works in one kind of job.

00:44:29.970 --> 00:44:31.830
I work in a totally
different job.

00:44:31.830 --> 00:44:34.400
Maybe we have different
thresholds for that reason.

00:44:34.400 --> 00:44:37.500
But then if you get two
people like me, both female.

00:44:37.500 --> 00:44:39.740
We both have the
same sort of job.

00:44:39.740 --> 00:44:43.530
There may be a way-- as
watching me through time,

00:44:43.530 --> 00:44:47.300
we may learn that I respond
to that intervention,

00:44:47.300 --> 00:44:50.570
those suggestions to get
up and move differently

00:44:50.570 --> 00:44:52.110
than the other person.

00:44:52.110 --> 00:44:53.760
And that's what the
algorithms will do.

00:44:53.760 --> 00:44:57.690
And that's my long term goal is
to have really cool algorithms

00:44:57.690 --> 00:44:59.890
that do that.

00:44:59.890 --> 00:45:02.790
So of course, I want
to help my colleagues.

00:45:02.790 --> 00:45:05.890
I'm part of big teams with
behavioral scientists, computer

00:45:05.890 --> 00:45:09.980
scientists, and I want them to
advance their science as well.

00:45:09.980 --> 00:45:13.000
And to the extent that
they advance their science,

00:45:13.000 --> 00:45:17.270
and I advance my science, and
that we can work with the--

00:45:17.270 --> 00:45:19.250
we have startups that
we're working with.

00:45:19.250 --> 00:45:22.190
To the extent that we can
help them be more effective,

00:45:22.190 --> 00:45:23.490
everything gets better.

00:45:23.490 --> 00:45:27.391
I can advance my science too.

00:45:27.391 --> 00:45:27.890
Yeah.

00:45:27.890 --> 00:45:29.810
So that's the long term goal.

00:45:29.810 --> 00:45:35.010
Now I got way off track.
[LAUGHS] That's fine.

00:45:35.010 --> 00:45:36.800
So I just want to--
I think I'm just

00:45:36.800 --> 00:45:38.520
going to show you this picture.

00:45:38.520 --> 00:45:42.170
I just want to harp on that
little availability issue

00:45:42.170 --> 00:45:45.720
a little bit more, because it's
something that we just really

00:45:45.720 --> 00:45:49.780
didn't understand in mobile
health until recently.

00:45:49.780 --> 00:45:54.550
So like here, I've just drawn in
a very cartoonish fashion what

00:45:54.550 --> 00:45:57.440
the effect of having a
suggestion versus not having

00:45:57.440 --> 00:45:59.580
a suggestion might look like.

00:45:59.580 --> 00:46:03.050
So I have 210 decision
points in Heart Steps.

00:46:03.050 --> 00:46:06.360
This is the difference between--
it's a standardized difference,

00:46:06.360 --> 00:46:11.540
so it's a signal to noise ratio
in terms of your activity,

00:46:11.540 --> 00:46:14.240
getting a suggestion versus not.

00:46:14.240 --> 00:46:17.280
So you can imagine
people may come in.

00:46:17.280 --> 00:46:19.052
So this is highly
simplistic, like I said,

00:46:19.052 --> 00:46:20.510
because you can
imagine it probably

00:46:20.510 --> 00:46:22.450
varies by day of the
week, time of the day,

00:46:22.450 --> 00:46:24.010
and so on-- weather.

00:46:24.010 --> 00:46:25.970
But anyhow, at the
beginning, there's

00:46:25.970 --> 00:46:27.810
not much going on,
because you just started

00:46:27.810 --> 00:46:28.893
getting these suggestions.

00:46:28.893 --> 00:46:30.630
You don't know how
to make use of them.

00:46:30.630 --> 00:46:32.739
You don't even
notice them at first.

00:46:32.739 --> 00:46:34.780
Then you start trying them
out when you get them.

00:46:34.780 --> 00:46:36.940
You don't try anything
out when you don't, so you

00:46:36.940 --> 00:46:38.990
see an effect increase.

00:46:38.990 --> 00:46:41.110
Then habituation comes in.

00:46:41.110 --> 00:46:44.720
I get tired of them, and
I stop paying attention.

00:46:44.720 --> 00:46:47.760
And so you don't see any
effect of a message on me.

00:46:47.760 --> 00:46:49.830
That's one reason
why you might see

00:46:49.830 --> 00:46:52.120
this kind of
curvilinear relationship

00:46:52.120 --> 00:46:54.380
with the decreasing effect.

00:46:54.380 --> 00:46:56.940
Another reason, though,
you might see a changing

00:46:56.940 --> 00:46:59.150
signal over time,
and we really need

00:46:59.150 --> 00:47:02.470
to understand that and somehow
bring that into the theory,

00:47:02.470 --> 00:47:06.200
is that the group of people who
are available to be intervened

00:47:06.200 --> 00:47:08.750
on varies with time.

00:47:08.750 --> 00:47:11.110
So there's going to be one
group of people that have

00:47:11.110 --> 00:47:14.000
really drank this Kool-Aid.

00:47:14.000 --> 00:47:16.220
They've started walking
most of those times

00:47:16.220 --> 00:47:19.550
or moving at those five
times, because they really

00:47:19.550 --> 00:47:21.834
want to get with the game.

00:47:21.834 --> 00:47:23.250
They're not going
to be available.

00:47:23.250 --> 00:47:25.708
We're not going to be looking
at the effect of those people

00:47:25.708 --> 00:47:26.940
as time goes on.

00:47:26.940 --> 00:47:29.900
So it could be that the
people are still available,

00:47:29.900 --> 00:47:32.870
or somehow they're
not as receptive,

00:47:32.870 --> 00:47:36.380
or the suggestions just don't
have the same meaning for them.

00:47:36.380 --> 00:47:38.190
So you might get a
decrease, because you

00:47:38.190 --> 00:47:40.970
have a different population
on which you're looking.

00:47:40.970 --> 00:47:44.060
Very interesting, and the
way in which you do science

00:47:44.060 --> 00:47:47.600
when your population is
changing over time is different.

00:47:47.600 --> 00:47:50.170
It's not something we've
done before in most

00:47:50.170 --> 00:47:52.440
areas of experimentation.

00:47:52.440 --> 00:47:54.780
Across all areas
of experimentation,

00:47:54.780 --> 00:47:56.680
we don't normally have this.

00:47:56.680 --> 00:48:02.220
It's an area that just comes up
mainly in mobile and wearables.

00:48:02.220 --> 00:48:03.050
OK.

00:48:03.050 --> 00:48:05.670
I just want to show
you some sample sizes,

00:48:05.670 --> 00:48:10.700
and this, if you
know about-- these

00:48:10.700 --> 00:48:15.480
are combinations of AB studies
and single case studies.

00:48:15.480 --> 00:48:18.592
And the minute
you know that, you

00:48:18.592 --> 00:48:20.770
know that when you
decide, when you try

00:48:20.770 --> 00:48:23.550
and assess whether or not
you have a treatment effect,

00:48:23.550 --> 00:48:26.450
you're going to compare both.

00:48:26.450 --> 00:48:28.680
Mark and I are in the study.

00:48:28.680 --> 00:48:29.880
He got a message.

00:48:29.880 --> 00:48:30.890
I didn't.

00:48:30.890 --> 00:48:33.990
You will compare whether or
not he was more active than me.

00:48:33.990 --> 00:48:36.960
Between person comparison--
that's the AB part.

00:48:36.960 --> 00:48:39.510
Also though, as I go
through this study,

00:48:39.510 --> 00:48:42.140
you'll see me sometimes get
a message, sometimes don't.

00:48:42.140 --> 00:48:43.390
Same for Mark.

00:48:43.390 --> 00:48:45.939
And you'll compare me both
sometimes when I get it

00:48:45.939 --> 00:48:47.480
and sometimes when
I don't, so that's

00:48:47.480 --> 00:48:49.600
a within person contrast.

00:48:49.600 --> 00:48:53.630
And so the minute you have
both between and within person

00:48:53.630 --> 00:48:55.780
contrast, that
means you don't need

00:48:55.780 --> 00:48:58.310
as big a number of
participants in your study,

00:48:58.310 --> 00:49:01.620
and you can run studies
in a much more cost

00:49:01.620 --> 00:49:02.490
efficient manner.

00:49:06.180 --> 00:49:09.620
So I'm going to show you
some sample sizes here.

00:49:09.620 --> 00:49:14.030
So this is a classical
way that we size trials.

00:49:14.030 --> 00:49:17.600
We say with 80% power, I
want to detect whether or not

00:49:17.600 --> 00:49:18.660
there's a signal.

00:49:18.660 --> 00:49:20.170
I want to get
enough participants

00:49:20.170 --> 00:49:23.580
so that with 80% power, I can
detect if there's a signal.

00:49:23.580 --> 00:49:27.020
However, I'll tolerate
making an error

00:49:27.020 --> 00:49:30.480
and saying that there's
a signal 5% of the time.

00:49:30.480 --> 00:49:34.510
That's the traditional
way you design studies.

00:49:34.510 --> 00:49:36.650
So this is traditional.

00:49:36.650 --> 00:49:39.145
Then you also-- because
we're in this whole setting

00:49:39.145 --> 00:49:42.660
of availability, we have to
anticipate whether or not

00:49:42.660 --> 00:49:45.080
people-- what fraction of
the time do you think people

00:49:45.080 --> 00:49:46.480
are going to be available?

00:49:46.480 --> 00:49:49.020
70% of the time, 50% the time?

00:49:49.020 --> 00:49:50.530
A good bit of the
time, they may not

00:49:50.530 --> 00:49:53.670
be available for
our intervention.

00:49:53.670 --> 00:49:56.100
And then there's the
signal to noise ratio

00:49:56.100 --> 00:49:57.770
that you want to detect.

00:49:57.770 --> 00:50:01.580
So in terms of the standard
deviation of step count,

00:50:01.580 --> 00:50:04.660
what kind of-- do
I want to detect

00:50:04.660 --> 00:50:07.850
0.5 movement in
standard deviations

00:50:07.850 --> 00:50:12.000
or 0.1 movement in standard
deviations in step count?

00:50:12.000 --> 00:50:14.650
So that's what
this is over here.

00:50:14.650 --> 00:50:19.890
And what you see is if you have
a pretty high availability,

00:50:19.890 --> 00:50:22.645
and you have a standardized
of you want to detect even

00:50:22.645 --> 00:50:30.480
a 0.1 standard deviation
change in activity,

00:50:30.480 --> 00:50:33.130
if you get a message
versus if you don't, you

00:50:33.130 --> 00:50:34.940
don't need that
many participants.

00:50:34.940 --> 00:50:36.810
Only 33 participants.

00:50:36.810 --> 00:50:39.420
And this is really interesting.

00:50:39.420 --> 00:50:43.130
First of all, these are very
small standardized effect size,

00:50:43.130 --> 00:50:46.520
and it could be that we need
to even look at smaller ones.

00:50:46.520 --> 00:50:50.260
But they accumulate
over time to the extent

00:50:50.260 --> 00:50:53.210
we can prevent habituation when
we use reinforcers like you

00:50:53.210 --> 00:50:55.470
were talking about earlier.

00:50:55.470 --> 00:50:57.250
They would accumulate
over time, so we

00:50:57.250 --> 00:51:01.360
can hope to magnify our effect.

00:51:01.360 --> 00:51:02.060
So it's cool.

00:51:04.750 --> 00:51:07.860
I just want to mention
that when people-- so we're

00:51:07.860 --> 00:51:10.660
involved in a number of
studies where people want

00:51:10.660 --> 00:51:14.120
to run whole series of
micro randomized studies,

00:51:14.120 --> 00:51:16.130
then to get to that
warm start, and then

00:51:16.130 --> 00:51:18.904
do the online personalisation.

00:51:18.904 --> 00:51:20.320
You have to be
really careful when

00:51:20.320 --> 00:51:22.190
you do design these studies.

00:51:22.190 --> 00:51:24.850
You have to underestimate
availability.

00:51:24.850 --> 00:51:27.320
So for example, if
you think people

00:51:27.320 --> 00:51:29.300
are going to be available
70% of the time,

00:51:29.300 --> 00:51:32.740
but they're only
available 50% of the time,

00:51:32.740 --> 00:51:33.820
that's not good for you.

00:51:33.820 --> 00:51:34.903
You need to underestimate.

00:51:38.550 --> 00:51:39.560
A lot of data.

00:51:39.560 --> 00:51:44.670
This type of data can be
used to form decision rules.

00:51:44.670 --> 00:51:45.560
It's nice.

00:51:45.560 --> 00:51:48.860
You can use it to
get your warm starts.

00:51:48.860 --> 00:51:52.310
I think I'll just go-- this
is some members of our team.

00:51:52.310 --> 00:51:55.000
I just want to indicate the
interdisciplinary nature

00:51:55.000 --> 00:51:56.000
of these kinds of teams.

00:51:56.000 --> 00:51:58.800
I think this would be-- you
would be accustomed to this,

00:51:58.800 --> 00:52:01.190
because you have a very
interdisciplinary group.

00:52:01.190 --> 00:52:04.272
We have behavioral
scientists here.

00:52:04.272 --> 00:52:05.480
You have computer scientists.

00:52:05.480 --> 00:52:08.090
We have guys who do human
computer interaction, how

00:52:08.090 --> 00:52:10.080
you relate to the computer.

00:52:10.080 --> 00:52:11.230
We have statisticians.

00:52:11.230 --> 00:52:12.990
We have a sociologist.

00:52:12.990 --> 00:52:15.370
We have another
behavioral scientist.

00:52:15.370 --> 00:52:18.540
This guy works mainly
only in wearables.

00:52:18.540 --> 00:52:20.680
So all of these
people come together

00:52:20.680 --> 00:52:23.740
for you to run these
kinds of studies

00:52:23.740 --> 00:52:25.670
and look for these signals.

00:52:25.670 --> 00:52:26.650
That's it.

00:52:26.650 --> 00:52:27.250
Thanks.

00:52:27.250 --> 00:52:30.600
[APPLAUSE]

