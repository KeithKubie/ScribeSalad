WEBVTT
Kind: captions
Language: en

00:00:22.050 --> 00:00:24.230
Hello, welcome everyone.

00:00:24.230 --> 00:00:26.980
I am Mike Salem and I'm here with CEO and founder of Catalia health,

00:00:26.980 --> 00:00:28.700
Cory Kidd. Cory, How are you?

00:00:28.700 --> 00:00:29.760
I'm doing well. Thanks.

00:00:29.760 --> 00:00:32.685
Great. So Cory, can you tell me a little bit about Catalia health?

00:00:32.685 --> 00:00:35.510
Sure. So Catalia health is a healthcare company.

00:00:35.510 --> 00:00:37.070
It's about two-and-a-half years old.

00:00:37.070 --> 00:00:40.225
We focus on chronic care management for patients.

00:00:40.225 --> 00:00:44.104
Now, we happen to do that using an interactive robot and artificial intelligence,

00:00:44.104 --> 00:00:48.101
so we have a solution that is focused on solving real challenges that patients have;

00:00:48.101 --> 00:00:50.335
but doing that not for a small group of patients,

00:00:50.335 --> 00:00:53.640
but something that can scale to a large number of patients.

00:00:53.640 --> 00:00:55.229
Can you tell me a little bit about your background, please?

00:00:55.229 --> 00:00:57.144
And how you got started in healthcare?

00:00:57.144 --> 00:01:00.530
So I've been working in healthcare technology for about 20 years now.

00:01:00.530 --> 00:01:03.195
I started off 20 years ago.

00:01:03.195 --> 00:01:04.920
I was at Georgia Tech doing an undergrad degree in

00:01:04.920 --> 00:01:09.120
computer science focused in - really on healthcare applications of that.

00:01:09.120 --> 00:01:11.459
Ended up - actually, by the time I finished my undergrad degree,

00:01:11.459 --> 00:01:13.249
being hired as research faculty.

00:01:13.249 --> 00:01:17.205
Helped to build a research lab focused on what we now call aging in place.

00:01:17.205 --> 00:01:21.225
So a lot of different technology in the home environment for people to

00:01:21.225 --> 00:01:23.310
be able to live at home longer rather than move into

00:01:23.310 --> 00:01:25.786
a nursing home or assisted living facility.

00:01:25.786 --> 00:01:28.275
In 2001, when I went back to grad school,

00:01:28.275 --> 00:01:30.441
ended up going to MIT to the Media Lab;

00:01:30.441 --> 00:01:33.225
wanted to stay in that broad space of healthcare technology,

00:01:33.225 --> 00:01:36.329
but start, you know, playing with the new fun, cutting-edge technology.

00:01:36.329 --> 00:01:41.910
At that point, the idea of social robots or interactive robots was very new.

00:01:41.910 --> 00:01:43.770
There'd been a few of them built at that point in time and I

00:01:43.770 --> 00:01:46.080
thought they might have some promise in this space,

00:01:46.080 --> 00:01:48.870
so ended up spending a couple of years doing some of the studies

00:01:48.870 --> 00:01:52.214
around the basic psychology of interaction.

00:01:52.214 --> 00:01:56.676
And what that lead into is now what I've been doing for the last 15,

00:01:56.676 --> 00:01:59.577
16 years in seeing that okay, this works really well.

00:01:59.577 --> 00:02:02.101
When we care about long term engagement with people,

00:02:02.101 --> 00:02:06.125
there's this huge set of problems in healthcare that we need to solve around engagement;

00:02:06.125 --> 00:02:07.965
let's put these two things together.

00:02:07.965 --> 00:02:09.944
And so my work has iteratively, then,

00:02:09.944 --> 00:02:13.000
applied that into new areas of

00:02:13.000 --> 00:02:17.640
healthcare as we've continued to evolve and adapt the technology to support that.

00:02:17.640 --> 00:02:18.840
Can you explain to me a little bit about

00:02:18.840 --> 00:02:21.314
human/robot interaction and why it's important in robotics?

00:02:21.314 --> 00:02:25.735
Sure. So - I mean, human/robot interaction as a field is pretty broadly defined.

00:02:25.735 --> 00:02:28.470
You know, you've got robots and people interacting with one another.

00:02:28.470 --> 00:02:31.030
In terms of, like, academic tradition,

00:02:31.030 --> 00:02:33.825
it comes out the field most of human/computer interaction,

00:02:33.825 --> 00:02:35.640
which in turn comes from psychology.

00:02:35.640 --> 00:02:38.790
So for me, in a lot of the work I've done over the last 15,

00:02:38.790 --> 00:02:41.055
16 years in human/robot interaction,

00:02:41.055 --> 00:02:43.740
a lot of that comes back to how do we as people interact with one

00:02:43.740 --> 00:02:47.954
another and what of that can we apply in the world of technology?

00:02:47.954 --> 00:02:51.525
This particular technology typically takes a form that can look at us,

00:02:51.525 --> 00:02:54.875
maybe talk to us, maybe roam around in our spaces.

00:02:54.875 --> 00:02:56.490
So why do you feel that that's important that we

00:02:56.490 --> 00:02:58.440
have technology that can kind of interact with

00:02:58.440 --> 00:03:02.894
us and kind of work closely with us in building that bond between that technology?

00:03:02.894 --> 00:03:04.614
I think it really depends on the application, right?

00:03:04.614 --> 00:03:06.060
Obviously there was - as humans,

00:03:06.060 --> 00:03:07.977
we built technology for a very long time.

00:03:07.977 --> 00:03:10.710
It's something that we do.

00:03:10.710 --> 00:03:14.670
And, you know, you've got a lot of technology now that is getting more and

00:03:14.670 --> 00:03:19.000
more complex and doing things in the same physical spaces as us.

00:03:19.000 --> 00:03:22.140
And so part of that means just moving around around us.

00:03:22.140 --> 00:03:25.289
Many of these take the form of robots or some other automation, right?

00:03:25.289 --> 00:03:28.870
So the word robot is not necessarily very clearly defined.

00:03:28.870 --> 00:03:31.230
All right? So we can think about a lot of things that we

00:03:31.230 --> 00:03:33.630
interact with today or that we imagine that

00:03:33.630 --> 00:03:35.730
we're going to be interacting in the next few years or they're going to

00:03:35.730 --> 00:03:38.282
share space with us and we need to know what that means, right?

00:03:38.282 --> 00:03:39.600
When this stuff is, you know,

00:03:39.600 --> 00:03:41.460
in our environments, in our offices,

00:03:41.460 --> 00:03:42.720
in our homes, you know,

00:03:42.720 --> 00:03:45.189
and other places where we're going to interact with it,

00:03:45.189 --> 00:03:47.290
understanding how that's going to work so that

00:03:47.290 --> 00:03:50.890
this technology is to our benefit and not our detriment is very important.

00:03:50.890 --> 00:03:53.738
So that's a very broad thing, very deliberately,

00:03:53.738 --> 00:03:55.740
in terms of, you know, how we think about this,

00:03:55.740 --> 00:03:57.880
the kinds of things that we're building.

00:03:57.880 --> 00:04:02.690
But I think that's a lot of what human/robot interaction as a field of study is about.

00:04:02.690 --> 00:04:05.880
Right? Understanding how do we solve these problems in a way

00:04:05.880 --> 00:04:09.650
that benefit us as humans in terms of using this technology.

00:04:09.650 --> 00:04:12.690
What kind of studies are you aware of that have, kind of,

00:04:12.690 --> 00:04:15.240
taken place in this area that really, kind of,

00:04:15.240 --> 00:04:16.579
started to drive this movement,

00:04:16.579 --> 00:04:19.050
that were kind of like a focal point that people started saying hey,

00:04:19.050 --> 00:04:21.090
we really need to kind of, you know,

00:04:21.090 --> 00:04:26.280
focus a lot more on this aspect of how people interface with machines and not necessarily

00:04:26.280 --> 00:04:28.800
just that the machine is here and it's going to do a job and we

00:04:28.800 --> 00:04:32.394
really don't care about how it ties in socially.

00:04:32.394 --> 00:04:34.230
Right. So - I mean, there are thousands of

00:04:34.230 --> 00:04:36.360
studies in human/robot interaction that have been done over

00:04:36.360 --> 00:04:38.670
the last two decades and that really builds on

00:04:38.670 --> 00:04:41.280
the field of human/computer interaction where you've got many,

00:04:41.280 --> 00:04:45.505
many more studies going back for about a half a century now, probably even longer.

00:04:45.505 --> 00:04:48.090
So - you know, it's a field that people have thought about for

00:04:48.090 --> 00:04:52.890
a long time and there's a lot of different aspects to these studies.

00:04:52.890 --> 00:04:54.775
Some of them are, you know,

00:04:54.775 --> 00:04:56.460
very broad and trying to understand - you know,

00:04:56.460 --> 00:04:58.200
you put robots into a particular environment,

00:04:58.200 --> 00:05:00.559
how do people interact or react?

00:05:00.559 --> 00:05:02.399
Some of them are very specific, right?

00:05:02.399 --> 00:05:04.019
So getting down to some of the details.

00:05:04.019 --> 00:05:05.590
You know, if we have a mobile robot,

00:05:05.590 --> 00:05:07.410
say they can move around.

00:05:07.410 --> 00:05:10.157
How close can it come to you and you're still comfortable, right?

00:05:10.157 --> 00:05:11.310
So very similar to,

00:05:11.310 --> 00:05:12.390
you know, interpersonal space.

00:05:12.390 --> 00:05:13.984
We're a few feet apart right now, right?

00:05:13.984 --> 00:05:15.620
This would be a bit different if we were in

00:05:15.620 --> 00:05:17.920
there right up close - next to one another, right?

00:05:17.920 --> 00:05:20.360
So you start to feel awkward when I get into your face,

00:05:20.360 --> 00:05:22.280
the same kind of thing with robots.

00:05:22.280 --> 00:05:23.680
Is it the same as with us?

00:05:23.680 --> 00:05:26.869
Is, you know, six inches too close with a robot?

00:05:26.869 --> 00:05:29.580
Definitely is. If, you know, we were talking six inches apart;

00:05:29.580 --> 00:05:31.430
three feet, this feels about right.

00:05:31.430 --> 00:05:34.550
Is it that the same with the robot? So you've got a whole range of things and you can

00:05:34.550 --> 00:05:39.165
imagine all different aspects of interpersonal interaction with other people.

00:05:39.165 --> 00:05:41.420
How do we apply those in the world of technology,

00:05:41.420 --> 00:05:43.610
specifically in the world of robotics?

00:05:43.610 --> 00:05:45.920
So there's this whole body of literature,

00:05:45.920 --> 00:05:47.807
again, looking at the very broad things, right?

00:05:47.807 --> 00:05:50.510
Robots in particular situations or how do they help us in

00:05:50.510 --> 00:05:54.020
education or healthcare or entertainment to the very specific.

00:05:54.020 --> 00:05:58.610
You know, if a robot does this particular action or says this thing or,

00:05:58.610 --> 00:06:00.099
you know, makes this sound or - right?

00:06:00.099 --> 00:06:01.740
These very specific pieces.

00:06:01.740 --> 00:06:03.705
There are a lot of studies around that as well.

00:06:03.705 --> 00:06:07.354
Now, doesn't mean that the field is done, right?

00:06:07.354 --> 00:06:10.605
There's so much more that we have to understand for a couple of reasons.

00:06:10.605 --> 00:06:12.560
One is, you know, there are

00:06:12.560 --> 00:06:15.665
perhaps an infinite number of those types of questions that we can answer.

00:06:15.665 --> 00:06:17.105
We can keep thinking of new ones;

00:06:17.105 --> 00:06:18.970
but also, the technology is evolving.

00:06:18.970 --> 00:06:21.560
If we look at what an interactive robot or a social robot is

00:06:21.560 --> 00:06:26.000
today versus what it was five years ago or 10 years ago or 20 years ago,

00:06:26.000 --> 00:06:27.510
the technology is very different,

00:06:27.510 --> 00:06:28.950
its capabilities are different.

00:06:28.950 --> 00:06:31.700
As we continue to develop new things,

00:06:31.700 --> 00:06:34.460
whether that be the technology or the use cases for it,

00:06:34.460 --> 00:06:37.610
then we need to keep exploring these interactions and again,

00:06:37.610 --> 00:06:39.860
going back to how do we make this technology in a way that

00:06:39.860 --> 00:06:43.175
benefits us as people and the users of it.

00:06:43.175 --> 00:06:48.095
So looking at some of these studies that have been done from a psychological standpoint,

00:06:48.095 --> 00:06:50.155
how would you conduct one of those types of studies?

00:06:50.155 --> 00:06:52.549
Like, what would you find that would be useful information?

00:06:52.549 --> 00:06:54.620
So for instance, I know you got close to my face a few

00:06:54.620 --> 00:06:56.750
seconds ago and you're like, does this make you uncomfortable?

00:06:56.750 --> 00:06:57.740
A great way to do it right.

00:06:57.740 --> 00:06:59.870
And you know, for some people it does, for me personally,

00:06:59.870 --> 00:07:02.195
I have a very small personal bubble so that doesn't bother me a whole lot.

00:07:02.195 --> 00:07:05.990
I saw that look.

00:07:05.990 --> 00:07:08.234
But in those situations, though,

00:07:08.234 --> 00:07:11.555
there's generally a distance that people typically like to hold, right?

00:07:11.555 --> 00:07:13.955
A lot of it is cultural, some of it is social,

00:07:13.955 --> 00:07:15.300
some of it is just personal.

00:07:15.300 --> 00:07:17.725
But how how would you go about setting up one of these studies?

00:07:17.725 --> 00:07:20.000
What kind of studies like these are typically conducted when you're

00:07:20.000 --> 00:07:22.564
trying to do some sort of research for human/robot interaction?

00:07:22.564 --> 00:07:25.102
Are you trying to say well, a robot four feet away is okay;

00:07:25.102 --> 00:07:27.885
robot two feet away, not so much.

00:07:27.885 --> 00:07:30.099
Or a robot that's two feet away that's six feet tall,

00:07:30.099 --> 00:07:32.435
I really don't like, but if it's only half-a-foot tall,

00:07:32.435 --> 00:07:34.090
it doesn't really bother me a whole lot.

00:07:34.090 --> 00:07:38.000
Sure. I mean, some of those questions are very specific, right?

00:07:38.000 --> 00:07:40.340
And you can imagine how to design a study and, you know,

00:07:40.340 --> 00:07:43.510
any particular study needs a design that's appropriate for it.

00:07:43.510 --> 00:07:45.005
But as a rule of thumb,

00:07:45.005 --> 00:07:46.605
again, I'd go back to psychology.

00:07:46.605 --> 00:07:52.685
So when I was starting off doing my master's degree at the MIT Media Lab 16 years ago,

00:07:52.685 --> 00:07:56.225
one of my mentors was the late Cliff Nass at Stanford.

00:07:56.225 --> 00:08:00.979
And Cliff semi-jokingly would talk about his research methodology as this;

00:08:00.979 --> 00:08:04.904
he'd say - so Cliff was an expert in human/computer interaction, right?

00:08:04.904 --> 00:08:07.010
Later went on to do some work in robotics,

00:08:07.010 --> 00:08:09.170
but he would say, you know,

00:08:09.170 --> 00:08:11.862
go into the psychology section of a library, right?

00:08:11.862 --> 00:08:14.720
I find a paper with a great study about two people

00:08:14.720 --> 00:08:18.290
interacting with one another and I would take one of those people,

00:08:18.290 --> 00:08:20.556
cross it out, put computer and steal it.

00:08:20.556 --> 00:08:23.720
Right? And so we ended up publishing,

00:08:23.720 --> 00:08:25.729
I don't know, possibly hundreds of studies,

00:08:25.729 --> 00:08:27.714
a lot of different studies of human/computer interaction.

00:08:27.714 --> 00:08:29.240
Now he was very clever, then,

00:08:29.240 --> 00:08:32.860
about how he translated those into the world of people interacting with technology,

00:08:32.860 --> 00:08:34.640
but in general, that's a great technique.

00:08:34.640 --> 00:08:37.100
So some of those early studies that I was doing, you know,

00:08:37.100 --> 00:08:38.420
more than a decade-and-a-half ago,

00:08:38.420 --> 00:08:40.042
Cliff and I designed together,

00:08:40.042 --> 00:08:41.810
really using that kind of methodology.

00:08:41.810 --> 00:08:44.715
Okay. If there's this kind of a problem that you want to solve,

00:08:44.715 --> 00:08:46.400
this kind of a thing you want to learn,

00:08:46.400 --> 00:08:50.044
again, there's a great precedent in how do we as people interact with one another;

00:08:50.044 --> 00:08:51.369
let's go borrow from that, right?

00:08:51.369 --> 00:08:54.850
Let's learn what we've already used in, you know,

00:08:54.850 --> 00:08:58.490
academic or research settings to understand how people are interacting

00:08:58.490 --> 00:09:02.395
with one another and understand how we apply that with technology.

00:09:02.395 --> 00:09:03.945
And so it's a great way to think about that.

00:09:03.945 --> 00:09:07.014
Again, what we're building in terms of the technology is very new;

00:09:07.014 --> 00:09:09.420
what we're creating in terms of the interactions is not.

00:09:09.420 --> 00:09:13.250
We really want to leverage what we as people already innately

00:09:13.250 --> 00:09:16.980
or naturally know what to do and how to do with other people,

00:09:16.980 --> 00:09:20.650
but understand where the technology fits into that.

00:09:20.650 --> 00:09:22.620
So when we talk about, kind of,

00:09:22.620 --> 00:09:25.790
this human/robot interaction and this technology building,

00:09:25.790 --> 00:09:27.650
this technology, obviously the idea of

00:09:27.650 --> 00:09:30.430
prototyping and building the actual physical robot.

00:09:30.430 --> 00:09:32.210
And there's a lot of work and a lot of studies that have

00:09:32.210 --> 00:09:34.390
been done on how to build the robot.

00:09:34.390 --> 00:09:34.632
Right.

00:09:34.632 --> 00:09:35.900
So what are some of the things that you do

00:09:35.900 --> 00:09:37.954
at Catalia Health when you were designing Mabu?

00:09:37.954 --> 00:09:41.480
So - you know, in the early days of building, you know,

00:09:41.480 --> 00:09:44.765
the application that we're doing in terms of what's going to interact with patients,

00:09:44.765 --> 00:09:46.639
we didn't need the actual physical robot, right?

00:09:46.639 --> 00:09:48.240
That could be built in parallel.

00:09:48.240 --> 00:09:50.810
So we started testing things out with patients.

00:09:50.810 --> 00:09:52.280
Now again, didn't have the robot yet,

00:09:52.280 --> 00:09:56.405
so the very first robot we had was - you know those little blue recycling bins?

00:09:56.405 --> 00:09:58.835
We took one of those, we turned it upside down,

00:09:58.835 --> 00:10:01.690
put two googly eyes on it and taped

00:10:01.690 --> 00:10:04.817
the tablet to it with some mock-up software running on it, right?

00:10:04.817 --> 00:10:06.140
And went and put that in front of patients.

00:10:06.140 --> 00:10:07.439
That was our first version.

00:10:07.439 --> 00:10:09.410
Next version, we got much more advanced.

00:10:09.410 --> 00:10:11.585
We took that and we put a Bluetooth speaker inside of it

00:10:11.585 --> 00:10:14.830
and one of our writers was in the next room acting as the robot.

00:10:14.830 --> 00:10:19.070
And these kind of interactions actually told us a lot because we could constrain

00:10:19.070 --> 00:10:22.740
things in such a way that we were trying to solve particular challenges,

00:10:22.740 --> 00:10:26.910
trying to think about how we would introduce this with people and what kind of

00:10:26.910 --> 00:10:29.220
conversation should we have or we could constrain

00:10:29.220 --> 00:10:32.680
what conversations we were having on our heads and see what people did.

00:10:32.680 --> 00:10:34.880
We got - it was pretty amazing, you know,

00:10:34.880 --> 00:10:37.960
you put this blue trash can down in front of someone, you know,

00:10:37.960 --> 00:10:40.739
the first - like they're going to look at you like you're a little bit crazy,

00:10:40.739 --> 00:10:43.820
but then you start having a conversation and they get right into it.

00:10:43.820 --> 00:10:49.680
We've got videos of some of our early patient testers who were using this and,

00:10:49.680 --> 00:10:52.335
you know, they start opening up after about 60,

00:10:52.335 --> 00:10:53.750
90 seconds of talking to this, right?

00:10:53.750 --> 00:10:56.445
Totally forget that it's a blue trash can sitting in front of them,

00:10:56.445 --> 00:10:58.260
but getting right into what we were trying to get.

00:10:58.260 --> 00:11:03.450
So you can achieve a lot with building very little hardware and software if you

00:11:03.450 --> 00:11:09.030
want to think about the user experience and the user interaction side of building robots.

00:11:09.030 --> 00:11:10.675
And so again, we were doing these in parallel, right?

00:11:10.675 --> 00:11:12.705
Part of it's building the hardware because ultimately,

00:11:12.705 --> 00:11:16.509
we want a product that we can put in front of a patient and have them interact with;

00:11:16.509 --> 00:11:19.740
but we can start doing a lot of the design of that before - you know,

00:11:19.740 --> 00:11:22.135
well before we have the actual robot.

00:11:22.135 --> 00:11:25.140
So why do you think it was that if you could put just a blue trash can with

00:11:25.140 --> 00:11:26.820
some googly eyes in front of people that they would

00:11:26.820 --> 00:11:28.845
open up and kind of feel this connection?

00:11:28.845 --> 00:11:32.049
What do you think it is about that that kind of triggers that?

00:11:32.049 --> 00:11:34.455
Yeah. So - I mean, it's a great question and it comes down to, you know,

00:11:34.455 --> 00:11:37.020
really what we focus on here at Catalia Health,

00:11:37.020 --> 00:11:39.450
which is not so much the technology.

00:11:39.450 --> 00:11:42.240
The technology is in support of the real problems that we're trying to solve,

00:11:42.240 --> 00:11:43.875
which is how do we engage a patient,

00:11:43.875 --> 00:11:46.500
keep them engaged for longer in their own healthcare.

00:11:46.500 --> 00:11:49.040
And for us, that comes down to really three things.

00:11:49.040 --> 00:11:53.494
So what we're building is the intersection of medicine; medical best practices.

00:11:53.494 --> 00:11:56.580
So in a particular group of patients dealing with

00:11:56.580 --> 00:12:00.900
a certain condition or disease and a certain treatment, what are the best practices?

00:12:00.900 --> 00:12:02.730
Like, what do we need to be talking to those patients about?

00:12:02.730 --> 00:12:05.139
What are the things that we follow up on with them?

00:12:05.139 --> 00:12:08.305
Two is psychology. Really a few types of psychology,

00:12:08.305 --> 00:12:11.030
so in particular, the psychology of relationships.

00:12:11.030 --> 00:12:12.870
How do we as people create,

00:12:12.870 --> 00:12:15.030
build up and maintain a relationship over time,

00:12:15.030 --> 00:12:17.010
as well as some aspects of the psychology of

00:12:17.010 --> 00:12:19.875
behavior change and a few other things that we draw from here.

00:12:19.875 --> 00:12:21.240
And then ultimately, for us,

00:12:21.240 --> 00:12:23.875
those are tied together with our artificial intelligence algorithms.

00:12:23.875 --> 00:12:28.650
So that's how we're really generating a conversation in real time for that patient.

00:12:28.650 --> 00:12:29.850
Now those first two, though,

00:12:29.850 --> 00:12:31.755
don't require the AI algorithms

00:12:31.755 --> 00:12:33.975
and those are the things that we can start testing right away.

00:12:33.975 --> 00:12:35.975
Right? We know what kind of conversations we're going to build.

00:12:35.975 --> 00:12:37.650
We may not be ready to deliver them purely through

00:12:37.650 --> 00:12:39.900
software when we start doing our testing,

00:12:39.900 --> 00:12:41.430
but we can create those conversations,

00:12:41.430 --> 00:12:43.480
we can have those conversations with a patient.

00:12:43.480 --> 00:12:46.290
And ultimately, those conversations that we have, you know,

00:12:46.290 --> 00:12:48.585
using humans with our patients,

00:12:48.585 --> 00:12:51.395
same as a doctor or a nurse is going to have,

00:12:51.395 --> 00:12:53.631
that's what we want to be building into our technology, right?

00:12:53.631 --> 00:12:57.240
So the technology is what ultimately gets us to the point where we can have

00:12:57.240 --> 00:12:59.910
those conversations in a very scalable way

00:12:59.910 --> 00:13:03.015
across tens or hundreds or thousands or millions of patients.

00:13:03.015 --> 00:13:05.475
Having those conversations with, you know,

00:13:05.475 --> 00:13:08.839
a speaker inside of an upside-down recycling bin

00:13:08.839 --> 00:13:10.980
is a great way to start to get us there, right?

00:13:10.980 --> 00:13:14.325
And really understanding the nature of that interaction because again,

00:13:14.325 --> 00:13:16.610
for us, it really comes back to the psychology.

00:13:16.610 --> 00:13:19.349
How do you actually engage that person?

00:13:19.349 --> 00:13:22.710
It shouldn't matter whether it's a robot doing it or a person doing it.

00:13:22.710 --> 00:13:24.140
Obviously there are some differences, right?

00:13:24.140 --> 00:13:26.040
And that's the stuff that we've studied along

00:13:26.040 --> 00:13:28.747
the way and many other people now have as well,

00:13:28.747 --> 00:13:30.930
but the nature of those conversations,

00:13:30.930 --> 00:13:33.685
you can deliver through many different means.

00:13:33.685 --> 00:13:36.510
I think you've hit on something that's really kind of important when you talk about

00:13:36.510 --> 00:13:39.415
robotics because a lot of people think when you're working with robots,

00:13:39.415 --> 00:13:41.215
it's just tech, right?

00:13:41.215 --> 00:13:45.217
It's the algorithms, it's the hardware, right?

00:13:45.217 --> 00:13:49.125
And that's not the point typically of a robot.

00:13:49.125 --> 00:13:52.499
And the point typically of a robot is to solve a problem in

00:13:52.499 --> 00:13:56.590
an efficient way or in a way that's not achievable otherwise.

00:13:56.590 --> 00:13:58.535
So doing what you've done,

00:13:58.535 --> 00:13:59.670
in, kind of, focusing,

00:13:59.670 --> 00:14:04.675
you're just saying this is the tech and the tech kind of just fits in, right?

00:14:04.675 --> 00:14:07.350
It might not be the most - it doesn't need to be cutting edge,

00:14:07.350 --> 00:14:09.480
it needs to be reliable and it needs to

00:14:09.480 --> 00:14:12.075
work well in the situations that we need to test it in.

00:14:12.075 --> 00:14:14.450
And then we can move forward to actually solving the bigger problem.

00:14:14.450 --> 00:14:14.880
Exactly.

00:14:14.880 --> 00:14:18.450
Which I think is great. Looking at

00:14:18.450 --> 00:14:22.160
these interactions with this Bluetooth speaker under the bin,

00:14:22.160 --> 00:14:24.560
I'm actually really kind of - it's kind of blowing my mind that

00:14:24.560 --> 00:14:27.945
that's the actual approach that you went with your first prototype.

00:14:27.945 --> 00:14:29.289
What kind of prototype, I guess,

00:14:29.289 --> 00:14:31.950
did you bring to investors first of all and what was

00:14:31.950 --> 00:14:35.700
the reaction once you brought it to investors, typically?

00:14:35.700 --> 00:14:37.290
So - you know, we've talked to a lot of

00:14:37.290 --> 00:14:39.150
different investors in the life of this company or,

00:14:39.150 --> 00:14:42.929
you know, even starting before the company officially existed, right?

00:14:42.929 --> 00:14:45.645
So started talking to investors about three years ago now.

00:14:45.645 --> 00:14:47.640
And you know, those very early days, you know,

00:14:47.640 --> 00:14:48.774
we didn't even have a trash can yet;

00:14:48.774 --> 00:14:50.528
you know, it was really concept.

00:14:50.528 --> 00:14:52.140
Raised money, you know,

00:14:52.140 --> 00:14:54.150
initial angel money based on that, right?

00:14:54.150 --> 00:14:55.500
Really, on the concept.

00:14:55.500 --> 00:14:58.080
Now, you know, I should say in the background, you know,

00:14:58.080 --> 00:14:59.730
before this, I built the first version

00:14:59.730 --> 00:15:01.560
of this and deployed it with patients 10 years ago.

00:15:01.560 --> 00:15:01.840
Sure.

00:15:01.840 --> 00:15:04.080
So it's not like this was a brand new concept.

00:15:04.080 --> 00:15:05.310
There's a lot of, you know,

00:15:05.310 --> 00:15:08.780
academic and medical research behind how and why this is going to work.

00:15:08.780 --> 00:15:12.679
But in terms of Catalia Health - and this company

00:15:12.679 --> 00:15:17.970
raised the initial money before any prototype for hardware or software existed.

00:15:17.970 --> 00:15:21.660
Now, the bulk of our first seed round came after we

00:15:21.660 --> 00:15:26.000
had a prototype that started to look like what the real product does now.

00:15:26.000 --> 00:15:28.800
So the more - you know, as with any company,

00:15:28.800 --> 00:15:33.015
the more you get in terms of fidelity and closeness to a real product,

00:15:33.015 --> 00:15:34.905
real customers, obviously, you know,

00:15:34.905 --> 00:15:37.375
that makes a much easier story with investors.

00:15:37.375 --> 00:15:39.270
And so for us, in the early days,

00:15:39.270 --> 00:15:42.000
it was really about here's the problem that we're focused on solving;

00:15:42.000 --> 00:15:44.319
here's the technology that we're going to be building;

00:15:44.319 --> 00:15:45.630
here's why we, as a team,

00:15:45.630 --> 00:15:47.699
are good to build that, right?

00:15:47.699 --> 00:15:51.000
In part, because I have done this before - I built this kind of technology,

00:15:51.000 --> 00:15:53.160
successfully delivered it to patients.

00:15:53.160 --> 00:15:56.435
And here's where we think we're going and why we can get there on a certain timeline.

00:15:56.435 --> 00:16:01.120
So that was a lot of that message in the early days of fundraising for Catalia Health.

00:16:01.120 --> 00:16:06.805
How many iterations did it take you finally feel comfortable with your final product?

00:16:06.805 --> 00:16:09.720
So in terms of the design of the physical thing

00:16:09.720 --> 00:16:12.866
or the software or the interactions or which part of it?

00:16:12.866 --> 00:16:15.344
Just from the physical - the physical robot, right?

00:16:15.344 --> 00:16:15.719
Yeah.

00:16:15.719 --> 00:16:18.990
'Cause you're trying to really focus on how a patient is going to engage with

00:16:18.990 --> 00:16:22.630
this and it's very important that the patient doesn't feel weird.

00:16:22.630 --> 00:16:23.130
Right.

00:16:23.130 --> 00:16:26.793
Right? So how do you kind of go about - okay,

00:16:26.793 --> 00:16:29.410
well, this is where we started, this was okay.

00:16:29.410 --> 00:16:31.349
You know, obviously you had the blue trash can,

00:16:31.349 --> 00:16:33.780
googly eyes, then a Bluetooth speaker.

00:16:33.780 --> 00:16:38.170
Right? And then it slowly - it seems like it slowly kind of built up over time.

00:16:38.170 --> 00:16:39.884
But there's a point where you say okay,

00:16:39.884 --> 00:16:41.980
I have all the functioning components that I need,

00:16:41.980 --> 00:16:43.980
but now I'm going to change the design

00:16:43.980 --> 00:16:48.760
because the eye doesn't look quite right or the head isn't quite the right shape.

00:16:48.760 --> 00:16:52.650
What kind of - how did you go about making those types of decisions?

00:16:52.650 --> 00:16:52.920
Sure.

00:16:52.920 --> 00:16:54.465
What kind of testing did you do,

00:16:54.465 --> 00:16:56.125
like to a focus group?

00:16:56.125 --> 00:17:01.829
And then do you feel that right now you're at a stable point with your prize?

00:17:01.829 --> 00:17:05.260
With designing our Mabu robot for Catalia Health.

00:17:05.260 --> 00:17:08.857
We actually did a very fast process of designing that actual robot, right?

00:17:08.857 --> 00:17:10.820
So the really early early stuff in terms of,

00:17:10.820 --> 00:17:12.736
you know, the trash can or, you know,

00:17:12.736 --> 00:17:15.365
if you can see the cardboard things behind us here,

00:17:15.365 --> 00:17:17.810
those were really for testing more the user experience,

00:17:17.810 --> 00:17:19.700
not the actual design of the robot, right?

00:17:19.700 --> 00:17:22.175
And we deliberately separated these things out.

00:17:22.175 --> 00:17:24.110
The actual design of the robot from

00:17:24.110 --> 00:17:26.785
beginning to end took us about four to five weeks. All right?

00:17:26.785 --> 00:17:29.120
Now we went with, you know, IDEO,

00:17:29.120 --> 00:17:31.130
a great design firm, worked with

00:17:31.130 --> 00:17:34.605
their San Francisco studio a few blocks from where we are now.

00:17:34.605 --> 00:17:36.860
So IDEO was actually one of the early investors in our company,

00:17:36.860 --> 00:17:38.930
we were start-up in residence so we spent five months

00:17:38.930 --> 00:17:42.005
working right in the middle of their studio.

00:17:42.005 --> 00:17:45.370
The industrial design process for this robot was, like I said, about five weeks.

00:17:45.370 --> 00:17:46.430
So we went in with, you know,

00:17:46.430 --> 00:17:48.810
here is the design brief.

00:17:48.810 --> 00:17:50.690
The first week or so was, you know,

00:17:50.690 --> 00:17:52.541
coming up with all sorts of crazy ideas, right?

00:17:52.541 --> 00:17:54.500
So taking that many different directions,

00:17:54.500 --> 00:17:55.940
things that look, you know,

00:17:55.940 --> 00:17:57.540
all different, you know,

00:17:57.540 --> 00:17:59.840
designs and styles and, you know,

00:17:59.840 --> 00:18:01.250
some things that might be an appliance,

00:18:01.250 --> 00:18:03.065
some things that are like totally sci-fi,

00:18:03.065 --> 00:18:05.890
some things that there's no way we could ever manufacture,

00:18:05.890 --> 00:18:09.200
but starting off with just this ideation phase.

00:18:09.200 --> 00:18:10.950
And from that, we started to narrow down.

00:18:10.950 --> 00:18:12.770
So let's get rid of the things that are just like

00:18:12.770 --> 00:18:15.540
totally weird and out there we don't think people would ever want.

00:18:15.540 --> 00:18:17.345
Let's get rid of things that, you know,

00:18:17.345 --> 00:18:18.665
maybe we could manufacture,

00:18:18.665 --> 00:18:20.845
but it would cost just a whole lot of money and as a start-up,

00:18:20.845 --> 00:18:22.585
that's just not a good thing.

00:18:22.585 --> 00:18:23.750
And that left us, you know,

00:18:23.750 --> 00:18:24.919
with a smaller set of things.

00:18:24.919 --> 00:18:26.880
And we started putting those in front of people. All right?

00:18:26.880 --> 00:18:28.775
So I wouldn't say it's, you know, particularly focus groups,

00:18:28.775 --> 00:18:33.560
but going out to places where we could interact with our potential patient population.

00:18:33.560 --> 00:18:35.980
So we help a lot of patients who tend to be older,

00:18:35.980 --> 00:18:39.700
not necessarily; tend to be dealing with chronic diseases.

00:18:39.700 --> 00:18:42.109
And so, you know, putting some of these designs in front of people.

00:18:42.109 --> 00:18:44.085
Now at first, that was stuff on paper, right?

00:18:44.085 --> 00:18:45.540
Drawings on the computer screen,

00:18:45.540 --> 00:18:47.375
on paper, getting some quick feedback.

00:18:47.375 --> 00:18:51.590
And we tried to really rapidly iterate and narrow in on what might work.

00:18:51.590 --> 00:18:52.935
And so it's a combination of,

00:18:52.935 --> 00:18:54.485
you know, feedback that we got from people,

00:18:54.485 --> 00:18:56.990
but also my experience of building stuff like this for

00:18:56.990 --> 00:18:59.900
the last 15 years by the time we were doing that design.

00:18:59.900 --> 00:19:01.370
And so just an understanding, then,

00:19:01.370 --> 00:19:03.250
of what people are going to react to.

00:19:03.250 --> 00:19:06.159
So our design brief going into that was really simple.

00:19:06.159 --> 00:19:10.101
One, this needs to have eyes because we know the power of this psychologically, right?

00:19:10.101 --> 00:19:11.870
Something that actually looks at you and makes

00:19:11.870 --> 00:19:14.460
eye contact while it's interacting is very important.

00:19:14.460 --> 00:19:17.090
Two, we wanted a screen on it.

00:19:17.090 --> 00:19:19.595
Speech technology is getting much better.

00:19:19.595 --> 00:19:21.320
So this was - you know, a couple of years ago,

00:19:21.320 --> 00:19:22.845
the Amazon Echo was out.

00:19:22.845 --> 00:19:25.640
Today we have, you know, more devices like this from,

00:19:25.640 --> 00:19:28.810
you know, Google and Apple and many other things that we can talk to.

00:19:28.810 --> 00:19:31.280
It's definitely getting better, there's a lot more that you can do.

00:19:31.280 --> 00:19:34.610
But in healthcare, we need to make sure that we're always very accurate.

00:19:34.610 --> 00:19:37.189
So one of the things that we do on the robot is when she's speaking,

00:19:37.189 --> 00:19:39.230
she's showing what she's saying on the screen.

00:19:39.230 --> 00:19:41.915
I can respond by talking to her or pressing a button.

00:19:41.915 --> 00:19:43.655
And sometimes we might want to say oh, hey,

00:19:43.655 --> 00:19:46.669
do you remember when your doctor showed you how to whatever, right?

00:19:46.669 --> 00:19:48.074
Can I show you that video again?

00:19:48.074 --> 00:19:49.375
So to be able to do things like that.

00:19:49.375 --> 00:19:51.200
So that was criteria number two.

00:19:51.200 --> 00:19:53.530
And then point number three was we wanted

00:19:53.530 --> 00:19:57.080
this roughly the size of a kitchen appliance, right?

00:19:57.080 --> 00:20:00.349
Now, we could build something that's humanoid as big as you or me;

00:20:00.349 --> 00:20:02.790
it's going to take up a lot of space in our home, right?

00:20:02.790 --> 00:20:04.575
That's not great for a lot of people.

00:20:04.575 --> 00:20:06.260
We could make this the size of a cell phone.

00:20:06.260 --> 00:20:08.870
Right? Possible, given the technology,

00:20:08.870 --> 00:20:11.240
it's going to get, you know, put behind the bookshelf or,

00:20:11.240 --> 00:20:12.415
you know, tossed in the kitchen drawer.

00:20:12.415 --> 00:20:14.160
People are going to forget about it too easily.

00:20:14.160 --> 00:20:15.800
So it's kind of roughly the size.

00:20:15.800 --> 00:20:18.410
And so, you know, with that set of criteria,

00:20:18.410 --> 00:20:21.559
we were able to then hone in on some of these early designs to say okay,

00:20:21.559 --> 00:20:23.030
here's the set of things that fit.

00:20:23.030 --> 00:20:25.725
Now, okay, we actually like this from this one, this from this one,

00:20:25.725 --> 00:20:29.665
let's put those together - and so just a very rapid iteration process.

00:20:29.665 --> 00:20:32.930
You know, having a team of professional industrial designers who were

00:20:32.930 --> 00:20:36.500
focused on this project and able to really quickly iterate let us go from,

00:20:36.500 --> 00:20:41.975
you know, initial concept to basically what we have today in a period of about a month.

00:20:41.975 --> 00:20:47.055
Was there any other design or talk about having the robot look more lifelike?

00:20:47.055 --> 00:20:50.990
So, you know, looking lifelike in a robot can be good or bad.

00:20:50.990 --> 00:20:53.600
And that's a matter of perspective and opinion, I think.

00:20:53.600 --> 00:20:56.690
There are some parts of the world where people are really focused

00:20:56.690 --> 00:21:00.135
on designing robots that might look very humanoid.

00:21:00.135 --> 00:21:04.990
There's this concept called The Uncanny Valley that I'm sure you're familiar with and,

00:21:04.990 --> 00:21:08.750
you know, we really don't want to fall into the bottom of The Uncanny Valley, right?

00:21:08.750 --> 00:21:11.614
This thing just ends up looking very creepy.

00:21:11.614 --> 00:21:13.400
Given what we're doing also,

00:21:13.400 --> 00:21:14.485
again going back to, you know,

00:21:14.485 --> 00:21:17.509
where we position ourselves as a company in healthcare,

00:21:17.509 --> 00:21:19.630
we want to make sure we set up the right expectations.

00:21:19.630 --> 00:21:23.770
So Uncanny Valley is often talked about as the look of this thing, right?

00:21:23.770 --> 00:21:25.249
You don't want to look too humanoid,

00:21:25.249 --> 00:21:29.600
but there are also a lot of other expectations that come out of that,

00:21:29.600 --> 00:21:32.797
not just the appearance, but the overall interaction.

00:21:32.797 --> 00:21:36.409
So for the same reason that we don't try to make this look like a person,

00:21:36.409 --> 00:21:37.990
we don't make it sound like a person.

00:21:37.990 --> 00:21:40.580
Right? Now we want a very intelligible voice,

00:21:40.580 --> 00:21:42.905
so we have a computer-generated voice that's, you know,

00:21:42.905 --> 00:21:47.305
easy to understand, but is not human-recorded speech.

00:21:47.305 --> 00:21:49.220
And also, for similar reasons,

00:21:49.220 --> 00:21:51.924
we don't have completely open-ended conversations with this.

00:21:51.924 --> 00:21:55.100
The robot's always directing the conversation because we don't want to set up

00:21:55.100 --> 00:21:59.562
expectations with our patients that this can just talk about anything, right?

00:21:59.562 --> 00:22:03.130
You can just sit down and talk to it because then the expectations are too high.

00:22:03.130 --> 00:22:05.390
So with that set of things,

00:22:05.390 --> 00:22:08.750
that helps us to start make these decisions about,

00:22:08.750 --> 00:22:09.920
you know, what this looks like,

00:22:09.920 --> 00:22:11.400
how humanoid should it be.

00:22:11.400 --> 00:22:12.765
If you look at our Mabu robot,

00:22:12.765 --> 00:22:15.020
it's - you know, you're never going to confuse this for a person, right?

00:22:15.020 --> 00:22:17.745
It's clearly yellow plastic or in the near future,

00:22:17.745 --> 00:22:19.789
other colors as well.

00:22:19.789 --> 00:22:21.199
It has human-like features;

00:22:21.199 --> 00:22:22.724
it has eyes that are going to look at you.

00:22:22.724 --> 00:22:24.240
Oh, let's see, there's a head on there,

00:22:24.240 --> 00:22:26.750
but the rest of it's, you know, this squat body on a screen, right?

00:22:26.750 --> 00:22:29.170
So it's not like this looks just like a human,

00:22:29.170 --> 00:22:34.479
but enough of those features to really draw on the psychology of interaction, right?

00:22:34.479 --> 00:22:37.114
Something looks at me,

00:22:37.114 --> 00:22:38.765
I immediately look back.

00:22:38.765 --> 00:22:42.325
So using those types of things in the design are very important,

00:22:42.325 --> 00:22:45.716
but trying to make it look like a person;

00:22:45.716 --> 00:22:48.950
one, it's going to lead to the wrong expectations and two, it's very expensive.

00:22:48.950 --> 00:22:51.020
Building a product like that starts to

00:22:51.020 --> 00:22:53.270
get really difficult to actually manufacture a scale.

00:22:53.270 --> 00:22:56.210
For the same reason that you have the eyes that kind of look at you,

00:22:56.210 --> 00:22:58.850
was there a conscious decision to not make a mouth that moves as well?

00:22:58.850 --> 00:23:01.025
Because you do fall into that Uncanny Valley,

00:23:01.025 --> 00:23:02.950
even though it would be more of a - like

00:23:02.950 --> 00:23:06.459
a computer-synthesized or computer-generated voice?

00:23:06.459 --> 00:23:08.900
That you were worried about the lips

00:23:08.900 --> 00:23:11.590
or the mouth kind of sinking up to the words and making sure -

00:23:11.590 --> 00:23:12.835
Yes. So it's a combination of things,

00:23:12.835 --> 00:23:14.320
right? Doing that is really hard.

00:23:14.320 --> 00:23:17.024
And you said it's one thing and two,

00:23:17.024 --> 00:23:19.250
I challenge you to go find a robot out there

00:23:19.250 --> 00:23:21.990
that has moving lips that actually looks good.

00:23:21.990 --> 00:23:24.925
It always ends up being a little bit creepy.

00:23:24.925 --> 00:23:27.080
Now, maybe there are some out there I haven't seen yet;

00:23:27.080 --> 00:23:29.280
there's a lot of people working on stuff like this right now,

00:23:29.280 --> 00:23:32.930
so it would be great if there is, but I think for what we're doing,

00:23:32.930 --> 00:23:36.950
it doesn't necessarily add a lot of value to it.

00:23:36.950 --> 00:23:40.130
People perceive this thing as talking to them even though the mouth is,

00:23:40.130 --> 00:23:43.595
you know a smile line embedded in the plastic.

00:23:43.595 --> 00:23:46.755
It's not like there's anything there that moves or even opens.

00:23:46.755 --> 00:23:49.690
People have no problem talking to this and having a conversation.

00:23:49.690 --> 00:23:51.675
And what I've seen is again,

00:23:51.675 --> 00:23:54.785
many robots out there do have moving lips.

00:23:54.785 --> 00:23:56.330
None of them, kind of,

00:23:56.330 --> 00:23:58.355
crossed that threshold of not being creepy,

00:23:58.355 --> 00:23:59.664
you're just wrong, right?

00:23:59.664 --> 00:24:03.870
And it's - many of them are now good enough that it's not like,

00:24:03.870 --> 00:24:05.410
ew, you know, that's weird,

00:24:05.410 --> 00:24:07.889
but there is this disconnect. All right?

00:24:07.889 --> 00:24:09.410
So Uncanny comes from Freud's "Unheimlich," right?

00:24:09.410 --> 00:24:13.575
So this feeling that something is just not quite right.

00:24:13.575 --> 00:24:16.520
And through a lot of the studies that I did more than a decade ago,

00:24:16.520 --> 00:24:21.470
we found that many of the features that are not quite right like that,

00:24:21.470 --> 00:24:23.120
people aren't necessarily going to say oh,

00:24:23.120 --> 00:24:27.100
that's wrong and be able to point that out and say this is a problem.

00:24:27.100 --> 00:24:29.090
But if you actually look at

00:24:29.090 --> 00:24:32.720
their experience with this robot and you compare one versus another,

00:24:32.720 --> 00:24:36.195
then in that underlying data about the psychology of the interaction,

00:24:36.195 --> 00:24:38.150
you can see these very clear differences.

00:24:38.150 --> 00:24:40.700
So part of it's just learning these lessons about what things work,

00:24:40.700 --> 00:24:44.945
what things don't and then how do we apply those in the design of future robots.

00:24:44.945 --> 00:24:49.125
Can you talk a little bit about the reaction that you received from patients when they,

00:24:49.125 --> 00:24:52.479
you know, unbox their first Mabu and kind of work with it in their house?

00:24:52.479 --> 00:24:54.776
Well, I think the reaction starts -

00:24:54.776 --> 00:24:57.310
like the reactions that we care about start even before that.

00:24:57.310 --> 00:25:00.940
So one of the things that we think about is how is this introduced to a patient.

00:25:00.940 --> 00:25:03.569
It's not like suddenly this box shows up at the doorstep, right?

00:25:03.569 --> 00:25:06.699
And they go oh, my God, there's a robot in here, right?

00:25:06.699 --> 00:25:09.110
So the patient's going to be introduced to us in some way.

00:25:09.110 --> 00:25:11.990
Right now that's either through their pharmacy,

00:25:11.990 --> 00:25:14.090
if they're getting a certain prescription that this might come

00:25:14.090 --> 00:25:16.400
with or in the near future,

00:25:16.400 --> 00:25:17.730
patients who are going to be discharged from

00:25:17.730 --> 00:25:19.670
the hospital might be getting one of these as well.

00:25:19.670 --> 00:25:22.690
So a nurse is going to be telling them about it.

00:25:22.690 --> 00:25:24.584
What's that language that we use right then?

00:25:24.584 --> 00:25:27.485
How do we set up the expectations in the right way

00:25:27.485 --> 00:25:31.845
and set up the relationship that we want this patient to have?

00:25:31.845 --> 00:25:35.000
How they're going to benefit from it in a way that they believe that?

00:25:35.000 --> 00:25:37.130
They believe this is a tool that is going to be

00:25:37.130 --> 00:25:40.469
useful for them and something that they're going to benefit from.

00:25:40.469 --> 00:25:42.920
So the initial thing that

00:25:42.920 --> 00:25:45.525
we think about is what's the language in that first conversation.

00:25:45.525 --> 00:25:48.425
Who's going to be delivering it to them and what should they be saying?

00:25:48.425 --> 00:25:52.849
How do we talk about the benefits to a patient before they ever even see this thing?

00:25:52.849 --> 00:25:55.280
So, you know, our patients get this box at home and

00:25:55.280 --> 00:25:58.170
it's probably the first time they've received a robot in the mail.

00:25:58.170 --> 00:26:00.355
And so maybe at first there's a little bit of confusion,

00:26:00.355 --> 00:26:02.205
so we try to make things as clear as possible, right?

00:26:02.205 --> 00:26:03.515
When they first open up the boxes,

00:26:03.515 --> 00:26:07.850
a little one page instruction sheet and it gives the whole long list of instructions,

00:26:07.850 --> 00:26:11.644
which is take it out of the box and plug it in. That's it.

00:26:11.644 --> 00:26:14.150
Right? So really simple and having a focus on making this

00:26:14.150 --> 00:26:16.730
really easy for someone to use is important,

00:26:16.730 --> 00:26:18.665
given who we're shipping these to.

00:26:18.665 --> 00:26:20.540
Some of our patients are going to be very tech

00:26:20.540 --> 00:26:22.485
savvy and they've got a couple of computers and,

00:26:22.485 --> 00:26:25.260
you know, their smartwatch and a cell phone or two.

00:26:25.260 --> 00:26:27.900
Some of them we know have never owned a computer. Right?

00:26:27.900 --> 00:26:32.735
So making it so that any one across this whole spectrum can use this is important.

00:26:32.735 --> 00:26:34.925
And they plug it in and, you know,

00:26:34.925 --> 00:26:37.400
of course it's essentially a computer or

00:26:37.400 --> 00:26:40.160
a phone kind of thing in here so it's going to take a minute to boot up.

00:26:40.160 --> 00:26:42.470
But as soon as it does, it looks at them, right?

00:26:42.470 --> 00:26:44.900
Oh, hey, good to see you. Thanks for taking me out of that box.

00:26:44.900 --> 00:26:47.200
Getting cramped in there.

00:26:47.200 --> 00:26:50.545
And so we try to make a little bit humorous at first and of course,

00:26:50.545 --> 00:26:53.050
we're reading the emotion on that person's face;

00:26:53.050 --> 00:26:54.084
did they like this humor,

00:26:54.084 --> 00:26:57.162
did they appreciate it or that's the dumbest thing I've ever heard, right?

00:26:57.162 --> 00:27:00.162
So we want to know that so we can start adapting to them right away.

00:27:00.162 --> 00:27:01.855
And so it's important for us to, you know,

00:27:01.855 --> 00:27:07.664
create some delight in that person right away so that they like interacting with us.

00:27:07.664 --> 00:27:09.640
And we want to bring a smile to their face.

00:27:09.640 --> 00:27:13.125
And from there we go into more of the here's why I'm here.

00:27:13.125 --> 00:27:17.560
I'm here to help you do with this condition that you're dealing with or,

00:27:17.560 --> 00:27:19.275
you know, with this drug that you're taking.

00:27:19.275 --> 00:27:20.890
And so making sure that, you know,

00:27:20.890 --> 00:27:22.690
in those first few seconds and in

00:27:22.690 --> 00:27:26.200
the first few minutes that we're really communicating to our patients,

00:27:26.200 --> 00:27:29.365
you know, why this is something of value to them.

00:27:29.365 --> 00:27:30.915
And by focusing on these things,

00:27:30.915 --> 00:27:32.810
so making sure that this is something that's

00:27:32.810 --> 00:27:36.010
a delightful experience to start and showing, you know,

00:27:36.010 --> 00:27:38.764
me as a patient why this has value to me,

00:27:38.764 --> 00:27:44.880
we get a very positive response from almost everyone that we've put these in front of.

00:27:44.880 --> 00:27:49.900
So one of our challenges over running various trials over the last decade,

00:27:49.900 --> 00:27:53.170
from the first hand-built version of this that I put in front of patients back in

00:27:53.170 --> 00:27:57.511
2007 to the early versions of Mabu that we've been putting in front of patients now,

00:27:57.511 --> 00:28:01.690
the challenge has been we give these to patients for - maybe it's half an hour,

00:28:01.690 --> 00:28:03.775
maybe it's a few days, maybe it's a few months;

00:28:03.775 --> 00:28:05.800
at the end of it, they won't give them back to us.

00:28:05.800 --> 00:28:07.975
So I think it's a great challenge to have,

00:28:07.975 --> 00:28:12.499
but it shows you what kind of reception these do have with patients who are using them.

00:28:12.499 --> 00:28:15.625
And I think that's a lot because you're focusing on the underlying problem.

00:28:15.625 --> 00:28:17.260
So you're really focused on

00:28:17.260 --> 00:28:19.750
the person at the end of the day and not the technology itself.

00:28:19.750 --> 00:28:21.505
I know we kind of talked about this a little bit earlier,

00:28:21.505 --> 00:28:26.550
but it's really about how do we help people and not how do we build a cool robot.

00:28:26.550 --> 00:28:29.920
Right? Look at all the bells and whistles that this has versus look at what I

00:28:29.920 --> 00:28:33.445
can actually do for somebody based on what I've helped put together.

00:28:33.445 --> 00:28:38.020
Right. So so we think about robotics as an enabling technology, not as an industry.

00:28:38.020 --> 00:28:40.660
We don't think of this as a robotics company.

00:28:40.660 --> 00:28:42.010
Right. The name of the company is not,

00:28:42.010 --> 00:28:43.930
you know, Catalia Robotics, but Catalia Health.

00:28:43.930 --> 00:28:45.505
We're a healthcare company.

00:28:45.505 --> 00:28:47.600
You know, roughly half,

00:28:47.600 --> 00:28:51.220
maybe a little under half of our employees have a technical background.

00:28:51.220 --> 00:28:56.320
Right? We've also got nurses and psychologists and doctors and writers and overall,

00:28:56.320 --> 00:28:57.935
we're solving a healthcare problem.

00:28:57.935 --> 00:29:00.365
The technology just happens to help us do.

00:29:00.365 --> 00:29:02.950
How do you ensure that your robot is

00:29:02.950 --> 00:29:06.160
performing more or less ethically for the space that you're in?

00:29:06.160 --> 00:29:09.260
So I know there's issues when you're dealing with healthcare,

00:29:09.260 --> 00:29:11.210
when you're tracking people's health.

00:29:11.210 --> 00:29:14.132
So I think there are two things that we think about here.

00:29:14.132 --> 00:29:15.141
One is, you know,

00:29:15.141 --> 00:29:17.260
we're building a new application

00:29:17.260 --> 00:29:21.125
for a group of patients and we think of it as - an application,

00:29:21.125 --> 00:29:23.290
for us, is really the intersection of, you know,

00:29:23.290 --> 00:29:26.285
some disease state, some condition and some treatment.

00:29:26.285 --> 00:29:28.690
So we're helping patients with a certain type of

00:29:28.690 --> 00:29:31.320
cancer take this oral drug every day, for example.

00:29:31.320 --> 00:29:35.504
Right? What do we want to be doing to benefit those patients?

00:29:35.504 --> 00:29:39.400
And so part of that for us is let's look into the challenges for patients today.

00:29:39.400 --> 00:29:42.749
What are the challenges that they have successfully staying on therapy?

00:29:42.749 --> 00:29:44.974
How do we go about solving those?

00:29:44.974 --> 00:29:46.685
And again, the technology, then,

00:29:46.685 --> 00:29:49.015
is a piece that lets us do this not for one patient,

00:29:49.015 --> 00:29:51.100
but for thousands, tens of thousands,

00:29:51.100 --> 00:29:53.004
hundreds of thousands of patients.

00:29:53.004 --> 00:29:55.360
And so that's really kind of a guiding principle here.

00:29:55.360 --> 00:30:00.370
What are we doing to help this patient population better manage their own therapy and

00:30:00.370 --> 00:30:02.562
get information back to their care team so that

00:30:02.562 --> 00:30:06.025
those people can help manage that group of patients.

00:30:06.025 --> 00:30:08.620
So that's that's one set of things and that's really the thing that

00:30:08.620 --> 00:30:11.800
drives how we design everything that we do.

00:30:11.800 --> 00:30:13.870
The other then is on the regulatory side, right?

00:30:13.870 --> 00:30:16.665
If we look at where we fit in terms of the healthcare industry,

00:30:16.665 --> 00:30:18.520
there are certain things that we can and can't

00:30:18.520 --> 00:30:21.020
do in terms of what type of a device this is.

00:30:21.020 --> 00:30:22.585
And the good thing about that is that's, you know,

00:30:22.585 --> 00:30:24.510
a very well-developed field, right?

00:30:24.510 --> 00:30:28.030
You have this big entity called FDA and there's a lot of

00:30:28.030 --> 00:30:32.650
guidance from FDA that is not necessarily about robotics.

00:30:32.650 --> 00:30:35.225
I can actually guarantee you that none of it is about robotics,

00:30:35.225 --> 00:30:39.190
but it's still very applicable in terms of what we're actually doing with our patients,

00:30:39.190 --> 00:30:42.660
the challenges and the problems that we're trying to solve for them.

00:30:42.660 --> 00:30:44.550
All of this guidance and, you know,

00:30:44.550 --> 00:30:47.050
the laws and the rules are very applicable to what we're doing,

00:30:47.050 --> 00:30:50.560
so we can use that also as a reference for the types of things

00:30:50.560 --> 00:30:54.970
we can and should and cannot and should not do with patients.

00:30:54.970 --> 00:30:56.590
So if you were designing the robot

00:30:56.590 --> 00:30:58.720
for it to work with the human outside of - let's say Mabu,

00:30:58.720 --> 00:31:00.970
for instance - and just in general,

00:31:00.970 --> 00:31:03.470
if you were somebody out there who was interested in building

00:31:03.470 --> 00:31:06.189
the robot to kind of work alongside a human,

00:31:06.189 --> 00:31:10.315
what would you suggest that that person do?

00:31:10.315 --> 00:31:12.370
Well, I think that, you know, just like we've been talking

00:31:12.370 --> 00:31:14.230
about in terms of the specifics of, you know,

00:31:14.230 --> 00:31:15.645
Mabu and what we're doing, I can tell you,

00:31:15.645 --> 00:31:18.400
the first thing is understanding what's the problem you're trying to solve.

00:31:18.400 --> 00:31:20.319
Right? So the example I gave is okay,

00:31:20.319 --> 00:31:23.740
we look at this patient population and understand what are the challenges today.

00:31:23.740 --> 00:31:25.684
Same thing. That applies anywhere.

00:31:25.684 --> 00:31:27.190
Any problem you're trying to solve,

00:31:27.190 --> 00:31:29.810
understand what are the real challenges today.

00:31:29.810 --> 00:31:33.725
So in other words, what that means is not start with the robot.

00:31:33.725 --> 00:31:38.020
Okay? Here are the features that we want to build into this piece of technology,

00:31:38.020 --> 00:31:42.445
this device, but let's look at the problem space that we're working in.

00:31:42.445 --> 00:31:43.666
Let's understand, you know,

00:31:43.666 --> 00:31:45.040
what we're doing or not doing,

00:31:45.040 --> 00:31:47.920
what are the challenges that people actually have and try

00:31:47.920 --> 00:31:51.250
to set up that question in such a way that,

00:31:51.250 --> 00:31:52.879
you know, we get real answers, right?

00:31:52.879 --> 00:31:57.050
In other words, we're not trying to project what we think we can or can't do onto that,

00:31:57.050 --> 00:31:59.550
but actually try to get some answers from people about what's

00:31:59.550 --> 00:32:03.980
happening and what are the real problems that need to be solved.

00:32:03.980 --> 00:32:05.780
So I have a question for you, Cory.

00:32:05.780 --> 00:32:08.475
So typically when I'm getting ready to build anything,

00:32:08.475 --> 00:32:10.335
I end up doing research.

00:32:10.335 --> 00:32:13.040
And my research always leads me down this rabbit hole,

00:32:13.040 --> 00:32:14.670
where all of a sudden, I'm like well,

00:32:14.670 --> 00:32:17.479
I just need to build something really quick to do X.

00:32:17.479 --> 00:32:21.110
Right? Let's say for instance I want to build a robot, like a robot head.

00:32:21.110 --> 00:32:23.130
Okay. Well, I just want to build a robot head really quick.

00:32:23.130 --> 00:32:24.630
So I bet you I can do this with, like,

00:32:24.630 --> 00:32:28.594
cardboard and maybe some toilet paper tubes and I'm probably okay.

00:32:28.594 --> 00:32:33.615
But then I start doing some research and I start looking around and I say well,

00:32:33.615 --> 00:32:38.280
actually, you know, Catalia Health has a great robot.

00:32:38.280 --> 00:32:41.250
It's nice, it's made of plastic and it's got nice eyes that move and everything.

00:32:41.250 --> 00:32:43.135
Maybe I should do something in plastic.

00:32:43.135 --> 00:32:46.034
So now I start looking at 3D printers and then I say well,

00:32:46.034 --> 00:32:47.520
you know, plastic is cheap,

00:32:47.520 --> 00:32:50.545
maybe I can move over toward something that's a little bit more durable,

00:32:50.545 --> 00:32:52.165
long term, like some metals.

00:32:52.165 --> 00:32:54.685
Now I'm looking at learning how to use a CNC machine - and it just

00:32:54.685 --> 00:32:58.860
kind of spirals forever and I just never get going.

00:32:58.860 --> 00:33:01.374
What is your suggestion on how to just get started?

00:33:01.374 --> 00:33:03.150
So you know, learning all those tools and, you know,

00:33:03.150 --> 00:33:05.990
how to work with all those materials - great skill set to have.

00:33:05.990 --> 00:33:09.530
Right? But what you've got to focus on is the problem that you're trying to solve.

00:33:09.530 --> 00:33:13.105
So it's kind of a theme of this conversation that we've had here.

00:33:13.105 --> 00:33:14.960
What is the real problem I want to solve?

00:33:14.960 --> 00:33:16.470
Is it figuring out how big this thing should

00:33:16.470 --> 00:33:18.587
be and cut a bunch of stuff out of paper, right?

00:33:18.587 --> 00:33:20.765
One small, medium, large.

00:33:20.765 --> 00:33:23.175
Is it deciding what degrees of freedom this has?

00:33:23.175 --> 00:33:25.500
Toilet paper tubes and,

00:33:25.500 --> 00:33:27.480
you know, other cardboard stuff.

00:33:27.480 --> 00:33:29.190
Great way to start mocking that up.

00:33:29.190 --> 00:33:30.840
Right? Next iteration, you know,

00:33:30.840 --> 00:33:32.600
you add some servos to the cardboard.

00:33:32.600 --> 00:33:34.305
I've built robots that way,

00:33:34.305 --> 00:33:37.410
literally hobby servos and cardboard put

00:33:37.410 --> 00:33:40.980
together and even some of the early studies that I've talked about,

00:33:40.980 --> 00:33:42.600
in terms of the psychology of the interaction,

00:33:42.600 --> 00:33:47.730
this is how those robots were built because solving those problems don't require a,

00:33:47.730 --> 00:33:53.520
you know, commercially polished robot that I'm going to sell to a million people.

00:33:53.520 --> 00:33:58.100
I think one of the challenges is focusing in on what's the real problem.

00:33:58.100 --> 00:34:01.050
Now, we've talked about this today in terms of the context of, you know,

00:34:01.050 --> 00:34:04.575
business and prototyping and a number of other aspects and I think this is

00:34:04.575 --> 00:34:08.694
something that's really easy to get distracted from, right?

00:34:08.694 --> 00:34:11.275
When you start talking about technology, it's always easy.

00:34:11.275 --> 00:34:12.415
Okay. Let's do this one extra thing.

00:34:12.415 --> 00:34:15.320
Or oh, wouldn't this be great to pull in here. Wait. Look at that.

00:34:15.320 --> 00:34:16.780
Right? Like you're saying.

00:34:16.780 --> 00:34:18.770
And put that in there.

00:34:18.770 --> 00:34:20.820
The challenge for anyone building this stuff

00:34:20.820 --> 00:34:23.965
is how do you make sure you maintain that focus.

00:34:23.965 --> 00:34:26.010
So let's say now I have a robot that I've

00:34:26.010 --> 00:34:28.695
built and I've finally gotten over this hurdle of,

00:34:28.695 --> 00:34:30.795
you know, using cardboard,

00:34:30.795 --> 00:34:33.930
toilet paper rolls or something like that and I

00:34:33.930 --> 00:34:37.039
want to actually test my prototype in front of somebody,

00:34:37.039 --> 00:34:38.340
but I'm a little reluctant.

00:34:38.340 --> 00:34:40.530
I don't feel confident with my design.

00:34:40.530 --> 00:34:42.720
What's the best way for me to kind of, you know,

00:34:42.720 --> 00:34:47.285
overcome that and how would I go about setting up that kind of an environment?

00:34:47.285 --> 00:34:49.590
So - you know, I think there's a few things to think about.

00:34:49.590 --> 00:34:50.700
One is do it, you know,

00:34:50.700 --> 00:34:52.350
as early and as often as possible.

00:34:52.350 --> 00:34:56.940
Right? So having whatever you're building in front of people is really valuable.

00:34:56.940 --> 00:34:58.200
And I think one thing you find what you're

00:34:58.200 --> 00:35:00.060
doing that is it's actually a lot easier than you

00:35:00.060 --> 00:35:03.520
think in terms of getting people to interact with stuff.

00:35:03.520 --> 00:35:05.470
Now, it depends a lot of what you're trying to build, all right?

00:35:05.470 --> 00:35:07.055
If you're building stuff like we are in healthcare,

00:35:07.055 --> 00:35:10.220
then I'm looking for a certain patient population to do it.

00:35:10.220 --> 00:35:12.420
If I'm building something for entertainment or

00:35:12.420 --> 00:35:15.610
a toy or that a lot of people would use, you know,

00:35:15.610 --> 00:35:16.900
go to your family or friends,

00:35:16.900 --> 00:35:18.090
whomever you can, you know,

00:35:18.090 --> 00:35:22.450
kind of get for five minutes to interact with this as early as possible, right?

00:35:22.450 --> 00:35:26.670
From the mock-up stage to every iteration of your prototype,

00:35:26.670 --> 00:35:28.088
put this in front of people.

00:35:28.088 --> 00:35:31.734
And understand what it is you're trying to get feedback on.

00:35:31.734 --> 00:35:33.930
Like if you want feedback on

00:35:33.930 --> 00:35:36.899
this as a business and how you're going to sell a lot of these,

00:35:36.899 --> 00:35:38.310
that's one type of question.

00:35:38.310 --> 00:35:41.970
If it's okay, I have this thing that moves in this certain way,

00:35:41.970 --> 00:35:43.110
how do people respond to that?

00:35:43.110 --> 00:35:44.825
That's a very different type of question.

00:35:44.825 --> 00:35:47.460
So one thing I see a lot - so I've taught courses in

00:35:47.460 --> 00:35:51.265
human robot interaction at a few different schools is, you know,

00:35:51.265 --> 00:35:57.650
students who are starting off on this early on are going to test this thing,

00:35:57.650 --> 00:36:00.833
but without figuring out what the questions are.

00:36:00.833 --> 00:36:04.199
If you think about kind of basics of science,

00:36:04.199 --> 00:36:05.280
same thing applies here, right?

00:36:05.280 --> 00:36:06.590
What's the question you're trying to ask?

00:36:06.590 --> 00:36:10.059
What's the hypothesis? Set this up like a study, right?

00:36:10.059 --> 00:36:15.175
Like an experiment. I want to learn this about this thing that I built.

00:36:15.175 --> 00:36:18.000
Okay. That means I need to ask this question or

00:36:18.000 --> 00:36:22.350
this question or I want to put this in front of a person and have it do this thing.

00:36:22.350 --> 00:36:24.940
So be very clear on what you're trying to do.

00:36:24.940 --> 00:36:28.470
And again, don't worry about the fidelity of your prototype.

00:36:28.470 --> 00:36:31.139
What's the worst thing that can happen by putting it out too early?

00:36:31.139 --> 00:36:32.828
Maybe you don't learn enough and you figure out okay,

00:36:32.828 --> 00:36:35.190
here's the thing I need to refine or you can

00:36:35.190 --> 00:36:37.680
go and you can spend months building this prototype that you think is

00:36:37.680 --> 00:36:41.580
really elegant and you finally put it in front of people and you realize that one of

00:36:41.580 --> 00:36:45.888
your base assumptions was wrong and then you go back and start all over.

00:36:45.888 --> 00:36:48.180
And so - and that's why I always stress,

00:36:48.180 --> 00:36:50.660
like, getting this out as early as possible.

00:36:50.660 --> 00:36:52.575
When I've had students building things,

00:36:52.575 --> 00:36:54.975
you know, in a semester-long class,

00:36:54.975 --> 00:37:00.920
often the most advanced technology I will let them use is a Lego Mindstorms.

00:37:00.920 --> 00:37:03.670
Right? Everything has to be simpler than that.

00:37:03.670 --> 00:37:05.870
You should do the quickest prototype because

00:37:05.870 --> 00:37:09.080
the feedback is more important in terms of what's the problem you're trying to

00:37:09.080 --> 00:37:12.980
solve and how are people going to respond to this general concept before you even worry

00:37:12.980 --> 00:37:17.205
about refining the actual hardware or software that goes into your robot.

00:37:17.205 --> 00:37:18.784
All that stuff is very time-consuming,

00:37:18.784 --> 00:37:20.750
but it's a whole lot easier when you know exactly

00:37:20.750 --> 00:37:23.170
what you want to build and how people are going to respond to it.

00:37:23.170 --> 00:37:25.970
So what are some of the major misconceptions that

00:37:25.970 --> 00:37:28.865
you find when dealing with human/robot interaction

00:37:28.865 --> 00:37:31.700
where a person tends to

00:37:31.700 --> 00:37:35.519
believe something is going to occur with the robot, but really doesn't?

00:37:35.519 --> 00:37:38.210
So I think the biggest thing is like at a cultural level,

00:37:38.210 --> 00:37:40.300
there's, like, this concept that robots are scary, right?

00:37:40.300 --> 00:37:41.667
People are not going to use this.

00:37:41.667 --> 00:37:45.470
The reality is when we put these in front of people, they love it.

00:37:45.470 --> 00:37:47.410
This thing is not scary.

00:37:47.410 --> 00:37:49.910
You know, we're not building some big sci-fi robot,

00:37:49.910 --> 00:37:51.795
you know, with lasers in its eyes.

00:37:51.795 --> 00:37:53.229
It was a cute little thing about this big that says oh, hey,

00:37:53.229 --> 00:37:56.670
how's it going and just has a simple conversation.

00:37:56.670 --> 00:37:58.015
And so while again,

00:37:58.015 --> 00:38:01.100
there might be this big conception that robots are scary things,

00:38:01.100 --> 00:38:03.915
or technology that talks you about your healthcare are not good,

00:38:03.915 --> 00:38:05.450
the on the ground reality,

00:38:05.450 --> 00:38:07.640
when we put this in front of patients, they love it.

00:38:07.640 --> 00:38:09.190
I think that's the biggest lesson.

00:38:09.190 --> 00:38:10.670
And part of that just comes from

00:38:10.670 --> 00:38:14.495
iteratively building and testing this with a lot of people over many years.

00:38:14.495 --> 00:38:19.150
So obviously, being CEO of your own company has its own advantages.

00:38:19.150 --> 00:38:20.250
Yes.

00:38:20.250 --> 00:38:23.779
What's your favorite thing that you like to do for your job?

00:38:23.779 --> 00:38:25.970
You know, it varies a lot in terms of,

00:38:25.970 --> 00:38:27.765
you know, what I'm doing on a particular day, right?

00:38:27.765 --> 00:38:29.420
As CEO of a small company,

00:38:29.420 --> 00:38:32.280
I do a lot of different jobs and roles.

00:38:32.280 --> 00:38:35.795
But for me, a lot of the really fun stuff comes back to the product that we're building.

00:38:35.795 --> 00:38:38.600
So we can build new features,

00:38:38.600 --> 00:38:39.875
not for the sake of technology,

00:38:39.875 --> 00:38:41.915
but for the sake of, you know, benefiting a patient

00:38:41.915 --> 00:38:44.780
and actually get those in front of people and see how they respond to it.

00:38:44.780 --> 00:38:46.755
So when I see someone who, you know,

00:38:46.755 --> 00:38:49.350
talks about this thing that we've built in the way that - you know,

00:38:49.350 --> 00:38:50.540
we've had patients say oh,

00:38:50.540 --> 00:38:52.030
I can't live without this, right?

00:38:52.030 --> 00:38:55.100
That's exciting. They see value in this for them and

00:38:55.100 --> 00:38:58.200
for their health and I love seeing that.

00:38:58.200 --> 00:39:01.439
For students who are interested in investigating HRI,

00:39:01.439 --> 00:39:05.790
what are some of the cutting edge areas that you see, kind of, developing right now?

00:39:05.790 --> 00:39:07.700
So you know, I think the interesting thing that's

00:39:07.700 --> 00:39:09.470
happening with robotics right now is a lot of

00:39:09.470 --> 00:39:13.385
the core technologies have matured to the point where we can build real stuff.

00:39:13.385 --> 00:39:17.337
And so what that means now is we don't have to think about this as a robotics industry,

00:39:17.337 --> 00:39:20.285
but as using robotics to solve a lot of other problems.

00:39:20.285 --> 00:39:22.580
And so I think many students who are interested in

00:39:22.580 --> 00:39:25.353
robotics have other areas of interest as well. All right?

00:39:25.353 --> 00:39:27.440
So if you think about Catalia Health at a really high level,

00:39:27.440 --> 00:39:28.760
we're combining this technology,

00:39:28.760 --> 00:39:34.080
robotics and AI, with the healthcare domain and a specific set of problems within there.

00:39:34.080 --> 00:39:36.185
And I think we're starting to see a lot of those intersections.

00:39:36.185 --> 00:39:40.280
And the very interesting companies that we're seeing come out that

00:39:40.280 --> 00:39:44.750
leverage robotics as a technology are the ones not trying to build a robot platform,

00:39:44.750 --> 00:39:47.445
but the ones who are trying to solve particular problems.

00:39:47.445 --> 00:39:49.320
And so in some senses,

00:39:49.320 --> 00:39:50.690
it's a very vague answer, right?

00:39:50.690 --> 00:39:53.010
I'm not saying, you know, this area or that area,

00:39:53.010 --> 00:39:54.950
but in some senses it's very concrete in

00:39:54.950 --> 00:39:57.680
that the interesting stuff that we're going to see over the next few years and

00:39:57.680 --> 00:40:00.560
the successful robotics companies are the ones who are solving

00:40:00.560 --> 00:40:04.245
very domain-specific problems leveraging robotics technology.

00:40:04.245 --> 00:40:06.915
And we're starting to see that pop up in a lot of different areas.

00:40:06.915 --> 00:40:08.330
My interest for a long time has been in

00:40:08.330 --> 00:40:10.645
healthcare so I'm most familiar with what's going on there,

00:40:10.645 --> 00:40:12.140
but we're seeing this across healthcare.

00:40:12.140 --> 00:40:14.235
There's a ton of these companies in education.

00:40:14.235 --> 00:40:16.730
There is a lot in, like, entertainment and play and, you know,

00:40:16.730 --> 00:40:20.444
with kids and - so many different applications of robotics,

00:40:20.444 --> 00:40:23.740
but it's where those companies and those people are really focused on solving a -

