WEBVTT
Kind: captions
Language: en

00:00:00.089 --> 00:00:01.459
in the early days of deep learning

00:00:01.459 --> 00:00:01.469
in the early days of deep learning
 

00:00:01.469 --> 00:00:03.409
in the early days of deep learning
people used to worry a lot about the

00:00:03.409 --> 00:00:03.419
people used to worry a lot about the
 

00:00:03.419 --> 00:00:05.660
people used to worry a lot about the
optimization algorithm getting stuck in

00:00:05.660 --> 00:00:05.670
optimization algorithm getting stuck in
 

00:00:05.670 --> 00:00:08.419
optimization algorithm getting stuck in
bad local optima but as the theory of

00:00:08.419 --> 00:00:08.429
bad local optima but as the theory of
 

00:00:08.429 --> 00:00:09.740
bad local optima but as the theory of
deep learning has advanced our

00:00:09.740 --> 00:00:09.750
deep learning has advanced our
 

00:00:09.750 --> 00:00:12.350
deep learning has advanced our
understanding of local optima is also

00:00:12.350 --> 00:00:12.360
understanding of local optima is also
 

00:00:12.360 --> 00:00:15.350
understanding of local optima is also
changing let me show you how we now

00:00:15.350 --> 00:00:15.360
changing let me show you how we now
 

00:00:15.360 --> 00:00:18.170
changing let me show you how we now
think about local optima and problems in

00:00:18.170 --> 00:00:18.180
think about local optima and problems in
 

00:00:18.180 --> 00:00:19.700
think about local optima and problems in
the optimization problem in deep

00:00:19.700 --> 00:00:19.710
the optimization problem in deep
 

00:00:19.710 --> 00:00:22.730
the optimization problem in deep
learning so this was a picture people

00:00:22.730 --> 00:00:22.740
learning so this was a picture people
 

00:00:22.740 --> 00:00:24.290
learning so this was a picture people
used to have in mind when they worried

00:00:24.290 --> 00:00:24.300
used to have in mind when they worried
 

00:00:24.300 --> 00:00:26.630
used to have in mind when they worried
about local optima maybe you're trying

00:00:26.630 --> 00:00:26.640
about local optima maybe you're trying
 

00:00:26.640 --> 00:00:28.580
about local optima maybe you're trying
to optimize some set of parameters and

00:00:28.580 --> 00:00:28.590
to optimize some set of parameters and
 

00:00:28.590 --> 00:00:31.460
to optimize some set of parameters and
we call them W 1 and W 2 and the height

00:00:31.460 --> 00:00:31.470
we call them W 1 and W 2 and the height
 

00:00:31.470 --> 00:00:33.770
we call them W 1 and W 2 and the height
of the surface is the cost function so

00:00:33.770 --> 00:00:33.780
of the surface is the cost function so
 

00:00:33.780 --> 00:00:35.330
of the surface is the cost function so
in this picture it looks like there are

00:00:35.330 --> 00:00:35.340
in this picture it looks like there are
 

00:00:35.340 --> 00:00:37.790
in this picture it looks like there are
a lot of local optima you know in in all

00:00:37.790 --> 00:00:37.800
a lot of local optima you know in in all
 

00:00:37.800 --> 00:00:39.770
a lot of local optima you know in in all
those places and it'd be easy for

00:00:39.770 --> 00:00:39.780
those places and it'd be easy for
 

00:00:39.780 --> 00:00:41.810
those places and it'd be easy for
gradient descents or one of the other

00:00:41.810 --> 00:00:41.820
gradient descents or one of the other
 

00:00:41.820 --> 00:00:43.850
gradient descents or one of the other
algorithms to get stuck on a local

00:00:43.850 --> 00:00:43.860
algorithms to get stuck on a local
 

00:00:43.860 --> 00:00:45.680
algorithms to get stuck on a local
optimum rather than find this way to a

00:00:45.680 --> 00:00:45.690
optimum rather than find this way to a
 

00:00:45.690 --> 00:00:48.740
optimum rather than find this way to a
global optimum it turns out that if you

00:00:48.740 --> 00:00:48.750
global optimum it turns out that if you
 

00:00:48.750 --> 00:00:50.959
global optimum it turns out that if you
are plotting a figure like this in two

00:00:50.959 --> 00:00:50.969
are plotting a figure like this in two
 

00:00:50.969 --> 00:00:53.540
are plotting a figure like this in two
dimensions then it's easy to create

00:00:53.540 --> 00:00:53.550
dimensions then it's easy to create
 

00:00:53.550 --> 00:00:55.160
dimensions then it's easy to create
plots like this of a lot of different

00:00:55.160 --> 00:00:55.170
plots like this of a lot of different
 

00:00:55.170 --> 00:00:57.799
plots like this of a lot of different
local optima and these very low

00:00:57.799 --> 00:00:57.809
local optima and these very low
 

00:00:57.809 --> 00:00:59.209
local optima and these very low
dimensional plots used to gather

00:00:59.209 --> 00:00:59.219
dimensional plots used to gather
 

00:00:59.219 --> 00:01:01.670
dimensional plots used to gather
intuition but this intuition isn't

00:01:01.670 --> 00:01:01.680
intuition but this intuition isn't
 

00:01:01.680 --> 00:01:03.319
intuition but this intuition isn't
actually correct it turns out if you

00:01:03.319 --> 00:01:03.329
actually correct it turns out if you
 

00:01:03.329 --> 00:01:06.380
actually correct it turns out if you
create in your network most points of 0

00:01:06.380 --> 00:01:06.390
create in your network most points of 0
 

00:01:06.390 --> 00:01:08.899
create in your network most points of 0
gradients are not local optima like

00:01:08.899 --> 00:01:08.909
gradients are not local optima like
 

00:01:08.909 --> 00:01:11.570
gradients are not local optima like
points like this instead most points of

00:01:11.570 --> 00:01:11.580
points like this instead most points of
 

00:01:11.580 --> 00:01:13.910
points like this instead most points of
0 gradients in the cost function are

00:01:13.910 --> 00:01:13.920
0 gradients in the cost function are
 

00:01:13.920 --> 00:01:16.160
0 gradients in the cost function are
actually saddle points so that's a point

00:01:16.160 --> 00:01:16.170
actually saddle points so that's a point
 

00:01:16.170 --> 00:01:18.560
actually saddle points so that's a point
with a zero gradient again this is maybe

00:01:18.560 --> 00:01:18.570
with a zero gradient again this is maybe
 

00:01:18.570 --> 00:01:23.060
with a zero gradient again this is maybe
W 1 W 2 and the highest heightens the

00:01:23.060 --> 00:01:23.070
W 1 W 2 and the highest heightens the
 

00:01:23.070 --> 00:01:25.670
W 1 W 2 and the highest heightens the
value of the cost function J but

00:01:25.670 --> 00:01:25.680
value of the cost function J but
 

00:01:25.680 --> 00:01:27.289
value of the cost function J but
informally a function in a very high

00:01:27.289 --> 00:01:27.299
informally a function in a very high
 

00:01:27.299 --> 00:01:29.690
informally a function in a very high
dimensional space if the gradient is 0

00:01:29.690 --> 00:01:29.700
dimensional space if the gradient is 0
 

00:01:29.700 --> 00:01:32.240
dimensional space if the gradient is 0
then in each direction it can either be

00:01:32.240 --> 00:01:32.250
then in each direction it can either be
 

00:01:32.250 --> 00:01:35.149
then in each direction it can either be
a convex light function or a concave

00:01:35.149 --> 00:01:35.159
a convex light function or a concave
 

00:01:35.159 --> 00:01:38.749
a convex light function or a concave
light function and if you are in say a

00:01:38.749 --> 00:01:38.759
light function and if you are in say a
 

00:01:38.759 --> 00:01:41.569
light function and if you are in say a
20,000 dimensional space then thread to

00:01:41.569 --> 00:01:41.579
20,000 dimensional space then thread to
 

00:01:41.579 --> 00:01:44.030
20,000 dimensional space then thread to
be a local optima all 20,000 directions

00:01:44.030 --> 00:01:44.040
be a local optima all 20,000 directions
 

00:01:44.040 --> 00:01:46.639
be a local optima all 20,000 directions
need to look like this and so the chance

00:01:46.639 --> 00:01:46.649
need to look like this and so the chance
 

00:01:46.649 --> 00:01:49.039
need to look like this and so the chance
of that happening is maybe very small

00:01:49.039 --> 00:01:49.049
of that happening is maybe very small
 

00:01:49.049 --> 00:01:51.800
of that happening is maybe very small
you know maybe 2 to the minus 20000

00:01:51.800 --> 00:01:51.810
you know maybe 2 to the minus 20000
 

00:01:51.810 --> 00:01:53.359
you know maybe 2 to the minus 20000
instead you're much more likely to get

00:01:53.359 --> 00:01:53.369
instead you're much more likely to get
 

00:01:53.369 --> 00:01:56.120
instead you're much more likely to get
some directions where the curve bends up

00:01:56.120 --> 00:01:56.130
some directions where the curve bends up
 

00:01:56.130 --> 00:01:59.810
some directions where the curve bends up
like so as well some directions where

00:01:59.810 --> 00:01:59.820
like so as well some directions where
 

00:01:59.820 --> 00:02:01.999
like so as well some directions where
the function is bending down rather than

00:02:01.999 --> 00:02:02.009
the function is bending down rather than
 

00:02:02.009 --> 00:02:05.359
the function is bending down rather than
have them all Bend upwards so that's why

00:02:05.359 --> 00:02:05.369
have them all Bend upwards so that's why
 

00:02:05.369 --> 00:02:07.429
have them all Bend upwards so that's why
in very high dimensional spaces you're

00:02:07.429 --> 00:02:07.439
in very high dimensional spaces you're
 

00:02:07.439 --> 00:02:09.109
in very high dimensional spaces you're
actually much more likely to run into a

00:02:09.109 --> 00:02:09.119
actually much more likely to run into a
 

00:02:09.119 --> 00:02:10.760
actually much more likely to run into a
saddle points like that shown on the

00:02:10.760 --> 00:02:10.770
saddle points like that shown on the
 

00:02:10.770 --> 00:02:11.740
saddle points like that shown on the
right then

00:02:11.740 --> 00:02:11.750
right then
 

00:02:11.750 --> 00:02:14.470
right then
local optimum oh and as for why the

00:02:14.470 --> 00:02:14.480
local optimum oh and as for why the
 

00:02:14.480 --> 00:02:16.840
local optimum oh and as for why the
surface is called a saddle point if you

00:02:16.840 --> 00:02:16.850
surface is called a saddle point if you
 

00:02:16.850 --> 00:02:19.000
surface is called a saddle point if you
can picture maybe this is a sort of

00:02:19.000 --> 00:02:19.010
can picture maybe this is a sort of
 

00:02:19.010 --> 00:02:21.880
can picture maybe this is a sort of
shadow you put on a horse right so maybe

00:02:21.880 --> 00:02:21.890
shadow you put on a horse right so maybe
 

00:02:21.890 --> 00:02:23.680
shadow you put on a horse right so maybe
if this is a horse I guess there's a

00:02:23.680 --> 00:02:23.690
if this is a horse I guess there's a
 

00:02:23.690 --> 00:02:25.360
if this is a horse I guess there's a
head of a horse as you I have a horse

00:02:25.360 --> 00:02:25.370
head of a horse as you I have a horse
 

00:02:25.370 --> 00:02:31.180
head of a horse as you I have a horse
you know I guess and right well another

00:02:31.180 --> 00:02:31.190
you know I guess and right well another
 

00:02:31.190 --> 00:02:32.710
you know I guess and right well another
great drawing of a horse but you get the

00:02:32.710 --> 00:02:32.720
great drawing of a horse but you get the
 

00:02:32.720 --> 00:02:36.100
great drawing of a horse but you get the
idea then you the rider will sit here in

00:02:36.100 --> 00:02:36.110
idea then you the rider will sit here in
 

00:02:36.110 --> 00:02:40.120
idea then you the rider will sit here in
the saddle so then so that's why this

00:02:40.120 --> 00:02:40.130
the saddle so then so that's why this
 

00:02:40.130 --> 00:02:43.060
the saddle so then so that's why this
point here where the derivative is zero

00:02:43.060 --> 00:02:43.070
point here where the derivative is zero
 

00:02:43.070 --> 00:02:48.010
point here where the derivative is zero
that point is called a saddle point it's

00:02:48.010 --> 00:02:48.020
that point is called a saddle point it's
 

00:02:48.020 --> 00:02:49.449
that point is called a saddle point it's
really the point to understand where

00:02:49.449 --> 00:02:49.459
really the point to understand where
 

00:02:49.459 --> 00:02:51.130
really the point to understand where
you're sitting s and that happens to

00:02:51.130 --> 00:02:51.140
you're sitting s and that happens to
 

00:02:51.140 --> 00:02:54.760
you're sitting s and that happens to
have you know derivative zero and so one

00:02:54.760 --> 00:02:54.770
have you know derivative zero and so one
 

00:02:54.770 --> 00:02:56.320
have you know derivative zero and so one
of the lessons we learned in history of

00:02:56.320 --> 00:02:56.330
of the lessons we learned in history of
 

00:02:56.330 --> 00:02:57.760
of the lessons we learned in history of
deep learning is that a lot of our

00:02:57.760 --> 00:02:57.770
deep learning is that a lot of our
 

00:02:57.770 --> 00:02:59.500
deep learning is that a lot of our
intuitions about low dimensional spaces

00:02:59.500 --> 00:02:59.510
intuitions about low dimensional spaces
 

00:02:59.510 --> 00:03:01.150
intuitions about low dimensional spaces
like what you can plot on the left

00:03:01.150 --> 00:03:01.160
like what you can plot on the left
 

00:03:01.160 --> 00:03:03.460
like what you can plot on the left
they really don't transfer to the very

00:03:03.460 --> 00:03:03.470
they really don't transfer to the very
 

00:03:03.470 --> 00:03:05.770
they really don't transfer to the very
high dimensional spaces then our

00:03:05.770 --> 00:03:05.780
high dimensional spaces then our
 

00:03:05.780 --> 00:03:07.449
high dimensional spaces then our
learning algorithms are operating over

00:03:07.449 --> 00:03:07.459
learning algorithms are operating over
 

00:03:07.459 --> 00:03:09.760
learning algorithms are operating over
because if you have twenty thousand

00:03:09.760 --> 00:03:09.770
because if you have twenty thousand
 

00:03:09.770 --> 00:03:12.880
because if you have twenty thousand
parameters then J is V a function over a

00:03:12.880 --> 00:03:12.890
parameters then J is V a function over a
 

00:03:12.890 --> 00:03:14.620
parameters then J is V a function over a
twenty thousand dimensional vector and

00:03:14.620 --> 00:03:14.630
twenty thousand dimensional vector and
 

00:03:14.630 --> 00:03:16.420
twenty thousand dimensional vector and
you're much more likely to see saddle

00:03:16.420 --> 00:03:16.430
you're much more likely to see saddle
 

00:03:16.430 --> 00:03:18.580
you're much more likely to see saddle
points than local optimum if local

00:03:18.580 --> 00:03:18.590
points than local optimum if local
 

00:03:18.590 --> 00:03:20.830
points than local optimum if local
optima aren't a problem then what is a

00:03:20.830 --> 00:03:20.840
optima aren't a problem then what is a
 

00:03:20.840 --> 00:03:23.979
optima aren't a problem then what is a
problem it turns out that plateaus can

00:03:23.979 --> 00:03:23.989
problem it turns out that plateaus can
 

00:03:23.989 --> 00:03:26.229
problem it turns out that plateaus can
really slow down learning and the

00:03:26.229 --> 00:03:26.239
really slow down learning and the
 

00:03:26.239 --> 00:03:28.479
really slow down learning and the
plateau is a region where the derivative

00:03:28.479 --> 00:03:28.489
plateau is a region where the derivative
 

00:03:28.489 --> 00:03:32.199
plateau is a region where the derivative
is close to zero for a long time so if

00:03:32.199 --> 00:03:32.209
is close to zero for a long time so if
 

00:03:32.209 --> 00:03:36.039
is close to zero for a long time so if
you are here then gradient descent will

00:03:36.039 --> 00:03:36.049
you are here then gradient descent will
 

00:03:36.049 --> 00:03:38.770
you are here then gradient descent will
move down the surface and because the

00:03:38.770 --> 00:03:38.780
move down the surface and because the
 

00:03:38.780 --> 00:03:41.410
move down the surface and because the
gradient is zero or near zero the

00:03:41.410 --> 00:03:41.420
gradient is zero or near zero the
 

00:03:41.420 --> 00:03:43.539
gradient is zero or near zero the
surface is quite flat you can actually

00:03:43.539 --> 00:03:43.549
surface is quite flat you can actually
 

00:03:43.549 --> 00:03:46.350
surface is quite flat you can actually
take a very long time you know to slowly

00:03:46.350 --> 00:03:46.360
take a very long time you know to slowly
 

00:03:46.360 --> 00:03:50.770
take a very long time you know to slowly
find your way to maybe this point on the

00:03:50.770 --> 00:03:50.780
find your way to maybe this point on the
 

00:03:50.780 --> 00:03:52.720
find your way to maybe this point on the
plateau and then because of a random

00:03:52.720 --> 00:03:52.730
plateau and then because of a random
 

00:03:52.730 --> 00:03:54.370
plateau and then because of a random
perturbation to the left or right maybe

00:03:54.370 --> 00:03:54.380
perturbation to the left or right maybe
 

00:03:54.380 --> 00:03:56.890
perturbation to the left or right maybe
then finally I'm gonna switch pen colors

00:03:56.890 --> 00:03:56.900
then finally I'm gonna switch pen colors
 

00:03:56.900 --> 00:03:59.140
then finally I'm gonna switch pen colors
for clarity your algorithm can then find

00:03:59.140 --> 00:03:59.150
for clarity your algorithm can then find
 

00:03:59.150 --> 00:04:01.120
for clarity your algorithm can then find
this way off the plateau but then to

00:04:01.120 --> 00:04:01.130
this way off the plateau but then to
 

00:04:01.130 --> 00:04:03.970
this way off the plateau but then to
take this very long slope off before

00:04:03.970 --> 00:04:03.980
take this very long slope off before
 

00:04:03.980 --> 00:04:05.410
take this very long slope off before
it's found this way here and they could

00:04:05.410 --> 00:04:05.420
it's found this way here and they could
 

00:04:05.420 --> 00:04:08.430
it's found this way here and they could
get off this plateau

00:04:08.430 --> 00:04:08.440
get off this plateau
 

00:04:08.440 --> 00:04:10.800
get off this plateau
so the takeaways from this video are

00:04:10.800 --> 00:04:10.810
so the takeaways from this video are
 

00:04:10.810 --> 00:04:13.110
so the takeaways from this video are
first you actually pretty unlikely to

00:04:13.110 --> 00:04:13.120
first you actually pretty unlikely to
 

00:04:13.120 --> 00:04:15.120
first you actually pretty unlikely to
get stuck in bad local optima so long as

00:04:15.120 --> 00:04:15.130
get stuck in bad local optima so long as
 

00:04:15.130 --> 00:04:16.440
get stuck in bad local optima so long as
you're training and reasonably launched

00:04:16.440 --> 00:04:16.450
you're training and reasonably launched
 

00:04:16.450 --> 00:04:18.479
you're training and reasonably launched
new network save a lot of parameters and

00:04:18.479 --> 00:04:18.489
new network save a lot of parameters and
 

00:04:18.489 --> 00:04:21.000
new network save a lot of parameters and
the cost function J is defined over a

00:04:21.000 --> 00:04:21.010
the cost function J is defined over a
 

00:04:21.010 --> 00:04:23.520
the cost function J is defined over a
relatively high dimensional space but

00:04:23.520 --> 00:04:23.530
relatively high dimensional space but
 

00:04:23.530 --> 00:04:25.920
relatively high dimensional space but
second that plateaus are a problem and

00:04:25.920 --> 00:04:25.930
second that plateaus are a problem and
 

00:04:25.930 --> 00:04:27.840
second that plateaus are a problem and
they can actually make learning pretty

00:04:27.840 --> 00:04:27.850
they can actually make learning pretty
 

00:04:27.850 --> 00:04:30.270
they can actually make learning pretty
slow and this is where algorithms like

00:04:30.270 --> 00:04:30.280
slow and this is where algorithms like
 

00:04:30.280 --> 00:04:32.700
slow and this is where algorithms like
momentum or our most proper atom can

00:04:32.700 --> 00:04:32.710
momentum or our most proper atom can
 

00:04:32.710 --> 00:04:34.110
momentum or our most proper atom can
really help you learning algorithm as

00:04:34.110 --> 00:04:34.120
really help you learning algorithm as
 

00:04:34.120 --> 00:04:38.280
really help you learning algorithm as
well and these are scenarios where more

00:04:38.280 --> 00:04:38.290
well and these are scenarios where more
 

00:04:38.290 --> 00:04:39.990
well and these are scenarios where more
sophisticated optimization algorithms

00:04:39.990 --> 00:04:40.000
sophisticated optimization algorithms
 

00:04:40.000 --> 00:04:42.570
sophisticated optimization algorithms
such as atom can actually speed up the

00:04:42.570 --> 00:04:42.580
such as atom can actually speed up the
 

00:04:42.580 --> 00:04:44.850
such as atom can actually speed up the
rate at which you could move down the

00:04:44.850 --> 00:04:44.860
rate at which you could move down the
 

00:04:44.860 --> 00:04:47.880
rate at which you could move down the
plateau and then get off the plateau so

00:04:47.880 --> 00:04:47.890
plateau and then get off the plateau so
 

00:04:47.890 --> 00:04:49.260
plateau and then get off the plateau so
because your networks are solving

00:04:49.260 --> 00:04:49.270
because your networks are solving
 

00:04:49.270 --> 00:04:51.240
because your networks are solving
optimization problems over such high

00:04:51.240 --> 00:04:51.250
optimization problems over such high
 

00:04:51.250 --> 00:04:53.220
optimization problems over such high
dimensional spaces to be honest I don't

00:04:53.220 --> 00:04:53.230
dimensional spaces to be honest I don't
 

00:04:53.230 --> 00:04:55.320
dimensional spaces to be honest I don't
think anyone has great intuitions about

00:04:55.320 --> 00:04:55.330
think anyone has great intuitions about
 

00:04:55.330 --> 00:04:57.570
think anyone has great intuitions about
what these spaces really look like and

00:04:57.570 --> 00:04:57.580
what these spaces really look like and
 

00:04:57.580 --> 00:04:59.010
what these spaces really look like and
our understanding of them is still

00:04:59.010 --> 00:04:59.020
our understanding of them is still
 

00:04:59.020 --> 00:05:00.990
our understanding of them is still
evolving but I hope this gives you some

00:05:00.990 --> 00:05:01.000
evolving but I hope this gives you some
 

00:05:01.000 --> 00:05:03.780
evolving but I hope this gives you some
better intuition about the challenges

00:05:03.780 --> 00:05:03.790
better intuition about the challenges
 

00:05:03.790 --> 00:05:05.880
better intuition about the challenges
that the optimization algorithms may

00:05:05.880 --> 00:05:05.890
that the optimization algorithms may
 

00:05:05.890 --> 00:05:08.670
that the optimization algorithms may
face so that congratulations on coming

00:05:08.670 --> 00:05:08.680
face so that congratulations on coming
 

00:05:08.680 --> 00:05:11.310
face so that congratulations on coming
to the end of this week's content please

00:05:11.310 --> 00:05:11.320
to the end of this week's content please
 

00:05:11.320 --> 00:05:13.920
to the end of this week's content please
take a look at this week's quiz as well

00:05:13.920 --> 00:05:13.930
take a look at this week's quiz as well
 

00:05:13.930 --> 00:05:16.050
take a look at this week's quiz as well
as the exercise I hope you enjoyed

00:05:16.050 --> 00:05:16.060
as the exercise I hope you enjoyed
 

00:05:16.060 --> 00:05:18.120
as the exercise I hope you enjoyed
practicing some of these ideas with this

00:05:18.120 --> 00:05:18.130
practicing some of these ideas with this
 

00:05:18.130 --> 00:05:20.160
practicing some of these ideas with this
week's forum exercise and I look forward

00:05:20.160 --> 00:05:20.170
week's forum exercise and I look forward
 

00:05:20.170 --> 00:05:21.660
week's forum exercise and I look forward
to seeing you at the start of next

00:05:21.660 --> 00:05:21.670
to seeing you at the start of next
 

00:05:21.670 --> 00:05:24.660
to seeing you at the start of next
week's videos

