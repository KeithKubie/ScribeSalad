WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.130
[MUSIC PLAYING]

00:00:06.130 --> 00:00:09.870
JEREMY HYLTON: I'm really
pleased to have Dr. Joy Rankin

00:00:09.870 --> 00:00:12.980
here today to talk
about her new book,

00:00:12.980 --> 00:00:15.815
"A People's History of
Computing in the United States,"

00:00:15.815 --> 00:00:16.710
if I got that right.

00:00:16.710 --> 00:00:18.360
JOY LISI RANKIN: Yes, you did.

00:00:18.360 --> 00:00:23.670
JEREMY HYLTON: And I sort of
discovered the book, you know,

00:00:23.670 --> 00:00:24.810
browsing on the web.

00:00:24.810 --> 00:00:26.850
And, you know, it
talked about sort

00:00:26.850 --> 00:00:28.920
of early computing experiences.

00:00:28.920 --> 00:00:32.520
And I really
connected with BASIC,

00:00:32.520 --> 00:00:35.670
which is going to come
up a lot in the book.

00:00:35.670 --> 00:00:39.630
I started programming in
BASIC and home computer

00:00:39.630 --> 00:00:44.030
when I was a kid,
and really enjoyed

00:00:44.030 --> 00:00:46.650
a book called "101
Basic Computer Games"

00:00:46.650 --> 00:00:48.480
that were very simple games.

00:00:48.480 --> 00:00:51.030
And it's interesting
that as I became

00:00:51.030 --> 00:00:53.430
a professional
programmer, I mostly

00:00:53.430 --> 00:00:57.570
spent talking about how much I
didn't really approve of BASIC.

00:00:57.570 --> 00:01:00.670
It was not a serious
language, right?

00:01:00.670 --> 00:01:03.550
But, you know, in thinking
about it and reading the book,

00:01:03.550 --> 00:01:07.235
I came to remember how
much fun it was and how--

00:01:07.235 --> 00:01:08.610
like how you turned
on a computer

00:01:08.610 --> 00:01:10.040
and there was BASIC in
front of you, right?

00:01:10.040 --> 00:01:12.498
Like, you know, you immediately
got the interpreter prompt,

00:01:12.498 --> 00:01:14.360
and that was pretty amazing.

00:01:14.360 --> 00:01:17.520
And I thought the message
of the book lined up so well

00:01:17.520 --> 00:01:19.590
with the three
respects we're trying

00:01:19.590 --> 00:01:22.920
to think about it at Google.

00:01:22.920 --> 00:01:25.410
One, respect the user,
since this is really

00:01:25.410 --> 00:01:28.470
all about letting users program
the computer and interact

00:01:28.470 --> 00:01:30.000
with it more directly.

00:01:30.000 --> 00:01:33.000
And I thought this book was
particularly interesting

00:01:33.000 --> 00:01:37.720
when we think about
respecting the opportunity.

00:01:37.720 --> 00:01:41.400
Because we have a
big business that

00:01:41.400 --> 00:01:44.290
sells computing to consumers.

00:01:44.290 --> 00:01:47.134
You know, we either pay through
ads or through the cloud

00:01:47.134 --> 00:01:47.925
computing business.

00:01:50.490 --> 00:01:52.470
And that's kind of intention--

00:01:52.470 --> 00:01:55.080
or, you know, the
business relationship

00:01:55.080 --> 00:01:57.360
is a little bit in tension
with empowering people

00:01:57.360 --> 00:01:58.900
to create whatever they want.

00:01:58.900 --> 00:02:01.050
Now, clearly cloud sort
of supports that, as well.

00:02:01.050 --> 00:02:05.010
But the-- I think the
early idea of search

00:02:05.010 --> 00:02:07.110
was to make the web
more powerful, right?

00:02:07.110 --> 00:02:10.949
That as the web got
better, Google got better.

00:02:10.949 --> 00:02:13.440
And as Google got better we
made the web better for people.

00:02:13.440 --> 00:02:16.560
And I think there's a little
bit of a tension between that

00:02:16.560 --> 00:02:22.150
and running a large corporation
and then a profitable business.

00:02:22.150 --> 00:02:25.020
And I thought some of the
stories you had to tell really

00:02:25.020 --> 00:02:28.020
helped remind us of how
many people are creators

00:02:28.020 --> 00:02:31.980
and how much creativity we
can unlock when we really

00:02:31.980 --> 00:02:34.260
think about them and
the opportunities we

00:02:34.260 --> 00:02:36.240
create for them.

00:02:36.240 --> 00:02:38.790
So that's why I was so excited
to have you come and talk

00:02:38.790 --> 00:02:39.870
about this book.

00:02:39.870 --> 00:02:43.530
So Dr. Rankin is a historian,
but in chatting with her today,

00:02:43.530 --> 00:02:46.800
I found out she was also
a math major at Dartmouth.

00:02:46.800 --> 00:02:49.540
She has written
some crypto code.

00:02:49.540 --> 00:02:51.840
So I think she brings a
really interesting perspective

00:02:51.840 --> 00:02:52.990
to the whole story.

00:02:52.990 --> 00:02:54.584
So let me turn it over to her.

00:02:54.584 --> 00:02:57.000
JOY LISI RANKIN: Thank you,
Jeremy, for that introduction,

00:02:57.000 --> 00:03:01.000
and thank you all
for being here today.

00:03:01.000 --> 00:03:04.920
I am so excited to have the
opportunity to speak with you

00:03:04.920 --> 00:03:06.870
and speak with Google.

00:03:06.870 --> 00:03:10.090
I am just going
to jump right in.

00:03:10.090 --> 00:03:12.345
JEREMY HYLTON: Switch
to that slide for you.

00:03:12.345 --> 00:03:13.470
JOY LISI RANKIN: All right.

00:03:13.470 --> 00:03:20.940
So in 1958, Tom Kurtz wanted
to run a computer program.

00:03:20.940 --> 00:03:23.850
Here is Tom Kurtz circa 1958.

00:03:23.850 --> 00:03:26.850
So to do this, to run
his computer program,

00:03:26.850 --> 00:03:31.170
he woke early one morning
and traveled about five miles

00:03:31.170 --> 00:03:34.170
from his home in
Hanover, New Hampshire

00:03:34.170 --> 00:03:36.930
to White River
Junction, Vermont.

00:03:36.930 --> 00:03:41.520
He carried with him a steel
box that looked like this one.

00:03:41.520 --> 00:03:44.220
And when he got to White
River Junction, Vermont,

00:03:44.220 --> 00:03:50.560
he boarded the 620
morning train to Boston.

00:03:50.560 --> 00:03:54.600
At Boston, he trades
the train for a taxi

00:03:54.600 --> 00:04:00.030
to get to the computer
center at MIT.

00:04:00.030 --> 00:04:02.970
Finally, finally, at
MIT's computer center,

00:04:02.970 --> 00:04:05.820
he opens the box that
he carries with him,

00:04:05.820 --> 00:04:09.030
and it contains hundreds
of cardboard cards.

00:04:09.030 --> 00:04:13.710
So some of those cards, very
carefully ordered and wrapped

00:04:13.710 --> 00:04:17.670
with an elastic band,
are his computer program.

00:04:17.670 --> 00:04:21.510
Other cards contain the
programs of his colleagues

00:04:21.510 --> 00:04:25.290
at Dartmouth College, where
he is a professor in the math

00:04:25.290 --> 00:04:26.550
department.

00:04:26.550 --> 00:04:29.100
So Dartmouth is a small
liberal arts college.

00:04:29.100 --> 00:04:32.750
During the 1950s, it doesn't
have its own computer,

00:04:32.750 --> 00:04:39.810
so the Dartmouth professors used
the computing resources at MIT.

00:04:39.810 --> 00:04:43.200
So Kurtz hands his
stack of punch cards

00:04:43.200 --> 00:04:49.200
over to the operator at the MIT
computer center and walks away.

00:04:49.200 --> 00:04:51.150
Hours later, at
the end of the day,

00:04:51.150 --> 00:04:54.720
he goes back to the computer
center to pick up his cards

00:04:54.720 --> 00:04:59.010
and pick up the very
precious printout results

00:04:59.010 --> 00:05:01.290
of his computer program.

00:05:01.290 --> 00:05:05.190
And he gets on the train back
home, and he sees, of course,

00:05:05.190 --> 00:05:07.350
some error messages.

00:05:07.350 --> 00:05:08.710
What does this mean?

00:05:08.710 --> 00:05:11.900
Well, it means he's going to
have to revise his program,

00:05:11.900 --> 00:05:17.490
re-punch the cards, and repeat
this process all over again two

00:05:17.490 --> 00:05:18.970
weeks later.

00:05:18.970 --> 00:05:21.660
So I love starting
with this story

00:05:21.660 --> 00:05:26.070
because it shows that individual
access to computing in 1958

00:05:26.070 --> 00:05:30.870
was so rare and so valuable that
Tom Kurtz was willing to spend

00:05:30.870 --> 00:05:36.270
a whole day to receive a few
minutes, maybe a few seconds,

00:05:36.270 --> 00:05:40.770
of access to computing,
and it also really shows

00:05:40.770 --> 00:05:45.000
Tom's perspective as a user.

00:05:45.000 --> 00:05:48.660
So this brings me
to my research.

00:05:48.660 --> 00:05:50.430
The question that
guides my research

00:05:50.430 --> 00:05:53.460
is, what does it mean to
write histories of computing

00:05:53.460 --> 00:05:57.660
and of technology
from the user up?

00:05:57.660 --> 00:06:00.870
My book, "A People's History
of Computing in the United

00:06:00.870 --> 00:06:03.780
States," answers that question.

00:06:03.780 --> 00:06:06.360
The people in this
"People's History"

00:06:06.360 --> 00:06:09.630
are students and
educators, people

00:06:09.630 --> 00:06:15.300
like Tom Kurtz who built
and used academic computing

00:06:15.300 --> 00:06:19.350
networks during the
1960s and the 1970s.

00:06:19.350 --> 00:06:22.260
So mine is a history
of our digital age

00:06:22.260 --> 00:06:27.060
that really emphasizes
creativity and collaboration

00:06:27.060 --> 00:06:28.960
and community.

00:06:28.960 --> 00:06:32.190
So for the next
15 minutes or so,

00:06:32.190 --> 00:06:36.270
I'm going to tell you how Kurtz
and thousands of other students

00:06:36.270 --> 00:06:40.050
and educators built a
computing network that spanned

00:06:40.050 --> 00:06:43.620
New England during the 1960s.

00:06:43.620 --> 00:06:47.730
And I want to show you how
computing was interwoven

00:06:47.730 --> 00:06:49.140
in their daily lives.

00:06:49.140 --> 00:06:54.150
It was part of their status,
their sports, their dates,

00:06:54.150 --> 00:06:56.310
their dating, and their gaming.

00:06:56.310 --> 00:07:01.650
And finally, I will conclude
with an opportunity.

00:07:01.650 --> 00:07:04.770
So all that time on the
train got Kurtz to thinking,

00:07:04.770 --> 00:07:07.870
there has to be a
better way to do this.

00:07:07.870 --> 00:07:11.430
And a few years after
that 1958 train ride,

00:07:11.430 --> 00:07:14.460
he proposed a better
way of computing

00:07:14.460 --> 00:07:19.740
to a fellow math professor
at Dartmouth, John Kemeny.

00:07:19.740 --> 00:07:22.440
And Kurtz pitched
three related ideas.

00:07:22.440 --> 00:07:25.050
First, all students
at the college

00:07:25.050 --> 00:07:27.074
should have access to computing.

00:07:27.074 --> 00:07:27.990
And this is important.

00:07:27.990 --> 00:07:29.835
Not just the scientists
or the engineers

00:07:29.835 --> 00:07:32.730
or the mathematicians,
but everyone, regardless

00:07:32.730 --> 00:07:34.740
of what they were studying.

00:07:34.740 --> 00:07:37.260
He also advocated
that computing should

00:07:37.260 --> 00:07:39.960
be free for the students,
not something they

00:07:39.960 --> 00:07:41.730
would have to pay for.

00:07:41.730 --> 00:07:44.040
And finally, he thought
the first two things

00:07:44.040 --> 00:07:47.340
could be accomplished by
a then mode of computing

00:07:47.340 --> 00:07:50.250
known as time-sharing.

00:07:50.250 --> 00:07:53.040
So what's time-sharing?

00:07:53.040 --> 00:07:54.960
Time-sharing is a
form of computing

00:07:54.960 --> 00:07:57.990
in which multiple
terminals, multiple users

00:07:57.990 --> 00:08:02.760
are simultaneously connected
to a single mainframe computer.

00:08:02.760 --> 00:08:06.210
It's a network because they're
connected by phone lines.

00:08:06.210 --> 00:08:09.660
So not necessarily a network
like we think of today,

00:08:09.660 --> 00:08:12.040
but all of those
terminals can still

00:08:12.040 --> 00:08:17.020
communicate with each
other via the mainframe.

00:08:17.020 --> 00:08:19.210
The terminals are
connected to the mainframe

00:08:19.210 --> 00:08:22.240
by long distance phone
lines or local phone lines,

00:08:22.240 --> 00:08:25.710
as the case may be, and the
terminals look like this.

00:08:25.710 --> 00:08:29.050
And so they're teletype writers,
or teletypes, originally used

00:08:29.050 --> 00:08:32.950
for telegraphy, then
used for computing,

00:08:32.950 --> 00:08:34.929
and like an old
fashioned typewriter

00:08:34.929 --> 00:08:36.370
with a printer attached.

00:08:36.370 --> 00:08:39.610
So users would enter
their programs,

00:08:39.610 --> 00:08:42.220
see them sort of printed
out, and then, the computer

00:08:42.220 --> 00:08:44.110
would communicate
back to the user

00:08:44.110 --> 00:08:50.710
by printing out program
results on the teletype writer.

00:08:50.710 --> 00:08:54.190
So what this means
is an individual user

00:08:54.190 --> 00:09:00.486
is able to write and debug a
program in minutes, not hours

00:09:00.486 --> 00:09:00.985
or weeks.

00:09:04.190 --> 00:09:09.820
So when Kurtz and Kemeny seek
funding for their network, what

00:09:09.820 --> 00:09:13.690
they emphasize is that at most
other university computing

00:09:13.690 --> 00:09:17.050
centers during the
1960s, those universities

00:09:17.050 --> 00:09:21.470
are focused on maximizing the
resources of the computer.

00:09:21.470 --> 00:09:23.770
Computers-- very
large, very expensive.

00:09:23.770 --> 00:09:25.390
The most efficient
thing and do is

00:09:25.390 --> 00:09:27.590
to keep them running
as much as possible,

00:09:27.590 --> 00:09:29.020
but that doesn't
do a lot of good

00:09:29.020 --> 00:09:31.870
for the people
using the computers.

00:09:31.870 --> 00:09:34.540
Instead, Kemeny
and Kurtz say, we

00:09:34.540 --> 00:09:36.370
want to build a
network that will

00:09:36.370 --> 00:09:42.100
be most convenient for the user
to enable as much creativity

00:09:42.100 --> 00:09:46.060
and productivity as
possible for those users

00:09:46.060 --> 00:09:49.015
rather than maximizing the
efficiency of the computer.

00:09:53.170 --> 00:09:54.940
They get their funding.

00:09:54.940 --> 00:09:58.870
Kurtz writes a series of
memos to guide the development

00:09:58.870 --> 00:09:59.410
efforts.

00:09:59.410 --> 00:10:01.450
And at this point, I
should say this network

00:10:01.450 --> 00:10:04.180
is programmed and
developed by Dartmouth

00:10:04.180 --> 00:10:07.120
undergraduate students.

00:10:07.120 --> 00:10:10.750
And Kurtz specifies--
and this is a quote--

00:10:10.750 --> 00:10:14.770
"in all cases where there is
a choice between simplicity

00:10:14.770 --> 00:10:18.980
and efficiency,
simplicity is chosen.

00:10:18.980 --> 00:10:22.330
Every effort will be
made to design a system

00:10:22.330 --> 00:10:24.950
convenient for the user."

00:10:24.950 --> 00:10:29.870
So I love this because clearly,
this is respect for the user.

00:10:29.870 --> 00:10:35.530
This is what it looks like for
Kemeny and Kurtz circa 1963.

00:10:35.530 --> 00:10:36.910
And at this point,
I'm just going

00:10:36.910 --> 00:10:40.270
to say that the Dartmouth
system, the network,

00:10:40.270 --> 00:10:42.070
was wildly popular.

00:10:42.070 --> 00:10:44.950
It was implemented in 1964.

00:10:44.950 --> 00:10:48.490
By 1968, on campus,
80% of the students

00:10:48.490 --> 00:10:50.530
are regularly using the network.

00:10:50.530 --> 00:10:52.150
They know BASIC.

00:10:52.150 --> 00:10:54.460
They program comfortably.

00:10:54.460 --> 00:10:56.890
And 40% of the
faculty are regularly

00:10:56.890 --> 00:10:58.945
using the network and BASIC.

00:11:02.770 --> 00:11:05.470
But-- and this is
equally important--

00:11:05.470 --> 00:11:10.270
the sort of generic idea,
just the word, "user"

00:11:10.270 --> 00:11:13.720
on the Dartmouth network
shows that that word

00:11:13.720 --> 00:11:17.560
masked social and
cultural hierarchies

00:11:17.560 --> 00:11:23.590
and norms and expectations,
especially around gender.

00:11:23.590 --> 00:11:27.520
So Kemeny and Kurtz designed
their time-sharing network

00:11:27.520 --> 00:11:32.380
because they believed that their
students, who were all men--

00:11:32.380 --> 00:11:35.770
during the 1960s, Dartmouth
is still men only as

00:11:35.770 --> 00:11:37.420
an undergraduate institution.

00:11:37.420 --> 00:11:40.540
And during the 1960s,
Dartmouth is almost exclusively

00:11:40.540 --> 00:11:43.000
white as an institution--

00:11:43.000 --> 00:11:45.640
that those white men would
be the future leaders

00:11:45.640 --> 00:11:47.110
of the United States.

00:11:47.110 --> 00:11:48.640
And they thought
that computing was

00:11:48.640 --> 00:11:51.820
essential to their future
leadership of the United

00:11:51.820 --> 00:11:52.930
States.

00:11:52.930 --> 00:11:56.020
They also envisioned
computing as an equalizer

00:11:56.020 --> 00:11:58.990
among their students, that
all of the students having

00:11:58.990 --> 00:12:01.120
that access for free
would be something

00:12:01.120 --> 00:12:03.640
of an equal opportunity.

00:12:03.640 --> 00:12:07.850
But the students
saw it differently.

00:12:07.850 --> 00:12:10.510
The students who had
programmed the network

00:12:10.510 --> 00:12:13.240
and continued to
develop and maintain it

00:12:13.240 --> 00:12:16.420
really created a space and
hierarchy for themselves

00:12:16.420 --> 00:12:19.960
based on their familiarity
with the system.

00:12:19.960 --> 00:12:23.580
So they mocked their peers
who thought of the teletypes

00:12:23.580 --> 00:12:26.980
as computers or complained
if a teletype wasn't

00:12:26.980 --> 00:12:29.350
working that the
computer wasn't working.

00:12:29.350 --> 00:12:33.280
And they played practical
jokes, substituting

00:12:33.280 --> 00:12:38.350
strings of meaningless text
into the output of a program

00:12:38.350 --> 00:12:41.935
that somebody had spent
hours or days writing.

00:12:45.360 --> 00:12:47.770
These programmer and
student peers, however,

00:12:47.770 --> 00:12:51.900
were united in their
enthusiasm for football

00:12:51.900 --> 00:12:56.370
played on the field
and on the network.

00:12:56.370 --> 00:13:00.480
So football-- to say
that football dominated

00:13:00.480 --> 00:13:02.490
Dartmouth's culture
during the 1960s

00:13:02.490 --> 00:13:03.870
is a bit of an understatement.

00:13:03.870 --> 00:13:07.830
Dartmouth won the Ivy League
Championship several times,

00:13:07.830 --> 00:13:11.520
and really, the
campus and football

00:13:11.520 --> 00:13:14.310
went hand-in-hand as cultures.

00:13:14.310 --> 00:13:16.960
And there were several
versions of football.

00:13:16.960 --> 00:13:20.940
FTBALL like this was one of them
that were developed and played

00:13:20.940 --> 00:13:26.310
on the network, including
a multiplayer version.

00:13:26.310 --> 00:13:31.380
So the popularity of the
football on the network

00:13:31.380 --> 00:13:35.400
really mirrored the celebration
of this very masculine, macho

00:13:35.400 --> 00:13:38.180
sport on campus.

00:13:38.180 --> 00:13:42.990
The football games also provided
an opportunity from women

00:13:42.990 --> 00:13:47.550
from nearby all women's
colleges to travel to Hanover

00:13:47.550 --> 00:13:50.280
to socialize with
the Dartmouth men,

00:13:50.280 --> 00:13:54.150
and many men recalled that
they brought their deeds

00:13:54.150 --> 00:13:57.600
to the campus computing centers
before or after the games

00:13:57.600 --> 00:14:00.120
to show off their
computing prowess

00:14:00.120 --> 00:14:04.560
or show off their
gaming expertise,

00:14:04.560 --> 00:14:07.860
even though several of
those women's colleges

00:14:07.860 --> 00:14:10.620
were also part of the
Dartmouth network, which

00:14:10.620 --> 00:14:15.460
meant the women themselves
were also computing, as well.

00:14:15.460 --> 00:14:17.790
So in the course
of my research, I

00:14:17.790 --> 00:14:21.600
was repeatedly struck by how
many Dartmouth men wanted

00:14:21.600 --> 00:14:25.290
to tell me and did tell
me stories about computing

00:14:25.290 --> 00:14:29.520
in connection with dating
and how they really

00:14:29.520 --> 00:14:33.570
were reinforcing existing gender
roles with their computing.

00:14:33.570 --> 00:14:37.350
So one told me he
used the teletype

00:14:37.350 --> 00:14:40.920
to print out this huge
banner that said "HEY,

00:14:40.920 --> 00:14:42.360
GIRL"-- in all caps.

00:14:42.360 --> 00:14:45.420
"HEY, GIRL, I MISS YOU."

00:14:45.420 --> 00:14:49.140
And he rolled up the huge banner
and put it in a poster tube

00:14:49.140 --> 00:14:51.840
and mailed it to
his then girlfriend.

00:14:51.840 --> 00:14:54.210
He was attending
college in New York.

00:14:54.210 --> 00:14:56.540
And now, they've been
married for almost 50 years.

00:14:56.540 --> 00:14:57.670
And this is one example.

00:14:57.670 --> 00:15:02.250
Like, I'm happy to give
you more during the Q&amp;A.

00:15:02.250 --> 00:15:06.040
But what do I mean
by gender roles here?

00:15:06.040 --> 00:15:09.240
So remember, we're
talking the 1960s.

00:15:09.240 --> 00:15:13.370
The Cold War nuclear
family was the norm.

00:15:13.370 --> 00:15:16.950
Right, this idea that there
is a breadwinner father

00:15:16.950 --> 00:15:22.410
and a domestic mother staying at
home and some kids in suburbia.

00:15:22.410 --> 00:15:25.290
And we can see this
even with computing.

00:15:25.290 --> 00:15:29.700
So John Kemeny goes on to become
president of Dartmouth College,

00:15:29.700 --> 00:15:33.390
and in 1966, the college
is dedicating its brand

00:15:33.390 --> 00:15:35.680
new computing center.

00:15:35.680 --> 00:15:39.510
And he gives a keynote speech
at the dedication and he

00:15:39.510 --> 00:15:42.180
talks about how great
it will be when everyone

00:15:42.180 --> 00:15:44.110
has terminals in their homes.

00:15:44.110 --> 00:15:46.050
At this point, some
faculty already

00:15:46.050 --> 00:15:50.010
have terminals in their
homes as part of the network.

00:15:50.010 --> 00:15:53.220
And he talks about housewives
using the terminals

00:15:53.220 --> 00:15:56.280
to do things like
program their chores

00:15:56.280 --> 00:16:00.570
and plan balanced diets and
meals for their families

00:16:00.570 --> 00:16:02.930
and do their shopping online.

00:16:02.930 --> 00:16:04.260
And those are the--

00:16:04.260 --> 00:16:09.330
that's the way that he
sees women using computing.

00:16:09.330 --> 00:16:13.410
But while he is talking about
housewives using computing

00:16:13.410 --> 00:16:16.200
to prepare balanced
meals, there are

00:16:16.200 --> 00:16:19.290
women working at
Dartmouth in that computer

00:16:19.290 --> 00:16:22.490
center in key roles.

00:16:22.490 --> 00:16:24.970
There is a disconnect there.

00:16:24.970 --> 00:16:29.190
So Janet Price, Diane
Hills, Diane Mather,

00:16:29.190 --> 00:16:32.520
they're all doing various kinds
of applications programming,

00:16:32.520 --> 00:16:33.600
coding.

00:16:33.600 --> 00:16:37.220
Nancy Broadhead
manages user services.

00:16:37.220 --> 00:16:42.240
The two Dianes are also
managing the software library.

00:16:42.240 --> 00:16:44.790
And the examples go on.

00:16:44.790 --> 00:16:47.880
I bring this up because
these women were not

00:16:47.880 --> 00:16:51.810
exceptions to the norm,
but rather represented

00:16:51.810 --> 00:16:54.570
the range of
possibilities for women

00:16:54.570 --> 00:17:02.000
doing professional computing in
the United States in the 1960s.

00:17:02.000 --> 00:17:05.150
And, as I mentioned,
many of the college women

00:17:05.150 --> 00:17:07.579
themselves had
access to computing

00:17:07.579 --> 00:17:09.050
at their own colleges.

00:17:09.050 --> 00:17:11.750
They, too, were doing
this personal computing,

00:17:11.750 --> 00:17:16.550
this social networking
through the Dartmouth network.

00:17:16.550 --> 00:17:19.970
During the 1960s, the
network encompassed

00:17:19.970 --> 00:17:22.849
tens of colleges
and high schools

00:17:22.849 --> 00:17:24.319
from Connecticut to Maine.

00:17:24.319 --> 00:17:27.800
So this is a map of about
20 of the high schools

00:17:27.800 --> 00:17:31.340
that were connected to the
network during the 1960s.

00:17:31.340 --> 00:17:35.840
Some were in very poor
rural farming communities.

00:17:35.840 --> 00:17:41.120
Others were elite, prestigious,
affluent boarding schools.

00:17:41.120 --> 00:17:42.005
A huge range.

00:17:45.110 --> 00:17:47.900
The students, however,
everywhere, they

00:17:47.900 --> 00:17:49.100
loved the network.

00:17:49.100 --> 00:17:53.330
They loved programming
and creating and BASIC.

00:17:53.330 --> 00:17:56.700
They kept their teletypes
running 12 hours a day,

00:17:56.700 --> 00:17:58.560
six days a week.

00:17:58.560 --> 00:18:00.980
Some of them even
woke up at 4 o'clock

00:18:00.980 --> 00:18:02.390
in the morning-- high schoolers.

00:18:02.390 --> 00:18:05.540
Like, high schoolers waking
up at 4 o'clock voluntarily

00:18:05.540 --> 00:18:08.000
just because they want to
get on the network and that's

00:18:08.000 --> 00:18:10.010
when they can do it.

00:18:10.010 --> 00:18:15.060
They created a multitude of
imaginative games and programs.

00:18:15.060 --> 00:18:17.480
These are just some
of the examples.

00:18:17.480 --> 00:18:24.320
High school newspaper layout,
battleship, bowling, and more.

00:18:24.320 --> 00:18:30.720
But, once again, this attempt
to sort of provide equal access,

00:18:30.720 --> 00:18:33.260
equal opportunities
through computing,

00:18:33.260 --> 00:18:36.980
it didn't quite
pan out that way.

00:18:36.980 --> 00:18:38.510
Why?

00:18:38.510 --> 00:18:42.320
Well, boys and young
men had much more access

00:18:42.320 --> 00:18:44.960
to the Dartmouth
network than girls.

00:18:44.960 --> 00:18:48.440
So during the 1960s, every
single private school

00:18:48.440 --> 00:18:52.520
that was connected to the
network was boys only.

00:18:52.520 --> 00:18:57.140
And those private schools had
72 hours of teletype access

00:18:57.140 --> 00:19:01.820
each week, compared to the
40 hours for public schools.

00:19:01.820 --> 00:19:07.190
So boys at private schools have
almost double the access time,

00:19:07.190 --> 00:19:11.690
the network time, being shared
compared with public school

00:19:11.690 --> 00:19:14.210
boys and girls.

00:19:14.210 --> 00:19:17.540
And the boys are often
in sort of newsletters

00:19:17.540 --> 00:19:20.030
about the program, reports.

00:19:20.030 --> 00:19:23.540
The boys are more often
recognized for their programs,

00:19:23.540 --> 00:19:27.140
even though girls are still
writing these programs,

00:19:27.140 --> 00:19:28.350
as well.

00:19:28.350 --> 00:19:32.670
So again, keep in
mind the 1960s.

00:19:32.670 --> 00:19:37.400
Although career expectations
and educational opportunities

00:19:37.400 --> 00:19:43.070
for girls and women would change
dramatically in the next decade

00:19:43.070 --> 00:19:45.920
during the 1960s, most
of the girls attending

00:19:45.920 --> 00:19:47.390
these public schools
in New England

00:19:47.390 --> 00:19:52.970
were expected to be
homemakers and to marry young.

00:19:52.970 --> 00:19:54.920
And at the time,
high school boys

00:19:54.920 --> 00:19:59.930
were enrolling in many more math
and science classes than girls,

00:19:59.930 --> 00:20:03.410
and the girls were just not
encouraged in computing,

00:20:03.410 --> 00:20:05.810
even if they were interested.

00:20:05.810 --> 00:20:10.400
But make no mistake, they
were still computing.

00:20:10.400 --> 00:20:14.480
So, for example,
13-year-old Julia Hawthorne

00:20:14.480 --> 00:20:17.810
wrote a game of
checkers in BASIC,

00:20:17.810 --> 00:20:20.180
and there are more examples.

00:20:20.180 --> 00:20:23.180
It's just that the
girls and women were not

00:20:23.180 --> 00:20:27.590
recognized or supported as
much as the boys and men,

00:20:27.590 --> 00:20:30.300
and the Cold War
gender norms helped

00:20:30.300 --> 00:20:33.140
erase their contributions.

00:20:33.140 --> 00:20:35.570
So to us now, 50 years
later, all of this

00:20:35.570 --> 00:20:38.760
seems a little bit remarkable.

00:20:38.760 --> 00:20:43.820
So Tom Kurtz and John
Kemeny, as I said,

00:20:43.820 --> 00:20:46.430
they envisioned computing
as an equalizer.

00:20:46.430 --> 00:20:51.230
I think they were trying to
implement a version of respect

00:20:51.230 --> 00:20:55.310
each other with this idea
of computing for all,

00:20:55.310 --> 00:21:01.230
but that's not how it played out
because society was not equal,

00:21:01.230 --> 00:21:01.730
right?

00:21:01.730 --> 00:21:03.500
There are different
expectations,

00:21:03.500 --> 00:21:08.510
we can clearly see, for boys
and girls, men and women,

00:21:08.510 --> 00:21:12.470
and different levels
of access and resources

00:21:12.470 --> 00:21:17.220
and recognition for
everyone involved.

00:21:17.220 --> 00:21:22.340
So here, I see the
opportunity to recognize

00:21:22.340 --> 00:21:26.600
that computing was not divorced
from the culture in which it

00:21:26.600 --> 00:21:28.010
arose.

00:21:28.010 --> 00:21:30.650
Kemeny and Kurtz
and their students,

00:21:30.650 --> 00:21:35.390
they built the network with
people like themselves in mind.

00:21:35.390 --> 00:21:37.370
And ditto for all
of the students

00:21:37.370 --> 00:21:39.230
who created BASIC programs.

00:21:39.230 --> 00:21:42.350
They were thinking about
themselves and their friends.

00:21:42.350 --> 00:21:46.940
And that is absolutely
understandable,

00:21:46.940 --> 00:21:51.320
but it also points to the
fact that technology is not

00:21:51.320 --> 00:21:55.550
a thing apart from society.

00:21:55.550 --> 00:22:01.710
Talking about tech people or
separating out tech companies,

00:22:01.710 --> 00:22:04.910
that's a false dichotomy
because it's always

00:22:04.910 --> 00:22:08.630
people who are creating,
who are inventing,

00:22:08.630 --> 00:22:12.050
who are problem solving
and moonshotting.

00:22:12.050 --> 00:22:14.060
And it's almost
always that they're

00:22:14.060 --> 00:22:17.970
doing those things with
other people in mind,

00:22:17.970 --> 00:22:21.650
even if that's not
explicitly stated.

00:22:21.650 --> 00:22:27.140
And often, still, the people who
aren't recognized as techie--

00:22:27.140 --> 00:22:29.750
the girls writing
the BASIC programs,

00:22:29.750 --> 00:22:32.450
teachers cultivating
computing communities,

00:22:32.450 --> 00:22:36.260
the women behind the scenes at
Dartmouth's computer center--

00:22:36.260 --> 00:22:41.440
they're still doing
crucial technological work.

00:22:41.440 --> 00:22:45.510
So today, all too often, we
talk about things, right?

00:22:45.510 --> 00:22:49.800
We talk about our computers
and our devices, the internet,

00:22:49.800 --> 00:22:53.650
the web, the algorithm.

00:22:53.650 --> 00:22:55.360
And that makes all
of those things

00:22:55.360 --> 00:22:58.780
seem disconnected from
us, from the people who

00:22:58.780 --> 00:23:01.940
create them and use
them and transform them.

00:23:01.940 --> 00:23:04.030
So there is the opportunity.

00:23:04.030 --> 00:23:09.430
By focusing on computing
rather than computers,

00:23:09.430 --> 00:23:13.900
we gain a completely different
picture of technology.

00:23:13.900 --> 00:23:18.940
We listen for and hear the
voices not usually heard,

00:23:18.940 --> 00:23:22.420
and we search for
and find the stories

00:23:22.420 --> 00:23:27.730
that have been overlooked or
never told in the first place.

00:23:27.730 --> 00:23:33.130
And we remember that technology
has always been about people.

00:23:36.112 --> 00:23:37.106
Thanks.

00:23:37.106 --> 00:23:40.088
[APPLAUSE]

00:23:46.685 --> 00:23:49.060
JEREMY HYLTON: We'll switch
you back to the local camera.

00:23:49.060 --> 00:23:50.200
JOY LISI RANKIN: All right.

00:23:50.200 --> 00:23:54.160
JEREMY HYLTON: And let's see.

00:23:54.160 --> 00:23:56.415
I think we need--

00:23:56.415 --> 00:23:58.510
we need a second
microphone, probably.

00:23:58.510 --> 00:23:59.270
Do we have a second microphone?

00:23:59.270 --> 00:23:59.550
OK.

00:23:59.550 --> 00:24:00.030
JOY LISI RANKIN:
Oh, look at that.

00:24:00.030 --> 00:24:01.470
JEREMY HYLTON: And I will--

00:24:01.470 --> 00:24:02.320
I'll get some notes.

00:24:05.250 --> 00:24:07.530
So I prepared a few questions.

00:24:07.530 --> 00:24:11.160
We would, of course, love
to have your questions, too.

00:24:11.160 --> 00:24:13.570
SPEAKER: So let's
see if we can control

00:24:13.570 --> 00:24:15.040
the camera a little bit.

00:24:20.930 --> 00:24:23.260
JEREMY HYLTON: I'll start
with a couple of mine

00:24:23.260 --> 00:24:27.570
since I had a little time to
think in advance about them.

00:24:27.570 --> 00:24:32.720
And, you know, if anyone
does want to ask a question,

00:24:32.720 --> 00:24:38.120
just feel free to jump in.

00:24:38.120 --> 00:24:41.210
I guess-- I mean, you
talked a lot about gender

00:24:41.210 --> 00:24:48.590
and you mentioned earlier
how we do, I guess,

00:24:48.590 --> 00:24:52.850
tend to forget about the role of
women in computing in the 60s.

00:24:52.850 --> 00:24:57.770
So, for example, my advisor
got his start working on CTSS

00:24:57.770 --> 00:24:59.530
and then went on
to work on Multics,

00:24:59.530 --> 00:25:00.890
and so I've read a
bunch of those papers,

00:25:00.890 --> 00:25:03.500
but it actually wasn't until I
read your book that I realized

00:25:03.500 --> 00:25:07.130
that I think the second author
on the CTSS paper was a woman,

00:25:07.130 --> 00:25:08.060
Marjorie.

00:25:08.060 --> 00:25:09.750
Everyone used
initials back then,

00:25:09.750 --> 00:25:13.870
so I only ever recognized
her by her last name, right?

00:25:13.870 --> 00:25:18.236
And so yeah, that was a
little invisible to me.

00:25:18.236 --> 00:25:19.610
You talk about
how it's connected

00:25:19.610 --> 00:25:25.010
to people I feel sometimes
like society and culture has

00:25:25.010 --> 00:25:27.980
moved farther ahead than
maybe computing has, right?

00:25:27.980 --> 00:25:31.460
So we have, I
think, more problems

00:25:31.460 --> 00:25:36.440
with equity in the industry
than we do in society at large.

00:25:36.440 --> 00:25:40.370
Like, I wonder if you think
that's fair or, you know,

00:25:40.370 --> 00:25:41.900
if there's a reason
for that, right?

00:25:41.900 --> 00:25:44.270
Is it that somehow,
computing didn't keep up

00:25:44.270 --> 00:25:45.650
with the rest of society?

00:25:45.650 --> 00:25:48.520
Usually, you think of
computing being ahead, right?

00:25:48.520 --> 00:25:50.900
JOY LISI RANKIN:
Yeah, I actually--

00:25:50.900 --> 00:25:53.640
I feel like what I'm about
to say sounds pessimistic,

00:25:53.640 --> 00:25:56.180
but I think right now at
this moment, in some ways--

00:25:56.180 --> 00:25:59.210
in this cultural
moment-- society

00:25:59.210 --> 00:26:03.800
is maybe about where
sort of technology is

00:26:03.800 --> 00:26:07.760
and that they're inseparable.

00:26:07.760 --> 00:26:10.160
I was listening to a
podcast the other day,

00:26:10.160 --> 00:26:13.970
and one of the speakers
said that in technology,

00:26:13.970 --> 00:26:17.840
and STEM more broadly, there's
not a pipeline problem.

00:26:17.840 --> 00:26:20.630
It's not about getting women
in because there have always

00:26:20.630 --> 00:26:24.320
been women doing science and
technology and computing.

00:26:24.320 --> 00:26:25.880
It's a gender problem.

00:26:25.880 --> 00:26:30.710
It's sort of how we think about
what is normal and expected

00:26:30.710 --> 00:26:34.400
of men, of women, not
just in culture broadly,

00:26:34.400 --> 00:26:35.930
but in particular cases.

00:26:35.930 --> 00:26:38.510
Part of why I love
the Dartmouth case

00:26:38.510 --> 00:26:43.580
is because you really see this
creation of masculine computing

00:26:43.580 --> 00:26:46.940
around football, around
playing computing pranks,

00:26:46.940 --> 00:26:49.700
around the fact that
there are women working

00:26:49.700 --> 00:26:52.700
in all sorts of ways in tech.

00:26:52.700 --> 00:26:54.800
And still, the
college president is

00:26:54.800 --> 00:26:57.960
talking about women doing
their grocery shopping.

00:26:57.960 --> 00:27:03.950
So I think there is work to
be done in tech and in society

00:27:03.950 --> 00:27:09.520
and really thinking about
the kinds of cultures

00:27:09.520 --> 00:27:12.850
or micro cultures, maybe,
corporate cultures, and how

00:27:12.850 --> 00:27:16.190
they both mirror what's
happening in society,

00:27:16.190 --> 00:27:20.800
as well as how they might be a
room to sort of steer and shift

00:27:20.800 --> 00:27:24.130
changes in society, as well.

00:27:24.130 --> 00:27:26.140
JEREMY HYLTON: Yeah.

00:27:26.140 --> 00:27:31.270
Maybe also mention as
set up to this question,

00:27:31.270 --> 00:27:34.080
the book talks about more
than Dartmouth and BASIC.

00:27:34.080 --> 00:27:35.040
JOY LISI RANKIN: Yes.

00:27:35.040 --> 00:27:36.498
JEREMY HYLTON: And
I thought that's

00:27:36.498 --> 00:27:39.216
really fascinating to see sort
of the range of early computer

00:27:39.216 --> 00:27:40.590
networks and what
they were like.

00:27:40.590 --> 00:27:42.300
So there was a--

00:27:42.300 --> 00:27:44.230
there's this great
network that Illinois

00:27:44.230 --> 00:27:46.540
built around a computer
with like a plasma

00:27:46.540 --> 00:27:47.710
display and a touch screen.

00:27:50.240 --> 00:27:51.400
JOY LISI RANKIN: By 1975.

00:27:51.400 --> 00:27:51.700
JEREMY HYLTON: Right.

00:27:51.700 --> 00:27:52.199
Yeah.

00:27:52.199 --> 00:27:54.740
Which was really impressive.

00:27:54.740 --> 00:27:59.530
And a network in
Minnesota that was

00:27:59.530 --> 00:28:04.730
run by the state for all
the schools in the state.

00:28:04.730 --> 00:28:11.410
And I guess I was curious
to what extent did you

00:28:11.410 --> 00:28:14.784
explore how maybe the gender
roles were different in some

00:28:14.784 --> 00:28:15.450
of those places?

00:28:15.450 --> 00:28:18.790
I know when you talk about
Illinois in the book,

00:28:18.790 --> 00:28:22.000
the one example you use of sort
of the teaching software that's

00:28:22.000 --> 00:28:25.360
developed was a woman
developing coursework

00:28:25.360 --> 00:28:27.370
for nursing students.

00:28:27.370 --> 00:28:29.230
I guess she happened
to be the husband

00:28:29.230 --> 00:28:31.960
of the creator of the partner
of the creator of the system.

00:28:31.960 --> 00:28:34.540
But was that different
by virtue of the nature

00:28:34.540 --> 00:28:39.310
of what it was used for in the
student population, or not?

00:28:39.310 --> 00:28:41.200
JOY LISI RANKIN: I
think it's the same sort

00:28:41.200 --> 00:28:44.140
of unusual juxtaposition
of the fact

00:28:44.140 --> 00:28:47.860
that-- so this network centered
at the University of Illinois

00:28:47.860 --> 00:28:51.460
called PLATO, originally, it
starts as a network that's

00:28:51.460 --> 00:28:56.080
focused on education, like
computer assisted instruction.

00:28:56.080 --> 00:28:59.170
And so in trying to
develop this system,

00:28:59.170 --> 00:29:02.230
they're working with
men and women experts

00:29:02.230 --> 00:29:04.190
from all sorts of
different fields.

00:29:04.190 --> 00:29:06.100
And as a result, you
end up with people

00:29:06.100 --> 00:29:08.890
like Maryann Bitzer,
who is creating

00:29:08.890 --> 00:29:11.650
nursing education for her
students, but as a result,

00:29:11.650 --> 00:29:15.460
also becoming she is
doing technological work.

00:29:15.460 --> 00:29:17.500
There's another
woman, Valarie Lamont,

00:29:17.500 --> 00:29:19.840
who actually creates
a program that's

00:29:19.840 --> 00:29:24.340
related to environmental
pollution in Urbana-Champaign.

00:29:24.340 --> 00:29:26.080
And again, she's a
political scientist,

00:29:26.080 --> 00:29:31.380
but doing this technological
communicative work.

00:29:31.380 --> 00:29:33.520
And so I think what
we see with PLATO--

00:29:33.520 --> 00:29:35.850
and I talk about this a bit
in the book-- is that there

00:29:35.850 --> 00:29:40.360
is a space for these women
to sort of do things that

00:29:40.360 --> 00:29:45.340
are defying the sort of cultural
norm, but at the same time,

00:29:45.340 --> 00:29:49.480
the network itself and the way
that it's spoken about more

00:29:49.480 --> 00:29:53.980
broadly in society sort of does
things like reinforce gender

00:29:53.980 --> 00:29:54.550
norms.

00:29:54.550 --> 00:29:57.820
So, for instance, it's
an amazing resource.

00:29:57.820 --> 00:30:00.850
There are four years of
their notes files, which

00:30:00.850 --> 00:30:04.750
were like their digital bulletin
board, and they preserved them.

00:30:04.750 --> 00:30:06.550
And you can go
online and sort of

00:30:06.550 --> 00:30:09.370
read through four years
of people's interactions

00:30:09.370 --> 00:30:11.500
on a message board.

00:30:11.500 --> 00:30:14.260
And those clearly
show sort of women

00:30:14.260 --> 00:30:15.760
complaining about harassment.

00:30:15.760 --> 00:30:18.340
Like, women who are working
on the system who are experts,

00:30:18.340 --> 00:30:20.530
they're treated
differently than the men.

00:30:20.530 --> 00:30:23.770
So it's this push and
pull, this tension

00:30:23.770 --> 00:30:29.980
of, ah, we've got a
space for women to do

00:30:29.980 --> 00:30:33.220
the technological work,
but it's not recognized,

00:30:33.220 --> 00:30:37.090
and they're often, because
of the culture, sort of--

00:30:37.090 --> 00:30:38.320
not even sort of, they are.

00:30:38.320 --> 00:30:39.160
They're harassed.

00:30:39.160 --> 00:30:40.390
They are marginalized.

00:30:40.390 --> 00:30:41.495
They're not recognized.

00:30:41.495 --> 00:30:42.370
JEREMY HYLTON: Right.

00:30:42.370 --> 00:30:46.450
Yeah, I guess the kind
of mix of aggressions

00:30:46.450 --> 00:30:50.980
and microaggressions that
I know my colleagues here

00:30:50.980 --> 00:30:54.220
face when we're
developing software today.

00:30:54.220 --> 00:30:59.050
So I guess, yeah, it turned
out to be a little sad

00:30:59.050 --> 00:31:03.220
that the same sort
of gender stereotypes

00:31:03.220 --> 00:31:04.870
continue to play out today.

00:31:04.870 --> 00:31:07.690
You know, I think we have
more awareness today,

00:31:07.690 --> 00:31:10.360
so maybe we can do better, but
I don't know that we're actually

00:31:10.360 --> 00:31:11.830
doing a lot better yet.

00:31:11.830 --> 00:31:13.913
JOY LISI RANKIN: I don't
mean-- you know, today, I

00:31:13.913 --> 00:31:17.200
think it sort of plays out
in media representations

00:31:17.200 --> 00:31:19.240
of-- and I talk about
this a bit in my book--

00:31:19.240 --> 00:31:21.020
the sort of Silicon
Valley mythology

00:31:21.020 --> 00:31:23.600
or the sort of idea
that like all technology

00:31:23.600 --> 00:31:26.290
is created by a few white guys.

00:31:26.290 --> 00:31:30.100
And everyone in this room,
I'm sure everyone in Google,

00:31:30.100 --> 00:31:31.900
knows that's not the case.

00:31:31.900 --> 00:31:35.890
You're all living and
breathing evidence of that.

00:31:35.890 --> 00:31:40.210
But still, sort of I
think in media coverage,

00:31:40.210 --> 00:31:43.660
popular culture, it's a
still a different idea,

00:31:43.660 --> 00:31:46.720
and it's one that kind
of privileges, you know,

00:31:46.720 --> 00:31:48.640
pale males.

00:31:48.640 --> 00:31:49.650
JEREMY HYLTON: Yeah.

00:31:49.650 --> 00:31:51.820
Yeah.

00:31:51.820 --> 00:31:54.190
I also wanted to
ask a little bit

00:31:54.190 --> 00:31:58.360
about this notion of
computing citizenship

00:31:58.360 --> 00:32:02.230
that you talk about, which I
thought was pretty interesting.

00:32:02.230 --> 00:32:05.820
And it played out in
all of the places.

00:32:05.820 --> 00:32:08.320
I guess at Dartmouth, it was
about being a good user, right,

00:32:08.320 --> 00:32:11.560
and sharing things, but
not using too much space

00:32:11.560 --> 00:32:14.350
for your programs that
were not worth sharing.

00:32:14.350 --> 00:32:18.550
And you sort of contrast
that with the citizenship

00:32:18.550 --> 00:32:21.900
and community with sort
of a consumer culture

00:32:21.900 --> 00:32:23.980
that maybe came
with a PC, right?

00:32:23.980 --> 00:32:26.200
Now, we want to sell
a PC into your home.

00:32:26.200 --> 00:32:29.080
It's a thing you buy, and it
comes with software on it,

00:32:29.080 --> 00:32:31.540
but it doesn't have
the same sharing.

00:32:31.540 --> 00:32:33.957
And maybe you could say a
little bit more about that idea.

00:32:33.957 --> 00:32:36.456
JOY LISI RANKIN: So when I was
on the journey of researching

00:32:36.456 --> 00:32:38.080
and writing this
book, I really--

00:32:38.080 --> 00:32:39.700
sort of thinking
from the user up.

00:32:39.700 --> 00:32:43.210
For a while, I struggled
because I was like, oh.

00:32:43.210 --> 00:32:45.100
Very quickly, it
became clear to me,

00:32:45.100 --> 00:32:46.450
like, these aren't just users.

00:32:46.450 --> 00:32:47.010
They are.

00:32:47.010 --> 00:32:48.610
They're writing software.

00:32:48.610 --> 00:32:49.930
They're creating games.

00:32:49.930 --> 00:32:53.830
They're figuring out how to do
social networking in the 1960s.

00:32:53.830 --> 00:32:56.920
And I debated, like, am I
going to call them authors?

00:32:56.920 --> 00:32:58.480
You know, they're writing.

00:32:58.480 --> 00:32:59.110
They're coding.

00:32:59.110 --> 00:33:00.790
Are they programmers?

00:33:00.790 --> 00:33:01.690
Are they makers?

00:33:01.690 --> 00:33:04.030
And none of that seemed right.

00:33:04.030 --> 00:33:06.470
And over the course
of my writing,

00:33:06.470 --> 00:33:09.280
I realized, well,
what's essential is

00:33:09.280 --> 00:33:11.950
they're all part of
these communities,

00:33:11.950 --> 00:33:17.110
and those communities, they're
focused on collaboration.

00:33:17.110 --> 00:33:20.110
And sort of at their
most ideal, there

00:33:20.110 --> 00:33:22.570
is this idea of computing
for the public good,

00:33:22.570 --> 00:33:25.150
computing as a
collective resource.

00:33:25.150 --> 00:33:29.560
And so I thought of the idea
of computing citizenship

00:33:29.560 --> 00:33:33.190
as a way to say they are
citizens of these computing

00:33:33.190 --> 00:33:34.000
communities.

00:33:34.000 --> 00:33:35.620
They are participating.

00:33:35.620 --> 00:33:40.540
And it's not a political
scientist's idea

00:33:40.540 --> 00:33:42.430
of citizenship,
although, in some ways,

00:33:42.430 --> 00:33:46.170
they're closely related in
terms of who is recognized,

00:33:46.170 --> 00:33:50.800
who is supported, who gets
a voice in the network

00:33:50.800 --> 00:33:52.490
and on the network.

00:33:52.490 --> 00:33:55.900
And so when I started thinking
about citizenship as a way

00:33:55.900 --> 00:33:58.150
to talk about their
participation,

00:33:58.150 --> 00:34:01.420
and also, I should
say, to emphasize

00:34:01.420 --> 00:34:04.390
all of the public
institutions that underwrote

00:34:04.390 --> 00:34:05.920
all of this networking, right?

00:34:05.920 --> 00:34:08.830
They are local and state
and federal governments.

00:34:08.830 --> 00:34:11.650
The National Science
Foundation, colleges,

00:34:11.650 --> 00:34:15.250
and public universities,
they're all pouring money

00:34:15.250 --> 00:34:16.940
into these computing networks.

00:34:16.940 --> 00:34:21.070
So here's another way that sort
of citizenship comes into play.

00:34:21.070 --> 00:34:25.650
Well, over the
course of the 1980s,

00:34:25.650 --> 00:34:27.630
and really, with
the introduction

00:34:27.630 --> 00:34:31.050
of personal computers
or home computers,

00:34:31.050 --> 00:34:33.000
no more is there
this idea of like,

00:34:33.000 --> 00:34:34.710
oh, computing might
be something we

00:34:34.710 --> 00:34:38.310
have as a public utility, which
there was very much this idea

00:34:38.310 --> 00:34:41.040
that there was going to
be a public computing

00:34:41.040 --> 00:34:43.739
utility in the 1960s or 1970s.

00:34:43.739 --> 00:34:46.316
And instead, it became
this idea of no,

00:34:46.316 --> 00:34:47.940
everyone is going to
go out and they're

00:34:47.940 --> 00:34:49.050
going to buy a computer.

00:34:49.050 --> 00:34:50.300
They're going to buy a device.

00:34:50.300 --> 00:34:51.690
They're going to buy software.

00:34:51.690 --> 00:34:55.170
Whereas in the 60s and 70s, the
communities I'm talking about

00:34:55.170 --> 00:34:57.060
are saying, "No, no,
we share our software.

00:34:57.060 --> 00:34:58.250
We share our BASIC.

00:34:58.250 --> 00:35:02.550
Like, BASIC is freely
available to whoever wants it.

00:35:02.550 --> 00:35:04.860
We publish BASIC programs.

00:35:04.860 --> 00:35:06.330
We circulate them."

00:35:06.330 --> 00:35:09.390
And so to me, I saw--

00:35:09.390 --> 00:35:12.120
sort of it's a foreclosure
of opportunity,

00:35:12.120 --> 00:35:14.460
this shift from
saying, "Computing

00:35:14.460 --> 00:35:15.910
could be for the public good.

00:35:15.910 --> 00:35:17.850
It could be a public utility.

00:35:17.850 --> 00:35:20.700
It could be regulated
by the government

00:35:20.700 --> 00:35:24.660
for better or perhaps for
worse" to this idea of "No, no.

00:35:24.660 --> 00:35:27.720
Everyone should go buy a
computer, buy a device,

00:35:27.720 --> 00:35:28.980
buy some software."

00:35:28.980 --> 00:35:30.812
That limits things.

00:35:30.812 --> 00:35:34.220
JEREMY HYLTON: Right,
it certainly makes

00:35:34.220 --> 00:35:37.310
the distribution of
computing less equal, right?

00:35:37.310 --> 00:35:39.770
Because, you know, you
need to have the affluence

00:35:39.770 --> 00:35:42.005
to afford to go buy
a computer, right?

00:35:42.005 --> 00:35:42.920
JOY LISI RANKIN: Yes.

00:35:42.920 --> 00:35:45.336
JEREMY HYLTON: And if you want
to connect to someone else,

00:35:45.336 --> 00:35:48.290
you have to pay yourself
for the network access.

00:35:48.290 --> 00:35:53.000
I was struck by how expensive
it was to be part of a network,

00:35:53.000 --> 00:35:55.911
and that was actually
mostly telephone charges.

00:35:55.911 --> 00:35:58.160
And you talked about how the
state of Minnesota, like,

00:35:58.160 --> 00:35:59.618
pooled its resources,
and they were

00:35:59.618 --> 00:36:02.900
able to negotiate with the
telephone company for better

00:36:02.900 --> 00:36:05.750
rates and better technology.

00:36:05.750 --> 00:36:09.120
JOY LISI RANKIN: And, I mean,
I love the Minnesota case.

00:36:09.120 --> 00:36:11.900
So Minnesota has a
statewide computing network

00:36:11.900 --> 00:36:16.280
that's built sort of
between 1965 and 1975.

00:36:16.280 --> 00:36:18.290
And they're very
explicit in saying,

00:36:18.290 --> 00:36:22.250
we're going to build this
as a public institution

00:36:22.250 --> 00:36:27.050
to provide equality of access
not just for people in the Twin

00:36:27.050 --> 00:36:29.960
Cities suburbs, but for
people in rural areas,

00:36:29.960 --> 00:36:33.980
for students who are
in less affluent areas.

00:36:33.980 --> 00:36:37.430
And they use their weight
as the state government

00:36:37.430 --> 00:36:41.030
to do things like negotiate
better phone line rates

00:36:41.030 --> 00:36:43.730
and negotiate lower
costs so that it really

00:36:43.730 --> 00:36:46.317
does become a public good.

00:36:46.317 --> 00:36:47.150
JEREMY HYLTON: Yeah.

00:36:47.150 --> 00:36:53.820
So networking is more
widely available today.

00:36:53.820 --> 00:36:55.610
JOY LISI RANKIN: Yes, it is.

00:36:55.610 --> 00:36:57.590
JEREMY HYLTON: And the
cost of the devices

00:36:57.590 --> 00:37:02.960
is lower, although a high end
smartphone is probably $1,000

00:37:02.960 --> 00:37:07.920
these days, so it's still
not very equal access.

00:37:07.920 --> 00:37:11.090
But, I mean, do you think
that notion of citizenship

00:37:11.090 --> 00:37:14.060
still is one we should
be thinking about today?

00:37:14.060 --> 00:37:17.810
I mean, right now, you are a
consumer of social networking

00:37:17.810 --> 00:37:22.140
apps, and you don't have a whole
lot of control over it, right?

00:37:22.140 --> 00:37:24.900
You know, would it be better
if there were more programmers,

00:37:24.900 --> 00:37:25.580
more citizens?

00:37:25.580 --> 00:37:27.260
JOY LISI RANKIN:
I absolutely think

00:37:27.260 --> 00:37:28.880
citizenship is
something we should

00:37:28.880 --> 00:37:33.550
be thinking about precisely
because we are consumers.

00:37:33.550 --> 00:37:37.470
I mean, so much of the
social media that we consume

00:37:37.470 --> 00:37:41.780
or so many of the apps
are made by corporations

00:37:41.780 --> 00:37:45.980
that are out to make money,
not out to do the public good.

00:37:45.980 --> 00:37:49.910
And those aren't necessarily
mutually exclusive,

00:37:49.910 --> 00:37:54.800
but it's a reminder that
if you have a company that

00:37:54.800 --> 00:37:57.110
is looking after
its bottom line,

00:37:57.110 --> 00:38:01.640
that's the bottom line, not the
people who are using the apps

00:38:01.640 --> 00:38:06.420
or using the platform
or whatever it is.

00:38:06.420 --> 00:38:11.330
So I absolutely think
competing citizenship

00:38:11.330 --> 00:38:14.630
is relevant to sort
of reimagine where

00:38:14.630 --> 00:38:19.820
do we want our very much
digital networked world to go.

00:38:19.820 --> 00:38:24.770
What would the conversations
around what we hope look like,

00:38:24.770 --> 00:38:27.950
and how can the past
inform us sort of

00:38:27.950 --> 00:38:30.380
in what we might want to
change and what we really

00:38:30.380 --> 00:38:35.487
liked about what things looked
like in the 1960s and 1970s?

00:38:35.487 --> 00:38:36.320
JEREMY HYLTON: Yeah.

00:38:36.320 --> 00:38:39.280
I tried to think what
version of programming

00:38:39.280 --> 00:38:42.860
is available to kids today.

00:38:42.860 --> 00:38:45.430
I've wondered, like
Minecraft is probably

00:38:45.430 --> 00:38:48.760
the closest they get to
creating things, right?

00:38:48.760 --> 00:38:52.450
I don't know how widespread
otherwise for kids

00:38:52.450 --> 00:38:56.057
actual creating of things is.

00:38:56.057 --> 00:38:57.640
JOY LISI RANKIN: I
mean, I think there

00:38:57.640 --> 00:39:03.790
are some pushes to
sort of get kids back

00:39:03.790 --> 00:39:06.020
into coding and programming.

00:39:06.020 --> 00:39:06.910
JEREMY HYLTON: Yeah.

00:39:06.910 --> 00:39:08.326
JOY LISI RANKIN:
But I think, too,

00:39:08.326 --> 00:39:11.560
with anything like that, there's
still the same caution of,

00:39:11.560 --> 00:39:15.460
well, how are our sort
of societal expectations

00:39:15.460 --> 00:39:19.180
around do we encourage
the boys more than girls,

00:39:19.180 --> 00:39:23.170
or sort of how do these things
play out at the same time?

00:39:23.170 --> 00:39:25.060
JEREMY HYLTON: Yeah.

00:39:25.060 --> 00:39:27.130
I have one more
question I thought

00:39:27.130 --> 00:39:28.990
I would ask, and then
try to open it up

00:39:28.990 --> 00:39:30.620
to audience questions.

00:39:30.620 --> 00:39:32.770
So you talked about a
lot of different sort

00:39:32.770 --> 00:39:36.437
of early computing networks
and systems and users,

00:39:36.437 --> 00:39:38.770
and it seemed like you had a
call at the end of the book

00:39:38.770 --> 00:39:41.350
that your list was
not exhaustive, right?

00:39:41.350 --> 00:39:43.360
It's not like you--

00:39:43.360 --> 00:39:46.804
like, you have other areas
that you've heard about

00:39:46.804 --> 00:39:48.970
or you think would be really
interesting for someone

00:39:48.970 --> 00:39:50.045
to investigate more.

00:39:50.045 --> 00:39:50.920
JOY LISI RANKIN: Yes.

00:39:50.920 --> 00:39:51.420
Yes.

00:39:51.420 --> 00:39:54.790
So other people,
other historians,

00:39:54.790 --> 00:39:57.220
other communications
and media scholars,

00:39:57.220 --> 00:40:02.560
whoever, my book basically
ends right around 1980.

00:40:02.560 --> 00:40:07.270
And I think sort of the
1980s into the early 1990s

00:40:07.270 --> 00:40:09.700
is another rich,
amazing time to say,

00:40:09.700 --> 00:40:12.220
all these people were
buying personal computers

00:40:12.220 --> 00:40:15.310
and there were so many
computers in schools.

00:40:15.310 --> 00:40:18.160
If we wrote those
histories from the user up,

00:40:18.160 --> 00:40:19.520
what would they look like?

00:40:19.520 --> 00:40:25.090
Similarly, we know a
bit about the WELL.

00:40:25.090 --> 00:40:26.980
Well, there was a
New York City version

00:40:26.980 --> 00:40:31.510
of that called Echo, East
Coast Hang Out, which

00:40:31.510 --> 00:40:37.330
was a huge online bulletin board
system during the 80s and 90s,

00:40:37.330 --> 00:40:39.550
another great source to look at.

00:40:39.550 --> 00:40:41.200
Like, what's happening?

00:40:41.200 --> 00:40:44.800
Sort of as more
people are networked,

00:40:44.800 --> 00:40:49.320
what does that look like?

00:40:49.320 --> 00:40:51.780
Yeah, there's, I
think, a whole world

00:40:51.780 --> 00:40:54.120
and a whole case of
the 80s and early 90s,

00:40:54.120 --> 00:40:55.680
and not just in
the United States.

00:40:55.680 --> 00:40:58.089
This is the other thing
that I am excited about.

00:40:58.089 --> 00:40:59.880
Some of my colleagues
are writing right now

00:40:59.880 --> 00:41:03.180
histories of networks
in the Soviet Union

00:41:03.180 --> 00:41:06.580
or networks in China,
these kinds of things.

00:41:06.580 --> 00:41:09.600
And I think all of those are
important to help us understand

00:41:09.600 --> 00:41:12.800
that the United States
isn't necessarily

00:41:12.800 --> 00:41:16.500
the isolated, unparalleled case,
but what are the similarities

00:41:16.500 --> 00:41:17.857
and differences?

00:41:17.857 --> 00:41:18.690
JEREMY HYLTON: Yeah.

00:41:18.690 --> 00:41:19.190
All right.

00:41:19.190 --> 00:41:19.700
Thanks.

00:41:19.700 --> 00:41:23.070
So anyone else want
to ask some questions?

00:41:23.070 --> 00:41:31.670
We can [INAUDIBLE]

00:41:31.670 --> 00:41:32.340
AUDIENCE: Hi.

00:41:32.340 --> 00:41:33.740
Thank you for your time.

00:41:33.740 --> 00:41:35.865
I was wondering if you
could talk a little bit more

00:41:35.865 --> 00:41:37.590
about where this
particular work fits

00:41:37.590 --> 00:41:40.950
into your professional
interests or your personal story

00:41:40.950 --> 00:41:45.735
or sort of how you came to be
to want to write this book and--

00:41:45.735 --> 00:41:46.860
JOY LISI RANKIN: Thank you.

00:41:46.860 --> 00:41:48.360
AUDIENCE: --put these
things together right now.

00:41:48.360 --> 00:41:48.990
JOY LISI RANKIN: Yeah.

00:41:48.990 --> 00:41:50.140
I love that question.

00:41:50.140 --> 00:41:54.510
So I was actually telling
Jeremy this a little bit before.

00:41:54.510 --> 00:41:57.510
I am maybe an atypical
historian in that I

00:41:57.510 --> 00:41:59.310
was a math major at Dartmouth.

00:41:59.310 --> 00:42:00.990
I was also a history major.

00:42:00.990 --> 00:42:05.660
And after college, I
worked for almost a decade

00:42:05.660 --> 00:42:09.060
at companies that
involve technology

00:42:09.060 --> 00:42:12.000
and education or
launching programs that

00:42:12.000 --> 00:42:14.760
had to do with education
and technology.

00:42:14.760 --> 00:42:18.120
And every single time,
every single place, we'd

00:42:18.120 --> 00:42:20.700
sort of roll something out
and watch what-- you know,

00:42:20.700 --> 00:42:21.780
we called them the users.

00:42:21.780 --> 00:42:23.280
Like, what are they going to do?

00:42:23.280 --> 00:42:25.620
And it was never
what we expected

00:42:25.620 --> 00:42:28.590
and it was always
creative and surprising,

00:42:28.590 --> 00:42:32.400
and sometimes, infuriating.

00:42:32.400 --> 00:42:34.290
And it was something
that at the time,

00:42:34.290 --> 00:42:37.590
I was like, this is not
at all what I expected.

00:42:37.590 --> 00:42:40.070
People are doing things
very differently,

00:42:40.070 --> 00:42:45.390
and in amazing often social
ways that we hadn't planned on.

00:42:49.130 --> 00:42:51.350
And so when I finally--

00:42:51.350 --> 00:42:54.320
sort of my path
meandered and I said,

00:42:54.320 --> 00:42:56.090
it's time for me to
go to grad school.

00:42:56.090 --> 00:42:58.400
You're going to pay me to
do this, to read and write.

00:42:58.400 --> 00:42:59.210
Yes, please.

00:42:59.210 --> 00:43:00.560
Sign me up.

00:43:00.560 --> 00:43:04.160
And I studied the history
of science and technology.

00:43:04.160 --> 00:43:07.820
And when it came time to pick a
project or look for a project,

00:43:07.820 --> 00:43:10.850
I was reminded that as an
undergraduate at Dartmouth--

00:43:10.850 --> 00:43:12.710
I was an undergrad at
Dartmouth in the 90s--

00:43:12.710 --> 00:43:16.070
we had this vibrant
computing culture of email.

00:43:16.070 --> 00:43:17.890
I never picked up the phone.

00:43:17.890 --> 00:43:19.890
I had other friends who
were like, what's email?

00:43:19.890 --> 00:43:21.410
And I was like, email is great.

00:43:21.410 --> 00:43:23.000
Like, I mean, right?

00:43:23.000 --> 00:43:26.990
How can I tell you what
email is if you don't know?

00:43:26.990 --> 00:43:29.450
And I remembered-- sort
of from being there,

00:43:29.450 --> 00:43:31.730
I knew that Dartmouth
had more of a history.

00:43:31.730 --> 00:43:33.980
So that was sort
of the first place

00:43:33.980 --> 00:43:35.750
that I started when
I said, I think

00:43:35.750 --> 00:43:38.780
I want to write a very
different history than most.

00:43:38.780 --> 00:43:41.030
Even among professional
historians,

00:43:41.030 --> 00:43:42.560
most computing
histories are very

00:43:42.560 --> 00:43:46.890
much like men and machines,
and that's changing recently,

00:43:46.890 --> 00:43:50.290
but it wasn't the history
that I wanted to write.

00:43:50.290 --> 00:43:52.880
And the final, the third
thing-- and I sort of

00:43:52.880 --> 00:43:55.150
recommend everyone in
life go check this out.

00:43:55.150 --> 00:43:57.530
There's a great book
called "Computer Lib:

00:43:57.530 --> 00:44:00.620
You Can and Must
Understand Computers Now,"

00:44:00.620 --> 00:44:01.970
written by Ted Nelson.

00:44:01.970 --> 00:44:05.660
I think he self-publishes
it in 1974.

00:44:05.660 --> 00:44:09.620
And in the first few pages
of that book, he has a map.

00:44:09.620 --> 00:44:10.760
It's a hand drawn map.

00:44:10.760 --> 00:44:14.450
And it's like, where
it's at, computing USA.

00:44:14.450 --> 00:44:17.120
And I looked at that
map, and yeah, there's

00:44:17.120 --> 00:44:19.490
MIT and the 128 area.

00:44:19.490 --> 00:44:22.550
And of course, there's like the
Bay Area and a few other places

00:44:22.550 --> 00:44:23.320
that I knew.

00:44:23.320 --> 00:44:25.550
But then, I saw
Dartmouth and I thought,

00:44:25.550 --> 00:44:27.980
haha, I'm onto something here.

00:44:27.980 --> 00:44:31.730
And I saw Minnesota
and I saw PLATO,

00:44:31.730 --> 00:44:33.590
and those were sort of
points for me to say,

00:44:33.590 --> 00:44:37.028
what's going on there?

00:44:37.028 --> 00:44:37.528
Yeah.

00:44:40.870 --> 00:44:44.470
AUDIENCE: So you had
some really interesting

00:44:44.470 --> 00:44:48.340
looks back at history and
how gender and computing

00:44:48.340 --> 00:44:49.462
interplayed.

00:44:49.462 --> 00:44:50.920
And you touched on
it a little bit,

00:44:50.920 --> 00:44:53.260
but I was curious if you had
more information about how

00:44:53.260 --> 00:44:55.720
race and computing
interplayed back then

00:44:55.720 --> 00:44:57.611
and how that's
changed over time.

00:44:57.611 --> 00:44:58.735
JOY LISI RANKIN: Thank you.

00:44:58.735 --> 00:45:00.110
That's a great
question, and it's

00:45:00.110 --> 00:45:02.560
something I talk a little
bit about in the book.

00:45:02.560 --> 00:45:05.930
For example-- and I'll use
the Dartmouth case again.

00:45:05.930 --> 00:45:08.200
So at this time, Dartmouth--

00:45:08.200 --> 00:45:11.920
in the early 60s, Dartmouth
has almost exclusively white

00:45:11.920 --> 00:45:12.910
students.

00:45:12.910 --> 00:45:17.340
And by the end of the 1960s,
as Kemeny becomes president,

00:45:17.340 --> 00:45:20.320
they're working hard to
actively sort of recruit

00:45:20.320 --> 00:45:24.940
and diversify Native
American students as well

00:45:24.940 --> 00:45:26.650
as black students.

00:45:26.650 --> 00:45:31.810
And there's this moment in time
where as all of these students

00:45:31.810 --> 00:45:35.710
are coming in, you can
see in the newsletter

00:45:35.710 --> 00:45:38.320
that they're giving to all the
first year students they're

00:45:38.320 --> 00:45:41.740
promoting sort of
homegrown programs

00:45:41.740 --> 00:45:46.030
about race that had been
developed to sort of combat

00:45:46.030 --> 00:45:52.150
racism and to really sort
of make people aware of some

00:45:52.150 --> 00:45:54.490
of the problems with racism.

00:45:54.490 --> 00:45:57.930
But those programs, which had
been developed in anthropology,

00:45:57.930 --> 00:45:59.950
they sort of came with
the warnings that said,

00:45:59.950 --> 00:46:03.820
like, we don't recommend
the use of these programs

00:46:03.820 --> 00:46:07.000
outside of classes for
advanced undergraduates

00:46:07.000 --> 00:46:09.940
because they could
be misunderstood

00:46:09.940 --> 00:46:11.680
if used the wrong way.

00:46:11.680 --> 00:46:14.510
And meanwhile, the computing
center is like, "Hey,

00:46:14.510 --> 00:46:15.740
all you first years.

00:46:15.740 --> 00:46:17.620
Check out these programs."

00:46:17.620 --> 00:46:20.830
And they're, I think
with all good intentions,

00:46:20.830 --> 00:46:24.100
inadvertently
reinscribing the racism,

00:46:24.100 --> 00:46:27.910
as well as calling attention to
the otherness of all of those

00:46:27.910 --> 00:46:32.200
students who aren't
white men and really--

00:46:32.200 --> 00:46:35.740
and this also sort of
needs to be emphasized--

00:46:35.740 --> 00:46:40.290
doing work of saying, oh, well,
computing is something we--

00:46:40.290 --> 00:46:43.960
race is something we can
study with computing.

00:46:43.960 --> 00:46:46.480
Therefore, it's
separate from computing,

00:46:46.480 --> 00:46:49.450
or it seems separate
from computing,

00:46:49.450 --> 00:46:51.100
when we know that's
not the case,

00:46:51.100 --> 00:46:53.500
when race, just
as much as gender,

00:46:53.500 --> 00:46:56.950
is part of the social
world of technology.

00:47:05.016 --> 00:47:06.640
AUDIENCE: It's not
a question, but it's

00:47:06.640 --> 00:47:09.580
a little bit of optimism.

00:47:09.580 --> 00:47:13.150
One of my markers for the
relationship between technology

00:47:13.150 --> 00:47:17.480
and society is something
that I learned from Google.

00:47:17.480 --> 00:47:20.830
If you do an image search for
scientist, even up to a year

00:47:20.830 --> 00:47:22.580
ago, it was all pale males.

00:47:22.580 --> 00:47:23.940
And I just did it just now.

00:47:23.940 --> 00:47:26.710
I break my rule about opening
my laptops in a meeting.

00:47:26.710 --> 00:47:28.390
And I checked, and
it's much better now.

00:47:28.390 --> 00:47:30.220
So that's a positive note there.

00:47:30.220 --> 00:47:31.560
Someone's working on it.

00:47:31.560 --> 00:47:33.580
And the power of a
Google image search

00:47:33.580 --> 00:47:37.450
to impact people's view of
a field is very important.

00:47:37.450 --> 00:47:40.257
And so there's a
happy note there.

00:47:40.257 --> 00:47:41.590
JOY LISI RANKIN: No, absolutely.

00:47:41.590 --> 00:47:42.630
There is a happy note.

00:47:42.630 --> 00:47:44.980
I mean, it's sort of--
it cuts both ways, right?

00:47:44.980 --> 00:47:47.200
It's the artificial
intelligence and machine

00:47:47.200 --> 00:47:49.690
learning saying if you asked
for a picture of a doctor,

00:47:49.690 --> 00:47:50.822
you get a man.

00:47:50.822 --> 00:47:53.030
If you ask for a picture of
a nurse, you get a woman.

00:47:53.030 --> 00:47:56.470
But my colleagues
and I and people

00:47:56.470 --> 00:47:59.350
here call attention to those
things and sort of increasingly

00:47:59.350 --> 00:48:02.380
become aware of
the responsibility

00:48:02.380 --> 00:48:03.880
that needs to be inherent.

00:48:03.880 --> 00:48:06.430
And, like I said, the fact
that tech isn't separate.

00:48:06.430 --> 00:48:07.990
Like, you're always
doing what you

00:48:07.990 --> 00:48:11.320
do with people in mind
and to be more mindful

00:48:11.320 --> 00:48:14.500
of who those people are.

00:48:14.500 --> 00:48:16.702
You know, it can
make a difference.

00:48:16.702 --> 00:48:18.305
Yeah.

00:48:18.305 --> 00:48:19.180
AUDIENCE: Baby steps.

00:48:19.180 --> 00:48:20.055
JOY LISI RANKIN: Yes.

00:48:27.600 --> 00:48:30.300
AUDIENCE: I thought it was
interesting you brought up

00:48:30.300 --> 00:48:34.800
the fixation on pipeline
today as, I would call it,

00:48:34.800 --> 00:48:39.389
a scapegoat from, you know,
actually doing something

00:48:39.389 --> 00:48:41.430
about inclusively with
the people who are already

00:48:41.430 --> 00:48:44.140
in the pipeline or
something like that.

00:48:44.140 --> 00:48:46.750
And it strikes me
that that's not

00:48:46.750 --> 00:48:50.710
an excuse you can use
in early computing.

00:48:50.710 --> 00:48:56.260
Do you find that there were
similar scapegoat-y type

00:48:56.260 --> 00:49:01.210
arguments in early computing
that limited diversity

00:49:01.210 --> 00:49:05.410
and inclusion, or
was it more direct

00:49:05.410 --> 00:49:07.900
and stereotyped in
the ways that you saw?

00:49:07.900 --> 00:49:11.110
JOY LISI RANKIN: So I'm going to
summarize a lot of history here

00:49:11.110 --> 00:49:14.230
and the work of my colleagues
to say-- so computers used

00:49:14.230 --> 00:49:17.920
to be a word that meant people.

00:49:17.920 --> 00:49:19.940
Pre World War II,
computers referred

00:49:19.940 --> 00:49:24.880
to people who did mathematical,
arithmetic computations,

00:49:24.880 --> 00:49:27.370
and those people were
most often women.

00:49:27.370 --> 00:49:31.150
And when computing, certainly
in the United States, as well as

00:49:31.150 --> 00:49:32.170
Great Britain--

00:49:32.170 --> 00:49:35.710
sort of when computing
became in the, we'll

00:49:35.710 --> 00:49:38.050
say sort of World War
I, World War II era,

00:49:38.050 --> 00:49:41.800
the work of business
administration office machines,

00:49:41.800 --> 00:49:46.810
people doing computing at that
time were most often women.

00:49:46.810 --> 00:49:50.320
And it's why, in some
ways, it's not surprising

00:49:50.320 --> 00:49:52.900
that there are so many women
doing computing in the United

00:49:52.900 --> 00:49:56.290
States during the
1950s and 1960s.

00:49:56.290 --> 00:50:01.510
What we've seen happen is that
as it was increasingly clear

00:50:01.510 --> 00:50:09.460
that computers would be crucial
to society and valuable work,

00:50:09.460 --> 00:50:13.000
sort of multiple
processes happen whereby

00:50:13.000 --> 00:50:17.620
women were pushed out because
the work became work seen

00:50:17.620 --> 00:50:20.050
as more prestigious,
and therefore,

00:50:20.050 --> 00:50:25.250
as computing professionalized,
became a more masculine field.

00:50:25.250 --> 00:50:27.100
So sort of aptitude
tests were given

00:50:27.100 --> 00:50:29.560
for programmers that
really emphasized

00:50:29.560 --> 00:50:34.240
masculine male
characteristics, for instance.

00:50:34.240 --> 00:50:39.460
So it wasn't sort of--

00:50:39.460 --> 00:50:42.700
in some ways, it was like
a reverse of inclusivity

00:50:42.700 --> 00:50:46.330
where the women were
over time pushed out

00:50:46.330 --> 00:50:48.490
and the cultures
around technology

00:50:48.490 --> 00:50:51.250
became more masculine
in ways that it

00:50:51.250 --> 00:50:54.880
was harder for them to be
in that work in that world.

00:50:57.494 --> 00:50:59.160
JEREMY HYLTON: We
have time for one more

00:50:59.160 --> 00:51:00.390
question, if there is one.

00:51:04.460 --> 00:51:06.431
AUDIENCE: Hi.

00:51:06.431 --> 00:51:08.180
This is related to the
previous questions.

00:51:08.180 --> 00:51:10.160
Can you talk a
little bit about what

00:51:10.160 --> 00:51:15.170
you see the role of
allyship is in closing some

00:51:15.170 --> 00:51:19.010
of these opportunity gaps
for minorities in tech,

00:51:19.010 --> 00:51:21.454
both in the past and maybe
presently, if you have time.

00:51:21.454 --> 00:51:22.370
JOY LISI RANKIN: Yeah.

00:51:22.370 --> 00:51:23.340
Thank you.

00:51:23.340 --> 00:51:33.120
So I think being an ally
is crucial at this moment

00:51:33.120 --> 00:51:40.620
to recognize sort of for all
of the people who are here

00:51:40.620 --> 00:51:42.870
around Google and
other companies

00:51:42.870 --> 00:51:48.480
that we identify as tech
companies to sort of really

00:51:48.480 --> 00:51:52.530
do sort of individual and
communal self-examination

00:51:52.530 --> 00:51:56.280
to say, what are the behaviors
or ideas or our cultures

00:51:56.280 --> 00:52:00.360
in a particular meeting,
on a particular team?

00:52:00.360 --> 00:52:01.455
How might those be--

00:52:04.150 --> 00:52:08.120
how might those be not
just sort of in some ways,

00:52:08.120 --> 00:52:12.640
being harmful to people
who are not like you,

00:52:12.640 --> 00:52:18.160
but also, how might they
be amplifying privilege

00:52:18.160 --> 00:52:29.640
or sort of just creating
more comfort for people

00:52:29.640 --> 00:52:30.810
who are like you?

00:52:30.810 --> 00:52:35.200
So I really do think it's
this ongoing level of saying,

00:52:35.200 --> 00:52:37.650
"OK, I'm here, and I
want to be an ally.

00:52:37.650 --> 00:52:43.320
I want to help support
and hire and work with

00:52:43.320 --> 00:52:47.790
and empower people who
aren't like me to enhance

00:52:47.790 --> 00:52:49.560
more diversity,"
whether it's gender

00:52:49.560 --> 00:52:51.930
or race or sexual identity.

00:52:51.930 --> 00:52:54.450
Sort of all the things.

00:52:54.450 --> 00:52:57.850
But it's really a level
of awareness of both.

00:52:57.850 --> 00:53:00.860
It's that sort of-- it's
the imaginary user, right?

00:53:00.860 --> 00:53:03.750
And not just the imaginary
user of technology,

00:53:03.750 --> 00:53:12.170
but in meetings, in spaces,
in corporate worlds,

00:53:12.170 --> 00:53:15.560
as well as sort of the worlds
created by the technology

00:53:15.560 --> 00:53:16.280
that we used.

00:53:21.210 --> 00:53:23.570
SPEAKER: Thank you very much.

00:53:23.570 --> 00:53:25.900
Again, Dr. Rankin.

00:53:25.900 --> 00:53:26.820
Professor Rankin.

00:53:26.820 --> 00:53:27.792
Joy.

00:53:27.792 --> 00:53:29.250
JOY LISI RANKIN:
Any of those work.

00:53:29.250 --> 00:53:31.702
SPEAKER: And Jeremy
for moderating.

00:53:31.702 --> 00:53:33.660
We are wrapping up our
talk, but for all of you

00:53:33.660 --> 00:53:37.380
here, we have copies of the
book that will be signed.

00:53:37.380 --> 00:53:41.020
So I encourage you to come
and avail yourself of that.

00:53:41.020 --> 00:53:42.090
Thank you very much.

00:53:42.090 --> 00:53:45.140
[APPLAUSE]

