WEBVTT
Kind: captions
Language: en

00:00:01.689 --> 00:00:02.689
[soft music]

00:00:02.689 --> 00:00:09.150
&gt;&gt;Male Presenter: Ever since Descartes, thinkers
have been fascinated with the mind-body dichotomy.

00:00:09.150 --> 00:00:15.860
With the emergence of computers in the middle
of the last century, computer scientists began

00:00:15.860 --> 00:00:22.490
to play with what Fred Brooks called "pure
thought stuff," thus putting our hands directly

00:00:22.490 --> 00:00:25.520
on this dichotomy.

00:00:25.520 --> 00:00:30.960
Like many other immensely powerful, or even
magical, substances, this thought stuff is

00:00:30.960 --> 00:00:39.510
dangerous. The digital pioneers, some of whom
are here, who innocently opened Pandora's

00:00:39.510 --> 00:00:45.230
Box did not realize that in so doing, they
would start a process that would not only

00:00:45.230 --> 00:00:51.680
change the fabric of society, but might even
begin to change the way the next generation--

00:00:51.680 --> 00:00:57.479
the digital natives--actually think. When
I was a child, everyone worried about the

00:00:57.479 --> 00:01:03.010
threat that nuclear weapons presented to human
society as a whole.

00:01:03.010 --> 00:01:08.660
Maybe, in fact, the real threat is our computers.
Who knows? Thirty years ago, Sherry Turkle,

00:01:08.660 --> 00:01:14.580
a sociologist and psychologist, realized that
something important was happening and came

00:01:14.580 --> 00:01:22.860
to MIT to begin studying this phenomenon at
one of its epicenters. Unlike the geeks, she

00:01:22.860 --> 00:01:25.770
did not study the computers themselves.

00:01:25.770 --> 00:01:33.390
But rather, she studied the computer users.
First the digital pioneers and subsequently,

00:01:33.390 --> 00:01:40.310
the first generation of digital natives. During
this epic journey of exploration and discovery,

00:01:40.310 --> 00:01:45.280
she has written numerous books, three of which
are particularly important reports on her

00:01:45.280 --> 00:01:48.030
observations of the new digital world.

00:01:48.030 --> 00:01:54.750
"The Second Self: Computers and the Human
Spirit," in 1984, "Life on the Screen: Identity

00:01:54.750 --> 00:02:01.909
in the Age of the Internet" in 1995 and today's
topic, her newest book published last week,

00:02:01.909 --> 00:02:08.890
"Alone Together: Why We Expect More from Technology
and Less from Each Other." I give you Sherry

00:02:08.890 --> 00:02:09.890
Turkle.

00:02:09.890 --> 00:02:12.310
&gt;&gt;Sherry Turkle: Thank you.

00:02:12.310 --> 00:02:13.310
[applause]

00:02:13.310 --> 00:02:20.160
It's a pleasure to be here and I'll be talking
mostly, of course, about "Alone Together."

00:02:20.160 --> 00:02:30.560
But let me just say a word about this trilogy,
because really this is the third in a, in

00:02:30.560 --> 00:02:34.410
a set. The first book, "The Second Self,"
was about the one-on-one of the psychology

00:02:34.410 --> 00:02:36.870
of why the person-machine psychology is so
compelling.

00:02:36.870 --> 00:02:45.890
And I wrote about how people really achieve
an almost Vulcan mind-meld with a computer

00:02:45.890 --> 00:02:51.520
when they work intensively with it. The psychology
to program, the psychology of really feeling

00:02:51.520 --> 00:02:56.370
at one with your machine and the way the machine
comes to externalize a piece of yourself.

00:02:56.370 --> 00:03:00.870
The line in the book that captured that for
me was the 13-year old girl who said, "When

00:03:00.870 --> 00:03:06.420
you program a computer, you put a little piece
of your mind into the computer's mind and

00:03:06.420 --> 00:03:10.310
you come to see yourself differently." And
that's what gave me the title, "The Second

00:03:10.310 --> 00:03:13.060
Self." And the second book, "Life on the Screen:
Identity in the Age of the Internet," was

00:03:13.060 --> 00:03:17.740
really about how we started to step through
the screen.

00:03:17.740 --> 00:03:24.860
And through avatars, the early virtual worlds,
MOOs, MUDs, the early AOL experiences, we

00:03:24.860 --> 00:03:30.020
really began to learn about what kind of lives
we could live on the screen. What kinds of

00:03:30.020 --> 00:03:36.860
identity play, what kinds of experiences was--.
Being other than ourselves, experimenting

00:03:36.860 --> 00:03:40.170
with self, we could have in this new playful
space.

00:03:40.170 --> 00:03:44.950
And I was very positive about that. I felt
it was a very exciting, new thing. And I want

00:03:44.950 --> 00:03:50.070
to get back to that in a minute when I make
a comment about reactions to "Alone Together."

00:03:50.070 --> 00:03:59.030
And then this new book is really about something
that I didn't see, that I began to see really

00:03:59.030 --> 00:04:03.690
right after I finished working on "Life on
the Screen."

00:04:03.690 --> 00:04:08.300
My model, what I was studying in "Life on
the Screen," when I studied all these people

00:04:08.300 --> 00:04:13.630
on Moos and MUDs and starting to be with AOL,
were basically people sitting at desks with

00:04:13.630 --> 00:04:19.290
windows open on their screens, cycling through
the windows on their stable, sitting computers

00:04:19.290 --> 00:04:25.410
and even laptops. Right, I would say, I finished
that book in 1990--.

00:04:25.410 --> 00:04:32.320
It came out in 1995, which means I finished
it in 1995, 1994 and a half. And then I would

00:04:32.320 --> 00:04:43.050
say in 1995 and a half, '96, I met Steve Mann,
who was basically wired up as a cyborg and

00:04:43.050 --> 00:04:47.650
showed me, along with the other people who
at that time at the media lab called themselves

00:04:47.650 --> 00:04:54.180
'cyborgs', what it meant to be always on,
with your technology always on and always

00:04:54.180 --> 00:04:55.180
on you.

00:04:55.180 --> 00:05:00.280
I mean, it was a glimpse into a future that
we're all living now. I mean, because basically,

00:05:00.280 --> 00:05:04.910
everybody here who has any kind of mobile
technology is living the world that those

00:05:04.910 --> 00:05:11.830
early cyborgs were pointing to. And it was
that--. In those days, the cyborgs, many people

00:05:11.830 --> 00:05:13.900
took those cyborgs to be handicapped.

00:05:13.900 --> 00:05:22.370
I mean, they looked, they were wearing backpacks,
an antenna. They were not elegant creatures

00:05:22.370 --> 00:05:26.020
and they were dealing more with people feeling
sorry for them and wondering if they needed

00:05:26.020 --> 00:05:34.940
special equipment and how to live looking
like that. But I hadn't seen, I hadn't realized

00:05:34.940 --> 00:05:41.830
in all of that discussion of identity play
and all that stuff, what it would mean, and

00:05:41.830 --> 00:05:45.870
this is my, this is my, my lack of seeing.

00:05:45.870 --> 00:05:51.740
I hadn't thought through, in "Life on the
Screen" what it would mean if it were always

00:05:51.740 --> 00:05:59.120
on, always on you. And basically, for the
past 15 years, I've been following that story.

00:05:59.120 --> 00:06:04.009
As that story has developed, I've been following
it for grown-ups who came to get used to it.

00:06:04.009 --> 00:06:09.140
And I've been growing it for the cohort of
children who then became teenagers and so

00:06:09.140 --> 00:06:13.479
forth, for different generations as they matured
as we've gotten used to living in that kind

00:06:13.479 --> 00:06:14.479
of culture.

00:06:14.479 --> 00:06:19.410
And as we've gotten used to living in that
kind of culture, [sniffs] I've developed some

00:06:19.410 --> 00:06:25.100
real concerns, as I said, mostly from the
point of view of a psychologist who studies

00:06:25.100 --> 00:06:30.440
developmental issues. And I just wanna say
in the beginning, I'll tell you a little bit

00:06:30.440 --> 00:06:31.759
about those concerns. Obviously, they're in
the book.

00:06:31.759 --> 00:06:39.200
They're the concerns of a psychologist and
an, and an anthropologist as I study these

00:06:39.200 --> 00:06:45.220
generations and I've studied contemporary
behavior. I take nothing back. I was on the

00:06:45.220 --> 00:06:52.210
cover of Wired Magazine. [laughs] I love all
my gadgets. I love computing. I love technology.

00:06:52.210 --> 00:06:57.280
I take nothing back from all the things I
said about the identity possibilities of the

00:06:57.280 --> 00:07:02.060
computer, about the wonderful things it brings
us.

00:07:02.060 --> 00:07:07.750
But that doesn't mean that as I've seen this
new development of always on, always on you

00:07:07.750 --> 00:07:13.830
computing and what it means to always have
the possibility of also being elsewhere, that

00:07:13.830 --> 00:07:19.030
I haven't seen some things that I think we
need to rethink and take seriously. And I've

00:07:19.030 --> 00:07:24.060
been very interested in the--. I'm thrilled
to speak at Google and to have time to [sniffs]

00:07:24.060 --> 00:07:28.850
interact with you because I've noticed in
the reviews of this book, and one that's coming

00:07:28.850 --> 00:07:29.850
out in the New York--.

00:07:29.850 --> 00:07:35.100
Particularly one that you haven't seen yet
because I just saw the advanced review. It's

00:07:35.100 --> 00:07:40.460
coming out in the Times on Sunday. It basically
is, "She was on the cover of Wired Magazine.

00:07:40.460 --> 00:07:49.960
She loved this stuff. Now, she doesn't." It's
like technology and I were dating and I--.

00:07:49.960 --> 00:07:56.530
Like a spurned lover. And I think there's
something about it.

00:07:56.530 --> 00:08:00.750
And there was one in the Boston Globe, similarly.
"She used to love technology. Now, she doesn't.

00:08:00.750 --> 00:08:10.240
What's the matter?" And I think there's something
about that reaction to that it's all about

00:08:10.240 --> 00:08:16.759
me and whether I like technology. It's all
about why don't I like technology anymore.

00:08:16.759 --> 00:08:25.860
That is so far away from what I intended in
writing this, what I intended when you read

00:08:25.860 --> 00:08:29.830
it, what I'm trying to be about.

00:08:29.830 --> 00:08:34.210
It's not all about me. It's not all about
my technology lover. It's not all about whether

00:08:34.210 --> 00:08:41.729
you feel spurned. It's not all about whether
you love or hate technology. Technology is

00:08:41.729 --> 00:08:50.050
our partner in making our lives, in constructing
the self and how our kids grow up. It's not

00:08:50.050 --> 00:08:57.390
like we're stuck with it. It is, it is, it
is our--. We are in a human dynamic partnership

00:08:57.390 --> 00:08:58.390
with technology.

00:08:58.390 --> 00:09:06.589
That is how it is and how it should be and
how it always will be. The question is--and

00:09:06.589 --> 00:09:14.110
here I differ from Kevin Kelly--it's not what
technology wants. It's what kind of partnership

00:09:14.110 --> 00:09:23.850
we make with technology to get it right to
serve our human purposes. So, I say this--.

00:09:23.850 --> 00:09:27.730
I don't mean to sound defensive. I don't say
this really in a spirit of defensiveness,

00:09:27.730 --> 00:09:37.959
but in a spirit of as you are at the epicenter,
here, of really making, setting the terms

00:09:37.959 --> 00:09:41.630
in some ways of this new kind of partnership.

00:09:41.630 --> 00:09:52.110
Be aware, be very aware, that there's a tendency
to play this game as though you're for or

00:09:52.110 --> 00:09:58.930
against technology. A tendency in the media,
a tendency in people who are cultural critics

00:09:58.930 --> 00:10:06.110
of technology who are out there. And I'm just
saying that I think that that's a dangerous

00:10:06.110 --> 00:10:13.500
frame of mind because this is not about being
for or against it, this is about getting the

00:10:13.500 --> 00:10:14.649
human journey right.

00:10:14.649 --> 00:10:22.690
So, for example, when Mark Zuckerberg says,
"Privacy is,"--what did he say? He said yesterday,

00:10:22.690 --> 00:10:33.850
"Privacy is no longer a norm." And I tweeted,
"What is intimacy without privacy? What is

00:10:33.850 --> 00:10:39.080
democracy without privacy?" Now, that wasn't
to take an anti-Facebook stance. And right

00:10:39.080 --> 00:10:45.370
away, I get back, "That's an anti-Facebook
position."

00:10:45.370 --> 00:10:50.500
I don't think asking "What is democracy without
privacy?" is an anti-Facebook position. I

00:10:50.500 --> 00:10:57.870
wasn't turning down Facebook on a date when
I raised that question. It wasn't an anti-technology

00:10:57.870 --> 00:11:04.290
position. It was simply saying, and certainly
to you at Google, "What about democracy without

00:11:04.290 --> 00:11:05.790
privacy? What about intimacy without--?"

00:11:05.790 --> 00:11:11.500
Let's talk about this. I mean, these are not
questions that necessarily take positions

00:11:11.500 --> 00:11:17.269
against technology. These were questions that
people need to ask in an open, free society.

00:11:17.269 --> 00:11:25.800
And they should be talked about. And I'll
get to more of these in greater detail in

00:11:25.800 --> 00:11:30.709
my formal remarks. But, to ask a question
like that, "What is intimacy without privacy?

00:11:30.709 --> 00:11:36.149
What is democracy without privacy?" and being
attacked as having an anti-Facebook position.

00:11:36.149 --> 00:11:42.480
I just think it's a curious--. It isn't my
intent, but I want those questions discussed.

00:11:42.480 --> 00:11:47.500
I want them discussed for kids. I want them
discussed for the society at large. I just

00:11:47.500 --> 00:11:51.380
think these are, these are questions that
shouldn't have to do with whether you like,

00:11:51.380 --> 00:12:00.620
or don't like, technology. So let me tell
you a little bit about how I got to this book.

00:12:00.620 --> 00:12:05.529
I mean, first of all, I told you about the
cyborgs that got, that got me thinking about

00:12:05.529 --> 00:12:11.360
the "always on" culture. But they really were
two, there are two threads in this book. Two

00:12:11.360 --> 00:12:17.329
things, basically, started happening that
complete--. 'Cause after they started happening,

00:12:17.329 --> 00:12:23.110
after I finished "Life on the Screen," where
I thought I would take a little bit of a break.

00:12:23.110 --> 00:12:28.490
And even though I'd been working on this book
15 years, it really reports on 15 years of

00:12:28.490 --> 00:12:34.339
research because two things started happening
that I felt that I couldn't write about until

00:12:34.339 --> 00:12:41.540
I saw them develop over the long span of time,
because I thought they were very complicated

00:12:41.540 --> 00:12:47.190
things. And I really wanted to see almost
a generation try to deal with them.

00:12:47.190 --> 00:12:54.350
And of course, you never know when to stop
and I almost arbitrarily chose a 15-year time

00:12:54.350 --> 00:13:00.110
frame as saying, "Look, it's a 15-year. Look
at how these two things have developed." And

00:13:00.110 --> 00:13:06.149
the first were sociable robots which, from
a certain point of view, may seem to you to

00:13:06.149 --> 00:13:13.089
have nothing at all to do with social networking,
but in my mind actually, the two stories interweave

00:13:13.089 --> 00:13:17.970
and I hope you'll find when you read the book,
if you read the book, that they interweave

00:13:17.970 --> 00:13:19.100
in interesting ways.

00:13:19.100 --> 00:13:27.730
So, sociable robots, what interests me about
them is that it turns out, because I'm obviously

00:13:27.730 --> 00:13:34.380
interested in questions of substitution for
the face-to-face, the one-on-one, the being

00:13:34.380 --> 00:13:42.390
with each other in a body-to-body, eye-to-eye
contact. The sociable robots are robots and

00:13:42.390 --> 00:13:49.380
I’m not against robots--love the robots--not
against robots that do stuff for you: vacuum,

00:13:49.380 --> 00:13:56.810
clean, dishes, the robots in war that, that
go after the bombs and go into the dangerous

00:13:56.810 --> 00:13:57.810
places.

00:13:57.810 --> 00:14:01.209
Very pro-robot woman, just on the record.

00:14:01.209 --> 00:14:02.209
[audience chuckles]

00:14:02.209 --> 00:14:09.209
But the robots that really started to get
me interested and upset again from a psychological,

00:14:09.209 --> 00:14:16.470
developmental perspective, are the robots
that make eye contact, gesture in your direction,

00:14:16.470 --> 00:14:24.050
say your name, and crucially, track your motion.
Because when a robot does that, and in the

00:14:24.050 --> 00:14:27.480
book I tell the story of my first meeting
with Cog, which is a robot that does that,

00:14:27.480 --> 00:14:33.800
when a robot does that it turns out that you
are toast 'cause you feel that somebody is

00:14:33.800 --> 00:14:34.950
home.

00:14:34.950 --> 00:14:39.800
Because we are biologically programmed. We
are human animals. We are programmed that

00:14:39.800 --> 00:14:44.640
when something does that to us, we feel as
though we're with another one of our kind.

00:14:44.640 --> 00:14:50.283
Because there simply was nothing in our biological
past that knew how to do that. And so, when

00:14:50.283 --> 00:14:57.190
we meet one of those, we feel that we're home
with another creature like us.

00:14:57.190 --> 00:15:02.839
And when I met Cog, again, just as I was finishing
"Life on the Screen," I think I met Cog in

00:15:02.839 --> 00:15:08.920
1995, I met, I met him in Rodney Brooks' laboratory.
There was another person who came into the

00:15:08.920 --> 00:15:14.400
laboratory with me and I knew exactly how
Cog worked. I had all the information in the

00:15:14.400 --> 00:15:20.010
world about how this robot worked and I started
competing for his attention.

00:15:20.010 --> 00:15:24.220
Excuse me, its attention. And there again,
it's almost impossible not to think of it--

00:15:24.220 --> 00:15:29.870
that was not an information processing error.
That was a Freudian slip. I absolutely could

00:15:29.870 --> 00:15:35.970
not help myself, but to want to have this
robot's attention. And part of me is saying,

00:15:35.970 --> 00:15:42.250
"Get a grip. Get a grip." And part of me,
it's not about information about how the robot

00:15:42.250 --> 00:15:44.250
worked. It is absolutely visceral.

00:15:44.250 --> 00:15:49.290
It's the quality of the experience. And so,
I began to do studies with kids and not kids,

00:15:49.290 --> 00:15:58.380
with these sociable robots that are designed
not to be intelligent. They're not smart.

00:15:58.380 --> 00:16:05.120
They're smart about pushing your Darwinian
buttons. So it's not about their intelligence.

00:16:05.120 --> 00:16:06.161
It's about our vulnerability. And I wrote
this book.

00:16:06.161 --> 00:16:17.350
The metaphor in "Alone Together," is about
human vulnerability and the technological

00:16:17.350 --> 00:16:23.490
affordances, that these are technologies that
are built around what is human vulnerability

00:16:23.490 --> 00:16:27.639
and what are the technological affordances
that push on those human vulnerabilities.

00:16:27.639 --> 00:16:35.900
And that's the story of the socialable robots,
in which people are actually willing and wanting

00:16:35.900 --> 00:16:45.900
to substitute robots that seem to care for
people in more circumstances than I ever would

00:16:45.900 --> 00:16:49.230
have dreamed possible.

00:16:49.230 --> 00:16:54.940
Women who want to have a robot boyfriend.
And I mean seriously, women who want to have

00:16:54.940 --> 00:17:00.389
a robot boyfriend because they basically say,
"Look, I'm just looking for a feeling of civility

00:17:00.389 --> 00:17:08.939
and comfort and somebody being home, somebody
being there." Really not caring if the robot

00:17:08.939 --> 00:17:12.379
understands in any deeper way.

00:17:12.379 --> 00:17:21.350
People wanting to have a robot companion.
People -- and it's as though a feeling of--.

00:17:21.350 --> 00:17:25.509
It's not about feeling that the robot understands.
It's a feeling about our having given up on

00:17:25.509 --> 00:17:33.399
each other in some kind of, in some kind of
way that was a way to track. And so, it's

00:17:33.399 --> 00:17:40.760
a complicated story about why people, again,
understanding the limitations of robots, are

00:17:40.760 --> 00:17:46.460
willing to settle for that, much as they're
willing to settle, I think, for giving up

00:17:46.460 --> 00:17:52.539
the human voice in, in, in,in other kinds
of communication.

00:17:52.539 --> 00:18:00.330
So the robot story and then the story of "I'd
rather text than talk." In the robot stories,

00:18:00.330 --> 00:18:06.429
I want to say one thing that surprised me.
I began to study the earliest sociable objects

00:18:06.429 --> 00:18:10.109
'cause I wanted to get really--I believe that
everything starts in the nurseries. One thing

00:18:10.109 --> 00:18:14.519
for me to meet Cog as a grown woman, but I
wasn't socialized to love these objects.

00:18:14.519 --> 00:18:19.989
And the first objects that were there to study
where the Tamagotchies and the Furbies. And

00:18:19.989 --> 00:18:25.190
the Tamagotchi is an incredibly important
actor. How many here ever remember Tamagotchi,

00:18:25.190 --> 00:18:32.200
so I--. OK, so you know. The Tamagotchi asked
for love. It asked for nurturance. And it

00:18:32.200 --> 00:18:38.619
turns out that nurturance is the killer app
in sociable robotics in the following sense.

00:18:38.619 --> 00:18:49.200
That we don't just nurture what we love. Human
beings are programmed to love what we nurture.

00:18:49.200 --> 00:18:58.049
So that any – a digital creature that asks
for nurturance gets us to love it. So, nurturance

00:18:58.049 --> 00:19:06.009
is the killer app in forging a connection
with a digital object. And if you look at

00:19:06.009 --> 00:19:11.840
people's relationships with digital objects,
the most successful ones--the Tamagotchi,

00:19:11.840 --> 00:19:15.179
the Furby--are the ones that ask you for something.

00:19:15.179 --> 00:19:24.080
And I traced that story and the history of
the sociable robotic revolution. Nurturance

00:19:24.080 --> 00:19:33.759
is the killer app for sociable robotics. And
that story, for me, goes along with the story

00:19:33.759 --> 00:19:38.419
from the other direction, which are the temptations
of the new relationships that we can have

00:19:38.419 --> 00:19:43.149
with each other via mobile connectivity, always
on and always on us.

00:19:43.149 --> 00:19:49.549
We now expect the control over time and emotional
resources that texting and messaging provide.

00:19:49.549 --> 00:19:54.950
We don't use the voice minutes on our phones.
We'd rather text than talk to the point that

00:19:54.950 --> 00:20:02.389
talking comes to seem intrusive, a demand
on our real time. This is the results of my

00:20:02.389 --> 00:20:03.389
interviews, hundreds and hundreds of interviews.

00:20:03.389 --> 00:20:06.929
We turn instead, obviously to Facebook, to
friending, to Twitter, to worlds in which

00:20:06.929 --> 00:20:15.211
we play avatars, being ourselves but not quite
ourselves. One of the very interesting things

00:20:15.211 --> 00:20:24.820
I found in studying both gaming and social
networking is there's a very surprising paradox

00:20:24.820 --> 00:20:30.629
is that when we play an other, for example,
an avatar in World of Warcraft, we end up

00:20:30.629 --> 00:20:32.429
playing aspects of ourselves.

00:20:32.429 --> 00:20:37.509
In other words, you think you're playing really
a very far out avatar, but you realize that

00:20:37.509 --> 00:20:42.259
you're playing an aspect of yourself, something
that I'd already seen and studied in the MUDing

00:20:42.259 --> 00:20:47.799
world. But when you go to places such as Facebook,
where you think you'll simply be yourself,

00:20:47.799 --> 00:20:49.580
you end up playing a role.

00:20:49.580 --> 00:21:00.559
You end up in a life of performance and self-presentation,
which has very important, very important decisive

00:21:00.559 --> 00:21:07.950
implications for adolescents because you can
say, "Oh, look. All the time, you're always

00:21:07.950 --> 00:21:11.970
performing. Everybody's always performing.
Everybody's always performing." But when you're

00:21:11.970 --> 00:21:20.940
really with people, face to face, in the real,
there are ways in which you have moments,

00:21:20.940 --> 00:21:28.460
you have glimpses, you have ways of feeling
like you're not performing, of having those

00:21:28.460 --> 00:21:32.179
moments when performance slips away and you
relax.

00:21:32.179 --> 00:21:39.850
You're yourself. And there's something about,
in adolescence, a constant culture in remembering

00:21:39.850 --> 00:21:46.039
this culture of performance that starts to
be a problem in Facebook. Particularly, for

00:21:46.039 --> 00:21:51.059
adolescents. So in these lives of performance,
I've found again, my work is qualitative.

00:21:51.059 --> 00:21:55.989
It's interviews. It's a lot of interviews.
It's interviews over long periods of time,

00:21:55.989 --> 00:22:00.269
but it is interview work. I'm trained as a
clinician.

00:22:00.269 --> 00:22:05.450
That's my value added. That's what I know
how to do. We face confusions. The first working

00:22:05.450 --> 00:22:10.830
title of the book was, "Cyber Intimacy/ Cyber
Solitudes" because I really found that the

00:22:10.830 --> 00:22:16.889
main finding was that people didn't know when
they were together and they didn't know when

00:22:16.889 --> 00:22:19.749
they were alone.

00:22:19.749 --> 00:22:22.950
And the original "Alone Together" sums that
up in a more popular language, but the original

00:22:22.950 --> 00:22:28.210
title was "Cyber Intimacy/ Cyber Solitude."
In our lives of performance, we face confusions.

00:22:28.210 --> 00:22:34.609
At the end of an evening of avatar to avatar
chat in a network game, people feel, at one

00:22:34.609 --> 00:22:39.220
moment, in possession of a full social life
and in the next moment, they feel curiously

00:22:39.220 --> 00:22:42.409
isolated in a tenuous complicity with strangers.

00:22:42.409 --> 00:22:48.590
We recreate ourselves with new bodies and
jobs and homes and romances. You build a following

00:22:48.590 --> 00:22:53.210
on Facebook. And then you wonder to which
degree your followers are your friends. You're

00:22:53.210 --> 00:22:59.779
together with other people and yet, one second
later, you feel utterly alone. So, in one

00:22:59.779 --> 00:23:03.720
of the stronger statements of the book, I
say you're tempted to substitute objects for

00:23:03.720 --> 00:23:07.389
people, or possibly to treat people as though
they were objects.

00:23:07.389 --> 00:23:12.789
Chat roulette is my favorite example of literally
treating people as though they're objects.

00:23:12.789 --> 00:23:18.599
You're an object. I don't like that object.
Next. I like that object. Next. So there's

00:23:18.599 --> 00:23:25.039
a parallel in solitude when you're with a
robot. You are alone. Trust me. You are in

00:23:25.039 --> 00:23:26.979
solitude when you're with a robot.

00:23:26.979 --> 00:23:33.179
You feel a new intimacy and when you're in
intimacy, you're with another person. When

00:23:33.179 --> 00:23:41.009
you're with another person online, you have
potential for intimacy. And people still,

00:23:41.009 --> 00:23:45.749
because of the medium, can feel a new and
unexpected solitude. And those are the two

00:23:45.749 --> 00:23:50.409
parts of the book that I try--decided to make
it one book--to try make those two things

00:23:50.409 --> 00:23:52.159
play off each other.

00:23:52.159 --> 00:23:57.529
So here's a psychological implication of this
new connectivity. Today's teenagers tell me

00:23:57.529 --> 00:24:01.999
that when they have a feeling, they turn to
their online, online contacts on Facebook

00:24:01.999 --> 00:24:07.690
or on their phones, because sharing the feeling
has become part of having the feeling. So

00:24:07.690 --> 00:24:14.309
when you live in a world of continual connection,
people who are only a touch away are there

00:24:14.309 --> 00:24:19.129
for continual validation. It's "I share, therefore
I am."

00:24:19.129 --> 00:24:27.259
We move from, "I have a feeling. I want to
make a call," to, "I want to have a feeling.

00:24:27.259 --> 00:24:35.479
I need to make a call." "I have a feeling.
I want to make a call" to "I want to have

00:24:35.479 --> 00:24:41.979
a feeling. I need to make a call", which is
a very different thing. Or, "I want to have

00:24:41.979 --> 00:24:48.859
a feeling". More likely, "I need to send a
text". When we use other people in this way,

00:24:48.859 --> 00:24:56.899
you can get used to seeing them as spare parts,
as ways to support our too fragile selves.

00:24:56.899 --> 00:25:00.480
So again, to summarize the thesis of the book
and why I put these robots and the connectivity

00:25:00.480 --> 00:25:08.940
together in a way that might seem not obvious.
Alone with robots we feel connected. Together

00:25:08.940 --> 00:25:13.869
with people but not fully relating to them,
we feel alone. So I say we're at the still

00:25:13.869 --> 00:25:19.519
center of a perfect storm. And I think of
this, I sum this up as the robotic moment,

00:25:19.519 --> 00:25:21.820
not because we all have robots.

00:25:21.820 --> 00:25:27.029
We don't all have robots. But I like to think
of it as the robotic moment because it's a

00:25:27.029 --> 00:25:32.080
technological moment, in which we fear that
our lives with technology have gotten out

00:25:32.080 --> 00:25:39.799
of control and we fantasize paradoxically,
and it's a fantasy. It is not like a happening.

00:25:39.799 --> 00:25:48.690
We fantasize that it is technology that will
help us reestablish control.

00:25:48.690 --> 00:25:57.669
So we feel our lives have gotten out of control.
Hundred and fifty emails. I mean, how many

00:25:57.669 --> 00:26:00.840
emails do you guys get a day? Seriously. Are
you ashamed to say, to speak it aloud, the

00:26:00.840 --> 00:26:02.289
number that dare not speak its name?

00:26:02.289 --> 00:26:03.500
&gt;&gt;Female Audience Member #1: I get 400.

00:26:03.500 --> 00:26:06.610
&gt;&gt;Sherry Turkle: You get 400? Do I hear five?

00:26:06.610 --> 00:26:07.669
[laughter]

00:26:07.669 --> 00:26:10.219
Anybody willing to admit to 500?

00:26:10.219 --> 00:26:13.009
&gt;&gt;Male Audience Member #2: Work emails, or
personally?

00:26:13.009 --> 00:26:18.539
&gt;&gt;Sherry Turkle: OK, let's just start, messages
of any sort. How much do you have--. How many

00:26:18.539 --> 00:26:28.019
messages of all sorts do you need to respond
to every day? [pause] Because the point here,

00:26:28.019 --> 00:26:36.749
I'm willing to admit to a thousand. I'm willing
to admit that I need to respond, between the

00:26:36.749 --> 00:26:43.049
email, the texts, the crucial Twitters that
say, "You're a jerk."

00:26:43.049 --> 00:26:44.049
[laughter]

00:26:44.049 --> 00:26:48.969
The Twitters that have to be answered because
they, they, or I, feel that they have personally

00:26:48.969 --> 00:26:59.080
insulted me. I'm willing to admit text messages,
urgent, please, letters of recommendation

00:26:59.080 --> 00:27:05.340
that need to be, 'cause now anybody can reach
me at any time.

00:27:05.340 --> 00:27:07.999
So a letter of recommendation that needs to
be done tomorrow no longer seems an outrageous

00:27:07.999 --> 00:27:12.859
request. It can be a thousand. It can be 1200.
Just constant, constant, constant, constant.

00:27:12.859 --> 00:27:19.200
And I'm -- the argument in this book is that
when you--and it's not an argument, this is

00:27:19.200 --> 00:27:24.989
actually the findings from my interviews--that
doctors, lawyers, Indian chiefs, management

00:27:24.989 --> 00:27:29.769
consultants, professionals of all stripes,
are willing to say to me that when you get

00:27:29.769 --> 00:27:39.899
that much input and you have to respond immediately,
you start to ask questions that you know can

00:27:39.899 --> 00:27:42.129
be answered quickly.

00:27:42.129 --> 00:27:47.450
You start to give answers that you can answer
quickly. You start to dumb down the questions

00:27:47.450 --> 00:27:52.539
and dumb down the answers. You start to get
into a vicious circle of asking the questions

00:27:52.539 --> 00:28:00.989
that can be answered quickly. We start to
degrade. It's not--. We're degrading our culture

00:28:00.989 --> 00:28:04.259
in terms of business problems in two ways.

00:28:04.259 --> 00:28:09.210
First of all, we're asking each other questions
that we can get quick answers to, because

00:28:09.210 --> 00:28:14.179
you're simply, the volume and the velocity
picks up to the point that you just know that

00:28:14.179 --> 00:28:17.919
you're dumbing things down because you know
somebody wants an answer right away. And so,

00:28:17.919 --> 00:28:21.509
you're gonna get that conversation at a lower
level so you can get back right away.

00:28:21.509 --> 00:28:28.399
And then we're dumbing it down in the sense
that we're not talking to the people in the

00:28:28.399 --> 00:28:35.009
next office. People--now maybe here you have
this magnificent space. I've, I've read about

00:28:35.009 --> 00:28:40.899
your culture and I hear that here, here you
make a big effort to avoid this. But the people

00:28:40.899 --> 00:28:46.990
I've interviewed talk about emailing people
in the next cubicle because they don't want

00:28:46.990 --> 00:28:49.249
to intrude on them.

00:28:49.249 --> 00:28:57.929
So we have to fight. Again, I mean, from what
I read, everything here is trying to tell

00:28:57.929 --> 00:29:07.840
you, "Please don't do that. Please talk to
each other." But out there, that's a norm

00:29:07.840 --> 00:29:13.329
to not want to intrude on the person in the
next cubicle. And actually, I begin the book

00:29:13.329 --> 00:29:18.839
– maybe I forget. I begin the book with
a story of a mother. I mean, it's not just

00:29:18.839 --> 00:29:19.839
in the business world.

00:29:19.839 --> 00:29:25.320
But of a mother who’s trying to hire a nanny.
And she goes to the nanny's house because

00:29:25.320 --> 00:29:27.259
she wants to see the nanny in her own home
environment. And the nanny's roommate opens

00:29:27.259 --> 00:29:34.460
the door and the mother says, "Well, I am
here to see this woman." And the girl starts

00:29:34.460 --> 00:29:41.519
to text her. She's texting a woman who literally
is 10 feet away.

00:29:41.519 --> 00:29:45.870
And she says, "Well, why don't you knock on
her door?" And she says, "Well, that would

00:29:45.870 --> 00:29:53.410
be intrusive." So the notion that any kind
of face-to-face becomes intrusive. And when

00:29:53.410 --> 00:30:01.559
I interview the people who don't want to intrude
on their colleagues by knocking on their door,

00:30:01.559 --> 00:30:03.589
by sharing the coffee, by, you know.

00:30:03.589 --> 00:30:08.529
And I say, "Well did you ever?" And they say,
"Oh, yeah." And they start to talk about it

00:30:08.529 --> 00:30:12.779
as though those were the Golden Years, the
good-ol' times. And I say, "Well, why was

00:30:12.779 --> 00:30:24.179
what was so collegial then, so intrusive now?"
And you can see them thinking and they end

00:30:24.179 --> 00:30:30.109
up with something like, "Well, they have so
much more to do now."

00:30:30.109 --> 00:30:36.089
And read that as they have so much more email.
They have so much more email to take care

00:30:36.089 --> 00:30:39.950
of now. And that's not so different from being
at Thanksgiving dinner, or being at a dinner

00:30:39.950 --> 00:30:45.309
party, or being at a conference and watching
everybody around you answer their texts and

00:30:45.309 --> 00:30:49.179
their phone calls, rather than pay attention
to you.

00:30:49.179 --> 00:30:56.629
I mean, there's something that's become more
compelling to us, or the mores has become

00:30:56.629 --> 00:31:03.320
more compelling to us to answer that virtual,
that person who's not there than really the

00:31:03.320 --> 00:31:11.279
people who are around us. And this is a question
of psychology. I mean, this is not the place

00:31:11.279 --> 00:31:13.179
to go into it. Read the book.

00:31:13.179 --> 00:31:18.759
But I mean, psychologically, I know a lot
about why the fantasy of that unknown call

00:31:18.759 --> 00:31:24.369
is more compelling than the person who's in
front of you. Just trust me. It is for very

00:31:24.369 --> 00:31:31.570
deep reasons. So, part of it is we're fighting
something very deep psychologically in us.

00:31:31.570 --> 00:31:39.959
But part of it is the way in which we establish
social norms around technology.

00:31:39.959 --> 00:31:46.619
And it has now become the social norm that
really you do, when you're having dinner,

00:31:46.619 --> 00:31:52.719
put the phones on the table. You really do
interrupt a conversation to take a call. I

00:31:52.719 --> 00:31:58.119
tell a story about going to a funeral and
having--of recently going to a funeral of

00:31:58.119 --> 00:32:01.409
a colleague--and people texting during the
funeral.

00:32:01.409 --> 00:32:07.470
Last night on the Colbert Report, one of the
people who were back hand, one of the stagehands,

00:32:07.470 --> 00:32:15.899
was talking to me about going to a memorial
gathering for a family member. They were sitting

00:32:15.899 --> 00:32:23.229
Shiva and people were texting during the Shiva.
And everybody has, everybody has a story.

00:32:23.229 --> 00:32:28.509
I happened to put some of the stories in the
book, but it's not because that's what ethnographers

00:32:28.509 --> 00:32:32.860
do. But it's not as though my stories, and
as an ethnographer I have a rule, that if

00:32:32.860 --> 00:32:37.609
I find a story and I only find one story like
it, [cell phone rings twice in the audience]

00:32:37.609 --> 00:32:44.440
or two stories like it, I don't tell it. This
is not about sensationalist reporting where

00:32:44.440 --> 00:32:46.589
if I find a weird story, I put it in the book.

00:32:46.589 --> 00:32:52.210
No. No. This is about--. I mean, I took 15
years to make sure I had a lot of stories

00:32:52.210 --> 00:32:55.419
and a lot of backup. This book has a deep
bench. So if I tell you people are texting

00:32:55.419 --> 00:33:02.919
at funerals, it's not because I once saw a
funeral where people were texting. We're giving

00:33:02.919 --> 00:33:11.969
up, we're giving up on something about ourselves.
And this is about--. This is in our control.

00:33:11.969 --> 00:33:17.639
Let me just end by saying, because I want
to make sure I have time to take your questions,

00:33:17.639 --> 00:33:32.649
I wanna end on just two things that--. [pause]
At this robotic moment we have to be concerned

00:33:32.649 --> 00:33:40.649
that the simplification and reduction of our
relationships is no longer something we complain

00:33:40.649 --> 00:33:42.499
about.

00:33:42.499 --> 00:33:49.739
Because we start down a path of substitution
with the idea that technology provides alternatives

00:33:49.739 --> 00:33:56.240
that are better than nothing. Then we think
that perhaps technology is better than some

00:33:56.240 --> 00:34:01.769
of the available human connections. And finally,
we play with the idea that technology might

00:34:01.769 --> 00:34:04.799
be better than any human connection.

00:34:04.799 --> 00:34:09.370
From better than nothing to simply better.
So let me give you one example from robotics

00:34:09.370 --> 00:34:14.889
and then I'll just end and talk about the
final, the final – just the wrap-up point.

00:34:14.889 --> 00:34:22.190
Some of the -- let me talk about the robotics.
So, this 11-year old girl starts off by saying

00:34:22.190 --> 00:34:27.159
that a robot dog might be better than a real
dog because her father is allergic to dogs.

00:34:27.159 --> 00:34:34.379
Next in the interview, she thinks the robot
dog is better than a real dog because it will

00:34:34.379 --> 00:34:45.119
never die. [pause] Then, the child allows
herself to muse that a robot dog could be

00:34:45.119 --> 00:34:52.990
made to stay a cute puppy, more gratifying
than any real dog could ever be because everybody

00:34:52.990 --> 00:34:58.549
would always love a puppy and here's a puppy
that would never die.

00:34:58.549 --> 00:35:06.200
So you start off with "allergic, it's better
than nothing." Then, "always stay a puppy,

00:35:06.200 --> 00:35:15.339
might be better than some dogs." "Never die,
always stay a puppy." You know, "a robot dog

00:35:15.339 --> 00:35:24.549
is better than any dog could ever be". So
in the book, I trace story after story, relationship

00:35:24.549 --> 00:35:28.369
after relationship, not in a spirit of being
sensationalistic.

00:35:28.369 --> 00:35:34.910
Not in the spirit of "Apocalypse Now," but
just to show the psychology of how you--.

00:35:34.910 --> 00:35:41.559
And this is why I ended up including the robots.
These very simple robots that look like nothing.

00:35:41.559 --> 00:35:58.400
I wanted to show how with even simple, simple
1995 objects, you can get people to talk about

00:35:58.400 --> 00:36:04.990
a psychology of how technological substitutes
can come to seem more potentially more valuable

00:36:04.990 --> 00:36:07.260
than the real thing.

00:36:07.260 --> 00:36:14.180
Elder-care robots, something I feel passionate
about. There's lots of money, lots of development

00:36:14.180 --> 00:36:20.089
of robotics for the elderly. Don't tell me
you're developing them in this--. This is

00:36:20.089 --> 00:36:27.510
something that I think is really a problem.
I call it love labor lost. Nanny-bots, Eldercare-bots,

00:36:27.510 --> 00:36:30.730
substituting robots for one really--.

00:36:30.730 --> 00:36:42.170
We have a human compact to care for each other.
People telling their life stories to robots

00:36:42.170 --> 00:36:48.170
who don't know what a life is and who won't
know what a life is. And who don't know--.

00:36:48.170 --> 00:36:51.220
Older people trying to tell their life story
and how they lost a child and how a child

00:36:51.220 --> 00:36:57.120
died to a robot who doesn't know what it is
to be a mother and have a child.

00:36:57.120 --> 00:37:03.210
These are not--. These are moments that it
doesn't mean that I fell out of love with

00:37:03.210 --> 00:37:09.940
technology. These are moments where it's important
to ask, "Why are we doing this? What is this

00:37:09.940 --> 00:37:14.279
technology for? What are we accomplishing?
Does this serve our human values?" When you

00:37:14.279 --> 00:37:17.000
watch this, when you watch these old people
doing this, you have to ask yourself, "What

00:37:17.000 --> 00:37:21.320
is the point? What human value is being served?"

00:37:21.320 --> 00:37:28.640
When you watch a child being baby--. The idea
of a babysitter robot and you think about

00:37:28.640 --> 00:37:36.250
robots that will socialize a child. A robot
that has a limited number of facial expressions.

00:37:36.250 --> 00:37:41.190
Or even, the robots of the future that will
have more of them. What will they know about

00:37:41.190 --> 00:37:47.130
the infinite variety of speech and gesture
and body movement.

00:37:47.130 --> 00:37:54.079
What, what, why would we want a robot to be
teaching our children about the fluidity of

00:37:54.079 --> 00:38:02.250
gesture and motion and voice? This is people
work. What is the point of these substitutions?

00:38:02.250 --> 00:38:11.309
So, this is why I bring these two issues of
sociable substitutions together. And in my

00:38:11.309 --> 00:38:17.819
view, sociable robots and online life both
suggest the possibility of relationships the

00:38:17.819 --> 00:38:22.140
way we want them, just as we can program a
made to measure robot.

00:38:22.140 --> 00:38:27.920
We can reinvent ourselves as comely avatars.
We can write the Facebook profile that pleases

00:38:27.920 --> 00:38:35.800
us. We can edit messages until they project
the self we want to be. We can edit messages

00:38:35.800 --> 00:38:42.510
until they project the self we want to be.
And we can keep things simple. Our new media

00:38:42.510 --> 00:38:45.510
are well-suited for accomplishing the rudimentary.

00:38:45.510 --> 00:38:50.760
And because this is what technology serves
up, we reduce our expectations of each other.

00:38:50.760 --> 00:38:55.770
An impatient high school senior says to me--this
is a great quote from the book--,"If you really

00:38:55.770 --> 00:39:00.430
need to reach me, just shoot me a text." And
he sounds to me just like my colleagues on

00:39:00.430 --> 00:39:04.319
a consulting job that I just finished, who
tell me they don't need to talk.

00:39:04.319 --> 00:39:08.440
They'd just rather avoid face to face meetings
and would prefer to communicate with real

00:39:08.440 --> 00:39:16.710
time texts. So, every technology asks us to
confront the questions of whether it expresses

00:39:16.710 --> 00:39:22.930
our human values, a question that demands,
again, that we ask what these are. And this

00:39:22.930 --> 00:39:27.480
makes technology a precious partner as we
think through our futures.

00:39:27.480 --> 00:39:34.099
We're at a moment of significant choice. We've
agreed to a series of experiments. I think

00:39:34.099 --> 00:39:41.740
of them as forbidden experiments on ourselves.
Robots to mind children and the elderly. Technologies

00:39:41.740 --> 00:39:48.290
that denigrate and deny privacy. Seductive
simulations that present themselves as real

00:39:48.290 --> 00:39:52.520
places to live. I think we deserve better.

00:39:52.520 --> 00:39:58.171
And when we remind ourselves that it's we
who decide how to keep technology busy. When

00:39:58.171 --> 00:40:03.470
we remind ourselves that it's not what technology
wants, it's what we want, I think we'll have

00:40:03.470 --> 00:40:09.869
better. And I just wanna end with several
points, some general and some specific for

00:40:09.869 --> 00:40:15.410
business. The general comment is just remember
what is the moment where you're at.

00:40:15.410 --> 00:40:23.009
And I think I said this in the beginning.
It sums up in the phrase, "Early days, just

00:40:23.009 --> 00:40:28.569
because you grew up the internet, just because
you grew up with the internet, don't think

00:40:28.569 --> 00:40:34.089
the internet is grown-up. It is time to make
the corrections. It is a moment of opportunity."

00:40:34.089 --> 00:40:43.269
Second, do not fall into the trap of allowing
yourself to use the phrase, "It's an addiction."

00:40:43.269 --> 00:40:47.250
When people talk about "I'm addicted to this,
I'm addicted to that, I'm addicted to Facebook,

00:40:47.250 --> 00:40:54.369
I'm addicted to Google", it gets you--. If
you're addicted to heroin, one thing to do,

00:40:54.369 --> 00:41:05.309
get off heroin. When people talk about being
addicted to Facebook, Google, surfing, searching,

00:41:05.309 --> 00:41:11.859
it makes them feel impotent and hopeless.

00:41:11.859 --> 00:41:19.990
We're not going to give up this technology.
We have to learn to live with it in a healthy,

00:41:19.990 --> 00:41:26.690
productive way. It's like food. It's more
like saying you can't control--. You're having

00:41:26.690 --> 00:41:30.849
a problem with the cookies. Well, let's really
think together about the cookies. I mean,

00:41:30.849 --> 00:41:36.200
you're having a problem with food? We have
to think about how we're live most healthfully

00:41:36.200 --> 00:41:37.310
with food.

00:41:37.310 --> 00:41:43.099
That's been a problem for a lot of people
over a long period of time. Sometimes, it

00:41:43.099 --> 00:41:49.930
can feel like an addiction. But this technology
is here to stay. And when people use this

00:41:49.930 --> 00:41:54.970
phrase, "the horse if out of the barn", it's
over. It's over. Privacy is over. Privacy

00:41:54.970 --> 00:42:03.069
is not over. We need privacy. Without privacy,
really you have some choices between dictatorship,

00:42:03.069 --> 00:42:04.069
demagogues.

00:42:04.069 --> 00:42:10.920
Privacy's not over. We have to just learn
how to live with this technology in the kind

00:42:10.920 --> 00:42:13.690
of privacy we need to have the kind of open
society we need. And there's like, "privacy

00:42:13.690 --> 00:42:16.839
is over". You have to learn how to live with
this. So all "the horses out of the barn",

00:42:16.839 --> 00:42:21.881
"we're addicted". They're just ways of deferring
and not dealing with the problem.

00:42:21.881 --> 00:42:27.920
OK, that's are my general--. That's my really
general thing. And then, things on a business

00:42:27.920 --> 00:42:32.910
level that I think you really need to be thinking
about, I've made reference to them, but I

00:42:32.910 --> 00:42:38.359
really wanna mark the moment. This thing about
asking ourselves simple questions because

00:42:38.359 --> 00:42:43.549
you want instant answers on email because
the volume and velocity of texting.

00:42:43.549 --> 00:42:48.269
You simply need to create environments where
we can ask the more complicated questions.

00:42:48.269 --> 00:42:53.700
Universities all over the country are just
turning off that wireless in the classroom.

00:42:53.700 --> 00:42:59.760
And because you're asking your students a
complicated question, you cannot afford to

00:42:59.760 --> 00:43:03.869
have them shopping while they're trying to
think of the answer.

00:43:03.869 --> 00:43:11.640
I mean, you really do need--. Some things
just need full attention. And our problems

00:43:11.640 --> 00:43:16.980
are so great that we just need to get back
into the habit of making some spaces for full

00:43:16.980 --> 00:43:23.599
attention. And second, we are too connected
to think. We need time for reflection, real

00:43:23.599 --> 00:43:29.910
reflection. We're not taking the time to think
because we're too overwhelmed by the volume

00:43:29.910 --> 00:43:31.900
and velocity of our constant connection.

00:43:31.900 --> 00:43:38.530
And this is a problem that we're gonna face
together in the next decade. We all need to

00:43:38.530 --> 00:43:43.130
be partners and it's not about liking technology
or not liking technology. It's just about

00:43:43.130 --> 00:43:51.369
forming a right kind of partnership with it.
Thank you.

00:43:51.369 --> 00:43:52.369
[applause]

00:43:52.369 --> 00:43:57.390
Thank you. And I'd be thrilled to take your
questions. Hoping to have questions.

00:43:57.390 --> 00:43:58.740
&gt;&gt;Male Audience Member #3: Hi.

00:43:58.740 --> 00:43:59.740
&gt;&gt;Sherry Turkle: Hi.

00:43:59.740 --> 00:44:04.710
&gt;&gt;Male Audience Member #3: So first of all,
thank you very much for the talk. It was very

00:44:04.710 --> 00:44:09.460
thinking enticing. And I think [clears throat]
one of the biggest things in the case for

00:44:09.460 --> 00:44:17.400
me to take away is that we simply need to
be aware of these things and be thinking about

00:44:17.400 --> 00:44:19.500
what we're doing.

00:44:19.500 --> 00:44:25.519
I do have a question regarding your "this
is people's work" statement.

00:44:25.519 --> 00:44:26.519
&gt;&gt;Sherry Turkle: This is--.

00:44:26.519 --> 00:44:27.519
&gt;&gt;Male Audience Member #3: This is people's
work. So--.

00:44:27.519 --> 00:44:28.650
&gt;&gt;Sherry Turkle: People's work.

00:44:28.650 --> 00:44:30.130
&gt;&gt;Male Audience Member #3: Yeah.

00:44:30.130 --> 00:44:32.310
&gt;&gt;Male Audience Member #4: Taking care of
the elderly.

00:44:32.310 --> 00:44:33.310
&gt;&gt;Male Audience Member #3: Taking care of
the elderly.

00:44:33.310 --> 00:44:34.310
&gt;&gt;Sherry Turkle: Ah, ah, ah, ah. Right, right,
right.

00:44:34.310 --> 00:44:39.603
&gt;&gt;Male Audience Member #3: So, it seems to
me that dealing with technology is something

00:44:39.603 --> 00:44:40.603
that's replacing some things that always happened,
right?

00:44:40.603 --> 00:44:41.603
&gt;&gt;Sherry Turkle: Right.

00:44:41.603 --> 00:44:42.603
&gt;&gt;Male Audience Member #3: Because in the
past, little kids had dolls. I have a nephew,

00:44:42.603 --> 00:44:50.500
2 years old. He has a doll. He puts her to
sleep. He feeds her.

00:44:50.500 --> 00:44:52.200
&gt;&gt;Sherry Turkle: Right.

00:44:52.200 --> 00:44:58.839
&gt;&gt;Male Audience Member #3: He takes care of
it as if it's a human being. I have friends

00:44:58.839 --> 00:45:00.170
who have pets and they talk to them. They
share everything with their pets.

00:45:00.170 --> 00:45:01.170
&gt;&gt;Sherry Turkle: Right.

00:45:01.170 --> 00:45:08.040
&gt;&gt;Male Audience Member #3: And these phenomenons
have always been the same, but maybe technology

00:45:08.040 --> 00:45:12.380
made it more vast and more common and allowed
people from all levels--.

00:45:12.380 --> 00:45:15.190
&gt;&gt;Sherry Turkle: So why do I think the social
robots are different?

00:45:15.190 --> 00:45:16.240
&gt;&gt;Male Audience Member #3: Yeah.

00:45:16.240 --> 00:45:20.160
&gt;&gt;Sherry Turkle: OK. That's a great question.
Because one of the things that I've been able

00:45:20.160 --> 00:45:27.380
to do is study dolls and social robots in
parallel. And what's the psychology of playing

00:45:27.380 --> 00:45:31.109
with dolls and what's the psychology of playing
with social robots. [sniffs]

00:45:31.109 --> 00:45:36.500
When you have a doll, and when, actually when
psychotherapists use dolls in therapy, what

00:45:36.500 --> 00:45:41.079
a doll allows you to do, the affordance, the
technological affordance of the doll, is that

00:45:41.079 --> 00:45:46.770
it allows for projection. So if a child is
feeling guilty--let's say a girl. She puts

00:45:46.770 --> 00:45:53.099
her Barbies in a row and she puts them in
detention. In other words, she feels guilty.

00:45:53.099 --> 00:45:59.609
She does unto the Barbie what she imagines
should be done unto herself. If you have a

00:45:59.609 --> 00:46:06.119
doll that's telling you how it feels, that's
making demands on you, that has its own needs,

00:46:06.119 --> 00:46:11.789
its own agenda, it's own -- it's a sociable--.
That's what it means to have a sociable doll.

00:46:11.789 --> 00:46:16.750
It's its own little--. It doesn't leave the
child that space.

00:46:16.750 --> 00:46:24.099
It takes the air out of the relationship from
that point of view and it's another demanding

00:46:24.099 --> 00:46:33.680
creature. So, it's, it's not serving-- before
we get into what other possible problems it

00:46:33.680 --> 00:46:43.539
might be causing, it's not allowing for the
wonderful things that your niece and nephew

00:46:43.539 --> 00:46:50.230
are doing with their stuffed animals, their
stuffed bears, their dolls, their pets.

00:46:50.230 --> 00:46:58.569
So, on the other hand, because of the special
seductions that it has, and it's built to

00:46:58.569 --> 00:47:08.240
be seductive, it draws the child in because
it makes the child feel recognized. And unlike

00:47:08.240 --> 00:47:16.520
the Barbies, when it asks for nurturance,
it really asks for nurturance. And unlike

00:47:16.520 --> 00:47:24.180
the Barbie when it asks for nurturance, it
says "thank you" back and forges this bond,

00:47:24.180 --> 00:47:27.220
but it's a bond that goes no place.

00:47:27.220 --> 00:47:33.540
So it forges a bond, but it's not a bond that
can then develop because really the doll has

00:47:33.540 --> 00:47:38.890
nothing to give back. So, what do I think?
Do I think these dolls will be out there?

00:47:38.890 --> 00:47:44.680
Do I think these dolls will have their place?
Do I think--. Of course. But I'm just saying,

00:47:44.680 --> 00:47:52.420
"Attention. You're playing with--." I don't
want to say you're playing with fire.

00:47:52.420 --> 00:48:00.759
You're playing with very fundamental processes
when you get into two-, three-, four-, five-year

00:48:00.759 --> 00:48:08.819
olds and how they deal with nurturance and
bonding and dolls. In other words, you can't

00:48:08.819 --> 00:48:14.809
have it both ways. You can't have, you can't
say you're creating a radical technology that's

00:48:14.809 --> 00:48:19.180
dealing with the most powerful processes of
socialization and then say, "Oh, well it's

00:48:19.180 --> 00:48:20.400
only a toy."

00:48:20.400 --> 00:48:25.329
And that's what I, that's what I object to
in the sociable robotics discourse. It's like,

00:48:25.329 --> 00:48:30.720
"Oh, we're dealing with the most powerful
processes of socialization. Oh, it's only

00:48:30.720 --> 00:48:35.910
a toy." And for the older people, there there's
gonna be a lot of room for disagreement. Be

00:48:35.910 --> 00:48:39.390
a lot of room for disagreement. I think where
the disagreement is gonna come, it's gonna

00:48:39.390 --> 00:48:44.279
come on Alzheimer's, it's gonna become in
the area of dementia.

00:48:44.279 --> 00:48:51.960
And there I think there's gonna be room for
a lot of disagreement in, in, in really a

00:48:51.960 --> 00:49:00.099
spirit of good will and honest disagreement.
That when people don't know who they're talking

00:49:00.099 --> 00:49:06.119
to, when you could argue they don't know who
they're talking to, does it matter who or--.

00:49:06.119 --> 00:49:14.599
Is it a who or is it an it? And I think there's
already some very interesting, very responsible

00:49:14.599 --> 00:49:15.829
conversation about that.

00:49:15.829 --> 00:49:23.359
But when you go into a nursing home and you--.
I mean, my daughter brings dogs and people

00:49:23.359 --> 00:49:29.070
to work with the dogs and the people together
in nursing homes in Cambridge. And she can't

00:49:29.070 --> 00:49:34.330
get people to go in. She tries to organize
the program to bring people in to working

00:49:34.330 --> 00:49:38.650
in this nursing home. It's time to bring some
people into this nursing home.

00:49:38.650 --> 00:49:45.619
And the argument for bringing in the robots
is that there are no people for these jobs.

00:49:45.619 --> 00:49:47.349
And there are a lot of people for these jobs
if they were paid and given compensation and

00:49:47.349 --> 00:49:57.930
trained to do it. One of the things I come
down very hard on, I mean, the one place in

00:49:57.930 --> 00:50:03.980
the book I do sound a little bit schoolmarmy
really is that argument there are no people--.

00:50:03.980 --> 00:50:09.019
Every robotics presentation I go to at these
conferences starts with this, "There are no

00:50:09.019 --> 00:50:13.880
people for these jobs. We need robots." And
when you look at the economics of this particular

00:50:13.880 --> 00:50:19.470
industry, plenty of people, just not enough
compensation and benefits. But there's gonna

00:50:19.470 --> 00:50:25.310
be room for disagreement on dementia. I mean,
but I think there should be responsible conversation

00:50:25.310 --> 00:50:28.569
about that. But Nanny-bots, I'm gonna take
a hard line.

00:50:28.569 --> 00:50:36.559
&gt;&gt;Male Audience Member #5: Thank you for coming.
My question is your thesis seems to be based

00:50:36.559 --> 00:50:43.319
on some kind of psychological and biological
determinism that we are fixed in how we deal

00:50:43.319 --> 00:50:47.940
with the world. We look at a robot and we
think it's a person when I know for a fact

00:50:47.940 --> 00:50:52.240
we can be reprogrammed, right? People were
programmed to deal, to treat slaves as some

00:50:52.240 --> 00:50:53.550
other, as the other.

00:50:53.550 --> 00:50:58.230
The Nazi's programmed Germans to treat Jews
as the other. So it's not like some of these

00:50:58.230 --> 00:51:04.569
are completely fixed. So the real question
is, and we had technological events that have

00:51:04.569 --> 00:51:11.390
changed our social interaction: radio, TV,
the move to the suburbs. So the real question

00:51:11.390 --> 00:51:17.650
is why do you think this is an apocalyptic
change and why do you think that we will not

00:51:17.650 --> 00:51:23.900
be able to compensate for it? And what do
you think that form of compensation will be?

00:51:23.900 --> 00:51:32.529
&gt;&gt;Sherry Turkle: Well, I'm arguing that, I'm
arguing that we will if we have our wits about

00:51:32.529 --> 00:51:49.839
us. That if what guides us is our, is where
we want it to go, we will form relationships

00:51:49.839 --> 00:51:57.480
with robots that will be in our best interest.
But I think it's, I'd put first, "where do

00:51:57.480 --> 00:52:04.940
we want it to go?" as opposed to "we will
somehow go someplace with it".

00:52:04.940 --> 00:52:09.809
I think that's the difference between -- Kevin
Kelly and I are very close friends and colleagues

00:52:09.809 --> 00:52:15.789
and I think the difference between us is that
he thinks that together we will evolve into

00:52:15.789 --> 00:52:25.220
something--what technology wants. And I think
that there is questions of human choice here

00:52:25.220 --> 00:52:26.759
about where do we want it to go.

00:52:26.759 --> 00:52:31.900
So I do think that we're going to evolve into
new kinds of partnership with technology.

00:52:31.900 --> 00:52:41.690
But I think that the question of whether,
for example, you--. The book begins with David

00:52:41.690 --> 00:52:47.200
Levy practically dedicating the book to me
because he thinks that I will, here's a good

00:52:47.200 --> 00:52:53.640
example. In, in, in 1980, the early 1980s,
I interviewed a lot of computer hackers who

00:52:53.640 --> 00:52:58.390
were socially, feeling socially left out at
MIT.

00:52:58.390 --> 00:53:03.530
And this one guy, called Anthony, it's not
his real name, it's his made-up name, who

00:53:03.530 --> 00:53:10.849
said he felt that he really couldn't get along
with girls. This was a limitation. And David

00:53:10.849 --> 00:53:12.830
Levy wrote a book. He's a computer scientist
and AI person. He wrote a book called, "Love

00:53:12.830 --> 00:53:17.220
and Sex with Robots," in which he talks about
marriage to a robot and he dedicated the book

00:53:17.220 --> 00:53:23.900
to Anthony because he thought that this would
be a perfect solution for Anthony.

00:53:23.900 --> 00:53:31.710
And he sends me a copy of the book. "To Sherry
Turkle's Anthony." I should give it to Anthony.

00:53:31.710 --> 00:53:35.819
This is a perfect--and he writes in the book,
"This is a perfect solution for Sherry Turkle's

00:53:35.819 --> 00:53:40.859
Anthony. We will evolve to a point where we
will happily--everybody like Anthony." In

00:53:40.859 --> 00:53:45.349
other words, people who feel a little bit
socially inept will have a robot to marry.

00:53:45.349 --> 00:53:50.560
We won't have to get over the problems of
being with people. We can all have robot husbands

00:53:50.560 --> 00:53:59.369
and wives. So, that's an example of where,
in David Levy's mind, we're gonna co-evolve

00:53:59.369 --> 00:54:00.369
so that we see robots as adequate partners.
In my mind--.

00:54:00.369 --> 00:54:01.369
&gt;&gt;Male Audience Member #5: Right, but you're
assuming that it's gonna last more than a

00:54:01.369 --> 00:54:06.450
week, right? I mean, there's this assumption
there that this is really gonna work.

00:54:06.450 --> 00:54:11.930
&gt;&gt;Sherry Turkle: I'm just, I'm just saying.
I'm just saying. From your point of view,

00:54:11.930 --> 00:54:17.970
hundred years from now, 150 years from now,
in terms of evolving and reprogramming, I'm

00:54:17.970 --> 00:54:22.579
trying to take your thought to a--. Here's
a guy who thinks as you do, but really takes

00:54:22.579 --> 00:54:25.390
it out there, we'll evolve, the robots will
evolve.

00:54:25.390 --> 00:54:30.380
Kevin Kelly said everybody's gonna co-evolve
and we'll be at a point where we'll see robots

00:54:30.380 --> 00:54:41.240
as adequate partners. And I say [pause] "Why?"
Anthony wanted to be--. He felt like a kid

00:54:41.240 --> 00:54:46.420
at a candy store with his nose pressed up
against the window. He wanted to be let in.

00:54:46.420 --> 00:54:51.289
He wanted to have more skills to be let into
the world where he would feel more comfortable

00:54:51.289 --> 00:54:52.289
with people.

00:54:52.289 --> 00:54:58.369
He didn't want to be told, "You're a little
socially inept. You get to get a robot bride."

00:54:58.369 --> 00:55:05.930
So I say human values first, and then let's
figure out how to, how to live with this technology

00:55:05.930 --> 00:55:12.390
in ways that--. I'm against these substitutions.
So in that sense, I would have to say in all

00:55:12.390 --> 00:55:20.920
honesty that I'm a little bit species o'centric.
I have a species chauvinism in contrast to

00:55:20.920 --> 00:55:24.609
these other writers about technology. [pause]

00:55:24.609 --> 00:55:28.489
That's it. I think that's a fair owning up.

00:55:28.489 --> 00:55:34.940
&gt;&gt;Male Audience Member #6: So, what you've
said makes me think about a technology transition

00:55:34.940 --> 00:55:48.852
that happened in the 1700s. Before that, novels
didn't exist and people didn't read novels.

00:55:48.852 --> 00:55:52.950
And people started reading novels and in particular,
the young people started reading them.

00:55:52.950 --> 00:56:00.259
And the establishment, the older generation,
were shocked and horrified and just very disturbed

00:56:00.259 --> 00:56:09.109
by the reading of novels. They thought that,
that the reading of novels would lead to deep

00:56:09.109 --> 00:56:18.369
perversity, to the destruction of society
as they knew it, to the end of humanity. And

00:56:18.369 --> 00:56:21.950
we came to peace with novels in some way.

00:56:21.950 --> 00:56:28.030
'Cause before that, you couldn't tell a story
with an inanimate object. You couldn't evoke

00:56:28.030 --> 00:56:35.380
emotion. You couldn’t evoke desire. You
couldn't evoke a lot of the human things that

00:56:35.380 --> 00:56:41.019
you are talking about in your analysis here
using inanimate objects. And with novels,

00:56:41.019 --> 00:56:46.950
suddenly that changed. And people were, in
a fundamental sense, rightly disturbed. I

00:56:46.950 --> 00:56:54.140
don't know how we got past that, but we clearly
did because now in fact, if I found my son

00:56:54.140 --> 00:56:56.740
reading a novel, I'd be delighted.

00:56:56.740 --> 00:56:58.329
[laughter]

00:56:58.329 --> 00:57:07.289
So, what I'm curious about is are there any
lessons for us in terms of your research?

00:57:07.289 --> 00:57:18.900
From our experience as a, the race, the human
race, with the comprehension of something

00:57:18.900 --> 00:57:24.730
like novels, fiction, the romantic novel that
came to be in that time.

00:57:24.730 --> 00:57:30.039
&gt;&gt;Sherry Turkle: [pause] Well, I think we're
going to become, and I think we already have

00:57:30.039 --> 00:57:43.970
become, appreciators of [pause] search as
a genre, as an occupation, as a way to explore

00:57:43.970 --> 00:57:53.960
the world. The notion of search, I mean, who
searched? I mean, you had an encyclopedia.

00:57:53.960 --> 00:58:01.359
You got to a static entry. It was boring.
It was long. It was written by people who

00:58:01.359 --> 00:58:03.109
knew where they were.

00:58:03.109 --> 00:58:15.089
But the notion of surfing, the notion of search,
the notion of this liquid float, I think that's

00:58:15.089 --> 00:58:26.660
become a language. Without getting into multitasking
and if you're doing it while you're making

00:58:26.660 --> 00:58:33.109
love, [audience laughter] without getting
into the--. Just getting into the genre thing,

00:58:33.109 --> 00:58:34.730
that you sit down to have an experience that
was unknown [pause] 20 years ago, 15 years

00:58:34.730 --> 00:58:38.460
ago. I mean, it just was an unknown experience.

00:58:38.460 --> 00:58:45.809
And I think all of us remember the first time
you had a connection fast enough that you

00:58:45.809 --> 00:58:49.880
had a--, because you needed a fast enough
connection to have a sense of what it is.

00:58:49.880 --> 00:58:54.211
I mean, it couldn't be that you sat there
and I mean, there just came a point when you

00:58:54.211 --> 00:58:59.240
had a new experience. And it must've been
like, for me, it was like reading--.

00:58:59.240 --> 00:59:05.069
It’s felt like a little bit like reading
that new novel. This was just something radically

00:59:05.069 --> 00:59:20.279
new. And I think it's been a miraculous, fabulous
thing to watch a world discover this new adventure.

00:59:20.279 --> 00:59:25.710
Now, I think just like you learn that if you
just never--. It's like there's a wonderful

00:59:25.710 --> 00:59:31.509
story by Woody Allen, the Kugelmass episode
where he lives inside "Madame Bovary".

00:59:31.509 --> 00:59:39.529
Does anybody know this Woody Allen story?
Anyway, he, he, he, he goes to visit a magician

00:59:39.529 --> 00:59:42.359
and the magician has a machine. And as you
step inside this machine, you get to be in

00:59:42.359 --> 00:59:51.039
any novel you want. So, he chooses a chapter
of "Madame Bovary", where Emma Bovary is making

00:59:51.039 --> 00:59:58.559
love with her lover in a carriage and it's
a very good chapter to be in.

00:59:58.559 --> 01:00:06.110
He, of course, is the lover. But then he's
stuck there in the Kugelmass--. Anyway, he

01:00:06.110 --> 01:00:15.089
can't get out of this. He's in "Madame Bovary".
And I think that just as you don't wanna be

01:00:15.089 --> 01:00:25.730
stuck in the novel, that it was great to be
in that one chapter, but then he's stuck with

01:00:25.730 --> 01:00:30.380
Emma Bovary and she starts to just be the
bitch she really is in "Madame Bovary".

01:00:30.380 --> 01:00:31.380
[laughter]

01:00:31.380 --> 01:00:37.740
He just wanted to be in the carriage with
her, but then there's the next chapter. You

01:00:37.740 --> 01:00:44.200
don't wanna be stuck in constant search. I
mean, I think that's the issue. If all you're

01:00:44.200 --> 01:00:50.239
doing all day is searchin' and surfin', we
have to learn to live with the novel and we

01:00:50.239 --> 01:00:54.420
have to learn to live with the capacity for
search and do other things, too.

01:00:54.420 --> 01:00:59.529
And I think that's where we're at now. We're
in this very rapidly learning curve, but we

01:00:59.529 --> 01:01:03.690
have to recognize it for what it is. It's
a very rapid learning curve of learning how

01:01:03.690 --> 01:01:09.519
to live with this very powerful new device.
It's very exciting, but it also needs us to

01:01:09.519 --> 01:01:17.450
be, it needs us to be aware and it needs us
to be not afraid to say we're doing something

01:01:17.450 --> 01:01:18.450
very hard.

01:01:18.450 --> 01:01:25.220
We're doing something very new, very complicated.
And we can't get jumpy and all nervous if

01:01:25.220 --> 01:01:32.309
somebody says, 'Think about it." So that's
what I'm saying. [pause]

01:01:32.309 --> 01:01:35.690
&gt;&gt;Female Audience Member #7: Hi.

01:01:35.690 --> 01:01:37.720
&gt;&gt;Sherry Turkle: Hi.

01:01:37.720 --> 01:01:48.549
&gt;&gt;Female Audience Member #7: Thank you for
your talk and thank you for coming. My question

01:01:48.549 --> 01:02:05.359
is how do you see this conversation evolving
because technology is so seductive. And with

01:02:05.359 --> 01:02:12.450
your example with food, food is also very
seductive and obesity is a rising problem.

01:02:12.450 --> 01:02:32.050
And so, how is it that we can force the conversation
to go in a way that seems unattractive? I

01:02:32.050 --> 01:02:40.410
tend to keep my phone off. I don't check my
email very often. I like writing long emails

01:02:40.410 --> 01:02:46.220
and having conversations with people and when
I do do that, when my friends--

01:02:46.220 --> 01:02:49.789
&gt;&gt;Sherry Turkle: The hostility towards you--

01:02:49.789 --> 01:03:00.280
&gt;&gt;Female Audience #7: It is. It's like everyone's
like, "Why couldn't I reach you? Is something

01:03:00.280 --> 01:03:08.300
wrong with your phone? [audience laughter]
Is something wrong with you?" And I'm the

01:03:08.300 --> 01:03:17.410
one who's an anachronism. So how is it that
we can have this conversation at all when

01:03:17.410 --> 01:03:24.770
the alternative just like, letting it evolve
and letting ourselves sink into technology

01:03:24.770 --> 01:03:27.040
is so much better?

01:03:27.040 --> 01:03:36.140
&gt;&gt;Sherry Turkle: Well, I think we're having
this conversation now. I mean, I'm only one

01:03:36.140 --> 01:03:50.339
person and there's only one book. I'm not
saying I wrote the--. But I think that the

01:03:50.339 --> 01:03:55.640
time is right. I think we've lived with this
now for--.

01:03:55.640 --> 01:03:56.640
We had to live with this first. I think that
we had to have a little experience. I think

01:03:56.640 --> 01:03:57.640
you can't like, have Google for 15 minutes
and then say, "Oh my God. I see ten of Facebook

01:03:57.640 --> 01:03:58.640
for five years. And then I see a problem."
I mean, I think you have to live with it.

01:03:58.640 --> 01:03:59.640
And I think people then have to say, "Well,
I see--." I had a friend who has a 13-year

01:03:59.640 --> 01:04:00.640
old daughter.

01:04:00.640 --> 01:04:01.640
And here's just an interesting example of
how, I think, people start to see it in ways

01:04:01.640 --> 01:04:02.640
that are surprising. And it can't always be
the same old story of, "Oh, my boyfriend put

01:04:02.640 --> 01:04:03.640
away his[ ]." This is an interesting example.
So she tells me about a birthday party and

01:04:03.640 --> 01:04:04.640
she has a 15-year old daughter. And 15-year
olds, there's a point at a 15-year old party

01:04:04.640 --> 01:04:05.640
where there's a lull and then they have to
work through it.

01:04:05.640 --> 01:04:06.640
And that's part of the point. They work through
it. But now they're all on their phones and

01:04:06.640 --> 01:04:07.640
so they all contact elsewhere. And they all
left because they all had else--. I mean,

01:04:07.640 --> 01:04:08.640
they all could bail. And I thought, I forget
if I told you this story. But I mean, there

01:04:08.640 --> 01:04:09.640
was something about that story and its particularity.
And you know that sense of always bailing.

01:04:09.640 --> 01:04:10.640
I think we're gonna start to see, just in
our everyday lives as we really have experiences,

01:04:10.640 --> 01:04:13.250
that we need to just begin to strike a balance.
And I think you're gonna see more and more

01:04:13.250 --> 01:04:21.940
people just saying, "We're giving up on something
here." So I think that it's not gonna be,

01:04:21.940 --> 01:04:25.509
it's gonna start with small things. It's gonna
start with people not taking calls.

01:04:25.509 --> 01:04:34.630
It's gonna start--. First of all, kids are
starting to complain. So just as my generation

01:04:34.630 --> 01:04:38.759
rebelled against how their parents treated
them. Then you had a group of parents who

01:04:38.759 --> 01:04:42.539
treated their children differently and their
parents. So that generation did the opposite.

01:04:42.539 --> 01:04:49.940
The children I am interviewing now, their
parents texted at dinner.

01:04:49.940 --> 01:04:59.829
And they work on--. Let me prep these. I'm
not sure the ages of everybody here, but the

01:04:59.829 --> 01:05:07.230
children I'm interviewing-- 8, 9, 10, 11,
12, 13, 14, 15-- their parents are texting

01:05:07.230 --> 01:05:15.869
at dinner. They are begging their mothers
to make shorter meals so that maybe then their

01:05:15.869 --> 01:05:18.680
fathers won't text during dinner. Their fathers
are texting on Sunday watching the games with

01:05:18.680 --> 01:05:19.680
them.

01:05:19.680 --> 01:05:24.349
Their mothers are having their phones out
while they're reading to them. Their mothers

01:05:24.349 --> 01:05:29.460
are pushing the swings with one hand and have
the phone in the other hand. I do fieldwork

01:05:29.460 --> 01:05:35.509
in a playground where there's texting and
they're watching the jungle gym, texting.

01:05:35.509 --> 01:05:39.150
The kid is going like this on the jungle gym
and the mother can't look up.

01:05:39.150 --> 01:05:43.950
These children are not gonna let their children
text, are not gonna text with their kids.

01:05:43.950 --> 01:05:48.880
So I mean, you talk about the accommodation
is that there's gonna be an evolution so that

01:05:48.880 --> 01:05:59.559
in a few years--. Last night on Steven Colbert,
I was describing this and he said, "Oh, why

01:05:59.559 --> 01:06:03.019
don't they just change it?" He says, "Why
don't they just change their Facebook status

01:06:03.019 --> 01:06:04.019
to 'abandoned'?"

01:06:04.019 --> 01:06:05.019
[laughter]

01:06:05.019 --> 01:06:08.779
You're gonna have, you're gonna have those
children not doing it. So I think we're gonna

01:06:08.779 --> 01:06:14.380
get--. So, I'm optimistic despite everything
that you read about, the reviews you read

01:06:14.380 --> 01:06:18.470
about me, I'm optimistic. I think that we're
gonna get better at this.

01:06:18.470 --> 01:06:23.339
And I think we're gonna have these robots
in these nursing homes and we're gonna start

01:06:23.339 --> 01:06:29.539
to say, "Couldn't we spend some money on training
some people? These robots – this doesn't

01:06:29.539 --> 01:06:33.910
-- this is starting to feel yucky." When you
really watch, I mean, how many people have

01:06:33.910 --> 01:06:45.359
had the experience of really watching a woman
who's lost a son, try to, explaining that,

01:06:45.359 --> 01:06:48.810
explaining that to a robot that's going, "Uh-huh.
Uh-huh. Uh-huh."

01:06:48.810 --> 01:06:57.609
Now, I have had that experience. It totally
creeped me out. But not that many people have

01:06:57.609 --> 01:07:04.990
really sat there and, and had that experience.
And I'm telling you, when more of us have

01:07:04.990 --> 01:07:08.710
had that experience of people pouring out
their little hearts to robots who you know

01:07:08.710 --> 01:07:19.299
understand nothing, I think we can say, I
say send in some college freshmen to this--.

01:07:19.299 --> 01:07:25.940
I say really mobilize and have an AmeriCorps
program for this. This is just not, this is

01:07:25.940 --> 01:07:33.200
not a happy feeling, here. And I think we're
gonna get more sophisticated about this. So

01:07:33.200 --> 01:07:35.780
I say stand your ground. OK.

01:07:35.780 --> 01:07:42.029
&gt;&gt;Male Audience Member #8: Yeah, I sort of
hope this is true, although I'm not sure that

01:07:42.029 --> 01:07:46.700
I [clears throat] am all that optimistic.
It seems to me that as technologists, we've

01:07:46.700 --> 01:07:51.220
effectively realized Joe Weizenbaum's worst
nightmares and we're living through it right

01:07:51.220 --> 01:07:58.410
now. And I see this in my own children and
how they interact with their people and also

01:07:58.410 --> 01:08:02.559
with younger people here at Google and so
on. And I'm wondering if we've gone past that

01:08:02.559 --> 01:08:03.559
point of no return.

01:08:03.559 --> 01:08:06.470
&gt;&gt;Sherry Turkle: Well, OK. This is what I--.
When I hear "the horse is out of the barn",

01:08:06.470 --> 01:08:17.430
read the book. But when I hear "the horse
is out of the barn", I mean, I taught with

01:08:17.430 --> 01:08:23.449
Joe and I got to MIT. I taught courses with
Joe and he felt that the story was over in

01:08:23.449 --> 01:08:33.330
1976. And I really believe that whenever we
take the position "the horse is out of the

01:08:33.330 --> 01:08:39.310
barn", "it's done", "we're addicted", "the
children are ruined","it’s over", we can't

01:08:39.310 --> 01:08:41.870
afford it. We can't afford it.

01:08:41.870 --> 01:08:49.989
It's like in politics. I mean, we've had and
we're gonna have some very, very dark days.

01:08:49.989 --> 01:09:01.230
And if you take the position it's over, you
can't--. We can't afford it. So I, who've

01:09:01.230 --> 01:09:10.520
had some very dark days writing this book,
I mean, as you can tell that I—[pause].

01:09:10.520 --> 01:09:13.730
Nanny-bots, they don't ring my bell.

01:09:13.730 --> 01:09:21.449
And going to conferences where people are
talking about the--. You travel in, in, in

01:09:21.449 --> 01:09:29.060
Japan. Is there a problem with Nanny-bots?
Is there a problem with elder-care bots? And

01:09:29.060 --> 01:09:34.020
teacher bots? I mean, these are like, it's
like, does anybody see a problem? Nobody raises

01:09:34.020 --> 01:09:40.900
their hand. I mean, there's not even a language
for discussing what the problem might be.

01:09:40.900 --> 01:09:50.380
I mean, sometimes you even need to struggle
to find a way to describe what the problem

01:09:50.380 --> 01:09:58.900
might be. Because, really, notions about individual
autonomy and privacy, these are not--. Here,

01:09:58.900 --> 01:10:07.909
I'm still speaking with a common, shared sense
of what these things might be. So, rave reviews

01:10:07.909 --> 01:10:15.520
about sex bots. No problem. I mean, not even
a conversation about why that might be a problem.

01:10:15.520 --> 01:10:26.239
So, I'm not saying that I don't share some
of your trepidation. But I'm absolutely convinced

01:10:26.239 --> 01:10:33.800
that we have not begun to have the conversation
we need to have, when still, a book like this,

01:10:33.800 --> 01:10:42.380
which is not an angry book. It's a, "Hey,
let’s talk" book is reviewed, "She doesn't

01:10:42.380 --> 01:10:49.650
like technology. She doesn't like technology,
I don't like her."

01:10:49.650 --> 01:10:54.120
And we can do better as a conversation. I
mean, not that some people haven't liked the

01:10:54.120 --> 01:11:00.290
book, but for people to respond that way means
that we have to work harder to have that conversation.

01:11:00.290 --> 01:11:01.739
[pause] Thank you very much. It's really been
a pleasure.

01:11:01.739 --> 01:11:01.820
[applause]

