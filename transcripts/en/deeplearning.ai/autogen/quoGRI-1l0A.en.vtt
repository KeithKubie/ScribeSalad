WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:01.760
in the last video you saw how the

00:00:01.760 --> 00:00:01.770
in the last video you saw how the
 

00:00:01.770 --> 00:00:05.720
in the last video you saw how the
attention model allows a neural network

00:00:05.720 --> 00:00:05.730
attention model allows a neural network
 

00:00:05.730 --> 00:00:08.150
attention model allows a neural network
to pay attention to only part of an

00:00:08.150 --> 00:00:08.160
to pay attention to only part of an
 

00:00:08.160 --> 00:00:10.430
to pay attention to only part of an
input sentence while generating a

00:00:10.430 --> 00:00:10.440
input sentence while generating a
 

00:00:10.440 --> 00:00:13.009
input sentence while generating a
translation much like a human translator

00:00:13.009 --> 00:00:13.019
translation much like a human translator
 

00:00:13.019 --> 00:00:13.820
translation much like a human translator
might

00:00:13.820 --> 00:00:13.830
might
 

00:00:13.830 --> 00:00:16.400
might
let's now formalize the intuition into

00:00:16.400 --> 00:00:16.410
let's now formalize the intuition into
 

00:00:16.410 --> 00:00:18.500
let's now formalize the intuition into
the exact details of how you would

00:00:18.500 --> 00:00:18.510
the exact details of how you would
 

00:00:18.510 --> 00:00:22.070
the exact details of how you would
implement an attention model so same as

00:00:22.070 --> 00:00:22.080
implement an attention model so same as
 

00:00:22.080 --> 00:00:24.920
implement an attention model so same as
in the previous video let's assume you

00:00:24.920 --> 00:00:24.930
in the previous video let's assume you
 

00:00:24.930 --> 00:00:27.380
in the previous video let's assume you
have an input sentence and you use a

00:00:27.380 --> 00:00:27.390
have an input sentence and you use a
 

00:00:27.390 --> 00:00:31.310
have an input sentence and you use a
bi-directional RNN or bi-directional GRE

00:00:31.310 --> 00:00:31.320
bi-directional RNN or bi-directional GRE
 

00:00:31.320 --> 00:00:33.950
bi-directional RNN or bi-directional GRE
or bi-directional STM to compute videos

00:00:33.950 --> 00:00:33.960
or bi-directional STM to compute videos
 

00:00:33.960 --> 00:00:37.880
or bi-directional STM to compute videos
on every words in practice gr use and LS

00:00:37.880 --> 00:00:37.890
on every words in practice gr use and LS
 

00:00:37.890 --> 00:00:41.000
on every words in practice gr use and LS
TMS are often used for this but maybe

00:00:41.000 --> 00:00:41.010
TMS are often used for this but maybe
 

00:00:41.010 --> 00:00:45.799
TMS are often used for this but maybe
I'll steal more common and so for the

00:00:45.799 --> 00:00:45.809
I'll steal more common and so for the
 

00:00:45.809 --> 00:00:49.130
I'll steal more common and so for the
for recurrence you would have a for

00:00:49.130 --> 00:00:49.140
for recurrence you would have a for
 

00:00:49.140 --> 00:00:52.369
for recurrence you would have a for
recurrence first time step activation

00:00:52.369 --> 00:00:52.379
recurrence first time step activation
 

00:00:52.379 --> 00:00:55.000
recurrence first time step activation
backward recurrence first time step

00:00:55.000 --> 00:00:55.010
backward recurrence first time step
 

00:00:55.010 --> 00:00:58.069
backward recurrence first time step
activation for the for recurrent second

00:00:58.069 --> 00:00:58.079
activation for the for recurrent second
 

00:00:58.079 --> 00:01:01.459
activation for the for recurrent second
time step activation backward and so on

00:01:01.459 --> 00:01:01.469
time step activation backward and so on
 

00:01:01.469 --> 00:01:03.229
time step activation backward and so on
there isn't one for all of them in this

00:01:03.229 --> 00:01:03.239
there isn't one for all of them in this
 

00:01:03.239 --> 00:01:08.060
there isn't one for all of them in this
a form with fifth time step a back with

00:01:08.060 --> 00:01:08.070
a form with fifth time step a back with
 

00:01:08.070 --> 00:01:10.310
a form with fifth time step a back with
fifth times there well you know we had a

00:01:10.310 --> 00:01:10.320
fifth times there well you know we had a
 

00:01:10.320 --> 00:01:13.070
fifth times there well you know we had a
a zero here technically we could also

00:01:13.070 --> 00:01:13.080
a zero here technically we could also
 

00:01:13.080 --> 00:01:18.499
a zero here technically we could also
have a I guess a backwards six as a

00:01:18.499 --> 00:01:18.509
have a I guess a backwards six as a
 

00:01:18.509 --> 00:01:20.330
have a I guess a backwards six as a
factor of all zero actually does a

00:01:20.330 --> 00:01:20.340
factor of all zero actually does a
 

00:01:20.340 --> 00:01:23.899
factor of all zero actually does a
vector of all zeros and then to simplify

00:01:23.899 --> 00:01:23.909
vector of all zeros and then to simplify
 

00:01:23.909 --> 00:01:28.100
vector of all zeros and then to simplify
the notation going forward at every time

00:01:28.100 --> 00:01:28.110
the notation going forward at every time
 

00:01:28.110 --> 00:01:31.249
the notation going forward at every time
step even though you have the features

00:01:31.249 --> 00:01:31.259
step even though you have the features
 

00:01:31.259 --> 00:01:33.380
step even though you have the features
computer from the for recurrence and

00:01:33.380 --> 00:01:33.390
computer from the for recurrence and
 

00:01:33.390 --> 00:01:36.469
computer from the for recurrence and
from the backward recurrence in a

00:01:36.469 --> 00:01:36.479
from the backward recurrence in a
 

00:01:36.479 --> 00:01:39.380
from the backward recurrence in a
bi-directional RNN I'm just going to use

00:01:39.380 --> 00:01:39.390
bi-directional RNN I'm just going to use
 

00:01:39.390 --> 00:01:45.249
bi-directional RNN I'm just going to use
a of T to represent both of these

00:01:45.249 --> 00:01:45.259
a of T to represent both of these
 

00:01:45.259 --> 00:01:46.670
a of T to represent both of these
concatenated together

00:01:46.670 --> 00:01:46.680
concatenated together
 

00:01:46.680 --> 00:01:48.800
concatenated together
so a T is going to be our feature vector

00:01:48.800 --> 00:01:48.810
so a T is going to be our feature vector
 

00:01:48.810 --> 00:01:54.050
so a T is going to be our feature vector
for time step T although to be

00:01:54.050 --> 00:01:54.060
for time step T although to be
 

00:01:54.060 --> 00:01:55.880
for time step T although to be
consistent with notation were used in a

00:01:55.880 --> 00:01:55.890
consistent with notation were used in a
 

00:01:55.890 --> 00:01:58.399
consistent with notation were used in a
second I'm going to call this T prime as

00:01:58.399 --> 00:01:58.409
second I'm going to call this T prime as
 

00:01:58.409 --> 00:02:00.530
second I'm going to call this T prime as
you're gonna use T prime to index into

00:02:00.530 --> 00:02:00.540
you're gonna use T prime to index into
 

00:02:00.540 --> 00:02:06.080
you're gonna use T prime to index into
the words in the French sentence next we

00:02:06.080 --> 00:02:06.090
the words in the French sentence next we
 

00:02:06.090 --> 00:02:09.499
the words in the French sentence next we
have our forward only so it's a single

00:02:09.499 --> 00:02:09.509
have our forward only so it's a single
 

00:02:09.509 --> 00:02:12.940
have our forward only so it's a single
Direction RNN which state

00:02:12.940 --> 00:02:12.950
Direction RNN which state
 

00:02:12.950 --> 00:02:17.240
Direction RNN which state
to generate the translation and so the

00:02:17.240 --> 00:02:17.250
to generate the translation and so the
 

00:02:17.250 --> 00:02:20.240
to generate the translation and so the
first time step it should generate y1

00:02:20.240 --> 00:02:20.250
first time step it should generate y1
 

00:02:20.250 --> 00:02:24.559
first time step it should generate y1
and this will have as input some context

00:02:24.559 --> 00:02:24.569
and this will have as input some context
 

00:02:24.569 --> 00:02:29.720
and this will have as input some context
C and if you want to index it with time

00:02:29.720 --> 00:02:29.730
C and if you want to index it with time
 

00:02:29.730 --> 00:02:32.569
C and if you want to index it with time
I guess you could write um c1 but

00:02:32.569 --> 00:02:32.579
I guess you could write um c1 but
 

00:02:32.579 --> 00:02:34.670
I guess you could write um c1 but
sometimes just write C without the

00:02:34.670 --> 00:02:34.680
sometimes just write C without the
 

00:02:34.680 --> 00:02:38.990
sometimes just write C without the
superscript 1 and this will depend on

00:02:38.990 --> 00:02:39.000
superscript 1 and this will depend on
 

00:02:39.000 --> 00:02:44.199
superscript 1 and this will depend on
the attention parameters so alpha 1 1

00:02:44.199 --> 00:02:44.209
the attention parameters so alpha 1 1
 

00:02:44.209 --> 00:02:48.289
the attention parameters so alpha 1 1
alpha 1 2 and so on

00:02:48.289 --> 00:02:48.299
alpha 1 2 and so on
 

00:02:48.299 --> 00:02:52.539
alpha 1 2 and so on
tells us how much attention and so these

00:02:52.539 --> 00:02:52.549
tells us how much attention and so these
 

00:02:52.549 --> 00:02:56.149
tells us how much attention and so these
alpha parameters tells us how much the

00:02:56.149 --> 00:02:56.159
alpha parameters tells us how much the
 

00:02:56.159 --> 00:02:59.089
alpha parameters tells us how much the
context will depend on the features

00:02:59.089 --> 00:02:59.099
context will depend on the features
 

00:02:59.099 --> 00:03:01.280
context will depend on the features
we're getting on the activations were

00:03:01.280 --> 00:03:01.290
we're getting on the activations were
 

00:03:01.290 --> 00:03:05.000
we're getting on the activations were
getting from the different time steps

00:03:05.000 --> 00:03:05.010
getting from the different time steps
 

00:03:05.010 --> 00:03:08.569
getting from the different time steps
and so the way we'll define the context

00:03:08.569 --> 00:03:08.579
and so the way we'll define the context
 

00:03:08.579 --> 00:03:10.879
and so the way we'll define the context
is there should be a way to some of the

00:03:10.879 --> 00:03:10.889
is there should be a way to some of the
 

00:03:10.889 --> 00:03:12.470
is there should be a way to some of the
features from the different times have

00:03:12.470 --> 00:03:12.480
features from the different times have
 

00:03:12.480 --> 00:03:17.990
features from the different times have
waited by these attention weights so

00:03:17.990 --> 00:03:18.000
waited by these attention weights so
 

00:03:18.000 --> 00:03:21.140
waited by these attention weights so
more formally the attention weights will

00:03:21.140 --> 00:03:21.150
more formally the attention weights will
 

00:03:21.150 --> 00:03:23.750
more formally the attention weights will
satisfy this that they're all be

00:03:23.750 --> 00:03:23.760
satisfy this that they're all be
 

00:03:23.760 --> 00:03:26.270
satisfy this that they're all be
non-negative so they'll be a zero

00:03:26.270 --> 00:03:26.280
non-negative so they'll be a zero
 

00:03:26.280 --> 00:03:29.599
non-negative so they'll be a zero
positive and they'll sum to one we'll

00:03:29.599 --> 00:03:29.609
positive and they'll sum to one we'll
 

00:03:29.609 --> 00:03:31.129
positive and they'll sum to one we'll
see you later how to make sure this is

00:03:31.129 --> 00:03:31.139
see you later how to make sure this is
 

00:03:31.139 --> 00:03:34.670
see you later how to make sure this is
true and we will have that the context

00:03:34.670 --> 00:03:34.680
true and we will have that the context
 

00:03:34.680 --> 00:03:37.280
true and we will have that the context
or the context at time one are often

00:03:37.280 --> 00:03:37.290
or the context at time one are often
 

00:03:37.290 --> 00:03:38.479
or the context at time one are often
drop that superscript

00:03:38.479 --> 00:03:38.489
drop that superscript
 

00:03:38.489 --> 00:03:41.719
drop that superscript
that's going to be sum over T prime all

00:03:41.719 --> 00:03:41.729
that's going to be sum over T prime all
 

00:03:41.729 --> 00:03:45.339
that's going to be sum over T prime all
the values of T prime of this weighted

00:03:45.339 --> 00:03:45.349
the values of T prime of this weighted
 

00:03:45.349 --> 00:03:52.430
the values of T prime of this weighted
sum of these um attention of these

00:03:52.430 --> 00:03:52.440
sum of these um attention of these
 

00:03:52.440 --> 00:03:56.599
sum of these um attention of these
activations so this term here are the

00:03:56.599 --> 00:03:56.609
activations so this term here are the
 

00:03:56.609 --> 00:04:00.349
activations so this term here are the
attention weights and this term here now

00:04:00.349 --> 00:04:00.359
attention weights and this term here now
 

00:04:00.359 --> 00:04:06.649
attention weights and this term here now
is comes from here and so alpha t t

00:04:06.649 --> 00:04:06.659
is comes from here and so alpha t t
 

00:04:06.659 --> 00:04:15.430
is comes from here and so alpha t t
prime is the amount of attention that y

00:04:15.430 --> 00:04:15.440
prime is the amount of attention that y
 

00:04:15.440 --> 00:04:26.190
prime is the amount of attention that y
t should pay to a of T prime

00:04:26.190 --> 00:04:26.200
t should pay to a of T prime
 

00:04:26.200 --> 00:04:28.450
t should pay to a of T prime
so in other words when you're generating

00:04:28.450 --> 00:04:28.460
so in other words when you're generating
 

00:04:28.460 --> 00:04:31.270
so in other words when you're generating
the teeth output words how much should

00:04:31.270 --> 00:04:31.280
the teeth output words how much should
 

00:04:31.280 --> 00:04:33.640
the teeth output words how much should
you be paying attention to the t price

00:04:33.640 --> 00:04:33.650
you be paying attention to the t price
 

00:04:33.650 --> 00:04:37.380
you be paying attention to the t price
input words so that's one step of

00:04:37.380 --> 00:04:37.390
input words so that's one step of
 

00:04:37.390 --> 00:04:40.150
input words so that's one step of
generating the output and then at the

00:04:40.150 --> 00:04:40.160
generating the output and then at the
 

00:04:40.160 --> 00:04:43.780
generating the output and then at the
next time step you generate the second

00:04:43.780 --> 00:04:43.790
next time step you generate the second
 

00:04:43.790 --> 00:04:47.050
next time step you generate the second
output and as a game done similarly

00:04:47.050 --> 00:04:47.060
output and as a game done similarly
 

00:04:47.060 --> 00:04:49.090
output and as a game done similarly
where now you have a new set of

00:04:49.090 --> 00:04:49.100
where now you have a new set of
 

00:04:49.100 --> 00:04:51.940
where now you have a new set of
attention weights they define a new way

00:04:51.940 --> 00:04:51.950
attention weights they define a new way
 

00:04:51.950 --> 00:04:55.510
attention weights they define a new way
to sum that generates a new context this

00:04:55.510 --> 00:04:55.520
to sum that generates a new context this
 

00:04:55.520 --> 00:04:58.210
to sum that generates a new context this
is also input and that allows you to

00:04:58.210 --> 00:04:58.220
is also input and that allows you to
 

00:04:58.220 --> 00:05:01.780
is also input and that allows you to
generate the second words only now yes

00:05:01.780 --> 00:05:01.790
generate the second words only now yes
 

00:05:01.790 --> 00:05:05.980
generate the second words only now yes
this way to sum becomes the context of

00:05:05.980 --> 00:05:05.990
this way to sum becomes the context of
 

00:05:05.990 --> 00:05:08.800
this way to sum becomes the context of
the second times that is somewhat of a t

00:05:08.800 --> 00:05:08.810
the second times that is somewhat of a t
 

00:05:08.810 --> 00:05:14.670
the second times that is somewhat of a t
prime alpha to T Prime so using these

00:05:14.670 --> 00:05:14.680
prime alpha to T Prime so using these
 

00:05:14.680 --> 00:05:17.710
prime alpha to T Prime so using these
context vectors c1 I'll just write that

00:05:17.710 --> 00:05:17.720
context vectors c1 I'll just write that
 

00:05:17.720 --> 00:05:26.290
context vectors c1 I'll just write that
back in C 2 and so on this network up

00:05:26.290 --> 00:05:26.300
back in C 2 and so on this network up
 

00:05:26.300 --> 00:05:29.340
back in C 2 and so on this network up
here looks like a pretty standard RNN

00:05:29.340 --> 00:05:29.350
here looks like a pretty standard RNN
 

00:05:29.350 --> 00:05:32.740
here looks like a pretty standard RNN
sequence with the context vectors as

00:05:32.740 --> 00:05:32.750
sequence with the context vectors as
 

00:05:32.750 --> 00:05:34.750
sequence with the context vectors as
output and we can just generate the

00:05:34.750 --> 00:05:34.760
output and we can just generate the
 

00:05:34.760 --> 00:05:39.010
output and we can just generate the
translation one word at a time we have

00:05:39.010 --> 00:05:39.020
translation one word at a time we have
 

00:05:39.020 --> 00:05:41.350
translation one word at a time we have
also defined how to compute the context

00:05:41.350 --> 00:05:41.360
also defined how to compute the context
 

00:05:41.360 --> 00:05:43.270
also defined how to compute the context
vectors in terms of these attention

00:05:43.270 --> 00:05:43.280
vectors in terms of these attention
 

00:05:43.280 --> 00:05:45.430
vectors in terms of these attention
waves and those features of the input

00:05:45.430 --> 00:05:45.440
waves and those features of the input
 

00:05:45.440 --> 00:05:48.580
waves and those features of the input
sentence so the only remaining thing to

00:05:48.580 --> 00:05:48.590
sentence so the only remaining thing to
 

00:05:48.590 --> 00:05:50.950
sentence so the only remaining thing to
do is to define how to actually compute

00:05:50.950 --> 00:05:50.960
do is to define how to actually compute
 

00:05:50.960 --> 00:05:53.890
do is to define how to actually compute
these attention ways let's do that on

00:05:53.890 --> 00:05:53.900
these attention ways let's do that on
 

00:05:53.900 --> 00:05:57.700
these attention ways let's do that on
the next slide so just a recap alpha t t

00:05:57.700 --> 00:05:57.710
the next slide so just a recap alpha t t
 

00:05:57.710 --> 00:05:59.650
the next slide so just a recap alpha t t
prime is the amount of attention you

00:05:59.650 --> 00:05:59.660
prime is the amount of attention you
 

00:05:59.660 --> 00:06:02.350
prime is the amount of attention you
should pay to a t prime when you're

00:06:02.350 --> 00:06:02.360
should pay to a t prime when you're
 

00:06:02.360 --> 00:06:04.210
should pay to a t prime when you're
trying to generate the teeth words in

00:06:04.210 --> 00:06:04.220
trying to generate the teeth words in
 

00:06:04.220 --> 00:06:08.680
trying to generate the teeth words in
the output translation so let me just

00:06:08.680 --> 00:06:08.690
the output translation so let me just
 

00:06:08.690 --> 00:06:10.330
the output translation so let me just
write down the formulas we talked about

00:06:10.330 --> 00:06:10.340
write down the formulas we talked about
 

00:06:10.340 --> 00:06:11.110
write down the formulas we talked about
how this works

00:06:11.110 --> 00:06:11.120
how this works
 

00:06:11.120 --> 00:06:13.630
how this works
this is formula you can use the compute

00:06:13.630 --> 00:06:13.640
this is formula you can use the compute
 

00:06:13.640 --> 00:06:15.730
this is formula you can use the compute
alpha T T prime which we're going to

00:06:15.730 --> 00:06:15.740
alpha T T prime which we're going to
 

00:06:15.740 --> 00:06:19.420
alpha T T prime which we're going to
compute these terms e TT Prime and then

00:06:19.420 --> 00:06:19.430
compute these terms e TT Prime and then
 

00:06:19.430 --> 00:06:21.220
compute these terms e TT Prime and then
use essentially of softmax to make sure

00:06:21.220 --> 00:06:21.230
use essentially of softmax to make sure
 

00:06:21.230 --> 00:06:24.460
use essentially of softmax to make sure
that these weights sum to 1 if you sum

00:06:24.460 --> 00:06:24.470
that these weights sum to 1 if you sum
 

00:06:24.470 --> 00:06:27.520
that these weights sum to 1 if you sum
over t prime so for every fixed value of

00:06:27.520 --> 00:06:27.530
over t prime so for every fixed value of
 

00:06:27.530 --> 00:06:32.110
over t prime so for every fixed value of
T these things sum to 1 if you're

00:06:32.110 --> 00:06:32.120
T these things sum to 1 if you're
 

00:06:32.120 --> 00:06:36.700
T these things sum to 1 if you're
summing over T prime and using this

00:06:36.700 --> 00:06:36.710
summing over T prime and using this
 

00:06:36.710 --> 00:06:38.380
summing over T prime and using this
software privatization

00:06:38.380 --> 00:06:38.390
software privatization
 

00:06:38.390 --> 00:06:40.730
software privatization
just ensures this property they sum to

00:06:40.730 --> 00:06:40.740
just ensures this property they sum to
 

00:06:40.740 --> 00:06:43.400
just ensures this property they sum to
one now how do you compute these factors

00:06:43.400 --> 00:06:43.410
one now how do you compute these factors
 

00:06:43.410 --> 00:06:46.280
one now how do you compute these factors
e well one way to do so is to use a

00:06:46.280 --> 00:06:46.290
e well one way to do so is to use a
 

00:06:46.290 --> 00:06:51.980
e well one way to do so is to use a
small new network as follows so s t

00:06:51.980 --> 00:06:51.990
small new network as follows so s t
 

00:06:51.990 --> 00:06:54.440
small new network as follows so s t
minus 1 was the neural network state

00:06:54.440 --> 00:06:54.450
minus 1 was the neural network state
 

00:06:54.450 --> 00:06:57.050
minus 1 was the neural network state
from the previous time step so here's

00:06:57.050 --> 00:06:57.060
from the previous time step so here's
 

00:06:57.060 --> 00:06:59.840
from the previous time step so here's
the network we have if you're trying to

00:06:59.840 --> 00:06:59.850
the network we have if you're trying to
 

00:06:59.850 --> 00:07:04.070
the network we have if you're trying to
generate YT then s T minus 1 was the

00:07:04.070 --> 00:07:04.080
generate YT then s T minus 1 was the
 

00:07:04.080 --> 00:07:05.690
generate YT then s T minus 1 was the
hidden state from the previous step

00:07:05.690 --> 00:07:05.700
hidden state from the previous step
 

00:07:05.700 --> 00:07:09.500
hidden state from the previous step
that's fed into s T and that's one input

00:07:09.500 --> 00:07:09.510
that's fed into s T and that's one input
 

00:07:09.510 --> 00:07:12.290
that's fed into s T and that's one input
to the very small neural network usually

00:07:12.290 --> 00:07:12.300
to the very small neural network usually
 

00:07:12.300 --> 00:07:14.450
to the very small neural network usually
one hidden layer neural network because

00:07:14.450 --> 00:07:14.460
one hidden layer neural network because
 

00:07:14.460 --> 00:07:16.070
one hidden layer neural network because
you need to compute these are locked and

00:07:16.070 --> 00:07:16.080
you need to compute these are locked and
 

00:07:16.080 --> 00:07:20.030
you need to compute these are locked and
then a T prime the features and from

00:07:20.030 --> 00:07:20.040
then a T prime the features and from
 

00:07:20.040 --> 00:07:23.060
then a T prime the features and from
time step T prime is the other input and

00:07:23.060 --> 00:07:23.070
time step T prime is the other input and
 

00:07:23.070 --> 00:07:25.610
time step T prime is the other input and
the intuition is if you want to decide

00:07:25.610 --> 00:07:25.620
the intuition is if you want to decide
 

00:07:25.620 --> 00:07:29.440
the intuition is if you want to decide
how much attention to pay to

00:07:29.440 --> 00:07:29.450
how much attention to pay to
 

00:07:29.450 --> 00:07:32.570
how much attention to pay to
deactivation at T prime well the things

00:07:32.570 --> 00:07:32.580
deactivation at T prime well the things
 

00:07:32.580 --> 00:07:34.670
deactivation at T prime well the things
that seems like it should depend the

00:07:34.670 --> 00:07:34.680
that seems like it should depend the
 

00:07:34.680 --> 00:07:36.890
that seems like it should depend the
mouse on is what is your own hidden

00:07:36.890 --> 00:07:36.900
mouse on is what is your own hidden
 

00:07:36.900 --> 00:07:38.660
mouse on is what is your own hidden
state activation from the previous time

00:07:38.660 --> 00:07:38.670
state activation from the previous time
 

00:07:38.670 --> 00:07:40.760
state activation from the previous time
step you don't have the current state

00:07:40.760 --> 00:07:40.770
step you don't have the current state
 

00:07:40.770 --> 00:07:42.710
step you don't have the current state
activation yet because the context fits

00:07:42.710 --> 00:07:42.720
activation yet because the context fits
 

00:07:42.720 --> 00:07:44.210
activation yet because the context fits
into this so you haven't computed that

00:07:44.210 --> 00:07:44.220
into this so you haven't computed that
 

00:07:44.220 --> 00:07:46.460
into this so you haven't computed that
but look at one of your hidden stages of

00:07:46.460 --> 00:07:46.470
but look at one of your hidden stages of
 

00:07:46.470 --> 00:07:47.990
but look at one of your hidden stages of
this errand and generating the output

00:07:47.990 --> 00:07:48.000
this errand and generating the output
 

00:07:48.000 --> 00:07:50.390
this errand and generating the output
translation and then for each of the

00:07:50.390 --> 00:07:50.400
translation and then for each of the
 

00:07:50.400 --> 00:07:52.250
translation and then for each of the
positions of each the worst look at

00:07:52.250 --> 00:07:52.260
positions of each the worst look at
 

00:07:52.260 --> 00:07:54.590
positions of each the worst look at
their features so it seems pretty

00:07:54.590 --> 00:07:54.600
their features so it seems pretty
 

00:07:54.600 --> 00:07:59.570
their features so it seems pretty
natural that alpha t t prime and e t t

00:07:59.570 --> 00:07:59.580
natural that alpha t t prime and e t t
 

00:07:59.580 --> 00:08:01.970
natural that alpha t t prime and e t t
prime should depend on these two

00:08:01.970 --> 00:08:01.980
prime should depend on these two
 

00:08:01.980 --> 00:08:04.070
prime should depend on these two
quantities but we don't know what the

00:08:04.070 --> 00:08:04.080
quantities but we don't know what the
 

00:08:04.080 --> 00:08:04.820
quantities but we don't know what the
function is

00:08:04.820 --> 00:08:04.830
function is
 

00:08:04.830 --> 00:08:06.560
function is
so one thing could do is just trim very

00:08:06.560 --> 00:08:06.570
so one thing could do is just trim very
 

00:08:06.570 --> 00:08:08.750
so one thing could do is just trim very
small neural network to learn whatever

00:08:08.750 --> 00:08:08.760
small neural network to learn whatever
 

00:08:08.760 --> 00:08:11.870
small neural network to learn whatever
this function should be and trust back

00:08:11.870 --> 00:08:11.880
this function should be and trust back
 

00:08:11.880 --> 00:08:15.260
this function should be and trust back
propagation trust gradient descent to

00:08:15.260 --> 00:08:15.270
propagation trust gradient descent to
 

00:08:15.270 --> 00:08:19.820
propagation trust gradient descent to
learn the right function and it turns

00:08:19.820 --> 00:08:19.830
learn the right function and it turns
 

00:08:19.830 --> 00:08:22.640
learn the right function and it turns
out that if you implement this whole

00:08:22.640 --> 00:08:22.650
out that if you implement this whole
 

00:08:22.650 --> 00:08:25.100
out that if you implement this whole
model and train it with gradient descent

00:08:25.100 --> 00:08:25.110
model and train it with gradient descent
 

00:08:25.110 --> 00:08:27.920
model and train it with gradient descent
the whole thing actually works this law

00:08:27.920 --> 00:08:27.930
the whole thing actually works this law
 

00:08:27.930 --> 00:08:30.290
the whole thing actually works this law
neural network does a pretty decent job

00:08:30.290 --> 00:08:30.300
neural network does a pretty decent job
 

00:08:30.300 --> 00:08:34.460
neural network does a pretty decent job
telling you how much attention YT should

00:08:34.460 --> 00:08:34.470
telling you how much attention YT should
 

00:08:34.470 --> 00:08:39.680
telling you how much attention YT should
pay to a t prime and this formula shows

00:08:39.680 --> 00:08:39.690
pay to a t prime and this formula shows
 

00:08:39.690 --> 00:08:41.810
pay to a t prime and this formula shows
that the attention weighs sum to one and

00:08:41.810 --> 00:08:41.820
that the attention weighs sum to one and
 

00:08:41.820 --> 00:08:45.260
that the attention weighs sum to one and
then as you chug along generating one

00:08:45.260 --> 00:08:45.270
then as you chug along generating one
 

00:08:45.270 --> 00:08:47.240
then as you chug along generating one
word at a time this neural network

00:08:47.240 --> 00:08:47.250
word at a time this neural network
 

00:08:47.250 --> 00:08:49.730
word at a time this neural network
actually pays attention to the right

00:08:49.730 --> 00:08:49.740
actually pays attention to the right
 

00:08:49.740 --> 00:08:51.350
actually pays attention to the right
parts of the input sentence

00:08:51.350 --> 00:08:51.360
parts of the input sentence
 

00:08:51.360 --> 00:08:53.869
parts of the input sentence
all this automatically using gradient

00:08:53.869 --> 00:08:53.879
all this automatically using gradient
 

00:08:53.879 --> 00:08:57.199
all this automatically using gradient
descent now one downside that this

00:08:57.199 --> 00:08:57.209
descent now one downside that this
 

00:08:57.209 --> 00:09:00.199
descent now one downside that this
algorithm is that it does take quadratic

00:09:00.199 --> 00:09:00.209
algorithm is that it does take quadratic
 

00:09:00.209 --> 00:09:02.749
algorithm is that it does take quadratic
time quadratic cost to run this

00:09:02.749 --> 00:09:02.759
time quadratic cost to run this
 

00:09:02.759 --> 00:09:07.609
time quadratic cost to run this
algorithm if you have TX worse than the

00:09:07.609 --> 00:09:07.619
algorithm if you have TX worse than the
 

00:09:07.619 --> 00:09:10.699
algorithm if you have TX worse than the
input and T Y worse than the output then

00:09:10.699 --> 00:09:10.709
input and T Y worse than the output then
 

00:09:10.709 --> 00:09:13.340
input and T Y worse than the output then
the total number of these attention

00:09:13.340 --> 00:09:13.350
the total number of these attention
 

00:09:13.350 --> 00:09:16.569
the total number of these attention
parameters is going to be T x times dy

00:09:16.569 --> 00:09:16.579
parameters is going to be T x times dy
 

00:09:16.579 --> 00:09:20.600
parameters is going to be T x times dy
and so this algorithm runs in quadratic

00:09:20.600 --> 00:09:20.610
and so this algorithm runs in quadratic
 

00:09:20.610 --> 00:09:24.249
and so this algorithm runs in quadratic
cost although in machine translation

00:09:24.249 --> 00:09:24.259
cost although in machine translation
 

00:09:24.259 --> 00:09:28.160
cost although in machine translation
applications where neither input nor

00:09:28.160 --> 00:09:28.170
applications where neither input nor
 

00:09:28.170 --> 00:09:30.979
applications where neither input nor
output sentences usually that long may

00:09:30.979 --> 00:09:30.989
output sentences usually that long may
 

00:09:30.989 --> 00:09:32.539
output sentences usually that long may
be quadratic constants actually

00:09:32.539 --> 00:09:32.549
be quadratic constants actually
 

00:09:32.549 --> 00:09:34.369
be quadratic constants actually
acceptable although there are some

00:09:34.369 --> 00:09:34.379
acceptable although there are some
 

00:09:34.379 --> 00:09:36.109
acceptable although there are some
research work on trying to reduce this

00:09:36.109 --> 00:09:36.119
research work on trying to reduce this
 

00:09:36.119 --> 00:09:40.819
research work on trying to reduce this
cost as well now so far I've been

00:09:40.819 --> 00:09:40.829
cost as well now so far I've been
 

00:09:40.829 --> 00:09:44.269
cost as well now so far I've been
describing the attention idea in the

00:09:44.269 --> 00:09:44.279
describing the attention idea in the
 

00:09:44.279 --> 00:09:47.660
describing the attention idea in the
context of machine translation without

00:09:47.660 --> 00:09:47.670
context of machine translation without
 

00:09:47.670 --> 00:09:50.689
context of machine translation without
going too much into detail this idea has

00:09:50.689 --> 00:09:50.699
going too much into detail this idea has
 

00:09:50.699 --> 00:09:52.850
going too much into detail this idea has
been applied to other problems as well

00:09:52.850 --> 00:09:52.860
been applied to other problems as well
 

00:09:52.860 --> 00:09:56.269
been applied to other problems as well
such as image captioning so in the image

00:09:56.269 --> 00:09:56.279
such as image captioning so in the image
 

00:09:56.279 --> 00:09:58.759
such as image captioning so in the image
captioning problem to toss this locally

00:09:58.759 --> 00:09:58.769
captioning problem to toss this locally
 

00:09:58.769 --> 00:10:01.009
captioning problem to toss this locally
picture and write a caption for that

00:10:01.009 --> 00:10:01.019
picture and write a caption for that
 

00:10:01.019 --> 00:10:04.309
picture and write a caption for that
picture so in this paper sight to the

00:10:04.309 --> 00:10:04.319
picture so in this paper sight to the
 

00:10:04.319 --> 00:10:07.400
picture so in this paper sight to the
bottom by Kevin chute Jimmy bar on Keros

00:10:07.400 --> 00:10:07.410
bottom by Kevin chute Jimmy bar on Keros
 

00:10:07.410 --> 00:10:09.590
bottom by Kevin chute Jimmy bar on Keros
Cameron show erinkoval rustle some food

00:10:09.590 --> 00:10:09.600
Cameron show erinkoval rustle some food
 

00:10:09.600 --> 00:10:11.900
Cameron show erinkoval rustle some food
and off Rich's MO and you're sure banjo

00:10:11.900 --> 00:10:11.910
and off Rich's MO and you're sure banjo
 

00:10:11.910 --> 00:10:15.049
and off Rich's MO and you're sure banjo
the author showed that you could have a

00:10:15.049 --> 00:10:15.059
the author showed that you could have a
 

00:10:15.059 --> 00:10:17.150
the author showed that you could have a
very similar architecture look at the

00:10:17.150 --> 00:10:17.160
very similar architecture look at the
 

00:10:17.160 --> 00:10:21.199
very similar architecture look at the
picture and pay attention only to parts

00:10:21.199 --> 00:10:21.209
picture and pay attention only to parts
 

00:10:21.209 --> 00:10:23.479
picture and pay attention only to parts
of the picture at a time while you're

00:10:23.479 --> 00:10:23.489
of the picture at a time while you're
 

00:10:23.489 --> 00:10:26.749
of the picture at a time while you're
writing a caption for a picture so if

00:10:26.749 --> 00:10:26.759
writing a caption for a picture so if
 

00:10:26.759 --> 00:10:28.249
writing a caption for a picture so if
you're interested in encourage you to

00:10:28.249 --> 00:10:28.259
you're interested in encourage you to
 

00:10:28.259 --> 00:10:30.909
you're interested in encourage you to
take a look at that paper as well and

00:10:30.909 --> 00:10:30.919
take a look at that paper as well and
 

00:10:30.919 --> 00:10:34.069
take a look at that paper as well and
you get to play with all this more in

00:10:34.069 --> 00:10:34.079
you get to play with all this more in
 

00:10:34.079 --> 00:10:37.400
you get to play with all this more in
the programming exercise whereas machine

00:10:37.400 --> 00:10:37.410
the programming exercise whereas machine
 

00:10:37.410 --> 00:10:39.379
the programming exercise whereas machine
translation is a very complicated

00:10:39.379 --> 00:10:39.389
translation is a very complicated
 

00:10:39.389 --> 00:10:42.499
translation is a very complicated
problem in the pro exercise you get to

00:10:42.499 --> 00:10:42.509
problem in the pro exercise you get to
 

00:10:42.509 --> 00:10:43.999
problem in the pro exercise you get to
implement and play up the attention

00:10:43.999 --> 00:10:44.009
implement and play up the attention
 

00:10:44.009 --> 00:10:46.429
implement and play up the attention
while yourself for the date

00:10:46.429 --> 00:10:46.439
while yourself for the date
 

00:10:46.439 --> 00:10:48.739
while yourself for the date
normalization problem so the problem in

00:10:48.739 --> 00:10:48.749
normalization problem so the problem in
 

00:10:48.749 --> 00:10:50.989
normalization problem so the problem in
putting a date like this there's

00:10:50.989 --> 00:10:50.999
putting a date like this there's
 

00:10:50.999 --> 00:10:53.539
putting a date like this there's
actually date of the Apollo moon landing

00:10:53.539 --> 00:10:53.549
actually date of the Apollo moon landing
 

00:10:53.549 --> 00:10:56.329
actually date of the Apollo moon landing
and normalizing it into standard formats

00:10:56.329 --> 00:10:56.339
and normalizing it into standard formats
 

00:10:56.339 --> 00:10:59.960
and normalizing it into standard formats
or a date like this and having a new

00:10:59.960 --> 00:10:59.970
or a date like this and having a new
 

00:10:59.970 --> 00:11:01.909
or a date like this and having a new
network a sequence a sequence model

00:11:01.909 --> 00:11:01.919
network a sequence a sequence model
 

00:11:01.919 --> 00:11:04.400
network a sequence a sequence model
normalize it to this format

00:11:04.400 --> 00:11:04.410
normalize it to this format
 

00:11:04.410 --> 00:11:06.769
normalize it to this format
by the way is the birth date of William

00:11:06.769 --> 00:11:06.779
by the way is the birth date of William
 

00:11:06.779 --> 00:11:07.490
by the way is the birth date of William
Shakespeare

00:11:07.490 --> 00:11:07.500
Shakespeare
 

00:11:07.500 --> 00:11:10.670
Shakespeare
also is believed to be and what you see

00:11:10.670 --> 00:11:10.680
also is believed to be and what you see
 

00:11:10.680 --> 00:11:12.319
also is believed to be and what you see
in the program exercises you can train

00:11:12.319 --> 00:11:12.329
in the program exercises you can train
 

00:11:12.329 --> 00:11:15.439
in the program exercises you can train
in your network to input dates in you

00:11:15.439 --> 00:11:15.449
in your network to input dates in you
 

00:11:15.449 --> 00:11:17.420
in your network to input dates in you
know any of these formats and have it

00:11:17.420 --> 00:11:17.430
know any of these formats and have it
 

00:11:17.430 --> 00:11:20.559
know any of these formats and have it
use an attention model to generate a

00:11:20.559 --> 00:11:20.569
use an attention model to generate a
 

00:11:20.569 --> 00:11:23.660
use an attention model to generate a
normalized format for these things one

00:11:23.660 --> 00:11:23.670
normalized format for these things one
 

00:11:23.670 --> 00:11:26.329
normalized format for these things one
other thing that sometimes fun to do is

00:11:26.329 --> 00:11:26.339
other thing that sometimes fun to do is
 

00:11:26.339 --> 00:11:28.850
other thing that sometimes fun to do is
to look at the visualizations of the

00:11:28.850 --> 00:11:28.860
to look at the visualizations of the
 

00:11:28.860 --> 00:11:32.059
to look at the visualizations of the
attention ways so here's a machine

00:11:32.059 --> 00:11:32.069
attention ways so here's a machine
 

00:11:32.069 --> 00:11:34.699
attention ways so here's a machine
translation example and here were

00:11:34.699 --> 00:11:34.709
translation example and here were
 

00:11:34.709 --> 00:11:36.590
translation example and here were
plotted in different colors the

00:11:36.590 --> 00:11:36.600
plotted in different colors the
 

00:11:36.600 --> 00:11:38.480
plotted in different colors the
magnitude of the different attention

00:11:38.480 --> 00:11:38.490
magnitude of the different attention
 

00:11:38.490 --> 00:11:41.150
magnitude of the different attention
ways I don't spend too much time on this

00:11:41.150 --> 00:11:41.160
ways I don't spend too much time on this
 

00:11:41.160 --> 00:11:44.119
ways I don't spend too much time on this
but you find that the corresponding

00:11:44.119 --> 00:11:44.129
but you find that the corresponding
 

00:11:44.129 --> 00:11:48.350
but you find that the corresponding
input and output words you find that the

00:11:48.350 --> 00:11:48.360
input and output words you find that the
 

00:11:48.360 --> 00:11:51.050
input and output words you find that the
attention weights will tend to be high

00:11:51.050 --> 00:11:51.060
attention weights will tend to be high
 

00:11:51.060 --> 00:11:53.059
attention weights will tend to be high
does suggesting that when it's

00:11:53.059 --> 00:11:53.069
does suggesting that when it's
 

00:11:53.069 --> 00:11:55.249
does suggesting that when it's
generating a specific word the output is

00:11:55.249 --> 00:11:55.259
generating a specific word the output is
 

00:11:55.259 --> 00:11:57.619
generating a specific word the output is
you know usually paying attention to the

00:11:57.619 --> 00:11:57.629
you know usually paying attention to the
 

00:11:57.629 --> 00:12:00.920
you know usually paying attention to the
correct words in the input and all this

00:12:00.920 --> 00:12:00.930
correct words in the input and all this
 

00:12:00.930 --> 00:12:03.019
correct words in the input and all this
including learning where to pay

00:12:03.019 --> 00:12:03.029
including learning where to pay
 

00:12:03.029 --> 00:12:05.420
including learning where to pay
attention when was all learned using

00:12:05.420 --> 00:12:05.430
attention when was all learned using
 

00:12:05.430 --> 00:12:07.990
attention when was all learned using
back propagation with an attention model

00:12:07.990 --> 00:12:08.000
back propagation with an attention model
 

00:12:08.000 --> 00:12:11.300
back propagation with an attention model
so that's it for the attention model

00:12:11.300 --> 00:12:11.310
so that's it for the attention model
 

00:12:11.310 --> 00:12:13.610
so that's it for the attention model
really one of the most powerful ideas in

00:12:13.610 --> 00:12:13.620
really one of the most powerful ideas in
 

00:12:13.620 --> 00:12:16.460
really one of the most powerful ideas in
deep learning I hope you enjoy

00:12:16.460 --> 00:12:16.470
deep learning I hope you enjoy
 

00:12:16.470 --> 00:12:18.139
deep learning I hope you enjoy
implementing and playing with some of

00:12:18.139 --> 00:12:18.149
implementing and playing with some of
 

00:12:18.149 --> 00:12:20.360
implementing and playing with some of
these ideas yourself later in this

00:12:20.360 --> 00:12:20.370
these ideas yourself later in this
 

00:12:20.370 --> 00:12:23.600
these ideas yourself later in this
week's pro exercises

