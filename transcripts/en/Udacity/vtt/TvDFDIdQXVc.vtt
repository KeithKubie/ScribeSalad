WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:02.550
So, you remember how we did panoramas?

00:00:02.550 --> 00:00:06.470
We basically found interesting points
in an image, and we described them.

00:00:06.470 --> 00:00:08.990
We built these descriptors,
these sift descriptors.

00:00:08.990 --> 00:00:13.080
We found punitive matches, possible
matches between them, and then we did

00:00:13.080 --> 00:00:15.660
something like RANSAC, or something like
that, in order to find the alignment.

00:00:16.850 --> 00:00:19.909
Well, we can actually do something for
recognition that's related.

00:00:21.290 --> 00:00:22.060
Now essentially

00:00:22.060 --> 00:00:23.880
what we we're going to do is to say
we're going to do the same thing.

00:00:23.880 --> 00:00:26.420
We're going to find interesting
points where these are sort of

00:00:26.420 --> 00:00:28.980
highly repeatable points
that you'll find.

00:00:28.980 --> 00:00:33.360
And what we're going to talk about
is describing the patches around

00:00:33.360 --> 00:00:34.500
those points.

00:00:34.500 --> 00:00:38.540
And then we're going to use the
collection of those described patches

00:00:38.540 --> 00:00:43.710
as a way of generally characterizing
the image for recognition.

00:00:43.710 --> 00:00:47.940
So, as we've talked about before,
each location in an image.

00:00:47.940 --> 00:00:51.630
So here we have different
locations in an image, all right.

00:00:51.630 --> 00:00:54.230
And a bunch of different
training images.

00:00:54.230 --> 00:00:58.860
And each one of these
has a descriptor that,

00:00:58.860 --> 00:01:02.410
in this what these curves are meant
to show is that they land somewhere

00:01:02.410 --> 00:01:06.360
in feature space, so this they land
in the descriptor's feature space.

00:01:06.360 --> 00:01:08.680
And you can imagine that you
get a collection of these, so

00:01:08.680 --> 00:01:11.520
that there's a whole bunch of
descriptors in a given image.

00:01:11.520 --> 00:01:13.870
And then we're going to make
the following assumption, and

00:01:13.870 --> 00:01:15.610
we actually made this
assumption before right?

00:01:15.610 --> 00:01:20.660
We assume that when we have
points that are sort of similar

00:01:20.660 --> 00:01:25.274
in the descriptor space that they are or
nearby in the descriptor space,

00:01:25.274 --> 00:01:31.010
that those patches are either the same
patch in a new image a similar patch.

00:01:31.010 --> 00:01:34.760
So one way of thinking about this is
it's a way of just giving a description

00:01:34.760 --> 00:01:38.240
of a little bit of local
content in the image.

00:01:38.240 --> 00:01:43.000
And so now if I get a new
picture that comes in, right?

00:01:43.000 --> 00:01:45.600
I can say let me find the patches in it.

00:01:45.600 --> 00:01:50.580
And see if I find lots of
the same local patches that

00:01:50.580 --> 00:01:55.810
were in my first image, and say,
okay, if I find a lot of those,

00:01:55.810 --> 00:02:00.130
then I would know that maybe this
new picture is the same thing,

00:02:00.130 --> 00:02:03.580
maybe same category or
same object, as the first picture.

00:02:03.580 --> 00:02:04.830
The problem, of course,

00:02:04.830 --> 00:02:08.810
is that any given picture can
have thousands of patches.

00:02:08.810 --> 00:02:11.330
So a collection of different objects,
and different images.

00:02:11.330 --> 00:02:13.550
There could be millions of these things.

00:02:13.550 --> 00:02:17.060
And the problem is, if I come up with
a new image, and I find a patch in it.

00:02:17.060 --> 00:02:18.070
I have this problem,

00:02:18.070 --> 00:02:21.970
of possibly having to search over
millions of different patches.

00:02:21.970 --> 00:02:23.460
I have to do something
a little more clever.

