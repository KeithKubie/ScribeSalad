WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.940
[MUSIC PLAYING]

00:00:06.860 --> 00:00:09.310
[CROWD NOISE]

00:00:15.622 --> 00:00:16.830
TIMOTHY JORDAN: Hi, everyone.

00:00:16.830 --> 00:00:18.740
Thanks for joining
us on I/O Live.

00:00:18.740 --> 00:00:19.578
I'm Timothy Jordan.

00:00:19.578 --> 00:00:21.120
EMILY FORTUNA: And
I'm Emily Fortuna.

00:00:21.120 --> 00:00:24.050
If you can't be here this
week at I/O, don't worry.

00:00:24.050 --> 00:00:25.140
We're here for you.

00:00:25.140 --> 00:00:29.810
Visit g.co/io/live to watch
and rewatch the keynotes

00:00:29.810 --> 00:00:32.768
and sessions, plus all the
exclusive interviews and demos

00:00:32.768 --> 00:00:35.060
our team of Googlers will be
bringing you over the next

00:00:35.060 --> 00:00:35.630
days.

00:00:35.630 --> 00:00:37.130
TIMOTHY JORDAN:
First up, we welcome

00:00:37.130 --> 00:00:40.280
Manuel Bronstein, VP of Product
Management here at Google

00:00:40.280 --> 00:00:43.240
to talk to us about the next
generation of Google Assistant.

00:00:43.240 --> 00:00:44.490
Manuel, thanks for joining us.

00:00:44.490 --> 00:00:46.010
MANUEL BRONSTEIN: Thanks
for having me here.

00:00:46.010 --> 00:00:46.220
TIMOTHY JORDAN: All right.

00:00:46.220 --> 00:00:48.170
Let's go really big
picture for a moment.

00:00:48.170 --> 00:00:50.750
When you think about how
the Google Assistant changes

00:00:50.750 --> 00:00:53.842
people's everyday lives for the
better, what does it look like?

00:00:53.842 --> 00:00:54.800
MANUEL BRONSTEIN: Yeah.

00:00:54.800 --> 00:00:58.640
So when we thought
about the product,

00:00:58.640 --> 00:01:01.650
our mission is to be the
best way to get things done.

00:01:01.650 --> 00:01:04.890
And when we say best, we
think about seamless, fastest,

00:01:04.890 --> 00:01:06.355
easiest way to get things done.

00:01:06.355 --> 00:01:07.730
And if you think
about our world,

00:01:07.730 --> 00:01:09.530
in some ways it's getting
more complex, right?

00:01:09.530 --> 00:01:10.920
Every time you try
to do something,

00:01:10.920 --> 00:01:13.128
you need to think about not
only what you want to do,

00:01:13.128 --> 00:01:14.070
but the how.

00:01:14.070 --> 00:01:16.250
And if we do our
job right and we're

00:01:16.250 --> 00:01:17.870
investing to make
it happen, you want

00:01:17.870 --> 00:01:19.850
to be able to go
from what do I want

00:01:19.850 --> 00:01:22.970
to get done to getting it done
without having to worry about

00:01:22.970 --> 00:01:24.500
the how, in many ways.

00:01:24.500 --> 00:01:26.990
And our hope is that
that saves you time.

00:01:26.990 --> 00:01:29.073
And we all know how
important it is for all of us

00:01:29.073 --> 00:01:31.490
to get a little bit more time
to do all the things that we

00:01:31.490 --> 00:01:32.490
care about.

00:01:32.490 --> 00:01:34.100
So we're very proud
to be investing

00:01:34.100 --> 00:01:36.350
in making the product the
best way to get things done.

00:01:36.350 --> 00:01:38.128
In many ways, to
give users time back.

00:01:38.128 --> 00:01:39.920
EMILY FORTUNA: So the
Assistant is becoming

00:01:39.920 --> 00:01:41.240
more and more personalized.

00:01:41.240 --> 00:01:43.320
How does that help us, as well?

00:01:43.320 --> 00:01:45.050
MANUEL BRONSTEIN: So
think about-- when

00:01:45.050 --> 00:01:46.310
people think about
an Assistant, they're

00:01:46.310 --> 00:01:47.893
thinking about a
product that is going

00:01:47.893 --> 00:01:50.060
to learn from them, that
is going to understand

00:01:50.060 --> 00:01:51.220
what they're saying.

00:01:51.220 --> 00:01:53.762
And in order for the system to
understand what you're saying,

00:01:53.762 --> 00:01:55.137
it needs to
understand your role.

00:01:55.137 --> 00:01:55.730
Right?

00:01:55.730 --> 00:01:58.130
If I say call my
wife, it needs to know

00:01:58.130 --> 00:02:01.250
that my wife is [? Ariana ?]
and what number to call.

00:02:01.250 --> 00:02:04.610
If I say set an
appointment for my kids,

00:02:04.610 --> 00:02:06.440
it should understand
who my kids are.

00:02:06.440 --> 00:02:08.539
So when the system starts
getting to know you,

00:02:08.539 --> 00:02:11.180
or it even knows where
is home, where is work--

00:02:11.180 --> 00:02:13.670
so that I can know, hey,
when I'm asking it to give me

00:02:13.670 --> 00:02:15.620
a direction, I said,
like, drive me to work--

00:02:15.620 --> 00:02:16.783
it can understand that.

00:02:16.783 --> 00:02:18.950
The system becomes more
useful because you can speak

00:02:18.950 --> 00:02:20.385
to it in a more natural way.

00:02:20.385 --> 00:02:22.010
You don't have to
think about addresses

00:02:22.010 --> 00:02:23.760
or you don't have to
think about commands.

00:02:23.760 --> 00:02:26.270
You speak to it the same way
you would speak to a friend.

00:02:26.270 --> 00:02:28.187
And then it will understand
what you're trying

00:02:28.187 --> 00:02:30.238
to do and help you get it done.

00:02:30.238 --> 00:02:32.280
TIMOTHY JORDAN: What is
your favorite new feature

00:02:32.280 --> 00:02:34.397
that Assistant
announced today and why?

00:02:34.397 --> 00:02:36.980
MANUEL BRONSTEIN: I mean, it's
really hard to pick a favorite.

00:02:36.980 --> 00:02:39.630
The team worked really, really
hard on many of the things

00:02:39.630 --> 00:02:41.130
that we announced today.

00:02:41.130 --> 00:02:43.650
I think that when you
think about the promise

00:02:43.650 --> 00:02:46.850
of the Assistant being the
best way to get things done,

00:02:46.850 --> 00:02:48.803
when it starts
getting really fast.

00:02:48.803 --> 00:02:50.720
And what we announced
with the next generation

00:02:50.720 --> 00:02:52.850
Assistant, one of the
key things about it--

00:02:52.850 --> 00:02:56.090
and we're seeing it in
a very proud way-- which

00:02:56.090 --> 00:03:00.020
is imagine that it starts
understanding you so fast so

00:03:00.020 --> 00:03:03.950
quickly that speaking into your
phone is faster than tapping.

00:03:03.950 --> 00:03:06.560
Or that tapping your phone
is going to feel slow.

00:03:06.560 --> 00:03:08.840
That's when you know
that you hit something

00:03:08.840 --> 00:03:10.850
that could change
user behavior and get

00:03:10.850 --> 00:03:14.180
people to start embedding
themselves into this product.

00:03:14.180 --> 00:03:17.390
Because it's actually
becoming very simple, but also

00:03:17.390 --> 00:03:19.550
very fast and very
efficient in helping you

00:03:19.550 --> 00:03:21.543
get your tasks done.

00:03:21.543 --> 00:03:22.710
EMILY FORTUNA: That's great.

00:03:22.710 --> 00:03:24.180
So in the past
couple years, we've

00:03:24.180 --> 00:03:26.340
seen Assistant play
a much larger role

00:03:26.340 --> 00:03:28.410
in homes and phones.

00:03:28.410 --> 00:03:30.180
And now in driving--

00:03:30.180 --> 00:03:32.575
so can you tell us about
the experience for drivers

00:03:32.575 --> 00:03:33.450
and what that's like.

00:03:33.450 --> 00:03:34.408
MANUEL BRONSTEIN: Yeah.

00:03:34.408 --> 00:03:36.480
I mean, when you think
about using your voice

00:03:36.480 --> 00:03:39.385
and being hands free,
actually, the car

00:03:39.385 --> 00:03:41.010
is a perfect environment
for it, right?

00:03:41.010 --> 00:03:42.900
Hopefully, people are
driving, and they're

00:03:42.900 --> 00:03:43.750
looking at the road.

00:03:43.750 --> 00:03:44.810
They're not looking
at their phones.

00:03:44.810 --> 00:03:45.440
They're not touching
the screens--

00:03:45.440 --> 00:03:46.270
TIMOTHY JORDAN: You don't
mean hands-free driving?

00:03:46.270 --> 00:03:47.130
MANUEL BRONSTEIN:
Hands-free driving--

00:03:47.130 --> 00:03:47.460
no.

00:03:47.460 --> 00:03:48.076
It's hands-free phone.

00:03:48.076 --> 00:03:48.512
TIMOTHY JORDAN:
Hands on the wheel.

00:03:48.512 --> 00:03:48.950
MANUEL BRONSTEIN: Yeah.

00:03:48.950 --> 00:03:49.560
TIMOTHY JORDAN:
Not on the phone.

00:03:49.560 --> 00:03:50.060
Yeah.

00:03:50.060 --> 00:03:51.460
MANUEL BRONSTEIN: Correct.

00:03:51.460 --> 00:03:53.460
So when you think about
it, actually, the car

00:03:53.460 --> 00:03:57.120
is one of the places where
the Assistant can be even more

00:03:57.120 --> 00:03:58.810
helpful, right?

00:03:58.810 --> 00:04:00.390
But the car is a
hard environment.

00:04:00.390 --> 00:04:03.870
Because if your windows are
down, there's a lot of noise.

00:04:03.870 --> 00:04:06.383
You may lose connectivity
and so forth.

00:04:06.383 --> 00:04:08.550
But one of the things that
we started thinking about

00:04:08.550 --> 00:04:10.585
is let's start from
the user and what

00:04:10.585 --> 00:04:12.210
is that thing that
you're trying to get

00:04:12.210 --> 00:04:13.950
done when you're in your car.

00:04:13.950 --> 00:04:15.960
I mean, there are mainly
three things, right?

00:04:15.960 --> 00:04:17.700
You're trying to
navigate from point A

00:04:17.700 --> 00:04:20.130
to point B. You want to
get to the place hopefully

00:04:20.130 --> 00:04:21.959
fast and safe.

00:04:21.959 --> 00:04:24.270
You also want to make sure
that when you're in the car

00:04:24.270 --> 00:04:25.660
you can get entertained, right?

00:04:25.660 --> 00:04:29.790
So you want to listen to music
or podcast or radio, right?

00:04:29.790 --> 00:04:32.513
And then of course, you
sometimes want to communicate.

00:04:32.513 --> 00:04:33.930
You want to make
a phone call, you

00:04:33.930 --> 00:04:37.140
want to receive a text message,
or send a text message.

00:04:37.140 --> 00:04:39.900
And think about doing all
these things with your hands

00:04:39.900 --> 00:04:42.150
while you're driving-- it's
probably not the right way

00:04:42.150 --> 00:04:42.650
to do it.

00:04:42.650 --> 00:04:43.720
It's not safe.

00:04:43.720 --> 00:04:46.140
But when you started doing
each of these scenarios,

00:04:46.140 --> 00:04:48.210
like driving from
point A to point B,

00:04:48.210 --> 00:04:49.830
communicating or
listening to media,

00:04:49.830 --> 00:04:52.050
and you can control
that with your voice,

00:04:52.050 --> 00:04:54.540
and it understands exactly
what you're trying to get done,

00:04:54.540 --> 00:04:56.190
then the product
becomes very magical.

00:04:56.190 --> 00:04:58.110
Because it's really
solving a real need

00:04:58.110 --> 00:04:59.610
and a real problem
in an environment

00:04:59.610 --> 00:05:01.463
that needs that help.

00:05:01.463 --> 00:05:02.880
TIMOTHY JORDAN:
Can you talk to us

00:05:02.880 --> 00:05:05.400
about how the next generation
of the Google Assistant

00:05:05.400 --> 00:05:08.340
will change the way that
people interact with it?

00:05:08.340 --> 00:05:08.940
Yeah.

00:05:08.940 --> 00:05:14.550
So if you think about when you
start forgetting about the how

00:05:14.550 --> 00:05:17.220
and you start worrying about
what is it that you want to do,

00:05:17.220 --> 00:05:21.900
you start relying on the
product in a very different way.

00:05:21.900 --> 00:05:24.090
That's when the behavior
change starts to happen.

00:05:24.090 --> 00:05:26.250
So in the first phase
of the Assistant,

00:05:26.250 --> 00:05:28.200
it should be very
responsive to you.

00:05:28.200 --> 00:05:30.000
It should understand
what you're saying

00:05:30.000 --> 00:05:32.010
and then help you get it done.

00:05:32.010 --> 00:05:34.290
As the product starts
learning from you,

00:05:34.290 --> 00:05:36.330
it can start getting
more proactive.

00:05:36.330 --> 00:05:39.170
So now when I get in my
car, it could flash to me

00:05:39.170 --> 00:05:41.340
some missed calls or
voicemails that I have so

00:05:41.340 --> 00:05:44.730
that if I want to return that
call, I'm reminded of that.

00:05:44.730 --> 00:05:47.130
Or if it knows
that in my calendar

00:05:47.130 --> 00:05:50.367
I have an appointment at
7 PM and it's 6 PM or 6:30

00:05:50.367 --> 00:05:52.950
and I haven't left, and it also
knows the information of where

00:05:52.950 --> 00:05:54.793
I'm trying to go and
knows the traffic,

00:05:54.793 --> 00:05:56.460
it could actually
send me a notification

00:05:56.460 --> 00:05:58.710
and let me know, hey, you're
probably going to be late

00:05:58.710 --> 00:05:59.960
if you don't leave now.

00:05:59.960 --> 00:06:01.800
And it's very interesting
when the product

00:06:01.800 --> 00:06:03.780
starts getting more proactive.

00:06:03.780 --> 00:06:05.520
I think that we're
going to see a switch.

00:06:05.520 --> 00:06:07.830
I mean, I see it a
lot with my kids.

00:06:07.830 --> 00:06:11.220
That in the same way that
when the screens became

00:06:11.220 --> 00:06:15.450
touch screens and people felt
that every screen is touchable,

00:06:15.450 --> 00:06:17.430
and you see a little kid
and they see an old TV

00:06:17.430 --> 00:06:18.570
and they try to
touch on the screen

00:06:18.570 --> 00:06:20.195
and the screen
doesn't work, I think

00:06:20.195 --> 00:06:21.570
that people are
going to get used

00:06:21.570 --> 00:06:25.950
to talking to their products and
expecting that the product will

00:06:25.950 --> 00:06:28.140
understand what you're
saying in a natural way

00:06:28.140 --> 00:06:29.182
and help you get it done.

00:06:29.182 --> 00:06:31.265
And I think that we're
going to see that evolution

00:06:31.265 --> 00:06:33.810
from basically understanding
your commands, what you want

00:06:33.810 --> 00:06:35.640
to do and help you get
it done, to getting

00:06:35.640 --> 00:06:38.320
very proactive, personal,
and more helpful,

00:06:38.320 --> 00:06:40.290
so anticipating your
needs and the things

00:06:40.290 --> 00:06:41.950
that you're trying
to accomplish.

00:06:41.950 --> 00:06:44.480
EMILY FORTUNA: So you've got
Assistant on your phone, home,

00:06:44.480 --> 00:06:46.020
and now driving.

00:06:46.020 --> 00:06:48.457
Where do you see Assistants
going in the future?

00:06:48.457 --> 00:06:50.290
MANUEL BRONSTEIN: So,
it's a great question.

00:06:50.290 --> 00:06:54.600
Look, I think that a personal
assistant needs to be with you.

00:06:54.600 --> 00:06:57.120
So of course, a lot of the work
that the team has been doing

00:06:57.120 --> 00:06:59.025
is making sure that the
product is available

00:06:59.025 --> 00:07:00.150
where you need it the most.

00:07:00.150 --> 00:07:01.680
I don't know
necessarily that it's

00:07:01.680 --> 00:07:03.650
going to go to
different places, right?

00:07:03.650 --> 00:07:06.410
I mean, at the end of the day,
we spend time in our homes,

00:07:06.410 --> 00:07:08.580
we spend time at work, we
spend time in our cars,

00:07:08.580 --> 00:07:10.230
we spend time with our phones--

00:07:10.230 --> 00:07:12.360
maybe too much time
with our phones.

00:07:12.360 --> 00:07:14.760
But if the product can
always be with you,

00:07:14.760 --> 00:07:18.040
then it can help you fulfill
some of those things.

00:07:18.040 --> 00:07:20.400
I think that in the future
what I'm hoping it helps--

00:07:20.400 --> 00:07:21.930
I mean, if you think
about the areas

00:07:21.930 --> 00:07:25.740
where people spend time
trying to get things done,

00:07:25.740 --> 00:07:27.210
that they wish that--

00:07:27.210 --> 00:07:28.740
you know, think about it, right?

00:07:28.740 --> 00:07:30.713
When you want to
go to a restaurant,

00:07:30.713 --> 00:07:32.130
the act of going
to the restaurant

00:07:32.130 --> 00:07:34.260
and having a nice meal
is awesome, right?

00:07:34.260 --> 00:07:37.218
Maybe calling to make a
reservation is not that great.

00:07:37.218 --> 00:07:39.510
But if the product can start
doing those things for you

00:07:39.510 --> 00:07:42.360
and remove that load
that load from you,

00:07:42.360 --> 00:07:43.650
it becomes incredibly useful.

00:07:43.650 --> 00:07:45.900
So I think that if you think
about more of those tasks

00:07:45.900 --> 00:07:48.690
that people are trying to get
done, and help you anticipate

00:07:48.690 --> 00:07:51.060
it, find that ticket
for an event for you

00:07:51.060 --> 00:07:54.960
that you want to go to, making
that appointment for you,

00:07:54.960 --> 00:07:57.090
and getting predictive
about those things,

00:07:57.090 --> 00:08:00.630
then the product becomes
extremely magical.

00:08:00.630 --> 00:08:02.130
TIMOTHY JORDAN:
Before you leave us,

00:08:02.130 --> 00:08:04.770
are there any hints you can
give us to find some Easter

00:08:04.770 --> 00:08:05.790
eggs in the Assistant?

00:08:05.790 --> 00:08:06.748
MANUEL BRONSTEIN: Sure.

00:08:06.748 --> 00:08:09.180
We recently launched a
new voice for the system

00:08:09.180 --> 00:08:10.520
with John Legend.

00:08:10.520 --> 00:08:11.768
And it's pretty cool.

00:08:11.768 --> 00:08:14.310
And if you asked, actually, if
you turn the voice to the John

00:08:14.310 --> 00:08:15.768
Legend voice for
Assistant, and you

00:08:15.768 --> 00:08:17.976
ask it to sing a song for
you or sing happy birthday,

00:08:17.976 --> 00:08:20.018
you're going to hear John
Legend singing for you.

00:08:20.018 --> 00:08:21.460
And it's actually pretty cool.

00:08:21.460 --> 00:08:22.220
EMILY FORTUNA:
Thank you, Manuel,

00:08:22.220 --> 00:08:23.870
so much for sharing
your insights.

00:08:23.870 --> 00:08:25.453
MANUEL BRONSTEIN:
So great to be here.

00:08:25.453 --> 00:08:27.540
[MUSIC PLAYING]

00:08:50.310 --> 00:08:52.340
EMILY FORTUNA: Welcome
to the boardwalk at I/O.

00:08:52.340 --> 00:08:55.450
This space really
captures the festival vibe

00:08:55.450 --> 00:08:57.950
and it's one thing
that makes I/O special.

00:08:57.950 --> 00:09:00.020
This year, we collaborate
with a bunch of artists

00:09:00.020 --> 00:09:02.560
to combine our tech
with their talent.

00:09:02.560 --> 00:09:13.530
[MUSIC PLAYING]

00:09:13.530 --> 00:09:16.230
This interactive
easel takes my doodles

00:09:16.230 --> 00:09:18.690
and makes them museum
quality works of art.

00:09:18.690 --> 00:09:21.400
So what I've drawn here is
obviously "Starry Night."

00:09:31.669 --> 00:09:32.647
[LAUGHTER]

00:09:32.647 --> 00:09:33.572
Whoa!

00:09:33.572 --> 00:09:34.114
AUDIENCE: Hi!

00:09:36.818 --> 00:09:38.026
EMILY FORTUNA: That's my job.

00:09:40.970 --> 00:09:43.460
This art installation is
called "Mosaic Virus."

00:09:43.460 --> 00:09:45.290
It uses machine
learning to create

00:09:45.290 --> 00:09:47.390
flowers whose
appearance is based

00:09:47.390 --> 00:09:50.220
on the fluctuation of bitcoin.

00:09:50.220 --> 00:09:51.200
They're pretty nice.

00:09:51.200 --> 00:09:53.033
I kind of want some of
these in my backyard.

00:09:56.570 --> 00:09:58.240
I'm here with Kenric and Alex.

00:09:58.240 --> 00:10:00.760
And they curated all of
the awesome works of art

00:10:00.760 --> 00:10:02.190
here on the boardwalk.

00:10:02.190 --> 00:10:06.252
Kenric, please tell us about
them and how you curated them.

00:10:06.252 --> 00:10:07.960
KENRIC MCDOWELL: Well,
we've been working

00:10:07.960 --> 00:10:09.258
with artists for a while.

00:10:09.258 --> 00:10:11.550
And particularly in the work
I do working with artists,

00:10:11.550 --> 00:10:13.940
I use artificial intelligence
and machine learning.

00:10:13.940 --> 00:10:17.740
And so we wanted to highlight
some collaborative projects

00:10:17.740 --> 00:10:20.290
by artists working with
machine learning that

00:10:20.290 --> 00:10:23.500
provide representation
of natural phenomena,

00:10:23.500 --> 00:10:26.440
like onformative's piece
here "Meandering River,"

00:10:26.440 --> 00:10:29.990
or Anna Riddler's "Mosaic
Virus with Tulips."

00:10:29.990 --> 00:10:33.820
So we really wanted people
to see images of themselves

00:10:33.820 --> 00:10:37.050
in collaboration with nature
through machine learning.

00:10:37.050 --> 00:10:39.006
EMILY FORTUNA: Very cool.

00:10:39.006 --> 00:10:41.520
And you alluded to
this, Kenric, this

00:10:41.520 --> 00:10:45.095
is an ongoing collaboration,
not just happening at I/O. Alex,

00:10:45.095 --> 00:10:46.470
can you tell us
a little bit more

00:10:46.470 --> 00:10:47.880
about the history behind that?

00:10:47.880 --> 00:10:48.110
ALEX CZETWERTYNSKI: Yeah.

00:10:48.110 --> 00:10:50.760
So for the last two years--
this is the third year

00:10:50.760 --> 00:10:53.280
that we've been doing
an arts program at I/O.

00:10:53.280 --> 00:10:55.670
And every year it grows.

00:10:55.670 --> 00:10:57.810
And it feels great
to see how there's

00:10:57.810 --> 00:10:59.960
support for this
kind of presentation

00:10:59.960 --> 00:11:01.440
at an event like this.

00:11:01.440 --> 00:11:03.300
And I think that this
year we're changing

00:11:03.300 --> 00:11:04.330
the format a little bit.

00:11:04.330 --> 00:11:07.500
And we're seeing how
essentially bringing the artwork

00:11:07.500 --> 00:11:11.520
on a place like the boardwalk
is allowing a deeper interaction

00:11:11.520 --> 00:11:12.730
with them.

00:11:12.730 --> 00:11:15.150
And I think the audience
is really responding really

00:11:15.150 --> 00:11:17.988
well to this kind of work,
like the "Formatives" piece.

00:11:17.988 --> 00:11:19.530
EMILY FORTUNA: Alex
and Kenric, thank

00:11:19.530 --> 00:11:22.830
you so much for sharing
and making this happen.

00:11:22.830 --> 00:11:24.660
It's really a lovely
place to hang out.

00:11:24.660 --> 00:11:25.910
ALEX CZETWERTYNSKI: Thank you.

00:11:28.832 --> 00:11:30.684
EMILY FORTUNA: Hey, guys!

00:11:30.684 --> 00:11:31.267
AUDIENCE: Hey!

00:11:41.660 --> 00:11:42.910
TIMOTHY JORDAN: Thanks, Emily.

00:11:42.910 --> 00:11:44.290
On this end of
the boardwalk, I'm

00:11:44.290 --> 00:11:46.660
joined by former attorney,
human rights activist,

00:11:46.660 --> 00:11:48.800
and inclusive
designer, Alex Roy.

00:11:48.800 --> 00:11:50.680
Lisa, thanks for
coming to I/O Live.

00:11:50.680 --> 00:11:52.610
ELISE ROY: Thanks
for having me here.

00:11:52.610 --> 00:11:55.027
TIMOTHY JORDAN: For those that
don't know your back story,

00:11:55.027 --> 00:11:56.530
you lost your hearing at age 10.

00:11:56.530 --> 00:11:57.260
ELISE ROY: I did.

00:11:57.260 --> 00:11:57.790
TIMOTHY JORDAN:
Which is why we're

00:11:57.790 --> 00:11:59.650
going to be using
Google's live transcribe

00:11:59.650 --> 00:12:02.920
app to make our conversation
happen in real time?

00:12:02.920 --> 00:12:06.010
Can you tell us what it means
to be an inclusive designer?

00:12:06.010 --> 00:12:07.180
ELISE ROY: Absolutely.

00:12:07.180 --> 00:12:10.720
So an inclusive
designer is someone

00:12:10.720 --> 00:12:13.240
that helps organizations
solve problems

00:12:13.240 --> 00:12:16.080
from the perspective of
people with disabilities

00:12:16.080 --> 00:12:17.350
or difference.

00:12:17.350 --> 00:12:21.070
It's based on this
human-centered design tenet

00:12:21.070 --> 00:12:24.370
that when we solve
for extremes, we often

00:12:24.370 --> 00:12:29.020
develop better solutions
than we solve for average.

00:12:29.020 --> 00:12:32.510
And that might seem kind
of counterintuitive,

00:12:32.510 --> 00:12:34.660
because the greatest
number of people

00:12:34.660 --> 00:12:38.590
tend to hover around average.

00:12:38.590 --> 00:12:43.750
But the problem is that no
one actually is average.

00:12:43.750 --> 00:12:47.590
And so when we solve
for average, then--

00:12:47.590 --> 00:12:50.190
and people are a little bit
over here, a little bit there--

00:12:50.190 --> 00:12:52.810
there's this gap
between what we create

00:12:52.810 --> 00:12:56.550
and what their needs are
that we're not meeting.

00:12:56.550 --> 00:13:00.250
If instead we solve
for those extremes,

00:13:00.250 --> 00:13:02.920
then we've already
covered that gap

00:13:02.920 --> 00:13:04.780
and we've created
something that's

00:13:04.780 --> 00:13:08.462
a lot more adaptable and
a lot more innovative.

00:13:08.462 --> 00:13:10.420
TIMOTHY JORDAN: That
makes so much sense to me.

00:13:10.420 --> 00:13:11.500
And it reminds me
of something else

00:13:11.500 --> 00:13:13.875
that you've talked about,
which is human-centered design.

00:13:13.875 --> 00:13:15.620
Can you define that
term for us, as well?

00:13:15.620 --> 00:13:17.080
ELISE ROY: Sure.

00:13:17.080 --> 00:13:21.640
So human-centered design is
a philosophy, a methodology

00:13:21.640 --> 00:13:24.080
for problem solving.

00:13:24.080 --> 00:13:26.170
It's based on a lot
of things that we've

00:13:26.170 --> 00:13:27.730
taken from the design room.

00:13:27.730 --> 00:13:31.630
And it can be applied to almost
every realm you could imagine

00:13:31.630 --> 00:13:35.260
from international development,
to privacy, to the sciences,

00:13:35.260 --> 00:13:36.970
to technology.

00:13:36.970 --> 00:13:43.300
And what differentiates it is
it's almost obsessive focus

00:13:43.300 --> 00:13:46.940
on designing around the human
that we're trying to reach.

00:13:46.940 --> 00:13:50.290
So in the past a lot
of times we think

00:13:50.290 --> 00:13:53.050
that when we're trying
to solve a problem,

00:13:53.050 --> 00:13:56.380
we want to get all the
experts into this room

00:13:56.380 --> 00:13:59.660
and have them figure
out how to solve it.

00:13:59.660 --> 00:14:01.660
The problem with
that is they have

00:14:01.660 --> 00:14:04.230
one point of view, their own.

00:14:04.230 --> 00:14:07.330
So if, for example, they're
a bunch of developers,

00:14:07.330 --> 00:14:10.480
they're probably going to create
something that's very complex

00:14:10.480 --> 00:14:13.780
and most of us, the
so-called average people,

00:14:13.780 --> 00:14:16.130
don't really understand.

00:14:16.130 --> 00:14:19.750
If instead we focus our
process on the people

00:14:19.750 --> 00:14:22.960
we're trying to reach, then
we develop something that

00:14:22.960 --> 00:14:25.480
fits their needs much better.

00:14:25.480 --> 00:14:28.450
And so there's a lot
of empathy involved,

00:14:28.450 --> 00:14:30.760
where you're trying
to understand

00:14:30.760 --> 00:14:31.810
what their needs are.

00:14:31.810 --> 00:14:35.230
You're following them around
in real life situations.

00:14:35.230 --> 00:14:38.050
You want to make
sure that you're also

00:14:38.050 --> 00:14:39.430
answering the right problem.

00:14:39.430 --> 00:14:43.940
So many people are
answering the wrong problem.

00:14:43.940 --> 00:14:47.560
And then there's ideation, which
is similar to brainstorming,

00:14:47.560 --> 00:14:51.850
but we use a lot
of design methods

00:14:51.850 --> 00:14:54.370
to really get people
thinking outside the box.

00:14:54.370 --> 00:14:55.610
It's a lot more powerful.

00:14:55.610 --> 00:14:58.510
We want people to do
a lot of crazy ideas.

00:14:58.510 --> 00:15:01.060
We want as many as
they can think of.

00:15:01.060 --> 00:15:05.950
And then we prototype,
which is a low fi

00:15:05.950 --> 00:15:08.520
solution that we
know we're going

00:15:08.520 --> 00:15:10.630
to mess up with, you know?

00:15:10.630 --> 00:15:14.780
But that's the purpose of it is
to fail and to learn from it.

00:15:14.780 --> 00:15:18.640
And then the last step is to
implement it and make sure

00:15:18.640 --> 00:15:20.470
that it's sustainable.

00:15:20.470 --> 00:15:25.570
TIMOTHY JORDAN: I love this
idea of having practices

00:15:25.570 --> 00:15:28.600
and mechanisms for
helping experts get out

00:15:28.600 --> 00:15:30.355
of their perspective.

00:15:30.355 --> 00:15:32.230
Because when you bring
an expert in the room,

00:15:32.230 --> 00:15:35.080
you're asking them to give
their expert opinion, not

00:15:35.080 --> 00:15:36.280
their general opinion.

00:15:36.280 --> 00:15:38.820
And that's so focused
most of the time, right?

00:15:38.820 --> 00:15:40.610
ELISE ROY: Exactly, exactly--

00:15:40.610 --> 00:15:42.700
that's also why I
love inclusive design.

00:15:42.700 --> 00:15:45.350
Because that brings people
even further outside

00:15:45.350 --> 00:15:46.586
of their tunnel vision.

00:15:46.586 --> 00:15:49.220
TIMOTHY JORDAN: This reminds
me of another quote of yours,

00:15:49.220 --> 00:15:53.470
if I may, you said, "When we
look for gaps of exclusion,

00:15:53.470 --> 00:15:56.110
we identify opportunities
for great change."

00:15:56.110 --> 00:15:58.120
And I find this
really inspiring.

00:15:58.120 --> 00:16:00.460
Do you have some examples
that you can share where

00:16:00.460 --> 00:16:02.500
you see this really clearly?

00:16:02.500 --> 00:16:03.760
ELISE ROY: Definitely--

00:16:03.760 --> 00:16:05.690
I can give you some
Google-centric examples.

00:16:05.690 --> 00:16:06.520
TIMOTHY JORDAN: OK.

00:16:06.520 --> 00:16:12.640
ELISE ROY: So Google's Chief
Internet evangelist Vint Cert,

00:16:12.640 --> 00:16:16.930
he created email
for his deaf wife.

00:16:16.930 --> 00:16:23.440
And as you know, that's become a
huge thing that everyone loves.

00:16:23.440 --> 00:16:26.110
But he didn't see
it as something

00:16:26.110 --> 00:16:30.040
that would be applied
to everyone initially.

00:16:30.040 --> 00:16:33.310
And most people didn't think
of it in that way at first.

00:16:33.310 --> 00:16:37.630
Because we had the capabilities
to talk to people on the phone.

00:16:37.630 --> 00:16:40.170
And that was the way
that business was run.

00:16:40.170 --> 00:16:43.580
But slowly, we realized
that people also

00:16:43.580 --> 00:16:46.550
have this desire to not
have to keep talking

00:16:46.550 --> 00:16:49.130
to someone on the phone.

00:16:49.130 --> 00:16:52.490
And they want to say something
real quick and be done with it.

00:16:52.490 --> 00:16:55.430
And sometimes we
also have this desire

00:16:55.430 --> 00:17:00.770
to formulate our thoughts before
we communicate with someone.

00:17:00.770 --> 00:17:04.460
And email fills
those hidden desires.

00:17:04.460 --> 00:17:07.700
Another example is YouTube.

00:17:07.700 --> 00:17:12.079
They inserted closed captioning
capabilities into their media

00:17:12.079 --> 00:17:15.230
player, which was really
amazing at the time.

00:17:15.230 --> 00:17:20.180
And closed captioning was
originally created for the deaf

00:17:20.180 --> 00:17:23.400
so that they could follow along.

00:17:23.400 --> 00:17:29.030
But what YouTube realized was
that if a video has closed

00:17:29.030 --> 00:17:33.050
captioning capabilities,
then the likelihood

00:17:33.050 --> 00:17:37.600
of it being watched to the
end was increased by 80%.

00:17:37.600 --> 00:17:38.570
TIMOTHY JORDAN: Wow.

00:17:38.570 --> 00:17:39.230
ELISE ROY: Yes.

00:17:39.230 --> 00:17:41.210
TIMOTHY JORDAN: Is that because
there's so many situations

00:17:41.210 --> 00:17:42.620
where you have the sound off?

00:17:42.620 --> 00:17:43.460
ELISE ROY: Exactly.

00:17:43.460 --> 00:17:44.252
TIMOTHY JORDAN: OK.

00:17:44.252 --> 00:17:48.140
ELISE ROY: So the reason why
designing for inclusion works

00:17:48.140 --> 00:17:52.460
is because we find ourselves,
as we go about our lives,

00:17:52.460 --> 00:17:55.850
in these momentary
situations where

00:17:55.850 --> 00:17:57.790
we have a momentary disability.

00:17:57.790 --> 00:17:59.330
So we might be
carrying something,

00:17:59.330 --> 00:18:02.450
and we lose the use of our arms.

00:18:02.450 --> 00:18:05.090
If we're in a bar,
closed captioning

00:18:05.090 --> 00:18:07.040
is helpful because
we can't hear.

00:18:07.040 --> 00:18:10.386
And if we are in
our office, and we

00:18:10.386 --> 00:18:14.593
want to sneak in a Facebook
video, captioning is helpful.

00:18:14.593 --> 00:18:16.010
TIMOTHY JORDAN:
So that's awesome.

00:18:16.010 --> 00:18:17.343
Those are really great examples.

00:18:17.343 --> 00:18:19.370
And I love how it's
really connecting

00:18:19.370 --> 00:18:22.520
this idea of designing
for the extremes

00:18:22.520 --> 00:18:24.608
and then it becomes more
usable for everyone.

00:18:24.608 --> 00:18:25.650
ELISE ROY: Exactly, yeah.

00:18:25.650 --> 00:18:27.067
TIMOTHY JORDAN:
It's really great.

00:18:27.067 --> 00:18:29.270
Elise, thank you so much
for joining us and sharing

00:18:29.270 --> 00:18:30.478
your gifts and your insights.

00:18:30.478 --> 00:18:31.590
I really appreciate it.

00:18:31.590 --> 00:18:33.382
ELISE ROY: Thank you
so much for having me.

00:18:33.382 --> 00:18:34.778
I really appreciate it.

00:18:34.778 --> 00:18:35.570
TIMOTHY JORDAN: OK.

00:18:35.570 --> 00:18:36.737
It's time for another break.

00:18:36.737 --> 00:18:39.080
Because the next sessions
are about to begin.

00:18:39.080 --> 00:18:40.700
You're watching I/O Live.

00:18:40.700 --> 00:18:43.190
[MUSIC PLAYING]

00:19:04.263 --> 00:19:05.930
EMILY FORTUNA: Welcome
back to I/O Live,

00:19:05.930 --> 00:19:08.360
your ticket to an
insider's view of I/O'19.

00:19:08.360 --> 00:19:09.280
I'm Emily Fortuna.

00:19:09.280 --> 00:19:11.780
If you like games, you're going
to enjoy what's coming next.

00:19:11.780 --> 00:19:16.070
Let's send it over Todd for a
look at the mega hamster demo.

00:19:16.070 --> 00:19:17.030
AUDIENCE: Hey!

00:19:17.030 --> 00:19:18.470
[MUSIC PLAYING]

00:19:22.187 --> 00:19:23.270
[BACKGROUND CONVERSATIONS]

00:19:23.270 --> 00:19:25.760
TODD KERPELMAN: I am here at
the gaming garage with Darren

00:19:25.760 --> 00:19:29.420
Hilton checking out what is sure
to be the next great big game,

00:19:29.420 --> 00:19:30.200
Mega Hamster.

00:19:30.200 --> 00:19:30.880
Hi, Darren.

00:19:30.880 --> 00:19:31.620
DARREN HILTON: Hi, Todd.

00:19:31.620 --> 00:19:31.860
How are you doing?

00:19:31.860 --> 00:19:32.735
TODD KERPELMAN: Good.

00:19:32.735 --> 00:19:34.280
Now the last time
I saw Mega Hamster,

00:19:34.280 --> 00:19:36.710
it did not have this
multiplayer component

00:19:36.710 --> 00:19:38.360
that I'm seeing
going on behind me.

00:19:38.360 --> 00:19:39.692
What's going on here?

00:19:39.692 --> 00:19:42.150
DARREN HILTON: So this is a
project that we did with Cloud.

00:19:42.150 --> 00:19:44.870
And so we decided to incorporate
the Agones and Open Match

00:19:44.870 --> 00:19:46.940
open source projects
into Mega Hamster.

00:19:46.940 --> 00:19:48.232
TODD KERPELMAN: What are those?

00:19:48.232 --> 00:19:49.230
Start with Open Match.

00:19:49.230 --> 00:19:51.647
DARREN HILTON: So Open Match
is a framework for connecting

00:19:51.647 --> 00:19:53.332
clients to our game servers.

00:19:53.332 --> 00:19:54.290
TODD KERPELMAN: Gotcha.

00:19:54.290 --> 00:19:57.380
Meaning that if you've got
a bunch of players that

00:19:57.380 --> 00:19:59.210
want to play a
multiplayer game together,

00:19:59.210 --> 00:20:01.250
I would use Open Match
to help connect them

00:20:01.250 --> 00:20:02.825
based on what kind of criteria.

00:20:02.825 --> 00:20:04.610
DARREN HILTON: Based on the
criteria that you lay out,

00:20:04.610 --> 00:20:06.193
their skill level,
that kind of thing.

00:20:06.193 --> 00:20:08.526
You can go in and kind of dig
in and give that criteria.

00:20:08.526 --> 00:20:10.068
TODD KERPELMAN: So
that kind of helps

00:20:10.068 --> 00:20:11.540
solve the problem
of finding people

00:20:11.540 --> 00:20:12.500
that actually want to play--

00:20:12.500 --> 00:20:12.940
DARREN HILTON: Exactly.

00:20:12.940 --> 00:20:14.525
TODD KERPELMAN: --with
maybe the same skill level.

00:20:14.525 --> 00:20:14.810
DARREN HILTON: Yes.

00:20:14.810 --> 00:20:16.890
TODD KERPELMAN: Or in
the same general area,

00:20:16.890 --> 00:20:18.190
have good network latency.

00:20:18.190 --> 00:20:19.840
DARREN HILTON: Exactly, exactly.

00:20:19.840 --> 00:20:20.210
TODD KERPELMAN: Excellent.

00:20:20.210 --> 00:20:21.380
DARREN HILTON: You don't want
to have that experience where

00:20:21.380 --> 00:20:23.870
you have low level players
being connected with high level

00:20:23.870 --> 00:20:27.425
players and then getting
a bad game experience.

00:20:27.425 --> 00:20:28.800
TODD KERPELMAN:
And then Agones--

00:20:28.800 --> 00:20:29.850
what is that?

00:20:29.850 --> 00:20:34.580
DARREN HILTON: So Agones is
a multiplayer gaming server

00:20:34.580 --> 00:20:36.548
application that's
built on Kubernetes.

00:20:36.548 --> 00:20:37.340
TODD KERPELMAN: OK.

00:20:37.340 --> 00:20:40.840
And so I know there's a lot
of multiplayer game servers

00:20:40.840 --> 00:20:41.340
out there.

00:20:41.340 --> 00:20:43.040
What makes Agones special?

00:20:43.040 --> 00:20:45.380
DARREN HILTON: So Agones is
special in that it can scale

00:20:45.380 --> 00:20:46.740
to the needs of your game.

00:20:46.740 --> 00:20:49.525
So if you wanted to add
thousands of servers--

00:20:49.525 --> 00:20:51.650
TODD KERPELMAN: You mean,
when Mega Hamster becomes

00:20:51.650 --> 00:20:54.230
a mega hit, we got millions and
Millions of people playing it.

00:20:54.230 --> 00:20:55.370
DARREN HILTON: And we totally
expect that to happen.

00:20:55.370 --> 00:20:55.900
TODD KERPELMAN: Of course.

00:20:55.900 --> 00:20:57.400
DARREN HILTON: It's
going to happen.

00:20:57.400 --> 00:21:00.393
The other thing, too, is
that it's cloud agnostic.

00:21:00.393 --> 00:21:01.560
It's an open source project.

00:21:01.560 --> 00:21:04.490
You can run it on any
cloud service you want.

00:21:04.490 --> 00:21:05.460
You can run it locally.

00:21:05.460 --> 00:21:07.755
You could run it on frame
on your own service.

00:21:07.755 --> 00:21:08.630
TODD KERPELMAN: Yeah.

00:21:08.630 --> 00:21:11.180
And so because it's
based on Kubernetes,

00:21:11.180 --> 00:21:12.680
it can scale to sort of--

00:21:12.680 --> 00:21:13.160
DARREN HILTON: It can scale.

00:21:13.160 --> 00:21:14.060
TODD KERPELMAN:
--basically, until I

00:21:14.060 --> 00:21:16.548
run out of servers or until
Google runs out of servers,

00:21:16.548 --> 00:21:17.840
which is going to take a while.

00:21:17.840 --> 00:21:18.240
DARREN HILTON: Exactly.

00:21:18.240 --> 00:21:20.120
And everything you
need for that server

00:21:20.120 --> 00:21:22.822
is contained in that
Kubernetes container.

00:21:22.822 --> 00:21:23.780
TODD KERPELMAN: Gotcha.

00:21:23.780 --> 00:21:26.210
And so now, with a dedicated
game server like Agones,

00:21:26.210 --> 00:21:29.090
I know sometimes in multiplayer
games, particularly mobile

00:21:29.090 --> 00:21:31.400
ones, you'll have the
clients talking to each other

00:21:31.400 --> 00:21:33.260
directly in a
peer-to-peer network.

00:21:33.260 --> 00:21:35.198
What makes a dedicated
game server better?

00:21:35.198 --> 00:21:36.990
DARREN HILTON: So the
dedicated game server

00:21:36.990 --> 00:21:40.520
gives you the added level
of security and that--

00:21:40.520 --> 00:21:42.620
that's [INAUDIBLE] truth.

00:21:42.620 --> 00:21:45.900
And the scalability's
another thing.

00:21:45.900 --> 00:21:47.990
So you can scale to
however you want it to,

00:21:47.990 --> 00:21:50.330
scale to meet the
needs of your game.

00:21:50.330 --> 00:21:53.660
And a peer-to-peer network is
relatively limited in that,

00:21:53.660 --> 00:21:55.790
generally limited to a
smaller amount of clients.

00:21:55.790 --> 00:21:57.480
TODD KERPELMAN: You can't
have 100 clients all talking--

00:21:57.480 --> 00:21:58.000
DARREN HILTON: You
can't have 100 clients.

00:21:58.000 --> 00:21:58.880
TODD KERPELMAN:
--that would just--

00:21:58.880 --> 00:22:01.490
that poor one client that's
acting as your server

00:22:01.490 --> 00:22:02.435
for that episode.

00:22:02.435 --> 00:22:04.310
DARREN HILTON: Somebody
could mess with that.

00:22:04.310 --> 00:22:05.685
It could lose it's game state.

00:22:05.685 --> 00:22:08.060
You know, there's a lot of
things can go wrong with that.

00:22:08.060 --> 00:22:08.750
TODD KERPELMAN: Gotcha.

00:22:08.750 --> 00:22:11.420
I'm guessing latency is probably
more likely to be consistent,

00:22:11.420 --> 00:22:14.120
because you're not
relying on that one client

00:22:14.120 --> 00:22:16.372
to be the server
for the entire game.

00:22:16.372 --> 00:22:17.330
DARREN HILTON: Exactly.

00:22:17.330 --> 00:22:18.288
TODD KERPELMAN: Gotcha.

00:22:18.288 --> 00:22:18.812
All right.

00:22:18.812 --> 00:22:20.270
And then I couldn't
help but notice

00:22:20.270 --> 00:22:23.090
you're wearing a very
stylish Firebase t-shirt.

00:22:23.090 --> 00:22:25.672
So Firebase is integrated
in this game, as well.

00:22:25.672 --> 00:22:26.630
DARREN HILTON: Exactly.

00:22:26.630 --> 00:22:28.463
So we're using a bunch
of Firebase products.

00:22:28.463 --> 00:22:32.300
We're using authentication,
cloud messaging, remote config,

00:22:32.300 --> 00:22:35.125
real time database,
and Crashlytics.

00:22:35.125 --> 00:22:36.500
TODD KERPELMAN:
Oh, Crashlytics--

00:22:36.500 --> 00:22:37.332
that's new, right?

00:22:37.332 --> 00:22:38.540
DARREN HILTON: New as of GDC.

00:22:38.540 --> 00:22:39.623
TODD KERPELMAN: All right.

00:22:39.623 --> 00:22:41.100
And is this running on Unity?

00:22:41.100 --> 00:22:41.920
DARREN HILTON: It's
running on Unity.

00:22:41.920 --> 00:22:43.503
TODD KERPELMAN: And
so now Crashlytics

00:22:43.503 --> 00:22:45.740
for Unity will actually
let me, as a developer,

00:22:45.740 --> 00:22:47.570
find out if there are
crashes in my game

00:22:47.570 --> 00:22:48.040
and where they're happening.

00:22:48.040 --> 00:22:48.998
DARREN HILTON: Exactly.

00:22:48.998 --> 00:22:50.900
So you can go through,
find your stack trace,

00:22:50.900 --> 00:22:53.482
and then narrow in on the bugs
that are affecting your game.

00:22:53.482 --> 00:22:54.440
TODD KERPELMAN: Gotcha.

00:22:54.440 --> 00:22:57.852
And have you found lots of
crashes in Mega Hamster?

00:22:57.852 --> 00:22:59.060
DARREN HILTON: Of course not.

00:22:59.060 --> 00:22:59.900
TODD KERPELMAN: Well,
of course, [INAUDIBLE]..

00:22:59.900 --> 00:23:01.087
Because it is solid code.

00:23:01.087 --> 00:23:02.170
DARREN HILTON: Rock solid.

00:23:02.170 --> 00:23:04.640
TODD KERPELMAN: Rock solid
code that never crashes.

00:23:04.640 --> 00:23:06.340
And with remote
config, I know one

00:23:06.340 --> 00:23:07.917
of the fun things
of remote config,

00:23:07.917 --> 00:23:09.500
especially as a game
developer, if you

00:23:09.500 --> 00:23:12.960
find some level is too hard or
some setting is too difficult,

00:23:12.960 --> 00:23:14.510
you can basically
kind of tweak that.

00:23:14.510 --> 00:23:16.820
And all your clients get
to get updated on the fly.

00:23:16.820 --> 00:23:17.700
DARREN HILTON: You
can turn that, yeah.

00:23:17.700 --> 00:23:19.408
And you can roll out
to all your clients,

00:23:19.408 --> 00:23:21.618
or you can roll out to a
segment of players, as well.

00:23:21.618 --> 00:23:23.660
TODD KERPELMAN: Like sort
of running an A/B test?

00:23:23.660 --> 00:23:24.520
DARREN HILTON: Yeah, exactly.

00:23:24.520 --> 00:23:25.270
You can test that.

00:23:25.270 --> 00:23:27.860
And if it's stable, then you can
roll up to everyone, as well.

00:23:27.860 --> 00:23:30.360
TODD KERPELMAN: If I wanted to
try out Mega Hamster at home,

00:23:30.360 --> 00:23:31.730
how would I be able to do that?

00:23:31.730 --> 00:23:33.980
DARREN HILTON: You can
download the single player game

00:23:33.980 --> 00:23:35.870
from the Play Store
or from the app store.

00:23:35.870 --> 00:23:36.953
TODD KERPELMAN: All right.

00:23:36.953 --> 00:23:38.900
And last question,
were any hamsters

00:23:38.900 --> 00:23:41.652
harmed in the making
of Mega Hamster?

00:23:41.652 --> 00:23:42.860
DARREN HILTON: I believe not.

00:23:42.860 --> 00:23:43.190
TODD KERPELMAN: All right.

00:23:43.190 --> 00:23:44.480
Well, that's always
good to hear.

00:23:44.480 --> 00:23:45.650
And with that, we're
going to take you over

00:23:45.650 --> 00:23:47.930
to Page Bailey, who's going
to tell you more about how

00:23:47.930 --> 00:23:49.380
to make your apps better.

00:23:49.380 --> 00:23:50.600
So take it away, Page.

00:23:55.374 --> 00:23:56.832
PAIGE BAILEY: Thanks
so much, Todd.

00:23:56.832 --> 00:23:59.740
I'm Paige Bailey, a
TensorFlow developer advocate.

00:23:59.740 --> 00:24:01.890
And I'm here today with
Doug Stevenson, who

00:24:01.890 --> 00:24:03.640
is going to tell us
all about the latest

00:24:03.640 --> 00:24:05.380
and greatest in Firebase.

00:24:05.380 --> 00:24:05.530
DOUG STEVENSON: Yeah.

00:24:05.530 --> 00:24:07.110
PAIGE BAILEY: So show us
what you have planned, Doug.

00:24:07.110 --> 00:24:07.902
DOUG STEVENSON: OK.

00:24:07.902 --> 00:24:09.730
So turns out
Firebase has a bunch

00:24:09.730 --> 00:24:12.610
of products that help you ensure
high quality of your app, which

00:24:12.610 --> 00:24:14.770
is very important for a
lot of app developers.

00:24:14.770 --> 00:24:18.040
Every app developer always has
to be able to diagnose crashes

00:24:18.040 --> 00:24:18.660
in their app--

00:24:18.660 --> 00:24:19.060
PAIGE BAILEY: Yes.

00:24:19.060 --> 00:24:20.310
DOUG STEVENSON: --as you know.

00:24:20.310 --> 00:24:23.020
So Firebase has a tool
called Crashlytics.

00:24:23.020 --> 00:24:25.770
With Crashlytics, the
integration is fairly simple.

00:24:25.770 --> 00:24:26.770
Add a plug-in.

00:24:26.770 --> 00:24:28.660
Add a build dependency
to your build.

00:24:28.660 --> 00:24:30.280
And it just starts
monitoring crashes.

00:24:30.280 --> 00:24:32.240
You don't have to write
any lines of code.

00:24:32.240 --> 00:24:33.330
So the app is crashing.

00:24:33.330 --> 00:24:36.560
You can see that it's rather
problematic for our users.

00:24:36.560 --> 00:24:38.767
But it turns out the engineers
made a good decision,

00:24:38.767 --> 00:24:40.600
and they put this new
home screen experiment

00:24:40.600 --> 00:24:41.767
behind a remote config flag.

00:24:41.767 --> 00:24:43.517
PAIGE BAILEY: So tell
me a little bit more

00:24:43.517 --> 00:24:44.684
about what remote config is.

00:24:44.684 --> 00:24:45.600
DOUG STEVENSON: Right.

00:24:45.600 --> 00:24:47.830
So remote config lets you
define key value pairs

00:24:47.830 --> 00:24:49.180
in the Firebase console.

00:24:49.180 --> 00:24:50.930
And then you can publish
those to your app

00:24:50.930 --> 00:24:54.160
without having to actually write
any lines of code in your app.

00:24:54.160 --> 00:24:57.223
So it gets them all,
almost immediately,

00:24:57.223 --> 00:24:58.640
as you change them
in the console.

00:24:58.640 --> 00:25:01.128
So the idea here is we
have a new parameter.

00:25:01.128 --> 00:25:02.920
It's called home screen
experiment enabled.

00:25:02.920 --> 00:25:04.170
And the default value is true.

00:25:04.170 --> 00:25:07.060
So this is what's enabling the
new home screen experiment.

00:25:07.060 --> 00:25:09.320
That's also causing
devices to crash.

00:25:09.320 --> 00:25:11.410
Now what we want to do
here is ask our engineer.

00:25:11.410 --> 00:25:14.420
And so our engineer thinks
that it's a translation issue.

00:25:14.420 --> 00:25:17.230
So we've narrowed it down
to Spanish speakers who

00:25:17.230 --> 00:25:18.500
are using the app on Android.

00:25:18.500 --> 00:25:21.220
So we can define a new
condition here for this flag

00:25:21.220 --> 00:25:22.840
and say, for
Spanish speakers who

00:25:22.840 --> 00:25:25.585
are using Android, what
we can do is disable

00:25:25.585 --> 00:25:26.710
the home screen experiment.

00:25:26.710 --> 00:25:28.600
So we'll set it to
false for all of them,

00:25:28.600 --> 00:25:31.735
and all the other users
can retain the value true.

00:25:31.735 --> 00:25:33.610
So now once this is
configured, what we'll do

00:25:33.610 --> 00:25:35.270
is publish this to our app.

00:25:35.270 --> 00:25:38.350
And the app, internally, will
use Firebase remote config

00:25:38.350 --> 00:25:41.110
to fetch parameters, pull
out that home screen flag,

00:25:41.110 --> 00:25:43.000
and enable the home
screen experiment if it's

00:25:43.000 --> 00:25:44.760
been enabled for this device.

00:25:44.760 --> 00:25:46.510
PAIGE BAILEY: And it's
amazing that you're

00:25:46.510 --> 00:25:49.150
able to narrow in with
such laser focus what

00:25:49.150 --> 00:25:52.263
the problem could be, and then
to change it without altering

00:25:52.263 --> 00:25:53.180
a single line of code.

00:25:53.180 --> 00:25:53.500
DOUG STEVENSON: Right.

00:25:53.500 --> 00:25:54.000
Right.

00:25:54.000 --> 00:25:56.390
So it's remotely fixing
your app, in some cases.

00:25:56.390 --> 00:25:59.110
And we do recommend that
people publish their apps

00:25:59.110 --> 00:26:01.450
with new features hidden
behind remote config flags

00:26:01.450 --> 00:26:03.700
so you can toggle them on
and off for cases like this.

00:26:03.700 --> 00:26:03.940
PAIGE BAILEY: Right.

00:26:03.940 --> 00:26:05.940
DOUG STEVENSON: So we'll
publish our new changes

00:26:05.940 --> 00:26:06.790
to remote config.

00:26:06.790 --> 00:26:08.807
We can see that the app
is no longer crashing.

00:26:08.807 --> 00:26:10.390
PAIGE BAILEY: I see
no more sad faces.

00:26:10.390 --> 00:26:11.848
DOUG STEVENSON: No
more sad faces--

00:26:11.848 --> 00:26:14.110
and the number of crash
free users has gone up.

00:26:14.110 --> 00:26:17.660
So we've reduced the crash
rate down to below 1%,

00:26:17.660 --> 00:26:20.080
which is really good-- so
a successful experiment.

00:26:20.080 --> 00:26:22.040
PAIGE BAILEY: That's amazing.

00:26:22.040 --> 00:26:23.435
So launch into the next demo.

00:26:23.435 --> 00:26:24.310
What do we have next?

00:26:24.310 --> 00:26:24.600
DOUG STEVENSON: Yeah.

00:26:24.600 --> 00:26:26.850
So another thing that
developers need to be able to do

00:26:26.850 --> 00:26:29.107
is compare what
their users prefer.

00:26:29.107 --> 00:26:30.190
PAIGE BAILEY: A/B testing.

00:26:30.190 --> 00:26:32.690
DOUG STEVENSON: Perform an A/B
test, yes-- so what we can do

00:26:32.690 --> 00:26:35.278
is use Firebase A/B testing to
create and run an experiment.

00:26:35.278 --> 00:26:36.820
So what we can do
in the console here

00:26:36.820 --> 00:26:39.340
is create an
experiment with a name,

00:26:39.340 --> 00:26:41.560
targeting half of our
users with some metrics

00:26:41.560 --> 00:26:42.640
that we want to track.

00:26:42.640 --> 00:26:44.860
And we're going to use
Firebase remote config again

00:26:44.860 --> 00:26:46.130
to configure this experiment.

00:26:46.130 --> 00:26:49.810
So for the control group,
dark mode will not be enabled.

00:26:49.810 --> 00:26:52.450
And for the variant group, we
will enable this experiment.

00:26:52.450 --> 00:26:54.010
So remote config
is going to tell

00:26:54.010 --> 00:26:55.750
half the devices to
run this experiment

00:26:55.750 --> 00:26:56.920
and have the devices not.

00:26:56.920 --> 00:27:00.140
And we'll compare the metrics
on that and see how it performs.

00:27:00.140 --> 00:27:01.870
PAIGE BAILEY: And all of this
is done through the console?

00:27:01.870 --> 00:27:03.860
DOUG STEVENSON: Yes, all of this
is done through the console.

00:27:03.860 --> 00:27:05.943
So we can see here our
experiment is ready to run.

00:27:05.943 --> 00:27:08.890
We're especially interested
in 15 day retentions.

00:27:08.890 --> 00:27:11.020
So we're going to use that
as the way to determine

00:27:11.020 --> 00:27:13.700
whether or not this
experiment is successful.

00:27:13.700 --> 00:27:15.693
So what we can do is
start this experiment.

00:27:15.693 --> 00:27:17.110
Bear in mind that
remote config is

00:27:17.110 --> 00:27:19.900
being used to pull
out that Boolean flag

00:27:19.900 --> 00:27:21.910
and enable dark mode
only for those half

00:27:21.910 --> 00:27:25.300
of all devices that are
configured to use it.

00:27:25.300 --> 00:27:27.593
And what we're going to
do is run this A/B test.

00:27:27.593 --> 00:27:29.260
And that's going to
push to our devices.

00:27:29.260 --> 00:27:32.287
And you can see some of our
devices are running dark mode,

00:27:32.287 --> 00:27:34.120
and some are running
light mode, and they're

00:27:34.120 --> 00:27:35.260
switching between the two.

00:27:35.260 --> 00:27:36.550
PAIGE BAILEY: I think I
like the light mode best.

00:27:36.550 --> 00:27:37.020
DOUG STEVENSON:
You like the light?

00:27:37.020 --> 00:27:37.520
OK.

00:27:37.520 --> 00:27:38.730
I'm partial to dark mode.

00:27:38.730 --> 00:27:39.300
But--

00:27:39.300 --> 00:27:40.390
PAIGE BAILEY: Well, let's
see what the users say.

00:27:40.390 --> 00:27:41.265
DOUG STEVENSON: Yeah.

00:27:41.265 --> 00:27:43.613
The users get to decide
based on our goal metrics.

00:27:43.613 --> 00:27:44.530
PAIGE BAILEY: Oh, man.

00:27:44.530 --> 00:27:44.830
DOUG STEVENSON: Yeah.

00:27:44.830 --> 00:27:46.820
It looks like dark mode
is the leader in this.

00:27:46.820 --> 00:27:49.360
And we can see the exact
mark on the retention

00:27:49.360 --> 00:27:50.500
that this experiment had.

00:27:50.500 --> 00:27:51.750
PAIGE BAILEY: This is amazing.

00:27:51.750 --> 00:27:53.530
I was a data scientist
in a previous life.

00:27:53.530 --> 00:27:55.420
And historically,
we would have to use

00:27:55.420 --> 00:27:58.720
weeks of training and custom
code and configurations

00:27:58.720 --> 00:28:01.150
and track all of the metrics
and visualizations ourselves.

00:28:01.150 --> 00:28:02.733
But this, looks
like you could do it

00:28:02.733 --> 00:28:04.150
just by clicking
a couple buttons.

00:28:04.150 --> 00:28:06.233
DOUG STEVENSON: Yes, an
entire engine is basically

00:28:06.233 --> 00:28:08.020
hidden by Firebase A/B testing.

00:28:08.020 --> 00:28:10.180
So we can use remote
config, and then push it out

00:28:10.180 --> 00:28:12.080
to all of our users.

00:28:12.080 --> 00:28:12.940
All right.

00:28:12.940 --> 00:28:14.740
The last thing that a
lot of app developers

00:28:14.740 --> 00:28:15.615
are concerned about--

00:28:15.615 --> 00:28:18.120
PAIGE BAILEY: And probably
the most important one.

00:28:18.120 --> 00:28:19.620
DOUG STEVENSON:
--yeah, and everyone

00:28:19.620 --> 00:28:22.782
wants their pages to load fast,
and their screens to be snappy.

00:28:22.782 --> 00:28:25.240
So what we can do is measure
that with Firebase Performance

00:28:25.240 --> 00:28:26.202
Monitoring.

00:28:26.202 --> 00:28:27.910
Now with Firebase
Performance Monitoring,

00:28:27.910 --> 00:28:30.178
you put basically
one SDK in your app

00:28:30.178 --> 00:28:31.720
and it automatically
collects things.

00:28:31.720 --> 00:28:33.280
And it goes out to
your users and it

00:28:33.280 --> 00:28:37.235
measures the behavior of your
app on your user's devices.

00:28:37.235 --> 00:28:38.860
But if you do need
to measure something

00:28:38.860 --> 00:28:42.250
in particular, like, say,
the load screen time, what

00:28:42.250 --> 00:28:43.605
you can do is create a trace.

00:28:43.605 --> 00:28:45.480
PAIGE BAILEY: And so
tell me what a trace is?

00:28:45.480 --> 00:28:46.030
DOUG STEVENSON: Ah, yeah.

00:28:46.030 --> 00:28:48.880
So a trace is something in
Firebase Performance Monitoring

00:28:48.880 --> 00:28:50.600
that measures the
duration of something.

00:28:50.600 --> 00:28:52.058
So here we're
creating a new trace.

00:28:52.058 --> 00:28:53.150
We're starting a timer.

00:28:53.150 --> 00:28:56.090
Then after we load all of our
image and text for this screen,

00:28:56.090 --> 00:28:57.030
we'll stop the timer.

00:28:57.030 --> 00:28:58.700
That becomes our metric.

00:28:58.700 --> 00:29:01.415
So that's going to get sent
to the Firebase Performance

00:29:01.415 --> 00:29:02.290
Monitoring dashboard.

00:29:02.290 --> 00:29:05.860
So we're going to run this code
on all of our users' devices,

00:29:05.860 --> 00:29:06.970
collect this metric.

00:29:06.970 --> 00:29:09.700
And now what you can see
here is the trend over time

00:29:09.700 --> 00:29:13.302
is that 555 milliseconds
of load time.

00:29:13.302 --> 00:29:14.760
PAIGE BAILEY: That
is way too long.

00:29:14.760 --> 00:29:15.650
DOUG STEVENSON:
It's half a second,

00:29:15.650 --> 00:29:17.200
but it still feels
like an eternity when

00:29:17.200 --> 00:29:18.170
you click the button.

00:29:18.170 --> 00:29:20.058
So what we need to do
is bring this down.

00:29:20.058 --> 00:29:21.850
Now it turns out this
app's been configured

00:29:21.850 --> 00:29:24.930
with Firebase remote config
to have a configurable image

00:29:24.930 --> 00:29:25.430
quality.

00:29:25.430 --> 00:29:28.610
So the idea is to speed up
the performance for our app,

00:29:28.610 --> 00:29:29.930
we'll ratchet down the quality.

00:29:29.930 --> 00:29:32.870
Maybe that will make the images
smaller and faster to load.

00:29:32.870 --> 00:29:34.870
So what we can do is
create a condition for this

00:29:34.870 --> 00:29:38.230
and say that half of our
users get a reduced quality.

00:29:38.230 --> 00:29:40.652
So when we finalize
this, half of our users

00:29:40.652 --> 00:29:42.610
are going to get the
default and the other half

00:29:42.610 --> 00:29:44.840
are going to get this
reduced image quality.

00:29:44.840 --> 00:29:47.440
And then we can compare
this in the dashboard.

00:29:47.440 --> 00:29:49.660
Now again, we do need
a few lines of code

00:29:49.660 --> 00:29:52.030
to enable this in remote config.

00:29:52.030 --> 00:29:54.700
When that's done, we can collect
this information on our users

00:29:54.700 --> 00:29:58.010
devices exactly as they're using
it and come up with results.

00:29:58.010 --> 00:30:01.507
So we were able to reduce
the time to 111 milliseconds.

00:30:01.507 --> 00:30:03.340
And our engineer thinks
this is a good idea.

00:30:03.340 --> 00:30:04.340
PAIGE BAILEY: Much faster--

00:30:04.340 --> 00:30:05.200
and I agree with them.

00:30:05.200 --> 00:30:07.492
And it also looks like you
can drill down into the data

00:30:07.492 --> 00:30:08.320
by device?

00:30:08.320 --> 00:30:08.650
DOUG STEVENSON: Yeah.

00:30:08.650 --> 00:30:08.920
Yeah.

00:30:08.920 --> 00:30:10.990
So right now, we're slicing
this by app version.

00:30:10.990 --> 00:30:13.660
But you can also use
other characteristics

00:30:13.660 --> 00:30:16.000
of the device and
the user to determine

00:30:16.000 --> 00:30:18.282
what the performance is for
very specific conditions.

00:30:18.282 --> 00:30:19.490
PAIGE BAILEY: That's amazing.

00:30:19.490 --> 00:30:22.510
And I love being able to see
the visualizations just out

00:30:22.510 --> 00:30:23.170
of the box.

00:30:23.170 --> 00:30:23.590
DOUG STEVENSON: Yeah.

00:30:23.590 --> 00:30:25.090
PAIGE BAILEY: So thank
you so much for sharing

00:30:25.090 --> 00:30:27.040
all of this great
functionality with Firebase.

00:30:27.040 --> 00:30:28.667
I can't wait to try
out an A/B test.

00:30:28.667 --> 00:30:30.250
DOUG STEVENSON:
Thanks for joining me.

00:30:30.250 --> 00:30:31.292
PAIGE BAILEY: Absolutely.

00:30:31.292 --> 00:30:32.120
Thank you so much.

00:30:32.120 --> 00:30:33.888
And back to you.

00:30:33.888 --> 00:30:35.680
EMILY FORTUNA: The
sandbox is a great place

00:30:35.680 --> 00:30:37.805
to check out a huge range
of new tech and hardware.

00:30:37.805 --> 00:30:39.222
Coming up later
in the show, we'll

00:30:39.222 --> 00:30:41.860
take you back to the sandbox
and highlight even more demos.

00:30:41.860 --> 00:30:43.000
You won't want to miss it.

00:30:43.000 --> 00:30:44.110
But first a quick break.

00:30:44.110 --> 00:30:45.485
And after the next
session, we'll

00:30:45.485 --> 00:30:47.185
see you back here
for more I/O Live.

00:30:47.185 --> 00:30:49.470
[MUSIC PLAYING]

00:31:10.672 --> 00:31:12.380
TIMOTHY JORDAN: Welcome
back to I/O Live.

00:31:12.380 --> 00:31:14.005
If you missed any
part of the festival,

00:31:14.005 --> 00:31:17.037
catch up at g.co/io/live.

00:31:17.037 --> 00:31:19.370
EMILY FORTUNA: Timothy, how
was the festival last night?

00:31:19.370 --> 00:31:20.662
TIMOTHY JORDAN: It was awesome.

00:31:20.662 --> 00:31:21.560
We went exploring.

00:31:21.560 --> 00:31:23.730
Check it out.

00:31:23.730 --> 00:31:24.730
SPEAKER 1: Hey, Timothy!

00:31:24.730 --> 00:31:26.140
We should go to After Dark.

00:31:26.140 --> 00:31:28.365
I want to go dancing.

00:31:28.365 --> 00:31:28.990
[MUSIC PLAYING]

00:31:28.990 --> 00:31:30.412
TIMOTHY JORDAN: What?

00:31:30.412 --> 00:31:31.745
SPEAKER 1: I want to go dancing!

00:31:31.745 --> 00:31:32.236
TIMOTHY JORDAN: All right.

00:31:32.236 --> 00:31:33.218
Let's do it!

00:31:33.218 --> 00:31:36.655
[LAUGHTER]

00:31:48.553 --> 00:31:50.058
So I'm an enormous space nerd.

00:31:50.058 --> 00:31:51.475
Do you have anything
space themed?

00:31:51.475 --> 00:31:53.427
SPEAKER 2: We have
an awesome space ship

00:31:53.427 --> 00:31:54.510
tattoo that's really cool.

00:31:54.510 --> 00:31:55.140
TIMOTHY JORDAN: I'm in.

00:31:55.140 --> 00:31:55.570
SPEAKER 3: I could
do that right here.

00:31:55.570 --> 00:31:56.112
Is that cool?

00:31:56.112 --> 00:31:57.864
TIMOTHY JORDAN: Sure.

00:31:57.864 --> 00:32:00.203
SPEAKER 1: Oww!

00:32:00.203 --> 00:32:02.328
SPEAKER 3: We're going to
put our unicorn on there.

00:32:02.328 --> 00:32:05.390
SPEAKER 1: We got a unicorn!

00:32:05.390 --> 00:32:06.180
Oh, OK.

00:32:06.180 --> 00:32:07.690
So it says eats over there
and there's hopefully--

00:32:07.690 --> 00:32:08.280
TIMOTHY JORDAN: Oh, yeah.

00:32:08.280 --> 00:32:09.200
You hungry?

00:32:09.200 --> 00:32:11.140
SPEAKER 1: --the
cones of [INAUDIBLE]..

00:32:19.390 --> 00:32:20.360
There's advice?

00:32:20.360 --> 00:32:21.270
TIMOTHY JORDAN: Yeah.

00:32:21.270 --> 00:32:21.937
SPEAKER 1: Yeah.

00:32:21.937 --> 00:32:22.635
Let's do it.

00:32:22.635 --> 00:32:25.060
AUDIENCE: What you do is
you can ask her a question.

00:32:25.060 --> 00:32:29.920
SPEAKER 1: What will I
have for lunch tomorrow?

00:32:29.920 --> 00:32:31.670
So, like, I think
I want a salad.

00:32:31.670 --> 00:32:33.874
But I really want pizza.

00:32:33.874 --> 00:32:34.798
SPEAKER 5: Hello!

00:32:34.798 --> 00:32:38.500
TIMOTHY JORDAN: Hi, [INAUDIBLE].

00:32:38.500 --> 00:32:39.770
I'm a little dusty up here.

00:32:44.265 --> 00:32:45.182
SPEAKER 5: [SQUEALING]

00:32:45.182 --> 00:32:46.599
TIMOTHY JORDAN:
It won't come off!

00:32:46.599 --> 00:32:48.626
It won't come off!

00:32:48.626 --> 00:32:55.050
[MUSIC PLAYING]

00:32:55.050 --> 00:32:56.490
SPEAKER 1: I don't know.

00:32:56.490 --> 00:33:00.095
This might be my new job.

00:33:00.095 --> 00:33:10.075
[MUSIC PLAYING]

00:33:27.870 --> 00:33:29.037
TIMOTHY JORDAN: [INAUDIBLE].

00:33:29.037 --> 00:33:31.930
SPEAKER 1: Timothy, thank you
so much for hanging out with me

00:33:31.930 --> 00:33:33.337
during my first I/O after dark.

00:33:33.337 --> 00:33:34.670
TIMOTHY JORDAN: Are you kidding?

00:33:34.670 --> 00:33:35.750
This was tons of fun.

00:33:35.750 --> 00:33:37.875
I mean, this is what makes
it a developer festival.

00:33:42.135 --> 00:33:44.010
EMILY FORTUNA: That
looked like a lot of fun.

00:33:44.010 --> 00:33:45.718
If you're here on
site, you'll definitely

00:33:45.718 --> 00:33:47.810
want to check out after
dark before you head home.

00:33:47.810 --> 00:33:49.800
Now let's check in with
Todd for an AR sandbox

00:33:49.800 --> 00:33:53.154
demo that is out of this world.

00:33:53.154 --> 00:34:03.530
[MUSIC PLAYING]

00:34:03.530 --> 00:34:05.030
TODD KERPELMAN:
Hello, I/O viewers--

00:34:05.030 --> 00:34:06.820
well, what better
way to follow up

00:34:06.820 --> 00:34:08.840
an interview with a
real life astronaut

00:34:08.840 --> 00:34:11.020
than to check out a
virtual spaceship?

00:34:11.020 --> 00:34:13.870
So I am here with Amanda in
the augmented reality booth.

00:34:13.870 --> 00:34:16.520
And you're going to show me
something cool from the NASA

00:34:16.520 --> 00:34:17.020
website.

00:34:17.020 --> 00:34:18.010
AMANDA LE: Yeah.

00:34:18.010 --> 00:34:20.380
So here you can see on the
NASA website they're showing

00:34:20.380 --> 00:34:22.540
3D models of their Rovers.

00:34:22.540 --> 00:34:24.610
Here we have the
Curiosity Rover.

00:34:24.610 --> 00:34:27.100
And they're actually using
the model VR web component

00:34:27.100 --> 00:34:29.326
to put that 3D asset
on their website.

00:34:29.326 --> 00:34:30.159
TODD KERPELMAN: Wow.

00:34:30.159 --> 00:34:30.867
That's very cool.

00:34:30.867 --> 00:34:32.590
Now you said it's
a web component.

00:34:32.590 --> 00:34:35.080
So do they need to like
NPM install something

00:34:35.080 --> 00:34:35.830
to get it working?

00:34:35.830 --> 00:34:37.960
AMANDA LE: It's actually
a mark up, so very simple

00:34:37.960 --> 00:34:39.298
to just put on your website.

00:34:39.298 --> 00:34:40.340
TODD KERPELMAN: I gotcha.

00:34:40.340 --> 00:34:41.409
Well, that's very cool.

00:34:41.409 --> 00:34:45.130
And this 3D model, what
file format is that?

00:34:45.130 --> 00:34:47.710
AMANDA LE: So we
recommend using GLTF.

00:34:47.710 --> 00:34:50.018
But it also supports
USDZ and GLB.

00:34:50.018 --> 00:34:50.810
TODD KERPELMAN: OK.

00:34:50.810 --> 00:34:52.960
And is that something most
commercial 3D modeling

00:34:52.960 --> 00:34:54.040
tools will export to?

00:34:54.040 --> 00:34:55.730
AMANDA LE: You should be
able to export to that.

00:34:55.730 --> 00:34:57.260
TODD KERPELMAN: I see you've got
that little button down there

00:34:57.260 --> 00:34:58.240
in the bottom right.

00:34:58.240 --> 00:34:59.200
What does that do?

00:34:59.200 --> 00:35:01.117
AMANDA LE: So this
actually is the new feature

00:35:01.117 --> 00:35:03.100
that we're launching
at I/O today.

00:35:03.100 --> 00:35:06.310
And what it allows you
to do is open up what

00:35:06.310 --> 00:35:07.660
we call Scene Viewer.

00:35:07.660 --> 00:35:10.720
So Scene Viewer allows
you to take this 3D model

00:35:10.720 --> 00:35:12.880
and actually place it in
your real world space.

00:35:12.880 --> 00:35:13.810
TODD KERPELMAN: Whoa.

00:35:13.810 --> 00:35:15.510
No way!

00:35:15.510 --> 00:35:18.010
AMANDA LE: So here you can see
the Curiosity Rover at scale.

00:35:18.010 --> 00:35:18.843
TODD KERPELMAN: Wow.

00:35:18.843 --> 00:35:22.270
It's like there's a real live
spaceship here in the booth,

00:35:22.270 --> 00:35:23.510
but not really.

00:35:23.510 --> 00:35:26.000
It's just actually
very nicely rendered.

00:35:26.000 --> 00:35:27.625
And it kind of fooled me.

00:35:27.625 --> 00:35:28.250
AMANDA LE: Yep.

00:35:28.250 --> 00:35:29.500
And so you can
actually manipulate

00:35:29.500 --> 00:35:30.470
this in your own space.

00:35:30.470 --> 00:35:31.780
So you'll see it at full scale.

00:35:31.780 --> 00:35:34.072
But you can also shrink it
down so that it can actually

00:35:34.072 --> 00:35:35.140
fit in your living room.

00:35:35.140 --> 00:35:35.410
TODD KERPELMAN: OK.

00:35:35.410 --> 00:35:35.980
Yes.

00:35:35.980 --> 00:35:39.310
So if I want to put the
Curiosity Rover on my coffee

00:35:39.310 --> 00:35:41.840
table, I now can, just by
shrinking it down a little bit.

00:35:41.840 --> 00:35:42.520
AMANDA LE: Yes.

00:35:42.520 --> 00:35:46.510
TODD KERPELMAN: And so how did
you get the size specification?

00:35:46.510 --> 00:35:48.130
Apparently this was
built real life,

00:35:48.130 --> 00:35:49.600
or this is showing
it in real life,

00:35:49.600 --> 00:35:50.650
and you're able
to shrink it down?

00:35:50.650 --> 00:35:50.830
AMANDA LE: Yeah.

00:35:50.830 --> 00:35:52.630
So NASA, when they
built the 3D model,

00:35:52.630 --> 00:35:54.650
decided to build it at scale.

00:35:54.650 --> 00:35:56.396
So that's the model
we're referencing.

00:35:56.396 --> 00:35:57.354
TODD KERPELMAN: Gotcha.

00:35:57.354 --> 00:35:58.120
Very cool.

00:35:58.120 --> 00:36:00.850
And so if I'm a
developer and I want

00:36:00.850 --> 00:36:02.500
to turn my model
via your component

00:36:02.500 --> 00:36:06.195
into this cool Scene Viewer
augmented reality experience,

00:36:06.195 --> 00:36:07.070
what do I have to do?

00:36:07.070 --> 00:36:07.480
AMANDA LE: Yeah.

00:36:07.480 --> 00:36:09.190
So there's no need to
build an app for this.

00:36:09.190 --> 00:36:09.982
TODD KERPELMAN: OK.

00:36:09.982 --> 00:36:13.000
AMANDA LE: All you actually
do is going into the mark up.

00:36:13.000 --> 00:36:14.710
And you add the AR
attribute, which

00:36:14.710 --> 00:36:18.000
is literally the letters A and
R. So it's super simple to do.

00:36:18.000 --> 00:36:19.480
TODD KERPELMAN: I type two
letters and I get all that?

00:36:19.480 --> 00:36:19.930
AMANDA LE: Yes.

00:36:19.930 --> 00:36:20.170
TODD KERPELMAN: All right.

00:36:20.170 --> 00:36:21.250
Well, that sounds easy.

00:36:21.250 --> 00:36:22.660
I think even I could do that.

00:36:22.660 --> 00:36:23.440
AMANDA LE: Yes.

00:36:23.440 --> 00:36:24.130
TODD KERPELMAN: Well,
I can use a little

00:36:24.130 --> 00:36:25.510
pick me up after that demo.

00:36:25.510 --> 00:36:30.132
Let's head on over here and see
if I can find some caffeine!

00:36:30.132 --> 00:36:31.018
Woo!

00:36:31.018 --> 00:36:31.910
Hey, Ashish.

00:36:31.910 --> 00:36:32.770
ASHISH SHAH: How
are you doing, Todd?

00:36:32.770 --> 00:36:33.790
TODD KERPELMAN: I am doing well.

00:36:33.790 --> 00:36:35.320
I'm already excited
for this demo,

00:36:35.320 --> 00:36:37.600
because I see there's an
espresso machine here.

00:36:37.600 --> 00:36:39.612
So walk me [INAUDIBLE].

00:36:39.612 --> 00:36:40.570
ASHISH SHAH: All right.

00:36:40.570 --> 00:36:42.610
So what we're
doing here is we're

00:36:42.610 --> 00:36:45.730
showing an espresso machine
coming to life in AR

00:36:45.730 --> 00:36:49.160
using this technology called
augmented images that's part of

00:36:49.160 --> 00:36:49.795
[INAUDIBLE].

00:36:49.795 --> 00:36:50.920
You want to give it a spin?

00:36:50.920 --> 00:36:52.510
TODD KERPELMAN:
Let's check it out.

00:36:52.510 --> 00:36:52.690
ASHISH SHAH: All right.

00:36:52.690 --> 00:36:54.330
So as soon as you
look at this marker--

00:36:54.330 --> 00:36:56.200
TODD KERPELMAN: So you're
showing the image there.

00:36:56.200 --> 00:36:56.930
ASHISH SHAH: Yep.

00:36:56.930 --> 00:36:59.097
TODD KERPELMAN: And it's
telling you to look around.

00:36:59.097 --> 00:37:02.290
ASHISH SHAH: And now it's
annotated the machine

00:37:02.290 --> 00:37:04.023
with information
about its features.

00:37:04.023 --> 00:37:04.690
See the little--

00:37:04.690 --> 00:37:06.315
TODD KERPELMAN: Is
this coming to life?

00:37:06.315 --> 00:37:07.210
ASHISH SHAH: --yep.

00:37:07.210 --> 00:37:10.568
It's telling you about
dose control, grinding.

00:37:10.568 --> 00:37:12.360
TODD KERPELMAN: I need
my doses controlled.

00:37:12.360 --> 00:37:14.110
ASHISH SHAH: And the
precise temperature--

00:37:14.110 --> 00:37:15.820
what would you do without that?

00:37:15.820 --> 00:37:19.060
And you can actually see an
espresso being made here.

00:37:19.060 --> 00:37:20.380
TODD KERPELMAN: Oh, fantastic.

00:37:20.380 --> 00:37:23.080
ASHISH SHAH: This is a really
powerful way for manufacturers

00:37:23.080 --> 00:37:26.140
to show features
of their product,

00:37:26.140 --> 00:37:29.540
which are otherwise
just lost in text.

00:37:29.540 --> 00:37:31.500
This really makes the
product come to life--

00:37:31.500 --> 00:37:32.160
TODD KERPELMAN: Yeah.

00:37:32.160 --> 00:37:33.827
ASHISH SHAH: --in a
really powerful way.

00:37:33.827 --> 00:37:36.930
And so this triggers the
experience, once it recognizes.

00:37:36.930 --> 00:37:39.910
And now what happens once
this image is recognized,

00:37:39.910 --> 00:37:42.010
it starts establishing
a correspondence

00:37:42.010 --> 00:37:43.762
between the phone and the image.

00:37:43.762 --> 00:37:45.220
TODD KERPELMAN: So
the phone always

00:37:45.220 --> 00:37:46.833
knows exactly where it is--

00:37:46.833 --> 00:37:48.500
ASHISH SHAH: With
respect to the image--

00:37:48.500 --> 00:37:50.792
and when I say it establishes
a correspondence in terms

00:37:50.792 --> 00:37:53.550
[INAUDIBLE],, that's
position and orientation.

00:37:53.550 --> 00:37:58.150
So position in like x
direction and y direction

00:37:58.150 --> 00:38:01.870
and in z direction, and also the
orientation in three degrees,

00:38:01.870 --> 00:38:03.520
like pitch, yaw, and roll.

00:38:03.520 --> 00:38:05.030
So the phone knows
exactly where it

00:38:05.030 --> 00:38:06.760
is with respect to the marker.

00:38:06.760 --> 00:38:07.065
TODD KERPELMAN: Gotcha.

00:38:07.065 --> 00:38:08.680
So this kind of
works best in places

00:38:08.680 --> 00:38:11.220
where you sort of have strict
control over the environment

00:38:11.220 --> 00:38:13.720
in which this image is going
to appear and what's around it?

00:38:13.720 --> 00:38:14.428
ASHISH SHAH: Yes.

00:38:14.428 --> 00:38:16.630
If you're trying to
augment real things,

00:38:16.630 --> 00:38:18.637
you want to make sure
that the image is

00:38:18.637 --> 00:38:21.220
rigid with respect to the thing
that you're trying to augment.

00:38:21.220 --> 00:38:21.790
TODD KERPELMAN: Gotcha.

00:38:21.790 --> 00:38:22.390
ASHISH SHAH: But
if you just want

00:38:22.390 --> 00:38:24.260
the image itself
specifically to come to life,

00:38:24.260 --> 00:38:24.910
that's all you need [INAUDIBLE].

00:38:24.910 --> 00:38:25.910
ASHISH SHAH: Absolutely.

00:38:25.910 --> 00:38:28.900
So think about a scenario
where you're at a store,

00:38:28.900 --> 00:38:31.390
and you want to see
what's inside a box.

00:38:31.390 --> 00:38:33.560
So you could use an
app to look at the box.

00:38:33.560 --> 00:38:35.783
It would recognize
the box as a marker,

00:38:35.783 --> 00:38:37.450
and you could actually
have a sneak peek

00:38:37.450 --> 00:38:38.728
at what's inside the box.

00:38:38.728 --> 00:38:39.520
TODD KERPELMAN: Oh.

00:38:39.520 --> 00:38:40.020
OK.

00:38:40.020 --> 00:38:41.920
So the model could
pop out of the box.

00:38:41.920 --> 00:38:42.430
ASHISH SHAH: Exactly.

00:38:42.430 --> 00:38:43.930
TODD KERPELMAN: And I can
see exactly what my waffle

00:38:43.930 --> 00:38:45.280
iron or toaster--

00:38:45.280 --> 00:38:46.450
I need stuff in my kitchen--

00:38:46.450 --> 00:38:47.050
that's all going to look like.

00:38:47.050 --> 00:38:48.350
ASHISH SHAH: Without
even opening it,

00:38:48.350 --> 00:38:49.200
without even opening it up.

00:38:49.200 --> 00:38:50.830
TODD KERPELMAN: And then I won't
get yelled at by shopkeepers.

00:38:50.830 --> 00:38:51.910
ASHISH SHAH: Yeah.

00:38:51.910 --> 00:38:53.210
TODD KERPELMAN: Well, thank
you very much, Ashish.

00:38:53.210 --> 00:38:55.510
I learned a lot about
augmented reality today.

00:38:55.510 --> 00:38:57.216
I'm going to grab this espresso.

00:38:57.216 --> 00:38:59.228
Oh, it's not real.

00:38:59.228 --> 00:39:00.270
But the coffee beans are.

00:39:00.270 --> 00:39:01.320
Can I eat the coffee beans?

00:39:01.320 --> 00:39:01.680
ASHISH SHAH: Yes.

00:39:01.680 --> 00:39:02.070
You can.

00:39:02.070 --> 00:39:03.120
TODD KERPELMAN: I'm going to
take a few coffee beans to go.

00:39:03.120 --> 00:39:04.050
ASHISH SHAH: I think you can
take the coffee beans, Todd.

00:39:04.050 --> 00:39:06.050
TODD KERPELMAN: Thank you
very much, livestream.

00:39:06.050 --> 00:39:08.482
I'm going to go get caffeinated.

00:39:08.482 --> 00:39:09.690
TIMOTHY JORDAN: Thanks, Todd.

00:39:09.690 --> 00:39:12.170
Got a comment about our AR
sandbox or anything else

00:39:12.170 --> 00:39:13.170
you've seen on the show?

00:39:13.170 --> 00:39:16.580
Make sure to share
it using #I/O'19.

00:39:16.580 --> 00:39:19.500
Also here's a quick shout
out to the more than 500

00:39:19.500 --> 00:39:22.240
extended viewing parties
from 60 countries

00:39:22.240 --> 00:39:23.460
joining us on the Livestream.

00:39:23.460 --> 00:39:26.480
AUDIENCE: Hi from I/O
[INAUDIBLE] Boulder.

00:39:26.480 --> 00:39:29.340
ASHISH SHAH: This is Google
Seattle's I/O extended.

00:39:29.340 --> 00:39:31.420
Hey, everybody!

00:39:31.420 --> 00:39:33.630
TIMOTHY JORDAN: Great to
have you along for the ride.

00:39:33.630 --> 00:39:35.700
Time for us to take a
break as the next sessions

00:39:35.700 --> 00:39:36.960
will begin shortly.

00:39:36.960 --> 00:39:40.080
We'll be back in about an
hour with more sandbox demos.

00:39:40.080 --> 00:39:41.450
You're watching I/O live.

00:39:49.338 --> 00:39:51.803
[MUSIC PLAYING]

00:40:05.723 --> 00:40:07.390
EMILY FORTUNA: Welcome
back to I/O Live.

00:40:07.390 --> 00:40:08.195
I'm Emily Fortuna.

00:40:08.195 --> 00:40:09.820
TIMOTHY JORDAN: And
I'm Timothy Jordan.

00:40:09.820 --> 00:40:12.190
We're joined by special
guests Eve Ekman,

00:40:12.190 --> 00:40:15.130
Director of Training at the
Greater Good Science Center

00:40:15.130 --> 00:40:17.530
at UC Berkeley, and
Collaborator William

00:40:17.530 --> 00:40:19.860
Russell, creative
director of Monom Berlin's

00:40:19.860 --> 00:40:21.760
Center for Spatial Sound.

00:40:21.760 --> 00:40:23.230
Welcome, both of you.

00:40:23.230 --> 00:40:24.397
It's great to have you here.

00:40:24.397 --> 00:40:25.813
WILLIAM RUSSELL:
Great to be here.

00:40:25.813 --> 00:40:27.220
EVE EKMAN: Yeah, thanks.

00:40:27.220 --> 00:40:28.810
EMILY FORTUNA: So
tell me-- you've

00:40:28.810 --> 00:40:31.930
created this really excellent
art installation here

00:40:31.930 --> 00:40:34.773
at Google I/O. Why make it here?

00:40:34.773 --> 00:40:35.440
EVE EKMAN: Yeah.

00:40:35.440 --> 00:40:36.790
So I was really fortunate.

00:40:36.790 --> 00:40:40.810
William's artistic vision
wanted to build upon a project

00:40:40.810 --> 00:40:42.550
that my dad and I
had collaborated

00:40:42.550 --> 00:40:44.260
on about three years ago.

00:40:44.260 --> 00:40:46.390
And this was an
atlas of emotion.

00:40:46.390 --> 00:40:48.910
The inspiration for this came
from His Holiness, the Dalai

00:40:48.910 --> 00:40:52.180
Lama, who is so
passionate about all of us

00:40:52.180 --> 00:40:54.160
finding our way to a calm mind.

00:40:54.160 --> 00:40:57.230
He thought maybe we
need a map to get there.

00:40:57.230 --> 00:41:00.280
So the idea was to take the best
of what we know from emotion

00:41:00.280 --> 00:41:02.650
research and create
a way for people

00:41:02.650 --> 00:41:04.870
to actually visualize emotion.

00:41:04.870 --> 00:41:06.670
And then, here at I/O,
we are giving people

00:41:06.670 --> 00:41:10.360
an opportunity to know what
it's like to experience emotion.

00:41:10.360 --> 00:41:12.730
So it seems so simple to
be aware of our emotions.

00:41:12.730 --> 00:41:14.920
And yet, for most
of us, we avoid,

00:41:14.920 --> 00:41:18.160
suppress, deny anything that
doesn't feel good, which

00:41:18.160 --> 00:41:20.440
actually doesn't help
us understand ourselves

00:41:20.440 --> 00:41:21.590
or understand others.

00:41:21.590 --> 00:41:24.220
So this is just one more
portal and way for people

00:41:24.220 --> 00:41:27.700
to get into these ideas of
emotion awareness, well-being,

00:41:27.700 --> 00:41:29.887
empathy, connection,
and compassion.

00:41:29.887 --> 00:41:31.720
TIMOTHY JORDAN: That
sounds really powerful.

00:41:31.720 --> 00:41:33.430
William, could you
describe the work?

00:41:33.430 --> 00:41:35.260
What is it going to
be like for those

00:41:35.260 --> 00:41:36.460
of us that can't be there?

00:41:36.460 --> 00:41:37.377
WILLIAM RUSSELL: Yeah.

00:41:37.377 --> 00:41:42.070
Well, the piece is being created
using a sound technology built

00:41:42.070 --> 00:41:44.140
by 4D sound.

00:41:44.140 --> 00:41:47.140
So the experience is there's
about 85 speakers spread

00:41:47.140 --> 00:41:49.480
out evenly throughout the room.

00:41:49.480 --> 00:41:51.850
And the idea is that it
creates sounds more naturally,

00:41:51.850 --> 00:41:54.580
more the way that we perceive
them in the real world, how

00:41:54.580 --> 00:41:56.470
we've evolved to
perceive them, which

00:41:56.470 --> 00:42:01.420
allows us to connect more deeply
to people's awareness of sound

00:42:01.420 --> 00:42:03.340
and how this interacts
with their body.

00:42:03.340 --> 00:42:04.930
But the idea in this
piece is actually

00:42:04.930 --> 00:42:07.500
we have artist
Amanda Gregory, who

00:42:07.500 --> 00:42:11.290
will be singing and
sort of theatrically

00:42:11.290 --> 00:42:14.230
performing and
controlling elements

00:42:14.230 --> 00:42:17.230
of the sound in the space,
as well as being triggered

00:42:17.230 --> 00:42:20.140
by the soundscape
that is around her.

00:42:20.140 --> 00:42:22.780
So the idea is she's
actually, in real time,

00:42:22.780 --> 00:42:25.290
trying to embody
as much as possible

00:42:25.290 --> 00:42:29.695
the experience around
her and respond to this

00:42:29.695 --> 00:42:34.000
and just be fully open and
aware to her emotional landscape

00:42:34.000 --> 00:42:36.310
at the time through
this natural sounds,

00:42:36.310 --> 00:42:39.460
but also tones and
frequencies that we've learned

00:42:39.460 --> 00:42:43.160
have certain effects on
the emotional awareness,

00:42:43.160 --> 00:42:43.820
essentially.

00:42:43.820 --> 00:42:45.610
There is an area
of universal things

00:42:45.610 --> 00:42:48.040
that we're finding that
do stimulate generally

00:42:48.040 --> 00:42:50.080
everyone in a similar way.

00:42:50.080 --> 00:42:53.380
But then there's a mysterious,
other, more complex world

00:42:53.380 --> 00:42:56.810
that we're looking forward
to explore further, as well.

00:42:56.810 --> 00:42:57.430
Yeah.

00:42:57.430 --> 00:43:01.030
TIMOTHY JORDAN: This
idea of paying attention

00:43:01.030 --> 00:43:03.340
to our emotions and
understanding those triggering

00:43:03.340 --> 00:43:06.850
states I think is really
relevant to us and our audience

00:43:06.850 --> 00:43:08.150
in software development.

00:43:08.150 --> 00:43:09.340
We were talking a little
earlier about this,

00:43:09.340 --> 00:43:10.840
and I think it's
really interesting,

00:43:10.840 --> 00:43:13.510
that software development
of any sufficient complexity

00:43:13.510 --> 00:43:14.860
requires lots of people.

00:43:14.860 --> 00:43:17.410
And therefore, the most
interesting software problems

00:43:17.410 --> 00:43:19.240
are actually people problems.

00:43:19.240 --> 00:43:23.200
And a lot of that dynamic
is governed by our emotions,

00:43:23.200 --> 00:43:25.450
how we react with each other,
how we react with, like,

00:43:25.450 --> 00:43:28.640
this big thing that we're
all trying to do together.

00:43:28.640 --> 00:43:32.680
And I think in that way,
it's really important for us

00:43:32.680 --> 00:43:34.248
to study emotional intelligence.

00:43:34.248 --> 00:43:36.040
And that's, I think,
why I'm really excited

00:43:36.040 --> 00:43:37.503
about this work being here.

00:43:37.503 --> 00:43:38.170
EVE EKMAN: Yeah.

00:43:38.170 --> 00:43:39.610
Yeah.

00:43:39.610 --> 00:43:41.810
I completely agree.

00:43:41.810 --> 00:43:44.230
And you know, I've been a
trained emotion scientist,

00:43:44.230 --> 00:43:45.520
maybe, since I was born.

00:43:45.520 --> 00:43:47.590
Since my dad is the
founder of this field,

00:43:47.590 --> 00:43:50.390
Paul Ekman, and really
mapped the human face.

00:43:50.390 --> 00:43:53.410
So how do we recognize
the universal language

00:43:53.410 --> 00:43:56.500
of what I'm showing you right
now, so anger versus I'm

00:43:56.500 --> 00:43:58.100
feeling really good.

00:43:58.100 --> 00:44:01.750
And when we can understand how
to interpret these signals,

00:44:01.750 --> 00:44:06.070
we have a lot of capacity
for clear communication.

00:44:06.070 --> 00:44:07.870
We are born with this skill--

00:44:07.870 --> 00:44:11.080
not only to read faces, but
to ourselves kind of show

00:44:11.080 --> 00:44:12.100
what we're feeling.

00:44:12.100 --> 00:44:14.890
And then, as William was
saying, at an embodied level,

00:44:14.890 --> 00:44:17.117
we resonate to the
emotions of others.

00:44:17.117 --> 00:44:18.700
So even if you're
trying your hardest,

00:44:18.700 --> 00:44:21.550
actually this project,
for this team,

00:44:21.550 --> 00:44:22.780
we don't need emotions here.

00:44:22.780 --> 00:44:24.310
We just gotta be efficient.

00:44:24.310 --> 00:44:25.240
Good luck!

00:44:25.240 --> 00:44:27.370
We are hardwired head
to toe for emotion.

00:44:27.370 --> 00:44:29.230
We absolutely have
no control over what

00:44:29.230 --> 00:44:30.670
triggers us emotionally.

00:44:30.670 --> 00:44:33.820
We have some capacity
to recognize, OK,

00:44:33.820 --> 00:44:36.130
I'm feeling frustrated,
annoyed, or anxious,

00:44:36.130 --> 00:44:38.875
and then respond
more constructively.

00:44:38.875 --> 00:44:40.750
So the whole goal with
emotional intelligence

00:44:40.750 --> 00:44:44.950
isn't to change from lots
of emotions to no emotions.

00:44:44.950 --> 00:44:47.200
It's to understand
and identify what

00:44:47.200 --> 00:44:49.960
is the message this
emotion is telling you

00:44:49.960 --> 00:44:53.530
and then respond in a way
that's the most effective.

00:44:53.530 --> 00:44:55.570
And really, that's constructive.

00:44:55.570 --> 00:44:57.440
And really, that's empathy.

00:44:57.440 --> 00:44:57.940
Right?

00:44:57.940 --> 00:45:00.730
That is an ability to recognize
what's happening to another

00:45:00.730 --> 00:45:03.490
and then identify what's
the best response.

00:45:03.490 --> 00:45:06.608
This is a core value of humans.

00:45:06.608 --> 00:45:08.900
And without it, we wouldn't
have survived as a species.

00:45:08.900 --> 00:45:11.660
So how do we harness that
for creativity, productivity,

00:45:11.660 --> 00:45:13.563
and engagement?

00:45:13.563 --> 00:45:15.230
EMILY FORTUNA: So I
think you've already

00:45:15.230 --> 00:45:17.170
gotten a little bit at this.

00:45:17.170 --> 00:45:19.430
But if there's
one thing that you

00:45:19.430 --> 00:45:23.780
hope that attendees or
viewers would take away

00:45:23.780 --> 00:45:27.630
from your work in future, the
general trend of your future

00:45:27.630 --> 00:45:28.883
work, what would it be?

00:45:28.883 --> 00:45:29.550
EVE EKMAN: Yeah.

00:45:29.550 --> 00:45:31.320
I bet we both have a
different response.

00:45:31.320 --> 00:45:34.040
I think I would be
really excited for people

00:45:34.040 --> 00:45:37.220
to become deeply interested
in their emotions.

00:45:37.220 --> 00:45:39.530
And not at a
I'm-going-to-fix-this, thinking

00:45:39.530 --> 00:45:40.970
way, but oh, my gosh.

00:45:40.970 --> 00:45:42.410
My emotions exist in my body.

00:45:42.410 --> 00:45:44.540
When I feel angry, there's
tightness in my jaw

00:45:44.540 --> 00:45:45.840
and constriction in my chest.

00:45:45.840 --> 00:45:49.550
And when I feel anxious, wow,
there's some like shoulder.

00:45:49.550 --> 00:45:52.010
And just that
level of awareness,

00:45:52.010 --> 00:45:55.220
what we call a meta awareness
or metacognitive awareness

00:45:55.220 --> 00:45:57.920
means you have some space
away from the experience

00:45:57.920 --> 00:46:00.005
and you might not be
quite so reactive.

00:46:00.005 --> 00:46:01.130
EMILY FORTUNA: And William?

00:46:01.130 --> 00:46:04.700
WILLIAM RUSSELL: I mean, I
mean, I agree with all of that.

00:46:04.700 --> 00:46:07.970
But the idea is that we want
to create these experiences so

00:46:07.970 --> 00:46:10.670
that they can feel these
things as deeply as possible

00:46:10.670 --> 00:46:13.460
and to really be
able to learn them

00:46:13.460 --> 00:46:15.560
through the use of
their whole body.

00:46:15.560 --> 00:46:18.350
Because the idea is not that
there's good or bad emotions.

00:46:18.350 --> 00:46:20.160
They're equal to each other.

00:46:20.160 --> 00:46:21.260
They all have their role.

00:46:21.260 --> 00:46:23.260
And for me, this has been
an incredible learning

00:46:23.260 --> 00:46:25.038
process that's just beginning.

00:46:25.038 --> 00:46:26.330
I mean, this is experiment one.

00:46:26.330 --> 00:46:28.490
So I'm learning a lot
from Eve about what

00:46:28.490 --> 00:46:31.070
is the real science
that we have and what

00:46:31.070 --> 00:46:33.510
is still to be explored.

00:46:33.510 --> 00:46:36.440
And I think this is going to be
a constantly evolving process.

00:46:36.440 --> 00:46:39.560
I mean, it came from a
discussion between the Dalai

00:46:39.560 --> 00:46:41.360
Lama and Paul Ekman originally.

00:46:41.360 --> 00:46:43.070
And it will continue
to be a discussion,

00:46:43.070 --> 00:46:46.780
because this is such a
rich territory to explore.

00:46:46.780 --> 00:46:47.780
TIMOTHY JORDAN: Awesome.

00:46:47.780 --> 00:46:50.780
William, Eve, thank you
so much for being here.

00:46:50.780 --> 00:46:51.780
We really appreciate it.

00:46:51.780 --> 00:46:52.990
And thank you for your work.

00:46:52.990 --> 00:46:54.615
EVE EKMAN: Yeah, and
I just really want

00:46:54.615 --> 00:46:58.400
to think whatever mastermind
behind this conference

00:46:58.400 --> 00:46:59.805
wanted to do AI for good.

00:46:59.805 --> 00:47:00.680
TIMOTHY JORDAN: Yeah.

00:47:00.680 --> 00:47:02.888
EVE EKMAN: I mean, as a
representative of the Greater

00:47:02.888 --> 00:47:05.190
Good Science Center, of
course I believe in that.

00:47:05.190 --> 00:47:08.390
But I think it's so nice to
infuse this experience with how

00:47:08.390 --> 00:47:12.020
technology can help us and
really support us in being

00:47:12.020 --> 00:47:13.640
the humans we want to be.

00:47:13.640 --> 00:47:15.270
Thank you.

00:47:15.270 --> 00:47:17.555
TIMOTHY JORDAN: Thank you.

00:47:17.555 --> 00:47:18.930
EMILY FORTUNA:
Now let's check in

00:47:18.930 --> 00:47:20.880
with our sandbox
team for more demos.

00:47:20.880 --> 00:47:23.375
[MUSIC PLAYING]

00:47:33.707 --> 00:47:34.540
WAYNE PIEKARSKI: Hi.

00:47:34.540 --> 00:47:35.440
I'm Wayne Piekarski.

00:47:35.440 --> 00:47:37.870
And I'm here today at
the Assistant sandbox

00:47:37.870 --> 00:47:40.015
area of Google I/O 2019.

00:47:40.015 --> 00:47:41.390
And we're in the
smart home area.

00:47:41.390 --> 00:47:43.750
And I have Carl Vogel here
from the product team.

00:47:43.750 --> 00:47:45.667
And what we're going to
be talking about today

00:47:45.667 --> 00:47:47.610
is smart home and
what it means and what

00:47:47.610 --> 00:47:48.610
it means for developers.

00:47:48.610 --> 00:47:50.152
So Carl, what is
the developer story?

00:47:50.152 --> 00:47:52.180
And what is smart
home all about?

00:47:52.180 --> 00:47:54.310
CARL VOGEL: Smart home
is a way for users

00:47:54.310 --> 00:47:56.770
to control all the connected
devices in their house,

00:47:56.770 --> 00:47:59.710
everything from smart lights
and smart thermostats to even

00:47:59.710 --> 00:48:01.910
smart plugs-- and even
vacuum cleaners, now.

00:48:01.910 --> 00:48:02.110
WAYNE PIEKARSKI: OK.

00:48:02.110 --> 00:48:02.710
Cool.

00:48:02.710 --> 00:48:04.660
So I mean, what does
it mean for developers?

00:48:04.660 --> 00:48:06.557
Can anyone build
for the smart home?

00:48:06.557 --> 00:48:07.390
CARL VOGEL: Exactly.

00:48:07.390 --> 00:48:09.667
So we have over a
3,500 brands that

00:48:09.667 --> 00:48:11.500
go ahead and integrate
with Google Assistant

00:48:11.500 --> 00:48:12.760
and are growing rapidly.

00:48:12.760 --> 00:48:15.070
Anyone can go into the
Actions on Google console

00:48:15.070 --> 00:48:16.780
and integrate
their smart devices

00:48:16.780 --> 00:48:19.553
into Google Assistant such
that users can control them

00:48:19.553 --> 00:48:21.220
with just the Assistant
and their voice.

00:48:21.220 --> 00:48:22.303
WAYNE PIEKARSKI: OK, cool.

00:48:22.303 --> 00:48:24.560
So this year, at Google
I/O 2019, [INAUDIBLE]

00:48:24.560 --> 00:48:26.062
announced local home SDK.

00:48:26.062 --> 00:48:26.770
CARL VOGEL: Yeah.

00:48:26.770 --> 00:48:27.860
WAYNE PIEKARSKI: So
what's that about,

00:48:27.860 --> 00:48:29.800
and what's new for
developers and why are they

00:48:29.800 --> 00:48:30.620
interested in this?

00:48:30.620 --> 00:48:30.820
CARL VOGEL: This is great.

00:48:30.820 --> 00:48:33.547
So developers today integrate
with the Google Assistant

00:48:33.547 --> 00:48:36.130
via the smart home API, which
is a cloud to cloud integration.

00:48:36.130 --> 00:48:36.760
WAYNE PIEKARSKI: OK.

00:48:36.760 --> 00:48:38.552
CARL VOGEL: And cloud
to cloud integrations

00:48:38.552 --> 00:48:41.150
naturally have some amount
of latency and reliability.

00:48:41.150 --> 00:48:43.390
So we want to provide
a method for developers

00:48:43.390 --> 00:48:46.300
to process those smart
home intents locally

00:48:46.300 --> 00:48:49.330
on Google home speakers and
Google [INAUDIBLE] such that we

00:48:49.330 --> 00:48:53.233
can turn on and off the lights
and other things much faster

00:48:53.233 --> 00:48:54.400
and with higher reliability.

00:48:54.400 --> 00:48:55.120
WAYNE PIEKARSKI:
We do it directly,

00:48:55.120 --> 00:48:56.495
by skipping the
cloud, basically.

00:48:56.495 --> 00:48:57.203
CARL VOGEL: Yeah.

00:48:57.203 --> 00:48:58.510
We skip that developers cloud.

00:48:58.510 --> 00:49:01.450
And they can replicate their
business logic there and run it

00:49:01.450 --> 00:49:02.110
on our device.

00:49:02.110 --> 00:49:02.943
WAYNE PIEKARSKI: OK.

00:49:02.943 --> 00:49:04.570
Well, what excites
me the most is

00:49:04.570 --> 00:49:06.430
that here at I/O, we
have this fantastic

00:49:06.430 --> 00:49:07.390
demo that we've built.

00:49:07.390 --> 00:49:07.930
CARL VOGEL: It's a great demo.

00:49:07.930 --> 00:49:10.138
WAYNE PIEKARSKI: So we have
this great train set here

00:49:10.138 --> 00:49:13.518
that's designed to demonstrate
local versus cloud execution.

00:49:13.518 --> 00:49:15.310
So I guess-- well,
there's this button here

00:49:15.310 --> 00:49:16.477
that I really want to press.

00:49:16.477 --> 00:49:18.460
CARL VOGEL: Give me one
second if I can, Wayne.

00:49:18.460 --> 00:49:20.855
So what we're looking at is
this is the local network.

00:49:20.855 --> 00:49:22.480
And this is the
internet once the query

00:49:22.480 --> 00:49:23.860
leaves the user's home.

00:49:23.860 --> 00:49:25.480
We have the life
of a local query

00:49:25.480 --> 00:49:27.160
and a life of a cloud query.

00:49:27.160 --> 00:49:29.180
So with that, go ahead
and push the button.

00:49:29.180 --> 00:49:30.620
WAYNE PIEKARSKI: So we're
going to race some trains here.

00:49:30.620 --> 00:49:32.140
They're going to go around,
and then these light bulbs

00:49:32.140 --> 00:49:33.460
are going to come on
when they're done.

00:49:33.460 --> 00:49:33.610
OK.

00:49:33.610 --> 00:49:34.225
Here we go.

00:49:34.225 --> 00:49:36.220
CARL VOGEL: So here we
have the user saying,

00:49:36.220 --> 00:49:37.220
turn on the lights.

00:49:37.220 --> 00:49:37.400
WAYNE PIEKARSKI: OK.

00:49:37.400 --> 00:49:39.067
CARL VOGEL: And we
see these two queries

00:49:39.067 --> 00:49:40.390
going to the Assistant server.

00:49:40.390 --> 00:49:42.682
And they're both going to be
processed in the same way.

00:49:42.682 --> 00:49:44.425
And via the smart home
API, they go ahead

00:49:44.425 --> 00:49:47.050
and we send that to the partner
cloud or developer cloud, where

00:49:47.050 --> 00:49:49.090
they go ahead and
process that intent,

00:49:49.090 --> 00:49:50.500
and ultimately,
then, fulfill it.

00:49:50.500 --> 00:49:52.487
If we know the device
is locally controllable,

00:49:52.487 --> 00:49:54.070
then we'll send that
command back down

00:49:54.070 --> 00:49:56.320
to the Google home device,
where we'll then pull up

00:49:56.320 --> 00:49:59.080
the developer's JavaScript
to process that intent, where

00:49:59.080 --> 00:50:01.590
they can communicate with their
light, and look, turn it on.

00:50:01.590 --> 00:50:01.930
[INTERPOSING VOICES]

00:50:01.930 --> 00:50:02.930
WAYNE PIEKARSKI: This one's
still coming by the cloud.

00:50:02.930 --> 00:50:03.030
CARL VOGEL: Yeah.

00:50:03.030 --> 00:50:03.760
It's still coming in.

00:50:03.760 --> 00:50:04.330
It's going to get there.

00:50:04.330 --> 00:50:04.880
WAYNE PIEKARSKI: There we go.

00:50:04.880 --> 00:50:05.588
CARL VOGEL: Yeah.

00:50:05.588 --> 00:50:09.730
So what this does again, it
highlights the reduced latency

00:50:09.730 --> 00:50:12.435
that we can get with the local
home SDK and local operations.

00:50:12.435 --> 00:50:14.060
WAYNE PIEKARSKI:
Well, this is awesome.

00:50:14.060 --> 00:50:14.810
I really love this train set.

00:50:14.810 --> 00:50:16.602
I've always loved trains
since I was a kid.

00:50:16.602 --> 00:50:17.873
So this was a really fun demo.

00:50:17.873 --> 00:50:19.540
So where can developers
go to learn more

00:50:19.540 --> 00:50:20.602
about the local home SDK?

00:50:20.602 --> 00:50:21.310
CARL VOGEL: Yeah.

00:50:21.310 --> 00:50:24.790
go.co/localhomeSDK
is our landing page.

00:50:24.790 --> 00:50:26.800
We also are giving
a talk Wednesday

00:50:26.800 --> 00:50:29.340
at 5:30 PM called "Local
Technologies for the Smart

00:50:29.340 --> 00:50:29.840
Home."

00:50:29.840 --> 00:50:31.900
So if you're interested,
check it out and learn more.

00:50:31.900 --> 00:50:33.040
WAYNE PIEKARSKI: And it's
also on YouTube, as well.

00:50:33.040 --> 00:50:33.490
So people--

00:50:33.490 --> 00:50:33.860
CARL VOGEL: Exactly.

00:50:33.860 --> 00:50:34.550
[INTERPOSING VOICES]

00:50:34.550 --> 00:50:34.680
WAYNE PIEKARSKI: Yeah.

00:50:34.680 --> 00:50:34.980
Awesome.

00:50:34.980 --> 00:50:36.200
Well, thanks for
your time, Carl.

00:50:36.200 --> 00:50:36.410
CARL VOGEL: Yeah.

00:50:36.410 --> 00:50:36.640
Thanks, Wayne.

00:50:36.640 --> 00:50:38.330
WAYNE PIEKARSKI: And thanks
for watching the video.

00:50:38.330 --> 00:50:40.190
This was the Assistant
sandbox area.

00:50:40.190 --> 00:50:42.043
So we'll see you next time.

00:50:42.043 --> 00:50:43.210
EMILY FORTUNA: Thanks, team.

00:50:43.210 --> 00:50:45.040
If you want to check
out any part of I/O,

00:50:45.040 --> 00:50:47.860
go to g.co/io/live.

00:50:47.860 --> 00:50:49.570
After these next
sessions, Ted Haas

00:50:49.570 --> 00:50:52.010
will join us on set for a
conversation on Android.

00:50:52.010 --> 00:50:53.677
And we've got more
from our sandbox.

00:50:53.677 --> 00:50:54.760
You won't want to miss it.

00:50:54.760 --> 00:50:57.410
You're watching I/O Live from
the heart of Silicon Valley.

00:50:57.410 --> 00:50:59.875
[MUSIC PLAYING]

00:51:17.042 --> 00:51:18.250
TIMOTHY JORDAN: Welcome back.

00:51:18.250 --> 00:51:20.290
Our final guest of
the day is Chet Haase.

00:51:20.290 --> 00:51:22.660
Whom you already know if
you're an Android developer.

00:51:22.660 --> 00:51:24.550
Though you might not
know that he's also

00:51:24.550 --> 00:51:26.320
the author of "Round and Holy--

00:51:26.320 --> 00:51:28.280
An Homage to Donuts."

00:51:28.280 --> 00:51:29.480
Chet, welcome to the show.

00:51:29.480 --> 00:51:30.272
CHET HAASE: Thanks.

00:51:30.272 --> 00:51:31.772
TIMOTHY JORDAN: So
we already talked

00:51:31.772 --> 00:51:33.850
with Dan Galpin about
all the announcements

00:51:33.850 --> 00:51:34.590
that were in the keynotes.

00:51:34.590 --> 00:51:34.930
CHET HAASE: OK.

00:51:34.930 --> 00:51:35.680
TIMOTHY JORDAN: So
I'm hoping that we can

00:51:35.680 --> 00:51:37.460
go kind of more big picture.

00:51:37.460 --> 00:51:38.350
Is that OK?

00:51:38.350 --> 00:51:40.720
CHET HAASE: You're saying,
you're saying fast forward.

00:51:40.720 --> 00:51:42.670
Don't talk about that
stuff we already heard.

00:51:42.670 --> 00:51:43.470
You're boring me.

00:51:43.470 --> 00:51:44.470
That's what I'm hearing.

00:51:44.470 --> 00:51:46.803
TIMOTHY JORDAN: I mean, I
wouldn't have put it that way.

00:51:46.803 --> 00:51:47.570
CHET HAASE: OK.

00:51:47.570 --> 00:51:49.210
TIMOTHY JORDAN:
Anyway, Kotlin first.

00:51:49.210 --> 00:51:50.380
CHET HAASE: Kotlin first.

00:51:50.380 --> 00:51:51.340
TIMOTHY JORDAN:
What does that mean?

00:51:51.340 --> 00:51:52.580
And sort of, like,
how do we get here

00:51:52.580 --> 00:51:54.590
and what does it mean for
developers moving forward?

00:51:54.590 --> 00:51:55.070
CHET HAASE: Sure.

00:51:55.070 --> 00:51:55.570
So--

00:51:55.570 --> 00:51:56.350
TIMOTHY JORDAN: Big picture.

00:51:56.350 --> 00:51:57.392
CHET HAASE: Big picture--

00:51:57.392 --> 00:51:59.273
so the big picture is
we announced Kotlin

00:51:59.273 --> 00:52:00.190
a couple of years ago.

00:52:00.190 --> 00:52:01.630
It turns out people
really like it.

00:52:01.630 --> 00:52:02.080
TIMOTHY JORDAN: That's good.

00:52:02.080 --> 00:52:04.000
CHET HAASE: A lot of developers
really liked it at that time

00:52:04.000 --> 00:52:04.330
anyway.

00:52:04.330 --> 00:52:06.622
What we announced was, yeah,
we understand you like it,

00:52:06.622 --> 00:52:09.170
and we're going to support
it as an official language.

00:52:09.170 --> 00:52:10.340
So it's a good thing.

00:52:10.340 --> 00:52:12.880
So people continued to
use it more and more.

00:52:12.880 --> 00:52:14.110
We agree with everyone.

00:52:14.110 --> 00:52:15.830
They should be using
it more and more.

00:52:15.830 --> 00:52:19.090
And now we want to invest
in it more and more

00:52:19.090 --> 00:52:23.135
and offer more capabilities for
them to use it more and more.

00:52:23.135 --> 00:52:24.010
You like that phrase?

00:52:24.010 --> 00:52:24.670
I like it a lot.

00:52:24.670 --> 00:52:25.462
I'm using it a lot.

00:52:25.462 --> 00:52:27.500
So let me give you a
couple of examples.

00:52:27.500 --> 00:52:31.030
So we have this set of
APIs called Jetpack.

00:52:31.030 --> 00:52:32.500
And increasing a
lot of the stuff

00:52:32.500 --> 00:52:34.000
that we're doing
in Jetpack, instead

00:52:34.000 --> 00:52:36.500
of including some of the stuff
that's coming out at I/O,

00:52:36.500 --> 00:52:39.730
it's going to be coming out
first with Kotlin capabilities,

00:52:39.730 --> 00:52:43.240
things like co-routine
integration into a room,

00:52:43.240 --> 00:52:46.060
also co-routing deep integration
into lifecycle and live data

00:52:46.060 --> 00:52:47.650
to make things easier.

00:52:47.650 --> 00:52:50.130
Kotlin only-- or
Kotlin first library

00:52:50.130 --> 00:52:52.090
is like the new Jetpack
composed that we just

00:52:52.090 --> 00:52:55.010
announced we're going to be
developing in the open today.

00:52:55.010 --> 00:52:56.860
So we're going to be
doing much more Kotlin

00:52:56.860 --> 00:53:02.150
stuff for Kotlin developers
outside of Android, as well.

00:53:02.150 --> 00:53:04.870
We're also investing in
more training and docs

00:53:04.870 --> 00:53:08.530
and sample code, things,
like, see, with JetBrains,

00:53:08.530 --> 00:53:09.950
we're doing Kotlin everywhere.

00:53:09.950 --> 00:53:12.280
It's this global
series of events,

00:53:12.280 --> 00:53:14.663
conference-type
educational events.

00:53:14.663 --> 00:53:16.080
There's also a new
Udacity course.

00:53:16.080 --> 00:53:19.030
I think it's developing Android
applications with Kotlin.

00:53:19.030 --> 00:53:22.240
Just launched all of those
episodes in that course.

00:53:22.240 --> 00:53:24.940
So just a lot more training
in general to help people

00:53:24.940 --> 00:53:27.310
learn and use Kotlin
more, because we think

00:53:27.310 --> 00:53:28.963
they should be doing that.

00:53:28.963 --> 00:53:31.380
TIMOTHY JORDAN: I think one
of the interesting discoveries

00:53:31.380 --> 00:53:33.910
I made the other day is that
there are developers now

00:53:33.910 --> 00:53:37.030
that started as Kotlin
Android developers.

00:53:37.030 --> 00:53:39.470
CHET HAASE: That blows my mind.

00:53:39.470 --> 00:53:40.450
TIMOTHY JORDAN: Right?

00:53:40.450 --> 00:53:42.610
Which is-- anyway, that's cool.

00:53:42.610 --> 00:53:46.764
You've been on Android
for a while now.

00:53:46.764 --> 00:53:47.530
And--

00:53:47.530 --> 00:53:48.860
CHET HAASE: Oh, I was
supposed to fill that in.

00:53:48.860 --> 00:53:50.780
So I joined, actually,
nine years ago this month.

00:53:50.780 --> 00:53:51.570
TIMOTHY JORDAN: 9 years?

00:53:51.570 --> 00:53:52.600
CHET HAASE: Yeah, 2010.

00:53:52.600 --> 00:53:55.940
TIMOTHY JORDAN:
Sometime-- in that time,

00:53:55.940 --> 00:53:59.265
I'm sure had a really unique
perspective on everything

00:53:59.265 --> 00:54:01.390
that's happened in the
developer space for Android.

00:54:01.390 --> 00:54:04.190
So here's two relatively
specific questions for you.

00:54:04.190 --> 00:54:07.210
One, what's something that's
super exceptional that you

00:54:07.210 --> 00:54:09.730
didn't expect when you
started working on Android?

00:54:09.730 --> 00:54:11.350
And what's something
that you still

00:54:11.350 --> 00:54:14.300
want to make better,
after all this time?

00:54:14.300 --> 00:54:16.732
CHET HAASE: So
super exceptional--

00:54:16.732 --> 00:54:18.440
there's a bunch of
things to choose from.

00:54:18.440 --> 00:54:19.870
There's all kinds of
interesting features.

00:54:19.870 --> 00:54:21.245
And development
software is cool,

00:54:21.245 --> 00:54:23.230
because new stuff
happens all the time.

00:54:23.230 --> 00:54:25.890
I'm going to pick
Jetpack, because I

00:54:25.890 --> 00:54:26.890
feel very close to this.

00:54:26.890 --> 00:54:29.490
Because a lot of that work
was coming out of the team

00:54:29.490 --> 00:54:31.240
that I was working
with, the toolkit team.

00:54:31.240 --> 00:54:33.070
Where we had all
these capabilities

00:54:33.070 --> 00:54:34.685
in support library--

00:54:34.685 --> 00:54:36.310
the worst thing about
support library--

00:54:36.310 --> 00:54:38.352
we called it support
library, but it was actually

00:54:38.352 --> 00:54:40.030
a plural set of libraries.

00:54:40.030 --> 00:54:42.280
So even the naming of
it was messed up, right?

00:54:42.280 --> 00:54:43.480
So we had AppCompat.

00:54:43.480 --> 00:54:45.720
And then we had
specific utilities.

00:54:45.720 --> 00:54:46.990
Recycler view was in there.

00:54:46.990 --> 00:54:47.200
TIMOTHY JORDAN: There
is a lot of stuff.

00:54:47.200 --> 00:54:48.640
CHET HAASE: All these
different things--

00:54:48.640 --> 00:54:50.590
it was a sandbox of
stuff, all very useful.

00:54:50.590 --> 00:54:53.170
What we're doing with Jetpack
is sort of putting it together

00:54:53.170 --> 00:54:55.630
into cohesive parts.

00:54:55.630 --> 00:54:58.150
So there are these modules that
you can use either together

00:54:58.150 --> 00:55:02.020
or separately for shipping it
in a more robust and predictable

00:55:02.020 --> 00:55:03.010
manner.

00:55:03.010 --> 00:55:04.360
We made sense out of the APIs.

00:55:04.360 --> 00:55:07.120
We refactored it with a
namespace that made sense.

00:55:07.120 --> 00:55:09.140
We took a lot of the
capabilities that we had

00:55:09.140 --> 00:55:11.620
and we put it into a
more sensible form that

00:55:11.620 --> 00:55:13.720
solves one of the biggest
problems with Android

00:55:13.720 --> 00:55:15.460
for developers,
which is how do I

00:55:15.460 --> 00:55:17.830
develop for multiple
versions of Android

00:55:17.830 --> 00:55:20.120
because that is the
developer reality.

00:55:20.120 --> 00:55:20.620
Right?

00:55:20.620 --> 00:55:23.680
So the ability to actually
write to a single API

00:55:23.680 --> 00:55:28.780
and have it work on releases
all the way back to API 14

00:55:28.780 --> 00:55:30.850
right now, I think, is huge.

00:55:30.850 --> 00:55:33.880
And for us to be able to do that
in a way where they can make

00:55:33.880 --> 00:55:36.730
sense out of those APIs and
the documentation and the maven

00:55:36.730 --> 00:55:40.603
artifacts they have to get,
I think, is really huge.

00:55:40.603 --> 00:55:41.770
What was the other question?

00:55:41.770 --> 00:55:43.990
TIMOTHY JORDAN: Something that
you still want to make better.

00:55:43.990 --> 00:55:46.090
CHET HAASE: Everything, I want
to make everything better.

00:55:46.090 --> 00:55:47.050
This is software.

00:55:47.050 --> 00:55:47.770
We have bugs.

00:55:47.770 --> 00:55:49.760
We have features
we want to work on.

00:55:49.760 --> 00:55:52.875
We have APIs that we regret,
because that's how APIs work.

00:55:52.875 --> 00:55:53.750
TIMOTHY JORDAN: Yeah.

00:55:53.750 --> 00:55:55.273
CHET HAASE: So
honestly, everything

00:55:55.273 --> 00:55:56.440
can get better all the time.

00:55:56.440 --> 00:55:59.140
I feel good about where
we're at, especially given

00:55:59.140 --> 00:56:00.850
where we were coming from.

00:56:00.850 --> 00:56:02.892
But I also feel like we
have a lot further to go.

00:56:02.892 --> 00:56:03.683
TIMOTHY JORDAN: OK.

00:56:03.683 --> 00:56:04.390
That's fair.

00:56:04.390 --> 00:56:04.890
All right.

00:56:04.890 --> 00:56:07.365
So your Twitter profile,
which I looked at recently--

00:56:07.365 --> 00:56:08.740
CHET HAASE: It
must be the truth.

00:56:08.740 --> 00:56:10.980
TIMOTHY JORDAN: --on
research for this interview,

00:56:10.980 --> 00:56:14.553
says you're the lead for
the Android UI toolkit.

00:56:14.553 --> 00:56:15.220
CHET HAASE: Yes.

00:56:15.220 --> 00:56:16.270
TIMOTHY JORDAN: So
what's that like?

00:56:16.270 --> 00:56:17.740
CHET HAASE: So the
first thing that

00:56:17.740 --> 00:56:20.622
tells me is that I need to
go update my Twitter profile.

00:56:20.622 --> 00:56:22.830
TIMOTHY JORDAN: This is my
subtle way of saying that.

00:56:22.830 --> 00:56:24.080
CHET HAASE: It is out of date.

00:56:24.080 --> 00:56:26.590
So I was the lead for the
Android UI toolkit team.

00:56:26.590 --> 00:56:29.620
So when I joined in 2010, I
joined the UI toolkit team.

00:56:29.620 --> 00:56:33.230
I worked on animations and
graphics and performance and UI

00:56:33.230 --> 00:56:33.730
stuff.

00:56:33.730 --> 00:56:36.838
And eventually, managed the team
for about the last five years,

00:56:36.838 --> 00:56:38.380
as the team was
growing, doing things

00:56:38.380 --> 00:56:41.140
like Jetpack and architecture
components and all

00:56:41.140 --> 00:56:43.420
the normal platform
toolkit stuff.

00:56:43.420 --> 00:56:45.820
And then about three
months ago, I made a change

00:56:45.820 --> 00:56:48.287
and moved to a team called
Developer Relations.

00:56:48.287 --> 00:56:49.870
I don't know if you
know what that is?

00:56:49.870 --> 00:56:51.350
TIMOTHY JORDAN:
I've heard of them.

00:56:51.350 --> 00:56:51.490
CHET HAASE: Yeah.

00:56:51.490 --> 00:56:52.480
TIMOTHY JORDAN: Sure.

00:56:52.480 --> 00:56:54.490
CHET HAASE: So I made
that switch, dropped

00:56:54.490 --> 00:56:56.710
all my management
responsibilities on the floor,

00:56:56.710 --> 00:56:59.092
and I'm now a developer
advocate in DevRel.

00:56:59.092 --> 00:57:00.550
TIMOTHY JORDAN:
Well, that's great.

00:57:00.550 --> 00:57:01.508
CHET HAASE: I think so.

00:57:01.508 --> 00:57:03.800
TIMOTHY JORDAN: Welcome to
the light side or dark side?

00:57:03.800 --> 00:57:05.508
CHET HAASE: I consider
it the light side.

00:57:05.508 --> 00:57:08.050
Because you know what, new
jobs always look better.

00:57:08.050 --> 00:57:10.000
And then they become old jobs.

00:57:10.000 --> 00:57:10.960
TIMOTHY JORDAN: Let's
hope this one stays

00:57:10.960 --> 00:57:12.040
the light side for some time.

00:57:12.040 --> 00:57:13.582
CHET HAASE: No, I'm
excited about it.

00:57:13.582 --> 00:57:15.550
So the way I look
at the job changes,

00:57:15.550 --> 00:57:18.250
I now get to do my
hobby as my real job.

00:57:18.250 --> 00:57:19.900
I was doing dev rel stuff.

00:57:19.900 --> 00:57:21.430
I was doing a bunch of outreach.

00:57:21.430 --> 00:57:22.180
TIMOTHY JORDAN: Don't tell them.

00:57:22.180 --> 00:57:22.847
CHET HAASE: Why?

00:57:22.847 --> 00:57:23.980
[LAUGHTER]

00:57:23.980 --> 00:57:25.420
My bosses won't watch it.

00:57:25.420 --> 00:57:26.673
It doesn't matter.

00:57:26.673 --> 00:57:28.840
But I've been doing dev rel
since I got here, right?

00:57:28.840 --> 00:57:31.420
I really love doing the
outreach stuff with developers.

00:57:31.420 --> 00:57:33.010
But I always felt
guilty about that

00:57:33.010 --> 00:57:34.810
because that wasn't
actually my day job.

00:57:34.810 --> 00:57:36.690
And now it is.

00:57:36.690 --> 00:57:38.740
TIMOTHY JORDAN:
You're welcome, world.

00:57:38.740 --> 00:57:39.728
OK.

00:57:39.728 --> 00:57:41.770
Something else that you
do that I want to mention

00:57:41.770 --> 00:57:45.573
is the ADB podcast, the
Android Developers Backstage.

00:57:45.573 --> 00:57:46.240
CHET HAASE: Yes.

00:57:46.240 --> 00:57:48.460
TIMOTHY JORDAN: So for the
handful of people watching

00:57:48.460 --> 00:57:50.220
that haven't subscribed
to that podcast yet--

00:57:50.220 --> 00:57:51.178
CHET HAASE: Wait, what?

00:57:51.178 --> 00:57:52.930
TIMOTHY JORDAN: --it
could be a couple--

00:57:52.930 --> 00:57:57.315
what episode is your favorite
that they should start with?

00:57:57.315 --> 00:57:57.940
CHET HAASE: OK.

00:57:57.940 --> 00:58:01.240
I'm going to start by saying
they are all my children.

00:58:01.240 --> 00:58:04.100
And there is no
favorite among them.

00:58:04.100 --> 00:58:07.040
But if I had to pick, I would
go way back in the archives

00:58:07.040 --> 00:58:10.055
to back when our audio quality,
frankly, was very poor.

00:58:10.055 --> 00:58:11.930
So you're going to have
to suffer through it.

00:58:11.930 --> 00:58:13.870
It's going to sound
like one of those 1930s,

00:58:13.870 --> 00:58:17.800
just past the talky
transition movies.

00:58:17.800 --> 00:58:21.550
But we talked to on Anwar,
who was at that time

00:58:21.550 --> 00:58:23.150
managing the art team.

00:58:23.150 --> 00:58:25.510
We talked to him about how
garbage collectors work.

00:58:25.510 --> 00:58:26.620
And it was so fascinating.

00:58:26.620 --> 00:58:29.170
So my favorite thing
about doing ADB is we

00:58:29.170 --> 00:58:30.700
get to learn how stuff works.

00:58:30.700 --> 00:58:33.580
We are not talking to people
about stuff that we know.

00:58:33.580 --> 00:58:36.940
We're talking about stuff
that we don't, by definition.

00:58:36.940 --> 00:58:39.400
Oh, I'd like to learn
more about travel, or art,

00:58:39.400 --> 00:58:42.100
or Android Studio, or
performance, or whatever.

00:58:42.100 --> 00:58:44.930
We grab people on the team that
know more about that stuff.

00:58:44.930 --> 00:58:46.040
And we have a
conversation with them.

00:58:46.040 --> 00:58:47.540
So we had a
conversation with Anwar.

00:58:47.540 --> 00:58:50.565
And we were so interested that
our little 45 minute episode

00:58:50.565 --> 00:58:51.940
stretched onto an
hour and a half

00:58:51.940 --> 00:58:53.770
before we ran out of words.

00:58:53.770 --> 00:58:55.610
And we cut it into two episodes.

00:58:55.610 --> 00:58:57.790
So probably my favorite,
because it best

00:58:57.790 --> 00:59:01.258
represents the oh, my gosh, do
we get into the details part.

00:59:01.258 --> 00:59:02.050
TIMOTHY JORDAN: OK.

00:59:02.050 --> 00:59:03.680
The art episode-- sss.

00:59:03.680 --> 00:59:06.070
CHET HAASE: Sss-- art episodes--

00:59:06.070 --> 00:59:06.640
there's two.

00:59:06.640 --> 00:59:09.180
It's like 30s,
somewhere in the 30s.

00:59:09.180 --> 00:59:10.570
And we're now at 112.

00:59:10.570 --> 00:59:12.130
TIMOTHY JORDAN: OK.

00:59:12.130 --> 00:59:13.430
Thanks for joining us, Chet.

00:59:13.430 --> 00:59:14.440
CHET HAASE: Thank you.

00:59:14.440 --> 00:59:15.232
TIMOTHY JORDAN: OK.

00:59:15.232 --> 00:59:17.620
Next up, let's finish
things off with some machine

00:59:17.620 --> 00:59:19.274
learning demos.

00:59:19.274 --> 00:59:20.720
[MUSIC PLAYING]

00:59:20.720 --> 00:59:23.130
[INTERPOSING VOICES]

00:59:28.700 --> 00:59:30.950
PAIGE BAILEY: I'm Paige
Bailey, a TensorFlow developer

00:59:30.950 --> 00:59:31.450
advocate.

00:59:31.450 --> 00:59:33.440
And I'm here today with
Carla Bromberg, who's

00:59:33.440 --> 00:59:36.090
going to tell us all about
our AI for social good efforts

00:59:36.090 --> 00:59:37.020
at Google.

00:59:37.020 --> 00:59:39.160
So tell us a little bit
more about it, Carla.

00:59:39.160 --> 00:59:40.410
CARLA BROMBERG: Thanks, Paige.

00:59:40.410 --> 00:59:43.250
So for AI for
social good, we aim

00:59:43.250 --> 00:59:45.470
to accelerate the application
of machine learning

00:59:45.470 --> 00:59:46.980
to the world's
biggest humanitarian

00:59:46.980 --> 00:59:48.442
and environmental issues.

00:59:48.442 --> 00:59:50.150
So today I'm going to
tell you about some

00:59:50.150 --> 00:59:51.858
of the core research
and engineering work

00:59:51.858 --> 00:59:53.480
we're doing a lot
more specifically,

00:59:53.480 --> 00:59:55.160
the flood forecasting
initiative.

00:59:55.160 --> 00:59:56.160
PAIGE BAILEY: Excellent.

00:59:56.160 --> 00:59:58.587
So is this the flood
forecasting initiative here?

00:59:58.587 --> 00:59:59.420
CARLA BROMBERG: Yes.

00:59:59.420 --> 01:00:01.550
So this is our demo.

01:00:01.550 --> 01:00:06.380
And so what this shows you
is how we're using machine

01:00:06.380 --> 01:00:08.330
learning to help
forecast and predict

01:00:08.330 --> 01:00:10.700
with better accuracy
where floods are.

01:00:10.700 --> 01:00:13.340
So that way we can
tell users when

01:00:13.340 --> 01:00:14.720
they're affected by floods.

01:00:14.720 --> 01:00:17.330
So earlier today
in the keynote, we

01:00:17.330 --> 01:00:19.100
announced that we're
rolling this out

01:00:19.100 --> 01:00:21.710
across a big portion of India.

01:00:21.710 --> 01:00:23.510
And I'm going to
actually show you

01:00:23.510 --> 01:00:26.160
the simulation that we have
for the demo here today.

01:00:26.160 --> 01:00:30.300
So this simulation is over
the course of 48 hours.

01:00:30.300 --> 01:00:32.780
So if you look, you can
see the water filling in.

01:00:32.780 --> 01:00:34.920
So this is without
using machine learning.

01:00:34.920 --> 01:00:38.730
You can see that the
accuracy is not very high.

01:00:38.730 --> 01:00:40.580
And you can see
also that there's

01:00:40.580 --> 01:00:44.720
a lot of regions that are
predicted to have floods.

01:00:44.720 --> 01:00:48.260
Now with Google's flood
forecasting effort,

01:00:48.260 --> 01:00:52.352
you can see, as you both use
the elevation maps that we've

01:00:52.352 --> 01:00:54.560
been working on that involve
machine learning as well

01:00:54.560 --> 01:00:58.710
as the models that we've made,
how much more accurate it is.

01:00:58.710 --> 01:01:01.820
So if you're a person
who lives in this region,

01:01:01.820 --> 01:01:03.930
earlier it would have
said it was flooding

01:01:03.930 --> 01:01:05.925
and then you would
have to evacuate,

01:01:05.925 --> 01:01:07.800
or you would think that
you need to evacuate.

01:01:07.800 --> 01:01:12.260
But actually, when you look
at the more precise models,

01:01:12.260 --> 01:01:13.410
you'll actually be safe.

01:01:13.410 --> 01:01:15.230
So what we're hoping
is through this work

01:01:15.230 --> 01:01:18.350
you can see that accuracy
has gone up to 79%.

01:01:18.350 --> 01:01:21.450
And then when people
get the alerts,

01:01:21.450 --> 01:01:24.270
they can have the
information that they need.

01:01:24.270 --> 01:01:26.960
So as you can see
on the phone, what

01:01:26.960 --> 01:01:29.300
they'll see moving into
this monsoon season

01:01:29.300 --> 01:01:32.135
is when you search on Google,
or when you go into Google Maps

01:01:32.135 --> 01:01:33.510
and you're in an
affected region,

01:01:33.510 --> 01:01:35.872
it'll give you some more
information so that you know

01:01:35.872 --> 01:01:37.580
how to keep yourself
and your family safe

01:01:37.580 --> 01:01:39.163
and whether or not
you should evacuate

01:01:39.163 --> 01:01:42.367
and when you get an alert
from Google public alerts.

01:01:42.367 --> 01:01:43.700
PAIGE BAILEY: This is fantastic.

01:01:43.700 --> 01:01:45.680
My academic background
is very focused

01:01:45.680 --> 01:01:49.130
on quantitative hydrogeology
and geospatial reasoning.

01:01:49.130 --> 01:01:52.280
And it's amazing to be able to
see the kind of accuracy uptick

01:01:52.280 --> 01:01:54.320
that you get just by
using machine learning

01:01:54.320 --> 01:01:55.880
with your elevation maps.

01:01:55.880 --> 01:01:57.077
This is really, really cool.

01:01:57.077 --> 01:01:58.160
CARLA BROMBERG: Thank you.

01:01:58.160 --> 01:01:59.035
We're really excited.

01:01:59.035 --> 01:02:01.850
And we hope we can save a lot
of lives ahead of this monsoon

01:02:01.850 --> 01:02:02.350
season.

01:02:02.350 --> 01:02:04.620
And we're hoping to also
expand this work across more

01:02:04.620 --> 01:02:05.120
countries.

01:02:05.120 --> 01:02:06.662
And for this, we're
partnering really

01:02:06.662 --> 01:02:08.713
closely with the
Indian government,

01:02:08.713 --> 01:02:10.880
and many others on the
ground, and first responders.

01:02:10.880 --> 01:02:12.630
So hopefully, we can
help get the word out

01:02:12.630 --> 01:02:14.420
so that people can
evacuate to safety.

01:02:14.420 --> 01:02:17.045
PAIGE BAILEY: Thank you so much
for telling me about this work.

01:02:17.045 --> 01:02:18.980
And let's go and take a
look at the other demo

01:02:18.980 --> 01:02:20.620
that we have from
AI for social good

01:02:20.620 --> 01:02:22.575
that's very focused
on whale detection

01:02:22.575 --> 01:02:23.450
CARLA BROMBERG: Yeah.

01:02:23.450 --> 01:02:25.370
Absolutely.

01:02:25.370 --> 01:02:26.120
EMILY FORTUNA: Hi.

01:02:26.120 --> 01:02:27.860
I'm here with Julie Cattiau.

01:02:27.860 --> 01:02:31.617
And she's here to tell me about
this really cool whale demo.

01:02:31.617 --> 01:02:32.450
JULIE CATTIAU: Yeah.

01:02:32.450 --> 01:02:34.410
Nice to meet you.

01:02:34.410 --> 01:02:38.360
So this is a demo that's related
to the bioacoustics project

01:02:38.360 --> 01:02:39.600
at Google.

01:02:39.600 --> 01:02:43.380
So we're a small team, part of
the AI for social good efforts.

01:02:43.380 --> 01:02:45.420
And so we've been
training machine

01:02:45.420 --> 01:02:48.740
learning model to detect
the presence of animals

01:02:48.740 --> 01:02:52.280
just by listening
to the audio sound.

01:02:52.280 --> 01:02:54.350
And so this project,
in particular,

01:02:54.350 --> 01:02:57.980
is a partnership with
NOAA, the National Oceanic

01:02:57.980 --> 01:03:00.080
and Atmospheric
Administration, which is part

01:03:00.080 --> 01:03:01.840
of the American government.

01:03:01.840 --> 01:03:05.780
NOAA in particular has a
network of microphones.

01:03:05.780 --> 01:03:09.490
These microphones have
been set up in 2005.

01:03:09.490 --> 01:03:12.990
And they're in different
spots in the Pacific Ocean.

01:03:12.990 --> 01:03:17.370
And so in the end, it
adds up to a lot of data.

01:03:17.370 --> 01:03:21.350
And so NOAA, the data said
that NOAA in particular

01:03:21.350 --> 01:03:24.140
was interested in getting
our help to analyze.

01:03:24.140 --> 01:03:27.720
It is close to 200,000
hours of audio.

01:03:27.720 --> 01:03:31.280
So as you can imagine, nobody
can listen to the whole thing.

01:03:31.280 --> 01:03:32.940
It's just way too long.

01:03:32.940 --> 01:03:36.350
And that's where machine
learning can be very helpful.

01:03:36.350 --> 01:03:39.620
Because with enough
examples of sounds,

01:03:39.620 --> 01:03:43.090
machine learning can be
applied to recognizing

01:03:43.090 --> 01:03:47.810
a particular species of animals
just by listening to the sound.

01:03:47.810 --> 01:03:51.540
And actually, if you think about
animals that live in the ocean,

01:03:51.540 --> 01:03:55.010
almost all of them produce
some sort of sound, even fish,

01:03:55.010 --> 01:03:56.720
which can be surprising.

01:03:56.720 --> 01:03:58.760
Boats definitely
make a lot of noise.

01:03:58.760 --> 01:04:01.100
They're not animals,
but they make noise.

01:04:01.100 --> 01:04:02.270
Whales make sounds.

01:04:02.270 --> 01:04:03.720
Dolphins do, too.

01:04:03.720 --> 01:04:07.220
And so the question
for us was can we

01:04:07.220 --> 01:04:11.150
identify which species our
present in audio data set

01:04:11.150 --> 01:04:15.530
and classify them using
artificial intelligence.

01:04:15.530 --> 01:04:17.870
So this demo here
that you can see

01:04:17.870 --> 01:04:22.270
represents three months
worth of data from that data

01:04:22.270 --> 01:04:24.510
set I was telling you about.

01:04:24.510 --> 01:04:27.440
And what you can
do is, if you slide

01:04:27.440 --> 01:04:32.220
this table along the data, you
can see that this is actually

01:04:32.220 --> 01:04:33.510
a spectrogram.

01:04:33.510 --> 01:04:36.780
So it's a visual
representation of a sound.

01:04:36.780 --> 01:04:40.650
This is frequency over time
for a particular sound.

01:04:40.650 --> 01:04:43.320
And so NOAA provided some
examples of humpback whale

01:04:43.320 --> 01:04:44.010
sounds.

01:04:44.010 --> 01:04:46.930
And then we trained the
model to recognize the sounds

01:04:46.930 --> 01:04:48.160
automatically.

01:04:48.160 --> 01:04:51.180
And we used this on
the whole pile of data

01:04:51.180 --> 01:04:54.420
that they had to tell
them where the whales were

01:04:54.420 --> 01:04:56.190
and at which point in time.

01:04:56.190 --> 01:05:00.120
When the color is light blue,
it means that our classifier

01:05:00.120 --> 01:05:02.370
has detected a whale.

01:05:02.370 --> 01:05:04.290
Would you like to listen
to the whale song?

01:05:04.290 --> 01:05:04.870
EMILY FORTUNA: Absolutely.

01:05:04.870 --> 01:05:05.620
JULIE CATTIAU: OK.

01:05:08.890 --> 01:05:11.610
EMILY FORTUNA: So getting lots
of ocean sounds right now.

01:05:11.610 --> 01:05:12.930
Oh, I hear the whale.

01:05:12.930 --> 01:05:14.020
JULIE CATTIAU: You do?

01:05:14.020 --> 01:05:14.978
EMILY FORTUNA: Awesome.

01:05:14.978 --> 01:05:16.750
[LAUGHTER]

01:05:16.750 --> 01:05:18.990
That's wonderful!

01:05:18.990 --> 01:05:20.370
Well, thank you so much.

01:05:20.370 --> 01:05:21.240
This is lovely.

01:05:21.240 --> 01:05:22.458
JULIE CATTIAU: Thank you.

01:05:22.458 --> 01:05:24.000
EMILY FORTUNA: It's
great that you're

01:05:24.000 --> 01:05:25.935
able to track whale
migration patterns.

01:05:25.935 --> 01:05:26.768
JULIE CATTIAU: Yeah.

01:05:26.768 --> 01:05:27.990
Thank you for your interest.

01:05:27.990 --> 01:05:30.240
TIMOTHY JORDAN: There are a
few more sessions to come.

01:05:30.240 --> 01:05:31.657
And later tonight,
we're all going

01:05:31.657 --> 01:05:34.590
to rock out to the Flaming
Lips as they integrate machine

01:05:34.590 --> 01:05:36.180
learning with their show.

01:05:36.180 --> 01:05:38.042
That'll be at 8:30
tonight Pacific time.

01:05:38.042 --> 01:05:39.500
So set your alarms
for what's going

01:05:39.500 --> 01:05:41.670
to be an amazing performance.

01:05:41.670 --> 01:05:45.480
And come back here tomorrow for
more special guests and demos.

01:05:45.480 --> 01:05:46.440
We'll see you then.

01:05:46.440 --> 01:05:49.490
[MUSIC PLAYING]

