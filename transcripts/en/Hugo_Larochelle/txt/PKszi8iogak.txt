Speaker 1:          00:00          Yeah.

Speaker 2:          00:01          In this video will introduce the idea of learning word representations.

Speaker 1:          00:06          Yeah.

Speaker 2:          00:07          In the previous video we introduced the notion of one hot encoding or 100 representation, which allows us to get a vector oral representation out of a word. And the way we do that is that we just create a vector filled with zeros of size, corresponding number of words in our vocabulary. And then we put a single one at the position associated with the idea of a word. And if we have a sequence of words, we can concatenate these one hot vectors and a, this way we can get a virtual representation that is more amenable to machine learning and and particular usage. For instance, in the neural network. Now unfortunately this vector is going to be very high dimensional and they are consequences, uh, associated with that, uh, particular were more vulnerable to overfitting because we're much higher dimensions. So we're going to have many more parameters to train. So we'll need more data for that. Uh, there might also be some problems computationally with manipulating such factors for certain operations. So now what we'll see in this video, we'll introduce this fundamental idea that had been introduced in the context of neural networks for tackling a as an alternative to using the a 100 encoding, uh, and which works really well in practice.

Speaker 1:          01:19          Yeah.

Speaker 2:          01:20          So the idea is that instead of using this fixed representation that makes no assumption about the similarity of words, we instead going to try to learn or a representation of vector representation for words a directly, which is going to, uh, hopefully, uh, uh, model the, the similarities in tactic in semantic between the words. So, um, what we'll do is that for each word, we'll learn some sort of mapping that takes the word id and then maps it into a representation CW, which was going to be in this case, a vector. So for instance, uh, in, uh, this is just a made up example before the word de, say it has the id one, then a CW might give us this vector. And then for a similar word, uh, uh, which would have in this example, the, uh, id, uh, to then it might give us a very similar vector where we see that the values in the vectors, I actually quite a similar, so the square distance you can distance between the two vectors would be, would be small. And then again in this made up example, having be would have a similar vector representation, cat and dog and so on. And so, um, the idea here we're introducing is decide, yeah, we will try to learn what these vectors are. And a, ideally we want to learn a representation which maintained similarity that syntactic and semantic between the words and uh, uh, as we'll see, it's going to be possible to do that in, uh, inside of the neural network.

Speaker 2:          03:02          So here's an example of, uh, some work representations that were learned with some given that with them by a blitzer Italian in 2004. Uh, so I'm showing this figure here because it really illustrates well this what we're trying to accomplish here. Uh, so the word representation that they learn, if they were projected in two dimensions, uh, they were able to learn representations such that, uh, the all the months, uh, were, uh, associated with the vector into d, which was, uh, in the same region of the two d space. Uh, here at learn all the different numbers, uh, hear that million and billion all the days of the week. We're all around the same, uh, position again in the two d space. So that's what we're trying to achieve. And trying to find a way of actually learning this, uh, vectorial space for, uh, the different words.

Speaker 2:          04:00          So for now, just imagine we actually had these word representations then we could use them instead of the one hot encoding to create the vector representations of a sequence of words that we could feed to our neural network. So if you add a problem where we wanted to, uh, make a prediction based on a window of 10 words, so wd one of two w 10, then similarly as with the a one having coatings, we could just concatenate the learn word representation. So we take CW one and then concatenated with CW to see what CW three up to CW 10, and then we'll use that as our input representation x and then we could feed that to the neural network. Um, now the question is how do we actually learn this, uh, uh, this mapping these vectors for all the different words in our vocabulary? Well, DLD, uh, is that we actually use gradient descent and the, we were going to do that does get, is that we are going to treat these words representations as parameters inside the neural network parameters that the neural network, when we are performing backpropagation we'll be able to, uh, to train when we do stochastic gradient descent.

Speaker 2:          05:11          So we won't only update the neural network parameters, we'll also update the vectors, uh, in our vector space. So each representation CW, uh, that's is in some given input x is going to be updated by taking its current value and then subtracting step size or learning, right time's degradent of whatever loss we're optimizing in our neural network with respect to that vector that is at the input of the neural network. And that will give us a new, uh, representation, uh, which we can use next time. We, we use the word ws input to a neural net. And so instead of just doing backpropagation, sorry, stochastic gradient descent on the parameters of their neural network. The idea here is that to learn the word representations, we'll also do gradient descent on the word representation themselves.

Speaker 1:          06:08          Yeah.

Speaker 2:          06:09          So to some of the procedure for learning, these were representations. We would first initialize all the word vectors randomly in some way, much like the parameters of the neural network. And then, uh, every time we would feed a representation as part of the input of the neural net, uh, then during backpropagation would back propagate the gradients, uh, towards the word representation. And then we would update the word representation also based on, uh, the standards to a gradient descent, a rule. And so as the neural network trains, it will not only adapt, it's a connections, it's hidden units and I'll put it's up with units to perform the task better. It will also adapt the word representation themselves so that it's better able to learn the task that it has to solve.

Speaker 1:          06:59          Okay.

Speaker 2:          07:00          So we'll often represent all of these word representations and store them into a matrix, which I'm calling see here in bold. And essentially the roles of that matrix will curse bond to the word representation CW. Now, uh, one thing we can notice is that obtaining CW, so for our word, IDW obtaining the rug representation if it's stored in a matrix, c actually just corresponds to taking the 100 vector of that word w and multiplying it on the left of the Matrix by the Matrix c. So if you think about it, if you take a vector filled with Zeros and a single one in some position and you multiply that by a matrix on its left, then the result is going to be the a row of the Matrix, uh, at the position where we put a one in the one hot factor. And so another way of viewing this, uh, operation of, of taking a, uh, a word id and providing a word, a representation or vector, is that we're actually taking the original 100 and coding representation of the word and we're projecting it on the columns of w.

Speaker 2:          08:19          And in other words, we're actually doing some sort of dimentionality reduction, reducing the dimensionality of the original one hut or a 100, uh, representation of the words into a lower dimensions, uh, all lower dimensional space, which, uh, uh, is going to be such that we're less likely to over fit for instance. And um, that's something we haven't mentioned thus far, but actually the size of the word representations, it's something that we can choose a, we, it's going to actually be treated as hyper parameter, much like they're hidden itself. There are neural network and also if you view this, uh, process of providing a word representation for were w as just this linear transformation, then it becomes more obvious why it is that we can actually propagate gradients and train this matrix. C, uh, it's just a linear transformation of some original vector representation of the, uh, words and much like the other Matrix multiplications we do in their neural net, we have no difficulty back, properly back propagate ingredients, uh, through these vector multiplications.

Speaker 2:          09:28          Now that being said, this is mainly conceptual. That is when we implement, uh, taking a word IDW and providing a vector, a CW, uh, we really implement that as a lookup table and not as expensive multiplication of a vector that's mostly zeros with a matrix. Uh, specifically one way of doing that is that CW would just return and the rate that points a to DWF row up the Matrix c. So that's the main idea for learning word representations. And in the following videos, we'll see different approaches that uses, uh, that use this idea for a solving natural language processing tasks. And the first one that we'll see is language modeling a task. And it's that it's in that context that we're representations have been first introduced. So in that context, it will become, I think, a much more concrete what me mean by a word representation.