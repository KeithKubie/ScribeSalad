WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:02.510
So what did we learn about
exploring exploration, Charles?

00:00:02.510 --> 00:00:04.366
&gt;&gt; Well, we actually learned a lot, but

00:00:04.366 --> 00:00:07.530
really I think you can summarize
it in just a couple things.

00:00:07.530 --> 00:00:11.850
We learned about bandits,
which is really about stochasticity and

00:00:11.850 --> 00:00:12.950
randomness.

00:00:12.950 --> 00:00:17.560
And in particular,
we learned about how we can

00:00:17.560 --> 00:00:22.010
estimate what we know using Hoeffding
bounds and maybe some union bounds.

00:00:22.010 --> 00:00:22.530
&gt;&gt; So, yeah.

00:00:22.530 --> 00:00:25.031
In particular we employed
Hoeffding bound,

00:00:25.031 --> 00:00:29.506
u nion bound to convince ourselves that
we have a sufficiently accurate estimate

00:00:29.506 --> 00:00:33.452
that we can get near optimal reward
in these kind of non-sequential but

00:00:33.452 --> 00:00:35.500
stochastic decision problems.

00:00:35.500 --> 00:00:39.097
&gt;&gt; Right, and that's really important
because a lot of the time you don't know

00:00:39.097 --> 00:00:41.088
what to do because you're not certain,
or

00:00:41.088 --> 00:00:43.321
how certain you really
are about what you know.

00:00:43.321 --> 00:00:46.779
And what the Hoefding and union bounds
tell us is how certain we really are and

00:00:46.779 --> 00:00:48.736
so we can know when
we're certain enough.

00:00:48.736 --> 00:00:50.485
&gt;&gt; Good, all right so
we have a stochastic world,

00:00:50.485 --> 00:00:52.910
we want to learn how it works so
that we can get near optimal reward.

00:00:52.910 --> 00:00:53.870
So we can optimize.

00:00:53.870 --> 00:00:55.042
&gt;&gt; Right, so that was the first thing.

00:00:55.042 --> 00:01:00.039
And then the second thing is we learned,
well, we learned about Rmax, really,

00:01:00.039 --> 00:01:03.724
and how Rmax works specifically
with deterministic MDPs.

00:01:03.724 --> 00:01:07.486
&gt;&gt; Right, and so that was about
optimism in the face of uncertainty and

00:01:07.486 --> 00:01:11.380
how it would cause us to explore or
discover new stuff about the world,

00:01:11.380 --> 00:01:12.898
kind of by planning ahead and

00:01:12.898 --> 00:01:16.730
trying to get to states where
new information could be gained.

00:01:16.730 --> 00:01:18.520
&gt;&gt; Right, well what it really says,
it's really,

00:01:18.520 --> 00:01:21.880
we believe the grass is always greener
until we get there and learn otherwise.

00:01:21.880 --> 00:01:22.500
&gt;&gt; All right, great.

00:01:22.500 --> 00:01:24.640
This let us deal with
sequential decision making,

00:01:24.640 --> 00:01:27.010
this let us deal with
stochastic decision making.

00:01:27.010 --> 00:01:28.020
&gt;&gt; And then we combine them.

00:01:28.020 --> 00:01:33.280
&gt;&gt; Right, where we used the bandit
idea to estimate the noisy parameters,

00:01:33.280 --> 00:01:35.740
the transition probabilities and
so forth, and

00:01:35.740 --> 00:01:39.450
we used the Rmax idea to make sure
that we visited things enough so

00:01:39.450 --> 00:01:41.390
that we could get those
accurate estimates.

00:01:41.390 --> 00:01:44.530
&gt;&gt; Right, so
Rmax basically remained Rmax, and

00:01:44.530 --> 00:01:47.753
the bandit stuff helped us with
the transition probabilities, so

00:01:47.753 --> 00:01:49.460
that we knew when we
actually believed them.

00:01:49.460 --> 00:01:52.990
So, they were kind of,
this isn't really true, but

00:01:52.990 --> 00:01:54.940
they were kind of deterministic enough.

00:01:54.940 --> 00:01:59.100
So we knew enough to know what
the transition model is, and

00:01:59.100 --> 00:02:01.395
once you know what the transition model
is you can just kind of go from there,

00:02:01.395 --> 00:02:04.040
because then you can just estimate
the rewards and do the right thing.

00:02:04.040 --> 00:02:08.090
&gt;&gt; Now this notion of, the way that
learning happens in the bandit

00:02:08.090 --> 00:02:10.590
here where we actually kind of
distinguish between known and

00:02:10.590 --> 00:02:13.420
unknown, can actually be generalized

00:02:13.420 --> 00:02:16.375
even beyond the kind of general
MDPs that we're talking about here.

00:02:17.580 --> 00:02:21.076
And I don't think we're going to get
a chance to talk about this, but

00:02:21.076 --> 00:02:25.200
the idea of KWIK learning is learning
transition probabilities using methods

00:02:25.200 --> 00:02:26.615
that know what they know.

00:02:26.615 --> 00:02:30.230
And if it can distinguish between
known and unknown, it can associate

00:02:30.230 --> 00:02:33.750
optimism with the unknown and then we
can make guarantees on how uniquely and

00:02:33.750 --> 00:02:36.690
efficiently near optimal
behavior can be learned.

00:02:36.690 --> 00:02:39.460
So this is kind of a learning
framework that my students and

00:02:39.460 --> 00:02:44.470
I have been studying to try to
generalize beyond learning in

00:02:44.470 --> 00:02:48.480
tabular MDPs to be able to generalize
between transition probabilities and

00:02:48.480 --> 00:02:49.390
different parts of the MDP.

00:02:49.390 --> 00:02:50.244
&gt;&gt; Well that all makes sense to me.

00:02:50.244 --> 00:02:51.781
So then the only question is,

00:02:51.781 --> 00:02:54.918
how does this help us do
practical reinforcement learning?

00:02:54.918 --> 00:02:56.845
&gt;&gt; All right, so
that pretty much ends this.

00:02:56.845 --> 00:03:00.145
&gt;&gt; [LAUGH]
&gt;&gt; Well, okay, so just briefly,

00:03:00.145 --> 00:03:04.310
I find that Rmax is actually a really
effective algorithm to use in practice.

00:03:04.310 --> 00:03:07.230
We don't set C,
we don't set that parameter of how often

00:03:07.230 --> 00:03:10.730
we need to see things to be what
the theory says we should set it to.

00:03:10.730 --> 00:03:13.330
We usually set it to something much,
much smaller than that.

00:03:13.330 --> 00:03:17.550
And we tried it by hand, strike a trade
off between making it too small,

00:03:17.550 --> 00:03:21.230
in which case it might actually not
learn near optimal behavior, and

00:03:21.230 --> 00:03:25.281
making it too big, in which case the
learner actually has to sniff around and

00:03:25.281 --> 00:03:28.690
visit many, many, many, many state
action pairs over and over again.

00:03:28.690 --> 00:03:32.390
So, Rmax itself does seems to
be a pretty effective algorithm.

00:03:32.390 --> 00:03:34.490
It makes very good use of the data.

00:03:34.490 --> 00:03:37.850
It's just we can't quite use it in
the theoretically specified way.

00:03:37.850 --> 00:03:39.160
&gt;&gt; Okay, well that's good enough for me.

00:03:39.160 --> 00:03:40.150
Seems pretty practical.

00:03:40.150 --> 00:03:40.740
Okay, so we're done.

00:03:40.740 --> 00:03:41.790
So, are we done with the class?

00:03:41.790 --> 00:03:42.290
What's next?

00:03:42.290 --> 00:03:43.620
&gt;&gt; We have more stuff!

00:03:43.620 --> 00:03:45.000
We have what's next, next.

00:03:45.000 --> 00:03:46.650
&gt;&gt; Oh cool, well I look forward to that.

