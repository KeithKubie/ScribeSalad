WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:02.570
&gt;&gt; So, we've answered the thing about continuous attributes.

00:00:02.570 --> 00:00:05.670
Now, here's another thing. When do we really stop?

00:00:05.670 --> 00:00:09.040
&gt;&gt; When we get all the answers right. When

00:00:09.040 --> 00:00:14.365
all the training examples are in the right category. Class.

00:00:14.365 --> 00:00:15.090
&gt;&gt; Right, so the the answer in

00:00:15.090 --> 00:00:18.490
the algorithm is when everything is classified correctly.

00:00:20.570 --> 00:00:23.840
That's a pretty good answer, Michael. But what if we have

00:00:23.840 --> 00:00:27.000
noise in our data? What if it's the case that we

00:00:27.000 --> 00:00:31.290
have two examples of the same object, the same instance, but

00:00:31.290 --> 00:00:33.620
they have two different labels? Then this will never be the case.

00:00:33.620 --> 00:00:38.720
&gt;&gt; Oh. So, then our algorithm goes into an infinite loop.

00:00:38.720 --> 00:00:40.480
&gt;&gt; Which seems like a bad idea.

00:00:40.480 --> 00:00:41.540
&gt;&gt; So we could have, we could, we could

00:00:41.540 --> 00:00:44.643
just say, or we've run out of attributes. [LAUGH]

00:00:44.643 --> 00:00:45.650
&gt;&gt; [LAUGH] Or we've run out

00:00:45.650 --> 00:00:46.970
of attributes. That's one way of doing

00:00:46.970 --> 00:00:48.450
it. In fact, that, that was, that's going to

00:00:48.450 --> 00:00:50.150
have to happen at some point, right?

00:00:50.150 --> 00:00:52.690
That's probably a slightly better answer. Although that

00:00:52.690 --> 00:00:55.610
doesn't help us in the case where we have continuous attributes and we might ask

00:00:55.610 --> 00:00:57.480
an infinite number of questions. So we probably

00:00:57.480 --> 00:01:00.880
need a slightly better criteria. Don't you think?

00:01:00.880 --> 00:01:01.950
&gt;&gt; Hm.

00:01:01.950 --> 00:01:05.860
&gt;&gt; So, what got us down this path, was thinking about what happens

00:01:05.860 --> 00:01:10.420
if we have noise. Why would we be worried about having noise anyway?

00:01:10.420 --> 00:01:11.110
&gt;&gt; Noise anyway.

00:01:11.110 --> 00:01:15.210
Well, I guess the training data might have gotten corrupted

00:01:15.210 --> 00:01:18.420
a little bit or maybe somebody copied something down wrong.

00:01:18.420 --> 00:01:21.470
&gt;&gt; Right, so since that's always a possibility,

00:01:21.470 --> 00:01:25.540
does it really make sense to trust the

00:01:25.540 --> 00:01:27.780
data completely, and go all the way to

00:01:27.780 --> 00:01:33.480
the point where we perfectly classify the training data?

00:01:33.480 --> 00:01:36.270
&gt;&gt; But Charles, if we can't trust our data, what can we trust?

00:01:36.270 --> 00:01:40.100
&gt;&gt; Well, we can trust our data, but we want to verify. [LAUGH]

00:01:40.100 --> 00:01:43.160
The whole point is generalization. And if it's possible for us to have

00:01:43.160 --> 00:01:46.260
a little bit of noise in the data, an error here or there,

00:01:46.260 --> 00:01:49.190
then we want to have some way to deal to handle that possibility, right?

00:01:49.190 --> 00:01:50.340
&gt;&gt; I guess so.

00:01:50.340 --> 00:01:52.060
&gt;&gt; So, what will we do?

00:01:52.060 --> 00:01:55.790
&gt;&gt; [LAUGH] I mean, we actually have a name for this, right? When you get

00:01:55.790 --> 00:01:58.750
really, really, really good at classifying your training

00:01:58.750 --> 00:02:01.270
data, but it doesn't help you to generalize,

00:02:01.270 --> 00:02:02.880
we have a name for that.

00:02:02.880 --> 00:02:05.910
&gt;&gt; Right. That sounds like overfitting.

00:02:05.910 --> 00:02:08.324
&gt;&gt; Exactly. We have to worry about overfitting.

00:02:09.410 --> 00:02:11.890
So you can overfit with the decision tree too?

00:02:11.890 --> 00:02:13.910
&gt;&gt; Yeah. What, you don't believe that?

00:02:13.910 --> 00:02:17.490
&gt;&gt; No, no, no. I was, I was being naive, I was being,

00:02:17.490 --> 00:02:19.685
I know that you can overfit a decision tree. [LAUGH] I was just.

00:02:19.685 --> 00:02:21.610
&gt;&gt; [LAUGH] Yeah but your [SOUND] is the [SOUND] that

00:02:21.610 --> 00:02:23.050
you use when you're, when you're like, I don't believe what

00:02:23.050 --> 00:02:24.500
you just said Charles, but I'm going to go along with

00:02:24.500 --> 00:02:26.370
it anyway, because I have to get off the phone soon.

00:02:26.370 --> 00:02:30.730
&gt;&gt; [LAUGH] Fair enough. I'll try to,

00:02:30.730 --> 00:02:32.770
I'll try to have a different personality then.

00:02:32.770 --> 00:02:35.380
&gt;&gt; [LAUGH] Okay, step one, have a

00:02:35.380 --> 00:02:39.010
different personality with maximal information gain. Okay, so

00:02:39.010 --> 00:02:42.550
we don't want to, we don't want to overfit. So we need to come up with some

00:02:42.550 --> 00:02:47.210
way of overfitting. Now the way you overfit in a decision tree is basically by

00:02:47.210 --> 00:02:48.700
having a tree that's too big, it's

00:02:48.700 --> 00:02:54.220
too complicated. All right. Violates Occam's Razor. So,

00:02:54.220 --> 00:02:58.980
what's a kind of, let's say, modification to something like ID3

00:02:58.980 --> 00:03:02.520
to our decision tree algorithm that will help us to avoid overfitting?

00:03:02.520 --> 00:03:04.260
&gt;&gt; Well last time we talked about overfitting,

00:03:04.260 --> 00:03:06.910
we said cross-validation was a good way of dealing

00:03:06.910 --> 00:03:09.400
with it, which, it allowed us to choose

00:03:09.400 --> 00:03:13.060
from among the different, say degrees of the polynomial.

00:03:13.060 --> 00:03:13.260
&gt;&gt; Right.

00:03:13.260 --> 00:03:15.870
&gt;&gt; So maybe we could do something like that?

00:03:15.870 --> 00:03:19.390
I don't know. Try all the different trees and,

00:03:19.390 --> 00:03:20.830
see which one has the lowest cross

00:03:20.830 --> 00:03:23.790
validation error? Maybe there's too many trees.

00:03:23.790 --> 00:03:25.580
&gt;&gt; Maybe, but that's a perfectly reasonable thing to do,

00:03:25.580 --> 00:03:27.610
right? You take out a validation set. You build a

00:03:27.610 --> 00:03:29.830
decision tree, and you test it on the, on the

00:03:29.830 --> 00:03:32.900
validation set and you pick whichever one has the lowest error

00:03:32.900 --> 00:03:35.440
in the validation sect, that's one way to avoid it.

00:03:35.440 --> 00:03:37.890
And then you have, don't have to worry about this question

00:03:37.890 --> 00:03:41.000
about stopping, you just grow the tree on the training

00:03:41.000 --> 00:03:44.980
set minus the validation set until it does well on that.

00:03:44.980 --> 00:03:46.776
And you check it against the crossvalid, you check it against

00:03:46.776 --> 00:03:50.092
the validation set, and you pick the best one. That's one way

00:03:50.092 --> 00:03:53.290
of doing it, and that would work perfectly fine. There is

00:03:53.290 --> 00:03:57.170
another way you can do it that's more efficient. Which is, you

00:03:57.170 --> 00:04:00.980
do the same idea validation, except that you hold out a

00:04:00.980 --> 00:04:05.110
set and as you, everytime you decide whether to expand the tree

00:04:05.110 --> 00:04:07.570
or not, you check to see how this would do so

00:04:07.570 --> 00:04:10.620
far in the validation set. And if the error is low enough,

00:04:10.620 --> 00:04:14.020
then you stop expanding the tree. That's one way of doing it.

00:04:14.020 --> 00:04:16.399
&gt;&gt; So is there, is there a problem in terms of, I mean if

00:04:16.399 --> 00:04:21.360
we're expanding the tree depth for search wise, we could be at, you know,

00:04:21.360 --> 00:04:23.710
we could be looking at one tiny little split on one side of the

00:04:23.710 --> 00:04:26.530
tree before we even look at any, anything on the other side of the tree.

00:04:26.530 --> 00:04:29.230
&gt;&gt; That's a fine point. So how would you fix that?

00:04:29.230 --> 00:04:30.870
&gt;&gt; Maybe expand breadth first?

00:04:30.870 --> 00:04:35.610
&gt;&gt; Yeah, that would probably do it. Anything else you could think of? Well,

00:04:35.610 --> 00:04:38.059
so, you could do pruning, right? You could go ahead and

00:04:38.059 --> 00:04:40.399
do the tree as if you didn't have to worry about

00:04:40.399 --> 00:04:43.699
over-fitting, and once you have the full tree built, you could

00:04:43.699 --> 00:04:46.325
then do a kind of, you could do pruning. You could

00:04:46.325 --> 00:04:48.845
go to the leaves of the tree and say, well, what

00:04:48.845 --> 00:04:51.814
if I collapse these leaves back up into the tree? How

00:04:51.814 --> 00:04:56.850
does that create error on my validation set? And if the

00:04:56.850 --> 00:05:00.660
error is too big, then you don't do it. And if it's

00:05:00.660 --> 00:05:02.150
very small, then you go ahead and do

00:05:02.150 --> 00:05:04.460
it. And that should help you with overfitting.

00:05:04.460 --> 00:05:06.800
So, that whole class of ways of doing

00:05:06.800 --> 00:05:09.880
it, is called pruning. And there's a whole bunch

00:05:09.880 --> 00:05:12.270
of different ways you might prune. But pruning,

00:05:12.270 --> 00:05:14.140
itself, is one way of dealing with overfitting,

00:05:14.140 --> 00:05:16.050
and giving you a smaller tree. And it's

00:05:16.050 --> 00:05:18.690
a very simple addition to the standard ID3 algorithm.

