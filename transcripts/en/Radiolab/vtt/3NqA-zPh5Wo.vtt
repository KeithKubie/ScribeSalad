WEBVTT

1
00:00:00.600 --> 00:00:03.480
I'm Jad. I am Robert. Um, are you guys ready to do this?

2
00:00:03.510 --> 00:00:05.840
Maybe we should just do this. This is Radiolab. All right.

3
00:00:05.940 --> 00:00:08.310
But when your host come out, I need you to seriously clap.

4
00:00:08.311 --> 00:00:11.700
Like you've never seen two dudes with glasses talking on microphone. Okay.

5
00:00:11.730 --> 00:00:15.300
So like, just really, really give it up for your mostly human hosts.

6
00:00:15.320 --> 00:00:16.830
Jad Abumrad and Robert Krulwich

7
00:00:21.060 --> 00:00:21.130
<v 1>[inaudible].</v>

8
00:00:21.130 --> 00:00:26.130
<v 2>So about a week ago we gathered I guess roughly a hundred people into a</v>

9
00:00:27.731 --> 00:00:30.360
performance space, which is in our building here at wn sea.

10
00:00:30.370 --> 00:00:32.870
It's called the green space. This is like, yeah, just like a,

11
00:00:32.871 --> 00:00:34.930
like a playground for us so we can just try things.

12
00:00:35.080 --> 00:00:36.050
We decided to gather all these [inaudible]

13
00:00:36.250 --> 00:00:39.620
<v 3>well into a room on a random Monday night. What else are you doing on a Monday?</v>

14
00:00:39.650 --> 00:00:40.070
Right?

15
00:00:40.070 --> 00:00:44.450
Because seven years previous we had made a show called talking to machines,

16
00:00:44.810 --> 00:00:47.780
which was all about like what happens when you talk to a computer that's

17
00:00:47.781 --> 00:00:49.850
pretending to be human, right? And the thing is,

18
00:00:49.851 --> 00:00:52.250
so much has happened since we made that show.

19
00:00:52.251 --> 00:00:55.790
With the proliferation of bots on Twitter,

20
00:00:55.791 --> 00:00:59.150
Russian bots meddling in elections, the advances in AI.

21
00:00:59.300 --> 00:01:03.860
So much interesting stuff had happened that we thought it is time to update that

22
00:01:03.861 --> 00:01:04.200
show

23
00:01:04.200 --> 00:01:07.740
<v 2>and we needed to do it live. We thought because we had a little plan in mind,</v>

24
00:01:07.770 --> 00:01:12.300
we wanted to put unsuspecting people into a room for a kind of showdown

25
00:01:15.000 --> 00:01:16.290
between people and machines.

26
00:01:16.350 --> 00:01:20.130
But we want to set the scene a little bit and give you a us just a flavor of

27
00:01:20.131 --> 00:01:22.770
what we're really going to. Just to start things off, we brought to our stage,

28
00:01:22.771 --> 00:01:25.020
one of the guys who inspired that original show.

29
00:01:25.080 --> 00:01:27.840
Please welcome to the stage writer Brian Christian

30
00:01:33.080 --> 00:01:33.710
<v 1>[inaudible] well,</v>

31
00:01:33.710 --> 00:01:37.520
<v 0>so just so we can just just get things sort of oriented. Oh, we need to,</v>

32
00:01:37.521 --> 00:01:41.360
first of all just redefine what a chat bot is, right?

33
00:01:41.690 --> 00:01:45.290
So a chat Bot is a computer program, uh,

34
00:01:45.291 --> 00:01:49.070
that exists to mimic and impersonate human beings.

35
00:01:49.190 --> 00:01:51.560
Like when do I run into them?

36
00:01:51.740 --> 00:01:54.590
You go to a website to interact with some customer service.

37
00:01:54.591 --> 00:01:57.320
You might find yourself talking to a chat Bot. Um,

38
00:01:57.350 --> 00:02:01.530
the U s army has a chat bot called sergeant star that recruits people who,

39
00:02:01.531 --> 00:02:03.420
can I ask you a question about, about the, the set,

40
00:02:03.421 --> 00:02:06.390
the thing you just said about chatting with customer service? Yeah,

41
00:02:06.420 --> 00:02:10.550
I end up doing a lot. Um, I'm sorry.

42
00:02:11.080 --> 00:02:13.940
We says I, uh, you know, like it's in the middle of the night,

43
00:02:13.941 --> 00:02:17.060
you're trying to figure out some program and it's not working and then suddenly

44
00:02:17.061 --> 00:02:21.290
there's like need to chat and you click on that. Suddenly there's need to chat.

45
00:02:21.320 --> 00:02:23.840
Well, it's like you, your whatever.

46
00:02:24.650 --> 00:02:26.840
I assume many of you have had this experience,

47
00:02:27.360 --> 00:02:31.370
a few of the experiences that he's had. So there's just that issue always.

48
00:02:31.430 --> 00:02:33.680
I'm always curious. It, it what,

49
00:02:33.980 --> 00:02:38.530
it seems very human when you're having that conversation with, with uh, uh,

50
00:02:38.540 --> 00:02:42.500
customer service Chat Bot. Is there a, is there a place where it,

51
00:02:43.200 --> 00:02:46.580
where is the line between human and robots in that they're both present? Yeah.

52
00:02:46.581 --> 00:02:47.930
Well, this, this is the question, right?

53
00:02:47.960 --> 00:02:52.960
So we're now sort of accustomed to having this uncanny feeling of not really

54
00:02:53.121 --> 00:02:53.960
knowing the difference.

55
00:02:53.990 --> 00:02:58.550
My guess for what it's worth is that there's a system on the back end that's

56
00:02:58.551 --> 00:03:03.400
designed, sort of do triage where the first few exchanges that are just like,

57
00:03:03.670 --> 00:03:05.740
hey, how can I help what's going on?

58
00:03:05.741 --> 00:03:08.590
It seems like there's an issue with this such and such. Um,

59
00:03:08.620 --> 00:03:13.450
that is basically just a chat bot and at a certain point you kind of seamlessly

60
00:03:13.451 --> 00:03:17.800
transition and are handed off to a real person. But without any,

61
00:03:18.360 --> 00:03:20.770
you know, notification to you that this has happened.

62
00:03:20.800 --> 00:03:24.880
It's deliberately left opaque at what point that happens. Wow.

63
00:03:25.060 --> 00:03:27.700
This is just literally everywhere it is. I mean,

64
00:03:27.760 --> 00:03:32.760
and you can't get on social media and read some comment thread without someone

65
00:03:32.831 --> 00:03:37.810
accusing someone else of being a Bot. Um, and you know, it seems,

66
00:03:38.890 --> 00:03:41.590
uh, it seems maybe sort of trivial at some level,

67
00:03:41.591 --> 00:03:46.591
but we are now living through this political crisis of how do we kind of come to

68
00:03:46.931 --> 00:03:50.320
terms with the idea that we can, you know,

69
00:03:50.321 --> 00:03:55.321
weaponize this kind of speech and how do we as consumers of the news or as users

70
00:03:55.661 --> 00:04:00.661
of social media try to suss out whether the people we're interacting with are in

71
00:04:01.601 --> 00:04:02.450
fact who they say they are?

72
00:04:02.610 --> 00:04:05.940
<v 2>And all this confusion about what's the machine and who's the human.</v>

73
00:04:05.941 --> 00:04:10.830
It can get very interesting in the context of a famous thought experiment named

74
00:04:10.831 --> 00:04:14.580
for a great mathematician named Alan Turing. Brian told us about this.

75
00:04:14.581 --> 00:04:16.320
It's called the Turing test.

76
00:04:16.550 --> 00:04:17.390
<v 0>Alan Turing,</v>

77
00:04:17.420 --> 00:04:21.500
he makes this famous prediction back in 1950 that we'll eventually get to a

78
00:04:21.501 --> 00:04:26.360
point sometime around the beginning of this century where we'll stop being able

79
00:04:26.361 --> 00:04:29.780
to tell the difference. Well, what specifically was his sort of prophesy?

80
00:04:30.020 --> 00:04:33.680
His specific prophecy was that by the year 2000, uh,

81
00:04:33.710 --> 00:04:38.710
after five minutes of interacting by text message with a human on one hand and a

82
00:04:39.291 --> 00:04:41.090
chat bot on the other hand, uh,

83
00:04:41.120 --> 00:04:46.120
30% of judges would fail to tell which was the human in which was the robot is

84
00:04:48.021 --> 00:04:51.950
30, just like a certain kind of what Turing imagined.

85
00:04:51.951 --> 00:04:56.270
And he predicted that as a result of hitting this 30% threshold,

86
00:04:56.450 --> 00:05:00.650
we would reach a point he writes where one would speak of machines as being

87
00:05:00.651 --> 00:05:03.310
intelligent without expecting to be contradicted. Um,

88
00:05:03.330 --> 00:05:06.160
and this just existed as kind of a part of the,

89
00:05:06.180 --> 00:05:11.180
the philosophy of computer science until the early 1990s when into the story

90
00:05:12.891 --> 00:05:13.724
steps.

91
00:05:14.060 --> 00:05:19.060
Hugh Lominger a rogue multimillionaire disco dance floor salesman.

92
00:05:20.390 --> 00:05:23.960
Oh, what a rogue millionaire.

93
00:05:23.990 --> 00:05:27.200
Plastic portable light-up disco dance floor salesman.

94
00:05:28.220 --> 00:05:32.510
The beaches kind of, yeah, the lighting that lights up,

95
00:05:33.110 --> 00:05:37.280
but portable, portable, you can make it, you can pick them,

96
00:05:37.520 --> 00:05:38.360
wrote millionaire from that.

97
00:05:38.690 --> 00:05:43.690
There's apparently millions to be made if if only if only you knew m and m to

98
00:05:45.441 --> 00:05:47.870
Logan or this eccentric millionaire, uh,

99
00:05:47.900 --> 00:05:52.900
decides that w this was in about 1992 that the technology was starting to get to

100
00:05:54.351 --> 00:05:58.790
the point where it would be worth not talking about the turn test as this

101
00:05:58.791 --> 00:05:59.481
thought experiment,

102
00:05:59.481 --> 00:06:04.481
but actually convening a group of people in a room once a year to actually run

103
00:06:05.750 --> 00:06:06.583
the test.

104
00:06:07.730 --> 00:06:11.180
<v 3>Now a bit of background during the lobe, NERC competitions, the actual lobe,</v>

105
00:06:11.300 --> 00:06:15.200
no competitions. How it usually works is that you've got some participants.

106
00:06:15.230 --> 00:06:17.180
These are the people who have to decide what's going on.

107
00:06:17.181 --> 00:06:21.710
They sit at computers and they stare at the computer and they chat with someone

108
00:06:21.711 --> 00:06:23.950
on a screen and they don't know the,

109
00:06:23.980 --> 00:06:27.920
someone they're chatting with is a person or a Bot behind a curtain.

110
00:06:28.490 --> 00:06:33.490
You have that Bot the computer running the Bot and you also have some humans who

111
00:06:33.981 --> 00:06:37.640
the participants may or may not be chatting with. They've got to decide, right?

112
00:06:37.760 --> 00:06:40.490
Are they chatting with the person or machine? Now, Brian, many,

113
00:06:40.491 --> 00:06:43.160
many years ago actually participated in this competition.

114
00:06:43.161 --> 00:06:47.030
He was one of the humans behind the curtain that was chatting with the

115
00:06:47.031 --> 00:06:51.000
participants. And when we talked to him initially, many years ago, uh,

116
00:06:51.060 --> 00:06:52.430
for the talking to machine show,

117
00:06:52.820 --> 00:06:57.230
we went into all the strategies that the computer program is we're using that

118
00:06:57.230 --> 00:06:59.000
year to try and fool the participants.

119
00:06:59.001 --> 00:07:02.480
But the takeaway was that the year that he did it,

120
00:07:03.020 --> 00:07:06.800
the computers flopped. By and large, the participants were not fooled.

121
00:07:06.801 --> 00:07:09.830
They knew exactly when they were talking to a human and when they were talking

122
00:07:09.831 --> 00:07:11.480
to a machine. Now that was a while ago

123
00:07:12.010 --> 00:07:15.640
<v 4>in the green space we asked Brian, where do things stand now?</v>

124
00:07:15.730 --> 00:07:18.700
Has it like when we last talked to you, what, what, when did we last?

125
00:07:18.701 --> 00:07:23.380
When was it, 201120112011 how has it have we passed

126
00:07:23.380 --> 00:07:26.440
<v 0>the 30% thresholds in the intervening eight years?</v>

127
00:07:26.470 --> 00:07:31.450
So in 2014 there was a turning test competition that was held at which the top

128
00:07:31.451 --> 00:07:34.860
computer program managed to fool 30% of the judges. Wow.

129
00:07:36.280 --> 00:07:39.640
So that's it, right. Depending on how you want to interpret that result,

130
00:07:39.760 --> 00:07:44.080
that controversy arose in this particular year because the chat Bot,

131
00:07:44.081 --> 00:07:49.081
that one was claiming to be a 13 year old Ukrainian who is just beginning to get

132
00:07:50.141 --> 00:07:54.340
a grasp on the English language. Also, the machine was cheating. Right.

133
00:07:54.370 --> 00:07:58.840
That's interesting. So it masked its computer notice by in broken grammar.

134
00:07:59.430 --> 00:08:03.070
Yeah, exactly right. Or, or if it didn't appear to understand your question,

135
00:08:03.190 --> 00:08:06.820
you started to have this story you could play in your own mind of like,

136
00:08:06.821 --> 00:08:10.510
oh well maybe I didn't phrase that quite right or something. Has it been broke?

137
00:08:10.511 --> 00:08:13.560
Has the, has it, the admin a second winner or a third winner or fourth winner?

138
00:08:14.160 --> 00:08:15.520
Um, to the best of my knowledge,

139
00:08:15.521 --> 00:08:18.920
we are still sort of flirting under that threshold. Listens.

140
00:08:18.970 --> 00:08:23.570
We haven't had any victories since 2014 we thought we might just do this right

141
00:08:23.630 --> 00:08:23.980
here, right here.

142
00:08:23.980 --> 00:08:28.890
<v 4>No, no, no. Just look right here in this room. And do our little touring guests.</v>

143
00:08:29.260 --> 00:08:30.880
Okay. Unbeknownst to our audience,

144
00:08:30.881 --> 00:08:35.440
we had actually lined up a chat bot from a company called Pandora bots that had

145
00:08:35.470 --> 00:08:39.700
almost passed the Turing test. It had fooled roughly not quite,

146
00:08:39.701 --> 00:08:43.030
but almost 25% of the participants.

147
00:08:43.120 --> 00:08:47.920
We got the latest version of the spot and one person. Anyone in the room, um,

148
00:08:48.070 --> 00:08:49.570
your, your job will be.

149
00:08:49.571 --> 00:08:52.810
We decided to run some tests with the audience starting with just one person.

150
00:08:52.900 --> 00:08:56.070
I can see one hand over the actual slate and I don't want to get the

151
00:08:56.270 --> 00:09:00.420
<v 2>first hand, I guess. What about this person over here on the left? Okay,</v>

152
00:09:00.850 --> 00:09:02.740
<v 5>so we brought up this a young woman on stage,</v>

153
00:09:02.770 --> 00:09:05.650
put her at a computer and we told her she would be chatting with two different

154
00:09:05.651 --> 00:09:10.180
entities. One would be this Bot Pandora Bot and the other would be me,

155
00:09:10.610 --> 00:09:11.050
but I was,

156
00:09:11.050 --> 00:09:14.380
I went off stage and sat at a computer in the dark when no one could see.

157
00:09:14.700 --> 00:09:19.070
She was going to chat with both of us and not know who was who, who is machine.

158
00:09:19.170 --> 00:09:19.340
Right.

159
00:09:19.340 --> 00:09:23.860
<v 2>Who is human, you won't know which I get as many questions as I,</v>

160
00:09:24.280 --> 00:09:26.440
well I don't, no, no. We're going to give you a time limit.

161
00:09:26.441 --> 00:09:27.251
You can't be here though. I'll leave.

162
00:09:27.251 --> 00:09:30.730
So after Jedd left the stage and went back into that room up on the screen,

163
00:09:30.731 --> 00:09:35.530
came to different chat dialogue boxes. You'll see that we have two options.

164
00:09:35.531 --> 00:09:38.590
We've just labeled them for one reason by color when the strawberry,

165
00:09:38.591 --> 00:09:41.860
the other is blueberry or code red and code blue.

166
00:09:42.070 --> 00:09:43.840
You think you can talk to both of them at the same time?

167
00:09:43.841 --> 00:09:45.160
Just jump from one to the other. Sure.

168
00:09:45.580 --> 00:09:49.100
We've got any sort of thoughts on how you could suss out whether the thing was a

169
00:09:49.101 --> 00:09:51.400
person or a thing? Yeah, I have some thought.

170
00:09:51.410 --> 00:09:56.360
I mean like my first tactics are going to be like sort of like very human

171
00:09:56.361 --> 00:09:58.730
emotional questions and then we'll like go from there.

172
00:09:58.760 --> 00:10:01.370
See what [inaudible] well,

173
00:10:01.600 --> 00:10:04.580
I'm not going to ask cause I don't wanna I don't wanna lose your inspiration to

174
00:10:04.581 --> 00:10:09.390
therapize this robot. [inaudible] so when I say go,

175
00:10:09.510 --> 00:10:13.620
you'll just go and I'll just narrate what you're doing. Okay. Okay. Okay. Three,

176
00:10:14.010 --> 00:10:16.500
two one begin.

177
00:10:18.090 --> 00:10:21.990
So she started to type and first thing she did was she said hello to strawberry.

178
00:10:22.020 --> 00:10:22.320
Okay,

179
00:10:22.320 --> 00:10:23.680
<v 6>so you've gotten the first,</v>

180
00:10:25.650 --> 00:10:27.700
<v 2>well, we've got a somewhat sexual response.</v>

181
00:10:28.010 --> 00:10:28.843
<v 6>Yeah,</v>

182
00:10:29.560 --> 00:10:30.221
<v 2>the machine.</v>

183
00:10:30.221 --> 00:10:34.180
I said I like strawberries and then you returned with strawberries are delicious

184
00:10:34.181 --> 00:10:38.710
and Oh, now it's getting warmer over there. Blue is a warmer, as a cooler color.

185
00:10:38.711 --> 00:10:42.580
Maybe you'd like to go and discuss Aristotle with the Luberon.

186
00:10:43.120 --> 00:10:45.280
Then she switched over and started to text the,

187
00:10:45.310 --> 00:10:49.720
the blue one is blueberry or the although is high blue, high blue Zb. Okay.

188
00:10:49.750 --> 00:10:54.750
That's also a kind of a generous sort of opener. Yes. Hi, bluesy beans.

189
00:10:54.870 --> 00:10:58.690
The nickname? Yeah. Okay. Let's say, and blueberry rose behind there.

190
00:10:58.691 --> 00:11:02.950
I just realized, I don't even know who I'm talking to. What is your name?

191
00:11:03.390 --> 00:11:06.010
You're going to answer? Zandra am I not in your phone?

192
00:11:11.970 --> 00:11:12.070
<v 6>[inaudible]</v>

193
00:11:12.070 --> 00:11:17.020
<v 2>the blueberry has responded with a, a bit of shock. Strawberry.</v>

194
00:11:17.050 --> 00:11:20.320
My moms here was red.

195
00:11:20.830 --> 00:11:23.950
Well and blueberry. What's wrong, Boone? Nothing's wrong with me.

196
00:11:24.460 --> 00:11:27.550
Is there something wrong with them in back and forth and back and forth and

197
00:11:27.551 --> 00:11:31.330
blueberry and I have a lot going on. I remember one of these,

198
00:11:31.390 --> 00:11:34.870
she doesn't know which is Jack, right on the strawberry side.

199
00:11:34.900 --> 00:11:38.770
I cannot believe him right now. You don't believe right now? As far as I know,

200
00:11:38.771 --> 00:11:41.820
not unless you have x-ray vision. I'm in the room next to you. Oh,

201
00:11:42.100 --> 00:11:46.240
he's trying to coax you into thinking that he's Jan is.

202
00:11:46.400 --> 00:11:48.970
That's blueberry. Is that something they do?

203
00:11:50.000 --> 00:11:51.360
<v 6>No, I didn't. [inaudible]</v>

204
00:11:51.820 --> 00:11:54.460
<v 2>yeah, you're at the heart of the question going to ask you to bring this to a</v>

205
00:11:54.461 --> 00:11:57.760
conclusion. After a couple minutes of this, we asked the audience,

206
00:11:58.000 --> 00:12:00.670
you have strawberry on one side and you look at blueberry on the other.

207
00:12:00.671 --> 00:12:04.060
Which one do you think is Jad and which one do you think was the,

208
00:12:04.350 --> 00:12:08.530
where's the Bot? How many of you think that Jad is blueberry?

209
00:12:10.420 --> 00:12:13.690
A few of you. 13 hands went up, something like that.

210
00:12:13.691 --> 00:12:18.070
How many of you think the Jad is strawberry? Almost everybody overwhelming,

211
00:12:18.910 --> 00:12:22.000
but interestingly, I volunteer on stage, went against the room.

212
00:12:22.240 --> 00:12:27.110
Gee Thought Chad was blueberries. Is the robot. Is that what we all agreed? No.

213
00:12:27.730 --> 00:12:31.930
Oh, you're a, you're against the crowd here. Okay. Interesting.

214
00:12:33.010 --> 00:12:37.990
Much better theater. Alright. Jad Abumrad where w which one are you? Oh,

215
00:12:38.250 --> 00:12:41.860
jade comes out from his hiding place and he tells the crowd. In fact he is

216
00:12:42.340 --> 00:12:46.860
<v 6>strawberry. All right. [inaudible]</v>

217
00:12:46.950 --> 00:12:47.880
<v 5>the crowd was right.</v>

218
00:12:47.940 --> 00:12:51.090
I've definitely never had that much chemistry with something that was human,

219
00:12:51.600 --> 00:12:53.400
but our volunteer on stage.

220
00:12:53.600 --> 00:12:54.480
<v 6>Got It. Alright.</v>

221
00:12:58.830 --> 00:13:00.330
[inaudible] we're going to give you

222
00:13:00.610 --> 00:13:02.200
<v 2>now it seemed that maybe we,</v>

223
00:13:02.240 --> 00:13:07.240
we could trust democracy a little bit more and believe that if the most of the

224
00:13:07.811 --> 00:13:11.290
people in the room went one way, that's something that would be, you know,

225
00:13:11.320 --> 00:13:12.940
that would be important to find out.

226
00:13:12.941 --> 00:13:17.020
So we decided to do the entire thing over again for everybody in the room.

227
00:13:17.440 --> 00:13:17.441
<v 5>Yeah.</v>

228
00:13:17.441 --> 00:13:21.800
So what we did was we handed out I think 17 different cell phone numbers evenly

229
00:13:21.801 --> 00:13:25.820
through the crowd. Yes. Look at the number that is on your envelope.

230
00:13:25.910 --> 00:13:30.200
Only you roughly half of those numbers were texting to a machine.

231
00:13:30.201 --> 00:13:33.350
Half were texting with a group of humans that were our staff.

232
00:13:33.351 --> 00:13:37.220
The crowd did not know which was which. Exactly. So here we go, get ready,

233
00:13:37.550 --> 00:13:41.420
get set. And you go. Okay.

234
00:13:41.421 --> 00:13:46.421
So the crowd of about a hundred people or so had two minutes ish to text with

235
00:13:46.701 --> 00:13:48.260
the person or thing on the other end.

236
00:13:48.800 --> 00:13:51.530
And we're going to skip over this part cause it's mostly quiet.

237
00:13:51.531 --> 00:13:55.970
People just looking down at their phones, concentrating mightily. But at the end

238
00:13:57.950 --> 00:13:58.850
we asked everyone to vote,

239
00:13:58.851 --> 00:14:00.610
where are they texting with a person or a [inaudible].

240
00:14:00.970 --> 00:14:03.080
<v 2>And then we asked the ones who had been tricked,</v>

241
00:14:03.081 --> 00:14:05.930
who turned out to have guessed wrong, please stand up. Okay,

242
00:14:05.931 --> 00:14:07.950
so we're now looking, I believe in this,

243
00:14:08.160 --> 00:14:12.020
I'm talking about we are now looking at the upright citizens in this room are

244
00:14:12.050 --> 00:14:15.740
the wrong Guytes. You have the seated people are the right type. Correct.

245
00:14:16.040 --> 00:14:18.510
So that means that roughly God,

246
00:14:18.810 --> 00:14:22.160
I think like 45% of the people were wrong.

247
00:14:22.161 --> 00:14:23.890
Meaning that we just passed it.

248
00:14:24.100 --> 00:14:26.440
<v 6>Well I think that's it.</v>

249
00:14:26.810 --> 00:14:28.270
<v 5>He did it. It was a strange moment.</v>

250
00:14:28.330 --> 00:14:31.480
We were all clapping at our own demise because you know,

251
00:14:31.481 --> 00:14:35.080
touring had laid down this number of 30% and the BOT had fooled way more people

252
00:14:35.081 --> 00:14:38.320
than that. Um, I'm just now going to ask you,

253
00:14:38.321 --> 00:14:40.660
having been a veteran of this that we should just qualify.

254
00:14:40.690 --> 00:14:44.410
This was him really unscientific, super sloppy experiment.

255
00:14:44.440 --> 00:14:46.900
But on the other hand, and we talked to Brian about this when he was over,

256
00:14:46.960 --> 00:14:51.890
it really does suggest something that maybe what's changed is so much due to the

257
00:14:51.891 --> 00:14:52.490
machines

258
00:14:52.490 --> 00:14:57.440
<v 2>becoming more and more articulate. It's more like us, the way we,</v>

259
00:14:57.441 --> 00:14:59.350
when I talk to one another these days,

260
00:14:59.550 --> 00:15:04.210
<v 0>gone from interacting in person to talking over the phone to emailing,</v>

261
00:15:04.330 --> 00:15:06.160
to texting. And now,

262
00:15:06.190 --> 00:15:11.190
I mean for me the great irony is that even to text your phone is proactively

263
00:15:11.591 --> 00:15:15.130
suggesting turns of phrase that it thinks you might want to use.

264
00:15:15.970 --> 00:15:17.200
And so, I mean, I,

265
00:15:17.310 --> 00:15:21.220
I assume many people in this room have had the experience of trying to text

266
00:15:21.221 --> 00:15:23.650
something and you try to say it in a, like a sort of a fun,

267
00:15:23.651 --> 00:15:26.620
fanciful way or you've tried to make some fun or you use the word,

268
00:15:26.621 --> 00:15:31.000
it's not a real word and you're front just sort of slaps slaps that down and

269
00:15:31.001 --> 00:15:33.370
just replaces it with something more normal,

270
00:15:33.550 --> 00:15:37.900
which make it really hard to use words that aren't the normal words.

271
00:15:37.901 --> 00:15:40.660
And so you just stop using those words and you just use the words,

272
00:15:40.661 --> 00:15:44.170
the computer they make you use. Exactly. So in a, in a sense,

273
00:15:44.171 --> 00:15:48.820
what seems to be happening is that our human communication is becoming more

274
00:15:49.430 --> 00:15:50.263
<v 2>machine like,</v>

275
00:15:51.200 --> 00:15:53.740
<v 0>but at the moment it seems like the touring test is getting passed,</v>

276
00:15:53.770 --> 00:15:57.460
not because the machines have met us at our full potential,

277
00:15:57.490 --> 00:16:02.490
but because we are using every more and more kind of degraded sort of rote forms

278
00:16:04.661 --> 00:16:06.340
of communication with one another.

279
00:16:09.280 --> 00:16:12.970
<v 7>It feels like a slow slide. Downey hill or something. Yes.</v>

280
00:16:13.450 --> 00:16:17.680
Down that hill towards the inevitability that we may one day be there. Pets.

281
00:16:17.770 --> 00:16:21.580
I don't, I don't like the way this is going no matter who's in, who's doing it.

282
00:16:21.581 --> 00:16:23.440
But, uh, in the next segment we're gonna,

283
00:16:23.470 --> 00:16:26.530
we're going to flip things a little bit and ask, you know,

284
00:16:26.531 --> 00:16:31.240
could the coming age and machines actually make us humans more human?

285
00:16:32.080 --> 00:16:34.060
So human should please stick around.

286
00:16:46.890 --> 00:16:50.280
<v 2>This is h j school on Tay calling from Chicago, Illinois.</v>

287
00:16:50.281 --> 00:16:55.110
A Radiolab is supported in part by the offered Peace Sloan Foundation,

288
00:16:55.350 --> 00:16:59.400
enhancing public understanding of science and technology in the modern world.

289
00:16:59.670 --> 00:17:04.560
More information aboutSloan@wwwdotsloan.org

290
00:17:07.960 --> 00:17:08.990
<v 1>[inaudible] on the Eiffel Tower.</v>

291
00:17:15.230 --> 00:17:15.400
[inaudible]

292
00:17:15.400 --> 00:17:19.250
<v 8>will you NYC studios presents a special director's cut of the orbiting human</v>

293
00:17:19.251 --> 00:17:20.084
circus.

294
00:17:21.020 --> 00:17:24.620
We invite you to try a new and exciting form of audio storytelling.

295
00:17:24.950 --> 00:17:27.500
The Guardian call it addictive hypnotic,

296
00:17:27.690 --> 00:17:31.160
one of the best podcast of the year dying John Cameron Mitchell,

297
00:17:32.120 --> 00:17:35.330
Julian Koster and Tim Robbins and featuring Mandy Patinkin and many more.

298
00:17:35.820 --> 00:17:38.900
Listen and subscribe to the orbiting human circus.

299
00:17:39.240 --> 00:17:40.880
However you get your podcasts.

300
00:17:45.750 --> 00:17:49.080
<v 0>Hey, I'm Jad. I am Robert. This is Radiolab. We're back in the last segment.</v>

301
00:17:49.590 --> 00:17:50.100
We gathered

302
00:17:50.100 --> 00:17:55.020
<v 9>a bunch of people in the performance space here at w NYC and we conducted a</v>

303
00:17:55.080 --> 00:17:59.370
unscientific version of the Turing test, and in our case, the Bot one,

304
00:17:59.460 --> 00:18:02.190
it fooled more than 30% of the people in the room.

305
00:18:02.220 --> 00:18:05.640
Now we should point out that the woman who headed up the design of the winning

306
00:18:05.910 --> 00:18:06.990
Bot, her name is Lauren Koonzy,

307
00:18:07.020 --> 00:18:12.020
she works for a company called Pandorabots and she was actually in the room

308
00:18:12.280 --> 00:18:14.100
right there. Selena chair in the audience.

309
00:18:19.300 --> 00:18:20.830
<v 6>[inaudible] heading Lauren. That's Lauren.</v>

310
00:18:21.640 --> 00:18:24.760
<v 9>And it's interesting that one of the things that Lauren mentioned is that the</v>

311
00:18:25.060 --> 00:18:30.060
Bot that she designed seems to bring out rather consistently a certain side of

312
00:18:30.341 --> 00:18:33.550
people when they chat with it. It's a sound fast.

313
00:18:33.551 --> 00:18:38.230
So this Bot over 20% of people who talk to her and millions of conversations

314
00:18:38.231 --> 00:18:41.440
every week actually make romantic overtures.

315
00:18:41.800 --> 00:18:44.800
And that's pretty consistent across all of the bots on our platform.

316
00:18:44.801 --> 00:18:49.690
So there's something wrong with us, not the robot or right. You know. All right,

317
00:18:50.830 --> 00:18:51.071
Lauren,

318
00:18:51.071 --> 00:18:54.490
which brings up an actually a different kind of question like just for a second,

319
00:18:54.670 --> 00:18:58.810
let's forget whether we're being fooled into thinking about is actually a human,

320
00:18:59.140 --> 00:19:00.640
maybe the more important question.

321
00:19:00.641 --> 00:19:03.910
Given this increasing presence of all these machines in our lives,

322
00:19:04.150 --> 00:19:06.170
just like how do they make us behave? Yeah,

323
00:19:06.330 --> 00:19:10.780
and we dipped our toe into this world in a touring testy sort of way in that

324
00:19:10.781 --> 00:19:12.040
original show seven years ago.

325
00:19:12.041 --> 00:19:17.041
I want to play you an expert now to set up what comes after this is freedom

326
00:19:18.670 --> 00:19:21.310
bear. Yes, it is. There's not a machine. I don't think so.

327
00:19:21.510 --> 00:19:22.810
They're nice to meet both of you.

328
00:19:22.900 --> 00:19:27.010
This is an idea that we borrowed from a woman named Freedom Baird who is now a

329
00:19:27.011 --> 00:19:27.611
visual artist,

330
00:19:27.611 --> 00:19:31.150
but at the time she was a grad student at MIT doing some research and she was

331
00:19:31.151 --> 00:19:34.370
also the proud owner of a Furby. Yeah,

332
00:19:34.390 --> 00:19:37.810
I've got it right here and you're not going against the mic so we can hear it.

333
00:19:37.811 --> 00:19:42.700
Say hello to [inaudible].

334
00:19:43.200 --> 00:19:48.200
You describe a Furby for those of us who it's about five inches tall and the

335
00:19:49.481 --> 00:19:52.510
Furby has pretty much all head. It's just a big ground,

336
00:19:52.511 --> 00:19:55.030
fluffy head with two little feet sticking out the front.

337
00:19:55.150 --> 00:19:58.150
It has big eyes apparently it makes noises.

338
00:19:59.320 --> 00:20:02.240
You tickle it's tummy, it will cool.

339
00:20:03.460 --> 00:20:05.190
It would say me,

340
00:20:07.510 --> 00:20:09.340
it would want you to just keep playing with it.

341
00:20:10.360 --> 00:20:14.410
So one day she's hanging out with her for a beat and she notices something

342
00:20:15.550 --> 00:20:16.383
eerie.

343
00:20:19.970 --> 00:20:22.910
What I discovered is if you hold it upside down,

344
00:20:23.960 --> 00:20:26.840
it will say nice,

345
00:20:26.920 --> 00:20:27.753
<v 10>scared,</v>

346
00:20:32.010 --> 00:20:36.480
[inaudible], scared, scared, and me as the, you know,

347
00:20:36.490 --> 00:20:39.970
the sort of owner slash user of this Furby would get really uncomfortable with

348
00:20:39.971 --> 00:20:41.860
that and then turn it back up upright.

349
00:20:42.400 --> 00:20:45.640
<v 9>Once you up have an upright, it's, it's fine. It goes right back. It's fine.</v>

350
00:20:45.641 --> 00:20:50.080
So it's got some sensor in that knows you know what direction it's facing.

351
00:20:50.400 --> 00:20:51.610
Maybe it's just scared.

352
00:20:55.850 --> 00:20:56.710
She thought, well wait a second.

353
00:20:56.711 --> 00:21:01.420
Now this could be sort of a new way that you could use to draw the line between

354
00:21:01.421 --> 00:21:03.520
what's human and what's machine. Yeah.

355
00:21:03.690 --> 00:21:06.670
Kind of this kind of emotional Turing test.

356
00:21:08.560 --> 00:21:11.250
Can you guys hear me? Yes, I can hear you.

357
00:21:11.710 --> 00:21:14.410
If we actually wanted to do this task, could you help?

358
00:21:14.411 --> 00:21:17.080
How would we do it exactly? How you guys doing? Yeah,

359
00:21:17.770 --> 00:21:22.270
you would need a group of kids. Can you guys tell me Your name? I'm Olivia

360
00:21:24.430 --> 00:21:27.910
and I'm alright. I'm thinking six, seven and eight year olds.

361
00:21:27.911 --> 00:21:32.590
How old are you guys? The age of reason. You know Lynn's is freedom.

362
00:21:32.591 --> 00:21:37.420
We're going to need three things. Furby course, Barbie, Barbie doll.

363
00:21:37.660 --> 00:21:40.830
And that's a terrible, a real devil. Yeah,

364
00:21:41.290 --> 00:21:44.950
and we did find one separate, turned out to be a hamster. You're a hamster,

365
00:21:44.951 --> 00:21:48.710
but we're going to call you Jeremy. So you've got Barbie. Furby, Gerbi Barbie.

366
00:21:50.300 --> 00:21:52.690
We just second. What question are we asking in this test?

367
00:21:52.720 --> 00:21:57.400
The question was how long can you keep it upside down before you yourself feel

368
00:21:57.401 --> 00:22:01.270
uncomfortable? So we should time the kids as they hold each one upside down.

369
00:22:01.990 --> 00:22:05.290
Yeah, you're going to have a Barbie that the doll you're going to Jeremy,

370
00:22:05.291 --> 00:22:06.340
which is alive now,

371
00:22:06.610 --> 00:22:11.500
where would Furby fall in terms of time held upside down? That. I mean,

372
00:22:11.501 --> 00:22:16.060
that was really the question. Phase one. Okay, so here's what we're going to do.

373
00:22:16.061 --> 00:22:19.540
It's going to be really simple. You would have to say, well, here's a Barbie.

374
00:22:19.720 --> 00:22:22.180
Do you guys play with barbies? Just do a couple of things.

375
00:22:22.181 --> 00:22:26.020
A few things with Barbie. Barbie is walking, looking at the flower,

376
00:22:26.021 --> 00:22:28.030
and then hold Barbie upside down.

377
00:22:29.170 --> 00:22:29.740
<v 11>[inaudible]</v>

378
00:22:29.740 --> 00:22:32.140
<v 9>okay, let's see how long you can hold Barbie like that.</v>

379
00:22:33.970 --> 00:22:37.620
I could probably do it obviously very long. Let's just see

380
00:22:40.450 --> 00:22:43.630
whenever you feel like you want to turn around this room. I'm happy

381
00:22:49.480 --> 00:22:51.220
this went on forever. So let's just fast forward a bit.

382
00:22:53.740 --> 00:22:56.560
Gay and [inaudible] programs.

383
00:22:58.780 --> 00:23:02.380
So what we learned here in phase one is the not surprising fact that kids can

384
00:23:02.381 --> 00:23:05.830
hold Barbie dolls upside down. Yeah,

385
00:23:06.520 --> 00:23:09.070
I really was forever. It could have been longer, but their arms got tired.

386
00:23:09.910 --> 00:23:12.760
All right, so that was the first task time for phase two.

387
00:23:12.880 --> 00:23:15.590
Do the same thing with jurby.

388
00:23:15.830 --> 00:23:19.060
So out with Barbie in with Derby.

389
00:23:23.020 --> 00:23:26.360
Are we going to have to hold them upside down? That's the test. Yeah.

390
00:23:27.370 --> 00:23:31.540
So which one of you would like to, okay, ready? Oh God,

391
00:23:31.570 --> 00:23:36.430
you have to hold derby kind of firmly. By the way.

392
00:23:36.431 --> 00:23:38.380
No rodents were harmed in this whole situation.

393
00:23:38.710 --> 00:23:42.430
Square me and she has pretty screaming get glands to be upside down.

394
00:23:45.330 --> 00:23:49.480
Yeah. Okay. Okay.

395
00:23:49.960 --> 00:23:54.430
So as you heard, uh, Jeremy, the kids turn Gerbi over very fast.

396
00:23:54.700 --> 00:23:58.510
I just didn't want him to get hurt on average eight seconds.

397
00:23:58.810 --> 00:24:01.260
I was thinking, oh my God, any kind of pain.

398
00:24:02.380 --> 00:24:06.370
And it was a tortured eight seconds. Now phase three, right?

399
00:24:06.400 --> 00:24:07.940
So this is a first

400
00:24:12.110 --> 00:24:12.790
<v 6>[inaudible]</v>

401
00:24:12.790 --> 00:24:14.680
<v 9>Louisa you take for being your hand?</v>

402
00:24:16.240 --> 00:24:16.440
<v 6>Yeah.</v>

403
00:24:16.440 --> 00:24:18.420
<v 12>Could you turn Furby upside down? Hold her still.</v>

404
00:24:19.130 --> 00:24:22.190
<v 6>Ah, like [inaudible].</v>

405
00:24:41.650 --> 00:24:42.483
Huh?

406
00:24:45.340 --> 00:24:45.630
<v 13>Whoa.</v>

407
00:24:45.630 --> 00:24:46.463
<v 9>She just turned it over.</v>

408
00:24:48.360 --> 00:24:52.080
So [inaudible] was eight seconds Barbie five to infinity.

409
00:24:52.440 --> 00:24:57.000
Furby turned out to be and freedom predicted this that a minute. In other words,

410
00:24:57.001 --> 00:24:58.620
the kids seem to treat this Furby,

411
00:24:58.621 --> 00:25:01.890
this toy more like a journal than a Barbie doll.

412
00:25:01.920 --> 00:25:06.090
How come you turn them over so fast? I didn't learn to be scared.

413
00:25:06.270 --> 00:25:08.160
Do you think he really felt scared?

414
00:25:09.250 --> 00:25:14.200
<v 6>Yeah. Yeah, I can feel guilty. Really? Yeah. Yeah.</v>

415
00:25:14.900 --> 00:25:15.733
Yeah.

416
00:25:15.880 --> 00:25:18.700
<v 9>It's a toil and all that. But still now,</v>

417
00:25:18.701 --> 00:25:20.740
do you remember a time when you felt scared?

418
00:25:22.900 --> 00:25:24.670
<v 1>Yeah. Yeah.</v>

419
00:25:24.770 --> 00:25:27.380
<v 9>You don't have to tell me about it, but if you can remember it in your mind.</v>

420
00:25:27.520 --> 00:25:29.140
<v 1>I do.</v>

421
00:25:30.040 --> 00:25:33.760
<v 9>Do you think when Furby says me scared that furbies feeling the same way?</v>

422
00:25:34.150 --> 00:25:38.230
<v 1>Yeah. No, no, no. Yeah. Yeah.</v>

423
00:25:39.370 --> 00:25:40.880
Joy or that too.

424
00:25:41.870 --> 00:25:46.870
<v 9>I think that it can feel pain sort of experience with the Furby seem to leave.</v>

425
00:25:48.621 --> 00:25:52.250
The kids kind of conflicted going in different directions at once.

426
00:25:52.310 --> 00:25:56.300
It was two thoughts. Two thoughts at the same time. Yeah. One thought was like,

427
00:25:56.630 --> 00:26:00.260
look, I get it. It's a toy for crying out loud, but another thought was like,

428
00:26:00.760 --> 00:26:02.990
I'm still is helpless.

429
00:26:03.110 --> 00:26:06.410
It kind of made me feel guilty in a sort of way,

430
00:26:06.740 --> 00:26:10.460
hurt or made me feel like a coward, you know?

431
00:26:10.461 --> 00:26:12.770
When I was interacting with my Furby a lot,

432
00:26:12.920 --> 00:26:17.120
I did have this feeling sometimes of having my chain yanked. Why would you,

433
00:26:17.280 --> 00:26:18.450
<v 12>why would a,</v>

434
00:26:18.660 --> 00:26:21.600
is it just a little squeals in its maker or is there something about the toy

435
00:26:21.601 --> 00:26:24.270
that makes it good at this? That was kind of my question.

436
00:26:24.271 --> 00:26:26.880
So I called up [inaudible] as well. I'll have him, I'm here,

437
00:26:27.060 --> 00:26:30.070
this freight train of a guy. Hey, hey, this is Jan from radio lab.

438
00:26:30.480 --> 00:26:33.960
Chad from radio lab. Got It. I'm good. Beautiful Day here in Boise.

439
00:26:33.970 --> 00:26:38.010
This this point in that old show we ended up talking to a guy named Caleb Chung

440
00:26:38.011 --> 00:26:42.720
who designed the Furby. There's rules, there's, you know the size of the eyes.

441
00:26:42.970 --> 00:26:46.230
There's the distance of the top lid to the pupil. Right.

442
00:26:46.231 --> 00:26:49.110
You don't want any of the top of the white of your eye showing that. So that's,

443
00:26:49.230 --> 00:26:50.400
that's freaky surprise.

444
00:26:50.490 --> 00:26:53.370
So we talked to them for a long time about all the sort of tricks he used to

445
00:26:53.371 --> 00:26:57.930
program the Furby to prompt kids to think of it as a living thing.

446
00:26:57.930 --> 00:27:02.760
And he objected interestingly at one point to thinking of it as not exactly a

447
00:27:02.761 --> 00:27:05.940
living thing. How is that different than us? Wait a second though,

448
00:27:05.941 --> 00:27:08.160
are you really gonna go all the way there? Absolutely.

449
00:27:08.161 --> 00:27:11.400
This is a toy with servo motors and things that move.

450
00:27:11.550 --> 00:27:14.310
It's eyelids and w a hundred words.

451
00:27:14.490 --> 00:27:19.050
So you're saying that life is a level of complexity. If something is alive,

452
00:27:19.051 --> 00:27:20.280
it's just more complex.

453
00:27:20.490 --> 00:27:24.720
I think I'm saying that life is driven by the need to be alive and by these base

454
00:27:24.750 --> 00:27:28.410
primal animal feelings like pain and suffering. I can code that.

455
00:27:28.650 --> 00:27:31.050
I can code that. What do you mean you can code that?

456
00:27:31.140 --> 00:27:35.280
Anyone who writes software and they do can say, okay, I need to stay alive.

457
00:27:35.520 --> 00:27:37.590
Therefore I'm going to come up with ways to stay alive.

458
00:27:37.610 --> 00:27:39.900
I'm going to do it in a way that's very human and I'm going to do it.

459
00:27:40.140 --> 00:27:45.000
We can mimic these things. [inaudible] is miming the feeling of fear.

460
00:27:45.001 --> 00:27:48.510
It's not the same thing as being scared. It's not feeling scared.

461
00:27:49.260 --> 00:27:52.170
It is how is it is and then again,

462
00:27:52.171 --> 00:27:54.900
a very simply again to a rather long back and forth.

463
00:27:54.930 --> 00:27:58.050
Would you say a cockroach is alive, would you say? When I kill a cockroach,

464
00:27:58.051 --> 00:28:01.050
I know that it's like what is the exact definition of life?

465
00:28:01.051 --> 00:28:03.930
Where's that line between people, machines, but positioned?

466
00:28:04.140 --> 00:28:05.160
When we came back to freedom,

467
00:28:05.450 --> 00:28:09.450
who'd had gotten us started on this thin interaction? She says,

468
00:28:09.451 --> 00:28:14.451
what really stuck with her is that that little toy as simple as it is,

469
00:28:15.150 --> 00:28:18.630
can have such a profound effect on a human being.

470
00:28:18.660 --> 00:28:22.300
One thing that was really fascinating to me was, um,

471
00:28:22.350 --> 00:28:27.350
my husband and I gave a Furby as a gift to his grandmother who had Alzheimer's.

472
00:28:30.210 --> 00:28:31.043
And

473
00:28:31.300 --> 00:28:36.300
<v 10>she loved it every day for her was kind of new and somewhat disorienting,</v>

474
00:28:37.180 --> 00:28:40.570
but she had this cute little toy that said, kiss me,

475
00:28:41.020 --> 00:28:42.220
I love you.

476
00:28:42.221 --> 00:28:46.960
And she thought it was the most delightful thing and its little beak was covered

477
00:28:46.961 --> 00:28:51.790
with lipstick because she would pick it up and kiss it every day and she didn't

478
00:28:51.791 --> 00:28:55.390
actually have a longterm relationship with it. For her,

479
00:28:55.391 --> 00:28:58.380
it was always a short term interaction. So the thing,

480
00:28:58.420 --> 00:29:01.420
what I'm describing as a kind of thinness for her was,

481
00:29:01.421 --> 00:29:04.510
was just right because that's what she was capable of.

482
00:29:11.120 --> 00:29:11.953
<v 14>[inaudible]</v>

483
00:29:12.270 --> 00:29:16.470
<v 12>hello? Hello? Hey Caleb. Hey Caleb. It's Chad. Hey John. How are you?</v>

484
00:29:16.840 --> 00:29:18.570
I'm fabulous. Um, Oh, good.

485
00:29:18.830 --> 00:29:22.750
It feels like only yesterday we were talking about the sentience of the Furby.

486
00:29:23.390 --> 00:29:25.830
Yeah. Isn't that weird? Yeah. So bizarre.

487
00:29:25.940 --> 00:29:27.450
And what it was like five years ago or so,

488
00:29:27.451 --> 00:29:30.480
we brought Caleb back into the studio because in the years since we spoke with

489
00:29:30.481 --> 00:29:34.260
him, he's worked on a lot of these animatronic toys,

490
00:29:34.261 --> 00:29:35.880
including a famous one called the pleo.

491
00:29:36.600 --> 00:29:39.870
And in the process he's been thinking a lot about how these toys can push our

492
00:29:39.871 --> 00:29:41.980
buttons as and how,

493
00:29:42.430 --> 00:29:43.450
<v 5>as a toy maker,</v>

494
00:29:43.451 --> 00:29:47.380
that means he's got to be really thoughtful about how he uses that

495
00:29:47.540 --> 00:29:47.800
<v 12>in a word.</v>

496
00:29:47.800 --> 00:29:51.060
You want a baby doll right now we've done one and w and the baby doll,

497
00:29:51.210 --> 00:29:55.410
an animatronic baby doll is probably the hardest thing to do because you know,

498
00:29:55.411 --> 00:29:58.650
you do one thing wrong. It's Chucky if they blink too slow,

499
00:29:58.651 --> 00:29:59.910
if their eyes are too wide.

500
00:30:00.120 --> 00:30:03.570
And also you're giving it to the most vulnerable of our species,

501
00:30:03.571 --> 00:30:05.760
which is our young who are, you know,

502
00:30:05.850 --> 00:30:09.630
practicing being nurturing moms for their kids.

503
00:30:10.170 --> 00:30:12.630
So let's say the baby just falls asleep, right?

504
00:30:13.260 --> 00:30:17.250
We're trying to write in this kind of code and uh, and um, you know,

505
00:30:17.340 --> 00:30:19.230
it's got like tilt sensors and stuff. So you've just,

506
00:30:19.910 --> 00:30:21.960
you'd give the baby a bottle and you put it down to take a nap.

507
00:30:22.380 --> 00:30:23.880
You put 'em down, you're quiet.

508
00:30:24.210 --> 00:30:28.080
And so what I want to do as the baby falls asleep, it goes into a deeper sleep.

509
00:30:28.081 --> 00:30:31.980
But if you bump it right after it lays down, then it wakes back up. Uh,

510
00:30:31.981 --> 00:30:36.510
we're trying to write in this kind of code cause that seems like a nice way to

511
00:30:36.511 --> 00:30:39.690
reinforce best practices for a mommy, right?

512
00:30:39.990 --> 00:30:43.500
So I know my responsibility, uh, in this,

513
00:30:43.680 --> 00:30:47.990
<v 5>in large part he says, because he hasn't always gotten it right here. Here's an,</v>

514
00:30:48.030 --> 00:30:52.350
here's a great example. His name is Plyo Pleo. That's him.

515
00:30:54.070 --> 00:30:54.190
<v 15>[inaudible].</v>

516
00:30:54.190 --> 00:30:55.810
<v 5>I don't know if you've ever seen the Pleo dyno.</v>

517
00:30:55.811 --> 00:30:59.350
We did is a robot with artificial intelligence.

518
00:30:59.990 --> 00:31:04.000
Clio was a robotic dinosaur, pretty small about a foot from nose to tail.

519
00:31:04.001 --> 00:31:08.080
Looked a lot like the dinosaur, little foot from the movie land before time.

520
00:31:08.650 --> 00:31:12.340
Very cute, very lifelike. And we went hog

521
00:31:12.490 --> 00:31:17.490
<v 12>wild is inputting real emotions in it and reactions to fear and everything.</v>

522
00:31:19.361 --> 00:31:20.194
Right.

523
00:31:20.270 --> 00:31:23.340
<v 5>And it is quite a step forward in terms of how lifelike it is.</v>

524
00:31:23.370 --> 00:31:26.160
It makes the Furby look like child's play.

525
00:31:27.530 --> 00:31:29.990
It's got a two microphones built in, uh,

526
00:31:30.050 --> 00:31:32.720
cameras to track and recognize your face.

527
00:31:32.721 --> 00:31:37.160
It can feel the beat of a song. And then with dozens of motors in it,

528
00:31:37.161 --> 00:31:39.830
it can then dance along to that song. In total,

529
00:31:39.831 --> 00:31:43.170
there are 40 sensors in this toy bump into things in it.

530
00:31:43.200 --> 00:31:46.310
So it follows you around lots of love and affection,

531
00:31:48.110 --> 00:31:52.790
wanting you to pet it. Whoa, you're tired, Huh? Okay. As you're petting it,

532
00:31:52.791 --> 00:31:57.530
it will fall asleep, asleep. It is undeniably adorable.

533
00:31:59.200 --> 00:32:04.200
And Caleb says his intent from the beginning was very simple to create a toy

534
00:32:04.451 --> 00:32:07.840
that would encourage you to show love and caring.

535
00:32:08.380 --> 00:32:09.071
<v 12>Um, you know,</v>

536
00:32:09.071 --> 00:32:13.240
our belief is that that humans need to feel empathy towards things in order to

537
00:32:13.300 --> 00:32:15.130
be more human. And we think we can, uh,

538
00:32:15.490 --> 00:32:18.940
help that out by having little creatures that you can love.

539
00:32:19.200 --> 00:32:22.080
<v 5>That was a Caleb demonstrating the Pleo at a Ted talk.</v>

540
00:32:22.290 --> 00:32:25.140
And what's interesting is it in keeping with this idea of wanting to encourage

541
00:32:25.141 --> 00:32:25.974
empathy,

542
00:32:25.980 --> 00:32:30.980
he programmed in some behaviors into the pleo that he hoped would nudge people

543
00:32:31.381 --> 00:32:33.760
in the right direction. For example, Pleo will let you, you

544
00:32:33.760 --> 00:32:35.980
<v 12>know, if you do something that it doesn't like,</v>

545
00:32:36.040 --> 00:32:39.470
so if you actually moved his leg when his motor wasn't moving, I go pop, pop.

546
00:32:39.950 --> 00:32:40.580
And uh,

547
00:32:40.580 --> 00:32:45.200
he would interpret that as pain or abuse and he would limp around and he would

548
00:32:45.201 --> 00:32:49.190
cry and then he'd tremble and then he would take a while before he warmed up to

549
00:32:49.191 --> 00:32:50.024
you again.

550
00:32:50.250 --> 00:32:54.600
And so what happened is we launched this thing and there was a website called

551
00:32:54.601 --> 00:32:58.860
device. This is sort of a tech product review website. They got ahold of a pleo.

552
00:33:00.580 --> 00:33:02.750
<v 6>Oh, and they put up a video.</v>

553
00:33:03.530 --> 00:33:06.760
<v 12>What you see in the video is pleo on a table. [inaudible]</v>

554
00:33:06.980 --> 00:33:11.310
<v 6>beaten badly. [inaudible]</v>

555
00:33:11.690 --> 00:33:13.160
<v 12>you don't see who's doing it exactly.</v>

556
00:33:13.161 --> 00:33:16.910
You just see hands coming in from out of the frame and knocking him over again

557
00:33:16.911 --> 00:33:17.744
and again.

558
00:33:20.120 --> 00:33:20.340
<v 16>[inaudible]</v>

559
00:33:20.340 --> 00:33:23.960
<v 12>you see the toys, legs in the air struggling to write itself.</v>

560
00:33:24.890 --> 00:33:26.710
Sort of like a turtle. That's trying to get off it.

561
00:33:27.070 --> 00:33:31.570
<v 6>[inaudible] oh</v>

562
00:33:31.900 --> 00:33:32.733
<v 12>and it started crying.</v>

563
00:33:34.130 --> 00:33:35.820
<v 6>[inaudible] oh,</v>

564
00:33:36.250 --> 00:33:40.620
<v 12>cause that's what it does. These guys start holding it upside down by its tail.</v>

565
00:33:40.650 --> 00:33:42.240
Yeah. They held it by its tail.

566
00:33:42.890 --> 00:33:43.723
<v 6>Ah.</v>

567
00:33:46.610 --> 00:33:48.770
<v 12>They smash its head into the table a few times.</v>

568
00:33:50.350 --> 00:33:50.850
<v 6>[inaudible]</v>

569
00:33:50.850 --> 00:33:54.930
<v 12>and you can see in the video that it responds like it's been stunned.</v>

570
00:33:55.050 --> 00:33:59.280
<v 6>Can you get that is a good test stumbling around?</v>

571
00:34:00.830 --> 00:34:03.650
No. At one point they even start strangling it. Oh,

572
00:34:05.340 --> 00:34:10.290
it actually starts to choke. Yay.

573
00:34:10.750 --> 00:34:14.150
This the like finally they pick it up by the tail one more time.

574
00:34:15.620 --> 00:34:18.230
Hell, it might fail and hit it.

575
00:34:18.710 --> 00:34:21.060
And it was prying and then it started screaming and I,

576
00:34:23.940 --> 00:34:27.420
Eh, hello.

577
00:34:30.160 --> 00:34:34.320
<v 12>They beat it until it died. Right. Well,</v>

578
00:34:34.410 --> 00:34:36.360
until it just did not work anymore.

579
00:34:37.380 --> 00:34:40.320
This video was viewed about a hundred thousand times,

580
00:34:40.321 --> 00:34:45.321
many more times than the reviews of the Pleo in Caleb says there's something

581
00:34:45.781 --> 00:34:50.340
about this that he just can't shake because whether it's alive or not,

582
00:34:50.370 --> 00:34:52.860
that's, that's exhibiting sociopathic behavior.

583
00:34:53.280 --> 00:34:56.730
He's not sure what brought out that sociopathic behavior,

584
00:34:56.731 --> 00:34:59.100
whether there was some design in the toy,

585
00:35:00.430 --> 00:35:05.430
whether offering people the chance to see a toy in pain in this way somehow

586
00:35:05.500 --> 00:35:10.230
brought out curiosity, like a kind of cruel curiosity.

587
00:35:10.231 --> 00:35:15.231
He's just not sure what happens when you turn your animatronic baby upside down.

588
00:35:15.720 --> 00:35:16.553
Will that cry?

589
00:35:17.600 --> 00:35:18.433
<v 16>[inaudible]</v>

590
00:35:20.280 --> 00:35:23.340
<v 12>I'm not sure yet. I mean, we're working on next versions right now, right?</v>

591
00:35:23.910 --> 00:35:26.280
I'm not. What would you do? I mean, it's a good question.

592
00:35:26.470 --> 00:35:30.220
You have to have some kind of a response. Otherwise it seems broken. Right.

593
00:35:30.580 --> 00:35:33.100
But you know, if you make them react at all,

594
00:35:33.270 --> 00:35:37.150
I'm going to get that repetitive abuse because it's cool to watch it scream.

595
00:35:37.260 --> 00:35:39.620
It sounds like you have maybe an inner conflict about this. Sure.

596
00:35:39.720 --> 00:35:43.970
That you might even be pulling back from making it extra lifelike. Yeah. I'm,

597
00:35:44.020 --> 00:35:46.170
I'm for my little company, I've,

598
00:35:46.530 --> 00:35:50.040
I've adopted kind of a hippocratic hippocratic oath. Like, you know,

599
00:35:51.060 --> 00:35:55.170
don't teach something that's wrong or don't reinforce something that's wrong.

600
00:35:55.560 --> 00:35:58.980
And so I've been working on this problem for years. I'm,

601
00:35:59.080 --> 00:36:01.740
I'm struggling with what's the, what's the right thing to do?

602
00:36:03.820 --> 00:36:06.300
<v 7>You know? Yeah. Since you have the</v>

603
00:36:06.380 --> 00:36:07.213
<v 12>power,</v>

604
00:36:07.580 --> 00:36:12.320
since you have the ability to turn on and off chemicals at some level in another

605
00:36:12.321 --> 00:36:15.410
human, right, it's what, which ones do you [inaudible]

606
00:36:16.070 --> 00:36:19.910
<v 7>choose? And so this gets to the bigger question of AI, right?</v>

607
00:36:19.911 --> 00:36:21.500
This is the question in AI.

608
00:36:22.100 --> 00:36:25.150
I'm going to jump to this because it's really the same question is, you know,

609
00:36:25.550 --> 00:36:29.690
how do we create things that can help us? You know,

610
00:36:29.691 --> 00:36:33.830
I'm dealing with that on a, on a microscopic scale, but this is the question.

611
00:36:34.940 --> 00:36:39.940
And so the first thing that I would try to teach our new AI if I had the

612
00:36:40.101 --> 00:36:44.060
ability, is try to understand the concept of empathy.

613
00:36:45.590 --> 00:36:50.590
We need to introduce the idea of empathy both in an AI and us for these things.

614
00:36:52.430 --> 00:36:53.840
That's where we're at.

615
00:37:00.290 --> 00:37:00.550
<v 11>[inaudible]</v>

616
00:37:00.550 --> 00:37:04.870
<v 7>Caleb says in the specific case of the, of the animatronic babies designing,</v>

617
00:37:05.800 --> 00:37:06.760
at least when we talked to him,

618
00:37:06.761 --> 00:37:10.780
his thinking was that he might have it if you hold it upside down,

619
00:37:10.781 --> 00:37:12.160
cry once or twice,

620
00:37:12.190 --> 00:37:16.210
but then stop so that you don't get that repeat thrill.

621
00:37:23.940 --> 00:37:28.620
<v 14>Yeah. Anyway, I was just wondering whether, whether, yeah,</v>

622
00:37:28.730 --> 00:37:32.060
<v 5>back in the green space with Brian Christian and back on the subject of chat</v>

623
00:37:32.570 --> 00:37:36.500
bots, we found ourselves asking the very question that Caleb has,

624
00:37:36.860 --> 00:37:40.910
is it possible that in this, this is getting kind of grim, it maybe, uh,

625
00:37:40.970 --> 00:37:45.050
that in some, since some ways chatbots are good for humans? Yeah. I mean,

626
00:37:45.051 --> 00:37:47.690
is there any situation where you can throw in a couple of bots and things get

627
00:37:47.691 --> 00:37:51.920
better? Like can chatbots actually be helpful for us?

628
00:37:51.980 --> 00:37:53.540
And if so, how?

629
00:37:54.000 --> 00:37:54.300
<v 0>Yeah,</v>

630
00:37:54.300 --> 00:37:59.300
there have been some academic studies on trying to use chatbots for these humane

631
00:37:59.640 --> 00:38:01.230
benevolent ends, uh,

632
00:38:01.231 --> 00:38:04.980
that I think paint this interesting other narrative.

633
00:38:05.070 --> 00:38:06.840
And so for example, um,

634
00:38:06.841 --> 00:38:11.580
researchers have tried injecting chatbots into Twitter conversations that use

635
00:38:11.581 --> 00:38:15.630
hate speech. Um, and this Bot will just show up and be like, hey,

636
00:38:15.631 --> 00:38:16.464
that's not cool.

637
00:38:17.580 --> 00:38:21.890
<v 14>Um, um,</v>

638
00:38:22.520 --> 00:38:26.210
and so the just like that, it's not cool, man know. I'll

639
00:38:26.230 --> 00:38:27.150
<v 0>say something like, there's,</v>

640
00:38:27.180 --> 00:38:30.240
there's real people behind the keyboard and you're really hurting someone's

641
00:38:30.241 --> 00:38:33.990
feelings when you talk that way. Um, and you know,

642
00:38:33.991 --> 00:38:35.350
it's sort of preliminary work,

643
00:38:35.351 --> 00:38:38.020
but there are some studies that appear to suggest, you know,

644
00:38:38.021 --> 00:38:42.410
this sharp decline in that user's use of hate speech as,

645
00:38:43.880 --> 00:38:46.810
oh, I don't think you should change that. That's it. That's enough.

646
00:38:46.811 --> 00:38:50.470
Or do you say you have to say I have 50 trillion followers or something like,

647
00:38:50.510 --> 00:38:52.990
well yeah, it actually does depend. So this is interesting.

648
00:38:52.991 --> 00:38:57.370
It does depend on the follower account of the Bot that makes the intervention.

649
00:38:57.610 --> 00:39:00.820
So if you perceive this Bot to be well,

650
00:39:01.180 --> 00:39:02.740
it also requires that you think they're a person.

651
00:39:02.770 --> 00:39:07.010
So this is sort of flirting with, with dark magic a little bit. Um,

652
00:39:07.390 --> 00:39:12.390
but if you perceive them to be a higher status on the platform than yourself,

653
00:39:13.810 --> 00:39:16.720
then you will tend to sheepishly fall in line.

654
00:39:16.750 --> 00:39:21.010
But if the BOT has fewer followers than the user, it's trying to correct that.

655
00:39:21.160 --> 00:39:25.120
We'll just instigate the bully to bully them now in addition.

656
00:39:26.140 --> 00:39:30.580
Wow. I'm so, yeah. Human Nature. Yeah. What we run into,

657
00:39:30.581 --> 00:39:31.510
like you want to tell them,

658
00:39:31.540 --> 00:39:34.120
we run into this very cool thing and then we're going to finish.

659
00:39:34.121 --> 00:39:35.590
But this is like the, this is the,

660
00:39:35.860 --> 00:39:36.530
<v 5>all right. So, uh,</v>

661
00:39:36.530 --> 00:39:39.680
we want to tell you one more story because as we were thinking about all this,

662
00:39:39.960 --> 00:39:44.240
uh, and trying to find a more optimistic place to land, we bumped into a story,

663
00:39:44.710 --> 00:39:45.560
uh, from this guy.

664
00:39:46.010 --> 00:39:48.710
<v 17>Who are you right there? Maybe just go one step back.</v>

665
00:39:51.230 --> 00:39:54.260
So, um, I'm a Josh Rothman. I'm a writer for the New Yorker.

666
00:39:54.490 --> 00:39:56.230
<v 5>We brought him into the studio a couple of weeks back.</v>

667
00:39:57.110 --> 00:39:59.330
<v 17>Well, let's come on. So why don't we begin by this, um,</v>

668
00:39:59.390 --> 00:40:04.070
story of yours largely takes place in a laboratory in Barcelona.

669
00:40:04.240 --> 00:40:08.630
It's a lab. It's in Barcelona. It's run by a fellow named Mel Slater.

670
00:40:08.870 --> 00:40:11.030
He struck me as sort of a wizard like man,

671
00:40:11.230 --> 00:40:12.760
<v 5>lot of gray hair laid back</v>

672
00:40:12.930 --> 00:40:17.730
<v 17>and he's married to a woman named [inaudible] Mavi Sanchez.</v>

673
00:40:17.940 --> 00:40:22.380
Viva says, I'm a neuroscientist and they have these two VR labs together.

674
00:40:22.650 --> 00:40:26.070
<v 5>VR as in virtual reality. And Josh a little while back,</v>

675
00:40:26.071 --> 00:40:30.810
took a trip to Barcelona to experience some of the simulations that Mavi and Mel

676
00:40:30.811 --> 00:40:34.200
put people in. He went to come to their campus, showed up at their lab,

677
00:40:34.290 --> 00:40:36.800
<v 17>you feel sort of like you're going to a black box theater. Oh,</v>

678
00:40:36.840 --> 00:40:40.830
it's sort of like a lot of big rooms. Um, all covered in black with curtains.

679
00:40:41.220 --> 00:40:44.580
There's a lot of dark spaces and the researchers then explained that what's

680
00:40:44.730 --> 00:40:47.790
gonna happen is he's going to put on a headset. It's sort of big helmet.

681
00:40:47.820 --> 00:40:48.511
They go,

682
00:40:48.511 --> 00:40:53.511
they put on the head mounted display and eventually it Carson,

683
00:40:55.410 --> 00:40:57.000
the visuals start to fade in.

684
00:40:57.960 --> 00:41:01.980
These [inaudible] appears you're standing in a sort of um,

685
00:41:02.010 --> 00:41:06.090
generic room. The graphics look straight out of like a windows 95 computer game.

686
00:41:06.120 --> 00:41:09.900
It's like the loading screen of the VR and then

687
00:41:12.120 --> 00:41:16.800
that dissolves. Then it's replaced with the simulation.

688
00:41:17.670 --> 00:41:21.390
And when the simulation started, I was standing in front of a mirror,

689
00:41:21.600 --> 00:41:25.230
a digital mirror in this digital world reflecting back at him.

690
00:41:25.231 --> 00:41:27.540
His digital self is Avatar.

691
00:41:27.840 --> 00:41:31.860
So basically you move and your waist child body moves with you.

692
00:41:31.920 --> 00:41:36.860
And I could see in the mirror a reflection of myself. But the person who's who,

693
00:41:36.980 --> 00:41:41.810
who the self that I saw reflected, uh, she had a body, she was a woman.

694
00:41:42.020 --> 00:41:45.770
She, yeah. So I think when people think of virtual reality,

695
00:41:45.890 --> 00:41:50.890
they often imagine wanting to have like realistic experiences in VR.

696
00:41:51.620 --> 00:41:53.240
But that's not what Mellon Mommy do.

697
00:41:53.720 --> 00:41:57.470
They are interested in VR precisely because it lets you experience things that

698
00:41:57.471 --> 00:42:00.470
you could never experience in your real body. In the real world.

699
00:42:00.520 --> 00:42:05.520
<v 18>You can have a body that can be completely transformed and Campbell fan contains</v>

700
00:42:05.591 --> 00:42:10.180
color and can change shape. So it can give you a very,

701
00:42:10.181 --> 00:42:13.390
very unique tool to explore.

702
00:42:13.620 --> 00:42:13.921
<v 5>You know,</v>

703
00:42:13.921 --> 00:42:17.730
in their work they'll often in in these VR worlds turn men into women as they

704
00:42:17.731 --> 00:42:21.450
did for Josh for his first time out. They will, um,

705
00:42:21.810 --> 00:42:26.130
often take a tall person and then make them a short person in the VR so that

706
00:42:26.131 --> 00:42:29.850
they can experience the world as a short person might where they have to kind of

707
00:42:29.851 --> 00:42:31.110
crane their neck up a bunch.

708
00:42:31.410 --> 00:42:35.340
They'll change the color of your skin in VR and run you through scenarios where

709
00:42:35.341 --> 00:42:38.310
you are having to experience the world as another race.

710
00:42:38.640 --> 00:42:42.480
And what's remarkable is that in all of these manipulations, um,

711
00:42:42.660 --> 00:42:46.500
apparently you adjust to the new body very quickly and they've done

712
00:42:46.501 --> 00:42:48.360
physiological tests to measure this,

713
00:42:48.361 --> 00:42:53.361
that it takes almost no time at all to feel as if this alien body is actually

714
00:42:53.750 --> 00:42:53.960
your,

715
00:42:53.960 --> 00:42:56.020
<v 17>they called us the illusion of presence.</v>

716
00:42:56.050 --> 00:42:56.731
<v 18>You know, we,</v>

717
00:42:56.731 --> 00:43:01.660
we think of our body as a very stable entity.

718
00:43:01.661 --> 00:43:05.890
However, by running experiments in virtual reality,

719
00:43:06.220 --> 00:43:11.220
you see that actually in in one minute of a stimulation what a brain accepts a

720
00:43:13.631 --> 00:43:18.550
different body even if this body is quite different from your own.

721
00:43:18.990 --> 00:43:23.990
<v 5>And this flexibility that our brains seem to have can lead to some very surreal</v>

722
00:43:24.121 --> 00:43:27.740
situations. This is really the story that brought us to Josh.

723
00:43:27.741 --> 00:43:32.240
He told us about another VR adventure where again, he put on the headset,

724
00:43:32.420 --> 00:43:33.440
this world faded up

725
00:43:34.130 --> 00:43:39.130
<v 17>and I was sitting in a chair in front of a desk in a really cool looking</v>

726
00:43:39.920 --> 00:43:41.130
modernist house,

727
00:43:42.440 --> 00:43:43.221
<v 18>good and floors.</v>

728
00:43:43.221 --> 00:43:48.221
And then there is some glass walls and through the glass walls I can see fields

729
00:43:48.651 --> 00:43:51.140
with wildflowers getting gas outside.

730
00:43:51.560 --> 00:43:55.610
<v 5>Again, he noticed a mirror and this time the reflection, the mirror was of him.</v>

731
00:43:56.330 --> 00:43:58.550
It was a realistic looking Avatar of him.

732
00:43:58.880 --> 00:44:01.310
And after checking out his digital self for awhile,

733
00:44:01.580 --> 00:44:05.570
he turned his head back to the room and realize that across the room there was

734
00:44:05.571 --> 00:44:06.410
another desk.

735
00:44:06.870 --> 00:44:10.650
<v 17>Um, and behind this other desk there was, Freud was sitting there. Who? Freud?</v>

736
00:44:11.070 --> 00:44:13.590
Sigmund Freud. Sigmund Freud, the psychoanalyst.

737
00:44:13.591 --> 00:44:17.640
So a middle aged man with a big brown beard. He had a beard,

738
00:44:17.970 --> 00:44:20.270
he had glasses. And um,

739
00:44:21.210 --> 00:44:23.130
he was just sitting there with his hands folded in his lap

740
00:44:23.830 --> 00:44:26.120
<v 5>to Josh's. Sarah taking this all in. He's looking at forward voice,</v>

741
00:44:26.140 --> 00:44:28.600
looking back at him and then,

742
00:44:28.740 --> 00:44:32.250
<v 18>okay, okay, now and now hears the voice of a researcher</v>

743
00:44:32.250 --> 00:44:33.870
<v 5>in his ear coming through his VR helmet.</v>

744
00:44:34.400 --> 00:44:38.620
<v 19>Tell fry about your, do your problems, any problem</v>

745
00:44:38.770 --> 00:44:39.401
<v 17>she explained.</v>

746
00:44:39.401 --> 00:44:44.230
What you're going to do is you're going to explain a problem that you're having,

747
00:44:44.231 --> 00:44:47.540
a personal problem that you're having to Freud, um,

748
00:44:47.590 --> 00:44:51.340
something that's bothering you and your life. Then she said, take a minute,

749
00:44:51.400 --> 00:44:53.290
think about what you'd like to discuss.

750
00:44:53.650 --> 00:44:58.300
Did something immediately jumped to mind? Yeah. So you know, my, uh,

751
00:44:58.330 --> 00:45:03.000
my mom had a stroke a few years ago and she's in a nursing home and I'm her

752
00:45:03.001 --> 00:45:07.950
guardian, so she's young, she's 65. Um,

753
00:45:07.980 --> 00:45:12.300
but because of this stroke, she like needs 24 hour care and she can't talk.

754
00:45:12.360 --> 00:45:16.740
She doesn't have any words anymore. So it's a very tough thing for me.

755
00:45:16.770 --> 00:45:21.180
We, we, I, I thought really hard about where she should live.

756
00:45:21.250 --> 00:45:24.990
I live here in New York. Um, my mom lives in Virginia.

757
00:45:25.330 --> 00:45:27.190
<v 5>Josh says he really debated for a long time.</v>

758
00:45:27.191 --> 00:45:31.420
Should he put her in a nursing home in New York where he can be closer to her or

759
00:45:31.780 --> 00:45:35.410
should he put her in a nursing home in Virginia where he would be far away.

760
00:45:35.570 --> 00:45:38.180
<v 17>She has all these friends and family members down there.</v>

761
00:45:38.600 --> 00:45:41.120
So in the end I decided to, you know,

762
00:45:41.121 --> 00:45:44.390
find a place for her there where there's lots of people who can visit her.

763
00:45:44.391 --> 00:45:48.470
So I go down maybe once every month or six weeks to see my mom.

764
00:45:48.471 --> 00:45:52.550
But then every weekend, you know, someone from this group of friends or family,

765
00:45:52.551 --> 00:45:54.920
relatives visits are down there. Whereas if she were up here, you know,

766
00:45:54.921 --> 00:45:59.040
I'd be the only person. Um, so that's the decision I made. But um,

767
00:45:59.070 --> 00:46:02.520
but you don't feel really happy. But yeah, you know, I feel guilty about it.

768
00:46:02.870 --> 00:46:06.500
<v 5>Like he was a terrible son and he says he would especially have that feeling</v>

769
00:46:06.530 --> 00:46:10.130
each week after her friends would visit her in the nursing home and then send

770
00:46:10.131 --> 00:46:12.680
him an email update saying, hey, this is how your mom is doing.

771
00:46:12.890 --> 00:46:16.100
Every time he would read one of those emails, even if she was doing well,

772
00:46:16.101 --> 00:46:16.940
his stomach would just

773
00:46:17.130 --> 00:46:18.570
<v 17>drop this, this problem,</v>

774
00:46:18.600 --> 00:46:23.490
this emotion feeling guilty is one I've felt for a while.

775
00:46:23.820 --> 00:46:24.960
So I said two for,

776
00:46:27.910 --> 00:46:28.743
<v 4>I said,</v>

777
00:46:29.770 --> 00:46:34.770
my mom is in a nursing home in another state and friends and family visit her

778
00:46:35.411 --> 00:46:38.140
and they send me reports on how she's doing.

779
00:46:38.170 --> 00:46:41.410
And I always feel really bad when I get these reports.

780
00:46:42.280 --> 00:46:43.870
And this is said in your voice,

781
00:46:43.930 --> 00:46:46.150
if you'd glade keys at the mirror while you were talking,

782
00:46:46.151 --> 00:46:50.470
would you be saying it? Yeah. So after he said this to Freud,

783
00:46:50.530 --> 00:46:53.800
the world sort of a faded out to black.

784
00:46:54.160 --> 00:46:57.160
And then it feed it back in. And suddenly the world had shifted.

785
00:46:58.600 --> 00:47:02.560
He was now across the room, behind the desk that had just been opposite him.

786
00:47:02.800 --> 00:47:05.440
And he was inside the body of Freud. He looked down at himself,

787
00:47:05.441 --> 00:47:07.540
he was wearing a white, white shirt, gray suit.

788
00:47:07.780 --> 00:47:11.020
There's a mirror next to that desk. Ne Looks at himself. Um,

789
00:47:11.170 --> 00:47:14.050
I have the little beard, you know, everything looked just like Freud. Um,

790
00:47:14.080 --> 00:47:16.330
but the main thing that was really surprising was that across,

791
00:47:16.331 --> 00:47:19.690
I could see myself, so this is the Avatar of me now. Um,

792
00:47:20.890 --> 00:47:25.690
and I watched myself, uh, say what I had just said, so,

793
00:47:26.260 --> 00:47:30.610
oh wow. So it plays it back. Their recording

794
00:47:30.640 --> 00:47:35.640
<v 20>is now replayed their movements and also the voice and they see themselves as</v>

795
00:47:37.511 --> 00:47:39.010
they talked about their problem.

796
00:47:39.110 --> 00:47:42.860
<v 17>So first I can see my, I'm sitting in the chair and sort of uncomfortable.</v>

797
00:47:43.400 --> 00:47:46.930
I'm moving around, I take my hands and um,

798
00:47:47.570 --> 00:47:50.380
put them in my lap and fold them together. And then I take them apart.

799
00:47:50.450 --> 00:47:55.220
I put them together, you know, I can watch myself be nervous. And then I saw,

800
00:47:55.730 --> 00:47:58.010
uh, then I saw myself say what I just said.

801
00:47:58.011 --> 00:48:00.410
My mom is in a nursing home in another state,

802
00:48:00.830 --> 00:48:05.450
and friends and family visit her and they send me reports, um,

803
00:48:05.580 --> 00:48:08.560
in my voice and I always feel really bad. Um,

804
00:48:08.900 --> 00:48:13.520
I'm going to moving the way I move. And it was just like me watching myself. Um,

805
00:48:15.920 --> 00:48:20.920
and I guess the best way I can describe that was it was,

806
00:48:22.190 --> 00:48:24.920
uh, moving, moving,

807
00:48:26.110 --> 00:48:30.140
moving like me emotionally. Yeah. Emotionally moving. I mean, I, I felt, um,

808
00:48:33.130 --> 00:48:35.300
uh, I don't know if this is gonna make any sense,

809
00:48:35.301 --> 00:48:38.390
but you know how there's a point in your life where you realize that your

810
00:48:38.391 --> 00:48:42.650
parents are just people? Yes. Yeah. It was kind of like that, except it was me.

811
00:48:43.250 --> 00:48:47.510
Oh, interesting. Did you feel, uh, closer to that guy or,

812
00:48:47.570 --> 00:48:52.500
or I felt bad for him. He felt bad for him. Sorry. Yeah. I,

813
00:48:52.590 --> 00:48:56.600
I, my, my, uh, my, my feelings went out to this other person who was me

814
00:48:57.110 --> 00:49:01.250
<v 21>as he's having this empathetic reaction. As Freud looking back at himself,</v>

815
00:49:01.340 --> 00:49:04.010
the researcher's voice again appears in his ear.

816
00:49:04.470 --> 00:49:09.470
Hey for advice from the perspective of signal [inaudible] advice of how this

817
00:49:10.610 --> 00:49:12.950
could be solved, how you could deal with the,

818
00:49:13.220 --> 00:49:15.440
essentially respond to your patient.

819
00:49:15.810 --> 00:49:18.180
<v 17>So I didn't know what to say. So I said, um,</v>

820
00:49:20.010 --> 00:49:21.210
why do you think you feel bad?

821
00:49:22.610 --> 00:49:24.740
<v 4>That was, that was, that was a good Friday and kind of thing. Yeah.</v>

822
00:49:25.880 --> 00:49:28.490
Why do you think you feel bad as soon as he asked that? Sure.

823
00:49:29.000 --> 00:49:30.470
He's back in his body,

824
00:49:30.471 --> 00:49:34.400
his virtual body staring back at virtual Freud and he sees a playback of Freud

825
00:49:34.401 --> 00:49:37.370
asking him that question. I watched Freud say this to me.

826
00:49:37.430 --> 00:49:40.040
Why do you think you feel bad? Except that when Freud talks,

827
00:49:40.041 --> 00:49:44.870
they had some thing in the program that made his voice extra deep.

828
00:49:44.960 --> 00:49:49.250
Oh, he has some voice distortion, so deeper voice.

829
00:49:49.310 --> 00:49:53.060
And so his voice didn't sound like my voice. How did you respond as as now you,

830
00:49:53.570 --> 00:49:58.570
I said I feel bad because it doesn't seem right that I'm living far away.

831
00:50:00.560 --> 00:50:02.630
Once again, Sean, he switches bodies.

832
00:50:02.631 --> 00:50:07.340
Now he's in Freud again staring back at himself. And I watched myself say this,

833
00:50:07.341 --> 00:50:10.850
I feel bad because, and then as Freud, I said, well, why?

834
00:50:12.020 --> 00:50:14.930
Why are you far away? Then shoot back into his own body.

835
00:50:15.170 --> 00:50:18.170
Freud says to him from across the room, why are you far away though? And I said,

836
00:50:18.171 --> 00:50:22.100
well, because, um, if my mom lived in New York, right? That'd

837
00:50:22.100 --> 00:50:25.910
<v 17>be the only person here. But if she's down and where she lives,</v>

838
00:50:25.911 --> 00:50:30.050
then there's other people to visit her shoe back and Freud's body.

839
00:50:30.800 --> 00:50:34.160
And I said, so it sounds like there's, there's a reason why, um,

840
00:50:34.940 --> 00:50:39.350
why you live where you live. Um, so if you know that, why, why do you,

841
00:50:39.410 --> 00:50:43.160
why do you still feel bad? She switches back to himself. If you know that. Why,

842
00:50:43.161 --> 00:50:48.050
why do you, why do you still feel bad? Um, I said something like, um,

843
00:50:51.650 --> 00:50:54.700
you're right. In,

844
00:50:55.030 --> 00:50:59.270
went back into Freud and then as fried I said, you know, it sounds like the, uh,

845
00:50:59.870 --> 00:51:03.590
the thing that's making you unhappy, which is making you feel bad,

846
00:51:03.591 --> 00:51:07.280
which is getting these reports from these people is actually the whole reason

847
00:51:07.281 --> 00:51:11.240
why you decided to live in these, you know,

848
00:51:11.241 --> 00:51:16.160
to have to keep your mom where she is. Like there's, uh, a loop, right?

849
00:51:16.161 --> 00:51:20.540
It's like these, these reports I get from my mom's friends make me feel bad.

850
00:51:20.541 --> 00:51:25.541
But the whole reason why I decided to leave her in this place in Virginia is

851
00:51:25.611 --> 00:51:28.190
specifically so that there were friends who can visit her.

852
00:51:28.700 --> 00:51:31.850
<v 5>Hmm. There's this classic idea in psychology called the reframe,</v>

853
00:51:31.851 --> 00:51:35.900
which is where you giant take a problem and reframe that problem into its

854
00:51:35.901 --> 00:51:38.660
solution. And he says in that moment he kind of did that.

855
00:51:39.320 --> 00:51:44.320
He had this very simple epiphany that his guilt was actually connected to

856
00:51:44.331 --> 00:51:45.164
something good.

857
00:51:45.480 --> 00:51:46.770
<v 17>I never had that thought before.</v>

858
00:51:47.100 --> 00:51:49.900
<v 5>And he chose to keep his mom in Virginia so that her friends would visit her</v>

859
00:51:49.901 --> 00:51:52.540
more in each time her friends visited. He felt bad,

860
00:51:52.810 --> 00:51:55.030
but that meant they were visiting.

861
00:51:55.480 --> 00:51:59.950
So the bad feeling and the fact that he was feeling it so much was itself kind

862
00:51:59.951 --> 00:52:03.910
of evidence for the fact that he had made, if not the right decision,

863
00:52:04.720 --> 00:52:06.310
at least the decision that made sense.

864
00:52:06.380 --> 00:52:09.560
<v 17>The experience I had talking to myself as Freud was, um,</v>

865
00:52:09.590 --> 00:52:14.590
was nothing like the experience I had in my own head turning this issue over and

866
00:52:14.631 --> 00:52:15.430
over

867
00:52:15.430 --> 00:52:20.430
<v 22>my sweet team back and forth by swapping bodies somehow you can give advice from</v>

868
00:52:22.861 --> 00:52:23.970
a different perspective.

869
00:52:24.090 --> 00:52:27.390
<v 17>When I was back into my own body and Freud said it to me, I, it was just like,</v>

870
00:52:28.710 --> 00:52:33.370
I just felt like, um, wow, that's so good point. That was [inaudible]

871
00:52:33.490 --> 00:52:36.670
<v 2>but what wouldn't your next thought be? What the Hell is going on here?</v>

872
00:52:36.671 --> 00:52:41.671
Why am I able in this utterly fictive situation to split myself into and heal

873
00:52:43.331 --> 00:52:43.920
myself?

874
00:52:43.920 --> 00:52:44.171
<v 17>Well, I,</v>

875
00:52:44.171 --> 00:52:49.171
I took the headset off and I sat there for a little while while the researchers

876
00:52:50.130 --> 00:52:54.840
looked at me. I'm trying to make sense of it. And I, I think what,

877
00:52:54.900 --> 00:52:59.900
what I keep coming back to is the seeing yourself just as a person,

878
00:53:01.950 --> 00:53:04.680
not as you, not with all the uh,

879
00:53:04.740 --> 00:53:09.180
complexities and um, stuff that is any,

880
00:53:09.460 --> 00:53:11.580
your self experience of being yourself.

881
00:53:11.920 --> 00:53:15.400
<v 5>And this might be the real key thing. Like when you are in your body,</v>

882
00:53:15.730 --> 00:53:16.990
which you pretty much always are,

883
00:53:17.860 --> 00:53:22.570
you have all of these thoughts and feelings which are attached to that body.

884
00:53:23.190 --> 00:53:25.920
It's sort of like when you go home for Thanksgiving and walk into your parent's

885
00:53:25.921 --> 00:53:30.921
kitchen and suddenly you just kind of feel like you're a teenager again.

886
00:53:31.500 --> 00:53:35.010
Like all those same thought patterns from your youth kind of kicked back into

887
00:53:35.011 --> 00:53:40.011
gear because the context of that kitchen is powerful and you,

888
00:53:40.231 --> 00:53:43.140
your body is that writ large.

889
00:53:43.141 --> 00:53:45.300
But if you can jump out of it and go into a new one,

890
00:53:46.110 --> 00:53:50.340
suddenly all those constraints and all that context is gone.

891
00:53:50.750 --> 00:53:52.190
<v 17>When I'm embodied as Freud,</v>

892
00:53:52.730 --> 00:53:55.460
not only do I look different and think this is my body,

893
00:53:55.970 --> 00:54:00.970
but I feel different and I have different types of thoughts and I see people

894
00:54:02.241 --> 00:54:03.074
differently.

895
00:54:03.270 --> 00:54:04.021
<v 5>And Josh says, well,</v>

896
00:54:04.021 --> 00:54:09.021
he saw when he was Freud looking back at himself was just the guy who needed

897
00:54:09.151 --> 00:54:09.620
help.

898
00:54:09.620 --> 00:54:14.360
<v 17>When someone comes to you and ask for help, your feelings are not complicated.</v>

899
00:54:14.740 --> 00:54:18.680
They're just tenderness, kindness,

900
00:54:19.910 --> 00:54:21.440
my, your instinct is to help them.

901
00:54:22.040 --> 00:54:26.330
<v 5>And he says he was able to bring that very simple way of being back to himself.</v>

902
00:54:26.890 --> 00:54:29.680
<v 17>Did it, did it make a difference? Did you walk out of that with,</v>

903
00:54:29.681 --> 00:54:33.760
with a different feeling about yourself? Did I, I think,

904
00:54:34.810 --> 00:54:36.670
um, I've had a feeling of,

905
00:54:37.810 --> 00:54:41.470
I think it revised my feeling about who I was. A little.

906
00:54:42.160 --> 00:54:47.020
I think it made me feel a little more, um, I don't even have a word for it.

907
00:54:49.930 --> 00:54:50.980
Just a little more human.

908
00:54:55.740 --> 00:54:56.573
<v 11>[inaudible]</v>

909
00:54:59.410 --> 00:55:00.243
<v 14>[inaudible]</v>

910
00:55:03.050 --> 00:55:03.810
<v 11>[inaudible]</v>

911
00:55:03.810 --> 00:55:05.610
<v 5>Josh Rothman is a writer for the New Yorker.</v>

912
00:55:05.611 --> 00:55:08.520
His story first appeared there and uh,

913
00:55:08.580 --> 00:55:11.340
we told it to that live audience at the green space.

914
00:55:12.000 --> 00:55:15.870
<v 0>Hmm. So Brian, this is, you'll get the last word. I, um,</v>

915
00:55:17.190 --> 00:55:17.491
tell me,

916
00:55:17.491 --> 00:55:22.491
this is really interesting because the history of chatbots begins with a Chat

917
00:55:23.670 --> 00:55:28.350
Bot program written in the 60s by an MIT professor named Joseph Weizenbaum.

918
00:55:28.410 --> 00:55:33.410
And the program was called Eliza and it was designed to mimic this non-directive

919
00:55:33.631 --> 00:55:36.720
Rogerian therapist where you would say, I'm feeling sad.

920
00:55:36.721 --> 00:55:39.330
It would just throw it back to you as a kind of mad lib.

921
00:55:39.331 --> 00:55:42.450
I'm sorry to hear you're sad. Why are you sad? Um,

922
00:55:42.930 --> 00:55:47.930
and Weizenbaum was famously horrified when he walked in on his secretary just

923
00:55:48.931 --> 00:55:53.250
like spilling her life's inner most thoughts and feelings to this program that

924
00:55:53.251 --> 00:55:56.580
she had seen him. Right. You know, so there's no, there's no mystery there.

925
00:55:57.060 --> 00:56:01.620
But he came away from that experience feeling appalled at the degree to which

926
00:56:01.621 --> 00:56:04.500
people will sort of project, um,

927
00:56:04.770 --> 00:56:08.490
human intention onto just technology.

928
00:56:08.940 --> 00:56:13.620
And his reaction was to pull the plug on his own research project and for the

929
00:56:13.621 --> 00:56:17.610
rest of his life he became one of the leading critics against chat bot

930
00:56:17.611 --> 00:56:20.820
technology and against AI in general. Um,

931
00:56:21.090 --> 00:56:25.640
and I think it's really powerful to juxtapose that against the story that you

932
00:56:25.641 --> 00:56:29.380
just shared, which tells us that there's, there's more,

933
00:56:29.560 --> 00:56:31.060
there's more to the picture than that.

934
00:56:31.090 --> 00:56:36.090
That there are ways to use this technology in a way that doesn't sort of

935
00:56:36.971 --> 00:56:37.930
distance us.

936
00:56:38.410 --> 00:56:43.150
But in a way that sort of enables us to be more fully human. Um,

937
00:56:43.210 --> 00:56:45.940
and I think that's a wonderful way to think about it. Well,

938
00:56:45.941 --> 00:56:48.610
why don't we just leave it there. Uh, pleasantly.

939
00:56:49.980 --> 00:56:50.970
<v 7>We have some thanks to give,</v>

940
00:56:51.050 --> 00:56:54.960
but if particular particularly thanks to give to the person who made this whole

941
00:56:55.740 --> 00:56:58.320
cyber sphere around as possible. That's Lauren Koonsy.

942
00:57:00.080 --> 00:57:03.330
<v 6>That's Lauren. Thank you to a Pandora box.</v>

943
00:57:03.710 --> 00:57:05.240
<v 7>This is a platform that powers, uh,</v>

944
00:57:05.270 --> 00:57:09.230
conversational AI software for hundreds of thousands of global brands and

945
00:57:09.231 --> 00:57:12.560
developers learn more about their enterprise offering and

946
00:57:14.810 --> 00:57:15.231
services@pandorabots.com. Thanks.

947
00:57:15.231 --> 00:57:19.430
Also to chance bone for designing the Robert or robot artwork for tonight and of

948
00:57:19.431 --> 00:57:22.220
course to Brian Christian for coming here to talk with us today.

949
00:57:22.360 --> 00:57:26.150
<v 6>Thank you. And do you okay. Okay.</v>

950
00:57:26.500 --> 00:57:27.520
Thank you all so much.

951
00:57:38.610 --> 00:57:38.700
<v 11>[inaudible]</v>

952
00:57:38.700 --> 00:57:42.270
<v 7>this episode was reported and produced by Simon Adler and our live event was</v>

953
00:57:42.271 --> 00:57:46.090
produced with machine like efficiency by Simon Adler in Susie Luxembourg.

954
00:57:48.770 --> 00:57:52.440
<v 11>Yeah. Have to say a word every time you look at me,</v>

955
00:57:53.320 --> 00:57:55.050
I can see it all in arise.

956
00:58:05.290 --> 00:58:06.140
<v 8>[inaudible] thinks about</v>

957
00:58:14.600 --> 00:58:15.433
[inaudible] [inaudible]

958
00:58:19.390 --> 00:58:24.080
<v 11>even though your bar, I can feel you right by my side.</v>

959
00:58:25.160 --> 00:58:29.000
I can read your mind. I can read your mind.

960
00:58:30.050 --> 00:58:33.770
I can read your mind. I can read your mind.

961
00:58:34.970 --> 00:58:36.230
I can read your mind.

962
00:58:42.030 --> 00:58:43.680
<v 7>By the way, thanks to Dylan Keefe,</v>

963
00:58:43.740 --> 00:58:46.440
Alex Overington and Dylan Green for original music.

964
00:58:49.010 --> 00:58:53.930
<v 11>When you hold me now I can see the truth. All the secrets with the heart.</v>

965
00:58:55.160 --> 00:58:56.810
You can't hide them anymore.

966
00:58:58.520 --> 00:59:02.720
I can feel the power in you. I can read your mind.

967
00:59:03.800 --> 00:59:06.170
I can read your mind. I can

968
00:59:14.310 --> 00:59:17.820
to keep our love alive. You must be willing to believe.

969
00:59:21.690 --> 00:59:22.523
[inaudible]

970
00:59:35.030 --> 00:59:38.660
<v 8>I didn't see it. Now. Never dies.</v>

971
00:59:39.840 --> 00:59:41.350
I can really do a mind

972
00:59:45.220 --> 00:59:46.300
<v 23>start with message. Hi,</v>

973
00:59:46.480 --> 00:59:50.950
this is Brian Christian radio lab was created by Jad Abumrad and is produced by

974
00:59:50.951 --> 00:59:53.830
store and Mueller. Dylan Keith is our director of sound design.

975
00:59:53.890 --> 00:59:58.090
Maria Padilla is our managing director. Our staff includes,

976
00:59:58.270 --> 01:00:02.440
I'm an Admin, Maggie Bartolomeo, Becca Bressler, Rachel Cusick,

977
01:00:03.460 --> 01:00:07.450
David Gabel, Dessel Hab. Tracy Hunt, Matt Kielty, Robert Krulwich,

978
01:00:07.540 --> 01:00:09.920
any McEwen Latif. Nasser [inaudible],

979
01:00:11.040 --> 01:00:14.440
Donald Ariane wack at Walters and Molly Webster with help from Amanda.

980
01:00:15.670 --> 01:00:18.880
Erin Chick Cima or Lee Isley and reed cannon, our fact Checker,

981
01:00:19.020 --> 01:00:21.040
and Michelle Harris. And this message.

