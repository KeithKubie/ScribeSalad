WEBVTT
Kind: captions
Language: en

00:00:03.020 --> 00:00:06.419
Hi, I’m Adriene Hill, and Welcome back to
Crash Course, Statistics.

00:00:06.419 --> 00:00:09.610
There’s a lot of talk of “randomness”
in statistics.

00:00:09.610 --> 00:00:13.860
It’s probably something you’ve heard a
lot in this series and in real life too.

00:00:13.860 --> 00:00:17.500
Randomness is tied to the idea of uncertainty.

00:00:17.510 --> 00:00:19.140
Like why are these fries here?

00:00:19.140 --> 00:00:20.880
And are they delicious?

00:00:20.880 --> 00:00:24.560
But just because something is random doesn’t mean we know nothing about it.

00:00:24.560 --> 00:00:30.060
For example, I might not know exactly how
many people will shop at my local Costco today,

00:00:30.060 --> 00:00:34.320
but I do know it’s probably more than 100,
and probably less than 1 million.

00:00:34.320 --> 00:00:39.060
Even with these very conservative guesses,
I still know something about the “randomness”

00:00:39.060 --> 00:00:40.240
of this variable.

00:00:40.240 --> 00:00:45.000
It’s an odd juxtaposition of what we know
and what we don’t at the same time.

00:00:45.000 --> 00:00:54.020
INTRO

00:00:54.020 --> 00:00:56.200
Lots of things in your everyday life are random.

00:00:56.200 --> 00:01:00.680
From dice rolls in your weekly Dungeons and Dragons game, to which card you draw next

00:01:00.680 --> 00:01:05.040
when playing Canasta, to how many people in your subway car are trekkies.

00:01:05.040 --> 00:01:10.880
Since individual values of these random variables aren’t that predictable, we generally look

00:01:10.880 --> 00:01:13.820
at the outcomes across multiple instances.

00:01:13.820 --> 00:01:18.780
Often, the best way to get a feel for the
behavior of random variables like dice values

00:01:18.780 --> 00:01:19.880
is to simulate them.

00:01:19.880 --> 00:01:24.560
Simulations allow us to explore options that didn’t happen, but could have happened.

00:01:24.560 --> 00:01:27.260
And when you get right down to it, that’s
what statistics is all about.

00:01:27.260 --> 00:01:32.860
Let’s look at a simulation to understand
more about the weight of a Large Fry at McDonalds.

00:01:32.860 --> 00:01:40.580
Supposedly, a Large Fry at McDonalds has about
168 grams of crispy, salty, potato-y goodness.

00:01:40.580 --> 00:01:44.729
But we know that the process of shoveling
piping hot fries into cardboard cartons, isn’t

00:01:44.729 --> 00:01:46.229
an exact science.

00:01:46.229 --> 00:01:50.480
But it most likely is a random process, which means that the weight of your fries, is a

00:01:50.480 --> 00:01:51.929
random variable.

00:01:51.929 --> 00:01:56.579
You don’t know exactly how many grams will be in your next order of fries, but you can

00:01:56.579 --> 00:02:01.209
know something about the random process that generates these weights, so you hop in your

00:02:01.209 --> 00:02:06.109
car, drive to the nearest McDonalds and order 10,000, large fries.

00:02:06.109 --> 00:02:09.539
Back at home you get out your scale and begin to unbag and weigh your fries… the first

00:02:09.539 --> 00:02:13.620
batch weighs 173.03 grams.

00:02:13.620 --> 00:02:15.580
Not bad, seems like you got an extra fry or
two.

00:02:15.580 --> 00:02:26.500
The next 4 orders weigh 169.05 152.41 153.80 174.60 grams respectively.

00:02:26.500 --> 00:02:32.000
After unbagging and weighing all 10,000 orders you plot a histogram of all the weights.

00:02:32.019 --> 00:02:35.709
Looking at this graph, we can see that McDonalds is pretty good at giving you your fair share

00:02:35.709 --> 00:02:36.360
of fries.

00:02:36.360 --> 00:02:39.520
Most orders have around 168 grams.

00:02:39.530 --> 00:02:44.200
But we can also see that the randomness of the carton-filling process means that we can

00:02:44.209 --> 00:02:50.340
expect to occasionally see orders with up
to 200 grams and as low as 130 grams... but

00:02:50.340 --> 00:02:53.230
those don’t happen too often.

00:02:53.230 --> 00:02:57.579
You may ask how many grams of fries you should expect to get on your next trip to McDonald’s...

00:02:57.579 --> 00:02:59.799
our best guess is the mean.

00:02:59.799 --> 00:03:04.629
It’s the amount we expect to get from this
random fry distributing process.

00:03:04.629 --> 00:03:08.300
You already know how to calculate the mean of a finite group of numbers--the sum of those

00:03:08.300 --> 00:03:13.719
numbers divided by the numbers of items, n--so let’s expand our definition so that we can

00:03:13.719 --> 00:03:15.329
take the mean of a distribution.

00:03:15.329 --> 00:03:19.879
Again, the mean is a type of expectation,
it’s called the expected value of the data

00:03:19.879 --> 00:03:23.080
because it's what we “expect” from our
data overall.

00:03:23.080 --> 00:03:28.469
Like how you “expect” that an American
woman would be the average height of 5’4

00:03:28.469 --> 00:03:32.549
or 163 centimeters if you didn't know anything else about her.

00:03:32.549 --> 00:03:36.319
With discrete distributions, where values
can only take on set numbers like how many

00:03:36.320 --> 00:03:41.840
sodas people drink at a party... calculating
the expectation looks similar to the mean formula.

00:03:41.840 --> 00:03:46.560
For simplicity’s sake, let’s say that
people will only take 0,1,2, or 3 sodas at

00:03:46.560 --> 00:03:47.560
your party.

00:03:47.560 --> 00:03:51.170
And you want to know how much soda you should expect a person will drink so that you can

00:03:51.170 --> 00:03:52.170
get enough.

00:03:52.170 --> 00:03:55.120
Cause nothing kills a party like running out
of soda…

00:03:55.120 --> 00:04:00.269
For each possible value, multiply it by the
relative frequency for that value and add

00:04:00.269 --> 00:04:02.170
all these products together.

00:04:02.170 --> 00:04:08.940
So if 10% of people will drink 0 sodas, 20%
will drink 1 soda, 40% will drink 2, and 30%

00:04:08.940 --> 00:04:13.489
will drink 3 sodas, we get this formula for
the expected value.

00:04:13.489 --> 00:04:18.330
Which equals 1.9 sodas, meaning you should buy about 2 sodas per guest.

00:04:18.330 --> 00:04:23.940
Notice that we didn’t have any actual counts for your guests...no one RSVPs anymore.

00:04:23.940 --> 00:04:28.540
This is the expected value of the distribution, and we can apply it to any number of guests

00:04:28.540 --> 00:04:29.120
we want.

00:04:29.120 --> 00:04:33.660
But not everything in life is measured discretely, sometimes...oftentimes… you’ll have continuous

00:04:33.670 --> 00:04:38.890
variables like height, or grams of fries,
which can take on any value at all.

00:04:38.890 --> 00:04:45.220
In theory, calculating the expectation for
a continuous distribution is exactly the same,

00:04:45.220 --> 00:04:50.030
except now we have an infinite number of values which means adding all of the products of

00:04:50.030 --> 00:04:52.630
values and frequencies isn’t really doable.

00:04:52.630 --> 00:04:57.200
Luckily Sir Isaac Newton invented the integral which allows us to take the sum of an infinite

00:04:57.200 --> 00:05:01.030
number of these products without actually
adding them all up by hand.

00:05:01.030 --> 00:05:02.290
You may see it written like this.

00:05:02.290 --> 00:05:06.960
But this is simply the fancy math way of saying, “multiply all the values by their frequencies

00:05:06.960 --> 00:05:08.750
and add ‘em up”.

00:05:08.750 --> 00:05:13.290
If we wanted to know the expectation of the weight of a large fry in grams, we can use

00:05:13.290 --> 00:05:18.070
this integral and the fact that the fry weights are normally distributed to calculate it.

00:05:18.070 --> 00:05:22.560
No matter how you’re calculating it, the
expectation of your data is an important thing to know.

00:05:22.560 --> 00:05:26.460
It not only characterizes the data, but it
can help you make sure that you know what

00:05:26.460 --> 00:05:31.180
to expect, from number of sodas to have at
a party, to how much joy you should expect

00:05:31.180 --> 00:05:36.340
from a belly full of fries; expectation helps
you understand something about randomness.

00:05:36.340 --> 00:05:41.760
But not everything...there’s still more
to know about random processes, like how spread

00:05:41.760 --> 00:05:44.500
out or how variable they are.

00:05:44.500 --> 00:05:47.340
Variance is also an expectation.

00:05:47.350 --> 00:05:50.580
It tells us how spread out we expect the data to be.

00:05:50.580 --> 00:05:54.900
The variance of the amount of money each family makes is pretty high, because people don’t

00:05:54.900 --> 00:05:56.230
all make the same amount of money.

00:05:56.230 --> 00:05:59.880
To make things easier, we can represent expectation like this.

00:05:59.880 --> 00:06:05.540
Variance is the expectation of all data points minus the mean squared.

00:06:05.540 --> 00:06:09.380
Since we’re subtracting the mean, we call
it mean-centered or “central”.

00:06:09.380 --> 00:06:14.761
In essence, we’re creating a new distribution (each value minus the mean squared) , and

00:06:14.761 --> 00:06:18.160
taking the expectation of this new distribution.

00:06:18.160 --> 00:06:22.210
Since expectation is always the same--we just sum a bunch of values times their frequencies

00:06:22.210 --> 00:06:24.600
-- these two formulas are the same.

00:06:24.600 --> 00:06:29.780
Since we’re taking each value minus the
mean to the second power, we also often call

00:06:29.780 --> 00:06:32.310
this the second moment of the data.

00:06:32.310 --> 00:06:36.120
Which is just the expectation of the mean-centered data to the second power.

00:06:36.120 --> 00:06:41.550
The second moment tells us how reliable the first expectation is...if you have a really

00:06:41.550 --> 00:06:45.650
high variance for your estimate of how much soda to buy for your party, you know that

00:06:45.650 --> 00:06:50.220
you might want to run to the store for a couple extra cases, since it’s possible that you

00:06:50.220 --> 00:06:52.930
might get a group of real soda guzzlers.

00:06:52.930 --> 00:06:56.970
So the mean is the first moment of a distribution of data, and the variance is the second.

00:06:56.970 --> 00:06:58.080
And we can keep going.

00:06:58.080 --> 00:07:03.210
There are a lot of moments since all we do
is keep raising to higher and higher powers,

00:07:03.210 --> 00:07:06.170
but the first four are the most useful for
our purposes.

00:07:06.170 --> 00:07:10.711
We’ve already covered the first two, but
the third moment--the expectation of the mean

00:07:10.720 --> 00:07:16.740
centered data to the third power--is also
something you might be familiar with: Skewness.

00:07:16.740 --> 00:07:22.420
Skewness tells us whether there are more extreme values on one side, like income or amount

00:07:22.420 --> 00:07:25.400
won in Vegas which are both right skewed.

00:07:25.400 --> 00:07:29.710
Think back to your algebra class...when you take something to an even power like 2, or

00:07:29.710 --> 00:07:32.370
4, your number is always positive.

00:07:32.370 --> 00:07:35.620
So even moments--like variance--are always positive.

00:07:35.620 --> 00:07:40.280
Variance counts extreme values on the right
and the left of the mean the same, since it

00:07:40.280 --> 00:07:41.870
squares them.

00:07:41.870 --> 00:07:47.610
-2 squared and 2 squared are both 4, so values that are 2 units above or two units below

00:07:47.610 --> 00:07:51.000
the mean both contribute equally to the variance.

00:07:51.000 --> 00:07:57.400
But odd powers like 3 can be negative or positive, so they count numbers above the mean differently

00:07:57.400 --> 00:07:58.740
than those below.

00:07:58.740 --> 00:08:02.640
Numbers smaller than the mean that are negative will still be negative when they’re taken

00:08:02.640 --> 00:08:04.330
to the third power.

00:08:04.330 --> 00:08:09.300
So the third moment--skewness-- is a measure of how skewed the distribution is.

00:08:09.300 --> 00:08:15.360
If there are a lot more extreme values smaller than the mean, skewness will tend to be negative.

00:08:15.360 --> 00:08:19.361
On the other hand, if there are a lot more
extreme values bigger than the mean, skewness

00:08:19.361 --> 00:08:20.620
will tend to be positive.

00:08:20.620 --> 00:08:25.070
We’ve seen that as humans, we’re pretty
good at seeing when a distribution is skewed,

00:08:25.070 --> 00:08:27.680
but it can be really useful to have a way
to quantify it.

00:08:27.680 --> 00:08:34.219
Just like the variance tells us how reliable
the mean is, skewness can tell us how reliable

00:08:34.219 --> 00:08:35.870
the variance is.

00:08:35.870 --> 00:08:41.040
If a distribution is really skewed, then the
variance is going to be a lot higher on one side.

00:08:41.040 --> 00:08:45.560
Imagine the distribution of the amount of
chips that people will eat at your party is

00:08:45.560 --> 00:08:50.720
skewed, you know that there’s a lot more
extreme values on one side...maybe some people

00:08:50.720 --> 00:08:52.710
forgot to eat dinner before they showed up.

00:08:52.710 --> 00:08:55.720
And finally, the Fourth moment - Kurtosis.

00:08:55.720 --> 00:09:00.540
Kurtosis is the Expectation of the mean centered data to the fourth power.

00:09:00.540 --> 00:09:05.260
And it’s a measure of how thick the tails
on a distribution are.

00:09:05.270 --> 00:09:09.900
This tells you how common it is to have values that are really far from the mean.

00:09:09.900 --> 00:09:13.430
When you’re playing music at your party,
the distribution of how loud people like the

00:09:13.430 --> 00:09:18.550
music to be might have high kurtosis; There’s a lot of people who want it quiet so they

00:09:18.550 --> 00:09:21.580
can talk, and others who ...don’t want to
talk.

00:09:21.580 --> 00:09:26.100
Though it’s not as common, kurtosis, along
with all the other moments, can help us have

00:09:26.100 --> 00:09:29.160
more information about a random distribution.

00:09:29.160 --> 00:09:33.400
For example, it can help us tell whether a
variable follows a normal distribution.

00:09:33.400 --> 00:09:39.420
You can see the mean--the first moment--tells us where a distribution is on a number line.

00:09:39.420 --> 00:09:42.920
When you change the mean, you slide the distribution left or right.

00:09:42.920 --> 00:09:46.950
The other moments tell us about the shape
and spread of the distribution, which stay

00:09:46.950 --> 00:09:50.030
the same no matter where we move the distribution.

00:09:50.030 --> 00:09:54.051
So it might make sense that when we add two independent random variables together, like

00:09:54.051 --> 00:09:59.540
the sum of two dice rolls, the mean of this
new distribution is the sum of the means of

00:09:59.540 --> 00:10:01.710
the two distributions being added.

00:10:01.710 --> 00:10:04.730
And this is true no matter how many means you add.

00:10:04.730 --> 00:10:09.590
Maybe your stats teacher has said “The mean of the sum is the sum of the means”.

00:10:09.590 --> 00:10:15.820
Similarly the variance of the sum of two independent variables is the sum of their variances.

00:10:15.820 --> 00:10:19.760
So if we do want to look at the distribution
of the values of two dice rolls, we can easily

00:10:19.760 --> 00:10:22.400
calculate the mean and standard deviation.

00:10:22.400 --> 00:10:28.770
The mean of one die roll is (1+2+3+4+5+6)/6 or 3.5

00:10:28.770 --> 00:10:34.080
The mean of two dice rolls would be 7, since it’s the mean of the first roll, plus the

00:10:34.080 --> 00:10:35.080
mean of the second roll.

00:10:35.080 --> 00:10:39.931
The variance of the value of one roll is about 2.9 which means the variance of the value

00:10:39.931 --> 00:10:43.080
after rolling two dice is about 5.8.

00:10:43.080 --> 00:10:47.860
And as for those fries, we’d expect to get
about 336 grams if we ordered two larges.

00:10:47.860 --> 00:10:51.560
Randomness is the reason you can’t be sure you’ll win in Las Vegas, or why you always

00:10:51.561 --> 00:10:54.680
have to leave early because you can’t predict how long you’ll have to wait for a parking

00:10:54.680 --> 00:10:59.040
spot, or why sometimes you bring an umbrella with you on days when it doesn’t end up raining.

00:10:59.040 --> 00:11:02.880
But the beauty of statistics is that it helps
us know something about this randomness and

00:11:02.880 --> 00:11:07.160
make better, more informed choices in the
midst of chaotic randomness.

00:11:07.160 --> 00:11:12.450
Like deciding whether a machine learning algorithm trained to recognize sheep is truly better

00:11:12.450 --> 00:11:16.010
than humans at recognizing sheep in unusual places.

00:11:16.010 --> 00:11:22.280
Or even whether the increase you observed
in fecal matter on people’s hands is really

00:11:22.300 --> 00:11:25.920
higher after using air dryers than paper towels.

00:11:25.920 --> 00:11:28.680
Thanks for watching, I’ll see you next time.

