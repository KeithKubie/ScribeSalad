WEBVTT
Kind: captions
Language: en

00:00:00.002 --> 00:00:03.961
One-hot encoding works very well from most problems until you

00:00:03.961 --> 00:00:06.842
get into situations where you have tens of thousands, or

00:00:06.842 --> 00:00:08.224
even millions of classes.

00:00:08.224 --> 00:00:11.641
In that case, your vector becomes really, really large and

00:00:11.641 --> 00:00:15.890
has mostly zeros everywhere and that becomes very inefficient.

00:00:15.890 --> 00:00:19.840
You'll see later how we can deal with these problems using embeddings.

00:00:19.840 --> 00:00:24.180
What's nice about this approach is that we can now measure how well we're doing

00:00:24.180 --> 00:00:26.220
by simply comparing two vectors.

00:00:26.220 --> 00:00:29.430
One that comes out of your classifiers and contains the probabilities of your

00:00:29.430 --> 00:00:34.070
classes and the one-hot encoded vector that corresponds to your labels.

00:00:34.070 --> 00:00:36.030
Let's see how we can do this in practice.

00:00:36.030 --> 00:00:40.450
The natural way to measure the distance between those two probability vectors is

00:00:40.450 --> 00:00:42.079
called the Cross Entropy.

00:00:42.079 --> 00:00:44.795
I'll denote it by D here for distance.

00:00:44.795 --> 00:00:47.354
Math, it looks like this.

00:00:47.354 --> 00:00:50.067
Be careful, the cross entropy is not symmetric and

00:00:50.067 --> 00:00:51.582
you have a nasty log in there.

00:00:51.582 --> 00:00:54.011
So you have to make sure that your labels and

00:00:54.011 --> 00:00:56.516
your distributions are in the right place.

00:00:56.516 --> 00:01:00.212
Your labels, because they're one-hot encoded, will have a lot of

00:01:00.212 --> 00:01:03.540
zeroes in them and you don't want to take the log of zeroes.

00:01:03.540 --> 00:01:07.350
For your distribution, the softmax will always guarantee that you have a little

00:01:07.350 --> 00:01:11.650
bit of probability going everywhere, so you never really take a log of zero.

00:01:11.650 --> 00:01:14.490
So let's recap, because we have a lot of pieces already.

00:01:14.490 --> 00:01:19.290
So we have an input, it's going to be turned into logits using a linear model,

00:01:19.290 --> 00:01:22.800
which is basically your matrix multiply and a bias.

00:01:22.800 --> 00:01:26.244
We're then going to feed the logits, which are scores,

00:01:26.244 --> 00:01:29.205
into a softmax to turn them into probabilities.

00:01:29.205 --> 00:01:32.803
And then we're going to compare those probabilities to the one-hot encoded

00:01:32.803 --> 00:01:35.330
labels using the cross entropy function.

00:01:35.330 --> 00:01:39.310
This entire setting is often called multinomial logistic classification.

