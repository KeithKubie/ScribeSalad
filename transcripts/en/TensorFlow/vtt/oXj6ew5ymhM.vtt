WEBVTT
Kind: captions
Language: en

00:00:00.262 --> 00:00:03.261
♪ (music) ♪

00:00:04.594 --> 00:00:05.655
Hi everybody.

00:00:05.655 --> 00:00:08.681
Exciting event so far,
and it's great to see all the questions

00:00:08.681 --> 00:00:10.403
that have been asked on social media.

00:00:10.403 --> 00:00:13.226
Please keep asking them
and putting #TensorFlow.

00:00:13.226 --> 00:00:15.410
We're here today to answer them live
on the live stream.

00:00:15.410 --> 00:00:17.367
- I'm Laurence Moroney.
- And I'm Paige Bailey,

00:00:17.367 --> 00:00:19.806
and remember it is #AskTensorFlow.

00:00:19.806 --> 00:00:21.123
#AskTensorFlow.

00:00:21.123 --> 00:00:24.140
Because we're the TensorFlow
that you shall be asking.

00:00:24.140 --> 00:00:27.482
Exactly, so we're here to try and answer
as many questions as we can.

00:00:27.482 --> 00:00:29.803
Sorry if we don't get to them all
but we'll do our best.

00:00:29.803 --> 00:00:31.242
So shall we take the first question?

00:00:31.242 --> 00:00:32.523
Absolutely, let's go for it.

00:00:32.523 --> 00:00:35.230
So, one of the first questions
that we're going to cover,

00:00:35.230 --> 00:00:37.071
and it's one question
I bet you've had it today,

00:00:37.075 --> 00:00:39.929
I get it almost every time
I meet a TensorFlower,

00:00:39.929 --> 00:00:42.406
is it's great to be able to do training

00:00:42.406 --> 00:00:45.348
and like you'll usually do my training
for a fixed number of epochs,

00:00:45.348 --> 00:00:48.355
but what happens
when I reach my desired accuracy metric,

00:00:48.355 --> 00:00:49.767
how do I cancel training?

00:00:49.767 --> 00:00:52.846
Right, like it doesn't make any sense
to keep using Compute

00:00:52.846 --> 00:00:55.573
if you've already gotten to a point
where your boss would say,

00:00:55.573 --> 00:01:00.052
"Okay, cool, 99% accuracy--
that's fine for us today."

00:01:00.052 --> 00:01:02.374
(Laurence) So shall we take a look
at how we'd actually do that?

00:01:02.374 --> 00:01:03.541
Absolutely, let's go for it.

00:01:03.541 --> 00:01:06.470
So, I've opened up a colab here
that you can see

00:01:06.470 --> 00:01:08.025
where I'm using callbacks,

00:01:08.025 --> 00:01:10.316
and callbacks are the way
that you would actually achieve this.

00:01:10.316 --> 00:01:14.464
So at the top of my colab here,
you can see I have class <i>myCallback,</i>

00:01:14.464 --> 00:01:17.728
and on this one then,
when an epoch ends in training,

00:01:17.728 --> 00:01:19.463
I'm able to take a look at the logs,

00:01:19.463 --> 00:01:21.716
and if the accuracy log,
for example, in this case,

00:01:21.716 --> 00:01:22.754
is greater than 60%--

00:01:22.754 --> 00:01:24.817
- 'cause I have a really nice boss,
- (Paige laughs)

00:01:24.817 --> 00:01:27.667
(Laurence) he's really happy
when my training's 60% accurate--

00:01:27.667 --> 00:01:29.379
then I would actually cancel the training.

00:01:29.379 --> 00:01:31.271
And then to be able to set that up,

00:01:31.271 --> 00:01:33.927
I'll just say I'm going to
create an object called callbacks,

00:01:33.927 --> 00:01:36.125
which is an instance of my callback class,

00:01:36.125 --> 00:01:37.986
and I love the way in Colab
when I <i>Double Click</i>,

00:01:37.986 --> 00:01:41.409
it actually highlights,
it's just a little thing that I like.

00:01:41.409 --> 00:01:43.123
And then down here on callbacks,

00:01:43.123 --> 00:01:45.271
I'll just say callbacks equals callbacks.

00:01:45.271 --> 00:01:47.171
And then when I actually do the training,

00:01:47.171 --> 00:01:49.379
and you know what,
I'm going to really show off

00:01:49.379 --> 00:01:53.148
and I'm going to make my runtime type
to be GPU so it goes nice and fast.

00:01:53.148 --> 00:01:54.448
So this is Fashion-MNIST.

00:01:54.448 --> 00:01:57.742
Let's do a little bit of training
on Fashion-MNIST with this one.

00:01:57.742 --> 00:02:00.870
And we're doing this live
so I'm connecting up to the VM.

00:02:00.870 --> 00:02:03.109
And here we go,
now it's actually training.

00:02:03.109 --> 00:02:04.641
It's getting ready to start.

00:02:04.641 --> 00:02:06.459
- It's on the first epoch.
- (Paige) It's showing you your RAM

00:02:06.459 --> 00:02:07.718
and disk utilization.

00:02:07.718 --> 00:02:10.998
(Laurence) We're on the first epoch.
The first epoch is progressing away,

00:02:10.998 --> 00:02:13.570
60,000 images being trained,

00:02:13.570 --> 00:02:15.921
and boom, I hit accuracy of 83%.

00:02:15.921 --> 00:02:18.203
- (Paige) That's pretty good.
- In just one epoch, right?

00:02:18.203 --> 00:02:21.133
But we can see now
that it actually reached 60% accuracy

00:02:21.133 --> 00:02:22.634
so it canceled the training.

00:02:22.634 --> 00:02:25.339
So callbacks are your friends
if you're doing this.

00:02:25.339 --> 00:02:27.569
Certainly when you're learning,
when you're experimenting,

00:02:27.569 --> 00:02:30.937
I used to,
before I learned about callbacks--

00:02:30.937 --> 00:02:32.189
I keep saying colabs--

00:02:32.189 --> 00:02:34.034
before I learned about callbacks,

00:02:34.034 --> 00:02:36.566
I would set something up
to train for a hundred epochs

00:02:36.566 --> 00:02:39.348
and then go and go to sleep,
and then wake up the next morning

00:02:39.348 --> 00:02:41.592
and find like after three epochs,
it had done its job,

00:02:41.592 --> 00:02:42.805
and I'd wasted my time.

00:02:42.805 --> 00:02:45.223
So, use callbacks, I think,
would be the answer to that.

00:02:45.223 --> 00:02:48.479
Absolutely, keras callbacks
are incredibly valuable.

00:02:48.479 --> 00:02:51.031
It doesn't just apply to accuracy either.

00:02:51.031 --> 00:02:52.961
There are a bunch of additional metrics

00:02:52.961 --> 00:02:55.617
that could be useful
for your particular workflow.

00:02:55.617 --> 00:02:59.231
And this code also would work
in TensorFlow 2.0.

00:02:59.231 --> 00:03:01.850
- So it's keras--
- (Laurence) Absolutely.

00:03:01.850 --> 00:03:04.023
- (Paige) Gosh I love keras--
- (Laurence) Keras, yes.

00:03:04.023 --> 00:03:05.669
This is a keras love affair right now.

00:03:05.669 --> 00:03:09.303
And actually one of the really neat things
about keras that you may not realize,

00:03:09.303 --> 00:03:11.061
and we've been talking about
TensorFlow 2.0,

00:03:11.061 --> 00:03:14.563
is that the same code that you write
for TensorFlow 1.x

00:03:14.563 --> 00:03:16.371
is the same code for 2.0

00:03:16.371 --> 00:03:20.091
but what's going on behind the scenes
is that it's executing equally in 2.0,

00:03:20.091 --> 00:03:21.280
instead of a graph mode.

00:03:21.280 --> 00:03:24.912
So even though this colab
I think I'm running at 1.13,

00:03:24.912 --> 00:03:28.122
this code will actually still run in 2.0,
without you modifying the code.

00:03:28.122 --> 00:03:29.124
(Paige) Absolutely.

00:03:29.124 --> 00:03:31.541
Alright, so shall we take
the next question?

00:03:31.541 --> 00:03:32.536
Oh, cool.

00:03:32.536 --> 00:03:36.986
So, our next question
is from Twitter it looks like,

00:03:36.986 --> 00:03:39.656
and "What about all the web developers

00:03:39.656 --> 00:03:41.052
who are new to AI,

00:03:41.052 --> 00:03:43.910
does the version 2.0
help them get started?"

00:03:43.910 --> 00:03:45.300
Oh, web developers.

00:03:45.300 --> 00:03:46.810
They are some-- oh, man.

00:03:46.810 --> 00:03:49.668
So I just got finished
talking to two of the folks

00:03:49.668 --> 00:03:51.966
on the TensorFlow.js--
and you just pulled it up--

00:03:51.966 --> 00:03:53.785
the TensorFlow.js team,

00:03:53.785 --> 00:03:55.617
about all of the cool new demos

00:03:55.617 --> 00:03:57.745
that they've seen
arise from the community.

00:03:57.745 --> 00:04:01.322
It's really such a vibrant ecosystem
of artists and creators

00:04:01.322 --> 00:04:05.111
that are using browser-based
or even server-based tools now,

00:04:05.111 --> 00:04:08.991
to create these machine learning models,

00:04:08.991 --> 00:04:10.241
training and running them.

00:04:10.241 --> 00:04:12.107
Yep, so I think for web developers

00:04:12.107 --> 00:04:15.425
there's a whole bunch of ways
that you can get started with this.

00:04:15.425 --> 00:04:16.793
So you've mentioned TensorFlow.js

00:04:16.793 --> 00:04:18.687
so let's talk about that
for a little bit first.

00:04:18.687 --> 00:04:20.967
The TensorFlow.js,
it's a JavaScript library

00:04:20.967 --> 00:04:24.498
and this JavaScript library will allow you
to train models in the browser,

00:04:24.498 --> 00:04:25.936
as well as executing them.

00:04:25.936 --> 00:04:28.593
And that actually blew my mind
when I first heard about it.

00:04:28.593 --> 00:04:29.835
The node bindings,

00:04:29.835 --> 00:04:35.234
like being able to use the GPU
inside your laptop with Google Chrome

00:04:35.234 --> 00:04:38.399
or your favorite flavor of browser

00:04:38.399 --> 00:04:40.014
to train a model.

00:04:40.014 --> 00:04:41.824
That is absurd!

00:04:41.824 --> 00:04:43.574
- (Laurence) Sci-fi now, right?
- Yeah, it is.

00:04:43.574 --> 00:04:44.644
We live in the future.

00:04:44.644 --> 00:04:46.251
So like you said, node bindings as well,

00:04:46.251 --> 00:04:49.615
so like with Node.js
so it's not just in browser JavaScript,

00:04:49.615 --> 00:04:52.267
it's also server side JavaScript
with Node, right?

00:04:52.267 --> 00:04:54.871
And am I supposed to say Node
or Node.js or--

00:04:54.871 --> 00:04:55.998
(Paige) I don't know.

00:04:55.998 --> 00:04:57.720
I'll say Node.

00:04:57.720 --> 00:05:00.642
And then of course, there's,
by the fact that it is in Node,

00:05:00.642 --> 00:05:01.817
one of my personal favorites,

00:05:01.817 --> 00:05:03.903
are you familiar with
Cloud Functions for Firebase?

00:05:03.903 --> 00:05:05.419
I'm not. Tell me more.

00:05:05.419 --> 00:05:06.524
I'm intrigued.

00:05:06.524 --> 00:05:08.329
So, I used to work on the Firebase team,

00:05:08.329 --> 00:05:10.086
so a shout out
to all my friends at Firebase.

00:05:10.086 --> 00:05:11.864
- Alright, I'm leaving.
- (laughs)

00:05:11.864 --> 00:05:12.875
No, I'm just kidding.

00:05:12.875 --> 00:05:14.731
I've heard so many good things
about Firebase.

00:05:14.731 --> 00:05:17.430
So it's for mobile developers
and for web developers.

00:05:17.430 --> 00:05:19.534
And one of the things
that Firebase gives you

00:05:19.534 --> 00:05:21.641
are these things called
Cloud Functions for Firebase.

00:05:21.641 --> 00:05:23.608
I've called up the webpage here
with the URL.

00:05:23.608 --> 00:05:26.523
But in summary,
what they do is that they allow you

00:05:26.523 --> 00:05:28.369
to execute functions on a backend

00:05:28.369 --> 00:05:31.292
without you needing to maintain
a server infrastructure,

00:05:31.292 --> 00:05:35.196
and allow you to execute these
in response to a trigger.

00:05:35.196 --> 00:05:38.611
So a trigger might be, for example,
an analytic event or a signin event.

00:05:38.611 --> 00:05:41.102
(Paige) Or you get new data,
and you need to process it.

00:05:41.102 --> 00:05:43.067
- (Laurence) Bingo!
- (Paige) Man, I should try this out

00:05:43.067 --> 00:05:44.446
for machine learning stuff.

00:05:44.446 --> 00:05:47.173
(Laurence) So now,
the fact that they run JavaScript code

00:05:47.173 --> 00:05:48.292
Node on the backend,

00:05:48.292 --> 00:05:51.659
now it's a case of you can actually
train models in a Cloud Function,

00:05:51.659 --> 00:05:53.045
which just-- for me.

00:05:53.045 --> 00:05:54.048
That's amazing.

00:05:54.048 --> 00:05:57.298
So, web developers,
there's lots of great options for you,

00:05:57.298 --> 00:05:58.655
however it is you want to do it,

00:05:58.655 --> 00:06:01.097
in the browser, on the backend,
in mobile, that kind of stuff,

00:06:01.097 --> 00:06:04.175
hopefully there's lots of great stuff
that you'll be able to get started with.

00:06:04.175 --> 00:06:07.539
Absolutely, and the question
about TensorFlow 2.0

00:06:07.539 --> 00:06:12.528
and whether it gives additional tools
for application developers,

00:06:12.528 --> 00:06:16.032
I think it would mostly be
in terms of those codes and tutorials

00:06:16.032 --> 00:06:17.425
that we were mentioning before.

00:06:17.425 --> 00:06:19.296
We've also released some courses,

00:06:19.296 --> 00:06:23.217
so it's easier than ever to get started.

00:06:23.217 --> 00:06:26.239
And the models
that you create using saved model

00:06:26.239 --> 00:06:29.703
can be deployed to TFLite,
to TensorFlow.js, to whatever.

00:06:29.703 --> 00:06:32.258
And the important thing is
we've been talking a lot about Keras--

00:06:32.258 --> 00:06:33.528
this thing that we love--

00:06:33.528 --> 00:06:36.797
and the keras layers
are supported in TensorFlow.js,

00:06:36.797 --> 00:06:38.891
so it's not just for Python developers.

00:06:38.891 --> 00:06:41.093
If you're a JS developer
you can define your layers.

00:06:41.093 --> 00:06:42.690
And an R developer.

00:06:42.690 --> 00:06:45.835
They have Keras for R
which is awesome.

00:06:45.835 --> 00:06:48.312
It was created by J.J. Allaire
and Francois Chollet,

00:06:48.312 --> 00:06:50.365
they have a book out about it.

00:06:50.365 --> 00:06:51.926
(Laurence) Nice, cool.

00:06:51.926 --> 00:06:53.768
So web developers,
lots of options for you.

00:06:53.768 --> 00:06:54.819
- Yep.
- Right, yep.

00:06:54.819 --> 00:06:56.134
Shall we take the next question?

00:06:56.134 --> 00:06:58.245
And this looks like
it also came from Twitter,

00:06:58.245 --> 00:07:02.421
and it's "Are there any TensorFlow.js
transfer learning examples

00:07:02.421 --> 00:07:03.641
for object detection?"

00:07:03.641 --> 00:07:07.959
So TensorFlow.js is popular,
we have learned.

00:07:07.959 --> 00:07:10.566
Yes, so object detection.

00:07:10.566 --> 00:07:12.921
So, how do we answer this one?

00:07:12.921 --> 00:07:15.869
So, it depends on what you mean
by object detection

00:07:15.869 --> 00:07:18.527
because in Google
we talk about object detection.

00:07:18.527 --> 00:07:22.549
We use that specific term for in an image
where you got lots of obejcts

00:07:22.549 --> 00:07:24.451
and you put bounding boxes
around them, right?

00:07:24.451 --> 00:07:26.819
Right now there are no samples for that.

00:07:26.819 --> 00:07:28.555
- Sorry.
- (Paige) No there aren't,

00:07:28.555 --> 00:07:32.426
but and lovely thing is
that the community is incredibly adept

00:07:32.426 --> 00:07:34.250
at creating TensorFlow.js examples.

00:07:34.250 --> 00:07:37.025
So example, the Teropa's CodePens.

00:07:37.025 --> 00:07:40.734
And then also Victor Dibia,
machine learning Google developer expert,

00:07:40.734 --> 00:07:45.365
had a great recent example
with using it to track hand movements

00:07:45.365 --> 00:07:46.343
in a browser.

00:07:46.343 --> 00:07:48.638
So, the question there
was really about transfer learning,

00:07:48.638 --> 00:07:51.285
and I think one of the things
that even though we don't have a demo

00:07:51.285 --> 00:07:52.938
of transfer learning or object detection,

00:07:52.938 --> 00:07:54.845
I'd like to show a demo
of transfer learning

00:07:54.845 --> 00:07:57.799
with being able to detect
a single item within a frame.

00:07:57.799 --> 00:08:00.533
So, we call that an image classification.

00:08:00.533 --> 00:08:02.595
So, can I roll the demo?

00:08:02.595 --> 00:08:03.612
Please do.

00:08:03.612 --> 00:08:06.061
- Are you going to do your favorite?
- I'm going to do Pac-Man.

00:08:06.061 --> 00:08:09.165
- Oh, you are-- I should've known.
- I'm a child of the '80s; I love Pac-Man.

00:08:09.165 --> 00:08:12.027
If you look carefully it says, actually,
"loading mobile net now."

00:08:12.027 --> 00:08:14.164
So what's happening
is that just downloaded

00:08:14.164 --> 00:08:15.678
the Mobilenet model.

00:08:15.678 --> 00:08:18.395
So what I'm going to do
is I'm going to add some new classes

00:08:18.395 --> 00:08:20.940
to the Mobilenet model and then 
use transfer learning to get them.

00:08:20.940 --> 00:08:22.751
(Paige) Will it see my finger--
there it goes.

00:08:22.751 --> 00:08:23.790
Do you want to do it?

00:08:23.790 --> 00:08:26.014
(Paige) No, no, no.
Go for it, show me how.

00:08:26.014 --> 00:08:27.423
So, Pac-Man-- old arcade game--

00:08:27.423 --> 00:08:30.420
you move up, down, left, and right,
and you try and run away from the ghost.

00:08:30.420 --> 00:08:33.644
So I'm going to try and train it
to move up when I point up like this.

00:08:33.644 --> 00:08:36.090
So I'm holding it down
and I'm gathering a bunch of samples.

00:08:36.090 --> 00:08:37.763
There're about 50 samples,

00:08:37.763 --> 00:08:39.446
and then when I go right like this,

00:08:39.446 --> 00:08:41.177
I didn't really
think this one through, though,

00:08:41.177 --> 00:08:43.021
'cause then turning left
is going to be hard.

00:08:43.021 --> 00:08:45.297
But, bear with me, and like 15.

00:08:45.297 --> 00:08:48.537
Maybe I'll do left like this
and get my head out of the way.

00:08:48.537 --> 00:08:49.932
(Paige laughs)

00:08:49.932 --> 00:08:51.508
A few samples like that,

00:08:51.508 --> 00:08:54.715
and then down will look like this.

00:08:54.715 --> 00:08:57.666
Hopefully, these aren't rude gestures
in some country.

00:08:57.666 --> 00:08:59.094
And something like that.

00:08:59.094 --> 00:09:01.295
So I have now picked
like 50 samples of these,

00:09:01.295 --> 00:09:05.029
and I'm going to retrain this mode
in the browser with transfer learning.

00:09:05.029 --> 00:09:07.770
So if you look over on the left here,
my learning rate, my batch size,

00:09:07.770 --> 00:09:09.474
I'm just going to train it for 20 epochs.

00:09:09.474 --> 00:09:12.825
And I'm going to start training
and we'll see, it starts going quickly.

00:09:12.825 --> 00:09:15.458
You see my loss started at four
and then went down--

00:09:15.458 --> 00:09:16.611
now it's at zero.

00:09:16.611 --> 00:09:20.420
So, that's like wow, it's probably a digit
beyond the six digits,

00:09:20.420 --> 00:09:22.010
it's never actually at zero,

00:09:22.010 --> 00:09:23.475
but we see we have a very low loss.

00:09:23.475 --> 00:09:24.957
So now we can actually give it a try.

00:09:24.957 --> 00:09:27.622
So let me see if I can avoid
getting eaten by ghosts.

00:09:27.622 --> 00:09:29.545
So, I'm going to start playing the game,

00:09:29.545 --> 00:09:30.785
and I'm going to move left,

00:09:30.785 --> 00:09:32.755
and you can see
the bounding box around it,

00:09:32.755 --> 00:09:34.767
kind of shows that--

00:09:34.767 --> 00:09:36.925
Up! Up.

00:09:36.925 --> 00:09:38.523
No, okay.

00:09:38.523 --> 00:09:39.937
Up! Right!

00:09:39.937 --> 00:09:41.204
- No, go right.
- (Paige) Oh no!

00:09:41.204 --> 00:09:43.632
(Laurence) I'm watching the screen
instead of watching Pac-Man

00:09:43.632 --> 00:09:45.589
but we can see now
that I've actually trained it.

00:09:45.589 --> 00:09:47.018
Let's try going right this time.

00:09:47.018 --> 00:09:49.174
Come on, right, there we go.

00:09:49.174 --> 00:09:50.378
And up.

00:09:50.378 --> 00:09:51.378
It thinks it's down.

00:09:51.378 --> 00:09:52.633
There we go, up, and right.

00:09:52.633 --> 00:09:53.660
Ah!

00:09:53.660 --> 00:09:55.281
We see, I'm not 
very good at this game.

00:09:55.281 --> 00:09:56.968
I wasn't even good at it
with the joystick.

00:09:56.968 --> 00:09:58.978
(Paige) You're just using this
as an excuse to play Pac-Man all day,

00:09:58.978 --> 00:10:01.182
- aren't you?
- Exactly!

00:10:01.182 --> 00:10:03.889
But there is just a great example
of using transfer learning.

00:10:03.889 --> 00:10:05.946
So if you can take this sample apart,

00:10:05.946 --> 00:10:07.751
and we have some other samples
that are out there

00:10:07.751 --> 00:10:09.307
for transfer learning in JavaScript,

00:10:09.307 --> 00:10:13.015
so you can just see how easy it was
for us to be able to extract the features

00:10:13.015 --> 00:10:15.099
from Mobilenet and then retrain it.

00:10:15.099 --> 00:10:16.789
It's actually moving as I'm talking,

00:10:16.789 --> 00:10:18.833
- (laughs)
- all my gestures to be able to use that.

00:10:18.833 --> 00:10:21.362
So, enough on Pac-Man,
shall we move to the next question?

00:10:21.362 --> 00:10:22.405
(Paige) Absolutely.

00:10:22.405 --> 00:10:27.056
So, and I'd also want to point out
that transfer learning can be used

00:10:27.056 --> 00:10:31.126
for a variety of use cases
other than just images too.

00:10:31.126 --> 00:10:33.915
So make sure to check out
all of the great examples

00:10:33.915 --> 00:10:35.691
that we've got listed on the website.

00:10:35.691 --> 00:10:36.896
Sounds good.

00:10:36.896 --> 00:10:39.828
And the next one also looks like
it was from Twitter--

00:10:39.828 --> 00:10:41.306
Twitter must be very popular.

00:10:41.306 --> 00:10:42.970
- I like Twitter.
- I <i>love</i> Twitter.

00:10:42.970 --> 00:10:45.405
Are you going to publish
the updated version

00:10:45.405 --> 00:10:48.140
of TensorFlow for Poets tutorial,
from Pete Warden.

00:10:48.140 --> 00:10:51.808
implementing TF 2.0, TFLite 2.0,

00:10:51.808 --> 00:10:54.071
and a lot of other shenanigans.

00:10:54.071 --> 00:10:55.541
Yeah, the neural network API.

00:10:55.541 --> 00:10:57.285
Faster inference on Android.

00:10:57.285 --> 00:11:00.381
Yeah, and I love Pete Warden's codelab

00:11:00.381 --> 00:11:03.751
on TensorFlow for Poets.

00:11:03.751 --> 00:11:05.137
He also had a great talk today.

00:11:05.137 --> 00:11:07.159
Oh, I didn't get to see it.

00:11:07.159 --> 00:11:08.521
Do you want to take this question?

00:11:08.521 --> 00:11:11.184
Sure, so the TensorFlow for Poets codelab,

00:11:11.184 --> 00:11:12.785
at some point we will update it.

00:11:12.785 --> 00:11:15.277
I don't think there's an updated version
available right now.

00:11:15.277 --> 00:11:19.785
But one of the things that I really liked
about the TensorFlow for Poets codelab

00:11:19.785 --> 00:11:22.890
was it got me building a model
very quickly that I could then use

00:11:22.890 --> 00:11:24.402
on a mobile device.

00:11:24.402 --> 00:11:28.646
But the drawback of that
was it was a bunch of scripts that I ran

00:11:28.646 --> 00:11:30.730
and I didn't really know
what was going on with them.

00:11:30.730 --> 00:11:34.170
So one of the things that we've been doing
is that we've decided to get a whole bunch

00:11:34.170 --> 00:11:36.189
of new TensorFlow Lite examples

00:11:36.189 --> 00:11:37.683
and put them online on the site.

00:11:37.683 --> 00:11:39.077
And I have them on here.

00:11:39.077 --> 00:11:40.136
So there's four new ones--

00:11:40.136 --> 00:11:42.174
gesture recognition, image classification,

00:11:42.174 --> 00:11:44.533
object detection, and speech recognition.

00:11:44.533 --> 00:11:46.507
And what's nice about these
is they're all open sourced,

00:11:46.507 --> 00:11:48.274
they're both Android and iOS,

00:11:48.274 --> 00:11:51.308
and they include full instructions
on how to build them for yourself.

00:11:51.308 --> 00:11:53.366
The image classification one 
is really fun.

00:11:53.366 --> 00:11:57.462
I'm actually going to try to run that
in my-- whoops, I don't want Bitly--

00:11:57.462 --> 00:12:02.280
I actually want to try and run that
in my Android emulator.

00:12:02.280 --> 00:12:05.154
So we can see it running
in an emulated environment.

00:12:05.154 --> 00:12:06.634
So let me get that started.

00:12:06.634 --> 00:12:09.011
Oh, we can see it being cached.

00:12:09.011 --> 00:12:11.183
So, for example, now here
I'm actually running it.

00:12:11.183 --> 00:12:13.399
It's doing a classification
of what's in the background.

00:12:13.399 --> 00:12:14.799
Like, if I hold up a water bottle--

00:12:14.799 --> 00:12:16.126
whoops, this way.

00:12:16.126 --> 00:12:18.730
There we go, see,
it actually detects it's a water bottle.

00:12:18.730 --> 00:12:21.854
Now this is running
in the Android emulator.

00:12:21.854 --> 00:12:23.462
This is using TensorFlow Lite,

00:12:23.462 --> 00:12:25.210
and this is the sample that's on there

00:12:25.210 --> 00:12:27.666
that basically does the same thing
that you would have seen

00:12:27.666 --> 00:12:29.995
in TensorFlow for Poets
where it's using Mobilenet

00:12:29.995 --> 00:12:32.335
and building an application
around Mobilenet.

00:12:32.335 --> 00:12:34.196
But if you look,
even running in the emulator,

00:12:34.196 --> 00:12:37.460
I'm getting inference times
in the 100, 170 milliseconds.

00:12:37.460 --> 00:12:39.862
- (Paige) It's so fast.
- (Laurence) How cool is that, right?

00:12:39.862 --> 00:12:42.153
(Paige) The ability to be able
to take large-scale models

00:12:42.153 --> 00:12:45.133
and pull them down to a manageable size

00:12:45.133 --> 00:12:47.873
on a mobile
or an embedded device, is huge.

00:12:47.873 --> 00:12:50.481
I'm really excited to see
what TensorFlow Lite does this year.

00:12:50.481 --> 00:12:52.841
Yep, so we're working on
a bunch of new tutorials,

00:12:52.841 --> 00:12:54.166
those samples are out there.

00:12:54.166 --> 00:12:56.503
If you take a look at their GitHub page,

00:12:56.503 --> 00:13:00.954
you'll see that there's details
on how it's built.

00:13:00.954 --> 00:13:02.107
Let me just go back on here.

00:13:02.107 --> 00:13:03.986
So, for example,
if I say "Explore on Android,"

00:13:03.986 --> 00:13:05.881
you'll see there's details
on how it's built,

00:13:05.881 --> 00:13:07.164
how you can put it all together,

00:13:07.164 --> 00:13:08.302
how you can compile it.

00:13:08.302 --> 00:13:11.624
You don't need to build TensorFlow
in order to use TensorFlow Lite--

00:13:11.624 --> 00:13:14.407
that was one bit of confusion
that folks had in the past.

00:13:14.407 --> 00:13:16.959
Now, it's just a case of what you add
to your <i>build.gradle</i>,

00:13:16.959 --> 00:13:18.248
or if you're an iOS developer,

00:13:18.248 --> 00:13:19.992
the pods that you add,
that kind of stuff.

00:13:19.992 --> 00:13:23.314
So you can go and start kicking the tires
on these applications for yourself.

00:13:23.314 --> 00:13:24.587
(Paige) Excellent.

00:13:24.587 --> 00:13:25.943
(Laurence) Alrighty.

00:13:26.383 --> 00:13:28.154
But we will have more codelabs

00:13:28.154 --> 00:13:32.281
and I would love to get Pete's
TensorFlow for Poets codelab updated,

00:13:32.281 --> 00:13:33.378
hopefully for IO.

00:13:33.378 --> 00:13:34.391
Yes.

00:13:34.391 --> 00:13:37.386
♪ (music) ♪

