WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.784
[MUSIC PLAYING]

00:00:05.110 --> 00:00:06.830
JACQUELINE PAN: Hi, everyone.

00:00:06.830 --> 00:00:10.210
I'm Jackie, and I'm the Lead
Program Manager on ML Fairness

00:00:10.210 --> 00:00:12.170
here at Google.

00:00:12.170 --> 00:00:14.430
So what is ML fairness?

00:00:14.430 --> 00:00:16.990
As some of you may
know, Google's mission

00:00:16.990 --> 00:00:19.210
is to organize the
world's information

00:00:19.210 --> 00:00:22.540
and make it universally
accessible and useful.

00:00:22.540 --> 00:00:25.260
Every one of our users
gives us their trust.

00:00:25.260 --> 00:00:28.510
And it's our responsibility
to do right by them.

00:00:28.510 --> 00:00:31.120
And as the impact
and reach of AI

00:00:31.120 --> 00:00:33.610
has grown across
societies and sectors,

00:00:33.610 --> 00:00:35.770
it's critical to ethically
design and deploy

00:00:35.770 --> 00:00:38.950
these systems in a
fair and inclusive way.

00:00:38.950 --> 00:00:41.530
Addressing fairness in
AI is an active area

00:00:41.530 --> 00:00:43.900
of research at
Google, from fostering

00:00:43.900 --> 00:00:46.120
a diverse and inclusive
workforce that

00:00:46.120 --> 00:00:50.050
embodies critical and diverse
knowledge to training models

00:00:50.050 --> 00:00:53.200
to remove or correct
problematic biases.

00:00:53.200 --> 00:00:56.080
There is no standard
definition of fairness,

00:00:56.080 --> 00:00:59.740
whether decisions are made
by humans or by machines.

00:00:59.740 --> 00:01:02.470
Far from a solved
problem, fairness in AI

00:01:02.470 --> 00:01:06.880
presents both an
opportunity and a challenge.

00:01:06.880 --> 00:01:09.610
Last summer, Google
outlined principles

00:01:09.610 --> 00:01:12.910
to guide the responsible
development and use of AI.

00:01:12.910 --> 00:01:15.670
One of them directly
speaks to ML fairness

00:01:15.670 --> 00:01:18.010
and making sure that our
technologies don't create

00:01:18.010 --> 00:01:21.140
or reinforce unfair bias.

00:01:21.140 --> 00:01:23.210
The principles
further state that we

00:01:23.210 --> 00:01:26.030
seek to avoid unjust
impacts on people related

00:01:26.030 --> 00:01:30.090
to sensitive characteristics
such as race, ethnicity,

00:01:30.090 --> 00:01:35.420
gender, nationality, income,
sexual orientation, ability,

00:01:35.420 --> 00:01:37.850
and political or
religious belief.

00:01:37.850 --> 00:01:40.610
Now let's take a look at how
unfair bias might be created

00:01:40.610 --> 00:01:42.920
or reinforced.

00:01:42.920 --> 00:01:44.930
An important step
on that path is

00:01:44.930 --> 00:01:47.330
acknowledging that humans are
at the center of technology

00:01:47.330 --> 00:01:50.510
design, in addition to
being impacted by it.

00:01:50.510 --> 00:01:53.510
And humans have not always
made product design decisions

00:01:53.510 --> 00:01:57.110
that are in line with
the needs of everyone.

00:01:57.110 --> 00:02:01.070
For example, because female
body-type crash test dummies

00:02:01.070 --> 00:02:04.490
weren't required until
2011, female drivers

00:02:04.490 --> 00:02:06.380
were more likely
than male drivers

00:02:06.380 --> 00:02:09.692
to be severely injured
in an accident.

00:02:09.692 --> 00:02:13.020
Band-Aids have long been
manufactured in a single

00:02:13.020 --> 00:02:13.700
color--

00:02:13.700 --> 00:02:15.380
a soft pink.

00:02:15.380 --> 00:02:17.610
In this tweet, you see
the personal experience

00:02:17.610 --> 00:02:20.640
of an individual using a
Band-Aid that matches his skin

00:02:20.640 --> 00:02:22.770
tone for the first time.

00:02:22.770 --> 00:02:26.010
A product that's designed and
intended for widespread use

00:02:26.010 --> 00:02:28.950
shouldn't fail for an
individual because of something

00:02:28.950 --> 00:02:31.110
that they can't change
about themselves.

00:02:31.110 --> 00:02:36.090
Products and technology
should just work for everyone.

00:02:36.090 --> 00:02:38.310
These choices may not
have been deliberate,

00:02:38.310 --> 00:02:40.350
but they still
reinforce the importance

00:02:40.350 --> 00:02:41.970
of being thoughtful
about technology

00:02:41.970 --> 00:02:46.460
design and the impact
it may have on humans.

00:02:46.460 --> 00:02:48.680
Why does Google care
about these problems?

00:02:48.680 --> 00:02:51.590
Well, our users are
diverse, and it's important

00:02:51.590 --> 00:02:53.720
that we provide an
experience that works equally

00:02:53.720 --> 00:02:56.030
well across all of our users.

00:02:56.030 --> 00:02:59.540
The good news is that
humans, you, have the power

00:02:59.540 --> 00:03:01.940
to approach these
problems differently,

00:03:01.940 --> 00:03:04.160
and to create technology
that is fair and more

00:03:04.160 --> 00:03:06.380
inclusive for more people.

00:03:06.380 --> 00:03:09.380
I'll give you a sense
of what that means.

00:03:09.380 --> 00:03:11.450
Take a look at these images.

00:03:11.450 --> 00:03:15.030
You'll notice where the
label "wedding" was applied

00:03:15.030 --> 00:03:18.770
to the images on the left,
and where it wasn't, the image

00:03:18.770 --> 00:03:20.210
on the right.

00:03:20.210 --> 00:03:22.370
The labels in these
photos demonstrate

00:03:22.370 --> 00:03:25.760
how one open source image
classifier trained on the Open

00:03:25.760 --> 00:03:29.810
Images Dataset does not properly
recognize wedding traditions

00:03:29.810 --> 00:03:32.320
from different
parts of the world.

00:03:32.320 --> 00:03:35.630
Open datasets, like open images,
are a necessary and critical

00:03:35.630 --> 00:03:38.270
part of developing
useful ML models,

00:03:38.270 --> 00:03:40.310
but some open
source datasets have

00:03:40.310 --> 00:03:43.070
been found to be geographically
skewed based on how

00:03:43.070 --> 00:03:45.230
and where they were collected.

00:03:45.230 --> 00:03:47.300
To bring greater
geographic diversity

00:03:47.300 --> 00:03:50.660
to open images, last year, we
enabled the global community

00:03:50.660 --> 00:03:53.840
of crowdsourced app users
to photograph the world

00:03:53.840 --> 00:03:56.660
around them and make their
photos available to researchers

00:03:56.660 --> 00:04:01.010
and developers as a part of the
Open Images Extended Dataset.

00:04:01.010 --> 00:04:04.140
We know that this is just an
early step on a long journey.

00:04:04.140 --> 00:04:06.080
And to build
inclusive ML products,

00:04:06.080 --> 00:04:08.870
training data must
represent global diversity

00:04:08.870 --> 00:04:12.140
along several dimensions.

00:04:12.140 --> 00:04:15.060
These are complex
sociotechnical challenges,

00:04:15.060 --> 00:04:18.320
and they need to be interrogated
from many different angles.

00:04:18.320 --> 00:04:20.140
It's about problem
formation and how

00:04:20.140 --> 00:04:23.685
you think about these systems
with human impact in mind.

00:04:23.685 --> 00:04:25.810
Let's talk a little bit
more about these challenges

00:04:25.810 --> 00:04:29.910
and where they can
manifest in an ML pipeline.

00:04:29.910 --> 00:04:33.030
Unfairness can enter the
system at any point in the ML

00:04:33.030 --> 00:04:37.170
pipeline, from data collection
and handling to model training

00:04:37.170 --> 00:04:38.670
to end use.

00:04:38.670 --> 00:04:42.840
Rarely can you identify a single
cause of or a single solution

00:04:42.840 --> 00:04:44.580
to these problems.

00:04:44.580 --> 00:04:48.330
Far more often, various
causes interact in ML systems

00:04:48.330 --> 00:04:50.340
to produce problematic outcomes.

00:04:50.340 --> 00:04:52.900
And a range of
solutions is needed.

00:04:52.900 --> 00:04:54.870
We try to disentangle
these interactions

00:04:54.870 --> 00:05:00.310
to identify root causes
and to find ways forward.

00:05:00.310 --> 00:05:04.450
This approach spans more than
just one team or discipline.

00:05:04.450 --> 00:05:07.840
ML fairness is an initiative to
help address these challenges.

00:05:07.840 --> 00:05:09.910
And it takes a lot of
different individuals

00:05:09.910 --> 00:05:12.460
with different
backgrounds to do this.

00:05:12.460 --> 00:05:15.280
We need to ask ourselves
questions like, how do people

00:05:15.280 --> 00:05:18.730
feel about fairness when they're
interacting with an ML system?

00:05:18.730 --> 00:05:22.060
How can you make systems
more transparent to users?

00:05:22.060 --> 00:05:26.830
And what's the societal
impact of an ML system?

00:05:26.830 --> 00:05:29.330
Bias problems run deep,
and they don't always

00:05:29.330 --> 00:05:31.370
manifest in the same way.

00:05:31.370 --> 00:05:33.800
As a result, we've had to
learn different techniques

00:05:33.800 --> 00:05:35.810
of addressing these challenges.

00:05:35.810 --> 00:05:38.060
Now we'll walk through some
of the lessons that Google

00:05:38.060 --> 00:05:40.600
has learned in evaluating
and improving our products,

00:05:40.600 --> 00:05:42.350
as well as tools and
techniques that we're

00:05:42.350 --> 00:05:43.910
developing in this race.

00:05:43.910 --> 00:05:46.773
Here to tell you more
about this is Tulsee.

00:05:46.773 --> 00:05:47.690
TULSEE DOSHI: Awesome.

00:05:47.690 --> 00:05:49.040
Thanks, Jackie.

00:05:49.040 --> 00:05:50.160
Hi, everyone.

00:05:50.160 --> 00:05:53.090
My name is Tulsee, and I lead
product for the ML Fairness

00:05:53.090 --> 00:05:54.810
effort here at Google.

00:05:54.810 --> 00:05:57.680
Today, I'll talk about three
different angles in which we've

00:05:57.680 --> 00:05:59.420
thought about and
acted on fairness

00:05:59.420 --> 00:06:01.385
concerns in our
products, and the lessons

00:06:01.385 --> 00:06:03.200
that we've learned from that.

00:06:03.200 --> 00:06:06.530
We'll also walk through our next
steps, tools, and techniques

00:06:06.530 --> 00:06:08.072
that we're developing.

00:06:08.072 --> 00:06:09.530
Of course, we know
that the lessons

00:06:09.530 --> 00:06:11.300
we're going to talk
about today are only

00:06:11.300 --> 00:06:13.980
some of the many ways
of tackling the problem.

00:06:13.980 --> 00:06:16.370
In fact, as you heard in
the keynote on Tuesday,

00:06:16.370 --> 00:06:18.110
we're continuing to
develop new methods,

00:06:18.110 --> 00:06:20.420
such as [INAUDIBLE],, to
understand our models

00:06:20.420 --> 00:06:21.440
and to improve them.

00:06:21.440 --> 00:06:23.820
And we hope to keep
learning with you.

00:06:23.820 --> 00:06:27.080
So with that, let's
start with data.

00:06:27.080 --> 00:06:29.870
As Jackie mentioned,
datasets are a key part

00:06:29.870 --> 00:06:32.120
of the ML development process.

00:06:32.120 --> 00:06:34.490
Data trains a model
and informs what

00:06:34.490 --> 00:06:37.310
a model learns from and sees.

00:06:37.310 --> 00:06:40.220
Data is also a critical part
of evaluating the model.

00:06:40.220 --> 00:06:43.310
The datasets we choose to
evaluate on indicate what we

00:06:43.310 --> 00:06:45.110
know about how the
model performs,

00:06:45.110 --> 00:06:47.540
and when it performs
well or doesn't.

00:06:47.540 --> 00:06:50.810
So let's start with an example.

00:06:50.810 --> 00:06:52.970
What you see on the screen
here is a screenshot

00:06:52.970 --> 00:06:54.770
from a game called
Quick Draw that

00:06:54.770 --> 00:06:57.770
was developed through the
Google AI Experiments program.

00:06:57.770 --> 00:07:00.440
In this game, people drew
images of different objects

00:07:00.440 --> 00:07:03.130
around the world, like
shoes or trees or cars.

00:07:03.130 --> 00:07:06.500
And we use those images to train
an image classification model.

00:07:06.500 --> 00:07:08.120
This model could
then play a game

00:07:08.120 --> 00:07:10.490
with the users, where a
user would draw an image

00:07:10.490 --> 00:07:13.220
and the model would guess
what that image was of.

00:07:13.220 --> 00:07:16.040
Here you see a whole bunch
of drawings of shoes.

00:07:16.040 --> 00:07:17.900
And actually, we
were really excited,

00:07:17.900 --> 00:07:20.930
because what better way
to get diverse input

00:07:20.930 --> 00:07:23.495
from a whole bunch of users than
to launch something globally

00:07:23.495 --> 00:07:25.370
where a whole bunch of
users across the world

00:07:25.370 --> 00:07:27.560
could draw images for what
they perceived an object

00:07:27.560 --> 00:07:29.290
to look like?

00:07:29.290 --> 00:07:32.380
But what we found as this
model started to collect data

00:07:32.380 --> 00:07:34.120
was that most of the
images that users

00:07:34.120 --> 00:07:37.330
drew of shoes looked like
that shoe in the top right,

00:07:37.330 --> 00:07:38.980
the blue shoe.

00:07:38.980 --> 00:07:42.160
So over time, as the model
saw more and more examples,

00:07:42.160 --> 00:07:44.650
it started to learn that a
shoe looked a certain way

00:07:44.650 --> 00:07:46.810
like that top right
shoe, and wasn't

00:07:46.810 --> 00:07:49.360
able to recognize the
shoe in the bottom right,

00:07:49.360 --> 00:07:51.280
the orange shoe.

00:07:51.280 --> 00:07:53.020
Even though we were
able to get data

00:07:53.020 --> 00:07:55.900
from a diverse set
of users, the shoes

00:07:55.900 --> 00:08:00.220
that the users chose
to draw or the users

00:08:00.220 --> 00:08:02.380
who actually engaged
with the product at all

00:08:02.380 --> 00:08:05.230
were skewed, and led
to skewed training data

00:08:05.230 --> 00:08:07.510
in what we actually received.

00:08:07.510 --> 00:08:09.400
This is a social
issue first, which

00:08:09.400 --> 00:08:12.568
is then exacerbated by our
technical implementation.

00:08:12.568 --> 00:08:14.860
Because when we're making
classification decisions that

00:08:14.860 --> 00:08:18.130
divide up the world into
parts, even if those parts are

00:08:18.130 --> 00:08:20.020
what is a shoe and
what isn't a shoe,

00:08:20.020 --> 00:08:21.760
we're making fundamental
judgment calls

00:08:21.760 --> 00:08:23.770
about what deserves
to be in one part

00:08:23.770 --> 00:08:25.870
or what deserves
to be in the other.

00:08:25.870 --> 00:08:28.223
It's easier to deal with when
we're talking about shoes,

00:08:28.223 --> 00:08:29.890
but it's harder to
talk about when we're

00:08:29.890 --> 00:08:32.919
classifying images of people.

00:08:32.919 --> 00:08:35.789
An example of this is
the Google Clips camera.

00:08:35.789 --> 00:08:38.250
This camera was designed to
recognize memorable moments

00:08:38.250 --> 00:08:40.289
in real-time streaming video.

00:08:40.289 --> 00:08:41.850
The idea is that
it automatically

00:08:41.850 --> 00:08:45.600
captures memorable motion photos
of friends, of family, or even

00:08:45.600 --> 00:08:47.650
of pets.

00:08:47.650 --> 00:08:49.400
And we designed the
Google Clips camera

00:08:49.400 --> 00:08:52.450
to have equitable
outcomes for all users.

00:08:52.450 --> 00:08:54.430
It, like all of our
camera products,

00:08:54.430 --> 00:08:58.480
should work for all families,
no matter who or where they are.

00:08:58.480 --> 00:09:00.760
It should work for
people of all skin tones,

00:09:00.760 --> 00:09:03.100
all age ranges,
and in all poses,

00:09:03.100 --> 00:09:05.350
and in all lighting conditions.

00:09:05.350 --> 00:09:07.420
As we started to
build this system,

00:09:07.420 --> 00:09:09.850
we realized that if we only
created training data that

00:09:09.850 --> 00:09:12.370
represented certain
types of families,

00:09:12.370 --> 00:09:14.380
the model would
also only recognize

00:09:14.380 --> 00:09:16.340
certain types of families.

00:09:16.340 --> 00:09:19.060
So we had to do a lot of work
to increase our training data's

00:09:19.060 --> 00:09:22.700
coverage and to make sure that
it would recognize everyone.

00:09:22.700 --> 00:09:26.000
We went global to collect these
datasets, collecting datasets

00:09:26.000 --> 00:09:28.400
of different types of families
in different environments

00:09:28.400 --> 00:09:31.010
conditions in different
lighting conditions.

00:09:31.010 --> 00:09:32.840
And in doing so, we
were able to make sure

00:09:32.840 --> 00:09:34.820
that not only could
we train a model that

00:09:34.820 --> 00:09:36.980
had diverse outcomes,
but that we could also

00:09:36.980 --> 00:09:39.170
evaluate this constrained
on a whole bunch

00:09:39.170 --> 00:09:42.713
of different variables
like lighting or space.

00:09:42.713 --> 00:09:44.630
This is something that
we're continuing to do,

00:09:44.630 --> 00:09:47.180
continuing to create automatic
fairness tests for our systems

00:09:47.180 --> 00:09:49.520
so that we can see how
they change over time

00:09:49.520 --> 00:09:52.723
and to continue to ensure that
they are inclusive of everyone.

00:09:55.690 --> 00:09:57.690
The biggest lesson we've
learned in this process

00:09:57.690 --> 00:10:00.470
is how important it is to
build training and evaluation

00:10:00.470 --> 00:10:03.630
datasets that represent all
the nuances of our target

00:10:03.630 --> 00:10:05.380
population.

00:10:05.380 --> 00:10:07.860
This both means making sure
that the data that we collect

00:10:07.860 --> 00:10:10.207
is diverse and
representative, but also

00:10:10.207 --> 00:10:11.790
that the different
contexts of the way

00:10:11.790 --> 00:10:13.800
that the users are
providing us this data

00:10:13.800 --> 00:10:16.090
is taken into account.

00:10:16.090 --> 00:10:17.760
Even if you have a
diverse set of users,

00:10:17.760 --> 00:10:19.843
that doesn't mean that the
images of shoes you get

00:10:19.843 --> 00:10:21.420
will be diverse.

00:10:21.420 --> 00:10:24.180
And so thinking about those
nuances and the trade-offs that

00:10:24.180 --> 00:10:26.070
might occur when you're
collecting your data

00:10:26.070 --> 00:10:29.120
is super important.

00:10:29.120 --> 00:10:31.220
Additionally, it's also
important to reflect

00:10:31.220 --> 00:10:34.220
on who that target
population might leave out.

00:10:34.220 --> 00:10:37.040
Who might not actually have
access to this product?

00:10:37.040 --> 00:10:40.760
Where are the blind spots
in who we're reaching?

00:10:40.760 --> 00:10:43.250
And lastly, how will the
data that you're collecting

00:10:43.250 --> 00:10:45.800
grow and change over time?

00:10:45.800 --> 00:10:47.750
As our users use our
products, they very

00:10:47.750 --> 00:10:50.940
rarely use them in exactly the
way we anticipated them to.

00:10:50.940 --> 00:10:53.450
And so what happens is the
way that we collect data,

00:10:53.450 --> 00:10:55.490
or the data that we even
need to be collecting,

00:10:55.490 --> 00:10:56.873
changes over time.

00:10:56.873 --> 00:10:58.790
And it's important that
our collection methods

00:10:58.790 --> 00:11:00.590
and our maintenance
methods are equally

00:11:00.590 --> 00:11:02.015
diverse as that initial process.

00:11:05.710 --> 00:11:08.890
But even if you have a perfectly
balanced, wonderful training

00:11:08.890 --> 00:11:10.960
dataset, that
doesn't necessarily

00:11:10.960 --> 00:11:14.920
imply that the output of your
model will be perfectly fair.

00:11:14.920 --> 00:11:18.400
Also, it can be hard to collect
completely diverse datasets

00:11:18.400 --> 00:11:19.850
at the start of a process.

00:11:19.850 --> 00:11:21.850
And you don't always know
what it is that you're

00:11:21.850 --> 00:11:23.590
missing from the beginning.

00:11:23.590 --> 00:11:26.950
Where are your blind spots
in what you're trying to do?

00:11:26.950 --> 00:11:29.980
Because of that, it's
always important to test,

00:11:29.980 --> 00:11:33.730
to test and measure these issues
at scale for individual groups,

00:11:33.730 --> 00:11:36.460
so that we can actually
identify where our model may not

00:11:36.460 --> 00:11:38.500
be performing as
well, and where we

00:11:38.500 --> 00:11:41.830
might want to think about
more principled improvements.

00:11:41.830 --> 00:11:43.420
The benefit of
measurement is also

00:11:43.420 --> 00:11:46.030
that you can start tracking
these changes over time.

00:11:46.030 --> 00:11:48.323
You can understand
how the model works.

00:11:48.323 --> 00:11:49.990
Similar to the way
that you would always

00:11:49.990 --> 00:11:52.280
want to have metrics for
your model as a whole,

00:11:52.280 --> 00:11:54.760
it's important to think about
how you slice those metrics,

00:11:54.760 --> 00:11:57.460
and how you can provide yourself
a holistic understanding of how

00:11:57.460 --> 00:12:01.290
this model or system
works for everybody.

00:12:01.290 --> 00:12:03.930
What's interesting is that
different fairness concerns

00:12:03.930 --> 00:12:06.780
may require different metrics,
even within the same product

00:12:06.780 --> 00:12:08.570
experience.

00:12:08.570 --> 00:12:10.770
A disproportionate
performance problem

00:12:10.770 --> 00:12:14.130
is when, for example, a model
works well for one group,

00:12:14.130 --> 00:12:16.092
but may not work as
well for another.

00:12:16.092 --> 00:12:17.550
For example, you
could have a model

00:12:17.550 --> 00:12:20.640
that doesn't recognize some
subset of users or errors

00:12:20.640 --> 00:12:23.780
more for that subset of users.

00:12:23.780 --> 00:12:26.570
In contrast, a
representational harm problem

00:12:26.570 --> 00:12:28.940
is when a model showcases
an offensive stereotype

00:12:28.940 --> 00:12:32.030
or harmful association.

00:12:32.030 --> 00:12:34.340
Maybe this doesn't
necessarily happen at scale.

00:12:34.340 --> 00:12:36.800
But even a single instance
can be hurtful and harmful

00:12:36.800 --> 00:12:37.855
to a set of users.

00:12:37.855 --> 00:12:39.230
And this requires
a different way

00:12:39.230 --> 00:12:42.400
of stress-testing the system.

00:12:42.400 --> 00:12:45.720
Here's an example where both
of those metrics may apply.

00:12:45.720 --> 00:12:49.530
The screenshot you see is from
our Jigsaw Perspective API.

00:12:49.530 --> 00:12:52.140
This API is designed to
detect hate and harassment

00:12:52.140 --> 00:12:55.080
in the context of
online conversations.

00:12:55.080 --> 00:12:57.360
The idea is, given a
particular sentence,

00:12:57.360 --> 00:12:59.640
we can classify whether
or not that sentence is

00:12:59.640 --> 00:13:03.560
perceived likely to be toxic.

00:13:03.560 --> 00:13:05.400
We have this API externally.

00:13:05.400 --> 00:13:08.480
So our users can actually write
sentences and give us feedback.

00:13:08.480 --> 00:13:10.820
And what we found
was one of our users

00:13:10.820 --> 00:13:13.820
articulated a particular
example that you see here.

00:13:13.820 --> 00:13:17.390
The sentence, "I am straight,"
is given a score of 0.04,

00:13:17.390 --> 00:13:20.780
and is classified as "unlikely
to be perceived as toxic."

00:13:20.780 --> 00:13:22.490
Whereas the
sentence, "I am gay,"

00:13:22.490 --> 00:13:24.830
was given a score
of 0.86, and was

00:13:24.830 --> 00:13:27.800
classified as "likely to
be perceived as toxic."

00:13:27.800 --> 00:13:30.680
Both of these are innocuous
identity statements,

00:13:30.680 --> 00:13:33.800
but one was given a
significantly higher score.

00:13:33.800 --> 00:13:36.380
This is something we would never
want to see in our products.

00:13:36.380 --> 00:13:38.000
And an example--
we not only wanted

00:13:38.000 --> 00:13:40.220
to fix this immediate
example, but we actually

00:13:40.220 --> 00:13:42.350
wanted to understand and
quantify these issues

00:13:42.350 --> 00:13:47.035
to ensure that we could
tackle this appropriately.

00:13:47.035 --> 00:13:48.910
The first thing we looked
at was this concept

00:13:48.910 --> 00:13:50.860
of "representational
harm," understanding

00:13:50.860 --> 00:13:53.140
these counterfactual
differences.

00:13:53.140 --> 00:13:56.260
For a particular sentence,
we would want the sentence

00:13:56.260 --> 00:13:58.570
to be classified the
same way regardless

00:13:58.570 --> 00:14:01.210
of the identity referenced
in the sentence.

00:14:01.210 --> 00:14:03.850
Whether it's, "I am
Muslim," "I am Jewish,"

00:14:03.850 --> 00:14:05.860
or "I am Christian,"
you would expect

00:14:05.860 --> 00:14:10.020
the score perceived by the
classifier to be the same.

00:14:10.020 --> 00:14:11.850
Being able to
provide these scores

00:14:11.850 --> 00:14:14.700
allowed us to understand
how the system performed.

00:14:14.700 --> 00:14:17.040
It allowed us to identify
places where our model might

00:14:17.040 --> 00:14:19.038
be more likely to be
biased, and allowed

00:14:19.038 --> 00:14:21.330
us to go in and actually
understand those concerns more

00:14:21.330 --> 00:14:23.190
deeply.

00:14:23.190 --> 00:14:25.550
But we also wanted to
understand overall error rates

00:14:25.550 --> 00:14:27.530
for particular groups.

00:14:27.530 --> 00:14:29.330
Were there particular
identities where,

00:14:29.330 --> 00:14:30.830
when referenced in
comments, we were

00:14:30.830 --> 00:14:34.040
more likely to have
errors versus others?

00:14:34.040 --> 00:14:36.250
This is where the
disproportionate performance

00:14:36.250 --> 00:14:37.930
question comes in.

00:14:37.930 --> 00:14:40.630
We wanted to develop
metrics on average

00:14:40.630 --> 00:14:42.520
for a particular
identity term that

00:14:42.520 --> 00:14:44.570
showcased, across
a set of comments,

00:14:44.570 --> 00:14:47.650
whether or not we were
more likely to classify.

00:14:47.650 --> 00:14:50.620
This was in both directions--
misclassifying something

00:14:50.620 --> 00:14:53.920
as toxic, but also
misclassifying something as not

00:14:53.920 --> 00:14:57.070
toxic when it truly was
a harmful statement.

00:14:57.070 --> 00:14:59.440
The three metrics you see
here capture different ways

00:14:59.440 --> 00:15:00.880
of looking at that problem.

00:15:00.880 --> 00:15:03.550
And the darker the color,
the darker the purple,

00:15:03.550 --> 00:15:05.860
the more likely we were
to have error rates.

00:15:05.860 --> 00:15:08.392
And you can see that in the
first version of this model,

00:15:08.392 --> 00:15:10.600
there were huge disparities
between different groups.

00:15:13.560 --> 00:15:16.380
So OK, we were able to
measure the problem.

00:15:16.380 --> 00:15:17.890
But then how do we improve it?

00:15:17.890 --> 00:15:20.165
How do we make sure
this doesn't happen?

00:15:20.165 --> 00:15:21.790
A lot of research
has been published in

00:15:21.790 --> 00:15:23.800
the last few years,
both internally

00:15:23.800 --> 00:15:26.260
within Google as
well as externally,

00:15:26.260 --> 00:15:27.910
that look at how to
train and improve

00:15:27.910 --> 00:15:31.300
our models in a way that still
allows them to be stable,

00:15:31.300 --> 00:15:33.970
to be resource-efficient,
and to be accurate,

00:15:33.970 --> 00:15:36.910
so that we can still deploy
them in production use cases.

00:15:36.910 --> 00:15:40.090
These approaches balance the
simplicity of implementation

00:15:40.090 --> 00:15:44.800
with the required accuracy and
quality that we would want.

00:15:44.800 --> 00:15:46.690
The simplest way to
think about this problem

00:15:46.690 --> 00:15:49.570
would be through the idea
of removals or block lists,

00:15:49.570 --> 00:15:52.930
taking steps to ensure that your
model can't access information

00:15:52.930 --> 00:15:55.480
in a way that could
lead to skewed outcomes.

00:15:55.480 --> 00:15:59.040
Take, for example, the sentence,
"Some people are Indian."

00:15:59.040 --> 00:16:02.010
We may actually want to remove
that identity term altogether,

00:16:02.010 --> 00:16:05.220
and replace it with a more
generic tag, "identity."

00:16:05.220 --> 00:16:07.590
if you do this for every
single identity term,

00:16:07.590 --> 00:16:10.828
your model wouldn't even have
access to identity information.

00:16:10.828 --> 00:16:12.870
It would simply know that
the sentence referenced

00:16:12.870 --> 00:16:14.160
an identity.

00:16:14.160 --> 00:16:17.490
As a result, it couldn't
make different decisions

00:16:17.490 --> 00:16:20.940
for different identities
or different user groups.

00:16:20.940 --> 00:16:23.730
This is a great way to make
sure that your model is

00:16:23.730 --> 00:16:27.580
agnostic of a particular
definition of an individual.

00:16:27.580 --> 00:16:30.327
At the same time,
it can be harmful.

00:16:30.327 --> 00:16:32.160
It actually might be
useful in certain cases

00:16:32.160 --> 00:16:34.710
to know when identity terms
are used in a way that

00:16:34.710 --> 00:16:37.020
is offensive or harmful.

00:16:37.020 --> 00:16:39.180
If a particular
term is often used

00:16:39.180 --> 00:16:40.775
in a negative or
derogatory context,

00:16:40.775 --> 00:16:42.150
we would want to
know that, so we

00:16:42.150 --> 00:16:44.640
could classify that as toxic.

00:16:44.640 --> 00:16:47.508
Sometimes, this context is
actually really important.

00:16:47.508 --> 00:16:49.050
But it's important
that we capture it

00:16:49.050 --> 00:16:50.650
in a nuanced and contextual way.

00:16:53.940 --> 00:16:55.800
Another way to think
about it is to go back

00:16:55.800 --> 00:16:59.370
to that first lesson, and
look back at the data.

00:16:59.370 --> 00:17:01.530
We can enable our
models to sample data

00:17:01.530 --> 00:17:04.599
from areas in which the model
seems to be underperforming.

00:17:04.599 --> 00:17:08.849
We could do this both manually
as well as algorithmically.

00:17:08.849 --> 00:17:10.740
On the manual side,
what you see on

00:17:10.740 --> 00:17:13.260
the right is a quote collected
through Google's Project

00:17:13.260 --> 00:17:14.760
Respect effort.

00:17:14.760 --> 00:17:16.920
Through Project Respect,
we went globally

00:17:16.920 --> 00:17:19.290
to collect more
and more comments

00:17:19.290 --> 00:17:22.329
of positive representations
of identity.

00:17:22.329 --> 00:17:24.212
This comment is
from a pride parade,

00:17:24.212 --> 00:17:26.670
where someone from Lithuania
talks about their gay friends,

00:17:26.670 --> 00:17:29.550
and how they're brilliant
and amazing people.

00:17:29.550 --> 00:17:32.400
Positive reflections of identity
are great examples for us

00:17:32.400 --> 00:17:35.280
to train our model, and to
support the model in developing

00:17:35.280 --> 00:17:37.710
a context and nuanced
understanding of comments,

00:17:37.710 --> 00:17:39.210
especially when the
model is usually

00:17:39.210 --> 00:17:41.490
trained from online
comments that may not always

00:17:41.490 --> 00:17:43.890
have the same flavor.

00:17:43.890 --> 00:17:46.320
We can also enable the model
to do this algorithmically

00:17:46.320 --> 00:17:48.240
through active sampling.

00:17:48.240 --> 00:17:49.798
The model can
identify the places

00:17:49.798 --> 00:17:51.840
where it has the least
confidence in its decision

00:17:51.840 --> 00:17:54.360
making, where it might
be underperforming.

00:17:54.360 --> 00:17:56.430
And it can actively
go out and sample more

00:17:56.430 --> 00:18:00.330
from the training dataset that
represents that type of data.

00:18:00.330 --> 00:18:02.700
We can continue to even
build more and more examples

00:18:02.700 --> 00:18:04.332
through synthetic examples.

00:18:04.332 --> 00:18:06.040
Similar to what you
saw at the beginning,

00:18:06.040 --> 00:18:08.640
we can create these short
sentences, like "I am,"

00:18:08.640 --> 00:18:10.812
"He is," "My friends are."

00:18:10.812 --> 00:18:13.020
And these sentences can
continue to provide the model

00:18:13.020 --> 00:18:18.920
understandings of when identity
can be used in natural context.

00:18:18.920 --> 00:18:21.800
We can even make changes
directly to our models

00:18:21.800 --> 00:18:24.187
by updating the models'
loss functions to minimize

00:18:24.187 --> 00:18:26.270
difference in performance
between different groups

00:18:26.270 --> 00:18:27.890
of individuals.

00:18:27.890 --> 00:18:30.230
Adversarial training
and min diff loss,

00:18:30.230 --> 00:18:32.060
two of the research
methods in this space,

00:18:32.060 --> 00:18:35.300
have actively looked at how
to effect your loss function

00:18:35.300 --> 00:18:38.570
to keep the model stable
and to keep it lightweight,

00:18:38.570 --> 00:18:40.620
while still enforcing
this kind of a penalty.

00:18:44.140 --> 00:18:45.760
What you saw earlier
were the results

00:18:45.760 --> 00:18:48.140
of the Toxicity V1 model.

00:18:48.140 --> 00:18:50.080
And as we made changes,
especially in terms

00:18:50.080 --> 00:18:52.600
of creating manual
synthetic examples

00:18:52.600 --> 00:18:56.640
and augmenting the
data performance,

00:18:56.640 --> 00:18:59.650
we were able to see
real improvements.

00:18:59.650 --> 00:19:01.890
This is the toxicity
V6 model, where

00:19:01.890 --> 00:19:03.960
you can see that the
colors get lighter

00:19:03.960 --> 00:19:06.120
as the performance for
individual identity groups

00:19:06.120 --> 00:19:08.210
gets better.

00:19:08.210 --> 00:19:10.830
We're really excited about the
progress that we've made here.

00:19:10.830 --> 00:19:13.890
But we know that there is
still a long ways to go.

00:19:13.890 --> 00:19:17.450
The results you see here are on
synthetic data, short identity

00:19:17.450 --> 00:19:19.800
statements like I
talked about earlier.

00:19:19.800 --> 00:19:22.430
But the story of bias can
become much more complex

00:19:22.430 --> 00:19:24.740
when you're talking about
real data, comments that

00:19:24.740 --> 00:19:27.910
are actually used in the wild.

00:19:27.910 --> 00:19:30.400
We're currently working
on evaluating our systems

00:19:30.400 --> 00:19:33.470
on real comments, building
up these datasets,

00:19:33.470 --> 00:19:36.070
and then trying to enhance our
understanding of performance

00:19:36.070 --> 00:19:38.500
and improvements in that space.

00:19:38.500 --> 00:19:40.720
While we've still seen
progress on real comments

00:19:40.720 --> 00:19:42.268
and improvements
from our changes,

00:19:42.268 --> 00:19:44.560
we know that this will actually
help more once we start

00:19:44.560 --> 00:19:46.140
looking at these real datasets.

00:19:46.140 --> 00:19:47.890
And actually, there's
a Kaggle competition

00:19:47.890 --> 00:19:50.260
live now if you're interested
in checking this out more.

00:19:53.060 --> 00:19:57.230
Overall, the biggest lesson is
"Test early and test often."

00:19:57.230 --> 00:19:59.840
Measuring your systems
is critical to actually

00:19:59.840 --> 00:20:02.090
understanding where
the problems exist,

00:20:02.090 --> 00:20:03.767
where our users
might be facing risk,

00:20:03.767 --> 00:20:05.600
or where our products
aren't working the way

00:20:05.600 --> 00:20:08.430
that we intend for them to be.

00:20:08.430 --> 00:20:10.920
Also, bias can affect
the user experience

00:20:10.920 --> 00:20:13.240
and cause issues in
many different forms.

00:20:13.240 --> 00:20:14.850
So it's important
to develop methods

00:20:14.850 --> 00:20:17.490
for measuring the
scale of each problem.

00:20:17.490 --> 00:20:20.580
Even a particular single
product may manifest bias

00:20:20.580 --> 00:20:22.030
in different ways.

00:20:22.030 --> 00:20:25.830
So we want to actually be sure
to measure those metrics, also.

00:20:25.830 --> 00:20:27.630
The other thing to
note is it's not always

00:20:27.630 --> 00:20:29.280
quantitative metrics.

00:20:29.280 --> 00:20:32.010
Qualitative metrics,
user research,

00:20:32.010 --> 00:20:33.600
and adversarial
testing of really,

00:20:33.600 --> 00:20:35.683
actually stress-testing
and poking at your product

00:20:35.683 --> 00:20:39.430
manually, can also be
really, really valuable.

00:20:39.430 --> 00:20:42.190
Lastly, it is possible
to take proactive steps

00:20:42.190 --> 00:20:45.628
in modeling that are aware of
your production constraints.

00:20:45.628 --> 00:20:47.170
These techniques
have been invaluable

00:20:47.170 --> 00:20:48.772
in our own internal use cases.

00:20:48.772 --> 00:20:50.980
And we will continue to
publish these methods for you

00:20:50.980 --> 00:20:52.340
to use, as well.

00:20:52.340 --> 00:20:54.870
You can actually go to
mlfairness.com to learn more.

00:20:57.830 --> 00:20:59.500
I also want to
talk about design.

00:20:59.500 --> 00:21:01.600
And this is our third
lesson for today.

00:21:01.600 --> 00:21:04.780
Because context is
really important.

00:21:04.780 --> 00:21:08.170
The way that our users interact
with our results is different.

00:21:08.170 --> 00:21:11.693
And our design decisions around
the results have consequences.

00:21:11.693 --> 00:21:13.860
Because the experience that
a user actually has with

00:21:13.860 --> 00:21:17.560
a product extends beyond the
performance of the model.

00:21:17.560 --> 00:21:20.020
It relates to how
users are actually

00:21:20.020 --> 00:21:21.340
engaging with the results.

00:21:21.340 --> 00:21:22.510
What are they seeing?

00:21:22.510 --> 00:21:24.580
What kind of information
are they being given?

00:21:24.580 --> 00:21:27.070
What kind of information do
they have that maybe the model

00:21:27.070 --> 00:21:28.600
may not have?

00:21:28.600 --> 00:21:31.140
Let's look at an example.

00:21:31.140 --> 00:21:34.100
Here you see an example from
the Google Translate product.

00:21:34.100 --> 00:21:35.970
And what you see
here is a translation

00:21:35.970 --> 00:21:38.670
from Turkish to English.

00:21:38.670 --> 00:21:40.990
Turkish is a
gender-neutral language,

00:21:40.990 --> 00:21:43.830
which means that in Turkish,
nouns aren't gendered.

00:21:43.830 --> 00:21:47.520
And "he," "she," or "it" are all
referenced through the pronoun,

00:21:47.520 --> 00:21:49.077
"O."

00:21:49.077 --> 00:21:49.910
I actually misspoke.

00:21:49.910 --> 00:21:53.790
I believe not all nouns are
gendered, but some may be.

00:21:53.790 --> 00:21:56.860
Thus, while the sentences
in Turkish, in this case,

00:21:56.860 --> 00:22:00.850
don't actually specify
gender, our product

00:22:00.850 --> 00:22:03.130
translates it to
common stereotypes.

00:22:03.130 --> 00:22:07.480
"She is a nurse,"
while "He is a doctor."

00:22:07.480 --> 00:22:09.860
So why does that happen?

00:22:09.860 --> 00:22:12.250
Well, Google Translate learns
from hundreds of millions

00:22:12.250 --> 00:22:14.950
of already translated
examples from the web.

00:22:14.950 --> 00:22:18.523
And it therefore also learns
the historical and social trends

00:22:18.523 --> 00:22:20.440
that have come with these
hundreds of millions

00:22:20.440 --> 00:22:23.710
of examples, the
historical trends of how

00:22:23.710 --> 00:22:26.980
we've thought of occupations
in society thus far.

00:22:26.980 --> 00:22:29.380
So it skews
masculine for doctor,

00:22:29.380 --> 00:22:32.560
whereas it skews
feminine for nurse.

00:22:32.560 --> 00:22:34.270
As we started to look
into this problem,

00:22:34.270 --> 00:22:36.460
we went back to those
first two lessons.

00:22:36.460 --> 00:22:39.050
OK, how can we make the
training data more diverse?

00:22:39.050 --> 00:22:40.870
How can we make it
more representative

00:22:40.870 --> 00:22:44.140
of the full gender diversity?

00:22:44.140 --> 00:22:46.780
Also, how could we
better train a model?

00:22:46.780 --> 00:22:48.850
How could we improve
and measure the space,

00:22:48.850 --> 00:22:51.220
and then make modeling changes?

00:22:51.220 --> 00:22:52.952
Both of these questions
are important.

00:22:52.952 --> 00:22:54.910
But what we started to
realize is how important

00:22:54.910 --> 00:22:57.980
context was in this situation.

00:22:57.980 --> 00:23:01.120
Take, for example, the
sentence, "Casey is my friend."

00:23:01.120 --> 00:23:04.120
Let's say we want to translate
to Spanish, in which case

00:23:04.120 --> 00:23:07.630
friend could be "amigo," the
masculine version, or "amiga,"

00:23:07.630 --> 00:23:09.820
the feminine version.

00:23:09.820 --> 00:23:16.030
Well, how do we know if Casey
is a male, a female, or a gender

00:23:16.030 --> 00:23:18.840
non-binary friend?

00:23:18.840 --> 00:23:21.240
We don't have that context.

00:23:21.240 --> 00:23:24.120
Even a perfectly
precise model trained

00:23:24.120 --> 00:23:28.110
on diverse data that represents
all kinds of professions

00:23:28.110 --> 00:23:30.550
would not have that context.

00:23:30.550 --> 00:23:32.220
And so we realized
that even if we do

00:23:32.220 --> 00:23:34.530
make our understandings
of terms more neutral,

00:23:34.530 --> 00:23:37.110
and even if we were to
build up model precision,

00:23:37.110 --> 00:23:39.690
we would actually want to give
this choice to the user, who

00:23:39.690 --> 00:23:41.340
actually understands
what they were

00:23:41.340 --> 00:23:45.590
trying to achieve with the
sentence in the translation.

00:23:45.590 --> 00:23:48.260
What we did is choose to
provide that to our users

00:23:48.260 --> 00:23:50.660
in the form of options
and selections.

00:23:50.660 --> 00:23:53.330
We translate "friend"
both to "amigo"

00:23:53.330 --> 00:23:55.340
and to "amiga," so
that the user can

00:23:55.340 --> 00:23:57.620
make a choice that is
informed based on the context

00:23:57.620 --> 00:24:00.370
that they have.

00:24:00.370 --> 00:24:03.810
Currently, this solution is only
available for a few languages.

00:24:03.810 --> 00:24:07.280
And it's also only available
for single terms like "friend."

00:24:07.280 --> 00:24:09.410
But we're actively working
on trying to expand it

00:24:09.410 --> 00:24:11.330
to more languages,
and also trying

00:24:11.330 --> 00:24:14.270
to be inclusive of larger
sentences and longer contexts,

00:24:14.270 --> 00:24:18.440
so we can actually tackle
the example you saw earlier.

00:24:18.440 --> 00:24:20.840
We're excited about this
line of thinking, though,

00:24:20.840 --> 00:24:23.750
because it enables us to think
about fairness beyond simply

00:24:23.750 --> 00:24:25.580
the data and the
model, but actually as

00:24:25.580 --> 00:24:28.743
a holistic experience that a
user engages with every day,

00:24:28.743 --> 00:24:30.410
and trying to make
sure that we actually

00:24:30.410 --> 00:24:33.540
build those communication lines
between the product and the end

00:24:33.540 --> 00:24:34.040
consumer.

00:24:37.050 --> 00:24:40.802
The biggest lesson we learned
here is that context is key.

00:24:40.802 --> 00:24:42.260
Think about the
ways that your user

00:24:42.260 --> 00:24:44.780
will be interacting with your
product and the information

00:24:44.780 --> 00:24:47.327
that they may have that
the model doesn't have,

00:24:47.327 --> 00:24:49.160
or the information that
the model might have

00:24:49.160 --> 00:24:51.110
that the user doesn't have.

00:24:51.110 --> 00:24:53.390
How do you enable the users
to communicate effectively

00:24:53.390 --> 00:24:56.570
with your product, but also
get back the right transparency

00:24:56.570 --> 00:24:58.010
from it?

00:24:58.010 --> 00:25:00.320
Sometimes, this is about
providing user options,

00:25:00.320 --> 00:25:01.790
like you saw with Translate.

00:25:01.790 --> 00:25:04.310
Sometimes, it's also just
about providing more context

00:25:04.310 --> 00:25:06.680
about the model's decisions,
and being a little bit more

00:25:06.680 --> 00:25:10.752
explainable and interpretable.

00:25:10.752 --> 00:25:12.710
The other piece that's
important is making sure

00:25:12.710 --> 00:25:15.380
that you get feedback
from diverse users.

00:25:15.380 --> 00:25:18.260
In this case, this was users
who spoke different languages,

00:25:18.260 --> 00:25:21.428
and who had different
definitions of identity.

00:25:21.428 --> 00:25:22.970
But it's also
important to make sure,

00:25:22.970 --> 00:25:25.070
as you're trying to get
feedback from users,

00:25:25.070 --> 00:25:26.695
that you think about
the different ways

00:25:26.695 --> 00:25:28.580
in which these users
provide you feedback.

00:25:28.580 --> 00:25:32.120
Not every user is equally
likely to be accepting

00:25:32.120 --> 00:25:34.102
of the same feedback
mechanism, or equally

00:25:34.102 --> 00:25:36.560
likely to proactively give you
feedback in, say, a feedback

00:25:36.560 --> 00:25:38.025
form on your product.

00:25:38.025 --> 00:25:39.650
So it's important to
actually make sure

00:25:39.650 --> 00:25:41.660
that whether that be
through user research,

00:25:41.660 --> 00:25:44.120
or through dog fooding, or
through different feedback

00:25:44.120 --> 00:25:46.370
mechanisms in your
product, that you identify

00:25:46.370 --> 00:25:49.130
different ways to access
different communities who

00:25:49.130 --> 00:25:54.300
might be more or less likely
to provide that information.

00:25:54.300 --> 00:25:57.710
Lastly, identify ways to
enable multiple experiences

00:25:57.710 --> 00:25:58.970
in your product.

00:25:58.970 --> 00:26:01.220
Identify the places where
there could be more than one

00:26:01.220 --> 00:26:03.300
correct answer, for example.

00:26:03.300 --> 00:26:07.380
And find ways to enable users to
have that different experience.

00:26:07.380 --> 00:26:10.050
Representing human culture
and all of its differences

00:26:10.050 --> 00:26:13.360
requires more than a theoretical
and technical toolkit.

00:26:13.360 --> 00:26:17.430
It requires a much more rich and
context-dependent experience.

00:26:17.430 --> 00:26:19.470
And that is really, at
the end of the day, what

00:26:19.470 --> 00:26:20.925
we want to provide our users.

00:26:24.640 --> 00:26:26.720
We hope that those
lessons were helpful.

00:26:26.720 --> 00:26:29.220
They've been lessons that we've
been really, really grateful

00:26:29.220 --> 00:26:32.020
to learn, and that we've started
to execute in our own products.

00:26:32.020 --> 00:26:33.370
But what's next?

00:26:33.370 --> 00:26:35.770
We're starting to put these
lessons into practice.

00:26:35.770 --> 00:26:38.530
And while we know that product
development in ML fairness

00:26:38.530 --> 00:26:40.642
is a context-dependent
experience,

00:26:40.642 --> 00:26:42.850
we do want to start building
some of the fundamentals

00:26:42.850 --> 00:26:45.880
in terms of tools, resources,
and best practices.

00:26:45.880 --> 00:26:48.640
Because we know how important
it is to at least start

00:26:48.640 --> 00:26:50.860
with those metrics,
start with the ability

00:26:50.860 --> 00:26:55.657
to collect diverse data, start
with consistent communication.

00:26:55.657 --> 00:26:57.490
One of the first things
we're thinking about

00:26:57.490 --> 00:26:59.560
is transparency frameworks.

00:26:59.560 --> 00:27:01.900
We want to create and
leverage frameworks that drive

00:27:01.900 --> 00:27:04.300
consistent communication--
both within Google,

00:27:04.300 --> 00:27:06.040
but also with the
industry at large--

00:27:06.040 --> 00:27:07.810
about fairness and
other risks that

00:27:07.810 --> 00:27:12.180
might exist with data
collection and modeling.

00:27:12.180 --> 00:27:14.910
We also want to build
tools and techniques,

00:27:14.910 --> 00:27:19.140
develop and socialize tools that
enable evaluating and improving

00:27:19.140 --> 00:27:21.540
fairness concerns.

00:27:21.540 --> 00:27:24.080
Let's talk about
transparency first.

00:27:24.080 --> 00:27:27.350
Today, we're committing to
a framework for transparency

00:27:27.350 --> 00:27:30.650
that ensures that we think
about, measure, and communicate

00:27:30.650 --> 00:27:34.067
about our models and data
in a way that is consistent.

00:27:34.067 --> 00:27:36.150
This is not about achieving
perfection in our data

00:27:36.150 --> 00:27:39.150
on models, although of
course we hope to get there.

00:27:39.150 --> 00:27:41.240
It's about the context
under which something

00:27:41.240 --> 00:27:43.230
is supposed to be used.

00:27:43.230 --> 00:27:45.230
What are its intended use cases?

00:27:45.230 --> 00:27:47.180
What is it not intended for?

00:27:47.180 --> 00:27:51.550
And how does it perform
across various users?

00:27:51.550 --> 00:27:53.980
We released our first
Data Card last October

00:27:53.980 --> 00:27:55.960
as part of the Open
Images Extended Dataset

00:27:55.960 --> 00:27:58.900
that you heard Jackie
talk about earlier.

00:27:58.900 --> 00:28:01.820
This Data Card allows us
to answer questions like,

00:28:01.820 --> 00:28:05.020
what are the intended use
cases of this dataset?

00:28:05.020 --> 00:28:07.370
What is the nature
of the content?

00:28:07.370 --> 00:28:09.640
What data was excluded, if any?

00:28:09.640 --> 00:28:12.290
Who collected the data?

00:28:12.290 --> 00:28:13.940
It also allows us
to go into some

00:28:13.940 --> 00:28:15.680
of the fairness considerations.

00:28:15.680 --> 00:28:19.100
Who labeled the data, and what
information did they have?

00:28:19.100 --> 00:28:20.900
How was the data sourced?

00:28:20.900 --> 00:28:23.510
And what is the
distribution of it?

00:28:23.510 --> 00:28:25.850
For Open Images
Extended, for example,

00:28:25.850 --> 00:28:28.130
while you can see that the
geographic distribution is

00:28:28.130 --> 00:28:32.790
extremely diverse, 80% percent
of the data comes from India.

00:28:32.790 --> 00:28:34.700
This is an important
finding for anyone

00:28:34.700 --> 00:28:37.220
who wants to use this
dataset, both for training

00:28:37.220 --> 00:28:39.030
or for testing purposes.

00:28:39.030 --> 00:28:41.462
It might inform how you
interpret your results.

00:28:41.462 --> 00:28:42.920
It also might inform
whether or not

00:28:42.920 --> 00:28:45.212
you choose to augment your
dataset with something else,

00:28:45.212 --> 00:28:46.640
for example.

00:28:46.640 --> 00:28:50.120
This kind of transparency
allows for open communication

00:28:50.120 --> 00:28:52.868
about what the actual use cases
of this dataset should be,

00:28:52.868 --> 00:28:54.035
and where it may have flaws.

00:28:56.560 --> 00:28:59.470
We want to take this a step
further with Model Cards.

00:28:59.470 --> 00:29:01.210
Here you see an
example screenshot

00:29:01.210 --> 00:29:03.640
for the Jigsaw
Perspective Toxicity API

00:29:03.640 --> 00:29:05.320
that we talked about earlier.

00:29:05.320 --> 00:29:07.480
With Model Cards, we want
to be able to give you

00:29:07.480 --> 00:29:10.240
an overview of what
the model is about,

00:29:10.240 --> 00:29:12.490
what metrics we use
to think about it,

00:29:12.490 --> 00:29:16.630
how it was architected, how it
was trained, how it was tested,

00:29:16.630 --> 00:29:18.250
what we think it
should be used for,

00:29:18.250 --> 00:29:21.100
and where we believe
that it has limitations.

00:29:21.100 --> 00:29:22.780
We hope that the
Model Card framework

00:29:22.780 --> 00:29:25.240
will work across models,
so not just for something

00:29:25.240 --> 00:29:28.690
like toxicity, but also
for a face detection model,

00:29:28.690 --> 00:29:31.600
or for any other use case
that we can think of.

00:29:31.600 --> 00:29:34.330
In each case, the framework
should be consistent.

00:29:34.330 --> 00:29:35.830
We can look at metrics.

00:29:35.830 --> 00:29:37.390
We can look at use cases.

00:29:37.390 --> 00:29:39.290
We can look at the
training and test data.

00:29:39.290 --> 00:29:42.390
And we can look at
the limitations.

00:29:42.390 --> 00:29:45.360
Each Model Card will also
have the quantitative metrics

00:29:45.360 --> 00:29:47.280
that tell you how it performs.

00:29:47.280 --> 00:29:49.440
Here, for example, you
can see an example set

00:29:49.440 --> 00:29:52.110
of metrics sliced by age.

00:29:52.110 --> 00:29:54.690
You can see the
performance on all ages,

00:29:54.690 --> 00:29:57.312
on the child age bucket,
on the adult age bucket,

00:29:57.312 --> 00:29:58.520
and on the senior age bucket.

00:30:02.250 --> 00:30:04.540
So how do you create
those metrics?

00:30:04.540 --> 00:30:06.270
How do you compute them?

00:30:06.270 --> 00:30:08.790
Well, we also want to be
able to provide you the tools

00:30:08.790 --> 00:30:12.900
to do this analysis, to be able
to create your own model cards,

00:30:12.900 --> 00:30:15.390
and also to be able to
improve your models over time.

00:30:18.330 --> 00:30:20.480
The first piece of the
set of tools and resources

00:30:20.480 --> 00:30:22.490
is open datasets.

00:30:22.490 --> 00:30:25.460
The Open Images Extended
Dataset is one of many datasets

00:30:25.460 --> 00:30:27.530
that we have and hope
to continue to open

00:30:27.530 --> 00:30:29.780
source in the coming years.

00:30:29.780 --> 00:30:32.720
In this example, the Open
Images Extended Dataset

00:30:32.720 --> 00:30:35.450
collects data from
crowdsourced users

00:30:35.450 --> 00:30:38.780
who are taking images of
objects in their own regions

00:30:38.780 --> 00:30:39.950
of the world.

00:30:39.950 --> 00:30:42.650
You can see, for example,
how a hospital or food might

00:30:42.650 --> 00:30:44.108
look different in
different places,

00:30:44.108 --> 00:30:46.150
and how important it is
for us to have that data.

00:30:49.220 --> 00:30:51.180
With the live
Kaggle competition,

00:30:51.180 --> 00:30:52.820
we also have open
sourced a dataset

00:30:52.820 --> 00:30:55.590
related to the
Perspective Toxicity API.

00:30:55.590 --> 00:30:57.530
I mentioned earlier
how important

00:30:57.530 --> 00:31:00.990
it is for us to look at
real comments and real data.

00:31:00.990 --> 00:31:02.780
So here, the Jigsaw
team has open

00:31:02.780 --> 00:31:05.750
sourced a dataset of real
comments from around the web.

00:31:05.750 --> 00:31:08.570
Each of these comments is
annotated with the identity

00:31:08.570 --> 00:31:11.120
that the comment references,
as well as whether or not

00:31:11.120 --> 00:31:13.610
the comment is toxic,
as well as other factors

00:31:13.610 --> 00:31:15.920
about the comment, as well.

00:31:15.920 --> 00:31:18.280
We hope that datasets
like these continue

00:31:18.280 --> 00:31:21.580
to be able to advance the
conversation, the evaluation,

00:31:21.580 --> 00:31:25.090
and the improvements
of fairness.

00:31:25.090 --> 00:31:27.220
Once you have a dataset,
the question becomes,

00:31:27.220 --> 00:31:29.030
how do you take
that step further?

00:31:29.030 --> 00:31:31.160
How do you evaluate the model?

00:31:31.160 --> 00:31:33.820
One thing you can do today
is deep-dive with the What-If

00:31:33.820 --> 00:31:35.390
tool.

00:31:35.390 --> 00:31:38.570
The What-If tool is available
as a Tensorboard plugin, as well

00:31:38.570 --> 00:31:40.700
as a Jupyter Notebook.

00:31:40.700 --> 00:31:42.950
You can deep-dive into
specific examples,

00:31:42.950 --> 00:31:46.550
and see how changing features
actually affects your outcome.

00:31:46.550 --> 00:31:48.860
You can understand different
fairness definitions,

00:31:48.860 --> 00:31:50.990
and how modifying the
threshold of your model

00:31:50.990 --> 00:31:54.520
might actually change the
goals that you're achieving.

00:31:54.520 --> 00:31:56.830
Here's a screenshot
of the What-If tool.

00:31:56.830 --> 00:32:00.100
What you see here on the right
is a whole bunch of data points

00:32:00.100 --> 00:32:02.140
that are classified
by your model.

00:32:02.140 --> 00:32:03.880
Data points of a
similar color have

00:32:03.880 --> 00:32:06.340
been given a similar score.

00:32:06.340 --> 00:32:08.740
You can select a
particular data point,

00:32:08.740 --> 00:32:10.700
and then with the
features on the right,

00:32:10.700 --> 00:32:12.940
you can actually modify
the feature value

00:32:12.940 --> 00:32:16.030
to see how changing the
input would potentially

00:32:16.030 --> 00:32:17.630
change the output.

00:32:17.630 --> 00:32:21.490
For example, if I changed the
age defined in this example,

00:32:21.490 --> 00:32:24.100
does it actually change
my classification?

00:32:24.100 --> 00:32:25.990
If it does, that might
tell me something

00:32:25.990 --> 00:32:28.840
about how age is
influencing my model,

00:32:28.840 --> 00:32:30.722
and where potentially,
there may be biases,

00:32:30.722 --> 00:32:32.680
or where I need to
deep-dive a little bit more.

00:32:35.530 --> 00:32:37.570
We also hope to take
this a step further

00:32:37.570 --> 00:32:39.190
with Fairness
Indicators, which will

00:32:39.190 --> 00:32:41.313
be launched later this year.

00:32:41.313 --> 00:32:42.730
Fairness Indicators
will be a tool

00:32:42.730 --> 00:32:45.510
that is built on top of
TensorFlow Model Analysis,

00:32:45.510 --> 00:32:49.810
and as a result, can work end
to end with the TFX pipeline.

00:32:49.810 --> 00:32:52.050
TSX stands for
TensorFlow Extended.

00:32:52.050 --> 00:32:55.180
And it's a platform that
allows you to train, evaluate,

00:32:55.180 --> 00:32:57.550
and serve your
models, all in one go.

00:32:57.550 --> 00:33:00.370
And so we're hoping to build
fairness into this workflow

00:33:00.370 --> 00:33:02.890
and into these processes.

00:33:02.890 --> 00:33:05.080
But Fairness Indicators
will also work alone.

00:33:05.080 --> 00:33:06.610
It'll work as an
independent tool

00:33:06.610 --> 00:33:10.570
that can be used with
any production pipeline.

00:33:10.570 --> 00:33:12.460
We hope that with
Fairness Indicators,

00:33:12.460 --> 00:33:16.450
you'll be able to actually
look at data on a large scale,

00:33:16.450 --> 00:33:18.940
and see actually how
your model performs.

00:33:18.940 --> 00:33:22.390
You can compute fairness metrics
for any individual group,

00:33:22.390 --> 00:33:26.230
and visualize these comparisons
to a baseline slice.

00:33:26.230 --> 00:33:28.810
Here, for example, you
can see the baseline slice

00:33:28.810 --> 00:33:31.510
as the overall average
metric in blue,

00:33:31.510 --> 00:33:33.010
and then you can
actually compare

00:33:33.010 --> 00:33:35.470
how individual groups
or individual slices

00:33:35.470 --> 00:33:38.380
compare to that baseline.

00:33:38.380 --> 00:33:40.920
For example, some may have
a higher false negative rate

00:33:40.920 --> 00:33:44.810
than average, while
others may have a lower.

00:33:44.810 --> 00:33:47.600
We'll provide feedback
about these main metrics

00:33:47.600 --> 00:33:51.570
that we believe have been useful
for various fairness use cases.

00:33:51.570 --> 00:33:53.850
You can then use
Fairness Indicators also

00:33:53.850 --> 00:33:55.560
to evaluate at
multiple thresholds

00:33:55.560 --> 00:33:58.420
to understand how
performance changes,

00:33:58.420 --> 00:34:00.210
and how maybe
changes to your model

00:34:00.210 --> 00:34:03.570
could actually lead to different
outcomes for different users.

00:34:03.570 --> 00:34:05.460
If you find a slice
that doesn't seem

00:34:05.460 --> 00:34:07.680
to be performing as well
as you expect it to,

00:34:07.680 --> 00:34:09.420
you can actually take
that slice further

00:34:09.420 --> 00:34:13.580
by deep-diving immediately
with the What-If tool.

00:34:13.580 --> 00:34:15.560
We will also be providing
confidence intervals,

00:34:15.560 --> 00:34:18.060
so that you can understand where
the differences that you're

00:34:18.060 --> 00:34:21.449
seeing are significant,
and where we may actually

00:34:21.449 --> 00:34:23.560
need more data to better
understand the problem.

00:34:26.710 --> 00:34:28.300
With Fairness
Indicators, we'll also

00:34:28.300 --> 00:34:29.802
be launching case
studies for how

00:34:29.802 --> 00:34:31.719
we've leveraged these
metrics and improvements

00:34:31.719 --> 00:34:34.179
in the past internally
in our own products.

00:34:34.179 --> 00:34:36.370
We hope that this will
help provide context

00:34:36.370 --> 00:34:39.040
about where we found
certain metrics useful, what

00:34:39.040 --> 00:34:41.020
kinds of insights
they've provided us,

00:34:41.020 --> 00:34:43.870
and where we found that certain
metrics actually haven't really

00:34:43.870 --> 00:34:46.120
served the full purpose.

00:34:46.120 --> 00:34:47.920
We'll also provide
benchmark datasets

00:34:47.920 --> 00:34:52.267
that can be immediately used
for vision and text use cases.

00:34:52.267 --> 00:34:54.100
We hope that Fairness
Indicators will simply

00:34:54.100 --> 00:34:57.340
be a start to being able to
ask questions of our models,

00:34:57.340 --> 00:35:00.460
understand fairness concerns,
and then eventually, over time,

00:35:00.460 --> 00:35:03.040
improve them.

00:35:03.040 --> 00:35:05.020
Our commitment to you
is that we continue

00:35:05.020 --> 00:35:07.990
to measure, improve,
and share our learnings

00:35:07.990 --> 00:35:09.610
related to fairness.

00:35:09.610 --> 00:35:11.890
It is important not only
that we make our own products

00:35:11.890 --> 00:35:14.008
work for all users,
but that we continue

00:35:14.008 --> 00:35:16.300
to share these best practices
and learnings so that we,

00:35:16.300 --> 00:35:19.160
as an industry, can continue
to develop fairer products--

00:35:19.160 --> 00:35:22.550
products that work
equitably for everybody.

00:35:22.550 --> 00:35:24.410
One thing I do
want to underscore

00:35:24.410 --> 00:35:27.080
is that we do know
that in order to create

00:35:27.080 --> 00:35:30.630
diverse products, products
that work for diverse users,

00:35:30.630 --> 00:35:34.035
it is also important to have
diverse voices in the room.

00:35:34.035 --> 00:35:35.660
This not only means
making sure that we

00:35:35.660 --> 00:35:38.420
have diverse voices internally
working on our products,

00:35:38.420 --> 00:35:40.160
but also means
that we include you

00:35:40.160 --> 00:35:43.080
as the community
in this process.

00:35:43.080 --> 00:35:44.990
We want your feedback
on our products,

00:35:44.990 --> 00:35:46.880
but we also want
to learn from you

00:35:46.880 --> 00:35:49.190
about how you're tackling
fairness and inclusion

00:35:49.190 --> 00:35:51.920
in your own work, what
lessons you're learning,

00:35:51.920 --> 00:35:53.927
what resources you're
finding useful.

00:35:53.927 --> 00:35:56.510
And we want to work with you to
continue and build and develop

00:35:56.510 --> 00:35:58.890
this resource toolkit,
so that we can continue,

00:35:58.890 --> 00:36:00.710
as an industry, to
build products that

00:36:00.710 --> 00:36:02.400
are inclusive for everyone.

00:36:02.400 --> 00:36:04.340
Thank you.

00:36:04.340 --> 00:36:07.390
[MUSIC PLAYING]

