WEBVTT
Kind: captions
Language: en

00:00:00.150 --> 00:00:01.400
&gt;&gt; All right Charles, so what would you, what would

00:00:01.400 --> 00:00:04.910
you call let's, let's let's think about this bottom one first.

00:00:04.910 --> 00:00:05.060
&gt;&gt; Okay.

00:00:05.060 --> 00:00:08.029
&gt;&gt; So what would you call an approach to reinforcement learning that

00:00:08.029 --> 00:00:10.990
what it does is it builds a model and then plans with it?

00:00:10.990 --> 00:00:12.960
&gt;&gt; A planner?

00:00:12.960 --> 00:00:16.149
&gt;&gt; Well, it's not a planner. I mean, the planner's inside of it.

00:00:16.149 --> 00:00:16.640
&gt;&gt; Sure.

00:00:16.640 --> 00:00:19.960
&gt;&gt; The overall system, this sort of blue box. Turns

00:00:19.960 --> 00:00:22.690
transitions into policies so it's kind of a reinforcement learner.

00:00:22.690 --> 00:00:23.180
&gt;&gt; Yeah.

00:00:23.180 --> 00:00:25.810
&gt;&gt; But it's one that builds a model inside.

00:00:27.410 --> 00:00:30.300
&gt;&gt; I would call that. You know what I would

00:00:30.300 --> 00:00:33.850
call that, I would call that model-based learning or model-based planning.

00:00:33.850 --> 00:00:36.402
&gt;&gt; Actually it's called model-based reinforcement learning.

00:00:36.402 --> 00:00:37.170
&gt;&gt; Mm-hm.

00:00:37.170 --> 00:00:39.030
&gt;&gt; So this is, this is, in fact, my,

00:00:39.030 --> 00:00:41.480
you said, reinforcement learning is your favorite kind of learning?

00:00:41.480 --> 00:00:41.714
&gt;&gt; Mm-hm.

00:00:41.714 --> 00:00:43.010
&gt;&gt; Model-based reinforcement learning is

00:00:43.010 --> 00:00:44.640
my favorite kind of of reinforcement learning.

00:00:44.640 --> 00:00:44.690
&gt;&gt; Mm.

00:00:44.690 --> 00:00:46.790
&gt;&gt; But we're not going to get to talk about it

00:00:46.790 --> 00:00:48.200
very much, so I wanted to at least have this

00:00:48.200 --> 00:00:50.510
slide to give people a chance to, at least, you

00:00:50.510 --> 00:00:52.350
know, these are all pieces that you can imagine doing. Right?

00:00:52.350 --> 00:00:54.810
This, this piece you can think of as being what

00:00:54.810 --> 00:00:57.050
we did in, when we're talking about supervised learning. And this

00:00:57.050 --> 00:00:59.310
piece is what you talked about last time. So you

00:00:59.310 --> 00:01:02.610
know, you could build a model based reinforcement learner that way.

00:01:02.610 --> 00:01:03.240
&gt;&gt; Yeah, that makes sense.

00:01:03.240 --> 00:01:05.880
&gt;&gt; Alright, how about this other idea, where, where you start off

00:01:05.880 --> 00:01:08.320
with a model and then you pretend you don't have a model, you

00:01:08.320 --> 00:01:12.430
pretend you're just in a learning situation by turning that model into,

00:01:12.430 --> 00:01:15.760
into transitions just by simulating them.

00:01:15.760 --> 00:01:17.940
Then the learner interacts with the simulator.

00:01:17.940 --> 00:01:19.080
And then spits out a policy.

00:01:19.080 --> 00:01:22.300
&gt;&gt; Well, I could come up with one of two answers.

00:01:22.300 --> 00:01:22.440
&gt;&gt; Okay.

00:01:22.440 --> 00:01:26.940
&gt;&gt; So I could try to do pattern matching on the answer to the second one. And

00:01:26.940 --> 00:01:28.600
since that one's model based, then this one's

00:01:28.600 --> 00:01:33.460
transition based, which is kind of cute. Or I

00:01:33.460 --> 00:01:35.500
could say, well one difference between at least

00:01:35.500 --> 00:01:38.420
inside the kind of blue box is it's sort

00:01:38.420 --> 00:01:40.540
of a model free reinforcement module. because the

00:01:40.540 --> 00:01:43.210
learner does not ever get to see the model.

00:01:43.210 --> 00:01:45.430
&gt;&gt; This is true, the learning piece is model free

00:01:45.430 --> 00:01:49.640
but we're using model free learner in service of planning.

00:01:49.640 --> 00:01:50.330
&gt;&gt; Okay.

00:01:50.330 --> 00:01:53.564
&gt;&gt; So, I don't know, I don't have a good

00:01:53.564 --> 00:01:59.400
this like model free planner, or an RL-based planner, maybe.

00:01:59.400 --> 00:02:01.010
Like I said, I wasn't going to grade it, I just

00:02:01.010 --> 00:02:03.290
was kind of interested to see what, what you'd say.

00:02:03.290 --> 00:02:07.690
&gt;&gt; Hm. Well I, I was pretty, I felt pretty good about the model-based RL one.

00:02:07.690 --> 00:02:08.250
&gt;&gt; Yeah, well this

00:02:08.250 --> 00:02:11.050
is the actual term that's used in the field. I'm, I'm sure there

00:02:11.050 --> 00:02:14.030
are some terms that are used here but nothing, nothing really very consistently.

00:02:14.030 --> 00:02:16.900
&gt;&gt; Hm. What if I called it the blue box planner?

00:02:16.900 --> 00:02:18.730
&gt;&gt; You could call it that, and, but had I, had I

00:02:18.730 --> 00:02:20.850
drawn it with a black box, it would, it had a better name.

00:02:20.850 --> 00:02:22.300
&gt;&gt; mm, oo, that would have been really

00:02:22.300 --> 00:02:24.880
cool, black box planner. I like that. Okay.

00:02:24.880 --> 00:02:27.000
&gt;&gt; But but I want to point out that one of the most

00:02:27.000 --> 00:02:29.390
successful applications of reinforcement learning are

00:02:29.390 --> 00:02:33.905
least most celebrated was Jerry Tassara's

00:02:33.905 --> 00:02:37.950
backgammon playing program, which used exactly this approach. Backgammon

00:02:37.950 --> 00:02:40.300
is a board game. So we have a complete model

00:02:40.300 --> 00:02:42.110
of it, but we don't really know how to plan

00:02:42.110 --> 00:02:44.598
it, it's a very complex, very large state space. So

00:02:44.598 --> 00:02:46.781
what he did is, he actually used a simulator

00:02:46.781 --> 00:02:49.023
backgammon, to generate transitions, and

00:02:49.023 --> 00:02:51.206
then used reinforcement learning approach

00:02:51.206 --> 00:02:54.100
TD, to, to create a policy for that. So it,

00:02:54.100 --> 00:02:58.080
his TD Gammon system followed exactly this, this overall pattern.

00:02:58.080 --> 00:02:59.276
&gt;&gt; Yeah, it was very influential

00:02:59.276 --> 00:03:01.820
work and generated many mildly embarrassing Master's theses.

00:03:01.820 --> 00:03:05.470
&gt;&gt; [LAUGH]

00:03:05.470 --> 00:03:07.140
I can only think of one. Oh, actually I

00:03:07.140 --> 00:03:08.920
can think, that's not true, I can think of two.

00:03:08.920 --> 00:03:09.540
&gt;&gt; Oh really?

00:03:09.540 --> 00:03:11.610
&gt;&gt; Well, which one are you, you're thinking of yours.

00:03:11.610 --> 00:03:14.620
&gt;&gt; Yes, I'm thinking of mine. I try not to think about it.

00:03:14.620 --> 00:03:17.830
&gt;&gt; The, the other one is, Justin Boyin, who's a good

00:03:17.830 --> 00:03:20.680
friend. And, a highly influential reinforcement

00:03:20.680 --> 00:03:24.440
learning contributor, and now Googler. Who,

00:03:24.440 --> 00:03:26.680
his Masters thesis was also, it's basically

00:03:26.680 --> 00:03:28.950
another TD. He did backgammon with RL.

00:03:28.950 --> 00:03:31.830
&gt;&gt; Oh, I didn't realize that. I, I actually like him. I think

00:03:31.830 --> 00:03:36.130
he's very cool. [LAUGH]. So, I'm sure that, his was not mildly embarrassing.

00:03:37.830 --> 00:03:39.395
&gt;&gt; I mean, you know, everybody's master's

00:03:39.395 --> 00:03:41.910
thesis are mildly embarrassing because you're, you're struggling

00:03:41.910 --> 00:03:44.310
to learn about how to talk about research,

00:03:44.310 --> 00:03:45.830
and so you're not going to get it perfect.

00:03:45.830 --> 00:03:48.550
&gt;&gt; Yeah, and in my case, it was it definitely model free.

00:03:48.550 --> 00:03:49.870
&gt;&gt; Right, the only

00:03:49.870 --> 00:03:53.170
non-embarrassing master's thesis that I'm aware of.

00:03:53.170 --> 00:03:54.660
&gt;&gt; Shanon's.

00:03:54.660 --> 00:03:58.850
&gt;&gt; Oh sure, alright, well Shannon. What was his master's thesis?

00:03:58.850 --> 00:04:00.240
&gt;&gt; It was information theory.

00:04:00.240 --> 00:04:01.850
&gt;&gt; Oh, yeah. It's pretty impressive.

00:04:01.850 --> 00:04:02.290
&gt;&gt; Yeah.

00:04:02.290 --> 00:04:04.760
&gt;&gt; Rob Schapire's master's thesis is pretty cool.

00:04:04.760 --> 00:04:05.970
&gt;&gt; I have no doubt.

00:04:05.970 --> 00:04:09.720
&gt;&gt; The boosting guy. He did, he did pom de pe learning.

00:04:09.720 --> 00:04:11.720
&gt;&gt; For his master's thesis?

00:04:11.720 --> 00:04:13.720
&gt;&gt; Yeah. With the diversity representation.

00:04:13.720 --> 00:04:14.830
&gt;&gt; I am a terrible human being.

00:04:14.830 --> 00:04:19.526
[LAUGH]. I mean wow. I mean it is true though, you look at someone like Shannon

00:04:19.526 --> 00:04:21.124
and you just think to yourself, oh, for

00:04:21.124 --> 00:04:23.823
his Master's thesis he did, he invented information theory.

00:04:23.823 --> 00:04:26.840
[LAUGH] You know, like wow, I wonder what he did for his PhD thesis? You know

00:04:26.840 --> 00:04:28.770
what he did for his PhD thesis? Nobody

00:04:28.770 --> 00:04:30.800
knows, nobody cares, because he based it on. [LAUGH]

00:04:30.800 --> 00:04:33.100
&gt;&gt; He, he had already done the information theories. They're like

00:04:33.100 --> 00:04:35.210
here, fill out a piece of paper, you can pick up

00:04:35.210 --> 00:04:37.330
a PHD. He did something on AI, I can't remember what

00:04:37.330 --> 00:04:39.970
it was, but it was, it was totally unimportant and nobody cares

00:04:39.970 --> 00:04:43.018
about it. But information theory? Yeah. [LAUGH].

00:04:43.018 --> 00:04:43.450
&gt;&gt; Thanks.

00:04:43.450 --> 00:04:46.000
&gt;&gt; All right, well, yeah, okay. [LAUGH].

00:04:46.000 --> 00:04:48.770
&gt;&gt; II think, I think Schapire's PHD was boosting.

00:04:50.460 --> 00:04:53.970
&gt;&gt; Thanks Rob. Make us all look bad, why don't you.

