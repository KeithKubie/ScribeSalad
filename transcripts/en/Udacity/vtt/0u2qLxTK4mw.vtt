WEBVTT
Kind: captions
Language: en

00:00:11.330 --> 00:00:14.910
Thank you, my name is Sebastian Thrun, I'm the founder of Udacity.

00:00:14.910 --> 00:00:19.735
I'm super, super delighted to have with me one of the true leaders, pioneers,

00:00:19.735 --> 00:00:25.140
and today's contemporary international figures in the age of Artificial Intelligence,

00:00:25.140 --> 00:00:26.940
Kai-Fu Lee. Kai-Fu, welcome.

00:00:26.940 --> 00:00:33.080
Thank you. I would introduce you the same way.

00:00:33.080 --> 00:00:36.150
Thank you. Kai-Fu actually just wrote a book.

00:00:36.150 --> 00:00:38.455
It's probably colored red.

00:00:38.455 --> 00:00:40.730
It's called AI Superpowers: China,

00:00:40.730 --> 00:00:42.440
Silicon Valley, and New World Order.

00:00:42.440 --> 00:00:47.270
He's right now on a US book tour. It's selling record levels.

00:00:47.270 --> 00:00:49.640
My girlfriend loves the book, so do I.

00:00:49.640 --> 00:00:55.045
And you're here with us at Udacity to answer questions and give us some insights.

00:00:55.045 --> 00:00:55.665
Yep.

00:00:55.665 --> 00:00:56.605
So, welcome.

00:00:56.605 --> 00:00:57.050
Thank you.

00:00:57.050 --> 00:00:59.900
Well, I should say I've known Kai-Fu for longer than I can imagine.

00:00:59.900 --> 00:01:01.640
I used to hear about him back in the day when I was

00:01:01.640 --> 00:01:03.920
an exchange student, Masters student at Carnegie Mellon.

00:01:03.920 --> 00:01:06.855
He was the guy next door, new faculty member.

00:01:06.855 --> 00:01:09.830
And I still remember that the faculty at Carnegie Mellon

00:01:09.830 --> 00:01:13.070
believed that they never had a superstar like him among themselves.

00:01:13.070 --> 00:01:15.305
And then he moved to Microsoft,

00:01:15.305 --> 00:01:17.060
Google, and eventually to China.

00:01:17.060 --> 00:01:18.560
started his own venture firm,

00:01:18.560 --> 00:01:20.605
Sinovation Ventures in China.

00:01:20.605 --> 00:01:22.565
In the book, I’m going to go straight to the chase here.

00:01:22.565 --> 00:01:25.370
In the book, you're telling us about AI in China.

00:01:25.370 --> 00:01:25.950
Right.

00:01:25.950 --> 00:01:28.295
What's different in China from the United States?

00:01:28.295 --> 00:01:32.265
Well. China's started very much behind.

00:01:32.265 --> 00:01:36.170
There were a lot of decent AI researchers in China.

00:01:36.170 --> 00:01:40.930
I was fortunate in 1998 to have founded the Microsoft Research Asia,

00:01:40.930 --> 00:01:46.390
which I think ended up training pretty much the whole generation of AI people in China.

00:01:46.390 --> 00:01:48.930
But AI wasn't in vogue back then,

00:01:48.930 --> 00:01:51.340
it was kind of not very popular.

00:01:51.340 --> 00:01:53.855
We didn't even want to use the word AI.

00:01:53.855 --> 00:01:56.520
There was a saying that "when it works,

00:01:56.520 --> 00:02:00.230
it's product engineering, whatever it doesn't work is AI.

00:02:00.230 --> 00:02:04.205
So, we were in that state for quite a while.

00:02:04.205 --> 00:02:11.555
But then, US deep learning Geoff Hinton and others really demonstrated that

00:02:11.555 --> 00:02:19.550
AI can solve a lot of narrow problems extremely well and China was behind.

00:02:19.550 --> 00:02:23.460
But it had this backbone of pretty smart researchers,

00:02:23.460 --> 00:02:29.880
and then some of the early China companies like Baidu and Sinovation Ventures,

00:02:29.880 --> 00:02:32.120
companies we funded, were doing some work.

00:02:32.120 --> 00:02:34.575
But it's really tiny compared to the US.

00:02:34.575 --> 00:02:39.850
But what happened was when AlphaGo beat Lee Sedol,

00:02:39.850 --> 00:02:41.755
that woke all of China up.

00:02:41.755 --> 00:02:47.780
that this ancient game requiring not just intelligence but wisdom,

00:02:47.780 --> 00:02:53.800
that how could a US/UK company just beat the best player in Asia,

00:02:53.800 --> 00:02:58.625
then there must be something about AlphaGo, about AI.

00:02:58.625 --> 00:03:04.864
And then China went on fire in the last two and a half years with entrepreneurs,

00:03:04.864 --> 00:03:11.260
VCs, governments all injecting money, talent, resources,

00:03:11.260 --> 00:03:13.400
And in the short two and a half years,

00:03:13.400 --> 00:03:18.005
China has caught up with the US in implementation,

00:03:18.005 --> 00:03:21.490
monetization, and valuation of AI.

00:03:21.490 --> 00:03:22.980
And that's what this book is about,

00:03:22.980 --> 00:03:24.600
a magical two and a half years 

00:03:24.600 --> 00:03:27.200
seems like an impossible task.

00:03:27.200 --> 00:03:29.460
Mission impossible, but it happened.

00:03:29.460 --> 00:03:33.665
So the Chinese government has announced putting tens of billions of dollars into AI.

00:03:33.665 --> 00:03:34.175
Yes.

00:03:34.175 --> 00:03:37.430
What does it mean and should we do the same in this country(the US)?

00:03:37.990 --> 00:03:41.200
Well, first of all, what does it mean?

00:03:41.200 --> 00:03:45.120
It actually means infrastructure spending.

00:03:45.120 --> 00:03:49.505
I think the Chinese government is not picking winners.

00:03:49.505 --> 00:03:52.795
Governments aren't very good at picking winners in technology.

00:03:52.795 --> 00:03:57.860
So they're, for example, putting in huge amounts of money to build

00:03:57.860 --> 00:04:03.835
a new city the size of Chicago that has autonomous driving built-in,

00:04:03.835 --> 00:04:05.565
and within that city,

00:04:05.565 --> 00:04:09.460
the center of the town will have two layers.

00:04:09.460 --> 00:04:13.770
The pedestrians, pets, bicycles will go on the top, green park,

00:04:13.770 --> 00:04:16.755
very beautiful place with no cars.

00:04:16.755 --> 00:04:22.170
All the cars, autonomous or not, will drive in B1 in that downtown.

00:04:22.170 --> 00:04:24.124
I mean you can imagine the expense,

00:04:24.124 --> 00:04:27.950
but that's the kind of effort that I think will make autonomous

00:04:27.950 --> 00:04:32.930
driving safe in the most potentially hazardous area,

00:04:32.930 --> 00:04:38.540
that is downtown, busy traffic with people, avoiding cars ever hitting people.

00:04:38.540 --> 00:04:44.780
So, Zhejiang province is building a new highway with sensors that will talk to cars,

00:04:44.780 --> 00:04:49.970
to give cars a lower chance of having accidents.

00:04:49.970 --> 00:04:54.520
So, I think that kind of large infrastructure spend is the largest.

00:04:54.520 --> 00:04:57.955
Obviously, there's also money being put into AI parks.

00:04:57.955 --> 00:05:02.070
The city of Nanjing shows a gigantic area,

00:05:02.070 --> 00:05:09.075
I think about two million square feet of space to be its new AI Park,

00:05:09.075 --> 00:05:13.630
and because Nanjing had really great schools and they now want to say,

00:05:13.630 --> 00:05:16.220
“Hey, AI is now our big big area.”

00:05:16.220 --> 00:05:20.495
So, I think infrastructure spend like this are the most powerful.

00:05:20.495 --> 00:05:25.515
There's some money being put into AI funds that are investing.

00:05:25.515 --> 00:05:28.360
So, yes. A lot of money's being put in.

00:05:28.360 --> 00:05:34.625
And I think it's largely in infrastructure, advanced research,

00:05:34.625 --> 00:05:41.240
making sure that companies can get funding and their products can take off.

00:05:41.240 --> 00:05:46.605
And yes, I think the US should do something like this if that money can be found in the budget.

00:05:46.605 --> 00:05:48.680
Yeah. So, in your book,

00:05:48.680 --> 00:05:50.030
you talk a bit about values.

00:05:50.030 --> 00:05:53.560
You talk about what drives the Chinese entrepreneur.

00:05:53.560 --> 00:05:55.140
What makes people successful?

00:05:55.140 --> 00:05:59.100
Can you tell us more about the Chinese thinking of entrepreneurship,

00:05:59.100 --> 00:06:01.070
about big businesses?

00:06:01.070 --> 00:06:05.155
Yeah. Well, I'm a big admirer of Silicon Valley,

00:06:05.155 --> 00:06:09.440
but I've also become a big admirer of the Chinese entrepreneurship style,

00:06:09.440 --> 00:06:13.280
and I think that's very important to iterating with AI.

00:06:13.280 --> 00:06:15.075
So, you know in Silicon Valley,

00:06:15.075 --> 00:06:18.660
there's Steve Jobs with the iPhone as the typical.

00:06:18.660 --> 00:06:21.665
What you're doing with Kitty Hawk is another great example.

00:06:21.665 --> 00:06:22.705
And Udacity.

00:06:22.705 --> 00:06:25.820
And Udacity. Yes. Udacity is a little bit-

00:06:25.820 --> 00:06:26.760
A much bigger company.

00:06:26.760 --> 00:06:29.330
Udacity is a bit more like Chinese-style.

00:06:29.330 --> 00:06:29.885
How so?

00:06:29.885 --> 00:06:32.890
Well, I think, you know, Kitty Hawk,

00:06:32.890 --> 00:06:37.370
iPhone is like a brilliant breakthrough technology that's going to

00:06:37.370 --> 00:06:42.200
change the world. The light bulb goes off and brilliant scientists leading forward.

00:06:42.200 --> 00:06:45.860
I think Udacity is wonderful but it's

00:06:45.860 --> 00:06:51.465
more, you know, operational excellence, getting more people, getting more content.

00:06:51.465 --> 00:06:54.800
So, that's actually something the Chinese really excel at.

00:06:54.800 --> 00:07:00.525
And in the book I talk about, you know, in the US there's a book called The Lean Startup,

00:07:00.525 --> 00:07:04.880
but actually China does the Lean Startup better than Silicon Valley,

00:07:04.880 --> 00:07:07.880
and also takes the Lean Startup from zero to one,

00:07:07.880 --> 00:07:09.385
and then one to a million.

00:07:09.385 --> 00:07:14.240
So, because the Chinese entrepreneurial environment is very,

00:07:14.240 --> 00:07:16.675
very tough, people compete fiercely.

00:07:16.675 --> 00:07:20.490
In the book I talk about, it's like Colosseum,

00:07:20.490 --> 00:07:25.875
where gladiators are thrown in and only one will survive.

00:07:25.875 --> 00:07:29.980
And also, there's not much frowning on copycats.

00:07:29.980 --> 00:07:31.640
I'm not talking about IP theft,

00:07:31.640 --> 00:07:34.535
I'm just talking about taking the look and feel from

00:07:34.535 --> 00:07:39.720
another product within legal bounds or a feature.

00:07:39.720 --> 00:07:43.670
So, in that kind of environment,

00:07:43.830 --> 00:07:51.490
the only way companies can win is by building a product that is impregnable, and uncopyable.

00:07:51.490 --> 00:07:58.680
So, the examples I give in the book are the Chinese Yelp or Groupon called Meituan,

00:07:58.680 --> 00:08:03.230
It would go through so much trouble and depth to

00:08:03.230 --> 00:08:08.385
create an infrastructure that will change the way Chinese people eat.

00:08:08.385 --> 00:08:10.960
And once you build that infrastructure,

00:08:10.960 --> 00:08:14.650
competitors can't copy it or it's too expensive to copy.

00:08:14.650 --> 00:08:22.235
And that infrastructure, essentially allows food to be delivered to your home within 30 minutes.

00:08:22.235 --> 00:08:25.024
You'd have a choice of maybe 500 restaurants, 

00:08:25.024 --> 00:08:27.595
Before you leave work, you choose what you want to eat.

00:08:27.595 --> 00:08:30.970
When you're home in 30 minutes, the food is there waiting for you,

00:08:30.970 --> 00:08:34.910
And the cost of delivery is about $0.70.

00:08:34.910 --> 00:08:38.779
And the way in which they achieved that operational excellence

00:08:38.779 --> 00:08:42.880
is by grinding away a few cents a month,

00:08:42.880 --> 00:08:47.390
figuring out new ways, how to find people who will work for less money,

00:08:47.390 --> 00:08:50.685
how to find cheaper ways of transportation,

00:08:50.685 --> 00:08:53.710
such as cheaper electrical mopeds,

00:08:53.710 --> 00:08:56.170
and then how to deal with change of batteries.

00:08:56.170 --> 00:09:03.015
and also the people, since they're not making a lot of money, there will be turnover.

00:09:03.015 --> 00:09:05.760
So, having an HR process that has

00:09:05.760 --> 00:09:08.880
600,000 people with massive turnover every year.

00:09:08.880 --> 00:09:13.625
All of this to Silicon Valley feels like a lot of operational burden,

00:09:13.625 --> 00:09:17.510
but by doing it well and getting the delivery down to $0.70,

00:09:17.510 --> 00:09:20.570
you've essentially erected a very high wall around

00:09:20.570 --> 00:09:24.650
your business so that your competitor,if they want to do the same,

00:09:24.650 --> 00:09:26.260
they have to spend a few billion dollars.

00:09:26.260 --> 00:09:27.520
So, it's like the Amazon,

00:09:27.520 --> 00:09:31.450
where we have Jeff Bezos “we have so thin margins I'm proud of it”

00:09:31.450 --> 00:09:32.335
It is like that.

00:09:32.335 --> 00:09:35.780
Eric Schmidt recently very famously said in a confidential meeting that made it to

00:09:35.780 --> 00:09:40.380
the press, that he sees there being two versions of the Internet.

00:09:40.380 --> 00:09:42.130
There'd be an Internet run by

00:09:42.130 --> 00:09:44.570
the world and maybe a Chinese Internet run by China.

00:09:44.570 --> 00:09:46.490
What do you think about that?

00:09:46.490 --> 00:09:48.985
Well, if you look at my phone and my apps,

00:09:48.985 --> 00:09:50.760
they are completely different from yours.

00:09:50.760 --> 00:09:51.920
So, from that perspective,

00:09:51.920 --> 00:09:58.410
absolutely. The Chinese people are using all Chinese apps and they've gotten very good.

00:09:58.410 --> 00:10:03.170
You know, WeChat is better than WhatsApp.

00:10:03.170 --> 00:10:05.895
I saw that you are still using WhatsApp.

00:10:05.895 --> 00:10:08.810
I'm not quite yet using WhatsApp.

00:10:08.810 --> 00:10:10.390
What was that you use to send?

00:10:10.390 --> 00:10:11.380
Most for text messaging, yeah.

00:10:11.380 --> 00:10:12.760
With text messaging?

00:10:12.760 --> 00:10:13.870
Oh, my God, you don't know.

00:10:13.870 --> 00:10:14.190
Oh, my God.

00:10:14.190 --> 00:10:14.695
Beats me.

00:10:14.695 --> 00:10:18.890
The CEO of Kitty Hawk using text messaging.

00:10:18.890 --> 00:10:23.095
I mean, WeChat. Well, the Chinese apps are really, really good.

00:10:23.095 --> 00:10:29.300
Actually, the ones that were inspired by the US,

00:10:29.300 --> 00:10:30.955
often they get better than the US.

00:10:30.955 --> 00:10:32.680
Not always, but often.

00:10:32.680 --> 00:10:38.245
And there are many now that are not inspired by the US, they're Chinese innovation.

00:10:38.245 --> 00:10:41.375
like there are two social networks with video.

00:10:41.375 --> 00:10:43.255
They don't exist here yet,

00:10:43.255 --> 00:10:48.595
and there's a gamification of e-commerce that in three years became $25 billion.

00:10:48.595 --> 00:10:52.475
So, all these concepts are now innovative in China,

00:10:52.475 --> 00:10:53.825
the apps are different,

00:10:53.825 --> 00:10:57.755
the Chinese users don't use much American apps and vice-versa.

00:10:57.755 --> 00:10:59.560
So, I think in that sense, I agree.

00:10:59.560 --> 00:11:00.640
Speaking about this, I mean,

00:11:00.640 --> 00:11:01.680
there's a whole bunch of companies,

00:11:01.680 --> 00:11:03.090
Udacity being no exception,

00:11:03.090 --> 00:11:04.590
who try do business in China,

00:11:04.590 --> 00:11:06.800
then starts opening offices in China,

00:11:06.800 --> 00:11:09.845
having joint ventures or similar vehicles.

00:11:09.845 --> 00:11:14.185
What's your advice for an American company who wants to go into the Chinese business?

00:11:14.185 --> 00:11:18.230
Well, I think Udacity is actually a very exceptional case.

00:11:18.230 --> 00:11:23.450
I think because what you carry is education and that's dear and common to all the people.

00:11:23.450 --> 00:11:29.005
So, there are few such things that can potentially cross the US and China.

00:11:29.005 --> 00:11:32.220
Pure core technology companies,

00:11:32.220 --> 00:11:35.450
educational content, entertainment are

00:11:35.450 --> 00:11:38.795
probably three that I think can cross the boundaries.

00:11:38.795 --> 00:11:42.280
All the other things will be very, very tough.

00:11:42.280 --> 00:11:48.860
Take Google, Facebook, Amazon, Uber, or Airbnb.

00:11:48.860 --> 00:11:52.360
They all have a very hard time in China.

00:11:52.360 --> 00:11:53.960
And why is that?

00:11:53.960 --> 00:11:58.760
Well, the Chinese have parallel universe that has been established,

00:11:58.760 --> 00:12:04.315
people have established patterns of how they use WeChat, Alibaba, Taobao,

00:12:04.315 --> 00:12:07.030
and so on and the brands are established,

00:12:07.030 --> 00:12:08.945
the apps are very easy to use,

00:12:08.945 --> 00:12:10.965
they cater to the Chinese way,

00:12:10.965 --> 00:12:18.240
and they're intertwined in the way that, you know, if you use the Chinese ride-sharing system,

00:12:18.240 --> 00:12:21.130
then you're naturally going to use WeChat

00:12:21.130 --> 00:12:24.265
which is then naturally going to cause you to use WeChat Pay.

00:12:24.265 --> 00:12:27.330
So, these things are like a puzzle put together.

00:12:27.330 --> 00:12:32.085
If you now grab Amazon and say we're going to put that puzzle and make

00:12:32.085 --> 00:12:34.170
it fit in

00:12:34.170 --> 00:12:38.240
this already completed Chinese puzzle, it just doesn't fit.

00:12:38.240 --> 00:12:46.550
So, my advice to most American companies that want to go to China is don't,

00:12:49.020 --> 00:12:52.080
unless you are in education,

00:12:52.080 --> 00:12:57.130
technology license, entertainment. Those areas I think are okay.

00:12:57.130 --> 00:13:00.920
I mean over time I do hope the two universes will have a chance to cross,

00:13:00.920 --> 00:13:02.675
but right now it's just very hard.

00:13:02.675 --> 00:13:04.960
Interesting. I’m gonna come back to your book. 

00:13:04.960 --> 00:13:09.205
This is gonna be the book to read for everybody for a long time to come

00:13:09.205 --> 00:13:13.115
in my opinion and I should say Kai-Fu Lee is truly apparently effective.

00:13:13.115 --> 00:13:14.840
When I joined Carnegie Mellon,

00:13:14.840 --> 00:13:18.115
my first program was to work on something called Bill, which was

00:13:18.115 --> 00:13:22.150
a world-class machine-learning Othello player designed by a professor

00:13:22.150 --> 00:13:26.300
called Kai-Fu Lee who sits next to me and he is a world-class programmer,

00:13:26.300 --> 00:13:27.485
I can attest to this.

00:13:27.485 --> 00:13:29.520
But I want to ask you about the AI end game.

00:13:29.520 --> 00:13:32.380
So, the AI is now in everybody's mind,

00:13:32.380 --> 00:13:35.685
we have people like Elon Musk saying AI will eat us for breakfast,

00:13:35.685 --> 00:13:39.260
Stephen Hawking says it's the biggest threat in humanity.

00:13:39.260 --> 00:13:43.930
Bill Gates has been skeptical about it. What is the end game?

00:13:43.930 --> 00:13:50.560
Well, I think we cannot project an end game when we don't know the next 20 moves,

00:13:50.560 --> 00:13:53.680
we're talking about, you know, Othello end goal right?

00:13:53.680 --> 00:14:00.390
Right now, we have a narrow AI,which is within a single domain,

00:14:00.390 --> 00:14:04.180
you can train brilliant systems that can play Go,

00:14:04.180 --> 00:14:06.685
that can make decisions for loan officers,

00:14:06.685 --> 00:14:11.300
advertising agents, accountants, paralegal, reporters.

00:14:11.300 --> 00:14:16.300
I think we're at the stage where we have this blossoming of narrow AI,

00:14:16.300 --> 00:14:23.695
and then the chasm between narrow AI to general AI,

00:14:23.695 --> 00:14:28.730
the so-called end game of human intelligence is like

00:14:28.730 --> 00:14:32.305
a black dark hole that I can't see

00:14:32.305 --> 00:14:36.365
any moves that will clearly get us from here to there.

00:14:36.365 --> 00:14:43.930
So, my opinion is that I think of narrow AI not as a first step,

00:14:43.930 --> 00:14:49.095
but as maybe the most important step. And it is like electricity,

00:14:49.095 --> 00:14:52.845
where apps can be built and it will create so much value and

00:14:52.845 --> 00:14:56.985
wealth and also displace a lot of jobs and present challenges.

00:14:56.985 --> 00:15:00.625
So, this book is actually just saying,

00:15:00.625 --> 00:15:02.670
"this first step is really important,

00:15:02.670 --> 00:15:03.910
let's play it right,

00:15:03.910 --> 00:15:05.460
and then if there's another step,

00:15:05.460 --> 00:15:09.160
we've got to survive the first step and make humanity better."

00:15:09.160 --> 00:15:11.520
So, I don't talk about the end game.

00:15:11.520 --> 00:15:15.235
Do you believe in general artificial intelligence?

00:15:15.235 --> 00:15:15.950
I do not.

00:15:15.950 --> 00:15:16.660
Why not?

00:15:16.660 --> 00:15:21.320
I choose to believe that we have a soul and that it cannot be replicated.

00:15:21.320 --> 00:15:27.900
I know this may not be the most popular view within the AI geeky community.

00:15:27.900 --> 00:15:30.890
Do you believe that without a soul we can

00:15:30.890 --> 00:15:33.735
accomplish a machine that can do many different tasks?

00:15:33.735 --> 00:15:36.060
I do, I believe we can.

00:15:36.630 --> 00:15:40.390
So, I think I see narrow AI becoming smarter.

00:15:40.390 --> 00:15:44.995
You know, it will have more and more capabilities. Autonomous driving, I mean Kitty Hawk,

00:15:44.995 --> 00:15:50.665
those are great examples, that single task is already multi-complex tasks,

00:15:50.665 --> 00:15:53.130
but I don't think Kitty Hawk has a soul,

00:15:53.130 --> 00:15:58.044
I don't think it can understand compassion and empathy,

00:15:58.044 --> 00:16:04.150
and I don't think people want to have robots to take care of them,

00:16:04.150 --> 00:16:06.110
be their nanny, and nurse,

00:16:06.110 --> 00:16:07.650
and doctor, and teacher.

00:16:07.650 --> 00:16:09.040
Let me push this a little bit.

00:16:09.040 --> 00:16:11.355
So, if I gave you a machine and say this is

00:16:11.355 --> 00:16:14.025
a piece of AI and it can take care of elderly people.

00:16:14.025 --> 00:16:18.275
How would you tell whether the machine has a soul or not?

00:16:18.275 --> 00:16:22.020
Well, I would say elderly people don't want it.

00:16:22.020 --> 00:16:25.945
So, if they don't want the product then it won't succeed.

00:16:25.945 --> 00:16:30.930
I've actually had an entrepreneur who built a pretty good robot for

00:16:30.930 --> 00:16:34.240
elderly care and it turns out that elderly people

00:16:34.240 --> 00:16:38.070
only use one function and that's called Customer Service.

00:16:38.070 --> 00:16:39.750
So, they push customer service

00:16:39.750 --> 00:16:40.500
to talk to a person?

00:16:40.500 --> 00:16:43.845
A video comes up and then the elderly person says,

00:16:43.845 --> 00:16:46.260
"Why hasn't my daughter called today?

00:16:46.260 --> 00:16:49.150
Let me tell you about my grandson."

00:16:49.150 --> 00:16:55.235
And I think there is an innate desire for people to interact with other people.

00:16:55.235 --> 00:16:57.770
So, even if the robot became quite good,

00:16:57.770 --> 00:17:01.090
it won't be the choice by the people.

00:17:01.090 --> 00:17:04.035
Now there has been work on AI, just to push a little harder here.

00:17:04.035 --> 00:17:04.460
Okay.

00:17:04.460 --> 00:17:06.595
That you see from University of Washington and other places that

00:17:06.595 --> 00:17:08.980
people are able to synthesize

00:17:08.980 --> 00:17:14.430
famous individuals like President Barack Obama in a way that looks believable.

00:17:14.430 --> 00:17:18.870
In fact, Udacity and our work with Georgia Tech.

00:17:18.870 --> 00:17:21.990
A Georgia Tech professor built an online agent,

00:17:21.990 --> 00:17:24.190
an online TA that throughout

00:17:24.190 --> 00:17:28.330
the entire class acted like a TA. And when the class was over,

00:17:28.330 --> 00:17:30.545
first this clone was actually a piece of A.I. not 

00:17:30.545 --> 00:17:32.975
a human being and every student was completely surprised.

00:17:32.975 --> 00:17:38.490
Do you think we cannot cross the chasm where we find AI agents believable?

00:17:38.490 --> 00:17:43.249
I think we can make them believable for certain tasks

00:17:43.249 --> 00:17:47.620
and there are many tasks for which we really don't need the human interaction.

00:17:47.620 --> 00:17:51.090
The teaching assistant may just as well be Duolingo,right?

00:17:51.090 --> 00:17:55.850
It doesn't have to be anthropomorphized.

00:17:55.850 --> 00:18:04.490
I think there are many things that require that human connection and that I don't see

00:18:04.490 --> 00:18:08.585
any progress that gives me

00:18:08.585 --> 00:18:14.400
confidence we can cross that bridge. And that bridge is meant to be,

00:18:14.400 --> 00:18:18.970
you know, we really build something that's a completely indistinguishable from human.

00:18:18.970 --> 00:18:22.970
I think it potentially brings disasters to us.

00:18:22.970 --> 00:18:25.900
Supposing it's doable, in the sense

00:18:25.900 --> 00:18:29.695
that what is the purpose and the meaning of our lives?

00:18:29.695 --> 00:18:36.995
And I think that will then cause a likely dystopia.

00:18:36.995 --> 00:18:43.810
So, I'm not saying it's impossible to build a machine with a soul,

00:18:43.810 --> 00:18:46.710
but I think we have to right now not believe

00:18:46.710 --> 00:18:50.085
it because I think having our soul and humanity

00:18:50.085 --> 00:18:53.345
is important and therefore people who choose

00:18:53.345 --> 00:18:57.040
to go after it they of course have the freedom to.

00:18:57.040 --> 00:18:58.725
But I think for the most of us,

00:18:58.725 --> 00:19:01.500
I think we need to hang onto our humanity,

00:19:01.500 --> 00:19:06.460
believe that we are unique and have our soul.

00:19:06.460 --> 00:19:10.870
And at the end of the day, I think if we are right,

00:19:10.870 --> 00:19:20.010
then we never fall into a downward vicious cycle of feeling of worthless.

00:19:20.010 --> 00:19:23.075
And even in the case we're wrong,

00:19:23.075 --> 00:19:29.135
I think believing we have a soul will make our lives more worth living,

00:19:29.135 --> 00:19:34.395
even if 100 years later Sebastian's student prooves that we're wrong,

00:19:34.395 --> 00:19:38.660
this 100 years will be better lived even if we're wrong.

00:19:38.660 --> 00:19:44.470
So there's a window left. This is really insightful, let me change gears a little bit.

00:19:44.470 --> 00:19:47.720
So, if you are a large-scale company,

00:19:47.720 --> 00:19:52.040
you're like an AT&amp;T and Verizon, a healthcare company like,

00:19:52.040 --> 00:19:57.240
Kartner Health, and you heard about the AI in the news,

00:19:57.240 --> 00:19:58.940
you read Kai-Fu's book,

00:19:58.940 --> 00:20:00.560
and you want to engage.

00:20:00.560 --> 00:20:03.765
What do you advise companies to do?

00:20:03.765 --> 00:20:08.330
Well, they have to recognize AI is coming and for

00:20:08.330 --> 00:20:12.120
the sake of their shareholders and their company's survival,

00:20:12.120 --> 00:20:16.460
they need to look at how to apply AI and make more money,

00:20:16.460 --> 00:20:22.565
as well as to displace humans in many of the jobs that Narrow AI can do.

00:20:22.565 --> 00:20:24.710
And also I think in doing so,

00:20:24.710 --> 00:20:28.835
they need to take on responsibility of the employees they would

00:20:28.835 --> 00:20:33.960
displace over the next five years that they are not just objects.

00:20:33.960 --> 00:20:38.125
Because if every company only optimizes shareholder value,

00:20:38.125 --> 00:20:40.980
there will be tens of millions of people on

00:20:40.980 --> 00:20:44.400
the streets because AI has taken their job and they can't

00:20:44.400 --> 00:20:47.410
find another job. Because whatever they can do, AI can do better,

00:20:47.410 --> 00:20:50.885
because AI will be displacing the most routine,

00:20:50.885 --> 00:20:53.130
the least skilled jobs,

00:20:53.130 --> 00:20:56.080
and it’ll be the hardest for people to find a new job.

00:20:56.080 --> 00:20:58.295
So, I like to think that

00:20:58.295 --> 00:21:04.449
a large company that can reap the rewards of using AI to displace its workers,

00:21:04.449 --> 00:21:10.390
ought to also have the responsibility to find training to help their employees find

00:21:10.390 --> 00:21:16.460
jobs that are not immediately displaced or hopefully never displaced by AI.

00:21:16.460 --> 00:21:18.640
We just would say AT&amp;T has been doing this very massively

00:21:18.640 --> 00:21:20.050
but let's dive into this.

00:21:20.050 --> 00:21:23.995
Do you think AI will be as big as like the Fourth Industrial Revolution?

00:21:23.995 --> 00:21:25.970
Certainly, absolutely.

00:21:25.970 --> 00:21:29.465
And what fraction of the labor force in

00:21:29.465 --> 00:21:31.235
developed nations like China and

00:21:31.235 --> 00:21:34.700
the United States do you think this is going to be replaced?

00:21:35.100 --> 00:21:39.365
I think if we project further,

00:21:39.365 --> 00:21:40.800
15 to 25 years,

00:21:40.800 --> 00:21:41.070
Yes.

00:21:41.070 --> 00:21:42.360
Probably about 50%.

00:21:42.360 --> 00:21:44.475
50% is what McKinsey says.

00:21:44.475 --> 00:21:47.935
What type of jobs do you think are most under fire right now?

00:21:47.935 --> 00:21:50.810
At this moment, it's not so bad.

00:21:50.810 --> 00:21:54.020
At this moment actually AI is creating a lot of jobs，right?

00:21:54.020 --> 00:21:57.670
People taking Udacity courses, getting into AI.

00:21:57.670 --> 00:22:00.925
and AI is creating a lot of jobs for people

00:22:00.925 --> 00:22:04.080
who have the potential skill set and interest,

00:22:04.080 --> 00:22:08.230
and the actual displacement are just being tweaked right now.

00:22:08.230 --> 00:22:13.085
But one of the reasons I wrote the book is that we're an investor in many companies,

00:22:13.085 --> 00:22:17.300
and we can basically count which ones are doing job displacements.

00:22:17.300 --> 00:22:18.800
We see, for example,

00:22:18.800 --> 00:22:23.690
we have invested in companies that are doing robotics for dishwashing,

00:22:23.690 --> 00:22:27.065
fruit picking, loan officers,

00:22:27.065 --> 00:22:32.215
customer service, traders,

00:22:32.215 --> 00:22:36.685
waiters and waitresses,

00:22:36.685 --> 00:22:39.210
fast food cooks, cashiers.

00:22:39.210 --> 00:22:43.800
So, I can think of eight jobs already with the technologies,

00:22:43.800 --> 00:22:46.445
that I see making rapid progress.

00:22:46.445 --> 00:22:49.685
You could add taxi drivers I assume，and truck drivers.

00:22:49.685 --> 00:22:52.870
That's right, we are investing in autonomous vehicle, yes.

00:22:52.870 --> 00:22:55.190
Those will be quite a bit later.

00:22:55.190 --> 00:23:00.205
But the eight professions I mentioned, I think, between 3 to 10 years,

00:23:00.205 --> 00:23:01.545
they'll be largely gone.

00:23:01.545 --> 00:23:05.885
What I find fascinating about those professions that you mentioned is two things,

00:23:05.885 --> 00:23:08.985
One, they're low skilled labor.

00:23:08.985 --> 00:23:09.780
Yes.

00:23:09.780 --> 00:23:13.310
And second, they often have a physical extension, where you

00:23:13.310 --> 00:23:16.825
can't pick fruits by just having a computer thinking about picking fruits,

00:23:16.825 --> 00:23:20.625
but they have to be a physical robot and that robot has to be food safe.

00:23:20.625 --> 00:23:24.430
Do you envision a new age of robotics?

00:23:24.470 --> 00:23:29.770
An age of robotics as in robots getting smarter and able to manipulate people?

00:23:29.770 --> 00:23:33.425
If you look at the business of robotics from my perspective, except for self-driving cars,

00:23:33.425 --> 00:23:39.075
I can't point at a single robotic technology that has really succeeded at scale.

00:23:39.075 --> 00:23:40.720
There's bits and pieces,

00:23:40.720 --> 00:23:43.415
there's Kiva and Amazon, advancements like in shelf delivery,

00:23:43.415 --> 00:23:46.340
and my analysis has always been like, in addition to being smart,

00:23:46.340 --> 00:23:48.665
you also have to pay for the hardware and maintain the hardware.

00:23:48.665 --> 00:23:49.025
Yeah.

00:23:49.025 --> 00:23:50.370
But you're saying, "No,

00:23:50.370 --> 00:23:51.430
not so fast, Sebastian."

00:23:51.430 --> 00:23:53.340
There's all kinds of drops there.

00:23:53.340 --> 00:23:55.970
It's not just the eyes, but also the physical actuation

00:23:55.970 --> 00:23:58.240
that's going to make enough progress. Is that what you're saying?

00:23:58.240 --> 00:24:00.030
Yes. I think I'm predicting that,

00:24:00.030 --> 00:24:04.190
but I also am more cautious in projecting that slowly and,

00:24:04.190 --> 00:24:08.730
we have to see there's equivalent progress in the mechanical aspects.

00:24:08.730 --> 00:24:12.280
But I believe we're seeing progress and also computer

00:24:12.280 --> 00:24:16.105
vision and deep learning and those technologies are getting integrated,

00:24:16.105 --> 00:24:19.185
and we're seeing functional prototypes,

00:24:19.185 --> 00:24:23.000
Actually probably, the first one will be the Kiva 2.0,right?

00:24:23.000 --> 00:24:27.960
The version that's completely human free in the warehouse putting things in boxes.

00:24:27.960 --> 00:24:30.230
That I think we certainly see. 

00:24:30.230 --> 00:24:34.105
If you're a young entrepreneur, be it in the United States,

00:24:34.105 --> 00:24:36.195
in China, or anywhere in the world,

00:24:36.195 --> 00:24:39.955
what should you work on if you want to do something significant in AI?

00:24:39.955 --> 00:24:41.470
Well.

00:24:41.470 --> 00:24:45.160
What are your best ideas? No one's listening.

00:24:46.290 --> 00:24:50.560
Well, if you're into making money,

00:24:50.560 --> 00:24:52.480
the lowest hanging fruit has to be in

00:24:52.480 --> 00:24:56.900
the financial areas because those are fabricated human

00:24:56.900 --> 00:25:01.665
games of simulation that were never meant for people to do.

00:25:01.665 --> 00:25:03.340
So, all the banks,

00:25:03.340 --> 00:25:06.850
insurance companies, payments, investments,

00:25:07.720 --> 00:25:11.160
Because you end up with a cash printing machine

00:25:11.160 --> 00:25:13.255
once your algorithm works.

00:25:13.255 --> 00:25:15.450
It doesn't have all the robotics,

00:25:15.450 --> 00:25:19.380
the warehouse, and manufacturing delivery, all those things.

00:25:19.380 --> 00:25:20.925
So, I think if you're into making money,

00:25:20.925 --> 00:25:22.855
there's still space in financial indusrty.

00:25:22.855 --> 00:25:23.220
Yeah.

00:25:23.220 --> 00:25:26.745
If you're into making a huge difference,

00:25:26.745 --> 00:25:33.290
then I think getting into robotics autonomous vehicle,

00:25:34.570 --> 00:25:41.430
the breakthroughs there will probably come back to help many things become possible.

00:25:41.430 --> 00:25:41.750
Okay.

00:25:41.750 --> 00:25:45.460
Autonomous vehicle I think is a particularly challenging one.

00:25:45.460 --> 00:25:48.635
I'm not as optimistic as Waymo in saying,

00:25:48.635 --> 00:25:50.800
okay we're going to deploy this year and next year,

00:25:50.800 --> 00:25:52.315
this and that will happen.

00:25:52.315 --> 00:25:57.525
But I think with so many smart people and money going into it,

00:25:57.525 --> 00:26:00.015
with all the automotive companies and

00:26:00.015 --> 00:26:03.600
electrical vehicle companies having given up to autonomous vehicles,

00:26:03.600 --> 00:26:09.110
I think they will eventually make progress and change the way we transport ourselves.

00:26:09.110 --> 00:26:14.890
And when that happens, a lot of those technologies will then come back to robotics and

00:26:14.890 --> 00:26:20.590
maybe solve the robotic problems that you think maybe are hard to solve at the moment.

00:26:20.590 --> 00:26:25.455
Anyway if anyone is listening, we have an AI for Finance Nanodegree that just launched. 

00:26:25.455 --> 00:26:29.345
But more importantly, blockchain.

00:26:29.345 --> 00:26:31.850
So I've heard opinions about blockchain,from like

00:26:31.850 --> 00:26:34.465
"this is just a nice piece of the puzzle", or perhaps,

00:26:34.465 --> 00:26:39.475
"this is great for drug money", all the way to “it’s going to change everything”, where do you stand? 

00:26:39.475 --> 00:26:42.315
I think we've got to solve the POW problem.

00:26:42.315 --> 00:26:45.920
That is the large amount of 

00:26:45.920 --> 00:26:50.095
computation needed to maintain the inalterability of contracts.

00:26:50.095 --> 00:26:53.050
So, when someone solves that problem, either with

00:26:53.050 --> 00:26:57.045
some form of computation that doesn't kill us with energy,

00:26:57.045 --> 00:27:00.205
or with a non-computing intensive approach,

00:27:00.205 --> 00:27:02.165
then I think it can take off,

00:27:02.165 --> 00:27:03.935
I think there will be many applications.

00:27:03.935 --> 00:27:05.490
Would you invest in Blockchain?

00:27:05.490 --> 00:27:06.275
Yes.

00:27:06.275 --> 00:27:07.930
You have invested in blockchain?

00:27:07.930 --> 00:27:15.410
Only in a great company called BitMe, it is somewhat of a blockchain company. 

00:27:15.410 --> 00:27:21.615
It's the largest Bitcoin mining hardware company.

00:27:21.615 --> 00:27:25.825
When we look at this layers of things you can invest in,such as their ICOs,

00:27:25.825 --> 00:27:28.770
and crypto, we are very worried about fraud there.

00:27:28.770 --> 00:27:30.000
We look at Blockchain,

00:27:30.000 --> 00:27:32.940
we don't see the killer app or worry about POW,

00:27:32.940 --> 00:27:34.720
but from an investment standpoint,

00:27:34.720 --> 00:27:38.145
the one place you can't lose is whatever the apps are,

00:27:38.145 --> 00:27:40.205
you gotta keep the mining going.

00:27:40.205 --> 00:27:42.770
You got to have the CPU.

00:27:42.770 --> 00:27:43.530
You’re gonna do the shovel,

00:27:43.530 --> 00:27:46.130
like in El Dorado, California, everyone is mining for gold, okay,

00:27:46.130 --> 00:27:47.340
and occasionally people succeed. But you have

00:27:47.340 --> 00:27:49.465
a fixed business because you give everyone the same shovel.

00:27:49.465 --> 00:27:52.975
That's right. This is our single, only investment in blockchain.

00:27:52.975 --> 00:27:53.520
Nice.

00:27:53.520 --> 00:27:56.580
And I think we've been pretty smart in doing that.

00:27:56.580 --> 00:27:59.910
If you were a bank and you'd just been here with Dodd Frank,

00:27:59.910 --> 00:28:04.465
with Basel III and all these regulation that came up with the 2008 crisis,

00:28:04.465 --> 00:28:07.350
and you're racking up literally hundreds of millions of

00:28:07.350 --> 00:28:10.410
dollars every year in regulatory and compliance costs,

00:28:10.410 --> 00:28:12.830
should you worry about blockchain?

00:28:12.830 --> 00:28:15.600
If I were a bank, I'd just shut myself

00:28:15.600 --> 00:28:19.370
down because I'm not doing any good for the society.

00:28:20.040 --> 00:28:22.870
I’m just kidding. I think it should all be electronic.

00:28:22.870 --> 00:28:31.075
I'm partly kidding but partly serious because we're in China.

00:28:31.075 --> 00:28:33.825
The credit card companies are already gone.

00:28:33.825 --> 00:28:39.365
You know the mobile payment has already taken over and that 2% 

00:28:39.365 --> 00:28:46.415
of tax that, you know, that's been placed on the western world is gone.

00:28:46.415 --> 00:28:48.210
China just freed itself from

00:28:48.210 --> 00:28:51.765
a 2% tax from not having to use credit cards by using

00:28:51.765 --> 00:28:57.445
purely, this is not blockchain nor is it crypto,

00:28:57.445 --> 00:29:04.195
it's just real solid normal Renminbi currency that is being exchanged on WeChat.

00:29:04.195 --> 00:29:08.815
I mean, in China you see people paying each other always by phone. 

00:29:08.815 --> 00:29:11.790
Nobody carries cash. People don't have credit cards, 

00:29:11.790 --> 00:29:15.340
even beggars on the street are there holding up a sign that says,

00:29:15.340 --> 00:29:17.255
"I'm hungry, scan me."

00:29:17.255 --> 00:29:21.380
you know, and I think in that we see the future of money,

00:29:21.380 --> 00:29:24.930
and whether there's blockchain or crypto, maybe...

00:29:24.930 --> 00:29:29.510
So, what can, say, Jamie Dimon from JP Morgan Chase or Urs Rohner from Credit Suisse.

00:29:29.510 --> 00:29:33.030
What can we actually learn from China?

00:29:33.030 --> 00:29:38.210
Well, I think the future of finance will go completely virtual.

00:29:38.210 --> 00:29:46.800
I think all the people in banks are doing jobs like routine, you know, lab rats in a wheel.

00:29:46.800 --> 00:29:48.560
They should all be done by AI.

00:29:48.560 --> 00:29:54.180
So, I realize there are a lot of regulations that don't allow banks to just go away.

00:29:54.180 --> 00:30:00.115
But, I think banks will probably be too slow to reinvent themselves,

00:30:00.115 --> 00:30:02.625
with blockchain or without.

00:30:02.625 --> 00:30:06.270
And we will end up with really just virtual currency,

00:30:06.270 --> 00:30:10.775
virtual banks, and pay less and less.

00:30:10.775 --> 00:30:13.665
I mean, if credit cards are taking a 2% tax,

00:30:13.665 --> 00:30:16.980
banks surely are taking another 2% tax on this society,

00:30:16.980 --> 00:30:19.630
Insurance companies surely are taking another 2%.

00:30:19.630 --> 00:30:23.590
I just look forward to the day that this narrow A.I. will displace

00:30:23.590 --> 00:30:29.080
those things and give that 6% back to us that we're entitled to.

00:30:29.820 --> 00:30:35.795
Some insurance companies live by leveraging risk.

00:30:35.795 --> 00:30:41.710
And if AI were to be really smart, it would say things like,

00:30:41.710 --> 00:30:45.305
okay your number one factor of risk is your pre-existing conditions,

00:30:45.305 --> 00:30:47.940
and number two factor of risk is your lifestyle as

00:30:47.940 --> 00:30:50.970
measured by your credit score or whatever you use for lifestyle measurement.

00:30:50.970 --> 00:30:52.460
It doesn't even take AI to get there.

00:30:52.460 --> 00:30:56.730
Don't you worry that we’ll get into a world where we just don't have

00:30:56.730 --> 00:31:02.035
insurance anymore and there will just be winners and losers?

00:31:02.035 --> 00:31:04.640
No, I think insurance should be

00:31:04.640 --> 00:31:08.280
personalized based on the risks you have and the need you have.

00:31:08.280 --> 00:31:14.635
So I think everyone should be able to design his or her own insurance needs of saying,

00:31:14.635 --> 00:31:17.480
well I want, you know, you can in natural language

00:31:17.480 --> 00:31:21.830
describe that if I die I want my family to have this much money.

00:31:21.830 --> 00:31:24.155
So how much do I need to pay you?

00:31:24.155 --> 00:31:28.965
And click, and then a personalized policy will come and you pay for that.

00:31:28.965 --> 00:31:31.470
And if you have a pre-existing condition, you're gonna pay twice as much?

00:31:31.470 --> 00:31:33.800
If you have a pre-existing condition, I mean...

00:31:33.800 --> 00:31:36.450
You went through pre-existing conditions some time in your life.

00:31:36.450 --> 00:31:45.115
I do. I do. (What do you think?) If the governments want to enforce that, insurance companies must do something,I mean, 

00:31:45.115 --> 00:31:47.860
whether it's virtual or physical, they can enforce that.

00:31:47.860 --> 00:31:53.075
I mean yes, my medical insurance did not abandon me

00:31:53.075 --> 00:31:55.680
once I had cancer. And I can't

00:31:55.680 --> 00:31:58.825
get any other insurance company so I better hang onto this one,

00:31:58.825 --> 00:32:01.840
But the same can apply to a virtual insurance, no?

00:32:01.840 --> 00:32:04.080
Okay. I'm going to change course a little

00:32:04.080 --> 00:32:06.560
bit and talk about the young people because many of

00:32:06.560 --> 00:32:08.730
our Udacity fellows are people in the world aspiring to

00:32:08.730 --> 00:32:12.190
be even 10% of who you are, Kai-Fu Lee, 

00:32:12.190 --> 00:32:15.390
You're a role model to many of us, myself included.

00:32:15.390 --> 00:32:18.175
But if you face your younger self,

00:32:18.175 --> 00:32:19.900
say at Tsinghua University,

00:32:19.900 --> 00:32:21.490
or at Carnegie Mellon University,

00:32:21.490 --> 00:32:25.005
what advice would you give to your younger self?

00:32:25.005 --> 00:32:29.065
Well especially considering that the age of AI is coming,

00:32:29.065 --> 00:32:31.925
I think it is all the more important that

00:32:31.925 --> 00:32:37.110
the young people follow their hearts and do things they're passionate about,

00:32:37.110 --> 00:32:43.300
because what the world tells you is increasingly getting outdated.

00:32:43.300 --> 00:32:48.580
Because I think a lot of parents and a lot of jobs in the society would

00:32:48.580 --> 00:32:53.925
have us think that some jobs are very desirable because they're stable and

00:32:53.925 --> 00:32:57.640
safe, or some jobs are desirable because they're high paying.

00:32:57.640 --> 00:32:59.610
But AI would disrupt a lot of that,

00:32:59.610 --> 00:33:01.535
Take radiologists for example,

00:33:01.535 --> 00:33:05.895
if you're going to a medical school, don't be a radiologist.

00:33:05.895 --> 00:33:07.865
Don't be a dermatologist either.

00:33:07.865 --> 00:33:10.725
Yeah exactly, don't be a dermatologist.

00:33:10.725 --> 00:33:13.440
Yeah, yeah. You did some work on that, right?

00:33:13.440 --> 00:33:16.105
You and Andrew are both doing great work on that.

00:33:16.105 --> 00:33:19.000
But do be a researcher.

00:33:19.000 --> 00:33:22.955
So one is do what you're good at and what you love,

00:33:22.955 --> 00:33:30.400
and second is, be aware of what AI is going to replace and don't go into those areas.

00:33:30.400 --> 00:33:33.005
Go into the areas where AI will enhance.

00:33:33.005 --> 00:33:38.410
Think about AI as either a job destroyer or a job enhancer.

00:33:38.410 --> 00:33:41.095
Pick the jobs where AI can be a job enhancer.

00:33:41.095 --> 00:33:42.520
Be a medical researcher,

00:33:42.520 --> 00:33:46.135
that is a great, great area,

00:33:46.135 --> 00:33:50.370
But some jobs will go away.

00:33:50.370 --> 00:33:53.300
Also, I think it's important to improve

00:33:53.300 --> 00:34:00.275
the human interaction skills because that's one thing AI cannot do,

00:34:00.275 --> 00:34:03.680
It is that true human touch,

00:34:03.830 --> 00:34:07.340
the compassion, the empathy,

00:34:07.340 --> 00:34:10.845
the altruism, the connection and the trust with people,

00:34:10.845 --> 00:34:13.040
and those soft skills sometimes are being

00:34:13.040 --> 00:34:15.895
forgotten by young people who spend all their time on the phone.

00:34:15.895 --> 00:34:18.340
So do spend time making friends,

00:34:18.340 --> 00:34:20.715
going to teamwork activities.

00:34:20.715 --> 00:34:24.805
Even before the age of AI, we know that incorporations,

00:34:24.805 --> 00:34:28.590
people with strong EQ actually,

00:34:28.590 --> 00:34:30.615
is more important than the IQ.

00:34:30.615 --> 00:34:33.400
Of course you want both, but everything else being equal,

00:34:33.400 --> 00:34:35.610
you have a basic level of both,

00:34:35.610 --> 00:34:42.575
people with high EQ are generally the ones who really progress well in their careers.

00:34:42.575 --> 00:34:44.695
Interesting. I’m gonna to ask you one last question.

00:34:44.695 --> 00:34:47.100
and I believe nobody answers it but,

00:34:47.100 --> 00:34:49.270
I’m gonna ask you anyhow because you are one of the leaders.

00:34:49.270 --> 00:34:53.200
Do you think that there will ever be an age where we can upload ourselves into a computer?

00:34:53.200 --> 00:35:00.815
I'm going to say no.

00:35:00.815 --> 00:35:01.675
Never.

00:35:01.675 --> 00:35:03.370
I would not like that to happen.

00:35:03.370 --> 00:35:06.270
Historians in the world take a moment, notice here:

00:35:06.270 --> 00:35:08.030
Kai-Fu Lee, world expert says,

00:35:08.030 --> 00:35:10.080
we'll never be able to put ourselves in a box.

00:35:10.080 --> 00:35:13.610
Well, I am not saying we can never do it,

00:35:13.610 --> 00:35:16.770
I'm saying I would very much like the answer to be no.

00:35:16.770 --> 00:35:22.755
The reason is I think our life has meaning because of our mortality,

00:35:22.755 --> 00:35:28.700
and this pursuit for immortality would disrupt the reasons of

00:35:28.700 --> 00:35:31.700
how we live our lives and I think there's

00:35:31.700 --> 00:35:35.225
a lot of wisdom (we've) accumulated over thousands of years.

00:35:35.225 --> 00:35:39.415
So, if we offer immortality to people, I worry

00:35:39.415 --> 00:35:44.035
that our humanity will change in a way that may be very negative.

00:35:44.035 --> 00:35:47.355
I'm not projecting it to be possible or not possible,

00:35:47.355 --> 00:35:49.870
but I would very much like for it not to be.

00:35:49.870 --> 00:35:56.410
Would you invest in Chinese version of Calico that tries to double human lifespan?

00:35:56.410 --> 00:36:00.640
To increase longevity or to create immortality?

00:36:00.640 --> 00:36:02.225
To make you live twice as long.

00:36:02.225 --> 00:36:03.925
Increase longevity, okay.

00:36:03.925 --> 00:36:05.825
Yes, I absolutely would.

00:36:05.825 --> 00:36:06.970
Up to what point? I mean beyond this point,

00:36:06.970 --> 00:36:08.500
Well, our bodies.

00:36:08.500 --> 00:36:09.280
to 200? 250? 300?

00:36:09.280 --> 00:36:12.985
Well, our bodies are only durable for 120 years, right? 

00:36:12.985 --> 00:36:13.915
How you know?

00:36:13.915 --> 00:36:16.080
Well, maybe your children's,

00:36:16.080 --> 00:36:17.840
children's bodies can live longer.

00:36:17.840 --> 00:36:23.405
I think our bodies have already been ruined by toxins and things like that.

00:36:23.405 --> 00:36:25.200
I was told 120,

00:36:25.200 --> 00:36:31.690
but I think having longevity doubling I would love to invest in that.

00:36:31.690 --> 00:36:35.635
Okay, now since I didn't, I’m now on my final question:

00:36:35.635 --> 00:36:40.950
Among all the great things being discussed today, from doubling human lifespan, to cold fusion,

00:36:40.950 --> 00:36:43.885
to flying cars, to curing all diseases,

00:36:43.885 --> 00:36:47.920
what do you think is going to actually happen in the next 30 or 40 years?

00:36:48.730 --> 00:36:51.580
Well, I think a lot will happen.

00:36:51.580 --> 00:36:54.455
I think flying cars will clearly happen,

00:36:54.455 --> 00:36:57.630
autonomous vehicles will clearly happen,

00:36:57.630 --> 00:37:01.080
narrow AI taking half the jobs that will clearly happen.

00:37:01.080 --> 00:37:01.765
That's big.

00:37:01.765 --> 00:37:05.609
Yeah a lot of the current traditional institutions 

00:37:05.609 --> 00:37:08.870
will go out of business. That's for sure.

00:37:08.870 --> 00:37:14.165
I think there will be progress in things like immunology,

00:37:14.165 --> 00:37:16.465
lengthening of our lives' longevity.

00:37:16.465 --> 00:37:22.590
I think CRISPR hopefully will find a good direction that can help us be better.

00:37:22.590 --> 00:37:27.125
And also I think AI will

00:37:27.125 --> 00:37:28.425
definitely cause a couple of

00:37:28.425 --> 00:37:32.720
small disasters and hopefully we'll be in a position to control them.

00:37:32.720 --> 00:37:36.780
Kai-Fu Lee, we're out of time so I can't ask 'what the disasters are' question.

00:37:36.780 --> 00:37:38.530
But it's so incredible to have you here.

00:37:38.530 --> 00:37:38.870
Thank you.

00:37:38.870 --> 00:37:39.890
You are one of my role models.

00:37:39.890 --> 00:37:42.110
I've known you for more than 20 years, and it's so

00:37:42.110 --> 00:37:44.535
great to have you on stage today here at Udacity.

00:37:44.535 --> 00:37:45.105
Thank you.

00:37:45.105 --> 00:37:46.480
Thank you.

00:37:47.030 --> 00:37:49.050
And buy this book.

00:37:49.050 --> 00:37:50.830
Thanks!

