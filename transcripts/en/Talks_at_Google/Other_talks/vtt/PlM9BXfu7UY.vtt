WEBVTT
Kind: captions
Language: en

00:00:01.960 --> 00:00:03.970
RUSSELL HARMON: My
name's Russ Harmon.

00:00:03.970 --> 00:00:07.090
I'm a full-time software
engineer here at Google

00:00:07.090 --> 00:00:10.050
like, I think, most
of you as well.

00:00:10.050 --> 00:00:11.520
And thank you for coming.

00:00:11.520 --> 00:00:13.970
This is Morten Kromberg.

00:00:13.970 --> 00:00:16.920
He's the CTO of Dyalog,
which is an implementation

00:00:16.920 --> 00:00:19.600
of a programming language.

00:00:19.600 --> 00:00:22.627
And let's get started.

00:00:22.627 --> 00:00:23.460
MORTEN KROMBERG: OK.

00:00:23.460 --> 00:00:25.140
Well, thank you for coming.

00:00:25.140 --> 00:00:27.810
And thank you, Russ, for
believing that they would come,

00:00:27.810 --> 00:00:30.930
and inviting me to come and give
this talk about something which

00:00:30.930 --> 00:00:33.650
is definitely not mainstream.

00:00:33.650 --> 00:00:35.900
In fact, Dyalog is
an implementation

00:00:35.900 --> 00:00:38.710
of an ancient programming
language called APL, which

00:00:38.710 --> 00:00:42.620
I think is now 52 years old.

00:00:42.620 --> 00:00:47.692
And I have been using it
for about 36 of those years.

00:00:47.692 --> 00:00:50.729
I remember, I did a
similar talk previously

00:00:50.729 --> 00:00:52.270
where a guy came up
to me afterwards.

00:00:52.270 --> 00:00:54.395
And he said, you've been
using the same programming

00:00:54.395 --> 00:00:58.040
language for 35 years,
and you're still happy?

00:00:58.040 --> 00:01:01.520
As if that was completely
impossible and unthinkable.

00:01:04.360 --> 00:01:07.490
So, I started off as
a young man should.

00:01:07.490 --> 00:01:11.320
I spent the money I'd
made selling newspapers,

00:01:11.320 --> 00:01:16.840
or delivering newspapers, and
buying this do-it-yourself kit.

00:01:16.840 --> 00:01:20.890
I soldered all those integrated
circuits onto a board.

00:01:20.890 --> 00:01:24.580
And I got this thing to work,
programed it in Z80 Bytecode.

00:01:24.580 --> 00:01:26.140
I'd never soldered
anything before,

00:01:26.140 --> 00:01:27.790
so it would only
run for about three

00:01:27.790 --> 00:01:31.061
or four minutes before crashing.

00:01:31.061 --> 00:01:32.560
Then things got a
little bit better.

00:01:32.560 --> 00:01:37.750
I found Commodore started making
these really nice machines.

00:01:37.750 --> 00:01:39.482
So I learned BASIC.

00:01:39.482 --> 00:01:41.190
And so things we're
going fairly normally

00:01:41.190 --> 00:01:44.407
until a friend of
the family one day

00:01:44.407 --> 00:01:46.490
brought me down to the
place where he was working,

00:01:46.490 --> 00:01:48.495
and he said, we're using
this thing called APL.

00:01:48.495 --> 00:01:54.000
And he typed this expression
here, 1 2 3 + 4 5 6,

00:01:54.000 --> 00:01:57.680
and the computer spat
out three numbers.

00:01:57.680 --> 00:02:03.840
And I thought well,
that's the end of that.

00:02:03.840 --> 00:02:06.670
And so I've been using
this technology now

00:02:06.670 --> 00:02:08.280
for 35 years-ish.

00:02:08.280 --> 00:02:12.640
I started out in 1979 a time--
well, Timesharing in those days

00:02:12.640 --> 00:02:14.430
was when you were
sharing a computer, not

00:02:14.430 --> 00:02:16.210
a condo in Mexico.

00:02:18.730 --> 00:02:21.230
The customers were
paying about $1

00:02:21.230 --> 00:02:24.390
per CPU second on
this machine, which

00:02:24.390 --> 00:02:27.140
was around the other side
of the world in Toronto.

00:02:27.140 --> 00:02:29.920
I was using it
from Oslo, Norway.

00:02:29.920 --> 00:02:32.740
And $1 per 1,000
characters of output

00:02:32.740 --> 00:02:36.389
that they received
over the network.

00:02:36.389 --> 00:02:38.430
It's not as bad as it
sounds, because it actually

00:02:38.430 --> 00:02:42.580
took several seconds
for that to print out.

00:02:42.580 --> 00:02:46.080
So you were still getting
some value for money.

00:02:46.080 --> 00:02:50.831
And now if we get to the recent
end of my experience with APL,

00:02:50.831 --> 00:02:52.330
the last couple of
things we've done

00:02:52.330 --> 00:02:56.130
is we've put out a
Raspberry Pi version, which

00:02:56.130 --> 00:02:59.590
I think is significantly more
powerful computer than the 360.

00:02:59.590 --> 00:03:01.580
Maybe doesn't have
the same I/O capacity,

00:03:01.580 --> 00:03:06.400
but certainly things have
changed a bit in this time.

00:03:06.400 --> 00:03:08.820
And last week, we also
released a version for the Mac.

00:03:08.820 --> 00:03:10.700
I see there are a few Macs here.

00:03:10.700 --> 00:03:14.890
So you can now get this, and
play with it on your Macs.

00:03:14.890 --> 00:03:17.940
So for me, it's
been a love affair

00:03:17.940 --> 00:03:20.240
lasting a very long time.

00:03:20.240 --> 00:03:24.690
But of course not
everybody was keen

00:03:24.690 --> 00:03:27.020
on APL when they saw
it for the first time.

00:03:27.020 --> 00:03:27.860
This is perhaps one.

00:03:27.860 --> 00:03:31.500
If you search on the
internet, there's

00:03:31.500 --> 00:03:34.350
still a lot of very old
stuff you'll find about APL.

00:03:34.350 --> 00:03:39.090
This is perhaps one of the most
famous statements about APL.

00:03:39.090 --> 00:03:41.320
If you actually
read up on it, you

00:03:41.320 --> 00:03:46.360
will find that Dijkstra wasn't
that uniformly negative on APL.

00:03:46.360 --> 00:03:48.960
And anyway, I think
there was a proposal that

00:03:48.960 --> 00:03:52.220
was arrogance should be measured
on a scale called the Dijkstra

00:03:52.220 --> 00:03:53.760
scale.

00:03:53.760 --> 00:03:55.720
But anyway, as you
grow up, and you

00:03:55.720 --> 00:03:57.670
learn to receive
feedback from people,

00:03:57.670 --> 00:04:00.490
you learn how to
read what people say.

00:04:00.490 --> 00:04:05.540
And there's part of
this that I really like.

00:04:05.540 --> 00:04:08.510
You may be surprised, I
like this last line here.

00:04:08.510 --> 00:04:11.110
I think being a coding
bum is actually something

00:04:11.110 --> 00:04:14.330
that's a positive statement.

00:04:14.330 --> 00:04:18.510
You want to conserve your
energy when you're writing code.

00:04:18.510 --> 00:04:21.519
Now the history of
APL is very long,

00:04:21.519 --> 00:04:23.530
and the language is
quite different from

00:04:23.530 --> 00:04:24.860
other programming languages.

00:04:24.860 --> 00:04:28.890
And I think it's important to
spend just a little bit of time

00:04:28.890 --> 00:04:32.430
looking at the man who invented
it, who is passed away.

00:04:32.430 --> 00:04:35.370
I guess one of the few
programming language creators,

00:04:35.370 --> 00:04:37.250
there's still only
a handful, who

00:04:37.250 --> 00:04:40.100
lived-- he lived until he
was 84 and passed away.

00:04:40.100 --> 00:04:44.300
It's not because he
passed away young.

00:04:44.300 --> 00:04:46.440
So he was originally
on a farm, and he

00:04:46.440 --> 00:04:47.760
went to a one-room school.

00:04:47.760 --> 00:04:52.286
He did nine years of schooling,
and went back to the farm.

00:04:52.286 --> 00:04:53.910
And that could have
been the end of it,

00:04:53.910 --> 00:04:56.770
if it hadn't been for
being drafted by the army.

00:04:56.770 --> 00:04:59.910
So, whatever you think of the
army, at least in this case,

00:04:59.910 --> 00:05:03.340
turned out to be a very good
thing that they drafted Ken.

00:05:03.340 --> 00:05:06.742
And he almost finished
high school in the service.

00:05:06.742 --> 00:05:08.200
And he found that
he really enjoyed

00:05:08.200 --> 00:05:11.135
mathematics and teaching
it to his fellow soldiers.

00:05:11.135 --> 00:05:13.510
And they told him that if he
didn't go back to university

00:05:13.510 --> 00:05:15.801
after the war, they were
going to come and beat him up.

00:05:15.801 --> 00:05:17.440
So he had to do that.

00:05:17.440 --> 00:05:21.880
So he embarked on an
academic career, which became

00:05:21.880 --> 00:05:23.500
quite an interesting one.

00:05:23.500 --> 00:05:27.980
He did doctoral work at Harvard
with Aiken and Leontief,

00:05:27.980 --> 00:05:30.480
who later won the Nobel
Prize in economics

00:05:30.480 --> 00:05:32.970
for his input-output matrices.

00:05:32.970 --> 00:05:35.840
Which, I think, also recognizes
a very early precursor

00:05:35.840 --> 00:05:37.820
of some of the
algorithms in Google

00:05:37.820 --> 00:05:41.760
page ranking, from what I
was able to read yesterday

00:05:41.760 --> 00:05:44.230
when I was researching that.

00:05:44.230 --> 00:05:45.380
But he taught at Harvard.

00:05:45.380 --> 00:05:48.180
He was teaching mathematics
after doing this work.

00:05:48.180 --> 00:05:49.910
And he was really
getting frustrated

00:05:49.910 --> 00:05:52.260
with what he felt
was the inadequacy

00:05:52.260 --> 00:05:54.280
of mathematical notation.

00:05:54.280 --> 00:05:55.820
And he developed
his own notation,

00:05:55.820 --> 00:05:59.720
wrote a book about it in 1962.

00:05:59.720 --> 00:06:02.860
But the people at Harvard
said, well, all you've done

00:06:02.860 --> 00:06:06.420
is publish that little book, and
they didn't grant him tenure.

00:06:06.420 --> 00:06:09.030
And he moved off to IBM
to work with people there,

00:06:09.030 --> 00:06:13.300
who then, after APL had been
used to teach mathematics

00:06:13.300 --> 00:06:16.900
and matrix algebra
for some years,

00:06:16.900 --> 00:06:20.080
implemented the first
interpreter for the language.

00:06:20.080 --> 00:06:22.740
So the language was initially
designed as a communication

00:06:22.740 --> 00:06:26.920
tool for humans, and used
in that way on blackboards

00:06:26.920 --> 00:06:31.910
for some years before the
first interpreter was created.

00:06:31.910 --> 00:06:33.870
And of course for
that little book,

00:06:33.870 --> 00:06:36.450
Iverson did later
receive the Turing award

00:06:36.450 --> 00:06:39.670
for his pioneering
work in education,

00:06:39.670 --> 00:06:43.780
in Timesharing, and so on.

00:06:43.780 --> 00:06:46.642
So, here's what was
really winding Ken up,

00:06:46.642 --> 00:06:48.850
especially when he was doing
the work on mathematics.

00:06:51.580 --> 00:06:56.400
He found that mathematics
had all these strange forms.

00:06:56.400 --> 00:06:59.650
There was a very wide
variety of syntactical forms.

00:06:59.650 --> 00:07:01.770
There were very strange
and inconsistent

00:07:01.770 --> 00:07:08.880
precedence rules-- what order
you do the suffixes, and so on.

00:07:08.880 --> 00:07:12.510
And particularly when he
was working with matrices,

00:07:12.510 --> 00:07:14.939
it was pretty frustrating.

00:07:14.939 --> 00:07:16.480
And so he came up
with this language,

00:07:16.480 --> 00:07:19.790
which has a very small
amount of different syntaxes.

00:07:19.790 --> 00:07:22.220
But rather than look
at the PowerPoint,

00:07:22.220 --> 00:07:23.889
I'm going to do
some live coding.

00:07:23.889 --> 00:07:25.930
Most of the interesting
part of this presentation

00:07:25.930 --> 00:07:27.440
will hopefully be
the live coding.

00:07:27.440 --> 00:07:28.955
So those of you who
can't read this,

00:07:28.955 --> 00:07:30.370
it may be worth
coming over here.

00:07:30.370 --> 00:07:32.078
Because I think at
least half of the time

00:07:32.078 --> 00:07:34.790
is going to be me
typing expressions.

00:07:34.790 --> 00:07:35.810
Is that wrapping?

00:07:38.400 --> 00:07:40.910
We had a bit of fun with
the resolution here.

00:07:40.910 --> 00:07:43.760
Am I on?

00:07:43.760 --> 00:07:44.910
I'm in now.

00:07:44.910 --> 00:07:46.020
OK.

00:07:46.020 --> 00:07:49.350
So, the first syntax you need is
to be able to create a matrix,

00:07:49.350 --> 00:07:51.550
arrays are fundamental
to the language.

00:07:51.550 --> 00:07:53.950
And just by juxtaposing
things, you create lists.

00:07:53.950 --> 00:07:58.900
So here we have a bunch
of numbers, three scalars,

00:07:58.900 --> 00:08:00.480
and a vector.

00:08:00.480 --> 00:08:01.999
And you can see the structure.

00:08:01.999 --> 00:08:04.040
The system, if you don't
assign it to a variable,

00:08:04.040 --> 00:08:06.760
it just outputs the result
of whatever expression

00:08:06.760 --> 00:08:08.660
you've typed.

00:08:08.660 --> 00:08:11.360
You'll see me use this
lamp symbol quite a lot.

00:08:11.360 --> 00:08:16.380
This is the lamp,
it illuminates code.

00:08:16.380 --> 00:08:19.260
There's a lot of that kind of
fun in the choice of symbols

00:08:19.260 --> 00:08:20.380
in APL.

00:08:20.380 --> 00:08:25.030
So, this is an array of two,
we call them character vectors;

00:08:25.030 --> 00:08:27.440
you might call them strings.

00:08:27.440 --> 00:08:28.300
Arrays can be named.

00:08:30.880 --> 00:08:34.710
And of course you can assign
another name to something.

00:08:34.710 --> 00:08:36.340
And then we'll make
a change to b here,

00:08:36.340 --> 00:08:38.980
and this is just to demonstrate
that all the arrays in APL

00:08:38.980 --> 00:08:40.520
are value types.

00:08:40.520 --> 00:08:42.289
These are not reference arrays.

00:08:42.289 --> 00:08:45.385
But they are immutable, and
they are all value types.

00:08:48.100 --> 00:08:52.307
So here is an array
called FormProps.

00:08:52.307 --> 00:08:54.390
And this is really just
to show you, very briefly,

00:08:54.390 --> 00:08:56.260
that we can do other
things in calculations.

00:08:56.260 --> 00:08:59.050
We can create a form
with those properties.

00:09:01.570 --> 00:09:04.080
And then you have
an object model

00:09:04.080 --> 00:09:06.830
where the properties
can be arrays,

00:09:06.830 --> 00:09:09.590
and I can set the caption.

00:09:09.590 --> 00:09:12.390
And I'm creating
a list box inside,

00:09:12.390 --> 00:09:15.660
which I will set to
have a background color.

00:09:15.660 --> 00:09:17.710
And I'm just going to do
that so I can maintain

00:09:17.710 --> 00:09:21.520
a list of the different
syntaxes as we go along here,

00:09:21.520 --> 00:09:23.810
since we're not looking
at the PowerPoint.

00:09:23.810 --> 00:09:25.600
So after the array,
the next form

00:09:25.600 --> 00:09:28.790
we have is a function
followed by an array.

00:09:28.790 --> 00:09:31.840
So, for example, iota
6, the index generator

00:09:31.840 --> 00:09:35.480
generates an array of
the numbers up to 6.

00:09:35.480 --> 00:09:38.280
This little thing pointing
down with a little step

00:09:38.280 --> 00:09:39.295
at the bottom is floor.

00:09:44.790 --> 00:09:46.930
The font size is just
a little bit too big.

00:09:46.930 --> 00:09:49.500
Is it OK if I bring the font
size down just a little bit?

00:09:56.380 --> 00:10:00.410
Yes, we have lost a bit
of real estate here.

00:10:00.410 --> 00:10:03.460
OK, so itemwise map
is implicit for all

00:10:03.460 --> 00:10:05.200
of what we call scalar
functions, which

00:10:05.200 --> 00:10:07.322
is about half-- at least
half of the functions.

00:10:07.322 --> 00:10:09.530
So if you have an array, it
just applies the function

00:10:09.530 --> 00:10:11.480
to every element of
the array without you

00:10:11.480 --> 00:10:16.150
having to say, map, or for
each, or anything like that.

00:10:16.150 --> 00:10:20.235
So the exclamation mark standing
before an array is factorial.

00:10:24.020 --> 00:10:25.870
And then we'll move
on to the next syntax.

00:10:25.870 --> 00:10:28.710
There's only six
or seven of these.

00:10:28.710 --> 00:10:31.740
The next one is a function
with an array on both sides.

00:10:31.740 --> 00:10:34.880
We call that a dyadic,
or infix, function.

00:10:34.880 --> 00:10:37.660
The first one we would
call a monadic function.

00:10:37.660 --> 00:10:39.660
Unfortunately, we started
using that terminology

00:10:39.660 --> 00:10:42.230
before monads were invented
in functional programming.

00:10:42.230 --> 00:10:43.630
And it's a bit hard.

00:10:43.630 --> 00:10:46.240
But I'm trying to use
prefix and infix here,

00:10:46.240 --> 00:10:49.480
to make it a little
bit more palatable.

00:10:49.480 --> 00:10:53.460
So, the thing that
would have been ceiling,

00:10:53.460 --> 00:10:56.310
if you give it two arguments,
it takes the largest

00:10:56.310 --> 00:10:59.170
of the two arguments.

00:10:59.170 --> 00:11:02.180
If you have the exclamation mark
with something on both sides,

00:11:02.180 --> 00:11:04.170
it's related to the factorial.

00:11:04.170 --> 00:11:06.520
It's the binomial, or
the gamma, function.

00:11:06.520 --> 00:11:09.520
If they're a floating
point, we have division,

00:11:09.520 --> 00:11:11.470
and so on and so forth.

00:11:11.470 --> 00:11:13.670
And of course, not
everything works.

00:11:13.670 --> 00:11:15.800
We have errors.

00:11:15.800 --> 00:11:18.960
And the same if you try and
divide a number by a character.

00:11:18.960 --> 00:11:20.850
That doesn't work.

00:11:20.850 --> 00:11:26.160
But here is this expression that
made me fall in love with APL.

00:11:26.160 --> 00:11:29.150
Two arrays, one
function in the middle,

00:11:29.150 --> 00:11:31.960
and it just maps without
you having to say anything.

00:11:31.960 --> 00:11:34.550
And it applies in the
same way to virtually

00:11:34.550 --> 00:11:36.470
all of these functions,
and also any functions

00:11:36.470 --> 00:11:40.890
that you might write yourself,
typically will just apply.

00:11:40.890 --> 00:11:43.300
Now, if there is not the
same number on both sides,

00:11:43.300 --> 00:11:45.970
you get what we
call a length error.

00:11:45.970 --> 00:11:48.130
But if there's exactly
one number on the one side

00:11:48.130 --> 00:11:49.880
and an array on the
other, we do something

00:11:49.880 --> 00:11:52.760
called scalar extension,
and the single number

00:11:52.760 --> 00:11:56.130
is applied to all of the items.

00:11:56.130 --> 00:11:59.060
So far, we've only used lists.

00:11:59.060 --> 00:12:00.430
Here is a matrix.

00:12:00.430 --> 00:12:04.430
So this is the 2 by
4 reshape of iota 12.

00:12:04.430 --> 00:12:08.594
So that gives us-- iota
8 would have been enough.

00:12:08.594 --> 00:12:10.510
I think I had a different
example in mind when

00:12:10.510 --> 00:12:13.790
I started writing that line.

00:12:13.790 --> 00:12:17.620
And I can do matrix
plus matrix, any rank.

00:12:17.620 --> 00:12:19.390
It all works exactly
the same way.

00:12:19.390 --> 00:12:23.190
There's no different rules to
learn as the rank increases.

00:12:23.190 --> 00:12:26.590
Writing rank in variant
code is trivial.

00:12:28.921 --> 00:12:29.420
Right.

00:12:29.420 --> 00:12:31.128
So I'm just changing
the print precision,

00:12:31.128 --> 00:12:33.240
because I'm going to take
the base 2 logarithm.

00:12:33.240 --> 00:12:35.320
So again, for a
matrix on the right,

00:12:35.320 --> 00:12:36.840
and a single number
on the left, we

00:12:36.840 --> 00:12:41.070
can take the base 2 logarithm
of all of these numbers.

00:12:41.070 --> 00:12:43.460
Now, not all of the functions
are these scalar functions.

00:12:43.460 --> 00:12:46.830
So all the mathematical and
logical functions are scalar.

00:12:46.830 --> 00:12:51.260
But the iota, if you give it
two arguments, is a look-up.

00:12:51.260 --> 00:12:53.520
So it returns something
with the same shape

00:12:53.520 --> 00:12:55.730
as the right argument.

00:12:55.730 --> 00:12:58.350
But it's looking each of
the items up in the left.

00:12:58.350 --> 00:13:00.940
So it's still array-oriented,
but it's not doing

00:13:00.940 --> 00:13:02.340
things a scalar at a time.

00:13:02.340 --> 00:13:04.930
Not everything is
a scalar function.

00:13:07.640 --> 00:13:14.280
And then we have, again,
slightly different terminology

00:13:14.280 --> 00:13:14.780
in APL.

00:13:14.780 --> 00:13:18.490
We distinguish between what we
call functions and operators.

00:13:18.490 --> 00:13:20.830
An operator is something
which typically takes

00:13:20.830 --> 00:13:23.540
a function as its operand.

00:13:23.540 --> 00:13:26.030
And the syntax there is you
have a function preceding

00:13:26.030 --> 00:13:26.730
an operator.

00:13:26.730 --> 00:13:32.210
For example, plus reduce,
the operator is on the right,

00:13:32.210 --> 00:13:34.970
and it's acting on the plus.

00:13:34.970 --> 00:13:39.280
Again, I can name functions the
same way I can name variables.

00:13:39.280 --> 00:13:41.000
So that gives me a
function that does

00:13:41.000 --> 00:13:43.620
the plus reduction of an array.

00:13:43.620 --> 00:13:46.800
So plus reduce is the same
as putting the plus function

00:13:46.800 --> 00:13:48.802
in between all the
items of the arrays.

00:13:48.802 --> 00:13:51.010
For some of you who are
doing functional programming,

00:13:51.010 --> 00:13:53.560
these concepts will be familiar.

00:13:53.560 --> 00:13:56.770
And they mostly came
out of APL originally.

00:13:56.770 --> 00:13:59.180
A lot of the terminology
was invented back

00:13:59.180 --> 00:14:03.820
in the late '50s and '60s.

00:14:03.820 --> 00:14:06.720
And all scalar functions
apply in the same way.

00:14:06.720 --> 00:14:08.300
So anything that
takes two arguments,

00:14:08.300 --> 00:14:14.620
you can do a reduction, and
it's going to just do the work.

00:14:14.620 --> 00:14:19.750
There's also, if you turn
the slash the other way,

00:14:19.750 --> 00:14:23.730
you get a scan, where it
takes a prefix of a vector

00:14:23.730 --> 00:14:26.320
with one item, then two items,
three items, four items,

00:14:26.320 --> 00:14:27.990
and does the reduction.

00:14:27.990 --> 00:14:31.655
So we have the accumulating
sum if you use it with plus.

00:14:35.170 --> 00:14:38.750
And then the reduction
also can take

00:14:38.750 --> 00:14:40.580
a left argument,
which is the window

00:14:40.580 --> 00:14:41.970
size that you want to use.

00:14:41.970 --> 00:14:44.630
So you can say, I'd like to do
the plus reduction of length

00:14:44.630 --> 00:14:46.170
three vectors.

00:14:49.210 --> 00:14:52.420
If we do a catenate reduction
with a length three,

00:14:52.420 --> 00:14:55.620
you can see that just catenates
the items together and doesn't

00:14:55.620 --> 00:14:56.620
do an operation on them.

00:14:56.620 --> 00:15:02.850
And you can see the windows
that were processed one by one.

00:15:02.850 --> 00:15:07.020
And then finally, we have the
function, operator function.

00:15:07.020 --> 00:15:11.230
A dyadic operator that
works on two functions,

00:15:11.230 --> 00:15:13.560
where the one that you
will all be familiar with

00:15:13.560 --> 00:15:15.550
is the vector product.

00:15:15.550 --> 00:15:20.140
So plus dot times inner product.

00:15:20.140 --> 00:15:24.630
So plus dot times is equivalent
to the plus reduction

00:15:24.630 --> 00:15:31.030
of the map of times, which is
how you do a vector product,

00:15:31.030 --> 00:15:32.070
of course.

00:15:32.070 --> 00:15:36.560
But in APL that's a
completely general construct,

00:15:36.560 --> 00:15:38.040
so it's not just plus and times.

00:15:38.040 --> 00:15:40.780
In many tools like
MATLAB or R, you

00:15:40.780 --> 00:15:43.950
will have this thing
extracted and presented

00:15:43.950 --> 00:15:48.090
as a matrix product, or vector
product, a named function.

00:15:48.090 --> 00:15:52.200
But in APL-- this is just
restating it the same way.

00:15:52.200 --> 00:15:54.030
In APL, it's completely general.

00:15:54.030 --> 00:15:57.050
So you could do an or
dot equals, which says,

00:15:57.050 --> 00:15:59.770
are any of the items equals?

00:15:59.770 --> 00:16:06.531
So it maps the equals, and
then it does an or reduction.

00:16:06.531 --> 00:16:07.030
OK?

00:16:10.740 --> 00:16:15.210
But that's nearly all
the syntax there is.

00:16:15.210 --> 00:16:17.200
The only thing you need
to remember when you're

00:16:17.200 --> 00:16:19.130
trying to parse the
syntax is that there

00:16:19.130 --> 00:16:20.860
are no precedence rules.

00:16:20.860 --> 00:16:22.700
Iverson was wound
up about all the

00:16:22.700 --> 00:16:28.730
different-- when you do sine
squared of x, what order are

00:16:28.730 --> 00:16:29.810
the operations done?

00:16:29.810 --> 00:16:31.900
So he picked this
one, which he felt

00:16:31.900 --> 00:16:35.940
was perhaps most representative
of the general precedence.

00:16:35.940 --> 00:16:39.010
Or this one was nice
and easy, anyway.

00:16:39.010 --> 00:16:42.840
And he said the
argument to the function

00:16:42.840 --> 00:16:46.500
is the result of executing
everything that's on the right.

00:16:46.500 --> 00:16:49.190
So this is the
factorial of iota six.

00:16:49.190 --> 00:16:51.110
So there are no
precedence rules.

00:16:51.110 --> 00:16:54.770
You just apply the function
to the result of computing

00:16:54.770 --> 00:16:57.280
what's on the right.

00:16:57.280 --> 00:17:03.290
And if it's a dyadic function,
then it essentially works

00:17:03.290 --> 00:17:08.050
in the same way, but the left
arguments are also included.

00:17:08.050 --> 00:17:10.496
So, if you were coming from
another programming language,

00:17:10.496 --> 00:17:12.079
and you looked at
something like this,

00:17:12.079 --> 00:17:15.440
you might assume that it's going
to do-- that's 10 divided by 5,

00:17:15.440 --> 00:17:16.970
minus 1.

00:17:16.970 --> 00:17:21.390
But in APL, of course, it's
10, divided by 5 minus 1.

00:17:21.390 --> 00:17:26.740
And it doesn't give you the
result you were thinking of.

00:17:26.740 --> 00:17:31.400
So remember that as you read
the rest of this, if you can.

00:17:31.400 --> 00:17:35.350
And the final piece of syntax
is an array, the square bracket

00:17:35.350 --> 00:17:38.140
indexing, which you would
be familiar with from most

00:17:38.140 --> 00:17:39.210
languages.

00:17:39.210 --> 00:17:41.210
The thing that's slightly
different about APL

00:17:41.210 --> 00:17:43.630
is that you can
index with an array.

00:17:43.630 --> 00:17:45.950
So the indices can be arrays.

00:17:45.950 --> 00:17:48.800
I think you do see that
in some other languages.

00:17:48.800 --> 00:17:52.150
So the shape of the
result has the same shape

00:17:52.150 --> 00:17:59.740
as the index array, rather than
the data that's being indexed.

00:17:59.740 --> 00:18:02.030
Of course, once
functional programming

00:18:02.030 --> 00:18:04.510
became better
understood-- most of APL

00:18:04.510 --> 00:18:06.560
predates really an
understanding of what

00:18:06.560 --> 00:18:08.510
functional programming was.

00:18:08.510 --> 00:18:10.680
So in 1984, it was
decided that there

00:18:10.680 --> 00:18:15.590
should be a function, comprised
by over-striking the two

00:18:15.590 --> 00:18:17.740
square brackets,
the squad indexing,

00:18:17.740 --> 00:18:22.030
so that you could do indexing
in a functional style.

00:18:22.030 --> 00:18:22.530
OK.

00:18:22.530 --> 00:18:26.242
So if this has whetted
your appetite at all,

00:18:26.242 --> 00:18:32.140
there is a website called
TryAPL.org where you

00:18:32.140 --> 00:18:34.750
can type your APL expressions.

00:18:34.750 --> 00:18:40.370
And there's an APL keyboard
that pops up if you need it.

00:18:40.370 --> 00:18:44.140
And there's a primer page, where
you can hover over each one.

00:18:44.140 --> 00:18:46.500
I mean, this list
here is all that there

00:18:46.500 --> 00:18:48.500
is to learn about the language.

00:18:48.500 --> 00:18:51.060
It may look a bit intimidating
with the squiggles,

00:18:51.060 --> 00:18:55.000
but there's not
that much to learn.

00:18:55.000 --> 00:18:56.940
You can hover over
these in TryAPL,

00:18:56.940 --> 00:18:59.040
and you can see the
definition of each one.

00:18:59.040 --> 00:19:02.930
And in the Windows
product, and on the Mac,

00:19:02.930 --> 00:19:07.287
we have this language bar where
you can hover over the symbols,

00:19:07.287 --> 00:19:08.620
and you can get the definitions.

00:19:08.620 --> 00:19:12.090
We run a programming competition
for students each year.

00:19:12.090 --> 00:19:15.620
And we had the winner one year
say, he would read the problem

00:19:15.620 --> 00:19:18.720
description, and then he would
just hover with the mouse

00:19:18.720 --> 00:19:20.870
slowly from left
to right until he

00:19:20.870 --> 00:19:23.340
saw how to solve the problem.

00:19:23.340 --> 00:19:26.870
And that was all the
documentation he needed.

00:19:26.870 --> 00:19:30.810
So if your brain clicks when
you see this array notation,

00:19:30.810 --> 00:19:33.680
there's these six rules,
and there's 40 functions,

00:19:33.680 --> 00:19:35.091
and off you go.

00:19:35.091 --> 00:19:37.090
Of course, there's more
to building applications

00:19:37.090 --> 00:19:37.590
than that.

00:19:40.840 --> 00:19:44.420
So, briefly back to
PowerPoint, unfortunately.

00:19:47.340 --> 00:19:48.670
So, we've looked at that.

00:19:48.670 --> 00:19:50.190
So just to recap
the fundamentals.

00:19:50.190 --> 00:19:53.400
There's only one data type
in APL, the immutable array.

00:19:53.400 --> 00:19:55.440
There's a little star
there that we'll get to

00:19:55.440 --> 00:19:57.930
at the bottom of the slide.

00:19:57.930 --> 00:20:06.470
Each item is either a number, or
a character, or another array.

00:20:06.470 --> 00:20:09.980
For example, a single number
is a zero-dimensional array,

00:20:09.980 --> 00:20:12.180
but it's still
considered an array.

00:20:12.180 --> 00:20:13.720
It's not different type.

00:20:13.720 --> 00:20:16.490
There isn't different
syntax to deal with it.

00:20:16.490 --> 00:20:19.490
For most primitive
functions, map is implicit.

00:20:19.490 --> 00:20:23.590
The functions are
either prefix or infix,

00:20:23.590 --> 00:20:28.380
but the operators are
either postfix or infix.

00:20:28.380 --> 00:20:32.576
And I don't know how
Iverson and his compatriots,

00:20:32.576 --> 00:20:34.950
when they were designing the
language, came up with that.

00:20:34.950 --> 00:20:37.170
But there's something
magical about this interplay

00:20:37.170 --> 00:20:39.970
between the functions being
prefix and the operators

00:20:39.970 --> 00:20:43.250
postfix when you're
designing a DSL.

00:20:43.250 --> 00:20:45.940
So writing an embedded
DSL is typically

00:20:45.940 --> 00:20:48.030
how a lot of APL
application development

00:20:48.030 --> 00:20:51.780
is done, with the
end user synthesizing

00:20:51.780 --> 00:20:54.870
their own functions from
these building blocks.

00:20:54.870 --> 00:20:58.410
The order of execution we looked
at, and here's the little star.

00:20:58.410 --> 00:21:00.470
We added-- at least
in Dyalog APL,

00:21:00.470 --> 00:21:03.880
we added the ability to have
objects; so like these forms,

00:21:03.880 --> 00:21:08.602
like dot net objects, or other
types of external objects.

00:21:08.602 --> 00:21:10.060
And of course once
you have an item

00:21:10.060 --> 00:21:13.110
in an array which is an object,
you lose the immutability.

00:21:13.110 --> 00:21:16.400
Because that thing
is a reference type.

00:21:16.400 --> 00:21:21.870
And we're not going to copy
them when we copy the arrays.

00:21:21.870 --> 00:21:26.170
So, how did this
work then in the end?

00:21:26.170 --> 00:21:30.410
He had this list, and he came
up with the new notation.

00:21:30.410 --> 00:21:32.830
And let's organize them and see.

00:21:32.830 --> 00:21:33.330
OK.

00:21:33.330 --> 00:21:36.060
So, that's a times b.

00:21:36.060 --> 00:21:40.770
That's the e to the power x,
no left argument, and the power

00:21:40.770 --> 00:21:43.860
function means e to the power.

00:21:43.860 --> 00:21:46.280
x divided by y.

00:21:46.280 --> 00:21:48.790
The base b logarithm of a.

00:21:48.790 --> 00:21:53.920
Notice the symbol is a log
seen edgewise, at the same time

00:21:53.920 --> 00:21:55.950
as it's clearly related
to this one, right?

00:21:55.950 --> 00:21:58.750
So there's a lot of that
kind of mnemonic stuff

00:21:58.750 --> 00:22:01.720
to help you learn the language.

00:22:01.720 --> 00:22:05.930
The nth root is a to the
power reciprocal of n.

00:22:05.930 --> 00:22:09.490
Matrix multiplication is
like vector multiplication.

00:22:09.490 --> 00:22:11.120
If you have two
matrices, it just

00:22:11.120 --> 00:22:14.950
does the vector product on
the columns on the one side,

00:22:14.950 --> 00:22:16.730
and the row on the other.

00:22:16.730 --> 00:22:19.730
fgx is f g x.

00:22:19.730 --> 00:22:24.140
The third trigonometric
function squared.

00:22:24.140 --> 00:22:28.250
Sigmas and pis are
easily expressed

00:22:28.250 --> 00:22:32.850
using plus reduce something
times iota something.

00:22:32.850 --> 00:22:36.430
And OK, there isn't going to be
room to the right of this one.

00:22:36.430 --> 00:22:38.550
But it still maps.

00:22:38.550 --> 00:22:41.320
I mean, if you are accustomed
to mathematical notation,

00:22:41.320 --> 00:22:44.730
the translation
of this into that

00:22:44.730 --> 00:22:49.030
is not a very difficult thing.

00:22:49.030 --> 00:22:53.440
And once you have experience
with the language,

00:22:53.440 --> 00:22:56.200
although it clearly
executes from right to left,

00:22:56.200 --> 00:22:59.940
we would say you read
it from left to right.

00:22:59.940 --> 00:23:02.240
So to read that statement
there, you would say,

00:23:02.240 --> 00:23:08.710
well, it's two times a divided
into minus b plus or minus

00:23:08.710 --> 00:23:11.860
the square root of
b squared minus 4ac,

00:23:11.860 --> 00:23:15.030
which is probably how you
would read the math as well.

00:23:15.030 --> 00:23:21.270
And if you're writing good APL
code, at least in my group,

00:23:21.270 --> 00:23:23.100
if you can't read it
from left to right,

00:23:23.100 --> 00:23:25.760
you should probably
split the function up.

00:23:25.760 --> 00:23:29.260
But that's something that
they're different opinions

00:23:29.260 --> 00:23:30.640
about that, of course.

00:23:34.470 --> 00:23:37.820
So, APL was instrumental
in discovering

00:23:37.820 --> 00:23:41.010
functional programming.

00:23:41.010 --> 00:23:44.980
And John Backus, the
inventor, I think, of FORTRAN.

00:23:44.980 --> 00:23:48.990
And Backus [INAUDIBLE] in
his Turing Award lecture,

00:23:48.990 --> 00:23:52.630
recognized Ken's work.

00:23:52.630 --> 00:23:56.230
But he did go on to
say-- he wasn't quite as

00:23:56.230 --> 00:24:00.350
unhappy as Dijkstra, of course.

00:24:00.350 --> 00:24:01.940
But he was still
unhappy with the fact

00:24:01.940 --> 00:24:05.100
that there were the world of
expressions and statements,

00:24:05.100 --> 00:24:07.872
and there were only these
three functional forms.

00:24:07.872 --> 00:24:09.580
There were actually
four, he missed scan.

00:24:12.520 --> 00:24:15.100
But since then, I
mean, that was in 1977.

00:24:15.100 --> 00:24:20.030
So APL today, although many
people equate APL with,

00:24:20.030 --> 00:24:21.520
they learned it once.

00:24:21.520 --> 00:24:24.070
They did some work in a
bank in the '70s or '80s.

00:24:24.070 --> 00:24:26.090
And they're not
aware of the work

00:24:26.090 --> 00:24:28.450
that's been done since then.

00:24:28.450 --> 00:24:30.060
So, we have new
functional forms.

00:24:30.060 --> 00:24:35.160
We have ways of defining
functions in a lexical style,

00:24:35.160 --> 00:24:37.380
a functional lexical style.

00:24:37.380 --> 00:24:44.430
So the first function here is
the definition of the average.

00:24:44.430 --> 00:24:46.710
Omega is a reference
to the right arguments.

00:24:46.710 --> 00:24:49.130
If you have a function
that just references omega,

00:24:49.130 --> 00:24:51.600
it's a prefix function.

00:24:51.600 --> 00:24:55.080
The next one, plus double,
is an infix function,

00:24:55.080 --> 00:24:58.790
because it references both
alpha, the left argument,

00:24:58.790 --> 00:25:00.520
and omega.

00:25:00.520 --> 00:25:04.940
And then the fibonacci function
there is a multi-line function.

00:25:04.940 --> 00:25:07.850
The omega equals 0 is a guard.

00:25:07.850 --> 00:25:11.690
So it says, if this is true,
then the result is simply that.

00:25:11.690 --> 00:25:13.210
Otherwise it continues.

00:25:13.210 --> 00:25:18.330
And the dell is a recursion,
so it's calling itself again.

00:25:21.170 --> 00:25:26.990
Let's see, how are
we doing for time?

00:25:26.990 --> 00:25:27.490
Yeah.

00:25:27.490 --> 00:25:29.290
So there's a whole bunch
of functional forms.

00:25:29.290 --> 00:25:30.780
We're going to be
using some of these,

00:25:30.780 --> 00:25:32.238
so I won't really
go into them now.

00:25:32.238 --> 00:25:38.554
But let's go back and look at
one of the new functional forms

00:25:38.554 --> 00:25:40.470
that I think is kind of
fun, that wasn't there

00:25:40.470 --> 00:25:42.100
when Backus was looking at it.

00:25:42.100 --> 00:25:45.250
So here is a function,
1 plus the reciprocal

00:25:45.250 --> 00:25:46.740
of the right argument.

00:25:46.740 --> 00:25:48.870
That returns 2.

00:25:48.870 --> 00:25:53.960
If you apply it twice,
and three times,

00:25:53.960 --> 00:25:56.110
it's heading towards
the golden ratio,

00:25:56.110 --> 00:26:00.180
for those of you who
recognize the math.

00:26:00.180 --> 00:26:02.000
Now, it gets a bit
tedious if you have

00:26:02.000 --> 00:26:03.150
to do that again and again.

00:26:03.150 --> 00:26:05.210
So there's a power
operator-- which

00:26:05.210 --> 00:26:09.310
you see is a very similar
symbol to the mathematical

00:26:09.310 --> 00:26:13.390
exponentiation, it just
has a dieresis above it--

00:26:13.390 --> 00:26:16.120
where you can say, execute,
repeat the function

00:26:16.120 --> 00:26:20.890
in this case, 4 times, 5 times.

00:26:20.890 --> 00:26:24.740
Or create a little
lambda function,

00:26:24.740 --> 00:26:27.920
which takes the right argument,
and applies it that many times.

00:26:27.920 --> 00:26:30.840
And apply that each to iota 10.

00:26:30.840 --> 00:26:33.190
And you can see we're
getting closer and closer

00:26:33.190 --> 00:26:35.970
to the golden ratio.

00:26:35.970 --> 00:26:37.540
But then there's
an extension to it,

00:26:37.540 --> 00:26:42.820
where if the right operand is
not a number but a function,

00:26:42.820 --> 00:26:47.100
then that function is applied
between successive results,

00:26:47.100 --> 00:26:51.300
and the iteration terminates
when it returns true.

00:26:51.300 --> 00:26:55.440
So this will continue until
we get to the fixed point,

00:26:55.440 --> 00:26:57.410
and we have the golden ratio.

00:27:01.410 --> 00:27:06.330
I think we're running
a little bit-- I'm

00:27:06.330 --> 00:27:09.580
talking too much, as always.

00:27:09.580 --> 00:27:12.340
So, I'm going to skip
over a little bit here.

00:27:15.210 --> 00:27:18.370
We saw an example
of a user-defined--

00:27:18.370 --> 00:27:22.230
we will see a slide with
some user-defined operators.

00:27:22.230 --> 00:27:24.030
So you can write
user-defined operators

00:27:24.030 --> 00:27:26.010
using the same curly
bracket notation.

00:27:26.010 --> 00:27:31.010
But you refer to the operands
as alpha alpha and omega omega.

00:27:31.010 --> 00:27:33.410
Works pretty much the same way.

00:27:33.410 --> 00:27:35.510
So, in addition to these
new functional forms,

00:27:35.510 --> 00:27:37.980
and if I don't talk
too much, we'll

00:27:37.980 --> 00:27:40.620
see pretty much all of them
in some live code a little bit

00:27:40.620 --> 00:27:43.180
later.

00:27:43.180 --> 00:27:48.660
Here's a slide with
a postfix operator,

00:27:48.660 --> 00:27:51.320
because it refers
only to alpha alpha;

00:27:51.320 --> 00:27:54.870
and a dyadic, or infix,
operator referring both

00:27:54.870 --> 00:27:57.560
to alpha alpha and omega omega.

00:27:57.560 --> 00:28:02.200
But I don't really have time
to spend as much time on that

00:28:02.200 --> 00:28:02.900
as I hoped.

00:28:02.900 --> 00:28:04.427
Anyway, so there's a gap here.

00:28:04.427 --> 00:28:06.260
You notice there's a
big hole in the middle.

00:28:06.260 --> 00:28:08.230
Because there's one
thing that Iverson

00:28:08.230 --> 00:28:12.550
couldn't figure out how to get
a good notation for in APL.

00:28:12.550 --> 00:28:14.010
And that's a
mathematical notation

00:28:14.010 --> 00:28:16.070
where you are adding
two functions together,

00:28:16.070 --> 00:28:22.040
which in math, you would write
f plus g in parentheses of x.

00:28:22.040 --> 00:28:25.660
And the story goes that
he's been searching for this

00:28:25.660 --> 00:28:26.980
for a couple of decades.

00:28:26.980 --> 00:28:31.350
Arthur may know more of
that story than I do.

00:28:31.350 --> 00:28:34.870
And then on the plane on the
way back from an APL conference

00:28:34.870 --> 00:28:36.390
in Australia, he woke up.

00:28:36.390 --> 00:28:38.020
The sun came through
the window, and he

00:28:38.020 --> 00:28:44.690
realized that the solution
was just to write f plus g x.

00:28:44.690 --> 00:28:47.480
Because that was a
syntax error in APL.

00:28:47.480 --> 00:28:52.070
You can't have a train of three
functions with no argument

00:28:52.070 --> 00:28:54.150
if you put them in parentheses.

00:28:54.150 --> 00:28:57.430
So that was exactly the
syntax he was looking for.

00:28:57.430 --> 00:28:59.690
But of course being
Iverson, he wasn't

00:28:59.690 --> 00:29:03.080
satisfied to have a
syntax for f plus g.

00:29:06.970 --> 00:29:09.470
He's thinking that it could
be any three functions.

00:29:09.470 --> 00:29:14.400
So there's a general definition
of a train of three functions.

00:29:14.400 --> 00:29:17.920
f g h, that gives
you a function which,

00:29:17.920 --> 00:29:24.250
if it has a right argument, it's
f of x and h of x, so the two

00:29:24.250 --> 00:29:26.360
functions on either side.

00:29:26.360 --> 00:29:29.800
And then the result is piped
into the middle function, which

00:29:29.800 --> 00:29:31.800
is why that's
referred to as a fork.

00:29:31.800 --> 00:29:35.640
Because it's bringing
the functions together

00:29:35.640 --> 00:29:39.010
to the central function.

00:29:39.010 --> 00:29:41.116
And if it's dyadic, it
extends the same way.

00:29:41.116 --> 00:29:42.740
And then you can also
have a definition

00:29:42.740 --> 00:29:46.750
for a train of two
functions together,

00:29:46.750 --> 00:29:50.200
which is called an atop,
but it's really just a fork

00:29:50.200 --> 00:29:55.010
that only has one arm.

00:29:55.010 --> 00:29:55.975
So are those useful?

00:29:55.975 --> 00:29:57.150
Or is it just--

00:30:15.350 --> 00:30:17.900
So, we've seen the definition.

00:30:17.900 --> 00:30:21.000
But it turns out that
something quite as ordinary

00:30:21.000 --> 00:30:24.400
as the average is a fork,
because the average is

00:30:24.400 --> 00:30:28.580
the sum divided by the count.

00:30:28.580 --> 00:30:33.370
So, plus, reduce, divide, and
the function called tally.

00:30:33.370 --> 00:30:36.050
You can see it's a
four-fingered animal,

00:30:36.050 --> 00:30:42.440
I guess, counting 1, 2, 3,
4, the symbol for tally,

00:30:42.440 --> 00:30:46.650
and there you are.

00:30:46.650 --> 00:30:50.060
And you can also take
derived functions,

00:30:50.060 --> 00:30:52.385
so the max reduce,
and the min reduce,

00:30:52.385 --> 00:30:56.530
and the comma in
between is also a fork.

00:30:56.530 --> 00:30:58.420
And in fact they extend.

00:30:58.420 --> 00:31:02.040
The notation is such that
they nest from the right.

00:31:02.040 --> 00:31:06.550
So that's the plus reduce
forked with the fork defined

00:31:06.550 --> 00:31:09.990
by max reduce and min reduce.

00:31:09.990 --> 00:31:13.940
So you can get this syntax
where you can say, well, give me

00:31:13.940 --> 00:31:17.350
the sum, give me the maximum,
and give me the minimum.

00:31:17.350 --> 00:31:19.850
It's not actually an
array of functions.

00:31:19.850 --> 00:31:24.960
But when you use it this way, it
works as that kind of notation.

00:31:38.390 --> 00:31:42.940
Now, the original
title of this talk

00:31:42.940 --> 00:31:44.900
was Pragmatic
Functional Programming.

00:31:44.900 --> 00:31:47.060
How many people
here would say they

00:31:47.060 --> 00:31:49.770
are functional programmers?

00:31:49.770 --> 00:31:50.990
It's not that many.

00:31:50.990 --> 00:31:53.420
So I have a little
section here where

00:31:53.420 --> 00:31:59.980
I show the APL equivalents of
Scheme examples, and so on.

00:31:59.980 --> 00:32:03.740
Maybe I'll skip the live
code, since there was only,

00:32:03.740 --> 00:32:06.680
I think, three people
who put their hands up.

00:32:06.680 --> 00:32:09.790
But as you can see--
and I think we've

00:32:09.790 --> 00:32:13.280
seen most of these in
the live code already--

00:32:13.280 --> 00:32:16.420
most of the classical
functional forms

00:32:16.420 --> 00:32:21.730
from a language like Scheme have
very simple equivalents in APL.

00:32:21.730 --> 00:32:28.319
Juxtaposition creates
lists, and so on.

00:32:28.319 --> 00:32:29.735
And you're not
Scheme programmers,

00:32:29.735 --> 00:32:31.925
so I'll skip over
that one as well.

00:32:31.925 --> 00:32:33.300
One of the things,
though, that I

00:32:33.300 --> 00:32:35.020
think I do want to
spend some time on,

00:32:35.020 --> 00:32:39.040
is sort of talking
about how when

00:32:39.040 --> 00:32:43.320
you write idiomatic code in APL,
it generalizes to higher ranks.

00:32:43.320 --> 00:32:47.340
It's rank invariant code.

00:32:47.340 --> 00:32:51.840
So in Scheme, the definition
of a vector product

00:32:51.840 --> 00:32:55.620
and the matrix multiplication
are quite different.

00:32:55.620 --> 00:33:02.670
There's a lot more code
there in the matrix form.

00:33:12.670 --> 00:33:17.270
So here is a function, the
question mark 6, do it again.

00:33:17.270 --> 00:33:19.370
Any guesses for what this does?

00:33:19.370 --> 00:33:23.030
Generates random numbers.

00:33:23.030 --> 00:33:27.330
The right argument gives
the range of integers.

00:33:27.330 --> 00:33:29.850
Now if you parse it in
array, it then applies that.

00:33:29.850 --> 00:33:33.680
So now we've rolled four dice.

00:33:33.680 --> 00:33:35.560
This reshape
function can be used

00:33:35.560 --> 00:33:38.310
to generate an array
of any rank and shape.

00:33:38.310 --> 00:33:42.090
So the 10 reshape of
6 gives you ten 6's.

00:33:42.090 --> 00:33:46.075
So I can say I want to roll
10 dice using this expression.

00:33:48.920 --> 00:33:50.984
Now I'm setting the
random seed here so

00:33:50.984 --> 00:33:52.900
that I get some numbers
that are going to work

00:33:52.900 --> 00:33:56.700
for the next few lines of code.

00:33:56.700 --> 00:34:04.900
So I roll 10 dice, and now
I want to do some analysis.

00:34:04.900 --> 00:34:09.040
So we haven't really seen
any logical functions yet,

00:34:09.040 --> 00:34:10.810
but they also work
in the same way.

00:34:10.810 --> 00:34:12.070
They are all scalar functions.

00:34:12.070 --> 00:34:16.580
So throws equals 5
returns-- well you

00:34:16.580 --> 00:34:18.880
might expect some
kind of Boolean array.

00:34:18.880 --> 00:34:22.040
And, in fact, APL programmers
call this a Boolean array,

00:34:22.040 --> 00:34:26.650
but actually it's
a 1 bit integer.

00:34:26.650 --> 00:34:29.649
We have 1 bit
integers, 8 bit, 16,

00:34:29.649 --> 00:34:31.502
32 bit integers in
the implementation.

00:34:34.260 --> 00:34:37.189
And the nice thing about
them being integers

00:34:37.189 --> 00:34:38.730
is that they are in
the domain of all

00:34:38.730 --> 00:34:40.040
the mathematical functions.

00:34:40.040 --> 00:34:42.940
So if I want to know how
many throws were equal to 5,

00:34:42.940 --> 00:34:46.130
I can just write
this expression.

00:34:46.130 --> 00:34:48.620
And I can use this Outer
Product thing to do

00:34:48.620 --> 00:34:49.840
a whole bunch of comparisons.

00:34:49.840 --> 00:34:53.250
So Outer Product takes
all the items on the left,

00:34:53.250 --> 00:34:56.090
and all the items on
the right, and combines

00:34:56.090 --> 00:34:57.220
them using the function.

00:34:57.220 --> 00:35:00.710
The null dot equal-- null
dot is the Outer Product,

00:35:00.710 --> 00:35:05.410
and the equals is the
function that you're applying.

00:35:05.410 --> 00:35:10.660
So I can find out how many there
were of each one of the six

00:35:10.660 --> 00:35:13.320
alternatives by
writing plus reduce,

00:35:13.320 --> 00:35:16.500
iota 6, null dot
equals, and my throws.

00:35:16.500 --> 00:35:19.880
And so now I'm
generating a million--

00:35:19.880 --> 00:35:25.250
I'm rolling a million dice
with the same expression.

00:35:25.250 --> 00:35:27.469
And you can see that
doesn't take-- well,

00:35:27.469 --> 00:35:30.010
on a modern computer, you might
not expect that to take time.

00:35:30.010 --> 00:35:31.670
But in an interpreted
language, you

00:35:31.670 --> 00:35:35.040
might think it would
take significant time.

00:35:35.040 --> 00:35:37.510
But of course, there's
virtually no interpretation

00:35:37.510 --> 00:35:38.220
going on here.

00:35:38.220 --> 00:35:42.930
You could almost think of the
APL expression as Bytecode.

00:35:42.930 --> 00:35:45.990
It's diving straight into
highly optimized C code, that's

00:35:45.990 --> 00:35:51.940
using the 1 byte integer data
type, because the range is six.

00:35:51.940 --> 00:35:56.500
And it can do this
extremely fast.

00:35:56.500 --> 00:36:00.940
And if you build user-defined
functions based on the APL

00:36:00.940 --> 00:36:05.820
primitives, of course without
really having to do anything.

00:36:05.820 --> 00:36:09.840
So I say, the square root
is omega to the power 0.5.

00:36:09.840 --> 00:36:12.270
I have a function which
is also rank invariant,

00:36:12.270 --> 00:36:16.440
because it's just parsing those
arrays to primitive functions.

00:36:16.440 --> 00:36:21.150
I can call it on our 2 by 4
mat that we defined earlier.

00:36:23.720 --> 00:36:27.670
And I can define the
hypotenuse as the square root

00:36:27.670 --> 00:36:32.310
of the Inner Product with power.

00:36:32.310 --> 00:36:36.710
So I take the left argument, the
right argument, I square them,

00:36:36.710 --> 00:36:40.400
and I do a plus reduction
of the result of that.

00:36:40.400 --> 00:36:42.020
And I take the square root.

00:36:42.020 --> 00:36:44.915
So the hypotenuse is 5.

00:36:47.590 --> 00:36:49.220
Print precision 6.

00:36:49.220 --> 00:36:51.940
But again, this function
just applies without me

00:36:51.940 --> 00:36:53.430
having to write any loops.

00:36:53.430 --> 00:37:00.840
It would apply to
arguments of higher ranks.

00:37:00.840 --> 00:37:06.860
And in fact, if you have a
dyadic user-defined function,

00:37:06.860 --> 00:37:09.150
it can be used with
most of these operators

00:37:09.150 --> 00:37:11.360
in the same way as a primitive.

00:37:11.360 --> 00:37:14.150
So this is the plus
dot hypotenuse.

00:37:14.150 --> 00:37:18.180
So it's the plus reduction
of the hypotenuse map

00:37:18.180 --> 00:37:18.955
on these vectors.

00:37:21.850 --> 00:37:23.430
So you can combine.

00:37:23.430 --> 00:37:28.232
And some people, if you're
coming from a typical software

00:37:28.232 --> 00:37:29.940
engineering background,
you'll say, well,

00:37:29.940 --> 00:37:32.480
this business that APL can only
have left argument and right

00:37:32.480 --> 00:37:33.180
argument.

00:37:33.180 --> 00:37:36.280
You're going to need more
arguments, three arguments,

00:37:36.280 --> 00:37:37.630
four arguments.

00:37:37.630 --> 00:37:40.220
But as soon as you allow
that, then the beauty

00:37:40.220 --> 00:37:45.210
of these operations, the
mathematical operations,

00:37:45.210 --> 00:37:45.840
collapses.

00:37:45.840 --> 00:37:47.270
And you end up in
a language which

00:37:47.270 --> 00:37:52.720
has lots of parentheses and
stuff, and can do more stuff,

00:37:52.720 --> 00:37:57.340
but isn't a tool of thought for
non-software engineer users,

00:37:57.340 --> 00:38:00.540
which is where most of the
really successfully APL users

00:38:00.540 --> 00:38:01.650
are coming from.

00:38:01.650 --> 00:38:05.715
Sort of any kind of engineer
other than a software engineer.

00:38:08.310 --> 00:38:11.780
And the Inner Product is
also carefully designed

00:38:11.780 --> 00:38:15.110
to extend to higher ranks.

00:38:15.110 --> 00:38:18.840
So here we have a
vector plus dot times.

00:38:18.840 --> 00:38:22.770
But if we have our
matrix, then we

00:38:22.770 --> 00:38:25.400
can say matrix plus
dot times in a vector.

00:38:25.400 --> 00:38:28.570
And that will do
the plus dot times,

00:38:28.570 --> 00:38:32.400
taking each vector on
the left and reusing

00:38:32.400 --> 00:38:37.100
the single vector, which
is available on the right.

00:38:37.100 --> 00:38:41.380
Mat or dot equals 7, the
7 is extended to a vector,

00:38:41.380 --> 00:38:43.190
and then reused.

00:38:43.190 --> 00:38:46.260
And we can find out if any
of the rows of the matrix

00:38:46.260 --> 00:38:49.530
contain a 7.

00:38:49.530 --> 00:38:51.500
We can make ourselves--
the first symbol

00:38:51.500 --> 00:38:53.670
there is transpose.

00:38:53.670 --> 00:38:56.810
So make a 3 by 4
matrix with those rows,

00:38:56.810 --> 00:38:58.400
and then turn them into columns.

00:38:58.400 --> 00:39:01.730
So we have a matrix
variable with 2 rows,

00:39:01.730 --> 00:39:05.130
and a coefficients
matrix with 3 columns.

00:39:05.130 --> 00:39:08.450
And then matrix multiplication
is still the same symbol,

00:39:08.450 --> 00:39:10.770
plus dot times,
because it's just

00:39:10.770 --> 00:39:12.660
cycling through the
rows on the left,

00:39:12.660 --> 00:39:16.700
and the columns on the right.

00:39:16.700 --> 00:39:17.740
Oh, three dimensions.

00:39:21.056 --> 00:39:24.032
AUDIENCE: Is that a unary minus?

00:39:24.032 --> 00:39:26.650
MORTEN KROMBERG: A unary minus.

00:39:26.650 --> 00:39:27.270
Good question.

00:39:27.270 --> 00:39:29.720
So it's the high minus.

00:39:29.720 --> 00:39:31.030
I should have mentioned that.

00:39:31.030 --> 00:39:31.529
Yes.

00:39:31.529 --> 00:39:40.090
In APL, the normal low
minus is a function.

00:39:40.090 --> 00:39:43.420
It's a prefix function that
applies to the whole array.

00:39:43.420 --> 00:39:49.110
So APL has to use a
different symbol for entering

00:39:49.110 --> 00:39:50.030
negative constants.

00:39:50.030 --> 00:39:51.700
So we use a high minus for that.

00:39:51.700 --> 00:39:53.930
Sorry, I should
have mentioned that.

00:39:53.930 --> 00:39:57.890
It's just second-nature,
and forgotten 30 years ago.

00:39:57.890 --> 00:39:58.881
Good question.

00:39:58.881 --> 00:39:59.380
Yep?

00:39:59.380 --> 00:40:01.535
RUSSELL HARMON: Just a reminder,
if people ask questions, either

00:40:01.535 --> 00:40:03.095
repeat it, or give them a mic.

00:40:03.095 --> 00:40:03.970
MORTEN KROMBERG: Yep.

00:40:03.970 --> 00:40:04.469
OK.

00:40:04.469 --> 00:40:09.140
So, I'll try to
remember that next time.

00:40:09.140 --> 00:40:11.650
So here's a cube.

00:40:11.650 --> 00:40:13.010
2 by 3 by 4.

00:40:13.010 --> 00:40:14.825
Reshape of iota 24.

00:40:19.130 --> 00:40:23.450
Now, we can do the plus
reduction of cubed.

00:40:23.450 --> 00:40:26.040
That works on the
vectors as it did before.

00:40:46.700 --> 00:40:50.280
So, we have a 2 by 3 by 4
matrix, we do a plus reduction,

00:40:50.280 --> 00:40:54.090
we lose the last dimension,
we get a 2 by 3 array

00:40:54.090 --> 00:40:58.540
back, which is the sum
of each one of the rows.

00:41:03.970 --> 00:41:06.640
We have a slash with
a bar through it,

00:41:06.640 --> 00:41:08.060
which says do the
plus reduction,

00:41:08.060 --> 00:41:10.120
but doing it on the
leading dimension.

00:41:10.120 --> 00:41:12.840
So it's going to be doing
the plus reduction down

00:41:12.840 --> 00:41:17.876
through the planes, so we get
the sum of the two planes.

00:41:17.876 --> 00:41:20.430
And of course, if
it's n dimensional,

00:41:20.430 --> 00:41:22.720
we can't keep adding
symbols for all the cases.

00:41:22.720 --> 00:41:26.050
We have a special symbol for
the first and last dimension.

00:41:26.050 --> 00:41:28.180
But you can also,
in brackets-- this

00:41:28.180 --> 00:41:31.800
is a slight anomalous
syntax-- but we

00:41:31.800 --> 00:41:34.690
can direct the reduction to go
along a particular dimension.

00:41:34.690 --> 00:41:38.190
In this case, go
down the columns.

00:41:38.190 --> 00:41:44.050
Now, the tally is the length
of the leading dimension.

00:41:44.050 --> 00:41:46.420
So that's 2 in this case.

00:41:46.420 --> 00:41:51.170
If we define our
average function

00:41:51.170 --> 00:41:54.320
as the sum along the
leading dimension divided

00:41:54.320 --> 00:41:56.560
by the tally, which is
the length of the leading

00:41:56.560 --> 00:42:00.330
dimension, it still
works with a vector,

00:42:00.330 --> 00:42:01.960
because that just
has one dimension.

00:42:01.960 --> 00:42:03.200
So that's fine.

00:42:06.520 --> 00:42:08.840
Sorry, the resolution
has changed a bit here.

00:42:11.490 --> 00:42:13.420
But the average
on the cube is now

00:42:13.420 --> 00:42:15.680
going along the first dimension.

00:42:15.680 --> 00:42:17.610
So it's summing the
planes, and dividing

00:42:17.610 --> 00:42:20.500
by the number of planes.

00:42:20.500 --> 00:42:23.400
But there's a nice interaction
with one of the newer operators

00:42:23.400 --> 00:42:25.810
that's been added
since Backus' time.

00:42:25.810 --> 00:42:27.160
There's a rank operator.

00:42:27.160 --> 00:42:29.900
So we can say, apply
the function average

00:42:29.900 --> 00:42:35.160
rank 1, which means, split the
argument array into sub-arrays

00:42:35.160 --> 00:42:35.910
of rank 1.

00:42:35.910 --> 00:42:41.340
So in this case, we would get
a 2 by 3 frame of vectors,

00:42:41.340 --> 00:42:42.640
and compute the averages.

00:42:42.640 --> 00:42:47.200
So the average function
is applied to each row.

00:42:47.200 --> 00:42:49.940
And that gives you, not
surprisingly, the average

00:42:49.940 --> 00:42:51.890
of each one of those rows.

00:42:51.890 --> 00:42:54.480
What's interesting, if you
take this function, which

00:42:54.480 --> 00:42:58.050
is written to compute the
average along the leading

00:42:58.050 --> 00:43:01.620
dimension, and you
apply it rank 2,

00:43:01.620 --> 00:43:03.420
that means you're
parsing to that

00:43:03.420 --> 00:43:06.240
function a matrix each time.

00:43:06.240 --> 00:43:08.480
So you take the
rank 2 sub-arrays,

00:43:08.480 --> 00:43:09.850
of which there are 2.

00:43:09.850 --> 00:43:12.160
Each one is 3 by 4.

00:43:12.160 --> 00:43:14.400
And you parse the
average function to that,

00:43:14.400 --> 00:43:18.230
and you can compute the
average of the columns.

00:43:18.230 --> 00:43:21.240
And then if you parse it
rank 3 that's, in this case

00:43:21.240 --> 00:43:23.830
since it's three dimensional,
that's the same as parsing

00:43:23.830 --> 00:43:26.570
the original array, and
it does the computation

00:43:26.570 --> 00:43:28.030
on the first dimension.

00:43:28.030 --> 00:43:34.040
So using this rank
conjunction, or operator,

00:43:34.040 --> 00:43:36.500
we can compute the
average along any one

00:43:36.500 --> 00:43:40.250
of the dimensions of the array,
using that same piece of code.

00:43:48.430 --> 00:43:48.930
Question?

00:43:48.930 --> 00:43:52.826
AUDIENCE: How do you go
about unshaping the cube

00:43:52.826 --> 00:43:56.452
and averaging over all the
numbers instead of [INAUDIBLE]?

00:43:56.452 --> 00:43:58.160
MORTEN KROMBERG: If
you wanted to average

00:43:58.160 --> 00:44:01.390
all the numbers,
the comma-- Sorry,

00:44:01.390 --> 00:44:04.320
the question is,
how do I average

00:44:04.320 --> 00:44:07.430
all of the numbers in
the cube without regard

00:44:07.430 --> 00:44:09.470
to the structure of the cube?

00:44:09.470 --> 00:44:11.720
What I would do then is use
a function called ravel.

00:44:11.720 --> 00:44:16.880
The comma, prefixed comma,
simply removes all structure

00:44:16.880 --> 00:44:18.620
from an array.

00:44:18.620 --> 00:44:24.157
So the average of
cube would be that.

00:44:24.157 --> 00:44:25.740
Or, of course, you
could apply average

00:44:25.740 --> 00:44:29.050
repeatedly losing-- well, yeah,
in this case that would work.

00:44:29.050 --> 00:44:31.550
But that's not a safe
technique always.

00:44:31.550 --> 00:44:32.990
OK.

00:44:32.990 --> 00:44:38.160
Now, one of the things, the
fact that the functions all

00:44:38.160 --> 00:44:42.700
map implicitly,
allows you to do is

00:44:42.700 --> 00:44:46.070
write what I would call
sort of switch-free logic.

00:44:46.070 --> 00:44:49.520
So if I have some data
here, 4 element array,

00:44:49.520 --> 00:44:53.100
and I wanted to pick
an item of the array

00:44:53.100 --> 00:44:55.320
if it was greater than 5.

00:44:55.320 --> 00:44:59.300
But if it was less than 5, I
wanted the value 5 instead.

00:44:59.300 --> 00:45:01.530
Then in a traditional
programming language,

00:45:01.530 --> 00:45:04.870
I would have written something,
if data sub I, make a loop,

00:45:04.870 --> 00:45:07.100
and then if the element
is greater than 5 keep it,

00:45:07.100 --> 00:45:09.110
otherwise return something else.

00:45:09.110 --> 00:45:13.840
But in APL, you would
just say data max 5.

00:45:13.840 --> 00:45:15.990
And element by
element, it would take

00:45:15.990 --> 00:45:20.350
the greatest five, and
the corresponding item

00:45:20.350 --> 00:45:21.070
of the array.

00:45:29.700 --> 00:45:32.680
Now, if we wanted
to, for example,

00:45:32.680 --> 00:45:35.300
increment each item
of data, depending

00:45:35.300 --> 00:45:38.670
on whether it lived
up to some condition.

00:45:38.670 --> 00:45:42.370
So we said, if the item is
an element of this list,

00:45:42.370 --> 00:45:47.200
then increment it by 1,
otherwise leave it as it is.

00:45:47.200 --> 00:45:53.220
Then in APL, you can say data
plus, data element 3, 7, 15,

00:45:53.220 --> 00:45:59.960
because data element
3, 7, 15, returns

00:45:59.960 --> 00:46:03.070
an array of the same
shape as the left argument

00:46:03.070 --> 00:46:04.820
for each item, telling
you whether it's

00:46:04.820 --> 00:46:08.170
a member of the set provided
as the right argument.

00:46:08.170 --> 00:46:12.080
And that Boolean array can just
be added to the original data.

00:46:12.080 --> 00:46:17.160
So if I wanted to add
2 in for each case,

00:46:17.160 --> 00:46:19.670
then I would say data
plus 2 times data

00:46:19.670 --> 00:46:22.510
is an element of something.

00:46:22.510 --> 00:46:25.430
I don't know how many
people here write CUDA code,

00:46:25.430 --> 00:46:27.390
or GPU code?

00:46:27.390 --> 00:46:29.420
Because I've been told
that the kind of code

00:46:29.420 --> 00:46:31.920
you need to write there, where
you have a bunch of processes

00:46:31.920 --> 00:46:35.720
executing in lockstep, is
very similar to writing code

00:46:35.720 --> 00:46:36.220
like this.

00:46:41.980 --> 00:46:43.320
And you can also merge.

00:46:43.320 --> 00:46:46.090
So I've got two arrays
here, x and y. x is 1,

00:46:46.090 --> 00:46:49.870
2, 3, 4; y is 10, 20, 30, 40.

00:46:49.870 --> 00:46:55.680
And I want to pick x
if the flag is true,

00:46:55.680 --> 00:46:58.620
or I want to pick the
corresponding element of y

00:46:58.620 --> 00:47:01.530
if it's false.

00:47:01.530 --> 00:47:06.470
Then I would say x times
flags plus y times not flags.

00:47:06.470 --> 00:47:07.670
And I can implement again.

00:47:07.670 --> 00:47:10.520
I can do a lot of
parallel processing

00:47:10.520 --> 00:47:11.650
without writing loops.

00:47:16.780 --> 00:47:19.470
So, you might say, well, if this
stuff is so wonderful, where

00:47:19.470 --> 00:47:20.890
have we all been?

00:47:20.890 --> 00:47:23.244
How many people here
had heard of APL?

00:47:23.244 --> 00:47:25.000
Oh, you had.

00:47:25.000 --> 00:47:25.500
OK.

00:47:25.500 --> 00:47:27.999
So, I should have asked at the
beginning what you had heard.

00:47:30.410 --> 00:47:32.190
But one of the
reasons you generally,

00:47:32.190 --> 00:47:35.320
at least as a software engineer,
hear very little about APL

00:47:35.320 --> 00:47:38.030
is that the people, the
successful APL users,

00:47:38.030 --> 00:47:41.584
I would claim, are people who
are any-- as I said before,

00:47:41.584 --> 00:47:43.750
any kind of engineer other
than a software engineer.

00:47:43.750 --> 00:47:45.840
So they don't really
feel comfortable

00:47:45.840 --> 00:47:47.350
discussing programming.

00:47:47.350 --> 00:47:49.910
They're solving some kind
of actuarial problem,

00:47:49.910 --> 00:47:51.870
or chemical engineering problem.

00:47:51.870 --> 00:47:53.990
They don't go to
software conferences.

00:47:53.990 --> 00:47:55.880
They absolutely
hate it when they

00:47:55.880 --> 00:47:58.730
have to change the way
that they do things.

00:47:58.730 --> 00:48:01.820
So when the PC came
along, and they

00:48:01.820 --> 00:48:03.700
were using APL on the
mainframe, they said,

00:48:03.700 --> 00:48:05.240
what is the advantage of this?

00:48:05.240 --> 00:48:06.600
I have to take my own backups.

00:48:06.600 --> 00:48:09.840
If it breaks, it's
my own problem.

00:48:09.840 --> 00:48:13.170
And a lot of the usage of APL
simply died with the mainframe,

00:48:13.170 --> 00:48:16.160
because the people didn't
realize the technology were

00:48:16.160 --> 00:48:19.600
changing under foot.

00:48:19.600 --> 00:48:23.360
And then we came into
this really dark period.

00:48:23.360 --> 00:48:25.960
I think even for people who
were software engineers,

00:48:25.960 --> 00:48:30.300
this was a very difficult time,
where the APIs were changing.

00:48:30.300 --> 00:48:33.140
And where APL programmers had
been able to write actually

00:48:33.140 --> 00:48:35.810
pretty nice user interfaces,
and so on, on the mainframe,

00:48:35.810 --> 00:48:40.210
they suddenly had to try and
adapt to all these software

00:48:40.210 --> 00:48:42.340
engineering paradigms that
meant nothing to them.

00:48:42.340 --> 00:48:48.350
And they really struggled
a lot in that period.

00:48:48.350 --> 00:48:50.870
Fortunately, I would say, this
seems to have been temporary.

00:48:50.870 --> 00:48:53.100
The new APIs that
are coming out now,

00:48:53.100 --> 00:48:56.240
the more object-oriented
frameworks like the .NET

00:48:56.240 --> 00:49:01.530
framework, the Java frameworks,
self-describing systems that

00:49:01.530 --> 00:49:05.530
can be used in very much the
same way as APL programmers are

00:49:05.530 --> 00:49:06.060
used to.

00:49:06.060 --> 00:49:07.320
They've got reflection.

00:49:07.320 --> 00:49:11.290
They can type expressions,
and experiment, and so on.

00:49:11.290 --> 00:49:14.940
It looks like we can start
providing APL users again

00:49:14.940 --> 00:49:17.320
with tools for building
user interfaces,

00:49:17.320 --> 00:49:19.610
web-based interfaces,
for example,

00:49:19.610 --> 00:49:21.710
that they will be able to use.

00:49:21.710 --> 00:49:23.950
And of course, the
focus in the rest

00:49:23.950 --> 00:49:27.160
of the community on
functional programming,

00:49:27.160 --> 00:49:32.300
and the need to take use
of parallel hardware, which

00:49:32.300 --> 00:49:34.380
is appearing everywhere,
is giving us,

00:49:34.380 --> 00:49:37.330
I think, some
courage to reappear.

00:49:37.330 --> 00:49:39.370
Functional programming
took about 80 years

00:49:39.370 --> 00:49:43.450
to catch on from the first
ideas were mentioned.

00:49:43.450 --> 00:49:45.660
Object-oriented programming
probably about 20

00:49:45.660 --> 00:49:51.460
from simular until Xerox started
using it really with Smalltalk

00:49:51.460 --> 00:49:52.530
and so on.

00:49:52.530 --> 00:49:55.910
Arrays are now
about 50 years old.

00:49:55.910 --> 00:49:59.420
If you want to successfully
build a group to use APL,

00:49:59.420 --> 00:50:04.210
I would say you need the
right mix of domain experts

00:50:04.210 --> 00:50:05.530
and software engineers.

00:50:05.530 --> 00:50:07.510
You can't just do with
the domain experts,

00:50:07.510 --> 00:50:09.850
because they'll build
things that will break.

00:50:09.850 --> 00:50:11.650
But you need to allow
them the freedom

00:50:11.650 --> 00:50:15.525
to experiment with
writing applications.

00:50:19.890 --> 00:50:20.720
Be pragmatic.

00:50:20.720 --> 00:50:24.970
We encourage a functional
style of programming,

00:50:24.970 --> 00:50:26.561
but we don't demand it.

00:50:26.561 --> 00:50:28.310
And what we're starting
to think, I think,

00:50:28.310 --> 00:50:30.870
is that languages like
APL-- maybe not APL

00:50:30.870 --> 00:50:32.710
itself, maybe
something that Google

00:50:32.710 --> 00:50:36.100
will invent that will
embed some of these ideas--

00:50:36.100 --> 00:50:38.410
will be the solution
to the next big problem

00:50:38.410 --> 00:50:42.220
after concurrency, which
is simply complexity.

00:50:42.220 --> 00:50:43.940
You need much more
powerful tools

00:50:43.940 --> 00:50:46.230
where the domain experts
can bring their knowledge

00:50:46.230 --> 00:50:48.550
to bear directly on the data.

00:50:48.550 --> 00:50:50.910
Because if they
have to explain it,

00:50:50.910 --> 00:50:54.760
even using agile
techniques to somebody,

00:50:54.760 --> 00:50:55.970
it's already too late.

00:51:00.600 --> 00:51:03.950
There are some customers,
or some organizations,

00:51:03.950 --> 00:51:06.244
who are happy to talk about
the fact that they use APL.

00:51:06.244 --> 00:51:07.660
Typically, they
are either sort of

00:51:07.660 --> 00:51:10.610
slightly embarrassed by
using this really left-field

00:51:10.610 --> 00:51:11.900
technology.

00:51:11.900 --> 00:51:14.482
Or they consider it
to be a secret weapon

00:51:14.482 --> 00:51:15.940
that they don't
want to talk about,

00:51:15.940 --> 00:51:19.730
because it increases
their productivity so much

00:51:19.730 --> 00:51:23.180
they'd rather not talk
about it for that reason.

00:51:23.180 --> 00:51:26.920
But there are quite
a few people using

00:51:26.920 --> 00:51:30.320
it to build some very
significant applications.

00:51:30.320 --> 00:51:35.590
So, I mean, it's been 50
years, 52 years, I think.

00:51:35.590 --> 00:51:38.650
A lot of the early criticism of
APL, I think, was quite valid.

00:51:38.650 --> 00:51:42.050
And we've been working hard
on addressing a lot of it.

00:51:42.050 --> 00:51:45.265
Today, our elevator
pitch is this text

00:51:45.265 --> 00:51:47.900
that's up on the screen now.

00:51:47.900 --> 00:51:51.930
We would say, we are an
array-first, multi-paradigm

00:51:51.930 --> 00:51:52.930
programming language.

00:51:52.930 --> 00:51:56.530
So we also support
functional styles.

00:51:56.530 --> 00:51:57.930
We have object orientation.

00:51:57.930 --> 00:52:00.570
You can create
classes that can be

00:52:00.570 --> 00:52:06.040
used in external object-oriented
frameworks, et cetera.

00:52:06.040 --> 00:52:08.930
But it's still very much based
on an APL language kernel.

00:52:12.090 --> 00:52:16.196
I do need to spend one
minute on this one.

00:52:16.196 --> 00:52:18.070
So you might think, if
you were using Scheme,

00:52:18.070 --> 00:52:23.580
or some other similar language,
that these two expressions

00:52:23.580 --> 00:52:24.880
are logically equivalent.

00:52:24.880 --> 00:52:25.060
Right?

00:52:25.060 --> 00:52:27.270
In fact, the first one's
better, because it consists

00:52:27.270 --> 00:52:29.760
of English language statements
that people can easily

00:52:29.760 --> 00:52:32.350
relate to, whereas
the one at the bottom

00:52:32.350 --> 00:52:37.850
is a pretty weird-looking
mathematical statement.

00:52:37.850 --> 00:52:40.009
But once you get
used to using APL,

00:52:40.009 --> 00:52:42.300
and you look at that second
statement there, you think,

00:52:42.300 --> 00:52:43.110
hang on.

00:52:43.110 --> 00:52:51.620
But that's just the plus
reduce iota each of iota 6.

00:52:51.620 --> 00:52:55.330
So, I can blend
those two together.

00:52:55.330 --> 00:52:58.500
And of course, the plus
reduce of the first n

00:52:58.500 --> 00:53:03.420
integers is really just
the argument plus times 1

00:53:03.420 --> 00:53:07.780
plus the argument
divided by 2, applied

00:53:07.780 --> 00:53:10.067
to each one of those functions.

00:53:10.067 --> 00:53:12.150
And when you've looked at
that for a little while,

00:53:12.150 --> 00:53:14.992
you realize that that's actually
just the plus scan of iota,

00:53:14.992 --> 00:53:18.250
of the first 6.

00:53:18.250 --> 00:53:22.430
But sometimes you need to be
experimenting interactively

00:53:22.430 --> 00:53:24.900
with these things.

00:53:24.900 --> 00:53:31.000
Before it's easy to see these.

00:53:31.000 --> 00:53:33.770
But our experience is,
at least the people

00:53:33.770 --> 00:53:35.350
who do get used to
this, it's really

00:53:35.350 --> 00:53:39.750
worthwhile learning
the symbols, and being

00:53:39.750 --> 00:53:44.110
able to apply what you
learned in school doing

00:53:44.110 --> 00:53:47.615
math, physics, whatever,
directly in your programming.

00:53:50.850 --> 00:53:52.350
Well, since Arthur
is here, I really

00:53:52.350 --> 00:53:54.183
want to show you this
one also, because this

00:53:54.183 --> 00:53:56.520
is one of my favorite quotes.

00:53:56.520 --> 00:54:00.270
By Arthur Whitney here, the
inventor of the K language.

00:54:00.270 --> 00:54:01.740
I think this is very, very true.

00:54:01.740 --> 00:54:05.095
A very, very true statement.

00:54:05.095 --> 00:54:08.950
And I think I'll leave it
there, and take some questions.

00:54:08.950 --> 00:54:09.450
Yes?

00:54:09.450 --> 00:54:12.102
AUDIENCE: What is 0.1 plus 0.2?

00:54:12.102 --> 00:54:14.310
MORTEN KROMBERG: 0.1 plus 0.2.

00:54:14.310 --> 00:54:20.251
The question is,
what is 0.1 plus 0.2.

00:54:20.251 --> 00:54:20.750
It's 0.3.

00:54:20.750 --> 00:54:23.008
AUDIENCE: Are those doubles?

00:54:23.008 --> 00:54:29.250
MORTEN KROMBERG: Well, APL
really only recognizes numbers.

00:54:29.250 --> 00:54:32.610
And it's doing everything it can
to hide the fact that it might

00:54:32.610 --> 00:54:34.995
be using a variety of different
internal representations

00:54:34.995 --> 00:54:36.670
from you.

00:54:36.670 --> 00:54:40.600
So if you compute something,
and it collapses to all zeros,

00:54:40.600 --> 00:54:42.650
then you end up with one
of these Boolean arrays,

00:54:42.650 --> 00:54:44.930
a bit per element.

00:54:44.930 --> 00:54:46.630
And in fact, the
interpreter spends

00:54:46.630 --> 00:54:49.050
quite a lot of time trying
to collapse data types.

00:54:49.050 --> 00:54:50.980
As it runs, it will
occasionally see

00:54:50.980 --> 00:54:54.270
whether it can collapse
down to a smaller data type.

00:54:54.270 --> 00:54:57.240
But the illusion that
we're trying to preserve

00:54:57.240 --> 00:55:00.670
is that as a mathematical
notation, we just have numbers.

00:55:00.670 --> 00:55:02.710
We don't really
recognize numeric types.

00:55:09.458 --> 00:55:17.520
AUDIENCE: So, this is a
really, really nice notation

00:55:17.520 --> 00:55:19.020
for linear algebra,
and stuff that's

00:55:19.020 --> 00:55:20.895
sort of isomorphic
to linear algebra.

00:55:20.895 --> 00:55:23.670
Could we expand this onto
more abstract algebra?

00:55:23.670 --> 00:55:26.439
Like could I do this computation
over like a monoid, or a ring?

00:55:26.439 --> 00:55:28.480
Because that would be
really nice for my software

00:55:28.480 --> 00:55:29.260
engineering.

00:55:29.260 --> 00:55:31.185
Or has anybody thought
about this stuff?

00:55:31.185 --> 00:55:34.611
MORTEN KROMBERG:
Sorry, a monoid?

00:55:34.611 --> 00:55:37.270
AUDIENCE: Operator of a
set with a closed operator

00:55:37.270 --> 00:55:39.630
that's associated
and has an identity.

00:55:39.630 --> 00:55:46.070
So like lists, and strings,
and max, and integers.

00:55:46.070 --> 00:55:49.460
MORTEN KROMBERG:
No, I mean, I think

00:55:49.460 --> 00:55:52.720
people did look
at that early on,

00:55:52.720 --> 00:55:55.340
people who also were using
other programming languages.

00:55:55.340 --> 00:55:57.880
Today, their usage
is really focused

00:55:57.880 --> 00:56:01.860
on numerical applications,
and data mining applications,

00:56:01.860 --> 00:56:05.900
and so on, where you're doing
a lot of number crunching.

00:56:05.900 --> 00:56:07.900
So, I don't have a good
answer to that question.

00:56:14.101 --> 00:56:16.590
AUDIENCE: You said
you're mainly focused

00:56:16.590 --> 00:56:19.720
on numeric applications,
but I'd still

00:56:19.720 --> 00:56:25.300
like to know, how do you
deal with promotion, or when

00:56:25.300 --> 00:56:27.060
an overflow occurs?

00:56:27.060 --> 00:56:30.290
And in this case, you're
probably not using

00:56:30.290 --> 00:56:36.200
a double, or i3 floating point.

00:56:36.200 --> 00:56:41.970
So, the question is, when do you
use big [? data ?] small end,

00:56:41.970 --> 00:56:45.210
fixed precision stuff, and
floating precision stuff.

00:56:45.210 --> 00:56:47.422
How do you choose that?

00:56:47.422 --> 00:56:53.410
MORTEN KROMBERG: If you want
to go to decimal, higher,

00:56:53.410 --> 00:56:57.100
we also support quadruple
precision decimal,

00:56:57.100 --> 00:56:59.360
the IEEE format.

00:56:59.360 --> 00:57:01.390
But that is a conscious
choice by the user.

00:57:01.390 --> 00:57:04.440
They have to say, I want to have
much higher precision decimals.

00:57:04.440 --> 00:57:07.580
Because those have a very
significant cost associated

00:57:07.580 --> 00:57:08.470
with them.

00:57:08.470 --> 00:57:12.896
But normally we go through
1 bit, 8 bit integers, 16,

00:57:12.896 --> 00:57:14.830
32-bit integers.

00:57:14.830 --> 00:57:17.500
We'll blow up into
doubles, and we also

00:57:17.500 --> 00:57:21.440
consider complex numbers
to be in that continuum

00:57:21.440 --> 00:57:24.186
of numeric representations.

00:57:24.186 --> 00:57:26.060
And that's all done
completely automatically.

00:57:29.310 --> 00:57:32.960
You can influence it, but
generally that's frowned upon.

00:57:32.960 --> 00:57:34.850
You might do that
if you need to cast

00:57:34.850 --> 00:57:37.402
a type to parse it
to some C program,

00:57:37.402 --> 00:57:38.360
or something like that.

00:57:38.360 --> 00:57:40.630
But generally in an APL
program, you wouldn't do that.

00:57:44.244 --> 00:57:45.910
RUSSELL HARMON: I've
got a few questions

00:57:45.910 --> 00:57:49.050
from our remote offices.

00:57:49.050 --> 00:57:52.750
So, has APL been used on
large data sets that require

00:57:52.750 --> 00:57:56.102
techniques such as sharding?

00:57:56.102 --> 00:57:59.340
MORTEN KROMBERG:
Yes, it certainly

00:57:59.340 --> 00:58:02.900
has by some of those customers.

00:58:02.900 --> 00:58:05.210
But it's not something
that we provide support

00:58:05.210 --> 00:58:08.150
for at the moment
natively in the language,

00:58:08.150 --> 00:58:15.050
other than that you can map
an array to a mapped file.

00:58:15.050 --> 00:58:17.520
But at the language level,
we don't provide tools

00:58:17.520 --> 00:58:19.805
to deal directly with sharding.

00:58:27.462 --> 00:58:30.075
AUDIENCE: Do you have any
crazy one-liners to show?

00:58:32.340 --> 00:58:33.840
MORTEN KROMBERG:
Well, I mean, there

00:58:33.840 --> 00:58:40.580
was the one that Russ sent
out in the invitation.

00:58:40.580 --> 00:58:43.225
I don't know how crazy it is.

00:58:43.225 --> 00:58:47.040
Let's see, can we find it.

00:58:47.040 --> 00:58:50.340
Write this expression
for finding

00:58:50.340 --> 00:58:54.360
all the primes between 1 and r.

00:58:54.360 --> 00:58:57.370
Which is one of
those things that-- I

00:58:57.370 --> 00:59:00.140
don't know where
this image came from,

00:59:00.140 --> 00:59:02.440
but it's sort of an
example of really bad APL

00:59:02.440 --> 00:59:05.310
code from the early
days, because it

00:59:05.310 --> 00:59:09.150
modifies the right argument.

00:59:09.150 --> 00:59:12.330
The first thing you do in
this expression is you take r,

00:59:12.330 --> 00:59:16.540
and you reassign it to
the one drop of something.

00:59:16.540 --> 00:59:19.970
And today, you would
want to write it

00:59:19.970 --> 00:59:22.350
as one of these lexically
scoped things, where

00:59:22.350 --> 00:59:27.470
that temporary array you
needed was a local variable.

00:59:27.470 --> 00:59:31.200
But do you want an
explanation of how that works?

00:59:31.200 --> 00:59:32.780
I mean, it's quite simple, so.

00:59:32.780 --> 00:59:35.250
No?

00:59:35.250 --> 00:59:37.130
Basically it's doing
an Outer Product.

00:59:37.130 --> 00:59:39.070
All the numbers
between 1 and r, it's

00:59:39.070 --> 00:59:41.790
multiplying them, doing
the Outer Product,

00:59:41.790 --> 00:59:43.830
doing all the multiplications.

00:59:43.830 --> 00:59:45.430
And then seeing
where the number is

00:59:45.430 --> 00:59:49.620
an element of all of those
products, of everything.

00:59:49.620 --> 00:59:53.440
So it, I mean it's a very direct
expression of what it means

00:59:53.440 --> 00:59:55.340
for something to be prime.

00:59:55.340 --> 00:59:58.690
But of course, it's
hideously inefficient.

00:59:58.690 --> 01:00:03.410
Although it's so
short that you could,

01:00:03.410 --> 01:00:05.700
at the interpreter level,
you could recognize

01:00:05.700 --> 01:00:07.670
that as an idiom and
say, well, the guy's

01:00:07.670 --> 01:00:12.300
obviously trying to
compute the prime numbers.

01:00:12.300 --> 01:00:15.790
And that's one of the
advantages of this notation.

01:00:15.790 --> 01:00:19.040
If you think about
that example in Scheme

01:00:19.040 --> 01:00:21.820
with the plus
reduction, and so on,

01:00:21.820 --> 01:00:23.620
I think the classical
thinking today

01:00:23.620 --> 01:00:26.740
is, you write that
kind of expression,

01:00:26.740 --> 01:00:28.965
and then some very clever
optimizing compiler

01:00:28.965 --> 01:00:31.170
is going to detect the
loops within loops.

01:00:31.170 --> 01:00:34.250
And it's going to fuse
them, and get rid of them.

01:00:34.250 --> 01:00:41.190
Whereas in APL, you have these
much higher-level constructions

01:00:41.190 --> 01:00:43.720
that an even an
interpreter can optimize.

01:00:43.720 --> 01:00:45.430
You don't actually
need-- I mean,

01:00:45.430 --> 01:00:47.410
we also want to do
loop fusion and stuff,

01:00:47.410 --> 01:00:49.740
and we're working on
writing a compiler.

01:00:49.740 --> 01:00:53.490
But it's much easier to
write highly optimized idiom

01:00:53.490 --> 01:00:58.584
recognition in APL than I
think it is in most languages.

01:00:58.584 --> 01:01:00.750
RUSSELL HARMON: Another
question from a road office.

01:01:00.750 --> 01:01:02.819
I thought something like
APL in its environment

01:01:02.819 --> 01:01:04.610
is what spreadsheets
should have been like.

01:01:04.610 --> 01:01:05.892
Comments?

01:01:05.892 --> 01:01:07.520
MORTEN KROMBERG:
What spreadsheets

01:01:07.520 --> 01:01:08.620
should have been like.

01:01:11.880 --> 01:01:14.770
Yeah, I mean, I think a lot
of the earliest spreadsheets

01:01:14.770 --> 01:01:16.960
on mainframes were
actually implemented

01:01:16.960 --> 01:01:19.015
in APL by APL users.

01:01:21.540 --> 01:01:24.330
But spreadsheets provide
much more than APL does.

01:01:24.330 --> 01:01:26.910
APL is a programming language,
whereas the spreadsheets

01:01:26.910 --> 01:01:30.510
have all the formatting
and very simple

01:01:30.510 --> 01:01:34.790
ways of storing data
built into this package.

01:01:34.790 --> 01:01:36.610
It's sort of a
mash-up kind of thing.

01:01:39.200 --> 01:01:41.170
I think that quite
a few applications

01:01:41.170 --> 01:01:43.580
where people would prototype
them in a spreadsheet,

01:01:43.580 --> 01:01:45.690
and then rewrite them
in APL, because they

01:01:45.690 --> 01:01:47.330
needed more structure.

01:01:47.330 --> 01:01:49.560
Spreadsheets are incredibly
dangerous things,

01:01:49.560 --> 01:01:51.670
because you can
over-type an expression,

01:01:51.670 --> 01:01:54.210
and you realize 10 years later
that you haven't actually

01:01:54.210 --> 01:01:57.080
been recomputing that value.

01:01:57.080 --> 01:02:01.240
Whereas APL gives you a
lot of the same features

01:02:01.240 --> 01:02:03.230
as a spreadsheet,
but it does require

01:02:03.230 --> 01:02:05.290
you to be a bit
more disciplined,

01:02:05.290 --> 01:02:08.490
and do a bit more work.

01:02:08.490 --> 01:02:10.422
Yeah?

01:02:10.422 --> 01:02:12.372
AUDIENCE: I have two questions.

01:02:12.372 --> 01:02:14.580
I'll let you decide which
of them you want to answer,

01:02:14.580 --> 01:02:15.261
or both.

01:02:15.261 --> 01:02:17.510
I was wondering if you could
say something, especially

01:02:17.510 --> 01:02:20.060
perhaps since Arthur is
here, about the relationship

01:02:20.060 --> 01:02:23.620
between APL and J and
K, and when one would

01:02:23.620 --> 01:02:25.430
look into one or the other.

01:02:25.430 --> 01:02:28.220
Why would I learn
APL versus J or K?

01:02:28.220 --> 01:02:29.800
Different question
which is similar

01:02:29.800 --> 01:02:32.860
is, for people who are less
familiar with these array-based

01:02:32.860 --> 01:02:35.150
languages, why would I
look at this these days

01:02:35.150 --> 01:02:39.698
versus something like R,
or MATLAB, or Mathematica?

01:02:39.698 --> 01:02:42.520
MORTEN KROMBERG: Maybe
do the last one first.

01:02:42.520 --> 01:02:47.530
Why APL rather than R, or
Mathematica, or MATLAB?

01:02:47.530 --> 01:02:51.750
I mean, those tools have all
been extremely successful,

01:02:51.750 --> 01:02:55.170
but I think not due to the
beauty of the programming

01:02:55.170 --> 01:02:55.850
language.

01:02:55.850 --> 01:02:57.320
It's more that
they have collected

01:02:57.320 --> 01:03:00.590
extremely good
libraries, and they've

01:03:00.590 --> 01:03:03.570
managed to present them in a
way where people-- engineers who

01:03:03.570 --> 01:03:05.810
needed to solve a problem
very, very quickly

01:03:05.810 --> 01:03:07.590
found the things they needed.

01:03:07.590 --> 01:03:09.960
For whatever application
they were doing,

01:03:09.960 --> 01:03:14.570
you could solve 95% of what you
needed with standard libraries.

01:03:14.570 --> 01:03:17.710
But when you need to go beyond
what the standard libraries can

01:03:17.710 --> 01:03:23.640
do, that's where I think
APL, the users that we

01:03:23.640 --> 01:03:30.790
have are typically people who
are operating outside the 90%.

01:03:30.790 --> 01:03:34.320
They have new ideas about
calculations that aren't

01:03:34.320 --> 01:03:36.640
available in those packages.

01:03:36.640 --> 01:03:40.300
I think APL could be a really
good future implementation

01:03:40.300 --> 01:03:43.620
language for many
of those packages.

01:03:43.620 --> 01:03:45.250
I sort of understand
why they didn't

01:03:45.250 --> 01:03:48.130
use APL when they started
those products back

01:03:48.130 --> 01:03:50.600
in the '90s, when the
APL interpreters really

01:03:50.600 --> 01:03:52.570
weren't up to it.

01:03:52.570 --> 01:03:54.604
Today, I think it
might be different

01:03:54.604 --> 01:03:55.770
if they were starting again.

01:03:55.770 --> 01:04:00.960
A lot of those people
did come out of APL.

01:04:00.960 --> 01:04:09.490
On the question of when would
you use K versus J versus APL,

01:04:09.490 --> 01:04:12.706
I mean, that's very
difficult. Clearly, K,

01:04:12.706 --> 01:04:14.080
if you're doing
anything that has

01:04:14.080 --> 01:04:21.790
to do with really large volumes
of data, and real-time trading,

01:04:21.790 --> 01:04:25.920
they would run rings
around an APL interpreter.

01:04:25.920 --> 01:04:28.900
I think APL products,
certainly like ours,

01:04:28.900 --> 01:04:34.350
have much more capability in
terms of interfaces to .NET.

01:04:34.350 --> 01:04:40.950
We allow you to create classes
that can be consumed directly

01:04:40.950 --> 01:04:43.100
by C#.

01:04:43.100 --> 01:04:44.970
We have interfaces
to databases, and are

01:04:44.970 --> 01:04:49.130
much more of a general
purpose programming language.

01:04:49.130 --> 01:04:50.890
I think better
debugging tools, I

01:04:50.890 --> 01:04:54.380
would say, at least
at the moment.

01:04:54.380 --> 01:04:57.170
Same is probably
true for J. With J,

01:04:57.170 --> 01:04:59.537
I would say, we're actually
at the moment in the process

01:04:59.537 --> 01:05:01.870
of adopting a lot of the
language features that were put

01:05:01.870 --> 01:05:07.440
into J. J is sort of like an
increased instruction set APL

01:05:07.440 --> 01:05:09.820
system, whereas I think--
Arthur, correct me

01:05:09.820 --> 01:05:14.660
if I'm wrong-- I would say K
is really a risk APL system.

01:05:14.660 --> 01:05:15.800
Right?

01:05:15.800 --> 01:05:18.920
We only need this, and throw
out the stuff that is sort

01:05:18.920 --> 01:05:21.540
of "bloat," and
make it run really,

01:05:21.540 --> 01:05:24.200
really fast on humongous data.

01:05:24.200 --> 01:05:25.050
Is that fair?

01:05:25.050 --> 01:05:26.030
ARTHUR WHITNEY: Yep.

01:05:26.030 --> 01:05:28.090
MORTEN KROMBERG: Yeah.

01:05:28.090 --> 01:05:30.640
We're sort of the
general purpose tool.

01:05:30.640 --> 01:05:32.890
J is very focused on teaching.

01:05:32.890 --> 01:05:36.020
So they have fantastic
mathematical labs,

01:05:36.020 --> 01:05:39.240
and examples of how to do math.

01:05:39.240 --> 01:05:40.380
Yes, Russ?

01:05:40.380 --> 01:05:43.751
RUSSELL HARMON: What
community-contributed libraries

01:05:43.751 --> 01:05:44.250
are there?

01:05:49.574 --> 01:05:50.990
MORTEN KROMBERG:
That's really one

01:05:50.990 --> 01:05:55.160
of the weak points of the
language as it stands today,

01:05:55.160 --> 01:05:58.020
and one of the things that we
see as our biggest challenge

01:05:58.020 --> 01:06:00.000
for the next few years.

01:06:00.000 --> 01:06:02.400
We've been building-- and
I'm talking about Dyalog

01:06:02.400 --> 01:06:05.140
specifically, we've
been building frameworks

01:06:05.140 --> 01:06:07.770
to deal with source
code in Unicode files

01:06:07.770 --> 01:06:11.880
rather than the binary
formats that APL has typically

01:06:11.880 --> 01:06:13.670
used in the past.

01:06:13.670 --> 01:06:18.230
And we are really trying to
construct an environment where

01:06:18.230 --> 01:06:22.240
we will see much more user
participation in terms

01:06:22.240 --> 01:06:24.520
of building libraries than
we've seen in the past.

01:06:24.520 --> 01:06:26.540
Typically, APL
vendors have provided

01:06:26.540 --> 01:06:31.710
nearly all the libraries in the
past, and that's a weakness.

01:06:31.710 --> 01:06:34.020
And it has a lot to do with
what I mentioned before,

01:06:34.020 --> 01:06:37.250
that the APL users
aren't really motivated.

01:06:37.250 --> 01:06:38.570
They're not software engineers.

01:06:38.570 --> 01:06:40.690
They're trying to
price a new insurance

01:06:40.690 --> 01:06:43.687
product to sell next week.

01:06:43.687 --> 01:06:45.270
And when they've
done that, their boss

01:06:45.270 --> 01:06:47.620
is going to give them
another problem to solve.

01:06:47.620 --> 01:06:51.340
And they're not really-- Python
and these languages which

01:06:51.340 --> 01:06:54.580
have some similarity
to APL have, I think,

01:06:54.580 --> 01:07:01.040
a larger traditional
software engineering

01:07:01.040 --> 01:07:02.860
part of the
community, which means

01:07:02.860 --> 01:07:07.182
they've been better at
building that kind of thing.

01:07:07.182 --> 01:07:10.000
AUDIENCE: All the
symbols are in Unicode?

01:07:10.000 --> 01:07:12.250
MORTEN KROMBERG: All the
symbols are in Unicode, yeah.

01:07:12.250 --> 01:07:13.300
And have been.

01:07:13.300 --> 01:07:16.980
I mean, they were
in Unicode 1.0.

01:07:16.980 --> 01:07:17.480
Yep.

01:07:17.480 --> 01:07:21.240
So, I mean, Unicode has
been so good for us.

01:07:21.240 --> 01:07:27.600
We just use sub version, or
whatever the latest thing is.

01:07:27.600 --> 01:07:32.460
We are moving to Git,
like everybody else.

01:07:32.460 --> 01:07:34.000
And it's just Unicode.

01:07:34.000 --> 01:07:36.811
I mean, all the diff tools out
there just work with APL now.

01:07:36.811 --> 01:07:38.060
You don't have to do anything.

01:07:38.060 --> 01:07:39.260
Everything just works.

01:07:39.260 --> 01:07:40.080
Yes?

01:07:40.080 --> 01:07:41.900
OK, I'm not sure.

01:07:41.900 --> 01:07:42.580
Yeah, go ahead.

01:07:42.580 --> 01:07:46.420
AUDIENCE: So about
the user libraries,

01:07:46.420 --> 01:07:49.200
user-generated libraries,
or user contribution

01:07:49.200 --> 01:07:51.450
to the language.

01:07:51.450 --> 01:07:53.790
You said you guys have been--

01:07:53.790 --> 01:07:55.620
MORTEN KROMBERG: Oh, yeah.

01:07:55.620 --> 01:07:59.850
We're working on tools to
make APL development much more

01:07:59.850 --> 01:08:01.530
similar to traditional
development.

01:08:01.530 --> 01:08:04.000
So using source code
management systems,

01:08:04.000 --> 01:08:07.900
dependency management,
tools, build tools,

01:08:07.900 --> 01:08:09.270
that everybody else is using.

01:08:09.270 --> 01:08:12.810
Whereas in the past, APL
programmers have tended to--

01:08:12.810 --> 01:08:15.370
or the large APL shops would
all build their own source code

01:08:15.370 --> 01:08:18.310
management tools,
where the data was

01:08:18.310 --> 01:08:23.380
stored in like spreadsheets,
binary, proprietary formats

01:08:23.380 --> 01:08:25.216
from the various vendors.

01:08:25.216 --> 01:08:26.340
But that's really changing.

01:08:26.340 --> 01:08:29.319
And we can see the new
people coming into APL today,

01:08:29.319 --> 01:08:32.350
compared to the
guys who started--

01:08:32.350 --> 01:08:34.229
There was this big wave
of people who started

01:08:34.229 --> 01:08:36.399
using APL in the '70s and '80s.

01:08:36.399 --> 01:08:40.120
And then as the market faded
when the mainframe died,

01:08:40.120 --> 01:08:43.470
those people carried on
being the main contributors.

01:08:43.470 --> 01:08:46.300
And they really didn't have
a modern way of approaching

01:08:46.300 --> 01:08:47.590
software development.

01:08:47.590 --> 01:08:50.720
We can now see we have
a second wave of people

01:08:50.720 --> 01:08:53.500
coming into the language, who
have a very different approach.

01:08:53.500 --> 01:08:55.950
And it will probably change
things quite dramatically

01:08:55.950 --> 01:08:58.881
over the next five
years, how APL is used.

01:08:58.881 --> 01:09:00.880
AUDIENCE: So the other
language that I've used--

01:09:00.880 --> 01:09:02.380
or the only language
which I've used

01:09:02.380 --> 01:09:07.229
that's really significantly
Unicode-y in its operators is

01:09:07.229 --> 01:09:08.700
Agda.

01:09:08.700 --> 01:09:11.260
And it has this really,
really lovely Emacs mode,

01:09:11.260 --> 01:09:13.779
where you can type right
arrow, and then it'll

01:09:13.779 --> 01:09:16.080
insert a little
right arrow for you.

01:09:16.080 --> 01:09:20.540
Is there something like
that, so I could write APL

01:09:20.540 --> 01:09:23.450
in like a text editor, or a
terminal, or something that's

01:09:23.450 --> 01:09:26.185
not an APL IDE?

01:09:26.185 --> 01:09:29.370
MORTEN KROMBERG: There are at
least a couple of Emacs modes

01:09:29.370 --> 01:09:34.080
out there if you
search for them.

01:09:34.080 --> 01:09:37.670
Or send me an email, and I
will give you links to people.

01:09:37.670 --> 01:09:40.649
We are in the process of-- we
just released this Mac version

01:09:40.649 --> 01:09:41.430
last week.

01:09:41.430 --> 01:09:45.100
And that's using a
completely new IDE.

01:09:45.100 --> 01:09:47.240
We've had this Windows
product for a decade, where

01:09:47.240 --> 01:09:48.531
nearly all the users have been.

01:09:48.531 --> 01:09:51.319
We're sort of very much
a Win 32 application.

01:09:51.319 --> 01:09:53.569
But we've now separated
the interpreter

01:09:53.569 --> 01:09:56.250
from these data
streams, and we're

01:09:56.250 --> 01:09:58.420
in the process of
publishing them.

01:09:58.420 --> 01:09:59.940
And it'll be
possible for anybody

01:09:59.940 --> 01:10:03.720
to write a front end to APL
in about another year, when we

01:10:03.720 --> 01:10:07.890
finally finalize that format.

01:10:07.890 --> 01:10:10.540
And then it'll become very
easy to write plug-ins

01:10:10.540 --> 01:10:13.440
for Emacs, Eclipse,
whatever you want.

01:10:16.680 --> 01:10:17.300
Yeah.

01:10:17.300 --> 01:10:18.920
But we do already
have some people

01:10:18.920 --> 01:10:21.210
who have built Emacs modes.

01:10:21.210 --> 01:10:21.840
Yeah?

01:10:21.840 --> 01:10:23.840
RUSSELL HARMON: I'm told
that Iverson originally

01:10:23.840 --> 01:10:28.380
intended to have APL to natively
apply to arrays and trees.

01:10:28.380 --> 01:10:29.624
What happened to the latter?

01:10:29.624 --> 01:10:31.790
MORTEN KROMBERG: What
happened to the trees, Arthur?

01:10:31.790 --> 01:10:35.498
ARTHUR WHITNEY: I don't know.

01:10:35.498 --> 01:10:39.220
MORTEN KROMBERG: I
mean, the nested arrays

01:10:39.220 --> 01:10:42.840
were added in 1983.

01:10:42.840 --> 01:10:47.010
Before 1983, all the arrays were
just rectangular, flat arrays

01:10:47.010 --> 01:10:51.240
the had to consist entirely
of numbers, or characters.

01:10:51.240 --> 01:10:53.090
And then there was
this thing called

01:10:53.090 --> 01:10:58.240
APL2, which came along from
IBM in 1984, where anything

01:10:58.240 --> 01:11:02.489
could become-- any element of
an array could be another array.

01:11:02.489 --> 01:11:04.280
And at that point, I
think, a lot of people

01:11:04.280 --> 01:11:06.380
were thinking about
things to traverse trees.

01:11:06.380 --> 01:11:08.460
Because clearly, if you
could have nested arrays,

01:11:08.460 --> 01:11:11.420
you can have trees.

01:11:11.420 --> 01:11:16.260
I think possibly a
part of the problem

01:11:16.260 --> 01:11:21.030
is that the efficiency
is a problem.

01:11:21.030 --> 01:11:25.060
Because you can write a very
highly optimized APL, or K,

01:11:25.060 --> 01:11:26.090
or J interpreter.

01:11:26.090 --> 01:11:27.600
When it's working
on flat arrays,

01:11:27.600 --> 01:11:29.970
it can actually
outperform hand-coded C,

01:11:29.970 --> 01:11:33.260
because it knows
things about how

01:11:33.260 --> 01:11:35.270
you're combining operations.

01:11:35.270 --> 01:11:36.980
Or it's better than
the hand-coded C

01:11:36.980 --> 01:11:38.813
that you would normally
write, because you'd

01:11:38.813 --> 01:11:40.810
have to go to superhuman
efforts to beat it.

01:11:40.810 --> 01:11:45.580
Of course, hand-coded C can
always beat an APL interpreter.

01:11:45.580 --> 01:11:48.030
But I think on trees,
because you would typically

01:11:48.030 --> 01:11:52.440
be recursively going through
a structure where there's

01:11:52.440 --> 01:11:58.270
a lot more pointers and so
on, you need a compiler to get

01:11:58.270 --> 01:12:02.510
really efficient tree traverse.

01:12:02.510 --> 01:12:06.460
Well, at least if the
trees have small nodes.

01:12:06.460 --> 01:12:07.690
I don't know.

01:12:07.690 --> 01:12:08.690
What do you say, Arthur?

01:12:08.690 --> 01:12:10.756
ARTHUR WHITNEY: I
hadn't heard that.

01:12:14.532 --> 01:12:16.860
MORTEN KROMBERG: Yeah, no,
that's definitely the case.

01:12:16.860 --> 01:12:17.360
That, yes.

01:12:17.360 --> 01:12:19.520
ARTHUR WHITNEY:
So, I don't know.

01:12:19.520 --> 01:12:23.290
MORTEN KROMBERG: Ken had,
I think in the APL book,

01:12:23.290 --> 01:12:26.800
there were ideas for notation
for tree traversal and so on.

01:12:26.800 --> 01:12:28.670
Yeah?

01:12:28.670 --> 01:12:31.020
AUDIENCE: [INAUDIBLE]?

01:12:31.020 --> 01:12:32.917
MORTEN KROMBERG:
It was called APL.

01:12:32.917 --> 01:12:33.875
A Programming Language.

01:12:36.570 --> 01:12:39.060
Which seems strange today
to software engineers,

01:12:39.060 --> 01:12:39.950
of course, who say,
it doesn't look

01:12:39.950 --> 01:12:41.116
like a programming language.

01:12:41.116 --> 01:12:43.040
But of course, he
was doing math.

01:12:43.040 --> 01:12:44.060
Right?

01:12:44.060 --> 01:12:47.000
So for him, the
choice of words-- Ken

01:12:47.000 --> 01:12:49.980
was always very careful with his
words-- it was exactly right.

01:12:49.980 --> 01:12:56.142
It was for programming steps
of mathematical operations.

01:12:56.142 --> 01:12:58.300
AUDIENCE: So, I've looked
at some tree stuff in J,

01:12:58.300 --> 01:13:01.460
and I think some
of the challenges

01:13:01.460 --> 01:13:04.370
are that a lot of tree
operations are incremental,

01:13:04.370 --> 01:13:08.400
whereas array languages like to
operate on the entire data set

01:13:08.400 --> 01:13:08.910
at a time.

01:13:08.910 --> 01:13:09.600
Right?

01:13:09.600 --> 01:13:11.880
And so when you break it
up into smaller operations,

01:13:11.880 --> 01:13:13.950
then you kind of get
the interpreted overhead

01:13:13.950 --> 01:13:14.665
coming in a little bit.

01:13:14.665 --> 01:13:16.190
MORTEN KROMBERG: Yep,
I think that's what

01:13:16.190 --> 01:13:17.023
I was trying to say.

01:13:17.023 --> 01:13:18.920
Yeah, I agree.

01:13:18.920 --> 01:13:20.130
That's a challenge.

01:13:20.130 --> 01:13:21.889
Now, of course,
we are working-- I

01:13:21.889 --> 01:13:23.555
don't know about the
other APL vendors--

01:13:23.555 --> 01:13:26.800
but we are working quite
hard on compilers now.

01:13:26.800 --> 01:13:31.789
I think Arthur is also
thinking about compilers.

01:13:31.789 --> 01:13:33.830
There have been several
attempts through the ages

01:13:33.830 --> 01:13:40.040
to write compilers for APL,
but on hardware 20 years ago,

01:13:40.040 --> 01:13:43.540
they rarely succeeded
in speeding things up

01:13:43.540 --> 01:13:46.590
by more than a factor of two.

01:13:46.590 --> 01:13:49.070
And people couldn't be bothered.

01:13:49.070 --> 01:13:51.740
They wouldn't accept the
inconvenience of a compiler

01:13:51.740 --> 01:13:55.950
compared to this inalienable,
dynamic, interactive way

01:13:55.950 --> 01:13:59.410
of life for a factor
of two improvement.

01:13:59.410 --> 01:14:01.430
But today, the way
the hardware is

01:14:01.430 --> 01:14:04.490
changing where you have
all this concurrency,

01:14:04.490 --> 01:14:09.250
and the speed of the CPU
relative to the speed

01:14:09.250 --> 01:14:11.710
of the memory has
changed dramatically

01:14:11.710 --> 01:14:14.620
in the last decade.

01:14:14.620 --> 01:14:17.630
The memory isn't getting any
faster, or not much faster,

01:14:17.630 --> 01:14:20.390
but they can do all kinds
of crazy, clever stuff

01:14:20.390 --> 01:14:22.370
inside the chips.

01:14:22.370 --> 01:14:26.590
So the win from creating
a compiler today

01:14:26.590 --> 01:14:28.900
could be orders of
magnitude speed up

01:14:28.900 --> 01:14:30.880
rather than just
that factor of two

01:14:30.880 --> 01:14:33.110
they were able to
achieve before.

01:14:33.110 --> 01:14:35.980
So there's several people
working now on compilers,

01:14:35.980 --> 01:14:36.480
I think.

01:14:36.480 --> 01:14:39.720
And then, we can
get back into trees,

01:14:39.720 --> 01:14:43.860
and extend the notation
to do tree traversal.

01:14:43.860 --> 01:14:47.580
Here's something that one of
our customers has built in APL.

01:14:47.580 --> 01:15:27.396
[MUSIC PLAYING]

01:16:17.593 --> 01:16:19.645
MORTEN KROMBERG: I'm
getting some feedback.

01:16:26.290 --> 01:16:28.900
So, this is a result of
an initiative in Finland,

01:16:28.900 --> 01:16:31.860
where the government
decided to make

01:16:31.860 --> 01:16:36.060
all of this 3D map data
available free-of-charge

01:16:36.060 --> 01:16:36.690
to everybody.

01:16:36.690 --> 01:16:40.770
And they launched a competition
for the best application

01:16:40.770 --> 01:16:43.080
to take advantage of it.

01:16:43.080 --> 01:16:44.710
And one of our users built this.

01:16:44.710 --> 01:16:47.360
It's a game that he sells
to private individuals,

01:16:47.360 --> 01:16:51.988
and to the search and rescue
operations in Finland.

01:16:56.020 --> 01:16:56.920
We're out of time.

01:16:56.920 --> 01:16:59.510
So we've had questions.

01:16:59.510 --> 01:17:01.420
And maybe I'll just
leave that one up.

01:17:01.420 --> 01:17:03.320
If you want to get
hold of it, TryAPL,

01:17:03.320 --> 01:17:05.500
there are free versions
available for students.

01:17:05.500 --> 01:17:08.520
If anybody here is really
interested, drop me a line,

01:17:08.520 --> 01:17:13.010
and I'm sure we
can get you a copy.

01:17:13.010 --> 01:17:16.454
So, that's it.

01:17:16.454 --> 01:17:17.870
Thank you very
much for listening.

01:17:17.870 --> 01:17:22.027
[APPLAUSE]

