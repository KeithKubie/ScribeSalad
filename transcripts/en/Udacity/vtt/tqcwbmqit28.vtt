WEBVTT
Kind: captions
Language: en

00:00:00.570 --> 00:00:06.490
To motivate the need for dynamic management of data and metadata, it's useful to

00:00:06.490 --> 00:00:08.720
look at the structure of a traditional

00:00:08.720 --> 00:00:12.600
NFS server which is centralized. In a traditional

00:00:12.600 --> 00:00:16.940
centralized NFS server what you have is

00:00:16.940 --> 00:00:20.670
the data blocks are residing on the disks.

00:00:20.670 --> 00:00:26.610
So in the memory of the server, the contents include the metadata for the files

00:00:26.610 --> 00:00:29.989
like the iNote structures. And file cache which is

00:00:29.989 --> 00:00:32.905
files that have been brought in from the disk

00:00:32.905 --> 00:00:36.170
are stored in the memory in what is called

00:00:36.170 --> 00:00:39.160
the file cache. So that future requests for the same

00:00:39.160 --> 00:00:41.480
files can be served from the memory of the

00:00:41.480 --> 00:00:44.040
server rather than going to the disk. And the server

00:00:44.040 --> 00:00:48.480
also keeps the client caching directory. That is, who

00:00:48.480 --> 00:00:52.320
on the local area network are currently accessing files that

00:00:52.320 --> 00:00:55.370
is the propriety of this particular server. And

00:00:55.370 --> 00:00:58.600
in a Unix file system the server is unconcerned

00:00:58.600 --> 00:01:00.980
about the semantics of file sharing. In other

00:01:00.980 --> 00:01:04.660
words, the assumption is that the server is caching

00:01:04.660 --> 00:01:08.920
for each client completely independently. And therefore if

00:01:08.920 --> 00:01:11.960
clients happened to share a file that is completely

00:01:11.960 --> 00:01:15.070
the problem of the clients, and the server

00:01:15.070 --> 00:01:17.370
is not concerned about that. So all of these

00:01:17.370 --> 00:01:20.538
contents that I just described to you, metadata,

00:01:20.538 --> 00:01:24.340
file cache, client caching directory. All of these are

00:01:24.340 --> 00:01:27.530
in the memory of a particular server. And

00:01:27.530 --> 00:01:31.380
if the server happens to be housing hot files

00:01:31.380 --> 00:01:34.280
used by a lot of users that is

00:01:34.280 --> 00:01:37.100
being served by this particular server. Then that's bad

00:01:37.100 --> 00:01:39.430
news for the server in terms of scalability, because

00:01:39.430 --> 00:01:42.600
it has to worry about the requests simultaneously coming

00:01:42.600 --> 00:01:45.560
from lots of clients for these hot files. And

00:01:45.560 --> 00:01:48.650
so it is constrained by the bandwidth that's available to

00:01:48.650 --> 00:01:51.200
access the files from the disk. It is constrained

00:01:51.200 --> 00:01:54.710
by the amount of memory space it's got, for caching

00:01:54.710 --> 00:01:56.780
files, and the metadata of the files, and so

00:01:56.780 --> 00:02:00.020
on. At the same time, there could be another server

00:02:00.020 --> 00:02:04.500
that also has adequate bandwidth to the storage, and, and

00:02:04.500 --> 00:02:08.470
memory space. But unfortunately it may be housing cold files.

00:02:08.470 --> 00:02:11.030
And therefore, there are not many clients for

00:02:11.030 --> 00:02:14.070
this server. So you can immediately see that

00:02:14.070 --> 00:02:16.550
the sort of centralization of the traditional file

00:02:16.550 --> 00:02:20.070
system results in hot spots. And that's the

00:02:20.070 --> 00:02:22.330
thing that we're trying to avoid in a

00:02:22.330 --> 00:02:25.790
distributed file system, and that's where dynamic management

00:02:25.790 --> 00:02:29.170
comes into play. So in XFS, it provides

00:02:29.170 --> 00:02:34.510
the same functionality as a centralized NFS Server,

00:02:34.510 --> 00:02:38.220
but it is distributed and the metadata management

00:02:38.220 --> 00:02:41.210
is dynamic. And that is, in a centralized

00:02:41.210 --> 00:02:46.060
file server, the mapping between the manager node

00:02:46.060 --> 00:02:49.500
for a file and the location of the

00:02:49.500 --> 00:02:55.090
file is the same. Or in other words, if the file happens to reside on the disk

00:02:55.090 --> 00:02:59.660
of this server, then this server is the guy that is going to handle the metadata

00:02:59.660 --> 00:03:01.670
management for this file as well. On the

00:03:01.670 --> 00:03:07.330
other hand, in XFS, metadata management is dynamically distributed.

00:03:07.330 --> 00:03:12.874
So, let's say that you have F1, F2, and F3 are the hot files. In that case,

00:03:12.874 --> 00:03:15.800
metadata management for files F2 and F3 can

00:03:15.800 --> 00:03:19.060
be done by some other node, say S3. And

00:03:19.060 --> 00:03:22.290
this server may have the cache for the

00:03:22.290 --> 00:03:25.170
file. So, in other words, all the data structures

00:03:25.170 --> 00:03:29.740
that we've talked about that has to reside in the memory of a particular server

00:03:29.740 --> 00:03:33.830
like metadata, file cache, and the caching information

00:03:33.830 --> 00:03:36.240
about who's having the files and so on.

00:03:36.240 --> 00:03:38.200
All of that can be distributed with

00:03:38.200 --> 00:03:42.015
dynamic management of data and metadata, which is

00:03:42.015 --> 00:03:45.700
the idea in XFS. And I'll shortly explain

00:03:45.700 --> 00:03:50.910
how exactly this dynamic management is facilitated by

00:03:50.910 --> 00:03:55.050
the implementation of XFS. So, in any systems research,

00:03:55.050 --> 00:03:57.950
there is always first the idea and then there's

00:03:57.950 --> 00:04:01.092
implementation. So the idea in XFS is that we

00:04:01.092 --> 00:04:05.790
want to manage the data and metadata management dynamically, and

00:04:05.790 --> 00:04:11.120
we'll see how that is done. And also, what we want to do is we don't want the

00:04:11.120 --> 00:04:13.560
cache for the files to be only at the

00:04:13.560 --> 00:04:15.930
server. What we would like to be able to do

00:04:15.930 --> 00:04:20.290
is, if a file is accessed by several different

00:04:20.290 --> 00:04:23.340
nodes, then they're living in the client caches of

00:04:23.340 --> 00:04:26.810
the different nodes. If a file is residing in

00:04:26.810 --> 00:04:30.030
the cache of a peer node, then it makes

00:04:30.030 --> 00:04:32.450
sense that if a new request comes from the

00:04:32.450 --> 00:04:35.478
same file, then getting that file from a peer

00:04:35.478 --> 00:04:38.200
cache may be much more efficient than getting it

00:04:38.200 --> 00:04:41.400
from the disk. And that way we can also conserve

00:04:41.400 --> 00:04:44.390
the total amount of memory that's available on the servers and use it

00:04:44.390 --> 00:04:47.170
more frugally by exploiting the memories that

00:04:47.170 --> 00:04:49.720
are available in the clients, so that

00:04:49.720 --> 00:04:55.470
the caching of the files can be done cooperatively among the clients. And that's

00:04:55.470 --> 00:04:58.855
the other nugget in the technical contribution

00:04:58.855 --> 00:05:01.319
of XFS, is the cooperative client caching.

