WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.311
[LOGO MUSIC]

00:00:04.730 --> 00:00:07.050
MATHIEU CHARTIER:
Hello everyone.

00:00:07.050 --> 00:00:09.210
Thank you all for coming,
and let's get started.

00:00:09.210 --> 00:00:15.280
So I'm Mathieu, and
today, my colleagues and I

00:00:15.280 --> 00:00:17.660
will be talking about how
we are making the Android

00:00:17.660 --> 00:00:22.820
Runtime, also known as ART,
faster in Android Q. We will

00:00:22.820 --> 00:00:27.170
also be showing you both some
internal knowledge about ART

00:00:27.170 --> 00:00:30.590
as well as some best
practices, such as using

00:00:30.590 --> 00:00:33.530
the reportfully drawn API
as well as when to use

00:00:33.530 --> 00:00:35.900
object pooling.

00:00:35.900 --> 00:00:39.380
And now a recap
about how ART works--

00:00:39.380 --> 00:00:42.920
so ART is the software layer in
between the Android operating

00:00:42.920 --> 00:00:45.720
system and the applications.

00:00:45.720 --> 00:00:48.200
It provides an
execution environment

00:00:48.200 --> 00:00:50.810
for running both Kotlin
and Java language

00:00:50.810 --> 00:00:53.900
applications on Android.

00:00:53.900 --> 00:00:56.700
To do this, ART does two things.

00:00:56.700 --> 00:00:59.720
It processes dex files, the
internal format of Android

00:00:59.720 --> 00:01:02.930
applications, through a
hybrid model consisting

00:01:02.930 --> 00:01:06.410
of interpretation, just
in time compilation,

00:01:06.410 --> 00:01:10.580
and profile-based ahead
of time compilation.

00:01:10.580 --> 00:01:14.510
ART also manages memory by
having automatic reclamation

00:01:14.510 --> 00:01:17.160
through a concurrent compacting
garbage collector that we

00:01:17.160 --> 00:01:18.560
introduced in Android Oreo.

00:01:22.060 --> 00:01:26.430
So an update on
profiles in the cloud--

00:01:26.430 --> 00:01:29.280
so ART has had profiles
that contain information

00:01:29.280 --> 00:01:34.560
about application usage
on Android since Nugget.

00:01:34.560 --> 00:01:36.450
And, until recently,
these profiles

00:01:36.450 --> 00:01:39.330
are only local to the device.

00:01:39.330 --> 00:01:42.930
These profiles are collected
by the just in time compiler

00:01:42.930 --> 00:01:46.740
and are saved to device storage
during application execution

00:01:46.740 --> 00:01:50.940
and later are used to
optimize the application.

00:01:50.940 --> 00:01:53.070
Since these were only
locally stored on the device

00:01:53.070 --> 00:01:55.440
until recently, this
meant that applications

00:01:55.440 --> 00:01:57.310
had to run a while,
have their profile

00:01:57.310 --> 00:02:01.112
saved, and then we had to
optimize the application when

00:02:01.112 --> 00:02:02.820
the phone was charging
or when the device

00:02:02.820 --> 00:02:05.640
was charging in the background.

00:02:05.640 --> 00:02:09.810
To address this, we introduced
a new service called Profiles

00:02:09.810 --> 00:02:13.340
in the Cloud, which was
announced last year at Google

00:02:13.340 --> 00:02:16.258
I/O. And today, we're going
to be providing some more

00:02:16.258 --> 00:02:17.550
updates about how that's going.

00:02:20.190 --> 00:02:24.780
So, before we dive into
profiles in the cloud,

00:02:24.780 --> 00:02:28.980
let's take a closer look at
what's inside of a profile.

00:02:28.980 --> 00:02:30.870
Profiles contain
detailed information

00:02:30.870 --> 00:02:34.530
about an application usage,
including what methods are run

00:02:34.530 --> 00:02:37.740
and what classes are
loaded during both startup

00:02:37.740 --> 00:02:42.070
and during a steady state
of the application usage.

00:02:42.070 --> 00:02:44.380
This enables ART to
compile the methods that

00:02:44.380 --> 00:02:46.720
are the most important
to machine code

00:02:46.720 --> 00:02:50.680
as well as optimize the
application for startup

00:02:50.680 --> 00:02:52.780
by prioritizing code
that is rendering

00:02:52.780 --> 00:02:56.250
startup for performance.

00:02:56.250 --> 00:02:59.640
Profiles in the cloud enable
downloading the profile

00:02:59.640 --> 00:03:03.042
alongside the application
during installation

00:03:03.042 --> 00:03:04.500
and then optimizing
the application

00:03:04.500 --> 00:03:06.390
during installation.

00:03:06.390 --> 00:03:09.270
This means that the users
get the performance directly

00:03:09.270 --> 00:03:11.310
after installation
and no longer have

00:03:11.310 --> 00:03:13.770
to wait for the application
to get optimized

00:03:13.770 --> 00:03:17.020
when the device is
charging in the background.

00:03:17.020 --> 00:03:18.780
So here we have
observed speed ups

00:03:18.780 --> 00:03:21.810
of around 15% faster
application startup directly

00:03:21.810 --> 00:03:25.030
after installation.

00:03:25.030 --> 00:03:28.240
So now let's dive into how
the whole profile in the cloud

00:03:28.240 --> 00:03:30.620
process works.

00:03:30.620 --> 00:03:33.910
So the main idea here is
that applications usually

00:03:33.910 --> 00:03:37.540
have commonly shared code paths
between a multitude of users

00:03:37.540 --> 00:03:39.005
and devices.

00:03:39.005 --> 00:03:41.380
So this means that most users
are going to have generally

00:03:41.380 --> 00:03:44.410
the same use case for a
startup and the application

00:03:44.410 --> 00:03:47.240
usage in general.

00:03:47.240 --> 00:03:49.600
So what we want to do
with profiles in the cloud

00:03:49.600 --> 00:03:52.060
is have the initial users
bootstrap performance

00:03:52.060 --> 00:03:53.560
for the rest of the users.

00:03:53.560 --> 00:03:57.580
And this often takes advantage
or benefits from the fact

00:03:57.580 --> 00:04:00.550
that developers roll out
applications incrementally

00:04:00.550 --> 00:04:03.620
with alpha beta channels.

00:04:03.620 --> 00:04:05.980
So once an application
is installed,

00:04:05.980 --> 00:04:07.540
the profiles are
uploaded to Play,

00:04:07.540 --> 00:04:10.330
and they're aggregated into
what we call a common core

00:04:10.330 --> 00:04:13.060
profile for the application.

00:04:13.060 --> 00:04:15.700
And in future installs,
this common core profile

00:04:15.700 --> 00:04:17.440
is downloaded alongside
the application

00:04:17.440 --> 00:04:19.570
and used to optimize
the application

00:04:19.570 --> 00:04:22.570
during installation.

00:04:22.570 --> 00:04:25.240
This improves both the steady
state performance as well as

00:04:25.240 --> 00:04:27.310
the startup.

00:04:27.310 --> 00:04:29.170
I'd like to note
that Android P also

00:04:29.170 --> 00:04:32.601
has API support that non-Play
devices can leverage.

00:04:35.370 --> 00:04:37.500
As for numbers, as
you can see here,

00:04:37.500 --> 00:04:39.140
YouTube has a large
startup improvement

00:04:39.140 --> 00:04:41.820
around 18% from
profiles in the cloud,

00:04:41.820 --> 00:04:45.760
and other applications also
have substantial improvement.

00:04:45.760 --> 00:04:48.840
This is field data of Google
applications collected

00:04:48.840 --> 00:04:50.220
from Pixel devices.

00:04:54.668 --> 00:04:56.210
And one of the best
parts of all this

00:04:56.210 --> 00:04:59.810
is that developers and users
get the benefits for free.

00:04:59.810 --> 00:05:02.120
Developers don't have to
write a single line of code

00:05:02.120 --> 00:05:04.370
to enable the profiles
for their applications,

00:05:04.370 --> 00:05:08.840
and users will have to take
any action to get the benefits.

00:05:08.840 --> 00:05:11.540
And right now, around
80% of installations

00:05:11.540 --> 00:05:14.690
on devices with
Google Play services

00:05:14.690 --> 00:05:17.540
use profiles in the cloud.

00:05:17.540 --> 00:05:19.730
Another interesting
observation is

00:05:19.730 --> 00:05:21.680
that the profiles
in the cloud seem

00:05:21.680 --> 00:05:24.500
to show that only around
20% of application code

00:05:24.500 --> 00:05:26.120
is commonly used.

00:05:26.120 --> 00:05:28.190
This could indicate that
there are opportunities

00:05:28.190 --> 00:05:31.800
to reduce code size by removing
unused code in applications.

00:05:34.380 --> 00:05:37.650
And now let's go to app startup.

00:05:37.650 --> 00:05:39.210
We have done some
improvements in Q,

00:05:39.210 --> 00:05:42.330
and I'll be going
over some of those.

00:05:42.330 --> 00:05:47.350
In Android Q, we have done
three major new improvements.

00:05:47.350 --> 00:05:50.430
We've improved the application
images originally introduced

00:05:50.430 --> 00:05:55.130
in Android Nugget to provide
a larger startup improvement.

00:05:55.130 --> 00:05:57.930
We have added pre-forking
of application processes

00:05:57.930 --> 00:06:01.033
to accelerate process
creation on Android.

00:06:01.033 --> 00:06:02.450
And we have added
a new generation

00:06:02.450 --> 00:06:03.940
of garbage collector.

00:06:06.520 --> 00:06:09.810
So starting with
application images,

00:06:09.810 --> 00:06:12.360
recall that
application images, as

00:06:12.360 --> 00:06:15.030
if you see in other
years' I/O talks,

00:06:15.030 --> 00:06:17.520
are actually
serialized snapshots

00:06:17.520 --> 00:06:19.920
containing the classes that
are the most commonly used

00:06:19.920 --> 00:06:22.930
during startup of
an application.

00:06:22.930 --> 00:06:24.660
So this optimizes
the performance

00:06:24.660 --> 00:06:28.830
by shifting the work of loading
those classes from happening

00:06:28.830 --> 00:06:31.380
during startup to
happening ahead of time

00:06:31.380 --> 00:06:35.400
in the ART ahead
of time compiler.

00:06:35.400 --> 00:06:37.290
So the application
image is generated

00:06:37.290 --> 00:06:40.320
by the compiler taking
both the application, APK,

00:06:40.320 --> 00:06:44.100
and the profile as inputs
and then using the profile

00:06:44.100 --> 00:06:46.260
to know specifically
what classes are loaded

00:06:46.260 --> 00:06:49.170
during startup and including
only these in the application

00:06:49.170 --> 00:06:50.470
images.

00:06:50.470 --> 00:06:53.700
This is done during
installation if there

00:06:53.700 --> 00:06:55.522
is a profile from
the cloud present.

00:06:55.522 --> 00:06:57.480
Otherwise, it's normally
done in the background

00:06:57.480 --> 00:07:00.423
when a device is charging.

00:07:00.423 --> 00:07:02.340
So how have we improved
our application images

00:07:02.340 --> 00:07:04.512
in Android Q?

00:07:04.512 --> 00:07:07.830
Well, we recently observed
that string interning caused

00:07:07.830 --> 00:07:10.360
specifically by using string
literals in applications

00:07:10.360 --> 00:07:13.270
startup was taking a
large amount of time.

00:07:13.270 --> 00:07:16.800
So we added an optimization
to include the string literals

00:07:16.800 --> 00:07:19.770
that are commonly used during
startup for the application

00:07:19.770 --> 00:07:23.220
inside of the application image.

00:07:23.220 --> 00:07:25.380
This is accomplished by
leveraging the profile

00:07:25.380 --> 00:07:27.360
to know specifically
what methods

00:07:27.360 --> 00:07:28.710
are executed during startup.

00:07:31.230 --> 00:07:33.120
So, as you can see
here, there is around

00:07:33.120 --> 00:07:36.130
a 2.5% improvement
on some applications,

00:07:36.130 --> 00:07:38.880
and this is a selection of
first party applications running

00:07:38.880 --> 00:07:41.500
on a Pixel to Excel.

00:07:41.500 --> 00:07:43.950
And now let me hand
it off to Chris

00:07:43.950 --> 00:07:47.051
for pre-forking
application processes.

00:07:47.051 --> 00:07:48.750
CHRIS WALLES:
Thank you, Mathieu.

00:07:48.750 --> 00:07:49.540
Hello, everyone.

00:07:49.540 --> 00:07:52.320
My name is Chris, and I
am here to talk about some

00:07:52.320 --> 00:07:55.040
of the changes we have made
to the Zygote in Android Q

00:07:55.040 --> 00:07:58.410
to help improve application
startup performance.

00:07:58.410 --> 00:08:00.720
For those of you
unfamiliar with the Zygote,

00:08:00.720 --> 00:08:03.450
this is the process in
Android that all applications

00:08:03.450 --> 00:08:04.920
are spawned from.

00:08:04.920 --> 00:08:09.870
This design allows us to
take several steps that

00:08:09.870 --> 00:08:11.760
are common to all
Android applications

00:08:11.760 --> 00:08:15.450
and perform them before the
applications are launched.

00:08:15.450 --> 00:08:18.180
Unfortunately, in previous
versions of Android,

00:08:18.180 --> 00:08:20.880
there were still several
application agnostic steps

00:08:20.880 --> 00:08:23.640
that had to be performed after
the application was launched.

00:08:26.180 --> 00:08:28.550
These include such things
as process spawning,

00:08:28.550 --> 00:08:32.539
thread creation driver
loading, and resource cleanup.

00:08:32.539 --> 00:08:35.990
In Android Q, we're introducing
the unspecialized app process

00:08:35.990 --> 00:08:36.919
pool.

00:08:36.919 --> 00:08:40.340
Unspecialized app
processes are created

00:08:40.340 --> 00:08:42.919
and perform these application
agnostic steps off

00:08:42.919 --> 00:08:46.400
the critical path of
application startup.

00:08:46.400 --> 00:08:47.840
This produces an
average speed-up

00:08:47.840 --> 00:08:51.620
of approximately 5 milliseconds
for most applications

00:08:51.620 --> 00:08:53.090
on a Pixel Two device.

00:08:53.090 --> 00:08:55.250
But more importantly,
it forms a foundation

00:08:55.250 --> 00:08:58.195
for further improvements
to application startup.

00:08:58.195 --> 00:08:59.570
So now, let's take
a look at some

00:08:59.570 --> 00:09:02.650
of the changes we've made to
the early stages of app startup

00:09:02.650 --> 00:09:05.270
to make it faster.

00:09:05.270 --> 00:09:07.670
On the left, we can see
the launching process

00:09:07.670 --> 00:09:09.580
from previous
versions of Android.

00:09:09.580 --> 00:09:12.980
It involves three communicating
processes, one of which

00:09:12.980 --> 00:09:16.220
must be created during this
critical launch window.

00:09:16.220 --> 00:09:19.730
It also involves inter-process
communication, or IPC,

00:09:19.730 --> 00:09:24.170
and this requires the Zygote
to wake up, block, unblock,

00:09:24.170 --> 00:09:26.330
and send a message
to the system server.

00:09:26.330 --> 00:09:28.190
All of these steps
take resources

00:09:28.190 --> 00:09:32.150
away from the operating
system and the application

00:09:32.150 --> 00:09:34.490
that the user is
trying to launch.

00:09:34.490 --> 00:09:37.430
On the right, we can see the
new, much-simpler app launch

00:09:37.430 --> 00:09:42.260
process in Android Q. The Zygote
has been completely removed

00:09:42.260 --> 00:09:45.530
from this critical path.

00:09:45.530 --> 00:09:48.980
Also, the new process that
will become the application

00:09:48.980 --> 00:09:51.830
already exists in the system and
has performed these application

00:09:51.830 --> 00:09:54.530
agnostic steps already.

00:09:54.530 --> 00:09:56.540
By removing the Zygote,
we also eliminate

00:09:56.540 --> 00:09:59.205
an entire inter-process
communication round.

00:09:59.205 --> 00:10:01.580
And this means that the new
application can talk directly

00:10:01.580 --> 00:10:05.600
to the system server and not
wake up the Zygote and consume

00:10:05.600 --> 00:10:08.215
system resources.

00:10:08.215 --> 00:10:10.340
In a couple slides, I'm
going to be talk about some

00:10:10.340 --> 00:10:14.660
of the metric and investigation
work we've done in Android Q

00:10:14.660 --> 00:10:18.170
to help improve applications
startup performance.

00:10:18.170 --> 00:10:20.480
But I wanted to briefly
mention the metric that

00:10:20.480 --> 00:10:22.760
is directly impacted
by these changes, which

00:10:22.760 --> 00:10:24.770
is time to first slice.

00:10:24.770 --> 00:10:27.320
Time to first slice is
a measure of how quickly

00:10:27.320 --> 00:10:31.260
we can begin running
application-specific code.

00:10:31.260 --> 00:10:33.550
And, as we can
see in this graph,

00:10:33.550 --> 00:10:35.750
there is a fairly uniform
improvement, again,

00:10:35.750 --> 00:10:37.880
of approximately five
milliseconds to the time

00:10:37.880 --> 00:10:39.920
to first slice metric
for these seven apps

00:10:39.920 --> 00:10:42.782
on a Pixel Two device.

00:10:42.782 --> 00:10:44.240
However, this is
just the beginning

00:10:44.240 --> 00:10:46.070
for the speed-ups
and improvements

00:10:46.070 --> 00:10:49.340
we can see from using
unspecialized app processes.

00:10:49.340 --> 00:10:51.950
Follow up work that is
currently in testing

00:10:51.950 --> 00:10:53.960
shows an additional
speed-up on the order

00:10:53.960 --> 00:10:56.930
of tens of milliseconds.

00:10:56.930 --> 00:10:58.940
The best news for
users and developers

00:10:58.940 --> 00:11:02.000
alike is that all applications
will see these improvements

00:11:02.000 --> 00:11:05.150
without requiring any changes
to their application or they way

00:11:05.150 --> 00:11:07.410
that they use their device.

00:11:07.410 --> 00:11:10.400
So now I'm going to talk briefly
about the measurement work

00:11:10.400 --> 00:11:14.630
that we've been doing in Android
Q. When performance tuning

00:11:14.630 --> 00:11:16.800
software as complicated
as an operating system,

00:11:16.800 --> 00:11:20.030
it's very important to get
a deep understanding of what

00:11:20.030 --> 00:11:22.040
is happening, when
it's happening,

00:11:22.040 --> 00:11:24.290
and how long it is taking.

00:11:24.290 --> 00:11:26.210
To this end, we've
spent a lot of time

00:11:26.210 --> 00:11:30.800
looking at traces in log files
over the last couple months.

00:11:30.800 --> 00:11:32.870
We've also developed
several new tools

00:11:32.870 --> 00:11:35.930
for extracting actionable
and useful insights

00:11:35.930 --> 00:11:38.190
from this large corpus of data.

00:11:38.190 --> 00:11:41.610
One of these tools is
publicly available to you,

00:11:41.610 --> 00:11:43.920
and that's the startup
analyzer script.

00:11:43.920 --> 00:11:46.370
This script will extract
detailed information

00:11:46.370 --> 00:11:49.190
about startup events
from trace files,

00:11:49.190 --> 00:11:52.250
including startup duration,
scheduling status,

00:11:52.250 --> 00:11:56.400
and binder transactions.

00:11:56.400 --> 00:11:58.520
One of the most
important questions

00:11:58.520 --> 00:12:01.160
that this work made us
ask ourself is, when

00:12:01.160 --> 00:12:04.058
does application startup end?

00:12:04.058 --> 00:12:05.600
Well, there are many
criteria that we

00:12:05.600 --> 00:12:10.340
can use that might signal
when startup is finished,

00:12:10.340 --> 00:12:13.900
including when the first frame
of the user interface is drawn.

00:12:13.900 --> 00:12:15.650
And this is certainly
a useful definition.

00:12:15.650 --> 00:12:18.410
It's one we can use to
optimize performance.

00:12:18.410 --> 00:12:22.070
But it doesn't tell us when the
user perceives the application

00:12:22.070 --> 00:12:24.260
to be usable, which
is what we consider

00:12:24.260 --> 00:12:27.085
to be the gold standard for
application startup endpoint.

00:12:30.053 --> 00:12:32.470
Unfortunately, we don't have
a way to automatically detect

00:12:32.470 --> 00:12:33.790
when this happens.

00:12:33.790 --> 00:12:36.610
We do, however, have an
API call that developers

00:12:36.610 --> 00:12:40.570
can use to tell us when
you think it happens.

00:12:40.570 --> 00:12:43.000
This makes it easier for us
to identify the application

00:12:43.000 --> 00:12:44.863
startup process
for individual apps

00:12:44.863 --> 00:12:46.780
and make sure that we
optimize the things that

00:12:46.780 --> 00:12:49.070
are important to your app.

00:12:49.070 --> 00:12:51.250
So if you would like
to help us speed up

00:12:51.250 --> 00:12:52.930
the performance of
your application,

00:12:52.930 --> 00:12:56.500
I highly recommend that you
use this reportfully drawn API

00:12:56.500 --> 00:12:58.700
call.

00:12:58.700 --> 00:13:00.198
So thank you for your time.

00:13:00.198 --> 00:13:02.740
And at this point, I'm going to
hand the stage over to Roland

00:13:02.740 --> 00:13:04.720
for a discussion of the
garbage collection work

00:13:04.720 --> 00:13:06.472
that we've done in Q.

00:13:06.472 --> 00:13:07.930
ROLAND LEVILLAIN:
Thank you, Chris.

00:13:13.840 --> 00:13:14.478
Hi, everyone.

00:13:14.478 --> 00:13:16.270
My name is Roland, and
I'm going to present

00:13:16.270 --> 00:13:18.103
the improvements we've
made in ART's Garbage

00:13:18.103 --> 00:13:21.200
Collector for Android Q.

00:13:21.200 --> 00:13:23.830
In Android Q, we've improved
ART's concurrent copying

00:13:23.830 --> 00:13:26.120
Garbage Collector in two ways--

00:13:26.120 --> 00:13:28.420
first, by adding support
for generational garbage

00:13:28.420 --> 00:13:31.570
collection, which makes
GC cheaper overall--

00:13:31.570 --> 00:13:35.200
and second, by using a two
phase collection strategy which

00:13:35.200 --> 00:13:38.290
makes garbage collection more
precise and able to reclaim

00:13:38.290 --> 00:13:40.360
more objects.

00:13:40.360 --> 00:13:42.400
During the development
of Android Q,

00:13:42.400 --> 00:13:44.410
we've also re-evaluated
the benefits

00:13:44.410 --> 00:13:48.430
of object pooling versus
standard allocation in ART.

00:13:48.430 --> 00:13:49.910
In the last section
of this talk,

00:13:49.910 --> 00:13:53.680
we'll share with you our
findings and new recommendation

00:13:53.680 --> 00:13:56.632
regarding object pooling.

00:13:56.632 --> 00:13:58.840
But for now, let's have a
look at garbage collection,

00:13:58.840 --> 00:14:01.830
generational garbage collection.

00:14:01.830 --> 00:14:04.630
In Android 8 Oreo, we introduced
a concurrent copying garbage

00:14:04.630 --> 00:14:07.450
collector, or CC
collector, in ART.

00:14:07.450 --> 00:14:10.240
This year, we are adding support
for generational collection

00:14:10.240 --> 00:14:11.680
to this collector.

00:14:11.680 --> 00:14:13.700
In Android Q, the CC
collector alternates

00:14:13.700 --> 00:14:17.570
full heap collections and
young generation collections.

00:14:17.570 --> 00:14:19.300
These young generation
collections only

00:14:19.300 --> 00:14:21.440
process a fraction of the heap.

00:14:21.440 --> 00:14:23.350
They are cheaper and
almost as effective

00:14:23.350 --> 00:14:25.450
as full heap collections.

00:14:25.450 --> 00:14:27.250
Before we look at
generational collection,

00:14:27.250 --> 00:14:29.560
let's have a quick look at how
the concurrent copying garbage

00:14:29.560 --> 00:14:30.227
collector works.

00:14:32.690 --> 00:14:35.930
The garbage collector in ART
is a concurrent copying one.

00:14:35.930 --> 00:14:38.560
It is concurrent because it
runs at the same time on an app

00:14:38.560 --> 00:14:42.620
thread, and it does not require
a long stop the world pause.

00:14:42.620 --> 00:14:44.510
This work, thanks
to a cooperation

00:14:44.510 --> 00:14:46.640
between the GC and
the app's code,

00:14:46.640 --> 00:14:48.500
all accesses to
object references

00:14:48.500 --> 00:14:51.020
are instrumented in
the runtime with what

00:14:51.020 --> 00:14:52.190
we call read barriers.

00:14:52.190 --> 00:14:56.010
And they ensure that the app
does not see a stale object.

00:14:56.010 --> 00:14:58.580
The GC is also
said to be copying

00:14:58.580 --> 00:15:01.850
because it moves live objects in
memory by making a copy of them

00:15:01.850 --> 00:15:04.950
and reclaiming the space that
held the original objects,

00:15:04.950 --> 00:15:06.940
including the data objects.

00:15:06.940 --> 00:15:10.040
This copying strategy means that
the GC is compacting the heap

00:15:10.040 --> 00:15:12.290
and preventing fragmentation.

00:15:12.290 --> 00:15:15.650
Let's see how this works.

00:15:15.650 --> 00:15:17.690
Copying collector
traditionally splits

00:15:17.690 --> 00:15:19.450
the manage heap in two spaces--

00:15:19.450 --> 00:15:22.885
a from space, which contains
the object currently allocated

00:15:22.885 --> 00:15:26.300
and used, and a two space, to
which live object are moved

00:15:26.300 --> 00:15:28.090
during garbage collection.

00:15:28.090 --> 00:15:31.370
During a GC cycle, the
collector traces the heap.

00:15:31.370 --> 00:15:35.090
It follows root references to
manage objects, for instance,

00:15:35.090 --> 00:15:37.940
from its third stack and
marks all objects reachable

00:15:37.940 --> 00:15:40.550
from these roots to compute
the set of live objects.

00:15:43.250 --> 00:15:46.270
The eventual objects in dark
red in this illustration

00:15:46.270 --> 00:15:50.710
are the ones which are not
visited by the collector.

00:15:50.710 --> 00:15:53.137
Reachable objects are
copied to the to-space

00:15:53.137 --> 00:15:54.595
as they are being
marked by the GC.

00:15:57.110 --> 00:15:59.530
And likewise, all other
reachable objects living

00:15:59.530 --> 00:16:01.530
in the from-space are
copied to the to-space.

00:16:01.530 --> 00:16:05.452
They're being marked,
thus compacting the heap.

00:16:05.452 --> 00:16:07.160
At the end of the
collection, the objects

00:16:07.160 --> 00:16:09.680
that haven't been
moved are the dead ones

00:16:09.680 --> 00:16:11.340
because they're not reachable.

00:16:11.340 --> 00:16:16.660
And the GC can then reclaim the
memory used by the from-space.

00:16:16.660 --> 00:16:19.270
Finally, the from-space and
the to-space are swapped,

00:16:19.270 --> 00:16:23.090
and new locations happen
in the new from-space.

00:16:23.090 --> 00:16:25.420
So that's heap
compaction in a nutshell.

00:16:25.420 --> 00:16:28.620
Let's see how this works in ART.

00:16:28.620 --> 00:16:30.830
So for practical
reasons, ART CC collector

00:16:30.830 --> 00:16:34.370
does not use two semi-spaces
but a region space composed

00:16:34.370 --> 00:16:37.490
of 256 kilobytes regions.

00:16:37.490 --> 00:16:39.410
As an app's memory
needs grow over time,

00:16:39.410 --> 00:16:44.760
new regions are allocated to
create space for new objects.

00:16:44.760 --> 00:16:46.740
As in the previous
example, we're

00:16:46.740 --> 00:16:50.052
highlighting unreachable
objects in dark red color.

00:16:50.052 --> 00:16:52.260
At the beginning of a GC
cycle, the garbage collector

00:16:52.260 --> 00:16:55.140
makes a decision for
each region in use.

00:16:55.140 --> 00:16:58.200
The first option is to
evacuate a region, which

00:16:58.200 --> 00:17:00.300
means moving all live
objects out of it

00:17:00.300 --> 00:17:02.460
and reclaim the memory
backing the region

00:17:02.460 --> 00:17:04.410
at the end of the cycle.

00:17:04.410 --> 00:17:07.980
This makes sense if there's a
good proportion of dead objects

00:17:07.980 --> 00:17:10.530
in this region
because evacuating

00:17:10.530 --> 00:17:13.380
it means it will
help compact the heap

00:17:13.380 --> 00:17:16.962
and free some physical memory.

00:17:16.962 --> 00:17:19.170
But if most of the allocated
objects in these regions

00:17:19.170 --> 00:17:22.200
are actually alive, there
would be little benefits

00:17:22.200 --> 00:17:25.920
to evacuate it and maybe some
costs because of the object

00:17:25.920 --> 00:17:27.089
copies.

00:17:27.089 --> 00:17:28.860
In that case, the
GC might decide

00:17:28.860 --> 00:17:30.720
to keep the region
as is, and we call

00:17:30.720 --> 00:17:35.510
that an unevacuated region in
dark green in this diagram.

00:17:35.510 --> 00:17:39.480
Until Android Q, the decision
to evacuate a region or not

00:17:39.480 --> 00:17:41.580
was based on information
from a previous GC

00:17:41.580 --> 00:17:44.365
cycle, which is not optimal.

00:17:44.365 --> 00:17:45.990
Later in this talk,
my colleague Lokesh

00:17:45.990 --> 00:17:47.073
will talk more about this.

00:17:49.700 --> 00:17:51.340
Tracing and marking
work similarly

00:17:51.340 --> 00:17:52.340
to the previous example.

00:17:52.340 --> 00:17:55.220
But instead of copying
evacuated object

00:17:55.220 --> 00:17:57.670
to a fixed to-space
area, they're

00:17:57.670 --> 00:18:00.390
copied to a freshly allocated
region called an evacuation

00:18:00.390 --> 00:18:00.890
region.

00:18:06.310 --> 00:18:08.530
If this region speeds up
during garbage collection,

00:18:08.530 --> 00:18:10.410
another region is
allocated and so on

00:18:10.410 --> 00:18:12.690
until the whole heap
has been traced.

00:18:12.690 --> 00:18:14.640
At the end of the GC
cycle, the to-space

00:18:14.640 --> 00:18:16.660
is in union of
all the evacuation

00:18:16.660 --> 00:18:20.590
regions and the region that had
been allocated for new object

00:18:20.590 --> 00:18:23.310
locations.

00:18:23.310 --> 00:18:25.920
Evacuated regions
eventually are reclaimed,

00:18:25.920 --> 00:18:28.830
and their memory pages are
returned to the system.

00:18:31.840 --> 00:18:35.290
Now that we've seen how ART's
concurrent collector works,

00:18:35.290 --> 00:18:40.230
let's have a look at this
generational version.

00:18:40.230 --> 00:18:42.900
Even if the CC collector does
not evacuate all regions,

00:18:42.900 --> 00:18:45.180
it has to trace all the objects.

00:18:45.180 --> 00:18:47.400
This is known as a
full heap collection.

00:18:47.400 --> 00:18:48.870
But consider this.

00:18:48.870 --> 00:18:50.910
If the GC were to
process only recently

00:18:50.910 --> 00:18:53.840
allocated objects, if will in
general collect most of them

00:18:53.840 --> 00:18:58.840
and for a fraction of the cost
of a full heap collection.

00:18:58.840 --> 00:19:00.930
This is what we call the
generational hypothesis.

00:19:00.930 --> 00:19:02.970
In most cases, young
objects are much more

00:19:02.970 --> 00:19:05.640
likely to die than old objects.

00:19:05.640 --> 00:19:08.610
ART's generational CC collector
introducing the concept

00:19:08.610 --> 00:19:12.030
of young generation collection--
only tracing recently

00:19:12.030 --> 00:19:13.660
allocated regions.

00:19:13.660 --> 00:19:15.313
These are called
minor collections

00:19:15.313 --> 00:19:16.980
as opposed to full
heap collections with

00:19:16.980 --> 00:19:19.630
are called major collections.

00:19:19.630 --> 00:19:21.270
This is GC heuristics,
which tries

00:19:21.270 --> 00:19:25.800
to use minor collection first,
and it runs major collections

00:19:25.800 --> 00:19:28.735
if needed.

00:19:28.735 --> 00:19:30.110
In a young generation
collection,

00:19:30.110 --> 00:19:32.210
we handle newly allocated
regions specially.

00:19:32.210 --> 00:19:34.550
These regions are the ones
which have been allocated

00:19:34.550 --> 00:19:36.920
in the previous GC
cycle, and the objects

00:19:36.920 --> 00:19:39.320
are much more likely
to be unreachable.

00:19:39.320 --> 00:19:42.370
Therefore, these regions are
the only one thereby created

00:19:42.370 --> 00:19:44.203
during a minor collection.

00:19:44.203 --> 00:19:45.620
And they are also
the only regions

00:19:45.620 --> 00:19:48.120
which are being traced
during a minor collection.

00:19:48.120 --> 00:19:50.138
Old generation
objects have survived

00:19:50.138 --> 00:19:51.680
at least one
collection, and they are

00:19:51.680 --> 00:19:53.700
more likely to be still alive.

00:19:53.700 --> 00:19:57.200
So they are not traced
during a minor collection.

00:19:57.200 --> 00:20:00.020
Overall, this results in
a much faster GC cycle.

00:20:07.740 --> 00:20:10.980
By tracing young generation
regions from the roots and--

00:20:10.980 --> 00:20:11.798
oh, sorry.

00:20:15.010 --> 00:20:18.500
In order to only trace objects
in the young generation,

00:20:18.500 --> 00:20:20.590
we need to determine
their reachability.

00:20:20.590 --> 00:20:22.330
To do so, we need
to track references

00:20:22.330 --> 00:20:23.800
from the roots to
the young object

00:20:23.800 --> 00:20:25.660
as we did in the full
heap collections,

00:20:25.660 --> 00:20:28.490
but we also need to trace
references from old object

00:20:28.490 --> 00:20:31.600
to young objects because the
old objects do not trace.

00:20:31.600 --> 00:20:37.480
We track old to young object
references using a remember set

00:20:37.480 --> 00:20:42.130
mechanism which keeps
tracks of references,

00:20:42.130 --> 00:20:44.970
objects whose fields have been
modified since the last GC

00:20:44.970 --> 00:20:45.470
cycle.

00:20:50.180 --> 00:20:54.450
Minor recollection-- sorry.

00:20:54.450 --> 00:20:56.380
Yeah, by tracing the
young generations

00:20:56.380 --> 00:20:58.210
from the roots
and remember sets,

00:20:58.210 --> 00:20:59.830
we can identify
live young objects

00:20:59.830 --> 00:21:02.140
and copy them out of the
region without the need

00:21:02.140 --> 00:21:03.530
to trace the whole heap.

00:21:03.530 --> 00:21:07.030
And this is the main benefit
of minor collections.

00:21:07.030 --> 00:21:09.550
Minor collections do not
possess all generation object

00:21:09.550 --> 00:21:12.280
that could have been reclaimed
in a full collection.

00:21:12.280 --> 00:21:14.415
We call that floating garbage.

00:21:14.415 --> 00:21:15.790
They will eventually
be collected

00:21:15.790 --> 00:21:18.340
during the next full
heap collection.

00:21:18.340 --> 00:21:20.565
Let's now see the impact
of generational CC

00:21:20.565 --> 00:21:23.470
on ART's performance.

00:21:23.470 --> 00:21:25.780
To assess the impact of
generational CC collection

00:21:25.780 --> 00:21:27.910
on ART, we've measured
the average time

00:21:27.910 --> 00:21:29.680
spent in the garbage
collector thread

00:21:29.680 --> 00:21:31.930
on some GC intensive benchmarks.

00:21:31.930 --> 00:21:34.270
We used the H2 benchmark
from the [INAUDIBLE] suite,

00:21:34.270 --> 00:21:36.760
which is an in memory
database benchmark,

00:21:36.760 --> 00:21:38.470
as well as our
internal guided sheets

00:21:38.470 --> 00:21:42.340
benchmark because both are
good examples of allocation

00:21:42.340 --> 00:21:45.160
and garbage
collection workloads.

00:21:45.160 --> 00:21:50.290
On average, the generational CC
collector is 38% faster on H2

00:21:50.290 --> 00:21:52.690
and 33% faster on
sheets when compared

00:21:52.690 --> 00:21:54.760
with the non-generational
garbage collector

00:21:54.760 --> 00:21:58.180
that we shipped in Android P.

00:21:58.180 --> 00:22:01.650
This is a direct result
of a younger generation

00:22:01.650 --> 00:22:04.950
collection being cheaper
in term of CPU time.

00:22:04.950 --> 00:22:07.840
So to put it in a nutshell,
using short-live objects

00:22:07.840 --> 00:22:10.840
is now cheaper.

00:22:10.840 --> 00:22:13.480
And also as a consequence
of these improvements,

00:22:13.480 --> 00:22:15.550
we expect that because
we are spending less time

00:22:15.550 --> 00:22:18.580
in the garbage collector, this
will benefit the device battery

00:22:18.580 --> 00:22:20.863
life.

00:22:20.863 --> 00:22:22.280
Improvements in
garbage collection

00:22:22.280 --> 00:22:26.430
often highlight trade offs
between CPU and RAM usage.

00:22:26.430 --> 00:22:28.700
Generational garbage collector
may keep some objects

00:22:28.700 --> 00:22:33.350
a bit longer in order to speed
up the overall GC execution.

00:22:33.350 --> 00:22:37.040
To talk more about how we can
improve these trade and heap

00:22:37.040 --> 00:22:40.737
compaction, I'm handing
it over to Lokesh.

00:22:40.737 --> 00:22:42.070
LOKESH GIDRA: Thank you, Roland.

00:22:47.180 --> 00:22:48.335
Hi, everyone.

00:22:48.335 --> 00:22:49.960
I hope you still have
some stamina left

00:22:49.960 --> 00:22:52.920
for more technical stuff.

00:22:52.920 --> 00:22:56.090
So let's start off with
talking a little bit more

00:22:56.090 --> 00:22:59.540
about how heap
compaction works in CC.

00:22:59.540 --> 00:23:03.200
So as you can imagine,
the more live objects

00:23:03.200 --> 00:23:06.590
there are in a
compacting region,

00:23:06.590 --> 00:23:09.380
the lesser memory
you would reclaim

00:23:09.380 --> 00:23:11.330
if you were to compact it.

00:23:11.330 --> 00:23:13.970
And so it makes
more sense to only--

00:23:13.970 --> 00:23:17.280
for a higher rate of
investment, to only

00:23:17.280 --> 00:23:20.700
compact those regions which have
considerable garbage in them.

00:23:20.700 --> 00:23:23.240
And that's exactly what
concurrent copying collector

00:23:23.240 --> 00:23:24.800
does.

00:23:24.800 --> 00:23:27.540
We maintain a per
region liveness

00:23:27.540 --> 00:23:31.250
stat, which indicates
how much lack of data

00:23:31.250 --> 00:23:33.680
is there in a particular
region and, based on that,

00:23:33.680 --> 00:23:38.300
decide which regions are worth
evacuating and which are not.

00:23:38.300 --> 00:23:40.910
Another interesting fact
about the CC algorithm

00:23:40.910 --> 00:23:44.180
is that, during a
GC cycle, it only

00:23:44.180 --> 00:23:48.170
goes through all the
reachable objects only once.

00:23:48.170 --> 00:23:55.040
While this is a very useful
fact about any GC algorithm

00:23:55.040 --> 00:23:59.360
because it shows how
efficient that algorithm is,

00:23:59.360 --> 00:24:02.720
let's see how it affects
the heap compaction

00:24:02.720 --> 00:24:05.590
decisions that we make.

00:24:05.590 --> 00:24:07.640
So this is a
hypothetical timeline

00:24:07.640 --> 00:24:10.290
of some app's execution.

00:24:10.290 --> 00:24:14.100
And as you can imagine, during
the execution, periodically,

00:24:14.100 --> 00:24:16.110
we need to perform
GC cycles in order

00:24:16.110 --> 00:24:19.210
to reclaim memory which is
consumed by unreachable objects

00:24:19.210 --> 00:24:23.560
so that future object
allocations can be solved.

00:24:23.560 --> 00:24:27.930
And since CC, until Android
P, was not relational, so all

00:24:27.930 --> 00:24:31.200
these GC work on all
the reachable objects.

00:24:31.200 --> 00:24:36.180
And therefore, I am calling
them full heap GC cycles.

00:24:36.180 --> 00:24:40.020
So when me start off
with the first GC cycle,

00:24:40.020 --> 00:24:43.020
by the time we visit all
the reachable objects,

00:24:43.020 --> 00:24:45.510
we know precisely
which region has

00:24:45.510 --> 00:24:47.032
how many live bytes in them.

00:24:47.032 --> 00:24:48.990
And therefore, we can
gather the liveness stats

00:24:48.990 --> 00:24:52.230
that I talked about
in the previous slide.

00:24:52.230 --> 00:24:55.560
As the time goes by, due to
the execution of application

00:24:55.560 --> 00:25:01.030
threads, the heap mutation would
result in more objects becoming

00:25:01.030 --> 00:25:04.050
unreachable.

00:25:04.050 --> 00:25:08.520
However, by the time the next
GC cycle is executed where

00:25:08.520 --> 00:25:11.100
you would actually use the
stats that you gathered

00:25:11.100 --> 00:25:15.990
in the previous GC cycle,
those liveness stats no longer

00:25:15.990 --> 00:25:21.580
show you the liveness
at that point in time.

00:25:21.580 --> 00:25:24.450
This leads to what Roland
earlier described--

00:25:24.450 --> 00:25:28.410
floating garbage, which is
the unreachable objects that

00:25:28.410 --> 00:25:30.010
the garbage collector
knows about,

00:25:30.010 --> 00:25:31.200
but it cannot collect them.

00:25:33.720 --> 00:25:36.060
And of course, as in
the previous GC cycle,

00:25:36.060 --> 00:25:38.280
by the end of this
GC cycle also,

00:25:38.280 --> 00:25:40.350
since we have gone through
all the life objects,

00:25:40.350 --> 00:25:42.630
we gathered the liveness
stats which then

00:25:42.630 --> 00:25:44.310
are used in the next GC cycle.

00:25:44.310 --> 00:25:46.230
And this goes and on.

00:25:46.230 --> 00:25:48.930
This is how decision
making works in Android P.

00:25:48.930 --> 00:25:52.470
Now let's see how introducing
generations into this

00:25:52.470 --> 00:25:58.620
affects the decision making
for region selection.

00:25:58.620 --> 00:26:02.280
As Roland described earlier,
the benefits of generational GC

00:26:02.280 --> 00:26:06.540
is that we can reduce the
number of full heap GC cycles

00:26:06.540 --> 00:26:09.952
by replacing them with more
frequent young collections,

00:26:09.952 --> 00:26:11.910
which are much more
lightweighted, as they only

00:26:11.910 --> 00:26:15.240
work on young objects,
young reachable objects.

00:26:15.240 --> 00:26:17.850
And, therefore,
what this leads to

00:26:17.850 --> 00:26:22.200
is that you would have
less full heap GC cycles

00:26:22.200 --> 00:26:25.890
but also that the time lag
between two full heap GC cycles

00:26:25.890 --> 00:26:28.460
would increase.

00:26:28.460 --> 00:26:33.430
And so, given that, when we
start execution, the first GC

00:26:33.430 --> 00:26:35.530
cycle being full
heap GC cycle, we

00:26:35.530 --> 00:26:39.310
compute the liveness as I
described in the previous case.

00:26:39.310 --> 00:26:44.110
However, every time when we
perform young collection,

00:26:44.110 --> 00:26:47.680
we cannot update these stats
because we only go through

00:26:47.680 --> 00:26:49.180
young objects.

00:26:49.180 --> 00:26:52.640
And, since we don't go through
all the reachable objects,

00:26:52.640 --> 00:26:57.460
It's not possible to
update the liveness stats.

00:26:57.460 --> 00:27:00.970
And, by the time we reach the
next full heap GC cycle where

00:27:00.970 --> 00:27:03.370
we actually use
the liveness stats,

00:27:03.370 --> 00:27:05.930
they become much
more stale than what

00:27:05.930 --> 00:27:10.090
it would have happened in
case of Android P or earlier.

00:27:10.090 --> 00:27:12.850
And therefore, it leads
to a larger amount

00:27:12.850 --> 00:27:15.980
of floating garbage.

00:27:15.980 --> 00:27:18.130
And this goes on and on.

00:27:18.130 --> 00:27:23.420
Given the limited availability
of memory on mobile devices,

00:27:23.420 --> 00:27:27.190
it makes sense to ensure
that this floating

00:27:27.190 --> 00:27:28.950
garbage is minimal.

00:27:28.950 --> 00:27:31.870
And the cause of
that we identified

00:27:31.870 --> 00:27:34.670
is clear-- that the
outdated liveness

00:27:34.670 --> 00:27:37.880
stats is what is hurting us.

00:27:37.880 --> 00:27:42.760
So we fixed the issue by
improving the heap compaction

00:27:42.760 --> 00:27:48.100
by replacing the full heap GC
cycle with another algorithm

00:27:48.100 --> 00:27:51.550
which is two-phased, where
the first phase would

00:27:51.550 --> 00:27:54.730
trace all the live
reachable objects

00:27:54.730 --> 00:27:57.070
and compute the liveness stats.

00:27:57.070 --> 00:28:00.010
And then the second phase, based
on these up-to-date liveness

00:28:00.010 --> 00:28:03.280
stats, figures out
revision, which

00:28:03.280 --> 00:28:07.330
regions deserve to be compacted,
and then performs compaction.

00:28:07.330 --> 00:28:11.230
This way, we reap both the
benefits of generational GC

00:28:11.230 --> 00:28:13.330
by ensuring that young
collection remains

00:28:13.330 --> 00:28:17.380
as it is as well as we fix the
problem of outdated liveness

00:28:17.380 --> 00:28:20.040
stats.

00:28:20.040 --> 00:28:22.020
Let's see, using
the same example

00:28:22.020 --> 00:28:26.680
that Roland was earlier using,
how this new algorithm works.

00:28:26.680 --> 00:28:31.090
So let's assume that our
evacuation criteria get

00:28:31.090 --> 00:28:34.020
a criteria for any
reason is that if it

00:28:34.020 --> 00:28:36.630
has less than three
live objects in it,

00:28:36.630 --> 00:28:38.820
then it deserves
to be compacted.

00:28:38.820 --> 00:28:41.640
So, in this example,
as you can see,

00:28:41.640 --> 00:28:44.250
that all the four
regions which are in use,

00:28:44.250 --> 00:28:48.160
they have less than three
live objects in them.

00:28:48.160 --> 00:28:50.700
So, by the time we finish
the first phase, which

00:28:50.700 --> 00:28:52.890
goes through all the
reachable objects,

00:28:52.890 --> 00:28:57.640
we have determined that all the
four regions have the liveness

00:28:57.640 --> 00:29:00.000
stats less than three objects.

00:29:00.000 --> 00:29:03.630
And therefore, they
deserve to be evacuated.

00:29:03.630 --> 00:29:06.210
And, by the time we finish
our full heap GC cycle,

00:29:06.210 --> 00:29:08.730
we managed to move all of
them into a single region,

00:29:08.730 --> 00:29:14.190
thereby reclaiming all the four
previously used regions back.

00:29:14.190 --> 00:29:19.920
Coming to what should it lead
to in terms of improvement,

00:29:19.920 --> 00:29:21.810
the direct improvement
of this change

00:29:21.810 --> 00:29:24.690
is that after every
full heap GC cycle,

00:29:24.690 --> 00:29:29.130
we should be collecting
more free freed bytes back

00:29:29.130 --> 00:29:31.440
from the garbage collector.

00:29:31.440 --> 00:29:33.600
And, to measure that,
we use the same set

00:29:33.600 --> 00:29:37.200
of applications and benchmark
data Roland earlier described.

00:29:37.200 --> 00:29:39.660
And, as you can
see that, for H2,

00:29:39.660 --> 00:29:44.400
the average freed bytes
improved by more than 178%

00:29:44.400 --> 00:29:48.330
whereas for sheets,
it improved by 68%.

00:29:48.330 --> 00:29:51.180
That's pretty good.

00:29:51.180 --> 00:29:55.680
In terms of impact of both the
GC improvements that Roland

00:29:55.680 --> 00:29:58.530
and I talked about on
the overall improvement

00:29:58.530 --> 00:30:04.500
on the benchmark score, we
again ran some benchmarks.

00:30:04.500 --> 00:30:08.790
And we observed that,
for H2, the score

00:30:08.790 --> 00:30:13.350
improved by more than 15%
for both ARM and ARM64

00:30:13.350 --> 00:30:17.310
whereas for sheets, it
improved by more than 5%.

00:30:17.310 --> 00:30:20.790
All right, enough bragging
about improvement in Q.

00:30:20.790 --> 00:30:23.920
Let me touch a different topic.

00:30:23.920 --> 00:30:26.100
Let's talk about
object allocation

00:30:26.100 --> 00:30:28.980
and object pooling
before I conclude.

00:30:28.980 --> 00:30:32.130
I'm sure many of you must
have experienced this.

00:30:32.130 --> 00:30:36.165
You start writing an app with
standard object allocation,

00:30:36.165 --> 00:30:38.970
you know-- simple way
using new operator.

00:30:38.970 --> 00:30:41.230
But as it grows
more and more, you

00:30:41.230 --> 00:30:44.460
start either observing that
there's just too much GC

00:30:44.460 --> 00:30:47.400
activity going on or you
are spending too much time

00:30:47.400 --> 00:30:49.110
allocating new objects.

00:30:49.110 --> 00:30:54.150
Or maybe just somebody said that
GC is inefficient in general,

00:30:54.150 --> 00:30:56.860
and we should avoid it.

00:30:56.860 --> 00:30:59.200
And then an idea
comes to your mind.

00:30:59.200 --> 00:31:02.260
What if I can cheat
the garbage collector?

00:31:02.260 --> 00:31:04.690
What if, instead of
giving those objects

00:31:04.690 --> 00:31:09.270
once I'm done using them
giving them back to the garbage

00:31:09.270 --> 00:31:13.080
collector, I could just
keep them around in pools

00:31:13.080 --> 00:31:16.770
and use them again and again?

00:31:16.770 --> 00:31:18.570
Isn't it obvious
that this should

00:31:18.570 --> 00:31:21.930
lead to less garbage and
therefore less GC activity

00:31:21.930 --> 00:31:26.010
and also that we would be
avoiding the allocation cost?

00:31:26.010 --> 00:31:29.490
Given that in the
last few years,

00:31:29.490 --> 00:31:33.150
Android's garbage collector
has made several improvements,

00:31:33.150 --> 00:31:36.240
we thought it's nice time
to actually relook into it

00:31:36.240 --> 00:31:41.490
and see if it really makes
sense to pool objects or not.

00:31:41.490 --> 00:31:44.240
So we developed a
micro-benchmark,

00:31:44.240 --> 00:31:48.900
the idea of basically
gathering two sets of metrics.

00:31:48.900 --> 00:31:52.740
One is, what is the
allocation overhead

00:31:52.740 --> 00:31:57.570
for standard allocation versus
acquiring from an object pool?

00:31:57.570 --> 00:32:00.390
And second is that
what is the overall CPU

00:32:00.390 --> 00:32:04.280
overhead of completing
the micro-benchmark?

00:32:04.280 --> 00:32:07.040
So, starting it on
allocation overhead,

00:32:07.040 --> 00:32:09.980
as you can see in this
chart that as you increase

00:32:09.980 --> 00:32:13.040
the number of fields of the
object that you are pooling,

00:32:13.040 --> 00:32:15.590
the overhead of
pooling an object

00:32:15.590 --> 00:32:19.907
versus standard allocation
is almost the same.

00:32:19.907 --> 00:32:21.740
So, from allocation
overheads point of view,

00:32:21.740 --> 00:32:24.900
it actually does not make much
sense to pool the objects.

00:32:27.430 --> 00:32:30.580
Going to the CPU
overhead, this chart

00:32:30.580 --> 00:32:34.420
shows the best case for pooling.

00:32:34.420 --> 00:32:37.030
And, obviously, as
expected, the best case

00:32:37.030 --> 00:32:40.180
was that basically, there is
no garbage collection activity

00:32:40.180 --> 00:32:41.410
at all.

00:32:41.410 --> 00:32:44.170
And therefore, the
total CPU overhead

00:32:44.170 --> 00:32:47.860
of completing the benchmark
in case of pooling

00:32:47.860 --> 00:32:51.190
was lesser than
standard allocation.

00:32:51.190 --> 00:32:53.420
However, there's a catch.

00:32:53.420 --> 00:32:56.830
The catch is that the best
case loses its practicality

00:32:56.830 --> 00:32:59.140
as soon as you go
beyond a test program

00:32:59.140 --> 00:33:01.640
and start having more and
more complicated code.

00:33:01.640 --> 00:33:02.440
Why?

00:33:02.440 --> 00:33:05.170
Because you can't be
pooling every type of

00:33:05.170 --> 00:33:07.030
object in your app.

00:33:07.030 --> 00:33:09.028
You just can't do that.

00:33:09.028 --> 00:33:10.570
And, therefore, we
thought we'll made

00:33:10.570 --> 00:33:13.750
the benchmark more reasonable.

00:33:13.750 --> 00:33:17.050
And so we added constant
short-lived allocations

00:33:17.050 --> 00:33:22.180
in between each iteration
to see how it affects it.

00:33:22.180 --> 00:33:25.690
And as you can see that
the gap between pooling

00:33:25.690 --> 00:33:29.830
versus standard
allocation has reduced.

00:33:29.830 --> 00:33:32.500
We went one step
further, and we made

00:33:32.500 --> 00:33:35.290
that short-lived
allocation proportional

00:33:35.290 --> 00:33:37.900
to the size of the object
that you're pooling.

00:33:37.900 --> 00:33:41.480
And now as you can see,
after a certain size,

00:33:41.480 --> 00:33:47.100
the overhead of pooling is
more than standard allocation.

00:33:47.100 --> 00:33:51.130
And the main reason behind this
is that even though by pooling,

00:33:51.130 --> 00:33:55.870
you are reducing the number of
GC cycles that gets invoked,

00:33:55.870 --> 00:33:57.580
but you are
increasing the memory

00:33:57.580 --> 00:33:59.350
footprint of your program.

00:33:59.350 --> 00:34:03.445
You have more objects each
time to collect or to process.

00:34:05.980 --> 00:34:08.540
And of course, in addition,
to higher footprint,

00:34:08.540 --> 00:34:10.690
there are many other
disadvantages too,

00:34:10.690 --> 00:34:13.210
like you bring back
all the nightmares

00:34:13.210 --> 00:34:16.120
of double free and
memory leaks back.

00:34:16.120 --> 00:34:20.770
And also, given the narrow gap
between standard allocation

00:34:20.770 --> 00:34:24.850
and pooling, it is very critical
that your implementation

00:34:24.850 --> 00:34:26.920
of the pool itself is efficient.

00:34:26.920 --> 00:34:31.000
Otherwise, pooling
would very quickly

00:34:31.000 --> 00:34:34.210
become more
inefficient as compared

00:34:34.210 --> 00:34:36.909
to standard allocation.

00:34:36.909 --> 00:34:39.610
So our recommendation
is that please

00:34:39.610 --> 00:34:42.820
don't use it unless you are
really sure of the benefits.

00:34:42.820 --> 00:34:47.050
And the best way to be sure
of benefits is to measure.

00:34:47.050 --> 00:34:50.590
If measure when you
want to introduce

00:34:50.590 --> 00:34:53.739
a new poll to your program, see
if it benefits you, and then

00:34:53.739 --> 00:34:57.100
only use it.

00:34:57.100 --> 00:34:59.820
However, I still
would say that there

00:34:59.820 --> 00:35:02.070
are some corner cases where
it could be useful, right?

00:35:02.070 --> 00:35:06.730
So for instance, if your
app has a requirement where

00:35:06.730 --> 00:35:10.510
very large objects
need to be used,

00:35:10.510 --> 00:35:12.250
then it may make
sense to pool them.

00:35:12.250 --> 00:35:15.310
Or if there are thread
local objects which

00:35:15.310 --> 00:35:17.740
get allocated and
then drop on the floor

00:35:17.740 --> 00:35:19.720
at a very high
frequency, then also

00:35:19.720 --> 00:35:21.680
it may make sense to pool them.

00:35:21.680 --> 00:35:24.370
But the key point
here is please measure

00:35:24.370 --> 00:35:27.810
before deciding to use them.

00:35:27.810 --> 00:35:33.760
OK, so, to recap, we talked
about how we improved the app

00:35:33.760 --> 00:35:37.450
startup time by
using cloud profiles,

00:35:37.450 --> 00:35:42.100
by improving the app images, and
by introducing the app process

00:35:42.100 --> 00:35:43.540
pools.

00:35:43.540 --> 00:35:46.990
On the GC's front, we already
have faster object allocation

00:35:46.990 --> 00:35:49.450
since Android Oreo.

00:35:49.450 --> 00:35:51.640
And, starting with
Q, we have introduced

00:35:51.640 --> 00:35:53.890
the concept of generational
garbage collection

00:35:53.890 --> 00:35:56.830
and, also, we improved the heap
compaction using the two phase

00:35:56.830 --> 00:35:58.900
algorithm that I talked about.

00:35:58.900 --> 00:36:01.330
And since it is our
endeavor to always keep

00:36:01.330 --> 00:36:05.390
improving the ART's
performance, we

00:36:05.390 --> 00:36:07.853
will continue to
work on these areas.

00:36:07.853 --> 00:36:10.270
And you can expect to have
more improvements in the future

00:36:10.270 --> 00:36:12.400
to follow.

00:36:12.400 --> 00:36:15.170
With that, thanks a lot for
attending the presentation.

00:36:15.170 --> 00:36:17.190
I hope it was useful.

