WEBVTT
Kind: captions
Language: en

00:00:00.390 --> 00:00:02.910
All right.
Now, follow me carefully, all right.

00:00:03.950 --> 00:00:05.090
u is some vector.

00:00:05.090 --> 00:00:06.090
Which vector?

00:00:06.090 --> 00:00:08.039
u, not me, u.

00:00:08.039 --> 00:00:13.610
A matrix times a vector, since
everything is linear, if I describe u

00:00:13.610 --> 00:00:18.410
as being made up of a bunch of
components that add up to u,

00:00:18.410 --> 00:00:21.940
it's the same thing as sigma times
those bunch of those components.

00:00:21.940 --> 00:00:29.370
So in particular, the components could
be the eigenvectors of sigma, so I could

00:00:29.370 --> 00:00:32.980
express u to be eigenvector of sigma
point to whatever direction they point.

00:00:32.980 --> 00:00:37.200
I can think of u as being made up
of the some of those, all right?

00:00:37.200 --> 00:00:40.890
So u is a unit vector.

00:00:40.890 --> 00:00:43.050
Okay, it can be expressed in term and

00:00:43.050 --> 00:00:46.900
it's what it says the linear sum
of those eigenvectors of sigma.

00:00:48.390 --> 00:00:52.170
The question is which of those
directions would return me the largest

00:00:52.170 --> 00:00:54.400
value, okay?

00:00:54.400 --> 00:00:57.560
Well, we know that

00:00:57.560 --> 00:01:02.130
sigma times an eigenvector is
equal to lambda eigenvector.

00:01:02.130 --> 00:01:05.459
If I'm going to pick a single
direction that will return the largest

00:01:05.459 --> 00:01:08.740
vector back, and then I'm going to
take the, the dot product itself to

00:01:08.740 --> 00:01:12.895
get the magnitude, I would need the
eigenvector with the largest eigenvalue.

00:01:14.020 --> 00:01:15.100
And at the heart of it,

00:01:15.100 --> 00:01:19.860
that That's actually why the eigenvector
with the largest eigenvalue

00:01:19.860 --> 00:01:24.090
is the direction along which
you get the greatest variance.

00:01:24.090 --> 00:01:26.800
So, you can either spend a little
time thinking about that and

00:01:26.800 --> 00:01:29.910
understand why that's true or
you can believe me.

00:01:29.910 --> 00:01:32.340
I guess I could have written it
out more, but I think most of you,

00:01:32.340 --> 00:01:35.890
at this point know that I take the, the
eigenvector, the largest eigenvalue of

00:01:35.890 --> 00:01:39.870
the covariance matrix, that's
the direction of the greatest variation.

00:01:39.870 --> 00:01:43.250
So that's what this says here, that
the direction that captures the maximum

00:01:43.250 --> 00:01:46.720
covariance of the data
is the eigenvector

00:01:46.720 --> 00:01:51.515
corresponding to the largest eigenvalue
of the data covariance matrix.

00:01:51.515 --> 00:01:53.700
Ta-da, that's what we're doing.

00:01:54.780 --> 00:01:59.450
So, furthermore, that's the longest one,
I can take the top, I don't know,

00:01:59.450 --> 00:02:00.640
k of them.

00:02:00.640 --> 00:02:03.660
Orthogonal direct, and
they're all orthogonal to each other.

00:02:03.660 --> 00:02:08.830
And that will capture the most variation
that I can, of k orthogonal vectors.

00:02:08.830 --> 00:02:11.474
So I can say well I've got
this 10,000 dimensional thing.

00:02:11.474 --> 00:02:14.190
Maybe I'll take the top 10, 20, 30, 40.

00:02:14.190 --> 00:02:19.093
That captures the, as much
variation as I can with 10, 20, 30,

00:02:19.093 --> 00:02:20.989
40 orthogonal vectors.

00:02:20.989 --> 00:02:24.690
So that's what I, I want to use.

00:02:24.690 --> 00:02:25.966
But in order to do that,

00:02:25.966 --> 00:02:29.680
in order to do PCA in images,
on images I have to teach you a trick.

00:02:29.680 --> 00:02:33.490
And this is the PCA for d much,
much, much bigger than m or n trick.

00:02:33.490 --> 00:02:34.440
All right.

00:02:35.690 --> 00:02:37.380
And so let's do the trick.

