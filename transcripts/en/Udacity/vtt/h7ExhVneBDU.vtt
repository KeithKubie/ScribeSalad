WEBVTT
Kind: captions
Language: en

00:00:00.272 --> 00:00:02.825
All right, so let's do a brief aside about the

00:00:02.825 --> 00:00:07.046
[LAUGH], this is like a bad history of reinforcement learning so

00:00:07.046 --> 00:00:08.840
it's not that it's a bad history, it's just badly

00:00:08.840 --> 00:00:11.730
told. So but the basic idea is this, once upon a

00:00:11.730 --> 00:00:15.459
time and we're going to call it, say the 1930s people

00:00:15.459 --> 00:00:18.390
observed. That if you put an animal, this is supposed to

00:00:18.390 --> 00:00:22.170
be a rat, in, in a, you know, box, say. And

00:00:22.170 --> 00:00:25.930
give it choices of looking in place B and place A

00:00:25.930 --> 00:00:27.550
and, say, one of them has cheese in it and the other one

00:00:27.550 --> 00:00:30.910
doesn't, but it can't see which because, you know, the doors are closed.

00:00:30.910 --> 00:00:34.340
And and let's say that we, we consistently do something like this. We

00:00:34.340 --> 00:00:38.270
turn on a red light whenever the cheese is in the A place. And

00:00:38.270 --> 00:00:41.370
we turn on a blue light whenever the cheese is in the, B

00:00:41.370 --> 00:00:44.840
place. And what you observe is if you do this consistently, then what you

00:00:44.840 --> 00:00:47.620
find is that if the animal sees the stimulus like the red light

00:00:47.620 --> 00:00:51.190
going on and it takes an action like peeking inside the, the or sticking

00:00:51.190 --> 00:00:53.830
it's head inside the box A and it gets

00:00:53.830 --> 00:00:55.750
a reward which in this case would be getting

00:00:55.750 --> 00:00:59.218
to eat some cheese. That that set up strengthens

00:00:59.218 --> 00:01:02.060
its response to the stimulus. In other words, in the

00:01:02.060 --> 00:01:04.129
future when it sees the red light it's going to

00:01:04.129 --> 00:01:06.580
be more likely to go in and look in box

00:01:06.580 --> 00:01:09.880
A. And this notion of strengthening, you can think

00:01:09.880 --> 00:01:13.290
of it as reinforcing the connection. Reinforcing just means strengthening.

00:01:13.290 --> 00:01:14.635
&gt;&gt; Hm.

00:01:14.635 --> 00:01:16.370
&gt;&gt; So, this was studied for a long time and there's,

00:01:16.370 --> 00:01:18.320
there's tremendous amount of interesting research

00:01:18.320 --> 00:01:19.730
about this, but if you start to,

00:01:19.730 --> 00:01:23.190
you know, think about it as a computer scientist. A natural way of, of

00:01:23.190 --> 00:01:25.920
kind of thinking about what this problem is, is that you know, a

00:01:25.920 --> 00:01:29.290
stimulus is kind of like a state, and an action is kind of like

00:01:29.290 --> 00:01:32.042
an action, and a reward, well, I kind of stole all these words

00:01:32.042 --> 00:01:34.990
but it's kind of like a reward. And it leads you to this idea

00:01:34.990 --> 00:01:38.160
that what you really want to do. Is figure out how to choose

00:01:38.160 --> 00:01:41.622
actions to maximize reward as a function of the state. And so we started

00:01:41.622 --> 00:01:45.067
to take this concept and we called it reinforcement learning because that's what

00:01:45.067 --> 00:01:47.270
the psychologists were calling it. But,

00:01:47.270 --> 00:01:49.530
for us it just means reward maximization.

00:01:49.530 --> 00:01:52.910
This notion of strengthening is really not part of the story for us

00:01:52.910 --> 00:01:56.300
at all. So, we're really taking this word and using it, you know, wrong.

00:01:56.300 --> 00:01:58.910
&gt;&gt; Hm. Well, that all makes sense except

00:01:58.910 --> 00:02:01.800
for one thing, which is the idea that this

00:02:01.800 --> 00:02:03.940
happened in the 1930s. Because we all know

00:02:03.940 --> 00:02:06.300
that rat dinosaurs did not exist in the 1930s.

00:02:07.640 --> 00:02:09.380
&gt;&gt; Yeah, they, well they went extinct in the 40s.

00:02:09.380 --> 00:02:10.080
&gt;&gt; Hm, hm.

00:02:11.270 --> 00:02:15.200
&gt;&gt; There's this one little epilogue to the story which I find really kind of

00:02:15.200 --> 00:02:17.900
amusing, and that is the computer scientists did

00:02:17.900 --> 00:02:20.510
pretty well at figuring out algorithms for solving

00:02:20.510 --> 00:02:23.480
problems like this. And the psychologists really do

00:02:23.480 --> 00:02:27.810
care about how stimuli and, and, motor actions

00:02:27.810 --> 00:02:30.660
and reward all Interact with each other and

00:02:30.660 --> 00:02:33.010
they've turned to the computer scientists to say,

00:02:33.010 --> 00:02:35.854
how might the brain be doing this? What, what is the problem that

00:02:35.854 --> 00:02:38.140
the brain actually might be solving? And

00:02:38.140 --> 00:02:40.160
so, guess what word [LAUGH] they borrowed

00:02:40.160 --> 00:02:44.520
back when they started to think about these things. Reinforcement. So, now they

00:02:44.520 --> 00:02:46.330
talk about reinforcement learning and they don't

00:02:46.330 --> 00:02:49.490
mean reinforcement, either. They mean reward maximization.

00:02:49.490 --> 00:02:50.105
&gt;&gt; So we won.

00:02:50.105 --> 00:02:53.437
&gt;&gt; [LAUGH] Yeah, I do not think that was really our plan but

00:02:53.437 --> 00:02:57.340
but it's, it's nice to know that we had some you know, impact.

00:02:57.340 --> 00:02:58.071
&gt;&gt; Well, that's very

00:02:58.071 --> 00:02:59.662
good you brought planning back into it and so

00:02:59.662 --> 00:03:02.105
they learned because of our plan, that's pretty good Michael.

00:03:02.105 --> 00:03:04.758
&gt;&gt; [LAUGH] All right, so that's the end of this aside.

00:03:04.758 --> 00:03:05.630
&gt;&gt; Okay.

00:03:05.630 --> 00:03:07.330
&gt;&gt; Let's go, let's go, let's go back to learning things.

