WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.260
ALEX PASSOS: My name is Alex.

00:00:01.260 --> 00:00:03.134
And I'm here to tell
you about something cool

00:00:03.134 --> 00:00:05.220
that we've been working
on in the TensorFlow team

00:00:05.220 --> 00:00:07.080
for the past few months.

00:00:07.080 --> 00:00:09.820
This is mostly work done by
Google Brain in Cambridge.

00:00:09.820 --> 00:00:13.800
And I'm here presenting because,
well, I'm not in Cambridge.

00:00:13.800 --> 00:00:16.140
So this project is
called AutoGraph.

00:00:16.140 --> 00:00:18.300
And what it is, is
essentially, a Python compiler

00:00:18.300 --> 00:00:20.470
that creates TensorFlow
graphs for you.

00:00:20.470 --> 00:00:24.400
So I'm sure, since you
walked into this room,

00:00:24.400 --> 00:00:25.950
you probably used
TensorFlow before.

00:00:25.950 --> 00:00:28.710
And you might have some
idea about how it works.

00:00:28.710 --> 00:00:30.300
And the way it works,
under the hood,

00:00:30.300 --> 00:00:33.330
is you build this graph to
represent your computation.

00:00:33.330 --> 00:00:36.700
And here, there is a
multiply, an add, and a relu.

00:00:36.700 --> 00:00:38.310
This is a very simple
neural network.

00:00:38.310 --> 00:00:41.060
And the nice thing about
having a TensorFlow graph

00:00:41.060 --> 00:00:43.090
is that once you
have this graph,

00:00:43.090 --> 00:00:44.730
you can deploy it to a server.

00:00:44.730 --> 00:00:46.360
You can train it on a TPU.

00:00:46.360 --> 00:00:48.965
You can run it on your mobile
phone with TensorFlow.js.

00:00:48.965 --> 00:00:50.340
You can run it on
a Raspberry Pi.

00:00:50.340 --> 00:00:51.910
There's all sorts of
things you can do with it.

00:00:51.910 --> 00:00:54.240
You have-- we have compilers
that take these graphs

00:00:54.240 --> 00:00:55.620
and produce
highly-optimized code

00:00:55.620 --> 00:00:57.690
for all sorts of platforms.

00:00:57.690 --> 00:01:00.180
The downside of this
graph is that you end up

00:01:00.180 --> 00:01:04.349
writing code like this, which
you see five lines of code,

00:01:04.349 --> 00:01:06.780
one of which has the
actual computation that you

00:01:06.780 --> 00:01:07.680
want to run.

00:01:07.680 --> 00:01:11.580
And in case you're too far
to read, it's the fourth one.

00:01:11.580 --> 00:01:14.207
Everything else is boilerplate.

00:01:14.207 --> 00:01:16.290
And some of this boilerplate
is easy to deal with,

00:01:16.290 --> 00:01:17.790
like these place holders.

00:01:17.790 --> 00:01:20.740
But some of the
boilerplate is not.

00:01:20.740 --> 00:01:24.480
For example, if you want
a TensorFlow model that

00:01:24.480 --> 00:01:26.550
has control flow
inside the model,

00:01:26.550 --> 00:01:29.996
you have to take your
mental idea of what code

00:01:29.996 --> 00:01:31.620
you want to write,
which probably looks

00:01:31.620 --> 00:01:35.160
like the example on the left,
where you have a Python "if,"

00:01:35.160 --> 00:01:38.370
and turn it into the code
on the right, which is what

00:01:38.370 --> 00:01:39.774
we need to generate a graph.

00:01:39.774 --> 00:01:41.190
And the reason why
we need this is

00:01:41.190 --> 00:01:44.715
that our computation graph,
because it can be put on a TPU,

00:01:44.715 --> 00:01:46.860
and put on a phone,
and run independently

00:01:46.860 --> 00:01:48.510
of the code that
produced this graph,

00:01:48.510 --> 00:01:50.437
has to have information
about both branches

00:01:50.437 --> 00:01:51.270
of your conditional.

00:01:51.270 --> 00:01:53.940
So we need to somehow tell
TensorFlow everything that

00:01:53.940 --> 00:01:55.974
could possibly happen
in this program.

00:01:55.974 --> 00:01:57.390
And so you end up
with code that's

00:01:57.390 --> 00:02:00.730
very redundant in this way.

00:02:00.730 --> 00:02:02.512
And we have answers for this.

00:02:02.512 --> 00:02:04.470
And we're working hard
to make your life easier

00:02:04.470 --> 00:02:05.790
if you want to use TensorFlow.

00:02:05.790 --> 00:02:08.280
One project I've
spent a lot of effort

00:02:08.280 --> 00:02:10.139
personally on is
eager execution.

00:02:10.139 --> 00:02:11.850
And the idea of
eager execution is

00:02:11.850 --> 00:02:13.800
to make your life
easier by saying,

00:02:13.800 --> 00:02:15.540
you don't have to
do graphs at all.

00:02:15.540 --> 00:02:18.180
And you can take
this idea fairly far.

00:02:18.180 --> 00:02:20.200
But eventually, you
hit some limits.

00:02:20.200 --> 00:02:20.700
Right?

00:02:20.700 --> 00:02:22.621
If you're-- with eager
execution enabled,

00:02:22.621 --> 00:02:24.120
you have a very
intuitive interface.

00:02:24.120 --> 00:02:25.590
You just write Python code.

00:02:25.590 --> 00:02:27.147
You use Python
control flow, Python

00:02:27.147 --> 00:02:29.730
infrastructures-- all the things
you're already familiar with.

00:02:29.730 --> 00:02:30.660
And it's nice.

00:02:30.660 --> 00:02:34.650
But you don't get this automatic
parallelism and distribution

00:02:34.650 --> 00:02:36.420
and scale that Frank
was telling you

00:02:36.420 --> 00:02:38.910
about with all the TPU
pods, because you still

00:02:38.910 --> 00:02:40.530
have to run Python code.

00:02:40.530 --> 00:02:42.210
And processors aren't
getting faster.

00:02:42.210 --> 00:02:45.030
And Python is inherently
single-threaded.

00:02:45.030 --> 00:02:48.330
We have a few ways of
getting around this.

00:02:48.330 --> 00:02:50.430
Like, from eager
execution, we have

00:02:50.430 --> 00:02:53.440
defun-- this primitive that
lets you build little graphlets

00:02:53.440 --> 00:02:54.330
and run them.

00:02:54.330 --> 00:02:57.407
So within that graphlet,
you have all the performance

00:02:57.407 --> 00:02:57.990
of graph mode.

00:02:57.990 --> 00:03:01.320
But you still can have eager
execution in your outer loop.

00:03:01.320 --> 00:03:02.820
We can also have
py_func, which lets

00:03:02.820 --> 00:03:06.540
you have full graph code
that can run on clusters

00:03:06.540 --> 00:03:09.630
of multiple GPUs,
but with eager code

00:03:09.630 --> 00:03:12.660
in the middle that's doing some
complicated control flow stuff.

00:03:12.660 --> 00:03:14.400
And these tools are useful.

00:03:14.400 --> 00:03:16.650
Both defun and py_func
are differentiable.

00:03:16.650 --> 00:03:18.480
They run on all
sorts of devices.

00:03:18.480 --> 00:03:20.500
They give you a
lot of flexibility.

00:03:20.500 --> 00:03:23.460
But it's a little
unsatisfying that you have

00:03:23.460 --> 00:03:24.637
these seams in your program.

00:03:24.637 --> 00:03:26.970
And you need to think about,
what's going to be a graph?

00:03:26.970 --> 00:03:28.810
What's not going to be a graph?

00:03:28.810 --> 00:03:30.970
What if we could do
better than this?

00:03:30.970 --> 00:03:34.250
And AutoGraph is this tool
that we've been working on,

00:03:34.250 --> 00:03:36.000
in the Google Brain
Cambridge office, that

00:03:36.000 --> 00:03:37.690
tries to do better than this.

00:03:37.690 --> 00:03:41.220
And what it lets you do is
write eager-style, Python code

00:03:41.220 --> 00:03:46.020
and have a compiler turn
this into the boring, very

00:03:46.020 --> 00:03:48.570
complicated, graph-style code.

00:03:48.570 --> 00:03:50.920
Because if you think
with me for a second,

00:03:50.920 --> 00:03:52.620
if you look at the
transformations that

00:03:52.620 --> 00:03:55.200
need to happen to turn
the code on the left

00:03:55.200 --> 00:03:58.020
into the code on the right,
that looks like something

00:03:58.020 --> 00:03:58.980
that can be automated.

00:03:58.980 --> 00:04:00.960
And AutoGraph is,
indeed, the thing that

00:04:00.960 --> 00:04:02.730
automates this transformation.

00:04:02.730 --> 00:04:04.650
It's pretty easy to use now.

00:04:04.650 --> 00:04:05.970
It's in TensorFlow.contrib.

00:04:05.970 --> 00:04:09.450
And when 2.0 comes out,
it will be in Core.

00:04:09.450 --> 00:04:11.410
You just import
AutoGraph from contrib.

00:04:11.410 --> 00:04:14.160
And you decorate your function
with autograph.convert.

00:04:14.160 --> 00:04:16.260
Then, when you call
it, under the hood,

00:04:16.260 --> 00:04:20.010
AutoGraph will generate and
run this more complicated code

00:04:20.010 --> 00:04:21.149
for you.

00:04:21.149 --> 00:04:25.500
And these are not
academic concerns.

00:04:25.500 --> 00:04:28.180
There's a lot of code out there
that has control flow in it.

00:04:28.180 --> 00:04:30.330
So for example,
the implementation

00:04:30.330 --> 00:04:34.110
of dynamic RNN in
TensorFlow is very hard

00:04:34.110 --> 00:04:36.630
to read if you're not familiar
with how TensorFlow works.

00:04:36.630 --> 00:04:38.740
But I can write one,
here, in the slide.

00:04:38.740 --> 00:04:40.380
And it's pretty easy.

00:04:40.380 --> 00:04:43.170
If you squint, you
have a "for" loop

00:04:43.170 --> 00:04:47.730
that loops over my sequential
data, applies an RNN cell,

00:04:47.730 --> 00:04:49.890
and has some logic in
it to mask the outputs,

00:04:49.890 --> 00:04:52.335
because the different
sequences in the minibatch

00:04:52.335 --> 00:04:53.460
can have different lengths.

00:04:53.460 --> 00:04:54.510
But this is not--

00:04:54.510 --> 00:04:56.010
this is 11 lines of code.

00:04:56.010 --> 00:04:57.630
There's no magic here.

00:04:57.630 --> 00:04:59.130
But if you run it
through AutoGraph,

00:04:59.130 --> 00:05:03.750
you get the actual code that
you see inside tf.dynamic_rnn,

00:05:03.750 --> 00:05:06.980
which is substantially
harder to read.

00:05:06.980 --> 00:05:08.480
And not just RNNs--

00:05:08.480 --> 00:05:11.180
in this-- I mean, this RNN
is a basic building block

00:05:11.180 --> 00:05:13.800
that we use in all sorts of
sequence-to-sequence models,

00:05:13.800 --> 00:05:16.175
when we're generating text,
when we're generating images,

00:05:16.175 --> 00:05:18.790
when we're turning
data into other data.

00:05:18.790 --> 00:05:20.630
And this is just one
example of this kind

00:05:20.630 --> 00:05:24.380
of control-flow-rich programs
that you often want to write,

00:05:24.380 --> 00:05:27.320
but that writing in
normal Graph TensorFlow

00:05:27.320 --> 00:05:29.900
can be more painful
than it needs to be.

00:05:29.900 --> 00:05:33.710
So what AutoGraph supports is
a subset of the Python language

00:05:33.710 --> 00:05:36.950
that covers what I hope
is what you most need

00:05:36.950 --> 00:05:39.170
to run in a TensorFlow program.

00:05:39.170 --> 00:05:40.570
So we can do print.

00:05:40.570 --> 00:05:42.320
And I don't know how
many of you have ever

00:05:42.320 --> 00:05:46.800
failed to use tf.print,
because I do about once a week.

00:05:46.800 --> 00:05:48.680
It's really hard, right?

00:05:48.680 --> 00:05:51.650
Also tf.assert-- it's very
easy to have your assertion

00:05:51.650 --> 00:05:54.980
silently dropped in the void.

00:05:54.980 --> 00:05:56.360
AutoGraph is also composible.

00:05:56.360 --> 00:05:58.100
So you can have
arbitrary Python code,

00:05:58.100 --> 00:06:00.610
with classes and functions
and complicated call trees.

00:06:00.610 --> 00:06:03.260
And it will turn the whole
thing into a graph for you.

00:06:03.260 --> 00:06:05.930
You can have nested control
flow, with breaks and continues

00:06:05.930 --> 00:06:07.400
and all sorts of stuff.

00:06:07.400 --> 00:06:09.020
And we can make that work.

00:06:09.020 --> 00:06:12.180
And we're still working
on it to make it better.

00:06:12.180 --> 00:06:15.390
So another nice
example, I think,

00:06:15.390 --> 00:06:18.530
to visualize what AutoGraph is
actually doing, is using what

00:06:18.530 --> 00:06:21.710
has been unanimously decided
to be the best interview

00:06:21.710 --> 00:06:24.290
question in the history of
humanity, which is FizzBuzz--

00:06:24.290 --> 00:06:26.255
this thing where you
loop over the numbers.

00:06:26.255 --> 00:06:28.880
And you print "Fizz" for some of
them, "Buzz" for some of them,

00:06:28.880 --> 00:06:30.470
"FizzBuzz" for some of them.

00:06:30.470 --> 00:06:32.420
And otherwise, you just
increment your counter

00:06:32.420 --> 00:06:33.620
and print the number.

00:06:33.620 --> 00:06:36.080
And you can see this is like
10 lines of Python code,

00:06:36.080 --> 00:06:37.390
pretty straightforward.

00:06:37.390 --> 00:06:39.980
But you should try to turn
it into in TensorFlow code.

00:06:39.980 --> 00:06:42.020
And we ran it through
AutoGraph, because I'm too

00:06:42.020 --> 00:06:44.450
lazy to write this code myself.

00:06:44.450 --> 00:06:47.090
It's not pretty.

00:06:47.090 --> 00:06:48.920
And if you think this
is it, this is not.

00:06:48.920 --> 00:06:52.100
This is it [LAUGHS].

00:06:52.100 --> 00:06:53.990
So this all looks nice.

00:06:53.990 --> 00:06:57.290
But I think you would be remiss
to believe me if you didn't

00:06:57.290 --> 00:06:58.745
understand how this works.

00:06:58.745 --> 00:07:00.620
So I want to spend a
few minutes and tell you

00:07:00.620 --> 00:07:03.870
what we're actually doing
to make this possible.

00:07:03.870 --> 00:07:06.980
And there's many ways
of conceptualizing this.

00:07:06.980 --> 00:07:10.040
But I think, really,
the core of AutoGraph

00:07:10.040 --> 00:07:12.950
is we're extending the notion
of what's possible with operator

00:07:12.950 --> 00:07:13.450
overloading.

00:07:13.450 --> 00:07:15.491
And I don't know if you're
familiar with operator

00:07:15.491 --> 00:07:16.280
overloading.

00:07:16.280 --> 00:07:19.490
It's a feature in Python
that lets you customize

00:07:19.490 --> 00:07:20.960
a little bit of the language.

00:07:20.960 --> 00:07:22.550
And we use this
heavily in TensorFlow.

00:07:22.550 --> 00:07:28.970
So for example, when you
write c = a * b in TensorFlow,

00:07:28.970 --> 00:07:31.940
we don't actually run the
Python multiplication operator.

00:07:31.940 --> 00:07:34.520
What actually runs is
the thing on the right,

00:07:34.520 --> 00:07:38.030
c = tf.multiply(a, b).

00:07:38.030 --> 00:07:40.640
And the way we turn one--
the code on the left

00:07:40.640 --> 00:07:43.880
into the code on the right is
by having this Tensor class,

00:07:43.880 --> 00:07:45.800
where a and b are tensors.

00:07:45.800 --> 00:07:51.050
This Tensor class defines this
_ _nul_ _ method, which Python,

00:07:51.050 --> 00:07:53.990
then, when it sees the
operator, it rewrites to a call

00:07:53.990 --> 00:07:55.670
to the multiply method.

00:07:55.670 --> 00:07:57.830
And then, we can use
this multiply method

00:07:57.830 --> 00:08:01.930
to make whatever code we want
run when you type the star.

00:08:01.930 --> 00:08:03.770
And this is pretty
straightforward

00:08:03.770 --> 00:08:06.590
and shouldn't be
big of a surprise.

00:08:06.590 --> 00:08:10.850
But what Python doesn't
let you do is this.

00:08:10.850 --> 00:08:12.410
There's no way in
Python to override

00:08:12.410 --> 00:08:14.600
the behavior of
the "if" operator

00:08:14.600 --> 00:08:16.680
to run some code
you want to run.

00:08:16.680 --> 00:08:20.420
So ideally, if Python
let us override _ _if_ _,

00:08:20.420 --> 00:08:24.410
we would be able to make all
sorts of graph rewrites that

00:08:24.410 --> 00:08:26.060
take control flow
into consideration.

00:08:26.060 --> 00:08:29.000
Sadly, we can't.

00:08:29.000 --> 00:08:32.570
So what we're trying
to do, really,

00:08:32.570 --> 00:08:35.090
is overwrite the syntax
that Python doesn't

00:08:35.090 --> 00:08:36.880
let us override the syntax.

00:08:36.880 --> 00:08:41.330
And the way we do this is we
read the Python abstract syntax

00:08:41.330 --> 00:08:42.979
tree of your code.

00:08:42.979 --> 00:08:45.020
We process it using Python's
[INAUDIBLE] modules.

00:08:45.020 --> 00:08:48.230
And we just run a bunch of
standard compiler passes.

00:08:48.230 --> 00:08:50.507
We rewrite loops so that
they have a single exit.

00:08:50.507 --> 00:08:52.340
We capture all the
variables that you assign

00:08:52.340 --> 00:08:53.709
in loops and conditionals.

00:08:53.709 --> 00:08:56.000
We unify the variables that
you mutate in either branch

00:08:56.000 --> 00:08:56.708
of a conditional.

00:08:56.708 --> 00:08:59.330
We do all these
boring transformations

00:08:59.330 --> 00:09:02.120
that need to happen, so that we
can take the code that you want

00:09:02.120 --> 00:09:05.420
to write, that has
advanced control flow,

00:09:05.420 --> 00:09:07.730
into the code that TensorFlow
wants you to write.

00:09:07.730 --> 00:09:10.625
And if you've been
following me so far,

00:09:10.625 --> 00:09:12.500
you might have a question
in your head, which

00:09:12.500 --> 00:09:15.500
is that, if you've written
TensorFlow program--

00:09:15.500 --> 00:09:16.624
AutoGraph didn't exist.

00:09:16.624 --> 00:09:17.540
You look at your code.

00:09:17.540 --> 00:09:19.940
It's full of "ifs," and
"while" loops, and "for" loops,

00:09:19.940 --> 00:09:20.520
and things like that.

00:09:20.520 --> 00:09:22.478
And you probably don't
want any of those things

00:09:22.478 --> 00:09:24.980
to end up in a graph,
because a lot of this

00:09:24.980 --> 00:09:26.210
is just configuration.

00:09:26.210 --> 00:09:27.830
You're choosing what
optimizer to use.

00:09:27.830 --> 00:09:30.110
Or you're choosing how many
layers your network has.

00:09:30.110 --> 00:09:33.800
And you have-- these things are
very easy to express in loops.

00:09:33.800 --> 00:09:37.220
But you might think
that if we rewrite

00:09:37.220 --> 00:09:38.900
every loop and
conditional in your code

00:09:38.900 --> 00:09:40.640
to be a TensorFlow
loop or conditional,

00:09:40.640 --> 00:09:42.310
you're going to end up with
all those things in your graph.

00:09:42.310 --> 00:09:43.370
And you don't.

00:09:43.370 --> 00:09:45.170
And the way you don't
is pretty clever,

00:09:45.170 --> 00:09:47.420
which is that we use
Python's dynamism

00:09:47.420 --> 00:09:49.730
to help us instead of hurt us.

00:09:49.730 --> 00:09:52.640
So when we rewrite the--

00:09:52.640 --> 00:09:57.710
in our operator overloaded logic
with _ _if_ _ and _ _while_ _,

00:09:57.710 --> 00:09:59.990
we don't call tf.cond.

00:09:59.990 --> 00:10:02.420
We call something
that, in spirit, looks

00:10:02.420 --> 00:10:05.060
like this function, where
if you pass at a tensor,

00:10:05.060 --> 00:10:06.400
it runs tf.cond.

00:10:06.400 --> 00:10:08.510
But if you pass at something
that's not a tensor,

00:10:08.510 --> 00:10:10.560
it runs a normal
Python conditional.

00:10:10.560 --> 00:10:13.256
So if you have code that's
not TensorFlow code,

00:10:13.256 --> 00:10:14.630
and you run it
through AutoGraph,

00:10:14.630 --> 00:10:17.240
we should preserve its meaning,
preserve its semantics.

00:10:17.240 --> 00:10:18.212
So it should be safe.

00:10:18.212 --> 00:10:19.670
And you can trust
that it will only

00:10:19.670 --> 00:10:22.542
change the behavior of code that
you want the behavior to change

00:10:22.542 --> 00:10:25.000
because it's doing something
that you're not allowed to do,

00:10:25.000 --> 00:10:27.291
like have an "if" that depends
on the value of a tensor

00:10:27.291 --> 00:10:29.060
at graph build time.

00:10:29.060 --> 00:10:33.450
And to get there, we had to do
a lot of work, and a lot of work

00:10:33.450 --> 00:10:35.880
that, as I was saying,
is the same work

00:10:35.880 --> 00:10:37.440
that any compiler has to do.

00:10:37.440 --> 00:10:39.450
We figure out what
variables we're using.

00:10:39.450 --> 00:10:43.170
We rewrite things in a single
static assignment form,

00:10:43.170 --> 00:10:45.750
so that we can handle
things like breaks,

00:10:45.750 --> 00:10:48.900
and continues, and breaks
inside "ifs," and functions

00:10:48.900 --> 00:10:50.520
with multiple return points.

00:10:50.520 --> 00:10:52.694
Because the core TensorFlow
syntax doesn't let

00:10:52.694 --> 00:10:53.610
you have those things.

00:10:53.610 --> 00:10:57.560
But thankfully, that is
just syntactic sugar.

00:10:57.560 --> 00:11:00.900
It's just simple transformations
that you can do on

00:11:00.900 --> 00:11:03.175
the-- you can remove all
these features by doing

00:11:03.175 --> 00:11:04.800
simple transformations
on the code that

00:11:04.800 --> 00:11:06.300
do not affect its meaning.

00:11:06.300 --> 00:11:08.940
And in AutoGraph, we do those
transformations for you.

00:11:08.940 --> 00:11:11.550
So you can really write the
code that you want to write.

00:11:11.550 --> 00:11:14.830
And we'll try to run
it as best as we can.

00:11:14.830 --> 00:11:18.930
So we do break and continue,
inline "if" expressions.

00:11:18.930 --> 00:11:21.540
We're working on
list comprehensions.

00:11:21.540 --> 00:11:24.470
We have basic support
for "for" loops.

00:11:24.470 --> 00:11:27.010
We let you do multiple
return statements.

00:11:27.010 --> 00:11:28.740
We also de-sugar "for"
loops into "while"

00:11:28.740 --> 00:11:30.020
loops because we can't--

00:11:30.020 --> 00:11:31.980
TensorFlow-- there's
no tf.for loop.

00:11:31.980 --> 00:11:35.310
But AutoGraph implements
tf.for loop for you.

00:11:35.310 --> 00:11:37.890
And a good way to think
about what we're doing here

00:11:37.890 --> 00:11:41.170
is that we're adding
another stage of processing

00:11:41.170 --> 00:11:42.570
in your computation.

00:11:42.570 --> 00:11:45.390
Right now, if you're using
TensorFlow from graphs,

00:11:45.390 --> 00:11:47.720
you write graph-style
code in Python.

00:11:47.720 --> 00:11:50.820
You give this to the TensorFlow
runtime to execute it.

00:11:50.820 --> 00:11:53.100
What AutoGraph lets
you do is write

00:11:53.100 --> 00:11:55.950
eager-style code in Python,
which is imperative,

00:11:55.950 --> 00:11:58.980
which has no control
dependencies,

00:11:58.980 --> 00:12:02.400
and all these things that you
don't want to think about.

00:12:02.400 --> 00:12:04.950
And AutoGraph will go
turn this eager-style code

00:12:04.950 --> 00:12:07.480
into Graph-style code for
you and then hand this

00:12:07.480 --> 00:12:09.480
over to TensorFlow runtime.

00:12:09.480 --> 00:12:12.030
And again, if you're at all
skeptical at this point--

00:12:12.030 --> 00:12:14.970
you're like, well, I've
used TensorFlow before.

00:12:14.970 --> 00:12:17.040
I've seen what the error
messages look like.

00:12:17.040 --> 00:12:21.126
If you're adding another
layer of computation,

00:12:21.126 --> 00:12:23.250
is that going to become
even harder than it already

00:12:23.250 --> 00:12:25.540
is, to debug what's going on?

00:12:25.540 --> 00:12:28.930
And the good news is
that it doesn't have to.

00:12:28.930 --> 00:12:31.350
So if you think about it,
we have these three stages

00:12:31.350 --> 00:12:32.520
of your computation now.

00:12:32.520 --> 00:12:34.980
We have, AutoGraph is
processing your code.

00:12:34.980 --> 00:12:36.330
That's stage 0.

00:12:36.330 --> 00:12:39.780
Then stage 1 is, your code is
being run to generate a graph.

00:12:39.780 --> 00:12:42.720
And stage 2 is, TensorFlow
is executing your code

00:12:42.720 --> 00:12:44.490
to actually do the
thing you want.

00:12:44.490 --> 00:12:47.574
If you get an error in stage
0, when AutoGraph is processing

00:12:47.574 --> 00:12:48.990
your code, you'll
just get a stack

00:12:48.990 --> 00:12:50.640
trace that points to AutoGraph.

00:12:50.640 --> 00:12:52.176
You file a bug
against us in GitHub.

00:12:52.176 --> 00:12:53.334
And they'll fix it.

00:12:53.334 --> 00:12:54.750
If we get an error
during stage 1,

00:12:54.750 --> 00:12:56.520
which is the code
that you wrote--

00:12:56.520 --> 00:12:59.310
well, the code that you wrote,
that AutoGraph transformed

00:12:59.310 --> 00:13:01.560
into code that you don't
know how to read because it's

00:13:01.560 --> 00:13:03.900
full of that extra boilerplate
that we didn't want

00:13:03.900 --> 00:13:05.380
to deal with in
the first place--

00:13:05.380 --> 00:13:07.440
and you get an error
there, we actually

00:13:07.440 --> 00:13:10.740
rewrite the Python
stack trace to point,

00:13:10.740 --> 00:13:13.970
not to the generated code
line that has an error,

00:13:13.970 --> 00:13:16.740
but to the line of code
that you actually wrote.

00:13:16.740 --> 00:13:21.810
So that you can connect your
actions with their consequences

00:13:21.810 --> 00:13:25.140
instead of having this
black box that deals with.

00:13:25.140 --> 00:13:26.910
And if you've used
TensorFlow, and you've

00:13:26.910 --> 00:13:28.380
seen the errors at runtime--

00:13:28.380 --> 00:13:30.000
we already show you
two stack traces,

00:13:30.000 --> 00:13:33.390
one for when the graph is
built and another one for when

00:13:33.390 --> 00:13:35.160
the session.run was called.

00:13:35.160 --> 00:13:39.030
And in principal, we can also
rewrite the stack traces here,

00:13:39.030 --> 00:13:43.000
so that the graph with
build stack trace shows

00:13:43.000 --> 00:13:45.360
the code you wrote, not the
code AutoGraph generated.

00:13:45.360 --> 00:13:47.920
And we're working on
making this happen.

00:13:47.920 --> 00:13:51.960
So what's in the
future of AutoGraph?

00:13:51.960 --> 00:13:53.280
We have a public beta.

00:13:53.280 --> 00:13:55.260
It's in tf.contrib.autograph.

00:13:55.260 --> 00:13:56.520
We encourage you to try it.

00:13:56.520 --> 00:13:57.434
It's ready to use.

00:13:57.434 --> 00:13:59.850
If you find a bug-- and I hope
you don't-- but if you find

00:13:59.850 --> 00:14:01.667
a bug, file something on GitHub.

00:14:01.667 --> 00:14:03.000
Let's work with us to get a fix.

00:14:05.540 --> 00:14:08.370
You might have seen that
we are starting to announce

00:14:08.370 --> 00:14:10.660
a lot of our plans for TF 2.0.

00:14:10.660 --> 00:14:12.900
And one of our big
things in TF 2.0

00:14:12.900 --> 00:14:15.570
is that eager execution is
going to be enabled by default.

00:14:15.570 --> 00:14:19.890
And to make crossing the
bridge between eager execution

00:14:19.890 --> 00:14:22.620
and graph deployability
easier, AutoGraph

00:14:22.620 --> 00:14:24.070
will be a key part of this.

00:14:24.070 --> 00:14:27.600
So we would love to get this
tested as much as we possibly

00:14:27.600 --> 00:14:29.940
can before TF 2.0 comes.

00:14:29.940 --> 00:14:33.180
Meanwhile, we're working on
improving our handling-- yes--

00:14:33.180 --> 00:14:37.110
but also, enhancing our
coverage of the Python language,

00:14:37.110 --> 00:14:39.300
adding more and more
and more operations

00:14:39.300 --> 00:14:41.100
to what AutoGraph
supports, so that you

00:14:41.100 --> 00:14:43.020
have to think less
and less and less

00:14:43.020 --> 00:14:46.110
about how your code is going
to be turned into a graph

00:14:46.110 --> 00:14:48.330
and more about, what do
you actually want your code

00:14:48.330 --> 00:14:50.260
to do in the first place?

00:14:50.260 --> 00:14:54.600
We also want to factor out the
source code transformation bit

00:14:54.600 --> 00:14:57.810
into its own library, so that
if you have some Python project

00:14:57.810 --> 00:15:00.040
where you want to
override _ _if_ _,

00:15:00.040 --> 00:15:02.040
you will be able to reuse
their code to do that.

00:15:02.040 --> 00:15:05.490
Because it's a pretty neat,
self-contained little thing

00:15:05.490 --> 00:15:07.530
that I think can
be useful broadly,

00:15:07.530 --> 00:15:10.080
beyond the TensorFlow universe.

00:15:10.080 --> 00:15:13.050
So thank you for
listening to me.

00:15:13.050 --> 00:15:16.860
But if-- strongly encourage
you to go open your laptops

00:15:16.860 --> 00:15:17.520
and do this.

00:15:17.520 --> 00:15:20.580
Google autograph collab, and
click on the first result.

00:15:20.580 --> 00:15:22.380
Or open this link here.

00:15:22.380 --> 00:15:24.870
This will point
you to a notebook

00:15:24.870 --> 00:15:26.700
that's hosted on
Google infrastructure.

00:15:26.700 --> 00:15:28.620
So you can use
our GPUs for free.

00:15:28.620 --> 00:15:32.500
And it has lots of little
examples for how-- like,

00:15:32.500 --> 00:15:33.750
this is code before AutoGraph.

00:15:33.750 --> 00:15:34.958
This is code after AutoGraph.

00:15:34.958 --> 00:15:36.720
This is what an
error looks like.

00:15:36.720 --> 00:15:37.990
These are things we can do.

00:15:37.990 --> 00:15:41.430
And I think playing around with
this can really help give you

00:15:41.430 --> 00:15:43.560
an idea that's a lot
more concrete than what

00:15:43.560 --> 00:15:44.880
I've been talking about.

00:15:44.880 --> 00:15:48.300
What are the actual
capabilities of this technology?

00:15:48.300 --> 00:15:50.900
And I'm sure if you try
this, you'll have a blast.

00:15:50.900 --> 00:15:53.150
Thank you.

00:15:53.150 --> 00:15:55.860
Oh, also, Frank and I are
going to be at the TensorFlow

00:15:55.860 --> 00:15:59.270
booth in the sponsor area if
you have any questions, even

00:15:59.270 --> 00:16:01.210
if you don't want ask them now.

00:16:01.210 --> 00:16:02.420
OK.

00:16:02.420 --> 00:16:03.470
Thank you.

00:16:03.470 --> 00:16:05.774
[APPLAUSE]

