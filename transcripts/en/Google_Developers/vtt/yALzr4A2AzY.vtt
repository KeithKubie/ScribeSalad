WEBVTT
Kind: captions
Language: en

00:00:03.792 --> 00:00:06.415
JONATHAN HSEU: Hi, I'm Jonathan.

00:00:06.415 --> 00:00:08.790
A lot of you are probably
running a lot of infrastructure

00:00:08.790 --> 00:00:11.100
already in your
organization or you

00:00:11.100 --> 00:00:13.130
want to think about running
various other pieces

00:00:13.130 --> 00:00:13.880
of infrastructure.

00:00:13.880 --> 00:00:18.460
And I'm going tell you about
TensorFlow fits into that.

00:00:18.460 --> 00:00:20.710
So, the clicker--

00:00:20.710 --> 00:00:24.630
[SIDE CONVERSATION]

00:00:34.940 --> 00:00:36.900
OK, thank you.

00:00:36.900 --> 00:00:40.620
So first, I'm going to talk
about all the steps you

00:00:40.620 --> 00:00:44.370
go from training to serving
a model in production.

00:00:44.370 --> 00:00:46.922
And it typically involves
these three steps.

00:00:46.922 --> 00:00:49.380
Usually, the raw data that you
have isn't good for training

00:00:49.380 --> 00:00:51.660
yet, so you have to do some
form of data preparation.

00:00:51.660 --> 00:00:56.010
After that, you do training
and then you serve your model.

00:00:56.010 --> 00:00:58.860
So, for data preparation,
there are three essential steps

00:00:58.860 --> 00:01:00.750
that are involved
in data preparation.

00:01:00.750 --> 00:01:03.090
So you'll import the data
from various sources.

00:01:03.090 --> 00:01:05.483
This might be Hive,
MySQL, and so on.

00:01:05.483 --> 00:01:07.650
You'll typically perform
some sort of pre-processing

00:01:07.650 --> 00:01:08.490
on the data.

00:01:08.490 --> 00:01:10.370
They'll compute
aggregates or do joins

00:01:10.370 --> 00:01:12.870
and then you'll export the file
in the file format supported

00:01:12.870 --> 00:01:16.180
by TensorFlow and I'll
explain that a bit later.

00:01:16.180 --> 00:01:19.800
So, a lot of people have been
using typically these three

00:01:19.800 --> 00:01:21.410
things for doing this process--

00:01:21.410 --> 00:01:24.440
SPARC, Hadoop, MapReduce,
or Apache Beam.

00:01:24.440 --> 00:01:27.090
And it works well with
all these at the moment.

00:01:27.090 --> 00:01:30.720
And as a visual
example, we have--

00:01:30.720 --> 00:01:32.380
you're feeding in
data from MySQL.

00:01:32.380 --> 00:01:33.870
Say it's user data.

00:01:33.870 --> 00:01:35.970
And you have data in Hive.

00:01:35.970 --> 00:01:38.400
An example might be web
impressions or web clicks.

00:01:38.400 --> 00:01:40.590
You want to join that
data, compute aggregates

00:01:40.590 --> 00:01:43.650
like user data per region.

00:01:43.650 --> 00:01:46.290
Often, you'll do vocabulary
generation, particularly

00:01:46.290 --> 00:01:49.320
if you're using embeddings.

00:01:49.320 --> 00:01:50.790
Vocabulary generation
essentially

00:01:50.790 --> 00:01:55.470
means, like, mapping from
words to IDs and the IDs map

00:01:55.470 --> 00:01:57.829
to an embedding
layer-- embedding.

00:01:57.829 --> 00:01:59.370
And then after that,
you'll typically

00:01:59.370 --> 00:02:00.990
have a training test split.

00:02:00.990 --> 00:02:05.070
And after this process,
you'll output to a file format

00:02:05.070 --> 00:02:07.080
that TensorFlow understands.

00:02:07.080 --> 00:02:09.150
And why might you
want to do this?

00:02:09.150 --> 00:02:13.550
So, the input file format
they use with TensorFlow

00:02:13.550 --> 00:02:14.970
actually matters quite a bit.

00:02:14.970 --> 00:02:18.210
So, one example is, suppose
you're training inception

00:02:18.210 --> 00:02:20.370
on P100 GPUs.

00:02:20.370 --> 00:02:22.161
You want to keep
the P100 saturated.

00:02:22.161 --> 00:02:23.910
Actually, one of the
most important things

00:02:23.910 --> 00:02:25.284
you can do to make
that work well

00:02:25.284 --> 00:02:27.170
is work on your input pipeline.

00:02:27.170 --> 00:02:28.670
And that involves
working with cues.

00:02:28.670 --> 00:02:31.050
There are tutorials on
the TensorFlow website.

00:02:31.050 --> 00:02:33.870
But specifically in this case,
I'll talk about file formats.

00:02:33.870 --> 00:02:36.720
Some file formats are
faster than others.

00:02:36.720 --> 00:02:38.540
We have specifically
optimized the path

00:02:38.540 --> 00:02:41.750
for working with tf.Example
and tf.SequenceExample protocol

00:02:41.750 --> 00:02:42.620
buffers.

00:02:42.620 --> 00:02:44.780
SequenceExample
is typically used

00:02:44.780 --> 00:02:47.780
when you're training
with RNNs and tf.Records

00:02:47.780 --> 00:02:49.670
is a TensorFlow
specific file format

00:02:49.670 --> 00:02:51.890
that we use with TensorFlow.

00:02:51.890 --> 00:02:54.170
And this is by far the fastest.

00:02:54.170 --> 00:02:56.600
We use this internally and
we've optimized this path

00:02:56.600 --> 00:02:58.110
pretty well.

00:02:58.110 --> 00:02:59.960
Additionally,
TensorFlow also natively

00:02:59.960 --> 00:03:03.755
has ops to support reading from
various file formats like CSV,

00:03:03.755 --> 00:03:06.800
JSON, fixed record length files.

00:03:06.800 --> 00:03:09.350
We have upcoming
support for Avro.

00:03:09.350 --> 00:03:10.235
So you can use those.

00:03:10.235 --> 00:03:12.568
Those are a little slower,
but since they're native ops,

00:03:12.568 --> 00:03:14.510
they actually work pretty well.

00:03:14.510 --> 00:03:16.520
More commonly,
people, if you, like,

00:03:16.520 --> 00:03:19.550
read examples of
TensorFlow code on the web,

00:03:19.550 --> 00:03:22.580
people will be feeding
data directly in Python.

00:03:22.580 --> 00:03:24.590
And this is by far
the most common.

00:03:24.590 --> 00:03:26.360
It's the easiest
to experiment with.

00:03:26.360 --> 00:03:27.740
It also has the
most flexibility,

00:03:27.740 --> 00:03:31.670
meaning that you can use
any file format that Python

00:03:31.670 --> 00:03:35.750
natively can understand and
then convert it to numpy arrays,

00:03:35.750 --> 00:03:37.940
which can be fed as
tensors in TensorFlow.

00:03:37.940 --> 00:03:40.460
It's also useful for settings
like reinforcement learning.

00:03:40.460 --> 00:03:42.830
But it's also the
slowest option.

00:03:42.830 --> 00:03:45.323
So if you're running
experiments, this makes sense.

00:03:45.323 --> 00:03:47.323
But if you're trying to
productionize a pipeline

00:03:47.323 --> 00:03:48.800
and you care about
training speed,

00:03:48.800 --> 00:03:51.383
then you might want to consider
switching to the first option.

00:03:54.320 --> 00:03:57.560
OK, I'd like to mention
actually why it's a bit slower.

00:03:57.560 --> 00:04:00.440
There are two things
that are involved--

00:04:00.440 --> 00:04:03.530
reading from specific
file formats in Python

00:04:03.530 --> 00:04:05.164
itself may not be too fast.

00:04:05.164 --> 00:04:06.080
So you have that part.

00:04:06.080 --> 00:04:11.240
And then when converting from
new pi arrays to TensorFlow

00:04:11.240 --> 00:04:12.830
tensors, there is
a copy involved

00:04:12.830 --> 00:04:15.750
and that induces
additional overhead.

00:04:15.750 --> 00:04:17.550
So I've recommended
tf.RecordSupport.

00:04:17.550 --> 00:04:19.500
Hopefully at this
point, you're convinced.

00:04:19.500 --> 00:04:21.310
And how well does it work?

00:04:21.310 --> 00:04:23.700
So, Apache beam, as
of a few weeks ago,

00:04:23.700 --> 00:04:26.970
actually has native support
for this file format.

00:04:26.970 --> 00:04:29.661
But a lot of you aren't
running Apache Beam

00:04:29.661 --> 00:04:31.660
and you'll be running
Hadoop MapReduce and SPARC

00:04:31.660 --> 00:04:32.580
jobs already.

00:04:32.580 --> 00:04:33.940
And you just want to use that.

00:04:33.940 --> 00:04:38.030
So if you go to the ecosystem
repository in the TensorFlow

00:04:38.030 --> 00:04:41.880
GitHub organization, we have
code to work with TF records

00:04:41.880 --> 00:04:42.640
there.

00:04:42.640 --> 00:04:44.765
Note that I didn't mention
protocol buffers at all.

00:04:44.765 --> 00:04:46.680
Protocol buffers actually
has native support

00:04:46.680 --> 00:04:50.710
for a lot of languages
you would be using.

00:04:50.710 --> 00:04:54.870
OK, so now that you've
exported this training data,

00:04:54.870 --> 00:04:56.957
you want to go to training.

00:04:56.957 --> 00:04:58.790
There are two common
ways of doing training.

00:04:58.790 --> 00:05:01.760
One is say you have a GPU
on your desktop or laptop.

00:05:01.760 --> 00:05:03.290
You want to do local training.

00:05:03.290 --> 00:05:07.640
This works well for the specific
cases of debugging or working

00:05:07.640 --> 00:05:09.260
with small order data sets.

00:05:09.260 --> 00:05:13.880
But as Megan mentioned
in the intro talk,

00:05:13.880 --> 00:05:16.400
distributed training--
like, 64 GPUs,

00:05:16.400 --> 00:05:20.990
you actually get a 58x speed up
in images processed per second

00:05:20.990 --> 00:05:24.230
if you run on
distributed starting

00:05:24.230 --> 00:05:27.140
with 64 GPUs versus one GPU.

00:05:27.140 --> 00:05:28.220
So you train much faster.

00:05:28.220 --> 00:05:30.170
And for a lot of people
in your organization,

00:05:30.170 --> 00:05:32.410
this could mean the
difference of taking a month

00:05:32.410 --> 00:05:34.410
to train a model
versus taking a day.

00:05:34.410 --> 00:05:36.560
And the feedback loop
is pretty important

00:05:36.560 --> 00:05:40.890
to make machine learning work
well in your organization.

00:05:40.890 --> 00:05:44.150
So, there are two
essential components

00:05:44.150 --> 00:05:45.980
if you want to start
with distributed

00:05:45.980 --> 00:05:47.660
training in your organization.

00:05:47.660 --> 00:05:49.880
Derek was talking
about cluster managers.

00:05:49.880 --> 00:05:52.790
We know organizations that
have been using all of these

00:05:52.790 --> 00:05:55.370
to make a distributed
TensorFlow work well.

00:05:55.370 --> 00:05:57.860
So we'd recommend any of these.

00:05:57.860 --> 00:06:00.280
By Hadoop here, I
mean Hadoop Yarn.

00:06:00.280 --> 00:06:04.220
And so we actually have
configuration examples

00:06:04.220 --> 00:06:06.230
for Kubernetes and
Mesos specifically

00:06:06.230 --> 00:06:07.736
in the ecosystem repository.

00:06:07.736 --> 00:06:09.860
So if you're already running
that, then you can get

00:06:09.860 --> 00:06:11.690
started pretty quickly that way.

00:06:11.690 --> 00:06:13.940
But in addition to
a Cluster Manager,

00:06:13.940 --> 00:06:16.641
you should also be running
distributed storage.

00:06:16.641 --> 00:06:18.390
Derek was talking about
this a little bit,

00:06:18.390 --> 00:06:20.150
but these are the ones
that we work with.

00:06:20.150 --> 00:06:24.110
We added HDFS support
in TensorFlow 11.

00:06:24.110 --> 00:06:25.550
TensorFlow also
has native support

00:06:25.550 --> 00:06:27.590
for reading from
Google Cloud Storage,

00:06:27.590 --> 00:06:29.720
and if you're running on
Amazon's cloud service,

00:06:29.720 --> 00:06:35.227
then users have reported it
working well with Amazon EFS.

00:06:35.227 --> 00:06:37.560
So, the reason you would want
to run distributed storage

00:06:37.560 --> 00:06:41.460
is mostly for two reasons--

00:06:41.460 --> 00:06:43.140
for your input
data so each worker

00:06:43.140 --> 00:06:47.250
can read from a single source
and for output for your model--

00:06:47.250 --> 00:06:51.250
so, for checkpoints in
the exported model itself.

00:06:51.250 --> 00:06:53.440
So, containers are
completely optional

00:06:53.440 --> 00:06:55.150
but highly recommended.

00:06:55.150 --> 00:06:58.600
So, containers isolate your
workers from their environment.

00:06:58.600 --> 00:07:01.720
And one reason this is useful
is because certain jobs

00:07:01.720 --> 00:07:04.630
might have certain
version dependencies that

00:07:04.630 --> 00:07:06.190
might differ from other jobs.

00:07:06.190 --> 00:07:08.500
And a specific example
here is eventually,

00:07:08.500 --> 00:07:10.580
we'll release TensorFlow 2.0.

00:07:10.580 --> 00:07:13.630
Senseful uses
semantic versioning,

00:07:13.630 --> 00:07:17.045
and at 2.0, we may make some
backwards incompatible changes.

00:07:17.045 --> 00:07:18.920
And suppose you want to
upgrade to TensorFlow

00:07:18.920 --> 00:07:20.840
2.0 in your organization.

00:07:20.840 --> 00:07:23.420
Then the containers
make it much easier

00:07:23.420 --> 00:07:25.670
to do gradual updates
because you don't have

00:07:25.670 --> 00:07:27.100
to update every job at once.

00:07:27.100 --> 00:07:29.120
You can just update jobs.

00:07:29.120 --> 00:07:31.820
You can pin the jobs to a
particular TensorFlow version

00:07:31.820 --> 00:07:33.770
and update each one one by one.

00:07:33.770 --> 00:07:36.290
So, getting started with
containers makes sense.

00:07:36.290 --> 00:07:39.290
Docker is supported in
both Mesos and Kubernetes

00:07:39.290 --> 00:07:43.280
and we recommend that.

00:07:43.280 --> 00:07:45.895
So, as a refresh for what
Derek was talking about,

00:07:45.895 --> 00:07:49.490
I'm going to show you what
distributed trading code looks

00:07:49.490 --> 00:07:52.100
like as a refresh of
what Derek mentioned

00:07:52.100 --> 00:07:55.220
and what configuration involves.

00:07:55.220 --> 00:07:59.802
So, there are essentially two
types of jobs that you have--

00:07:59.802 --> 00:08:01.640
parameter servers and workers.

00:08:01.640 --> 00:08:04.460
And Derek mentioned that
the most common setup

00:08:04.460 --> 00:08:07.750
and the setup that we recommend
is between graph replication.

00:08:07.750 --> 00:08:10.177
And in that case, workers
operate completely

00:08:10.177 --> 00:08:10.760
independently.

00:08:10.760 --> 00:08:13.250
So the workers don't
necessarily talk to each other

00:08:13.250 --> 00:08:15.480
or even know about each other.

00:08:15.480 --> 00:08:19.010
They only communicate with
the parameter servers.

00:08:19.010 --> 00:08:21.710
And for that, Derek
actually showed this but

00:08:21.710 --> 00:08:23.490
across multiple slides.

00:08:23.490 --> 00:08:27.140
This is the code involved if
you're using core TensorFlow.

00:08:27.140 --> 00:08:28.910
With the higher level
APIs, a lot of this

00:08:28.910 --> 00:08:31.400
goes away and becomes
much fewer lines of code.

00:08:31.400 --> 00:08:34.549
But in essence,
your Cluster Manager

00:08:34.549 --> 00:08:38.650
specifies all the other
parameter servers and workers

00:08:38.650 --> 00:08:40.580
that this worker must
communicate with.

00:08:40.580 --> 00:08:41.690
You start a server.

00:08:41.690 --> 00:08:43.890
If you're a parameter
server, you just stop there.

00:08:43.890 --> 00:08:45.590
But if you're a
worker, then you need

00:08:45.590 --> 00:08:48.650
to assign variables
to parameter servers

00:08:48.650 --> 00:08:52.040
and set the worker device
so that this graph belongs

00:08:52.040 --> 00:08:55.300
on this particular worker.

00:08:55.300 --> 00:08:59.050
So, configuration really
depends on which cluster manager

00:08:59.050 --> 00:09:01.780
and which distributed
storage system you chose.

00:09:01.780 --> 00:09:04.300
We have examples, again, in
that repository dimension,

00:09:04.300 --> 00:09:05.770
that ecosystem repository.

00:09:05.770 --> 00:09:07.510
And we're adding
more and feel free

00:09:07.510 --> 00:09:10.990
to contribute any that you know
that work well in your existing

00:09:10.990 --> 00:09:12.365
infrastructure.

00:09:12.365 --> 00:09:13.740
Additionally, I'd
like to mention

00:09:13.740 --> 00:09:18.542
that Yahoo! recently open
sourced code to run on SPARC.

00:09:18.542 --> 00:09:19.500
And why is this useful?

00:09:19.500 --> 00:09:20.940
Because you might
think that SPARC

00:09:20.940 --> 00:09:23.040
is like a data
processing framework.

00:09:23.040 --> 00:09:26.220
Why would it be useful to run
on SPARC rather than a Cluster

00:09:26.220 --> 00:09:27.030
Manager?

00:09:27.030 --> 00:09:31.170
So, SPARC actually has support
for running on Mesos, Yarn

00:09:31.170 --> 00:09:32.910
or in standalone
mode, and a lot of you

00:09:32.910 --> 00:09:36.810
will already be running
SPARC in your organization.

00:09:36.810 --> 00:09:39.390
And this actually might be the
easiest way to get started.

00:09:39.390 --> 00:09:43.230
And for sure, it's difficult
to run distributed TensorFlow

00:09:43.230 --> 00:09:45.270
on Hadoop Yarn.

00:09:45.270 --> 00:09:46.920
And for sure, it's
much easier to get

00:09:46.920 --> 00:09:49.650
started if you're running
SPARC on top of Yarn

00:09:49.650 --> 00:09:54.240
than to try to run distributed
TensorFlow on Yarn directly.

00:09:54.240 --> 00:09:56.390
So I'd recommend this
in specific cases

00:09:56.390 --> 00:09:59.420
where you're already
running SPARC.

00:09:59.420 --> 00:10:04.100
And Dandelion talked
about TensorBoard.

00:10:04.100 --> 00:10:07.265
You might wonder, how do you use
this with distributed training?

00:10:07.265 --> 00:10:09.140
So, with distributed
training, typically, you

00:10:09.140 --> 00:10:11.360
have output which are
summaries and events that

00:10:11.360 --> 00:10:13.340
go to a directory
that you specified

00:10:13.340 --> 00:10:15.440
on distributed storage.

00:10:15.440 --> 00:10:17.090
All you need to do
to run TensorBoard

00:10:17.090 --> 00:10:18.920
is run the TensorBoard
command and point it

00:10:18.920 --> 00:10:20.700
to that distributed
storage directory.

00:10:20.700 --> 00:10:22.460
It will start in
HTTP server locally

00:10:22.460 --> 00:10:23.544
that you can just look at.

00:10:23.544 --> 00:10:24.710
You can look at your losses.

00:10:24.710 --> 00:10:26.540
You can go through the
embedding visualizer

00:10:26.540 --> 00:10:29.720
and how that stuff works.

00:10:29.720 --> 00:10:31.690
So, like, I've skipped
a lot of steps here.

00:10:31.690 --> 00:10:33.580
Like, suppose you
need to do hyper

00:10:33.580 --> 00:10:34.706
parameter tuning and so on.

00:10:34.706 --> 00:10:36.705
But suppose you're already
done with all of that

00:10:36.705 --> 00:10:38.230
and you're happy
with your model.

00:10:38.230 --> 00:10:40.390
You want to export it and
serve it in production.

00:10:40.390 --> 00:10:42.800
So currently, we have two
common ways of doing that,

00:10:42.800 --> 00:10:44.740
but in the long
term, it will be one.

00:10:44.740 --> 00:10:47.050
We highly recommend
you go with SaveModel.

00:10:47.050 --> 00:10:49.970
It's going to be the standard
file format for TensorFlow

00:10:49.970 --> 00:10:51.250
going forward.

00:10:51.250 --> 00:10:54.220
And the advantage to
SaveModel over everything else

00:10:54.220 --> 00:10:56.020
is that it includes
all the assets

00:10:56.020 --> 00:10:58.090
you need to serve your
model in production.

00:10:58.090 --> 00:10:59.710
I mentioned that
vocabularies are

00:10:59.710 --> 00:11:03.835
pretty common for embeddings and
you use a specific vocabulary

00:11:03.835 --> 00:11:04.556
at training time.

00:11:04.556 --> 00:11:06.805
You want to make sure to use
the exact same vocabulary

00:11:06.805 --> 00:11:07.513
and serving time.

00:11:07.513 --> 00:11:09.260
It includes all
those files for you.

00:11:09.260 --> 00:11:11.590
So the other
advantage to SaveModel

00:11:11.590 --> 00:11:13.360
is it's a bunch
of graphs and you

00:11:13.360 --> 00:11:16.030
can specify which one you want
to use for a particular use

00:11:16.030 --> 00:11:17.410
case.

00:11:17.410 --> 00:11:21.440
GraphDef right now is pretty
common in particular uses,

00:11:21.440 --> 00:11:22.690
particularly on mobile.

00:11:22.690 --> 00:11:26.020
So, in the mobile
example, you typically

00:11:26.020 --> 00:11:28.690
freeze your variables
into constants

00:11:28.690 --> 00:11:30.100
in your TensorFlow graph.

00:11:30.100 --> 00:11:31.870
The advantage to
GraphDef right now

00:11:31.870 --> 00:11:35.530
is that GraphDef is actually
a protocol buffer which

00:11:35.530 --> 00:11:36.866
accesses like a single file.

00:11:36.866 --> 00:11:38.740
So you only have to
distribute a single file.

00:11:38.740 --> 00:11:41.200
That's much easier to deploy.

00:11:41.200 --> 00:11:43.120
But that advantage
is going away.

00:11:43.120 --> 00:11:48.580
As of recently, you can
also freeze a SaveModel.

00:11:48.580 --> 00:11:52.530
So hopefully, you've
exploited the SaveModel.

00:11:52.530 --> 00:11:54.790
You want to serve it.

00:11:54.790 --> 00:11:57.430
Noah-- the subsequent talk is
actually going go much more

00:11:57.430 --> 00:12:01.140
into TensorFlow serving and
I'm only going to talk about

00:12:01.140 --> 00:12:02.550
the top two here--

00:12:02.550 --> 00:12:05.670
TensorFlow serving and
in process TensorFlow.

00:12:05.670 --> 00:12:08.370
So, these are both options
for serving your TensorFlow

00:12:08.370 --> 00:12:13.320
model after you've exploited it,
but they are quite different.

00:12:13.320 --> 00:12:15.480
TensorFlow serving,
in its typical mode,

00:12:15.480 --> 00:12:19.650
runs as a separate service
that you make RPCs to.

00:12:19.650 --> 00:12:23.250
So, why might you choose
one over the other?

00:12:23.250 --> 00:12:26.450
So, Noah's going to go
much more in depth here,

00:12:26.450 --> 00:12:30.820
but serving a model actually
has a lot of hidden nuance

00:12:30.820 --> 00:12:33.320
that you might not be aware of
when you just think about it.

00:12:33.320 --> 00:12:35.512
So, if you're just applying
a model as a one off,

00:12:35.512 --> 00:12:37.970
then it might make sense to
just use in-process TensorFlow.

00:12:37.970 --> 00:12:38.810
Flow

00:12:38.810 --> 00:12:41.330
But if you think about--

00:12:41.330 --> 00:12:42.890
like, suppose you
have a job that

00:12:42.890 --> 00:12:45.890
trains every day on
new data and you want

00:12:45.890 --> 00:12:48.220
to serve a new model every day.

00:12:48.220 --> 00:12:52.570
How might you start
serving that new model?

00:12:52.570 --> 00:12:53.800
You can think about it.

00:12:53.800 --> 00:12:56.470
There's like two basic
ways that might happen.

00:12:56.470 --> 00:12:58.660
In your server, you can
unload the completely unload

00:12:58.660 --> 00:13:01.160
the old model and then
wait for that to finish,

00:13:01.160 --> 00:13:03.852
wait for all requests to
finish, and load your new model.

00:13:03.852 --> 00:13:05.560
This is memory efficient
because you only

00:13:05.560 --> 00:13:07.570
have one model loaded
at a single time.

00:13:07.570 --> 00:13:09.460
Or, you can simultaneously
load the new model

00:13:09.460 --> 00:13:11.730
while it's running and
double your memory usage.

00:13:11.730 --> 00:13:14.770
tf.Serving offers that as
a configuration option.

00:13:14.770 --> 00:13:17.620
Additionally, for
efficiency, particularly

00:13:17.620 --> 00:13:20.780
if you're serving on GPUs,
you want to batch your inputs.

00:13:20.780 --> 00:13:23.050
This also has some
benefit on CPUs,

00:13:23.050 --> 00:13:25.900
but particularly if
you're serving on GPUs,

00:13:25.900 --> 00:13:29.740
you want a batch to
fully utilize your GPUs.

00:13:29.740 --> 00:13:31.667
As you can imagine,
there's some nuance

00:13:31.667 --> 00:13:33.250
to how you want to
batch, particularly

00:13:33.250 --> 00:13:34.720
in low latency settings.

00:13:34.720 --> 00:13:37.540
So, tf.Serving handles that.

00:13:37.540 --> 00:13:40.060
Also, if you're serving
multiple models,

00:13:40.060 --> 00:13:42.070
isolation between those models--

00:13:42.070 --> 00:13:44.170
they're going to contend
for hardware resources

00:13:44.170 --> 00:13:49.380
and there's some difficulties
in isolating between them.

00:13:49.380 --> 00:13:50.760
It's particularly intensiveful.

00:13:50.760 --> 00:13:53.020
You have to think
about which threads--

00:13:53.020 --> 00:13:55.087
like, how to size
thread pools and so on.

00:13:55.087 --> 00:13:56.920
You don't have to think
about that if you're

00:13:56.920 --> 00:13:59.350
on TensorFlow serving at all.

00:13:59.350 --> 00:14:01.300
So, that's the next talk.

00:14:01.300 --> 00:14:03.950
Now that I've explained all
the benefits, you're like, OK,

00:14:03.950 --> 00:14:06.560
then why not just use
that all the time?

00:14:06.560 --> 00:14:09.440
Why should I use in
process TensorFlow at all?

00:14:09.440 --> 00:14:13.120
So, in process TensorFlow is
actually the standard mode

00:14:13.120 --> 00:14:15.010
for certain things,
specifically if you're

00:14:15.010 --> 00:14:17.150
running on a mobile device.

00:14:17.150 --> 00:14:18.700
Pete actually went over this.

00:14:18.700 --> 00:14:21.792
Basically, you have a TensorFlow
session and you run it.

00:14:21.792 --> 00:14:23.500
If you're running
batch inference-- like,

00:14:23.500 --> 00:14:26.100
if you already have a TensorFlow
model, you have a ton of data

00:14:26.100 --> 00:14:28.465
and you want to run inference
over that data using Beam

00:14:28.465 --> 00:14:31.060
or SPARC, then it's
much better to just use

00:14:31.060 --> 00:14:33.760
in process TensorFlow than
to start another tf.Serving

00:14:33.760 --> 00:14:36.060
service.

00:14:36.060 --> 00:14:38.440
There's the specific case
of very strict latency

00:14:38.440 --> 00:14:40.060
requirements where
you don't want

00:14:40.060 --> 00:14:44.290
to do the RPCs back and forth
with TensorFlow serving,

00:14:44.290 --> 00:14:45.550
but this is a very rare case.

00:14:45.550 --> 00:14:49.000
Usually, the latency
involved there is very small.

00:14:49.000 --> 00:14:50.980
Operationally speaking,
it's much easier

00:14:50.980 --> 00:14:53.050
to run one service
than to run two.

00:14:53.050 --> 00:14:55.450
So if you want to
simplify your deployments,

00:14:55.450 --> 00:14:57.720
that's also a reason to do this.

00:14:57.720 --> 00:14:59.460
But regardless of
which one you choose,

00:14:59.460 --> 00:15:04.380
the first two steps between
them is practically the same.

00:15:04.380 --> 00:15:06.774
You want to export your state
model, make it accessible,

00:15:06.774 --> 00:15:08.940
and then you have to write
the code to do inference.

00:15:08.940 --> 00:15:10.860
And I'll show you what that
looks like in a moment.

00:15:10.860 --> 00:15:13.320
For in process TensorFlow,
you have the additional step

00:15:13.320 --> 00:15:16.230
of linking to the TensorFlow
shared library, which

00:15:16.230 --> 00:15:18.030
we actually distribute.

00:15:18.030 --> 00:15:20.610
And you can see that in our
Go and Java instructions

00:15:20.610 --> 00:15:22.318
and you can just
download it right there,

00:15:22.318 --> 00:15:25.050
or you can build it yourself.

00:15:25.050 --> 00:15:27.436
So, here's the Go example of
what inference looks like.

00:15:27.436 --> 00:15:29.060
It's actually not
too much code at all.

00:15:29.060 --> 00:15:32.420
And actually, if you go to
the Go repository on GitHub,

00:15:32.420 --> 00:15:36.170
you can actually see
real inception code

00:15:36.170 --> 00:15:37.370
that works in Go.

00:15:37.370 --> 00:15:40.690
So, what it involves is first
loading your saved model.

00:15:40.690 --> 00:15:43.580
You pass the directory
that the save model is in.

00:15:43.580 --> 00:15:45.740
As I mentioned before,
save models involve

00:15:45.740 --> 00:15:47.270
multiple TensorFlow graphs.

00:15:47.270 --> 00:15:49.910
So, after you specify
the directory,

00:15:49.910 --> 00:15:51.740
there is a list of--
set of tags that you

00:15:51.740 --> 00:15:54.080
can use to specify
which TensorFlow

00:15:54.080 --> 00:15:56.000
graph you want to load.

00:15:56.000 --> 00:15:57.860
When you execute your
model, all of you

00:15:57.860 --> 00:16:00.470
are probably already familiar
with TensorFlow sessions

00:16:00.470 --> 00:16:02.340
if you've ever used
TensorFlow at all.

00:16:02.340 --> 00:16:04.540
It's essentially the same
thing as the Python API

00:16:04.540 --> 00:16:06.830
but in another language.

00:16:06.830 --> 00:16:07.910
You take a tensor.

00:16:07.910 --> 00:16:09.800
Here, we have a batch of images.

00:16:09.800 --> 00:16:13.820
And you run your TensorFlow
model using that tensor.

00:16:13.820 --> 00:16:17.450
And as output, you
get probabilities

00:16:17.450 --> 00:16:20.770
for each classification for
each image in the batch.

00:16:20.770 --> 00:16:23.240
And you do something with it.

00:16:23.240 --> 00:16:25.700
So I showed Go there.

00:16:25.700 --> 00:16:28.430
Jeff mentioned that we support
a ton of different languages.

00:16:28.430 --> 00:16:32.654
These languages are all on our
TensorFlow GitHub repository.

00:16:32.654 --> 00:16:34.820
So you can actually just
go to the TensorFlow GitHub

00:16:34.820 --> 00:16:38.395
repository and start
using any of these.

00:16:38.395 --> 00:16:40.260
All of these
languages can actually

00:16:40.260 --> 00:16:44.010
be used to build and
execute TensorFlow graphs,

00:16:44.010 --> 00:16:48.660
But a common question is, can
any language besides Python

00:16:48.660 --> 00:16:50.400
be used for training?

00:16:50.400 --> 00:16:54.610
And the answer is a bit nuanced,
but the short of it is no.

00:16:54.610 --> 00:16:57.270
And the reason is because a
lot of the supporting libraries

00:16:57.270 --> 00:17:01.710
like the training libraries,
the high level APIs, optimizers,

00:17:01.710 --> 00:17:04.680
the RNN library, all of those
are only available in Python

00:17:04.680 --> 00:17:05.490
at the moment.

00:17:05.490 --> 00:17:08.608
So I would personally
recommend that you only

00:17:08.608 --> 00:17:11.460
do training in Python, but
you can do inference in any

00:17:11.460 --> 00:17:14.535
of these other languages.

00:17:14.535 --> 00:17:16.410
The instructions for
using the shared library

00:17:16.410 --> 00:17:19.770
are in the readmes that are
associated with each language.

00:17:19.770 --> 00:17:22.940
So you can either download
it or build it yourself.

00:17:22.940 --> 00:17:25.990
I won't go into too
much detail here.

00:17:25.990 --> 00:17:27.910
So, as a summary--

00:17:27.910 --> 00:17:29.444
so, these are the basic steps.

00:17:29.444 --> 00:17:31.360
You're probably running
infrastructure already

00:17:31.360 --> 00:17:32.284
around it.

00:17:32.284 --> 00:17:33.700
The point I want
to emphasize here

00:17:33.700 --> 00:17:37.060
is that, especially if
you have a lot of data

00:17:37.060 --> 00:17:39.160
in your organization,
you really should

00:17:39.160 --> 00:17:41.830
be thinking about how to make
distributed trading work well

00:17:41.830 --> 00:17:44.380
and what you should
be running to do that.

00:17:44.380 --> 00:17:46.600
As Megan said, there's
like a 58x speed

00:17:46.600 --> 00:17:48.640
up when running
on 64 GPUs and you

00:17:48.640 --> 00:17:50.890
can imagine what
kind of effect that

00:17:50.890 --> 00:17:55.000
would have on the engineers
working with TensorFlow,

00:17:55.000 --> 00:17:57.100
in training TensorFlow models.

00:17:57.100 --> 00:17:59.140
And thank you.

00:17:59.140 --> 00:18:03.390
[MUSIC PLAYING]

