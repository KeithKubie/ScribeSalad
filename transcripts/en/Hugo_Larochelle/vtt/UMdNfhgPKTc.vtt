WEBVTT

1
00:00:00.510 --> 00:00:01.160
Yeah.

2
00:00:01.160 --> 00:00:05.780
<v 1>In this video we'll see a second algorithm for updating the dictionary matrix in</v>

3
00:00:05.781 --> 00:00:08.400
the spar schooling model.
And a,
this,

4
00:00:08.401 --> 00:00:11.330
a different algorithm is going to be based on blah,
blah,

5
00:00:11.340 --> 00:00:13.190
coordinate descent optimization.

6
00:00:15.060 --> 00:00:18.570
So we're looking again at the problem of uh,

7
00:00:18.630 --> 00:00:21.960
optimizing the Dictionary d,
uh,

8
00:00:21.961 --> 00:00:26.850
where we are going to assume that the spires representations for each input in

9
00:00:26.851 --> 00:00:31.350
our dataset are fixed and do not depend on the,
and a,

10
00:00:31.351 --> 00:00:32.820
so if we do this,
we've uh,

11
00:00:32.930 --> 00:00:37.320
seen before that it corresponds to minimizing with respect to our dictionary

12
00:00:37.530 --> 00:00:42.530
only the average reconstruction term because we're assuming that h of x is

13
00:00:42.651 --> 00:00:45.930
fixed.
So we don't have this term here anymore.
Okay.

14
00:00:46.230 --> 00:00:49.980
And we have the constraint that again,
the columns of Matrix d,

15
00:00:49.981 --> 00:00:53.880
which are the atoms in our dictionary,
a must be of unit norm.

16
00:00:57.790 --> 00:01:02.590
So this alternative,
a algorithm is going to be based on,
uh,
a,

17
00:01:02.680 --> 00:01:07.000
uh,
an idea for a type of algorithm for optimization known as black corner.

18
00:01:07.001 --> 00:01:09.850
That corner dissent,
uh,

19
00:01:09.960 --> 00:01:14.960
it's cornered descent because we are going to optimize with respect to each

20
00:01:15.311 --> 00:01:19.480
single variable while keeping the others fixed.

21
00:01:19.481 --> 00:01:22.900
And that's how we're going to update each variable,
which respect to which each.

22
00:01:22.901 --> 00:01:24.830
So each parameter with respect to Itch,
uh,

23
00:01:24.860 --> 00:01:29.860
we're optimizing and it's a block corner descent algorithm because we are going

24
00:01:30.101 --> 00:01:34.000
to do that for not just a single variable retrospect to all others,

25
00:01:34.001 --> 00:01:37.840
but with respect to a block of variables.
In our case,

26
00:01:38.080 --> 00:01:42.710
the blocks are going to be each atom,
each column,
uh,

27
00:01:42.730 --> 00:01:44.530
Jay of,
um,

28
00:01:44.620 --> 00:01:48.580
our Matrix d and uh,
uh,

29
00:01:48.670 --> 00:01:51.040
the advantage is going to be that we'll get an update,

30
00:01:51.041 --> 00:01:53.110
we'll which does not require a learning rate.

31
00:01:53.111 --> 00:01:58.111
So what we'll actually do is that we'll consider updating only the JF column of

32
00:01:59.500 --> 00:02:04.500
our Dictionary Matrix deep and we actually find the best value of that column

33
00:02:05.891 --> 00:02:10.240
with respect to all other parameters.
So we'll actually find the,
uh,
uh,

34
00:02:10.270 --> 00:02:12.790
the optimum with respect to just this column,

35
00:02:13.180 --> 00:02:16.840
ignoring also the unit long constraint.
And by doing this,

36
00:02:16.841 --> 00:02:21.550
then we don't have a learning.
We just do a,
a step that has us reach exactly,

37
00:02:21.850 --> 00:02:24.760
uh,
the,
the optimum retrospected,
the jet column.

38
00:02:25.600 --> 00:02:29.560
And then what we do is that we just iterate and cycle over each column.

39
00:02:29.561 --> 00:02:31.710
So we'll do this for the first column,
second column.

40
00:02:31.730 --> 00:02:35.470
So on once we reach the last column would go back to the first column.

41
00:02:35.500 --> 00:02:37.840
And we had to write like this until convergence.

42
00:02:38.110 --> 00:02:42.910
And we also have to introduce a projection step to make sure that the,
uh,
uh,

43
00:02:42.940 --> 00:02:46.630
columns of,
uh,
r Adams off unit norm.
Okay.

44
00:02:47.050 --> 00:02:49.390
So let's see how we can do this.
Uh,

45
00:02:49.391 --> 00:02:53.110
so let's say we are optimizing which respected the JF column and we keep all of

46
00:02:53.111 --> 00:02:56.680
the column,
the other columns of Matrix d,
uh,
fixed.

47
00:02:57.310 --> 00:02:58.960
So one way of finding the optimum,

48
00:02:59.110 --> 00:03:02.680
just find a fixed points that is take the gradient of the reconstruction error

49
00:03:02.681 --> 00:03:06.910
and set it to zero.
So if we go back at our objective,

50
00:03:07.360 --> 00:03:12.360
if I want the Green Bridge suspected that g column of the reconstruction of the,

51
00:03:12.761 --> 00:03:15.610
sorry,
of the Matrix d here at to take the,
uh,

52
00:03:15.650 --> 00:03:18.100
they're there with respect to the reconstruction terms.

53
00:03:18.101 --> 00:03:20.440
So two would disappear and it would,

54
00:03:20.710 --> 00:03:22.780
so we have a two here which would cancel out,

55
00:03:23.200 --> 00:03:28.080
and then we'd have just parenthesis like this.
And I have to multiply by,
uh,

56
00:03:28.120 --> 00:03:32.520
the partial derivative of the reconstruction with respect to,
uh,

57
00:03:32.560 --> 00:03:36.610
the elements in my JF column of Matrix Steam and a,

58
00:03:36.611 --> 00:03:37.930
so the Matrix,

59
00:03:38.080 --> 00:03:43.080
that gea column is going to be multiplied by Djf element of my representation.

60
00:03:43.690 --> 00:03:48.610
So all I have here is that I'll have a h of x,

61
00:03:48.611 --> 00:03:53.611
t j because that's the term that's being multiplied by Dgf,

62
00:03:54.400 --> 00:03:57.370
uh,
the elements in the JF column of Matrix team.

63
00:03:59.390 --> 00:04:01.190
Okay.
So that's what I have here actually.

64
00:04:01.750 --> 00:04:02.583
<v 2>Uh,</v>

65
00:04:02.940 --> 00:04:05.670
<v 1>so this is exactly why that derived in the previous slide.</v>

66
00:04:06.630 --> 00:04:08.020
So what I want to do is fine.

67
00:04:08.021 --> 00:04:13.021
What's the value for the GF column of d such that this is satisfied so that the,

68
00:04:14.220 --> 00:04:17.310
uh,
gradient is equal to zero.

69
00:04:18.000 --> 00:04:22.800
And I'm going to assume all of that,
all other columns are fixed.
So,
uh,

70
00:04:22.830 --> 00:04:26.940
first thing to notice is that this reconstruction is really just a,

71
00:04:26.941 --> 00:04:29.100
some of the columns,

72
00:04:29.460 --> 00:04:34.460
all columns of Matrix d multiplied by each associated element in,

73
00:04:35.040 --> 00:04:37.890
uh,
the,
uh,
sparse coding representation.

74
00:04:38.370 --> 00:04:41.490
And so what I've done here is that I've separated this,

75
00:04:41.491 --> 00:04:46.491
some of overall columns into a sum over all columns different than j and then

76
00:04:48.451 --> 00:04:52.050
just a single term that,
that that corresponds to.
Uh,

77
00:04:52.080 --> 00:04:55.590
so the term associated with the JF column of Matrix team.
Okay,

78
00:04:55.930 --> 00:04:58.710
I'm doing this because I want to separate out the column,

79
00:04:58.711 --> 00:05:02.970
the GF column here so I can put it back on the left and isolated and get an

80
00:05:02.971 --> 00:05:05.300
expression for it.
Okay.

81
00:05:06.260 --> 00:05:11.260
Now what I can do is that I can now distribute this,

82
00:05:12.490 --> 00:05:12.830
uh,

83
00:05:12.830 --> 00:05:17.600
into the parenthesis so I can just take this term and put it on the left.

84
00:05:18.230 --> 00:05:20.930
So what I would have is that if I multiply that side,

85
00:05:20.931 --> 00:05:25.931
I'd have a squared here and then I have h x t j right here.

86
00:05:27.920 --> 00:05:30.470
And then,
uh,
if I have this,

87
00:05:30.471 --> 00:05:33.920
then I can just take the sum over t of justice term,
uh,

88
00:05:33.921 --> 00:05:37.370
which is a negative and put it on the left.

89
00:05:37.460 --> 00:05:38.780
So that's exactly what I've done here.

90
00:05:38.781 --> 00:05:43.781
I've got my summer over t of the Juke column of the Times h x t squared j.

91
00:05:47.930 --> 00:05:50.750
So that's this,
this whole term here with the sun.

92
00:05:51.350 --> 00:05:55.760
And then I'm keeping whatever's left on the right of the right hand side of my

93
00:05:55.761 --> 00:06:00.410
equation.
So I see that what I wrote here is exactly here.

94
00:06:01.010 --> 00:06:01.843
<v 3>Okay,</v>

95
00:06:03.010 --> 00:06:03.910
<v 1>no,
next,</v>

96
00:06:04.150 --> 00:06:07.870
well I want to isolate with respect to this and now I notice that it does not

97
00:06:07.871 --> 00:06:12.871
depend on tea so I can actually put it outside of the sun and then I'd be left

98
00:06:14.051 --> 00:06:17.920
with the Jf Colin Times the sum of disarm,

99
00:06:17.921 --> 00:06:19.390
which does depend on teeth.

100
00:06:19.660 --> 00:06:23.950
And so I can then take that factor in divided on each side of the equation.

101
00:06:24.250 --> 00:06:29.020
So I'd,
I'd be left with just the JF column.
I have one over this,

102
00:06:29.021 --> 00:06:33.660
some over hxd squared,
uh,
j.
So this is,
uh,

103
00:06:34.060 --> 00:06:38.860
this is this thing here that,
uh,
is now on the right hand side.

104
00:06:39.340 --> 00:06:44.320
And then this here is just the same thing here.
Okay.

105
00:06:44.830 --> 00:06:45.850
So now by doing this,

106
00:06:45.851 --> 00:06:50.851
I've been able to isolate for the JF column and a,

107
00:06:50.951 --> 00:06:55.330
and so that's a fixed point of my reconstruction error with respect only to the

108
00:06:55.331 --> 00:06:59.170
JF column of Matrix team.
And now the A,

109
00:06:59.180 --> 00:07:00.910
if we wanted to make sure that this is a minimum,

110
00:07:00.911 --> 00:07:05.380
we need to look at a second order conditions or make sure that the secondary Div

111
00:07:05.400 --> 00:07:09.340
at this point,
we're actually at this vector.

112
00:07:09.550 --> 00:07:13.570
The action would need to be,
uh,
uh,
say a positive,

113
00:07:13.571 --> 00:07:16.420
definite and a.
So if we did that,

114
00:07:16.421 --> 00:07:19.810
then we'd actually find that it is indeed a minimum.
So I won't do it here,

115
00:07:19.811 --> 00:07:21.490
but you can do it as an exercise.

116
00:07:22.570 --> 00:07:27.570
And so this gives me a analytical expression for obtaining a new value for my GF

117
00:07:30.010 --> 00:07:30.250
Adam.

118
00:07:30.250 --> 00:07:34.420
And a notice that it does not require learning rate like the projected grade in

119
00:07:34.430 --> 00:07:36.640
dissent algorithm that we've seen in the previous video.

120
00:07:36.910 --> 00:07:38.350
So that's one of the big advantage here.

121
00:07:38.351 --> 00:07:41.710
It's just more convenient not to have to specify a learning rate.

122
00:07:45.070 --> 00:07:48.640
Now let's play a little bit with that expression that uh,

123
00:07:48.641 --> 00:07:53.470
so there's an analytical expression for the new value for the JF column a.

124
00:07:53.471 --> 00:07:57.820
So one thing that we can do is that we can,
uh,

125
00:07:57.850 --> 00:08:02.140
take this term here,
the scalar and put it inside the parenthesis.

126
00:08:02.320 --> 00:08:06.760
So now I have it here and also have it here.

127
00:08:07.390 --> 00:08:08.223
And uh,

128
00:08:08.250 --> 00:08:12.850
so it's been introduced here and multiplied here.

129
00:08:13.420 --> 00:08:17.440
And then the other thing I can do is that I can take the sum over tea and put
it,

130
00:08:18.110 --> 00:08:22.330
uh,
I guess distributed in the parenthesis.
I have it right in front here,

131
00:08:22.331 --> 00:08:25.750
but also right in front here.
So before this other,

132
00:08:25.751 --> 00:08:27.580
some over I instead.

133
00:08:27.581 --> 00:08:31.870
So I'm just switching the order in the sun for this term here.
So if I,

134
00:08:31.900 --> 00:08:32.860
if I do this,

135
00:08:33.070 --> 00:08:38.070
then I get this term here and I get this other term here where now the sum is

136
00:08:39.911 --> 00:08:41.860
inside.
Now,
uh,

137
00:08:42.190 --> 00:08:47.190
the advantage of doing this is that now we notice that I have a few terms which

138
00:08:47.440 --> 00:08:52.180
don't depend on matrix t.
So first here,

139
00:08:52.480 --> 00:08:56.480
this term here,
which is the sum of Dj value in my,
uh,

140
00:08:56.520 --> 00:08:58.830
sparse codes for all the training examples.

141
00:08:59.450 --> 00:09:04.170
I may examples in my dataset that doesn't depend on d f h is a fixed,

142
00:09:04.560 --> 00:09:09.030
uh,
here.
Also,
we don't see,
uh,
in the expression the value of d.

143
00:09:09.210 --> 00:09:13.560
It's just x multiplied by a DJF element of my,

144
00:09:13.561 --> 00:09:16.050
as far as codes.
And the sum over all my dataset.

145
00:09:16.560 --> 00:09:20.540
And similarly here we have the cross product between the IMF element of the

146
00:09:20.541 --> 00:09:23.910
sparse code and the j element summed over all the data sets.

147
00:09:23.911 --> 00:09:25.410
So that also doesn't depend on the,

148
00:09:26.190 --> 00:09:30.720
so what I could do is precompute a matrix a which is just the,

149
00:09:30.721 --> 00:09:35.220
some of the other product between the sparks coding representations for all

150
00:09:35.221 --> 00:09:39.870
training examples and a,
so the element Ij would just be,

151
00:09:40.560 --> 00:09:43.530
so the element igm matrix a would just be disturbed here.

152
00:09:44.280 --> 00:09:49.280
And similarly the element JJ would be just the diagonal of a to the element on a

153
00:09:50.911 --> 00:09:55.911
diagonal matrix a which is be that term here as well as four B.

154
00:09:56.401 --> 00:10:00.680
If we did the other product of the input times the transpose a,

155
00:10:00.710 --> 00:10:05.010
so the other product of the input and it's far as representation.
Well,

156
00:10:05.220 --> 00:10:09.840
if I just took the JFF column of that Matrix B,

157
00:10:09.900 --> 00:10:12.030
I would get disturbed.
Okay.

158
00:10:12.750 --> 00:10:17.670
So now the advantage of doing this is that I can do this before I do any

159
00:10:17.671 --> 00:10:22.590
optimization of matrix t and keep that in memory and always use this
computation.

160
00:10:22.660 --> 00:10:27.510
Uh,
instead of actually computing it every time I update one Adam.

161
00:10:28.020 --> 00:10:28.853
Okay.

162
00:10:28.980 --> 00:10:31.520
So in terms of an algorithm that's going to be much more efficient because I can

163
00:10:31.521 --> 00:10:35.940
do just do it once a before I start optimizing with respect to the matrix.

164
00:10:37.080 --> 00:10:42.080
And so here the expression we have is just replacing with the different a and B

165
00:10:43.951 --> 00:10:47.900
terms.
And I'm doing another thing which is that,
so,
uh,

166
00:10:47.910 --> 00:10:49.930
if I want to write it,
um,

167
00:10:50.070 --> 00:10:53.790
so this sum is over all eyes except Jay.

168
00:10:54.320 --> 00:10:57.510
Now if I take the column a Matrix,

169
00:10:57.800 --> 00:11:02.070
the column Djf column of Matrix a,
I'm multiplying by Dee.
I'm going to get,

170
00:11:02.450 --> 00:11:06.470
I'm going to get this sum here,
but from one to a,
I'm,

171
00:11:06.740 --> 00:11:08.920
I'm going to get the whole song which was like to ally.

172
00:11:09.000 --> 00:11:12.270
So I have to subtract from this term here,

173
00:11:12.420 --> 00:11:17.400
the case where i's equal to Jane.
So,
uh,
because there's a negative here,

174
00:11:17.401 --> 00:11:22.350
I need to add this term.
And so that's why I have this term here as well.
Okay.

175
00:11:23.520 --> 00:11:26.340
So now I have an expression which is much simpler.

176
00:11:26.341 --> 00:11:28.920
Assuming I've computed all of these terms,

177
00:11:28.921 --> 00:11:33.620
which just corresponds to computing is some of our products between either,
uh,

178
00:11:33.660 --> 00:11:34.770
the,
uh,

179
00:11:34.800 --> 00:11:38.630
as far as coding representations with itself or the input with the sparkling

180
00:11:38.670 --> 00:11:43.650
representation.
And so now we're ready to look at,
uh,

181
00:11:43.651 --> 00:11:48.570
what the code looks like.
So,
uh,
here I would need to compute,

182
00:11:48.880 --> 00:11:53.800
uh,
a and B as specified before with the,
uh,

183
00:11:54.010 --> 00:11:54.220
you know,

184
00:11:54.220 --> 00:11:58.870
expression that we had on the previous slide and then one that once this is
done,

185
00:11:59.260 --> 00:12:04.060
uh,
as long as Dee is not converge for every iteration,
what I do is that I cycle,

186
00:12:04.061 --> 00:12:09.061
I iterate over each column of the Matrix d so each Adam in my dictionary and

187
00:12:09.791 --> 00:12:14.770
then I perform my corner descent update.
So that's the expression I had before.

188
00:12:15.340 --> 00:12:19.920
And then also have to project back on the,
uh,
in the space of,
uh,
uh,

189
00:12:19.990 --> 00:12:23.670
you did norm,
uh,
Adams.
So I have to just take all my,
my,
my column,

190
00:12:23.671 --> 00:12:27.910
I just updated and we normalize it because I want my Adams in my dictionary to

191
00:12:27.911 --> 00:12:30.370
be normalized.
And then I had to read like this,

192
00:12:31.030 --> 00:12:34.870
and this algorithm will actually convert to solution and notice that now this

193
00:12:34.871 --> 00:12:38.470
algorithm,
unlike the projected grade in dissent,
doesn't have a learning rate.

194
00:12:38.471 --> 00:12:41.680
So we don't have to specify it.
So it's a more convenient algorithm to use.

195
00:12:42.760 --> 00:12:47.310
So like I said,
this is known as a pluck cornered descent algorithm.

196
00:12:47.311 --> 00:12:49.960
It's a could say it's a projected Lockhorn dissent,

197
00:12:50.120 --> 00:12:54.790
a algorithm because of the projection we have here.
Uh,
and,

198
00:12:54.791 --> 00:12:55.301
uh,

199
00:12:55.301 --> 00:12:59.020
and its big advantage too to remember is that it does not require learning rate.

200
00:12:59.021 --> 00:13:03.910
So sometimes people would prefer using this,
uh,
for,
for convenience.
All right,

201
00:13:03.911 --> 00:13:07.540
so that's another alternative for updating the dictionary,
uh,

202
00:13:07.570 --> 00:13:08.920
during a sparse coding.

