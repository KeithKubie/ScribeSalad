WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.900
[MUSIC PLAYING]

00:00:03.900 --> 00:00:06.440
MARTIN WICKE: So Dan
mentioned that things

00:00:06.440 --> 00:00:07.400
will be tweeted out.

00:00:07.400 --> 00:00:12.380
If you want to tweet something,
use TFDevSummit and people

00:00:12.380 --> 00:00:13.330
can see it.

00:00:13.330 --> 00:00:15.020
A quick shout-out
to all the people

00:00:15.020 --> 00:00:18.890
who've joined us in viewing
parties all around the world,

00:00:18.890 --> 00:00:21.020
literally.

00:00:21.020 --> 00:00:22.670
It's awesome to
have you all here.

00:00:22.670 --> 00:00:25.190
And of course, with
the live stream,

00:00:25.190 --> 00:00:27.550
that's not limited
to physical presence.

00:00:27.550 --> 00:00:32.012
OK, I think I'm all set up.

00:00:32.012 --> 00:00:32.720
Thanks very much.

00:00:32.720 --> 00:00:35.490
I'm Martin Wicke.

00:00:35.490 --> 00:00:38.030
I'm going to talk
about high-level APIs,

00:00:38.030 --> 00:00:42.244
and so really--

00:00:42.244 --> 00:00:43.160
why would you do that?

00:00:43.160 --> 00:00:44.380
TensorFlow is perfect.

00:00:44.380 --> 00:00:48.610
Well, so shortly after we
released TensorFlow, really,

00:00:48.610 --> 00:00:50.980
it got a lot of nice reviews.

00:00:50.980 --> 00:00:52.510
And then some of
the things people

00:00:52.510 --> 00:00:54.452
said, well, it's
really low level.

00:00:54.452 --> 00:00:56.160
And this is my favorite
quote about this.

00:00:56.160 --> 00:00:57.430
The first thing you
realize about TensorFlow--

00:00:57.430 --> 00:00:59.530
it's a really low-level
library, meaning you'll

00:00:59.530 --> 00:01:01.849
be multiplying
matrices and vectors.

00:01:01.849 --> 00:01:03.640
Now, there's nothing
wrong with multiplying

00:01:03.640 --> 00:01:04.599
matrices and vectors.

00:01:04.599 --> 00:01:07.600
In fact, it is one of
TensorFlow's great strengths

00:01:07.600 --> 00:01:10.540
that you can do this
at a very low level.

00:01:10.540 --> 00:01:14.260
It allows us to
be very flexible.

00:01:14.260 --> 00:01:16.870
You can easily combine
TensorFlow operators to create

00:01:16.870 --> 00:01:18.520
any algorithm you can think of.

00:01:18.520 --> 00:01:20.020
Because they're at
a very low level,

00:01:20.020 --> 00:01:22.432
you don't have to fit
them together awkwardly.

00:01:22.432 --> 00:01:23.890
It's very clear
what a matmul does.

00:01:23.890 --> 00:01:27.145
It will always be a matmul.

00:01:27.145 --> 00:01:29.020
It's used in many
algorithms, so you can just

00:01:29.020 --> 00:01:31.990
use it to do whatever you want.

00:01:31.990 --> 00:01:36.170
It's also extensible,
so you can recombine

00:01:36.170 --> 00:01:41.870
these small little packets
of code in arbitrary ways.

00:01:41.870 --> 00:01:43.660
And then you can
create a new algorithm

00:01:43.660 --> 00:01:47.942
without having to change core
TensorFlow, which is great.

00:01:47.942 --> 00:01:49.650
And so a lot of,
actually, our Python API

00:01:49.650 --> 00:01:54.210
is built that way, where we have
taken the op kernels or the ops

00:01:54.210 --> 00:01:56.220
and combined them
simply in Python

00:01:56.220 --> 00:01:57.780
to create new functionality.

00:01:57.780 --> 00:02:00.120
And as you will see, we'll
use those extensively

00:02:00.120 --> 00:02:02.600
to make high-level APIs.

00:02:02.600 --> 00:02:04.200
It's also maintainable.

00:02:04.200 --> 00:02:08.310
So have some history with
this at Google, obviously.

00:02:08.310 --> 00:02:11.910
And other people have probably
experienced this as well.

00:02:11.910 --> 00:02:15.120
If you have large,
fused operations,

00:02:15.120 --> 00:02:16.320
they accumulate features.

00:02:16.320 --> 00:02:19.050
And as they accumulate
features, they accumulate crud,

00:02:19.050 --> 00:02:22.050
and things go out of
fashion, and you end up

00:02:22.050 --> 00:02:24.662
with these large,
unwieldy pieces.

00:02:24.662 --> 00:02:26.370
And so we avoid that
completely by having

00:02:26.370 --> 00:02:28.840
these small operations
that are really timeless.

00:02:28.840 --> 00:02:31.600
Nothing much was going
to change to a matmul.

00:02:31.600 --> 00:02:34.950
It's always going to be a
matmul, so that's great.

00:02:34.950 --> 00:02:38.010
However, of course, this
also has disadvantages.

00:02:38.010 --> 00:02:39.810
And it has
disadvantages for users.

00:02:39.810 --> 00:02:42.780
And one of them is that we
lack higher-level primitives

00:02:42.780 --> 00:02:47.100
in core TensorFlow if we only
stay at this very low level.

00:02:49.950 --> 00:02:55.959
And we may have all the tools
you need to build your model,

00:02:55.959 --> 00:02:58.250
but you may be working at a
level of abstraction that's

00:02:58.250 --> 00:03:00.166
actually not suitable
for the kind of thinking

00:03:00.166 --> 00:03:01.210
that you're doing.

00:03:01.210 --> 00:03:03.460
And then also, we cannot
really publish out-of-the-box

00:03:03.460 --> 00:03:05.010
algorithms this way.

00:03:05.010 --> 00:03:09.730
And this is why we are
building a high-level API

00:03:09.730 --> 00:03:11.230
for TensorFlow.

00:03:11.230 --> 00:03:15.130
So starting in 1.0, we're
creating this high-level API.

00:03:15.130 --> 00:03:16.600
We're building
this API by taking

00:03:16.600 --> 00:03:19.240
components that have been built
over time by other people.

00:03:19.240 --> 00:03:21.550
Now, the idea of
building high-level APIs

00:03:21.550 --> 00:03:24.370
on top of TensorFlow is
not particularly novel.

00:03:24.370 --> 00:03:26.170
So lots of people have done it.

00:03:26.170 --> 00:03:27.910
Much of that has
happened in contrib,

00:03:27.910 --> 00:03:31.510
in our contrib
part of TensorFlow.

00:03:31.510 --> 00:03:33.020
So there's been
tf.contrib.learn,

00:03:33.020 --> 00:03:35.830
tf.contrib.slim,
SKFlow, which is

00:03:35.830 --> 00:03:37.660
integrated in tf.contrib.learn.

00:03:37.660 --> 00:03:39.490
And we're taking
the pieces of those

00:03:39.490 --> 00:03:41.680
and making a fully
supported, high-level API

00:03:41.680 --> 00:03:45.110
inside of core TensorFlow.

00:03:45.110 --> 00:03:46.790
Now, what that
should give you is

00:03:46.790 --> 00:03:48.410
you will be able
to iterate faster,

00:03:48.410 --> 00:03:52.970
over model architectures
or in general, things

00:03:52.970 --> 00:03:55.761
that you may want to
build with TensorFlow.

00:03:55.761 --> 00:03:57.260
Those higher-level
abstractions will

00:03:57.260 --> 00:03:59.810
allow us to encode
best practices for you,

00:03:59.810 --> 00:04:03.100
so it's harder for
you to make mistakes.

00:04:03.100 --> 00:04:05.870
And we'll be able to
provide primitives

00:04:05.870 --> 00:04:08.290
that will make it really easy
for you to write code that's

00:04:08.290 --> 00:04:10.720
scalable right out
the door, and you

00:04:10.720 --> 00:04:12.710
don't have to re-engineer
it once you realize,

00:04:12.710 --> 00:04:15.160
oh, a single machine
is not enough

00:04:15.160 --> 00:04:18.589
or a single GPU is not enough.

00:04:18.589 --> 00:04:20.440
And then finally,
we'll prepare all

00:04:20.440 --> 00:04:23.560
of this code to make it
really easy for you to deploy.

00:04:23.560 --> 00:04:27.070
So at the level of abstraction
that we'll be talking about,

00:04:27.070 --> 00:04:29.627
you'll be able to say, OK,
export this to serving.

00:04:29.627 --> 00:04:31.210
And you'll hear a
whole talk about how

00:04:31.210 --> 00:04:32.894
to serve models later.

00:04:32.894 --> 00:04:34.810
Export this to serving,
and it will just work.

00:04:34.810 --> 00:04:37.540
That is just a promise.

00:04:37.540 --> 00:04:38.470
So how do we do this?

00:04:38.470 --> 00:04:40.110
So you've seen this before.

00:04:40.110 --> 00:04:42.460
Rajat's talked about it.

00:04:42.460 --> 00:04:45.850
This is a very
popular illustration,

00:04:45.850 --> 00:04:51.140
and what we're doing
is, on the bottom,

00:04:51.140 --> 00:04:53.335
you have TensorFlow that
you know about-- the op

00:04:53.335 --> 00:04:55.520
kernels, the TensorFlow
engine, and then the front

00:04:55.520 --> 00:04:57.186
ends that you're
typically dealing with.

00:04:57.186 --> 00:05:00.440
And all of the things that I'm
going to be talking about today

00:05:00.440 --> 00:05:01.970
are built on the
Python front end.

00:05:01.970 --> 00:05:05.990
So the first step is to build
a higher level of abstraction

00:05:05.990 --> 00:05:11.030
for building models,
and we call this layers.

00:05:11.030 --> 00:05:13.070
It is modeled after
neural network layers,

00:05:13.070 --> 00:05:15.780
but it's not limited to
neural networks, necessarily--

00:05:15.780 --> 00:05:18.200
and basically,
larger pieces of code

00:05:18.200 --> 00:05:21.120
that are grouped together
in sensible ways.

00:05:21.120 --> 00:05:25.790
And on top of that,
we offer Estimator,

00:05:25.790 --> 00:05:28.302
which is a class that
streamlines all of the things

00:05:28.302 --> 00:05:30.760
that you have to do in order
to actually train and evaluate

00:05:30.760 --> 00:05:32.857
a model, which is
most of the work.

00:05:32.857 --> 00:05:34.690
The actual model
architecture is usually not

00:05:34.690 --> 00:05:37.080
all that complicated.

00:05:37.080 --> 00:05:39.070
And for simple
cases, you're also

00:05:39.070 --> 00:05:40.750
going to be able to
use the Keras model.

00:05:40.750 --> 00:05:43.240
And again, there's
a talk directly

00:05:43.240 --> 00:05:44.404
after this one about Keras.

00:05:44.404 --> 00:05:45.820
And if you're
interested in Keras,

00:05:45.820 --> 00:05:47.837
if you're a user of Keras,
you should definitely

00:05:47.837 --> 00:05:48.420
wait for that.

00:05:48.420 --> 00:05:50.762
There's going to be
a lot more detail.

00:05:50.762 --> 00:05:52.720
So for simple cases, you
can use a Keras model.

00:05:52.720 --> 00:05:55.303
The estimator is a little more
production-oriented or for more

00:05:55.303 --> 00:05:57.230
complex cases.

00:05:57.230 --> 00:05:59.887
And then finally, we have
now this estimator interface,

00:05:59.887 --> 00:06:00.470
which is nice.

00:06:00.470 --> 00:06:03.050
And we can offer you models
that are completely built out

00:06:03.050 --> 00:06:04.550
of the box, ready to go.

00:06:04.550 --> 00:06:07.640
And there's a number of talks
in the afternoon that actually

00:06:07.640 --> 00:06:11.450
will talk about what kind
of models are offered

00:06:11.450 --> 00:06:13.560
and how they work.

00:06:13.560 --> 00:06:17.960
OK, so let me start
by introducing layers.

00:06:17.960 --> 00:06:21.800
So you've seen this before,
as well, in Dandelion's talk.

00:06:21.800 --> 00:06:25.360
This is a convolutional
network that is used for--

00:06:25.360 --> 00:06:28.451
well, this particular one, I
think, is used on [INAUDIBLE].

00:06:28.451 --> 00:06:30.889
This is written down
in a way that you may

00:06:30.889 --> 00:06:32.180
see it written down in a paper.

00:06:32.180 --> 00:06:33.890
This could be an
illustration in a paper,

00:06:33.890 --> 00:06:35.360
except for the color scheme.

00:06:35.360 --> 00:06:38.510
And our goal would be that you
can write this network down

00:06:38.510 --> 00:06:42.860
in code, without switching
levels of abstraction here.

00:06:42.860 --> 00:06:45.050
So I want each of these
layers in the neural network

00:06:45.050 --> 00:06:50.070
to be represented with a
single function in my code.

00:06:50.070 --> 00:06:52.730
So if I have a
convolutional layer,

00:06:52.730 --> 00:06:56.450
I want there to be a
convolutional layer function

00:06:56.450 --> 00:06:59.570
that does the same
thing, conceptually,

00:06:59.570 --> 00:07:02.000
and that I can just
use and replace

00:07:02.000 --> 00:07:03.570
my illustration with some code.

00:07:03.570 --> 00:07:05.027
So if I have a
convolution layer,

00:07:05.027 --> 00:07:07.110
a convolution function,
if I have a pooling layer,

00:07:07.110 --> 00:07:10.669
I make a pooling function, and
so on, and so on, and so on.

00:07:10.669 --> 00:07:11.460
I've added dropout.

00:07:11.460 --> 00:07:12.590
That's the only
difference to the one

00:07:12.590 --> 00:07:14.006
that you've seen
earlier, I think,

00:07:14.006 --> 00:07:15.250
because dropout is better.

00:07:17.890 --> 00:07:20.980
OK, so now, if you write your
models this way, what happens

00:07:20.980 --> 00:07:23.227
is that you avoid a whole
lot of cognitive load.

00:07:23.227 --> 00:07:25.060
You don't have to worry
so much about, oh, I

00:07:25.060 --> 00:07:26.018
have to make variables.

00:07:26.018 --> 00:07:28.930
I have to call this function,
that function, reshape.

00:07:28.930 --> 00:07:31.540
All of this is done for you,
and there's a lot of things

00:07:31.540 --> 00:07:33.040
that you probably
wouldn't have done

00:07:33.040 --> 00:07:37.240
if you wrote it from scratch
that are really useful to have.

00:07:37.240 --> 00:07:40.680
So again, you will be able
to iterate much faster

00:07:40.680 --> 00:07:42.750
using this type of working.

00:07:42.750 --> 00:07:46.290
And then we introduce, or we
encode these best practices

00:07:46.290 --> 00:07:47.250
for you.

00:07:47.250 --> 00:07:48.720
And a big one is scoping.

00:07:48.720 --> 00:07:53.850
So you've seen that the graph
visualizer actually uses scopes

00:07:53.850 --> 00:07:55.980
to group nodes in
the graph together,

00:07:55.980 --> 00:07:58.634
so it's easier to get an
overview over big networks.

00:07:58.634 --> 00:08:00.550
All of that is done for
you if you use layers,

00:08:00.550 --> 00:08:02.490
so each layer will show
up as a little box.

00:08:02.490 --> 00:08:04.489
And only if you want to
inspect the actual going

00:08:04.489 --> 00:08:08.270
on inside, which you probably
don't, you have to move in.

00:08:08.270 --> 00:08:10.535
All the layers support
variable batch size.

00:08:10.535 --> 00:08:12.435
So once you move
it to serving, you

00:08:12.435 --> 00:08:14.810
don't have to re-engineer your
model because suddenly you

00:08:14.810 --> 00:08:16.220
don't have a batch size of 32.

00:08:16.220 --> 00:08:18.000
You have a batch size of 1.

00:08:18.000 --> 00:08:21.020
All these kind of things are
things that are easy to forget,

00:08:21.020 --> 00:08:24.391
but we do them for
you, so it's OK.

00:08:24.391 --> 00:08:26.140
I want to mention that
all of these layers

00:08:26.140 --> 00:08:27.223
are compatible with Keras.

00:08:27.223 --> 00:08:30.190
So if you use one of
these functional layers,

00:08:30.190 --> 00:08:31.650
that is in fact equivalent.

00:08:31.650 --> 00:08:35.440
In fact, the implementation
is shared to saying,

00:08:35.440 --> 00:08:39.159
use a dense layer object
that is in Keras style,

00:08:39.159 --> 00:08:42.010
and apply it to the same inputs.

00:08:42.010 --> 00:08:44.290
And you'll hear about
this in the next talk,

00:08:44.290 --> 00:08:46.990
but soon we'll be
able to-- we'll

00:08:46.990 --> 00:08:49.480
have a Keras module
that is just completely

00:08:49.480 --> 00:08:50.940
compatible with the Keras API.

00:08:50.940 --> 00:08:53.190
And Francois will talk about
this in much more detail,

00:08:53.190 --> 00:08:55.450
so I'll leave it alone.

00:08:55.450 --> 00:08:59.570
OK, so this now makes it
really easy to build models.

00:08:59.570 --> 00:09:02.800
But models are really only a
small part of the total effort.

00:09:02.800 --> 00:09:04.889
And the real effort is
getting them to train,

00:09:04.889 --> 00:09:07.430
and they maybe getting them to
train in a distributed manner.

00:09:07.430 --> 00:09:08.870
And that's what
Estimator is for.

00:09:08.870 --> 00:09:14.180
Estimator is an interface
to abstract away

00:09:14.180 --> 00:09:16.280
what the actual
implementation looks like.

00:09:16.280 --> 00:09:19.850
So to start, let's
consider a generic model.

00:09:19.850 --> 00:09:21.700
Any model, any machine
learning model,

00:09:21.700 --> 00:09:23.450
will look somewhat like this.

00:09:23.450 --> 00:09:25.700
You'll have some
inputs, possibly labels

00:09:25.700 --> 00:09:28.040
if it's supervised learning.

00:09:28.040 --> 00:09:30.800
There's some sort of model,
and it produces predictions.

00:09:30.800 --> 00:09:33.890
Now, if that is
trainable, then this

00:09:33.890 --> 00:09:36.440
looks a little bit different,
where instead of predictions,

00:09:36.440 --> 00:09:39.710
you can also produce some
sort of training operations.

00:09:39.710 --> 00:09:42.170
Often, this training operation
is built on the predictions,

00:09:42.170 --> 00:09:44.216
but it's not necessarily so.

00:09:44.216 --> 00:09:45.590
And in TensorFlow
land, what that

00:09:45.590 --> 00:09:47.810
looks like is that
you have a train

00:09:47.810 --> 00:09:50.407
op that you call in the loop.

00:09:50.407 --> 00:09:52.240
And each time you call
it in loop, probably,

00:09:52.240 --> 00:09:55.270
it's going to feed a mini
batch or absorb a mini batch

00:09:55.270 --> 00:09:58.211
and then update the weights
of the model somehow.

00:09:58.211 --> 00:10:00.710
And we're not going to take a
position on what that op looks

00:10:00.710 --> 00:10:02.510
like exactly, but
it exists, and it's

00:10:02.510 --> 00:10:04.252
going to be run in the loop.

00:10:04.252 --> 00:10:06.085
Of course, if you're
doing machine learning,

00:10:06.085 --> 00:10:07.168
you also want to evaluate.

00:10:07.168 --> 00:10:10.080
And while for small
or for simple cases,

00:10:10.080 --> 00:10:12.350
you can use the predictions
to evaluate and just

00:10:12.350 --> 00:10:15.920
feed some test data into
it, in more general cases

00:10:15.920 --> 00:10:19.250
it's very useful to have a
separate evaluation graph

00:10:19.250 --> 00:10:24.950
or evaluation op that produces
your evaluation metrics.

00:10:24.950 --> 00:10:28.400
And again, because your
evaluation data set may not

00:10:28.400 --> 00:10:30.560
fit in memory, you want
to run this in a loop

00:10:30.560 --> 00:10:32.440
and feed it mini batches.

00:10:32.440 --> 00:10:34.390
So basically, what
we're saying here

00:10:34.390 --> 00:10:38.050
for Estimator is that every
model looks like this.

00:10:38.050 --> 00:10:42.060
And so we draw a box around
it, and we call it Estimator.

00:10:42.060 --> 00:10:44.792
And Estimator is really going to
be configured using this Model

00:10:44.792 --> 00:10:47.250
function, which effectively
contains the code that you have

00:10:47.250 --> 00:10:50.850
written anyway, which produces
this training op, this eval op,

00:10:50.850 --> 00:10:52.760
these predictions.

00:10:52.760 --> 00:10:54.410
But it's just a single function.

00:10:54.410 --> 00:10:56.090
You pass it to
Estimator, and then

00:10:56.090 --> 00:11:00.320
what you get in return is an
interface that always stays

00:11:00.320 --> 00:11:03.920
the same, and that you can
write downstream infrastructure

00:11:03.920 --> 00:11:08.580
against, and that takes away
some of the work that you would

00:11:08.580 --> 00:11:10.580
otherwise have to do
manually and every time you

00:11:10.580 --> 00:11:11.950
have the model.

00:11:11.950 --> 00:11:13.430
So in particular,
it encapsulates

00:11:13.430 --> 00:11:15.410
all the information and
all the necessary code

00:11:15.410 --> 00:11:19.250
about sessions and graphs, and
it has the loops inside of it

00:11:19.250 --> 00:11:20.870
so that you don't
have to write them.

00:11:20.870 --> 00:11:23.880
Writing these loops
properly is error prone,

00:11:23.880 --> 00:11:27.625
and by using an estimator, you
don't have to worry about this.

00:11:27.625 --> 00:11:29.750
You worry about what you
actually care about, which

00:11:29.750 --> 00:11:30.958
is the details of your model.

00:11:33.870 --> 00:11:37.060
So there's one more
thing that we added,

00:11:37.060 --> 00:11:43.610
which makes sense only in the
context of TensorFlow, which

00:11:43.610 --> 00:11:46.940
is that you can export your
model to a saved model, which

00:11:46.940 --> 00:11:50.630
is a data format that is
directly usable in TensorFlow

00:11:50.630 --> 00:11:54.660
Serving, which you'll
hear about later.

00:11:54.660 --> 00:11:56.790
OK, so this, again, it
encodes best practices.

00:11:56.790 --> 00:12:00.170
It deals with
end-of-input exceptions.

00:12:00.170 --> 00:12:04.110
It deals with workers going
down in disputed setting.

00:12:04.110 --> 00:12:08.430
It produces the right
summaries at the right time,

00:12:08.430 --> 00:12:11.300
all these things.

00:12:11.300 --> 00:12:13.670
You can easily
deploy it to serving

00:12:13.670 --> 00:12:15.680
by simply calling
the Export-Save model

00:12:15.680 --> 00:12:18.960
and using that save model
on the model server.

00:12:18.960 --> 00:12:20.220
And it's scalable by design.

00:12:20.220 --> 00:12:24.270
So what you get for free
here is, if you have a model

00:12:24.270 --> 00:12:27.240
that you can distribute
using data parallelism,

00:12:27.240 --> 00:12:32.160
this will simply work, and you
won't have to worry about it.

00:12:32.160 --> 00:12:36.236
So now I will tell you about
some models that we have built.

00:12:36.236 --> 00:12:37.860
And I won't tell you
about the details.

00:12:37.860 --> 00:12:40.470
You'll hear more about
that in the afternoon.

00:12:40.470 --> 00:12:43.050
And I won't bore you
with more diagrams,

00:12:43.050 --> 00:12:46.470
but let's just see some code.

00:12:46.470 --> 00:12:50.790
So here is a more
or less complete

00:12:50.790 --> 00:12:55.420
piece of code that lets
you write a linear model.

00:12:55.420 --> 00:12:57.556
It's a very simple
model, linear model.

00:12:57.556 --> 00:13:00.180
And you see the class name here
is Linear Regressor, so that is

00:13:00.180 --> 00:13:01.388
one of our canned estimators.

00:13:01.388 --> 00:13:05.130
It is an estimator in
the is-instance sense,

00:13:05.130 --> 00:13:09.129
but it has a slightly
different interface.

00:13:09.129 --> 00:13:11.670
Or it has the same interface,
but it has a slightly different

00:13:11.670 --> 00:13:13.210
constructor.

00:13:13.210 --> 00:13:15.210
The only thing I have to
tell this estimator is,

00:13:15.210 --> 00:13:19.890
what imports do you want or
do I want you to work on?

00:13:19.890 --> 00:13:25.210
And for this, we have made
this declarative language

00:13:25.210 --> 00:13:28.950
to specify input data,
which is Feature columns.

00:13:28.950 --> 00:13:30.900
And here in this
example, I just say,

00:13:30.900 --> 00:13:36.060
well, this model expects
a thing called Area

00:13:36.060 --> 00:13:37.920
is going to be a real number.

00:13:37.920 --> 00:13:40.800
And if you were to
parse it out of an input

00:13:40.800 --> 00:13:44.319
stream of some sort, let's
say a TF example, the field

00:13:44.319 --> 00:13:45.610
it has in there is square foot.

00:13:51.380 --> 00:13:54.170
Then there is another real
number, and it comes--

00:13:54.170 --> 00:13:55.640
the name is numrooms.

00:13:55.640 --> 00:13:57.380
And then there is an
integerize feature,

00:13:57.380 --> 00:14:00.650
which is really a string
that I treat as an integer.

00:14:00.650 --> 00:14:03.860
And I know that it's never
going to be higher than 100,000.

00:14:03.860 --> 00:14:05.720
That's a zip code.

00:14:05.720 --> 00:14:09.536
Now, the feature integerization
is a little bit--

00:14:09.536 --> 00:14:10.910
it's not entirely
straightforward

00:14:10.910 --> 00:14:12.160
if you want to do it yourself.

00:14:12.160 --> 00:14:15.140
So you can think of this as a
one-hot vector that comes in,

00:14:15.140 --> 00:14:17.264
although it's not implemented
that way because that

00:14:17.264 --> 00:14:18.950
would be terribly inefficient.

00:14:18.950 --> 00:14:21.380
But again, you don't really
have to worry about that.

00:14:21.380 --> 00:14:24.090
So now, all I need to do is--

00:14:24.090 --> 00:14:28.692
I already have an
estimator here,

00:14:28.692 --> 00:14:30.900
so I just have to call
[INAUDIBLE] and predict on it.

00:14:30.900 --> 00:14:32.536
The interface always
stays the same,

00:14:32.536 --> 00:14:34.410
which is nice because
if I want to experiment

00:14:34.410 --> 00:14:36.120
with different
ones, I can simply

00:14:36.120 --> 00:14:38.040
say, oh, let me swap that out.

00:14:38.040 --> 00:14:40.920
Instead of a linear model,
I want a DNN model, so

00:14:40.920 --> 00:14:43.270
a Dense Neural Network.

00:14:43.270 --> 00:14:45.610
And then all I have to do
is change the class name.

00:14:45.610 --> 00:14:47.560
And this is a more
complex model, so it has,

00:14:47.560 --> 00:14:49.360
actually, some hyper-parameters.

00:14:49.360 --> 00:14:51.460
And in this case, I
can say, oh, how many

00:14:51.460 --> 00:14:53.710
units or how many layers
do I want in this model?

00:14:53.710 --> 00:14:55.234
And how many units
should they have?

00:14:55.234 --> 00:14:56.900
And otherwise, it
really stays the same.

00:14:56.900 --> 00:14:58.358
The only difference
here-- and it's

00:14:58.358 --> 00:15:02.290
worth pointing out that
dense neural networks really

00:15:02.290 --> 00:15:05.470
don't like getting
integerized features directly

00:15:05.470 --> 00:15:07.510
or large one-hot
features directly.

00:15:07.510 --> 00:15:09.760
So instead, I'm
going to embed this.

00:15:09.760 --> 00:15:13.570
And again, using this
nice declarative language

00:15:13.570 --> 00:15:15.490
for input specification,
I can just

00:15:15.490 --> 00:15:17.890
say, oh, make an
embedding column out

00:15:17.890 --> 00:15:21.142
of this integerized feature,
and then we're done.

00:15:21.142 --> 00:15:22.850
And this embeds it
into eight dimensions,

00:15:22.850 --> 00:15:24.064
so it's very straightforward.

00:15:24.064 --> 00:15:25.480
Now, all I have
to do is make sure

00:15:25.480 --> 00:15:27.160
that the input
functions actually

00:15:27.160 --> 00:15:32.132
produce features of that
name, and I'm good to go.

00:15:32.132 --> 00:15:33.840
So really, the only
question that remains

00:15:33.840 --> 00:15:37.740
is, when can I have it?

00:15:37.740 --> 00:15:41.460
So right now with
1.0, which I think

00:15:41.460 --> 00:15:44.700
should be released as of
right now or something--

00:15:44.700 --> 00:15:50.020
I skipped the last 20
minutes of releasing this.

00:15:50.020 --> 00:15:51.907
You have layers in core.

00:15:51.907 --> 00:15:53.740
And the distinction
between core and contrib

00:15:53.740 --> 00:15:55.990
is really-- in core,
things don't change.

00:15:55.990 --> 00:15:58.150
Things are backwards
compatible until release 2.0,

00:15:58.150 --> 00:16:00.010
and nobody's thinking
about that right now.

00:16:00.010 --> 00:16:03.820
So if you have something
in core, it's stable.

00:16:03.820 --> 00:16:04.960
You should use it.

00:16:04.960 --> 00:16:08.880
If you have something in
contrib, the API may change.

00:16:08.880 --> 00:16:11.529
And depending on your needs, you
may or may not want to use it.

00:16:11.529 --> 00:16:12.445
So layers are in core.

00:16:12.445 --> 00:16:13.720
They're stable.

00:16:13.720 --> 00:16:17.320
Estimators and canned estimators
right now are still in contrib.

00:16:17.320 --> 00:16:19.900
We anticipate that
we move Estimator--

00:16:19.900 --> 00:16:25.149
the base Estimator, the custom
Estimator-- into core at 1.1.

00:16:25.149 --> 00:16:26.940
Canned estimators will
still be in contrib,

00:16:26.940 --> 00:16:29.970
and they will move
to core subsequently.

00:16:29.970 --> 00:16:33.074
So in 1.2, we expect to
have some of them in,

00:16:33.074 --> 00:16:35.490
and then there will be more,
and we'll keep building more.

00:16:35.490 --> 00:16:37.239
And you'll hear about
it in the afternoon,

00:16:37.239 --> 00:16:40.320
how many we have already.

00:16:40.320 --> 00:16:45.630
Traditionally, these
roadmap-- these releases

00:16:45.630 --> 00:16:49.290
have happened six to
eight weeks apart.

00:16:49.290 --> 00:16:51.810
So that's kind of the
timeline you're looking at.

00:16:51.810 --> 00:16:55.830
All right, and with
that, I am done.

00:16:55.830 --> 00:17:01.110
And the next talk will be by
Francois, author of Keras,

00:17:01.110 --> 00:17:02.700
about Keras.

00:17:02.700 --> 00:17:06.349
[MUSIC PLAYING]

