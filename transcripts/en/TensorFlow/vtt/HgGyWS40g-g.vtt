WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.400
[MUSIC PLAYING]

00:00:07.252 --> 00:00:07.960
NOAM SHAZEER: Hi.

00:00:07.960 --> 00:00:09.847
My name is Noam Shazeer.

00:00:09.847 --> 00:00:11.680
I'm going to tell you
about Mesh-TensorFlow,

00:00:11.680 --> 00:00:16.560
which is a system we built for
training giant models on TPU

00:00:16.560 --> 00:00:17.060
pods.

00:00:20.330 --> 00:00:23.150
So we're going to talk
about data-parallelism

00:00:23.150 --> 00:00:27.290
and model-parallelism,
and also about why

00:00:27.290 --> 00:00:29.240
I want to train giant
models and why we need

00:00:29.240 --> 00:00:30.560
model parallelism to do it.

00:00:30.560 --> 00:00:33.350
Then I'll tell you about
the Mesh-TensorFlow system.

00:00:36.110 --> 00:00:39.890
So first data-parallelism--
this is how, roughly, everybody

00:00:39.890 --> 00:00:43.100
trains neural networks if you
have distributed hardware.

00:00:43.100 --> 00:00:46.370
This is what we use
on TPU pods at Google.

00:00:46.370 --> 00:00:51.980
And generally, you put your
entire model on every device,

00:00:51.980 --> 00:00:56.240
split up the training batch
into a lot of little trunks,

00:00:56.240 --> 00:00:57.950
one on each device.

00:00:57.950 --> 00:00:59.030
Run it.

00:00:59.030 --> 00:01:01.640
Then add up all the
gradients on the parameters

00:01:01.640 --> 00:01:05.379
across all the devices
and do your update.

00:01:05.379 --> 00:01:06.600
This works really well.

00:01:06.600 --> 00:01:12.800
The communication is fast
on all kinds of networks,

00:01:12.800 --> 00:01:15.680
on lots of different types
of networks, in fact.

00:01:15.680 --> 00:01:18.390
So this is roughly
what everyone's doing.

00:01:18.390 --> 00:01:21.440
The great things about
it are it's universal.

00:01:21.440 --> 00:01:24.956
You can use any
model architecture.

00:01:24.956 --> 00:01:29.180
It's fast to compile because
you're writing a SPMD code.

00:01:29.180 --> 00:01:31.520
You're writing code for
what one device is doing,

00:01:31.520 --> 00:01:34.250
and then you can
compile it and send it

00:01:34.250 --> 00:01:37.340
to every device which
is using the same code.

00:01:37.340 --> 00:01:39.950
And you get roughly
full utilization

00:01:39.950 --> 00:01:43.380
because every device is
doing roughly the same thing.

00:01:43.380 --> 00:01:46.710
So if they're all
similar, then nobody

00:01:46.710 --> 00:01:48.200
is waiting for anybody else.

00:01:48.200 --> 00:01:51.850
And the communication is fast.

00:01:51.850 --> 00:01:55.180
The only problem is that you
cannot train giant models.

00:01:55.180 --> 00:02:00.650
Because your entire model
has to fit on every device.

00:02:00.650 --> 00:02:02.970
So why do I want to
train giant models?

00:02:02.970 --> 00:02:06.170
Well, I like working
on language modeling.

00:02:06.170 --> 00:02:08.509
There are lots of important
applications, machine

00:02:08.509 --> 00:02:13.230
translation, question answering,
dialogue, sentimental analysis,

00:02:13.230 --> 00:02:15.470
lots of interesting things
to do with language.

00:02:15.470 --> 00:02:20.160
And we find that quality
improves with model size.

00:02:20.160 --> 00:02:24.020
So a bigger model tends to
know more about the world,

00:02:24.020 --> 00:02:25.850
understand things
better, and give you

00:02:25.850 --> 00:02:27.680
overall better results.

00:02:27.680 --> 00:02:30.340
There's plenty of data out
there to train a giant model.

00:02:30.340 --> 00:02:33.470
Just download the text of the
web, common crawl, whatever,

00:02:33.470 --> 00:02:37.040
and you've got
billions to trillions

00:02:37.040 --> 00:02:38.520
of words of training data.

00:02:38.520 --> 00:02:40.820
And in fact, you can
train one big model

00:02:40.820 --> 00:02:44.730
and then fine tune it to do
lots of different things.

00:02:44.730 --> 00:02:46.130
There's been a lot of research--

00:02:46.130 --> 00:02:52.050
OpenAI, and BERT at Google on
transfer learning and language.

00:02:52.050 --> 00:02:54.950
So it's a great candidate
for building giant models.

00:02:54.950 --> 00:02:59.600
Now, as an example, I trained
a transformer language model

00:02:59.600 --> 00:03:01.670
with roughly 100
million parameters

00:03:01.670 --> 00:03:04.220
on the text of Wikipedia.

00:03:04.220 --> 00:03:07.070
The Abraham Lincoln
article was in the dev set,

00:03:07.070 --> 00:03:10.790
in the held out set, and I told
it to generate a random Abraham

00:03:10.790 --> 00:03:11.870
Lincoln article.

00:03:11.870 --> 00:03:14.390
And it looks
roughly grammatical.

00:03:14.390 --> 00:03:17.540
It remembers somehow
that he's a politician,

00:03:17.540 --> 00:03:18.522
American politician.

00:03:18.522 --> 00:03:20.480
There's plenty it doesn't
know about the world,

00:03:20.480 --> 00:03:23.480
like who Abraham Lincoln
was, or that America

00:03:23.480 --> 00:03:27.470
doesn't have a prime minister,
and lots of other stuff.

00:03:27.470 --> 00:03:29.630
But if you make
the similar model,

00:03:29.630 --> 00:03:32.600
just bigger, here's with five
billion parameters instead

00:03:32.600 --> 00:03:33.440
of 100 million.

00:03:33.440 --> 00:03:36.410
And now it seems to have picked
up a lot more about Abraham

00:03:36.410 --> 00:03:37.400
Lincoln.

00:03:37.400 --> 00:03:39.527
Roughly half of that
stuff is correct.

00:03:39.527 --> 00:03:40.110
But you know--

00:03:40.110 --> 00:03:41.295
[LAUGHTER]

00:03:41.295 --> 00:03:42.650
--mostly it's fake news.

00:03:42.650 --> 00:03:45.840
But there are more important
applications out there

00:03:45.840 --> 00:03:48.728
than generating fake news.

00:03:48.728 --> 00:03:50.270
But this is just a
nice demonstration

00:03:50.270 --> 00:03:53.910
that model size is important.

00:03:53.910 --> 00:03:57.770
What would a model look like
with a trillion parameters?

00:03:57.770 --> 00:03:59.120
We have not done that yet.

00:03:59.120 --> 00:04:03.190
But we hope to do that soon.

00:04:03.190 --> 00:04:03.740
OK.

00:04:03.740 --> 00:04:06.500
So if all the parameters
will not fit on one core,

00:04:06.500 --> 00:04:09.110
we need to do something called
model-parallelism, which

00:04:09.110 --> 00:04:11.300
means that we're
splitting the model itself

00:04:11.300 --> 00:04:14.010
between different devices.

00:04:14.010 --> 00:04:17.100
And that should let us
train really large models,

00:04:17.100 --> 00:04:19.950
and it should also be very
good for inference latency,

00:04:19.950 --> 00:04:22.770
because now the
computation for one example

00:04:22.770 --> 00:04:26.320
can be split across
multiple devices.

00:04:26.320 --> 00:04:28.780
The problem is it's
very tricky to design

00:04:28.780 --> 00:04:31.420
these kinds of algorithms.

00:04:31.420 --> 00:04:33.330
How do people tend to do it now?

00:04:33.330 --> 00:04:34.922
Well, you use device placement.

00:04:34.922 --> 00:04:36.880
You say this operation
is going on this device.

00:04:36.880 --> 00:04:38.950
This operation is
going on that device.

00:04:38.950 --> 00:04:41.050
TensorFlow makes
it easy to do that.

00:04:41.050 --> 00:04:44.120
Still, it's tricky to design
an efficient algorithm.

00:04:44.120 --> 00:04:46.090
And you end up
with a giant graph,

00:04:46.090 --> 00:04:48.580
if you're generating
enough operations

00:04:48.580 --> 00:04:52.060
to go on 2,000 different cores.

00:04:52.060 --> 00:04:56.490
Here's an example of
some model-parallelism

00:04:56.490 --> 00:04:59.430
by device placement from
Google Neural Multimachine

00:04:59.430 --> 00:05:00.600
Translation.

00:05:00.600 --> 00:05:04.260
They had eight LSTMs,
which they distributed

00:05:04.260 --> 00:05:06.090
across eight different GPUs.

00:05:06.090 --> 00:05:08.820
The softmax layer they
put somewhere else.

00:05:08.820 --> 00:05:12.330
And you need some kind
of interesting pipelining

00:05:12.330 --> 00:05:15.330
to keep all the GPUs busy.

00:05:15.330 --> 00:05:20.150
It works, but a lot of work to
get this thing to work right.

00:05:20.150 --> 00:05:22.920
Now we're going to take a
totally different approach,

00:05:22.920 --> 00:05:26.270
which is going to be
inspired by what works well

00:05:26.270 --> 00:05:28.800
about synchronous
data-parallelism.

00:05:28.800 --> 00:05:34.440
So we will have every processor
involved in every operation.

00:05:34.440 --> 00:05:37.440
We're going to use SPMD
style programming, where

00:05:37.440 --> 00:05:40.480
you'll have the same
program on every device.

00:05:40.480 --> 00:05:44.250
And it's going to use collective
communication, like allreduce,

00:05:44.250 --> 00:05:48.590
just like data-parallelism.

00:05:48.590 --> 00:05:52.310
And our library for doing this
is called Mesh-TensorFlow.

00:05:52.310 --> 00:05:54.620
We should be able to
implement data-parallelism,

00:05:54.620 --> 00:05:56.120
model-parallelism.

00:05:56.120 --> 00:05:58.040
We should be able to split--

00:05:58.040 --> 00:06:01.940
in different dimensions,
like split an image or video

00:06:01.940 --> 00:06:05.920
spatially or any sorts of
combinations of these things.

00:06:05.920 --> 00:06:08.800
And we're targeting
hardware where

00:06:08.800 --> 00:06:11.800
you have a homogeneous
set of similar processors,

00:06:11.800 --> 00:06:15.610
ideally well-connected
like a TPU pod.

00:06:15.610 --> 00:06:19.270
We've got these two dimensional
supercomputers at Google

00:06:19.270 --> 00:06:21.220
that we've been using.

00:06:21.220 --> 00:06:23.650
And you're going to view
your set of processors

00:06:23.650 --> 00:06:25.555
as an n-dimensional mesh.

00:06:25.555 --> 00:06:28.180
It doesn't have to correspond to
a physical n-dimensional mesh.

00:06:28.180 --> 00:06:31.090
You could view a two
dimensional mesh of processors

00:06:31.090 --> 00:06:32.950
as a one dimensional mesh.

00:06:32.950 --> 00:06:37.750
But, of course, performance will
depend on those considerations.

00:06:37.750 --> 00:06:40.290
So how does this all work?

00:06:40.290 --> 00:06:43.980
Well, in data-parallelism,
you can view it

00:06:43.980 --> 00:06:48.690
as splitting the batch dimension
of the computation across all

00:06:48.690 --> 00:06:49.740
your processors.

00:06:49.740 --> 00:06:52.530
So any tensor that
has a batch dimension

00:06:52.530 --> 00:06:55.290
is going to be split across
all of the processors.

00:06:55.290 --> 00:06:59.700
And any tensor that does not
have a batch dimension, meaning

00:06:59.700 --> 00:07:03.688
the parameters, gets
fully replicated.

00:07:03.688 --> 00:07:05.230
Now we're going to
do the same thing,

00:07:05.230 --> 00:07:07.260
but for model-parallelism,
we will choose

00:07:07.260 --> 00:07:09.730
different dimensions to split.

00:07:09.730 --> 00:07:12.930
So maybe dimensions representing
the sizes of hidden layers--

00:07:12.930 --> 00:07:15.270
and we will decide to
split those dimensions

00:07:15.270 --> 00:07:20.830
across the set of processors.

00:07:20.830 --> 00:07:23.670
And the communication
will happen--

00:07:23.670 --> 00:07:26.710
usually, an operation will
not involve communication.

00:07:26.710 --> 00:07:30.040
But some operations will involve
collective communication,

00:07:30.040 --> 00:07:32.200
like all reduce--
particularly, when you're

00:07:32.200 --> 00:07:34.190
reducing out split dimensions.

00:07:34.190 --> 00:07:37.090
And this is going to be
somewhat similar to how

00:07:37.090 --> 00:07:40.910
things work in synchronous
data-parallelism.

00:07:40.910 --> 00:07:45.050
So let's do an example, a simple
three layer neural network.

00:07:45.050 --> 00:07:47.450
Input layer X, hidden
layer H, output layer

00:07:47.450 --> 00:07:49.580
Y, and we have two
weight matrices,

00:07:49.580 --> 00:07:54.080
W and V. The
data-parallel way to do

00:07:54.080 --> 00:07:59.450
this is that we are going to
split anything with a batch

00:07:59.450 --> 00:08:03.620
dimension, meaning the
activations X, H, and Y.

00:08:03.620 --> 00:08:08.570
So all of those tensors are
split evenly across processors.

00:08:08.570 --> 00:08:13.660
And each tensor that does
not have a batch dimension, W

00:08:13.660 --> 00:08:17.140
and V, will be replicated
across every processor.

00:08:17.140 --> 00:08:19.240
So here it's showing what
Processor 0 is doing,

00:08:19.240 --> 00:08:21.070
what Processor 1 is doing.

00:08:21.070 --> 00:08:23.020
They're both doing
something roughly similar,

00:08:23.020 --> 00:08:25.930
except they have different
halves of the activation.

00:08:25.930 --> 00:08:28.550
You don't see any communication
in the forwards pass.

00:08:28.550 --> 00:08:30.910
But if you were to see the
backwards pass where you're

00:08:30.910 --> 00:08:33.190
computing the
parameter gradients,

00:08:33.190 --> 00:08:39.250
you would see some matmuls
where the split dimension b gets

00:08:39.250 --> 00:08:42.110
reduced out, and there would
be some all reduces in there.

00:08:42.110 --> 00:08:45.130
So we're going to now,
instead of splitting V,

00:08:45.130 --> 00:08:50.410
let's split the size of the
hidden layer, dimension H.

00:08:50.410 --> 00:08:51.610
So we do that.

00:08:51.610 --> 00:08:56.400
And now X and Y, the input and
output, are fully replicated.

00:08:56.400 --> 00:08:58.620
Because they do not
have an H dimension.

00:08:58.620 --> 00:09:01.590
But the hidden layer
H is split because it

00:09:01.590 --> 00:09:03.120
does have an H dimension.

00:09:03.120 --> 00:09:06.360
And the parameter
matrices, W and V,

00:09:06.360 --> 00:09:07.590
also have an H dimension.

00:09:07.590 --> 00:09:09.160
So they're split.

00:09:09.160 --> 00:09:13.260
So again, you have a
parallel-computation on the two

00:09:13.260 --> 00:09:16.230
processors, and you see an
all reduced communication

00:09:16.230 --> 00:09:19.380
when you're computing Y
because we're reducing out

00:09:19.380 --> 00:09:21.420
the split dimension H.

00:09:21.420 --> 00:09:23.820
We didn't have to
split H. Instead,

00:09:23.820 --> 00:09:29.370
we could have split V, which
is the dimension of X and Y.

00:09:29.370 --> 00:09:33.630
So in that case, you would have
a different pattern of which

00:09:33.630 --> 00:09:36.000
tensors are split and
which ones are replicated.

00:09:36.000 --> 00:09:39.010
And you'd have communication
in different places.

00:09:39.010 --> 00:09:42.390
And if you want to
get really fancy,

00:09:42.390 --> 00:09:46.110
let's do data-parallelism and
model-parallelism at once.

00:09:46.110 --> 00:09:49.140
We're going to
split dimension b,

00:09:49.140 --> 00:09:53.520
the batch across one axis of our
two dimensional supercomputer.

00:09:53.520 --> 00:09:55.230
And we're going to
split the hidden layer

00:09:55.230 --> 00:09:59.970
H across the other axis
of our mesh of processors.

00:09:59.970 --> 00:10:05.340
So now, we have different
sensors being either split

00:10:05.340 --> 00:10:08.730
in one dimension and
replicated in the other,

00:10:08.730 --> 00:10:12.480
or since tensor H has
both of those dimensions,

00:10:12.480 --> 00:10:15.120
it ends up piled among
all the processors.

00:10:15.120 --> 00:10:16.920
And there are going
to be all reduced

00:10:16.920 --> 00:10:21.900
communications in there, but
not across all the processors.

00:10:21.900 --> 00:10:24.300
They'll be partitioned
all reduces just

00:10:24.300 --> 00:10:27.340
across rows or just
across columns.

00:10:27.340 --> 00:10:32.950
So the general case is give all
of the tensor dimensions names.

00:10:32.950 --> 00:10:36.820
And we define the layout
of the communication

00:10:36.820 --> 00:10:40.510
as a map from tensor
dimensions to mesh dimensions,

00:10:40.510 --> 00:10:42.520
saying which tensor
dimensions get split

00:10:42.520 --> 00:10:44.530
across which mesh dimensions.

00:10:44.530 --> 00:10:46.720
For example, in
the previous slide

00:10:46.720 --> 00:10:49.480
we had the batch
tensor dimensions

00:10:49.480 --> 00:10:53.220
split across processor rows,
and the hidden size dimensions

00:10:53.220 --> 00:10:55.800
split across processor columns.

00:10:55.800 --> 00:10:58.470
We did this to our transformer,
machine translation

00:10:58.470 --> 00:11:00.780
slash language
model, and here are

00:11:00.780 --> 00:11:03.060
the layouts we used
for data-parallelism,

00:11:03.060 --> 00:11:07.170
model-parallelism and
combined parallelism for that.

00:11:07.170 --> 00:11:09.840
For the model-parallelism,
it works

00:11:09.840 --> 00:11:12.240
to split the size
of the vocabulary,

00:11:12.240 --> 00:11:14.190
the size of the feed
forward hidden layer,

00:11:14.190 --> 00:11:16.270
and the number of
attention heads.

00:11:16.270 --> 00:11:18.270
And if you do that,
you end up splitting up

00:11:18.270 --> 00:11:20.580
all of your
communication very nicely

00:11:20.580 --> 00:11:22.950
and get a nice model
parallel algorithm.

00:11:22.950 --> 00:11:25.800
And that can also be combined
with data-parallelism

00:11:25.800 --> 00:11:28.200
by splitting the batch
across the other dimension

00:11:28.200 --> 00:11:30.330
of the supercomputer.

00:11:30.330 --> 00:11:33.300
Now, picking a good layout
is, for now, something

00:11:33.300 --> 00:11:37.680
that you need a
well-trained human to do.

00:11:37.680 --> 00:11:40.620
You need to make sure that all
of your expensive operations

00:11:40.620 --> 00:11:41.400
are split.

00:11:41.400 --> 00:11:45.780
You're not allowed to split two
dimensions of the same tensor

00:11:45.780 --> 00:11:49.470
across different
dimensions of the mesh.

00:11:49.470 --> 00:11:53.160
And depending on what
dimensions you chop up,

00:11:53.160 --> 00:11:56.110
it may result in more
or less communication.

00:11:56.110 --> 00:12:00.570
So example, data-parallelism--
you'd like the batch to be big.

00:12:00.570 --> 00:12:02.820
Otherwise, you're going to
get a lot of communication.

00:12:02.820 --> 00:12:05.190
Similarly, if you're
splitting up a hidden layer,

00:12:05.190 --> 00:12:08.710
you might want that
layer to be really big.

00:12:08.710 --> 00:12:10.590
So how do you use
Mesh-TensorFlow?

00:12:10.590 --> 00:12:13.620
Well, download our
open source repository.

00:12:13.620 --> 00:12:17.370
You build a graph in Python,
much like a regular TensorFlow

00:12:17.370 --> 00:12:22.800
graph except that you're
using named dimensions.

00:12:22.800 --> 00:12:24.870
You define what your
mesh is and how it maps

00:12:24.870 --> 00:12:27.000
to your physical processors.

00:12:27.000 --> 00:12:30.120
You define your layout of
what gets split across what.

00:12:30.120 --> 00:12:33.210
And then Mesh-TensorFlow turns
your mesh TensorFlow graph

00:12:33.210 --> 00:12:35.520
into part of a TensorFlow graph.

00:12:35.520 --> 00:12:37.857
And you still use
TensorFlow for anything else

00:12:37.857 --> 00:12:40.440
you want to use it for, like the
data pipelines and everything

00:12:40.440 --> 00:12:41.940
else.

00:12:41.940 --> 00:12:45.600
So far we've trained
transformer models on up to five

00:12:45.600 --> 00:12:48.090
billion parameters
on entire TPU pods,

00:12:48.090 --> 00:12:52.380
getting a good performance
out of the thing.

00:12:52.380 --> 00:12:55.200
And these giant
models give state

00:12:55.200 --> 00:13:00.300
of the art quality on
some benchmark tasks,

00:13:00.300 --> 00:13:03.090
like Latin language modeling
and machine translation,

00:13:03.090 --> 00:13:04.740
not surprisingly.

00:13:04.740 --> 00:13:05.910
Bigger models are better.

00:13:05.910 --> 00:13:08.650
Lots of people are
finding that out.

00:13:08.650 --> 00:13:12.480
And in the future, we would
like to try even bigger models,

00:13:12.480 --> 00:13:16.110
I think, with some
well-placed sparsity.

00:13:16.110 --> 00:13:18.480
We would have the
computation to train models

00:13:18.480 --> 00:13:19.820
with a trillion parameters.

00:13:19.820 --> 00:13:22.750
We've tried up to a couple
hundred billion for now.

00:13:22.750 --> 00:13:23.800
And it runs.

00:13:23.800 --> 00:13:26.940
So next thing is to see if we
can get a trillion parameter

00:13:26.940 --> 00:13:29.820
model to run and give
us great quality.

00:13:29.820 --> 00:13:32.700
And this would be
useful for other things,

00:13:32.700 --> 00:13:35.430
like low-latency
inference and situations

00:13:35.430 --> 00:13:41.510
where you have giant inputs
that you want to process.

00:13:41.510 --> 00:13:42.920
And for now, what works?

00:13:42.920 --> 00:13:49.370
Well, we're emitting SPMD code
for TPU, including Cloud TPU.

00:13:49.370 --> 00:13:52.460
So this runs nicely
on TPU's pods.

00:13:52.460 --> 00:13:57.290
And for CPU and GPU, it's still
emitting the old fashioned

00:13:57.290 --> 00:13:58.460
device placement code.

00:13:58.460 --> 00:14:02.210
So it runs, but not as scalable.

00:14:02.210 --> 00:14:05.210
Everything is out
there on GitHub

00:14:05.210 --> 00:14:11.780
and runs with TensorFlow 1--
not yet with TensorFlow 2.

00:14:11.780 --> 00:14:15.830
And then, in the future,
we want to use this

00:14:15.830 --> 00:14:17.750
for different types of models.

00:14:17.750 --> 00:14:20.510
It would be great to automate
the process of choosing

00:14:20.510 --> 00:14:22.540
a distributed layout.

00:14:22.540 --> 00:14:25.040
Because then you wouldn't need
to know about much TensorFlow

00:14:25.040 --> 00:14:27.770
and it would just figure out how
to distribute your computation

00:14:27.770 --> 00:14:29.460
for you.

00:14:29.460 --> 00:14:33.530
And we welcome contributions
to the open source code

00:14:33.530 --> 00:14:37.020
or contact us.

00:14:37.020 --> 00:14:42.700
And I'd like to thank
all of my collaborators,

00:14:42.700 --> 00:14:46.180
the authors of our paper.

00:14:46.180 --> 00:14:49.590
I'd also like to thank the
TensorFlow teams and XLA

00:14:49.590 --> 00:14:52.547
team for a lot of
technical support

00:14:52.547 --> 00:14:54.380
and help with all of
this, implementing what

00:14:54.380 --> 00:14:56.270
we needed to be implemented.

00:14:56.270 --> 00:15:01.250
And everything's out there in
our open source repository.

00:15:01.250 --> 00:15:02.120
Thank you.

00:15:02.120 --> 00:15:03.920
[APPLAUSE]

00:15:03.920 --> 00:15:06.670
[MUSIC PLAYING]

