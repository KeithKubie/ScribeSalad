Speaker 1:          00:00          Okay.

Speaker 2:          00:01          In this video, we'll look at a little bit more the details of why it's difficult to train deep neural network.

Speaker 1:          00:07          Okay.

Speaker 2:          00:10          So I mentioned in the previous video that historically we've often found that training a single hidden their neural network without say a second hidden, they're like in this case, uh, would often work better than having a two or more hidden layers. And, uh, even though there are some problems where we'd actually think that we would gain by having many hidden layers. So, uh, well I'll talk about in this video is two types of reasons where we tend to see this difficulty, which will help us then introduce ways of, of, uh, addressing these difficulties and, uh, uh, solve our problems and get effective learning algorithms or deep neural networks.

Speaker 1:          00:52          Yeah,

Speaker 2:          00:53          yeah. Actually, two types of problems was, are, uh, or difficulty difficulties, which are quite different. Uh, so the first is that a training, a deep neural network is a harder optimization problem than training a single hidden, they're known network, um, in particular. So in this case, we would be in an under fitting a situation where if we had a better optimization procedure, then, uh, we should be able to, uh, if, if indeed we are having a harder time to optimize on neural network, then it means we're probably could fit the data better. So it would be then in under fitting a situation, which would mean that, uh, we would get better results in digitalization by, uh, optimizing better fitting better our training data. So this, uh, difficulty optimization is often a, it is trading by, uh, the vanishing gradient problem, which is a well known problem in a neural networks, uh, essentially corresponds to the following, uh, following situation.

Speaker 2:          01:59          So, uh, we, uh, if we go back to the backpropagation algorithm, we know that whenever we are taking the gradient of the activation of a unit and then we passing that gradient to get degree in, in of the pre activation, we have to multiply by the partial derivative of the activation with respect to their pre activation. Now if the hidden unit is at, it's a core close to a saturation point, that is, it's uh, it has a, it's a activation close to the highest or lowest value it can take. It means that the partial dude IV is going to be very close to zero, which means that the gradient with respect to the pre activation is going to be close to zero because we're multiplying by that partial derivative. And so it means that at this point, not all the great is being back propagated towards the other layers.

Speaker 2:          02:51          Below is going to be very close to zero. And if we repeat this process many times, and we have many saturated hidden units in means, uh, as we get closer, closer to the input layer, we get great ins that are closer and closer to zero. So that's why we say that the gradient is Spanish ing. And this is essentially caused by the, uh, saturated units because saturated in units as essentially a partial do they have a of zero. And so that essentially shrinks the gradient, uh, towards zero. And so that's a big problem because intuitively the advantage of having hidden units that are non linear and saturate is that it makes the prediction non linear and we want that nonlinearity. So we actually want the neural network to get to a regime which is close to the saturation point to take advantage of, uh, the nonlinear already of the hidden units.

Speaker 2:          03:45          But as we get there, we get great ins that become a closer and closer to zero and yet towards the vanishing gradient situation. Um, and so there's sort of this dilemma here, which is very unfortunate, which is that when we get to a situation where we're signing to be in the interesting configuration of the neural network, which is non linear, while it becomes actually harder and harder to optimize because the green it is, becomes very small for the first hidden layers, which might not train, uh, at all. Uh, with respect to how quickly the top hit in layers with train. And which means that these hidden units here are, these units here might start to adapt to hidden units that are more or less, uh, a random because they're, they're not actually training as quickly as the connections that are here. For instance, this is in fact a well known problem in record neural networks.

Speaker 2:          04:38          The fact that there's this dilemma between, uh, getting gradient that pass as well through the hidden layers and being in a situation where the neural network is in a configuration that allows for saturation and there's more interesting, uh, I'm not going to talk about this too much, but there's quite a bit of a few papers about that. So that's the first hypothesis that optimize optimization is harder and this is made harder in particular by the fact that hidden units, saturate and block gradients when we back propagate them towards the, uh, all the lower hidden layers is second possibility is that we are actually not in or fitting situation. But in overfitting situation that is when we're considering the possibility of having many hidden layers. It means that we're exploring a more complex space of functions. So functions are more and more complicated and there are many, many more of them that are functions as we add layers and a possible functions we can, we can model.

Speaker 2:          05:37          And also just one way of, of sort of informally measuring the complexity of a space of functions is to consider the number of parameters if these functions are parametrized. So in the deep neural net, the parameters are all the connections. And of course when we add more layers we have more parameters. So, um, you know, we can expect that we'd get some overfitting problems where we'd be in this, uh, hi Varun, slow bias situation. Where are the family of deep networks corresponds to a very large space of possible functions and forgiven training set. Uh, there'd be many function that would reach, say zero error or close to zero error and uh, and, and also if we varied the training set because our space of functions is very, very complex, it can adapt very easily. Then we actually get very many different, uh, neural networks as we say, varied the training center. So this would correspond to a high variance. We have low bias because neural nets have a lot of capacity, deep neural nets, perhaps even more capacity. So they, for any training set, it's actually able to reach a very low training error, uh, because of that high capacity. And so we might be in a situation where the variance is too high. Uh, and, uh, even though we have low bias and thus we were not great at generalization and thus were overfitting.

Speaker 2:          07:01          So these two problems are actually quite different, but depending on the situation, the intuition is that there's one that dominates over the other. Um, so if we are under fitting and there are ways we can try to measure this, if the training error is very close to the validation set there, then perhaps we'll under fitting then one solution is to use better optimization methods than just to cast a great in dissent. And this isn't current active area of research as in the right now in 2013. Uh, and I'm not going to talk too much about this because I don't want to talk too much about optimization in this class, but there are references on the website of the, uh, of discourse. And we've got to talk more about the second hypothesis, uh, over fitting, which in my experience tends to, uh, be more prevalent in problems where say we're doing, we're doing classification, for instance, which is the main topic of this class. And now the approach is essentially going to be to use better reclamation methods, uh, for deep neural networks. And we'll look at two types of virtualization approaches. One, which is based on the corporation of unsupervised learning and the training procedure. And another one, which is based on the use of so-called dropout, uh, training, which is also can be, uh, which we can understand as a way of improving regularization in deep neural networks.