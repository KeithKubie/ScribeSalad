WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.000
In linear regression, we are given data and data has still has more than one dimension.

00:00:06.000 --> 00:00:09.000
We have just studied for two dimensions throughout this class.

00:00:09.000 --> 00:00:13.000
So this might be the data and we're looking for the best line that fits the data.

00:00:13.000 --> 00:00:18.000
We'll put differently the parameters a and b that we discussed before that defines the line.

00:00:18.000 --> 00:00:20.000
The word best is interesting.

00:00:20.000 --> 00:00:24.000
Obviously, it's impossible to put a line through the data points over here.

00:00:24.000 --> 00:00:27.000
These are what's called nonlinear data that go up and down.

00:00:27.000 --> 00:00:32.000
Data often looks like this even if the relationship between x and y is linear--

00:00:32.000 --> 00:00:37.000
that's usually what's called noise in the data, some amount of randomness you can't explain.

00:00:37.000 --> 00:00:45.000
So in finding the best fit, we're trying to find a line that minimizes the difference between the data

00:00:45.000 --> 00:00:49.000
and the line in the y direction and that's somewhat counter intuitive.

00:00:49.000 --> 00:00:53.000
You'd normally think the distance is given by these lines over here in red

00:00:53.000 --> 00:00:56.000
and that is the shortest distance irrespective of x and y.

00:00:56.000 --> 00:01:03.000
But in linear regression, it turns out that we're minimizing the distance just in the y direction.

00:01:03.000 --> 00:01:06.000
And there's a lot of theory why this is a good idea.

00:01:06.000 --> 00:01:12.000
In summary, we assume that our data is the result of this blank some unknown linear function,

00:01:12.000 --> 00:01:19.000
bx+a+noise and if this noise is assumed to be Gaussian, they're minimizing the quadratic deviation

00:01:19.000 --> 00:01:23.000
between the data points and the line happens to be the correct mathematical answer.

00:01:23.000 --> 00:01:28.000
But leaving this aside, what we're doing is we are adding over all data points

00:01:28.000 --> 00:01:34.000
the difference between our function and the y value of the data point with the square

00:01:34.000 --> 00:01:37.000
and that distance is the distance we're minimizing.

00:01:37.000 --> 00:01:40.000
So let's look into this with examples--for the following four data points,

00:01:40.000 --> 00:01:43.000
which of these lines has the smallest quadratic error

00:01:43.000 --> 99:59:59.000
and would be the best regression line--pick one of the three.

