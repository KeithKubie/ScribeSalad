WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.380
RYAN HICKMAN: Hello.

00:00:04.380 --> 00:00:05.900
Everyone excited they
got notebooks?

00:00:09.801 --> 00:00:11.655
I wish we were giving
out free robots.

00:00:17.510 --> 00:00:18.650
Well, people are still
coming in.

00:00:18.650 --> 00:00:21.270
But let me see by a show
of hands, who here

00:00:21.270 --> 00:00:24.620
really loves robots?

00:00:24.620 --> 00:00:25.340
Good, good.

00:00:25.340 --> 00:00:27.370
We've got everybody.

00:00:27.370 --> 00:00:31.610
And my friend PR2.

00:00:31.610 --> 00:00:33.190
So you came to the right talk.

00:00:33.190 --> 00:00:34.200
This is Cloud Robotics.

00:00:34.200 --> 00:00:36.440
This is a tech talk.

00:00:36.440 --> 00:00:38.930
And I am Ryan Hickman
from Google.

00:00:38.930 --> 00:00:41.520
I also have with me
Damon Kohler.

00:00:41.520 --> 00:00:44.030
We're both on the Cloud Robotics
team at Google, which

00:00:44.030 --> 00:00:46.390
I'm sure you've never heard
of before today.

00:00:46.390 --> 00:00:53.230
And we have Brian Gerkey and Ken
Conley from Willow Garage.

00:00:53.230 --> 00:00:55.090
So today, I'm going to give you
an introduction to what

00:00:55.090 --> 00:00:56.235
Cloud Robotics is.

00:00:56.235 --> 00:00:59.440
It's probably something you've
never heard of or thought

00:00:59.440 --> 00:01:01.740
about in the past. So I'm going
to tell you what it is

00:01:01.740 --> 00:01:03.580
by concept.

00:01:03.580 --> 00:01:05.890
And then Ken and Brian are going
to give you an overview

00:01:05.890 --> 00:01:09.580
of ROS, an open source platform
for robots, and a

00:01:09.580 --> 00:01:13.550
demo of the PR2 doing
some matrix.

00:01:13.550 --> 00:01:15.640
Then, Damon is going to talk
about work we've done with

00:01:15.640 --> 00:01:19.060
Willow Garage to port
ROS to Android.

00:01:19.060 --> 00:01:22.182
So Android apps can run ROS and
talk to the amazing PR2.

00:01:26.280 --> 00:01:28.720
And then I'm going to close out
with a demo of a prototype

00:01:28.720 --> 00:01:31.530
object recognition service that
we created, and then give

00:01:31.530 --> 00:01:33.920
all of you some action items
so that you can all become

00:01:33.920 --> 00:01:35.980
roboticists.

00:01:35.980 --> 00:01:37.960
Then we'll stick around for
Q&amp;A and we've also got the

00:01:37.960 --> 00:01:39.140
feedback link up there.

00:01:39.140 --> 00:01:41.300
People have been using that
and the hash tags.

00:01:44.060 --> 00:01:47.160
So what is a cloud-connected
robot?

00:01:47.160 --> 00:01:50.990
If you look at this PR2 over
here, it's an amazing machine.

00:01:50.990 --> 00:01:52.890
But it still has a
limited amount of

00:01:52.890 --> 00:01:54.620
memory and storage space.

00:01:54.620 --> 00:01:57.840
It cannot know everything.

00:01:57.840 --> 00:02:01.010
It can't process all the sensors
that it has in real

00:02:01.010 --> 00:02:03.656
time in all the ways
that we want it to.

00:02:03.656 --> 00:02:06.710
It's just limited, even
as great as it is.

00:02:06.710 --> 00:02:09.940
But if you tap into the cloud,
we can move some of that

00:02:09.940 --> 00:02:12.820
perception of the world
around the robot--

00:02:12.820 --> 00:02:15.210
the understanding of
what the task it's

00:02:15.210 --> 00:02:17.260
been given to do is--

00:02:17.260 --> 00:02:19.470
the robot can share information
with other humans

00:02:19.470 --> 00:02:21.330
and with other robots.

00:02:21.330 --> 00:02:24.410
And it can react in a
smarter way by using

00:02:24.410 --> 00:02:25.660
brand new cloud services.

00:02:28.840 --> 00:02:32.800
So how are we going to connect
hardware with the cloud?

00:02:32.800 --> 00:02:36.070
Well, if yesterday you saw the
open accessory API was

00:02:36.070 --> 00:02:38.070
announced at the keynote, and
some of you might have

00:02:38.070 --> 00:02:39.350
gone to that talk.

00:02:39.350 --> 00:02:43.100
Hopefully, some of you got
the ADK development kit.

00:02:43.100 --> 00:02:44.620
I see some thumbs up.

00:02:44.620 --> 00:02:48.420
So you can now take the sensors
of an Android device--

00:02:48.420 --> 00:02:51.370
the touchscreen, the microphone,
the speaker, the

00:02:51.370 --> 00:02:54.270
gyroscopes, the memory,
the processors--

00:02:54.270 --> 00:02:56.930
and you can use that
for a robot.

00:02:56.930 --> 00:03:03.920
You just need to jack in motors,
actuators, lights, and

00:03:03.920 --> 00:03:04.546
give it mobility.

00:03:04.546 --> 00:03:08.490
So you can have an Android app
that actually physically

00:03:08.490 --> 00:03:09.750
interacts with the world.

00:03:09.750 --> 00:03:12.280
And we think robots are
my favorite use

00:03:12.280 --> 00:03:15.615
case of that new API.

00:03:15.615 --> 00:03:18.200
So Google also has some cloud
services which are really

00:03:18.200 --> 00:03:19.830
useful for robotics.

00:03:19.830 --> 00:03:22.660
If you've ever used Google
Goggles on your phone, you

00:03:22.660 --> 00:03:24.690
know that you take a picture
of something, and then the

00:03:24.690 --> 00:03:27.130
phone takes that picture,
sends it up to a cloud

00:03:27.130 --> 00:03:30.155
service, and it's compared
against the massive database

00:03:30.155 --> 00:03:31.420
of potential matches.

00:03:31.420 --> 00:03:35.350
It's compared to more things
than you could ever store on

00:03:35.350 --> 00:03:37.120
your robot.

00:03:37.120 --> 00:03:39.530
And then it comes back, and in
seconds, it tells your phone,

00:03:39.530 --> 00:03:41.450
it tells the app, what was that
thing you just took a

00:03:41.450 --> 00:03:42.280
picture of.

00:03:42.280 --> 00:03:44.510
That's really powerful
for robots.

00:03:44.510 --> 00:03:46.430
If robots are going to roam
around in the world and

00:03:46.430 --> 00:03:49.520
encounter things that they did
not expect to run into, they

00:03:49.520 --> 00:03:52.355
need to look at them, send that
data up into the cloud,

00:03:52.355 --> 00:03:54.600
and say, robot, this is what
you just ran into.

00:03:54.600 --> 00:03:56.260
This how you should
interact with it.

00:03:56.260 --> 00:03:59.750
That would be a great
cloud service.

00:03:59.750 --> 00:04:02.310
We also have mapping and
navigation services.

00:04:02.310 --> 00:04:04.300
If you've ever used turn-by-turn
directions on

00:04:04.300 --> 00:04:07.180
your mobile phone, you know
how powerful it is to open

00:04:07.180 --> 00:04:10.470
your phone and have it know
exactly where you are, where

00:04:10.470 --> 00:04:12.730
you're going, and how
to get there.

00:04:12.730 --> 00:04:15.090
Well, that would also be
terrific for robots.

00:04:15.090 --> 00:04:17.510
Imagine if they knew where they
were and how to get where

00:04:17.510 --> 00:04:18.560
they need to go.

00:04:18.560 --> 00:04:21.040
They could also know where other
robots were and where

00:04:21.040 --> 00:04:23.060
all the humans were.

00:04:23.060 --> 00:04:26.710
That would be a great service.

00:04:26.710 --> 00:04:28.630
And then, Google has
some competencies

00:04:28.630 --> 00:04:32.010
with voice and text.

00:04:32.010 --> 00:04:35.900
We have voice recognition,
optical character recognition,

00:04:35.900 --> 00:04:39.520
then language translation,
and then text-to-speech.

00:04:39.520 --> 00:04:42.140
So if you put all of that
together, that's going to

00:04:42.140 --> 00:04:45.550
allow me to say, robot, fetch
me a beer, instead of doing

00:04:45.550 --> 00:04:48.050
what Brian is doing today,
with lots of

00:04:48.050 --> 00:04:49.300
clicking and typing.

00:04:52.840 --> 00:04:53.950
So in essence, the
cloud is going to

00:04:53.950 --> 00:04:55.512
enable smarter robots.

00:04:55.512 --> 00:04:58.580
It starts with that
off-the-shelf hardware.

00:04:58.580 --> 00:05:01.270
The mobile phones and the
tablets of today and

00:05:01.270 --> 00:05:05.160
commercial sensors that you
see on the top of the PR2.

00:05:05.160 --> 00:05:08.930
And tapping those into a common
set of APIs, a common

00:05:08.930 --> 00:05:10.140
open framework.

00:05:10.140 --> 00:05:14.170
That's going to enable you to
solve hard robotics problems

00:05:14.170 --> 00:05:16.190
that haven't been
solved before.

00:05:16.190 --> 00:05:20.880
Because the basics will be taken
care of for you now.

00:05:20.880 --> 00:05:22.385
And using the cloud
gives us scalable

00:05:22.385 --> 00:05:24.860
CPU memory and storage.

00:05:24.860 --> 00:05:27.250
You essentially have unlimited
amounts of knowledge that you

00:05:27.250 --> 00:05:28.150
can tap into.

00:05:28.150 --> 00:05:30.120
Unlimited processing power.

00:05:30.120 --> 00:05:31.880
You don't have to be worried
about your power

00:05:31.880 --> 00:05:33.860
budget on the robot.

00:05:33.860 --> 00:05:36.620
You can just send all of that
data off to the cloud, and

00:05:36.620 --> 00:05:40.720
have one, 10, 10,000 servers
all crunching

00:05:40.720 --> 00:05:41.970
the data for you.

00:05:44.300 --> 00:05:46.690
So to give you an overview of
ROS and an introduction to

00:05:46.690 --> 00:05:48.630
what that is and some
demos, I'm going to

00:05:48.630 --> 00:05:49.930
hand it over to Ken.

00:05:49.930 --> 00:05:54.470
KEN CONLEY: Thank you, Ryan.

00:05:54.470 --> 00:05:57.390
Hi everyone, I'm Ken Conley from
Willow Garage, and this

00:05:57.390 --> 00:05:58.940
is my colleague Brian Gerkey.

00:05:58.940 --> 00:06:02.710
Today we're going to speak to
you about ROS in the cloud.

00:06:02.710 --> 00:06:06.290
Just like Android provides you
tools and libraries to develop

00:06:06.290 --> 00:06:09.270
applications for smartphones
and tablets,

00:06:09.270 --> 00:06:11.990
ROS does for robots.

00:06:11.990 --> 00:06:15.070
And just like Android, ROS
is completely free and

00:06:15.070 --> 00:06:18.950
open-source for you to
customize and extend.

00:06:18.950 --> 00:06:22.260
We have developers around the
world at the top research

00:06:22.260 --> 00:06:26.870
labs, like MIT, Stanford,
Berkeley, University of

00:06:26.870 --> 00:06:30.650
Pennsylvania, Georgia Tech, and
much more, all providing

00:06:30.650 --> 00:06:33.750
libraries for you to use
with your robot.

00:06:33.750 --> 00:06:36.080
In fact, there are thousands
of them.

00:06:36.080 --> 00:06:39.590
All the way from low-level
sensor drivers to computer

00:06:39.590 --> 00:06:43.190
vision algorithms to some of
the latest and greatest

00:06:43.190 --> 00:06:44.475
research results being
publishing

00:06:44.475 --> 00:06:47.940
in conferences today.

00:06:47.940 --> 00:06:51.080
Now you may wonder, what can
these libraries help me build?

00:06:51.080 --> 00:06:54.460
A lot of it depends on what
your robot looks like.

00:06:54.460 --> 00:06:57.870
ROS runs on robots like the one
here with two arms and a

00:06:57.870 --> 00:07:00.050
mobile base and a lot
of sensors on top.

00:07:00.050 --> 00:07:03.220
But ROS runs on many other
different types of robot,

00:07:03.220 --> 00:07:05.970
including robots that fly
through the air, like

00:07:05.970 --> 00:07:11.170
quadrotors, as well robots at
sea and robots in the ocean,

00:07:11.170 --> 00:07:14.490
both in and underneath
the water.

00:07:14.490 --> 00:07:17.870
Some of these robots are built
out of plywood and motors by

00:07:17.870 --> 00:07:20.190
graduate students, and others
are ones that you can buy off

00:07:20.190 --> 00:07:24.280
the shelf and start programming
immediately.

00:07:24.280 --> 00:07:26.380
One of the questions I get most
frequently asked about

00:07:26.380 --> 00:07:29.130
ROS is, what do I
need to run ROS?

00:07:29.130 --> 00:07:31.870
Well, that depends on
what you want to do.

00:07:31.870 --> 00:07:34.150
If you're going to build your
own autonomous car, you're

00:07:34.150 --> 00:07:36.680
probably going to need some
servers in the trunk.

00:07:36.680 --> 00:07:39.380
But instead, if you're just
trying to visualize data from

00:07:39.380 --> 00:07:42.460
a surfboard, you might be
interested in knowing that ROS

00:07:42.460 --> 00:07:45.860
can run on platforms as small
as Arduino, Beagle Boards,

00:07:45.860 --> 00:07:52.960
PandaBoards, and other low
cost, small platforms.

00:07:52.960 --> 00:07:54.730
So what is ROS?

00:07:54.730 --> 00:07:58.520
Well, at its heart, ROS is a
message-passing system, based

00:07:58.520 --> 00:08:00.460
on an anonymous,
publish/subscribe

00:08:00.460 --> 00:08:03.020
architecture.

00:08:03.020 --> 00:08:06.940
In ROS, you have nodes, which
are processes, and they

00:08:06.940 --> 00:08:10.190
communicate with each other over
topics which are usually

00:08:10.190 --> 00:08:11.410
network sockets.

00:08:11.410 --> 00:08:14.080
So these nodes can be
on one computer or

00:08:14.080 --> 00:08:15.450
they can be on many.

00:08:15.450 --> 00:08:18.540
And so to find each other,
there's a ROS core, which acts

00:08:18.540 --> 00:08:20.024
as a name service.

00:08:22.930 --> 00:08:25.890
ROS has language findings in
a variety of languages,

00:08:25.890 --> 00:08:29.490
including C++, Python,
LISP, and Java.

00:08:29.490 --> 00:08:31.720
And it also has command line
tools that let you interact

00:08:31.720 --> 00:08:32.880
with it directly.

00:08:32.880 --> 00:08:36.140
So to give you a quick Hello
world example in ROS, we're

00:08:36.140 --> 00:08:38.490
going to use a command line
tool called rostopic.

00:08:38.490 --> 00:08:42.390
On the first line, we do a
rostopic publish to the

00:08:42.390 --> 00:08:45.400
chatter topic, a string
containing Hello world, and

00:08:45.400 --> 00:08:48.430
we're going to publish it
10 times per second.

00:08:48.430 --> 00:08:50.840
Now, on another terminal, or on
another computer, you can

00:08:50.840 --> 00:08:53.900
type rostopic echo of the
chatter topic, and you'll see

00:08:53.900 --> 00:08:55.920
that data displaying
to your screen.

00:08:55.920 --> 00:08:59.960
And it's pretty simple to just
start exchanging messages.

00:08:59.960 --> 00:09:02.330
Of course, you need a lot more
than messages to build a robot

00:09:02.330 --> 00:09:06.010
application, so we provide a lot
of functionality as well.

00:09:06.010 --> 00:09:10.190
We focused on three main areas
in ROS for our own

00:09:10.190 --> 00:09:10.610
development.

00:09:10.610 --> 00:09:13.460
Perception, mobility,
and manipulation.

00:09:13.460 --> 00:09:15.340
Because we believe a combination
of these three

00:09:15.340 --> 00:09:18.700
capabilities are what you need
to build robots that are meant

00:09:18.700 --> 00:09:21.440
to interact in environments
designed for humans.

00:09:21.440 --> 00:09:24.110
So if you're navigating around
a crowded living room and

00:09:24.110 --> 00:09:27.480
having to avoid people or your
cat, or if you're trying to

00:09:27.480 --> 00:09:29.820
get a robot to do your laundry,
you're going to need

00:09:29.820 --> 00:09:31.070
these capabilities.

00:09:33.930 --> 00:09:36.370
In order to design these,
we also needed a robot.

00:09:36.370 --> 00:09:37.805
And so we built our own.

00:09:37.805 --> 00:09:39.460
It's sitting over
here to my left.

00:09:39.460 --> 00:09:43.210
It's called the PR2, and it's
built by Willow Garage.

00:09:43.210 --> 00:09:47.530
We built the PR2 to be
a world-class mobile

00:09:47.530 --> 00:09:49.210
manipulation research
platform.

00:09:49.210 --> 00:09:51.850
So all the best researchers out
there in robotics would be

00:09:51.850 --> 00:09:56.430
able to do anything they could
dream of with this platform.

00:09:56.430 --> 00:09:58.720
So of course, since it is a
research platform, it has a

00:09:58.720 --> 00:09:59.810
pretty big price tag.

00:09:59.810 --> 00:10:01.590
It's $400,000.

00:10:01.590 --> 00:10:03.980
But for all of you in the
audience that contribute to

00:10:03.980 --> 00:10:09.960
open source, we have a discount
price of $280,000.

00:10:09.960 --> 00:10:11.306
Just want to put that out
there, in case you

00:10:11.306 --> 00:10:13.550
want to order one.

00:10:13.550 --> 00:10:15.080
For its brains, it has--

00:10:15.080 --> 00:10:15.840
it's pretty beefy.

00:10:15.840 --> 00:10:18.240
It's got two servers in it,
each with eight core Intel

00:10:18.240 --> 00:10:21.270
Xeon processors and
24 GB of RAM.

00:10:21.270 --> 00:10:22.765
It's covered head to
toe in sensors.

00:10:22.765 --> 00:10:25.475
It's got seven cameras, multiple
laser rangefinders.

00:10:25.475 --> 00:10:28.440
And it's got these really
awesome arms. And we put these

00:10:28.440 --> 00:10:31.970
capabilities and computation in
there so researchers would

00:10:31.970 --> 00:10:34.570
really be able to do innovative
applications that

00:10:34.570 --> 00:10:36.500
would get us toward
that Jetsons,

00:10:36.500 --> 00:10:39.440
Rosie the Robot future.

00:10:39.440 --> 00:10:41.490
So here's the video of what some
researchers at Berkeley

00:10:41.490 --> 00:10:43.460
did a year ago with the PR2.

00:10:43.460 --> 00:10:46.020
They got it to fold towels.

00:10:46.020 --> 00:10:48.600
When they first published this
video, it said 50x on it,

00:10:48.600 --> 00:10:50.640
because it's took 25
minutes per towel,

00:10:50.640 --> 00:10:52.440
which is pretty slow.

00:10:52.440 --> 00:10:54.440
But just last month, we
shot this new video.

00:10:54.440 --> 00:10:56.375
They have it running
five times faster.

00:10:56.375 --> 00:11:00.020
It only takes between two and
six minutes per towel.

00:11:00.020 --> 00:11:01.980
And they were also able to get
rid of a lot of custom

00:11:01.980 --> 00:11:03.650
hardware they used the
first time around.

00:11:03.650 --> 00:11:05.800
So they were able to improve
the performance both in

00:11:05.800 --> 00:11:07.300
software and the hardware
requirements.

00:11:10.530 --> 00:11:13.260
So, fairly soon these
researchers--

00:11:16.776 --> 00:11:17.985
This is real, folks.

00:11:17.985 --> 00:11:19.860
You can do it.

00:11:19.860 --> 00:11:22.330
So, soon these researchers think
that they'll have all

00:11:22.330 --> 00:11:24.480
the basic problems of laundry
solved, from loading the

00:11:24.480 --> 00:11:26.400
washing machine to emptying
the dryer to

00:11:26.400 --> 00:11:27.690
folding their clothes.

00:11:27.690 --> 00:11:29.850
But this is the perfect
opportunity for the cloud.

00:11:29.850 --> 00:11:33.440
Because fashion, as we know,
is constantly changing.

00:11:33.440 --> 00:11:36.690
So even as these researchers
add new features like pants

00:11:36.690 --> 00:11:40.020
and baby clothes and socks, you
want your robot to be able

00:11:40.020 --> 00:11:42.410
to fold your laundry whether
it's a brand new t-shirt that

00:11:42.410 --> 00:11:47.340
you got from a conference or a
Snuggie you got for Christmas.

00:11:47.340 --> 00:11:50.090
So now, we're going
to give you a live

00:11:50.090 --> 00:11:51.340
demo of ROS in action.

00:11:51.340 --> 00:11:53.720
I'm going to show you a tool
called rviz, which is probably

00:11:53.720 --> 00:11:56.030
the most widely used
tool in ROS.

00:11:56.030 --> 00:11:57.590
It's a 3-D visualizer.

00:11:57.590 --> 00:12:01.680
Now what we see here on the
screen is a model of the PR2.

00:12:01.680 --> 00:12:03.760
This is actually connected
to this PR2.

00:12:03.760 --> 00:12:05.970
So as I move it, around you can
see that the display on

00:12:05.970 --> 00:12:08.180
the screen updates.

00:12:08.180 --> 00:12:10.820
We can add all sorts of 3-D data
into this view, so we can

00:12:10.820 --> 00:12:14.240
understand what the robot
is seeing and thinking.

00:12:14.240 --> 00:12:17.600
So right here, we see data
from this tilting laser.

00:12:17.600 --> 00:12:21.750
So as you can see, it's able
to see and build a nice 3-D

00:12:21.750 --> 00:12:24.180
model of the room that
we're sitting in.

00:12:24.180 --> 00:12:26.400
We can also see other sensors.

00:12:26.400 --> 00:12:29.090
Like on top of the head,
we have this Kinect.

00:12:29.090 --> 00:12:31.130
the Can we bring the stage
lights up real quick?

00:12:40.050 --> 00:12:43.480
It's a little dark, but you can
see there's some nice 3-D

00:12:43.480 --> 00:12:45.470
color images of people sitting
in the front audience.

00:12:45.470 --> 00:12:47.570
And I'll note, this is
just a normal Kinect

00:12:47.570 --> 00:12:49.120
sitting on a head.

00:12:49.120 --> 00:12:53.110
You two can just buy one from
Best Buy or Fry's Electronics

00:12:53.110 --> 00:12:57.590
and get started using this
software on your own.

00:12:57.590 --> 00:13:01.320
Now, you may wonder, what sort
of tools do we provide to you

00:13:01.320 --> 00:13:02.970
to make all this possible?

00:13:02.970 --> 00:13:05.250
Because this is actually
pretty complicated.

00:13:05.250 --> 00:13:08.430
We have two different sensors
and a complicated robot, and

00:13:08.430 --> 00:13:10.830
even as I move the head of the
robot up and down, you can see

00:13:10.830 --> 00:13:13.450
that the 3-D cloud is
correctly moving and

00:13:13.450 --> 00:13:15.710
re-registering its position
in the world.

00:13:15.710 --> 00:13:17.520
Well, we have libraries in
ROS that do that for you

00:13:17.520 --> 00:13:18.130
automatically.

00:13:18.130 --> 00:13:22.230
And one them is called tf, which
stands for transforms.

00:13:22.230 --> 00:13:25.220
So just like in physics, where
you need a reference frame to

00:13:25.220 --> 00:13:27.530
understand your data, in
robotics we call these

00:13:27.530 --> 00:13:28.620
coordinate frames.

00:13:28.620 --> 00:13:30.640
And with these coordinate
frames, they may be as simple

00:13:30.640 --> 00:13:33.850
as saying, here's a position
on a map, or I can say

00:13:33.850 --> 00:13:36.970
something more complicated like,
this is a position one

00:13:36.970 --> 00:13:38.450
meter in front of my hand.

00:13:38.450 --> 00:13:41.600
Or I can attach it to data and
say, this is data collected

00:13:41.600 --> 00:13:44.140
from this sensor mounted
to this point on

00:13:44.140 --> 00:13:45.550
the head of my robot.

00:13:45.550 --> 00:13:47.140
And it will do all
the rest for you.

00:13:47.140 --> 00:13:49.480
So we can see what they
look like here.

00:13:49.480 --> 00:13:52.820
As you can see, there's lots
of these on the robot, and

00:13:52.820 --> 00:13:56.880
they allow us to make maps
simple so that we can just

00:13:56.880 --> 00:13:58.130
focus on building
applications.

00:14:01.270 --> 00:14:03.430
So in ROS, the message
is the medium.

00:14:03.430 --> 00:14:07.260
By that, I mean instead of
having code APIs in C++ or

00:14:07.260 --> 00:14:10.870
Java that you call to make a
robot do something, in ROS,

00:14:10.870 --> 00:14:12.650
you just publish a message.

00:14:12.650 --> 00:14:14.950
And that will get the robot
to do an action.

00:14:14.950 --> 00:14:17.360
So we're going to run through
some quick examples that show

00:14:17.360 --> 00:14:20.400
you that moving a robot's pretty
easy, and you can do it

00:14:20.400 --> 00:14:24.980
in just, in this case, three
lines of Python.

00:14:24.980 --> 00:14:27.710
So in this example, on the first
line I'm going to create

00:14:27.710 --> 00:14:31.640
a point, and I'm going to put
it one meter in front of the

00:14:31.640 --> 00:14:34.030
base link, which is one of these
coordinate frames I just

00:14:34.030 --> 00:14:36.630
showed you in the visualizer.

00:14:36.630 --> 00:14:39.290
On the second line, I'm going to
create a message, and this

00:14:39.290 --> 00:14:43.160
message is a point head goal,
which will contain this point

00:14:43.160 --> 00:14:46.050
that I want to have
the robot look at.

00:14:46.050 --> 00:14:48.260
And on the last line, we're
simply going to publish it to

00:14:48.260 --> 00:14:50.870
a topic that makes the
robot move its head.

00:15:00.220 --> 00:15:04.230
As you can see, robot's now
looking at the base.

00:15:04.230 --> 00:15:07.080
Now, with just a couple more
lines of code, we can make it

00:15:07.080 --> 00:15:08.330
do something more interesting.

00:15:11.020 --> 00:15:13.500
Instead of having it look at the
base, I'm going to have a

00:15:13.500 --> 00:15:15.760
look at the hand.

00:15:15.760 --> 00:15:18.980
And instead of publishing one
message, I'll publish messages

00:15:18.980 --> 00:15:21.600
ten times per second, so that
the robot will be able to

00:15:21.600 --> 00:15:22.980
track the movement
of the hand.

00:15:27.680 --> 00:15:30.520
As you can see, it's now looking
at the gripper as I

00:15:30.520 --> 00:15:32.500
move it around.

00:15:32.500 --> 00:15:34.120
In five lines of code,
I now have the

00:15:34.120 --> 00:15:36.500
robot tracking an object.

00:15:36.500 --> 00:15:38.280
And as you remember before,
those coordinate frames could

00:15:38.280 --> 00:15:40.620
be anywhere, so I can attach
those to any sort of object,

00:15:40.620 --> 00:15:42.750
and get the head moving around
and tracking it.

00:15:48.070 --> 00:15:50.580
Now I started off showing you a
Hello world example where it

00:15:50.580 --> 00:15:52.670
just printed Hello world
to the screen,

00:15:52.670 --> 00:15:54.150
but this is a robot.

00:15:54.150 --> 00:15:56.040
It doesn't have a screen
to print hello to.

00:15:56.040 --> 00:15:58.000
It'll have to wave to us
instead, which is actually a

00:15:58.000 --> 00:15:59.990
lot more fun.

00:15:59.990 --> 00:16:01.870
So this looks a little bit
more complicated, because

00:16:01.870 --> 00:16:05.210
instead of creating a point,
we're going to create a pose.

00:16:05.210 --> 00:16:09.160
Because we have to control the
orientation of the arm.

00:16:09.160 --> 00:16:11.100
This is all actually
pretty simple.

00:16:11.100 --> 00:16:13.390
So for the orientation, we're
going to use a quaternion,

00:16:13.390 --> 00:16:16.520
which is also used in some
APIs in Android.

00:16:16.520 --> 00:16:18.970
And for the rest of it, all
we're going to do is take a

00:16:18.970 --> 00:16:22.150
point a half a meter in front
of the side-plate of this

00:16:22.150 --> 00:16:24.850
robot and move it back and forth
as a sine wave. So it

00:16:24.850 --> 00:16:27.330
looks like fancy math, but
remember sine waves, they just

00:16:27.330 --> 00:16:28.390
go up and down?

00:16:28.390 --> 00:16:31.310
We're just going to move one
back and forth like this.

00:16:31.310 --> 00:16:37.780
Now if we run this, we'll
see we have the robot

00:16:37.780 --> 00:16:39.030
waving hello at us.

00:16:41.550 --> 00:16:47.400
And if we run our previous
example and another process,

00:16:47.400 --> 00:16:49.000
we'll see that the head
of the robot is now

00:16:49.000 --> 00:16:50.460
tracking the hand.

00:16:50.460 --> 00:16:52.050
And this, in a nutshell,
is how you

00:16:52.050 --> 00:16:54.000
program robots with ROS.

00:16:54.000 --> 00:16:57.070
We have a bunch of nodes, and
they each try and do one thing

00:16:57.070 --> 00:16:58.910
well and no more.

00:16:58.910 --> 00:17:02.030
And then they can use ROS to
communicate with each other to

00:17:02.030 --> 00:17:03.582
do more complex behaviors.

00:17:03.582 --> 00:17:05.800
So whether you're just trying
to get the head of the robot

00:17:05.800 --> 00:17:09.450
to follow a hand, or if you're
trying to fold towels, you can

00:17:09.450 --> 00:17:10.700
all do it just using ROS.

00:17:21.720 --> 00:17:24.180
So this is the obviously a talk
about cloud robotics.

00:17:24.180 --> 00:17:26.200
Well, ROS was designed from
the ground up to be

00:17:26.200 --> 00:17:27.390
distributed.

00:17:27.390 --> 00:17:30.400
This PR2 has two computers,
in it but our original PR2

00:17:30.400 --> 00:17:32.520
prototypes had four computers.

00:17:32.520 --> 00:17:35.425
And we wanted to fully harness
that computational power for

00:17:35.425 --> 00:17:37.100
our applications.

00:17:37.100 --> 00:17:39.190
But what if you were able to
take the nodes that were

00:17:39.190 --> 00:17:41.300
running in one of these
computers, and just move them

00:17:41.300 --> 00:17:42.860
into the cloud instead?

00:17:42.860 --> 00:17:45.590
And take advantage of those
object recognition, voice

00:17:45.590 --> 00:17:48.190
services, mapping and
navigation, and other great

00:17:48.190 --> 00:17:51.040
things that the cloud
has to offer?

00:17:51.040 --> 00:17:54.120
Of course, we can do this,
but we want to know why.

00:17:54.120 --> 00:17:57.590
Well, the main reason is that
personal robots need to be

00:17:57.590 --> 00:17:59.270
inexpensive.

00:17:59.270 --> 00:18:01.870
Even with my employee discount,
I'm probably not

00:18:01.870 --> 00:18:03.900
going to have a PR2 in my
house folding laundry.

00:18:03.900 --> 00:18:06.530
It's a research platform,
and it's very expensive.

00:18:06.530 --> 00:18:09.350
And so for us to see that as
robot app developers, we need

00:18:09.350 --> 00:18:12.000
an inexpensive platform.

00:18:12.000 --> 00:18:15.150
So what makes robots like
the PR2 expensive?

00:18:15.150 --> 00:18:17.110
One of the main costs
is the servers.

00:18:17.110 --> 00:18:19.510
And not just in terms
of dollar costs.

00:18:19.510 --> 00:18:22.480
The majority of the power in the
PR2 goes to powering the

00:18:22.480 --> 00:18:25.200
computers, not the motors.

00:18:25.200 --> 00:18:28.990
And if you remove just one of
the two computers in the PR2,

00:18:28.990 --> 00:18:31.790
you double the battery life,
from two to four hours.

00:18:31.790 --> 00:18:34.600
So that means computation for
robots has costs not just in

00:18:34.600 --> 00:18:38.380
terms of money, but battery,
cooling, and space.

00:18:38.380 --> 00:18:42.100
All which are cheap
in the cloud.

00:18:42.100 --> 00:18:45.450
Another thing that makes robots
expensive is sensors.

00:18:45.450 --> 00:18:49.110
Just three years ago, when we
started in creating ROS and

00:18:49.110 --> 00:18:51.460
the PR2, if you wanted to build
your own little robot

00:18:51.460 --> 00:18:54.975
like this that just saw in two
dimensions, you'd have to buy

00:18:54.975 --> 00:18:58.140
a laser that cost well
over $1,000.

00:18:58.140 --> 00:19:02.310
But just last November,
Microsoft released the Kinect,

00:19:02.310 --> 00:19:06.480
which is based on technology
developed by PrimeSense.

00:19:06.480 --> 00:19:10.190
This is fantastic, because
suddenly even high schoolers

00:19:10.190 --> 00:19:12.710
who are shopping at their
favorite electronics store

00:19:12.710 --> 00:19:18.950
could buy a robotics-grade 3-D
sensor for them to develop on

00:19:18.950 --> 00:19:20.340
at home and on their
own desks.

00:19:23.100 --> 00:19:26.750
We really want to leverage the
potential of this new device,

00:19:26.750 --> 00:19:29.770
so we sponsored a contest a
month later to see what the

00:19:29.770 --> 00:19:32.910
developers in our community
could do using the ROS, the

00:19:32.910 --> 00:19:36.050
Kinect, and some computer vision
libraries that we have.

00:19:36.050 --> 00:19:38.750
And this is what they
came up with.

00:19:38.750 --> 00:19:42.980
So in this first example, you
simply draw some buttons, you

00:19:42.980 --> 00:19:45.380
press them, and you have
your own soundboard.

00:19:48.970 --> 00:19:52.000
And of course, people also use
the Kinect with actual robots.

00:19:52.000 --> 00:19:55.050
So, flying a helicopter around,
and using the Kinect

00:19:55.050 --> 00:19:59.420
to find obstacles, as well fly
down the middle of a corridor.

00:19:59.420 --> 00:20:01.170
People also used it much
like you would use

00:20:01.170 --> 00:20:02.550
it in a video game.

00:20:02.550 --> 00:20:05.650
So you wave your arms around,
the Kinect tracks you, and

00:20:05.650 --> 00:20:07.460
you're able to get a robot
to mimic your actions

00:20:07.460 --> 00:20:08.360
identically.

00:20:08.360 --> 00:20:12.140
You can perform complex tasks
like playing chess or having

00:20:12.140 --> 00:20:15.010
your robot fetch you a tissue.

00:20:15.010 --> 00:20:18.840
People also used it
as a 3-D sensor.

00:20:18.840 --> 00:20:21.550
You can use it-- you can wave it
around and use it to build

00:20:21.550 --> 00:20:24.490
a 3-D map of your environment,
or you can move it around a

00:20:24.490 --> 00:20:27.390
single object and use it to
create a detailed model of

00:20:27.390 --> 00:20:28.620
just that object.

00:20:28.620 --> 00:20:31.010
And we think developers will
be able to do a lot of

00:20:31.010 --> 00:20:34.220
exciting new applications
using these sorts of

00:20:34.220 --> 00:20:35.440
technologies.

00:20:35.440 --> 00:20:36.840
And I should note that
everything that you see in

00:20:36.840 --> 00:20:39.220
this video is open source
for you to use

00:20:39.220 --> 00:20:40.470
and play with yourself.

00:20:43.030 --> 00:20:45.240
As developers, we know that
we need a common hardware

00:20:45.240 --> 00:20:48.320
platform if we want to become
app developers.

00:20:48.320 --> 00:20:51.440
And so we've done that with a
mobile 3-D sensing platform

00:20:51.440 --> 00:20:53.430
that we call TurtleBot.

00:20:53.430 --> 00:20:56.440
TurtleBot provides you a Kinect,
a dual-core atom

00:20:56.440 --> 00:21:00.370
netbook, and an iRobot Create
base integrated complete with

00:21:00.370 --> 00:21:03.170
open source libraries and tools,
so that you can start

00:21:03.170 --> 00:21:06.340
writing robot apps
for the home.

00:21:06.340 --> 00:21:08.880
You may be wondering, what sort
of apps can I develop

00:21:08.880 --> 00:21:11.180
with these sorts of
capabilities?

00:21:11.180 --> 00:21:13.110
So let's look at a Google
Streetview car.

00:21:13.110 --> 00:21:16.050
Or as I like to call it, a
Google Streetview robot.

00:21:16.050 --> 00:21:20.360
Because if you look at it, up
top it has cameras so it can

00:21:20.360 --> 00:21:24.260
take panoramic images, it has
lasers so it can see in 3-D,

00:21:24.260 --> 00:21:26.860
it has GPS so it knows where it
is, and of course, it can

00:21:26.860 --> 00:21:28.070
drive around.

00:21:28.070 --> 00:21:30.280
Well, as it turns out, the
TurtleBot has all these same

00:21:30.280 --> 00:21:33.240
capabilities, just at
a different scale.

00:21:33.240 --> 00:21:35.080
And that scale is the home.

00:21:35.080 --> 00:21:37.820
And so you could use it to
build your own home-view

00:21:37.820 --> 00:21:39.680
alternative to streetview.

00:21:39.680 --> 00:21:42.240
And as you have your robot
going around, creating

00:21:42.240 --> 00:21:45.110
panoramic maps of your home,
you could feed it to object

00:21:45.110 --> 00:21:48.350
recognizers in the cloud, which
could help you start

00:21:48.350 --> 00:21:51.020
building an index of the
objects in your house.

00:21:51.020 --> 00:21:53.920
And as we know, if you have a
crawler and if you have an

00:21:53.920 --> 00:21:58.320
indexer, you can build
a search engine.

00:21:58.320 --> 00:22:03.370
At long last, you can finally
find your keys.

00:22:03.370 --> 00:22:04.950
There's probably a good
reason why we call web

00:22:04.950 --> 00:22:07.060
crawlers for robots.

00:22:07.060 --> 00:22:09.050
Now, a home search engine would
be very different from a

00:22:09.050 --> 00:22:13.100
web search engine, because it
would give us a new class of

00:22:13.100 --> 00:22:15.360
data, as developers,
to play with.

00:22:15.360 --> 00:22:18.180
It'll tell us, what are the
objects in my house?

00:22:18.180 --> 00:22:19.140
Where are they located?

00:22:19.140 --> 00:22:20.360
Where have they been?

00:22:20.360 --> 00:22:22.090
And it could even give us
information about their

00:22:22.090 --> 00:22:23.650
dimensions.

00:22:23.650 --> 00:22:26.310
All sorts of new data that
we can build on top of.

00:22:30.440 --> 00:22:33.420
And because it is a 3-D sensor,
we could also use it

00:22:33.420 --> 00:22:36.270
to create new pipelines from
physical to digital back to

00:22:36.270 --> 00:22:37.650
physical again.

00:22:37.650 --> 00:22:39.940
So when personal computers first
came out, they had dot

00:22:39.940 --> 00:22:42.490
matrix printers so we can
print documents out.

00:22:42.490 --> 00:22:44.840
And soon after, we had scanners,
so we can reapply

00:22:44.840 --> 00:22:47.670
those documents back
into digital form.

00:22:47.670 --> 00:22:50.160
Well, on the right of this
slide, you'll see a

00:22:50.160 --> 00:22:53.690
Thing-O-Matic from MakerBot,
which is a $1,200 3-D printer

00:22:53.690 --> 00:22:56.690
that you and your friends could
build for yourselves.

00:22:56.690 --> 00:23:01.150
So fairly soon, we could use
technology like 3-D sensors

00:23:01.150 --> 00:23:06.020
and 3-D printers to create new
pipelines in 3-D for creating

00:23:06.020 --> 00:23:07.910
objects and printing them
back out again.

00:23:11.020 --> 00:23:13.780
And also, we want to
challenge you as

00:23:13.780 --> 00:23:15.350
developers to think about--

00:23:15.350 --> 00:23:18.240
if you had a robot that was a
mobile 3-D sensing platform,

00:23:18.240 --> 00:23:21.440
and you combined it with a
smartphone or a tablet, what

00:23:21.440 --> 00:23:24.190
sort of new applications
could you build?

00:23:24.190 --> 00:23:26.850
What could you do if you were
able to combine the libraries,

00:23:26.850 --> 00:23:30.160
tools, and hardware that you
get with Android, and you

00:23:30.160 --> 00:23:32.230
combine it with a robot
running ROS?

00:23:32.230 --> 00:23:34.275
Well, to talk to you about ROS
and Android, I'm going to hand

00:23:34.275 --> 00:23:35.950
it over to Damon Kohler
from Google.

00:23:38.890 --> 00:23:46.660
DAMON KOHLER: Thanks
again, I'm Damon.

00:23:46.660 --> 00:23:48.665
I'm a Googler, and I'm
going to talk to you

00:23:48.665 --> 00:23:50.350
about ROS and Android.

00:23:50.350 --> 00:23:54.380
So, to make ROS work on Android,
I spent the last few

00:23:54.380 --> 00:23:56.380
months working pretty quickly
with Ken and the rest of

00:23:56.380 --> 00:23:58.960
Willow Garage to
create rosjava.

00:23:58.960 --> 00:24:02.960
And rosjava is the first pure
java implementation of ROS.

00:24:02.960 --> 00:24:06.800
And that allows us to achieve
Android compatibility.

00:24:06.800 --> 00:24:08.800
So, the entire project is
open source, just like

00:24:08.800 --> 00:24:11.100
all the rest of ROS.

00:24:11.100 --> 00:24:13.320
It's currently in early
release under heavy

00:24:13.320 --> 00:24:16.090
development, but you guys
can check it out.

00:24:16.090 --> 00:24:18.510
And all of a code examples
I'm about to show you are

00:24:18.510 --> 00:24:22.930
available in their complete
form later on the site.

00:24:22.930 --> 00:24:25.540
So what does a node look
like in rosjava?

00:24:25.540 --> 00:24:27.790
Well, right now, we're going to
implement the simple Hello

00:24:27.790 --> 00:24:31.230
world, the first Hello world
that Ken demonstrated.

00:24:31.230 --> 00:24:33.660
And so we start with
a talker node.

00:24:33.660 --> 00:24:35.620
The talker nodes implement
node main.

00:24:35.620 --> 00:24:38.220
And node main simply gives
a main loop entry

00:24:38.220 --> 00:24:39.750
point to all the nodes.

00:24:39.750 --> 00:24:43.160
And the main loop entry point
takes a node configuration.

00:24:43.160 --> 00:24:44.950
That configuration contains
things like the

00:24:44.950 --> 00:24:46.880
URI for the ROS core.

00:24:46.880 --> 00:24:48.130
That's the DNS node.

00:24:50.460 --> 00:24:53.510
The publisher node, in its
main loop, takes the node

00:24:53.510 --> 00:24:55.560
configuration and passes
that into the

00:24:55.560 --> 00:24:56.940
constructor for a new node.

00:24:56.940 --> 00:24:59.930
And we'll call that note
the talker node.

00:24:59.930 --> 00:25:02.110
In that talker node, we'll
create a new publisher.

00:25:02.110 --> 00:25:05.820
And that publisher will take a
ROS string message, and it

00:25:05.820 --> 00:25:09.640
will create the publisher
for the chatter topic.

00:25:09.640 --> 00:25:13.150
Then, in a loop, we just simply
put the Hello world

00:25:13.150 --> 00:25:15.850
string into the ROS string
message, and we publish it

00:25:15.850 --> 00:25:18.920
once per second.

00:25:18.920 --> 00:25:21.650
So now we need to look at the
other side, the subscriber.

00:25:21.650 --> 00:25:23.420
So we create a new
listener node.

00:25:23.420 --> 00:25:26.300
Take the config again, and we
create a subscriber for this

00:25:26.300 --> 00:25:28.260
last chatter topic.

00:25:28.260 --> 00:25:30.880
For that subscriber, we have a
new message listener, and it

00:25:30.880 --> 00:25:33.020
expects raw string messages.

00:25:33.020 --> 00:25:35.160
And then on every new message,
we simply print the Hello

00:25:35.160 --> 00:25:39.410
world string that we received
to standard out.

00:25:39.410 --> 00:25:41.950
So to make that work, we use
another command line tool from

00:25:41.950 --> 00:25:43.500
ROS called rosrun.

00:25:43.500 --> 00:25:45.050
And we run the two nodes.

00:25:45.050 --> 00:25:47.325
And then once they both come
up, then you'll see Hello

00:25:47.325 --> 00:25:52.770
world printed to standard
out once per second.

00:25:52.770 --> 00:25:55.290
So what does that look
like on Android?

00:25:55.290 --> 00:25:58.700
So here we have the same Hello
world code with an additional

00:25:58.700 --> 00:26:00.770
counter that lets you
see every time a new

00:26:00.770 --> 00:26:02.270
message comes in.

00:26:02.270 --> 00:26:04.524
And this is running entirely
on that Android device.

00:26:07.190 --> 00:26:11.080
So to start that, we have our
main activity for Android, and

00:26:11.080 --> 00:26:13.570
then in that main activity,
we create a node runner.

00:26:13.570 --> 00:26:15.990
And the node runner, in this
case, is taking the place of

00:26:15.990 --> 00:26:19.620
the rosrun command line tool.

00:26:19.620 --> 00:26:21.900
And instead of running all
of the nodes in separate

00:26:21.900 --> 00:26:24.240
processes here, we'll run them
in separate threads.

00:26:28.070 --> 00:26:31.890
So, in onCreate, we're going
to to find the ROS TextView

00:26:31.890 --> 00:26:34.500
that we put into the layout,
and then we're going to set

00:26:34.500 --> 00:26:37.840
the topic name for that ROS
text view to /chatter.

00:26:37.840 --> 00:26:40.220
And that's the topic that
we'll subscribe to.

00:26:40.220 --> 00:26:43.570
And then we're going to execute
it, just like we do

00:26:43.570 --> 00:26:45.200
with the talker node.

00:26:45.200 --> 00:26:47.465
So that ROS TextView
is both an Android

00:26:47.465 --> 00:26:49.430
TextView and a ROS node.

00:26:52.230 --> 00:26:54.250
So if you actually take a look
at the inside of that ROS

00:26:54.250 --> 00:26:58.340
TextView, it extends TextView,
and it implements node main.

00:26:58.340 --> 00:27:01.080
So in that TextView, we have
the topic name and the node

00:27:01.080 --> 00:27:03.360
for that view.

00:27:03.360 --> 00:27:06.650
So in the main loop, we take
the node, we create a

00:27:06.650 --> 00:27:11.000
subscriber, we subscribe to the
topic name that was set,

00:27:11.000 --> 00:27:13.860
and then on every new message,
we post a new runnable to the

00:27:13.860 --> 00:27:19.480
UI thread, so that we can update
the TextView text.

00:27:19.480 --> 00:27:21.980
And that's how Hello
world works.

00:27:21.980 --> 00:27:23.750
But Hello World is kind of
boring, and we have all these

00:27:23.750 --> 00:27:25.740
cool sensors on Android
devices.

00:27:25.740 --> 00:27:28.010
So, in this example, we're
actually publishing the

00:27:28.010 --> 00:27:30.680
orientation of the
device to rviz.

00:27:30.680 --> 00:27:33.040
And rviz is visualizing that as
a set of coordinates that

00:27:33.040 --> 00:27:35.170
rotate as the orientation
of the phone changes.

00:27:39.730 --> 00:27:44.130
So in this case, our node will
grab the Android sensor

00:27:44.130 --> 00:27:47.420
service, or the sensor manager,
rather, and it will

00:27:47.420 --> 00:27:49.110
create a new sensor
listener for the

00:27:49.110 --> 00:27:51.270
rotation vector sensor.

00:27:51.270 --> 00:27:55.000
The rotation vector sensor
kindly returns quaternions,

00:27:55.000 --> 00:27:57.710
which is the preferred
representation of orientation

00:27:57.710 --> 00:27:59.780
for ROS, so that gets rid
of a lot of the work we

00:27:59.780 --> 00:28:02.060
would have had to do.

00:28:02.060 --> 00:28:05.020
So every time we get a new
sensor event from Android,

00:28:05.020 --> 00:28:07.570
then we're going to take that
quaternion and put it into a

00:28:07.570 --> 00:28:09.330
ROS quaternion message.

00:28:09.330 --> 00:28:11.830
And then we're going to use
that to create another ROS

00:28:11.830 --> 00:28:14.750
Pose message, which is what Ken
was using earlier for the

00:28:14.750 --> 00:28:17.410
Hello World example
with the waving.

00:28:17.410 --> 00:28:19.320
Since we're not tracking the
position of the phone, we're

00:28:19.320 --> 00:28:21.830
going to lock it to the origin,
and then we're going

00:28:21.830 --> 00:28:24.990
to just instead publish
its orientation.

00:28:24.990 --> 00:28:27.550
So on every new sensor event
that we get from Android, we

00:28:27.550 --> 00:28:30.530
publish a new ROS PoseStamped
message with the orientation

00:28:30.530 --> 00:28:31.780
of the device.

00:28:33.970 --> 00:28:36.090
So there's lots of other
sensors, besides orientation,

00:28:36.090 --> 00:28:37.250
that are useful.

00:28:37.250 --> 00:28:39.760
Cameras are super useful, and
the PR2 has seven of them,

00:28:39.760 --> 00:28:40.800
like I said.

00:28:40.800 --> 00:28:44.150
So this particular example, we
are subscribing to the camera

00:28:44.150 --> 00:28:47.900
from the PR2 and displaying
that on the tablet.

00:28:47.900 --> 00:28:50.740
To do that, we use the
ROS image_view.

00:28:50.740 --> 00:28:54.270
And we set up a ROS image_view
that accepts a compressed

00:28:54.270 --> 00:28:58.220
image ROS message, and then we
set the topic name to /camera

00:28:58.220 --> 00:29:02.100
that we want to subscribe to,
and then we execute the node.

00:29:02.100 --> 00:29:04.620
Easy as that.

00:29:04.620 --> 00:29:07.050
But PR2s aren't the only
things with cameras.

00:29:07.050 --> 00:29:08.810
Your Android devices have
cameras as well.

00:29:08.810 --> 00:29:11.400
So in this example, we actually
have the camera being

00:29:11.400 --> 00:29:13.300
published from one
device and being

00:29:13.300 --> 00:29:14.550
subscribed to on the other.

00:29:17.270 --> 00:29:19.610
To do the camera publishing,
we use a ROS

00:29:19.610 --> 00:29:20.800
camera preview view.

00:29:20.800 --> 00:29:23.170
It gets rid of all the
camera code that you

00:29:23.170 --> 00:29:24.310
would usually write.

00:29:24.310 --> 00:29:26.490
And we set the topic name that
we want to publish those

00:29:26.490 --> 00:29:29.680
images too, /camera, and then
we execute the node again.

00:29:32.530 --> 00:29:35.190
So at this point, what I'd like
to do is take all those

00:29:35.190 --> 00:29:37.750
little demos that I showed you
and sort of wrap them up into

00:29:37.750 --> 00:29:40.130
one package and show
you how it actually

00:29:40.130 --> 00:29:41.380
interacts with the PR2.

00:29:46.460 --> 00:29:47.710
Ready?

00:29:52.870 --> 00:29:54.840
Great.

00:29:54.840 --> 00:29:58.260
You can see Ken's tablet that
has the camera picture coming

00:29:58.260 --> 00:30:00.200
from the PR2's head.

00:30:00.200 --> 00:30:02.880
And then if he puts his finger
on the screen and changes the

00:30:02.880 --> 00:30:06.030
orientation of the tablet, the
PR2 tracks the orientation of

00:30:06.030 --> 00:30:07.170
the tablet.

00:30:07.170 --> 00:30:09.120
So now you're actually inside
the head of the PR2.

00:30:18.810 --> 00:30:21.570
So I've just shown you a couple
of the-- well, can we

00:30:21.570 --> 00:30:23.560
switch back?

00:30:23.560 --> 00:30:23.930
Excellent.

00:30:23.930 --> 00:30:26.360
I've just shown you a few of the
possibilities of actually

00:30:26.360 --> 00:30:28.930
integrating Android devices with
advanced robots like the

00:30:28.930 --> 00:30:33.480
PR2 or still advanced
but more accessible

00:30:33.480 --> 00:30:35.510
robots like the TurtleBot.

00:30:35.510 --> 00:30:38.580
But there's lots and lots
more options out there.

00:30:38.580 --> 00:30:40.580
So, with the Open Accessory
API that was announced

00:30:40.580 --> 00:30:42.780
yesterday, you can start
connecting Android devices

00:30:42.780 --> 00:30:45.370
directly to actuators and
external sensors.

00:30:45.370 --> 00:30:47.010
But you don't even
have to do that.

00:30:47.010 --> 00:30:50.100
Your Android device has tons
of sensors on board already

00:30:50.100 --> 00:30:51.800
that are exceptionally
useful to robots.

00:30:51.800 --> 00:30:54.470
So now, with rosjava, you can
actually connect those robots

00:30:54.470 --> 00:30:55.750
to your Android devices to take

00:30:55.750 --> 00:30:58.140
advantage of those things.

00:30:58.140 --> 00:31:01.040
And in addition, Android
devices typically have

00:31:01.040 --> 00:31:02.550
wireless access.

00:31:02.550 --> 00:31:05.110
That mean your Android device,
when it becomes an integral

00:31:05.110 --> 00:31:09.015
part of your robot, becomes
its link to the cloud.

00:31:09.015 --> 00:31:12.530
It gives the robot the ability
to access that unlimited CPU

00:31:12.530 --> 00:31:15.340
memory and storage that Ryan
was talking about.

00:31:15.340 --> 00:31:17.430
So with that, I'll give
it back to Ryan.

00:31:17.430 --> 00:31:19.880
RYAN HICKMAN: Thanks, guys.

00:31:26.356 --> 00:31:30.970
So who in here has written
Android apps before?

00:31:30.970 --> 00:31:31.760
Oh, that's pretty good.

00:31:31.760 --> 00:31:34.880
There were three competing
Android talks right now.

00:31:34.880 --> 00:31:38.130
Who in here has written
web apps before?

00:31:38.130 --> 00:31:39.330
Oh, that's awesome.

00:31:39.330 --> 00:31:42.710
Who in here has written apps
for robots before?

00:31:42.710 --> 00:31:43.500
Wow.

00:31:43.500 --> 00:31:45.330
That's incredible.

00:31:45.330 --> 00:31:47.810
So we would love to double or
triple that, though, and hit

00:31:47.810 --> 00:31:48.300
all of you.

00:31:48.300 --> 00:31:51.100
And I hope if you came into this
talk not knowing anything

00:31:51.100 --> 00:31:53.730
about robots-- which looked like
it was about 2/3 of you--

00:31:53.730 --> 00:31:58.460
I hope you can now see why tying
the robot to Android and

00:31:58.460 --> 00:32:01.470
then to the cloud means that
you're reducing the processing

00:32:01.470 --> 00:32:02.890
need on the robot.

00:32:02.890 --> 00:32:05.470
And so you're reducing using the
battery load on the robot,

00:32:05.470 --> 00:32:07.830
and so you're reducing the
weight of the robot and the

00:32:07.830 --> 00:32:09.190
cost of the robot.

00:32:09.190 --> 00:32:13.210
And so the efficiency of
reducing all of that, while at

00:32:13.210 --> 00:32:15.950
the same time tapping into new
cloud services that make it

00:32:15.950 --> 00:32:17.830
even do more than it
ever did before.

00:32:17.830 --> 00:32:23.700
So the price-performance ratio
shift here is pretty dramatic.

00:32:23.700 --> 00:32:25.820
So I'm going to give you one
demo we put together here of

00:32:25.820 --> 00:32:27.320
object recognition.

00:32:27.320 --> 00:32:30.000
Since we're not launching any
new cloud services today,

00:32:30.000 --> 00:32:32.990
we're releasing rosjava today,
we wanted to show you what

00:32:32.990 --> 00:32:35.140
could we do if we took a
technology we already had.

00:32:35.140 --> 00:32:37.370
So we worked with the Google
Goggles team--

00:32:37.370 --> 00:32:39.360
and we wrapped it around
a web service and

00:32:39.360 --> 00:32:41.880
created a API for robots.

00:32:41.880 --> 00:32:44.060
And what it allowed us
to do was to train a

00:32:44.060 --> 00:32:45.395
custom corpus of images.

00:32:45.395 --> 00:32:48.120
So you might want your robot to
recognize something that's

00:32:48.120 --> 00:32:51.490
not already in the Google
Goggles system, for example.

00:32:51.490 --> 00:32:54.950
And then, once we stored that
knowledge in the cloud, any

00:32:54.950 --> 00:32:57.240
robot could then access it.

00:32:57.240 --> 00:32:59.430
So what you'll see here
is, [? Chaitanya ?]

00:32:59.430 --> 00:33:02.710
is typing in the name of one
of the Android figurines.

00:33:02.710 --> 00:33:04.770
And we went and labelled
them all.

00:33:04.770 --> 00:33:06.500
So this was the honeycomb
figurine.

00:33:06.500 --> 00:33:09.520
He types in the name on the
phone, and then he starts

00:33:09.520 --> 00:33:12.320
taking pictures of it from
different angles.

00:33:12.320 --> 00:33:14.730
And doing that from the phone
and sending those pictures up

00:33:14.730 --> 00:33:17.800
to the cloud then trains the
cloud for what the honeycomb

00:33:17.800 --> 00:33:19.850
figurine looks like.

00:33:19.850 --> 00:33:23.070
We also had a web-based
interface so that if your

00:33:23.070 --> 00:33:25.020
robot was remotely
at an object, you

00:33:25.020 --> 00:33:25.870
could still train it.

00:33:25.870 --> 00:33:28.760
So we did the cupcake bug droid,
and then hit learn

00:33:28.760 --> 00:33:30.970
object and trained
it that way.

00:33:30.970 --> 00:33:33.800
And then, because I like
cupcakes, I asked the

00:33:33.800 --> 00:33:35.440
Turtlebot to go find me one.

00:33:45.751 --> 00:33:49.620
So what you had there was, train
once, on one system,

00:33:49.620 --> 00:33:51.380
store in the cloud, and
then all of the

00:33:51.380 --> 00:33:54.370
systems can access it.

00:33:54.370 --> 00:33:57.660
So there's actually a demo of
this running live upstairs.

00:33:57.660 --> 00:34:01.480
Hasbro has Project Phondox,
which are these small robots

00:34:01.480 --> 00:34:03.950
with Android phones walking
around the table.

00:34:03.950 --> 00:34:08.000
And when they can get wireless
access, they are recognizing

00:34:08.000 --> 00:34:09.300
different cards--

00:34:09.300 --> 00:34:12.440
letters on the cards,
Transformers, Autobots, and

00:34:12.440 --> 00:34:13.400
Decepticons--

00:34:13.400 --> 00:34:15.679
and they run from the
Decepticons and they smile and

00:34:15.679 --> 00:34:17.350
greet the Autobots.

00:34:17.350 --> 00:34:18.909
And this is running
in the cloud.

00:34:18.909 --> 00:34:20.969
And we actually had an
interesting moment on Monday

00:34:20.969 --> 00:34:24.469
where we had 15 of the robots
out there, and we reset the

00:34:24.469 --> 00:34:27.560
system when we got here, and
none of them knew anything.

00:34:27.560 --> 00:34:28.900
They were all quite dumb.

00:34:28.900 --> 00:34:31.510
And then we just took one of
them, and we held up the

00:34:31.510 --> 00:34:33.480
cards, and we trained
that one.

00:34:33.480 --> 00:34:35.310
And then all 15 of
them had that

00:34:35.310 --> 00:34:37.001
knowledge at the same time.

00:34:39.710 --> 00:34:42.730
So what robotics problems
can you tackle?

00:34:42.730 --> 00:34:45.060
Well maybe you are already
really good at processing

00:34:45.060 --> 00:34:46.429
large amounts of data.

00:34:46.429 --> 00:34:49.840
Or someone on your team is
good at machine learning.

00:34:49.840 --> 00:34:52.330
Maybe you have expertise sharing
knowledge amongst

00:34:52.330 --> 00:34:56.130
different users or between
different devices.

00:34:56.130 --> 00:35:00.100
Or maybe you have a very
accessible application, or

00:35:00.100 --> 00:35:03.130
you're good with new forms
of user interaction.

00:35:03.130 --> 00:35:05.540
Because it's much better, as I
said earlier, to talk to the

00:35:05.540 --> 00:35:08.670
robot and ask it to fetch you
the beer than to walk up to it

00:35:08.670 --> 00:35:12.310
and grab a mouse and try to
click your way there.

00:35:12.310 --> 00:35:14.700
So what we would like all of
you to think about when you

00:35:14.700 --> 00:35:18.350
leave here is how can you
ROS-enable your web service or

00:35:18.350 --> 00:35:19.846
application?

00:35:19.846 --> 00:35:21.430
It's great if it's open
source, but it

00:35:21.430 --> 00:35:22.190
doesn't have to be.

00:35:22.190 --> 00:35:24.790
This could be a new form of
business, which is launching

00:35:24.790 --> 00:35:28.400
new APIs for robotics that
process information in a

00:35:28.400 --> 00:35:31.740
special way that solves
a real-world problem.

00:35:31.740 --> 00:35:34.520
And we want you to think about
ROS-enabling Android apps.

00:35:34.520 --> 00:35:37.500
So you can use rosjava, and
you can write an app with

00:35:37.500 --> 00:35:40.780
views and user interfaces that
are doing complex robotic

00:35:40.780 --> 00:35:43.480
processing behind the scenes
and also connecting with

00:35:43.480 --> 00:35:45.360
hardware and actuators.

00:35:45.360 --> 00:35:47.345
And then you can put those
apps in the market and

00:35:47.345 --> 00:35:50.670
distribute them out to any of
the 50 plus platforms that

00:35:50.670 --> 00:35:51.990
support ROS today.

00:35:51.990 --> 00:35:54.610
And I'm sure many more will come
online when people start

00:35:54.610 --> 00:35:57.980
connecting hardware to
Android in new ways.

00:35:57.980 --> 00:35:59.820
And you don't have to
go through Android.

00:35:59.820 --> 00:36:02.230
You could take the PR2 and
give it a direct cloud

00:36:02.230 --> 00:36:03.080
connection.

00:36:03.080 --> 00:36:06.200
You could take any sensor, give
it a network ID, tap it

00:36:06.200 --> 00:36:10.430
into a cloud service, and
it adds to the system.

00:36:10.430 --> 00:36:13.650
So, to get started, go
to cloudrobotics.com.

00:36:13.650 --> 00:36:15.910
That's going to take you to the
rosjava site, where you

00:36:15.910 --> 00:36:16.690
can download that.

00:36:16.690 --> 00:36:19.120
You can see the tutorials
that Ken and

00:36:19.120 --> 00:36:20.580
Damon showed you today.

00:36:20.580 --> 00:36:22.470
And if you haven't already been
there yet, we want you to

00:36:22.470 --> 00:36:24.680
come upstairs to the third
floor in the Android

00:36:24.680 --> 00:36:25.830
interactive zone.

00:36:25.830 --> 00:36:27.350
We have the TurtleBots
running around.

00:36:27.350 --> 00:36:28.750
You can touch them, you
can talk to some of

00:36:28.750 --> 00:36:29.840
the folks from Willow.

00:36:29.840 --> 00:36:33.440
We also have Hasbro there with
Project Phondox, and you can

00:36:33.440 --> 00:36:35.910
play with those and see
how those work.

00:36:35.910 --> 00:36:38.300
And then for you still in the
Bay Area 10 days from now,

00:36:38.300 --> 00:36:40.647
we'll be at the Maker Faire
which is a chance for you to

00:36:40.647 --> 00:36:43.760
see more do-it-yourself and
hobbyist devices connected to

00:36:43.760 --> 00:36:46.550
Android, running ROS, connected
to the cloud.

00:36:46.550 --> 00:36:48.610
So we're going to stick around
for some Q&amp;A, but I want to

00:36:48.610 --> 00:36:51.400
thank all of my presenters
today and the PR2

00:36:51.400 --> 00:36:52.650
for letting us demo.

00:37:01.440 --> 00:37:02.220
Great.

00:37:02.220 --> 00:37:05.360
So if anyone has questions,
please just come up to the mic

00:37:05.360 --> 00:37:07.590
and just speak up, and we're
happy to answer them.

00:37:20.170 --> 00:37:22.350
AUDIENCE: A quick
one on rosjava.

00:37:22.350 --> 00:37:24.555
Is it a pure java
implementation, or are there

00:37:24.555 --> 00:37:26.650
some native dependencies
on that?

00:37:26.650 --> 00:37:29.540
DAMON KOHLER: That's a pure java
implementation of ROS.

00:37:29.540 --> 00:37:30.540
AUDIENCE: Awesome.

00:37:30.540 --> 00:37:33.712
AUDIENCE: Any plans
on supporting

00:37:33.712 --> 00:37:34.850
other language findings?

00:37:34.850 --> 00:37:37.676
I know Damon started the
Scripting Layer for Android so

00:37:37.676 --> 00:37:40.580
you could run Python and
do stuff like that.

00:37:40.580 --> 00:37:42.910
DAMON KOHLER: ROS actually has
quite a few language findings.

00:37:42.910 --> 00:37:45.100
I think Ken could probably
rattle them off for you.

00:37:45.100 --> 00:37:48.990
KEN CONLEY: So the first class
ones are C++ and Python and

00:37:48.990 --> 00:37:53.651
LISP and we also have
experimental support for Lua.

00:37:53.651 --> 00:37:55.956
AUDIENCE: I mean on the
Android itself.

00:37:55.956 --> 00:37:59.210
DAMON KOHLER: I'll be working
on rosjava for

00:37:59.210 --> 00:38:00.460
the foreseeable future.

00:38:03.620 --> 00:38:09.970
AUDIENCE: With the realm of
unmanned ground vehicles, how

00:38:09.970 --> 00:38:13.845
would you compare or contrast
you guys' direction in terms

00:38:13.845 --> 00:38:17.661
of ROS with your Northrup
Grummans, your Lockheeds, that

00:38:17.661 --> 00:38:21.706
are working on that problem?

00:38:21.706 --> 00:38:23.960
Are there some kind of cloud
services that could be

00:38:23.960 --> 00:38:25.980
specifically useful for that?

00:38:25.980 --> 00:38:27.740
RYAN HICKMAN: Yeah, I definitely
think cloud

00:38:27.740 --> 00:38:30.120
services are useful to
robots as a whole.

00:38:30.120 --> 00:38:33.830
And what we've seen is military
developments in the

00:38:33.830 --> 00:38:36.670
past turn commercial, whether
it was satellites or

00:38:36.670 --> 00:38:38.530
going to the moon.

00:38:38.530 --> 00:38:40.920
I think it's GPS--

00:38:40.920 --> 00:38:43.170
inertia measurement
units on missiles

00:38:43.170 --> 00:38:44.610
are now in your Wiimote.

00:38:44.610 --> 00:38:47.440
And it's a $10 chip, and
it used to be $60,000.

00:38:47.440 --> 00:38:50.880
So all of those things are
turning into commercial,

00:38:50.880 --> 00:38:53.750
low-cost devices that all
of us can then afford.

00:38:53.750 --> 00:38:56.690
And what's happening is more
and more of the basics of

00:38:56.690 --> 00:38:59.440
robotics are just being
taken care of for you.

00:38:59.440 --> 00:39:03.470
And then new people are tackling
the hard problems as

00:39:03.470 --> 00:39:05.700
they come about.

00:39:05.700 --> 00:39:09.550
AUDIENCE: So, it seems a natural
question, have you

00:39:09.550 --> 00:39:14.410
talked to Commander Pike about
using Go in this environment?

00:39:14.410 --> 00:39:17.538
I'm sure he'd be disappointed
if the answer is no.

00:39:17.538 --> 00:39:19.190
RYAN HICKMAN: I have
not conversations

00:39:19.190 --> 00:39:20.430
with Commander Pike.

00:39:20.430 --> 00:39:26.992
AUDIENCE: Yeah, because it seems
like Go would be a very

00:39:26.992 --> 00:39:31.050
natural fit, where you could
delegate to an army robot, and

00:39:31.050 --> 00:39:37.791
they work it out amongst
themselves, who does what.

00:39:37.791 --> 00:39:39.826
RYAN HICKMAN: A great
contribution for somebody who

00:39:39.826 --> 00:39:42.100
knows Go and is interested in
robots would be to write the

00:39:42.100 --> 00:39:44.458
first Go client for ROS.

00:39:44.458 --> 00:39:48.675
AUDIENCE: If only Go had
a way to talk to Java.

00:39:48.675 --> 00:39:51.500
RYAN HICKMAN: All right.

00:39:51.500 --> 00:39:53.920
Any more questions?

00:39:53.920 --> 00:39:54.890
No?

00:39:54.890 --> 00:39:56.780
Well, thank you all
for coming.

