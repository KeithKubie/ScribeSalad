WEBVTT
Kind: captions
Language: en

00:00:00.820 --> 00:00:01.040
JU-KAY KWEK: OK.

00:00:01.040 --> 00:00:04.140
I think we're ready to go.

00:00:04.140 --> 00:00:05.150
Good morning, everybody.

00:00:05.150 --> 00:00:08.130
This is session 315.

00:00:08.130 --> 00:00:08.990
As you can read from
the title, it's

00:00:08.990 --> 00:00:11.000
about big data mashups.

00:00:11.000 --> 00:00:12.490
My name is Ju-kay Kwek.

00:00:12.490 --> 00:00:15.050
I'm in product management at
Google, and I lead the

00:00:15.050 --> 00:00:17.240
BigQuery team.

00:00:17.240 --> 00:00:20.510
For those of you who are a
little less familiar, just a

00:00:20.510 --> 00:00:21.970
quick introduction.

00:00:21.970 --> 00:00:26.030
BigQuery is a service that we
launched about a year ago.

00:00:26.030 --> 00:00:28.670
It's a fully managed analytics
service in the cloud.

00:00:28.670 --> 00:00:31.810
It lets you ingest
a lot of data--

00:00:31.810 --> 00:00:34.630
multiple terabytes, hundreds
of terabytes--

00:00:34.630 --> 00:00:37.310
interactively analyze that
data using the power of

00:00:37.310 --> 00:00:40.730
Google's data centers, and
then securely share the

00:00:40.730 --> 00:00:43.750
results, whether with your BI
tools, with the rest of your

00:00:43.750 --> 00:00:45.840
team, and so forth.

00:00:45.840 --> 00:00:49.110
Like I said, we launched the
service about a year ago, and

00:00:49.110 --> 00:00:51.530
for my team, one of the most
exciting and gratifying things

00:00:51.530 --> 00:00:54.640
has been working with some
leading customers.

00:00:54.640 --> 00:00:58.160
Because we know the use cases
that this technology has been

00:00:58.160 --> 00:01:00.970
used for inside of Google
in production for the

00:01:00.970 --> 00:01:02.440
last couple of years.

00:01:02.440 --> 00:01:06.420
And now that we've externalized
significant parts

00:01:06.420 --> 00:01:10.510
of it, the functionality, we've
really been excited to

00:01:10.510 --> 00:01:13.260
see, what are people going to
do with the technology?

00:01:13.260 --> 00:01:17.060
And we would be here a long time
if I could list out all

00:01:17.060 --> 00:01:19.970
of the use cases, but I thought
today we would talk

00:01:19.970 --> 00:01:21.430
about just one.

00:01:21.430 --> 00:01:24.530
And so it's with great pleasure
that today we have

00:01:24.530 --> 00:01:28.300
with us Derek Stevenson
from Shutterfly.

00:01:28.300 --> 00:01:31.550
Shutterfly was one of
our early customers.

00:01:31.550 --> 00:01:33.440
They have a great use case we're
going to talk about.

00:01:33.440 --> 00:01:36.620
And just for context, Derek
works on Shutterfly's

00:01:36.620 --> 00:01:37.860
analytics team.

00:01:37.860 --> 00:01:41.220
He leads the team responsible
for big data strategy and

00:01:41.220 --> 00:01:42.560
analytics technologies.

00:01:42.560 --> 00:01:45.800
So we've worked very closely
with them on what you'll see

00:01:45.800 --> 00:01:48.210
are some really gnarly
problems.

00:01:48.210 --> 00:01:50.970
Just a quick run through
of the agenda.

00:01:50.970 --> 00:01:52.800
We're just going to do a little
bit of introduction

00:01:52.800 --> 00:01:56.790
into what Shutterfly does, who
the customers are, why

00:01:56.790 --> 00:02:00.150
analytics is really, really
important for a business like

00:02:00.150 --> 00:02:06.190
Shutterfly, and walk through
what they did with BigQuery in

00:02:06.190 --> 00:02:09.580
terms of integrating it into
their big data and analytics

00:02:09.580 --> 00:02:10.840
infrastructure.

00:02:10.840 --> 00:02:13.880
We'll go through a couple of
examples down at the code

00:02:13.880 --> 00:02:17.760
level so that people get a good
idea about how we can

00:02:17.760 --> 00:02:22.490
actually, in a very practical
way, get things like data

00:02:22.490 --> 00:02:24.870
ingestion in the tooling set
up running very quickly.

00:02:24.870 --> 00:02:27.480
We'll run through a couple of
query examples, and I think

00:02:27.480 --> 00:02:30.100
you'll see the results
are quite fun.

00:02:30.100 --> 00:02:32.250
And then we'll back out, and
we'll do about 10 minutes of

00:02:32.250 --> 00:02:34.580
Q&amp;A at the very end.

00:02:34.580 --> 00:02:37.430
So with that, I'm going to hand
over to Derek, and he's

00:02:37.430 --> 00:02:39.740
going to walk us through
today's presentation.

00:02:39.740 --> 00:02:41.390
Derek?

00:02:41.390 --> 00:02:43.740
DEREK STEVENSON:
Thanks, Ju-kay.

00:02:43.740 --> 00:02:45.750
Thank you all for being
here today.

00:02:45.750 --> 00:02:50.690
I'm excited to share some of
the learnings we came to

00:02:50.690 --> 00:02:53.260
through our use of BigQuery.

00:02:53.260 --> 00:02:56.160
As Ju-kay mentioned, we'll
outline some of the challenges

00:02:56.160 --> 00:03:00.540
that we face with big data, what
it means to us, because

00:03:00.540 --> 00:03:05.440
as many of you know, "big"
is always relative.

00:03:05.440 --> 00:03:08.220
A quick introduction to
BigQuery, for those of you who

00:03:08.220 --> 00:03:09.700
are not familiar with it.

00:03:09.700 --> 00:03:12.640
I'll also talk about our
solution architecture, and as

00:03:12.640 --> 00:03:15.350
Ju-kay mentioned, how it fits
into that broader picture.

00:03:15.350 --> 00:03:18.060
Talk about some specific
examples from an analytic

00:03:18.060 --> 00:03:22.070
standpoint that will hopefully
spark some ideas for how you

00:03:22.070 --> 00:03:24.690
can apply it in your
own environments.

00:03:24.690 --> 00:03:29.680
And of course, we'll wrap up
with conclusions and Q&amp;A.

00:03:29.680 --> 00:03:32.160
So I'd like to get a quick
show of hands.

00:03:32.160 --> 00:03:35.520
How many of you have smartphones
that you regularly

00:03:35.520 --> 00:03:38.130
take pictures with?

00:03:38.130 --> 00:03:39.670
OK, as expected.

00:03:39.670 --> 00:03:41.940
How many of you still
take pictures with a

00:03:41.940 --> 00:03:45.100
point-and-shoot camera?

00:03:45.100 --> 00:03:46.500
OK, a few less.

00:03:46.500 --> 00:03:49.316
And how about SLRs?

00:03:49.316 --> 00:03:50.120
OK, good.

00:03:50.120 --> 00:03:54.950
So we have, obviously, healthy
engagement with photography

00:03:54.950 --> 00:03:57.570
and the category.

00:03:57.570 --> 00:03:59.850
I'd like to tell you a little
bit about Shutterfly as

00:03:59.850 --> 00:04:01.230
background.

00:04:01.230 --> 00:04:03.580
Our vision is to make the
world a better place by

00:04:03.580 --> 00:04:06.120
helping people share
life's joy.

00:04:06.120 --> 00:04:11.240
And it's a pretty aspirational
vision, but we do that through

00:04:11.240 --> 00:04:15.700
the creation of photo products
such as photo books, cards,

00:04:15.700 --> 00:04:20.410
and stationery, photo gifts that
are meaningful and that

00:04:20.410 --> 00:04:25.570
often go hand-in-hand with
gift-giving occasions.

00:04:25.570 --> 00:04:28.790
Shutterfly has been
around since 1999.

00:04:28.790 --> 00:04:32.510
Last year, we had over $600
million in revenue, over 7

00:04:32.510 --> 00:04:34.760
million customers.

00:04:34.760 --> 00:04:37.990
And to give you an idea of the
scale of data that we're

00:04:37.990 --> 00:04:41.900
dealing with, just one of many
data points is, how many

00:04:41.900 --> 00:04:43.330
images do we deal with?

00:04:43.330 --> 00:04:48.160
And in our case, it's 19 billion
images, and growing

00:04:48.160 --> 00:04:50.850
faster every year.

00:04:50.850 --> 00:04:56.520
So I'd like to do a
quick pop quiz.

00:04:56.520 --> 00:05:00.930
Does anyone have any ideas
for what this might be?

00:05:00.930 --> 00:05:02.357
Go ahead and shout it out.

00:05:02.357 --> 00:05:03.700
AUDIENCE: DNA?

00:05:03.700 --> 00:05:05.180
DEREK STEVENSON: Yeah.

00:05:05.180 --> 00:05:08.330
When we were going through a
dress rehearsal, that was the

00:05:08.330 --> 00:05:11.080
number one guess on this.

00:05:11.080 --> 00:05:14.350
We've not quite perfected
sequencing our customers' DNA

00:05:14.350 --> 00:05:19.300
for customer marketing purposes,
but maybe next year.

00:05:19.300 --> 00:05:19.470
No.

00:05:19.470 --> 00:05:23.350
So how many of you are
Tableau users?

00:05:23.350 --> 00:05:24.800
I see a few hands out there.

00:05:24.800 --> 00:05:29.820
So this is a visualization to
demonstrate events that occur

00:05:29.820 --> 00:05:31.370
over a period of time.

00:05:31.370 --> 00:05:36.250
And what this is, there's about
4,000 marks on this

00:05:36.250 --> 00:05:40.900
graph, and each one of those
represents an image, a picture

00:05:40.900 --> 00:05:42.680
that was taken at a
particular time.

00:05:42.680 --> 00:05:47.740
It's all the images from my
account on Shutterfly, and it

00:05:47.740 --> 00:05:49.580
spans about 10 years,
and each row

00:05:49.580 --> 00:05:51.970
represents a discrete camera.

00:05:51.970 --> 00:05:56.010
And when I first pulled this
data, I was sure I'd done

00:05:56.010 --> 00:05:56.760
something wrong.

00:05:56.760 --> 00:05:58.140
I couldn't believe that
there were that

00:05:58.140 --> 00:06:00.610
many different cameras.

00:06:00.610 --> 00:06:04.490
It turns out, though, that it
does, in fact, represent all

00:06:04.490 --> 00:06:07.780
the different cameras that are
capturing memories that are

00:06:07.780 --> 00:06:10.570
meaningful to me and
commemorating

00:06:10.570 --> 00:06:12.320
a variety of occasions.

00:06:12.320 --> 00:06:17.510
And it, I think, speaks to
the explosion of digital

00:06:17.510 --> 00:06:22.740
technology and cameras and their
omnipresent status in

00:06:22.740 --> 00:06:27.170
our lives, but also speaks to
the connections between people

00:06:27.170 --> 00:06:31.560
and how images are so
meaningful to us.

00:06:31.560 --> 00:06:34.590
And you can see that the
data is not random.

00:06:34.590 --> 00:06:38.310
It commemorates, again,
occasions such as getting

00:06:38.310 --> 00:06:43.010
married, or having children, or
going on family vacations.

00:06:43.010 --> 00:06:46.420
The Stanford brainwashing has
begun, so if you have any

00:06:46.420 --> 00:06:48.170
questions about how
to do that, let me

00:06:48.170 --> 00:06:50.630
know after the talk.

00:06:50.630 --> 00:06:52.630
So this is great stuff.

00:06:52.630 --> 00:06:54.440
We've done a little
mini-analysis and

00:06:54.440 --> 00:06:55.915
visualization on a data set.

00:06:58.950 --> 00:07:00.330
What's the fuss?

00:07:00.330 --> 00:07:03.930
Why is this a big deal?

00:07:03.930 --> 00:07:06.350
Well, insight requires
interpretation, and we

00:07:06.350 --> 00:07:08.920
happened to look at one specific
account, my account,

00:07:08.920 --> 00:07:11.360
and a specific set of images.

00:07:11.360 --> 00:07:14.540
But how do you scale that sort
of analysis to seven million

00:07:14.540 --> 00:07:19.170
customers, to 19 billion
images and growing?

00:07:19.170 --> 00:07:23.100
And how do you do it in a way
that is relevant and specific

00:07:23.100 --> 00:07:25.080
to our particular business?

00:07:25.080 --> 00:07:27.840
So one thing about Shutterfly
that's worth noting is we're a

00:07:27.840 --> 00:07:29.600
very seasonal business.

00:07:29.600 --> 00:07:33.940
About 50% of our revenue comes
in the fourth quarter.

00:07:33.940 --> 00:07:36.440
So there's this compression
where we're making decisions

00:07:36.440 --> 00:07:39.420
on a daily and sometimes
hourly basis.

00:07:39.420 --> 00:07:43.450
And to do that effectively, to
derive insights and respond to

00:07:43.450 --> 00:07:45.490
key business questions depending
on what customers

00:07:45.490 --> 00:07:50.080
are doing, you need to be able
to do that very quickly.

00:07:50.080 --> 00:07:53.890
So in order to tackle some of
those issues, oftentimes more

00:07:53.890 --> 00:07:58.760
traditional approaches require
certain compromises around how

00:07:58.760 --> 00:08:01.130
you deal with data.

00:08:01.130 --> 00:08:03.780
You might do sampling, you might
pre-aggregate certain

00:08:03.780 --> 00:08:07.440
results, and as a result, you
drop out some detail.

00:08:07.440 --> 00:08:14.100
So what sometimes happens is
you're limited to a subset of

00:08:14.100 --> 00:08:16.850
your customers, or a level of
detail with your customers,

00:08:16.850 --> 00:08:20.480
that doesn't allow the
inevitable follow-up questions

00:08:20.480 --> 00:08:23.420
that the business
is going to ask.

00:08:23.420 --> 00:08:28.220
So what we want to try to do
here is tackle this carefully

00:08:28.220 --> 00:08:30.370
and retain all of that
level of detail.

00:08:30.370 --> 00:08:33.730
I think that's ultimately what
a lot of business owners are

00:08:33.730 --> 00:08:37.720
after, is they want to have all
of that data available to

00:08:37.720 --> 00:08:39.940
them at any time.

00:08:39.940 --> 00:08:44.560
And yet when we tackle how to
solve those problems, we want

00:08:44.560 --> 00:08:48.570
to do it in a way that is
adhering to key constraints,

00:08:48.570 --> 00:08:52.320
key considerations that we
have as a business around

00:08:52.320 --> 00:08:53.350
scale and performance.

00:08:53.350 --> 00:08:56.490
So obviously, as I mentioned,
it's got to be fast, and it's

00:08:56.490 --> 00:08:59.300
got to be, in some regard,
unlimited in how much data

00:08:59.300 --> 00:09:02.130
you're going to be able
to put into it.

00:09:02.130 --> 00:09:04.790
Also, we're trying to be
very cost-conscious.

00:09:04.790 --> 00:09:07.265
So there's obviously ways
to tackle this--

00:09:09.810 --> 00:09:12.880
if you have a huge, unlimited
budget, there are technologies

00:09:12.880 --> 00:09:13.440
that are out there.

00:09:13.440 --> 00:09:17.750
But with all the innovation in
the space, we believe there

00:09:17.750 --> 00:09:20.830
are more cost-effective ways
to do some of this.

00:09:20.830 --> 00:09:22.460
And the last is simplicity.

00:09:22.460 --> 00:09:26.090
And what I mean by simplicity,
it's not just simplicity for

00:09:26.090 --> 00:09:28.780
implementation, how do you stand
this environment up and

00:09:28.780 --> 00:09:31.980
get running with it, but
also for adoption.

00:09:31.980 --> 00:09:36.030
And what that means is, are
analysts comfortable using the

00:09:36.030 --> 00:09:39.290
technology in a way that is
relevant to them and that

00:09:39.290 --> 00:09:43.310
doesn't impose a huge
learning curve?

00:09:43.310 --> 00:09:46.730
So at Shutterfly, I've been
there almost nine years now,

00:09:46.730 --> 00:09:49.820
and from day one, data has
always been seen as a

00:09:49.820 --> 00:09:51.630
competitive advantage.

00:09:51.630 --> 00:09:55.250
And we as analysts are
responsible for focusing our

00:09:55.250 --> 00:09:57.650
analytics efforts on
actual analytics.

00:09:57.650 --> 00:10:00.960
What that means is, how do we
focus the attention at any

00:10:00.960 --> 00:10:05.700
given time on a given business
problem to the right data set?

00:10:05.700 --> 00:10:11.740
Not all of your data at any
given time, but for any given

00:10:11.740 --> 00:10:13.780
business question you might
try to be answering-- the

00:10:13.780 --> 00:10:15.440
questions that we answer in the
fourth quarter are very

00:10:15.440 --> 00:10:19.140
different than those we answer
at other times of the year.

00:10:19.140 --> 00:10:21.750
And we do that with a focus
on customer value.

00:10:21.750 --> 00:10:24.220
How do we derive increased
customer value

00:10:24.220 --> 00:10:26.400
through those insights?

00:10:26.400 --> 00:10:29.720
We strongly believe in owning
our own data and methodology

00:10:29.720 --> 00:10:34.940
and not relying on a third
party, if you will, to

00:10:34.940 --> 00:10:36.400
accomplish that.

00:10:36.400 --> 00:10:39.010
And we have an experienced
analytics team that helps us

00:10:39.010 --> 00:10:42.570
use a variety of techniques
to get there.

00:10:42.570 --> 00:10:44.210
For us, big data
is everywhere.

00:10:44.210 --> 00:10:47.520
There's a wide variety
of applications.

00:10:47.520 --> 00:10:53.830
And this is not an exhaustive
list, but it gives you,

00:10:53.830 --> 00:10:55.550
hopefully, some ideas
of how it might

00:10:55.550 --> 00:10:58.130
apply to your business.

00:10:58.130 --> 00:11:01.270
I'll talk a little bit more
about the first couple

00:11:01.270 --> 00:11:03.500
marketing mix models and
multichannel attributions.

00:11:03.500 --> 00:11:07.990
So some of you may be familiar
with this, but the basic idea

00:11:07.990 --> 00:11:11.110
is that marketing often has a
particular budget with which

00:11:11.110 --> 00:11:14.040
they can invest in their
marketing channels.

00:11:14.040 --> 00:11:18.190
And one common question that
we have is where should I

00:11:18.190 --> 00:11:19.290
spend that dollar?

00:11:19.290 --> 00:11:22.390
What's going to get me
the biggest return?

00:11:22.390 --> 00:11:26.090
And it's a complex question to
answer, depending on the time

00:11:26.090 --> 00:11:29.010
of year, depending on the
marketing channels

00:11:29.010 --> 00:11:30.650
available to you.

00:11:30.650 --> 00:11:33.410
So we're going to talk a little
bit more about that and

00:11:33.410 --> 00:11:37.830
take this to hopefully a little
more specific example,

00:11:37.830 --> 00:11:41.600
so you can perhaps draw some
connections to how you might

00:11:41.600 --> 00:11:42.380
apply it yourselves.

00:11:42.380 --> 00:11:47.170
So in this case, this is a
simplified view on it, but you

00:11:47.170 --> 00:11:49.340
can get an idea of the
scale of the problem.

00:11:49.340 --> 00:11:53.330
So we've got user data that's
in the millions of records,

00:11:53.330 --> 00:11:56.180
user attributes that
go with that.

00:11:56.180 --> 00:11:58.710
Marketing emails, when you're
thinking about all the emails

00:11:58.710 --> 00:12:00.840
you send from a marketing
standpoint over a couple

00:12:00.840 --> 00:12:03.810
years, when you're doing
year-over-year analysis, for

00:12:03.810 --> 00:12:07.830
example, you can see how that
would quickly scale up into

00:12:07.830 --> 00:12:08.700
the hundreds of millions.

00:12:08.700 --> 00:12:13.110
And that starts to strain the
limits of our more traditional

00:12:13.110 --> 00:12:15.340
approaches to dealing
with data.

00:12:15.340 --> 00:12:17.690
The third is web traffic, and
that's really where the

00:12:17.690 --> 00:12:18.910
problem presents itself.

00:12:18.910 --> 00:12:21.980
You're in the billions of rows,
and in some cases, you

00:12:21.980 --> 00:12:24.570
have hundreds of columns of
attributes that you're

00:12:24.570 --> 00:12:25.850
collecting.

00:12:25.850 --> 00:12:28.090
And when you try to
cross-reference these data

00:12:28.090 --> 00:12:30.760
sets, it starts to break
down when you get into

00:12:30.760 --> 00:12:33.380
that level of detail.

00:12:33.380 --> 00:12:35.410
There's a variety of other
domains of data that this

00:12:35.410 --> 00:12:36.300
applies to.

00:12:36.300 --> 00:12:40.410
We already talked earlier
about image data.

00:12:40.410 --> 00:12:44.020
There's web traffic data, social
and mobile are entirely

00:12:44.020 --> 00:12:46.370
new categories of data that have
exploded over the last

00:12:46.370 --> 00:12:47.670
couple of years.

00:12:47.670 --> 00:12:52.550
So there's a variety
of applications.

00:12:52.550 --> 00:12:56.400
So this hopefully looks pretty
familiar to many of you who

00:12:56.400 --> 00:12:58.710
are dealing with data
warehousing and

00:12:58.710 --> 00:13:01.590
data ingestion issues.

00:13:01.590 --> 00:13:05.060
We're collecting data, storing
it, preparing it for analysis,

00:13:05.060 --> 00:13:08.620
using a variety of tools to
dig into the data, and

00:13:08.620 --> 00:13:11.410
ultimately it's with a focus
on acting on the data.

00:13:11.410 --> 00:13:15.670
What decisions can you make as
a result of your insights?

00:13:15.670 --> 00:13:19.940
Traditionally, the
responsibilities around

00:13:19.940 --> 00:13:24.950
collection, storage, preparation
have been the

00:13:24.950 --> 00:13:25.820
domain of IT.

00:13:25.820 --> 00:13:28.520
So I wanted to get a quick
sense-- how many of you are

00:13:28.520 --> 00:13:32.190
really on the IT side?

00:13:32.190 --> 00:13:34.920
And how about on the other
side, analytics and more

00:13:34.920 --> 00:13:37.140
business-oriented?

00:13:37.140 --> 00:13:41.040
So what I want to try to do is
paint a picture that hopefully

00:13:41.040 --> 00:13:44.160
for those of you on the business
side have a sense of

00:13:44.160 --> 00:13:47.940
what IT's concerns are and vice
versa, and where we're

00:13:47.940 --> 00:13:51.000
trying to go with this.

00:13:51.000 --> 00:13:55.030
When you introduce an entirely
new category of data that is

00:13:55.030 --> 00:13:57.040
much larger, perhaps, than
your systems have been

00:13:57.040 --> 00:14:02.280
designed for, you run into a
number of issues, and IT

00:14:02.280 --> 00:14:04.430
typically bears the
brunt of that.

00:14:04.430 --> 00:14:08.070
How does IT deal with entirely
new categories of data, and

00:14:08.070 --> 00:14:13.370
entirely new scales of data that
the analysts and business

00:14:13.370 --> 00:14:16.360
owners are looking
to leverage?

00:14:16.360 --> 00:14:22.640
And where we want to go with
this is, how do we work around

00:14:22.640 --> 00:14:23.520
some of those constraints?

00:14:23.520 --> 00:14:26.430
How do we start to enable some
of those pipelines in a more

00:14:26.430 --> 00:14:28.500
efficient manner?

00:14:28.500 --> 00:14:30.380
When you step back,
this is really a

00:14:30.380 --> 00:14:32.670
multi-dimensional problem.

00:14:32.670 --> 00:14:33.770
You've probably heard the terms

00:14:33.770 --> 00:14:36.620
volume, variety, velocity.

00:14:36.620 --> 00:14:39.810
We looked at some examples of
the dimensions you might be

00:14:39.810 --> 00:14:43.200
looking at-- users, email,
traffic, web traffic.

00:14:43.200 --> 00:14:45.590
And you might have aggregated
some of that data.

00:14:45.590 --> 00:14:49.110
But as you know, as soon as an
analyst looks at the data and

00:14:49.110 --> 00:14:51.160
understands something in that
aggregate, they're going to

00:14:51.160 --> 00:14:52.960
say, well, what's the next
level of detail?

00:14:52.960 --> 00:14:55.850
So you might be missing out on
some of the connections from

00:14:55.850 --> 00:14:58.280
that more detailed data.

00:14:58.280 --> 00:15:01.530
You might also through, for
example, mergers and

00:15:01.530 --> 00:15:04.250
acquisitions, acquire
a new company.

00:15:04.250 --> 00:15:09.950
And all of a sudden, it's your
job as an IT person to deal

00:15:09.950 --> 00:15:11.910
with this entirely new category
of data that your

00:15:11.910 --> 00:15:14.080
systems were never
designed for.

00:15:14.080 --> 00:15:17.570
So how do you adopt certain
strategies to tackle that

00:15:17.570 --> 00:15:19.960
effectively?

00:15:19.960 --> 00:15:22.380
What I think probably many of
you in the room are familiar

00:15:22.380 --> 00:15:26.160
with is when you talk to the
business, everyone just wants

00:15:26.160 --> 00:15:30.520
all the data at any given level
of detail, and they

00:15:30.520 --> 00:15:34.740
don't necessarily appreciate
some of the trade-offs.

00:15:34.740 --> 00:15:39.180
So how do we fill that
gap with BigQuery?

00:15:39.180 --> 00:15:43.620
A quick intro, or
resummarization, of BigQuery--

00:15:43.620 --> 00:15:46.520
no limits on the volume of data
that you can put into the

00:15:46.520 --> 00:15:50.390
environment, extremely scalable
and fast performance

00:15:50.390 --> 00:15:54.560
on queries, a variety of ways of
interacting with it via SQL

00:15:54.560 --> 00:15:59.930
as well as APIs, and then
interoperability with existing

00:15:59.930 --> 00:16:05.630
tools such as Tableau,
Excel, and so forth.

00:16:05.630 --> 00:16:10.290
So what we're striving for is
to go from this situation to

00:16:10.290 --> 00:16:12.840
one more like this, where we're

00:16:12.840 --> 00:16:14.060
doing a couple of things.

00:16:14.060 --> 00:16:17.100
First, we're empowering analysts
to push back further

00:16:17.100 --> 00:16:22.420
in that pipeline of data
analysis and processing and

00:16:22.420 --> 00:16:25.430
enabling them without having to
reformulate the data into

00:16:25.430 --> 00:16:30.450
structures that might be better
suited for analysis,

00:16:30.450 --> 00:16:32.960
but as we talked about earlier,
bring certain

00:16:32.960 --> 00:16:35.340
compromises.

00:16:35.340 --> 00:16:37.310
One of the other things to note
here is that we're not

00:16:37.310 --> 00:16:40.100
ripping out the old way of doing
things, necessarily.

00:16:40.100 --> 00:16:43.170
We're finding complementary
ways to use them so that

00:16:43.170 --> 00:16:46.700
BigQuery, at least at this
stage, is enabling us to deal

00:16:46.700 --> 00:16:49.370
with these categories of data
that we otherwise have not

00:16:49.370 --> 00:16:51.800
been able to deal with.

00:16:51.800 --> 00:16:56.740
The other thing to notice is
that this isn't minimizing the

00:16:56.740 --> 00:16:57.770
role of IT.

00:16:57.770 --> 00:17:01.830
In fact, what we hope this
achieves is more efficiency

00:17:01.830 --> 00:17:04.960
for IT, where they can spend
more time on the hard problems

00:17:04.960 --> 00:17:08.640
that they're uniquely suited to
solve around collection and

00:17:08.640 --> 00:17:12.650
management of these categories
of data that are bigger than

00:17:12.650 --> 00:17:15.069
ever before.

00:17:15.069 --> 00:17:18.339
If they don't have to spend
their time preparing data and

00:17:18.339 --> 00:17:23.380
aggregating and building cubes,
I've been on the IT

00:17:23.380 --> 00:17:25.430
side, and I can tell you
that's probably not the

00:17:25.430 --> 00:17:27.890
funnest part of the job.

00:17:27.890 --> 00:17:31.110
So we really think this enables
new insights and lets

00:17:31.110 --> 00:17:37.230
you access all levels of detail,
web scale data, and

00:17:37.230 --> 00:17:40.330
connect it with all of your
other transactional data.

00:17:40.330 --> 00:17:42.810
And we firmly believe that this
is a critical element in

00:17:42.810 --> 00:17:47.600
how we inform the product and
marketing strategies and

00:17:47.600 --> 00:17:49.300
ultimately hopefully
bring a better

00:17:49.300 --> 00:17:51.650
experience to our customers.

00:17:51.650 --> 00:17:54.480
There's a whole host of ways in
which our customers use our

00:17:54.480 --> 00:17:58.110
products, and many are very
innovative, and we want to

00:17:58.110 --> 00:18:02.180
continue to support
those needs.

00:18:02.180 --> 00:18:05.320
If you're like my son, at this
point you're thinking, let's

00:18:05.320 --> 00:18:05.920
get on with it.

00:18:05.920 --> 00:18:06.940
Let's see some real code.

00:18:06.940 --> 00:18:09.690
This is a developer conference,
after all.

00:18:09.690 --> 00:18:12.790
So I'll go into that in
some more detail.

00:18:12.790 --> 00:18:17.600
So taking that earlier ETL flow,
this is a use case that

00:18:17.600 --> 00:18:19.920
we applied with web
traffic data.

00:18:19.920 --> 00:18:23.470
We took our web hit data, we
appended some user data to

00:18:23.470 --> 00:18:27.930
this, we also appended
some categorization

00:18:27.930 --> 00:18:29.780
data for the website.

00:18:29.780 --> 00:18:33.720
In other words, how do you divvy
up your site in a way

00:18:33.720 --> 00:18:37.540
that makes sense from a
business standpoint?

00:18:37.540 --> 00:18:42.180
And then output it to flat files
and use the utilities

00:18:42.180 --> 00:18:45.910
that are provided with Google
Cloud Storage and BigQuery to

00:18:45.910 --> 00:18:49.780
import it into BigQuery
itself.

00:18:49.780 --> 00:18:53.250
One comment I want to make here
is notice that we're,

00:18:53.250 --> 00:18:56.780
through this ETL process,
denormalizing data, which is

00:18:56.780 --> 00:19:01.310
generally a best practice for
a platform like BigQuery.

00:19:01.310 --> 00:19:04.970
But as you'll see later on, it's
not always critical to do

00:19:04.970 --> 00:19:09.020
this, and in fact, it may not be
practical to anticipate all

00:19:09.020 --> 00:19:12.770
ways in which you want to
denormalize your data.

00:19:12.770 --> 00:19:16.700
So this is a shell script we
put together to do the

00:19:16.700 --> 00:19:18.640
transfer to BigQuery.

00:19:18.640 --> 00:19:21.930
One thing I should mention is
that cloud storage goes

00:19:21.930 --> 00:19:25.460
hand-in-hand with BigQuery, so
as an initial step, we're

00:19:25.460 --> 00:19:29.740
transferring data into that
environment and using it as a

00:19:29.740 --> 00:19:33.600
staging area for import.

00:19:33.600 --> 00:19:35.990
First thing we do is
define the bucket.

00:19:35.990 --> 00:19:40.320
So this is, if you will, the
file path within cloud storage

00:19:40.320 --> 00:19:43.480
to which we're going to
transfer the data.

00:19:43.480 --> 00:19:48.410
Within cloud storage, we copy
from our local file system

00:19:48.410 --> 00:19:49.640
into that environment.

00:19:49.640 --> 00:19:53.350
And in our example, we had this
all running on the same

00:19:53.350 --> 00:19:57.630
ETL system that was generating
the output, the CSVs.

00:19:57.630 --> 00:20:04.110
And then we call the actual
BigQuery client library to

00:20:04.110 --> 00:20:07.740
load the data into our project
with some command line

00:20:07.740 --> 00:20:11.310
parameters around how the
file is formatted.

00:20:11.310 --> 00:20:13.690
And the thing to note here
is the notation.

00:20:13.690 --> 00:20:18.070
I guess Shutterfly.hitdata, in
the relational role, would be

00:20:18.070 --> 00:20:20.520
sort of the schema.tablename,
if you will.

00:20:23.680 --> 00:20:26.580
So here's an example of a
query that we ran, then.

00:20:26.580 --> 00:20:28.430
Once the data is in there,
what were we

00:20:28.430 --> 00:20:29.880
able to do with it?

00:20:29.880 --> 00:20:34.590
As you can see from the syntax,
it's very familiar to

00:20:34.590 --> 00:20:37.270
anyone who has done
a lot of SQL.

00:20:37.270 --> 00:20:38.980
And that's really
important to us.

00:20:38.980 --> 00:20:42.480
We have an analyst team that
understands SQL quite well,

00:20:42.480 --> 00:20:46.780
and uses it as a means to do
a lot of ad hoc analysis.

00:20:46.780 --> 00:20:51.420
And so having that familiarity
is great for an option.

00:20:51.420 --> 00:20:54.130
We're running this on 2 billion
rows, 400 columns,

00:20:54.130 --> 00:20:56.880
about 4 terabytes of data.

00:20:56.880 --> 00:20:59.820
Full customer and transactional
detail.

00:20:59.820 --> 00:21:03.640
And we ran a variety of
queries, many of which

00:21:03.640 --> 00:21:07.990
generally took somewhere between
20 to 60 seconds.

00:21:07.990 --> 00:21:09.800
We really didn't have to
do any data modeling.

00:21:09.800 --> 00:21:13.640
So we assemble it prior to
loading it, and once it's

00:21:13.640 --> 00:21:16.620
loaded, it's a large,
denormalized table.

00:21:16.620 --> 00:21:18.970
Don't have to build indexes,
don't have to break out data

00:21:18.970 --> 00:21:21.720
into separate columns,
necessarily.

00:21:21.720 --> 00:21:25.740
So very efficient from
that standpoint.

00:21:25.740 --> 00:21:28.180
In this particular example,
we're looking at the time

00:21:28.180 --> 00:21:31.930
spent on the part of the site,
so we're doing a self-join on

00:21:31.930 --> 00:21:34.330
that hit data, we're identifying
a population of

00:21:34.330 --> 00:21:39.520
customers and where they came
from in the first place, so

00:21:39.520 --> 00:21:40.870
those are the attributes
you see with

00:21:40.870 --> 00:21:43.400
channel ID and sub ID.

00:21:43.400 --> 00:21:46.790
For those that purchase on a
specific day, we're retaining

00:21:46.790 --> 00:21:49.120
that customer grain and joining
it back to the hit

00:21:49.120 --> 00:21:54.170
data table that contains other
traffic information.

00:21:54.170 --> 00:21:57.800
And in this example, we're
looking at their traffic

00:21:57.800 --> 00:22:00.550
patterns leading up
to that purchase.

00:22:00.550 --> 00:22:04.840
This particular query
ran in five seconds.

00:22:04.840 --> 00:22:09.220
So pretty impressive
performance.

00:22:09.220 --> 00:22:11.410
Another thing that's valuable
is Big Join.

00:22:11.410 --> 00:22:16.100
This is a feature that was
released not too long ago, and

00:22:16.100 --> 00:22:18.680
in this example, we're doing
the same sort of pattern.

00:22:18.680 --> 00:22:20.640
We're taking a population
of customers.

00:22:20.640 --> 00:22:22.420
This time, though, it's
defined outside of

00:22:22.420 --> 00:22:23.810
the hit data set.

00:22:23.810 --> 00:22:27.880
We're finding basically those
customers that were targeted

00:22:27.880 --> 00:22:31.000
in an email campaign and the
segment to which they were

00:22:31.000 --> 00:22:33.600
assigned, if we're doing, for
example, subject line testing,

00:22:33.600 --> 00:22:35.320
that sort of thing.

00:22:35.320 --> 00:22:37.860
We're taking that data set and,
through the Join Each

00:22:37.860 --> 00:22:41.020
clause, joining it back
to that hit data.

00:22:41.020 --> 00:22:46.210
So Join Each is the syntax for
big joins, and it's joining 10

00:22:46.210 --> 00:22:50.650
million rows on the subselect
with the original

00:22:50.650 --> 00:22:53.250
2 billion row table.

00:22:53.250 --> 00:22:56.880
In this case, Query came back
between five and six minutes,

00:22:56.880 --> 00:22:59.490
so very good performance for
the type of operation that

00:22:59.490 --> 00:23:00.740
we're doing.

00:23:03.180 --> 00:23:07.210
One thing I should mention, I
guess, is you can't always

00:23:07.210 --> 00:23:09.030
anticipate all the different
ways in which you're going to

00:23:09.030 --> 00:23:11.960
want to denormalize your data or
anticipate the connections

00:23:11.960 --> 00:23:14.190
that the business is going
to ask you to do.

00:23:14.190 --> 00:23:18.130
And so this is an example
where you can envision

00:23:18.130 --> 00:23:21.710
bringing in your own data set
that might be entirely

00:23:21.710 --> 00:23:24.540
different, but is still relevant
in that it defines a

00:23:24.540 --> 00:23:28.100
population of customers, and
then cross-referencing it with

00:23:28.100 --> 00:23:31.890
very large data sets, such
as your web data.

00:23:31.890 --> 00:23:34.980
Last example I'll go through
is around pattern matching.

00:23:34.980 --> 00:23:38.480
So if you come from a database
background, we've all been

00:23:38.480 --> 00:23:42.030
conditioned to never do likes
on strings, or do

00:23:42.030 --> 00:23:45.230
pattern-matching, that those
are expensive operations.

00:23:45.230 --> 00:23:49.810
And in this example, we're
identifying, again, a

00:23:49.810 --> 00:23:55.690
population of customers that
had done a specific event.

00:23:55.690 --> 00:23:58.590
And the way it's recorded in the
information is basically

00:23:58.590 --> 00:24:00.670
in a comma-delimited field.

00:24:00.670 --> 00:24:04.100
And we're doing a regular
expression match to find that

00:24:04.100 --> 00:24:09.180
event, and once identified,
again, via the user ID,

00:24:09.180 --> 00:24:10.870
joining it back to
the traffic.

00:24:10.870 --> 00:24:14.670
And this came back between
three and four minutes.

00:24:14.670 --> 00:24:17.410
So again, very fast performance
on regular

00:24:17.410 --> 00:24:19.830
expressions of strings that
might be in your data set,

00:24:19.830 --> 00:24:22.460
where perhaps it's not
structured when

00:24:22.460 --> 00:24:23.710
it first comes in.

00:24:26.850 --> 00:24:30.240
Last comment I want to go
through in terms of adoption

00:24:30.240 --> 00:24:31.930
is tool compatibility.

00:24:31.930 --> 00:24:34.030
So we've been using Tableau
for a while.

00:24:34.030 --> 00:24:36.980
It's a very powerful tool
for visualization.

00:24:36.980 --> 00:24:41.880
And I wanted to show a couple
of examples of things that

00:24:41.880 --> 00:24:45.090
we've done where if you connect
Tableau with these

00:24:45.090 --> 00:24:48.030
huge data sets, you can
visualize them in a really

00:24:48.030 --> 00:24:49.065
interesting way.

00:24:49.065 --> 00:24:52.720
So we've taken a cohort of
images uploaded on a certain

00:24:52.720 --> 00:24:55.010
day and looked at the time
between when they were

00:24:55.010 --> 00:24:57.940
uploaded and when they were
first taken, and you can see

00:24:57.940 --> 00:24:59.685
some really interesting
patterns jump out.

00:25:02.640 --> 00:25:06.790
Another example is looking at
when the pictures were taken

00:25:06.790 --> 00:25:10.650
at different times of day, and
discerning patterns around how

00:25:10.650 --> 00:25:14.460
customers are behaving with
their cameras, and how that

00:25:14.460 --> 00:25:17.120
might translate to
product creation.

00:25:17.120 --> 00:25:20.470
So that compatibility with
Tableau, and there are other

00:25:20.470 --> 00:25:23.020
tools that BigQuery is
integrated with, and I think

00:25:23.020 --> 00:25:25.880
that list continues to expand,
but for us it's really

00:25:25.880 --> 00:25:30.470
powerful way to get a lot of
summarized data, visualized

00:25:30.470 --> 00:25:33.830
data, out of huge data sets.

00:25:33.830 --> 00:25:36.180
So conclusions and learnings.

00:25:36.180 --> 00:25:39.960
We think this enables new
categories of analysis, and

00:25:39.960 --> 00:25:41.640
for us it's retaining
this customer grain.

00:25:41.640 --> 00:25:42.650
So that's an important,
I think,

00:25:42.650 --> 00:25:43.740
concept to keep in mind.

00:25:43.740 --> 00:25:47.020
As analysts, and in particular
in our group, we're often

00:25:47.020 --> 00:25:51.080
thinking about customer
behavior, and we need that

00:25:51.080 --> 00:25:52.450
level of detail.

00:25:52.450 --> 00:25:54.980
We think it'll grow with
the business, so it's a

00:25:54.980 --> 00:25:56.620
pay-as-you-go model.

00:25:56.620 --> 00:26:01.450
You're not locked into a huge
investment up front.

00:26:01.450 --> 00:26:04.420
We also think that this really
empowers the analyst to be

00:26:04.420 --> 00:26:08.310
more agile and have
more iterations.

00:26:08.310 --> 00:26:12.430
And it improves the efficiency
of IT, as well.

00:26:12.430 --> 00:26:15.130
Ultimately it supports our focus
on actionable analytics,

00:26:15.130 --> 00:26:17.880
finding the right data
at the right time.

00:26:17.880 --> 00:26:20.320
And we think there is a wide
variety of other areas we can

00:26:20.320 --> 00:26:21.020
apply this to.

00:26:21.020 --> 00:26:23.700
And as I mentioned, this is not
an exhaustive list, but

00:26:23.700 --> 00:26:26.160
are some of the areas
that we expect to be

00:26:26.160 --> 00:26:28.050
able to leverage this.

00:26:28.050 --> 00:26:31.840
We will continue to monitor the
rapid innovation in this

00:26:31.840 --> 00:26:32.630
space as well.

00:26:32.630 --> 00:26:38.020
And as you probably are very
well aware of, there's a

00:26:38.020 --> 00:26:40.080
tremendous amount of
activity on how to

00:26:40.080 --> 00:26:42.700
leverage big data sets.

00:26:42.700 --> 00:26:46.290
And we'll continue to look at
those right up until Google

00:26:46.290 --> 00:26:47.540
decides to buy them.

00:26:51.190 --> 00:26:54.630
Just to wrap up, we think we're
tackling some pretty

00:26:54.630 --> 00:26:57.790
interesting and complex
analytics.

00:26:57.790 --> 00:27:01.100
We are doing it in a way that
is thoughtful, and we're

00:27:01.100 --> 00:27:04.530
evaluating a lot of ways to do
that, a lot of interesting

00:27:04.530 --> 00:27:05.080
techniques.

00:27:05.080 --> 00:27:09.070
And we think BigQuery is
a novel way to do that.

00:27:09.070 --> 00:27:11.150
One of the things that's really
important to analysts

00:27:11.150 --> 00:27:14.670
is knowing that their analytics
and insights are

00:27:14.670 --> 00:27:17.720
being used, in other words,
that there's some decision

00:27:17.720 --> 00:27:21.080
being driven out of some new
insight that you have.

00:27:21.080 --> 00:27:25.000
And we think we're doing this
in a way that is really

00:27:25.000 --> 00:27:29.760
meaningful and insightful,
and we continue to grow.

00:27:29.760 --> 00:27:31.750
We've been very fortunate
in our growth.

00:27:31.750 --> 00:27:35.380
And hopefully, some of what
we've gone through today has

00:27:35.380 --> 00:27:37.100
sparked your interest.

00:27:37.100 --> 00:27:43.600
We, as I mentioned, are always
on the lookout for talented

00:27:43.600 --> 00:27:46.200
engineers and analysts that
can help us solve some of

00:27:46.200 --> 00:27:47.690
these problems.

00:27:47.690 --> 00:27:50.670
So come see me afterwards
if any of this is

00:27:50.670 --> 00:27:52.270
of interest to you.

00:27:52.270 --> 00:27:53.500
Thank you very much.

00:27:53.500 --> 00:27:55.230
I'll turn it back
over to Ju-kay.

00:27:55.230 --> 00:28:01.630
[APPLAUSE]

00:28:01.630 --> 00:28:03.640
JU-KAY KWEK: Thank you
very much, Derek.

00:28:03.640 --> 00:28:08.640
Just to summarize, we saw how
Shutterfly, because they're so

00:28:08.640 --> 00:28:12.400
focused on good customer
experience and making a great

00:28:12.400 --> 00:28:15.550
experience for a very large set
of customers, has really

00:28:15.550 --> 00:28:18.510
looked for ways to be innovative
and use analytics

00:28:18.510 --> 00:28:20.650
to become more effective
and more efficient

00:28:20.650 --> 00:28:22.060
in how they do that.

00:28:22.060 --> 00:28:24.440
And one of the things that our
team really loves about this

00:28:24.440 --> 00:28:27.690
use case is how practical it
is in terms of the direct

00:28:27.690 --> 00:28:31.590
applicability to Shutterfly's
operations as a business, and

00:28:31.590 --> 00:28:34.010
we're really glad to see that
they're able to use the

00:28:34.010 --> 00:28:37.520
technology to take their
business to the next step, and

00:28:37.520 --> 00:28:42.470
to look at new ways of serving
their customers better.

00:28:42.470 --> 00:28:45.890
On behalf of the BigQuery team,
we really want to thank

00:28:45.890 --> 00:28:49.810
the Shutterfly team and all our
customers for helping us

00:28:49.810 --> 00:28:55.440
to learn about this space, solve
new sets of problems out

00:28:55.440 --> 00:28:57.600
there in business.

00:28:57.600 --> 00:29:00.920
And I just want to say, if
you're using BigQuery already,

00:29:00.920 --> 00:29:05.050
we want to hear from you about
these new use cases.

00:29:05.050 --> 00:29:08.440
If you're not using BigQuery,
definitely give it a try.

00:29:08.440 --> 00:29:11.640
It's part of this larger suite
of cloud platform products--

00:29:11.640 --> 00:29:15.520
App Engine, Compute Engine,
cloud storage, many more

00:29:15.520 --> 00:29:16.660
things in the pipeline.

00:29:16.660 --> 00:29:19.010
And so we strongly encourage
you to take a look.

00:29:19.010 --> 00:29:23.200
Novel, efficient, scalable
applications with a very

00:29:23.200 --> 00:29:26.990
practical application
to your business.

00:29:26.990 --> 00:29:29.870
So we're going to go to Q&amp;A, but
right before that, I just

00:29:29.870 --> 00:29:32.360
wanted to highlight a couple of
other sessions that we have

00:29:32.360 --> 00:29:33.510
at the conference.

00:29:33.510 --> 00:29:37.720
So day one through day three,
we've got some great sessions

00:29:37.720 --> 00:29:40.480
that involve BigQuery and also
talk about big data.

00:29:40.480 --> 00:29:44.130
So I want to just point out a
few quickly, because I think

00:29:44.130 --> 00:29:48.070
it's right after lunch, we've
got a great session with the

00:29:48.070 --> 00:29:50.750
Maps team talking about all
the ships in the world.

00:29:50.750 --> 00:29:53.730
And so they're actually going to
show some new Maps features

00:29:53.730 --> 00:29:57.380
and how they visualize a huge
amount of data about the

00:29:57.380 --> 00:30:01.231
geoposition and location of
basically all the ships in the

00:30:01.231 --> 00:30:03.260
world at any one time.

00:30:03.260 --> 00:30:05.150
It's a really, really cool
session, and they're going to

00:30:05.150 --> 00:30:07.100
talk about how they do that.

00:30:07.100 --> 00:30:09.050
There's another session--

00:30:09.050 --> 00:30:11.440
if you've looked around and
you've seen all these little

00:30:11.440 --> 00:30:15.270
devices gathering data, we've
got these Arduino devices that

00:30:15.270 --> 00:30:17.480
our developer relations team has
put up, and they've strung

00:30:17.480 --> 00:30:21.020
together a great data-sensing
pipeline.

00:30:21.020 --> 00:30:23.140
And we're basically measuring
things like humidity and

00:30:23.140 --> 00:30:26.630
volume, what's going on in real
time at the conference.

00:30:26.630 --> 00:30:29.240
And you see some displays
of this--

00:30:29.240 --> 00:30:31.080
I think there's one right
outside, actually--

00:30:31.080 --> 00:30:33.090
showing what's going on
in the conference.

00:30:33.090 --> 00:30:36.110
The developer relations team is
going to be talking about

00:30:36.110 --> 00:30:39.520
how they did this using BigQuery
and the other pieces

00:30:39.520 --> 00:30:41.810
of the cloud platform that
I described just now.

00:30:41.810 --> 00:30:44.980
Tomorrow there's another great
session run by Gamesys, which

00:30:44.980 --> 00:30:47.240
is a gaming company, and
they're going to show a

00:30:47.240 --> 00:30:50.830
different use case about how
they used BigQuery and some of

00:30:50.830 --> 00:30:55.250
our other services to do a very,
very detailed analysis

00:30:55.250 --> 00:30:59.440
of the users, and really help
to drive user behavior and

00:30:59.440 --> 00:31:02.750
adoption of their
new features.

00:31:02.750 --> 00:31:05.540
So I'll leave this up, and we'll
basically go to about 10

00:31:05.540 --> 00:31:07.060
minutes of question
and answer.

00:31:07.060 --> 00:31:10.010
And if I can ask, if anyone has
questions, please come up

00:31:10.010 --> 00:31:11.960
to the two mics in front
of the aisle.

00:31:17.830 --> 00:31:18.790
AUDIENCE: Hi, hello.

00:31:18.790 --> 00:31:24.210
Can you talk a little bit about
the tools you use in

00:31:24.210 --> 00:31:27.150
order to prepare the data before
uploading to cloud

00:31:27.150 --> 00:31:31.930
storage, and also, how long does
it take to upload those

00:31:31.930 --> 00:31:35.702
big files, and pricing,
if it's possible?

00:31:35.702 --> 00:31:37.190
DEREK STEVENSON: Sure.

00:31:37.190 --> 00:31:39.390
So the tools that we use--

00:31:39.390 --> 00:31:43.120
we use a variety of ETL tools.

00:31:43.120 --> 00:31:45.590
I think there's a lot of
different ways to do it.

00:31:45.590 --> 00:31:52.060
We happen to use a combination
of Informatica or Ab Initio.

00:31:52.060 --> 00:31:54.330
So those are certainly powerful
tools for doing that

00:31:54.330 --> 00:31:57.050
sort of thing, but there's
a wide variety of options

00:31:57.050 --> 00:31:58.910
available to you.

00:31:58.910 --> 00:32:04.200
Hadoop processing might
be another approach.

00:32:04.200 --> 00:32:08.900
In our case, transferring the
data itself, we're fortunate

00:32:08.900 --> 00:32:11.770
in that we have pretty good
connections to the internet.

00:32:11.770 --> 00:32:17.150
So transferring the data over
was relatively quick.

00:32:17.150 --> 00:32:20.480
I can't give you a specific
number, but if you come see me

00:32:20.480 --> 00:32:23.690
afterwards, we can certainly
get you that information.

00:32:23.690 --> 00:32:26.080
AUDIENCE: Pricing?

00:32:26.080 --> 00:32:29.670
Because I guess storing all that
information in Google's

00:32:29.670 --> 00:32:30.640
[INAUDIBLE]

00:32:30.640 --> 00:32:32.510
must be--

00:32:32.510 --> 00:32:34.200
is it expensive?

00:32:34.200 --> 00:32:35.700
JU-KAY KWEK: So you
can look this up

00:32:35.700 --> 00:32:37.360
online, it's on our website.

00:32:37.360 --> 00:32:39.470
But basically, BigQuery is
priced along two vectors.

00:32:39.470 --> 00:32:43.760
It's pay-as-you-go right now, so
basically, storage is $0.12

00:32:43.760 --> 00:32:45.860
per gigabyte per month.

00:32:45.860 --> 00:32:49.240
And that's basically for
multiple replication.

00:32:49.240 --> 00:32:51.010
You don't have to manage
any of that.

00:32:51.010 --> 00:32:53.090
Your data is very secure.

00:32:53.090 --> 00:32:56.480
Then the second pricing is for
queries, and that's priced by

00:32:56.480 --> 00:32:59.670
the amount of data you query,
and that's basically 3 and 1/2

00:32:59.670 --> 00:33:01.060
cents per gigabyte.

00:33:01.060 --> 00:33:04.320
So for example, some of those
large queries might get more

00:33:04.320 --> 00:33:08.230
expensive, but the way we've
structured the pricing is that

00:33:08.230 --> 00:33:12.660
we believe that if you really
have a large amount of data,

00:33:12.660 --> 00:33:15.910
like we saw, and it's really
important to be able to query

00:33:15.910 --> 00:33:18.800
that in full grain, then that's
a higher value to you.

00:33:18.800 --> 00:33:20.660
So what we've tried to do is
align the pricing with the

00:33:20.660 --> 00:33:23.002
value that you get
from the service.

00:33:23.002 --> 00:33:24.420
AUDIENCE: And one
last question.

00:33:24.420 --> 00:33:27.400
Any plans to integrate this
with Compute Engine?

00:33:27.400 --> 00:33:30.830
For example, if I want to do
complex event processing, I

00:33:30.830 --> 00:33:35.770
guess Compute Engine will be the
best edition to do that.

00:33:35.770 --> 00:33:38.770
JU-KAY KWEK: We have multiple
plans to integrate, not just

00:33:38.770 --> 00:33:40.780
with Compute Engine,
but with App

00:33:40.780 --> 00:33:43.100
Engine, with the datastore.

00:33:43.100 --> 00:33:45.300
I would love to hear about a
specific use case that you

00:33:45.300 --> 00:33:48.390
have, and maybe we can point
you in the right direction.

00:33:48.390 --> 00:33:50.420
Thank you.

00:33:50.420 --> 00:33:51.350
AUDIENCE: Hi.

00:33:51.350 --> 00:33:55.090
My question is about when
you sometimes get a

00:33:55.090 --> 00:33:58.800
result set too large.

00:33:58.800 --> 00:34:01.595
Not as the final result, but
as an intermediate result,

00:34:01.595 --> 00:34:06.100
like you group each by something
that needs to

00:34:06.100 --> 00:34:09.050
generate an intermediate
result set.

00:34:09.050 --> 00:34:12.690
Is the only solution to that to
put it in the new table, or

00:34:12.690 --> 00:34:17.690
is there another way to deal
with those issues?

00:34:17.690 --> 00:34:18.770
JU-KAY KWEK: I see a
few nodding heads.

00:34:18.770 --> 00:34:23.699
So result too large is one of
the few limitations that

00:34:23.699 --> 00:34:25.120
BigQuery does have.

00:34:25.120 --> 00:34:28.010
Basically, what happens is
because we give you the

00:34:28.010 --> 00:34:31.219
ability to query a really,
really large data set, it's

00:34:31.219 --> 00:34:33.659
equally possible that some of
the results that come back can

00:34:33.659 --> 00:34:36.040
be very, very massive,
especially now that we have

00:34:36.040 --> 00:34:37.800
things like the Big
Join feature.

00:34:37.800 --> 00:34:41.020
So a couple of common
workarounds would be being

00:34:41.020 --> 00:34:43.639
very careful to scope out the
number of columns that you

00:34:43.639 --> 00:34:47.540
specify in, say, for example,
your select statements.

00:34:47.540 --> 00:34:52.130
So the extreme would be select
star, but if you select very,

00:34:52.130 --> 00:34:55.880
very specifically, that's one
way of reducing the amount of

00:34:55.880 --> 00:34:57.220
total data returned.

00:34:57.220 --> 00:34:59.940
Another technique in the
other dimension is to

00:34:59.940 --> 00:35:01.440
do with table sharding.

00:35:01.440 --> 00:35:09.750
So if you think about how, say,
time series, if you think

00:35:09.750 --> 00:35:12.700
about what is the most efficient
query pattern that

00:35:12.700 --> 00:35:14.940
you have on your time-- say,
for example, it's a weekly

00:35:14.940 --> 00:35:17.790
report but you run, or maybe
it's a daily report that you

00:35:17.790 --> 00:35:21.330
run, then it actually makes
sense to design your table

00:35:21.330 --> 00:35:23.630
ingestion so that you're
creating a

00:35:23.630 --> 00:35:25.320
specific set of tables.

00:35:25.320 --> 00:35:28.240
And that way, you can actually
reduce the amount of total

00:35:28.240 --> 00:35:29.750
data that gets returned,
as well.

00:35:29.750 --> 00:35:33.180
It also is more cost-effective.

00:35:33.180 --> 00:35:36.560
AUDIENCE: Paying doesn't
help with that, right?

00:35:36.560 --> 00:35:37.950
It's just storage
that you pay.

00:35:37.950 --> 00:35:41.370
But you cannot pay to get a
larger intermediate result

00:35:41.370 --> 00:35:43.540
set, or something like that.

00:35:43.540 --> 00:35:45.950
JU-KAY KWEK: So the question
is, can I pay more to get

00:35:45.950 --> 00:35:47.954
larger results?

00:35:47.954 --> 00:35:51.480
Unfortunately, you can't do that
now, but if you've looked

00:35:51.480 --> 00:35:54.980
at the limits with every
release, we're actually

00:35:54.980 --> 00:35:57.470
dramatically increasing
the amount of results

00:35:57.470 --> 00:35:58.740
that you can return.

00:35:58.740 --> 00:36:00.970
Right now, we've got it up to,
I think it's about 128

00:36:00.970 --> 00:36:03.360
megabytes compressed data.

00:36:03.360 --> 00:36:07.320
We are going to push that
back, so watch that.

00:36:07.320 --> 00:36:07.990
I think you'll like it.

00:36:07.990 --> 00:36:08.940
AUDIENCE: Thanks,

00:36:08.940 --> 00:36:10.641
JU-KAY KWEK: Thank you.

00:36:10.641 --> 00:36:11.040
AUDIENCE: Hey, guys.

00:36:11.040 --> 00:36:13.800
So I had a question about
the import process.

00:36:13.800 --> 00:36:17.790
You mentioned in your diagram
one of the steps was to

00:36:17.790 --> 00:36:19.980
actually take the data and
put it into CSV files.

00:36:19.980 --> 00:36:22.790
And then you also mentioned
that it's optimal to

00:36:22.790 --> 00:36:25.180
denormalize the data, but then
you also mentioned that there

00:36:25.180 --> 00:36:27.810
were scenarios that it might
not be optimal to

00:36:27.810 --> 00:36:29.260
denormalize all of it.

00:36:29.260 --> 00:36:32.410
So I'm wondering, from when
you make that decision to

00:36:32.410 --> 00:36:35.520
denormalize data or to keep
it normalized, what--

00:36:35.520 --> 00:36:37.100
I understand the reason
why you would want

00:36:37.100 --> 00:36:37.910
to denormalize it.

00:36:37.910 --> 00:36:40.080
But can you give a couple
examples, why you would want

00:36:40.080 --> 00:36:42.580
to leave it normalized, and
then, does it have an impact

00:36:42.580 --> 00:36:45.240
on a BigQuery perspective?

00:36:45.240 --> 00:36:46.540
JU-KAY KWEK: Maybe I can address
that with just with a

00:36:46.540 --> 00:36:49.650
few comments, and actually, I'd
love it, because Derek has

00:36:49.650 --> 00:36:52.100
a great thought process
about this.

00:36:52.100 --> 00:36:55.420
Table denormalization is
basically an efficiency that

00:36:55.420 --> 00:36:59.960
allows you to very, very quickly
work against the data.

00:36:59.960 --> 00:37:04.390
The question is, why have we
recommended that, and then

00:37:04.390 --> 00:37:06.780
also increase the
ability to join?

00:37:06.780 --> 00:37:09.290
The answer to that is, we've
noticed a lot of different

00:37:09.290 --> 00:37:11.380
styles, legitimate needs
to query the data

00:37:11.380 --> 00:37:13.300
in different ways.

00:37:13.300 --> 00:37:17.692
So to give an example
from Shutterfly--

00:37:17.692 --> 00:37:21.750
I'll hand it over to you in a
second-- what we noticed is

00:37:21.750 --> 00:37:25.740
basically, if you think about
data within the organization,

00:37:25.740 --> 00:37:29.720
not all data is equally divided
up, and there's

00:37:29.720 --> 00:37:32.600
certain things that make more
sense to cluster together.

00:37:32.600 --> 00:37:35.740
And so what Shutterfly did was
to pull that together into a

00:37:35.740 --> 00:37:39.030
kind of domain silo of data, and
when you need to bring in

00:37:39.030 --> 00:37:41.980
a new set, or when you need to
look at the other part of the

00:37:41.980 --> 00:37:44.500
organization and bring in data
from there, that's when Join

00:37:44.500 --> 00:37:45.870
is very useful.

00:37:45.870 --> 00:37:48.180
But why don't you comment on
that from your experience?

00:37:48.180 --> 00:37:48.620
DEREK STEVENSON: Sure.

00:37:48.620 --> 00:37:53.500
I think it's not so much that
you don't want to denormalize.

00:37:53.500 --> 00:37:57.770
It's more that circumstances
dictate that you can't

00:37:57.770 --> 00:37:58.410
denormalize.

00:37:58.410 --> 00:38:01.170
So if you've spent time building
an infrastructure

00:38:01.170 --> 00:38:05.300
that denormalizes and imports
that into BigQuery, and now

00:38:05.300 --> 00:38:07.800
you realize there's this new
category of data that you want

00:38:07.800 --> 00:38:11.490
to connect, the process of going
through that pipeline

00:38:11.490 --> 00:38:15.330
again and reloading your data to
BigQuery, because it's sort

00:38:15.330 --> 00:38:18.760
of a write once model, isn't
always practical.

00:38:18.760 --> 00:38:21.170
Or if it's something you know
you need to do, it's going to

00:38:21.170 --> 00:38:22.040
take some time.

00:38:22.040 --> 00:38:28.380
So as an interim step, you can
imagine where an analyst can

00:38:28.380 --> 00:38:32.020
put together their own data set
on their desktop or laptop

00:38:32.020 --> 00:38:36.600
and import it directly, and
not be bogged down with

00:38:36.600 --> 00:38:40.030
needing to hand that
over to IT, to flow

00:38:40.030 --> 00:38:41.950
through that pipeline.

00:38:41.950 --> 00:38:43.200
AUDIENCE: Thanks, guys.

00:38:45.360 --> 00:38:46.960
AUDIENCE: Hello.

00:38:46.960 --> 00:38:49.530
I had a question if you
evaluated any other

00:38:49.530 --> 00:38:53.870
alternatives that came up as
second place runners-up, maybe

00:38:53.870 --> 00:38:57.200
in-house solutions, or if you
found anything that you maybe

00:38:57.200 --> 00:39:00.350
tried that didn't quite work.

00:39:00.350 --> 00:39:02.720
Our data is kind of the same
ballpark as yours, and I've

00:39:02.720 --> 00:39:05.335
been playing with BigQuery and
been very happy with it, but

00:39:05.335 --> 00:39:07.760
I've also been trying to find
other things that maybe are in

00:39:07.760 --> 00:39:10.790
the same-- you maybe don't get
your query in 20 seconds, but

00:39:10.790 --> 00:39:13.160
you can get it in 15 minutes
or something, instead of

00:39:13.160 --> 00:39:16.150
overnight, like Hive and
solutions like that.

00:39:16.150 --> 00:39:18.740
Did you play with anything
like that?

00:39:18.740 --> 00:39:23.410
DEREK STEVENSON: So we have
looked at those options and

00:39:23.410 --> 00:39:27.190
continue to evaluate where
those best fit.

00:39:27.190 --> 00:39:31.010
I think, to comment on Hive and
Hadoop, for example, our

00:39:31.010 --> 00:39:33.410
core competency from an
engineering standpoint is

00:39:33.410 --> 00:39:36.890
really focusing on product
development.

00:39:36.890 --> 00:39:42.420
So how do customers save time in
how they create photo books

00:39:42.420 --> 00:39:43.250
and things like that.

00:39:43.250 --> 00:39:49.580
So our primary focus isn't
necessarily building a team to

00:39:49.580 --> 00:39:52.270
manage a Hadoop environment.

00:39:52.270 --> 00:39:55.060
So some of our constraints are
perhaps more practical and

00:39:55.060 --> 00:39:56.070
less technical.

00:39:56.070 --> 00:40:00.010
They're also taking
into account the

00:40:00.010 --> 00:40:01.720
personnel side of things.

00:40:01.720 --> 00:40:05.810
I think that's certainly one
direction you could go.

00:40:05.810 --> 00:40:08.310
We've also looked at column
store databases.

00:40:08.310 --> 00:40:12.340
Typically the pricing structure
with those requires

00:40:12.340 --> 00:40:17.880
kind of a big upfront
commitment, and so those are

00:40:17.880 --> 00:40:20.330
more difficult conversations
with the folks

00:40:20.330 --> 00:40:22.032
that sign the checks.

00:40:22.032 --> 00:40:22.440
AUDIENCE: Sure.

00:40:22.440 --> 00:40:24.700
Did you try anything like a
relational database type,

00:40:24.700 --> 00:40:28.100
MySQL or Postgres or
anything like that?

00:40:28.100 --> 00:40:30.770
DEREK STEVENSON: A lot of our
existing approaches use those

00:40:30.770 --> 00:40:34.520
tools and techniques, but we
feel like the complexity to

00:40:34.520 --> 00:40:39.100
work around some of the
constraints are pretty

00:40:39.100 --> 00:40:42.480
difficult to do in a
scalable fashion.

00:40:42.480 --> 00:40:45.480
But I'd be happy to chat
more after the

00:40:45.480 --> 00:40:46.400
session, if you'd like.

00:40:46.400 --> 00:40:46.900
AUDIENCE: OK.

00:40:46.900 --> 00:40:48.150
Thank you.

00:40:50.210 --> 00:40:51.380
AUDIENCE: Hi there.

00:40:51.380 --> 00:40:55.630
Are there any plans to make
AdWords data available in

00:40:55.630 --> 00:40:57.930
BigQuery data sets?

00:40:57.930 --> 00:40:59.960
JU-KAY KWEK: AdWords
data specifically?

00:40:59.960 --> 00:41:01.760
I would say that--

00:41:01.760 --> 00:41:05.080
I can't answer that
specifically, but if you look

00:41:05.080 --> 00:41:08.680
at what's going on, one of the
sessions actually today is

00:41:08.680 --> 00:41:11.300
about Google Analytics
and AdSense.

00:41:11.300 --> 00:41:13.530
I think you can get a
sense of where we're

00:41:13.530 --> 00:41:15.250
going with all of this.

00:41:15.250 --> 00:41:18.820
And so we definitely
see the value.

00:41:18.820 --> 00:41:22.530
We hear a lot of it from
our customers.

00:41:22.530 --> 00:41:24.790
If you have a situation
where-- it

00:41:24.790 --> 00:41:25.610
sounds like you do--

00:41:25.610 --> 00:41:27.360
I'd love to hear more about it,
actually, to understand

00:41:27.360 --> 00:41:31.090
the requirements together with
the different Ads teams.

00:41:31.090 --> 00:41:32.855
Do you happen to use, say,
Google Analytics?

00:41:32.855 --> 00:41:33.495
Do you use AdSense?

00:41:33.495 --> 00:41:34.745
AUDIENCE: Yeah, we do.

00:41:37.880 --> 00:41:40.300
I mean, our AdWords
data reports

00:41:40.300 --> 00:41:42.110
are hundreds of gigabytes.

00:41:42.110 --> 00:41:46.220
So it's a bit inefficient to
download 100 gigs and then

00:41:46.220 --> 00:41:47.990
re-upload it to Google
while you guys

00:41:47.990 --> 00:41:49.770
already have that data.

00:41:49.770 --> 00:41:52.740
If we're doing all that work
anyway, that opens up the

00:41:52.740 --> 00:41:55.150
space to competitors, because
we're also now

00:41:55.150 --> 00:41:58.540
looking at AWS Redshift.

00:41:58.540 --> 00:42:01.135
But if you guys made that
data available, it

00:42:01.135 --> 00:42:02.330
would be the clincher.

00:42:02.330 --> 00:42:04.240
We'd go straight to BigQuery.

00:42:04.240 --> 00:42:06.510
JU-KAY KWEK: We actually have
some members of the

00:42:06.510 --> 00:42:09.230
Analytics team here.

00:42:09.230 --> 00:42:11.760
We'd love to actually hear a
little bit more about this.

00:42:11.760 --> 00:42:14.870
We have a couple of things that
we're trying, that if

00:42:14.870 --> 00:42:16.700
you're interested, we'd
love to work with you.

00:42:16.700 --> 00:42:17.040
AUDIENCE: Sure.

00:42:17.040 --> 00:42:18.260
I'll chat with you afterwards.

00:42:18.260 --> 00:42:18.850
JU-KAY KWEK: Great.

00:42:18.850 --> 00:42:20.740
Thanks.

00:42:20.740 --> 00:42:21.430
AUDIENCE: Hello.

00:42:21.430 --> 00:42:24.280
I would like to ask you if you
can share with us some

00:42:24.280 --> 00:42:28.540
information about limitation,
about the number of the column

00:42:28.540 --> 00:42:30.180
that a table can have.

00:42:30.180 --> 00:42:34.850
I mean, I know that in order to
get best performance using

00:42:34.850 --> 00:42:38.180
Google BigQuery, we should
normalize all the data.

00:42:38.180 --> 00:42:42.870
So if I have to create a table
from merging a lot of

00:42:42.870 --> 00:42:46.610
different tables, I would like
to know the maximum number of

00:42:46.610 --> 00:42:48.470
columns I can have.

00:42:48.470 --> 00:42:52.690
JU-KAY KWEK: We actually have
some uses where customers with

00:42:52.690 --> 00:42:55.720
tables with columns
in the thousands.

00:42:55.720 --> 00:43:00.820
So as far as practical limits
go, I think you can scale

00:43:00.820 --> 00:43:04.730
quite significantly in a
horizontal fashion-- probably

00:43:04.730 --> 00:43:07.486
more than you'll have
column needs for.

00:43:07.486 --> 00:43:07.950
AUDIENCE: OK.

00:43:07.950 --> 00:43:10.070
Thank you.

00:43:10.070 --> 00:43:11.000
AUDIENCE: Hi.

00:43:11.000 --> 00:43:14.600
Do you have any plans to
introduce an OBDC driver so we

00:43:14.600 --> 00:43:17.150
can access the data more
efficiently from Excel and

00:43:17.150 --> 00:43:19.700
from other tools?

00:43:19.700 --> 00:43:21.740
JU-KAY KWEK: So we
don't publicly

00:43:21.740 --> 00:43:24.640
advertise an ODBC driver.

00:43:24.640 --> 00:43:27.320
There are companies, third
parties unrelated to Google,

00:43:27.320 --> 00:43:30.330
that have done things like
produce the JDBC driver, if

00:43:30.330 --> 00:43:31.350
you go and search out there.

00:43:31.350 --> 00:43:34.110
Those do exist.

00:43:34.110 --> 00:43:37.680
There is actually a connector,
an Excel

00:43:37.680 --> 00:43:38.970
connector, for BigQuery.

00:43:38.970 --> 00:43:41.090
AUDIENCE: It limits it in that
the number of results that you

00:43:41.090 --> 00:43:44.300
can return is pretty small,
and you have to return the

00:43:44.300 --> 00:43:46.840
results and then manipulate
them in Excel.

00:43:46.840 --> 00:43:48.470
So that's really where
the issue came in.

00:43:48.470 --> 00:43:48.510
OK.

00:43:48.510 --> 00:43:50.350
I'll have to look for
those third parties.

00:43:50.350 --> 00:43:52.520
The other question that I've got
is, where BigQuery falls

00:43:52.520 --> 00:43:56.490
down for us is the limitation
on handling GIS data.

00:43:56.490 --> 00:43:59.780
So we want to be able to take
latitudes and longitudes and

00:43:59.780 --> 00:44:03.020
say, well, tell me what the zip
code is, and aggregate by

00:44:03.020 --> 00:44:06.030
zip code, analyze by state.

00:44:06.030 --> 00:44:10.220
And we just find that the
limitations in BigQuery today,

00:44:10.220 --> 00:44:11.620
working with GIS data--

00:44:11.620 --> 00:44:14.620
I mean, there really isn't
any ability to do that.

00:44:14.620 --> 00:44:16.850
So I'd recommend that if there
is an option in future to

00:44:16.850 --> 00:44:19.190
extend or to handle GIS data,
it would be tremendously

00:44:19.190 --> 00:44:20.600
useful for us.

00:44:20.600 --> 00:44:22.770
JU-KAY KWEK: So I think the
obvious one is the session

00:44:22.770 --> 00:44:25.020
afterwards on the ships is
probably going to be of

00:44:25.020 --> 00:44:28.780
interest to you, especially
how they built that app.

00:44:28.780 --> 00:44:31.740
It's actually quite easy to
integrate things like the Maps

00:44:31.740 --> 00:44:33.130
API with BigQuery API.

00:44:33.130 --> 00:44:37.640
Obviously that's not the easiest
solution right now,

00:44:37.640 --> 00:44:38.650
but those are some of
the things that

00:44:38.650 --> 00:44:40.650
are possible today.

00:44:40.650 --> 00:44:44.570
Another thing that we have seen
is we have some customers

00:44:44.570 --> 00:44:48.540
who have done geo-based
analysis using

00:44:48.540 --> 00:44:49.970
BI tools like Tableau.

00:44:49.970 --> 00:44:51.990
Tableau does a great job of
throwing up, you mentioned zip

00:44:51.990 --> 00:44:53.930
code and so on.

00:44:53.930 --> 00:44:56.120
We actually have a case study
that we're working on where

00:44:56.120 --> 00:44:57.890
they do exactly that.

00:44:57.890 --> 00:44:58.350
AUDIENCE: Interesting.

00:44:58.350 --> 00:44:58.830
Thank you.

00:44:58.830 --> 00:44:59.554
JU-KAY KWEK: I'll tell
you more if you're

00:44:59.554 --> 00:45:00.830
going to hang around.

00:45:00.830 --> 00:45:01.190
AUDIENCE: Sounds good.

00:45:01.190 --> 00:45:01.840
Thanks.

00:45:01.840 --> 00:45:03.313
WES CHEN: Just a follow-up.

00:45:03.313 --> 00:45:08.223
So just FYI, I think Google App
Engine has a Search API

00:45:08.223 --> 00:45:09.696
but also has [? geo-searching ?]

00:45:09.696 --> 00:45:10.187
as well.

00:45:10.187 --> 00:45:12.151
So if your data
[? is something ?] extremely

00:45:12.151 --> 00:45:16.079
large, or if you need a subset
of the data that you

00:45:16.079 --> 00:45:20.007
[INAUDIBLE], send it to App
Engine Datastore [INAUDIBLE]

00:45:20.007 --> 00:45:22.953
use the Search API to
search [INAUDIBLE].

00:45:25.910 --> 00:45:27.440
JU-KAY KWEK: And just to repeat
that because it wasn't

00:45:27.440 --> 00:45:30.120
on the mic, that's
Wes Chen from our

00:45:30.120 --> 00:45:32.060
Developer Relations team.

00:45:32.060 --> 00:45:36.551
App Engine Search API actually
has the ability to handle

00:45:36.551 --> 00:45:38.900
geo-searching and store
those points in

00:45:38.900 --> 00:45:41.570
the App Engine Datastore.

00:45:41.570 --> 00:45:42.220
Thanks for the question.

00:45:42.220 --> 00:45:43.540
I'd love to hear more
about that.

00:45:43.540 --> 00:45:45.310
And thank you all for
your attention

00:45:45.310 --> 00:45:46.780
and your great questions.

00:45:46.780 --> 00:45:48.270
Thank you for using
the product.

00:45:48.270 --> 00:45:49.650
You help us to make it better.

