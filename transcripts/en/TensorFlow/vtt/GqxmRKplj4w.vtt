WEBVTT
Kind: captions
Language: en

00:00:02.300 --> 00:00:04.645
JOSH DILLON: Hi, I'm
Josh, and today going

00:00:04.645 --> 00:00:06.940
to be talking about
TensorFlow Probability, which

00:00:06.940 --> 00:00:10.290
is a project I've been
working on for two years now.

00:00:10.290 --> 00:00:12.140
So what is TensorFlow
Probability?

00:00:12.140 --> 00:00:15.810
So we are part of the
TensorFlow ecosystem.

00:00:15.810 --> 00:00:18.800
It's a library built
using TensorFlow,

00:00:18.800 --> 00:00:22.220
and the idea is to make it
easy to combine deep learning

00:00:22.220 --> 00:00:24.860
with probabilistic modeling.

00:00:24.860 --> 00:00:28.520
We are useful for statisticians
and data scientists

00:00:28.520 --> 00:00:31.970
to whom we can provide
R-like capabilities, which

00:00:31.970 --> 00:00:36.740
take advantage of GPU and
TPU, and to ML researchers

00:00:36.740 --> 00:00:39.770
and practitioners so you
can build deep models, which

00:00:39.770 --> 00:00:41.640
capture uncertainty.

00:00:41.640 --> 00:00:44.180
So why should you care?

00:00:44.180 --> 00:00:47.450
So a neural net that
predicts binary outcomes

00:00:47.450 --> 00:00:52.180
is just a Bernoulli distribution
that's parameterized

00:00:52.180 --> 00:00:54.320
by something fancy.

00:00:54.320 --> 00:00:56.180
So suppose you have this.

00:00:56.180 --> 00:00:57.620
You've got your
sort of V1 model.

00:00:57.620 --> 00:00:58.610
Looks great.

00:00:58.610 --> 00:01:01.430
Now what?

00:01:01.430 --> 00:01:04.430
That's where TensorFlow
Probability can help you out.

00:01:04.430 --> 00:01:09.380
Using our software, you can
encode additional information

00:01:09.380 --> 00:01:10.370
in your problem.

00:01:10.370 --> 00:01:12.840
You can control
prediction variance.

00:01:12.840 --> 00:01:15.380
You can even possibly
ask tougher questions.

00:01:15.380 --> 00:01:17.540
No longer assume that
pixels are independent,

00:01:17.540 --> 00:01:18.540
because guess what?

00:01:18.540 --> 00:01:20.528
They're not.

00:01:20.528 --> 00:01:22.910
This is what we're going
to be talking about.

00:01:22.910 --> 00:01:25.520
So the main take home
message for this talk

00:01:25.520 --> 00:01:30.820
is TensorFlow Probability
is a bunch of low level

00:01:30.820 --> 00:01:33.220
tools, a collection
of low level tools

00:01:33.220 --> 00:01:35.230
which are aimed at
trying to make it easier

00:01:35.230 --> 00:01:38.620
for you to express what you
know about your problem.

00:01:38.620 --> 00:01:40.750
To not try to
shoehorn your problem

00:01:40.750 --> 00:01:45.030
into a neural net
architecture, but rather

00:01:45.030 --> 00:01:48.420
describe what you know and take
advantage of what you know.

00:01:48.420 --> 00:01:51.480
And these sort of images over
here, we'll talk about a few

00:01:51.480 --> 00:01:54.480
of them, but each
of them represents

00:01:54.480 --> 00:01:57.456
a part of the TensorFlow
Probability package.

00:01:57.456 --> 00:02:00.170
OK, so in the simplest
form, how would you

00:02:00.170 --> 00:02:01.470
use TensorFlow Probability?

00:02:01.470 --> 00:02:04.140
Sort of like a get our
feet wet type example.

00:02:04.140 --> 00:02:06.950
So we offer generalized
linear models.

00:02:06.950 --> 00:02:10.199
Think logistic regression,
linear regression.

00:02:10.199 --> 00:02:14.560
Very boring stuff maybe, but
it's a good starting point.

00:02:14.560 --> 00:02:17.520
So you'll see this pattern
throughout the TensorFlow

00:02:17.520 --> 00:02:19.810
Probability software
stack and how you use it.

00:02:19.810 --> 00:02:21.310
But basically, you
specify a model,

00:02:21.310 --> 00:02:23.580
in this case, Bernoulli
corresponding logistic

00:02:23.580 --> 00:02:24.570
regression.

00:02:24.570 --> 00:02:26.140
And then you just fit it.

00:02:26.140 --> 00:02:28.830
And in this case, we're using
L1 regularization and L2

00:02:28.830 --> 00:02:30.780
so you can get sparse weights.

00:02:30.780 --> 00:02:32.610
And why you should
care about this

00:02:32.610 --> 00:02:36.090
is it's using a second order
solver under the hood, which

00:02:36.090 --> 00:02:38.400
means that up to
floating point precision,

00:02:38.400 --> 00:02:41.280
you would never need more
than 30 iterations of this.

00:02:41.280 --> 00:02:44.700
And in practice, maybe three
or four is all it takes.

00:02:44.700 --> 00:02:46.440
And since you can
take advantage of GPU,

00:02:46.440 --> 00:02:48.400
it's like a drop-in
replacement, it

00:02:48.400 --> 00:02:51.830
takes advantage of GPU
drop-in replacement

00:02:51.830 --> 00:02:54.920
for, say, R in this case.

00:02:54.920 --> 00:02:56.900
So that's just
kind of the canned,

00:02:56.900 --> 00:02:59.450
like an example of some of
the canned stuff we offer.

00:02:59.450 --> 00:03:02.030
Where things really
get exciting are

00:03:02.030 --> 00:03:03.500
this sort of suite of tools.

00:03:03.500 --> 00:03:06.440
So first we are going to
talk about distributions,

00:03:06.440 --> 00:03:08.780
which are probably what
you think they are.

00:03:08.780 --> 00:03:11.712
We'll also talk about
bijectors in this talk.

00:03:11.712 --> 00:03:14.390
TensorFlow Probability
provides probabilistic layers,

00:03:14.390 --> 00:03:16.770
things that wrap up
variational inference

00:03:16.770 --> 00:03:20.070
with different
distributional assumptions.

00:03:20.070 --> 00:03:22.970
We have a probabilistic
programming language, which

00:03:22.970 --> 00:03:24.890
is the successor of Edward.

00:03:24.890 --> 00:03:28.850
That's also part of the
TensorFlow Probability package.

00:03:28.850 --> 00:03:30.590
And then on the
inference side-- that's

00:03:30.590 --> 00:03:32.350
kind of for building models.

00:03:32.350 --> 00:03:35.360
On the inference side,
we've got a collection

00:03:35.360 --> 00:03:38.540
of Markov chain Monte
Carlo transition kernels

00:03:38.540 --> 00:03:40.970
and tools to use them,
diagnostic criteria, that sort

00:03:40.970 --> 00:03:43.820
of thing; tools for variational
inference in a numerically

00:03:43.820 --> 00:03:46.430
stable way; and
various optimizers,

00:03:46.430 --> 00:03:50.930
like stochastic gradient
monument descent, BFGS,

00:03:50.930 --> 00:03:54.920
[INAUDIBLE],, sort of the
stuff not stochastic gradient

00:03:54.920 --> 00:03:58.700
descent, maybe some of which are
more useful for single machine

00:03:58.700 --> 00:04:03.880
settings, others baking in
probability with optimization.

00:04:03.880 --> 00:04:08.540
OK, so a distribution, I hope
this is boring because nothing

00:04:08.540 --> 00:04:11.780
here should be really fancy.

00:04:11.780 --> 00:04:13.730
Capability of
drawing samples, you

00:04:13.730 --> 00:04:17.300
can compute probabilities,
CDF, 1 minus the CDF,

00:04:17.300 --> 00:04:19.409
mean, variance, all
the usual stuff.

00:04:19.409 --> 00:04:21.200
Little more interesting
here at the bottom.

00:04:21.200 --> 00:04:23.510
The event shape and,
you can't see it,

00:04:23.510 --> 00:04:24.860
but it says batch shape.

00:04:24.860 --> 00:04:28.460
So TensorFlow Probability
distributions--

00:04:28.460 --> 00:04:32.720
to take advantage of vectorized
[INAUDIBLE] hardware,

00:04:32.720 --> 00:04:33.975
you specify--

00:04:33.975 --> 00:04:35.600
you call the distribution
once, but you

00:04:35.600 --> 00:04:37.560
specify multiple parameters.

00:04:37.560 --> 00:04:40.749
So here's an example.

00:04:40.749 --> 00:04:43.040
We're building a normal, but
we're passing two location

00:04:43.040 --> 00:04:43.800
parameters.

00:04:43.800 --> 00:04:46.040
So when you call
sample on this, it's

00:04:46.040 --> 00:04:50.120
going to return two samples
every one time you call

00:04:50.120 --> 00:04:51.684
sample, if that makes sense.

00:04:51.684 --> 00:04:53.600
One will correspond to
the normal distribution

00:04:53.600 --> 00:04:57.020
parameterized with mean minus
1, the other with mean 1.

00:04:57.020 --> 00:05:01.580
It turns out this very simple
idea is extremely powerful

00:05:01.580 --> 00:05:03.800
and lets you immediately
take advantage

00:05:03.800 --> 00:05:06.980
of vector computation.

00:05:06.980 --> 00:05:10.790
So not only do
distributions have

00:05:10.790 --> 00:05:13.700
this sort of like small
tweak from other libraries

00:05:13.700 --> 00:05:16.460
or packages, but we've
got a bunch of them

00:05:16.460 --> 00:05:18.450
and you can combine them
in interesting ways.

00:05:18.450 --> 00:05:21.320
So it's not super important
what distribution this is.

00:05:21.320 --> 00:05:23.660
The point is we're
making a mixture,

00:05:23.660 --> 00:05:25.580
combining categorical
distributions

00:05:25.580 --> 00:05:28.587
with multivariate normal with
a diagonal parameterization,

00:05:28.587 --> 00:05:30.170
and it all just kind
of fits together,

00:05:30.170 --> 00:05:34.680
and you can do cool things
using simple building blocks.

00:05:34.680 --> 00:05:36.800
And that's a theme that's
pervasive in TensorFlow

00:05:36.800 --> 00:05:39.140
Probability-- simple
ideas scaled up

00:05:39.140 --> 00:05:43.670
to be a powerful
framework and formalism.

00:05:43.670 --> 00:05:46.160
So here's another example
of a distribution we have,

00:05:46.160 --> 00:05:47.630
Gaussian Processes.

00:05:47.630 --> 00:05:50.610
I think this is cool
because in a few lines,

00:05:50.610 --> 00:05:53.280
you can learn uncertainty.

00:05:53.280 --> 00:05:55.795
So notice that
the model has sort

00:05:55.795 --> 00:05:57.920
of different beliefs in
areas where there's no data

00:05:57.920 --> 00:05:59.540
and it's tight where there is.

00:05:59.540 --> 00:06:02.240
You could easily turn this
into a layer in your neural net

00:06:02.240 --> 00:06:04.860
if you wanted to.

00:06:04.860 --> 00:06:07.280
OK, so distributions,
there's a bunch of them.

00:06:07.280 --> 00:06:09.050
They have these sort
of batch semantics.

00:06:09.050 --> 00:06:10.430
They're cool.

00:06:10.430 --> 00:06:12.830
Onto our second building
block, bijectors.

00:06:12.830 --> 00:06:16.590
So a bijector is useful for
transforming a random variable.

00:06:16.590 --> 00:06:20.630
It is, think like log and x.

00:06:20.630 --> 00:06:22.970
You may on the
forward transformation

00:06:22.970 --> 00:06:27.920
take the exponential of
some random variable,

00:06:27.920 --> 00:06:30.140
and then to reverse it
you take the logarithm.

00:06:30.140 --> 00:06:33.110
So the forward is useful
for computing samples

00:06:33.110 --> 00:06:35.960
and the inverse is useful
for computing probabilities.

00:06:35.960 --> 00:06:40.340
So a bijector is a
bijective diffeomorphism,

00:06:40.340 --> 00:06:45.200
a differentiable isopmorphism
between two spaces.

00:06:45.200 --> 00:06:47.840
And those spaces represent sort
of an input random variable

00:06:47.840 --> 00:06:49.257
and an output random variable.

00:06:49.257 --> 00:06:51.590
And because we're interested
in computing probabilities,

00:06:51.590 --> 00:06:53.173
we have to keep track
of the Jacobian.

00:06:53.173 --> 00:06:55.130
So just change your
variables and an integral.

00:06:55.130 --> 00:06:57.320
And so that's what
this implements.

00:06:57.320 --> 00:06:59.180
We also have the
notion of shape.

00:06:59.180 --> 00:07:01.340
Because here, again,
everything supports these sort

00:07:01.340 --> 00:07:04.669
of batch shape semantics.

00:07:04.669 --> 00:07:06.210
So what would you
use a bijector for?

00:07:06.210 --> 00:07:09.950
So behind the slide
is an amazing idea.

00:07:09.950 --> 00:07:12.080
You can take a
neural net and use

00:07:12.080 --> 00:07:16.070
it to transform any
distribution you want,

00:07:16.070 --> 00:07:18.990
and sort of get an
arbitrarily rich distribution.

00:07:18.990 --> 00:07:21.800
So this little
piece of code here

00:07:21.800 --> 00:07:25.880
really is just two hidden
layers, two dense hidden layer

00:07:25.880 --> 00:07:26.750
neural net.

00:07:26.750 --> 00:07:30.230
And then it's wrapped up
inside this autoregressive flow

00:07:30.230 --> 00:07:32.510
bijector, which
transforms a normal.

00:07:32.510 --> 00:07:34.260
Now, here's why this is amazing.

00:07:34.260 --> 00:07:37.850
You could plug this in as
your loss in the output.

00:07:37.850 --> 00:07:42.130
Like this could be your loss
basically, the final line here.

00:07:42.130 --> 00:07:42.630
Whoops.

00:07:42.630 --> 00:07:43.755
I shouldn't have done that.

00:07:43.755 --> 00:07:46.400
On the final line is just
this distribution.logprob.

00:07:46.400 --> 00:07:48.650
That's an arbitrarily
rich distribution

00:07:48.650 --> 00:07:50.510
capable of learning
variance, not

00:07:50.510 --> 00:07:52.280
prescribed by like a Bernoulli.

00:07:52.280 --> 00:07:54.421
The variance is p
times 1 minus p.

00:07:54.421 --> 00:07:56.170
And unless your data
actually is generated

00:07:56.170 --> 00:07:57.544
by Bernoulli
distribution, that's

00:07:57.544 --> 00:08:02.140
a fairly restrictive assumption,
in part because anytime that's

00:08:02.140 --> 00:08:06.580
not the case, it's very
sensitive to mis-specification.

00:08:06.580 --> 00:08:08.410
So this is like a
much richer family.

00:08:08.410 --> 00:08:10.870
And it sort of combines
immediately neural nets

00:08:10.870 --> 00:08:13.180
and distributions.

00:08:13.180 --> 00:08:16.750
So another cool thing,
you can reverse bijectors.

00:08:16.750 --> 00:08:20.530
And this little one line
change was a whole other paper.

00:08:20.530 --> 00:08:23.920
And we see this phenomenon in
TensorFlow probability a lot.

00:08:23.920 --> 00:08:27.310
Because everything's low level
and modular, one little change,

00:08:27.310 --> 00:08:29.140
brand new idea.

00:08:29.140 --> 00:08:29.640
OK.

00:08:29.640 --> 00:08:31.140
So that's kind of
some background.

00:08:31.140 --> 00:08:34.450
Let's go through an example
of how you might use this.

00:08:34.450 --> 00:08:37.544
So this is from a book,
"Bayesian Methods for Hackers,"

00:08:37.544 --> 00:08:38.960
which we'll talk
about at the end.

00:08:38.960 --> 00:08:42.049
And the question is, so I guess
the guy who wrote this book,

00:08:42.049 --> 00:08:43.250
he got a girlfriend.

00:08:43.250 --> 00:08:46.710
And at some point his text
messaging frequency changed.

00:08:46.710 --> 00:08:50.790
So the question is, can
we find that in the data?

00:08:50.790 --> 00:08:52.590
And maybe you'd guess 22 days.

00:08:52.590 --> 00:08:54.489
Or maybe 40 some days.

00:08:54.489 --> 00:08:55.030
I don't know.

00:08:55.030 --> 00:08:55.990
Let's see.

00:08:55.990 --> 00:08:57.240
So here's a simple model.

00:08:57.240 --> 00:08:59.610
We'll posit that
there was a rate

00:08:59.610 --> 00:09:02.490
of text messages
in some pre period

00:09:02.490 --> 00:09:04.740
and a rate in some post period.

00:09:04.740 --> 00:09:07.110
And the question is,
was there a change over?

00:09:07.110 --> 00:09:10.050
And that's the sort of math,
or statistical program,

00:09:10.050 --> 00:09:11.260
as I like to call it.

00:09:11.260 --> 00:09:13.110
That statistical
program translates

00:09:13.110 --> 00:09:15.450
into TensorFlow probability
in an almost one to one

00:09:15.450 --> 00:09:20.580
way, exponential, uniform,
flip it over, final Poisson.

00:09:20.580 --> 00:09:22.350
And to compute the
joint log prob,

00:09:22.350 --> 00:09:25.462
we just add everything
up in log space.

00:09:25.462 --> 00:09:28.910
And using that we can
sample from the posterior.

00:09:28.910 --> 00:09:31.220
And so what we
find is, yes, there

00:09:31.220 --> 00:09:35.540
was one rate around 18 text
messages a day I guess.

00:09:35.540 --> 00:09:36.879
Another around 23.

00:09:36.879 --> 00:09:39.170
And it turns out that the
highest posterior probability

00:09:39.170 --> 00:09:40.760
was on day 44.

00:09:40.760 --> 00:09:42.890
So how did we get
these posterior samples

00:09:42.890 --> 00:09:45.910
from the joint log probability?

00:09:45.910 --> 00:09:47.830
We used MCMC.

00:09:47.830 --> 00:09:53.110
So our MCMC library has
several transition kernels.

00:09:53.110 --> 00:09:54.730
I think one of the
more powerful ones

00:09:54.730 --> 00:09:57.220
because it takes advantage
of automatic differentiation

00:09:57.220 --> 00:09:59.110
is Hamiltonian Monte Carlo.

00:09:59.110 --> 00:10:02.020
And all we do to use that
is take our joint log

00:10:02.020 --> 00:10:04.370
problem, which you saw
in the previous slide,

00:10:04.370 --> 00:10:07.060
and just pin whatever
you want to condition on.

00:10:07.060 --> 00:10:09.700
So in this case, we're going
to condition on count data.

00:10:09.700 --> 00:10:13.540
And we want to sample the tau
and the two lambdas, the rates

00:10:13.540 --> 00:10:15.940
and the changeover point.

00:10:15.940 --> 00:10:17.740
So we set this up.

00:10:17.740 --> 00:10:18.820
Whoops.

00:10:18.820 --> 00:10:21.270
We ask for some
number of results,

00:10:21.270 --> 00:10:23.980
burn in steps, or the
usual MCMC business.

00:10:23.980 --> 00:10:26.770
Something a little different
here is this transformer.

00:10:26.770 --> 00:10:30.310
The transformer takes a
constrained random variable

00:10:30.310 --> 00:10:34.750
and unconstrains it, because
HMC is taking a gradient step.

00:10:34.750 --> 00:10:36.640
And it may step out of bounds.

00:10:36.640 --> 00:10:40.339
And so since the lambda
terms are rates of a Poisson,

00:10:40.339 --> 00:10:41.380
they need to be positive.

00:10:41.380 --> 00:10:44.890
So the x bijector goes
to and from positive real

00:10:44.890 --> 00:10:46.960
to unconstrained real.

00:10:46.960 --> 00:10:47.860
So too with tau.

00:10:47.860 --> 00:10:50.590
That was on the 01 interval.

00:10:50.590 --> 00:10:52.690
And so using sigmoid,
which you can't see here,

00:10:52.690 --> 00:10:55.354
we transformed to and from.

00:10:55.354 --> 00:10:57.520
And day 44.

00:10:57.520 --> 00:11:00.190
It turns out that really
was when he started dating.

00:11:00.190 --> 00:11:05.340
And so it seems like
Bayesian inference was right.

00:11:05.340 --> 00:11:05.840
OK.

00:11:05.840 --> 00:11:09.085
So super hard graphical model,
which we won't talk about.

00:11:09.085 --> 00:11:11.210
But the point is, there's
a whole lot of math here,

00:11:11.210 --> 00:11:12.840
and it's really scary.

00:11:12.840 --> 00:11:14.740
Not really.

00:11:14.740 --> 00:11:18.510
Each line basically
transforms one to one.

00:11:18.510 --> 00:11:21.000
So you pull out
some graphical model

00:11:21.000 --> 00:11:24.030
from the literature before
neural nets got really popular

00:11:24.030 --> 00:11:24.840
again.

00:11:24.840 --> 00:11:28.490
And you can code it up in
TensorFlow probability.

00:11:28.490 --> 00:11:30.720
And where things get
amazing is you can actually

00:11:30.720 --> 00:11:33.270
parametrize these distributions
with a neural net,

00:11:33.270 --> 00:11:34.980
thus getting the
benefit of both.

00:11:34.980 --> 00:11:37.090
And you can differentiate
through the whole thing.

00:11:37.090 --> 00:11:41.220
So it's really sort of what's
old is new again, yet in a way

00:11:41.220 --> 00:11:43.380
that you can take advantage
of modern hardware.

00:11:43.380 --> 00:11:47.211
So just one to one
between math and TFP.

00:11:47.211 --> 00:11:47.710
OK.

00:11:47.710 --> 00:11:49.960
So we did see a little
bit of the deep learning,

00:11:49.960 --> 00:11:52.450
the masked R regressive flow.

00:11:52.450 --> 00:11:55.330
And I mentioned you can
re-parametrize stuff.

00:11:55.330 --> 00:11:57.430
So here's sort of the idea
of re-parametrization.

00:11:57.430 --> 00:12:00.400
So, as we know, probabilistic
graphical models

00:12:00.400 --> 00:12:02.780
tend to be computationally
very intensive.

00:12:02.780 --> 00:12:07.180
Neural nets are really
good at embedding data

00:12:07.180 --> 00:12:08.920
into a lower dimensional space.

00:12:08.920 --> 00:12:12.490
Why not take your complex,
computationally intensive,

00:12:12.490 --> 00:12:15.100
probabilistic graphical
model and parametrize it

00:12:15.100 --> 00:12:15.850
with a neural net?

00:12:15.850 --> 00:12:17.683
And that's kind of what
this slide is saying

00:12:17.683 --> 00:12:18.880
we should think about doing.

00:12:18.880 --> 00:12:23.020
So you've heard of GANs.

00:12:23.020 --> 00:12:25.300
So variational auto
encoders are kind

00:12:25.300 --> 00:12:27.880
of the probabilistic
analog of the GAN.

00:12:27.880 --> 00:12:31.420
That's the adversarial
networks trying

00:12:31.420 --> 00:12:33.970
to fight each other to come
up with a good balance.

00:12:33.970 --> 00:12:36.520
It actually has a
probabilistic sort of analog.

00:12:36.520 --> 00:12:37.390
And this is it.

00:12:37.390 --> 00:12:41.260
So in this case, the posterior
distribution takes, say,

00:12:41.260 --> 00:12:46.630
an image, and is a distribution
over a low dimensional space

00:12:46.630 --> 00:12:49.990
Z. And the likelihood
is a distribution

00:12:49.990 --> 00:12:52.810
that takes a low
dimensional representation

00:12:52.810 --> 00:12:54.250
and outputs back the image.

00:12:54.250 --> 00:12:58.030
And using variational inference,
which really just consists

00:12:58.030 --> 00:13:00.310
of 10 lines of
code, you can take

00:13:00.310 --> 00:13:03.220
these different
distributions, which

00:13:03.220 --> 00:13:05.740
are themselves parametrized
by neural nets,

00:13:05.740 --> 00:13:09.430
and just fit it with Monte
Carlo variational inference,

00:13:09.430 --> 00:13:12.370
taking advantage of TensorFlow's
automatic differentiation.

00:13:12.370 --> 00:13:15.421
So it all kind of
fits together nicely.

00:13:15.421 --> 00:13:15.920
OK.

00:13:15.920 --> 00:13:19.240
So that was a lot of information
that we kind of breezed through

00:13:19.240 --> 00:13:20.350
quickly.

00:13:20.350 --> 00:13:22.690
We are in the
process of rewriting

00:13:22.690 --> 00:13:24.370
this "Bayesian
Methods for Hackers"

00:13:24.370 --> 00:13:26.770
book using TensorFlow
probability.

00:13:26.770 --> 00:13:28.660
It already exists.

00:13:28.660 --> 00:13:30.850
I think there's like
a PyMC version of it.

00:13:30.850 --> 00:13:33.994
And so we've started
all the chapters.

00:13:33.994 --> 00:13:35.410
One and two are
in the best shape.

00:13:35.410 --> 00:13:36.730
So definitely start with those.

00:13:36.730 --> 00:13:40.590
In chapter one you'll find
the text message example.

00:13:40.590 --> 00:13:43.160
But that's basically it.

00:13:43.160 --> 00:13:47.980
So in conclusion,
TensorFlow probability

00:13:47.980 --> 00:13:50.630
helps you combine deep learning
with probabilistic modeling

00:13:50.630 --> 00:13:52.960
so you can encode
additional domain

00:13:52.960 --> 00:13:54.570
knowledge about your problem.

00:13:54.570 --> 00:13:55.600
Pip install.

00:13:55.600 --> 00:13:57.120
Easy to use.

00:13:57.120 --> 00:14:00.550
And you can check it out as
part of the TensorFlow ecosystem

00:14:00.550 --> 00:14:02.510
to learn more.

00:14:02.510 --> 00:14:04.132
Thanks.

00:14:04.132 --> 00:14:06.765
And I've got a few minutes
here for questions,

00:14:06.765 --> 00:14:07.860
if anyone has any.

00:14:13.349 --> 00:14:16.343
Yeah.

00:14:16.343 --> 00:14:32.277
AUDIENCE: [INAUDIBLE]

00:14:32.277 --> 00:14:33.110
JOSHUA DILLON: Yeah.

00:14:33.110 --> 00:14:35.810
So the question is, can
I quantify uncertainty

00:14:35.810 --> 00:14:37.990
in a neural net basically
using this stuff?

00:14:37.990 --> 00:14:39.410
And the answer is,
absolutely yes.

00:14:39.410 --> 00:14:41.480
That's why you would
use this stuff.

00:14:41.480 --> 00:14:44.150
In fact, the larger question
of why would you even

00:14:44.150 --> 00:14:46.910
use probabilistic modeling
is probably because you

00:14:46.910 --> 00:14:48.740
want to quantify uncertainty.

00:14:48.740 --> 00:14:51.920
And so I pulled back to this
variation autoencoder slide,

00:14:51.920 --> 00:14:57.410
because what's happening is
it's a little hard to see here,

00:14:57.410 --> 00:15:01.190
cause it's just code, but
this low dimensional space

00:15:01.190 --> 00:15:05.300
is basically inducing
uncertainty as a bottleneck.

00:15:05.300 --> 00:15:07.220
And all of your
neural nets do this.

00:15:07.220 --> 00:15:11.095
Often you'll have a
smaller, hidden layer,

00:15:11.095 --> 00:15:12.470
going from a larger
hidden layer,

00:15:12.470 --> 00:15:14.150
to a smaller, back to a larger.

00:15:14.150 --> 00:15:17.990
So the point with this is, just
do that in a principled way.

00:15:17.990 --> 00:15:21.920
Keep track of what you lose by
sort of compressing it down.

00:15:21.920 --> 00:15:24.980
And in so doing,
then you actually get

00:15:24.980 --> 00:15:27.390
a measure of how much you lost.

00:15:27.390 --> 00:15:30.650
And so while this is a
variational autoencoder,

00:15:30.650 --> 00:15:33.170
the supervised learning
sort of alternative to this

00:15:33.170 --> 00:15:35.540
would be variational
information bottleneck.

00:15:35.540 --> 00:15:37.890
And the code for that is
almost exactly the same.

00:15:37.890 --> 00:15:40.910
The only difference is
you're reconstructing a label

00:15:40.910 --> 00:15:42.650
from some input x.

00:15:42.650 --> 00:15:45.370
So you go from x, to z, to y.

00:15:45.370 --> 00:15:49.240
So image, low dimensional,
back to the thing

00:15:49.240 --> 00:15:50.580
you're trying to predict.

00:15:50.580 --> 00:15:51.080
OK.

00:15:51.080 --> 00:15:52.280
So I'm out of time.

00:15:52.280 --> 00:16:01.250
And with that, I will
take it over to you.

