WEBVTT
Kind: captions
Language: en

00:00:05.616 --> 00:00:07.690
DAVID: Good morning.

00:00:07.690 --> 00:00:11.700
Today at Google we are delighted
to welcome Mr. Timothy Edgar.

00:00:11.700 --> 00:00:14.730
Mr. Edgar is a former National
Security and Intelligence

00:00:14.730 --> 00:00:18.480
official, cybersecurity
expert, privacy lawyer,

00:00:18.480 --> 00:00:21.220
and civil liberties activist.

00:00:21.220 --> 00:00:23.100
He joined the American
Civil Liberties Union

00:00:23.100 --> 00:00:25.410
just before 9/11,
and spent five years

00:00:25.410 --> 00:00:29.430
fighting in Congress against
abuses in the war on terror.

00:00:29.430 --> 00:00:31.980
He then left the ACLU to try
to make a difference by going

00:00:31.980 --> 00:00:34.350
inside America's growing
surveillance state,

00:00:34.350 --> 00:00:40.320
and went on to advise the
George W. Bush and Obama

00:00:40.320 --> 00:00:44.460
Administrations on privacy
issues and cybersecurity.

00:00:44.460 --> 00:00:48.360
In 2013, he left government
for Brown University

00:00:48.360 --> 00:00:51.810
to help launch its professional
cybersecurity degree program,

00:00:51.810 --> 00:00:54.210
and he is now a senior
fellow at Brown's Watson

00:00:54.210 --> 00:00:57.750
Institute of International
and Public Affairs.

00:00:57.750 --> 00:01:00.570
He's here today to discuss
his new book titled

00:01:00.570 --> 00:01:03.510
"Beyond Snowden: Privacy,
Mass Surveillance,

00:01:03.510 --> 00:01:05.670
and the Struggle
to Reform the NSA."

00:01:05.670 --> 00:01:08.170
Please join me in welcoming
Mr. Timothy Edgar to Google.

00:01:10.811 --> 00:01:12.810
TIMOTHY EDGAR: Well, thank
you very much, David,

00:01:12.810 --> 00:01:14.560
and thank you to
Google for having me

00:01:14.560 --> 00:01:17.550
for this talk on privacy,
mass surveillance,

00:01:17.550 --> 00:01:19.950
and the struggle
to reform the NSA.

00:01:19.950 --> 00:01:24.450
And I'll be talking about my
new book, "Beyond Snowden."

00:01:24.450 --> 00:01:26.640
And first I want to
start out with thoughts

00:01:26.640 --> 00:01:29.040
about making a
difference, and what

00:01:29.040 --> 00:01:34.660
it takes to make a difference
in our age of mass surveillance.

00:01:34.660 --> 00:01:37.320
Now, I had a theory that I could
make a difference by working

00:01:37.320 --> 00:01:38.910
within the surveillance
state, and I

00:01:38.910 --> 00:01:47.520
tried to do that for a period
of about 2005, 2006, up to 2013.

00:01:47.520 --> 00:01:50.430
And there was another person
inside the surveillance state

00:01:50.430 --> 00:01:53.000
at that time who I
actually never met,

00:01:53.000 --> 00:01:57.390
and he had a different idea
about how to make a difference.

00:01:57.390 --> 00:01:59.670
His name was Edward Snowden.

00:01:59.670 --> 00:02:02.340
And instead of working
within the surveillance state

00:02:02.340 --> 00:02:05.370
to try to reform it from
the inside the way I did,

00:02:05.370 --> 00:02:09.270
he decided to make this
extraordinary decision

00:02:09.270 --> 00:02:12.780
to steal troves of
classified documents,

00:02:12.780 --> 00:02:15.870
go off to China and then
Russia, and kind of blow up

00:02:15.870 --> 00:02:17.590
the whole system.

00:02:17.590 --> 00:02:21.150
So I want to talk a little
bit about which tactic worked

00:02:21.150 --> 00:02:24.130
better, which was
more effective,

00:02:24.130 --> 00:02:28.850
and then think about what
we can do going forward,

00:02:28.850 --> 00:02:33.250
as the book is entitled
"Beyond Snowden."

00:02:33.250 --> 00:02:36.270
So to not hide the ball, I'm
going to give you my conclusion

00:02:36.270 --> 00:02:38.040
up front.

00:02:38.040 --> 00:02:40.980
The results that I had
working within the system

00:02:40.980 --> 00:02:42.390
were results that I'm proud of.

00:02:42.390 --> 00:02:44.970
We made some significant
achievements.

00:02:44.970 --> 00:02:50.250
We took some Bush Era, post 9/11
domestic surveillance programs,

00:02:50.250 --> 00:02:53.310
or at least programs that
surveilled domestic data,

00:02:53.310 --> 00:02:57.870
we put them under the
framework of the secret Foreign

00:02:57.870 --> 00:03:01.060
Intelligence Surveillance
Court, briefed Congress on them.

00:03:01.060 --> 00:03:02.640
We brought them
within, basically,

00:03:02.640 --> 00:03:06.674
the framework of our Foreign
Intelligence Surveillance Act.

00:03:06.674 --> 00:03:08.340
We also made some
important improvements

00:03:08.340 --> 00:03:10.440
in oversight of those programs.

00:03:10.440 --> 00:03:13.050
I worked personally
on those improvements

00:03:13.050 --> 00:03:16.410
to create what I call a
middle way between purely

00:03:16.410 --> 00:03:19.590
domestic surveillance, where
you have individual court

00:03:19.590 --> 00:03:22.830
orders of wiretaps, and the
kind of foreign surveillance

00:03:22.830 --> 00:03:24.960
the NSA does with
far less oversight.

00:03:24.960 --> 00:03:27.180
We did something kind
of in the middle--

00:03:27.180 --> 00:03:29.280
the prism and upstream
collection programs

00:03:29.280 --> 00:03:31.850
you may have heard of--

00:03:31.850 --> 00:03:35.010
and we designed the
oversight system for that.

00:03:35.010 --> 00:03:38.520
But I have to admit, we really
missed the broader impact

00:03:38.520 --> 00:03:40.710
of all of these mass
surveillance programs

00:03:40.710 --> 00:03:43.230
on the privacy not only
of the United States,

00:03:43.230 --> 00:03:44.610
but of the whole world.

00:03:44.610 --> 00:03:47.190
And the reason that we
missed that broader impact

00:03:47.190 --> 00:03:50.720
is that we were working within
a very outdated legal framework,

00:03:50.720 --> 00:03:54.630
a framework that really is from
the analog age of the 1970s,

00:03:54.630 --> 00:03:59.520
and the main goal of which is to
prevent abusive domestic spying

00:03:59.520 --> 00:04:00.710
for political purposes.

00:04:00.710 --> 00:04:05.370
It's not to safeguard
privacy in a global age

00:04:05.370 --> 00:04:07.230
of mass surveillance.

00:04:07.230 --> 00:04:09.210
So what were the results
that Edward Snowden

00:04:09.210 --> 00:04:14.880
had in 2013 with his massive
revelations of NSA spying?

00:04:14.880 --> 00:04:19.310
Well, four years later,
we can render somewhat

00:04:19.310 --> 00:04:22.460
of a verdict on the results
of the Snowden revelations.

00:04:22.460 --> 00:04:26.060
First, there was an absolute
revolution in transparency.

00:04:26.060 --> 00:04:30.210
And we'll talk in more detail in
just a few moments about that.

00:04:30.210 --> 00:04:33.962
But this isn't just Snowden's
own leaks of information.

00:04:33.962 --> 00:04:35.420
Obviously, that
made those programs

00:04:35.420 --> 00:04:37.970
that he leaked transparent,
but the government

00:04:37.970 --> 00:04:40.950
fought back with
more transparency,

00:04:40.950 --> 00:04:42.320
which was a bit of a surprise.

00:04:42.320 --> 00:04:43.880
Now, they did this
in a tactical way

00:04:43.880 --> 00:04:46.374
to try to defend themselves
against accusations.

00:04:46.374 --> 00:04:47.790
These programs
were out of control

00:04:47.790 --> 00:04:50.880
and didn't have any
privacy safeguards.

00:04:50.880 --> 00:04:54.140
But I would argue
that this has created

00:04:54.140 --> 00:04:57.500
a lasting change in the culture
of secrecy of the intelligence

00:04:57.500 --> 00:04:59.120
community, although
a fragile one,

00:04:59.120 --> 00:05:01.370
and there's a real danger
of going back to bad habits.

00:05:01.370 --> 00:05:03.710
So a real revolution there.

00:05:03.710 --> 00:05:07.130
An end to a domestic
bulk collection program--

00:05:07.130 --> 00:05:09.927
in 2015, Congress
basically took this program

00:05:09.927 --> 00:05:11.510
that I'd worked on
for many years that

00:05:11.510 --> 00:05:13.940
was kept secret from
the American people,

00:05:13.940 --> 00:05:15.470
and decided to end it.

00:05:15.470 --> 00:05:18.620
It had not proved as
effective as we had thought,

00:05:18.620 --> 00:05:22.430
and there was a better way
to collect telephone records.

00:05:22.430 --> 00:05:25.210
That was a major reform
to the Patriot Act.

00:05:25.210 --> 00:05:27.200
And reforms to the secret
Foreign Intelligence

00:05:27.200 --> 00:05:30.890
Surveillance Court that
resulted from the reaction

00:05:30.890 --> 00:05:34.160
to Snowden, including
appointing outside lawyers

00:05:34.160 --> 00:05:38.730
to argue the other side in
cases that affected privacy.

00:05:38.730 --> 00:05:41.960
But I think that the most
important change that

00:05:41.960 --> 00:05:46.010
resulted from Snowden's decision
to leak classified information

00:05:46.010 --> 00:05:50.570
in 2013 was a change to our
entire conception of privacy

00:05:50.570 --> 00:05:54.080
that the government and the
intelligence community used.

00:05:54.080 --> 00:05:56.690
For decades, privacy
meant Americans.

00:05:56.690 --> 00:05:59.180
When I was in government,
when I told people

00:05:59.180 --> 00:06:01.995
that I worked on privacy
and civil liberties issues,

00:06:01.995 --> 00:06:03.620
the first thing that
came to their mind

00:06:03.620 --> 00:06:06.600
was, well, how much
data about Americans

00:06:06.600 --> 00:06:08.450
do we have in this program?

00:06:08.450 --> 00:06:12.080
If it's a fair amount, then
we'll let Tim look at it.

00:06:12.080 --> 00:06:14.540
If this is mainly a
foreign program, you know,

00:06:14.540 --> 00:06:17.810
this really isn't something
for the privacy office.

00:06:17.810 --> 00:06:22.369
Now we have rules to protect
the privacy of foreigners.

00:06:22.369 --> 00:06:24.410
Now, these rules-- I don't
want to oversell them.

00:06:24.410 --> 00:06:26.210
They're relatively modest.

00:06:26.210 --> 00:06:28.280
But the fact that
there are rules at all

00:06:28.280 --> 00:06:32.270
in a policy directive that
President Obama issued roughly

00:06:32.270 --> 00:06:35.870
in 2014, implemented
a year later,

00:06:35.870 --> 00:06:38.810
is really a revolution for
the intelligence community.

00:06:38.810 --> 00:06:40.280
So what's the verdict here?

00:06:40.280 --> 00:06:41.930
Well, I think the
verdict is obvious.

00:06:41.930 --> 00:06:44.480
Snowden's way was
much more damaging,

00:06:44.480 --> 00:06:46.670
but it was also
much more effective

00:06:46.670 --> 00:06:50.180
than my way of working
inside the system.

00:06:50.180 --> 00:06:52.760
So the question I want
to ask and I talk about

00:06:52.760 --> 00:06:54.734
in the book is,
what are we going

00:06:54.734 --> 00:06:55.900
to do with that information?

00:06:55.900 --> 00:06:58.100
Can we do better than that?

00:06:58.100 --> 00:07:00.320
Are we just going to wait
around for another 10 years

00:07:00.320 --> 00:07:02.660
and hope that another
Snowden emerges

00:07:02.660 --> 00:07:05.270
to tell us about
mass surveillance

00:07:05.270 --> 00:07:08.010
programs in the future?

00:07:08.010 --> 00:07:09.890
Or is there a way
to reform the system

00:07:09.890 --> 00:07:12.140
so that people like
me on the inside

00:07:12.140 --> 00:07:16.640
have a fighting chance to
ensure effective protections

00:07:16.640 --> 00:07:22.380
of privacy without the need
for a set of massive leaks?

00:07:22.380 --> 00:07:24.340
I'm going to go back
in time a little bit

00:07:24.340 --> 00:07:27.970
to talk a little bit about the
birth of the mass surveillance

00:07:27.970 --> 00:07:28.800
state.

00:07:28.800 --> 00:07:33.550
And that's going to bring us
back to September 11, 2001.

00:07:33.550 --> 00:07:36.260
A few weeks after
that terrible attack,

00:07:36.260 --> 00:07:38.920
I was an ACLU lawyer
in Washington,

00:07:38.920 --> 00:07:42.310
and we were gathered on
the steps of the Capitol,

00:07:42.310 --> 00:07:45.340
trying to persuade lawmakers
as they walked over

00:07:45.340 --> 00:07:48.970
to vote for the Patriot Act, a
new anti-terrorism surveillance

00:07:48.970 --> 00:07:52.210
law, that they really ought to
stop and think about what they

00:07:52.210 --> 00:07:56.380
were doing, that the powers
that they were voting to give

00:07:56.380 --> 00:07:58.870
the government had not
been fully thought through,

00:07:58.870 --> 00:08:01.720
and that we needed to take
some time to figure out

00:08:01.720 --> 00:08:04.990
how to better protect our
country while still preserving

00:08:04.990 --> 00:08:06.760
privacy and civil liberties.

00:08:06.760 --> 00:08:08.590
We didn't get much of a hearing.

00:08:08.590 --> 00:08:10.360
The law passed overwhelmingly.

00:08:10.360 --> 00:08:13.480
Both parties voted
overwhelmingly for it.

00:08:13.480 --> 00:08:16.960
And back in the
office, our director,

00:08:16.960 --> 00:08:20.800
Laura Murphy, who you can
see here in this slide,

00:08:20.800 --> 00:08:24.130
was asking the question, are
we becoming a surveillance

00:08:24.130 --> 00:08:25.820
society?

00:08:25.820 --> 00:08:29.077
And it was a real
question that we, frankly,

00:08:29.077 --> 00:08:30.160
didn't know the answer to.

00:08:30.160 --> 00:08:31.868
Because we didn't know
how the government

00:08:31.868 --> 00:08:35.080
was planning to
use its new powers.

00:08:35.080 --> 00:08:38.289
So we got together
with other groups

00:08:38.289 --> 00:08:42.039
to try to imagine
hypothetical scenarios that

00:08:42.039 --> 00:08:46.520
would illustrate the dangers
of these new spying powers.

00:08:46.520 --> 00:08:49.300
One of the examples
we used was based

00:08:49.300 --> 00:08:56.020
on a real historical example,
when the FBI had used earlier

00:08:56.020 --> 00:08:58.690
surveillance powers to go and
try to get people's library

00:08:58.690 --> 00:09:00.580
records to uncover
people that might

00:09:00.580 --> 00:09:03.670
be susceptible to
becoming radicalized

00:09:03.670 --> 00:09:06.100
and spying for the Soviet Union.

00:09:06.100 --> 00:09:08.946
So we worked with librarians
to call attention to the fact

00:09:08.946 --> 00:09:11.320
that under the Patriot Act,
there were these broad powers

00:09:11.320 --> 00:09:13.720
to get records, any
kinds of records,

00:09:13.720 --> 00:09:16.330
in order to prevent terrorism.

00:09:16.330 --> 00:09:21.370
Now, the government at that time
kind of ridiculed these fears.

00:09:21.370 --> 00:09:23.590
Bush's Attorney,
General John Ashcroft,

00:09:23.590 --> 00:09:26.380
said that we were
trying to scare people

00:09:26.380 --> 00:09:30.160
with phantoms of lost liberty--

00:09:30.160 --> 00:09:32.620
phantoms of lost liberty.

00:09:32.620 --> 00:09:37.060
And I remember thinking
a little bit, you know,

00:09:37.060 --> 00:09:40.000
actually, the attorney
general has a point.

00:09:40.000 --> 00:09:42.130
We don't know how
these surveillance

00:09:42.130 --> 00:09:45.500
powers are being used.

00:09:45.500 --> 00:09:49.780
And so we were, in a
sense, fighting phantoms.

00:09:49.780 --> 00:09:53.560
In 2005 and 2006, I
had an opportunity

00:09:53.560 --> 00:09:57.520
to go inside the
surveillance state.

00:09:57.520 --> 00:09:59.950
The intelligence community
was being reorganized in order

00:09:59.950 --> 00:10:01.990
to better share
information, and a lot

00:10:01.990 --> 00:10:04.600
of the reason it was
being reorganized

00:10:04.600 --> 00:10:06.460
was to better share
information that

00:10:06.460 --> 00:10:08.350
was domestic with
information that

00:10:08.350 --> 00:10:11.950
was foreign about
terrorist organizations.

00:10:11.950 --> 00:10:14.680
And so there are civil liberties
concerns, privacy concerns

00:10:14.680 --> 00:10:17.050
with doing that.

00:10:17.050 --> 00:10:19.390
When they created the
new director of National

00:10:19.390 --> 00:10:22.340
Intelligence to be the head
of the intelligence community,

00:10:22.340 --> 00:10:25.120
we had a victory at the
ACLU and other groups,

00:10:25.120 --> 00:10:30.250
because we created, in that
office, a privacy official that

00:10:30.250 --> 00:10:32.620
would report directly to
the head of the intelligence

00:10:32.620 --> 00:10:34.120
community.

00:10:34.120 --> 00:10:38.380
And much to my surprise, they
looked to me and said, Tim,

00:10:38.380 --> 00:10:40.360
would you like to
come work for us

00:10:40.360 --> 00:10:44.260
and be the second
employee of this privacy

00:10:44.260 --> 00:10:47.200
office, the first deputy for
civil liberties in the history

00:10:47.200 --> 00:10:48.545
of the intelligence community?

00:10:48.545 --> 00:10:50.170
And I thought long
and hard about this.

00:10:50.170 --> 00:10:51.970
This is 2006.

00:10:51.970 --> 00:10:54.310
My first interview for
the job, by the way,

00:10:54.310 --> 00:10:57.590
at the headquarters of the old
CIA building in Washington,

00:10:57.590 --> 00:11:01.150
was the same day that the
New York Times broke a story

00:11:01.150 --> 00:11:05.380
about Bush bypassing the Foreign
Intelligence Surveillance Act

00:11:05.380 --> 00:11:07.640
and authorizing
warrantless wiretapping.

00:11:07.640 --> 00:11:10.150
So this gave me a
little pause as to going

00:11:10.150 --> 00:11:12.430
into the Bush Administration.

00:11:12.430 --> 00:11:15.430
But I decided I really just
couldn't resist the chance

00:11:15.430 --> 00:11:17.440
to find out about
all these phantoms

00:11:17.440 --> 00:11:19.000
that we have been
fighting, and see

00:11:19.000 --> 00:11:23.530
if I could make a
difference on the inside.

00:11:23.530 --> 00:11:24.850
What did I learn?

00:11:24.850 --> 00:11:27.970
Well, I learned that there
had been a transformation

00:11:27.970 --> 00:11:31.960
in our intelligence apparatus.

00:11:31.960 --> 00:11:35.410
And it wasn't just about
international terrorism

00:11:35.410 --> 00:11:37.420
or Al-Qaeda or 9/11.

00:11:37.420 --> 00:11:40.990
It was also about the internet
and the transformation

00:11:40.990 --> 00:11:46.300
of technology over the
past couple of decades.

00:11:46.300 --> 00:11:48.310
The map that you
see up there shows

00:11:48.310 --> 00:11:52.780
what the NSA, at the time,
was calling its home field

00:11:52.780 --> 00:11:54.580
advantage.

00:11:54.580 --> 00:11:57.790
Look at all that data flowing
right through the United

00:11:57.790 --> 00:12:00.070
States of America.

00:12:00.070 --> 00:12:04.900
And yet, our laws
basically provided

00:12:04.900 --> 00:12:07.630
court review and
protections for this data,

00:12:07.630 --> 00:12:09.310
based on the theory
that this was

00:12:09.310 --> 00:12:12.460
potentially domestic
surveillance, even if the data

00:12:12.460 --> 00:12:14.110
belonged to foreign people.

00:12:14.110 --> 00:12:16.900
But it turns out there were
some loopholes and weaknesses

00:12:16.900 --> 00:12:19.930
in the way those laws worked,
and that's because of the way

00:12:19.930 --> 00:12:21.820
the Supreme Court
had interpreted

00:12:21.820 --> 00:12:23.720
the Fourth Amendment.

00:12:23.720 --> 00:12:25.707
Foreign people outside
the United States,

00:12:25.707 --> 00:12:27.290
the Supreme Court
had said, they don't

00:12:27.290 --> 00:12:30.740
have rights under the Fourth
Amendment to the Constitution.

00:12:30.740 --> 00:12:34.530
And also, data that isn't
the content of communication,

00:12:34.530 --> 00:12:36.590
even if you're an American,
if the data is not

00:12:36.590 --> 00:12:38.660
the content of
communication, if it's only

00:12:38.660 --> 00:12:42.530
the metadata, that also
doesn't have Fourth Amendment

00:12:42.530 --> 00:12:43.880
protection.

00:12:43.880 --> 00:12:48.770
So using creative lawyering,
the Bush Administration

00:12:48.770 --> 00:12:52.640
helped to create an innovative
series of mass surveillance

00:12:52.640 --> 00:12:56.330
programs that took advantage
of this home field advantage.

00:12:56.330 --> 00:12:58.100
Bulk collection
of metadata-- that

00:12:58.100 --> 00:13:01.400
means collecting all
of it, analyzing it

00:13:01.400 --> 00:13:03.920
for patterns that
would hopefully

00:13:03.920 --> 00:13:05.780
uncover terrorist groups.

00:13:05.780 --> 00:13:09.290
And then going after the
content of communications

00:13:09.290 --> 00:13:11.600
where the target,
the direct target

00:13:11.600 --> 00:13:14.430
is someone who's a foreign
person outside the United

00:13:14.430 --> 00:13:16.940
States, but they
might be incidentally,

00:13:16.940 --> 00:13:20.330
as we say, talking to Americans.

00:13:20.330 --> 00:13:24.200
So that was the home field
advantage of the NSA.

00:13:24.200 --> 00:13:26.630
Our job, really, was to
take these programs that

00:13:26.630 --> 00:13:30.620
started very quickly after
9/11 in enormous secrecy,

00:13:30.620 --> 00:13:33.620
bypassing altogether the Foreign
Intelligence Surveillance

00:13:33.620 --> 00:13:36.380
Court that's supposed to review
this kind of data gathering,

00:13:36.380 --> 00:13:39.110
and putting that under
checks and balances.

00:13:39.110 --> 00:13:41.930
But we did it almost
entirely in secret.

00:13:41.930 --> 00:13:44.930
Now, there was that leak
that I told you about,

00:13:44.930 --> 00:13:46.940
the Bush Warrantless
Wiretapping Program,

00:13:46.940 --> 00:13:49.160
but major parts of the
program remained secret

00:13:49.160 --> 00:13:52.400
while we were in the process
of institutionalizing it

00:13:52.400 --> 00:13:55.670
under all three
branches of government.

00:13:55.670 --> 00:13:59.370
Along comes Obama in 2008.

00:13:59.370 --> 00:14:05.200
We all remember that hopeful
period in American history.

00:14:05.200 --> 00:14:10.710
And one of the things he
did during his campaign

00:14:10.710 --> 00:14:15.330
was to promise a review of
all NSA surveillance programs.

00:14:15.330 --> 00:14:19.530
It's part of a broader reset
of the whole war on terror.

00:14:19.530 --> 00:14:22.120
He said, of course we
need to fight terrorism.

00:14:22.120 --> 00:14:26.040
But we shouldn't sacrifice our
rights and liberties to do so.

00:14:26.040 --> 00:14:29.730
And his rhetoric was
inspiring to many.

00:14:29.730 --> 00:14:32.940
However, some people
might have noticed

00:14:32.940 --> 00:14:36.360
that in the summer of 2008
he had voted to approve

00:14:36.360 --> 00:14:38.610
a broad NSA surveillance law.

00:14:38.610 --> 00:14:40.080
Now, he did it
very thoughtfully,

00:14:40.080 --> 00:14:42.420
as we would expect from the
former constitutional law

00:14:42.420 --> 00:14:45.270
professor, with a long
post to his supporters

00:14:45.270 --> 00:14:48.340
who were disappointed,
explaining why he did that.

00:14:48.340 --> 00:14:51.810
And a lot of it had to do with
his main concern being checks

00:14:51.810 --> 00:14:55.740
and balances, saying that
President Bush had put too

00:14:55.740 --> 00:14:57.660
much power on the president.

00:14:57.660 --> 00:15:00.330
He thought Congress should
authorize these programs,

00:15:00.330 --> 00:15:02.880
courts should look
at them, and that's

00:15:02.880 --> 00:15:06.480
why he was voting for this law.

00:15:06.480 --> 00:15:09.490
Now at this time, major parts
of the program, as I mentioned,

00:15:09.490 --> 00:15:10.800
were still secret.

00:15:10.800 --> 00:15:13.710
We knew about monitoring
international telephone calls.

00:15:13.710 --> 00:15:16.350
That's what Obama had
voted for in 2008.

00:15:16.350 --> 00:15:20.040
We didn't know-- and when I say
"we," I actually don't mean we.

00:15:20.040 --> 00:15:21.020
I mean you.

00:15:21.020 --> 00:15:23.850
You didn't know about bulk
collection of metadata.

00:15:23.850 --> 00:15:29.220
I had learned about it
a few years earlier.

00:15:29.220 --> 00:15:30.630
Now when Obama
comes into office,

00:15:30.630 --> 00:15:32.941
I get a wonderful opportunity
to go to the White House

00:15:32.941 --> 00:15:35.190
to serve as the first privacy
official in the National

00:15:35.190 --> 00:15:36.980
Security Council.

00:15:36.980 --> 00:15:39.980
And I was hopeful that we would
be able to make broader changes

00:15:39.980 --> 00:15:42.170
than we had made
under President Bush,

00:15:42.170 --> 00:15:43.970
that we'd be able to
say, OK, we've put

00:15:43.970 --> 00:15:45.350
these programs under the court.

00:15:45.350 --> 00:15:46.880
We briefed them to Congress.

00:15:46.880 --> 00:15:49.790
Now let's have a
conversation about privacy.

00:15:49.790 --> 00:15:51.440
That never really happened.

00:15:51.440 --> 00:15:54.090
Obama was briefed on
the program, said, well,

00:15:54.090 --> 00:15:56.520
if the Court's OK with
them, I'm OK with them.

00:15:59.590 --> 00:16:01.340
Fast forward to 2013.

00:16:01.340 --> 00:16:05.160
I'm beginning to wrap up
my government service,

00:16:05.160 --> 00:16:07.770
looking at working for Brown.

00:16:07.770 --> 00:16:09.540
Meanwhile, Mr.
Snowden is thinking

00:16:09.540 --> 00:16:13.250
about doing something
much more dramatic

00:16:13.250 --> 00:16:14.910
in leaving the government.

00:16:14.910 --> 00:16:19.170
And along comes my former
boss, Jim Klapper, the Director

00:16:19.170 --> 00:16:23.100
of National Intelligence, in
an open hearing in the Senate,

00:16:23.100 --> 00:16:25.950
and he's asked a question
by Senator Ron Wyden.

00:16:25.950 --> 00:16:27.900
He says, does the
NSA collect records

00:16:27.900 --> 00:16:32.060
on millions or hundreds
of millions of Americans?

00:16:32.060 --> 00:16:34.590
Now, this is a very
loaded question.

00:16:34.590 --> 00:16:36.730
And it's a question that
Senator Wyden already

00:16:36.730 --> 00:16:38.140
knows the answer to.

00:16:38.140 --> 00:16:39.970
He's been briefed
on the programs.

00:16:39.970 --> 00:16:43.360
So has everyone else on
the intelligence committee.

00:16:43.360 --> 00:16:46.390
And in fact, that was part
of the checks and balances

00:16:46.390 --> 00:16:47.620
that we had worked on.

00:16:47.620 --> 00:16:49.270
But Wyden is upset.

00:16:49.270 --> 00:16:52.510
He can't get anyone's
attention, which makes sense.

00:16:52.510 --> 00:16:53.800
It's a secret program.

00:16:53.800 --> 00:16:56.080
So he decides to
grill the director

00:16:56.080 --> 00:16:58.060
of National
Intelligence in public,

00:16:58.060 --> 00:17:00.850
asking him a tough question.

00:17:00.850 --> 00:17:03.070
Now, of course, this
puts General Klapper

00:17:03.070 --> 00:17:05.609
in a rather difficult position.

00:17:05.609 --> 00:17:08.240
He could answer the
question honestly

00:17:08.240 --> 00:17:13.920
and say, well, Senator, as
you know, the answer is yes.

00:17:13.920 --> 00:17:16.130
That would reveal the
existence of the program.

00:17:16.130 --> 00:17:20.750
He could also say, I can neither
confirm nor deny whether or not

00:17:20.750 --> 00:17:22.550
the NSA collects
hundreds of millions

00:17:22.550 --> 00:17:24.140
of records of Americans.

00:17:24.140 --> 00:17:28.140
That would also kind of imply
that the answer is yes--

00:17:28.140 --> 00:17:31.010
one of the problems with
the neither confirm nor deny

00:17:31.010 --> 00:17:32.520
language.

00:17:32.520 --> 00:17:37.310
So instead he says,
no, sir, not wittingly.

00:17:37.310 --> 00:17:40.380
Now, as we all know,
that was not true.

00:17:40.380 --> 00:17:43.370
There are a variety of
arguments as to exactly

00:17:43.370 --> 00:17:44.990
what was in Klapper's mind.

00:17:44.990 --> 00:17:47.930
Only Klapper knows the
answer to that question.

00:17:47.930 --> 00:17:52.610
I talk about in the book the way
in which, in a very unique way,

00:17:52.610 --> 00:17:57.050
this one person was whipsawed
by competing obligations.

00:17:57.050 --> 00:17:59.150
Obviously, he has an
obligation to tell

00:17:59.150 --> 00:18:02.240
the truth and the whole
truth in sworn testimony.

00:18:02.240 --> 00:18:04.640
But he also has the
direct obligation

00:18:04.640 --> 00:18:06.560
to keep the government secrets.

00:18:06.560 --> 00:18:09.290
Because the government had
decided to keep this secret,

00:18:09.290 --> 00:18:12.830
that put him in the tough
spot that he was in.

00:18:12.830 --> 00:18:15.390
Well, in just a
couple of months,

00:18:15.390 --> 00:18:18.200
the first stories start to
appear from Snowden's trove

00:18:18.200 --> 00:18:19.370
of documents.

00:18:19.370 --> 00:18:21.140
And we find out that
the truth was not,

00:18:21.140 --> 00:18:22.760
no, sir, not wittingly.

00:18:22.760 --> 00:18:25.160
The truth was, yes we scan.

00:18:28.550 --> 00:18:32.510
It turns out, yes, we do have
programs of mass surveillance

00:18:32.510 --> 00:18:34.640
that affect millions
and hundreds of millions

00:18:34.640 --> 00:18:36.650
of Americans.

00:18:36.650 --> 00:18:39.140
So just to summarize
very briefly,

00:18:39.140 --> 00:18:41.210
what are the things that
we learned from Snowden?

00:18:41.210 --> 00:18:42.584
Well, we learned
of the existence

00:18:42.584 --> 00:18:44.690
of this domestic bulk
collection program

00:18:44.690 --> 00:18:50.230
of telephone records, the orders
to the telephone companies.

00:18:50.230 --> 00:18:52.480
We learned about
internet surveillance,

00:18:52.480 --> 00:18:55.090
programs called upstream
collection and PRISM,

00:18:55.090 --> 00:18:58.690
also called down
stream collection.

00:18:58.690 --> 00:19:00.640
And that's the program
involving looking

00:19:00.640 --> 00:19:03.954
at foreign communications,
but incidentally,

00:19:03.954 --> 00:19:05.620
looks at Americans
who are communicating

00:19:05.620 --> 00:19:07.310
with those foreign targets.

00:19:07.310 --> 00:19:08.920
That's the content program.

00:19:08.920 --> 00:19:10.300
But we learn a lot more.

00:19:10.300 --> 00:19:12.880
We learn about many
other NSA programs

00:19:12.880 --> 00:19:15.160
that undermine the
security of communications

00:19:15.160 --> 00:19:15.890
around the world.

00:19:15.890 --> 00:19:19.510
Now, in a way, this is
not really a surprise.

00:19:19.510 --> 00:19:21.970
This is what the
NSA's job is, is

00:19:21.970 --> 00:19:24.010
to break into communications.

00:19:24.010 --> 00:19:26.410
It's just that a lot of people
didn't know how good they

00:19:26.410 --> 00:19:29.370
were at their job.

00:19:29.370 --> 00:19:33.660
So maybe President Obama has
encouraged a lot of people

00:19:33.660 --> 00:19:37.200
to understand that
their agencies actually

00:19:37.200 --> 00:19:38.970
have a lot of dedicated
public servants

00:19:38.970 --> 00:19:41.010
that do their job
really, really well,

00:19:41.010 --> 00:19:42.930
like breaking into
communications.

00:19:42.930 --> 00:19:45.600
But we hadn't had
a public debate

00:19:45.600 --> 00:19:47.520
about how this was
affecting the privacy

00:19:47.520 --> 00:19:49.860
and security of the whole
world's communications,

00:19:49.860 --> 00:19:53.400
and that's because
of excessive secrecy.

00:19:53.400 --> 00:19:55.010
So what happened as a result?

00:19:55.010 --> 00:19:58.680
Well, we ended up with
an intelligence community

00:19:58.680 --> 00:20:02.550
that reacted in a
way that is unusual.

00:20:02.550 --> 00:20:06.450
The normal reaction is,
we don't comment on leaks

00:20:06.450 --> 00:20:07.740
of classified information.

00:20:07.740 --> 00:20:10.350
We don't confirm programs.

00:20:10.350 --> 00:20:13.020
We don't talk about it.

00:20:13.020 --> 00:20:14.820
This didn't work.

00:20:14.820 --> 00:20:19.222
There was just too much
political heat for Jim Klapper

00:20:19.222 --> 00:20:20.680
to be able to go
out there and say,

00:20:20.680 --> 00:20:22.221
I'm not going to
comment on whether I

00:20:22.221 --> 00:20:24.450
was lying to the Senate
a couple of months ago.

00:20:24.450 --> 00:20:26.970
I'm not going to comment on all
these massive stories that's

00:20:26.970 --> 00:20:28.620
got everybody up in arms.

00:20:28.620 --> 00:20:31.140
So they decided to do
the opposite, which

00:20:31.140 --> 00:20:35.760
is to release thousands
of pages of documents.

00:20:35.760 --> 00:20:40.680
A lot of people don't know
this, but by March of 2014,

00:20:40.680 --> 00:20:46.380
the government had released
double the documents about NSA

00:20:46.380 --> 00:20:49.170
surveillance that had been
leaked by Edward Snowden,

00:20:49.170 --> 00:20:53.130
on its own transparency website
called IC on the Record.

00:20:53.130 --> 00:20:57.450
Now, these documents were
mostly orders and rules

00:20:57.450 --> 00:21:00.510
about how this surveillance
could be conducted,

00:21:00.510 --> 00:21:02.550
what the privacy
oversight was, things

00:21:02.550 --> 00:21:04.720
that I'd worked on for years.

00:21:04.720 --> 00:21:07.410
Many of them were opinions
of the Foreign Intelligence

00:21:07.410 --> 00:21:11.022
Surveillance Court, redacted
in some places, but really,

00:21:11.022 --> 00:21:12.480
tremendous amount
of detail that we

00:21:12.480 --> 00:21:14.370
learned from those documents.

00:21:14.370 --> 00:21:17.670
And then going forward,
issuing an annual intelligence

00:21:17.670 --> 00:21:20.910
transparency report with
details that the government had

00:21:20.910 --> 00:21:23.640
for decades argued
were classified, now

00:21:23.640 --> 00:21:26.670
made public on an annual basis.

00:21:26.670 --> 00:21:28.860
But it went beyond transparency.

00:21:28.860 --> 00:21:32.700
It also resulted in
significant accountability

00:21:32.700 --> 00:21:38.500
that actually, I would argue,
strengthened intelligence.

00:21:38.500 --> 00:21:40.430
Now this is my great image.

00:21:40.430 --> 00:21:45.240
Stole it from the Electronic
Frontier Foundation, EFF,

00:21:45.240 --> 00:21:47.170
all your data.

00:21:47.170 --> 00:21:52.000
I actually think that
this may, in some ways,

00:21:52.000 --> 00:21:55.060
not do exactly what the EFF
wants, because it kind of makes

00:21:55.060 --> 00:21:57.050
the NSA look pretty cool.

00:21:57.050 --> 00:22:02.640
Anyway, Congress
in 2015 was looking

00:22:02.640 --> 00:22:05.880
at whether to renew the
legal authority, the Patriot

00:22:05.880 --> 00:22:11.130
Act, that allows them to collect
in bulk all this metadata.

00:22:11.130 --> 00:22:13.560
And essentially, they
decided, no, we're

00:22:13.560 --> 00:22:15.090
going to reform the program.

00:22:15.090 --> 00:22:18.450
A major reason for that
is that a privacy board

00:22:18.450 --> 00:22:23.400
reviewed, in detail, the
government's claims about how

00:22:23.400 --> 00:22:25.740
this had helped with
terrorist attacks,

00:22:25.740 --> 00:22:28.890
and found that in each
of those instances,

00:22:28.890 --> 00:22:31.050
this massive program
of bulk collection

00:22:31.050 --> 00:22:33.970
had actually provided no
unique value to the government.

00:22:33.970 --> 00:22:38.340
Now, this is an area where
I felt quite embarrassed.

00:22:38.340 --> 00:22:41.460
I had worked for years on
oversight of this program

00:22:41.460 --> 00:22:44.130
to make sure that people
were obeying the privacy

00:22:44.130 --> 00:22:47.580
rules around access
to this bulk database.

00:22:47.580 --> 00:22:51.750
I had been told that there was
important terrorist incidents

00:22:51.750 --> 00:22:55.580
in which this bulk
database had been used,

00:22:55.580 --> 00:22:58.870
and I basically accepted
that at face value.

00:22:58.870 --> 00:23:01.350
And after the Snowden
revelations came out,

00:23:01.350 --> 00:23:04.380
I even defended them on
National Public Radio and said,

00:23:04.380 --> 00:23:06.870
you know, they've helped
to stop a certain number

00:23:06.870 --> 00:23:08.460
of terrorist attacks.

00:23:08.460 --> 00:23:10.590
When this board
looked in more detail,

00:23:10.590 --> 00:23:13.140
they found that, in fact, the
government had had that data

00:23:13.140 --> 00:23:14.470
from other sources.

00:23:14.470 --> 00:23:17.100
So it wasn't as valuable
as we had thought.

00:23:17.100 --> 00:23:20.340
That was an important reason
Congress ended that program.

00:23:20.340 --> 00:23:23.040
Now, I say ended, but it's
important to understand,

00:23:23.040 --> 00:23:25.920
they ended the bulk collection
part of the program.

00:23:25.920 --> 00:23:29.010
The NSA analysts still were
given access to this metadata

00:23:29.010 --> 00:23:32.520
but on a case by case basis.

00:23:32.520 --> 00:23:37.140
It turns out that that made
the program more effective.

00:23:37.140 --> 00:23:40.230
It gave them access to
more, rather than less,

00:23:40.230 --> 00:23:41.460
relevant data.

00:23:41.460 --> 00:23:44.130
Because without the
secrecy and the logistics

00:23:44.130 --> 00:23:46.080
of having to collect
all the data,

00:23:46.080 --> 00:23:48.300
they could look at
smaller providers.

00:23:48.300 --> 00:23:52.680
They could get a
richer source of data

00:23:52.680 --> 00:23:55.270
when they weren't
collecting all of it.

00:23:55.270 --> 00:23:59.670
So that's an important and
valuable lesson about reform.

00:23:59.670 --> 00:24:01.980
I also talked a little bit
about the secret court.

00:24:01.980 --> 00:24:07.050
The secret court really
is a genuine federal court

00:24:07.050 --> 00:24:11.820
with judges that are approved by
the Senate, lifetime of tenure.

00:24:11.820 --> 00:24:13.740
They are not quite
the rubber stamp

00:24:13.740 --> 00:24:15.690
that people think
they are, but they

00:24:15.690 --> 00:24:19.410
get skewed by the existence of
a one sided argument process,

00:24:19.410 --> 00:24:22.290
which we saw very much after
the Snowden revelations had

00:24:22.290 --> 00:24:24.180
distorted the law.

00:24:24.180 --> 00:24:27.300
In the Freedom Act, one of
the important provisions

00:24:27.300 --> 00:24:29.680
in that law basically
says when there

00:24:29.680 --> 00:24:32.280
is an important legal
issue, the court

00:24:32.280 --> 00:24:34.890
should appoint another lawyer
to argue the other side

00:24:34.890 --> 00:24:37.640
of that legal issue.

00:24:37.640 --> 00:24:38.954
Interesting idea.

00:24:38.954 --> 00:24:40.370
Something I learned
in law school,

00:24:40.370 --> 00:24:43.310
tends to strengthen the
quality of judicial decisions.

00:24:43.310 --> 00:24:44.750
I think it's already done that.

00:24:44.750 --> 00:24:46.544
I think we need to
go a lot further

00:24:46.544 --> 00:24:48.710
with reforming the Foreign
Intelligence Surveillance

00:24:48.710 --> 00:24:49.840
Court.

00:24:49.840 --> 00:24:54.260
It was set up to review wiretap
applications, basically to say,

00:24:54.260 --> 00:24:57.440
is there probable cause that
you are spying for the Russians,

00:24:57.440 --> 00:25:00.020
that you are part of Al-Qaeda?

00:25:00.020 --> 00:25:02.000
That's a fairly
straightforward decision

00:25:02.000 --> 00:25:03.920
that judges make every day.

00:25:03.920 --> 00:25:05.780
Now it's overseeing
vast programs

00:25:05.780 --> 00:25:08.120
of transnational surveillance.

00:25:08.120 --> 00:25:12.020
And to have just a federal
judge and a tiny staff

00:25:12.020 --> 00:25:15.050
perform that function,
to me, is not going

00:25:15.050 --> 00:25:17.090
to provide effective oversight.

00:25:17.090 --> 00:25:19.880
I argue that instead of
just bringing in lawyers,

00:25:19.880 --> 00:25:23.480
we also need to bring in
technologists to help the FISA

00:25:23.480 --> 00:25:24.890
Court.

00:25:24.890 --> 00:25:27.560
There's been a number of
incidents in which the FISA

00:25:27.560 --> 00:25:30.710
Court has reviewed
NSA programs where

00:25:30.710 --> 00:25:33.030
the NSA made major
mistakes in how

00:25:33.030 --> 00:25:34.280
they implemented the programs.

00:25:34.280 --> 00:25:35.655
And for the most
part, these were

00:25:35.655 --> 00:25:39.830
mistakes involving what I call
failed lawyer technologist

00:25:39.830 --> 00:25:41.270
communication.

00:25:41.270 --> 00:25:43.430
The lawyer says, we've
been authorized to do this,

00:25:43.430 --> 00:25:45.260
the technologist hears
something different.

00:25:45.260 --> 00:25:47.296
The technologist comes
back later and says,

00:25:47.296 --> 00:25:48.170
this is how it works.

00:25:48.170 --> 00:25:50.690
The lawyer says, oh
my god, I've been

00:25:50.690 --> 00:25:52.250
lying to the FISA
Court for two years

00:25:52.250 --> 00:25:54.710
because I didn't realize
we were getting this data.

00:25:54.710 --> 00:25:58.790
So having technologists in
the FISA Court would help.

00:25:58.790 --> 00:26:01.110
But the most lasting
challenge was really

00:26:01.110 --> 00:26:03.900
something that
came as a result--

00:26:03.900 --> 00:26:08.400
I'd like to say it's because
of our country's strong support

00:26:08.400 --> 00:26:10.690
for the human rights of
everyone around the world,

00:26:10.690 --> 00:26:12.440
including the right
to privacy, but that's

00:26:12.440 --> 00:26:14.640
really not the reason.

00:26:14.640 --> 00:26:17.250
After the Snowden
revelations, people

00:26:17.250 --> 00:26:18.972
like me, former
intelligence officials

00:26:18.972 --> 00:26:20.430
and current
intelligence officials,

00:26:20.430 --> 00:26:22.805
went out there and said, you
have nothing to worry about.

00:26:22.805 --> 00:26:25.961
We have strong protections
for the privacy of Americans.

00:26:25.961 --> 00:26:27.460
A lot of companies
in Silicon Valley

00:26:27.460 --> 00:26:29.830
said, please stop
talking about that.

00:26:29.830 --> 00:26:31.880
This is hurting our business.

00:26:31.880 --> 00:26:34.640
We have customers and
users around the world.

00:26:34.640 --> 00:26:36.910
We don't want them all
migrating to other services.

00:26:36.910 --> 00:26:40.480
You have to do something to
reassure the rest of the world

00:26:40.480 --> 00:26:42.970
that you care about
their privacy, too.

00:26:42.970 --> 00:26:44.530
This photo here
is from a meeting

00:26:44.530 --> 00:26:48.550
that President Obama had with
ex-CEOs in December of 2013.

00:26:48.550 --> 00:26:51.160
You may recognize the
man he's talking to.

00:26:53.980 --> 00:26:57.160
So Google, Facebook,
Yahoo, others,

00:26:57.160 --> 00:27:00.460
they have a lot of political
power in a way that voters

00:27:00.460 --> 00:27:03.400
in Germany and Brazil don't.

00:27:03.400 --> 00:27:07.480
And that caused the
administration to say,

00:27:07.480 --> 00:27:09.470
we have to do
something about this.

00:27:09.470 --> 00:27:12.490
So they issued something called
Presidential Policy Directive

00:27:12.490 --> 00:27:14.170
28.

00:27:14.170 --> 00:27:16.900
That flows trippingly
off the tongue.

00:27:16.900 --> 00:27:18.400
I've been accused
sometimes of being

00:27:18.400 --> 00:27:20.740
a wonk on surveillance issues.

00:27:20.740 --> 00:27:22.870
I won't quiz you after
this talk about what

00:27:22.870 --> 00:27:28.870
is Presidential Policy Directive
28 or FISA Section 702,

00:27:28.870 --> 00:27:30.910
but this is important.

00:27:30.910 --> 00:27:33.520
This policy directive
basically laid down

00:27:33.520 --> 00:27:36.490
a marker for the
intelligence community saying

00:27:36.490 --> 00:27:39.280
that foreign privacy
matters, that there

00:27:39.280 --> 00:27:42.370
have to be rules to protect
the personal information that

00:27:42.370 --> 00:27:44.980
is collected by
agencies like the NSA

00:27:44.980 --> 00:27:47.290
in these bulk
collection programs.

00:27:47.290 --> 00:27:52.030
And this particular
quote, to me,

00:27:52.030 --> 00:27:55.900
sums it up, that everyone has
to be treated with dignity

00:27:55.900 --> 00:27:58.210
and respect regardless
of their nationality

00:27:58.210 --> 00:28:01.210
or wherever they might
reside, and all persons

00:28:01.210 --> 00:28:02.980
have legitimate
privacy interests

00:28:02.980 --> 00:28:05.980
in the handling of their
personal information.

00:28:05.980 --> 00:28:09.070
This was a revolutionary concept
for the intelligence community.

00:28:09.070 --> 00:28:10.390
What do you mean all persons?

00:28:10.390 --> 00:28:12.090
What about-- I thought
it was just Americans.

00:28:12.090 --> 00:28:13.423
I though it was just US persons.

00:28:13.423 --> 00:28:16.150
Now I have to worry
about everybody else?

00:28:16.150 --> 00:28:20.320
So here's an area where I
don't think we've done enough

00:28:20.320 --> 00:28:21.760
on surveillance reform.

00:28:21.760 --> 00:28:25.312
And actually, in many of these
areas we haven't done enough.

00:28:25.312 --> 00:28:26.770
And that has to do
with what I call

00:28:26.770 --> 00:28:29.590
in the book,
technological magic.

00:28:29.590 --> 00:28:32.230
For years, the
intelligence community

00:28:32.230 --> 00:28:35.860
had been studying how to
use encryption and privacy

00:28:35.860 --> 00:28:39.310
preserving technologies
to extract information

00:28:39.310 --> 00:28:43.420
from massive datasets
with real privacy

00:28:43.420 --> 00:28:45.670
guarantees for all the
people's information

00:28:45.670 --> 00:28:48.040
that are in that data set.

00:28:48.040 --> 00:28:50.440
We learned after some of
the Snowden revelations

00:28:50.440 --> 00:28:55.120
that, yeah, the NSA vacuums up
huge quantities of information.

00:28:55.120 --> 00:28:58.750
Usually they have some
purpose for doing this other

00:28:58.750 --> 00:29:00.940
than looking at
everybody's information,

00:29:00.940 --> 00:29:03.010
and they use people
like me, lawyers,

00:29:03.010 --> 00:29:06.610
to try to enforce policy
constraints on the analysts.

00:29:06.610 --> 00:29:07.930
And that can be fragile.

00:29:07.930 --> 00:29:12.040
You know, people can violate
those policy constraints.

00:29:12.040 --> 00:29:14.290
Math is a lot less
fragile when it comes

00:29:14.290 --> 00:29:16.720
to those kind of guarantees.

00:29:16.720 --> 00:29:18.940
So this is a picture of
[INAUDIBLE] my colleague

00:29:18.940 --> 00:29:22.750
at Brown University,
who has pioneered

00:29:22.750 --> 00:29:26.590
many of these encrypted
search techniques.

00:29:26.590 --> 00:29:27.950
We haven't implemented them.

00:29:27.950 --> 00:29:29.200
You know, we're studying them.

00:29:29.200 --> 00:29:30.491
We're continuing to study them.

00:29:30.491 --> 00:29:32.840
Here it is, 2017, four
years after Snowden.

00:29:32.840 --> 00:29:35.740
So this is an area where
we have a lot of work

00:29:35.740 --> 00:29:39.930
to do when it comes to
surveillance reform.

00:29:39.930 --> 00:29:42.870
I want to close thinking
about the danger of abuse

00:29:42.870 --> 00:29:45.690
of these mass
surveillance capabilities.

00:29:45.690 --> 00:29:49.800
When Ed Snowden was asked
in his first interview

00:29:49.800 --> 00:29:52.480
in this hotel in Hong Kong,
well, why are you doing this?

00:29:52.480 --> 00:29:56.790
Why are you revealing all
of these secret programs

00:29:56.790 --> 00:30:02.190
and putting yourself in such
risk when you haven't really

00:30:02.190 --> 00:30:07.920
shown us examples of the kind
of domestic political abuse

00:30:07.920 --> 00:30:10.600
of surveillance that
we'd seen in the past?

00:30:10.600 --> 00:30:14.900
We didn't have examples from
Edward Snowden of, you know,

00:30:14.900 --> 00:30:16.940
Obama using
surveillance authority

00:30:16.940 --> 00:30:19.040
to go after his
political opponents

00:30:19.040 --> 00:30:21.050
in order to discover gossip.

00:30:21.050 --> 00:30:23.670
We didn't have those examples
from the Bush Administration,

00:30:23.670 --> 00:30:24.890
either.

00:30:24.890 --> 00:30:28.520
And what he said to
that was, well, we're

00:30:28.520 --> 00:30:31.580
not always going to have
enlightened leaders,

00:30:31.580 --> 00:30:32.180
essentially.

00:30:32.180 --> 00:30:36.290
He's worried about building
an architecture of oppression

00:30:36.290 --> 00:30:38.660
in which it's very fragile.

00:30:38.660 --> 00:30:42.230
Some day, he said, someone
else will be elected,

00:30:42.230 --> 00:30:45.200
and they'll flip the
switch, and this apparatus

00:30:45.200 --> 00:30:48.410
could be turned against us.

00:30:48.410 --> 00:30:52.130
Well, in 2016 a lot of
people in my old community

00:30:52.130 --> 00:30:54.800
in the national security
world were very worried

00:30:54.800 --> 00:30:58.640
that that person had been found,
and had just been elected.

00:30:58.640 --> 00:31:03.110
During the campaign-- we all
know about Donald Trump's

00:31:03.110 --> 00:31:06.080
rhetoric during the
campaign and after that

00:31:06.080 --> 00:31:08.570
seemed to denigrate important
constitutional liberties

00:31:08.570 --> 00:31:12.960
and rights, respect for
religious minorities,

00:31:12.960 --> 00:31:17.390
and just the kind of
open promises of abuse

00:31:17.390 --> 00:31:19.460
of civil liberties
we'd really never seen

00:31:19.460 --> 00:31:25.070
before from any president, even
presidents like Richard Nixon

00:31:25.070 --> 00:31:26.570
hadn't gone out
there and bragged

00:31:26.570 --> 00:31:30.274
about the kinds of abuses
that they wanted to engage in.

00:31:30.274 --> 00:31:31.940
And because of that,
people may not even

00:31:31.940 --> 00:31:36.320
have noticed that he actually
endorsed warrantless searches

00:31:36.320 --> 00:31:38.150
in his campaign.

00:31:38.150 --> 00:31:40.810
There is an interview
in which Donald Trump

00:31:40.810 --> 00:31:43.830
is asked what he's going to do
about the dangers of terrorism.

00:31:43.830 --> 00:31:46.040
He says, I want surveillance.

00:31:46.040 --> 00:31:49.250
I want surveillance of
the Muslim community.

00:31:49.250 --> 00:31:51.500
And yes, warrantless
surveillance

00:31:51.500 --> 00:31:53.550
might be necessary.

00:31:53.550 --> 00:31:56.390
Now, of course, also
the dangers of abuse

00:31:56.390 --> 00:31:59.892
are not just something
Trump might engage in.

00:31:59.892 --> 00:32:02.100
There's something that Trump
supporters have suddenly

00:32:02.100 --> 00:32:06.495
woken up and discovered in
their fears of the deep state.

00:32:09.390 --> 00:32:12.320
A few months ago,
Donald Trump himself

00:32:12.320 --> 00:32:15.140
discovered the dangers
of surveillance abuse,

00:32:15.140 --> 00:32:16.760
tweeting out, "Terrible!

00:32:16.760 --> 00:32:20.820
I just found out that
Obama had my 'wires tapped'

00:32:20.820 --> 00:32:22.920
in Trump Tower."

00:32:22.920 --> 00:32:25.350
Now, this shows a shocking
ignorance of the fact

00:32:25.350 --> 00:32:30.090
that no president under our laws
can tap the wires of someone

00:32:30.090 --> 00:32:31.280
else without--

00:32:31.280 --> 00:32:33.030
I was about to say
without their consent--

00:32:33.030 --> 00:32:37.260
without permission
from the FISA court.

00:32:37.260 --> 00:32:40.260
But it also shows
perhaps an opportunity

00:32:40.260 --> 00:32:43.140
among political
opportunity where

00:32:43.140 --> 00:32:46.620
people who may fear
Trump and his potential

00:32:46.620 --> 00:32:49.530
for abusing our
civil liberties could

00:32:49.530 --> 00:32:53.970
unite with those who support
him and are worried about people

00:32:53.970 --> 00:32:56.940
in the intelligence community
abusing their power to go

00:32:56.940 --> 00:33:01.170
after him and his supporters.

00:33:01.170 --> 00:33:05.790
I want to close by talking a
little bit about what should we

00:33:05.790 --> 00:33:08.220
do going forward.

00:33:08.220 --> 00:33:10.680
You know, part of
the reason I called

00:33:10.680 --> 00:33:13.470
the book "Beyond
Snowden" was to say

00:33:13.470 --> 00:33:17.460
that we've had a debate about
Snowden that has focused often

00:33:17.460 --> 00:33:21.780
on him and what should
we do about him.

00:33:21.780 --> 00:33:25.740
And I have said that
because of the reforms

00:33:25.740 --> 00:33:28.980
that his disclosures
caused, my view is

00:33:28.980 --> 00:33:31.620
that he should be
pardoned, even though I

00:33:31.620 --> 00:33:34.950
do agree that they also
caused significant damage

00:33:34.950 --> 00:33:36.820
to national security.

00:33:36.820 --> 00:33:38.820
But I think more
importantly than that

00:33:38.820 --> 00:33:41.910
is to reform our
surveillance apparatus.

00:33:41.910 --> 00:33:44.970
So I've offered a
detailed and rich list

00:33:44.970 --> 00:33:48.060
of policy recommendations
near the end,

00:33:48.060 --> 00:33:49.440
boiling them down a bit.

00:33:49.440 --> 00:33:52.980
We need even more transparency
about how our surveillance

00:33:52.980 --> 00:33:54.420
system works.

00:33:54.420 --> 00:33:57.150
We need laws that respect
everyone's privacy,

00:33:57.150 --> 00:33:59.280
not just the privacy
of Americans,

00:33:59.280 --> 00:34:02.220
and we need to do a lot more
to use the magic of crypto

00:34:02.220 --> 00:34:06.000
and other technology
that can help us obtain

00:34:06.000 --> 00:34:09.060
the data we want without
obtaining the data

00:34:09.060 --> 00:34:10.739
that we don't want.

00:34:10.739 --> 00:34:13.650
Taken together, I
would say that we

00:34:13.650 --> 00:34:18.449
need a serious
overhaul of how we

00:34:18.449 --> 00:34:21.239
protect privacy and civil
liberties in the age

00:34:21.239 --> 00:34:23.170
of mass surveillance.

00:34:23.170 --> 00:34:27.659
And this puts me sort of
right smack in the middle,

00:34:27.659 --> 00:34:30.840
on the very, very far edge
of the national security

00:34:30.840 --> 00:34:33.840
community, which finds
many of my recommendations

00:34:33.840 --> 00:34:38.010
unworkable, but sometimes
on the other side

00:34:38.010 --> 00:34:41.699
of civil libertarians who
tell me, Tim, you know,

00:34:41.699 --> 00:34:43.739
we just need to junk
all these programs.

00:34:43.739 --> 00:34:48.060
We're never going to make
them safe for democracy.

00:34:48.060 --> 00:34:52.230
And my view on that is
simply that the programs

00:34:52.230 --> 00:34:56.159
are, in some cases, very
useful for national security.

00:34:56.159 --> 00:34:58.860
And one thing we
learned from the debate

00:34:58.860 --> 00:35:01.020
is that the public,
for the most part,

00:35:01.020 --> 00:35:03.750
wants the intelligence
community to have these robots

00:35:03.750 --> 00:35:04.440
capabilities.

00:35:04.440 --> 00:35:06.581
They just want more oversight.

00:35:06.581 --> 00:35:08.580
And so that's what I've
tried to do in the book,

00:35:08.580 --> 00:35:10.632
is tell us how we can do that.

00:35:10.632 --> 00:35:11.590
So thank you very much.

00:35:17.278 --> 00:35:19.820
AUDIENCE: So in
addition to data being

00:35:19.820 --> 00:35:24.230
used for political
purposes, what

00:35:24.230 --> 00:35:27.440
other major concerns should
we have, as Americans,

00:35:27.440 --> 00:35:35.028
for our data being in
the hands of the NSA?

00:35:35.028 --> 00:35:38.740
TIMOTHY EDGAR: Well, I
think abuse of our data

00:35:38.740 --> 00:35:42.810
is something we've
seen does happen.

00:35:42.810 --> 00:35:46.300
It certainly
happened in the past.

00:35:46.300 --> 00:35:50.170
And it could be used for
a variety of bad reasons.

00:35:50.170 --> 00:35:54.760
It could be used by an
unscrupulous politician

00:35:54.760 --> 00:35:57.790
to try to leak
damaging information

00:35:57.790 --> 00:35:59.170
for political purposes.

00:35:59.170 --> 00:36:02.230
It could be used to
intimidate a whistleblower.

00:36:02.230 --> 00:36:06.580
It could be used just for
overbroad invasion of privacy.

00:36:06.580 --> 00:36:08.290
When a program is
set up, in order

00:36:08.290 --> 00:36:10.630
to safeguard national
security, there

00:36:10.630 --> 00:36:13.690
can sometimes be mission
creep, where it then

00:36:13.690 --> 00:36:18.280
is used for a broader societal
interest, law enforcement,

00:36:18.280 --> 00:36:20.060
and so forth.

00:36:20.060 --> 00:36:25.030
So I think that
the real point is

00:36:25.030 --> 00:36:30.220
that we are still very early in
the 21st century when it comes

00:36:30.220 --> 00:36:33.700
to grappling with how
do we protect ourselves

00:36:33.700 --> 00:36:38.380
in this age of mass data
and of mass surveillance.

00:36:38.380 --> 00:36:42.700
And my argument is that our
laws have to reflect that.

00:36:42.700 --> 00:36:47.170
We can't just be content
with a law that says, well,

00:36:47.170 --> 00:36:50.680
if you want to get the
content of my telephone calls,

00:36:50.680 --> 00:36:53.080
or the content of
my email messages,

00:36:53.080 --> 00:36:57.040
you're going to need a
secret court warrant.

00:36:57.040 --> 00:36:58.600
What about all my metadata?

00:36:58.600 --> 00:37:00.790
What about all
that communications

00:37:00.790 --> 00:37:04.690
that you have of me talking
with people outside the country?

00:37:04.690 --> 00:37:05.920
I worry about that, too.

00:37:08.662 --> 00:37:11.490
AUDIENCE: What can you
share about the research

00:37:11.490 --> 00:37:14.310
into blockchain or smart
contract technologies

00:37:14.310 --> 00:37:17.310
to improve transparency
and privacy

00:37:17.310 --> 00:37:20.280
and maintain a level
of surveillance?

00:37:20.280 --> 00:37:22.500
TIMOTHY EDGAR: Well, both
of those technologies

00:37:22.500 --> 00:37:25.230
are very promising, and
they show different ways

00:37:25.230 --> 00:37:28.050
in which we might be able
to use new and innovative

00:37:28.050 --> 00:37:29.580
technologies.

00:37:29.580 --> 00:37:33.150
For blockchain, I guess the
area that I think of immediately

00:37:33.150 --> 00:37:34.320
is audit.

00:37:34.320 --> 00:37:40.020
I think about the possibility
of having a way of auditing how

00:37:40.020 --> 00:37:42.820
people are using data.

00:37:42.820 --> 00:37:46.500
But one of the
ironies of our failure

00:37:46.500 --> 00:37:49.230
to employ some of these
technical approaches

00:37:49.230 --> 00:37:53.700
in our efforts to protect
privacy and civil liberties

00:37:53.700 --> 00:37:56.520
from mass surveillance
is that much

00:37:56.520 --> 00:38:00.690
of the research into of
these technologies, including

00:38:00.690 --> 00:38:03.870
blockchain, including
encrypted search,

00:38:03.870 --> 00:38:05.730
was funded by the
government itself,

00:38:05.730 --> 00:38:08.190
in some cases by the
intelligence community.

00:38:08.190 --> 00:38:11.310
And yet we still haven't
really come to grips

00:38:11.310 --> 00:38:15.060
with how do we deploy these
technologies at the scale

00:38:15.060 --> 00:38:17.040
that an agency like
the NSA would use,

00:38:17.040 --> 00:38:19.770
despite the fact that we've
made real breakthroughs,

00:38:19.770 --> 00:38:23.167
as I discuss in the
book, in using them.

00:38:23.167 --> 00:38:25.250
I would need to give it a
little bit more thought,

00:38:25.250 --> 00:38:27.666
but I think that's exactly the
conversation that we should

00:38:27.666 --> 00:38:30.900
be having, is getting
people like the people

00:38:30.900 --> 00:38:33.570
here at Google together with
people from the intelligence

00:38:33.570 --> 00:38:36.990
community, and exchanging
ideas about here's

00:38:36.990 --> 00:38:38.550
the tools and tactics
and techniques

00:38:38.550 --> 00:38:43.770
that we have that are really
interesting crypto techniques.

00:38:43.770 --> 00:38:44.610
What are your needs?

00:38:44.610 --> 00:38:47.130
What are you trying
to extract from data?

00:38:47.130 --> 00:38:49.770
How are you trying to
audit against misuse,

00:38:49.770 --> 00:38:51.510
those kinds of things?

00:38:51.510 --> 00:38:54.990
We had some early meetings
when I joined the ODNI

00:38:54.990 --> 00:38:59.130
more than a decade ago, and they
funded some research programs.

00:38:59.130 --> 00:39:02.190
But we haven't done
enough to transform that

00:39:02.190 --> 00:39:08.070
into reality of deploying
these technologies.

00:39:08.070 --> 00:39:09.900
AUDIENCE: Hi,
thanks for the talk.

00:39:09.900 --> 00:39:11.110
Really interesting.

00:39:11.110 --> 00:39:14.520
This is a really
difficult question to ask,

00:39:14.520 --> 00:39:18.180
and you know, I want
to balance it and word

00:39:18.180 --> 00:39:22.050
it correctly, given
that, you know, I

00:39:22.050 --> 00:39:23.490
knew what it felt like on 9/11.

00:39:23.490 --> 00:39:25.530
I remember that so clearly.

00:39:25.530 --> 00:39:27.270
I lived in Israel
for a while and had

00:39:27.270 --> 00:39:30.090
my own fair share
of situations there

00:39:30.090 --> 00:39:33.690
that made me worry about
terrorism and threats to me

00:39:33.690 --> 00:39:35.500
or my family's life
and things like that.

00:39:35.500 --> 00:39:39.450
But a lot of analysis has
been done surrounding,

00:39:39.450 --> 00:39:45.030
like, the value of
a human life, in how

00:39:45.030 --> 00:39:46.220
the government values that.

00:39:46.220 --> 00:39:47.880
And people have done
that with the Department

00:39:47.880 --> 00:39:49.200
of Transportation, for example.

00:39:49.200 --> 00:39:52.020
How much money is invested to
making our highways safe so

00:39:52.020 --> 00:39:55.320
that we can protect the people
who use them, et cetera.

00:39:55.320 --> 00:39:57.200
And people have done this a lot.

00:39:57.200 --> 00:39:59.530
It's not an exact science
or anything like that,

00:39:59.530 --> 00:40:05.440
but given how big the security
state is, really, in the United

00:40:05.440 --> 00:40:07.440
States, and given that
you've kind of been there

00:40:07.440 --> 00:40:09.460
and you've seen
what it can prevent,

00:40:09.460 --> 00:40:10.770
what it can't prevent--

00:40:10.770 --> 00:40:14.610
you've had your fair share
of knowledge of that--

00:40:14.610 --> 00:40:16.110
what are your thoughts on that?

00:40:16.110 --> 00:40:24.890
I mean, how important do you
think it is to focus on this?

00:40:24.890 --> 00:40:26.210
Are we over invested?

00:40:26.210 --> 00:40:30.959
Are we under-invested given how
much could happen if we never

00:40:30.959 --> 00:40:32.250
had these protections in place?

00:40:32.250 --> 00:40:35.790
Also, the fact that
there are some costs

00:40:35.790 --> 00:40:37.060
associated with that.

00:40:37.060 --> 00:40:39.960
This surveillance
can really ruin faith

00:40:39.960 --> 00:40:42.534
in technology that actually
could protect people

00:40:42.534 --> 00:40:43.950
against financial
crime and things

00:40:43.950 --> 00:40:46.140
like that, because now
suddenly people don't trust

00:40:46.140 --> 00:40:49.282
giving their information away,
where it actually might protect

00:40:49.282 --> 00:40:51.240
them from things that
are more likely to happen

00:40:51.240 --> 00:40:53.040
than terrorism.

00:40:53.040 --> 00:40:55.047
So yeah, it's a
complicated question

00:40:55.047 --> 00:40:56.130
and I hope it makes sense.

00:40:56.130 --> 00:40:57.610
TIMOTHY EDGAR: No,
I think it does.

00:40:57.610 --> 00:41:00.090
And the question
really comes down to,

00:41:00.090 --> 00:41:04.150
have we overreacted to
the risk of terrorism?

00:41:04.150 --> 00:41:07.770
I think most people would say
that we have in certain ways

00:41:07.770 --> 00:41:09.550
overreacted.

00:41:09.550 --> 00:41:12.120
I think in 2017
it looks different

00:41:12.120 --> 00:41:15.550
than it did on the
morning after 9/11.

00:41:15.550 --> 00:41:18.330
We had a lot of experiencing
counter terrorism.

00:41:18.330 --> 00:41:20.910
And I think what I would
suggest is that we simply

00:41:20.910 --> 00:41:28.750
look a lot more clear-eyed at
the question of effectiveness.

00:41:28.750 --> 00:41:33.040
So two examples-- I talked to
you about the bulk collection

00:41:33.040 --> 00:41:36.130
of telephone records,
and how that program--

00:41:36.130 --> 00:41:38.680
we spent a lot of
time putting together

00:41:38.680 --> 00:41:42.280
the legal rules that should
govern access to the database,

00:41:42.280 --> 00:41:46.000
and ways to protect privacy.

00:41:46.000 --> 00:41:48.310
But this was a massive
database of call records

00:41:48.310 --> 00:41:51.370
which clearly posed
a risk to privacy.

00:41:51.370 --> 00:41:54.880
And we had some anecdotal
information, basically,

00:41:54.880 --> 00:41:57.400
about how it had been
used to prevent terrorism.

00:41:57.400 --> 00:41:59.710
But when that was put to
the test by the Privacy

00:41:59.710 --> 00:42:02.230
and Civil Liberties Oversight
Board, after the Snowden

00:42:02.230 --> 00:42:05.890
revelations, the conclusion
was, actually, this

00:42:05.890 --> 00:42:06.940
was not necessary.

00:42:06.940 --> 00:42:08.830
This whole program that
had been in existence

00:42:08.830 --> 00:42:13.330
for more than a decade did
not provide unique value

00:42:13.330 --> 00:42:15.250
in preventing any
terrorist incidents.

00:42:15.250 --> 00:42:18.220
Even though it had been used
in some terrorism cases,

00:42:18.220 --> 00:42:19.960
they already had that data.

00:42:19.960 --> 00:42:23.200
Now you contrast that to the
PRISM and Upstream collection

00:42:23.200 --> 00:42:27.400
programs, which Congress is
debating right now this fall,

00:42:27.400 --> 00:42:31.120
and the privacy board basically
came to the opposite conclusion

00:42:31.120 --> 00:42:35.350
and said, of course this program
has privacy implications,

00:42:35.350 --> 00:42:40.210
but we've examined carefully the
effectiveness of the program,

00:42:40.210 --> 00:42:42.160
at least in terms of
the terrorist incidents

00:42:42.160 --> 00:42:44.740
the government says
it has prevented,

00:42:44.740 --> 00:42:46.570
we've looked at each
case, and yes, it

00:42:46.570 --> 00:42:52.150
has provided valuable and
unique contribution to help head

00:42:52.150 --> 00:42:54.790
off those terrorist attacks.

00:42:54.790 --> 00:42:58.600
Now, you could, I suppose,
do some actuarial calculation

00:42:58.600 --> 00:43:01.300
and say, well, if those
attacks had happened,

00:43:01.300 --> 00:43:03.370
and a certain number
of people had died,

00:43:03.370 --> 00:43:05.980
this would be the damage.

00:43:05.980 --> 00:43:08.170
And then we have
to somehow assign

00:43:08.170 --> 00:43:10.120
a value to the
privacy that has been

00:43:10.120 --> 00:43:13.300
invaded by having the program.

00:43:13.300 --> 00:43:14.900
I wouldn't go that far.

00:43:14.900 --> 00:43:16.420
I think it's already
a step forward

00:43:16.420 --> 00:43:19.540
just to ask the basic
question, did it work at all?

00:43:19.540 --> 00:43:24.100
We've seen that we can get a lot
more privacy simply by asking

00:43:24.100 --> 00:43:28.190
that question, even if we put
infinite value on human life

00:43:28.190 --> 00:43:31.500
and we say, you know,
preventing a terrorist attack

00:43:31.500 --> 00:43:33.610
is worth these
kinds of programs.

00:43:33.610 --> 00:43:35.410
If it's not actually
doing that at all,

00:43:35.410 --> 00:43:37.595
then the question
becomes a lot easier.

00:43:37.595 --> 00:43:41.710
AUDIENCE: If one studies
the terrorism literature,

00:43:41.710 --> 00:43:45.460
the purpose of terrorism
is to terrorize

00:43:45.460 --> 00:43:53.080
and to cause the empire
to overreact, and thus

00:43:53.080 --> 00:43:55.150
internally destroy itself.

00:43:55.150 --> 00:43:59.070
That's in all the writings.

00:43:59.070 --> 00:44:02.020
And we play right into
the hands of the--

00:44:02.020 --> 00:44:06.580
and so terrorism accomplishes
what they want, unfortunately.

00:44:06.580 --> 00:44:10.600
So is there any
recognition that they're

00:44:10.600 --> 00:44:15.220
playing into the hands of the
terrorists by overreacting?

00:44:15.220 --> 00:44:16.880
TIMOTHY EDGAR: That's
a great question.

00:44:16.880 --> 00:44:20.620
I would say this comes back
a little bit to the Trump

00:44:20.620 --> 00:44:22.960
Administration.

00:44:22.960 --> 00:44:25.270
You know, Trump is--

00:44:25.270 --> 00:44:27.770
the rhetoric that he
used in the campaign

00:44:27.770 --> 00:44:30.730
gave a lot of people
who were experienced

00:44:30.730 --> 00:44:33.400
in the national security
world a lot of fear

00:44:33.400 --> 00:44:37.810
that we would have these kinds
of massive overreactions,

00:44:37.810 --> 00:44:40.680
and that we would play into
the hands of terrorist groups

00:44:40.680 --> 00:44:44.800
that were promoting a narrative
of a clash of civilizations.

00:44:44.800 --> 00:44:47.920
And we've seen with the
travel ban and other policies

00:44:47.920 --> 00:44:52.180
that President Trump
has, in fact, done that.

00:44:52.180 --> 00:44:55.420
At the same time, you have a
permanent national security

00:44:55.420 --> 00:44:59.050
establishment that has learned
some lessons, I would argue,

00:44:59.050 --> 00:45:04.960
over the past decade and
a half or more since 9/11,

00:45:04.960 --> 00:45:07.960
the policy changes that
were made by the Obama

00:45:07.960 --> 00:45:12.100
administration to protect the
privacy interests of foreigners

00:45:12.100 --> 00:45:14.980
and their personal information--
that's Presidential Policy

00:45:14.980 --> 00:45:16.300
Directive 28--

00:45:16.300 --> 00:45:18.610
a lot of my former
colleagues said, well,

00:45:18.610 --> 00:45:20.560
that's out the window.

00:45:20.560 --> 00:45:22.180
Trump doesn't care
about foreigners.

00:45:22.180 --> 00:45:24.260
He's America first.

00:45:24.260 --> 00:45:28.480
So day one, you know, we'll
see that directive rescinded.

00:45:28.480 --> 00:45:29.410
It hasn't been.

00:45:29.410 --> 00:45:31.780
It hasn't been, and in
fact, has been reaffirmed

00:45:31.780 --> 00:45:34.240
by the new director of
National Intelligence

00:45:34.240 --> 00:45:36.820
on very pragmatic grounds,
which is that we need

00:45:36.820 --> 00:45:40.060
the support of many
other countries

00:45:40.060 --> 00:45:46.030
in our intelligence and our
intelligence activities.

00:45:46.030 --> 00:45:47.740
I would also caution,
even though we've

00:45:47.740 --> 00:45:50.290
been talking a lot
about terrorism,

00:45:50.290 --> 00:45:53.620
the intelligence apparatus
is not just about terrorism.

00:45:53.620 --> 00:45:56.800
It's about broad
foreign policy interests

00:45:56.800 --> 00:46:01.900
of a very powerful
country, the United States.

00:46:01.900 --> 00:46:04.210
And that's another
reason, I think,

00:46:04.210 --> 00:46:06.280
that the reaction to
the Snowden revelations

00:46:06.280 --> 00:46:11.200
was different than we've seen
to other spying programs that

00:46:11.200 --> 00:46:13.090
were more clearly
focused on terrorism.

00:46:13.090 --> 00:46:18.430
It was pretty clear that
if the NSA is spying

00:46:18.430 --> 00:46:20.620
on the chancellor
of Germany, it's

00:46:20.620 --> 00:46:23.170
not because they suspect that
the chancellor of Germany

00:46:23.170 --> 00:46:24.820
is a terrorist.

00:46:24.820 --> 00:46:27.220
There are foreign policy
interests involved

00:46:27.220 --> 00:46:29.330
in surveillance programs.

00:46:29.330 --> 00:46:31.630
So that's an important
conversation to have.

00:46:31.630 --> 00:46:35.710
My view is that the
calculation is different

00:46:35.710 --> 00:46:40.960
when you're talking about
broad intelligence purposes,

00:46:40.960 --> 00:46:44.380
and that there are some mass
surveillance capabilities

00:46:44.380 --> 00:46:48.640
that we should not be using
for those broader intelligence

00:46:48.640 --> 00:46:49.210
objectives.

00:46:49.210 --> 00:46:52.120
We should only be using
them for security threats

00:46:52.120 --> 00:46:53.800
like terrorism.

00:46:53.800 --> 00:46:57.730
So that would be my
recommendation to Congress

00:46:57.730 --> 00:46:59.370
when it comes to
PRISM and Upstream,

00:46:59.370 --> 00:47:03.280
is do we really need to be
using this program for broader

00:47:03.280 --> 00:47:06.880
foreign policy interests, as
opposed to for the terrorists

00:47:06.880 --> 00:47:09.250
purposes that
Congress, you know,

00:47:09.250 --> 00:47:12.070
was thinking about when
they passed that law?

00:47:12.070 --> 00:47:13.930
AUDIENCE: I think
one last question--

00:47:13.930 --> 00:47:19.142
how do we, as citizens, forward
the cause of civil liberties?

00:47:19.142 --> 00:47:21.970
TIMOTHY EDGAR: It's
a great question.

00:47:21.970 --> 00:47:26.340
Well, I know that some
of the folks in this room

00:47:26.340 --> 00:47:29.370
may know that Senator
Dianne Feinstein is

00:47:29.370 --> 00:47:31.890
an important leader on
intelligence issues.

00:47:31.890 --> 00:47:36.960
She is also not, I would say,
a reliable civil libertarian,

00:47:36.960 --> 00:47:38.700
to put it mildly.

00:47:38.700 --> 00:47:44.670
So pressure on Senator Feinstein
from people in California

00:47:44.670 --> 00:47:47.310
will have an impact on the
debate Congress is having right

00:47:47.310 --> 00:47:50.070
now on Prism and Upstream.

00:47:50.070 --> 00:47:54.990
Also, educating people on
how the technology works.

00:47:54.990 --> 00:47:58.900
This is an important
audience here at Google

00:47:58.900 --> 00:48:02.940
that can look at some of the
programs I'm talking about

00:48:02.940 --> 00:48:06.600
and have an intelligent
conversation about IP addresses

00:48:06.600 --> 00:48:12.480
and content of communications
and how selectors are used,

00:48:12.480 --> 00:48:15.540
taking advantage of that
transparency I talk about.

00:48:15.540 --> 00:48:17.850
There's just an enormous
amount of information

00:48:17.850 --> 00:48:21.390
that used to be classified
at the top secret level, when

00:48:21.390 --> 00:48:24.720
I worked in government, that has
been declassified and released

00:48:24.720 --> 00:48:27.810
about how much of this
surveillance works.

00:48:27.810 --> 00:48:31.117
But it's still impenetrable
to a lot of people.

00:48:31.117 --> 00:48:32.700
One reason I wrote
the book is to make

00:48:32.700 --> 00:48:35.400
it understandable
to ordinary people.

00:48:35.400 --> 00:48:38.910
It's a reason that I
use ordinary words,

00:48:38.910 --> 00:48:42.630
like mass surveillance
instead of jargon

00:48:42.630 --> 00:48:45.900
that the intelligence
agencies like, like collection

00:48:45.900 --> 00:48:49.710
of signals intelligence
under executive order, 12

00:48:49.710 --> 00:48:51.540
triple three.

00:48:51.540 --> 00:48:54.360
And this has caused some
of my former colleagues

00:48:54.360 --> 00:48:59.280
to say, hey, this is
scary language, Tim.

00:48:59.280 --> 00:49:00.660
You know, you're scaring people.

00:49:03.440 --> 00:49:06.510
You should tell them about all
of our privacy protections.

00:49:06.510 --> 00:49:11.220
But my view is it's important to
use plain and ordinary language

00:49:11.220 --> 00:49:13.890
about what the
government is up to.

00:49:13.890 --> 00:49:16.140
If we agree with
what they're doing

00:49:16.140 --> 00:49:20.350
and we think the protections
are good enough, then so be it.

00:49:20.350 --> 00:49:22.560
But we shouldn't be having
a conversation that's

00:49:22.560 --> 00:49:27.540
clouded in euphemisms, like
signals intelligence instead

00:49:27.540 --> 00:49:30.180
of mass surveillance.

00:49:30.180 --> 00:49:33.810
So you all here in
the tech community

00:49:33.810 --> 00:49:38.640
can help illuminate these
issues for the public.

00:49:38.640 --> 00:49:43.140
And organizations
like EFF and ACLU

00:49:43.140 --> 00:49:47.250
are always looking for
people who can help them out.

00:49:47.250 --> 00:49:50.340
In fact, the ACLU of
northern California

00:49:50.340 --> 00:49:51.390
has an open position.

00:49:51.390 --> 00:49:54.270
I don't necessarily want
to recruit everybody here

00:49:54.270 --> 00:49:57.720
away from Google, but
for a technologist.

00:49:57.720 --> 00:50:01.290
And whether you end up
doing like what I did

00:50:01.290 --> 00:50:04.770
and working as an activist,
or whether you just

00:50:04.770 --> 00:50:08.070
want to join with the
group and help them out,

00:50:08.070 --> 00:50:10.800
I just think that the
talent in this room

00:50:10.800 --> 00:50:13.860
could be so helpful to
those kinds of groups

00:50:13.860 --> 00:50:16.380
as they advocate for
privacy and civil liberties.

00:50:16.380 --> 00:50:19.890
And I would also encourage you
to look into government service

00:50:19.890 --> 00:50:22.230
and to see what
you can do to make

00:50:22.230 --> 00:50:24.960
a difference on the inside.

00:50:24.960 --> 00:50:28.290
I am very proud of the
service I did, as limited

00:50:28.290 --> 00:50:30.780
as some of my victories were.

00:50:30.780 --> 00:50:34.680
And having people who understand
how the internet works

00:50:34.680 --> 00:50:39.090
and how technology works
will help the lawyers

00:50:39.090 --> 00:50:41.670
and the officials ask the right
questions, which they often

00:50:41.670 --> 00:50:43.750
haven't asked in the past.

00:50:43.750 --> 00:50:44.980
DAVID: Well, thank you, Tim.

00:50:44.980 --> 00:50:47.420
Let's give him one
more round of applause.

