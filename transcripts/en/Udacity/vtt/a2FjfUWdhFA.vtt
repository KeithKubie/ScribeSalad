WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:04.390
I actually pulled out an older piece
of work focusing just on the gesture.

00:00:04.390 --> 00:00:10.730
This is by Nam and Wohn of recognizing
hand gestures using some HMMs.

00:00:10.730 --> 00:00:14.580
Just because it was very paradigmatic
of how people would do things.

00:00:14.580 --> 00:00:16.490
Well, what's kind of cool
about their system was,

00:00:16.490 --> 00:00:21.150
they had a bunch of different gestures
that they wanted to indicate.

00:00:21.150 --> 00:00:22.190
Like nouns.

00:00:22.190 --> 00:00:22.770
You know,

00:00:22.770 --> 00:00:27.500
chairs, vase, lamp, bulb because of
course that's a natural vocabulary.

00:00:27.500 --> 00:00:28.450
I have no idea.

00:00:28.450 --> 00:00:33.100
And then you want to do things like
put it down or bring it or put or

00:00:33.100 --> 00:00:35.180
discard or jump.

00:00:35.180 --> 00:00:38.630
I don't, don't even know why that was a
particular verb that they wanted to use.

00:00:38.630 --> 00:00:42.060
But the idea was that there
was a fixed vocabulary.

00:00:42.060 --> 00:00:43.760
The inputs that they
were going to use and

00:00:43.760 --> 00:00:47.172
they were going to control it with the
hand, was a couple of different kinds.

00:00:47.172 --> 00:00:50.770
Right?
They had something to do about hand

00:00:50.770 --> 00:00:55.060
configuration or posture, something to
do with how your fingers were shaped.

00:00:55.060 --> 00:00:58.140
Okay.
They also had how you're holding your

00:00:58.140 --> 00:01:05.940
palm and then they had a sequence
of hand position, a motion, okay?

00:01:05.940 --> 00:01:09.260
And they wanted to re,
use all of this information

00:01:09.260 --> 00:01:12.300
to make a decision about what
gesture you were indicating.

00:01:12.300 --> 00:01:16.420
So the first thing you should ask is
how are they getting this information?

00:01:16.420 --> 00:01:18.320
How are they getting this information?

00:01:18.320 --> 00:01:19.730
&gt;&gt; Boy, Meghan woke up.

00:01:19.730 --> 00:01:21.650
Well they were using something
called a data glove.

00:01:21.650 --> 00:01:22.390
Here's a picture.

00:01:22.390 --> 00:01:25.670
So a data glove is this thing that
you can wear on your hand that had

00:01:25.670 --> 00:01:29.700
both accelerometer and gyro information
about how it was being globally

00:01:29.700 --> 00:01:34.200
manipulated and also knew something
about how you were moving your fingers.

00:01:34.200 --> 00:01:37.990
So they built this kind
of interesting system.

00:01:37.990 --> 00:01:42.990
And the reason I liked it is,
they actually are using

00:01:42.990 --> 00:01:47.690
these three sensors the angles,
the orientation, the position, right?

00:01:47.690 --> 00:01:51.460
And what's interesting
is they use the HMMs

00:01:51.460 --> 00:01:55.790
just to get information about
that motion, trajectory.

00:01:55.790 --> 00:01:56.380
Right?

00:01:56.380 --> 00:02:00.050
They had other methods of
putting into their system

00:02:00.050 --> 00:02:03.690
how your orientation was changing and
what the angles were.

00:02:03.690 --> 00:02:06.100
Right?
So the idea is that you might have some

00:02:06.100 --> 00:02:10.940
basic overall probabilistic
integration system, and in order to

00:02:10.940 --> 00:02:15.570
make use of that, you needed things
that would output the probability or

00:02:15.570 --> 00:02:19.860
the likelihood of getting
a particular movement for

00:02:19.860 --> 00:02:23.530
a given gesture, and HMMs were exactly
the kind of thing to use for that.

00:02:24.570 --> 00:02:27.090
In fact, discriminative methods
might have been harder to use for

00:02:27.090 --> 00:02:29.360
that because they need to
combine these probabilities.

00:02:29.360 --> 00:02:32.440
So here was something very
simple that they did.

00:02:32.440 --> 00:02:34.430
They took the,
I shouldn't say very soon.

00:02:34.430 --> 00:02:37.040
Here's what they did,
it required some engineering.

00:02:37.040 --> 00:02:41.050
You take your raw data, which was
a trajectory that you would move in xyz,

00:02:41.050 --> 00:02:43.870
and the first thing they did was,
they said, well you know,

00:02:43.870 --> 00:02:46.910
your gesture is pretty much planar.

00:02:46.910 --> 00:02:47.660
Right?

00:02:47.660 --> 00:02:50.180
The plane could be this way,
could be this way,

00:02:50.180 --> 00:02:54.080
but the idea is that a single gesture
didn't move in more than a plane.

00:02:54.080 --> 00:02:55.100
I might even go like this, but

00:02:55.100 --> 00:02:58.000
you'll notice even this
figure 8 is in this plane.

00:02:59.000 --> 00:02:59.940
All right?

00:02:59.940 --> 00:03:05.550
So they find the plane that the gesture
fit best to, projected down.

00:03:05.550 --> 00:03:07.690
And then they did what's
called a chain code.

00:03:07.690 --> 00:03:11.430
We didn't actually talk about chain
codes, if you did some machine vision,

00:03:11.430 --> 00:03:12.500
you would learn about chain code.

00:03:12.500 --> 00:03:15.465
It was a way of encoding
a contour as being you know,

00:03:15.465 --> 00:03:17.155
maybe you can only take
one of eight steps.

00:03:17.155 --> 00:03:18.405
Right?
So you go up,

00:03:18.405 --> 00:03:21.675
you go you take a direction north.

00:03:21.675 --> 00:03:22.855
You go northeast.

00:03:22.855 --> 00:03:26.685
You go east, east, east.,
south, southeast, southwest and

00:03:26.685 --> 00:03:27.965
then eventually you come around.

00:03:27.965 --> 00:03:36.230
And you could think of the entire
contour as a sequence of discrete steps.

00:03:36.230 --> 00:03:36.820
Right.

00:03:36.820 --> 00:03:38.790
So north, east, south, right?

00:03:38.790 --> 00:03:42.230
So sequence of discrete symbols, ah-hah.

00:03:42.230 --> 00:03:43.410
HMMs.

00:03:43.410 --> 00:03:49.040
So they trained a very simple
left-to-right HMM, let-to-right

00:03:49.040 --> 00:03:53.750
meaning that you go through every state
and you, you don't skip states you have

00:03:53.750 --> 00:03:57.130
these loopback things because you may
stay in a state for a while where,

00:03:57.130 --> 00:04:01.950
essentially you are going kind of, let's
suppose you are going north, northeast.

00:04:01.950 --> 00:04:03.230
But you didn't have that as a direction.

00:04:03.230 --> 00:04:06.560
Well, north northeast, that state
you might sometimes put out north.

00:04:06.560 --> 00:04:08.540
Sometimes put out northeast,
etc, whatever.

00:04:08.540 --> 00:04:11.020
But you'd stay in that state for awhile.

00:04:11.020 --> 00:04:11.840
It would output,

00:04:11.840 --> 00:04:15.180
there would be two symbols it could
output with high probability.

00:04:15.180 --> 00:04:18.740
But that state for
example would never output south.

00:04:18.740 --> 00:04:21.519
Okay, so that would be,
you'd stay in the state for a while.

00:04:21.519 --> 00:04:26.540
So they trained up these HMMs and
they did one HMM for

00:04:26.540 --> 00:04:29.580
sort of each gesture, okay?

00:04:29.580 --> 00:04:33.820
They also did some stuff that had
to do with what they call junctures

00:04:33.820 --> 00:04:37.200
doing transitions between gestures so
that it, it could parse them.

00:04:37.200 --> 00:04:39.080
I don't think I'll,
I'll talk about that here.

00:04:39.080 --> 00:04:42.720
I just will say that people hook
up multiple HMM's together,

00:04:42.720 --> 00:04:47.130
to form sort of an HMM that can deal
with a sequence of gestures, and it's,

00:04:47.130 --> 00:04:50.950
it's not too much more complicated
than the original HMM.

00:04:50.950 --> 00:04:54.080
So they do that and then they test it.

00:04:54.080 --> 00:04:59.050
And because they're testing it on
a very controlled vocabulary, using

00:04:59.050 --> 00:05:04.790
features defined to work, because they
want their paper to be accepted, okay?

00:05:04.790 --> 00:05:08.370
You see results that look like this,
okay?

00:05:08.370 --> 00:05:09.080
So here are their hits.

00:05:09.080 --> 00:05:14.490
This is the percentage of, hits
are the percentage of the actual targets

00:05:14.490 --> 00:05:19.210
that were correctly labelled, misses are
when you don't label a thing correctly.

00:05:19.210 --> 00:05:20.360
And as you can see,

00:05:20.360 --> 00:05:25.290
they're doing tests of over a 100 of
each of these examples and almost

00:05:25.290 --> 00:05:30.760
all of their gestures are recognized
with a relatively high accuracy.

00:05:30.760 --> 00:05:35.130
You can see there are one or
two that was that were not so high and

00:05:35.130 --> 00:05:37.410
what and
showing a table like this is one thing,

00:05:37.410 --> 00:05:41.020
a slightly more interesting table
might be a confusion matrix.

00:05:41.020 --> 00:05:46.250
This is like we showed for the SVMs for
the bag of words where you go back and

00:05:46.250 --> 00:05:48.100
take a look at the slide
you'll see a table.

00:05:48.100 --> 00:05:50.706
Then it says how often is
an airplane confused as a face, or

00:05:50.706 --> 00:05:51.790
that kind of thing.

00:05:51.790 --> 00:05:54.930
Normally if you're going to report
results of that doing classification.

00:05:54.930 --> 00:05:58.535
Don't just provide hits and misses,
also provide a confusion matrix

00:05:58.535 --> 00:06:02.110
because sometimes the actual
source of the performance

00:06:02.110 --> 00:06:05.700
is that some of the categories were
much more confusable than the others.

00:06:05.700 --> 00:06:10.950
You have no way of knowing that if
you just report sort of performance

00:06:10.950 --> 00:06:15.230
rate overall, but you need to be
able to see the the, the confusion.

00:06:15.230 --> 00:06:17.490
What, which, which element, or

00:06:17.490 --> 00:06:19.530
which categories were confused
with which categories.

