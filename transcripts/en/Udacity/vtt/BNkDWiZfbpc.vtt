WEBVTT
Kind: captions
Language: en

00:00:00.220 --> 00:00:03.560
What they showed in some of the recent
work is that you run into problems if

00:00:03.560 --> 00:00:06.880
you try to minimize the loss function
that we talked about which is

00:00:06.880 --> 00:00:09.280
trying to minimize the kind
of GTD error directly.

00:00:09.280 --> 00:00:13.550
And instead, you actually get something
that's much more well behaved if you

00:00:13.550 --> 00:00:17.170
incorporate the function approximation
itself into the last function.

00:00:17.170 --> 00:00:20.530
So take advantage of the fact that we
know that we're changing parameters

00:00:20.530 --> 00:00:24.110
that are alternately going to influence
a linear representation of the function.

00:00:24.110 --> 00:00:26.540
And then through that linear
representation the function that that's

00:00:26.540 --> 00:00:28.810
where we're going to
actually measure the loss.

00:00:28.810 --> 00:00:31.700
And so the projection with respect
to the function approximate or

00:00:31.700 --> 00:00:35.350
ends up showing up in the last function
and that makes things happier.

00:00:35.350 --> 00:00:36.320
&gt;&gt; Nice, well done.

00:00:36.320 --> 00:00:38.800
So we had successes we had problems and

00:00:38.800 --> 00:00:40.580
then we have successes
again it's pretty good.

00:00:40.580 --> 00:00:44.160
&gt;&gt; Yeah and one of the thing to
note on the successes side is that

00:00:44.160 --> 00:00:45.460
the current We can talk about this so

00:00:45.460 --> 00:00:49.920
much but the go to algorithm these
&gt;&gt; Is called fitted Q iteration,

00:00:49.920 --> 00:00:52.630
which is a way of mixing
function approximation and

00:00:52.630 --> 00:00:54.730
Q value learning together.

00:00:54.730 --> 00:00:58.590
Again, if [LAUGH] it has the same
kinds of problems, it need not work,

00:00:58.590 --> 00:01:01.310
it need not converge,
it can behave really badly, but

00:01:01.310 --> 00:01:04.290
this is something that a lot of
people have gotten to work decently.

00:01:04.290 --> 00:01:06.810
So if you're you know if you're trying
to solve these kinds of problems this is

00:01:06.810 --> 00:01:07.810
this is the go to.

00:01:07.810 --> 00:01:12.300
GTD is not quite there yet
it's more of theoretical interest but

00:01:12.300 --> 00:01:15.650
the hope is that these will actually
come more together in the future.

00:01:15.650 --> 00:01:18.580
And there's one other topic that we that
we spent time on that I think is worth

00:01:18.580 --> 00:01:21.397
mentioning which is
the notion of Averagers.

00:01:21.397 --> 00:01:22.310
&gt;&gt; Yes.
&gt;&gt; And

00:01:22.310 --> 00:01:25.530
the cool thing about an Averager was
not only where things well behaved

00:01:25.530 --> 00:01:28.260
you got convergence with
a function approximater,

00:01:28.260 --> 00:01:32.730
but it actually could be viewed
as a kind of MDP itself.

00:01:32.730 --> 00:01:36.270
The function approximater or could be
viewed as kind of MDP itself which,

00:01:36.270 --> 00:01:39.650
let's us get all kinds of nice
properties like that there's a unique.

00:01:39.650 --> 00:01:41.890
Value function that it
needs to converge to.

00:01:41.890 --> 00:01:45.960
One thing we didn't talk about
that's also really related to this

00:01:45.960 --> 00:01:50.520
this topic is in the Averagers case we
have a set of anchor points for that?

00:01:50.520 --> 00:01:51.230
&gt;&gt; Yes I do.

00:01:51.230 --> 00:01:54.760
&gt;&gt; As the number of anchor
points increases, the error

00:01:54.760 --> 00:01:58.560
in the value function approximation
that you get actually goes down.

00:01:58.560 --> 00:02:03.630
And so you can actually converge
over time on not only an answer but

00:02:03.630 --> 00:02:06.260
actually the right answer,
the right value function in the limit.

00:02:06.260 --> 00:02:10.080
&gt;&gt; Right because this is just K and
N, or was a lot like K and N.

00:02:10.080 --> 00:02:11.910
And that would be true for
canon as well.

00:02:11.910 --> 00:02:12.540
&gt;&gt; Yes.

00:02:12.540 --> 00:02:14.100
&gt;&gt; Alright if you had
an infinite number of points for

00:02:14.100 --> 00:02:16.110
example you could actually
represent the point.

00:02:16.110 --> 00:02:19.310
&gt;&gt; So in optional reading we'll actually
put a link to it to the first paper

00:02:19.310 --> 00:02:21.710
that really goes through that
argument and shows that.

00:02:21.710 --> 00:02:25.870
Yeah in fact as the amount of data
increases things like averages kernel

00:02:25.870 --> 00:02:29.438
methods converge to the optimal value
function the true value function.

00:02:29.438 --> 00:02:30.170
&gt;&gt; That's pretty cool, so

00:02:30.170 --> 00:02:32.400
that everything we learn
including stuff we did learn?

00:02:32.400 --> 00:02:33.710
&gt;&gt; I think that's
everything we learned and

00:02:33.710 --> 00:02:35.650
a bunch of things that we didn't learn,
and

00:02:35.650 --> 00:02:41.552
then I probably should also just
say LSPI and just leave it at that.

00:02:41.552 --> 00:02:44.260
&gt;&gt; [LAUGH] I think that's right we

00:02:44.260 --> 00:02:47.870
have to leave them to go learn something
on their own or those with a point B.

00:02:47.870 --> 00:02:48.910
&gt;&gt; Yes.

00:02:48.910 --> 00:02:50.120
Least squares policy iteration,

00:02:50.120 --> 00:02:53.230
which is which is a kind of linear
function approximation where

00:02:53.230 --> 00:02:56.260
policy improvement is actually
incorporated into the system itself, and

00:02:56.260 --> 00:03:01.100
it has some super nice properties and
people have gotten it to do good things.

00:03:01.100 --> 00:03:05.150
The tricky thing is always the feature
set, getting the right features so

00:03:05.150 --> 00:03:08.010
that linear function
approximation does an okay job.

00:03:08.010 --> 00:03:10.090
&gt;&gt; Well it's always true, isn't it?

00:03:10.090 --> 00:03:10.590
&gt;&gt; Exactly.

00:03:11.590 --> 00:03:14.810
&gt;&gt; If we could always have the right
features, then the problem becomes easy.

00:03:14.810 --> 00:03:15.870
&gt;&gt; So that's a lot to learn.

00:03:15.870 --> 00:03:16.980
&gt;&gt; Yeah, and to have learned.

00:03:16.980 --> 00:03:18.250
What are we going to next time, Michael?

00:03:18.250 --> 00:03:19.540
&gt;&gt; Here, I'll give you a hint.

00:03:19.540 --> 00:03:20.660
I'm hiding the slide.

00:03:20.660 --> 00:03:23.060
We're going to talk about
partially observable models.

00:03:23.060 --> 00:03:24.490
&gt;&gt; Hm, okay, that makes sense.

00:03:24.490 --> 00:03:26.605
That means that I got lots
of opportunities for puns.

00:03:26.605 --> 00:03:29.200
&gt;&gt; [LAUGH]
&gt;&gt; All right, we'll see you there then.

00:03:29.200 --> 00:03:30.741
Wouldn't want to miss that.

00:03:30.741 --> 00:03:32.340
&gt;&gt; [LAUGH] See it has already begun.

00:03:32.340 --> 00:03:33.940
All right well by, Michael.

00:03:33.940 --> 00:03:34.580
&gt;&gt; See you, Charles.

