WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.760
&gt;&gt; CHANEZON: Okay. Hello, everybody. Again,
I'm Patrick Chanezon, developer advocate at

00:00:07.760 --> 00:00:14.790
Google. I'm French but I live in San Francisco.
And this is the second of the talk about our

00:00:14.790 --> 00:00:20.619
cloud computing offering. So in the first
talk, I talked about our platform offering

00:00:20.619 --> 00:00:28.539
which was Google App Engine and Google App
Engine for business. And now in this talk,

00:00:28.539 --> 00:00:35.609
I'm going to tell you about some new offerings
that are more at the infrastructure layer.

00:00:35.609 --> 00:00:42.319
So what is called infrastructure as a service
that Google has announced at Google I/O in

00:00:42.319 --> 00:00:49.549
May. So these three offerings are not available
for everybody yet. It's in trusted testers

00:00:49.549 --> 00:00:56.660
right now. So if you're interested in participating
into the trusted tester, I'll show you or

00:00:56.660 --> 00:01:03.369
I'll tweet the URL later today. Just follow
my Twitter account where you can sign up to

00:01:03.369 --> 00:01:09.850
be--to be part of the Trusted Tester Program.
So I'm going to tell you about Google Storage,

00:01:09.850 --> 00:01:16.590
BigQuery, and the Prediction API. For me,
these are the most exciting of our cloud computing

00:01:16.590 --> 00:01:24.780
offering especially BigQuery and the Prediction
APIs because they solve problems that are

00:01:24.780 --> 00:01:32.110
very hard to solve manually by yourself and
they expose some of the Google infrastructure

00:01:32.110 --> 00:01:42.410
and Google services that I don't think there
are equivalents for them in other cloud providers.

00:01:42.410 --> 00:01:47.980
Again, the mobile agenda for the day is there,
and you can rate the sessions over there.

00:01:47.980 --> 00:01:54.700
Okay, so Google storage for developer Prediction
API and BigQuery. So cloud computing, as I

00:01:54.700 --> 00:02:01.950
told you in the previous session, it's a very
loaded word. Everybody's trying to mark their

00:02:01.950 --> 00:02:10.780
offering with the term cloud. At Google, the
way we understand cloud is like having some

00:02:10.780 --> 00:02:20.510
providers reach some economies of scale when
they have lots of servers and they can automate

00:02:20.510 --> 00:02:27.130
lots of processes, and then offering that
as a service for third-party developers. So

00:02:27.130 --> 00:02:33.010
that doesn't encompass what some vendors are
calling private clouds; this is really the

00:02:33.010 --> 00:02:41.930
public clouds. Okay. So, yeah, the analyst
classifies stuff as the infrastructure platform,

00:02:41.930 --> 00:02:46.700
and software as a service. In this session,
we're going to talk really about the infrastructure

00:02:46.700 --> 00:02:53.040
part. So Google cloud Offerings, I told you
about that before. So we have software as

00:02:53.040 --> 00:02:59.219
a service with Google Apps, the Apps Marketplace
and then the Apps, you can build on top of

00:02:59.219 --> 00:03:04.860
Google App Engine platform. For us, it's Google
App Engine with Java and Python. And then

00:03:04.860 --> 00:03:10.810
we start to offer infrastructure with Storage,
Prediction, and BigQuery. So let's talk about

00:03:10.810 --> 00:03:17.230
Storage first. Basically, Google Storage is
a very simple storage service with a REST

00:03:17.230 --> 00:03:24.450
API, and we're using the same API as Amazon
S3, so you can use the tools that you're using

00:03:24.450 --> 00:03:31.481
with S3 with Google Storage. But we added
a few features to them like, for example,

00:03:31.481 --> 00:03:42.049
for access control. Okay. So you can use that
for hosting static content, for doing backups.

00:03:42.049 --> 00:03:48.989
There are lots of companies who do synchronization
software or you install some software on Windows

00:03:48.989 --> 00:03:55.230
or Mac, and it completely integrates with
the finder or with Windows Explorer. And then,

00:03:55.230 --> 00:03:59.529
you can mark some folders to be synchronized
in the cloud and you can get them back on

00:03:59.529 --> 00:04:06.269
other machines. So, some of these providers
are using Google Storage behind the scene.

00:04:06.269 --> 00:04:13.480
You can use that for sharing data. It can
be used as data storage for applications.

00:04:13.480 --> 00:04:18.480
And last but not least--and I think it's one
of the most interesting use cases--it can

00:04:18.480 --> 00:04:26.320
be used as storage for computation which means
Google Storage for is an entry point. When

00:04:26.320 --> 00:04:31.450
you put your data in there, it's an entry
point to access other Google services like

00:04:31.450 --> 00:04:38.100
Prediction and BigQuery. So basically, once
your data is in Google's cloud, we're going

00:04:38.100 --> 00:04:44.200
to offer you more services so that you can
do useful stuff with them. So, the benefits

00:04:44.200 --> 00:04:50.200
of Google Storage: its high-performance. Basically,
we are--we are leveraging all the magical

00:04:50.200 --> 00:04:56.070
infrastructure that we have at Google with
data centers all around the world; some replication

00:04:56.070 --> 00:05:06.200
layer for the content and the caching servers.
And there are some people who made some evaluation

00:05:06.200 --> 00:05:12.090
of their latency of Google Storage all around
the world and it's very, very good. Security

00:05:12.090 --> 00:05:19.150
and privacy, a very important aspect and differentiator
for Google Storage. In Storage, we have some

00:05:19.150 --> 00:05:25.350
pretty deep access control rules that you
can set where you can say--and it's integrated

00:05:25.350 --> 00:05:31.220
with Google Accounts, so that means at the
API level, you can say, "This file will be

00:05:31.220 --> 00:05:37.280
accessible only by these Google accounts or
this Google group which is a group accounts

00:05:37.280 --> 00:05:45.590
in view or edit mode. And then these rules
can be enforced at the browser level". So

00:05:45.590 --> 00:05:51.370
that means you don't need to have to have
a server to enforce access control between

00:05:51.370 --> 00:05:56.870
the end-client and your file. And this is
very useful when you're building application;

00:05:56.870 --> 00:06:03.050
we take care of access control for you. And
then it's super easy to use because we leverage

00:06:03.050 --> 00:06:08.550
the existing--all the existing tools that
were used with the S3 API because we support

00:06:08.550 --> 00:06:15.780
the same API. So that means that you can use
tools like Boto library in Python, or S3Fox,

00:06:15.780 --> 00:06:25.150
the Firefox extension to manage your files,
or Java libraries or things like that. So

00:06:25.150 --> 00:06:31.640
technical details; it's a very simple REST
API. Who among you have been using Amazon

00:06:31.640 --> 00:06:42.960
S3 before? Okay. So basically, you'll feel
at home; it's the same API. So, resources

00:06:42.960 --> 00:06:48.460
are organized by buckets. Buckets are just
flat containers; there's no hierarchy in there.

00:06:48.460 --> 00:06:55.340
And then under that, you put resources that
are represented by--identified by URI. And

00:06:55.340 --> 00:07:02.800
some people have been using, like, resource
naming with Flash in order to replicate file

00:07:02.800 --> 00:07:09.530
system hierarchy; but it's not exactly the
same semantics. You can put any type of objects.

00:07:09.530 --> 00:07:17.280
The limit in size is 100 gigabytes per object.
So, access control, I told you about that.

00:07:17.280 --> 00:07:24.060
There are two ways to authenticate the request;
either you sign them using your access key

00:07:24.060 --> 00:07:29.700
that you get when you sign-up for the API
or you use the web browser login with the

00:07:29.700 --> 00:07:37.970
Google Account. So in terms of performance
and scalability, yes, as I told you, it's

00:07:37.970 --> 00:07:45.280
100 gigabytes per object maximum. You can
have thousands of buckets and unlimited number

00:07:45.280 --> 00:07:56.080
of objects. The data itself is replicated
within multiple U.S. data centers. And so--but

00:07:56.080 --> 00:08:04.550
then, we are leveraging all the Google network
of edge servers that have all around the world

00:08:04.550 --> 00:08:09.710
that provide caching for these files. So what
that means that when you're measuring latency

00:08:09.710 --> 00:08:20.000
to get your files from anywhere around the
world, usually, it's pretty fast. Okay. Yes.

00:08:20.000 --> 00:08:27.120
And then another--a few other differentiators
is this very strong data consistency which

00:08:27.120 --> 00:08:34.621
is, when you have written a file, if it's--if
it gets read from anywhere around the world,

00:08:34.621 --> 00:08:40.820
once you have received the acknowledgement
that it was written, you read the same file.

00:08:40.820 --> 00:08:48.890
And you can do range get queries as well.
So privacy and security, it's a key-based

00:08:48.890 --> 00:08:56.400
authentication system. We also support authenticated
downloads from the web browser using Google

00:08:56.400 --> 00:09:04.310
Accounts. You can share with the individuals,
all with Google groups, and you can--at the

00:09:04.310 --> 00:09:10.730
API level, and that's, I think, a differentiator
with S3. We added a few API calls that let

00:09:10.730 --> 00:09:20.380
you specify an ACL for buckets and objects.
So in terms of tools, we have one tool called

00:09:20.380 --> 00:09:28.340
the Google Storage Manager. Maybe I'll show
it to you. Let me see where my browser is.

00:09:28.340 --> 00:10:07.170
Okay. Okay, so the Google Storage Manager
is a very simple user interface to manage

00:10:07.170 --> 00:10:14.610
your buckets--so these are my buckets--and
then, manage your files in there. So these

00:10:14.610 --> 00:10:20.941
are the files that are uploaded. So once you
have uploaded the files using the API, then

00:10:20.941 --> 00:10:30.310
you can decide to share them publicly. So
this is an image that I decide to share publicly.

00:10:30.310 --> 00:10:36.710
And when I share something publicly, I get
a link that I can then share with people.

00:10:36.710 --> 00:10:45.839
And if I had access control on that, I would
be obliged to log in to access it. So it's

00:10:45.839 --> 00:10:53.560
a pretty simple, basic tool where you can
upload files manually and things like that.

00:10:53.560 --> 00:11:00.830
There's another tool called GSUtil which is
a command line tool that allows you to do

00:11:00.830 --> 00:11:06.260
all the operations that you can do with the
API but from the command line. And GSUtil

00:11:06.260 --> 00:11:13.250
is itself built in Python on top of the Boto
Python library that was used with S3 and they

00:11:13.250 --> 00:11:24.180
added some Google Storage specific commands
to it for managing ACLs, for example. So,

00:11:24.180 --> 00:12:52.080
with GSUtil, you can say--actually, I can
show it to you maybe. So here, basically,

00:12:52.080 --> 00:12:57.270
I'm copying all these images under the Chanezon
bucket that I have created for my account.

00:12:57.270 --> 00:13:04.540
And the first time you use GSUtil, you need
to identify with a token. And then it's copying

00:13:04.540 --> 00:13:16.370
all the files. Yes, connection is pretty slow
there. So maybe I'm just going to close it.

00:13:16.370 --> 00:13:42.520
And just to show you--actually I had these
files already in there. So, if I sort by last

00:13:42.520 --> 00:13:54.540
updated... Okay. I don't know. You get the
ID. Okay, so that's GSUtil. And GSUtil allows

00:13:54.540 --> 00:14:02.610
you to manage your access control rules as
well. So Google Storage is used within Google

00:14:02.610 --> 00:14:09.551
by lots of Google services already. So, google.org
used it for the Haiti relief imagery. When

00:14:09.551 --> 00:14:18.960
there was an earthquake in Haiti, we just
put some Maps imagery in there. Google Buttons

00:14:18.960 --> 00:14:26.170
is using it; Picnik is using it for all their
images; Panoramio for images as well; DoubleClick

00:14:26.170 --> 00:14:31.790
and YouTube are using it as well. And it's
used by BigQuery and the Prediction API that

00:14:31.790 --> 00:14:37.910
I'm going to tell you about. There are lots
of companies who have started using Google

00:14:37.910 --> 00:14:46.190
Storage for developers, so VMWare. Syncplicity
is a solution for syncing your files, as I

00:14:46.190 --> 00:14:52.390
told you before. Memeo is using it; it's the
same kind of solution. The Guardian has been

00:14:52.390 --> 00:14:58.880
using it for some of their apps. Socialwok,
they're using every products we have. Widgetbox

00:14:58.880 --> 00:15:08.839
has been using it for serving images and resizing
them. The pricing is--so, it's 17 cents per

00:15:08.839 --> 00:15:18.200
gigabyte per month and then 10 cents per gigabyte
for upload, and 30 cents per gigabyte in APAC

00:15:18.200 --> 00:15:28.850
for downloads, and $0.15 in America and EMEA;
and then, the requests have different pricing.

00:15:28.850 --> 00:15:35.580
So right now, this is a preview and it's in
the US only; but we've been doing lots of

00:15:35.580 --> 00:15:41.040
developer events around the world and everybody's
interested in using that plus using BigQuery

00:15:41.040 --> 00:15:48.950
or the Prediction API. So, we asked the product
team and they gave us the okay to put a form

00:15:48.950 --> 00:15:57.690
out for non-US developers to sign up for it.
So there's some waivers in there to tell you,

00:15:57.690 --> 00:16:02.640
"This is not supported yet," and all that,
and it's only for the US, but if you want

00:16:02.640 --> 00:16:14.230
to play with it, just go ahead. So, I'll--I'll
Tweet the link later. So, Google Storage,

00:16:14.230 --> 00:16:20.149
pretty simple; you can store or any kind of
data in there. There's many tools and libraries

00:16:20.149 --> 00:16:27.930
available, and it's a pretty simple API to
play with. Now, where it gets interesting

00:16:27.930 --> 00:16:35.470
is that Google Storage is only the first layer
of our infrastructure offering. Once you put

00:16:35.470 --> 00:16:41.750
your data in there, you can start applying,
like, Google services that don't exist anywhere

00:16:41.750 --> 00:16:48.730
else. Two of them are the Prediction API and
BigQuery. So, I'm going to start with the

00:16:48.730 --> 00:16:55.480
Prediction API. Prediction API is basically
a packaging of our machine learning algorithms

00:16:55.480 --> 00:17:03.750
that we're using at Google for many different
products, especially in Ads. And how many

00:17:03.750 --> 00:17:10.410
of you are familiar with machine learning
here? Pretty good. Pretty good turnout. So

00:17:10.410 --> 00:17:18.199
as you may know, applying machine learning
to your data can be time-consuming; you need

00:17:18.199 --> 00:17:27.250
to know the algorithms pretty well. Usually,
you need to tune them. There's lots of--lots

00:17:27.250 --> 00:17:34.990
of technically involved decisions and iteration
to do in order to leverage to that in a web

00:17:34.990 --> 00:17:42.400
application. So with the Prediction API, we
try to make that easier for developers. So,

00:17:42.400 --> 00:17:50.130
the idea is very simple, it's--you're creating
a CSV file with lots of columns. The first

00:17:50.130 --> 00:17:56.370
column is the value you want to predict and
all the other columns are considered features.

00:17:56.370 --> 00:18:02.060
You put that in Google Storage, then you ask
the Prediction API to train a model on this

00:18:02.060 --> 00:18:08.250
data, and then you have another API call to
check the status on the model. Usually, after

00:18:08.250 --> 00:18:16.140
ten minutes or maybe an hour--I don't remember
well what's the latency--but less than an

00:18:16.140 --> 00:18:22.810
hour, your model is trained and is ready,
and it gives you an estimate accuracy for

00:18:22.810 --> 00:18:29.070
the model. And once you've done that, you
can start asking for prediction. This is a

00:18:29.070 --> 00:18:38.049
very simple REST API where you send JSON payloads
to the API. And what you send is all the features

00:18:38.049 --> 00:18:42.799
except the first one that you want to predict
and it will give you a prediction for what

00:18:42.799 --> 00:18:48.800
their first feature should be. So, this is
a very simple example where we have put a

00:18:48.800 --> 00:18:57.260
CSV file where the first column is the language,
and then the second column is the sentence.

00:18:57.260 --> 00:19:04.600
And so we put thousands of sentences in various
languages; and then after that, you just send

00:19:04.600 --> 00:19:13.000
it a sentence and it will detect the language
for you. So this is what the model looks like.

00:19:13.000 --> 00:19:20.470
So, you create this file, CSV file with all
these sentences, then you ask--and their language.

00:19:20.470 --> 00:19:24.920
Then you ask the Prediction API to train on
that and once it's trained, you send it new

00:19:24.920 --> 00:19:33.179
sentences and it should detect what language
it is. So that can be used--that kind of API

00:19:33.179 --> 00:19:38.740
can be used in many different contexts where
the barrier to entry to using machine learning

00:19:38.740 --> 00:19:45.820
was too high; the Prediction API makes it
super easy to get started with. Basically,

00:19:45.820 --> 00:19:50.980
in a few hours, you can build your file, put
it to Google Storage, train a model, and you're

00:19:50.980 --> 00:20:00.150
up and running.
So this is an example that my colleagues built

00:20:00.150 --> 00:20:09.080
where--it's kind of a business-oriented example
where you want to categorize and respond to

00:20:09.080 --> 00:20:15.610
emails based on the language in which they
are written. And so, you want to predict the

00:20:15.610 --> 00:20:19.419
language of emails; so it's pretty much the
same thing. So you upload your data, your

00:20:19.419 --> 00:20:27.840
training data, to Google Storage; so the training
data would look something like that. To upload

00:20:27.840 --> 00:20:32.600
it to Google Storage, you use GSUtil, the
command line I showed you where you copy your

00:20:32.600 --> 00:20:41.450
data to your buckets. Once you have it, you
can train a model. So for that, you use a

00:20:41.450 --> 00:20:48.400
post to the Prediction API and to the training
endpoint where you specify as a parameter

00:20:48.400 --> 00:20:54.230
the bucket where you stored your data. So
then, the training runs asynchronously and

00:20:54.230 --> 00:21:02.500
it can take from ten minutes to an hour and
during that time, you can do a get to just

00:21:02.500 --> 00:21:08.450
get the status of the training. And so, when
it's not ready, it will tell you, "In process."

00:21:08.450 --> 00:21:15.260
When it's ready, it will tell you, okay, it's
ready and the accuracy is a figure between

00:21:15.260 --> 00:21:24.270
zero and one. And then after that, you can
apply the train model to new data. So, when

00:21:24.270 --> 00:21:28.299
you have new data which is a sentence that
comes in an email, you just send that to the

00:21:28.299 --> 00:21:37.520
Prediction API wrapped into a JSON payload
like this. And then, the result will be a

00:21:37.520 --> 00:21:48.520
JSON result where it will tell you the output
label--here it's French. And you can also

00:21:48.520 --> 00:21:57.970
have multiple outputs with different scores;
they added that pretty recently. So that's

00:21:57.970 --> 00:22:04.910
how we would use that in Python. So it's like
a few lines of Python just to get a prediction

00:22:04.910 --> 00:22:16.820
result. So in terms of capabilities, input
features can be numeric or unstructured text.

00:22:16.820 --> 00:22:22.540
Be careful about the unstructured text. I
will show you an example I built where I scrape

00:22:22.540 --> 00:22:27.940
HTML pages and you need to clean them up pretty
well if you want good results. The output

00:22:27.940 --> 00:22:35.929
can be up to hundreds of discrete categories.
They use--so the kind of technique that's

00:22:35.929 --> 00:22:40.160
used there is called Supervised Learning.
So they're using many different techniques

00:22:40.160 --> 00:22:46.360
but the tuning of the algorithm is completely
automated and it's done asynchronously. So,

00:22:46.360 --> 00:22:52.111
you won't have access, you don't know which
algorithm are going to be applied. And then,

00:22:52.111 --> 00:23:02.860
you can access that from anywhere where you
could access HTTP, it's just a REST API. But

00:23:02.860 --> 00:23:09.441
it's very easy to use from App Engine with
URL Fetch from Apps Script. An Apps Script

00:23:09.441 --> 00:23:17.130
is a pretty cool case where it allows you
to add scripts to your--to Google Apps, for

00:23:17.130 --> 00:23:24.400
example, in a spreadsheet. So you could have
a spreadsheet pulling prediction from the

00:23:24.400 --> 00:23:34.460
content of cell values, and you can use that
in your desktop apps, as well. So, the--what

00:23:34.460 --> 00:23:42.179
they added in the version 1.1 which is from
a month ago, I think, is multi-category prediction,

00:23:42.179 --> 00:23:51.630
continuous output and mixed inputs where you
can put inputs in numeric or text. So, that's

00:23:51.630 --> 00:23:57.820
prediction. Let me show you my example. Actually,
I'm going to go down there. I think I put

00:23:57.820 --> 00:24:08.160
a slide down there that explains--yeah. So
when I--when I saw that API coming out, I

00:24:08.160 --> 00:24:13.110
was pretty excited. And what I wanted to do
is--I've been using Delicious for tagging

00:24:13.110 --> 00:24:19.470
links in the past five years. And I think
I have 6,000 URLs in there that I categorized

00:24:19.470 --> 00:24:27.370
in 14,000 tags. And these tags are very personal
to me, that's the way I see information, the

00:24:27.370 --> 00:24:33.881
way I categorize it, really very personally
for the topics I'm interested in which is

00:24:33.881 --> 00:24:39.980
HTML5 and cloud computing and social software
and stuff like that. So I read all these articles

00:24:39.980 --> 00:24:48.390
and I tag them. Now, what I like to happen
is that instead of having to tag them myself,

00:24:48.390 --> 00:24:53.380
I said, "This system would be good to learn
from my habits and take all this knowledge,"

00:24:53.380 --> 00:24:59.180
and now, when I find a new article, it would
provide me with the tags I would be more likely

00:24:59.180 --> 00:25:04.470
to use for the article. So, that's what I
was trying to do. So, I used the Delicious

00:25:04.470 --> 00:25:11.700
API to get all my tags and then all my tags
and URLs. I wrote a small Python script to

00:25:11.700 --> 00:25:17.380
scrape all the pages and clean them up. And
actually, that's where I need to do a better

00:25:17.380 --> 00:25:26.200
job. So, I end up with a CSV file that has
a tag, a URL, and then the full text and I

00:25:26.200 --> 00:25:34.350
remove all the HTML tags of all the pages.
So that file was too big for--I think Prediction

00:25:34.350 --> 00:25:39.130
right now is limited to 100 megabytes, so
I had to clean up and make a smaller data

00:25:39.130 --> 00:25:48.549
set. But after that, I put that data set into
Google Storage and then I asked for training--I

00:25:48.549 --> 00:25:56.410
asked the Prediction API to train it for me.
So, there are some command lines example...

00:25:56.410 --> 00:26:26.700
Actually, let me show you [INDISTINCT]. Yes,
so that's why the next slide [INDISTINCT]

00:26:26.700 --> 00:26:34.680
is pretty small. So the bucket in which I
have uploaded--so I uploaded my training set

00:26:34.680 --> 00:26:40.929
which is called smalltags.CSV because I had
to reduce the size into Chanezon, so that's

00:26:40.929 --> 00:26:48.360
where my model has trained. And then there's
the token thing for authentication and all

00:26:48.360 --> 00:26:58.640
that. For--yes. So for the model, what I'm
doing is I'm going to pass as a parameter

00:26:58.640 --> 00:27:06.260
to that script a URL of an article that I've
read. And here, what it's going to do is it's

00:27:06.260 --> 00:27:20.150
going to get the URL, remove all the--remove
all the HTML tags, and then build a JSON data

00:27:20.150 --> 00:27:26.820
structure with the text as an input. And then,
I'm just going to ask a prediction for it.

00:27:26.820 --> 00:27:32.740
So I pass the authentication data, and then
I say, "For that model," which is the model

00:27:32.740 --> 00:27:42.140
there, "Please predict the tags I would have
used for it." And--okay, let's not cheat and

00:27:42.140 --> 00:27:51.200
let's use a random article. So I'm going to
take Techmeme from today. I'm completely addicted

00:27:51.200 --> 00:28:04.710
to Techmeme. Let's say, "Hands on the Nexus
Two by Samsung." Okay. So I take this random

00:28:04.710 --> 00:28:12.000
new article that I haven't read yet. And the
goal of this is--what I'm planning to do is

00:28:12.000 --> 00:28:18.570
to build a Chrome extension and an app engine
backend for it where, when I browse, it will

00:28:18.570 --> 00:28:40.840
tell me the categories automatically. Let's
see if it works. So first, it's pulling the

00:28:40.840 --> 00:28:49.460
content of it then cleaning it up. So I tried
different cleaner in there, like Beautiful

00:28:49.460 --> 00:28:58.060
Soup and HTML to texts.bio or something like
that. And they didn't give me very good results.

00:28:58.060 --> 00:29:07.780
Actually, the accuracy of my training, once
I've done it, I think was like 005% or something

00:29:07.780 --> 00:29:14.049
which is not very good. So this is a typical
example where it fails. It tells me it's about

00:29:14.049 --> 00:29:22.830
RSS. Not really. I had another article that
I just tried before coming which was about

00:29:22.830 --> 00:29:33.190
Apple, and this one worked a little bit better.
So I received this JSON payload that tells

00:29:33.190 --> 00:29:41.340
me the output should be that. Okay, so this
one is well categorized. So that's an example

00:29:41.340 --> 00:29:50.030
I built in a few hours and I need to work
on it again, obviously. My colleague, Nick

00:29:50.030 --> 00:29:56.940
Johnson, wrote an excellent blog post. His
example is much better. I think he has a 63%

00:29:56.940 --> 00:30:04.640
accuracy for it. So he took a different data
set; he took all the--do you guys know reddit?

00:30:04.640 --> 00:30:10.830
Yes. So, reddit is a site where you can categorize
articles and it's done by lots of people.

00:30:10.830 --> 00:30:17.429
So in my case, I had used Delicious which
I use as a single user. What he did is that

00:30:17.429 --> 00:30:24.980
he took all the content of reddit for one
week, like a few thousand of posts and subreddits

00:30:24.980 --> 00:30:32.070
which are the categories that users are categorizing
them in. He put that as the training set and

00:30:32.070 --> 00:30:40.350
then he reserved a part of the training set
for verification, and he had a 63% success

00:30:40.350 --> 00:31:09.960
on that one. And he built an App Engine app
out of it. I think I can show you. Okay. And

00:31:09.960 --> 00:31:17.270
he has a live demo, so let's try something.
Okay. So you have a--you have to put a title

00:31:17.270 --> 00:31:27.620
and a domain. Actually, let's try the one
that I tried before, this guy. So--and he

00:31:27.620 --> 00:32:05.460
just uses the domain, not the full URL. I
guess that may help as well.

00:32:05.460 --> 00:32:11.419
Okay, let's see if it works. So then, he's
calling App Engine who's taking this data,

00:32:11.419 --> 00:32:17.559
submitting it to the Prediction API, getting
a prediction back and telling it to us if

00:32:17.559 --> 00:32:44.400
it works. Tough luck. At least he puts the
code in there so you can do it yourself, and

00:32:44.400 --> 00:32:52.770
see for yourself whether it works. So I highly
recommend his blog post; it's pretty good.

00:32:52.770 --> 00:33:00.100
Okay. So now, let's talk about BigQuery. So,
Prediction API allows you to use machine learning

00:33:00.100 --> 00:33:06.580
in your own application. Another issue that
people have with big data, you have lots of

00:33:06.580 --> 00:33:12.980
data like maybe billions of rows. If you put
them into a relational database, it's very

00:33:12.980 --> 00:33:21.150
slow. You can put it into a proprietary analytics
engine, but it's expensive. What we've done

00:33:21.150 --> 00:33:25.421
at Google, because we have the same kind of
issue, we need to analyze billions of rows

00:33:25.421 --> 00:33:31.650
of data, for example, for all the clicks when
people click on ads. So Googlers developed

00:33:31.650 --> 00:33:38.809
internal infrastructure for doing queries
on all these data. And this infrastructure

00:33:38.809 --> 00:33:45.590
has been exposed to developers in a service
called BigQuery. So that's what we're using.

00:33:45.590 --> 00:33:52.020
Basically, you can do SQL-like queries on
huge sets of data and the response is very

00:33:52.020 --> 00:34:02.300
fast. The API is a REST API using JSON the
same way, and it's very simple to use. So

00:34:02.300 --> 00:34:07.990
you can use that for building interactive
tools for embedding into Google spreadsheets

00:34:07.990 --> 00:34:14.100
because you can use it from Apps Script using
the REST API. It's scalable to billions of

00:34:14.100 --> 00:34:21.730
rows, very fast response and the queries are
in SQL, so it's very simple. The way it works,

00:34:21.730 --> 00:34:28.470
the same way as the Prediction API which is
you upload your rowed data as a CSV into Google

00:34:28.470 --> 00:34:36.790
Storage, then you have a REST API which I
think is not ready yet but should be very

00:34:36.790 --> 00:34:42.780
soon. You have a REST API that allows you
to send a JSON payload that defines your schema.

00:34:42.780 --> 00:34:49.820
Basically defines the name of the fields in
your CSV and the type of these fields. In

00:34:49.820 --> 00:34:57.490
my example, I had to send my CSV to Google
Storage and tell the team to just apply my

00:34:57.490 --> 00:35:03.310
schema. So you define your schema and then
after that, you just query it using a REST

00:35:03.310 --> 00:35:09.120
API and sending your SQL orders that look
something like that. So you can do "GROUP

00:35:09.120 --> 00:35:15.470
BY", "ORDER BY." You have some functions like
Math, String and Time, and you have some statistical

00:35:15.470 --> 00:35:24.290
functions like "TOP" or "COUNT DISTINCT."
The API is very simple; you do a get on the

00:35:24.290 --> 00:35:30.250
table name. And the table name is essentially
the bucket and file where you have put it

00:35:30.250 --> 00:35:42.950
in Google Storage, then you can query it.
And the query--so that's the query of the

00:35:42.950 --> 00:35:49.021
results. No, that's the results. So you send
a query that's a SQL query; and the results

00:35:49.021 --> 00:35:56.800
you receive look something like that, a bunch
of results. Security, it's like client log-in

00:35:56.800 --> 00:36:07.369
or OAuth or AuthSub. It's typical Google security
stuff for our APIs. It supports HTTPS and

00:36:07.369 --> 00:36:14.839
they have--they have some example where they
took the whole revision history of all Wikipedia,

00:36:14.839 --> 00:36:22.050
put that into storage, defined a table for
it. And then you can start to do things like

00:36:22.050 --> 00:36:33.770
select the top title--the top five titles
and give me the number of revisions for these

00:36:33.770 --> 00:36:40.790
from the Wikipedia table and WPM space--I
don't know what it is. But basically, you

00:36:40.790 --> 00:36:49.150
can say, for a specific topic, give me the
top pages that talk about that and that have

00:36:49.150 --> 00:37:00.270
the most revisions. And so that's the result.
And there's a command line as well. I wanted

00:37:00.270 --> 00:37:07.200
to show it to you but I have an old version
of it. And so, there's a command line, you

00:37:07.200 --> 00:37:12.650
just can type your query and you get the result
right away. And the most impressive aspect

00:37:12.650 --> 00:37:18.060
is when you integrate that using Apps Script,
because it's a REST API giving you back JSON.

00:37:18.060 --> 00:37:23.750
So in Apps Script, you can just make a call
using input data. You put the URL to your

00:37:23.750 --> 00:37:29.920
query because it's just a get and then the
results, you can put it in your table. So

00:37:29.920 --> 00:37:36.000
here in this example, we're going to type
a search term and then it's going to use the

00:37:36.000 --> 00:37:41.020
search term as a parameter in the query, and
it's going to give us the title and number

00:37:41.020 --> 00:37:48.190
of revision for the documents who have the
most revision for that topic in Wikipedia.

00:37:48.190 --> 00:37:54.780
So, you type Google and you get these results
and then you can assemble them in diagrams

00:37:54.780 --> 00:38:02.540
like this. So basically, the kind of heavy
duty data processing that used to require

00:38:02.540 --> 00:38:09.960
a lot of server side infrastructure and experience;
now, you just need to put your data in Google

00:38:09.960 --> 00:38:16.460
Storage and then, you build your front-end
using very lightweight JavaScript infrastructure,

00:38:16.460 --> 00:38:26.190
or even directly in Google Apps. Yes, so I
told you about my tagger example, the subreddit.

00:38:26.190 --> 00:38:34.740
So that's Nick's blog post, and that's it.
So basically, with what we announced in May

00:38:34.740 --> 00:38:41.600
is that Google is starting to offer infrastructure-type
services in the cloud. First one is Google

00:38:41.600 --> 00:38:48.230
Storage, and it's point of entry for more
exciting services like Prediction or BigQuery

00:38:48.230 --> 00:38:54.020
that are hard to do when you're doing that
by yourself. You can learn more over there,

00:38:54.020 --> 00:38:57.940
and when this session will be finished--and
I have five minutes--I'll just upload on my

00:38:57.940 --> 00:39:04.780
slides to SlideShare, and will Tweet about
it. And I will Tweet the URL to the sign-up

00:39:04.780 --> 00:39:13.770
sheet as well where you can sign up for Google
Storage, Prediction and BigQuery. Okay. So

00:39:13.770 --> 00:39:19.530
we have to finish. I'll be in the--I think
there's a room about cloud stuff there. I'll

00:39:19.530 --> 00:39:22.560
go on. Thanks.

