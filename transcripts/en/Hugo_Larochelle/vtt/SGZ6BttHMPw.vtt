WEBVTT

1
00:00:00.570 --> 00:00:03.750
In this video,
we'll introduce the concept of an artificial neuron,

2
00:00:03.780 --> 00:00:07.440
which is going to be the basic building block we'll be using to construct

3
00:00:07.441 --> 00:00:09.030
complicated neural networks.

4
00:00:10.950 --> 00:00:14.820
So in artificial neuron is simply computational unit,

5
00:00:14.850 --> 00:00:19.850
which will make a particular computation based on other units it's connected to.

6
00:00:20.880 --> 00:00:21.930
So,
uh,

7
00:00:21.931 --> 00:00:26.931
in the case of a single artificial neuron like this one here,

8
00:00:27.970 --> 00:00:28.350
uh,

9
00:00:28.350 --> 00:00:33.150
it's going to be connected directly on an input description of some object we

10
00:00:33.151 --> 00:00:36.720
want to extract information from.
So this input description,

11
00:00:36.721 --> 00:00:39.580
we'll call it x.
So here,
uh,

12
00:00:39.690 --> 00:00:44.690
both ex is a sense essentially a vector which contains the scaler x one up to x

13
00:00:47.000 --> 00:00:50.460
d where these the size of the vector.
Okay.

14
00:00:50.461 --> 00:00:53.820
So notice that I'll be using bold as uh,

15
00:00:53.850 --> 00:00:58.160
when I'm denoting vectors and matrices as well.
And uh,

16
00:00:58.200 --> 00:01:02.070
whenever it's not involved and it corresponds to a scalar value.
Okay.

17
00:01:02.071 --> 00:01:05.640
So x is the element of Vector X.

18
00:01:06.360 --> 00:01:11.360
And so artificial neuron is going to read the information from this x vector and

19
00:01:13.411 --> 00:01:18.030
perform a particular computation,
which will dictate its value,

20
00:01:18.031 --> 00:01:19.620
the value of the neuron.

21
00:01:19.890 --> 00:01:24.890
And this valley will essentially indicate whether some particular feature or

22
00:01:25.500 --> 00:01:30.500
information about the subject that we're manipulating is present or not in the

23
00:01:31.651 --> 00:01:34.560
object,
uh,
described by the vector x.

24
00:01:35.430 --> 00:01:39.120
Now the computation it will perform,
can be decomposed into two steps.

25
00:01:39.450 --> 00:01:41.620
There's going to be first a,
uh,

26
00:01:41.760 --> 00:01:44.730
the computation of a pre activation of the neuron.

27
00:01:45.030 --> 00:01:46.620
Sometimes when you look at the literature,

28
00:01:46.621 --> 00:01:50.700
you see the expression instead input activation.
Uh,

29
00:01:50.730 --> 00:01:55.230
but I'll avoid using input activation because four x we often use also the,
uh,

30
00:01:55.260 --> 00:01:59.200
for the word input to describe it.
So instead,
I'll,

31
00:01:59.201 --> 00:02:03.630
I'll talk about this as the pre activation and the pre activation,

32
00:02:03.660 --> 00:02:08.660
which I'll note a of x for a given neuron is simply going to be some scale or B,

33
00:02:10.290 --> 00:02:14.020
which we're going to call a refer to as the bias,

34
00:02:14.021 --> 00:02:17.730
the neuron bias plus the,
uh,

35
00:02:17.760 --> 00:02:22.680
product between a weight factor w with some,
uh,

36
00:02:22.710 --> 00:02:24.150
the input vector x.

37
00:02:24.750 --> 00:02:29.010
So we can write it into vector form here or we can write it into a more scalar

38
00:02:29.011 --> 00:02:33.490
form where we have B plus the summation over all ISO,

39
00:02:33.540 --> 00:02:38.540
all indices within the vector of the IMF element of w Times d a l I f element of

40
00:02:42.180 --> 00:02:43.013
X.

41
00:02:44.190 --> 00:02:48.750
And then from this pre activation there neuron Luke compute its activation,

42
00:02:48.780 --> 00:02:53.780
which is sometimes referred to as the output activation of the neuron by simply

43
00:02:53.851 --> 00:02:58.770
taking the pre activation and passing it through a,
uh,

44
00:02:58.900 --> 00:03:00.160
activation function.

45
00:03:00.190 --> 00:03:04.390
So this z function here is what we're going to call activation function and are

46
00:03:04.391 --> 00:03:09.391
going to be different choices of potential activation functions we will be able

47
00:03:09.821 --> 00:03:13.540
to use.
So the UPC code activation,
uh,

48
00:03:13.541 --> 00:03:18.370
I will often know that that's h of x and that will be the activation.

49
00:03:18.860 --> 00:03:21.670
If I don't say output,
uh,
if I just say activation,

50
00:03:21.671 --> 00:03:24.430
I need the output activation of a given neuron.

51
00:03:24.730 --> 00:03:28.870
And so if we just put the expression for Amx,
uh,

52
00:03:28.970 --> 00:03:29.920
explicitly,

53
00:03:30.060 --> 00:03:34.870
it will be the activation function applied on this linear transformation of the

54
00:03:34.900 --> 00:03:38.140
input vector,
a made of the elements XII.

55
00:03:39.400 --> 00:03:42.990
So w we will refer to it as the,
uh,

56
00:03:43.240 --> 00:03:47.410
a vector of connection weights.
And that's because,
uh,

57
00:03:47.470 --> 00:03:49.270
as in this visualization here,

58
00:03:49.271 --> 00:03:54.271
we can think of the elements of w as the strength of the connection between the

59
00:03:54.791 --> 00:03:58.570
neurons to which are a neuron is connected to,

60
00:03:58.571 --> 00:04:02.090
in this case these neurons would be input neurons.
They would be,
uh,

61
00:04:02.350 --> 00:04:06.670
neurons that take the value of each of the element in the input vector.

62
00:04:07.720 --> 00:04:12.720
So w will be the vector of connection weights be will be a bias.

63
00:04:13.210 --> 00:04:18.040
It's a bias because it's,
uh,
if we have no input,
uh,

64
00:04:18.070 --> 00:04:20.380
B would be the pre activation.

65
00:04:20.410 --> 00:04:23.350
So by observing a particular input,

66
00:04:23.530 --> 00:04:28.010
then we are essentially going away from this initial value be,
uh,

67
00:04:28.190 --> 00:04:32.590
uh,
of the pre activation of the neuron.
And then as I said,
gee,

68
00:04:32.591 --> 00:04:34.090
will be the activation function.

69
00:04:36.380 --> 00:04:41.380
So there is a two d visualization of the activation of a neuron.

70
00:04:42.040 --> 00:04:46.330
So imagine we have a vector made of two element x one and x two,

71
00:04:47.290 --> 00:04:51.790
and then we'll note as why.
So I could have written instead h of x here,

72
00:04:52.840 --> 00:04:55.390
uh,
as the,
uh,
uh,

73
00:04:55.420 --> 00:04:58.480
output a activation of the neuron.
Okay.

74
00:04:58.960 --> 00:05:03.160
So first thing we see here is that on this axis here,
uh,

75
00:05:03.270 --> 00:05:05.800
we get values dialogue between minus one and one.

76
00:05:05.980 --> 00:05:08.830
And that will be because in this specific example,

77
00:05:09.040 --> 00:05:13.720
the range of values we can get once we pass the pre activation through the

78
00:05:13.721 --> 00:05:17.080
activation function is a number between minus one and one.

79
00:05:18.400 --> 00:05:22.240
And now what we see is that for a different values of x,

80
00:05:22.241 --> 00:05:26.050
we get a different output.
And specifically for,
uh,

81
00:05:26.051 --> 00:05:31.051
all the values that Lee here we get an output for the neuron and activation of

82
00:05:31.301 --> 00:05:35.890
minus one,
uh,
oh,
very close to minus one.
And for all values of x,

83
00:05:35.891 --> 00:05:39.880
here we get a value of one.
So in this particular case,

84
00:05:39.881 --> 00:05:44.300
we have a not to official neuron that detects whether,
uh,

85
00:05:44.350 --> 00:05:49.350
some given input point x one x two belongs to this part of the space or this

86
00:05:50.561 --> 00:05:53.820
part of this space.
So he can think of it as a,

87
00:05:53.840 --> 00:05:58.840
of the artificial neuron as a binary classifier that separates points in one

88
00:05:59.481 --> 00:06:02.330
region.
And some other region.
For now,

89
00:06:02.331 --> 00:06:05.570
we won't discuss how we actually find the parameters of the neuron that

90
00:06:05.571 --> 00:06:09.230
determine what regions it's actually separating.
So for now,

91
00:06:09.231 --> 00:06:14.231
we'll just assume that someone has given us the value of the wave vector w and

92
00:06:15.440 --> 00:06:19.550
the bias vector B,
which determines the shape of dysfunction.

93
00:06:20.510 --> 00:06:25.150
Uh,
and we'll see how we can actually have neurons that will train to,
uh,

94
00:06:25.190 --> 00:06:27.870
discover good values of these parameters.
So for now,

95
00:06:27.871 --> 00:06:32.420
we'll just assume that someone has given us these values.
Also,

96
00:06:32.421 --> 00:06:33.350
geometrically,

97
00:06:33.650 --> 00:06:38.650
it turns out that the vector w we'll actually be perpendicular with respect to

98
00:06:40.910 --> 00:06:42.180
the,
uh,

99
00:06:42.250 --> 00:06:46.070
hybrid plane in this space that separates these two regions,

100
00:06:46.071 --> 00:06:50.180
that the neuron,
uh,
is distinguishing is separating.

101
00:06:50.510 --> 00:06:55.510
So this vector w determines essentially the orientation of this ridge between

102
00:06:55.911 --> 00:06:59.150
the two parts of this space that,
uh,
the neuron,
the separating,

103
00:06:59.960 --> 00:07:01.400
and then the bias be,

104
00:07:01.430 --> 00:07:05.630
we'll essentially determine along this direction where the rich will be.

105
00:07:06.110 --> 00:07:10.460
So if the bias is,
uh,
uh,
if the biases large,

106
00:07:10.550 --> 00:07:12.860
so when the bias is increasing,

107
00:07:13.040 --> 00:07:17.930
then the ridge will be moving in the opposite direction of w.

108
00:07:18.410 --> 00:07:21.970
So this would correspond to a,
uh,

109
00:07:22.280 --> 00:07:25.850
perhaps negative value of beam.
And if we had the,
uh,

110
00:07:26.060 --> 00:07:30.590
more strongly positive value of B than the ridge would be a moving in this

111
00:07:30.591 --> 00:07:34.250
direction.
Okay.
So I want to exactly explain why this is,

112
00:07:34.251 --> 00:07:38.240
but you can sit down and try to figure out why it is that w is perpendicular to

113
00:07:38.241 --> 00:07:41.330
this ridge.
And why is it that when bees decree is increasing,

114
00:07:41.600 --> 00:07:44.240
then the rich would be moving,
uh,
this way.

115
00:07:45.140 --> 00:07:49.700
So this is the description of the computation that the artificial neuron is

116
00:07:49.701 --> 00:07:50.210
performing.

