WEBVTT
Kind: captions
Language: en

00:00:06.330 --> 00:00:09.010
SPEAKER: Today at Google, we're
excited to welcome Mr. Noam

00:00:09.010 --> 00:00:10.300
Cohen.

00:00:10.300 --> 00:00:13.870
Mr. Cohen is a former technology
columnist at the New York Times

00:00:13.870 --> 00:00:16.810
where, beginning in
2007, he wrote the Bit

00:00:16.810 --> 00:00:20.080
By Bit column, which covered
the impact of the internet

00:00:20.080 --> 00:00:21.760
on society.

00:00:21.760 --> 00:00:24.890
He recently left the Times in
order to write his first book,

00:00:24.890 --> 00:00:26.950
which he is here
today to discuss.

00:00:26.950 --> 00:00:30.850
It's titled, "The Know-It-Alls:
The Rise of Silicon Valley

00:00:30.850 --> 00:00:34.830
as a Political Powerhouse
and Social Wrecking Ball."

00:00:34.830 --> 00:00:36.840
The book is an
incisive biography

00:00:36.840 --> 00:00:39.990
of some of Silicon Valley's
most venerated leaders.

00:00:39.990 --> 00:00:42.900
It offers a critique of the
current state of the internet

00:00:42.900 --> 00:00:45.930
and explores the implications
of radical individualism

00:00:45.930 --> 00:00:49.521
on democratic ideals
in American life.

00:00:49.521 --> 00:00:51.980
Please join me in welcoming
to Google, Mr. Noam Cohen.

00:00:51.980 --> 00:00:55.210
NOAM COHEN: Thank you.

00:00:55.210 --> 00:00:57.400
Nice to be here, and I feel
it's a little like going

00:00:57.400 --> 00:00:58.858
to the belly of
the beast, but it's

00:00:58.858 --> 00:01:00.080
been very welcoming in here.

00:01:00.080 --> 00:01:02.265
Nice and warm in the belly.

00:01:02.265 --> 00:01:04.890
I'm glad to be here.

00:01:04.890 --> 00:01:06.500
[INTERPOSING VOICES]

00:01:07.952 --> 00:01:09.910
SPEAKER: So I'd to just
start at the beginning.

00:01:09.910 --> 00:01:11.410
Where did the idea for
the book come from?

00:01:11.410 --> 00:01:13.451
NOAM COHEN: Right, so,
you know as you mentioned,

00:01:13.451 --> 00:01:16.679
I was a tech writer at the
Times for a lot of years.

00:01:16.679 --> 00:01:18.220
I would write in
the business section

00:01:18.220 --> 00:01:19.820
but I often would not
write about businesses,

00:01:19.820 --> 00:01:21.330
I'd I write a lot
about Wikipedia,

00:01:21.330 --> 00:01:22.600
I'd write about free software.

00:01:22.600 --> 00:01:25.990
I was really intrigued by
how the internet and the web

00:01:25.990 --> 00:01:28.509
was changing society and
little less about the business

00:01:28.509 --> 00:01:29.300
implications of it.

00:01:29.300 --> 00:01:31.130
And you know, I was
always kind of curious--

00:01:31.130 --> 00:01:33.504
I was in the business section,
but I just didn't really--

00:01:33.504 --> 00:01:39.290
I didn't really focus on it
companies and such like that.

00:01:39.290 --> 00:01:41.560
But over time I kind
of couldn't keep

00:01:41.560 --> 00:01:45.550
my head buried to see that how
these great innovations were

00:01:45.550 --> 00:01:46.750
becoming part of businesses.

00:01:46.750 --> 00:01:48.580
And I wanted to
sort of see how we

00:01:48.580 --> 00:01:49.990
got to our current situation.

00:01:49.990 --> 00:01:53.380
The situation where so
much of our interaction

00:01:53.380 --> 00:01:55.720
with the internet is
through big companies

00:01:55.720 --> 00:01:58.390
like Google and
Amazon and Facebook.

00:01:58.390 --> 00:02:01.870
And I really felt there
was a corrosion coming,

00:02:01.870 --> 00:02:04.511
so that sort of values
that were being caught

00:02:04.511 --> 00:02:06.510
through these companies
and through the internet

00:02:06.510 --> 00:02:08.320
or becoming quite
libertarian and quite

00:02:08.320 --> 00:02:11.670
a threat to the
social fabric and what

00:02:11.670 --> 00:02:14.200
I would call libertarian ideas.

00:02:14.200 --> 00:02:18.520
And of course, the meddling
in the 2016 election

00:02:18.520 --> 00:02:19.820
became this real watershed.

00:02:19.820 --> 00:02:22.300
So I had these
concerns, but it kind of

00:02:22.300 --> 00:02:24.560
made things much
more clear as to why

00:02:24.560 --> 00:02:26.720
the stakes were very high.

00:02:26.720 --> 00:02:29.844
That our democracy
could be at stake.

00:02:29.844 --> 00:02:32.260
Rather than kind of being a
critic of Silicon Valley being

00:02:32.260 --> 00:02:34.090
like, is Google making you dumb?

00:02:34.090 --> 00:02:36.880
Or do young people care about
privacy, the question was like,

00:02:36.880 --> 00:02:40.270
are these companies kind of
compatible with democracy?

00:02:40.270 --> 00:02:42.820
So that give my book
a lot of urgency.

00:02:42.820 --> 00:02:44.680
And obviously, I worked
on it before that,

00:02:44.680 --> 00:02:48.170
and I was trying to kind of
look at these kind of questions.

00:02:48.170 --> 00:02:51.370
And when I kind of came
up with-- an operating

00:02:51.370 --> 00:02:54.100
idea of the book is that there
are these two forces that

00:02:54.100 --> 00:02:55.636
account for what's going on.

00:02:55.636 --> 00:02:57.510
One force is the history
of computer science.

00:02:57.510 --> 00:02:59.570
Like you said, I went
really deep dive,

00:02:59.570 --> 00:03:02.920
looking at how computer
science can be developed

00:03:02.920 --> 00:03:04.360
and also Stanford.

00:03:04.360 --> 00:03:05.830
And so until I'd
done the book, I

00:03:05.830 --> 00:03:09.310
hadn't realized how important AI
was to the history of computer

00:03:09.310 --> 00:03:09.880
science.

00:03:09.880 --> 00:03:11.254
I guess maybe
that's well-known--

00:03:11.254 --> 00:03:12.960
I didn't realize that is what--

00:03:12.960 --> 00:03:15.430
I focused one chapter about
John McCarthy, who was

00:03:15.430 --> 00:03:16.840
this fairly significant figure.

00:03:16.840 --> 00:03:18.040
Particularly in
Google, I would think

00:03:18.040 --> 00:03:19.990
he'd be very venerated,
like you were saying.

00:03:19.990 --> 00:03:23.860
I know that early on, he was in
the same building with Sergey

00:03:23.860 --> 00:03:26.620
Brin and Larry Page and they
were grad students at Stanford.

00:03:26.620 --> 00:03:29.385
He came early on to Google
to visit the campus,

00:03:29.385 --> 00:03:31.510
and I was really struck
that even the Google search

00:03:31.510 --> 00:03:35.510
engine early on used McCarthy's
website, its early website

00:03:35.510 --> 00:03:37.694
at Stanford-- they used
that as a way of proving

00:03:37.694 --> 00:03:39.610
that personalized search
would be much better.

00:03:39.610 --> 00:03:42.670
They took all the knowledge
you get from McCarthy's website

00:03:42.670 --> 00:03:45.130
and they plug it
into their page rank

00:03:45.130 --> 00:03:49.170
and it produced some
much better results.

00:03:49.170 --> 00:03:51.200
So this McCarthy is such
a fascinating figure,

00:03:51.200 --> 00:03:53.283
and he's sort of not been
written about that much.

00:03:53.283 --> 00:03:55.600
He becomes like a surrogate
for computer science for me

00:03:55.600 --> 00:04:01.120
that he was born to communist
parents, he was a math genius,

00:04:01.120 --> 00:04:03.900
and over time, he was fascinated
with artificial intelligence

00:04:03.900 --> 00:04:05.962
and wanted to create
a thinking being.

00:04:05.962 --> 00:04:07.920
And so this early pioneer
of computer science--

00:04:07.920 --> 00:04:09.420
he created a list program--

00:04:09.420 --> 00:04:12.850
he was motivated by
making a thinking being.

00:04:12.850 --> 00:04:15.741
And that's kind of what's
driven computer science history,

00:04:15.741 --> 00:04:16.240
I found.

00:04:16.240 --> 00:04:18.281
So I was looking at that,
so what does that mean,

00:04:18.281 --> 00:04:20.260
McCarthy became a
mentor to the hackers?

00:04:20.260 --> 00:04:23.020
And it means that
that idea of seeing

00:04:23.020 --> 00:04:25.960
a machine as a person and
a person as a machine,

00:04:25.960 --> 00:04:28.340
and this idea that computers
can really save the world,

00:04:28.340 --> 00:04:30.640
was really important
to McCarthy.

00:04:30.640 --> 00:04:34.570
And that thread is what I'm kind
of borrowing from Steven Levy.

00:04:34.570 --> 00:04:37.090
So I call it the hacker
ethic, this idea that

00:04:37.090 --> 00:04:39.610
being highly individualistic,
what you see on the Google

00:04:39.610 --> 00:04:41.800
campus, people are free
to do what they want,

00:04:41.800 --> 00:04:45.580
really challenging authority,
and also kind of not

00:04:45.580 --> 00:04:47.440
very valuing
intelligence very highly,

00:04:47.440 --> 00:04:49.330
and not kind of
caring about diversity

00:04:49.330 --> 00:04:52.150
of women or minorities
and sort of this idea

00:04:52.150 --> 00:04:54.700
that merit and intelligence is
what gets you where you are.

00:04:54.700 --> 00:04:57.040
So that was one thread,
was, this history

00:04:57.040 --> 00:04:57.910
of computer science.

00:04:57.910 --> 00:04:59.959
I went deep into
that and McCarthy.

00:04:59.959 --> 00:05:01.250
The second thread was Stanford.

00:05:01.250 --> 00:05:02.950
I kind of learned
how Stanford became

00:05:02.950 --> 00:05:05.560
so crucial to the idea of
entrepreneurship and the idea

00:05:05.560 --> 00:05:08.020
that you should take
innovations that come,

00:05:08.020 --> 00:05:11.396
research from your alumni, your
students, and your faculty,

00:05:11.396 --> 00:05:12.520
and put them to the market.

00:05:12.520 --> 00:05:15.700
And that kind of
began with this fellow

00:05:15.700 --> 00:05:19.060
who has a chapter in a book,
Lewis Terman, who brought

00:05:19.060 --> 00:05:20.200
Hewlett-Packard together.

00:05:20.200 --> 00:05:22.270
These two graduate students
of his in the '30s.

00:05:22.270 --> 00:05:24.269
He said, you two should
go to business together.

00:05:24.269 --> 00:05:25.900
You gave them some
money to start up,

00:05:25.900 --> 00:05:28.420
he gave them their first
contract with Walt Disney back

00:05:28.420 --> 00:05:30.550
in the 30s-- and he sort
of preached this idea

00:05:30.550 --> 00:05:36.550
that research should have a
practical business-related use.

00:05:36.550 --> 00:05:39.400
And that, he thought, would help
Stanford rise up in stature.

00:05:39.400 --> 00:05:41.605
When he took over as
provost in the 50s,

00:05:41.605 --> 00:05:44.080
Stanford was considered
this kind of middling school

00:05:44.080 --> 00:05:46.530
for kind of well-off
people in Los Angeles,

00:05:46.530 --> 00:05:47.710
like a quote I saw.

00:05:47.710 --> 00:05:50.440
And by the time he left,
Stanford was going way up.

00:05:50.440 --> 00:05:52.930
And, of course, we now
today see how many companies

00:05:52.930 --> 00:05:56.410
have come out of Stanford, and
that money finds its way back

00:05:56.410 --> 00:05:56.920
to Stanford.

00:05:56.920 --> 00:06:00.220
Stanford is this rich, booming,
highly-prestigious school.

00:06:00.220 --> 00:06:03.430
So those two threads,
kind of the hacker ethic

00:06:03.430 --> 00:06:05.770
and the entrepreneurism
is kind of where I see how

00:06:05.770 --> 00:06:07.840
we got to where we are today.

00:06:07.840 --> 00:06:10.690
I felt like I got an
answer to my question.

00:06:10.690 --> 00:06:12.670
SPEAKER: So it sort of
seems like in the book

00:06:12.670 --> 00:06:18.120
you trace a long period of
time starting with McCarthy.

00:06:18.120 --> 00:06:19.180
Is that the '50s?

00:06:19.180 --> 00:06:21.160
NOAM COHEN: Yeah, he
was this figure, right?

00:06:21.160 --> 00:06:25.600
So he is known for being kind
of a mentor, Uncle John they

00:06:25.600 --> 00:06:29.011
called him at MIT, the
mentor to the hackers.

00:06:29.011 --> 00:06:31.510
In the '50s at MIT, he taught
first sort of computer science

00:06:31.510 --> 00:06:33.400
class or freshmen in the '50s.

00:06:33.400 --> 00:06:36.064
And I think what was really
vital about McCarthy too,

00:06:36.064 --> 00:06:37.480
which I talk about
in the book, is

00:06:37.480 --> 00:06:39.604
that this idea he came up
with, time sharing, which

00:06:39.604 --> 00:06:43.600
was that, prior to time
sharing, the computers were

00:06:43.600 --> 00:06:46.900
these mainframes that
you had to approach

00:06:46.900 --> 00:06:48.070
like as priestly class.

00:06:48.070 --> 00:06:49.570
You had to kind of
go to an operator

00:06:49.570 --> 00:06:52.210
and give them questions,
and they would come back

00:06:52.210 --> 00:06:54.100
like [INAUDIBLE] answers.

00:06:54.100 --> 00:06:57.210
And instead, he said, no, we
should have terminals hooked up

00:06:57.210 --> 00:06:59.200
to this one main frame,
so lots of individuals

00:06:59.200 --> 00:07:01.660
can interact with the
computer directly.

00:07:01.660 --> 00:07:03.820
I think for him that
was key to AI, this idea

00:07:03.820 --> 00:07:05.890
that if you're going to
make a thinking machine,

00:07:05.890 --> 00:07:08.400
they should interact with
as many people as possible

00:07:08.400 --> 00:07:10.900
and sort and learn from
people and sort of judge

00:07:10.900 --> 00:07:12.530
them on how they
interact with people.

00:07:12.530 --> 00:07:14.560
So he became a real force for
making computers much more

00:07:14.560 --> 00:07:17.170
accessible, which again, we're
living with today, this idea

00:07:17.170 --> 00:07:20.670
that computers aren't an oracle,
they're a personal thing.

00:07:20.670 --> 00:07:23.880
Really, the cloud I think
even comes from that idea.

00:07:23.880 --> 00:07:26.000
SPEAKER: So John
McCarthy-- that was sort of

00:07:26.000 --> 00:07:28.075
in the first
generation of hackers,

00:07:28.075 --> 00:07:30.950
who had that the hacker ethic.

00:07:30.950 --> 00:07:35.170
And then later this sort of
entrepreneur Frederick Terman

00:07:35.170 --> 00:07:37.434
at Stanford sort of
brought this other side

00:07:37.434 --> 00:07:39.100
of what you called
the know-it-all, sort

00:07:39.100 --> 00:07:42.400
of the business-minded
application side.

00:07:42.400 --> 00:07:45.790
How would you
distinguish, say, McCarthy

00:07:45.790 --> 00:07:48.010
in the first generation
of hackers' motivations

00:07:48.010 --> 00:07:50.800
and the way that they
approached computing

00:07:50.800 --> 00:07:54.310
and what they wanted
computers to do for society--

00:07:54.310 --> 00:07:57.070
how would you distinguish that
with what maybe entrepreneurs

00:07:57.070 --> 00:07:58.390
in technology are doing today?

00:07:58.390 --> 00:08:00.100
NOAM COHEN: I was
really struck that how

00:08:00.100 --> 00:08:01.730
idealistic McCarthy was.

00:08:01.730 --> 00:08:03.610
McCarthy never
went into business.

00:08:03.610 --> 00:08:05.740
And this idea of a
singularity, right?

00:08:05.740 --> 00:08:08.990
This idea that computers are
going to get smarter than us,

00:08:08.990 --> 00:08:10.707
we people, and then
eventually, smarter

00:08:10.707 --> 00:08:12.790
computers are going to
make even smarter computers

00:08:12.790 --> 00:08:14.930
and then it's like
off to the races.

00:08:14.930 --> 00:08:17.080
So he, in '68, gave
a lecture at Stanford

00:08:17.080 --> 00:08:18.820
that raised that exact point.

00:08:18.820 --> 00:08:21.655
But rather than fearing this
would mean that computers

00:08:21.655 --> 00:08:23.530
are going to take over
the world that they'll

00:08:23.530 --> 00:08:26.470
be our overlords, he
said, when that happens,

00:08:26.470 --> 00:08:30.130
the first thing we do
is, we ask the question--

00:08:30.130 --> 00:08:32.650
hey computer, we made you,
you're smarter than us,

00:08:32.650 --> 00:08:33.730
what should we do now?

00:08:33.730 --> 00:08:36.030
He believed intelligence
was a good thing.

00:08:36.030 --> 00:08:39.860
He didn't fear it,
I assume he thought

00:08:39.860 --> 00:08:44.360
having a thinking computer
would explain the cosmos.

00:08:44.360 --> 00:08:46.480
We have telescopes; we're
trying to understand

00:08:46.480 --> 00:08:47.862
what we're doing here--

00:08:47.862 --> 00:08:49.570
a smarter computer is
going to really get

00:08:49.570 --> 00:08:51.025
an answer-- what is this?

00:08:51.025 --> 00:08:51.900
What is the universe?

00:08:51.900 --> 00:08:57.580
And so that idealism and that
almost impractical nature

00:08:57.580 --> 00:09:01.729
I think was true of that
era of AI researchers.

00:09:01.729 --> 00:09:03.520
And you read about the
Stanford lab, right?

00:09:03.520 --> 00:09:05.230
McCarthy made this transition.

00:09:05.230 --> 00:09:06.064
He went from MIT--

00:09:06.064 --> 00:09:08.230
he was sort of frustrated
by the weather, frustrated

00:09:08.230 --> 00:09:10.230
by other things, and he
moved to Stanford in '62

00:09:10.230 --> 00:09:13.000
and recreated the
MIT lab at Stanford.

00:09:13.000 --> 00:09:16.450
And it became this real
hothouse of this really

00:09:16.450 --> 00:09:20.260
kind of innovative and also
just like free spirits.

00:09:20.260 --> 00:09:22.270
And you know, there was
actually Russell Brand--

00:09:22.270 --> 00:09:24.670
the famous founder of
the Whole Earth Catalog--

00:09:24.670 --> 00:09:29.410
who wrote this profile of the
Stanford AI Lab in the '60s.

00:09:29.410 --> 00:09:32.030
It was when they were making
the first video game Space War.

00:09:32.030 --> 00:09:35.410
It just was a very loosely
goosey and not nearly as

00:09:35.410 --> 00:09:37.600
efficiency-driven kind of world.

00:09:37.600 --> 00:09:38.530
Now, cut to today--

00:09:38.530 --> 00:09:42.300
I think partly it's out
of the failure of AI,

00:09:42.300 --> 00:09:44.530
is how I see things,
that AI didn't

00:09:44.530 --> 00:09:48.460
succeed in creating this
cosmos-understanding thinking

00:09:48.460 --> 00:09:49.330
being.

00:09:49.330 --> 00:09:51.490
Rather, it created something
that kind of imitates

00:09:51.490 --> 00:09:55.840
a person and that quality has
been very useful for businesses

00:09:55.840 --> 00:09:57.790
because these algorithms
aren't actually

00:09:57.790 --> 00:09:59.170
answering the big questions.

00:09:59.170 --> 00:10:01.720
They aren't approaching
what we consider special

00:10:01.720 --> 00:10:02.980
about human intelligence.

00:10:02.980 --> 00:10:05.960
They're just sort of doing
things much more efficiently.

00:10:05.960 --> 00:10:09.730
And that lack of
success has actually

00:10:09.730 --> 00:10:14.635
led to these businesses making
great hay of that development.

00:10:14.635 --> 00:10:16.760
I'll put it this way-- it
was interesting to watch,

00:10:16.760 --> 00:10:18.718
you talk about looking
at the history of this--

00:10:18.718 --> 00:10:20.811
the history of chess
computers itself

00:10:20.811 --> 00:10:22.060
is a fascinating thing, right?

00:10:22.060 --> 00:10:24.460
When McCarthy created
one of the first chess

00:10:24.460 --> 00:10:25.705
programs-- it was really
his under-graduates

00:10:25.705 --> 00:10:26.913
that did all the work, right?

00:10:26.913 --> 00:10:28.900
He was the professor.

00:10:28.900 --> 00:10:31.520
And it was amazing
to think that chess,

00:10:31.520 --> 00:10:34.930
this intellectual activity
that only people who

00:10:34.930 --> 00:10:37.990
smoke pipes and are
reading great books,

00:10:37.990 --> 00:10:40.960
and that computer could
be better than a human,

00:10:40.960 --> 00:10:41.650
it's amazing.

00:10:41.650 --> 00:10:44.950
So it won its first
match, and in fact, there

00:10:44.950 --> 00:10:48.470
was a famous critic of AI,
this guy Hubert Dreyfus,

00:10:48.470 --> 00:10:50.122
who was at Berkeley, right?

00:10:50.122 --> 00:10:51.580
A philosophy
professor that kind of

00:10:51.580 --> 00:10:53.520
questioned the whole
idea of AI, and he said,

00:10:53.520 --> 00:10:54.686
and what is AI doing anyway?

00:10:54.686 --> 00:10:56.500
It can't any beat a
12-year-old in chess.

00:10:56.500 --> 00:10:58.000
So they became
obsessed with beating

00:10:58.000 --> 00:11:00.670
Hubert Dreyfus at chess,
which they did in 1966.

00:11:00.670 --> 00:11:03.520
It was a big match because
he didn't back out of it,

00:11:03.520 --> 00:11:05.110
and they beat Hubert
Dreyfus at chess.

00:11:05.200 --> 00:11:07.430
The New Yorker wrote
about it as a big event.

00:11:07.430 --> 00:11:10.010
And so you think that was
a huge accomplishment,

00:11:10.010 --> 00:11:11.590
and of course now,
chess computers

00:11:11.590 --> 00:11:13.069
are better than any human--

00:11:13.069 --> 00:11:15.610
but of course McCarthy realized
that, what was succeeding at?

00:11:15.610 --> 00:11:17.680
It wasn't thinking better.

00:11:17.680 --> 00:11:19.790
It was just calculating
so much faster.

00:11:19.790 --> 00:11:21.550
And so in that
sense, that speaks

00:11:21.550 --> 00:11:23.200
to exactly what we're at.

00:11:23.200 --> 00:11:25.306
We need a computer--
he would never

00:11:25.306 --> 00:11:27.430
have dreamed that a computer
could have been better

00:11:27.430 --> 00:11:28.502
than any chess match?

00:11:28.502 --> 00:11:30.460
It was a really ambitious
thing to think about.

00:11:30.460 --> 00:11:32.500
Now that's easily done,
but it doesn't have

00:11:32.500 --> 00:11:33.880
any real payoff in that sense.

00:11:33.880 --> 00:11:35.950
So I think today,
people are more focused

00:11:35.950 --> 00:11:37.334
on making a great
chess computer,

00:11:37.334 --> 00:11:40.000
and that's an awesome thing, and
McCarthy would have been like--

00:11:40.000 --> 00:11:42.074
was, in fact--
that's not anything.

00:11:42.074 --> 00:11:43.490
So that's the
difference, I'd say.

00:11:43.490 --> 00:11:45.212
The idealism, the
ambition maybe.

00:11:45.212 --> 00:11:47.420
SPEAKER: And you think it's
the market influence that

00:11:47.420 --> 00:11:48.420
maybe shifted the focus?

00:11:48.420 --> 00:11:49.711
NOAM COHEN: Well, I don't know.

00:11:49.711 --> 00:11:52.120
I think maybe McCarthy would
have just not done anything

00:11:52.120 --> 00:11:54.292
with it, because if it
didn't answer his question,

00:11:54.292 --> 00:11:55.250
he would have moved on.

00:11:55.250 --> 00:11:57.010
I think the markets
at least show

00:11:57.010 --> 00:11:59.470
there is a real utility
to it, a real utility

00:11:59.470 --> 00:12:02.200
to these algorithms
that have a real power.

00:12:02.200 --> 00:12:03.700
It wasn't what he
was interested in.

00:12:03.700 --> 00:12:05.574
I'll tell one other--

00:12:05.574 --> 00:12:07.240
I'm almost thinking
about the next thing

00:12:07.240 --> 00:12:09.740
I want to write about, and I
mentioned a bit in this op-ed I

00:12:09.740 --> 00:12:11.290
wrote in the Times about Eliza.

00:12:11.290 --> 00:12:14.470
To me, Eliza, this program,
is really interesting to me

00:12:14.470 --> 00:12:15.790
about all of these questions.

00:12:15.790 --> 00:12:19.570
Because Eliza was written
by this MIT professor Joseph

00:12:19.570 --> 00:12:22.720
Weizenbaum who was a bit of
a rival McCarthy's, and it

00:12:22.720 --> 00:12:25.840
was a chat bot, and really
crude, he wrote it so quickly.

00:12:25.840 --> 00:12:27.580
And it just imitated
a therapist, right?

00:12:27.580 --> 00:12:30.670
So it just sort of spit back
what you would say to it.

00:12:30.670 --> 00:12:33.520
You'd say, oh, I'm really
nervous to be here.

00:12:33.520 --> 00:12:34.880
Why are you nervous?

00:12:34.880 --> 00:12:37.150
Well, you know, my family
doesn't like to talk a lot.

00:12:37.150 --> 00:12:37.800
Does your family bother you?

00:12:37.800 --> 00:12:40.660
I'm just asking very open-ended
questions with no knowledge,

00:12:40.660 --> 00:12:41.940
using key word, right?

00:12:41.940 --> 00:12:44.710
Key words that were kind
of repeated back to you--

00:12:44.710 --> 00:12:46.690
so Weizenbaum made
this program and was

00:12:46.690 --> 00:12:49.270
shocked to realize that his
own secretary was interacting

00:12:49.270 --> 00:12:49.790
with it.

00:12:49.790 --> 00:12:52.150
And he's looking over her
shoulder, and she's like,

00:12:52.150 --> 00:12:53.350
can you give me
some privacy please?

00:12:53.350 --> 00:12:54.183
And he's like, what?

00:12:54.183 --> 00:12:56.530
You're just talking to
a bot I made in one day.

00:12:56.530 --> 00:12:59.740
And he realized there was a
huge power to these computers

00:12:59.740 --> 00:13:01.750
and a huge ability
of people trust

00:13:01.750 --> 00:13:03.730
them and to be manipulated.

00:13:03.730 --> 00:13:05.000
And he was interested in that.

00:13:05.000 --> 00:13:07.000
So I think the same
thing, that McCarthy just

00:13:07.000 --> 00:13:09.000
wouldn't have been
interested in using computers

00:13:09.000 --> 00:13:11.950
to imitate people,
to manipulate them.

00:13:11.950 --> 00:13:13.480
That wasn't what he was doing.

00:13:13.480 --> 00:13:15.146
I don't think it's a
market force thing,

00:13:15.146 --> 00:13:17.850
I think market people
saw that's a great tool.

00:13:20.530 --> 00:13:22.950
SPEAKER: What is that,
you label something there,

00:13:22.950 --> 00:13:25.070
a libertarian in the book?

00:13:25.070 --> 00:13:26.790
What is that?

00:13:26.790 --> 00:13:27.540
NOAM COHEN: Right.

00:13:27.540 --> 00:13:28.498
I guess I should focus.

00:13:28.498 --> 00:13:30.730
The point I'm trying to say
is that the know-it-alls

00:13:30.730 --> 00:13:35.150
are kind of promoting a
libertarian ideology, very

00:13:35.150 --> 00:13:36.890
hyper-individualistic ideology.

00:13:36.890 --> 00:13:39.770
And I think it comes from
a variety of reasons,

00:13:39.770 --> 00:13:43.730
like the hacker ethic is part of
that, the good part of saying,

00:13:43.730 --> 00:13:44.930
leave me alone.

00:13:44.930 --> 00:13:47.180
I fly my own free flag.

00:13:47.180 --> 00:13:49.070
And so it's part of
that, but also it's

00:13:49.070 --> 00:13:52.280
just the breaking
down of institutions

00:13:52.280 --> 00:13:53.610
is also part of it.

00:13:53.610 --> 00:13:56.010
And kind of people
doing more on their own.

00:13:56.010 --> 00:13:59.300
So to me, the
libertarian is kind

00:13:59.300 --> 00:14:05.556
of people who don't see
the costs of libertarianism

00:14:05.556 --> 00:14:06.930
but they see the
good side, which

00:14:06.930 --> 00:14:10.170
is the freedom people have.

00:14:10.170 --> 00:14:12.510
I was telling them beforehand,
like Elon Musk is maybe

00:14:12.510 --> 00:14:14.677
an example of this or Jeff
Bezos I think is another,

00:14:14.677 --> 00:14:17.218
because he's a big supporter of
Libertarian Magazine, Reason.

00:14:17.218 --> 00:14:19.016
And he identifies I
think in a lot of ways

00:14:19.016 --> 00:14:21.170
with libertarianism, but
also I think sees himself

00:14:21.170 --> 00:14:22.170
as a little bit liberal.

00:14:22.170 --> 00:14:27.450
And it's this idea that, oh,
I take ideas from both sides

00:14:27.450 --> 00:14:31.050
or, I see the
libertarianism allowing

00:14:31.050 --> 00:14:34.530
you to have gay marriage,
and have freedom

00:14:34.530 --> 00:14:36.360
that smoke wherever you want--

00:14:36.360 --> 00:14:39.480
and then sort of not seeing
the world government.

00:14:39.480 --> 00:14:41.700
Seeing government as a
bad thing-- so, saying,

00:14:41.700 --> 00:14:44.910
government is actually hurting
people, it's limiting people.

00:14:44.910 --> 00:14:48.090
There's no freedom
to do your own car,

00:14:48.090 --> 00:14:49.714
or that free speech limits.

00:14:49.714 --> 00:14:51.380
So rather than seeing
free speech limits

00:14:51.380 --> 00:14:55.170
as about protecting
people who are weak--

00:14:55.170 --> 00:14:58.220
minorities and women-- you
can see free speech protection

00:14:58.220 --> 00:15:00.930
is clamping down an individual.

00:15:00.930 --> 00:15:02.020
I can't say what I want.

00:15:02.020 --> 00:15:04.320
So you can kind of frame
it like a liberal thing.

00:15:04.320 --> 00:15:08.240
I think that's kind of
saying that libertarians,

00:15:08.240 --> 00:15:11.040
they're trying to put a
much more positive spin

00:15:11.040 --> 00:15:11.760
on libertarians.

00:15:11.760 --> 00:15:14.540
Maybe not like this kind of
feudalism of libertarianism,

00:15:14.540 --> 00:15:17.400
which is that we're all
in a battle for resources

00:15:17.400 --> 00:15:19.500
and let the strongest win--

00:15:19.500 --> 00:15:20.440
that's bad PR.

00:15:20.440 --> 00:15:24.760
I think this is a more kind of
humane face for libertarianism.

00:15:24.760 --> 00:15:27.600
I think that's kind of what is
coming out of Silicon Valley.

00:15:27.600 --> 00:15:29.799
It's not this harsh,
you know, other

00:15:29.799 --> 00:15:31.340
than maybe Peter
Thiel, or something.

00:15:31.340 --> 00:15:33.300
It's not a harsh
libertarian view.

00:15:33.300 --> 00:15:36.458
It's this more soft, we
want everyone to excel.

00:15:36.458 --> 00:15:37.583
And that's what it's about.

00:15:37.583 --> 00:15:39.400
It's that individualism.

00:15:39.400 --> 00:15:42.230
So yeah, I think
that's a key issue.

00:15:42.230 --> 00:15:45.450
SPEAKER: And so in the book, you
critique the sort of internet

00:15:45.450 --> 00:15:49.210
that's evolved out of this
sort of libertarian ethic.

00:15:49.210 --> 00:15:52.440
It's sort of commercialized
and centralized--

00:15:52.440 --> 00:15:54.180
and you say, in
place of that, we

00:15:54.180 --> 00:15:57.640
should be focusing more on
local, like local institutions

00:15:57.640 --> 00:16:02.700
and newspapers, trade unions--

00:16:02.700 --> 00:16:07.030
what was the inspiration
for that in your mind?

00:16:07.030 --> 00:16:09.610
Who were your influences that
may have shaped your thinking?

00:16:09.610 --> 00:16:13.585
NOAM COHEN: I guess I was
saying, an important thinker,

00:16:13.585 --> 00:16:14.335
this philosopher--

00:16:14.335 --> 00:16:16.890
[INAUDIBLE] Moller
Okin, who wrote a book

00:16:16.890 --> 00:16:19.050
about, it's called,
"Justice, Family,

00:16:19.050 --> 00:16:22.114
and--" it's a critique
of libertarianism

00:16:22.114 --> 00:16:23.280
from a feminist perspective.

00:16:23.280 --> 00:16:25.560
And it really spoke to me,
because basically, what she

00:16:25.560 --> 00:16:29.760
was saying is that,
libertarianism

00:16:29.760 --> 00:16:31.050
has to devalue women.

00:16:31.050 --> 00:16:35.020
It's not a coincidence that
there aren't tech sisters.

00:16:35.020 --> 00:16:40.090
Tech bros are a thing because
libertarianism is this idea

00:16:40.090 --> 00:16:42.820
that you kind of arrive
at your station in life

00:16:42.820 --> 00:16:45.610
on your own work and worth.

00:16:45.610 --> 00:16:47.880
And it kind of
assumes that we all

00:16:47.880 --> 00:16:51.114
land as fully-formed
adults and then

00:16:51.114 --> 00:16:53.530
we kind of deal with how we're
going to allocate resources

00:16:53.530 --> 00:16:54.160
in our society.

00:16:54.160 --> 00:16:56.980
And she was pointing out
that, now, obviously, today we

00:16:56.980 --> 00:16:59.200
have more equal share
over raising kids.

00:16:59.200 --> 00:17:03.140
But fundamentally, young adults
arrived there through women.

00:17:03.140 --> 00:17:05.599
And she was making
a kind of argument--

00:17:05.599 --> 00:17:07.490
reductio ad absurdum
argument-- of saying

00:17:07.490 --> 00:17:10.760
that if the extreme libertarians
say whatever you produce

00:17:10.760 --> 00:17:13.490
is your own, she was saying,
well then, if women produce

00:17:13.490 --> 00:17:16.859
children, do they own them?

00:17:16.859 --> 00:17:20.270
She was trying to cut
through all of the fallacy

00:17:20.270 --> 00:17:22.970
that we're just individuals
striving to get there,

00:17:22.970 --> 00:17:25.010
that you're a
product of a family,

00:17:25.010 --> 00:17:26.861
and then family is
part of a neighborhood,

00:17:26.861 --> 00:17:28.319
that neighbor is
part of a society.

00:17:28.319 --> 00:17:30.700
And so I think that that was
really influential on me.

00:17:30.700 --> 00:17:34.610
To see that in some ways the
kind of male culture of Silicon

00:17:34.610 --> 00:17:37.550
Valley and libertarian culture
of Silicon Valley were related.

00:17:37.550 --> 00:17:43.220
It's kind of wishing away how
we got here and kind of just

00:17:43.220 --> 00:17:44.390
wanting to be--

00:17:44.390 --> 00:17:45.860
it's very adolescent,
I would say.

00:17:45.860 --> 00:17:48.130
I say that in good and bad
ways-- adolescent instinct.

00:17:48.130 --> 00:17:51.980
To be, I'm here, I want to
make my mark on the world.

00:17:51.980 --> 00:17:53.540
You know, who cares?

00:17:53.540 --> 00:17:54.830
Daddy, mommy, go away--

00:17:54.830 --> 00:17:56.290
I'm in charge now.

00:17:56.290 --> 00:17:58.680
But it's not true of society.

00:17:58.680 --> 00:18:00.770
I think that's one
of the themes I

00:18:00.770 --> 00:18:05.930
got to in the book was about
the fantasy of the internet,

00:18:05.930 --> 00:18:10.160
that basically the internet
is imagining a world free

00:18:10.160 --> 00:18:13.140
from a lot of the pull
of history and of racism,

00:18:13.140 --> 00:18:14.600
misogyny.

00:18:14.600 --> 00:18:18.460
I look at this famous document,
this John Perry Barlow

00:18:18.460 --> 00:18:20.210
"Declaration of
Cyber-space Independence,"

00:18:20.210 --> 00:18:22.400
which he wrote in the 1990's.

00:18:22.400 --> 00:18:26.890
He's a founder of EFF, this
very important digital rights

00:18:26.890 --> 00:18:27.390
movement.

00:18:27.390 --> 00:18:28.620
He was a lyricist for
the Grateful Dead.

00:18:28.620 --> 00:18:30.290
And he basically
just says, we're

00:18:30.290 --> 00:18:33.032
making a world were there is
no racism, there is no gender.

00:18:33.032 --> 00:18:33.740
It's a new world.

00:18:33.740 --> 00:18:35.780
Go away, governments,
we're free here.

00:18:35.780 --> 00:18:39.350
And it's like, that fantasy is
part of what the problem is,

00:18:39.350 --> 00:18:41.110
and it's part of the
attraction obviously,

00:18:41.110 --> 00:18:43.235
the internet-- who wouldn't
want to live in a world

00:18:43.235 --> 00:18:45.600
where we don't have to
deal with a legacy problem?

00:18:45.600 --> 00:18:49.880
SPEAKER: And so you say
that the internet should

00:18:49.880 --> 00:18:53.870
resemble local communities, and
it should reflect that more.

00:18:53.870 --> 00:18:56.850
Are there companies, are
there pockets of the internet

00:18:56.850 --> 00:18:59.100
that you think are doing
that well or that you admire?

00:18:59.100 --> 00:18:59.910
NOAM COHEN: Yeah,
I think it's funny.

00:18:59.910 --> 00:19:01.451
I think that,
obviously, Wikipedia is

00:19:01.451 --> 00:19:03.020
something I wrote a lot about.

00:19:03.020 --> 00:19:04.210
And I feel Wikipedia--

00:19:04.210 --> 00:19:06.710
part of what we talk about, the
problem with Silicon Valley.

00:19:06.710 --> 00:19:09.500
I think, is also
the scalability idea

00:19:09.500 --> 00:19:11.300
that everything has a scale.

00:19:11.300 --> 00:19:15.260
And once you scale, once
Facebook has two billion users

00:19:15.260 --> 00:19:17.300
and Google has more
than a billion,

00:19:17.300 --> 00:19:18.990
you can't have a human touch.

00:19:18.990 --> 00:19:21.400
So I think of Wikipedia--

00:19:21.400 --> 00:19:23.770
it has bots, and it does
automate a lot of things,

00:19:23.770 --> 00:19:27.270
but it fundamentally is still
aspiring to be human scale.

00:19:27.270 --> 00:19:30.115
And people definitely
dissect about the problems

00:19:30.115 --> 00:19:30.790
of Wikipedia.

00:19:30.790 --> 00:19:33.700
And definitely, this appearance
of no hierarchy, but there

00:19:33.700 --> 00:19:35.890
obviously are people
who are administrators,

00:19:35.890 --> 00:19:38.080
and their clicks and
all that kind of stuff.

00:19:38.080 --> 00:19:39.700
I mean to minimize that,
but fundamentally, those

00:19:39.700 --> 00:19:40.405
are human problems.

00:19:40.405 --> 00:19:41.946
So I look at something
like Wikipedia

00:19:41.946 --> 00:19:46.690
and admire that it still aspires
to be a human-scale project.

00:19:46.690 --> 00:19:50.702
I think that is a minimum
requirement for any kind

00:19:50.702 --> 00:19:51.535
of internet project.

00:19:51.535 --> 00:19:57.650
It has to have that human scale,
not the digital billion users

00:19:57.650 --> 00:19:58.150
scale.

00:19:58.150 --> 00:20:00.040
And we were kind of talking
about Tim Berners-Lee before,

00:20:00.040 --> 00:20:01.998
and in this sense, this
is kind of what I think

00:20:01.998 --> 00:20:03.340
Berners-Lee was talking about.

00:20:03.340 --> 00:20:06.650
Early on, he wanted this editing
tool with a browser, right?

00:20:06.650 --> 00:20:09.250
There was a big issue about
whether that first browser

00:20:09.250 --> 00:20:11.900
he made versus the one that
Mosaic, the famous one Marc

00:20:11.900 --> 00:20:14.020
Andreessen made that
became Netscape and became

00:20:14.020 --> 00:20:15.730
the progenitor of all this--

00:20:15.730 --> 00:20:17.320
would have an editing tool.

00:20:17.320 --> 00:20:20.260
For Berners-Lee, why was an
editing tool so important?

00:20:20.260 --> 00:20:24.550
It just was like a metaphor
or a way of making sure

00:20:24.550 --> 00:20:28.150
that people could feel that
the internet wasn't passive,

00:20:28.150 --> 00:20:31.010
the web wasn't passive,
that you were part of it.

00:20:31.010 --> 00:20:33.437
And I think that
is just so vital.

00:20:33.437 --> 00:20:34.770
I don't know how to describe it.

00:20:34.770 --> 00:20:37.210
Obviously, you could say
that Twitter, and Facebook,

00:20:37.210 --> 00:20:40.250
and Google are all about
giving people a voice.

00:20:40.250 --> 00:20:41.810
So I totally get that.

00:20:41.810 --> 00:20:44.230
But it's more the sense
of ownership of it

00:20:44.230 --> 00:20:47.710
and creation that is
done in your sphere,

00:20:47.710 --> 00:20:49.600
not in a company sphere.

00:20:49.600 --> 00:20:53.080
So I just think that's what
I about with localness.

00:20:53.080 --> 00:20:56.090
And I think that, it's
really interesting,

00:20:56.090 --> 00:21:00.114
I was on a program, a
radio show in New York,

00:21:00.114 --> 00:21:02.030
and a colleague that
used to be from the Times

00:21:02.030 --> 00:21:03.730
was writing about
local newspapers

00:21:03.730 --> 00:21:06.040
and was talking about how
local newspapers as we

00:21:06.040 --> 00:21:09.770
know are kind of dying, and
that part of it is advertising,

00:21:09.770 --> 00:21:12.130
even local advertising
through Facebook and Google

00:21:12.130 --> 00:21:14.500
can be so targeted--

00:21:14.500 --> 00:21:16.660
and it kind of, it's
like they found a way

00:21:16.660 --> 00:21:19.270
to reach you want to
reach Bayside Queens,

00:21:19.270 --> 00:21:21.790
they found a way to reach
Bayside Queens in a way that

00:21:21.790 --> 00:21:24.010
doesn't involve being
in Bayside Queens,

00:21:24.010 --> 00:21:26.470
or the way Bayside
Queens is organized.

00:21:26.470 --> 00:21:28.660
It's just sort of
through other triggers,

00:21:28.660 --> 00:21:30.400
and that's kind of a problem.

00:21:30.400 --> 00:21:32.710
It's solving a lot of
problems-- getting local ads,

00:21:32.710 --> 00:21:34.690
and reaching localities,
by it's not doing it

00:21:34.690 --> 00:21:38.665
through a structure that's
useful, if that makes sense.

00:21:38.665 --> 00:21:40.540
And that's kind of what
I'm getting at-- it's

00:21:40.540 --> 00:21:43.300
not just the mere localness
or the mere individuality,

00:21:43.300 --> 00:21:47.200
but part of a
structure, a society,

00:21:47.200 --> 00:21:48.832
where we're all
aware of each other.

00:21:48.832 --> 00:21:50.290
SPEAKER: Do you
think people become

00:21:50.290 --> 00:21:54.510
less civic-minded when you
take that community out of it?

00:21:54.510 --> 00:21:56.909
NOAM COHEN: Maybe
you've seen on Twitter,

00:21:56.909 --> 00:21:58.450
people have been
putting this graphic

00:21:58.450 --> 00:22:00.520
out a lot of the rise of
polarization of politics

00:22:00.520 --> 00:22:05.210
and mapping that with the
rise of social networks.

00:22:05.210 --> 00:22:06.360
Yeah, it's a sense--

00:22:06.360 --> 00:22:08.890
I was talking with a
friend about this that when

00:22:08.890 --> 00:22:11.740
advertisements are so
targeted, it can be targeted,

00:22:11.740 --> 00:22:15.052
and not everyone sees them--
and I guess what people call

00:22:15.052 --> 00:22:15.760
a filter bubble--

00:22:15.760 --> 00:22:18.790
I don't act like I'm coming
up with new insights here.

00:22:18.790 --> 00:22:21.550
These are kind of problems
that people saw all along.

00:22:21.550 --> 00:22:24.941
I guess what I wanted
to do in this book,

00:22:24.941 --> 00:22:26.440
rather than just
criticize, was sort

00:22:26.440 --> 00:22:29.380
of explain in a
very understanding

00:22:29.380 --> 00:22:31.750
way how we got here.

00:22:31.750 --> 00:22:35.680
How did well intentions
end up in this wrong path?

00:22:35.680 --> 00:22:39.600
And so to me, a
different interviewer

00:22:39.600 --> 00:22:41.302
was asking me about
these mottoes--

00:22:41.302 --> 00:22:43.885
don't be evil, whatever you want
to call it, whatever that was

00:22:43.885 --> 00:22:47.260
at Google, or, move fast and
break things-- and it's like,

00:22:47.260 --> 00:22:50.560
I don't think these are cynical
tricks that these companies are

00:22:50.560 --> 00:22:51.060
playing.

00:22:51.060 --> 00:22:53.790
I mean, they show
where it started from.

00:22:53.790 --> 00:22:56.400
It's almost like it's
such a powerful tool,

00:22:56.400 --> 00:22:59.440
these social networks are so
massive and so strong that

00:22:59.440 --> 00:23:00.340
if we don't--

00:23:00.340 --> 00:23:03.400
it's almost like you
need outside advice,

00:23:03.400 --> 00:23:06.130
and we as a society
have to grapple with it.

00:23:06.130 --> 00:23:07.925
Something I've talked
about a bunch was--

00:23:07.925 --> 00:23:09.550
because, for some of
these appearances,

00:23:09.550 --> 00:23:12.840
was just looking at the hearings
about the Russian influence

00:23:12.840 --> 00:23:13.529
on the election.

00:23:13.529 --> 00:23:14.070
I was struck.

00:23:14.070 --> 00:23:18.180
There is a scene where Al
Franken, Senator Al Franken,

00:23:18.180 --> 00:23:20.010
was asking the General
Council at Facebook

00:23:20.010 --> 00:23:22.740
about whether he
could just pledge

00:23:22.740 --> 00:23:26.790
not to take political ads
that were paid for in rubles.

00:23:26.790 --> 00:23:27.420
Right?

00:23:27.420 --> 00:23:29.430
It seems almost like a joke.

00:23:29.430 --> 00:23:31.450
And the council said,
that's a very good signal,

00:23:31.450 --> 00:23:32.870
we are really mindful of that.

00:23:32.870 --> 00:23:34.120
Thank you for pointing it out.

00:23:34.120 --> 00:23:37.320
He was like, can you just
say that you will not accept

00:23:37.320 --> 00:23:38.850
political ads paid in rubles?

00:23:38.850 --> 00:23:40.552
And he would not commit to it.

00:23:40.552 --> 00:23:42.010
And Al Franken got
very frustrated,

00:23:42.010 --> 00:23:43.395
and kind of yelled at him, even.

00:23:43.395 --> 00:23:45.520
And I was thinking, why
would he not agree to that?

00:23:45.520 --> 00:23:48.760
Such a straightforward pledge.

00:23:48.760 --> 00:23:51.420
But I think fundamentally
it spoke to Facebook saying,

00:23:51.420 --> 00:23:53.940
you, Al Franken, are
not telling us how we're

00:23:53.940 --> 00:23:55.324
going to do our algorithm.

00:23:55.324 --> 00:23:56.490
That's for us to figure out.

00:23:56.490 --> 00:23:57.740
It's a good point you make.

00:23:57.740 --> 00:23:58.650
It's an important signal.

00:23:58.650 --> 00:24:01.150
We're going to factor it in and
we'll get back to you on it.

00:24:01.150 --> 00:24:02.940
And I think that
arrogance is really

00:24:02.940 --> 00:24:04.564
why the book's called
the Know-It-Alls.

00:24:04.564 --> 00:24:06.300
It has to end.

00:24:06.300 --> 00:24:09.330
We as a society have to
answer these questions.

00:24:09.330 --> 00:24:13.450
It can't be that a handful of
leaders of these companies,

00:24:13.450 --> 00:24:15.690
with all good
intentions, are going

00:24:15.690 --> 00:24:18.150
to come up with the answer.

00:24:18.150 --> 00:24:21.110
I've always had this motto of--

00:24:21.110 --> 00:24:22.860
maybe it's a little
too self-deprecating--

00:24:22.860 --> 00:24:24.526
but the wise man knows
he knows nothing.

00:24:24.526 --> 00:24:28.460
I mean, the truly wise
person, the true know-it-all

00:24:28.460 --> 00:24:29.920
would know that they need help.

00:24:29.920 --> 00:24:34.020
And I hope that's kind
of where we're going.

00:24:34.020 --> 00:24:35.664
SPEAKER: That's
the Socrates model.

00:24:35.664 --> 00:24:36.330
NOAM COHEN: Yes.

00:24:36.330 --> 00:24:38.400
Yes, exactly.

00:24:38.400 --> 00:24:40.270
SPEAKER: So changing
gears a little bit.

00:24:40.270 --> 00:24:43.050
So throughout the book, you
seem to have this resistance

00:24:43.050 --> 00:24:45.110
to quantification.

00:24:45.110 --> 00:24:48.150
So whether it be measuring
intelligence with IQ

00:24:48.150 --> 00:24:50.610
or measuring the
impact of education

00:24:50.610 --> 00:24:52.890
with different
number metrics, it

00:24:52.890 --> 00:24:55.140
seems to you not only
to be reductionist

00:24:55.140 --> 00:24:57.780
but also to be even
harmful in some cases.

00:24:57.780 --> 00:25:03.180
And it seems in
today's world, it's

00:25:03.180 --> 00:25:06.640
a common desire to
want to be data-driven

00:25:06.640 --> 00:25:09.630
and to optimize around data.

00:25:09.630 --> 00:25:11.880
It seems like you're
sort of a contrarian

00:25:11.880 --> 00:25:13.120
and pushing back on that.

00:25:13.120 --> 00:25:14.470
So could you talk a
little bit about that?

00:25:14.470 --> 00:25:15.178
NOAM COHEN: Yeah.

00:25:15.178 --> 00:25:17.110
And that's such
a great question.

00:25:17.110 --> 00:25:18.570
This relates to journalism too.

00:25:18.570 --> 00:25:21.300
You'll see these debates
about data-driven journalism

00:25:21.300 --> 00:25:25.100
and I guess I'm always going
to think of a better story

00:25:25.100 --> 00:25:27.460
as being more
effective than data.

00:25:27.460 --> 00:25:33.090
And I guess I am suspicious
of this kind of reducing

00:25:33.090 --> 00:25:36.810
such ineffable qualities like
intelligence to a number.

00:25:36.810 --> 00:25:39.780
I do bring it back to that
history in the book of AI,

00:25:39.780 --> 00:25:43.770
because I think fundamentally,
if you think a computer can

00:25:43.770 --> 00:25:46.650
be a thinking being, it's
like, you're basically saying,

00:25:46.650 --> 00:25:49.020
to be a thinking being
is just to have a head,

00:25:49.020 --> 00:25:51.650
to have circuits in your brain.

00:25:51.650 --> 00:25:54.120
And I think we
recognize that it's

00:25:54.120 --> 00:25:56.880
a very odd image, to think
of just a brain being

00:25:56.880 --> 00:25:58.140
all that is important.

00:25:58.140 --> 00:25:59.850
I think about even
my own personal life,

00:25:59.850 --> 00:26:01.890
it's like something my
wife and I talk about,

00:26:01.890 --> 00:26:03.600
the mind-body
divide and the idea

00:26:03.600 --> 00:26:07.960
that there is more to
life than just the mind.

00:26:07.960 --> 00:26:11.787
And I bring it up because
it's seeing intelligence only

00:26:11.787 --> 00:26:13.620
through what your brain
does, and not seeing

00:26:13.620 --> 00:26:16.750
how you function in the
world is a real mistake.

00:26:16.750 --> 00:26:20.570
And I think of it like,
it's ruthlessly efficient.

00:26:20.570 --> 00:26:24.580
We were talking about how it
reminds me of, kind of, even

00:26:24.580 --> 00:26:28.150
people who take over companies
and see their value if you

00:26:28.150 --> 00:26:30.060
break it up and
sell this part off

00:26:30.060 --> 00:26:32.400
and don't see the
whole of a company

00:26:32.400 --> 00:26:34.335
or the whole of society.

00:26:34.335 --> 00:26:37.410
You could argue that arguably
you look at what Facebook does,

00:26:37.410 --> 00:26:40.680
it really basically
breaks our population up

00:26:40.680 --> 00:26:41.880
into 14 categories.

00:26:41.880 --> 00:26:47.290
And it's just pernicious
because it's not seeing a whole.

00:26:47.290 --> 00:26:49.740
And I guess that is
data-driven; they're

00:26:49.740 --> 00:26:51.660
breaking up 14 categories.

00:26:51.660 --> 00:26:55.160
They're using data and analytics
to come up with those numbers.

00:26:55.160 --> 00:26:58.262
I just think it's a mistake.

00:26:58.262 --> 00:26:59.970
I guess it fundamentally
puts me, though,

00:26:59.970 --> 00:27:01.290
not on the side of efficiency.

00:27:01.290 --> 00:27:03.380
I think about,
obviously I had friends,

00:27:03.380 --> 00:27:05.910
especially in the paper, New
York Times-- who really believe

00:27:05.910 --> 00:27:07.470
in data-driven journalism.

00:27:07.470 --> 00:27:10.770
I remember having a very good
friend, a very smart friend,

00:27:10.770 --> 00:27:13.020
who explained to me that
the whole idea of being

00:27:13.020 --> 00:27:18.070
hot in basketball, like being
on a tear, is just not true.

00:27:18.070 --> 00:27:19.710
And I was like, it
can't not be true.

00:27:19.710 --> 00:27:21.012
I've seen people.

00:27:21.012 --> 00:27:23.470
And then he'll break down stats
and show you that it isn't.

00:27:23.470 --> 00:27:25.855
And so I don't want
to sit there and cling

00:27:25.855 --> 00:27:28.630
to things that are just
fundamentally not true.

00:27:28.630 --> 00:27:30.600
But that said, I do think
there are these sort

00:27:30.600 --> 00:27:32.790
of inexpressible qualities.

00:27:32.790 --> 00:27:38.566
I don't know that efficiency
is the most important thing.

00:27:38.566 --> 00:27:40.440
I think there are other
things that get lost.

00:27:40.440 --> 00:27:41.814
I'll make one last
point on that,

00:27:41.814 --> 00:27:45.000
and maybe bring it back to
sort of my haranguing tone,

00:27:45.000 --> 00:27:48.680
but it's like--

00:27:48.680 --> 00:27:50.839
if you focus on
efficiency, what am I

00:27:50.839 --> 00:27:52.880
saying about we break up
companies or something--

00:27:52.880 --> 00:27:57.710
you're extracting the most
value out of this thing

00:27:57.710 --> 00:27:59.780
you have by breaking
it up, but you are not

00:27:59.780 --> 00:28:00.780
looking at the costs.

00:28:00.780 --> 00:28:02.180
It's very easy to
sort of dismiss

00:28:02.180 --> 00:28:05.320
the costs of doing that.

00:28:05.320 --> 00:28:08.330
And I think that if we just
put a number on intelligence,

00:28:08.330 --> 00:28:11.007
you can find yourself lacking.

00:28:11.007 --> 00:28:12.590
What if your goal
was to hire the best

00:28:12.590 --> 00:28:15.080
people for your company
to solve a problem?

00:28:15.080 --> 00:28:17.210
That won't necessarily
lead to the best results

00:28:17.210 --> 00:28:18.740
because I don't
think you fully--

00:28:18.740 --> 00:28:21.750
again, the wise man
knows he knows nothing.

00:28:21.750 --> 00:28:24.740
You aren't going be able
to create a number of way

00:28:24.740 --> 00:28:27.950
to judge intelligence the way
that is useful whatever project

00:28:27.950 --> 00:28:29.574
you have in mind, I would say.

00:28:29.574 --> 00:28:30.740
So that's why I'm skeptical.

00:28:30.740 --> 00:28:32.550
I do know the power of this.

00:28:32.550 --> 00:28:34.300
And I think part of
what writing this book

00:28:34.300 --> 00:28:36.674
is sort of dealing with the
fact that these companies are

00:28:36.674 --> 00:28:37.334
so successful.

00:28:37.334 --> 00:28:39.500
I'd say the biggest push
back I get from people when

00:28:39.500 --> 00:28:42.980
I talk about it is
this idea that people

00:28:42.980 --> 00:28:44.970
love these companies.

00:28:44.970 --> 00:28:46.622
So what are you talking about?

00:28:46.622 --> 00:28:48.580
You're basically telling
people they don't know

00:28:48.580 --> 00:28:51.269
what's in their own interests.

00:28:51.269 --> 00:28:53.810
SPEAKER: So at the end of the
book, where do we go from here?

00:28:53.810 --> 00:28:59.030
You say that to rely on
the companies themselves

00:28:59.030 --> 00:29:03.140
to change the course
would be blasphemous

00:29:03.140 --> 00:29:04.400
or something like that.

00:29:04.400 --> 00:29:05.610
So why is that the case?

00:29:05.610 --> 00:29:07.068
NOAM COHEN: It's
kind of what we're

00:29:07.068 --> 00:29:09.520
talking about where it's
the humility that we need,

00:29:09.520 --> 00:29:13.100
that we need us as a society to
come together and kind of come

00:29:13.100 --> 00:29:16.760
up with some rules of how
we're going to live together

00:29:16.760 --> 00:29:18.281
with these companies.

00:29:18.281 --> 00:29:20.030
Things are moving so
fast that it's almost

00:29:20.030 --> 00:29:23.300
like we need to slow down
and talk about it together.

00:29:23.300 --> 00:29:24.980
I was struck that,
again, looking

00:29:24.980 --> 00:29:29.180
at that hearing, that there
is a kind of consensus

00:29:29.180 --> 00:29:30.780
on the Republicans
and Democrats.

00:29:30.780 --> 00:29:32.882
The sharpest questioner
I thought at that hearing

00:29:32.882 --> 00:29:34.715
was the Senator John
Kennedy from Louisiana,

00:29:34.715 --> 00:29:36.950
that was asking a
lot of hard questions

00:29:36.950 --> 00:29:40.610
and very suspicious about
all the data collection.

00:29:40.610 --> 00:29:46.400
And so I think that yeah, I
do think we, going forward,

00:29:46.400 --> 00:29:49.280
we need to empower people.

00:29:49.280 --> 00:29:53.240
We need to maybe lessen
the power of the companies.

00:29:53.240 --> 00:29:56.310
I talk a lot about regulation
and the European style

00:29:56.310 --> 00:29:58.580
of regulation, European
buyers would maybe

00:29:58.580 --> 00:30:02.300
say that the right
to be forgotten

00:30:02.300 --> 00:30:04.850
is a violation of free speech
or however you want to view it,

00:30:04.850 --> 00:30:06.680
but that it's a very
important value.

00:30:06.680 --> 00:30:09.357
So I lean a lot on
regulation stuff,

00:30:09.357 --> 00:30:10.940
and I can say that
people resist that.

00:30:10.940 --> 00:30:13.610
And talking to a friend, I
think another way to think of it

00:30:13.610 --> 00:30:16.550
is also, why not
empower people to get

00:30:16.550 --> 00:30:18.296
more control of their data?

00:30:18.296 --> 00:30:19.670
I saw the Times
today had a piece

00:30:19.670 --> 00:30:22.230
about there should be
taxation on data collection

00:30:22.230 --> 00:30:26.600
and I would more say, I was
looking back when I was reading

00:30:26.600 --> 00:30:30.530
about McCarthy and
Weizenbaum, that in the 70s,

00:30:30.530 --> 00:30:32.370
Nixon set up a
commission about privacy.

00:30:32.370 --> 00:30:35.150
I guess people were really
concerned about medical records

00:30:35.150 --> 00:30:36.770
and they're asking,
what should we

00:30:36.770 --> 00:30:38.728
do with this new thing,
computerized collection

00:30:38.728 --> 00:30:39.620
of data?

00:30:39.620 --> 00:30:44.600
And they suggested that
corporations and the government

00:30:44.600 --> 00:30:47.260
should have to report
what they have on you.

00:30:47.260 --> 00:30:50.120
And it ended up that we only
said it about the government,

00:30:50.120 --> 00:30:51.277
not for corporations.

00:30:51.277 --> 00:30:52.110
This is before that.

00:30:52.110 --> 00:30:53.730
Obviously, I'm not blaming
this on Google or Facebook

00:30:53.730 --> 00:30:54.000
or any of those.

00:30:54.000 --> 00:30:55.640
This was way before there
where big companies.

00:30:55.640 --> 00:30:58.098
But I guess there was IBM, they
were people that had data--

00:30:58.098 --> 00:30:59.990
so I think that might
be a step forward.

00:30:59.990 --> 00:31:02.404
Rather than viewing
as the government--

00:31:02.404 --> 00:31:04.820
I've been talking to people
about the power of narratives.

00:31:04.820 --> 00:31:06.480
Maybe that's a better narrative.

00:31:06.480 --> 00:31:07.610
Instead of the narrative
being government

00:31:07.610 --> 00:31:09.140
should come down and
stop these companies

00:31:09.140 --> 00:31:10.850
from doing what
they're doing great--

00:31:10.850 --> 00:31:13.070
no, maybe we should
empower people

00:31:13.070 --> 00:31:16.400
in the libertarian way
almost, sort of say,

00:31:16.400 --> 00:31:17.520
you control your data.

00:31:17.520 --> 00:31:18.700
And then give you more.

00:31:18.700 --> 00:31:20.810
And might be a way of
sort of slowing the brakes

00:31:20.810 --> 00:31:23.630
on a lot of stuff and
kind of getting people

00:31:23.630 --> 00:31:26.790
to not participate in the
system that is causing damage.

00:31:26.790 --> 00:31:28.760
So thinking of new
ideas, but I do

00:31:28.760 --> 00:31:30.480
feel like that's a
very tough question.

00:31:30.480 --> 00:31:31.820
What am I saying?

00:31:31.820 --> 00:31:33.860
And I didn't want this
book to be only a screed.

00:31:33.860 --> 00:31:36.020
I really wanted it
to be a look at how

00:31:36.020 --> 00:31:38.240
we got here and a
look at, I guess,

00:31:38.240 --> 00:31:41.540
in some ways these
unappreciated people and events.

00:31:41.540 --> 00:31:47.660
Computer science and Stanford
and the pull of libertarian

00:31:47.660 --> 00:31:50.660
ideas and why they make sense
to a certain group of people,

00:31:50.660 --> 00:31:53.470
and why that is so
attractive but maybe

00:31:53.470 --> 00:31:55.600
it would make sense
in a computer lab

00:31:55.600 --> 00:31:58.520
but not make sense
in our whole country.

00:31:58.520 --> 00:31:59.910
So same thing.

00:31:59.910 --> 00:32:02.900
So I think that is
where I would go.

00:32:02.900 --> 00:32:05.160
I would go breaking,
making things smaller

00:32:05.160 --> 00:32:07.229
and giving people more
power over their data.

00:32:07.229 --> 00:32:09.770
But it's so for us all to talk
about, I guess, is where I am.

00:32:09.770 --> 00:32:11.652
I'm hoping it will
lead to a dialogue.

00:32:11.652 --> 00:32:13.610
SPEAKER: I think that's
a great note to end on.

00:32:13.610 --> 00:32:16.020
I think we can take
some audience questions.

00:32:20.191 --> 00:32:20.690
Jessie.

00:32:28.140 --> 00:32:32.145
AUDIENCE MEMBER 1: You talked
a lot about the data issue,

00:32:32.145 --> 00:32:33.020
owning your own data.

00:32:33.020 --> 00:32:35.460
That seems like it's
kind of at the forefront

00:32:35.460 --> 00:32:40.390
of the public
conversation right now.

00:32:40.390 --> 00:32:42.910
If you had to make
a prediction, do you

00:32:42.910 --> 00:32:45.160
think any of this
stuff that's kind

00:32:45.160 --> 00:32:50.170
of being proposed
legislatively to curb maybe

00:32:50.170 --> 00:32:53.860
the monopolies that
Google and Facebook have,

00:32:53.860 --> 00:32:56.131
will actually take into effect?

00:32:56.131 --> 00:32:58.130
NOAM COHEN: Yeah, it I
had to make a prediction,

00:32:58.130 --> 00:33:00.520
I would be kind of
leery about it actually.

00:33:00.520 --> 00:33:03.650
I think that much
change will happen.

00:33:03.650 --> 00:33:06.110
I think part of
what people believe

00:33:06.110 --> 00:33:09.360
is this, at least there
are companies, right?

00:33:09.360 --> 00:33:12.900
I was with some
person from a panel

00:33:12.900 --> 00:33:15.480
that-- you know, imagine if
it was a Chinese company that

00:33:15.480 --> 00:33:18.720
was the Facebook of America,
then how would we feel?

00:33:18.720 --> 00:33:21.840
So I think in that
sense, especially

00:33:21.840 --> 00:33:24.540
under this current
president with this very

00:33:24.540 --> 00:33:29.582
populist nationalist view,
there's probably a real comfort

00:33:29.582 --> 00:33:31.290
at least in having
these big companies be

00:33:31.290 --> 00:33:32.130
American companies.

00:33:32.130 --> 00:33:34.385
So I wonder what's
going to happen.

00:33:34.385 --> 00:33:35.760
I think it's a
really fascinating

00:33:35.760 --> 00:33:41.140
and I do think that the election
meddling has been a watershed.

00:33:41.140 --> 00:33:44.010
It sort of shifted the public
consciousness, because prior

00:33:44.010 --> 00:33:45.570
to something as
significant as that,

00:33:45.570 --> 00:33:47.640
this idea that maybe our
democracy was tampered with--

00:33:47.640 --> 00:33:49.050
and obviously, it was
tampered with in many ways.

00:33:49.050 --> 00:33:50.680
But this was one outlet.

00:33:50.680 --> 00:33:54.000
And maybe it was done kind
of without the people running

00:33:54.000 --> 00:33:56.114
these companies noticing--

00:33:56.114 --> 00:33:57.780
that kind of woke up
people and gives it

00:33:57.780 --> 00:33:58.800
a much greater gravity.

00:33:59.380 --> 00:34:02.490
These other kind of concerns
become much more trivial

00:34:02.490 --> 00:34:07.100
about what these social
networks are doing.

00:34:07.100 --> 00:34:09.080
Now I think it has
woken everyone up.

00:34:09.080 --> 00:34:11.550
I genuinely don't know how
it's all going to shake out.

00:34:11.550 --> 00:34:13.880
I think there are polls
in all different ways.

00:34:13.880 --> 00:34:15.860
I mean, it's going to
be amazing to watch.

00:34:15.860 --> 00:34:17.520
And I do think, who
even knows what's

00:34:17.520 --> 00:34:18.811
going to happen in our country?

00:34:18.811 --> 00:34:22.579
If there's a wave, election,
maybe Democrats sort of bounce

00:34:22.579 --> 00:34:23.870
back, that could change things.

00:34:23.870 --> 00:34:25.250
Especially a populist
Democrat might

00:34:25.250 --> 00:34:26.989
have a very different
view in all of this

00:34:26.989 --> 00:34:29.744
than a populist Republican.

00:34:29.744 --> 00:34:31.452
It's a great question,
something like I'm

00:34:31.452 --> 00:34:33.327
going to be watching
very carefully for sure.

00:34:34.424 --> 00:34:36.090
SPEAKER: One more
question it that's OK.

00:34:39.659 --> 00:34:42.870
I guess you could read
your book and then become

00:34:42.870 --> 00:34:47.460
kind of just a hard-line
kind of Luddite.

00:34:47.460 --> 00:34:55.080
What's the conclusion and
that's actionable after reading

00:34:55.080 --> 00:34:57.720
your book in terms of
solving some of the problems

00:34:57.720 --> 00:35:00.540
you outline that
might involve going

00:35:00.540 --> 00:35:02.707
with the grain of history,
or kind of working with--

00:35:02.707 --> 00:35:05.290
NOAM COHEN: I definitely do not
want to come off as a Luddite.

00:35:05.290 --> 00:35:08.050
And I definitely believe
that these tools are amazing.

00:35:08.050 --> 00:35:10.314
So I do think it
becomes sort of--

00:35:10.314 --> 00:35:12.730
I guess why McCarthy was such
an interesting figure to me,

00:35:12.730 --> 00:35:14.040
because that, and
this is something

00:35:14.040 --> 00:35:15.415
that maybe David
and I were going

00:35:15.415 --> 00:35:18.180
to talk about-- is this
idea that this research was

00:35:18.180 --> 00:35:19.740
done under government auspices.

00:35:19.740 --> 00:35:23.151
I mean, there was a really
interesting story of McCarthy.

00:35:23.151 --> 00:35:24.900
I didn't get in the
book, and I can't even

00:35:24.900 --> 00:35:26.340
believe this is even true--

00:35:26.340 --> 00:35:32.760
McCarthy in the 70s had this
idea that, back in that day,

00:35:32.760 --> 00:35:36.030
India had a hostile
relation with the US--

00:35:36.030 --> 00:35:37.680
so there was all
this money that they

00:35:37.680 --> 00:35:40.511
owed that US that they only
would give back in rupees,

00:35:40.511 --> 00:35:41.010
right?

00:35:41.010 --> 00:35:43.490
And he had this idea
that you should digitized

00:35:43.490 --> 00:35:47.060
the Library of Congress
by having people in India

00:35:47.060 --> 00:35:48.840
enter it in.

00:35:48.840 --> 00:35:50.717
And he was thinking
of this in the 70s.

00:35:50.717 --> 00:35:52.300
But he was thinking
about it not like,

00:35:52.300 --> 00:35:54.550
I should do it so I can have
a company could own this,

00:35:54.550 --> 00:35:56.591
but he was thinking that
like we, the government,

00:35:56.591 --> 00:35:57.360
should do it.

00:35:57.360 --> 00:36:00.090
I'm very much in favor
of research innovation

00:36:00.090 --> 00:36:02.110
and I don't want to be
perceived as a Luddite,

00:36:02.110 --> 00:36:03.840
but I think maybe
having more of this

00:36:03.840 --> 00:36:06.660
being done under
government auspices,

00:36:06.660 --> 00:36:12.280
and being thought of as
helping all of society and kind

00:36:12.280 --> 00:36:15.130
of respecting people's
privacy and data

00:36:15.130 --> 00:36:16.640
would be a great step forward.

00:36:16.640 --> 00:36:19.134
So I definitely was very
worried that this book

00:36:19.134 --> 00:36:20.300
would be perceived that way.

00:36:20.300 --> 00:36:21.479
And I really struggled--

00:36:21.479 --> 00:36:24.145
I guess that's why I also wanted
to talk so much about Wikipedia

00:36:24.145 --> 00:36:28.540
because I see that such an
example of that, of technology

00:36:28.540 --> 00:36:31.480
being used to really help
people understand the world

00:36:31.480 --> 00:36:33.235
and without many
of the downsides

00:36:33.235 --> 00:36:35.075
that I'm talking about.

00:36:35.075 --> 00:36:36.700
SPEAKER: What about
like, for instance,

00:36:36.700 --> 00:36:39.670
at Google we have it's
called 20% time, where you

00:36:39.670 --> 00:36:41.320
can pursue your own projects.

00:36:41.320 --> 00:36:44.710
So do you think there's an
opportunity there for people

00:36:44.710 --> 00:36:48.592
to maybe pursue maybe
more local projects?

00:36:48.592 --> 00:36:49.300
NOAM COHEN: I do.

00:36:49.300 --> 00:36:50.508
I think you were saying that.

00:36:50.508 --> 00:36:53.020
I think it'd be really
interesting to see

00:36:53.020 --> 00:36:56.650
how you could create
projects that would force

00:36:56.650 --> 00:36:58.150
these local community bonds.

00:36:58.150 --> 00:36:59.980
I did write an
article for the Times

00:36:59.980 --> 00:37:02.620
about Red Hook in
Brooklyn, which

00:37:02.620 --> 00:37:04.910
is a very isolated
neighborhood in Brooklyn.

00:37:04.910 --> 00:37:07.870
And it's a poor
neighborhood, it is

00:37:07.870 --> 00:37:09.610
kind of becoming
gentrified-- and they

00:37:09.610 --> 00:37:12.310
were trying to create a
mesh network out there.

00:37:12.310 --> 00:37:17.650
And the way that they can use
that local internet to inform

00:37:17.650 --> 00:37:20.050
people about their
community, and so I do

00:37:20.050 --> 00:37:22.600
think-- and I don't
work to Google,

00:37:22.600 --> 00:37:24.809
that would be amazing to
see what the minds here

00:37:24.809 --> 00:37:25.600
would come up with.

00:37:28.640 --> 00:37:31.310
Because what I was
saying was the community,

00:37:31.310 --> 00:37:34.060
it's not just being able to
target a community through data

00:37:34.060 --> 00:37:36.400
points, but like building
the connections of community.

00:37:36.400 --> 00:37:38.110
I know that Mark Zuckerberg
talks about this a lot

00:37:38.110 --> 00:37:38.860
with Facebook too.

00:37:38.860 --> 00:37:42.940
But I mean, you'd have to do
this in a real genuine offline

00:37:42.940 --> 00:37:45.460
way.

00:37:45.460 --> 00:37:48.610
And I've seen that
being tried in Red Hook,

00:37:48.610 --> 00:37:51.540
where they are trying to use
the tech, with young people

00:37:51.540 --> 00:37:53.825
to get them to use
the Wi-Fi and maybe

00:37:53.825 --> 00:37:55.450
when they access the
Wi-Fi they kind of

00:37:55.450 --> 00:37:57.370
see things about
their neighborhood.

00:37:57.370 --> 00:37:59.209
And events are sort
of, offline events

00:37:59.209 --> 00:38:01.000
are sort of promoted
that way or something.

00:38:01.000 --> 00:38:03.190
So it would be amazing to
see what could be done,

00:38:03.190 --> 00:38:05.398
but I think those are the
kind of essential questions

00:38:05.398 --> 00:38:07.990
about keeping it local,
creating stronger bonds.

00:38:07.990 --> 00:38:11.590
So that would be amazing to see
what 20% time would lead to.

00:38:14.799 --> 00:38:15.590
SPEAKER: All right.

00:38:15.590 --> 00:38:16.510
Well, thank you.

00:38:16.510 --> 00:38:17.360
NOAM COHEN: No
further questions.

00:38:17.360 --> 00:38:17.960
That's great.

00:38:17.960 --> 00:38:19.370
Thank you, David.

00:38:19.370 --> 00:38:21.740
Thank you.

