WEBVTT
Kind: captions
Language: en

00:00:00.570 --> 00:00:02.445
ANWAR GHULOUM: Thanks
for coming to the talk.

00:00:02.445 --> 00:00:05.436
We're going to talk about ART,
L's runtime replacing Dalvik.

00:00:05.436 --> 00:00:07.560
Hopefully you'll leave,
with a better understanding

00:00:07.560 --> 00:00:12.030
of what ART is and what
improvements you can expect.

00:00:12.030 --> 00:00:15.760
We'll also talk a bit about
64-bit support in ART,

00:00:15.760 --> 00:00:18.860
and in the platform at the end.

00:00:18.860 --> 00:00:22.800
So I'll be presenting today
along with my colleagues, Brian

00:00:22.800 --> 00:00:25.400
Carlstrom and Ian Rogers from
the Android runtime and tools

00:00:25.400 --> 00:00:25.900
team.

00:00:25.900 --> 00:00:26.894
I'm Anwar Ghuloum.

00:00:26.894 --> 00:00:29.060
They'll talk about different
aspects of the runtime.

00:00:29.060 --> 00:00:31.345
They'll be coming
up pretty shortly.

00:00:31.345 --> 00:00:34.280
Thanks guys.

00:00:34.280 --> 00:00:36.460
First, why ART?

00:00:36.460 --> 00:00:39.740
So when Dalvik was
originally designed,

00:00:39.740 --> 00:00:41.980
the processors we were
targeting were single core,

00:00:41.980 --> 00:00:46.160
relatively poor performance,
relatively little flash

00:00:46.160 --> 00:00:48.140
and RAM available.

00:00:48.140 --> 00:00:50.000
But things have changed.

00:00:50.000 --> 00:00:52.510
We had a compact bicode
that was designed

00:00:52.510 --> 00:00:57.430
for fast interpretation,
relatively unsophisticated GC,

00:00:57.430 --> 00:01:00.260
and it wasn't designed
for multiprocessing.

00:01:00.260 --> 00:01:02.390
But since then, what
we've seen is we've

00:01:02.390 --> 00:01:04.260
got phone shipping
with eight cores.

00:01:04.260 --> 00:01:08.980
We've got devices that have
improved in performance

00:01:08.980 --> 00:01:12.380
over the G1 by 50x or more in
terms of raw CPU performance.

00:01:12.380 --> 00:01:16.950
We've got 4x more RAM,
sometimes more, 64x more flash,

00:01:16.950 --> 00:01:21.050
high resolution screens,
extremely powerful GPUs.

00:01:21.050 --> 00:01:23.560
We made a lot of changes in
Dalvik to improve things.

00:01:23.560 --> 00:01:26.970
We added a JIT, we added a
concurrent collection garbage

00:01:26.970 --> 00:01:30.770
collector, and we added
support for multiprocessing.

00:01:30.770 --> 00:01:34.860
But we felt frankly, that the
ecosystem was outpacing us,

00:01:34.860 --> 00:01:35.970
that we had to do more.

00:01:39.420 --> 00:01:42.390
So ART was born.

00:01:42.390 --> 00:01:45.170
So our motivation
for ART is that you

00:01:45.170 --> 00:01:48.530
shouldn't notice
the runtime at all.

00:01:48.530 --> 00:01:51.864
You're app should be buttery and
smooth, even though using GC.

00:01:51.864 --> 00:01:53.280
Code should be
fast without having

00:01:53.280 --> 00:01:55.645
to resort to JNI or native code.

00:01:58.607 --> 00:02:00.440
Start up time should
be fast, without having

00:02:00.440 --> 00:02:04.240
to warm up a JIT, and so on.

00:02:04.240 --> 00:02:05.900
The runtime in
general should scale

00:02:05.900 --> 00:02:08.919
to the parallelism
modern processors,

00:02:08.919 --> 00:02:11.470
as well as the complexity of
modern mobile applications.

00:02:11.470 --> 00:02:13.636
In other words, it should
be really solid foundation

00:02:13.636 --> 00:02:14.589
for the future.

00:02:14.589 --> 00:02:16.130
So in this talk,
we're going to start

00:02:16.130 --> 00:02:17.740
with telling you
about performance

00:02:17.740 --> 00:02:20.910
and compiling in ART; how
ART compiles ahead of time,

00:02:20.910 --> 00:02:23.909
and what kind of performance
improvements that brings you.

00:02:23.909 --> 00:02:25.700
We'll then talk about
the garbage collector

00:02:25.700 --> 00:02:29.090
and how GCed apps can
be buttery and smooth.

00:02:29.090 --> 00:02:31.850
And then we'll talk
a bit about 64-bit.

00:02:31.850 --> 00:02:35.268
So with that, I'll
hand it over to Brian.

00:02:35.268 --> 00:02:38.270
BRIAN CARLSTROM: Thanks, Anwar.

00:02:38.270 --> 00:02:40.624
So I'm going to start
by talking about a user.

00:02:40.624 --> 00:02:42.040
We have this public
bug that's one

00:02:42.040 --> 00:02:43.480
of our favorites
on the ART team.

00:02:43.480 --> 00:02:45.910
So a user wanting a
performance boosting thing.

00:02:45.910 --> 00:02:48.330
And if you read the
details in the bug,

00:02:48.330 --> 00:02:51.025
they want to take their existing
phone and their existing apps

00:02:51.025 --> 00:02:52.680
and they just want to get
a system software update.

00:02:52.680 --> 00:02:54.720
And they want faster
performance for their apps,

00:02:54.720 --> 00:02:55.510
want better battery life.

00:02:55.510 --> 00:02:57.810
They want to use less memories
so they can multicast more.

00:02:57.810 --> 00:02:59.685
And we really like this
bug because it really

00:02:59.685 --> 00:03:01.859
matches what Anwar said
about the ART manifesto,

00:03:01.859 --> 00:03:03.900
about trying to transparently
improve performance

00:03:03.900 --> 00:03:06.350
for apps without people
having to do anything.

00:03:06.350 --> 00:03:08.830
So what can we do about that?

00:03:08.830 --> 00:03:12.170
So as we've already said,
we started with Dalvik.

00:03:12.170 --> 00:03:14.070
And it had some performance
boosting things.

00:03:14.070 --> 00:03:15.790
It had a fast interpreter.

00:03:15.790 --> 00:03:18.410
JIT was added, another
performance boosting thing.

00:03:18.410 --> 00:03:20.322
And what can we do for ART?

00:03:20.322 --> 00:03:21.530
What can we do going forward?

00:03:21.530 --> 00:03:26.210
And we said well, Dalvik was
built for really targeting ARM.

00:03:26.210 --> 00:03:28.610
Now in more mobile
device, modern devices,

00:03:28.610 --> 00:03:30.791
we have a lot more ABI's
we have to support.

00:03:30.791 --> 00:03:32.790
We want to do potentially
different optimization

00:03:32.790 --> 00:03:34.770
levels for different use cases.

00:03:34.770 --> 00:03:37.430
So rather than just trying to
build one performance thing,

00:03:37.430 --> 00:03:40.230
we want to build a framework
of performance boosting things.

00:03:40.230 --> 00:03:43.190
And so we're going to
build a flexible compiler

00:03:43.190 --> 00:03:45.426
infrastructure that can
allow us to do that.

00:03:45.426 --> 00:03:46.800
So this just a
little box diagram

00:03:46.800 --> 00:03:48.660
of what we're doing
at the ART compilers.

00:03:48.660 --> 00:03:50.160
As you'd expect,
we have a front end

00:03:50.160 --> 00:03:52.450
that can read our dex
bicode for compilation.

00:03:52.450 --> 00:03:54.680
We have a compiler driver,
and what's interesting

00:03:54.680 --> 00:03:56.804
that it directs to various
different compilers that

00:03:56.804 --> 00:03:57.625
are available in L.

00:03:57.625 --> 00:03:59.750
We have three main compilers
that we're using in L,

00:03:59.750 --> 00:04:02.569
but there's already seeds in the
AOSP code for other compilers

00:04:02.569 --> 00:04:04.360
that we're working on
for the future, seeds

00:04:04.360 --> 00:04:07.172
of future performance
boosting things.

00:04:07.172 --> 00:04:10.589
What we have for L is a compiler
we call a quick compiler, which

00:04:10.589 --> 00:04:11.880
is based on the old Dalvik JIT.

00:04:11.880 --> 00:04:12.860
So it does many of
the same things,

00:04:12.860 --> 00:04:13.859
but does more than that.

00:04:13.859 --> 00:04:16.121
And we'll talk about
that in the next slide.

00:04:16.121 --> 00:04:18.029
It focuses on
compiling your dex code

00:04:18.029 --> 00:04:22.000
to native ARM code, x86
code, MIPS code, et cetera.

00:04:22.000 --> 00:04:24.170
We also have a JNI compiler
that helps build bridges

00:04:24.170 --> 00:04:26.260
from your dex code to
any native code you've

00:04:26.260 --> 00:04:27.529
done within the NDK.

00:04:27.529 --> 00:04:29.320
And finally, we have
a dex to dex optimizer

00:04:29.320 --> 00:04:31.403
which just quickens code
we decide not to compile,

00:04:31.403 --> 00:04:33.680
that we just want to do
fast interpretation on.

00:04:33.680 --> 00:04:36.540
In addition, we have to
have numerous back ends,

00:04:36.540 --> 00:04:40.670
and so we're supporting the
32 ABIs that Androis currently

00:04:40.670 --> 00:04:42.840
supports, as well as
the new 64-bit ABIs that

00:04:42.840 --> 00:04:45.860
are coming in L.

00:04:45.860 --> 00:04:48.350
So what are some of
the other benefits

00:04:48.350 --> 00:04:50.520
of doing ahead of
time compilation?

00:04:50.520 --> 00:04:52.830
One big difference between
ahead of time compilation

00:04:52.830 --> 00:04:55.170
and the JIT we had in
Dalvik is that we optimized

00:04:55.170 --> 00:04:58.150
the whole program potentially
and instead of just focusing

00:04:58.150 --> 00:05:01.020
on small traces of the program,
loops, or things like that.

00:05:01.020 --> 00:05:02.050
So the kind of
optimizations you did

00:05:02.050 --> 00:05:04.170
when you're targeting like small
parts the program, or loops,

00:05:04.170 --> 00:05:05.440
are very different
than when you're

00:05:05.440 --> 00:05:06.731
trying to do the whole program.

00:05:06.731 --> 00:05:10.080
And we try to do optimizations
that can generally apply

00:05:10.080 --> 00:05:12.030
to all kind of object
oriented programming

00:05:12.030 --> 00:05:13.670
that you might be
doing in Android.

00:05:13.670 --> 00:05:16.420
And so some examples are, we
try to make virtual method calls

00:05:16.420 --> 00:05:18.040
faster when possible,
we try to lower

00:05:18.040 --> 00:05:20.230
the overhead for
interface and vacations,

00:05:20.230 --> 00:05:21.942
we try to avoid some
implicit checks that

00:05:21.942 --> 00:05:23.400
need to be done
when you're calling

00:05:23.400 --> 00:05:25.110
from one class to another to
make sure it's initialized,

00:05:25.110 --> 00:05:27.360
and when you're calling
constructors or static fields.

00:05:27.360 --> 00:05:29.870
So exercising site fields, or
calling [INAUDIBLE] methods.

00:05:29.870 --> 00:05:30.950
And finally, there's
a lot of things

00:05:30.950 --> 00:05:32.490
are sort of implicit
checks that are done.

00:05:32.490 --> 00:05:34.073
Not just growing
languages, that there

00:05:34.073 --> 00:05:35.894
are no pointers,
and uncommon cases,

00:05:35.894 --> 00:05:37.560
and StackOverflow,
and things like that.

00:05:37.560 --> 00:05:39.260
So we try to make all those
kind of common case things

00:05:39.260 --> 00:05:41.260
faster so we can improve
the general performance

00:05:41.260 --> 00:05:42.820
of your program.

00:05:42.820 --> 00:05:44.530
But we've also seen
some other benefits

00:05:44.530 --> 00:05:45.946
from of ahead of
time compilation.

00:05:45.946 --> 00:05:47.540
Those things that
the user wanted.

00:05:47.540 --> 00:05:49.290
We do see better battery
life, and there's

00:05:49.290 --> 00:05:50.510
two reasons for that.

00:05:50.510 --> 00:05:52.530
One is because we're compiling,
and only in the installation

00:05:52.530 --> 00:05:54.070
time, we compile once
as opposed to a JIT

00:05:54.070 --> 00:05:56.361
based environment where every
time you run the program,

00:05:56.361 --> 00:05:58.110
it typically re-JITs,
rediscovers things,

00:05:58.110 --> 00:05:59.200
redoes work.

00:05:59.200 --> 00:06:01.420
It's kind of wasteful every
time you run your program

00:06:01.420 --> 00:06:03.340
through doing the
same compilations.

00:06:03.340 --> 00:06:07.090
Second thing we see is that
generally faster performing

00:06:07.090 --> 00:06:09.700
code saves battery because
you can more efficiently run

00:06:09.700 --> 00:06:14.500
and get back to a lower state
on the processor quicker.

00:06:14.500 --> 00:06:16.296
We also see a better
Svelte memory.

00:06:16.296 --> 00:06:17.920
Remember Project
Svelte from last year,

00:06:17.920 --> 00:06:20.780
trying to run better on
lower memory devices.

00:06:20.780 --> 00:06:22.730
The reason we see
better Svelte numbers

00:06:22.730 --> 00:06:25.960
are because the code
is written out to disc,

00:06:25.960 --> 00:06:28.819
and can be paged in and out
by the kernel on demand.

00:06:28.819 --> 00:06:31.110
If there's a lot of applications
running, multitasking,

00:06:31.110 --> 00:06:32.990
the kernel can kind of manage
paging things in and out,

00:06:32.990 --> 00:06:34.406
as opposed to the
JIT, where there

00:06:34.406 --> 00:06:37.114
would be a kind of set of
private pages per application

00:06:37.114 --> 00:06:38.280
which couldn't be paged out.

00:06:38.280 --> 00:06:40.980
So that meant that if the kernel
needed to free up more memory,

00:06:40.980 --> 00:06:42.110
it basically has
to kill a process,

00:06:42.110 --> 00:06:43.568
and you lose one
of your processors

00:06:43.568 --> 00:06:45.370
you maybe are trying
to multitask between.

00:06:45.370 --> 00:06:49.150
So how do we do that?

00:06:49.150 --> 00:06:52.030
This shows the life cycle of an
APK, the Android package that

00:06:52.030 --> 00:06:54.470
comes from developers
to the users.

00:06:54.470 --> 00:06:58.150
And so the typical steps in
the APK, a life in a life cycle

00:06:58.150 --> 00:06:59.470
are, you take your source code.

00:06:59.470 --> 00:07:01.100
You compile it, you
make a dex file,

00:07:01.100 --> 00:07:02.660
and can combine it
with native code

00:07:02.660 --> 00:07:04.570
from the NDK and resources,
and you basically put in a zip

00:07:04.570 --> 00:07:05.550
file we call the APK.

00:07:05.550 --> 00:07:08.040
And that gets distributed
through Google Play

00:07:08.040 --> 00:07:09.080
to the user.

00:07:09.080 --> 00:07:10.475
So on the user
side they take it,

00:07:10.475 --> 00:07:11.850
and they install
the application.

00:07:11.850 --> 00:07:13.700
And the native code
and the resources

00:07:13.700 --> 00:07:16.505
generally get packed unpacked
into the application directory.

00:07:16.505 --> 00:07:18.380
But the dex code requires
further processing.

00:07:18.380 --> 00:07:21.685
And this is true both for Dalvik
and for ART, as you see here.

00:07:21.685 --> 00:07:23.810
In Dalvik, they ran it
through a tool called dexopt

00:07:23.810 --> 00:07:25.835
during installation,
which is some already

00:07:25.835 --> 00:07:27.576
did some ahead of
time things that ahead

00:07:27.576 --> 00:07:28.701
of time class verification.

00:07:28.701 --> 00:07:31.270
Again, try to quicken things
up to improve performance

00:07:31.270 --> 00:07:33.350
for the interpreter.

00:07:33.350 --> 00:07:34.916
Dex to ODE is the
tool that ART uses.

00:07:34.916 --> 00:07:37.290
And again, this is sort of
transparent behind the scenes.

00:07:37.290 --> 00:07:39.120
But it compiles the
application in addition

00:07:39.120 --> 00:07:41.010
to doing some of that Dalvik
did in terms of verifying

00:07:41.010 --> 00:07:42.843
things ahead of time,
and quickening things,

00:07:42.843 --> 00:07:45.480
it also does that compilation
to native code for the target

00:07:45.480 --> 00:07:47.420
for the particular device.

00:07:47.420 --> 00:07:48.880
And the key takeaway
here is that

00:07:48.880 --> 00:07:51.380
from a developer's point of
view and a user's point of view,

00:07:51.380 --> 00:07:53.530
kind of the flow of how
the APK works is the same.

00:07:53.530 --> 00:07:56.116
And hopefully, you're just
going to get faster, a better

00:07:56.116 --> 00:07:56.615
experience.

00:07:58.955 --> 00:08:00.330
This is a little
bit of a picture

00:08:00.330 --> 00:08:02.579
of what it looks like when
you're running the dex tool

00:08:02.579 --> 00:08:04.780
inside if you were
inside of a tool.

00:08:04.780 --> 00:08:06.820
We actually start up
a minimal run time

00:08:06.820 --> 00:08:09.107
that loads the
framework code and some

00:08:09.107 --> 00:08:10.690
of the commonly used
framework objects

00:08:10.690 --> 00:08:11.750
so that when we
compile the code,

00:08:11.750 --> 00:08:14.010
we kind have an idea of what the
Android system you're running

00:08:14.010 --> 00:08:15.830
on looks like so we
can make optimization

00:08:15.830 --> 00:08:18.970
specific to that target device.

00:08:18.970 --> 00:08:21.967
So once we kind of initialize
this framework environment,

00:08:21.967 --> 00:08:23.550
we then go ahead and
load the compiler

00:08:23.550 --> 00:08:25.070
and load the coder compiler
of your application,

00:08:25.070 --> 00:08:26.405
and generate an ELF file.

00:08:32.319 --> 00:08:33.860
So what does this
ELF file look like?

00:08:33.860 --> 00:08:34.530
Sorry, yeah.

00:08:34.530 --> 00:08:36.109
This is the ELF,
kind of an overview

00:08:36.109 --> 00:08:37.400
of what these files looks like.

00:08:37.400 --> 00:08:39.510
ELF is a standard file
format for representing

00:08:39.510 --> 00:08:41.770
executable code.

00:08:41.770 --> 00:08:42.789
It is a standard file.

00:08:42.789 --> 00:08:44.350
We have kind of the
sections you want to expect.

00:08:44.350 --> 00:08:46.308
We have a symbol table,
we have read only data.

00:08:46.308 --> 00:08:49.382
We have some text that
contains the actual code.

00:08:49.382 --> 00:08:51.840
One difference between how we
use ELF files in more of a C,

00:08:51.840 --> 00:08:54.250
C++ kind of language would use
them is we don't use symbols

00:08:54.250 --> 00:08:55.624
to find every
single entry point.

00:08:55.624 --> 00:08:59.241
Because typically a Java
program has many, many methods.

00:08:59.241 --> 00:09:01.740
And having a symbol for each
one as unique would be too big.

00:09:01.740 --> 00:09:04.330
So instead, we use the
things that are already

00:09:04.330 --> 00:09:07.130
in the next dex bicode,
type indexes and method IDs

00:09:07.130 --> 00:09:08.010
to identify things.

00:09:08.010 --> 00:09:10.135
And we have this old data
structure that we instead

00:09:10.135 --> 00:09:12.190
look up using the symbols
in the symbol table.

00:09:12.190 --> 00:09:14.077
And once we have that
data structure loaded,

00:09:14.077 --> 00:09:16.160
we can quickly navigate
when we're loading a class

00:09:16.160 --> 00:09:18.150
to find all the methods
for a particular thing

00:09:18.150 --> 00:09:21.077
and link them up so we can
execute the code quickly.

00:09:21.077 --> 00:09:23.410
One other thing that we do
that's a little bit different

00:09:23.410 --> 00:09:26.890
is we actually keep the
original dex files around

00:09:26.890 --> 00:09:28.224
in the metadata in the ELF file.

00:09:28.224 --> 00:09:29.890
We use that first
sort of the class meta

00:09:29.890 --> 00:09:31.110
data for loading classes.

00:09:31.110 --> 00:09:32.680
You also use it
for other purposes,

00:09:32.680 --> 00:09:35.200
such as when you want to run
the debugger and single stuff,

00:09:35.200 --> 00:09:35.700
your code.

00:09:35.700 --> 00:09:37.130
We keep the original code
around so we can give you

00:09:37.130 --> 00:09:38.504
an accurate
debugging experience.

00:09:40.960 --> 00:09:44.190
One final thing to note here
is that in a compiled code

00:09:44.190 --> 00:09:45.850
for the class on the
box at the bottom,

00:09:45.850 --> 00:09:47.225
you can see it
actually does have

00:09:47.225 --> 00:09:50.080
direct links into framework
code as well framework objects,

00:09:50.080 --> 00:09:52.220
so we can efficiently
call things

00:09:52.220 --> 00:09:54.479
and not have to do runtime
lookups to find references

00:09:54.479 --> 00:09:56.770
to framework's code, which
generally speeds application

00:09:56.770 --> 00:09:57.270
execution.

00:10:02.561 --> 00:10:04.060
So this is just
kind of a time based

00:10:04.060 --> 00:10:05.750
view of what a
compilation looks like.

00:10:05.750 --> 00:10:08.270
On the lower left,
we have a systrace

00:10:08.270 --> 00:10:10.630
of a compilation of
a pretty meaty app.

00:10:10.630 --> 00:10:12.590
It's running on a
four core device.

00:10:12.590 --> 00:10:14.650
And I've tried to break out the
colored boxes on the lower left

00:10:14.650 --> 00:10:17.108
there to the right so you can
kind of get an idea of what's

00:10:17.108 --> 00:10:18.990
going on there, and
I'll talk through that.

00:10:18.990 --> 00:10:20.700
You can see most
of it actually does

00:10:20.700 --> 00:10:21.990
get good parallelism
during compilation.

00:10:21.990 --> 00:10:24.320
That's part of taking more
advantage of the modern devices

00:10:24.320 --> 00:10:26.361
whereas the dexopt and
and other tools for Dalvik

00:10:26.361 --> 00:10:28.247
were more single threaded.

00:10:28.247 --> 00:10:30.830
So we do have a serial face at
the beginning, where we extract

00:10:30.830 --> 00:10:32.909
out the dex files
for processing.

00:10:32.909 --> 00:10:35.200
But then we do a pass, most
of the passes are parallel.

00:10:35.200 --> 00:10:36.820
And the first pass
is class resolution,

00:10:36.820 --> 00:10:38.210
where basically load
all the classes,

00:10:38.210 --> 00:10:39.220
load their fields and methods.

00:10:39.220 --> 00:10:40.761
And this is where
the compiler learns

00:10:40.761 --> 00:10:43.370
what the layout of a class is,
what the label of the v table

00:10:43.370 --> 00:10:46.000
is for the methods in the
class so we can again, use

00:10:46.000 --> 00:10:48.100
that later on
during compilation.

00:10:48.100 --> 00:10:50.050
The next phase is
class verification.

00:10:50.050 --> 00:10:52.050
We walk through the
bike codes of the class.

00:10:52.050 --> 00:10:54.730
For most almost all
methods we encounter,

00:10:54.730 --> 00:10:56.500
we find that they
are verified, and we

00:10:56.500 --> 00:10:57.833
can mark that class as verified.

00:10:57.833 --> 00:11:03.160
And again, skip doing
that during every runtime.

00:11:03.160 --> 00:11:05.480
One other thing we
do, there are cases

00:11:05.480 --> 00:11:07.510
where we have failures
during verification.

00:11:07.510 --> 00:11:09.841
So for example, if you
install an APK that's

00:11:09.841 --> 00:11:12.340
maybe meant for the L release,
and you install it on KitKat,

00:11:12.340 --> 00:11:14.970
you may reference new
APIs from the L release.

00:11:14.970 --> 00:11:17.620
And so we deal with that by
noticing this [INAUDIBLE]

00:11:17.620 --> 00:11:19.620
is a soft failure
during verification,

00:11:19.620 --> 00:11:21.136
we mark that one
thing and so later

00:11:21.136 --> 00:11:22.510
on when the compiler
compiles, it

00:11:22.510 --> 00:11:24.926
can know that well, I'm not
sure if this code is avialable

00:11:24.926 --> 00:11:26.900
or not and put a slower
path in so that we

00:11:26.900 --> 00:11:28.130
can do the correct
thing and handle it

00:11:28.130 --> 00:11:30.421
if the code accidentally goes
down this path of calling

00:11:30.421 --> 00:11:33.140
a method that doesn't actually
exist in this release.

00:11:33.140 --> 00:11:34.880
So if all that passes,
we do this pass

00:11:34.880 --> 00:11:36.660
called the initialization pass.

00:11:36.660 --> 00:11:38.830
And we actually check
for any method, or sorry,

00:11:38.830 --> 00:11:40.400
any class that was
probably verified.

00:11:40.400 --> 00:11:43.260
If it all verified OK and
has no class initializer,

00:11:43.260 --> 00:11:46.042
we'll go ahead and mark it
as initialized ahead of time

00:11:46.042 --> 00:11:47.750
so that we know when
we're compiling code

00:11:47.750 --> 00:11:49.880
that references that class, that
we don't have to do any class

00:11:49.880 --> 00:11:50.796
initialization checks.

00:11:50.796 --> 00:11:55.305
Again, another optimization
for object only programming.

00:11:55.305 --> 00:11:56.680
And so finally
you can see, we've

00:11:56.680 --> 00:11:57.930
spent not quite half the time.

00:11:57.930 --> 00:11:59.760
But a lot of time in
the program down there

00:11:59.760 --> 00:12:02.218
through that purple section
during verification, and now we

00:12:02.218 --> 00:12:03.500
finally start to do compiling.

00:12:03.500 --> 00:12:07.087
And this is where the compiler
driver takes over and directs

00:12:07.087 --> 00:12:08.920
the different methods
to different compilers

00:12:08.920 --> 00:12:13.140
potentially, choosing to compile
for native code or JNI bridges,

00:12:13.140 --> 00:12:15.610
or just doing dex to dex
for things were not commonly

00:12:15.610 --> 00:12:16.910
run class initializers.

00:12:16.910 --> 00:12:19.440
We don't waste time or
space compiling it out.

00:12:19.440 --> 00:12:21.230
So finally, we have
a link step where

00:12:21.230 --> 00:12:23.780
we combine all the references
between the compiled code

00:12:23.780 --> 00:12:27.050
any references to the
framer's code and metadata.

00:12:27.050 --> 00:12:30.770
And then we link that together,
and we write our own ELF file.

00:12:30.770 --> 00:12:33.236
So what does this get us?

00:12:33.236 --> 00:12:35.110
So this is the slide
from yesterday's keynote

00:12:35.110 --> 00:12:37.690
I wanted to go into a
little bit more detail.

00:12:37.690 --> 00:12:39.790
You can see here
the blue bars are

00:12:39.790 --> 00:12:41.470
the Dalvik JIT as a baseline.

00:12:41.470 --> 00:12:43.620
This is our Nexus 5,
kind of an AOSP build

00:12:43.620 --> 00:12:47.330
from recently around
L snapshot time.

00:12:47.330 --> 00:12:48.950
The red bar show
the ART performance.

00:12:48.950 --> 00:12:51.400
And then you can see our
collection of common benchmarks

00:12:51.400 --> 00:12:53.950
we are doing better
than the Dalvik JIT.

00:12:53.950 --> 00:12:56.540
The lowest number there is
the AnTuTU composite score.

00:12:56.540 --> 00:12:58.070
And I wanted to just talk
about the for a second.

00:12:58.070 --> 00:12:59.570
Because the composite,
it's a composite

00:12:59.570 --> 00:13:01.010
of some things that
are running dex code,

00:13:01.010 --> 00:13:02.210
and some things that are
running native codes.

00:13:02.210 --> 00:13:03.876
We can only get
improvement on the parts

00:13:03.876 --> 00:13:06.445
of that composite benchmark
that are running dex code.

00:13:06.445 --> 00:13:08.894
And then we do get some
improvement on those areas.

00:13:08.894 --> 00:13:10.310
A lot of the other
benchmarks, you

00:13:10.310 --> 00:13:12.685
can see we have this general
kind of bars of improvement.

00:13:12.685 --> 00:13:14.960
Many of those are kind of
these historical benchmarks,

00:13:14.960 --> 00:13:17.560
like Drystone, or ford porter
from C or Fortran or something.

00:13:17.560 --> 00:13:19.518
And they're not really
written in a Java style.

00:13:19.518 --> 00:13:21.400
What we really like is
the chest bench score,

00:13:21.400 --> 00:13:23.770
because that's actually more
reflective of an Android

00:13:23.770 --> 00:13:27.229
developer writing functionary
code in a modern way

00:13:27.229 --> 00:13:28.520
where the benefits they'll see.

00:13:28.520 --> 00:13:29.880
And I think hopefully
that's more representative

00:13:29.880 --> 00:13:31.980
of what you'll see for
most applications doing

00:13:31.980 --> 00:13:35.690
kind of compute in Android.

00:13:35.690 --> 00:13:36.972
So with that. just a reminder.

00:13:36.972 --> 00:13:38.930
This is kind of a snapshot
of where we are now.

00:13:38.930 --> 00:13:41.320
We have additional work
we've been doing in AOSP

00:13:41.320 --> 00:13:42.590
that hopefully some of
that will make it into L,

00:13:42.590 --> 00:13:45.430
so we'll have more performance
boosting things before we ship.

00:13:45.430 --> 00:13:46.570
And with that, I
want to introduce

00:13:46.570 --> 00:13:48.861
Ian Rogers, whose going to
talk about other performance

00:13:48.861 --> 00:13:51.010
boosting things, the
ART garbage collectors.

00:13:54.320 --> 00:13:56.170
IAN ROGERS: Thank you, Brian.

00:13:56.170 --> 00:14:00.110
So-- very loud.

00:14:00.110 --> 00:14:02.280
So I'm going to talk
about garbage collection,

00:14:02.280 --> 00:14:06.310
and basically how we're going
to achieve fast allocation.

00:14:06.310 --> 00:14:08.220
What we've been really
focusing on less junk,

00:14:08.220 --> 00:14:10.740
giving you a buttery experience
when you're using Android,

00:14:10.740 --> 00:14:14.010
and minimizing the
memory footprint.

00:14:14.010 --> 00:14:17.790
So what is the memory
manager trying to do?

00:14:17.790 --> 00:14:21.800
We're trying to allocate
objects inside a application.

00:14:21.800 --> 00:14:24.237
We need to track on
behalf of the application

00:14:24.237 --> 00:14:26.820
whether those objects are still
in use, whether they're alive.

00:14:26.820 --> 00:14:30.670
And need to reclaim them when
they're no longer in use.

00:14:30.670 --> 00:14:33.814
We want to do all this and
we want to do it for free.

00:14:33.814 --> 00:14:38.060
And this is a real advantage
in the Android ecosystem

00:14:38.060 --> 00:14:42.640
is that this burden is taken
away from the developer.

00:14:42.640 --> 00:14:45.280
So let's think about
memory management schemes.

00:14:45.280 --> 00:14:47.240
There's no approach
which is free.

00:14:47.240 --> 00:14:48.896
If you have native
code, you have

00:14:48.896 --> 00:14:50.770
to worry about when
you're allocating objects

00:14:50.770 --> 00:14:52.180
when you're going
to reclaim them.

00:14:52.180 --> 00:14:54.388
You have to worry about
complex data structures which

00:14:54.388 --> 00:14:55.606
might have cycles in them.

00:14:55.606 --> 00:14:57.480
You have to worry about
multi-threaded issues

00:14:57.480 --> 00:15:00.350
like when an object says
accessed by multiple threads.

00:15:00.350 --> 00:15:02.550
So there are costs
associated with that.

00:15:02.550 --> 00:15:04.210
If you're using
reference counting,

00:15:04.210 --> 00:15:07.090
than you can find that a
thread that lowers a reference

00:15:07.090 --> 00:15:09.100
count suddenly ends
up having to reclaim

00:15:09.100 --> 00:15:11.110
all of the garbage in
the known universe.

00:15:11.110 --> 00:15:13.110
And all of these things
can create a lot of work

00:15:13.110 --> 00:15:16.100
on an application thread
and cause significant junk

00:15:16.100 --> 00:15:19.670
and problems for the
application developer.

00:15:19.670 --> 00:15:22.460
So what we've been
focusing on in ART

00:15:22.460 --> 00:15:24.910
is making GC
something that doesn't

00:15:24.910 --> 00:15:26.979
get in the way of
the application

00:15:26.979 --> 00:15:29.270
and doesn't get in the way
of the application developer

00:15:29.270 --> 00:15:31.750
so they're freed from
worrying about it.

00:15:31.750 --> 00:15:36.560
And our principal consideration
has been how to reduce junk.

00:15:36.560 --> 00:15:38.840
And this comes around
from post times

00:15:38.840 --> 00:15:40.120
from the garbage collector.

00:15:40.120 --> 00:15:44.030
And I'll go into
detail about that.

00:15:44.030 --> 00:15:49.050
So let's look at what's going
on inside a garbage collection.

00:15:49.050 --> 00:15:52.480
Here, we have some threads
though, the wiggly lines.

00:15:52.480 --> 00:15:55.450
We have a heap, which
is the rectangle.

00:15:55.450 --> 00:15:59.010
And inside the heap are
green and red rectangles

00:15:59.010 --> 00:16:01.700
indicative of live objects
which are objects which

00:16:01.700 --> 00:16:04.340
are in use by the application
and dead objects, which

00:16:04.340 --> 00:16:07.534
are objects that the application
can no longer access.

00:16:07.534 --> 00:16:09.200
The blue object on
the left is an object

00:16:09.200 --> 00:16:11.010
that we're trying to allocate.

00:16:13.950 --> 00:16:18.290
So the job that the garbage
collector's got to do

00:16:18.290 --> 00:16:21.640
is at allocation time, it's
got to find places for objects

00:16:21.640 --> 00:16:24.000
to fit in to get allocated.

00:16:24.000 --> 00:16:27.950
And it's also a GC time,
it's got to go and find

00:16:27.950 --> 00:16:30.510
all of these live objects,
these green objects.

00:16:30.510 --> 00:16:32.460
And having found
all of them, it then

00:16:32.460 --> 00:16:33.900
has to go and free them up.

00:16:36.610 --> 00:16:41.430
So let's see what happens
inside of a Dalvik.

00:16:41.430 --> 00:16:45.050
When we're going and
finding this initial set

00:16:45.050 --> 00:16:48.110
of live objects, Dalvik
will suspend the application

00:16:48.110 --> 00:16:48.640
threads.

00:16:48.640 --> 00:16:52.360
It also suspends threads
inside of the virtual machine.

00:16:52.360 --> 00:16:53.860
So we tend to refer
to these threads

00:16:53.860 --> 00:16:55.280
as a whole as mutator threads.

00:16:55.280 --> 00:16:57.910
They mutate the state
of the Java heap.

00:16:57.910 --> 00:17:00.450
And to do this, Dalvik
suspends these threads

00:17:00.450 --> 00:17:02.790
so that it can crawl
less stacks and find

00:17:02.790 --> 00:17:05.599
what objects those
stacks are referencing.

00:17:05.599 --> 00:17:09.884
Once it's determined, this
initial reachable set,

00:17:09.884 --> 00:17:12.032
it then traverses it
in a concurrent phase.

00:17:12.032 --> 00:17:13.490
So the garbage
collector is running

00:17:13.490 --> 00:17:15.700
alongside the application.

00:17:15.700 --> 00:17:18.760
Once it's found
all of the objects,

00:17:18.760 --> 00:17:20.730
it then goes and
does another pause.

00:17:20.730 --> 00:17:22.760
The reason for this
pause is to make sure

00:17:22.760 --> 00:17:24.359
that the application
wasn't playing

00:17:24.359 --> 00:17:26.510
any games with the
garbage collector

00:17:26.510 --> 00:17:28.834
and hiding objects which the
garbage collector thought

00:17:28.834 --> 00:17:29.875
it had already processed.

00:17:32.410 --> 00:17:34.270
In this phase, the
card table will

00:17:34.270 --> 00:17:36.970
be examined by the
garbage collector.

00:17:36.970 --> 00:17:40.460
Finally, having determined that
everything has been marked,

00:17:40.460 --> 00:17:43.230
the garbage collector
restarts all the threads,

00:17:43.230 --> 00:17:45.457
and it concurrently
sweeps and frees up

00:17:45.457 --> 00:17:46.790
the object which were allocated.

00:17:49.870 --> 00:17:55.250
So in Dalvik, the pauses
have been highly optimized.

00:17:55.250 --> 00:17:57.670
And the first pause is
around 3 to 4 milliseconds,

00:17:57.670 --> 00:18:00.420
and the second pause is
around 5 to 6 milliseconds.

00:18:00.420 --> 00:18:03.970
But the sum total of that
is about 10 milliseconds.

00:18:03.970 --> 00:18:06.330
If we're going to achieve
60 frames per second,

00:18:06.330 --> 00:18:09.749
we need to be able to run the
frames in 16 milliseconds.

00:18:09.749 --> 00:18:11.790
And taking 10 milliseconds
for garbage collection

00:18:11.790 --> 00:18:14.880
pauses out of that
60 milliseconds

00:18:14.880 --> 00:18:16.460
means we get dropped frames.

00:18:16.460 --> 00:18:24.420
So here is an assist trace of
Dalvik running the Google Play

00:18:24.420 --> 00:18:25.120
store.

00:18:25.120 --> 00:18:26.810
And we're doing
a fling, so we're

00:18:26.810 --> 00:18:31.080
scrolling down and allowing
lots of bitmaps to scroll by.

00:18:31.080 --> 00:18:32.560
And the bitmaps
are causing objects

00:18:32.560 --> 00:18:35.940
to get created, and
caught, and triggering GCs.

00:18:35.940 --> 00:18:38.730
And at the top,
you can see what's

00:18:38.730 --> 00:18:41.890
running on each individual CPU.

00:18:41.890 --> 00:18:44.070
Beneath this are
the vsync events,

00:18:44.070 --> 00:18:46.820
which are the screen
refresh events.

00:18:46.820 --> 00:18:50.400
Below this, we have the
vending application,

00:18:50.400 --> 00:18:53.360
the name of the Play
Store application.

00:18:53.360 --> 00:18:56.210
And its rendering
frames, and you

00:18:56.210 --> 00:18:59.330
can see that in the dark green.

00:18:59.330 --> 00:19:02.279
And ideally, what we would
see is with every vsync event,

00:19:02.279 --> 00:19:03.570
there would be some dark green.

00:19:03.570 --> 00:19:06.510
But you can see in the sensor
here that the dark green isn't

00:19:06.510 --> 00:19:09.490
there, and that's because
of a dropped frame.

00:19:09.490 --> 00:19:11.590
It was causing a drop frame.

00:19:11.590 --> 00:19:13.940
At the bottom,
we've got the view

00:19:13.940 --> 00:19:16.110
of what's going on
inside of Dalvik.

00:19:16.110 --> 00:19:20.220
And we can see that
there are GCs going on.

00:19:20.220 --> 00:19:23.820
They're explicit GCs
going on back to back.

00:19:23.820 --> 00:19:25.870
Another pauses within this.

00:19:25.870 --> 00:19:28.990
And we can see that the
second pause came in just

00:19:28.990 --> 00:19:32.390
before the vsync event where
we had the dropped frame.

00:19:32.390 --> 00:19:33.594
So this isn't good.

00:19:33.594 --> 00:19:36.260
This isn't giving us the buttery
experience you want on Android.

00:19:39.220 --> 00:19:41.900
And one of the things
that was on that systrace,

00:19:41.900 --> 00:19:44.400
we saw that GC cycles were
happening back to back.

00:19:44.400 --> 00:19:46.070
And why was this happening?

00:19:46.070 --> 00:19:50.840
Well, the application was
trying to avoid a pathology.

00:19:50.840 --> 00:19:53.600
And kind of what happens
in Dalvik's behavior

00:19:53.600 --> 00:19:57.520
where Dalvik is trying to
avoid a heap fragmentation

00:19:57.520 --> 00:19:59.940
and allowing the heap
to grow, because it's

00:19:59.940 --> 00:20:03.240
worried about large
objects getting reclaimed,

00:20:03.240 --> 00:20:06.705
and then small objects coming in
and fragmenting the heap making

00:20:06.705 --> 00:20:08.830
it so that the wrong places
where large objects can

00:20:08.830 --> 00:20:11.890
come and fit into the heap.

00:20:11.890 --> 00:20:15.610
So this is kind of shown
on this graphic here.

00:20:15.610 --> 00:20:17.900
We've got the large blue object.

00:20:17.900 --> 00:20:20.510
We failed to find
somewhere to fit it.

00:20:20.510 --> 00:20:22.380
We're worried about
fragmentation,

00:20:22.380 --> 00:20:25.210
and so Dalvik is going to
suspend all of the application

00:20:25.210 --> 00:20:28.410
threads and do it
a GC for our log,

00:20:28.410 --> 00:20:31.930
to then go and free up all
of the objects it can so

00:20:31.930 --> 00:20:33.500
hopefully, do a
better job at fitting

00:20:33.500 --> 00:20:37.720
this large object into the heap.

00:20:37.720 --> 00:20:39.610
What does that look like
for the application?

00:20:39.610 --> 00:20:41.980
It looks like one large pause.

00:20:41.980 --> 00:20:46.230
And these pauses are typically
around 50 milliseconds.

00:20:46.230 --> 00:20:51.270
So this is going to translate
into four dropped frames.

00:20:51.270 --> 00:20:55.410
Here, we can see again, a
systrace of a Google Maps

00:20:55.410 --> 00:20:57.640
application this time.

00:20:57.640 --> 00:21:02.780
And we can see that's
we've got two GCs happening

00:21:02.780 --> 00:21:04.190
at the bottom.

00:21:04.190 --> 00:21:09.380
And for in this
situation, the two GCs

00:21:09.380 --> 00:21:13.640
have caused a pause of around
60 milliseconds, and 4 to 5

00:21:13.640 --> 00:21:14.420
dropped frames.

00:21:17.680 --> 00:21:20.730
So for us, everything
is awesome.

00:21:20.730 --> 00:21:23.760
We fixed all the problems.

00:21:23.760 --> 00:21:25.590
Open bugs, if we haven't.

00:21:25.590 --> 00:21:26.800
Please

00:21:26.800 --> 00:21:32.030
[APPLAUSE]

00:21:32.030 --> 00:21:35.150
So we've made the
garbage collector faster

00:21:35.150 --> 00:21:38.040
by new backup
collection algorithms.

00:21:38.040 --> 00:21:40.340
We've reduced the
number of causes

00:21:40.340 --> 00:21:44.170
and made the causes
themselves shorter.

00:21:44.170 --> 00:21:49.030
We've taken ergonomics
decisions and strategies

00:21:49.030 --> 00:21:50.920
to reduce the amount
of fragmentation

00:21:50.920 --> 00:21:53.960
so that we no longer
need the GC for analog.

00:21:53.960 --> 00:21:56.360
And we've done all
of this with support

00:21:56.360 --> 00:21:57.920
for things like
moving and compacting

00:21:57.920 --> 00:22:01.540
collectors and ways that
allow us to use less memory.

00:22:05.200 --> 00:22:08.780
So what do pauses look like in
the ART's garbage collector?

00:22:08.780 --> 00:22:11.750
Going back to the
earlier graphic,

00:22:11.750 --> 00:22:14.710
we've got rid of
the first pause.

00:22:14.710 --> 00:22:18.330
What we do to create this
initial set of objects

00:22:18.330 --> 00:22:20.272
that the garbage collector
needs to traverse

00:22:20.272 --> 00:22:21.725
is the garbage
collector requests

00:22:21.725 --> 00:22:25.850
that the applications go
and mark their own stacks.

00:22:25.850 --> 00:22:29.510
So this is a very short thing
for the application threads

00:22:29.510 --> 00:22:30.120
to do.

00:22:30.120 --> 00:22:32.420
And after they've done it,
they can carry on running.

00:22:32.420 --> 00:22:34.730
So there's no need to suspend
all of the applications

00:22:34.730 --> 00:22:35.850
threads.

00:22:35.850 --> 00:22:40.690
So this checkpointing, as it
gets called gets done first,

00:22:40.690 --> 00:22:42.795
and the garbage
collector waits for that.

00:22:42.795 --> 00:22:44.420
And then it enters
the concurrent phase

00:22:44.420 --> 00:22:47.650
that we had before
in the Dalvik CMS

00:22:47.650 --> 00:22:50.130
asking where we're going
to mark all of the objects.

00:22:50.130 --> 00:22:52.930
Then we have the second pause.

00:22:52.930 --> 00:22:55.110
And we've managed to
make pause a lot shorter

00:22:55.110 --> 00:22:57.820
than the Dalvik pause.

00:22:57.820 --> 00:23:01.680
We've done this by tricks like
[INAUDIBLE] cleaning where

00:23:01.680 --> 00:23:04.620
we take work out of the
pause and do it ahead of it,

00:23:04.620 --> 00:23:06.055
and then we just
double check that

00:23:06.055 --> 00:23:08.070
what we did before
the pause was correct.

00:23:11.640 --> 00:23:15.660
If we look at systrace, this is
going back to the Google Play

00:23:15.660 --> 00:23:20.110
Store application,
and doing a fling.

00:23:20.110 --> 00:23:23.620
We've got the vsync events
and frames being rendered.

00:23:23.620 --> 00:23:26.270
We've got a garbage
collection pause

00:23:26.270 --> 00:23:31.520
happening just where the
single pause is indicated.

00:23:31.520 --> 00:23:34.230
And no dropped frames.

00:23:34.230 --> 00:23:37.890
The other thing that you
can see on this systrace

00:23:37.890 --> 00:23:40.020
is that the GCs aren't
occuring, the back

00:23:40.020 --> 00:23:42.560
to back GCs that was coming
in the Play Store before.

00:23:42.560 --> 00:23:45.420
And that's because ART doesn't
suffer from the fragmentation

00:23:45.420 --> 00:23:47.700
problems that Dalvik had.

00:23:47.700 --> 00:23:52.320
And so we now trigger, treat
the system GCs as something

00:23:52.320 --> 00:23:54.470
are optional, and we
only use them as a hint

00:23:54.470 --> 00:23:57.100
that we should trigger a
garbage collection cycle.

00:23:57.100 --> 00:23:59.920
Whereas with Dalvik,
they would always

00:23:59.920 --> 00:24:02.870
trigger a concurrent
garbage collection cycle.

00:24:02.870 --> 00:24:05.550
And so there are no
dropped frames here.

00:24:05.550 --> 00:24:09.520
And that single pause is
measuring about 3 milliseconds.

00:24:09.520 --> 00:24:11.652
So that was kind of a lot
faster than the pauses

00:24:11.652 --> 00:24:12.860
that you're seeing in Dalvik.

00:24:15.580 --> 00:24:21.100
So how did we get rid of
the GC for analog events?

00:24:21.100 --> 00:24:24.895
So the problem we were having in
Dalvik was these large objects

00:24:24.895 --> 00:24:25.520
were coming in.

00:24:25.520 --> 00:24:30.466
And these large objects would
be bitmaps in the common case.

00:24:30.466 --> 00:24:37.090
And so what we do in ART
is we move the bitmaps out

00:24:37.090 --> 00:24:41.570
of the main heap, and put them
into their own separate, still

00:24:41.570 --> 00:24:43.560
part of the managed heap.

00:24:43.560 --> 00:24:46.030
But it's a separate area in
the managed heap especially

00:24:46.030 --> 00:24:50.730
dedicated to large arrays
of primitive objects.

00:24:50.730 --> 00:24:52.840
And this is also useful
for us, because we

00:24:52.840 --> 00:24:56.000
know that by definition, a
raise of primitive objects

00:24:56.000 --> 00:24:58.160
don't have references
to other objects.

00:24:58.160 --> 00:25:01.850
And so we can improve the
bookkeeping and the performance

00:25:01.850 --> 00:25:03.600
of the garbage collector
as a consequence.

00:25:07.300 --> 00:25:09.680
What does this look
like for an application

00:25:09.680 --> 00:25:11.260
like for Google Maps?

00:25:11.260 --> 00:25:15.310
So on the left, we've
got the concurrent marks

00:25:15.310 --> 00:25:17.020
GCs happening in Dalvik.

00:25:17.020 --> 00:25:19.600
The first pause, around
3 to 4 milliseconds,

00:25:19.600 --> 00:25:22.370
the second pause around
5 to 6 milliseconds.

00:25:22.370 --> 00:25:25.300
And total coming in at
just under 10 milliseconds.

00:25:25.300 --> 00:25:28.120
These are times from a Nexus 4.

00:25:28.120 --> 00:25:30.030
The GC for analogs,
which typically

00:25:30.030 --> 00:25:33.420
taking the average one was
around 54 milliseconds.

00:25:33.420 --> 00:25:36.410
And so that translates into
a lot of dropped frames.

00:25:36.410 --> 00:25:40.590
What is ART looking like
in the same test scenario?

00:25:40.590 --> 00:25:43.040
Well the average
pauses are coming

00:25:43.040 --> 00:25:45.550
in at 2 and 1/2 milliseconds.

00:25:45.550 --> 00:25:47.040
So that's around
four times faster.

00:25:50.410 --> 00:25:55.140
We've also found that
one of the areas where

00:25:55.140 --> 00:25:57.640
we needed to improve was
in terms of our allocation

00:25:57.640 --> 00:25:58.420
performance.

00:25:58.420 --> 00:26:02.500
So we were working with the
Google Spreadsheets team

00:26:02.500 --> 00:26:06.242
internally inside of Google,
and they were really excited

00:26:06.242 --> 00:26:07.950
by ART, because they've
been experiencing

00:26:07.950 --> 00:26:09.500
some bad performance.

00:26:09.500 --> 00:26:11.480
And so they switched
over to using ART,

00:26:11.480 --> 00:26:13.590
but they didn't realize
that the full performance

00:26:13.590 --> 00:26:15.660
that they were expecting to see.

00:26:15.660 --> 00:26:19.200
So you can see that
on this bar chart.

00:26:19.200 --> 00:26:24.450
On the left normalized to one
is the Dalvik performance.

00:26:24.450 --> 00:26:28.000
And when they switched
over to using ART,

00:26:28.000 --> 00:26:31.570
they managed to get
a 25% performance

00:26:31.570 --> 00:26:34.810
improvement on a Nexus 4.

00:26:34.810 --> 00:26:36.000
However, they wanted more.

00:26:36.000 --> 00:26:36.860
They wanted the 2X.

00:26:36.860 --> 00:26:40.250
They wanted the 4X.

00:26:40.250 --> 00:26:44.270
So there were other things we
did in ART, having seen this.

00:26:44.270 --> 00:26:46.270
So one of the things was
we started specializing

00:26:46.270 --> 00:26:50.550
the allocation paths so that
the allocation paths cannot

00:26:50.550 --> 00:26:55.110
allocate certain classes of
objects faster than others

00:26:55.110 --> 00:26:58.880
by knowing things like the size
of the object ahead of time.

00:26:58.880 --> 00:27:02.610
And this achieved
a performance win,

00:27:02.610 --> 00:27:05.630
but not the dramatic performance
win we would really have liked.

00:27:05.630 --> 00:27:08.320
So the next thing that
we did was something

00:27:08.320 --> 00:27:10.560
that we'd wanted to
do for a long time.

00:27:10.560 --> 00:27:13.210
And we implemented our
own memory allocator.

00:27:13.210 --> 00:27:17.910
So Dalvik and ART
in KitKat we're

00:27:17.910 --> 00:27:21.310
basing on top of Doug Lea's
memory allocator, which

00:27:21.310 --> 00:27:25.160
is the common one used
in Unix's and so on.

00:27:25.160 --> 00:27:27.200
And this supports
a [INAUDIBLE] free,

00:27:27.200 --> 00:27:29.450
but it's not really
tuned for Java,

00:27:29.450 --> 00:27:33.160
and it's not really tuned for
a multi-threaded environment.

00:27:33.160 --> 00:27:37.010
So to make sure that it
behaves in a correct manner,

00:27:37.010 --> 00:27:40.000
it has a single
global lock around it.

00:27:40.000 --> 00:27:42.240
And so the garbage collector
could be freeing things,

00:27:42.240 --> 00:27:44.365
and that would conflict
with the application trying

00:27:44.365 --> 00:27:46.090
to allocate objects.

00:27:46.090 --> 00:27:48.670
And you had multiple allocations
happening at the same time,

00:27:48.670 --> 00:27:52.440
then they would get
run sequentially.

00:27:52.440 --> 00:27:57.070
The rosallac, as we call it,
the rows of slots allocator,

00:27:57.070 --> 00:28:00.820
has a number of optimizations
to improve performance.

00:28:00.820 --> 00:28:03.050
The first of which,
is the small objects,

00:28:03.050 --> 00:28:06.010
the small temporary
objects that is

00:28:06.010 --> 00:28:10.880
so useful in
languages like Java.

00:28:10.880 --> 00:28:14.860
They get allocated in
a thread local region.

00:28:14.860 --> 00:28:17.229
And so because the
region is thread local,

00:28:17.229 --> 00:28:18.770
this means that
allocations can occur

00:28:18.770 --> 00:28:22.850
with no locking and no kind
of sequential behavior.

00:28:22.850 --> 00:28:26.890
For larger objects, the
bends that they fit into

00:28:26.890 --> 00:28:28.430
have their own individual locks.

00:28:28.430 --> 00:28:31.210
So again, it's reducing the
contention for the locks,

00:28:31.210 --> 00:28:33.750
and is a lot more parallelism.

00:28:33.750 --> 00:28:37.960
This is translated into a huge
performance for this benchmark,

00:28:37.960 --> 00:28:40.750
and you can see that
the memory allocation

00:28:40.750 --> 00:28:42.320
performance has improved 10x.

00:28:46.050 --> 00:28:49.450
So we've been working on a
number of different garbage

00:28:49.450 --> 00:28:51.420
collection algorithms for ART.

00:28:51.420 --> 00:28:53.264
We're making ART
into a framework

00:28:53.264 --> 00:28:55.680
that developers and partners
can come along and contribute

00:28:55.680 --> 00:28:59.160
to and improve upon.

00:28:59.160 --> 00:29:01.930
The first new garbage
collector that ART added

00:29:01.930 --> 00:29:03.750
was a sticky garbage collector.

00:29:03.750 --> 00:29:06.870
The sticky garbage collector
takes advantage of the fact

00:29:06.870 --> 00:29:08.950
that we can know
what objects were

00:29:08.950 --> 00:29:12.440
allocated since
the last GC cycle.

00:29:12.440 --> 00:29:14.530
And because the generational
hypothesis tells us

00:29:14.530 --> 00:29:17.670
that these objects are
the most likely to be

00:29:17.670 --> 00:29:22.630
useful to be freed up, then
we focus our energy on these.

00:29:22.630 --> 00:29:25.110
And because garbage
collection at the time

00:29:25.110 --> 00:29:26.860
is proportional to the
amount of live data

00:29:26.860 --> 00:29:28.040
we're going to
process, and we're

00:29:28.040 --> 00:29:29.706
going to process less
because we're only

00:29:29.706 --> 00:29:32.530
going to consider the live
data since the last GC,

00:29:32.530 --> 00:29:36.710
the sticky GC is typically
running 2x or 3x faster

00:29:36.710 --> 00:29:38.370
than the current
garbage collector,

00:29:38.370 --> 00:29:41.187
and reclaiming the
same amount of memory.

00:29:41.187 --> 00:29:43.020
When the sticky garbage
collector can't run,

00:29:43.020 --> 00:29:46.890
we run the regular CMS collector
for either the applications

00:29:46.890 --> 00:29:50.510
heap, or the applications
heap in the zygote.

00:29:50.510 --> 00:29:53.440
And the ratio between running
the sticky garbage collector

00:29:53.440 --> 00:29:56.390
and the regular concurrent
garbage collector,

00:29:56.390 --> 00:29:58.660
we tend to run the sticky
one five times for every one

00:29:58.660 --> 00:30:01.890
time we need to run
the partial one.

00:30:01.890 --> 00:30:04.420
We're also working on
moving garbage collectors

00:30:04.420 --> 00:30:06.830
where this slide comes in.

00:30:06.830 --> 00:30:10.410
And what are moving
garbage collectors?

00:30:10.410 --> 00:30:13.430
So moving garbage
collectors, they

00:30:13.430 --> 00:30:15.920
want to do the
same reset creation

00:30:15.920 --> 00:30:17.560
and marking of objects.

00:30:17.560 --> 00:30:20.530
But what they want to do
during the sweeping phase

00:30:20.530 --> 00:30:25.030
is they want to move all
of those objects together.

00:30:25.030 --> 00:30:28.790
And this basically removes all
of the fragmentation and so on.

00:30:28.790 --> 00:30:31.700
We do this in ART
via two approaches.

00:30:31.700 --> 00:30:33.900
We have a semi space
garbage collector

00:30:33.900 --> 00:30:36.300
where we create a
new memory region,

00:30:36.300 --> 00:30:38.430
and we evacuate the
objects out of a heap

00:30:38.430 --> 00:30:41.230
and into that newly
created space.

00:30:41.230 --> 00:30:42.990
And we also have a
mock compact algorithm

00:30:42.990 --> 00:30:45.710
that's coming online
at the moment where

00:30:45.710 --> 00:30:47.650
we compact things in place.

00:30:52.610 --> 00:30:54.960
So moving collectors are good.

00:30:54.960 --> 00:30:58.100
There's less fragmentation,
it saves heap.

00:30:58.100 --> 00:31:00.850
The downside to them
is that if you're

00:31:00.850 --> 00:31:02.570
going to be moving
objects around,

00:31:02.570 --> 00:31:04.320
the effects of
moving an object has

00:31:04.320 --> 00:31:06.494
to be seen by the
application atomically.

00:31:06.494 --> 00:31:08.660
There are two approaches
you can take to doing this.

00:31:08.660 --> 00:31:12.450
You can introduce read barriers
so that the read barrier

00:31:12.450 --> 00:31:15.930
handles the fact that the object
is moving around in the heap.

00:31:15.930 --> 00:31:17.560
Or the most common
approach is just

00:31:17.560 --> 00:31:20.350
to suspend all of the
application threads again.

00:31:20.350 --> 00:31:25.020
And this isn't good from
a GC junk point of view.

00:31:25.020 --> 00:31:27.972
So we want to use the moving
collector to compact the heap

00:31:27.972 --> 00:31:29.180
and get these memory savings.

00:31:29.180 --> 00:31:30.590
So when can we do it?

00:31:30.590 --> 00:31:32.490
So we know certain
things about how

00:31:32.490 --> 00:31:34.380
applications run in Android.

00:31:34.380 --> 00:31:36.470
We know that it's
worthwhile to compact

00:31:36.470 --> 00:31:38.730
the heap joined zygote startup.

00:31:38.730 --> 00:31:40.497
Because if we save
space in the zygote,

00:31:40.497 --> 00:31:42.330
we're going to save it
for every application

00:31:42.330 --> 00:31:44.820
after the Android runtime is up.

00:31:44.820 --> 00:31:48.547
The other place we
can use COMPAxiON

00:31:48.547 --> 00:31:50.380
is when applications
go into the background,

00:31:50.380 --> 00:31:52.195
so when you press
the Home screen.

00:31:52.195 --> 00:31:53.570
And so we have
various ergonomics

00:31:53.570 --> 00:31:56.290
determining when is
a good idea to run,

00:31:56.290 --> 00:31:59.545
to switch the garbage collector
over to a compacting algorithm.

00:32:02.320 --> 00:32:04.740
We have various
means of determining

00:32:04.740 --> 00:32:07.810
whether an application
can perceive junk.

00:32:07.810 --> 00:32:10.530
We don't want to do things
like have the Play music

00:32:10.530 --> 00:32:12.790
application, which isn't a
foregrounded application.

00:32:12.790 --> 00:32:14.248
We don't want to
have junk in that,

00:32:14.248 --> 00:32:17.220
because that would
give you choppy audio.

00:32:17.220 --> 00:32:20.590
And so hopefully we've
done our jobs right,

00:32:20.590 --> 00:32:22.380
and it's all working, and so on.

00:32:22.380 --> 00:32:27.400
And so you'll get
less memory usage.

00:32:27.400 --> 00:32:30.239
So with that, let
me hand back Anwar.

00:32:30.239 --> 00:32:31.280
ANWAR GHULOUM: All right.

00:32:31.280 --> 00:32:33.320
Thanks Ian.

00:32:33.320 --> 00:32:37.570
So a few words about 64-bit
support coming with L.

00:32:37.570 --> 00:32:40.890
So it's one of the major
features of L. Partners

00:32:40.890 --> 00:32:43.400
have been shipping, or
readying to ship 64-bit SFZs,

00:32:43.400 --> 00:32:45.330
and we're working
pretty closely with them

00:32:45.330 --> 00:32:48.570
to make 64-bit userspace happen.

00:32:48.570 --> 00:32:50.680
So why 64-bit?

00:32:50.680 --> 00:32:52.597
Well you might
argue that we don't

00:32:52.597 --> 00:32:54.180
need the additional
address space now,

00:32:54.180 --> 00:32:56.950
but if you look at
the keynote yesterday,

00:32:56.950 --> 00:32:59.990
Android usage is
diversifying pretty rapidly

00:32:59.990 --> 00:33:01.650
from wearables to TVs.

00:33:01.650 --> 00:33:10.480
And bearing in mind,
I think it's really--

00:33:10.480 --> 00:33:13.530
and there's definitely value
today in supporting 64-bit.

00:33:13.530 --> 00:33:17.710
We see nice performance
gains in 64-bit apps

00:33:17.710 --> 00:33:20.417
running on 64-bit capable cores.

00:33:20.417 --> 00:33:22.750
There are new instructions,
domain specific instructions

00:33:22.750 --> 00:33:25.385
for media encrypto, where
we get huge speedups that

00:33:25.385 --> 00:33:27.110
can benefit the entire platform.

00:33:27.110 --> 00:33:28.775
But even in general
purpose apps,

00:33:28.775 --> 00:33:30.880
we're seeing nice speedups.

00:33:30.880 --> 00:33:33.390
So we'll take a
look at that first,

00:33:33.390 --> 00:33:35.170
and then I'll get
into a few details

00:33:35.170 --> 00:33:38.100
of how we make this work.

00:33:38.100 --> 00:33:42.770
So as I said, we spec
compute intensive stuff

00:33:42.770 --> 00:33:45.790
to take biggest
advantage of this.

00:33:45.790 --> 00:33:50.390
We're working closely with ARM,
with Intel, MIPS, Qualcomm,

00:33:50.390 --> 00:33:53.730
and others to make sure that
we really deliver on this.

00:33:53.730 --> 00:33:56.380
So what I'm showing
you here is graphs

00:33:56.380 --> 00:34:00.350
of speedup of 64-bit
apps over 32-bit apps

00:34:00.350 --> 00:34:02.750
on the same silicon.

00:34:02.750 --> 00:34:07.380
On upper left hand side
as you face the screen,

00:34:07.380 --> 00:34:10.020
I'm showing you speedup
in terms of a multiplier.

00:34:10.020 --> 00:34:13.870
This is on Intel's Bay Trail.

00:34:13.870 --> 00:34:18.340
Processor's a four core SOC
for some custom render script

00:34:18.340 --> 00:34:19.090
scripts.

00:34:19.090 --> 00:34:20.840
And we're seeing some
really nice speedups

00:34:20.840 --> 00:34:26.040
going to 64-bit
up to and over 4x.

00:34:26.040 --> 00:34:29.350
For crypto, we use open SSL's
a primary crypto engine.

00:34:29.350 --> 00:34:35.230
If you look at ARM64 support,
ARMv8 support on Cortex A53

00:34:35.230 --> 00:34:39.181
and Cortex A57 for the
open SSL speed benchmark,

00:34:39.181 --> 00:34:41.389
we're seeing some really
nice speedups there as well.

00:34:41.389 --> 00:34:43.514
Again, this is used throughout
the entire platform.

00:34:43.514 --> 00:34:45.560
Any applications that's
using Android crypto

00:34:45.560 --> 00:34:46.630
is probably going to
benefit from this.

00:34:46.630 --> 00:34:49.380
And again, those are multipliers
on the vertical axis sourcing

00:34:49.380 --> 00:34:52.469
about, a 15x improvement there.

00:34:52.469 --> 00:34:56.340
And even for native stuff, we're
seeing some nice improvements.

00:34:56.340 --> 00:34:57.960
Panorama benchmark
is a benchmark

00:34:57.960 --> 00:35:00.640
that we use internally to
evaluate tool chain updates.

00:35:00.640 --> 00:35:04.130
It's meant to be
representative of a user taking

00:35:04.130 --> 00:35:06.810
a bunch of photos and then
waiting for them to get

00:35:06.810 --> 00:35:10.770
stitched together in a
nice little panorama.

00:35:10.770 --> 00:35:15.940
What we see here is that on
Cortex ARMS, Cortex A53 and A57

00:35:15.940 --> 00:35:20.420
cores simply recompiling
it for 64-bit,

00:35:20.420 --> 00:35:24.440
we see up to 20%
boost for this one.

00:35:24.440 --> 00:35:27.340
That means the end user
is sitting there waiting

00:35:27.340 --> 00:35:29.940
for a panorama, and
happens 20% faster.

00:35:29.940 --> 00:35:33.970
But it also uses 20% less power
probably in terms of CPU core.

00:35:33.970 --> 00:35:37.979
And that's a really nice thing.

00:35:37.979 --> 00:35:38.770
But what about ART?

00:35:38.770 --> 00:35:40.330
So for ART, we
weren't really looking

00:35:40.330 --> 00:35:43.130
for big performance
gains of 64-bit.

00:35:43.130 --> 00:35:44.990
Obviously we were
hoping, but we've

00:35:44.990 --> 00:35:48.970
been pleased with what
we've seen thus far.

00:35:48.970 --> 00:35:51.080
We expected most of this
again, to come again

00:35:51.080 --> 00:35:53.360
in the computer
intensive workloads.

00:35:53.360 --> 00:35:56.820
So looking at ART performance
here, this is on Baytrail.

00:35:56.820 --> 00:36:01.290
We have on the left,
speedups of going

00:36:01.290 --> 00:36:04.790
to ART 32-bit from
Dalvik 32-bit.

00:36:04.790 --> 00:36:07.400
So they're getting some really
nice speedups on spec JVM

00:36:07.400 --> 00:36:09.750
up to over 6x.

00:36:09.750 --> 00:36:12.430
But the cherry on
top is that we get up

00:36:12.430 --> 00:36:15.310
to another 30% speedup
going to 64-bit.

00:36:15.310 --> 00:36:17.380
And again, all these
numbers, it's early days.

00:36:17.380 --> 00:36:18.880
There's a lot more
tuning that we're

00:36:18.880 --> 00:36:22.840
going to be doing for 64-bit
with ARM Intel and MIPS.

00:36:22.840 --> 00:36:25.010
So expect more to
come from this.

00:36:29.310 --> 00:36:30.810
OK, so how are we
getting this done?

00:36:30.810 --> 00:36:33.140
Well, as Brian mentioned, we
have compilers for 64-bit,

00:36:33.140 --> 00:36:38.310
for x86 64, for ARM64,
and MIPS64 coming soon.

00:36:38.310 --> 00:36:41.240
We've extended the zygote
model for app creation

00:36:41.240 --> 00:36:42.450
by having dual zygotes.

00:36:42.450 --> 00:36:44.350
So we have a zygote
for 32-bit apps,

00:36:44.350 --> 00:36:45.869
and a zygote for 64-bit apps.

00:36:45.869 --> 00:36:47.910
What happens when you
launch an app or a service,

00:36:47.910 --> 00:36:50.580
we detect what ABI it needs,
or what ABIs it can use,

00:36:50.580 --> 00:36:54.820
and then we delegate to that
zygote to start up the app.

00:36:54.820 --> 00:36:59.610
So what that means is that, and
I'll say more about this later,

00:36:59.610 --> 00:37:01.240
is that your apps,
you're 32-bit apps

00:37:01.240 --> 00:37:05.165
are still going to work
in your 64-bit device.

00:37:05.165 --> 00:37:06.790
The other thing we
were concerned about

00:37:06.790 --> 00:37:08.100
is memory bloat.

00:37:08.100 --> 00:37:09.640
How much more
memory is this going

00:37:09.640 --> 00:37:12.410
to take having two zygotes, and
then having 64-bit references?

00:37:12.410 --> 00:37:15.215
Well in ART, we're using
compressed references.

00:37:15.215 --> 00:37:18.860
We're using just 32 bits for
object references on the heap.

00:37:18.860 --> 00:37:21.070
So that mitigates
some of this concern.

00:37:25.000 --> 00:37:29.990
So the cool thing about
this is that if you

00:37:29.990 --> 00:37:33.230
have an application that's just
running on ART, that's just

00:37:33.230 --> 00:37:38.050
dex code written in Java or
whatever, it'll just work.

00:37:38.050 --> 00:37:40.780
As long as there's no native
code you can download that app,

00:37:40.780 --> 00:37:44.870
and if you have a 64-bit device,
it'll run on the 64-bit VM.

00:37:44.870 --> 00:37:49.020
That's 85% of apps in Play
Store that are immediately

00:37:49.020 --> 00:37:50.632
64-bit ready.

00:37:50.632 --> 00:37:52.090
Developers don't
need to recompile,

00:37:52.090 --> 00:37:55.270
they don't need to upload
a new version or whatever.

00:37:55.270 --> 00:37:58.112
It just works, and it's
ready for you for free

00:37:58.112 --> 00:37:59.320
when you get your new device.

00:37:59.320 --> 00:38:01.420
So another performance
boosting thing

00:38:01.420 --> 00:38:06.650
that ART and L and our SSE
partners are bringing you.

00:38:06.650 --> 00:38:09.240
The other thing is that
we'll be shipping NDK support

00:38:09.240 --> 00:38:12.219
And RenderScript support
for 64-bit with L as well.

00:38:12.219 --> 00:38:14.760
So you can take advantage of
all that other compute intensive

00:38:14.760 --> 00:38:15.990
stuff as well.

00:38:19.000 --> 00:38:21.180
OK, so to learn more
about ART, there's

00:38:21.180 --> 00:38:23.560
a couple of articles you should
check out on android.com.

00:38:23.560 --> 00:38:24.990
The first one,
"Introducing ART,"

00:38:24.990 --> 00:38:28.100
covers some of the same ground
that we've covered here.

00:38:28.100 --> 00:38:30.240
The second one, "Verifying
App Behavior on ART,"

00:38:30.240 --> 00:38:31.820
goes into more
detail on how ART can

00:38:31.820 --> 00:38:33.977
help you find bugs
in your application.

00:38:33.977 --> 00:38:35.810
Especially around J&amp;I,
you should definitely

00:38:35.810 --> 00:38:37.730
check that out.

00:38:37.730 --> 00:38:41.870
And with that, we'll
take questions.

00:38:41.870 --> 00:38:42.370
Thank you.

00:38:52.647 --> 00:38:53.730
AUDIENCE: Can I start off?

00:38:53.730 --> 00:38:54.970
ANWAR GHULOUM: Yeah, go for it.

00:38:54.970 --> 00:38:55.260
AUDIENCE: Cool.

00:38:55.260 --> 00:38:56.780
So I was wondering
how ART is going

00:38:56.780 --> 00:38:59.530
to jive with byte code injection
that might happen right

00:38:59.530 --> 00:39:02.377
after compilation
or even at runtime.

00:39:02.377 --> 00:39:03.376
ANWAR GHULOUM: Go ahead.

00:39:03.376 --> 00:39:04.209
BRIAN CARLSTROM: No.

00:39:04.209 --> 00:39:06.374
I'll let you do it.

00:39:06.374 --> 00:39:07.540
IAN ROGERS: So, maybe I'll--

00:39:07.540 --> 00:39:08.440
ANWAR GHULOUM: Oh, yeah.

00:39:08.440 --> 00:39:09.065
IAN ROGERS: OK.

00:39:09.065 --> 00:39:12.770
So the model that Dalvik
has and ART continues

00:39:12.770 --> 00:39:16.990
is that for class loaders,
we have to have everything

00:39:16.990 --> 00:39:20.140
that the class loader
has backed up by a file.

00:39:20.140 --> 00:39:24.570
So Dalvik never had supports
for the kind of doing end memory

00:39:24.570 --> 00:39:27.330
injection of
instructions, and so on.

00:39:27.330 --> 00:39:29.130
If you have a file on
the disk, then this

00:39:29.130 --> 00:39:30.740
is something we can do
ahead of time compilation

00:39:30.740 --> 00:39:32.670
for and put into our
cache so that we're not

00:39:32.670 --> 00:39:34.710
regenerating it all of the time.

00:39:34.710 --> 00:39:37.900
So basically, it works the
same way as with Dalvik.

00:39:37.900 --> 00:39:39.562
There might be a
little bit more time

00:39:39.562 --> 00:39:41.020
when we're doing
the compilation so

00:39:41.020 --> 00:39:45.426
that you might notice something
at the initial startup, a pause

00:39:45.426 --> 00:39:47.050
at startup, because
of the compilation.

00:39:47.050 --> 00:39:50.170
But our compilation times are
really good and really fast, so

00:39:50.170 --> 00:39:52.072
hopefully imperceptible.

00:39:52.072 --> 00:39:53.030
AUDIENCE: Cool, thanks.

00:39:56.208 --> 00:39:57.570
ANWAR GHULOUM: Go ahead.

00:39:57.570 --> 00:39:59.270
AUDIENCE: What's the
effect on the boot

00:39:59.270 --> 00:40:01.680
time compared to Dalvik?

00:40:01.680 --> 00:40:05.550
ANWAR GHULOUM: So first boot
when we're compiling things,

00:40:05.550 --> 00:40:06.770
things can slow down a bit.

00:40:06.770 --> 00:40:09.460
Compile time takes a bit
longer, and the scale things

00:40:09.460 --> 00:40:11.680
we're really talking
about is an app

00:40:11.680 --> 00:40:15.880
goes from taking one
second to DEX-HOP to say,

00:40:15.880 --> 00:40:17.454
2 and 1/2, 3 seconds to DEX-HOP.

00:40:17.454 --> 00:40:19.120
But if you're doing
that over many apps.

00:40:19.120 --> 00:40:21.230
it adds up.

00:40:21.230 --> 00:40:23.760
That's just at first boot,
or after an OTA, which

00:40:23.760 --> 00:40:26.340
are relatively
infrequent events.

00:40:26.340 --> 00:40:30.900
In general, we think
boot time will be faster.

00:40:30.900 --> 00:40:32.610
The one mitigating
thing though is

00:40:32.610 --> 00:40:34.901
because we're doing COMPAxiON
boot time, to really make

00:40:34.901 --> 00:40:36.420
the heap more efficient.

00:40:36.420 --> 00:40:38.024
There is potential
to extend things.

00:40:38.024 --> 00:40:39.940
Although recently, I
think a patch went up too

00:40:39.940 --> 00:40:41.436
to optimize that
down, it'll only

00:40:41.436 --> 00:40:43.060
do the COMPAxiON sort
of toward the end

00:40:43.060 --> 00:40:44.580
so we're not
extending boot time.

00:40:44.580 --> 00:40:47.030
But code should be faster,
starting up system services

00:40:47.030 --> 00:40:48.250
and so on should be faster.

00:40:48.250 --> 00:40:49.625
And things should
just be faster.

00:40:49.625 --> 00:40:51.680
AUDIENCE: Thank you.

