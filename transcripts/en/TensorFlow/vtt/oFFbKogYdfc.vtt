WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.276
[MUSIC PLAYING]

00:00:06.552 --> 00:00:09.356
KARMEL ALLISON: Hi, and
welcome to Coding TensorFlow.

00:00:09.356 --> 00:00:11.230
I'm Karmel Allison, and
I'm here to guide you

00:00:11.230 --> 00:00:14.740
through a scenario using
TensorFlow's high-level APIs.

00:00:14.740 --> 00:00:17.810
This video is the first
in a three-part series.

00:00:17.810 --> 00:00:19.900
In this, we'll look at
data and, in particular,

00:00:19.900 --> 00:00:22.654
how to prepare and load your
data for machine learning.

00:00:22.654 --> 00:00:24.820
The rest of the series is
available on this channel,

00:00:24.820 --> 00:00:28.060
so don't forget to hit
that subscribe button.

00:00:28.060 --> 00:00:31.180
Building a machine learning
model is a multi-stage process.

00:00:31.180 --> 00:00:34.510
You have to collect, clean, and
process your data, prototype

00:00:34.510 --> 00:00:36.460
and iterate on your
model architecture,

00:00:36.460 --> 00:00:39.130
train and evaluate
results, prepare your model

00:00:39.130 --> 00:00:41.139
for production
serving, and then, you

00:00:41.139 --> 00:00:43.180
have to do it all over
again because the model is

00:00:43.180 --> 00:00:47.150
a living thing that will have
to be updated and improved.

00:00:47.150 --> 00:00:50.300
TensorFlow high-level APIs
aim to help you at each stage

00:00:50.300 --> 00:00:51.830
of your model's lifecycle--

00:00:51.830 --> 00:00:54.320
from the beginning of an
idea to training and serving

00:00:54.320 --> 00:00:56.600
large-scale applications.

00:00:56.600 --> 00:00:59.510
In this series, I will
walk through the key steps

00:00:59.510 --> 00:01:01.850
in developing a
machine learning model

00:01:01.850 --> 00:01:04.701
and show you what TensorFlow
provides for you at each step.

00:01:04.701 --> 00:01:06.950
And then, I'll also cover
some of the new developments

00:01:06.950 --> 00:01:08.366
that we are working
on to continue

00:01:08.366 --> 00:01:11.055
to improve your workflow.

00:01:11.055 --> 00:01:12.180
We start with the problem--

00:01:12.180 --> 00:01:13.770
an associated data set.

00:01:13.770 --> 00:01:16.500
We will use the covertype
data set from the US Forestry

00:01:16.500 --> 00:01:18.930
Service and Colorado
State University which

00:01:18.930 --> 00:01:22.110
has about 500,000 rows of
geophysical data collected

00:01:22.110 --> 00:01:24.929
from particular regions
in National Forest areas.

00:01:24.929 --> 00:01:26.970
We are going to use the
features in this data set

00:01:26.970 --> 00:01:30.932
to try to predict the soil type
that was found in each region.

00:01:30.932 --> 00:01:33.390
And there are a mix of features
that we'll be working with.

00:01:33.390 --> 00:01:34.530
Some are real values--

00:01:34.530 --> 00:01:37.290
elevation, slope,
aspect, and so on.

00:01:37.290 --> 00:01:39.510
Some are real values
that have been binned

00:01:39.510 --> 00:01:42.420
into an 8-bit scale, and
some are categorical values

00:01:42.420 --> 00:01:45.090
that assign integers to soil
types and wilderness area

00:01:45.090 --> 00:01:46.320
names.

00:01:46.320 --> 00:01:48.660
If we inspect the first
couple rows of our data,

00:01:48.660 --> 00:01:50.020
this is what we see--

00:01:50.020 --> 00:01:54.350
integers, no header, so we have
to work from the Info file.

00:01:54.350 --> 00:01:57.470
OK, so here we can see that we
have some of our real values,

00:01:57.470 --> 00:01:59.470
and it looks like some
of the categorical values

00:01:59.470 --> 00:02:02.890
are one-hot encoded, and
some are just categories.

00:02:02.890 --> 00:02:04.570
Some features span
multiple cells,

00:02:04.570 --> 00:02:06.820
so we'll have to handle that.

00:02:06.820 --> 00:02:07.930
Where do we start?

00:02:07.930 --> 00:02:09.857
What's the first thing
we should do here?

00:02:09.857 --> 00:02:12.190
I'm going to suggest to you
that when you're prototyping

00:02:12.190 --> 00:02:15.190
a new model in TensorFlow, the
very first thing you should do

00:02:15.190 --> 00:02:16.930
is enable eager execution.

00:02:16.930 --> 00:02:17.720
It's simple.

00:02:17.720 --> 00:02:20.350
You just add a single line
after importing TensorFlow,

00:02:20.350 --> 00:02:21.920
and you're good to go.

00:02:21.920 --> 00:02:24.370
The way it does that is
rather than deferring

00:02:24.370 --> 00:02:28.150
execution of your TensorFlow
graph, it runs ops immediately.

00:02:28.150 --> 00:02:30.790
The result is that you can
write your models in eager

00:02:30.790 --> 00:02:32.501
while you're experimenting
and iterating,

00:02:32.501 --> 00:02:34.750
but you still get the full
benefit of TensorFlow graph

00:02:34.750 --> 00:02:37.120
execution when it comes
time to train and deploy

00:02:37.120 --> 00:02:38.352
your model at scale.

00:02:38.352 --> 00:02:40.060
The first thing we're
going to want to do

00:02:40.060 --> 00:02:43.000
is load our data in and process
the data in columns so that we

00:02:43.000 --> 00:02:44.800
can feed it into a model.

00:02:44.800 --> 00:02:48.550
The data is a CSV file with
55 columns of integers.

00:02:48.550 --> 00:02:50.930
We'll go over each of
those in detail in a bit,

00:02:50.930 --> 00:02:53.147
but first we will use
the TensorFlow CSV data

00:02:53.147 --> 00:02:55.417
set to load our data from disk.

00:02:55.417 --> 00:02:58.000
This particular data set doesn't
have a header, but if it did,

00:02:58.000 --> 00:03:01.190
we could process that as
well with the CSV data set.

00:03:01.190 --> 00:03:03.470
Now, a TensorFlow data
set is similar to a NumPy

00:03:03.470 --> 00:03:06.640
array or a Pandas DataFrame
in that it reads and processes

00:03:06.640 --> 00:03:07.330
data.

00:03:07.330 --> 00:03:09.850
But instead of being optimized
for in-memory analysis,

00:03:09.850 --> 00:03:12.700
it is designed to take data,
run the set of operations

00:03:12.700 --> 00:03:14.590
that are necessary to
process and consume

00:03:14.590 --> 00:03:16.240
that data for training.

00:03:16.240 --> 00:03:19.740
Here, we are telling TensorFlow
to read our data from disk,

00:03:19.740 --> 00:03:22.570
parse the CSV, and
process the incoming data

00:03:22.570 --> 00:03:25.150
as a vector of 55 integers.

00:03:25.150 --> 00:03:27.800
Because we are running with
eager execution enabled,

00:03:27.800 --> 00:03:30.890
our data set here does
already represent our data,

00:03:30.890 --> 00:03:33.580
and we can even check to
see what each row currently

00:03:33.580 --> 00:03:34.600
looks like.

00:03:34.600 --> 00:03:37.960
If we take the first row, we can
see that right now, each row is

00:03:37.960 --> 00:03:39.940
a tuple of 55 integer tensors--

00:03:39.940 --> 00:03:41.920
not yet processed,
batched, or even split

00:03:41.920 --> 00:03:44.170
into features and labels.

00:03:44.170 --> 00:03:46.570
So we have tuples
of 55 integers,

00:03:46.570 --> 00:03:49.390
but we want our data to reflect
the structure of the data we

00:03:49.390 --> 00:03:50.680
know is in there.

00:03:50.680 --> 00:03:52.450
For that, we can
write a function

00:03:52.450 --> 00:03:55.090
to apply to our
data set row by row.

00:03:55.090 --> 00:03:56.840
This function will
take in the tuple

00:03:56.840 --> 00:04:00.190
of 55 integers in each row.

00:04:00.190 --> 00:04:04.150
A data set is expected to return
tuples of features and labels.

00:04:04.150 --> 00:04:07.000
So our goal with each
row is to parse the row

00:04:07.000 --> 00:04:10.000
and return the set of features
we care about plus a class

00:04:10.000 --> 00:04:11.240
label.

00:04:11.240 --> 00:04:13.210
So what needs to
go in-between here?

00:04:13.210 --> 00:04:15.430
This function is going
to be applied at runtime

00:04:15.430 --> 00:04:18.279
to each row of data, but it
will be applied efficiently

00:04:18.279 --> 00:04:20.079
by TensorFlow data sets.

00:04:20.079 --> 00:04:22.330
So this is a good place
to put things like image

00:04:22.330 --> 00:04:24.190
processing or
adding random noise

00:04:24.190 --> 00:04:26.170
or other special
transformations.

00:04:26.170 --> 00:04:28.870
In our case, we will handle
most of our transformations

00:04:28.870 --> 00:04:32.180
using feature columns which
I will explain more in a bit,

00:04:32.180 --> 00:04:33.970
so our main goal in
the parsing function

00:04:33.970 --> 00:04:36.370
is to make sure we
correctly separate and group

00:04:36.370 --> 00:04:38.760
our columns of features.

00:04:38.760 --> 00:04:42.500
So for example, if you read over
the details of the data set,

00:04:42.500 --> 00:04:45.260
you will see that the soil type
is a categorical feature that

00:04:45.260 --> 00:04:46.940
is one-hot encoded.

00:04:46.940 --> 00:04:49.640
It is spread out over
40 of our integers.

00:04:49.640 --> 00:04:53.180
We combine those here into
a single length-40 tensor

00:04:53.180 --> 00:04:56.210
so that we can learn soil type
as a single feature rather than

00:04:56.210 --> 00:04:58.340
40 independent features.

00:04:58.340 --> 00:05:00.227
Then we can combine
the soil-type tensor

00:05:00.227 --> 00:05:02.060
with the other features
which are spread out

00:05:02.060 --> 00:05:05.240
over the set of 55 columns
in the original data set.

00:05:05.240 --> 00:05:07.400
We can splice the tuple
of incoming values

00:05:07.400 --> 00:05:09.200
to make sure we get
everything we need.

00:05:09.200 --> 00:05:11.836
And then we zip those up with
human-readable column names

00:05:11.836 --> 00:05:13.460
to get a dictionary
of features that we

00:05:13.460 --> 00:05:15.590
can process further later.

00:05:15.590 --> 00:05:18.800
Finally, we convert our one
hot-encoded wilderness area

00:05:18.800 --> 00:05:22.790
class into a class label
that is in the range 0 to 3.

00:05:22.790 --> 00:05:24.290
We could leave them
one-hot encoded

00:05:24.290 --> 00:05:26.660
as well, and for some
model architectures or loss

00:05:26.660 --> 00:05:29.300
calculations that
might be preferable.

00:05:29.300 --> 00:05:32.750
And that gives us features
and a label for each row.

00:05:32.750 --> 00:05:35.390
We then map this function
to our data row-wise,

00:05:35.390 --> 00:05:38.690
and then we batch the rows
into sets of 64 examples.

00:05:38.690 --> 00:05:40.640
Using TensorFlow
data sets here allows

00:05:40.640 --> 00:05:42.860
us to take advantage of
many built-in performance

00:05:42.860 --> 00:05:45.110
optimizations that
data sets provide

00:05:45.110 --> 00:05:46.730
for this type of
mapping and batching

00:05:46.730 --> 00:05:48.959
to help remove I/O bottlenecks.

00:05:48.959 --> 00:05:51.500
There are many other tricks for
I/O performance optimization,

00:05:51.500 --> 00:05:53.720
depending on your system,
that we won't cover here,

00:05:53.720 --> 00:05:56.930
but a guide is included
in the description below.

00:05:56.930 --> 00:05:58.850
Because we're using
eager execution,

00:05:58.850 --> 00:06:01.580
we can check to see what our
data looks like after this,

00:06:01.580 --> 00:06:04.280
and you can see that now we
have parsed dictionaries of ints

00:06:04.280 --> 00:06:06.230
with nice human-readable names.

00:06:06.230 --> 00:06:07.820
Each feature has been batched.

00:06:07.820 --> 00:06:11.382
So a feature that is a single
number is a length-64 tensor,

00:06:11.382 --> 00:06:13.340
and we can see that our
conversion of soil type

00:06:13.340 --> 00:06:16.940
results in a tensor with
a shape of 64 by 40.

00:06:16.940 --> 00:06:19.550
We can also see that we have
a single tensor for the class

00:06:19.550 --> 00:06:23.240
labels which has the
category indices as expected.

00:06:23.240 --> 00:06:25.190
Just to keep our eyes
on the big picture here,

00:06:25.190 --> 00:06:26.640
let's see where we are.

00:06:26.640 --> 00:06:28.280
We've taken our
raw data and put it

00:06:28.280 --> 00:06:30.950
into a TensorFlow data set
that generates dictionaries

00:06:30.950 --> 00:06:33.230
of feature tensors and labels.

00:06:33.230 --> 00:06:35.240
But something is still
wrong with the integers

00:06:35.240 --> 00:06:36.800
we have as features here.

00:06:36.800 --> 00:06:39.380
Anyone care to venture a guess?

00:06:39.380 --> 00:06:41.750
We have lots of feature
types-- some are continuous,

00:06:41.750 --> 00:06:44.510
some are categorical,
some are one-hot encoded.

00:06:44.510 --> 00:06:46.250
We need to represent
these in a way that

00:06:46.250 --> 00:06:48.544
is meaningful to an ML model.

00:06:48.544 --> 00:06:50.210
You'll see how to fix
that using feature

00:06:50.210 --> 00:06:53.150
columns in part two of this
series right here on YouTube.

00:06:53.150 --> 00:06:55.070
So don't forget to hit
that subscribe button

00:06:55.070 --> 00:06:56.028
and I'll see you there.

00:06:56.028 --> 00:06:59.260
[MUSIC PLAYING]

