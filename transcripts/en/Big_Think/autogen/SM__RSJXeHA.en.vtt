WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:04.580
 
[Music]

00:00:04.580 --> 00:00:04.590
[Music]
 

00:00:04.590 --> 00:00:06.260
[Music]
when we come to artificial intelligence

00:00:06.260 --> 00:00:06.270
when we come to artificial intelligence
 

00:00:06.270 --> 00:00:08.120
when we come to artificial intelligence
and the possibility of their becoming

00:00:08.120 --> 00:00:08.130
and the possibility of their becoming
 

00:00:08.130 --> 00:00:11.119
and the possibility of their becoming
conscious we reach a profound

00:00:11.119 --> 00:00:11.129
conscious we reach a profound
 

00:00:11.129 --> 00:00:13.490
conscious we reach a profound
philosophical difficulty

00:00:13.490 --> 00:00:13.500
philosophical difficulty
 

00:00:13.500 --> 00:00:17.269
philosophical difficulty
I am a philosophical naturalist I am

00:00:17.269 --> 00:00:17.279
I am a philosophical naturalist I am
 

00:00:17.279 --> 00:00:19.790
I am a philosophical naturalist I am
committed to the view that there is

00:00:19.790 --> 00:00:19.800
committed to the view that there is
 

00:00:19.800 --> 00:00:22.579
committed to the view that there is
nothing in our brains that violate the

00:00:22.579 --> 00:00:22.589
nothing in our brains that violate the
 

00:00:22.589 --> 00:00:23.870
nothing in our brains that violate the
laws of physics there's nothing that

00:00:23.870 --> 00:00:23.880
laws of physics there's nothing that
 

00:00:23.880 --> 00:00:27.290
laws of physics there's nothing that
could not in principle be reproduced in

00:00:27.290 --> 00:00:27.300
could not in principle be reproduced in
 

00:00:27.300 --> 00:00:30.019
could not in principle be reproduced in
technology it hasn't been done yet we're

00:00:30.019 --> 00:00:30.029
technology it hasn't been done yet we're
 

00:00:30.029 --> 00:00:31.489
technology it hasn't been done yet we're
probably quite a long way away from it

00:00:31.489 --> 00:00:31.499
probably quite a long way away from it
 

00:00:31.499 --> 00:00:34.040
probably quite a long way away from it
but I see no reason why he shouldn't

00:00:34.040 --> 00:00:34.050
but I see no reason why he shouldn't
 

00:00:34.050 --> 00:00:38.570
but I see no reason why he shouldn't
reach the point where a human-made robot

00:00:38.570 --> 00:00:38.580
reach the point where a human-made robot
 

00:00:38.580 --> 00:00:42.320
reach the point where a human-made robot
is capable of consciousness and a

00:00:42.320 --> 00:00:42.330
is capable of consciousness and a
 

00:00:42.330 --> 00:00:45.110
is capable of consciousness and a
feeling pain we can feel pain why

00:00:45.110 --> 00:00:45.120
feeling pain we can feel pain why
 

00:00:45.120 --> 00:00:48.620
feeling pain we can feel pain why
shouldn't they and this is profoundly

00:00:48.620 --> 00:00:48.630
shouldn't they and this is profoundly
 

00:00:48.630 --> 00:00:50.150
shouldn't they and this is profoundly
disturbing because it kind of goes

00:00:50.150 --> 00:00:50.160
disturbing because it kind of goes
 

00:00:50.160 --> 00:00:52.400
disturbing because it kind of goes
against the grain to think that that a

00:00:52.400 --> 00:00:52.410
against the grain to think that that a
 

00:00:52.410 --> 00:00:57.220
against the grain to think that that a
machine made of metal and silicon chips

00:00:57.220 --> 00:00:57.230
machine made of metal and silicon chips
 

00:00:57.230 --> 00:01:00.230
machine made of metal and silicon chips
could feel pain but I don't see why they

00:01:00.230 --> 00:01:00.240
could feel pain but I don't see why they
 

00:01:00.240 --> 00:01:02.960
could feel pain but I don't see why they
would not and so that this moral

00:01:02.960 --> 00:01:02.970
would not and so that this moral
 

00:01:02.970 --> 00:01:05.469
would not and so that this moral
consideration of how to treat

00:01:05.469 --> 00:01:05.479
consideration of how to treat
 

00:01:05.479 --> 00:01:07.340
consideration of how to treat
artificially artificially intelligent

00:01:07.340 --> 00:01:07.350
artificially artificially intelligent
 

00:01:07.350 --> 00:01:11.389
artificially artificially intelligent
robots will arise in the future and it's

00:01:11.389 --> 00:01:11.399
robots will arise in the future and it's
 

00:01:11.399 --> 00:01:13.999
robots will arise in the future and it's
a problem which philosophers and moral

00:01:13.999 --> 00:01:14.009
a problem which philosophers and moral
 

00:01:14.009 --> 00:01:16.010
a problem which philosophers and moral
philosophers are already talking about

00:01:16.010 --> 00:01:16.020
philosophers are already talking about
 

00:01:16.020 --> 00:01:18.800
philosophers are already talking about
once again and I'm committed to the view

00:01:18.800 --> 00:01:18.810
once again and I'm committed to the view
 

00:01:18.810 --> 00:01:20.450
once again and I'm committed to the view
that this is possible I'm committed to

00:01:20.450 --> 00:01:20.460
that this is possible I'm committed to
 

00:01:20.460 --> 00:01:21.709
that this is possible I'm committed to
the view that anything that a human

00:01:21.709 --> 00:01:21.719
the view that anything that a human
 

00:01:21.719 --> 00:01:23.779
the view that anything that a human
brain can do can be replicated in

00:01:23.779 --> 00:01:23.789
brain can do can be replicated in
 

00:01:23.789 --> 00:01:30.130
brain can do can be replicated in
silicon and so I'm sympathetic to the

00:01:30.130 --> 00:01:30.140
silicon and so I'm sympathetic to the
 

00:01:30.140 --> 00:01:32.090
silicon and so I'm sympathetic to the
misgivings that have been expressed by

00:01:32.090 --> 00:01:32.100
misgivings that have been expressed by
 

00:01:32.100 --> 00:01:34.520
misgivings that have been expressed by
highly respected figures like Elon Musk

00:01:34.520 --> 00:01:34.530
highly respected figures like Elon Musk
 

00:01:34.530 --> 00:01:36.410
highly respected figures like Elon Musk
and Stephen Hawking

00:01:36.410 --> 00:01:36.420
and Stephen Hawking
 

00:01:36.420 --> 00:01:39.499
and Stephen Hawking
that we ought to be worried that on the

00:01:39.499 --> 00:01:39.509
that we ought to be worried that on the
 

00:01:39.509 --> 00:01:40.910
that we ought to be worried that on the
precautionary principle we should worry

00:01:40.910 --> 00:01:40.920
precautionary principle we should worry
 

00:01:40.920 --> 00:01:47.209
precautionary principle we should worry
about a takeover perhaps even by robots

00:01:47.209 --> 00:01:47.219
about a takeover perhaps even by robots
 

00:01:47.219 --> 00:01:50.109
about a takeover perhaps even by robots
by our own creation especially if they

00:01:50.109 --> 00:01:50.119
by our own creation especially if they
 

00:01:50.119 --> 00:01:53.539
by our own creation especially if they
reproduce themselves and potentially

00:01:53.539 --> 00:01:53.549
reproduce themselves and potentially
 

00:01:53.549 --> 00:01:56.989
reproduce themselves and potentially
even evolved by reproduction and don't

00:01:56.989 --> 00:01:56.999
even evolved by reproduction and don't
 

00:01:56.999 --> 00:02:00.020
even evolved by reproduction and don't
need us anymore this is a a

00:02:00.020 --> 00:02:00.030
need us anymore this is a a
 

00:02:00.030 --> 00:02:02.809
need us anymore this is a a
science-fiction speculation at the

00:02:02.809 --> 00:02:02.819
science-fiction speculation at the
 

00:02:02.819 --> 00:02:04.910
science-fiction speculation at the
moment but I think philosophically I'm

00:02:04.910 --> 00:02:04.920
moment but I think philosophically I'm
 

00:02:04.920 --> 00:02:06.399
moment but I think philosophically I'm
committed to the view that it is

00:02:06.399 --> 00:02:06.409
committed to the view that it is
 

00:02:06.409 --> 00:02:11.180
committed to the view that it is
possible and like any major advance we

00:02:11.180 --> 00:02:11.190
possible and like any major advance we
 

00:02:11.190 --> 00:02:12.110
possible and like any major advance we
need to apply the precautionary

00:02:12.110 --> 00:02:12.120
need to apply the precautionary
 

00:02:12.120 --> 00:02:13.970
need to apply the precautionary
principle and ask ourselves what the

00:02:13.970 --> 00:02:13.980
principle and ask ourselves what the
 

00:02:13.980 --> 00:02:14.740
principle and ask ourselves what the
consequence

00:02:14.740 --> 00:02:14.750
consequence
 

00:02:14.750 --> 00:02:19.960
consequence
is might be it could be said that the

00:02:19.960 --> 00:02:19.970
is might be it could be said that the
 

00:02:19.970 --> 00:02:22.180
is might be it could be said that the
sum of not human happiness but the sum

00:02:22.180 --> 00:02:22.190
sum of not human happiness but the sum
 

00:02:22.190 --> 00:02:25.720
sum of not human happiness but the sum
of sentient being happiness might be

00:02:25.720 --> 00:02:25.730
of sentient being happiness might be
 

00:02:25.730 --> 00:02:28.930
of sentient being happiness might be
improved they might make a better job do

00:02:28.930 --> 00:02:28.940
improved they might make a better job do
 

00:02:28.940 --> 00:02:31.120
improved they might make a better job do
a better job of running the world than

00:02:31.120 --> 00:02:31.130
a better job of running the world than
 

00:02:31.130 --> 00:02:32.820
a better job of running the world than
we are certainly than we are at present

00:02:32.820 --> 00:02:32.830
we are certainly than we are at present
 

00:02:32.830 --> 00:02:35.590
we are certainly than we are at present
and so perhaps it might be the bad thing

00:02:35.590 --> 00:02:35.600
and so perhaps it might be the bad thing
 

00:02:35.600 --> 00:02:38.699
and so perhaps it might be the bad thing
if we went extinct and our our

00:02:38.699 --> 00:02:38.709
if we went extinct and our our
 

00:02:38.709 --> 00:02:41.650
if we went extinct and our our
civilization our the memory of

00:02:41.650 --> 00:02:41.660
civilization our the memory of
 

00:02:41.660 --> 00:02:44.050
civilization our the memory of
Shakespeare and Beethoven and

00:02:44.050 --> 00:02:44.060
Shakespeare and Beethoven and
 

00:02:44.060 --> 00:02:48.250
Shakespeare and Beethoven and
Michelangelo persisted in silico rather

00:02:48.250 --> 00:02:48.260
Michelangelo persisted in silico rather
 

00:02:48.260 --> 00:02:53.620
Michelangelo persisted in silico rather
than in brains and and our form of life

00:02:53.620 --> 00:02:53.630
than in brains and and our form of life
 

00:02:53.630 --> 00:02:56.020
than in brains and and our form of life
and one could foresee at future time

00:02:56.020 --> 00:02:56.030
and one could foresee at future time
 

00:02:56.030 --> 00:03:00.370
and one could foresee at future time
when silicon beings look back on a dawn

00:03:00.370 --> 00:03:00.380
when silicon beings look back on a dawn
 

00:03:00.380 --> 00:03:03.540
when silicon beings look back on a dawn
age when when the earth was peopled by

00:03:03.540 --> 00:03:03.550
age when when the earth was peopled by
 

00:03:03.550 --> 00:03:09.750
age when when the earth was peopled by
soft squishy watery organic beings and

00:03:09.750 --> 00:03:09.760
soft squishy watery organic beings and
 

00:03:09.760 --> 00:03:12.790
soft squishy watery organic beings and
who knows that might be better but we're

00:03:12.790 --> 00:03:12.800
who knows that might be better but we're
 

00:03:12.800 --> 00:03:14.470
who knows that might be better but we're
really in the science fiction territory

00:03:14.470 --> 00:03:14.480
really in the science fiction territory
 

00:03:14.480 --> 00:03:16.300
really in the science fiction territory
now

00:03:16.300 --> 00:03:16.310
now
 

00:03:16.310 --> 00:03:19.550
now
[Music]

