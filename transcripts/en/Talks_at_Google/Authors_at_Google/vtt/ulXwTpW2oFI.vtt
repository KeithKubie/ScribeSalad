WEBVTT
Kind: captions
Language: en

00:00:02.952 --> 00:00:04.097
[APPLAUSE]

00:00:04.097 --> 00:00:05.680
RICHARD SUSSKIND:
Thank you very much,

00:00:05.680 --> 00:00:07.138
and thank you,
Google, for inviting

00:00:07.138 --> 00:00:09.880
us to talk about our book about
the future of the professions,

00:00:09.880 --> 00:00:13.810
which really focuses on
the impact of technology

00:00:13.810 --> 00:00:16.597
and how we share
expertise in society.

00:00:16.597 --> 00:00:18.680
And Daniel and I will be
doing a double act today.

00:00:18.680 --> 00:00:23.150
I'll be talking after Daniel
about trends in technology

00:00:23.150 --> 00:00:24.890
and the evolution of
professional service

00:00:24.890 --> 00:00:26.390
and artificial intelligence.

00:00:26.390 --> 00:00:29.040
Before that, though, Daniel
will be setting the scene,

00:00:29.040 --> 00:00:32.510
discussing what we regard as
the two possible futures that

00:00:32.510 --> 00:00:34.010
are panning out for
the professions,

00:00:34.010 --> 00:00:35.759
and he'll also give
you a flavor of what's

00:00:35.759 --> 00:00:39.540
happening in the vanguard
of the professions today.

00:00:39.540 --> 00:00:42.600
After I complete my
words on AI, Daniel

00:00:42.600 --> 00:00:45.594
will then speak about jobs
and the future of jobs.

00:00:45.594 --> 00:00:47.260
I'll come back and
speak about expertise

00:00:47.260 --> 00:00:50.262
and a couple of moral issues
that arise from our book.

00:00:50.262 --> 00:00:52.220
So we will speak to you
for about half an hour,

00:00:52.220 --> 00:00:53.422
maybe 40 minutes.

00:00:53.422 --> 00:00:55.755
And we'll cover these main
topics as snappily as we can.

00:00:55.755 --> 00:00:56.254
Thank you.

00:01:01.550 --> 00:01:04.110
DANIEL SUSSKIND: So we're often
asked why we wrote the book.

00:01:04.110 --> 00:01:06.300
What can I say about a
co-author who, in many ways,

00:01:06.300 --> 00:01:10.080
has become like a father to me.

00:01:10.080 --> 00:01:11.550
My dad's been
working in the field

00:01:11.550 --> 00:01:15.310
of artificial intelligence and
the law for the past 35 years,

00:01:15.310 --> 00:01:20.040
looking at how it all
transforms the legal profession.

00:01:20.040 --> 00:01:22.920
And what he's found is
that in talking, generally,

00:01:22.920 --> 00:01:26.080
to audiences of lawyers,
at the end, a stray doctor,

00:01:26.080 --> 00:01:28.656
a stray accountant, a
stray teacher will come up

00:01:28.656 --> 00:01:31.030
and say, that's very interesting
in the legal profession,

00:01:31.030 --> 00:01:33.560
but what you're talking
about could apply very well

00:01:33.560 --> 00:01:36.030
in our profession as well.

00:01:36.030 --> 00:01:39.701
So we were first chatting about
this about five years ago.

00:01:39.701 --> 00:01:41.700
I was working in a policy
unit at Downing Street

00:01:41.700 --> 00:01:44.075
at the time, tax policy, health
policy, education policy,

00:01:44.075 --> 00:01:47.760
and I had a good overview of
lots of different professions.

00:01:47.760 --> 00:01:49.769
And it was clear that
significant change was

00:01:49.769 --> 00:01:52.060
in the air, and that these
different professions seemed

00:01:52.060 --> 00:01:55.040
to be facing a common
set of challenges.

00:01:55.040 --> 00:01:58.020
So we had the idea
of investigating

00:01:58.020 --> 00:01:59.960
the professions more generally.

00:01:59.960 --> 00:02:02.770
And we set out on a journey
together to do exactly that,

00:02:02.770 --> 00:02:06.030
and the result was this book.

00:02:06.030 --> 00:02:10.430
So our research-- we looked
at eight professions-- health,

00:02:10.430 --> 00:02:13.100
education, law, journalism,
divinity, management,

00:02:13.100 --> 00:02:15.810
consulting, tax and
audit, and architecture.

00:02:15.810 --> 00:02:18.354
But the thinking applies to more
than just those professions.

00:02:18.354 --> 00:02:20.130
It's about the
professions in general.

00:02:20.130 --> 00:02:23.370
We have about 100
interviews we draw on,

00:02:23.370 --> 00:02:26.370
800, more than 100 sources.

00:02:26.370 --> 00:02:28.720
And the picture that we
get is a radical change,

00:02:28.720 --> 00:02:31.920
and our book is trying
to make sense of that.

00:02:31.920 --> 00:02:33.480
[INAUDIBLE] my dad
said before, we

00:02:33.480 --> 00:02:35.600
see two futures for
the professions.

00:02:35.600 --> 00:02:37.630
And the first one is
reassuringly familiar.

00:02:37.630 --> 00:02:39.407
Professionals continue
to work as they

00:02:39.407 --> 00:02:41.365
have done since the middle
of the 19th century,

00:02:41.365 --> 00:02:43.210
but they use these
new technologies

00:02:43.210 --> 00:02:45.710
to streamline and optimize the
traditional way in which they

00:02:45.710 --> 00:02:46.426
worked.

00:02:46.426 --> 00:02:48.800
So there's lots of examples
of this-- doctors using Skype

00:02:48.800 --> 00:02:53.510
to talk to patients, architects
using computer-assisted design

00:02:53.510 --> 00:02:56.390
technology to build
more complicated,

00:02:56.390 --> 00:02:59.758
bigger buildings, teachers
using online resources to help

00:02:59.758 --> 00:03:01.392
in the classroom.

00:03:01.392 --> 00:03:03.600
But the second future's a
very different proposition.

00:03:03.600 --> 00:03:06.150
It involves a
transformation in the way

00:03:06.150 --> 00:03:07.640
that the expertise
of professionals

00:03:07.640 --> 00:03:09.590
is made available.

00:03:09.590 --> 00:03:11.760
And what we call
increasingly capable

00:03:11.760 --> 00:03:15.100
systems and machines will not
only streamline and optimize

00:03:15.100 --> 00:03:17.160
the traditional ways in
which professions work,

00:03:17.160 --> 00:03:19.275
but it will actively
displace work

00:03:19.275 --> 00:03:22.590
in traditional professions.

00:03:22.590 --> 00:03:25.644
And for now, we see these two
futures developing in parallel,

00:03:25.644 --> 00:03:27.060
but in the long
run, we anticipate

00:03:27.060 --> 00:03:30.010
that that second
future will dominate.

00:03:30.010 --> 00:03:33.230
We'll find new and better ways
to share expertise in society,

00:03:33.230 --> 00:03:35.790
and that will lead to
a gradual dismantling

00:03:35.790 --> 00:03:38.165
of the traditional professions.

00:03:38.165 --> 00:03:40.150
And that's broadly where
the latest evidence

00:03:40.150 --> 00:03:42.680
and our thinking leads us.

00:03:42.680 --> 00:03:44.650
But our research
also led us to ask

00:03:44.650 --> 00:03:47.280
a more fundamental
question, which is why do we

00:03:47.280 --> 00:03:49.386
have the professions at all?

00:03:49.386 --> 00:03:51.386
And our answer, and it
runs through all the work

00:03:51.386 --> 00:03:53.650
that we've done is,
that in analogous ways,

00:03:53.650 --> 00:03:58.340
the professions are a solution
to the same problem, which

00:03:58.340 --> 00:04:01.560
is that none of us have
sufficient expertise

00:04:01.560 --> 00:04:05.170
to cope with all the
daily challenges in life.

00:04:05.170 --> 00:04:06.540
No one can know everything.

00:04:06.540 --> 00:04:08.580
Human beings, the
term we use for this,

00:04:08.580 --> 00:04:10.110
they have limited understanding.

00:04:10.110 --> 00:04:12.980
And so they turn to
doctors, lawyers, teachers,

00:04:12.980 --> 00:04:16.519
who have the knowledge
that they don't to get on.

00:04:16.519 --> 00:04:19.329
In what we call a
brick-based society,

00:04:19.329 --> 00:04:20.980
the professions, as
I said, are the way

00:04:20.980 --> 00:04:23.460
that we solve these
daily challenges.

00:04:23.460 --> 00:04:26.380
They have knowledge, experience,
skills, and know-how.

00:04:26.380 --> 00:04:29.030
And the term we use for
this is practical expertise.

00:04:29.030 --> 00:04:31.110
They've got the
practical expertise

00:04:31.110 --> 00:04:33.185
but that those they help do not.

00:04:33.185 --> 00:04:34.940
They operate under
a grand bargain,

00:04:34.940 --> 00:04:37.680
which is an arrangement that
gives the professions the right

00:04:37.680 --> 00:04:39.160
to provide certain
services, often

00:04:39.160 --> 00:04:40.810
to the exclusion of others.

00:04:40.810 --> 00:04:43.070
And they act as gatekeepers.

00:04:43.070 --> 00:04:44.950
They're asked to
look offer, to curate

00:04:44.950 --> 00:04:46.658
their own respective
bodies of knowledge.

00:04:46.658 --> 00:04:48.500
The doctors look after
medical knowledge.

00:04:48.500 --> 00:04:51.530
Lawyers look after legal
knowledge, and so on.

00:04:51.530 --> 00:04:53.770
So that's our analysis
of the professions

00:04:53.770 --> 00:04:55.240
in a brick-based society.

00:04:55.240 --> 00:04:57.150
But we're no longer in
a brick-based society.

00:04:57.150 --> 00:04:59.191
We're in what we call a
technology-based internet

00:04:59.191 --> 00:05:01.930
society, and our
professions are creaking.

00:05:01.930 --> 00:05:03.180
They're unaffordable.

00:05:03.180 --> 00:05:05.710
Most people, most
organizations, can't

00:05:05.710 --> 00:05:09.320
afford the advice of
first-rate professionals,

00:05:09.320 --> 00:05:11.089
or indeed, any professionals.

00:05:11.089 --> 00:05:11.880
They're antiquated.

00:05:11.880 --> 00:05:15.060
They rely upon old techniques
for producing and sharing

00:05:15.060 --> 00:05:17.060
knowledge and information.

00:05:17.060 --> 00:05:18.020
They're opaque.

00:05:18.020 --> 00:05:20.930
And sometimes, this is because
the work the professionals do

00:05:20.930 --> 00:05:23.740
is genuinely too complex for
lay people to understand,

00:05:23.740 --> 00:05:25.910
but other times,
lots of ways there's

00:05:25.910 --> 00:05:31.420
intentional obfuscation,
and they underperform.

00:05:31.420 --> 00:05:33.550
Given the way the
professionals are organized,

00:05:33.550 --> 00:05:35.550
the work and experience
of the very best

00:05:35.550 --> 00:05:38.450
can only ever be enjoyed by
a privileged and lucky few.

00:05:38.450 --> 00:05:43.410
And the expertise of the very
best is a very finite resource.

00:05:43.410 --> 00:05:44.960
And so we should
ask the question,

00:05:44.960 --> 00:05:47.330
as we move from this
brick-based industrial society

00:05:47.330 --> 00:05:50.490
to an internet
society, might there

00:05:50.490 --> 00:05:54.290
be entirely new ways of
organizing professional work,

00:05:54.290 --> 00:05:56.470
ways that are more
affordable, more accessible

00:05:56.470 --> 00:05:58.820
than the traditional approach?

00:05:58.820 --> 00:06:01.440
Might we be able to make some
of this practical expertise

00:06:01.440 --> 00:06:03.320
available online?

00:06:03.320 --> 00:06:05.430
Do we still need
these old gatekeepers?

00:06:08.060 --> 00:06:09.900
And to help us think
through these issues,

00:06:09.900 --> 00:06:12.275
we decided to find out what
was going on at the vanguard.

00:06:12.275 --> 00:06:15.330
So we spoke to the thinkers
and leading practitioners.

00:06:15.330 --> 00:06:20.610
And I want to now just give
a flavor of what we found.

00:06:20.610 --> 00:06:24.370
So in education, more people
signed up in a single year

00:06:24.370 --> 00:06:26.250
to Harvard's online
courses than had

00:06:26.250 --> 00:06:29.840
attended Harvard in its
entire 377 year existence

00:06:29.840 --> 00:06:31.570
up to that point.

00:06:31.570 --> 00:06:35.290
Khan Academy, online collection
of instructional videos,

00:06:35.290 --> 00:06:36.300
practice problems.

00:06:36.300 --> 00:06:37.100
It's great stuff.

00:06:37.100 --> 00:06:39.370
I use it to teach some
of my students maths.

00:06:39.370 --> 00:06:42.480
It has 10 million
unique users each month.

00:06:42.480 --> 00:06:45.320
That's, in a way, a higher
effective attendance

00:06:45.320 --> 00:06:49.910
than the entire school
population in England.

00:06:49.910 --> 00:06:53.680
In medicine, 190 million people,
not all of them hypochondriacs,

00:06:53.680 --> 00:06:58.635
visit WebMD, which is an online
network of help websites.

00:06:58.635 --> 00:07:00.990
190 million visits is more
than the number of visits

00:07:00.990 --> 00:07:04.740
to all the traditional
doctors working in the US.

00:07:04.740 --> 00:07:06.530
Similarly, the US
Food and Drug Agency

00:07:06.530 --> 00:07:10.260
said, by 2018, there will be
at least 1.5 billion people

00:07:10.260 --> 00:07:12.080
in the world with at
least one medical app

00:07:12.080 --> 00:07:13.402
on their smartphone.

00:07:13.402 --> 00:07:16.000
In journalism, on
its sixth birthday,

00:07:16.000 --> 00:07:18.100
the Huffington Post had
more unique visitors

00:07:18.100 --> 00:07:22.130
than the New York Times,
which was 164 years old.

00:07:22.130 --> 00:07:25.430
Bleacher Report, an online
blog written by 2,000 people,

00:07:25.430 --> 00:07:29.010
now has 22 million
unique users each month,

00:07:29.010 --> 00:07:32.320
enough to rival CNN Sports.

00:07:32.320 --> 00:07:33.810
Associated Press
last year started

00:07:33.810 --> 00:07:36.520
to use algorithms to
computerize the production

00:07:36.520 --> 00:07:38.115
of their earnings reports.

00:07:38.115 --> 00:07:39.490
And when they did
that, they were

00:07:39.490 --> 00:07:41.690
going to produce
15 times as many

00:07:41.690 --> 00:07:44.530
as they could when they relied
upon professional financial

00:07:44.530 --> 00:07:47.170
journalists [INAUDIBLE].

00:07:47.170 --> 00:07:49.010
In the legal world,
every year on eBay

00:07:49.010 --> 00:07:50.540
there are 60 million
disputes that

00:07:50.540 --> 00:07:54.190
arise and are resolved online
without lawyers using what's

00:07:54.190 --> 00:07:56.020
called an e-mediation platform.

00:07:56.020 --> 00:08:02.156
Bear in mind, again, 60
million is 40 times as many

00:08:02.156 --> 00:08:03.530
as the number of
civil cases that

00:08:03.530 --> 00:08:05.780
are launched in the entire
English or Welsh justice

00:08:05.780 --> 00:08:07.740
system.

00:08:07.740 --> 00:08:11.076
Again, the legal world, the
best recognized legal brand

00:08:11.076 --> 00:08:12.700
in the US isn't a
traditional law firm,

00:08:12.700 --> 00:08:16.222
it's Legalzoom.com, an online
provider of legal advice

00:08:16.222 --> 00:08:19.760
and automated document
drafting service.

00:08:19.760 --> 00:08:22.730
In tax, last year in
America, 48 million Americans

00:08:22.730 --> 00:08:25.690
used online tax preparation
software rather than

00:08:25.690 --> 00:08:27.340
a traditional tax accountant.

00:08:27.340 --> 00:08:29.740
And here, HMRC, they've
got a fraud detection

00:08:29.740 --> 00:08:31.340
system called Connect.

00:08:31.340 --> 00:08:34.309
It sifts through each time
about 1 billion pieces of data,

00:08:34.309 --> 00:08:36.029
and it's said that's
more data than is

00:08:36.029 --> 00:08:37.400
held in the British Library.

00:08:37.400 --> 00:08:39.673
No mean feat, given the fact
that the British Library

00:08:39.673 --> 00:08:41.256
has a copy of every
single book that's

00:08:41.256 --> 00:08:45.330
ever been published in the UK.

00:08:45.330 --> 00:08:47.862
In architecture,
a few years ago,

00:08:47.862 --> 00:08:50.020
a firm, Gramazio and
Kohler, used a swarm

00:08:50.020 --> 00:08:52.475
of autonomous flying
robots, pictured

00:08:52.475 --> 00:08:55.950
there, to build a structure
out of 1,500 bricks.

00:08:55.950 --> 00:08:58.530
And a Dutch architecture
firm, DUS Architects,

00:08:58.530 --> 00:09:01.275
has begun to print and
assemble a house made entirely

00:09:01.275 --> 00:09:03.680
of printed parts,
using a printer that

00:09:03.680 --> 00:09:07.597
can print objects that are
three and half meters tall.

00:09:07.597 --> 00:09:09.430
In consulting, Accenture,
a consulting firm,

00:09:09.430 --> 00:09:10.971
no longer employees
just consultants.

00:09:10.971 --> 00:09:14.420
It has 750 hospital
nurses on staff.

00:09:14.420 --> 00:09:17.530
And Deloitte, which was founded
170 years ago as an audit

00:09:17.530 --> 00:09:21.630
practice, now has
200,000 professionals,

00:09:21.630 --> 00:09:25.480
and its own full scale
university set in a 700,000

00:09:25.480 --> 00:09:28.650
square foot campus in Texas.

00:09:28.650 --> 00:09:31.080
So in Second Life,
the online world where

00:09:31.080 --> 00:09:34.510
half a million people
control their own avatars,

00:09:34.510 --> 00:09:36.160
there's a group of Christians.

00:09:36.160 --> 00:09:38.700
And on an island, a virtual
island called Epiphany,

00:09:38.700 --> 00:09:42.120
they've built a
slightly gloomy-looking

00:09:42.120 --> 00:09:43.120
Anglican cathedral.

00:09:43.120 --> 00:09:47.200
It offers daily services,
a weekly Bible study class,

00:09:47.200 --> 00:09:49.650
and counseling services.

00:09:49.650 --> 00:09:53.100
And again in religion, 2011,
amidst some controversy,

00:09:53.100 --> 00:09:56.660
the Vatican granted
the first imprimatur

00:09:56.660 --> 00:10:00.479
to this app called Confession.

00:10:00.479 --> 00:10:02.020
An imprimatur is
the official license

00:10:02.020 --> 00:10:04.120
that the Catholic Church
gives to religious texts.

00:10:04.120 --> 00:10:07.470
So they gave it to
this app, which helps

00:10:07.470 --> 00:10:08.990
you prepare for confession.

00:10:08.990 --> 00:10:15.320
And it's got tools
for tracking sin,

00:10:15.320 --> 00:10:18.940
and dropdown panels and
options for contrition.

00:10:18.940 --> 00:10:22.020
And the controversy was
that the Vatican said

00:10:22.020 --> 00:10:25.157
that this will help you
prepare for confession,

00:10:25.157 --> 00:10:26.490
but it won't replace confession.

00:10:26.490 --> 00:10:30.520
And there was an argument
about that with a local church.

00:10:30.520 --> 00:10:32.480
So we believe that
all these developments

00:10:32.480 --> 00:10:34.410
across the professions
are related.

00:10:34.410 --> 00:10:36.620
They reflect a transformation
in the professions,

00:10:36.620 --> 00:10:38.510
and that's what we've
been studying together

00:10:38.510 --> 00:10:41.550
for the past five years.

00:10:41.550 --> 00:10:42.050
Thank you.

00:10:45.080 --> 00:10:46.472
RICHARD SUSSKIND:
Thank you, Dan.

00:10:46.472 --> 00:10:48.180
So when we looked
across the professions,

00:10:48.180 --> 00:10:50.940
looked at the evidence that
Daniel has discussed with you,

00:10:50.940 --> 00:10:54.070
and much else besides, we
identified eight patterns

00:10:54.070 --> 00:10:56.932
and 30 trends that we found
across the professions.

00:10:56.932 --> 00:10:58.890
And I'm not going to
delve into each one today.

00:10:58.890 --> 00:11:00.556
That's a chapter at
the end of the book.

00:11:00.556 --> 00:11:03.070
But I wanted to
just choose three

00:11:03.070 --> 00:11:04.670
to give you a
flavor of the trend

00:11:04.670 --> 00:11:06.120
level of what's happening.

00:11:06.120 --> 00:11:08.910
Above all else, I suspect
we're seeing a move away

00:11:08.910 --> 00:11:13.420
from what we called bespoke
handling of professional work.

00:11:13.420 --> 00:11:17.280
So this is professionals
acting as craftspeople,

00:11:17.280 --> 00:11:21.170
with each case, or problem,
or patient, or client

00:11:21.170 --> 00:11:24.580
having unique
circumstances, and starting

00:11:24.580 --> 00:11:28.290
with a blank sheet of paper
and fashioning a solution

00:11:28.290 --> 00:11:29.510
with a handcrafting method.

00:11:29.510 --> 00:11:30.968
We're seeing a move
away from that.

00:11:30.968 --> 00:11:34.046
We're seeing what we call
decomposition, a breaking down

00:11:34.046 --> 00:11:35.670
of often very complex
professional work

00:11:35.670 --> 00:11:39.195
into component parts, and
finding the most efficient way

00:11:39.195 --> 00:11:41.640
to do each part of work.

00:11:41.640 --> 00:11:44.940
This is a new division of labor
across professional services.

00:11:44.940 --> 00:11:46.690
And what we're also
seeing as part of this

00:11:46.690 --> 00:11:49.310
is, that many of the components,
once you start breaking down

00:11:49.310 --> 00:11:52.280
professional work, although
the whole looks complicated,

00:11:52.280 --> 00:11:54.330
many of the some parts
in medicine, in law,

00:11:54.330 --> 00:11:56.700
in tax and audit, right across
the professional services

00:11:56.700 --> 00:11:59.830
market, many of the subparts
actually can be routinized.

00:11:59.830 --> 00:12:01.580
And here's where we
can get great savings,

00:12:01.580 --> 00:12:03.870
and here's where we can
actually use technology

00:12:03.870 --> 00:12:05.430
in new and imaginative ways.

00:12:05.430 --> 00:12:09.210
So we've discuss at length
each of these trends

00:12:09.210 --> 00:12:10.500
and many others.

00:12:10.500 --> 00:12:12.254
And we're not saying
all professions are

00:12:12.254 --> 00:12:13.670
going at the same
pace, but we are

00:12:13.670 --> 00:12:15.950
saying there's a
common set of patterns

00:12:15.950 --> 00:12:17.810
visible across the professions.

00:12:17.810 --> 00:12:19.989
And technology is
clearly underpinning

00:12:19.989 --> 00:12:20.780
much of the change.

00:12:20.780 --> 00:12:24.450
Now, we're not here to
lecture Google on technology.

00:12:24.450 --> 00:12:26.470
We just wanted to make a
couple of observations.

00:12:26.470 --> 00:12:28.400
First of all, in
1996, I wrote a book

00:12:28.400 --> 00:12:29.759
called "The Future of Law."

00:12:29.759 --> 00:12:31.550
And this was about the
impact of technology

00:12:31.550 --> 00:12:33.280
on lawyers and the courts.

00:12:33.280 --> 00:12:36.300
And in that book-- and this
will seem remarkable now-- one

00:12:36.300 --> 00:12:38.010
of my themes was
that the dominant way

00:12:38.010 --> 00:12:39.468
that lawyers and
clients would come

00:12:39.468 --> 00:12:43.742
to communicate in the
future would be by email.

00:12:43.742 --> 00:12:45.060
Now, I joke not.

00:12:45.060 --> 00:12:46.560
The Law Society of
England, they all

00:12:46.560 --> 00:12:49.330
said I shouldn't be
allowed to speak in public.

00:12:49.330 --> 00:12:52.330
They said I was bringing the
profession into disrepute

00:12:52.330 --> 00:12:55.622
by suggesting that clients
and lawyers would use email

00:12:55.622 --> 00:12:57.080
with one another,
and that I didn't

00:12:57.080 --> 00:13:00.881
understand confidentiality, that
I didn't understand security.

00:13:00.881 --> 00:13:02.880
Please don't take for
granted in the professions

00:13:02.880 --> 00:13:04.338
that people will
embrace technology

00:13:04.338 --> 00:13:07.940
very rapidly We have a very
resistant group of users.

00:13:07.940 --> 00:13:09.870
The way we make
sense of technology--

00:13:09.870 --> 00:13:12.362
and I'm half a technologist
and half a lawyer-- we

00:13:12.362 --> 00:13:14.820
organize our thinking about
technology under four headings.

00:13:14.820 --> 00:13:16.320
We identify, as so
many other people

00:13:16.320 --> 00:13:18.823
do, an exponential or at
least explosive growth

00:13:18.823 --> 00:13:23.130
in the underpinning
technologies, processing power,

00:13:23.130 --> 00:13:26.177
bandwidth, data
storage capacity.

00:13:26.177 --> 00:13:28.010
We also talk-- and
Daniel's used this phrase

00:13:28.010 --> 00:13:30.250
before-- a lot of
increasingly capable machines.

00:13:30.250 --> 00:13:31.334
More of which [INAUDIBLE].

00:13:31.334 --> 00:13:33.708
Our assisting machines are
becoming increasingly capable.

00:13:33.708 --> 00:13:35.420
The point again,
as so many others

00:13:35.420 --> 00:13:38.030
have done, the fact that
our devices are becoming

00:13:38.030 --> 00:13:40.600
increasingly pervasive,
and that circuitry is also

00:13:40.600 --> 00:13:46.050
being embedded in objects around
us, and indeed in human beings.

00:13:46.050 --> 00:13:48.770
And finally, in making
sense of the American world

00:13:48.770 --> 00:13:52.300
of technology, we talk about the
ways in which human beings are

00:13:52.300 --> 00:13:53.931
increasingly connected.

00:13:53.931 --> 00:13:55.930
Just to focus on that one
question of increasing

00:13:55.930 --> 00:13:58.830
capability, to unpack that
a little, what interests us

00:13:58.830 --> 00:14:01.150
is the progress being
made recently on big data,

00:14:01.150 --> 00:14:04.510
on data analytics, and the way
in which large bodies of data

00:14:04.510 --> 00:14:06.990
can yield fascinating and
interesting insights, patterns,

00:14:06.990 --> 00:14:08.490
correlations.

00:14:08.490 --> 00:14:11.330
We're interested in the
development of search,

00:14:11.330 --> 00:14:14.619
as well as Watson type
technologies, where certain

00:14:14.619 --> 00:14:16.410
question and answer
scenarios, you actually

00:14:16.410 --> 00:14:18.330
have systems solving problems.

00:14:18.330 --> 00:14:21.290
We're also fascinated
by effective computing.

00:14:21.290 --> 00:14:23.890
I'll say more on
that in a second.

00:14:23.890 --> 00:14:25.090
And of course robotics.

00:14:25.090 --> 00:14:27.310
Effective computing,
this is less known,

00:14:27.310 --> 00:14:28.810
even amongst technologists.

00:14:28.810 --> 00:14:32.990
This is devoted to the ways in
which machines, systems, can

00:14:32.990 --> 00:14:35.270
both detect and
express human emotions.

00:14:35.270 --> 00:14:37.550
The notion that a machine
can look at one's face

00:14:37.550 --> 00:14:39.675
and tell whether or not
you're angry, or surprised,

00:14:39.675 --> 00:14:42.550
or disgusted, the idea now
that machines, more accurately

00:14:42.550 --> 00:14:45.030
than human beings, can
tell whether or not a smile

00:14:45.030 --> 00:14:47.430
is fake smile or
a genuine small.

00:14:47.430 --> 00:14:49.500
So this growing
body of evidence,

00:14:49.500 --> 00:14:51.400
this growing investment
in a whole bundle

00:14:51.400 --> 00:14:54.235
of technologies, very
crudely improving greatly

00:14:54.235 --> 00:14:56.870
in user interface,
but allowing machines

00:14:56.870 --> 00:14:59.740
not just to behave inertly,
but actually, as I say,

00:14:59.740 --> 00:15:03.850
both express and detect,
determine the emotions

00:15:03.850 --> 00:15:05.590
of their users.

00:15:05.590 --> 00:15:07.920
And the final elements of
the big four at the top

00:15:07.920 --> 00:15:09.610
was that our machines are
increasingly connected.

00:15:09.610 --> 00:15:11.193
And of course, one
can speak endlessly

00:15:11.193 --> 00:15:14.120
about social networking, but
just focusing specifically

00:15:14.120 --> 00:15:17.590
on the professions, we're
finding within professions

00:15:17.590 --> 00:15:21.030
there are closed social
networks in medicine, in law,

00:15:21.030 --> 00:15:22.950
in education, in architecture.

00:15:22.950 --> 00:15:25.326
We're also finding-- and a
classic example of patients

00:15:25.326 --> 00:15:28.090
like me-- the opportunity
for what we call communities

00:15:28.090 --> 00:15:30.100
of experience, where
people who have

00:15:30.100 --> 00:15:32.000
been the recipients of
professional service

00:15:32.000 --> 00:15:36.600
can come along and share their
experience in law, in medicine,

00:15:36.600 --> 00:15:39.250
in religion, in general
problem solving,

00:15:39.250 --> 00:15:42.437
in tax, and in
architecture as well.

00:15:42.437 --> 00:15:44.770
And then, of course, the whole
notion of crowd sourcing.

00:15:44.770 --> 00:15:46.144
And we're seeing
in architecture,

00:15:46.144 --> 00:15:48.460
in medicine, in business
problems generally,

00:15:48.460 --> 00:15:50.670
new ways of reaching
out to large communities

00:15:50.670 --> 00:15:53.637
of individuals who can
contribute to problem solving.

00:15:53.637 --> 00:15:55.720
This is radical stuff in
the world of professions.

00:15:55.720 --> 00:15:58.730
We classically think of lawyers,
and doctors, and auditors,

00:15:58.730 --> 00:16:01.570
and counselors, one-to-one
professional advice,

00:16:01.570 --> 00:16:04.010
consultative model, on
an hourly billing basis.

00:16:04.010 --> 00:16:05.800
This is blowing all
of the out the water,

00:16:05.800 --> 00:16:10.220
that at our fingertips we might
have expertise more readily

00:16:10.220 --> 00:16:12.160
usable, accessible,
understandable, than ever

00:16:12.160 --> 00:16:13.177
before.

00:16:13.177 --> 00:16:14.760
And when you think
of the capabilities

00:16:14.760 --> 00:16:17.750
of human professionals-- and
we see a lot about this--

00:16:17.750 --> 00:16:18.900
we identify four.

00:16:18.900 --> 00:16:21.090
It's the cognitive
capabilities, the ability

00:16:21.090 --> 00:16:22.920
to think, to solve
problems, to reason,

00:16:22.920 --> 00:16:25.310
the manual capabilities,
to work with one's hands.

00:16:25.310 --> 00:16:26.315
And you think of vets.

00:16:26.315 --> 00:16:28.800
You think of
surgeons or dentists.

00:16:28.800 --> 00:16:31.830
You think of the emotional
capabilities, to listen to,

00:16:31.830 --> 00:16:34.870
to empathize with one's patients
and clients, and of course

00:16:34.870 --> 00:16:36.540
the moral dimension as well.

00:16:36.540 --> 00:16:38.060
So what we track
through the book

00:16:38.060 --> 00:16:41.410
is to say, well, what
elements of these capability,

00:16:41.410 --> 00:16:43.970
looking ahead in a
sane way, do not think

00:16:43.970 --> 00:16:45.639
could be taken on by machines?

00:16:45.639 --> 00:16:47.430
And so many of the
cognitive tasks, so many

00:16:47.430 --> 00:16:49.763
of the manual tasks, and so
many of the emotional tasks,

00:16:49.763 --> 00:16:51.430
we think it's
entirely foreseeable

00:16:51.430 --> 00:16:53.390
that these tasks
will, in the future,

00:16:53.390 --> 00:16:57.350
be undertaken either by
machines autonomously,

00:16:57.350 --> 00:17:00.791
or by machines with
less qualified people

00:17:00.791 --> 00:17:01.540
than we use today.

00:17:01.540 --> 00:17:04.359
The moral dimension I'm going
to turn to at the very end.

00:17:04.359 --> 00:17:06.180
But let me put this
in a different way.

00:17:06.180 --> 00:17:08.849
What we're seeing
is an evolution

00:17:08.849 --> 00:17:11.400
of professional
service, from a model

00:17:11.400 --> 00:17:14.230
where the way in which
professional service is

00:17:14.230 --> 00:17:17.290
delivered is as some form
craft, the doctor, the lawyer,

00:17:17.290 --> 00:17:20.480
the accountant, to a
model where we standardize

00:17:20.480 --> 00:17:23.180
in terms of process, checklist,
procedure, manuals, practice

00:17:23.180 --> 00:17:23.760
guides.

00:17:23.760 --> 00:17:26.480
In terms of substance, you've
got your standard forms,

00:17:26.480 --> 00:17:28.055
templates, precedents.

00:17:28.055 --> 00:17:29.930
And then we go further,
within our hospitals,

00:17:29.930 --> 00:17:33.570
within our firms, within
our schools, we systematize.

00:17:33.570 --> 00:17:35.470
And so use automatic
workflow systems

00:17:35.470 --> 00:17:37.420
with automatic document
generation systems

00:17:37.420 --> 00:17:38.530
and so forth.

00:17:38.530 --> 00:17:41.250
And all of this falls
pretty much in the category

00:17:41.250 --> 00:17:43.120
of the first sort
of future, where

00:17:43.120 --> 00:17:44.610
we're streamlining
and optimizing

00:17:44.610 --> 00:17:46.032
the old ways of working.

00:17:46.032 --> 00:17:47.490
But what, of course,
is fascinating

00:17:47.490 --> 00:17:50.790
in our current era is the
ability to externalize this,

00:17:50.790 --> 00:17:53.340
to make this content,
this knowledge,

00:17:53.340 --> 00:17:57.060
this practical expertise
available on an online basis.

00:17:57.060 --> 00:17:58.860
Now, some organizations,
particularly

00:17:58.860 --> 00:18:01.530
professional firms, see a great
commercial opportunity here.

00:18:01.530 --> 00:18:03.280
We can make that
expertise available

00:18:03.280 --> 00:18:04.342
as a chargeable offering.

00:18:04.342 --> 00:18:05.800
Others, particularly
the government

00:18:05.800 --> 00:18:08.920
and charitable sector,
will say, actually,

00:18:08.920 --> 00:18:12.070
we can make this expertise
and knowledge available online

00:18:12.070 --> 00:18:12.800
at no charge.

00:18:12.800 --> 00:18:15.350
We'll still control the content,
but we'll make it available.

00:18:15.350 --> 00:18:17.891
And the third option, to which
we're very strongly attracted,

00:18:17.891 --> 00:18:19.010
is the commons model.

00:18:19.010 --> 00:18:21.660
And in the spirit of
Wikipedia, rather than

00:18:21.660 --> 00:18:24.200
simply encyclopedic
information, practical expertise

00:18:24.200 --> 00:18:27.150
and guidance available in
the open source spirit,

00:18:27.150 --> 00:18:29.870
held not by major
organizations or governments,

00:18:29.870 --> 00:18:35.210
but actually as a commons
for the use of everyone.

00:18:35.210 --> 00:18:38.540
And this, we see, is the
movement that we call,

00:18:38.540 --> 00:18:41.910
very crudely, and others use
the same term, commoditization,

00:18:41.910 --> 00:18:45.160
a move away from the
craft of the professions

00:18:45.160 --> 00:18:48.080
towards online availability
with a charitable,

00:18:48.080 --> 00:18:51.750
non-charitable, or available
on a commons basis.

00:18:51.750 --> 00:18:53.470
Let me turn to say
a little bit AI,

00:18:53.470 --> 00:18:54.960
and then I'll head
to the the '80s,

00:18:54.960 --> 00:18:58.640
I wrote my doctorate at
Oxford on AI and law.

00:18:58.640 --> 00:19:01.360
And looking back
now, I'm beginning

00:19:01.360 --> 00:19:03.977
to make sense of what's
happening in this field.

00:19:03.977 --> 00:19:06.310
I was involved in the '80s
in what we called our first--

00:19:06.310 --> 00:19:08.840
or we now call our first wave
of artificial intelligence.

00:19:08.840 --> 00:19:11.280
And I did my doctorate
from '83 to '86.

00:19:11.280 --> 00:19:16.170
Then after for a couple
of years, from '86 to '88,

00:19:16.170 --> 00:19:18.320
I worked on the
development of this.

00:19:18.320 --> 00:19:20.790
And I want to reassure
you that at the time,

00:19:20.790 --> 00:19:22.960
that was quite a
cool screen design.

00:19:22.960 --> 00:19:23.610
It looked good.

00:19:23.610 --> 00:19:25.670
That was the best.

00:19:25.670 --> 00:19:28.780
And actually, the colors matched
well, as with our book today.

00:19:28.780 --> 00:19:30.500
But what happened was this.

00:19:30.500 --> 00:19:33.160
I just finished
my graduate work .

00:19:33.160 --> 00:19:35.690
And the dean of the law
school at Oxford, a man

00:19:35.690 --> 00:19:38.210
called Professor
Capron, a partner

00:19:38.210 --> 00:19:40.020
at a major
international law firm,

00:19:40.020 --> 00:19:42.180
he did a book on a very
complex area of law

00:19:42.180 --> 00:19:44.230
called the
[INAUDIBLE] law, which

00:19:44.230 --> 00:19:46.895
is a small corner of
the law of limitation.

00:19:46.895 --> 00:19:49.520
And he said it's so complicated
no one actually understands it.

00:19:49.520 --> 00:19:52.690
Why don't you be
one of my examiners?

00:19:52.690 --> 00:19:55.040
He said, why don't we build
a system in this area?

00:19:55.040 --> 00:19:57.850
And this was an era when floppy
disks genuinely were floppy.

00:19:57.850 --> 00:19:59.516
And this is what the
system looked like,

00:19:59.516 --> 00:20:01.464
5.25 floppy disks, two of them.

00:20:01.464 --> 00:20:03.630
We wrote a book, and published
it with the two disks

00:20:03.630 --> 00:20:04.210
in the back.

00:20:04.210 --> 00:20:06.660
But just to give you a flavor
of what we were up against,

00:20:06.660 --> 00:20:09.080
section 2 of this act shall
not apply to an action

00:20:09.080 --> 00:20:10.729
to which this section applies.

00:20:10.729 --> 00:20:13.103
And that was one of the more
readily understandable parts

00:20:13.103 --> 00:20:14.580
of the legislation.

00:20:14.580 --> 00:20:16.590
So what we were trying
to do was-- surely

00:20:16.590 --> 00:20:18.820
we can offer a pathway
through this complex web

00:20:18.820 --> 00:20:21.930
of almost unintelligible,
interrelated rules.

00:20:21.930 --> 00:20:24.870
And quite frankly, it was a form
of decision tree we developed.

00:20:24.870 --> 00:20:26.370
But it was a little
more complicated

00:20:26.370 --> 00:20:29.850
than a decision tree because
of the nature of the content

00:20:29.850 --> 00:20:30.715
involved.

00:20:30.715 --> 00:20:33.582
And typically a question
would look like this.

00:20:33.582 --> 00:20:35.790
The content doesn't matter,
but just saying so far it

00:20:35.790 --> 00:20:38.129
looks as if the basis of
your claim is negligent,

00:20:38.129 --> 00:20:39.420
shall we proceed on this basis?

00:20:39.420 --> 00:20:40.970
Yes or no, and navigate through.

00:20:40.970 --> 00:20:43.580
Or basically you're offering
other people the opportunity

00:20:43.580 --> 00:20:47.880
to navigate through this expert
model of this area of law.

00:20:47.880 --> 00:20:50.090
That is a very small
part of the whole system.

00:20:50.090 --> 00:20:52.180
It had two million
paths through it.

00:20:52.180 --> 00:20:54.290
In a sense, we had to
chart almost every one

00:20:54.290 --> 00:20:55.410
of these ourselves.

00:20:55.410 --> 00:20:57.340
So this was the
first wave of AI.

00:20:57.340 --> 00:20:59.840
What we did was, it was
called knowledge elicitation,

00:20:59.840 --> 00:21:00.970
knowledge acquisition.

00:21:00.970 --> 00:21:02.430
You sat down with
a human expert.

00:21:02.430 --> 00:21:04.179
You mined the [INAUDIBLE]
from their head.

00:21:04.179 --> 00:21:06.340
You developed it into
some form of flow chart.

00:21:06.340 --> 00:21:08.589
You put it into a system so
other people could use it.

00:21:08.589 --> 00:21:09.685
It wasn't just in law.

00:21:09.685 --> 00:21:11.560
At the time, there was
a lot of work going on

00:21:11.560 --> 00:21:14.170
in rule-based expert systems,
as they were known in medicine.

00:21:14.170 --> 00:21:17.376
And I at the time, then
worked for a few years ago

00:21:17.376 --> 00:21:18.500
with [INAUDIBLE] and Young.

00:21:18.500 --> 00:21:21.050
They were working in tax, and
audit, and consulting as well.

00:21:21.050 --> 00:21:22.540
So that was the
model of the '80s,

00:21:22.540 --> 00:21:25.595
the expertise taken
from the human brain

00:21:25.595 --> 00:21:27.290
and dropped into a box.

00:21:27.290 --> 00:21:30.330
Turns out, though, quite costly
to build and maintain systems,

00:21:30.330 --> 00:21:32.860
because professional
knowledge changes so rapidly.

00:21:32.860 --> 00:21:35.530
Little incentive for the
commercial organizations

00:21:35.530 --> 00:21:36.440
to invest.

00:21:36.440 --> 00:21:38.650
Remember, most professionals
charge by the hour,

00:21:38.650 --> 00:21:40.550
so why would you
want to reduce, as we

00:21:40.550 --> 00:21:42.470
were reducing a task
from 2 hours to 2 minutes

00:21:42.470 --> 00:21:43.970
if you're charging
by the hour, then

00:21:43.970 --> 00:21:45.780
the market's not
wanting that of you.

00:21:45.780 --> 00:21:47.470
So there's no
incentive to introduce

00:21:47.470 --> 00:21:48.890
this kind of efficiency.

00:21:48.890 --> 00:21:50.510
And of course, the
web came along.

00:21:50.510 --> 00:21:52.426
And this wasn't doing
the same as the AI

00:21:52.426 --> 00:21:55.475
that we were aspiring to,
but it offered, at least

00:21:55.475 --> 00:21:58.760
intuitively, an easy way to
make content guidance available

00:21:58.760 --> 00:22:00.410
online quickly, cheaply.

00:22:00.410 --> 00:22:03.450
And so the AI winter, at
least in the professions,

00:22:03.450 --> 00:22:05.590
I attribute to the web.

00:22:05.590 --> 00:22:07.185
Because people were
not distracted,

00:22:07.185 --> 00:22:08.940
but people were e-tracted.

00:22:08.940 --> 00:22:12.190
to a new, easier way of
working with their knowledge.

00:22:12.190 --> 00:22:14.042
For us, a turning
point came in the '97.

00:22:14.042 --> 00:22:16.250
It's well known that's when
Kasparov, the world chess

00:22:16.250 --> 00:22:19.630
champion, was
beaten by Deep Blue.

00:22:19.630 --> 00:22:22.460
Now, in the '80s, working
in the computer laboratory,

00:22:22.460 --> 00:22:24.710
we used to discuss this a
lot, and we were of the view

00:22:24.710 --> 00:22:27.280
that a computer system could
never beat a grandmaster.

00:22:27.280 --> 00:22:28.354
Why?

00:22:28.354 --> 00:22:30.520
Well, that's because up
until the level of a master,

00:22:30.520 --> 00:22:33.750
masters could explain how
they went about playing chess.

00:22:33.750 --> 00:22:37.150
When you got to a certain
level, the creative insight,

00:22:37.150 --> 00:22:39.985
the bolts out of the blue,
the genius, the experts

00:22:39.985 --> 00:22:41.610
themselves, the
grandmasters themselves

00:22:41.610 --> 00:22:43.060
couldn't explain
what they were doing.

00:22:43.060 --> 00:22:44.976
And this applied right
across the professions.

00:22:44.976 --> 00:22:47.415
The great lawyers, the great
doctors, they would also say,

00:22:47.415 --> 00:22:48.040
it's intuition.

00:22:48.040 --> 00:22:49.050
It's gut reaction.

00:22:49.050 --> 00:22:49.700
It's knee-jerk.

00:22:49.700 --> 00:22:50.772
We can't explain it.

00:22:50.772 --> 00:22:53.230
And we thought, if you can't
explain it, we can't model it.

00:22:53.230 --> 00:22:56.490
And therefore, we're limited
to a fairly modest, but quite

00:22:56.490 --> 00:22:59.264
useful level of performance
by these systems.

00:22:59.264 --> 00:23:00.930
But what we hadn't
banked on, of course,

00:23:00.930 --> 00:23:03.410
was exponential increase
in processing power.

00:23:03.410 --> 00:23:06.240
And Kasparov was beaten not
by a system that sometimes was

00:23:06.240 --> 00:23:07.840
cleverer or more insightful.

00:23:07.840 --> 00:23:11.550
It was by brute
force processing,

00:23:11.550 --> 00:23:13.060
and, well, the
reality is that it's

00:23:13.060 --> 00:23:15.610
a system that could feed up
to a billion possible moves

00:23:15.610 --> 00:23:17.000
a second.

00:23:17.000 --> 00:23:19.670
And so it is in so
many other professions,

00:23:19.670 --> 00:23:23.020
these systems are
outperforming human beings,

00:23:23.020 --> 00:23:28.660
not by copying what human beings
do, but by brute force search,

00:23:28.660 --> 00:23:30.497
and also by a very
large quantity of data.

00:23:30.497 --> 00:23:32.830
Now, Patrick Winston, one of
the fathers of AI research,

00:23:32.830 --> 00:23:33.390
put it very well.

00:23:33.390 --> 00:23:35.139
There are lots of ways
of being smart that

00:23:35.139 --> 00:23:36.014
aren't smart like us.

00:23:36.014 --> 00:23:38.139
In the '80s, we thought
the only way of being smart

00:23:38.139 --> 00:23:39.280
was being smart like us.

00:23:39.280 --> 00:23:41.570
You copied human experts,
knowledge, and performance,

00:23:41.570 --> 00:23:43.020
and put into a system.

00:23:43.020 --> 00:23:44.250
Why would we do that?

00:23:44.250 --> 00:23:46.010
Systems can actually
outperform us

00:23:46.010 --> 00:23:48.100
by working in entirely
different ways.

00:23:48.100 --> 00:23:49.530
And this leads us
to what we feel

00:23:49.530 --> 00:23:50.920
is quite an important insight.

00:23:50.920 --> 00:23:52.450
We call this the
AI fallacy, and it

00:23:52.450 --> 00:23:53.910
pervades the
academic literature,

00:23:53.910 --> 00:23:55.770
the popular literature,
and actually

00:23:55.770 --> 00:23:57.510
it's just a common
misconception.

00:23:57.510 --> 00:24:00.940
We define it as this, it's
the mistaken assumption

00:24:00.940 --> 00:24:03.800
that the only way to develop
systems that perform tasks

00:24:03.800 --> 00:24:05.830
at the level of
experts or higher

00:24:05.830 --> 00:24:09.450
is to replicate the thinking
process of human specialists.

00:24:09.450 --> 00:24:10.950
And so very often,
people say, well,

00:24:10.950 --> 00:24:14.470
a system can't perform at
level, because it can't think,

00:24:14.470 --> 00:24:17.730
or because we don't know
how human beings operate.

00:24:17.730 --> 00:24:19.260
But we think this is fallacious.

00:24:19.260 --> 00:24:20.670
Let me give you an example.

00:24:20.670 --> 00:24:23.180
Frequently, a lawyer
or a doctor will say,

00:24:23.180 --> 00:24:28.410
a computer can't have
judgement like a human being.

00:24:28.410 --> 00:24:30.640
Now, that may well
be true, but that's

00:24:30.640 --> 00:24:33.170
assuming that that
is the right question

00:24:33.170 --> 00:24:35.600
to ask, how can we
somehow replicate

00:24:35.600 --> 00:24:36.850
judgement in a machine?

00:24:36.850 --> 00:24:38.139
We prefer to take a step back.

00:24:38.139 --> 00:24:39.680
We do this a lot in
our book, and ask

00:24:39.680 --> 00:24:41.055
the question, to
what the problem

00:24:41.055 --> 00:24:43.410
is judgement the solution?

00:24:43.410 --> 00:24:45.620
Because the purpose
of these systems

00:24:45.620 --> 00:24:49.950
is not simply to streamline.

00:24:49.950 --> 00:24:52.330
The current way of
working is find,

00:24:52.330 --> 00:24:54.420
using the power of technology,
new ways of solving

00:24:54.420 --> 00:24:55.720
the problems.

00:24:55.720 --> 00:24:57.470
And when you think
about it, judgement

00:24:57.470 --> 00:24:58.787
solves problems of uncertainty.

00:24:58.787 --> 00:25:01.245
And when you think about it
also, so much of the technology

00:25:01.245 --> 00:25:02.786
that you and others
are involved with

00:25:02.786 --> 00:25:05.956
is precisely powerful
ways of dealing

00:25:05.956 --> 00:25:08.330
with handling uncertainty,
particularly when large bodies

00:25:08.330 --> 00:25:09.880
of data are involved.

00:25:09.880 --> 00:25:14.220
And so judgements, the way human
beings cope with uncertainty,

00:25:14.220 --> 00:25:16.540
very often we're seeing
across the professions

00:25:16.540 --> 00:25:18.700
that technology, and
of course computing,

00:25:18.700 --> 00:25:20.840
a very large stores
of data, actually

00:25:20.840 --> 00:25:22.348
work in entirely different ways.

00:25:22.348 --> 00:25:25.139
There's new ways of
coping with uncertainty.

00:25:25.139 --> 00:25:26.680
People often ask,
can machines think?

00:25:26.680 --> 00:25:29.310
We love the answer to this
in relation to Watson.

00:25:29.310 --> 00:25:34.240
The day after Watson beat the
contestants on the Jeopardy

00:25:34.240 --> 00:25:36.850
quiz program, John
Searle, a very well known

00:25:36.850 --> 00:25:38.440
American philosopher
said, "Watson

00:25:38.440 --> 00:25:39.869
doesn't know it
won in Jeopardy."

00:25:39.869 --> 00:25:42.160
And it didn't take people
down to the pub to celebrate.

00:25:42.160 --> 00:25:44.589
It didn't reflect on what
we had and hadn't won.

00:25:44.589 --> 00:25:46.130
It doesn't think
the same ways we do.

00:25:46.130 --> 00:25:50.550
But the key point is,
this non-thinking machine,

00:25:50.550 --> 00:25:52.170
it can outperform human beings.

00:25:52.170 --> 00:25:54.170
So what we are pointing
to is a category

00:25:54.170 --> 00:25:57.250
of increasingly capable
non-thinking machines.

00:25:57.250 --> 00:26:00.870
In a sense, we sidestep a lot of
the interesting psychological,

00:26:00.870 --> 00:26:03.170
philosophical questions
about AI, but point out,

00:26:03.170 --> 00:26:06.045
there's a second
wave of AI that one

00:26:06.045 --> 00:26:09.460
can trace to Kasparov, where
we're seeing systems being

00:26:09.460 --> 00:26:11.670
developed that
outperform human beings,

00:26:11.670 --> 00:26:13.544
but they don't work
like human beings.

00:26:13.544 --> 00:26:15.085
And this is the leap
that most people

00:26:15.085 --> 00:26:17.610
find hard to make, because
they somehow think,

00:26:17.610 --> 00:26:20.260
the identify things that human
beings do that they can't

00:26:20.260 --> 00:26:22.660
imagine computers can do,
because they'll say things

00:26:22.660 --> 00:26:25.720
like, you can't program
a computer to do XYZ,

00:26:25.720 --> 00:26:27.580
failing to understand
that problems can

00:26:27.580 --> 00:26:29.190
be cracked in different ways.

00:26:29.190 --> 00:26:31.255
And that's the
second wave of AI.

00:26:31.255 --> 00:26:33.380
Now, this has profound
implications for employment.

00:26:33.380 --> 00:26:35.320
Because what we're seeing is
more and more of the tasks that

00:26:35.320 --> 00:26:37.730
are undertaken by
professionals will actually

00:26:37.730 --> 00:26:41.550
be undertaken by machines or by
less experienced people working

00:26:41.550 --> 00:26:42.480
with machines.

00:26:42.480 --> 00:26:44.590
The implications of
that for the job market

00:26:44.590 --> 00:26:46.298
is what Daniel's now
going to talk about.

00:26:50.492 --> 00:26:52.700
DANIEL SUSSKIND: So I think
this leads to two related

00:26:52.700 --> 00:26:53.616
questions being asked.

00:26:53.616 --> 00:26:58.430
The first is, will there be any
jobs left for professionals?

00:26:58.430 --> 00:27:01.270
And then the second is, now
what can humans do that machines

00:27:01.270 --> 00:27:02.750
cannot do?

00:27:02.750 --> 00:27:05.630
And I want to look at each
of these questions in turn.

00:27:05.630 --> 00:27:08.530
So the first question, will
there be any jobs left?

00:27:08.530 --> 00:27:11.210
And the answer to this largely
depends upon time scales.

00:27:11.210 --> 00:27:13.150
In the medium term,
we think technology

00:27:13.150 --> 00:27:16.582
will lead to a reduction in
traditional professional roles,

00:27:16.582 --> 00:27:18.790
but at the same time, we
also think it will give rise

00:27:18.790 --> 00:27:20.240
to a whole new set of roles.

00:27:20.240 --> 00:27:23.130
So just to pick one,
the knowledge engineers.

00:27:23.130 --> 00:27:25.444
This is the sort of thing my
dad was doing in the '80s,

00:27:25.444 --> 00:27:27.860
getting the practical expertise
outside of someone's head,

00:27:27.860 --> 00:27:31.770
and putting it into a system so
that lay people could use it,

00:27:31.770 --> 00:27:35.490
paraprofessionals, less expert,
less specialist people who

00:27:35.490 --> 00:27:37.510
use these new systems
and machines to do

00:27:37.510 --> 00:27:39.560
the sorts of things
that in the past

00:27:39.560 --> 00:27:42.990
would have required an
expert or a specialist.

00:27:42.990 --> 00:27:45.330
But the long run's a
very different story,

00:27:45.330 --> 00:27:48.384
and that's what I
want to focus on now.

00:27:48.384 --> 00:27:50.550
And here, broadly, there
are two schools of thought.

00:27:50.550 --> 00:27:54.090
There's the optimists, and
there's the pessimists.

00:27:54.090 --> 00:27:55.597
And the pessimists say, no.

00:27:55.597 --> 00:27:57.930
There isn't going to be any
work for human professionals

00:27:57.930 --> 00:27:59.070
to do in the future.

00:27:59.070 --> 00:28:01.060
Machines are becoming
increasingly capable.

00:28:01.060 --> 00:28:03.800
They're able to perform more
and more to-do, more and more

00:28:03.800 --> 00:28:04.826
types of work.

00:28:04.826 --> 00:28:07.200
And so there will be less and
less work for humans to do.

00:28:07.200 --> 00:28:09.080
That's the pessimists.

00:28:09.080 --> 00:28:11.580
Now, the optimists
say, that's wrong.

00:28:11.580 --> 00:28:14.370
Not only is there work today
that only humans can do,

00:28:14.370 --> 00:28:17.230
but tomorrow in the future,
we'll create new types of work,

00:28:17.230 --> 00:28:19.690
perhaps work that we
can't conceive of today.

00:28:19.690 --> 00:28:23.000
And that will be work that
can be done by human beings.

00:28:23.000 --> 00:28:25.330
In particular, the
technical criticism

00:28:25.330 --> 00:28:27.960
that the optimists make of
the pessimists is they say,

00:28:27.960 --> 00:28:30.430
the pessimists are committing
the lump of labor fallacy.

00:28:30.430 --> 00:28:33.390
And this is the idea,
the mistaken belief,

00:28:33.390 --> 00:28:36.900
that there is some lump of work
in the economy to be divided up

00:28:36.900 --> 00:28:39.570
between people and
machines, and machines

00:28:39.570 --> 00:28:41.810
doing more of that leaves
less for people to do.

00:28:41.810 --> 00:28:43.330
The optimists say,
no, that's it.

00:28:43.330 --> 00:28:45.490
As the economy grows and
we become more productive,

00:28:45.490 --> 00:28:46.656
we create new types of work.

00:28:46.656 --> 00:28:48.736
And people can do that instead.

00:28:48.736 --> 00:28:51.360
And it's also here that you see
another line of argument, which

00:28:51.360 --> 00:28:55.650
is that optimal future
involves human beings

00:28:55.650 --> 00:28:57.630
and machines working
together, each bringing

00:28:57.630 --> 00:29:01.320
their own distinctive
capability.

00:29:01.320 --> 00:29:03.470
And so the question
is, who is right here?

00:29:07.070 --> 00:29:09.880
One of the difficulties in
thinking about this problem

00:29:09.880 --> 00:29:11.800
is that when we talk
about the future of work,

00:29:11.800 --> 00:29:14.310
we tend to talk about the
different jobs that people do.

00:29:14.310 --> 00:29:16.768
So in the professions, we talk
about doctors, and teachers,

00:29:16.768 --> 00:29:18.330
and nurses, and lawyers.

00:29:18.330 --> 00:29:21.400
But the term "jobs" isn't
entirely illuminating,

00:29:21.400 --> 00:29:25.060
because a job isn't a
monolithic, indivisible lump

00:29:25.060 --> 00:29:26.540
of work.

00:29:26.540 --> 00:29:29.000
Instead, to think about
the future of work,

00:29:29.000 --> 00:29:30.630
we have to look at
what people actually

00:29:30.630 --> 00:29:34.960
do in their jobs, the type of
tasks that make up their jobs.

00:29:34.960 --> 00:29:38.470
Just to give an example, if you
look at nursing, for example,

00:29:38.470 --> 00:29:41.925
25 years ago, what a nurse did
was very different from what

00:29:41.925 --> 00:29:42.880
a nurse does today.

00:29:42.880 --> 00:29:45.300
Then it might have involved
bedpans and bedside

00:29:45.300 --> 00:29:46.440
conversation.

00:29:46.440 --> 00:29:49.440
Today, nurses can prescribe
certain types of medication

00:29:49.440 --> 00:29:52.440
and even perform minor surgery.

00:29:52.440 --> 00:29:54.470
The tasks that nurses
do has changed,

00:29:54.470 --> 00:29:58.470
but we use the same job
title, nurses, for both.

00:29:58.470 --> 00:30:00.600
So why does this matter
about the future of work?

00:30:00.600 --> 00:30:04.681
Well, the Economist reviewed
our book two weeks ago.

00:30:04.681 --> 00:30:06.180
It was a positive
review, otherwise,

00:30:06.180 --> 00:30:08.800
I wouldn't have mentioned it.

00:30:08.800 --> 00:30:11.350
And alongside the piece
was this great illustration

00:30:11.350 --> 00:30:14.860
of Professor Doctor Robot QC.

00:30:14.860 --> 00:30:17.500
And I think there's a sense
in a lot of thinking about

00:30:17.500 --> 00:30:21.094
the future of work that one
day, a doctor, or a nurse,

00:30:21.094 --> 00:30:22.760
or a teacher is going
to turn up at work

00:30:22.760 --> 00:30:26.490
and find Professor Doctor Robot
QC or one of his relatives

00:30:26.490 --> 00:30:27.600
sitting in their chair.

00:30:27.600 --> 00:30:30.020
Their job would have
been entirely replaced.

00:30:30.020 --> 00:30:32.300
and we just don't think
that's how it will play out.

00:30:32.300 --> 00:30:35.500
If you imagine a technology
that replaces a task

00:30:35.500 --> 00:30:37.925
that a professional
used to do, so here

00:30:37.925 --> 00:30:42.440
a less glamorous-looking
remote monitoring system.

00:30:42.440 --> 00:30:45.440
This means that people
need to go to the doctor

00:30:45.440 --> 00:30:47.740
less for a medical examination.

00:30:47.740 --> 00:30:50.850
What we want to say is that the
job that the professional does

00:30:50.850 --> 00:30:51.420
has changed.

00:30:51.420 --> 00:30:55.600
The doctor, maybe
she'll spend less time

00:30:55.600 --> 00:30:59.320
doing medical check-ups, but
she might spend more time

00:30:59.320 --> 00:31:01.800
reading the latest research.

00:31:01.800 --> 00:31:05.630
And this is why the
term job is unhelpful.

00:31:05.630 --> 00:31:07.370
It's too high
level, and it masks

00:31:07.370 --> 00:31:10.350
this deeper, underlying
churn in the tasks

00:31:10.350 --> 00:31:12.770
that people are doing.

00:31:12.770 --> 00:31:18.190
Of course, we can now return to
the optimists and pessimists,

00:31:18.190 --> 00:31:19.460
and rephrase it.

00:31:19.460 --> 00:31:21.110
The optimists, they
think that there's

00:31:21.110 --> 00:31:23.930
going to be enough professional
tasks for human beings

00:31:23.930 --> 00:31:26.222
to do in the future, whereas
the pessimists are saying,

00:31:26.222 --> 00:31:28.596
no, there's not going to be
enough tasks for human beings

00:31:28.596 --> 00:31:29.700
to do in the future.

00:31:29.700 --> 00:31:32.030
And the point here
is that, of course,

00:31:32.030 --> 00:31:34.130
it's true that if
enough tasks are lost,

00:31:34.130 --> 00:31:36.650
it no longer makes sense to
talk of the job under which all

00:31:36.650 --> 00:31:37.930
these tasks are bundled.

00:31:37.930 --> 00:31:41.371
But we have to start from
the level of the tasks.

00:31:41.371 --> 00:31:43.120
So what do we think
now with this thinking

00:31:43.120 --> 00:31:43.970
in terms of tasks?

00:31:43.970 --> 00:31:47.720
Well, out both the optimists and
pessimists are right and wrong.

00:31:47.720 --> 00:31:50.170
The pessimists are right in
recognizing that machines

00:31:50.170 --> 00:31:53.700
are becoming increasingly
capable at existing tasks,

00:31:53.700 --> 00:31:55.320
but they're wrong
to ignore the fact

00:31:55.320 --> 00:31:58.390
that there may be new tasks
to be done in the future.

00:31:58.390 --> 00:31:59.890
The optimists are
right to recognize

00:31:59.890 --> 00:32:01.780
there will be new tasks that
will done in the future,

00:32:01.780 --> 00:32:04.238
but they're wrong to think that
machines rather than people

00:32:04.238 --> 00:32:06.750
will be best placed to
perform these tasks.

00:32:06.750 --> 00:32:08.520
So our conclusion is
that machines will

00:32:08.520 --> 00:32:10.370
become increasingly capable.

00:32:10.370 --> 00:32:12.730
They'll take on more and
more of today's tasks.

00:32:12.730 --> 00:32:15.480
New tasks will no doubt
emerge, but it's likely

00:32:15.480 --> 00:32:17.900
that machines will take
on these tasks as well.

00:32:17.900 --> 00:32:19.910
And there is a
serious point here,

00:32:19.910 --> 00:32:22.240
which is that we're not
challenging the presumption

00:32:22.240 --> 00:32:24.365
that there'll be new work
to be done in the future,

00:32:24.365 --> 00:32:26.910
but we are challenging
this idea that necessarily

00:32:26.910 --> 00:32:30.147
people rather than machines
will be best placed to do it.

00:32:30.147 --> 00:32:32.646
And so we find it hard to avoid
the conclusion that there'll

00:32:32.646 --> 00:32:35.950
be a steady decline in the
demand for human professional

00:32:35.950 --> 00:32:37.859
in the long run.

00:32:37.859 --> 00:32:39.900
The best and the brightest
will last the longest,

00:32:39.900 --> 00:32:43.260
either of those who do tasks
that can't be done by machines,

00:32:43.260 --> 00:32:45.580
or tasks that ought not
to be done by machines.

00:32:45.580 --> 00:32:48.092
And that's something that my
dad will return to in a moment.

00:32:48.092 --> 00:32:49.800
But the key point here
is that there just

00:32:49.800 --> 00:32:52.120
won't be enough of
these professional tasks

00:32:52.120 --> 00:32:55.586
to keep armies of traditional
professionals in employment.

00:32:55.586 --> 00:32:57.460
At this point, talking
to professionals,

00:32:57.460 --> 00:32:59.810
they say, but surely!

00:32:59.810 --> 00:33:02.790
But surely there must be some
tasks that machines can never

00:33:02.790 --> 00:33:04.210
do, even in the long run.

00:33:04.210 --> 00:33:06.434
And this comes to the second
question that I set up,

00:33:06.434 --> 00:33:08.350
are there things that
machines could never do?

00:33:11.620 --> 00:33:13.660
When we describe this
situation, we tend to say

00:33:13.660 --> 00:33:15.310
or we say that
professionals have

00:33:15.310 --> 00:33:19.330
a Rubik's Cube conception of
what machines are capable of.

00:33:19.330 --> 00:33:21.240
So here is an
expression of a man

00:33:21.240 --> 00:33:25.000
who builds a machine out
of LEGO and a smartphone

00:33:25.000 --> 00:33:29.588
that can solve a Rubik's
Cube in 3.4 seconds.

00:33:29.588 --> 00:33:32.510
You pop the Rubik's Cube in
there, press go, it sinks in,

00:33:32.510 --> 00:33:33.900
pops up, it's solved.

00:33:33.900 --> 00:33:37.470
And I think most professional
see this and say, yeah,

00:33:37.470 --> 00:33:40.580
this is clearly a manually
dexterous and complicated task,

00:33:40.580 --> 00:33:42.690
but of course
machines can do it.

00:33:42.690 --> 00:33:44.170
It's rule based.

00:33:44.170 --> 00:33:46.400
It's clearly defined.

00:33:46.400 --> 00:33:48.160
It's logical.

00:33:48.160 --> 00:33:50.480
It's a routine task.

00:33:50.480 --> 00:33:53.230
And professionals then said,
but what we do is non-routine.

00:33:56.240 --> 00:33:59.740
We rely on creativity.

00:33:59.740 --> 00:34:00.680
We rely on judgement.

00:34:00.680 --> 00:34:01.510
We rely on empathy.

00:34:01.510 --> 00:34:03.150
All of these are
non-routine things,

00:34:03.150 --> 00:34:05.560
and surely machines can
never do non-routine tasks.

00:34:05.560 --> 00:34:08.100
And this is also a distinction,
the non-routine routine one,

00:34:08.100 --> 00:34:11.280
that runs through lots and
lots of academic writing

00:34:11.280 --> 00:34:14.330
on the future of work.

00:34:14.330 --> 00:34:15.540
And we say not so fast.

00:34:15.540 --> 00:34:17.448
Two mistakes are
being made here.

00:34:17.448 --> 00:34:19.239
And the first is this
idea of decomposition

00:34:19.239 --> 00:34:20.238
that my dad spoke about.

00:34:20.238 --> 00:34:22.330
To start with, when you
take professional work

00:34:22.330 --> 00:34:24.260
and break it down into
its component parts,

00:34:24.260 --> 00:34:26.300
it turns out that
many of those parts

00:34:26.300 --> 00:34:28.679
are routine rather
than non-routine.

00:34:28.679 --> 00:34:31.610
Actually, not much
that a professional

00:34:31.610 --> 00:34:35.949
does, in cases not much at all,
relies upon empathy, judgement,

00:34:35.949 --> 00:34:39.087
creativity in a way
that is often said.

00:34:39.087 --> 00:34:41.170
But the second point comes
back to the AI fantasy,

00:34:41.170 --> 00:34:46.520
which is that even those
non-routine tasks that exist

00:34:46.520 --> 00:34:48.889
can often be done in
different ways by machines.

00:34:48.889 --> 00:34:51.159
The mistake is to think
that because a machine can't

00:34:51.159 --> 00:34:53.010
think it can never be created.

00:34:53.010 --> 00:34:55.500
Because a machine can't
feel, it can't be empathetic.

00:35:00.980 --> 00:35:03.890
What we often notice
here, and what we find

00:35:03.890 --> 00:35:06.345
is that these machines operate
often in very inhuman ways,

00:35:06.345 --> 00:35:08.053
but they can take on
these tasks as well.

00:35:11.230 --> 00:35:13.890
So this line of thinking on
professional tasks prompts

00:35:13.890 --> 00:35:18.900
a more fundamental
question, and that's--

00:35:18.900 --> 00:35:20.150
RICHARD SUSSKIND: You're done.

00:35:20.150 --> 00:35:22.302
DANIEL SUSSKIND: Yeah.

00:35:22.302 --> 00:35:23.468
RICHARD SUSSKIND: Thank you.

00:35:26.004 --> 00:35:27.670
As Daniel said at the
outset, we thought

00:35:27.670 --> 00:35:29.835
we were writing a book about
the future of the professions,

00:35:29.835 --> 00:35:31.501
but it turned out we
were really writing

00:35:31.501 --> 00:35:33.810
a book about the
future of expertise,

00:35:33.810 --> 00:35:37.160
how you capture and share
expertise in society.

00:35:37.160 --> 00:35:40.760
And the question that we
therefore ask and then answer

00:35:40.760 --> 00:35:43.380
in the book is precisely
that in more formal terms,

00:35:43.380 --> 00:35:46.200
the production and distribution
of practical expertise

00:35:46.200 --> 00:35:46.960
in society.

00:35:46.960 --> 00:35:49.170
So how do we produce
and distribute

00:35:49.170 --> 00:35:51.100
practical expertise in society?

00:35:51.100 --> 00:35:52.870
The current answer,
the traditional model,

00:35:52.870 --> 00:35:53.750
is the professions.

00:35:53.750 --> 00:35:55.708
And I know it's been that
way since the outset.

00:35:55.708 --> 00:35:58.320
That's the way that we found
best both to generate and make

00:35:58.320 --> 00:35:59.890
available expertise.

00:35:59.890 --> 00:36:02.870
We put particular
occupational groups in charge,

00:36:02.870 --> 00:36:06.400
we give them exclusive rights,
and they get on with it.

00:36:06.400 --> 00:36:08.484
And we've identified
problems [INAUDIBLE] with it.

00:36:08.484 --> 00:36:10.233
But what we've also
identified in our book

00:36:10.233 --> 00:36:11.480
were six alternative models.

00:36:11.480 --> 00:36:14.013
And there's no time today to
delve into each, but in a sense

00:36:14.013 --> 00:36:16.120
this is at the
heart of the book,

00:36:16.120 --> 00:36:18.422
because not only are we
saying there's problems

00:36:18.422 --> 00:36:19.380
with the current model.

00:36:19.380 --> 00:36:21.629
We're actually saying there's
some viable alternatives

00:36:21.629 --> 00:36:22.250
out there.

00:36:22.250 --> 00:36:23.708
And we've touched
on a few of them,

00:36:23.708 --> 00:36:25.860
for example, the knowledge
engineering model.

00:36:25.860 --> 00:36:28.150
Rather than having
a human expert

00:36:28.150 --> 00:36:31.030
dispense the advice on a
one-to-one consultative

00:36:31.030 --> 00:36:33.620
advisory basis, that
human expert's knowledge

00:36:33.620 --> 00:36:35.790
and experience is
modded into the system

00:36:35.790 --> 00:36:38.020
and made available on
a one-to-many basis.

00:36:38.020 --> 00:36:39.880
That's the knowledge
in [INAUDIBLE].

00:36:39.880 --> 00:36:43.170
Pretty much, actually,
according with the first wave

00:36:43.170 --> 00:36:44.867
of AI in the '80s.

00:36:44.867 --> 00:36:47.450
Then there's the communities of
experience model, for example.

00:36:47.450 --> 00:36:47.960
We touched on this.

00:36:47.960 --> 00:36:49.550
This is gonna be
terribly significant.

00:36:49.550 --> 00:36:53.030
Where recipients of professional
advice, patients, clients,

00:36:53.030 --> 00:36:54.930
actually come
together and say, this

00:36:54.930 --> 00:36:56.410
is how I sorted out the problem.

00:36:56.410 --> 00:36:58.370
This is what I was
advised, or this is

00:36:58.370 --> 00:37:00.000
how I sorted it out personally.

00:37:00.000 --> 00:37:01.812
And I will make that
available to others.

00:37:01.812 --> 00:37:03.020
And we'll see this community.

00:37:03.020 --> 00:37:06.145
We think it may well need to
be moderated by specialist

00:37:06.145 --> 00:37:08.160
individuals, but
we'll see communities

00:37:08.160 --> 00:37:10.370
of professional experience
build up, which may well

00:37:10.370 --> 00:37:12.590
be the first port of
call for many individuals

00:37:12.590 --> 00:37:14.510
when they need
guidance on anything.

00:37:14.510 --> 00:37:15.927
The most extreme
version of which

00:37:15.927 --> 00:37:17.635
are book has been
allied with exclusively

00:37:17.635 --> 00:37:19.364
is the machine-generated model.

00:37:19.364 --> 00:37:21.280
That's the idea that
there might be autonomous

00:37:21.280 --> 00:37:25.010
machines that are actually both
generating and distributing

00:37:25.010 --> 00:37:26.350
practical expertise.

00:37:26.350 --> 00:37:28.990
We can see decades from
now that in many areas

00:37:28.990 --> 00:37:31.035
of the professions,
this will be the case.

00:37:31.035 --> 00:37:33.660
But what we don't want to leave
anyone with the impression with

00:37:33.660 --> 00:37:37.200
is that it's either the case
of autonomous robots dispensing

00:37:37.200 --> 00:37:39.340
advice, or the
traditional model.

00:37:39.340 --> 00:37:41.940
There are a variety of
models in the world.

00:37:41.940 --> 00:37:44.030
I want to close with
the issue of morality.

00:37:44.030 --> 00:37:45.988
Because it seemed to us--
and we addressed them

00:37:45.988 --> 00:37:48.380
both in the book-- there are
two big moral issues that

00:37:48.380 --> 00:37:52.100
arise from the subjects
we are addressing.

00:37:52.100 --> 00:37:56.400
And the first in chapter 7,
or actually the conclusion,

00:37:56.400 --> 00:37:59.520
we asked the question,
what future should we want?

00:37:59.520 --> 00:38:02.270
The first question we asked
was this, what tasks ought not

00:38:02.270 --> 00:38:04.340
be handled by machines?

00:38:04.340 --> 00:38:07.080
So even if machines,
in one way or another,

00:38:07.080 --> 00:38:09.020
machines work with less
well qualified people,

00:38:09.020 --> 00:38:10.890
can match the
cognitive, the manual,

00:38:10.890 --> 00:38:16.009
the effective capabilities,
of a human professionals,

00:38:16.009 --> 00:38:17.800
might there not be some
areas of human life

00:38:17.800 --> 00:38:20.620
we simply don't want to
hand over to machines?

00:38:20.620 --> 00:38:24.420
Do we want machines to be making
decisions about the turning off

00:38:24.420 --> 00:38:26.230
of life support systems?

00:38:26.230 --> 00:38:29.860
Do we want machines to be
passing life sentences?

00:38:29.860 --> 00:38:34.472
Do we really want the moral buck
to be stopping at the robot?

00:38:34.472 --> 00:38:36.930
And most of us feel-- it may
be irrational-- but most of us

00:38:36.930 --> 00:38:39.530
feel, I think, an
intuitive unease

00:38:39.530 --> 00:38:41.340
when we think about this.

00:38:41.340 --> 00:38:44.800
And we actually think this
is a huge issue, which

00:38:44.800 --> 00:38:47.010
probably extends well
beyond the professions.

00:38:47.010 --> 00:38:49.430
And we liken it to
the issue of IVF

00:38:49.430 --> 00:38:51.950
as it was discussed in
the early '80s, where

00:38:51.950 --> 00:38:53.905
a very famous
philosopher, Mary Warnock,

00:38:53.905 --> 00:38:58.030
was invited to do an inquiry
into the moral implications

00:38:58.030 --> 00:38:59.840
of this American technology.

00:38:59.840 --> 00:39:01.960
So in our book, we
called for a similar kind

00:39:01.960 --> 00:39:05.840
of public government-led
discussion, debate,

00:39:05.840 --> 00:39:09.410
about the limits of the use
of this kind of technology

00:39:09.410 --> 00:39:10.500
in employment.

00:39:10.500 --> 00:39:13.620
And of course, the professions
were a special example, but so

00:39:13.620 --> 00:39:17.171
many pressing moral issues
arising within the professions.

00:39:17.171 --> 00:39:19.170
Now is the time to start
debating whether or not

00:39:19.170 --> 00:39:22.060
we think it's appropriate
that these tasks, or some

00:39:22.060 --> 00:39:24.350
of these tasks, are
taken on by machines.

00:39:24.350 --> 00:39:28.380
And the second question
now weighs even bigger.

00:39:28.380 --> 00:39:30.800
We're seeing that this practical
expertise, the knowledge,

00:39:30.800 --> 00:39:35.240
experience, insight,
know-how of human experts

00:39:35.240 --> 00:39:39.269
can actually be captured and
made available in new ways.

00:39:39.269 --> 00:39:41.560
So the question is, if human
beings through their labor

00:39:41.560 --> 00:39:43.520
and often in the
service, and this

00:39:43.520 --> 00:39:47.300
is captured and
available largely online,

00:39:47.300 --> 00:39:51.586
who should own and control
tomorrow's practical expertise?

00:39:51.586 --> 00:39:53.210
Now, we say in the
book, we're actually

00:39:53.210 --> 00:39:54.892
coming to a fork in the road.

00:39:54.892 --> 00:39:56.850
We're not quite there
yet, but there's two very

00:39:56.850 --> 00:39:58.230
different paths.

00:39:58.230 --> 00:40:05.110
One is where you see big
business, government,

00:40:05.110 --> 00:40:07.970
withholding the control
and therefore dispensing.

00:40:07.970 --> 00:40:09.590
It's a new form of bargain.

00:40:09.590 --> 00:40:11.660
It's a new grand bargain,
a new exclusivity,

00:40:11.660 --> 00:40:13.490
dispensing this
practical expertise.

00:40:13.490 --> 00:40:15.360
And the other, and it
might sound fanciful,

00:40:15.360 --> 00:40:17.900
but it's more of a commons
that insofar as possible,

00:40:17.900 --> 00:40:20.360
and if we can provide the
incentives to encourage people

00:40:20.360 --> 00:40:23.240
to contribute to this,
we see a road ahead

00:40:23.240 --> 00:40:27.700
where practical expertise is
available at no or at low cost

00:40:27.700 --> 00:40:30.420
right across the
global community,

00:40:30.420 --> 00:40:33.660
derived from the
best human experts,

00:40:33.660 --> 00:40:36.630
but actually delivered
in a more effective way.

00:40:36.630 --> 00:40:38.297
Now, are money is on
that second option,

00:40:38.297 --> 00:40:40.546
but that's something that
we want to discuss with you.

00:40:40.546 --> 00:40:42.230
Thank you for listening.

00:40:42.230 --> 00:40:44.539
[APPLAUSE]

00:40:49.134 --> 00:40:50.550
MALE SPEAKER: Thank
you very much.

00:40:50.550 --> 00:40:52.080
For this very interesting talk.

00:40:52.080 --> 00:40:52.830
Who has questions?

00:40:56.770 --> 00:41:00.790
That is the most hands I've
ever seen for questions.

00:41:00.790 --> 00:41:02.790
AUDIENCE: So just
to start off, when

00:41:02.790 --> 00:41:05.940
you saying is the first place.

00:41:05.940 --> 00:41:14.250
I was very struck that you
didn't mention the competence

00:41:14.250 --> 00:41:16.750
aspect, which I imagine
most professions would

00:41:16.750 --> 00:41:20.520
say was their bar
of entry, and that's

00:41:20.520 --> 00:41:23.020
what they're guaranteeing in
exchange for the exclusivity.

00:41:30.680 --> 00:41:38.810
DANIEL SUSSKIND: What you see
is in practice [INAUDIBLE]

00:41:38.810 --> 00:41:41.950
medicine for example,
diagnosis is a bit [INAUDIBLE].

00:41:41.950 --> 00:41:44.740
We were on a panel the
other day with a doctor

00:41:44.740 --> 00:41:46.790
making exactly that defense.

00:41:46.790 --> 00:41:52.010
He said the most important thing
that I do is diagnose patients.

00:41:52.010 --> 00:41:54.780
And that is something
that I do very well.

00:41:54.780 --> 00:41:58.790
In fact, if you look at the
data, up to 20% of the time,

00:41:58.790 --> 00:42:03.850
doctors make mistakes
with diagnoses.

00:42:03.850 --> 00:42:06.000
And what you then see,
outside of the professions,

00:42:06.000 --> 00:42:08.138
are machines operating
in very different ways,

00:42:08.138 --> 00:42:11.053
diagnosing in very
different ways that

00:42:11.053 --> 00:42:13.830
have for certain types
of cancer or reviewing

00:42:13.830 --> 00:42:16.492
certain types of
scans and radiography,

00:42:16.492 --> 00:42:18.950
and here is the view that they
can outperform human beings.

00:42:28.560 --> 00:42:30.200
So it's important
not to set a bar

00:42:30.200 --> 00:42:33.040
for those outside
the professions that

00:42:33.040 --> 00:42:35.500
doesn't match the one the
professions currently achieve.

00:42:35.500 --> 00:42:38.180
Generally, the
standard, the instinct

00:42:38.180 --> 00:42:41.550
is to say-- there's
another example.

00:42:41.550 --> 00:42:43.810
There's an autonomous
robot in San Francisco

00:42:43.810 --> 00:42:46.110
that delivers prescriptions.

00:42:46.110 --> 00:42:49.330
And it has delivered 6
million prescriptions,

00:42:49.330 --> 00:42:51.820
and it's made one error so far.

00:42:51.820 --> 00:42:53.280
They said it's due
to human error.

00:42:53.280 --> 00:42:55.238
And a lot of people say,
that's not acceptable.

00:42:55.238 --> 00:42:57.060
It shouldn't be making an error.

00:42:57.060 --> 00:43:00.422
At best, human pharmacists
make an error 1% of a time.

00:43:00.422 --> 00:43:02.380
6 million prescriptions,
1% of the time, that's

00:43:02.380 --> 00:43:07.527
60,000 mistakes, not one.

00:43:07.527 --> 00:43:09.610
So the short answer is
outside of the professions,

00:43:09.610 --> 00:43:11.568
there appear to be lots
of systems and machines

00:43:11.568 --> 00:43:13.310
operating in very
different ways that

00:43:13.310 --> 00:43:16.760
challenge this idea that
professions are necessarily

00:43:16.760 --> 00:43:19.180
the only way to provide
high quality expertise.

00:43:19.180 --> 00:43:20.560
RICHARD SUSSKIND: But you are
right that that's the argument

00:43:20.560 --> 00:43:21.690
the professionals will use.

00:43:21.690 --> 00:43:24.780
It's part of what we
call the grand bargain.

00:43:24.780 --> 00:43:27.285
The question you ask is, why
do we give exclusive rights?

00:43:27.285 --> 00:43:28.470
[RINGTONE]

00:43:28.470 --> 00:43:30.090
Well, this is
tremendously important.

00:43:30.090 --> 00:43:31.950
Your health is
tremendously important.

00:43:31.950 --> 00:43:35.194
Your legal welfare,
this is vital stuff.

00:43:35.194 --> 00:43:38.512
You don't want any Joe
conducting brain surgery,

00:43:38.512 --> 00:43:41.235
just as you don't want any
joe in the High court standing

00:43:41.235 --> 00:43:43.240
up and representing you.

00:43:43.240 --> 00:43:47.870
And therefore, we should
have the exclusive rights

00:43:47.870 --> 00:43:50.310
over certain areas
of human activity,

00:43:50.310 --> 00:43:52.370
because what we can
just about guarantee

00:43:52.370 --> 00:43:54.453
is that we'll deliver a
higher quality than anyone

00:43:54.453 --> 00:43:58.922
else who hasn't got our
training or ongoing educations.

00:43:58.922 --> 00:44:01.280
As Daniel says, I think
our book basically

00:44:01.280 --> 00:44:03.300
is challenging the
grand bargain, saying

00:44:03.300 --> 00:44:05.815
that grand bargain made sense
in a brick-based industrial

00:44:05.815 --> 00:44:08.190
society from the
19th century onwards,

00:44:08.190 --> 00:44:13.340
but actually, once you start
decomposing and looking

00:44:13.340 --> 00:44:15.756
at much of the work
that's done, as Dan says,

00:44:15.756 --> 00:44:17.630
there are different
ways, systems, processes,

00:44:17.630 --> 00:44:21.800
people who can deliver as
high a quality of service

00:44:21.800 --> 00:44:24.330
without the exclusivity
which probably

00:44:24.330 --> 00:44:26.580
bring the mystification, the
greater cost, the greater

00:44:26.580 --> 00:44:29.972
complexity, and the alienation
that many of them suffer.

00:44:29.972 --> 00:44:31.930
DANIEL SUSSKIND: Just to
add one thing to that,

00:44:31.930 --> 00:44:34.620
the example we talked
about at the start,

00:44:34.620 --> 00:44:37.050
with eBay and the 60
millions disputes that

00:44:37.050 --> 00:44:38.960
are resolved online,
in the absence

00:44:38.960 --> 00:44:41.800
of that e-mediation platform,
those 60 million disputes

00:44:41.800 --> 00:44:43.860
would have probably
gone unresolved.

00:44:43.860 --> 00:44:46.750
That is 40 times as many
of a similar type of claim

00:44:46.750 --> 00:44:50.120
that result in the
English justice system.

00:44:50.120 --> 00:44:55.000
So it may be the case that
this e-mediation platform,

00:44:55.000 --> 00:44:56.960
on a case by case
basis, provides--

00:44:56.960 --> 00:44:59.240
unless you get the best
judge in the country

00:44:59.240 --> 00:45:01.420
to adjudicate each
dispute on eBay,

00:45:01.420 --> 00:45:03.021
it may be the case
that that judge

00:45:03.021 --> 00:45:04.270
would reach a better decision.

00:45:04.270 --> 00:45:06.010
We're talking about
60 million people

00:45:06.010 --> 00:45:08.710
who probably wouldn't
have received

00:45:08.710 --> 00:45:09.800
a resolution without it.

00:45:09.800 --> 00:45:11.300
It would have been
a resolution they

00:45:11.300 --> 00:45:13.270
weren't happy with otherwise.

00:45:13.270 --> 00:45:15.750
There's a quotation
from Voltaire,

00:45:15.750 --> 00:45:18.240
"you shouldn't let the best
be the enemy of the good."

00:45:18.240 --> 00:45:21.610
Lots of these systems offer good
alternatives not to the best,

00:45:21.610 --> 00:45:23.890
but to nothing, which
people currently have.

00:45:23.890 --> 00:45:26.932
And this is a story that you
see across the professions.

00:45:26.932 --> 00:45:28.140
What are the professions for?

00:45:28.140 --> 00:45:30.514
They're responsible for some
of the most important things

00:45:30.514 --> 00:45:31.700
that we do in society.

00:45:31.700 --> 00:45:35.440
And for the reasons I've said,
in lots of different ways,

00:45:35.440 --> 00:45:36.660
they're failing.

00:45:36.660 --> 00:45:38.744
And these systems, in
case, can offer better,

00:45:38.744 --> 00:45:40.410
but in some cases,
they just offer good,

00:45:40.410 --> 00:45:42.800
and good is better than nothing.

00:45:42.800 --> 00:45:44.490
And even now that's
true. [INAUDIBLE].

00:45:49.250 --> 00:45:50.187
AUDIENCE: Yeah.

00:45:50.187 --> 00:45:52.312
The next question is, have
you considered something

00:45:52.312 --> 00:45:54.740
about how you handle outliers?

00:45:54.740 --> 00:45:57.360
So you have the case like
for the airline pilot, where

00:45:57.360 --> 00:45:58.860
most of the time,
the autopilot does

00:45:58.860 --> 00:46:01.150
a great job of flying the
plane, and probably better

00:46:01.150 --> 00:46:02.797
than the real pilot.

00:46:02.797 --> 00:46:04.630
But then you get into
an emergency situation

00:46:04.630 --> 00:46:06.052
where he has to
choose whether to land

00:46:06.052 --> 00:46:08.093
on the river, or the golf
course, or the highway.

00:46:10.400 --> 00:46:13.958
Doing the routine work makes
the airline pilot better

00:46:13.958 --> 00:46:14.950
in an emergency.

00:46:14.950 --> 00:46:19.640
And so by segmenting the
work, you lose that extreme.

00:46:19.640 --> 00:46:22.156
So I guess in this
situation, if the best

00:46:22.156 --> 00:46:25.747
is the enemy of the good,
who decides when actually

00:46:25.747 --> 00:46:28.247
the best is important enough
that we don't go with automated

00:46:28.247 --> 00:46:30.075
planes or whatever else.

00:46:30.075 --> 00:46:32.450
RICHARD SUSSKIND: There's
actually a few questions there.

00:46:32.450 --> 00:46:34.760
One is, how do you
become an expert?

00:46:34.760 --> 00:46:36.780
It's the old training ground.

00:46:36.780 --> 00:46:38.780
You used to cut your teeth
in more routine work,

00:46:38.780 --> 00:46:42.040
so we've got a whole
section discussing that.

00:46:42.040 --> 00:46:44.000
The other question,
actually, funnily enough

00:46:44.000 --> 00:46:46.880
is something I can
[INAUDIBLE] never

00:46:46.880 --> 00:46:50.080
really answered in the '80s.

00:46:50.080 --> 00:46:52.130
Because my view, when
I was working on expert

00:46:52.130 --> 00:46:53.697
systems was that
these systems were

00:46:53.697 --> 00:46:55.113
very good at solving
what I called

00:46:55.113 --> 00:46:56.946
clear cases of
the expert domain.

00:46:56.946 --> 00:46:59.112
By that I meant problems
that were easy for experts,

00:46:59.112 --> 00:47:01.836
but not too easy
for non-experts.

00:47:01.836 --> 00:47:03.960
But these systems weren't
very good for sorting out

00:47:03.960 --> 00:47:04.810
problems for expects.

00:47:04.810 --> 00:47:06.560
They'd say, actually,
that's quite tricky.

00:47:06.560 --> 00:47:09.350
But the difficulty is that
if you're not an expert,

00:47:09.350 --> 00:47:12.767
you don't know if a problem
is easy or hard, even

00:47:12.767 --> 00:47:13.920
for an expert.

00:47:13.920 --> 00:47:17.110
So there's a border
mindset of gray.

00:47:17.110 --> 00:47:19.260
Sometimes, it's
perfectly obvious

00:47:19.260 --> 00:47:20.676
that something's
very complicated,

00:47:20.676 --> 00:47:22.410
but sometimes also
there's a gray area,

00:47:22.410 --> 00:47:23.985
where you might
default to what looks

00:47:23.985 --> 00:47:28.060
like a rather simple solution,
but actually miss a nuance.

00:47:28.060 --> 00:47:30.230
And our inclination
is, actually--

00:47:30.230 --> 00:47:33.012
to Dan's point again, the
best is the enemy of the good,

00:47:33.012 --> 00:47:34.720
in many areas of
professional work, which

00:47:34.720 --> 00:47:37.270
aren't threats of life of
death, we probably just

00:47:37.270 --> 00:47:39.020
need to accept that
what we're now getting

00:47:39.020 --> 00:47:42.407
is 19 times out of 10
rather than 0 out of 0.

00:47:42.407 --> 00:47:44.490
Sorry, 19 times out of 20,
rather than 0 out of 0.

00:47:49.490 --> 00:47:52.430
The great insight you've made
there is where there really

00:47:52.430 --> 00:47:56.360
are terribly high
risks that follow,

00:47:56.360 --> 00:47:58.486
we really will need
experts involved

00:47:58.486 --> 00:48:00.606
doing the triage
in the first place,

00:48:00.606 --> 00:48:02.231
seeing whether or
not this is something

00:48:02.231 --> 00:48:04.818
that can actually be
disposed of by a machine

00:48:04.818 --> 00:48:05.934
or by an individual.

00:48:05.934 --> 00:48:07.350
Going back to the
earlier question

00:48:07.350 --> 00:48:09.810
of how you become
an expert, it's

00:48:09.810 --> 00:48:12.880
fascinating, because
it's a training question.

00:48:12.880 --> 00:48:14.810
How do we train people?

00:48:14.810 --> 00:48:16.390
And I'll give you
example in law.

00:48:16.390 --> 00:48:19.340
In law, it could be an
indictment, I think,

00:48:19.340 --> 00:48:21.456
of major law firms, but
the way you want to learn

00:48:21.456 --> 00:48:24.300
is during one's traineeship,
for example, to be a litigator,

00:48:24.300 --> 00:48:27.195
is you're often
assigned to a basement

00:48:27.195 --> 00:48:30.340
room with a huge
body of documents,

00:48:30.340 --> 00:48:33.563
either electronically
or in folders.

00:48:33.563 --> 00:48:35.306
And it's called document review.

00:48:35.306 --> 00:48:37.080
You have to review
these documents.

00:48:37.080 --> 00:48:39.230
And you do that looking
for relevant ones.

00:48:39.230 --> 00:48:40.688
And I did a little
bit of research,

00:48:40.688 --> 00:48:42.540
went asking some junior
lawyers and said,

00:48:42.540 --> 00:48:45.570
if this was automated--
and there's now

00:48:45.570 --> 00:48:50.030
technology that could outperform
junior lawyers-- would

00:48:50.030 --> 00:48:55.356
this mean that you're losing
a major learning opportunity?

00:48:55.356 --> 00:48:57.690
And the response was,
look, we get how to do this

00:48:57.690 --> 00:48:58.834
after two or three days.

00:48:58.834 --> 00:49:01.656
We don't need to do it
for two or three years.

00:49:01.656 --> 00:49:02.985
[LAUGHTER]

00:49:02.985 --> 00:49:05.114
I rather cynically say,
don't confuse training

00:49:05.114 --> 00:49:05.864
with exploitation.

00:49:09.090 --> 00:49:10.540
The training of
many professionals

00:49:10.540 --> 00:49:11.623
is a little bit of a deal.

00:49:11.623 --> 00:49:13.830
You go in there to
a high branded firm.

00:49:13.830 --> 00:49:17.480
The market's still happy to pay
fairly high rates for people

00:49:17.480 --> 00:49:19.119
who are fairly junior.

00:49:19.119 --> 00:49:20.910
And the way the market
is, the clients just

00:49:20.910 --> 00:49:22.405
say, actually, we're just paying
for these young professionals

00:49:22.405 --> 00:49:23.400
to do our work.

00:49:23.400 --> 00:49:25.766
And until 2005, 2006, there
wasn't even much wrong

00:49:25.766 --> 00:49:30.049
with that, because there was
more money swilling around.

00:49:30.049 --> 00:49:32.950
But now you're even seeing in
accounting, consulting, tax,

00:49:32.950 --> 00:49:36.970
law, audit, [INAUDIBLE] clients
say we no longer want to pay.

00:49:36.970 --> 00:49:39.800
So it's actually a
fascinating challenge

00:49:39.800 --> 00:49:40.760
to the business model.

00:49:40.760 --> 00:49:43.952
Because, actually, what it means
is that many professional firms

00:49:43.952 --> 00:49:45.452
are actually going
to have to invest

00:49:45.452 --> 00:49:47.530
in very practical training
rather than hoping

00:49:47.530 --> 00:49:50.034
it can be picked up on the job.

00:49:50.034 --> 00:49:52.910
My answer in the longer run
is-- and one would expect this,

00:49:52.910 --> 00:49:57.170
and I hope it resonates-- is
that-- it's part of our book

00:49:57.170 --> 00:50:01.757
as well, [INAUDIBLE] education.

00:50:01.757 --> 00:50:04.172
There are far more
powerful ways, I think,

00:50:04.172 --> 00:50:07.070
of exposing people
to a far wider range

00:50:07.070 --> 00:50:08.955
of professional
problems and situations

00:50:08.955 --> 00:50:10.996
using simulated learning
environments rather than

00:50:10.996 --> 00:50:12.449
natural learning environments.

00:50:12.449 --> 00:50:14.240
And we're shown this
[INAUDIBLE] university

00:50:14.240 --> 00:50:16.192
training young lawyers.

00:50:16.192 --> 00:50:19.566
We've created a virtual
town called [INAUDIBLE]

00:50:19.566 --> 00:50:22.940
in which young, aspiring
lawyers practice law for a year.

00:50:22.940 --> 00:50:27.062
And actually we have simulated
the actual negotiations,

00:50:27.062 --> 00:50:29.160
the actual court
appearances, the interactions

00:50:29.160 --> 00:50:33.008
with the clients in
a way the, frankly,

00:50:33.008 --> 00:50:34.932
slightly random exposure
to the experience

00:50:34.932 --> 00:50:38.150
you get in the world of training
with a firm doesn't really

00:50:38.150 --> 00:50:40.400
provide.

00:50:40.400 --> 00:50:43.150
My major point is,
most clients say

00:50:43.150 --> 00:50:45.284
when they ask you questions,
they actually say,

00:50:45.284 --> 00:50:46.622
that's your problem.

00:50:46.622 --> 00:50:49.266
It's a training problem
you've got to sort it out.

00:50:49.266 --> 00:50:50.587
But if we can find a way of
sorting our problems that's

00:50:50.587 --> 00:50:52.078
cheaper, quicker,
more effective,

00:50:52.078 --> 00:50:54.066
we want you to
provide [INAUDIBLE].

00:50:54.066 --> 00:50:55.557
But you raise a major challenge.

00:50:57.675 --> 00:51:00.300
So these are quite long answers,
but they're complex questions.

00:51:03.270 --> 00:51:06.330
AUDIENCE: So I want to get
your thought about letting

00:51:06.330 --> 00:51:08.562
a machine, say, run a country.

00:51:11.220 --> 00:51:14.330
RICHARD SUSSKIND: I
think that raises really

00:51:14.330 --> 00:51:16.210
the first moral question.

00:51:16.210 --> 00:51:20.240
Do we think that-- I'll start
with the moral question.

00:51:20.240 --> 00:51:22.740
Are we comfortable
with the notion

00:51:22.740 --> 00:51:27.500
that decisions and issues
of policy, people's welfare,

00:51:27.500 --> 00:51:29.947
and economy, and justice,
and health, and so

00:51:29.947 --> 00:51:32.105
forth are handed
over to machines

00:51:32.105 --> 00:51:33.230
from a moral point of view?

00:51:33.230 --> 00:51:35.865
And I'm not making
an observation there.

00:51:35.865 --> 00:51:38.365
I think we need to have a public
debate about whether or not

00:51:38.365 --> 00:51:41.180
our intuitions about this
actually are well founded.

00:51:41.180 --> 00:51:44.610
But I think most people
would draw the line there.

00:51:44.610 --> 00:51:47.205
We'd say we don't really
want a robot at the helm.

00:51:47.205 --> 00:51:49.246
But I could see a whole
bundle of other arguments

00:51:49.246 --> 00:51:50.890
that would say,
well, actually you

00:51:50.890 --> 00:51:53.650
could say a machine might
have far better data

00:51:53.650 --> 00:51:56.650
at its fingertips, be able to
make more rational decisions.

00:51:56.650 --> 00:51:59.150
I think, on balance,
I would default

00:51:59.150 --> 00:52:01.108
to suggesting that
what we will do

00:52:01.108 --> 00:52:05.642
is have responsible human
beings making moral decisions

00:52:05.642 --> 00:52:08.012
but increasingly supported
by these powerful systems.

00:52:08.012 --> 00:52:10.777
We had a fascinating
discussion here in the book--

00:52:10.777 --> 00:52:12.360
it's not quite your
question, but it's

00:52:12.360 --> 00:52:16.900
an extreme example-- with a
very senior military individual.

00:52:16.900 --> 00:52:21.389
Because we're thinking the
profession of being of soldier,

00:52:21.389 --> 00:52:24.650
we thought we might cover,
but we didn't in the end.

00:52:24.650 --> 00:52:29.797
And with a lot of reflection
in this context of machines

00:52:29.797 --> 00:52:31.130
conducting warfare and so forth.

00:52:31.130 --> 00:52:32.610
These are tricky issues.

00:52:32.610 --> 00:52:36.240
They become trickiest
where the machines wildly

00:52:36.240 --> 00:52:40.107
outperform human beings in tasks
which are very important to us

00:52:40.107 --> 00:52:42.910
from a moral decision
point of view.

00:52:42.910 --> 00:52:45.460
Because the intuition that
a human being must involved

00:52:45.460 --> 00:52:48.516
may well be trumped by this
massive leap in performance

00:52:48.516 --> 00:52:50.850
by a machine.

00:52:50.850 --> 00:52:53.716
I heard your question as a
little bit of a provocation.

00:52:53.716 --> 00:52:58.750
It doesn't really need to be one
observation though-- we don't

00:52:58.750 --> 00:53:00.858
see the professions-- and
if you consider politics

00:53:00.858 --> 00:53:04.070
to be a profession--
we don't see this

00:53:04.070 --> 00:53:07.534
to be an overnight revolution.

00:53:07.534 --> 00:53:09.325
Daniel's point about
Professor Doctor Robot

00:53:09.325 --> 00:53:10.980
you see sitting at
the desk one day,

00:53:10.980 --> 00:53:13.830
it's not that that
appears one day.

00:53:13.830 --> 00:53:15.070
It's not a tipping point.

00:53:15.070 --> 00:53:16.850
It's not a revolution.

00:53:16.850 --> 00:53:19.504
But nor is it a
leisurely evolution.

00:53:19.504 --> 00:53:21.462
We view it as an
incremental transformation.

00:53:21.462 --> 00:53:23.667
So what we think we'll
see in the professions,

00:53:23.667 --> 00:53:26.165
and I suppose in
politics too is,

00:53:26.165 --> 00:53:29.920
you'll see step
changes over the years.

00:53:29.920 --> 00:53:31.420
And you'll look
back, say, 30 years,

00:53:31.420 --> 00:53:33.766
and say, wow, it
is unrecognizable.

00:53:33.766 --> 00:53:35.390
But it's not like in
some areas of life

00:53:35.390 --> 00:53:38.035
where there genuinely is
a fundamental stopping

00:53:38.035 --> 00:53:40.670
of an old era and a
starting of a new era.

00:53:40.670 --> 00:53:43.470
So I think what we'll
find in politics, possibly

00:53:43.470 --> 00:53:46.858
a more evolutionary role
than a transformational role.

00:53:46.858 --> 00:53:49.160
It's steady, rapid change.

00:53:49.160 --> 00:53:50.870
Did you want to [INAUDIBLE].

00:53:56.852 --> 00:53:59.352
DANIEL SUSSKIND: It's also the
case that those big questions

00:53:59.352 --> 00:54:01.018
are fascinating,
interesting, important,

00:54:01.018 --> 00:54:02.980
but there is, just,
again, to come back

00:54:02.980 --> 00:54:06.028
to the spirit of the
conversation we were having

00:54:06.028 --> 00:54:09.560
before, in the day-to-day,
many of the systems

00:54:09.560 --> 00:54:12.440
and machines that exist,
they can't run a country,

00:54:12.440 --> 00:54:15.320
but they can do a good job at
the things that lots of people

00:54:15.320 --> 00:54:16.280
don't have access to.

00:54:16.280 --> 00:54:19.240
So we shouldn't be blinded or
scared by those big questions.

00:54:19.240 --> 00:54:22.990
And to thinking about today,
and the next months and years,

00:54:22.990 --> 00:54:25.292
how can we use these
increasingly capable systems

00:54:25.292 --> 00:54:29.592
and machines to just do a
better job for more people who

00:54:29.592 --> 00:54:33.299
currently don't have
access to a professional.

00:54:33.299 --> 00:54:35.215
RICHARD SUSSKIND: If you
think of the division

00:54:35.215 --> 00:54:40.619
of the legislature, executive,
and judiciary of government,

00:54:40.619 --> 00:54:41.160
I'm involved.

00:54:41.160 --> 00:54:44.690
I was actually advisor to
the [INAUDIBLE] justice.

00:54:44.690 --> 00:54:47.340
Since '98, I've been
advising the computerization

00:54:47.340 --> 00:54:48.496
of the court system.

00:54:48.496 --> 00:54:50.610
This is a glacial business.

00:54:50.610 --> 00:54:51.920
It moves very, very slowly.

00:54:51.920 --> 00:54:53.330
Dan's absolutely right.

00:54:53.330 --> 00:54:55.550
Your question's fascinating,
but actually, there's

00:54:55.550 --> 00:54:58.320
a lot we can do, for example,
in our justice system,

00:54:58.320 --> 00:55:00.700
how we run our courts with
the kind of technologies

00:55:00.700 --> 00:55:04.290
that frankly many of you
absolutely take for granted.

00:55:04.290 --> 00:55:07.526
They would greatly improve
access to justice today.

00:55:07.526 --> 00:55:12.032
And so investment's probably
not so much in the replacement

00:55:12.032 --> 00:55:13.394
of the cabinet.

00:55:13.394 --> 00:55:13.985
It's more--

00:55:13.985 --> 00:55:15.610
DANIEL SUSSKIND:
Getting rid of the fax

00:55:15.610 --> 00:55:16.640
machines in the court system.

00:55:16.640 --> 00:55:17.931
RICHARD SUSSKIND: That's right.

00:55:17.931 --> 00:55:21.110
It's the machinery
of government.

00:55:21.110 --> 00:55:24.094
It could at least be
streamlined considerably.

00:55:24.094 --> 00:55:26.260
MALE SPEAKER: I think there
are many more questions,

00:55:26.260 --> 00:55:28.934
but unfortunately, we
have run out of time.

00:55:28.934 --> 00:55:30.350
MALE SPEAKER: I
think you're gonna

00:55:30.350 --> 00:55:31.760
stay around for [INAUDIBLE].

00:55:31.760 --> 00:55:34.169
RICHARD SUSSKIND: I can stay
around, [INAUDIBLE] yes.

00:55:34.169 --> 00:55:36.460
FEMALE SPEAKER: I've just
been waiting for a long time.

00:55:36.460 --> 00:55:37.960
Can you take one more?

00:55:37.960 --> 00:55:39.800
[LAUGHTER]

00:55:39.800 --> 00:55:41.607
DANIEL SUSSKIND:
I'll take one more.

00:55:41.607 --> 00:55:43.440
MALE SPEAKER: Can we
take one more question?

00:55:43.440 --> 00:55:45.065
MALE SPEAKER: With
a very quick answer.

00:55:45.065 --> 00:55:46.991
[INAUDIBLE]

00:55:46.991 --> 00:55:49.200
DANIEL SUSSKIND: Finish
with a big one, then.

00:55:49.200 --> 00:55:52.775
FEMALE SPEAKER: I
was thinking that-- I

00:55:52.775 --> 00:55:56.545
don't want to be
insulting, but when

00:55:56.545 --> 00:56:00.830
you think about the adjectives
that you picked, for example,

00:56:00.830 --> 00:56:03.400
when you say that we don't
need any professionals,

00:56:03.400 --> 00:56:06.360
you see that as a
pessimistic point of view,

00:56:06.360 --> 00:56:09.800
but when you look at-- let's say
you're not in the UK anymore,

00:56:09.800 --> 00:56:13.470
and let's look from the view of
all the countries in the world.

00:56:13.470 --> 00:56:15.350
For example, in
the Ebola outbreak

00:56:15.350 --> 00:56:18.840
in Africa, people
really needed doctors,

00:56:18.840 --> 00:56:20.500
but they didn't have doctors.

00:56:20.500 --> 00:56:23.200
And they really needed
nurses, and they didn't really

00:56:23.200 --> 00:56:24.160
have nurses.

00:56:24.160 --> 00:56:28.030
And some of the failed states,
the reason that they are still

00:56:28.030 --> 00:56:31.190
failed states is not only
because of terrorism,

00:56:31.190 --> 00:56:36.520
or military issues, but they
don't have enough professions.

00:56:36.520 --> 00:56:39.770
For example, I, myself,
at my private school,

00:56:39.770 --> 00:56:44.100
we were 75 kids for one teacher.

00:56:44.100 --> 00:56:45.770
Can you imagine it?

00:56:45.770 --> 00:56:47.630
And so it was really hard.

00:56:47.630 --> 00:56:50.957
So what I was saying is I think
it's an optimistic future when

00:56:50.957 --> 00:56:52.040
we don't need professions.

00:56:55.864 --> 00:56:58.280
RICHARD SUSSKIND: I hope we
didn't come across negatively.

00:56:58.280 --> 00:57:00.780
When Daniel was talking about
pessimists and optimists,

00:57:00.780 --> 00:57:03.520
that was very much from the
professionals' point of view.

00:57:03.520 --> 00:57:06.020
DANIEL SUSSKIND: Professionals
saying that's a pessimistic--

00:57:06.020 --> 00:57:07.936
RICHARD SUSSKIND: No,
you're absolutely right.

00:57:07.936 --> 00:57:10.570
Just a tiny little riff,
you know the language.

00:57:10.570 --> 00:57:13.740
I'm sure it's been used a lot
within Google about disruption.

00:57:13.740 --> 00:57:17.306
We, although we're big fans
of Clayton Christiansen's book

00:57:17.306 --> 00:57:20.330
on "The Innovator's Dilemma"
and his work in disruption,

00:57:20.330 --> 00:57:22.150
we expressly say we're
not using that term.

00:57:22.150 --> 00:57:26.860
Because to say that
medicine, and law,

00:57:26.860 --> 00:57:29.700
and so forth are being
disrupted actually

00:57:29.700 --> 00:57:32.180
is a very provider-supplier
point of view.

00:57:32.180 --> 00:57:34.690
From the point of view of
the recipient of the service,

00:57:34.690 --> 00:57:37.080
this might mean greater,
cheaper, better access

00:57:37.080 --> 00:57:38.220
to medicine and health.

00:57:38.220 --> 00:57:41.280
So we think the word disruption
carries with it phrases

00:57:41.280 --> 00:57:43.586
which are slightly contentious.

00:57:43.586 --> 00:57:45.460
You are absolutely right
in your observation.

00:57:45.460 --> 00:57:47.320
What you say,
though, when you say

00:57:47.320 --> 00:57:48.778
the reason these
states have failed

00:57:48.778 --> 00:57:50.405
is because they don't
have professions,

00:57:50.405 --> 00:57:51.542
you see, we would put
it rather differently.

00:57:51.542 --> 00:57:53.830
We'd say the reasons
the states have failed

00:57:53.830 --> 00:57:55.980
is because they don't have
solutions to the which

00:57:55.980 --> 00:57:58.220
the professions are thought
historically to be the answer.

00:57:58.220 --> 00:57:59.345
And what we're saying
is, actually, they

00:57:59.345 --> 00:58:00.600
need not be the answer.

00:58:00.600 --> 00:58:01.975
So what I'm working,
for example,

00:58:01.975 --> 00:58:04.349
I've done a little bit of work
with third world countries

00:58:04.349 --> 00:58:05.620
on new justice systems.

00:58:05.620 --> 00:58:07.080
I say, don't replicate ours.

00:58:07.080 --> 00:58:07.980
You can leapfrog.

00:58:07.980 --> 00:58:11.590
You don't need to go through
the process we've gone through,

00:58:11.590 --> 00:58:14.610
what ends up being rather
antiquated and not totally fit

00:58:14.610 --> 00:58:15.790
for [INAUDIBLE].

00:58:15.790 --> 00:58:18.130
You can jump to, for example,
online dispute resolution.

00:58:18.130 --> 00:58:19.855
That's a great
question to finish on.

00:58:19.855 --> 00:58:20.980
Thanks very much, everyone.

00:58:20.980 --> 00:58:22.521
DANIEL SUSSKIND:
Thank you very much.

00:58:22.521 --> 00:58:24.133
[APPLAUSE]

