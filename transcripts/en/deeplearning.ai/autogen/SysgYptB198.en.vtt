WEBVTT
Kind: captions
Language: en

00:00:00.050 --> 00:00:02.330
 
for most of this week you've been using

00:00:02.330 --> 00:00:02.340
for most of this week you've been using
 

00:00:02.340 --> 00:00:06.019
for most of this week you've been using
a encoder/decoder architecture full

00:00:06.019 --> 00:00:06.029
a encoder/decoder architecture full
 

00:00:06.029 --> 00:00:08.419
a encoder/decoder architecture full
machine translation one RNN reads in a

00:00:08.419 --> 00:00:08.429
machine translation one RNN reads in a
 

00:00:08.429 --> 00:00:10.759
machine translation one RNN reads in a
sentence and then different one outputs

00:00:10.759 --> 00:00:10.769
sentence and then different one outputs
 

00:00:10.769 --> 00:00:13.339
sentence and then different one outputs
a sentence there's a modification to

00:00:13.339 --> 00:00:13.349
a sentence there's a modification to
 

00:00:13.349 --> 00:00:15.829
a sentence there's a modification to
this called the attention model that

00:00:15.829 --> 00:00:15.839
this called the attention model that
 

00:00:15.839 --> 00:00:18.290
this called the attention model that
makes all this work much better

00:00:18.290 --> 00:00:18.300
makes all this work much better
 

00:00:18.300 --> 00:00:20.269
makes all this work much better
the attention algorithm the attention

00:00:20.269 --> 00:00:20.279
the attention algorithm the attention
 

00:00:20.279 --> 00:00:21.679
the attention algorithm the attention
idea has been one of the most

00:00:21.679 --> 00:00:21.689
idea has been one of the most
 

00:00:21.689 --> 00:00:24.200
idea has been one of the most
influential ideas in deep learning let's

00:00:24.200 --> 00:00:24.210
influential ideas in deep learning let's
 

00:00:24.210 --> 00:00:27.050
influential ideas in deep learning let's
take a look at how that works so get a

00:00:27.050 --> 00:00:27.060
take a look at how that works so get a
 

00:00:27.060 --> 00:00:29.990
take a look at how that works so get a
very long french sentence like this what

00:00:29.990 --> 00:00:30.000
very long french sentence like this what
 

00:00:30.000 --> 00:00:32.780
very long french sentence like this what
we are asking this green and coda neural

00:00:32.780 --> 00:00:32.790
we are asking this green and coda neural
 

00:00:32.790 --> 00:00:34.459
we are asking this green and coda neural
network to do is to read in the whole

00:00:34.459 --> 00:00:34.469
network to do is to read in the whole
 

00:00:34.469 --> 00:00:36.440
network to do is to read in the whole
sentence and then memorize the whole

00:00:36.440 --> 00:00:36.450
sentence and then memorize the whole
 

00:00:36.450 --> 00:00:39.350
sentence and then memorize the whole
sentence and store it in the activations

00:00:39.350 --> 00:00:39.360
sentence and store it in the activations
 

00:00:39.360 --> 00:00:42.229
sentence and store it in the activations
conveyed here and then for the purple

00:00:42.229 --> 00:00:42.239
conveyed here and then for the purple
 

00:00:42.239 --> 00:00:44.350
conveyed here and then for the purple
Network the decoding network to then

00:00:44.350 --> 00:00:44.360
Network the decoding network to then
 

00:00:44.360 --> 00:00:47.900
Network the decoding network to then
generate the English translation Jane

00:00:47.900 --> 00:00:47.910
generate the English translation Jane
 

00:00:47.910 --> 00:00:49.310
generate the English translation Jane
went to Africa last September enjoy the

00:00:49.310 --> 00:00:49.320
went to Africa last September enjoy the
 

00:00:49.320 --> 00:00:50.869
went to Africa last September enjoy the
culture many wonderful people she came

00:00:50.869 --> 00:00:50.879
culture many wonderful people she came
 

00:00:50.879 --> 00:00:52.250
culture many wonderful people she came
by raven but how wonderful trip wasn't

00:00:52.250 --> 00:00:52.260
by raven but how wonderful trip wasn't
 

00:00:52.260 --> 00:00:55.610
by raven but how wonderful trip wasn't
tempting we need to go to now the way a

00:00:55.610 --> 00:00:55.620
tempting we need to go to now the way a
 

00:00:55.620 --> 00:00:57.889
tempting we need to go to now the way a
human translator would translate the

00:00:57.889 --> 00:00:57.899
human translator would translate the
 

00:00:57.899 --> 00:01:00.560
human translator would translate the
sentence is not to first read the whole

00:01:00.560 --> 00:01:00.570
sentence is not to first read the whole
 

00:01:00.570 --> 00:01:02.420
sentence is not to first read the whole
French sentence and then memorize the

00:01:02.420 --> 00:01:02.430
French sentence and then memorize the
 

00:01:02.430 --> 00:01:04.310
French sentence and then memorize the
whole thing and then regurgitate an

00:01:04.310 --> 00:01:04.320
whole thing and then regurgitate an
 

00:01:04.320 --> 00:01:06.219
whole thing and then regurgitate an
English sentence from scratch instead

00:01:06.219 --> 00:01:06.229
English sentence from scratch instead
 

00:01:06.229 --> 00:01:10.160
English sentence from scratch instead
what a human translator would do is read

00:01:10.160 --> 00:01:10.170
what a human translator would do is read
 

00:01:10.170 --> 00:01:13.160
what a human translator would do is read
the first part of it maybe generate part

00:01:13.160 --> 00:01:13.170
the first part of it maybe generate part
 

00:01:13.170 --> 00:01:15.050
the first part of it maybe generate part
of the translation you know look at the

00:01:15.050 --> 00:01:15.060
of the translation you know look at the
 

00:01:15.060 --> 00:01:16.789
of the translation you know look at the
second part generate a few more words

00:01:16.789 --> 00:01:16.799
second part generate a few more words
 

00:01:16.799 --> 00:01:19.249
second part generate a few more words
look at a few more words generate a few

00:01:19.249 --> 00:01:19.259
look at a few more words generate a few
 

00:01:19.259 --> 00:01:20.240
look at a few more words generate a few
more words and so on

00:01:20.240 --> 00:01:20.250
more words and so on
 

00:01:20.250 --> 00:01:22.399
more words and so on
you kind of work part by part through

00:01:22.399 --> 00:01:22.409
you kind of work part by part through
 

00:01:22.409 --> 00:01:23.929
you kind of work part by part through
the sentence because it's just really

00:01:23.929 --> 00:01:23.939
the sentence because it's just really
 

00:01:23.939 --> 00:01:26.810
the sentence because it's just really
difficult to memorize the whole long

00:01:26.810 --> 00:01:26.820
difficult to memorize the whole long
 

00:01:26.820 --> 00:01:30.319
difficult to memorize the whole long
sentence like that and so what you see

00:01:30.319 --> 00:01:30.329
sentence like that and so what you see
 

00:01:30.329 --> 00:01:33.319
sentence like that and so what you see
for the encoder/decoder architecture

00:01:33.319 --> 00:01:33.329
for the encoder/decoder architecture
 

00:01:33.329 --> 00:01:36.649
for the encoder/decoder architecture
above is that it works quite well for

00:01:36.649 --> 00:01:36.659
above is that it works quite well for
 

00:01:36.659 --> 00:01:38.690
above is that it works quite well for
short sentences so my achievement

00:01:38.690 --> 00:01:38.700
short sentences so my achievement
 

00:01:38.700 --> 00:01:41.179
short sentences so my achievement
relatively high blue score but for very

00:01:41.179 --> 00:01:41.189
relatively high blue score but for very
 

00:01:41.189 --> 00:01:44.510
relatively high blue score but for very
long sentences maybe longer than 30 or

00:01:44.510 --> 00:01:44.520
long sentences maybe longer than 30 or
 

00:01:44.520 --> 00:01:46.910
long sentences maybe longer than 30 or
40 words you know the performance comes

00:01:46.910 --> 00:01:46.920
40 words you know the performance comes
 

00:01:46.920 --> 00:01:49.550
40 words you know the performance comes
down so the the blue score might look

00:01:49.550 --> 00:01:49.560
down so the the blue score might look
 

00:01:49.560 --> 00:01:52.520
down so the the blue score might look
like this as the sentence length varies

00:01:52.520 --> 00:01:52.530
like this as the sentence length varies
 

00:01:52.530 --> 00:01:55.490
like this as the sentence length varies
and short sentences are just hard to

00:01:55.490 --> 00:01:55.500
and short sentences are just hard to
 

00:01:55.500 --> 00:01:57.410
and short sentences are just hard to
translate how to get all the words right

00:01:57.410 --> 00:01:57.420
translate how to get all the words right
 

00:01:57.420 --> 00:02:01.490
translate how to get all the words right
and long sentences it doesn't do well on

00:02:01.490 --> 00:02:01.500
and long sentences it doesn't do well on
 

00:02:01.500 --> 00:02:03.350
and long sentences it doesn't do well on
because it's just difficult together in

00:02:03.350 --> 00:02:03.360
because it's just difficult together in
 

00:02:03.360 --> 00:02:05.300
because it's just difficult together in
your network to memorize a super long

00:02:05.300 --> 00:02:05.310
your network to memorize a super long
 

00:02:05.310 --> 00:02:08.630
your network to memorize a super long
sentence so in this and the next video

00:02:08.630 --> 00:02:08.640
sentence so in this and the next video
 

00:02:08.640 --> 00:02:11.869
sentence so in this and the next video
you see the attention model which

00:02:11.869 --> 00:02:11.879
you see the attention model which
 

00:02:11.879 --> 00:02:13.640
you see the attention model which
translates maybe a bit more like

00:02:13.640 --> 00:02:13.650
translates maybe a bit more like
 

00:02:13.650 --> 00:02:15.710
translates maybe a bit more like
humans might looking at part of a

00:02:15.710 --> 00:02:15.720
humans might looking at part of a
 

00:02:15.720 --> 00:02:18.319
humans might looking at part of a
sentence at a time and with an attention

00:02:18.319 --> 00:02:18.329
sentence at a time and with an attention
 

00:02:18.329 --> 00:02:20.300
sentence at a time and with an attention
model machine translation systems

00:02:20.300 --> 00:02:20.310
model machine translation systems
 

00:02:20.310 --> 00:02:22.899
model machine translation systems
performance can look like this because

00:02:22.899 --> 00:02:22.909
performance can look like this because
 

00:02:22.909 --> 00:02:25.580
performance can look like this because
by working one part of a sentence at a

00:02:25.580 --> 00:02:25.590
by working one part of a sentence at a
 

00:02:25.590 --> 00:02:28.759
by working one part of a sentence at a
time you don't see this huge dip which

00:02:28.759 --> 00:02:28.769
time you don't see this huge dip which
 

00:02:28.769 --> 00:02:30.530
time you don't see this huge dip which
is really measuring the ability of a

00:02:30.530 --> 00:02:30.540
is really measuring the ability of a
 

00:02:30.540 --> 00:02:32.270
is really measuring the ability of a
neural network to memorize a long

00:02:32.270 --> 00:02:32.280
neural network to memorize a long
 

00:02:32.280 --> 00:02:35.869
neural network to memorize a long
sentence which maybe isn't what we most

00:02:35.869 --> 00:02:35.879
sentence which maybe isn't what we most
 

00:02:35.879 --> 00:02:38.780
sentence which maybe isn't what we most
badly need in your own network to do so

00:02:38.780 --> 00:02:38.790
badly need in your own network to do so
 

00:02:38.790 --> 00:02:41.000
badly need in your own network to do so
in this video I want to just give you

00:02:41.000 --> 00:02:41.010
in this video I want to just give you
 

00:02:41.010 --> 00:02:44.869
in this video I want to just give you
some intuition about how attention works

00:02:44.869 --> 00:02:44.879
some intuition about how attention works
 

00:02:44.879 --> 00:02:46.879
some intuition about how attention works
and then we'll flesh out the details in

00:02:46.879 --> 00:02:46.889
and then we'll flesh out the details in
 

00:02:46.889 --> 00:02:50.270
and then we'll flesh out the details in
the next video the attention model was

00:02:50.270 --> 00:02:50.280
the next video the attention model was
 

00:02:50.280 --> 00:02:54.559
the next video the attention model was
due to zoomy tree bother new Kemper

00:02:54.559 --> 00:02:54.569
due to zoomy tree bother new Kemper
 

00:02:54.569 --> 00:02:56.990
due to zoomy tree bother new Kemper
intro and Yoshi Avenger and even though

00:02:56.990 --> 00:02:57.000
intro and Yoshi Avenger and even though
 

00:02:57.000 --> 00:02:59.210
intro and Yoshi Avenger and even though
it was originally developed for machine

00:02:59.210 --> 00:02:59.220
it was originally developed for machine
 

00:02:59.220 --> 00:03:01.490
it was originally developed for machine
translation is spread to many other

00:03:01.490 --> 00:03:01.500
translation is spread to many other
 

00:03:01.500 --> 00:03:04.369
translation is spread to many other
application areas as well and this is

00:03:04.369 --> 00:03:04.379
application areas as well and this is
 

00:03:04.379 --> 00:03:07.280
application areas as well and this is
really a very influential very seminal

00:03:07.280 --> 00:03:07.290
really a very influential very seminal
 

00:03:07.290 --> 00:03:10.240
really a very influential very seminal
paper in the deep learning literature

00:03:10.240 --> 00:03:10.250
paper in the deep learning literature
 

00:03:10.250 --> 00:03:13.280
paper in the deep learning literature
so let's illustrate this with a short

00:03:13.280 --> 00:03:13.290
so let's illustrate this with a short
 

00:03:13.290 --> 00:03:16.009
so let's illustrate this with a short
sentence even though these ideas were

00:03:16.009 --> 00:03:16.019
sentence even though these ideas were
 

00:03:16.019 --> 00:03:18.619
sentence even though these ideas were
maybe developed more for long sentences

00:03:18.619 --> 00:03:18.629
maybe developed more for long sentences
 

00:03:18.629 --> 00:03:20.479
maybe developed more for long sentences
but it'll be easier to illustrate these

00:03:20.479 --> 00:03:20.489
but it'll be easier to illustrate these
 

00:03:20.489 --> 00:03:22.789
but it'll be easier to illustrate these
ideas of a simpler example we have our

00:03:22.789 --> 00:03:22.799
ideas of a simpler example we have our
 

00:03:22.799 --> 00:03:25.610
ideas of a simpler example we have our
usual sentence jane visit murphy console

00:03:25.610 --> 00:03:25.620
usual sentence jane visit murphy console
 

00:03:25.620 --> 00:03:30.199
usual sentence jane visit murphy console
tom and let's say that we use a RNN and

00:03:30.199 --> 00:03:30.209
tom and let's say that we use a RNN and
 

00:03:30.209 --> 00:03:31.750
tom and let's say that we use a RNN and
in this case i'm going to use a

00:03:31.750 --> 00:03:31.760
in this case i'm going to use a
 

00:03:31.760 --> 00:03:36.009
in this case i'm going to use a
bi-directional RNN in order to compute

00:03:36.009 --> 00:03:36.019
bi-directional RNN in order to compute
 

00:03:36.019 --> 00:03:39.920
bi-directional RNN in order to compute
some set of features for each of the

00:03:39.920 --> 00:03:39.930
some set of features for each of the
 

00:03:39.930 --> 00:03:42.409
some set of features for each of the
input words and here i've drawn the

00:03:42.409 --> 00:03:42.419
input words and here i've drawn the
 

00:03:42.419 --> 00:03:45.530
input words and here i've drawn the
standard by there's the RNN with outputs

00:03:45.530 --> 00:03:45.540
standard by there's the RNN with outputs
 

00:03:45.540 --> 00:03:49.149
standard by there's the RNN with outputs
y 1 y 2 y 3 and so on up to y 5 but

00:03:49.149 --> 00:03:49.159
y 1 y 2 y 3 and so on up to y 5 but
 

00:03:49.159 --> 00:03:51.050
y 1 y 2 y 3 and so on up to y 5 but
we're not doing a word-for-word

00:03:51.050 --> 00:03:51.060
we're not doing a word-for-word
 

00:03:51.060 --> 00:03:53.930
we're not doing a word-for-word
translation so let me get rid of the Y's

00:03:53.930 --> 00:03:53.940
translation so let me get rid of the Y's
 

00:03:53.940 --> 00:03:57.199
translation so let me get rid of the Y's
on top but using a bi-directional RNN

00:03:57.199 --> 00:03:57.209
on top but using a bi-directional RNN
 

00:03:57.209 --> 00:03:59.300
on top but using a bi-directional RNN
what we're done is for each of the words

00:03:59.300 --> 00:03:59.310
what we're done is for each of the words
 

00:03:59.310 --> 00:04:01.250
what we're done is for each of the words
or really for each of the five positions

00:04:01.250 --> 00:04:01.260
or really for each of the five positions
 

00:04:01.260 --> 00:04:03.349
or really for each of the five positions
in the sentence you can compute a very

00:04:03.349 --> 00:04:03.359
in the sentence you can compute a very
 

00:04:03.359 --> 00:04:07.670
in the sentence you can compute a very
rich set of features about the words in

00:04:07.670 --> 00:04:07.680
rich set of features about the words in
 

00:04:07.680 --> 00:04:10.039
rich set of features about the words in
the sentence and maybe surrounding words

00:04:10.039 --> 00:04:10.049
the sentence and maybe surrounding words
 

00:04:10.049 --> 00:04:13.879
the sentence and maybe surrounding words
in every position now let's go ahead and

00:04:13.879 --> 00:04:13.889
in every position now let's go ahead and
 

00:04:13.889 --> 00:04:17.330
in every position now let's go ahead and
generate the English translation we're

00:04:17.330 --> 00:04:17.340
generate the English translation we're
 

00:04:17.340 --> 00:04:20.360
generate the English translation we're
going to use another RNN to generate the

00:04:20.360 --> 00:04:20.370
going to use another RNN to generate the
 

00:04:20.370 --> 00:04:23.600
going to use another RNN to generate the
English translation so you know here's

00:04:23.600 --> 00:04:23.610
English translation so you know here's
 

00:04:23.610 --> 00:04:26.390
English translation so you know here's
my RNN node as usual and instead of

00:04:26.390 --> 00:04:26.400
my RNN node as usual and instead of
 

00:04:26.400 --> 00:04:27.330
my RNN node as usual and instead of
using a

00:04:27.330 --> 00:04:27.340
using a
 

00:04:27.340 --> 00:04:29.730
using a
to denote the activation in order to

00:04:29.730 --> 00:04:29.740
to denote the activation in order to
 

00:04:29.740 --> 00:04:32.159
to denote the activation in order to
avoid confusion with the activations

00:04:32.159 --> 00:04:32.169
avoid confusion with the activations
 

00:04:32.169 --> 00:04:33.510
avoid confusion with the activations
down here I'm just going to use a

00:04:33.510 --> 00:04:33.520
down here I'm just going to use a
 

00:04:33.520 --> 00:04:36.480
down here I'm just going to use a
different notation I'm going to use s to

00:04:36.480 --> 00:04:36.490
different notation I'm going to use s to
 

00:04:36.490 --> 00:04:39.420
different notation I'm going to use s to
denote the hidden state in this RN and

00:04:39.420 --> 00:04:39.430
denote the hidden state in this RN and
 

00:04:39.430 --> 00:04:41.820
denote the hidden state in this RN and
up here and so instead of writing a 1

00:04:41.820 --> 00:04:41.830
up here and so instead of writing a 1
 

00:04:41.830 --> 00:04:45.960
up here and so instead of writing a 1
I'm going to write s 1 and so we hope in

00:04:45.960 --> 00:04:45.970
I'm going to write s 1 and so we hope in
 

00:04:45.970 --> 00:04:48.930
I'm going to write s 1 and so we hope in
this model that the first word that

00:04:48.930 --> 00:04:48.940
this model that the first word that
 

00:04:48.940 --> 00:04:51.990
this model that the first word that
generates will be Jane right to generate

00:04:51.990 --> 00:04:52.000
generates will be Jane right to generate
 

00:04:52.000 --> 00:04:55.920
generates will be Jane right to generate
Jane visits Africa in September now the

00:04:55.920 --> 00:04:55.930
Jane visits Africa in September now the
 

00:04:55.930 --> 00:04:58.050
Jane visits Africa in September now the
question is when you're trying to

00:04:58.050 --> 00:04:58.060
question is when you're trying to
 

00:04:58.060 --> 00:05:00.510
question is when you're trying to
generate this first word this output

00:05:00.510 --> 00:05:00.520
generate this first word this output
 

00:05:00.520 --> 00:05:03.450
generate this first word this output
what part of the input French sentence

00:05:03.450 --> 00:05:03.460
what part of the input French sentence
 

00:05:03.460 --> 00:05:06.330
what part of the input French sentence
should you be looking at seems like you

00:05:06.330 --> 00:05:06.340
should you be looking at seems like you
 

00:05:06.340 --> 00:05:08.100
should you be looking at seems like you
should be looking primarily at this

00:05:08.100 --> 00:05:08.110
should be looking primarily at this
 

00:05:08.110 --> 00:05:10.050
should be looking primarily at this
first word maybe in a few other words

00:05:10.050 --> 00:05:10.060
first word maybe in a few other words
 

00:05:10.060 --> 00:05:12.210
first word maybe in a few other words
close behind but you don't need to be

00:05:12.210 --> 00:05:12.220
close behind but you don't need to be
 

00:05:12.220 --> 00:05:14.310
close behind but you don't need to be
looking way at the end of the sentence

00:05:14.310 --> 00:05:14.320
looking way at the end of the sentence
 

00:05:14.320 --> 00:05:17.219
looking way at the end of the sentence
so what the attention model would be

00:05:17.219 --> 00:05:17.229
so what the attention model would be
 

00:05:17.229 --> 00:05:20.279
so what the attention model would be
computing is a set of attention weights

00:05:20.279 --> 00:05:20.289
computing is a set of attention weights
 

00:05:20.289 --> 00:05:26.400
computing is a set of attention weights
and we're going to use alpha 1 1 to

00:05:26.400 --> 00:05:26.410
and we're going to use alpha 1 1 to
 

00:05:26.410 --> 00:05:28.890
and we're going to use alpha 1 1 to
denote when you're generating the first

00:05:28.890 --> 00:05:28.900
denote when you're generating the first
 

00:05:28.900 --> 00:05:30.719
denote when you're generating the first
words how much should you be paying

00:05:30.719 --> 00:05:30.729
words how much should you be paying
 

00:05:30.729 --> 00:05:34.469
words how much should you be paying
attention to this first piece of

00:05:34.469 --> 00:05:34.479
attention to this first piece of
 

00:05:34.479 --> 00:05:37.200
attention to this first piece of
information here and then we'll also

00:05:37.200 --> 00:05:37.210
information here and then we'll also
 

00:05:37.210 --> 00:05:39.420
information here and then we'll also
come up with a second

00:05:39.420 --> 00:05:39.430
come up with a second
 

00:05:39.430 --> 00:05:43.050
come up with a second
that's called attention wait alpha 1 2

00:05:43.050 --> 00:05:43.060
that's called attention wait alpha 1 2
 

00:05:43.060 --> 00:05:45.029
that's called attention wait alpha 1 2
which tells us while we're trying to

00:05:45.029 --> 00:05:45.039
which tells us while we're trying to
 

00:05:45.039 --> 00:05:46.940
which tells us while we're trying to
compute the first word hopefully Jane

00:05:46.940 --> 00:05:46.950
compute the first word hopefully Jane
 

00:05:46.950 --> 00:05:51.480
compute the first word hopefully Jane
how much attention how much the tension

00:05:51.480 --> 00:05:51.490
how much attention how much the tension
 

00:05:51.490 --> 00:05:53.790
how much attention how much the tension
to repaint to this second word from the

00:05:53.790 --> 00:05:53.800
to repaint to this second word from the
 

00:05:53.800 --> 00:05:58.890
to repaint to this second word from the
input and so on and alpha 1 3 and so on

00:05:58.890 --> 00:05:58.900
input and so on and alpha 1 3 and so on
 

00:05:58.900 --> 00:06:01.589
input and so on and alpha 1 3 and so on
and together this will tell us what is

00:06:01.589 --> 00:06:01.599
and together this will tell us what is
 

00:06:01.599 --> 00:06:04.529
and together this will tell us what is
exactly the context come to know the

00:06:04.529 --> 00:06:04.539
exactly the context come to know the
 

00:06:04.539 --> 00:06:06.990
exactly the context come to know the
seat that we should be paying attention

00:06:06.990 --> 00:06:07.000
seat that we should be paying attention
 

00:06:07.000 --> 00:06:11.400
seat that we should be paying attention
to and that is input to this RN and unit

00:06:11.400 --> 00:06:11.410
to and that is input to this RN and unit
 

00:06:11.410 --> 00:06:15.510
to and that is input to this RN and unit
2 then try to generate the first word so

00:06:15.510 --> 00:06:15.520
2 then try to generate the first word so
 

00:06:15.520 --> 00:06:17.219
2 then try to generate the first word so
that's one step of the RNN there will

00:06:17.219 --> 00:06:17.229
that's one step of the RNN there will
 

00:06:17.229 --> 00:06:19.020
that's one step of the RNN there will
fashion allow these details in the next

00:06:19.020 --> 00:06:19.030
fashion allow these details in the next
 

00:06:19.030 --> 00:06:23.490
fashion allow these details in the next
video for the second step of this RNN

00:06:23.490 --> 00:06:23.500
video for the second step of this RNN
 

00:06:23.500 --> 00:06:26.850
video for the second step of this RNN
we're going to have a new hidden state

00:06:26.850 --> 00:06:26.860
we're going to have a new hidden state
 

00:06:26.860 --> 00:06:29.550
we're going to have a new hidden state
s2 and we're going to have a new set of

00:06:29.550 --> 00:06:29.560
s2 and we're going to have a new set of
 

00:06:29.560 --> 00:06:33.350
s2 and we're going to have a new set of
attention weights so we're going to have

00:06:33.350 --> 00:06:33.360
attention weights so we're going to have
 

00:06:33.360 --> 00:06:37.080
attention weights so we're going to have
alpha 2 1 to tell us when we're

00:06:37.080 --> 00:06:37.090
alpha 2 1 to tell us when we're
 

00:06:37.090 --> 00:06:37.840
alpha 2 1 to tell us when we're
generating this

00:06:37.840 --> 00:06:37.850
generating this
 

00:06:37.850 --> 00:06:40.300
generating this
I guess this would be visits maybe

00:06:40.300 --> 00:06:40.310
I guess this would be visits maybe
 

00:06:40.310 --> 00:06:43.120
I guess this would be visits maybe
without being the grantee label how much

00:06:43.120 --> 00:06:43.130
without being the grantee label how much
 

00:06:43.130 --> 00:06:44.650
without being the grantee label how much
should be paying attention to the first

00:06:44.650 --> 00:06:44.660
should be paying attention to the first
 

00:06:44.660 --> 00:06:48.040
should be paying attention to the first
word in the French input and also alpha

00:06:48.040 --> 00:06:48.050
word in the French input and also alpha
 

00:06:48.050 --> 00:06:51.640
word in the French input and also alpha
to 2 and so on how much should be paying

00:06:51.640 --> 00:06:51.650
to 2 and so on how much should be paying
 

00:06:51.650 --> 00:06:53.320
to 2 and so on how much should be paying
attention to the word visits how much

00:06:53.320 --> 00:06:53.330
attention to the word visits how much
 

00:06:53.330 --> 00:06:55.030
attention to the word visits how much
should I pay attention to their freak

00:06:55.030 --> 00:06:55.040
should I pay attention to their freak
 

00:06:55.040 --> 00:06:57.550
should I pay attention to their freak
and so on and of course the first one

00:06:57.550 --> 00:06:57.560
and so on and of course the first one
 

00:06:57.560 --> 00:07:00.430
and so on and of course the first one
regenerator Jane is also an infant to

00:07:00.430 --> 00:07:00.440
regenerator Jane is also an infant to
 

00:07:00.440 --> 00:07:03.820
regenerator Jane is also an infant to
this and it read some context they were

00:07:03.820 --> 00:07:03.830
this and it read some context they were
 

00:07:03.830 --> 00:07:05.590
this and it read some context they were
paying attention to the second step

00:07:05.590 --> 00:07:05.600
paying attention to the second step
 

00:07:05.600 --> 00:07:07.900
paying attention to the second step
there's also an input and that together

00:07:07.900 --> 00:07:07.910
there's also an input and that together
 

00:07:07.910 --> 00:07:10.870
there's also an input and that together
will generate the second word and that

00:07:10.870 --> 00:07:10.880
will generate the second word and that
 

00:07:10.880 --> 00:07:15.910
will generate the second word and that
leads us to the third step s Li this is

00:07:15.910 --> 00:07:15.920
leads us to the third step s Li this is
 

00:07:15.920 --> 00:07:18.850
leads us to the third step s Li this is
an input and we have some new context

00:07:18.850 --> 00:07:18.860
an input and we have some new context
 

00:07:18.860 --> 00:07:21.640
an input and we have some new context
see that depends on you know the various

00:07:21.640 --> 00:07:21.650
see that depends on you know the various
 

00:07:21.650 --> 00:07:24.940
see that depends on you know the various
alpha me for the different time steps it

00:07:24.940 --> 00:07:24.950
alpha me for the different time steps it
 

00:07:24.950 --> 00:07:26.680
alpha me for the different time steps it
tells us how much should we be paying

00:07:26.680 --> 00:07:26.690
tells us how much should we be paying
 

00:07:26.690 --> 00:07:28.840
tells us how much should we be paying
attention to the different words from

00:07:28.840 --> 00:07:28.850
attention to the different words from
 

00:07:28.850 --> 00:07:33.400
attention to the different words from
the input French sentence and so on so

00:07:33.400 --> 00:07:33.410
the input French sentence and so on so
 

00:07:33.410 --> 00:07:35.560
the input French sentence and so on so
some things I haven't specified yet but

00:07:35.560 --> 00:07:35.570
some things I haven't specified yet but
 

00:07:35.570 --> 00:07:37.870
some things I haven't specified yet but
that will go further into detail in the

00:07:37.870 --> 00:07:37.880
that will go further into detail in the
 

00:07:37.880 --> 00:07:41.080
that will go further into detail in the
next video is how exactly this context

00:07:41.080 --> 00:07:41.090
next video is how exactly this context
 

00:07:41.090 --> 00:07:43.450
next video is how exactly this context
defined and the goal of the context is

00:07:43.450 --> 00:07:43.460
defined and the goal of the context is
 

00:07:43.460 --> 00:07:45.490
defined and the goal of the context is
for the third word is to really should

00:07:45.490 --> 00:07:45.500
for the third word is to really should
 

00:07:45.500 --> 00:07:46.930
for the third word is to really should
capture that maybe you should be looking

00:07:46.930 --> 00:07:46.940
capture that maybe you should be looking
 

00:07:46.940 --> 00:07:52.360
capture that maybe you should be looking
around this part of the sentence and in

00:07:52.360 --> 00:07:52.370
around this part of the sentence and in
 

00:07:52.370 --> 00:07:54.910
around this part of the sentence and in
the formula you used to do that will

00:07:54.910 --> 00:07:54.920
the formula you used to do that will
 

00:07:54.920 --> 00:07:57.850
the formula you used to do that will
defer to the next video as well as how

00:07:57.850 --> 00:07:57.860
defer to the next video as well as how
 

00:07:57.860 --> 00:08:00.160
defer to the next video as well as how
do you compute these attention weights

00:08:00.160 --> 00:08:00.170
do you compute these attention weights
 

00:08:00.170 --> 00:08:04.330
do you compute these attention weights
and you see in the next video that alpha

00:08:04.330 --> 00:08:04.340
and you see in the next video that alpha
 

00:08:04.340 --> 00:08:06.310
and you see in the next video that alpha
VT which is when you're trying to

00:08:06.310 --> 00:08:06.320
VT which is when you're trying to
 

00:08:06.320 --> 00:08:08.560
VT which is when you're trying to
generate the third word they're supposed

00:08:08.560 --> 00:08:08.570
generate the third word they're supposed
 

00:08:08.570 --> 00:08:10.360
generate the third word they're supposed
to be Africa just getting the right

00:08:10.360 --> 00:08:10.370
to be Africa just getting the right
 

00:08:10.370 --> 00:08:16.330
to be Africa just getting the right
output the amount that this RNN step

00:08:16.330 --> 00:08:16.340
output the amount that this RNN step
 

00:08:16.340 --> 00:08:18.820
output the amount that this RNN step
should be paying attention to the French

00:08:18.820 --> 00:08:18.830
should be paying attention to the French
 

00:08:18.830 --> 00:08:23.620
should be paying attention to the French
word that time T that depends on the

00:08:23.620 --> 00:08:23.630
word that time T that depends on the
 

00:08:23.630 --> 00:08:26.680
word that time T that depends on the
activations of the bi-directional RNN at

00:08:26.680 --> 00:08:26.690
activations of the bi-directional RNN at
 

00:08:26.690 --> 00:08:30.670
activations of the bi-directional RNN at
time T so it goes so depend on the

00:08:30.670 --> 00:08:30.680
time T so it goes so depend on the
 

00:08:30.680 --> 00:08:32.709
time T so it goes so depend on the
foreign activations and the backward

00:08:32.709 --> 00:08:32.719
foreign activations and the backward
 

00:08:32.719 --> 00:08:35.620
foreign activations and the backward
activations at time T and it will depend

00:08:35.620 --> 00:08:35.630
activations at time T and it will depend
 

00:08:35.630 --> 00:08:37.900
activations at time T and it will depend
on the state from the previous step so

00:08:37.900 --> 00:08:37.910
on the state from the previous step so
 

00:08:37.910 --> 00:08:41.140
on the state from the previous step so
it depends on s2 and these things

00:08:41.140 --> 00:08:41.150
it depends on s2 and these things
 

00:08:41.150 --> 00:08:43.330
it depends on s2 and these things
together will influence how much you pay

00:08:43.330 --> 00:08:43.340
together will influence how much you pay
 

00:08:43.340 --> 00:08:45.220
together will influence how much you pay
attention to a specific word than the

00:08:45.220 --> 00:08:45.230
attention to a specific word than the
 

00:08:45.230 --> 00:08:47.530
attention to a specific word than the
input French sentence but we'll fetch

00:08:47.530 --> 00:08:47.540
input French sentence but we'll fetch
 

00:08:47.540 --> 00:08:49.950
input French sentence but we'll fetch
all these details in the next

00:08:49.950 --> 00:08:49.960
all these details in the next
 

00:08:49.960 --> 00:08:52.540
all these details in the next
but the key intuition to take away is

00:08:52.540 --> 00:08:52.550
but the key intuition to take away is
 

00:08:52.550 --> 00:08:56.110
but the key intuition to take away is
that this way the RNN marches forward

00:08:56.110 --> 00:08:56.120
that this way the RNN marches forward
 

00:08:56.120 --> 00:08:58.390
that this way the RNN marches forward
generating one word at a time until

00:08:58.390 --> 00:08:58.400
generating one word at a time until
 

00:08:58.400 --> 00:09:02.769
generating one word at a time until
eventually it generates the EOS and at

00:09:02.769 --> 00:09:02.779
eventually it generates the EOS and at
 

00:09:02.779 --> 00:09:05.950
eventually it generates the EOS and at
every step there are these attention

00:09:05.950 --> 00:09:05.960
every step there are these attention
 

00:09:05.960 --> 00:09:09.280
every step there are these attention
ways alpha TT prime that tells it when

00:09:09.280 --> 00:09:09.290
ways alpha TT prime that tells it when
 

00:09:09.290 --> 00:09:10.980
ways alpha TT prime that tells it when
they're trying to generate the teeth

00:09:10.980 --> 00:09:10.990
they're trying to generate the teeth
 

00:09:10.990 --> 00:09:13.480
they're trying to generate the teeth
English word how much should you be

00:09:13.480 --> 00:09:13.490
English word how much should you be
 

00:09:13.490 --> 00:09:16.030
English word how much should you be
paying attention to the T prime French

00:09:16.030 --> 00:09:16.040
paying attention to the T prime French
 

00:09:16.040 --> 00:09:18.820
paying attention to the T prime French
word and this allows it on every time

00:09:18.820 --> 00:09:18.830
word and this allows it on every time
 

00:09:18.830 --> 00:09:21.519
word and this allows it on every time
step to look only maybe within a local

00:09:21.519 --> 00:09:21.529
step to look only maybe within a local
 

00:09:21.529 --> 00:09:23.800
step to look only maybe within a local
window of the French sentence that pay

00:09:23.800 --> 00:09:23.810
window of the French sentence that pay
 

00:09:23.810 --> 00:09:26.680
window of the French sentence that pay
attention to when generating a specific

00:09:26.680 --> 00:09:26.690
attention to when generating a specific
 

00:09:26.690 --> 00:09:29.320
attention to when generating a specific
English word so I hope this video

00:09:29.320 --> 00:09:29.330
English word so I hope this video
 

00:09:29.330 --> 00:09:31.660
English word so I hope this video
conveyed some intuition about the

00:09:31.660 --> 00:09:31.670
conveyed some intuition about the
 

00:09:31.670 --> 00:09:33.610
conveyed some intuition about the
attention model and then you now have a

00:09:33.610 --> 00:09:33.620
attention model and then you now have a
 

00:09:33.620 --> 00:09:35.950
attention model and then you now have a
rough sense of maybe how the algorithm

00:09:35.950 --> 00:09:35.960
rough sense of maybe how the algorithm
 

00:09:35.960 --> 00:09:37.960
rough sense of maybe how the algorithm
works let's go onto the next video to

00:09:37.960 --> 00:09:37.970
works let's go onto the next video to
 

00:09:37.970 --> 00:09:39.670
works let's go onto the next video to
flesh out the details of the attention

00:09:39.670 --> 00:09:39.680
flesh out the details of the attention
 

00:09:39.680 --> 00:09:42.190
flesh out the details of the attention
model

