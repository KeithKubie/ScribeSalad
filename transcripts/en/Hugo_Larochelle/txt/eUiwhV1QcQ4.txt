Speaker 1:          00:00          In this video, we'll see set cop processing, which is a very useful preprocessing to apply to our data before we run a sparse coding algorithm. So sparse coding is trying to learn about the structure, the statistical structure of our input data, and we find in practice that it's often beneficial to remove a part of the structure, which is a quote unquote obvious, uh, which is not particularly interesting and, uh, would not correspond to, will not allow us to extract, uh, uh, interesting features. Uh, so for instance, the mean of the data is not something that's particularly interesting for representing our different data points into a feature vector Leighton representation and also some of the, uh, linner, uh, covert. So linear correlations or the covariants essentially of our data is also not a kind of statistical structure of dependence that's particularly interesting. And so for that reason, what we often do in that ca preprocessing, it's just removed a mean and also, uh, uh, changed the, uh, do preprocessing such that the empirical covariance of our data is going to be the identity, uh, which would mean that the covariance between the preprocess data, uh, between the elements of the vectors in the process data is going to be zero.

Speaker 1:          01:22          And then the variance of each element in RP process data, uh, is going to be one. And so that's a process known as, as whitening. And so, uh, and uh, statistical jargon will say that by doing this we'll remove the first and second order statistical structure in our data. And so, uh, it means that our sparse calling algorithm will then perhaps extract structure that's a bit more complicated than higher order. And so, uh, does that say be Ross things very simple. I just described the procedure. So what we do is first we compute the empirical mean of our training data as well as the empirical covariance Matrix. So new hat will be empirical mean and the Sigma Hat, the empirical co-parents Matrix. And we also assume that, uh, we have computed a Egan value, Egan vector, a decomposition of our matrix. So here the, and vectors are the columns of the Matrix, you and then, uh, here, lambda has a zero on it's off diagonal terms.

Speaker 1:          02:26          It's a square matrix with zero, and it's off diagonal terms and the diagonal terms, uh, uh, correspond to the EIG and values that are sorted. And now's it CEO said,Z is simply a linear transform that takes, I give an input and pre process it as follows. It takes the input, subtracts the empirical mean, and then multiply by this transformation here, which is similar to this one here. So the difference is only that the diagonal matrix in the middle is going to be lambda where taken the diagonal terms. And then, uh, we've, uh, taken the exponential, uh, as so exponentiate it by minus one hat. So in other words, the diagonal terms are going to be one over the square root of the diagonal terms of this lambda matrix here. Uh, so the Egan values of our empirical covariance Matrix. Okay. So now let's see how this transformation actually removes the mean and also make sure that the empirical covariance matrix is now the identity matrix.

Speaker 1:          03:36          So first with the empirical mean, now, if we take the empirical mean of our preprocess data, it means we take one over t times the sum over all of our training data of the transformed data. So x t minus the empirical mean multiply by the transformation matrix. Uh, now because this term, this matrix here does not depend on t and nor does the empirical mean that I can put the sum right inside here. So I can have that. This is equal to d linear transformation times the, uh, average of x t minus the empirical me. And now the empirical means just the average of the XT. So this is just sigma, uh, sorry, a new hat. And so we have new hat minus when you have to, which is zero times distance formation, that's going to be zero. So we do have that, the empirical mean is equal to zero. Once we've performed this transformation,

Speaker 2:          04:34          yeah.

Speaker 1:          04:35          Now let's show that the empirical covariance matrix is also now the identity. Once we've done this preprocessing. So, uh, here's the expression for the, uh, empirical covariance Matrix for our transformed data. So it's the outer product of the transformed data with itself. Uh, so normally you'd expect minus the empirical mean, uh, but we've shown that it's zero. So we just have the other products of the, uh, transform factors. And now, uh, similarly, uh, because this matrix here and you a hat, this matrix here and you had here does not depend on tea, then I can take the sum and uh, introduce it a further into the expression. So I'll have one over t minus one times some of our t of these terms here. Uh, which, so where x t depends on, uh, on,

Speaker 2:          05:29          uh, on team. Um,

Speaker 1:          05:32          and so we have this expression. So essentially I've been able to factor out that transformation here out and hear out and put this, um,

Speaker 2:          05:41          with Eh, within here or right here. Uh, so

Speaker 1:          05:45          I actually haven't had to use the fact that you have, doesn't depend on TV. And now we notice that this is actually the empirical covariance Matrix. So this is a similar hat. Now, Sigma Hat in its, uh, Egan vector, Egan value decomposition is equal to this. And now, well, we have a bunch of terms are going to cancel out. So first, because the Egan vectors are auto normal. Then if I take the, uh, a matrix, you, uh, you transpose, so you transpose, it's rows are the Egan vectors times the Matrix. You whose columns are the agon vectors, then that's going to be the entity. So zero off diagonal, then one on the diagonal because the vectors are a orthogonal and they're normalized. So this disappears. This disappears for the same reason. Now I have the a minus. Uh, so the inverse square root of the elements of a Lambda Matrix Times dilemma matrix.

Speaker 1:          06:57          Well, it means that I can make that disappear and have just one half here. Now I have Landon one half, uh, times, uh, lambda minus one half. So again, this is going to cancel out. That's going to be the identity. So now I have you times you transpose a, which is equivalent to the Egan Value Egan vectors. So if I have the essentially have in the middle, right here as if I had the identity. So I have you times the identity times you transpose, which is, which gets, can be understood as the a possible Egan value. We can vector decomposition of the identity matrix. And so that means that you times and entity times you transpose is really just the identity matrix. Okay. So we've shown that if we take the covariance Matrix, the empirical covariance Matrix of the transformed data with Etsy processing, we get that. Now this covariance Matrix has, uh, is the identity.

Speaker 1:          07:59          So the variants of all the elements in the transform vectors in our training set is one, the empirical barons and the covariance between each a pair of, uh, uh, different elements in these transform vectors is also zero, right? So we've seen that desi pre processing removes the mean and in terms of our data so that the elements are El linearly independent, so they're not core linearly correlated. And if we do this, then empirically we find that we, after performing district processing, we run sparse coding. We tend to get a better, a better feature representation of our data.