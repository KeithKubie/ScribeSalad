WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:00.590
HI Michael.

00:00:00.590 --> 00:00:01.580
&gt;&gt; Hey how's it going?

00:00:01.580 --> 00:00:04.270
&gt;&gt; So I want to talk about something today Michael.

00:00:04.270 --> 00:00:07.930
I want to talk about Bayesian Learning, and I've been inspired

00:00:07.930 --> 00:00:11.160
by our last discussion on learning theory to think a

00:00:11.160 --> 00:00:13.740
little bit more about what it is exactly that we're trying

00:00:13.740 --> 00:00:16.460
to do. I'm in the mood beyond specific algorithms to

00:00:16.460 --> 00:00:19.310
just think more generally The sort that learning people want us

00:00:19.310 --> 00:00:21.380
to do, learning theory people want us to do and I

00:00:21.380 --> 00:00:23.790
think Bayesian Learning is a nice place to start. Sound fair?

00:00:23.790 --> 00:00:25.350
&gt;&gt; Yeah, that sounds really cool, I think

00:00:25.350 --> 00:00:29.230
that might be a nice formal framework for thinking about some of these problems.

00:00:29.230 --> 00:00:31.930
&gt;&gt; Good. Good. So, I'm going to start out. By

00:00:31.930 --> 00:00:34.170
making a few assertions, which I hope you will

00:00:34.170 --> 00:00:37.260
agree with, and if you agree with this then

00:00:37.260 --> 00:00:39.000
we'll be able to kind of move forward and ask

00:00:39.000 --> 00:00:42.420
some pretty cool questions okay? So Bayesian learning, so

00:00:42.420 --> 00:00:44.950
the kind of idea here behind Bayesian learning is

00:00:44.950 --> 00:00:48.590
this sort of fundamental Underlying assumption about what we're

00:00:48.590 --> 00:00:50.420
trying to do with the machine learning. So, I've written

00:00:50.420 --> 00:00:52.310
it down here, here's what I'm going to claim

00:00:52.310 --> 00:00:54.090
we're trying to do. We are trying to learn

00:00:54.090 --> 00:00:59.310
the best hypothesis we can given some data and

00:00:59.310 --> 00:01:01.790
some domain knowledge. Do you buy that as an assertion?

00:01:01.790 --> 00:01:04.819
&gt;&gt; Yeah, it's, and pretty much everything we've talked about so far

00:01:04.819 --> 00:01:07.880
has had a form kind of like that. We're searching through a hypothesis

00:01:07.880 --> 00:01:12.330
base and As you've pointed out on multiple occasions there's this kind

00:01:12.330 --> 00:01:15.670
of extra domain knowledge that comes into play for example when you pick

00:01:15.670 --> 00:01:19.143
a like a similarity metric for something like k nearest neighbors

00:01:19.143 --> 00:01:20.830
&gt;&gt; Right and of course we always have the

00:01:20.830 --> 00:01:23.300
data because we're machine learning people and we always have

00:01:23.300 --> 00:01:26.580
data. So this is what we've been trying to do

00:01:26.580 --> 00:01:29.240
and I'm going to suggest that we can be a

00:01:29.240 --> 00:01:31.830
little bit more precise about what we mean by

00:01:31.830 --> 00:01:33.930
best and I'm going to try to do that and

00:01:33.930 --> 00:01:37.510
see if you agree with me. Okay, so I'm going

00:01:37.510 --> 00:01:40.680
to rewrite what I've written already except I'm replacing best

00:01:40.680 --> 00:01:44.760
with most probable. Okay. So what I'm going to claim is that what

00:01:44.760 --> 00:01:47.340
we've really been trying to do with all these algorithms we're doing is

00:01:47.340 --> 00:01:51.960
we're trying to learn the most likely or the most probable hypothesis

00:01:51.960 --> 00:01:55.440
given the data and whatever domain knowledge we bring to bear. You buy that?

00:01:55.440 --> 00:02:00.050
&gt;&gt; Interesting. I'm not sure yet. I mean, so is it

00:02:00.050 --> 00:02:03.780
the hypothesis that it's most likely to be returned by the algorithm?

00:02:03.780 --> 00:02:05.830
&gt;&gt; No, it's the hypothesis

00:02:05.830 --> 00:02:09.210
that we think is most likely, given the data that we've seen.

00:02:09.210 --> 00:02:12.570
Given the training set and given whatever domain knowledge that we bring to

00:02:12.570 --> 00:02:15.120
bear on the problem, the best hypothesis is the one that is

00:02:15.120 --> 00:02:20.310
most likely, that is Most probable. Or most l, probably the correct one.

00:02:20.310 --> 00:02:23.640
&gt;&gt; Interesting. Well, are we going to be able to connect that to what we

00:02:23.640 --> 00:02:25.410
were talking before? Which is generally we were

00:02:25.410 --> 00:02:28.490
selecting hypotheses based on things like their error.

00:02:28.490 --> 00:02:31.060
&gt;&gt; Yes. Exactly. We are going to be able to connect that.

00:02:31.060 --> 00:02:32.900
We are definitly going to be able to connect that. But.

00:02:32.900 --> 00:02:33.300
&gt;&gt; Okay.

00:02:33.300 --> 00:02:35.150
&gt;&gt; I can;t go forward unless I can convince

00:02:35.150 --> 00:02:38.080
you that it's reasonable to at least start out thinking

00:02:38.080 --> 00:02:41.440
about best being the same thing as most probable. Yeah,

00:02:41.440 --> 00:02:43.300
I'm willing to go forward with this. It sounds interesting.

00:02:43.300 --> 00:02:45.900
&gt;&gt; So if you're willing to move forward with this, then I want to write

00:02:45.900 --> 00:02:49.040
one more thing down and then we can sort of dive into it. So if

00:02:49.040 --> 00:02:52.400
you buy that we're trying to learn the most probable hypothesis, the most likely

00:02:52.400 --> 00:02:56.840
one, the one that has the highest chance of being correct given the data, and

00:02:56.840 --> 00:02:58.890
our domain knowledge, then we can write that

00:02:58.890 --> 00:03:00.950
down in math speak pretty simply. It's the

00:03:00.950 --> 00:03:04.710
probability of, some particular hypothesis h, drawn from

00:03:04.710 --> 00:03:07.410
some hypothesis class. Given some amount of data

00:03:07.410 --> 00:03:08.840
which I'm just going to refer to as D

00:03:08.840 --> 00:03:11.440
from now on. Okay? And that's just, exactly

00:03:11.440 --> 00:03:14.100
what we said just above when we talk

00:03:14.100 --> 00:03:17.120
about the most probable age, given the data. Okay?

00:03:17.120 --> 00:03:19.270
&gt;&gt; Well wait. Two things. One is so D

00:03:19.270 --> 00:03:21.420
is not distribution which we've had in the past.

00:03:21.420 --> 00:03:21.910
&gt;&gt; That's true.
&gt;&gt; So I

00:03:21.910 --> 00:03:25.130
guess as long as we keep that straight. And the other one is

00:03:25.130 --> 00:03:26.730
No that's, you're just telling me

00:03:26.730 --> 00:03:29.280
the probability of some particular hypothesis h.

00:03:29.280 --> 00:03:31.580
&gt;&gt; That's right. So, we want to somehow,

00:03:31.580 --> 00:03:34.620
given this quantity we want to find, the

00:03:34.620 --> 00:03:40.010
best or the, most likely, of the hypothesis given this. Does that make sense?

00:03:40.010 --> 00:03:40.360
&gt;&gt; Yes.

00:03:40.360 --> 00:03:47.220
&gt;&gt; So we want to find the argmax, of h, drawn from Your hypothesis class.

00:03:47.220 --> 00:03:49.260
That is we want to find the hypothesis drawn from

00:03:49.260 --> 00:03:52.800
the hypothesis class that has the highest probability given the data.

00:03:52.800 --> 00:03:53.400
&gt;&gt; Perfect.

00:03:53.400 --> 00:03:57.250
&gt;&gt; Okay, good. So we're going to spend the next 45 hours.

00:03:57.250 --> 00:03:59.039
&gt;&gt; [LAUGH]

00:03:59.039 --> 00:04:00.310
&gt;&gt; Exploring this expression.

00:04:00.310 --> 00:04:03.480
&gt;&gt; Okay so that's like what, like 6 hours per letter.

00:04:03.480 --> 00:04:06.000
&gt;&gt; [LAUGH] Yeah and that's fine because its important.

