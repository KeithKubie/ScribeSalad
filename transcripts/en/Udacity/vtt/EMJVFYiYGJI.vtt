WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:04.480
Recall the Lemma that relates the number
of misses in an LRU cache to the number

00:00:04.480 --> 00:00:06.750
in one with optimal replacement.

00:00:06.750 --> 00:00:10.090
&gt;From this Lemma there's
a corollary about regularity.

00:00:10.090 --> 00:00:15.310
You can design an algorithm for an ideal
cache and if that analysis shows that

00:00:15.310 --> 00:00:20.480
the number of misses is regular, then
an LRU cache will perform just as well.

00:00:20.480 --> 00:00:22.500
So let's see how to use this.

00:00:22.500 --> 00:00:24.640
Suppose you design a matrix
multiply algorithm,

00:00:24.640 --> 00:00:30.020
assuming a conventional non stronsense
scheme and you do so for an ideal cache.

00:00:30.020 --> 00:00:34.310
Let's say that your analysis shows your
algorithm does n cubed over L root z,

00:00:34.310 --> 00:00:35.170
misses.

00:00:35.170 --> 00:00:39.080
For this example let's further assume
that the line size is one word.

00:00:39.080 --> 00:00:43.450
So will this algorithm perform well
on a machine with an LRU cache?

00:00:43.450 --> 00:00:45.570
Let's check the regularity condition.

00:00:45.570 --> 00:00:48.060
What happens when you
double the cash size?

00:00:48.060 --> 00:00:51.450
Well, it introduces just
a small constant factor.

00:00:51.450 --> 00:00:54.250
So the regularity
condition is satisfied.

00:00:54.250 --> 00:00:58.780
And because Qopt is regular,
QLRU will be proportional to it.

00:00:58.780 --> 00:01:02.660
By the Lemma, you could also evaluate
the right-hand side of this relation.

00:01:02.660 --> 00:01:04.060
Here's what you get.

00:01:04.060 --> 00:01:07.550
Basically an upper bound on
the number of LRU misses.

00:01:07.550 --> 00:01:10.320
This brings us to one of
life's great pleasures,

00:01:10.320 --> 00:01:13.640
well assuming you're a bit of a geek,
or is it nerd?

00:01:13.640 --> 00:01:15.900
Let's just say enthusiast, shall we?

00:01:15.900 --> 00:01:18.740
The pleasure is that
the Lemma is not hard to see.

00:01:18.740 --> 00:01:22.860
For simplicity, let's assume that
the transfer size is one word.

00:01:22.860 --> 00:01:25.740
Now imagine the sequence,
or trace, of all load and

00:01:25.740 --> 00:01:28.430
store operations performed by a program.

00:01:28.430 --> 00:01:30.820
Divide this trace into phases.

00:01:30.820 --> 00:01:33.360
Let's define these
phases in a special way.

00:01:33.360 --> 00:01:38.690
In particular, suppose that each phase
references exactly Z unique addresses.

00:01:38.690 --> 00:01:43.220
Just to be clear, each phase might
contain many more than Z loads and

00:01:43.220 --> 00:01:44.220
stores.

00:01:44.220 --> 00:01:47.020
The phases might also have widely
varying numbers of loads and

00:01:47.020 --> 00:01:49.560
stores as the illustration suggests, but

00:01:49.560 --> 00:01:54.680
the number of unique addresses is
always exactly Z in each phase.

00:01:54.680 --> 00:01:58.740
Now let's say the machine
has an LRU cache of size Z.

00:01:58.740 --> 00:02:01.870
Think about what happens during
some phase, let's say phase i.

00:02:01.870 --> 00:02:05.960
At the beginning of the phase,
it's possible that the cache is full.

00:02:05.960 --> 00:02:09.340
So, the first time each unique
address of phase i is seen,

00:02:09.340 --> 00:02:11.360
it could cause an eviction.

00:02:11.360 --> 00:02:13.320
Thus, by the definition of a phase,

00:02:13.320 --> 00:02:16.650
the number of cache misses
could be as high as z.

00:02:16.650 --> 00:02:20.040
Now think about what happens on
a cache with optimal replacement, but

00:02:20.040 --> 00:02:22.480
half the capacity, Z over 2.

00:02:22.480 --> 00:02:27.170
Remember this optimal cache can gaze
into its crystal ball to see all future

00:02:27.170 --> 00:02:28.330
accesses.

00:02:28.330 --> 00:02:30.990
So during the previous phase i- 1,

00:02:30.990 --> 00:02:33.930
what might the optimal
replacement policy have done?

00:02:33.930 --> 00:02:39.110
Well it may have arranged itself so
that the first z/2 unique addresses seen

00:02:39.110 --> 00:02:44.350
in phase i are exactly in cache,
that would mean no evictions.

00:02:44.350 --> 00:02:48.630
But since the optimal cache is
only of size z/2, then it must,

00:02:48.630 --> 00:02:51.280
at that point, start loading addresses.

00:02:51.280 --> 00:02:52.820
In other words, the smaller but

00:02:52.820 --> 00:02:57.440
all seeing cache has to perform
at least z over 2 transfers.

00:02:57.440 --> 00:03:01.180
The competitiveness Lemma then
follows from these two facts.

00:03:01.180 --> 00:03:04.380
One is at most Z and
twice the other is at least Z.

