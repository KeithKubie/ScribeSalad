WEBVTT
Kind: captions
Language: en

00:00:02.570 --> 00:00:05.540 align:start position:0%
 
all<00:00:03.570><c> right</c><00:00:03.750><c> so</c><00:00:03.960><c> let's</c><00:00:04.170><c> get</c><00:00:04.290><c> started</c><00:00:04.740><c> so</c><00:00:04.950><c> thank</c>

00:00:05.540 --> 00:00:05.550 align:start position:0%
all right so let's get started so thank
 

00:00:05.550 --> 00:00:08.180 align:start position:0%
all right so let's get started so thank
you<00:00:05.700><c> all</c><00:00:05.730><c> for</c><00:00:06.120><c> coming</c><00:00:06.450><c> to</c><00:00:06.779><c> day</c><00:00:07.350><c> two</c><00:00:07.410><c> of</c><00:00:07.920><c> six</c>

00:00:08.180 --> 00:00:08.190 align:start position:0%
you all for coming to day two of six
 

00:00:08.190 --> 00:00:11.810 align:start position:0%
you all for coming to day two of six
s-191<00:00:09.080><c> we're</c><00:00:10.080><c> really</c><00:00:10.260><c> excited</c><00:00:10.590><c> to</c><00:00:10.949><c> to</c><00:00:11.639><c> have</c>

00:00:11.810 --> 00:00:11.820 align:start position:0%
s-191 we're really excited to to have
 

00:00:11.820 --> 00:00:13.699 align:start position:0%
s-191 we're really excited to to have
you<00:00:11.969><c> for</c><00:00:12.180><c> these</c><00:00:12.270><c> two</c><00:00:12.480><c> lectures</c><00:00:12.719><c> and</c><00:00:13.199><c> lab</c><00:00:13.469><c> today</c>

00:00:13.699 --> 00:00:13.709 align:start position:0%
you for these two lectures and lab today
 

00:00:13.709 --> 00:00:16.160 align:start position:0%
you for these two lectures and lab today
which<00:00:14.369><c> are</c><00:00:14.520><c> largely</c><00:00:14.850><c> going</c><00:00:15.059><c> to</c><00:00:15.360><c> be</c><00:00:15.449><c> focused</c><00:00:16.020><c> on</c>

00:00:16.160 --> 00:00:16.170 align:start position:0%
which are largely going to be focused on
 

00:00:16.170 --> 00:00:21.590 align:start position:0%
which are largely going to be focused on
deep<00:00:17.010><c> learning</c><00:00:17.220><c> for</c><00:00:17.789><c> computer</c><00:00:18.330><c> vision</c><00:00:19.730><c> so</c><00:00:20.730><c> to</c>

00:00:21.590 --> 00:00:21.600 align:start position:0%
deep learning for computer vision so to
 

00:00:21.600 --> 00:00:24.410 align:start position:0%
deep learning for computer vision so to
motivate<00:00:21.720><c> I</c><00:00:22.460><c> think</c><00:00:23.460><c> we</c><00:00:23.550><c> can</c><00:00:23.670><c> all</c><00:00:23.790><c> agree</c><00:00:23.910><c> that</c>

00:00:24.410 --> 00:00:24.420 align:start position:0%
motivate I think we can all agree that
 

00:00:24.420 --> 00:00:26.690 align:start position:0%
motivate I think we can all agree that
vision<00:00:24.870><c> is</c><00:00:25.050><c> one</c><00:00:25.440><c> of</c><00:00:25.710><c> the</c><00:00:25.950><c> most</c><00:00:26.130><c> important</c>

00:00:26.690 --> 00:00:26.700 align:start position:0%
vision is one of the most important
 

00:00:26.700 --> 00:00:29.000 align:start position:0%
vision is one of the most important
human<00:00:27.120><c> senses</c><00:00:27.720><c> and</c><00:00:27.960><c> sighted</c><00:00:28.320><c> people</c><00:00:28.500><c> rely</c><00:00:28.980><c> on</c>

00:00:29.000 --> 00:00:29.010 align:start position:0%
human senses and sighted people rely on
 

00:00:29.010 --> 00:00:31.880 align:start position:0%
human senses and sighted people rely on
vision<00:00:29.520><c> quite</c><00:00:30.060><c> a</c><00:00:30.090><c> lot</c><00:00:30.330><c> for</c><00:00:31.170><c> everything</c><00:00:31.560><c> from</c>

00:00:31.880 --> 00:00:31.890 align:start position:0%
vision quite a lot for everything from
 

00:00:31.890 --> 00:00:34.369 align:start position:0%
vision quite a lot for everything from
navigating<00:00:32.610><c> in</c><00:00:32.910><c> the</c><00:00:33.090><c> physical</c><00:00:33.449><c> world</c><00:00:33.510><c> to</c>

00:00:34.369 --> 00:00:34.379 align:start position:0%
navigating in the physical world to
 

00:00:34.379 --> 00:00:37.550 align:start position:0%
navigating in the physical world to
recognizing<00:00:34.950><c> and</c><00:00:35.430><c> manipulating</c><00:00:36.120><c> objects</c><00:00:36.690><c> to</c>

00:00:37.550 --> 00:00:37.560 align:start position:0%
recognizing and manipulating objects to
 

00:00:37.560 --> 00:00:39.590 align:start position:0%
recognizing and manipulating objects to
interpreting<00:00:38.250><c> facial</c><00:00:38.520><c> expressions</c><00:00:39.300><c> and</c>

00:00:39.590 --> 00:00:39.600 align:start position:0%
interpreting facial expressions and
 

00:00:39.600 --> 00:00:42.110 align:start position:0%
interpreting facial expressions and
understanding<00:00:40.320><c> emotion</c><00:00:40.860><c> and</c><00:00:41.370><c> I</c><00:00:41.790><c> think</c><00:00:41.970><c> it's</c>

00:00:42.110 --> 00:00:42.120 align:start position:0%
understanding emotion and I think it's
 

00:00:42.120 --> 00:00:44.300 align:start position:0%
understanding emotion and I think it's
safe<00:00:42.270><c> to</c><00:00:42.300><c> say</c><00:00:42.450><c> that</c><00:00:42.840><c> for</c><00:00:43.380><c> many</c><00:00:43.560><c> of</c><00:00:43.739><c> for</c><00:00:44.040><c> all</c><00:00:44.130><c> of</c>

00:00:44.300 --> 00:00:44.310 align:start position:0%
safe to say that for many of for all of
 

00:00:44.310 --> 00:00:46.580 align:start position:0%
safe to say that for many of for all of
us<00:00:44.370><c> or</c><00:00:44.579><c> many</c><00:00:44.700><c> of</c><00:00:44.910><c> us</c><00:00:45.060><c> vision</c><00:00:45.570><c> is</c><00:00:45.899><c> a</c><00:00:45.930><c> huge</c><00:00:46.320><c> part</c>

00:00:46.580 --> 00:00:46.590 align:start position:0%
us or many of us vision is a huge part
 

00:00:46.590 --> 00:00:49.460 align:start position:0%
us or many of us vision is a huge part
of<00:00:46.770><c> our</c><00:00:46.920><c> lives</c><00:00:47.130><c> and</c><00:00:47.730><c> that's</c><00:00:48.360><c> largely</c><00:00:48.870><c> thanks</c>

00:00:49.460 --> 00:00:49.470 align:start position:0%
of our lives and that's largely thanks
 

00:00:49.470 --> 00:00:52.790 align:start position:0%
of our lives and that's largely thanks
to<00:00:49.649><c> the</c><00:00:50.040><c> power</c><00:00:50.280><c> of</c><00:00:50.730><c> evolution</c><00:00:51.800><c> evolutionary</c>

00:00:52.790 --> 00:00:52.800 align:start position:0%
to the power of evolution evolutionary
 

00:00:52.800 --> 00:00:55.220 align:start position:0%
to the power of evolution evolutionary
biologists<00:00:53.640><c> traced</c><00:00:54.090><c> the</c><00:00:54.329><c> origins</c><00:00:54.780><c> of</c><00:00:54.930><c> vision</c>

00:00:55.220 --> 00:00:55.230 align:start position:0%
biologists traced the origins of vision
 

00:00:55.230 --> 00:00:58.250 align:start position:0%
biologists traced the origins of vision
back<00:00:56.090><c> 540</c><00:00:57.090><c> million</c><00:00:57.450><c> years</c><00:00:57.719><c> ago</c><00:00:57.870><c> to</c><00:00:58.020><c> the</c>

00:00:58.250 --> 00:00:58.260 align:start position:0%
back 540 million years ago to the
 

00:00:58.260 --> 00:01:01.100 align:start position:0%
back 540 million years ago to the
Cambrian<00:00:58.980><c> explosion</c><00:00:59.070><c> and</c><00:00:59.940><c> the</c><00:01:00.660><c> reason</c><00:01:01.079><c> that</c>

00:01:01.100 --> 00:01:01.110 align:start position:0%
Cambrian explosion and the reason that
 

00:01:01.110 --> 00:01:04.429 align:start position:0%
Cambrian explosion and the reason that
vision<00:01:02.010><c> seems</c><00:01:02.370><c> so</c><00:01:02.610><c> easy</c><00:01:02.940><c> for</c><00:01:03.239><c> us</c><00:01:03.480><c> as</c><00:01:03.780><c> humans</c><00:01:04.259><c> is</c>

00:01:04.429 --> 00:01:04.439 align:start position:0%
vision seems so easy for us as humans is
 

00:01:04.439 --> 00:01:07.489 align:start position:0%
vision seems so easy for us as humans is
because<00:01:05.250><c> we</c><00:01:05.399><c> have</c><00:01:05.579><c> 540</c><00:01:06.539><c> million</c><00:01:06.899><c> years</c><00:01:07.259><c> of</c>

00:01:07.489 --> 00:01:07.499 align:start position:0%
because we have 540 million years of
 

00:01:07.499 --> 00:01:09.709 align:start position:0%
because we have 540 million years of
data<00:01:08.189><c> that</c><00:01:08.520><c> evolution</c><00:01:09.060><c> has</c><00:01:09.209><c> effectively</c>

00:01:09.709 --> 00:01:09.719 align:start position:0%
data that evolution has effectively
 

00:01:09.719 --> 00:01:12.319 align:start position:0%
data that evolution has effectively
trained<00:01:10.350><c> on</c><00:01:10.560><c> and</c><00:01:10.859><c> if</c><00:01:11.609><c> you</c><00:01:11.729><c> compare</c><00:01:12.060><c> that</c><00:01:12.270><c> to</c>

00:01:12.319 --> 00:01:12.329 align:start position:0%
trained on and if you compare that to
 

00:01:12.329 --> 00:01:15.319 align:start position:0%
trained on and if you compare that to
other<00:01:13.079><c> capabilities</c><00:01:14.009><c> like</c><00:01:14.039><c> bipedal</c><00:01:14.759><c> movement</c>

00:01:15.319 --> 00:01:15.329 align:start position:0%
other capabilities like bipedal movement
 

00:01:15.329 --> 00:01:18.019 align:start position:0%
other capabilities like bipedal movement
and<00:01:15.539><c> language</c><00:01:16.469><c> the</c><00:01:16.799><c> difference</c><00:01:17.219><c> is</c><00:01:17.490><c> is</c><00:01:17.789><c> quite</c>

00:01:18.019 --> 00:01:18.029 align:start position:0%
and language the difference is is quite
 

00:01:18.029 --> 00:01:21.230 align:start position:0%
and language the difference is is quite
significant<00:01:18.779><c> and</c><00:01:19.459><c> starting</c><00:01:20.459><c> in</c><00:01:20.579><c> the</c><00:01:20.700><c> 1960s</c>

00:01:21.230 --> 00:01:21.240 align:start position:0%
significant and starting in the 1960s
 

00:01:21.240 --> 00:01:23.870 align:start position:0%
significant and starting in the 1960s
there<00:01:21.719><c> was</c><00:01:21.869><c> a</c><00:01:21.899><c> surge</c><00:01:22.439><c> of</c><00:01:22.709><c> interest</c><00:01:23.279><c> in</c><00:01:23.549><c> both</c>

00:01:23.870 --> 00:01:23.880 align:start position:0%
there was a surge of interest in both
 

00:01:23.880 --> 00:01:26.859 align:start position:0%
there was a surge of interest in both
the<00:01:24.450><c> neural</c><00:01:24.779><c> basis</c><00:01:25.229><c> of</c><00:01:25.439><c> vision</c><00:01:25.799><c> and</c><00:01:25.950><c> how</c><00:01:26.549><c> to</c>

00:01:26.859 --> 00:01:26.869 align:start position:0%
the neural basis of vision and how to
 

00:01:26.869 --> 00:01:29.480 align:start position:0%
the neural basis of vision and how to
systematically<00:01:27.869><c> characterize</c><00:01:28.529><c> visual</c>

00:01:29.480 --> 00:01:29.490 align:start position:0%
systematically characterize visual
 

00:01:29.490 --> 00:01:31.999 align:start position:0%
systematically characterize visual
processing<00:01:29.969><c> and</c><00:01:30.329><c> this</c><00:01:30.929><c> led</c><00:01:31.200><c> to</c><00:01:31.469><c> computer</c>

00:01:31.999 --> 00:01:32.009 align:start position:0%
processing and this led to computer
 

00:01:32.009 --> 00:01:34.789 align:start position:0%
processing and this led to computer
scientists<00:01:32.759><c> beginning</c><00:01:33.689><c> to</c><00:01:33.810><c> wonder</c><00:01:34.020><c> about</c><00:01:34.380><c> how</c>

00:01:34.789 --> 00:01:34.799 align:start position:0%
scientists beginning to wonder about how
 

00:01:34.799 --> 00:01:36.830 align:start position:0%
scientists beginning to wonder about how
findings<00:01:35.670><c> in</c><00:01:35.909><c> neuroscience</c><00:01:36.420><c> could</c><00:01:36.719><c> be</c>

00:01:36.830 --> 00:01:36.840 align:start position:0%
findings in neuroscience could be
 

00:01:36.840 --> 00:01:39.769 align:start position:0%
findings in neuroscience could be
applied<00:01:37.229><c> to</c><00:01:37.590><c> achieve</c><00:01:38.389><c> artificial</c><00:01:39.389><c> computer</c>

00:01:39.769 --> 00:01:39.779 align:start position:0%
applied to achieve artificial computer
 

00:01:39.779 --> 00:01:42.739 align:start position:0%
applied to achieve artificial computer
vision<00:01:40.139><c> and</c><00:01:40.979><c> it</c><00:01:41.130><c> all</c><00:01:41.340><c> started</c><00:01:41.909><c> with</c><00:01:42.060><c> these</c>

00:01:42.739 --> 00:01:42.749 align:start position:0%
vision and it all started with these
 

00:01:42.749 --> 00:01:45.760 align:start position:0%
vision and it all started with these
series<00:01:43.529><c> of</c><00:01:43.649><c> seminal</c><00:01:44.189><c> experiments</c><00:01:44.909><c> from</c><00:01:45.060><c> two</c>

00:01:45.760 --> 00:01:45.770 align:start position:0%
series of seminal experiments from two
 

00:01:45.770 --> 00:01:48.409 align:start position:0%
series of seminal experiments from two
neuroscientists<00:01:46.770><c> David</c><00:01:47.490><c> Hubel</c><00:01:47.700><c> and</c><00:01:48.060><c> Torsten</c>

00:01:48.409 --> 00:01:48.419 align:start position:0%
neuroscientists David Hubel and Torsten
 

00:01:48.419 --> 00:01:50.419 align:start position:0%
neuroscientists David Hubel and Torsten
vessel<00:01:49.139><c> who</c><00:01:49.319><c> were</c><00:01:49.439><c> working</c><00:01:49.799><c> at</c><00:01:49.919><c> Harvard</c><00:01:50.310><c> at</c>

00:01:50.419 --> 00:01:50.429 align:start position:0%
vessel who were working at Harvard at
 

00:01:50.429 --> 00:01:52.370 align:start position:0%
vessel who were working at Harvard at
the<00:01:50.579><c> time</c><00:01:50.759><c> and</c><00:01:51.090><c> they</c><00:01:51.689><c> were</c><00:01:51.749><c> looking</c><00:01:52.169><c> at</c>

00:01:52.370 --> 00:01:52.380 align:start position:0%
the time and they were looking at
 

00:01:52.380 --> 00:01:55.039 align:start position:0%
the time and they were looking at
processing<00:01:53.159><c> in</c><00:01:53.369><c> the</c><00:01:53.399><c> visual</c><00:01:53.819><c> cortex</c><00:01:54.329><c> of</c><00:01:54.539><c> cats</c>

00:01:55.039 --> 00:01:55.049 align:start position:0%
processing in the visual cortex of cats
 

00:01:55.049 --> 00:01:57.319 align:start position:0%
processing in the visual cortex of cats
and<00:01:55.529><c> what</c><00:01:56.279><c> they</c><00:01:56.429><c> were</c><00:01:56.490><c> able</c><00:01:56.939><c> to</c><00:01:56.969><c> demonstrate</c>

00:01:57.319 --> 00:01:57.329 align:start position:0%
and what they were able to demonstrate
 

00:01:57.329 --> 00:02:00.589 align:start position:0%
and what they were able to demonstrate
was<00:01:58.049><c> that</c><00:01:58.319><c> their</c><00:01:58.649><c> neural</c><00:01:59.219><c> mechanisms</c><00:01:59.969><c> for</c>

00:02:00.589 --> 00:02:00.599 align:start position:0%
was that their neural mechanisms for
 

00:02:00.599 --> 00:02:03.199 align:start position:0%
was that their neural mechanisms for
spatially<00:02:01.319><c> invariant</c><00:02:02.069><c> pattern</c><00:02:02.489><c> recognition</c>

00:02:03.199 --> 00:02:03.209 align:start position:0%
spatially invariant pattern recognition
 

00:02:03.209 --> 00:02:05.569 align:start position:0%
spatially invariant pattern recognition
and<00:02:03.419><c> that</c><00:02:04.139><c> certain</c><00:02:04.590><c> neurons</c><00:02:04.979><c> in</c><00:02:05.399><c> the</c><00:02:05.549><c> visual</c>

00:02:05.569 --> 00:02:05.579 align:start position:0%
and that certain neurons in the visual
 

00:02:05.579 --> 00:02:08.559 align:start position:0%
and that certain neurons in the visual
cortex<00:02:06.149><c> respond</c><00:02:06.869><c> very</c><00:02:07.380><c> specifically</c><00:02:08.160><c> to</c>

00:02:08.559 --> 00:02:08.569 align:start position:0%
cortex respond very specifically to
 

00:02:08.569 --> 00:02:11.480 align:start position:0%
cortex respond very specifically to
specific<00:02:09.569><c> patterns</c><00:02:10.080><c> and</c><00:02:10.349><c> regions</c><00:02:10.800><c> of</c><00:02:11.040><c> visual</c>

00:02:11.480 --> 00:02:11.490 align:start position:0%
specific patterns and regions of visual
 

00:02:11.490 --> 00:02:14.210 align:start position:0%
specific patterns and regions of visual
stimuli<00:02:11.970><c> and</c><00:02:12.620><c> furthermore</c><00:02:13.620><c> that</c><00:02:13.800><c> there's</c><00:02:14.130><c> an</c>

00:02:14.210 --> 00:02:14.220 align:start position:0%
stimuli and furthermore that there's an
 

00:02:14.220 --> 00:02:15.950 align:start position:0%
stimuli and furthermore that there's an
exquisite<00:02:14.850><c> hierarchy</c>

00:02:15.950 --> 00:02:15.960 align:start position:0%
exquisite hierarchy
 

00:02:15.960 --> 00:02:18.170 align:start position:0%
exquisite hierarchy
of<00:02:16.050><c> neural</c><00:02:16.530><c> layers</c><00:02:16.770><c> that</c><00:02:17.100><c> exist</c><00:02:17.490><c> within</c><00:02:17.700><c> the</c>

00:02:18.170 --> 00:02:18.180 align:start position:0%
of neural layers that exist within the
 

00:02:18.180 --> 00:02:20.810 align:start position:0%
of neural layers that exist within the
visual<00:02:18.450><c> cortex</c><00:02:18.720><c> and</c><00:02:19.200><c> these</c><00:02:20.010><c> concepts</c><00:02:20.550><c> have</c>

00:02:20.810 --> 00:02:20.820 align:start position:0%
visual cortex and these concepts have
 

00:02:20.820 --> 00:02:23.000 align:start position:0%
visual cortex and these concepts have
transformed<00:02:21.510><c> both</c><00:02:21.800><c> neuroscience</c><00:02:22.800><c> and</c>

00:02:23.000 --> 00:02:23.010 align:start position:0%
transformed both neuroscience and
 

00:02:23.010 --> 00:02:26.090 align:start position:0%
transformed both neuroscience and
artificial<00:02:23.670><c> intelligence</c><00:02:23.910><c> alike</c><00:02:24.660><c> and</c><00:02:25.100><c> today</c>

00:02:26.090 --> 00:02:26.100 align:start position:0%
artificial intelligence alike and today
 

00:02:26.100 --> 00:02:28.340 align:start position:0%
artificial intelligence alike and today
we're<00:02:26.700><c> going</c><00:02:26.940><c> to</c><00:02:27.090><c> learn</c><00:02:27.300><c> about</c><00:02:27.660><c> how</c><00:02:28.140><c> to</c><00:02:28.170><c> use</c>

00:02:28.340 --> 00:02:28.350 align:start position:0%
we're going to learn about how to use
 

00:02:28.350 --> 00:02:30.950 align:start position:0%
we're going to learn about how to use
deep<00:02:28.740><c> learning</c><00:02:28.950><c> to</c><00:02:29.700><c> build</c><00:02:29.850><c> powerful</c><00:02:30.540><c> computer</c>

00:02:30.950 --> 00:02:30.960 align:start position:0%
deep learning to build powerful computer
 

00:02:30.960 --> 00:02:33.890 align:start position:0%
deep learning to build powerful computer
vision<00:02:31.230><c> systems</c><00:02:31.770><c> that</c><00:02:32.090><c> have</c><00:02:33.090><c> been</c><00:02:33.330><c> have</c><00:02:33.870><c> been</c>

00:02:33.890 --> 00:02:33.900 align:start position:0%
vision systems that have been have been
 

00:02:33.900 --> 00:02:35.710 align:start position:0%
vision systems that have been have been
shown<00:02:34.260><c> to</c><00:02:34.290><c> be</c><00:02:34.530><c> capable</c><00:02:34.770><c> of</c><00:02:35.220><c> extraordinary</c>

00:02:35.710 --> 00:02:35.720 align:start position:0%
shown to be capable of extraordinary
 

00:02:35.720 --> 00:02:40.940 align:start position:0%
shown to be capable of extraordinary
complex<00:02:37.280><c> computer</c><00:02:38.280><c> vision</c><00:02:38.580><c> tasks</c><00:02:39.800><c> so</c><00:02:40.800><c> now</c>

00:02:40.940 --> 00:02:40.950 align:start position:0%
complex computer vision tasks so now
 

00:02:40.950 --> 00:02:42.440 align:start position:0%
complex computer vision tasks so now
that<00:02:41.010><c> we've</c><00:02:41.280><c> gone</c><00:02:41.430><c> in</c><00:02:41.580><c> a</c><00:02:41.610><c> sense</c><00:02:41.880><c> at</c><00:02:42.180><c> a</c><00:02:42.210><c> very</c>

00:02:42.440 --> 00:02:42.450 align:start position:0%
that we've gone in a sense at a very
 

00:02:42.450 --> 00:02:44.660 align:start position:0%
that we've gone in a sense at a very
high<00:02:42.690><c> level</c><00:02:43.050><c> of</c><00:02:43.140><c> why</c><00:02:43.590><c> this</c><00:02:43.650><c> is</c><00:02:43.980><c> important</c><00:02:44.460><c> and</c>

00:02:44.660 --> 00:02:44.670 align:start position:0%
high level of why this is important and
 

00:02:44.670 --> 00:02:46.730 align:start position:0%
high level of why this is important and
sort<00:02:45.150><c> of</c><00:02:45.240><c> how</c><00:02:45.420><c> our</c><00:02:45.690><c> brains</c><00:02:45.990><c> may</c><00:02:46.230><c> process</c>

00:02:46.730 --> 00:02:46.740 align:start position:0%
sort of how our brains may process
 

00:02:46.740 --> 00:02:48.830 align:start position:0%
sort of how our brains may process
visual<00:02:46.920><c> information</c><00:02:47.190><c> we</c><00:02:48.180><c> can</c><00:02:48.390><c> turn</c><00:02:48.660><c> our</c>

00:02:48.830 --> 00:02:48.840 align:start position:0%
visual information we can turn our
 

00:02:48.840 --> 00:02:52.340 align:start position:0%
visual information we can turn our
attention<00:02:48.870><c> to</c><00:02:49.380><c> what</c><00:02:50.130><c> computers</c><00:02:50.640><c> see</c><00:02:51.090><c> how</c><00:02:52.050><c> does</c>

00:02:52.340 --> 00:02:52.350 align:start position:0%
attention to what computers see how does
 

00:02:52.350 --> 00:02:57.530 align:start position:0%
attention to what computers see how does
a<00:02:52.380><c> computer</c><00:02:52.620><c> process</c><00:02:53.280><c> an</c><00:02:53.430><c> image</c><00:02:53.870><c> so</c><00:02:54.870><c> well</c><00:02:56.510><c> to</c><00:02:57.510><c> a</c>

00:02:57.530 --> 00:02:57.540 align:start position:0%
a computer process an image so well to a
 

00:02:57.540 --> 00:03:00.350 align:start position:0%
a computer process an image so well to a
computer<00:02:57.960><c> images</c><00:02:58.530><c> are</c><00:02:58.740><c> just</c><00:02:58.980><c> numbers</c><00:02:59.360><c> so</c>

00:03:00.350 --> 00:03:00.360 align:start position:0%
computer images are just numbers so
 

00:03:00.360 --> 00:03:02.480 align:start position:0%
computer images are just numbers so
suppose<00:03:00.720><c> we</c><00:03:00.750><c> have</c><00:03:00.930><c> this</c><00:03:01.230><c> picture</c><00:03:01.500><c> of</c><00:03:01.890><c> Abraham</c>

00:03:02.480 --> 00:03:02.490 align:start position:0%
suppose we have this picture of Abraham
 

00:03:02.490 --> 00:03:05.720 align:start position:0%
suppose we have this picture of Abraham
Lincoln<00:03:02.880><c> it's</c><00:03:03.420><c> made</c><00:03:03.690><c> up</c><00:03:03.840><c> of</c><00:03:03.960><c> pixels</c><00:03:04.260><c> and</c><00:03:04.730><c> since</c>

00:03:05.720 --> 00:03:05.730 align:start position:0%
Lincoln it's made up of pixels and since
 

00:03:05.730 --> 00:03:07.700 align:start position:0%
Lincoln it's made up of pixels and since
this<00:03:05.880><c> is</c><00:03:06.060><c> a</c><00:03:06.090><c> grayscale</c><00:03:06.660><c> image</c><00:03:06.690><c> each</c><00:03:07.320><c> of</c><00:03:07.590><c> these</c>

00:03:07.700 --> 00:03:07.710 align:start position:0%
this is a grayscale image each of these
 

00:03:07.710 --> 00:03:11.360 align:start position:0%
this is a grayscale image each of these
pixels<00:03:08.040><c> is</c><00:03:08.490><c> just</c><00:03:08.550><c> a</c><00:03:09.120><c> single</c><00:03:09.540><c> number</c><00:03:09.750><c> and</c><00:03:10.370><c> we</c>

00:03:11.360 --> 00:03:11.370 align:start position:0%
pixels is just a single number and we
 

00:03:11.370 --> 00:03:13.940 align:start position:0%
pixels is just a single number and we
can<00:03:11.400><c> represent</c><00:03:11.670><c> this</c><00:03:12.150><c> image</c><00:03:12.600><c> as</c><00:03:12.840><c> a</c><00:03:13.230><c> 2d</c><00:03:13.740><c> matrix</c>

00:03:13.940 --> 00:03:13.950 align:start position:0%
can represent this image as a 2d matrix
 

00:03:13.950 --> 00:03:16.880 align:start position:0%
can represent this image as a 2d matrix
of<00:03:14.460><c> numbers</c><00:03:14.940><c> one</c><00:03:15.540><c> for</c><00:03:15.780><c> each</c><00:03:15.960><c> pixel</c><00:03:16.260><c> in</c><00:03:16.650><c> the</c>

00:03:16.880 --> 00:03:16.890 align:start position:0%
of numbers one for each pixel in the
 

00:03:16.890 --> 00:03:19.460 align:start position:0%
of numbers one for each pixel in the
image<00:03:16.920><c> and</c><00:03:17.430><c> this</c><00:03:17.940><c> is</c><00:03:18.090><c> how</c><00:03:18.270><c> a</c><00:03:18.630><c> computer</c><00:03:19.020><c> sees</c>

00:03:19.460 --> 00:03:19.470 align:start position:0%
image and this is how a computer sees
 

00:03:19.470 --> 00:03:22.880 align:start position:0%
image and this is how a computer sees
this<00:03:20.250><c> image</c><00:03:20.810><c> likewise</c><00:03:21.810><c> if</c><00:03:22.050><c> we</c><00:03:22.110><c> were</c><00:03:22.380><c> to</c><00:03:22.650><c> have</c><00:03:22.830><c> a</c>

00:03:22.880 --> 00:03:22.890 align:start position:0%
this image likewise if we were to have a
 

00:03:22.890 --> 00:03:26.270 align:start position:0%
this image likewise if we were to have a
RGB<00:03:23.460><c> color</c><00:03:24.420><c> image</c><00:03:24.810><c> not</c><00:03:25.110><c> grayscale</c><00:03:25.680><c> we</c><00:03:26.100><c> can</c>

00:03:26.270 --> 00:03:26.280 align:start position:0%
RGB color image not grayscale we can
 

00:03:26.280 --> 00:03:29.300 align:start position:0%
RGB color image not grayscale we can
represent<00:03:26.460><c> that</c><00:03:26.910><c> with</c><00:03:27.240><c> a</c><00:03:27.450><c> 3d</c><00:03:27.990><c> array</c><00:03:28.320><c> where</c><00:03:29.100><c> now</c>

00:03:29.300 --> 00:03:29.310 align:start position:0%
represent that with a 3d array where now
 

00:03:29.310 --> 00:03:31.490 align:start position:0%
represent that with a 3d array where now
we<00:03:29.370><c> have</c><00:03:29.760><c> two</c><00:03:30.060><c> D</c><00:03:30.210><c> matrices</c><00:03:30.630><c> for</c><00:03:31.050><c> each</c><00:03:31.200><c> of</c><00:03:31.380><c> the</c>

00:03:31.490 --> 00:03:31.500 align:start position:0%
we have two D matrices for each of the
 

00:03:31.500 --> 00:03:35.540 align:start position:0%
we have two D matrices for each of the
channels<00:03:31.740><c> are</c><00:03:32.190><c> g</c><00:03:32.820><c> and</c><00:03:33.060><c> b</c><00:03:34.100><c> so</c><00:03:35.100><c> now</c><00:03:35.220><c> that</c><00:03:35.280><c> we</c><00:03:35.520><c> have</c>

00:03:35.540 --> 00:03:35.550 align:start position:0%
channels are g and b so now that we have
 

00:03:35.550 --> 00:03:38.210 align:start position:0%
channels are g and b so now that we have
a<00:03:35.760><c> way</c><00:03:36.000><c> to</c><00:03:36.510><c> represent</c><00:03:36.720><c> images</c><00:03:37.470><c> to</c><00:03:37.800><c> computers</c>

00:03:38.210 --> 00:03:38.220 align:start position:0%
a way to represent images to computers
 

00:03:38.220 --> 00:03:40.550 align:start position:0%
a way to represent images to computers
we<00:03:38.760><c> can</c><00:03:38.790><c> think</c><00:03:39.210><c> about</c><00:03:39.540><c> what</c><00:03:39.900><c> types</c><00:03:40.350><c> of</c>

00:03:40.550 --> 00:03:40.560 align:start position:0%
we can think about what types of
 

00:03:40.560 --> 00:03:42.980 align:start position:0%
we can think about what types of
computer<00:03:41.010><c> computer</c><00:03:41.700><c> vision</c><00:03:42.030><c> tasks</c><00:03:42.480><c> we</c><00:03:42.960><c> can</c>

00:03:42.980 --> 00:03:42.990 align:start position:0%
computer computer vision tasks we can
 

00:03:42.990 --> 00:03:46.520 align:start position:0%
computer computer vision tasks we can
perform<00:03:43.350><c> and</c><00:03:44.180><c> two</c><00:03:45.180><c> very</c><00:03:45.510><c> common</c><00:03:45.780><c> types</c><00:03:46.200><c> of</c>

00:03:46.520 --> 00:03:46.530 align:start position:0%
perform and two very common types of
 

00:03:46.530 --> 00:03:48.860 align:start position:0%
perform and two very common types of
tasks<00:03:47.070><c> and</c><00:03:47.160><c> machine</c><00:03:47.490><c> learning</c><00:03:47.660><c> broadly</c><00:03:48.660><c> are</c>

00:03:48.860 --> 00:03:48.870 align:start position:0%
tasks and machine learning broadly are
 

00:03:48.870 --> 00:03:50.660 align:start position:0%
tasks and machine learning broadly are
those<00:03:49.140><c> of</c><00:03:49.380><c> regression</c><00:03:49.920><c> and</c><00:03:50.100><c> those</c><00:03:50.430><c> of</c>

00:03:50.660 --> 00:03:50.670 align:start position:0%
those of regression and those of
 

00:03:50.670 --> 00:03:53.270 align:start position:0%
those of regression and those of
classification<00:03:51.540><c> in</c><00:03:51.780><c> a</c><00:03:52.560><c> regression</c><00:03:52.860><c> our</c>

00:03:53.270 --> 00:03:53.280 align:start position:0%
classification in a regression our
 

00:03:53.280 --> 00:03:56.360 align:start position:0%
classification in a regression our
output<00:03:53.850><c> takes</c><00:03:54.210><c> a</c><00:03:54.450><c> continuous</c><00:03:55.110><c> value</c><00:03:55.290><c> while</c><00:03:56.040><c> in</c>

00:03:56.360 --> 00:03:56.370 align:start position:0%
output takes a continuous value while in
 

00:03:56.370 --> 00:03:58.640 align:start position:0%
output takes a continuous value while in
classification<00:03:56.880><c> our</c><00:03:57.450><c> output</c><00:03:57.930><c> takes</c><00:03:58.230><c> a</c><00:03:58.410><c> single</c>

00:03:58.640 --> 00:03:58.650 align:start position:0%
classification our output takes a single
 

00:03:58.650 --> 00:04:01.730 align:start position:0%
classification our output takes a single
class<00:03:59.040><c> label</c><00:03:59.550><c> so</c><00:04:00.450><c> for</c><00:04:01.230><c> example</c><00:04:01.590><c> let's</c>

00:04:01.730 --> 00:04:01.740 align:start position:0%
class label so for example let's
 

00:04:01.740 --> 00:04:03.320 align:start position:0%
class label so for example let's
consider<00:04:01.920><c> the</c><00:04:02.190><c> task</c><00:04:02.580><c> of</c><00:04:02.910><c> image</c>

00:04:03.320 --> 00:04:03.330 align:start position:0%
consider the task of image
 

00:04:03.330 --> 00:04:06.230 align:start position:0%
consider the task of image
classification<00:04:03.860><c> say</c><00:04:04.860><c> we</c><00:04:05.070><c> want</c><00:04:05.340><c> to</c><00:04:05.610><c> predict</c><00:04:06.060><c> a</c>

00:04:06.230 --> 00:04:06.240 align:start position:0%
classification say we want to predict a
 

00:04:06.240 --> 00:04:09.080 align:start position:0%
classification say we want to predict a
single<00:04:06.630><c> label</c><00:04:06.810><c> for</c><00:04:07.290><c> some</c><00:04:07.770><c> image</c><00:04:08.130><c> and</c><00:04:08.490><c> let's</c>

00:04:09.080 --> 00:04:09.090 align:start position:0%
single label for some image and let's
 

00:04:09.090 --> 00:04:10.910 align:start position:0%
single label for some image and let's
say<00:04:09.240><c> we</c><00:04:09.360><c> have</c><00:04:09.390><c> a</c><00:04:09.540><c> bunch</c><00:04:09.810><c> of</c><00:04:09.930><c> images</c><00:04:10.440><c> of</c><00:04:10.650><c> US</c>

00:04:10.910 --> 00:04:10.920 align:start position:0%
say we have a bunch of images of US
 

00:04:10.920 --> 00:04:13.070 align:start position:0%
say we have a bunch of images of US
presidents<00:04:11.520><c> and</c><00:04:11.880><c> we</c><00:04:12.210><c> want</c><00:04:12.420><c> to</c><00:04:12.570><c> build</c><00:04:12.810><c> a</c>

00:04:13.070 --> 00:04:13.080 align:start position:0%
presidents and we want to build a
 

00:04:13.080 --> 00:04:16.160 align:start position:0%
presidents and we want to build a
classification<00:04:13.950><c> pipeline</c><00:04:14.760><c> to</c><00:04:15.060><c> tell</c><00:04:15.300><c> us</c><00:04:15.510><c> which</c>

00:04:16.160 --> 00:04:16.170 align:start position:0%
classification pipeline to tell us which
 

00:04:16.170 --> 00:04:18.890 align:start position:0%
classification pipeline to tell us which
President<00:04:16.739><c> is</c><00:04:16.890><c> in</c><00:04:16.920><c> an</c><00:04:17.280><c> image</c><00:04:17.810><c> outputting</c><00:04:18.810><c> the</c>

00:04:18.890 --> 00:04:18.900 align:start position:0%
President is in an image outputting the
 

00:04:18.900 --> 00:04:21.770 align:start position:0%
President is in an image outputting the
probability<00:04:19.620><c> that</c><00:04:19.650><c> that</c><00:04:20.160><c> image</c><00:04:20.609><c> is</c><00:04:20.850><c> of</c><00:04:21.750><c> a</c>

00:04:21.770 --> 00:04:21.780 align:start position:0%
probability that that image is of a
 

00:04:21.780 --> 00:04:25.280 align:start position:0%
probability that that image is of a
particular<00:04:22.200><c> President</c><00:04:23.070><c> and</c><00:04:24.050><c> so</c><00:04:25.050><c> you</c><00:04:25.110><c> can</c>

00:04:25.280 --> 00:04:25.290 align:start position:0%
particular President and so you can
 

00:04:25.290 --> 00:04:27.640 align:start position:0%
particular President and so you can
imagine<00:04:25.410><c> right</c><00:04:26.040><c> that</c><00:04:26.280><c> in</c><00:04:26.490><c> order</c><00:04:26.880><c> to</c><00:04:26.910><c> cry</c>

00:04:27.640 --> 00:04:27.650 align:start position:0%
imagine right that in order to cry
 

00:04:27.650 --> 00:04:30.070 align:start position:0%
imagine right that in order to cry
classified<00:04:28.340><c> these</c><00:04:28.520><c> images</c><00:04:29.090><c> our</c><00:04:29.600><c> pipeline</c>

00:04:30.070 --> 00:04:30.080 align:start position:0%
classified these images our pipeline
 

00:04:30.080 --> 00:04:32.439 align:start position:0%
classified these images our pipeline
needs<00:04:30.320><c> to</c><00:04:30.500><c> be</c><00:04:30.590><c> able</c><00:04:30.830><c> to</c><00:04:31.160><c> tell</c><00:04:31.370><c> what</c><00:04:31.940><c> is</c><00:04:32.090><c> unique</c>

00:04:32.439 --> 00:04:32.449 align:start position:0%
needs to be able to tell what is unique
 

00:04:32.449 --> 00:04:34.480 align:start position:0%
needs to be able to tell what is unique
about<00:04:32.570><c> a</c><00:04:32.750><c> picture</c><00:04:33.110><c> of</c><00:04:33.229><c> Lincoln</c><00:04:33.460><c> versus</c><00:04:34.460><c> a</c>

00:04:34.480 --> 00:04:34.490 align:start position:0%
about a picture of Lincoln versus a
 

00:04:34.490 --> 00:04:36.760 align:start position:0%
about a picture of Lincoln versus a
picture<00:04:34.880><c> of</c><00:04:35.000><c> Washington</c><00:04:35.630><c> versus</c><00:04:36.229><c> a</c><00:04:36.410><c> picture</c>

00:04:36.760 --> 00:04:36.770 align:start position:0%
picture of Washington versus a picture
 

00:04:36.770 --> 00:04:40.150 align:start position:0%
picture of Washington versus a picture
of<00:04:37.009><c> Obama</c><00:04:37.400><c> and</c><00:04:38.440><c> another</c><00:04:39.440><c> way</c><00:04:39.650><c> to</c><00:04:39.710><c> think</c><00:04:40.039><c> about</c>

00:04:40.150 --> 00:04:40.160 align:start position:0%
of Obama and another way to think about
 

00:04:40.160 --> 00:04:42.670 align:start position:0%
of Obama and another way to think about
this<00:04:40.430><c> problem</c><00:04:40.940><c> at</c><00:04:41.150><c> a</c><00:04:41.210><c> high</c><00:04:41.600><c> level</c><00:04:41.840><c> is</c><00:04:42.139><c> in</c><00:04:42.410><c> terms</c>

00:04:42.670 --> 00:04:42.680 align:start position:0%
this problem at a high level is in terms
 

00:04:42.680 --> 00:04:44.439 align:start position:0%
this problem at a high level is in terms
of<00:04:42.830><c> the</c><00:04:42.949><c> features</c><00:04:43.160><c> that</c><00:04:43.370><c> are</c><00:04:43.610><c> characteristic</c>

00:04:44.439 --> 00:04:44.449 align:start position:0%
of the features that are characteristic
 

00:04:44.449 --> 00:04:47.560 align:start position:0%
of the features that are characteristic
of<00:04:44.600><c> a</c><00:04:44.900><c> particular</c><00:04:45.080><c> class</c><00:04:45.620><c> and</c><00:04:46.570><c> classification</c>

00:04:47.560 --> 00:04:47.570 align:start position:0%
of a particular class and classification
 

00:04:47.570 --> 00:04:50.680 align:start position:0%
of a particular class and classification
it<00:04:47.720><c> can</c><00:04:47.960><c> then</c><00:04:48.139><c> be</c><00:04:48.380><c> thought</c><00:04:48.590><c> of</c><00:04:48.620><c> as</c><00:04:49.690><c> involving</c>

00:04:50.680 --> 00:04:50.690 align:start position:0%
it can then be thought of as involving
 

00:04:50.690 --> 00:04:52.629 align:start position:0%
it can then be thought of as involving
detection<00:04:51.350><c> of</c><00:04:51.470><c> the</c><00:04:51.620><c> features</c><00:04:51.979><c> in</c><00:04:52.280><c> a</c><00:04:52.400><c> given</c>

00:04:52.629 --> 00:04:52.639 align:start position:0%
detection of the features in a given
 

00:04:52.639 --> 00:04:56.290 align:start position:0%
detection of the features in a given
image<00:04:52.850><c> and</c><00:04:53.860><c> sort</c><00:04:54.860><c> of</c><00:04:54.889><c> deciding</c><00:04:55.400><c> okay</c><00:04:55.639><c> well</c><00:04:56.090><c> if</c>

00:04:56.290 --> 00:04:56.300 align:start position:0%
image and sort of deciding okay well if
 

00:04:56.300 --> 00:04:58.719 align:start position:0%
image and sort of deciding okay well if
the<00:04:56.570><c> feature</c><00:04:56.780><c> is</c><00:04:57.020><c> for</c><00:04:57.320><c> a</c><00:04:57.380><c> particular</c><00:04:57.979><c> class</c>

00:04:58.719 --> 00:04:58.729 align:start position:0%
the feature is for a particular class
 

00:04:58.729 --> 00:05:01.689 align:start position:0%
the feature is for a particular class
are<00:04:59.060><c> present</c><00:04:59.539><c> in</c><00:04:59.690><c> an</c><00:05:00.139><c> image</c><00:05:00.470><c> we</c><00:05:01.280><c> can</c><00:05:01.460><c> then</c>

00:05:01.689 --> 00:05:01.699 align:start position:0%
are present in an image we can then
 

00:05:01.699 --> 00:05:04.150 align:start position:0%
are present in an image we can then
predict<00:05:02.240><c> that</c><00:05:02.570><c> that</c><00:05:02.990><c> image</c><00:05:03.320><c> is</c><00:05:03.470><c> of</c><00:05:03.710><c> that</c><00:05:03.919><c> class</c>

00:05:04.150 --> 00:05:04.160 align:start position:0%
predict that that image is of that class
 

00:05:04.160 --> 00:05:07.060 align:start position:0%
predict that that image is of that class
with<00:05:04.610><c> a</c><00:05:04.639><c> higher</c><00:05:04.940><c> probability</c><00:05:05.240><c> and</c><00:05:06.050><c> so</c><00:05:06.860><c> if</c>

00:05:07.060 --> 00:05:07.070 align:start position:0%
with a higher probability and so if
 

00:05:07.070 --> 00:05:08.730 align:start position:0%
with a higher probability and so if
we're<00:05:07.250><c> building</c><00:05:07.460><c> a</c><00:05:07.910><c> image</c><00:05:08.449><c> classification</c>

00:05:08.730 --> 00:05:08.740 align:start position:0%
we're building a image classification
 

00:05:08.740 --> 00:05:12.250 align:start position:0%
we're building a image classification
pipeline<00:05:09.740><c> our</c><00:05:10.460><c> model</c><00:05:10.880><c> needs</c><00:05:11.120><c> to</c><00:05:11.330><c> know</c><00:05:11.539><c> what</c>

00:05:12.250 --> 00:05:12.260 align:start position:0%
pipeline our model needs to know what
 

00:05:12.260 --> 00:05:14.500 align:start position:0%
pipeline our model needs to know what
those<00:05:12.470><c> features</c><00:05:12.740><c> are</c><00:05:13.190><c> and</c><00:05:13.490><c> it</c><00:05:14.060><c> needs</c><00:05:14.270><c> to</c><00:05:14.389><c> be</c>

00:05:14.500 --> 00:05:14.510 align:start position:0%
those features are and it needs to be
 

00:05:14.510 --> 00:05:16.840 align:start position:0%
those features are and it needs to be
able<00:05:14.660><c> to</c><00:05:14.810><c> detect</c><00:05:15.289><c> those</c><00:05:15.680><c> features</c><00:05:16.100><c> in</c><00:05:16.280><c> an</c>

00:05:16.840 --> 00:05:16.850 align:start position:0%
able to detect those features in an
 

00:05:16.850 --> 00:05:19.270 align:start position:0%
able to detect those features in an
image<00:05:17.180><c> in</c><00:05:17.389><c> order</c><00:05:18.080><c> to</c><00:05:18.320><c> generate</c><00:05:18.889><c> this</c>

00:05:19.270 --> 00:05:19.280 align:start position:0%
image in order to generate this
 

00:05:19.280 --> 00:05:22.839 align:start position:0%
image in order to generate this
prediction<00:05:20.889><c> one</c><00:05:21.889><c> way</c><00:05:22.039><c> to</c><00:05:22.099><c> solve</c><00:05:22.460><c> this</c><00:05:22.639><c> problem</c>

00:05:22.839 --> 00:05:22.849 align:start position:0%
prediction one way to solve this problem
 

00:05:22.849 --> 00:05:25.719 align:start position:0%
prediction one way to solve this problem
is<00:05:23.300><c> to</c><00:05:23.630><c> leverage</c><00:05:23.870><c> our</c><00:05:24.830><c> knowledge</c><00:05:25.370><c> about</c><00:05:25.520><c> a</c>

00:05:25.719 --> 00:05:25.729 align:start position:0%
is to leverage our knowledge about a
 

00:05:25.729 --> 00:05:28.450 align:start position:0%
is to leverage our knowledge about a
particular<00:05:26.330><c> field</c><00:05:26.660><c> say</c><00:05:27.349><c> those</c><00:05:27.979><c> of</c><00:05:28.160><c> human</c>

00:05:28.450 --> 00:05:28.460 align:start position:0%
particular field say those of human
 

00:05:28.460 --> 00:05:31.540 align:start position:0%
particular field say those of human
faces<00:05:28.669><c> and</c><00:05:29.150><c> use</c><00:05:29.870><c> our</c><00:05:30.199><c> prior</c><00:05:30.770><c> knowledge</c><00:05:31.039><c> to</c>

00:05:31.540 --> 00:05:31.550 align:start position:0%
faces and use our prior knowledge to
 

00:05:31.550 --> 00:05:35.050 align:start position:0%
faces and use our prior knowledge to
define<00:05:32.000><c> those</c><00:05:32.870><c> features</c><00:05:33.320><c> ourselves</c><00:05:34.010><c> and</c><00:05:34.220><c> so</c><00:05:35.030><c> a</c>

00:05:35.050 --> 00:05:35.060 align:start position:0%
define those features ourselves and so a
 

00:05:35.060 --> 00:05:37.089 align:start position:0%
define those features ourselves and so a
classification<00:05:35.630><c> pipeline</c><00:05:36.349><c> would</c><00:05:36.530><c> then</c><00:05:36.770><c> try</c>

00:05:37.089 --> 00:05:37.099 align:start position:0%
classification pipeline would then try
 

00:05:37.099 --> 00:05:39.370 align:start position:0%
classification pipeline would then try
to<00:05:37.160><c> detect</c><00:05:37.669><c> these</c><00:05:38.240><c> manually</c><00:05:38.960><c> defined</c>

00:05:39.370 --> 00:05:39.380 align:start position:0%
to detect these manually defined
 

00:05:39.380 --> 00:05:42.159 align:start position:0%
to detect these manually defined
features<00:05:39.680><c> and</c><00:05:40.099><c> images</c><00:05:40.550><c> and</c><00:05:40.760><c> use</c><00:05:41.510><c> the</c><00:05:41.690><c> results</c>

00:05:42.159 --> 00:05:42.169 align:start position:0%
features and images and use the results
 

00:05:42.169 --> 00:05:44.290 align:start position:0%
features and images and use the results
of<00:05:42.229><c> some</c><00:05:42.560><c> sort</c><00:05:42.800><c> of</c><00:05:42.860><c> detection</c><00:05:43.250><c> algorithm</c><00:05:43.940><c> to</c>

00:05:44.290 --> 00:05:44.300 align:start position:0%
of some sort of detection algorithm to
 

00:05:44.300 --> 00:05:47.589 align:start position:0%
of some sort of detection algorithm to
do<00:05:44.449><c> the</c><00:05:44.599><c> classification</c><00:05:45.460><c> but</c><00:05:46.460><c> there's</c><00:05:47.060><c> a</c><00:05:47.330><c> big</c>

00:05:47.589 --> 00:05:47.599 align:start position:0%
do the classification but there's a big
 

00:05:47.599 --> 00:05:50.379 align:start position:0%
do the classification but there's a big
problem<00:05:47.870><c> with</c><00:05:48.320><c> this</c><00:05:48.470><c> approach</c><00:05:48.530><c> and</c><00:05:49.240><c> if</c><00:05:50.240><c> you</c>

00:05:50.379 --> 00:05:50.389 align:start position:0%
problem with this approach and if you
 

00:05:50.389 --> 00:05:53.170 align:start position:0%
problem with this approach and if you
remember<00:05:50.710><c> images</c><00:05:51.710><c> are</c><00:05:51.830><c> just</c><00:05:52.010><c> 3d</c><00:05:52.460><c> arrays</c><00:05:52.909><c> of</c>

00:05:53.170 --> 00:05:53.180 align:start position:0%
remember images are just 3d arrays of
 

00:05:53.180 --> 00:05:55.540 align:start position:0%
remember images are just 3d arrays of
effectively<00:05:53.780><c> brightness</c><00:05:54.260><c> values</c><00:05:54.650><c> and</c><00:05:54.919><c> they</c>

00:05:55.540 --> 00:05:55.550 align:start position:0%
effectively brightness values and they
 

00:05:55.550 --> 00:05:57.700 align:start position:0%
effectively brightness values and they
can<00:05:55.729><c> have</c><00:05:55.940><c> lots</c><00:05:56.330><c> and</c><00:05:56.750><c> lots</c><00:05:57.050><c> and</c><00:05:57.229><c> lots</c><00:05:57.500><c> of</c>

00:05:57.700 --> 00:05:57.710 align:start position:0%
can have lots and lots and lots of
 

00:05:57.710 --> 00:06:00.640 align:start position:0%
can have lots and lots and lots of
variation<00:05:58.340><c> such</c><00:05:58.909><c> as</c><00:05:59.060><c> occlusion</c><00:05:59.650><c> variations</c>

00:06:00.640 --> 00:06:00.650 align:start position:0%
variation such as occlusion variations
 

00:06:00.650 --> 00:06:03.879 align:start position:0%
variation such as occlusion variations
in<00:06:00.830><c> illumination</c><00:06:01.520><c> and</c><00:06:02.050><c> intraclass</c><00:06:03.050><c> variation</c>

00:06:03.879 --> 00:06:03.889 align:start position:0%
in illumination and intraclass variation
 

00:06:03.889 --> 00:06:07.210 align:start position:0%
in illumination and intraclass variation
and<00:06:04.659><c> if</c><00:06:05.659><c> we</c><00:06:05.810><c> want</c><00:06:06.020><c> to</c><00:06:06.110><c> build</c><00:06:06.380><c> a</c><00:06:06.650><c> robust</c>

00:06:07.210 --> 00:06:07.220 align:start position:0%
and if we want to build a robust
 

00:06:07.220 --> 00:06:10.120 align:start position:0%
and if we want to build a robust
pipeline<00:06:07.669><c> for</c><00:06:08.270><c> doing</c><00:06:09.169><c> this</c><00:06:09.289><c> classification</c>

00:06:10.120 --> 00:06:10.130 align:start position:0%
pipeline for doing this classification
 

00:06:10.130 --> 00:06:13.029 align:start position:0%
pipeline for doing this classification
task<00:06:10.430><c> our</c><00:06:10.940><c> model</c><00:06:11.690><c> has</c><00:06:11.900><c> to</c><00:06:12.199><c> be</c><00:06:12.320><c> invariant</c><00:06:12.889><c> to</c>

00:06:13.029 --> 00:06:13.039 align:start position:0%
task our model has to be invariant to
 

00:06:13.039 --> 00:06:15.159 align:start position:0%
task our model has to be invariant to
these<00:06:13.190><c> variations</c><00:06:13.849><c> while</c><00:06:14.479><c> still</c><00:06:14.930><c> being</c>

00:06:15.159 --> 00:06:15.169 align:start position:0%
these variations while still being
 

00:06:15.169 --> 00:06:18.310 align:start position:0%
these variations while still being
sensitive<00:06:15.740><c> to</c><00:06:16.039><c> the</c><00:06:16.699><c> differences</c><00:06:17.300><c> that</c><00:06:17.360><c> define</c>

00:06:18.310 --> 00:06:18.320 align:start position:0%
sensitive to the differences that define
 

00:06:18.320 --> 00:06:22.960 align:start position:0%
sensitive to the differences that define
the<00:06:18.590><c> individual</c><00:06:19.250><c> classes</c><00:06:21.789><c> even</c><00:06:22.789><c> though</c><00:06:22.940><c> our</c>

00:06:22.960 --> 00:06:22.970 align:start position:0%
the individual classes even though our
 

00:06:22.970 --> 00:06:25.450 align:start position:0%
the individual classes even though our
pipeline<00:06:23.690><c> could</c><00:06:23.990><c> use</c><00:06:24.349><c> these</c><00:06:24.889><c> features</c><00:06:25.310><c> that</c>

00:06:25.450 --> 00:06:25.460 align:start position:0%
pipeline could use these features that
 

00:06:25.460 --> 00:06:28.870 align:start position:0%
pipeline could use these features that
we<00:06:25.699><c> the</c><00:06:26.479><c> human</c><00:06:26.659><c> define</c><00:06:27.349><c> where</c><00:06:28.130><c> this</c><00:06:28.310><c> manual</c>

00:06:28.870 --> 00:06:28.880 align:start position:0%
we the human define where this manual
 

00:06:28.880 --> 00:06:31.270 align:start position:0%
we the human define where this manual
extraction<00:06:29.150><c> will</c><00:06:29.690><c> break</c><00:06:29.930><c> down</c><00:06:29.960><c> is</c><00:06:30.470><c> actually</c>

00:06:31.270 --> 00:06:31.280 align:start position:0%
extraction will break down is actually
 

00:06:31.280 --> 00:06:34.390 align:start position:0%
extraction will break down is actually
in<00:06:31.460><c> the</c><00:06:31.729><c> detection</c><00:06:32.240><c> task</c><00:06:32.449><c> and</c><00:06:32.930><c> that's</c><00:06:33.770><c> again</c>

00:06:34.390 --> 00:06:34.400 align:start position:0%
in the detection task and that's again
 

00:06:34.400 --> 00:06:36.459 align:start position:0%
in the detection task and that's again
due<00:06:34.669><c> to</c><00:06:34.699><c> the</c><00:06:34.940><c> incredible</c><00:06:35.270><c> variability</c><00:06:35.840><c> in</c>

00:06:36.459 --> 00:06:36.469 align:start position:0%
due to the incredible variability in
 

00:06:36.469 --> 00:06:39.790 align:start position:0%
due to the incredible variability in
visual<00:06:37.070><c> data</c><00:06:38.289><c> because</c><00:06:39.289><c> of</c><00:06:39.469><c> this</c><00:06:39.590><c> the</c>

00:06:39.790 --> 00:06:39.800 align:start position:0%
visual data because of this the
 

00:06:39.800 --> 00:06:41.330 align:start position:0%
visual data because of this the
detection<00:06:40.190><c> of</c><00:06:40.400><c> these</c><00:06:40.580><c> features</c><00:06:41.000><c> is</c>

00:06:41.330 --> 00:06:41.340 align:start position:0%
detection of these features is
 

00:06:41.340 --> 00:06:43.060 align:start position:0%
detection of these features is
actually<00:06:41.760><c> really</c><00:06:42.030><c> difficult</c><00:06:42.330><c> in</c><00:06:42.720><c> practice</c>

00:06:43.060 --> 00:06:43.070 align:start position:0%
actually really difficult in practice
 

00:06:43.070 --> 00:06:45.740 align:start position:0%
actually really difficult in practice
because<00:06:44.070><c> your</c><00:06:44.430><c> detection</c><00:06:44.940><c> algorithm</c><00:06:45.510><c> would</c>

00:06:45.740 --> 00:06:45.750 align:start position:0%
because your detection algorithm would
 

00:06:45.750 --> 00:06:48.110 align:start position:0%
because your detection algorithm would
need<00:06:46.050><c> to</c><00:06:46.320><c> withstand</c><00:06:46.710><c> each</c><00:06:47.310><c> of</c><00:06:47.699><c> these</c>

00:06:48.110 --> 00:06:48.120 align:start position:0%
need to withstand each of these
 

00:06:48.120 --> 00:06:52.040 align:start position:0%
need to withstand each of these
different<00:06:48.540><c> variations</c><00:06:49.790><c> so</c><00:06:50.790><c> how</c><00:06:51.570><c> can</c><00:06:51.630><c> we</c><00:06:51.900><c> do</c>

00:06:52.040 --> 00:06:52.050 align:start position:0%
different variations so how can we do
 

00:06:52.050 --> 00:06:55.490 align:start position:0%
different variations so how can we do
better<00:06:52.160><c> we</c><00:06:53.160><c> want</c><00:06:53.370><c> a</c><00:06:53.490><c> way</c><00:06:53.669><c> to</c><00:06:53.840><c> both</c><00:06:54.840><c> extract</c>

00:06:55.490 --> 00:06:55.500 align:start position:0%
better we want a way to both extract
 

00:06:55.500 --> 00:06:58.040 align:start position:0%
better we want a way to both extract
features<00:06:55.800><c> and</c><00:06:56.310><c> detect</c><00:06:57.030><c> their</c><00:06:57.270><c> presence</c><00:06:57.870><c> in</c>

00:06:58.040 --> 00:06:58.050 align:start position:0%
features and detect their presence in
 

00:06:58.050 --> 00:07:00.890 align:start position:0%
features and detect their presence in
images<00:06:58.500><c> automatically</c><00:06:59.220><c> in</c><00:06:59.610><c> a</c><00:07:00.030><c> hierarchical</c>

00:07:00.890 --> 00:07:00.900 align:start position:0%
images automatically in a hierarchical
 

00:07:00.900 --> 00:07:03.830 align:start position:0%
images automatically in a hierarchical
fashion<00:07:01.080><c> and</c><00:07:01.880><c> again</c><00:07:02.880><c> right</c><00:07:03.180><c> you</c><00:07:03.389><c> came</c><00:07:03.630><c> to</c><00:07:03.810><c> a</c>

00:07:03.830 --> 00:07:03.840 align:start position:0%
fashion and again right you came to a
 

00:07:03.840 --> 00:07:07.219 align:start position:0%
fashion and again right you came to a
class<00:07:04.110><c> on</c><00:07:04.320><c> deep</c><00:07:04.530><c> learning</c><00:07:05.240><c> we</c><00:07:06.240><c> we</c><00:07:06.870><c> we</c>

00:07:07.219 --> 00:07:07.229 align:start position:0%
class on deep learning we we we
 

00:07:07.229 --> 00:07:09.140 align:start position:0%
class on deep learning we we we
hypothesize<00:07:07.919><c> right</c><00:07:08.160><c> that</c><00:07:08.370><c> we</c><00:07:08.460><c> could</c><00:07:08.639><c> use</c><00:07:08.940><c> a</c>

00:07:09.140 --> 00:07:09.150 align:start position:0%
hypothesize right that we could use a
 

00:07:09.150 --> 00:07:11.780 align:start position:0%
hypothesize right that we could use a
neural<00:07:09.630><c> network</c><00:07:09.840><c> based</c><00:07:10.290><c> approach</c><00:07:10.889><c> to</c><00:07:11.520><c> learn</c>

00:07:11.780 --> 00:07:11.790 align:start position:0%
neural network based approach to learn
 

00:07:11.790 --> 00:07:14.030 align:start position:0%
neural network based approach to learn
visual<00:07:12.540><c> features</c><00:07:12.900><c> directly</c><00:07:13.470><c> from</c><00:07:13.770><c> data</c>

00:07:14.030 --> 00:07:14.040 align:start position:0%
visual features directly from data
 

00:07:14.040 --> 00:07:17.659 align:start position:0%
visual features directly from data
without<00:07:14.669><c> any</c><00:07:15.020><c> you</c><00:07:16.020><c> know</c><00:07:16.139><c> manual</c><00:07:16.830><c> definition</c>

00:07:17.659 --> 00:07:17.669 align:start position:0%
without any you know manual definition
 

00:07:17.669 --> 00:07:20.240 align:start position:0%
without any you know manual definition
and<00:07:17.970><c> to</c><00:07:18.690><c> learn</c><00:07:18.900><c> a</c><00:07:19.229><c> hierarchy</c><00:07:19.979><c> of</c><00:07:20.010><c> these</c>

00:07:20.240 --> 00:07:20.250 align:start position:0%
and to learn a hierarchy of these
 

00:07:20.250 --> 00:07:22.460 align:start position:0%
and to learn a hierarchy of these
features<00:07:20.669><c> to</c><00:07:21.240><c> construct</c><00:07:21.750><c> a</c><00:07:21.960><c> representation</c>

00:07:22.460 --> 00:07:22.470 align:start position:0%
features to construct a representation
 

00:07:22.470 --> 00:07:24.379 align:start position:0%
features to construct a representation
of<00:07:22.979><c> the</c><00:07:23.070><c> image</c><00:07:23.220><c> that's</c><00:07:23.639><c> internal</c><00:07:24.210><c> to</c><00:07:24.360><c> the</c>

00:07:24.379 --> 00:07:24.389 align:start position:0%
of the image that's internal to the
 

00:07:24.389 --> 00:07:27.409 align:start position:0%
of the image that's internal to the
network<00:07:24.840><c> for</c><00:07:25.650><c> example</c><00:07:25.680><c> if</c><00:07:26.280><c> we</c><00:07:26.910><c> wanted</c><00:07:27.180><c> to</c><00:07:27.330><c> be</c>

00:07:27.409 --> 00:07:27.419 align:start position:0%
network for example if we wanted to be
 

00:07:27.419 --> 00:07:29.930 align:start position:0%
network for example if we wanted to be
able<00:07:27.690><c> to</c><00:07:27.780><c> classify</c><00:07:28.020><c> images</c><00:07:28.530><c> of</c><00:07:28.950><c> faces</c><00:07:29.160><c> maybe</c>

00:07:29.930 --> 00:07:29.940 align:start position:0%
able to classify images of faces maybe
 

00:07:29.940 --> 00:07:32.629 align:start position:0%
able to classify images of faces maybe
we<00:07:30.210><c> could</c><00:07:30.360><c> learn</c><00:07:30.750><c> how</c><00:07:30.990><c> to</c><00:07:31.050><c> detect</c><00:07:31.639><c> low-level</c>

00:07:32.629 --> 00:07:32.639 align:start position:0%
we could learn how to detect low-level
 

00:07:32.639 --> 00:07:35.330 align:start position:0%
we could learn how to detect low-level
features<00:07:32.789><c> like</c><00:07:33.600><c> edges</c><00:07:34.020><c> and</c><00:07:34.169><c> dark</c><00:07:34.410><c> spots</c><00:07:34.560><c> mid</c>

00:07:35.330 --> 00:07:35.340 align:start position:0%
features like edges and dark spots mid
 

00:07:35.340 --> 00:07:38.060 align:start position:0%
features like edges and dark spots mid
level<00:07:35.700><c> features</c><00:07:36.060><c> like</c><00:07:36.389><c> eyes</c><00:07:36.600><c> ears</c><00:07:37.139><c> and</c><00:07:37.470><c> noses</c>

00:07:38.060 --> 00:07:38.070 align:start position:0%
level features like eyes ears and noses
 

00:07:38.070 --> 00:07:40.189 align:start position:0%
level features like eyes ears and noses
and<00:07:38.490><c> then</c><00:07:38.970><c> high</c><00:07:39.270><c> level</c><00:07:39.660><c> features</c><00:07:39.990><c> that</c>

00:07:40.189 --> 00:07:40.199 align:start position:0%
and then high level features that
 

00:07:40.199 --> 00:07:43.540 align:start position:0%
and then high level features that
actually<00:07:40.680><c> resemble</c><00:07:41.250><c> facial</c><00:07:41.760><c> structure</c><00:07:42.180><c> and</c>

00:07:43.540 --> 00:07:43.550 align:start position:0%
actually resemble facial structure and
 

00:07:43.550 --> 00:07:46.909 align:start position:0%
actually resemble facial structure and
we'll<00:07:44.550><c> see</c><00:07:44.789><c> how</c><00:07:45.150><c> neural</c><00:07:45.690><c> networks</c><00:07:46.110><c> will</c><00:07:46.560><c> allow</c>

00:07:46.909 --> 00:07:46.919 align:start position:0%
we'll see how neural networks will allow
 

00:07:46.919 --> 00:07:49.279 align:start position:0%
we'll see how neural networks will allow
us<00:07:47.130><c> to</c><00:07:47.160><c> directly</c><00:07:48.060><c> learn</c><00:07:48.630><c> these</c><00:07:48.840><c> visual</c>

00:07:49.279 --> 00:07:49.289 align:start position:0%
us to directly learn these visual
 

00:07:49.289 --> 00:07:52.190 align:start position:0%
us to directly learn these visual
features<00:07:49.680><c> from</c><00:07:50.370><c> visual</c><00:07:50.789><c> data</c><00:07:51.510><c> if</c><00:07:51.840><c> we</c>

00:07:52.190 --> 00:07:52.200 align:start position:0%
features from visual data if we
 

00:07:52.200 --> 00:07:56.690 align:start position:0%
features from visual data if we
construct<00:07:52.560><c> them</c><00:07:52.919><c> cleverly</c><00:07:55.250><c> going</c><00:07:56.250><c> back</c><00:07:56.430><c> in</c>

00:07:56.690 --> 00:07:56.700 align:start position:0%
construct them cleverly going back in
 

00:07:56.700 --> 00:07:59.330 align:start position:0%
construct them cleverly going back in
lecture<00:07:57.389><c> 1</c><00:07:57.599><c> right</c><00:07:57.900><c> we</c><00:07:58.410><c> learned</c><00:07:58.650><c> about</c><00:07:58.919><c> fully</c>

00:07:59.330 --> 00:07:59.340 align:start position:0%
lecture 1 right we learned about fully
 

00:07:59.340 --> 00:08:01.909 align:start position:0%
lecture 1 right we learned about fully
connected<00:08:00.050><c> architectures</c><00:08:01.050><c> where</c><00:08:01.590><c> you</c><00:08:01.800><c> can</c>

00:08:01.909 --> 00:08:01.919 align:start position:0%
connected architectures where you can
 

00:08:01.919 --> 00:08:04.129 align:start position:0%
connected architectures where you can
have<00:08:02.130><c> multiple</c><00:08:02.400><c> hidden</c><00:08:02.970><c> layers</c><00:08:03.120><c> and</c><00:08:03.450><c> where</c>

00:08:04.129 --> 00:08:04.139 align:start position:0%
have multiple hidden layers and where
 

00:08:04.139 --> 00:08:06.140 align:start position:0%
have multiple hidden layers and where
each<00:08:04.410><c> neuron</c><00:08:04.740><c> in</c><00:08:05.130><c> a</c><00:08:05.250><c> given</c><00:08:05.700><c> layer</c><00:08:05.880><c> is</c>

00:08:06.140 --> 00:08:06.150 align:start position:0%
each neuron in a given layer is
 

00:08:06.150 --> 00:08:08.330 align:start position:0%
each neuron in a given layer is
connected<00:08:06.930><c> to</c><00:08:06.960><c> every</c><00:08:07.349><c> single</c><00:08:07.560><c> neuron</c><00:08:07.979><c> in</c><00:08:08.220><c> the</c>

00:08:08.330 --> 00:08:08.340 align:start position:0%
connected to every single neuron in the
 

00:08:08.340 --> 00:08:11.480 align:start position:0%
connected to every single neuron in the
subsequent<00:08:08.700><c> layer</c><00:08:09.030><c> and</c><00:08:09.479><c> let's</c><00:08:10.440><c> say</c><00:08:10.650><c> that</c><00:08:10.950><c> we</c>

00:08:11.480 --> 00:08:11.490 align:start position:0%
subsequent layer and let's say that we
 

00:08:11.490 --> 00:08:13.610 align:start position:0%
subsequent layer and let's say that we
wanted<00:08:11.880><c> to</c><00:08:11.940><c> use</c><00:08:12.330><c> a</c><00:08:12.570><c> fully</c><00:08:12.870><c> connected</c><00:08:13.169><c> neural</c>

00:08:13.610 --> 00:08:13.620 align:start position:0%
wanted to use a fully connected neural
 

00:08:13.620 --> 00:08:17.000 align:start position:0%
wanted to use a fully connected neural
network<00:08:13.950><c> for</c><00:08:13.979><c> image</c><00:08:14.700><c> classification</c><00:08:15.479><c> in</c><00:08:16.410><c> this</c>

00:08:17.000 --> 00:08:17.010 align:start position:0%
network for image classification in this
 

00:08:17.010 --> 00:08:19.909 align:start position:0%
network for image classification in this
case<00:08:17.280><c> our</c><00:08:17.520><c> 2d</c><00:08:17.940><c> input</c><00:08:18.330><c> image</c><00:08:18.660><c> is</c><00:08:18.990><c> transformed</c>

00:08:19.909 --> 00:08:19.919 align:start position:0%
case our 2d input image is transformed
 

00:08:19.919 --> 00:08:22.550 align:start position:0%
case our 2d input image is transformed
into<00:08:20.190><c> a</c><00:08:20.340><c> vector</c><00:08:20.580><c> of</c><00:08:20.880><c> pixel</c><00:08:21.240><c> values</c><00:08:21.690><c> and</c><00:08:21.960><c> this</c>

00:08:22.550 --> 00:08:22.560 align:start position:0%
into a vector of pixel values and this
 

00:08:22.560 --> 00:08:24.680 align:start position:0%
into a vector of pixel values and this
vector<00:08:23.099><c> is</c><00:08:23.430><c> then</c><00:08:23.550><c> fed</c><00:08:23.849><c> into</c><00:08:23.910><c> the</c><00:08:24.330><c> network</c>

00:08:24.680 --> 00:08:24.690 align:start position:0%
vector is then fed into the network
 

00:08:24.690 --> 00:08:27.200 align:start position:0%
vector is then fed into the network
where<00:08:25.560><c> each</c><00:08:25.800><c> neuron</c><00:08:26.190><c> in</c><00:08:26.430><c> the</c><00:08:26.580><c> hidden</c><00:08:26.849><c> in</c><00:08:26.970><c> the</c>

00:08:27.200 --> 00:08:27.210 align:start position:0%
where each neuron in the hidden in the
 

00:08:27.210 --> 00:08:29.360 align:start position:0%
where each neuron in the hidden in the
first<00:08:27.419><c> hidden</c><00:08:27.630><c> layer</c><00:08:27.750><c> is</c><00:08:28.169><c> connected</c><00:08:29.070><c> to</c><00:08:29.099><c> all</c>

00:08:29.360 --> 00:08:29.370 align:start position:0%
first hidden layer is connected to all
 

00:08:29.370 --> 00:08:32.089 align:start position:0%
first hidden layer is connected to all
neurons<00:08:29.760><c> in</c><00:08:30.150><c> the</c><00:08:30.360><c> input</c><00:08:30.690><c> layer</c><00:08:30.840><c> and</c><00:08:31.200><c> here</c>

00:08:32.089 --> 00:08:32.099 align:start position:0%
neurons in the input layer and here
 

00:08:32.099 --> 00:08:34.519 align:start position:0%
neurons in the input layer and here
hopefully<00:08:32.640><c> you</c><00:08:32.760><c> can</c><00:08:32.880><c> appreciate</c><00:08:33.240><c> that</c><00:08:33.539><c> by</c>

00:08:34.519 --> 00:08:34.529 align:start position:0%
hopefully you can appreciate that by
 

00:08:34.529 --> 00:08:38.149 align:start position:0%
hopefully you can appreciate that by
squashing<00:08:35.219><c> our</c><00:08:35.370><c> 2d</c><00:08:35.820><c> our</c><00:08:36.659><c> 2d</c><00:08:37.320><c> matrix</c><00:08:37.500><c> into</c><00:08:38.130><c> this</c>

00:08:38.149 --> 00:08:38.159 align:start position:0%
squashing our 2d our 2d matrix into this
 

00:08:38.159 --> 00:08:41.060 align:start position:0%
squashing our 2d our 2d matrix into this
1d<00:08:38.940><c> vector</c><00:08:39.360><c> and</c><00:08:39.630><c> defining</c><00:08:40.380><c> these</c><00:08:40.709><c> fully</c>

00:08:41.060 --> 00:08:41.070 align:start position:0%
1d vector and defining these fully
 

00:08:41.070 --> 00:08:44.990 align:start position:0%
1d vector and defining these fully
connected<00:08:42.979><c> connections</c><00:08:43.979><c> all</c><00:08:44.310><c> spatial</c>

00:08:44.990 --> 00:08:45.000 align:start position:0%
connected connections all spatial
 

00:08:45.000 --> 00:08:47.470 align:start position:0%
connected connections all spatial
information<00:08:45.900><c> is</c><00:08:46.080><c> completely</c><00:08:46.680><c> lost</c>

00:08:47.470 --> 00:08:47.480 align:start position:0%
information is completely lost
 

00:08:47.480 --> 00:08:50.120 align:start position:0%
information is completely lost
furthermore<00:08:48.480><c> in</c><00:08:48.750><c> in</c><00:08:49.200><c> defining</c><00:08:49.740><c> the</c><00:08:49.829><c> network</c>

00:08:50.120 --> 00:08:50.130 align:start position:0%
furthermore in in defining the network
 

00:08:50.130 --> 00:08:52.240 align:start position:0%
furthermore in in defining the network
in<00:08:50.280><c> this</c><00:08:50.430><c> way</c><00:08:50.670><c> we</c><00:08:51.089><c> end</c><00:08:51.300><c> up</c><00:08:51.420><c> having</c><00:08:51.600><c> men</c>

00:08:52.240 --> 00:08:52.250 align:start position:0%
in this way we end up having men
 

00:08:52.250 --> 00:08:54.940 align:start position:0%
in this way we end up having men
many<00:08:52.490><c> different</c><00:08:52.970><c> parameters</c><00:08:53.510><c> right</c><00:08:53.780><c> you</c><00:08:54.710><c> need</c>

00:08:54.940 --> 00:08:54.950 align:start position:0%
many different parameters right you need
 

00:08:54.950 --> 00:08:57.040 align:start position:0%
many different parameters right you need
a<00:08:55.010><c> different</c><00:08:55.340><c> weight</c><00:08:55.970><c> parameter</c><00:08:56.480><c> for</c><00:08:56.660><c> every</c>

00:08:57.040 --> 00:08:57.050 align:start position:0%
a different weight parameter for every
 

00:08:57.050 --> 00:08:59.380 align:start position:0%
a different weight parameter for every
single<00:08:57.500><c> neural</c><00:08:58.130><c> connection</c><00:08:58.640><c> in</c><00:08:58.790><c> your</c><00:08:58.820><c> network</c>

00:08:59.380 --> 00:08:59.390 align:start position:0%
single neural connection in your network
 

00:08:59.390 --> 00:09:02.140 align:start position:0%
single neural connection in your network
because<00:09:00.260><c> it's</c><00:09:00.470><c> fully</c><00:09:00.680><c> connected</c><00:09:01.160><c> and</c><00:09:01.340><c> this</c>

00:09:02.140 --> 00:09:02.150 align:start position:0%
because it's fully connected and this
 

00:09:02.150 --> 00:09:04.840 align:start position:0%
because it's fully connected and this
means<00:09:02.360><c> that</c><00:09:02.950><c> training</c><00:09:03.950><c> in</c><00:09:04.040><c> network</c><00:09:04.340><c> like</c><00:09:04.580><c> this</c>

00:09:04.840 --> 00:09:04.850 align:start position:0%
means that training in network like this
 

00:09:04.850 --> 00:09:07.770 align:start position:0%
means that training in network like this
on<00:09:05.150><c> a</c><00:09:05.630><c> task</c><00:09:06.260><c> like</c><00:09:06.680><c> image</c><00:09:07.220><c> classification</c>

00:09:07.770 --> 00:09:07.780 align:start position:0%
on a task like image classification
 

00:09:07.780 --> 00:09:11.250 align:start position:0%
on a task like image classification
becomes<00:09:08.780><c> infeasible</c><00:09:09.200><c> in</c><00:09:09.650><c> practice</c>

00:09:11.250 --> 00:09:11.260 align:start position:0%
becomes infeasible in practice
 

00:09:11.260 --> 00:09:13.960 align:start position:0%
becomes infeasible in practice
importantly<00:09:12.260><c> right</c><00:09:12.590><c> visual</c><00:09:13.160><c> data</c><00:09:13.430><c> has</c><00:09:13.760><c> this</c>

00:09:13.960 --> 00:09:13.970 align:start position:0%
importantly right visual data has this
 

00:09:13.970 --> 00:09:16.900 align:start position:0%
importantly right visual data has this
really<00:09:14.450><c> rich</c><00:09:14.660><c> spatial</c><00:09:15.380><c> structure</c><00:09:15.830><c> how</c><00:09:16.490><c> can</c><00:09:16.760><c> we</c>

00:09:16.900 --> 00:09:16.910 align:start position:0%
really rich spatial structure how can we
 

00:09:16.910 --> 00:09:19.390 align:start position:0%
really rich spatial structure how can we
leverage<00:09:17.150><c> this</c><00:09:17.540><c> to</c><00:09:17.960><c> inform</c><00:09:18.470><c> the</c><00:09:18.710><c> architecture</c>

00:09:19.390 --> 00:09:19.400 align:start position:0%
leverage this to inform the architecture
 

00:09:19.400 --> 00:09:23.410 align:start position:0%
leverage this to inform the architecture
of<00:09:19.430><c> the</c><00:09:19.730><c> network</c><00:09:20.120><c> that</c><00:09:20.630><c> we</c><00:09:20.690><c> design</c><00:09:22.120><c> to</c><00:09:23.120><c> do</c><00:09:23.270><c> this</c>

00:09:23.410 --> 00:09:23.420 align:start position:0%
of the network that we design to do this
 

00:09:23.420 --> 00:09:26.590 align:start position:0%
of the network that we design to do this
let's<00:09:24.170><c> represent</c><00:09:24.410><c> our</c><00:09:25.040><c> 2d</c><00:09:25.460><c> input</c><00:09:25.760><c> image</c><00:09:26.030><c> as</c><00:09:26.240><c> an</c>

00:09:26.590 --> 00:09:26.600 align:start position:0%
let's represent our 2d input image as an
 

00:09:26.600 --> 00:09:28.480 align:start position:0%
let's represent our 2d input image as an
array<00:09:26.900><c> of</c><00:09:26.930><c> pixel</c><00:09:27.440><c> values</c><00:09:27.770><c> like</c><00:09:27.980><c> I</c><00:09:28.100><c> mentioned</c>

00:09:28.480 --> 00:09:28.490 align:start position:0%
array of pixel values like I mentioned
 

00:09:28.490 --> 00:09:30.790 align:start position:0%
array of pixel values like I mentioned
before<00:09:28.580><c> and</c><00:09:29.030><c> one</c><00:09:29.870><c> way</c><00:09:30.080><c> we</c><00:09:30.110><c> can</c><00:09:30.440><c> immediately</c>

00:09:30.790 --> 00:09:30.800 align:start position:0%
before and one way we can immediately
 

00:09:30.800 --> 00:09:32.830 align:start position:0%
before and one way we can immediately
use<00:09:31.160><c> the</c><00:09:31.640><c> spatial</c><00:09:32.120><c> structure</c><00:09:32.570><c> that's</c>

00:09:32.830 --> 00:09:32.840 align:start position:0%
use the spatial structure that's
 

00:09:32.840 --> 00:09:35.580 align:start position:0%
use the spatial structure that's
inherent<00:09:33.470><c> to</c><00:09:33.530><c> this</c><00:09:33.770><c> input</c><00:09:33.980><c> is</c><00:09:34.460><c> to</c><00:09:35.150><c> connect</c>

00:09:35.580 --> 00:09:35.590 align:start position:0%
inherent to this input is to connect
 

00:09:35.590 --> 00:09:38.500 align:start position:0%
inherent to this input is to connect
patches<00:09:36.590><c> of</c><00:09:36.830><c> the</c><00:09:36.950><c> input</c><00:09:37.370><c> to</c><00:09:37.820><c> neurons</c><00:09:38.150><c> in</c><00:09:38.390><c> the</c>

00:09:38.500 --> 00:09:38.510 align:start position:0%
patches of the input to neurons in the
 

00:09:38.510 --> 00:09:41.350 align:start position:0%
patches of the input to neurons in the
hidden<00:09:38.780><c> layer</c><00:09:39.670><c> another</c><00:09:40.670><c> way</c><00:09:40.880><c> of</c><00:09:41.030><c> thinking</c>

00:09:41.350 --> 00:09:41.360 align:start position:0%
hidden layer another way of thinking
 

00:09:41.360 --> 00:09:43.450 align:start position:0%
hidden layer another way of thinking
about<00:09:41.390><c> this</c><00:09:41.750><c> is</c><00:09:41.960><c> that</c><00:09:42.200><c> each</c><00:09:42.470><c> neuron</c><00:09:42.920><c> in</c><00:09:43.220><c> a</c>

00:09:43.450 --> 00:09:43.460 align:start position:0%
about this is that each neuron in a
 

00:09:43.460 --> 00:09:45.940 align:start position:0%
about this is that each neuron in a
hidden<00:09:43.790><c> layer</c><00:09:43.970><c> only</c><00:09:44.240><c> sees</c><00:09:44.840><c> a</c><00:09:45.200><c> particular</c>

00:09:45.940 --> 00:09:45.950 align:start position:0%
hidden layer only sees a particular
 

00:09:45.950 --> 00:09:48.640 align:start position:0%
hidden layer only sees a particular
region<00:09:46.520><c> of</c><00:09:46.670><c> what</c><00:09:47.480><c> the</c><00:09:47.720><c> input</c><00:09:48.140><c> to</c><00:09:48.290><c> that</c><00:09:48.320><c> layer</c>

00:09:48.640 --> 00:09:48.650 align:start position:0%
region of what the input to that layer
 

00:09:48.650 --> 00:09:51.880 align:start position:0%
region of what the input to that layer
is<00:09:48.980><c> and</c><00:09:49.430><c> this</c><00:09:50.150><c> not</c><00:09:50.390><c> only</c><00:09:50.720><c> reduces</c><00:09:51.380><c> the</c><00:09:51.530><c> number</c>

00:09:51.880 --> 00:09:51.890 align:start position:0%
is and this not only reduces the number
 

00:09:51.890 --> 00:09:55.150 align:start position:0%
is and this not only reduces the number
of<00:09:52.040><c> weights</c><00:09:52.310><c> in</c><00:09:52.670><c> our</c><00:09:52.820><c> model</c><00:09:53.290><c> but</c><00:09:54.290><c> also</c><00:09:54.350><c> allows</c>

00:09:55.150 --> 00:09:55.160 align:start position:0%
of weights in our model but also allows
 

00:09:55.160 --> 00:09:57.460 align:start position:0%
of weights in our model but also allows
us<00:09:55.430><c> to</c><00:09:55.610><c> leverage</c><00:09:55.730><c> the</c><00:09:56.090><c> fact</c><00:09:56.270><c> that</c><00:09:56.570><c> in</c><00:09:56.960><c> an</c><00:09:57.140><c> image</c>

00:09:57.460 --> 00:09:57.470 align:start position:0%
us to leverage the fact that in an image
 

00:09:57.470 --> 00:09:59.620 align:start position:0%
us to leverage the fact that in an image
pixels<00:09:58.220><c> that</c><00:09:58.280><c> are</c><00:09:58.460><c> spatially</c><00:09:59.089><c> close</c><00:09:59.360><c> to</c><00:09:59.510><c> each</c>

00:09:59.620 --> 00:09:59.630 align:start position:0%
pixels that are spatially close to each
 

00:09:59.630 --> 00:10:02.380 align:start position:0%
pixels that are spatially close to each
other<00:09:59.839><c> are</c><00:10:00.160><c> probably</c><00:10:01.160><c> somehow</c><00:10:01.640><c> related</c><00:10:02.030><c> and</c>

00:10:02.380 --> 00:10:02.390 align:start position:0%
other are probably somehow related and
 

00:10:02.390 --> 00:10:05.710 align:start position:0%
other are probably somehow related and
so<00:10:03.290><c> I'd</c><00:10:03.830><c> like</c><00:10:03.860><c> you</c><00:10:04.339><c> to</c><00:10:04.520><c> really</c><00:10:04.790><c> notice</c><00:10:05.000><c> how</c><00:10:05.660><c> the</c>

00:10:05.710 --> 00:10:05.720 align:start position:0%
so I'd like you to really notice how the
 

00:10:05.720 --> 00:10:10.630 align:start position:0%
so I'd like you to really notice how the
only<00:10:06.440><c> region</c><00:10:06.800><c> how</c><00:10:08.410><c> only</c><00:10:09.410><c> a</c><00:10:09.589><c> region</c><00:10:10.070><c> of</c><00:10:10.310><c> the</c>

00:10:10.630 --> 00:10:10.640 align:start position:0%
only region how only a region of the
 

00:10:10.640 --> 00:10:13.150 align:start position:0%
only region how only a region of the
input<00:10:10.970><c> layer</c><00:10:11.320><c> influences</c><00:10:12.320><c> this</c><00:10:12.560><c> particular</c>

00:10:13.150 --> 00:10:13.160 align:start position:0%
input layer influences this particular
 

00:10:13.160 --> 00:10:16.720 align:start position:0%
input layer influences this particular
neuron<00:10:13.550><c> and</c><00:10:14.560><c> we</c><00:10:15.560><c> can</c><00:10:15.710><c> define</c><00:10:16.040><c> connections</c>

00:10:16.720 --> 00:10:16.730 align:start position:0%
neuron and we can define connections
 

00:10:16.730 --> 00:10:19.750 align:start position:0%
neuron and we can define connections
across<00:10:17.300><c> the</c><00:10:17.540><c> whole</c><00:10:17.750><c> input</c><00:10:18.050><c> by</c><00:10:18.860><c> applying</c><00:10:19.339><c> the</c>

00:10:19.750 --> 00:10:19.760 align:start position:0%
across the whole input by applying the
 

00:10:19.760 --> 00:10:21.820 align:start position:0%
across the whole input by applying the
same<00:10:20.030><c> principle</c><00:10:20.540><c> of</c><00:10:20.630><c> connecting</c><00:10:21.140><c> patches</c><00:10:21.650><c> in</c>

00:10:21.820 --> 00:10:21.830 align:start position:0%
same principle of connecting patches in
 

00:10:21.830 --> 00:10:23.800 align:start position:0%
same principle of connecting patches in
the<00:10:21.860><c> input</c><00:10:22.310><c> layer</c><00:10:22.460><c> to</c><00:10:23.180><c> neurons</c><00:10:23.480><c> in</c><00:10:23.690><c> the</c>

00:10:23.800 --> 00:10:23.810 align:start position:0%
the input layer to neurons in the
 

00:10:23.810 --> 00:10:27.010 align:start position:0%
the input layer to neurons in the
subsequent<00:10:24.650><c> subsequent</c><00:10:25.490><c> layer</c><00:10:25.850><c> and</c><00:10:26.240><c> we</c><00:10:26.810><c> do</c>

00:10:27.010 --> 00:10:27.020 align:start position:0%
subsequent subsequent layer and we do
 

00:10:27.020 --> 00:10:28.900 align:start position:0%
subsequent subsequent layer and we do
this<00:10:27.200><c> by</c><00:10:27.410><c> actually</c><00:10:27.620><c> sliding</c><00:10:28.550><c> the</c><00:10:28.700><c> patch</c>

00:10:28.900 --> 00:10:28.910 align:start position:0%
this by actually sliding the patch
 

00:10:28.910 --> 00:10:32.170 align:start position:0%
this by actually sliding the patch
window<00:10:29.750><c> across</c><00:10:30.110><c> the</c><00:10:30.260><c> input</c><00:10:30.830><c> image</c><00:10:31.160><c> in</c><00:10:31.400><c> this</c>

00:10:32.170 --> 00:10:32.180 align:start position:0%
window across the input image in this
 

00:10:32.180 --> 00:10:34.560 align:start position:0%
window across the input image in this
case<00:10:32.420><c> we're</c><00:10:32.839><c> sliding</c><00:10:33.110><c> it</c><00:10:33.410><c> by</c><00:10:33.530><c> two</c><00:10:33.589><c> units</c><00:10:34.160><c> and</c>

00:10:34.560 --> 00:10:34.570 align:start position:0%
case we're sliding it by two units and
 

00:10:34.570 --> 00:10:37.570 align:start position:0%
case we're sliding it by two units and
in<00:10:35.570><c> doing</c><00:10:36.050><c> this</c><00:10:36.140><c> we</c><00:10:36.290><c> take</c><00:10:36.740><c> into</c><00:10:37.010><c> account</c><00:10:37.130><c> the</c>

00:10:37.570 --> 00:10:37.580 align:start position:0%
in doing this we take into account the
 

00:10:37.580 --> 00:10:39.460 align:start position:0%
in doing this we take into account the
spatial<00:10:37.880><c> structure</c><00:10:38.360><c> that's</c><00:10:38.690><c> inherent</c><00:10:39.230><c> to</c><00:10:39.350><c> the</c>

00:10:39.460 --> 00:10:39.470 align:start position:0%
spatial structure that's inherent to the
 

00:10:39.470 --> 00:10:42.340 align:start position:0%
spatial structure that's inherent to the
input<00:10:40.330><c> but</c><00:10:41.330><c> remember</c><00:10:41.780><c> right</c><00:10:41.960><c> that</c><00:10:41.990><c> our</c>

00:10:42.340 --> 00:10:42.350 align:start position:0%
input but remember right that our
 

00:10:42.350 --> 00:10:45.190 align:start position:0%
input but remember right that our
ultimate<00:10:42.860><c> task</c><00:10:43.130><c> is</c><00:10:43.490><c> to</c><00:10:43.850><c> learn</c><00:10:44.360><c> visual</c>

00:10:45.190 --> 00:10:45.200 align:start position:0%
ultimate task is to learn visual
 

00:10:45.200 --> 00:10:48.520 align:start position:0%
ultimate task is to learn visual
features<00:10:45.740><c> and</c><00:10:46.210><c> the</c><00:10:47.210><c> way</c><00:10:47.360><c> we</c><00:10:47.690><c> achieve</c><00:10:48.080><c> this</c><00:10:48.320><c> is</c>

00:10:48.520 --> 00:10:48.530 align:start position:0%
features and the way we achieve this is
 

00:10:48.530 --> 00:10:50.560 align:start position:0%
features and the way we achieve this is
by<00:10:48.740><c> weighting</c><00:10:49.400><c> these</c><00:10:49.550><c> connections</c><00:10:50.210><c> between</c>

00:10:50.560 --> 00:10:50.570 align:start position:0%
by weighting these connections between
 

00:10:50.570 --> 00:10:54.130 align:start position:0%
by weighting these connections between
the<00:10:50.900><c> patch</c><00:10:51.110><c> and</c><00:10:51.470><c> the</c><00:10:51.890><c> neuron</c><00:10:52.190><c> and</c><00:10:52.839><c> the</c><00:10:53.839><c> neuron</c>

00:10:54.130 --> 00:10:54.140 align:start position:0%
the patch and the neuron and the neuron
 

00:10:54.140 --> 00:10:56.410 align:start position:0%
the patch and the neuron and the neuron
in<00:10:54.320><c> the</c><00:10:54.410><c> next</c><00:10:54.770><c> layer</c><00:10:55.339><c> so</c><00:10:55.730><c> as</c><00:10:55.820><c> to</c><00:10:56.000><c> detect</c>

00:10:56.410 --> 00:10:56.420 align:start position:0%
in the next layer so as to detect
 

00:10:56.420 --> 00:11:02.829 align:start position:0%
in the next layer so as to detect
particular<00:10:57.020><c> features</c><00:11:01.000><c> so</c><00:11:02.000><c> this</c><00:11:02.450><c> this</c>

00:11:02.829 --> 00:11:02.839 align:start position:0%
particular features so this this
 

00:11:02.839 --> 00:11:05.770 align:start position:0%
particular features so this this
principle<00:11:03.440><c> is</c><00:11:03.650><c> is</c><00:11:03.950><c> called</c>

00:11:05.770 --> 00:11:05.780 align:start position:0%
principle is is called
 

00:11:05.780 --> 00:11:08.110 align:start position:0%
principle is is called
we<00:11:05.990><c> think</c><00:11:06.530><c> of</c><00:11:06.770><c> what</c><00:11:07.220><c> you</c><00:11:07.370><c> can</c><00:11:07.520><c> think</c><00:11:07.700><c> of</c><00:11:07.850><c> it</c><00:11:07.970><c> as</c>

00:11:08.110 --> 00:11:08.120 align:start position:0%
we think of what you can think of it as
 

00:11:08.120 --> 00:11:11.530 align:start position:0%
we think of what you can think of it as
is<00:11:08.450><c> applying</c><00:11:09.230><c> a</c><00:11:09.590><c> filter</c><00:11:10.250><c> essentially</c><00:11:11.210><c> a</c><00:11:11.240><c> set</c>

00:11:11.530 --> 00:11:11.540 align:start position:0%
is applying a filter essentially a set
 

00:11:11.540 --> 00:11:14.680 align:start position:0%
is applying a filter essentially a set
of<00:11:11.660><c> weights</c><00:11:11.870><c> to</c><00:11:12.440><c> extract</c><00:11:12.920><c> some</c><00:11:13.490><c> sort</c><00:11:13.820><c> of</c><00:11:13.940><c> local</c>

00:11:14.680 --> 00:11:14.690 align:start position:0%
of weights to extract some sort of local
 

00:11:14.690 --> 00:11:16.960 align:start position:0%
of weights to extract some sort of local
features<00:11:14.840><c> that</c><00:11:15.260><c> are</c><00:11:15.350><c> present</c><00:11:15.800><c> in</c><00:11:15.950><c> your</c><00:11:16.490><c> input</c>

00:11:16.960 --> 00:11:16.970 align:start position:0%
features that are present in your input
 

00:11:16.970 --> 00:11:19.840 align:start position:0%
features that are present in your input
image<00:11:17.410><c> and</c><00:11:18.410><c> we</c><00:11:18.800><c> can</c><00:11:18.950><c> apply</c><00:11:19.100><c> multiple</c>

00:11:19.840 --> 00:11:19.850 align:start position:0%
image and we can apply multiple
 

00:11:19.850 --> 00:11:23.020 align:start position:0%
image and we can apply multiple
different<00:11:20.260><c> filters</c><00:11:21.260><c> to</c><00:11:21.800><c> extract</c><00:11:22.310><c> different</c>

00:11:23.020 --> 00:11:23.030 align:start position:0%
different filters to extract different
 

00:11:23.030 --> 00:11:26.200 align:start position:0%
different filters to extract different
types<00:11:23.300><c> of</c><00:11:23.510><c> features</c><00:11:23.780><c> and</c><00:11:24.550><c> furthermore</c><00:11:25.550><c> we</c><00:11:26.180><c> can</c>

00:11:26.200 --> 00:11:26.210 align:start position:0%
types of features and furthermore we can
 

00:11:26.210 --> 00:11:28.450 align:start position:0%
types of features and furthermore we can
spatially<00:11:26.930><c> share</c><00:11:27.230><c> the</c><00:11:27.500><c> parameters</c><00:11:28.040><c> of</c><00:11:28.250><c> each</c>

00:11:28.450 --> 00:11:28.460 align:start position:0%
spatially share the parameters of each
 

00:11:28.460 --> 00:11:31.210 align:start position:0%
spatially share the parameters of each
of<00:11:28.670><c> these</c><00:11:28.790><c> filters</c><00:11:29.060><c> across</c><00:11:29.960><c> the</c><00:11:30.170><c> input</c><00:11:30.560><c> so</c>

00:11:31.210 --> 00:11:31.220 align:start position:0%
of these filters across the input so
 

00:11:31.220 --> 00:11:33.580 align:start position:0%
of these filters across the input so
that<00:11:31.640><c> features</c><00:11:32.360><c> that</c><00:11:32.540><c> matter</c><00:11:32.780><c> in</c><00:11:32.990><c> one</c><00:11:33.230><c> part</c><00:11:33.500><c> of</c>

00:11:33.580 --> 00:11:33.590 align:start position:0%
that features that matter in one part of
 

00:11:33.590 --> 00:11:36.130 align:start position:0%
that features that matter in one part of
the<00:11:33.650><c> image</c><00:11:34.070><c> will</c><00:11:34.640><c> still</c><00:11:34.940><c> matter</c><00:11:35.420><c> elsewhere</c><00:11:35.990><c> in</c>

00:11:36.130 --> 00:11:36.140 align:start position:0%
the image will still matter elsewhere in
 

00:11:36.140 --> 00:11:40.570 align:start position:0%
the image will still matter elsewhere in
the<00:11:36.290><c> image</c><00:11:36.530><c> in</c><00:11:38.140><c> practice</c><00:11:39.140><c> this</c><00:11:39.560><c> amounts</c><00:11:40.400><c> to</c>

00:11:40.570 --> 00:11:40.580 align:start position:0%
the image in practice this amounts to
 

00:11:40.580 --> 00:11:42.490 align:start position:0%
the image in practice this amounts to
this<00:11:40.760><c> patchy</c><00:11:41.120><c> operation</c><00:11:41.930><c> that's</c><00:11:42.140><c> called</c>

00:11:42.490 --> 00:11:42.500 align:start position:0%
this patchy operation that's called
 

00:11:42.500 --> 00:11:45.460 align:start position:0%
this patchy operation that's called
convolution<00:11:43.130><c> and</c><00:11:43.670><c> if</c><00:11:44.660><c> we</c><00:11:44.840><c> first</c><00:11:45.020><c> think</c><00:11:45.200><c> about</c>

00:11:45.460 --> 00:11:45.470 align:start position:0%
convolution and if we first think about
 

00:11:45.470 --> 00:11:48.250 align:start position:0%
convolution and if we first think about
this<00:11:45.680><c> at</c><00:11:45.890><c> a</c><00:11:45.920><c> high</c><00:11:46.190><c> level</c><00:11:46.630><c> suppose</c><00:11:47.630><c> we</c><00:11:47.810><c> have</c><00:11:48.050><c> a</c>

00:11:48.250 --> 00:11:48.260 align:start position:0%
this at a high level suppose we have a
 

00:11:48.260 --> 00:11:51.040 align:start position:0%
this at a high level suppose we have a
four<00:11:48.500><c> by</c><00:11:48.650><c> four</c><00:11:48.680><c> filter</c><00:11:49.360><c> which</c><00:11:50.360><c> means</c><00:11:50.660><c> we</c><00:11:50.870><c> have</c>

00:11:51.040 --> 00:11:51.050 align:start position:0%
four by four filter which means we have
 

00:11:51.050 --> 00:11:54.220 align:start position:0%
four by four filter which means we have
16<00:11:51.890><c> different</c><00:11:52.070><c> weights</c><00:11:52.460><c> right</c><00:11:52.990><c> and</c><00:11:53.990><c> we're</c>

00:11:54.220 --> 00:11:54.230 align:start position:0%
16 different weights right and we're
 

00:11:54.230 --> 00:11:56.830 align:start position:0%
16 different weights right and we're
going<00:11:54.410><c> to</c><00:11:54.470><c> apply</c><00:11:54.830><c> this</c><00:11:54.890><c> same</c><00:11:55.880><c> filter</c><00:11:56.300><c> to</c><00:11:56.570><c> four</c>

00:11:56.830 --> 00:11:56.840 align:start position:0%
going to apply this same filter to four
 

00:11:56.840 --> 00:12:00.310 align:start position:0%
going to apply this same filter to four
by<00:11:57.020><c> four</c><00:11:57.290><c> patches</c><00:11:57.620><c> in</c><00:11:58.100><c> the</c><00:11:58.160><c> input</c><00:11:59.030><c> and</c><00:11:59.270><c> use</c><00:12:00.110><c> the</c>

00:12:00.310 --> 00:12:00.320 align:start position:0%
by four patches in the input and use the
 

00:12:00.320 --> 00:12:03.040 align:start position:0%
by four patches in the input and use the
result<00:12:00.710><c> of</c><00:12:00.950><c> that</c><00:12:01.190><c> filter</c><00:12:01.850><c> operation</c><00:12:02.330><c> to</c>

00:12:03.040 --> 00:12:03.050 align:start position:0%
result of that filter operation to
 

00:12:03.050 --> 00:12:05.620 align:start position:0%
result of that filter operation to
define<00:12:03.470><c> the</c><00:12:03.800><c> state</c><00:12:04.070><c> of</c><00:12:04.280><c> the</c><00:12:04.400><c> neuron</c><00:12:04.670><c> that</c><00:12:05.480><c> the</c>

00:12:05.620 --> 00:12:05.630 align:start position:0%
define the state of the neuron that the
 

00:12:05.630 --> 00:12:08.800 align:start position:0%
define the state of the neuron that the
patch<00:12:05.840><c> is</c><00:12:06.170><c> connected</c><00:12:06.680><c> to</c><00:12:06.710><c> right</c><00:12:07.280><c> then</c><00:12:08.210><c> we're</c>

00:12:08.800 --> 00:12:08.810 align:start position:0%
patch is connected to right then we're
 

00:12:08.810 --> 00:12:10.960 align:start position:0%
patch is connected to right then we're
going<00:12:09.020><c> to</c><00:12:09.080><c> shift</c><00:12:09.410><c> our</c><00:12:09.590><c> filter</c><00:12:09.950><c> over</c><00:12:10.370><c> by</c><00:12:10.610><c> a</c>

00:12:10.960 --> 00:12:10.970 align:start position:0%
going to shift our filter over by a
 

00:12:10.970 --> 00:12:13.900 align:start position:0%
going to shift our filter over by a
certain<00:12:11.660><c> width</c><00:12:11.839><c> like</c><00:12:12.290><c> two</c><00:12:12.530><c> pixels</c><00:12:12.950><c> grab</c><00:12:13.700><c> the</c>

00:12:13.900 --> 00:12:13.910 align:start position:0%
certain width like two pixels grab the
 

00:12:13.910 --> 00:12:16.420 align:start position:0%
certain width like two pixels grab the
next<00:12:14.120><c> patch</c><00:12:14.390><c> and</c><00:12:14.720><c> apply</c><00:12:15.350><c> that</c><00:12:15.430><c> filtering</c>

00:12:16.420 --> 00:12:16.430 align:start position:0%
next patch and apply that filtering
 

00:12:16.430 --> 00:12:19.060 align:start position:0%
next patch and apply that filtering
operation<00:12:17.030><c> again</c><00:12:17.330><c> and</c><00:12:17.600><c> this</c><00:12:18.260><c> is</c><00:12:18.320><c> how</c><00:12:18.440><c> we</c><00:12:18.650><c> can</c>

00:12:19.060 --> 00:12:19.070 align:start position:0%
operation again and this is how we can
 

00:12:19.070 --> 00:12:21.010 align:start position:0%
operation again and this is how we can
start<00:12:19.310><c> to</c><00:12:19.430><c> think</c><00:12:19.460><c> about</c><00:12:19.760><c> convolution</c><00:12:20.600><c> at</c><00:12:20.780><c> a</c>

00:12:21.010 --> 00:12:21.020 align:start position:0%
start to think about convolution at a
 

00:12:21.020 --> 00:12:23.410 align:start position:0%
start to think about convolution at a
really<00:12:21.350><c> high</c><00:12:21.380><c> level</c><00:12:21.790><c> but</c><00:12:22.790><c> you're</c><00:12:23.150><c> probably</c>

00:12:23.410 --> 00:12:23.420 align:start position:0%
really high level but you're probably
 

00:12:23.420 --> 00:12:25.210 align:start position:0%
really high level but you're probably
wondering<00:12:24.020><c> how</c><00:12:24.440><c> does</c><00:12:24.500><c> this</c><00:12:24.770><c> actually</c><00:12:24.980><c> work</c>

00:12:25.210 --> 00:12:25.220 align:start position:0%
wondering how does this actually work
 

00:12:25.220 --> 00:12:27.490 align:start position:0%
wondering how does this actually work
what<00:12:26.000><c> am</c><00:12:26.089><c> I</c><00:12:26.210><c> talking</c><00:12:26.270><c> about</c><00:12:26.660><c> when</c><00:12:26.990><c> I</c><00:12:27.020><c> keep</c>

00:12:27.490 --> 00:12:27.500 align:start position:0%
what am I talking about when I keep
 

00:12:27.500 --> 00:12:29.829 align:start position:0%
what am I talking about when I keep
saying<00:12:27.770><c> oh</c><00:12:27.950><c> features</c><00:12:28.880><c> extract</c><00:12:29.360><c> visual</c>

00:12:29.829 --> 00:12:29.839 align:start position:0%
saying oh features extract visual
 

00:12:29.839 --> 00:12:31.900 align:start position:0%
saying oh features extract visual
features<00:12:30.260><c> how</c><00:12:30.890><c> does</c><00:12:31.100><c> this</c><00:12:31.250><c> convolution</c>

00:12:31.900 --> 00:12:31.910 align:start position:0%
features how does this convolution
 

00:12:31.910 --> 00:12:34.660 align:start position:0%
features how does this convolution
operation<00:12:32.140><c> allow</c><00:12:33.140><c> us</c><00:12:33.350><c> to</c><00:12:33.500><c> do</c><00:12:33.650><c> this</c><00:12:33.800><c> so</c><00:12:34.040><c> let's</c>

00:12:34.660 --> 00:12:34.670 align:start position:0%
operation allow us to do this so let's
 

00:12:34.670 --> 00:12:36.790 align:start position:0%
operation allow us to do this so let's
make<00:12:35.030><c> this</c><00:12:35.180><c> concrete</c><00:12:35.720><c> by</c><00:12:36.080><c> walking</c><00:12:36.530><c> through</c><00:12:36.770><c> a</c>

00:12:36.790 --> 00:12:36.800 align:start position:0%
make this concrete by walking through a
 

00:12:36.800 --> 00:12:40.750 align:start position:0%
make this concrete by walking through a
couple<00:12:37.220><c> of</c><00:12:37.580><c> examples</c><00:12:39.130><c> suppose</c><00:12:40.130><c> we</c><00:12:40.460><c> want</c><00:12:40.670><c> to</c>

00:12:40.750 --> 00:12:40.760 align:start position:0%
couple of examples suppose we want to
 

00:12:40.760 --> 00:12:43.750 align:start position:0%
couple of examples suppose we want to
classify<00:12:41.140><c> X's</c><00:12:42.140><c> from</c><00:12:42.740><c> a</c><00:12:42.830><c> set</c><00:12:43.130><c> of</c><00:12:43.250><c> black</c><00:12:43.520><c> and</c>

00:12:43.750 --> 00:12:43.760 align:start position:0%
classify X's from a set of black and
 

00:12:43.760 --> 00:12:46.210 align:start position:0%
classify X's from a set of black and
white<00:12:43.820><c> images</c><00:12:44.360><c> of</c><00:12:44.510><c> letters</c><00:12:44.839><c> where</c><00:12:45.710><c> black</c><00:12:45.980><c> is</c>

00:12:46.210 --> 00:12:46.220 align:start position:0%
white images of letters where black is
 

00:12:46.220 --> 00:12:48.130 align:start position:0%
white images of letters where black is
equal<00:12:46.520><c> to</c><00:12:46.640><c> minus</c><00:12:46.940><c> 1</c><00:12:47.180><c> and</c><00:12:47.360><c> white</c><00:12:47.930><c> is</c>

00:12:48.130 --> 00:12:48.140 align:start position:0%
equal to minus 1 and white is
 

00:12:48.140 --> 00:12:51.430 align:start position:0%
equal to minus 1 and white is
represented<00:12:48.800><c> by</c><00:12:48.830><c> a</c><00:12:48.950><c> value</c><00:12:49.400><c> of</c><00:12:49.580><c> 1</c><00:12:49.820><c> to</c><00:12:50.440><c> classify</c>

00:12:51.430 --> 00:12:51.440 align:start position:0%
represented by a value of 1 to classify
 

00:12:51.440 --> 00:12:53.800 align:start position:0%
represented by a value of 1 to classify
it's<00:12:51.740><c> it's</c><00:12:52.130><c> really</c><00:12:52.520><c> not</c><00:12:52.730><c> possible</c><00:12:52.790><c> to</c><00:12:53.480><c> simply</c>

00:12:53.800 --> 00:12:53.810 align:start position:0%
it's it's really not possible to simply
 

00:12:53.810 --> 00:12:56.050 align:start position:0%
it's it's really not possible to simply
compare<00:12:54.260><c> the</c><00:12:54.470><c> two</c><00:12:54.620><c> matrices</c><00:12:55.040><c> to</c><00:12:55.700><c> see</c><00:12:55.880><c> if</c>

00:12:56.050 --> 00:12:56.060 align:start position:0%
compare the two matrices to see if
 

00:12:56.060 --> 00:12:58.180 align:start position:0%
compare the two matrices to see if
they're<00:12:56.240><c> equal</c><00:12:56.420><c> because</c><00:12:57.320><c> we</c><00:12:57.440><c> want</c><00:12:57.800><c> to</c><00:12:57.920><c> be</c><00:12:58.040><c> able</c>

00:12:58.180 --> 00:12:58.190 align:start position:0%
they're equal because we want to be able
 

00:12:58.190 --> 00:13:01.920 align:start position:0%
they're equal because we want to be able
to<00:12:58.400><c> classify</c><00:12:59.270><c> and</c><00:12:59.570><c> act</c><00:12:59.870><c> as</c><00:13:00.260><c> an</c><00:13:00.860><c> X</c><00:13:01.100><c> even</c><00:13:01.550><c> if</c><00:13:01.700><c> it's</c>

00:13:01.920 --> 00:13:01.930 align:start position:0%
to classify and act as an X even if it's
 

00:13:01.930 --> 00:13:05.410 align:start position:0%
to classify and act as an X even if it's
transformed<00:13:02.930><c> rotated</c><00:13:03.890><c> reflected</c><00:13:04.730><c> deformed</c>

00:13:05.410 --> 00:13:05.420 align:start position:0%
transformed rotated reflected deformed
 

00:13:05.420 --> 00:13:10.060 align:start position:0%
transformed rotated reflected deformed
etc<00:13:07.210><c> instead</c><00:13:08.210><c> we</c><00:13:08.510><c> want</c><00:13:08.750><c> our</c><00:13:08.930><c> model</c><00:13:09.380><c> to</c><00:13:09.589><c> compare</c>

00:13:10.060 --> 00:13:10.070 align:start position:0%
etc instead we want our model to compare
 

00:13:10.070 --> 00:13:13.260 align:start position:0%
etc instead we want our model to compare
the<00:13:10.280><c> images</c><00:13:10.730><c> of</c><00:13:10.940><c> an</c><00:13:11.360><c> X</c><00:13:11.630><c> piece</c><00:13:12.350><c> by</c><00:13:12.650><c> piece</c><00:13:12.680><c> and</c>

00:13:13.260 --> 00:13:13.270 align:start position:0%
the images of an X piece by piece and
 

00:13:13.270 --> 00:13:16.329 align:start position:0%
the images of an X piece by piece and
those<00:13:14.270><c> important</c><00:13:14.870><c> pieces</c><00:13:15.110><c> that</c><00:13:15.320><c> it</c><00:13:15.680><c> learns</c><00:13:16.040><c> to</c>

00:13:16.329 --> 00:13:16.339 align:start position:0%
those important pieces that it learns to
 

00:13:16.339 --> 00:13:18.870 align:start position:0%
those important pieces that it learns to
look<00:13:16.520><c> for</c><00:13:16.760><c> are</c><00:13:17.000><c> the</c><00:13:17.330><c> features</c><00:13:17.690><c> and</c>

00:13:18.870 --> 00:13:18.880 align:start position:0%
look for are the features and
 

00:13:18.880 --> 00:13:21.810 align:start position:0%
look for are the features and
our<00:13:19.000><c> model</c><00:13:19.420><c> can</c><00:13:19.630><c> get</c><00:13:19.900><c> rough</c><00:13:20.830><c> feature</c><00:13:21.220><c> matches</c>

00:13:21.810 --> 00:13:21.820 align:start position:0%
our model can get rough feature matches
 

00:13:21.820 --> 00:13:24.450 align:start position:0%
our model can get rough feature matches
in<00:13:22.030><c> roughly</c><00:13:22.480><c> the</c><00:13:22.570><c> same</c><00:13:22.870><c> positions</c><00:13:23.460><c> relatively</c>

00:13:24.450 --> 00:13:24.460 align:start position:0%
in roughly the same positions relatively
 

00:13:24.460 --> 00:13:26.850 align:start position:0%
in roughly the same positions relatively
speaking<00:13:24.700><c> in</c><00:13:25.060><c> two</c><00:13:25.510><c> different</c><00:13:25.720><c> images</c><00:13:26.320><c> it</c><00:13:26.560><c> can</c>

00:13:26.850 --> 00:13:26.860 align:start position:0%
speaking in two different images it can
 

00:13:26.860 --> 00:13:30.150 align:start position:0%
speaking in two different images it can
get<00:13:27.160><c> a</c><00:13:27.220><c> lot</c><00:13:27.490><c> better</c><00:13:27.970><c> sense</c><00:13:28.390><c> at</c><00:13:28.630><c> seeing</c><00:13:29.560><c> the</c>

00:13:30.150 --> 00:13:30.160 align:start position:0%
get a lot better sense at seeing the
 

00:13:30.160 --> 00:13:32.370 align:start position:0%
get a lot better sense at seeing the
similarity<00:13:30.910><c> between</c><00:13:30.940><c> different</c><00:13:31.720><c> examples</c><00:13:32.260><c> of</c>

00:13:32.370 --> 00:13:32.380 align:start position:0%
similarity between different examples of
 

00:13:32.380 --> 00:13:36.660 align:start position:0%
similarity between different examples of
exes<00:13:33.870><c> so</c><00:13:34.870><c> each</c><00:13:35.110><c> feature</c><00:13:35.470><c> is</c><00:13:35.800><c> like</c><00:13:36.130><c> a</c><00:13:36.340><c> mini</c>

00:13:36.660 --> 00:13:36.670 align:start position:0%
exes so each feature is like a mini
 

00:13:36.670 --> 00:13:37.680 align:start position:0%
exes so each feature is like a mini
image<00:13:37.030><c> right</c>

00:13:37.680 --> 00:13:37.690 align:start position:0%
image right
 

00:13:37.690 --> 00:13:40.320 align:start position:0%
image right
a<00:13:37.720><c> small</c><00:13:38.520><c> two-dimensional</c><00:13:39.520><c> array</c><00:13:39.820><c> of</c><00:13:40.060><c> values</c>

00:13:40.320 --> 00:13:40.330 align:start position:0%
a small two-dimensional array of values
 

00:13:40.330 --> 00:13:43.620 align:start position:0%
a small two-dimensional array of values
and<00:13:40.840><c> we</c><00:13:41.440><c> can</c><00:13:41.470><c> use</c><00:13:41.890><c> these</c><00:13:42.370><c> filters</c><00:13:43.000><c> to</c><00:13:43.270><c> pick</c><00:13:43.480><c> up</c>

00:13:43.620 --> 00:13:43.630 align:start position:0%
and we can use these filters to pick up
 

00:13:43.630 --> 00:13:46.260 align:start position:0%
and we can use these filters to pick up
on<00:13:43.840><c> the</c><00:13:44.560><c> features</c><00:13:44.950><c> that</c><00:13:45.160><c> are</c><00:13:45.310><c> common</c><00:13:45.520><c> to</c><00:13:45.760><c> exes</c>

00:13:46.260 --> 00:13:46.270 align:start position:0%
on the features that are common to exes
 

00:13:46.270 --> 00:13:49.320 align:start position:0%
on the features that are common to exes
so<00:13:47.080><c> in</c><00:13:47.200><c> the</c><00:13:47.290><c> case</c><00:13:47.470><c> of</c><00:13:47.650><c> exes</c><00:13:48.100><c> right</c><00:13:48.340><c> filters</c>

00:13:49.320 --> 00:13:49.330 align:start position:0%
so in the case of exes right filters
 

00:13:49.330 --> 00:13:52.110 align:start position:0%
so in the case of exes right filters
that<00:13:49.540><c> can</c><00:13:50.050><c> pick</c><00:13:50.230><c> up</c><00:13:50.380><c> on</c><00:13:50.560><c> diagonal</c><00:13:51.100><c> lines</c><00:13:51.370><c> and</c><00:13:51.730><c> a</c>

00:13:52.110 --> 00:13:52.120 align:start position:0%
that can pick up on diagonal lines and a
 

00:13:52.120 --> 00:13:54.690 align:start position:0%
that can pick up on diagonal lines and a
crossing<00:13:52.590><c> capture</c><00:13:53.590><c> what's</c><00:13:53.920><c> important</c><00:13:54.520><c> about</c>

00:13:54.690 --> 00:13:54.700 align:start position:0%
crossing capture what's important about
 

00:13:54.700 --> 00:13:58.830 align:start position:0%
crossing capture what's important about
an<00:13:55.300><c> X</c><00:13:56.340><c> so</c><00:13:57.340><c> we</c><00:13:57.430><c> can</c><00:13:57.580><c> probably</c><00:13:57.820><c> capture</c><00:13:58.660><c> these</c>

00:13:58.830 --> 00:13:58.840 align:start position:0%
an X so we can probably capture these
 

00:13:58.840 --> 00:14:01.410 align:start position:0%
an X so we can probably capture these
features<00:13:59.290><c> in</c><00:13:59.590><c> the</c><00:14:00.010><c> arms</c><00:14:00.340><c> and</c><00:14:00.580><c> center</c><00:14:00.970><c> of</c><00:14:01.150><c> any</c>

00:14:01.410 --> 00:14:01.420 align:start position:0%
features in the arms and center of any
 

00:14:01.420 --> 00:14:04.800 align:start position:0%
features in the arms and center of any
image<00:14:01.690><c> of</c><00:14:01.990><c> an</c><00:14:02.170><c> X</c><00:14:02.820><c> and</c><00:14:03.820><c> I'd</c><00:14:04.000><c> like</c><00:14:04.180><c> you</c><00:14:04.330><c> to</c><00:14:04.480><c> notice</c>

00:14:04.800 --> 00:14:04.810 align:start position:0%
image of an X and I'd like you to notice
 

00:14:04.810 --> 00:14:07.590 align:start position:0%
image of an X and I'd like you to notice
that<00:14:04.990><c> these</c><00:14:05.200><c> smaller</c><00:14:05.650><c> matrices</c><00:14:06.390><c> are</c><00:14:07.390><c> the</c>

00:14:07.590 --> 00:14:07.600 align:start position:0%
that these smaller matrices are the
 

00:14:07.600 --> 00:14:09.660 align:start position:0%
that these smaller matrices are the
filters<00:14:07.840><c> of</c><00:14:08.200><c> weights</c><00:14:08.470><c> that</c><00:14:08.800><c> we'll</c><00:14:09.490><c> actually</c>

00:14:09.660 --> 00:14:09.670 align:start position:0%
filters of weights that we'll actually
 

00:14:09.670 --> 00:14:12.330 align:start position:0%
filters of weights that we'll actually
use<00:14:09.910><c> to</c><00:14:10.510><c> detect</c><00:14:10.870><c> the</c><00:14:11.200><c> corresponding</c><00:14:11.980><c> features</c>

00:14:12.330 --> 00:14:12.340 align:start position:0%
use to detect the corresponding features
 

00:14:12.340 --> 00:14:15.120 align:start position:0%
use to detect the corresponding features
in<00:14:12.580><c> the</c><00:14:12.700><c> input</c><00:14:13.030><c> image</c><00:14:13.360><c> now</c><00:14:14.320><c> all</c><00:14:14.680><c> that's</c><00:14:14.950><c> left</c>

00:14:15.120 --> 00:14:15.130 align:start position:0%
in the input image now all that's left
 

00:14:15.130 --> 00:14:18.240 align:start position:0%
in the input image now all that's left
is<00:14:15.670><c> to</c><00:14:15.700><c> define</c><00:14:16.210><c> an</c><00:14:16.390><c> operation</c><00:14:16.900><c> that</c><00:14:17.770><c> picks</c><00:14:18.070><c> up</c>

00:14:18.240 --> 00:14:18.250 align:start position:0%
is to define an operation that picks up
 

00:14:18.250 --> 00:14:20.880 align:start position:0%
is to define an operation that picks up
where<00:14:18.520><c> these</c><00:14:18.760><c> features</c><00:14:19.240><c> pop</c><00:14:19.870><c> up</c><00:14:20.080><c> in</c><00:14:20.320><c> our</c><00:14:20.560><c> image</c>

00:14:20.880 --> 00:14:20.890 align:start position:0%
where these features pop up in our image
 

00:14:20.890 --> 00:14:25.040 align:start position:0%
where these features pop up in our image
and<00:14:21.750><c> that</c><00:14:22.750><c> operation</c><00:14:23.200><c> is</c><00:14:23.560><c> convolution</c><00:14:24.340><c> and</c>

00:14:25.040 --> 00:14:25.050 align:start position:0%
and that operation is convolution and
 

00:14:25.050 --> 00:14:27.900 align:start position:0%
and that operation is convolution and
convolution<00:14:26.050><c> is</c><00:14:26.290><c> able</c><00:14:26.680><c> to</c><00:14:26.800><c> preserve</c><00:14:27.040><c> the</c>

00:14:27.900 --> 00:14:27.910 align:start position:0%
convolution is able to preserve the
 

00:14:27.910 --> 00:14:30.630 align:start position:0%
convolution is able to preserve the
spatial<00:14:28.390><c> relationship</c><00:14:29.020><c> between</c><00:14:29.200><c> pixels</c><00:14:29.680><c> by</c>

00:14:30.630 --> 00:14:30.640 align:start position:0%
spatial relationship between pixels by
 

00:14:30.640 --> 00:14:33.030 align:start position:0%
spatial relationship between pixels by
learning<00:14:30.940><c> image</c><00:14:31.450><c> features</c><00:14:31.810><c> in</c><00:14:32.020><c> small</c><00:14:32.440><c> squares</c>

00:14:33.030 --> 00:14:33.040 align:start position:0%
learning image features in small squares
 

00:14:33.040 --> 00:14:36.600 align:start position:0%
learning image features in small squares
of<00:14:33.340><c> the</c><00:14:33.550><c> input</c><00:14:33.880><c> and</c><00:14:34.380><c> to</c><00:14:35.380><c> do</c><00:14:35.530><c> this</c><00:14:35.680><c> what</c><00:14:36.280><c> we</c><00:14:36.340><c> do</c>

00:14:36.600 --> 00:14:36.610 align:start position:0%
of the input and to do this what we do
 

00:14:36.610 --> 00:14:38.880 align:start position:0%
of the input and to do this what we do
is<00:14:36.850><c> we</c><00:14:37.060><c> simply</c><00:14:37.330><c> perform</c><00:14:38.050><c> an</c><00:14:38.320><c> element-wise</c>

00:14:38.880 --> 00:14:38.890 align:start position:0%
is we simply perform an element-wise
 

00:14:38.890 --> 00:14:42.300 align:start position:0%
is we simply perform an element-wise
multiplication<00:14:39.960><c> between</c><00:14:40.960><c> the</c><00:14:41.680><c> filter</c><00:14:41.980><c> weight</c>

00:14:42.300 --> 00:14:42.310 align:start position:0%
multiplication between the filter weight
 

00:14:42.310 --> 00:14:44.880 align:start position:0%
multiplication between the filter weight
matrix<00:14:42.820><c> and</c><00:14:43.120><c> the</c><00:14:43.780><c> patch</c><00:14:44.020><c> of</c><00:14:44.260><c> the</c><00:14:44.350><c> input</c><00:14:44.710><c> image</c>

00:14:44.880 --> 00:14:44.890 align:start position:0%
matrix and the patch of the input image
 

00:14:44.890 --> 00:14:48.630 align:start position:0%
matrix and the patch of the input image
of<00:14:45.250><c> the</c><00:14:46.000><c> same</c><00:14:46.270><c> dimensions</c><00:14:46.960><c> and</c><00:14:47.230><c> this</c><00:14:48.100><c> results</c>

00:14:48.630 --> 00:14:48.640 align:start position:0%
of the same dimensions and this results
 

00:14:48.640 --> 00:14:51.540 align:start position:0%
of the same dimensions and this results
in<00:14:48.820><c> this</c><00:14:49.360><c> case</c><00:14:49.630><c> in</c><00:14:49.930><c> a</c><00:14:50.410><c> three</c><00:14:50.680><c> by</c><00:14:50.830><c> three</c><00:14:51.130><c> matrix</c>

00:14:51.540 --> 00:14:51.550 align:start position:0%
in this case in a three by three matrix
 

00:14:51.550 --> 00:14:55.530 align:start position:0%
in this case in a three by three matrix
and<00:14:52.560><c> here</c><00:14:53.560><c> in</c><00:14:53.680><c> this</c><00:14:53.800><c> example</c><00:14:54.160><c> all</c><00:14:54.640><c> the</c><00:14:55.150><c> entries</c>

00:14:55.530 --> 00:14:55.540 align:start position:0%
and here in this example all the entries
 

00:14:55.540 --> 00:14:58.620 align:start position:0%
and here in this example all the entries
in<00:14:55.570><c> this</c><00:14:55.840><c> matrix</c><00:14:55.900><c> are</c><00:14:56.530><c> 1</c><00:14:56.980><c> and</c><00:14:57.370><c> that's</c><00:14:58.240><c> because</c>

00:14:58.620 --> 00:14:58.630 align:start position:0%
in this matrix are 1 and that's because
 

00:14:58.630 --> 00:15:00.360 align:start position:0%
in this matrix are 1 and that's because
everything<00:14:59.140><c> is</c><00:14:59.380><c> black</c><00:14:59.680><c> and</c><00:14:59.710><c> white</c><00:14:59.920><c> either</c>

00:15:00.360 --> 00:15:00.370 align:start position:0%
everything is black and white either
 

00:15:00.370 --> 00:15:04.170 align:start position:0%
everything is black and white either
minus<00:15:00.880><c> 1</c><00:15:01.090><c> and</c><00:15:01.240><c> 1</c><00:15:01.420><c> and</c><00:15:02.580><c> this</c><00:15:03.580><c> indicates</c><00:15:03.910><c> that</c>

00:15:04.170 --> 00:15:04.180 align:start position:0%
minus 1 and 1 and this indicates that
 

00:15:04.180 --> 00:15:06.240 align:start position:0%
minus 1 and 1 and this indicates that
there<00:15:04.420><c> is</c><00:15:04.480><c> a</c><00:15:04.630><c> perfect</c><00:15:05.320><c> correspondence</c>

00:15:06.240 --> 00:15:06.250 align:start position:0%
there is a perfect correspondence
 

00:15:06.250 --> 00:15:09.420 align:start position:0%
there is a perfect correspondence
between<00:15:06.790><c> our</c><00:15:07.210><c> filter</c><00:15:07.630><c> matrix</c><00:15:08.140><c> and</c><00:15:08.380><c> the</c><00:15:09.220><c> patch</c>

00:15:09.420 --> 00:15:09.430 align:start position:0%
between our filter matrix and the patch
 

00:15:09.430 --> 00:15:11.730 align:start position:0%
between our filter matrix and the patch
of<00:15:09.460><c> the</c><00:15:09.700><c> input</c><00:15:10.120><c> image</c><00:15:10.300><c> where</c><00:15:11.080><c> we</c><00:15:11.230><c> multiplied</c>

00:15:11.730 --> 00:15:11.740 align:start position:0%
of the input image where we multiplied
 

00:15:11.740 --> 00:15:14.130 align:start position:0%
of the input image where we multiplied
it<00:15:11.950><c> right</c><00:15:12.220><c> so</c><00:15:12.790><c> this</c><00:15:12.940><c> is</c><00:15:13.120><c> our</c><00:15:13.270><c> filter</c><00:15:13.630><c> this</c><00:15:13.960><c> is</c>

00:15:14.130 --> 00:15:14.140 align:start position:0%
it right so this is our filter this is
 

00:15:14.140 --> 00:15:16.950 align:start position:0%
it right so this is our filter this is
our<00:15:14.290><c> patch</c><00:15:14.610><c> they</c><00:15:15.610><c> directly</c><00:15:16.030><c> correspond</c><00:15:16.660><c> and</c>

00:15:16.950 --> 00:15:16.960 align:start position:0%
our patch they directly correspond and
 

00:15:16.960 --> 00:15:22.080 align:start position:0%
our patch they directly correspond and
the<00:15:17.080><c> result</c><00:15:17.440><c> is</c><00:15:18.180><c> is</c><00:15:19.180><c> as</c><00:15:19.390><c> follows</c><00:15:20.880><c> finally</c><00:15:21.880><c> if</c>

00:15:22.080 --> 00:15:22.090 align:start position:0%
the result is is as follows finally if
 

00:15:22.090 --> 00:15:24.360 align:start position:0%
the result is is as follows finally if
we<00:15:22.330><c> add</c><00:15:22.510><c> all</c><00:15:22.870><c> the</c><00:15:23.020><c> elements</c><00:15:23.500><c> of</c><00:15:23.620><c> this</c><00:15:23.800><c> of</c><00:15:24.100><c> this</c>

00:15:24.360 --> 00:15:24.370 align:start position:0%
we add all the elements of this of this
 

00:15:24.370 --> 00:15:26.970 align:start position:0%
we add all the elements of this of this
matrix<00:15:24.750><c> this</c><00:15:25.750><c> is</c><00:15:25.810><c> the</c><00:15:25.990><c> result</c><00:15:26.440><c> of</c><00:15:26.650><c> convolving</c>

00:15:26.970 --> 00:15:26.980 align:start position:0%
matrix this is the result of convolving
 

00:15:26.980 --> 00:15:30.180 align:start position:0%
matrix this is the result of convolving
this<00:15:27.880><c> 3x3</c><00:15:28.630><c> filter</c><00:15:29.020><c> with</c><00:15:29.500><c> that</c><00:15:29.680><c> particular</c>

00:15:30.180 --> 00:15:30.190 align:start position:0%
this 3x3 filter with that particular
 

00:15:30.190 --> 00:15:31.310 align:start position:0%
this 3x3 filter with that particular
region<00:15:30.430><c> of</c><00:15:30.730><c> the</c><00:15:30.850><c> input</c>

00:15:31.310 --> 00:15:31.320 align:start position:0%
region of the input
 

00:15:31.320 --> 00:15:35.330 align:start position:0%
region of the input
and<00:15:31.410><c> we</c><00:15:32.100><c> get</c><00:15:32.250><c> back</c><00:15:32.490><c> to</c><00:15:32.670><c> number</c><00:15:32.970><c> nine</c><00:15:34.130><c> so</c><00:15:35.130><c> let's</c>

00:15:35.330 --> 00:15:35.340 align:start position:0%
and we get back to number nine so let's
 

00:15:35.340 --> 00:15:37.460 align:start position:0%
and we get back to number nine so let's
consider<00:15:35.550><c> another</c><00:15:35.940><c> example</c><00:15:36.210><c> right</c><00:15:36.780><c> right</c><00:15:37.260><c> to</c>

00:15:37.460 --> 00:15:37.470 align:start position:0%
consider another example right right to
 

00:15:37.470 --> 00:15:39.700 align:start position:0%
consider another example right right to
hopefully<00:15:38.130><c> drive</c><00:15:38.580><c> this</c><00:15:38.790><c> home</c><00:15:39.000><c> even</c><00:15:39.330><c> further</c>

00:15:39.700 --> 00:15:39.710 align:start position:0%
hopefully drive this home even further
 

00:15:39.710 --> 00:15:43.250 align:start position:0%
hopefully drive this home even further
suppose<00:15:40.710><c> we</c><00:15:40.980><c> want</c><00:15:41.220><c> to</c><00:15:41.310><c> compute</c><00:15:42.300><c> the</c>

00:15:43.250 --> 00:15:43.260 align:start position:0%
suppose we want to compute the
 

00:15:43.260 --> 00:15:46.490 align:start position:0%
suppose we want to compute the
convolution<00:15:43.590><c> of</c><00:15:43.980><c> this</c><00:15:44.160><c> 5x5</c><00:15:45.500><c> representation</c>

00:15:46.490 --> 00:15:46.500 align:start position:0%
convolution of this 5x5 representation
 

00:15:46.500 --> 00:15:50.120 align:start position:0%
convolution of this 5x5 representation
of<00:15:46.620><c> an</c><00:15:46.710><c> image</c><00:15:46.830><c> and</c><00:15:47.250><c> this</c><00:15:47.760><c> 3x3</c><00:15:48.510><c> filter</c><00:15:49.050><c> to</c><00:15:49.950><c> do</c>

00:15:50.120 --> 00:15:50.130 align:start position:0%
of an image and this 3x3 filter to do
 

00:15:50.130 --> 00:15:51.920 align:start position:0%
of an image and this 3x3 filter to do
this<00:15:50.280><c> we</c><00:15:50.490><c> need</c><00:15:50.670><c> to</c><00:15:50.700><c> cover</c><00:15:51.000><c> the</c><00:15:51.240><c> entirety</c><00:15:51.900><c> of</c>

00:15:51.920 --> 00:15:51.930 align:start position:0%
this we need to cover the entirety of
 

00:15:51.930 --> 00:15:54.590 align:start position:0%
this we need to cover the entirety of
the<00:15:52.110><c> input</c><00:15:52.470><c> image</c><00:15:52.620><c> by</c><00:15:53.370><c> sliding</c><00:15:54.060><c> this</c><00:15:54.210><c> filter</c>

00:15:54.590 --> 00:15:54.600 align:start position:0%
the input image by sliding this filter
 

00:15:54.600 --> 00:15:58.070 align:start position:0%
the input image by sliding this filter
over<00:15:55.140><c> over</c><00:15:55.860><c> the</c><00:15:56.040><c> over</c><00:15:56.640><c> the</c><00:15:56.730><c> image</c><00:15:57.080><c> performing</c>

00:15:58.070 --> 00:15:58.080 align:start position:0%
over over the over the image performing
 

00:15:58.080 --> 00:16:01.340 align:start position:0%
over over the over the image performing
this<00:15:58.400><c> element</c><00:15:59.400><c> wise</c><00:15:59.640><c> multiplication</c><00:16:00.080><c> at</c><00:16:01.080><c> each</c>

00:16:01.340 --> 00:16:01.350 align:start position:0%
this element wise multiplication at each
 

00:16:01.350 --> 00:16:04.070 align:start position:0%
this element wise multiplication at each
step<00:16:01.680><c> and</c><00:16:01.920><c> adding</c><00:16:02.910><c> the</c><00:16:03.030><c> outputs</c><00:16:03.510><c> that</c><00:16:03.690><c> result</c>

00:16:04.070 --> 00:16:04.080 align:start position:0%
step and adding the outputs that result
 

00:16:04.080 --> 00:16:06.700 align:start position:0%
step and adding the outputs that result
after<00:16:04.590><c> each</c><00:16:04.830><c> element</c><00:16:05.730><c> wise</c><00:16:05.880><c> multiplication</c>

00:16:06.700 --> 00:16:06.710 align:start position:0%
after each element wise multiplication
 

00:16:06.710 --> 00:16:10.490 align:start position:0%
after each element wise multiplication
so<00:16:07.710><c> let's</c><00:16:07.920><c> see</c><00:16:08.100><c> what</c><00:16:08.280><c> this</c><00:16:08.550><c> looks</c><00:16:08.820><c> like</c><00:16:09.500><c> first</c>

00:16:10.490 --> 00:16:10.500 align:start position:0%
so let's see what this looks like first
 

00:16:10.500 --> 00:16:12.350 align:start position:0%
so let's see what this looks like first
we<00:16:10.830><c> start</c><00:16:11.130><c> off</c><00:16:11.250><c> in</c><00:16:11.310><c> this</c><00:16:11.610><c> upper</c><00:16:11.940><c> left</c><00:16:12.150><c> corner</c>

00:16:12.350 --> 00:16:12.360 align:start position:0%
we start off in this upper left corner
 

00:16:12.360 --> 00:16:16.310 align:start position:0%
we start off in this upper left corner
we<00:16:13.140><c> multiply</c><00:16:13.710><c> this</c><00:16:13.980><c> filter</c><00:16:14.400><c> by</c><00:16:15.060><c> the</c><00:16:15.090><c> values</c><00:16:16.020><c> of</c>

00:16:16.310 --> 00:16:16.320 align:start position:0%
we multiply this filter by the values of
 

00:16:16.320 --> 00:16:20.330 align:start position:0%
we multiply this filter by the values of
our<00:16:16.650><c> of</c><00:16:17.430><c> our</c><00:16:17.850><c> input</c><00:16:18.210><c> image</c><00:16:18.570><c> and</c><00:16:18.980><c> add</c><00:16:19.980><c> the</c>

00:16:20.330 --> 00:16:20.340 align:start position:0%
our of our input image and add the
 

00:16:20.340 --> 00:16:24.860 align:start position:0%
our of our input image and add the
result<00:16:20.670><c> and</c><00:16:22.040><c> we</c><00:16:23.040><c> end</c><00:16:23.250><c> up</c><00:16:23.550><c> with</c><00:16:23.880><c> the</c><00:16:24.450><c> value</c><00:16:24.750><c> of</c><00:16:24.780><c> 4</c>

00:16:24.860 --> 00:16:24.870 align:start position:0%
result and we end up with the value of 4
 

00:16:24.870 --> 00:16:27.620 align:start position:0%
result and we end up with the value of 4
this<00:16:25.740><c> results</c><00:16:26.250><c> in</c><00:16:26.400><c> the</c><00:16:26.430><c> first</c><00:16:26.790><c> entry</c><00:16:27.270><c> in</c><00:16:27.450><c> our</c>

00:16:27.620 --> 00:16:27.630 align:start position:0%
this results in the first entry in our
 

00:16:27.630 --> 00:16:29.960 align:start position:0%
this results in the first entry in our
output<00:16:28.110><c> matrix</c><00:16:28.290><c> which</c><00:16:29.070><c> we</c><00:16:29.250><c> can</c><00:16:29.430><c> call</c><00:16:29.610><c> the</c>

00:16:29.960 --> 00:16:29.970 align:start position:0%
output matrix which we can call the
 

00:16:29.970 --> 00:16:32.900 align:start position:0%
output matrix which we can call the
feature<00:16:30.180><c> map</c><00:16:30.560><c> we</c><00:16:31.560><c> next</c><00:16:31.920><c> slide</c><00:16:32.160><c> the</c><00:16:32.430><c> 3</c><00:16:32.730><c> by</c><00:16:32.880><c> 3</c>

00:16:32.900 --> 00:16:32.910 align:start position:0%
feature map we next slide the 3 by 3
 

00:16:32.910 --> 00:16:36.140 align:start position:0%
feature map we next slide the 3 by 3
filter<00:16:33.570><c> over</c><00:16:34.260><c> by</c><00:16:34.440><c> 1</c><00:16:34.680><c> to</c><00:16:35.220><c> grab</c><00:16:35.460><c> the</c><00:16:35.670><c> next</c><00:16:35.910><c> patch</c>

00:16:36.140 --> 00:16:36.150 align:start position:0%
filter over by 1 to grab the next patch
 

00:16:36.150 --> 00:16:38.090 align:start position:0%
filter over by 1 to grab the next patch
and<00:16:36.510><c> repeat</c><00:16:37.290><c> this</c><00:16:37.500><c> element</c><00:16:37.950><c> wise</c>

00:16:38.090 --> 00:16:38.100 align:start position:0%
and repeat this element wise
 

00:16:38.100 --> 00:16:40.940 align:start position:0%
and repeat this element wise
multiplication<00:16:38.130><c> in</c><00:16:39.120><c> addition</c><00:16:39.600><c> this</c><00:16:40.470><c> gives</c><00:16:40.770><c> us</c>

00:16:40.940 --> 00:16:40.950 align:start position:0%
multiplication in addition this gives us
 

00:16:40.950 --> 00:16:44.090 align:start position:0%
multiplication in addition this gives us
our<00:16:41.370><c> second</c><00:16:41.820><c> entry</c><00:16:42.150><c> 3</c><00:16:42.450><c> we</c><00:16:43.440><c> continue</c><00:16:43.980><c> this</c>

00:16:44.090 --> 00:16:44.100 align:start position:0%
our second entry 3 we continue this
 

00:16:44.100 --> 00:16:47.240 align:start position:0%
our second entry 3 we continue this
process<00:16:44.190><c> until</c><00:16:44.910><c> we</c><00:16:45.840><c> have</c><00:16:45.990><c> covered</c><00:16:46.470><c> the</c>

00:16:47.240 --> 00:16:47.250 align:start position:0%
process until we have covered the
 

00:16:47.250 --> 00:16:49.420 align:start position:0%
process until we have covered the
entirety<00:16:47.970><c> of</c><00:16:48.150><c> this</c><00:16:48.270><c> input</c><00:16:48.720><c> image</c>

00:16:49.420 --> 00:16:49.430 align:start position:0%
entirety of this input image
 

00:16:49.430 --> 00:16:52.520 align:start position:0%
entirety of this input image
progressively<00:16:50.430><c> sliding</c><00:16:51.150><c> our</c><00:16:51.390><c> filter</c><00:16:51.840><c> to</c>

00:16:52.520 --> 00:16:52.530 align:start position:0%
progressively sliding our filter to
 

00:16:52.530 --> 00:16:54.440 align:start position:0%
progressively sliding our filter to
cover<00:16:52.770><c> it</c><00:16:52.950><c> doing</c><00:16:53.670><c> this</c><00:16:53.820><c> element</c><00:16:54.270><c> wise</c>

00:16:54.440 --> 00:16:54.450 align:start position:0%
cover it doing this element wise
 

00:16:54.450 --> 00:16:58.040 align:start position:0%
cover it doing this element wise
multiplication<00:16:55.310><c> patch</c><00:16:56.310><c> by</c><00:16:56.640><c> patch</c><00:16:56.930><c> adding</c><00:16:57.930><c> the</c>

00:16:58.040 --> 00:16:58.050 align:start position:0%
multiplication patch by patch adding the
 

00:16:58.050 --> 00:17:01.640 align:start position:0%
multiplication patch by patch adding the
result<00:16:58.410><c> and</c><00:16:59.180><c> filling</c><00:17:00.180><c> out</c><00:17:00.360><c> our</c><00:17:00.630><c> feature</c><00:17:01.290><c> map</c>

00:17:01.640 --> 00:17:01.650 align:start position:0%
result and filling out our feature map
 

00:17:01.650 --> 00:17:03.200 align:start position:0%
result and filling out our feature map
and<00:17:01.950><c> that's</c><00:17:02.820><c> it</c>

00:17:03.200 --> 00:17:03.210 align:start position:0%
and that's it
 

00:17:03.210 --> 00:17:06.319 align:start position:0%
and that's it
that's<00:17:03.750><c> convolution</c><00:17:04.500><c> and</c><00:17:04.730><c> this</c><00:17:05.730><c> feature</c><00:17:06.000><c> map</c>

00:17:06.319 --> 00:17:06.329 align:start position:0%
that's convolution and this feature map
 

00:17:06.329 --> 00:17:08.329 align:start position:0%
that's convolution and this feature map
right<00:17:06.689><c> this</c><00:17:06.930><c> is</c><00:17:07.079><c> a</c><00:17:07.110><c> toy</c><00:17:07.380><c> example</c><00:17:07.980><c> but</c><00:17:08.189><c> in</c>

00:17:08.329 --> 00:17:08.339 align:start position:0%
right this is a toy example but in
 

00:17:08.339 --> 00:17:09.920 align:start position:0%
right this is a toy example but in
practice<00:17:08.850><c> you</c><00:17:09.000><c> can</c><00:17:09.120><c> imagine</c><00:17:09.390><c> that</c><00:17:09.660><c> this</c>

00:17:09.920 --> 00:17:09.930 align:start position:0%
practice you can imagine that this
 

00:17:09.930 --> 00:17:13.340 align:start position:0%
practice you can imagine that this
feature<00:17:10.230><c> map</c><00:17:10.560><c> reflects</c><00:17:11.459><c> where</c><00:17:12.180><c> in</c><00:17:12.480><c> the</c><00:17:12.720><c> input</c>

00:17:13.340 --> 00:17:13.350 align:start position:0%
feature map reflects where in the input
 

00:17:13.350 --> 00:17:16.189 align:start position:0%
feature map reflects where in the input
image<00:17:13.709><c> was</c><00:17:13.949><c> activated</c><00:17:14.939><c> by</c><00:17:14.970><c> this</c><00:17:15.270><c> filter</c><00:17:15.660><c> where</c>

00:17:16.189 --> 00:17:16.199 align:start position:0%
image was activated by this filter where
 

00:17:16.199 --> 00:17:19.250 align:start position:0%
image was activated by this filter where
in<00:17:16.350><c> the</c><00:17:16.500><c> input</c><00:17:16.800><c> image</c><00:17:17.329><c> that</c><00:17:18.329><c> filter</c><00:17:18.720><c> picked</c><00:17:19.079><c> up</c>

00:17:19.250 --> 00:17:19.260 align:start position:0%
in the input image that filter picked up
 

00:17:19.260 --> 00:17:21.350 align:start position:0%
in the input image that filter picked up
on<00:17:19.439><c> right</c><00:17:19.680><c> because</c><00:17:20.010><c> higher</c><00:17:20.459><c> values</c><00:17:20.970><c> are</c><00:17:21.180><c> going</c>

00:17:21.350 --> 00:17:21.360 align:start position:0%
on right because higher values are going
 

00:17:21.360 --> 00:17:23.329 align:start position:0%
on right because higher values are going
to<00:17:21.449><c> represent</c><00:17:21.750><c> like</c><00:17:22.620><c> sort</c><00:17:22.920><c> of</c><00:17:22.980><c> a</c><00:17:23.070><c> greater</c>

00:17:23.329 --> 00:17:23.339 align:start position:0%
to represent like sort of a greater
 

00:17:23.339 --> 00:17:26.390 align:start position:0%
to represent like sort of a greater
activation<00:17:23.790><c> if</c><00:17:24.240><c> you</c><00:17:24.360><c> will</c><00:17:24.540><c> and</c><00:17:24.839><c> so</c><00:17:25.770><c> this</c><00:17:26.220><c> is</c>

00:17:26.390 --> 00:17:26.400 align:start position:0%
activation if you will and so this is
 

00:17:26.400 --> 00:17:28.910 align:start position:0%
activation if you will and so this is
really<00:17:26.579><c> the</c><00:17:27.000><c> the</c><00:17:27.480><c> bare-bones</c><00:17:27.770><c> mechanism</c><00:17:28.770><c> of</c>

00:17:28.910 --> 00:17:28.920 align:start position:0%
really the the bare-bones mechanism of
 

00:17:28.920 --> 00:17:32.900 align:start position:0%
really the the bare-bones mechanism of
this<00:17:29.160><c> convolution</c><00:17:29.760><c> operation</c><00:17:30.090><c> and</c><00:17:31.910><c> to</c>

00:17:32.900 --> 00:17:32.910 align:start position:0%
this convolution operation and to
 

00:17:32.910 --> 00:17:35.440 align:start position:0%
this convolution operation and to
consider<00:17:33.270><c> really</c><00:17:33.840><c> how</c><00:17:34.050><c> powerful</c><00:17:34.710><c> this</c><00:17:34.890><c> is</c>

00:17:35.440 --> 00:17:35.450 align:start position:0%
consider really how powerful this is
 

00:17:35.450 --> 00:17:39.290 align:start position:0%
consider really how powerful this is
different<00:17:36.450><c> different</c><00:17:37.380><c> different</c><00:17:38.300><c> weight</c>

00:17:39.290 --> 00:17:39.300 align:start position:0%
different different different weight
 

00:17:39.300 --> 00:17:42.020 align:start position:0%
different different different weight
filters<00:17:39.780><c> can</c><00:17:40.080><c> be</c><00:17:40.290><c> used</c><00:17:40.680><c> to</c><00:17:40.920><c> produce</c><00:17:41.100><c> distinct</c>

00:17:42.020 --> 00:17:42.030 align:start position:0%
filters can be used to produce distinct
 

00:17:42.030 --> 00:17:45.110 align:start position:0%
filters can be used to produce distinct
feature<00:17:42.630><c> Maps</c><00:17:43.080><c> so</c><00:17:44.010><c> this</c><00:17:44.190><c> is</c><00:17:44.370><c> a</c><00:17:44.400><c> very</c><00:17:44.730><c> fit</c>

00:17:45.110 --> 00:17:45.120 align:start position:0%
feature Maps so this is a very fit
 

00:17:45.120 --> 00:17:47.480 align:start position:0%
feature Maps so this is a very fit
famous<00:17:45.420><c> picture</c><00:17:45.809><c> of</c><00:17:46.260><c> this</c><00:17:46.440><c> woman</c><00:17:46.650><c> who's</c>

00:17:47.480 --> 00:17:47.490 align:start position:0%
famous picture of this woman who's
 

00:17:47.490 --> 00:17:51.260 align:start position:0%
famous picture of this woman who's
called<00:17:47.880><c> Lenna</c><00:17:48.330><c> and</c><00:17:49.260><c> as</c><00:17:50.220><c> you</c><00:17:50.370><c> can</c><00:17:50.520><c> see</c><00:17:50.730><c> here</c><00:17:51.059><c> in</c>

00:17:51.260 --> 00:17:51.270 align:start position:0%
called Lenna and as you can see here in
 

00:17:51.270 --> 00:17:53.630 align:start position:0%
called Lenna and as you can see here in
these<00:17:51.480><c> three</c><00:17:51.540><c> examples</c><00:17:51.960><c> we've</c><00:17:52.590><c> taken</c><00:17:53.160><c> three</c>

00:17:53.630 --> 00:17:53.640 align:start position:0%
these three examples we've taken three
 

00:17:53.640 --> 00:17:56.900 align:start position:0%
these three examples we've taken three
different<00:17:53.910><c> filters</c><00:17:54.830><c> applied</c><00:17:55.830><c> this</c><00:17:56.250><c> filter</c><00:17:56.670><c> to</c>

00:17:56.900 --> 00:17:56.910 align:start position:0%
different filters applied this filter to
 

00:17:56.910 --> 00:18:00.920 align:start position:0%
different filters applied this filter to
the<00:17:57.030><c> same</c><00:17:57.330><c> input</c><00:17:58.160><c> image</c><00:17:59.160><c> and</c><00:17:59.570><c> generated</c><00:18:00.570><c> three</c>

00:18:00.920 --> 00:18:00.930 align:start position:0%
the same input image and generated three
 

00:18:00.930 --> 00:18:04.970 align:start position:0%
the same input image and generated three
very<00:18:01.380><c> different</c><00:18:01.680><c> outputs</c><00:18:02.460><c> and</c><00:18:03.410><c> as</c><00:18:04.410><c> you</c><00:18:04.830><c> can</c>

00:18:04.970 --> 00:18:04.980 align:start position:0%
very different outputs and as you can
 

00:18:04.980 --> 00:18:06.890 align:start position:0%
very different outputs and as you can
see<00:18:05.130><c> by</c><00:18:05.309><c> simply</c><00:18:05.580><c> changing</c><00:18:06.000><c> the</c><00:18:06.390><c> weights</c><00:18:06.630><c> of</c>

00:18:06.890 --> 00:18:06.900 align:start position:0%
see by simply changing the weights of
 

00:18:06.900 --> 00:18:09.440 align:start position:0%
see by simply changing the weights of
the<00:18:07.020><c> filters</c><00:18:07.440><c> we</c><00:18:07.950><c> can</c><00:18:08.130><c> detect</c><00:18:08.460><c> we</c><00:18:08.940><c> can</c><00:18:09.120><c> detect</c>

00:18:09.440 --> 00:18:09.450 align:start position:0%
the filters we can detect we can detect
 

00:18:09.450 --> 00:18:12.170 align:start position:0%
the filters we can detect we can detect
and<00:18:09.750><c> extract</c><00:18:10.290><c> different</c><00:18:11.070><c> features</c><00:18:11.460><c> like</c>

00:18:12.170 --> 00:18:12.180 align:start position:0%
and extract different features like
 

00:18:12.180 --> 00:18:14.510 align:start position:0%
and extract different features like
edges<00:18:12.660><c> that</c><00:18:12.990><c> are</c><00:18:13.140><c> present</c><00:18:13.620><c> in</c><00:18:13.800><c> the</c><00:18:14.190><c> input</c>

00:18:14.510 --> 00:18:14.520 align:start position:0%
edges that are present in the input
 

00:18:14.520 --> 00:18:19.610 align:start position:0%
edges that are present in the input
image<00:18:14.820><c> this</c><00:18:15.809><c> is</c><00:18:16.020><c> really</c><00:18:16.320><c> really</c><00:18:16.590><c> powerful</c><00:18:18.620><c> so</c>

00:18:19.610 --> 00:18:19.620 align:start position:0%
image this is really really powerful so
 

00:18:19.620 --> 00:18:21.770 align:start position:0%
image this is really really powerful so
hopefully<00:18:20.190><c> you</c><00:18:20.309><c> can</c><00:18:20.429><c> now</c><00:18:20.490><c> appreciate</c><00:18:21.030><c> how</c>

00:18:21.770 --> 00:18:21.780 align:start position:0%
hopefully you can now appreciate how
 

00:18:21.780 --> 00:18:24.440 align:start position:0%
hopefully you can now appreciate how
convolution<00:18:22.590><c> allows</c><00:18:22.950><c> us</c><00:18:23.220><c> to</c><00:18:23.790><c> capitalize</c><00:18:24.420><c> on</c>

00:18:24.440 --> 00:18:24.450 align:start position:0%
convolution allows us to capitalize on
 

00:18:24.450 --> 00:18:27.799 align:start position:0%
convolution allows us to capitalize on
the<00:18:25.230><c> spatial</c><00:18:25.679><c> structure</c><00:18:25.980><c> that's</c><00:18:26.750><c> inherent</c><00:18:27.750><c> in</c>

00:18:27.799 --> 00:18:27.809 align:start position:0%
the spatial structure that's inherent in
 

00:18:27.809 --> 00:18:30.080 align:start position:0%
the spatial structure that's inherent in
visual<00:18:28.260><c> data</c><00:18:28.440><c> and</c><00:18:28.770><c> use</c><00:18:29.429><c> these</c><00:18:29.610><c> sets</c><00:18:29.940><c> of</c>

00:18:30.080 --> 00:18:30.090 align:start position:0%
visual data and use these sets of
 

00:18:30.090 --> 00:18:33.350 align:start position:0%
visual data and use these sets of
weights<00:18:30.360><c> to</c><00:18:30.660><c> extract</c><00:18:31.170><c> local</c><00:18:31.740><c> features</c><00:18:32.100><c> and</c><00:18:32.460><c> we</c>

00:18:33.350 --> 00:18:33.360 align:start position:0%
weights to extract local features and we
 

00:18:33.360 --> 00:18:35.090 align:start position:0%
weights to extract local features and we
can<00:18:33.510><c> very</c><00:18:33.750><c> easily</c><00:18:33.840><c> detect</c><00:18:34.590><c> different</c>

00:18:35.090 --> 00:18:35.100 align:start position:0%
can very easily detect different
 

00:18:35.100 --> 00:18:37.490 align:start position:0%
can very easily detect different
features<00:18:35.400><c> simply</c><00:18:36.090><c> by</c><00:18:36.420><c> applying</c><00:18:37.080><c> different</c>

00:18:37.490 --> 00:18:37.500 align:start position:0%
features simply by applying different
 

00:18:37.500 --> 00:18:41.180 align:start position:0%
features simply by applying different
filters<00:18:38.160><c> and</c><00:18:38.480><c> these</c><00:18:39.480><c> concepts</c><00:18:40.080><c> of</c><00:18:40.290><c> preserving</c>

00:18:41.180 --> 00:18:41.190 align:start position:0%
filters and these concepts of preserving
 

00:18:41.190 --> 00:18:43.010 align:start position:0%
filters and these concepts of preserving
spatial<00:18:41.610><c> structure</c><00:18:42.090><c> and</c><00:18:42.360><c> local</c><00:18:42.780><c> feature</c>

00:18:43.010 --> 00:18:43.020 align:start position:0%
spatial structure and local feature
 

00:18:43.020 --> 00:18:45.770 align:start position:0%
spatial structure and local feature
extraction<00:18:43.800><c> using</c><00:18:44.580><c> these</c><00:18:44.910><c> convolutions</c><00:18:45.390><c> are</c>

00:18:45.770 --> 00:18:45.780 align:start position:0%
extraction using these convolutions are
 

00:18:45.780 --> 00:18:48.860 align:start position:0%
extraction using these convolutions are
at<00:18:46.320><c> the</c><00:18:46.740><c> core</c><00:18:47.070><c> of</c><00:18:47.100><c> the</c><00:18:47.490><c> neural</c><00:18:47.730><c> networks</c><00:18:48.150><c> used</c>

00:18:48.860 --> 00:18:48.870 align:start position:0%
at the core of the neural networks used
 

00:18:48.870 --> 00:18:52.040 align:start position:0%
at the core of the neural networks used
for<00:18:49.260><c> computer</c><00:18:49.920><c> vision</c><00:18:49.950><c> tasks</c><00:18:50.750><c> which</c><00:18:51.750><c> are</c>

00:18:52.040 --> 00:18:52.050 align:start position:0%
for computer vision tasks which are
 

00:18:52.050 --> 00:18:54.400 align:start position:0%
for computer vision tasks which are
called<00:18:52.410><c> convolutional</c><00:18:53.309><c> neural</c><00:18:53.520><c> networks</c><00:18:54.030><c> or</c>

00:18:54.400 --> 00:18:54.410 align:start position:0%
called convolutional neural networks or
 

00:18:54.410 --> 00:18:59.260 align:start position:0%
called convolutional neural networks or
CNN's<00:18:56.420><c> so</c><00:18:57.420><c> sort</c><00:18:58.170><c> of</c><00:18:58.230><c> with</c><00:18:58.380><c> these</c><00:18:58.530><c> bare</c><00:18:58.770><c> bones</c>

00:18:59.260 --> 00:18:59.270 align:start position:0%
CNN's so sort of with these bare bones
 

00:18:59.270 --> 00:19:02.120 align:start position:0%
CNN's so sort of with these bare bones
mechanism<00:19:00.270><c> under</c><00:19:00.630><c> our</c><00:19:00.660><c> belt</c><00:19:01.110><c> we</c><00:19:01.830><c> can</c><00:19:01.980><c> think</c>

00:19:02.120 --> 00:19:02.130 align:start position:0%
mechanism under our belt we can think
 

00:19:02.130 --> 00:19:04.400 align:start position:0%
mechanism under our belt we can think
about<00:19:02.250><c> how</c><00:19:02.429><c> we</c><00:19:02.670><c> can</c><00:19:02.970><c> utilize</c><00:19:03.390><c> this</c><00:19:03.690><c> to</c><00:19:04.170><c> build</c>

00:19:04.400 --> 00:19:04.410 align:start position:0%
about how we can utilize this to build
 

00:19:04.410 --> 00:19:06.440 align:start position:0%
about how we can utilize this to build
neural<00:19:05.010><c> networks</c><00:19:05.490><c> for</c><00:19:05.670><c> computer</c><00:19:06.420><c> vision</c>

00:19:06.440 --> 00:19:06.450 align:start position:0%
neural networks for computer vision
 

00:19:06.450 --> 00:19:09.610 align:start position:0%
neural networks for computer vision
tasks<00:19:07.140><c> so</c><00:19:07.980><c> let's</c><00:19:08.280><c> first</c><00:19:08.520><c> consider</c><00:19:08.760><c> a</c><00:19:09.000><c> CNN</c>

00:19:09.610 --> 00:19:09.620 align:start position:0%
tasks so let's first consider a CNN
 

00:19:09.620 --> 00:19:13.180 align:start position:0%
tasks so let's first consider a CNN
designed<00:19:10.620><c> for</c><00:19:10.950><c> image</c><00:19:11.280><c> classification</c>

00:19:13.180 --> 00:19:13.190 align:start position:0%
designed for image classification
 

00:19:13.190 --> 00:19:15.470 align:start position:0%
designed for image classification
remember<00:19:14.190><c> the</c><00:19:14.340><c> goal</c><00:19:14.520><c> here</c><00:19:14.880><c> is</c><00:19:14.910><c> to</c><00:19:15.300><c> learn</c>

00:19:15.470 --> 00:19:15.480 align:start position:0%
remember the goal here is to learn
 

00:19:15.480 --> 00:19:18.020 align:start position:0%
remember the goal here is to learn
features<00:19:16.470><c> directly</c><00:19:16.980><c> from</c><00:19:17.340><c> the</c><00:19:17.520><c> image</c><00:19:17.790><c> data</c>

00:19:18.020 --> 00:19:18.030 align:start position:0%
features directly from the image data
 

00:19:18.030 --> 00:19:20.750 align:start position:0%
features directly from the image data
this<00:19:18.900><c> means</c><00:19:19.200><c> learning</c><00:19:19.590><c> the</c><00:19:19.890><c> weights</c><00:19:20.130><c> of</c><00:19:20.370><c> those</c>

00:19:20.750 --> 00:19:20.760 align:start position:0%
this means learning the weights of those
 

00:19:20.760 --> 00:19:23.540 align:start position:0%
this means learning the weights of those
filters<00:19:21.090><c> and</c><00:19:21.770><c> using</c><00:19:22.770><c> these</c><00:19:22.950><c> learned</c><00:19:23.250><c> feature</c>

00:19:23.540 --> 00:19:23.550 align:start position:0%
filters and using these learned feature
 

00:19:23.550 --> 00:19:26.260 align:start position:0%
filters and using these learned feature
maps<00:19:23.910><c> for</c><00:19:24.330><c> classification</c><00:19:25.290><c> of</c><00:19:25.470><c> these</c><00:19:25.620><c> images</c>

00:19:26.260 --> 00:19:26.270 align:start position:0%
maps for classification of these images
 

00:19:26.270 --> 00:19:28.940 align:start position:0%
maps for classification of these images
now<00:19:27.270><c> there</c><00:19:27.450><c> are</c><00:19:27.540><c> three</c><00:19:27.750><c> main</c><00:19:27.780><c> operations</c><00:19:28.679><c> to</c><00:19:28.920><c> a</c>

00:19:28.940 --> 00:19:28.950 align:start position:0%
now there are three main operations to a
 

00:19:28.950 --> 00:19:32.299 align:start position:0%
now there are three main operations to a
CNN<00:19:29.280><c> the</c><00:19:30.179><c> first</c><00:19:30.450><c> is</c><00:19:30.809><c> convolution</c><00:19:31.440><c> which</c><00:19:32.130><c> we</c>

00:19:32.299 --> 00:19:32.309 align:start position:0%
CNN the first is convolution which we
 

00:19:32.309 --> 00:19:35.510 align:start position:0%
CNN the first is convolution which we
went<00:19:32.490><c> through</c><00:19:32.700><c> right</c><00:19:32.910><c> and</c><00:19:33.679><c> we</c><00:19:34.679><c> saw</c><00:19:34.890><c> how</c><00:19:35.190><c> we</c><00:19:35.250><c> can</c>

00:19:35.510 --> 00:19:35.520 align:start position:0%
went through right and we saw how we can
 

00:19:35.520 --> 00:19:37.370 align:start position:0%
went through right and we saw how we can
apply<00:19:35.850><c> these</c><00:19:35.910><c> filters</c><00:19:36.510><c> to</c><00:19:36.750><c> generate</c><00:19:37.140><c> future</c>

00:19:37.370 --> 00:19:37.380 align:start position:0%
apply these filters to generate future
 

00:19:37.380 --> 00:19:39.530 align:start position:0%
apply these filters to generate future
maps<00:19:37.710><c> the</c><00:19:38.429><c> second</c><00:19:38.880><c> is</c><00:19:39.090><c> applying</c>

00:19:39.530 --> 00:19:39.540 align:start position:0%
maps the second is applying
 

00:19:39.540 --> 00:19:41.780 align:start position:0%
maps the second is applying
non-linearity<00:19:40.470><c> the</c><00:19:40.740><c> same</c><00:19:40.980><c> concept</c><00:19:41.520><c> from</c>

00:19:41.780 --> 00:19:41.790 align:start position:0%
non-linearity the same concept from
 

00:19:41.790 --> 00:19:44.660 align:start position:0%
non-linearity the same concept from
lecture<00:19:42.150><c> one</c><00:19:42.360><c> and</c><00:19:42.630><c> the</c><00:19:43.500><c> third</c><00:19:43.710><c> key</c><00:19:44.010><c> idea</c><00:19:44.429><c> is</c>

00:19:44.660 --> 00:19:44.670 align:start position:0%
lecture one and the third key idea is
 

00:19:44.670 --> 00:19:47.000 align:start position:0%
lecture one and the third key idea is
pooling<00:19:45.240><c> which</c><00:19:45.600><c> is</c><00:19:45.750><c> effectively</c><00:19:46.350><c> like</c><00:19:46.679><c> a</c><00:19:46.710><c> down</c>

00:19:47.000 --> 00:19:47.010 align:start position:0%
pooling which is effectively like a down
 

00:19:47.010 --> 00:19:50.120 align:start position:0%
pooling which is effectively like a down
sampling<00:19:47.580><c> operation</c><00:19:48.540><c> to</c><00:19:48.720><c> reduce</c><00:19:49.170><c> the</c><00:19:49.559><c> size</c><00:19:49.800><c> of</c>

00:19:50.120 --> 00:19:50.130 align:start position:0%
sampling operation to reduce the size of
 

00:19:50.130 --> 00:19:55.330 align:start position:0%
sampling operation to reduce the size of
of<00:19:50.520><c> a</c><00:19:51.030><c> map</c><00:19:51.240><c> and</c><00:19:52.520><c> finally</c><00:19:53.520><c> the</c><00:19:53.670><c> computation</c><00:19:54.420><c> of</c>

00:19:55.330 --> 00:19:55.340 align:start position:0%
of a map and finally the computation of
 

00:19:55.340 --> 00:19:57.280 align:start position:0%
of a map and finally the computation of
class<00:19:55.640><c> scores</c><00:19:56.000><c> and</c><00:19:56.180><c> actually</c><00:19:56.540><c> outputting</c><00:19:57.080><c> a</c>

00:19:57.280 --> 00:19:57.290 align:start position:0%
class scores and actually outputting a
 

00:19:57.290 --> 00:19:59.470 align:start position:0%
class scores and actually outputting a
prediction<00:19:57.800><c> for</c><00:19:58.310><c> the</c><00:19:58.430><c> class</c><00:19:58.640><c> of</c><00:19:58.880><c> an</c><00:19:59.000><c> image</c><00:19:59.330><c> is</c>

00:19:59.470 --> 00:19:59.480 align:start position:0%
prediction for the class of an image is
 

00:19:59.480 --> 00:20:02.260 align:start position:0%
prediction for the class of an image is
achieved<00:20:00.170><c> by</c><00:20:00.440><c> a</c><00:20:00.710><c> fully</c><00:20:01.310><c> connected</c><00:20:01.640><c> layer</c><00:20:02.000><c> at</c>

00:20:02.260 --> 00:20:02.270 align:start position:0%
achieved by a fully connected layer at
 

00:20:02.270 --> 00:20:04.720 align:start position:0%
achieved by a fully connected layer at
the<00:20:02.300><c> end</c><00:20:02.570><c> of</c><00:20:02.690><c> our</c><00:20:02.780><c> network</c><00:20:03.140><c> and</c><00:20:03.400><c> so</c><00:20:04.400><c> in</c>

00:20:04.720 --> 00:20:04.730 align:start position:0%
the end of our network and so in
 

00:20:04.730 --> 00:20:07.180 align:start position:0%
the end of our network and so in
training<00:20:05.360><c> we</c><00:20:05.780><c> train</c><00:20:06.110><c> our</c><00:20:06.230><c> model</c><00:20:06.620><c> on</c><00:20:06.830><c> a</c><00:20:06.860><c> set</c><00:20:07.160><c> of</c>

00:20:07.180 --> 00:20:07.190 align:start position:0%
training we train our model on a set of
 

00:20:07.190 --> 00:20:09.880 align:start position:0%
training we train our model on a set of
images<00:20:07.490><c> and</c><00:20:08.000><c> we</c><00:20:08.570><c> actually</c><00:20:08.990><c> learn</c><00:20:09.290><c> those</c>

00:20:09.880 --> 00:20:09.890 align:start position:0%
images and we actually learn those
 

00:20:09.890 --> 00:20:12.040 align:start position:0%
images and we actually learn those
weights<00:20:10.190><c> of</c><00:20:10.520><c> the</c><00:20:10.940><c> filters</c><00:20:11.330><c> that</c><00:20:11.630><c> are</c><00:20:11.720><c> going</c><00:20:11.900><c> to</c>

00:20:12.040 --> 00:20:12.050 align:start position:0%
weights of the filters that are going to
 

00:20:12.050 --> 00:20:14.860 align:start position:0%
weights of the filters that are going to
be<00:20:12.170><c> used</c><00:20:12.440><c> in</c><00:20:12.590><c> the</c><00:20:12.740><c> network</c><00:20:13.100><c> as</c><00:20:13.420><c> well</c><00:20:14.420><c> as</c><00:20:14.600><c> the</c>

00:20:14.860 --> 00:20:14.870 align:start position:0%
be used in the network as well as the
 

00:20:14.870 --> 00:20:17.830 align:start position:0%
be used in the network as well as the
weights<00:20:15.410><c> in</c><00:20:15.680><c> the</c><00:20:15.800><c> fully</c><00:20:16.010><c> connected</c><00:20:16.490><c> layer</c><00:20:17.030><c> and</c>

00:20:17.830 --> 00:20:17.840 align:start position:0%
weights in the fully connected layer and
 

00:20:17.840 --> 00:20:20.140 align:start position:0%
weights in the fully connected layer and
we'll<00:20:18.410><c> go</c><00:20:18.560><c> through</c><00:20:18.800><c> each</c><00:20:18.980><c> of</c><00:20:19.010><c> these</c><00:20:19.280><c> to</c><00:20:19.940><c> break</c>

00:20:20.140 --> 00:20:20.150 align:start position:0%
we'll go through each of these to break
 

00:20:20.150 --> 00:20:23.410 align:start position:0%
we'll go through each of these to break
down<00:20:20.390><c> this</c><00:20:20.810><c> basic</c><00:20:21.260><c> architecture</c><00:20:21.890><c> of</c><00:20:22.270><c> CNN</c>

00:20:23.410 --> 00:20:23.420 align:start position:0%
down this basic architecture of CNN
 

00:20:23.420 --> 00:20:25.660 align:start position:0%
down this basic architecture of CNN
so<00:20:24.170><c> first</c><00:20:24.470><c> right</c><00:20:24.890><c> as</c><00:20:25.040><c> we've</c><00:20:25.220><c> already</c><00:20:25.370><c> seen</c>

00:20:25.660 --> 00:20:25.670 align:start position:0%
so first right as we've already seen
 

00:20:25.670 --> 00:20:27.790 align:start position:0%
so first right as we've already seen
let's<00:20:26.270><c> consider</c><00:20:26.630><c> the</c><00:20:26.990><c> convolution</c><00:20:27.680><c> operation</c>

00:20:27.790 --> 00:20:27.800 align:start position:0%
let's consider the convolution operation
 

00:20:27.800 --> 00:20:31.570 align:start position:0%
let's consider the convolution operation
as<00:20:28.480><c> before</c><00:20:29.480><c> each</c><00:20:30.080><c> neuron</c><00:20:30.470><c> in</c><00:20:30.860><c> a</c><00:20:31.100><c> hidden</c><00:20:31.400><c> layer</c>

00:20:31.570 --> 00:20:31.580 align:start position:0%
as before each neuron in a hidden layer
 

00:20:31.580 --> 00:20:33.610 align:start position:0%
as before each neuron in a hidden layer
will<00:20:32.150><c> compute</c><00:20:32.570><c> a</c><00:20:32.720><c> weighted</c><00:20:32.780><c> sum</c><00:20:33.320><c> of</c><00:20:33.380><c> its</c>

00:20:33.610 --> 00:20:33.620 align:start position:0%
will compute a weighted sum of its
 

00:20:33.620 --> 00:20:36.490 align:start position:0%
will compute a weighted sum of its
inputs<00:20:34.090><c> apply</c><00:20:35.090><c> a</c><00:20:35.120><c> bias</c><00:20:35.420><c> and</c><00:20:35.780><c> activate</c><00:20:36.290><c> with</c><00:20:36.470><c> a</c>

00:20:36.490 --> 00:20:36.500 align:start position:0%
inputs apply a bias and activate with a
 

00:20:36.500 --> 00:20:38.230 align:start position:0%
inputs apply a bias and activate with a
non-linearity<00:20:37.100><c> that's</c><00:20:37.520><c> the</c><00:20:37.640><c> same</c><00:20:37.880><c> exact</c>

00:20:38.230 --> 00:20:38.240 align:start position:0%
non-linearity that's the same exact
 

00:20:38.240 --> 00:20:40.690 align:start position:0%
non-linearity that's the same exact
concept<00:20:38.480><c> from</c><00:20:38.840><c> lecture</c><00:20:39.650><c> one</c><00:20:39.860><c> but</c><00:20:40.520><c> what's</c>

00:20:40.690 --> 00:20:40.700 align:start position:0%
concept from lecture one but what's
 

00:20:40.700 --> 00:20:42.700 align:start position:0%
concept from lecture one but what's
special<00:20:40.820><c> here</c><00:20:41.390><c> is</c><00:20:41.540><c> this</c><00:20:41.690><c> local</c><00:20:42.380><c> connectivity</c>

00:20:42.700 --> 00:20:42.710 align:start position:0%
special here is this local connectivity
 

00:20:42.710 --> 00:20:45.610 align:start position:0%
special here is this local connectivity
the<00:20:43.610><c> fact</c><00:20:43.850><c> that</c><00:20:43.970><c> each</c><00:20:44.300><c> neuron</c><00:20:44.600><c> in</c><00:20:44.930><c> a</c><00:20:45.320><c> hidden</c>

00:20:45.610 --> 00:20:45.620 align:start position:0%
the fact that each neuron in a hidden
 

00:20:45.620 --> 00:20:48.220 align:start position:0%
the fact that each neuron in a hidden
layer<00:20:45.770><c> is</c><00:20:46.040><c> only</c><00:20:46.370><c> seeing</c><00:20:47.180><c> a</c><00:20:47.480><c> patch</c><00:20:47.720><c> of</c><00:20:47.750><c> what</c>

00:20:48.220 --> 00:20:48.230 align:start position:0%
layer is only seeing a patch of what
 

00:20:48.230 --> 00:20:51.880 align:start position:0%
layer is only seeing a patch of what
comes<00:20:48.470><c> before</c><00:20:48.680><c> it</c><00:20:49.390><c> and</c><00:20:50.390><c> so</c><00:20:51.170><c> this</c><00:20:51.350><c> relation</c>

00:20:51.880 --> 00:20:51.890 align:start position:0%
comes before it and so this relation
 

00:20:51.890 --> 00:20:54.610 align:start position:0%
comes before it and so this relation
defines<00:20:52.310><c> how</c><00:20:53.060><c> neurons</c><00:20:53.600><c> and</c><00:20:53.840><c> convolutional</c>

00:20:54.610 --> 00:20:54.620 align:start position:0%
defines how neurons and convolutional
 

00:20:54.620 --> 00:20:59.080 align:start position:0%
defines how neurons and convolutional
layers<00:20:55.070><c> are</c><00:20:55.310><c> connected</c><00:20:57.640><c> what</c><00:20:58.640><c> it</c><00:20:58.730><c> boils</c><00:20:59.030><c> down</c>

00:20:59.080 --> 00:20:59.090 align:start position:0%
layers are connected what it boils down
 

00:20:59.090 --> 00:21:01.690 align:start position:0%
layers are connected what it boils down
to<00:20:59.270><c> is</c><00:20:59.720><c> the</c><00:20:59.930><c> same</c><00:21:00.170><c> idea</c><00:21:00.590><c> of</c><00:21:00.650><c> applying</c><00:21:01.040><c> a</c><00:21:01.370><c> window</c>

00:21:01.690 --> 00:21:01.700 align:start position:0%
to is the same idea of applying a window
 

00:21:01.700 --> 00:21:03.760 align:start position:0%
to is the same idea of applying a window
of<00:21:01.820><c> weights</c><00:21:02.320><c> computing</c><00:21:03.320><c> the</c><00:21:03.440><c> linear</c>

00:21:03.760 --> 00:21:03.770 align:start position:0%
of weights computing the linear
 

00:21:03.770 --> 00:21:06.610 align:start position:0%
of weights computing the linear
combination<00:21:04.520><c> of</c><00:21:04.730><c> those</c><00:21:05.570><c> weights</c><00:21:05.840><c> against</c><00:21:06.530><c> the</c>

00:21:06.610 --> 00:21:06.620 align:start position:0%
combination of those weights against the
 

00:21:06.620 --> 00:21:09.670 align:start position:0%
combination of those weights against the
input<00:21:06.980><c> and</c><00:21:07.190><c> then</c><00:21:07.700><c> activating</c><00:21:08.480><c> with</c><00:21:08.690><c> non</c><00:21:09.140><c> with</c>

00:21:09.670 --> 00:21:09.680 align:start position:0%
input and then activating with non with
 

00:21:09.680 --> 00:21:11.500 align:start position:0%
input and then activating with non with
a<00:21:09.710><c> nonlinear</c><00:21:10.340><c> activation</c><00:21:10.790><c> function</c><00:21:11.330><c> after</c>

00:21:11.500 --> 00:21:11.510 align:start position:0%
a nonlinear activation function after
 

00:21:11.510 --> 00:21:15.940 align:start position:0%
a nonlinear activation function after
applying<00:21:12.350><c> a</c><00:21:12.590><c> bias</c><00:21:14.440><c> another</c><00:21:15.440><c> thing</c><00:21:15.710><c> we</c><00:21:15.830><c> can</c>

00:21:15.940 --> 00:21:15.950 align:start position:0%
applying a bias another thing we can
 

00:21:15.950 --> 00:21:18.580 align:start position:0%
applying a bias another thing we can
think<00:21:16.010><c> about</c><00:21:16.280><c> is</c><00:21:16.790><c> the</c><00:21:17.750><c> fact</c><00:21:17.780><c> that</c><00:21:18.080><c> within</c><00:21:18.500><c> a</c>

00:21:18.580 --> 00:21:18.590 align:start position:0%
think about is the fact that within a
 

00:21:18.590 --> 00:21:20.830 align:start position:0%
think about is the fact that within a
single<00:21:18.800><c> convolutional</c><00:21:19.670><c> layer</c><00:21:19.940><c> we</c><00:21:20.690><c> can</c>

00:21:20.830 --> 00:21:20.840 align:start position:0%
single convolutional layer we can
 

00:21:20.840 --> 00:21:22.930 align:start position:0%
single convolutional layer we can
actually<00:21:21.170><c> have</c><00:21:21.530><c> many</c><00:21:21.830><c> different</c><00:21:22.400><c> filters</c>

00:21:22.930 --> 00:21:22.940 align:start position:0%
actually have many different filters
 

00:21:22.940 --> 00:21:24.940 align:start position:0%
actually have many different filters
that<00:21:22.970><c> we</c><00:21:23.300><c> are</c><00:21:23.420><c> learning</c><00:21:23.660><c> different</c><00:21:24.410><c> sets</c><00:21:24.830><c> of</c>

00:21:24.940 --> 00:21:24.950 align:start position:0%
that we are learning different sets of
 

00:21:24.950 --> 00:21:27.790 align:start position:0%
that we are learning different sets of
weights<00:21:25.220><c> to</c><00:21:25.820><c> be</c><00:21:25.850><c> able</c><00:21:26.150><c> to</c><00:21:26.450><c> extract</c><00:21:26.930><c> different</c>

00:21:27.790 --> 00:21:27.800 align:start position:0%
weights to be able to extract different
 

00:21:27.800 --> 00:21:31.150 align:start position:0%
weights to be able to extract different
features<00:21:28.250><c> and</c><00:21:28.520><c> so</c><00:21:29.330><c> the</c><00:21:29.480><c> output</c><00:21:29.720><c> layer</c><00:21:30.140><c> after</c><00:21:30.530><c> a</c>

00:21:31.150 --> 00:21:31.160 align:start position:0%
features and so the output layer after a
 

00:21:31.160 --> 00:21:33.550 align:start position:0%
features and so the output layer after a
convolution<00:21:31.850><c> operation</c><00:21:32.000><c> will</c><00:21:32.630><c> have</c><00:21:32.840><c> a</c><00:21:32.870><c> volume</c>

00:21:33.550 --> 00:21:33.560 align:start position:0%
convolution operation will have a volume
 

00:21:33.560 --> 00:21:38.320 align:start position:0%
convolution operation will have a volume
where<00:21:34.400><c> the</c><00:21:34.760><c> height</c><00:21:35.090><c> and</c><00:21:36.100><c> the</c><00:21:37.100><c> width</c><00:21:37.310><c> are</c><00:21:37.610><c> the</c>

00:21:38.320 --> 00:21:38.330 align:start position:0%
where the height and the width are the
 

00:21:38.330 --> 00:21:41.800 align:start position:0%
where the height and the width are the
spatial<00:21:38.840><c> dimensions</c><00:21:39.610><c> dependent</c><00:21:40.610><c> on</c><00:21:40.810><c> the</c>

00:21:41.800 --> 00:21:41.810 align:start position:0%
spatial dimensions dependent on the
 

00:21:41.810 --> 00:21:45.810 align:start position:0%
spatial dimensions dependent on the
input<00:21:41.990><c> layer</c><00:21:42.350><c> so</c><00:21:42.680><c> if</c><00:21:42.860><c> we</c><00:21:43.040><c> had</c><00:21:43.250><c> say</c><00:21:43.880><c> a</c><00:21:44.120><c> 40</c><00:21:44.660><c> by</c><00:21:44.840><c> 40</c>

00:21:45.810 --> 00:21:45.820 align:start position:0%
input layer so if we had say a 40 by 40
 

00:21:45.820 --> 00:21:48.880 align:start position:0%
input layer so if we had say a 40 by 40
input<00:21:46.820><c> image</c><00:21:47.180><c> the</c><00:21:47.840><c> width</c><00:21:48.110><c> and</c><00:21:48.320><c> height</c><00:21:48.380><c> would</c>

00:21:48.880 --> 00:21:48.890 align:start position:0%
input image the width and height would
 

00:21:48.890 --> 00:21:52.950 align:start position:0%
input image the width and height would
be<00:21:48.950><c> 40</c><00:21:49.550><c> by</c><00:21:49.700><c> 40</c><00:21:50.620><c> assuming</c><00:21:51.620><c> that</c><00:21:51.800><c> you</c><00:21:52.220><c> know</c><00:21:52.250><c> the</c>

00:21:52.950 --> 00:21:52.960 align:start position:0%
be 40 by 40 assuming that you know the
 

00:21:52.960 --> 00:21:55.660 align:start position:0%
be 40 by 40 assuming that you know the
dimensionality<00:21:54.190><c> scales</c><00:21:55.190><c> after</c><00:21:55.580><c> the</c>

00:21:55.660 --> 00:21:55.670 align:start position:0%
dimensionality scales after the
 

00:21:55.670 --> 00:22:01.230 align:start position:0%
dimensionality scales after the
operation<00:21:56.270><c> and</c><00:21:58.210><c> and</c><00:21:59.210><c> these</c><00:21:59.810><c> dimensions</c><00:22:00.440><c> are</c>

00:22:01.230 --> 00:22:01.240 align:start position:0%
operation and and these dimensions are
 

00:22:01.240 --> 00:22:04.810 align:start position:0%
operation and and these dimensions are
dependent<00:22:02.240><c> on</c><00:22:02.740><c> the</c><00:22:03.740><c> size</c><00:22:04.040><c> of</c><00:22:04.250><c> the</c><00:22:04.340><c> filter</c><00:22:04.670><c> as</c>

00:22:04.810 --> 00:22:04.820 align:start position:0%
dependent on the size of the filter as
 

00:22:04.820 --> 00:22:06.160 align:start position:0%
dependent on the size of the filter as
well<00:22:04.850><c> as</c><00:22:05.330><c> the</c>

00:22:06.160 --> 00:22:06.170 align:start position:0%
well as the
 

00:22:06.170 --> 00:22:08.920 align:start position:0%
well as the
degree<00:22:06.680><c> to</c><00:22:06.890><c> which</c><00:22:07.040><c> we're</c><00:22:07.370><c> sliding</c><00:22:08.270><c> it</c><00:22:08.450><c> over</c>

00:22:08.920 --> 00:22:08.930 align:start position:0%
degree to which we're sliding it over
 

00:22:08.930 --> 00:22:12.820 align:start position:0%
degree to which we're sliding it over
the<00:22:09.170><c> over</c><00:22:09.770><c> the</c><00:22:09.890><c> input</c><00:22:10.040><c> layer</c><00:22:11.650><c> finally</c><00:22:12.650><c> the</c>

00:22:12.820 --> 00:22:12.830 align:start position:0%
the over the input layer finally the
 

00:22:12.830 --> 00:22:15.310 align:start position:0%
the over the input layer finally the
depth<00:22:13.070><c> is</c><00:22:13.450><c> defined</c><00:22:14.450><c> by</c><00:22:14.660><c> the</c><00:22:14.720><c> number</c><00:22:15.110><c> of</c>

00:22:15.310 --> 00:22:15.320 align:start position:0%
depth is defined by the number of
 

00:22:15.320 --> 00:22:19.320 align:start position:0%
depth is defined by the number of
different<00:22:15.890><c> filters</c><00:22:16.280><c> that</c><00:22:16.310><c> we</c><00:22:16.940><c> we</c><00:22:17.540><c> are</c><00:22:17.630><c> using</c>

00:22:19.320 --> 00:22:19.330 align:start position:0%
different filters that we we are using
 

00:22:19.330 --> 00:22:22.420 align:start position:0%
different filters that we we are using
the<00:22:20.330><c> last</c><00:22:21.050><c> key</c><00:22:21.380><c> thing</c><00:22:21.620><c> that</c><00:22:21.770><c> I</c><00:22:21.800><c> would</c><00:22:21.920><c> like</c><00:22:22.190><c> to</c>

00:22:22.420 --> 00:22:22.430 align:start position:0%
the last key thing that I would like to
 

00:22:22.430 --> 00:22:24.730 align:start position:0%
the last key thing that I would like to
like<00:22:23.420><c> for</c><00:22:23.660><c> you</c><00:22:23.720><c> to</c><00:22:23.870><c> keep</c><00:22:24.020><c> in</c><00:22:24.170><c> mind</c><00:22:24.200><c> is</c><00:22:24.590><c> this</c>

00:22:24.730 --> 00:22:24.740 align:start position:0%
like for you to keep in mind is this
 

00:22:24.740 --> 00:22:27.850 align:start position:0%
like for you to keep in mind is this
notion<00:22:25.040><c> of</c><00:22:25.400><c> the</c><00:22:25.880><c> receptive</c><00:22:26.420><c> field</c><00:22:26.750><c> which</c><00:22:27.560><c> is</c>

00:22:27.850 --> 00:22:27.860 align:start position:0%
notion of the receptive field which is
 

00:22:27.860 --> 00:22:30.190 align:start position:0%
notion of the receptive field which is
essentially<00:22:28.670><c> a</c><00:22:28.700><c> term</c><00:22:29.030><c> to</c><00:22:29.330><c> describe</c><00:22:29.540><c> the</c><00:22:29.960><c> fact</c>

00:22:30.190 --> 00:22:30.200 align:start position:0%
essentially a term to describe the fact
 

00:22:30.200 --> 00:22:35.560 align:start position:0%
essentially a term to describe the fact
that<00:22:30.640><c> locations</c><00:22:31.640><c> and</c><00:22:31.850><c> input</c><00:22:32.210><c> layers</c><00:22:33.250><c> and</c><00:22:34.570><c> are</c>

00:22:35.560 --> 00:22:35.570 align:start position:0%
that locations and input layers and are
 

00:22:35.570 --> 00:22:40.120 align:start position:0%
that locations and input layers and are
connected<00:22:36.320><c> to</c><00:22:36.560><c> excuse</c><00:22:37.490><c> me</c><00:22:37.610><c> a</c><00:22:38.470><c> neuron</c><00:22:39.470><c> in</c><00:22:39.770><c> a</c>

00:22:40.120 --> 00:22:40.130 align:start position:0%
connected to excuse me a neuron in a
 

00:22:40.130 --> 00:22:42.670 align:start position:0%
connected to excuse me a neuron in a
downstream<00:22:41.030><c> layer</c><00:22:41.270><c> is</c><00:22:41.540><c> only</c><00:22:41.840><c> connected</c><00:22:42.470><c> to</c><00:22:42.650><c> a</c>

00:22:42.670 --> 00:22:42.680 align:start position:0%
downstream layer is only connected to a
 

00:22:42.680 --> 00:22:45.900 align:start position:0%
downstream layer is only connected to a
particular<00:22:43.600><c> location</c><00:22:44.600><c> in</c><00:22:44.900><c> its</c><00:22:45.230><c> in</c><00:22:45.500><c> its</c>

00:22:45.900 --> 00:22:45.910 align:start position:0%
particular location in its in its
 

00:22:45.910 --> 00:22:48.490 align:start position:0%
particular location in its in its
respective<00:22:46.910><c> input</c><00:22:47.300><c> layer</c><00:22:47.480><c> and</c><00:22:47.510><c> that</c><00:22:48.170><c> is</c>

00:22:48.490 --> 00:22:48.500 align:start position:0%
respective input layer and that is
 

00:22:48.500 --> 00:22:54.610 align:start position:0%
respective input layer and that is
termed<00:22:48.860><c> its</c><00:22:49.160><c> receptive</c><00:22:49.790><c> field</c><00:22:51.850><c> okay</c><00:22:53.080><c> so</c><00:22:54.080><c> this</c>

00:22:54.610 --> 00:22:54.620 align:start position:0%
termed its receptive field okay so this
 

00:22:54.620 --> 00:22:59.200 align:start position:0%
termed its receptive field okay so this
kind<00:22:54.920><c> of</c><00:22:55.070><c> at</c><00:22:55.550><c> a</c><00:22:55.730><c> high</c><00:22:55.940><c> level</c><00:22:57.370><c> explains</c><00:22:58.370><c> sort</c><00:22:59.120><c> of</c>

00:22:59.200 --> 00:22:59.210 align:start position:0%
kind of at a high level explains sort of
 

00:22:59.210 --> 00:23:01.360 align:start position:0%
kind of at a high level explains sort of
how<00:22:59.390><c> these</c><00:22:59.600><c> convolutional</c><00:23:00.380><c> operations</c><00:23:01.070><c> work</c>

00:23:01.360 --> 00:23:01.370 align:start position:0%
how these convolutional operations work
 

00:23:01.370 --> 00:23:04.690 align:start position:0%
how these convolutional operations work
in<00:23:01.670><c> within</c><00:23:02.120><c> convolutional</c><00:23:02.960><c> layers</c><00:23:03.400><c> the</c><00:23:04.400><c> next</c>

00:23:04.690 --> 00:23:04.700 align:start position:0%
in within convolutional layers the next
 

00:23:04.700 --> 00:23:06.670 align:start position:0%
in within convolutional layers the next
step<00:23:04.880><c> that</c><00:23:04.910><c> I</c><00:23:05.150><c> mentioned</c><00:23:05.600><c> is</c><00:23:05.810><c> applying</c><00:23:06.350><c> a</c>

00:23:06.670 --> 00:23:06.680 align:start position:0%
step that I mentioned is applying a
 

00:23:06.680 --> 00:23:09.460 align:start position:0%
step that I mentioned is applying a
non-linearity<00:23:07.280><c> to</c><00:23:07.910><c> the</c><00:23:08.420><c> output</c><00:23:08.930><c> of</c><00:23:09.080><c> a</c>

00:23:09.460 --> 00:23:09.470 align:start position:0%
non-linearity to the output of a
 

00:23:09.470 --> 00:23:13.060 align:start position:0%
non-linearity to the output of a
convolutional<00:23:10.160><c> layer</c><00:23:10.400><c> and</c><00:23:11.500><c> exactly</c><00:23:12.500><c> the</c><00:23:13.040><c> same</c>

00:23:13.060 --> 00:23:13.070 align:start position:0%
convolutional layer and exactly the same
 

00:23:13.070 --> 00:23:16.060 align:start position:0%
convolutional layer and exactly the same
concept<00:23:13.550><c> as</c><00:23:13.970><c> the</c><00:23:14.420><c> first</c><00:23:14.630><c> lecture</c><00:23:15.050><c> we</c><00:23:15.200><c> do</c><00:23:15.860><c> this</c>

00:23:16.060 --> 00:23:16.070 align:start position:0%
concept as the first lecture we do this
 

00:23:16.070 --> 00:23:18.700 align:start position:0%
concept as the first lecture we do this
because<00:23:16.310><c> image</c><00:23:17.060><c> data</c><00:23:17.330><c> is</c><00:23:17.630><c> highly</c><00:23:18.170><c> nonlinear</c>

00:23:18.700 --> 00:23:18.710 align:start position:0%
because image data is highly nonlinear
 

00:23:18.710 --> 00:23:21.220 align:start position:0%
because image data is highly nonlinear
right<00:23:18.950><c> and</c><00:23:19.250><c> in</c><00:23:19.760><c> CNN's</c><00:23:20.300><c> it's</c><00:23:20.600><c> very</c><00:23:20.840><c> common</c>

00:23:21.220 --> 00:23:21.230 align:start position:0%
right and in CNN's it's very common
 

00:23:21.230 --> 00:23:24.160 align:start position:0%
right and in CNN's it's very common
practice<00:23:21.440><c> to</c><00:23:21.920><c> apply</c><00:23:22.780><c> nonlinearities</c><00:23:23.780><c> after</c>

00:23:24.160 --> 00:23:24.170 align:start position:0%
practice to apply nonlinearities after
 

00:23:24.170 --> 00:23:26.650 align:start position:0%
practice to apply nonlinearities after
every<00:23:24.560><c> convolution</c><00:23:25.250><c> operation</c><00:23:25.510><c> that</c><00:23:26.510><c> is</c>

00:23:26.650 --> 00:23:26.660 align:start position:0%
every convolution operation that is
 

00:23:26.660 --> 00:23:29.740 align:start position:0%
every convolution operation that is
after<00:23:27.050><c> each</c><00:23:27.410><c> convolutional</c><00:23:28.190><c> layer</c><00:23:28.430><c> and</c><00:23:28.820><c> the</c>

00:23:29.740 --> 00:23:29.750 align:start position:0%
after each convolutional layer and the
 

00:23:29.750 --> 00:23:32.290 align:start position:0%
after each convolutional layer and the
most<00:23:29.960><c> common</c><00:23:30.440><c> activation</c><00:23:31.280><c> function</c><00:23:31.700><c> that</c><00:23:32.150><c> is</c>

00:23:32.290 --> 00:23:32.300 align:start position:0%
most common activation function that is
 

00:23:32.300 --> 00:23:35.380 align:start position:0%
most common activation function that is
used<00:23:32.330><c> is</c><00:23:32.870><c> called</c><00:23:33.410><c> the</c><00:23:33.620><c> relu</c><00:23:33.950><c> function</c><00:23:34.670><c> which</c>

00:23:35.380 --> 00:23:35.390 align:start position:0%
used is called the relu function which
 

00:23:35.390 --> 00:23:36.900 align:start position:0%
used is called the relu function which
is<00:23:35.630><c> essentially</c><00:23:36.230><c> a</c><00:23:36.260><c> pixel</c><00:23:36.740><c> by</c><00:23:36.860><c> pixel</c>

00:23:36.900 --> 00:23:36.910 align:start position:0%
is essentially a pixel by pixel
 

00:23:36.910 --> 00:23:40.060 align:start position:0%
is essentially a pixel by pixel
operation<00:23:37.910><c> that</c><00:23:38.660><c> reply</c><00:23:39.110><c> replaces</c><00:23:39.830><c> all</c>

00:23:40.060 --> 00:23:40.070 align:start position:0%
operation that reply replaces all
 

00:23:40.070 --> 00:23:42.520 align:start position:0%
operation that reply replaces all
negative<00:23:40.550><c> values</c><00:23:40.580><c> that</c><00:23:41.300><c> follow</c><00:23:41.720><c> from</c><00:23:42.140><c> a</c>

00:23:42.520 --> 00:23:42.530 align:start position:0%
negative values that follow from a
 

00:23:42.530 --> 00:23:45.130 align:start position:0%
negative values that follow from a
convolution<00:23:43.130><c> with</c><00:23:43.280><c> zero</c><00:23:43.820><c> and</c><00:23:44.180><c> you</c><00:23:44.750><c> can</c><00:23:44.900><c> think</c>

00:23:45.130 --> 00:23:45.140 align:start position:0%
convolution with zero and you can think
 

00:23:45.140 --> 00:23:50.190 align:start position:0%
convolution with zero and you can think
of<00:23:45.230><c> this</c><00:23:45.350><c> as</c><00:23:45.410><c> sort</c><00:23:45.740><c> of</c><00:23:45.860><c> a</c><00:23:45.980><c> threshold</c><00:23:49.120><c> incurring</c>

00:23:50.190 --> 00:23:50.200 align:start position:0%
of this as sort of a threshold incurring
 

00:23:50.200 --> 00:23:53.380 align:start position:0%
of this as sort of a threshold incurring
indicates<00:23:51.200><c> sort</c><00:23:51.440><c> of</c><00:23:51.530><c> negative</c><00:23:52.460><c> direction</c><00:23:53.150><c> of</c>

00:23:53.380 --> 00:23:53.390 align:start position:0%
indicates sort of negative direction of
 

00:23:53.390 --> 00:23:58.090 align:start position:0%
indicates sort of negative direction of
a<00:23:54.460><c> negative</c><00:23:55.460><c> detection</c><00:23:56.090><c> of</c><00:23:56.210><c> that</c><00:23:57.100><c> associated</c>

00:23:58.090 --> 00:23:58.100 align:start position:0%
a negative detection of that associated
 

00:23:58.100 --> 00:24:03.910 align:start position:0%
a negative detection of that associated
feature<00:24:00.940><c> the</c><00:24:01.940><c> final</c><00:24:02.330><c> key</c><00:24:02.510><c> operation</c><00:24:03.110><c> in</c><00:24:03.320><c> cnn's</c>

00:24:03.910 --> 00:24:03.920 align:start position:0%
feature the final key operation in cnn's
 

00:24:03.920 --> 00:24:07.540 align:start position:0%
feature the final key operation in cnn's
is<00:24:04.190><c> pooling</c><00:24:04.550><c> and</c><00:24:04.850><c> pooling</c><00:24:05.510><c> is</c><00:24:05.780><c> a</c><00:24:06.550><c> operation</c>

00:24:07.540 --> 00:24:07.550 align:start position:0%
is pooling and pooling is a operation
 

00:24:07.550 --> 00:24:10.330 align:start position:0%
is pooling and pooling is a operation
that's<00:24:07.730><c> used</c><00:24:08.090><c> to</c><00:24:08.420><c> reduce</c><00:24:09.130><c> dimensionality</c><00:24:10.130><c> and</c>

00:24:10.330 --> 00:24:10.340 align:start position:0%
that's used to reduce dimensionality and
 

00:24:10.340 --> 00:24:13.210 align:start position:0%
that's used to reduce dimensionality and
to<00:24:10.850><c> preserve</c><00:24:11.120><c> spatial</c><00:24:11.990><c> invariants</c><00:24:12.590><c> and</c><00:24:12.740><c> a</c>

00:24:13.210 --> 00:24:13.220 align:start position:0%
to preserve spatial invariants and a
 

00:24:13.220 --> 00:24:16.840 align:start position:0%
to preserve spatial invariants and a
common<00:24:13.640><c> technique</c><00:24:14.000><c> is</c><00:24:14.300><c> called</c><00:24:15.050><c> max</c><00:24:15.380><c> pooling</c>

00:24:16.840 --> 00:24:16.850 align:start position:0%
common technique is called max pooling
 

00:24:16.850 --> 00:24:18.910 align:start position:0%
common technique is called max pooling
it's<00:24:17.540><c> shown</c><00:24:17.840><c> in</c><00:24:17.870><c> this</c><00:24:18.110><c> example</c><00:24:18.320><c> and</c><00:24:18.800><c> it's</c>

00:24:18.910 --> 00:24:18.920 align:start position:0%
it's shown in this example and it's
 

00:24:18.920 --> 00:24:21.220 align:start position:0%
it's shown in this example and it's
exactly<00:24:19.520><c> what</c><00:24:19.760><c> it</c><00:24:19.880><c> sounds</c><00:24:20.090><c> you</c><00:24:20.540><c> simply</c><00:24:20.960><c> take</c>

00:24:21.220 --> 00:24:21.230 align:start position:0%
exactly what it sounds you simply take
 

00:24:21.230 --> 00:24:24.010 align:start position:0%
exactly what it sounds you simply take
the<00:24:21.470><c> maximum</c><00:24:21.830><c> value</c><00:24:22.010><c> in</c><00:24:22.520><c> a</c><00:24:22.640><c> patch</c><00:24:22.880><c> in</c><00:24:23.270><c> this</c>

00:24:24.010 --> 00:24:24.020 align:start position:0%
the maximum value in a patch in this
 

00:24:24.020 --> 00:24:28.360 align:start position:0%
the maximum value in a patch in this
case<00:24:24.230><c> a</c><00:24:24.260><c> 2x2</c><00:24:24.800><c> patch</c><00:24:26.440><c> that</c><00:24:27.440><c> is</c><00:24:27.740><c> is</c><00:24:28.100><c> being</c>

00:24:28.360 --> 00:24:28.370 align:start position:0%
case a 2x2 patch that is is being
 

00:24:28.370 --> 00:24:31.410 align:start position:0%
case a 2x2 patch that is is being
applied<00:24:28.700><c> with</c><00:24:29.060><c> a</c><00:24:29.090><c> stride</c><00:24:29.450><c> of</c><00:24:29.690><c> 2</c><00:24:30.350><c> over</c><00:24:31.130><c> this</c>

00:24:31.410 --> 00:24:31.420 align:start position:0%
applied with a stride of 2 over this
 

00:24:31.420 --> 00:24:36.220 align:start position:0%
applied with a stride of 2 over this
over<00:24:32.420><c> this</c><00:24:32.540><c> array</c><00:24:33.100><c> and</c><00:24:34.120><c> the</c><00:24:35.120><c> key</c><00:24:35.360><c> idea</c><00:24:35.810><c> here</c><00:24:35.870><c> is</c>

00:24:36.220 --> 00:24:36.230 align:start position:0%
over this array and the key idea here is
 

00:24:36.230 --> 00:24:38.170 align:start position:0%
over this array and the key idea here is
that<00:24:36.260><c> we're</c><00:24:36.860><c> reducing</c><00:24:37.190><c> the</c><00:24:37.460><c> dimensionality</c>

00:24:38.170 --> 00:24:38.180 align:start position:0%
that we're reducing the dimensionality
 

00:24:38.180 --> 00:24:40.480 align:start position:0%
that we're reducing the dimensionality
going<00:24:38.870><c> from</c><00:24:39.080><c> one</c><00:24:39.350><c> layer</c><00:24:39.590><c> to</c><00:24:39.620><c> the</c><00:24:39.950><c> next</c><00:24:40.280><c> and</c><00:24:40.430><c> I</c>

00:24:40.480 --> 00:24:40.490 align:start position:0%
going from one layer to the next and I
 

00:24:40.490 --> 00:24:42.460 align:start position:0%
going from one layer to the next and I
encourage<00:24:40.910><c> you</c><00:24:40.940><c> to</c><00:24:41.210><c> think</c><00:24:41.240><c> about</c><00:24:41.570><c> other</c><00:24:42.080><c> ways</c>

00:24:42.460 --> 00:24:42.470 align:start position:0%
encourage you to think about other ways
 

00:24:42.470 --> 00:24:45.100 align:start position:0%
encourage you to think about other ways
in<00:24:42.830><c> which</c><00:24:42.860><c> we</c><00:24:43.040><c> can</c><00:24:43.460><c> perform</c><00:24:43.940><c> this</c><00:24:44.750><c> sort</c><00:24:44.990><c> of</c>

00:24:45.100 --> 00:24:45.110 align:start position:0%
in which we can perform this sort of
 

00:24:45.110 --> 00:24:49.090 align:start position:0%
in which we can perform this sort of
down<00:24:45.470><c> sampling</c><00:24:45.920><c> operation</c><00:24:47.590><c> so</c><00:24:48.590><c> these</c><00:24:48.770><c> are</c><00:24:48.950><c> the</c>

00:24:49.090 --> 00:24:49.100 align:start position:0%
down sampling operation so these are the
 

00:24:49.100 --> 00:24:51.490 align:start position:0%
down sampling operation so these are the
three<00:24:49.340><c> key</c><00:24:49.670><c> operations</c><00:24:50.330><c> to</c><00:24:50.840><c> cnn's</c>

00:24:51.490 --> 00:24:51.500 align:start position:0%
three key operations to cnn's
 

00:24:51.500 --> 00:24:53.770 align:start position:0%
three key operations to cnn's
and<00:24:51.620><c> we're</c><00:24:52.250><c> now</c><00:24:52.430><c> ready</c><00:24:52.700><c> to</c><00:24:53.090><c> put</c><00:24:53.450><c> them</c><00:24:53.600><c> together</c>

00:24:53.770 --> 00:24:53.780 align:start position:0%
and we're now ready to put them together
 

00:24:53.780 --> 00:24:55.780 align:start position:0%
and we're now ready to put them together
to<00:24:54.110><c> actually</c><00:24:54.470><c> construct</c><00:24:54.740><c> our</c><00:24:55.040><c> network</c><00:24:55.430><c> and</c>

00:24:55.780 --> 00:24:55.790 align:start position:0%
to actually construct our network and
 

00:24:55.790 --> 00:24:58.660 align:start position:0%
to actually construct our network and
with<00:24:56.540><c> cnn's</c><00:24:57.020><c> the</c><00:24:57.350><c> key</c><00:24:57.560><c> the</c><00:24:57.950><c> key</c><00:24:58.160><c> is</c><00:24:58.370><c> that</c><00:24:58.400><c> we</c>

00:24:58.660 --> 00:24:58.670 align:start position:0%
with cnn's the key the key is that we
 

00:24:58.670 --> 00:25:00.100 align:start position:0%
with cnn's the key the key is that we
can<00:24:58.850><c> layer</c><00:24:59.090><c> these</c><00:24:59.360><c> operations</c>

00:25:00.100 --> 00:25:00.110 align:start position:0%
can layer these operations
 

00:25:00.110 --> 00:25:02.740 align:start position:0%
can layer these operations
hierarchically<00:25:01.070><c> and</c><00:25:01.280><c> by</c><00:25:01.880><c> layering</c><00:25:02.240><c> them</c><00:25:02.570><c> in</c>

00:25:02.740 --> 00:25:02.750 align:start position:0%
hierarchically and by layering them in
 

00:25:02.750 --> 00:25:05.860 align:start position:0%
hierarchically and by layering them in
this<00:25:02.870><c> way</c><00:25:03.140><c> our</c><00:25:03.760><c> network</c><00:25:04.760><c> can</c><00:25:05.210><c> be</c><00:25:05.240><c> trained</c><00:25:05.690><c> to</c>

00:25:05.860 --> 00:25:05.870 align:start position:0%
this way our network can be trained to
 

00:25:05.870 --> 00:25:08.110 align:start position:0%
this way our network can be trained to
learn<00:25:06.080><c> a</c><00:25:06.380><c> hierarchy</c><00:25:07.010><c> of</c><00:25:07.040><c> features</c><00:25:07.460><c> present</c><00:25:07.820><c> in</c>

00:25:08.110 --> 00:25:08.120 align:start position:0%
learn a hierarchy of features present in
 

00:25:08.120 --> 00:25:11.920 align:start position:0%
learn a hierarchy of features present in
the<00:25:08.240><c> image</c><00:25:08.390><c> data</c><00:25:09.490><c> so</c><00:25:10.490><c> CNN</c><00:25:11.390><c> for</c><00:25:11.630><c> image</c>

00:25:11.920 --> 00:25:11.930 align:start position:0%
the image data so CNN for image
 

00:25:11.930 --> 00:25:13.960 align:start position:0%
the image data so CNN for image
classification<00:25:12.170><c> can</c><00:25:12.890><c> be</c><00:25:12.920><c> broken</c><00:25:13.250><c> down</c><00:25:13.610><c> into</c>

00:25:13.960 --> 00:25:13.970 align:start position:0%
classification can be broken down into
 

00:25:13.970 --> 00:25:16.810 align:start position:0%
classification can be broken down into
two<00:25:14.570><c> parts</c><00:25:14.710><c> first</c><00:25:15.710><c> this</c><00:25:16.220><c> feature</c><00:25:16.520><c> learning</c>

00:25:16.810 --> 00:25:16.820 align:start position:0%
two parts first this feature learning
 

00:25:16.820 --> 00:25:20.140 align:start position:0%
two parts first this feature learning
pipeline<00:25:17.480><c> where</c><00:25:18.320><c> we</c><00:25:18.530><c> learn</c><00:25:18.800><c> features</c><00:25:19.430><c> in</c><00:25:19.700><c> our</c>

00:25:20.140 --> 00:25:20.150 align:start position:0%
pipeline where we learn features in our
 

00:25:20.150 --> 00:25:23.130 align:start position:0%
pipeline where we learn features in our
input<00:25:20.900><c> images</c><00:25:21.380><c> through</c><00:25:21.860><c> convolution</c><00:25:22.220><c> and</c>

00:25:23.130 --> 00:25:23.140 align:start position:0%
input images through convolution and
 

00:25:23.140 --> 00:25:27.120 align:start position:0%
input images through convolution and
through<00:25:24.140><c> these</c><00:25:24.470><c> convolutional</c><00:25:25.250><c> layers</c><00:25:25.490><c> and</c>

00:25:27.120 --> 00:25:27.130 align:start position:0%
through these convolutional layers and
 

00:25:27.130 --> 00:25:30.310 align:start position:0%
through these convolutional layers and
finally<00:25:28.130><c> the</c><00:25:28.700><c> the</c><00:25:29.120><c> second</c><00:25:29.480><c> half</c><00:25:29.690><c> is</c><00:25:30.020><c> that</c>

00:25:30.310 --> 00:25:30.320 align:start position:0%
finally the the second half is that
 

00:25:30.320 --> 00:25:32.080 align:start position:0%
finally the the second half is that
these<00:25:30.530><c> convolutional</c><00:25:31.220><c> and</c><00:25:31.430><c> pooling</c><00:25:31.850><c> layers</c>

00:25:32.080 --> 00:25:32.090 align:start position:0%
these convolutional and pooling layers
 

00:25:32.090 --> 00:25:34.900 align:start position:0%
these convolutional and pooling layers
will<00:25:32.780><c> output</c><00:25:33.320><c> high</c><00:25:33.890><c> level</c><00:25:34.280><c> features</c><00:25:34.640><c> that</c><00:25:34.790><c> are</c>

00:25:34.900 --> 00:25:34.910 align:start position:0%
will output high level features that are
 

00:25:34.910 --> 00:25:37.450 align:start position:0%
will output high level features that are
present<00:25:35.390><c> in</c><00:25:35.450><c> the</c><00:25:35.570><c> input</c><00:25:35.900><c> images</c><00:25:36.320><c> and</c><00:25:36.590><c> we</c><00:25:37.310><c> can</c>

00:25:37.450 --> 00:25:37.460 align:start position:0%
present in the input images and we can
 

00:25:37.460 --> 00:25:39.940 align:start position:0%
present in the input images and we can
then<00:25:37.640><c> pass</c><00:25:37.940><c> these</c><00:25:38.300><c> features</c><00:25:38.690><c> on</c><00:25:38.930><c> to</c><00:25:39.680><c> fully</c>

00:25:39.940 --> 00:25:39.950 align:start position:0%
then pass these features on to fully
 

00:25:39.950 --> 00:25:42.460 align:start position:0%
then pass these features on to fully
connected<00:25:40.490><c> layers</c><00:25:40.760><c> to</c><00:25:41.300><c> actually</c><00:25:41.780><c> do</c><00:25:42.290><c> the</c>

00:25:42.460 --> 00:25:42.470 align:start position:0%
connected layers to actually do the
 

00:25:42.470 --> 00:25:45.280 align:start position:0%
connected layers to actually do the
classification<00:25:43.280><c> task</c><00:25:43.550><c> and</c><00:25:44.050><c> these</c><00:25:45.050><c> fully</c>

00:25:45.280 --> 00:25:45.290 align:start position:0%
classification task and these fully
 

00:25:45.290 --> 00:25:48.520 align:start position:0%
classification task and these fully
connected<00:25:45.770><c> layers</c><00:25:46.010><c> can</c><00:25:46.870><c> effectively</c><00:25:47.870><c> output</c>

00:25:48.520 --> 00:25:48.530 align:start position:0%
connected layers can effectively output
 

00:25:48.530 --> 00:25:51.040 align:start position:0%
connected layers can effectively output
a<00:25:48.680><c> probability</c><00:25:49.280><c> distribution</c><00:25:49.900><c> for</c><00:25:50.900><c> the</c>

00:25:51.040 --> 00:25:51.050 align:start position:0%
a probability distribution for the
 

00:25:51.050 --> 00:25:54.070 align:start position:0%
a probability distribution for the
images<00:25:51.560><c> membership</c><00:25:52.400><c> over</c><00:25:52.610><c> a</c><00:25:52.760><c> set</c><00:25:53.030><c> of</c><00:25:53.270><c> possible</c>

00:25:54.070 --> 00:25:54.080 align:start position:0%
images membership over a set of possible
 

00:25:54.080 --> 00:25:56.980 align:start position:0%
images membership over a set of possible
classes<00:25:54.350><c> and</c><00:25:54.820><c> a</c><00:25:55.820><c> common</c><00:25:56.210><c> way</c><00:25:56.390><c> that</c><00:25:56.660><c> this</c><00:25:56.810><c> is</c>

00:25:56.980 --> 00:25:56.990 align:start position:0%
classes and a common way that this is
 

00:25:56.990 --> 00:25:59.140 align:start position:0%
classes and a common way that this is
achieved<00:25:57.110><c> is</c><00:25:57.500><c> using</c><00:25:57.890><c> this</c><00:25:58.040><c> function</c><00:25:58.550><c> called</c>

00:25:59.140 --> 00:25:59.150 align:start position:0%
achieved is using this function called
 

00:25:59.150 --> 00:26:03.150 align:start position:0%
achieved is using this function called
the<00:25:59.240><c> softmax</c><00:26:00.310><c> whose</c><00:26:01.310><c> output</c><00:26:01.930><c> represents</c><00:26:02.930><c> a</c>

00:26:03.150 --> 00:26:03.160 align:start position:0%
the softmax whose output represents a
 

00:26:03.160 --> 00:26:06.060 align:start position:0%
the softmax whose output represents a
categorical<00:26:04.390><c> probability</c><00:26:05.390><c> distribution</c>

00:26:06.060 --> 00:26:06.070 align:start position:0%
categorical probability distribution
 

00:26:06.070 --> 00:26:08.770 align:start position:0%
categorical probability distribution
over<00:26:07.070><c> the</c><00:26:07.370><c> set</c><00:26:07.580><c> of</c><00:26:07.850><c> classes</c><00:26:08.300><c> that</c><00:26:08.480><c> you're</c>

00:26:08.770 --> 00:26:08.780 align:start position:0%
over the set of classes that you're
 

00:26:08.780 --> 00:26:13.870 align:start position:0%
over the set of classes that you're
interested<00:26:09.110><c> in</c><00:26:11.170><c> okay</c><00:26:12.170><c> so</c><00:26:12.250><c> the</c><00:26:13.250><c> final</c><00:26:13.520><c> key</c>

00:26:13.870 --> 00:26:13.880 align:start position:0%
interested in okay so the final key
 

00:26:13.880 --> 00:26:16.890 align:start position:0%
interested in okay so the final key
piece<00:26:14.150><c> right</c><00:26:14.630><c> is</c><00:26:14.930><c> how</c><00:26:15.800><c> do</c><00:26:15.860><c> we</c><00:26:16.070><c> train</c><00:26:16.310><c> this</c><00:26:16.580><c> and</c>

00:26:16.890 --> 00:26:16.900 align:start position:0%
piece right is how do we train this and
 

00:26:16.900 --> 00:26:20.230 align:start position:0%
piece right is how do we train this and
it's<00:26:17.900><c> the</c><00:26:18.440><c> same</c><00:26:18.680><c> idea</c><00:26:18.890><c> as</c><00:26:19.160><c> we</c><00:26:19.400><c> introduced</c><00:26:20.060><c> in</c>

00:26:20.230 --> 00:26:20.240 align:start position:0%
it's the same idea as we introduced in
 

00:26:20.240 --> 00:26:23.980 align:start position:0%
it's the same idea as we introduced in
lecture<00:26:20.360><c> 1</c><00:26:21.040><c> back</c><00:26:22.040><c> propagation</c><00:26:22.570><c> the</c><00:26:23.570><c> important</c>

00:26:23.980 --> 00:26:23.990 align:start position:0%
lecture 1 back propagation the important
 

00:26:23.990 --> 00:26:25.780 align:start position:0%
lecture 1 back propagation the important
thing<00:26:24.170><c> to</c><00:26:24.320><c> keep</c><00:26:24.470><c> in</c><00:26:24.620><c> mind</c><00:26:24.920><c> is</c><00:26:25.190><c> what</c><00:26:25.520><c> it</c><00:26:25.640><c> is</c>

00:26:25.780 --> 00:26:25.790 align:start position:0%
thing to keep in mind is what it is
 

00:26:25.790 --> 00:26:30.330 align:start position:0%
thing to keep in mind is what it is
we're<00:26:26.060><c> learning</c><00:26:26.570><c> when</c><00:26:26.750><c> we're</c><00:26:26.930><c> training</c><00:26:27.940><c> SIA</c>

00:26:30.330 --> 00:26:30.340 align:start position:0%
we're learning when we're training SIA
 

00:26:30.340 --> 00:26:34.180 align:start position:0%
we're learning when we're training SIA
what<00:26:31.340><c> we</c><00:26:31.580><c> learn</c><00:26:31.880><c> when</c><00:26:32.240><c> we</c><00:26:32.270><c> train</c><00:26:33.110><c> a</c><00:26:33.170><c> CNN</c><00:26:33.770><c> model</c>

00:26:34.180 --> 00:26:34.190 align:start position:0%
what we learn when we train a CNN model
 

00:26:34.190 --> 00:26:36.550 align:start position:0%
what we learn when we train a CNN model
is<00:26:34.400><c> the</c><00:26:34.760><c> weights</c><00:26:35.000><c> of</c><00:26:35.270><c> those</c><00:26:35.690><c> convolution</c>

00:26:36.550 --> 00:26:36.560 align:start position:0%
is the weights of those convolution
 

00:26:36.560 --> 00:26:41.050 align:start position:0%
is the weights of those convolution
filters<00:26:37.300><c> at</c><00:26:38.800><c> another</c><00:26:39.800><c> degree</c><00:26:40.100><c> of</c><00:26:40.310><c> abstraction</c>

00:26:41.050 --> 00:26:41.060 align:start position:0%
filters at another degree of abstraction
 

00:26:41.060 --> 00:26:43.060 align:start position:0%
filters at another degree of abstraction
right<00:26:41.480><c> you</c><00:26:41.660><c> can</c><00:26:41.810><c> think</c><00:26:42.050><c> of</c><00:26:42.170><c> this</c><00:26:42.320><c> as</c><00:26:42.500><c> what</c>

00:26:43.060 --> 00:26:43.070 align:start position:0%
right you can think of this as what
 

00:26:43.070 --> 00:26:45.430 align:start position:0%
right you can think of this as what
features<00:26:43.520><c> the</c><00:26:43.730><c> network</c><00:26:44.120><c> is</c><00:26:44.360><c> learning</c><00:26:45.110><c> to</c>

00:26:45.430 --> 00:26:45.440 align:start position:0%
features the network is learning to
 

00:26:45.440 --> 00:26:48.760 align:start position:0%
features the network is learning to
detect<00:26:45.830><c> in</c><00:26:46.960><c> addition</c><00:26:47.960><c> we</c><00:26:48.110><c> also</c><00:26:48.350><c> learn</c><00:26:48.740><c> the</c>

00:26:48.760 --> 00:26:48.770 align:start position:0%
detect in addition we also learn the
 

00:26:48.770 --> 00:26:50.830 align:start position:0%
detect in addition we also learn the
weights<00:26:49.160><c> for</c><00:26:49.790><c> the</c><00:26:49.940><c> fully</c><00:26:50.120><c> connected</c><00:26:50.600><c> layers</c>

00:26:50.830 --> 00:26:50.840 align:start position:0%
weights for the fully connected layers
 

00:26:50.840 --> 00:26:53.290 align:start position:0%
weights for the fully connected layers
if<00:26:51.110><c> we're</c><00:26:51.530><c> performing</c><00:26:51.920><c> a</c><00:26:52.520><c> classification</c>

00:26:53.290 --> 00:26:53.300 align:start position:0%
if we're performing a classification
 

00:26:53.300 --> 00:26:57.730 align:start position:0%
if we're performing a classification
task<00:26:53.810><c> and</c><00:26:55.180><c> since</c><00:26:56.180><c> our</c><00:26:56.510><c> output</c><00:26:57.050><c> in</c><00:26:57.290><c> this</c><00:26:57.500><c> case</c>

00:26:57.730 --> 00:26:57.740 align:start position:0%
task and since our output in this case
 

00:26:57.740 --> 00:26:59.740 align:start position:0%
task and since our output in this case
is<00:26:58.070><c> going</c><00:26:58.610><c> to</c><00:26:58.700><c> be</c><00:26:58.910><c> a</c><00:26:59.150><c> probability</c>

00:26:59.740 --> 00:26:59.750 align:start position:0%
is going to be a probability
 

00:26:59.750 --> 00:27:02.380 align:start position:0%
is going to be a probability
distribution<00:27:00.340><c> we</c><00:27:01.340><c> can</c><00:27:01.520><c> use</c><00:27:01.760><c> that</c><00:27:01.970><c> cross</c>

00:27:02.380 --> 00:27:02.390 align:start position:0%
distribution we can use that cross
 

00:27:02.390 --> 00:27:04.240 align:start position:0%
distribution we can use that cross
entropy<00:27:02.690><c> loss</c><00:27:03.200><c> that</c><00:27:03.410><c> was</c><00:27:03.530><c> introduced</c><00:27:04.070><c> in</c>

00:27:04.240 --> 00:27:04.250 align:start position:0%
entropy loss that was introduced in
 

00:27:04.250 --> 00:27:07.180 align:start position:0%
entropy loss that was introduced in
lecture<00:27:04.400><c> 1</c><00:27:04.850><c> to</c><00:27:05.540><c> optimize</c><00:27:06.140><c> via</c><00:27:06.770><c> back</c>

00:27:07.180 --> 00:27:07.190 align:start position:0%
lecture 1 to optimize via back
 

00:27:07.190 --> 00:27:12.300 align:start position:0%
lecture 1 to optimize via back
propagation<00:27:09.730><c> so</c><00:27:10.730><c> arguably</c><00:27:11.480><c> the</c><00:27:11.780><c> most</c><00:27:11.990><c> famous</c>

00:27:12.300 --> 00:27:12.310 align:start position:0%
propagation so arguably the most famous
 

00:27:12.310 --> 00:27:16.660 align:start position:0%
propagation so arguably the most famous
example<00:27:13.310><c> of</c><00:27:13.520><c> cnn's</c><00:27:14.360><c> and</c><00:27:15.070><c> for</c><00:27:16.070><c> cnn's</c><00:27:16.460><c> for</c>

00:27:16.660 --> 00:27:16.670 align:start position:0%
example of cnn's and for cnn's for
 

00:27:16.670 --> 00:27:18.460 align:start position:0%
example of cnn's and for cnn's for
classification<00:27:17.090><c> and</c><00:27:17.540><c> maybe</c><00:27:17.750><c> cnn's</c><00:27:18.350><c> in</c>

00:27:18.460 --> 00:27:18.470 align:start position:0%
classification and maybe cnn's in
 

00:27:18.470 --> 00:27:21.760 align:start position:0%
classification and maybe cnn's in
general<00:27:19.280><c> is</c><00:27:19.430><c> those</c><00:27:20.210><c> trained</c><00:27:20.900><c> and</c><00:27:21.200><c> tested</c><00:27:21.410><c> on</c>

00:27:21.760 --> 00:27:21.770 align:start position:0%
general is those trained and tested on
 

00:27:21.770 --> 00:27:25.150 align:start position:0%
general is those trained and tested on
the<00:27:22.070><c> famous</c><00:27:22.460><c> imagenet</c><00:27:23.390><c> data</c><00:27:23.780><c> set</c><00:27:24.050><c> an</c><00:27:24.350><c> image</c>

00:27:25.150 --> 00:27:25.160 align:start position:0%
the famous imagenet data set an image
 

00:27:25.160 --> 00:27:28.330 align:start position:0%
the famous imagenet data set an image
net<00:27:25.490><c> is</c><00:27:25.790><c> a</c><00:27:26.060><c> massive</c><00:27:26.690><c> data</c><00:27:26.900><c> set</c><00:27:27.170><c> with</c><00:27:27.440><c> over</c><00:27:27.800><c> 14</c>

00:27:28.330 --> 00:27:28.340 align:start position:0%
net is a massive data set with over 14
 

00:27:28.340 --> 00:27:31.660 align:start position:0%
net is a massive data set with over 14
million<00:27:28.580><c> images</c><00:27:29.030><c> across</c><00:27:29.690><c> over</c><00:27:30.670><c> 20,000</c>

00:27:31.660 --> 00:27:31.670 align:start position:0%
million images across over 20,000
 

00:27:31.670 --> 00:27:34.150 align:start position:0%
million images across over 20,000
different<00:27:32.080><c> categories</c><00:27:33.080><c> and</c><00:27:33.290><c> this</c><00:27:33.950><c> was</c>

00:27:34.150 --> 00:27:34.160 align:start position:0%
different categories and this was
 

00:27:34.160 --> 00:27:37.020 align:start position:0%
different categories and this was
created<00:27:34.640><c> and</c><00:27:34.880><c> curated</c><00:27:35.150><c> by</c><00:27:35.480><c> a</c><00:27:35.660><c> lab</c><00:27:36.170><c> at</c><00:27:36.410><c> Stanford</c>

00:27:37.020 --> 00:27:37.030 align:start position:0%
created and curated by a lab at Stanford
 

00:27:37.030 --> 00:27:42.490 align:start position:0%
created and curated by a lab at Stanford
and<00:27:38.470><c> is</c><00:27:39.470><c> really</c><00:27:39.740><c> become</c><00:27:40.490><c> a</c><00:27:40.960><c> very</c><00:27:41.960><c> widely</c><00:27:42.260><c> used</c>

00:27:42.490 --> 00:27:42.500 align:start position:0%
and is really become a very widely used
 

00:27:42.500 --> 00:27:45.910 align:start position:0%
and is really become a very widely used
data<00:27:43.070><c> set</c><00:27:43.370><c> across</c><00:27:44.210><c> a</c><00:27:44.330><c> machine</c><00:27:44.780><c> learning</c><00:27:45.170><c> so</c>

00:27:45.910 --> 00:27:45.920 align:start position:0%
data set across a machine learning so
 

00:27:45.920 --> 00:27:48.310 align:start position:0%
data set across a machine learning so
just<00:27:46.610><c> to</c><00:27:46.730><c> take</c><00:27:46.880><c> an</c><00:27:47.030><c> example</c><00:27:47.330><c> in</c><00:27:47.750><c> image</c><00:27:48.110><c> net</c>

00:27:48.310 --> 00:27:48.320 align:start position:0%
just to take an example in image net
 

00:27:48.320 --> 00:27:51.160 align:start position:0%
just to take an example in image net
there<00:27:48.530><c> are</c><00:27:48.940><c> 1,400</c><00:27:49.940><c> nine</c><00:27:50.120><c> different</c><00:27:50.810><c> pictures</c>

00:27:51.160 --> 00:27:51.170 align:start position:0%
there are 1,400 nine different pictures
 

00:27:51.170 --> 00:27:54.610 align:start position:0%
there are 1,400 nine different pictures
of<00:27:51.440><c> bananas</c><00:27:51.950><c> and</c><00:27:52.540><c> even</c><00:27:53.540><c> better</c><00:27:53.720><c> than</c><00:27:54.020><c> the</c><00:27:54.380><c> size</c>

00:27:54.610 --> 00:27:54.620 align:start position:0%
of bananas and even better than the size
 

00:27:54.620 --> 00:27:57.730 align:start position:0%
of bananas and even better than the size
of<00:27:54.920><c> this</c><00:27:55.700><c> data</c><00:27:55.940><c> set</c><00:27:56.300><c> I</c><00:27:56.540><c> really</c><00:27:57.230><c> appreciated</c>

00:27:57.730 --> 00:27:57.740 align:start position:0%
of this data set I really appreciated
 

00:27:57.740 --> 00:28:01.380 align:start position:0%
of this data set I really appreciated
their<00:27:58.400><c> description</c><00:27:59.060><c> of</c><00:27:59.210><c> what</c><00:27:59.480><c> a</c><00:27:59.780><c> banana</c><00:28:00.080><c> is</c>

00:28:01.380 --> 00:28:01.390 align:start position:0%
their description of what a banana is
 

00:28:01.390 --> 00:28:04.030 align:start position:0%
their description of what a banana is
succinctly<00:28:02.390><c> described</c><00:28:02.900><c> as</c><00:28:03.080><c> an</c><00:28:03.230><c> elongated</c>

00:28:04.030 --> 00:28:04.040 align:start position:0%
succinctly described as an elongated
 

00:28:04.040 --> 00:28:06.280 align:start position:0%
succinctly described as an elongated
crescent-shaped<00:28:04.580><c> yellow</c><00:28:05.330><c> fruit</c><00:28:05.690><c> with</c><00:28:05.960><c> soft</c>

00:28:06.280 --> 00:28:06.290 align:start position:0%
crescent-shaped yellow fruit with soft
 

00:28:06.290 --> 00:28:09.340 align:start position:0%
crescent-shaped yellow fruit with soft
sweet<00:28:06.650><c> /</c><00:28:07.090><c> which</c><00:28:08.090><c> both</c><00:28:08.330><c> gives</c><00:28:08.570><c> a</c><00:28:08.840><c> pretty</c><00:28:09.110><c> good</c>

00:28:09.340 --> 00:28:09.350 align:start position:0%
sweet / which both gives a pretty good
 

00:28:09.350 --> 00:28:11.770 align:start position:0%
sweet / which both gives a pretty good
description<00:28:09.590><c> of</c><00:28:10.100><c> what</c><00:28:10.610><c> a</c><00:28:10.640><c> banana</c><00:28:11.300><c> is</c><00:28:11.510><c> and</c>

00:28:11.770 --> 00:28:11.780 align:start position:0%
description of what a banana is and
 

00:28:11.780 --> 00:28:15.360 align:start position:0%
description of what a banana is and
speaks<00:28:12.110><c> to</c><00:28:12.230><c> its</c><00:28:12.590><c> obvious</c><00:28:13.460><c> deliciousness</c>

00:28:15.360 --> 00:28:15.370 align:start position:0%
speaks to its obvious deliciousness
 

00:28:15.370 --> 00:28:17.860 align:start position:0%
speaks to its obvious deliciousness
so<00:28:16.370><c> the</c><00:28:16.490><c> creators</c><00:28:16.910><c> of</c><00:28:16.940><c> image</c><00:28:17.420><c> net</c><00:28:17.630><c> also</c>

00:28:17.860 --> 00:28:17.870 align:start position:0%
so the creators of image net also
 

00:28:17.870 --> 00:28:20.340 align:start position:0%
so the creators of image net also
created<00:28:18.740><c> a</c><00:28:18.830><c> set</c><00:28:19.070><c> of</c><00:28:19.100><c> visual</c><00:28:20.000><c> recognition</c>

00:28:20.340 --> 00:28:20.350 align:start position:0%
created a set of visual recognition
 

00:28:20.350 --> 00:28:24.070 align:start position:0%
created a set of visual recognition
challenges<00:28:21.350><c> on</c><00:28:22.190><c> their</c><00:28:22.730><c> data</c><00:28:22.910><c> set</c><00:28:23.180><c> and</c><00:28:23.810><c> most</c>

00:28:24.070 --> 00:28:24.080 align:start position:0%
challenges on their data set and most
 

00:28:24.080 --> 00:28:26.110 align:start position:0%
challenges on their data set and most
notably<00:28:24.320><c> the</c><00:28:24.680><c> image</c><00:28:24.950><c> net</c><00:28:25.190><c> classification</c>

00:28:26.110 --> 00:28:26.120 align:start position:0%
notably the image net classification
 

00:28:26.120 --> 00:28:29.140 align:start position:0%
notably the image net classification
task<00:28:26.360><c> which</c><00:28:27.290><c> is</c><00:28:27.650><c> really</c><00:28:28.580><c> simple</c><00:28:28.790><c> it's</c>

00:28:29.140 --> 00:28:29.150 align:start position:0%
task which is really simple it's
 

00:28:29.150 --> 00:28:32.050 align:start position:0%
task which is really simple it's
produced<00:28:29.810><c> a</c><00:28:29.990><c> list</c><00:28:30.230><c> of</c><00:28:30.500><c> the</c><00:28:30.950><c> object</c><00:28:31.520><c> different</c>

00:28:32.050 --> 00:28:32.060 align:start position:0%
produced a list of the object different
 

00:28:32.060 --> 00:28:34.540 align:start position:0%
produced a list of the object different
object<00:28:32.450><c> categories</c><00:28:33.020><c> that</c><00:28:33.560><c> are</c><00:28:33.890><c> present</c><00:28:34.400><c> in</c>

00:28:34.540 --> 00:28:34.550 align:start position:0%
object categories that are present in
 

00:28:34.550 --> 00:28:38.350 align:start position:0%
object categories that are present in
images<00:28:35.150><c> across</c><00:28:35.900><c> a</c><00:28:36.610><c> ground</c><00:28:37.610><c> truth</c><00:28:37.790><c> set</c><00:28:38.180><c> of</c>

00:28:38.350 --> 00:28:38.360 align:start position:0%
images across a ground truth set of
 

00:28:38.360 --> 00:28:40.470 align:start position:0%
images across a ground truth set of
1,000<00:28:39.200><c> different</c><00:28:39.320><c> categories</c>

00:28:40.470 --> 00:28:40.480 align:start position:0%
1,000 different categories
 

00:28:40.480 --> 00:28:42.810 align:start position:0%
1,000 different categories
and<00:28:40.570><c> in</c><00:28:41.230><c> this</c><00:28:41.590><c> competition</c><00:28:41.950><c> they</c><00:28:42.400><c> measured</c>

00:28:42.810 --> 00:28:42.820 align:start position:0%
and in this competition they measured
 

00:28:42.820 --> 00:28:46.500 align:start position:0%
and in this competition they measured
the<00:28:42.940><c> accuracy</c><00:28:43.570><c> of</c><00:28:44.850><c> the</c><00:28:45.850><c> models</c><00:28:46.210><c> that</c><00:28:46.270><c> were</c>

00:28:46.500 --> 00:28:46.510 align:start position:0%
the accuracy of the models that were
 

00:28:46.510 --> 00:28:49.169 align:start position:0%
the accuracy of the models that were
submitted<00:28:46.990><c> in</c><00:28:47.890><c> terms</c><00:28:48.280><c> of</c><00:28:48.490><c> the</c><00:28:48.640><c> rate</c><00:28:48.880><c> at</c><00:28:49.150><c> which</c>

00:28:49.169 --> 00:28:49.179 align:start position:0%
submitted in terms of the rate at which
 

00:28:49.179 --> 00:28:52.169 align:start position:0%
submitted in terms of the rate at which
the<00:28:49.809><c> model</c><00:28:50.049><c> did</c><00:28:50.380><c> not</c><00:28:50.760><c> output</c><00:28:51.760><c> the</c><00:28:51.910><c> correct</c>

00:28:52.169 --> 00:28:52.179 align:start position:0%
the model did not output the correct
 

00:28:52.179 --> 00:28:55.140 align:start position:0%
the model did not output the correct
label<00:28:52.570><c> in</c><00:28:52.840><c> its</c><00:28:53.200><c> top</c><00:28:53.650><c> five</c><00:28:53.919><c> predictions</c><00:28:54.549><c> for</c><00:28:54.880><c> a</c>

00:28:55.140 --> 00:28:55.150 align:start position:0%
label in its top five predictions for a
 

00:28:55.150 --> 00:28:59.880 align:start position:0%
label in its top five predictions for a
particular<00:28:55.900><c> image</c><00:28:56.230><c> and</c><00:28:57.809><c> the</c><00:28:58.809><c> results</c><00:28:59.260><c> of</c><00:28:59.410><c> this</c>

00:28:59.880 --> 00:28:59.890 align:start position:0%
particular image and the results of this
 

00:28:59.890 --> 00:29:02.010 align:start position:0%
particular image and the results of this
image<00:29:00.220><c> net</c><00:29:00.429><c> classification</c><00:29:01.210><c> challenge</c><00:29:01.809><c> are</c>

00:29:02.010 --> 00:29:02.020 align:start position:0%
image net classification challenge are
 

00:29:02.020 --> 00:29:05.610 align:start position:0%
image net classification challenge are
pretty<00:29:02.320><c> astonishing</c><00:29:03.929><c> 2012</c><00:29:04.929><c> was</c><00:29:05.230><c> the</c><00:29:05.350><c> first</c>

00:29:05.610 --> 00:29:05.620 align:start position:0%
pretty astonishing 2012 was the first
 

00:29:05.620 --> 00:29:09.060 align:start position:0%
pretty astonishing 2012 was the first
time<00:29:05.950><c> a</c><00:29:06.190><c> CNN</c><00:29:06.929><c> won</c><00:29:07.929><c> the</c><00:29:08.290><c> challenge</c><00:29:08.740><c> and</c><00:29:08.830><c> this</c>

00:29:09.060 --> 00:29:09.070 align:start position:0%
time a CNN won the challenge and this
 

00:29:09.070 --> 00:29:12.210 align:start position:0%
time a CNN won the challenge and this
was<00:29:09.250><c> the</c><00:29:09.400><c> famous</c><00:29:09.610><c> CNN</c><00:29:10.230><c> called</c><00:29:11.230><c> Alex</c><00:29:11.710><c> net</c><00:29:11.919><c> and</c>

00:29:12.210 --> 00:29:12.220 align:start position:0%
was the famous CNN called Alex net and
 

00:29:12.220 --> 00:29:15.390 align:start position:0%
was the famous CNN called Alex net and
since<00:29:13.000><c> then</c><00:29:13.240><c> the</c><00:29:13.450><c> neural</c><00:29:13.720><c> networks</c><00:29:14.110><c> that</c><00:29:14.890><c> have</c>

00:29:15.390 --> 00:29:15.400 align:start position:0%
since then the neural networks that have
 

00:29:15.400 --> 00:29:17.190 align:start position:0%
since then the neural networks that have
have<00:29:15.640><c> neural</c><00:29:16.330><c> networks</c><00:29:16.630><c> have</c><00:29:16.809><c> completely</c>

00:29:17.190 --> 00:29:17.200 align:start position:0%
have neural networks have completely
 

00:29:17.200 --> 00:29:20.030 align:start position:0%
have neural networks have completely
dominated<00:29:17.950><c> this</c><00:29:18.130><c> competition</c><00:29:18.730><c> and</c><00:29:19.000><c> the</c><00:29:19.480><c> error</c>

00:29:20.030 --> 00:29:20.040 align:start position:0%
dominated this competition and the error
 

00:29:20.040 --> 00:29:22.770 align:start position:0%
dominated this competition and the error
that<00:29:21.040><c> the</c><00:29:21.520><c> state</c><00:29:21.730><c> of</c><00:29:21.880><c> the</c><00:29:21.970><c> art</c><00:29:22.150><c> is</c><00:29:22.360><c> able</c><00:29:22.690><c> to</c>

00:29:22.770 --> 00:29:22.780 align:start position:0%
that the state of the art is able to
 

00:29:22.780 --> 00:29:24.860 align:start position:0%
that the state of the art is able to
achieve<00:29:22.900><c> keeps</c><00:29:23.410><c> decreasing</c><00:29:23.650><c> and</c><00:29:24.100><c> decreasing</c>

00:29:24.860 --> 00:29:24.870 align:start position:0%
achieve keeps decreasing and decreasing
 

00:29:24.870 --> 00:29:29.340 align:start position:0%
achieve keeps decreasing and decreasing
surpassing<00:29:25.870><c> human</c><00:29:26.679><c> error</c><00:29:26.890><c> in</c><00:29:27.450><c> 2015</c><00:29:28.450><c> with</c><00:29:29.049><c> the</c>

00:29:29.340 --> 00:29:29.350 align:start position:0%
surpassing human error in 2015 with the
 

00:29:29.350 --> 00:29:33.440 align:start position:0%
surpassing human error in 2015 with the
famous<00:29:29.740><c> ResNet</c><00:29:30.580><c> Network</c><00:29:31.540><c> which</c><00:29:31.780><c> had</c><00:29:31.990><c> 152</c>

00:29:33.440 --> 00:29:33.450 align:start position:0%
famous ResNet Network which had 152
 

00:29:33.450 --> 00:29:38.240 align:start position:0%
famous ResNet Network which had 152
convolutional<00:29:34.450><c> layers</c><00:29:34.690><c> in</c><00:29:35.500><c> its</c><00:29:36.100><c> design</c><00:29:36.490><c> but</c>

00:29:38.240 --> 00:29:38.250 align:start position:0%
convolutional layers in its design but
 

00:29:38.250 --> 00:29:41.610 align:start position:0%
convolutional layers in its design but
with<00:29:39.250><c> improved</c><00:29:39.760><c> accuracy</c><00:29:40.200><c> the</c><00:29:41.200><c> number</c><00:29:41.500><c> of</c>

00:29:41.610 --> 00:29:41.620 align:start position:0%
with improved accuracy the number of
 

00:29:41.620 --> 00:29:43.650 align:start position:0%
with improved accuracy the number of
layers<00:29:41.890><c> in</c><00:29:42.220><c> these</c><00:29:42.429><c> networks</c><00:29:42.730><c> has</c><00:29:43.299><c> steadily</c>

00:29:43.650 --> 00:29:43.660 align:start position:0%
layers in these networks has steadily
 

00:29:43.660 --> 00:29:47.310 align:start position:0%
layers in these networks has steadily
been<00:29:43.900><c> increasing</c><00:29:44.830><c> so</c><00:29:45.309><c> take</c><00:29:46.120><c> it</c><00:29:46.360><c> as</c><00:29:46.570><c> what</c><00:29:47.200><c> you</c>

00:29:47.310 --> 00:29:47.320 align:start position:0%
been increasing so take it as what you
 

00:29:47.320 --> 00:29:50.370 align:start position:0%
been increasing so take it as what you
will<00:29:48.030><c> you</c><00:29:49.030><c> know</c><00:29:49.150><c> there's</c><00:29:49.750><c> something</c><00:29:49.960><c> to</c><00:29:50.110><c> be</c>

00:29:50.370 --> 00:29:50.380 align:start position:0%
will you know there's something to be
 

00:29:50.380 --> 00:29:53.039 align:start position:0%
will you know there's something to be
said<00:29:50.410><c> about</c><00:29:50.860><c> building</c><00:29:51.570><c> deeper</c><00:29:52.570><c> and</c><00:29:52.720><c> deeper</c>

00:29:53.039 --> 00:29:53.049 align:start position:0%
said about building deeper and deeper
 

00:29:53.049 --> 00:29:54.810 align:start position:0%
said about building deeper and deeper
networks<00:29:53.530><c> to</c><00:29:53.770><c> achieve</c><00:29:54.070><c> higher</c><00:29:54.309><c> and</c><00:29:54.610><c> higher</c>

00:29:54.810 --> 00:29:54.820 align:start position:0%
networks to achieve higher and higher
 

00:29:54.820 --> 00:29:59.130 align:start position:0%
networks to achieve higher and higher
accuracies<00:29:56.910><c> so</c><00:29:57.910><c> so</c><00:29:58.090><c> far</c><00:29:58.299><c> we've</c><00:29:58.510><c> only</c><00:29:58.780><c> talked</c>

00:29:59.130 --> 00:29:59.140 align:start position:0%
accuracies so so far we've only talked
 

00:29:59.140 --> 00:30:02.460 align:start position:0%
accuracies so so far we've only talked
about<00:29:59.549><c> classification</c><00:30:00.549><c> but</c><00:30:01.540><c> in</c><00:30:01.660><c> truth</c><00:30:01.929><c> CNN's</c>

00:30:02.460 --> 00:30:02.470 align:start position:0%
about classification but in truth CNN's
 

00:30:02.470 --> 00:30:05.150 align:start position:0%
about classification but in truth CNN's
are<00:30:02.710><c> extremely</c><00:30:03.700><c> flexible</c><00:30:03.850><c> architecture</c><00:30:04.840><c> and</c>

00:30:05.150 --> 00:30:05.160 align:start position:0%
are extremely flexible architecture and
 

00:30:05.160 --> 00:30:08.340 align:start position:0%
are extremely flexible architecture and
have<00:30:06.160><c> been</c><00:30:06.400><c> shown</c><00:30:06.429><c> to</c><00:30:07.390><c> be</c><00:30:07.510><c> really</c><00:30:07.870><c> powerful</c>

00:30:08.340 --> 00:30:08.350 align:start position:0%
have been shown to be really powerful
 

00:30:08.350 --> 00:30:09.870 align:start position:0%
have been shown to be really powerful
for<00:30:08.650><c> a</c><00:30:08.679><c> number</c><00:30:09.190><c> of</c><00:30:09.400><c> different</c><00:30:09.760><c> applications</c>

00:30:09.870 --> 00:30:09.880 align:start position:0%
for a number of different applications
 

00:30:09.880 --> 00:30:13.409 align:start position:0%
for a number of different applications
and<00:30:10.799><c> when</c><00:30:11.799><c> we</c><00:30:11.890><c> considered</c><00:30:12.340><c> a</c><00:30:12.490><c> CNN</c><00:30:13.179><c> for</c>

00:30:13.409 --> 00:30:13.419 align:start position:0%
and when we considered a CNN for
 

00:30:13.419 --> 00:30:15.690 align:start position:0%
and when we considered a CNN for
classification<00:30:14.230><c> I</c><00:30:14.350><c> showed</c><00:30:14.679><c> this</c><00:30:15.190><c> general</c>

00:30:15.690 --> 00:30:15.700 align:start position:0%
classification I showed this general
 

00:30:15.700 --> 00:30:18.060 align:start position:0%
classification I showed this general
pipeline<00:30:16.450><c> schematic</c><00:30:16.990><c> where</c><00:30:17.530><c> we</c><00:30:17.679><c> had</c><00:30:17.860><c> two</c>

00:30:18.060 --> 00:30:18.070 align:start position:0%
pipeline schematic where we had two
 

00:30:18.070 --> 00:30:20.340 align:start position:0%
pipeline schematic where we had two
parts<00:30:18.429><c> the</c><00:30:18.700><c> feature</c><00:30:18.940><c> learning</c><00:30:19.240><c> part</c><00:30:19.690><c> and</c><00:30:19.870><c> the</c>

00:30:20.340 --> 00:30:20.350 align:start position:0%
parts the feature learning part and the
 

00:30:20.350 --> 00:30:23.580 align:start position:0%
parts the feature learning part and the
classification<00:30:20.830><c> part</c><00:30:21.400><c> and</c><00:30:22.230><c> what</c><00:30:23.230><c> makes</c><00:30:23.440><c> a</c>

00:30:23.580 --> 00:30:23.590 align:start position:0%
classification part and what makes a
 

00:30:23.590 --> 00:30:25.230 align:start position:0%
classification part and what makes a
convolutional<00:30:24.280><c> neural</c><00:30:24.490><c> network</c><00:30:24.940><c> a</c>

00:30:25.230 --> 00:30:25.240 align:start position:0%
convolutional neural network a
 

00:30:25.240 --> 00:30:27.510 align:start position:0%
convolutional neural network a
convolutional<00:30:26.169><c> neural</c><00:30:26.380><c> network</c><00:30:26.890><c> is</c><00:30:27.100><c> really</c>

00:30:27.510 --> 00:30:27.520 align:start position:0%
convolutional neural network is really
 

00:30:27.520 --> 00:30:30.120 align:start position:0%
convolutional neural network is really
this<00:30:27.730><c> feature</c><00:30:28.030><c> learning</c><00:30:28.330><c> portion</c><00:30:28.990><c> and</c><00:30:29.200><c> after</c>

00:30:30.120 --> 00:30:30.130 align:start position:0%
this feature learning portion and after
 

00:30:30.130 --> 00:30:31.950 align:start position:0%
this feature learning portion and after
that<00:30:30.280><c> we</c><00:30:30.490><c> can</c><00:30:30.610><c> really</c><00:30:30.880><c> change</c><00:30:30.970><c> the</c><00:30:31.510><c> second</c>

00:30:31.950 --> 00:30:31.960 align:start position:0%
that we can really change the second
 

00:30:31.960 --> 00:30:34.860 align:start position:0%
that we can really change the second
part<00:30:32.200><c> to</c><00:30:32.500><c> suit</c><00:30:33.400><c> the</c><00:30:33.700><c> application</c><00:30:34.360><c> that</c><00:30:34.540><c> we</c>

00:30:34.860 --> 00:30:34.870 align:start position:0%
part to suit the application that we
 

00:30:34.870 --> 00:30:37.669 align:start position:0%
part to suit the application that we
desire<00:30:35.230><c> so</c><00:30:36.160><c> for</c><00:30:36.370><c> example</c><00:30:36.730><c> this</c><00:30:36.910><c> portion</c><00:30:37.450><c> is</c>

00:30:37.669 --> 00:30:37.679 align:start position:0%
desire so for example this portion is
 

00:30:37.679 --> 00:30:39.960 align:start position:0%
desire so for example this portion is
going<00:30:38.679><c> to</c><00:30:38.950><c> look</c><00:30:39.130><c> different</c><00:30:39.520><c> for</c><00:30:39.580><c> different</c>

00:30:39.960 --> 00:30:39.970 align:start position:0%
going to look different for different
 

00:30:39.970 --> 00:30:43.110 align:start position:0%
going to look different for different
image<00:30:40.900><c> classification</c><00:30:41.169><c> domains</c><00:30:42.100><c> and</c><00:30:42.370><c> we</c><00:30:43.000><c> can</c>

00:30:43.110 --> 00:30:43.120 align:start position:0%
image classification domains and we can
 

00:30:43.120 --> 00:30:45.600 align:start position:0%
image classification domains and we can
also<00:30:43.360><c> introduce</c><00:30:43.900><c> new</c><00:30:44.380><c> architectures</c><00:30:45.100><c> for</c>

00:30:45.600 --> 00:30:45.610 align:start position:0%
also introduce new architectures for
 

00:30:45.610 --> 00:30:48.450 align:start position:0%
also introduce new architectures for
different<00:30:45.970><c> types</c><00:30:46.150><c> of</c><00:30:46.299><c> tasks</c><00:30:46.840><c> such</c><00:30:46.990><c> as</c><00:30:47.590><c> object</c>

00:30:48.450 --> 00:30:48.460 align:start position:0%
different types of tasks such as object
 

00:30:48.460 --> 00:30:51.060 align:start position:0%
different types of tasks such as object
recognition<00:30:49.140><c> segmentation</c><00:30:50.140><c> and</c><00:30:50.380><c> the</c><00:30:50.830><c> image</c>

00:30:51.060 --> 00:30:51.070 align:start position:0%
recognition segmentation and the image
 

00:30:51.070 --> 00:30:53.769 align:start position:0%
recognition segmentation and the image
captioning<00:30:51.690><c> so</c><00:30:52.690><c> I'd</c><00:30:52.780><c> like</c><00:30:52.929><c> to</c><00:30:52.990><c> consider</c>

00:30:53.769 --> 00:30:53.779 align:start position:0%
captioning so I'd like to consider
 

00:30:53.779 --> 00:30:55.719 align:start position:0%
captioning so I'd like to consider
three<00:30:53.899><c> different</c><00:30:54.289><c> applications</c><00:30:55.070><c> of</c><00:30:55.279><c> CNN's</c>

00:30:55.719 --> 00:30:55.729 align:start position:0%
three different applications of CNN's
 

00:30:55.729 --> 00:30:58.719 align:start position:0%
three different applications of CNN's
beyond<00:30:56.239><c> image</c><00:30:56.690><c> classification</c><00:30:57.129><c> the</c><00:30:58.129><c> first</c><00:30:58.399><c> is</c>

00:30:58.719 --> 00:30:58.729 align:start position:0%
beyond image classification the first is
 

00:30:58.729 --> 00:31:01.599 align:start position:0%
beyond image classification the first is
semantic<00:30:59.389><c> segmentation</c><00:31:00.169><c> where</c><00:31:00.830><c> the</c><00:31:01.039><c> task</c><00:31:01.309><c> is</c>

00:31:01.599 --> 00:31:01.609 align:start position:0%
semantic segmentation where the task is
 

00:31:01.609 --> 00:31:04.089 align:start position:0%
semantic segmentation where the task is
to<00:31:01.759><c> assign</c><00:31:02.149><c> each</c><00:31:02.509><c> pixel</c><00:31:03.049><c> in</c><00:31:03.259><c> the</c><00:31:03.590><c> image</c><00:31:03.619><c> an</c>

00:31:04.089 --> 00:31:04.099 align:start position:0%
to assign each pixel in the image an
 

00:31:04.099 --> 00:31:06.969 align:start position:0%
to assign each pixel in the image an
object<00:31:04.969><c> class</c><00:31:05.149><c> to</c><00:31:05.570><c> produce</c><00:31:05.779><c> a</c><00:31:06.229><c> segmentation</c>

00:31:06.969 --> 00:31:06.979 align:start position:0%
object class to produce a segmentation
 

00:31:06.979 --> 00:31:10.330 align:start position:0%
object class to produce a segmentation
of<00:31:07.279><c> the</c><00:31:07.399><c> image</c><00:31:07.669><c> object</c><00:31:08.479><c> detection</c><00:31:09.169><c> where</c><00:31:09.710><c> we</c>

00:31:10.330 --> 00:31:10.340 align:start position:0%
of the image object detection where we
 

00:31:10.340 --> 00:31:13.599 align:start position:0%
of the image object detection where we
want<00:31:10.580><c> to</c><00:31:10.789><c> detect</c><00:31:11.149><c> instances</c><00:31:11.929><c> of</c><00:31:12.609><c> specific</c>

00:31:13.599 --> 00:31:13.609 align:start position:0%
want to detect instances of specific
 

00:31:13.609 --> 00:31:16.269 align:start position:0%
want to detect instances of specific
objects<00:31:14.299><c> in</c><00:31:14.419><c> the</c><00:31:14.570><c> image</c><00:31:14.690><c> and</c><00:31:15.139><c> finally</c><00:31:15.859><c> image</c>

00:31:16.269 --> 00:31:16.279 align:start position:0%
objects in the image and finally image
 

00:31:16.279 --> 00:31:18.459 align:start position:0%
objects in the image and finally image
captioning<00:31:16.820><c> where</c><00:31:17.090><c> the</c><00:31:17.299><c> task</c><00:31:17.509><c> is</c><00:31:17.809><c> to</c><00:31:17.840><c> generate</c>

00:31:18.459 --> 00:31:18.469 align:start position:0%
captioning where the task is to generate
 

00:31:18.469 --> 00:31:21.959 align:start position:0%
captioning where the task is to generate
a<00:31:19.029><c> language</c><00:31:20.029><c> description</c><00:31:20.450><c> of</c><00:31:20.809><c> the</c><00:31:21.229><c> image</c><00:31:21.349><c> that</c>

00:31:21.959 --> 00:31:21.969 align:start position:0%
a language description of the image that
 

00:31:21.969 --> 00:31:26.940 align:start position:0%
a language description of the image that
captures<00:31:22.969><c> its</c><00:31:23.239><c> semantic</c><00:31:23.719><c> meaning</c><00:31:25.239><c> so</c><00:31:26.239><c> first</c>

00:31:26.940 --> 00:31:26.950 align:start position:0%
captures its semantic meaning so first
 

00:31:26.950 --> 00:31:29.680 align:start position:0%
captures its semantic meaning so first
let's<00:31:27.950><c> talk</c><00:31:28.099><c> about</c><00:31:28.159><c> semantic</c><00:31:28.909><c> segmentation</c>

00:31:29.680 --> 00:31:29.690 align:start position:0%
let's talk about semantic segmentation
 

00:31:29.690 --> 00:31:32.379 align:start position:0%
let's talk about semantic segmentation
with<00:31:30.259><c> this</c><00:31:30.679><c> architecture</c><00:31:30.889><c> called</c><00:31:31.580><c> fully</c>

00:31:32.379 --> 00:31:32.389 align:start position:0%
with this architecture called fully
 

00:31:32.389 --> 00:31:35.609 align:start position:0%
with this architecture called fully
convolutional<00:31:33.259><c> networks</c><00:31:33.710><c> or</c><00:31:33.889><c> f</c><00:31:34.190><c> c</c><00:31:34.580><c> ends</c><00:31:35.059><c> and</c>

00:31:35.609 --> 00:31:35.619 align:start position:0%
convolutional networks or f c ends and
 

00:31:35.619 --> 00:31:38.769 align:start position:0%
convolutional networks or f c ends and
here<00:31:36.619><c> the</c><00:31:37.429><c> way</c><00:31:37.519><c> it</c><00:31:37.549><c> works</c><00:31:37.849><c> is</c><00:31:38.119><c> the</c><00:31:38.450><c> network</c>

00:31:38.769 --> 00:31:38.779 align:start position:0%
here the way it works is the network
 

00:31:38.779 --> 00:31:41.609 align:start position:0%
here the way it works is the network
takes<00:31:39.109><c> in</c><00:31:39.409><c> an</c><00:31:39.649><c> input</c><00:31:40.159><c> of</c><00:31:40.279><c> arbitrary</c><00:31:40.940><c> size</c><00:31:41.210><c> and</c>

00:31:41.609 --> 00:31:41.619 align:start position:0%
takes in an input of arbitrary size and
 

00:31:41.619 --> 00:31:46.119 align:start position:0%
takes in an input of arbitrary size and
produces<00:31:42.619><c> an</c><00:31:42.739><c> output</c><00:31:43.039><c> of</c><00:31:44.379><c> of</c><00:31:45.379><c> corresponding</c>

00:31:46.119 --> 00:31:46.129 align:start position:0%
produces an output of of corresponding
 

00:31:46.129 --> 00:31:48.489 align:start position:0%
produces an output of of corresponding
size<00:31:46.339><c> where</c><00:31:46.909><c> each</c><00:31:47.239><c> pixel</c><00:31:47.509><c> has</c><00:31:47.899><c> been</c><00:31:48.109><c> assigned</c>

00:31:48.489 --> 00:31:48.499 align:start position:0%
size where each pixel has been assigned
 

00:31:48.499 --> 00:31:50.619 align:start position:0%
size where each pixel has been assigned
an<00:31:48.799><c> object</c><00:31:49.369><c> class</c><00:31:49.580><c> which</c><00:31:50.269><c> we</c><00:31:50.450><c> can</c><00:31:50.599><c> then</c>

00:31:50.619 --> 00:31:50.629 align:start position:0%
an object class which we can then
 

00:31:50.629 --> 00:31:54.279 align:start position:0%
an object class which we can then
visualize<00:31:50.989><c> as</c><00:31:51.529><c> a</c><00:31:51.559><c> segmentation</c><00:31:52.249><c> and</c><00:31:53.109><c> so</c><00:31:54.109><c> as</c>

00:31:54.279 --> 00:31:54.289 align:start position:0%
visualize as a segmentation and so as
 

00:31:54.289 --> 00:31:57.009 align:start position:0%
visualize as a segmentation and so as
before<00:31:54.469><c> we</c><00:31:55.309><c> have</c><00:31:55.519><c> a</c><00:31:55.549><c> series</c><00:31:56.029><c> of</c><00:31:56.059><c> convolutional</c>

00:31:57.009 --> 00:31:57.019 align:start position:0%
before we have a series of convolutional
 

00:31:57.019 --> 00:32:02.079 align:start position:0%
before we have a series of convolutional
layers<00:31:59.349><c> arranged</c><00:32:00.349><c> in</c><00:32:00.469><c> this</c><00:32:01.089><c> hierarchical</c>

00:32:02.079 --> 00:32:02.089 align:start position:0%
layers arranged in this hierarchical
 

00:32:02.089 --> 00:32:06.519 align:start position:0%
layers arranged in this hierarchical
fashion<00:32:02.559><c> to</c><00:32:03.559><c> create</c><00:32:03.739><c> this</c><00:32:04.839><c> learned</c><00:32:05.839><c> hierarchy</c>

00:32:06.519 --> 00:32:06.529 align:start position:0%
fashion to create this learned hierarchy
 

00:32:06.529 --> 00:32:09.249 align:start position:0%
fashion to create this learned hierarchy
of<00:32:06.769><c> features</c><00:32:07.489><c> but</c><00:32:08.389><c> then</c><00:32:08.629><c> we</c><00:32:08.839><c> can</c><00:32:09.019><c> supplement</c>

00:32:09.249 --> 00:32:09.259 align:start position:0%
of features but then we can supplement
 

00:32:09.259 --> 00:32:14.109 align:start position:0%
of features but then we can supplement
this<00:32:11.109><c> these</c><00:32:12.109><c> down</c><00:32:12.379><c> sampling</c><00:32:12.919><c> operations</c><00:32:13.879><c> with</c>

00:32:14.109 --> 00:32:14.119 align:start position:0%
this these down sampling operations with
 

00:32:14.119 --> 00:32:17.440 align:start position:0%
this these down sampling operations with
up<00:32:14.570><c> sampling</c><00:32:15.289><c> operations</c><00:32:15.950><c> that</c><00:32:16.669><c> increase</c><00:32:17.239><c> the</c>

00:32:17.440 --> 00:32:17.450 align:start position:0%
up sampling operations that increase the
 

00:32:17.450 --> 00:32:19.269 align:start position:0%
up sampling operations that increase the
resolution<00:32:17.779><c> of</c><00:32:18.139><c> the</c><00:32:18.259><c> output</c><00:32:18.679><c> from</c><00:32:18.950><c> those</c>

00:32:19.269 --> 00:32:19.279 align:start position:0%
resolution of the output from those
 

00:32:19.279 --> 00:32:21.759 align:start position:0%
resolution of the output from those
feature<00:32:19.580><c> learning</c><00:32:19.849><c> layers</c><00:32:20.299><c> and</c><00:32:20.629><c> then</c><00:32:21.469><c> you</c><00:32:21.589><c> can</c>

00:32:21.759 --> 00:32:21.769 align:start position:0%
feature learning layers and then you can
 

00:32:21.769 --> 00:32:24.909 align:start position:0%
feature learning layers and then you can
combine<00:32:22.009><c> the</c><00:32:22.489><c> output</c><00:32:23.499><c> from</c><00:32:24.499><c> these</c><00:32:24.679><c> up</c>

00:32:24.909 --> 00:32:24.919 align:start position:0%
combine the output from these up
 

00:32:24.919 --> 00:32:27.310 align:start position:0%
combine the output from these up
sampling<00:32:25.429><c> layers</c><00:32:25.669><c> with</c><00:32:26.299><c> those</c><00:32:26.539><c> from</c><00:32:26.869><c> the</c><00:32:26.899><c> down</c>

00:32:27.310 --> 00:32:27.320 align:start position:0%
sampling layers with those from the down
 

00:32:27.320 --> 00:32:29.289 align:start position:0%
sampling layers with those from the down
sampling<00:32:27.799><c> layers</c><00:32:28.039><c> to</c><00:32:28.339><c> actually</c><00:32:28.729><c> produce</c><00:32:28.909><c> a</c>

00:32:29.289 --> 00:32:29.299 align:start position:0%
sampling layers to actually produce a
 

00:32:29.299 --> 00:32:32.829 align:start position:0%
sampling layers to actually produce a
segmentation<00:32:30.729><c> one</c><00:32:31.729><c> application</c><00:32:32.479><c> of</c><00:32:32.659><c> this</c>

00:32:32.829 --> 00:32:32.839 align:start position:0%
segmentation one application of this
 

00:32:32.839 --> 00:32:35.009 align:start position:0%
segmentation one application of this
sort<00:32:33.109><c> of</c><00:32:33.169><c> architecture</c><00:32:33.469><c> is</c><00:32:34.070><c> to</c><00:32:34.429><c> the</c><00:32:34.580><c> real-time</c>

00:32:35.009 --> 00:32:35.019 align:start position:0%
sort of architecture is to the real-time
 

00:32:35.019 --> 00:32:39.070 align:start position:0%
sort of architecture is to the real-time
segmentation<00:32:36.019><c> of</c><00:32:36.489><c> driving</c><00:32:37.489><c> scenes</c><00:32:37.729><c> so</c><00:32:38.599><c> this</c>

00:32:39.070 --> 00:32:39.080 align:start position:0%
segmentation of driving scenes so this
 

00:32:39.080 --> 00:32:41.229 align:start position:0%
segmentation of driving scenes so this
was<00:32:39.349><c> a</c><00:32:39.649><c> results</c><00:32:40.309><c> from</c><00:32:40.489><c> a</c><00:32:40.580><c> couple</c><00:32:40.820><c> years</c><00:32:41.119><c> ago</c>

00:32:41.229 --> 00:32:41.239 align:start position:0%
was a results from a couple years ago
 

00:32:41.239 --> 00:32:43.959 align:start position:0%
was a results from a couple years ago
where<00:32:41.960><c> the</c><00:32:42.710><c> authors</c><00:32:43.159><c> were</c><00:32:43.190><c> using</c><00:32:43.789><c> this</c>

00:32:43.959 --> 00:32:43.969 align:start position:0%
where the authors were using this
 

00:32:43.969 --> 00:32:48.459 align:start position:0%
where the authors were using this
encoder<00:32:44.539><c> decoder</c><00:32:44.869><c> like</c><00:32:45.849><c> structure</c><00:32:47.229><c> where</c><00:32:48.229><c> you</c>

00:32:48.459 --> 00:32:48.469 align:start position:0%
encoder decoder like structure where you
 

00:32:48.469 --> 00:32:51.070 align:start position:0%
encoder decoder like structure where you
have<00:32:48.619><c> down</c><00:32:49.159><c> sampling</c><00:32:49.669><c> layers</c><00:32:49.999><c> followed</c><00:32:50.749><c> by</c><00:32:50.869><c> up</c>

00:32:51.070 --> 00:32:51.080 align:start position:0%
have down sampling layers followed by up
 

00:32:51.080 --> 00:32:54.879 align:start position:0%
have down sampling layers followed by up
sampling<00:32:51.589><c> layers</c><00:32:52.210><c> to</c><00:32:53.210><c> to</c><00:32:53.779><c> produce</c><00:32:54.440><c> these</c>

00:32:54.879 --> 00:32:54.889 align:start position:0%
sampling layers to to produce these
 

00:32:54.889 --> 00:32:58.149 align:start position:0%
sampling layers to to produce these
these<00:32:55.219><c> segmentations</c><00:32:56.059><c> and</c><00:32:56.419><c> last</c><00:32:57.080><c> year</c><00:32:57.349><c> this</c>

00:32:58.149 --> 00:32:58.159 align:start position:0%
these segmentations and last year this
 

00:32:58.159 --> 00:32:59.560 align:start position:0%
these segmentations and last year this
was<00:32:58.429><c> sort</c><00:32:58.639><c> of</c><00:32:58.729><c> the</c><00:32:58.879><c> state</c><00:32:59.149><c> of</c><00:32:59.269><c> the</c><00:32:59.389><c> art</c>

00:32:59.560 --> 00:32:59.570 align:start position:0%
was sort of the state of the art
 

00:32:59.570 --> 00:33:01.869 align:start position:0%
was sort of the state of the art
performance<00:33:00.320><c> that</c><00:33:00.529><c> you</c><00:33:01.249><c> could</c><00:33:01.399><c> achieve</c><00:33:01.519><c> with</c>

00:33:01.869 --> 00:33:01.879 align:start position:0%
performance that you could achieve with
 

00:33:01.879 --> 00:33:04.269 align:start position:0%
performance that you could achieve with
an<00:33:02.029><c> architecture</c><00:33:02.629><c> like</c><00:33:02.839><c> this</c><00:33:02.869><c> in</c><00:33:03.379><c> in</c><00:33:04.009><c> doing</c>

00:33:04.269 --> 00:33:04.279 align:start position:0%
an architecture like this in in doing
 

00:33:04.279 --> 00:33:05.190 align:start position:0%
an architecture like this in in doing
semantic<00:33:04.789><c> segment</c>

00:33:05.190 --> 00:33:05.200 align:start position:0%
semantic segment
 

00:33:05.200 --> 00:33:08.190 align:start position:0%
semantic segment
pation<00:33:05.559><c> and</c><00:33:06.370><c> deep</c><00:33:07.360><c> learning</c><00:33:07.539><c> is</c><00:33:07.870><c> moving</c>

00:33:08.190 --> 00:33:08.200 align:start position:0%
pation and deep learning is moving
 

00:33:08.200 --> 00:33:10.560 align:start position:0%
pation and deep learning is moving
extremely<00:33:08.620><c> fast</c><00:33:09.100><c> and</c><00:33:09.370><c> now</c><00:33:09.669><c> the</c><00:33:09.730><c> new</c><00:33:10.210><c> state</c><00:33:10.539><c> of</c>

00:33:10.560 --> 00:33:10.570 align:start position:0%
extremely fast and now the new state of
 

00:33:10.570 --> 00:33:13.080 align:start position:0%
extremely fast and now the new state of
the<00:33:10.690><c> art</c><00:33:10.809><c> in</c><00:33:11.289><c> semantic</c><00:33:12.039><c> segmentation</c><00:33:12.760><c> is</c><00:33:12.909><c> what</c>

00:33:13.080 --> 00:33:13.090 align:start position:0%
the art in semantic segmentation is what
 

00:33:13.090 --> 00:33:15.299 align:start position:0%
the art in semantic segmentation is what
you<00:33:13.179><c> see</c><00:33:13.389><c> here</c><00:33:13.419><c> and</c><00:33:13.990><c> it's</c><00:33:14.799><c> actually</c><00:33:15.190><c> the</c><00:33:15.279><c> same</c>

00:33:15.299 --> 00:33:15.309 align:start position:0%
you see here and it's actually the same
 

00:33:15.309 --> 00:33:17.700 align:start position:0%
you see here and it's actually the same
the<00:33:15.970><c> same</c><00:33:16.179><c> authors</c><00:33:16.600><c> as</c><00:33:16.750><c> that</c><00:33:16.899><c> previous</c><00:33:17.320><c> result</c>

00:33:17.700 --> 00:33:17.710 align:start position:0%
the same authors as that previous result
 

00:33:17.710 --> 00:33:20.879 align:start position:0%
the same authors as that previous result
but<00:33:18.279><c> with</c><00:33:18.549><c> an</c><00:33:18.700><c> improved</c><00:33:19.210><c> architecture</c><00:33:20.080><c> where</c>

00:33:20.879 --> 00:33:20.889 align:start position:0%
but with an improved architecture where
 

00:33:20.889 --> 00:33:23.039 align:start position:0%
but with an improved architecture where
now<00:33:21.100><c> they're</c><00:33:21.370><c> using</c><00:33:21.549><c> one</c><00:33:21.970><c> network</c><00:33:22.240><c> trained</c><00:33:22.899><c> to</c>

00:33:23.039 --> 00:33:23.049 align:start position:0%
now they're using one network trained to
 

00:33:23.049 --> 00:33:26.279 align:start position:0%
now they're using one network trained to
do<00:33:23.200><c> three</c><00:33:23.559><c> tasks</c><00:33:24.130><c> simultaneously</c><00:33:25.289><c> semantic</c>

00:33:26.279 --> 00:33:26.289 align:start position:0%
do three tasks simultaneously semantic
 

00:33:26.289 --> 00:33:30.779 align:start position:0%
do three tasks simultaneously semantic
segmentation<00:33:27.029><c> shown</c><00:33:28.029><c> here</c><00:33:28.950><c> depth</c><00:33:29.950><c> estimation</c>

00:33:30.779 --> 00:33:30.789 align:start position:0%
segmentation shown here depth estimation
 

00:33:30.789 --> 00:33:34.769 align:start position:0%
segmentation shown here depth estimation
and<00:33:32.370><c> instant</c><00:33:33.370><c> segmentation</c><00:33:34.059><c> which</c><00:33:34.510><c> means</c>

00:33:34.769 --> 00:33:34.779 align:start position:0%
and instant segmentation which means
 

00:33:34.779 --> 00:33:37.049 align:start position:0%
and instant segmentation which means
identifying<00:33:35.380><c> different</c><00:33:35.889><c> instances</c><00:33:36.700><c> of</c><00:33:36.850><c> the</c>

00:33:37.049 --> 00:33:37.059 align:start position:0%
identifying different instances of the
 

00:33:37.059 --> 00:33:39.899 align:start position:0%
identifying different instances of the
same<00:33:37.299><c> object</c><00:33:37.809><c> type</c><00:33:38.019><c> and</c><00:33:38.350><c> as</c><00:33:39.039><c> you</c><00:33:39.250><c> can</c><00:33:39.399><c> see</c><00:33:39.610><c> in</c>

00:33:39.899 --> 00:33:39.909 align:start position:0%
same object type and as you can see in
 

00:33:39.909 --> 00:33:41.340 align:start position:0%
same object type and as you can see in
this<00:33:40.210><c> upper</c><00:33:40.600><c> right</c><00:33:40.750><c> corner</c>

00:33:41.340 --> 00:33:41.350 align:start position:0%
this upper right corner
 

00:33:41.350 --> 00:33:43.259 align:start position:0%
this upper right corner
these<00:33:41.620><c> segmentation</c><00:33:42.399><c> results</c><00:33:42.429><c> are</c><00:33:43.000><c> pretty</c>

00:33:43.259 --> 00:33:43.269 align:start position:0%
these segmentation results are pretty
 

00:33:43.269 --> 00:33:45.180 align:start position:0%
these segmentation results are pretty
astonishing<00:33:43.659><c> and</c><00:33:44.169><c> they've</c><00:33:44.710><c> they</c>

00:33:45.180 --> 00:33:45.190 align:start position:0%
astonishing and they've they
 

00:33:45.190 --> 00:33:47.070 align:start position:0%
astonishing and they've they
significantly<00:33:45.940><c> improved</c><00:33:46.330><c> in</c><00:33:46.539><c> terms</c><00:33:46.570><c> of</c><00:33:46.929><c> their</c>

00:33:47.070 --> 00:33:47.080 align:start position:0%
significantly improved in terms of their
 

00:33:47.080 --> 00:33:50.340 align:start position:0%
significantly improved in terms of their
crypts<00:33:47.350><c> crispness</c><00:33:48.450><c> compared</c><00:33:49.450><c> to</c><00:33:49.720><c> the</c>

00:33:50.340 --> 00:33:50.350 align:start position:0%
crypts crispness compared to the
 

00:33:50.350 --> 00:33:56.159 align:start position:0%
crypts crispness compared to the
previous<00:33:51.010><c> result</c><00:33:54.000><c> another</c><00:33:55.000><c> way</c><00:33:55.240><c> CNN's</c><00:33:55.840><c> have</c>

00:33:56.159 --> 00:33:56.169 align:start position:0%
previous result another way CNN's have
 

00:33:56.169 --> 00:33:58.590 align:start position:0%
previous result another way CNN's have
been<00:33:56.470><c> extended</c><00:33:57.190><c> is</c><00:33:57.519><c> for</c><00:33:57.820><c> object</c><00:33:58.269><c> detection</c>

00:33:58.590 --> 00:33:58.600 align:start position:0%
been extended is for object detection
 

00:33:58.600 --> 00:34:01.259 align:start position:0%
been extended is for object detection
where<00:33:59.529><c> here</c><00:33:59.919><c> the</c><00:34:00.100><c> task</c><00:34:00.340><c> is</c><00:34:00.610><c> to</c><00:34:00.820><c> learn</c><00:34:01.000><c> features</c>

00:34:01.259 --> 00:34:01.269 align:start position:0%
where here the task is to learn features
 

00:34:01.269 --> 00:34:03.690 align:start position:0%
where here the task is to learn features
that<00:34:01.720><c> characterize</c><00:34:02.139><c> particular</c><00:34:02.980><c> regions</c><00:34:03.490><c> of</c>

00:34:03.690 --> 00:34:03.700 align:start position:0%
that characterize particular regions of
 

00:34:03.700 --> 00:34:06.240 align:start position:0%
that characterize particular regions of
the<00:34:04.149><c> input</c><00:34:04.480><c> image</c><00:34:04.630><c> then</c><00:34:05.289><c> classify</c><00:34:05.980><c> those</c>

00:34:06.240 --> 00:34:06.250 align:start position:0%
the input image then classify those
 

00:34:06.250 --> 00:34:09.060 align:start position:0%
the input image then classify those
regions<00:34:06.730><c> as</c><00:34:06.880><c> belonging</c><00:34:07.690><c> to</c><00:34:08.440><c> particular</c>

00:34:09.060 --> 00:34:09.070 align:start position:0%
regions as belonging to particular
 

00:34:09.070 --> 00:34:11.819 align:start position:0%
regions as belonging to particular
object<00:34:09.909><c> classes</c><00:34:10.359><c> and</c><00:34:10.599><c> the</c><00:34:11.139><c> pipeline</c><00:34:11.589><c> for</c>

00:34:11.819 --> 00:34:11.829 align:start position:0%
object classes and the pipeline for
 

00:34:11.829 --> 00:34:14.220 align:start position:0%
object classes and the pipeline for
doing<00:34:12.010><c> this</c><00:34:12.129><c> is</c><00:34:12.399><c> is</c><00:34:12.730><c> an</c><00:34:13.240><c> architecture</c><00:34:13.839><c> called</c>

00:34:14.220 --> 00:34:14.230 align:start position:0%
doing this is is an architecture called
 

00:34:14.230 --> 00:34:16.470 align:start position:0%
doing this is is an architecture called
our<00:34:14.349><c> CNN</c><00:34:15.040><c> and</c><00:34:15.339><c> it's</c><00:34:15.460><c> pretty</c><00:34:16.179><c> straight</c><00:34:16.450><c> forward</c>

00:34:16.470 --> 00:34:16.480 align:start position:0%
our CNN and it's pretty straight forward
 

00:34:16.480 --> 00:34:19.760 align:start position:0%
our CNN and it's pretty straight forward
so<00:34:17.169><c> given</c><00:34:17.530><c> an</c><00:34:17.649><c> input</c><00:34:17.859><c> image</c><00:34:18.310><c> this</c><00:34:19.119><c> algorithm</c>

00:34:19.760 --> 00:34:19.770 align:start position:0%
so given an input image this algorithm
 

00:34:19.770 --> 00:34:22.680 align:start position:0%
so given an input image this algorithm
extracts<00:34:20.770><c> a</c><00:34:20.859><c> set</c><00:34:21.069><c> of</c><00:34:21.250><c> region</c><00:34:22.089><c> proposals</c>

00:34:22.680 --> 00:34:22.690 align:start position:0%
extracts a set of region proposals
 

00:34:22.690 --> 00:34:25.470 align:start position:0%
extracts a set of region proposals
bottom-up<00:34:23.500><c> computes</c><00:34:24.490><c> features</c><00:34:25.000><c> for</c><00:34:25.270><c> these</c>

00:34:25.470 --> 00:34:25.480 align:start position:0%
bottom-up computes features for these
 

00:34:25.480 --> 00:34:28.020 align:start position:0%
bottom-up computes features for these
proposals<00:34:26.139><c> using</c><00:34:26.710><c> convolutional</c><00:34:27.520><c> layers</c><00:34:27.730><c> and</c>

00:34:28.020 --> 00:34:28.030 align:start position:0%
proposals using convolutional layers and
 

00:34:28.030 --> 00:34:31.940 align:start position:0%
proposals using convolutional layers and
then<00:34:28.629><c> classifies</c><00:34:29.349><c> each</c><00:34:29.770><c> region</c><00:34:30.550><c> proposal</c><00:34:31.149><c> and</c>

00:34:31.940 --> 00:34:31.950 align:start position:0%
then classifies each region proposal and
 

00:34:31.950 --> 00:34:34.700 align:start position:0%
then classifies each region proposal and
there<00:34:32.950><c> have</c><00:34:33.099><c> been</c><00:34:33.129><c> many</c><00:34:33.460><c> many</c><00:34:34.060><c> different</c>

00:34:34.700 --> 00:34:34.710 align:start position:0%
there have been many many different
 

00:34:34.710 --> 00:34:37.889 align:start position:0%
there have been many many different
approaches<00:34:35.710><c> to</c><00:34:36.069><c> computing</c><00:34:36.970><c> these</c><00:34:37.480><c> different</c>

00:34:37.889 --> 00:34:37.899 align:start position:0%
approaches to computing these different
 

00:34:37.899 --> 00:34:40.050 align:start position:0%
approaches to computing these different
computing<00:34:38.770><c> and</c><00:34:38.919><c> estimating</c><00:34:39.520><c> these</c><00:34:39.669><c> region</c>

00:34:40.050 --> 00:34:40.060 align:start position:0%
computing and estimating these region
 

00:34:40.060 --> 00:34:42.659 align:start position:0%
computing and estimating these region
proposals<00:34:40.629><c> step</c><00:34:41.260><c> two</c><00:34:41.530><c> in</c><00:34:41.740><c> this</c><00:34:41.919><c> in</c><00:34:42.310><c> this</c>

00:34:42.659 --> 00:34:42.669 align:start position:0%
proposals step two in this in this
 

00:34:42.669 --> 00:34:48.059 align:start position:0%
proposals step two in this in this
pipeline<00:34:43.319><c> and</c><00:34:45.720><c> as</c><00:34:46.720><c> this</c><00:34:47.020><c> has</c><00:34:47.230><c> resulted</c><00:34:47.409><c> in</c><00:34:47.859><c> a</c>

00:34:48.059 --> 00:34:48.069 align:start position:0%
pipeline and as this has resulted in a
 

00:34:48.069 --> 00:34:49.980 align:start position:0%
pipeline and as this has resulted in a
number<00:34:48.250><c> of</c><00:34:48.429><c> different</c><00:34:48.669><c> extensions</c><00:34:49.659><c> of</c><00:34:49.839><c> this</c>

00:34:49.980 --> 00:34:49.990 align:start position:0%
number of different extensions of this
 

00:34:49.990 --> 00:34:53.399 align:start position:0%
number of different extensions of this
of<00:34:50.200><c> this</c><00:34:50.560><c> general</c><00:34:50.980><c> principle</c><00:34:52.050><c> the</c><00:34:53.050><c> final</c>

00:34:53.399 --> 00:34:53.409 align:start position:0%
of this general principle the final
 

00:34:53.409 --> 00:34:56.010 align:start position:0%
of this general principle the final
application<00:34:54.369><c> that</c><00:34:55.030><c> I'd</c><00:34:55.149><c> like</c><00:34:55.300><c> to</c><00:34:55.419><c> consider</c><00:34:55.780><c> is</c>

00:34:56.010 --> 00:34:56.020 align:start position:0%
application that I'd like to consider is
 

00:34:56.020 --> 00:34:59.099 align:start position:0%
application that I'd like to consider is
image<00:34:56.440><c> captioning</c><00:34:56.980><c> and</c><00:34:57.160><c> so</c><00:34:57.900><c> suppose</c><00:34:58.900><c> we're</c>

00:34:59.099 --> 00:34:59.109 align:start position:0%
image captioning and so suppose we're
 

00:34:59.109 --> 00:35:00.960 align:start position:0%
image captioning and so suppose we're
given<00:34:59.380><c> this</c><00:34:59.530><c> image</c><00:34:59.859><c> of</c><00:35:00.069><c> a</c><00:35:00.130><c> cat</c><00:35:00.339><c> riding</c><00:35:00.880><c> the</c>

00:35:00.960 --> 00:35:00.970 align:start position:0%
given this image of a cat riding the
 

00:35:00.970 --> 00:35:04.020 align:start position:0%
given this image of a cat riding the
skateboard<00:35:01.270><c> in</c><00:35:01.890><c> classification</c><00:35:02.890><c> our</c><00:35:03.460><c> task</c>

00:35:04.020 --> 00:35:04.030 align:start position:0%
skateboard in classification our task
 

00:35:04.030 --> 00:35:06.990 align:start position:0%
skateboard in classification our task
could<00:35:04.750><c> be</c><00:35:04.900><c> to</c><00:35:05.050><c> output</c><00:35:05.500><c> the</c><00:35:05.619><c> class</c><00:35:05.859><c> label</c><00:35:06.400><c> for</c>

00:35:06.990 --> 00:35:07.000 align:start position:0%
could be to output the class label for
 

00:35:07.000 --> 00:35:10.200 align:start position:0%
could be to output the class label for
this<00:35:07.240><c> particular</c><00:35:07.720><c> image</c><00:35:07.900><c> cat</c><00:35:08.859><c> and</c><00:35:09.250><c> as</c><00:35:09.819><c> we</c><00:35:10.000><c> saw</c>

00:35:10.200 --> 00:35:10.210 align:start position:0%
this particular image cat and as we saw
 

00:35:10.210 --> 00:35:12.240 align:start position:0%
this particular image cat and as we saw
this<00:35:10.810><c> is</c><00:35:11.020><c> done</c><00:35:11.200><c> by</c><00:35:11.410><c> feeding</c><00:35:11.710><c> the</c><00:35:11.950><c> image</c>

00:35:12.240 --> 00:35:12.250 align:start position:0%
this is done by feeding the image
 

00:35:12.250 --> 00:35:14.790 align:start position:0%
this is done by feeding the image
through<00:35:12.730><c> a</c><00:35:12.819><c> set</c><00:35:13.119><c> of</c><00:35:13.150><c> convolutional</c><00:35:13.960><c> layers</c><00:35:14.260><c> to</c>

00:35:14.790 --> 00:35:14.800 align:start position:0%
through a set of convolutional layers to
 

00:35:14.800 --> 00:35:17.460 align:start position:0%
through a set of convolutional layers to
extract<00:35:15.250><c> features</c><00:35:15.490><c> and</c><00:35:15.970><c> then</c><00:35:16.660><c> a</c><00:35:16.690><c> set</c><00:35:17.020><c> of</c><00:35:17.050><c> fully</c>

00:35:17.460 --> 00:35:17.470 align:start position:0%
extract features and then a set of fully
 

00:35:17.470 --> 00:35:18.220 align:start position:0%
extract features and then a set of fully
connected

00:35:18.220 --> 00:35:18.230 align:start position:0%
connected
 

00:35:18.230 --> 00:35:22.210 align:start position:0%
connected
to<00:35:18.730><c> generate</c><00:35:19.730><c> a</c><00:35:19.760><c> prediction</c><00:35:20.800><c> in</c><00:35:21.800><c> image</c>

00:35:22.210 --> 00:35:22.220 align:start position:0%
to generate a prediction in image
 

00:35:22.220 --> 00:35:23.890 align:start position:0%
to generate a prediction in image
captioning<00:35:22.760><c> what</c><00:35:22.970><c> we</c><00:35:23.089><c> want</c><00:35:23.300><c> to</c><00:35:23.420><c> do</c><00:35:23.630><c> is</c>

00:35:23.890 --> 00:35:23.900 align:start position:0%
captioning what we want to do is
 

00:35:23.900 --> 00:35:26.620 align:start position:0%
captioning what we want to do is
generate<00:35:24.800><c> a</c><00:35:24.980><c> sentence</c><00:35:25.460><c> that</c><00:35:25.609><c> describes</c><00:35:26.060><c> the</c>

00:35:26.620 --> 00:35:26.630 align:start position:0%
generate a sentence that describes the
 

00:35:26.630 --> 00:35:29.530 align:start position:0%
generate a sentence that describes the
semantic<00:35:27.140><c> content</c><00:35:27.320><c> of</c><00:35:27.770><c> the</c><00:35:27.980><c> image</c><00:35:28.240><c> so</c><00:35:29.240><c> if</c><00:35:29.390><c> we</c>

00:35:29.530 --> 00:35:29.540 align:start position:0%
semantic content of the image so if we
 

00:35:29.540 --> 00:35:33.040 align:start position:0%
semantic content of the image so if we
take<00:35:29.750><c> that</c><00:35:29.810><c> same</c><00:35:30.730><c> CNN</c><00:35:31.730><c> Network</c><00:35:32.119><c> from</c><00:35:32.420><c> before</c>

00:35:33.040 --> 00:35:33.050 align:start position:0%
take that same CNN Network from before
 

00:35:33.050 --> 00:35:35.650 align:start position:0%
take that same CNN Network from before
and<00:35:33.349><c> instead</c><00:35:33.800><c> of</c><00:35:34.190><c> fully</c><00:35:34.609><c> connected</c><00:35:35.089><c> layers</c><00:35:35.359><c> at</c>

00:35:35.650 --> 00:35:35.660 align:start position:0%
and instead of fully connected layers at
 

00:35:35.660 --> 00:35:39.130 align:start position:0%
and instead of fully connected layers at
the<00:35:35.780><c> end</c><00:35:35.930><c> we</c><00:35:36.530><c> replace</c><00:35:36.980><c> it</c><00:35:37.190><c> with</c><00:35:37.369><c> an</c><00:35:37.609><c> RNN</c><00:35:38.240><c> what</c>

00:35:39.130 --> 00:35:39.140 align:start position:0%
the end we replace it with an RNN what
 

00:35:39.140 --> 00:35:41.849 align:start position:0%
the end we replace it with an RNN what
we<00:35:39.260><c> can</c><00:35:39.470><c> do</c><00:35:39.619><c> is</c><00:35:39.890><c> we</c><00:35:40.099><c> can</c><00:35:40.280><c> use</c><00:35:40.520><c> a</c><00:35:40.940><c> set</c><00:35:41.450><c> of</c>

00:35:41.849 --> 00:35:41.859 align:start position:0%
we can do is we can use a set of
 

00:35:41.859 --> 00:35:44.349 align:start position:0%
we can do is we can use a set of
convolutional<00:35:42.859><c> layers</c><00:35:43.130><c> to</c><00:35:43.460><c> extract</c><00:35:44.030><c> visual</c>

00:35:44.349 --> 00:35:44.359 align:start position:0%
convolutional layers to extract visual
 

00:35:44.359 --> 00:35:48.220 align:start position:0%
convolutional layers to extract visual
features<00:35:45.099><c> encode</c><00:35:46.099><c> them</c><00:35:46.369><c> in</c><00:35:46.970><c> put</c><00:35:47.510><c> them</c><00:35:47.660><c> into</c><00:35:47.990><c> a</c>

00:35:48.220 --> 00:35:48.230 align:start position:0%
features encode them in put them into a
 

00:35:48.230 --> 00:35:49.870 align:start position:0%
features encode them in put them into a
recurrent<00:35:48.920><c> neural</c><00:35:49.040><c> network</c><00:35:49.520><c> which</c><00:35:49.730><c> we</c>

00:35:49.870 --> 00:35:49.880 align:start position:0%
recurrent neural network which we
 

00:35:49.880 --> 00:35:53.760 align:start position:0%
recurrent neural network which we
learned<00:35:50.089><c> about</c><00:35:50.240><c> yesterday</c><00:35:50.930><c> and</c><00:35:52.240><c> then</c>

00:35:53.760 --> 00:35:53.770 align:start position:0%
learned about yesterday and then
 

00:35:53.770 --> 00:35:57.359 align:start position:0%
learned about yesterday and then
generate<00:35:54.770><c> a</c><00:35:55.010><c> sentence</c><00:35:55.910><c> that</c><00:35:56.000><c> describes</c><00:35:56.630><c> the</c>

00:35:57.359 --> 00:35:57.369 align:start position:0%
generate a sentence that describes the
 

00:35:57.369 --> 00:35:59.710 align:start position:0%
generate a sentence that describes the
semantic<00:35:58.369><c> content</c><00:35:58.550><c> that's</c><00:35:59.000><c> present</c><00:35:59.480><c> in</c><00:35:59.540><c> that</c>

00:35:59.710 --> 00:35:59.720 align:start position:0%
semantic content that's present in that
 

00:35:59.720 --> 00:36:02.079 align:start position:0%
semantic content that's present in that
image<00:35:59.930><c> and</c><00:36:00.290><c> the</c><00:36:00.950><c> reason</c><00:36:01.220><c> we</c><00:36:01.339><c> can</c><00:36:01.490><c> do</c><00:36:01.670><c> this</c><00:36:01.849><c> is</c>

00:36:02.079 --> 00:36:02.089 align:start position:0%
image and the reason we can do this is
 

00:36:02.089 --> 00:36:04.089 align:start position:0%
image and the reason we can do this is
that<00:36:02.270><c> the</c><00:36:02.450><c> output</c><00:36:02.960><c> of</c><00:36:03.109><c> these</c><00:36:03.290><c> convolutional</c>

00:36:04.089 --> 00:36:04.099 align:start position:0%
that the output of these convolutional
 

00:36:04.099 --> 00:36:06.730 align:start position:0%
that the output of these convolutional
layers<00:36:04.369><c> gives</c><00:36:05.210><c> us</c><00:36:05.420><c> a</c><00:36:05.599><c> fixed</c><00:36:06.050><c> length</c><00:36:06.230><c> encoding</c>

00:36:06.730 --> 00:36:06.740 align:start position:0%
layers gives us a fixed length encoding
 

00:36:06.740 --> 00:36:09.970 align:start position:0%
layers gives us a fixed length encoding
that<00:36:07.660><c> initializes</c><00:36:08.660><c> our</c><00:36:09.079><c> that</c><00:36:09.530><c> we</c><00:36:09.650><c> can</c><00:36:09.800><c> use</c><00:36:09.950><c> to</c>

00:36:09.970 --> 00:36:09.980 align:start position:0%
that initializes our that we can use to
 

00:36:09.980 --> 00:36:14.339 align:start position:0%
that initializes our that we can use to
initialize<00:36:10.609><c> an</c><00:36:10.820><c> RNN</c><00:36:11.210><c> and</c><00:36:11.450><c> train</c><00:36:12.079><c> it</c><00:36:12.290><c> on</c><00:36:12.589><c> this</c>

00:36:14.339 --> 00:36:14.349 align:start position:0%
initialize an RNN and train it on this
 

00:36:14.349 --> 00:36:19.290 align:start position:0%
initialize an RNN and train it on this
captioning<00:36:15.349><c> task</c><00:36:17.109><c> so</c><00:36:18.109><c> these</c><00:36:18.349><c> are</c><00:36:18.470><c> three</c><00:36:18.829><c> very</c>

00:36:19.290 --> 00:36:19.300 align:start position:0%
captioning task so these are three very
 

00:36:19.300 --> 00:36:24.040 align:start position:0%
captioning task so these are three very
concrete<00:36:21.520><c> fundamental</c><00:36:22.520><c> applications</c><00:36:23.420><c> of</c><00:36:23.720><c> of</c>

00:36:24.040 --> 00:36:24.050 align:start position:0%
concrete fundamental applications of of
 

00:36:24.050 --> 00:36:28.030 align:start position:0%
concrete fundamental applications of of
CNN's<00:36:25.450><c> but</c><00:36:26.450><c> to</c><00:36:26.569><c> take</c><00:36:26.780><c> it</c><00:36:26.930><c> a</c><00:36:27.020><c> step</c><00:36:27.050><c> further</c><00:36:27.260><c> in</c>

00:36:28.030 --> 00:36:28.040 align:start position:0%
CNN's but to take it a step further in
 

00:36:28.040 --> 00:36:31.569 align:start position:0%
CNN's but to take it a step further in
terms<00:36:28.670><c> of</c><00:36:28.940><c> sort</c><00:36:29.839><c> of</c><00:36:29.960><c> the</c><00:36:30.560><c> depth</c><00:36:31.040><c> and</c><00:36:31.280><c> breadth</c>

00:36:31.569 --> 00:36:31.579 align:start position:0%
terms of sort of the depth and breadth
 

00:36:31.579 --> 00:36:33.069 align:start position:0%
terms of sort of the depth and breadth
of<00:36:31.819><c> impact</c><00:36:32.300><c> that</c><00:36:32.450><c> these</c><00:36:32.780><c> sort</c><00:36:33.020><c> of</c>

00:36:33.069 --> 00:36:33.079 align:start position:0%
of impact that these sort of
 

00:36:33.079 --> 00:36:35.410 align:start position:0%
of impact that these sort of
architectures<00:36:33.770><c> have</c><00:36:33.800><c> had</c><00:36:34.250><c> across</c><00:36:34.970><c> a</c><00:36:35.000><c> variety</c>

00:36:35.410 --> 00:36:35.420 align:start position:0%
architectures have had across a variety
 

00:36:35.420 --> 00:36:39.099 align:start position:0%
architectures have had across a variety
of<00:36:35.720><c> fields</c><00:36:36.550><c> I'd</c><00:36:37.550><c> like</c><00:36:37.760><c> to</c><00:36:37.910><c> first</c><00:36:38.150><c> appreciate</c>

00:36:39.099 --> 00:36:39.109 align:start position:0%
of fields I'd like to first appreciate
 

00:36:39.109 --> 00:36:41.710 align:start position:0%
of fields I'd like to first appreciate
the<00:36:39.500><c> fact</c><00:36:39.680><c> that</c><00:36:39.950><c> these</c><00:36:40.579><c> advances</c><00:36:41.240><c> would</c><00:36:41.480><c> not</c>

00:36:41.710 --> 00:36:41.720 align:start position:0%
the fact that these advances would not
 

00:36:41.720 --> 00:36:45.609 align:start position:0%
the fact that these advances would not
have<00:36:42.170><c> been</c><00:36:42.349><c> possible</c><00:36:42.619><c> without</c><00:36:43.520><c> the</c><00:36:44.619><c> curation</c>

00:36:45.609 --> 00:36:45.619 align:start position:0%
have been possible without the curation
 

00:36:45.619 --> 00:36:48.460 align:start position:0%
have been possible without the curation
and<00:36:45.920><c> availability</c><00:36:46.579><c> of</c><00:36:46.819><c> large</c><00:36:47.270><c> well</c><00:36:48.020><c> annotated</c>

00:36:48.460 --> 00:36:48.470 align:start position:0%
and availability of large well annotated
 

00:36:48.470 --> 00:36:52.030 align:start position:0%
and availability of large well annotated
image<00:36:49.160><c> datasets</c><00:36:49.640><c> and</c><00:36:49.940><c> this</c><00:36:50.569><c> is</c><00:36:50.750><c> what</c><00:36:51.200><c> has</c><00:36:51.829><c> been</c>

00:36:52.030 --> 00:36:52.040 align:start position:0%
image datasets and this is what has been
 

00:36:52.040 --> 00:36:54.309 align:start position:0%
image datasets and this is what has been
fundamental<00:36:52.760><c> to</c><00:36:52.790><c> really</c><00:36:53.660><c> rapidly</c>

00:36:54.309 --> 00:36:54.319 align:start position:0%
fundamental to really rapidly
 

00:36:54.319 --> 00:36:56.710 align:start position:0%
fundamental to really rapidly
accelerating<00:36:55.190><c> the</c><00:36:55.339><c> progress</c><00:36:55.790><c> in</c><00:36:56.660><c> the</c>

00:36:56.710 --> 00:36:56.720 align:start position:0%
accelerating the progress in the
 

00:36:56.720 --> 00:36:58.870 align:start position:0%
accelerating the progress in the
development<00:36:57.050><c> of</c><00:36:57.640><c> convolutional</c><00:36:58.640><c> neural</c>

00:36:58.870 --> 00:36:58.880 align:start position:0%
development of convolutional neural
 

00:36:58.880 --> 00:37:01.290 align:start position:0%
development of convolutional neural
networks<00:36:59.359><c> and</c><00:36:59.599><c> so</c><00:37:00.140><c> some</c><00:37:00.410><c> really</c><00:37:00.770><c> famous</c>

00:37:01.290 --> 00:37:01.300 align:start position:0%
networks and so some really famous
 

00:37:01.300 --> 00:37:04.180 align:start position:0%
networks and so some really famous
examples<00:37:02.300><c> of</c><00:37:02.540><c> image</c><00:37:03.109><c> data</c><00:37:03.290><c> sets</c><00:37:03.650><c> are</c><00:37:03.920><c> shown</c>

00:37:04.180 --> 00:37:04.190 align:start position:0%
examples of image data sets are shown
 

00:37:04.190 --> 00:37:08.920 align:start position:0%
examples of image data sets are shown
here<00:37:05.560><c> amnesty</c><00:37:06.560><c> in</c><00:37:06.740><c> today's</c><00:37:07.040><c> lab</c><00:37:07.510><c> image</c><00:37:08.510><c> net</c>

00:37:08.920 --> 00:37:08.930 align:start position:0%
here amnesty in today's lab image net
 

00:37:08.930 --> 00:37:12.540 align:start position:0%
here amnesty in today's lab image net
which<00:37:09.140><c> I</c><00:37:09.260><c> already</c><00:37:09.589><c> mentioned</c><00:37:10.460><c> and</c><00:37:10.900><c> the</c><00:37:11.900><c> places</c>

00:37:12.540 --> 00:37:12.550 align:start position:0%
which I already mentioned and the places
 

00:37:12.550 --> 00:37:15.010 align:start position:0%
which I already mentioned and the places
data<00:37:13.550><c> set</c><00:37:13.790><c> which</c><00:37:13.940><c> is</c><00:37:14.060><c> out</c><00:37:14.210><c> of</c><00:37:14.329><c> MIT</c><00:37:14.599><c> of</c>

00:37:15.010 --> 00:37:15.020 align:start position:0%
data set which is out of MIT of
 

00:37:15.020 --> 00:37:19.510 align:start position:0%
data set which is out of MIT of
different<00:37:15.950><c> scenes</c><00:37:16.250><c> and</c><00:37:16.640><c> landscapes</c><00:37:17.329><c> and</c><00:37:18.369><c> as</c><00:37:19.369><c> I</c>

00:37:19.510 --> 00:37:19.520 align:start position:0%
different scenes and landscapes and as I
 

00:37:19.520 --> 00:37:21.940 align:start position:0%
different scenes and landscapes and as I
as<00:37:19.970><c> I</c><00:37:20.150><c> sort</c><00:37:20.359><c> of</c><00:37:20.420><c> alluded</c><00:37:20.510><c> to</c><00:37:20.839><c> the</c><00:37:21.170><c> impact</c><00:37:21.710><c> of</c>

00:37:21.940 --> 00:37:21.950 align:start position:0%
as I sort of alluded to the impact of
 

00:37:21.950 --> 00:37:24.670 align:start position:0%
as I sort of alluded to the impact of
these<00:37:22.579><c> sorts</c><00:37:22.910><c> of</c><00:37:23.000><c> approaches</c><00:37:23.810><c> has</c><00:37:24.440><c> been</c>

00:37:24.670 --> 00:37:24.680 align:start position:0%
these sorts of approaches has been
 

00:37:24.680 --> 00:37:28.359 align:start position:0%
these sorts of approaches has been
extremely<00:37:25.310><c> far-reaching</c><00:37:25.880><c> and</c><00:37:26.329><c> and</c><00:37:26.660><c> deep</c><00:37:27.369><c> no</c>

00:37:28.359 --> 00:37:28.369 align:start position:0%
extremely far-reaching and and deep no
 

00:37:28.369 --> 00:37:31.630 align:start position:0%
extremely far-reaching and and deep no
pun<00:37:28.579><c> intended</c>

00:37:31.630 --> 00:37:31.640 align:start position:0%
 
 

00:37:31.640 --> 00:37:33.850 align:start position:0%
 
and<00:37:32.270><c> one</c><00:37:32.510><c> area</c><00:37:32.750><c> that</c><00:37:32.930><c> convolutional</c><00:37:33.650><c> neural</c>

00:37:33.850 --> 00:37:33.860 align:start position:0%
and one area that convolutional neural
 

00:37:33.860 --> 00:37:35.830 align:start position:0%
and one area that convolutional neural
networks<00:37:34.340><c> have</c><00:37:34.550><c> been</c><00:37:34.700><c> have</c><00:37:35.180><c> made</c><00:37:35.450><c> a</c><00:37:35.480><c> really</c>

00:37:35.830 --> 00:37:35.840 align:start position:0%
networks have been have made a really
 

00:37:35.840 --> 00:37:37.900 align:start position:0%
networks have been have made a really
big<00:37:35.990><c> impact</c><00:37:36.350><c> is</c><00:37:36.650><c> in</c><00:37:36.800><c> face</c><00:37:37.040><c> detection</c><00:37:37.730><c> and</c>

00:37:37.900 --> 00:37:37.910 align:start position:0%
big impact is in face detection and
 

00:37:37.910 --> 00:37:39.670 align:start position:0%
big impact is in face detection and
recognition<00:37:38.120><c> software</c><00:37:38.690><c> and</c><00:37:38.960><c> this</c><00:37:39.200><c> is</c><00:37:39.380><c> you</c>

00:37:39.670 --> 00:37:39.680 align:start position:0%
recognition software and this is you
 

00:37:39.680 --> 00:37:41.110 align:start position:0%
recognition software and this is you
know<00:37:39.770><c> every</c><00:37:40.100><c> time</c><00:37:40.280><c> you</c><00:37:40.430><c> pick</c><00:37:40.580><c> up</c><00:37:40.760><c> your</c><00:37:40.910><c> phone</c>

00:37:41.110 --> 00:37:41.120 align:start position:0%
know every time you pick up your phone
 

00:37:41.120 --> 00:37:43.540 align:start position:0%
know every time you pick up your phone
this<00:37:41.750><c> your</c><00:37:42.350><c> your</c><00:37:42.680><c> phone</c><00:37:42.860><c> is</c><00:37:43.070><c> running</c><00:37:43.250><c> these</c>

00:37:43.540 --> 00:37:43.550 align:start position:0%
this your your phone is running these
 

00:37:43.550 --> 00:37:45.640 align:start position:0%
this your your phone is running these
sorts<00:37:43.850><c> of</c><00:37:43.940><c> algorithms</c><00:37:44.480><c> to</c><00:37:44.900><c> pick</c><00:37:45.110><c> up</c><00:37:45.260><c> you</c><00:37:45.620><c> know</c>

00:37:45.640 --> 00:37:45.650 align:start position:0%
sorts of algorithms to pick up you know
 

00:37:45.650 --> 00:37:49.780 align:start position:0%
sorts of algorithms to pick up you know
your<00:37:46.580><c> face</c><00:37:46.820><c> and</c><00:37:46.850><c> your</c><00:37:47.480><c> friends</c><00:37:47.840><c> face</c><00:37:48.070><c> and</c><00:37:49.070><c> this</c>

00:37:49.780 --> 00:37:49.790 align:start position:0%
your face and your friends face and this
 

00:37:49.790 --> 00:37:51.220 align:start position:0%
your face and your friends face and this
type<00:37:50.090><c> of</c><00:37:50.120><c> software</c><00:37:50.540><c> is</c><00:37:50.750><c> pretty</c><00:37:51.050><c> much</c>

00:37:51.220 --> 00:37:51.230 align:start position:0%
type of software is pretty much
 

00:37:51.230 --> 00:37:53.680 align:start position:0%
type of software is pretty much
everywhere<00:37:51.740><c> from</c><00:37:52.100><c> social</c><00:37:52.490><c> media</c><00:37:52.520><c> to</c><00:37:53.030><c> security</c>

00:37:53.680 --> 00:37:53.690 align:start position:0%
everywhere from social media to security
 

00:37:53.690 --> 00:37:56.620 align:start position:0%
everywhere from social media to security
and<00:37:54.590><c> in</c><00:37:55.220><c> today's</c><00:37:55.520><c> lab</c><00:37:55.820><c> you'll</c><00:37:56.270><c> have</c><00:37:56.480><c> the</c>

00:37:56.620 --> 00:37:56.630 align:start position:0%
and in today's lab you'll have the
 

00:37:56.630 --> 00:37:59.650 align:start position:0%
and in today's lab you'll have the
chance<00:37:56.900><c> to</c><00:37:57.050><c> build</c><00:37:57.350><c> a</c><00:37:57.650><c> CNN</c><00:37:58.280><c> based</c><00:37:58.660><c> architecture</c>

00:37:59.650 --> 00:37:59.660 align:start position:0%
chance to build a CNN based architecture
 

00:37:59.660 --> 00:38:01.750 align:start position:0%
chance to build a CNN based architecture
for<00:37:59.900><c> facial</c><00:38:00.350><c> detection</c><00:38:00.500><c> and</c><00:38:01.040><c> you'll</c><00:38:01.580><c> actually</c>

00:38:01.750 --> 00:38:01.760 align:start position:0%
for facial detection and you'll actually
 

00:38:01.760 --> 00:38:04.230 align:start position:0%
for facial detection and you'll actually
take<00:38:02.120><c> this</c><00:38:02.270><c> a</c><00:38:02.330><c> step</c><00:38:02.450><c> further</c><00:38:02.690><c> by</c><00:38:03.290><c> exploring</c>

00:38:04.230 --> 00:38:04.240 align:start position:0%
take this a step further by exploring
 

00:38:04.240 --> 00:38:06.790 align:start position:0%
take this a step further by exploring
how<00:38:05.240><c> these</c><00:38:05.450><c> how</c><00:38:05.990><c> these</c><00:38:06.020><c> models</c><00:38:06.560><c> can</c><00:38:06.770><c> be</c>

00:38:06.790 --> 00:38:06.800 align:start position:0%
how these how these models can be
 

00:38:06.800 --> 00:38:09.580 align:start position:0%
how these how these models can be
potentially<00:38:07.490><c> biased</c><00:38:07.840><c> based</c><00:38:08.840><c> on</c><00:38:09.140><c> the</c><00:38:09.380><c> nature</c>

00:38:09.580 --> 00:38:09.590 align:start position:0%
potentially biased based on the nature
 

00:38:09.590 --> 00:38:12.570 align:start position:0%
potentially biased based on the nature
of<00:38:09.740><c> the</c><00:38:10.070><c> training</c><00:38:10.460><c> data</c><00:38:10.490><c> that</c><00:38:10.940><c> they</c><00:38:11.090><c> use</c>

00:38:12.570 --> 00:38:12.580 align:start position:0%
of the training data that they use
 

00:38:12.580 --> 00:38:15.280 align:start position:0%
of the training data that they use
another<00:38:13.580><c> application</c><00:38:13.850><c> area</c><00:38:14.600><c> that</c><00:38:14.810><c> has</c><00:38:14.990><c> led</c><00:38:15.260><c> to</c>

00:38:15.280 --> 00:38:15.290 align:start position:0%
another application area that has led to
 

00:38:15.290 --> 00:38:17.470 align:start position:0%
another application area that has led to
a<00:38:15.590><c> lot</c><00:38:15.800><c> of</c><00:38:15.830><c> excitement</c><00:38:16.280><c> is</c><00:38:16.580><c> in</c><00:38:16.910><c> autonomous</c>

00:38:17.470 --> 00:38:17.480 align:start position:0%
a lot of excitement is in autonomous
 

00:38:17.480 --> 00:38:22.060 align:start position:0%
a lot of excitement is in autonomous
vehicles<00:38:17.990><c> and</c><00:38:18.670><c> self-driving</c><00:38:19.670><c> cars</c><00:38:20.060><c> so</c><00:38:21.070><c> the</c>

00:38:22.060 --> 00:38:22.070 align:start position:0%
vehicles and self-driving cars so the
 

00:38:22.070 --> 00:38:24.160 align:start position:0%
vehicles and self-driving cars so the
man<00:38:22.280><c> in</c><00:38:22.460><c> this</c><00:38:22.580><c> video</c><00:38:22.790><c> was</c><00:38:23.210><c> a</c><00:38:23.450><c> guest</c><00:38:23.720><c> lecturer</c>

00:38:24.160 --> 00:38:24.170 align:start position:0%
man in this video was a guest lecturer
 

00:38:24.170 --> 00:38:26.740 align:start position:0%
man in this video was a guest lecturer
that<00:38:24.200><c> we</c><00:38:24.590><c> had</c><00:38:24.830><c> last</c><00:38:25.340><c> year</c><00:38:25.580><c> and</c><00:38:25.730><c> he</c><00:38:26.210><c> was</c><00:38:26.240><c> really</c>

00:38:26.740 --> 00:38:26.750 align:start position:0%
that we had last year and he was really
 

00:38:26.750 --> 00:38:30.130 align:start position:0%
that we had last year and he was really
fun<00:38:27.050><c> and</c><00:38:27.260><c> dynamic</c><00:38:27.820><c> and</c><00:38:28.820><c> this</c><00:38:29.390><c> is</c><00:38:29.570><c> work</c><00:38:29.780><c> from</c>

00:38:30.130 --> 00:38:30.140 align:start position:0%
fun and dynamic and this is work from
 

00:38:30.140 --> 00:38:32.230 align:start position:0%
fun and dynamic and this is work from
Nvidia<00:38:30.740><c> where</c><00:38:31.400><c> they</c><00:38:31.580><c> have</c><00:38:31.760><c> this</c><00:38:31.910><c> pipeline</c>

00:38:32.230 --> 00:38:32.240 align:start position:0%
Nvidia where they have this pipeline
 

00:38:32.240 --> 00:38:34.780 align:start position:0%
Nvidia where they have this pipeline
where<00:38:32.960><c> they</c><00:38:33.110><c> take</c><00:38:33.380><c> a</c><00:38:33.560><c> single</c><00:38:33.830><c> image</c><00:38:34.340><c> from</c><00:38:34.670><c> a</c>

00:38:34.780 --> 00:38:34.790 align:start position:0%
where they take a single image from a
 

00:38:34.790 --> 00:38:37.840 align:start position:0%
where they take a single image from a
camera<00:38:35.060><c> on</c><00:38:35.360><c> the</c><00:38:35.840><c> car</c><00:38:36.080><c> feed</c><00:38:36.890><c> it</c><00:38:37.040><c> into</c><00:38:37.190><c> a</c><00:38:37.340><c> CNN</c>

00:38:37.840 --> 00:38:37.850 align:start position:0%
camera on the car feed it into a CNN
 

00:38:37.850 --> 00:38:40.930 align:start position:0%
camera on the car feed it into a CNN
that<00:38:38.300><c> directly</c><00:38:38.840><c> outputs</c><00:38:39.410><c> a</c><00:38:39.820><c> single</c><00:38:40.820><c> number</c>

00:38:40.930 --> 00:38:40.940 align:start position:0%
that directly outputs a single number
 

00:38:40.940 --> 00:38:42.760 align:start position:0%
that directly outputs a single number
which<00:38:41.300><c> is</c><00:38:41.450><c> a</c><00:38:41.480><c> predicted</c><00:38:42.050><c> steering</c><00:38:42.590><c> wheel</c>

00:38:42.760 --> 00:38:42.770 align:start position:0%
which is a predicted steering wheel
 

00:38:42.770 --> 00:38:46.120 align:start position:0%
which is a predicted steering wheel
angle<00:38:43.340><c> and</c><00:38:44.110><c> Beyond</c><00:38:45.110><c> self-driving</c><00:38:45.710><c> cars</c>

00:38:46.120 --> 00:38:46.130 align:start position:0%
angle and Beyond self-driving cars
 

00:38:46.130 --> 00:38:48.520 align:start position:0%
angle and Beyond self-driving cars
NVIDIA<00:38:46.550><c> has</c><00:38:46.790><c> a</c><00:38:46.820><c> really</c><00:38:47.450><c> large-scale</c><00:38:47.990><c> research</c>

00:38:48.520 --> 00:38:48.530 align:start position:0%
NVIDIA has a really large-scale research
 

00:38:48.530 --> 00:38:50.920 align:start position:0%
NVIDIA has a really large-scale research
effort<00:38:49.130><c> that's</c><00:38:49.610><c> focused</c><00:38:50.330><c> on</c><00:38:50.450><c> computer</c><00:38:50.900><c> vision</c>

00:38:50.920 --> 00:38:50.930 align:start position:0%
effort that's focused on computer vision
 

00:38:50.930 --> 00:38:54.280 align:start position:0%
effort that's focused on computer vision
and<00:38:51.440><c> on</c><00:38:52.040><c> Friday</c><00:38:52.370><c> we'll</c><00:38:52.730><c> hear</c><00:38:53.120><c> from</c><00:38:53.330><c> the</c><00:38:54.020><c> leader</c>

00:38:54.280 --> 00:38:54.290 align:start position:0%
and on Friday we'll hear from the leader
 

00:38:54.290 --> 00:38:56.830 align:start position:0%
and on Friday we'll hear from the leader
of<00:38:54.620><c> Nvidia's</c><00:38:55.340><c> entire</c><00:38:55.730><c> computer</c><00:38:56.270><c> vision</c><00:38:56.600><c> team</c>

00:38:56.830 --> 00:38:56.840 align:start position:0%
of Nvidia's entire computer vision team
 

00:38:56.840 --> 00:38:59.890 align:start position:0%
of Nvidia's entire computer vision team
and<00:38:57.880><c> he'll</c><00:38:58.880><c> talk</c><00:38:59.090><c> about</c><00:38:59.330><c> some</c><00:38:59.570><c> of</c><00:38:59.660><c> the</c><00:38:59.780><c> latest</c>

00:38:59.890 --> 00:38:59.900 align:start position:0%
and he'll talk about some of the latest
 

00:38:59.900 --> 00:39:02.350 align:start position:0%
and he'll talk about some of the latest
and<00:39:00.200><c> greatest</c><00:39:00.500><c> research</c><00:39:00.950><c> that</c><00:39:01.880><c> they're</c><00:39:02.060><c> doing</c>

00:39:02.350 --> 00:39:02.360 align:start position:0%
and greatest research that they're doing
 

00:39:02.360 --> 00:39:06.160 align:start position:0%
and greatest research that they're doing
there<00:39:03.970><c> finally</c><00:39:04.970><c> there's</c><00:39:05.210><c> been</c><00:39:05.360><c> a</c><00:39:05.690><c> pretty</c>

00:39:06.160 --> 00:39:06.170 align:start position:0%
there finally there's been a pretty
 

00:39:06.170 --> 00:39:08.590 align:start position:0%
there finally there's been a pretty
significant<00:39:06.860><c> impact</c><00:39:07.010><c> of</c><00:39:07.580><c> of</c><00:39:07.880><c> these</c><00:39:08.180><c> types</c><00:39:08.450><c> of</c>

00:39:08.590 --> 00:39:08.600 align:start position:0%
significant impact of of these types of
 

00:39:08.600 --> 00:39:10.420 align:start position:0%
significant impact of of these types of
architectures<00:39:09.260><c> in</c><00:39:09.470><c> medicine</c><00:39:09.950><c> and</c><00:39:10.100><c> healthcare</c>

00:39:10.420 --> 00:39:10.430 align:start position:0%
architectures in medicine and healthcare
 

00:39:10.430 --> 00:39:12.700 align:start position:0%
architectures in medicine and healthcare
where<00:39:11.300><c> deep</c><00:39:11.660><c> learning</c><00:39:11.810><c> models</c><00:39:12.350><c> are</c><00:39:12.500><c> being</c>

00:39:12.700 --> 00:39:12.710 align:start position:0%
where deep learning models are being
 

00:39:12.710 --> 00:39:14.950 align:start position:0%
where deep learning models are being
applied<00:39:12.980><c> to</c><00:39:13.130><c> the</c><00:39:13.640><c> analysis</c><00:39:14.180><c> of</c><00:39:14.360><c> a</c><00:39:14.450><c> whole</c><00:39:14.720><c> host</c>

00:39:14.950 --> 00:39:14.960 align:start position:0%
applied to the analysis of a whole host
 

00:39:14.960 --> 00:39:18.430 align:start position:0%
applied to the analysis of a whole host
of<00:39:15.110><c> types</c><00:39:16.100><c> of</c><00:39:16.280><c> medical</c><00:39:17.090><c> images</c><00:39:17.540><c> so</c><00:39:18.110><c> this</c><00:39:18.290><c> is</c><00:39:18.410><c> a</c>

00:39:18.430 --> 00:39:18.440 align:start position:0%
of types of medical images so this is a
 

00:39:18.440 --> 00:39:20.440 align:start position:0%
of types of medical images so this is a
paper<00:39:18.740><c> from</c><00:39:19.130><c> Nature</c><00:39:19.490><c> Medicine</c><00:39:19.670><c> from</c><00:39:20.150><c> just</c><00:39:20.360><c> a</c>

00:39:20.440 --> 00:39:20.450 align:start position:0%
paper from Nature Medicine from just a
 

00:39:20.450 --> 00:39:23.080 align:start position:0%
paper from Nature Medicine from just a
few<00:39:20.630><c> weeks</c><00:39:20.810><c> ago</c><00:39:21.070><c> where</c><00:39:22.070><c> it</c><00:39:22.610><c> was</c><00:39:22.700><c> a</c><00:39:22.730><c> multi</c>

00:39:23.080 --> 00:39:23.090 align:start position:0%
few weeks ago where it was a multi
 

00:39:23.090 --> 00:39:26.230 align:start position:0%
few weeks ago where it was a multi
Institute<00:39:23.630><c> team</c><00:39:23.870><c> and</c><00:39:24.260><c> they</c><00:39:24.860><c> presented</c><00:39:25.220><c> a</c><00:39:25.700><c> CNN</c>

00:39:26.230 --> 00:39:26.240 align:start position:0%
Institute team and they presented a CNN
 

00:39:26.240 --> 00:39:29.430 align:start position:0%
Institute team and they presented a CNN
that<00:39:26.690><c> uses</c><00:39:27.140><c> a</c><00:39:27.170><c> pretty</c><00:39:27.890><c> standard</c><00:39:28.340><c> architecture</c>

00:39:29.430 --> 00:39:29.440 align:start position:0%
that uses a pretty standard architecture
 

00:39:29.440 --> 00:39:33.330 align:start position:0%
that uses a pretty standard architecture
to<00:39:30.440><c> identify</c><00:39:31.160><c> rare</c><00:39:31.700><c> genetic</c><00:39:32.000><c> conditions</c><00:39:32.900><c> from</c>

00:39:33.330 --> 00:39:33.340 align:start position:0%
to identify rare genetic conditions from
 

00:39:33.340 --> 00:39:36.340 align:start position:0%
to identify rare genetic conditions from
analysis<00:39:34.340><c> of</c><00:39:34.370><c> just</c><00:39:34.910><c> a</c><00:39:35.030><c> picture</c><00:39:35.360><c> of</c><00:39:35.570><c> a</c><00:39:35.840><c> child's</c>

00:39:36.340 --> 00:39:36.350 align:start position:0%
analysis of just a picture of a child's
 

00:39:36.350 --> 00:39:39.310 align:start position:0%
analysis of just a picture of a child's
face<00:39:36.560><c> and</c><00:39:36.890><c> in</c><00:39:37.790><c> their</c><00:39:37.940><c> paper</c><00:39:38.180><c> they</c><00:39:38.360><c> report</c><00:39:39.170><c> that</c>

00:39:39.310 --> 00:39:39.320 align:start position:0%
face and in their paper they report that
 

00:39:39.320 --> 00:39:41.290 align:start position:0%
face and in their paper they report that
their<00:39:39.470><c> model</c><00:39:39.890><c> can</c><00:39:40.100><c> actually</c><00:39:40.640><c> outperform</c>

00:39:41.290 --> 00:39:41.300 align:start position:0%
their model can actually outperform
 

00:39:41.300 --> 00:39:43.190 align:start position:0%
their model can actually outperform
physicians<00:39:42.200><c> when</c><00:39:42.830><c> Tess</c>

00:39:43.190 --> 00:39:43.200 align:start position:0%
physicians when Tess
 

00:39:43.200 --> 00:39:45.530 align:start position:0%
physicians when Tess
on<00:39:43.440><c> a</c><00:39:43.500><c> set</c><00:39:44.130><c> of</c><00:39:44.250><c> images</c><00:39:44.640><c> that</c><00:39:44.819><c> are</c><00:39:44.940><c> would</c><00:39:45.359><c> be</c>

00:39:45.530 --> 00:39:45.540 align:start position:0%
on a set of images that are would be
 

00:39:45.540 --> 00:39:49.970 align:start position:0%
on a set of images that are would be
relevant<00:39:45.869><c> to</c><00:39:46.079><c> a</c><00:39:47.240><c> clinical</c><00:39:48.240><c> scenario</c><00:39:48.690><c> and</c><00:39:49.049><c> one</c>

00:39:49.970 --> 00:39:49.980 align:start position:0%
relevant to a clinical scenario and one
 

00:39:49.980 --> 00:39:51.770 align:start position:0%
relevant to a clinical scenario and one
reason<00:39:50.309><c> that</c><00:39:50.339><c> work</c><00:39:50.970><c> like</c><00:39:51.180><c> this</c><00:39:51.359><c> is</c><00:39:51.540><c> really</c>

00:39:51.770 --> 00:39:51.780 align:start position:0%
reason that work like this is really
 

00:39:51.780 --> 00:39:54.140 align:start position:0%
reason that work like this is really
exciting<00:39:52.319><c> is</c><00:39:52.500><c> because</c><00:39:52.859><c> it</c><00:39:53.040><c> presents</c><00:39:53.220><c> sort</c><00:39:54.059><c> of</c>

00:39:54.140 --> 00:39:54.150 align:start position:0%
exciting is because it presents sort of
 

00:39:54.150 --> 00:39:56.329 align:start position:0%
exciting is because it presents sort of
another<00:39:54.420><c> standard</c><00:39:54.990><c> approach</c><00:39:55.589><c> standardized</c>

00:39:56.329 --> 00:39:56.339 align:start position:0%
another standard approach standardized
 

00:39:56.339 --> 00:39:58.970 align:start position:0%
another standard approach standardized
approach<00:39:56.790><c> to</c><00:39:57.510><c> identifying</c><00:39:58.290><c> and</c><00:39:58.619><c> diagnosing</c>

00:39:58.970 --> 00:39:58.980 align:start position:0%
approach to identifying and diagnosing
 

00:39:58.980 --> 00:40:02.180 align:start position:0%
approach to identifying and diagnosing
in<00:39:59.790><c> this</c><00:39:59.940><c> case</c><00:40:00.150><c> genetic</c><00:40:00.900><c> disorders</c><00:40:01.349><c> and</c><00:40:01.559><c> you</c>

00:40:02.180 --> 00:40:02.190 align:start position:0%
in this case genetic disorders and you
 

00:40:02.190 --> 00:40:03.829 align:start position:0%
in this case genetic disorders and you
can<00:40:02.339><c> imagine</c><00:40:02.520><c> that</c><00:40:02.880><c> this</c><00:40:03.000><c> could</c><00:40:03.059><c> be</c><00:40:03.390><c> combined</c>

00:40:03.829 --> 00:40:03.839 align:start position:0%
can imagine that this could be combined
 

00:40:03.839 --> 00:40:07.010 align:start position:0%
can imagine that this could be combined
with<00:40:04.099><c> already</c><00:40:05.099><c> existing</c><00:40:05.609><c> clinical</c><00:40:05.790><c> tests</c><00:40:06.450><c> to</c>

00:40:07.010 --> 00:40:07.020 align:start position:0%
with already existing clinical tests to
 

00:40:07.020 --> 00:40:11.980 align:start position:0%
with already existing clinical tests to
improve<00:40:08.569><c> classification</c><00:40:09.569><c> or</c><00:40:09.809><c> subtyping</c>

00:40:11.980 --> 00:40:11.990 align:start position:0%
improve classification or subtyping
 

00:40:11.990 --> 00:40:15.109 align:start position:0%
improve classification or subtyping
alright<00:40:12.990><c> so</c><00:40:13.349><c> to</c><00:40:13.410><c> summarize</c><00:40:14.099><c> what</c><00:40:14.940><c> we've</c>

00:40:15.109 --> 00:40:15.119 align:start position:0%
alright so to summarize what we've
 

00:40:15.119 --> 00:40:17.780 align:start position:0%
alright so to summarize what we've
covered<00:40:15.390><c> in</c><00:40:15.960><c> today's</c><00:40:16.260><c> lecture</c><00:40:16.619><c> we</c><00:40:17.309><c> first</c>

00:40:17.780 --> 00:40:17.790 align:start position:0%
covered in today's lecture we first
 

00:40:17.790 --> 00:40:20.089 align:start position:0%
covered in today's lecture we first
considered<00:40:18.270><c> sort</c><00:40:18.900><c> of</c><00:40:18.990><c> the</c><00:40:19.049><c> origins</c><00:40:19.799><c> of</c><00:40:19.950><c> the</c>

00:40:20.089 --> 00:40:20.099 align:start position:0%
considered sort of the origins of the
 

00:40:20.099 --> 00:40:22.430 align:start position:0%
considered sort of the origins of the
computer<00:40:20.490><c> vision</c><00:40:20.819><c> problem</c><00:40:21.299><c> and</c><00:40:21.480><c> how</c><00:40:22.109><c> we</c><00:40:22.170><c> can</c>

00:40:22.430 --> 00:40:22.440 align:start position:0%
computer vision problem and how we can
 

00:40:22.440 --> 00:40:25.579 align:start position:0%
computer vision problem and how we can
represent<00:40:22.700><c> images</c><00:40:23.700><c> as</c><00:40:23.880><c> arrays</c><00:40:24.750><c> of</c><00:40:25.109><c> brightness</c>

00:40:25.579 --> 00:40:25.589 align:start position:0%
represent images as arrays of brightness
 

00:40:25.589 --> 00:40:28.640 align:start position:0%
represent images as arrays of brightness
values<00:40:25.980><c> and</c><00:40:26.359><c> what</c><00:40:27.359><c> convolutions</c><00:40:28.020><c> are</c><00:40:28.230><c> and</c><00:40:28.530><c> how</c>

00:40:28.640 --> 00:40:28.650 align:start position:0%
values and what convolutions are and how
 

00:40:28.650 --> 00:40:31.250 align:start position:0%
values and what convolutions are and how
they<00:40:28.859><c> work</c><00:40:28.890><c> and</c><00:40:29.390><c> we</c><00:40:30.390><c> then</c><00:40:30.599><c> discussed</c><00:40:31.079><c> the</c>

00:40:31.250 --> 00:40:31.260 align:start position:0%
they work and we then discussed the
 

00:40:31.260 --> 00:40:33.890 align:start position:0%
they work and we then discussed the
basic<00:40:31.890><c> architecture</c><00:40:32.099><c> of</c><00:40:32.900><c> convolutional</c>

00:40:33.890 --> 00:40:33.900 align:start position:0%
basic architecture of convolutional
 

00:40:33.900 --> 00:40:36.530 align:start position:0%
basic architecture of convolutional
neural<00:40:34.109><c> networks</c><00:40:34.619><c> and</c><00:40:35.180><c> kind</c><00:40:36.180><c> of</c><00:40:36.240><c> went</c><00:40:36.420><c> in</c>

00:40:36.530 --> 00:40:36.540 align:start position:0%
neural networks and kind of went in
 

00:40:36.540 --> 00:40:38.599 align:start position:0%
neural networks and kind of went in
depth<00:40:36.809><c> on</c><00:40:37.020><c> how</c><00:40:37.049><c> cnn's</c><00:40:37.710><c> can</c><00:40:37.950><c> be</c><00:40:38.099><c> used</c><00:40:38.400><c> for</c>

00:40:38.599 --> 00:40:38.609 align:start position:0%
depth on how cnn's can be used for
 

00:40:38.609 --> 00:40:40.970 align:start position:0%
depth on how cnn's can be used for
classification<00:40:39.030><c> and</c><00:40:39.599><c> finally</c><00:40:40.470><c> we</c><00:40:40.589><c> talked</c><00:40:40.859><c> a</c>

00:40:40.970 --> 00:40:40.980 align:start position:0%
classification and finally we talked a
 

00:40:40.980 --> 00:40:42.559 align:start position:0%
classification and finally we talked a
bit<00:40:41.160><c> about</c><00:40:41.309><c> extensions</c><00:40:42.270><c> and</c><00:40:42.510><c> the</c>

00:40:42.559 --> 00:40:42.569 align:start position:0%
bit about extensions and the
 

00:40:42.569 --> 00:40:44.720 align:start position:0%
bit about extensions and the
applications<00:40:43.230><c> of</c><00:40:43.410><c> the</c><00:40:43.950><c> basic</c><00:40:44.309><c> CNN</c>

00:40:44.720 --> 00:40:44.730 align:start position:0%
applications of the basic CNN
 

00:40:44.730 --> 00:40:47.809 align:start position:0%
applications of the basic CNN
architecture<00:40:45.450><c> and</c><00:40:45.660><c> why</c><00:40:46.589><c> they</c><00:40:47.220><c> have</c><00:40:47.579><c> been</c><00:40:47.760><c> so</c>

00:40:47.809 --> 00:40:47.819 align:start position:0%
architecture and why they have been so
 

00:40:47.819 --> 00:40:52.460 align:start position:0%
architecture and why they have been so
impactful<00:40:48.690><c> over</c><00:40:49.589><c> the</c><00:40:49.710><c> past</c><00:40:49.920><c> several</c><00:40:50.190><c> years</c><00:40:51.470><c> so</c>

00:40:52.460 --> 00:40:52.470 align:start position:0%
impactful over the past several years so
 

00:40:52.470 --> 00:40:54.980 align:start position:0%
impactful over the past several years so
I'm<00:40:52.950><c> happy</c><00:40:53.700><c> to</c><00:40:53.940><c> take</c><00:40:54.119><c> questions</c><00:40:54.329><c> at</c><00:40:54.839><c> the</c><00:40:54.960><c> end</c>

00:40:54.980 --> 00:40:54.990 align:start position:0%
I'm happy to take questions at the end
 

00:40:54.990 --> 00:40:57.230 align:start position:0%
I'm happy to take questions at the end
of<00:40:55.319><c> the</c><00:40:55.470><c> at</c><00:40:56.099><c> the</c><00:40:56.220><c> end</c><00:40:56.339><c> of</c><00:40:56.430><c> the</c><00:40:56.549><c> lecture</c><00:40:56.880><c> portion</c>

00:40:57.230 --> 00:40:57.240 align:start position:0%
of the at the end of the lecture portion
 

00:40:57.240 --> 00:41:00.859 align:start position:0%
of the at the end of the lecture portion
you<00:40:58.020><c> feel</c><00:40:58.619><c> free</c><00:40:59.220><c> feel</c><00:40:59.430><c> free</c><00:40:59.940><c> to</c><00:40:59.970><c> come</c><00:41:00.329><c> to</c><00:41:00.540><c> the</c>

00:41:00.859 --> 00:41:00.869 align:start position:0%
you feel free feel free to come to the
 

00:41:00.869 --> 00:41:04.940 align:start position:0%
you feel free feel free to come to the
front<00:41:01.079><c> to</c><00:41:01.430><c> speak</c><00:41:02.430><c> to</c><00:41:02.549><c> Alexander</c><00:41:03.059><c> or</c><00:41:03.299><c> myself</c><00:41:03.950><c> so</c>

00:41:04.940 --> 00:41:04.950 align:start position:0%
front to speak to Alexander or myself so
 

00:41:04.950 --> 00:41:07.160 align:start position:0%
front to speak to Alexander or myself so
with<00:41:05.609><c> that</c><00:41:05.760><c> I'm</c><00:41:06.390><c> going</c><00:41:06.630><c> to</c><00:41:06.720><c> hand</c><00:41:06.900><c> it</c><00:41:06.990><c> off</c><00:41:07.140><c> to</c>

00:41:07.160 --> 00:41:07.170 align:start position:0%
with that I'm going to hand it off to
 

00:41:07.170 --> 00:41:12.580 align:start position:0%
with that I'm going to hand it off to
him<00:41:07.530><c> for</c><00:41:08.430><c> the</c><00:41:09.030><c> second</c><00:41:09.359><c> lecture</c><00:41:09.510><c> of</c><00:41:09.869><c> today</c>

00:41:12.580 --> 00:41:12.590 align:start position:0%
him for the second lecture of today
 

00:41:12.590 --> 00:41:18.050 align:start position:0%
 
[Applause]

