WEBVTT
Kind: captions
Language: en

00:00:00.290 --> 00:00:03.960
So the argument for
RAID 6 being an overkill is that

00:00:03.960 --> 00:00:08.870
when in RAID 5 a disk fails, it takes
up to a couple of days to replace.

00:00:08.870 --> 00:00:12.080
Sometimes you have a spare disk and
usually a data center will have that,

00:00:12.080 --> 00:00:13.360
so it's going to take less than an hour.

00:00:14.480 --> 00:00:19.199
And there is a very, very low
probability that another disk in

00:00:19.199 --> 00:00:22.790
the array will fail in those three days.

00:00:22.790 --> 00:00:26.310
If it fails after these three days,
it doesn't matter because by then we

00:00:26.310 --> 00:00:30.170
are back to being protected
by the extra disk.

00:00:30.170 --> 00:00:34.560
So the failure of the second
disk needs to happen in the time

00:00:34.560 --> 00:00:36.950
when one of the disk is
in the failed state.

00:00:38.080 --> 00:00:42.860
But that assumes that the disk
failures are independent.

00:00:42.860 --> 00:00:45.200
But the failures can be
related to each other,

00:00:45.200 --> 00:00:48.410
in which case this is
not correct to assume.

00:00:48.410 --> 00:00:51.010
A typical scenario where
this can happen is,

00:00:51.010 --> 00:00:55.550
let's say that we have a RAID five with
five disks and one of the disk fails.

00:00:55.550 --> 00:00:58.510
Let's say that disk number
two in the array has failed.

00:00:58.510 --> 00:01:01.430
The system reports that
a disk has failed and

00:01:01.430 --> 00:01:03.180
continues functioning normally.

00:01:03.180 --> 00:01:06.920
Remember than it can do that
with one failed disk in RAID 5.

00:01:06.920 --> 00:01:11.650
And now the operator gets a replacement
disk, opens the computer case and

00:01:11.650 --> 00:01:15.980
sees five drives like this,
knows that the number two has failed so

00:01:15.980 --> 00:01:19.540
takes this one out and
puts the new drive in.

00:01:19.540 --> 00:01:24.370
But for example, the numbering of
the disks was actually 0,1,2,3,4 so

00:01:24.370 --> 00:01:26.890
we pulled out the wrong drive.

00:01:26.890 --> 00:01:29.000
This was the failed one.

00:01:29.000 --> 00:01:33.730
When we pulled the second one,
that was the second failure, and

00:01:33.730 --> 00:01:36.230
now we really have two failed disks.

00:01:36.230 --> 00:01:38.950
One failed because it failed,

00:01:38.950 --> 00:01:43.390
the other one failed in the course of
trying to repair the first failure.

00:01:43.390 --> 00:01:47.060
So that's, for example,
a case of correlated failures,

00:01:47.060 --> 00:01:50.610
and in that case,
we cannot assume independence.

00:01:50.610 --> 00:01:56.040
So pretty much when a disk fails,
the actions to repair it are creating

00:01:56.040 --> 00:02:01.430
a lot more risk for the other disks and
may cause a second failure.

00:02:01.430 --> 00:02:05.720
And that, for example, where you
want to have RAID 6, when one fails and

00:02:05.720 --> 00:02:06.799
you're trying to repair it.

00:02:07.810 --> 00:02:11.750
If another one fails for
any reason, independent or

00:02:11.750 --> 00:02:14.830
dependent on the first failure,
you're still safe.

