WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.880
[MUSIC PLAYING]

00:00:08.442 --> 00:00:09.650
JOSH DILLON: Hello, everyone.

00:00:09.650 --> 00:00:13.000
I'm Josh Dillon, and I'm a lead
on the TensorFlow probability

00:00:13.000 --> 00:00:13.880
team.

00:00:13.880 --> 00:00:16.540
And today, I'm going to talk
to you about probability stuff

00:00:16.540 --> 00:00:18.740
and how it relates
to TensorFlow stuff.

00:00:18.740 --> 00:00:21.750
So let's find out
what that means.

00:00:21.750 --> 00:00:24.360
OK, so these days,
machine learning

00:00:24.360 --> 00:00:26.640
often means specifying
deep model architectures

00:00:26.640 --> 00:00:28.620
and then fitting
them under some loss.

00:00:28.620 --> 00:00:31.080
And happily, Keras makes
specifying model architecture

00:00:31.080 --> 00:00:32.420
relatively easy.

00:00:32.420 --> 00:00:33.660
But what about the loss?

00:00:33.660 --> 00:00:35.940
Choosing the right
loss is tough.

00:00:35.940 --> 00:00:38.370
Improving one-- even
a reasonable one--

00:00:38.370 --> 00:00:39.780
can be even tougher.

00:00:39.780 --> 00:00:42.240
And once you fit your model,
how do you know it's good?

00:00:42.240 --> 00:00:44.250
Does accuracy tell
the full picture?

00:00:44.250 --> 00:00:46.938
Why not use mean, entropy, mode?

00:00:46.938 --> 00:00:48.480
Wouldn't it be great
if there existed

00:00:48.480 --> 00:00:51.840
some mathematical framework,
which unified these ideas?

00:00:51.840 --> 00:00:53.280
Better still,
wouldn't it be nice

00:00:53.280 --> 00:00:56.430
if it was plug and play with
Keras and the rest of TF?

00:00:56.430 --> 00:00:59.610
This would make comparing models
easier by simply maximizing

00:00:59.610 --> 00:01:01.500
likelihood and having
readily available

00:01:01.500 --> 00:01:03.420
evaluative statistics.

00:01:03.420 --> 00:01:05.670
We can rapidly prototype
different generating

00:01:05.670 --> 00:01:08.480
assumptions and quickly
reject the bad ones.

00:01:08.480 --> 00:01:11.730
In short, wouldn't it be
great if we could do this--

00:01:11.730 --> 00:01:14.580
just say I want to
maximize the log likelihood

00:01:14.580 --> 00:01:17.580
and then summarize
what I learned easily

00:01:17.580 --> 00:01:20.380
and in a unified way?

00:01:20.380 --> 00:01:22.330
So let's play with that idea.

00:01:22.330 --> 00:01:26.020
Here, we have a data
set-- these blue dots.

00:01:26.020 --> 00:01:28.870
And our task--
our pretend task--

00:01:28.870 --> 00:01:32.890
is to predict the y-coordinate
from the x-coordinate.

00:01:32.890 --> 00:01:36.400
And the way you might do this
is specify some deep model.

00:01:36.400 --> 00:01:39.280
And of course, you might
choose the mean squared error

00:01:39.280 --> 00:01:41.420
as your loss function.

00:01:41.420 --> 00:01:43.420
OK.

00:01:43.420 --> 00:01:47.830
But our wish here is to
think probabilistically.

00:01:47.830 --> 00:01:50.830
And so that means maximizing
the log likelihood,

00:01:50.830 --> 00:01:53.030
as indicated here with
this lambda function--

00:01:53.030 --> 00:01:56.110
the negative random
variable log_prob under y.

00:01:56.110 --> 00:01:57.970
And what we want,
in addition to that,

00:01:57.970 --> 00:01:59.920
is to get back a
distribution-- a thing

00:01:59.920 --> 00:02:01.810
that has attached
to it statistics

00:02:01.810 --> 00:02:05.720
that we can use to evaluate
what we just learned.

00:02:05.720 --> 00:02:08.210
If only such a
thing were possible.

00:02:08.210 --> 00:02:11.420
Of course, it is, and
you can do this now.

00:02:11.420 --> 00:02:15.350
Using TensorFlow probability
distribution layers,

00:02:15.350 --> 00:02:20.930
you can specify the model
as part of your deep net.

00:02:20.930 --> 00:02:24.020
And the loss now
is actually part

00:02:24.020 --> 00:02:26.490
of the model, sort of
the way it used to be--

00:02:26.490 --> 00:02:28.250
the way it's meant to be.

00:02:28.250 --> 00:02:30.000
And so let's unpack
what's happening here.

00:02:30.000 --> 00:02:31.700
So we have two dense layers.

00:02:31.700 --> 00:02:33.200
That's sort of
business as usual.

00:02:33.200 --> 00:02:35.960
The second one
outputs one float,

00:02:35.960 --> 00:02:37.880
and that one float
is parameterizing

00:02:37.880 --> 00:02:40.280
a normal distributions mean.

00:02:40.280 --> 00:02:44.820
And that's being done through
this distribution lambda layer.

00:02:44.820 --> 00:02:47.600
In so being, we're
able to find this line.

00:02:47.600 --> 00:02:48.680
That looks great.

00:02:48.680 --> 00:02:52.820
And the best part is,
once we instantiate

00:02:52.820 --> 00:02:55.760
this model with test points,
we have back a distribution

00:02:55.760 --> 00:02:56.840
instance--

00:02:56.840 --> 00:02:58.880
for which you get not
just the mean, which

00:02:58.880 --> 00:03:00.770
is what you'd get
today, but also--

00:03:00.770 --> 00:03:04.310
get not just the mean, which is
what you'd get today, but also

00:03:04.310 --> 00:03:07.670
entropy variance, standard
deviation, all of these things.

00:03:07.670 --> 00:03:10.340
And you can even compare between
this and other distributions,

00:03:10.340 --> 00:03:12.700
as we'll see later.

00:03:12.700 --> 00:03:14.860
But if we look at
this data, something's

00:03:14.860 --> 00:03:16.780
still a little
fishy here, right?

00:03:16.780 --> 00:03:21.670
Notice that as the magnitude of
x increases, the variance of y

00:03:21.670 --> 00:03:22.960
also seems to increase.

00:03:22.960 --> 00:03:25.940
So that means that maybe our
model's a little suspicious.

00:03:25.940 --> 00:03:28.920
So since we're in this
probabilistic framework

00:03:28.920 --> 00:03:30.760
and we're no longer
doing loss hacking--

00:03:30.760 --> 00:03:32.450
we're actually
building a model--

00:03:32.450 --> 00:03:34.120
what can we do to fix this?

00:03:34.120 --> 00:03:37.262
Answer-- learn the variance too.

00:03:37.262 --> 00:03:38.470
It's actually pretty obvious.

00:03:38.470 --> 00:03:40.330
If we're fitting a
normal, why on earth

00:03:40.330 --> 00:03:42.400
do we think that the
variance would just be 1?

00:03:42.400 --> 00:03:43.817
And by the way,
that's what you're

00:03:43.817 --> 00:03:45.730
doing when you use
mean squared error.

00:03:45.730 --> 00:03:48.730
And so now, to achieve
this, all I had to do

00:03:48.730 --> 00:03:51.330
is make my previous
layer output two floats.

00:03:51.330 --> 00:03:53.440
I pass one in as the
mean to the normal, one

00:03:53.440 --> 00:03:57.310
in as the standard
deviation of the normal.

00:03:57.310 --> 00:04:02.320
And presto chango, now I've
learned the standard deviation

00:04:02.320 --> 00:04:03.800
from the data itself.

00:04:03.800 --> 00:04:05.890
That's what the green lines are.

00:04:05.890 --> 00:04:09.040
So this is really cool, because
now, if you're a statistician,

00:04:09.040 --> 00:04:13.150
you would say, hey, I'm able
to handle heteroscedasticity.

00:04:13.150 --> 00:04:14.920
If you want a $10
word, you can call

00:04:14.920 --> 00:04:16.990
this aleatoric uncertainty.

00:04:16.990 --> 00:04:19.029
And what this really
means is that you're

00:04:19.029 --> 00:04:20.500
learning known unknowns.

00:04:20.500 --> 00:04:23.500
It means that the data
itself had variance,

00:04:23.500 --> 00:04:24.640
and you learned it.

00:04:24.640 --> 00:04:26.440
And it cost you
basically nothing

00:04:26.440 --> 00:04:28.660
to do but a few keystrokes.

00:04:28.660 --> 00:04:31.630
And furthermore, the way in
which you saw how to do this

00:04:31.630 --> 00:04:33.362
was self-evident
from the very fact

00:04:33.362 --> 00:04:35.320
that you were using a
normal distribution which

00:04:35.320 --> 00:04:38.740
had this curious constant
just sitting there.

00:04:38.740 --> 00:04:39.910
So this is good.

00:04:39.910 --> 00:04:41.350
But, hm, I don't know.

00:04:41.350 --> 00:04:43.900
Is there enough data for
which we can reliably

00:04:43.900 --> 00:04:45.888
claim that this red line
is actually the mean,

00:04:45.888 --> 00:04:47.305
and these green
lines are actually

00:04:47.305 --> 00:04:48.672
the standard deviation?

00:04:48.672 --> 00:04:50.380
How would we know if
we have enough data?

00:04:50.380 --> 00:04:52.620
Is there anything
else we can do?

00:04:52.620 --> 00:04:54.060
Of course, there is.

00:04:54.060 --> 00:04:56.965
Why learn just a
single set of weights?

00:04:56.965 --> 00:05:00.810
A Keras dense slayer has two
components-- a kernel matrix

00:05:00.810 --> 00:05:02.460
and a bias vector.

00:05:02.460 --> 00:05:05.580
What makes you think that
those point estimates are

00:05:05.580 --> 00:05:09.660
the best, especially given that
your data set itself is random

00:05:09.660 --> 00:05:13.230
and possibly inadequate to
meaningfully and reliably learn

00:05:13.230 --> 00:05:14.800
those point estimates?

00:05:14.800 --> 00:05:17.070
Instead, if you use a
TensorFlow probability

00:05:17.070 --> 00:05:20.430
dense variational layer, you can
actually learn a distribution

00:05:20.430 --> 00:05:21.640
overweight.

00:05:21.640 --> 00:05:24.750
This is the same as
learning an ensemble that's

00:05:24.750 --> 00:05:26.350
infinitely large.

00:05:26.350 --> 00:05:28.140
But luckily, it
doesn't take infinitely

00:05:28.140 --> 00:05:29.790
long to train this ensemble.

00:05:29.790 --> 00:05:32.040
In fact, it takes just
a little bit longer

00:05:32.040 --> 00:05:35.610
than what it took to train
on the previous slides.

00:05:35.610 --> 00:05:37.970
And as you can see
here, all I had to do

00:05:37.970 --> 00:05:41.620
is replace Keras.dense with
TFP.dense variational layer,

00:05:41.620 --> 00:05:43.100
and in so doing,
achieve this kind

00:05:43.100 --> 00:05:45.340
of Bayesian weight uncertainty.

00:05:45.340 --> 00:05:48.650
The $10 word here is
epistemic uncertainty.

00:05:48.650 --> 00:05:51.200
But again, I like to think
of it as unknown unknowns.

00:05:51.200 --> 00:05:53.490
I'm not sure what my
data is not telling me,

00:05:53.490 --> 00:05:55.580
so I'm going to be careful
in the bookkeeping I

00:05:55.580 --> 00:05:58.710
make when tracking the
weights that I learn.

00:05:58.710 --> 00:06:00.150
As a consequence,
of course, this

00:06:00.150 --> 00:06:03.300
means that any model you make,
any instantiation of this model

00:06:03.300 --> 00:06:05.550
is now actually a random
variable because the weight's

00:06:05.550 --> 00:06:06.660
a random variable.

00:06:06.660 --> 00:06:09.955
And that's why you see
here all of the lines.

00:06:09.955 --> 00:06:10.830
There are many lines.

00:06:10.830 --> 00:06:12.550
We have an ensemble of them.

00:06:12.550 --> 00:06:14.850
But if you were to average
those and take the sample

00:06:14.850 --> 00:06:16.530
standard deviation,
say, then that

00:06:16.530 --> 00:06:18.540
would give you an estimate
of credible intervals

00:06:18.540 --> 00:06:19.560
over your prediction.

00:06:19.560 --> 00:06:20.820
So now you can go
to your customer

00:06:20.820 --> 00:06:22.820
and say, look, here's
what I think would happen,

00:06:22.820 --> 00:06:25.050
and here's how much
you should trust me.

00:06:25.050 --> 00:06:26.700
So this is great, right?

00:06:26.700 --> 00:06:29.113
But we seem to have lost
the heteroscedastic part.

00:06:29.113 --> 00:06:30.780
Notice that the blue
dots are still more

00:06:30.780 --> 00:06:32.800
dispersed on the
right-hand side.

00:06:32.800 --> 00:06:34.440
So can we do both?

00:06:34.440 --> 00:06:35.190
Of course, we can.

00:06:35.190 --> 00:06:36.600
It's all modular.

00:06:36.600 --> 00:06:39.930
I just have my dense operational
layer output 2 floats, instead

00:06:39.930 --> 00:06:41.730
of one, like we did before.

00:06:41.730 --> 00:06:45.540
Feed that into my output layer,
which is a normal distribution.

00:06:45.540 --> 00:06:49.500
And presto chango, I'm
learning both known and unknown

00:06:49.500 --> 00:06:54.100
unknowns, and all it cost
me was a few keystrokes.

00:06:54.100 --> 00:06:57.600
And so what you see here now
is an ensemble of standard

00:06:57.600 --> 00:07:02.030
deviations associated with
the known unknown parts--

00:07:02.030 --> 00:07:06.000
the variance present or
observable in the y-axis--

00:07:06.000 --> 00:07:08.580
as well as a number
or an ensemble

00:07:08.580 --> 00:07:12.080
of these mean regressions.

00:07:12.080 --> 00:07:13.240
OK, that's cool.

00:07:13.240 --> 00:07:14.810
So I like where this is going.

00:07:14.810 --> 00:07:17.890
But I have to ask, what makes
you think a line is even

00:07:17.890 --> 00:07:20.290
the right thing to fit here?

00:07:20.290 --> 00:07:23.320
Is there another distribution
we could choose, a richer

00:07:23.320 --> 00:07:25.600
distribution, that
would actually find

00:07:25.600 --> 00:07:28.730
the right form of the data?

00:07:28.730 --> 00:07:30.470
And of course,
the answer is yes.

00:07:30.470 --> 00:07:32.260
It's a Gaussian process.

00:07:32.260 --> 00:07:35.090
By tossing in this
fancy distribution,

00:07:35.090 --> 00:07:37.150
it turns out that the
data wasn't linear at all.

00:07:37.150 --> 00:07:39.025
No wonder we had such
a hard time fitting it.

00:07:39.025 --> 00:07:42.160
It was sinusoidal, and the
Gaussian process can see this.

00:07:42.160 --> 00:07:44.080
How can the Gaussian
process see this?

00:07:44.080 --> 00:07:47.980
Because it treats the loss
itself as a random variable.

00:07:47.980 --> 00:07:51.010
Now, how could you do that,
if you're just specifying mean

00:07:51.010 --> 00:07:52.150
squared error as your loss?

00:07:52.150 --> 00:07:52.990
You can't.

00:07:52.990 --> 00:07:55.330
It has to be part of your
model, and that's the power

00:07:55.330 --> 00:07:57.070
of probabilistic modeling.

00:07:57.070 --> 00:08:00.730
When you bake in these
ideas into one model,

00:08:00.730 --> 00:08:04.270
you get to move things
around fluidly between weight

00:08:04.270 --> 00:08:07.120
uncertainty and
variance in the data,

00:08:07.120 --> 00:08:10.860
and even uncertainty in the loss
function you're fitting itself.

00:08:10.860 --> 00:08:16.080
And so the question is, how
can this all be so easy?

00:08:16.080 --> 00:08:17.922
How did it all fit together?

00:08:17.922 --> 00:08:19.858
It's TensorFlow Probability.

00:08:22.790 --> 00:08:26.120
So TensorFlow Probability
is a collection

00:08:26.120 --> 00:08:29.060
of tools designed to make
probabilistic reasoning

00:08:29.060 --> 00:08:31.040
in TensorFlow easier.

00:08:31.040 --> 00:08:33.053
It is not going to
make your job easy.

00:08:33.053 --> 00:08:34.970
It's just going to give
you the tools you need

00:08:34.970 --> 00:08:36.500
to express the ideas you have.

00:08:36.500 --> 00:08:39.230
You still have to have domain
knowledge and expertise.

00:08:39.230 --> 00:08:42.380
But you can encode that
domain knowledge and expertise

00:08:42.380 --> 00:08:44.390
in a probabilistic
formalism, and TFP

00:08:44.390 --> 00:08:46.070
has the tools to do that.

00:08:46.070 --> 00:08:48.290
Statisticians and
data scientists

00:08:48.290 --> 00:08:50.690
will be able to write and
launch the same model.

00:08:50.690 --> 00:08:54.260
Gone are the days of hacking
your model in R and importing

00:08:54.260 --> 00:08:58.190
it over to a faster language,
like C++, or even TensorFlow.

00:08:58.190 --> 00:09:00.320
You can do it all in
the same framework.

00:09:00.320 --> 00:09:02.060
ML researchers and
practitioners will

00:09:02.060 --> 00:09:04.940
be able to make predictions
with uncertainty.

00:09:04.940 --> 00:09:06.590
If you predict the
light is green,

00:09:06.590 --> 00:09:10.570
you'd better be pretty
confident that you should go.

00:09:10.570 --> 00:09:12.480
You can do that with
probabilistic modeling

00:09:12.480 --> 00:09:16.510
and TensorFlow Probability.

00:09:16.510 --> 00:09:21.040
So we saw one small part of TFP.

00:09:21.040 --> 00:09:24.490
Broadly speaking, the tools
are broken in two components--

00:09:24.490 --> 00:09:27.760
those tools useful for
building models and those tools

00:09:27.760 --> 00:09:30.070
useful for doing
inference on those models.

00:09:30.070 --> 00:09:32.740
On the model building side,
you saw the normal distribution

00:09:32.740 --> 00:09:35.433
and the variation of Gaussian
process distribution.

00:09:35.433 --> 00:09:38.630
A distribution is just is a
collection of simple summary

00:09:38.630 --> 00:09:41.670
statistics, exactly like it
is in every other library.

00:09:41.670 --> 00:09:42.950
There's a few differences.

00:09:42.950 --> 00:09:44.930
R distribution
support, this concept

00:09:44.930 --> 00:09:47.090
of batch shape, which
automatically takes advantage

00:09:47.090 --> 00:09:49.178
of vector processing hardware.

00:09:49.178 --> 00:09:50.720
But for the most
part, they should be

00:09:50.720 --> 00:09:52.800
pretty natural and easy to use.

00:09:52.800 --> 00:09:55.010
We also have something
called bijecters,

00:09:55.010 --> 00:09:57.625
which is a library for
transforming random variables.

00:09:57.625 --> 00:09:59.000
In the simplest
case, this can be

00:09:59.000 --> 00:10:02.510
like taking the x of the normal,
and now you have a lognormal.

00:10:02.510 --> 00:10:04.130
In more complicated
cases, it can

00:10:04.130 --> 00:10:06.973
involve transforming a random
variable with a neural network.

00:10:06.973 --> 00:10:09.140
This includes things like
mask autoregressive flows,

00:10:09.140 --> 00:10:11.210
if you've heard
about it, real MVPs,

00:10:11.210 --> 00:10:15.760
and other sophisticated
probabilistic models.

00:10:15.760 --> 00:10:16.660
You saw layers.

00:10:16.660 --> 00:10:19.650
We also have some losses that
help you build Monte Carlo

00:10:19.650 --> 00:10:23.430
approximations to otherwise
intractable calculations.

00:10:23.430 --> 00:10:25.590
Edward2 is our probabilistic
programming language

00:10:25.590 --> 00:10:29.070
that helps you combine different
random variables as one.

00:10:29.070 --> 00:10:32.160
On the inference side,
no Bayesian library

00:10:32.160 --> 00:10:34.590
would be complete without
Markov chain Monte Carlo

00:10:34.590 --> 00:10:38.670
tools, within which we have
several transition kernels.

00:10:38.670 --> 00:10:41.122
One of them is called
Hamiltonian Monte Carlo,

00:10:41.122 --> 00:10:43.080
which naturally takes
advantage of TensorFlow's

00:10:43.080 --> 00:10:45.860
automatic differentiation
capability.

00:10:45.860 --> 00:10:47.270
We also have tools
for performing

00:10:47.270 --> 00:10:48.950
variational inference--
again, taking

00:10:48.950 --> 00:10:51.950
advantage of TF's automatic
differentiation and optimizer

00:10:51.950 --> 00:10:53.180
toolbox.

00:10:53.180 --> 00:10:54.890
And of course, we have
our own optimizers

00:10:54.890 --> 00:10:56.870
that often come up in
probabilistic modeling

00:10:56.870 --> 00:11:00.290
problems, such as Nelder-Mead,
BFGS, things like that.

00:11:00.290 --> 00:11:03.980
The point is, this toolbox
has maybe not everything,

00:11:03.980 --> 00:11:06.230
but certainly, it
has most of what

00:11:06.230 --> 00:11:10.400
you might need to do fancier
modeling to actually get

00:11:10.400 --> 00:11:12.618
more out of your
machine learning model.

00:11:12.618 --> 00:11:13.910
And it doesn't have to be hard.

00:11:13.910 --> 00:11:15.410
You saw the Keras
examples were just

00:11:15.410 --> 00:11:19.160
a sequence of one line changes.

00:11:19.160 --> 00:11:20.930
So of course,
TensorFlow probability

00:11:20.930 --> 00:11:23.180
is used widely around Alphabet.

00:11:23.180 --> 00:11:25.190
DeepMind uses it extensively.

00:11:25.190 --> 00:11:26.570
Google Brain uses it.

00:11:26.570 --> 00:11:29.570
Google accelerated sciences,
product areas-- infrastructure

00:11:29.570 --> 00:11:32.580
areas even use it for
planning purposes.

00:11:32.580 --> 00:11:34.130
But it's also used
outside of Google.

00:11:34.130 --> 00:11:38.180
So Baker Hughes GE is one of
our early adopters of TensorFlow

00:11:38.180 --> 00:11:40.250
probability, and
they use it to build

00:11:40.250 --> 00:11:42.050
models to detect anomalies.

00:11:42.050 --> 00:11:45.290
Anomaly detection is a very
hard problem because, hopefully,

00:11:45.290 --> 00:11:47.330
your data set never
has the anomaly

00:11:47.330 --> 00:11:48.650
you're trying to detect.

00:11:48.650 --> 00:11:51.110
For example, anyone
who flew out here

00:11:51.110 --> 00:11:53.260
would be happy to know
that Baker Hughes GE uses

00:11:53.260 --> 00:11:57.410
its anomaly detection to predict
the lifespan of jet engines.

00:11:57.410 --> 00:12:01.130
And if we had a data set that
had a failing jet engine,

00:12:01.130 --> 00:12:02.510
that would be a tragedy.

00:12:02.510 --> 00:12:06.710
And so using math, we can
get around this by actually--

00:12:06.710 --> 00:12:08.240
or they get around this--

00:12:08.240 --> 00:12:12.720
by modeling models and then
trying to, in the abstract,

00:12:12.720 --> 00:12:14.720
figure out if those are
going to be good models.

00:12:14.720 --> 00:12:17.230
So what you see is their
data processing pipeline.

00:12:17.230 --> 00:12:20.240
The orange boxes use TensorFlow
probability extensively.

00:12:20.240 --> 00:12:25.160
The orange bordered box is
where they use TensorFlow.

00:12:25.160 --> 00:12:28.730
And the basic flow is to try
to treat the model itself

00:12:28.730 --> 00:12:30.650
as a random variable,
and then determine

00:12:30.650 --> 00:12:33.140
if it's going to be a
good model on otherwise

00:12:33.140 --> 00:12:35.270
an incomplete data set.

00:12:35.270 --> 00:12:38.120
And from this,
they're able to do--

00:12:38.120 --> 00:12:40.730
they get remarkable
results, dramatic decreases

00:12:40.730 --> 00:12:44.260
in false positives and false
negatives over very large data

00:12:44.260 --> 00:12:46.275
sets in complicated systems.

00:12:50.870 --> 00:12:54.050
So the question is, who will
be the next success story?

00:12:54.050 --> 00:12:57.770
Try it out-- it's an open
source Python package

00:12:57.770 --> 00:12:59.390
built using
TensorFlow that makes

00:12:59.390 --> 00:13:02.880
it easy to combine deep learning
with probabilistic models.

00:13:02.880 --> 00:13:04.380
You can PIP install it.

00:13:04.380 --> 00:13:07.102
Check out
tensorflow.org/probability.

00:13:07.102 --> 00:13:08.810
And if you're interested
in learning more

00:13:08.810 --> 00:13:10.950
about Bayesian approaches,
check out this book,

00:13:10.950 --> 00:13:13.940
which we rewrote using
TensorFlow probability,

00:13:13.940 --> 00:13:16.490
within which you can learn,
like I said, Bayesian methods,

00:13:16.490 --> 00:13:19.090
but also just how to use
TensorFlow probability.

00:13:19.090 --> 00:13:20.840
If you're not a Bayesian,
that's fine too.

00:13:20.840 --> 00:13:24.350
We have numerous tools
for frequentists.

00:13:24.350 --> 00:13:26.570
We have a second
order generalized

00:13:26.570 --> 00:13:29.132
linear model solver, which--

00:13:29.132 --> 00:13:31.590
you should care, because if
you're doing linear regression,

00:13:31.590 --> 00:13:34.620
it could solve that problem
on the order of 30 iterations,

00:13:34.620 --> 00:13:38.450
which definitely cannot be said
of a standard gradient descent.

00:13:38.450 --> 00:13:40.833
And if you want to find out
more about this example,

00:13:40.833 --> 00:13:42.500
you can check out our
GitHub repository,

00:13:42.500 --> 00:13:45.650
where you'll find several
Jupyter notebooks.

00:13:45.650 --> 00:13:46.550
Thanks.

00:13:46.550 --> 00:13:49.900
[MUSIC PLAYING]

