WEBVTT
Kind: captions
Language: en

00:00:00.179 --> 00:00:01.179
I'm Soroush Vosoughi.

00:00:01.179 --> 00:00:03.330
I'm a postdoc at the MIT Media Lab.

00:00:03.330 --> 00:00:08.980
I also did my PhD at MIT, and I've been a
fellow here at Berkman since September.

00:00:08.980 --> 00:00:12.380
The question I'm really interested in and
what I'm working on as a fellow at Berkman

00:00:12.380 --> 00:00:18.620
and also as a postdoc at MIT Media Lab is
interventions to dampen the effects of misinformation

00:00:18.620 --> 00:00:19.620
on social media.

00:00:19.620 --> 00:00:24.890
My PhD focused on automatic detection of rumors
on social media.

00:00:24.890 --> 00:00:30.280
Right now I'm interested in intervention strategies,
so one idea I have is maybe an automated tool

00:00:30.280 --> 00:00:36.120
like a bot on Twitter and on Facebook that
would detect misinformation using the algorithm

00:00:36.120 --> 00:00:37.579
I developed for my thesis.

00:00:37.579 --> 00:00:41.280
And then contact people who are on the path
of the misinformation to let them know that

00:00:41.280 --> 00:00:42.790
they might be exposed to this thing.

00:00:42.790 --> 00:00:46.579
Kind of vaccinating them before they're actually
exposed to the virus of misinformation.

00:00:46.579 --> 00:00:51.520
Now I think it's become pretty obvious that
rumors and misinformation in different domains

00:00:51.520 --> 00:00:58.340
are super important, and damaging to society,
specifically rumors in the political domain.

00:00:58.340 --> 00:01:03.950
They undermine the core democratic values
of our society, because if you don't have

00:01:03.950 --> 00:01:08.520
a shared truth with the other people who are
voting in the same election as you then you're

00:01:08.520 --> 00:01:10.420
not judging the candidates based on the same
facts.

00:01:10.420 --> 00:01:14.580
I think technology is always neutral, almost
always neutral.

00:01:14.580 --> 00:01:18.210
So it can be used for good or evil.

00:01:18.210 --> 00:01:23.650
What excites me, and what makes me fearful
is actually the same technology, which is

00:01:23.650 --> 00:01:27.310
specifically recent advances in deep neural
networks.

00:01:27.310 --> 00:01:33.590
A lot of the problems in classical AI have
already been solved using this new method

00:01:33.590 --> 00:01:37.659
in the last decade, so problems that we thought
we would not be able to solve in a century

00:01:37.659 --> 00:01:38.659
we've already solved.

00:01:38.659 --> 00:01:39.780
So that's really exciting.

00:01:39.780 --> 00:01:44.210
But again, the same algorithms and systems
that we've used to solve these problems, they're

00:01:44.210 --> 00:01:45.210
big black boxes.

00:01:45.210 --> 00:01:47.210
We never know exactly what goes in them.

00:01:47.210 --> 00:01:51.790
And so if you give them too much power to
govern our society, they might actually make

00:01:51.790 --> 00:01:54.920
decisions that we would never understand,
and that we'll never be able to interpret

00:01:54.920 --> 00:01:55.639
â€” and that scares me.

