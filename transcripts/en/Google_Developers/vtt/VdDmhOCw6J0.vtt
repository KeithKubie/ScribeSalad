WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.640
[MUSIC PLAYING]

00:00:04.410 --> 00:00:05.910
DANIEL VISENTIN:
So as Zak said, I'm

00:00:05.910 --> 00:00:08.010
from DeepMind's
applied group, where

00:00:08.010 --> 00:00:10.890
I work on a team that applies
DeepMind's research to machine

00:00:10.890 --> 00:00:13.946
learning and
reinforcement learning--

00:00:13.946 --> 00:00:16.320
DeepMind's research in machine
learning and reinforcement

00:00:16.320 --> 00:00:17.977
learning to Google's products.

00:00:17.977 --> 00:00:19.560
Today I'm going to
briefly talk to you

00:00:19.560 --> 00:00:22.650
about why we chose to use
TensorFlow, how we use it,

00:00:22.650 --> 00:00:24.150
and how it's helped us.

00:00:26.730 --> 00:00:28.740
So the choice of a
numerical computing

00:00:28.740 --> 00:00:32.400
platform such as TensorFlow
is incredibly important to us.

00:00:32.400 --> 00:00:37.860
It's how we express our ideas,
and it's the piece of software

00:00:37.860 --> 00:00:39.880
our engineers and
scientists spend most

00:00:39.880 --> 00:00:42.270
of their time interacting with.

00:00:42.270 --> 00:00:45.540
The quality of that platform has
a direct and significant effect

00:00:45.540 --> 00:00:47.790
on the quality of our work
and the speed with which we

00:00:47.790 --> 00:00:49.740
can deliver results.

00:00:49.740 --> 00:00:52.560
And there are several aspects
to what we look for in that.

00:00:52.560 --> 00:00:54.870
So from flexibility,
our researches

00:00:54.870 --> 00:00:58.500
continue to concoct all these
weird and wonderful networks,

00:00:58.500 --> 00:01:00.810
which we're hoping
to put into use.

00:01:00.810 --> 00:01:05.770
Usability-- using this platform
has to be easy and natural.

00:01:05.770 --> 00:01:07.955
Simple things like
getting some embeddings

00:01:07.955 --> 00:01:10.410
and passing through a
feed-forward network

00:01:10.410 --> 00:01:12.000
have to be trivial.

00:01:12.000 --> 00:01:13.560
Composing things should be easy.

00:01:13.560 --> 00:01:15.726
We should be able to take
all these components which

00:01:15.726 --> 00:01:18.390
people are developing
and knock them together

00:01:18.390 --> 00:01:19.946
to create new things.

00:01:19.946 --> 00:01:22.320
We need to be able to reach
into the guts of that network

00:01:22.320 --> 00:01:25.770
to see what it's doing, and
we need tools for visualizing

00:01:25.770 --> 00:01:28.840
and debugging what's going on.

00:01:28.840 --> 00:01:31.910
The platform also
has to be scalable.

00:01:31.910 --> 00:01:35.230
We see this move towards
increasingly more data,

00:01:35.230 --> 00:01:37.480
increasingly larger
networks trained

00:01:37.480 --> 00:01:41.470
on more powerful accelerators
across multiple servers.

00:01:41.470 --> 00:01:45.730
And the more that trend
continues, the better.

00:01:45.730 --> 00:01:47.670
We need to be performant.

00:01:47.670 --> 00:01:50.200
So training machine
learning models

00:01:50.200 --> 00:01:52.300
is the new compile link cycle.

00:01:52.300 --> 00:01:54.070
It often takes days
or even weeks for us

00:01:54.070 --> 00:01:56.740
to get results, which reduces
the speed with which we

00:01:56.740 --> 00:01:59.500
can iterate and try new things.

00:01:59.500 --> 00:02:03.610
So the faster it goes,
the faster we can develop.

00:02:03.610 --> 00:02:05.070
It's also important
at serve time,

00:02:05.070 --> 00:02:08.430
where latency is a critical
factor in what we can do.

00:02:08.430 --> 00:02:12.336
We often have quite
tight latency boundaries.

00:02:12.336 --> 00:02:13.710
The more performant
the model is,

00:02:13.710 --> 00:02:17.610
the more expressive and more
computationally powerful

00:02:17.610 --> 00:02:22.260
models we can use, which will
give us bigger accuracy gains.

00:02:22.260 --> 00:02:24.540
Also with training,
performance matters,

00:02:24.540 --> 00:02:27.690
because when we're training
on real data at serve time,

00:02:27.690 --> 00:02:29.160
the speed with
which we can train

00:02:29.160 --> 00:02:31.020
affects the freshness
of the model, which

00:02:31.020 --> 00:02:35.220
often has downstream effects
on our accuracy at serve time.

00:02:35.220 --> 00:02:37.590
And finally,
production readiness--

00:02:37.590 --> 00:02:40.215
we often spend all this time
doing research, and evaluation,

00:02:40.215 --> 00:02:41.880
and experimentation
of our models.

00:02:41.880 --> 00:02:43.300
And when we finally come
up with a good thing,

00:02:43.300 --> 00:02:45.090
we want to get it into
production as soon

00:02:45.090 --> 00:02:47.220
as possible to do
live experiments

00:02:47.220 --> 00:02:49.840
and ultimately scale it up to
all the traffic that we have.

00:02:52.910 --> 00:02:54.890
So previously at
DeepMind, our projects

00:02:54.890 --> 00:02:57.980
were all implemented in
either Torch or Disbelief.

00:02:57.980 --> 00:02:59.870
As TensorFlow began
to mature and head

00:02:59.870 --> 00:03:01.940
towards its first
open source release,

00:03:01.940 --> 00:03:04.633
we started looking at it
as an interesting thing.

00:03:07.550 --> 00:03:09.860
In the applied group, we
began experimenting with it

00:03:09.860 --> 00:03:11.930
by trying a few new projects.

00:03:11.930 --> 00:03:13.670
Meanwhile, the
research team, who

00:03:13.670 --> 00:03:15.830
have this very
stable, much used code

00:03:15.830 --> 00:03:18.170
base they needed to
think about, began

00:03:18.170 --> 00:03:23.000
by carefully evaluating the
suitability by reimplementing

00:03:23.000 --> 00:03:25.040
several existing projects.

00:03:25.040 --> 00:03:26.870
Based on the results
of that, we decided

00:03:26.870 --> 00:03:28.920
that TensorFlow was
the way forward,

00:03:28.920 --> 00:03:32.460
and so we started to migrate.

00:03:32.460 --> 00:03:34.040
So over the past
year or so, we've

00:03:34.040 --> 00:03:35.460
been working on that process.

00:03:35.460 --> 00:03:38.450
And as of today, most of our
code that we currently use

00:03:38.450 --> 00:03:39.850
is in TensorFlow.

00:03:39.850 --> 00:03:41.960
And all the stuff--

00:03:41.960 --> 00:03:44.840
the significant amount of stuff
we're building from now on

00:03:44.840 --> 00:03:46.700
will be in TensorFlow.

00:03:46.700 --> 00:03:49.470
What's quite cool about this is
we made this decision a year,

00:03:49.470 --> 00:03:52.760
a year and a half ago, and
since then, the TensorFlow

00:03:52.760 --> 00:03:55.100
and the ecosystem
around it has improved

00:03:55.100 --> 00:03:56.190
beyond our expectations.

00:03:56.190 --> 00:03:57.981
So we're even more
happy with this decision

00:03:57.981 --> 00:04:00.929
now than we were then.

00:04:00.929 --> 00:04:02.970
So I'm going to spend the
rest of my time talking

00:04:02.970 --> 00:04:05.094
a bit more specifically
about how we use TensorFlow

00:04:05.094 --> 00:04:06.210
and how it helps us.

00:04:06.210 --> 00:04:09.551
And since I'm from the applied
group, I'll start there.

00:04:09.551 --> 00:04:11.550
So one of the coolest
things we've done recently

00:04:11.550 --> 00:04:15.600
is on optimizing the energy
usage in Google's data centers.

00:04:15.600 --> 00:04:19.140
This began when the technical
infrastructure team came to us

00:04:19.140 --> 00:04:21.329
after seeing the AlphaGo
matches and wondered

00:04:21.329 --> 00:04:24.870
how we could use RL to
control the data centers more

00:04:24.870 --> 00:04:26.170
efficiently.

00:04:26.170 --> 00:04:28.800
So we work with them,
and subsequently worked

00:04:28.800 --> 00:04:32.970
with teams in Google Brain
to build models to control

00:04:32.970 --> 00:04:35.910
the cooling infrastructure.

00:04:35.910 --> 00:04:38.880
Briefly, this is what a cold
aisle in one of Google's data

00:04:38.880 --> 00:04:40.230
centers looks like.

00:04:40.230 --> 00:04:41.527
This is the servers.

00:04:41.527 --> 00:04:43.360
Some of them, maybe,
are running TensorFlow,

00:04:43.360 --> 00:04:45.080
and they're getting
hot doing so.

00:04:45.080 --> 00:04:47.250
So we have these fans
at the back, which

00:04:47.250 --> 00:04:48.930
pull that cold air
over the servers

00:04:48.930 --> 00:04:52.650
into this hot aisle, where it
travels down and is transferred

00:04:52.650 --> 00:04:55.000
into a water cooling system.

00:04:55.000 --> 00:04:57.240
So here are some
pipes and some pumps.

00:04:57.240 --> 00:04:59.970
And the pipes take the hot
water from the hot aisle

00:04:59.970 --> 00:05:02.550
to the chillers on the roof,
and the cold water from those

00:05:02.550 --> 00:05:05.032
chillers back to the cold aisle.

00:05:05.032 --> 00:05:06.990
And here's what those
cooling towers look like.

00:05:06.990 --> 00:05:09.060
They're just big fans
which take the water

00:05:09.060 --> 00:05:12.540
and cool it down into the air.

00:05:12.540 --> 00:05:14.850
So we spent a while
working on this offline.

00:05:14.850 --> 00:05:18.210
We had to take into
account safety.

00:05:18.210 --> 00:05:21.990
The model couldn't go
and try random things,

00:05:21.990 --> 00:05:24.330
otherwise we'd break
billion-dollar data centers,

00:05:24.330 --> 00:05:26.020
which is never great.

00:05:26.020 --> 00:05:28.620
And we had to work out how
to do a bit of exploration

00:05:28.620 --> 00:05:30.990
as well, because if we
just learned on the data

00:05:30.990 --> 00:05:34.016
which we had available,
the model would probably

00:05:34.016 --> 00:05:35.640
never learn to do
anything more optimal

00:05:35.640 --> 00:05:40.410
than what the human
operators had done.

00:05:40.410 --> 00:05:42.570
So we iterated a bit,
and when it eventually

00:05:42.570 --> 00:05:44.520
came time to experiment,
we turned it on,

00:05:44.520 --> 00:05:46.680
and we saw this nice drop.

00:05:46.680 --> 00:05:49.080
&gt;From my experience, it's
kind of rare to see something

00:05:49.080 --> 00:05:51.060
so defined and
sustained like this,

00:05:51.060 --> 00:05:52.320
so we're quite happy with it.

00:05:55.500 --> 00:05:58.530
So we had this initial
result, and it came time

00:05:58.530 --> 00:05:59.640
to implement it.

00:05:59.640 --> 00:06:02.130
Because the data
centers didn't really

00:06:02.130 --> 00:06:06.720
have the infrastructure
set up to run, and train,

00:06:06.720 --> 00:06:09.660
and evaluate, and verify
machine learning models,

00:06:09.660 --> 00:06:13.140
we had to write a lot
of this ourselves.

00:06:13.140 --> 00:06:13.860
So we did that.

00:06:13.860 --> 00:06:15.480
We took our original
Torch models

00:06:15.480 --> 00:06:16.740
and moved them to TensorFlow.

00:06:16.740 --> 00:06:18.900
We built up all this
training, and validation,

00:06:18.900 --> 00:06:20.724
and serving stuff.

00:06:20.724 --> 00:06:22.140
But over the past
year, what we've

00:06:22.140 --> 00:06:26.535
witnessed the development
in these high-level APIs

00:06:26.535 --> 00:06:29.910
around TensorFlow and components
like TensorFlow Serving, which

00:06:29.910 --> 00:06:33.259
basically negate a lot of
the work that we had to do,

00:06:33.259 --> 00:06:35.550
which from our perspective
is great because we'd rather

00:06:35.550 --> 00:06:37.550
focus on the modeling
aspects than all this kind

00:06:37.550 --> 00:06:38.950
of infrastructure stuff.

00:06:41.974 --> 00:06:44.152
Cool.

00:06:44.152 --> 00:06:45.610
So another example
where TensorFlow

00:06:45.610 --> 00:06:47.590
helped us is we
previously had this system

00:06:47.590 --> 00:06:51.310
called Gorila, which stands for
general reinforcement learning

00:06:51.310 --> 00:06:54.302
architecture, with some extra
letters thrown in there.

00:06:57.040 --> 00:06:58.900
And we wrote this,
initially, to speed up

00:06:58.900 --> 00:07:01.630
training of DQN style agents.

00:07:01.630 --> 00:07:06.940
So what this was was a framework
built on top of disbelief.

00:07:06.940 --> 00:07:09.790
We had the parameters
of an agent stored

00:07:09.790 --> 00:07:11.610
across multiple
parameter servers,

00:07:11.610 --> 00:07:13.389
and multiple instances
of that agent

00:07:13.389 --> 00:07:15.430
pulling the parameters
from that parameter server

00:07:15.430 --> 00:07:18.640
and interacting with
environments to get experience.

00:07:18.640 --> 00:07:23.200
That experience was then stored
in a distributed memory where

00:07:23.200 --> 00:07:26.431
multiple learners sampled
from it to make updates

00:07:26.431 --> 00:07:27.430
to the model parameters.

00:07:27.430 --> 00:07:30.297
And the whole thing kind
of cycled around like that.

00:07:30.297 --> 00:07:32.380
By doing it this way, we
could get more experience

00:07:32.380 --> 00:07:34.210
faster because we
had multiple agents,

00:07:34.210 --> 00:07:35.710
and we could learn
faster because we

00:07:35.710 --> 00:07:38.030
had multiple learners.

00:07:38.030 --> 00:07:40.029
But often, more than the
speed of convergence,

00:07:40.029 --> 00:07:42.070
we often saw that sometimes
these models actually

00:07:42.070 --> 00:07:45.809
did better than models
trained on single servers.

00:07:45.809 --> 00:07:47.350
And we think that
had something to do

00:07:47.350 --> 00:07:49.750
with the breaking
of correlations

00:07:49.750 --> 00:07:53.470
between the examples we
trained on from time to time.

00:07:53.470 --> 00:07:55.270
This also was one
of the inspirations

00:07:55.270 --> 00:07:57.970
for the A3C training algorithm,
which our researchers later

00:07:57.970 --> 00:08:00.780
developed.

00:08:00.780 --> 00:08:02.760
So with the introduction
of TensorFlow,

00:08:02.760 --> 00:08:04.566
we were able to
basically deprecate this

00:08:04.566 --> 00:08:05.940
because much of
the functionality

00:08:05.940 --> 00:08:07.890
is handled by
TensorFlow or can be

00:08:07.890 --> 00:08:11.400
written from within
TensorFlow, which is great

00:08:11.400 --> 00:08:14.821
because it's less time
we need to spend on code

00:08:14.821 --> 00:08:16.320
and maintaining it,
and more time we

00:08:16.320 --> 00:08:20.120
can spend on our
actual projects.

00:08:20.120 --> 00:08:22.380
More generally, since we've
started using TensorFlow,

00:08:22.380 --> 00:08:24.330
we've seen a reduction
across the board

00:08:24.330 --> 00:08:27.390
in the amount of ancillary
work we have to perform.

00:08:27.390 --> 00:08:29.390
And things are only getting
better as TensorFlow

00:08:29.390 --> 00:08:32.909
and the ecosystem around
it continues to develop.

00:08:32.909 --> 00:08:34.740
So one of these is
the high-level APIs

00:08:34.740 --> 00:08:38.100
which are developing, which take
care of a lot of the work we

00:08:38.100 --> 00:08:40.350
previously had to do
to start a new project,

00:08:40.350 --> 00:08:42.990
to build a prototype, to
train it and evaluate it,

00:08:42.990 --> 00:08:46.590
meaning we can spend
more time on modeling.

00:08:46.590 --> 00:08:49.770
It's easy to experiment
with models in TensorFlow.

00:08:49.770 --> 00:08:52.770
It's easy to put together
a baseline in a couple

00:08:52.770 --> 00:08:53.400
lines of code.

00:08:53.400 --> 00:08:58.090
It's easy to change
things about it.

00:08:58.090 --> 00:09:01.220
So for example, if we
have a bunch of embeddings

00:09:01.220 --> 00:09:02.840
from various time
series, do we want

00:09:02.840 --> 00:09:04.675
to concatenate the last
10 of them together?

00:09:04.675 --> 00:09:06.050
Do we want to
convolve over them?

00:09:06.050 --> 00:09:09.879
Do we want to pass them through
an LSTM to get the state?

00:09:09.879 --> 00:09:11.670
It's all just a couple
of lines of changes,

00:09:11.670 --> 00:09:14.150
which is quite nice.

00:09:14.150 --> 00:09:16.940
The distributed
training helps us.

00:09:16.940 --> 00:09:21.037
We can try more things
quicker and sooner.

00:09:21.037 --> 00:09:23.120
And finally, once we've
gone through all this work

00:09:23.120 --> 00:09:25.703
and we're excited and wanting
to put them all into production,

00:09:25.703 --> 00:09:28.460
TF Serving kind of gets rid
of a lot of the pain that

00:09:28.460 --> 00:09:29.686
was previously there.

00:09:33.020 --> 00:09:36.230
So all this amounts to a quicker
development cycle, meaning we

00:09:36.230 --> 00:09:38.150
can support more projects.

00:09:38.150 --> 00:09:40.520
We can experiment more,
and we can deliver

00:09:40.520 --> 00:09:43.240
quicker, which is great.

00:09:43.240 --> 00:09:45.740
And what's more, we've yet to
come across an instance in any

00:09:45.740 --> 00:09:47.510
of these collaborations
with Google

00:09:47.510 --> 00:09:50.282
where TensorFlow
has not been suited

00:09:50.282 --> 00:09:52.490
to the problem we're working
on, hasn't been flexible

00:09:52.490 --> 00:09:54.197
enough, which is great.

00:09:54.197 --> 00:09:56.030
So I'm excited about
the possibilities here,

00:09:56.030 --> 00:09:58.530
and I hope we can tell you more
about some of these projects

00:09:58.530 --> 00:10:00.720
soon.

00:10:00.720 --> 00:10:03.540
So the applied team is
only one part of DeepMind,

00:10:03.540 --> 00:10:05.680
and our friends in
research are also

00:10:05.680 --> 00:10:08.380
busy doing lots of
cool stuff and have

00:10:08.380 --> 00:10:12.290
begun to take advantage
of TensorFlow to do so.

00:10:12.290 --> 00:10:13.940
So one of the obvious
things and one

00:10:13.940 --> 00:10:16.090
of the most exciting
moments for us last year

00:10:16.090 --> 00:10:18.430
was when AlphaGo succeeded
in a series of matches

00:10:18.430 --> 00:10:20.830
against Lee Sedol, one of
the greatest Go players

00:10:20.830 --> 00:10:22.690
of the last decade.

00:10:22.690 --> 00:10:24.070
What made this so
exciting was it

00:10:24.070 --> 00:10:26.153
was a combination of a
great technical achievement

00:10:26.153 --> 00:10:28.540
in making a strong Go
player, but it also

00:10:28.540 --> 00:10:31.840
represented something of a new
beginning in the game of Go.

00:10:31.840 --> 00:10:33.100
Interest boomed.

00:10:33.100 --> 00:10:38.140
Go boards sold out across the
world, which was interesting.

00:10:38.140 --> 00:10:40.437
And players responded, not
with a sense of defeat,

00:10:40.437 --> 00:10:42.520
but they were excited about
it as a new beginning,

00:10:42.520 --> 00:10:45.190
because they could work
with these machines

00:10:45.190 --> 00:10:49.520
to explore more deeply the
mysteries of this ancient game.

00:10:49.520 --> 00:10:51.040
More to the point,
AlphaGo was one

00:10:51.040 --> 00:10:53.530
of the first significant
uses of TensorFlow

00:10:53.530 --> 00:10:56.020
at DeepMind, where it was
used to great advantage

00:10:56.020 --> 00:11:00.860
to train these networks,
powering AlphaGo.

00:11:00.860 --> 00:11:03.370
So I won't spend too much
time talking about Go itself.

00:11:03.370 --> 00:11:05.740
Just suffice it it say, it's
a perfect information game

00:11:05.740 --> 00:11:08.290
with a very high
branching factor and a lot

00:11:08.290 --> 00:11:10.670
of moves in a particular game.

00:11:10.670 --> 00:11:15.391
So your standard minimax
search obviously won't work.

00:11:15.391 --> 00:11:17.140
The main component of
AlphaGo was composed

00:11:17.140 --> 00:11:18.540
of these two networks--

00:11:18.540 --> 00:11:21.450
a policy network
and a value network.

00:11:21.450 --> 00:11:24.460
The policy network was trained
to, given a game state,

00:11:24.460 --> 00:11:27.780
work out which move the
opponent was going to make.

00:11:27.780 --> 00:11:32.500
And the value network was
trained to, given a game state,

00:11:32.500 --> 00:11:34.475
predict how likely we
were to win that game.

00:11:37.900 --> 00:11:40.220
So naively, while you could
just use the value network

00:11:40.220 --> 00:11:42.010
to evaluate your
next set of moves,

00:11:42.010 --> 00:11:45.040
in practice that wasn't quite
good enough for what we needed.

00:11:45.040 --> 00:11:48.700
So we resorted to a
bit of a compromise,

00:11:48.700 --> 00:11:50.890
using these nets with a
Monte Carlo tree search

00:11:50.890 --> 00:11:52.960
to get better results.

00:11:52.960 --> 00:11:55.399
And the way this
basically works is,

00:11:55.399 --> 00:11:57.190
if you do a full Monte
Carlo search, that's

00:11:57.190 --> 00:11:58.370
way too many possibilities.

00:11:58.370 --> 00:12:02.140
So we use this policy network
to guide the paths in the search

00:12:02.140 --> 00:12:04.990
tree to the most likely paths.

00:12:04.990 --> 00:12:08.267
And then at some point further
on in one of those paths,

00:12:08.267 --> 00:12:09.850
we'd use the value
network to evaluate

00:12:09.850 --> 00:12:12.130
how good that path was,
meaning we didn't have

00:12:12.130 --> 00:12:15.060
to go all the way to the end.

00:12:15.060 --> 00:12:17.185
Why the value network
performed better in that case

00:12:17.185 --> 00:12:20.800
is because as we unrolled
these paths, more of the game

00:12:20.800 --> 00:12:21.490
had unfolded.

00:12:21.490 --> 00:12:23.680
And things were more
certain, so the value network

00:12:23.680 --> 00:12:24.930
had a better prediction.

00:12:27.629 --> 00:12:29.420
But the real thing
where TensorFlow came in

00:12:29.420 --> 00:12:35.330
was the training of this thing,
which was quite significant.

00:12:35.330 --> 00:12:37.580
It took quite a
significant amount of work.

00:12:37.580 --> 00:12:40.670
So we started by taking
games from expert players

00:12:40.670 --> 00:12:42.590
and training this
policy network.

00:12:42.590 --> 00:12:44.600
And then we began
this iterative process

00:12:44.600 --> 00:12:46.640
where we use
reinforcement learning

00:12:46.640 --> 00:12:49.820
to train a value network
from that policy network,

00:12:49.820 --> 00:12:52.400
and then update the
policy network to produce

00:12:52.400 --> 00:12:54.200
better moves.

00:12:54.200 --> 00:12:57.752
Previously, doing this
on Torch, it was slow.

00:12:57.752 --> 00:12:59.210
And when we moved
to TensorFlow, we

00:12:59.210 --> 00:13:02.600
were able to bring the whole
power of distributed training

00:13:02.600 --> 00:13:05.120
to bear on this, which made
the whole thing faster.

00:13:05.120 --> 00:13:07.370
It meant our researchers
could iterate quicker and try

00:13:07.370 --> 00:13:08.015
new things.

00:13:10.580 --> 00:13:12.860
Another cool thing
from this year

00:13:12.860 --> 00:13:15.860
was WaveNet, which
we use to generate

00:13:15.860 --> 00:13:18.620
realistic audio signals.

00:13:18.620 --> 00:13:22.370
So this is hard in general,
because sound waves have

00:13:22.370 --> 00:13:24.110
this complex
oscillating structure

00:13:24.110 --> 00:13:28.400
with a very high sampling rate,
on the order of 16 kilohertz.

00:13:28.400 --> 00:13:30.080
Generating an audio
signal requires

00:13:30.080 --> 00:13:33.020
a model which is able to
reproduce this sort of signal

00:13:33.020 --> 00:13:37.220
at both these very
fine-grained oscillations,

00:13:37.220 --> 00:13:39.710
on the order of milliseconds,
and the whole signal

00:13:39.710 --> 00:13:43.040
over seconds, which is
difficult because of the amount

00:13:43.040 --> 00:13:44.230
of information.

00:13:44.230 --> 00:13:45.980
So there were two main
approaches to this,

00:13:45.980 --> 00:13:48.460
previously--

00:13:48.460 --> 00:13:51.780
concatenative models
and parametric models.

00:13:51.780 --> 00:13:55.065
So a concatenative model
worked by having a speaker say

00:13:55.065 --> 00:13:56.690
a whole bunch of
sound, and chopping it

00:13:56.690 --> 00:13:58.550
up into little bits, and
then reassembling those

00:13:58.550 --> 00:13:59.756
to say whatever you wanted.

00:14:02.270 --> 00:14:04.340
What you often see
from these is the sound

00:14:04.340 --> 00:14:07.190
seems kind of choppy
because these transitions

00:14:07.190 --> 00:14:10.250
between these sound segments
don't quite match up.

00:14:10.250 --> 00:14:12.820
It's also difficult
to modify the voice,

00:14:12.820 --> 00:14:15.560
either adding a new speaker
or altering the emphasis

00:14:15.560 --> 00:14:16.155
or emotion.

00:14:18.900 --> 00:14:22.170
Similarly there, the other
was parametric models,

00:14:22.170 --> 00:14:25.110
of which WaveNet is one, where
all the information required

00:14:25.110 --> 00:14:28.140
to generate speech is stored
in the parameters of the model.

00:14:28.140 --> 00:14:32.580
And you supply information
about what you want to say

00:14:32.580 --> 00:14:37.750
and the characteristics of
that as inputs to the model.

00:14:37.750 --> 00:14:40.230
However, prior to
WaveNet, we noticed

00:14:40.230 --> 00:14:42.616
that parametric text
to speech tended

00:14:42.616 --> 00:14:44.865
to sound a bit less natural
than concatenative models.

00:14:47.560 --> 00:14:52.940
So WaveNet changes the
paradigm that was previously

00:14:52.940 --> 00:14:58.010
in parametric models
by directly modeling

00:14:58.010 --> 00:15:02.210
the waveform of the audio
signal, one sample at a time.

00:15:02.210 --> 00:15:05.480
So as well as yielding more
natural sounding speech

00:15:05.480 --> 00:15:08.060
by directly modeling
the [INAUDIBLE],

00:15:08.060 --> 00:15:10.460
it means that WaveNet can
produce any kind of audio, not

00:15:10.460 --> 00:15:12.620
just speech.

00:15:12.620 --> 00:15:14.810
So the architecture
for this evolved out

00:15:14.810 --> 00:15:16.940
of work on pixel
RNNs and pixel CNNs,

00:15:16.940 --> 00:15:18.986
which were used for
generating images.

00:15:18.986 --> 00:15:20.360
And the key piece
of architecture

00:15:20.360 --> 00:15:28.910
here was the use of
a dilated convolution

00:15:28.910 --> 00:15:35.060
with an exponentially increasing
stride as the layers increased.

00:15:35.060 --> 00:15:36.470
What this allowed
the model to do

00:15:36.470 --> 00:15:38.839
was to have a receptive
field size that

00:15:38.839 --> 00:15:40.630
was exponential in the
number of the layers

00:15:40.630 --> 00:15:42.530
in the model, which
allowed it to incorporate

00:15:42.530 --> 00:15:44.752
all this into information
about the signal.

00:15:44.752 --> 00:15:46.460
The other cool thing
about this structure

00:15:46.460 --> 00:15:49.730
is that in the first
half of the input,

00:15:49.730 --> 00:15:52.020
compared to the
second half, there

00:15:52.020 --> 00:15:54.230
are twice the number
of connections,

00:15:54.230 --> 00:15:58.705
meaning it uses that
information to a greater degree.

00:16:03.460 --> 00:16:05.970
So here are some
samples generated

00:16:05.970 --> 00:16:08.290
from the various systems.

00:16:08.290 --> 00:16:13.358
So on the left, we have audio
from a concatenative model,

00:16:13.358 --> 00:16:14.342
[INAUDIBLE] play--

00:16:21.722 --> 00:16:24.277
the left one, yeah.

00:16:24.277 --> 00:16:25.860
CONCATENATIVE MODEL:
"The Blue Lagoon"

00:16:25.860 --> 00:16:28.470
is a 1980 American
romance and adventure film

00:16:28.470 --> 00:16:30.930
directed by Randal Kleiser.

00:16:30.930 --> 00:16:32.680
DANIEL VISENTIN: So
if you listen closely,

00:16:32.680 --> 00:16:35.340
you can perceive some
of the discontinuities

00:16:35.340 --> 00:16:37.920
in the generated audio, due
to the slight mismatches

00:16:37.920 --> 00:16:40.955
in the transitions between
the different recorded chunks.

00:16:40.955 --> 00:16:42.830
So in the middle, we
have a parametric model.

00:16:42.830 --> 00:16:43.380
PARAMETRIC MODEL:
"The Blue Lagoon"

00:16:43.380 --> 00:16:46.140
is a 1980 American
romance and adventure film

00:16:46.140 --> 00:16:47.537
directed by Randal Kleiser.

00:16:47.537 --> 00:16:49.120
DANIEL VISENTIN:
Which is less choppy,

00:16:49.120 --> 00:16:51.490
but it doesn't have
that natural quality we

00:16:51.490 --> 00:16:53.290
associate with normal speech--

00:16:53.290 --> 00:16:55.590
and so, and finally,
we have WaveNet.

00:16:55.590 --> 00:16:58.230
WAVENET: "The Blue Lagoon"
is a 1980 American romance

00:16:58.230 --> 00:17:00.737
and adventure film
directed by Randal Kleiser.

00:17:00.737 --> 00:17:02.820
DANIEL VISENTIN: Which,
compared to the other two,

00:17:02.820 --> 00:17:05.670
is very smooth and natural--

00:17:05.670 --> 00:17:07.890
the only slight thing
that WaveNet does

00:17:07.890 --> 00:17:10.348
is it has a bit of noise,
due to the sampling procedure

00:17:10.348 --> 00:17:12.041
and the neural network.

00:17:12.041 --> 00:17:13.500
So maybe if we play
them all again,

00:17:13.500 --> 00:17:17.290
just so people can
see, hear, from--

00:17:17.290 --> 00:17:18.124
SPEAKER: [INAUDIBLE]

00:17:18.124 --> 00:17:19.974
DANIEL VISENTIN: Yeah,
just from left to right.

00:17:19.974 --> 00:17:20.710
SPEAKER: You want
to play them again?

00:17:20.710 --> 00:17:21.960
DANIEL VISENTIN: Yeah, thanks.

00:17:21.960 --> 00:17:23.542
CONCATENATIVE MODEL:
"The Blue Lagoon"

00:17:23.542 --> 00:17:25.800
is a 1980 American
romance and adventure film

00:17:25.800 --> 00:17:28.722
directed by Randal Kleiser.

00:17:28.722 --> 00:17:30.180
PARAMETRIC MODEL:
"The Blue Lagoon"

00:17:30.180 --> 00:17:32.910
is a 1980 American
romance and adventure film

00:17:32.910 --> 00:17:35.550
directed by Randal Kleiser.

00:17:35.550 --> 00:17:39.046
WAVENET: "The Blue Lagoon"
is a 1980 American--

00:17:39.046 --> 00:17:40.045
DANIEL VISENTIN: Thanks.

00:17:40.045 --> 00:17:42.360
[LAUGHTER]

00:17:42.360 --> 00:17:44.680
So we compared WaveNet
against other models

00:17:44.680 --> 00:17:46.210
through the use
of opinion scores.

00:17:46.210 --> 00:17:49.420
And what we found by human
raters, and what we found

00:17:49.420 --> 00:17:52.840
was that it narrowed the gap
to human speech by about 50%,

00:17:52.840 --> 00:17:53.567
which is great.

00:17:56.520 --> 00:18:00.050
And because WaveNet models,
they model the raw audio signal,

00:18:00.050 --> 00:18:02.704
we can also get it to produce
sound other than speech.

00:18:02.704 --> 00:18:04.370
So here's an example
where we trained it

00:18:04.370 --> 00:18:06.765
on a corpus of classical music.

00:18:06.765 --> 00:18:11.130
[MUSIC PLAYING]

00:18:18.405 --> 00:18:20.935
Thanks-- as you can hear,
it's able to generate--

00:18:20.935 --> 00:18:24.057
oh, go back one, yeah, thanks.

00:18:24.057 --> 00:18:25.640
As you can hear,
it's able to generate

00:18:25.640 --> 00:18:30.900
realistic sounding piano notes
on a moment-to-moment basis.

00:18:30.900 --> 00:18:34.220
And finally, with what must be
one of my favorite paper titles

00:18:34.220 --> 00:18:36.740
of the year, "Learning to
Learn by Gradient Descent

00:18:36.740 --> 00:18:40.160
by Gradient Descent," in
which some of our researchers

00:18:40.160 --> 00:18:44.302
trained a neural network
to train a neural network,

00:18:44.302 --> 00:18:46.760
which I think is a great example
of the kind of flexibility

00:18:46.760 --> 00:18:48.645
that TensorFlow offers.

00:18:48.645 --> 00:18:50.020
And I think, going
forward, we're

00:18:50.020 --> 00:18:53.240
going to see more of this kind
of crazy, out-there stuff,

00:18:53.240 --> 00:18:55.940
like with models learning
architectures of models,

00:18:55.940 --> 00:19:00.350
models adding bits and pieces
to themselves, and so on.

00:19:00.350 --> 00:19:04.532
And so it's exciting to see
that TensorFlow can handle that.

00:19:04.532 --> 00:19:06.240
And I know, at least
from us at DeepMind,

00:19:06.240 --> 00:19:08.750
that TensorFlow is going to
be our choice of how we model

00:19:08.750 --> 00:19:11.214
these things going forward.

00:19:11.214 --> 00:19:12.880
So if you're interested
in more details,

00:19:12.880 --> 00:19:14.960
you can look at our
website, deepmind.com.

00:19:14.960 --> 00:19:18.190
There's a blog post linked
here, which rounds up some

00:19:18.190 --> 00:19:20.980
of the things from last year.

00:19:20.980 --> 00:19:23.620
And we also have links
to the over 100 papers

00:19:23.620 --> 00:19:26.140
that we've published so far.

00:19:26.140 --> 00:19:27.310
Thanks for listening.

00:19:27.310 --> 00:19:29.110
[APPLAUSE]

00:19:29.110 --> 00:19:32.760
[MUSIC PLAYING]

