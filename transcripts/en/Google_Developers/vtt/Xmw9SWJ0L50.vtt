WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.681
[MUSIC PLAYING]

00:00:03.827 --> 00:00:05.410
HENG-TZE CHENG: Wide
and Deep learning

00:00:05.410 --> 00:00:08.020
combines the power
of memorization

00:00:08.020 --> 00:00:10.880
and generalization,
and it does that

00:00:10.880 --> 00:00:13.510
by jointly training
widening your models

00:00:13.510 --> 00:00:14.890
and deepening your networks.

00:00:14.890 --> 00:00:16.760
We're sharing a
research paper about it

00:00:16.760 --> 00:00:20.620
and also the implementation
with an easy-to-use API

00:00:20.620 --> 00:00:24.860
in TensorFlow, which is an open
source library for a machine

00:00:24.860 --> 00:00:26.090
intelligence.

00:00:26.090 --> 00:00:29.760
So you might wonder what Wide
and Deep learning is good for.

00:00:29.760 --> 00:00:33.140
Wide and Deep learning is
useful for generic large scale

00:00:33.140 --> 00:00:36.710
regression and classification
problems with sparse inputs,

00:00:36.710 --> 00:00:39.380
things like recommendation
systems, search,

00:00:39.380 --> 00:00:40.770
and ranking problems.

00:00:40.770 --> 00:00:44.910
Now imagine you wanted to
build a search engine for food.

00:00:44.910 --> 00:00:47.700
Given a query, you want
to recommend the items

00:00:47.700 --> 00:00:49.770
that your users
will like the most.

00:00:49.770 --> 00:00:52.270
Using widening your
models, you can actually

00:00:52.270 --> 00:00:56.160
use a wide set of cross-product
features transformations

00:00:56.160 --> 00:00:58.960
to memorize specific
feature combinations.

00:00:58.960 --> 00:01:01.960
An example would be
when the users say

00:01:01.960 --> 00:01:04.349
the query, "fried
chicken," your model

00:01:04.349 --> 00:01:06.860
might memorize that
chicken and waffles

00:01:06.860 --> 00:01:09.220
is more relevant than
chicken fried rice.

00:01:09.220 --> 00:01:11.830
But one limitation
is that it's actually

00:01:11.830 --> 00:01:15.700
hard to generalize to
previously unseen combinations

00:01:15.700 --> 00:01:17.860
without manual
feature engineering.

00:01:17.860 --> 00:01:20.930
So instead, using
deep neuronetworks,

00:01:20.930 --> 00:01:23.740
you can now generalize better
through lower dimension

00:01:23.740 --> 00:01:24.550
embeddings.

00:01:24.550 --> 00:01:27.970
For example, your model might
learn to recommend burgers

00:01:27.970 --> 00:01:30.030
given the query, "fried
chicken," because they

00:01:30.030 --> 00:01:31.670
are similar types of food.

00:01:31.670 --> 00:01:35.760
However, sometimes memorizing
specific combinations

00:01:35.760 --> 00:01:39.080
as rules and exceptions
is very important.

00:01:39.080 --> 00:01:41.950
When people ask for
iced decaf latte,

00:01:41.950 --> 00:01:44.630
you don't really want to
overgeneralize and give them

00:01:44.630 --> 00:01:46.960
hot latte no matter
how close they

00:01:46.960 --> 00:01:48.400
are in the embedding space.

00:01:48.400 --> 00:01:52.900
So by jointly training
Wide and Deep models,

00:01:52.900 --> 00:01:54.950
we actually allow
them to complement

00:01:54.950 --> 00:01:57.884
each other's strengths
and weaknesses.

00:01:57.884 --> 00:01:59.800
MUSTAFA IPSIR: To help
developers get started,

00:01:59.800 --> 00:02:03.740
we released Wide and Deep
as part of the TF Learn API.

00:02:03.740 --> 00:02:06.800
TF Learn is a high level
machine learning library

00:02:06.800 --> 00:02:08.410
on top of TensorFlow.

00:02:08.410 --> 00:02:11.720
The API helps users focus
on the important questions

00:02:11.720 --> 00:02:13.480
like, how will you
design your features,

00:02:13.480 --> 00:02:15.450
and what is your
model structure?

00:02:15.450 --> 00:02:18.770
You can create a Wide
and Deep classifier

00:02:18.770 --> 00:02:21.040
with just a few lines of code.

00:02:21.040 --> 00:02:22.910
Then you specify
the features you

00:02:22.910 --> 00:02:27.290
use in the widening model
and the deep neuronetworks,

00:02:27.290 --> 00:02:29.800
and we handle the joint
training under the hood.

00:02:29.800 --> 00:02:31.770
There are different
needs and requirements

00:02:31.770 --> 00:02:35.270
from Deep learning networks
and Wide [INAUDIBLE] models.

00:02:35.270 --> 00:02:37.340
We found a way to balance this.

00:02:37.340 --> 00:02:40.090
We provide a simple feature
engineering interface

00:02:40.090 --> 00:02:42.000
that lets you
specify embeddings,

00:02:42.000 --> 00:02:45.190
crosses, and
bucketization easily.

00:02:45.190 --> 00:02:47.860
For example, to learn
the relationship

00:02:47.860 --> 00:02:51.390
between a specific query
and a specific item,

00:02:51.390 --> 00:02:55.140
you can define across columns
with a single line of code.

00:02:55.140 --> 00:02:57.520
Similarly, to learn
generalization,

00:02:57.520 --> 00:02:59.830
you can define an
embedding column

00:02:59.830 --> 00:03:01.580
with a single line of code.

00:03:01.580 --> 00:03:03.140
HENG-TZE CHENG:
So to get started,

00:03:03.140 --> 00:03:06.310
we encourage developers to
check out our blog posts

00:03:06.310 --> 00:03:10.560
in the description, which links
to our tutorials, code samples,

00:03:10.560 --> 00:03:12.330
and our research paper.

00:03:12.330 --> 00:03:14.070
We really hope more
and more people

00:03:14.070 --> 00:03:16.310
will find these
useful in their work

00:03:16.310 --> 00:03:20.940
and explore the possibilities of
Wide and Deep Learning with us.

