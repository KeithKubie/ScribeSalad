WEBVTT

1
00:00:00.880 --> 00:00:01.260
Okay.

2
00:00:01.260 --> 00:00:04.140
<v 1>In this video,
we'll quickly talk about how,
uh,</v>

3
00:00:04.170 --> 00:00:08.880
training would work in a more general conditional random fields set up.

4
00:00:11.350 --> 00:00:11.480
<v 0>Yeah.</v>

5
00:00:11.480 --> 00:00:16.010
<v 1>We've mentioned in previous videos that we do not need to restrict ourselves to</v>

6
00:00:16.011 --> 00:00:19.790
linear chain,
a conditional random fields.
If it were a modeling,

7
00:00:19.850 --> 00:00:21.650
a pixel labeling problem,

8
00:00:21.651 --> 00:00:26.180
we could use a grid structure where each node would correspond to one of the

9
00:00:26.181 --> 00:00:27.110
Pixel in the image.

10
00:00:27.111 --> 00:00:32.111
And we'd have factors that would express preferences on the joint value of

11
00:00:32.600 --> 00:00:37.070
neighboring pixels in the image.
A or we could have a very,
uh,

12
00:00:37.100 --> 00:00:42.100
complicated structure a over our graph where we'd have several types of loops

13
00:00:42.651 --> 00:00:46.670
and we can either be able to have factors that would involve more than just a

14
00:00:46.671 --> 00:00:47.750
two variables,

15
00:00:48.590 --> 00:00:53.590
could even have a structured at varies for different pairs of training inputs

16
00:00:54.410 --> 00:00:59.060
and targets.
So,
uh,
I'm mentioning had an example where we have webpages,

17
00:00:59.420 --> 00:01:00.080
uh,

18
00:01:00.080 --> 00:01:05.080
that and we're modeling the label between a web pages that share a link.

19
00:01:05.870 --> 00:01:08.640
And if we had different groups of web pages,

20
00:01:08.650 --> 00:01:13.160
then a different groups of webpages might have a different,

21
00:01:13.190 --> 00:01:15.930
we'll of course have different network structures.

22
00:01:15.931 --> 00:01:20.690
So from modeling the webpages from,
uh,
the pages of one,

23
00:01:20.800 --> 00:01:24.700
uh,
on,
in one university website versus,

24
00:01:24.701 --> 00:01:26.180
and other University of websites,

25
00:01:26.480 --> 00:01:31.190
of course these would be different pages and the connected differently.
So,

26
00:01:31.250 --> 00:01:33.170
um,
they're conditional random fields.

27
00:01:33.410 --> 00:01:37.160
A framework is very general for modeling the interaction between random

28
00:01:37.161 --> 00:01:40.910
variables.
So concepts that we're trying to model.

29
00:01:41.570 --> 00:01:44.870
So what about training such general conditional random fields?

30
00:01:47.510 --> 00:01:47.680
<v 0>Okay.</v>

31
00:01:47.680 --> 00:01:51.850
<v 1>Um,
well,
uh,
to do that,
we'd still perform great in descent.</v>

32
00:01:51.980 --> 00:01:55.870
So for a given pair of a target and input,

33
00:01:56.110 --> 00:02:01.110
we would need to derive what is the expression for the partial derivative of the

34
00:02:01.961 --> 00:02:05.830
negative log.
Likelihood of observing some given target,

35
00:02:05.831 --> 00:02:09.790
given some input with respect to any parameter theta in our model.

36
00:02:10.300 --> 00:02:12.520
And uh,
it also,
uh,

37
00:02:12.521 --> 00:02:16.060
it actually takes a fairly simple expression,
uh,

38
00:02:16.250 --> 00:02:19.390
that's very general.
So that's partial.
There,

39
00:02:19.391 --> 00:02:24.391
there will always be minus the sum over all the factors of the partial they live

40
00:02:25.181 --> 00:02:29.410
with respect to my parameter of interests of the luck factors.

41
00:02:29.680 --> 00:02:32.980
So over all the factors I'm taking the,
uh,

42
00:02:33.100 --> 00:02:37.750
some of the partial list of the each luck factor with respect to my parameter.

43
00:02:38.260 --> 00:02:38.601
And I'm,

44
00:02:38.601 --> 00:02:43.580
I'm subtracting two that the expectation over the,

45
00:02:43.590 --> 00:02:46.990
uh,
what could be the true label,
why?
Uh,

46
00:02:47.010 --> 00:02:50.500
I did this is the expectation with respect to uh,

47
00:02:50.590 --> 00:02:53.290
the model again,

48
00:02:53.291 --> 00:02:57.040
the sum of the partial [inaudible] of the luck factors.

49
00:02:57.340 --> 00:03:02.170
So here we have a difference between what a agreement of what is a based on what

50
00:03:02.171 --> 00:03:06.520
is observed minus a gradient based on what the model thinks.

51
00:03:07.060 --> 00:03:11.080
It's conditioned on the input but we're doing an expectation over what the model

52
00:03:11.081 --> 00:03:13.920
thinks is most likely.
And uh,

53
00:03:14.080 --> 00:03:17.530
if you think about this expression when it's trying to do is that it's trying to

54
00:03:17.531 --> 00:03:22.531
make the luck factor larger for the values of y that are in the training set and

55
00:03:23.430 --> 00:03:28.430
an expectation is trying to make the luck factors smaller in expectation for

56
00:03:30.490 --> 00:03:34.460
essentially every value of y based on what the model,
um,

57
00:03:34.570 --> 00:03:38.380
currently thinks the way the science probability to other values of y.

58
00:03:38.830 --> 00:03:41.680
So if the model becomes very good than p of y,

59
00:03:41.681 --> 00:03:44.660
given x t is going to be peaked again,

60
00:03:44.810 --> 00:03:48.550
assigned a very high probability to the true,
uh,
uh,

61
00:03:48.580 --> 00:03:51.460
target yt and then zero to everything else,

62
00:03:51.490 --> 00:03:55.480
in which case these two terms with cancel out and otherwise training with

63
00:03:55.481 --> 00:03:57.490
proceed until,
uh,

64
00:03:57.520 --> 00:04:02.520
we have converged to a conditional distribution that's a very close to has I'm

65
00:04:03.370 --> 00:04:06.820
essentially perfect probability to the,
uh,
the true answer.

66
00:04:08.320 --> 00:04:10.130
Now in this expression,
uh,

67
00:04:10.340 --> 00:04:14.320
the main problem is computing the expectation over why here.

68
00:04:14.321 --> 00:04:19.090
So this expectation here of this whole expression.
So for one thing,

69
00:04:19.091 --> 00:04:24.091
and it's an expectation over all values for all the elements in the why vector,

70
00:04:25.650 --> 00:04:26.483
uh,

71
00:04:26.500 --> 00:04:31.180
but usually since each of the lock factors will only involved or maybe a pair or

72
00:04:31.181 --> 00:04:33.280
a triplet or just a subset of the why's,

73
00:04:33.281 --> 00:04:37.480
then usually this expectation reduces to an expectation over just a subset of

74
00:04:37.481 --> 00:04:40.130
the barrel goals.
Uh,
so,
you know,

75
00:04:40.131 --> 00:04:42.900
use fun sense in linear chain conditional random fields.

76
00:04:42.901 --> 00:04:47.560
We saw that essentially these expectations are only over either a single yk

77
00:04:47.620 --> 00:04:49.450
variable or a pair of white key variables.

78
00:04:49.451 --> 00:04:53.830
So often it's only going to involve a few of these white variables.

79
00:04:54.460 --> 00:04:56.260
Uh,
that being said,
uh,

80
00:04:56.290 --> 00:05:01.290
computing the conditional distribution over these subset of y variables.

81
00:05:02.080 --> 00:05:05.950
Uh,
if the graph is not a tree or a chain,

82
00:05:06.490 --> 00:05:07.790
that's still attractive,

83
00:05:07.791 --> 00:05:12.130
but we have to approximate that conditional distribution.
And what we can use is,

84
00:05:12.260 --> 00:05:15.900
uh,
the loopy variant of belief propagation,
uh,

85
00:05:15.940 --> 00:05:17.980
to approximate that conditional distribution.

86
00:05:20.440 --> 00:05:24.870
So is it just a reminder of how we can use,
uh,

87
00:05:25.070 --> 00:05:28.810
the log messages as computed by belief propagation?

88
00:05:28.811 --> 00:05:32.680
So we're running a belief propagation,
usually in luck space.

89
00:05:32.860 --> 00:05:37.860
We run it where we iterate over all the messages that are being passed between

90
00:05:38.051 --> 00:05:41.080
factors and variables and vice versa.

91
00:05:41.500 --> 00:05:45.820
And we go over these messages several times until the value of the messages

92
00:05:45.821 --> 00:05:49.780
doesn't change.
And once this has converged,
then we can compute,
for instance,

93
00:05:49.781 --> 00:05:54.781
the marginal probability over just a single y given x as just the exponential of

94
00:05:55.861 --> 00:05:56.531
the luck factor.

95
00:05:56.531 --> 00:06:01.531
That only involves why k plus the sum for all the other factors of the log

96
00:06:03.021 --> 00:06:05.450
messages,
uh,
that comes from them.

97
00:06:06.420 --> 00:06:09.440
So this way we have a way of approximating,
in this case,

98
00:06:09.441 --> 00:06:13.820
the marginal distribution,
but we can generalize this formula to any subset of,

99
00:06:13.880 --> 00:06:17.990
of why gays.
Um,
so I won't go more into the details of this.

100
00:06:18.140 --> 00:06:19.460
This is just give you a headstart.

101
00:06:19.520 --> 00:06:23.060
If you want to go look at the literature for how to perform and,
uh,

102
00:06:23.090 --> 00:06:26.060
train general a conditional random fields,

103
00:06:26.090 --> 00:06:28.280
models using loopy belief propagation.

