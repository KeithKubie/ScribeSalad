WEBVTT
Kind: captions
Language: en

00:00:03.130 --> 00:00:05.170
MARZIA NICCOLAI: Hi, welcome
to Optimizing

00:00:05.170 --> 00:00:06.190
your App Engine App.

00:00:06.190 --> 00:00:06.825
I'm Marzia Niccolai.

00:00:06.825 --> 00:00:08.770
GREG DARKE: I'm Greg Darke.

00:00:08.770 --> 00:00:09.620
TROY TRIMBLE: And I'm
Troy Trimble.

00:00:09.620 --> 00:00:10.510
MARZIA NICCOLAI: And
we're going to be

00:00:10.510 --> 00:00:12.390
talking to you today.

00:00:12.390 --> 00:00:15.190
Here's a little overview
of the agenda

00:00:15.190 --> 00:00:16.129
that we'll be covering.

00:00:16.129 --> 00:00:19.620
First, we're going to go into
some coding tips on how to

00:00:19.620 --> 00:00:22.460
write your App Engine app
more efficiently.

00:00:22.460 --> 00:00:24.900
Then we're going to cover some
performance settings.

00:00:24.900 --> 00:00:27.300
And then we're also going to
talk a little bit about what

00:00:27.300 --> 00:00:29.830
we're planning in the future and
how this will impact the

00:00:29.830 --> 00:00:34.230
way you scale and
write your app.

00:00:34.230 --> 00:00:37.470
First, I just want to talk a
little bit about latency

00:00:37.470 --> 00:00:40.470
versus cost, which is really
what we're trying to cover

00:00:40.470 --> 00:00:42.450
here when we mean optimizing.

00:00:42.450 --> 00:00:44.340
Now, these aren't always
conflicting goals.

00:00:44.340 --> 00:00:47.450
Sometimes these things are
conflicting and sometimes when

00:00:47.450 --> 00:00:50.220
you optimize for one, you
optimize for the other.

00:00:50.220 --> 00:00:52.760
My general rule of thumb when
I think about these things

00:00:52.760 --> 00:00:56.860
are, if I make something more
efficient, that's generally

00:00:56.860 --> 00:01:00.580
also going to lower its cost.

00:01:00.580 --> 00:01:04.160
If I'm using more resources to
accomplish lower latency,

00:01:04.160 --> 00:01:08.880
that's usually going to be
higher costs for me.

00:01:08.880 --> 00:01:12.630
And sometimes with App Engine
and the way we have our

00:01:12.630 --> 00:01:15.560
performance settings available,
you can choose to

00:01:15.560 --> 00:01:18.950
lower the amount of resources
you're using.

00:01:18.950 --> 00:01:22.290
It's going to be less cost,
but it's also going to be

00:01:22.290 --> 00:01:25.580
slightly slower latency.

00:01:25.580 --> 00:01:27.280
Now, the other question
is, what do we

00:01:27.280 --> 00:01:29.710
mean by good latency?

00:01:29.710 --> 00:01:33.190
So I've put up a slide here from
an admin console graph on

00:01:33.190 --> 00:01:35.840
an application that I work on,
and I've highlighted your

00:01:35.840 --> 00:01:39.340
lowest latency handler and our
highest latency handler.

00:01:39.340 --> 00:01:42.620
Now, for the base handler, 46
milliseconds, I'm just going

00:01:42.620 --> 00:01:44.360
to say I think that's
pretty good.

00:01:44.360 --> 00:01:47.000
It's what your user's going to
hit the first time they come

00:01:47.000 --> 00:01:48.710
to your app.

00:01:48.710 --> 00:01:52.780
46 milliseconds, they get
something served to them.

00:01:52.780 --> 00:01:52.990
Now.

00:01:52.990 --> 00:01:57.080
The highest latency handler,
you're going to ask 942

00:01:57.080 --> 00:01:58.760
milliseconds, basically,
1 second.

00:01:58.760 --> 00:02:00.830
Is that good or that bad?

00:02:00.830 --> 00:02:02.830
Well, I mean, a lot
of that depends.

00:02:02.830 --> 00:02:05.560
And a lot of that really covers
the heart of what we're

00:02:05.560 --> 00:02:07.590
talking about today,
which is, well,

00:02:07.590 --> 00:02:09.100
who's seeing that request?

00:02:09.100 --> 00:02:12.260
If your user's seeing that
request, is a second good?

00:02:12.260 --> 00:02:14.760
But if it's just some requests
running in the backend, maybe

00:02:14.760 --> 00:02:17.560
I don't care if it's
942 milliseconds.

00:02:17.560 --> 00:02:19.970
Of course, in this case,
it's something that's

00:02:19.970 --> 00:02:22.100
asynchronously requested
but served to my user.

00:02:22.100 --> 00:02:24.620
So I see that 942
milliseconds.

00:02:24.620 --> 00:02:26.020
I'm saying, well,
it's not bad.

00:02:26.020 --> 00:02:28.680
But could I be doing
something better?

00:02:28.680 --> 00:02:29.930
I probably could.

00:02:32.880 --> 00:02:34.810
Apparently, I have to be
standing here to get

00:02:34.810 --> 00:02:35.880
the thing to work.

00:02:35.880 --> 00:02:39.430
OK, so I just want to say that
we're using Python here for

00:02:39.430 --> 00:02:40.530
our code examples.

00:02:40.530 --> 00:02:45.230
But really, a lot of these
things are language agnostic.

00:02:45.230 --> 00:02:47.380
When we do say something that's
language specific,

00:02:47.380 --> 00:02:48.540
we'll call it out.

00:02:48.540 --> 00:02:50.760
And even the Python, please
don't copy and paste this

00:02:50.760 --> 00:02:54.210
Python in your app because it's
more as an example of

00:02:54.210 --> 00:02:58.720
good and bad things to do, not
really production-ready code.

00:02:58.720 --> 00:03:01.330
So the first thing, what I'm
going to talk about, is

00:03:01.330 --> 00:03:02.510
Datastore tips.

00:03:02.510 --> 00:03:05.910
And I'm going to structure this,
first, things that we've

00:03:05.910 --> 00:03:07.980
actually seen, things
not to do.

00:03:07.980 --> 00:03:10.650
And then some general tips
of things to do.

00:03:10.650 --> 00:03:14.310
Now, the first thing I get
asked are, what are some

00:03:14.310 --> 00:03:17.210
general strategies when I'm
actually writing my models for

00:03:17.210 --> 00:03:18.360
the Datastore?

00:03:18.360 --> 00:03:22.000
And what I always try to tell
people is think about the

00:03:22.000 --> 00:03:26.030
pages that you're serving to
the users and try to design

00:03:26.030 --> 00:03:29.600
your models so that those pages
are served efficiently.

00:03:29.600 --> 00:03:33.100
And, of course, pages that you
rarely see, you shouldn't

00:03:33.100 --> 00:03:36.170
spend as much time optimizing
as pages that you see a lot.

00:03:36.170 --> 00:03:38.800
Because those are going to be
the bulk of your requests and

00:03:38.800 --> 00:03:40.320
the bulk of your costs.

00:03:40.320 --> 00:03:43.150
And this isn't really related to
optimizing your app per se,

00:03:43.150 --> 00:03:44.470
but everyone wanted
me to plug.

00:03:44.470 --> 00:03:47.970
Make sure you put your app
on the higher application

00:03:47.970 --> 00:03:50.990
Datastore or migrate your app
because now you can't really

00:03:50.990 --> 00:03:53.330
create apps on the Master/Slave
Datastore.

00:03:53.330 --> 00:03:55.690
But this is going to be the most
important of all because

00:03:55.690 --> 00:03:57.820
you want your app to be reliable
above everything

00:03:57.820 --> 00:04:01.830
else, and the HRD Datastore
will help you do that.

00:04:01.830 --> 00:04:04.050
So I have some specific examples
of things I'm going

00:04:04.050 --> 00:04:04.890
to go over.

00:04:04.890 --> 00:04:07.720
And I'll just dive
right into those.

00:04:07.720 --> 00:04:10.410
This is my first Datastore
anti-pattern.

00:04:13.890 --> 00:04:16.279
It generally looks something
like this.

00:04:16.279 --> 00:04:19.640
You have a query and you're
fetching one result.

00:04:19.640 --> 00:04:21.360
And basically, what this code's
supposed to say is I

00:04:21.360 --> 00:04:24.410
know what model I want to
retrieve to the Datastore.

00:04:24.410 --> 00:04:27.990
So I have to ask myself, why
am I going to query for it?

00:04:27.990 --> 00:04:31.050
Instead, we have something
called a Datastore get.

00:04:31.050 --> 00:04:34.390
And here I have a get by key
name, which is something I can

00:04:34.390 --> 00:04:35.550
set when I create the model.

00:04:35.550 --> 00:04:38.515
There's also Datastore IDs if
you don't set a key name.

00:04:41.110 --> 00:04:43.240
If you know what the model
you're expecting is, you

00:04:43.240 --> 00:04:45.340
really want to try to write
your app so you can use as

00:04:45.340 --> 00:04:47.390
many gets as possible.

00:04:47.390 --> 00:04:50.660
And the next slide really
shows the reason.

00:04:50.660 --> 00:04:53.660
If you look at what happened
when I did a query, well, I

00:04:53.660 --> 00:04:56.950
did two Datastore read ops, and
it took me 59 milliseconds

00:04:56.950 --> 00:04:59.990
because I had to scan through
some indexes.

00:04:59.990 --> 00:05:03.040
After, 1 Datastore fetch
op and 4 milliseconds.

00:05:03.040 --> 00:05:08.140
This is really a no-brainer, 50%
less and 14 times faster.

00:05:08.140 --> 00:05:11.540
So this is what I mean, cost
and efficiency not always

00:05:11.540 --> 00:05:14.180
contradictory.

00:05:14.180 --> 00:05:18.080
Of course, for Python, really
this is something that's

00:05:18.080 --> 00:05:19.210
language-specific.

00:05:19.210 --> 00:05:21.960
Don't use fetch at all.

00:05:21.960 --> 00:05:25.790
Fetch actually calls another
function called run and then

00:05:25.790 --> 00:05:27.690
puts it in a list.

00:05:27.690 --> 00:05:30.170
So you can just cut
out run directly,

00:05:30.170 --> 00:05:31.950
which returns a generator.

00:05:31.950 --> 00:05:36.420
And that's going to be less
memory for your application.

00:05:36.420 --> 00:05:39.020
And anecdotally,
no guarantees.

00:05:39.020 --> 00:05:42.450
It's about 10% to 15% faster
because it uses an

00:05:42.450 --> 00:05:45.480
asynchronous prefetch, which is
something that Greg's going

00:05:45.480 --> 00:05:48.830
to cover a little bit later,
asynchronicity.

00:05:48.830 --> 00:05:53.370
But this is something to call
out specifically for Python.

00:05:53.370 --> 00:05:56.270
Now, my next Datastore
anti-design pattern is

00:05:56.270 --> 00:05:59.040
something that you may not be
using in your app today

00:05:59.040 --> 00:06:00.270
because it's something
we just--

00:06:00.270 --> 00:06:03.140
I think it was two or three
releases ago, something called

00:06:03.140 --> 00:06:04.720
projection queries.

00:06:04.720 --> 00:06:07.710
Projection queries are really
good if you're reading data

00:06:07.710 --> 00:06:10.660
only and it's a small entity.

00:06:10.660 --> 00:06:15.470
This first thing is what an old
query would look like on

00:06:15.470 --> 00:06:20.980
App Engine, a model where and
then the filter thing.

00:06:20.980 --> 00:06:24.300
Now, the second one looks very
familiar to those of us who

00:06:24.300 --> 00:06:27.640
love SQL, which is I'm actually
just selecting the

00:06:27.640 --> 00:06:29.950
fields I actually
want to read.

00:06:29.950 --> 00:06:33.610
This again, was something not
supported until recently.

00:06:33.610 --> 00:06:37.910
But now that it is, if
you just want the

00:06:37.910 --> 00:06:39.980
newest update of Firefox--

00:06:39.980 --> 00:06:42.900
I think we can wait.

00:06:42.900 --> 00:06:46.640
Let's go back to full screen.

00:06:46.640 --> 00:06:52.610
So I just want to print out
these two fields from my

00:06:52.610 --> 00:06:57.080
model, so that's all I'm going
to retrieve, text and author.

00:06:57.080 --> 00:07:02.450
And the big news here is the
first one is a full Datastore

00:07:02.450 --> 00:07:06.450
read op, which is $0.07 for
every 100,000 ops in App

00:07:06.450 --> 00:07:07.640
Engine pricing.

00:07:07.640 --> 00:07:13.580
The second query, $0.01 per
100,000 ops, 85% less.

00:07:13.580 --> 00:07:15.970
And this is something that's
really easy to switch out.

00:07:15.970 --> 00:07:18.850
But again, one thing to know
about projection queries is

00:07:18.850 --> 00:07:21.680
you can't write to an entity
return from a projection

00:07:21.680 --> 00:07:24.090
queries, so something
to be aware of.

00:07:24.090 --> 00:07:26.550
You can always, if you find
something you want to write,

00:07:26.550 --> 00:07:28.650
fetch that entity
and write it.

00:07:28.650 --> 00:07:30.710
Or you don't have to replace
it everywhere.

00:07:30.710 --> 00:07:35.010
But where you can use it,
you really should.

00:07:35.010 --> 00:07:42.290
OK, one of my favorite/least
favorite anti-design patterns,

00:07:42.290 --> 00:07:44.270
something that looks really
natural to all of us--

00:07:44.270 --> 00:07:46.470
limit offset.

00:07:46.470 --> 00:07:48.370
And it seems like a good idea.

00:07:48.370 --> 00:07:51.860
Except what happens in App
Engine when you specify an

00:07:51.860 --> 00:07:55.260
offset is it scans through those
first 10 indexes and

00:07:55.260 --> 00:07:56.870
then reads the next 10.

00:07:56.870 --> 00:08:01.360
So basically, you're almost
doing 20 results worth of work

00:08:01.360 --> 00:08:02.950
with this query.

00:08:02.950 --> 00:08:06.420
What we haven't set in App
Engine is a Datastore cursor.

00:08:06.420 --> 00:08:10.000
What cursors do is they return
the place where that query

00:08:10.000 --> 00:08:11.350
left off to you.

00:08:11.350 --> 00:08:14.680
So the next time you go to run
that query, you can specify

00:08:14.680 --> 00:08:18.820
that cursor so it knows
where to start from.

00:08:18.820 --> 00:08:22.900
And again, this is something
where you see here 25 fetch

00:08:22.900 --> 00:08:25.460
ops in the first example because
we're doing almost all

00:08:25.460 --> 00:08:28.020
of the work of the query.

00:08:28.020 --> 00:08:31.730
And after, we're just doing the
10 that we want, and it's

00:08:31.730 --> 00:08:36.559
47% less, 13 times faster.

00:08:36.559 --> 00:08:38.460
So now we're going to get into
some things that you can

00:08:38.460 --> 00:08:41.289
really do that will help
improve the efficiency.

00:08:41.289 --> 00:08:44.110
And you remember one thing
that I talked about was

00:08:44.110 --> 00:08:46.330
minimizing the amount
of work you're doing

00:08:46.330 --> 00:08:47.990
when serving a page.

00:08:47.990 --> 00:08:50.810
So we've recently introduced
something that's called

00:08:50.810 --> 00:08:54.330
embedded entities, and it's a
way to store structured data

00:08:54.330 --> 00:08:55.840
within a model.

00:08:55.840 --> 00:08:58.290
And what this really can help
you do is denormalize your

00:08:58.290 --> 00:09:00.020
data in the sensible way.

00:09:00.020 --> 00:09:04.680
So in this case, what you see
is a contact where I want to

00:09:04.680 --> 00:09:07.120
store addresses within
a contact.

00:09:07.120 --> 00:09:09.780
So I may want to store these
addresses elsewhere or use

00:09:09.780 --> 00:09:11.300
them somewhere else.

00:09:11.300 --> 00:09:15.850
But I can also store this
model in my contact.

00:09:15.850 --> 00:09:19.190
And if I need to put all that
on one page, all the

00:09:19.190 --> 00:09:20.370
information is there.

00:09:20.370 --> 00:09:22.190
One query, it's really great.

00:09:22.190 --> 00:09:25.050
Now, this isn't something that's
going to save you any

00:09:25.050 --> 00:09:27.720
money, but it is something
that's going to help your app

00:09:27.720 --> 00:09:30.170
run faster.

00:09:30.170 --> 00:09:35.700
And the last thing I want to
point out is don't index

00:09:35.700 --> 00:09:38.440
things that don't need
to be indexed.

00:09:38.440 --> 00:09:41.680
When App Engine serves results,
it always serves them

00:09:41.680 --> 00:09:44.530
for a precomputed index
for a query.

00:09:44.530 --> 00:09:47.900
So usually, what we do for
most product models on a

00:09:47.900 --> 00:09:50.980
property is we write all of
the indexes every time you

00:09:50.980 --> 00:09:52.910
create or update that.

00:09:52.910 --> 00:09:54.640
But you know that there's some
things that you're not

00:09:54.640 --> 00:09:56.390
actually ever going to
want to query on.

00:09:56.390 --> 00:09:59.050
In this case, I have something
called a display name.

00:09:59.050 --> 00:10:01.550
That's just something that
probably was supposed to look

00:10:01.550 --> 00:10:04.200
nice on the page that I want
to store in my model.

00:10:04.200 --> 00:10:06.970
I'm never really going to query
on this, so why am I

00:10:06.970 --> 00:10:09.790
going to write an index
row every time

00:10:09.790 --> 00:10:11.090
I update this model?

00:10:11.090 --> 00:10:14.620
So by simply putting index=false
here, when I

00:10:14.620 --> 00:10:17.570
update this, that index
row won't get written.

00:10:17.570 --> 00:10:19.060
Now, I won't be able
to query on it.

00:10:19.060 --> 00:10:20.440
But in this case,
I don't care.

00:10:20.440 --> 00:10:22.630
And I've also saved myself some
money because I'm not

00:10:22.630 --> 00:10:25.480
writing an index.

00:10:25.480 --> 00:10:29.250
Of course, now that we've
covered some Datastore stuff,

00:10:29.250 --> 00:10:30.820
we can do better.

00:10:30.820 --> 00:10:33.710
Because the data store's still
a lot of work, and so you can

00:10:33.710 --> 00:10:35.260
actually add a layer of caching

00:10:35.260 --> 00:10:37.130
on top of the Datastore.

00:10:37.130 --> 00:10:39.730
It's caching, caching, caching
because there's actually three

00:10:39.730 --> 00:10:42.590
different kinds of caching
we're going to cover.

00:10:42.590 --> 00:10:44.820
The first one is something
we always are

00:10:44.820 --> 00:10:45.700
talking a lot about.

00:10:45.700 --> 00:10:47.230
It's Memcache.

00:10:47.230 --> 00:10:50.560
Memcache is a fast memory cache
that we have available.

00:10:50.560 --> 00:10:53.240
It's basically something where
you can place something that

00:10:53.240 --> 00:10:54.510
you frequently--

00:10:54.510 --> 00:10:57.040
that is hard to compute, but
you're frequently reading,

00:10:57.040 --> 00:10:59.520
either from the Datastore,
or URLFetch, or anything.

00:10:59.520 --> 00:11:01.580
But we use the Datastore here.

00:11:01.580 --> 00:11:06.480
So in this case, I took the
guest book example, where

00:11:06.480 --> 00:11:09.220
you're reading greetings and
we're serving the greetings

00:11:09.220 --> 00:11:10.880
every time you hit
the guest book.

00:11:10.880 --> 00:11:13.290
So there's no need to pull all
those greetings from the

00:11:13.290 --> 00:11:16.420
Datastore every time, even if
it hasn't been updated.

00:11:16.420 --> 00:11:20.520
So what I can do instead is
simply add all of that data to

00:11:20.520 --> 00:11:24.320
Memcache and pull it when
I look for that page.

00:11:24.320 --> 00:11:25.650
And if it's not there,
I can always

00:11:25.650 --> 00:11:26.580
read from the Datastore.

00:11:26.580 --> 00:11:29.490
Memcache doesn't take
that long of a time.

00:11:29.490 --> 00:11:31.640
I do want to point out one thing
that I've done in this

00:11:31.640 --> 00:11:35.030
example, which is instead of
actually storing the models in

00:11:35.030 --> 00:11:38.360
Memcache, I've stored
some rendered HTML.

00:11:38.360 --> 00:11:41.080
And the reason I do this is a
gotcha that we see sometimes,

00:11:41.080 --> 00:11:45.450
which is Memcache pickles your
data or serializes your data.

00:11:45.450 --> 00:11:48.040
So what happens is, if your
model has changed between

00:11:48.040 --> 00:11:49.480
writing and reading
from Memcache, you

00:11:49.480 --> 00:11:50.920
might get in trouble.

00:11:50.920 --> 00:11:53.560
This is one way to get around
it, or it's just something you

00:11:53.560 --> 00:11:56.400
can be aware about when you
update models on your live

00:11:56.400 --> 00:11:57.650
serving application.

00:12:01.070 --> 00:12:03.430
The next thing I'm going to
cover is instance caching.

00:12:03.430 --> 00:12:04.790
Well, what's instance caching?

00:12:04.790 --> 00:12:08.100
This is basically the memory of
your App Engine instance,

00:12:08.100 --> 00:12:11.210
and it's storing something
in that memory when

00:12:11.210 --> 00:12:13.490
the instance is alive.

00:12:13.490 --> 00:12:15.700
It allows you to have your
own eviction policy.

00:12:15.700 --> 00:12:18.000
Something I didn't mention was
Memcache was the least

00:12:18.000 --> 00:12:19.610
recently used evicted.

00:12:19.610 --> 00:12:21.490
In this case, storing something
as a global

00:12:21.490 --> 00:12:24.470
variable, it allows you to store
it the way you want it.

00:12:24.470 --> 00:12:26.370
It is also faster
than Memcache.

00:12:26.370 --> 00:12:29.050
It's, well, near
instantaneous.

00:12:29.050 --> 00:12:32.700
Memcache is pretty fast, but
there may be some times when

00:12:32.700 --> 00:12:33.990
you want it to be faster.

00:12:33.990 --> 00:12:36.870
And in this case, you can just
store it as a global variable

00:12:36.870 --> 00:12:39.700
in your instance.

00:12:39.700 --> 00:12:43.300
But for those Python users,
really what you need to be

00:12:43.300 --> 00:12:45.840
doing is using NDB instead.

00:12:45.840 --> 00:12:49.740
I've used usually DB examples
in most of my code samples

00:12:49.740 --> 00:12:51.980
because NDB is relatively
used.

00:12:51.980 --> 00:12:55.330
But I've actually written here
what a real Memcache storing

00:12:55.330 --> 00:12:58.100
and reading algorithm
would look like.

00:12:58.100 --> 00:13:00.590
I'm not even going to bother to
go through it because it's

00:13:00.590 --> 00:13:03.630
got like locks and tombstones
and all of this crazy stuff.

00:13:03.630 --> 00:13:06.420
But it's actually hard
to get this right.

00:13:06.420 --> 00:13:08.850
Storing things in Memcache,
storing things in instance

00:13:08.850 --> 00:13:12.670
caches, things being written,
having to update, NDB actually

00:13:12.670 --> 00:13:14.630
is doing all of this for you.

00:13:14.630 --> 00:13:17.320
So if you're a Python user
and you're not using

00:13:17.320 --> 00:13:18.670
NDB, check it out.

00:13:18.670 --> 00:13:21.990
Hopefully, it's pretty easy to
translate one to the other.

00:13:21.990 --> 00:13:25.050
And you'll get a lot of
really cool savings.

00:13:25.050 --> 00:13:28.100
I don't know that much about
Java, but I hear Objectify has

00:13:28.100 --> 00:13:29.110
some of the same stuff.

00:13:29.110 --> 00:13:31.560
So you should check
that out, too.

00:13:36.120 --> 00:13:39.590
And finally, here's just a
summary of what the difference

00:13:39.590 --> 00:13:41.990
is between Datastore,
Memcache, and

00:13:41.990 --> 00:13:43.190
instance caching are.

00:13:43.190 --> 00:13:45.350
You can see here the latency.

00:13:45.350 --> 00:13:48.460
Obviously, Datastore has
the biggest latency.

00:13:48.460 --> 00:13:50.500
But also, you're storing
the most things there.

00:13:50.500 --> 00:13:54.420
And it's written there, and
it's written near forever

00:13:54.420 --> 00:13:56.850
unless you choose
to remove it.

00:13:56.850 --> 00:14:00.300
In terms of the storage,
something to call out in

00:14:00.300 --> 00:14:03.950
instance caching specifically,
is we do allow you to have

00:14:03.950 --> 00:14:06.930
larger size instances
in App Engine.

00:14:06.930 --> 00:14:09.530
The smallest size is the
default, but we do have bigger

00:14:09.530 --> 00:14:11.250
sizes that costs more
money, that

00:14:11.250 --> 00:14:12.630
have more memory available.

00:14:12.630 --> 00:14:15.580
And one thing you could do with
that memory is store some

00:14:15.580 --> 00:14:18.050
stuff in there to make your
application faster.

00:14:20.940 --> 00:14:23.850
And the last thing I'm going
to cover in caching is

00:14:23.850 --> 00:14:28.700
something that really will have
a huge benefit for pages

00:14:28.700 --> 00:14:32.050
of your application that are
rarely updated with new

00:14:32.050 --> 00:14:35.140
content, which is set cache
control headers.

00:14:35.140 --> 00:14:37.100
It's such an easy thing to do.

00:14:37.100 --> 00:14:41.580
But what it does is when
that request is sent to

00:14:41.580 --> 00:14:44.030
intermediate servers and
browsers, it tells those

00:14:44.030 --> 00:14:46.930
servers and browsers how long
to hold that page before

00:14:46.930 --> 00:14:49.010
re-requesting it from
the server.

00:14:49.010 --> 00:14:51.340
So that means that your app's
actually not going to have to

00:14:51.340 --> 00:14:53.720
really do almost any work
for these requests.

00:14:53.720 --> 00:14:57.140
You get those requests only for
bandwidth in App Engine.

00:14:57.140 --> 00:15:02.030
And the thing you want to be
careful, of course, is that

00:15:02.030 --> 00:15:05.860
you can't ever force these
pages to be re-requested

00:15:05.860 --> 00:15:08.710
before the expiration time.

00:15:08.710 --> 00:15:10.950
So please set it reasonably.

00:15:10.950 --> 00:15:12.820
I mean, we've actually
seen apps that have

00:15:12.820 --> 00:15:14.770
set it for 365 days.

00:15:14.770 --> 00:15:17.200
And of course, they set
it, and they realize

00:15:17.200 --> 00:15:18.230
they've made a mistake.

00:15:18.230 --> 00:15:20.270
Well, once we've served
the requests,

00:15:20.270 --> 00:15:21.310
it's out of our hands.

00:15:21.310 --> 00:15:22.650
We can't force the servers.

00:15:22.650 --> 00:15:24.670
We can't force the browser
to re-request anyway.

00:15:24.670 --> 00:15:28.050
So make this a sane value,
but still use it.

00:15:28.050 --> 00:15:31.530
And in fact, if you go in the
admin console, you can go to

00:15:31.530 --> 00:15:34.620
the drop-down and do request
by type per second.

00:15:34.620 --> 00:15:37.070
And what you'll see here is it
actually breaks down for this

00:15:37.070 --> 00:15:41.040
application how many static
requests, but also, how many

00:15:41.040 --> 00:15:43.910
requests are served
from edge caching.

00:15:43.910 --> 00:15:46.470
And you can see here that those
requests I'm only really

00:15:46.470 --> 00:15:47.780
paying bandwidth for.

00:15:47.780 --> 00:15:50.160
My server's not doing any work,
or my instance isn't

00:15:50.160 --> 00:15:51.520
doing any work.

00:15:51.520 --> 00:15:55.070
So it's a great way to save
yourself some money and save

00:15:55.070 --> 00:15:59.650
your computing power to do
something more interesting.

00:15:59.650 --> 00:16:01.630
Now I'm going to hand it off
to Greg who's going to talk

00:16:01.630 --> 00:16:04.350
about batch API requests.

00:16:04.350 --> 00:16:06.940
GREG DARKE: So, what are
batch API requests?

00:16:06.940 --> 00:16:10.630
Batch API requests allow you to
make operations on multiple

00:16:10.630 --> 00:16:13.470
pieces of data in single RPC.

00:16:13.470 --> 00:16:17.840
And doing this allows the App
Engine servers to actually fan

00:16:17.840 --> 00:16:19.525
out your request to
multiple servers.

00:16:19.525 --> 00:16:23.920
And it allows you to amortize
the cost of that RPC over

00:16:23.920 --> 00:16:25.420
multiple pieces of data.

00:16:25.420 --> 00:16:26.930
So what do I mean by that?

00:16:26.930 --> 00:16:29.770
Well, I'm going to use the
example of Taskqueue here.

00:16:29.770 --> 00:16:32.040
So say, for example,
I'd like to insert

00:16:32.040 --> 00:16:35.260
50 tasks into Taskqueue.

00:16:35.260 --> 00:16:37.830
Just simple code, it goes
through loops and adds them.

00:16:37.830 --> 00:16:40.260
At the top here is a graph
from Appstats.

00:16:40.260 --> 00:16:44.210
It was introduced by Guido van
Rossum in a Google I/O talk in

00:16:44.210 --> 00:16:46.350
2010, so I suggest you have
a look at that for more

00:16:46.350 --> 00:16:47.590
information.

00:16:47.590 --> 00:16:50.270
But in this graph here, you can
actually see each of the

00:16:50.270 --> 00:16:51.740
RPCs that's made by
the application.

00:16:51.740 --> 00:16:54.220
I've actually trimmed off just
after about the first 10

00:16:54.220 --> 00:16:56.790
because it goes down to 50.

00:16:56.790 --> 00:17:00.070
As you can see, each request
here is taking about 10 to 15

00:17:00.070 --> 00:17:01.090
milliseconds.

00:17:01.090 --> 00:17:05.260
So overall, it's over 600
milliseconds to enter these

00:17:05.260 --> 00:17:08.490
actual requests into
Taskqueue.

00:17:08.490 --> 00:17:11.900
So if I change this code
slightly, so I still construct

00:17:11.900 --> 00:17:14.720
all the tasks, add them to a
list, I'm using the batch API

00:17:14.720 --> 00:17:16.650
interface, which is q.add.

00:17:16.650 --> 00:17:20.240
You can see that all the tasks
are inserted, but it's only

00:17:20.240 --> 00:17:21.650
taking 22 milliseconds.

00:17:21.650 --> 00:17:25.160
So it makes the requests
much, much faster.

00:17:25.160 --> 00:17:29.470
So we have many APIs that
actually support this--

00:17:29.470 --> 00:17:32.400
Memcache, Datastore, Taskqueue,
Full Text Search.

00:17:32.400 --> 00:17:35.230
They actually all support
being able to operate on

00:17:35.230 --> 00:17:39.080
multiple entities for Datastore,
add multiple

00:17:39.080 --> 00:17:41.550
documents to an index for
full-text search.

00:17:41.550 --> 00:17:44.490
And Memcache, you can
either get or put

00:17:44.490 --> 00:17:47.040
multiple entries there.

00:17:47.040 --> 00:17:49.210
So some things you will have to
be careful of when you're

00:17:49.210 --> 00:17:52.390
using the batch API with
Taskqueue, for example, is

00:17:52.390 --> 00:17:56.350
that you can actually only
insert up to 32 megabytes

00:17:56.350 --> 00:17:57.630
worth of requests there.

00:17:57.630 --> 00:18:01.200
So using the batch APIs can be
slightly more complicated and

00:18:01.200 --> 00:18:03.930
has some gotchas around the size
and the number of things

00:18:03.930 --> 00:18:04.950
that you can add to them.

00:18:04.950 --> 00:18:07.060
But overall, it's a considerable
win to both

00:18:07.060 --> 00:18:12.050
latency and it makes your
application faster.

00:18:12.050 --> 00:18:14.630
So I will just leave batch API
requests for a second.

00:18:14.630 --> 00:18:17.800
I'll go onto using asynchronous
workflows.

00:18:17.800 --> 00:18:22.420
So asynchronous workflows allows
you to start an RPC and

00:18:22.420 --> 00:18:24.940
then continue doing work
in your application.

00:18:24.940 --> 00:18:27.880
This allows you to do multiple
slow operations in parallel,

00:18:27.880 --> 00:18:31.240
such as performing multiple URL
fetches as I've got here.

00:18:31.240 --> 00:18:34.840
It also allows you to perform
CPU-intensive operations while

00:18:34.840 --> 00:18:36.720
waiting for the IO
to complete.

00:18:36.720 --> 00:18:40.530
This was alluded to by Marzia
earlier on with Datastore.

00:18:40.530 --> 00:18:44.340
When you're doing a query in
Datastore using the .run

00:18:44.340 --> 00:18:47.860
interface in Python and also in
Java, what actually happens

00:18:47.860 --> 00:18:51.960
is we do an asynchronous call
to fetch the next set of

00:18:51.960 --> 00:18:55.010
entities, so that we're
performing the IO to find the

00:18:55.010 --> 00:18:56.870
next entries while you'll
still iterating over the

00:18:56.870 --> 00:19:00.380
previous batch that
we returned.

00:19:00.380 --> 00:19:02.795
So I'm just going to
go over a quick--

00:19:02.795 --> 00:19:06.560
over an example of how to use
asynchronous RPCs to speed up

00:19:06.560 --> 00:19:08.890
URL fetch in this example.

00:19:08.890 --> 00:19:10.370
I'm going to give two
examples of this.

00:19:10.370 --> 00:19:12.870
This is using the slightly older
method using callbacks.

00:19:12.870 --> 00:19:14.590
And then I'll go over
the new one.

00:19:14.590 --> 00:19:18.600
So the main thing that you need
to write here for using

00:19:18.600 --> 00:19:21.320
any of the asynchronous APIs
is you'll actually need an

00:19:21.320 --> 00:19:24.900
event loop that keeps track
of the RPCs and calls the

00:19:24.900 --> 00:19:28.390
wait_any function, which will
automatically call the

00:19:28.390 --> 00:19:30.190
callbacks when they're ready.

00:19:30.190 --> 00:19:34.960
I have the do-fetch_call down
here, which inside it I have a

00:19:34.960 --> 00:19:37.530
closure to construct the
actual callback.

00:19:37.530 --> 00:19:40.500
And I'm passing it to
make_fetch_call, which is the

00:19:40.500 --> 00:19:46.110
actual asynchronous version of
fetch in the URL fetch API.

00:19:46.110 --> 00:19:48.540
In other APIs, the asynchronous
version is

00:19:48.540 --> 00:19:50.960
generally named the same
as the normal version.

00:19:50.960 --> 00:19:53.560
So if Datastore is get, the
asynchronous version is

00:19:53.560 --> 00:19:56.810
get_asynch, and this
follows on.

00:19:56.810 --> 00:19:59.560
URL fetch was our first
synchronous

00:19:59.560 --> 00:20:02.650
API and is thus different.

00:20:02.650 --> 00:20:07.840
So with this example here, I'm
passing the callback into the

00:20:07.840 --> 00:20:09.310
create RPC function.

00:20:09.310 --> 00:20:14.080
And so when the actual call is
being made and the request is

00:20:14.080 --> 00:20:17.080
finished, this callback will
occur, k and we'll

00:20:17.080 --> 00:20:19.260
be given the data.

00:20:19.260 --> 00:20:22.690
So as you can see, this version
here actually doesn't

00:20:22.690 --> 00:20:25.150
pass anything back from
that callback back

00:20:25.150 --> 00:20:26.460
into the main function.

00:20:26.460 --> 00:20:28.950
That's actually quite difficult
to do using the

00:20:28.950 --> 00:20:31.650
callback model of programming
here.

00:20:31.650 --> 00:20:33.880
So I will give you--

00:20:33.880 --> 00:20:37.100
this is the new version
using NDB tasklets.

00:20:37.100 --> 00:20:39.350
The main difference here is
that instead of having to

00:20:39.350 --> 00:20:42.500
write your own event loop, NDB
has one written for you.

00:20:42.500 --> 00:20:46.680
And, in fact, it actually makes
use of the new yield

00:20:46.680 --> 00:20:48.180
statement as co-routines.

00:20:48.180 --> 00:20:50.590
So all you have to do is wrap
the function with this

00:20:50.590 --> 00:20:54.820
NDB.tasklet decorator, and then
you just yield any RPCs.

00:20:54.820 --> 00:20:58.630
And this allows you to write
programs that look like they

00:20:58.630 --> 00:21:02.990
flow straight down but actually
will be calling into

00:21:02.990 --> 00:21:04.570
and out of the event loop.

00:21:04.570 --> 00:21:06.570
So you can see this example
here does the same thing.

00:21:06.570 --> 00:21:08.590
It calls make_fetch_call.

00:21:08.590 --> 00:21:10.690
It just passes the
result back.

00:21:10.690 --> 00:21:13.340
And we've actually got the
return statement here.

00:21:13.340 --> 00:21:16.260
So in NDB, with tasklets, you
have to actually raise the

00:21:16.260 --> 00:21:19.240
return exception, and that
result gets passed back into

00:21:19.240 --> 00:21:21.230
the list down the bottom.

00:21:21.230 --> 00:21:24.060
So if we have a look at the
actual before and after.

00:21:24.060 --> 00:21:28.520
So before, with fetching four
RPCs-- sorry, four websites,

00:21:28.520 --> 00:21:30.770
it actually takes nearly
4 seconds.

00:21:30.770 --> 00:21:33.010
Doing it with an asynchronous
model, you can still do the

00:21:33.010 --> 00:21:35.710
processing in between each
of these requests.

00:21:35.710 --> 00:21:38.880
And in this case, it takes
just over 3 seconds.

00:21:38.880 --> 00:21:42.190
So a fairly large saving.

00:21:42.190 --> 00:21:45.540
But if this was in your main
handler, as Marzia alluded to

00:21:45.540 --> 00:21:48.790
earlier, this still was probably
way too slow for you.

00:21:52.876 --> 00:21:55.350
Some of the APIs that support
asynchronous--

00:21:55.350 --> 00:21:59.420
Blobstore, Memcache, URLFetch,
and Datastore.

00:21:59.420 --> 00:22:03.610
So what you may actually want
to do with requests, if you

00:22:03.610 --> 00:22:06.170
wanted to gather data from an
external site and then be able

00:22:06.170 --> 00:22:09.120
to serve it as a part of your
homepage request, what you may

00:22:09.120 --> 00:22:11.740
want to do is actually cache
that data and actually cache

00:22:11.740 --> 00:22:13.630
it in Datastore.

00:22:13.630 --> 00:22:16.980
So Datastore, as we said
before, is about 50

00:22:16.980 --> 00:22:18.940
milliseconds per request.

00:22:18.940 --> 00:22:22.080
Whereas if you're looking at the
same thing for doing a URL

00:22:22.080 --> 00:22:24.530
fetch to a slow external
resource, that can actually

00:22:24.530 --> 00:22:26.290
take over 2 seconds.

00:22:30.000 --> 00:22:34.050
So what you may actually want
to use offline processes for

00:22:34.050 --> 00:22:36.510
is you can actually defer
updating those pages.

00:22:36.510 --> 00:22:40.940
Because otherwise, when the
cache expires with just using

00:22:40.940 --> 00:22:43.700
a naive solution, you would
actually still have to make

00:22:43.700 --> 00:22:47.190
one person wait for that
2-second request to be able to

00:22:47.190 --> 00:22:50.260
gather it again and
fill your cache.

00:22:50.260 --> 00:22:54.820
Other things you can use
offline processes for--

00:22:54.820 --> 00:22:57.610
Doing slow external, say,
billing processes.

00:22:57.610 --> 00:23:00.760
You can also use it to make
workflow systems or perform

00:23:00.760 --> 00:23:02.430
fan-in, which is described
in Brett

00:23:02.430 --> 00:23:05.560
Slatkin's talk from 2010.

00:23:05.560 --> 00:23:08.970
So I will actually go over
an example of how to use

00:23:08.970 --> 00:23:11.090
Taskqueue and actually
do that URL fetching

00:23:11.090 --> 00:23:13.050
example I gave before.

00:23:13.050 --> 00:23:14.850
So I'm using NDB here.

00:23:14.850 --> 00:23:18.340
The reason I'm using NDB is that
it gives the transparent

00:23:18.340 --> 00:23:20.580
caching to Memcache and
the instance cache.

00:23:20.580 --> 00:23:24.790
So if this was a fairly highly
used property and serving on

00:23:24.790 --> 00:23:28.030
your home page, then this
actually may not have to even

00:23:28.030 --> 00:23:29.120
go to Datastore at all.

00:23:29.120 --> 00:23:31.670
It could already be fetched in
Memcache, or it could also

00:23:31.670 --> 00:23:35.440
just be in the instance
cache already.

00:23:35.440 --> 00:23:41.370
So going to the get URL
method, so I'll

00:23:41.370 --> 00:23:42.330
skip truncate time.

00:23:42.330 --> 00:23:44.360
It's just a helper method
used to ensure

00:23:44.360 --> 00:23:45.540
that we don't fetch--

00:23:45.540 --> 00:23:48.350
I don't create too many tasks
to actually fetch this.

00:23:48.350 --> 00:23:51.260
So if we look at the get URL
method, we construct a key

00:23:51.260 --> 00:23:53.790
name based on the URL.

00:23:53.790 --> 00:23:57.025
We're using sha1, so that way,
we can actually ensure that

00:23:57.025 --> 00:23:59.260
the name space of all these
keys is fairly evenly

00:23:59.260 --> 00:24:01.270
distributed.

00:24:01.270 --> 00:24:04.780
So the first that we do is we
just try and fetch it from

00:24:04.780 --> 00:24:09.110
Datastore, if it's there.

00:24:09.110 --> 00:24:13.440
If it's not there, we'll just
fetch it straight from the

00:24:13.440 --> 00:24:15.360
external resource,
save it, and just

00:24:15.360 --> 00:24:16.370
return it straight away.

00:24:16.370 --> 00:24:20.120
So in the case of the first user
viewing this page, they

00:24:20.120 --> 00:24:26.600
may actually have to pay the
2-second request delay here.

00:24:26.600 --> 00:24:29.330
If you were implementing this
in your own code, you can

00:24:29.330 --> 00:24:35.500
actually send a 302 back to the
user or send a placeholder

00:24:35.500 --> 00:24:37.950
so that the page will render.

00:24:37.950 --> 00:24:40.670
And then it can actually go back
to and try and re-request

00:24:40.670 --> 00:24:41.980
the page so that the
user actually

00:24:41.980 --> 00:24:44.270
sees something before.

00:24:44.270 --> 00:24:50.780
So here, if we do find it in
Memcache or in Datastore, we

00:24:50.780 --> 00:24:53.620
check if the data has expired.

00:24:53.620 --> 00:24:57.370
If it has, we just defer
a task into Taskqueue.

00:24:57.370 --> 00:25:00.360
And that will then
asynchronously go and fetch

00:25:00.360 --> 00:25:03.630
the new version while we return
the current version

00:25:03.630 --> 00:25:05.710
that's in the cache
back to the user.

00:25:05.710 --> 00:25:08.000
And as you can see, with the
update cache method, it's just

00:25:08.000 --> 00:25:11.590
a fairly simple fetch from the
external resource and then

00:25:11.590 --> 00:25:14.350
blindly put it to Datastore.

00:25:14.350 --> 00:25:17.465
So I will now pass along to
Troy who will talk about

00:25:17.465 --> 00:25:18.220
performance settings.

00:25:18.220 --> 00:25:19.200
TROY TRIMBLE: Great.

00:25:19.200 --> 00:25:20.670
Thanks, Greg.

00:25:20.670 --> 00:25:29.200
[SIDE CONVERSATION]

00:25:29.200 --> 00:25:29.770
TROY TRIMBLE: Test, test.

00:25:29.770 --> 00:25:30.620
All right, there we go.

00:25:30.620 --> 00:25:31.990
Let's just do it.

00:25:31.990 --> 00:25:33.500
We'll do it live.

00:25:33.500 --> 00:25:34.070
All right.

00:25:34.070 --> 00:25:34.690
Hi, everybody.

00:25:34.690 --> 00:25:37.170
I'm a senior software engineer
on App Engine.

00:25:37.170 --> 00:25:38.920
And today I'm going to be
talking to you about

00:25:38.920 --> 00:25:40.780
performance settings that you
can set for your app to

00:25:40.780 --> 00:25:43.770
optimize for either low
cost or low latency.

00:25:43.770 --> 00:25:45.650
So performance settings
are something we

00:25:45.650 --> 00:25:47.240
introduced last year.

00:25:47.240 --> 00:25:49.420
We haven't talked to them
publicly as far as I know,

00:25:49.420 --> 00:25:51.370
like in a conference
to this setting.

00:25:51.370 --> 00:25:53.070
So there's a few of them.

00:25:53.070 --> 00:25:54.670
They're on the application
settings portion

00:25:54.670 --> 00:25:55.990
of the admin console.

00:25:55.990 --> 00:25:57.930
And you can see here that
there's the front-end instance

00:25:57.930 --> 00:26:00.140
class that Marzia touched
on earlier.

00:26:00.140 --> 00:26:02.210
There's idle instances, and
there's pending latency.

00:26:05.160 --> 00:26:07.440
So the first thing I'd
like to talk to is

00:26:07.440 --> 00:26:09.010
optimizing for low latency.

00:26:09.010 --> 00:26:11.070
So why do you want to optimize
for low latency?

00:26:11.070 --> 00:26:13.860
Well, it's pretty
straightforward.

00:26:13.860 --> 00:26:16.650
You have a web UI or a mobile
frontend, and you want to have

00:26:16.650 --> 00:26:20.800
a very, very snappy, good web
experience for your users.

00:26:20.800 --> 00:26:23.480
So the first setting that you
can use is what we call

00:26:23.480 --> 00:26:25.180
minimum idle instances.

00:26:25.180 --> 00:26:27.980
And what this tells the App
Engine scheduler is that you

00:26:27.980 --> 00:26:31.200
would prefer that it keep around
at least this number of

00:26:31.200 --> 00:26:34.120
idle instances at all times
for your application.

00:26:34.120 --> 00:26:36.880
And what these idle instances
are used for is to send

00:26:36.880 --> 00:26:39.630
traffic to them when all of
your active or dynamic

00:26:39.630 --> 00:26:42.530
instances are currently busy
serving other requests.

00:26:42.530 --> 00:26:44.920
And so this is extremely useful
for things like bursty

00:26:44.920 --> 00:26:48.070
or unpredictable traffic, such
that you don't have to suffer

00:26:48.070 --> 00:26:51.400
the dreaded loading request
for a user-facing request.

00:26:51.400 --> 00:26:53.720
And so what we will typically
tell both internal and

00:26:53.720 --> 00:26:56.060
external partners
to do is to--

00:26:56.060 --> 00:26:58.040
if they're going to have some
sort of event, like a Google

00:26:58.040 --> 00:27:00.720
I/O event, or some sort of
press, or some new feature

00:27:00.720 --> 00:27:02.670
that they think is going to just
drive a bunch of traffic

00:27:02.670 --> 00:27:05.540
to their site, that they
temporarily increase this

00:27:05.540 --> 00:27:08.550
number in order to sort of
buffer that bursty traffic.

00:27:08.550 --> 00:27:10.790
And then, later on they can
turn it back down again in

00:27:10.790 --> 00:27:13.090
order to save some more money.

00:27:13.090 --> 00:27:17.320
The second setting that you can
set to optimize for low

00:27:17.320 --> 00:27:19.720
latency is the max pending
latency setting.

00:27:19.720 --> 00:27:22.310
And what this setting tells
the scheduler is that you

00:27:22.310 --> 00:27:26.480
would prefer that no request to
your application wait for

00:27:26.480 --> 00:27:29.840
longer than this amount of time
before being processed by

00:27:29.840 --> 00:27:31.290
one of your instances.

00:27:31.290 --> 00:27:32.960
And so again, this is something
to tell the

00:27:32.960 --> 00:27:37.900
scheduler that if you really
want a sappy user experience

00:27:37.900 --> 00:27:42.600
that it not wait for an
instance to turn up.

00:27:42.600 --> 00:27:44.550
But it just automatically--

00:27:44.550 --> 00:27:46.630
the scheduler should
automatically provision a

00:27:46.630 --> 00:27:49.600
number of instances in order
to keep the average pending

00:27:49.600 --> 00:27:52.980
latency below this threshold.

00:27:52.980 --> 00:27:56.340
And so here's an example of the
instances console, which

00:27:56.340 --> 00:27:57.980
is also in the admin console.

00:27:57.980 --> 00:28:01.560
And you can see here that in
this case we've set three for

00:28:01.560 --> 00:28:03.520
the min idle instances
setting.

00:28:03.520 --> 00:28:06.290
And that is represented on the
right by the availability as

00:28:06.290 --> 00:28:09.960
resident, and then the dynamic
ones below that are sort of

00:28:09.960 --> 00:28:11.190
the active dynamic clones.

00:28:11.190 --> 00:28:13.990
So you can see, as I mentioned
earlier, that the majority of

00:28:13.990 --> 00:28:15.900
the requests to this application
have been handled

00:28:15.900 --> 00:28:18.150
by the dynamic instances and
only a few have been handled

00:28:18.150 --> 00:28:19.400
by the resident ones.

00:28:22.940 --> 00:28:25.870
Here's an example of the
settings when actually setting

00:28:25.870 --> 00:28:29.580
the min idle instances and
the max pending latency.

00:28:29.580 --> 00:28:31.980
In this case, the admin console
is warning me because

00:28:31.980 --> 00:28:35.970
I did not appropriately set the
warmup request setting in

00:28:35.970 --> 00:28:37.460
my app.yaml or
appengine-web.xml.

00:28:40.270 --> 00:28:44.450
And this is a requirement for
min idle instances to be used

00:28:44.450 --> 00:28:45.880
appropriately by
the scheduler.

00:28:45.880 --> 00:28:48.460
And that's because the scheduler
will spin up these

00:28:48.460 --> 00:28:50.780
idle instances in the background
using what we call

00:28:50.780 --> 00:28:52.010
warmup requests.

00:28:52.010 --> 00:28:55.610
And you can read more
about that online.

00:28:55.610 --> 00:28:57.800
I'd also like to touch on,
just very briefly, a new

00:28:57.800 --> 00:29:00.240
feature that just came
out with 1.7.0.

00:29:00.240 --> 00:29:02.630
And that is the page speed
service integration.

00:29:02.630 --> 00:29:05.370
And I won't go into great depth
here, but it is a part

00:29:05.370 --> 00:29:06.420
of the performance settings.

00:29:06.420 --> 00:29:09.140
And what it will do is basically
will cache and do

00:29:09.140 --> 00:29:12.240
content rewriting for a number
of your static resources in

00:29:12.240 --> 00:29:15.870
order to, basically speed up
your service data content.

00:29:18.730 --> 00:29:21.860
So next I'd like to talk to
optimizing for low cost.

00:29:21.860 --> 00:29:24.560
And what I mean here, I mean
everybody wants to optimize

00:29:24.560 --> 00:29:26.070
for low cost.

00:29:26.070 --> 00:29:28.750
But in the performance settings
instance, what we're

00:29:28.750 --> 00:29:31.790
really talking about here are
users who are hobbyists, or

00:29:31.790 --> 00:29:33.600
people who have applications
that they're not really

00:29:33.600 --> 00:29:34.770
monetizing their traffic.

00:29:34.770 --> 00:29:37.420
They're running this thing for
free, and they want to just

00:29:37.420 --> 00:29:40.170
try and keep their costs low
at the expense of having

00:29:40.170 --> 00:29:43.220
potentially higher latency
user requests to their

00:29:43.220 --> 00:29:44.560
application.

00:29:44.560 --> 00:29:46.020
And the first setting
that we have

00:29:46.020 --> 00:29:48.510
here is max idle instances.

00:29:48.510 --> 00:29:50.850
And the idea is that you're
telling the App Engine

00:29:50.850 --> 00:29:53.820
scheduler that you would prefer
that it not keep around

00:29:53.820 --> 00:29:56.540
any more than this number of
idle instances to help serve

00:29:56.540 --> 00:29:57.830
your application.

00:29:57.830 --> 00:30:01.170
And this also factors into how
we calculate the front-end

00:30:01.170 --> 00:30:03.280
instance hours charged
for your application.

00:30:03.280 --> 00:30:06.880
Because if you told us I only
want three idle instances.

00:30:06.880 --> 00:30:09.730
And for whatever reason, the
scheduler put up 10 idle

00:30:09.730 --> 00:30:11.530
instances, well, then you
shouldn't be charged

00:30:11.530 --> 00:30:13.770
for the extra 7.

00:30:13.770 --> 00:30:16.800
The second setting here is
the min pending latency.

00:30:16.800 --> 00:30:19.410
And what this says to the
scheduler is you would prefer

00:30:19.410 --> 00:30:23.590
that the scheduler allow
requests to wait at least this

00:30:23.590 --> 00:30:27.660
long before causing a loading
request and spinning up a new

00:30:27.660 --> 00:30:29.240
instance to serve the request.

00:30:29.240 --> 00:30:31.310
And what this does is basically
maximizes your

00:30:31.310 --> 00:30:34.500
utilization of the instances
that you already have to try

00:30:34.500 --> 00:30:38.080
and keep costs low.

00:30:38.080 --> 00:30:40.790
So again, here's an example
of me setting these.

00:30:40.790 --> 00:30:44.610
I've set the max idle instance
to 3 and the min pending

00:30:44.610 --> 00:30:48.010
latency to 750 milliseconds.

00:30:48.010 --> 00:30:50.290
And at this point, I'd like to
just take a brief moment

00:30:50.290 --> 00:30:54.010
explain why I have specific
numbers here of 3 and 750

00:30:54.010 --> 00:30:54.970
milliseconds.

00:30:54.970 --> 00:30:56.740
These are not generally
applicable to all

00:30:56.740 --> 00:30:57.780
applications.

00:30:57.780 --> 00:30:59.950
They are just examples
that I've chosen.

00:30:59.950 --> 00:31:02.000
The right number for your
application is completely

00:31:02.000 --> 00:31:03.910
dependent upon your
application.

00:31:03.910 --> 00:31:06.520
And we encourage you to try
and experiment with these

00:31:06.520 --> 00:31:09.060
settings in order to find the
appropriate value for your

00:31:09.060 --> 00:31:11.640
application.

00:31:11.640 --> 00:31:14.730
Next, I'd like to talk about a
very serious anti-pattern that

00:31:14.730 --> 00:31:17.650
we've seen far too many
times in the wild.

00:31:17.650 --> 00:31:21.060
And this is basically using
both the low cost and low

00:31:21.060 --> 00:31:23.580
latency settings together
simultaneously for your

00:31:23.580 --> 00:31:24.980
application.

00:31:24.980 --> 00:31:26.350
And the problem with this--
and you can see

00:31:26.350 --> 00:31:27.340
what I've done here--

00:31:27.340 --> 00:31:29.920
is that both min and max
idle instance is 15.

00:31:29.920 --> 00:31:32.610
And the delta between the min
pending and max pending

00:31:32.610 --> 00:31:35.270
latency is 50 milliseconds.

00:31:35.270 --> 00:31:37.610
And the problem with these
settings is that it confuses

00:31:37.610 --> 00:31:39.810
the App Engine scheduler
so much.

00:31:39.810 --> 00:31:42.250
It just doesn't know how to
operate within such a thin

00:31:42.250 --> 00:31:44.470
margin of these thresholds.

00:31:44.470 --> 00:31:48.650
And what you see, as the App
Engine admin, is that your

00:31:48.650 --> 00:31:49.350
application--

00:31:49.350 --> 00:31:51.470
the scheduler will spin up
these instances for your

00:31:51.470 --> 00:31:53.810
application, but they'll be
extremely short-lived.

00:31:53.810 --> 00:31:56.790
They'll come into existence and
go out very, very quickly

00:31:56.790 --> 00:32:00.320
while it tries to basically
satisfy these two thresholds.

00:32:00.320 --> 00:32:03.390
And it basically makes a bunch
of churn in the system.

00:32:03.390 --> 00:32:06.460
And it also will devalue things
like instance caching

00:32:06.460 --> 00:32:09.250
because your instance cache
won't be around that long.

00:32:09.250 --> 00:32:12.450
So our recommendation
is to not do this.

00:32:12.450 --> 00:32:15.240
Our recommendation is to do
something like this where you

00:32:15.240 --> 00:32:18.390
use just the low cost or just
the low latency settings, and

00:32:18.390 --> 00:32:19.895
you set the other ones
to automatic.

00:32:23.330 --> 00:32:26.100
So now I'd like to talk about
App Engine servers, which is

00:32:26.100 --> 00:32:28.720
an ongoing project right now,
and how it'll affect the

00:32:28.720 --> 00:32:31.170
performance settings that you
can set for your application

00:32:31.170 --> 00:32:34.630
to make them more powerful and
more flexible going forward.

00:32:34.630 --> 00:32:37.510
But first, I'd like to talk
about the motivation for why

00:32:37.510 --> 00:32:38.420
we're doing this.

00:32:38.420 --> 00:32:41.870
And the only way to do that is
by talking about what today's

00:32:41.870 --> 00:32:43.430
App Engine hierarchy
looks like.

00:32:43.430 --> 00:32:46.240
So as you can see here, we have
a top-level application.

00:32:46.240 --> 00:32:48.470
Each application has some
set of servers--

00:32:48.470 --> 00:32:49.930
or sorry, excuse
me, backends--

00:32:49.930 --> 00:32:51.280
and some sort of versions.

00:32:51.280 --> 00:32:53.040
And each of those has some
set of instances

00:32:53.040 --> 00:32:54.430
associated with them.

00:32:54.430 --> 00:32:56.770
And as a lot of you know if
you've been playing with the

00:32:56.770 --> 00:33:00.160
performance settings before,
only the default version

00:33:00.160 --> 00:33:02.320
receives the performance
settings that I

00:33:02.320 --> 00:33:04.370
talked about just now.

00:33:04.370 --> 00:33:07.430
And so what we've heard from
customers, from talking to

00:33:07.430 --> 00:33:09.960
them and getting feedback on
the architecture as it is

00:33:09.960 --> 00:33:12.160
today, is that one of the things
they really don't like

00:33:12.160 --> 00:33:14.800
is that they only have a single
version that will auto

00:33:14.800 --> 00:33:15.570
scale for them.

00:33:15.570 --> 00:33:17.090
And only a single version
will receive

00:33:17.090 --> 00:33:17.950
the performance setting.

00:33:17.950 --> 00:33:20.650
So all the non-default versions,
they don't get these

00:33:20.650 --> 00:33:23.100
performance settings.

00:33:23.100 --> 00:33:25.570
So what we've done is we're
going to move to a world that

00:33:25.570 --> 00:33:27.830
looks more like this,
basically.

00:33:27.830 --> 00:33:29.190
We're going to introduce
a new level,

00:33:29.190 --> 00:33:30.240
which is called a server.

00:33:30.240 --> 00:33:32.070
And what a server is, it is a

00:33:32.070 --> 00:33:34.330
conceptual grouping of versions.

00:33:34.330 --> 00:33:37.850
The idea being though, that in
this new world the default

00:33:37.850 --> 00:33:41.240
version of a server will now
be able to receive the

00:33:41.240 --> 00:33:44.540
performance settings and have
those be active and also be

00:33:44.540 --> 00:33:45.820
able to do autoscaling.

00:33:45.820 --> 00:33:48.160
So you can have multiple servers
with multiple default

00:33:48.160 --> 00:33:51.290
versions or a single default
version per server, but each

00:33:51.290 --> 00:33:54.290
of those will be able to scale
independently of one another.

00:33:54.290 --> 00:33:57.540
So, for example, you can have
a server with a default

00:33:57.540 --> 00:34:00.300
version that is for your
patchwork, for your MapReduce.

00:34:00.300 --> 00:34:04.030
And those performance settings
can try to utilize the

00:34:04.030 --> 00:34:06.530
instances as much as possible
because all you're trying to

00:34:06.530 --> 00:34:08.560
do is get throughput.

00:34:08.560 --> 00:34:11.239
You don't need low latency,
you just need throughput.

00:34:11.239 --> 00:34:14.800
Similarly, you continue to have
your web UI frontend that

00:34:14.800 --> 00:34:17.690
has extremely low latency and
high min idle instances in

00:34:17.690 --> 00:34:20.840
order to handle bursty traffic
and give a really snappy user

00:34:20.840 --> 00:34:22.449
experience.

00:34:22.449 --> 00:34:25.040
And so to give an example of
what these might look like in

00:34:25.040 --> 00:34:27.310
the future, like how we're
going to do performance

00:34:27.310 --> 00:34:29.340
settings in the future, is that
we're going to actually

00:34:29.340 --> 00:34:32.969
take them from the admin console
and put them into your

00:34:32.969 --> 00:34:36.580
app.yaml or your App
Engine web.xml.

00:34:36.580 --> 00:34:39.380
And so when you upload your
code, you will also upload the

00:34:39.380 --> 00:34:40.969
performance settings
that you feel

00:34:40.969 --> 00:34:42.620
appropriate for that code.

00:34:42.620 --> 00:34:45.940
And to give an example of that,
here is, for example, my

00:34:45.940 --> 00:34:47.449
mobile frontend server.

00:34:47.449 --> 00:34:50.030
So you can see here this is in
app.yaml, but there will be

00:34:50.030 --> 00:34:52.870
the equivalent in the
App Engine web.xml.

00:34:52.870 --> 00:34:54.760
And you can see here, though,
that there's a new field

00:34:54.760 --> 00:34:55.639
called server.

00:34:55.639 --> 00:34:57.250
This is my mobile frontend.

00:34:57.250 --> 00:34:58.730
We're going to have
also a new section

00:34:58.730 --> 00:34:59.830
called server settings.

00:34:59.830 --> 00:35:02.000
And so you can see here it's
exactly the same settings that

00:35:02.000 --> 00:35:04.780
we had in the admin console.

00:35:04.780 --> 00:35:08.110
In this case, I've set to an F2
and 25 min idle instances

00:35:08.110 --> 00:35:10.140
and the max pending
latency of 250.

00:35:10.140 --> 00:35:15.080
The automatics are there just
for completeness, but are

00:35:15.080 --> 00:35:18.340
actually unnecessary.

00:35:18.340 --> 00:35:21.330
Those are the default values.

00:35:21.330 --> 00:35:24.010
And so this is an example of
what today is traditionally

00:35:24.010 --> 00:35:24.880
called a backend.

00:35:24.880 --> 00:35:26.260
This is what it will look
like in the future.

00:35:26.260 --> 00:35:29.050
We're actually going to move
the settings from what is

00:35:29.050 --> 00:35:32.520
today in backends.yaml or
backends.xml and we're going

00:35:32.520 --> 00:35:37.350
to put them into the actual
app.yaml or App Engine web.xml

00:35:37.350 --> 00:35:38.210
going forward.

00:35:38.210 --> 00:35:41.580
And so, in this case, you see
this is my geo backend server.

00:35:41.580 --> 00:35:43.690
It has 10 instances.

00:35:43.690 --> 00:35:46.200
Static, just 10 instances,
no autoscaling.

00:35:46.200 --> 00:35:48.680
And it has an instance class of
B8 because I need a bunch

00:35:48.680 --> 00:35:53.060
of memory to do a bunch
of great geo caching.

00:35:53.060 --> 00:35:56.730
And so just to recap, Marzia
covered a lot of great

00:35:56.730 --> 00:35:59.790
Datastore modeling patterns and
anti-patterns, along with

00:35:59.790 --> 00:36:01.630
a number of caching
techniques.

00:36:01.630 --> 00:36:04.270
Greg talked about the batch
APIs, some of the synchronous

00:36:04.270 --> 00:36:07.500
APIs that we have and some
offline processing techniques.

00:36:07.500 --> 00:36:10.080
And I covered today's
performance settings and what

00:36:10.080 --> 00:36:11.990
they're going to look
like in the future.

00:36:11.990 --> 00:36:15.420
And so with that, we'd like to
thank all of you for coming.

00:36:15.420 --> 00:36:17.180
It's been a pleasure to
talk to you today.

00:36:17.180 --> 00:36:19.350
And we would like to open up
the floor for questions.

00:36:19.350 --> 00:36:21.990
I'd like to say very briefly
though, if you have a question

00:36:21.990 --> 00:36:24.520
that pertains specifically to
your app that you don't think

00:36:24.520 --> 00:36:27.190
would apply to other people in
the room, that you come find

00:36:27.190 --> 00:36:29.930
us afterwards given that this
is kind of a forum where we

00:36:29.930 --> 00:36:32.620
want to answer general questions
as much as possible.

00:36:32.620 --> 00:36:34.047
So thank you very much.

00:36:34.047 --> 00:36:40.320
[APPLAUSE]

00:36:40.320 --> 00:36:42.610
AUDIENCE: You were talking about
just selecting certain

00:36:42.610 --> 00:36:44.140
fields and that it's
much cheaper.

00:36:46.840 --> 00:36:49.270
I'm wondering why--

00:36:49.270 --> 00:36:50.135
MARZIA NICCOLAI: Project--
hello?

00:36:50.135 --> 00:36:51.045
It's on?

00:36:51.045 --> 00:36:51.500
Oh.

00:36:51.500 --> 00:36:52.530
Projection queries?

00:36:52.530 --> 00:36:54.940
AUDIENCE: Yes.

00:36:54.940 --> 00:36:56.870
Could it be also cheaper
than a get or

00:36:56.870 --> 00:36:58.120
is it just for querying?

00:37:01.960 --> 00:37:04.290
MARZIA NICCOLAI: They're both
small Datastore ops, so

00:37:04.290 --> 00:37:06.770
they're basically equivalent.

00:37:06.770 --> 00:37:08.980
But obviously, if you
are going to--

00:37:08.980 --> 00:37:10.490
I mean, you should always
use gets first.

00:37:10.490 --> 00:37:13.440
But if you have to do a query,
then using a projection query

00:37:13.440 --> 00:37:16.760
is going to save you money.

00:37:16.760 --> 00:37:18.750
I mean, if your entity
is sufficiently

00:37:18.750 --> 00:37:20.470
small, it's the best.

00:37:20.470 --> 00:37:22.650
Because sometimes projection
queries can take slightly

00:37:22.650 --> 00:37:25.060
longer if the entity
is too large.

00:37:25.060 --> 00:37:28.220
But for only a few fields,
it's a small op

00:37:28.220 --> 00:37:29.540
versus a read op.

00:37:29.540 --> 00:37:35.580
AUDIENCE: And if you do query
and you do it keys only, and

00:37:35.580 --> 00:37:39.920
then do the gets separately and
interleaf with Memcache,

00:37:39.920 --> 00:37:43.320
would your recommendation be
just to do a projection query

00:37:43.320 --> 00:37:45.890
or do a key only query and then
try to find the entities

00:37:45.890 --> 00:37:47.640
in the Memcache?

00:37:47.640 --> 00:37:49.550
MARZIA NICCOLAI: So in that
case, if you're doing a keys

00:37:49.550 --> 00:37:52.960
only query and then a get, it's
two small ops versus the

00:37:52.960 --> 00:37:55.530
one small op for the
projection query.

00:37:55.530 --> 00:37:58.145
So I believe you'd still be
better off, in that case, to

00:37:58.145 --> 00:37:59.310
do projection queries.

00:37:59.310 --> 00:38:02.580
Oh, and also, we have chocolate,
like the last

00:38:02.580 --> 00:38:05.240
session for people who
ask questions.

00:38:05.240 --> 00:38:09.460
So if you want afterwards, you
can come up and get some.

00:38:09.460 --> 00:38:11.680
AUDIENCE: Just curious if you
have some debugging tips for

00:38:11.680 --> 00:38:12.330
slow queries?

00:38:12.330 --> 00:38:14.390
I often find, especially if
I'm fetching a lot of

00:38:14.390 --> 00:38:16.640
information, it seems to be
going really slow, and it's

00:38:16.640 --> 00:38:18.630
kind of hard to debug because
it's somewhat asynchronous so

00:38:18.630 --> 00:38:20.530
you don't really see the cost
until later when you're using

00:38:20.530 --> 00:38:21.240
the information.

00:38:21.240 --> 00:38:23.370
And there's not a lot
of visibility into--

00:38:23.370 --> 00:38:25.610
under the hood, it seems like
when you're making queries

00:38:25.610 --> 00:38:27.650
that are within a bunch of
keys, there's lots of

00:38:27.650 --> 00:38:29.350
situations where it's ultimately
making multiple

00:38:29.350 --> 00:38:31.420
individual fetches and not doing
the sort of batch fetch

00:38:31.420 --> 00:38:33.220
you might expect a real
SQL database to do.

00:38:33.220 --> 00:38:36.040
So just, in general, how
do I go and look and

00:38:36.040 --> 00:38:37.490
see, why is this slow?

00:38:37.490 --> 00:38:39.040
Is it really hitting an index?

00:38:39.040 --> 00:38:40.470
What is it that's
taking so long?

00:38:40.470 --> 00:38:41.500
Is it the amount of
data that's being

00:38:41.500 --> 00:38:42.160
transferred, et cetera.

00:38:42.160 --> 00:38:44.190
Because it feels a little
opaque to me sometimes.

00:38:44.190 --> 00:38:46.660
MARZIA NICCOLAI: I think queries
always hit indexes.

00:38:46.660 --> 00:38:47.250
But--

00:38:47.250 --> 00:38:48.610
PETE WHITE: [INAUDIBLE].

00:38:48.610 --> 00:38:49.380
MARZIA NICCOLAI: Right.

00:38:49.380 --> 00:38:52.370
But I think in this
case when--

00:38:52.370 --> 00:38:55.370
so I'm not a super Datastore
expert, but like zigzag

00:38:55.370 --> 00:38:57.460
queries where you're joining
multiple things.

00:38:57.460 --> 00:39:00.750
I mean, I'm probably not the
right person to answer this in

00:39:00.750 --> 00:39:02.390
detail, maybe Greg.

00:39:02.390 --> 00:39:04.250
GREG DARKE: I'll just say if you
want to come and chat with

00:39:04.250 --> 00:39:07.183
us afterwards, we'd probably be
able to help you with that.

00:39:07.183 --> 00:39:07.970
AUDIENCE: OK.

00:39:07.970 --> 00:39:11.750
But I just wonder, are there
basic pages on the stats or

00:39:11.750 --> 00:39:13.220
something that show you,
kind of here are the

00:39:13.220 --> 00:39:14.410
queries you're making.

00:39:14.410 --> 00:39:15.885
Here are the slow queries, or
here's how long they're taking

00:39:15.885 --> 00:39:17.310
in general, or here's how much
data you're transferring, or

00:39:17.310 --> 00:39:18.440
how many different RPCs
you're actually

00:39:18.440 --> 00:39:20.060
making on the backend?

00:39:20.060 --> 00:39:21.560
TROY TRIMBLE: The Appstat stuff
that we were showing

00:39:21.560 --> 00:39:24.850
those, where like most of the
clips in here with sort of the

00:39:24.850 --> 00:39:27.140
timelines and stuff like that,
which most of them were in

00:39:27.140 --> 00:39:30.140
Greg's talk, that's an extremely
valuable debugging

00:39:30.140 --> 00:39:31.950
tool that you can do on a
single request basis.

00:39:31.950 --> 00:39:32.980
And it does have--

00:39:32.980 --> 00:39:35.620
at least for Python, it has a
full stack dump where it will

00:39:35.620 --> 00:39:38.410
drop you to the exact line
of code that executed it.

00:39:38.410 --> 00:39:40.810
So you really can drill down and
say, like, oh, this query

00:39:40.810 --> 00:39:42.000
took a really long time.

00:39:42.000 --> 00:39:44.100
It executed out of order
with this other one.

00:39:44.100 --> 00:39:45.910
And so this is the one I
need to really find.

00:39:45.910 --> 00:39:47.750
And it will tell you what the
line of code that is.

00:39:47.750 --> 00:39:48.000
AUDIENCE: Cool.

00:39:48.000 --> 00:39:48.920
MARZIA NICCOLAI: But
I also think--

00:39:48.920 --> 00:39:53.000
I mean, so there used to be
more queries that required

00:39:53.000 --> 00:39:56.690
composite indexes that then
they made not required.

00:39:56.690 --> 00:39:58.490
But you can still
re-add composite

00:39:58.490 --> 00:40:00.440
indexes for some queries.

00:40:00.440 --> 00:40:02.820
Those will write more indexes,
but they might be faster.

00:40:02.820 --> 00:40:06.010
But again, it's like there's a
lot of dependencies there that

00:40:06.010 --> 00:40:08.560
I'm probably not the best
expert to tell you.

00:40:08.560 --> 00:40:11.450
But that is one option also
when it used to require a

00:40:11.450 --> 00:40:14.540
composite index and now
no longer does.

00:40:14.540 --> 00:40:18.320
Adding back the composite index
might speed up the query

00:40:18.320 --> 00:40:19.280
in that case.

00:40:19.280 --> 00:40:20.086
AUDIENCE: Thanks.

00:40:20.086 --> 00:40:20.910
GREG DARKE: Up the back?

00:40:20.910 --> 00:40:23.290
AUDIENCE: Yeah, I
had a question.

00:40:23.290 --> 00:40:26.110
I'm wondering if you had any
plans of reducing the pricing

00:40:26.110 --> 00:40:29.660
maybe or maybe offering tier
pricing for things like games

00:40:29.660 --> 00:40:33.060
where it's harder to monetize
at the rate that the current

00:40:33.060 --> 00:40:34.310
pricing is.

00:40:36.350 --> 00:40:38.170
TROY TRIMBLE: I mean, we have
no plans on reducing pricing

00:40:38.170 --> 00:40:40.340
right now as far as I know.

00:40:40.340 --> 00:40:41.260
We're always evaluating.

00:40:41.260 --> 00:40:44.040
We know that we just came out
with new pricing for Compute

00:40:44.040 --> 00:40:46.260
Engine and stuff like
that that is tiered.

00:40:46.260 --> 00:40:48.890
So that seems to be the way that
things are going, but we

00:40:48.890 --> 00:40:50.140
have nothing to announce
today, unfortunately.

00:40:53.006 --> 00:40:53.910
AUDIENCE: Great.

00:40:53.910 --> 00:40:54.720
Thanks.

00:40:54.720 --> 00:40:57.510
I've noticed whenever I
configure a resident instance

00:40:57.510 --> 00:41:01.860
with the min idle instance, my
app, at like 2:00 in the

00:41:01.860 --> 00:41:04.660
morning when no one's hitting it
and I have an idle instance

00:41:04.660 --> 00:41:09.810
that I'm paying for, if I hit up
my app, it prefers to just

00:41:09.810 --> 00:41:15.270
spin up a dynamic instance
instead of kind of using what

00:41:15.270 --> 00:41:17.060
I'm paying for.

00:41:17.060 --> 00:41:20.620
MARZIA NICCOLAI: So that min
idle instances are really more

00:41:20.620 --> 00:41:23.240
geared toward handling
bursty traffic.

00:41:23.240 --> 00:41:25.620
So we always prefer
dynamic instances.

00:41:25.620 --> 00:41:29.010
But we keep the idle instances
around in case we haven't

00:41:29.010 --> 00:41:34.090
predicted correctly whether or
not there's enough dynamic

00:41:34.090 --> 00:41:35.260
instances around to serve it.

00:41:35.260 --> 00:41:37.000
So that's really the purpose.

00:41:37.000 --> 00:41:40.730
What I always kind of use in
an example is, if you're

00:41:40.730 --> 00:41:43.730
launching a website and you
might have a really steep,

00:41:43.730 --> 00:41:45.200
really sharp jump.

00:41:45.200 --> 00:41:47.410
And it's hard for us to predict
exactly how that's

00:41:47.410 --> 00:41:48.290
going to go.

00:41:48.290 --> 00:41:51.030
So then you might want to spin
up a lot of idle instances

00:41:51.030 --> 00:41:54.100
because those instances
can serve a request

00:41:54.100 --> 00:41:55.460
kind of in a pinch.

00:41:55.460 --> 00:41:58.680
But when you have traffic that's
more steady state or

00:41:58.680 --> 00:42:01.830
easy kind of flowing traffic,
the idle instances aren't as

00:42:01.830 --> 00:42:04.450
useful because we're usually
really good at predicting when

00:42:04.450 --> 00:42:06.270
to do a dynamic instance.

00:42:06.270 --> 00:42:08.910
AUDIENCE: So it's for burst,
not start-up time?

00:42:08.910 --> 00:42:09.900
MARZIA NICCOLAI: I mean,
it's for both.

00:42:09.900 --> 00:42:13.365
But you should think of it as
something that can serve a

00:42:13.365 --> 00:42:17.480
request while the dynamic
request is spinning up.

00:42:17.480 --> 00:42:17.930
AUDIENCE: Bingo.

00:42:17.930 --> 00:42:18.500
OK, cool.

00:42:18.500 --> 00:42:19.750
Thank you.

00:42:22.330 --> 00:42:22.630
AUDIENCE: Hello.

00:42:22.630 --> 00:42:24.180
Thank you for App Engine.

00:42:24.180 --> 00:42:28.000
And you said not to use limit
and offset, obviously because

00:42:28.000 --> 00:42:29.940
that's very inefficient,
right?

00:42:29.940 --> 00:42:32.230
TROY TRIMBLE: It's just really
offset in that case.

00:42:32.230 --> 00:42:33.030
AUDIENCE: Oh, offset.

00:42:33.030 --> 00:42:33.500
That's right.

00:42:33.500 --> 00:42:34.660
TROY TRIMBLE: Limit's fine,
just no offset.

00:42:34.660 --> 00:42:35.610
AUDIENCE: I meant that, yeah.

00:42:35.610 --> 00:42:39.260
But what about if you want to
page back and page forward?

00:42:39.260 --> 00:42:41.280
Cursor, you use cursor, right?

00:42:41.280 --> 00:42:43.560
MARZIA NICCOLAI: There's a start
and end cursor, both.

00:42:43.560 --> 00:42:45.580
I just use the start cursor.

00:42:45.580 --> 00:42:45.840
AUDIENCE: Right.

00:42:45.840 --> 00:42:46.430
Start cursor.

00:42:46.430 --> 00:42:51.160
But let's say you have a pay and
you want to go backward.

00:42:51.160 --> 00:42:53.800
Can you go backwards
and forward?

00:42:53.800 --> 00:42:55.220
The cursor only goes
forward, right?

00:42:55.220 --> 00:42:57.780
TROY TRIMBLE: No, you can set
an end cursor on a query.

00:42:57.780 --> 00:42:58.560
That's what she's saying.

00:42:58.560 --> 00:43:02.450
So by default, it's like there's
start_cursor and

00:43:02.450 --> 00:43:03.640
end_cursor.

00:43:03.640 --> 00:43:05.010
So you can say--

00:43:05.010 --> 00:43:08.520
I believe it's at the end, like,
basically of your query,

00:43:08.520 --> 00:43:09.970
you should have that cursor
should be the

00:43:09.970 --> 00:43:10.910
end of it, I believe.

00:43:10.910 --> 00:43:12.590
I know I'm not explaining
that very well.

00:43:12.590 --> 00:43:13.800
AUDIENCE: There's got to
be a better way of--

00:43:13.800 --> 00:43:14.660
AUDIENCE: Yeah, yeah.

00:43:14.660 --> 00:43:15.510
But you can't always--

00:43:15.510 --> 00:43:18.350
I can't always do that.

00:43:18.350 --> 00:43:19.600
I'll ask after.

00:43:24.430 --> 00:43:27.750
AUDIENCE: Is server
configuration enabled now or

00:43:27.750 --> 00:43:29.980
next version?

00:43:29.980 --> 00:43:32.480
MARZIA NICCOLAI: So the server
configuration is just what

00:43:32.480 --> 00:43:34.680
we're working on implementing
now.

00:43:34.680 --> 00:43:37.020
And one of the main reasons to
bring it up is because--

00:43:37.020 --> 00:43:41.300
I mean, I think we really are
looking for feedback on how

00:43:41.300 --> 00:43:44.130
that might be the most useful
for you and give you an idea

00:43:44.130 --> 00:43:46.190
of what we're thinking
about in the future.

00:43:46.190 --> 00:43:49.040
We don't have a target
release date, per se.

00:43:49.040 --> 00:43:50.380
TROY TRIMBLE: Yeah, we're
working on this

00:43:50.380 --> 00:43:51.230
actively right now.

00:43:51.230 --> 00:43:53.395
If you'd like to talk more about
it, I encourage you to

00:43:53.395 --> 00:43:55.920
come find me after we're done
here, and I'd love to hear

00:43:55.920 --> 00:43:57.730
feedback that you have
on it or thoughts.

00:43:57.730 --> 00:43:58.230
Definitely.

00:43:58.230 --> 00:43:59.480
That goes for everybody.

00:44:02.280 --> 00:44:06.290
AUDIENCE: Is there any way to
visualize pending latency?

00:44:06.290 --> 00:44:10.670
The one that you don't see in
the request and to me it seems

00:44:10.670 --> 00:44:12.470
it's nowhere reported.

00:44:12.470 --> 00:44:14.230
Also on the dashboard, right?

00:44:14.230 --> 00:44:16.020
MARZIA NICCOLAI: It's
in the logs.

00:44:16.020 --> 00:44:19.100
And what I would actually plug
for here is actually we don't

00:44:19.100 --> 00:44:20.580
have this--

00:44:20.580 --> 00:44:23.050
we actually haven't written
this, but it's probably really

00:44:23.050 --> 00:44:25.870
useful, is we have a Logs Reader
API, which actually

00:44:25.870 --> 00:44:29.370
allows you to either MapReduce
or read through your logs.

00:44:29.370 --> 00:44:33.260
And you could do something like
read the pending latency.

00:44:33.260 --> 00:44:34.960
Read all your requests, the
information about your

00:44:34.960 --> 00:44:37.970
requests, dump it into something
like BigQuery, or

00:44:37.970 --> 00:44:40.000
even write a custom kind
of visualization for

00:44:40.000 --> 00:44:41.300
those kinds of logs.

00:44:41.300 --> 00:44:46.250
I mean, the raw tools for the
stuff is here, but it's kind

00:44:46.250 --> 00:44:50.960
of very nascent in terms of
making these really easy to

00:44:50.960 --> 00:44:51.590
use for you.

00:44:51.590 --> 00:44:52.830
But it's possible.

00:44:52.830 --> 00:44:55.980
And I think it's really exciting
to be able to read

00:44:55.980 --> 00:44:58.540
your logs, dump them into
BigQuery, do queries.

00:44:58.540 --> 00:45:00.220
And I think it's something
we want to enable

00:45:00.220 --> 00:45:02.080
better in the future.

00:45:02.080 --> 00:45:04.800
But obviously, you guys
could do it now.

00:45:04.800 --> 00:45:06.760
And there's always
open sourcing it.

00:45:06.760 --> 00:45:09.090
I mean, not to plug it, but
this is something I really

00:45:09.090 --> 00:45:11.760
want to use and haven't
had time to write.

00:45:11.760 --> 00:45:13.200
If you wanted to write
it, you could.

00:45:13.200 --> 00:45:14.760
And it would be really useful.

00:45:14.760 --> 00:45:16.620
But for your own application
and for other people's

00:45:16.620 --> 00:45:17.190
applications.

00:45:17.190 --> 00:45:19.050
TROY TRIMBLE: Yeah, well, it's
not well visualized in graphs

00:45:19.050 --> 00:45:21.090
or anything like that, like
on the dashboard.

00:45:21.090 --> 00:45:24.250
Like she said, it is not only in
the Logs API, the data you

00:45:24.250 --> 00:45:26.400
get back from there, but it is
in the Logs Reader in the

00:45:26.400 --> 00:45:27.140
admin console.

00:45:27.140 --> 00:45:29.660
It's right in the first line
of every request, tells you

00:45:29.660 --> 00:45:30.550
like a pending--

00:45:30.550 --> 00:45:31.460
it's like pending_ms.

00:45:31.460 --> 00:45:32.440
And it tells you.

00:45:32.440 --> 00:45:33.460
MARZIA NICCOLAI: If it sat in
the pending queue, it's

00:45:33.460 --> 00:45:37.280
pending_latency in the logs.

00:45:37.280 --> 00:45:39.750
AUDIENCE: I'm curious about the
implementation of embedded

00:45:39.750 --> 00:45:41.770
entities and how the performance
characteristics

00:45:41.770 --> 00:45:44.456
would compare to other kind of
straightforward options that

00:45:44.456 --> 00:45:46.720
you have right now, like
JSONifying an entity, or

00:45:46.720 --> 00:45:47.970
pickling it.

00:45:50.770 --> 00:45:54.740
GREG DARKE: So the embedded
entities are actually just

00:45:54.740 --> 00:45:58.020
another Datastore model that's
just been serialized and then

00:45:58.020 --> 00:46:00.060
stored in a blob field.

00:46:00.060 --> 00:46:00.630
Sorry.

00:46:00.630 --> 00:46:03.770
For local structured property,
they're stored in a blob field

00:46:03.770 --> 00:46:04.350
in the object.

00:46:04.350 --> 00:46:06.010
And you can actually compress
that if you want.

00:46:06.010 --> 00:46:11.200
For structured properly,
we take the name of the

00:46:11.200 --> 00:46:14.300
properties, and we mutate them
so that we can actually still

00:46:14.300 --> 00:46:16.650
allow you to do queries
over that.

00:46:16.650 --> 00:46:20.820
So as for the performance, the
serialization time is the same

00:46:20.820 --> 00:46:23.590
as it would take to serialize
the two entities.

00:46:23.590 --> 00:46:26.550
The right performance for local
structured property is

00:46:26.550 --> 00:46:29.250
just the amount of time required
to put that data.

00:46:29.250 --> 00:46:30.910
With local structured
properties, because there's no

00:46:30.910 --> 00:46:34.670
extra indexes, you don't have
to pay the time waiting for

00:46:34.670 --> 00:46:37.000
those indexes to be written
to propagate.

00:46:37.000 --> 00:46:38.250
But other than that, yeah.

00:46:44.310 --> 00:46:47.480
AUDIENCE: So you talked about
local entities, local

00:46:47.480 --> 00:46:51.510
structured entities, which you
said is linearized data.

00:46:51.510 --> 00:46:54.190
So what happens when
you write it back?

00:46:54.190 --> 00:46:57.200
Does it again redistribute
it and write it?

00:46:57.200 --> 00:46:58.940
Or do you write it
in one place?

00:46:58.940 --> 00:47:01.720
Let's say, for example, you have
blog post and you have

00:47:01.720 --> 00:47:02.750
profile information.

00:47:02.750 --> 00:47:05.570
Profile information being
a local entity into--

00:47:05.570 --> 00:47:06.840
MARZIA NICCOLAI: So it
only writes it in the

00:47:06.840 --> 00:47:07.780
place you write it.

00:47:07.780 --> 00:47:13.400
I didn't really cover how you
might use this, like fully to

00:47:13.400 --> 00:47:13.850
denormalize.

00:47:13.850 --> 00:47:17.260
It's just to point out how you
can store a structure.

00:47:17.260 --> 00:47:19.400
When you put it in the one
entity, it just writes that

00:47:19.400 --> 00:47:19.950
one entity.

00:47:19.950 --> 00:47:21.230
It doesn't update
it everywhere.

00:47:21.230 --> 00:47:22.900
You still have to do
that yourself.

00:47:22.900 --> 00:47:25.250
But it's really nice because it
gives you a nice, logical

00:47:25.250 --> 00:47:28.990
way of understanding and
structuring this to facilitate

00:47:28.990 --> 00:47:31.840
denormalization, which obviously
helps with queries.

00:47:31.840 --> 00:47:33.500
So I didn't cover
that part of it.

00:47:33.500 --> 00:47:37.830
AUDIENCE: The max limit of
writing 100 or 1,000 entities

00:47:37.830 --> 00:47:41.790
per transaction still applies
when you write it in--

00:47:41.790 --> 00:47:44.690
GREG DARKE: So with local
structured property, it

00:47:44.690 --> 00:47:47.370
actually only counts
as a single

00:47:47.370 --> 00:47:48.830
entity that you're writing.

00:47:48.830 --> 00:47:50.860
But you still have to
adhere to the one

00:47:50.860 --> 00:47:52.780
megabyte per entity limit.

00:47:52.780 --> 00:47:56.600
So you can actually have as
many values in that local

00:47:56.600 --> 00:47:59.020
structured property as you want,
as long as they will fit

00:47:59.020 --> 00:48:02.900
within the one megabyte size
limit of the entity that

00:48:02.900 --> 00:48:05.690
contains that local structured
property.

00:48:05.690 --> 00:48:07.600
AUDIENCE: So if I have a
thousand blog posts and if I

00:48:07.600 --> 00:48:11.950
write one profile which is
1 MB, it's still fine?

00:48:11.950 --> 00:48:15.440
So you don't have to worry
about writing it as 1,000

00:48:15.440 --> 00:48:17.116
megabytes, right?

00:48:17.116 --> 00:48:20.230
Am I making sense here?

00:48:20.230 --> 00:48:24.300
MARZIA NICCOLAI: I'm not sure
that we understand exactly.

00:48:24.300 --> 00:48:28.780
So the structured property is
stored within the one entity.

00:48:28.780 --> 00:48:30.880
AUDIENCE: Think of it like
a blog post and a profile

00:48:30.880 --> 00:48:31.810
information.

00:48:31.810 --> 00:48:35.380
A profile information lives
inside the blog post because

00:48:35.380 --> 00:48:37.280
you want to denormalize
it just in

00:48:37.280 --> 00:48:39.000
case you want to query.

00:48:39.000 --> 00:48:43.080
So if you have to write a
change to the profile

00:48:43.080 --> 00:48:43.580
information--

00:48:43.580 --> 00:48:45.110
MARZIA NICCOLAI: You'd have
to write it everywhere.

00:48:45.110 --> 00:48:46.360
AUDIENCE: Oh, OK.

00:48:49.930 --> 00:48:52.280
AUDIENCE: So I have a question
about queries.

00:48:52.280 --> 00:48:55.545
Let's suppose I have an
entity with 10 fields.

00:48:55.545 --> 00:48:59.010
I'm assuming if I query all 10
fields, it's a full-entity

00:48:59.010 --> 00:48:59.890
read, right?

00:48:59.890 --> 00:49:03.100
What determines the threshold,
if it's a small read or--

00:49:03.100 --> 00:49:05.760
MARZIA NICCOLAI: If you use
a projection query and you

00:49:05.760 --> 00:49:08.520
actually call out all 10
fields, it's still

00:49:08.520 --> 00:49:10.720
just the small op.

00:49:10.720 --> 00:49:13.630
The thing that might happen
there is the index scans might

00:49:13.630 --> 00:49:14.790
take a little bit longer.

00:49:14.790 --> 00:49:18.340
So it might not be faster,
but it will be cheaper.

00:49:18.340 --> 00:49:18.810
AUDIENCE: Awesome.

00:49:18.810 --> 00:49:19.725
Thank you.

00:49:19.725 --> 00:49:20.460
MARZIA NICCOLAI: Wait, 7?

00:49:20.460 --> 00:49:21.710
AUDIENCE: [INAUDIBLE].

00:49:25.070 --> 00:49:25.930
MARZIA NICCOLAI: No, no.

00:49:25.930 --> 00:49:26.770
It's one small op.

00:49:26.770 --> 00:49:30.250
It's one small op.

00:49:30.250 --> 00:49:33.200
We had Alfred.

00:49:33.200 --> 00:49:36.080
AUDIENCE: There was an issue
where, I forget what version,

00:49:36.080 --> 00:49:40.790
but the static files in the log
were no longer showing the

00:49:40.790 --> 00:49:45.020
refer in the access log.

00:49:45.020 --> 00:49:48.072
Is Page Speed going to fix
that, or is that going to

00:49:48.072 --> 00:49:49.389
still have that issue?

00:49:49.389 --> 00:49:50.639
AUDIENCE: [INAUDIBLE].

00:49:53.061 --> 00:49:56.090
GREG DARKE: Do you want
to answer it?

00:49:56.090 --> 00:49:59.783
MARZIA NICCOLAI: Page Speed,
I think, is orthogonal.

00:49:59.783 --> 00:50:01.172
TROY TRIMBLE: Meet Pete
White, everybody.

00:50:01.172 --> 00:50:04.410
[APPLAUSE]

00:50:04.410 --> 00:50:06.380
PETE WHITE: For that specific
issue, I know the thing there

00:50:06.380 --> 00:50:09.690
was that we do not get every
piece of information that we

00:50:09.690 --> 00:50:12.750
have from normal requests
for edge cache requests.

00:50:12.750 --> 00:50:14.440
So if you have billing enabled,
you're serving a

00:50:14.440 --> 00:50:17.240
static resource, and it's coming
out of our edge cache.

00:50:17.240 --> 00:50:21.590
We don't necessarily have the
response size, or the refer,

00:50:21.590 --> 00:50:24.125
or I think like the user
agent field, so

00:50:24.125 --> 00:50:26.270
some of that's missing.

00:50:26.270 --> 00:50:29.000
MARZIA NICCOLAI: So Page Speed
is actually our integration

00:50:29.000 --> 00:50:34.010
with the other Google
service Page Speed.

00:50:34.010 --> 00:50:38.480
So that is totally kind of
unrelated to this issue.

00:50:38.480 --> 00:50:44.660
What that does is that uses that
service to compress like

00:50:44.660 --> 00:50:48.040
CSS files and sometimes
JavaScript files and serve

00:50:48.040 --> 00:50:49.490
them from the page
speed cache.

00:50:49.490 --> 00:50:51.490
So it's really quick.

00:50:51.490 --> 00:50:56.300
So these two issues
are not related.

00:50:56.300 --> 00:50:58.120
AUDIENCE: Just a small comment
on my question before.

00:50:58.120 --> 00:51:00.410
With graphing pending latency,
I just checked.

00:51:00.410 --> 00:51:03.710
Actually, the latest ProdEagle
library allows you to graph

00:51:03.710 --> 00:51:06.230
out the pending latency.

00:51:06.230 --> 00:51:08.860
Just as FYI for everybody
who was wondering.

00:51:08.860 --> 00:51:09.210
MARZIA NICCOLAI: Cool.

00:51:09.210 --> 00:51:09.800
GREG DARKE: Great.

00:51:09.800 --> 00:51:09.970
AUDIENCE: Thank you.

00:51:09.970 --> 00:51:11.220
GREG DARKE: Thanks.

00:51:14.290 --> 00:51:16.750
AUDIENCE: If you're already
minifying and combining your

00:51:16.750 --> 00:51:22.700
JS and CSS files, does Page
Speed give you any advantage?

00:51:22.700 --> 00:51:24.480
TROY TRIMBLE: I'd refer to the
documentation on this one.

00:51:24.480 --> 00:51:26.315
I know that basically they have
a number of options that

00:51:26.315 --> 00:51:28.780
you can choose depending
on what you want to do.

00:51:28.780 --> 00:51:30.840
For example, there's
CSS inlining.

00:51:30.840 --> 00:51:32.270
And I don't know if they
do spriting or

00:51:32.270 --> 00:51:33.310
anything like that.

00:51:33.310 --> 00:51:36.480
But, I mean, basically each
individual option does some

00:51:36.480 --> 00:51:38.530
optimization that is separate
from the others.

00:51:38.530 --> 00:51:41.420
So it really depends on what
you're looking for.

00:51:41.420 --> 00:51:43.050
AUDIENCE: Would it be an
advantage putting it in the

00:51:43.050 --> 00:51:44.870
Page Speed cache?

00:51:44.870 --> 00:51:45.140
TROY TRIMBLE: Yeah.

00:51:45.140 --> 00:51:47.520
I mean, caching is
always good.

00:51:47.520 --> 00:51:50.120
So yeah, I mean, it would be an
advantage to put it in the

00:51:50.120 --> 00:51:50.930
Page Speed cache.

00:51:50.930 --> 00:51:51.440
Yeah.

00:51:51.440 --> 00:51:52.120
AUDIENCE: OK.

00:51:52.120 --> 00:51:53.030
I'll experiment with that.

00:51:53.030 --> 00:51:54.280
Thank you.

00:51:56.100 --> 00:51:57.550
PETE WHITE: I think there will
be better documentation

00:51:57.550 --> 00:52:00.250
forthcoming, but the Page Speed
service is very, very

00:52:00.250 --> 00:52:04.310
similar to the open source
page speed--

00:52:04.310 --> 00:52:06.310
Mod Page Speed Apache plug-in.

00:52:06.310 --> 00:52:08.870
So the sorts of things there
are exactly what we're

00:52:08.870 --> 00:52:10.560
exposing, just built
into your app.

00:52:13.330 --> 00:52:18.280
AUDIENCE: I'm already using the
Google Closure Tools that

00:52:18.280 --> 00:52:19.590
does a pretty good
job already.

00:52:19.590 --> 00:52:20.890
That would be nice.

00:52:25.830 --> 00:52:27.210
GREG DARKE: Thank
you very much.

00:52:27.210 --> 00:52:28.160
If anyone else wants to--

00:52:28.160 --> 00:52:29.390
MARZIA NICCOLAI: Yeah, if you
asked a question, remember to

00:52:29.390 --> 00:52:30.630
come get your chocolate bars.

00:52:30.630 --> 00:52:31.920
And if you want to
come talk to us

00:52:31.920 --> 00:52:34.010
individually, we'll be here.

00:52:34.010 --> 00:52:36.788
[APPLAUSE]

