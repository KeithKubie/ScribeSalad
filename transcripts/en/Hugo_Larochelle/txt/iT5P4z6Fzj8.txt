Speaker 1:          00:00          In this video, we'll discuss the capacity of a single neuron. That is the complexity of the computations that it can perform.

Speaker 2:          00:08          Okay.

Speaker 1:          00:09          So we've seen before that, uh, in two dimensions, if we draw the, uh, output or the activation of a given neuron, there will look something like this where we'll get, uh, some rich between two parts of the space where the Ridge is essentially defined by the orientation of the is defined by the vector w and the bias will also determine the position of this rich. So with a a neuron that performs a comp, this type of computation, we could perform binary classification that is a, if we use this sigmoidal, uh, activation function, we could interpret the activation of a neuron as estimating the probability that some input x belongs to the class one. And so let's assume in binary classification, why can be either a zero or one and then we could think of the output or the activation of a neuron as giving us, what's the estimate of the neuron that, uh, the input actually belongs to the category one and then the problem that would belong to the category.

Speaker 1:          01:22          To, uh, based on the Neuron Testament would just be one minus this because probabilities must sum to one. And so we could do this if we use a sigmoid because the sigmoid is a, a bounded between zero and one. And so we're always guaranteed that we get at the output or the activation of a neuron, a number which can be interpreted as a probability. So this in fact this exact form for a classifier is known as a logistic regression classifier. And the way it's performing classification is that if the output of the neuron, so in other words, uh, if it's estimate of the probability that x belongs to the class, one is greater than 0.5, then we will categorize the input into the class or category one and otherwise our classifier with output prediction that the input belongs to, um, the class zero. And so if we were to draw this, uh, in two d than uh, and if we looked at what is known as the dishes decision boundary, where the decision boundary is essentially the surface where, uh, the, uh, some input can equally belong to either class zero or one could equally belong to any two class, then what we would get actually is that, uh, the classifier is performing a linear classification.

Speaker 1:          02:50          That is, it's drawing a line between two regions, the regions that it's associates with one class and the other region that associates with the other class. And just drawing a boundary which is actually linear. So, uh, in today's corresponds to align and in more dimensions would correspond to a hyper plane.

Speaker 1:          03:11          So if we have a problem where we want to classify objects, uh, described by input vectors into two different classes, if we can draw a hyperplane or align into the between these two types of objects, then a single artificial neuron could do that for us. It could model that type of decision process. So here's a few example of a simple functions which can be modeled by a linear classifier. So if we have a binary inputs that are either zero or one, so we have [inaudible] that can be zero one x two it can be zero one and then we want to model the or function which takes the aura of x one and x two so four zero zero it would output a zero so that will be class zero and for a one zero one one or one or a zero one it would output once.

Speaker 1:          04:07          So the triangles are correspond to class one. Well we can see that if we draw this, we get easily pass align between all the circles and all the triangles, all the Zeros and all the ones is another function, a bit more complicated. The end function over the negation of x one and x two. Uh, then we get that all of these guys will be of course zero and this will be a class a one. And indeed, again, we can pass a line between the two classes and we have another example, but instead of negation of x two where we get a class one here and the others are zero, again, we can pass a straight line. So these simple functions can easily be modeled by a single artificial neuron.

Speaker 1:          04:54          However, there are many problems in practice, are not well separated linearly and actually the are very simple functions that are not even uh, uh, linearly separable. So there's an example of another simple function, the x or function. So this is a function that outputs zero. If, uh, either both inputs x one and x two are zero or they're both one and it all the outputs one. So for these two guys, when either one of the inputs are a one. So in this case here, we have one here we have zero four at this point. And here at this one, the x one is one and x two is zero for this point. And so in this case we see that we cannot draw just a single line that would actually separate, uh, on one side all of the Zeros and on one side, all the ones.

Speaker 1:          05:50          So it's not linearly separable. And yet it's a actually a very, very simple problem. So this suggests that a single artificial neuron will not be sufficient for many problems where we want to perform this kind of binary classification. However, we noticed that it's such a simple function that if we had instead plotted on this axis deer result of applying the n function over the negotiation of x one and x two like we've seen in the previous slide. And on this axis we actually drew the uh, we used as the value in this axis, the output of the EMF function over x one and the negation of x two. Then this point and this point we be collapsed over a single point. And then this point would curse bond. Now to that point and this point to that point. And so in this case, we could actually draw a single line between the circles and the triangles between the points associated with class zero and the points associated with class one.

Speaker 1:          06:54          So what this is saying is that if we get, if we compute an alternate representation or better representation of our input vector, the problem might become linearly separable. And this also suggests that if we have these, this other representation, which is actually representable by a single neuron, and that's the case for this and function in this and function we've seen before, that we can actually model these two functions by a single neuron. Then this suggests that by having other neurons connected by, uh, other neurons, which would, uh, so where we have a neuron that computes this and around the compute stack, if we connected these two neurons to another neuron connected directly to these artificial neurons here, then we might be able to, uh, compute a, uh, function, a certain functions that require more complicated, uh, computations and a more complicated decision surface. And this will be the main intuition behind developing more complicated a multilayer neural networks, which we'll see in the next video.