WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.850
Hey Charles, this is when we get to
talk about reinforcement learning.

00:00:02.850 --> 00:00:04.280
&gt;&gt; Hi Michael.
This is when I get to

00:00:04.280 --> 00:00:05.700
hear about reinforcement learning.

00:00:05.700 --> 00:00:07.410
&gt;&gt; Wow.
I'm glad we're on the same page.

00:00:07.410 --> 00:00:08.520
So-
&gt;&gt; Are we on the same page?

00:00:08.520 --> 00:00:09.970
Is this all of reinforcement learning or

00:00:09.970 --> 00:00:11.490
is it just the reinforcement
learning basics?

00:00:11.490 --> 00:00:13.050
&gt;&gt; We're going to start with the basics.

00:00:13.050 --> 00:00:14.370
&gt;&gt; Oh, okay.
I can't wait to hear what that is.

00:00:14.370 --> 00:00:17.170
&gt;&gt; So the first concept to try
to understand when you're doing

00:00:17.170 --> 00:00:20.730
reinforcement learning is that a lot of
it takes place as a conversation between

00:00:20.730 --> 00:00:22.650
an agent and an environment.

00:00:22.650 --> 00:00:26.430
&gt;&gt; Okay, so like right now, you're
the agent and I'm the environment.

00:00:27.430 --> 00:00:29.670
&gt;&gt; Actually I think I'm
going to have you be the agent.

00:00:29.670 --> 00:00:30.570
&gt;&gt; Okay.

00:00:30.570 --> 00:00:32.400
&gt;&gt; And we'll just imagine some kind of,
I don't know,

00:00:32.400 --> 00:00:33.520
like a video game environment.

00:00:33.520 --> 00:00:35.010
&gt;&gt; That sounds reasonable.

00:00:35.010 --> 00:00:36.345
By the way did you
notice I've lost weight?

00:00:36.345 --> 00:00:39.010
&gt;&gt; [LAUGH] Oh, good job,
how did you do that?

00:00:39.010 --> 00:00:40.375
&gt;&gt; Well, I got drawn as a stick figure.

00:00:40.375 --> 00:00:42.580
&gt;&gt; [LAUGH] That's fair.

00:00:42.580 --> 00:00:46.300
So here we are, the agent and
the environment, and

00:00:46.300 --> 00:00:50.460
the conversation basically talks
about what is going back and

00:00:50.460 --> 00:00:52.030
forth between the agent and
the environment.

00:00:52.030 --> 00:00:54.620
So, the environment is going
to reveal itself to you,

00:00:54.620 --> 00:00:57.550
to the agent, in the form of states, S.

00:00:58.940 --> 00:01:04.480
You then get to have some influence
on the environment by taking actions,

00:01:04.480 --> 00:01:09.550
a, and then you also receive back,
before the next state,

00:01:10.640 --> 00:01:15.710
some kind of reward for the most
recent state action combination.

00:01:15.710 --> 00:01:16.420
&gt;&gt; Okay.
Fair enough.

00:01:17.530 --> 00:01:20.430
&gt;&gt; So this is the same kind of
elements that we have in an MDP.

00:01:20.430 --> 00:01:23.310
But the important thing is that instead
of just being given an MDP as some kind

00:01:23.310 --> 00:01:25.415
of a graph structure and
then we get to compute on it.

00:01:25.415 --> 00:01:29.750
Really the computation's happening
inside the head of the agent and

00:01:29.750 --> 00:01:33.070
the information about the environment
is really only available through

00:01:33.070 --> 00:01:35.020
the course of this interaction.

00:01:35.020 --> 00:01:36.510
So does that make some sense?

00:01:36.510 --> 00:01:42.110
&gt;&gt; It does make some sense but I guess
how is that any different from the MDP?

00:01:42.110 --> 00:01:45.150
Well, it's the same story as how
a policy interacts with an MDP.

00:01:45.150 --> 00:01:48.820
Right, where this is playing
the role of the MDP,

00:01:48.820 --> 00:01:52.175
and this is playing
the role of the policy, pi.

00:01:52.175 --> 00:01:52.810
&gt;&gt; Mm-hm.

00:01:52.810 --> 00:01:56.260
&gt;&gt; But now, again, the computational
aspect of the system here,

00:01:56.260 --> 00:02:00.520
the agent doesn't know the environment,
it's not living inside the agent's head.

00:02:00.520 --> 00:02:04.760
Instead the agent is just experiencing
the environment by interacting with it.

00:02:04.760 --> 00:02:07.860
It can then you know if it so
chose build

00:02:07.860 --> 00:02:11.020
some kind of a model of the environment
in its head and then think about that.

00:02:11.020 --> 00:02:12.720
But what's in the agent's head and

00:02:12.720 --> 00:02:15.810
what's in the environment are two
different things in this set up.

00:02:15.810 --> 00:02:16.400
&gt;&gt; Okay fair enough.

00:02:16.400 --> 00:02:17.570
I get that.

00:02:17.570 --> 00:02:20.380
&gt;&gt; So maybe I can make
this a little bit clearer.

00:02:20.380 --> 00:02:22.760
So let's actually put
you in this environment.

00:02:22.760 --> 00:02:23.700
What do you say?

00:02:23.700 --> 00:02:25.490
&gt;&gt; Okay, metaphorically?

00:02:25.490 --> 00:02:26.670
&gt;&gt; No, let's just do it.

00:02:26.670 --> 00:02:27.170
&gt;&gt; Sure.

