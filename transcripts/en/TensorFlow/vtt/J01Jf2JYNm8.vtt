WEBVTT
Kind: captions
Language: en

00:00:00.065 --> 00:00:02.690
ANDREW SELLE: So today I'm going
to talk about TensorFlow Lite,

00:00:02.690 --> 00:00:04.732
and I'll give you an
intro to what that is.

00:00:04.732 --> 00:00:05.690
My name's Andrew Selle.

00:00:05.690 --> 00:00:09.800
I'm a software engineer down in
Google in Mountain View land.

00:00:09.800 --> 00:00:11.922
All right, so introduction.

00:00:11.922 --> 00:00:14.380
We're going to be talking about
machine learning on-device.

00:00:14.380 --> 00:00:17.150
So how many of you are
interested on running

00:00:17.150 --> 00:00:19.170
your machine learning
models on-device?

00:00:19.170 --> 00:00:20.800
How many already do that?

00:00:20.800 --> 00:00:21.500
OK, so a lot.

00:00:21.500 --> 00:00:22.190
So that's great.

00:00:22.190 --> 00:00:24.340
So you already know that
machine learning on-device

00:00:24.340 --> 00:00:24.911
is important.

00:00:24.911 --> 00:00:26.660
So I'm going to give
you a scenario that's

00:00:26.660 --> 00:00:28.760
perhaps a little bit
out of the ordinary.

00:00:28.760 --> 00:00:30.230
Suppose I'm going camping.

00:00:30.230 --> 00:00:31.550
I don't have any power.

00:00:31.550 --> 00:00:33.200
I don't have any
network connectivity.

00:00:33.200 --> 00:00:35.040
I'm out in the mountains.

00:00:35.040 --> 00:00:36.540
And I want to detect bears.

00:00:36.540 --> 00:00:39.112
So I want to hang my cell
phone on the outside of my tent

00:00:39.112 --> 00:00:40.820
so that if I'm in the
middle of the night

00:00:40.820 --> 00:00:43.010
and the bear is coming
for me, I'll know.

00:00:43.010 --> 00:00:44.900
I don't know what to
do if that happens,

00:00:44.900 --> 00:00:49.280
but that's a great example of
what you do with on-device ML.

00:00:49.280 --> 00:00:51.622
So you basically want
to have low latency.

00:00:51.622 --> 00:00:54.080
You don't want to wait for the
bear to already be upon you.

00:00:54.080 --> 00:00:55.610
You want some early warning.

00:00:55.610 --> 00:00:58.376
You want to make sure it works
without a data connection.

00:00:58.376 --> 00:01:00.500
The data can stay on the
device and you have access

00:01:00.500 --> 00:01:02.480
to the sensors.

00:01:02.480 --> 00:01:05.209
So there's a lot more kind of
practical and important use

00:01:05.209 --> 00:01:06.046
cases than that.

00:01:06.046 --> 00:01:07.670
But that kind of sets
the stage for it.

00:01:07.670 --> 00:01:11.630
And so people are, of course,
doing a lot more ML on-device.

00:01:11.630 --> 00:01:13.700
So that's why we
started TensorFlow Lite.

00:01:13.700 --> 00:01:16.580
So in short, TensorFlow
Lite is our solution

00:01:16.580 --> 00:01:18.230
for running on-device
machine learning

00:01:18.230 --> 00:01:21.440
with low latency and
a small binary size

00:01:21.440 --> 00:01:24.020
but on many platforms.

00:01:24.020 --> 00:01:27.450
So here's an example of
what TensorFlow Lite can do.

00:01:27.450 --> 00:01:30.030
So let's play our video.

00:01:30.030 --> 00:01:31.405
So, suppose we
have these objects

00:01:31.405 --> 00:01:32.654
and we want to recognize them.

00:01:32.654 --> 00:01:34.710
Well, I know what they
are, but I want my phone

00:01:34.710 --> 00:01:36.040
to be able to do that.

00:01:36.040 --> 00:01:37.749
So we use an image
classification model.

00:01:37.749 --> 00:01:38.790
So here we have a marker.

00:01:38.790 --> 00:01:40.930
In the bottom you see
kind of the confidence.

00:01:40.930 --> 00:01:42.600
So this is an image recognition.

00:01:42.600 --> 00:01:44.850
It's happening in real time.

00:01:44.850 --> 00:01:47.190
It can do kind of
ordinary office objects,

00:01:47.190 --> 00:01:51.030
but also important objects
like TensorFlow logos.

00:01:51.030 --> 00:01:52.860
And it also runs on
multiple platforms.

00:01:52.860 --> 00:01:56.640
So here we have it running on an
Android phone and an IOS phone.

00:01:56.640 --> 00:01:58.230
But we're not just
limited to phones.

00:01:58.230 --> 00:02:01.715
We can also do things like
this Android Things Toolkit.

00:02:01.715 --> 00:02:03.840
And there is going to be
a couple more later, which

00:02:03.840 --> 00:02:04.840
I'll show you some more.

00:02:04.840 --> 00:02:06.640
So now that we've
seen it in action,

00:02:06.640 --> 00:02:11.080
let's talk a little bit more
about what TensorFlow Lite is.

00:02:11.080 --> 00:02:12.550
Do you want to see
the video again?

00:02:12.550 --> 00:02:13.160
I don't know.

00:02:16.700 --> 00:02:18.090
OK, there we go.

00:02:18.090 --> 00:02:20.400
The unfortunate
thing about on-device

00:02:20.400 --> 00:02:21.930
though is it's much harder.

00:02:21.930 --> 00:02:25.934
It has tight memory constraints,
you need to be low energy,

00:02:25.934 --> 00:02:28.350
and you don't have as much
computation as you do available

00:02:28.350 --> 00:02:29.580
on the cloud.

00:02:29.580 --> 00:02:31.785
So TensorFlow Lite
has a few properties

00:02:31.785 --> 00:02:33.910
that are going to sort of
deal with these problems.

00:02:33.910 --> 00:02:36.190
So the first one is it
needs to be portable.

00:02:36.190 --> 00:02:39.660
You know, normal PCs
we run on, but we also

00:02:39.660 --> 00:02:43.170
run on mobile phones, we run
on Raspberry Pi, or other Linux

00:02:43.170 --> 00:02:45.920
SOCs, IoT type devices.

00:02:45.920 --> 00:02:47.970
And we also want to go
to much smaller devices

00:02:47.970 --> 00:02:50.340
like microcontrollers.

00:02:50.340 --> 00:02:56.062
So, next slide.

00:02:56.062 --> 00:02:57.520
The internet doesn't
like me today.

00:03:02.362 --> 00:03:04.320
Well, we'll skip that
slide, whatever that was.

00:03:04.320 --> 00:03:08.190
But in any case,
basically, this portability

00:03:08.190 --> 00:03:10.752
is achieved through the
TensorFlow Lite file format.

00:03:10.752 --> 00:03:12.960
So once you have a trained
TensorFlow model, like you

00:03:12.960 --> 00:03:14.585
author, you could
author it with Swift,

00:03:14.585 --> 00:03:16.890
you could author it with
Python, whatever you do,

00:03:16.890 --> 00:03:18.840
you produce a saved
model in a graph form.

00:03:18.840 --> 00:03:21.750
The serialized form is then
converted to TensorFlow Lite

00:03:21.750 --> 00:03:24.300
format, which then is your
gateway to running on all

00:03:24.300 --> 00:03:25.830
these different platforms.

00:03:25.830 --> 00:03:28.410
And we have one more
special converter

00:03:28.410 --> 00:03:29.910
which allows you
to go to Core ML

00:03:29.910 --> 00:03:33.030
if you want to target IOS
in this especially way.

00:03:33.030 --> 00:03:35.050
The second property is
that it's optimizable.

00:03:35.050 --> 00:03:37.050
We have model compression,
we have quantization,

00:03:37.050 --> 00:03:38.539
we have CPU kernel fusion.

00:03:38.539 --> 00:03:40.080
These are all
optimization techniques

00:03:40.080 --> 00:03:43.530
to ensure that we have the
best performance, as well

00:03:43.530 --> 00:03:45.434
as a small size.

00:03:45.434 --> 00:03:47.350
And this is achieved
through the architecture.

00:03:47.350 --> 00:03:49.729
So we have a converter, which
we already talked about,

00:03:49.729 --> 00:03:51.270
and then we have an
interpreter core.

00:03:51.270 --> 00:03:53.220
The interpreter core
delegates to kernels

00:03:53.220 --> 00:03:56.220
that know how to do things,
just like in TensorFlow.

00:03:56.220 --> 00:03:57.870
But unlike in
TensorFlow these are

00:03:57.870 --> 00:04:00.671
optimized for mobile and small
devices with NEON on ARM.

00:04:00.671 --> 00:04:02.670
An additional thing that
we have that TensorFlow

00:04:02.670 --> 00:04:04.800
doesn't have is the
notion of delegation.

00:04:04.800 --> 00:04:08.880
So we can delegate to GPUs
or Edge TPUs or accelerators.

00:04:08.880 --> 00:04:12.060
And this basically
gives TensorFlow Lite

00:04:12.060 --> 00:04:13.650
the chance to give
part of the graph

00:04:13.650 --> 00:04:18.209
to a hardware accelerator
that can do processing

00:04:18.209 --> 00:04:20.850
in a special way.

00:04:20.850 --> 00:04:24.390
So one of those we've talked
about before is in an API.

00:04:24.390 --> 00:04:27.990
We're excited to
announce that in 2018 Q4

00:04:27.990 --> 00:04:31.260
we're looking to release
our OpenGL-based GPU

00:04:31.260 --> 00:04:35.070
delegate, which will give us
better performance on GPUs,

00:04:35.070 --> 00:04:37.910
and will also accelerate
things like MobileNet

00:04:37.910 --> 00:04:39.450
and another vision-based models.

00:04:39.450 --> 00:04:41.040
So that's really exciting.

00:04:41.040 --> 00:04:43.710
In addition, at Cloud Next
there was an announcement

00:04:43.710 --> 00:04:45.120
about Edge TPUs.

00:04:45.120 --> 00:04:48.120
And Edge TPUs are
also special as well

00:04:48.120 --> 00:04:50.040
because they give
us the ability to do

00:04:50.040 --> 00:04:51.960
high performance
per watt, and also

00:04:51.960 --> 00:04:53.220
fit into a small footprint.

00:04:53.220 --> 00:04:58.260
So for example, the device
is there on that penny there,

00:04:58.260 --> 00:04:59.730
but it's on a development board.

00:04:59.730 --> 00:05:02.104
So you can put it in many
different form factors as well.

00:05:04.680 --> 00:05:07.111
Then the third property is
that it's parametrically sized.

00:05:07.111 --> 00:05:08.610
So we know the
TensorFlow Lite needs

00:05:08.610 --> 00:05:10.890
to fit on small devices,
especially very small devices

00:05:10.890 --> 00:05:14.190
like MCUs, And there, you
might need to only include

00:05:14.190 --> 00:05:15.330
the ops that you need.

00:05:15.330 --> 00:05:18.180
So our base interpreter
is about 80 kilobytes,

00:05:18.180 --> 00:05:20.640
and with all built-in
ops it's 750 kilobytes.

00:05:20.640 --> 00:05:22.770
We're moving to a world
where you can parameterize

00:05:22.770 --> 00:05:24.853
what you put into the
TensorFlow Lite interpreter,

00:05:24.853 --> 00:05:26.880
so you can trade off
the ability to handle

00:05:26.880 --> 00:05:30.990
new models that use new ops, and
the ability to only ship what

00:05:30.990 --> 00:05:35.250
you need in your application.

00:05:35.250 --> 00:05:38.190
So, we introduced
TensorFlow Lite last year

00:05:38.190 --> 00:05:42.030
and we've been asking users what
they think of TensorFlow Lite.

00:05:42.030 --> 00:05:44.130
And they really love the
cross platform deployment

00:05:44.130 --> 00:05:46.560
that they can deploy
to IOS, to Android

00:05:46.560 --> 00:05:48.120
with the same kind of format.

00:05:48.120 --> 00:05:52.410
They like that they can decouple
the distribution of the binary

00:05:52.410 --> 00:05:55.200
from the distribution
of the model.

00:05:55.200 --> 00:05:57.074
And they like the
inference speed increases.

00:05:57.074 --> 00:05:59.490
And they're really excited
about the hardware acceleration

00:05:59.490 --> 00:06:00.810
roadmap.

00:06:00.810 --> 00:06:04.500
But the biggest
feedback that we got

00:06:04.500 --> 00:06:08.070
is that we should focus on ease
of use, we should add more ops,

00:06:08.070 --> 00:06:10.292
and we should work on
model optimization,

00:06:10.292 --> 00:06:12.000
and we should provide
more documentation.

00:06:12.000 --> 00:06:13.650
And so we've listened.

00:06:13.650 --> 00:06:15.300
So what I want to
do in this talk

00:06:15.300 --> 00:06:17.646
is focus on the user experience.

00:06:17.646 --> 00:06:19.020
But before we do
that, let's look

00:06:19.020 --> 00:06:21.324
at some of the users that
are already using it so far.

00:06:21.324 --> 00:06:22.740
We have a lot of
first party users

00:06:22.740 --> 00:06:25.114
and a lot of third party users
that are excited about it.

00:06:25.114 --> 00:06:29.520
And I hope after this talk
you'll be interested as well.

00:06:29.520 --> 00:06:31.890
So, the user's experience.

00:06:31.890 --> 00:06:33.950
I'm a new user and I want
to use TensorFlow Lite.

00:06:33.950 --> 00:06:34.662
How do I do it?

00:06:34.662 --> 00:06:36.870
Well, I think of it as kind
of like learning to swim.

00:06:36.870 --> 00:06:38.578
You can think of two
things you might do.

00:06:38.578 --> 00:06:40.970
You might wade, where you
don't really have to swim.

00:06:40.970 --> 00:06:42.800
But it's really
easy to get started

00:06:42.800 --> 00:06:44.940
and you get to cool off.

00:06:44.940 --> 00:06:46.940
The second thing is
that you can swim

00:06:46.940 --> 00:06:48.420
where you can do custom models.

00:06:48.420 --> 00:06:49.530
So we're going to talk
about both of those.

00:06:49.530 --> 00:06:51.244
But before we get
into that, there's

00:06:51.244 --> 00:06:52.910
an easier thing and
then a harder thing.

00:06:52.910 --> 00:06:55.550
So the easier thing is to dip
your toes, which are demos.

00:06:55.550 --> 00:06:58.070
And the harder thing
is to just dive full in

00:06:58.070 --> 00:07:00.290
and have full kind of
mastery of the whole water,

00:07:00.290 --> 00:07:01.790
and that would be
optimizing models.

00:07:01.790 --> 00:07:04.040
And we'll talk
about those as well.

00:07:04.040 --> 00:07:07.712
So as far as using demo apps,
you can go to our website,

00:07:07.712 --> 00:07:09.920
you can download the demos
and compile it and run it.

00:07:09.920 --> 00:07:11.390
It'll give you a flavor
of what can be done.

00:07:11.390 --> 00:07:13.010
I showed you one
of those demo apps.

00:07:13.010 --> 00:07:14.987
You can try it for yourself.

00:07:14.987 --> 00:07:16.820
The next step is to use
a pre trained model.

00:07:16.820 --> 00:07:18.990
So the demo app uses
a pre trained model.

00:07:18.990 --> 00:07:21.270
So you can use that model
in your application.

00:07:21.270 --> 00:07:24.187
So if you have something that
could benefit from say ImageNet

00:07:24.187 --> 00:07:26.270
style classification, you
can just take that model

00:07:26.270 --> 00:07:27.119
and include it.

00:07:27.119 --> 00:07:29.160
Another thing that's really
useful is retraining.

00:07:29.160 --> 00:07:31.460
So let me show you a
retraining workflow, which

00:07:31.460 --> 00:07:35.220
is you take a pre trained model
and you kind of customize it.

00:07:35.220 --> 00:07:37.334
Great, so we're
running this video.

00:07:37.334 --> 00:07:39.500
We're showing you the
scissors and the Post-It Notes

00:07:39.500 --> 00:07:42.830
as before, and
here's an application

00:07:42.830 --> 00:07:46.069
that I built on PC that
allows me to do retraining.

00:07:46.069 --> 00:07:48.110
But we're running inference
with TensorFlow Lite.

00:07:48.110 --> 00:07:51.110
So it knows the scissors,
it knows the Post-It Notes,

00:07:51.110 --> 00:07:53.300
but what if we got to a
really important object, one

00:07:53.300 --> 00:07:55.610
that we haven't
seen before, perhaps

00:07:55.610 --> 00:08:02.090
like something everybody has,
the a steel TensorFlow logo.

00:08:02.090 --> 00:08:04.180
How is it going to do on that?

00:08:04.180 --> 00:08:06.132
Well, not well is the
unfortunate thing.

00:08:06.132 --> 00:08:08.590
But the good thing about machine
learning is we can fix it.

00:08:08.590 --> 00:08:10.250
We just need some examples.

00:08:10.250 --> 00:08:12.460
So here, this app allows
us to collect examples.

00:08:12.460 --> 00:08:14.626
And you could imagine putting
this into a mobile app

00:08:14.626 --> 00:08:16.480
where you just move
your phone around

00:08:16.480 --> 00:08:18.100
and it captures a
bunch of examples.

00:08:18.100 --> 00:08:22.190
I'm going to do the same
thing, except on Linux.

00:08:22.190 --> 00:08:25.732
It's a little bit more
boring, but it does a job.

00:08:25.732 --> 00:08:27.190
So we want to get
a lot of examples

00:08:27.190 --> 00:08:28.510
to get a lot of generalization.

00:08:28.510 --> 00:08:30.884
So once we're happy with that,
we'll hit the Train button

00:08:30.884 --> 00:08:33.789
and that's going to do
some machine learning.

00:08:33.789 --> 00:08:36.669
And once it's converged, we're
going to have a new model.

00:08:36.669 --> 00:08:38.830
It's going to convert it
to TensorFlow Lite, which

00:08:38.830 --> 00:08:39.580
is going to be really great.

00:08:39.580 --> 00:08:40.371
We can test it out.

00:08:40.371 --> 00:08:43.059
See if it's now detecting this.

00:08:43.059 --> 00:08:45.190
And indeed it is.

00:08:45.190 --> 00:08:46.427
That's good.

00:08:46.427 --> 00:08:48.010
The other cool thing
about that is now

00:08:48.010 --> 00:08:50.180
that we have this TF
Lite flat buffer model,

00:08:50.180 --> 00:08:55.080
we can now run it on our
device and it works as well.

00:08:55.080 --> 00:08:56.640
All right, great.

00:08:56.640 --> 00:08:59.750
So, now that we've done pre
train models and we've done--

00:08:59.750 --> 00:09:01.250
let's get into full on swimming.

00:09:01.250 --> 00:09:03.560
There's basically four steps
that we need to work on.

00:09:03.560 --> 00:09:05.157
The first one is
building and training

00:09:05.157 --> 00:09:06.990
the model, which we've
already talked about.

00:09:06.990 --> 00:09:09.364
You could do that with, again,
Swift would be a great way

00:09:09.364 --> 00:09:10.260
to do that.

00:09:10.260 --> 00:09:11.960
The second one is
converting the model.

00:09:11.960 --> 00:09:13.110
Third one is
validating the model.

00:09:13.110 --> 00:09:14.526
And the fourth is
deploying model.

00:09:14.526 --> 00:09:15.734
Let's dive into them.

00:09:15.734 --> 00:09:17.150
Well, we're not
going to dive yet.

00:09:17.150 --> 00:09:18.024
We'll swim into them.

00:09:18.024 --> 00:09:18.800
OK.

00:09:18.800 --> 00:09:19.980
Build and train the model.

00:09:19.980 --> 00:09:24.350
So the first thing to do is to
get a saved model of your model

00:09:24.350 --> 00:09:25.650
and then use the converter.

00:09:25.650 --> 00:09:26.900
This can be invoked in Python.

00:09:26.900 --> 00:09:28.280
So you could have
your training script

00:09:28.280 --> 00:09:29.210
and you could have
the last thing

00:09:29.210 --> 00:09:31.910
you do is to just always
convert it to TensorFlow Lite.

00:09:31.910 --> 00:09:33.500
I in fact recommend
that, because that

00:09:33.500 --> 00:09:35.041
will allow you to
make sure that it's

00:09:35.041 --> 00:09:36.750
convertible right
from the start.

00:09:36.750 --> 00:09:39.320
So you give it the
saved model in,

00:09:39.320 --> 00:09:41.680
and you provide the
TF Lite buffer out.

00:09:41.680 --> 00:09:42.230
Great.

00:09:42.230 --> 00:09:44.810
And then when you're done
with that, it will convert.

00:09:44.810 --> 00:09:47.360
Except we don't
have all the ops.

00:09:47.360 --> 00:09:48.290
So sometimes it won't.

00:09:48.290 --> 00:09:52.230
So sometimes you want to
visualize the TensorFlow model.

00:09:52.230 --> 00:09:54.480
So a lot of models do
work, but some of them

00:09:54.480 --> 00:09:55.730
are going to have missing ops.

00:09:55.730 --> 00:09:58.370
So as we've said, we've
listened to your feedback

00:09:58.370 --> 00:10:00.700
and to address this we've
provided these visualizers

00:10:00.700 --> 00:10:02.510
so you can understand
your models better.

00:10:02.510 --> 00:10:04.910
They're kind of
analogous to TensorBoard.

00:10:04.910 --> 00:10:08.360
In addition, we've also
added 75 built-in ops,

00:10:08.360 --> 00:10:11.280
and we're announcing
a new feature,

00:10:11.280 --> 00:10:14.690
which will allow us
to run TensorFlow

00:10:14.690 --> 00:10:16.946
kernels in TensorFlow Lite.

00:10:16.946 --> 00:10:18.320
So basically, this
will allow you

00:10:18.320 --> 00:10:20.360
to run normal TensorFlow
kernels that we

00:10:20.360 --> 00:10:21.721
don't have a built-in op for.

00:10:21.721 --> 00:10:23.720
There is a trade to that,
because that increases

00:10:23.720 --> 00:10:25.490
your binary size considerably.

00:10:25.490 --> 00:10:27.950
However, it's a great
way to get started,

00:10:27.950 --> 00:10:31.760
and it will kind of allow you to
get into using TensorFlow Lite

00:10:31.760 --> 00:10:34.100
and deploy your model
if binary size is not

00:10:34.100 --> 00:10:36.350
your primary constraint.

00:10:36.350 --> 00:10:38.210
OK, great.

00:10:38.210 --> 00:10:40.750
Once you have your model
working and converted,

00:10:40.750 --> 00:10:42.470
you will definitely
want to validate it.

00:10:42.470 --> 00:10:44.060
Every step of
machine learning it's

00:10:44.060 --> 00:10:45.851
extremely important to
make sure it's still

00:10:45.851 --> 00:10:46.940
running the way you think.

00:10:46.940 --> 00:10:49.730
So if you have it working
in your Python test bench

00:10:49.730 --> 00:10:52.160
and it's running, you need
to make sure it's also

00:10:52.160 --> 00:10:53.130
running in your app.

00:10:53.130 --> 00:10:55.790
This is just good practice,
that end-to-end things

00:10:55.790 --> 00:10:57.530
are producing the right answer.

00:10:57.530 --> 00:10:59.322
In addition, you might
want to do profiling

00:10:59.322 --> 00:11:01.321
and you might want to
look at what your size is.

00:11:01.321 --> 00:11:03.650
Once you've done that, you
want to convey this model

00:11:03.650 --> 00:11:07.050
to the next phase,
which is optimization.

00:11:07.050 --> 00:11:08.850
We're not going to
talk about that later,

00:11:08.850 --> 00:11:11.120
but that's what you would
do with those results.

00:11:11.120 --> 00:11:14.060
OK, so how do you deploy your
Model we have several APIs.

00:11:14.060 --> 00:11:16.160
In the previous time
I talked about this,

00:11:16.160 --> 00:11:20.930
we had c++ and Java.

00:11:20.930 --> 00:11:24.000
Kind of in May or so we
introduced a Python API.

00:11:24.000 --> 00:11:26.530
And I'm excited to talk
about our C API, which

00:11:26.530 --> 00:11:28.280
is a way in which we're
going to implement

00:11:28.280 --> 00:11:31.830
all of our different APIs,
similar to how TensorFlow does

00:11:31.830 --> 00:11:32.330
it.

00:11:32.330 --> 00:11:35.645
In addition, we're also
introducing an experimental C

00:11:35.645 --> 00:11:39.530
Sharp API, which allows you
to use it in a lot of toolkits

00:11:39.530 --> 00:11:40.670
that are C Sharp based.

00:11:40.670 --> 00:11:41.900
The most notable of which--

00:11:41.900 --> 00:11:43.310
which is a feature request--

00:11:43.310 --> 00:11:43.910
was Unity.

00:11:43.910 --> 00:11:46.820
So if you want to integrate
it into say a game.

00:11:46.820 --> 00:11:50.810
OK, and then third, Objective
C to get a more idiomatic

00:11:50.810 --> 00:11:53.131
traditional IOS experience.

00:11:53.131 --> 00:11:53.630
Great.

00:11:56.320 --> 00:11:58.730
Let me just give you an
example of some code here.

00:11:58.730 --> 00:12:00.760
So the basic idea is
you give it the file

00:12:00.760 --> 00:12:03.340
name of the flat buffer
model, you fill in the inputs,

00:12:03.340 --> 00:12:04.660
and you call invoke.

00:12:04.660 --> 00:12:06.070
Then you read out the outputs.

00:12:06.070 --> 00:12:08.290
That's how all the APIs
has work, no matter

00:12:08.290 --> 00:12:09.340
what language they are.

00:12:09.340 --> 00:12:10.659
So this was Python.

00:12:10.659 --> 00:12:11.700
The same is true in Java.

00:12:11.700 --> 00:12:13.322
The same is true in c++.

00:12:13.322 --> 00:12:15.280
Perhaps the C one is a
little bit more verbose,

00:12:15.280 --> 00:12:18.600
but it should be
pretty intuitive.

00:12:18.600 --> 00:12:21.100
OK, now that we
know how to swim,

00:12:21.100 --> 00:12:23.090
let's go into
diving into models.

00:12:23.090 --> 00:12:24.620
How do we optimize a model?

00:12:24.620 --> 00:12:26.200
So once you have
your model working,

00:12:26.200 --> 00:12:28.400
you might want to get the
best performance possible,

00:12:28.400 --> 00:12:30.358
and you might want to
leverage custom hardware.

00:12:30.358 --> 00:12:33.015
This traditionally implies
modifying the model.

00:12:33.015 --> 00:12:35.140
So the way in which we're
going to do this is we're

00:12:35.140 --> 00:12:36.160
going to put this into
our [INAUDIBLE] loop.

00:12:36.160 --> 00:12:38.740
We had our four steps
before as part of swimming,

00:12:38.740 --> 00:12:40.660
but now we have the
additional diving model

00:12:40.660 --> 00:12:42.900
where we do optimization.

00:12:42.900 --> 00:12:44.230
And how does optimization work?

00:12:44.230 --> 00:12:46.720
We're introducing this thing
called the model optimization

00:12:46.720 --> 00:12:49.300
toolkit, and the
cool thing about it

00:12:49.300 --> 00:12:51.490
is it allows you to optimize
your model either post

00:12:51.490 --> 00:12:53.240
training or during training.

00:12:53.240 --> 00:12:55.370
So that means that you
can do a lot of things

00:12:55.370 --> 00:12:59.290
without retraining the
model, but to get the--

00:12:59.290 --> 00:13:01.750
let me just give an
example, which is right now

00:13:01.750 --> 00:13:03.400
we're doing quantization.

00:13:03.400 --> 00:13:05.210
So there's two ways
to do quantization.

00:13:05.210 --> 00:13:08.830
One is, you take your model
and look at what ranges it uses

00:13:08.830 --> 00:13:12.070
and then just say we're going
to quantize this model right now

00:13:12.070 --> 00:13:13.310
by just setting those ranges.

00:13:13.310 --> 00:13:16.580
So that's called post
training quantization.

00:13:16.580 --> 00:13:19.342
So all you need to do that is
add a flag to the conversion.

00:13:19.342 --> 00:13:21.050
I showed you the Python
converter before.

00:13:21.050 --> 00:13:22.600
There's also a command line one.

00:13:22.600 --> 00:13:25.930
But both of them have this
option to quantize the weights.

00:13:25.930 --> 00:13:29.050
In addition, if you want to do
a training time quantization,

00:13:29.050 --> 00:13:30.820
we introduced the tool
kit for doing this.

00:13:30.820 --> 00:13:34.720
This is now kind of put under
the model optimization toolkit,

00:13:34.720 --> 00:13:37.157
and this will basically create
you a new training graph

00:13:37.157 --> 00:13:38.740
that when you run
it, it will give you

00:13:38.740 --> 00:13:43.060
the most optimal training
quantized graph that you could.

00:13:43.060 --> 00:13:46.240
It kind of takes advantage--
it takes into account that it

00:13:46.240 --> 00:13:47.440
is going to be quantized.

00:13:47.440 --> 00:13:50.690
So basically, the loss function
is aware of the quantization.

00:13:50.690 --> 00:13:52.210
So that's good.

00:13:52.210 --> 00:13:54.520
OK.

00:13:54.520 --> 00:13:57.670
So, just one more thing
that I want to talk about,

00:13:57.670 --> 00:14:00.060
which is roadmap.

00:14:00.060 --> 00:14:03.000
We're looking actively into
things like on-device training,

00:14:03.000 --> 00:14:06.540
which will, of course, require
us to investigate control flow.

00:14:06.540 --> 00:14:09.960
We're adding more techniques
to the optimization toolkit.

00:14:09.960 --> 00:14:14.610
We'd also like to provide more
hardware acceleration support.

00:14:14.610 --> 00:14:16.950
And the last thing is
for TensorFlow 2.0,

00:14:16.950 --> 00:14:19.740
we're moving out of contrib
and into TensorFlow.

00:14:19.740 --> 00:14:23.790
So we'll be under
TensorFlow slash Lite.

00:14:23.790 --> 00:14:26.820
OK, so a couple demos.

00:14:26.820 --> 00:14:30.300
I wanted to show a couple
of models that were using--

00:14:30.300 --> 00:14:31.830
so TensorFlow Lite.

00:14:31.830 --> 00:14:34.606
So for example,
here's one model that

00:14:34.606 --> 00:14:35.730
allows you to see the gaze.

00:14:35.730 --> 00:14:37.860
So it's running in real time.

00:14:37.860 --> 00:14:41.130
It basically puts
boxes around people

00:14:41.130 --> 00:14:43.410
and kind of gives a
vector of which direction

00:14:43.410 --> 00:14:44.160
they're looking.

00:14:44.160 --> 00:14:46.410
And this is running in real
time on top of TensorFlow

00:14:46.410 --> 00:14:47.870
Lite on an Edge TPU.

00:14:54.180 --> 00:14:55.610
Let me show you another one.

00:14:55.610 --> 00:14:58.090
Oh, sorry.

00:14:58.090 --> 00:15:00.210
OK.

00:15:00.210 --> 00:15:01.490
It's very tricky.

00:15:01.490 --> 00:15:02.924
There we go.

00:15:02.924 --> 00:15:04.840
Here's another one that's
kind of interesting.

00:15:04.840 --> 00:15:07.980
Again, this is using
a variant of SSD.

00:15:07.980 --> 00:15:09.870
It's basically three
autonomous agents,

00:15:09.870 --> 00:15:12.510
or two autonomous agents
and one human driven agent.

00:15:12.510 --> 00:15:15.240
Two of the agents are trying
to catch the other one

00:15:15.240 --> 00:15:16.740
and the other one's
trying to avoid.

00:15:16.740 --> 00:15:18.660
And they're all
input in this SSD.

00:15:18.660 --> 00:15:21.630
Basically, the upshot of this
is that it uses SSD that's

00:15:21.630 --> 00:15:23.130
accelerated with Edge TPUs.

00:15:23.130 --> 00:15:28.310
It's about 40% faster using
to Edge TPUs and TF Lite.

00:15:28.310 --> 00:15:33.410
And I have one more demo,
which is an app that's

00:15:33.410 --> 00:15:36.140
using TF Lite called Yummly.

00:15:36.140 --> 00:15:39.680
And basically, this
is able to give you

00:15:39.680 --> 00:15:40.960
recipes based on what it sees.

00:15:40.960 --> 00:15:42.270
So let's just see it in action.

00:15:42.270 --> 00:15:44.020
So this was originally
built on TF Mobile,

00:15:44.020 --> 00:15:46.310
but then moved to TF Lite.

00:15:46.310 --> 00:15:50.180
So, this is their demo.

00:15:50.180 --> 00:15:53.051
So essentially, you point your
phone at what's in your fridge

00:15:53.051 --> 00:15:54.800
and it will tell you
what to make with it.

00:15:57.548 --> 00:15:59.090
This is good for
me, because I don't

00:15:59.090 --> 00:16:01.910
have any creativity
on cooking and I have

00:16:01.910 --> 00:16:03.432
a lot of random ingredients.

00:16:06.090 --> 00:16:09.690
So we're really excited by what
people are using TF Lite for.

00:16:09.690 --> 00:16:11.140
I want to show
you one more demo,

00:16:11.140 --> 00:16:15.270
which I just made last week
with some of my colleagues.

00:16:15.270 --> 00:16:17.920
And it's basically running
on a microcontroller.

00:16:17.920 --> 00:16:20.460
So this is basically
a microcontroller

00:16:20.460 --> 00:16:22.380
with a touch screen
that has only one

00:16:22.380 --> 00:16:25.470
megabyte of flash memory
and 340 kilobytes of RAM.

00:16:25.470 --> 00:16:29.730
So this is sort of
pretty small, and we're

00:16:29.730 --> 00:16:31.080
doing speech recognition on it.

00:16:31.080 --> 00:16:33.370
So I say, yes.

00:16:33.370 --> 00:16:34.610
And it says, yes.

00:16:34.610 --> 00:16:36.810
It says no.

00:16:36.810 --> 00:16:37.600
It says no.

00:16:37.600 --> 00:16:40.290
And if I say some random
thing, it says unknown

00:16:40.290 --> 00:16:41.070
So pre-recorded.

00:16:41.070 --> 00:16:43.480
Unfortunately, I don't
have the sound on yet.

00:16:43.480 --> 00:16:46.420
But this is just showing that
we can run the same interpreter

00:16:46.420 --> 00:16:48.710
code on these really
small devices.

00:16:48.710 --> 00:16:51.850
So we can go all the way to IoT,
which I think is super exciting

00:16:51.850 --> 00:16:54.970
and will introduce a whole
new set of applications

00:16:54.970 --> 00:16:56.300
that are possible.

00:16:56.300 --> 00:17:04.050
So with that, I already
told you this, which

00:17:04.050 --> 00:17:06.640
we're moving out of contrib.

00:17:06.640 --> 00:17:09.030
I'd like you guys to
try out TensorFlow.

00:17:09.030 --> 00:17:10.147
Send us some information.

00:17:10.147 --> 00:17:11.730
If you're interested
in discussing it,

00:17:11.730 --> 00:17:15.270
go on to our mailing list,
tflite@tensorflow.org.

00:17:15.270 --> 00:17:18.270
And we're really excited
to hear about new use cases

00:17:18.270 --> 00:17:20.530
and to hear feedback.

00:17:20.530 --> 00:17:21.900
So thank you.

00:17:21.900 --> 00:17:26.520
We're going to-- both of us
are going to be at the booth

00:17:26.520 --> 00:17:30.030
over in the grand ballroom.

00:17:30.030 --> 00:17:33.230
So if you want to talk to
us more about either Swift

00:17:33.230 --> 00:17:36.360
or TensorFlow Lite, that would
be a great time to do it.

00:17:36.360 --> 00:17:37.980
And thank you.

00:17:37.980 --> 00:17:39.830
[APPLAUSE]

