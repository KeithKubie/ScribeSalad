WEBVTT
Kind: captions
Language: en

00:00:13.381 --> 00:00:17.396
I work on helping computers
communicate about the world around us.

00:00:17.754 --> 00:00:19.547
There are a lot of ways to do this,

00:00:19.571 --> 00:00:22.163
and I like to focus on helping computers

00:00:22.187 --> 00:00:25.061
to talk about what they see
and understand.

00:00:25.514 --> 00:00:27.085
Given a scene like this,

00:00:27.109 --> 00:00:29.014
a modern computer-vision algorithm

00:00:29.038 --> 00:00:32.133
can tell you that there's a woman
and there's a dog.

00:00:32.157 --> 00:00:34.863
It can tell you that the woman is smiling.

00:00:34.887 --> 00:00:38.760
It might even be able to tell you
that the dog is incredibly cute.

00:00:38.784 --> 00:00:40.133
I work on this problem

00:00:40.157 --> 00:00:44.369
thinking about how humans
understand and process the world.

00:00:45.577 --> 00:00:48.529
The thoughts, memories and stories

00:00:48.553 --> 00:00:51.371
that a scene like this
might evoke for humans.

00:00:51.395 --> 00:00:55.680
All the interconnections
of related situations.

00:00:55.704 --> 00:00:58.830
Maybe you've seen
a dog like this one before,

00:00:58.854 --> 00:01:01.823
or you've spent time
running on a beach like this one,

00:01:01.847 --> 00:01:06.625
and that further evokes thoughts
and memories of a past vacation,

00:01:06.649 --> 00:01:08.569
past times to the beach,

00:01:08.593 --> 00:01:11.196
times spent running around
with other dogs.

00:01:11.688 --> 00:01:16.895
One of my guiding principles
is that by helping computers to understand

00:01:16.919 --> 00:01:19.815
what it's like to have these experiences,

00:01:19.839 --> 00:01:25.015
to understand what we share
and believe and feel,

00:01:26.094 --> 00:01:30.404
then we're in a great position
to start evolving computer technology

00:01:30.428 --> 00:01:35.015
in a way that's complementary
with our own experiences.

00:01:35.539 --> 00:01:38.926
So, digging more deeply into this,

00:01:38.950 --> 00:01:44.855
a few years ago I began working on helping
computers to generate human-like stories

00:01:44.879 --> 00:01:46.545
from sequences of images.

00:01:47.427 --> 00:01:49.331
So, one day,

00:01:49.355 --> 00:01:53.977
I was working with my computer to ask it
what it thought about a trip to Australia.

00:01:54.768 --> 00:01:57.688
It took a look at the pictures,
and it saw a koala.

00:01:58.236 --> 00:01:59.879
It didn't know what the koala was,

00:01:59.903 --> 00:02:02.902
but it said it thought
it was an interesting-looking creature.

00:02:04.053 --> 00:02:08.057
Then I shared with it a sequence of images
about a house burning down.

00:02:09.704 --> 00:02:12.989
It took a look at the images and it said,

00:02:13.013 --> 00:02:16.513
"This is an amazing view!
This is spectacular!"

00:02:17.450 --> 00:02:19.545
It sent chills down my spine.

00:02:20.983 --> 00:02:25.555
It saw a horrible, life-changing
and life-destroying event

00:02:25.579 --> 00:02:27.961
and thought it was something positive.

00:02:27.985 --> 00:02:31.426
I realized that it recognized
the contrast,

00:02:31.450 --> 00:02:34.149
the reds, the yellows,

00:02:34.173 --> 00:02:37.251
and thought it was something
worth remarking on positively.

00:02:37.928 --> 00:02:39.543
And part of why it was doing this

00:02:39.577 --> 00:02:42.522
was because most
of the images I had given it

00:02:42.546 --> 00:02:44.386
were positive images.

00:02:44.903 --> 00:02:48.561
That's because people
tend to share positive images

00:02:48.585 --> 00:02:50.775
when they talk about their experiences.

00:02:51.267 --> 00:02:53.808
When was the last time
you saw a selfie at a funeral?

00:02:55.434 --> 00:02:58.529
I realized that,
as I worked on improving AI

00:02:58.553 --> 00:03:02.267
task by task, dataset by dataset,

00:03:02.291 --> 00:03:05.188
that I was creating massive gaps,

00:03:05.212 --> 00:03:09.211
holes and blind spots
in what it could understand.

00:03:10.307 --> 00:03:11.641
And while doing so,

00:03:11.665 --> 00:03:14.148
I was encoding all kinds of biases.

00:03:15.029 --> 00:03:18.347
Biases that reflect a limited viewpoint,

00:03:18.371 --> 00:03:20.632
limited to a single dataset --

00:03:21.283 --> 00:03:25.141
biases that can reflect
human biases found in the data,

00:03:25.165 --> 00:03:28.269
such as prejudice and stereotyping.

00:03:29.554 --> 00:03:32.611
I thought back to the evolution
of the technology

00:03:32.635 --> 00:03:35.137
that brought me to where I was that day --

00:03:35.966 --> 00:03:38.199
how the first color images

00:03:38.223 --> 00:03:41.271
were calibrated against
a white woman's skin,

00:03:41.665 --> 00:03:45.810
meaning that color photography
was biased against black faces.

00:03:46.514 --> 00:03:49.439
And that same bias, that same blind spot

00:03:49.463 --> 00:03:51.330
continued well into the '90s.

00:03:51.701 --> 00:03:54.855
And the same blind spot
continues even today

00:03:54.879 --> 00:03:58.577
in how well we can recognize
different people's faces

00:03:58.601 --> 00:04:00.801
in facial recognition technology.

00:04:01.323 --> 00:04:04.466
I though about the state of the art
in research today,

00:04:04.490 --> 00:04:09.004
where we tend to limit our thinking
to one dataset and one problem.

00:04:09.688 --> 00:04:14.569
And that in doing so, we were creating
more blind spots and biases

00:04:14.593 --> 00:04:16.870
that the AI could further amplify.

00:04:17.712 --> 00:04:19.791
I realized then
that we had to think deeply

00:04:19.815 --> 00:04:25.334
about how the technology we work on today
looks in five years, in 10 years.

00:04:25.990 --> 00:04:29.132
Humans evolve slowly,
with time to correct for issues

00:04:29.156 --> 00:04:32.690
in the interaction of humans
and their environment.

00:04:33.276 --> 00:04:38.705
In contrast, artificial intelligence
is evolving at an incredibly fast rate.

00:04:39.013 --> 00:04:40.786
And that means that it really matters

00:04:40.810 --> 00:04:43.127
that we think about this
carefully right now --

00:04:44.180 --> 00:04:47.188
that we reflect on our own blind spots,

00:04:47.212 --> 00:04:49.529
our own biases,

00:04:49.553 --> 00:04:53.410
and think about how that's informing
the technology we're creating

00:04:53.434 --> 00:04:57.336
and discuss what the technology of today
will mean for tomorrow.

00:04:58.593 --> 00:05:01.784
CEOs and scientists have weighed in
on what they think

00:05:01.808 --> 00:05:05.133
the artificial intelligence technology
of the future will be.

00:05:05.157 --> 00:05:06.775
Stephen Hawking warns that

00:05:06.799 --> 00:05:09.806
"Artificial intelligence
could end mankind."

00:05:10.307 --> 00:05:12.990
Elon Musk warns
that it's an existential risk

00:05:13.014 --> 00:05:16.588
and one of the greatest risks
that we face as a civilization.

00:05:17.665 --> 00:05:19.117
Bill Gates has made the point,

00:05:19.141 --> 00:05:22.326
"I don't understand
why people aren't more concerned."

00:05:23.412 --> 00:05:24.730
But these views --

00:05:25.618 --> 00:05:27.352
they're part of the story.

00:05:28.079 --> 00:05:30.499
The math, the models,

00:05:30.523 --> 00:05:33.593
the basic building blocks
of artificial intelligence

00:05:33.617 --> 00:05:36.752
are something that we call access
and all work with.

00:05:36.776 --> 00:05:40.561
We have open-source tools
for machine learning and intelligence

00:05:40.585 --> 00:05:42.319
that we can contribute to.

00:05:42.919 --> 00:05:46.259
And beyond that,
we can share our experience.

00:05:46.760 --> 00:05:50.228
We can share our experiences
with technology and how it concerns us

00:05:50.252 --> 00:05:51.719
and how it excites us.

00:05:52.251 --> 00:05:54.118
We can discuss what we love.

00:05:55.244 --> 00:05:57.275
We can communicate with foresight

00:05:57.299 --> 00:06:02.156
about the aspects of technology
that could be more beneficial

00:06:02.180 --> 00:06:04.780
or could be more problematic over time.

00:06:05.799 --> 00:06:09.942
If we all focus on opening up
the discussion on AI

00:06:09.966 --> 00:06:11.775
with foresight towards the future,

00:06:13.093 --> 00:06:17.363
this will help create a general
conversation and awareness

00:06:17.387 --> 00:06:19.900
about what AI is now,

00:06:21.212 --> 00:06:23.213
what it can become

00:06:23.237 --> 00:06:25.022
and all the things that we need to do

00:06:25.046 --> 00:06:28.799
in order to enable that outcome
that best suits us.

00:06:29.490 --> 00:06:33.164
We already see and know this
in the technology that we use today.

00:06:33.767 --> 00:06:37.647
We use smart phones
and digital assistants and Roombas.

00:06:38.457 --> 00:06:39.607
Are they evil?

00:06:40.268 --> 00:06:41.815
Maybe sometimes.

00:06:42.664 --> 00:06:43.997
Are they beneficial?

00:06:45.005 --> 00:06:46.538
Yes, they're that, too.

00:06:48.236 --> 00:06:49.997
And they're not all the same.

00:06:50.489 --> 00:06:54.029
And there you already see
a light shining on what the future holds.

00:06:54.942 --> 00:06:58.561
The future continues on
from what we build and create right now.

00:06:59.165 --> 00:07:01.807
We set into motion that domino effect

00:07:01.831 --> 00:07:04.431
that carves out AI's evolutionary path.

00:07:05.173 --> 00:07:08.044
In our time right now,
we shape the AI of tomorrow.

00:07:08.566 --> 00:07:12.265
Technology that immerses us
in augmented realities

00:07:12.289 --> 00:07:14.855
bringing to life past worlds.

00:07:15.844 --> 00:07:20.156
Technology that helps people
to share their experiences

00:07:20.180 --> 00:07:22.442
when they have difficulty communicating.

00:07:23.323 --> 00:07:27.855
Technology built on understanding
the streaming visual worlds

00:07:27.879 --> 00:07:30.958
used as technology for self-driving cars.

00:07:32.490 --> 00:07:35.903
Technology built on understanding images
and generating language,

00:07:35.927 --> 00:07:39.990
evolving into technology that helps people
who are visually impaired

00:07:40.014 --> 00:07:42.814
be better able to access the visual world.

00:07:42.838 --> 00:07:46.099
And we also see how technology
can lead to problems.

00:07:46.885 --> 00:07:48.313
We have technology today

00:07:48.337 --> 00:07:52.172
that analyzes physical
characteristics we're born with --

00:07:52.196 --> 00:07:55.468
such as the color of our skin
or the look of our face --

00:07:55.492 --> 00:07:59.296
in order to determine whether or not
we might be criminals or terrorists.

00:07:59.688 --> 00:08:02.593
We have technology
that crunches through our data,

00:08:02.617 --> 00:08:05.513
even data relating
to our gender or our race,

00:08:05.537 --> 00:08:08.402
in order to determine whether or not
we might get a loan.

00:08:09.494 --> 00:08:11.073
All that we see now

00:08:11.097 --> 00:08:14.714
is a snapshot in the evolution
of artificial intelligence.

00:08:15.763 --> 00:08:17.541
Because where we are right now,

00:08:17.565 --> 00:08:19.803
is within a moment of that evolution.

00:08:20.690 --> 00:08:24.492
That means that what we do now
will affect what happens down the line

00:08:24.516 --> 00:08:25.716
and in the future.

00:08:26.063 --> 00:08:30.014
If we want AI to evolve
in a way that helps humans,

00:08:30.038 --> 00:08:32.839
then we need to define
the goals and strategies

00:08:32.863 --> 00:08:34.596
that enable that path now.

00:08:35.680 --> 00:08:39.418
What I'd like to see is something
that fits well with humans,

00:08:39.442 --> 00:08:42.242
with our culture and with the environment.

00:08:43.435 --> 00:08:47.919
Technology that aids and assists
those of us with neurological conditions

00:08:47.943 --> 00:08:49.664
or other disabilities

00:08:49.688 --> 00:08:52.904
in order to make life
equally challenging for everyone.

00:08:54.097 --> 00:08:55.518
Technology that works

00:08:55.542 --> 00:08:59.475
regardless of your demographics
or the color of your skin.

00:09:00.383 --> 00:09:05.125
And so today, what I focus on
is the technology for tomorrow

00:09:05.149 --> 00:09:06.882
and for 10 years from now.

00:09:08.530 --> 00:09:11.164
AI can turn out in many different ways.

00:09:11.688 --> 00:09:12.913
But in this case,

00:09:12.937 --> 00:09:16.265
it isn't a self-driving car
without any destination.

00:09:16.884 --> 00:09:19.284
This is the car that we are driving.

00:09:19.953 --> 00:09:23.548
We choose when to speed up
and when to slow down.

00:09:23.572 --> 00:09:25.972
We choose if we need to make a turn.

00:09:26.868 --> 00:09:29.868
We choose what the AI
of the future will be.

00:09:31.186 --> 00:09:32.523
There's a vast playing field

00:09:32.547 --> 00:09:35.512
of all the things that artificial
intelligence can become.

00:09:36.064 --> 00:09:37.864
It will become many things.

00:09:39.694 --> 00:09:41.426
And it's up to us now,

00:09:41.450 --> 00:09:44.511
in order to figure out
what we need to put in place

00:09:44.535 --> 00:09:48.342
to make sure the outcomes
of artificial intelligence

00:09:48.366 --> 00:09:51.432
are the ones that will be
better for all of us.

00:09:51.456 --> 00:09:52.606
Thank you.

00:09:52.630 --> 00:09:54.817
(Applause)

