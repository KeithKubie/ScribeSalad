WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.680
My name is Edmond Awad. I'm a
postdoctoral associate the MIT Media Lab,

00:00:04.680 --> 00:00:08.550
in the research group, Scalable
Cooperation.

00:00:08.550 --> 00:00:13.710
Last year I co-developed a website
called Moral Machine which basically

00:00:13.710 --> 00:00:18.480
tries to understand the human perspective of moral decisions made by machines.

00:00:18.480 --> 00:00:24.119
The Moral Machine is a website that generates random moral dilemmas that are faced by

00:00:24.119 --> 00:00:27.840
a driverless car and it asks you "what do
you think the car should do in this

00:00:27.840 --> 00:00:32.399
dilemma?" So for example we could show
you, the car is heading to the street,

00:00:32.399 --> 00:00:37.079
and then suddenly a few pedestrians
jump in front of it, and the brake fails,

00:00:37.079 --> 00:00:41.040
the car has either to continue through
those pedestrians, or swerve into a

00:00:41.040 --> 00:00:44.969
barrier, killing the passengers, which
have different characteristics

00:00:44.969 --> 00:00:49.920
and it asks you to choose one of the two options. So there are two purposes.

00:00:49.920 --> 00:00:55.670
The first purpose is to collect data

00:00:55.670 --> 00:00:59.219
about what kind of moral principles people think the machine

00:00:59.219 --> 00:01:03.870
should employ when they are facing
the moral trade-off. The second goal is

00:01:03.870 --> 00:01:09.420
to popularize the discussion about moral
decisions made by machines.

00:01:09.420 --> 00:01:13.979
I think the most interesting part is that there are some cultural differences,

00:01:13.979 --> 00:01:18.420
like people in different countries have
different preferences, and this of course

00:01:18.420 --> 00:01:23.100
will have its own application on the
policy making between different

00:01:23.100 --> 00:01:28.140
countries. One of the questions that we
always receive from people is

00:01:28.140 --> 00:01:33.420
saying, "Are really gonna use this
data that you collected into programming

00:01:33.420 --> 00:01:37.380
a self-driving cars?" Which is also kind
of scary because it means that anybody

00:01:37.380 --> 00:01:41.430
on the internet could have made a
decision to do this and, you know, the

00:01:41.430 --> 00:01:46.350
human bias. And of course the goal is
not to to say

00:01:46.350 --> 00:01:50.369
"the majority should decide what
kind of moral decisions the machine should make."

00:01:50.369 --> 00:01:55.829
The idea is to provide one input
for policy makers and regulators who are

00:01:55.829 --> 00:01:59.549
always interested to know what the
people's reaction will be. We'll use this

00:01:59.549 --> 00:02:05.320
data to report what the public's view or reaction is to this kind of decision.

00:02:05.320 --> 00:02:11.509
 

