WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:03.220
These problems are often
called the exploding and

00:00:03.220 --> 00:00:04.330
vanishing gradient problems.

00:00:04.330 --> 00:00:09.720
And we're going to fix them,
surprisingly in very different ways.

00:00:09.720 --> 00:00:13.940
One using a very simple hack, and
the other one with a very elegant but

00:00:13.940 --> 00:00:16.300
slightly complicated
change to the model.

00:00:16.300 --> 00:00:18.990
The simple hack is called
gradient clipping.

00:00:18.990 --> 00:00:21.830
In order to prevent the gradients
from growing inbounded,

00:00:21.830 --> 00:00:26.600
you can compute their norm and shrink
their step, when the norm grows too big.

00:00:26.600 --> 00:00:29.310
It's hacky, but
it's cheap and effective.

00:00:29.310 --> 00:00:32.549
The more difficult thing to
fix is gradient vanishing.

00:00:32.549 --> 00:00:36.200
Vanishing gradients make your model
only remember recent events and

00:00:36.200 --> 00:00:38.250
forget the more distant past,

00:00:38.250 --> 00:00:42.580
which means recurrent neural nets tend
to not work well past a few time steps.

