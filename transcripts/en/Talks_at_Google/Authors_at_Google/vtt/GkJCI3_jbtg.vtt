WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.283
[MUSIC PLAYING]

00:00:06.100 --> 00:00:07.780
BRUCE SCHNEIER: Thank you.

00:00:07.780 --> 00:00:10.390
So this is the--

00:00:10.390 --> 00:00:12.940
this is the book.

00:00:12.940 --> 00:00:16.150
So it has one-- my
first-ever clickbait title.

00:00:16.150 --> 00:00:17.890
[LAUGHTER]

00:00:19.135 --> 00:00:20.456
And I like the cover.

00:00:20.456 --> 00:00:21.830
I like the cover
for two reasons.

00:00:21.830 --> 00:00:24.320
One, there's only one
button that says, OK,

00:00:24.320 --> 00:00:25.455
when it's clearly not OK.

00:00:25.455 --> 00:00:27.342
[LAUGHTER]

00:00:27.342 --> 00:00:29.800
And it looks like this thing's
been throwing error messages

00:00:29.800 --> 00:00:32.331
for the past hour, and nobody
has been paying any attention

00:00:32.331 --> 00:00:32.830
to them.

00:00:32.830 --> 00:00:34.602
[LAUGHTER]

00:00:36.380 --> 00:00:38.890
So this is-- a
book as a computer.

00:00:38.890 --> 00:00:44.560
And what I'm writing about
is security in a world

00:00:44.560 --> 00:00:46.540
where everything is a computer.

00:00:46.540 --> 00:00:49.900
And I think this is the way
we need to conceptualize

00:00:49.900 --> 00:00:52.030
the world we're building.

00:00:52.030 --> 00:00:55.850
This smart phone is
not a phone, it's

00:00:55.850 --> 00:00:58.070
a computer that
makes phone calls.

00:00:58.070 --> 00:01:00.290
And similarly,
your microwave oven

00:01:00.290 --> 00:01:02.090
is a computer that
makes things hot.

00:01:02.090 --> 00:01:05.269
Your refrigerator is a computer
that keeps things cold.

00:01:05.269 --> 00:01:08.540
An ATM machine is a
computer with money inside.

00:01:08.540 --> 00:01:11.562
A car is now a computer with
four wheels and an engine.

00:01:11.562 --> 00:01:12.520
Actually, that's wrong.

00:01:12.520 --> 00:01:15.530
A car is about 100-plus
distributed system

00:01:15.530 --> 00:01:17.570
with four wheels and an engine.

00:01:17.570 --> 00:01:18.440
Right?

00:01:18.440 --> 00:01:20.520
This is more than the internet.

00:01:20.520 --> 00:01:23.870
It's the internet of things, but
it's more than that, as well.

00:01:23.870 --> 00:01:26.570
In my book, I use the
term internet plus.

00:01:26.570 --> 00:01:30.530
I hate having to invent
a term, but there really

00:01:30.530 --> 00:01:36.920
isn't a term we have for
the internet, the computers,

00:01:36.920 --> 00:01:41.090
the things, the big systems,
like power plants, the data

00:01:41.090 --> 00:01:45.220
stores, the
processes, the people.

00:01:45.220 --> 00:01:47.880
And it's that
holistic system that I

00:01:47.880 --> 00:01:52.500
think we need to look at
when we look at security.

00:01:52.500 --> 00:01:55.410
So if everything is
becoming a computer,

00:01:55.410 --> 00:01:59.270
it means two things
that are relevant here.

00:01:59.270 --> 00:02:03.090
That internet security
becomes everything security,

00:02:03.090 --> 00:02:07.650
and all the lessons and problems
of internets and computers

00:02:07.650 --> 00:02:10.440
become problems everywhere.

00:02:10.440 --> 00:02:14.640
So let me start with
six quick lessons

00:02:14.640 --> 00:02:17.380
of computer security, which
will be true about everything

00:02:17.380 --> 00:02:18.970
everywhere.

00:02:18.970 --> 00:02:21.330
Some of them are
obvious in computers,

00:02:21.330 --> 00:02:23.070
not so obvious elsewhere.

00:02:23.070 --> 00:02:26.160
The first, most software is
poorly written and insecure,

00:02:26.160 --> 00:02:26.660
right?

00:02:26.660 --> 00:02:27.690
We know this.

00:02:27.690 --> 00:02:30.360
Basic reason is the
market doesn't want

00:02:30.360 --> 00:02:32.470
to pay for quality software.

00:02:32.470 --> 00:02:34.200
Good, fast, cheap--
pick any two.

00:02:34.200 --> 00:02:37.550
We have picked fast
and cheap over good.

00:02:37.550 --> 00:02:41.910
With very expensive exceptions,
like avionics and the space

00:02:41.910 --> 00:02:46.720
shuttle, most software
is really lousy.

00:02:46.720 --> 00:02:48.910
It kind of just barely works.

00:02:48.910 --> 00:02:53.620
Now, for security, lots
of vulnerabilities.

00:02:53.620 --> 00:02:54.534
Sorry-- lots of bugs.

00:02:54.534 --> 00:02:56.200
Some of those bugs
have vulnerabilities.

00:02:56.200 --> 00:02:58.710
Some of those vulnerabilities
are exploitable,

00:02:58.710 --> 00:03:00.570
which means modern
software has lots

00:03:00.570 --> 00:03:03.360
of exploitable
vulnerabilities, and that's not

00:03:03.360 --> 00:03:06.070
going to change any time soon.

00:03:06.070 --> 00:03:08.820
The second lesson is that
the internet was never

00:03:08.820 --> 00:03:10.410
designed with security in mind.

00:03:10.410 --> 00:03:13.200
That seems ridiculous today.

00:03:13.200 --> 00:03:16.020
But if you think back to the
late '70s and early '80s,

00:03:16.020 --> 00:03:17.760
there were two things
that were true.

00:03:17.760 --> 00:03:20.850
One, the internet was used
for nothing important ever.

00:03:20.850 --> 00:03:22.620
[LAUGHTER]

00:03:22.620 --> 00:03:26.550
And two, you had to be a member
of a research institution

00:03:26.550 --> 00:03:29.740
to have access to it.

00:03:29.740 --> 00:03:32.020
And you read the
early designers,

00:03:32.020 --> 00:03:36.820
and they talked about how
limiting physical access

00:03:36.820 --> 00:03:39.580
was a security
measure, and the fact

00:03:39.580 --> 00:03:42.250
that you could
exclude bad actors

00:03:42.250 --> 00:03:46.260
meant that you didn't have
to worry much about security.

00:03:46.260 --> 00:03:48.560
So a decision was
made deliberately

00:03:48.560 --> 00:03:50.270
to leave security
to the end points,

00:03:50.270 --> 00:03:53.240
not put it in the network.

00:03:53.240 --> 00:03:55.190
Fast forward today,
and we are still

00:03:55.190 --> 00:03:59.900
living with the results of
that in the domain name system,

00:03:59.900 --> 00:04:03.530
in routing, in packet
security, email addresses.

00:04:03.530 --> 00:04:07.040
Again and again, the
protocols don't have security,

00:04:07.040 --> 00:04:09.800
and we are stuck with them.

00:04:09.800 --> 00:04:13.940
Third lesson, the extensibility
of computerized systems

00:04:13.940 --> 00:04:15.860
means they can be
used against us.

00:04:15.860 --> 00:04:18.649
Extensibility is not something
that non-computer people

00:04:18.649 --> 00:04:20.110
are used to.

00:04:20.110 --> 00:04:21.880
Basically, what
I mean by that is

00:04:21.880 --> 00:04:24.970
you can't constrain the
functionality of a computer

00:04:24.970 --> 00:04:27.580
because it's software.

00:04:27.580 --> 00:04:30.580
When I was a kid, I had a
telephone-- big, black thing

00:04:30.580 --> 00:04:31.780
attached to the wall.

00:04:31.780 --> 00:04:33.060
Great device.

00:04:33.060 --> 00:04:35.410
But no matter how
hard I tried, I

00:04:35.410 --> 00:04:38.420
couldn't make it be anything
other than a telephone.

00:04:38.420 --> 00:04:40.270
This is a computer
that makes phone calls.

00:04:40.270 --> 00:04:42.220
It can do anything
you want, right?

00:04:42.220 --> 00:04:44.710
There's an app for that.

00:04:44.710 --> 00:04:48.130
Because this can be programmed,
because it's a computer,

00:04:48.130 --> 00:04:49.540
it can do anything.

00:04:49.540 --> 00:04:52.350
You can't constrain
its functionality.

00:04:52.350 --> 00:04:54.250
It means several
things for security.

00:04:54.250 --> 00:04:58.360
Hard to test this thing
because what it does changes,

00:04:58.360 --> 00:05:00.580
how it's configured changes.

00:05:00.580 --> 00:05:04.040
And it can get additional
features you don't want.

00:05:04.040 --> 00:05:07.570
That's what malware is.

00:05:07.570 --> 00:05:09.700
So you can put
malware on this phone,

00:05:09.700 --> 00:05:12.670
or on an internet-connected
refrigerator in a way

00:05:12.670 --> 00:05:15.640
that you can't
possibly ever do it

00:05:15.640 --> 00:05:19.020
in an old electromechanical
refrigerator.

00:05:19.020 --> 00:05:22.510
Because they're not computers.

00:05:22.510 --> 00:05:25.570
Fourth lesson is
about complexity.

00:05:25.570 --> 00:05:26.820
A lot of ways I can say this.

00:05:26.820 --> 00:05:28.780
Basically, the
complexity of computers

00:05:28.780 --> 00:05:30.910
means attack is
easier than defense.

00:05:30.910 --> 00:05:34.540
I can spend an hour
on that sentence,

00:05:34.540 --> 00:05:38.170
but complex systems
are hard to secure.

00:05:38.170 --> 00:05:39.930
And it's the most
complex machine mankind

00:05:39.930 --> 00:05:42.750
has ever built by
a lot, which makes

00:05:42.750 --> 00:05:45.880
this incredibly hard to secure.

00:05:45.880 --> 00:05:48.340
Hard to design
securely, hard to test--

00:05:48.340 --> 00:05:49.790
everything about it.

00:05:49.790 --> 00:05:54.360
It is easier to attack a
system than to defend it.

00:05:54.360 --> 00:05:57.190
A fifth lesson is that there
are new vulnerabilities

00:05:57.190 --> 00:05:58.870
in the interconnections.

00:05:58.870 --> 00:06:00.960
As we connect things
to each other,

00:06:00.960 --> 00:06:04.300
vulnerabilities in one
thing affect other things.

00:06:04.300 --> 00:06:05.380
Lots of examples.

00:06:05.380 --> 00:06:07.840
The dying botnets.

00:06:07.840 --> 00:06:11.560
Vulnerabilities in
internet-connected digital

00:06:11.560 --> 00:06:14.320
video recorders-- and
webcams, primarily--

00:06:14.320 --> 00:06:17.260
allowed an attacker to create
a botnet that dropped a domain

00:06:17.260 --> 00:06:19.980
name server that, in
turn, dropped a couple

00:06:19.980 --> 00:06:22.720
of dozen real popular websites.

00:06:22.720 --> 00:06:27.610
In 2013, Target corporation,
attack through a vulnerability

00:06:27.610 --> 00:06:29.890
in the HVAC
contractor of several

00:06:29.890 --> 00:06:32.950
of their
mid-Pennsylvania stores.

00:06:32.950 --> 00:06:36.230
Earlier this year, there was a
story of a casino in Las Vegas.

00:06:36.230 --> 00:06:38.110
We don't know the
name of the casino.

00:06:38.110 --> 00:06:42.540
They had their high
roller database stolen,

00:06:42.540 --> 00:06:44.140
and the hackers got in through--

00:06:44.140 --> 00:06:46.870
and I'm not making this up--
their internet-connected fish

00:06:46.870 --> 00:06:47.370
tank.

00:06:47.370 --> 00:06:51.298
[LAUGHTER]

00:06:52.780 --> 00:06:59.350
So vulnerabilities-- this can
be hard because sometimes,

00:06:59.350 --> 00:07:03.490
nobody's at fault. I read
a blog a few months ago

00:07:03.490 --> 00:07:06.640
about a vulnerability that
results from the way Google

00:07:06.640 --> 00:07:09.610
treats email addresses.

00:07:09.610 --> 00:07:11.970
The dots don't
matter for your name.

00:07:11.970 --> 00:07:15.850
And the way Netflix treats email
addresses, the dots do matter.

00:07:15.850 --> 00:07:20.360
Turns out, you can play
some games with that.

00:07:20.360 --> 00:07:22.260
Who do we blame?

00:07:22.260 --> 00:07:25.122
I'm not sure we blame anybody.

00:07:25.122 --> 00:07:27.330
There's a vulnerability in
PGP, which is actually not

00:07:27.330 --> 00:07:28.470
really vulnerability in PGP.

00:07:28.470 --> 00:07:30.053
It's vulnerability
in the way emailers

00:07:30.053 --> 00:07:33.200
handle PGP, which everyone
is convinced everyone else is

00:07:33.200 --> 00:07:36.480
at fault. And these
kind of things

00:07:36.480 --> 00:07:39.800
are going to happen
more and more.

00:07:39.800 --> 00:07:44.590
The last lesson is that
attacks always get better.

00:07:44.590 --> 00:07:48.420
Attacks always get
easier, faster, cheaper.

00:07:48.420 --> 00:07:50.170
Some of this is Moore's law.

00:07:50.170 --> 00:07:54.000
Computers get faster,
so password guessing

00:07:54.000 --> 00:07:56.550
gets faster as computers get
faster, not because we're

00:07:56.550 --> 00:07:57.615
smarter about it.

00:07:57.615 --> 00:08:00.640
But we also get smarter.

00:08:00.640 --> 00:08:03.760
Attackers adapt, attackers
figure out new things,

00:08:03.760 --> 00:08:06.180
and expertise flows downhill.

00:08:06.180 --> 00:08:08.980
What, today, is a top-secret
NSA program, tomorrow, becomes

00:08:08.980 --> 00:08:12.700
a PhD thesis, and the next
day is a common hacker tool.

00:08:12.700 --> 00:08:16.330
And you can see this
again and again.

00:08:16.330 --> 00:08:19.810
An example might be IMSI
catchers, fake cell phone

00:08:19.810 --> 00:08:20.800
towers, sting rays.

00:08:24.750 --> 00:08:27.750
The cause of them
is that cell phones

00:08:27.750 --> 00:08:29.600
don't authenticate to towers.

00:08:29.600 --> 00:08:33.169
They automatically trust
anybody who says, I'm a tower.

00:08:33.169 --> 00:08:34.590
So if you put up
a fake tower, you

00:08:34.590 --> 00:08:37.139
can now query phones
and get their addresses

00:08:37.139 --> 00:08:39.970
and sort of know who is there.

00:08:39.970 --> 00:08:43.679
This was something that
the NSA used, the FBI used.

00:08:43.679 --> 00:08:47.270
Big government
secret for a while.

00:08:47.270 --> 00:08:48.770
Expertise flowed downhill.

00:08:48.770 --> 00:08:49.520
A few years ago--

00:08:49.520 --> 00:08:50.186
I think it was--

00:08:50.186 --> 00:08:53.120
Motherboard looked
around the DC,

00:08:53.120 --> 00:08:56.450
found a couple of dozen of them
run by we-don't-know-who around

00:08:56.450 --> 00:08:58.390
US government buildings.

00:08:58.390 --> 00:09:00.990
Right now, you can
go on Alibaba.com,

00:09:00.990 --> 00:09:03.500
buy one of those things
for about $1,000.

00:09:03.500 --> 00:09:07.704
In China, they're used
to send spam to phones.

00:09:07.704 --> 00:09:09.370
You'd get a
software-defined radio card.

00:09:09.370 --> 00:09:13.980
You can download free
software and make your own.

00:09:13.980 --> 00:09:16.620
What started out as
something that was hard to do

00:09:16.620 --> 00:09:19.370
is now easy to do.

00:09:19.370 --> 00:09:22.730
So those are my six
lessons that are going

00:09:22.730 --> 00:09:24.800
to be true for everything.

00:09:24.800 --> 00:09:28.040
And none of that is
new, but up to now,

00:09:28.040 --> 00:09:31.570
it's been basically
a manageable problem.

00:09:31.570 --> 00:09:34.690
But I think that's
going to change.

00:09:34.690 --> 00:09:40.090
And the reasons are automation,
autonomy, and physical agency.

00:09:40.090 --> 00:09:44.860
Computers that can do things.

00:09:44.860 --> 00:09:48.680
So if you do computer security,
you've heard of the CIA triad--

00:09:48.680 --> 00:09:51.110
confidentiality, integrity,
and availability.

00:09:51.110 --> 00:09:54.860
Three basic properties
we deal with in security.

00:09:54.860 --> 00:09:58.730
By and large, most of what our
issues are are confidentiality.

00:09:58.730 --> 00:10:02.750
Someone stole and
misused our data.

00:10:02.750 --> 00:10:07.130
That's Equifax, that's Office
of Personnel Management, that's

00:10:07.130 --> 00:10:08.450
Cambridge Analytica.

00:10:08.450 --> 00:10:10.460
That's all of the
data thefts ever.

00:10:14.630 --> 00:10:18.650
But when you have
computers that can affect

00:10:18.650 --> 00:10:21.440
the world in a direct
physical manner,

00:10:21.440 --> 00:10:25.940
integrity and availability
become much more serious.

00:10:25.940 --> 00:10:27.530
Because the computers
can do stuff.

00:10:27.530 --> 00:10:30.120
There's real risk to
life and property.

00:10:30.120 --> 00:10:32.870
So yes, I'm concerned that
someone hacks my hospital

00:10:32.870 --> 00:10:35.510
and steals my private
patient medical records.

00:10:35.510 --> 00:10:39.600
But I'm much more concerned
that they change my blood type.

00:10:39.600 --> 00:10:43.820
I don't want them to
hack my car and use

00:10:43.820 --> 00:10:46.460
the bluetooth microphone to
listen in on conversations.

00:10:46.460 --> 00:10:50.010
But I really don't want
them to disable the brakes.

00:10:50.010 --> 00:10:53.590
Those are data integrity and
data availability attacks,

00:10:53.590 --> 00:10:55.890
respectively.

00:10:55.890 --> 00:10:59.680
So suddenly, the effects
are much greater.

00:10:59.680 --> 00:11:03.130
And this is cars,
medical devices, drones,

00:11:03.130 --> 00:11:07.840
any kind of weapon systems,
thermostats, power plants,

00:11:07.840 --> 00:11:10.510
smart city anything, appliances.

00:11:13.850 --> 00:11:18.380
I blogged, a couple of days ago,
about an attack where someone--

00:11:18.380 --> 00:11:22.610
just theoretical-- if you could
hack enough major appliances,

00:11:22.610 --> 00:11:26.120
can turn power on and
off, and synchronization,

00:11:26.120 --> 00:11:29.840
and affect the load
on power plants,

00:11:29.840 --> 00:11:31.910
and potentially cause blackouts.

00:11:31.910 --> 00:11:36.030
Now, very much a side effect,
but once I say that, you say,

00:11:36.030 --> 00:11:39.730
well, yeah, duh, of
course you can do that.

00:11:39.730 --> 00:11:42.470
Very different sort of attack.

00:11:42.470 --> 00:11:46.070
There's a fundamental difference
between my spreadsheet crashes,

00:11:46.070 --> 00:11:49.590
I may lose my data, and my
implanted defibrillator crashes

00:11:49.590 --> 00:11:50.810
and I lose my life.

00:11:50.810 --> 00:11:55.140
And it could be the same CPU,
the same operating system,

00:11:55.140 --> 00:11:58.640
the same vulnerability,
the same attack software.

00:11:58.640 --> 00:12:02.090
Because of what the
computer can do,

00:12:02.090 --> 00:12:04.571
the effects are much different.

00:12:04.571 --> 00:12:06.570
So at the same time we're
getting this increased

00:12:06.570 --> 00:12:11.130
functionality, there are
some longstanding security

00:12:11.130 --> 00:12:15.390
paradigms that are failing,
and I'll give three.

00:12:15.390 --> 00:12:17.390
The first one is patching.

00:12:17.390 --> 00:12:24.010
Patching is how we get security,
and it's now having trouble.

00:12:24.010 --> 00:12:27.880
Actually, there's two reasons
why our phones and computers

00:12:27.880 --> 00:12:29.230
are as secure as they are.

00:12:29.230 --> 00:12:32.680
The first is that there are
security engineers at Apple,

00:12:32.680 --> 00:12:35.560
at Microsoft, at Google that
are designing them as secure

00:12:35.560 --> 00:12:37.420
as they are in the first place.

00:12:37.420 --> 00:12:41.320
And those engineers can
quickly write and push down

00:12:41.320 --> 00:12:44.530
patches when vulnerabilities
are discovered.

00:12:44.530 --> 00:12:46.180
That's a pretty good ecosystem.

00:12:46.180 --> 00:12:48.360
We do that well.

00:12:48.360 --> 00:12:51.750
The problem is, it doesn't work
for low-cost embedded systems

00:12:51.750 --> 00:12:54.670
like DVRs and routers.

00:12:54.670 --> 00:12:58.330
These are designed and built
offshore by third parties,

00:12:58.330 --> 00:13:01.180
by ad hoc teams that come
together, design them, and then

00:13:01.180 --> 00:13:02.920
split apart.

00:13:02.920 --> 00:13:07.270
There aren't people who can
write those patches when

00:13:07.270 --> 00:13:09.660
a vulnerability is discovered.

00:13:09.660 --> 00:13:11.580
Even worse, a lot
of these devices

00:13:11.580 --> 00:13:14.930
have no way to patch them.

00:13:14.930 --> 00:13:21.214
If your DVR is vulnerable
to the hack that

00:13:21.214 --> 00:13:22.880
allows it to be
[INAUDIBLE] to a botnet,

00:13:22.880 --> 00:13:25.490
the only way you can
patch it is to throw it

00:13:25.490 --> 00:13:27.920
away and buy a new one.

00:13:27.920 --> 00:13:28.970
That's the mechanism.

00:13:28.970 --> 00:13:31.300
We have no other.

00:13:31.300 --> 00:13:33.880
Now, actually, throw it
away and buy a new one

00:13:33.880 --> 00:13:37.040
is a reasonable
security measure.

00:13:37.040 --> 00:13:39.660
We do get security in the
fact that the lifecycle

00:13:39.660 --> 00:13:44.310
of phones and computers is
about three to five years.

00:13:44.310 --> 00:13:48.940
That's not true
for consumer goods.

00:13:48.940 --> 00:13:52.180
You're going to replace
your DVR every 10 years,

00:13:52.180 --> 00:13:55.640
your refrigerator
every 25 years.

00:13:55.640 --> 00:13:58.990
I bought a programmable
thermostat last year.

00:13:58.990 --> 00:14:01.640
I expect to replace it
approximately never.

00:14:01.640 --> 00:14:03.080
[LAUGHTER]

00:14:04.790 --> 00:14:07.630
Think about it in
terms of a car.

00:14:07.630 --> 00:14:08.940
You buy a car today.

00:14:08.940 --> 00:14:11.170
Let's say the software
is two years old.

00:14:11.170 --> 00:14:14.050
You're going to drive it
for 10 years, sell it.

00:14:14.050 --> 00:14:17.604
Someone else buys it, drives
it for 10 years, they sell it.

00:14:17.604 --> 00:14:19.270
Someone else buys it,
puts it on a boat,

00:14:19.270 --> 00:14:22.120
sends it to South America, where
someone else there buys it,

00:14:22.120 --> 00:14:24.620
drives it for another
10 to 20 years.

00:14:24.620 --> 00:14:30.160
When you go home, find
a computer from 1976.

00:14:30.160 --> 00:14:33.790
Try to boot it, try to run
it, try to make it secure.

00:14:33.790 --> 00:14:39.490
We actually have no idea how
to secure 40-year-old consumer

00:14:39.490 --> 00:14:40.030
software.

00:14:40.030 --> 00:14:43.450
We haven't the faintest clue.

00:14:43.450 --> 00:14:45.670
And we need to figure it out.

00:14:45.670 --> 00:14:47.730
So what, does Chrysler
maintain a test bed

00:14:47.730 --> 00:14:53.820
of 200 chassis for vulnerability
testing and for patch testing?

00:14:53.820 --> 00:14:56.330
Is that the mechanism?

00:14:56.330 --> 00:14:59.410
We're not going to be able
to treat these goods like we

00:14:59.410 --> 00:15:02.530
treat phones and computers.

00:15:02.530 --> 00:15:06.340
If we start forcing
the computer lifecycle

00:15:06.340 --> 00:15:08.950
onto all these other things,
we are probably literally

00:15:08.950 --> 00:15:11.080
going to cook the planet.

00:15:11.080 --> 00:15:14.156
So we need some other
way, and we don't have it.

00:15:14.156 --> 00:15:17.870
The second thing that's
failing is authentication.

00:15:17.870 --> 00:15:22.630
We've always been only
OK at authentication.

00:15:22.630 --> 00:15:25.290
But authentication
is going to change.

00:15:25.290 --> 00:15:28.990
Right now, authentication
tends to be me authenticating

00:15:28.990 --> 00:15:32.551
to some object or service.

00:15:32.551 --> 00:15:34.920
What we're going to
see an explosion in

00:15:34.920 --> 00:15:38.500
is thing-to-thing
authentication, where objects

00:15:38.500 --> 00:15:41.020
seem to authentic to objects.

00:15:41.020 --> 00:15:42.520
And there's going
to be a lot of it.

00:15:42.520 --> 00:15:44.530
Imagine a driverless
car, or even

00:15:44.530 --> 00:15:47.930
some kind of
computer-assisted driving car.

00:15:47.930 --> 00:15:50.380
It will want to
authenticate to thousands

00:15:50.380 --> 00:15:54.340
of other cars, road
signs, emergency

00:15:54.340 --> 00:15:56.875
vehicles and signals--

00:15:56.875 --> 00:15:58.380
lots of things.

00:15:58.380 --> 00:16:00.780
And we don't know how
to do that at scale.

00:16:00.780 --> 00:16:07.050
Or you might have 100
IoT objects in your orbit

00:16:07.050 --> 00:16:08.580
to authenticate to each other.

00:16:08.580 --> 00:16:10.790
That's 10,000 authentications.

00:16:10.790 --> 00:16:13.140
1,000 objects, a
million authentications.

00:16:13.140 --> 00:16:17.620
Right now, this tends
to be our IoT hub.

00:16:17.620 --> 00:16:19.530
If you have an IoT
anything, you likely

00:16:19.530 --> 00:16:22.530
control it via your phone.

00:16:22.530 --> 00:16:27.180
I'm not sure that scales
to that many things.

00:16:27.180 --> 00:16:30.230
And while we can do
thing-to-thing authentication,

00:16:30.230 --> 00:16:33.880
it's very much deliberate.

00:16:33.880 --> 00:16:35.860
So right now, when
I get into my car,

00:16:35.860 --> 00:16:38.310
this phone authenticates
the car automatically.

00:16:38.310 --> 00:16:39.520
That works.

00:16:39.520 --> 00:16:40.600
Bluetooth works.

00:16:40.600 --> 00:16:44.230
But it works because I
was there to set it up.

00:16:44.230 --> 00:16:48.040
And I'll do that for 10
things, for 20 things.

00:16:48.040 --> 00:16:50.080
I'm not doing it for 1,000.

00:16:50.080 --> 00:16:52.780
I'm not doing it for a million.

00:16:52.780 --> 00:16:56.430
So we need some way to do
this automatic thing-to-thing

00:16:56.430 --> 00:17:00.850
authentication at scale,
and we don't have it.

00:17:00.850 --> 00:17:04.079
The third thing that's
failing is supply chain.

00:17:04.079 --> 00:17:08.910
Supply and chain security is
actually insurmountably hard.

00:17:08.910 --> 00:17:12.550
Now, we've seen-- you've seen
the papers in the past year.

00:17:12.550 --> 00:17:14.190
It's been one of two stories.

00:17:14.190 --> 00:17:15.750
It's Kaspersky.

00:17:15.750 --> 00:17:19.440
Should we trust a Russian-made
anti-virus program?

00:17:19.440 --> 00:17:21.599
And Huawei and NZTE.

00:17:21.599 --> 00:17:25.200
Should we trust Chinese-made
phone equipment?

00:17:25.200 --> 00:17:29.410
But that really is just
the tip of the iceberg.

00:17:29.410 --> 00:17:31.230
There are other stories,
not just the US.

00:17:31.230 --> 00:17:35.600
It turns out, in 2014,
China banned Kaspersky.

00:17:35.600 --> 00:17:38.770
They also banned
Symantec, by the way.

00:17:38.770 --> 00:17:43.100
2017, a story from India
identifying 45 Chinese phone

00:17:43.100 --> 00:17:46.130
apps that they say
shouldn't be used.

00:17:46.130 --> 00:17:48.780
In 1997-- I don't know
if people remember--

00:17:48.780 --> 00:17:51.970
there were worries in
the US about Checkpoint,

00:17:51.970 --> 00:17:55.220
an Israeli-made
security product.

00:17:55.220 --> 00:17:57.690
Should we trust it?

00:17:57.690 --> 00:18:01.200
Also, I like to remember
a 2008 program called

00:18:01.200 --> 00:18:06.120
Mujahideen Secrets, which was
an ISIS-created encryption

00:18:06.120 --> 00:18:09.750
program because, of course, you
can't trust Western encryption

00:18:09.750 --> 00:18:11.750
programs.

00:18:11.750 --> 00:18:15.410
But the country of
origin of the product

00:18:15.410 --> 00:18:18.440
is just the tip of the iceberg.

00:18:18.440 --> 00:18:20.440
Where are the chips made?

00:18:20.440 --> 00:18:22.700
Where is the software written?

00:18:22.700 --> 00:18:25.490
Where is the device
[? fabbed? ?]

00:18:25.490 --> 00:18:27.440
Where are the programmers from?

00:18:30.660 --> 00:18:32.700
This iPhone probably
has, what, a couple

00:18:32.700 --> 00:18:35.446
of hundred different
passports that

00:18:35.446 --> 00:18:36.570
are programming this thing?

00:18:36.570 --> 00:18:38.970
It's not made in the US.

00:18:38.970 --> 00:18:44.270
And every part of the
chain is a vulnerability.

00:18:44.270 --> 00:18:49.170
There are papers showing how you
can take a good chip design--

00:18:49.170 --> 00:18:54.465
the masks-- and maliciously
put in another layer,

00:18:54.465 --> 00:18:56.090
and compromise the
security of the chip

00:18:56.090 --> 00:18:57.740
without the
designers knowing it,

00:18:57.740 --> 00:19:00.950
and it doesn't
show up in testing.

00:19:00.950 --> 00:19:03.070
There was another paper
about two years ago.

00:19:03.070 --> 00:19:06.460
You can hack an iPhone through
a malicious replacement screen.

00:19:09.130 --> 00:19:14.510
You have to trust every
piece of the system.

00:19:14.510 --> 00:19:17.960
The distribution mechanisms.

00:19:17.960 --> 00:19:22.900
We've seen backdoors
in Cisco equipment.

00:19:22.900 --> 00:19:27.680
Remember the NSA
intercepted Cisco's routers

00:19:27.680 --> 00:19:29.960
being sent to the Syrian
telephone company?

00:19:29.960 --> 00:19:32.084
That was one of the great
pictures from the Snowden

00:19:32.084 --> 00:19:33.030
documents.

00:19:33.030 --> 00:19:37.390
We've seen fake apps in
the Google Play Store.

00:19:37.390 --> 00:19:41.710
We know that Russia attacked
Ukraine through a software

00:19:41.710 --> 00:19:44.170
update mechanism.

00:19:44.170 --> 00:19:46.340
I think my favorite story--

00:19:46.340 --> 00:19:47.290
this is a hard one.

00:19:47.290 --> 00:19:52.270
2003, there was actually a very
clever, very subtle backdoor

00:19:52.270 --> 00:19:55.150
that almost made it into Linux.

00:19:55.150 --> 00:19:58.390
We caught it, and we kind
of just barely caught it.

00:19:58.390 --> 00:20:01.350
We got very lucky.

00:20:01.350 --> 00:20:03.690
And you look at the code.

00:20:03.690 --> 00:20:04.660
It really is hard.

00:20:04.660 --> 00:20:08.110
You have to look for
it to see the backdoor.

00:20:08.110 --> 00:20:09.810
That could have
easily gotten in.

00:20:09.810 --> 00:20:13.690
We don't know what else
has gotten in, in what.

00:20:13.690 --> 00:20:15.810
And solving this is hard.

00:20:15.810 --> 00:20:17.910
No one wants a US-only iPhone.

00:20:17.910 --> 00:20:22.290
It's probably, A, impossible
and, B, it'll cost 10x.

00:20:22.290 --> 00:20:26.370
Our industry is, at every
level, international.

00:20:26.370 --> 00:20:29.910
It is deeply international.

00:20:29.910 --> 00:20:34.080
From the programmers, to
the objects, to the cloud,

00:20:34.080 --> 00:20:36.300
the services.

00:20:36.300 --> 00:20:41.120
We will not be able
to solve this easily.

00:20:41.120 --> 00:20:44.170
So in a lot of ways,
this is a perfect storm.

00:20:44.170 --> 00:20:46.900
Things are failing
just as everything

00:20:46.900 --> 00:20:49.510
is becoming interconnected.

00:20:49.510 --> 00:20:53.850
And I think we've been OK
with a unregulated tech space

00:20:53.850 --> 00:20:59.480
because it fundamentally didn't
matter, and that's changing.

00:20:59.480 --> 00:21:03.220
And I think this is
primarily a policy problem.

00:21:03.220 --> 00:21:08.300
And in my book, I spend
most of the time on policy,

00:21:08.300 --> 00:21:10.910
and I talk about a lot of
different policy levers

00:21:10.910 --> 00:21:13.580
we have to improve this.

00:21:13.580 --> 00:21:17.450
I talk about standards,
regulations, liabilities,

00:21:17.450 --> 00:21:20.240
courts, international treaties.

00:21:20.240 --> 00:21:22.917
The things is, it's a very
hard political battle.

00:21:22.917 --> 00:21:24.500
And I don't think
we're going to have,

00:21:24.500 --> 00:21:28.360
in the US, a total
catastrophic event.

00:21:28.360 --> 00:21:31.300
I look more to Europe to lead.

00:21:31.300 --> 00:21:33.040
I could go through
all of this, but I

00:21:33.040 --> 00:21:36.460
want to give two principles
that I want to pull out.

00:21:36.460 --> 00:21:39.010
The first is that
defense must dominate.

00:21:39.010 --> 00:21:41.530
I think we, as a
national policy,

00:21:41.530 --> 00:21:45.190
need to decide
that defense wins.

00:21:45.190 --> 00:21:48.520
That no longer can
we accept insecurity

00:21:48.520 --> 00:21:50.420
for offense purposes.

00:21:50.420 --> 00:21:53.950
That, as these computers
become more critical,

00:21:53.950 --> 00:21:56.790
defense is more important.

00:21:56.790 --> 00:21:59.030
Gone are the days when
you can attack their stuff

00:21:59.030 --> 00:22:00.350
and defend our stuff.

00:22:00.350 --> 00:22:02.390
Everyone uses the same stuff.

00:22:02.390 --> 00:22:06.590
We all use TCP/IP and Cisco
routers, and Microsoft Word,

00:22:06.590 --> 00:22:09.450
and PDF files.

00:22:09.450 --> 00:22:12.840
And it's just one world,
one network, one answer.

00:22:12.840 --> 00:22:17.190
Either we secure our stuff,
thereby, incidentally,

00:22:17.190 --> 00:22:19.620
securing the bad guys' stuff.

00:22:19.620 --> 00:22:21.480
Or we keep our stuff
vulnerable in order

00:22:21.480 --> 00:22:25.080
to attack the bad guys,
thereby, incidentally,

00:22:25.080 --> 00:22:26.474
rendering us vulnerable.

00:22:26.474 --> 00:22:27.390
And that's our choice.

00:22:30.270 --> 00:22:32.970
And it means a whole
bunch of things.

00:22:32.970 --> 00:22:35.490
To disclose and fix
vulnerabilities;

00:22:35.490 --> 00:22:38.220
to design for security,
not for surveillance;

00:22:38.220 --> 00:22:42.690
encrypt as much as
possible; to really separate

00:22:42.690 --> 00:22:47.070
security from spying; make
law enforcement smarter

00:22:47.070 --> 00:22:48.720
so they can actually
solve crimes

00:22:48.720 --> 00:22:55.220
even though there is security;
and create better norms.

00:22:55.220 --> 00:22:59.090
One other principle is that we
need to build for resilience.

00:22:59.090 --> 00:23:00.620
We need to start
designing systems

00:23:00.620 --> 00:23:03.240
assuming they will fail.

00:23:03.240 --> 00:23:04.910
And how do we contain failures?

00:23:04.910 --> 00:23:07.100
How do we avoid catastrophes?

00:23:07.100 --> 00:23:10.230
How do we fail safe,
or fail secure?

00:23:10.230 --> 00:23:13.860
Where can we remove
functionality or delete data?

00:23:13.860 --> 00:23:18.300
How do we have systems monitor
other systems to try to provide

00:23:18.300 --> 00:23:22.020
some level of redundancy?

00:23:22.020 --> 00:23:23.640
And I think the
missing piece here

00:23:23.640 --> 00:23:28.420
is government, that the market
will not do this on its own.

00:23:30.930 --> 00:23:34.900
But I have a problem
handing this to government

00:23:34.900 --> 00:23:37.960
because there really isn't an
existing regulatory structure

00:23:37.960 --> 00:23:40.440
that could tackle this
at a systemic level.

00:23:40.440 --> 00:23:42.640
That's because there's a
mismatch between the way

00:23:42.640 --> 00:23:45.250
government works and
the way tech works.

00:23:45.250 --> 00:23:47.320
Government operates in silos.

00:23:47.320 --> 00:23:52.660
The FAA regulates aircraft, the
FDA regulates medical devices.

00:23:52.660 --> 00:23:55.120
The FTC regulates
consumer goods.

00:23:55.120 --> 00:23:56.875
Someone else does cars.

00:23:59.500 --> 00:24:02.500
And each agency will have its
own rules, and own approach,

00:24:02.500 --> 00:24:04.450
and own systems.

00:24:04.450 --> 00:24:06.070
And that's not the internet.

00:24:06.070 --> 00:24:08.680
The internet is this
free-wheeling system

00:24:08.680 --> 00:24:11.200
of integrated
objects and networks,

00:24:11.200 --> 00:24:14.650
and it grows horizontally,
and it kicks down barriers.

00:24:14.650 --> 00:24:17.770
And it makes people
able to do things

00:24:17.770 --> 00:24:19.690
they never could do before.

00:24:19.690 --> 00:24:22.650
And all of that
rhetoric is true.

00:24:22.650 --> 00:24:29.440
Right now, this device
logs my health information,

00:24:29.440 --> 00:24:35.540
communicates with my car,
monitors my energy use,

00:24:35.540 --> 00:24:37.820
and makes phone calls.

00:24:37.820 --> 00:24:39.950
That's four different--
probably five

00:24:39.950 --> 00:24:41.861
different regulatory agencies.

00:24:41.861 --> 00:24:43.235
And this is just
getting started.

00:24:46.540 --> 00:24:50.250
We're not sure how to do this.

00:24:50.250 --> 00:24:53.600
So in my book, I talk
about a bunch of options.

00:24:53.600 --> 00:24:59.240
And what I have, and I think
we're going to get eventually,

00:24:59.240 --> 00:25:05.210
is a new government agency that
will have some jurisdiction

00:25:05.210 --> 00:25:07.490
over computers.

00:25:07.490 --> 00:25:12.120
This is a hard sell to
a low-government crowd.

00:25:12.120 --> 00:25:14.330
But there is a lot of
precedent for this.

00:25:14.330 --> 00:25:17.450
In the last century, pretty
much all major technologies

00:25:17.450 --> 00:25:19.880
led to the formation of
new government agencies.

00:25:19.880 --> 00:25:26.200
Cars did, planes did, radio
did, nuclear power did.

00:25:26.200 --> 00:25:29.470
Because government needs to
consolidate its expertise.

00:25:32.190 --> 00:25:34.590
And that's what happens
first, and then there

00:25:34.590 --> 00:25:37.990
is need to regulate.

00:25:37.990 --> 00:25:39.910
I don't think
markets solve this.

00:25:39.910 --> 00:25:43.000
Markets are short-term,
markets are profit-motivated.

00:25:43.000 --> 00:25:45.760
Markets don't take
society into account.

00:25:45.760 --> 00:25:51.090
Markets can't solve
collective action problems.

00:25:51.090 --> 00:25:54.370
So of course, there are
lots of problems with this.

00:25:54.370 --> 00:25:58.480
Governments are terrible
at being proactive.

00:25:58.480 --> 00:26:01.030
Regulatory capture
is a real issue.

00:26:03.660 --> 00:26:06.590
I think there are differences
between security and safety

00:26:06.590 --> 00:26:08.200
that matter here.

00:26:08.200 --> 00:26:10.670
Safety against things like
a hurricane, and security

00:26:10.670 --> 00:26:14.310
against an adaptive, malicious,
intelligent adversary

00:26:14.310 --> 00:26:17.104
are very different things.

00:26:17.104 --> 00:26:21.260
And we live in a fast-moving,
technological environment.

00:26:21.260 --> 00:26:25.040
And it's hard to see how
government can stay ahead

00:26:25.040 --> 00:26:26.635
of tech.

00:26:26.635 --> 00:26:29.260
This is something that's changed
in the past couple of decades.

00:26:29.260 --> 00:26:32.930
Tech moves faster than policy.

00:26:32.930 --> 00:26:36.920
The devil's in the details,
and I don't have them.

00:26:36.920 --> 00:26:41.260
But this is a conversation
I think we need to have.

00:26:41.260 --> 00:26:43.540
Because I believe that
governments will get

00:26:43.540 --> 00:26:46.190
involved regardless.

00:26:46.190 --> 00:26:49.760
The risks are too great and
the stakes are too high.

00:26:49.760 --> 00:26:53.240
Governments are already
involved in physical systems.

00:26:53.240 --> 00:26:58.760
They already regulate cars
and appliances and toys

00:26:58.760 --> 00:27:04.510
and power plants
and medical systems.

00:27:04.510 --> 00:27:10.150
So they already have this
ability and need and desire

00:27:10.150 --> 00:27:14.050
to regulate those
things as computers.

00:27:14.050 --> 00:27:17.140
But how do we give them the
expertise to do it right?

00:27:17.140 --> 00:27:21.700
My guess is the courts are going
to do some things relatively

00:27:21.700 --> 00:27:24.840
quickly, because
cases will appear,

00:27:24.840 --> 00:27:28.850
and that the regulatory
agencies will follow.

00:27:28.850 --> 00:27:33.160
I think Congress comes last,
but don't count them out.

00:27:33.160 --> 00:27:38.120
Nothing motivates the
US government like fear.

00:27:38.120 --> 00:27:40.970
Think back to the terrorist
acts of September 11.

00:27:40.970 --> 00:27:44.630
We had a very small
government administration

00:27:44.630 --> 00:27:49.060
create a massive
bureaucracy out of thin air.

00:27:49.060 --> 00:27:52.430
And that was all fear-motivated.

00:27:52.430 --> 00:27:54.610
And when something
happens, there

00:27:54.610 --> 00:27:58.920
will be a push that
something must be done.

00:27:58.920 --> 00:28:02.420
And we are past the choice
of government involvement

00:28:02.420 --> 00:28:03.950
versus no government
involvement.

00:28:03.950 --> 00:28:06.590
Our choice now is smart
government involvement

00:28:06.590 --> 00:28:08.630
versus stupid
government involvement.

00:28:08.630 --> 00:28:11.120
And the more we can
talk about this now,

00:28:11.120 --> 00:28:14.920
the more we can make
sure it's smart.

00:28:14.920 --> 00:28:19.200
My guess is any good regulation
will incent private industry.

00:28:19.200 --> 00:28:22.070
But I think the reason we
have such bad security is not

00:28:22.070 --> 00:28:23.300
technological.

00:28:23.300 --> 00:28:25.820
It's more economic.

00:28:25.820 --> 00:28:27.800
There's lots of good tech.

00:28:27.800 --> 00:28:31.730
And while some of these
problems are hard,

00:28:31.730 --> 00:28:33.710
they're "send a man
to the moon" hard,

00:28:33.710 --> 00:28:37.500
they're not "faster-than-light
travel" hard.

00:28:37.500 --> 00:28:40.900
And once the incentives
are in place,

00:28:40.900 --> 00:28:42.840
industry will figure
out how to do it right.

00:28:42.840 --> 00:28:46.200
A good example might
be credit cards.

00:28:46.200 --> 00:28:47.760
In the early days
of credit cards,

00:28:47.760 --> 00:28:51.610
we were all liable
for fraud and losses.

00:28:51.610 --> 00:28:55.000
That changed in 1978, the
Fair Credit Reporting Act.

00:28:55.000 --> 00:28:57.820
That's what mandated that the
maximum liability for credit

00:28:57.820 --> 00:29:01.042
card fraud for the
consumer is $50.

00:29:01.042 --> 00:29:02.500
And you understand
what that means.

00:29:02.500 --> 00:29:04.416
That means I could
take my card, fling it

00:29:04.416 --> 00:29:06.210
into the middle of
this room, give you

00:29:06.210 --> 00:29:08.890
all lessons on
forging my signature,

00:29:08.890 --> 00:29:11.420
and my maximum liability is $50.

00:29:11.420 --> 00:29:13.000
It might be worth
it for the fun.

00:29:13.000 --> 00:29:16.270
[LAUGHTER]

00:29:16.270 --> 00:29:17.830
But what that meant--

00:29:17.830 --> 00:29:21.340
that change, that even if
the consumer is at fault,

00:29:21.340 --> 00:29:24.990
the credit card
company is liable,

00:29:24.990 --> 00:29:28.800
that led to all sorts
of security measures.

00:29:28.800 --> 00:29:34.800
That led to online verification
of credit and card validity.

00:29:34.800 --> 00:29:38.250
That led to
anti-forgery measures,

00:29:38.250 --> 00:29:40.860
like the holograms and
the micro-printing.

00:29:40.860 --> 00:29:44.340
That led to mailing the card,
and the activation information

00:29:44.340 --> 00:29:48.200
separately, and requiring you to
call from a known phone number.

00:29:48.200 --> 00:29:49.700
And actually, most
importantly, that

00:29:49.700 --> 00:29:51.800
enabled the back
end expert systems

00:29:51.800 --> 00:29:56.480
that troll the transaction
database looking

00:29:56.480 --> 00:29:58.340
at fraudulent spending patterns.

00:29:58.340 --> 00:30:02.720
None of that would have happened
if the consumers were liable.

00:30:02.720 --> 00:30:05.750
Because the consumers had
no ability to implement

00:30:05.750 --> 00:30:07.190
any of that.

00:30:07.190 --> 00:30:10.250
You want the entity
that can fix the problem

00:30:10.250 --> 00:30:12.140
to be responsible
for the problem.

00:30:12.140 --> 00:30:15.190
That is just smart policy.

00:30:15.190 --> 00:30:17.290
So I see a lot of
innovation that's

00:30:17.290 --> 00:30:21.450
not happening because the
incentives are mismatched.

00:30:21.450 --> 00:30:25.070
So I think Europe is
moving in this direction.

00:30:25.070 --> 00:30:27.830
The EU is, right now,
the regulatory superpower

00:30:27.830 --> 00:30:28.980
on the planet.

00:30:28.980 --> 00:30:31.610
And they are not afraid
to use their power.

00:30:31.610 --> 00:30:34.580
We've seen that in the
GDPR in the privacy space.

00:30:34.580 --> 00:30:38.140
I think they're going to
turn to security next.

00:30:38.140 --> 00:30:43.250
They're already working on what
responsible disclosure means.

00:30:43.250 --> 00:30:44.870
You ever see, on
manufactured goods,

00:30:44.870 --> 00:30:46.910
there's a label called CE?

00:30:46.910 --> 00:30:48.050
That's an EU label.

00:30:48.050 --> 00:30:53.390
It basically means "meets
all applicable standards."

00:30:53.390 --> 00:30:57.130
They're working on
standards for cybersecurity.

00:30:57.130 --> 00:30:59.980
And you will see them get
incorporated into trade

00:30:59.980 --> 00:31:03.050
agreements, into GATT.

00:31:03.050 --> 00:31:08.680
And there's an interesting
rising tide effect.

00:31:08.680 --> 00:31:10.570
It's not necessarily obvious.

00:31:10.570 --> 00:31:12.070
The car you buy in
the United States

00:31:12.070 --> 00:31:13.485
is not the car
you buy in Mexico.

00:31:13.485 --> 00:31:14.860
Environmental laws
are different,

00:31:14.860 --> 00:31:16.810
and the cars are tuned
to the different laws.

00:31:19.950 --> 00:31:23.100
Not true in the computer space.

00:31:23.100 --> 00:31:28.640
The Facebook you get is pretty
much the same everywhere.

00:31:28.640 --> 00:31:30.550
And if you can imagine
there is some security

00:31:30.550 --> 00:31:33.095
regulation on a toy, the
manufacturer meets it,

00:31:33.095 --> 00:31:35.470
they're not going to have a
separate build for the United

00:31:35.470 --> 00:31:35.740
States.

00:31:35.740 --> 00:31:38.073
They're going to sell it
everywhere because it's easier.

00:31:41.730 --> 00:31:43.806
There'll be times
when that's not true.

00:31:43.806 --> 00:31:45.180
I think Facebook
would like to be

00:31:45.180 --> 00:31:49.710
able to differentiate between
someone who is subject to GDPR,

00:31:49.710 --> 00:31:50.694
someone who is not.

00:31:50.694 --> 00:31:52.110
Because there's
more revenue to be

00:31:52.110 --> 00:31:55.110
gained through the
greater surveillance.

00:31:55.110 --> 00:31:56.700
But when you get
to things, I think

00:31:56.700 --> 00:32:02.010
it's more likely that it'll be a
rising tide and we all benefit.

00:32:02.010 --> 00:32:05.100
United States look to the
states more, specifically

00:32:05.100 --> 00:32:07.750
New York, Massachusetts,
California,

00:32:07.750 --> 00:32:10.390
which are more
aggressive in this space.

00:32:13.395 --> 00:32:14.520
But I think this is coming.

00:32:18.000 --> 00:32:23.920
And I want to close
with, I guess, a call.

00:32:23.920 --> 00:32:29.150
What we need to do is to
get involved in policy.

00:32:29.150 --> 00:32:33.020
Technologists need to
get involved in policy.

00:32:33.020 --> 00:32:36.440
As internet security
becomes everything security,

00:32:36.440 --> 00:32:38.420
internet security
technology becomes

00:32:38.420 --> 00:32:42.290
more important to
overall security policy.

00:32:42.290 --> 00:32:45.410
And all of the
security policy debates

00:32:45.410 --> 00:32:48.870
will have strong
technological components.

00:32:48.870 --> 00:32:51.570
We will never get
the policy right

00:32:51.570 --> 00:32:54.390
if the policymakers
get the tech wrong.

00:32:54.390 --> 00:32:58.060
It will all look like
the Facebook hearings,

00:32:58.060 --> 00:33:00.390
which were embarrassing.

00:33:00.390 --> 00:33:03.180
And you see it even in some--
you see it in the going dark

00:33:03.180 --> 00:33:06.090
debate, you see it in
the equities debate,

00:33:06.090 --> 00:33:08.640
you see it in voting
machine debates,

00:33:08.640 --> 00:33:12.010
in driverless car
security debates.

00:33:12.010 --> 00:33:15.250
That we need
technologists in the room

00:33:15.250 --> 00:33:17.590
during policy discussions.

00:33:17.590 --> 00:33:20.040
We have to fix this.

00:33:20.040 --> 00:33:22.500
We need technologists
on congressional staffs,

00:33:22.500 --> 00:33:26.550
at NGOs, doing investigative
journalism, in the government

00:33:26.550 --> 00:33:28.380
agencies, in the White House.

00:33:32.370 --> 00:33:35.110
We need to make this happen.

00:33:35.110 --> 00:33:39.870
And right now, you just
don't have that ecosystem.

00:33:39.870 --> 00:33:43.280
So if you think about
public interest law, 1970s,

00:33:43.280 --> 00:33:45.380
there was no such thing
as public interest law.

00:33:45.380 --> 00:33:46.940
There actually wasn't.

00:33:46.940 --> 00:33:51.200
It was created primarily by the
Ford Foundation, oddly enough,

00:33:51.200 --> 00:33:57.500
that funded law clinics, funded
internships in different NGOs.

00:33:57.500 --> 00:34:01.090
And now, you want to make
partner at a major law firm,

00:34:01.090 --> 00:34:03.130
you are expected to do
public interest work.

00:34:06.120 --> 00:34:11.880
Today at Harvard Law School,
20% of the graduating class

00:34:11.880 --> 00:34:14.790
doesn't go into
corporations or law firms.

00:34:14.790 --> 00:34:16.860
They go into public
interest law.

00:34:16.860 --> 00:34:19.679
And the university has
soul-searching seminars

00:34:19.679 --> 00:34:23.060
because that
percentage is so low.

00:34:23.060 --> 00:34:27.800
Percentage of computer science
graduates is probably zero.

00:34:27.800 --> 00:34:29.090
We need to fix that.

00:34:29.090 --> 00:34:32.929
And that's more than
just every Googler needs

00:34:32.929 --> 00:34:34.429
to do an internship,
because there

00:34:34.429 --> 00:34:37.420
aren't spaces for those people.

00:34:37.420 --> 00:34:40.540
So we got to fix the supply,
got to fix the demand,

00:34:40.540 --> 00:34:43.199
the ecosystem to link the two.

00:34:43.199 --> 00:34:45.104
This is, of course,
bigger than security.

00:34:45.104 --> 00:34:48.690
I think, pretty much, all
the major societal problems

00:34:48.690 --> 00:34:51.659
of this century have a
strong tech component--

00:34:51.659 --> 00:34:54.930
climate change, future
of work, foreign policy.

00:34:58.010 --> 00:35:06.190
And we need to be in the room,
or bad policy happens to us.

00:35:06.190 --> 00:35:07.300
So that's my talk.

00:35:07.300 --> 00:35:10.180
There's, of course, a lot more
in the book that I didn't say,

00:35:10.180 --> 00:35:13.368
and I'm happy to take questions.

00:35:13.368 --> 00:35:17.706
[APPLAUSE]

00:35:23.972 --> 00:35:24.960
AUDIENCE: Hi.

00:35:24.960 --> 00:35:30.570
Do you imagine that some of
the sociopolitical things

00:35:30.570 --> 00:35:33.750
that we're seeing crop up
fit within this framework?

00:35:33.750 --> 00:35:37.620
Or do you think that that might
be an entirely separate problem

00:35:37.620 --> 00:35:40.260
that needs an entirely
separate set of solutions?

00:35:40.260 --> 00:35:41.820
BRUCE SCHNEIER: I
think it's related.

00:35:41.820 --> 00:35:44.920
The problems I'm talking about
are pretty purely technical.

00:35:44.920 --> 00:35:49.590
The problems of internet
as a propaganda vehicle

00:35:49.590 --> 00:35:53.250
are, I think, much more
systemic and societal.

00:35:53.250 --> 00:35:57.030
I do blame surveillance
capitalism for a bunch of it.

00:35:57.030 --> 00:36:00.900
The business model that
prioritizes engagement,

00:36:00.900 --> 00:36:05.010
rather than quality, has learned
that, if you're pissed off,

00:36:05.010 --> 00:36:07.720
you stay on Facebook more.

00:36:07.720 --> 00:36:11.580
So I think there are
pieces that fit in.

00:36:11.580 --> 00:36:13.745
So some related, some different.

00:36:13.745 --> 00:36:15.870
AUDIENCE: You seem to be
talking a lot about policy

00:36:15.870 --> 00:36:18.549
that the United States and,
to some extent, the EU can do.

00:36:18.549 --> 00:36:20.340
But I wonder, what do
you think will happen

00:36:20.340 --> 00:36:22.950
as policy everywhere is--

00:36:22.950 --> 00:36:25.112
policy is local, the
internet is global.

00:36:25.112 --> 00:36:26.320
How's that going to play out?

00:36:26.320 --> 00:36:28.770
BRUCE SCHNEIER: So I think
that never goes away.

00:36:28.770 --> 00:36:31.740
And some of it's going to be
the rising tide I talked about

00:36:31.740 --> 00:36:33.510
that-- especially when--

00:36:33.510 --> 00:36:36.910
less about privacy, but
when you get to safety,

00:36:36.910 --> 00:36:39.720
I think it's more
likely that we benefit

00:36:39.720 --> 00:36:45.330
from a European regulation that
ensures that the smart vacuum

00:36:45.330 --> 00:36:47.910
cleaner you bought can't
be taken over by somebody,

00:36:47.910 --> 00:36:50.520
and then attack
you and trip you.

00:36:50.520 --> 00:36:53.130
We're likely to benefit
from that more than, look,

00:36:53.130 --> 00:36:57.750
you can't have a
microphone on the thing.

00:36:57.750 --> 00:37:01.920
We have to assume that there
will be malicious things

00:37:01.920 --> 00:37:04.510
in whatever system we have.

00:37:04.510 --> 00:37:06.229
So if we have a
US-only regulation,

00:37:06.229 --> 00:37:07.770
it will clean up a
lot of the problem

00:37:07.770 --> 00:37:10.590
because Walmart won't be
able to sell the bad stuff.

00:37:10.590 --> 00:37:13.590
But you can still buy it and
mail order from Alibaba.com.

00:37:13.590 --> 00:37:17.550
So there will be some stuff in
the network that is malicious.

00:37:17.550 --> 00:37:20.080
Much lower percentage,
easier problem.

00:37:20.080 --> 00:37:22.000
We're still going to
have to deal with that.

00:37:22.000 --> 00:37:24.450
And I don't think it ever
goes away because we're not

00:37:24.450 --> 00:37:25.741
going to have world government.

00:37:25.741 --> 00:37:28.200
There will be a
jurisdiction, or there

00:37:28.200 --> 00:37:31.200
will be homebrew
stuff that doesn't

00:37:31.200 --> 00:37:32.860
meet whatever regs we have.

00:37:32.860 --> 00:37:34.085
That will always happen.

00:37:34.085 --> 00:37:35.460
AUDIENCE: You talk
about the need

00:37:35.460 --> 00:37:39.381
for intelligent technologists to
get involved in making policy,

00:37:39.381 --> 00:37:41.130
but there are only so
many hours in a day,

00:37:41.130 --> 00:37:44.250
and probably most of us
would be taking a huge pay

00:37:44.250 --> 00:37:47.230
cut to go work in government
and lend our expertise there.

00:37:47.230 --> 00:37:49.680
So how can we fix
the incentives there?

00:37:49.680 --> 00:37:51.540
BRUCE SCHNEIER: Some
of it is desire.

00:37:51.540 --> 00:37:53.790
I know ACLU attorneys
that are making

00:37:53.790 --> 00:37:57.090
1/3 of what they would
make at a big law firm,

00:37:57.090 --> 00:38:00.360
and they get more resumes
than they have positions.

00:38:00.360 --> 00:38:02.130
So it works in law.

00:38:02.130 --> 00:38:05.310
The desire to actually
make the world better

00:38:05.310 --> 00:38:07.330
turns out to be a
prime motivator.

00:38:07.330 --> 00:38:09.690
So I think, once we
have the ecosystem,

00:38:09.690 --> 00:38:11.910
we will get the supply.

00:38:11.910 --> 00:38:15.210
I think that enough of us will
say, we've had great careers.

00:38:15.210 --> 00:38:17.450
We're going to take a break.

00:38:17.450 --> 00:38:20.000
Or we're going to do
something before we go work

00:38:20.000 --> 00:38:22.730
at a startup or a big company.

00:38:22.730 --> 00:38:26.000
Or maybe, there will be
a use for sabbaticals,

00:38:26.000 --> 00:38:30.440
like you see in law firms
or bits of pro bono work,

00:38:30.440 --> 00:38:32.720
like a 20% project.

00:38:32.720 --> 00:38:35.990
So yes, people will
be making less money.

00:38:35.990 --> 00:38:40.842
I don't think that is
going to harm the system.

00:38:40.842 --> 00:38:42.800
I think we just need to
get the system working.

00:38:42.800 --> 00:38:46.640
AUDIENCE: The most jarring thing
I saw you write, as a Googler,

00:38:46.640 --> 00:38:51.637
was that data is a toxic asset.

00:38:51.637 --> 00:38:53.470
What do you say about
this to this audience?

00:38:53.470 --> 00:38:53.578
BRUCE SCHNEIER: I know.

00:38:53.578 --> 00:38:54.786
What does that make you guys?

00:38:54.786 --> 00:38:56.013
[LAUGHTER]

00:38:59.450 --> 00:39:02.270
The promise of big
data has been save it

00:39:02.270 --> 00:39:04.580
all, figure out what
to do with it later.

00:39:04.580 --> 00:39:08.390
And that's been driven by
the marginal cost of saving

00:39:08.390 --> 00:39:10.970
it has dropped to zero.

00:39:10.970 --> 00:39:12.950
It's basically cheaper
now to save it all than

00:39:12.950 --> 00:39:14.756
to figure out what to save.

00:39:14.756 --> 00:39:19.520
Disk storage is free, processing
is free, transport is free.

00:39:19.520 --> 00:39:24.170
But it turns out that
data is a toxic asset.

00:39:24.170 --> 00:39:27.950
For most companies, having
it is an enormous liability

00:39:27.950 --> 00:39:29.789
because someone is
going to hack it.

00:39:29.789 --> 00:39:30.830
It's going to get stolen.

00:39:30.830 --> 00:39:32.760
You're going to lose it.

00:39:32.760 --> 00:39:36.110
And I think we need to
start talking about data not

00:39:36.110 --> 00:39:43.940
as this magic goodness,
but it decays in value

00:39:43.940 --> 00:39:46.190
and there are dangers
in storing it.

00:39:46.190 --> 00:39:49.430
The best way to secure
your data is to delete it.

00:39:49.430 --> 00:39:54.520
And you're going to delete it
if you know you don't need it.

00:39:54.520 --> 00:39:58.830
So I've seen lots of studies on
data and shopping preferences.

00:39:58.830 --> 00:40:01.740
And it turns out, some pieces
of data are very valuable,

00:40:01.740 --> 00:40:04.030
and a lot of it just
isn't very valuable.

00:40:04.030 --> 00:40:07.290
So is it worth the
extra 0.25% of accuracy

00:40:07.290 --> 00:40:10.020
to have this data that
is potentially dangerous,

00:40:10.020 --> 00:40:12.030
and will get you
fined or embarrassed,

00:40:12.030 --> 00:40:14.940
and stock takes a hit
if it gets stolen?

00:40:14.940 --> 00:40:17.280
So I think we need to make
more of those decisions.

00:40:17.280 --> 00:40:19.020
That the data is radioactive.

00:40:19.020 --> 00:40:20.250
It's toxic.

00:40:20.250 --> 00:40:22.824
We keep it if we need to.

00:40:22.824 --> 00:40:24.240
But if we don't,
we get rid of it,

00:40:24.240 --> 00:40:26.698
and we figure out how to get
rid of it safely and securely.

00:40:29.450 --> 00:40:31.100
Take, I don't know, Waze.

00:40:31.100 --> 00:40:36.100
Waze is a surveillance-based
system, very personal data.

00:40:36.100 --> 00:40:39.520
But probably only valuable
for, like, 10 minutes.

00:40:39.520 --> 00:40:42.770
Or at least can be sampled.

00:40:42.770 --> 00:40:46.540
I mean, lots of ways I can
treat that data, understanding

00:40:46.540 --> 00:40:50.240
it's a toxic asset,
get my value at much

00:40:50.240 --> 00:40:53.180
less risk to my organization.

00:40:53.180 --> 00:40:54.580
And that's what I mean by that.

00:40:54.580 --> 00:40:56.290
AUDIENCE: It's
interesting that there

00:40:56.290 --> 00:40:59.740
are ways to anonymize
stuff, but there seems

00:40:59.740 --> 00:41:03.010
to be no demand and no supply.

00:41:03.010 --> 00:41:06.820
It's marginally more expensive
to do federated machine

00:41:06.820 --> 00:41:08.620
learning than do
everything in the center,

00:41:08.620 --> 00:41:13.179
but companies don't care, and
consumers decidedly don't care.

00:41:13.179 --> 00:41:14.720
BRUCE SCHNEIER:
Consumers don't care.

00:41:14.720 --> 00:41:17.830
That's why-- you need
these decisions made not

00:41:17.830 --> 00:41:20.390
by consumers, but by citizens.

00:41:20.390 --> 00:41:21.580
Consumers don't care.

00:41:21.580 --> 00:41:25.616
Consumers are buying
the Big Mac at 10% off.

00:41:25.616 --> 00:41:26.740
Consumers truly don't care.

00:41:26.740 --> 00:41:29.260
At the point of
purchase, nobody cares.

00:41:29.260 --> 00:41:32.560
At the point of reflection,
people care a lot.

00:41:32.560 --> 00:41:36.050
And that's why you don't
want the market doing this.

00:41:36.050 --> 00:41:41.970
You want us, as our
best selves, doing this.

00:41:41.970 --> 00:41:45.940
And about anonymity, it
is harder than you think.

00:41:45.940 --> 00:41:50.940
Most of our ways of
anonymizing data fails.

00:41:50.940 --> 00:41:54.810
It is a very hard problem.

00:41:54.810 --> 00:41:57.589
The anonymity
research is really--

00:41:57.589 --> 00:41:59.130
actually, breaking
anonymity research

00:41:59.130 --> 00:42:00.620
is very good these days.

00:42:00.620 --> 00:42:03.000
And outstripping the
anonymity research.

00:42:03.000 --> 00:42:03.900
Go for the next.

00:42:03.900 --> 00:42:05.010
AUDIENCE: One of the things
that I'm thinking about

00:42:05.010 --> 00:42:07.750
is, a lot of times, when you
see a big vulnerability-- so

00:42:07.750 --> 00:42:10.050
say there's a big operating
system vulnerability-- it's

00:42:10.050 --> 00:42:11.550
actually a genuine mistake.

00:42:11.550 --> 00:42:14.265
It's not that someone put
it in there on purpose.

00:42:14.265 --> 00:42:15.570
It's they missed something.

00:42:15.570 --> 00:42:17.765
So how does regulation
solve that problem?

00:42:17.765 --> 00:42:19.890
Sure, you could have some
great regulation in place

00:42:19.890 --> 00:42:21.610
that something's supposed
to be done a certain way.

00:42:21.610 --> 00:42:23.901
But oh, the implementation
was slightly off or slightly

00:42:23.901 --> 00:42:24.720
broken.

00:42:24.720 --> 00:42:27.651
How do you fix that
genuine mistake,

00:42:27.651 --> 00:42:30.150
even if they were trying to do
what the regulation specified

00:42:30.150 --> 00:42:31.890
as this would be
a secure system?

00:42:31.890 --> 00:42:34.800
BRUCE SCHNEIER: So you'd
be surprised, but financial

00:42:34.800 --> 00:42:36.210
motive--

00:42:36.210 --> 00:42:38.370
money motivates companies.

00:42:38.370 --> 00:42:40.800
If companies will be
fined a lot of money

00:42:40.800 --> 00:42:42.570
if their employees
make a mistake,

00:42:42.570 --> 00:42:43.650
they figure out ways
for their employees

00:42:43.650 --> 00:42:44.760
to make fewer mistakes.

00:42:44.760 --> 00:42:46.801
AUDIENCE: But doesn't that
only take effect after

00:42:46.801 --> 00:42:48.647
the mistake has already been--

00:42:48.647 --> 00:42:50.480
the evil has already
been done, essentially,

00:42:50.480 --> 00:42:51.050
when a mistake is found.

00:42:51.050 --> 00:42:52.710
BRUCE SCHNEIER: Initially, but
there's a deterrence effect.

00:42:52.710 --> 00:42:53.251
AUDIENCE: OK.

00:42:53.251 --> 00:42:54.420
BRUCE SCHNEIER: So yes.

00:42:54.420 --> 00:42:56.820
Arresting someone for
murder only takes effect

00:42:56.820 --> 00:42:59.160
after he's done
murder, but the goal

00:42:59.160 --> 00:43:01.950
is that the threat of
being arrested for murder

00:43:01.950 --> 00:43:05.000
will keep you from
murdering someone tomorrow.

00:43:05.000 --> 00:43:08.760
So we want this
deterrence effect.

00:43:08.760 --> 00:43:11.790
How to reduce software mistakes?

00:43:11.790 --> 00:43:13.889
We actually know a
lot of techniques

00:43:13.889 --> 00:43:15.930
that pretty much all
software manufacturers never

00:43:15.930 --> 00:43:18.073
do because it would be
slightly more expensive.

00:43:18.073 --> 00:43:18.880
AUDIENCE: Yeah.

00:43:18.880 --> 00:43:20.880
BRUCE SCHNEIER: But if
it's a lot more expensive

00:43:20.880 --> 00:43:24.410
not to do them, suddenly
the math changes.

00:43:24.410 --> 00:43:26.590
And I need the math to change.

00:43:26.590 --> 00:43:29.640
I need security to be
cheaper than insecurity.

00:43:29.640 --> 00:43:33.480
Right now, the market rewards,
let's just take the chance.

00:43:36.600 --> 00:43:39.499
Let's hope for the best.

00:43:39.499 --> 00:43:40.040
AUDIENCE: OK.

00:43:40.040 --> 00:43:41.415
BRUCE SCHNEIER:
And no industry--

00:43:41.415 --> 00:43:42.811
did I say this already?

00:43:42.811 --> 00:43:43.310
Remind me.

00:43:43.310 --> 00:43:43.740
Yes, no?

00:43:43.740 --> 00:43:44.100
No.

00:43:44.100 --> 00:43:44.599
OK.

00:43:44.599 --> 00:43:48.110
No industry, in the past
100 or something years,

00:43:48.110 --> 00:43:52.010
has improved security and
safety without being forced to.

00:43:52.010 --> 00:43:57.830
Cars, planes, pharmaceuticals,
medical devices,

00:43:57.830 --> 00:44:02.940
food production,
restaurants, consumer goods,

00:44:02.940 --> 00:44:07.190
workplace, most recently,
financial products.

00:44:07.190 --> 00:44:12.970
The market rewards doing a
bad job, hoping for the best.

00:44:12.970 --> 00:44:17.020
And I think it's too risky
to allow that anymore.

00:44:17.020 --> 00:44:17.860
AUDIENCE: Hi.

00:44:17.860 --> 00:44:20.440
You mentioned that you were
embarrassed by the Zuckerberg

00:44:20.440 --> 00:44:21.580
hearings.

00:44:21.580 --> 00:44:23.936
BRUCE SCHNEIER: I was
embarrassed by the questions

00:44:23.936 --> 00:44:25.060
at the Zuckerberg hearings.

00:44:25.060 --> 00:44:25.640
AUDIENCE: OK.

00:44:25.640 --> 00:44:26.450
BRUCE SCHNEIER:
Let's just be fair.

00:44:26.450 --> 00:44:27.910
The congressmen embarrassed me.

00:44:27.910 --> 00:44:29.480
AUDIENCE: Yeah, yeah.

00:44:29.480 --> 00:44:30.760
So I assumed correctly.

00:44:30.760 --> 00:44:33.630
I assumed you were
embarrassed by the senators.

00:44:33.630 --> 00:44:34.810
BRUCE SCHNEIER: Yes, yes.

00:44:34.810 --> 00:44:37.150
AUDIENCE: Whereas I have
the opposite problem.

00:44:37.150 --> 00:44:39.450
I was embarrassed by Zuckerberg.

00:44:39.450 --> 00:44:40.260
What I--

00:44:40.260 --> 00:44:41.560
BRUCE SCHNEIER: To be fair,
there's a lot of embarrassment

00:44:41.560 --> 00:44:42.150
to go around.

00:44:42.150 --> 00:44:42.795
AUDIENCE: Yes, it's true.

00:44:42.795 --> 00:44:43.320
No, but--

00:44:43.320 --> 00:44:44.170
BRUCE SCHNEIER: We
can both be right.

00:44:44.170 --> 00:44:44.570
AUDIENCE: I understand.

00:44:44.570 --> 00:44:45.070
Yeah.

00:44:45.070 --> 00:44:47.620
But I have a serious
point here, though.

00:44:47.620 --> 00:44:50.290
While the senators
don't know about tech,

00:44:50.290 --> 00:44:55.090
I think the tech doesn't
know about law, ethics,

00:44:55.090 --> 00:44:58.330
political science, philosophy.

00:44:58.330 --> 00:45:00.520
Do you think Mark
Zuckerberg can even

00:45:00.520 --> 00:45:04.720
teach an introductory college
course on free speech?

00:45:04.720 --> 00:45:08.140
Has he even read what anyone
has ever said about it?

00:45:08.140 --> 00:45:11.970
So shouldn't we all be
learning about the world?

00:45:11.970 --> 00:45:12.990
BRUCE SCHNEIER: Yes.

00:45:12.990 --> 00:45:13.800
[LAUGHTER]

00:45:13.800 --> 00:45:15.330
This has to go in
both directions.

00:45:15.330 --> 00:45:15.580
AUDIENCE: Yes.

00:45:15.580 --> 00:45:16.496
BRUCE SCHNEIER: Right?

00:45:16.496 --> 00:45:18.610
I want techies in
policy positions.

00:45:18.610 --> 00:45:21.690
I want policy people
in tech companies.

00:45:21.690 --> 00:45:23.065
So yes, I think we need both.

00:45:23.065 --> 00:45:23.690
AUDIENCE: Yeah.

00:45:23.690 --> 00:45:25.990
BRUCE SCHNEIER: We need both
sides talking to each other.

00:45:25.990 --> 00:45:26.656
AUDIENCE: Right.

00:45:26.656 --> 00:45:29.519
BRUCE SCHNEIER: And so
I agree with you 100%.

00:45:29.519 --> 00:45:30.310
AUDIENCE: Good, OK.

00:45:30.310 --> 00:45:33.256
[LAUGHTER]

00:45:34.820 --> 00:45:37.290
BRUCE SCHNEIER: So right now,
I teach internet security

00:45:37.290 --> 00:45:41.380
at the Harvard Kennedy School,
at a public policy institution.

00:45:41.380 --> 00:45:44.404
So I'm trying to push
people in that direction.

00:45:44.404 --> 00:45:45.820
At the same time,
there are people

00:45:45.820 --> 00:45:47.778
at Harvard, in the computer
science department,

00:45:47.778 --> 00:45:51.035
trying to teach policy issues,
going the other direction.

00:45:51.035 --> 00:45:53.076
AUDIENCE: I think you
probably know [INAUDIBLE]..

00:45:53.076 --> 00:45:54.900
[LAUGHTER]

00:45:54.900 --> 00:45:57.130
BRUCE SCHNEIER: I know,
but I'm not in charge.

00:45:57.130 --> 00:46:00.380
AUDIENCE: You mentioned
sort of shock events

00:46:00.380 --> 00:46:02.410
as things that drive
government policy.

00:46:02.410 --> 00:46:04.960
And I thought the
example of 9/11

00:46:04.960 --> 00:46:08.080
was instructive, maybe in a
way you did or did not intend,

00:46:08.080 --> 00:46:10.420
in that the government
response to 9/11

00:46:10.420 --> 00:46:13.330
was to launch two illegal
wars and create a surveillance

00:46:13.330 --> 00:46:15.164
state that violates
our civil liberties

00:46:15.164 --> 00:46:16.080
on a day-to-day basis.

00:46:16.080 --> 00:46:16.320
BRUCE SCHNEIER: No, no, no.

00:46:16.320 --> 00:46:16.560
AUDIENCE: So--

00:46:16.560 --> 00:46:16.810
BRUCE SCHNEIER: I did
intend to evoke that.

00:46:16.810 --> 00:46:18.490
AUDIENCE: --I'm not
sure that's a good--

00:46:18.490 --> 00:46:19.930
I guess I'm curious how you--

00:46:19.930 --> 00:46:20.590
BRUCE SCHNEIER: It's terrible.

00:46:20.590 --> 00:46:20.980
Right.

00:46:20.980 --> 00:46:22.646
AUDIENCE: So how do
you see the reaction

00:46:22.646 --> 00:46:25.297
to the mounting threats in
technology as being different?

00:46:25.297 --> 00:46:27.880
What's going to prevent the same
sort of thing from happening?

00:46:27.880 --> 00:46:29.180
BRUCE SCHNEIER:
Absolutely nothing.

00:46:29.180 --> 00:46:29.752
AUDIENCE: OK.

00:46:29.752 --> 00:46:31.210
BRUCE SCHNEIER:
And that's my fear.

00:46:31.210 --> 00:46:34.210
That something bad will happen.

00:46:34.210 --> 00:46:37.310
Congress will say,
something must be done.

00:46:37.310 --> 00:46:38.590
This is something.

00:46:38.590 --> 00:46:40.010
Therefore, we must do it.

00:46:40.010 --> 00:46:41.176
AUDIENCE: It has to be done.

00:46:41.176 --> 00:46:41.960
[LAUGHTER]

00:46:41.960 --> 00:46:44.590
BRUCE SCHNEIER: So my goal of
having this conversation now,

00:46:44.590 --> 00:46:49.600
before this happens, is that
we will, as a community,

00:46:49.600 --> 00:46:55.180
figure out what should be done
when we have the luxury of time

00:46:55.180 --> 00:46:58.690
and insights and patience.

00:46:58.690 --> 00:47:01.960
Because I agree with you
that there is a disaster.

00:47:01.960 --> 00:47:04.900
We will get a disaster
as a response,

00:47:04.900 --> 00:47:06.620
and it will be just as bad.

00:47:06.620 --> 00:47:08.230
So let's get ahead
of it this time.

00:47:08.230 --> 00:47:09.980
Let's do better.

00:47:09.980 --> 00:47:12.520
AUDIENCE: How do you
envision preventing

00:47:12.520 --> 00:47:15.269
everything degenerating to
the lowest common denominator?

00:47:15.269 --> 00:47:17.560
You said, client side, you
can't really restrict people

00:47:17.560 --> 00:47:19.090
from doing what they want.

00:47:19.090 --> 00:47:20.980
Even if we say, OK,
any company that

00:47:20.980 --> 00:47:23.926
wants to make money in the US
has to follow these provisions.

00:47:23.926 --> 00:47:25.300
I'm just going to
encrypt my data

00:47:25.300 --> 00:47:27.040
and send it to
Alibaba Translate.

00:47:27.040 --> 00:47:28.690
It's 1/3 the price
of Google Translate,

00:47:28.690 --> 00:47:31.390
but they steal all my data.

00:47:31.390 --> 00:47:33.910
How do we prevent this?

00:47:33.910 --> 00:47:35.440
Is there anything we can do?

00:47:35.440 --> 00:47:36.160
BRUCE SCHNEIER:
Some of the answer

00:47:36.160 --> 00:47:38.260
is going to be no,
some, it's going to yes.

00:47:38.260 --> 00:47:42.100
So if you think about
other consumer goods,

00:47:42.100 --> 00:47:44.740
we do make it hard for
consumers to modify something.

00:47:44.740 --> 00:47:47.530
It's actually hard
to modify your car

00:47:47.530 --> 00:47:49.690
to violate emissions control.

00:47:49.690 --> 00:47:52.390
You can do it, but it's hard.

00:47:52.390 --> 00:47:55.090
And then we try to
have spot checks.

00:47:55.090 --> 00:47:57.580
You can imagine
some sort of regime.

00:47:57.580 --> 00:48:00.104
You can imagine some
system that tries

00:48:00.104 --> 00:48:01.270
to maintain security anyway.

00:48:01.270 --> 00:48:04.240
Because there will be
a minority doing that.

00:48:04.240 --> 00:48:09.040
I think, once we start
hitting the problem for real,

00:48:09.040 --> 00:48:10.900
we'll come up with
tech solutions.

00:48:10.900 --> 00:48:15.130
Ways for the system to watch
itself, other systems to watch

00:48:15.130 --> 00:48:16.310
each other.

00:48:16.310 --> 00:48:17.980
Can we do this non-invasively?

00:48:17.980 --> 00:48:20.000
I think we have
to figure it out.

00:48:20.000 --> 00:48:21.520
So I don't have
the answers here,

00:48:21.520 --> 00:48:24.850
but these are
certainly the problems.

00:48:24.850 --> 00:48:28.360
AUDIENCE: I really liked your
phrasing of the problem of,

00:48:28.360 --> 00:48:33.060
we need to give up on offense
so we can go all in on defense.

00:48:33.060 --> 00:48:35.740
And I think it's
pretty clear to me

00:48:35.740 --> 00:48:37.950
where a lot of the
offensive focus

00:48:37.950 --> 00:48:40.340
is, in terms of law enforcement.

00:48:40.340 --> 00:48:43.930
But I think one thing
that remains mostly

00:48:43.930 --> 00:48:46.990
an unknown is on
the military side,

00:48:46.990 --> 00:48:49.060
and how there is a
ton of investment

00:48:49.060 --> 00:48:50.980
in military offensive stuff.

00:48:50.980 --> 00:48:53.080
We kind of know a
little bit more,

00:48:53.080 --> 00:48:56.470
maybe, about what Russia and
China are using offensively

00:48:56.470 --> 00:48:58.470
against us.

00:48:58.470 --> 00:49:00.850
BRUCE SCHNEIER: We aren't
seeing the good stuff yet.

00:49:00.850 --> 00:49:02.300
AUDIENCE: I hope not.

00:49:02.300 --> 00:49:03.100
Anyway--

00:49:03.100 --> 00:49:04.260
BRUCE SCHNEIER: We're not
sure what we hope, right?

00:49:04.260 --> 00:49:04.759
[LAUGHTER]

00:49:04.759 --> 00:49:06.080
AUDIENCE: Yeah.

00:49:06.080 --> 00:49:10.560
But do we have a sense
of what the military--

00:49:10.560 --> 00:49:14.170
the US military, let's
say-- would be giving up

00:49:14.170 --> 00:49:17.710
to give up this offensive idea?

00:49:17.710 --> 00:49:21.190
And-- I don't know--
how willing they

00:49:21.190 --> 00:49:22.441
would be to go that direction?

00:49:22.441 --> 00:49:24.148
BRUCE SCHNEIER: They
wouldn't be willing,

00:49:24.148 --> 00:49:25.905
but it's not their
job to be willing.

00:49:25.905 --> 00:49:27.280
That's why you
don't want the NSA

00:49:27.280 --> 00:49:32.260
in charge of your privacy policy
because that's not their job.

00:49:32.260 --> 00:49:36.610
We need people above
the military, the NSA,

00:49:36.610 --> 00:49:38.620
to make these tradeoffs.

00:49:38.620 --> 00:49:42.460
Because they are security
versus security tradeoffs.

00:49:42.460 --> 00:49:45.790
Is the security we get
from being able to spy on

00:49:45.790 --> 00:49:48.040
and hack the bad
guys greater or less

00:49:48.040 --> 00:49:50.620
than the security we get
from the bad guys being

00:49:50.620 --> 00:49:54.340
unable to spy on and hack us?

00:49:54.340 --> 00:49:55.150
Right?

00:49:55.150 --> 00:49:57.820
So security versus
surveillance is the wrong way

00:49:57.820 --> 00:49:58.490
to describe it.

00:49:58.490 --> 00:50:00.370
It's security versus security.

00:50:00.370 --> 00:50:04.180
So someone above the military
needs to decide that.

00:50:04.180 --> 00:50:05.590
It can't be the military.

00:50:05.590 --> 00:50:08.330
Because the military is not
in charge of overall policy.

00:50:08.330 --> 00:50:10.090
They're in charge of
the military part.

00:50:10.090 --> 00:50:12.580
And what we know about the
capabilities is very little.

00:50:12.580 --> 00:50:15.760
We get some shadows
of it here or there,

00:50:15.760 --> 00:50:18.340
and it seems to be,
on the one hand,

00:50:18.340 --> 00:50:21.250
cruder than we'd like it to be.

00:50:21.250 --> 00:50:25.650
On the other hand, StuckStack
was pretty impressive.

00:50:25.650 --> 00:50:29.160
In general, the stuff you
see is the minimum tech

00:50:29.160 --> 00:50:31.410
it has to be to succeed.

00:50:31.410 --> 00:50:37.290
There's sort of this myth of
these super powerful cyber

00:50:37.290 --> 00:50:38.640
attacks.

00:50:38.640 --> 00:50:41.340
They're basically one
iota more than just

00:50:41.340 --> 00:50:44.920
barely necessary to succeed.

00:50:44.920 --> 00:50:47.440
You don't need to do
more if you can take out

00:50:47.440 --> 00:50:50.590
the DNC with a pretty
sloppy phishing campaign.

00:50:50.590 --> 00:50:51.950
[LAUGHTER]

00:50:52.450 --> 00:50:54.670
Why bother using
your good stuff?

00:50:54.670 --> 00:50:57.500
So there's a lot
we just don't know.

00:50:57.500 --> 00:51:00.040
AUDIENCE: At risk of
revisiting an earlier question,

00:51:00.040 --> 00:51:02.360
I was interested in
what you thought about--

00:51:02.360 --> 00:51:03.740
one of the things,
often, you see

00:51:03.740 --> 00:51:05.156
cynical in the
finance industry is

00:51:05.156 --> 00:51:06.770
that they think that
people in finance

00:51:06.770 --> 00:51:09.320
can outmaneuver all the people
who are regulating them,

00:51:09.320 --> 00:51:10.820
in part because
they're lesser paid.

00:51:10.820 --> 00:51:12.695
So I was wondering if
you could revisit that.

00:51:12.695 --> 00:51:14.656
Because I think law has,
maybe, the exception

00:51:14.656 --> 00:51:16.280
because it's a little
bit more directly

00:51:16.280 --> 00:51:18.752
related to human rights
and things like that.

00:51:18.752 --> 00:51:20.210
BRUCE SCHNEIER:
Yeah, I don't know.

00:51:20.210 --> 00:51:24.350
Certainly, I worry about
regulatory capture,

00:51:24.350 --> 00:51:26.360
regulations being evaded.

00:51:26.360 --> 00:51:28.520
I think all of those
are real risks.

00:51:28.520 --> 00:51:30.440
This is not a great
answer I have,

00:51:30.440 --> 00:51:33.340
it's just the best one I have.

00:51:33.340 --> 00:51:36.060
Because I don't
see any way to put

00:51:36.060 --> 00:51:39.580
a backstop against this
massive corporate power

00:51:39.580 --> 00:51:41.020
other than government power.

00:51:41.020 --> 00:51:44.830
Now, in a sense, I
don't want either power.

00:51:44.830 --> 00:51:48.160
But tech naturally
concentrates power,

00:51:48.160 --> 00:51:51.180
at least as it's
configured today.

00:51:51.180 --> 00:51:54.005
So that's my missing piece.

00:51:54.005 --> 00:51:55.380
I think you're
right that that is

00:51:55.380 --> 00:51:58.920
a serious problem and
worry, and something

00:51:58.920 --> 00:52:00.540
we just have to deal with.

00:52:00.540 --> 00:52:02.580
Policy is iterative.

00:52:02.580 --> 00:52:05.340
As techies, it's
hard to accept that.

00:52:05.340 --> 00:52:07.610
We like to get the answer
right and implement it.

00:52:07.610 --> 00:52:10.710
Whereas policy gets the
answer slightly less wrong

00:52:10.710 --> 00:52:11.530
every few months.

00:52:14.110 --> 00:52:17.910
But that's the way it works.

00:52:17.910 --> 00:52:20.460
The real question is, can
we do this at tech speed?

00:52:20.460 --> 00:52:24.000
And that, really, I think,
is an open question.

00:52:24.000 --> 00:52:25.570
So with that, I'm going to end.

00:52:25.570 --> 00:52:26.314
Thank you all.

00:52:26.314 --> 00:52:27.480
Thanks for filling the room.

00:52:27.480 --> 00:52:28.380
Thanks for coming.

00:52:28.380 --> 00:52:32.630
[APPLAUSE]

