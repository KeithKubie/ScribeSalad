WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:01.910
&gt;&gt; Okay Micheal. So we've talked about PCA

00:00:01.910 --> 00:00:05.410
and ICA. And they both work remarkably well

00:00:05.410 --> 00:00:07.760
in the specific domains that they're designed for.

00:00:07.760 --> 00:00:10.070
And they've been applied for decades on a

00:00:10.070 --> 00:00:11.762
wide variety of problems for doing this sort

00:00:11.762 --> 00:00:13.660
of future transformation. But I'm going to just

00:00:13.660 --> 00:00:16.250
very briefly describe two other alternatives, and sort

00:00:16.250 --> 00:00:17.660
of give you a notion of the space, okay?

00:00:17.660 --> 00:00:18.430
&gt;&gt; Sure.

00:00:18.430 --> 00:00:21.400
&gt;&gt; Okay, the first one is kind of irritating,

00:00:22.610 --> 00:00:24.275
but I feel obligated to share it with you.

00:00:24.275 --> 00:00:28.000
Ad it's called, well it's got many different names, but

00:00:28.000 --> 00:00:32.070
I'm going to call it RCA just because I like the symmetry.

00:00:33.120 --> 00:00:37.280
And RCA stands for random components analysis. So what do you

00:00:37.280 --> 00:00:41.440
think random components analysis does? This is also called random projection.

00:00:41.440 --> 00:00:45.290
&gt;&gt; I'm going to guess, instead of finding dimensions with,

00:00:45.290 --> 00:00:49.380
say, high variance, it's just going to pick any direction.

00:00:49.380 --> 00:00:53.570
&gt;&gt; That's exactly right. What RCA does is it generates random directions.

00:00:53.570 --> 00:00:56.410
&gt;&gt; Then I guess it projects the data out into those directions.

00:00:56.410 --> 00:00:59.160
&gt;&gt; That's exactly right. It's like saying it picks a random

00:00:59.160 --> 00:01:05.730
P to a random matrix to project your data onto. This

00:01:05.730 --> 00:01:09.234
matrix is, in some sense, just any random linear combination. And

00:01:09.234 --> 00:01:12.780
you want to know something? It works. It works remarkably well.

00:01:12.780 --> 00:01:15.010
&gt;&gt; At what though? In terms

00:01:15.010 --> 00:01:16.690
of like reconstruction?

00:01:16.690 --> 00:01:18.935
&gt;&gt; At reconstruction. Well not particularly well at

00:01:18.935 --> 00:01:20.070
reconstruction. But you know what it works really well,

00:01:20.070 --> 00:01:21.940
it works really well if the next thing

00:01:21.940 --> 00:01:23.550
you're going to do is some kind of classification.

00:01:23.550 --> 00:01:24.890
&gt;&gt; Hm,

00:01:24.890 --> 00:01:29.230
&gt;&gt; Now why is it you think that it actually works? Can you imagine why

00:01:29.230 --> 00:01:31.300
just picking a bunch of random directions

00:01:31.300 --> 00:01:33.370
and projecting onto those random directions might work?

00:01:33.370 --> 00:01:36.410
&gt;&gt; Well, it does mix things together differently. I don't know why the

00:01:36.410 --> 00:01:38.070
original data wouldn't work. Then this would

00:01:38.070 --> 00:01:40.020
work better. Unless the original data is

00:01:40.020 --> 00:01:43.620
somewhow is purposely made to not work.

00:01:43.620 --> 00:01:44.620
&gt;&gt; Well remember what we're doing here,

00:01:44.620 --> 00:01:46.870
right. We're starting with N dimensions. And we're

00:01:46.870 --> 00:01:50.490
projecting down to M dimensions, where M is

00:01:50.490 --> 00:01:53.140
significantly lower than N. So, I started with

00:01:53.140 --> 00:01:55.160
a bunch of dimensions. Now remember, the real

00:01:55.160 --> 00:01:56.590
problem here is not that I can't gather

00:01:56.590 --> 00:01:58.930
the data from the N dimensions. It''s that

00:01:58.930 --> 00:02:01.070
there's a whole bunch of them, cursor dimensionality.

00:02:01.070 --> 00:02:01.524
&gt;&gt; Yes.

00:02:01.524 --> 00:02:03.220
&gt;&gt; So I need to have a lot of data.

00:02:03.220 --> 00:02:05.219
So if I don't have a lot of data, at least

00:02:05.219 --> 00:02:09.810
certainly not an exponential amount of data, so to speak, and I project a

00:02:09.810 --> 00:02:12.000
lower dimension, why would random projections still

00:02:12.000 --> 00:02:15.110
give me something that helps with classification?

00:02:15.110 --> 00:02:17.490
&gt;&gt; So it's, it's as if it's maintaining some

00:02:17.490 --> 00:02:19.600
of the information from these other dimensions, even though

00:02:19.600 --> 00:02:21.200
there's fewer of them now. They're all kind of

00:02:21.200 --> 00:02:24.120
mixed together, but they signal might still be there.

00:02:24.120 --> 00:02:26.730
&gt;&gt; That's exactly right. And because you project into a

00:02:26.730 --> 00:02:31.080
lower dimension, you end up dealing with a cursor dimensionality problem,

00:02:31.080 --> 00:02:32.270
which is sort of the whole point of this, or

00:02:32.270 --> 00:02:34.360
one of the whole points of this in the first place.

00:02:34.360 --> 00:02:36.220
And really, a, a way I think of summarizing what you're

00:02:36.220 --> 00:02:39.190
saying is, that this manages to still pick up some correlations.

00:02:41.350 --> 00:02:44.030
So if I take random linear combinations of all of my

00:02:44.030 --> 00:02:47.680
features, then there's still information from all of my features there.

00:02:47.680 --> 00:02:49.960
So in practice, at least in my experience, Michael, it turns

00:02:49.960 --> 00:02:53.860
out that M, the number of lower dimensions you project into, for

00:02:53.860 --> 00:02:57.950
a randomized components analysis, or randomized projections, tends to be bigger

00:02:57.950 --> 00:03:01.150
than the M that you would get by doing something like PCA.

00:03:01.150 --> 00:03:03.090
So you don't end up projecting down to sort of the

00:03:03.090 --> 00:03:06.470
lowest possible dimensional space. But you still project down to a lower

00:03:06.470 --> 00:03:09.590
dimensional space that happens to capture your correlations, or at

00:03:09.590 --> 00:03:12.620
least capture some of the correlations, which often ends up

00:03:12.620 --> 00:03:15.610
working very well for a learner or a classifier down

00:03:15.610 --> 00:03:17.970
the road. You can actually see how, in this case, you

00:03:17.970 --> 00:03:21.580
might even project into another set of dimensions M, where

00:03:21.580 --> 00:03:26.230
those number dimensions are actually bigger than the number of

00:03:26.230 --> 00:03:28.710
dimensions you started out with. This, in some sense, is

00:03:28.710 --> 00:03:31.580
almost exactly what we did with perceptons in solving X or.

00:03:31.580 --> 00:03:34.090
Basically, you're projecting into higher dimensional spaces

00:03:34.090 --> 00:03:35.660
by doing this. Does that all make sense?

00:03:35.660 --> 00:03:37.700
&gt;&gt; Yeah, I think so. I mean it makes sense

00:03:37.700 --> 00:03:39.840
to me that it would be not as efficient in

00:03:39.840 --> 00:03:42.830
some sense, as PCA, because it sort of reminds me

00:03:42.830 --> 00:03:45.080
of, you know, if I want to, if I want

00:03:45.080 --> 00:03:47.880
to paint my wall, I can very carefully paint all

00:03:47.880 --> 00:03:49.970
the little pieces of it. Or, I could just splatter

00:03:49.970 --> 00:03:52.560
stuff at it. It generally takes more when you splatter

00:03:52.560 --> 00:03:56.180
because you're not being as systematic, but it does, ultimately,

00:03:56.180 --> 00:03:57.798
cover your wall.

00:03:57.798 --> 00:04:02.650
&gt;&gt; [LAUGH], Yeah. I think that's an interesting analogy, and I'm going to

00:04:02.650 --> 00:04:06.244
go with an apt one. Okay, so, this sort of thing works. What

00:04:06.244 --> 00:04:10.030
advantages does it actually have over PCA and ICA? Can you imagine one?

00:04:10.030 --> 00:04:12.550
There's one in particular which I think sort of jumps out at you.

00:04:12.550 --> 00:04:16.470
&gt;&gt; RCA? Well, I don't know. Is that, is that a good quiz question maybe?

00:04:16.470 --> 00:04:18.930
&gt;&gt; Sure, let's make it a quick quiz.

