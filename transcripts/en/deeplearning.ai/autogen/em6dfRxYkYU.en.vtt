WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:01.910
 
so you've seen the equations for how to

00:00:01.910 --> 00:00:01.920
so you've seen the equations for how to
 

00:00:01.920 --> 00:00:04.070
so you've seen the equations for how to
infer and - norm for maybe a single

00:00:04.070 --> 00:00:04.080
infer and - norm for maybe a single
 

00:00:04.080 --> 00:00:06.440
infer and - norm for maybe a single
hidden layer let's see how it fits in to

00:00:06.440 --> 00:00:06.450
hidden layer let's see how it fits in to
 

00:00:06.450 --> 00:00:09.020
hidden layer let's see how it fits in to
the training of a deep network so let's

00:00:09.020 --> 00:00:09.030
the training of a deep network so let's
 

00:00:09.030 --> 00:00:10.700
the training of a deep network so let's
figure of a neural network like this

00:00:10.700 --> 00:00:10.710
figure of a neural network like this
 

00:00:10.710 --> 00:00:13.610
figure of a neural network like this
you've seen me say before that you can

00:00:13.610 --> 00:00:13.620
you've seen me say before that you can
 

00:00:13.620 --> 00:00:15.799
you've seen me say before that you can
view each hidden unit as computing two

00:00:15.799 --> 00:00:15.809
view each hidden unit as computing two
 

00:00:15.809 --> 00:00:19.010
view each hidden unit as computing two
things first it computes Z and then it

00:00:19.010 --> 00:00:19.020
things first it computes Z and then it
 

00:00:19.020 --> 00:00:21.170
things first it computes Z and then it
applies the activation function to

00:00:21.170 --> 00:00:21.180
applies the activation function to
 

00:00:21.180 --> 00:00:24.500
applies the activation function to
compute a and so you can think of each

00:00:24.500 --> 00:00:24.510
compute a and so you can think of each
 

00:00:24.510 --> 00:00:27.830
compute a and so you can think of each
of these circles as representing a two

00:00:27.830 --> 00:00:27.840
of these circles as representing a two
 

00:00:27.840 --> 00:00:32.330
of these circles as representing a two
step computation and similarly for the

00:00:32.330 --> 00:00:32.340
step computation and similarly for the
 

00:00:32.340 --> 00:00:38.840
step computation and similarly for the
next layer that's V 2 1 and a 2 1 and so

00:00:38.840 --> 00:00:38.850
next layer that's V 2 1 and a 2 1 and so
 

00:00:38.850 --> 00:00:44.930
next layer that's V 2 1 and a 2 1 and so
on so if you were not supplying bascomb

00:00:44.930 --> 00:00:44.940
on so if you were not supplying bascomb
 

00:00:44.940 --> 00:00:49.100
on so if you were not supplying bascomb
you would have an input X feed into the

00:00:49.100 --> 00:00:49.110
you would have an input X feed into the
 

00:00:49.110 --> 00:00:51.560
you would have an input X feed into the
first hidden layer and then first

00:00:51.560 --> 00:00:51.570
first hidden layer and then first
 

00:00:51.570 --> 00:00:55.100
first hidden layer and then first
compute z1 and this is governed by the

00:00:55.100 --> 00:00:55.110
compute z1 and this is governed by the
 

00:00:55.110 --> 00:00:59.029
compute z1 and this is governed by the
parameters W 1 and B 1 and then

00:00:59.029 --> 00:00:59.039
parameters W 1 and B 1 and then
 

00:00:59.039 --> 00:01:02.180
parameters W 1 and B 1 and then
ordinarily you would feed Z 1 into the

00:01:02.180 --> 00:01:02.190
ordinarily you would feed Z 1 into the
 

00:01:02.190 --> 00:01:05.090
ordinarily you would feed Z 1 into the
activation function to compute a 1 but

00:01:05.090 --> 00:01:05.100
activation function to compute a 1 but
 

00:01:05.100 --> 00:01:07.190
activation function to compute a 1 but
what we'll do in batchelomez take this

00:01:07.190 --> 00:01:07.200
what we'll do in batchelomez take this
 

00:01:07.200 --> 00:01:14.899
what we'll do in batchelomez take this
value Z 1 and apply national sometimes

00:01:14.899 --> 00:01:14.909
value Z 1 and apply national sometimes
 

00:01:14.909 --> 00:01:17.510
value Z 1 and apply national sometimes
abbreviated BN to it and that's going to

00:01:17.510 --> 00:01:17.520
abbreviated BN to it and that's going to
 

00:01:17.520 --> 00:01:21.490
abbreviated BN to it and that's going to
be governed by parameters beta 1 and

00:01:21.490 --> 00:01:21.500
be governed by parameters beta 1 and
 

00:01:21.500 --> 00:01:25.100
be governed by parameters beta 1 and
gamma 1 and this will give you this new

00:01:25.100 --> 00:01:25.110
gamma 1 and this will give you this new
 

00:01:25.110 --> 00:01:29.210
gamma 1 and this will give you this new
normalized value V 1 and then you feed

00:01:29.210 --> 00:01:29.220
normalized value V 1 and then you feed
 

00:01:29.220 --> 00:01:31.880
normalized value V 1 and then you feed
that to the activation function to get a

00:01:31.880 --> 00:01:31.890
that to the activation function to get a
 

00:01:31.890 --> 00:01:37.670
that to the activation function to get a
1 which is a G 1 applied to V tilde 1

00:01:37.670 --> 00:01:37.680
1 which is a G 1 applied to V tilde 1
 

00:01:37.680 --> 00:01:40.700
1 which is a G 1 applied to V tilde 1
now you've done the computation for the

00:01:40.700 --> 00:01:40.710
now you've done the computation for the
 

00:01:40.710 --> 00:01:43.310
now you've done the computation for the
first layer where this passion ohms that

00:01:43.310 --> 00:01:43.320
first layer where this passion ohms that
 

00:01:43.320 --> 00:01:45.649
first layer where this passion ohms that
really occurs you know in between the

00:01:45.649 --> 00:01:45.659
really occurs you know in between the
 

00:01:45.659 --> 00:01:48.620
really occurs you know in between the
computation from Z and a next you take

00:01:48.620 --> 00:01:48.630
computation from Z and a next you take
 

00:01:48.630 --> 00:01:53.179
computation from Z and a next you take
this value a 1 and use it to compute V 2

00:01:53.179 --> 00:01:53.189
this value a 1 and use it to compute V 2
 

00:01:53.189 --> 00:01:57.139
this value a 1 and use it to compute V 2
and so this is now governed by W 2 B 2

00:01:57.139 --> 00:01:57.149
and so this is now governed by W 2 B 2
 

00:01:57.149 --> 00:02:00.410
and so this is now governed by W 2 B 2
and similar to what you did for the

00:02:00.410 --> 00:02:00.420
and similar to what you did for the
 

00:02:00.420 --> 00:02:02.450
and similar to what you did for the
first layer you would take me 2 and

00:02:02.450 --> 00:02:02.460
first layer you would take me 2 and
 

00:02:02.460 --> 00:02:05.240
first layer you would take me 2 and
apply it through passional reading that

00:02:05.240 --> 00:02:05.250
apply it through passional reading that
 

00:02:05.250 --> 00:02:08.990
apply it through passional reading that
BN now this is governed by rational

00:02:08.990 --> 00:02:09.000
BN now this is governed by rational
 

00:02:09.000 --> 00:02:11.809
BN now this is governed by rational
parameters specific to the next layer

00:02:11.809 --> 00:02:11.819
parameters specific to the next layer
 

00:02:11.819 --> 00:02:16.190
parameters specific to the next layer
beta 2 gamma 2 and now this gives you

00:02:16.190 --> 00:02:16.200
beta 2 gamma 2 and now this gives you
 

00:02:16.200 --> 00:02:19.699
beta 2 gamma 2 and now this gives you
the total - and you use that to compute

00:02:19.699 --> 00:02:19.709
the total - and you use that to compute
 

00:02:19.709 --> 00:02:23.000
the total - and you use that to compute
a 2 by applying G activation function

00:02:23.000 --> 00:02:23.010
a 2 by applying G activation function
 

00:02:23.010 --> 00:02:26.449
a 2 by applying G activation function
and so on so once again the Batchelor

00:02:26.449 --> 00:02:26.459
and so on so once again the Batchelor
 

00:02:26.459 --> 00:02:28.009
and so on so once again the Batchelor
ohms that you know kind of happens

00:02:28.009 --> 00:02:28.019
ohms that you know kind of happens
 

00:02:28.019 --> 00:02:31.729
ohms that you know kind of happens
between computing Z and computing a and

00:02:31.729 --> 00:02:31.739
between computing Z and computing a and
 

00:02:31.739 --> 00:02:33.830
between computing Z and computing a and
the intuition is that instead of using

00:02:33.830 --> 00:02:33.840
the intuition is that instead of using
 

00:02:33.840 --> 00:02:36.530
the intuition is that instead of using
the unnormalized value Z you're going to

00:02:36.530 --> 00:02:36.540
the unnormalized value Z you're going to
 

00:02:36.540 --> 00:02:39.259
the unnormalized value Z you're going to
use the normalized value V Tilda that's

00:02:39.259 --> 00:02:39.269
use the normalized value V Tilda that's
 

00:02:39.269 --> 00:02:40.940
use the normalized value V Tilda that's
in the first layer and the second layer

00:02:40.940 --> 00:02:40.950
in the first layer and the second layer
 

00:02:40.950 --> 00:02:42.589
in the first layer and the second layer
as well as they're using the unknown

00:02:42.589 --> 00:02:42.599
as well as they're using the unknown
 

00:02:42.599 --> 00:02:45.589
as well as they're using the unknown
value z2 you're going to use the mean

00:02:45.589 --> 00:02:45.599
value z2 you're going to use the mean
 

00:02:45.599 --> 00:02:48.349
value z2 you're going to use the mean
and variance normalized values Z tilde -

00:02:48.349 --> 00:02:48.359
and variance normalized values Z tilde -
 

00:02:48.359 --> 00:02:52.460
and variance normalized values Z tilde -
so the parameters or your network are

00:02:52.460 --> 00:02:52.470
so the parameters or your network are
 

00:02:52.470 --> 00:02:56.899
so the parameters or your network are
going to be W 1 B 1 it turns out we'll

00:02:56.899 --> 00:02:56.909
going to be W 1 B 1 it turns out we'll
 

00:02:56.909 --> 00:02:58.789
going to be W 1 B 1 it turns out we'll
get rid of the B parameters that we'll

00:02:58.789 --> 00:02:58.799
get rid of the B parameters that we'll
 

00:02:58.799 --> 00:03:01.539
get rid of the B parameters that we'll
see why in the next slide but for now

00:03:01.539 --> 00:03:01.549
see why in the next slide but for now
 

00:03:01.549 --> 00:03:06.470
see why in the next slide but for now
imagine your parameters are the usual W

00:03:06.470 --> 00:03:06.480
imagine your parameters are the usual W
 

00:03:06.480 --> 00:03:11.479
imagine your parameters are the usual W
1 B 1 3w l BL and we've added to this

00:03:11.479 --> 00:03:11.489
1 B 1 3w l BL and we've added to this
 

00:03:11.489 --> 00:03:14.000
1 B 1 3w l BL and we've added to this
new network additional parameters beta 1

00:03:14.000 --> 00:03:14.010
new network additional parameters beta 1
 

00:03:14.010 --> 00:03:19.699
new network additional parameters beta 1
gamma 1 beta 2 gamma 2 and so on you

00:03:19.699 --> 00:03:19.709
gamma 1 beta 2 gamma 2 and so on you
 

00:03:19.709 --> 00:03:22.009
gamma 1 beta 2 gamma 2 and so on you
know for each layer in which you are

00:03:22.009 --> 00:03:22.019
know for each layer in which you are
 

00:03:22.019 --> 00:03:25.369
know for each layer in which you are
applying bash mall for clarity note that

00:03:25.369 --> 00:03:25.379
applying bash mall for clarity note that
 

00:03:25.379 --> 00:03:27.890
applying bash mall for clarity note that
these betas here these nothing to do

00:03:27.890 --> 00:03:27.900
these betas here these nothing to do
 

00:03:27.900 --> 00:03:29.930
these betas here these nothing to do
with the hyper parameter beta that we

00:03:29.930 --> 00:03:29.940
with the hyper parameter beta that we
 

00:03:29.940 --> 00:03:33.530
with the hyper parameter beta that we
had for momentum over the computing the

00:03:33.530 --> 00:03:33.540
had for momentum over the computing the
 

00:03:33.540 --> 00:03:35.689
had for momentum over the computing the
various exponentially weighted averages

00:03:35.689 --> 00:03:35.699
various exponentially weighted averages
 

00:03:35.699 --> 00:03:38.360
various exponentially weighted averages
you know the authors of the atom paper

00:03:38.360 --> 00:03:38.370
you know the authors of the atom paper
 

00:03:38.370 --> 00:03:41.330
you know the authors of the atom paper
had used beta in their paper to denote

00:03:41.330 --> 00:03:41.340
had used beta in their paper to denote
 

00:03:41.340 --> 00:03:43.219
had used beta in their paper to denote
that hyper parameter the authors that

00:03:43.219 --> 00:03:43.229
that hyper parameter the authors that
 

00:03:43.229 --> 00:03:45.589
that hyper parameter the authors that
the Bashan on paper had used data to

00:03:45.589 --> 00:03:45.599
the Bashan on paper had used data to
 

00:03:45.599 --> 00:03:48.080
the Bashan on paper had used data to
denote this parameter but these are two

00:03:48.080 --> 00:03:48.090
denote this parameter but these are two
 

00:03:48.090 --> 00:03:50.330
denote this parameter but these are two
completely different betas I decided to

00:03:50.330 --> 00:03:50.340
completely different betas I decided to
 

00:03:50.340 --> 00:03:53.539
completely different betas I decided to
stick with beta in both cases in case

00:03:53.539 --> 00:03:53.549
stick with beta in both cases in case
 

00:03:53.549 --> 00:03:55.839
stick with beta in both cases in case
you read the original papers but the

00:03:55.839 --> 00:03:55.849
you read the original papers but the
 

00:03:55.849 --> 00:03:59.449
you read the original papers but the
beta 1 beta 2 and so on that - norm

00:03:59.449 --> 00:03:59.459
beta 1 beta 2 and so on that - norm
 

00:03:59.459 --> 00:04:02.809
beta 1 beta 2 and so on that - norm
tries to learn is a different data than

00:04:02.809 --> 00:04:02.819
tries to learn is a different data than
 

00:04:02.819 --> 00:04:05.679
tries to learn is a different data than
the hyper parameter beta used in

00:04:05.679 --> 00:04:05.689
the hyper parameter beta used in
 

00:04:05.689 --> 00:04:09.619
the hyper parameter beta used in
momentum and in the abdomen rmsprop

00:04:09.619 --> 00:04:09.629
momentum and in the abdomen rmsprop
 

00:04:09.629 --> 00:04:13.099
momentum and in the abdomen rmsprop
algorithms but so now that these are the

00:04:13.099 --> 00:04:13.109
algorithms but so now that these are the
 

00:04:13.109 --> 00:04:15.080
algorithms but so now that these are the
new parameters of your algorithm you

00:04:15.080 --> 00:04:15.090
new parameters of your algorithm you
 

00:04:15.090 --> 00:04:17.629
new parameters of your algorithm you
would then use whether optimization you

00:04:17.629 --> 00:04:17.639
would then use whether optimization you
 

00:04:17.639 --> 00:04:20.390
would then use whether optimization you
want such as gradient descent in order

00:04:20.390 --> 00:04:20.400
want such as gradient descent in order
 

00:04:20.400 --> 00:04:22.940
want such as gradient descent in order
to implement it so for example you might

00:04:22.940 --> 00:04:22.950
to implement it so for example you might
 

00:04:22.950 --> 00:04:25.310
to implement it so for example you might
compute D beta

00:04:25.310 --> 00:04:25.320
compute D beta
 

00:04:25.320 --> 00:04:27.620
compute D beta
all forgiven layer and then update the

00:04:27.620 --> 00:04:27.630
all forgiven layer and then update the
 

00:04:27.630 --> 00:04:31.070
all forgiven layer and then update the
parameters beta gets updated as beta -

00:04:31.070 --> 00:04:31.080
parameters beta gets updated as beta -
 

00:04:31.080 --> 00:04:36.620
parameters beta gets updated as beta -
learning rate times e beta l and you can

00:04:36.620 --> 00:04:36.630
learning rate times e beta l and you can
 

00:04:36.630 --> 00:04:40.070
learning rate times e beta l and you can
also use atom or RS prop or momentum in

00:04:40.070 --> 00:04:40.080
also use atom or RS prop or momentum in
 

00:04:40.080 --> 00:04:42.440
also use atom or RS prop or momentum in
order to update the parameters beta and

00:04:42.440 --> 00:04:42.450
order to update the parameters beta and
 

00:04:42.450 --> 00:04:43.030
order to update the parameters beta and
gamma

00:04:43.030 --> 00:04:43.040
gamma
 

00:04:43.040 --> 00:04:46.190
gamma
not just gradient descent and even

00:04:46.190 --> 00:04:46.200
not just gradient descent and even
 

00:04:46.200 --> 00:04:48.350
not just gradient descent and even
though in the previous video I'll

00:04:48.350 --> 00:04:48.360
though in the previous video I'll
 

00:04:48.360 --> 00:04:51.170
though in the previous video I'll
explain what the bash zone operation

00:04:51.170 --> 00:04:51.180
explain what the bash zone operation
 

00:04:51.180 --> 00:04:53.320
explain what the bash zone operation
does confuse me in in variances in

00:04:53.320 --> 00:04:53.330
does confuse me in in variances in
 

00:04:53.330 --> 00:04:57.430
does confuse me in in variances in
subtraction / them if they're using a

00:04:57.430 --> 00:04:57.440
subtraction / them if they're using a
 

00:04:57.440 --> 00:05:00.070
subtraction / them if they're using a
deep learning programming framework

00:05:00.070 --> 00:05:00.080
deep learning programming framework
 

00:05:00.080 --> 00:05:02.390
deep learning programming framework
usually you won't have to implement the

00:05:02.390 --> 00:05:02.400
usually you won't have to implement the
 

00:05:02.400 --> 00:05:04.450
usually you won't have to implement the
bash norm step on a Shalom layer

00:05:04.450 --> 00:05:04.460
bash norm step on a Shalom layer
 

00:05:04.460 --> 00:05:07.670
bash norm step on a Shalom layer
yourself then so the program framework

00:05:07.670 --> 00:05:07.680
yourself then so the program framework
 

00:05:07.680 --> 00:05:10.310
yourself then so the program framework
so it can be one line of code so for

00:05:10.310 --> 00:05:10.320
so it can be one line of code so for
 

00:05:10.320 --> 00:05:13.400
so it can be one line of code so for
example in the tender flow framework you

00:05:13.400 --> 00:05:13.410
example in the tender flow framework you
 

00:05:13.410 --> 00:05:15.680
example in the tender flow framework you
can implement batch normalization with

00:05:15.680 --> 00:05:15.690
can implement batch normalization with
 

00:05:15.690 --> 00:05:17.990
can implement batch normalization with
you know this function will talk more

00:05:17.990 --> 00:05:18.000
you know this function will talk more
 

00:05:18.000 --> 00:05:19.700
you know this function will talk more
about programming frameworks later but

00:05:19.700 --> 00:05:19.710
about programming frameworks later but
 

00:05:19.710 --> 00:05:21.650
about programming frameworks later but
in practice you might not end up needing

00:05:21.650 --> 00:05:21.660
in practice you might not end up needing
 

00:05:21.660 --> 00:05:23.660
in practice you might not end up needing
to implement all these details yourself

00:05:23.660 --> 00:05:23.670
to implement all these details yourself
 

00:05:23.670 --> 00:05:25.760
to implement all these details yourself
but it's still worth knowing how it

00:05:25.760 --> 00:05:25.770
but it's still worth knowing how it
 

00:05:25.770 --> 00:05:26.300
but it's still worth knowing how it
works

00:05:26.300 --> 00:05:26.310
works
 

00:05:26.310 --> 00:05:28.690
works
so you can get a better understanding of

00:05:28.690 --> 00:05:28.700
so you can get a better understanding of
 

00:05:28.700 --> 00:05:31.520
so you can get a better understanding of
what your code is doing but implementing

00:05:31.520 --> 00:05:31.530
what your code is doing but implementing
 

00:05:31.530 --> 00:05:33.320
what your code is doing but implementing
backbone is often you know something

00:05:33.320 --> 00:05:33.330
backbone is often you know something
 

00:05:33.330 --> 00:05:35.420
backbone is often you know something
like one line of code in the deep

00:05:35.420 --> 00:05:35.430
like one line of code in the deep
 

00:05:35.430 --> 00:05:38.060
like one line of code in the deep
learning frameworks now so far we've

00:05:38.060 --> 00:05:38.070
learning frameworks now so far we've
 

00:05:38.070 --> 00:05:39.740
learning frameworks now so far we've
talked about bash them as if you were

00:05:39.740 --> 00:05:39.750
talked about bash them as if you were
 

00:05:39.750 --> 00:05:42.020
talked about bash them as if you were
training on your entire training side at

00:05:42.020 --> 00:05:42.030
training on your entire training side at
 

00:05:42.030 --> 00:05:44.330
training on your entire training side at
a time as if you're using batch gradient

00:05:44.330 --> 00:05:44.340
a time as if you're using batch gradient
 

00:05:44.340 --> 00:05:47.870
a time as if you're using batch gradient
descent in practice bash them is usually

00:05:47.870 --> 00:05:47.880
descent in practice bash them is usually
 

00:05:47.880 --> 00:05:50.750
descent in practice bash them is usually
applied with mini batches of your

00:05:50.750 --> 00:05:50.760
applied with mini batches of your
 

00:05:50.760 --> 00:05:52.790
applied with mini batches of your
training sets so the way you actually

00:05:52.790 --> 00:05:52.800
training sets so the way you actually
 

00:05:52.800 --> 00:05:54.650
training sets so the way you actually
apply bash them is you take your first

00:05:54.650 --> 00:05:54.660
apply bash them is you take your first
 

00:05:54.660 --> 00:06:00.470
apply bash them is you take your first
mini batch and compute z1 same as we did

00:06:00.470 --> 00:06:00.480
mini batch and compute z1 same as we did
 

00:06:00.480 --> 00:06:01.850
mini batch and compute z1 same as we did
on the previous slide using the

00:06:01.850 --> 00:06:01.860
on the previous slide using the
 

00:06:01.860 --> 00:06:06.230
on the previous slide using the
parameters W 1 b1 and then you take this

00:06:06.230 --> 00:06:06.240
parameters W 1 b1 and then you take this
 

00:06:06.240 --> 00:06:08.450
parameters W 1 b1 and then you take this
just this mini batch and compute the

00:06:08.450 --> 00:06:08.460
just this mini batch and compute the
 

00:06:08.460 --> 00:06:11.630
just this mini batch and compute the
mean and variance of the Z ones on just

00:06:11.630 --> 00:06:11.640
mean and variance of the Z ones on just
 

00:06:11.640 --> 00:06:14.770
mean and variance of the Z ones on just
this mini batch and then bash norm with

00:06:14.770 --> 00:06:14.780
this mini batch and then bash norm with
 

00:06:14.780 --> 00:06:17.420
this mini batch and then bash norm with
subtract by the mean and divide by the

00:06:17.420 --> 00:06:17.430
subtract by the mean and divide by the
 

00:06:17.430 --> 00:06:20.030
subtract by the mean and divide by the
standard deviation and then rescale by

00:06:20.030 --> 00:06:20.040
standard deviation and then rescale by
 

00:06:20.040 --> 00:06:25.610
standard deviation and then rescale by
beta 1 gamma 1 to give you V 1 and all

00:06:25.610 --> 00:06:25.620
beta 1 gamma 1 to give you V 1 and all
 

00:06:25.620 --> 00:06:27.710
beta 1 gamma 1 to give you V 1 and all
this is on the first mini batch then

00:06:27.710 --> 00:06:27.720
this is on the first mini batch then
 

00:06:27.720 --> 00:06:30.860
this is on the first mini batch then
you'll apply the activation function you

00:06:30.860 --> 00:06:30.870
you'll apply the activation function you
 

00:06:30.870 --> 00:06:35.120
you'll apply the activation function you
know to get a 1 oh and then you compute

00:06:35.120 --> 00:06:35.130
know to get a 1 oh and then you compute
 

00:06:35.130 --> 00:06:37.540
know to get a 1 oh and then you compute
V 2 using

00:06:37.540 --> 00:06:37.550
V 2 using
 

00:06:37.550 --> 00:06:40.719
V 2 using
w2 b 2 and so on

00:06:40.719 --> 00:06:40.729
w2 b 2 and so on
 

00:06:40.729 --> 00:06:43.719
w2 b 2 and so on
so you do all this in order to perform

00:06:43.719 --> 00:06:43.729
so you do all this in order to perform
 

00:06:43.729 --> 00:06:46.210
so you do all this in order to perform
one step of say gradient descent on the

00:06:46.210 --> 00:06:46.220
one step of say gradient descent on the
 

00:06:46.220 --> 00:06:48.400
one step of say gradient descent on the
first mini badge and then you go to the

00:06:48.400 --> 00:06:48.410
first mini badge and then you go to the
 

00:06:48.410 --> 00:06:51.309
first mini badge and then you go to the
second mini batch x2 and you do

00:06:51.309 --> 00:06:51.319
second mini batch x2 and you do
 

00:06:51.319 --> 00:06:53.140
second mini batch x2 and you do
something similar where will you now

00:06:53.140 --> 00:06:53.150
something similar where will you now
 

00:06:53.150 --> 00:06:55.480
something similar where will you now
compute z1 on the second mini badge and

00:06:55.480 --> 00:06:55.490
compute z1 on the second mini badge and
 

00:06:55.490 --> 00:06:58.420
compute z1 on the second mini badge and
then use batch them to compute z1 tilde

00:06:58.420 --> 00:06:58.430
then use batch them to compute z1 tilde
 

00:06:58.430 --> 00:07:02.860
then use batch them to compute z1 tilde
and so here in this - known step you'd

00:07:02.860 --> 00:07:02.870
and so here in this - known step you'd
 

00:07:02.870 --> 00:07:06.790
and so here in this - known step you'd
be normalizing veto they are using just

00:07:06.790 --> 00:07:06.800
be normalizing veto they are using just
 

00:07:06.800 --> 00:07:08.830
be normalizing veto they are using just
the data in your second mini batch so

00:07:08.830 --> 00:07:08.840
the data in your second mini batch so
 

00:07:08.840 --> 00:07:11.200
the data in your second mini batch so
this - don't stop here is looking at the

00:07:11.200 --> 00:07:11.210
this - don't stop here is looking at the
 

00:07:11.210 --> 00:07:12.969
this - don't stop here is looking at the
examples in your second mini batch

00:07:12.969 --> 00:07:12.979
examples in your second mini batch
 

00:07:12.979 --> 00:07:15.430
examples in your second mini batch
computing the mean and variance is of

00:07:15.430 --> 00:07:15.440
computing the mean and variance is of
 

00:07:15.440 --> 00:07:18.279
computing the mean and variance is of
the V ones on just a mini batch and then

00:07:18.279 --> 00:07:18.289
the V ones on just a mini batch and then
 

00:07:18.289 --> 00:07:20.860
the V ones on just a mini batch and then
rescaling by beta and gamma to get the

00:07:20.860 --> 00:07:20.870
rescaling by beta and gamma to get the
 

00:07:20.870 --> 00:07:25.270
rescaling by beta and gamma to get the
two there and so on and you do this with

00:07:25.270 --> 00:07:25.280
two there and so on and you do this with
 

00:07:25.280 --> 00:07:28.439
two there and so on and you do this with
a third mini batch and keep training

00:07:28.439 --> 00:07:28.449
a third mini batch and keep training
 

00:07:28.449 --> 00:07:31.120
a third mini batch and keep training
now that's one detail to the

00:07:31.120 --> 00:07:31.130
now that's one detail to the
 

00:07:31.130 --> 00:07:33.999
now that's one detail to the
parameterization that I want to clean up

00:07:33.999 --> 00:07:34.009
parameterization that I want to clean up
 

00:07:34.009 --> 00:07:35.950
parameterization that I want to clean up
which is previously I said that the

00:07:35.950 --> 00:07:35.960
which is previously I said that the
 

00:07:35.960 --> 00:07:40.120
which is previously I said that the
parameters was WL BL finish layer as

00:07:40.120 --> 00:07:40.130
parameters was WL BL finish layer as
 

00:07:40.130 --> 00:07:45.820
parameters was WL BL finish layer as
well as beta L and gamma L now notice

00:07:45.820 --> 00:07:45.830
well as beta L and gamma L now notice
 

00:07:45.830 --> 00:07:50.379
well as beta L and gamma L now notice
that the way VL is computed is as

00:07:50.379 --> 00:07:50.389
that the way VL is computed is as
 

00:07:50.389 --> 00:07:55.629
that the way VL is computed is as
follows ZL equals W L times a of L minus

00:07:55.629 --> 00:07:55.639
follows ZL equals W L times a of L minus
 

00:07:55.639 --> 00:08:00.640
follows ZL equals W L times a of L minus
1 plus B of L but what - column does is

00:08:00.640 --> 00:08:00.650
1 plus B of L but what - column does is
 

00:08:00.650 --> 00:08:02.320
1 plus B of L but what - column does is
going to look at the mini batch and

00:08:02.320 --> 00:08:02.330
going to look at the mini batch and
 

00:08:02.330 --> 00:08:05.379
going to look at the mini batch and
normalize dll - first set mean 0 and

00:08:05.379 --> 00:08:05.389
normalize dll - first set mean 0 and
 

00:08:05.389 --> 00:08:07.510
normalize dll - first set mean 0 and
standard variance and in a rescale by

00:08:07.510 --> 00:08:07.520
standard variance and in a rescale by
 

00:08:07.520 --> 00:08:10.330
standard variance and in a rescale by
beta and gamma but what that means is

00:08:10.330 --> 00:08:10.340
beta and gamma but what that means is
 

00:08:10.340 --> 00:08:12.909
beta and gamma but what that means is
that whatever the value of B L is

00:08:12.909 --> 00:08:12.919
that whatever the value of B L is
 

00:08:12.919 --> 00:08:14.290
that whatever the value of B L is
actually going to just get subtracted

00:08:14.290 --> 00:08:14.300
actually going to just get subtracted
 

00:08:14.300 --> 00:08:16.570
actually going to just get subtracted
out because during that bashful

00:08:16.570 --> 00:08:16.580
out because during that bashful
 

00:08:16.580 --> 00:08:18.100
out because during that bashful
normalization step you're going to

00:08:18.100 --> 00:08:18.110
normalization step you're going to
 

00:08:18.110 --> 00:08:20.709
normalization step you're going to
compute the means of the V else and and

00:08:20.709 --> 00:08:20.719
compute the means of the V else and and
 

00:08:20.719 --> 00:08:23.080
compute the means of the V else and and
subtract out the mean and so adding any

00:08:23.080 --> 00:08:23.090
subtract out the mean and so adding any
 

00:08:23.090 --> 00:08:26.860
subtract out the mean and so adding any
constant to all of the examples in a

00:08:26.860 --> 00:08:26.870
constant to all of the examples in a
 

00:08:26.870 --> 00:08:28.510
constant to all of the examples in a
mini - it doesn't change anything

00:08:28.510 --> 00:08:28.520
mini - it doesn't change anything
 

00:08:28.520 --> 00:08:30.939
mini - it doesn't change anything
because any constant you add will get

00:08:30.939 --> 00:08:30.949
because any constant you add will get
 

00:08:30.949 --> 00:08:32.829
because any constant you add will get
cancelled out by the mean subtraction

00:08:32.829 --> 00:08:32.839
cancelled out by the mean subtraction
 

00:08:32.839 --> 00:08:36.040
cancelled out by the mean subtraction
step so if you're using bash norm you

00:08:36.040 --> 00:08:36.050
step so if you're using bash norm you
 

00:08:36.050 --> 00:08:38.199
step so if you're using bash norm you
can actually eliminate that parameter or

00:08:38.199 --> 00:08:38.209
can actually eliminate that parameter or
 

00:08:38.209 --> 00:08:40.329
can actually eliminate that parameter or
if you want to think of it as setting it

00:08:40.329 --> 00:08:40.339
if you want to think of it as setting it
 

00:08:40.339 --> 00:08:42.519
if you want to think of it as setting it
permanently to zero so then the

00:08:42.519 --> 00:08:42.529
permanently to zero so then the
 

00:08:42.529 --> 00:08:45.720
permanently to zero so then the
parameterization becomes V L is just WL

00:08:45.720 --> 00:08:45.730
parameterization becomes V L is just WL
 

00:08:45.730 --> 00:08:50.980
parameterization becomes V L is just WL
x al minus 1 oh and then you compute

00:08:50.980 --> 00:08:50.990
x al minus 1 oh and then you compute
 

00:08:50.990 --> 00:08:56.940
x al minus 1 oh and then you compute
VL normalized and we compute Z tilde

00:08:56.940 --> 00:08:56.950
VL normalized and we compute Z tilde
 

00:08:56.950 --> 00:09:05.560
VL normalized and we compute Z tilde
equals gamma ZL plus beta you end up

00:09:05.560 --> 00:09:05.570
equals gamma ZL plus beta you end up
 

00:09:05.570 --> 00:09:08.320
equals gamma ZL plus beta you end up
using this parameter beta L in order to

00:09:08.320 --> 00:09:08.330
using this parameter beta L in order to
 

00:09:08.330 --> 00:09:11.889
using this parameter beta L in order to
decide what the mean of Z tilde L which

00:09:11.889 --> 00:09:11.899
decide what the mean of Z tilde L which
 

00:09:11.899 --> 00:09:15.310
decide what the mean of Z tilde L which
is what gets passed the link layer so

00:09:15.310 --> 00:09:15.320
is what gets passed the link layer so
 

00:09:15.320 --> 00:09:18.550
is what gets passed the link layer so
just a recap because bash norm zeroes

00:09:18.550 --> 00:09:18.560
just a recap because bash norm zeroes
 

00:09:18.560 --> 00:09:22.540
just a recap because bash norm zeroes
out the mean of these VL values in the

00:09:22.540 --> 00:09:22.550
out the mean of these VL values in the
 

00:09:22.550 --> 00:09:25.389
out the mean of these VL values in the
layer there's no point having this

00:09:25.389 --> 00:09:25.399
layer there's no point having this
 

00:09:25.399 --> 00:09:28.300
layer there's no point having this
parameter BL and so you might as well

00:09:28.300 --> 00:09:28.310
parameter BL and so you might as well
 

00:09:28.310 --> 00:09:30.910
parameter BL and so you might as well
get rid of it and instead it's sort of

00:09:30.910 --> 00:09:30.920
get rid of it and instead it's sort of
 

00:09:30.920 --> 00:09:34.449
get rid of it and instead it's sort of
replaced by beta L which is a parameter

00:09:34.449 --> 00:09:34.459
replaced by beta L which is a parameter
 

00:09:34.459 --> 00:09:37.150
replaced by beta L which is a parameter
that controls that ends up affecting the

00:09:37.150 --> 00:09:37.160
that controls that ends up affecting the
 

00:09:37.160 --> 00:09:40.120
that controls that ends up affecting the
shift or the bias terms finally remember

00:09:40.120 --> 00:09:40.130
shift or the bias terms finally remember
 

00:09:40.130 --> 00:09:43.600
shift or the bias terms finally remember
that the dimension of VL because if

00:09:43.600 --> 00:09:43.610
that the dimension of VL because if
 

00:09:43.610 --> 00:09:45.100
that the dimension of VL because if
you're doing this on one example is

00:09:45.100 --> 00:09:45.110
you're doing this on one example is
 

00:09:45.110 --> 00:09:50.680
you're doing this on one example is
going to be NL by one and so BL at

00:09:50.680 --> 00:09:50.690
going to be NL by one and so BL at
 

00:09:50.690 --> 00:09:54.130
going to be NL by one and so BL at
dimension n L by 1 if n L is the number

00:09:54.130 --> 00:09:54.140
dimension n L by 1 if n L is the number
 

00:09:54.140 --> 00:09:57.699
dimension n L by 1 if n L is the number
of hidden units in layer L and so the

00:09:57.699 --> 00:09:57.709
of hidden units in layer L and so the
 

00:09:57.709 --> 00:10:00.760
of hidden units in layer L and so the
dimension of beta L and gamma L is also

00:10:00.760 --> 00:10:00.770
dimension of beta L and gamma L is also
 

00:10:00.770 --> 00:10:05.170
dimension of beta L and gamma L is also
going to be ml by one because um that's

00:10:05.170 --> 00:10:05.180
going to be ml by one because um that's
 

00:10:05.180 --> 00:10:07.389
going to be ml by one because um that's
the number of hidden juniors you have

00:10:07.389 --> 00:10:07.399
the number of hidden juniors you have
 

00:10:07.399 --> 00:10:10.630
the number of hidden juniors you have
you have NL hidden units and so beta and

00:10:10.630 --> 00:10:10.640
you have NL hidden units and so beta and
 

00:10:10.640 --> 00:10:13.600
you have NL hidden units and so beta and
gamma L are used to scale the mean and

00:10:13.600 --> 00:10:13.610
gamma L are used to scale the mean and
 

00:10:13.610 --> 00:10:16.269
gamma L are used to scale the mean and
variance of each of the hidden units to

00:10:16.269 --> 00:10:16.279
variance of each of the hidden units to
 

00:10:16.279 --> 00:10:18.550
variance of each of the hidden units to
whatever the network wants to set them

00:10:18.550 --> 00:10:18.560
whatever the network wants to set them
 

00:10:18.560 --> 00:10:21.400
whatever the network wants to set them
to so let's go all together and describe

00:10:21.400 --> 00:10:21.410
to so let's go all together and describe
 

00:10:21.410 --> 00:10:23.740
to so let's go all together and describe
how you can implement gradient descent

00:10:23.740 --> 00:10:23.750
how you can implement gradient descent
 

00:10:23.750 --> 00:10:26.110
how you can implement gradient descent
using rational assuming you're using

00:10:26.110 --> 00:10:26.120
using rational assuming you're using
 

00:10:26.120 --> 00:10:29.500
using rational assuming you're using
mini-batch gradient descent you iterate

00:10:29.500 --> 00:10:29.510
mini-batch gradient descent you iterate
 

00:10:29.510 --> 00:10:32.800
mini-batch gradient descent you iterate
for T equals 1 to the number of mini

00:10:32.800 --> 00:10:32.810
for T equals 1 to the number of mini
 

00:10:32.810 --> 00:10:38.790
for T equals 1 to the number of mini
batches you would implement for prop on

00:10:38.790 --> 00:10:38.800
batches you would implement for prop on
 

00:10:38.800 --> 00:10:42.430
batches you would implement for prop on
mini-batch XT and during forward prop in

00:10:42.430 --> 00:10:42.440
mini-batch XT and during forward prop in
 

00:10:42.440 --> 00:10:47.680
mini-batch XT and during forward prop in
each hidden layer um use bash norm to

00:10:47.680 --> 00:10:47.690
each hidden layer um use bash norm to
 

00:10:47.690 --> 00:10:54.610
each hidden layer um use bash norm to
replace VL with Z tilde L and so this

00:10:54.610 --> 00:10:54.620
replace VL with Z tilde L and so this
 

00:10:54.620 --> 00:10:57.880
replace VL with Z tilde L and so this
ensures that within that mini badge the

00:10:57.880 --> 00:10:57.890
ensures that within that mini badge the
 

00:10:57.890 --> 00:11:00.910
ensures that within that mini badge the
value V end up with some normalized mean

00:11:00.910 --> 00:11:00.920
value V end up with some normalized mean
 

00:11:00.920 --> 00:11:01.390
value V end up with some normalized mean
and the

00:11:01.390 --> 00:11:01.400
and the
 

00:11:01.400 --> 00:11:03.519
and the
and the values and the version with the

00:11:03.519 --> 00:11:03.529
and the values and the version with the
 

00:11:03.529 --> 00:11:06.040
and the values and the version with the
normalize mean and variance is this Z

00:11:06.040 --> 00:11:06.050
normalize mean and variance is this Z
 

00:11:06.050 --> 00:11:13.480
normalize mean and variance is this Z
tilde L and then you use back prop to

00:11:13.480 --> 00:11:13.490
tilde L and then you use back prop to
 

00:11:13.490 --> 00:11:20.440
tilde L and then you use back prop to
compute DW DB for all the values of L D

00:11:20.440 --> 00:11:20.450
compute DW DB for all the values of L D
 

00:11:20.450 --> 00:11:25.390
compute DW DB for all the values of L D
beta P gamma although technically since

00:11:25.390 --> 00:11:25.400
beta P gamma although technically since
 

00:11:25.400 --> 00:11:27.550
beta P gamma although technically since
we've gotten rid of B this actually now

00:11:27.550 --> 00:11:27.560
we've gotten rid of B this actually now
 

00:11:27.560 --> 00:11:31.780
we've gotten rid of B this actually now
goes away and then finally you update

00:11:31.780 --> 00:11:31.790
goes away and then finally you update
 

00:11:31.790 --> 00:11:35.410
goes away and then finally you update
the parameters so you know W gets

00:11:35.410 --> 00:11:35.420
the parameters so you know W gets
 

00:11:35.420 --> 00:11:38.740
the parameters so you know W gets
updated as W minus the learning rate

00:11:38.740 --> 00:11:38.750
updated as W minus the learning rate
 

00:11:38.750 --> 00:11:42.430
updated as W minus the learning rate
times this as usual beta is updated as

00:11:42.430 --> 00:11:42.440
times this as usual beta is updated as
 

00:11:42.440 --> 00:11:46.990
times this as usual beta is updated as
theta minus the learning rate times DB

00:11:46.990 --> 00:11:47.000
theta minus the learning rate times DB
 

00:11:47.000 --> 00:11:50.650
theta minus the learning rate times DB
and similarly for gamma and if you've

00:11:50.650 --> 00:11:50.660
and similarly for gamma and if you've
 

00:11:50.660 --> 00:11:52.810
and similarly for gamma and if you've
computed the gradients as follows you

00:11:52.810 --> 00:11:52.820
computed the gradients as follows you
 

00:11:52.820 --> 00:11:55.390
computed the gradients as follows you
can use gradient descent that's what

00:11:55.390 --> 00:11:55.400
can use gradient descent that's what
 

00:11:55.400 --> 00:11:57.660
can use gradient descent that's what
I've written down here but this also

00:11:57.660 --> 00:11:57.670
I've written down here but this also
 

00:11:57.670 --> 00:12:00.610
I've written down here but this also
works with gradient descents with

00:12:00.610 --> 00:12:00.620
works with gradient descents with
 

00:12:00.620 --> 00:12:07.120
works with gradient descents with
momentum or rmsprop um or Adam where

00:12:07.120 --> 00:12:07.130
momentum or rmsprop um or Adam where
 

00:12:07.130 --> 00:12:09.280
momentum or rmsprop um or Adam where
instead of taking this gradient descent

00:12:09.280 --> 00:12:09.290
instead of taking this gradient descent
 

00:12:09.290 --> 00:12:11.320
instead of taking this gradient descent
update you can use the updates given by

00:12:11.320 --> 00:12:11.330
update you can use the updates given by
 

00:12:11.330 --> 00:12:14.680
update you can use the updates given by
these other algorithms as we discuss in

00:12:14.680 --> 00:12:14.690
these other algorithms as we discuss in
 

00:12:14.690 --> 00:12:17.320
these other algorithms as we discuss in
the previous week's videos so these are

00:12:17.320 --> 00:12:17.330
the previous week's videos so these are
 

00:12:17.330 --> 00:12:18.910
the previous week's videos so these are
the optimization algorithms as well can

00:12:18.910 --> 00:12:18.920
the optimization algorithms as well can
 

00:12:18.920 --> 00:12:21.370
the optimization algorithms as well can
be used to update the parameters beta

00:12:21.370 --> 00:12:21.380
be used to update the parameters beta
 

00:12:21.380 --> 00:12:24.760
be used to update the parameters beta
and gamma that passional added to your

00:12:24.760 --> 00:12:24.770
and gamma that passional added to your
 

00:12:24.770 --> 00:12:26.680
and gamma that passional added to your
algorithm so I hope that gives you a

00:12:26.680 --> 00:12:26.690
algorithm so I hope that gives you a
 

00:12:26.690 --> 00:12:28.630
algorithm so I hope that gives you a
sense of how you could implement a song

00:12:28.630 --> 00:12:28.640
sense of how you could implement a song
 

00:12:28.640 --> 00:12:30.760
sense of how you could implement a song
from scratch if you want to do if you're

00:12:30.760 --> 00:12:30.770
from scratch if you want to do if you're
 

00:12:30.770 --> 00:12:31.960
from scratch if you want to do if you're
using one of the deep learning

00:12:31.960 --> 00:12:31.970
using one of the deep learning
 

00:12:31.970 --> 00:12:33.370
using one of the deep learning
programming frameworks which we'll talk

00:12:33.370 --> 00:12:33.380
programming frameworks which we'll talk
 

00:12:33.380 --> 00:12:35.440
programming frameworks which we'll talk
more about later hopefully you can just

00:12:35.440 --> 00:12:35.450
more about later hopefully you can just
 

00:12:35.450 --> 00:12:37.600
more about later hopefully you can just
call someone else's implementation in

00:12:37.600 --> 00:12:37.610
call someone else's implementation in
 

00:12:37.610 --> 00:12:39.310
call someone else's implementation in
the Prima framework which would make

00:12:39.310 --> 00:12:39.320
the Prima framework which would make
 

00:12:39.320 --> 00:12:42.340
the Prima framework which would make
using bash storm much easier now in case

00:12:42.340 --> 00:12:42.350
using bash storm much easier now in case
 

00:12:42.350 --> 00:12:43.750
using bash storm much easier now in case
national still seems a little bit

00:12:43.750 --> 00:12:43.760
national still seems a little bit
 

00:12:43.760 --> 00:12:45.820
national still seems a little bit
mysterious um if you know still not

00:12:45.820 --> 00:12:45.830
mysterious um if you know still not
 

00:12:45.830 --> 00:12:48.340
mysterious um if you know still not
quite sure why speeds up training so

00:12:48.340 --> 00:12:48.350
quite sure why speeds up training so
 

00:12:48.350 --> 00:12:50.920
quite sure why speeds up training so
dramatically let's go to the next video

00:12:50.920 --> 00:12:50.930
dramatically let's go to the next video
 

00:12:50.930 --> 00:12:53.440
dramatically let's go to the next video
and talk more about why - Tom really

00:12:53.440 --> 00:12:53.450
and talk more about why - Tom really
 

00:12:53.450 --> 00:12:56.920
and talk more about why - Tom really
works and what is really doing

