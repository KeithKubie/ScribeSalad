WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.864
[MUSIC PLAYING]

00:00:04.864 --> 00:00:06.280
JEFF DEAN: All
right, thanks Zach.

00:00:06.280 --> 00:00:08.910
And thank you all for
coming, and welcome everyone

00:00:08.910 --> 00:00:09.922
on the livesteam.

00:00:09.922 --> 00:00:12.380
We're really, really excited
about the TensorFlow developer

00:00:12.380 --> 00:00:13.820
summit.

00:00:13.820 --> 00:00:16.900
So when we open sourced
TensorFlow about a little bit

00:00:16.900 --> 00:00:18.700
more than a year
ago, we really were

00:00:18.700 --> 00:00:20.980
hoping to build a
machine learning platform

00:00:20.980 --> 00:00:22.409
for everyone in the world.

00:00:22.409 --> 00:00:23.950
It's very clear that
machine learning

00:00:23.950 --> 00:00:25.408
is going to be
incredibly important

00:00:25.408 --> 00:00:27.940
across many, many different
kinds of applications

00:00:27.940 --> 00:00:30.400
and problem domains across
many fields of science,

00:00:30.400 --> 00:00:31.650
across many industries.

00:00:31.650 --> 00:00:34.150
And we wanted to build an open
source platform that everyone

00:00:34.150 --> 00:00:39.140
could share and could contribute
to that was fast, flexible,

00:00:39.140 --> 00:00:40.240
and production-ready.

00:00:40.240 --> 00:00:42.070
And that's really the
kind of three mantras

00:00:42.070 --> 00:00:43.780
that we've been
working towards as we

00:00:43.780 --> 00:00:46.610
continue to evolve TensorFlow.

00:00:46.610 --> 00:00:48.170
But first, a bit of history.

00:00:48.170 --> 00:00:51.950
So our group has been doing
deep learning research for about

00:00:51.950 --> 00:00:53.690
five, five and a half years.

00:00:53.690 --> 00:00:56.660
And the first system we
built was called Disbelief.

00:00:56.660 --> 00:00:59.480
And this was a system
that worked well for us

00:00:59.480 --> 00:01:01.040
in our early work in this area.

00:01:01.040 --> 00:01:05.000
So we published a paper
about Disbelief in NIPS 2012,

00:01:05.000 --> 00:01:07.020
it was well received.

00:01:07.020 --> 00:01:09.170
Disbelief had a lot of
really nice properties.

00:01:09.170 --> 00:01:12.710
It was very scalable,
so we could train models

00:01:12.710 --> 00:01:14.450
in very large
numbers of computers,

00:01:14.450 --> 00:01:16.739
we could train very
large, powerful models.

00:01:16.739 --> 00:01:18.530
We did a lot of
interesting work with that,

00:01:18.530 --> 00:01:20.580
and so did other
people at Google,

00:01:20.580 --> 00:01:22.850
including some of the
early inception image

00:01:22.850 --> 00:01:25.190
work, a lot of work with
our speech recognition team

00:01:25.190 --> 00:01:27.470
to improve our speech
recognition systems,

00:01:27.470 --> 00:01:29.900
work with our search team to
improve our search ranking

00:01:29.900 --> 00:01:32.480
system by deploying neural nets
in the middle of the search

00:01:32.480 --> 00:01:34.220
stack.

00:01:34.220 --> 00:01:36.350
But ultimately we realized
that Disbelief was not

00:01:36.350 --> 00:01:38.180
really exactly what
we wanted as we

00:01:38.180 --> 00:01:40.150
got more experience
with applying

00:01:40.150 --> 00:01:41.150
these kinds of problems.

00:01:41.150 --> 00:01:43.116
It was designed for
CPUs because that's

00:01:43.116 --> 00:01:44.990
what we had in our data
centers when we first

00:01:44.990 --> 00:01:46.310
started this work.

00:01:46.310 --> 00:01:48.586
GPU support was added
later, but it was kind of

00:01:48.586 --> 00:01:49.460
bolted onto the side.

00:01:49.460 --> 00:01:52.670
It wasn't really thought through
as much as we would have liked.

00:01:52.670 --> 00:01:54.200
And the system
worked really well

00:01:54.200 --> 00:01:56.840
if what you wanted to express
worked in the programming model

00:01:56.840 --> 00:01:57.890
that Disbelief had.

00:01:57.890 --> 00:01:59.480
But if you wanted to do
something a little bit

00:01:59.480 --> 00:02:01.470
outside of what it was
originally designed for,

00:02:01.470 --> 00:02:02.970
it was a little bit
hard, especially

00:02:02.970 --> 00:02:04.320
for research purposes.

00:02:04.320 --> 00:02:07.040
So what we wanted to
do with TensorFlow

00:02:07.040 --> 00:02:12.280
is take a step back and redesign
what we had with Disbelief,

00:02:12.280 --> 00:02:14.030
and keep the nice
properties of Disbelief,

00:02:14.030 --> 00:02:16.400
but make it more flexible
and more general purpose.

00:02:16.400 --> 00:02:19.910
And so TensorFlow really
takes the scalable properties

00:02:19.910 --> 00:02:22.280
that Disbelief had and
the production readiness,

00:02:22.280 --> 00:02:25.010
but then extends it to be much
more flexible for research.

00:02:25.010 --> 00:02:27.740
And so we really want to scan
this whole continuum of really

00:02:27.740 --> 00:02:31.190
flexible ideas, and then
being able to take those ideas

00:02:31.190 --> 00:02:33.140
and put them all the
way into production

00:02:33.140 --> 00:02:35.670
without having to go into
a new kind of system.

00:02:35.670 --> 00:02:37.889
So that's why we
built TensorFlow,

00:02:37.889 --> 00:02:40.430
and we wanted to open source it
so that everyone in the world

00:02:40.430 --> 00:02:42.110
could actually use it.

00:02:42.110 --> 00:02:44.120
We also felt it was really
important to support

00:02:44.120 --> 00:02:45.110
many platforms.

00:02:45.110 --> 00:02:46.700
Disbelief was really
designed mostly

00:02:46.700 --> 00:02:50.120
for data center applications,
but obviously mobile platforms

00:02:50.120 --> 00:02:53.270
are really important, being
able to run workloads on CPUs,

00:02:53.270 --> 00:02:56.390
and GPUs, and custom
accelerators like PPUs

00:02:56.390 --> 00:02:58.370
that Google has, and
that other startup

00:02:58.370 --> 00:03:01.010
companies and large companies
are developing in this space

00:03:01.010 --> 00:03:04.520
as machine learning acceleration
becomes more important.

00:03:04.520 --> 00:03:06.590
And running on a variety
of mobile platforms.

00:03:06.590 --> 00:03:09.710
Android and iOS support
and Raspberry Pi support

00:03:09.710 --> 00:03:11.540
have been added in
TensorFlow because we

00:03:11.540 --> 00:03:13.970
want to be able to take
models and run them wherever

00:03:13.970 --> 00:03:16.770
machine learning needs occur.

00:03:16.770 --> 00:03:19.020
And we also want to be able
to run on cloud platforms.

00:03:19.020 --> 00:03:22.440
So you can run this on Google
Cloud or other cloud systems

00:03:22.440 --> 00:03:25.560
using TensorFlow
running in raw VMs.

00:03:25.560 --> 00:03:28.019
Google Cloud also has a
high level cloud machine

00:03:28.019 --> 00:03:30.060
merging service that is
more of a managed service

00:03:30.060 --> 00:03:31.268
where you can run TensorFlow.

00:03:31.268 --> 00:03:35.409
And so TensorFlow works really
well on Google Cloud, as well.

00:03:35.409 --> 00:03:37.450
And we wanted to add
support for other languages.

00:03:37.450 --> 00:03:42.510
So when we initially released
TensorFlow we had Python

00:03:42.510 --> 00:03:43.530
and c++ support.

00:03:43.530 --> 00:03:45.606
And over time we and
others in the community

00:03:45.606 --> 00:03:47.730
have added other language
bindings and other higher

00:03:47.730 --> 00:03:50.100
level language support so
that different languages can

00:03:50.100 --> 00:03:52.910
access the power of TensorFlow.

00:03:52.910 --> 00:03:54.830
We've also worked
pretty hard on making it

00:03:54.830 --> 00:03:57.320
so that we can have
good visualization tools

00:03:57.320 --> 00:04:01.094
and monitoring tools for
machine learning, training,

00:04:01.094 --> 00:04:03.260
and inference so that you
can really understand what

00:04:03.260 --> 00:04:04.426
it is your models are doing.

00:04:04.426 --> 00:04:07.280
Really getting a
good sense of what's

00:04:07.280 --> 00:04:09.500
happening in those
processes allows researchers

00:04:09.500 --> 00:04:11.150
to be more productive,
and to really

00:04:11.150 --> 00:04:14.644
dive into what's going on.

00:04:14.644 --> 00:04:16.269
So we've had a lot
of progress to date,

00:04:16.269 --> 00:04:18.660
so I'll just highlight
some of that.

00:04:18.660 --> 00:04:23.057
So the area to the left is
mostly our Disbelief system,

00:04:23.057 --> 00:04:25.140
and we use that for a
variety of different things.

00:04:25.140 --> 00:04:27.450
But you can really
see within Google

00:04:27.450 --> 00:04:30.090
that as we released
TensorFlow internally,

00:04:30.090 --> 00:04:33.240
and then eventually as an open
source product, that we've

00:04:33.240 --> 00:04:35.190
been able to have
many, many more people

00:04:35.190 --> 00:04:37.770
and more teams at Google
use machine learning,

00:04:37.770 --> 00:04:41.470
and use TensorFlow to
improve both our products,

00:04:41.470 --> 00:04:44.340
so it's integrated into many
features in search, and Gmail,

00:04:44.340 --> 00:04:46.830
and translating maps,
and many other things,

00:04:46.830 --> 00:04:49.090
but also to use it
for research use.

00:04:49.090 --> 00:04:50.760
So we've done
hundreds of projects

00:04:50.760 --> 00:04:53.460
that use TensorFlow,
published hundreds of papers

00:04:53.460 --> 00:04:55.484
using TensorFlow as the
basis for our machine

00:04:55.484 --> 00:04:56.233
learning research.

00:05:00.680 --> 00:05:03.440
And it's also now the number
one repository in machine

00:05:03.440 --> 00:05:04.820
learning category on GitHub.

00:05:04.820 --> 00:05:06.278
So we're pretty
excited about that.

00:05:06.278 --> 00:05:08.900
It's been really well received
in the external community.

00:05:08.900 --> 00:05:12.260
And you can see that
TensorFlow adoption has really

00:05:12.260 --> 00:05:14.460
accelerated quite quickly.

00:05:14.460 --> 00:05:17.127
And I think that's
testament to the community

00:05:17.127 --> 00:05:19.460
that we've started to build
and are continuing to build,

00:05:19.460 --> 00:05:21.807
where lots of people are
contributing to TensorFlow

00:05:21.807 --> 00:05:24.140
and using it for all kinds
of interesting and innovative

00:05:24.140 --> 00:05:25.520
ways.

00:05:25.520 --> 00:05:28.820
We have almost 500
non-google contributors

00:05:28.820 --> 00:05:31.790
have contributed
code to TensorFlow.

00:05:31.790 --> 00:05:36.620
We have about 1,000 commits per
month in the source code base.

00:05:36.620 --> 00:05:38.540
Many, many people
outside in the community

00:05:38.540 --> 00:05:41.480
have created tutorials,
and other kinds of model

00:05:41.480 --> 00:05:43.610
implantations and
TensorFlow translations

00:05:43.610 --> 00:05:46.230
of the documentation,
interesting projects.

00:05:46.230 --> 00:05:48.530
There's about 5,500
GitHub repositories now

00:05:48.530 --> 00:05:50.900
with TensorFlow in the
title, only four or five

00:05:50.900 --> 00:05:52.720
of which are the
official repositories.

00:05:52.720 --> 00:05:55.220
And we've had a lot of direct
engagement with this community

00:05:55.220 --> 00:05:57.620
by answering questions
on Stack Overflow

00:05:57.620 --> 00:05:59.690
dealing with issue
and feature requests

00:05:59.690 --> 00:06:00.882
that people have submitted.

00:06:00.882 --> 00:06:02.840
And one of the things
we're really happy to see

00:06:02.840 --> 00:06:04.880
is that the use in
machine learning classes

00:06:04.880 --> 00:06:07.380
is starting to happen in
different universities.

00:06:07.380 --> 00:06:09.900
So Toronto, Berkeley, and
Stanford, for example,

00:06:09.900 --> 00:06:13.370
are all using TensorFlow as
the basis of important machine

00:06:13.370 --> 00:06:17.610
learning classes in
their curriculum.

00:06:17.610 --> 00:06:20.310
Since we launched, we've been
adding features and working

00:06:20.310 --> 00:06:22.230
with the community
to understand what

00:06:22.230 --> 00:06:25.380
features are really important
that they want to see.

00:06:25.380 --> 00:06:29.016
And just to highlight
a few, TensorFlow

00:06:29.016 --> 00:06:31.140
distributed training support
was added a few months

00:06:31.140 --> 00:06:33.210
after our initial launch.

00:06:33.210 --> 00:06:35.490
Various high level
APIs have been

00:06:35.490 --> 00:06:37.557
added so that people
can more easily express

00:06:37.557 --> 00:06:39.390
the kinds of models
they want without having

00:06:39.390 --> 00:06:42.220
to necessarily specify things
that are very low level.

00:06:42.220 --> 00:06:43.890
Things like TensorFlow Slim.

00:06:43.890 --> 00:06:46.560
We've added Windows
support, and for example,

00:06:46.560 --> 00:06:49.830
HTFS support, support for
CUDA [INAUDIBLE] and CuDNN.

00:06:49.830 --> 00:06:52.879
Many, many things here that the
community has been asking for

00:06:52.879 --> 00:06:54.420
have made it into
different releases,

00:06:54.420 --> 00:06:57.240
and we've kept up about a
release every couple of months

00:06:57.240 --> 00:06:58.200
cadence.

00:06:58.200 --> 00:07:00.009
Minor release.

00:07:00.009 --> 00:07:02.050
And all of this has happened
in just over a year.

00:07:02.050 --> 00:07:03.133
So this is pretty amazing.

00:07:03.133 --> 00:07:05.400
This community is really
vibrant and thriving.

00:07:05.400 --> 00:07:07.859
I'm really excited to be part
of a big open source project.

00:07:07.859 --> 00:07:10.108
A lot of the work that I've
done in Google in the past

00:07:10.108 --> 00:07:11.580
has been not open
souce projects,

00:07:11.580 --> 00:07:13.710
and it's really nice to
see the external usage

00:07:13.710 --> 00:07:17.302
of the TensorFlow system, and
what people are doing with it.

00:07:20.620 --> 00:07:24.670
I'm going to invite Rajat Monga,
our TensorFlow engineering

00:07:24.670 --> 00:07:27.550
lead, up to talk about
what's new today.

00:07:27.550 --> 00:07:29.770
Thank you.

00:07:29.770 --> 00:07:32.650
RAJAT MONGA: Thank you, Jeff.

00:07:32.650 --> 00:07:34.410
Thank you.

00:07:34.410 --> 00:07:35.640
Hello, everyone.

00:07:35.640 --> 00:07:38.330
So there are lots of
amazing announcements today.

00:07:38.330 --> 00:07:40.290
And let's get started.

00:07:40.290 --> 00:07:42.820
Let me start with the first
one and the most exciting one.

00:07:42.820 --> 00:07:51.920
Today we are excited to
announce TensorFlow 1.0.

00:07:51.920 --> 00:07:54.650
Jeff talked about a number of
things that have come together

00:07:54.650 --> 00:07:56.487
to really get us to 1.0.

00:07:56.487 --> 00:07:58.820
And before I get into more
of the things, the new things

00:07:58.820 --> 00:08:00.470
that we're announcing
today, let's

00:08:00.470 --> 00:08:03.194
take a moment to just thank
all the wonderful people who

00:08:03.194 --> 00:08:04.110
have made this happen.

00:08:04.110 --> 00:08:06.830
So kudos to an amazing team
that's come together to make

00:08:06.830 --> 00:08:08.450
TensorFlow a great product.

00:08:08.450 --> 00:08:10.302
Thanks to all of
you, the community,

00:08:10.302 --> 00:08:11.510
who's really gotten together.

00:08:11.510 --> 00:08:13.310
There are nearly
500 contributors,

00:08:13.310 --> 00:08:16.430
as Jeff mentioned, from outside
Google who've contributed code

00:08:16.430 --> 00:08:17.840
back to TensorFlow.

00:08:17.840 --> 00:08:19.964
And the many others on
Stack Overflow and GitHub

00:08:19.964 --> 00:08:22.130
who are helping with issues,
and support, and so on.

00:08:22.130 --> 00:08:24.144
So thank you all.

00:08:24.144 --> 00:08:26.060
Let's get started, let's
take a look at what's

00:08:26.060 --> 00:08:28.940
there in TensorFlow 1.0.

00:08:28.940 --> 00:08:29.680
It's fast.

00:08:29.680 --> 00:08:32.650
Performance has always
been important to us.

00:08:32.650 --> 00:08:34.150
It's one of the
things that's really

00:08:34.150 --> 00:08:35.525
helped deep learning
get to where

00:08:35.525 --> 00:08:38.110
it is to give the great
results that it's been getting.

00:08:38.110 --> 00:08:43.234
And today with 1.0 we're showing
a 58X speed up on 64 GPUs

00:08:43.234 --> 00:08:44.650
running the inception
v3, which is

00:08:44.650 --> 00:08:49.240
one of the great benchmarks
for image performance.

00:08:49.240 --> 00:08:50.404
It's flexible.

00:08:50.404 --> 00:08:52.195
Jeff mentioned some of
the high level APIs,

00:08:52.195 --> 00:08:53.540
we're announcing even more.

00:08:53.540 --> 00:08:56.410
We want to make it easy for
you to do all the things.

00:08:56.410 --> 00:08:57.910
TensorFlow's
philosophy has always

00:08:57.910 --> 00:09:02.330
been to give you the power
to do whatever you want,

00:09:02.330 --> 00:09:05.260
but also make it easy, and
this makes it even easier.

00:09:05.260 --> 00:09:08.510
And it's production-ready.

00:09:08.510 --> 00:09:10.870
With 1.0 and semantic
versioning that will follow,

00:09:10.870 --> 00:09:14.800
we're guaranteeing that we'll
ensure backwards compatibility

00:09:14.800 --> 00:09:16.060
for all future changes.

00:09:16.060 --> 00:09:18.910
So as you get new
versions and new features,

00:09:18.910 --> 00:09:21.460
you don't have to worry
about breaking things.

00:09:21.460 --> 00:09:24.640
And many more as some of the
guests later will talk about.

00:09:24.640 --> 00:09:26.600
But let's take a look
at some of these.

00:09:26.600 --> 00:09:29.540
Let's start at the
high level APIs.

00:09:29.540 --> 00:09:31.250
One of our goals has
always been to bring

00:09:31.250 --> 00:09:32.970
machine learning to everyone.

00:09:32.970 --> 00:09:34.970
And these high level APIs
really make it easier,

00:09:34.970 --> 00:09:36.840
and hopefully will
get to more people.

00:09:36.840 --> 00:09:39.245
So let's take a look
at what they are.

00:09:39.245 --> 00:09:40.620
This is a picture
that's probably

00:09:40.620 --> 00:09:44.240
familiar to a lot of you if
you've been using TensorFlow.

00:09:44.240 --> 00:09:46.910
In the middle is the core
distributed execution engine

00:09:46.910 --> 00:09:49.440
written in c++ for
high performance.

00:09:49.440 --> 00:09:51.200
It abstracts away
all the various kinds

00:09:51.200 --> 00:09:53.590
of devices, some of which
Jeff showed earlier,

00:09:53.590 --> 00:09:55.640
like CPUs, GPUs, and so on.

00:09:55.640 --> 00:09:56.930
And does a great job at that.

00:09:56.930 --> 00:09:59.150
On top of it are the
various languages,

00:09:59.150 --> 00:10:00.870
like the Python front
and the c++ front.

00:10:00.870 --> 00:10:02.536
And again, many
different languages that

00:10:02.536 --> 00:10:03.980
are being supported there.

00:10:03.980 --> 00:10:05.940
Now today we're announcing
a number of APIs

00:10:05.940 --> 00:10:08.000
on top of the Python front end.

00:10:08.000 --> 00:10:10.250
The first one is layers.

00:10:10.250 --> 00:10:13.880
So these allow really, if you're
using deep learning models,

00:10:13.880 --> 00:10:16.760
to build fully connected
convolution pooling and all

00:10:16.760 --> 00:10:18.260
the standard layers
that you come up

00:10:18.260 --> 00:10:21.290
with as pre-built components so
you don't have to really start

00:10:21.290 --> 00:10:23.060
making them from scratch.

00:10:23.060 --> 00:10:25.080
Again, making it easier for you.

00:10:25.080 --> 00:10:28.520
On top of that, Keras is
an API that a lot of you

00:10:28.520 --> 00:10:30.060
are probably familiar with.

00:10:30.060 --> 00:10:31.010
It's pretty popular.

00:10:31.010 --> 00:10:33.030
And we are announcing
the integration of Keras

00:10:33.030 --> 00:10:35.660
with TensorFlow,
and implementation

00:10:35.660 --> 00:10:37.970
to make it, again, easier
to build those models.

00:10:37.970 --> 00:10:41.660
And an estimated API that really
helps you take those models,

00:10:41.660 --> 00:10:43.820
combine them with your
data sets, et cetera,

00:10:43.820 --> 00:10:47.150
make it easy for you to train
and evaluate these models.

00:10:47.150 --> 00:10:49.105
And finally, on top
of all of these,

00:10:49.105 --> 00:10:51.230
there are a number of models
that are already there

00:10:51.230 --> 00:10:52.800
for you in the box for you.

00:10:52.800 --> 00:10:56.660
For example, linear regression,
simple DNNs, and so on,

00:10:56.660 --> 00:10:59.390
that you can start with
that you can actually

00:10:59.390 --> 00:11:02.750
run on your core to train stuff
with very few lines of code.

00:11:05.800 --> 00:11:08.350
Next up, so
TensorFlow's is always

00:11:08.350 --> 00:11:10.870
done great in deep learning,
but we built the core

00:11:10.870 --> 00:11:12.490
to be really
flexible to allow you

00:11:12.490 --> 00:11:14.950
to do all kinds of
amazing things with it.

00:11:14.950 --> 00:11:17.800
And today we are
announcing broad machine

00:11:17.800 --> 00:11:20.440
learning support with the help
of many teams across Google

00:11:20.440 --> 00:11:22.490
and folks externally, as well.

00:11:22.490 --> 00:11:25.680
So let's take a look at
everything that we are doing.

00:11:25.680 --> 00:11:27.690
Of course as with
everything else,

00:11:27.690 --> 00:11:29.171
performance is
important for these.

00:11:29.171 --> 00:11:31.170
So all of these implementations
are pretty fast,

00:11:31.170 --> 00:11:33.419
and you can take advantage
of them on a single machine

00:11:33.419 --> 00:11:36.234
or distributed in a cluster.

00:11:36.234 --> 00:11:37.650
If you're trying
to do clustering,

00:11:37.650 --> 00:11:39.320
K-means is an
obvious choice, so we

00:11:39.320 --> 00:11:43.220
have an implementation of
K-means in TensorFlow for you.

00:11:43.220 --> 00:11:45.890
If you're doing classification,
support vector machines

00:11:45.890 --> 00:11:47.090
are used often.

00:11:47.090 --> 00:11:49.640
And this might be a great place
to start, especially if you

00:11:49.640 --> 00:11:51.547
have predefined features.

00:11:51.547 --> 00:11:53.755
But then going from there,
if you want to, let's say,

00:11:53.755 --> 00:11:57.080
learn features or do something
fancier, the same kind of APIs

00:11:57.080 --> 00:11:59.930
are there for you to try deep
learning or other algorithms

00:11:59.930 --> 00:12:01.842
as well.

00:12:01.842 --> 00:12:03.800
We also have decision
trees and random forests,

00:12:03.800 --> 00:12:06.200
again for classification
and regression.

00:12:06.200 --> 00:12:07.276
And many, many more.

00:12:07.276 --> 00:12:08.900
Ashish will be talking
more and in more

00:12:08.900 --> 00:12:10.270
detail about these later today.

00:12:13.730 --> 00:12:16.810
Let's look at some new
platforms and accelerators.

00:12:16.810 --> 00:12:18.820
Jeff showed a whole
number of bunch of them

00:12:18.820 --> 00:12:20.470
that were there earlier.

00:12:20.470 --> 00:12:24.960
I'm just going to mention a
few that we've added recently.

00:12:24.960 --> 00:12:28.000
TensorFlow's included in
IBM's PowerAI distribution.

00:12:28.000 --> 00:12:29.960
IBM announced this
pretty recently.

00:12:29.960 --> 00:12:33.220
So if you're using IBM power
systems for deep learning,

00:12:33.220 --> 00:12:35.200
now there's a
software package that

00:12:35.200 --> 00:12:38.410
includes TensorFlow to give you
high performance and the ease

00:12:38.410 --> 00:12:41.620
of use that you
would like there.

00:12:41.620 --> 00:12:43.300
It's also supported
by accelerators

00:12:43.300 --> 00:12:46.330
such as the Movidius
Myriad 2 accelerator.

00:12:46.330 --> 00:12:50.320
This can be used for
embedded platforms.

00:12:50.320 --> 00:12:52.160
It takes the TensorFlow
modeling and really

00:12:52.160 --> 00:12:55.880
allows you to run that
efficiently on these chips.

00:12:55.880 --> 00:12:59.360
And most recently, there's been
support for Qualcomm's Hexagon

00:12:59.360 --> 00:13:00.500
DSP.

00:13:00.500 --> 00:13:02.030
And to talk about
that, I would like

00:13:02.030 --> 00:13:04.520
to welcome Travis Lanier
who's here with us

00:13:04.520 --> 00:13:05.840
from Qualcomm today.

00:13:09.055 --> 00:13:13.089
Travis is senior product
managing director at Qualcomm.

00:13:13.089 --> 00:13:14.130
TRAVIS LANIER: Thank you.

00:13:18.435 --> 00:13:20.060
We're very excited
to have collaborated

00:13:20.060 --> 00:13:24.800
with Google to optimize
TensorFlow for Hexagon DSP.

00:13:24.800 --> 00:13:27.650
If you take a look at this
little video we have looping up

00:13:27.650 --> 00:13:30.920
here, you can see about
an 8x performance increase

00:13:30.920 --> 00:13:33.710
by moving it from
the CPU to the DSP.

00:13:33.710 --> 00:13:36.410
And keep in mind, this is
happening with a mobile phone's

00:13:36.410 --> 00:13:37.290
power profile.

00:13:37.290 --> 00:13:40.246
So this is very power
efficient, really high

00:13:40.246 --> 00:13:41.620
power processing
compared to what

00:13:41.620 --> 00:13:43.203
you've been able to
see for these type

00:13:43.203 --> 00:13:45.002
neural tasks on a mobile phone.

00:13:47.885 --> 00:13:49.510
Now the good news is
that we've already

00:13:49.510 --> 00:13:50.870
done the optimization for you.

00:13:50.870 --> 00:13:53.759
So if you have a TensorFlow
model and you move it over,

00:13:53.759 --> 00:13:56.050
you should be able to see a
similar type of performance

00:13:56.050 --> 00:14:01.080
uplift on our Hexagon DSP.

00:14:01.080 --> 00:14:05.430
We will be open sourcing this
software, including the driver,

00:14:05.430 --> 00:14:08.100
and it'll be available on our
Dragonboard 820 development

00:14:08.100 --> 00:14:08.830
platform.

00:14:08.830 --> 00:14:09.374
Thank you.

00:14:14.120 --> 00:14:15.620
RAJAT MONGA: Thank you, Travis.

00:14:15.620 --> 00:14:17.621
So Pete Warden will be
talking in more detail

00:14:17.621 --> 00:14:19.370
about our support for
mobile and embedded.

00:14:19.370 --> 00:14:20.990
It's clearly very,
very important,

00:14:20.990 --> 00:14:25.220
and you can see more in
his talk later today.

00:14:25.220 --> 00:14:27.230
So we care about
performance in many ways.

00:14:27.230 --> 00:14:30.502
You've seen some of those areas.

00:14:30.502 --> 00:14:32.710
As you think of TensorFlow
as a programming languages

00:14:32.710 --> 00:14:35.030
here building these
amazing models,

00:14:35.030 --> 00:14:37.370
what better way to look at
performance than the compiler

00:14:37.370 --> 00:14:38.690
compilation techniques?

00:14:38.690 --> 00:14:40.700
So today we are
announcing XLA, which

00:14:40.700 --> 00:14:43.310
is an experimental
TensorFlow compiler.

00:14:43.310 --> 00:14:45.050
It's really built
for TensorFlow,

00:14:45.050 --> 00:14:48.530
so a compiler for TensorFlow.

00:14:48.530 --> 00:14:50.722
XLA stands for accelerator
linear algebra,

00:14:50.722 --> 00:14:52.430
basically accelerates
the kinds of models

00:14:52.430 --> 00:14:54.740
you make with TensorFlow.

00:14:54.740 --> 00:14:58.830
It basically takes a graph
and compiles that down

00:14:58.830 --> 00:15:00.780
to assembly code for
the architecture that

00:15:00.780 --> 00:15:03.450
matters to you for the kind
of chip that you are running.

00:15:03.450 --> 00:15:06.060
And it can be used for
just in time compilation,

00:15:06.060 --> 00:15:07.520
for example in
training workloads,

00:15:07.520 --> 00:15:09.240
et cetera, whether
in the data center

00:15:09.240 --> 00:15:12.175
or elsewhere, and also for
ahead of time compilation

00:15:12.175 --> 00:15:14.550
where it just takes the graphs,
compiles it down to code,

00:15:14.550 --> 00:15:16.008
and then you can
just ship the code

00:15:16.008 --> 00:15:17.434
and not worry about
anything else.

00:15:17.434 --> 00:15:19.350
Again, Chris and Todd
will be talking about it

00:15:19.350 --> 00:15:22.490
later this morning, so
there's a lot more there.

00:15:22.490 --> 00:15:24.950
And with that, I
would like to welcome

00:15:24.950 --> 00:15:27.467
Megan Kacholia, engineering
director at Brain who's

00:15:27.467 --> 00:15:29.300
been leading our
performance efforts to talk

00:15:29.300 --> 00:15:30.383
about TensorFlow in depth.

00:15:36.062 --> 00:15:38.020
MEGAN KACHOLIA: Thanks, Rajat.

00:15:38.020 --> 00:15:41.180
I want to start just by diving
a bit more into performance.

00:15:41.180 --> 00:15:42.980
So as we've talked
about, TensorFlow

00:15:42.980 --> 00:15:44.750
is really made for the
community at large.

00:15:44.750 --> 00:15:46.550
And that spans a lot
of different use cases,

00:15:46.550 --> 00:15:48.980
whether you're looking at things
from the research perspective

00:15:48.980 --> 00:15:51.050
or thinking about just
production environments

00:15:51.050 --> 00:15:52.034
and what that means.

00:15:52.034 --> 00:15:54.200
On the research side, being
able to have performance

00:15:54.200 --> 00:15:56.460
improvements, get more and more
performance out of TensorFlow

00:15:56.460 --> 00:15:58.610
and out of the machines
you're using underneath it

00:15:58.610 --> 00:16:00.200
gives you a lot of flexibility.

00:16:00.200 --> 00:16:01.950
Whether it's want to
iterate more quickly,

00:16:01.950 --> 00:16:03.533
you want to train
models faster, maybe

00:16:03.533 --> 00:16:05.450
you want to run more
experiments in parallel.

00:16:05.450 --> 00:16:07.022
So it's very
important for us to be

00:16:07.022 --> 00:16:09.230
able to continue to improve
on performance to provide

00:16:09.230 --> 00:16:11.180
that flexibility on
the research side.

00:16:11.180 --> 00:16:12.740
But we don't want to forget
about the production side,

00:16:12.740 --> 00:16:13.292
as well.

00:16:13.292 --> 00:16:15.500
And for production there's,
again, many different use

00:16:15.500 --> 00:16:16.100
cases.

00:16:16.100 --> 00:16:18.530
Whether you're looking at
large scale server farms,

00:16:18.530 --> 00:16:20.361
or also thinking about
embedded use cases

00:16:20.361 --> 00:16:21.360
like we've talked about.

00:16:21.360 --> 00:16:23.930
Whether it's on device,
running on a mobile device,

00:16:23.930 --> 00:16:25.610
or some other kind
of smaller device

00:16:25.610 --> 00:16:27.020
that has a special setup.

00:16:27.020 --> 00:16:28.670
And again, even
then we're thinking

00:16:28.670 --> 00:16:30.500
about lots of different
types of hardware,

00:16:30.500 --> 00:16:32.540
and low latency serving
is also part of this.

00:16:32.540 --> 00:16:34.190
I think a lot of times
people think of TensorFlow

00:16:34.190 --> 00:16:36.080
and they think of just
the training perspective,

00:16:36.080 --> 00:16:38.163
but we've done a lot of
work in open-source things

00:16:38.163 --> 00:16:41.000
through our initial
releases that provide

00:16:41.000 --> 00:16:42.140
ways of serving as well.

00:16:42.140 --> 00:16:43.640
This is low latency
serving that you

00:16:43.640 --> 00:16:45.636
can use in a production setup.

00:16:45.636 --> 00:16:47.510
But what good is it to
talk about performance

00:16:47.510 --> 00:16:49.970
without actually
looking at numbers?

00:16:49.970 --> 00:16:52.460
So this is just a
little setup first.

00:16:52.460 --> 00:16:53.840
Let me explain some of this.

00:16:53.840 --> 00:16:56.390
So what we're
looking at here is we

00:16:56.390 --> 00:16:59.300
have revised our implementation
of the Inception v3 model,

00:16:59.300 --> 00:17:01.450
and we're currently working
on open sourcing that,

00:17:01.450 --> 00:17:03.110
and we'll have that
out in a few weeks.

00:17:03.110 --> 00:17:05.780
But all of the graphs shown
here are using that model

00:17:05.780 --> 00:17:06.994
under the covers.

00:17:06.994 --> 00:17:09.410
And then we're looking across
two different machine types.

00:17:09.410 --> 00:17:13.020
So we've done some benchmarking
on the NVIDIA DGX-1, a very,

00:17:13.020 --> 00:17:14.629
very powerful
machine, so that we

00:17:14.629 --> 00:17:16.670
can see how TensorFlow
really scales when you are

00:17:16.670 --> 00:17:19.040
using much, much faster GPUs.

00:17:19.040 --> 00:17:21.594
And also looking at K80s, since
those are much more widely

00:17:21.594 --> 00:17:23.510
available, especially
in all the various cloud

00:17:23.510 --> 00:17:26.192
computing platforms
that are out there.

00:17:26.192 --> 00:17:27.650
And this is just
the ideal scaling.

00:17:27.650 --> 00:17:29.762
So let's see how
TensorFlow stacks up.

00:17:29.762 --> 00:17:31.220
So the first thing
we're looking at

00:17:31.220 --> 00:17:33.800
is, again, looking at training
this Inception v3 model

00:17:33.800 --> 00:17:35.182
with synthetic data.

00:17:35.182 --> 00:17:36.890
I think the really
cool thing to call out

00:17:36.890 --> 00:17:39.320
here is that even when
you're moving from a K80

00:17:39.320 --> 00:17:41.967
to a much more powerful
machine like a DGX-1,

00:17:41.967 --> 00:17:43.550
you're still seeing
impressive scaling

00:17:43.550 --> 00:17:46.460
numbers across the eight
GPUs that we're showing here.

00:17:46.460 --> 00:17:49.100
But it's not good to just
look at synthetic data.

00:17:49.100 --> 00:17:52.190
I know that's what is
generally used for benchmarking

00:17:52.190 --> 00:17:53.690
in the industry,
but we also want

00:17:53.690 --> 00:17:55.010
to understand, how
do things actually

00:17:55.010 --> 00:17:55.990
perform with real data?

00:17:55.990 --> 00:17:57.140
Since that's what
I'm assuming people

00:17:57.140 --> 00:17:59.000
are actually using for
building their models,

00:17:59.000 --> 00:18:00.458
and running their
production setup,

00:18:00.458 --> 00:18:01.820
and doing their research.

00:18:01.820 --> 00:18:03.960
So let's plop that
in there, as well.

00:18:03.960 --> 00:18:06.620
Now you can see even when
you move from synthetic data

00:18:06.620 --> 00:18:09.650
to real data, the scaling
is still very impressive.

00:18:09.650 --> 00:18:12.170
And TensorFlow is scaling
well across the eight GPUs

00:18:12.170 --> 00:18:15.140
here, whether you're looking
at a powerful DGX-1 setup

00:18:15.140 --> 00:18:18.890
or you're looking at
something like a K80, as well.

00:18:18.890 --> 00:18:21.520
But it's not just about
the single machine setups,

00:18:21.520 --> 00:18:23.920
we also need to think about
distributed performance.

00:18:23.920 --> 00:18:25.990
And for distributed
performance as well, we've

00:18:25.990 --> 00:18:28.240
been doing a lot of work to
improve that, and improve

00:18:28.240 --> 00:18:30.370
our scaling to make sure that,
again, whatever machinery

00:18:30.370 --> 00:18:32.110
you have and that
you want to be using,

00:18:32.110 --> 00:18:35.142
TensorFlow can make
the most use of it

00:18:35.142 --> 00:18:36.850
and be able to give
you whatever benefits

00:18:36.850 --> 00:18:38.270
and flexibility you need.

00:18:38.270 --> 00:18:41.680
So here we're showing a 58x
speed up across 64 GPUs.

00:18:41.680 --> 00:18:43.720
This is the number that
Rajat quoted as well

00:18:43.720 --> 00:18:46.489
when we were talking about
the TensorFlow 1.0 release.

00:18:46.489 --> 00:18:49.030
So I think it's really exciting,
just all of the improvements

00:18:49.030 --> 00:18:51.113
that we've made on the
performance side of things,

00:18:51.113 --> 00:18:52.750
and all the flexibility
and the options

00:18:52.750 --> 00:18:55.524
that it gives back
to the community.

00:18:55.524 --> 00:18:56.940
Now I want to go
just a little bit

00:18:56.940 --> 00:18:58.940
into examples on the
research and the production

00:18:58.940 --> 00:19:00.770
side of things.

00:19:00.770 --> 00:19:03.140
So neural machine translation
as a common example we've

00:19:03.140 --> 00:19:04.340
talked about a lot.

00:19:04.340 --> 00:19:07.304
So here you can see with the
old phrase-based model, which

00:19:07.304 --> 00:19:08.720
actually had
hundreds of thousands

00:19:08.720 --> 00:19:10.580
of lines of hand-tuned code.

00:19:10.580 --> 00:19:13.850
When we moved that to a
neural model in TensorFlow,

00:19:13.850 --> 00:19:16.040
we're actually able to
see huge improvements

00:19:16.040 --> 00:19:18.620
across various language pairs,
in some cases reducing errors

00:19:18.620 --> 00:19:21.230
by up to 85%.

00:19:21.230 --> 00:19:22.690
Here's a slightly
more fun example.

00:19:22.690 --> 00:19:24.660
It's a little interesting
to look at this graph.

00:19:24.660 --> 00:19:26.310
But this is neural
architecture search,

00:19:26.310 --> 00:19:27.996
where you're
essentially building

00:19:27.996 --> 00:19:30.120
a neural network in TensorFlow
that can figure out,

00:19:30.120 --> 00:19:31.560
what is the right
under the covers

00:19:31.560 --> 00:19:33.859
neural architecture to
use for solving a problem?

00:19:33.859 --> 00:19:36.150
And it's great to see these
cutting-edge research cases

00:19:36.150 --> 00:19:39.740
also being done in TensorFlow.

00:19:39.740 --> 00:19:43.190
And here's another example
that has been open-sourced

00:19:43.190 --> 00:19:44.640
by Google Research.

00:19:44.640 --> 00:19:46.525
So looking at things
like captioning images.

00:19:46.525 --> 00:19:48.650
And it's not just about
providing a caption for it,

00:19:48.650 --> 00:19:51.050
it's providing a
human-sounding caption for it.

00:19:51.050 --> 00:19:53.510
So it's very impressive to
combine this type of language

00:19:53.510 --> 00:19:56.390
understanding and also
applying that with the images

00:19:56.390 --> 00:20:00.130
and doing that in TensorFlow.

00:20:00.130 --> 00:20:01.850
But again, it's not
just about research.

00:20:01.850 --> 00:20:04.058
We also need to look at
different types of production

00:20:04.058 --> 00:20:05.030
use cases, as well.

00:20:05.030 --> 00:20:06.966
And it's important
to, again, remember

00:20:06.966 --> 00:20:09.340
that TensorFlow, when we talk
about production use cases,

00:20:09.340 --> 00:20:11.020
we're talking about both
the training side of things,

00:20:11.020 --> 00:20:13.600
and also using TensorFlow
for low latency serving.

00:20:16.300 --> 00:20:18.451
A cool example here is
something like Word Lens.

00:20:18.451 --> 00:20:20.950
So this is something where, a
lot of times when people think

00:20:20.950 --> 00:20:24.400
about production use cases, they
think about large data centers.

00:20:24.400 --> 00:20:25.810
But a lot of
production use cases

00:20:25.810 --> 00:20:27.466
these days are actually apps.

00:20:27.466 --> 00:20:28.840
You release an
app, it's actually

00:20:28.840 --> 00:20:30.250
running on someone's device.

00:20:30.250 --> 00:20:32.320
So here's an example
with Word Lens,

00:20:32.320 --> 00:20:35.920
where it's combining translation
and vision, essentially,

00:20:35.920 --> 00:20:37.480
and actually running
on the device

00:20:37.480 --> 00:20:40.450
using TensorFlow to
be able to provide you

00:20:40.450 --> 00:20:44.127
what that sign might look
like in a different language.

00:20:44.127 --> 00:20:45.710
And of course at
Google, we make a lot

00:20:45.710 --> 00:20:48.212
of use of tons of TensorFlow
and deep learning.

00:20:48.212 --> 00:20:49.670
So we talked about
translate a lot.

00:20:49.670 --> 00:20:51.500
There are other
examples, like using it

00:20:51.500 --> 00:20:54.170
in Gmail for spam
detection, using it

00:20:54.170 --> 00:20:58.190
in Street View for understanding
signs, street sign recognition.

00:20:58.190 --> 00:21:00.560
And also using it in,
perhaps, the Google Play

00:21:00.560 --> 00:21:02.630
Store for app recommendations.

00:21:02.630 --> 00:21:04.910
But while it's great that
there's so many use cases

00:21:04.910 --> 00:21:06.620
internally at Google,
that's not really

00:21:06.620 --> 00:21:08.062
the whole point of TensorFlow.

00:21:08.062 --> 00:21:09.770
The point of TensorFlow
is to figure out,

00:21:09.770 --> 00:21:11.520
how can we give this
back to the community

00:21:11.520 --> 00:21:14.180
and make it so that everyone can
be making use of deep learning

00:21:14.180 --> 00:21:16.070
and be able to use
TensorFlow to further,

00:21:16.070 --> 00:21:18.290
whether is the research
or the production needs?

00:21:18.290 --> 00:21:21.710
So it's great to have such
a large list of companies

00:21:21.710 --> 00:21:24.320
that are actually making
use of TensorFlow as well.

00:21:24.320 --> 00:21:25.821
So we've already
heard from Qualcomm

00:21:25.821 --> 00:21:28.153
stepping up, will be other
things that will be mentioned

00:21:28.153 --> 00:21:29.470
throughout the day, as well.

00:21:29.470 --> 00:21:30.620
But we really want
to make sure we're

00:21:30.620 --> 00:21:32.578
engaging with the community,
understanding what

00:21:32.578 --> 00:21:35.330
the needs are so that over time
this list of other folks who

00:21:35.330 --> 00:21:37.700
are using TensorFlow
in their own use cases

00:21:37.700 --> 00:21:39.006
will continue to grow.

00:21:39.006 --> 00:21:41.130
And with that, I'd like to
hand it back off to Jeff

00:21:41.130 --> 00:21:45.415
to talk a bit more
about what's ahead.

00:21:45.415 --> 00:21:46.581
JEFF DEAN: Thank you, Megan.

00:21:49.640 --> 00:21:51.290
So I think one of
the things that's

00:21:51.290 --> 00:21:53.749
great about the community
is that TensorFlow is really

00:21:53.749 --> 00:21:54.290
for everyone.

00:21:54.290 --> 00:21:55.730
And some of the
things that people

00:21:55.730 --> 00:21:58.640
have done with TensorFlow are
things that we would never

00:21:58.640 --> 00:21:59.430
have thought of.

00:21:59.430 --> 00:22:01.940
Because really, people
have their own problems,

00:22:01.940 --> 00:22:04.980
and they're using TensorFlow
in very creative ways.

00:22:04.980 --> 00:22:07.600
So you may have seen this
story a few months ago

00:22:07.600 --> 00:22:10.206
of this Japanese cucumber
farmer and his wife

00:22:10.206 --> 00:22:12.830
and their son, who happens to be
an automotive embedded systems

00:22:12.830 --> 00:22:13.730
engineer.

00:22:13.730 --> 00:22:17.870
And every harvest
season, the wife

00:22:17.870 --> 00:22:19.910
would spend a lot of
time sorting cucumbers.

00:22:19.910 --> 00:22:22.460
Because you need to sort them
for market into big ones,

00:22:22.460 --> 00:22:25.790
small ones, curved
ones, not curved ones.

00:22:25.790 --> 00:22:28.400
And so they rigged up a
system using a Raspberry

00:22:28.400 --> 00:22:31.400
Pi and a camera and
TensorFlow to essentially

00:22:31.400 --> 00:22:34.340
sort the cucumbers automatically
into the right shapes.

00:22:34.340 --> 00:22:36.554
And so this is a creative
use of TensorFlow

00:22:36.554 --> 00:22:37.970
that we would not
have thought of.

00:22:37.970 --> 00:22:39.830
It's really cool that
people are finding

00:22:39.830 --> 00:22:41.510
really neat uses for it.

00:22:41.510 --> 00:22:43.994
And now there's not
a lot of human labor

00:22:43.994 --> 00:22:46.160
that needs to be done to
sort these cucumbers, which

00:22:46.160 --> 00:22:49.570
doesn't sound like a
very interesting task.

00:22:49.570 --> 00:22:51.220
Another area we're
really excited about

00:22:51.220 --> 00:22:54.190
is the applications of
machine learning for medicine.

00:22:54.190 --> 00:22:57.580
And our group has
been doing some work

00:22:57.580 --> 00:22:59.830
on the diagnosis of
diabetic retinopathy

00:22:59.830 --> 00:23:02.050
using computer vision.

00:23:02.050 --> 00:23:04.800
So the task here is essentially
to take a retinal image

00:23:04.800 --> 00:23:06.550
like this-- you go to
the ophthalmologist,

00:23:06.550 --> 00:23:08.650
they typically take
an image like this.

00:23:08.650 --> 00:23:11.230
And then the ophthalmologist
looks at this

00:23:11.230 --> 00:23:16.060
to see if there are any early
signs of diabetic retinopathy.

00:23:16.060 --> 00:23:18.490
And this is really important.

00:23:18.490 --> 00:23:21.407
Because if you catch this
disease early enough,

00:23:21.407 --> 00:23:22.240
it's very treatable.

00:23:22.240 --> 00:23:24.281
But if you don't, it's
actually the leading cause

00:23:24.281 --> 00:23:26.390
of blindness in the world.

00:23:26.390 --> 00:23:29.692
And so we now have a
system in an article that

00:23:29.692 --> 00:23:31.900
was published in the "Journal
of the American Medical

00:23:31.900 --> 00:23:34.330
Association" by
people in our group

00:23:34.330 --> 00:23:36.940
that shows that a computer
vision model trained

00:23:36.940 --> 00:23:38.560
on the right kind
of data can actually

00:23:38.560 --> 00:23:41.930
be as good, or slightly better
than the median ophthalmologist

00:23:41.930 --> 00:23:43.562
at diagnosing
diabetic retinopathy.

00:23:43.562 --> 00:23:45.520
And this is really
important, because [MIC CUT]

00:23:45.520 --> 00:23:46.410
to the world--

00:23:49.200 --> 00:23:51.830
and what's really
lacking is access

00:23:51.830 --> 00:23:55.210
to ophthalmologists to
actually interpret the images.

00:23:55.210 --> 00:23:59.010
And so this can really
help increase access.

00:23:59.010 --> 00:24:01.410
There's other uses
in medicine, as well.

00:24:01.410 --> 00:24:05.040
So I'd like to invite Brett
Coprell from Stanford.

00:24:05.040 --> 00:24:07.622
He's a Stanford PhD student
in the EE department,

00:24:07.622 --> 00:24:09.330
and he's been doing
some really nice work

00:24:09.330 --> 00:24:12.797
on dermatology assessment with
the computer vision models.

00:24:12.797 --> 00:24:13.380
Thanks, Brett.

00:24:13.380 --> 00:24:14.170
BRETT COPRELL: Hey, everyone.

00:24:14.170 --> 00:24:14.920
I'm Brett Coprell.

00:24:18.444 --> 00:24:20.610
My colleagues and I at
Stanford have trained a model

00:24:20.610 --> 00:24:23.430
using TensorFlow to classify
images of dangerous skin

00:24:23.430 --> 00:24:24.120
lesions.

00:24:24.120 --> 00:24:26.970
We're thrilled to have achieved
dermatologist-level performance

00:24:26.970 --> 00:24:30.840
at recognizing melanoma and
other types of skin cancer.

00:24:30.840 --> 00:24:34.419
At a sensitivity of
98%, we're 90% specific.

00:24:34.419 --> 00:24:36.960
I look forward to sharing the
details with you during my talk

00:24:36.960 --> 00:24:37.850
later today.

00:24:37.850 --> 00:24:41.172
And if you have a copy of
"Nature," I'm happy to sign it.

00:24:41.172 --> 00:24:43.620
JEFF DEAN: Very good,
all right, thank you.

00:24:43.620 --> 00:24:46.080
Thank you, Brett.

00:24:46.080 --> 00:24:48.000
The cover article in
"Nature" is like the kind

00:24:48.000 --> 00:24:50.420
of really pillar of achievement
in the field of science,

00:24:50.420 --> 00:24:52.420
so this is really
fantastic work.

00:24:52.420 --> 00:24:55.200
And we actually had a
corkboard outside the venue

00:24:55.200 --> 00:24:57.690
here today to talk about
other kinds of creative uses

00:24:57.690 --> 00:24:59.310
that people are
doing for TensorFlow,

00:24:59.310 --> 00:25:01.350
and I just picked a couple.

00:25:01.350 --> 00:25:04.790
So continuing on
the medical theme,

00:25:04.790 --> 00:25:07.230
one group is doing
classification and clustering

00:25:07.230 --> 00:25:10.650
of MRI based brain networks to
diagnose psychiatric disorders

00:25:10.650 --> 00:25:11.810
and plan treatment.

00:25:11.810 --> 00:25:13.500
So that's pretty cool.

00:25:13.500 --> 00:25:15.810
And another one is
doing a painting robot,

00:25:15.810 --> 00:25:17.810
which sounds pretty cool also.

00:25:17.810 --> 00:25:20.550
So I think it's just really nice
to see all the creative uses

00:25:20.550 --> 00:25:24.600
that the community is making
for putting TensorFlow to use

00:25:24.600 --> 00:25:27.060
in solving their
problems, and also

00:25:27.060 --> 00:25:29.130
that the community is
contributing to improving

00:25:29.130 --> 00:25:30.960
the base core TensorFlow.

00:25:30.960 --> 00:25:33.900
So we're really excited
to have everyone here,

00:25:33.900 --> 00:25:35.490
both in person and
on the livestream

00:25:35.490 --> 00:25:38.190
for our first
TensorFlow dev summit.

00:25:38.190 --> 00:25:41.010
And we want to see all the
amazing things that you

00:25:41.010 --> 00:25:42.930
guys can do with TensorFlow.

00:25:42.930 --> 00:25:44.170
So thank you very much.

00:25:46.639 --> 00:25:48.180
ZAK STONE: It's been
just over a year

00:25:48.180 --> 00:25:49.880
since we open
sourced TensorFlow,

00:25:49.880 --> 00:25:51.810
and we've been thrilled
to see the adoption

00:25:51.810 --> 00:25:54.240
by the community and
the pace of development,

00:25:54.240 --> 00:25:56.666
both here at Google and
all around the world.

00:25:56.666 --> 00:25:58.820
JEFF DEAN: TensorFlow is
really the primary tool

00:25:58.820 --> 00:26:00.769
that we're using for
a lot of our machine

00:26:00.769 --> 00:26:02.310
learning work in
all of our products.

00:26:02.310 --> 00:26:04.110
Toward the end of
last year, we actually

00:26:04.110 --> 00:26:07.050
rolled out a completely
new translation system that

00:26:07.050 --> 00:26:08.660
was based on deep neural nets.

00:26:08.660 --> 00:26:11.999
In Gmail, we were actually able
to roll out a TensorFlow model

00:26:11.999 --> 00:26:14.040
that, by understanding
the context of the message

00:26:14.040 --> 00:26:16.440
you just received, we can
predict likely replies

00:26:16.440 --> 00:26:19.050
and this is a feature
we call Smart Reply.

00:26:19.050 --> 00:26:21.720
LILY PENG: Diabetic retinopathy
is the fastest growing cause

00:26:21.720 --> 00:26:22.440
of blindness.

00:26:22.440 --> 00:26:24.450
It's a complication of diabetes.

00:26:24.450 --> 00:26:26.520
We gathered a very
large data set

00:26:26.520 --> 00:26:28.890
and had doctors
grade the images.

00:26:28.890 --> 00:26:31.410
And then we, using TensorFlow,
trained a neural net

00:26:31.410 --> 00:26:34.140
that does a pretty good job
of predicting whether or not

00:26:34.140 --> 00:26:37.350
there is diabetic
retinopathy in the image.

00:26:37.350 --> 00:26:40.187
DOUG ECK: Can we use something
like TensorFlow to make music,

00:26:40.187 --> 00:26:42.270
to make art, and to allow
us to communicate better

00:26:42.270 --> 00:26:43.510
with each other?

00:26:43.510 --> 00:26:45.660
With TensorFlow, we're
able to think abstractly,

00:26:45.660 --> 00:26:47.834
almost at a level of
like improvisation

00:26:47.834 --> 00:26:48.750
with machine learning.

00:26:48.750 --> 00:26:51.150
We're able to try new
things, to chunk models

00:26:51.150 --> 00:26:53.220
together in ways that
were impossible before we

00:26:53.220 --> 00:26:54.794
had that kind of expressivity.

00:27:00.222 --> 00:27:01.680
AMANDA HODGSON:
Dugongs are classed

00:27:01.680 --> 00:27:04.570
as vulnerable to
extinction globally.

00:27:04.570 --> 00:27:07.400
So we do a lot of aerial
surveys using drones.

00:27:07.400 --> 00:27:11.370
Then once you've done a
survey of a really large area,

00:27:11.370 --> 00:27:14.100
you end up with tens, if
not hundreds of thousands

00:27:14.100 --> 00:27:15.690
of photos.

00:27:15.690 --> 00:27:19.720
The goal was to find a way to
automate that whole process.

00:27:19.720 --> 00:27:22.135
And that's where we've
been using TensorFlow.

00:27:22.135 --> 00:27:23.760
ZAK STONE: One of
the things that we've

00:27:23.760 --> 00:27:26.880
been focusing on this year
with TensorFlow is performance.

00:27:26.880 --> 00:27:29.370
We've been especially
excited to release support

00:27:29.370 --> 00:27:30.990
for distributed training.

00:27:30.990 --> 00:27:32.406
MEGAN KACHOLIA:
We want to make it

00:27:32.406 --> 00:27:35.340
easier for people to use so they
don't have to necessarily know

00:27:35.340 --> 00:27:38.430
all of the underlying internals
in order to get the distributed

00:27:38.430 --> 00:27:40.100
performance the best it can be.

00:27:40.100 --> 00:27:42.797
[INAUDIBLE] is something that
can compile down TensorFlow.

00:27:42.797 --> 00:27:44.880
Maybe you want to compile
your graph ahead of time

00:27:44.880 --> 00:27:47.130
and get it down to
something much more compact

00:27:47.130 --> 00:27:49.877
in terms of memory size, so
that that way you can easily

00:27:49.877 --> 00:27:51.960
load it and execute it on
something that might not

00:27:51.960 --> 00:27:54.780
have as much storage
space, like a mobile phone,

00:27:54.780 --> 00:27:57.179
or some other portable,
smaller device.

00:27:57.179 --> 00:27:59.720
RICK MAULE: When we introduced
the Hexagon vector extensions,

00:27:59.720 --> 00:28:04.040
what we had in mind was
enhancing user experiences

00:28:04.040 --> 00:28:05.649
with imaging features.

00:28:05.649 --> 00:28:07.190
ERICH PLONDKE: So
the TensorFlow team

00:28:07.190 --> 00:28:09.560
said that you only needed
low precision multipliers

00:28:09.560 --> 00:28:12.197
to be able to execute these
neural networks efficiently.

00:28:12.197 --> 00:28:14.780
So we did some tests, and on the
same graph, Inception section

00:28:14.780 --> 00:28:17.540
v3, we were eight times
faster and four times lower

00:28:17.540 --> 00:28:20.000
power than running on the CPUs.

00:28:20.000 --> 00:28:21.920
RICK MAULE: TensorFlow
is great to work

00:28:21.920 --> 00:28:24.890
with, easy to work with,
lots of capability.

00:28:24.890 --> 00:28:27.350
So our engineering teams
and their engineering teams

00:28:27.350 --> 00:28:30.914
working together, we were able
to do something very exciting.

00:28:30.914 --> 00:28:32.330
This is just the
beginning of what

00:28:32.330 --> 00:28:34.790
will end up being a long
evolution of some great things

00:28:34.790 --> 00:28:37.992
we can do with machine
learning and image processing.

00:28:37.992 --> 00:28:39.950
JOSH GORDON: In addition
to sharing TensorFlow,

00:28:39.950 --> 00:28:42.170
Google has also
shared a ecosystem

00:28:42.170 --> 00:28:44.300
of tools, which
contains everything

00:28:44.300 --> 00:28:47.300
you need to go all the way
from research to production.

00:28:47.300 --> 00:28:49.580
One such tool is
TensorFlow Serving,

00:28:49.580 --> 00:28:53.241
and this is a open source, high
performance serving solution.

00:28:53.241 --> 00:28:55.490
Another great tool, which
is actually quite beautiful,

00:28:55.490 --> 00:28:57.410
is the embedding visualizer.

00:28:57.410 --> 00:28:59.150
And you can use the
embedding visualizer

00:28:59.150 --> 00:29:03.110
to interactively explore
high dimensional data sets.

00:29:03.110 --> 00:29:04.890
On the education
side, General Assembly

00:29:04.890 --> 00:29:06.960
has done great work
teaching TensorFlow.

00:29:06.960 --> 00:29:09.260
NEHEMIAH LOURY: For
my final project,

00:29:09.260 --> 00:29:13.190
I was really interested in
doing lyrics generation.

00:29:13.190 --> 00:29:16.520
And TensorFlow was a really
great match for that,

00:29:16.520 --> 00:29:20.270
because it allowed me to build
out and utilize the models

00:29:20.270 --> 00:29:21.902
that I needed to be successful.

00:29:21.902 --> 00:29:23.360
ZAK STONE: The
TensorFlow community

00:29:23.360 --> 00:29:26.090
is thriving around
the world, and we're

00:29:26.090 --> 00:29:29.360
excited about as many people
as possible being part of it.

00:29:29.360 --> 00:29:32.787
TensorFlow is an open
source project for everyone.

00:29:32.787 --> 00:29:34.370
We're looking forward
to building this

00:29:34.370 --> 00:29:37.780
into something even better, and
more useful, and more powerful

00:29:37.780 --> 00:29:41.410
in collaboration with the
whole worldwide community.

