WEBVTT
Kind: captions
Language: en

00:00:03.080 --> 00:00:06.420
Hi, I’m Adriene Hill, and welcome back to
Crash Course Statistics.

00:00:06.420 --> 00:00:11.700
You might have heard that Power Posing affects how powerful you feel and can change hormone levels.

00:00:11.700 --> 00:00:15.700
If it does we’d expect to see that effect
over and over and over.

00:00:15.700 --> 00:00:16.700
Study after study.

00:00:16.700 --> 00:00:20.420
And it would be pretty disappointing if one
study concludes that eating carrots improves

00:00:20.420 --> 00:00:26.039
your vision, and then after you rushed to
sign up for monthly carrot deliveries...5

00:00:26.039 --> 00:00:30.300
similar studies find no evidence that munching carrots is good for your eyes.

00:00:30.300 --> 00:00:34.240
To make sure that an experimental result is
sound, we want to replicate the findings.

00:00:34.240 --> 00:00:36.360
Results need to be confirmed.

00:00:36.360 --> 00:00:39.720
Which is why replication--re-running studies
to confirm results

00:00:39.720 --> 00:00:44.960
--and reproducible analysis--the ability for
other scientists to repeat the analyses you

00:00:44.960 --> 00:00:48.510
did on your data--are essential in science.

00:00:48.510 --> 00:00:53.480
These issues affect basically every field
from Artificial Intelligence research to social science.

00:00:53.480 --> 00:01:02.880
INTRO

00:01:02.890 --> 00:01:07.780
A few years ago scientists at a biotech company called Amgen decided to try to replicate more

00:01:07.790 --> 00:01:10.600
than 50 big-deal cancer treatment studies.

00:01:10.600 --> 00:01:13.780
These were studies that had been published in respected journals.

00:01:13.780 --> 00:01:19.530
And the Amgen scientists were only able to
replicate the original results 11-percent

00:01:19.530 --> 00:01:20.100
of the time.

00:01:20.100 --> 00:01:26.280
In another reproducibility study...a group
of 270 scientists re-ran 100 psychology studies

00:01:26.280 --> 00:01:30.219
that had been published in 2008 in top-notch journals.

00:01:30.219 --> 00:01:33.689
Fewer than half of the published results were replicated.

00:01:33.689 --> 00:01:37.799
Stanford researcher, Dr. John Ioannidis has
claimed that “false findings may be the

00:01:37.799 --> 00:01:42.429
majority or even the vast majority of published research claims”.

00:01:42.429 --> 00:01:46.600
The journal Nature published a survey a few years back and asked researchers if they

00:01:46.600 --> 00:01:50.869
thought there was a reproducibility crisis
in science.

00:01:50.869 --> 00:01:56.939
52% called it a “significant crisis” another
38% called it a “slight crisis”.

00:01:56.940 --> 00:02:02.060
And 90% of researchers thinking they have
some size of crisis on their hands is big deal.

00:02:02.060 --> 00:02:07.040
The “replicability crisis” has been used
in political debates to undermine scientific

00:02:07.040 --> 00:02:08.310
research.

00:02:08.310 --> 00:02:12.450
Political activists, especially those that
hold opinions that run counter to scientific

00:02:12.450 --> 00:02:18.360
research, have jumped on the problem of replicability as a way to discredit science more broadly.

00:02:18.360 --> 00:02:22.100
And when a medical study winds up with invalid conclusions researchers could head down

00:02:22.100 --> 00:02:26.410
the wrong path people could get misguided
treatments based on faulty conclusions they

00:02:26.410 --> 00:02:31.540
could get sicker even...and a whole lot of
money could be wasted researching and providing

00:02:31.540 --> 00:02:32.550
those treatments.

00:02:32.550 --> 00:02:35.280
So, what’s causing science’s replication
problem?

00:02:35.280 --> 00:02:36.690
There are a lot of answers.

00:02:36.690 --> 00:02:41.140
Some of them involve unscrupulous researchers--researchers that are more concerned with attention and

00:02:41.140 --> 00:02:44.190
publishing and splashy headlines than good science.

00:02:44.190 --> 00:02:45.710
Here we’re talking about fraud.

00:02:45.710 --> 00:02:46.820
Falsified data.

00:02:46.820 --> 00:02:48.580
Intentional p-hacking.

00:02:48.580 --> 00:02:50.720
Statistical evil doers.

00:02:50.730 --> 00:02:54.960
But many reasons scientific studies aren’t
replicable are less nefarious.

00:02:54.970 --> 00:03:00.020
One issue related to replication--re-doing
studies--is reproducibility of the analyses

00:03:00.020 --> 00:03:01.020
in a paper.

00:03:01.020 --> 00:03:04.930
There’s not always one prescribed way of
analyzing a data set.

00:03:04.930 --> 00:03:10.040
A researcher named Brian Nosek and his team invited 29 groups of researchers to analyze

00:03:10.040 --> 00:03:16.730
the same data set--and attempt to answer whether or not soccer referees give more red cards

00:03:16.730 --> 00:03:19.980
to dark-skinned players than light-skinned
ones?

00:03:19.980 --> 00:03:21.000
Seems simple enough.

00:03:21.000 --> 00:03:25.500
These researchers were all working with the SAME data--but they ran different tests.

00:03:25.500 --> 00:03:26.500
Some used linear regressions.

00:03:26.500 --> 00:03:28.470
Some went with Bayesian models.

00:03:28.470 --> 00:03:32.340
And it’s not just the models that the researchers could have differed on.

00:03:32.340 --> 00:03:36.220
You also have freedom to exclude different
outliers, or look at different groups.

00:03:36.220 --> 00:03:40.530
Twenty of the groups found a statistically
significant relationship between skin color

00:03:40.530 --> 00:03:41.570
and red cards.

00:03:41.570 --> 00:03:48.080
Nine groups didn’t.The point, says researchers, is that no one analysis is gonna find THE

00:03:48.080 --> 00:03:50.100
answer, THE singular truth.

00:03:50.100 --> 00:03:53.500
When researchers aren’t clear about how
they analyzed their data, from which data

00:03:53.500 --> 00:03:58.970
points they excluded, to the exact model they ran, it can make it hard for someone to reproduce

00:03:58.970 --> 00:04:00.380
their results.

00:04:00.380 --> 00:04:01.950
Even if they had the same data.

00:04:01.950 --> 00:04:05.540
Good papers will have detailed descriptions of researchers’ methods.

00:04:05.540 --> 00:04:10.220
When you replicate a study you usually know what model the researcher used or you can ask.

00:04:10.220 --> 00:04:14.800
But if scientists aren’t clear or consistent
about this, it just puts another roadblock

00:04:14.800 --> 00:04:16.320
in the way of good replication.

00:04:16.320 --> 00:04:19.400
There are other reasons for the replicability
crisis.

00:04:19.400 --> 00:04:23.020
Some researchers and the folks who report
on scientific research don’t fully understand

00:04:23.020 --> 00:04:24.169
p-values.

00:04:24.169 --> 00:04:27.120
They make claims that statistical evidence
doesn’t support.

00:04:27.120 --> 00:04:33.060
Back in 2016, the American Statistical Association released a statement meant to help researchers

00:04:33.060 --> 00:04:36.630
understand and use P values better.

00:04:36.630 --> 00:04:44.129
It was reportedly the first time the 170-plus
year old organization made this type of explicit

00:04:44.129 --> 00:04:45.180
recommendations.

00:04:45.180 --> 00:04:50.759
Among the guidelines the Statistical Association published: “Scientific conclusions and business

00:04:50.760 --> 00:04:58.260
or policy decisions should not be based only on whether a p-value passes a specific threshold.”

00:04:58.260 --> 00:05:04.840
And "A p-value, or statistical significance,
does not measure the size of an effect or

00:05:04.840 --> 00:05:06.840
the importance of a result.”

00:05:06.840 --> 00:05:08.610
P-values need to be understood in context.

00:05:08.610 --> 00:05:13.849
A significant result doesn’t mean we ought
to all rush out and change what we’re doing.

00:05:13.849 --> 00:05:19.810
But if you like carrots, by all means keep
eating them.

00:05:19.810 --> 00:05:24.919
Another reason science produces results that can’t be reproduced is that published studies

00:05:24.919 --> 00:05:30.740
have a bias toward overestimating effects--in part because they were published because they

00:05:30.740 --> 00:05:32.729
had a low p-value.

00:05:32.729 --> 00:05:36.720
Some studies look promising and then aren’t reproducible because they were based on a fluke.

00:05:36.720 --> 00:05:40.160
When the study is repeated the fluke doesn’t
repeat itself.

00:05:40.169 --> 00:05:44.300
The website fivethirtyeight offers up this
explanation: Say you were looking at the relationship

00:05:44.310 --> 00:05:46.430
of height and college majors.

00:05:46.430 --> 00:05:50.999
You gather up your data--including a class
of math majors with a few exceptionally tall

00:05:50.999 --> 00:05:56.070
kids and a class of philosophy majors with
an unusually short student.

00:05:56.070 --> 00:05:58.340
When you compare the averages-- ha ha!

00:05:58.340 --> 00:05:59.340
Look at that!

00:05:59.340 --> 00:06:01.969
Math majors are taller than philosophy majors.

00:06:01.969 --> 00:06:06.699
You have statistically significant results
but when you repeat the study those differences

00:06:06.699 --> 00:06:11.340
disappear there’s regression to the mean
which gives you a more accurate picture of

00:06:11.340 --> 00:06:16.140
pretty similar average heights of each major and nothing all that interesting to write about.

00:06:16.140 --> 00:06:18.140
Except a correction to your first paper.

00:06:18.140 --> 00:06:19.809
Small sample sizes also get blamed.

00:06:19.809 --> 00:06:25.050
The fewer subjects in a study, the more likely you get skewed and unreplicable results.

00:06:25.050 --> 00:06:27.059
DFTBAQ, my friends.

00:06:27.059 --> 00:06:30.500
Even when results make sense to you--DFTBAQ.

00:06:30.500 --> 00:06:36.240
So where can researchers start improving the process--to help solve this reproducibility crisis.

00:06:36.240 --> 00:06:39.740
For one, researchers argue they need to do
a whole lot more replication.

00:06:39.740 --> 00:06:45.020
Replication allows us to weed out false significant effects: the flukes and the “too good to

00:06:45.020 --> 00:06:48.680
be true” effects that unfortunately make
great headlines.

00:06:48.680 --> 00:06:52.509
We need to get rid of the idea that one significant test is solid proof of anything.

00:06:52.509 --> 00:06:53.509
It isn’t.

00:06:53.509 --> 00:06:58.150
In fact, we need to get rid of the idea that
one significant test is even great evidence

00:06:58.150 --> 00:06:59.150
of anything.

00:06:59.150 --> 00:07:00.150
But replication is expensive.

00:07:00.150 --> 00:07:02.410
And it’s not as sexy as making a new discovery.

00:07:02.410 --> 00:07:06.749
It doesn’t attract the same media attention
institutional acclaim or funders.

00:07:06.749 --> 00:07:11.449
Who wants to say “I found the effect that
my colleague found yesterday!”?

00:07:11.449 --> 00:07:15.000
So say researchers we’ve gotta come up with ways to change those incentives.

00:07:15.000 --> 00:07:19.740
We need to find more funding for replication studies and change the way we all view the

00:07:19.740 --> 00:07:21.539
value of replication.

00:07:21.540 --> 00:07:27.180
Some people call for more publication of “null results”--those that DON’T support the hypothesis.

00:07:27.180 --> 00:07:31.780
This would allow quality research to be published, even if it didn’t show an effect, making

00:07:31.780 --> 00:07:36.289
p-hacking a little less enticing, since you
could still get null results published.

00:07:36.289 --> 00:07:40.719
Some researchers argue another way to help correct the reproducibility crisis is by reconsidering

00:07:40.719 --> 00:07:46.319
the standard p-value cut off of .05 for statistical significance.

00:07:46.319 --> 00:07:47.319
Is it stringent enough?

00:07:47.319 --> 00:07:48.789
Or should researchers move  it?

00:07:48.789 --> 00:07:54.789
In 2017, a group of more than 70 researchers co-authored a paper calling for a change in

00:07:54.789 --> 00:08:00.540
the default P-value threshold from .05 to
.005.

00:08:00.540 --> 00:08:05.939
They wrote: “This simple step would immediately improve the reproducibility of scientific

00:08:05.939 --> 00:08:08.589
research in many fields.”

00:08:08.589 --> 00:08:14.520
Calling results with a p-value of less than
.05 statistically significant they argue results

00:08:14.520 --> 00:08:18.800
in a high rate of false positives...even when
that research is done correctly.

00:08:18.800 --> 00:08:22.220
Let’s just look at one area of research
we’ve talked about before: Social Priming

00:08:22.220 --> 00:08:27.469
-- the idea that certain actions or conditions can affect the way you behave.

00:08:27.469 --> 00:08:31.719
One famous case of social priming is a study where subjects who were exposed to words related

00:08:31.719 --> 00:08:39.190
to old age--like Florida, bingo, grey, or
retired--walked more slowly after exposure

00:08:39.190 --> 00:08:41.090
than those who were shown neutral words.

00:08:41.090 --> 00:08:44.839
But recently, many researchers have expressed concerns that some of these social priming

00:08:44.839 --> 00:08:46.680
results may not hold up.

00:08:46.680 --> 00:08:50.450
When they began to see that, many experiments were done with many different priming mechanisms

00:08:50.450 --> 00:08:51.960
and outcome variables.

00:08:51.960 --> 00:08:56.010
And we’re making this data up here, but
let’s say that out of 1000 studies done,

00:08:56.010 --> 00:09:00.639
about 10% or 100 ended up with real effects of social priming.

00:09:00.639 --> 00:09:05.501
This is a table that displays how often our
studies resulted in true positives, false

00:09:05.501 --> 00:09:08.930
positives, true negatives, and false negatives.

00:09:08.930 --> 00:09:13.290
The top row shows the 900 studies where social priming DIDN’T work.

00:09:13.290 --> 00:09:19.459
Because we used a threshold of 0.05, 5% of those 900 studies will still be statistically

00:09:19.459 --> 00:09:22.500
significant even though there was no effect.

00:09:22.500 --> 00:09:25.399
Those 45 are our false positives.

00:09:25.399 --> 00:09:30.529
That leaves 855 studies where social priming didn't work and we caught it.

00:09:30.529 --> 00:09:32.160
Those are our true negatives.

00:09:32.160 --> 00:09:36.280
The next row contains the 100 studies where social priming DID work.

00:09:36.280 --> 00:09:40.110
In those studies, there were actual effects
of social priming.

00:09:40.110 --> 00:09:43.570
There you can see our true positives (60)
and false negatives (40).

00:09:43.570 --> 00:09:45.130
So what does that mean?

00:09:45.130 --> 00:09:49.459
Well, remember, statistical power is the ability to detect real effects.

00:09:49.459 --> 00:09:53.790
Sometimes we can fail to get a significant
result, even if an effect of a certain size

00:09:53.790 --> 00:09:54.790
is real.

00:09:54.790 --> 00:09:59.130
One estimate suggests that most psychology studies have an average of 60% power.

00:09:59.130 --> 00:10:04.149
So, that 60 on our table represents the 60
studies that had a significant effect that

00:10:04.149 --> 00:10:05.610
was observed.

00:10:05.610 --> 00:10:08.459
The other 40 weren’t caught, giving us false
negatives.

00:10:08.459 --> 00:10:13.339
Using our table, we can look at the percent
of significant results that come from studies

00:10:13.339 --> 00:10:14.880
with no effect.

00:10:14.880 --> 00:10:16.420
Our “False Alarms”.

00:10:16.420 --> 00:10:22.819
Our False Discovery Rate is 45 divided by
105, or 42.9%.

00:10:22.819 --> 00:10:27.160
That means of all the significant effects
that were recorded and published in our thought

00:10:27.160 --> 00:10:32.030
experiment, a bit less than HALF of them are false positives.

00:10:32.030 --> 00:10:36.230
Which shows, as we mentioned before, that having a statistically significant effect

00:10:36.230 --> 00:10:37.750
doesn’t make it REAL.

00:10:37.750 --> 00:10:45.310
All else being equal if we had changed the
p-value threshold from .05 to .005, we would

00:10:45.310 --> 00:10:46.709
have way less false positives.

00:10:46.709 --> 00:10:50.389
To make the work of reproducibility easier,
there are also pushes underway to encourage

00:10:50.389 --> 00:10:52.959
researchers to share their data more widely.

00:10:52.959 --> 00:10:57.670
In the United Kingdom, for example, many research funders expect researchers will make publicly

00:10:57.670 --> 00:11:02.850
funded research data available--recognizing the data as a public good.

00:11:02.850 --> 00:11:07.490
Academic journals also play a role in the
conversation around reproducibility--many

00:11:07.490 --> 00:11:13.089
of the most prestigious journals have adopted guidelines and policies that put more emphasis

00:11:13.089 --> 00:11:16.120
on reproducibility and transparency.

00:11:16.120 --> 00:11:20.280
In part, to help boost public trust in science
and the scientific process.

00:11:20.280 --> 00:11:23.100
Let’s go back to power posing before we
finish today.

00:11:23.100 --> 00:11:25.100
Really get that blood flowing.

00:11:25.100 --> 00:11:26.180
Confidence building!

00:11:26.180 --> 00:11:31.700
A study on power posing was published in Psychological Science back in 2010 that showed that power

00:11:31.700 --> 00:11:34.810
posing could change hormone levels and boost confidence.

00:11:34.810 --> 00:11:39.100
A TED talk about power posing was viewed more than 40 million times.

00:11:39.100 --> 00:11:40.260
Want a raise?

00:11:40.260 --> 00:11:44.620
Respect and awe from your friends and family and enemies?

00:11:44.620 --> 00:11:46.620
Power pose.

00:11:46.620 --> 00:11:47.300
Or not.

00:11:47.300 --> 00:11:51.640
After power-posing went mainstream other researchers tried to replicate the study--with a larger

00:11:51.649 --> 00:11:53.980
sample--and didn’t come up with the same
results.

00:11:53.990 --> 00:11:58.730
Other researchers found significant problems with the original study and came to the conclusion

00:11:58.730 --> 00:12:04.750
that quote “the existing evidence is too
weak to justify a search for moderators or

00:12:04.750 --> 00:12:10.020
to advocate for people to engage in power
posing to better their lives.”

00:12:10.020 --> 00:12:12.420
Power-posing got labeled pseudo-science.

00:12:12.420 --> 00:12:17.300
And then in 2018 the original author published a response to some of the critiques about

00:12:17.300 --> 00:12:23.269
power-posing...with an analysis that suggested the poses could help people feel more confident

00:12:23.269 --> 00:12:24.269
and powerful.

00:12:24.269 --> 00:12:29.759
Now, the newest paper doesn’t seem to address all of the critiques about Power Posing study

00:12:29.759 --> 00:12:34.440
but it comes to the conclusion that researchers shouldn't give up research about the effects

00:12:34.440 --> 00:12:36.920
of Power Posing quite yet.

00:12:36.920 --> 00:12:38.440
No.

00:12:38.440 --> 00:12:39.580
No.

00:12:39.580 --> 00:12:41.120
These are not power poses.

00:12:41.120 --> 00:12:44.300
I’m just trying to find something that indicates confusion.

00:12:44.300 --> 00:12:47.819
This back and forth of the power posing debate does make it harder to know what’s likely

00:12:47.819 --> 00:12:48.920
to be true.

00:12:48.920 --> 00:12:54.760
But it also shows the VALUE of replication
and even the reproducibility crisis in research.

00:12:54.760 --> 00:12:59.940
Science is a push and pull of ideas--researchers are constantly iterating and expanding on

00:12:59.949 --> 00:13:01.779
ideas that came before.

00:13:01.779 --> 00:13:02.980
They refine results.

00:13:02.980 --> 00:13:04.880
Build on other people’s findings.

00:13:04.880 --> 00:13:10.720
Replication is an essential part of the path
to scientific progress and real breakthroughs.

00:13:10.720 --> 00:13:16.640
The reproducibility crisis means more people are taking the replication step of the process seriously.

00:13:16.640 --> 00:13:20.120
Replication has helped us accomplish some pretty important things.

00:13:20.140 --> 00:13:25.020
Like help change peoples minds about whether smoking caused increases in lung cancer, even

00:13:25.020 --> 00:13:30.730
though researchers could never do a Randomized Controlled Trial to demonstrate causation.

00:13:30.730 --> 00:13:34.220
Evidence piled up, and now smoking rates are incredibly low.

00:13:34.220 --> 00:13:39.230
No single study is gonna show us the way the world REALLY is but that study and the studies

00:13:39.230 --> 00:13:44.560
that follow it that do and don’t find the
same relationships will get us closer and closer.

00:13:44.560 --> 00:13:50.560
And one day maybe we’ll know--with more
explicit certainty whether or not we oughta

00:13:50.560 --> 00:13:55.460
be putting on our hands on our hips and doing the wonder woman before a big job interview.

00:13:55.460 --> 00:13:57.760
Thanks for watching, I’ll see 
you next time.

