WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.425
[MUSIC PLAYING]

00:00:04.370 --> 00:00:05.590
MATEJ PFAJFAR: Hi, everyone.

00:00:05.590 --> 00:00:09.300
My name is Matej Pfajfar, and
I'm an engineering director

00:00:09.300 --> 00:00:11.040
for Android Machine Learning.

00:00:11.040 --> 00:00:13.800
It's really great to
be here with you today

00:00:13.800 --> 00:00:16.680
to talk about how
Android developers can

00:00:16.680 --> 00:00:20.310
use Machine Learning to build
great features for Android

00:00:20.310 --> 00:00:22.920
users.

00:00:22.920 --> 00:00:26.790
Android today has a wide range
of Machine Learning tools

00:00:26.790 --> 00:00:29.400
available for everyone--

00:00:29.400 --> 00:00:33.750
designers, product
managers to engineers,

00:00:33.750 --> 00:00:38.880
mobile developers, and Machine
Learning experts alike.

00:00:38.880 --> 00:00:42.130
Now, Android is a platform.

00:00:42.130 --> 00:00:43.680
And one of the ways
you can measure

00:00:43.680 --> 00:00:47.250
the success of a platform
is through the great things

00:00:47.250 --> 00:00:50.990
that you folks
build on top of it.

00:00:50.990 --> 00:00:54.320
In Android Machine Learning,
we measure our success

00:00:54.320 --> 00:00:56.510
by your success.

00:00:56.510 --> 00:01:00.940
Today Android runs on
2.5 billion devices,

00:01:00.940 --> 00:01:06.080
and a great many of them are
already using on-device ML.

00:01:06.080 --> 00:01:10.550
On-device Machine Learning is
no longer a thing of the future.

00:01:10.550 --> 00:01:13.295
It is here right now, today.

00:01:15.910 --> 00:01:19.090
Now, Google is an
AI-first company,

00:01:19.090 --> 00:01:23.670
so we use Machine Learning in
lots of places, for example,

00:01:23.670 --> 00:01:25.450
in Lookout.

00:01:25.450 --> 00:01:30.580
It's an application which
aims to help blind or visually

00:01:30.580 --> 00:01:33.700
impaired people gain more
confidence by helping

00:01:33.700 --> 00:01:38.060
them understand their
physical surroundings.

00:01:38.060 --> 00:01:42.400
We also use Machine Learning
extensively under the hood

00:01:42.400 --> 00:01:46.720
of Android to
optimize battery usage

00:01:46.720 --> 00:01:51.730
and allow you to get more done
on a single battery charge.

00:01:51.730 --> 00:01:54.320
For those of you new
to Machine Learning,

00:01:54.320 --> 00:01:56.380
let's have a quick recap.

00:01:56.380 --> 00:01:59.200
Why do we use Machine
Learning at all?

00:01:59.200 --> 00:02:03.060
Well, Machine Learning
allows us to solve

00:02:03.060 --> 00:02:07.450
certain types of problems
in very elegant ways.

00:02:07.450 --> 00:02:12.010
For example, say you had
to write code to detect

00:02:12.010 --> 00:02:14.570
if someone was walking.

00:02:14.570 --> 00:02:16.480
And if you had access
to their speed,

00:02:16.480 --> 00:02:19.780
you could write a fairly
simple bit of code

00:02:19.780 --> 00:02:24.490
to say, hey, if the speed is
less than a certain threshold,

00:02:24.490 --> 00:02:28.450
say four miles per hour, then
this person must be walking

00:02:28.450 --> 00:02:32.090
and not, for example, running.

00:02:32.090 --> 00:02:36.060
Extending this to running
seems straightforward enough.

00:02:36.060 --> 00:02:39.350
And if we wanted to
cover biking as well,

00:02:39.350 --> 00:02:43.080
we just need to add another
boundary condition to our code

00:02:43.080 --> 00:02:46.640
and we're pretty
much done, right?

00:02:46.640 --> 00:02:50.630
What about other types of
activities, for example,

00:02:50.630 --> 00:02:52.240
golfing?

00:02:52.240 --> 00:02:54.850
So if you think
about it, golfing

00:02:54.850 --> 00:02:59.080
involves a lot of walking,
but there is also carrying

00:02:59.080 --> 00:03:01.270
and swinging a bunch of clubs.

00:03:01.270 --> 00:03:04.570
So how do we go about
describing that?

00:03:04.570 --> 00:03:08.670
This is where Machine
Learning can help.

00:03:08.670 --> 00:03:12.900
In traditional programming,
we express rules in languages,

00:03:12.900 --> 00:03:19.080
such as Kotlin, Java, or C,
and we then apply those rules

00:03:19.080 --> 00:03:21.750
to problems of interest.

00:03:21.750 --> 00:03:26.140
In Machine Learning, the
paradigm is somewhat different.

00:03:26.140 --> 00:03:28.890
Instead of writing
a bunch of rules,

00:03:28.890 --> 00:03:34.780
we take a lot of label
data, like photos

00:03:34.780 --> 00:03:38.020
of people engaged in
different types of activities

00:03:38.020 --> 00:03:42.200
with corresponding labels
for those activities.

00:03:42.200 --> 00:03:45.700
And if we feed this data
to our neural network,

00:03:45.700 --> 00:03:48.520
it will learn the
business rules for us

00:03:48.520 --> 00:03:51.370
without us having
to define them.

00:03:51.370 --> 00:03:53.650
And in fact,
Machine Learning has

00:03:53.650 --> 00:03:57.460
proven to be remarkably
effective at solving

00:03:57.460 --> 00:04:01.030
many different types
of problems, including,

00:04:01.030 --> 00:04:04.580
for example, activity detection.

00:04:04.580 --> 00:04:07.270
And right now you're
probably thinking, wow,

00:04:07.270 --> 00:04:09.490
this must be really complicated.

00:04:09.490 --> 00:04:15.160
Well, in fact, it's not, but
we'll get to that in a minute.

00:04:15.160 --> 00:04:18.579
I've talked to you about how we
use Machine Learning at Google.

00:04:18.579 --> 00:04:23.740
But it's really exciting to see
other Android developers using

00:04:23.740 --> 00:04:27.130
Machine Learning and the
tools that we've provided

00:04:27.130 --> 00:04:29.910
to build really cool stuff.

00:04:29.910 --> 00:04:34.660
A group of engineers in
Delhi built Air Cognizer.

00:04:34.660 --> 00:04:36.720
It's an application
which measures

00:04:36.720 --> 00:04:40.350
the levels of air pollution
using your phone's camera

00:04:40.350 --> 00:04:41.880
alone.

00:04:41.880 --> 00:04:44.550
Now, it is pretty
amazing that you

00:04:44.550 --> 00:04:47.190
can detect the
presence of particles

00:04:47.190 --> 00:04:51.990
smaller than 2.5 micrometers
using just a camera

00:04:51.990 --> 00:04:55.890
and a neural network or two.

00:04:55.890 --> 00:04:58.060
Then there's Gradeup School.

00:04:58.060 --> 00:05:01.720
It's a learning app which
allows you to get answers

00:05:01.720 --> 00:05:04.480
to questions you need
help with by simply

00:05:04.480 --> 00:05:08.270
taking a photo of them
with your mobile phone.

00:05:08.270 --> 00:05:11.630
Now, to be clear, my
school days are long gone,

00:05:11.630 --> 00:05:13.200
but I'm a parent.

00:05:13.200 --> 00:05:15.000
I have seven-year-old twins.

00:05:15.000 --> 00:05:17.450
And it is absolutely
awesome that they

00:05:17.450 --> 00:05:20.570
have this type of
technology available to them

00:05:20.570 --> 00:05:23.720
as they go through school.

00:05:23.720 --> 00:05:26.150
My family and I live
in London, but I

00:05:26.150 --> 00:05:28.670
spend a lot of
time in California

00:05:28.670 --> 00:05:32.710
where everyone seems
to be a wine expert.

00:05:32.710 --> 00:05:36.340
So to help me take part in
conversations about wine,

00:05:36.340 --> 00:05:38.030
I have Vivino.

00:05:38.030 --> 00:05:42.070
It, too, uses Machine
Learning to help you explore

00:05:42.070 --> 00:05:44.515
good wine with good friends.

00:05:47.090 --> 00:05:50.750
But why on-device Machine
Learning specifically?

00:05:50.750 --> 00:05:54.190
Well, there are several reasons.

00:05:54.190 --> 00:05:57.240
For one, we have
privacy, a topic

00:05:57.240 --> 00:06:00.120
that's very important
both to ourselves as well

00:06:00.120 --> 00:06:03.350
as to our users.

00:06:03.350 --> 00:06:07.430
In many cases, it's more
appropriate to just keep

00:06:07.430 --> 00:06:09.950
everything on device
so we don't have

00:06:09.950 --> 00:06:16.230
to worry about how the data
is processed in the cloud.

00:06:16.230 --> 00:06:21.210
On-device Machine Learning works
when there is no connectivity

00:06:21.210 --> 00:06:23.370
or when the network is patchy.

00:06:23.370 --> 00:06:27.450
And if, like me, you spend any
time on underground trains,

00:06:27.450 --> 00:06:31.070
you will know that this
is pretty useful stuff.

00:06:31.070 --> 00:06:33.560
By running our
processing on device,

00:06:33.560 --> 00:06:38.840
we can remove network latency
out of the equation altogether.

00:06:38.840 --> 00:06:42.230
And couple this with the
latest advancements in hardware

00:06:42.230 --> 00:06:45.020
acceleration, and
we can give users

00:06:45.020 --> 00:06:50.630
truly real-time experiences
across a wide range of Android

00:06:50.630 --> 00:06:52.570
devices.

00:06:52.570 --> 00:06:55.830
In fact, we have
built this capability

00:06:55.830 --> 00:06:59.150
into the Android OS itself.

00:06:59.150 --> 00:07:03.770
With the Neural Networks API,
applications using TensorFlow

00:07:03.770 --> 00:07:08.180
can achieve great performance
by accelerating their workloads

00:07:08.180 --> 00:07:15.350
on different types of hardware,
like GPUs, DSPs, and NPUs.

00:07:15.350 --> 00:07:18.860
And with Android Q,
the Neural Networks API

00:07:18.860 --> 00:07:24.020
will be able to support even
more great new use cases

00:07:24.020 --> 00:07:26.910
with even better performance.

00:07:26.910 --> 00:07:30.980
We have worked really hard
with our ecosystem partners

00:07:30.980 --> 00:07:34.770
to achieve this, and we're
very proud of the results.

00:07:34.770 --> 00:07:38.270
For example, on a
MediaTek Helio P90,

00:07:38.270 --> 00:07:43.040
we were able to achieve a
9x increase in performance

00:07:43.040 --> 00:07:46.510
when running ML Kit's
Face Detection model.

00:07:46.510 --> 00:07:49.390
But it's not only about speed.

00:07:49.390 --> 00:07:53.450
Power consumption is
equally as important.

00:07:53.450 --> 00:07:57.590
And on Qualcomm's
Snapdragon 855 AI engine,

00:07:57.590 --> 00:08:03.150
we are able to achieve a 4x
increase in energy efficiency

00:08:03.150 --> 00:08:06.700
when running very powerful
text recognition models

00:08:06.700 --> 00:08:09.350
available in Google Lens.

00:08:09.350 --> 00:08:12.800
These are all really
impressive results.

00:08:12.800 --> 00:08:14.860
But I did say that
this was going

00:08:14.860 --> 00:08:16.760
to be easier than it looks.

00:08:16.760 --> 00:08:19.770
So let's get to that.

00:08:19.770 --> 00:08:22.350
We have made great
Machine Learning

00:08:22.350 --> 00:08:26.910
tools that will make you feel
like a Machine Learning expert.

00:08:26.910 --> 00:08:29.970
First, we are
launching the People

00:08:29.970 --> 00:08:33.330
Plus AI Guidebook,
a framework which

00:08:33.330 --> 00:08:37.799
helps you explore Machine
Learning opportunities

00:08:37.799 --> 00:08:40.289
for your application.

00:08:40.289 --> 00:08:43.020
In Android Platform,
we have made

00:08:43.020 --> 00:08:46.800
available wonderful Machine
Learning tools from speech

00:08:46.800 --> 00:08:48.750
to computer vision.

00:08:48.750 --> 00:08:51.690
Then we have ML Kit.

00:08:51.690 --> 00:08:55.440
If you want to leverage
Google Smarts out of the box,

00:08:55.440 --> 00:08:59.280
then ML Kit is the
product for you.

00:08:59.280 --> 00:09:04.430
Google Cloud's AutoML allows
you to bring your own data set.

00:09:04.430 --> 00:09:07.250
And it will train a
Machine Learning model

00:09:07.250 --> 00:09:09.560
for your specific use case.

00:09:09.560 --> 00:09:12.860
And what's new this year
is that this model can be

00:09:12.860 --> 00:09:16.360
downloaded and run on device.

00:09:16.360 --> 00:09:19.390
And last but not
least, TensorFlow

00:09:19.390 --> 00:09:23.770
puts the full power of Machine
Learning at your fingertips

00:09:23.770 --> 00:09:26.010
in an easy to use way.

00:09:26.010 --> 00:09:28.360
Clearly there's
lots to talk about.

00:09:28.360 --> 00:09:29.820
So let's get started.

00:09:29.820 --> 00:09:33.780
Hoi will walk us through our
People Plus AI Guidebook.

00:09:33.780 --> 00:09:34.963
Come on up, Hoi.

00:09:34.963 --> 00:09:38.420
[APPLAUSE]

00:09:38.420 --> 00:09:40.440
HOI LAM: Thank you, Matej.

00:09:40.440 --> 00:09:42.210
Hi, everyone, I'm Hoi.

00:09:42.210 --> 00:09:44.580
I come from the Android
Developer Relations team,

00:09:44.580 --> 00:09:47.160
and I lead our Machine
Learning effort.

00:09:50.210 --> 00:09:52.885
And if you want to follow me on
Twitter, my handle is #hoitab,

00:09:52.885 --> 00:09:57.370
H-O-I-T-A-B, to get the
latest on Android ML.

00:09:57.370 --> 00:09:59.830
So when I'm speaking
to developers

00:09:59.830 --> 00:10:03.640
from around the world what is
the most common question that I

00:10:03.640 --> 00:10:04.960
get?

00:10:04.960 --> 00:10:09.610
Is it how many layers I should
put into my new network?

00:10:09.610 --> 00:10:13.110
Or is it which ML
Kit API to use?

00:10:13.110 --> 00:10:15.780
Actually, it's this one.

00:10:15.780 --> 00:10:18.870
Hey, my boss has asked me
to put some Machine Learning

00:10:18.870 --> 00:10:20.250
into our app.

00:10:20.250 --> 00:10:23.230
Can you help?

00:10:23.230 --> 00:10:26.410
Fortunately, yes, we can.

00:10:26.410 --> 00:10:31.700
And also, you actually
know a lot of the answer

00:10:31.700 --> 00:10:34.400
because Machine Learning
is no different compared

00:10:34.400 --> 00:10:36.930
to any other new technology.

00:10:36.930 --> 00:10:40.640
Think about the first time when
Material Design was launched.

00:10:40.640 --> 00:10:42.930
How do you work with it?

00:10:42.930 --> 00:10:47.070
You work with product
designers and engineering

00:10:47.070 --> 00:10:50.850
all combined to decide,
hey, what kind of problem

00:10:50.850 --> 00:10:53.720
are we trying to solve
with our product?

00:10:53.720 --> 00:10:57.290
How does it fit in
within the user journey?

00:10:57.290 --> 00:11:00.760
And finally, how do we
actually implement it?

00:11:00.760 --> 00:11:02.990
And it is not a single effort.

00:11:02.990 --> 00:11:06.770
You don't just do Machine
Learning, take the box

00:11:06.770 --> 00:11:08.490
and say, yep, we're done.

00:11:08.490 --> 00:11:11.610
We don't need to do any
more Machine Learning.

00:11:11.610 --> 00:11:16.010
It is a very iterative process
when you have a new technology.

00:11:16.010 --> 00:11:17.930
You want to test the
boundary a little bit.

00:11:17.930 --> 00:11:19.250
You want to collaborate.

00:11:19.250 --> 00:11:25.000
And you want to see how the
user really use your product.

00:11:25.000 --> 00:11:28.130
And as Matej said,
at this I/O, we

00:11:28.130 --> 00:11:30.960
are launching the People
Plus AI Guidebook.

00:11:30.960 --> 00:11:32.910
There are six different
sections to help

00:11:32.910 --> 00:11:36.900
you structure your conversation
with your designers and product

00:11:36.900 --> 00:11:38.220
managers.

00:11:38.220 --> 00:11:42.210
You can access it from this URL.

00:11:42.210 --> 00:11:45.780
One of my personal favorite
sections of the new guidebook

00:11:45.780 --> 00:11:49.550
is this one, dealing
with graceful failure.

00:11:49.550 --> 00:11:51.190
We are all good
programmers here,

00:11:51.190 --> 00:11:54.550
so I believe thinking
of our edge cases

00:11:54.550 --> 00:11:56.650
is just part of the job.

00:11:56.650 --> 00:11:59.770
But with Machine Learning,
what is different is

00:11:59.770 --> 00:12:01.135
those are no longer edge cases.

00:12:03.990 --> 00:12:05.750
In the conventional
programming world,

00:12:05.750 --> 00:12:08.180
when you have yes
or no button, it's

00:12:08.180 --> 00:12:12.050
fairly easy to deal with when
the user click Yes or No.

00:12:12.050 --> 00:12:13.520
In the new Machine
Learning world,

00:12:13.520 --> 00:12:18.260
what happens when a user
say, well, actually 3:00 PM

00:12:18.260 --> 00:12:20.320
is not great.

00:12:20.320 --> 00:12:23.530
Does that mean yes
or does it mean no?

00:12:23.530 --> 00:12:25.690
You need to think through
that process of how

00:12:25.690 --> 00:12:28.400
do you deal with
errors and failures

00:12:28.400 --> 00:12:31.860
because it is part
of the main flow.

00:12:31.860 --> 00:12:34.470
And if you want to find out
more about this People Plus AI

00:12:34.470 --> 00:12:36.450
Guidebook, I would
strongly encourage

00:12:36.450 --> 00:12:40.590
you to go to the
session at 1:30 tomorrow

00:12:40.590 --> 00:12:43.410
afternoon where our Google
Design team will share

00:12:43.410 --> 00:12:45.560
with you the content
of the guidebook,

00:12:45.560 --> 00:12:49.993
as well as real world usage
of the guidebook and advice

00:12:49.993 --> 00:12:50.910
among Google products.

00:12:54.130 --> 00:12:57.370
Next, I want to talk about
the Android platform itself

00:12:57.370 --> 00:12:59.860
and what we offer as
part of the platform.

00:12:59.860 --> 00:13:03.370
In particular, I want
to talk about two APIs,

00:13:03.370 --> 00:13:05.340
or two sets of APIs even.

00:13:05.340 --> 00:13:08.410
So the first one
is to do a speech.

00:13:08.410 --> 00:13:12.640
I'm sure that a lot of you
have seen it from the keynote.

00:13:12.640 --> 00:13:17.630
So let's switch
over to my phone.

00:13:17.630 --> 00:13:21.290
So this is a little app
that I put together,

00:13:21.290 --> 00:13:26.600
and it uses the Android platform
API to do speech dictation,

00:13:26.600 --> 00:13:30.720
as well as reading it back to
me via the Text to Speech API.

00:13:30.720 --> 00:13:33.920
So fingers crossed,
this is a live demo.

00:13:33.920 --> 00:13:37.120
Let's see if it works.

00:13:37.120 --> 00:13:42.160
I'm really glad to be
here in California.

00:13:42.160 --> 00:13:44.120
So as you can see, it dictates.

00:13:44.120 --> 00:13:47.010
And let's see how it
reads back out to me.

00:13:47.010 --> 00:13:50.630
GOOGLE VOICE: I'm really glad
to be here in California.

00:13:50.630 --> 00:13:53.130
HOI LAM: Hey, so it works.

00:13:53.130 --> 00:13:54.176
Woo-hoo.

00:13:54.176 --> 00:13:57.600
[APPLAUSE]

00:13:57.600 --> 00:13:59.520
All right.

00:13:59.520 --> 00:14:01.860
So this secret button
here because what I just

00:14:01.860 --> 00:14:03.570
demonstrated is English.

00:14:03.570 --> 00:14:07.200
And today, English is just
table stake for dictation.

00:14:07.200 --> 00:14:11.040
How about trying it in
my native Cantonese?

00:14:11.040 --> 00:14:13.670
So let's try this.

00:14:13.670 --> 00:14:15.020
[SPEAKING CANTONESE]

00:14:19.648 --> 00:14:25.580
[APPLAUSE]

00:14:25.580 --> 00:14:27.980
So what I've just
said there is, I'm

00:14:27.980 --> 00:14:31.830
really happy to share
my experience with you.

00:14:31.830 --> 00:14:34.055
And let's see how
it reads it out.

00:14:34.055 --> 00:14:35.720
GOOGLE VOICE:
[SPEAKING CANTONESE]

00:14:38.745 --> 00:14:41.290
HOI LAM: Totally works.

00:14:41.290 --> 00:14:42.660
So let's switch back to slides.

00:14:47.000 --> 00:14:47.500
OK.

00:14:47.500 --> 00:14:51.748
So I demonstrated that
API on the Google Pixel 3,

00:14:51.748 --> 00:14:53.540
and many of you might
have the question of,

00:14:53.540 --> 00:14:55.300
hey, where is this API?

00:14:55.300 --> 00:14:58.570
Is it part of the Android
X Support Library?

00:14:58.570 --> 00:15:00.340
Is it part of Android Pie?

00:15:00.340 --> 00:15:01.450
Is it even Q?

00:15:01.450 --> 00:15:06.570
How come I didn't hear about
it at What's New for Android?

00:15:06.570 --> 00:15:10.860
This may surprise you because
actually, it wasn't new.

00:15:10.860 --> 00:15:13.800
So we launched this
API for text to speech

00:15:13.800 --> 00:15:18.210
10 years ago at Google I/O. In
fact, it's such a long time ago

00:15:18.210 --> 00:15:22.140
that YouTube doesn't support
uploading any video that's

00:15:22.140 --> 00:15:23.653
longer than 10 minutes.

00:15:23.653 --> 00:15:25.320
So I strongly encourage
you to check out

00:15:25.320 --> 00:15:29.820
part nine of the
keynote during that year

00:15:29.820 --> 00:15:32.490
because what you'll
hear is really

00:15:32.490 --> 00:15:35.880
fundamental improvement
in the quality of sound

00:15:35.880 --> 00:15:38.155
that comes back out.

00:15:38.155 --> 00:15:39.780
And that's one of
the beauty of Android

00:15:39.780 --> 00:15:42.210
is that we never
stop innovating.

00:15:42.210 --> 00:15:44.670
So the APIs that you
were using, we just

00:15:44.670 --> 00:15:48.450
keep that updated through time.

00:15:48.450 --> 00:15:52.070
How about the Speech
Recognizer, the dictation part?

00:15:52.070 --> 00:15:55.880
That was launched as part
of API 8, Android Froyo.

00:15:55.880 --> 00:15:59.270
So this is actionable
for you today.

00:15:59.270 --> 00:16:02.390
Your users should be able to
access both Text to Speech,

00:16:02.390 --> 00:16:05.040
as well as Speech Recognizer.

00:16:05.040 --> 00:16:09.750
So next, how easy is
it to implement this?

00:16:09.750 --> 00:16:11.960
So let's go through the
seven steps I went through

00:16:11.960 --> 00:16:14.510
with the Speech Recognizer.

00:16:14.510 --> 00:16:16.400
So the first thing
that you'd need to do

00:16:16.400 --> 00:16:19.320
is to create your
own speech listener.

00:16:19.320 --> 00:16:22.320
Within that class, there are
multiple abstract methods

00:16:22.320 --> 00:16:24.080
that you'd need to implement.

00:16:24.080 --> 00:16:27.160
And I will highlight
three of them.

00:16:27.160 --> 00:16:29.470
The first one is
on partial result.

00:16:29.470 --> 00:16:31.950
So as I was speaking
to the phone,

00:16:31.950 --> 00:16:37.050
you see part of the results
coming back as I spoke.

00:16:37.050 --> 00:16:39.600
And what that does is
it give user feedback

00:16:39.600 --> 00:16:43.530
that you're listening to
them and the machine actually

00:16:43.530 --> 00:16:46.680
understands what
they are saying.

00:16:46.680 --> 00:16:49.590
Another thing that you can do
to give the user more feedback

00:16:49.590 --> 00:16:52.650
is on RMS Change,
which detects the noise

00:16:52.650 --> 00:16:55.680
level around the phone
so you can display

00:16:55.680 --> 00:16:59.070
the variation in noise level
back to your user and show

00:16:59.070 --> 00:17:03.730
that you're really
literally listening to them.

00:17:03.730 --> 00:17:07.240
And finally, when you get
the result in on result,

00:17:07.240 --> 00:17:10.230
you can access the
top confidence result

00:17:10.230 --> 00:17:14.043
by just going into the
first element of the array.

00:17:14.043 --> 00:17:16.460
And there, you will find a
string that you're looking for.

00:17:19.430 --> 00:17:22.569
In addition, an optional
step is you can also

00:17:22.569 --> 00:17:25.839
look at the other results that
the machine was coming back

00:17:25.839 --> 00:17:26.470
with.

00:17:26.470 --> 00:17:29.470
So for example, when I'm
saying "I am," the machine

00:17:29.470 --> 00:17:34.480
might come back with "I
apostrophe m," or "I space am."

00:17:34.480 --> 00:17:38.560
You can access both
through this method.

00:17:38.560 --> 00:17:41.290
In order to use it, you need
to initialize your own Speech

00:17:41.290 --> 00:17:43.330
Recognizer.

00:17:43.330 --> 00:17:47.750
And after that, you also need
to have a speaking intent.

00:17:47.750 --> 00:17:49.230
Within all these
options, I would

00:17:49.230 --> 00:17:51.320
like to highlight two of them.

00:17:51.320 --> 00:17:53.260
First, I would
strongly recommend

00:17:53.260 --> 00:17:55.840
that you enable
partial result. Again,

00:17:55.840 --> 00:17:57.580
it's about giving
the user feedback

00:17:57.580 --> 00:17:59.670
that things are working.

00:17:59.670 --> 00:18:04.680
Second, as here I specify,
you can specify the language,

00:18:04.680 --> 00:18:08.920
and this is Cantonese.

00:18:08.920 --> 00:18:11.870
And when you are ready,
you just feed the intent

00:18:11.870 --> 00:18:16.740
into the recognizer that you
have created and away you go.

00:18:16.740 --> 00:18:20.060
It's that simple, seven steps.

00:18:20.060 --> 00:18:24.840
Next, I want to go through
the camera API itself.

00:18:24.840 --> 00:18:27.430
You might ask, hey,
what is going on?

00:18:27.430 --> 00:18:29.200
Why are you talking
about camera?

00:18:29.200 --> 00:18:31.800
I thought this is
Machine Learning.

00:18:31.800 --> 00:18:35.370
As many of us know, many
Machine Learning use cases

00:18:35.370 --> 00:18:39.300
leverage the camera in
order to do analysis.

00:18:39.300 --> 00:18:42.420
And when I first
started six months ago,

00:18:42.420 --> 00:18:45.300
I thought, hey, this
is easy, it's Android.

00:18:45.300 --> 00:18:46.350
I know Android.

00:18:46.350 --> 00:18:47.550
This is great.

00:18:47.550 --> 00:18:49.200
Let me try it out.

00:18:49.200 --> 00:18:53.070
And this happened to me when
I first load up my sample.

00:18:53.070 --> 00:18:54.660
It just say, oh, wait.

00:18:54.660 --> 00:18:55.980
You're using camera 1.

00:18:55.980 --> 00:18:57.620
Why?

00:18:57.620 --> 00:18:59.740
We deprecated it in API 21.

00:18:59.740 --> 00:19:01.530
Why are you still using it?

00:19:01.530 --> 00:19:04.020
What's wrong with you?

00:19:04.020 --> 00:19:07.020
And I go, yeah, I work
in Android [INAUDIBLE],,

00:19:07.020 --> 00:19:08.730
I know Android.

00:19:08.730 --> 00:19:10.710
I will update a sample.

00:19:10.710 --> 00:19:14.400
So I went to camera 2.

00:19:14.400 --> 00:19:17.040
I went to the basic
camera 2 sample

00:19:17.040 --> 00:19:19.320
and I find this particular
class and go, yeah,

00:19:19.320 --> 00:19:22.200
all I need to do
is understand it.

00:19:22.200 --> 00:19:28.200
And I scroll, I scroll,
I scroll because it's

00:19:28.200 --> 00:19:31.150
1,000 lines of code.

00:19:31.150 --> 00:19:33.900
And it's like, oh, my
word, I am so dumb.

00:19:33.900 --> 00:19:38.110
I don't understand
why Google hired me.

00:19:38.110 --> 00:19:41.410
And actually, after a
while, I find the confidence

00:19:41.410 --> 00:19:44.590
to talk to a lot of
developers, and they also

00:19:44.590 --> 00:19:46.280
have the same problem.

00:19:46.280 --> 00:19:49.810
So thank you for those of
you that share my pain.

00:19:49.810 --> 00:19:54.020
I am perhaps OK as a developer.

00:19:54.020 --> 00:19:56.740
So bear that in mind,
at this Google I/O,

00:19:56.740 --> 00:19:59.830
we are launching CameraX.

00:19:59.830 --> 00:20:03.230
The Support Library is
really to do two things--

00:20:03.230 --> 00:20:04.940
improve the ease
of use so that you

00:20:04.940 --> 00:20:07.250
don't need to have
1,000 lines of code

00:20:07.250 --> 00:20:10.660
and three hours of this
session to present it to you,

00:20:10.660 --> 00:20:14.780
and the second thing is to
increase device compatibility.

00:20:14.780 --> 00:20:17.900
At launch, we will
support three use cases--

00:20:17.900 --> 00:20:22.130
preview, image capture,
and my personal favorite,

00:20:22.130 --> 00:20:23.780
image analysis.

00:20:23.780 --> 00:20:26.070
You can see where
this is going, right?

00:20:26.070 --> 00:20:29.320
So how easy is this?

00:20:29.320 --> 00:20:32.090
I told you it is not
1,000 lines of code.

00:20:32.090 --> 00:20:34.370
So let me just present
to you the four steps

00:20:34.370 --> 00:20:39.070
that I went through to implement
the image analysis use case.

00:20:39.070 --> 00:20:42.220
The first thing I did, just
like the speech listener,

00:20:42.220 --> 00:20:44.290
is to create analyzer.

00:20:44.290 --> 00:20:48.120
And the red box is where
you put your ML code.

00:20:48.120 --> 00:20:49.040
So that's prep.

00:20:49.040 --> 00:20:51.060
That's our analyzer.

00:20:51.060 --> 00:20:54.300
In order to run it,
the next step I do

00:20:54.300 --> 00:20:57.340
is to create a conflagration
object for the use case.

00:20:57.340 --> 00:21:00.550
Here, you will notice
that we basically

00:21:00.550 --> 00:21:02.010
define how the
threading is going

00:21:02.010 --> 00:21:05.020
to be run in very simple terms.

00:21:05.020 --> 00:21:10.090
And my personal favorite is this
option, acquire latest image.

00:21:10.090 --> 00:21:12.970
How often your pipeline
in Machine Learning

00:21:12.970 --> 00:21:15.040
is backed up because
your phone can't

00:21:15.040 --> 00:21:19.150
deal with all the number
of images coming through?

00:21:19.150 --> 00:21:21.490
This solves the problem
by only giving you

00:21:21.490 --> 00:21:23.530
the latest image that it has.

00:21:23.530 --> 00:21:25.810
So you no longer need
to do your own code

00:21:25.810 --> 00:21:28.280
to say, hey, let's throw
away the next five image

00:21:28.280 --> 00:21:31.720
and let's get on
with the latest one.

00:21:31.720 --> 00:21:35.540
This automatically
does it for you.

00:21:35.540 --> 00:21:37.790
After that, you create
a use case feeding

00:21:37.790 --> 00:21:39.500
in the configuration
and the analyzer

00:21:39.500 --> 00:21:42.090
that you have put it together.

00:21:42.090 --> 00:21:44.450
And last but not
least, just feeding it

00:21:44.450 --> 00:21:46.720
into a Lifecycle owner.

00:21:46.720 --> 00:21:49.820
Just bind it in a
Lifecycle like for example

00:21:49.820 --> 00:21:53.540
the AppCompatActivity or
the Fragments that you love.

00:21:53.540 --> 00:22:00.100
That's all you need to do,
it's that simple, four steps.

00:22:00.100 --> 00:22:02.650
I'm sure a lot of you would be
interested to find out more,

00:22:02.650 --> 00:22:05.740
and as a result, we do
have an extra session.

00:22:05.740 --> 00:22:06.330
Wey-hey.

00:22:06.330 --> 00:22:09.340
At 5:30 tomorrow
afternoon, not tomorrow

00:22:09.340 --> 00:22:13.000
morning, tomorrow afternoon
where our team from CameraX

00:22:13.000 --> 00:22:14.860
will be presenting this in full.

00:22:14.860 --> 00:22:17.170
And you can ask
them the questions

00:22:17.170 --> 00:22:19.506
that are in your mind.

00:22:19.506 --> 00:22:23.020
With that I'll welcome
Dong to talk about ML Kit.

00:22:23.020 --> 00:22:23.758
Come on up, Dong.

00:22:23.758 --> 00:22:25.987
[APPLAUSE]

00:22:25.987 --> 00:22:27.070
DONG CHEN: Thank you, Hoi.

00:22:30.370 --> 00:22:31.580
Hello, everyone.

00:22:31.580 --> 00:22:33.190
My name is Dong Chen.

00:22:33.190 --> 00:22:36.316
I'm a tech lead of ML Kit team.

00:22:36.316 --> 00:22:42.190
ML Kit was launched one
year ago at last Google I/O.

00:22:42.190 --> 00:22:45.440
You may wonder what has happened
to ML Kit since its launch.

00:22:45.440 --> 00:22:47.800
How is it adopted
by our developers?

00:22:47.800 --> 00:22:50.980
What's new and what's
cool with ML Kit?

00:22:50.980 --> 00:22:53.200
In the next few
minutes, I will lead you

00:22:53.200 --> 00:22:55.890
through these questions.

00:22:55.890 --> 00:22:59.570
First of all, a little refresher
of ML Kit for those people

00:22:59.570 --> 00:23:01.370
who are new to it.

00:23:01.370 --> 00:23:05.420
ML Kit is Google's Machine
Learning SDK for mobile.

00:23:05.420 --> 00:23:08.720
We bring the best of a
Google Machine Learning model

00:23:08.720 --> 00:23:11.180
onto mobile devices
behind a simple

00:23:11.180 --> 00:23:15.932
to use API consistent
across both Android and iOS.

00:23:15.932 --> 00:23:19.300
It is aimed at making
Machine Learning easy

00:23:19.300 --> 00:23:23.080
and democratizing AI for
make it accessible to all

00:23:23.080 --> 00:23:25.730
the mobile developers.

00:23:25.730 --> 00:23:28.280
Just because you need
to use Machine Learning,

00:23:28.280 --> 00:23:32.090
it doesn't mean you need
to be a ML expert or a data

00:23:32.090 --> 00:23:34.606
scientist with a PhD.

00:23:34.606 --> 00:23:37.770
You shouldn't need to worry
about collecting data, training

00:23:37.770 --> 00:23:41.800
models, tuning model, and
running it on mobile device.

00:23:41.800 --> 00:23:43.950
ML Kit will take care
of all that for you.

00:23:47.100 --> 00:23:50.610
Since launch a year ago,
ML Kit has a strong start

00:23:50.610 --> 00:23:52.710
and a phenomenal growth.

00:23:52.710 --> 00:23:55.920
Its powerful models,
combined with simple API,

00:23:55.920 --> 00:23:58.110
have enabled many
mobile developers

00:23:58.110 --> 00:24:02.660
to add a Machine Learning
feature onto their mobile apps.

00:24:02.660 --> 00:24:05.960
We're seeing user
engagement accelerating

00:24:05.960 --> 00:24:11.140
with active user sessions
growing by 60% month to month.

00:24:11.140 --> 00:24:14.830
Developers are building
really interesting mobile apps

00:24:14.830 --> 00:24:20.560
with ML Kit, ranging from social
sharing app for fishing lovers

00:24:20.560 --> 00:24:25.810
to shopping assistant apps for
consumers of shoes, clothes

00:24:25.810 --> 00:24:28.440
and furniture.

00:24:28.440 --> 00:24:30.480
They have used a
full range of ML Kit

00:24:30.480 --> 00:24:36.200
APIs, such as text recognition,
barcode scanning, image

00:24:36.200 --> 00:24:38.990
labeling, et cetera.

00:24:38.990 --> 00:24:43.520
Our face detection models
are 18 times faster

00:24:43.520 --> 00:24:45.700
as compared to the
last Google I/O

00:24:45.700 --> 00:24:48.530
and 13% to 24% more accurate.

00:24:48.530 --> 00:24:54.950
In addition, we also launched
Face Counter Detection.

00:24:54.950 --> 00:24:57.290
Besides the Vision
APIs, we have now

00:24:57.290 --> 00:25:00.690
also expanded into Natural
Language processing.

00:25:00.690 --> 00:25:02.780
We introduced Language
Identification

00:25:02.780 --> 00:25:07.020
feature for detecting a language
based on the textual input.

00:25:07.020 --> 00:25:11.120
We also launched Smart Reply for
producing meaningful responses

00:25:11.120 --> 00:25:13.840
for our Casual Chat.

00:25:13.840 --> 00:25:16.600
If this base API
doesn't fit your need,

00:25:16.600 --> 00:25:21.340
we also support hosting
or running custom models.

00:25:21.340 --> 00:25:23.980
And we did not stop here.

00:25:23.980 --> 00:25:26.040
I'm thrilled to
share that we're now

00:25:26.040 --> 00:25:30.660
launching several exciting
features for ML Kit.

00:25:30.660 --> 00:25:34.410
First of all, we're launching
Object Detection and Tracking.

00:25:34.410 --> 00:25:39.090
You can now use ML Kit to detect
single and multiple objects

00:25:39.090 --> 00:25:40.800
in a static image.

00:25:40.800 --> 00:25:44.100
In addition, you can track
them in a streaming video

00:25:44.100 --> 00:25:47.010
while your phone moves.

00:25:47.010 --> 00:25:51.210
This will be very useful for
identifying interesting items

00:25:51.210 --> 00:25:53.100
for further processing.

00:25:53.100 --> 00:25:56.370
As shown in this
video, using your phone

00:25:56.370 --> 00:26:00.340
ML Kit will identify and track
the potted plant in the room.

00:26:00.340 --> 00:26:04.110
And with that detection result,
the app will look up the item

00:26:04.110 --> 00:26:08.070
and show you a product catalog
with prices and the links

00:26:08.070 --> 00:26:10.800
for shopping similar items.

00:26:10.800 --> 00:26:15.900
Secondly, we're launching ML
Kit on Device Translation API.

00:26:15.900 --> 00:26:19.170
For the very first
time, mobile developers

00:26:19.170 --> 00:26:22.890
could tap into the same model
powering the offline Google

00:26:22.890 --> 00:26:26.080
Translate app free of charge.

00:26:26.080 --> 00:26:29.010
The translation will write
entirely on the device.

00:26:29.010 --> 00:26:32.160
In your own, app you can
now add a translation

00:26:32.160 --> 00:26:38.040
between any pair of languages
from 59 supported languages.

00:26:38.040 --> 00:26:40.560
You will also have control
over which language

00:26:40.560 --> 00:26:43.020
packs to download,
when to download them,

00:26:43.020 --> 00:26:47.000
and how long you're going to
keep on the mobile device.

00:26:47.000 --> 00:26:49.040
At last Google
I/O, we introduced

00:26:49.040 --> 00:26:53.620
On-Device and the cloud-based
Image Labeling API.

00:26:53.620 --> 00:26:57.370
The On-Device API can
identify hundreds of labels.

00:26:57.370 --> 00:27:01.210
And the Cloud API can identify
more than 10,000 labels.

00:27:01.210 --> 00:27:04.720
For this beautiful photo
using the On-Device API,

00:27:04.720 --> 00:27:08.260
we look at labels like
a dog, pet, mountain.

00:27:08.260 --> 00:27:09.960
This is great.

00:27:09.960 --> 00:27:11.980
But these pre-trained
Google models

00:27:11.980 --> 00:27:15.820
can only provide
generic classifications.

00:27:15.820 --> 00:27:18.910
What if I want to
recognize my dog's name

00:27:18.910 --> 00:27:21.440
or detect the dog's breed?

00:27:21.440 --> 00:27:25.550
That will require me to
train my own custom model.

00:27:25.550 --> 00:27:28.750
And for that, AutoML
is a powerful tool

00:27:28.750 --> 00:27:31.630
which can help you with
model architecture search,

00:27:31.630 --> 00:27:34.290
as well as model training.

00:27:34.290 --> 00:27:39.210
Two years ago, Google launched
the cloud-based AutoML.

00:27:39.210 --> 00:27:45.060
Those powerful and very large
models can only run on a server

00:27:45.060 --> 00:27:48.300
and doesn't fit into
the mobile device.

00:27:48.300 --> 00:27:53.000
What if I want to utilize
the power of AutoML,

00:27:53.000 --> 00:27:55.670
at the same time, I also
want to take advantage

00:27:55.670 --> 00:28:00.680
of the cost, speed, and privacy
benefits of On-Device Machine

00:28:00.680 --> 00:28:04.920
Learning, as Matej just
discussed minutes ago?

00:28:04.920 --> 00:28:07.320
I'm excited to share
the great news with you.

00:28:07.320 --> 00:28:11.380
Now we're launching ML
Kit AutoML Vision Edge.

00:28:11.380 --> 00:28:14.160
It provides the same easy
to use Image Labeling

00:28:14.160 --> 00:28:18.660
API, plus a TensorFlow Lite
model customized for you,

00:28:18.660 --> 00:28:20.960
trained by AutoML.

00:28:20.960 --> 00:28:23.270
You can now bring your
own images of the items

00:28:23.270 --> 00:28:27.110
you're interested in, as little
as tens of images per item

00:28:27.110 --> 00:28:29.100
depending on your scenario.

00:28:29.100 --> 00:28:30.830
We will then train
the [INAUDIBLE]

00:28:30.830 --> 00:28:36.570
for you using the AutoML based
on the photos you provided.

00:28:36.570 --> 00:28:39.630
And we will also find
and tune the model

00:28:39.630 --> 00:28:43.830
with the performance and size
optimized for mobile devices.

00:28:43.830 --> 00:28:48.780
You generate a TensorFlow Lite
model customized for you which

00:28:48.780 --> 00:28:52.250
runs entirely on a device.

00:28:52.250 --> 00:28:55.930
For example, if I want to
build a mobile app that

00:28:55.930 --> 00:28:58.990
can classify different
type of flowers,

00:28:58.990 --> 00:29:03.100
I can use AutoML to
help me train the model.

00:29:03.100 --> 00:29:06.640
To do that, I will first
use the Firebase console web

00:29:06.640 --> 00:29:11.180
interface to upload the
photos of flowers I have.

00:29:11.180 --> 00:29:13.390
Once I've uploaded
all the flower photos,

00:29:13.390 --> 00:29:15.265
I'll start training
my own model.

00:29:15.265 --> 00:29:18.880
And for each training job, I
can choose the best options

00:29:18.880 --> 00:29:23.560
that fit my requirements in
terms of inference, latency,

00:29:23.560 --> 00:29:27.010
classification,
accuracy, and model size.

00:29:27.010 --> 00:29:30.940
In addition, I can choose
how many compute hours

00:29:30.940 --> 00:29:32.830
I'm willing to spend
on the training that

00:29:32.830 --> 00:29:34.820
fits in my budget.

00:29:34.820 --> 00:29:36.990
Once the model
training completes,

00:29:36.990 --> 00:29:40.910
will show the quality of the
model in terms of precision

00:29:40.910 --> 00:29:43.030
and recall rate.

00:29:43.030 --> 00:29:47.690
Now, with the model trained,
how do I use it in my own app?

00:29:47.690 --> 00:29:50.630
ML Kit will help you with that.

00:29:50.630 --> 00:29:54.340
I'm going to show you one
slide of a Kotlin code, which

00:29:54.340 --> 00:29:57.020
will implement a
flower classifier using

00:29:57.020 --> 00:29:59.370
the model we have just trained.

00:29:59.370 --> 00:30:04.120
First, I will create a
model with the name flowers.

00:30:04.120 --> 00:30:06.610
Then I will register
the flowers model

00:30:06.610 --> 00:30:09.320
with Firebase Model Manager.

00:30:09.320 --> 00:30:11.980
The register remote
model call indicates

00:30:11.980 --> 00:30:14.480
model will be downloaded
remotely from the Firebase

00:30:14.480 --> 00:30:16.460
console.

00:30:16.460 --> 00:30:18.995
And you don't need to worry
about the model downloading.

00:30:18.995 --> 00:30:22.390
ML Kit will take
care of that for you.

00:30:22.390 --> 00:30:24.530
You also have the
option to download

00:30:24.530 --> 00:30:27.330
the model from Firebase
console yourself and bundle it

00:30:27.330 --> 00:30:29.640
inside your app.

00:30:29.640 --> 00:30:34.050
Next, I will specify the options
for using this flowers model.

00:30:34.050 --> 00:30:39.840
I will specify this confidence
threshold as 0.45 for my model,

00:30:39.840 --> 00:30:43.180
depending how the
model is trained.

00:30:43.180 --> 00:30:47.240
With that option, I will then
create my flower labeler.

00:30:47.240 --> 00:30:49.960
And these APIs may
look familiar to you,

00:30:49.960 --> 00:30:53.320
as they are really very similar
to the existing On-Device

00:30:53.320 --> 00:30:56.020
and the Cloud
Image Labeler APIs,

00:30:56.020 --> 00:30:59.320
except now we're using a
model trained by AutoML.

00:30:59.320 --> 00:31:01.700
Isn't that beautiful?

00:31:01.700 --> 00:31:04.790
Finally, I can run
my flower labeler

00:31:04.790 --> 00:31:08.780
to detect and classify
the flowers inside a photo

00:31:08.780 --> 00:31:10.520
or a streaming video.

00:31:10.520 --> 00:31:11.340
Yay.

00:31:11.340 --> 00:31:15.140
With just a few lines of code
I have built my own flower

00:31:15.140 --> 00:31:18.860
classifier running a model
trained by AutoML based

00:31:18.860 --> 00:31:21.310
on the photos I have.

00:31:21.310 --> 00:31:23.920
For those who are new
to ML Kit, I strongly

00:31:23.920 --> 00:31:26.270
encourage you to try it out.

00:31:26.270 --> 00:31:30.540
You will be amazed how
simple, yet powerful, it is.

00:31:30.540 --> 00:31:33.840
Last but not least, we are
launching new Material Design

00:31:33.840 --> 00:31:37.810
Guidelines for ML
use cases on mobile.

00:31:37.810 --> 00:31:41.230
Machine Learning has not
only added magic features

00:31:41.230 --> 00:31:44.570
to mobile apps, but also
affected more and more product

00:31:44.570 --> 00:31:46.230
experiences.

00:31:46.230 --> 00:31:49.680
To provide a great
end-to-end user experience,

00:31:49.680 --> 00:31:53.730
we're publishing new
Material Design Guidelines

00:31:53.730 --> 00:31:56.670
to make our ML
product experience

00:31:56.670 --> 00:32:00.540
usable, beautiful
and understandable.

00:32:00.540 --> 00:32:03.450
Our Object Detection
Tracking Showcase app

00:32:03.450 --> 00:32:06.480
was developed by following
all these guidelines.

00:32:06.480 --> 00:32:09.600
And its full source code
is available on GitHub

00:32:09.600 --> 00:32:12.870
for both Android and iOS.

00:32:12.870 --> 00:32:15.450
To learn more about ML Kit,
please come to our session

00:32:15.450 --> 00:32:16.710
tomorrow at 12:30.

00:32:16.710 --> 00:32:19.290
We'll show you some
cool demos and walk you

00:32:19.290 --> 00:32:21.930
through all the new
features we're launching.

00:32:21.930 --> 00:32:24.390
And to learn more about
Material Design for ML,

00:32:24.390 --> 00:32:27.940
please come to our session
at 12:30 on Thursday.

00:32:27.940 --> 00:32:30.190
With that, I'll hand over
to Laurence, who will

00:32:30.190 --> 00:32:31.360
talk about TensorFlow Lite.

00:32:31.360 --> 00:32:31.860
Thank you.

00:32:32.475 --> 00:32:33.642
LAURENCE MORONEY: Thank you.

00:32:33.642 --> 00:32:38.045
[APPLAUSE]

00:32:38.045 --> 00:32:38.920
So, hi, everybody.

00:32:38.920 --> 00:32:41.490
We've seen a lot of great stuff
that's available on Android.

00:32:41.490 --> 00:32:43.710
We've seen how some of the
native APIs and Android

00:32:43.710 --> 00:32:45.460
can be used with
Machine Learning.

00:32:45.460 --> 00:32:47.430
And we've also seen
how ML Kits can

00:32:47.430 --> 00:32:49.920
be used with preexisting
models as well

00:32:49.920 --> 00:32:51.367
as building your own models.

00:32:51.367 --> 00:32:52.950
I'm going to switch
gears a little bit

00:32:52.950 --> 00:32:55.530
and talk about TensorFlow
and how TensorFlow

00:32:55.530 --> 00:32:57.840
can be used for building
custom models that

00:32:57.840 --> 00:33:01.290
could be deployed into ML Kit
or could be deployed directly

00:33:01.290 --> 00:33:02.460
to your device.

00:33:02.460 --> 00:33:05.670
Who here has never
heard of TensorFlow?

00:33:05.670 --> 00:33:07.690
OK, I'm doing my job, good.

00:33:07.690 --> 00:33:08.780
One.

00:33:08.780 --> 00:33:11.640
So going back to what
Matej mentioned earlier on,

00:33:11.640 --> 00:33:14.780
so TensorFlow is our API for
building Machine Learn models

00:33:14.780 --> 00:33:16.660
and for running Machine
Learning models.

00:33:16.660 --> 00:33:18.770
And going back to Matej's
slides from earlier on,

00:33:18.770 --> 00:33:20.450
the whole idea around
Machine Learning

00:33:20.450 --> 00:33:23.710
is I like to think of it as
a new programming paradigm.

00:33:23.710 --> 00:33:25.490
We're all software
developers, and it's

00:33:25.490 --> 00:33:29.150
a new way for us to write
code to open up new scenarios.

00:33:29.150 --> 00:33:30.890
And instead of us
thinking of the rules

00:33:30.890 --> 00:33:33.170
and expressing them
in code ourselves,

00:33:33.170 --> 00:33:36.470
what if we could actually
label a bunch of data

00:33:36.470 --> 00:33:38.870
and then have the machine
infer the rules that

00:33:38.870 --> 00:33:40.400
make that actual data?

00:33:40.400 --> 00:33:42.830
And if you come away with
nothing else about Machine

00:33:42.830 --> 00:33:45.110
Learning, think
about it in that way.

00:33:45.110 --> 00:33:45.960
So TensorFlow.

00:33:45.960 --> 00:33:47.960
The architecture of
TensorFlow from a high level

00:33:47.960 --> 00:33:49.490
looks like this.

00:33:49.490 --> 00:33:51.230
On the left-hand
side of this diagram

00:33:51.230 --> 00:33:53.660
are all the things that you
use for building models,

00:33:53.660 --> 00:33:55.380
and we tend to
call that training.

00:33:55.380 --> 00:33:58.250
So there's APIs in there
like keras and estimators

00:33:58.250 --> 00:34:00.290
and this distribution
strategy if you're

00:34:00.290 --> 00:34:03.810
building big complex models so
you can spread it across CPUs

00:34:03.810 --> 00:34:05.910
and TPUs, that kind of thing.

00:34:05.910 --> 00:34:08.090
But then on the right-hand
side is deployment.

00:34:08.090 --> 00:34:09.679
So your models,
once they're built,

00:34:09.679 --> 00:34:13.219
they can run in servers with
TFX and TensorFlow Serving.

00:34:13.219 --> 00:34:15.440
They can run in the
browser with TensorFlow.js

00:34:15.440 --> 00:34:17.630
or on Node servers
with TensorFlow.js.

00:34:17.630 --> 00:34:19.080
But then what we're going
to be talking about today

00:34:19.080 --> 00:34:20.747
is TensorFlow Lite,
where you can put it

00:34:20.747 --> 00:34:22.975
on mobile devices
including Android.

00:34:22.975 --> 00:34:24.350
So I'm going to
give a quick demo

00:34:24.350 --> 00:34:27.060
if we can switch to the laptop.

00:34:27.060 --> 00:34:29.600
So here, I've designed the
simplest neural network

00:34:29.600 --> 00:34:31.460
that I could come up with.

00:34:31.460 --> 00:34:34.510
At the back, if you cannot read
this code, just go like that.

00:34:37.008 --> 00:34:37.800
You cannot read it?

00:34:37.800 --> 00:34:38.830
OK, I'll make it bigger.

00:34:38.830 --> 00:34:42.340
Or do you just like doing this?

00:34:42.340 --> 00:34:43.719
Is that good?

00:34:43.719 --> 00:34:45.960
If you can read
it, go like this.

00:34:45.960 --> 00:34:47.409
OK.

00:34:47.409 --> 00:34:49.213
So this is the
simplest neural network

00:34:49.213 --> 00:34:51.130
I could come up with
where, if you can imagine

00:34:51.130 --> 00:34:54.310
a neural network, it is a
series of connected layers

00:34:54.310 --> 00:34:56.210
of these things called neurons.

00:34:56.210 --> 00:34:58.430
And so in this one,
I have a single layer

00:34:58.430 --> 00:35:00.490
and it has a single
neuron in it.

00:35:00.490 --> 00:35:02.830
So it's basically
just a simple function

00:35:02.830 --> 00:35:04.610
that calculates something.

00:35:04.610 --> 00:35:07.000
So I'm going to feed that
with a bunch of data.

00:35:07.000 --> 00:35:09.940
And here's the relationships
between the data on here,

00:35:09.940 --> 00:35:11.380
my X's and my Y's.

00:35:11.380 --> 00:35:14.087
Can anybody figure out what
these data are actually doing?

00:35:14.087 --> 00:35:16.420
If you were at my earlier
talk you've seen this already,

00:35:16.420 --> 00:35:17.200
I know.

00:35:17.200 --> 00:35:21.240
But the 0 and the
32 might be a clue.

00:35:21.240 --> 00:35:23.560
Yes, it's Celsius to
Fahrenheit converter.

00:35:23.560 --> 00:35:26.280
It's kind of given away
by the name F-to-C.

00:35:26.280 --> 00:35:29.770
Hoi thought that was
French to Cantonese, but--

00:35:29.770 --> 00:35:31.540
So what I'm going to
do is I'm just going

00:35:31.540 --> 00:35:32.897
to give it a bunch of data.

00:35:32.897 --> 00:35:34.480
And then what a
neural network will do

00:35:34.480 --> 00:35:35.938
is it will iterate
across that data

00:35:35.938 --> 00:35:37.646
and figure out the
patterns between them.

00:35:37.646 --> 00:35:38.688
So I'm going to run that.

00:35:38.688 --> 00:35:40.960
I know we're short on time
and the party's coming up.

00:35:40.960 --> 00:35:42.620
So let me run
through it quickly.

00:35:42.620 --> 00:35:45.280
So it's gone through
500 iterations of trying

00:35:45.280 --> 00:35:46.880
to figure out the pattern.

00:35:46.880 --> 00:35:50.470
And at the bottom-- this is so
big I have to scroll forever.

00:35:50.470 --> 00:35:53.600
Then at the bottom,
I'll go onto this node.

00:35:53.600 --> 00:35:55.390
So if I, for
example, pass in 100,

00:35:55.390 --> 00:35:59.290
I know 100 degrees
centigrade is 212 Fahrenheit.

00:35:59.290 --> 00:36:03.250
I'm getting 211.3 because I
only had a small amount of data.

00:36:03.250 --> 00:36:05.380
I only had six values,
and it's giving me

00:36:05.380 --> 00:36:08.860
a very high probability of the
relationship between those two

00:36:08.860 --> 00:36:11.450
numbers being centigrade
to Fahrenheit.

00:36:11.450 --> 00:36:12.740
So now I've built a model.

00:36:12.740 --> 00:36:14.193
It's a very, very simple model.

00:36:14.193 --> 00:36:15.610
But what am I going
to do with it?

00:36:15.610 --> 00:36:17.420
I want to use that on mobile.

00:36:17.420 --> 00:36:20.500
So with TensorFlow, the first
thing that you have to do

00:36:20.500 --> 00:36:21.690
is you save it out.

00:36:21.690 --> 00:36:23.620
And we've got a format
called Saved Model,

00:36:23.620 --> 00:36:26.900
which is used as a consistent
format across everything.

00:36:26.900 --> 00:36:29.830
So I'm going to save this out as
a Saved Model with a save keras

00:36:29.830 --> 00:36:31.520
model like this.

00:36:31.520 --> 00:36:33.190
And when that saves
out hopefully,

00:36:33.190 --> 00:36:35.440
if it does after a
bunch of warnings,

00:36:35.440 --> 00:36:38.260
it's going to give me the
path that it saved it to.

00:36:38.260 --> 00:36:39.820
And I'm going to take that--

00:36:39.820 --> 00:36:41.920
and this notebook is online.

00:36:41.920 --> 00:36:44.390
I'll share the code
for it in a moment--

00:36:44.390 --> 00:36:46.180
and I'm going to
run that in here.

00:36:46.180 --> 00:36:48.790
And now what it's doing is
it's calling the TF Lite

00:36:48.790 --> 00:36:51.760
converter to convert
that model to TF Lite.

00:36:51.760 --> 00:36:54.157
And it's given me
a number here, 620.

00:36:54.157 --> 00:36:55.740
Can anybody guess
what that number is?

00:36:58.490 --> 00:36:59.870
It's not an HTTP code.

00:36:59.870 --> 00:37:01.230
It's the size.

00:37:01.230 --> 00:37:04.220
So that model that I just
built, that's the size in bytes.

00:37:04.220 --> 00:37:08.870
So it's 620 bytes, and I
have a model called TF Lite--

00:37:08.870 --> 00:37:10.760
sorry, called
model.tflite-- that

00:37:10.760 --> 00:37:14.010
has been saved out at my temp
directory, and it's right here.

00:37:14.010 --> 00:37:16.610
So I can now download that
model and use that on Android.

00:37:16.610 --> 00:37:20.720
Can we switch back to
the slides, please?

00:37:20.720 --> 00:37:23.660
So now that I have that model,
this is the conversion process

00:37:23.660 --> 00:37:24.620
that I went through.

00:37:24.620 --> 00:37:26.470
I built it in TensorFlow,
I saved it out,

00:37:26.470 --> 00:37:29.450
I used the TF Lite converter,
and then I created a TF Lite

00:37:29.450 --> 00:37:30.690
model out of that.

00:37:30.690 --> 00:37:32.930
And here's the code that
I showed to do that.

00:37:32.930 --> 00:37:36.890
This QR code is the URI of the
notebook that I just showed.

00:37:36.890 --> 00:37:39.140
So if you want to play with
the notebook for yourself,

00:37:39.140 --> 00:37:41.160
it's on there.

00:37:41.160 --> 00:37:45.080
So now at runtime,
the diagram becomes

00:37:45.080 --> 00:37:47.970
we feed data into the model
and we get predictions out.

00:37:47.970 --> 00:37:49.470
So what I'm going
to do is I'm going

00:37:49.470 --> 00:37:52.070
to run this model on
mobile using Kotlin.

00:37:52.070 --> 00:37:54.050
So to be able to do
that, first of all

00:37:54.050 --> 00:37:58.490
I deploy my model to my Android
device as an asset file.

00:37:58.490 --> 00:38:00.380
That asset file, again,
can be just treated

00:38:00.380 --> 00:38:02.990
like any other asset in Android.

00:38:02.990 --> 00:38:05.900
Now one thing that on Android--
and the number one bug when I

00:38:05.900 --> 00:38:07.940
work with people
who use TF Lite--

00:38:07.940 --> 00:38:10.070
one thing is that
when you deploy assets

00:38:10.070 --> 00:38:14.120
to Android, Android itself
actually zips up those assets.

00:38:14.120 --> 00:38:16.070
It compresses them,
and TF Lite won't

00:38:16.070 --> 00:38:18.290
be able to interpret
it if it's compressed.

00:38:18.290 --> 00:38:20.420
So in your AAPT
options, make sure you

00:38:20.420 --> 00:38:23.510
say do not compress TF Lite.

00:38:23.510 --> 00:38:27.680
Then, in my build.gradle, I just
set the implementation to give

00:38:27.680 --> 00:38:29.430
me TensorFlow.Lite--

00:38:29.430 --> 00:38:30.560
TensorFlow Lite.

00:38:30.560 --> 00:38:33.300
And now in my code, I'm going
to do a couple of things.

00:38:33.300 --> 00:38:36.560
So first of all, I have to
create an instance of a TF Lite

00:38:36.560 --> 00:38:39.650
interpreter and something
called a mapped byte buffer.

00:38:39.650 --> 00:38:41.300
The mapped byte
buffer, what I'm doing

00:38:41.300 --> 00:38:43.760
is that that's just reading
the model out of assets

00:38:43.760 --> 00:38:46.370
so that I can create an
instance of that model.

00:38:46.370 --> 00:38:48.680
I'm going to set in the
options the number of threads

00:38:48.680 --> 00:38:50.090
I want the model to run on.

00:38:50.090 --> 00:38:53.090
And then the interpreter
itself can then run on those.

00:38:53.090 --> 00:38:55.370
My code itself-- and I'm
going to skip over this slide

00:38:55.370 --> 00:38:56.360
because we're
running out of time.

00:38:56.360 --> 00:38:58.070
I'm just going to
show it in the demo.

00:38:58.070 --> 00:39:00.360
So can we switch to the
demo machine, please?

00:39:03.400 --> 00:39:06.850
So I have a very
simple application

00:39:06.850 --> 00:39:08.380
running in the emulator here.

00:39:08.380 --> 00:39:10.540
This application, all it
has is a single button

00:39:10.540 --> 00:39:12.940
that says Do Inference,
but when I click that I'm

00:39:12.940 --> 00:39:14.145
going to go to Breakpoints.

00:39:14.145 --> 00:39:17.120
And now here's the breakpoint
with my actual code.

00:39:17.120 --> 00:39:19.660
So I want to pass the
number 100 into the model

00:39:19.660 --> 00:39:22.280
and get back like
211 and change.

00:39:22.280 --> 00:39:25.600
So that's the conversion of
one temperature to another.

00:39:25.600 --> 00:39:27.080
So I've created an array.

00:39:27.080 --> 00:39:29.590
And it's just a simple array
with one element in it.

00:39:29.590 --> 00:39:32.440
And that element contains
the 100 that I'm passing in.

00:39:32.440 --> 00:39:35.110
My output value is going
to be a byte buffer

00:39:35.110 --> 00:39:38.260
because the model is going to
feed me back a stream of bytes.

00:39:38.260 --> 00:39:40.360
We're going really
low-level with this.

00:39:40.360 --> 00:39:42.790
But I know that those bytes
are going to be a float,

00:39:42.790 --> 00:39:44.050
and a float is four bytes.

00:39:44.050 --> 00:39:47.050
So I'm just allocating a
byte buffer with four bytes.

00:39:47.050 --> 00:39:49.220
And then I'm going
to step through this

00:39:49.220 --> 00:39:50.410
and I'm going to run it.

00:39:50.410 --> 00:39:52.690
And when I run it, I'm
passing to TensorFlow Lite's

00:39:52.690 --> 00:39:56.500
interpreter the input value
of my input value array

00:39:56.500 --> 00:39:58.030
and then my output
value array where

00:39:58.030 --> 00:39:59.930
it's going to write the answer.

00:39:59.930 --> 00:40:02.260
So it writes the
answer into that,

00:40:02.260 --> 00:40:04.270
and now I can read
the answer out of that

00:40:04.270 --> 00:40:07.210
by just calling the get
float of that byte buffer.

00:40:07.210 --> 00:40:10.570
And we'll see here in the debug
that it was 211 and change.

00:40:10.570 --> 00:40:13.750
So it's a very,
very simple example,

00:40:13.750 --> 00:40:15.610
and it seems a
little bit complex

00:40:15.610 --> 00:40:18.310
with me using these arrays and
these byte buffers to do it.

00:40:18.310 --> 00:40:21.010
But the idea is that it's going
to be exactly the same code

00:40:21.010 --> 00:40:23.848
pattern that you use for a
far more complex example.

00:40:23.848 --> 00:40:26.140
We kind of just want to make
it so that it's consistent

00:40:26.140 --> 00:40:28.690
regardless of the
example that you use.

00:40:28.690 --> 00:40:31.210
I also want to show a really
cool new Kotlin language

00:40:31.210 --> 00:40:32.360
feature here.

00:40:32.360 --> 00:40:34.870
So if you see like on line--

00:40:34.870 --> 00:40:37.330
Android Studio numbers
my lines for me,

00:40:37.330 --> 00:40:41.350
and if you can see on it I
have a line 57 has TFLite.run.

00:40:41.350 --> 00:40:46.540
And actually in Kotlin,
I can then go, go to 47,

00:40:46.540 --> 00:40:48.950
and it will rerun it for me.

00:40:48.950 --> 00:40:52.185
No, not really.

00:40:52.185 --> 00:40:54.160
I'm seeing who was
paying attention.

00:40:54.160 --> 00:40:57.830
So if we can switch
back to the slides.

00:40:57.830 --> 00:41:00.042
So that was like a
very, very quick look

00:41:00.042 --> 00:41:01.750
at how you would
actually write that code

00:41:01.750 --> 00:41:05.170
and host that custom model
for yourself in TF Lite.

00:41:05.170 --> 00:41:06.640
So today, we've
actually seen all

00:41:06.640 --> 00:41:08.932
of these options that are
available for you in Android.

00:41:08.932 --> 00:41:11.530
And I'd like to invite
Matej back up onto the stage

00:41:11.530 --> 00:41:12.280
to wrap us up.

00:41:15.150 --> 00:41:15.810
Thanks, Matej.

00:41:15.810 --> 00:41:17.720
MATEJ PFAJFAR: Thank
you very much, Laurence.

00:41:17.720 --> 00:41:19.790
All right, folks,
there you have it.

00:41:19.790 --> 00:41:23.660
Android really has ML
capability built right into it.

00:41:23.660 --> 00:41:27.990
And we have great tools to help
you take advantage of that--

00:41:27.990 --> 00:41:31.540
the People Plus AI
Guidebook, Android APIs,

00:41:31.540 --> 00:41:36.740
ML Kit's exciting new features,
AutoML On-Device capability,

00:41:36.740 --> 00:41:41.060
and if you want to build your
own, TensorFlow is your friend.

00:41:41.060 --> 00:41:44.330
We've also created a new section
on the Android Developers

00:41:44.330 --> 00:41:47.120
website, which details
our Machine Learning

00:41:47.120 --> 00:41:48.860
offering across Google.

00:41:48.860 --> 00:41:53.720
Please check it out at
developer.android.com/ml.

00:41:53.720 --> 00:41:56.390
Today is just the
beginning for us at I/O,

00:41:56.390 --> 00:41:58.740
and we have many
great talks coming up,

00:41:58.740 --> 00:42:01.160
which will go into more
detail about everything

00:42:01.160 --> 00:42:03.050
that you have heard just now.

00:42:03.050 --> 00:42:05.670
And finally, if
you have questions,

00:42:05.670 --> 00:42:09.170
my team and I will be in the
AIML dome after this talk,

00:42:09.170 --> 00:42:11.460
so please do come and see us.

00:42:11.460 --> 00:42:13.640
Thank you so much for
being here with us today,

00:42:13.640 --> 00:42:14.880
and enjoy the rest of I/O.

00:42:14.880 --> 00:42:18.230
[MUSIC PLAYING]

