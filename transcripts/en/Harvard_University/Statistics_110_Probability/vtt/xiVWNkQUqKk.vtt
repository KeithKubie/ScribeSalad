WEBVTT
Kind: captions
Language: en

00:00:00.232 --> 00:00:02.730
So we're talking about
joint distributions, right?

00:00:02.730 --> 00:00:06.580
And there's a lot more to do with that,
so to just continue.

00:00:06.580 --> 00:00:14.529
So last time, we calculated the expected
distance between two iid uniforms, okay?

00:00:14.529 --> 00:00:18.599
So I wanted to do this analagous
problem for the normal.

00:00:18.599 --> 00:00:23.390
Because I think that's another nice
related example that has a different

00:00:23.390 --> 00:00:26.200
approach that makes it easier, okay?

00:00:26.200 --> 00:00:30.250
So last time,
we did expected absolute difference.

00:00:30.250 --> 00:00:33.452
This is just an example, but
I think it's a nice example.

00:00:33.452 --> 00:00:36.874
Expected absolute difference
between two uniforms, and

00:00:36.874 --> 00:00:40.410
what if we wanna do the same
thing with normals?

00:00:40.410 --> 00:00:44.754
So we wanna find
the expected value of say,

00:00:44.754 --> 00:00:50.250
let's call them Z1 and Z2.

00:00:50.250 --> 00:00:56.401
So, we did this with uniform last time,
now assume these are iid standard normal.

00:00:59.471 --> 00:01:02.013
Okay, wo last time we did this for
uniform,

00:01:02.013 --> 00:01:04.715
using the 2D version of LOTUS, right?

00:01:04.715 --> 00:01:06.745
Completely analogous to LOTUS,

00:01:06.745 --> 00:01:10.521
except we had a double integral
instead of a single integral.

00:01:10.521 --> 00:01:16.341
So these are iid standard normal.

00:01:16.341 --> 00:01:23.806
So, we could write down the 2D LOTUS here,
and try to do that integral.

00:01:23.806 --> 00:01:27.170
And because they're iid,
the joint PDF of Z1 and

00:01:27.170 --> 00:01:30.474
Z2 is just the product of
the two marginal PDFs.

00:01:30.474 --> 00:01:33.634
And well,
we could just try to do that integral, and

00:01:33.634 --> 00:01:36.720
we could probably get it with some effort.

00:01:36.720 --> 00:01:40.940
But that's not a good way to do this
problem, it's better to stop and

00:01:40.940 --> 00:01:44.140
think about the structure of the problem,
okay?

00:01:44.140 --> 00:01:49.290
So in the case of the uniforms,
we've never particularly studied,

00:01:49.290 --> 00:01:52.080
what are the properties of
the difference of two uniforms?

00:01:52.080 --> 00:01:52.700
On the other hand,

00:01:52.700 --> 00:01:55.990
the difference of normals is
something we've talked about before.

00:01:55.990 --> 00:01:58.220
So instead of jumping right into
this two-dimensional thing,

00:01:58.220 --> 00:02:00.848
let's see if we can actually
simplify the problem first.

00:02:00.848 --> 00:02:07.376
So in fact, we've mentioned before that
the sum of independent normals is normal.

00:02:07.376 --> 00:02:09.203
We haven't proven that yet, but

00:02:09.203 --> 00:02:11.885
we have all the tools to
be able to prove that now.

00:02:11.885 --> 00:02:17.381
So let's just do that quickly
to verify what I said before

00:02:17.381 --> 00:02:22.600
about the sum of normals,
so just a little theorem.

00:02:22.600 --> 00:02:25.430
This is gonna be easy now,
because we know MGFs.

00:02:26.790 --> 00:02:29.946
The sum of normals, so
we stated this before.

00:02:29.946 --> 00:02:34.941
If X is, let's say N(mu 1,
sigma 1 squared), and

00:02:34.941 --> 00:02:40.605
y is N(mu 2, sigma 2 squared) and
they're independent,

00:02:44.411 --> 00:02:47.111
X has to be independent of Y,
otherwise this won't work.

00:02:47.111 --> 00:02:52.671
Then the sum, we talked about this before,

00:02:52.671 --> 00:02:57.331
by linearity the means just add, and

00:02:57.331 --> 00:03:00.504
also the variances add.

00:03:00.504 --> 00:03:03.624
And we talked about the fact
that if we took a difference,

00:03:03.624 --> 00:03:05.774
we would take the difference of means.

00:03:05.774 --> 00:03:08.590
But we would still add the variances,
not subtract.

00:03:08.590 --> 00:03:14.070
Because if this were -Y, you would
just think of it as plus -Y, okay?

00:03:14.070 --> 00:03:20.370
So anyway, let's just prove this fact now,
which we haven't done yet,

00:03:20.370 --> 00:03:23.332
and this is just an easy MGF calculation.

00:03:23.332 --> 00:03:33.670
So we just use the MGFs, So
let's get the MGF of X + Y.

00:03:33.670 --> 00:03:37.408
Since they're independent, we talked
about the fact that since they're

00:03:37.408 --> 00:03:40.680
independent ,we can just multiple
the MGF of X times the MGF of Y.

00:03:40.680 --> 00:03:45.947
The MGF of a normal, well, we derived
the MGF of a standard normal before.

00:03:45.947 --> 00:03:49.081
But it's very easy to get from
a standard normal to any normal, right?

00:03:49.081 --> 00:03:54.202
If we do this thing, mu + sigma z, we can
immediately get the MGF of any normal.

00:03:54.202 --> 00:03:58.916
And that's just gonna be e to the mu 1 t,

00:03:58.916 --> 00:04:02.241
this is the MGF of x, mu 1 t,

00:04:02.241 --> 00:04:07.850
+ one-half sigma 1 squared t squared.

00:04:07.850 --> 00:04:10.520
That's the MGF of X.

00:04:10.520 --> 00:04:13.513
We multiply by the MGF of Y,
which is the same thing,

00:04:13.513 --> 00:04:15.442
you just change the subscripts.

00:04:15.442 --> 00:04:21.394
Mu 2 t + one-half sigma 2
squared t squared equals,

00:04:23.892 --> 00:04:28.220
Now let's just write this as
one exponential and factor.

00:04:28.220 --> 00:04:33.084
So that's e to the mu 1 + mu 2 t,
just factor out

00:04:33.084 --> 00:04:38.213
the t + one-half,
this is all up in the exponent.

00:04:38.213 --> 00:04:43.941
One-half t squared (sigma 1
squared + sigma 2 squared), right?

00:04:43.941 --> 00:04:47.680
Sigma 1 squared plus sigma
2 squared t squared.

00:04:48.890 --> 00:04:52.720
Okay well, I ran out of space on this
board, but that's the end of the proof.

00:04:52.720 --> 00:04:57.757
Because all we have to do is just say,
look, that's the MGF.

00:04:57.757 --> 00:05:02.437
I have little more space,
that's the MGF of N(mu 1 + mu 2,

00:05:02.437 --> 00:05:09.693
sigma 1 squared + sigma 2 squared),
All right, so since the MGF determines

00:05:09.693 --> 00:05:13.170
the distribution, then that's the end,
we don't have to do anything else.

00:05:13.170 --> 00:05:17.026
So, it's a very easy calculation,
using MGFs.

00:05:17.026 --> 00:05:22.929
Okay, so now that we've proven that fact,
and we see this thing, z1- z2.

00:05:22.929 --> 00:05:27.270
Rather than jumping into the 2D LOTUS,
let's just say, what is that?

00:05:27.270 --> 00:05:32.468
Well note that Z1- Z2 is

00:05:32.468 --> 00:05:38.750
N(0, 2), just add the variances.

00:05:40.060 --> 00:05:44.904
So really all we're asking is for
the expected value of,

00:05:47.805 --> 00:05:51.911
Expected value of absolute value of,
now when we say N(0, 2),

00:05:51.911 --> 00:05:56.200
let's once again think about that
as location and scale, right?

00:05:56.200 --> 00:05:59.710
We could take a standard normal, and
multiply by the square root of 2, and

00:05:59.710 --> 00:06:00.900
that would give us variance 2.

00:06:00.900 --> 00:06:05.698
So the easiest way to think
of this is as square root

00:06:05.698 --> 00:06:10.626
of 2 times Z,
where Z is standard normal, right?

00:06:10.626 --> 00:06:14.324
That's just the scale,
that gives it variance 2.

00:06:14.324 --> 00:06:21.457
Now this is just square root of 2 E|Z|.

00:06:21.457 --> 00:06:24.451
Now its just a one-dimensional LOTUS.

00:06:24.451 --> 00:06:26.799
And this is a LOTUS that
you've actually seen.

00:06:26.799 --> 00:06:29.959
If you studied strategic practice five,
we did this.

00:06:29.959 --> 00:06:32.974
But whether you remember
ever looking at that or not,

00:06:32.974 --> 00:06:35.256
doesn't matter, this is a easy LOTUS.

00:06:35.256 --> 00:06:39.195
Whereas here, you have to do a double
integral, here I just write down LOTUS.

00:06:39.195 --> 00:06:42.199
So I'll do this quickly,
cuz on the strategic practice,

00:06:42.199 --> 00:06:43.743
it's just write down LOTUS.

00:06:43.743 --> 00:06:47.676
Integral minus infinity,

00:06:47.676 --> 00:06:52.806
to infinity |z| 1 over root 2 pi e

00:06:52.806 --> 00:06:59.128
to -z squared over 2 dz, And

00:06:59.128 --> 00:07:04.494
notice that this is an even function,

00:07:08.955 --> 00:07:11.949
That is, if we replace z by -z,
we get the same thing.

00:07:11.949 --> 00:07:17.502
So we can just multiply by 2 and
go from 0 to infinity.

00:07:17.502 --> 00:07:20.960
And once we go from zero to infinity,
we can drop the absolute values.

00:07:20.960 --> 00:07:23.180
Then it's just z e to
the minus z squared over 2.

00:07:23.180 --> 00:07:26.049
That's a really easy
u-substitution integral, right,

00:07:26.049 --> 00:07:30.060
cuz you can just let u equals z squared,
or u equals z squared over 2 if you like.

00:07:30.060 --> 00:07:34.277
And then you get exactly what you want,
so that's then an easy integral.

00:07:34.277 --> 00:07:37.821
And if you simplify it,
you get square root 2 over pi,

00:07:37.821 --> 00:07:40.301
which should be an easy calculation.

00:07:40.301 --> 00:07:45.200
It's also on the strategic practice, so I
won't write out more of that calculation.

00:07:45.200 --> 00:07:49.471
So then that becomes just
a simple one-dimensional LOTUS,

00:07:49.471 --> 00:07:52.327
that's a much better way to think of it.

00:07:52.327 --> 00:07:56.838
All right, so just an example that you
don't always have to jump into the 2D

00:07:56.838 --> 00:08:00.310
LOTUS, just cuz you have this
function of two variables.

00:08:01.990 --> 00:08:05.342
Okay, so that's a continuous example.

00:08:05.342 --> 00:08:09.270
I wanted to do some more discrete stuff.

00:08:09.270 --> 00:08:15.802
In particular,
to introduce the multinomial distribution.

00:08:17.841 --> 00:08:22.438
Which is by far the most important
discrete multivariate distribution, and

00:08:22.438 --> 00:08:25.490
I'll tell you multivariate
distribution means.

00:08:25.490 --> 00:08:27.285
So this is gonna be
called the multinomial.

00:08:29.952 --> 00:08:35.532
A multivariate distribution just
means that's a joint distribution for

00:08:35.532 --> 00:08:38.597
more than one random variable, right?

00:08:38.597 --> 00:08:45.421
So we have all these normals and
Poisson and geometric, and so on.

00:08:45.421 --> 00:08:48.420
Those are all univariate distributions,
cuz we have one random variable.

00:08:48.420 --> 00:08:51.510
Now we're working with more than
one random variable at once.

00:08:51.510 --> 00:08:56.034
And for this course, there's really only
two multivariate distributions that you

00:08:56.034 --> 00:08:57.189
need to know by name.

00:08:57.189 --> 00:09:00.733
One is the multinomial,
which we are about to do.

00:09:00.733 --> 00:09:04.914
The other one is the multinomial which
is the generalization of the normal

00:09:04.914 --> 00:09:09.373
distribution to higher dimensions,
and we'll get to that one later, okay?

00:09:09.373 --> 00:09:15.155
So multinomial as the name might suggest
it's a generalization of the binomial,

00:09:15.155 --> 00:09:15.753
right?

00:09:15.753 --> 00:09:17.693
Bi becomes multi, okay?

00:09:17.693 --> 00:09:22.621
So it's like a higher dimensional
version of the binomial,

00:09:22.621 --> 00:09:26.110
and let's just introduce it by its story.

00:09:26.110 --> 00:09:33.764
So this is the definition and
story, Of the multinomial,

00:09:33.764 --> 00:09:38.679
which I'll sometimes just
abbreviate to mult of np.

00:09:38.679 --> 00:09:43.330
It has to parameters,
n and p like the binomial,

00:09:43.330 --> 00:09:48.105
except in this case,
this p is actually a vector.

00:09:48.105 --> 00:09:52.140
So p = vector, let's say P1 through Pk,

00:09:52.140 --> 00:09:57.081
where we assume that that's
a probability vector.

00:09:59.399 --> 00:10:03.388
And by probability vector all,
I mean is that these are non negative and

00:10:03.388 --> 00:10:04.130
add up to 1.

00:10:04.130 --> 00:10:08.133
Cuz we're gonna think of them as
probabilities for disjoint cases,

00:10:08.133 --> 00:10:10.556
so that encompasses all possibilities.

00:10:10.556 --> 00:10:15.400
So we want pj greater than or equal to 0,

00:10:15.400 --> 00:10:18.964
and the sum of all pj's = 1.

00:10:18.964 --> 00:10:20.744
That's the assumption, okay?

00:10:20.744 --> 00:10:24.231
So the binomial would just be
if this is one dimensional and

00:10:24.231 --> 00:10:27.588
then we just have binomial np,
but now we have k of them.

00:10:27.588 --> 00:10:30.498
So the intuition is that in the binomial,

00:10:30.498 --> 00:10:34.087
we just talked about success and
failure, right?

00:10:34.087 --> 00:10:38.021
There are two possible outcomes,
there are two categories.

00:10:38.021 --> 00:10:44.708
Multinomial means instead of two
categories, we have k categories, okay?

00:10:44.708 --> 00:10:46.832
So it's a natural extension, right?

00:10:46.832 --> 00:10:50.426
And binomial, we have to classify
everything as either success or

00:10:50.426 --> 00:10:51.783
failure for each trial.

00:10:51.783 --> 00:10:56.265
Here we have more than two
possible result, okay?

00:10:56.265 --> 00:11:00.521
So we say that x is

00:11:00.521 --> 00:11:05.082
multinomial np,

00:11:09.015 --> 00:11:13.524
We think of that as saying that, so

00:11:13.524 --> 00:11:17.884
in this case, X is also a vector.

00:11:17.884 --> 00:11:22.140
This is a multivaried distribution so

00:11:22.140 --> 00:11:26.532
X = (X1 to Xk), if we can think of X.

00:11:26.532 --> 00:11:30.030
So like in the binomial,
we have n independent trials.

00:11:34.067 --> 00:11:38.300
But I'll just call them objects
instead of trials and each object,

00:11:38.300 --> 00:11:43.733
objects could be people, could be trials,
could be anything, so just very general.

00:11:43.733 --> 00:11:48.060
We have n objects that we
are categorizing, okay?

00:11:48.060 --> 00:11:55.576
We have n objects, which we are
independently putting into k categories.

00:11:56.806 --> 00:12:00.675
So there are k possible categories, and
the binomial is just success or failure,

00:12:00.675 --> 00:12:02.087
but now we have k categories.

00:12:06.172 --> 00:12:06.878
And there,

00:12:06.878 --> 00:12:11.621
each object is independently determined
which category it falls into, okay?

00:12:11.621 --> 00:12:15.172
Just like in the binomial,
we had independent Bernoulli trials.

00:12:15.172 --> 00:12:20.032
And if Pj is the probability
of category j,

00:12:20.032 --> 00:12:26.107
by P of category j,
I mean the probability that any one

00:12:26.107 --> 00:12:32.461
of these objects is in category j,
has probability Pj.

00:12:32.461 --> 00:12:37.408
And we interpret Xj is just the count,

00:12:37.408 --> 00:12:42.517
is the number of objects in category j.

00:12:42.517 --> 00:12:45.641
All right, so that was a lot of writing,
but the concept is really simple.

00:12:45.641 --> 00:12:49.033
We just have n things that we're
breaking them into categories, and

00:12:49.033 --> 00:12:52.142
then we just see how many objects
are in each category, right?

00:12:52.142 --> 00:12:56.741
So it's very natural, you can make up
as many examples of this as you want,

00:12:56.741 --> 00:12:58.283
really easily, right?

00:12:58.283 --> 00:13:00.923
Just anytime you're classifying
things into categories.

00:13:00.923 --> 00:13:03.477
It's very, very general.

00:13:03.477 --> 00:13:06.133
Okay, so let's find the PMF.

00:13:08.707 --> 00:13:11.889
So this is gonna be a joint PMF,
cuz it's a joint distribution.

00:13:14.284 --> 00:13:19.434
So we want the probability that x1 = n1,

00:13:19.434 --> 00:13:24.167
blah, blah, blah xk = nk., right?

00:13:24.167 --> 00:13:28.339
That's a joint PMF, we just need to say
what's the probability that there are n1

00:13:28.339 --> 00:13:32.460
objects in the first category and
to in the second category and so on?

00:13:32.460 --> 00:13:37.183
And we can immediately write
down the answer just by thinking

00:13:37.183 --> 00:13:40.434
back to how do we derive the binomial PMF.

00:13:40.434 --> 00:13:45.416
All we have to do is imagine
any particular sequence,

00:13:45.416 --> 00:13:52.850
it's gonna be P1 to the n1, P2 to the n2,
blah, blah, blah, Pk to the nk.

00:13:55.110 --> 00:13:58.091
Just to have a little
intuitive example in mind,

00:13:58.091 --> 00:14:02.248
let's just suppose this is very
similar to how we did the binomial.

00:14:02.248 --> 00:14:04.084
But just to quickly review and
generalize that.

00:14:04.084 --> 00:14:06.147
Suppose we just have three categories,

00:14:06.147 --> 00:14:08.461
just to have a little
mental picture in mind.

00:14:08.461 --> 00:14:11.458
We had three categories,
lnd let's just say our sequence, and

00:14:11.458 --> 00:14:13.245
let's just write one, two, three.

00:14:13.245 --> 00:14:15.578
Where one means category one and so on.

00:14:15.578 --> 00:14:24.406
So we might have a sequence like 23311112,
for example, okay?

00:14:24.406 --> 00:14:30.694
So let's put a couple more 2s, there
are four 2s two 3s four 1s for example.

00:14:30.694 --> 00:14:32.960
This says that the first
object is category 2, right?

00:14:32.960 --> 00:14:36.200
We're just categorising
the objects one by one.

00:14:36.200 --> 00:14:39.377
So any particular sequence like this,

00:14:39.377 --> 00:14:44.682
the probability would be P1 is
the probability of category one.

00:14:44.682 --> 00:14:48.711
Multiple to the power of how
many ones there are, right?

00:14:48.711 --> 00:14:51.313
I need to put another one there.

00:14:51.313 --> 00:14:52.983
P2 to the power of the number of twos and
so on.

00:14:52.983 --> 00:14:56.035
That will be the probability
of any specific

00:14:56.035 --> 00:14:59.341
sequence that has the desired counts,
right?

00:14:59.341 --> 00:15:01.393
But then we can permute
this however we want,

00:15:01.393 --> 00:15:03.935
then it's just going back
to those counting problems.

00:15:03.935 --> 00:15:07.198
How many ways are there to permute
the letters in the word pepper, or

00:15:07.198 --> 00:15:10.133
the letters in the word Mississippi or
something like that.

00:15:10.133 --> 00:15:12.844
Where you start with n factorial, but

00:15:12.844 --> 00:15:17.347
that overcounts because the twos
could have been in any order.

00:15:17.347 --> 00:15:18.581
The threes could have been in any order,

00:15:18.581 --> 00:15:20.078
the ones could have been in any order,
and so on.

00:15:20.078 --> 00:15:22.267
So you have to adjust for
that overcounting.

00:15:22.267 --> 00:15:27.459
Exactly like we did for the binomial,
so we just divide by n1 factorial,

00:15:27.459 --> 00:15:31.897
n2 factorial, blah, blah,
blah n k factorial to account for

00:15:31.897 --> 00:15:37.213
all the ways you could permute the 3s,
permute the 1s, permute the 2s.

00:15:37.213 --> 00:15:40.685
Of course, there's a constraint here,

00:15:40.685 --> 00:15:44.860
this is if n1 plus blah,
blah, blah plus nk = n.

00:15:46.010 --> 00:15:47.560
Otherwise, it doesn't make sense, right?

00:15:47.560 --> 00:15:48.863
Cuz we have n objects.

00:15:48.863 --> 00:15:52.467
We're assuming that every object
is in exactly one category.

00:15:52.467 --> 00:15:55.710
So it wouldn't make sense if
we added up these counts and

00:15:55.710 --> 00:15:58.338
they had too many or
too few, makes no sense.

00:15:58.338 --> 00:16:00.132
So it's 0, otherwise.

00:16:00.132 --> 00:16:05.450
That is if the sum of
these ns is not this n,

00:16:05.450 --> 00:16:08.520
then it's impossible, so it's 0.

00:16:11.472 --> 00:16:12.910
So that didn't require a calculation.

00:16:12.910 --> 00:16:16.040
It just required thinking
about an example like that and

00:16:16.040 --> 00:16:18.499
just so
different ways to promote things, right.

00:16:19.820 --> 00:16:22.020
So that's the joint PMF.

00:16:23.750 --> 00:16:25.570
It looks a lot like the binomial,
all right.

00:16:26.930 --> 00:16:31.000
So it's a generalization of the binomial
when you have more than two categories.

00:16:32.420 --> 00:16:36.440
So we'll come back to some other
properties of the multinomial later, but

00:16:36.440 --> 00:16:40.380
just to do a couple quick
properties to think about.

00:16:40.380 --> 00:16:44.002
We could ask about the marginal
distribution, conditional distribution,

00:16:44.002 --> 00:16:44.901
things like that.

00:16:48.990 --> 00:16:52.769
So let's think about
the marginal distribution first.

00:16:56.250 --> 00:17:00.189
Okay so we're letting X be multinomial.

00:17:03.153 --> 00:17:04.132
N, p.

00:17:04.132 --> 00:17:08.653
Sometimes I'll subscript a k,
just to indicate what the dimension is, so

00:17:08.653 --> 00:17:10.320
the number of categories.

00:17:12.260 --> 00:17:17.830
And suppose we want the marginal,
find the marginal distribution

00:17:19.400 --> 00:17:22.850
of just one of these component,
let's say Xj.

00:17:24.350 --> 00:17:29.250
So Xj is just how many people or
how many objects are in category j.

00:17:29.250 --> 00:17:32.008
We want its marginal distribution.

00:17:32.008 --> 00:17:34.181
What do you think that is?

00:17:41.287 --> 00:17:45.820
Yeah, binomial, why did you say binomial?

00:17:45.820 --> 00:17:48.926
Exactly, it's either nk or it isn't nk.

00:17:48.926 --> 00:17:51.515
So I mean if you said if
you look at your notes,

00:17:51.515 --> 00:17:54.991
how do you get from joint distribution
to marginal distribution?

00:17:54.991 --> 00:17:57.991
I would say if you take this thing and

00:17:57.991 --> 00:18:04.380
do k- 1 sigma sign sum over all
possible things, do a lot of algebra.

00:18:04.380 --> 00:18:07.410
But that's not thinking about it, right?

00:18:07.410 --> 00:18:11.270
To marginalize we'd sum up the joint or
we integrate in the continuous case.

00:18:11.270 --> 00:18:15.490
We sum in the discrete case,
sum of everything we don't want, okay?

00:18:15.490 --> 00:18:19.520
But instead let's just think about
the story, think about what it means.

00:18:19.520 --> 00:18:26.340
As you just said, each of these objects,
either it's in category j or it isn't.

00:18:26.340 --> 00:18:29.310
We're assuming they're
all independent trials.

00:18:29.310 --> 00:18:33.720
So if we define success to
mean being in category j,

00:18:33.720 --> 00:18:38.800
the probability of success
is pj in our object.

00:18:38.800 --> 00:18:40.210
So that's just immediate.

00:18:40.210 --> 00:18:45.300
I didn't write justification for this but
that just proved itself from the story

00:18:45.300 --> 00:18:48.516
you know it's a complete truth
just to say because the binomial,

00:18:48.516 --> 00:18:51.160
it's independent Bernoulli trial.

00:18:51.160 --> 00:18:54.732
That's the probability of success, okay?

00:18:54.732 --> 00:19:00.064
So we can get that immediately and in
particular that also gives us the mean and

00:19:00.064 --> 00:19:04.670
the variance without having to
do a calculation, E(Xj) = npj.

00:19:04.670 --> 00:19:09.785
And the expected value of the variance
because we derived the variance

00:19:09.785 --> 00:19:14.044
of the binomial before we
don't need to re-derive that.

00:19:14.044 --> 00:19:19.614
We already know the variance of a binomial
is np(1- p) so this npj (1- pj),

00:19:19.614 --> 00:19:24.480
no additional work needed
because we know it's binomial.

00:19:24.480 --> 00:19:27.290
Okay, so that's just immediate from
thinking about what this means.

00:19:28.620 --> 00:19:30.690
So that's one property.

00:19:30.690 --> 00:19:32.730
That's the marginals.

00:19:32.730 --> 00:19:36.680
And let's think about kind
of something similar.

00:19:37.770 --> 00:19:44.485
Let's call this, well,
I call this the lumping property.

00:19:48.108 --> 00:19:52.318
What happened, the question is,
we have all these categories,

00:19:52.318 --> 00:19:58.130
well what happens if we decide to merge
certain categories together, right?

00:19:58.130 --> 00:20:01.610
Okay, so just to have an example in mind,

00:20:03.370 --> 00:20:08.330
let's let K = 10, so
we're thinking of X as a vector.

00:20:08.330 --> 00:20:15.030
X1 through X10 and
just to have a concrete example in mind.

00:20:15.030 --> 00:20:23.364
So this is multinomial, let's say this
is multinomial, and, P1 through P10.

00:20:23.364 --> 00:20:25.665
And to have a concrete example in mind,

00:20:25.665 --> 00:20:30.280
well let's imagine we're in a country
that has ten political parties.

00:20:30.280 --> 00:20:34.930
Okay, and you take n people and
assume that the people

00:20:34.930 --> 00:20:38.610
are independent of each other, and
you wanna know how many people are in, and

00:20:38.610 --> 00:20:43.140
assume that everyone in this country is
a member of one of these ten parties.

00:20:43.140 --> 00:20:48.040
Okay, and then you take all
these people and you say, okay.

00:20:48.040 --> 00:20:50.310
Ask each person which party they're in.

00:20:51.890 --> 00:20:54.310
X1 is the number of people in
the first political party,

00:20:54.310 --> 00:20:58.350
X2 is the number in the second one,
and so on, right?

00:20:58.350 --> 00:21:00.900
So that that would be multinomial if

00:21:00.900 --> 00:21:04.960
these are the probabilities of the
different party memberships, all right?

00:21:04.960 --> 00:21:12.680
So now, what I call the lumping property
is what if it's a country where it's,

00:21:12.680 --> 00:21:17.190
there are only two dominant parties, and
all the other parties are much smaller?

00:21:17.190 --> 00:21:20.235
And so it might be kind of unwieldy to
deal with this ten dimensional vector.

00:21:20.235 --> 00:21:22.592
Maybe we wanna compress
all the third party,

00:21:22.592 --> 00:21:26.437
so suppose that the first two are kind
of the two dominant major parties and

00:21:26.437 --> 00:21:30.362
the rest of them are kind of minor, so
we may wanna just lump them together.

00:21:30.362 --> 00:21:32.129
So that's why I call it
the lumping property,

00:21:32.129 --> 00:21:34.260
lump all the other parties together.

00:21:34.260 --> 00:21:40.792
So what if we considered,
let's see, let Y = X1,

00:21:40.792 --> 00:21:46.660
X2, and
then group all these ones together.

00:21:46.660 --> 00:21:50.170
So I'll just add them up,
X3 plus blah, blah, blah,

00:21:50.170 --> 00:21:56.290
plus X10, right.

00:21:56.290 --> 00:21:59.650
So this would be like party one,
party two, and

00:21:59.650 --> 00:22:02.676
then other third party grouped together.

00:22:02.676 --> 00:22:05.611
Without doing any calculation or
algebra whatsoever,

00:22:05.611 --> 00:22:08.430
we can immediately write
down the distribution of Y.

00:22:08.430 --> 00:22:11.385
Y is just gonna be multinomial.

00:22:15.232 --> 00:22:16.535
Same n.

00:22:16.535 --> 00:22:21.533
And then all we've done is group
these categories together,

00:22:21.533 --> 00:22:24.704
but then it's the same problem again,

00:22:24.704 --> 00:22:30.485
it's just it has a larger probability
just lump together all those Ps.

00:22:33.841 --> 00:22:36.908
Okay, so this should be
obvious from the story, right.

00:22:36.908 --> 00:22:39.220
It's the same problem again.

00:22:39.220 --> 00:22:42.571
So just like we emphasized with
the binomial we can define success and

00:22:42.571 --> 00:22:43.856
failure however we want.

00:22:43.856 --> 00:22:48.406
Here we can rearrange the categories and
whatever, the only thing we

00:22:48.406 --> 00:22:53.740
need to make sure of is that each
object is in exactly one category.

00:22:53.740 --> 00:22:57.330
So it would not work if you could
be in more than one category or

00:22:57.330 --> 00:22:58.100
be in no categories.

00:22:58.100 --> 00:23:00.920
But if you define your
categories such that

00:23:00.920 --> 00:23:06.400
it's true that each object isn't
exactly one, then you get multinomial.

00:23:06.400 --> 00:23:09.020
Didn't need to do any algebra or
calculus to show that.

00:23:09.020 --> 00:23:10.050
So that's pretty nice.

00:23:11.110 --> 00:23:14.825
Similarly, let's get
the conditional distribution.

00:23:18.297 --> 00:23:21.730
So what if we wanted, so
again x is multinomial.

00:23:23.680 --> 00:23:29.159
What if we want a conditional distribution
where we got to learn what X1 is and

00:23:29.159 --> 00:23:34.570
we want the conditional distribution
of the rest given that we now know X1.

00:23:37.351 --> 00:23:39.193
So we want a conditional,

00:23:39.193 --> 00:23:44.070
you might call it a conditional
joint PMF because you're given X1.

00:23:45.910 --> 00:23:50.000
Let's say that we're given that X1 = n1,
okay?

00:23:50.000 --> 00:23:54.610
And then we want the conditional joint
distribution of everything else.

00:23:54.610 --> 00:23:58.330
So we know exactly how many
people are in the first category.

00:23:58.330 --> 00:23:59.820
But we don't know about the rest of them.

00:24:01.862 --> 00:24:06.139
Well, given that X1 = n1,

00:24:06.139 --> 00:24:12.991
we want the joint PMF of
the rest X2 through Xk.

00:24:18.304 --> 00:24:19.980
Still gonna be multinomial, but

00:24:19.980 --> 00:24:23.010
we have to be a little bit careful
with getting the parameters right.

00:24:26.170 --> 00:24:29.537
So now this is gonna be k- 1 dimensional,

00:24:29.537 --> 00:24:34.024
cuz we know how many people
are in the first category, but

00:24:34.024 --> 00:24:38.159
we're looking at the remaining
k- 1 categories.

00:24:38.159 --> 00:24:41.302
And the number of people, well,

00:24:41.302 --> 00:24:47.090
n- n1 have been allocated
into the first party, okay?

00:24:47.090 --> 00:24:50.390
So we have n- n1 people left.

00:24:50.390 --> 00:24:56.670
And then we just have to get
the probability vector, right?

00:24:58.710 --> 00:25:03.700
Now if we just wrote p2 through pk,
that would be a common mistake, but

00:25:03.700 --> 00:25:06.750
it should be easy to see that that's
a mistake because those don't add up to 1.

00:25:07.910 --> 00:25:11.530
So it can't just be p2 through pk, right?

00:25:12.880 --> 00:25:16.630
I'm imagining that I've taken, and
it doesn't matter which people.

00:25:16.630 --> 00:25:18.590
I can imagine,
I'm conditioning on the count.

00:25:18.590 --> 00:25:23.554
But then I could further condition on
which specific people are in category one,

00:25:23.554 --> 00:25:25.310
and then use symmetry.

00:25:25.310 --> 00:25:31.070
So I guess, so I may as well just assume
that the first n1 people are in category

00:25:31.070 --> 00:25:37.700
one, okay, but to get these ps, well, then
we have to think conditionally, right?

00:25:37.700 --> 00:25:40.716
So let's call this vector,

00:25:40.716 --> 00:25:46.496
let's call it p2 prime through
pk prime where somehow

00:25:46.496 --> 00:25:51.660
we have to figure out what's p2 prime and
so on.

00:25:51.660 --> 00:25:56.120
Because without the primes it doesn't
add up to one makes no sense.

00:25:56.120 --> 00:25:58.340
So let's find p2 prime for example.

00:26:04.020 --> 00:26:09.000
Intuitively, I want this to
be proportional to P2, right?

00:26:09.000 --> 00:26:12.020
Cuz I know how many people
are in the first party, but

00:26:12.020 --> 00:26:16.840
that shouldn't kind of affect the relative
distribution of the rest of the party.

00:26:16.840 --> 00:26:19.700
So basically you just have to renormalize.

00:26:19.700 --> 00:26:20.910
If I want to write that out,

00:26:20.910 --> 00:26:25.370
mathematically I would say P2
equals the probability of being in

00:26:27.270 --> 00:26:30.310
category 2, given

00:26:33.850 --> 00:26:38.409
a random object being in category 2 given,
that it's not in category 1.

00:26:40.900 --> 00:26:44.830
Because we've already thrown out
the ones that are in category 1.

00:26:44.830 --> 00:26:49.594
So just by the definition of conditional
probability, being in category 2 I

00:26:49.594 --> 00:26:54.577
take the intersection of this and this,
but once you say you're in category 2,

00:26:54.577 --> 00:26:58.340
you know you're not in category 1,
so that's redundant.

00:26:58.340 --> 00:27:00.580
So the numerator is just P2.

00:27:00.580 --> 00:27:04.865
And the denominator is 1- P1,

00:27:04.865 --> 00:27:12.215
that is just the probability
of not being in category 1.

00:27:12.215 --> 00:27:17.000
Or we could also write it as P2
over P2 + blah, blah, blah + Pk.

00:27:17.000 --> 00:27:21.717
So all this says is we've taken these and
you know similarly for

00:27:21.717 --> 00:27:26.880
the other ones Pj prime equals Pj
over P2 + blah, blah, blah + Pk.

00:27:26.880 --> 00:27:32.230
All this says is that we've renormalized
this, it's still multinomial.

00:27:32.230 --> 00:27:35.785
Okay, so multinomials have really
nice properties like this and you can

00:27:35.785 --> 00:27:39.873
see these things just by thinking about
what it means without doing a calculation.

00:27:39.873 --> 00:27:47.100
So that's a very useful distribution for
lots of applications.

00:27:49.980 --> 00:27:55.750
Okay, so we'll say more about
the multinomial in the next lecture or

00:27:55.750 --> 00:27:57.290
the lecture after.

00:27:57.290 --> 00:28:02.490
But I wanna do one more
continuous example as well,

00:28:02.490 --> 00:28:06.910
an example where we actually
do need to do a calculation.

00:28:09.030 --> 00:28:12.420
And this is another kind of famous one.

00:28:16.260 --> 00:28:21.603
Good example of how do
we work with joint PDFs,

00:28:21.603 --> 00:28:25.630
which I think we need more practice with
or at least you need more practice with,

00:28:25.630 --> 00:28:28.280
then I'll try to help with that,
so this is a good example.

00:28:30.400 --> 00:28:33.350
I call this the Cauchy Interview Problem.

00:28:37.170 --> 00:28:41.095
I call the Cauchy Interview Problem
not because Cauchy ask

00:28:41.095 --> 00:28:43.417
this as an interview problem, but

00:28:43.417 --> 00:28:48.560
because it sounds more interesting to
call it that than the Cauchy Problem.

00:28:48.560 --> 00:28:50.838
But actually for some reason,

00:28:50.838 --> 00:28:55.919
this doesn't seem like it should
be a common interview problem, but

00:28:55.919 --> 00:29:01.527
I've actually seen this on several
occasions asked as an interview problem

00:29:01.527 --> 00:29:07.470
just I think to test whether you can do
work with joint PDFs and things like that.

00:29:07.470 --> 00:29:10.085
So okay, it is an interview problem,

00:29:10.085 --> 00:29:13.289
though it sort of
shouldn't be in some sense.

00:29:13.289 --> 00:29:16.911
Anyway, I have to tell you what
the Cauchy, I mean Cauchy was a famous

00:29:16.911 --> 00:29:21.610
mathematician, but in this context, Cauchy
is referring to a specific distribution.

00:29:24.007 --> 00:29:25.730
The Cauchy distribution.

00:29:30.860 --> 00:29:35.754
It's a famous distribution
that has a lot of

00:29:35.754 --> 00:29:40.020
kind of weird, scary properties.

00:29:40.020 --> 00:29:43.943
I just got some of these distribution
plushies that I found online.

00:29:43.943 --> 00:29:47.650
I might bring them, but they're
a little bit small to show you here.

00:29:47.650 --> 00:29:52.570
But if you come to my office hours,
you can see them in my office.

00:29:52.570 --> 00:29:58.620
But little pillows illustrating different
distributions, I have them in my office.

00:29:58.620 --> 00:30:03.558
The Cauchy is called the evil Cauchy and

00:30:03.558 --> 00:30:06.404
it looks pretty evil.

00:30:06.404 --> 00:30:09.270
And so let me first tell you
what the distribution is, and

00:30:09.270 --> 00:30:11.940
then tell you a little
bit about why is it evil.

00:30:11.940 --> 00:30:14.660
And then we'll try to find its PDF,
which as I said,

00:30:14.660 --> 00:30:18.190
has been a common interview problem,
find the PDF of a Cauchy.

00:30:18.190 --> 00:30:19.830
That's the problem, okay?

00:30:19.830 --> 00:30:24.760
So the Cauchy is
the distribution of let's say,

00:30:24.760 --> 00:30:31.850
X over Y with x and y iid standard normal.

00:30:31.850 --> 00:30:34.900
So it's a simple definition,

00:30:34.900 --> 00:30:40.110
just take a ratio of two iid standard
normals and we call that a Cauchy.

00:30:40.110 --> 00:30:42.620
And you can see why that could
be a useful distribution for

00:30:42.620 --> 00:30:48.440
a lot of different applications where
ratios is a pretty natural thing.

00:30:48.440 --> 00:30:51.450
So that's a Cauchy, and
the problem is find the PDF.

00:30:55.760 --> 00:30:58.940
Of this random variable, okay?

00:30:58.940 --> 00:31:00.368
Let's call this thing T.

00:31:03.470 --> 00:31:04.990
Find PDF of T.

00:31:04.990 --> 00:31:08.091
So we're defining T to be
the ratio of iid standard normals.

00:31:08.091 --> 00:31:09.219
We want to find its PDF.

00:31:09.219 --> 00:31:13.430
All right, so that doesn't yet
answer why this is evil.

00:31:13.430 --> 00:31:18.397
Well, some properties of the Cauchy
that we're not gonna prove right now,

00:31:18.397 --> 00:31:22.059
but just to kind of foreshadow
why is this thing so evil.

00:31:22.059 --> 00:31:24.160
First of all,
it does not have an expected value.

00:31:24.160 --> 00:31:28.630
If you try to compute e,
expected value, it'll blow up.

00:31:28.630 --> 00:31:30.269
No, no, no, that's not that evil.

00:31:30.269 --> 00:31:34.523
There are a lot of distributions where if
you try to compute the expected value,

00:31:34.523 --> 00:31:35.230
it blows up.

00:31:35.230 --> 00:31:38.345
So it does not have a mean,
it doesn't have a variance.

00:31:38.345 --> 00:31:43.411
The thing that's really evil about the
Cauchy is that If you take iid cauchys,

00:31:43.411 --> 00:31:47.120
so let's say don't just have T,
we have T1 through TN.

00:31:47.120 --> 00:31:51.500
They're just iid ratios of normals.

00:31:53.300 --> 00:31:58.420
When we get to the law of large numbers
later in the course, we'll see that when

00:31:58.420 --> 00:32:03.695
we average a bunch of iid random
variables, we What happens

00:32:03.695 --> 00:32:08.797
if we average a lot of them is that
should be close to their mean, right?

00:32:08.797 --> 00:32:13.782
You average a lot of IID things
that should be close to the mean.

00:32:13.782 --> 00:32:15.468
In this case there is no mean.

00:32:15.468 --> 00:32:19.412
But the weird fact is that if you
average all these Cauchy, IID Cauchy

00:32:19.412 --> 00:32:24.810
the distribution of that average is still
a Cauchy, doesn't change the distribution.

00:32:24.810 --> 00:32:29.510
You can average a million IID
cosye it's still gonna be Cauchy.

00:32:29.510 --> 00:32:35.260
So in some sence you that's kind of you're
hoping soil as you collect more and

00:32:35.260 --> 00:32:38.610
more data you're hoping to converge
to the truth in some sense.

00:32:38.610 --> 00:32:41.740
In this case if all you do
is average then you just

00:32:41.740 --> 00:32:44.560
not getting anywhere
the distribution doesn't change.

00:32:44.560 --> 00:32:45.780
There are other ways to work with,

00:32:45.780 --> 00:32:48.660
if you had Cauchy data there
are other ways to work with it.

00:32:48.660 --> 00:32:52.060
It would be a bad idea to just take
the naively average everything,

00:32:52.060 --> 00:32:54.720
there are other things you could do.

00:32:54.720 --> 00:32:57.200
Okay, so
any way that's the Cauchy Distribution.

00:32:57.200 --> 00:33:02.410
Now let's find the PDF, just for
practice with our joint distributions.

00:33:02.410 --> 00:33:04.569
And there are several
ways we could do this.

00:33:05.610 --> 00:33:10.640
One way would be to use
the law of total probability,

00:33:10.640 --> 00:33:13.520
and condition on y to make things easier.

00:33:13.520 --> 00:33:15.840
And that's a perfectly good way to do it.

00:33:15.840 --> 00:33:19.700
But I think I wanna just start by
practicing just more directly how to just

00:33:20.710 --> 00:33:22.660
directly get the CDF.

00:33:22.660 --> 00:33:26.780
Let's find the CDF, and
take the derivative and get the PDF.

00:33:26.780 --> 00:33:31.170
So with the CDF we could use
the law of total probability, but

00:33:31.170 --> 00:33:32.860
let's just directly write down.

00:33:32.860 --> 00:33:35.930
It's going to be a double
integral because we have an X and

00:33:35.930 --> 00:33:40.910
a Y and let's just write down that double
integral and see if we can do it, okay?

00:33:40.910 --> 00:33:43.800
So let's find the CDF.

00:33:43.800 --> 00:33:47.900
So the probability that
x over y is less than or

00:33:47.900 --> 00:33:51.720
equal to some number, t,
that's what we need for the CDF.

00:33:51.720 --> 00:33:56.070
This is practice with, this is an event,

00:33:57.300 --> 00:33:59.370
it's an event that the ratio
is less than or equal to t.

00:33:59.370 --> 00:34:04.240
We want to find some probability of
an event where it's based on x and

00:34:04.240 --> 00:34:07.230
y, so unless we can think
of some clever trick for

00:34:07.230 --> 00:34:10.430
simplifying this we basically
have to do a double integral.

00:34:10.430 --> 00:34:14.470
Or else, we can use the law of total
probability and do a single integral,

00:34:14.470 --> 00:34:19.300
but I actually don't think
that's any easier here.

00:34:19.300 --> 00:34:22.030
So my first impulse would be to
multiply both sides by y here.

00:34:22.030 --> 00:34:27.168
But you have to be careful in doing
that because y could be negative,

00:34:27.168 --> 00:34:31.135
so we can simplify this
a little bit by using symmetry

00:34:31.135 --> 00:34:35.870
first, And putting absolute values.

00:34:35.870 --> 00:34:39.532
This follows from the symmetry
of of the normal.

00:34:44.041 --> 00:34:48.961
And you can think through for yourself
exactly how I'm using symmetry here, but

00:34:48.961 --> 00:34:51.070
the basic idea is with the normal.

00:34:51.070 --> 00:34:56.060
If I have a standard normal and multiply
it by minus 1 it's still standard normal,

00:34:56.060 --> 00:35:00.560
if I multiply it, if I randomly chose say
with probably one half multiply it by

00:35:00.560 --> 00:35:04.360
minus 1, probably one half do
nothing It's still standard normal.

00:35:05.370 --> 00:35:09.160
Have the same symmetry in the denominator,
so sort of have two symmetric things.

00:35:09.160 --> 00:35:12.370
And we might as well just kind
of absorb the plusses and

00:35:12.370 --> 00:35:16.110
minuses and write it this way,
follows from symmetry.

00:35:16.110 --> 00:35:19.890
The reason I wanted to do that is just so
that I could write this as x less than or

00:35:19.890 --> 00:35:24.230
equal to t absolute y, without

00:35:24.230 --> 00:35:28.320
having to flip the inequality or worrying
about whether the inequality flips.

00:35:28.320 --> 00:35:35.150
Now let's just write this down
as a double integral, okay?

00:35:35.150 --> 00:35:38.383
We are saying that x so

00:35:38.383 --> 00:35:43.054
we can either dxdy or dydx, but

00:35:43.054 --> 00:35:48.996
let's suppose that we are doing dx dy.

00:35:48.996 --> 00:35:54.494
And to get a probability well what we do,

00:35:54.494 --> 00:35:58.700
we integrate the joint PDF over

00:35:58.700 --> 00:36:03.245
whatever region we want, okay?

00:36:03.245 --> 00:36:07.750
So Y goes from minus infinity to infinity.

00:36:09.040 --> 00:36:12.330
And, the main thing again to be careful
about, is the limits of integration.

00:36:12.330 --> 00:36:18.700
X, the inner limits can depend on Y,

00:36:18.700 --> 00:36:23.770
and X, we're looking at the region
that goes up to t absolute y.

00:36:23.770 --> 00:36:29.450
So x goes from minus infinity
to t absolute y, and

00:36:29.450 --> 00:36:35.760
then what we're integrating
is just the joint PDF, right?

00:36:35.760 --> 00:36:42.370
So the joint PDF is 1 over root 2 pie,
e to the minus x squared over 2.

00:36:42.370 --> 00:36:48.800
And then same thing for the y,
1 over root 2 pi e to the minus y square,

00:36:48.800 --> 00:36:53.170
because they're IID standard normal.

00:36:53.170 --> 00:36:58.110
So the other term, e to the minus y
squared over 2, doesn't depend on x.

00:36:58.110 --> 00:37:01.520
So I could write it here but
I could immediately then pull it out here.

00:37:01.520 --> 00:37:05.160
So I may as well write it here so
that it's not interfering with this part.

00:37:05.160 --> 00:37:07.440
Sets e to the minus y square over two and

00:37:07.440 --> 00:37:10.660
there is another 1/2 pi
this just stick over there.

00:37:10.660 --> 00:37:14.970
So all I did is write
down the normal PDF for

00:37:14.970 --> 00:37:20.030
x and the normal PDF for y and
I took the y part cuz that depends on x.

00:37:20.030 --> 00:37:24.832
That looks pretty ugly so
let's see if we can do it, well,

00:37:24.832 --> 00:37:31.695
one thing that we could simplify is just
recognizing what do we actually have here.

00:37:31.695 --> 00:37:34.683
So we have this integral,
minus infinity to infinity,

00:37:34.683 --> 00:37:38.447
e to the minus y squared over 2, and
then we have this inner integral.

00:37:40.859 --> 00:37:43.600
Okay?
Now in one sense we can't do

00:37:43.600 --> 00:37:44.230
this integral.

00:37:45.995 --> 00:37:52.090
Because that's the normal PDF and you can
prove that you can't do that integral.

00:37:52.090 --> 00:37:54.850
And in another sense,
not only can you do that integral,

00:37:54.850 --> 00:37:56.606
you already know what
that integral is right?

00:37:56.606 --> 00:38:01.839
That's just capital phi evaluated here,
that's just the normal CDF.

00:38:01.839 --> 00:38:06.842
So actually it's just phi,
so depending on whether you

00:38:06.842 --> 00:38:12.597
consider that doing the integral or
not, it's just that, dy.

00:38:12.597 --> 00:38:16.910
That's just the definition of
the standard, normal CDF, okay?

00:38:18.970 --> 00:38:22.610
Now these absolute value signs
are a little bit annoying.

00:38:22.610 --> 00:38:27.640
So, let's notice that we
have an even function,

00:38:27.640 --> 00:38:30.980
because y squared, absolute value y,
this is an even function.

00:38:30.980 --> 00:38:33.860
So we may as well go from 0 to
infinity instead, and multiply by 2.

00:38:33.860 --> 00:38:38.428
So then we'd have a square
root of 2 over pi.

00:38:38.428 --> 00:38:41.794
I just multiplied by 2, and

00:38:41.794 --> 00:38:46.423
then we're going from 0 to infinity,

00:38:46.423 --> 00:38:52.189
e to the -y squared over
two capital phi of ty dy.

00:38:58.489 --> 00:39:04.793
All right, and then you know, the clock
is ticking on our job interview and

00:39:04.793 --> 00:39:09.713
we've get here and
it's sort of possibly start to panic.

00:39:09.713 --> 00:39:14.060
And that capital phi is
an intractable integral,

00:39:14.060 --> 00:39:20.260
that's why we call it capital phi,
it's cuz we couldn't do it.

00:39:20.260 --> 00:39:25.031
Now, you are being asked in
your interview to integrate

00:39:25.031 --> 00:39:28.077
an integral that you couldn't do,

00:39:28.077 --> 00:39:34.098
sounds pretty bad, However.

00:39:35.806 --> 00:39:39.787
One thing that might help,
is that on the interview,

00:39:39.787 --> 00:39:45.700
we were asked to find the PDF,
not to find the CDF, that's the CDF.

00:39:45.700 --> 00:39:48.810
And we know that the PDF is
the derivative of the CDF, so

00:39:48.810 --> 00:39:53.120
the PDF is the derivative of the integral
of an integral that we can't do.

00:39:53.120 --> 00:39:55.940
So somehow maybe that will save us.

00:39:55.940 --> 00:39:57.539
So let's take the derivative.

00:40:00.198 --> 00:40:08.120
So here's the PDF,
PDF is the derivative of the CDF.

00:40:08.120 --> 00:40:12.900
This thing is capital F(t),
if we call the CDF capital F.

00:40:12.900 --> 00:40:19.193
The PDF is the derivative, F'(t).

00:40:19.193 --> 00:40:21.585
So we're taking the derivative
with respect to t,

00:40:21.585 --> 00:40:24.410
not with respect to y
which would make no sense.

00:40:24.410 --> 00:40:26.950
Notice that this y is a dummy variable,
okay?

00:40:26.950 --> 00:40:32.282
This is a function of t we're taking
the derivative with respect to t.

00:40:32.282 --> 00:40:35.097
Okay, now there's a theorem
in calculus that says,

00:40:35.097 --> 00:40:39.666
under some pretty mild conditions, if you
have a reasonably well-behaved thing that

00:40:39.666 --> 00:40:44.120
you're integrating, you can exchange
the derivative and the interval.

00:40:44.120 --> 00:40:46.861
This is a very,
very well behaved function.

00:40:46.861 --> 00:40:52.898
Capital Phi is just a continuous
differentiable thing between 0 and 1.

00:40:52.898 --> 00:40:56.617
Either the -y squared over 2,
that's infinitely differentiable.

00:40:56.617 --> 00:41:00.746
It decays to 0 very fast, so
this is a very, very nice function.

00:41:00.746 --> 00:41:04.076
So there's gonna be no technical problem
whatsoever with swapping the derivative

00:41:04.076 --> 00:41:04.843
and the integral.

00:41:04.843 --> 00:41:10.171
We're gonna take the derivative
of this with respect to t,

00:41:10.171 --> 00:41:13.910
and then we're gonna try to simplify it.

00:41:13.910 --> 00:41:17.580
So we take the derivative,
bring the derivative inside, okay?

00:41:17.580 --> 00:41:24.750
So we have the integral 0 to infinity,
e to the -y squared over 2.

00:41:25.980 --> 00:41:31.635
We're differentiating with respect to t,
we're bringing in a d with respect to dt.

00:41:31.635 --> 00:41:34.985
So we're treating either the -y square
derivitive which behaves as a constant

00:41:34.985 --> 00:41:37.050
when we're differentiating
with respect to t.

00:41:37.050 --> 00:41:41.858
Then we take the derivative of
capital Phi of ty, by the chain rule,

00:41:41.858 --> 00:41:45.460
y is gonna come out, because we
are differentiating with respect to t.

00:41:45.460 --> 00:41:48.470
So y is going to come out
from the chain rule, y.

00:41:48.470 --> 00:41:53.246
And then we just need
the derivative of this, but

00:41:53.246 --> 00:41:57.557
the derivative of
the standard normal CDF is

00:41:57.557 --> 00:42:02.800
the standard normal PDF,
which is 1 over root 2pi,

00:42:02.800 --> 00:42:07.833
e to the -z squared over 2
in general where z is ty.

00:42:07.833 --> 00:42:13.673
So it's e to the -t squared,
y squared over 2.

00:42:13.673 --> 00:42:16.885
I just squared this thing divided by 2dy.

00:42:19.290 --> 00:42:20.540
Now let's see if we can do it.

00:42:23.745 --> 00:42:27.490
So the square root of 2 here
cancels this square root of 2.

00:42:27.490 --> 00:42:31.092
We have square root of pi, square root
of pi, so we're gonna get 1 over pi.

00:42:32.995 --> 00:42:38.104
And then we just need to integrate

00:42:38.104 --> 00:42:43.213
from 0 to infinity of ye to the -t

00:42:43.213 --> 00:42:47.780
squared y squared over 2dy.

00:42:54.387 --> 00:42:56.475
Now this looks like an integral we can do.

00:42:56.475 --> 00:42:59.112
&gt;&gt; [INAUDIBLE]
&gt;&gt; The other what?

00:42:59.112 --> 00:43:08.057
&gt;&gt; [INAUDIBLE]
&gt;&gt; Say that again.

00:43:08.057 --> 00:43:12.107
&gt;&gt; [INAUDIBLE]
&gt;&gt; There's another e to the -y squared

00:43:12.107 --> 00:43:13.020
over 2.

00:43:13.020 --> 00:43:14.400
Yeah, I forgot that one, thank you.

00:43:15.630 --> 00:43:18.640
There's another,
we'll just combine that one with this one.

00:43:18.640 --> 00:43:20.102
So that would be 1.

00:43:20.102 --> 00:43:23.950
Uh-oh, I guess don't get hired,
that's sad.

00:43:23.950 --> 00:43:26.820
There's another,
e to the -y squared over 2 that I forgot.

00:43:26.820 --> 00:43:31.110
But now, I thank you, I put it back, okay?

00:43:31.110 --> 00:43:35.170
I haven't interviewed for
any jobs since I came here five years ago,

00:43:35.170 --> 00:43:36.970
so I'm kinda rusty.

00:43:36.970 --> 00:43:42.271
So I put back either the -y squared
over 2 that you helped me with,

00:43:42.271 --> 00:43:45.220
and now that should be okay, right?

00:43:45.220 --> 00:43:47.400
Now, this is an integral we can do,

00:43:47.400 --> 00:43:51.546
because we know that the derivative
of y squared is gonna be 2y, and

00:43:51.546 --> 00:43:56.860
that's gonna be taken care of there,
now it's an easy u substitution again.

00:43:56.860 --> 00:44:02.096
So we can just let u = let's say 1

00:44:02.096 --> 00:44:06.959
+ t squared y squared over 2.

00:44:06.959 --> 00:44:12.943
Just make that substitution, so
then this just becomes e to the -u, okay?

00:44:12.943 --> 00:44:19.032
So du = 1 + t squared times,

00:44:19.032 --> 00:44:27.566
now we're treating t as constant again.

00:44:27.566 --> 00:44:33.189
We're changing the variable y,
transforming it to u.

00:44:33.189 --> 00:44:39.737
So the derivative y squared over 2 is y,
so we have y times 1 + t squared dy.

00:44:39.737 --> 00:44:46.120
So we have the Ydy, we're just
missing the 1 + t squared, okay?

00:44:46.120 --> 00:44:48.930
So I'll just multiply and
divide by 1 + t squared.

00:44:48.930 --> 00:44:53.390
Then we're just integrating e to the -u
du, which is a very, very easy integral.

00:44:53.390 --> 00:44:55.470
We know that that's 1,
either just by doing it or

00:44:55.470 --> 00:44:59.580
because it's the integral of
the exponential PDF again.

00:44:59.580 --> 00:45:06.984
Okay, so then we immediately now have the
answer, 1 over pi 1 + t squared for all t.

00:45:12.719 --> 00:45:14.170
So that's the PDF.

00:45:16.240 --> 00:45:21.490
If we wanted the CDF,
all we would have to do is integrate this,

00:45:21.490 --> 00:45:23.440
then it's gonna do some arc tangent thing.

00:45:24.530 --> 00:45:27.520
All right, so that's the Cauchy.

00:45:27.520 --> 00:45:33.224
And let me just quickly just show you
how you would start the other method,

00:45:33.224 --> 00:45:36.450
which would be the probability.

00:45:36.450 --> 00:45:39.645
I'm not gonna do the whole thing,
because at some point,

00:45:39.645 --> 00:45:42.630
that's just gonna reduce
back to this method.

00:45:42.630 --> 00:45:44.400
But just to show you
what it would look like.

00:45:50.322 --> 00:45:52.887
Just as a quick alternative without
going through the whole thing,

00:45:52.887 --> 00:45:53.970
cuz it's gonna be similar.

00:45:53.970 --> 00:45:55.246
But it's useful to have both methods.

00:45:55.246 --> 00:46:01.360
So this would be the method using the
double integral, okay, and that's the PDF.

00:46:01.360 --> 00:46:03.647
Which by the way,
we should check thati that's a valid PDF.

00:46:03.647 --> 00:46:05.410
Does it integrate to 1?

00:46:05.410 --> 00:46:09.123
Well, if you integrate that thing,
you'll get an arctangent thing.

00:46:09.123 --> 00:46:12.230
And you can check that when you
evaluate the arctangent thing,

00:46:12.230 --> 00:46:13.410
you will get 1, okay?

00:46:14.690 --> 00:46:19.460
So just quickly, the alternative
using the law of total probability.

00:46:19.460 --> 00:46:22.867
x less than or
equal to t absolute value of y.

00:46:24.930 --> 00:46:31.900
You kind of just think to yourself,
what do we wish that we knew here?

00:46:31.900 --> 00:46:36.217
We could decide to condition on x, or
we could decide to condition on y.

00:46:40.089 --> 00:46:45.630
This is gonna be the integral,
let's say we condition on y.

00:46:45.630 --> 00:46:48.973
The probability x less than or equal to t,

00:46:48.973 --> 00:46:52.994
absolute value of Y,
given, let's say, Y = y.

00:46:52.994 --> 00:46:57.060
This would be the law of total
probability, right, just conditioning.

00:46:57.060 --> 00:46:59.642
We can choose whether to condition on x or
to condition on y, but

00:46:59.642 --> 00:47:00.925
I think I wanna condition on y.

00:47:03.981 --> 00:47:08.717
Okay, law of total probability,
remember the discrete case we just seen,

00:47:08.717 --> 00:47:13.180
sum over all cases, p of a given,
b, whatever, p of v, whatever.

00:47:13.180 --> 00:47:14.880
We have a partition.

00:47:14.880 --> 00:47:17.570
And in this case,
we're integrating instead of summing, so

00:47:17.570 --> 00:47:21.532
we're conditioning on y and
then we're multiplying by 5y.

00:47:21.532 --> 00:47:25.205
Lower case 5y is the standard normal PDF.

00:47:28.923 --> 00:47:32.500
All right, well,
let's see if this helps at all.

00:47:36.020 --> 00:47:42.000
This is saying to treat Y as just
being known to equal little y, okay?

00:47:42.000 --> 00:47:47.050
So I can plug in little y there.

00:47:47.050 --> 00:47:49.620
And then the tricky part here is that

00:47:49.620 --> 00:47:52.040
we need to use the fact
that x is independent of y.

00:47:52.040 --> 00:47:56.030
Because if x are not independent of y,
you could plug this thing in, but

00:47:56.030 --> 00:47:59.370
then you still have this condition, okay?

00:47:59.370 --> 00:48:02.251
But since they're independent,
you can plug in Y = y and

00:48:02.251 --> 00:48:06.400
then get rid of the condition,
because they're independent.

00:48:06.400 --> 00:48:10.168
So when we do that,
that's just gonna be phi.

00:48:10.168 --> 00:48:15.190
The probability that x is less than or
equal to t absolute value of y,

00:48:15.190 --> 00:48:20.340
is just phi of tf to the value of y,
just by definition, right?

00:48:20.340 --> 00:48:21.874
Because we're plugging in y,

00:48:21.874 --> 00:48:25.186
that's just a constant probability
of x less than some constant.

00:48:25.186 --> 00:48:29.150
It's just the standard
normal CDF evaluated there.

00:48:29.150 --> 00:48:35.172
Phi of (y)dy, which I think is
the same as the integral we had.

00:48:35.172 --> 00:48:37.710
Does that look the same?

00:48:37.710 --> 00:48:41.040
Yeah, so over there, I just wrote out
what this is, but it's the same thing.

00:48:41.040 --> 00:48:43.170
And then proceed in the same way.

00:48:43.170 --> 00:48:45.940
So that would be a second way to do this.

00:48:45.940 --> 00:48:48.880
We'll see a third way later on,

00:48:48.880 --> 00:48:51.140
just because this is
a common interview question.

00:48:51.140 --> 00:48:55.408
So it's good to have more than three or
more than two ways to do it.

00:48:55.408 --> 00:48:57.172
All right, so I'll stop for now.

00:48:57.172 --> 00:48:59.210
I'll see you Wednesday.

