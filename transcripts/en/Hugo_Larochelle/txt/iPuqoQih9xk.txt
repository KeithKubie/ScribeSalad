Speaker 1:          00:00          In this video, we'll discuss some extensions of the standard restricted Boltzmann machine.

Speaker 2:          00:07          Okay.

Speaker 1:          00:08          So, uh, we've seen the basic restricted Boltzmann machine and a dark do things that makes the restricted Boltzmann machine, uh, basic. The first is that we have, um, we have only considered binary visible units. And so we'll talk a little bit more about, uh, whether we can extend it to other types of observation. And the other part of it is the fact that it's restricted. That is we are not allowing for connections between either, uh, the elements in the visible layers are the elements in the hidden layers. So we also talk about, uh, uh, what happens when the Boltzmann machine is not restricted.

Speaker 1:          00:51          So first the generalization to other observations. Um, one particular observation type that we'd like to be able to support is a unbounded wheels. So values between minus infinity to plus update it. And it turns out that there's an extension known as the Galician Bernoulli that allows to a model will valid observations. Uh, and the extension is actually a fairly straight forward to obtain. Uh, all it requires is that we added quadratic term into the energy function, uh, which, uh, and the quadratic term needs to be positive. So, uh, so here we have the quarterly Terman question. Uh, and uh, if we do this, then the only thing that this is changing actually the definition or the, uh, um, a corresponding conditional distribution p of x given h, uh, that's because we've added the term that only depends on it on x. So it's a constant, if we were to compute p of a h given x and uh, actually what this is going to yield is a Gaussian distribution, uh, which is such that it's conditional mean is going to be just a linear transformation of the hidden layer.

Speaker 1:          02:06          And the covariance matrix is actually going to be an identity covariance matrix. We have a variance of one over each arm in the vex, a conditional veterans of one given the admin layer and a no correlation between the elements in the Matrix. Um, you can, uh, there's a variant which is a bit more involved, which, uh, where we can actually play with the a and model the conditional covariance matrix. Uh, I encourage you to look at the literature for more details on this, but a very simple thing to do to, uh, in the sense, uh, avoid having to do this is to just normalize the training set, uh, by, uh, first subtracting the mean. So it's just a will allow us to, uh, uh, make it easier to not have to learn the mean of the, uh, the, the average value of the inputs. Uh, and then to sort of standardize it in terms of it's a covariance a structure.

Speaker 1:          03:04          Then we could just divide each of the input by its training set, standard deviation. So if we do this, then, uh, it's uh, uh, it tends to be easier to learn this and Bernoulli rpm and intense. Also to be a bit less important to learn the conditional a covariance or the conditional variants of each element in the input vector. Still, it's a model that that is still a bit harder to train than a regular RBM in particular in practice, unless we use a, uh, usually smaller learning rate than what we use for the standard RBM. Uh, we tend to find sometimes that training can diverged so you have to be a bit more careful in the selection of the learning rate. But, uh, still, uh, it's still the fact that, you know, this is a valid model for a real valued, uh, observations and, uh, it's been used by some with success to learn good features from real value factors.

Speaker 3:          04:09          Uh,

Speaker 1:          04:10          it will learn different features. So if we train it on the amness Dataset, this is an example of what you can get a we do get are sort of pen stroke detectors, like hidden units. Uh, other features here are harder to interpret. Uh, you can get more meaningful features by introducing some sparsity in the hidden layer. Uh, you can consult the, uh, course website for literature on that. Uh, but I'm just showing this really do illustrate the fact that, uh, you will get different features. Uh, if you use, say, a gash in our Bernoulli RBN than if you use a regular RBM.

Speaker 3:          04:51          Okay.

Speaker 1:          04:51          And then there exist extensions for other types of observations. For instance, binomial observations. So an observation between a zero and an infant, you're a c n a. So he can look at this paper for more about that. This can be useful in this case if you want to actually model explicitly the integer value intensity of pixels, uh, instead of treating it as either a binary value or real value value, uh, for modeling multinomial observation. So for instance, words in text, uh, I give you two references here. Darson and challenges related to modeling words, uh, which are related to, uh, the fact that words are intrinsically very high dimensional objects that can come from very large vocabulary. And so these two papers are example, uh, examples of papers that tried to deal with this difficulty. And A, I encourage you to look at the course website to, uh, get more, uh, uh, more papers on other types of Barrons for modeling, different types of observations, uh, and in particular for modeling veal valued data. There are other more sophisticated extensions of the RBM, which, uh, for instance, for, uh, uh, modeling pixels, uh, can, uh, actually perform better than the Gash in our Bernoulli RBM in its basic form as I've described it.

Speaker 1:          06:21          And finally, what about if we make a restrictive Bolsa machine non restricted? Uh, in fact, uh, originally the, uh, Boltzmann machine was proposed and it's only later that the restricted Ferran was proposed. Uh, how I think the fact that if we didn't have connections between the hidden layer and the visible layer, then a certain things became a much more efficiently, uh, certain computations were much more efficient to, for instance, we get some interesting conditional independence between layers. But in the original Boltzmann machine you would have connections between, uh, the input elements. So that would allow us to model direct, uh, dependencies between elements of the input vector. And we do this by adding this term in the energy function. So we have a matrix which a wood model, our preference, uh, between the values of, uh, pairs of input elements in the input vector. And then, uh, we could have again, such connections by adding a term like this in the energy function, which will allow us to model directly the dependencies between a hidden units and have certain times of interaction patterns, inhibition and excitation patterns between the, uh, hidden units.

Speaker 1:          07:41          And, uh, sometimes we can actually use just one and not the other, uh, in which case we'll talk about a semi restricted Boltzmann machine. So this would be a Boltzmann machine, for instance. We could decide to remove those only have interactions between the hidden units. In this case, uh, the, the visible layer would be conditionally, uh, each element in the visible layer. We conditioned independent given the hidden unit, but he would not be true the other way around. Uh, and, uh, uh, there's been some papers that have shown that for modeling natural image patches, uh, removing this, but only maintaining these connections can be beneficial. So again, there's a lot of literature on, uh, tried to, uh, actually do try to incorporate these types of interactions between either hidden units are visible units, coach you to look at the literature in the, uh, you know, by googling different papers on Boltzmann machine. The main thing to remember is that if we do introduce these interactions, perhaps who have a more powerful model, but it's much harder to train because if we want to do contrast the divergence, for instance, sampling from this model, performing Gib sampling now requires that we update each element in our undirected graphical model individually given the others. Because now we don't have the nice conditional independence property that we have in the restricted Boltzmann machine. So for that reason, this model, the footballs, the machine is not as popular as the restricted Boltzmann machine.