WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:00.810
 

00:00:00.810 --> 00:00:01.620
Hi.

00:00:01.620 --> 00:00:05.190
In this problem, we're going to
be dealing with a variation

00:00:05.190 --> 00:00:07.710
of the usual coin-flipping
problem.

00:00:07.710 --> 00:00:11.830
But in this case, the bias
itself of the coin

00:00:11.830 --> 00:00:13.410
is going to be random.

00:00:13.410 --> 00:00:16.460
So you could think of it as, you
don't even know what the

00:00:16.460 --> 00:00:18.930
probability of heads
for the coin is.

00:00:18.930 --> 00:00:22.150
So as usual, we're still taking
one coin and we're

00:00:22.150 --> 00:00:23.260
flipping it n times.

00:00:23.260 --> 00:00:29.340
But the difference here is that
the bias is because it

00:00:29.340 --> 00:00:32.759
was random variable Q. And
we're told that the

00:00:32.759 --> 00:00:37.390
expectation of this bias is some
mu and that the variance

00:00:37.390 --> 00:00:40.000
of the bias is some sigma
squared, which

00:00:40.000 --> 00:00:42.810
we're told is positive.

00:00:42.810 --> 00:00:45.930
And what we're going to be
asked is find a bunch of

00:00:45.930 --> 00:00:50.620
different expectations,
covariances, and variances.

00:00:50.620 --> 00:00:52.970
And we'll see that this problem
gives us some good

00:00:52.970 --> 00:00:57.050
exercise in a few concepts, a
lot of iterated expectations,

00:00:57.050 --> 00:01:01.265
which, again, tells you that
when you take the expectation

00:01:01.265 --> 00:01:05.730
of a conditional expectation,
it's just the expectation of

00:01:05.730 --> 00:01:09.350
the inner random variable.

00:01:09.350 --> 00:01:11.790
The covariance of two random
variables is just the

00:01:11.790 --> 00:01:13.890
expectation of the product
minus the product of the

00:01:13.890 --> 00:01:15.320
expectations.

00:01:15.320 --> 00:01:19.730
Law of total variance is the
expectation of a variance, of

00:01:19.730 --> 00:01:22.050
a conditional variance plus the
variance of a conditional

00:01:22.050 --> 00:01:23.800
expectation.

00:01:23.800 --> 00:01:26.090
And the last thing, of course,
we're dealing with a bunch of

00:01:26.090 --> 00:01:28.210
Bernoulli random variables,
coin flips.

00:01:28.210 --> 00:01:31.000
So as a reminder, for a
Bernoulli random variable, if

00:01:31.000 --> 00:01:35.770
you know what the bias is, it's
some known quantity p,

00:01:35.770 --> 00:01:38.980
then the expectation of the
Bernoulii is just p, and the

00:01:38.980 --> 00:01:43.260
variance of the Bernoulli
is p times 1 minus p.

00:01:43.260 --> 00:01:44.740
So let's get started.

00:01:44.740 --> 00:01:47.080
The problem tells us that we're
going to define some

00:01:47.080 --> 00:01:48.100
random variables.

00:01:48.100 --> 00:01:52.650
So xi is going to be a Bernoulli
random variable for

00:01:52.650 --> 00:01:53.900
the i coin flip.

00:01:53.900 --> 00:01:56.830
 

00:01:56.830 --> 00:02:02.160
So xi is going to be 1 if the i
coin flip was heads and 0 if

00:02:02.160 --> 00:02:03.390
it was tails.

00:02:03.390 --> 00:02:06.360
And one very important thing
that the problem states is

00:02:06.360 --> 00:02:10.919
that conditional on Q, the
random bias, so if we know

00:02:10.919 --> 00:02:17.120
what the random bias is, then
all the coin flips are

00:02:17.120 --> 00:02:18.155
independent.

00:02:18.155 --> 00:02:20.630
And that's going to be important
for us when we

00:02:20.630 --> 00:02:23.580
calculate all these values.

00:02:23.580 --> 00:02:28.250
OK, so the first thing that we
need to calculate is the

00:02:28.250 --> 00:02:31.465
expectation of each of these
individual Bernoulli random

00:02:31.465 --> 00:02:33.271
variables, xi.

00:02:33.271 --> 00:02:35.960
So how do we go about
calculating what this is?

00:02:35.960 --> 00:02:38.210
Well, the problem
gives us a int.

00:02:38.210 --> 00:02:41.240
It tells us to try using the law
of iterated expectations.

00:02:41.240 --> 00:02:44.040
But in order to use it, you need
to figure out what you

00:02:44.040 --> 00:02:45.280
need the condition on.

00:02:45.280 --> 00:02:46.940
What this y?

00:02:46.940 --> 00:02:48.640
What takes place in y?

00:02:48.640 --> 00:02:53.910
And in this case, a good
candidate for what you

00:02:53.910 --> 00:02:58.070
condition on would be
the bias, the Q that

00:02:58.070 --> 00:02:59.310
we're unsure about.

00:02:59.310 --> 00:03:03.960
So let's try doing that
and see what we get.

00:03:03.960 --> 00:03:09.790
So we write out the law of
iterated expectations with Q.

00:03:09.790 --> 00:03:14.110
So now hopefully, we can
simplify it with this

00:03:14.110 --> 00:03:16.100
inter-conditional
expectation is.

00:03:16.100 --> 00:03:17.250
Well, what is it really?

00:03:17.250 --> 00:03:22.620
It's saying, given what Q is,
what is the expectation of

00:03:22.620 --> 00:03:25.330
this Bernoulli random
interval xi?

00:03:25.330 --> 00:03:31.500
Well, we know that if we knew
what the bias was, then the

00:03:31.500 --> 00:03:33.690
expectation is just
the bias itself.

00:03:33.690 --> 00:03:36.120
But in this case, the
bias is random.

00:03:36.120 --> 00:03:38.210
But remember a conditional
expectation is

00:03:38.210 --> 00:03:39.510
still a random variable.

00:03:39.510 --> 00:03:44.810
And so in this case, this
actually just simplifies into

00:03:44.810 --> 00:03:51.390
Q. So whatever the bias is, the
expectation is just equal

00:03:51.390 --> 00:03:53.830
to the bias.

00:03:53.830 --> 00:03:56.590
And so that's what
it tells us.

00:03:56.590 --> 00:04:01.590
And this part is easy because
we're given that the

00:04:01.590 --> 00:04:05.290
expectation of q is mu.

00:04:05.290 --> 00:04:09.682
And then the problem also
defines the random variable x.

00:04:09.682 --> 00:04:13.120
X is the total number of heads
within the n tosses.

00:04:13.120 --> 00:04:21.709
Or you can think of it as a sum
of all these individual xi

00:04:21.709 --> 00:04:24.290
Bernoulli random variables.

00:04:24.290 --> 00:04:26.830
And now, what can
we do with this?

00:04:26.830 --> 00:04:30.770
Well we can remember that
linearity of expectations

00:04:30.770 --> 00:04:33.820
allows us to split
up this sum.

00:04:33.820 --> 00:04:36.200
Expectation of a sum, we could
split up into a sum of

00:04:36.200 --> 00:04:38.020
expectations.

00:04:38.020 --> 00:04:41.770
So this is actually just
expectation of x1 plus dot dot

00:04:41.770 --> 00:04:46.540
dot plus all the way to
expectation of xn.

00:04:46.540 --> 00:04:48.810
All right.

00:04:48.810 --> 00:04:52.680
And now, remember that we're
flipping the same coin.

00:04:52.680 --> 00:04:55.040
We don't know what the bias is,
but for all the n flips,

00:04:55.040 --> 00:04:56.730
it's the same coin.

00:04:56.730 --> 00:05:00.570
And so each of these
expectations of xi should be

00:05:00.570 --> 00:05:03.440
the same, no matter
what xi is.

00:05:03.440 --> 00:05:06.240
And each one of them is mu.

00:05:06.240 --> 00:05:08.220
We already calculated
that earlier.

00:05:08.220 --> 00:05:11.320
And there's 10 of them, so the
answer would be n times mu.

00:05:11.320 --> 00:05:15.080
 

00:05:15.080 --> 00:05:24.730
So let's move on to part B.
Part B now asks us to find

00:05:24.730 --> 00:05:31.840
what the covariance is
between xi and xj.

00:05:31.840 --> 00:05:36.570
And we have to be a little bit
careful here because there are

00:05:36.570 --> 00:05:39.330
two different scenarios, one
where i and j are different

00:05:39.330 --> 00:05:42.640
indices, different tosses,
and another where i

00:05:42.640 --> 00:05:44.720
and j are the same.

00:05:44.720 --> 00:05:47.250
So we have to consider both
of these cases separately.

00:05:47.250 --> 00:05:51.990
Let's first do the case where
x and i are different.

00:05:51.990 --> 00:05:56.140
So i does not equal j.

00:05:56.140 --> 00:06:03.720
In this case, we can just apply
the formula that we

00:06:03.720 --> 00:06:05.790
talked about in the beginning.

00:06:05.790 --> 00:06:12.530
So this covariance is just equal
to the expectation of xi

00:06:12.530 --> 00:06:26.350
times xj minus the expectation
of xi times expectation of xj.

00:06:26.350 --> 00:06:32.920
All right, so we actually know
what these two are, right?

00:06:32.920 --> 00:06:34.400
Expectation of xi is mu.

00:06:34.400 --> 00:06:35.770
Expectation of xj is also mu.

00:06:35.770 --> 00:06:37.460
So this part is just
mu squared.

00:06:37.460 --> 00:06:39.820
But we need to figure out
what this expectation

00:06:39.820 --> 00:06:42.840
of xi times xj is.

00:06:42.840 --> 00:06:49.140
Well, the expectation of xi
times xj, we can again use the

00:06:49.140 --> 00:06:50.830
law of iterated expectations.

00:06:50.830 --> 00:06:55.160
So let's try conditioning
on cue again.

00:06:55.160 --> 00:07:00.070
 

00:07:00.070 --> 00:07:01.800
And remember we said
that this second

00:07:01.800 --> 00:07:04.110
part is just mu squared.

00:07:04.110 --> 00:07:06.910
 

00:07:06.910 --> 00:07:09.980
All right, well, how can
we simplify this

00:07:09.980 --> 00:07:11.870
inner-conditional expectation?

00:07:11.870 --> 00:07:14.860
Well, we can use the fact that
the problem tells us that,

00:07:14.860 --> 00:07:19.020
conditioned on Q, the tosses
are independent.

00:07:19.020 --> 00:07:23.090
So that means that, conditioned
on Q, xi and xj

00:07:23.090 --> 00:07:24.270
are independent.

00:07:24.270 --> 00:07:27.800
And remember, when random
variables are independent, the

00:07:27.800 --> 00:07:31.100
expectation of product, you
could simplify that to be the

00:07:31.100 --> 00:07:33.480
product of the expectations.

00:07:33.480 --> 00:07:36.390
And because we're in the
condition world on Q, you have

00:07:36.390 --> 00:07:38.960
to remember that it's going
to be a product of two

00:07:38.960 --> 00:07:41.910
conditional expectations.

00:07:41.910 --> 00:07:48.550
So this will be expectation of
xi given Q times expectation

00:07:48.550 --> 00:07:56.920
of xj given Q minus
mu squared still.

00:07:56.920 --> 00:08:01.400
All right, now what is this?

00:08:01.400 --> 00:08:05.360
Well the expectation of xi given
Q, we already argued

00:08:05.360 --> 00:08:09.500
earlier here that it should just
be Q. And then the same

00:08:09.500 --> 00:08:10.660
thing for xj.

00:08:10.660 --> 00:08:15.700
That should also be Q. So this
is just expectation of Q

00:08:15.700 --> 00:08:18.430
squared minus mu squared.

00:08:18.430 --> 00:08:21.660
 

00:08:21.660 --> 00:08:26.740
All right, now if we look at
this, what is the expectation

00:08:26.740 --> 00:08:30.230
of Q squared minus mu squared?

00:08:30.230 --> 00:08:33.830
Well, remember mu is just,
we're told that mu is the

00:08:33.830 --> 00:08:37.990
expectation of Q. So what we
have is the expectation of Q

00:08:37.990 --> 00:08:43.299
squared minus the quantity
expectation of Q squared.

00:08:43.299 --> 00:08:45.040
And what is that, exactly?

00:08:45.040 --> 00:08:47.700
That is just the formula or
the definition of what the

00:08:47.700 --> 00:08:49.050
variance of Q should be.

00:08:49.050 --> 00:08:52.910
So this is, in fact, exactly
equal to the variance of Q,

00:08:52.910 --> 00:08:56.540
which we're told is
sigma squared.

00:08:56.540 --> 00:08:59.500
All right, so what we found is
that for i not equal to j, the

00:08:59.500 --> 00:09:02.065
coherence of xi and
xj is exactly

00:09:02.065 --> 00:09:04.360
equal to sigma squared.

00:09:04.360 --> 00:09:07.590
And remember, we're told that
sigma squared is positive.

00:09:07.590 --> 00:09:08.450
So what does that tell us?

00:09:08.450 --> 00:09:13.640
That tells us that xi and xj, or
i not equal to j, these two

00:09:13.640 --> 00:09:15.740
random variables
are correlated.

00:09:15.740 --> 00:09:17.800
And so, because they're
correlated, they can't be

00:09:17.800 --> 00:09:18.820
independent.

00:09:18.820 --> 00:09:21.720
Remember, if two intervals are
independent, that means

00:09:21.720 --> 00:09:24.300
they're uncorrelated.

00:09:24.300 --> 00:09:25.550
But the converse isn't true.

00:09:25.550 --> 00:09:28.150
 

00:09:28.150 --> 00:09:31.130
But if we do know that two
random variables are

00:09:31.130 --> 00:09:33.050
correlated, that means that
they can't be independent.

00:09:33.050 --> 00:09:35.920
 

00:09:35.920 --> 00:09:40.150
And now let's finish this by
considering the second case.

00:09:40.150 --> 00:09:45.290
The second case is when i
actually does equal j.

00:09:45.290 --> 00:09:50.360
And in that case, well, the
covariance of xi and xi is

00:09:50.360 --> 00:09:54.040
just another way of writing
the variance of xi.

00:09:54.040 --> 00:10:01.690
So covariance, xi, xi, it's
just the variance of xi.

00:10:01.690 --> 00:10:03.320
And what is that?

00:10:03.320 --> 00:10:08.590
That is just the expectation
of xi squared minus

00:10:08.590 --> 00:10:16.420
expectation of xi quantity
squared.

00:10:16.420 --> 00:10:18.290
And again, we know what
the second term is.

00:10:18.290 --> 00:10:21.260
The second term is expectation
of xi quantity squared.

00:10:21.260 --> 00:10:26.540
Expectation of xi we know from
part A is just mu, right?

00:10:26.540 --> 00:10:28.670
So that's just second term
is just mu squared.

00:10:28.670 --> 00:10:32.250
But what is the expectation
of xi squared?

00:10:32.250 --> 00:10:35.220
Well, we can think about
this a little bit more.

00:10:35.220 --> 00:10:40.020
And you can realize that xi
squared is actually exactly

00:10:40.020 --> 00:10:41.920
the same thing as just xi.

00:10:41.920 --> 00:10:45.150
And this is just a special case
because xi is a Bernoulli

00:10:45.150 --> 00:10:46.230
random variable.

00:10:46.230 --> 00:10:49.210
Because Bernoulli is
either 0 or 1.

00:10:49.210 --> 00:10:52.010
And if it's 0 and you square
it, it's still 0.

00:10:52.010 --> 00:10:54.380
And if it's 1 and you square
it, it's still 1.

00:10:54.380 --> 00:10:58.980
So squaring it doesn't
really doesn't

00:10:58.980 --> 00:11:00.140
actually change anything.

00:11:00.140 --> 00:11:03.390
It's exactly the same thing as
the original random variable.

00:11:03.390 --> 00:11:07.130
And so, because this is a
Bernoulli random variable,

00:11:07.130 --> 00:11:11.340
this is exactly just the
expectation of xi.

00:11:11.340 --> 00:11:13.880
And we said this part
is just mu squared.

00:11:13.880 --> 00:11:17.950
So this is just expectation of
xi, which we said was mu.

00:11:17.950 --> 00:11:21.730
So the answer is just
mu minus mu squared.

00:11:21.730 --> 00:11:24.460
 

00:11:24.460 --> 00:11:31.880
OK, so this completes part B.
And the answer that we wanted

00:11:31.880 --> 00:11:38.190
was that in fact, xi and xj are
in fact not independent.

00:11:38.190 --> 00:11:39.130
Right.

00:11:39.130 --> 00:11:45.960
So let's write down some facts
that we'll want to remember.

00:11:45.960 --> 00:11:51.610
One of them is that expectation
of xi is mu.

00:11:51.610 --> 00:11:56.660
And we also want to remember
what this covariance is.

00:11:56.660 --> 00:12:04.290
The covariance of xi and xj is
equal to sigma squared when i

00:12:04.290 --> 00:12:06.470
does not equal j.

00:12:06.470 --> 00:12:10.570
So we'll be using these
facts again later.

00:12:10.570 --> 00:12:18.780
And the variance of xi is equal
to mu minus mu squared.

00:12:18.780 --> 00:12:22.120
 

00:12:22.120 --> 00:12:27.830
So now let's move on to the last
part, part C, which asks

00:12:27.830 --> 00:12:34.550
us to calculate the variance
of x in two different ways.

00:12:34.550 --> 00:12:39.110
So the first way we'll
do it is using the

00:12:39.110 --> 00:12:41.830
law of total variance.

00:12:41.830 --> 00:12:47.470
So the law of total variance
will tell us that we can write

00:12:47.470 --> 00:12:51.940
the variance of x as a sum
of two different parts.

00:12:51.940 --> 00:12:56.240
So the first is variance of x
expectation of the variance of

00:12:56.240 --> 00:13:03.740
x conditioned on something
plus the variance of the

00:13:03.740 --> 00:13:07.320
initial expectation of x
conditioned on something.

00:13:07.320 --> 00:13:10.030
And as you might have guessed,
what we're going to condition

00:13:10.030 --> 00:13:16.330
on is Q.

00:13:16.330 --> 00:13:18.670
Let's calculate what these
two things are.

00:13:18.670 --> 00:13:21.170
So let's do the two
terms separately.

00:13:21.170 --> 00:13:23.470
What is the expectation
of the conditional

00:13:23.470 --> 00:13:26.490
variance of x given Q?

00:13:26.490 --> 00:13:29.750
 

00:13:29.750 --> 00:13:33.550
Well, what is--

00:13:33.550 --> 00:13:36.140
this, we can write out x.

00:13:36.140 --> 00:13:41.880
Because x, remember, is just
the sum of a bunch of these

00:13:41.880 --> 00:13:43.270
Bernoulli random variables.

00:13:43.270 --> 00:13:46.290
 

00:13:46.290 --> 00:13:50.380
And now what we'll do was, well,
again, use the important

00:13:50.380 --> 00:13:54.900
fact that the x's, we're told,
are conditionally independent,

00:13:54.900 --> 00:13:56.710
conditional on Q.

00:13:56.710 --> 00:14:00.450
And because they're independent,
remember the

00:14:00.450 --> 00:14:03.560
variance of a sum is not the
sum of the variance.

00:14:03.560 --> 00:14:06.730
It's only the sum of the
variance if the terms in the

00:14:06.730 --> 00:14:08.480
sum are independent.

00:14:08.480 --> 00:14:10.880
In this case, they are
conditionally independent

00:14:10.880 --> 00:14:15.730
given Q. So we can in fact split
this up and write it as

00:14:15.730 --> 00:14:20.340
the variance of x1 given Q
plus all the way to the

00:14:20.340 --> 00:14:30.980
variance of xn given Q.

00:14:30.980 --> 00:14:33.960
And in fact, all these
are the same, right?

00:14:33.960 --> 00:14:39.530
So we just have n copies of the
variance of, say, x1 given

00:14:39.530 --> 00:14:43.310
Q. Now, what is the variance
of x1 given Q?

00:14:43.310 --> 00:14:46.770
Well, x1 is just a Bernoulli
random variable.

00:14:46.770 --> 00:14:51.620
But the difference is that for
x, we don't know what the bias

00:14:51.620 --> 00:14:54.060
or what the Q is.

00:14:54.060 --> 00:14:57.910
Because it's some
random bias Q

00:14:57.910 --> 00:15:01.010
But just like we said earlier
in part A, when we talked

00:15:01.010 --> 00:15:07.640
about the expectation of x1
given Q, this is actually just

00:15:07.640 --> 00:15:13.250
Q times 1 minus Q. Because if
you knew what the bias were,

00:15:13.250 --> 00:15:14.810
it would be p times 1 minus p.

00:15:14.810 --> 00:15:16.860
So the bias times 1
minus the bias.

00:15:16.860 --> 00:15:19.190
But you don't know what it is.

00:15:19.190 --> 00:15:21.060
But if you did, it
would just be q.

00:15:21.060 --> 00:15:23.870
So what we do is we just plug
in Q, and you get Q

00:15:23.870 --> 00:15:26.770
times 1 minus 2.

00:15:26.770 --> 00:15:36.110
All right, and now this
is expectation of n.

00:15:36.110 --> 00:15:38.960
I can pull out the n.

00:15:38.960 --> 00:15:43.470
So it's n times the expectation
of Q minus Q

00:15:43.470 --> 00:15:51.090
squared, which is just n times
expectation Q, we can use

00:15:51.090 --> 00:15:55.450
linearity of expectations again,
expectation of Q is mu.

00:15:55.450 --> 00:16:00.540
And the expectation of Q 2
squared is, well, we can do

00:16:00.540 --> 00:16:01.230
that on the side.

00:16:01.230 --> 00:16:08.840
Expectation of Q squared is
the variance of Q plus

00:16:08.840 --> 00:16:14.230
expectation of Q quantity
squared.

00:16:14.230 --> 00:16:22.120
So that's just sigma squared
plus mu squared.

00:16:22.120 --> 00:16:27.810
And so this is just going to
be then minus sigma squared

00:16:27.810 --> 00:16:29.060
minus mu squared.

00:16:29.060 --> 00:16:32.080
 

00:16:32.080 --> 00:16:33.820
All right, so that's
the first term.

00:16:33.820 --> 00:16:35.950
Now let's do the second term.

00:16:35.950 --> 00:16:43.720
The variance the conditional
expectation of x given Q. And

00:16:43.720 --> 00:16:52.740
again, what we can do is we can
write x as the sum of all

00:16:52.740 --> 00:16:55.435
these xi's.

00:16:55.435 --> 00:16:59.270
 

00:16:59.270 --> 00:17:04.730
And now we can apply linearity
of expectations.

00:17:04.730 --> 00:17:08.705
So we would get n times one
of these expectations.

00:17:08.705 --> 00:17:13.440
 

00:17:13.440 --> 00:17:18.530
And remember, we said earlier
the expectation of x1 given Q

00:17:18.530 --> 00:17:23.720
is just Q. So it's the variance
of n times Q.

00:17:23.720 --> 00:17:26.375
And remember now, n is just--

00:17:26.375 --> 00:17:27.460
it's not random.

00:17:27.460 --> 00:17:29.680
It's just some number.

00:17:29.680 --> 00:17:32.070
So when you pull it out of a
variance, you square it.

00:17:32.070 --> 00:17:36.290
So this is n squared times
the variance of Q.

00:17:36.290 --> 00:17:39.130
And the variance of Q we're
given is sigma squared.

00:17:39.130 --> 00:17:42.660
So this is n squared times
sigma squared.

00:17:42.660 --> 00:17:45.280
 

00:17:45.280 --> 00:17:47.860
So the final answer is
just a combination

00:17:47.860 --> 00:17:49.250
of these two terms.

00:17:49.250 --> 00:17:54.290
This one and this one.

00:17:54.290 --> 00:17:56.010
So let's write it out.

00:17:56.010 --> 00:17:59.295
The variance of x, then,
is equal to--

00:17:59.295 --> 00:18:02.790
 

00:18:02.790 --> 00:18:04.580
we can combine terms
a little bit.

00:18:04.580 --> 00:18:08.010
So the first one, let's
take the mus and

00:18:08.010 --> 00:18:08.730
we'll put them together.

00:18:08.730 --> 00:18:11.325
So it's n mu minus mu squared.

00:18:11.325 --> 00:18:15.830
 

00:18:15.830 --> 00:18:22.660
And then we have n squared times
sigma squared from this

00:18:22.660 --> 00:18:28.520
term and minus n times sigma
squared from this term.

00:18:28.520 --> 00:18:34.450
So it would be n squared minus
n times sigma squared, or n

00:18:34.450 --> 00:18:38.400
times n minus 1 times
sigma squared.

00:18:38.400 --> 00:18:40.970
So that is the final answer
that we get for

00:18:40.970 --> 00:18:42.220
the variance of x.

00:18:42.220 --> 00:18:45.030
 

00:18:45.030 --> 00:18:47.450
And now, let's try doing
it another way.

00:18:47.450 --> 00:18:51.800
 

00:18:51.800 --> 00:18:53.960
So that's one way of doing it.

00:18:53.960 --> 00:18:57.140
That's using the law of total
expectations and conditioning

00:18:57.140 --> 00:19:05.880
on Q. Another way of finding
the variance of x is to use

00:19:05.880 --> 00:19:11.330
the formula involving
covariances, right?

00:19:11.330 --> 00:19:18.652
And we can use that because x is
actually a sum of multiple

00:19:18.652 --> 00:19:23.590
random variables
x1 through xn.

00:19:23.590 --> 00:19:40.780
And the formula for this is, you
have n variance terms plus

00:19:40.780 --> 00:19:44.110
all these other ones.

00:19:44.110 --> 00:19:48.140
Where i is not equal to j, you
have the covariance terms.

00:19:48.140 --> 00:19:51.770
And really, it's just, you can
think of it as a double sum of

00:19:51.770 --> 00:19:59.150
all pairs of xi and xj where if
i and j happen just to be

00:19:59.150 --> 00:20:02.710
the same, that it simplifies
to be just the variance.

00:20:02.710 --> 00:20:06.240
Now, so we pulled theses n terms
out because they are

00:20:06.240 --> 00:20:10.770
different than these because
they have a different value.

00:20:10.770 --> 00:20:14.060
And now fortunately, we've
already calculated what these

00:20:14.060 --> 00:20:16.690
values are in part B. So we
can just plug them them.

00:20:16.690 --> 00:20:18.890
All the variances
are the same.

00:20:18.890 --> 00:20:21.300
And there's n of them,
so we get n times the

00:20:21.300 --> 00:20:22.260
variance of each one.

00:20:22.260 --> 00:20:26.960
The variance of each one we
calculated already was mu

00:20:26.960 --> 00:20:29.790
minus mu squared.

00:20:29.790 --> 00:20:32.630
And then, we have all
the terms were i is

00:20:32.630 --> 00:20:34.210
not equal to j.

00:20:34.210 --> 00:20:39.650
Well, there are actually n
squared minus n of them.

00:20:39.650 --> 00:20:44.040
So because you can take any one
of the n's to be the first

00:20:44.040 --> 00:20:48.110
to be i, any one of
the n to be j.

00:20:48.110 --> 00:20:49.890
So that gives you
n squared pairs.

00:20:49.890 --> 00:20:52.590
But then you have to subtract
out all the ones where i and j

00:20:52.590 --> 00:20:53.190
are the same.

00:20:53.190 --> 00:20:54.320
And there are n of them.

00:20:54.320 --> 00:20:59.250
So that leaves you with n
squared minus n of these pairs

00:20:59.250 --> 00:21:01.600
where i is not equal to j.

00:21:01.600 --> 00:21:04.130
And the coherence for this case
where i is not equal to

00:21:04.130 --> 00:21:08.176
j, we also calculated in part B.
That's just sigma squared.

00:21:08.176 --> 00:21:13.050
All right, and now if we compare
these two, we'll see

00:21:13.050 --> 00:21:15.610
that they are proportionally
exactly the same.

00:21:15.610 --> 00:21:18.510
 

00:21:18.510 --> 00:21:23.700
So we've use two different
methods to calculate the

00:21:23.700 --> 00:21:27.510
variance, one using this
summation and one using the

00:21:27.510 --> 00:21:29.860
law of total variance.

00:21:29.860 --> 00:21:33.040
So what do we learn
from this problem?

00:21:33.040 --> 00:21:37.430
Well, we saw that first of all,
in order to find some

00:21:37.430 --> 00:21:40.940
expectations, it's very useful
to use law of iterated

00:21:40.940 --> 00:21:41.700
expectations.

00:21:41.700 --> 00:21:44.620
But the trick is to figure out
what you should condition on.

00:21:44.620 --> 00:21:47.780
And that's kind of an
art that you learn

00:21:47.780 --> 00:21:49.230
through more practice.

00:21:49.230 --> 00:21:52.920
But one good rule of thumb is,
when you have kind of a

00:21:52.920 --> 00:21:57.650
hierarchy or layers of
randomness where one layer of

00:21:57.650 --> 00:22:00.640
randomness depends
on the randomness

00:22:00.640 --> 00:22:01.960
of the layer above--

00:22:01.960 --> 00:22:05.780
so in this case, whether or
not you get heads or tails

00:22:05.780 --> 00:22:09.600
depends on, that's random, but
that depends on the randomness

00:22:09.600 --> 00:22:12.040
on the level above, which
was the random

00:22:12.040 --> 00:22:14.150
bias of the coin itself.

00:22:14.150 --> 00:22:19.410
So the rule of thumb is, when
you want to calculate the

00:22:19.410 --> 00:22:23.360
expectations for the layer where
you're talking about

00:22:23.360 --> 00:22:27.710
heads or tails, it's useful to
condition on the layer above

00:22:27.710 --> 00:22:30.590
where that is, in this case,
the random bias.

00:22:30.590 --> 00:22:34.430
Because once you condition on
the layer above, that makes

00:22:34.430 --> 00:22:36.210
the next level much simpler.

00:22:36.210 --> 00:22:39.830
Because you kind of assume that
you know what all the

00:22:39.830 --> 00:22:42.650
previous levels of randomness
are, and that helps you

00:22:42.650 --> 00:22:47.480
calculate what the expectation
for this current level.

00:22:47.480 --> 00:22:52.180
And the rest of the problem was
just kind of going through

00:22:52.180 --> 00:22:54.160
exercises of actually
applying the--

00:22:54.160 --> 00:22:55.410
 

