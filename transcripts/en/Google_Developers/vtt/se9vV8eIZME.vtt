WEBVTT
Kind: captions
Language: en

00:00:00.810 --> 00:00:03.290
Google Compute Engine is a great
place for running Hadoop

00:00:03.290 --> 00:00:05.560
to solve your big
data challenges.

00:00:05.560 --> 00:00:07.330
Compute Engine runs on
Google's advance

00:00:07.330 --> 00:00:10.100
infrastructure, developed
through 14 years of experience

00:00:10.100 --> 00:00:12.930
building high performance web
services, supporting billions

00:00:12.930 --> 00:00:14.190
of queries per day.

00:00:14.190 --> 00:00:16.660
So you'll get consistent I/O
performance from Compute

00:00:16.660 --> 00:00:19.110
Engine that scales out
predictably, which is

00:00:19.110 --> 00:00:21.770
extremely helpful when you're
developing solutions with big

00:00:21.770 --> 00:00:23.560
data tools like Hadoop.

00:00:23.560 --> 00:00:25.820
I'm going to walk you through a
demo of setting up Hadoop on

00:00:25.820 --> 00:00:28.600
Compute Engine using a sample
application from our Google

00:00:28.600 --> 00:00:31.320
Cloud Platform repository
on GitHub.

00:00:31.320 --> 00:00:33.290
Hadoop is an open source
implementation of the

00:00:33.290 --> 00:00:35.970
MapReduce paper published
by Google.

00:00:35.970 --> 00:00:37.990
Google has been using MapReduce
for many years to

00:00:37.990 --> 00:00:40.640
distribute data processing
workloads over large clusters

00:00:40.640 --> 00:00:42.910
of machines.

00:00:42.910 --> 00:00:46.200
Today, we're going to show how
to process 255 gigabytes of

00:00:46.200 --> 00:00:49.080
text to compute the number of
times each word appears in our

00:00:49.080 --> 00:00:51.240
data set, and then output
the results in a

00:00:51.240 --> 00:00:53.760
sorted list using Hadoop.

00:00:53.760 --> 00:00:55.800
First, we'll download the Hadoop
getting started sample

00:00:55.800 --> 00:00:56.950
from GitHub.

00:00:56.950 --> 00:00:59.600
This sample sets up a Hadoop
cluster on Compute Engine that

00:00:59.600 --> 00:01:01.780
executes a MapReduce task.

00:01:01.780 --> 00:01:04.550
To download the sample, simply
run that git command from

00:01:04.550 --> 00:01:05.900
within the directory
where you'd like to

00:01:05.900 --> 00:01:07.890
download the sample to.

00:01:07.890 --> 00:01:10.430
Once we've got the sample
downloaded, we'll cd into the

00:01:10.430 --> 00:01:12.200
solutions Google Compute Engine

00:01:12.200 --> 00:01:15.510
cluster for Hadoop directory.

00:01:15.510 --> 00:01:17.880
Next, we'll need to make sure
that we've got the necessary

00:01:17.880 --> 00:01:20.810
prerequisites to work with
Compute Engine and Hadoop.

00:01:20.810 --> 00:01:24.180
The first tool we need to set up
for use is gcutil, which is

00:01:24.180 --> 00:01:25.660
a command line tool
for interacting

00:01:25.660 --> 00:01:27.090
with Compute Engine.

00:01:27.090 --> 00:01:31.350
Follow that URL to download and
install gcutil, then open

00:01:31.350 --> 00:01:33.880
up the readme.mb file that's
included with the Hadoop

00:01:33.880 --> 00:01:36.980
sample and find the
gcutil section.

00:01:36.980 --> 00:01:39.630
Follow those instructions to set
up the default project and

00:01:39.630 --> 00:01:42.750
SSH keys to be used by gcutil.

00:01:42.750 --> 00:01:46.260
The other tool that we'll be
using is gsutil, which allows

00:01:46.260 --> 00:01:48.700
us to run commands on Google
Cloud Storage.

00:01:48.700 --> 00:01:50.460
Follow the URL you see
on the slide to

00:01:50.460 --> 00:01:53.090
download and install gsutil.

00:01:53.090 --> 00:01:56.080
Once we've got gcutil and gsutil
installed, we're ready

00:01:56.080 --> 00:01:58.700
to set up our Hadoop cluster.

00:01:58.700 --> 00:01:59.970
First, we'll need
to download and

00:01:59.970 --> 00:02:01.660
install the Hadoop package.

00:02:01.660 --> 00:02:03.140
Let's do that now.

00:02:03.140 --> 00:02:05.580
You can see that when I run an
ls command, that I'm currently

00:02:05.580 --> 00:02:08.320
in the sample directory, which
is where we'll run the

00:02:08.320 --> 00:02:10.740
following curl command to
download the Hadoop package.

00:02:20.480 --> 00:02:23.150
Then, we'll extract the
downloaded Hadoop package

00:02:23.150 --> 00:02:24.400
using the tar command.

00:02:30.030 --> 00:02:32.150
Then we'll apply a patch that's
included with the

00:02:32.150 --> 00:02:33.400
Hadoop sample.

00:02:37.360 --> 00:02:39.710
And finally, we'll update
the package again

00:02:39.710 --> 00:02:40.960
using the tar command.

00:02:53.740 --> 00:02:58.850
Next, we'll download the Open
JDK and dependent packages.

00:02:58.850 --> 00:03:01.480
To begin this step, let's
create a folder called

00:03:01.480 --> 00:03:10.255
deb_packages under the root
directory of the sample and

00:03:10.255 --> 00:03:11.505
then cd into that directory.

00:03:16.300 --> 00:03:18.150
The sites where you can download
these dependent

00:03:18.150 --> 00:03:20.560
packages from are listed in
the sample's readme file,

00:03:20.560 --> 00:03:23.520
under the section titled
Download Open JDK and

00:03:23.520 --> 00:03:25.090
Dependent Packages.

00:03:25.090 --> 00:03:27.420
Listing the files in
deb_packages, you should see a

00:03:27.420 --> 00:03:30.710
similar list of six files once
you've completed this step.

00:03:35.880 --> 00:03:38.640
This sample uses a Google Cloud
Storage bucket from

00:03:38.640 --> 00:03:41.160
which Compute Engine instances
will download the Hadoop

00:03:41.160 --> 00:03:43.620
package and generated files
for the Hadoop setup.

00:03:46.900 --> 00:03:49.560
Here's a short gsutil command
you can run to create a bucket

00:03:49.560 --> 00:03:50.750
for this purpose.

00:03:50.750 --> 00:03:53.110
Be sure that the bucket is
created in the same project in

00:03:53.110 --> 00:03:55.490
which we'll be running
the Hadoop cluster.

00:03:55.490 --> 00:03:58.500
Notice that I'm specifying the
-l flag with a value of

00:03:58.500 --> 00:04:00.130
US-CENTRAL2.

00:04:00.130 --> 00:04:02.320
Doing so ensures that our cloud
storage bucket will be

00:04:02.320 --> 00:04:04.910
created in the same region as
our Compute Engine instances

00:04:04.910 --> 00:04:06.550
that will be running Hadoop.

00:04:06.550 --> 00:04:09.600
This way, our data transfers
across both services will be

00:04:09.600 --> 00:04:12.520
extremely fast, and we take
advantage of the Google Cloud

00:04:12.520 --> 00:04:16.000
platform's capability of free
data transfers between cloud

00:04:16.000 --> 00:04:17.410
services in the same region.

00:04:22.840 --> 00:04:25.860
Next, we'll set up our client
ID and client secret, which

00:04:25.860 --> 00:04:28.580
will be used to access the
Compute Engine API.

00:04:28.580 --> 00:04:32.220
Go to cloud.google.com/console
and click on the APIs and Auth

00:04:32.220 --> 00:04:35.570
section to create a registered
app for your project.

00:04:35.570 --> 00:04:40.310
Then open the file
gce_cluster.py in an editor

00:04:40.310 --> 00:04:43.300
and replace the values for
CLIENT_ID and CLIENT_SECRET

00:04:43.300 --> 00:04:45.595
with the values that you just
created in the Cloud Console.

00:04:50.940 --> 00:04:52.590
We have a few more dependencies
that we'll need

00:04:52.590 --> 00:04:55.010
to have in place for our Hadoop
cluster, which you can

00:04:55.010 --> 00:04:57.560
find listed in the sample's
readme file under the section

00:04:57.560 --> 00:05:00.390
titled Download and Set
Up Python Libraries.

00:05:00.390 --> 00:05:02.880
Just follow the step by step
instructions there to get the

00:05:02.880 --> 00:05:06.860
Python API client library
ready to use.

00:05:06.860 --> 00:05:08.990
Now that we've got everything
in place, we're ready to

00:05:08.990 --> 00:05:12.240
create a Hadoop cluster using
the sample application.

00:05:12.240 --> 00:05:14.650
The main script of the sample
is a Python file,

00:05:14.650 --> 00:05:17.360
compute_cluster_for_hadoop.py.

00:05:17.360 --> 00:05:19.280
To get familiar with the
commands available, you can

00:05:19.280 --> 00:05:22.620
run this script with
the --help flag.

00:05:22.620 --> 00:05:24.640
This script has four
subcommands--

00:05:24.640 --> 00:05:28.100
setup, start, mapreduce,
and shutdown--

00:05:28.100 --> 00:05:31.550
which we'll test
out right now.

00:05:31.550 --> 00:05:34.720
Starting with the setup command,
we'll run compute

00:05:34.720 --> 00:05:39.160
cluster for Hadoop setup,
followed by our project ID and

00:05:39.160 --> 00:05:40.830
the name of our Google
Cloud Storage bucket.

00:05:43.650 --> 00:05:46.830
The setup command creates an SSH
key used for communication

00:05:46.830 --> 00:05:49.340
between Hadoop instances.

00:05:49.340 --> 00:05:52.060
It also uploads the Hadoop
package and its dependencies

00:05:52.060 --> 00:05:54.970
to Google Cloud storage so that
Compute Engine instances

00:05:54.970 --> 00:05:58.280
can download them to
set up Hadoop.

00:05:58.280 --> 00:06:00.780
Running setup also creates a
firewall in the Compute Engine

00:06:00.780 --> 00:06:04.075
network to allow users access
to the Hadoop web consoles.

00:06:08.700 --> 00:06:11.550
OK, now that the setup process
has completed, we're ready to

00:06:11.550 --> 00:06:15.220
start up our Hadoop cluster.

00:06:15.220 --> 00:06:17.630
We can do that by running the
compute cluster for Hadoop

00:06:17.630 --> 00:06:20.840
script with the start
subcommand, followed by our

00:06:20.840 --> 00:06:24.990
project ID and the name of our
Google Cloud Storage bucket.

00:06:24.990 --> 00:06:27.470
After that, we can specify the
number of instances we'd like

00:06:27.470 --> 00:06:29.390
to start up in our
Hadoop cluster.

00:06:29.390 --> 00:06:31.930
By default, it starts
six instances, one

00:06:31.930 --> 00:06:33.640
master and five workers.

00:06:33.640 --> 00:06:36.190
Let's bump that up to
97 instances and

00:06:36.190 --> 00:06:37.440
start up the cluster.

00:06:48.540 --> 00:06:50.570
And the startup process
is complete.

00:06:50.570 --> 00:06:53.540
That took roughly 8 and 1/2
minutes, pretty fast for

00:06:53.540 --> 00:06:57.150
starting up a 97-node cluster
with Hadoop ready for use.

00:06:57.150 --> 00:07:00.140
The startup process returns a
response which includes links

00:07:00.140 --> 00:07:05.090
to the HDFS Console and
the MapReduce Console.

00:07:05.090 --> 00:07:08.140
Here's the HDFS NameNode
Console, which shows that the

00:07:08.140 --> 00:07:11.980
Hadoop file system is running as
expected, with all 97 nodes

00:07:11.980 --> 00:07:13.570
that we created.

00:07:13.570 --> 00:07:16.250
And here's the MapReduce
Console, where we can monitor

00:07:16.250 --> 00:07:18.610
our MapReduce jobs as
they're running.

00:07:18.610 --> 00:07:21.300
With our Hadoop cluster up and
running, we can now start our

00:07:21.300 --> 00:07:22.550
MapReduce job.

00:07:25.530 --> 00:07:28.590
Using the compute cluster for
Hadoop script, we'll run the

00:07:28.590 --> 00:07:33.160
mapreduce subcommand and pass
it the following values.

00:07:33.160 --> 00:07:36.520
First, we'll specify the project
ID, followed by the

00:07:36.520 --> 00:07:38.920
Cloud Storage bucket which will
be used for temporary

00:07:38.920 --> 00:07:42.660
storage while our MapReduce
is running.

00:07:42.660 --> 00:07:45.660
Next, we'll specify an input,
which is a Cloud Storage

00:07:45.660 --> 00:07:49.580
bucket where our input
file or files reside.

00:07:49.580 --> 00:07:51.930
This bucket currently contains
the results of a Hadoop

00:07:51.930 --> 00:07:54.490
MapReduce that I ran previously
to pull out a

00:07:54.490 --> 00:07:57.560
subset of English titles
for every object in the

00:07:57.560 --> 00:08:01.430
255-gigabyte Freebase source
file that Google has made

00:08:01.430 --> 00:08:03.120
publicly available.

00:08:03.120 --> 00:08:06.220
So this input bucket now holds
a single large text file with

00:08:06.220 --> 00:08:08.860
nearly 200,000 titles,
formatted with one

00:08:08.860 --> 00:08:11.740
title on each line.

00:08:11.740 --> 00:08:14.530
The output value is a Cloud
Storage bucket where the

00:08:14.530 --> 00:08:18.630
results of our MapReduce
will be placed.

00:08:18.630 --> 00:08:21.720
For the mapper, we'll specify
the shortest-to-longest-mapper

00:08:21.720 --> 00:08:24.390
Python program, which is
located in the sample

00:08:24.390 --> 00:08:25.975
directory of our sample
application.

00:08:29.070 --> 00:08:31.180
For the reducer, we'll
specify the

00:08:31.180 --> 00:08:33.505
shortest-to-longest-reducer
Python program.

00:08:36.549 --> 00:08:39.130
The next value we'll pass to
the mapreduce subcommand is

00:08:39.130 --> 00:08:42.240
the number of mappers, which
we're setting here as 95.

00:08:45.210 --> 00:08:47.470
Finally, since we want a single
file containing our

00:08:47.470 --> 00:08:49.700
MapReduce results,
we'll specify a

00:08:49.700 --> 00:08:52.390
reducer count of one.

00:08:52.390 --> 00:08:54.750
Now let's run the command
to start our MapReduce.

00:09:17.680 --> 00:09:21.400
And now our MapReduce job has
completed, and it only took

00:09:21.400 --> 00:09:23.200
two minutes to run.

00:09:23.200 --> 00:09:25.070
We can open up the Cloud
Storage bucket that we

00:09:25.070 --> 00:09:27.590
specified as the output location
in our mapreduce

00:09:27.590 --> 00:09:31.030
command to see the results
of our MapReduce job.

00:09:31.030 --> 00:09:33.020
And there's our output file.

00:09:33.020 --> 00:09:34.665
Let's open the file to
see the results.

00:09:39.230 --> 00:09:40.220
And there we are.

00:09:40.220 --> 00:09:42.620
Here's our word count results,
with the words listed from

00:09:42.620 --> 00:09:44.710
shortest to longest and
in alphabetical order.

00:09:54.080 --> 00:09:55.720
Excellent, success.

00:09:55.720 --> 00:09:58.080
Now that our work is done,
the final step is to

00:09:58.080 --> 00:10:00.400
shut down the cluster.

00:10:00.400 --> 00:10:03.850
Jumping back to the terminal
window, we can easily do that

00:10:03.850 --> 00:10:06.870
by running the compute cluster
for Hadoop script with the

00:10:06.870 --> 00:10:10.020
shutdown subcommand and
specifying our project ID.

00:10:12.970 --> 00:10:15.250
There, the cluster is
now shutting down.

00:10:20.230 --> 00:10:23.600
So we've now seen the process of
setting up a Hadoop cluster

00:10:23.600 --> 00:10:26.940
on Compute Engine using the
Getting Started sample.

00:10:26.940 --> 00:10:29.710
We also saw how to use the
sample to run a MapReduce job

00:10:29.710 --> 00:10:33.510
that processes a large data set
and generates a result.

00:10:33.510 --> 00:10:35.570
Using the Hadoop sample
application as a starting

00:10:35.570 --> 00:10:38.410
point, we've got all the pieces
in place to begin using

00:10:38.410 --> 00:10:41.000
Hadoop on Google's powerful
infrastructure.

00:10:41.000 --> 00:10:42.250
Thank you for watching.

