Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video we'll look at what is the capacity of a multilayer neural network.

Speaker 2:          00:07          Now we are already seeing that and artificial single artificial neuron was limited at its capacity. There are certain classification problems that it cannot actually model if it's, if they're not linearly separable. However, we've looked at an approach where we would first learn a representation or transformation of our original space, uh, in such a way that, uh, we could get a new representation where the data was then linearly separable. And then we've used that as a justification for having a, what we call the single hidden layer neural network, uh, where we would first compute a representation that hidden representation. So I transformation of our input space and hopefully in such a way that then the output of our neural network could then perform its jobs, its job more, more, much more easily. So in the context of classification, uh, so that, uh, it could actually perform the classification, uh, in the, uh, original space, perfectly distinguishing between the, uh, inputs in the class zero on class one.

Speaker 2:          01:19          Now, uh, can we actually do a show anything formal about that? So, in other words, if we assume that we have a function of f, f of x that takes this particular form that computes a hidden, they are pre activation applies an activation function, computes and output linear combination. So literary combining the new representation, linear representation, uh, sorry, hidden representation of uh, the input and then applying some activation function at the output to have a function that performs all of these transformations. What kind of functions can we actually model with this particular form and other functions that we cannot model? So let's look at this a little bit and try to get some intuition about around this problem.

Speaker 1:          02:07          Cool.

Speaker 2:          02:09          No, here's the first example where we have a single layer neural network with two hidden units, which a compute these two functions here.

Speaker 1:          02:22          Okay.

Speaker 2:          02:24          Now, uh, you should be able to convince yourself that, uh, add the output layer. If you have an output learner neuron, then we can actually combine these two hidden units to get a new function, which respect to our original input. That would actually look something like this. We'd get a ridge that goes up here and then that goes down here. And essentially the way we can get this is by taking this neuron and subtracting from it. Disney Rod. So by getting a linear, a combination of [inaudible] with this neurons specifically, essentially this neuron minus that neuron, we can actually get a ridge that is a localized in a particular range of a combinations of x and x one and x two values. So indeed, if we're here, then this neuron as a small value, but this neuron too. So we get a small value at the output.

Speaker 2:          03:20          Now if we're in this range, sorry, excuse another color, if we are in this range, then this neuron that has a large value, but there's some, this neuron also still has a low value. So because of that we get a large value minus a low value. So we get at the output a fairly large value. And then if we reach this part here, then this is still large, but this is now the second neuron has not become larger. So the value at the output we'll now drop because we're not subtracting value that has become larger at the second, uh, hidden neuron. So we see, so this is, you know, intuitively how we can get from these two simple neurons. A more complicated output, which is localized in a particular range of x one and x two values.

Speaker 2:          04:12          And our weekend go, uh, into a context that's a bit more complicated where we could get for neurons that would compute these four functions. And by combining them in a fairly similar fashion, we can actually get a d output, a localized bump. So four values, uh, of these, uh, inputs x one and x two. Within this part here, we could get an output, which actually is large and a low everywhere else here. Okay. And then we could actually, uh, continue making a more complex output by having four other neurons that would allow us to compute, say, another bump maybe here, or maybe we could get a bump that's here with these, uh, with four other neurons. So we can see that by combining in two dimensions, these, uh, for neurons together and having sets of four neurons for different parts of the input space, we could actually add a bunch of different bumps in different, uh, parts of the space and sort of carve out a, uh, more and more complicated function.

Speaker 2:          05:21          So if we think of this, uh, if we go in the sending of a classification, this means that we could have, uh, chunks of regions that are actually separated and, but that correspond to the same class. So say that our two corresponds to class one and our one class zero. Then by having uh, uh, several bumps here and bumps here and no bumps in the [inaudible] region here and here, we can actually get there fairly complicated decision and a fairly complicated decision, uh, classification decision with a neural network that could have enough, um, hidden units to place bumps at the right places. Uh, uh, to model our complicated decision function.

Speaker 2:          06:16          Not can we actually be formal about this? Uh, well it turns out we can. So we can show that a neural network with a single hidden layer is actually what is known as a universal approximator. Uh, so, uh, more specifically, we can say that a single hidden layer neural network with a linear output can approximate any continuous function arbitrarily well as long as it has enough hidden units. So this was shown by Hornik in 1991. So there are more details and technical details behind this theorem. So I encourage you to consult the original paper, but this is essentially saying that if you have a function which is continuous, so that does not vary too much, um, then we can actually model it as long as we give our neural network enough hidden units. So there are configurations of these, uh, hidden units and the output unit, there are values of their weights and biases such that we can model any function arbitrarily.

Speaker 2:          07:16          Well, uh, this was all actually applies whether you're using sigmoid hidden units or tench hidden units. And there are also many other, uh, hidden layer activation functions that, uh, makes this result true that, that complies with this result. And so what this is saying is that by using a neural networks, we, uh, are actually allowing ourselves to model many types of arbitrarily complicated, fairly arbitrarily complicated function. So this is a pretty good result. Now notice that it actually doesn't mean that there is a learning algorithm. So an algorithm that would find a behavior, the parameters of these, uh, of all the units in the neural network that would actually, uh, allow us to, uh, um, model any function. So in other words, uh, there might exists values of the connections, uh, weights and the biases for a neural network with enough hidden units that would allow us to model any functional HR well, but it doesn't mean that we actually have the learning algorithm that will find the values of these connections and biases for any given function. And, uh, this is, uh, an in fact, there aren't really any known algorithm that can do this for any types of function. And, uh, we'll see that, uh, actually this is a whole field of research trying to develop more and more, uh, improve learning algorithms that can learn, uh, more and more types of functions and more specifically that can solve more and more, uh, types of, uh, problems usually related to artificial intelligence.