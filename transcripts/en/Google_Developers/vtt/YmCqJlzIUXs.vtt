WEBVTT
Kind: captions
Language: en

00:00:00.880 --> 00:00:02.940
JAMES DONG: Good afternoon.

00:00:02.940 --> 00:00:04.370
My name is James Dong.

00:00:04.370 --> 00:00:07.250
I'm working on the Android
Media team.

00:00:07.250 --> 00:00:12.850
Today, I'm here to present you
set of new low-level media

00:00:12.850 --> 00:00:19.121
APIs we published in Android
Jellybean release.

00:00:19.121 --> 00:00:23.370
So this set of low-level media
APIs allows the Java

00:00:23.370 --> 00:00:27.770
applications to directly access
the low-level decoding,

00:00:27.770 --> 00:00:31.540
and the encoding capabilities of
the media codecs available

00:00:31.540 --> 00:00:35.300
on a specific Android device.

00:00:35.300 --> 00:00:40.630
We use this set of new
low-level media APIs.

00:00:40.630 --> 00:00:46.620
As Java developers, you are
given much more flexibility

00:00:46.620 --> 00:00:52.330
and a finer granularity control
on how your media

00:00:52.330 --> 00:00:55.770
application behaves.

00:00:55.770 --> 00:00:58.660
Today, my talk is partitioned
into two parts.

00:00:58.660 --> 00:01:02.350
In the first a part, I will
briefly talk about the

00:01:02.350 --> 00:01:08.130
existing Android media
architecture and the existing

00:01:08.130 --> 00:01:12.755
high level media APIs prior
to Jellybean release.

00:01:12.755 --> 00:01:17.160
In the second part, I will focus
on my talk on the new

00:01:17.160 --> 00:01:20.460
low level media APIs
we published

00:01:20.460 --> 00:01:22.890
in Jellybean release.

00:01:22.890 --> 00:01:28.770
So let's start off with the
brief introduction to the

00:01:28.770 --> 00:01:30.480
media framework architecture.

00:01:34.700 --> 00:01:36.320
Android power the devices.

00:01:36.320 --> 00:01:41.840
Typically, you see many media
rated applications, such as

00:01:41.840 --> 00:01:49.150
Camera, Music app,
Gallery, YouTube,

00:01:49.150 --> 00:01:52.030
movie player, et cetera.

00:01:52.030 --> 00:01:55.170
All of these applications
basically use the same

00:01:55.170 --> 00:01:56.800
architecture.

00:01:56.800 --> 00:01:59.650
Each application is partitioned
into two separate

00:01:59.650 --> 00:02:05.450
address spaces, the application
process and the

00:02:05.450 --> 00:02:07.560
media server process.

00:02:07.560 --> 00:02:11.240
The communication between the
application process and the

00:02:11.240 --> 00:02:15.430
MediaServer process is using the
Android remote procedure

00:02:15.430 --> 00:02:17.880
call, or Binder call.

00:02:17.880 --> 00:02:23.970
In the MediaServer process, a
set of commonly used media

00:02:23.970 --> 00:02:27.510
services, such as the playback,
recording, audio

00:02:27.510 --> 00:02:31.070
routing, are provided.

00:02:31.070 --> 00:02:36.340
The MediaServer process then
talks to the drivers in a

00:02:36.340 --> 00:02:40.770
kernel through a hardware
abstract layer.

00:02:40.770 --> 00:02:44.520
For decoding and the encoding
purpose, the hardware

00:02:44.520 --> 00:02:48.940
abstraction layer is the OpenMAX
integration layer.

00:02:48.940 --> 00:02:53.080
You can find out a detailed
reference UI, basically, at

00:02:53.080 --> 00:02:56.340
the end of my slides if
you want to find out

00:02:56.340 --> 00:02:57.590
details about OpenMAX.

00:03:03.140 --> 00:03:06.890
So prior to Jellybean release,
we already have a set of

00:03:06.890 --> 00:03:10.210
high-level media APIs.

00:03:10.210 --> 00:03:14.890
With these high-level media
APIs, you are able to write

00:03:14.890 --> 00:03:19.220
applications quickly.

00:03:19.220 --> 00:03:23.090
On this slide, I show you a
really simple example on how

00:03:23.090 --> 00:03:27.770
to write a playback, or
streaming application using a

00:03:27.770 --> 00:03:32.020
high-level media API, which is
the MediaPlayer.Java class.

00:03:35.390 --> 00:03:37.850
As this point, I assume you
are pretty familiar with

00:03:37.850 --> 00:03:40.180
MediaPlayer.Java class.

00:03:40.180 --> 00:03:42.380
And I will quickly
walk through the

00:03:42.380 --> 00:03:45.630
example on this slide.

00:03:45.630 --> 00:03:49.300
First, what you need to do is
to create a media player

00:03:49.300 --> 00:03:55.080
object, and then set its data
source by passing it a URL.

00:03:55.080 --> 00:04:00.510
The URL can be a local file on
your Android device, or it can

00:04:00.510 --> 00:04:05.220
be pointing to a media data
source that is located on a

00:04:05.220 --> 00:04:07.920
remote server.

00:04:07.920 --> 00:04:11.000
So after you config the media
player object, then you can

00:04:11.000 --> 00:04:14.840
send the prepare message to
the media object, which

00:04:14.840 --> 00:04:19.290
basically will validate the data
sources, see whether this

00:04:19.290 --> 00:04:22.820
file format is supported
by the parser.

00:04:22.820 --> 00:04:30.050
And if the parsing succeeded,
then you can use the common

00:04:30.050 --> 00:04:38.390
set of media controls like
start, pause, and to control

00:04:38.390 --> 00:04:40.880
the payback or streaming.

00:04:40.880 --> 00:04:45.810
You can also jump to a specific
time position by

00:04:45.810 --> 00:04:52.100
calling seekTo if the data
source is seekable.

00:04:52.100 --> 00:04:55.960
So it depends on the interaction
between users and

00:04:55.960 --> 00:04:56.680
the application.

00:04:56.680 --> 00:05:00.850
This process may be iterated
multiple times.

00:05:00.850 --> 00:05:05.330
And then, at the end of a
playback session, you want to

00:05:05.330 --> 00:05:10.780
terminate the playback session
by calling stop.

00:05:10.780 --> 00:05:13.890
Immediately after the
termination for playback

00:05:13.890 --> 00:05:19.870
session, you want to release
all of the associated

00:05:19.870 --> 00:05:25.140
resources with media player
object by calling release.

00:05:25.140 --> 00:05:29.870
So as you can see from this
extremely simplified example,

00:05:29.870 --> 00:05:34.100
you do not need to write
too many lines of code.

00:05:34.100 --> 00:05:39.660
And it provides a high
abstraction level on how you

00:05:39.660 --> 00:05:45.530
can control a media playback
or streaming application.

00:05:45.530 --> 00:05:50.670
So with these existing
high-level media APIs prior to

00:05:50.670 --> 00:05:56.830
Jellybean release, we are able
to meet the needs of mostly

00:05:56.830 --> 00:06:00.130
commonly used media application
use cases.

00:06:00.130 --> 00:06:07.360
And we are able to support a
commonly used file formats.

00:06:07.360 --> 00:06:12.810
In addition, we are able to
support most commonly used

00:06:12.810 --> 00:06:21.040
streaming format, a protocol
like HTTP, RTSP, [INAUDIBLE].

00:06:21.040 --> 00:06:28.620
However, as a powered Java
developer, you are often given

00:06:28.620 --> 00:06:31.740
the challenge to do more.

00:06:31.740 --> 00:06:36.520
So what if you are given a task
to write an application

00:06:36.520 --> 00:06:40.620
to support a file format that
is not yet supported by the

00:06:40.620 --> 00:06:42.100
Android platform?

00:06:42.100 --> 00:06:47.090
What if you are given a task
to write a streaming

00:06:47.090 --> 00:06:51.640
application to use a streaming
protocol that is not yet

00:06:51.640 --> 00:06:55.240
supported by the Android
platform?

00:06:55.240 --> 00:07:00.090
What if you want to do some
image processing on the output

00:07:00.090 --> 00:07:05.230
of the decoder, and then you
send the [INAUDIBLE]

00:07:05.230 --> 00:07:06.730
for rendering?

00:07:06.730 --> 00:07:12.130
So what if after the image
processing, you want to send

00:07:12.130 --> 00:07:17.595
the output to be used as an
input to an encoder, and then

00:07:17.595 --> 00:07:20.750
encode it and then streaming
the result out?

00:07:20.750 --> 00:07:25.660
For these kinds of advanced
challenges, with the existing

00:07:25.660 --> 00:07:30.550
high-level media APIs, to
be honest, you probably

00:07:30.550 --> 00:07:32.590
cannot do too much.

00:07:32.590 --> 00:07:38.010
So that's why we published a
set of new low-level media

00:07:38.010 --> 00:07:43.260
APIs in Jellybean release
to address this set of

00:07:43.260 --> 00:07:45.810
challenges.

00:07:45.810 --> 00:07:52.010
So this new set of low-level
media APIs mainly consist of

00:07:52.010 --> 00:07:53.540
three parts.

00:07:53.540 --> 00:07:57.830
The first part is
a file parser.

00:07:57.830 --> 00:08:03.240
Basically, it's a new class
called MediaExtractor.

00:08:03.240 --> 00:08:07.890
It allows you to parse all the
currently supported file

00:08:07.890 --> 00:08:12.600
formats on Android platform.

00:08:12.600 --> 00:08:18.450
The second part of the new
low-level API is the ability

00:08:18.450 --> 00:08:23.650
to query the capabilities of the
media codecs available on

00:08:23.650 --> 00:08:26.120
an Android device.

00:08:26.120 --> 00:08:31.690
Why you need this capability is
you need to find out what

00:08:31.690 --> 00:08:34.789
are the available codecs so
that you can create the

00:08:34.789 --> 00:08:40.840
specific media codec to do
decoding and encoding directly

00:08:40.840 --> 00:08:42.789
from your application process.

00:08:42.789 --> 00:08:48.190
So that's why you need the third
part of the API, which

00:08:48.190 --> 00:08:49.440
is the MediaCodec.Javaclass.

00:08:53.730 --> 00:08:59.335
So this class provide the direct
access of decoding and

00:08:59.335 --> 00:09:05.250
the encoding functions of the
available media codecs on an

00:09:05.250 --> 00:09:07.670
Android device.

00:09:07.670 --> 00:09:11.850
So I will start with the first
part, the file parser.

00:09:15.130 --> 00:09:18.770
Before I want to talk about the
details of the APIs in the

00:09:18.770 --> 00:09:24.260
MediaExtractor class, a basic
understanding of a general

00:09:24.260 --> 00:09:30.380
file structure for a typical
media data source might help.

00:09:30.380 --> 00:09:37.110
So a typical media data source
consists of one or more

00:09:37.110 --> 00:09:41.110
elementary streams, which
we call tracks.

00:09:41.110 --> 00:09:45.880
Each track then belongs to a
specific type, like audio or

00:09:45.880 --> 00:09:48.680
video or text.

00:09:48.680 --> 00:09:57.730
So for each specific track, it
has one or more sample datas

00:09:57.730 --> 00:09:59.600
associated with it.

00:09:59.600 --> 00:10:05.820
For each sample data, it can be
a decoder specific data, or

00:10:05.820 --> 00:10:13.150
it can be the actual compressed
access unit of

00:10:13.150 --> 00:10:15.530
audio frame, or video frame.

00:10:15.530 --> 00:10:21.280
So it can has data itself, and
it also provides information

00:10:21.280 --> 00:10:25.540
about the media data, such as
the time stamp you need to

00:10:25.540 --> 00:10:28.520
present the data.

00:10:28.520 --> 00:10:34.230
And usually, for video purpose,
it will also tell you

00:10:34.230 --> 00:10:39.400
whether the current sample
is a sync frame.

00:10:39.400 --> 00:10:44.350
So with this basic understanding
of the file

00:10:44.350 --> 00:10:49.530
structure of a typical media
source, then I will to

00:10:49.530 --> 00:10:55.350
introduce you the first you
low-level media API, which is

00:10:55.350 --> 00:10:58.000
MediaExtractor.

00:10:58.000 --> 00:11:01.990
So if you look at the
MediaExtractor, we provide a

00:11:01.990 --> 00:11:07.490
defaulting constructor for you
to create an instance of a

00:11:07.490 --> 00:11:09.180
MediaExtractor.

00:11:09.180 --> 00:11:13.310
And then, you can use a set of
data source, which I assume

00:11:13.310 --> 00:11:18.310
you are pretty familiar with a
set of data source methods

00:11:18.310 --> 00:11:22.460
similar to what media
player has to

00:11:22.460 --> 00:11:25.130
configure your data source.

00:11:25.130 --> 00:11:29.080
So once the data source is
configured, you can find out

00:11:29.080 --> 00:11:32.860
the total number of available
tracks from your data source

00:11:32.860 --> 00:11:34.450
by calling getTrackCount.

00:11:38.010 --> 00:11:42.090
And then, for each individual
track count, you can find of

00:11:42.090 --> 00:11:48.560
detailed format about it by
calling getTrackFormat.

00:11:48.560 --> 00:11:54.870
So getTrackFormat will return
you a object of media format,

00:11:54.870 --> 00:12:01.450
which is another API we released
in Android Jellybean.

00:12:01.450 --> 00:12:06.630
I'll talk about media format
in the next slide.

00:12:06.630 --> 00:12:13.430
So in order to use the parser,
then you need to select a

00:12:13.430 --> 00:12:17.090
specific media track to use,
to read the data from.

00:12:20.180 --> 00:12:24.260
In order to do that, you
can call selectTrack.

00:12:24.260 --> 00:12:28.470
So for instance, a media data
source may have multiple audio

00:12:28.470 --> 00:12:31.210
tracks for different
languages.

00:12:31.210 --> 00:12:35.710
So you want to select a specific
language for your

00:12:35.710 --> 00:12:37.020
application to use.

00:12:37.020 --> 00:12:40.100
Then you can use selectTrack.

00:12:40.100 --> 00:12:44.340
You can also deselect this track
so that you won't pass

00:12:44.340 --> 00:12:46.700
data from this specific
media track.

00:12:53.390 --> 00:12:59.120
You can also make do a random
seek within the data source by

00:12:59.120 --> 00:13:03.250
calling seekTo, which
takes two arguments.

00:13:03.250 --> 00:13:06.180
First one is the time position
you want to seek to.

00:13:06.180 --> 00:13:10.870
The second argument is basically
the seek mode.

00:13:10.870 --> 00:13:14.800
So for example, if you want to
seek to a sync frame, or you

00:13:14.800 --> 00:13:19.060
want to seek to a time position
that is after but

00:13:19.060 --> 00:13:23.690
closer to a sync frame.

00:13:23.690 --> 00:13:29.080
So there are a number of seek
modes you can use to affect

00:13:29.080 --> 00:13:35.580
the behavior, how you seek to
inside the data source.

00:13:35.580 --> 00:13:40.440
If you don't want to do a random
seek, you just want to

00:13:40.440 --> 00:13:44.060
move to next sample position,
you can just simply call

00:13:44.060 --> 00:13:50.950
advance, which will move to the
next sample sequentially.

00:13:50.950 --> 00:13:55.870
So once you reach a specific
time position and then you can

00:13:55.870 --> 00:14:01.950
read media data or decoder
specific information data

00:14:01.950 --> 00:14:05.980
using readSampleData message.

00:14:05.980 --> 00:14:10.030
So this is readSampleData
message takes two arguments.

00:14:10.030 --> 00:14:14.690
The first one is the buffer
that you want to read the

00:14:14.690 --> 00:14:16.690
sample data into.

00:14:16.690 --> 00:14:20.410
The second argument, basically,
provide an offset

00:14:20.410 --> 00:14:25.250
that you'll start to
put your data in.

00:14:25.250 --> 00:14:30.520
It also returns an integer to
indicate whether your read is

00:14:30.520 --> 00:14:33.730
successful, or whether
it reaches the end

00:14:33.730 --> 00:14:37.660
of your data source.

00:14:37.660 --> 00:14:42.000
So if it's non-active value,
it indicates the read is

00:14:42.000 --> 00:14:43.690
successful.

00:14:43.690 --> 00:14:48.190
If it is an active value, that
indicates the read reaches the

00:14:48.190 --> 00:14:51.580
end of a stream.

00:14:51.580 --> 00:14:54.570
So for each example, as I just
mentioned in the previous

00:14:54.570 --> 00:14:59.380
slide, you can find a bunch
of metadata associated.

00:14:59.380 --> 00:15:04.180
So one of the important pieces
of information is the

00:15:04.180 --> 00:15:06.010
presentation time.

00:15:06.010 --> 00:15:08.670
So in order to find the
presentation time for that

00:15:08.670 --> 00:15:12.140
sample, you can call
getSampleTime.

00:15:12.140 --> 00:15:15.080
Reach will return the
presentation time in

00:15:15.080 --> 00:15:17.720
microseconds.

00:15:17.720 --> 00:15:21.510
And you can also find out the
flags associated with this

00:15:21.510 --> 00:15:25.750
particular sample, such as
whether this sample is a sync

00:15:25.750 --> 00:15:34.610
frame, whether the sample is
decoder specific information.

00:15:34.610 --> 00:15:41.810
If your data source is not a
local file source, then you

00:15:41.810 --> 00:15:45.900
need to cache the data.

00:15:45.900 --> 00:15:50.640
So in order to find out how many
cached data in a RAM, or

00:15:50.640 --> 00:15:57.010
in a memory, you can use
getCachedDuration to find out

00:15:57.010 --> 00:16:00.900
how much amount of data actually
cached in memory.

00:16:00.900 --> 00:16:06.240
And you can also use
hasCacheReachedEndOfStream to

00:16:06.240 --> 00:16:11.490
find out whether the data
cached in memory already

00:16:11.490 --> 00:16:13.165
reached the end of
a stream or not.

00:16:16.500 --> 00:16:19.470
As I mentioned a little bit
earlier, in order to find out

00:16:19.470 --> 00:16:27.590
details about a specific media
track, you need to use a new

00:16:27.590 --> 00:16:30.510
class called MediaFormat.

00:16:30.510 --> 00:16:36.820
MediaFormat.Javaclass basically
encapsulate a set of

00:16:36.820 --> 00:16:38.070
key value pairs.

00:16:41.020 --> 00:16:45.880
And in addition to that, it also
makes available a bunch

00:16:45.880 --> 00:16:49.970
of status in the getter's
message, so that you can find

00:16:49.970 --> 00:16:54.120
out the value or modify
the value associated

00:16:54.120 --> 00:16:56.380
with a given key.

00:16:56.380 --> 00:17:01.810
For example, if you want to
find out an audio track's

00:17:01.810 --> 00:17:07.390
number of audio channels, you
can just provide a key channel

00:17:07.390 --> 00:17:12.930
count to find out the value
about the number of channels.

00:17:12.930 --> 00:17:19.420
So to give you an example on
how these two API works, I

00:17:19.420 --> 00:17:21.300
will show you two examples.

00:17:21.300 --> 00:17:25.190
The first example basically
shows you a typically use of

00:17:25.190 --> 00:17:29.170
the MediaExtractor
API we provided.

00:17:29.170 --> 00:17:33.240
So first off, you need to
create a MediaExtractor

00:17:33.240 --> 00:17:37.300
instance by calling its
default constructor.

00:17:37.300 --> 00:17:42.000
And then, just like your media
player object, you'll set its

00:17:42.000 --> 00:17:43.600
data source.

00:17:43.600 --> 00:17:49.100
So once its data source is set,
and then you can find the

00:17:49.100 --> 00:17:52.850
total available tracks from that
data source by calling

00:17:52.850 --> 00:17:54.850
getTrackCount.

00:17:54.850 --> 00:18:02.270
So by iterate through all the
available tracks, so you can

00:18:02.270 --> 00:18:06.360
find out the media track you are
interested in and then you

00:18:06.360 --> 00:18:11.050
select that specific media track
by calling selectTrack.

00:18:11.050 --> 00:18:15.990
So once you select the
particular media track you are

00:18:15.990 --> 00:18:21.080
interested in, then you are able
to read data from that

00:18:21.080 --> 00:18:26.580
track by calling
readSampleData.

00:18:26.580 --> 00:18:30.900
So readSampleData basically will
read the sample the give

00:18:30.900 --> 00:18:36.610
time position into the
buffer it provided.

00:18:36.610 --> 00:18:41.660
If the readSampleData returns
a negative value, that means

00:18:41.660 --> 00:18:44.520
you already reached
the end of stream.

00:18:44.520 --> 00:18:47.740
If it is a positive number,
that means the read is

00:18:47.740 --> 00:18:48.960
successful.

00:18:48.960 --> 00:18:53.010
And in addition to the media
data, you can find out it's

00:18:53.010 --> 00:18:55.730
metadata, like its presentation
time, sample

00:18:55.730 --> 00:19:00.240
time, and flags associated
with the sample.

00:19:00.240 --> 00:19:03.190
And you can decide whether you
want to move to the next

00:19:03.190 --> 00:19:07.510
sample by calling
advance message.

00:19:07.510 --> 00:19:11.200
Or you want to jump to a
random time position by

00:19:11.200 --> 00:19:13.020
calling seekTo.

00:19:13.020 --> 00:19:16.850
So this process can
iterate until you

00:19:16.850 --> 00:19:19.180
reach the end of stream.

00:19:19.180 --> 00:19:22.320
Once you're done with the
MediaExtractor, then you just

00:19:22.320 --> 00:19:27.550
call release message to release
all the resources

00:19:27.550 --> 00:19:28.800
associated with this object.

00:19:31.550 --> 00:19:36.610
So how to find out a media track
that you are interested

00:19:36.610 --> 00:19:40.220
in, you'll need to know the
detail to track information.

00:19:40.220 --> 00:19:48.650
That's why you need to use
this MediaFormat class.

00:19:48.650 --> 00:19:53.430
So with this MediaFormat class,
you can find out the

00:19:53.430 --> 00:19:56.890
mime stream for a specific
media track.

00:19:56.890 --> 00:20:00.250
And from the mime stream, you
are able to tell whether this

00:20:00.250 --> 00:20:05.620
media track is audio or video,
or any other media type.

00:20:05.620 --> 00:20:09.190
And if it is audio, you can find
out number of channels,

00:20:09.190 --> 00:20:14.200
sample rate, by using a getter
message, as I mentioned.

00:20:14.200 --> 00:20:19.260
If it's video, you can use
the getter message, like

00:20:19.260 --> 00:20:23.280
getInteger, to find out
the video resolution.

00:20:23.280 --> 00:20:27.470
So the use for MediaFormat
class is really

00:20:27.470 --> 00:20:28.720
straightforward.

00:20:31.720 --> 00:20:37.760
I want to emphasize the file
parser MediaExtractor class

00:20:37.760 --> 00:20:42.460
basically supports all the
existing formats currently we

00:20:42.460 --> 00:20:44.970
are supporting on Android
platform.

00:20:44.970 --> 00:20:51.280
However, if you ask to support
a file format that is not

00:20:51.280 --> 00:20:54.520
currently supported, you
need to implement

00:20:54.520 --> 00:20:56.860
your own file parser.

00:20:56.860 --> 00:20:59.920
So this MediaExtractor basically
just provides you an

00:20:59.920 --> 00:21:03.750
example of how you can implement
your own media

00:21:03.750 --> 00:21:06.400
extractor to support a new file

00:21:06.400 --> 00:21:10.270
format that is not supported.

00:21:10.270 --> 00:21:14.700
So that's the first part of
the low-level media API.

00:21:14.700 --> 00:21:18.660
Now, let me move on to the
second part of the media API.

00:21:18.660 --> 00:21:25.620
Basically, it's the ability to
query the capabilities of

00:21:25.620 --> 00:21:31.070
media codecs available
on an Android device.

00:21:31.070 --> 00:21:37.060
So we published two new APIs,
MediaCodecList and

00:21:37.060 --> 00:21:39.510
MediaCodeInfo.

00:21:39.510 --> 00:21:46.160
So let me show you how
these two API works.

00:21:46.160 --> 00:21:48.380
For MediaCodecList,
it's very simple.

00:21:48.380 --> 00:21:52.270
There are only two public
APIs you can call.

00:21:52.270 --> 00:21:56.460
The first one lets you'll find
out the total number of

00:21:56.460 --> 00:21:59.600
available codec count on
an Android device.

00:22:03.910 --> 00:22:08.310
The first one, basically,
is getCodecCount method.

00:22:08.310 --> 00:22:12.550
The second method is
getCodecInfoAt message, which

00:22:12.550 --> 00:22:17.340
allows you to find out the
detail capabilities of a

00:22:17.340 --> 00:22:20.010
specific MediaCodec.

00:22:20.010 --> 00:22:26.120
So you need to use the new API,
MediaCodecInfo, in order

00:22:26.120 --> 00:22:30.650
to find out details,
capabilities of a MediaCodec.

00:22:30.650 --> 00:22:35.350
So the MediaCodecInfo class will
tell you what's the codec

00:22:35.350 --> 00:22:39.240
name for this MediaCodec,
whether it is a

00:22:39.240 --> 00:22:42.090
decoder or an encoder.

00:22:42.090 --> 00:22:47.100
And you can also find out the
number of MIME types that this

00:22:47.100 --> 00:22:49.380
specific MediaCodec supports.

00:22:49.380 --> 00:22:54.680
So just for your information,
the same MediaCodec may be

00:22:54.680 --> 00:22:57.310
able to support multiple
MIME types.

00:22:57.310 --> 00:23:01.350
So that's why the
getSupportedTypes returns a

00:23:01.350 --> 00:23:06.620
rate of stream.

00:23:06.620 --> 00:23:10.040
So it basically tells
you all the MIME

00:23:10.040 --> 00:23:11.900
types it will support.

00:23:11.900 --> 00:23:15.990
By iterating through the
supported types, you are able

00:23:15.990 --> 00:23:22.690
to find out the capabilities for
a specific MIME type for

00:23:22.690 --> 00:23:25.350
this given MediaCodec.

00:23:28.000 --> 00:23:32.250
So you need this inner class
called CodecCapabilities in

00:23:32.250 --> 00:23:35.980
order to find out the
specific capability.

00:23:35.980 --> 00:23:39.380
So what are the capabilities
exposed through this API?

00:23:39.380 --> 00:23:43.040
Basically, two pieces
of information.

00:23:43.040 --> 00:23:47.120
The first one is a list of
color formats that is

00:23:47.120 --> 00:23:50.080
supported by this codec.

00:23:50.080 --> 00:23:53.950
The second piece of information,
basically, is a

00:23:53.950 --> 00:24:00.250
list of profiles and levels
supported by this MediaCodec.

00:24:00.250 --> 00:24:11.720
So to give you an example on
how this MediaCodecList and

00:24:11.720 --> 00:24:17.950
MediaCodecInfo works, here
is a very simple example.

00:24:17.950 --> 00:24:23.310
OK, so first you call
MediaCodecList.getCodecCount

00:24:23.310 --> 00:24:29.390
to find out all the available
codecs on a specific device.

00:24:29.390 --> 00:24:32.660
So once you have the count, then
you can iterate through

00:24:32.660 --> 00:24:40.360
it by calling getCodecInfoAt
to find out the detailed

00:24:40.360 --> 00:24:44.770
compatibility of a specific
MediaCodec.

00:24:44.770 --> 00:24:50.670
So this method returns you an
object of MediaCodecInfo.

00:24:50.670 --> 00:24:56.630
So using that info object, you
can find out whether this

00:24:56.630 --> 00:24:59.620
codec is encoder or decoder.

00:24:59.620 --> 00:25:03.120
And you can find out all the
MIME types it supports by

00:25:03.120 --> 00:25:04.420
calling getSupportedTypes.

00:25:08.040 --> 00:25:12.750
With the rate of supported
types, then you can iterate

00:25:12.750 --> 00:25:18.590
through all of them and find
out the capability for each

00:25:18.590 --> 00:25:24.170
MIME type that this media
codec can do.

00:25:24.170 --> 00:25:30.690
So in this particular example, I
would like to find out which

00:25:30.690 --> 00:25:39.950
MediaCodec is capable of
decoding an AVC video that is

00:25:39.950 --> 00:25:43.380
encoded in high profile,
level 4.0.

00:25:43.380 --> 00:25:45.300
So how to find out that.

00:25:45.300 --> 00:25:50.250
And first, you can check the
type, see whether the MIME

00:25:50.250 --> 00:25:55.740
type supported by this
MediaCodec is AVC.

00:25:55.740 --> 00:26:00.690
So basically, just check the
type against video slash AVC.

00:26:00.690 --> 00:26:04.410
You can find out whether this
MediaCodec is support AVC.

00:26:04.410 --> 00:26:09.610
So if this video codec is
support AVC, then you can find

00:26:09.610 --> 00:26:16.720
out the capabilities
of this MediaCodec.

00:26:16.720 --> 00:26:19.280
So there are two pieces
of information,

00:26:19.280 --> 00:26:20.690
as I mentioned earlier.

00:26:20.690 --> 00:26:24.170
One is that the list of
output color format.

00:26:24.170 --> 00:26:26.610
And the second one is the
list of profiles and

00:26:26.610 --> 00:26:28.640
levels getting exposed.

00:26:28.640 --> 00:26:31.580
So you can just iterate through
the list of profiles

00:26:31.580 --> 00:26:35.810
and the levels exposed by the
capability object, and then

00:26:35.810 --> 00:26:40.050
compel to see whether this
MediaCodec is supporting high

00:26:40.050 --> 00:26:41.530
profile level four.

00:26:46.780 --> 00:26:50.240
So the use of this
MediaCodecList, and the

00:26:50.240 --> 00:26:55.255
MediaCodecInfo API is pretty
much straightforward.

00:26:58.950 --> 00:27:02.690
Behind a theme how
this API works.

00:27:02.690 --> 00:27:11.630
For each Jellybean Android
device, we require a registry

00:27:11.630 --> 00:27:17.720
XML file to be put on the
device, which actually tells

00:27:17.720 --> 00:27:22.100
you all the available
Media Codecs.

00:27:22.100 --> 00:27:29.340
So on this slide, I show you
a simple example of this

00:27:29.340 --> 00:27:33.120
media_codecs.xml file.

00:27:33.120 --> 00:27:37.700
So in this XML file, you can
see there are two lists of

00:27:37.700 --> 00:27:38.890
Media Codecs.

00:27:38.890 --> 00:27:41.590
One list is for encoder.

00:27:41.590 --> 00:27:44.750
The second list is
for decoder.

00:27:44.750 --> 00:27:48.700
Regardless of whether it is
a decoder instance, or an

00:27:48.700 --> 00:27:58.490
encoder instance, each of
MediaCodec entry has a single

00:27:58.490 --> 00:28:00.130
attribute for name.

00:28:00.130 --> 00:28:04.400
So this name will tell you the
MediaCodec name that you got

00:28:04.400 --> 00:28:06.410
from MediaCodecInfo class.

00:28:11.490 --> 00:28:13.610
So that's the getName method.

00:28:13.610 --> 00:28:17.170
It will return you the name
attribute of the MediaCodec.

00:28:20.280 --> 00:28:26.020
Any media codec, actually,
with the same name, can

00:28:26.020 --> 00:28:28.700
support multiple MIME types.

00:28:28.700 --> 00:28:34.040
So if you have multiple MIME
type support for a single

00:28:34.040 --> 00:28:38.890
MediaCodec, you can have
multiple attributes for type.

00:28:38.890 --> 00:28:46.020
That's why if you look at
MediaCodecInfo class, it has a

00:28:46.020 --> 00:28:49.100
method called getSupportedTypes,
so which

00:28:49.100 --> 00:28:54.580
will return all the MIME types
that you like available in

00:28:54.580 --> 00:28:59.510
this media_codecs.xml file for
a specific MediaCodec entry.

00:29:03.730 --> 00:29:10.700
So that MediaCodecList and
MediaCodecInfo classes

00:29:10.700 --> 00:29:18.210
addresses the need, and what are
the available codecs own

00:29:18.210 --> 00:29:21.000
the specific Android device.

00:29:21.000 --> 00:29:26.260
So once you know the available
codecs, you can use a media

00:29:26.260 --> 00:29:33.930
codec class to create an
instance of that codec to

00:29:33.930 --> 00:29:38.750
directly access the low-level
decoding or encoding

00:29:38.750 --> 00:29:40.620
capabilities.

00:29:40.620 --> 00:29:42.860
So how to do that.

00:29:42.860 --> 00:29:48.030
First, let me show you the
API available in this

00:29:48.030 --> 00:29:49.280
MediaCodec.Java class.

00:29:52.440 --> 00:29:56.750
The three methods in this class
allows you to create a

00:29:56.750 --> 00:29:58.340
MediaCodec.

00:29:58.340 --> 00:30:02.290
So a media codec can be created
either by the MIME

00:30:02.290 --> 00:30:07.040
type it supports, or if you know
the specific name for the

00:30:07.040 --> 00:30:10.620
MediaCodec, you can just
pass the name.

00:30:10.620 --> 00:30:13.010
You can create a decoding
instance, or an encoding

00:30:13.010 --> 00:30:14.470
instance this way.

00:30:16.970 --> 00:30:20.480
Once you have a MediaCodec
instance, then you need to

00:30:20.480 --> 00:30:27.650
configure this MediaCodec for
decoding or encoding purpose

00:30:27.650 --> 00:30:30.450
using config method.

00:30:30.450 --> 00:30:34.480
This config method requires
four arguments.

00:30:34.480 --> 00:30:37.800
The first argument
is MediaFormat.

00:30:37.800 --> 00:30:43.530
So MediaFormat, usually you can
get the return value from

00:30:43.530 --> 00:30:45.340
your MediaExtractor.

00:30:45.340 --> 00:30:50.720
So let me go back a few slides,
just to show you.

00:30:50.720 --> 00:30:54.090
See for example, the
getTrackFormat returns you the

00:30:54.090 --> 00:30:56.010
MediaFormat objects.

00:30:56.010 --> 00:31:06.570
So you can just pass that
MediaFormat object to the

00:31:06.570 --> 00:31:08.790
config method.

00:31:08.790 --> 00:31:15.180
So if it's a video track, if you
decide to render the video

00:31:15.180 --> 00:31:19.710
to a surface, you
pass the second

00:31:19.710 --> 00:31:22.050
argument, the surface argument.

00:31:22.050 --> 00:31:25.650
If your data source is
encrypted, and then you need

00:31:25.650 --> 00:31:31.300
to provide a MediaCrypto
object.

00:31:31.300 --> 00:31:36.440
The last flags indicate to the
API whether this is an encoder

00:31:36.440 --> 00:31:37.850
or decoder instance.

00:31:37.850 --> 00:31:40.810
So why you need that is because
if they the instance

00:31:40.810 --> 00:31:47.790
is created by the third create
by codec name, you are not

00:31:47.790 --> 00:31:50.810
able to tell whether this is
for encoding or decoding.

00:31:50.810 --> 00:31:55.090
So that's why you need a flag to
tell, so this config is for

00:31:55.090 --> 00:31:59.780
decoding purpose, or for
encoding purpose.

00:31:59.780 --> 00:32:04.470
The next set of API basically
is a standard set of media

00:32:04.470 --> 00:32:07.120
control, like, you can
do start, stop.

00:32:07.120 --> 00:32:12.240
You can do flash, I have a
list of flash [? gear. ?]

00:32:12.240 --> 00:32:16.630
I will escape those APIs,
basically just standard

00:32:16.630 --> 00:32:18.312
MediaControl API.

00:32:18.312 --> 00:32:27.560
In a core of MediaCodec API,
it's basically how we pass

00:32:27.560 --> 00:32:31.860
data back and the forth between
the media process and

00:32:31.860 --> 00:32:35.170
the application process and
the media server process.

00:32:35.170 --> 00:32:39.620
We are using array of byte
buffers for this purpose.

00:32:39.620 --> 00:32:43.870
There are two arrays of
byte buffers, one

00:32:43.870 --> 00:32:46.040
used for the input.

00:32:46.040 --> 00:32:49.090
The second one is
use for output.

00:32:49.090 --> 00:32:52.400
The actual communication between
your application

00:32:52.400 --> 00:32:55.770
process and the media server
process actually is not

00:32:55.770 --> 00:32:58.780
bypassing the byte
buffers directly.

00:32:58.780 --> 00:33:07.280
Instead, it is passing the index
into the array, rather

00:33:07.280 --> 00:33:11.360
than just pass the byte
buffer directly.

00:33:11.360 --> 00:33:18.990
So the next set of APIs
basically tells you how to use

00:33:18.990 --> 00:33:22.220
these buffers.

00:33:22.220 --> 00:33:24.880
So for input, you have
three methods.

00:33:24.880 --> 00:33:26.130
The first one is to
getInputBuffers.

00:33:30.190 --> 00:33:37.320
You only can call this message
after you start, after you

00:33:37.320 --> 00:33:39.690
call start.

00:33:39.690 --> 00:33:44.670
So this method will tell you the
array of byte buffers used

00:33:44.670 --> 00:33:49.030
for this codec session, either
an encoding session, or a

00:33:49.030 --> 00:33:52.100
decoding session.

00:33:52.100 --> 00:33:57.940
Once you know the array of byte
buffers, and then you can

00:33:57.940 --> 00:34:03.780
find out whether there are any
input buffers available for

00:34:03.780 --> 00:34:08.520
you to use by calling
dequeueInputBuffer.

00:34:08.520 --> 00:34:13.020
This dequeueInputBuffer take
an argument with timeout.

00:34:13.020 --> 00:34:18.159
So if you give a timeout value
negative value, basically you

00:34:18.159 --> 00:34:24.030
just wait forever until an input
buffer is available.

00:34:24.030 --> 00:34:29.699
Otherwise, you just wait for the
specified timeout period.

00:34:29.699 --> 00:34:34.909
And if there is no buffer
available, it will return a

00:34:34.909 --> 00:34:39.850
negative integer to indicate
it's timeout, and there is no

00:34:39.850 --> 00:34:42.270
input buffer available.

00:34:42.270 --> 00:34:46.679
So if there is an available
input buffer before the

00:34:46.679 --> 00:34:53.550
timeout period, then it will
return a valid byte array

00:34:53.550 --> 00:34:57.690
index, which basically
is a nonactive value.

00:34:57.690 --> 00:35:04.540
So using this index, and then
you can find out the right

00:35:04.540 --> 00:35:12.060
byte buffer using the input
array of byte buffers.

00:35:12.060 --> 00:35:16.520
So once you will find out the
right buffer to use, and then

00:35:16.520 --> 00:35:22.250
you can put valid input data
inside this byte buffer.

00:35:22.250 --> 00:35:25.210
So you can either do some image
processing before you

00:35:25.210 --> 00:35:27.450
put your data in, that's fine.

00:35:27.450 --> 00:35:29.720
It's up to you.

00:35:29.720 --> 00:35:35.590
But after you prepare your data
and to put it into the

00:35:35.590 --> 00:35:39.430
byte buffer, you can use the
next method, queueInputBuffer,

00:35:39.430 --> 00:35:45.490
to send this input buffer
to the codec.

00:35:45.490 --> 00:35:50.160
So this queueInputBuffer takes
a bunch of arguments.

00:35:50.160 --> 00:35:52.660
Those are pretty obvious.

00:35:52.660 --> 00:35:55.870
But I want to talk about
the first argument.

00:35:55.870 --> 00:35:58.490
As you can see, it is index.

00:35:58.490 --> 00:36:00.600
It is not the byte buffer.

00:36:00.600 --> 00:36:04.850
So you need to pass a
valid index to this

00:36:04.850 --> 00:36:10.040
queueInputBuffer, which usually
you got from the

00:36:10.040 --> 00:36:12.170
return value of
dequeueInputBuffer.

00:36:16.089 --> 00:36:20.000
The next argument, basically, is
offset, basically indicate

00:36:20.000 --> 00:36:22.330
the starting position
of your valid data

00:36:22.330 --> 00:36:24.170
inside that byte buffer.

00:36:24.170 --> 00:36:28.100
And size basically a number bias
of valid data you put in

00:36:28.100 --> 00:36:30.868
your byte buffer.

00:36:30.868 --> 00:36:35.230
The ctsTimeUs argument basically
tells you what is

00:36:35.230 --> 00:36:39.430
the presentation time for this
input buffer, and the flags,

00:36:39.430 --> 00:36:44.760
basically, a bunch of associated
flags associated

00:36:44.760 --> 00:36:47.550
with this byte buffer--

00:36:47.550 --> 00:36:51.810
like, whether this is a key
frame, something like that.

00:36:51.810 --> 00:36:56.630
So similarly for output buffer,
you can find out the

00:36:56.630 --> 00:36:59.810
array byte buffers by call
getOutputBuffers.

00:37:02.350 --> 00:37:06.600
Similarly, we are not using
the byte buffer directly

00:37:06.600 --> 00:37:12.680
between your application and
media server process.

00:37:12.680 --> 00:37:20.090
We are passing index of the
output byte buffer.

00:37:20.090 --> 00:37:23.850
So in order to find out whether
an output buffer is

00:37:23.850 --> 00:37:28.430
available, you'll call
dequeueOutputBuffer.

00:37:28.430 --> 00:37:33.290
The resume value will indicate
whether you get a

00:37:33.290 --> 00:37:36.240
valid output buffer.

00:37:36.240 --> 00:37:41.110
If it is a negative value,
there are three cases.

00:37:41.110 --> 00:37:44.380
It can be a timeout,
so indicate there

00:37:44.380 --> 00:37:46.650
was no buffer available.

00:37:46.650 --> 00:37:52.180
It can indicate something like
the output format changed.

00:37:52.180 --> 00:37:55.230
For decoding applications,
usually it happens.

00:37:55.230 --> 00:37:59.280
So you start to decode,
and you find out the

00:37:59.280 --> 00:38:01.080
output buffer changed.

00:38:01.080 --> 00:38:05.770
So an active return value from
dequeueOutputBuffer will tell

00:38:05.770 --> 00:38:09.640
you the output buffer
format changed,

00:38:09.640 --> 00:38:11.590
output format changed.

00:38:11.590 --> 00:38:14.830
The third negative value
can return from

00:38:14.830 --> 00:38:22.310
dequeueOutputBuffer is an
indication of output buffer

00:38:22.310 --> 00:38:24.510
itself changed.

00:38:24.510 --> 00:38:29.390
So after you receive that return
value, you need to call

00:38:29.390 --> 00:38:36.580
getOutputBuffer method again to
get a refresh on the array

00:38:36.580 --> 00:38:38.020
of output buffers.

00:38:40.970 --> 00:38:46.100
If dequeueOutputBuffer return
a nonactive value, meaning a

00:38:46.100 --> 00:38:51.550
valid buffer index, then you
can use that for the

00:38:51.550 --> 00:38:56.310
subsequent call,
releaseOutputBuffer.

00:38:56.310 --> 00:38:59.800
So before you call
releaseOutputBuffer, you may

00:38:59.800 --> 00:39:05.340
also do some image processing
on the buffer, and using the

00:39:05.340 --> 00:39:11.040
valid index to find out the
correct byte buffer

00:39:11.040 --> 00:39:14.070
that hold the data.

00:39:14.070 --> 00:39:17.970
And then you can do some image
processing with that data.

00:39:17.970 --> 00:39:21.750
And then you call this
releaseOutputBuffer.

00:39:21.750 --> 00:39:24.025
ReleaseOutputBuffer, basically,

00:39:24.025 --> 00:39:25.270
there are two arguments.

00:39:25.270 --> 00:39:29.160
The first one, same thing,
is index into the

00:39:29.160 --> 00:39:31.450
array of byte buffers.

00:39:31.450 --> 00:39:33.640
The second is a Boolean value.

00:39:33.640 --> 00:39:37.550
Basically, indicate whether you
want to do rendering for

00:39:37.550 --> 00:39:39.060
this output buffer.

00:39:39.060 --> 00:39:42.900
For video, rendering means you
want to show the video frame.

00:39:42.900 --> 00:39:45.180
For audio, many of you probably
want to send it to

00:39:45.180 --> 00:39:48.410
audio sync for decoding
purpose.

00:39:51.020 --> 00:39:57.140
So for each byte buffer,
regardless whether it's input

00:39:57.140 --> 00:40:02.430
or output, we provide you
a set of metadata

00:40:02.430 --> 00:40:04.120
associated with it.

00:40:04.120 --> 00:40:07.280
If you look at the
dequeueOutputBuffer, the first

00:40:07.280 --> 00:40:12.640
argument is BufferInfo.

00:40:12.640 --> 00:40:18.320
The BufferInfo object tells you
the metadata associated

00:40:18.320 --> 00:40:20.150
with a byte buffer.

00:40:20.150 --> 00:40:25.560
So you can find out information
like what's the

00:40:25.560 --> 00:40:30.830
number bias that this buffer
holds the valid data.

00:40:30.830 --> 00:40:35.840
And what are the starting
position, like the valid data

00:40:35.840 --> 00:40:39.390
begin inside this byte buffer.

00:40:39.390 --> 00:40:45.350
What is the presentation time
for this data inside the byte

00:40:45.350 --> 00:40:50.990
buffer, and also the flags,
such as whether this byte

00:40:50.990 --> 00:40:56.240
buffer holds a sync frame data,
or whether this hold the

00:40:56.240 --> 00:41:00.350
decoder specific information
or codec config flag, or

00:41:00.350 --> 00:41:07.860
whether this actually indicate
is end of stream sample.

00:41:07.860 --> 00:41:12.740
So to show you how to
use the MediaCodec

00:41:12.740 --> 00:41:17.310
API, have two sides.

00:41:17.310 --> 00:41:20.340
The first slide basically shows
you a typical use of the

00:41:20.340 --> 00:41:23.140
low-level MediaCodec APIs.

00:41:23.140 --> 00:41:27.760
So you'll start off by creating
a MediaCodec instance

00:41:27.760 --> 00:41:31.550
by calling one of the three
create methods.

00:41:31.550 --> 00:41:36.780
So in this particular case, you
create a decoding instance

00:41:36.780 --> 00:41:40.450
using the supported MIME type.

00:41:40.450 --> 00:41:44.380
And then, you need to configure
this codec by

00:41:44.380 --> 00:41:47.410
calling config method.

00:41:47.410 --> 00:41:51.050
So the config method, basically,
it needs to know

00:41:51.050 --> 00:41:56.330
the format that this
codec deal with.

00:41:56.330 --> 00:42:00.945
That format can come from your
MediaExtractor, if you provide

00:42:00.945 --> 00:42:05.340
a customized MediaExtractor
implementation, or you use the

00:42:05.340 --> 00:42:09.240
MediaExtractor class we provided
for you, and by

00:42:09.240 --> 00:42:11.220
calling getTrackFormat.

00:42:11.220 --> 00:42:15.000
So that returns the right format
for you, so that your

00:42:15.000 --> 00:42:19.700
codec can deal with that
specific media track.

00:42:19.700 --> 00:42:24.930
So once this MediaCodec is
configured, you can just start

00:42:24.930 --> 00:42:30.240
decoding process by
calling start.

00:42:30.240 --> 00:42:35.170
After you call in start, then it
is time to get the array of

00:42:35.170 --> 00:42:40.700
input byte buffers, and an
array of output buffers.

00:42:40.700 --> 00:42:44.190
And you can also find out
the output format

00:42:44.190 --> 00:42:46.390
for this media track.

00:42:48.960 --> 00:42:50.730
Then, follow that.

00:42:50.730 --> 00:42:53.120
Basically, it's an iterative
process.

00:42:53.120 --> 00:42:57.300
Basically, you just send input
data to the codec to do the

00:42:57.300 --> 00:43:00.330
coding or encoding, and then
you get output from the

00:43:00.330 --> 00:43:02.840
decoder or encoder.

00:43:02.840 --> 00:43:07.670
So that's an iterative process
in the follow up as I've shown

00:43:07.670 --> 00:43:09.020
on the slide.

00:43:09.020 --> 00:43:15.800
So after you are done with this
process, you stop the

00:43:15.800 --> 00:43:20.930
decoding process, and then you
release all the resources

00:43:20.930 --> 00:43:22.290
associated with this
MediaCodec.

00:43:28.600 --> 00:43:33.040
Let me go to the next slide to
show you how to actually send

00:43:33.040 --> 00:43:37.910
input data to the decoder for
decoding, and how do to get

00:43:37.910 --> 00:43:40.710
output data from the decoder.

00:43:40.710 --> 00:43:44.690
So this slide shows
you the details.

00:43:44.690 --> 00:43:49.720
The first part shows you how
to send encode data to the

00:43:49.720 --> 00:43:51.590
decoder instance.

00:43:51.590 --> 00:43:53.845
So firstly, you call
codec.dequeueInputBuffer.

00:43:57.620 --> 00:43:59.670
You check its return value.

00:43:59.670 --> 00:44:02.790
If its return value is
nonactive, meaning greater

00:44:02.790 --> 00:44:10.810
than or equal to zero, then it
is a valid buffer index,

00:44:10.810 --> 00:44:13.780
meaning there is
an input buffer

00:44:13.780 --> 00:44:15.790
available for you to use.

00:44:15.790 --> 00:44:20.240
So at this point, you know which
buffer is available by

00:44:20.240 --> 00:44:25.710
using the index, which you can
find out along with the array

00:44:25.710 --> 00:44:28.570
of input buffers.

00:44:28.570 --> 00:44:31.840
And you can do image processing

00:44:31.840 --> 00:44:33.580
on the input buffer.

00:44:33.580 --> 00:44:37.370
And once you're done with your
image processing, you can call

00:44:37.370 --> 00:44:43.160
queueInputBuffer with the
right buffer index.

00:44:43.160 --> 00:44:47.760
And then, that API will send
the input buffer to the

00:44:47.760 --> 00:44:49.250
decoding instance
for decoding.

00:44:51.840 --> 00:44:55.130
If the return is inactive,
that means it probably is

00:44:55.130 --> 00:44:57.630
timeout, so you need to retry.

00:45:01.360 --> 00:45:06.960
Next, I'll show you how to get
the output from the decoder.

00:45:06.960 --> 00:45:09.190
So you call

00:45:09.190 --> 00:45:14.980
codec.dequeueOutputBuffer with a timeout.

00:45:14.980 --> 00:45:18.830
Same thing it returns you
an integer value.

00:45:18.830 --> 00:45:21.600
If it is a nonactive value--

00:45:21.600 --> 00:45:28.720
meaning, you find, basically,
there is an output buffer

00:45:28.720 --> 00:45:31.340
available, meaning the
decoder already

00:45:31.340 --> 00:45:34.130
generate an output data.

00:45:34.130 --> 00:45:38.100
And it's already stored in one
of the output buffers.

00:45:38.100 --> 00:45:42.610
So how to find out that byte
buffer, you use the return

00:45:42.610 --> 00:45:45.660
value from dequeueOutputBuffer,
use that

00:45:45.660 --> 00:45:52.390
as an index into the array
output byte buffers.

00:45:52.390 --> 00:45:55.760
And of course, you can also
decide to do some image

00:45:55.760 --> 00:45:57.110
processing with it.

00:45:57.110 --> 00:45:59.300
It's up to you.

00:45:59.300 --> 00:46:01.930
After you're done with it, and
then you can release this

00:46:01.930 --> 00:46:08.750
output buffer back to the output
buffer array pool.

00:46:08.750 --> 00:46:11.850
If the return value from
dequeueOutputBuffer is

00:46:11.850 --> 00:46:16.060
negative, there are three
cases I mentioned.

00:46:16.060 --> 00:46:18.220
The slides only show
two cases.

00:46:18.220 --> 00:46:20.040
So the first case is timeout.

00:46:20.040 --> 00:46:22.000
Basically, you can check
whether it's timeout.

00:46:22.000 --> 00:46:27.470
If it's timeout, you can retry
until you find an output

00:46:27.470 --> 00:46:29.390
buffer available.

00:46:29.390 --> 00:46:33.760
Otherwise, if the
return value of

00:46:33.760 --> 00:46:35.010
INFO_OUTPUT_BUFFERS_CHANGED.

00:46:37.270 --> 00:46:39.380
That means, the buffer
size or number of

00:46:39.380 --> 00:46:40.680
buffers may have changed.

00:46:40.680 --> 00:46:47.100
So you need to get a refresh on
the array of byte buffers

00:46:47.100 --> 00:46:48.210
for output.

00:46:48.210 --> 00:46:48.960
So you'll call

00:46:48.960 --> 00:46:52.110
getOutputBuffers for that purpose.

00:46:52.110 --> 00:46:56.710
For the third case, if
the return value is

00:46:56.710 --> 00:47:03.100
INFO_OUTPUT_FORMAT_CHANGED, that
means the decoder will

00:47:03.100 --> 00:47:06.940
find out the codec format
basically changed, or the

00:47:06.940 --> 00:47:13.280
subsequent sample output will
conform to the new format.

00:47:13.280 --> 00:47:16.550
So in order to find out the new
output format, you can use

00:47:16.550 --> 00:47:19.150
getOutputFormat for
that purpose.

00:47:23.980 --> 00:47:32.520
So to wrap up the API for
MediaCodec, I want to mention

00:47:32.520 --> 00:47:34.540
a couple things more.

00:47:34.540 --> 00:47:38.020
First, although the example
is given using a decoding

00:47:38.020 --> 00:47:44.960
instance, most of the APIs
in this MediaCodec class

00:47:44.960 --> 00:47:49.090
basically is the same for
encoding instance and decode

00:47:49.090 --> 00:47:53.170
instance, so that you do not
need to learn two sets of

00:47:53.170 --> 00:47:57.470
different APIs.

00:47:57.470 --> 00:48:02.320
Second thing I want to point
out is if you're using

00:48:02.320 --> 00:48:07.390
MediaCodec API, it is up to
the application to do the

00:48:07.390 --> 00:48:13.120
audio and video synchronization
because we

00:48:13.120 --> 00:48:18.950
return the buffer to tell you
what's the presentation time.

00:48:18.950 --> 00:48:24.620
Now, the application has the
information to do proper audio

00:48:24.620 --> 00:48:27.360
and video synchronization.

00:48:27.360 --> 00:48:32.550
Well, with the release of this
low-level MediaCodec API, now

00:48:32.550 --> 00:48:38.330
it is up to you to write more
sophisticated media

00:48:38.330 --> 00:48:42.355
applications to your
imagination.

00:48:45.780 --> 00:48:50.840
Before I want to open the floor
for questions from the

00:48:50.840 --> 00:48:56.490
audience, I want to thanks
Andreas, in particular, for

00:48:56.490 --> 00:49:01.250
developing the low-level media
APIs for Jellybean release.

00:49:01.250 --> 00:49:05.880
Also, I want to point out, if
you want to find out more

00:49:05.880 --> 00:49:10.820
information about OpenMAX spec,
you can use that URL on

00:49:10.820 --> 00:49:14.090
the Q&amp;A slide.

00:49:14.090 --> 00:49:20.240
Now, let me open floor for
questions from the audience.

00:49:20.240 --> 00:49:24.160
If there is any questions.

00:49:24.160 --> 00:49:25.544
Thank you.

00:49:25.544 --> 00:49:33.128
[APPLAUSE]

00:49:33.128 --> 00:49:34.080
AUDIENCE: Hi.

00:49:34.080 --> 00:49:35.040
Thank you for the
presentation.

00:49:35.040 --> 00:49:36.510
So I have two questions.

00:49:36.510 --> 00:49:39.850
One is, is the actual decoding
and encoding happening inside

00:49:39.850 --> 00:49:42.220
of the media server
process still?

00:49:42.220 --> 00:49:43.030
JAMES DONG: Yes, that's true.

00:49:43.030 --> 00:49:43.800
AUDIENCE: OK.

00:49:43.800 --> 00:49:47.080
And so in terms of battery
consumption, if you were to

00:49:47.080 --> 00:49:50.180
write a media player application
using these APIs,

00:49:50.180 --> 00:49:53.730
versus a natively written one,
how will they compare?

00:49:53.730 --> 00:49:56.620
JAMES DONG: Yeah, that's
a really good question.

00:49:56.620 --> 00:50:02.660
So we did some experiment,
although our test app is not

00:50:02.660 --> 00:50:04.010
instrumented.

00:50:04.010 --> 00:50:09.800
And basically, the usual
findings using the MediaCodec

00:50:09.800 --> 00:50:18.610
API, we find out the total
[INAUDIBLE] consumption for

00:50:18.610 --> 00:50:24.450
the application, basically, is
roughly 10 to 20 medians above

00:50:24.450 --> 00:50:30.250
what do we have using
media player class.

00:50:30.250 --> 00:50:31.390
So a little bit more.

00:50:31.390 --> 00:50:34.150
But I think it's acceptable.

00:50:34.150 --> 00:50:34.820
AUDIENCE: Thanks you.

00:50:34.820 --> 00:50:36.070
JAMES DONG: Sure, thanks.

00:50:38.160 --> 00:50:39.440
AUDIENCE: Thank you for
the presentation.

00:50:39.440 --> 00:50:45.700
I have a question about this
low-level codec APIs.

00:50:45.700 --> 00:50:50.320
So does that mean that I can
write applications to do the

00:50:50.320 --> 00:50:54.970
real-time video chat
with those APIs?

00:50:54.970 --> 00:51:02.090
My understanding is that those
codec will enable leveraging

00:51:02.090 --> 00:51:04.250
the hardware capabilities
through

00:51:04.250 --> 00:51:06.010
OpenMAX protocol, right?

00:51:06.010 --> 00:51:07.340
JAMES DONG: That's true.

00:51:07.340 --> 00:51:12.060
The MediaCodec API actually
allows you to access the

00:51:12.060 --> 00:51:15.410
low-level decoding/encoding
capabilities, regardless

00:51:15.410 --> 00:51:19.390
whether the decoder is a
software decoder or is a

00:51:19.390 --> 00:51:22.150
hardware decoder, or regardless
whether encoder is

00:51:22.150 --> 00:51:26.950
a software encoder or hardware
accelerated encoder.

00:51:26.950 --> 00:51:32.850
So you can leverage the
hardware accelerated

00:51:32.850 --> 00:51:34.350
MediaCodec.

00:51:34.350 --> 00:51:38.400
AUDIENCE: And my understanding
is that prior to the

00:51:38.400 --> 00:51:42.260
Jellybean, those are
not exposed, right?

00:51:42.260 --> 00:51:46.660
At least for encoding for
the hardware capability?

00:51:46.660 --> 00:51:47.140
JAMES DONG: That's true.

00:51:47.140 --> 00:51:50.500
That's why I said, this is a
new set of low-level media

00:51:50.500 --> 00:51:56.910
APIs only available in
Jellybean, and after.

00:51:56.910 --> 00:51:58.030
AUDIENCE: OK.

00:51:58.030 --> 00:51:59.840
Thank you for the answer.

00:51:59.840 --> 00:52:02.200
JAMES DONG: Thank you.

00:52:02.200 --> 00:52:03.780
AUDIENCE: I have
two questions.

00:52:03.780 --> 00:52:08.270
One, is there an x264
hardware-based encoder that

00:52:08.270 --> 00:52:11.800
goes with the 4.1 that is
accessible through the

00:52:11.800 --> 00:52:15.310
low-level APIs?

00:52:15.310 --> 00:52:17.320
JAMES DONG: To answer
your question, yes.

00:52:17.320 --> 00:52:22.060
For example, the current
device, for example,

00:52:22.060 --> 00:52:22.820
[INAUDIBLE]

00:52:22.820 --> 00:52:27.550
and [? Prime ?] all provide you
the hardware accelerated

00:52:27.550 --> 00:52:32.970
AVC decoder, which are able to
decode AVC high profile level

00:52:32.970 --> 00:52:36.250
for videos.

00:52:36.250 --> 00:52:40.540
So really, it depends on
the device capability.

00:52:40.540 --> 00:52:45.850
So if you have a low end device,
you may not be able to

00:52:45.850 --> 00:52:48.980
have the hardware to do that.

00:52:48.980 --> 00:52:52.660
So that's why you need
that capability query

00:52:52.660 --> 00:52:54.520
APIs to find out.

00:52:54.520 --> 00:52:57.290
AUDIENCE: And my second
question, is there an RTSP

00:52:57.290 --> 00:53:05.540
streaming component that could
integrate with the encoders

00:53:05.540 --> 00:53:08.590
that work with the the
new media APIs?

00:53:08.590 --> 00:53:10.690
JAMES DONG: RTSP streaming--

00:53:10.690 --> 00:53:11.230
AUDIENCE: Server.

00:53:11.230 --> 00:53:11.970
JAMES DONG: Server.

00:53:11.970 --> 00:53:15.150
Yeah, server we do not have.

00:53:15.150 --> 00:53:17.200
We have a client
implementation.

00:53:17.200 --> 00:53:18.520
AUDIENCE: OK, thank you.

00:53:18.520 --> 00:53:21.200
JAMES DONG: Thanks.

00:53:21.200 --> 00:53:22.130
AUDIENCE: Two questions.

00:53:22.130 --> 00:53:24.540
First of all, with the release
of Jellybean, are you adding

00:53:24.540 --> 00:53:29.580
any extra codecs that kind of
implement these APIs that

00:53:29.580 --> 00:53:32.880
would serve as good examples?

00:53:32.880 --> 00:53:35.050
JAMES DONG: Your question
basically asks whether we have

00:53:35.050 --> 00:53:38.710
additional Media Codecs
to support the--

00:53:38.710 --> 00:53:40.740
AUDIENCE: That were added
in Jellybean versus ICS?

00:53:40.740 --> 00:53:42.280
JAMES DONG: Added
in Jellybean.

00:53:42.280 --> 00:53:47.610
Well, this really depends
on the device you have.

00:53:47.610 --> 00:53:53.080
But the commonly available
codecs are basically for

00:53:53.080 --> 00:53:57.450
video, you have AVC decoder, AVC
encoder, you have impact

00:53:57.450 --> 00:54:02.980
for video encoder/decoder, you
have H.263 encoder/decoder.

00:54:02.980 --> 00:54:06.000
You would expect it
to have a commonly

00:54:06.000 --> 00:54:07.650
available Android device.

00:54:07.650 --> 00:54:11.860
For audio, you have a bunch
of format to support

00:54:11.860 --> 00:54:14.180
a decoder for MP3.

00:54:14.180 --> 00:54:22.160
For encoder, you have AAC,
AMR Narrowband/Wideband.

00:54:22.160 --> 00:54:23.430
So all of those are supported.

00:54:23.430 --> 00:54:25.150
AUDIENCE: OK.

00:54:25.150 --> 00:54:30.800
Second, you kind of mentioned
the MediaCrypto, I believe it

00:54:30.800 --> 00:54:36.010
was, for the actual DRM, I
would assume, production.

00:54:36.010 --> 00:54:37.820
Can you go into a little more
detail on what would be

00:54:37.820 --> 00:54:40.140
involved in using that
in your app?

00:54:40.140 --> 00:54:40.970
JAMES DONG: Yeah.

00:54:40.970 --> 00:54:44.330
That, unfortunately, is out
of the scope of this

00:54:44.330 --> 00:54:49.710
presentation, which is
another talk itself.

00:54:49.710 --> 00:54:53.590
But basically, we publish
a new API, called

00:54:53.590 --> 00:54:56.700
a MediaCrypto class.

00:54:56.700 --> 00:55:03.040
You can do decoding of
encrypted content.

00:55:03.040 --> 00:55:06.510
So currently, we implemented
this and verified with

00:55:06.510 --> 00:55:12.296
Widevine DRM, Widevine
encrypted content.

00:55:12.296 --> 00:55:14.160
AUDIENCE: OK.

00:55:14.160 --> 00:55:16.386
JAMES DONG: Does that answer
your question?

00:55:16.386 --> 00:55:19.410
Yeah, otherwise, like I say,
is another long talk,

00:55:19.410 --> 00:55:21.780
basically, about MediaCrypto
API.

00:55:21.780 --> 00:55:24.750
So once the API is public,
you can look at it.

00:55:24.750 --> 00:55:27.490
It's pretty simple also.

00:55:27.490 --> 00:55:29.130
AUDIENCE: The API's
are going to be--

00:55:29.130 --> 00:55:31.780
JAMES DONG: For MediaCrypto.

00:55:31.780 --> 00:55:33.910
AUDIENCE: When is the actual
documentation of that going to

00:55:33.910 --> 00:55:35.850
be totally available?

00:55:35.850 --> 00:55:38.560
JAMES DONG: It's going to
be released with SDK.

00:55:38.560 --> 00:55:39.440
AUDIENCE: OK, great.

00:55:39.440 --> 00:55:40.320
Thanks.

00:55:40.320 --> 00:55:41.600
JAMES DONG: OK, thank you.

00:55:41.600 --> 00:55:43.490
AUDIENCE: So you mentioned
in the configuration

00:55:43.490 --> 00:55:45.150
that it uses a surface.

00:55:45.150 --> 00:55:48.010
What does it use that
for, that value?

00:55:48.010 --> 00:55:48.790
JAMES DONG: OK.

00:55:48.790 --> 00:55:52.340
The second argument for
configure method is surface.

00:55:52.340 --> 00:55:59.330
So if you want to render your
video output, for example,

00:55:59.330 --> 00:56:03.030
then you want to pass a surface,
so that it can render

00:56:03.030 --> 00:56:04.690
that video output.

00:56:04.690 --> 00:56:06.265
That's for that purpose.

00:56:06.265 --> 00:56:06.630
AUDIENCE: OK.

00:56:06.630 --> 00:56:10.610
And how does that work with
the syncing of the audio?

00:56:10.610 --> 00:56:13.200
Would you render the movie, and
then have another thread

00:56:13.200 --> 00:56:15.190
that's parsing your audio?

00:56:15.190 --> 00:56:18.440
JAMES DONG: Yeah, it's up to
the application, basically.

00:56:18.440 --> 00:56:22.390
You probably need to write
a multiple threads.

00:56:22.390 --> 00:56:24.840
One thread is handling
the video.

00:56:24.840 --> 00:56:26.620
The other thread is handling
the audio.

00:56:26.620 --> 00:56:29.210
But it's up to the application
to do that.

00:56:29.210 --> 00:56:30.130
AUDIENCE: OK, so you
just passed the

00:56:30.130 --> 00:56:32.100
surface for us to use?

00:56:32.100 --> 00:56:32.530
JAMES DONG: Yes.

00:56:32.530 --> 00:56:35.410
We just passed the surface
as the video sync.

00:56:35.410 --> 00:56:36.365
AUDIENCE: All right,
thank you.

00:56:36.365 --> 00:56:38.380
JAMES DONG: Yeah, thank you.

00:56:38.380 --> 00:56:39.030
AUDIENCE: Hi.

00:56:39.030 --> 00:56:41.750
In Ice Cream Sandwich, you
released a new low-level

00:56:41.750 --> 00:56:43.660
streaming API.

00:56:43.660 --> 00:56:49.760
Will Jellybean build upon,
or replace that one?

00:56:49.760 --> 00:56:51.565
JAMES DONG: Sorry, let
me turn this one off.

00:56:57.580 --> 00:56:58.740
Sorry, what was your
question again?

00:56:58.740 --> 00:57:01.160
AUDIENCE: In Ice Cream Sandwich,
you released the new

00:57:01.160 --> 00:57:04.510
OpenMAX AL streaming
low-level library.

00:57:04.510 --> 00:57:08.850
Will Jellybean replace
or build upon that?

00:57:08.850 --> 00:57:10.820
JAMES DONG: Well, it's used for

00:57:10.820 --> 00:57:12.070
different purpose, basically.

00:57:16.400 --> 00:57:18.600
It's used for a different
purpose.

00:57:18.600 --> 00:57:24.220
And I think from Jellybean and
on, we should focus on

00:57:24.220 --> 00:57:26.460
MediaCodec API.

00:57:26.460 --> 00:57:29.050
AUDIENCE: Will the ICS one
still be supported then?

00:57:29.050 --> 00:57:31.320
JAMES DONG: It will be
still supported.

00:57:31.320 --> 00:57:32.090
AUDIENCE: OK.

00:57:32.090 --> 00:57:33.820
And then, you mentioned
MediaCrypto.

00:57:33.820 --> 00:57:38.420
Is there a way to protect the
decrypted yet still encoded

00:57:38.420 --> 00:57:40.470
data using MediaCrypto?

00:57:40.470 --> 00:57:41.370
JAMES DONG: Yes.

00:57:41.370 --> 00:57:45.240
For encrypted counting actually,
you are not able to

00:57:45.240 --> 00:57:50.660
do actually access the
output byte buffer.

00:57:50.660 --> 00:57:53.740
So it's opaque data,
basically, to

00:57:53.740 --> 00:57:56.900
the application process.

00:57:56.900 --> 00:58:02.320
So if you have hardware assisted
protection for your

00:58:02.320 --> 00:58:06.340
video path, then the
output buffer

00:58:06.340 --> 00:58:08.700
actually is hardware protected.

00:58:08.700 --> 00:58:09.680
AUDIENCE: Does it just
support Widevine?

00:58:09.680 --> 00:58:12.290
Or do custom DRMs support
it, too, then?

00:58:12.290 --> 00:58:13.900
JAMES DONG: Currently, we
only have verified the

00:58:13.900 --> 00:58:15.690
implementation with Widevine.

00:58:15.690 --> 00:58:18.670
But I can imagine you're
also able to

00:58:18.670 --> 00:58:21.250
work with other schemes.

00:58:21.250 --> 00:58:22.600
AUDIENCE: OK, thank you.

00:58:22.600 --> 00:58:23.850
JAMES DONG: Thanks.

00:58:28.860 --> 00:58:33.340
AUDIENCE: My first question is,
is there any constraint

00:58:33.340 --> 00:58:37.380
for threading access
of these buffers?

00:58:37.380 --> 00:58:42.650
Say, usually we use different
thread for correct data, and

00:58:42.650 --> 00:58:46.560
then maybe another
to call a codec.

00:58:46.560 --> 00:58:51.660
So is there any constraint on
thread part for accessing the

00:58:51.660 --> 00:58:53.820
input buffer or output buffer?

00:58:56.610 --> 00:58:59.230
JAMES DONG: Sorry,
can you rephrase?

00:58:59.230 --> 00:59:00.460
AUDIENCE: Oh, and then
make it simple.

00:59:00.460 --> 00:59:05.980
When I access the input buffer
and output buffer, can I use

00:59:05.980 --> 00:59:07.210
[INAUDIBLE]

00:59:07.210 --> 00:59:07.750
thread?

00:59:07.750 --> 00:59:10.320
Is there any constraint
for it?

00:59:10.320 --> 00:59:11.630
JAMES DONG: No, there
shouldn't be.

00:59:11.630 --> 00:59:15.790
As long as you have a valid
input buffer or output buffer,

00:59:15.790 --> 00:59:18.280
you can use it for
your own purpose.

00:59:18.280 --> 00:59:19.930
AUDIENCE: OK.

00:59:19.930 --> 00:59:22.410
My second question is,
you mentioned about

00:59:22.410 --> 00:59:25.860
the extractor decoder.

00:59:25.860 --> 00:59:29.800
Is there any way to get to the
hardware color conversion,

00:59:29.800 --> 00:59:32.630
like YUV to RGB?

00:59:32.630 --> 00:59:32.860
JAMES DONG: No.

00:59:32.860 --> 00:59:35.720
There is no API we're published
in Jellybean for

00:59:35.720 --> 00:59:39.150
that purpose, for color
conversion.

00:59:39.150 --> 00:59:44.820
AUDIENCE: And you mentioned the
codec information can tell

00:59:44.820 --> 00:59:47.540
profile and the level
supported.

00:59:47.540 --> 00:59:51.960
And is there any way to know
the profile or level of the

00:59:51.960 --> 00:59:53.210
media source?

00:59:56.710 --> 00:59:59.670
JAMES DONG: The profile and
the levels for the media

00:59:59.670 --> 01:00:03.340
source, no.

01:00:03.340 --> 01:00:06.100
Currently, we are not
supporting that.

01:00:06.100 --> 01:00:08.930
So as an application developer,
you need to know

01:00:08.930 --> 01:00:11.080
the source, basically.

01:00:11.080 --> 01:00:12.250
AUDIENCE: Maybe not.

01:00:12.250 --> 01:00:16.440
JAMES DONG: If you do not,
then the codec API will

01:00:16.440 --> 01:00:22.140
indicate it's not able to handle
if the codec is not

01:00:22.140 --> 01:00:23.190
available to handle.

01:00:23.190 --> 01:00:24.680
AUDIENCE: I haven't
read the document.

01:00:24.680 --> 01:00:27.700
Is there any [INAUDIBLE]
callback since?

01:00:33.670 --> 01:00:38.010
JAMES DONG: Well, for example,
the setDataSource and

01:00:38.010 --> 01:00:44.660
getTrackCount, you may not get
right number if the file

01:00:44.660 --> 01:00:46.500
format is not supported,
for example.

01:00:46.500 --> 01:00:49.330
AUDIENCE: Say for the audio is
supported, the video is not

01:00:49.330 --> 01:00:53.500
supported, so maybe check a
count where there will be one

01:00:53.500 --> 01:00:54.770
instead of two?

01:00:54.770 --> 01:00:56.520
JAMES DONG: Yes.

01:00:56.520 --> 01:00:57.710
AUDIENCE: OK.

01:00:57.710 --> 01:01:02.170
And my last minor question is,
in your code showing on the

01:01:02.170 --> 01:01:07.900
screen, you used two equal to
compare the codec MIME type.

01:01:07.900 --> 01:01:10.060
Is it safe?

01:01:10.060 --> 01:01:11.510
JAMES DONG: Two equal, sorry?

01:01:11.510 --> 01:01:12.780
AUDIENCE: Two equal signal.

01:01:12.780 --> 01:01:16.080
Instead of caller stream
equals two.

01:01:16.080 --> 01:01:18.520
JAMES DONG: Oh Yeah, that's
probably my--

01:01:18.520 --> 01:01:22.060
Yeah, but basically just compel
the MIME type against

01:01:22.060 --> 01:01:25.940
the supported MIME type.

01:01:25.940 --> 01:01:26.400
AUDIENCE: OK, thank you.

01:01:26.400 --> 01:01:27.650
JAMES DONG: Sure.

01:01:30.730 --> 01:01:30.940
AUDIENCE: Hello.

01:01:30.940 --> 01:01:33.670
You haven't mentioned the
Android indicator.

01:01:33.670 --> 01:01:36.610
So are the same set of API
changes also available on the

01:01:36.610 --> 01:01:38.470
native side, like from
the NDK side?

01:01:38.470 --> 01:01:38.920
JAMES DONG: No.

01:01:38.920 --> 01:01:39.350
AUDIENCE: No?

01:01:39.350 --> 01:01:40.650
JAMES DONG: This is
all Java API.

01:01:40.650 --> 01:01:41.050
AUDIENCE: OK.

01:01:41.050 --> 01:01:44.210
And have there been any changes
to NDK exposed APIs at

01:01:44.210 --> 01:01:46.980
all for Jellybean?

01:01:46.980 --> 01:01:49.380
JAMES DONG: Not as
far as I know.

01:01:49.380 --> 01:01:54.650
These are just Java APIs,
not related to NDK.

01:01:54.650 --> 01:01:56.500
AUDIENCE: From the looks of your
sample code, it's really

01:01:56.500 --> 01:01:58.420
just a thin wrapper around
some native code?

01:01:58.420 --> 01:01:59.350
Like, most of the codes.

01:01:59.350 --> 01:02:02.400
JAMES DONG: Yes, those will be
available at the native layer,

01:02:02.400 --> 01:02:04.380
but it's not part of NDK.

01:02:04.380 --> 01:02:06.560
AUDIENCE: So you cannot actually
call these from--

01:02:06.560 --> 01:02:09.490
JAMES DONG: No, we do not
recommend you to call directly

01:02:09.490 --> 01:02:11.120
into the native code.

01:02:11.120 --> 01:02:13.340
AUDIENCE: Because some audio
APIs are actually

01:02:13.340 --> 01:02:14.920
exposed in the NDK?

01:02:14.920 --> 01:02:18.920
JAMES DONG: Yeah, if it is
exposed in the NDK, you have

01:02:18.920 --> 01:02:21.510
the guarantee, like
we intended.

01:02:21.510 --> 01:02:27.840
But if it's not a part of the
NDK, then we may change it.

01:02:27.840 --> 01:02:29.620
AUDIENCE: If you want to write
some really low latency code,

01:02:29.620 --> 01:02:32.390
for example, and like you're
calling native code anyway.

01:02:32.390 --> 01:02:35.770
So it kind of makes sense
to actually expose

01:02:35.770 --> 01:02:38.780
these codecs natively.

01:02:38.780 --> 01:02:41.170
JAMES DONG: But for Jellybean
release, we are

01:02:41.170 --> 01:02:41.955
not exposing that.

01:02:41.955 --> 01:02:42.270
AUDIENCE: OK, Thank you.

01:02:42.270 --> 01:02:44.210
JAMES DONG: Yeah, thanks.

01:02:44.210 --> 01:02:44.570
AUDIENCE: Hi.

01:02:44.570 --> 01:02:47.640
So you mentioned about we need
to do the YUV to RGB

01:02:47.640 --> 01:02:48.600
conversion.

01:02:48.600 --> 01:02:51.290
So can the surface be backward
surface texture?

01:02:51.290 --> 01:02:55.130
In that case, will we get
something as a YUV texture,

01:02:55.130 --> 01:02:56.700
and we can use [INAUDIBLE]

01:02:56.700 --> 01:02:59.280
to do the color conversion?

01:02:59.280 --> 01:03:02.050
JAMES DONG: Sorry, I haven't
mentioned that.

01:03:02.050 --> 01:03:05.915
But it's up to you to do
the image processing.

01:03:05.915 --> 01:03:09.230
It's not necessary you
do color conversion.

01:03:09.230 --> 01:03:14.830
OK, so the media API basically
gives you the raw data, like

01:03:14.830 --> 01:03:19.390
the output of uncompressed
video frame, for example.

01:03:19.390 --> 01:03:21.730
You can do image processing
on it and--

01:03:21.730 --> 01:03:22.520
AUDIENCE: It is in what format?

01:03:22.520 --> 01:03:23.700
Is it RGB?

01:03:23.700 --> 01:03:25.220
JAMES DONG: Well, the formatter
will tell you

01:03:25.220 --> 01:03:27.270
basically, you can call
getOutputFormat.

01:03:27.270 --> 01:03:29.330
You can find out what's
the format there.

01:03:29.330 --> 01:03:32.130
AUDIENCE: And it is expected
to be in an RGB format, and

01:03:32.130 --> 01:03:34.203
not an intermediate
YUV format, is it?

01:03:37.310 --> 01:03:38.740
JAMES DONG: No, it's
a decoder output.

01:03:38.740 --> 01:03:43.350
So mostly, it is YUV
output format.

01:03:43.350 --> 01:03:45.870
AUDIENCE: So that means we need
to the color conversion.

01:03:45.870 --> 01:03:48.470
JAMES DONG: Well, it depends
on how you do your image

01:03:48.470 --> 01:03:49.770
processing, yes.

01:03:49.770 --> 01:03:51.055
AUDIENCE: So if you don't image
processing, and will

01:03:51.055 --> 01:03:54.790
just say boolean render to be
true, the underlying platform

01:03:54.790 --> 01:03:57.560
will do the color conversion and
render it to the surface?

01:03:57.560 --> 01:03:58.490
JAMES DONG: Yes.

01:03:58.490 --> 01:04:00.580
JAMES DONG: And are these
buffers getting

01:04:00.580 --> 01:04:01.400
copied in any way?

01:04:01.400 --> 01:04:05.420
Like, the decoder could use the
decoder resources to push

01:04:05.420 --> 01:04:05.940
the decoded buffers.

01:04:05.940 --> 01:04:11.770
Is it getting copied from the
GPU memory to CPU memory?

01:04:11.770 --> 01:04:15.370
JAMES DONG: Well, we try to
avoid [INAUDIBLE] copies as

01:04:15.370 --> 01:04:16.150
much as we can.

01:04:16.150 --> 01:04:19.850
And as far as a node, there
is no copy at all

01:04:19.850 --> 01:04:21.100
at the media level.

01:04:21.100 --> 01:04:24.510
And the only traffic
communication it added for

01:04:24.510 --> 01:04:28.820
this media API is the indices
of these input buffer and

01:04:28.820 --> 01:04:30.330
output buffer arrays.

01:04:30.330 --> 01:04:36.170
Basically, that's the added
communication traffic between

01:04:36.170 --> 01:04:38.980
the application and the
media server process.

01:04:38.980 --> 01:04:42.200
AUDIENCE: But then, if the
decoder is using GPU memory,

01:04:42.200 --> 01:04:45.190
and if it has to be passed as
a [INAUDIBLE] into Java,

01:04:45.190 --> 01:04:48.120
wouldn't it have to be a
non-GPU memory for the

01:04:48.120 --> 01:04:51.650
application to do any kind
of post-processing?

01:04:51.650 --> 01:04:55.670
JAMES DONG: Well, the byte
buffer array is basically just

01:04:55.670 --> 01:04:57.600
a wrap around of the
native buffers.

01:04:57.600 --> 01:05:04.600
It may not necessarily say a
specific format for that.

01:05:04.600 --> 01:05:05.980
AUDIENCE: So it could
be textures, too?

01:05:05.980 --> 01:05:06.890
JAMES DONG: It could be.

01:05:06.890 --> 01:05:07.670
AUDIENCE: OK.

01:05:07.670 --> 01:05:09.260
And how do we know if it's
a texture or not?

01:05:09.260 --> 01:05:11.920
JAMES DONG: You don't
know at this point.

01:05:11.920 --> 01:05:14.030
There is no API that
tells you about it.

01:05:14.030 --> 01:05:15.640
AUDIENCE: But if you have to
post-processing, we should

01:05:15.640 --> 01:05:17.470
know the [INAUDIBLE]

01:05:17.470 --> 01:05:20.120
inside the byte buffer in order
to read those datas?

01:05:20.120 --> 01:05:22.670
JAMES DONG: Well, the data
itself, actually, is arranged

01:05:22.670 --> 01:05:25.420
in a way that you
can access it.

01:05:25.420 --> 01:05:31.390
That's basically, find out the
output, the YUV format, and

01:05:31.390 --> 01:05:32.610
the size of it.

01:05:32.610 --> 01:05:33.480
AUDIENCE: I see.

01:05:33.480 --> 01:05:36.190
And the encoded data is passed
as like elementary streams?

01:05:36.190 --> 01:05:38.490
JAMES DONG: Sorry, I think we
are running out of time.

01:05:38.490 --> 01:05:43.480
And we can take this offline.

01:05:43.480 --> 01:05:45.200
All right?

01:05:45.200 --> 01:05:46.180
OK.

01:05:46.180 --> 01:05:47.430
Thank you.

