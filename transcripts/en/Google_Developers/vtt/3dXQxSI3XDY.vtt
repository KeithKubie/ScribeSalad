WEBVTT
Kind: captions
Language: en

00:00:01.362 --> 00:00:02.270
Cool.

00:00:02.270 --> 00:00:04.910
Hello, everyone.

00:00:04.910 --> 00:00:05.890
Hey.

00:00:05.890 --> 00:00:06.612
Good morning.

00:00:06.612 --> 00:00:07.570
Thanks for coming here.

00:00:07.570 --> 00:00:10.000
It's probably been a long week.

00:00:10.000 --> 00:00:11.960
The last couple of days
have been crazy here,

00:00:11.960 --> 00:00:13.980
and the traffic's
not been fun either.

00:00:13.980 --> 00:00:16.630
So hopefully it's been
working all well for you guys.

00:00:16.630 --> 00:00:20.240
Some of you have made your
way from a different parts

00:00:20.240 --> 00:00:22.190
of the country, many
different areas.

00:00:22.190 --> 00:00:24.740
But I hope it's been worth it.

00:00:24.740 --> 00:00:28.890
And there's a lot of interesting
things that we've been doing.

00:00:28.890 --> 00:00:31.390
So we want to talk a little
bit about some of them.

00:00:31.390 --> 00:00:34.090
You've heard our keynotes and
the great products we have,

00:00:34.090 --> 00:00:35.150
et cetera.

00:00:35.150 --> 00:00:37.060
Today I'm going to talk a little
bit about machine learning,

00:00:37.060 --> 00:00:37.560
right?

00:00:37.560 --> 00:00:39.230
The title says machine
learning is not

00:00:39.230 --> 00:00:40.919
the future, which
is kind of weird

00:00:40.919 --> 00:00:43.460
given that we're talking about
machine learning in everything

00:00:43.460 --> 00:00:45.221
we're doing here.

00:00:45.221 --> 00:00:47.220
But the point that I
wanted to bring across here

00:00:47.220 --> 00:00:48.970
is it's not really the future.

00:00:48.970 --> 00:00:51.010
It's part of everything
we have today

00:00:51.010 --> 00:00:53.180
as you're seeing without
products as all the things

00:00:53.180 --> 00:00:54.221
that we're talking about.

00:00:54.221 --> 00:00:55.950
We're using machine
learning today

00:00:55.950 --> 00:00:57.289
in everything we're doing.

00:00:57.289 --> 00:00:58.830
And I want to talk
about some of that

00:00:58.830 --> 00:01:01.240
and tell you how
you can do that too.

00:01:01.240 --> 00:01:04.625
But first, let's start
with this all started.

00:01:04.625 --> 00:01:07.760
So and this goes back to 1997.

00:01:07.760 --> 00:01:10.060
I don't know how many of
you remember this Tik-Tok

00:01:10.060 --> 00:01:11.840
from "The Wizard of Oz."

00:01:11.840 --> 00:01:15.880
He's one of the early versions
of robots in modern times,

00:01:15.880 --> 00:01:16.380
really.

00:01:16.380 --> 00:01:19.230
Although there are references
to automatons going back

00:01:19.230 --> 00:01:21.160
to the 5th Century
B.C. And stuff,

00:01:21.160 --> 00:01:24.462
even the Egyptians and
others way back when.

00:01:24.462 --> 00:01:26.170
But in the modern
times, this is probably

00:01:26.170 --> 00:01:28.030
one of the earliest
references to something

00:01:28.030 --> 00:01:29.960
that was close to a robot.

00:01:29.960 --> 00:01:31.500
Any of you familiar
with Tik-Tok?

00:01:31.500 --> 00:01:34.360
How many of you are
familiar with Tik-Tok?

00:01:34.360 --> 00:01:36.260
Not too many.

00:01:36.260 --> 00:01:38.150
Let me tell you a
little bit about him.

00:01:38.150 --> 00:01:41.870
So this one is-- in
fact, the term robot

00:01:41.870 --> 00:01:44.290
wasn't even coined
back when Tik-Tok

00:01:44.290 --> 00:01:47.000
was created by Frank Baum.

00:01:47.000 --> 00:01:53.186
And what this robot would do
is it had a winding thing.

00:01:53.186 --> 00:01:54.560
And you needed to
wind the robot.

00:01:54.560 --> 00:01:55.610
And it would run.

00:01:55.610 --> 00:01:59.640
And it could do pretty much
what a human could do but not

00:01:59.640 --> 00:02:01.070
really be alive.

00:02:01.070 --> 00:02:03.340
And it was a great
reference to things

00:02:03.340 --> 00:02:05.780
that we've always wanted to do.

00:02:05.780 --> 00:02:09.400
And so a lot of
this AI and robotics

00:02:09.400 --> 00:02:13.120
and a lot of these things have
been part of science fiction

00:02:13.120 --> 00:02:14.830
for a very, very long time.

00:02:14.830 --> 00:02:17.510
In fact, a lot of what
we do today in science

00:02:17.510 --> 00:02:22.720
has been driven by people,
literally authors, et cetera,

00:02:22.720 --> 00:02:25.570
are able to really build
up and think about what

00:02:25.570 --> 00:02:28.135
the future might be like.

00:02:28.135 --> 00:02:30.260
This is one example, but
there are many, many more.

00:02:30.260 --> 00:02:33.620
For example, if some of
you have read Asimov,

00:02:33.620 --> 00:02:34.870
he has a bunch of books.

00:02:34.870 --> 00:02:37.510
And he has this whole
robotics cities,

00:02:37.510 --> 00:02:40.580
again way back in the '30s
where he talks about robots

00:02:40.580 --> 00:02:42.320
and how they might
be like and the kind

00:02:42.320 --> 00:02:44.120
of things that come with it.

00:02:44.120 --> 00:02:46.180
And that was very interesting.

00:02:46.180 --> 00:02:47.790
A lot of people
over the years have

00:02:47.790 --> 00:02:51.600
been inspired by these science
fiction books and movies

00:02:51.600 --> 00:02:54.770
to do a lot of
interesting things.

00:02:54.770 --> 00:02:57.250
Another example of that
is back in the '70s

00:02:57.250 --> 00:02:59.770
from "The Hitchhikers
Guide to the Galaxy"

00:02:59.770 --> 00:03:02.730
there is this robot
called Marvin,

00:03:02.730 --> 00:03:06.955
which is this depressed--
it's always depressing.

00:03:06.955 --> 00:03:09.800
He's always talking about these
things he's not happy about.

00:03:09.800 --> 00:03:13.040
He's just too smart for all
the things happening it.

00:03:13.040 --> 00:03:17.327
And it's really a
great example of what

00:03:17.327 --> 00:03:18.910
people have been
thinking about what's

00:03:18.910 --> 00:03:21.540
going to happen in the future.

00:03:21.540 --> 00:03:23.210
And now going to
another example,

00:03:23.210 --> 00:03:25.180
"Star Trek," another thing.

00:03:25.180 --> 00:03:29.050
The example that I
put here, you can

00:03:29.050 --> 00:03:32.290
go to Data in "Star Trek"
who talks about-- this

00:03:32.290 --> 00:03:37.070
is a poem he composes, which
I thought was pretty funny,

00:03:37.070 --> 00:03:40.530
and how people have
been thinking about AIs

00:03:40.530 --> 00:03:42.192
and what they would
do and what they

00:03:42.192 --> 00:03:43.400
would be doing in the future.

00:03:43.400 --> 00:03:46.300
In this case, again,
it's like a robot,

00:03:46.300 --> 00:03:49.620
like a humanoid, somebody
who can do a lot of things,

00:03:49.620 --> 00:03:52.420
but is not quite human.

00:03:52.420 --> 00:03:58.280
Then coming to more recent
times, more closer to 2001,

00:03:58.280 --> 00:04:01.391
if you've seen Steven
Spielberg's movie "AI" called

00:04:01.391 --> 00:04:04.690
"AI" itself, they have
these mechanical robots

00:04:04.690 --> 00:04:06.690
that can really do-- that
look like humans, that

00:04:06.690 --> 00:04:07.814
can do all kinds of things.

00:04:07.814 --> 00:04:08.930
But they don't feel.

00:04:08.930 --> 00:04:12.590
And then they build
this little kid David,

00:04:12.590 --> 00:04:15.120
who can actually feel
and love as well,

00:04:15.120 --> 00:04:16.980
and really changes
how the perception is

00:04:16.980 --> 00:04:21.320
and how they think about
AI or what it means

00:04:21.320 --> 00:04:23.232
to the people around them.

00:04:23.232 --> 00:04:24.690
And then much more
recently there's

00:04:24.690 --> 00:04:27.940
this movie called
"Her" in 2013, which

00:04:27.940 --> 00:04:31.690
talks about AI without the
shape, without the robotics,

00:04:31.690 --> 00:04:32.700
and all of that.

00:04:32.700 --> 00:04:36.820
But AI is just that live
within your computer as an OS,

00:04:36.820 --> 00:04:38.900
essentially, that can
interact with you.

00:04:38.900 --> 00:04:42.480
That's your assistant,
but much more than that.

00:04:42.480 --> 00:04:43.600
It actually feels as well.

00:04:43.600 --> 00:04:45.860
It understands
things, et cetera.

00:04:45.860 --> 00:04:47.760
So all these are great.

00:04:47.760 --> 00:04:49.270
There are so many
different things

00:04:49.270 --> 00:04:50.770
that people have
been talking about,

00:04:50.770 --> 00:04:52.500
people have been thinking about.

00:04:52.500 --> 00:04:55.080
But these are still
science fiction.

00:04:55.080 --> 00:04:58.340
This is not really what machine
learning is about today.

00:04:58.340 --> 00:05:00.930
This has always been the future.

00:05:00.930 --> 00:05:02.252
It's still the future.

00:05:02.252 --> 00:05:03.960
Maybe at some point
it will be a reality.

00:05:03.960 --> 00:05:05.490
But that's not
where we are today.

00:05:05.490 --> 00:05:07.410
That's not what
we're talking about.

00:05:07.410 --> 00:05:09.742
But there are some real
things that we can do today.

00:05:09.742 --> 00:05:11.450
And those are some of
the things that I'm

00:05:11.450 --> 00:05:13.870
going to talk about today.

00:05:13.870 --> 00:05:17.180
So we've actually made
over the last few years

00:05:17.180 --> 00:05:19.790
real progress in terms of all
the different things that we

00:05:19.790 --> 00:05:22.655
can do with AI with
machine learning,

00:05:22.655 --> 00:05:24.530
use all the products
that you see around you.

00:05:24.530 --> 00:05:26.820
And I'm going to talk a
little bit about those.

00:05:26.820 --> 00:05:28.860
There's so much
in there that has

00:05:28.860 --> 00:05:33.080
benefited from machine learning
from what we call AI as well.

00:05:33.080 --> 00:05:38.200
And so this is some of the
smallest of products, really,

00:05:38.200 --> 00:05:41.790
at Google that we made that used
machine learning in some ways.

00:05:41.790 --> 00:05:44.312
But at Google, anytime
we think of a product

00:05:44.312 --> 00:05:45.645
there's, of course, programming.

00:05:45.645 --> 00:05:48.910
And you build it, and you do
all sorts of things with it.

00:05:48.910 --> 00:05:51.880
But machine learning is an
integral part of everything

00:05:51.880 --> 00:05:54.980
we do in building that, because
we want these products to be

00:05:54.980 --> 00:05:58.030
really smart to give
you the right things,

00:05:58.030 --> 00:06:00.540
to not just follow your
actions, but really

00:06:00.540 --> 00:06:03.820
give you the right things when
you want them as you want them.

00:06:03.820 --> 00:06:07.300
And I'll go over some examples
of these in later slides

00:06:07.300 --> 00:06:09.430
as well.

00:06:09.430 --> 00:06:12.650
So before I go
into other things,

00:06:12.650 --> 00:06:15.150
let's just go a little bit
into what deep learning

00:06:15.150 --> 00:06:17.690
and machine learning is about.

00:06:17.690 --> 00:06:21.695
And I'm just quickly going
to give you some examples

00:06:21.695 --> 00:06:24.230
on a website that we have.

00:06:24.230 --> 00:06:28.610
So part of this slide is going
to talk about TensorFlow.

00:06:28.610 --> 00:06:31.890
And recently we put
the site up called

00:06:31.890 --> 00:06:34.340
playground.tensorflow.org
that allows

00:06:34.340 --> 00:06:37.120
you to play with neural nets
that allows you to really do

00:06:37.120 --> 00:06:43.390
different kinds of things, and
allows you to really understand

00:06:43.390 --> 00:06:45.860
how these networks work,
how machine learning works,

00:06:45.860 --> 00:06:48.360
and be able to play with
some of those problems.

00:06:48.360 --> 00:06:50.840
So I'm going to start with
a very, very basic problem

00:06:50.840 --> 00:06:51.810
classification.

00:06:51.810 --> 00:06:54.430
The goal in this case is there
are two kinds of clients.

00:06:54.430 --> 00:06:57.160
I just want to classify
that it's ARB, in this case

00:06:57.160 --> 00:06:59.540
the orange or the blue ones.

00:06:59.540 --> 00:07:01.480
And I'm going to use
the very simplest case.

00:07:01.480 --> 00:07:04.320
It's a very simple
linear classification,

00:07:04.320 --> 00:07:06.210
if any of you guys
know what that is.

00:07:06.210 --> 00:07:08.030
But the idea is you
have some inputs.

00:07:08.030 --> 00:07:11.250
In this case, the x-axis
and the y-axis are inputs.

00:07:11.250 --> 00:07:13.020
And based on those
two inputs you

00:07:13.020 --> 00:07:17.240
want to decide if it's a
blue one or an orange one.

00:07:17.240 --> 00:07:20.030
And so what this model
is going to learn

00:07:20.030 --> 00:07:21.800
is some parameters
to figure out-- OK.

00:07:21.800 --> 00:07:25.040
If I get some point, I get the
an x1 value and a x2 value,

00:07:25.040 --> 00:07:26.240
how do I decide that?

00:07:26.240 --> 00:07:27.870
It's some mathematical
computation

00:07:27.870 --> 00:07:28.980
that it needs to do.

00:07:28.980 --> 00:07:31.120
So in this case, it
does that iteratively.

00:07:31.120 --> 00:07:32.420
Let's just quickly run this.

00:07:32.420 --> 00:07:34.220
I've just set it up
as a linear model.

00:07:34.220 --> 00:07:36.650
So lets just run it very quick.

00:07:36.650 --> 00:07:38.610
And as you see here, its
looking at x1 and x2.

00:07:38.610 --> 00:07:40.267
And very, very
quickly it figures out

00:07:40.267 --> 00:07:41.350
how to separate those out.

00:07:41.350 --> 00:07:43.090
It just draw a
line in the middle.

00:07:43.090 --> 00:07:45.500
And its just optimizing
what that line looks

00:07:45.500 --> 00:07:48.740
like, how to separate
these out very simply.

00:07:48.740 --> 00:07:50.480
So this was a very easy problem.

00:07:50.480 --> 00:07:52.630
This is typically not how
the real problems are,

00:07:52.630 --> 00:07:55.010
but it's a great place to start.

00:07:55.010 --> 00:07:58.460
So let's go to a slightly
more complex problem now.

00:07:58.460 --> 00:08:02.535
So let's go to
this one here where

00:08:02.535 --> 00:08:04.500
you still have those
two kinds of points,

00:08:04.500 --> 00:08:06.110
but now they're
clustered differently.

00:08:06.110 --> 00:08:08.140
So there's one in the
center-- is a circle,

00:08:08.140 --> 00:08:10.020
and the rest are around it.

00:08:10.020 --> 00:08:13.204
So can we actually solve this
using the same kind of methods?

00:08:13.204 --> 00:08:15.120
Can we actually classify
using the same thing?

00:08:15.120 --> 00:08:17.260
If I run this, do you
think it would work?

00:08:17.260 --> 00:08:17.895
Any guesses?

00:08:20.690 --> 00:08:23.910
Let's try it out.

00:08:23.910 --> 00:08:26.020
So it's trying to
separate this out.

00:08:26.020 --> 00:08:28.152
It's basically trying
to draw a line.

00:08:28.152 --> 00:08:30.470
But it's lost.

00:08:30.470 --> 00:08:32.030
It's really not
making a progress

00:08:32.030 --> 00:08:34.559
if you see what's
happening here.

00:08:34.559 --> 00:08:36.650
So there are a few things
you can do about this.

00:08:36.650 --> 00:08:38.870
One, in terms of
classification, again,

00:08:38.870 --> 00:08:42.360
going a little bit into machine
learning into the details,

00:08:42.360 --> 00:08:45.700
you can add more features, more
kinds of things about the input

00:08:45.700 --> 00:08:48.550
data that helped
the model understand

00:08:48.550 --> 00:08:50.160
or how to separate them.

00:08:50.160 --> 00:08:53.840
So if you guys remember your
math back from high school

00:08:53.840 --> 00:08:57.880
or college, in this
particular case,

00:08:57.880 --> 00:08:59.920
given how these
clusters are structured,

00:08:59.920 --> 00:09:02.570
if you looked at a
couple more features

00:09:02.570 --> 00:09:04.430
from based on the inputs,
like in this case,

00:09:04.430 --> 00:09:07.480
x1 squared and x2
squared, you can probably

00:09:07.480 --> 00:09:09.141
use those to separate
this out better.

00:09:09.141 --> 00:09:09.890
So let's try this.

00:09:09.890 --> 00:09:12.730
Let's add these two
and see what happens.

00:09:12.730 --> 00:09:14.950
And taking those
two values, now it

00:09:14.950 --> 00:09:16.870
knows how to separate
those and clearly

00:09:16.870 --> 00:09:20.880
separate it out into two
separate, different classes.

00:09:20.880 --> 00:09:22.420
So this works.

00:09:22.420 --> 00:09:23.440
This great great, right?

00:09:23.440 --> 00:09:25.040
This is what happens
or used to happen

00:09:25.040 --> 00:09:28.330
in machine learning very
often for every single thing

00:09:28.330 --> 00:09:28.900
that you do.

00:09:28.900 --> 00:09:32.940
If you take a bunch of inputs,
you want to solve some problem.

00:09:32.940 --> 00:09:34.665
It's typically the
inputs themselves

00:09:34.665 --> 00:09:36.500
that are not in the
same kind of mode

00:09:36.500 --> 00:09:38.550
that you need to
solve the problem.

00:09:38.550 --> 00:09:41.240
And what you do is
called feature generation

00:09:41.240 --> 00:09:43.740
where you are
combining or crossing

00:09:43.740 --> 00:09:47.464
those features in different ways
to really solve your problem.

00:09:47.464 --> 00:09:48.880
But there's a lot
of work you need

00:09:48.880 --> 00:09:51.317
to do in figuring out the
right features that make sense.

00:09:51.317 --> 00:09:52.900
In this case, it was
a simple problem.

00:09:52.900 --> 00:09:56.390
So we knew x squared, x1 squared
or x2 squared would work.

00:09:56.390 --> 00:09:58.930
But in some cases, the
problem is not as simple

00:09:58.930 --> 00:10:01.050
and you really need to
do a little bit more.

00:10:01.050 --> 00:10:04.010
So for those kind
of cases, we have

00:10:04.010 --> 00:10:05.510
something called
deep learning which

00:10:05.510 --> 00:10:07.060
seems to work really well.

00:10:07.060 --> 00:10:08.680
So let's go to go
back to this example

00:10:08.680 --> 00:10:11.710
and let's see if we can solve
it in a different way as well.

00:10:11.710 --> 00:10:14.670
So let's take out these
two features first.

00:10:14.670 --> 00:10:17.060
Let's change this linear
to a nonlinearity.

00:10:17.060 --> 00:10:19.760
In this case, I'm going to
pick correctify linear unit.

00:10:19.760 --> 00:10:22.680
And I'm going to add a hidden
layer with a few more neurons.

00:10:22.680 --> 00:10:25.670
So what this is doing is
it has some inputs, as we

00:10:25.670 --> 00:10:27.900
said in this case just x1 x2.

00:10:27.900 --> 00:10:30.290
I'm adding another
layer, which is basically

00:10:30.290 --> 00:10:32.750
trying to combine those
inputs in whatever

00:10:32.750 --> 00:10:35.416
way is interesting to solve
our particular problem.

00:10:35.416 --> 00:10:37.790
And then let's see if we can
train this to actually solve

00:10:37.790 --> 00:10:40.500
the same problem as well.

00:10:40.500 --> 00:10:43.720
So if you see this, we took
the same features, no feature

00:10:43.720 --> 00:10:44.290
generation.

00:10:44.290 --> 00:10:45.380
We just added one layer.

00:10:45.380 --> 00:10:47.700
And this is a simple
problem, so one layer works.

00:10:47.700 --> 00:10:50.820
And it's very easily able
to separate those out.

00:10:50.820 --> 00:10:53.740
This is essentially what deep
learning is doing for you.

00:10:53.740 --> 00:10:57.360
As you add more layers, it
can understand and learn

00:10:57.360 --> 00:10:59.470
the right layers
of abstraction that

00:10:59.470 --> 00:11:02.172
are the most interesting to
solve your current problem.

00:11:02.172 --> 00:11:03.880
And of course, it
depends on the problem.

00:11:03.880 --> 00:11:06.662
It doesn't have to be a
simple classification.

00:11:06.662 --> 00:11:07.620
It could be regression.

00:11:07.620 --> 00:11:09.590
It could be recommendation,
whatever you are trying to do,

00:11:09.590 --> 00:11:10.170
really.

00:11:10.170 --> 00:11:13.820
You can apply similar
techniques to do that.

00:11:13.820 --> 00:11:18.660
So let's go back to our slides.

00:11:18.660 --> 00:11:21.760
In a sense, deep
learning's basically

00:11:21.760 --> 00:11:23.820
the same machine learning
ideas with more layers

00:11:23.820 --> 00:11:26.930
there that allow you to get
different kinds of abstraction.

00:11:26.930 --> 00:11:30.130
Of course, as part of that you
get many, many more parameters.

00:11:30.130 --> 00:11:31.590
You learn things differently.

00:11:31.590 --> 00:11:33.410
This example is
actually for what

00:11:33.410 --> 00:11:38.270
we call inception retreat, which
is our state of the art image

00:11:38.270 --> 00:11:39.900
model for image classification.

00:11:39.900 --> 00:11:44.107
So And we'll go through some
examples of how we use it.

00:11:44.107 --> 00:11:45.690
But this basically
gives you something

00:11:45.690 --> 00:11:46.606
nearer human accuracy.

00:11:51.010 --> 00:11:53.760
So machine learning has
been around for a long time,

00:11:53.760 --> 00:11:55.190
in fact way more than 10 years.

00:11:55.190 --> 00:11:57.730
What's really
changed since then?

00:11:57.730 --> 00:12:00.460
One of them is more
compute, definitely.

00:12:00.460 --> 00:12:02.750
Over the years, our
computers have gotten faster.

00:12:02.750 --> 00:12:03.910
We have more of them.

00:12:03.910 --> 00:12:09.450
We've been using newer chips,
CPUs, GPUs, now 2PUs here.

00:12:09.450 --> 00:12:14.550
And so a lot of things
that we're doing here--

00:12:14.550 --> 00:12:16.910
and more computer allows
us to really build

00:12:16.910 --> 00:12:19.890
more complex models,
more bigger models.

00:12:19.890 --> 00:12:23.500
But that needs a
lot of other things

00:12:23.500 --> 00:12:28.060
to go with it as well for
that to work, in this case,

00:12:28.060 --> 00:12:30.920
better algorithms and
more data help as well.

00:12:30.920 --> 00:12:34.849
So over the years we've
improved the algorithms slowly.

00:12:34.849 --> 00:12:37.390
A lot of these algorithms that
we talk about in deep learning

00:12:37.390 --> 00:12:39.795
are 30, 40, 50 years old.

00:12:39.795 --> 00:12:41.170
But over the last
10 years, we've

00:12:41.170 --> 00:12:43.920
seen some improvements
in the right places.

00:12:43.920 --> 00:12:46.260
For example, in a
case of deep learning,

00:12:46.260 --> 00:12:48.460
there are some improvements
to back propagation

00:12:48.460 --> 00:12:51.630
or the kind of nonlinearities
that we use that really make it

00:12:51.630 --> 00:12:53.610
much easier to optimize those.

00:12:53.610 --> 00:12:55.090
It used to be
really, really hard

00:12:55.090 --> 00:12:57.590
to train some of these models,
even if you had the compute.

00:12:57.590 --> 00:12:58.840
Even if you had the data.

00:12:58.840 --> 00:13:01.006
Now it's much easier and
there are better techniques

00:13:01.006 --> 00:13:01.560
to do that.

00:13:01.560 --> 00:13:04.030
So the math and the
algorithms is improved.

00:13:04.030 --> 00:13:06.650
And of course, the amount
of data that we have,

00:13:06.650 --> 00:13:07.640
that keeps growing.

00:13:07.640 --> 00:13:10.890
And that really is something
that machine learning can help.

00:13:10.890 --> 00:13:14.360
If you think about it with
small amounts of data humans

00:13:14.360 --> 00:13:14.990
are great.

00:13:14.990 --> 00:13:17.380
You can maybe read
10 pages of a book

00:13:17.380 --> 00:13:19.680
and understand that
and sift it down.

00:13:19.680 --> 00:13:21.882
But what if you had
a million pages?

00:13:21.882 --> 00:13:24.340
You can't really expect a human
to go through those million

00:13:24.340 --> 00:13:27.945
pages, understand those,
and summarize it for you.

00:13:27.945 --> 00:13:30.764
You really need some machines
that can automatically

00:13:30.764 --> 00:13:31.430
do this for you.

00:13:31.430 --> 00:13:33.513
And that's really where
machine learning comes in.

00:13:36.580 --> 00:13:38.650
So now let's talk a
little bit about what's

00:13:38.650 --> 00:13:41.090
happening in research
in machine learning now.

00:13:41.090 --> 00:13:44.880
We've talked a little bit
about science-fiction,

00:13:44.880 --> 00:13:47.331
mentioned a bunch of
products that are using this.

00:13:47.331 --> 00:13:49.580
And I'll go a little bit
more into the products later.

00:13:49.580 --> 00:13:52.240
But let's talk a little bit
about the research, where this

00:13:52.240 --> 00:13:54.020
is going, some of
the examples of where

00:13:54.020 --> 00:13:58.970
things of made improvements
in the recent past.

00:13:58.970 --> 00:14:01.790
So the first one I want
to talk about is AlphaGo.

00:14:01.790 --> 00:14:04.580
How many of you of
heard of AlphaGo?

00:14:04.580 --> 00:14:06.110
Wow, quite a few.

00:14:06.110 --> 00:14:11.210
So if you remember actually
almost 20 years ago now,

00:14:11.210 --> 00:14:14.120
there was deep blue, which
is a similar chess computer.

00:14:14.120 --> 00:14:17.290
In some sense it was similar
that the goal was, OK.

00:14:17.290 --> 00:14:19.420
Can we really beat
the best humans,

00:14:19.420 --> 00:14:23.040
Casper of the reigning champion
in that time using computers?

00:14:23.040 --> 00:14:25.930
And back in 1997
was the first time

00:14:25.930 --> 00:14:28.490
Deep Blue, which is this
computer built by IBM

00:14:28.490 --> 00:14:31.860
that actually beat
the world champion,

00:14:31.860 --> 00:14:35.930
beat Kasparov in a full
championship match.

00:14:35.930 --> 00:14:37.037
That was pretty amazing.

00:14:37.037 --> 00:14:38.120
And that was 20 years ago.

00:14:38.120 --> 00:14:40.470
It took a long time to
go from there to Go,

00:14:40.470 --> 00:14:42.840
partly because Go
itself as a game is

00:14:42.840 --> 00:14:45.170
way more complex than chess is.

00:14:45.170 --> 00:14:49.340
The possibilities that you have
at every level at every step

00:14:49.340 --> 00:14:52.690
are much, much bigger, much
more than what chess is.

00:14:52.690 --> 00:14:54.370
And so one of the
differences there

00:14:54.370 --> 00:14:57.020
is with Deep Blue
you could get away

00:14:57.020 --> 00:15:00.800
with some smarts and
a lot of brute force.

00:15:00.800 --> 00:15:02.800
You could have a
large supercomputer.

00:15:02.800 --> 00:15:07.199
You have some basic smarts and
a lot of rules around-- there

00:15:07.199 --> 00:15:08.740
are a lot of chess
players there that

00:15:08.740 --> 00:15:11.660
actually taught Deep Blue in
the sense it would train them

00:15:11.660 --> 00:15:12.397
on-- OK.

00:15:12.397 --> 00:15:13.230
Here the good moves.

00:15:13.230 --> 00:15:14.688
Here are not, et
cetera, et cetera,

00:15:14.688 --> 00:15:16.780
plus allowing it to do
a lot of brute force

00:15:16.780 --> 00:15:19.000
in trying out-- OK, looking
ahead how many different

00:15:19.000 --> 00:15:20.260
moves you can do.

00:15:20.260 --> 00:15:21.950
Now those same
techniques just could not

00:15:21.950 --> 00:15:24.033
work for Go because of the
different possibilities

00:15:24.033 --> 00:15:24.730
at every step.

00:15:24.730 --> 00:15:28.440
At every step we can do a
really large number of moves,

00:15:28.440 --> 00:15:32.110
and supporting that or being
able to go past a few steps

00:15:32.110 --> 00:15:35.220
is extremely hard, even
for modern day computers.

00:15:35.220 --> 00:15:37.370
So what really
worked in this case

00:15:37.370 --> 00:15:40.510
was a combination of
smartness and the computer.

00:15:40.510 --> 00:15:42.270
And in case of
smartness, they applied

00:15:42.270 --> 00:15:45.750
deep learning to figure out, OK,
looking at a board, what kind

00:15:45.750 --> 00:15:47.540
of moves make the most sense?

00:15:47.540 --> 00:15:50.050
And then let's try them out
and see what makes sense

00:15:50.050 --> 00:15:52.650
and which ones of
these play well.

00:15:52.650 --> 00:15:55.650
And so it's a combination of
improved algorithms, improved

00:15:55.650 --> 00:15:58.240
machine learning, combined
with the compute power as well.

00:16:01.790 --> 00:16:04.640
Another example
is ImageNet, which

00:16:04.640 --> 00:16:09.600
is this data set of a
million images with 1,000

00:16:09.600 --> 00:16:11.360
classes of different kinds.

00:16:11.360 --> 00:16:13.076
In fact, a lot of
classes are so close

00:16:13.076 --> 00:16:14.450
together that it's
very hard even

00:16:14.450 --> 00:16:17.040
for humans who don't know
about them to identify

00:16:17.040 --> 00:16:18.080
without learning.

00:16:18.080 --> 00:16:20.580
For example, there are
breeds of dogs which look

00:16:20.580 --> 00:16:22.460
very similar, at least to me.

00:16:22.460 --> 00:16:27.260
For people who really know
dogs, of course, it's easier.

00:16:27.260 --> 00:16:31.420
Over the last few years,
over the last five years,

00:16:31.420 --> 00:16:33.360
especially on this
data set, this

00:16:33.360 --> 00:16:36.880
as really driven the state
of, well, vision understanding

00:16:36.880 --> 00:16:38.340
for machines is.

00:16:38.340 --> 00:16:40.940
So about five years
ago, the accuracy

00:16:40.940 --> 00:16:43.800
that computers would
get was maybe 70%, 75%.

00:16:43.800 --> 00:16:48.880
This was the measured top five
accuracy, which is basically

00:16:48.880 --> 00:16:53.290
is the most-- the correct answer
among the top five results

00:16:53.290 --> 00:16:55.700
that the computer predicts?

00:16:55.700 --> 00:16:58.350
Now, five years ago this
was something like 75%.

00:16:58.350 --> 00:16:59.690
Today it's past.

00:16:59.690 --> 00:17:02.610
It's over 96%, which is
considered to be better

00:17:02.610 --> 00:17:03.840
than human accuracy.

00:17:03.840 --> 00:17:07.560
Humans also make mistakes
in this kind of thing.

00:17:07.560 --> 00:17:10.040
So it's really coming a
long way and just shows

00:17:10.040 --> 00:17:12.380
the kind of progress we can
make if we really get down

00:17:12.380 --> 00:17:13.629
to improving machine learning.

00:17:16.060 --> 00:17:19.260
This one's an interesting one,
taking the kind of image models

00:17:19.260 --> 00:17:23.420
that we've trained and
really letting them dream.

00:17:23.420 --> 00:17:25.460
This is what we call Deep Dream.

00:17:25.460 --> 00:17:27.460
There are some
folks at Google who

00:17:27.460 --> 00:17:29.180
basically took
those new networks

00:17:29.180 --> 00:17:31.830
and wanted to understand what's
happening in these networks.

00:17:31.830 --> 00:17:33.340
What are they learning right?

00:17:33.340 --> 00:17:34.870
And so in this
case, what they did

00:17:34.870 --> 00:17:37.560
was took a few neurons
in the network,

00:17:37.560 --> 00:17:40.400
just let them train
from that point

00:17:40.400 --> 00:17:43.190
and try to see what
we can generate

00:17:43.190 --> 00:17:45.450
from those neurons, what
they might be understanding

00:17:45.450 --> 00:17:46.510
from the images.

00:17:46.510 --> 00:17:48.407
And so they converged
to some of these.

00:17:48.407 --> 00:17:50.240
So in some of these
images in the top right,

00:17:50.240 --> 00:17:53.380
it's basically started from
some kind of palace or whatever

00:17:53.380 --> 00:17:56.060
and combined those, and
[INAUDIBLE] of that.

00:17:56.060 --> 00:17:59.257
The one in the top left it
has some fountains in there

00:17:59.257 --> 00:18:02.430
and a bunch of other things
that sort of combined.

00:18:02.430 --> 00:18:08.090
And this is very interesting
because in some ways

00:18:08.090 --> 00:18:10.340
there are similarities to
what people do as well.

00:18:10.340 --> 00:18:12.540
Of course, this is a
very different view.

00:18:12.540 --> 00:18:15.590
But it's starting to
learn in many, many ways.

00:18:15.590 --> 00:18:20.590
Machine learning is really
doing the same kind of learning

00:18:20.590 --> 00:18:21.670
that people might do.

00:18:24.260 --> 00:18:26.390
So now I'm going to
give you some examples.

00:18:26.390 --> 00:18:29.750
Let me just go to this and
play some, a couple more

00:18:29.750 --> 00:18:31.890
videos for you.

00:18:31.890 --> 00:18:35.210
In this case, it's doing what--
it's basically taking the video

00:18:35.210 --> 00:18:37.220
and applying different
kinds of painting styles.

00:18:37.220 --> 00:18:39.803
In this particular one it's just
picking a blue painting style

00:18:39.803 --> 00:18:42.340
and applying it to the
existing video in real time.

00:18:42.340 --> 00:18:44.471
Now it's a charcoal sketch.

00:18:44.471 --> 00:18:45.970
This is just an
example of how we're

00:18:45.970 --> 00:18:49.440
mixing machine learning
with things like art

00:18:49.440 --> 00:18:52.320
to all kinds of
interesting things in

00:18:52.320 --> 00:18:56.120
and interesting
combinations things that you

00:18:56.120 --> 00:18:58.270
can do with these.

00:18:58.270 --> 00:19:01.430
Here's another one where it
makes it look like a newspaper.

00:19:01.430 --> 00:19:03.960
So this one's based
on a paper based on

00:19:03.960 --> 00:19:06.939
by Justin Johnson and
others here at Sanford.

00:19:06.939 --> 00:19:08.980
Let me give you another
example of similar things

00:19:08.980 --> 00:19:10.070
that we can do.

00:19:10.070 --> 00:19:15.500
So this one is interesting
where the network

00:19:15.500 --> 00:19:17.390
learned from a bunch
of Chinese characters.

00:19:17.390 --> 00:19:19.920
It looked at how
they're drawn, and was

00:19:19.920 --> 00:19:22.450
able to really draw completely
new Chinese characters I've

00:19:22.450 --> 00:19:23.270
never seen before.

00:19:23.270 --> 00:19:24.910
These aren't real
characters, actually,

00:19:24.910 --> 00:19:27.010
for those of you who
understand the script.

00:19:27.010 --> 00:19:28.620
I don't, but then
they look very real.

00:19:28.620 --> 00:19:30.620
Even for people who
understand the script or who

00:19:30.620 --> 00:19:33.835
know the script, they feel
like they're almost there,

00:19:33.835 --> 00:19:35.960
they're clearly not the
right character or anything

00:19:35.960 --> 00:19:37.706
that they've ever seen.

00:19:37.706 --> 00:19:40.110
But it's very understanding
what's happening there.

00:19:40.110 --> 00:19:42.690
It gets tagged it's
supposed to look like this

00:19:42.690 --> 00:19:46.766
and is able to understand
that whole domain as well.

00:19:50.238 --> 00:19:54.720
I'm going to show you
a couple more videos.

00:19:54.720 --> 00:20:02.370
Here's one where it's basically
learned from a lot of numbers.

00:20:02.370 --> 00:20:05.060
These are house numbers
from Street View.

00:20:05.060 --> 00:20:09.910
Let's just go back here and
play it and pause it at the end.

00:20:09.910 --> 00:20:14.060
And so it's learned a bunch of--
it's looked at house numbers

00:20:14.060 --> 00:20:21.080
from Street View and is able
to really generate those.

00:20:21.080 --> 00:20:23.420
It's actually never seen
these particular numbers.

00:20:23.420 --> 00:20:25.150
The network is generating them.

00:20:25.150 --> 00:20:29.250
This is the kind of things that
these networks can do today.

00:20:29.250 --> 00:20:33.230
And the last one, in this case,
it's actually generating faces.

00:20:33.230 --> 00:20:36.690
And so, again, it learned
from a large face data set

00:20:36.690 --> 00:20:39.320
and now is able to generate
faces in all kinds of ways.

00:20:39.320 --> 00:20:41.420
Again, these are not faces
that's seen directly.

00:20:41.420 --> 00:20:43.920
It's regenerating them based
on all it's understanding

00:20:43.920 --> 00:20:45.080
of what faces look like.

00:20:47.770 --> 00:20:49.840
Yeah.

00:20:49.840 --> 00:20:52.710
Another one that we've
actually released very recently

00:20:52.710 --> 00:20:55.350
is this model to
parse sentences.

00:20:55.350 --> 00:20:59.500
So this is a very semantic
parsing of sentences

00:20:59.500 --> 00:21:01.120
where it looks at a sentence.

00:21:01.120 --> 00:21:03.410
In this case, I booked
a ticket to Google,

00:21:03.410 --> 00:21:06.580
and tries to understand
what the subjects are,

00:21:06.580 --> 00:21:09.790
all the grammatical
stuff about that,

00:21:09.790 --> 00:21:12.260
actually things that I
personally don't understand,

00:21:12.260 --> 00:21:13.560
and it's hard for me to learn.

00:21:13.560 --> 00:21:15.330
But this is able
to actually learn

00:21:15.330 --> 00:21:17.640
from a lot of sentences
that were given to it.

00:21:17.640 --> 00:21:20.370
In this example, it's actually
making the sentence piece

00:21:20.370 --> 00:21:23.580
by piece, and then building
a tree of hierarchy,

00:21:23.580 --> 00:21:26.000
and doing the
parsing in this case.

00:21:26.000 --> 00:21:28.320
This kind of thing
is extremely useful

00:21:28.320 --> 00:21:31.510
when you're trying to
understand natural language.

00:21:31.510 --> 00:21:35.060
And this just goes to show
how much these computers can

00:21:35.060 --> 00:21:38.092
understand the language that
you and I speak in some cases

00:21:38.092 --> 00:21:40.550
better than humans can as well
or at least better than kids

00:21:40.550 --> 00:21:42.667
can.

00:21:42.667 --> 00:21:45.250
Here's another example, sort of
going back to the data example

00:21:45.250 --> 00:21:49.185
where in science fiction
this-- here's this sort of poem

00:21:49.185 --> 00:21:50.680
that a computer might generate.

00:21:50.680 --> 00:21:54.030
This is actually something that
was generated by a real program

00:21:54.030 --> 00:21:57.570
today or a real model today.

00:21:57.570 --> 00:22:01.410
It's clearly not great, but
in this particular case,

00:22:01.410 --> 00:22:03.260
it just learned from
a bunch of books.

00:22:03.260 --> 00:22:06.150
It read those books or
understood those books.

00:22:06.150 --> 00:22:10.240
And was able to generate
this particular poem.

00:22:12.880 --> 00:22:15.170
So now let's talk about
this grateful research.

00:22:15.170 --> 00:22:16.690
Now let's talk
about a few products

00:22:16.690 --> 00:22:17.760
and how we're using them.

00:22:17.760 --> 00:22:19.689
So we had this
slide early on where

00:22:19.689 --> 00:22:22.230
I showed you a bunch of products
that [INAUDIBLE] using this.

00:22:22.230 --> 00:22:24.210
Let's go into some
of those products.

00:22:24.210 --> 00:22:26.214
Let's see how we are
actually using them.

00:22:26.214 --> 00:22:28.380
What are the kind of things
we're doing with machine

00:22:28.380 --> 00:22:31.780
learning in these products?

00:22:31.780 --> 00:22:33.980
First a search--
I'm sure most of you

00:22:33.980 --> 00:22:36.410
have-- or probably all
of you have used search

00:22:36.410 --> 00:22:38.250
or are using search
in some ways.

00:22:38.250 --> 00:22:41.230
This is a very good example of
how machine learning is used.

00:22:41.230 --> 00:22:42.894
The slide that I
showed you here,

00:22:42.894 --> 00:22:44.310
there are at least
two ways we are

00:22:44.310 --> 00:22:45.476
using machine learning here.

00:22:45.476 --> 00:22:47.490
The first, if you
use Y Searcher--

00:22:47.490 --> 00:22:49.590
so if you click that icon
on the right for the mic

00:22:49.590 --> 00:22:51.254
and you say something,
there's machine

00:22:51.254 --> 00:22:53.420
learning being used to
understand what you're saying

00:22:53.420 --> 00:22:54.950
and converting it to text.

00:22:54.950 --> 00:22:57.230
So you say something.

00:22:57.230 --> 00:22:59.880
That's really a bunch of
bytes for the computer.

00:22:59.880 --> 00:23:02.510
To convert that into
actual English or whatever,

00:23:02.510 --> 00:23:04.630
like you're speaking, it
needs to understand that.

00:23:04.630 --> 00:23:07.200
And there are pretty
sophisticated machine

00:23:07.200 --> 00:23:09.220
learning models that
do that for you today.

00:23:09.220 --> 00:23:10.700
Then the next part
is once you have

00:23:10.700 --> 00:23:13.820
that, what results to show you?

00:23:13.820 --> 00:23:17.400
Google has billions or trillions
of pages really indexed,

00:23:17.400 --> 00:23:19.350
which you probably
don't even think about.

00:23:19.350 --> 00:23:21.600
What you see typically
when you do that search

00:23:21.600 --> 00:23:24.010
is just 10 pages
on that first page.

00:23:24.010 --> 00:23:25.570
And you really
want those 10 pages

00:23:25.570 --> 00:23:28.880
to be the best ones, the most
appropriate ones for you.

00:23:28.880 --> 00:23:31.096
In some cases, you might
go to the next page or so.

00:23:31.096 --> 00:23:33.470
But most people are just going
to look at that first page

00:23:33.470 --> 00:23:34.760
and make their decision.

00:23:34.760 --> 00:23:37.560
So how do we really take
those billions of pages,

00:23:37.560 --> 00:23:39.960
match them to your
query, and then even then

00:23:39.960 --> 00:23:42.407
it's probably millions
or maybe more,

00:23:42.407 --> 00:23:44.490
and then put them down to
those, whittle them down

00:23:44.490 --> 00:23:45.812
to those top 10 pages?

00:23:45.812 --> 00:23:47.270
And that's, again,
machine learning

00:23:47.270 --> 00:23:49.010
at work in many, many forms.

00:23:49.010 --> 00:23:52.110
We're taking all these pages,
understanding your query,

00:23:52.110 --> 00:23:54.140
matching it through all
the data that we have.

00:23:54.140 --> 00:23:56.670
Maybe if you're looking for a
restaurant near Mountain View,

00:23:56.670 --> 00:23:58.502
we're going to leverage
that and understand

00:23:58.502 --> 00:24:00.710
that that needs to be combined
with the kind of pages

00:24:00.710 --> 00:24:01.670
that are there.

00:24:01.670 --> 00:24:03.972
If it's something else,
again, understanding

00:24:03.972 --> 00:24:06.180
the query and the documents
together, combining them,

00:24:06.180 --> 00:24:07.950
and then ranking them,
and sorting them,

00:24:07.950 --> 00:24:09.840
that something that machine
learning does really,

00:24:09.840 --> 00:24:10.340
really well.

00:24:12.800 --> 00:24:14.220
This example is of pictures.

00:24:14.220 --> 00:24:16.040
We talked about vision models.

00:24:16.040 --> 00:24:18.010
They're actually used in
everything you do now.

00:24:18.010 --> 00:24:23.460
So for example, over the years,
image collections, the amount

00:24:23.460 --> 00:24:25.320
of photos I have are huge.

00:24:25.320 --> 00:24:26.340
They're just growing.

00:24:26.340 --> 00:24:28.370
With digital cameras
coming on, it's

00:24:28.370 --> 00:24:30.645
been so much easier to
take pictures and just

00:24:30.645 --> 00:24:31.520
keep collecting them.

00:24:31.520 --> 00:24:32.914
You never throw them away.

00:24:32.914 --> 00:24:34.580
But then when you
want to find a picture

00:24:34.580 --> 00:24:36.746
and you really want to go
through those collections,

00:24:36.746 --> 00:24:38.390
it's really, really hard.

00:24:38.390 --> 00:24:39.766
So how can you do that?

00:24:39.766 --> 00:24:41.390
In this case, the
same kind of research

00:24:41.390 --> 00:24:44.480
that I was talking about,
which understands those images,

00:24:44.480 --> 00:24:47.260
is really indexing and
labeling all the pictures

00:24:47.260 --> 00:24:48.830
you have in Google Photos.

00:24:48.830 --> 00:24:50.790
And in this particular
example, if you

00:24:50.790 --> 00:24:52.290
search for cherry
blossom, it really

00:24:52.290 --> 00:24:54.512
shows you all your art
pictures of cherry blossom,

00:24:54.512 --> 00:24:55.720
and can display them for you.

00:25:00.090 --> 00:25:02.580
Another example is email.

00:25:02.580 --> 00:25:04.820
So there is this thing
called Smart Reply, which

00:25:04.820 --> 00:25:06.300
we launched a few
months ago, which

00:25:06.300 --> 00:25:09.480
you might be using in email.

00:25:09.480 --> 00:25:14.292
And what happens there
is you get an email.

00:25:14.292 --> 00:25:16.000
You typically, in this
case, you probably

00:25:16.000 --> 00:25:18.312
just want to get a
quick, easy answer.

00:25:18.312 --> 00:25:19.770
And it just gives
you some choices.

00:25:19.770 --> 00:25:21.000
Here, pick this one.

00:25:21.000 --> 00:25:23.330
And what it's doing
is it's really

00:25:23.330 --> 00:25:25.820
understanding what your
email thread is about

00:25:25.820 --> 00:25:28.182
and then suggesting a bunch
of replies ranking them

00:25:28.182 --> 00:25:29.890
whatever makes the
most sense and showing

00:25:29.890 --> 00:25:32.400
them the most interesting
ones to you if it makes sense.

00:25:35.250 --> 00:25:37.070
Another one is Google Music.

00:25:37.070 --> 00:25:39.480
There are many things
happening in there.

00:25:39.480 --> 00:25:44.107
And one of the things that we do
is we recommend a lot of things

00:25:44.107 --> 00:25:46.190
that you might want to
play, maybe albums that you

00:25:46.190 --> 00:25:48.600
might want to play,
maybe actual songs

00:25:48.600 --> 00:25:50.790
that you might want to
play based on the time,

00:25:50.790 --> 00:25:53.120
based on what you are
trying to do right now,

00:25:53.120 --> 00:25:54.750
based on your interest.

00:25:54.750 --> 00:25:57.460
It's really understanding
all of that, sorting them,

00:25:57.460 --> 00:25:59.200
and, again, ranking
them, recommending

00:25:59.200 --> 00:26:00.590
the right things for you.

00:26:00.590 --> 00:26:04.780
We only want to make it easy
for you to find what you want.

00:26:04.780 --> 00:26:07.820
The ideal case would
be you go there.

00:26:07.820 --> 00:26:09.260
Let's say it's
afternoon or let's

00:26:09.260 --> 00:26:10.940
say you're driving
back from work.

00:26:10.940 --> 00:26:14.320
And you typically like to
listen to, say, classical songs

00:26:14.320 --> 00:26:15.120
at that time.

00:26:15.120 --> 00:26:16.578
It should really
just recommend you

00:26:16.578 --> 00:26:18.600
the classical song and
maybe the top station

00:26:18.600 --> 00:26:20.170
that you do every time.

00:26:20.170 --> 00:26:22.680
You shouldn't have to look
for it just because-- just

00:26:22.680 --> 00:26:25.870
before this, you were
doing something else.

00:26:25.870 --> 00:26:27.210
So all of these are examples.

00:26:27.210 --> 00:26:30.200
Machine learning can
really make a difference

00:26:30.200 --> 00:26:33.020
and are helping us in
our everyday lives.

00:26:33.020 --> 00:26:37.570
And one more that I'm going
to talk about is spam.

00:26:37.570 --> 00:26:40.660
So everybody uses email.

00:26:40.660 --> 00:26:42.700
A few years ago,
many 5, 10 years ago,

00:26:42.700 --> 00:26:44.720
spam was this huge
issue for everyone

00:26:44.720 --> 00:26:47.702
where most of the
emailing in our inbox

00:26:47.702 --> 00:26:48.660
is starting to be spam.

00:26:48.660 --> 00:26:50.980
You have to actually
filter them out manually.

00:26:50.980 --> 00:26:54.700
So it's not that from that
time on to now spam is reduced.

00:26:54.700 --> 00:26:56.740
In fact, spam
continues to increase.

00:26:56.740 --> 00:26:59.280
It's just there are
filters, and algorithms

00:26:59.280 --> 00:27:02.220
have gotten a lot better
at recognizing what's spam.

00:27:02.220 --> 00:27:03.632
Again, they're not perfect.

00:27:03.632 --> 00:27:05.090
Occasionally you'll
still see spam.

00:27:05.090 --> 00:27:07.780
Or maybe we would say something
is spam when it's not.

00:27:07.780 --> 00:27:09.940
But they're getting
so much better,

00:27:09.940 --> 00:27:12.150
that I don't go to
my spam for spam box

00:27:12.150 --> 00:27:14.110
and see what I
might have missed,

00:27:14.110 --> 00:27:15.110
or the other way around.

00:27:15.110 --> 00:27:17.200
I rarely get emails
that's actually spam,

00:27:17.200 --> 00:27:20.450
even though if you actually read
the numbers about how much spam

00:27:20.450 --> 00:27:23.795
emails there are,
these are still

00:27:23.795 --> 00:27:26.770
way more, maybe 10 times more
than the real emails of spam

00:27:26.770 --> 00:27:29.580
itself.

00:27:29.580 --> 00:27:32.651
So lots of improvements
in machine learning,

00:27:32.651 --> 00:27:34.150
lots of improvements
in the products

00:27:34.150 --> 00:27:36.120
that are driven by
machine learning.

00:27:36.120 --> 00:27:39.180
So it's not about the algorithms
or the technology itself.

00:27:39.180 --> 00:27:42.610
It's about how do we apply it
to the products that we build?

00:27:42.610 --> 00:27:44.890
How do we make things
better for all the things

00:27:44.890 --> 00:27:46.520
that we're trying to do?

00:27:46.520 --> 00:27:49.070
All of this technology,
and algorithms

00:27:49.070 --> 00:27:50.750
in the computer
that we're building,

00:27:50.750 --> 00:27:53.800
it's all towards
solving real problems,

00:27:53.800 --> 00:27:58.650
solving real needs for
things that we do today.

00:27:58.650 --> 00:28:02.360
So now let's talk a little
bit about TensorFlow.

00:28:02.360 --> 00:28:04.756
That's the product I need.

00:28:04.756 --> 00:28:08.730
And it was built to help push
machine learning forward.

00:28:08.730 --> 00:28:10.190
Our team is a research team.

00:28:10.190 --> 00:28:12.509
And part of the goal for
us was to really push

00:28:12.509 --> 00:28:13.550
machine learning forward.

00:28:13.550 --> 00:28:15.500
And that's why we built it.

00:28:15.500 --> 00:28:17.010
But that's not the angle, right?

00:28:17.010 --> 00:28:18.900
We want to take that
machine learning as well

00:28:18.900 --> 00:28:21.233
and apply it like I was saying
in all of these products.

00:28:21.233 --> 00:28:25.900
TensorFlow allows us to do
that, and now you as well.

00:28:25.900 --> 00:28:29.090
And I'm going to tell you
of how we can do this.

00:28:29.090 --> 00:28:32.580
So it is open source.

00:28:32.580 --> 00:28:34.910
It's a machine learning
lab available on GitHub.

00:28:34.910 --> 00:28:36.570
It's, in fact, the
most popular machine

00:28:36.570 --> 00:28:38.920
learning library we opened
source six months ago.

00:28:38.920 --> 00:28:41.260
And it's rapidly
going to the top.

00:28:41.260 --> 00:28:42.400
There's a lot of interest.

00:28:42.400 --> 00:28:45.560
There are a lot of people
using it in all kinds of areas.

00:28:45.560 --> 00:28:48.120
Some of the research that
I talked about, actually

00:28:48.120 --> 00:28:51.460
people outside Google who've
taken up TensorFlow, and used

00:28:51.460 --> 00:28:55.200
it in doing a lot of those--
building those new things

00:28:55.200 --> 00:28:58.240
and new ideas, people
are including it

00:28:58.240 --> 00:29:00.900
in different kinds
of products as well.

00:29:00.900 --> 00:29:05.705
So at the core, it's a library
that's-- let's go a step

00:29:05.705 --> 00:29:06.205
forward.

00:29:08.820 --> 00:29:11.072
So what's this library
for right there?

00:29:11.072 --> 00:29:13.030
There are many different
things that we can do.

00:29:13.030 --> 00:29:15.990
Like I was saying, our group
is doing a lot of research.

00:29:15.990 --> 00:29:18.820
So it is for researchers who
want to take machine learning

00:29:18.820 --> 00:29:21.320
and really push it forward,
make it much better than it is,

00:29:21.320 --> 00:29:24.360
improve the algorithms, improve
how we can do things with it.

00:29:24.360 --> 00:29:25.980
So it is for them.

00:29:25.980 --> 00:29:27.630
But then there are
data scientists

00:29:27.630 --> 00:29:29.920
who want to really take
those algorithms-- let's

00:29:29.920 --> 00:29:31.631
see our data, let's
see our product.

00:29:31.631 --> 00:29:33.130
And you want to
improve that product

00:29:33.130 --> 00:29:37.190
by playing this machine learning
to the data that you have.

00:29:37.190 --> 00:29:39.390
And so the data scientists
can take those algorithms

00:29:39.390 --> 00:29:40.700
because they're also open now.

00:29:40.700 --> 00:29:43.410
They're made available to
you on all kinds of places.

00:29:43.410 --> 00:29:44.590
You can take those.

00:29:44.590 --> 00:29:47.570
You can use those and
make some improvements.

00:29:47.570 --> 00:29:50.030
And then there are
developers, like a lot of us

00:29:50.030 --> 00:29:54.170
here, actually, who are
trying to make, say, an app,

00:29:54.170 --> 00:29:56.660
and want to make it better,
or say, apply a product

00:29:56.660 --> 00:29:57.980
to their data center.

00:29:57.980 --> 00:30:01.300
And TensorFlow allows you
to really take all the work

00:30:01.300 --> 00:30:04.030
that researchers and data
scientists have done,

00:30:04.030 --> 00:30:05.840
and really run those
same models, the train

00:30:05.840 --> 00:30:09.330
models in production using the
exact same APIs without having

00:30:09.330 --> 00:30:11.190
to worry about-- OK.

00:30:11.190 --> 00:30:12.690
This seems great,
but how am I going

00:30:12.690 --> 00:30:13.980
to deploy this to production.

00:30:13.980 --> 00:30:16.660
That's another complexity
that that's harder.

00:30:16.660 --> 00:30:20.070
So we definitely try to make
sure that from the beginning,

00:30:20.070 --> 00:30:22.420
from the point that you get
started in machine learning,

00:30:22.420 --> 00:30:24.350
to actually pushing
it out, to production,

00:30:24.350 --> 00:30:25.640
it's a very short time cycle.

00:30:25.640 --> 00:30:28.210
You shouldn't be stuck
because of infrastructure.

00:30:28.210 --> 00:30:30.640
That's taken care of you.

00:30:30.640 --> 00:30:32.820
That's taken care of for you.

00:30:32.820 --> 00:30:35.900
And you can focus on
what's important for you

00:30:35.900 --> 00:30:38.520
in the most ways, which is
really building the products,

00:30:38.520 --> 00:30:40.560
helping your customers,
helping your users.

00:30:44.080 --> 00:30:46.760
So just a couple examples
of TensorFlow-- not going

00:30:46.760 --> 00:30:48.260
to go too detailed
into the code,

00:30:48.260 --> 00:30:50.840
but this is a very simple
example where we just

00:30:50.840 --> 00:30:54.010
take two inputs, do
a matrix multiply,

00:30:54.010 --> 00:30:58.140
and just print out the
result. So what it's doing

00:30:58.140 --> 00:31:00.900
is in the first two
cases, it's creating

00:31:00.900 --> 00:31:03.220
two nodes in the graph.

00:31:03.220 --> 00:31:06.980
And actually, let
me just-- yeah.

00:31:06.980 --> 00:31:09.390
It's creating two
nodes in the graph,

00:31:09.390 --> 00:31:12.424
and really doing a
multiplication, which

00:31:12.424 --> 00:31:14.840
is another node, which takes
the inputs from the other two

00:31:14.840 --> 00:31:15.517
nodes.

00:31:15.517 --> 00:31:17.100
And then the last
one really evaluates

00:31:17.100 --> 00:31:18.749
based on the two
inputs, combines them,

00:31:18.749 --> 00:31:19.790
and generates the output.

00:31:23.640 --> 00:31:25.850
This one's actually
a regression problem.

00:31:25.850 --> 00:31:28.020
So we talked about
classification earlier.

00:31:28.020 --> 00:31:30.630
In this case, it's trying to
predict some values based on,

00:31:30.630 --> 00:31:31.879
can we have some inputs?

00:31:31.879 --> 00:31:32.920
Can we have some outputs?

00:31:32.920 --> 00:31:34.779
You want to learn to
predict the outputs.

00:31:34.779 --> 00:31:37.320
It's basically doing something
similar to what we were seeing

00:31:37.320 --> 00:31:39.640
on the playground where
you're taking inputs,

00:31:39.640 --> 00:31:43.882
you're multiplying by matrix,
and you haven't lost it.

00:31:43.882 --> 00:31:45.340
You're trying to
optimize it, which

00:31:45.340 --> 00:31:47.380
is a typical machine
learning thing.

00:31:47.380 --> 00:31:48.690
And then you just loop through.

00:31:48.690 --> 00:31:51.750
So the last, at the end,
you basically have these.

00:31:51.750 --> 00:31:55.140
You're going through a bunch
of steps, and say 200 times

00:31:55.140 --> 00:31:57.695
in this particular case,
and you just print this out.

00:31:57.695 --> 00:31:59.320
And over time you're
basically learning

00:31:59.320 --> 00:32:01.160
to predict things better.

00:32:01.160 --> 00:32:03.765
Again, very simplistic
examples, typical models

00:32:03.765 --> 00:32:04.640
might be much bigger.

00:32:04.640 --> 00:32:06.100
But in a lot of
cases, you might also

00:32:06.100 --> 00:32:07.980
be able to use models
that are already there.

00:32:11.674 --> 00:32:13.340
Why don't we just
give you some examples

00:32:13.340 --> 00:32:15.850
of the kind of performance
we provide comparing it

00:32:15.850 --> 00:32:19.590
to some standard benchmarks
that are there on an open source

00:32:19.590 --> 00:32:20.820
website.

00:32:20.820 --> 00:32:23.800
And early on when we
launched TensorFlow,

00:32:23.800 --> 00:32:26.410
it was in the fastest
library out there.

00:32:26.410 --> 00:32:28.832
And there were a lot of
improvements that we had to do.

00:32:28.832 --> 00:32:31.290
But over the last six months,
we've really focused on that.

00:32:31.290 --> 00:32:33.560
And in this point,
it's really at the top

00:32:33.560 --> 00:32:37.170
in terms of compare compatible
to everything else out there.

00:32:37.170 --> 00:32:38.717
And we continue to improve it.

00:32:38.717 --> 00:32:39.800
There's not the end of it.

00:32:39.800 --> 00:32:41.841
There are many more
improvements that you can do.

00:32:41.841 --> 00:32:45.350
In this case, we're combating
four different kinds of models.

00:32:45.350 --> 00:32:47.960
In this case, they happen
to be all image models.

00:32:47.960 --> 00:32:52.120
And as you see, the
load is actually better.

00:32:52.120 --> 00:32:55.100
In a few cases, it's
just slightly worse.

00:32:55.100 --> 00:32:56.740
In few cases it's
slight-- in one case

00:32:56.740 --> 00:32:57.990
it's likely better, and so on.

00:32:57.990 --> 00:33:00.140
So it's roughly about the
same ballpark performance

00:33:00.140 --> 00:33:02.410
at this point.

00:33:02.410 --> 00:33:04.610
Now, one thing you can do
with TensorFlow as well

00:33:04.610 --> 00:33:06.480
is you can run it across
many, many machines

00:33:06.480 --> 00:33:08.300
rather then running
on just one machine.

00:33:08.300 --> 00:33:10.460
So the previous benchmark
that I was showing you

00:33:10.460 --> 00:33:12.820
was basically running
on a single GPU

00:33:12.820 --> 00:33:15.270
and training these
models on GPUs, something

00:33:15.270 --> 00:33:16.734
that a lot of people do.

00:33:16.734 --> 00:33:19.150
In this particular case, we're
training on many, many more

00:33:19.150 --> 00:33:21.990
machines, so each machine
having a single GPU

00:33:21.990 --> 00:33:23.420
going all the way to 100.

00:33:23.420 --> 00:33:25.790
And you can see it
scales quite well.

00:33:25.790 --> 00:33:27.640
In fact, up to 16
it's almost linear.

00:33:27.640 --> 00:33:30.420
And then it's still going to be
used to scale you in past that.

00:33:30.420 --> 00:33:32.919
And this is something that
we use internally as well

00:33:32.919 --> 00:33:35.210
to train really large models
and large data sets, which

00:33:35.210 --> 00:33:38.610
is-- as the data is going,
it's really important to be

00:33:38.610 --> 00:33:41.520
able to do this, to be able
to run on clusters, which many

00:33:41.520 --> 00:33:42.710
of you probably do already.

00:33:47.420 --> 00:33:48.780
Oops.

00:33:48.780 --> 00:33:51.580
So next I'm going to talk a
little bit about TensorFlow

00:33:51.580 --> 00:33:52.560
for Poets.

00:33:52.560 --> 00:33:54.210
So there's actually
a code lot for this

00:33:54.210 --> 00:33:56.543
as well if you want to look
at it and look at it online,

00:33:56.543 --> 00:33:59.100
or I believe there's
a place here too.

00:33:59.100 --> 00:34:02.470
So the idea here is-- let's
say you have a bunch of images,

00:34:02.470 --> 00:34:03.910
in this case
flowers, and you want

00:34:03.910 --> 00:34:05.879
to learn to recognize
those flowers,

00:34:05.879 --> 00:34:07.670
or maybe you just
identify if it's a flower

00:34:07.670 --> 00:34:10.750
or if these are five
different kinds of flowers.

00:34:10.750 --> 00:34:14.730
And what you can do
is you don't have

00:34:14.730 --> 00:34:17.280
to have a million examples.

00:34:17.280 --> 00:34:19.710
You might just have
1,000 examples, lets say.

00:34:19.710 --> 00:34:23.120
But you can get started from
a model that works really,

00:34:23.120 --> 00:34:23.620
really well.

00:34:23.620 --> 00:34:25.076
For example, our
best image model

00:34:25.076 --> 00:34:26.909
that you've open sourced
and made available,

00:34:26.909 --> 00:34:29.110
and that's what this
example talks about.

00:34:29.110 --> 00:34:32.389
Take the top part of
it, just train that

00:34:32.389 --> 00:34:35.080
on your data set from the point
that it's already trained on.

00:34:35.080 --> 00:34:37.889
So it's just going to learn
to classify the flowers.

00:34:37.889 --> 00:34:40.090
It already understands
image well,

00:34:40.090 --> 00:34:44.610
so it doesn't have to really
know what kind of features

00:34:44.610 --> 00:34:46.230
are important for
classifying images.

00:34:46.230 --> 00:34:47.270
It knows that already.

00:34:47.270 --> 00:34:50.870
It can just take your particular
models, your particular data,

00:34:50.870 --> 00:34:54.125
and learn to just separate those
out into different classes.

00:34:57.699 --> 00:34:59.240
So there are a few
tools that I would

00:34:59.240 --> 00:35:02.030
like to mention at the end
that you can play with,

00:35:02.030 --> 00:35:07.170
or that you can use before
you go in terms of just making

00:35:07.170 --> 00:35:09.600
things work well for you.

00:35:09.600 --> 00:35:13.370
So a lot of you are probably
familiar with Apache Spark,

00:35:13.370 --> 00:35:17.580
which is used for doing
analysis and doing

00:35:17.580 --> 00:35:19.830
learning on large data sets.

00:35:19.830 --> 00:35:22.350
So there is an
integration of Spark

00:35:22.350 --> 00:35:24.540
with TensorFlow--
somebody at Databricks

00:35:24.540 --> 00:35:27.190
did that-- where you can
actually take some images.

00:35:27.190 --> 00:35:27.860
You can create.

00:35:27.860 --> 00:35:29.370
It's very simplistic
in the sense

00:35:29.370 --> 00:35:31.140
you can create a lot
of different models

00:35:31.140 --> 00:35:32.930
and really use it to
pick the best model.

00:35:32.930 --> 00:35:35.986
You do that a lot because there
are some having parameters

00:35:35.986 --> 00:35:38.360
like, say, learning rate or
how big should your model be,

00:35:38.360 --> 00:35:38.859
et cetera.

00:35:38.859 --> 00:35:41.110
You maybe try 10
different models and use

00:35:41.110 --> 00:35:42.240
that to pick the best one.

00:35:46.570 --> 00:35:48.390
Another thing I would
like to point you do

00:35:48.390 --> 00:35:52.190
is Google's own Cloud
Machine Learning Platform.

00:35:52.190 --> 00:35:54.330
We announced this a
couple of months ago.

00:35:54.330 --> 00:35:58.110
It's under limited preview
right now, but you can sign up,

00:35:58.110 --> 00:36:00.080
and we will make it
accessible to you

00:36:00.080 --> 00:36:03.370
soon, where you can really have
the same power of TensorFlow

00:36:03.370 --> 00:36:04.809
that you have on
your machine say,

00:36:04.809 --> 00:36:06.600
but maybe don't have
the compute resources,

00:36:06.600 --> 00:36:09.250
or you don't want to manage
all the resources that you need

00:36:09.250 --> 00:36:10.807
to really run at large scale.

00:36:10.807 --> 00:36:12.640
And cloud takes care
of all of that for you.

00:36:12.640 --> 00:36:15.941
So you can experiment
on your machine,

00:36:15.941 --> 00:36:18.190
make sure you understand
what you want to do, and then

00:36:18.190 --> 00:36:20.095
push it off to the
cloud, let it take care

00:36:20.095 --> 00:36:23.021
of running large scale stuff.

00:36:23.021 --> 00:36:24.520
There are a few
examples of what you

00:36:24.520 --> 00:36:28.110
can do with models that have
already been trained by others

00:36:28.110 --> 00:36:31.570
and really take them
and just apply them.

00:36:31.570 --> 00:36:33.990
There are some APIs
on the forum today

00:36:33.990 --> 00:36:36.349
for what's called the
Mobile Mission APIs, which

00:36:36.349 --> 00:36:37.640
allows you to do face tracking.

00:36:37.640 --> 00:36:39.770
So let's say your pictures,
it allows you to-- it

00:36:39.770 --> 00:36:41.030
identifies parts of the faces.

00:36:41.030 --> 00:36:43.940
It first identifies
the face, and then

00:36:43.940 --> 00:36:46.170
parts of the face like
the eyes, nose, et cetera,

00:36:46.170 --> 00:36:48.169
so you can do different
kinds of things with it.

00:36:48.169 --> 00:36:50.470
It also recognizes
barcodes and QR codes

00:36:50.470 --> 00:36:54.880
and automatically understands
and parses them for you.

00:36:54.880 --> 00:36:58.680
On the cloud there are also
some APIs for vision and speech

00:36:58.680 --> 00:37:01.390
in translation, which you can
basically just send your image,

00:37:01.390 --> 00:37:02.950
and it tells you
what kind of image

00:37:02.950 --> 00:37:05.160
it is among a lot of
different classes.

00:37:05.160 --> 00:37:08.600
For speech, again, you can
upload data, upload speech,

00:37:08.600 --> 00:37:10.830
and it will give you back
text for whatever kind.

00:37:10.830 --> 00:37:13.710
For translation, you can
give it text in one language,

00:37:13.710 --> 00:37:16.390
say English or Japanese or
whatever makes sense for you,

00:37:16.390 --> 00:37:19.300
and it's going to convert
that to another language.

00:37:19.300 --> 00:37:21.721
So many different APIs,
and this is a growing area

00:37:21.721 --> 00:37:23.554
that you are going to
need to watch out for.

00:37:26.160 --> 00:37:27.580
So now for the future.

00:37:27.580 --> 00:37:28.760
It is really.

00:37:28.760 --> 00:37:31.940
I started by saying it's really
not the future, but it is.

00:37:31.940 --> 00:37:34.120
It i the present and the future.

00:37:34.120 --> 00:37:36.920
I just wanted to call out
that it's important for you

00:37:36.920 --> 00:37:39.920
to start thinking about it now
and not wait for the future,

00:37:39.920 --> 00:37:40.730
really.

00:37:40.730 --> 00:37:44.080
And it's really
your turn to start

00:37:44.080 --> 00:37:46.540
thinking about how you can
incorporate machine learning

00:37:46.540 --> 00:37:49.680
into everything that you do.

00:37:49.680 --> 00:37:51.300
What people are
looking for today

00:37:51.300 --> 00:37:55.010
is not just applications
that do what they want.

00:37:55.010 --> 00:37:57.237
There are tons of things
that are already doing that.

00:37:57.237 --> 00:37:58.320
How can you add that edge?

00:37:58.320 --> 00:38:01.150
How can you how you make it
really better for that user?

00:38:01.150 --> 00:38:04.390
By understanding what
they want before they

00:38:04.390 --> 00:38:06.240
ask for every single thing.

00:38:06.240 --> 00:38:08.940
And that's really
something that you can do.

00:38:08.940 --> 00:38:11.179
Think about this as you
build your next application,

00:38:11.179 --> 00:38:12.470
as you build your next project.

00:38:12.470 --> 00:38:14.428
And I think that's how
we'll change the future.

00:38:14.428 --> 00:38:18.450
That's how we'll make machine
learning part of the future.

00:38:18.450 --> 00:38:21.877
So I wanted to leave you with
some place with some pointers

00:38:21.877 --> 00:38:23.710
to where you can read
more about TensorFlow.

00:38:23.710 --> 00:38:28.240
It's on GitHub, and their
docs are tensorflow.org.

00:38:28.240 --> 00:38:30.850
And going back to my
reference for Tik-Tok,

00:38:30.850 --> 00:38:34.960
Tik-Tok is waiting for you
to really make the difference

00:38:34.960 --> 00:38:37.580
to do these things now.

00:38:37.580 --> 00:38:38.370
And thank you.

00:38:38.370 --> 00:38:44.070
[APPLAUSE]

00:38:44.070 --> 00:38:47.120
[MUSIC PLAYING]

