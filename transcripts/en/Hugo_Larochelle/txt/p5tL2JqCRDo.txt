Speaker 1:          00:00          In this video, we'll finally look at the, uh, expression for the dividends of the last function with respect to the parameters in our neural network. So we're finally ready to, uh, get the expression for, uh, the great end of the last function with respect to the parameters that we need to, uh, apply these to castic gradient descent out with them for training a neural network.

Speaker 2:          00:26          Yeah.

Speaker 1:          00:26          So let's first look at the partial it is with respect to the weights. So, uh, so when we are looking at the partial, the they've for the weight I j uh, for the hidden layer, Kay. So one of the parcel with this, uh, the loss function. Now I'm the only pre activation that depends on this weight is going to be deeply activation at the hidden layer k for the IMF hidden unit. So Wij or k Ij is going to connect that hidden unit with the, uh, hidden unit below in the using its activation for the GF hidden unit that the layer below. So using the chain rule, we can write that partial derivative as the product of the partial. They'll live up the last function with respect to deeply activation at the kid hidden layer for hidden unit. I times the partial native of that active, that pre activation with respect to our parameter.

Speaker 1:          01:28          And now we know that the reactivation is a linear function with respect to the weight. So it's a linear transformation of the activation of the hidden layer below and now in this son. So the gradient, which respect Wij for the [inaudible] layer of this term, the biases zero, this does not depend on the weights. And here we have a some that involves a bunch of different weights, but the only one that matters that is a, that involves Wij is the one for the value of j that corresponds to this j here. And so, uh, here, then we'll have this parameter. So the parameter multiply by the activation of the JFK, then unit at the layer below. So if we take the narratives, a narrative of that wear suits back to Wk Ij, uh, we just get a, the activation value. So this is just equal to the activation of the JFF hidden unit in the hidden layer. That's just below the hidden layer came my last one.

Speaker 2:          02:33          Yeah.

Speaker 1:          02:34          And uh, now, uh, so we just take that and we multiply it by the, uh, pre activation, a partial derivative, which we've seen before. We've seen how to uh, wishing a form. We've seen the formula for, okay, now what if we want the gradient. So if on the grain and with respect to a matrix, then that's going to be instead of being a of partial data live is going to be a matrix of partial dividends where the elements I Jay is going to be the partial derivative with respect to Wij for day Keith in layer, if you go back in the expression, we see that this parcel, the four elements at row I n column j is a just a multiplication of the pre activation, a partial David dative for neuron. I times the value, the activation value of the neuron, uh, for the JF neuron at the layer below.

Speaker 1:          03:38          So what we can do is take the vector of a pre activation gradients. So that's uh, that's what the grain in vector is going to be. The vector of partial. That is for all potential, uh, p activation in the layer cake. And then multiply that by the vector of activations at the layer below. But we're going to transpose it to get a row vector. Now if we take a column vector times a row vector, we're going to get a matrix and d element at position. I, a row I n Colin Jay is simply going to be the IAFF elements of the column Vector Times Djf elements of the row vector. And so indeed we'll get a matrix where the element Ij is going to be the multiplication of the ASM and of that vector times DJF element of that vector, which turns out to be, uh, the actual value of the partial data with this Spec to connection I Jay. So we get this expression actually does correspond to the gradients with respect to the full matrix w for the hidden layer cake.

Speaker 1:          04:59          And, uh, now we can look at the partial that is with respect to the biases. Uh, so again, the uh, only pre activation that depends on the IMF bias for the hidden layer cake is going to be deeply activation for hidden layer gay looking at it's hidden unit. So we can take with the chain rule that partial there they have time, the partial [inaudible] of that pre activation for the eye, uh, hitting unit, hidden layer k with respect to our parameter. And uh, looking at this formula, we see that this term does not depend on the bias, but this term depends on it. And uh, it's actually just that variable. So if we think the partial derivative, which respected that is going to be one. So this term multiplied by that, just essentially cancel this out because it's equal to one. So we only get the value of the partial daily of the pre activation for the IMF hidden unit. Any food put that into a vector. Well that's just the gradients for the p activation. So actually the gradient for the bias for the hidden, they are k, it's just a gradient for the reactivation. So it's actually very simple. So with that, we have an expression for the grain is for all of our weights and all our biases that we need to use to apply stochastic gradient descent, training of neural networks.