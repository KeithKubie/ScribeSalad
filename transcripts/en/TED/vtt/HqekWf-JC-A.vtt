WEBVTT
Kind: captions
Language: en

00:00:12.800 --> 00:00:16.360
So why do you think
the rich should pay more in taxes?

00:00:16.400 --> 00:00:18.776
Why did you buy the latest iPhone?

00:00:18.800 --> 00:00:21.256
Why did you pick your current partner?

00:00:21.280 --> 00:00:24.696
And why did so many people
vote for Donald Trump?

00:00:24.720 --> 00:00:27.240
What were the reasons, why did they do it?

00:00:27.990 --> 00:00:30.096
So we ask this kind
of question all the time,

00:00:30.120 --> 00:00:31.856
and we expect to get an answer.

00:00:31.880 --> 00:00:35.016
And when being asked,
we expect ourselves to know the answer,

00:00:35.040 --> 00:00:37.520
to simply tell why we did as we did.

00:00:38.440 --> 00:00:40.160
But do we really know why?

00:00:41.000 --> 00:00:44.456
So when you say that you prefer
George Clooney to Tom Hanks,

00:00:44.479 --> 00:00:46.536
due to his concern for the environment,

00:00:46.560 --> 00:00:47.760
is that really true?

00:00:48.560 --> 00:00:51.056
So you can be perfectly sincere
and genuinely believe

00:00:51.080 --> 00:00:54.016
that this is the reason
that drives your choice,

00:00:54.040 --> 00:00:56.640
but to me, it may still feel
like something is missing.

00:00:57.560 --> 00:01:00.736
As it stands, due to
the nature of subjectivity,

00:01:00.760 --> 00:01:05.080
it is actually very hard to ever prove
that people are wrong about themselves.

00:01:06.600 --> 00:01:08.736
So I'm an experimental psychologist,

00:01:08.760 --> 00:01:12.296
and this is the problem
we've been trying to solve in our lab.

00:01:12.320 --> 00:01:14.496
So we wanted to create an experiment

00:01:14.520 --> 00:01:18.056
that would allow us to challenge
what people say about themselves,

00:01:18.080 --> 00:01:20.760
regardless of how certain they may seem.

00:01:21.960 --> 00:01:24.696
But tricking people
about their own mind is hard.

00:01:24.720 --> 00:01:27.096
So we turned to the professionals.

00:01:27.120 --> 00:01:28.320
The magicians.

00:01:29.120 --> 00:01:32.016
So they're experts at creating
the illusion of a free choice.

00:01:32.040 --> 00:01:34.336
So when they say, "Pick a card, any card,"

00:01:34.360 --> 00:01:37.280
the only thing you know
is that your choice is no longer free.

00:01:38.200 --> 00:01:40.576
So we had a few fantastic
brainstorming sessions

00:01:40.600 --> 00:01:42.456
with a group of Swedish magicians,

00:01:42.480 --> 00:01:44.122
and they helped us create a method

00:01:44.147 --> 00:01:48.120
in which we would be able to manipulate
the outcome of people's choices.

00:01:48.760 --> 00:01:51.696
This way we would know
when people are wrong about themselves,

00:01:51.720 --> 00:01:53.760
even if they don't know this themselves.

00:01:54.480 --> 00:01:59.136
So I will now show you
a short movie showing this manipulation.

00:01:59.160 --> 00:02:00.576
So it's quite simple.

00:02:00.600 --> 00:02:02.736
The participants make a choice,

00:02:02.760 --> 00:02:05.016
but I end up giving them the opposite.

00:02:05.040 --> 00:02:08.560
And then we want to see:
How did they react, and what did they say?

00:02:09.240 --> 00:02:12.400
So it's quite simple, but see
if you can spot the magic going on.

00:02:13.440 --> 00:02:16.960
And this was shot with real participants,
they don't know what's going on.

00:02:19.000 --> 00:02:21.216
(Video) Petter Johansson:
Hi, my name's Petter.

00:02:21.240 --> 00:02:22.455
Woman: Hi, I'm Becka.

00:02:22.479 --> 00:02:24.616
PJ: I'm going to show you
pictures like this.

00:02:24.640 --> 00:02:27.536
And you'll have to decide
which one you find more attractive.

00:02:27.560 --> 00:02:28.776
Becka: OK.

00:02:28.800 --> 00:02:31.976
PJ: And then sometimes,
I will ask you why you prefer that face.

00:02:32.000 --> 00:02:33.216
Becka: OK.

00:02:33.240 --> 00:02:34.440
PJ: Ready?
Becka: Yeah.

00:02:43.120 --> 00:02:44.936
PJ: Why did you prefer that one?

00:02:44.960 --> 00:02:46.456
Becka: The smile, I think.

00:02:46.480 --> 00:02:47.680
PJ: Smile.

00:02:52.400 --> 00:02:53.640
Man: One on the left.

00:02:57.520 --> 00:02:59.160
Again, this one just struck me.

00:02:59.760 --> 00:03:01.376
Interesting shot.

00:03:01.400 --> 00:03:04.400
Since I'm a photographer,
I like the way it's lit and looks.

00:03:06.280 --> 00:03:08.320
Petter Johansson: But now comes the trick.

00:03:10.120 --> 00:03:11.400
(Video) Woman 1: This one.

00:03:16.240 --> 00:03:18.520
PJ: So they get the opposite
of their choice.

00:03:20.520 --> 00:03:22.120
And let's see what happens.

00:03:28.240 --> 00:03:29.440
Woman 2: Um ...

00:03:35.760 --> 00:03:38.560
I think he seems a little more
innocent than the other guy.

00:03:45.360 --> 00:03:46.600
Man: The one on the left.

00:03:49.280 --> 00:03:52.976
I like her smile
and contour of the nose and face.

00:03:53.000 --> 00:03:55.760
So it's a little more interesting
to me, and her haircut.

00:04:00.040 --> 00:04:01.240
Woman 3: This one.

00:04:03.520 --> 00:04:05.096
I like the smirky look better.

00:04:05.120 --> 00:04:07.120
PJ: You like the smirky look better?

00:04:09.680 --> 00:04:12.856
(Laughter)

00:04:12.880 --> 00:04:14.080
Woman 3: This one.

00:04:15.280 --> 00:04:16.680
PJ: What made you choose him?

00:04:17.520 --> 00:04:20.416
Woman 3: I don't know,
he looks a little bit like the Hobbit.

00:04:20.440 --> 00:04:22.496
(Laughter)

00:04:22.520 --> 00:04:24.016
PJ: And what happens in the end

00:04:24.040 --> 00:04:27.136
when I tell them the true nature
of the experiment?

00:04:27.160 --> 00:04:29.616
Yeah, that's it. I just have to
ask a few questions.

00:04:29.640 --> 00:04:30.856
Man: Sure.

00:04:30.880 --> 00:04:33.856
PJ: What did you think
of this experiment, was it easy or hard?

00:04:33.880 --> 00:04:35.120
Man: It was easy.

00:04:36.040 --> 00:04:37.376
PJ: During the experiments,

00:04:37.400 --> 00:04:40.736
I actually switched
the pictures three times.

00:04:40.760 --> 00:04:42.336
Was this anything you noticed?

00:04:42.360 --> 00:04:44.176
Man: No. I didn't notice any of that.

00:04:44.200 --> 00:04:45.696
PJ: Not at all?
Man: No.

00:04:45.720 --> 00:04:47.816
Switching the pictures as far as ...

00:04:47.840 --> 00:04:51.656
PJ: Yeah, you were pointing at one of them
but I actually gave you the opposite.

00:04:51.680 --> 00:04:53.496
Man: The opposite one.
OK, when you --

00:04:53.520 --> 00:04:55.776
No. Shows you how much
my attention span was.

00:04:55.800 --> 00:04:57.320
(Laughter)

00:04:58.880 --> 00:05:01.896
PJ: Did you notice that sometimes
during the experiment

00:05:01.920 --> 00:05:04.056
I switched the pictures?

00:05:04.080 --> 00:05:06.096
Woman 2: No, I did not notice that.

00:05:06.120 --> 00:05:09.120
PJ: You were pointing at one,
but then I gave you the other one.

00:05:09.920 --> 00:05:11.536
No inclination of that happening?

00:05:11.560 --> 00:05:13.136
Woman 2: No.

00:05:13.160 --> 00:05:14.416
Woman 2: I did not notice.

00:05:14.440 --> 00:05:16.376
(Laughs)

00:05:16.400 --> 00:05:17.616
PJ: Thank you.

00:05:17.640 --> 00:05:19.016
Woman 2: Thank you.

00:05:19.040 --> 00:05:21.096
PJ: OK, so as you probably
figured out now,

00:05:21.120 --> 00:05:23.376
the trick is that I have
two cards in each hand,

00:05:23.400 --> 00:05:24.976
and when I hand one of them over,

00:05:25.000 --> 00:05:29.360
the black one kind of disappears
into the black surface on the table.

00:05:30.640 --> 00:05:32.376
So using pictures like this,

00:05:32.400 --> 00:05:36.776
normally not more than 20 percent
of the participants detect these tries.

00:05:36.800 --> 00:05:38.216
And as you saw in the movie,

00:05:38.240 --> 00:05:41.416
when in the end
we explain what's going on,

00:05:41.440 --> 00:05:45.816
they're very surprised and often refuse
to believe the trick has been made.

00:05:45.840 --> 00:05:50.616
So this shows that this effect
is quite robust and a genuine effect.

00:05:50.640 --> 00:05:53.296
But if you're interested
in self-knowledge, as I am,

00:05:53.320 --> 00:05:54.656
the more interesting bit is,

00:05:54.680 --> 00:05:58.616
OK, so what did they say
when they explained these choices?

00:05:58.640 --> 00:06:00.136
So we've done a lot of analysis

00:06:00.160 --> 00:06:02.240
of the verbal reports
in these experiments.

00:06:03.360 --> 00:06:05.816
And this graph simply shows

00:06:05.840 --> 00:06:10.616
that if you compare
what they say in a manipulated trial

00:06:10.640 --> 00:06:12.016
with a nonmanipulated trial,

00:06:12.040 --> 00:06:14.816
that is when they explain
a normal choice they've made

00:06:14.840 --> 00:06:17.336
and one where we manipulated the outcome,

00:06:17.360 --> 00:06:19.816
we find that they are remarkably similar.

00:06:19.840 --> 00:06:22.896
So they are just as emotional,
just as specific,

00:06:22.920 --> 00:06:26.120
and they are expressed
with the same level of certainty.

00:06:27.120 --> 00:06:29.456
So the strong conclusion to draw from this

00:06:29.480 --> 00:06:31.696
is that if there are no differences

00:06:31.720 --> 00:06:35.416
between a real choice
and a manipulated choice,

00:06:35.440 --> 00:06:37.880
perhaps we make things up all the time.

00:06:38.680 --> 00:06:40.016
But we've also done studies

00:06:40.040 --> 00:06:43.056
where we try to match what they say
with the actual faces.

00:06:43.080 --> 00:06:44.960
And then we find things like this.

00:06:45.760 --> 00:06:50.816
So here, this male participant,
he preferred the girl to the left,

00:06:50.840 --> 00:06:52.696
he ended up with the one to the right.

00:06:52.720 --> 00:06:55.536
And then, he explained
his choice like this.

00:06:55.560 --> 00:06:56.856
"She is radiant.

00:06:56.880 --> 00:06:59.976
I would rather have approached her
at the bar than the other one.

00:07:00.000 --> 00:07:01.616
And I like earrings."

00:07:01.640 --> 00:07:05.136
And whatever made him choose
the girl on the left to begin with,

00:07:05.160 --> 00:07:06.736
it can't have been the earrings,

00:07:06.760 --> 00:07:09.616
because they were actually
sitting on the girl on the right.

00:07:09.640 --> 00:07:13.416
So this is a clear example
of a post hoc construction.

00:07:13.440 --> 00:07:16.240
So they just explained
the choice afterwards.

00:07:17.320 --> 00:07:19.616
So what this experiment shows is,

00:07:19.640 --> 00:07:23.296
OK, so if we fail to detect
that our choices have been changed,

00:07:23.320 --> 00:07:26.520
we will immediately start
to explain them in another way.

00:07:27.520 --> 00:07:28.776
And what we also found

00:07:28.800 --> 00:07:32.016
is that the participants
often come to prefer the alternative,

00:07:32.040 --> 00:07:34.296
that they were led to believe they liked.

00:07:34.320 --> 00:07:36.336
So if we let them do the choice again,

00:07:36.360 --> 00:07:40.120
they will now choose the face
they had previously rejected.

00:07:41.520 --> 00:07:43.816
So this is the effect
we call "choice blindness."

00:07:43.840 --> 00:07:46.056
And we've done
a number of different studies --

00:07:46.080 --> 00:07:48.616
we've tried consumer choices,

00:07:48.640 --> 00:07:53.056
choices based on taste and smell
and even reasoning problems.

00:07:53.080 --> 00:07:55.136
But what you all want to know is of course

00:07:55.160 --> 00:07:59.096
does this extend also
to more complex, more meaningful choices?

00:07:59.120 --> 00:08:02.200
Like those concerning
moral and political issues.

00:08:04.400 --> 00:08:08.616
So the next experiment,
it needs a little bit of a background.

00:08:08.640 --> 00:08:12.896
So in Sweden, the political landscape

00:08:12.920 --> 00:08:16.280
is dominated by a left-wing
and a right-wing coalition.

00:08:17.720 --> 00:08:22.136
And the voters may move a little bit
between the parties within each coalition,

00:08:22.160 --> 00:08:24.920
but there is very little movement
between the coalitions.

00:08:25.680 --> 00:08:27.656
And before each elections,

00:08:27.680 --> 00:08:31.896
the newspapers and the polling institutes

00:08:31.920 --> 00:08:34.536
put together what they call
"an election compass"

00:08:34.560 --> 00:08:37.896
which consists of a number
of dividing issues

00:08:37.920 --> 00:08:40.256
that sort of separates the two coalitions.

00:08:40.280 --> 00:08:44.015
Things like if tax on gasoline
should be increased

00:08:44.039 --> 00:08:48.135
or if the 13 months of paid parental leave

00:08:48.159 --> 00:08:50.655
should be split equally
between the two parents

00:08:50.679 --> 00:08:53.400
in order to increase gender equality.

00:08:54.840 --> 00:08:57.056
So, before the last Swedish election,

00:08:57.080 --> 00:08:59.680
we created an election compass of our own.

00:09:00.480 --> 00:09:02.616
So we walked up to people in the street

00:09:02.640 --> 00:09:05.976
and asked if they wanted
to do a quick political survey.

00:09:06.000 --> 00:09:08.456
So first we had them state
their voting intention

00:09:08.480 --> 00:09:09.840
between the two coalitions.

00:09:10.560 --> 00:09:14.336
Then we asked them
to answer 12 of these questions.

00:09:14.360 --> 00:09:16.336
They would fill in their answers,

00:09:16.360 --> 00:09:17.976
and we would ask them to discuss,

00:09:18.000 --> 00:09:23.496
so OK, why do you think
tax on gas should be increased?

00:09:23.520 --> 00:09:25.616
And we'd go through the questions.

00:09:25.640 --> 00:09:29.536
Then we had a color coded template

00:09:29.560 --> 00:09:32.496
that would allow us
to tally their overall score.

00:09:32.520 --> 00:09:35.976
So this person would have
one, two, three, four

00:09:36.000 --> 00:09:39.296
five, six, seven, eight, nine
scores to the left,

00:09:39.320 --> 00:09:42.000
so he would lean to the left, basically.

00:09:42.800 --> 00:09:47.240
And in the end, we also had them
fill in their voting intention once more.

00:09:48.160 --> 00:09:50.440
But of course, there was
also a trick involved.

00:09:51.360 --> 00:09:53.536
So first, we walked up to people,

00:09:53.560 --> 00:09:55.616
we asked them
about their voting intention

00:09:55.640 --> 00:09:57.896
and then when they started filling in,

00:09:57.920 --> 00:10:03.376
we would fill in a set of answers
going in the opposite direction.

00:10:03.400 --> 00:10:05.976
We would put it under the notepad.

00:10:06.000 --> 00:10:08.776
And when we get the questionnaire,

00:10:08.800 --> 00:10:12.120
we would simply glue it on top
of the participant's own answer.

00:10:16.000 --> 00:10:17.240
So there, it's gone.

00:10:24.280 --> 00:10:26.656
And then we would ask
about each of the questions:

00:10:26.680 --> 00:10:28.216
How did you reason here?

00:10:28.240 --> 00:10:29.976
And they'll state the reasons,

00:10:30.000 --> 00:10:32.480
together we will sum up
their overall score.

00:10:34.800 --> 00:10:38.480
And in the end, they will state
their voting intention again.

00:10:41.960 --> 00:10:43.616
So what we find first of all here,

00:10:43.640 --> 00:10:47.856
is that very few of these
manipulations are detected.

00:10:47.880 --> 00:10:50.536
And they're not detected
in the sense that they realize,

00:10:50.560 --> 00:10:52.416
"OK, you must have changed my answer,"

00:10:52.440 --> 00:10:53.696
it was more the case that,

00:10:53.720 --> 00:10:56.896
"OK, I must've misunderstood
the question the first time I read it.

00:10:56.920 --> 00:10:58.160
Can I please change it?"

00:10:59.080 --> 00:11:04.216
And even if a few of these
manipulations were changed,

00:11:04.240 --> 00:11:06.376
the overall majority was missed.

00:11:06.400 --> 00:11:10.056
So we managed to switch 90 percent
of the participants' answers

00:11:10.080 --> 00:11:13.240
from left to right, right to left,
their overall profile.

00:11:14.800 --> 00:11:19.200
And what happens then when
they are asked to motivate their choices?

00:11:20.160 --> 00:11:23.216
And here we find much more
interesting verbal reports

00:11:23.240 --> 00:11:25.256
than compared to the faces.

00:11:25.280 --> 00:11:28.640
People say things like this,
and I'll read it to you.

00:11:29.720 --> 00:11:33.456
So, "Large-scale governmental surveillance
of email and internet traffic

00:11:33.480 --> 00:11:37.816
ought to be permissible as means to combat
international crime and terrorism."

00:11:37.840 --> 00:11:40.556
"So you agree to some extent
with this statement." "Yes."

00:11:40.580 --> 00:11:42.080
"So how did you reason here?"

00:11:43.600 --> 00:11:48.536
"Well, like, as it is so hard to get
at international crime and terrorism,

00:11:48.560 --> 00:11:51.336
I think there should be
those kinds of tools."

00:11:51.360 --> 00:11:54.976
And then the person remembers an argument
from the newspaper in the morning.

00:11:55.000 --> 00:11:56.616
"Like in the newspaper today,

00:11:56.640 --> 00:12:00.016
it said they can like,
listen to mobile phones from prison,

00:12:00.040 --> 00:12:03.576
if a gang leader tries to continue
his crimes from inside.

00:12:03.600 --> 00:12:06.416
And I think it's madness
that we have so little power

00:12:06.440 --> 00:12:08.096
that we can't stop those things

00:12:08.120 --> 00:12:11.056
when we actually have
the possibility to do so."

00:12:11.080 --> 00:12:13.776
And then there's a little bit
back and forth in the end:

00:12:13.800 --> 00:12:16.376
"I don't like that they have access
to everything I do,

00:12:16.400 --> 00:12:18.976
but I still think
it's worth it in the long run."

00:12:19.000 --> 00:12:21.536
So, if you didn't know that this person

00:12:21.560 --> 00:12:23.816
just took part in
a choice blindness experiment,

00:12:23.840 --> 00:12:25.696
I don't think you would question

00:12:25.720 --> 00:12:28.840
that this is the true attitude
of that person.

00:12:29.800 --> 00:12:32.656
And what happens in the end,
with the voting intention?

00:12:32.680 --> 00:12:37.376
What we find -- that one is also
clearly affected by the questionnaire.

00:12:37.400 --> 00:12:39.136
So we have 10 participants

00:12:39.160 --> 00:12:42.136
shifting from left to right
or from right to left.

00:12:42.160 --> 00:12:44.696
We have another 19
that go from clear voting intention

00:12:44.720 --> 00:12:46.176
to being uncertain.

00:12:46.200 --> 00:12:49.296
Some go from being uncertain
to clear voting intention.

00:12:49.320 --> 00:12:54.056
And then there is a number of participants
staying uncertain throughout.

00:12:54.080 --> 00:12:55.656
And that number is interesting

00:12:55.680 --> 00:13:00.296
because if you look
at what the polling institutes say

00:13:00.320 --> 00:13:01.976
the closer you get to an election,

00:13:02.000 --> 00:13:04.136
the only people that are sort of in play

00:13:04.160 --> 00:13:06.816
are the ones that are
considered uncertain.

00:13:06.840 --> 00:13:10.056
But we show there is a much larger number

00:13:10.080 --> 00:13:12.880
that would actually consider
shifting their attitudes.

00:13:13.640 --> 00:13:17.136
And here I must point out, of course,
that you are not allowed to use this

00:13:17.160 --> 00:13:19.776
as an actual method
to change people's votes

00:13:19.800 --> 00:13:21.296
before an election,

00:13:21.320 --> 00:13:24.936
and we clearly debriefed them afterwards

00:13:24.960 --> 00:13:27.256
and gave them every
opportunity to change back

00:13:27.280 --> 00:13:29.760
to whatever they thought first.

00:13:30.600 --> 00:13:32.936
But what this shows is
that if you can get people

00:13:32.960 --> 00:13:38.496
to see the opposite view and engage
in a conversation with themselves,

00:13:38.520 --> 00:13:41.440
that could actually make them
change their views.

00:13:42.400 --> 00:13:43.600
OK.

00:13:44.760 --> 00:13:46.416
So what does it all mean?

00:13:46.440 --> 00:13:48.856
What do I think is going on here?

00:13:48.880 --> 00:13:50.096
So first of all,

00:13:50.120 --> 00:13:54.976
a lot of what we call self-knowledge
is actually self-interpretation.

00:13:55.000 --> 00:13:57.496
So I see myself make a choice,

00:13:57.520 --> 00:14:00.296
and then when I'm asked why,

00:14:00.320 --> 00:14:02.856
I just try to make
as much sense of it as possible

00:14:02.880 --> 00:14:04.816
when I make an explanation.

00:14:04.840 --> 00:14:07.856
But we do this so quickly
and with such ease

00:14:07.880 --> 00:14:12.160
that we think we actually know the answer
when we answer why.

00:14:13.040 --> 00:14:16.136
And as it is an interpretation,

00:14:16.160 --> 00:14:18.456
of course we sometimes make mistakes.

00:14:18.480 --> 00:14:22.000
The same way we make mistakes
when we try to understand other people.

00:14:23.160 --> 00:14:26.856
So beware when you ask people
the question "why"

00:14:26.880 --> 00:14:31.776
because what may happen
is that, if you asked them,

00:14:31.800 --> 00:14:35.816
"So why do you support this issue?"

00:14:35.840 --> 00:14:39.056
"Why do you stay in this job
or this relationship?" --

00:14:39.080 --> 00:14:42.496
what may happen when you ask why
is that you actually create an attitude

00:14:42.520 --> 00:14:44.760
that wasn't there
before you asked the question.

00:14:45.440 --> 00:14:48.616
And this is of course important
in your professional life, as well,

00:14:48.640 --> 00:14:49.856
or it could be.

00:14:49.880 --> 00:14:52.416
If, say, you design something
and then you ask people,

00:14:52.440 --> 00:14:54.696
"Why do you think this is good or bad?"

00:14:54.720 --> 00:14:57.776
Or if you're a journalist
asking a politician,

00:14:57.800 --> 00:15:00.176
"So, why did you make this decision?"

00:15:00.200 --> 00:15:02.136
Or if indeed you are a politician

00:15:02.160 --> 00:15:04.800
and try to explain
why a certain decision was made.

00:15:06.080 --> 00:15:09.656
So this may all seem a bit disturbing.

00:15:09.680 --> 00:15:13.176
But if you want to look at it
from a positive direction,

00:15:13.200 --> 00:15:14.936
it could be seen as showing,

00:15:14.960 --> 00:15:18.336
OK, so we're actually
a little bit more flexible than we think.

00:15:18.360 --> 00:15:20.256
We can change our minds.

00:15:20.280 --> 00:15:22.736
Our attitudes are not set in stone.

00:15:22.760 --> 00:15:25.936
And we can also change
the minds of others,

00:15:25.960 --> 00:15:28.336
if we can only get them
to engage with the issue

00:15:28.360 --> 00:15:30.040
and see it from the opposite view.

00:15:31.400 --> 00:15:35.336
And in my own personal life,
since starting with this research --

00:15:35.360 --> 00:15:37.936
So my partner and I,
we've always had the rule

00:15:37.960 --> 00:15:40.256
that you're allowed to take things back.

00:15:40.280 --> 00:15:42.616
Just because I said
I liked something a year ago,

00:15:42.640 --> 00:15:44.680
doesn't mean I have to like it still.

00:15:45.480 --> 00:15:48.296
And getting rid of the need
to stay consistent

00:15:48.320 --> 00:15:52.680
is actually a huge relief and makes
relational life so mush easier to live.

00:15:53.720 --> 00:15:56.080
Anyway, so the conclusion must be:

00:15:57.320 --> 00:15:59.816
know that you don't know yourself.

00:15:59.840 --> 00:16:02.160
Or at least not as well
as you think you do.

00:16:03.480 --> 00:16:04.696
Thanks.

00:16:04.720 --> 00:16:09.360
(Applause)

