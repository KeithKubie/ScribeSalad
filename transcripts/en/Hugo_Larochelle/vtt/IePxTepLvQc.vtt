WEBVTT

1
00:00:00.430 --> 00:00:01.110
Yeah.

2
00:00:01.110 --> 00:00:05.200
<v 1>In this video we'll see an online variant of the dictionary learning spar</v>

3
00:00:05.201 --> 00:00:09.570
scolding algorithm,
uh,
which is going to be able to scale to very large datasets.

4
00:00:11.640 --> 00:00:11.810
<v 0>Okay.</v>

5
00:00:11.810 --> 00:00:13.760
<v 1>We've seen there the previous video,
uh,</v>

6
00:00:13.830 --> 00:00:18.830
that we could learn a dictionary Matrix d for some giving Dataset by alternating

7
00:00:19.310 --> 00:00:24.310
between inferring the sparse codes for all of our training data and then

8
00:00:24.470 --> 00:00:27.020
updating the dictionary by,
uh,

9
00:00:27.200 --> 00:00:30.530
assuming that the part's codes are fixed and then finding the best dictionary

10
00:00:30.531 --> 00:00:34.390
for,
uh,
these sparse codes,
uh,
through,
uh,

11
00:00:34.440 --> 00:00:38.300
the computation of this,
uh,
a these a and B matrices.

12
00:00:38.301 --> 00:00:42.140
And then running block corner,
that corner that dissent,
uh,

13
00:00:42.200 --> 00:00:47.090
to get a new improved dictionary matrix and then performing alternating between

14
00:00:47.091 --> 00:00:50.450
these two updates until the dictionary matrix has not changed.

15
00:00:51.170 --> 00:00:56.170
Now one problem with this algorithm is that if tea is very large capital t,

16
00:00:57.451 --> 00:01:01.010
so the number of training inputs in our data set is very large.

17
00:01:01.370 --> 00:01:06.370
We'll be spending most of our time computing amb and we'll be performing only a

18
00:01:06.651 --> 00:01:10.650
single a dictionary update,
uh,

19
00:01:10.651 --> 00:01:13.580
after having visited all of the training data.

20
00:01:16.020 --> 00:01:16.300
<v 0>Yeah.</v>

21
00:01:16.300 --> 00:01:19.180
<v 1>So these types of algorithms that perform a single,</v>

22
00:01:19.340 --> 00:01:24.340
a new update of our parameters after a whole pass over the whole training set is

23
00:01:25.481 --> 00:01:30.130
known as a batch,
as a batch algorithm.
Uh,

24
00:01:30.131 --> 00:01:34.990
so it's batch as opposed to online where an online algorithm would be able to

25
00:01:34.991 --> 00:01:39.991
take a single new observation and update itself to adapt itself to the new

26
00:01:40.001 --> 00:01:40.834
observation.

27
00:01:41.080 --> 00:01:45.640
And one big advantage of an algorithm that's like this is that if I have a large

28
00:01:45.680 --> 00:01:47.310
Dataset,
uh,

29
00:01:47.320 --> 00:01:51.650
it will be able to make many updates of its parameters,
uh,

30
00:01:51.820 --> 00:01:55.480
before it,
uh,
before reaching the end of the Dataset.

31
00:01:55.750 --> 00:01:59.770
And it can also be applied in the context where we are constantly obtaining new

32
00:01:59.771 --> 00:02:02.590
data,
uh,
in an online fashion.
So for instance,

33
00:02:02.591 --> 00:02:05.980
if it were downloading images from the web constantly,

34
00:02:07.240 --> 00:02:11.260
and,
uh,
so that's,
uh,
that's one big problem.

35
00:02:11.410 --> 00:02:16.030
I want to say that I say it's a single update after each fast.

36
00:02:16.060 --> 00:02:19.300
Of course when we run corner the sense,
uh,

37
00:02:19.320 --> 00:02:23.160
d cornered the scent Algorithm Inter,
uh,
inside,

38
00:02:23.200 --> 00:02:27.970
it does many updates of the dictionary.
But,
so when I see single update,

39
00:02:27.971 --> 00:02:32.110
I mean that,
uh,
after calling once the block corner,

40
00:02:32.130 --> 00:02:34.220
the call and until the,

41
00:02:34.221 --> 00:02:37.920
once it has converged and give us a given us a new d,
uh,

42
00:02:37.930 --> 00:02:42.550
we get this new improved value of t,
uh,
after,

43
00:02:42.610 --> 00:02:45.960
uh,
only once after having passed over the whole training sets,

44
00:02:45.970 --> 00:02:48.800
I have income computed,
the A and B matrix.
That's it.

45
00:02:48.801 --> 00:02:52.840
So that's what I mean by a single update here.
And so,

46
00:02:52.841 --> 00:02:57.070
so this is not going to scale very well to very large datasets and a,

47
00:02:57.071 --> 00:02:59.820
so what we'd like is that we'd to get an argument,

48
00:02:59.821 --> 00:03:03.730
I can perform an update given a new observation just based on the new

49
00:03:03.731 --> 00:03:08.020
observation at that.
It's,
it's a dictionary a immediately.

50
00:03:08.800 --> 00:03:13.500
And so,
uh,
to get this,
we'll do the following if,
ah,

51
00:03:13.530 --> 00:03:15.860
two together and online parent of our algorithm.

52
00:03:16.270 --> 00:03:19.810
So given the new eight x team and so,

53
00:03:19.880 --> 00:03:24.010
and we'll be iterating over each [inaudible] we can get from our dataset picking

54
00:03:24.011 --> 00:03:25.720
one,
uh,
sequentially,

55
00:03:26.890 --> 00:03:31.890
we'll first performed the inference of its parts codes for that given the

56
00:03:32.651 --> 00:03:36.670
current training example.
And then,
um,

57
00:03:36.760 --> 00:03:40.870
what we'll do is that will updates are a and B matrix,
uh,

58
00:03:40.871 --> 00:03:42.790
using a running average.

59
00:03:42.820 --> 00:03:47.820
So what I mean by that is that we'll have an update of matrix B of the form.

60
00:03:48.071 --> 00:03:52.570
Well,
I'll take B,
uh,
better times B,

61
00:03:52.660 --> 00:03:57.660
and then I'll add that one minus better the new outer product,

62
00:03:57.790 --> 00:04:01.570
either x with age or age with itself.
Uh,

63
00:04:01.630 --> 00:04:04.480
so I'll incorporate that into my a running average.

64
00:04:04.510 --> 00:04:08.920
So here instead of t plus one,
it should be small team.

65
00:04:09.100 --> 00:04:13.540
It's the same t as,
as this t here.
And uh,

66
00:04:13.541 --> 00:04:17.440
so this means that EMV will be what is known as a running average.

67
00:04:17.441 --> 00:04:22.330
So it's an average that we keep updating every time we see a new example.
And,
uh,

68
00:04:22.360 --> 00:04:24.650
it's such that the latest,
uh,

69
00:04:24.950 --> 00:04:29.290
example we've incorporated into the running average have a bigger weight than

70
00:04:29.530 --> 00:04:33.250
examples that I've been incorporated,
uh,
uh,

71
00:04:33.310 --> 00:04:36.520
preview much more previously in the,
uh,
while the algorithm was running.

72
00:04:37.150 --> 00:04:41.280
And so a Beta is a number between zero and one,
and it's sort of,
uh,

73
00:04:41.670 --> 00:04:46.600
uh,
represents how much we want to,
uh,
uh,

74
00:04:46.690 --> 00:04:49.600
put importance into previously visited examples.

75
00:04:49.710 --> 00:04:53.410
Bet As close to one that it means that we'll put a lot of weight on the previous

76
00:04:53.411 --> 00:04:55.720
value Beta and less weight on the new value data.

77
00:04:56.200 --> 00:05:00.640
So unfortunately he will have a hyper parameter that we have to tune and it's a

78
00:05:00.641 --> 00:05:04.680
akin to the learning rates say to training,
uh,

79
00:05:05.140 --> 00:05:07.840
Natto in quarter or uh,

80
00:05:07.870 --> 00:05:11.170
a an RBM with some stochastic gradient descent procedure.

81
00:05:11.470 --> 00:05:13.690
So in that sense it's not such a big deal,

82
00:05:14.020 --> 00:05:17.390
but now we have to specify this better.
And so we'll,
uh,

83
00:05:17.500 --> 00:05:22.500
so to sum up then for the given like c and its latent sparse representation,

84
00:05:23.321 --> 00:05:28.321
we'll update our running average over AMD and then we'll use these matrices to

85
00:05:29.410 --> 00:05:33.220
update our current value of the dictionary matrix deep.

86
00:05:33.550 --> 00:05:37.930
And to update it.
We'll run the block corner descent algorithm,

87
00:05:38.200 --> 00:05:43.200
but we'll warm started or will start from the previous value of t.

88
00:05:43.720 --> 00:05:47.560
And now since based on just a new example,
XD,

89
00:05:47.561 --> 00:05:48.910
a single new example,

90
00:05:49.000 --> 00:05:54.000
we don't expect d to change too much then actually performing these block corn.

91
00:05:54.091 --> 00:05:57.980
The step shouldn't take too much time cause we already fairly close to a new

92
00:05:57.981 --> 00:06:02.390
solution,
uh,
to,
to a solution that incorporates the information from XD.

93
00:06:02.660 --> 00:06:05.340
And,
uh,
so,
uh,
this,
uh,

94
00:06:05.450 --> 00:06:07.960
part should converge very fairly equipment.

95
00:06:10.260 --> 00:06:12.960
So let's put everything into a single algorithm.

96
00:06:12.961 --> 00:06:17.370
And so the advantage of this algorithm,
uh,
which is probably the algorithm you,

97
00:06:17.371 --> 00:06:20.160
you really should implement if you're on the apply it to any type of data,

98
00:06:20.161 --> 00:06:22.530
even if your training data is small.
Uh,

99
00:06:22.570 --> 00:06:26.040
so this and my head is going to have the advantage of being able to scaling up

100
00:06:26.041 --> 00:06:29.310
to any size of training data.
Uh,
so,
so,
you know,

101
00:06:29.340 --> 00:06:34.110
I recommend implementing disparent instead as opposed to the previous one that

102
00:06:34.111 --> 00:06:35.670
alternates in that is batch.

103
00:06:36.180 --> 00:06:40.770
So the first step is to initialize deep.
So we had to do this also before,

104
00:06:40.771 --> 00:06:43.040
I just didn't talk about it yet.
Um,

105
00:06:43.250 --> 00:06:47.880
notice you should in initialize d two zero for one thing,
it doesn't respect the,

106
00:06:47.940 --> 00:06:52.770
uh,
unit norm constraint for the Adams.
And also if these equal to zero,

107
00:06:52.771 --> 00:06:56.370
then it's impossible to reconstruct the,
the,
uh,
uh,
the input.

108
00:06:56.370 --> 00:06:57.690
So you can get sort of in trouble.

109
00:06:57.691 --> 00:07:01.440
So can try to think about why the equals zero would be a problem.

110
00:07:01.820 --> 00:07:06.810
What I would recommend is that to initialize the can use a just random a matrix

111
00:07:06.930 --> 00:07:10.500
that you then normalize,
uh,
uh,

112
00:07:10.560 --> 00:07:15.510
each of the columns or you can take a,
if you have a say 10 items,

113
00:07:15.511 --> 00:07:19.230
you decided that you wanted there late and representation of Sitel which is very

114
00:07:19.231 --> 00:07:21.750
smart and normally you use bigger,
but say at 10,

115
00:07:22.020 --> 00:07:26.890
you could just take the first 10 example XD,
uh,

116
00:07:26.960 --> 00:07:31.960
normalize the examples to you that norm and use each example as a initial value

117
00:07:32.341 --> 00:07:33.260
for the columns of tea.

118
00:07:33.300 --> 00:07:37.410
That's something that people sometimes do and tends to work well in practice.

119
00:07:38.400 --> 00:07:42.980
So initialized d in some way and then until Diaz converged.

120
00:07:42.981 --> 00:07:46.400
So we'll assume that uh,
we're not in a,

121
00:07:46.460 --> 00:07:49.680
we're not using a stream of data when we have a data set and we uh,

122
00:07:50.070 --> 00:07:52.920
we are cycling over the example x team.

123
00:07:53.490 --> 00:07:57.960
Then for each XD we'll enforce for a will infer it's parsed code.

124
00:07:58.710 --> 00:08:03.710
We'll update the running averages and then we'll perform the block corner,

125
00:08:04.711 --> 00:08:08.970
the scent algorithm using the value,
the new value of Matrix and the,

126
00:08:09.630 --> 00:08:13.380
and so while the as not converge,
we'll uh,

127
00:08:13.410 --> 00:08:14.730
go over the columns,

128
00:08:14.731 --> 00:08:19.620
the Adams in matrix a d and then we'll perform the updates were familiar with

129
00:08:19.621 --> 00:08:23.660
that.
We've seen in the description of the block corner,
the set.
Okay.

130
00:08:23.670 --> 00:08:27.110
So now we have an algorithm that can learn for some given data,

131
00:08:27.111 --> 00:08:32.111
set a good dictionary matrix that will allow us to extract some sparse

132
00:08:32.581 --> 00:08:37.230
representations that can maintain some useful information about the input,

133
00:08:37.260 --> 00:08:40.830
but it is also spars.
And now this algorithm disparative.

134
00:08:40.890 --> 00:08:43.650
This algorithm can also scale to very large datasets.

135
00:08:46.920 --> 00:08:50.760
Well,
I should say that this algorithm was taken,
uh,
in,

136
00:08:50.761 --> 00:08:55.761
is highly inspired from this paper by [inaudible] back pounds and,

137
00:08:55.920 --> 00:09:00.210
uh,
Shapiro in,
uh,
2000,
uh,
nine,
uh,

138
00:09:00.270 --> 00:09:03.180
you can go look it up.
It's a very interesting paper.

