WEBVTT
Kind: captions
Language: en

00:00:04.990 --> 00:00:07.170
MEGAN SMITH: Hello, and
welcome to Women Techmakers.

00:00:07.170 --> 00:00:10.150
This is a series where we bring
incredible technical women

00:00:10.150 --> 00:00:11.080
to screen.

00:00:11.080 --> 00:00:16.120
And I'm joined by this amazing
robotics brain trust today.

00:00:16.120 --> 00:00:20.060
Cori Lanthan is the CEO and
founder of AnthroTronix,

00:00:20.060 --> 00:00:23.590
and she's going to tell us
about that and her other work.

00:00:23.590 --> 00:00:27.010
And Yoky Matsuoka is here,
who is the VP of technology

00:00:27.010 --> 00:00:29.760
at Nest, and she has an
incredible also robotics

00:00:29.760 --> 00:00:30.260
background.

00:00:30.260 --> 00:00:31.890
So welcome to both of you.

00:00:31.890 --> 00:00:32.119
YOKY MATSUOKA: Thanks.

00:00:32.119 --> 00:00:33.077
CORI LATHAN: Thank you.

00:00:33.077 --> 00:00:38.510
MEGAN SMITH: So I think that the
robotics future is so exciting,

00:00:38.510 --> 00:00:42.320
and you guys came in really
early into this world.

00:00:42.320 --> 00:00:45.570
You, through an
incredible athletic twist,

00:00:45.570 --> 00:00:49.000
which I want you to talk
about, that Yoky came in.

00:00:49.000 --> 00:00:52.570
And Cori, you through
the space entry.

00:00:52.570 --> 00:00:56.120
So maybe talk a
little bit about that,

00:00:56.120 --> 00:00:58.870
and we'll come to what
you're doing in a minute.

00:00:58.870 --> 00:01:00.440
So how did you get
into this world?

00:01:00.440 --> 00:01:02.400
YOKY MATSUOKA:
Yeah, so it's really

00:01:02.400 --> 00:01:04.840
different from many people--
how people get into robotics.

00:01:04.840 --> 00:01:08.170
But when I was growing up, there
were no initiatives with robots

00:01:08.170 --> 00:01:09.020
yet.

00:01:09.020 --> 00:01:11.510
And I was a tennis
player-- tennis junkie--

00:01:11.510 --> 00:01:13.170
that's all I knew how to do.

00:01:13.170 --> 00:01:15.450
And in college, when I was
getting a lot of injuries,

00:01:15.450 --> 00:01:17.330
I was searching
for something else.

00:01:17.330 --> 00:01:20.420
And I picked the most
athletically interesting

00:01:20.420 --> 00:01:22.770
engineering, which
happened to be robotics.

00:01:22.770 --> 00:01:26.600
So I stumbled upon a lab that
was building a hopping robot.

00:01:26.600 --> 00:01:28.590
And I thought, this is
kind of like tennis.

00:01:28.590 --> 00:01:31.230
If I'm going to build a little
tennis buddy robot for myself,

00:01:31.230 --> 00:01:34.210
then I've got to learn how
to do this hopping and legged

00:01:34.210 --> 00:01:35.240
robotic work.

00:01:35.240 --> 00:01:36.324
So that's where I started.

00:01:36.324 --> 00:01:38.781
MEGAN SMITH: One of the things
that happens a lot in school

00:01:38.781 --> 00:01:40.916
is that some of the ways
we teach science and math

00:01:40.916 --> 00:01:43.290
kind of turn kids off to it,
because they're not actually

00:01:43.290 --> 00:01:43.820
doing it.

00:01:43.820 --> 00:01:44.960
They're learning
the history of it.

00:01:44.960 --> 00:01:46.790
So they don't really
experience how to make stuff.

00:01:46.790 --> 00:01:47.480
YOKY MATSUOKA: That's so true.

00:01:47.480 --> 00:01:48.060
Yeah, actually--

00:01:48.060 --> 00:01:50.310
MEGAN SMITH: Was there
something early in your history

00:01:50.310 --> 00:01:51.811
that taught you,
hey, I can do this?

00:01:51.811 --> 00:01:53.601
YOKY MATSUOKA: You know
what, I didn't even

00:01:53.601 --> 00:01:55.480
connect that math
and all those things

00:01:55.480 --> 00:01:57.330
are connected to
what I wanted to do.

00:01:57.330 --> 00:02:00.250
I liked math, and I was
good at it, but that was it.

00:02:00.250 --> 00:02:02.340
MEGAN SMITH: So you
can much later to this.

00:02:02.340 --> 00:02:04.050
YOKY MATSUOKA: My brain
was totally somewhere else.

00:02:04.050 --> 00:02:04.830
MEGAN SMITH: And
you were nationally

00:02:04.830 --> 00:02:05.860
ranked tennis player.

00:02:05.860 --> 00:02:06.400
YOKY MATSUOKA: Yeah.

00:02:06.400 --> 00:02:06.850
Nationally ranked.

00:02:06.850 --> 00:02:07.160
Yeah.

00:02:07.160 --> 00:02:07.570
That's right.

00:02:07.570 --> 00:02:08.070
Yeah.

00:02:08.070 --> 00:02:09.789
And then the
connection happened--

00:02:09.789 --> 00:02:10.970
like robotics, that's fun.

00:02:10.970 --> 00:02:12.290
I want to build things.

00:02:12.290 --> 00:02:14.040
That would be really fun to do.

00:02:14.040 --> 00:02:16.770
And then turns out, it's
like, oh, math is useful.

00:02:16.770 --> 00:02:18.780
And computer science is useful.

00:02:18.780 --> 00:02:22.860
All those things slowly
start to come into places.

00:02:22.860 --> 00:02:25.500
MEGAN SMITH: Cori, we
have a funny overlap

00:02:25.500 --> 00:02:30.100
because my freshman job was
with Professor Dave Akin at MIT,

00:02:30.100 --> 00:02:32.150
and that building
machining stuff

00:02:32.150 --> 00:02:34.110
as a junior person on
the space systems lab.

00:02:34.110 --> 00:02:38.089
And he later went to Maryland,
where you were on the faculty

00:02:38.089 --> 00:02:39.880
adjunct, and you worked
with them, I think.

00:02:39.880 --> 00:02:42.502
So I know that you did a lot
of really interesting work

00:02:42.502 --> 00:02:44.960
with robotics and people in
space, putting things together.

00:02:44.960 --> 00:02:46.168
Talk a little bit about that.

00:02:46.168 --> 00:02:48.180
And also, early on,
how you'd come in?

00:02:48.180 --> 00:02:50.610
CORI LATHAN: Well,
my interest really

00:02:50.610 --> 00:02:52.130
came from being a space junkie.

00:02:52.130 --> 00:02:54.360
So you were a tennis junkie,
I was a space junkie.

00:02:54.360 --> 00:02:56.260
And I was fascinated
with science fiction.

00:02:56.260 --> 00:02:58.450
I read science
fiction voraciously

00:02:58.450 --> 00:03:00.370
and loved Star Trek.

00:03:00.370 --> 00:03:03.930
And so I think that
combined with-- I

00:03:03.930 --> 00:03:06.810
really wanted to know, how do
humans live and work in space.

00:03:06.810 --> 00:03:08.330
I wanted to be an astronaut.

00:03:08.330 --> 00:03:12.380
And so that is what inspired
me to learn more about what

00:03:12.380 --> 00:03:15.220
does it take to have humans
live and work in space.

00:03:15.220 --> 00:03:17.920
And that's what propelled me, I
think, into a graduate program

00:03:17.920 --> 00:03:20.180
of aerospace
engineering, and then

00:03:20.180 --> 00:03:22.280
finding out the
technologies that

00:03:22.280 --> 00:03:24.409
were emerging that
could enable this.

00:03:24.409 --> 00:03:26.575
Certainly we know about
robots from science fiction.

00:03:26.575 --> 00:03:29.920
Well, what robots were
actually available?

00:03:29.920 --> 00:03:32.020
Or what technologies
were actually available?

00:03:32.020 --> 00:03:34.304
And so it was, at the
time, a very exciting time

00:03:34.304 --> 00:03:34.970
to make it real.

00:03:34.970 --> 00:03:36.280
Yeah.

00:03:36.280 --> 00:03:37.680
MEGAN SMITH: Very cool.

00:03:37.680 --> 00:03:39.270
It's interesting
about Star Trek,

00:03:39.270 --> 00:03:43.867
because I was watching the
Maker Series, makers.com,

00:03:43.867 --> 00:03:45.950
which is all these incredible
stories about women.

00:03:45.950 --> 00:03:48.790
And they had Nichelle Nichols,
who played Lieutenant Uhura,

00:03:48.790 --> 00:03:51.400
and there was a
moment that she talked

00:03:51.400 --> 00:03:54.340
about where Martin Luther
King came and was talking

00:03:54.340 --> 00:03:56.780
to her about why it was
so important that she was

00:03:56.780 --> 00:04:01.090
in the team-- they need women
and minorities in that team.

00:04:01.090 --> 00:04:03.640
He actually said that it was
so important for civil rights

00:04:03.640 --> 00:04:06.060
for her to be in
everyone's living room.

00:04:06.060 --> 00:04:09.762
And I think the role modeling
of media and stories come out,

00:04:09.762 --> 00:04:10.470
it's interesting.

00:04:10.470 --> 00:04:12.261
CORI LATHAN: Yeah, and
that actually brings

00:04:12.261 --> 00:04:16.000
to mind-- there was a great
Star Trek episode between Spock

00:04:16.000 --> 00:04:18.589
and Uhura where
she's freaking out

00:04:18.589 --> 00:04:21.010
because they're in some
crisis and she said,

00:04:21.010 --> 00:04:22.520
I don't know if I can do this.

00:04:22.520 --> 00:04:25.330
And Spock says, I can think of
no better person for the job.

00:04:25.330 --> 00:04:26.080
MEGAN SMITH: Nice.

00:04:26.080 --> 00:04:28.602
CORI LATHAN: And I'm like, yes!

00:04:28.602 --> 00:04:29.810
MEGAN SMITH: It's incredible.

00:04:29.810 --> 00:04:32.030
And that's actually a theme.

00:04:32.030 --> 00:04:34.260
You notice that
sometimes women sometimes

00:04:34.260 --> 00:04:35.850
don't have as much confidence.

00:04:35.850 --> 00:04:39.250
And so to have a colleague, in
this case a fictional character

00:04:39.250 --> 00:04:41.630
colleague, just
reinforce how qualified

00:04:41.630 --> 00:04:44.290
you are is a really great thing.

00:04:44.290 --> 00:04:46.250
So, nice media moment.

00:04:46.250 --> 00:04:50.285
So now you guys have moved
out of those original places

00:04:50.285 --> 00:04:53.080
and into your current
roles, which are just

00:04:53.080 --> 00:04:54.740
incredibly breakthrough places.

00:04:54.740 --> 00:04:56.750
So can you guys
talk a little bit

00:04:56.750 --> 00:04:59.070
about Nest-- what you guys
are trying to achieve,

00:04:59.070 --> 00:05:00.770
what your vision is there?

00:05:00.770 --> 00:05:02.520
Talk a little bit
about AnthroTronix

00:05:02.520 --> 00:05:03.930
and what the vision is there.

00:05:03.930 --> 00:05:05.495
So maybe go with Yoky.

00:05:05.495 --> 00:05:06.870
YOKY MATSUOKA:
Sure, I can start.

00:05:06.870 --> 00:05:11.440
I think my passion
with Nest is really

00:05:11.440 --> 00:05:14.130
about human interaction
with devices.

00:05:14.130 --> 00:05:17.180
And what we build at Nest
is no longer something

00:05:17.180 --> 00:05:18.770
that people might
imagine as robots.

00:05:18.770 --> 00:05:20.410
It's mounted on the wall.

00:05:20.410 --> 00:05:21.290
It doesn't have legs.

00:05:21.290 --> 00:05:22.270
It doesn't move.

00:05:22.270 --> 00:05:25.020
But it's essentially
the same thing.

00:05:25.020 --> 00:05:27.580
It actually controls
house temperature.

00:05:27.580 --> 00:05:30.230
It monitors through sensors
about all the things

00:05:30.230 --> 00:05:31.500
that's happening in the house.

00:05:31.500 --> 00:05:33.860
So we still consider
it to be a robot.

00:05:33.860 --> 00:05:36.230
And so now, what can we do?

00:05:36.230 --> 00:05:39.740
Really the true goal
is about letting people

00:05:39.740 --> 00:05:41.980
be who they want to
be, because now they're

00:05:41.980 --> 00:05:44.449
living with some intelligent
technology on the wall.

00:05:44.449 --> 00:05:46.990
MEGAN SMITH: So not try to have
them adapt to the technology,

00:05:46.990 --> 00:05:49.215
but have the technology
be more fluid.

00:05:49.215 --> 00:05:52.160
YOKY MATSUOKA: Yeah, to let
them be who they want to be.

00:05:52.160 --> 00:05:53.330
So here's an example.

00:05:53.330 --> 00:05:56.440
So Nest thermostat--
turns out 50%

00:05:56.440 --> 00:05:58.500
of the energy used in
a residential space

00:05:58.500 --> 00:06:00.500
is by heating and
cooling the house.

00:06:00.500 --> 00:06:01.650
We didn't know that.

00:06:01.650 --> 00:06:05.100
And so if we could actually
even save 10% or 20% of that,

00:06:05.100 --> 00:06:06.950
that's a huge chunk of it.

00:06:06.950 --> 00:06:08.320
So how can we do it?

00:06:08.320 --> 00:06:11.540
And if we ask people, do
you like to save energy?

00:06:11.540 --> 00:06:13.330
Everybody says, of course I do.

00:06:13.330 --> 00:06:15.580
And then if you say, well,
did you turn off the lights

00:06:15.580 --> 00:06:16.371
when you left home?

00:06:16.371 --> 00:06:17.456
They say, yes.

00:06:17.456 --> 00:06:19.830
And then when you say, well,
did you turn off the heating

00:06:19.830 --> 00:06:20.713
when you left home?

00:06:20.713 --> 00:06:22.205
They're like, um,
I can't remember.

00:06:24.980 --> 00:06:25.940
Lighting is 10%.

00:06:25.940 --> 00:06:27.191
Heating is 50%.

00:06:27.191 --> 00:06:29.440
Why do we know to turn off
the light, but not heating?

00:06:29.440 --> 00:06:31.740
So why don't we augment
with the technology?

00:06:31.740 --> 00:06:33.860
When we know that
they've left the house,

00:06:33.860 --> 00:06:35.500
why don't we turn
it down for them?

00:06:35.500 --> 00:06:37.915
So those are the places where
we're a little bit weak at,

00:06:37.915 --> 00:06:39.350
but technology's good at.

00:06:39.350 --> 00:06:42.100
Why don't we understand
where that right intersection

00:06:42.100 --> 00:06:43.680
is and do the right
thing for them,

00:06:43.680 --> 00:06:45.580
without removing
control from people?

00:06:45.580 --> 00:06:46.630
MEGAN SMITH: Right.

00:06:46.630 --> 00:06:49.070
Because you let people
control from their app.

00:06:49.070 --> 00:06:50.777
They can control from
the device itself.

00:06:50.777 --> 00:06:51.610
YOKY MATSUOKA: Yeah.

00:06:51.610 --> 00:06:53.990
And then there is a schedule
that we learn over time,

00:06:53.990 --> 00:06:56.930
because people are relatively
predictable when they leave.

00:06:56.930 --> 00:06:59.500
So the machines, they learn
for them, but at the same time,

00:06:59.500 --> 00:07:01.340
people can always override them.

00:07:01.340 --> 00:07:02.590
And it's incredibly important.

00:07:02.590 --> 00:07:04.673
Through even experiments,
it's really interesting.

00:07:04.673 --> 00:07:07.390
Through the field trial, we
learned that the moment we

00:07:07.390 --> 00:07:10.960
interfere a little bit with
people's decision making,

00:07:10.960 --> 00:07:11.872
they get all mad.

00:07:11.872 --> 00:07:13.330
So that's one of
the things that we

00:07:13.330 --> 00:07:17.250
learn as well through robotics.

00:07:17.250 --> 00:07:19.390
We really respect
what they want,

00:07:19.390 --> 00:07:22.380
and then still try to achieve
the same goal with them.

00:07:22.380 --> 00:07:24.950
MEGAN SMITH: It's interesting,
because from science fiction,

00:07:24.950 --> 00:07:27.210
maybe we think robots
are like Terminator--

00:07:27.210 --> 00:07:28.830
these extreme things.

00:07:28.830 --> 00:07:32.240
And perhaps some of them
will be humanoid in that way,

00:07:32.240 --> 00:07:36.700
but I think the point that
robotic or assistive technology

00:07:36.700 --> 00:07:39.230
can be exactly in the
direction that NASA's going,

00:07:39.230 --> 00:07:41.712
or what Roomba was doing
with cleaning robots.

00:07:41.712 --> 00:07:42.920
YOKY MATSUOKA: Task specific.

00:07:42.920 --> 00:07:45.378
MEGAN SMITH: I work with some
of my colleagues at Google X,

00:07:45.378 --> 00:07:46.980
and they have the
self-driving car.

00:07:46.980 --> 00:07:49.060
So that's really
kind of a robot,

00:07:49.060 --> 00:07:51.087
in a way-- taking
a transport robot.

00:07:51.087 --> 00:07:52.170
YOKY MATSUOKA: Completely.

00:07:52.170 --> 00:07:52.570
Yeah,

00:07:52.570 --> 00:07:54.736
MEGAN SMITH: So thinking
about robotics in that way.

00:07:54.736 --> 00:07:56.882
So say--

00:07:56.882 --> 00:07:58.340
CORI LATHAN: Well
I'd love to build

00:07:58.340 --> 00:08:01.230
on what Yoky was saying,
because it really resonates

00:08:01.230 --> 00:08:02.780
with the work that we're doing.

00:08:02.780 --> 00:08:05.720
I mean, we've been hearing
about the internet of things.

00:08:05.720 --> 00:08:07.690
And what that means
to me practically

00:08:07.690 --> 00:08:10.680
is the traditional
idea of a robot, which

00:08:10.680 --> 00:08:16.280
is this humanoid-type thing
or this standalone piece

00:08:16.280 --> 00:08:20.060
of machinery, has really changed
over the past 10 or 15 years

00:08:20.060 --> 00:08:24.550
as the consumer electronics
revolution has exploded.

00:08:24.550 --> 00:08:28.580
So a robot is simply a
sensor, some processing,

00:08:28.580 --> 00:08:29.510
and an actuator.

00:08:29.510 --> 00:08:32.974
So our smartphones
are now robots.

00:08:32.974 --> 00:08:34.140
YOKY MATSUOKA: That's right.

00:08:34.140 --> 00:08:37.340
CORI LATHAN: So the work
you're doing, as you said,

00:08:37.340 --> 00:08:40.340
it's still robotics.

00:08:40.340 --> 00:08:42.390
And so bringing it back
to some of the things

00:08:42.390 --> 00:08:45.420
that I've been doing in
my company, AnthroTronix,

00:08:45.420 --> 00:08:48.870
so Anthro, human,
Tronix, instrumentation.

00:08:48.870 --> 00:08:52.860
And we've looked at for
the past 10 or 15 years,

00:08:52.860 --> 00:08:57.170
how do you capture human
capability and use that

00:08:57.170 --> 00:09:01.450
to interface to an
electronic platform?

00:09:01.450 --> 00:09:03.760
And in the early days,
that electronic platform

00:09:03.760 --> 00:09:04.930
was a robot.

00:09:04.930 --> 00:09:06.750
So for example, we
worked with kids

00:09:06.750 --> 00:09:10.250
with disabilities trying
to capture their movements.

00:09:10.250 --> 00:09:14.180
So a child with Cerebral Palsy
who had very spastic movements,

00:09:14.180 --> 00:09:16.050
we could capture
those, and they could

00:09:16.050 --> 00:09:18.140
operate a robot
who could explore

00:09:18.140 --> 00:09:20.920
their environment, which was
something they couldn't do.

00:09:20.920 --> 00:09:27.230
So now, we fast forward,
and we're doing things

00:09:27.230 --> 00:09:30.900
like taking a
smartphone and capturing

00:09:30.900 --> 00:09:33.410
both passive and
active movements

00:09:33.410 --> 00:09:38.830
to learn about the status
of your brain function,

00:09:38.830 --> 00:09:42.020
of your capabilities,
and use that information

00:09:42.020 --> 00:09:45.210
to then operate on the world.

00:09:45.210 --> 00:09:47.420
And in case with kids,
it might be a robot,

00:09:47.420 --> 00:09:48.545
but it could be a computer.

00:09:48.545 --> 00:09:53.250
In case of the elderly, it might
be something in your kitchen.

00:09:53.250 --> 00:09:57.370
So the concept of taking
active and passive monitoring

00:09:57.370 --> 00:10:00.220
and capturing that and using
that to interface to technology

00:10:00.220 --> 00:10:04.056
is still very relevant today,
as it was 15 years ago.

00:10:04.056 --> 00:10:06.210
MEGAN SMITH: Very much so.

00:10:06.210 --> 00:10:09.050
Also I was thinking about
education a little bit-- just

00:10:09.050 --> 00:10:11.870
the ability to be
watching or sensing

00:10:11.870 --> 00:10:14.660
what the kids are feeling,
or how much they're actually

00:10:14.660 --> 00:10:16.680
learning, and leveraging
them [INAUDIBLE].

00:10:16.680 --> 00:10:17.430
CORI LATHAN: Yeah.

00:10:17.430 --> 00:10:20.960
And I think there's
actually a lot of interest

00:10:20.960 --> 00:10:24.050
right now in brain-computer
interfaces and the concept

00:10:24.050 --> 00:10:28.110
of an intelligent
tutor as something

00:10:28.110 --> 00:10:31.180
that actually can sense
how well are you learning.

00:10:31.180 --> 00:10:34.290
And the technologies that
are being used to sense that

00:10:34.290 --> 00:10:35.360
are really changing.

00:10:35.360 --> 00:10:39.760
I mean, we now have EEG that's
accessible to consumers that

00:10:39.760 --> 00:10:42.100
can say, yeah, you
really are getting it.

00:10:42.100 --> 00:10:45.020
And so one of the things
that we've been working on

00:10:45.020 --> 00:10:47.360
with the Navy is
actually team training.

00:10:47.360 --> 00:10:49.830
How can you measure both
physiological and brain

00:10:49.830 --> 00:10:54.060
response and other
aspects of the training

00:10:54.060 --> 00:10:56.830
and the learning to say, is
the team really a good team?

00:10:56.830 --> 00:10:58.700
Is the team learning
well together,

00:10:58.700 --> 00:10:59.860
not just the individual?

00:10:59.860 --> 00:11:02.276
MEGAN SMITH: That's interesting,
because Regina Dugan, who

00:11:02.276 --> 00:11:05.850
led DARPA, who's now at
Google-- was at Motorola--

00:11:05.850 --> 00:11:08.340
she was talking about some
work in the sports area

00:11:08.340 --> 00:11:11.490
around-- they were calling it
hustle on a basketball court.

00:11:11.490 --> 00:11:15.080
So not just looking
as a team at when

00:11:15.080 --> 00:11:17.710
you have an astonishing player
like Michael Jordan or any

00:11:17.710 --> 00:11:20.630
of these folks performing, but
when, Michael's doing really

00:11:20.630 --> 00:11:22.860
well, who else is on the court.

00:11:22.860 --> 00:11:25.600
And so who has amazing
hustle to play really

00:11:25.600 --> 00:11:28.710
well in that context, and
the instrumentation of sports

00:11:28.710 --> 00:11:31.580
in general, but the
instrumentation on behalf

00:11:31.580 --> 00:11:34.200
of understanding teamwork
and collaboration,

00:11:34.200 --> 00:11:36.877
both in that arena, but in any
arena is pretty interesting.

00:11:36.877 --> 00:11:38.210
YOKY MATSUOKA: It's interesting.

00:11:38.210 --> 00:11:40.970
Actually, before Nest,
the life that I had

00:11:40.970 --> 00:11:45.220
was a professor also working in
this neuroscience and robotics

00:11:45.220 --> 00:11:45.847
area.

00:11:45.847 --> 00:11:47.430
And one of the things
that we realized

00:11:47.430 --> 00:11:51.830
is that we can measure effort
through just robot contacting

00:11:51.830 --> 00:11:55.890
with humans, and then simply
observing movements and how

00:11:55.890 --> 00:11:57.790
active the movements
are, and what's

00:11:57.790 --> 00:11:59.244
normal, what's not normal.

00:11:59.244 --> 00:12:01.160
Those are the things
that really came through.

00:12:01.160 --> 00:12:04.580
So in context of
stroke rehabilitation,

00:12:04.580 --> 00:12:06.170
we realized that
we can even tell

00:12:06.170 --> 00:12:08.640
when people were paying
attention or making

00:12:08.640 --> 00:12:11.200
really good progress
compared to the last week.

00:12:11.200 --> 00:12:13.575
And I think those things are
incredibly useful to mix in,

00:12:13.575 --> 00:12:15.116
and as you say, with
the teams sports

00:12:15.116 --> 00:12:18.030
too, who is currently putting
in, almost like hustling a lot,

00:12:18.030 --> 00:12:21.087
and then who are the great
players to be combined with.

00:12:21.087 --> 00:12:22.670
And then there's a
whole lot of things

00:12:22.670 --> 00:12:24.544
that we might not be
able to bring out simply

00:12:24.544 --> 00:12:28.940
by just-- humans are good
at getting those things out,

00:12:28.940 --> 00:12:32.240
but I think just getting those
things out in a robotic form,

00:12:32.240 --> 00:12:34.680
and especially in dealing
with people with disabilities,

00:12:34.680 --> 00:12:36.677
I think that's
really fascinating.

00:12:36.677 --> 00:12:38.510
MEGAN SMITH: It's
interesting because we all

00:12:38.510 --> 00:12:42.120
have some part of MIT in
common, and there's a new plan

00:12:42.120 --> 00:12:46.510
to start a center for
advanced human mechatronics,

00:12:46.510 --> 00:12:48.800
and how do you help people.

00:12:48.800 --> 00:12:51.210
Specifically, Hugh
Herr is leading that.

00:12:51.210 --> 00:12:54.070
And he's someone who lost
his lower legs in a climbing

00:12:54.070 --> 00:12:57.100
accident and made his own
amazing robotics legs.

00:12:57.100 --> 00:12:59.920
And I encourage people to
watch the Ted Talk that he did.

00:13:02.460 --> 00:13:05.300
On the Ted stage, he
was saying, can we

00:13:05.300 --> 00:13:11.770
eliminate disability in general
by adopting these technologies?

00:13:11.770 --> 00:13:14.920
And I think it's interesting,
the leveraging, what

00:13:14.920 --> 00:13:17.985
you were talking about, the
effort, encouraging people,

00:13:17.985 --> 00:13:19.360
knowing what's
going on with them

00:13:19.360 --> 00:13:21.776
and helping to encourage them
so they can bring themselves

00:13:21.776 --> 00:13:22.520
further ahead.

00:13:22.520 --> 00:13:24.550
That's pretty interesting
medical opportunity

00:13:24.550 --> 00:13:27.800
and learning opportunity.

00:13:27.800 --> 00:13:30.050
YOKY MATSUOKA: We also have
a neuroscience background,

00:13:30.050 --> 00:13:34.290
but there are
things that we don't

00:13:34.290 --> 00:13:36.650
take advantage of that we know.

00:13:36.650 --> 00:13:39.180
For example, when
people have stroke,

00:13:39.180 --> 00:13:42.670
there's a lot of time that
passes sitting in a hospital,

00:13:42.670 --> 00:13:45.090
then going home,
and then their brain

00:13:45.090 --> 00:13:46.632
is almost getting
worse and worse.

00:13:46.632 --> 00:13:48.090
And we already know
in neuroscience

00:13:48.090 --> 00:13:50.339
of like how long should we
wait, how long should we--

00:13:50.339 --> 00:13:51.880
at some point, we
should aggressively

00:13:51.880 --> 00:13:53.080
start rehabilitating.

00:13:53.080 --> 00:13:56.020
So what if technology was
available to immerse them

00:13:56.020 --> 00:13:58.870
in a right way at the right
time, already in the hospital?

00:13:58.870 --> 00:14:02.410
So this big picture of a
virtual reality environment

00:14:02.410 --> 00:14:05.590
where we put them, and
then they don't actually

00:14:05.590 --> 00:14:08.230
even get to experience
this getting worse,

00:14:08.230 --> 00:14:11.210
my movements are not
as good, but continue

00:14:11.210 --> 00:14:12.820
to stimulate the
brain, so that when

00:14:12.820 --> 00:14:14.990
they are ready to walk
out of the hospital,

00:14:14.990 --> 00:14:16.530
they're much better off.

00:14:16.530 --> 00:14:18.970
You know what's
interesting, I know

00:14:18.970 --> 00:14:21.050
Mark Kelly, who's an
amazing astronaut,

00:14:21.050 --> 00:14:24.410
and Gabby Giffords, his
wife, amazing Congresswoman

00:14:24.410 --> 00:14:26.530
who was, unfortunately, shot.

00:14:26.530 --> 00:14:29.886
Mark shared that when she went
to the hospital in Houston,

00:14:29.886 --> 00:14:32.510
that one of the things they did
was exactly as you were saying.

00:14:32.510 --> 00:14:36.410
Very early on, she was not
really quite conscious,

00:14:36.410 --> 00:14:37.940
but they would lift
her up and begin

00:14:37.940 --> 00:14:41.790
to do things-- help her body
physically-- her brain began

00:14:41.790 --> 00:14:44.130
to work again with her
body in an interesting way.

00:14:44.130 --> 00:14:48.302
So the speed to
helping someone recover

00:14:48.302 --> 00:14:50.260
turns out to be, as you
said, really important.

00:14:50.260 --> 00:14:52.160
So leveraging robotics
for that is interesting.

00:14:52.160 --> 00:14:54.493
CORI LATHAN: And let me just
build on that for a moment.

00:14:54.493 --> 00:14:57.720
I think the idea of
eliminating disability

00:14:57.720 --> 00:15:01.920
is an amazing moon shot
idea that actually we

00:15:01.920 --> 00:15:02.690
could go after.

00:15:02.690 --> 00:15:04.960
And it's not just
eliminating disability.

00:15:04.960 --> 00:15:07.300
How about eliminating aging?

00:15:07.300 --> 00:15:08.910
And I don't mean
in this longevity,

00:15:08.910 --> 00:15:09.990
you live forever thing.

00:15:09.990 --> 00:15:13.660
I mean you eliminate
the disability

00:15:13.660 --> 00:15:16.880
that seems to define aging.

00:15:16.880 --> 00:15:19.750
As you get older, you have
the reduction of skills

00:15:19.750 --> 00:15:21.840
and the reduction of activities.

00:15:21.840 --> 00:15:24.200
If we could eliminate
that, I would

00:15:24.200 --> 00:15:26.490
argue you've eliminated
aging as well.

00:15:26.490 --> 00:15:28.870
And I think some of these
technologies that we're

00:15:28.870 --> 00:15:31.995
talking about are
the key to that.

00:15:31.995 --> 00:15:33.370
MEGAN SMITH: So
one of the themes

00:15:33.370 --> 00:15:37.430
here is very much technology
in service of a purpose,

00:15:37.430 --> 00:15:38.190
of impact.

00:15:38.190 --> 00:15:43.780
And I know both of you guys
are involved a lot in helping

00:15:43.780 --> 00:15:46.270
younger people see and
come into this field.

00:15:46.270 --> 00:15:49.200
You have an organization
that you either founded

00:15:49.200 --> 00:15:50.029
or are a part of.

00:15:50.029 --> 00:15:51.570
Can you talk a little
bit about that?

00:15:51.570 --> 00:15:55.740
But I think somehow with
young people especially--

00:15:55.740 --> 00:15:58.350
sometimes, as we said,
today in K to 12,

00:15:58.350 --> 00:16:01.280
we're teaching science and
math in this boring way,

00:16:01.280 --> 00:16:04.340
rather than, here's the stuff
that could really matter

00:16:04.340 --> 00:16:05.240
for people's lives.

00:16:05.240 --> 00:16:07.323
And by the way, you happen
to use science and math

00:16:07.323 --> 00:16:10.620
as the language for it, and
bring it with the purpose.

00:16:10.620 --> 00:16:12.730
So maybe say a little bit
about the organization

00:16:12.730 --> 00:16:13.771
and what you're thinking.

00:16:13.771 --> 00:16:16.330
CORI LATHAN: Sure.

00:16:16.330 --> 00:16:19.680
I founded one organization, now
I'm actually part of another.

00:16:19.680 --> 00:16:21.550
The organization that
I founded while I

00:16:21.550 --> 00:16:23.650
was a grad student
at MIT is called Keys

00:16:23.650 --> 00:16:25.070
to Empowering Youth.

00:16:25.070 --> 00:16:29.550
And it was really my
"aha" moment, that really,

00:16:29.550 --> 00:16:31.380
until I got to
graduate school, I

00:16:31.380 --> 00:16:34.617
didn't know that women didn't go
in to science and engineering.

00:16:34.617 --> 00:16:35.950
MEGAN SMITH: That's interesting.

00:16:35.950 --> 00:16:36.940
Why was that?

00:16:36.940 --> 00:16:41.190
CORI LATHAN: I think maybe
it's this personality of women

00:16:41.190 --> 00:16:43.725
who-- I don't know.

00:16:43.725 --> 00:16:45.350
YOKY MATSUOKA: I
totally agree, though.

00:16:45.350 --> 00:16:46.940
Ignorance helps a little bit.

00:16:46.940 --> 00:16:48.940
MEGAN SMITH: You know, the
women in physics say this, too.

00:16:48.940 --> 00:16:50.939
Because the two areas
that continued to be lower

00:16:50.939 --> 00:16:54.450
are computer science, 15% to
20%, and physics, 15% to 20%.

00:16:54.450 --> 00:16:56.430
But people just power through.

00:16:56.430 --> 00:16:59.030
CORI LATHAN: Yeah.

00:16:59.030 --> 00:17:01.030
Until I got to grad
school, I really just

00:17:01.030 --> 00:17:03.060
didn't even know that
this was an issue.

00:17:03.060 --> 00:17:06.609
And so it really hit me
like a ton of bricks.

00:17:06.609 --> 00:17:10.589
I mean, you're at MIT, which
the undergraduate population,

00:17:10.589 --> 00:17:12.660
they're doing a much
better job at getting--

00:17:12.660 --> 00:17:13.770
MEGAN SMITH: Yeah,
they're at almost 50/50,

00:17:13.770 --> 00:17:14.800
or they're basically even.

00:17:14.800 --> 00:17:16.966
CORI LATHAN: I don't know
if the graduate population

00:17:16.966 --> 00:17:19.334
and the faculty has
budged from 20%--

00:17:19.334 --> 00:17:20.500
MEGAN SMITH: It's coming up.

00:17:20.500 --> 00:17:24.569
So you tend to see grad
school 30%, and then

00:17:24.569 --> 00:17:28.160
the faculty anywhere from
10% in certain fields

00:17:28.160 --> 00:17:29.930
to actually like 30% or 40%.

00:17:29.930 --> 00:17:30.755
It just depends.

00:17:30.755 --> 00:17:32.880
Chemical engineering is
very balanced, for example.

00:17:32.880 --> 00:17:35.650
Or in some of the sciences,
like biology, are very balanced.

00:17:35.650 --> 00:17:39.137
But a couple of the engineering
still, we've got to push them.

00:17:39.137 --> 00:17:41.470
YOKY MATSUOKA: Megan know
this thing about MIT actually.

00:17:41.470 --> 00:17:41.774
MEGAN SMITH: I do.

00:17:41.774 --> 00:17:42.880
I'm on their board.

00:17:42.880 --> 00:17:44.824
[INTERPOSING VOICES]

00:17:44.824 --> 00:17:45.990
Because we're working on it.

00:17:45.990 --> 00:17:46.970
It's important.

00:17:46.970 --> 00:17:50.840
CORI LATHAN: I got
there in '89, and I

00:17:50.840 --> 00:17:53.480
was one of two women
in my entering class

00:17:53.480 --> 00:17:59.572
for graduate school, and often
the only woman in a class.

00:17:59.572 --> 00:18:01.280
So it really hit me
like a ton of bricks.

00:18:01.280 --> 00:18:04.230
And so I started doing all
reading and the research

00:18:04.230 --> 00:18:06.960
and finding out all the
things that everyone already

00:18:06.960 --> 00:18:10.750
knew-- that there
was this issue.

00:18:10.750 --> 00:18:18.266
And I really was profoundly
affected by the Sadkers--

00:18:18.266 --> 00:18:20.970
I think Myra and David Sadker--
I probably shouldn't even

00:18:20.970 --> 00:18:23.290
say that because I don't
know the actual names.

00:18:23.290 --> 00:18:28.500
I read some sociologists' work
about the lack of the brain

00:18:28.500 --> 00:18:29.710
trust.

00:18:29.710 --> 00:18:33.040
So I think this was my
awakening as a woman in science

00:18:33.040 --> 00:18:34.200
and engineering.

00:18:34.200 --> 00:18:37.580
And so I was looking at
that core-- at the time,

00:18:37.580 --> 00:18:42.520
people were talking about
junior high and third, fourth,

00:18:42.520 --> 00:18:43.465
fifth grade--

00:18:43.465 --> 00:18:44.800
MEGAN SMITH: Where the
drop-offs were going.

00:18:44.800 --> 00:18:46.633
CORI LATHAN: Where the
drop-offs were going.

00:18:46.633 --> 00:18:48.990
Well actually, I guess sixth,
seventh, eighth grade--

00:18:48.990 --> 00:18:50.600
junior high.

00:18:50.600 --> 00:18:52.510
And that spoke to
me as well because I

00:18:52.510 --> 00:18:55.010
found that junior high had
been such a disempowering

00:18:55.010 --> 00:18:56.560
experience for everybody.

00:18:56.560 --> 00:18:58.685
MEGAN SMITH: Can we just
change how we do all that?

00:18:58.685 --> 00:19:01.700
CORI LATHAN: So
for one IAP you can

00:19:01.700 --> 00:19:03.570
propose, for the January term--

00:19:03.570 --> 00:19:03.830
MEGAN SMITH: Because
the January break--

00:19:03.830 --> 00:19:06.380
CORI LATHAN: January
break at MIT, we proposed,

00:19:06.380 --> 00:19:10.047
let's bring these girls, 11 to
13, and bring them on campus

00:19:10.047 --> 00:19:11.630
and just show them
all the cool stuff.

00:19:11.630 --> 00:19:12.850
And so it went from there.

00:19:12.850 --> 00:19:17.010
We started Keys to
Empowering Youth, science

00:19:17.010 --> 00:19:18.180
and technology mentoring.

00:19:18.180 --> 00:19:20.520
The idea is we
brought them on campus

00:19:20.520 --> 00:19:26.760
to talk about stereotypes, show
them some great applications

00:19:26.760 --> 00:19:29.300
of science and technology,
and do some problem solving.

00:19:29.300 --> 00:19:32.260
And so that now,
it's still at MIT.

00:19:32.260 --> 00:19:34.500
It's also at
University of Maryland.

00:19:34.500 --> 00:19:39.190
I'm not involved with it at the
organizational level anymore,

00:19:39.190 --> 00:19:41.372
but sometimes I'll go and speak.

00:19:41.372 --> 00:19:43.080
So anyway-- sorry that
was a long answer.

00:19:43.080 --> 00:19:44.880
MEGAN SMITH: But the key
point is the hands-on.

00:19:44.880 --> 00:19:45.310
CORI LATHAN: I should
probably figure out

00:19:45.310 --> 00:19:46.190
how to summarize that.

00:19:46.190 --> 00:19:47.685
MEGAN SMITH: The
hands-on experience.

00:19:47.685 --> 00:19:47.934
CORI LATHAN: Yeah.

00:19:47.934 --> 00:19:49.240
It was to have the hands-on.

00:19:49.240 --> 00:19:50.170
Really, the hands-on.

00:19:50.170 --> 00:19:51.461
YOKY MATSUOKA: I think so, too.

00:19:51.461 --> 00:19:54.180
I had started an organization
as well, in a nonprofit,

00:19:54.180 --> 00:19:59.780
in almost exactly that format--
that there's an engineering

00:19:59.780 --> 00:20:00.380
problem.

00:20:00.380 --> 00:20:04.550
We specifically pick people
who could benefit from devices

00:20:04.550 --> 00:20:07.230
that can enable
them in some way.

00:20:07.230 --> 00:20:09.405
But then we specifically
formed a team

00:20:09.405 --> 00:20:13.720
made of mostly young girls
to come up with a solution

00:20:13.720 --> 00:20:15.300
that they can build on.

00:20:15.300 --> 00:20:18.710
And that specifically targeted
about fifth through eight

00:20:18.710 --> 00:20:19.760
grader girls.

00:20:19.760 --> 00:20:24.190
And they just really connected
some of the school problems

00:20:24.190 --> 00:20:27.150
that they didn't even think
was related and said, well,

00:20:27.150 --> 00:20:28.540
why don't we use physics?

00:20:28.540 --> 00:20:30.940
We can use this--
catapult this thing

00:20:30.940 --> 00:20:32.760
over to this side of the body.

00:20:32.760 --> 00:20:35.870
And I think that was
just a great connection

00:20:35.870 --> 00:20:38.566
to really motivate
them to realize, wait,

00:20:38.566 --> 00:20:39.940
the word "math"
and "engineering"

00:20:39.940 --> 00:20:42.360
and all those things also
can relate to the world

00:20:42.360 --> 00:20:43.900
like helping people.

00:20:43.900 --> 00:20:48.290
And then somehow that was just
such an "aha" moment for them.

00:20:48.290 --> 00:20:50.350
MEGAN SMITH: This is
the central issue.

00:20:50.350 --> 00:20:53.460
We did some research, and
talking to girls and also boys

00:20:53.460 --> 00:20:56.990
who are opting out of the
STEM fields, the STEAM fields,

00:20:56.990 --> 00:20:59.630
and it was over and
over again, not really

00:20:59.630 --> 00:21:02.645
connecting the impact-- not
really connecting the purpose.

00:21:02.645 --> 00:21:04.520
And so if we can get
past that-- and you guys

00:21:04.520 --> 00:21:08.250
are such an incredible
example of the kind of impact

00:21:08.250 --> 00:21:11.470
and change you can
see in the world

00:21:11.470 --> 00:21:12.950
by working on
these technologies.

00:21:12.950 --> 00:21:14.710
YOKY MATSUOKA: Yeah.

00:21:14.710 --> 00:21:16.740
Learning those examples
early on, right?

00:21:16.740 --> 00:21:17.490
CORI LATHAN: Yeah.

00:21:17.490 --> 00:21:19.560
And actually just wanted
to add one more thing.

00:21:19.560 --> 00:21:21.310
One of the other
times that I think

00:21:21.310 --> 00:21:24.690
it becomes very discouraging
for women in engineering

00:21:24.690 --> 00:21:26.730
is in college and towards
the end of college.

00:21:26.730 --> 00:21:28.570
You find that, I
think a lot of college

00:21:28.570 --> 00:21:31.740
grads then move on
to something else.

00:21:31.740 --> 00:21:33.490
They have this great
technical background,

00:21:33.490 --> 00:21:35.530
but they go into
law or medicine.

00:21:35.530 --> 00:21:39.100
They don't see that the
engineering can actually

00:21:39.100 --> 00:21:41.520
help the world and
solve real problems.

00:21:41.520 --> 00:21:41.920
MEGAN SMITH: Do
you think they are

00:21:41.920 --> 00:21:43.760
doing that because they
don't feel welcome,

00:21:43.760 --> 00:21:46.610
or they don't see the
purpose, you think?

00:21:46.610 --> 00:21:47.690
Or some combination?

00:21:47.690 --> 00:21:50.240
CORI LATHAN: I think it's
probably a combination of both.

00:21:50.240 --> 00:21:51.820
And one of the other
organizations--

00:21:51.820 --> 00:21:56.150
I'm on the board of Engineering
World Health, which takes

00:21:56.150 --> 00:21:58.390
undergrad biomedical
engineers and gives

00:21:58.390 --> 00:22:00.940
them experience abroad in
developing countries for 10

00:22:00.940 --> 00:22:01.740
weeks.

00:22:01.740 --> 00:22:07.530
And so it gives them this really
empowering, hands-on experience

00:22:07.530 --> 00:22:09.550
over the summer during
their college time.

00:22:09.550 --> 00:22:11.430
And I think it helps
them to really see

00:22:11.430 --> 00:22:13.240
the application of
what they're doing.

00:22:13.240 --> 00:22:15.700
And I think their
perception, right--

00:22:15.700 --> 00:22:18.382
If you go to school of
nursing and then ask women,

00:22:18.382 --> 00:22:19.340
why are you doing this?

00:22:19.340 --> 00:22:21.257
And they said, because
we want to help people.

00:22:21.257 --> 00:22:23.256
So, you know you could
also help in engineering.

00:22:23.256 --> 00:22:24.740
She says, oh, I don't qualify.

00:22:24.740 --> 00:22:26.230
I'm not good at it.

00:22:26.230 --> 00:22:27.890
But then, what
makes you say that?

00:22:27.890 --> 00:22:29.530
And then when you
drill down, they just

00:22:29.530 --> 00:22:31.820
misunderstood what it
means to be good enough

00:22:31.820 --> 00:22:34.170
to do the engineering.

00:22:34.170 --> 00:22:36.210
MEGAN SMITH: And
also, unfortunately,

00:22:36.210 --> 00:22:40.270
sometimes the amazing historic
women and minority leaders

00:22:40.270 --> 00:22:43.450
aren't as well known.

00:22:43.450 --> 00:22:45.760
We know Turing
really well, but we

00:22:45.760 --> 00:22:47.130
don't know Grace Hopper as well.

00:22:47.130 --> 00:22:49.505
We're starting to because of
the Grace Hopper Conference.

00:22:49.505 --> 00:22:51.650
We know Babbage-- first
mechanical computer.

00:22:51.650 --> 00:22:54.250
We don't know Ada, the first
person to suggest programming.

00:22:54.250 --> 00:22:59.287
And so we need to work
on historic [INAUDIBLE].

00:22:59.287 --> 00:23:01.120
YOKY MATSUOKA: And also
I think, role model.

00:23:01.120 --> 00:23:04.430
I think women, because of a
little bit of the security--

00:23:04.430 --> 00:23:06.500
because there are a
lack of role models,

00:23:06.500 --> 00:23:08.660
they feel insecure
about themselves.

00:23:08.660 --> 00:23:12.310
And then seeing more role models
seems to really help latch

00:23:12.310 --> 00:23:16.532
on to girls and then
say, oh, I can do it too.

00:23:16.532 --> 00:23:17.240
MEGAN SMITH: Yes.

00:23:17.240 --> 00:23:18.730
even if it wasn't half-half.

00:23:18.730 --> 00:23:19.707
If people were there.

00:23:19.707 --> 00:23:20.915
YOKY MATSUOKA: Yeah, exactly.

00:23:20.915 --> 00:23:21.750
Yeah.

00:23:21.750 --> 00:23:24.290
MEGAN SMITH: So speaking
of role models and heroes,

00:23:24.290 --> 00:23:26.540
are there people that come
to mind for you guys-- have

00:23:26.540 --> 00:23:29.730
have people that really
inspired you or helped you

00:23:29.730 --> 00:23:32.580
when it was challenging?

00:23:32.580 --> 00:23:34.680
Anybody come to mind
that's out there?

00:23:36.879 --> 00:23:38.670
CORI LATHAN: It's funny,
because I remember

00:23:38.670 --> 00:23:40.214
reading an interview
that you did,

00:23:40.214 --> 00:23:42.130
and you were saying you
always had a hard time

00:23:42.130 --> 00:23:43.672
thinking of role
models or something.

00:23:43.672 --> 00:23:45.421
YOKY MATSUOKA: Role
model in a women forum

00:23:45.421 --> 00:23:46.600
has been very difficult.

00:23:46.600 --> 00:23:47.270
CORI LATHAN: That's
right, because I

00:23:47.270 --> 00:23:48.330
was thinking the same thing.

00:23:48.330 --> 00:23:49.163
YOKY MATSUOKA: Yeah.

00:23:49.163 --> 00:23:51.910
CORI LATHAN: But to
answer your question,

00:23:51.910 --> 00:23:54.160
when I was thinking
about that, two people

00:23:54.160 --> 00:23:57.980
came to mind-- Captain Kirk.

00:23:57.980 --> 00:24:00.130
So I find it very
interesting the male role

00:24:00.130 --> 00:24:03.910
model's fictional, because then
the female would be Sally Ride.

00:24:03.910 --> 00:24:07.890
So again, going back to
my space buff routes.

00:24:07.890 --> 00:24:10.750
I think those were two very
inspirational models for me.

00:24:10.750 --> 00:24:11.875
YOKY MATSUOKA: Interesting.

00:24:11.875 --> 00:24:13.620
So for me, role
models-- because I

00:24:13.620 --> 00:24:16.320
didn't know anything but
tennis-- is all tennis figures.

00:24:16.320 --> 00:24:22.680
So John McEnroe, the ability for
him to play the kind of shots

00:24:22.680 --> 00:24:24.365
that nobody else
did was one thing.

00:24:24.365 --> 00:24:26.620
But something about
his personality,

00:24:26.620 --> 00:24:28.840
this rebellious
nature, and I think

00:24:28.840 --> 00:24:31.960
there's something that wanted
to break out of the mold for me,

00:24:31.960 --> 00:24:33.470
and John McEnroe did it.

00:24:33.470 --> 00:24:36.120
And I thought, wow,
very encouraging.

00:24:36.120 --> 00:24:40.850
It's something that he's
not ashamed to be different.

00:24:40.850 --> 00:24:44.642
That really allowed me
to have him as an idol.

00:24:44.642 --> 00:24:45.808
MEGAN SMITH: Amazing person.

00:24:48.620 --> 00:24:54.920
So what's some of the
best advice either of you

00:24:54.920 --> 00:24:57.376
guys got that you can think of?

00:24:57.376 --> 00:24:58.917
Or things that you
did that you were

00:24:58.917 --> 00:25:00.500
like, that with the
right thing to do.

00:25:00.500 --> 00:25:03.145
You know, when you kind of
hit a fork or you're like,

00:25:03.145 --> 00:25:04.520
should I go this
way or that way?

00:25:04.520 --> 00:25:07.742
What were some of the moments
that you can think of?

00:25:07.742 --> 00:25:09.200
YOKY MATSUOKA:
Those are hard ones.

00:25:14.580 --> 00:25:17.620
CORI LATHAN: Just a small--
maybe more of an anecdote.

00:25:17.620 --> 00:25:23.010
I found that the verbal advice--
I'm a gatherer of information,

00:25:23.010 --> 00:25:25.640
so I think I tend to get
advice from a lot of people

00:25:25.640 --> 00:25:28.492
and then synthesize
and make a decision,

00:25:28.492 --> 00:25:29.950
so it's hard to
think of one thing.

00:25:29.950 --> 00:25:41.360
But when I was probably 11 or
12 or 13, that formative time,

00:25:41.360 --> 00:25:44.180
I did a summer camp
with a dance teacher.

00:25:44.180 --> 00:25:47.350
And it wasn't so much what
this dance teacher said to me.

00:25:47.350 --> 00:25:50.290
He made me come out
of myself and not

00:25:50.290 --> 00:25:52.560
worry about what
other people we're

00:25:52.560 --> 00:25:54.680
saying or thinking or doing.

00:25:54.680 --> 00:25:56.450
And so it was one
of the times, I

00:25:56.450 --> 00:25:59.620
think a very important time
in my life, where it was,

00:25:59.620 --> 00:26:00.440
listen to your gut.

00:26:00.440 --> 00:26:02.040
It was, be who you are.

00:26:02.040 --> 00:26:04.900
And he made me dance in front of
people, and I am not a dancer.

00:26:04.900 --> 00:26:08.830
So that was actually, I think,
a great formative moment

00:26:08.830 --> 00:26:10.640
in the form of physical advice.

00:26:13.150 --> 00:26:14.340
YOKY MATSUOKA: Interesting.

00:26:14.340 --> 00:26:15.110
Yeah.

00:26:15.110 --> 00:26:19.240
I think mine is a really
weird, but let me just try.

00:26:19.240 --> 00:26:19.990
Ignorance.

00:26:19.990 --> 00:26:22.070
I sort of mentioned
this a little bit.

00:26:22.070 --> 00:26:25.730
Sometimes, when you
are thinking too much,

00:26:25.730 --> 00:26:31.700
just stop thinking about
it, and just let go,

00:26:31.700 --> 00:26:33.230
seems to actually help.

00:26:33.230 --> 00:26:35.160
So I think we all
like to analyze it

00:26:35.160 --> 00:26:37.127
to death about some things.

00:26:37.127 --> 00:26:38.710
I shouldn't do this,
and this is not--

00:26:38.710 --> 00:26:41.320
I'm not going to be good at it,
so I really shouldn't do it.

00:26:41.320 --> 00:26:43.719
I think that thought
process-- sometimes

00:26:43.719 --> 00:26:45.260
it's good to just
say, you know what,

00:26:45.260 --> 00:26:47.000
I'm just not going
to worry about it.

00:26:47.000 --> 00:26:50.230
It's easy to say, hard to do.

00:26:50.230 --> 00:26:53.754
As well as, I think
some of us-- and I

00:26:53.754 --> 00:26:55.920
think you mentioned a little
bit about not noticing,

00:26:55.920 --> 00:26:58.860
and I also feel like I
didn't notice how few of us

00:26:58.860 --> 00:27:00.560
were around until pretty late.

00:27:00.560 --> 00:27:05.190
And that really helped me to get
through some of those moments.

00:27:05.190 --> 00:27:07.250
And I think that's
one of the things

00:27:07.250 --> 00:27:09.040
that-- if there is
some ignorance that we

00:27:09.040 --> 00:27:11.529
can put in ourselves, I
think that'd be great.

00:27:11.529 --> 00:27:13.320
MEGAN SMITH: You know
what's interesting is

00:27:13.320 --> 00:27:16.320
that we do unconscious bias
training on all Googlers,

00:27:16.320 --> 00:27:19.250
and one of the
things we learn is--

00:27:19.250 --> 00:27:21.090
there's a study that
shows if there's

00:27:21.090 --> 00:27:24.370
10 characteristics
for a job, on average,

00:27:24.370 --> 00:27:26.380
men will apply if they
have at least three,

00:27:26.380 --> 00:27:29.210
and women will apply for jobs
if they have at least seven.

00:27:29.210 --> 00:27:31.790
And so I think in some ways
maybe women are conditioned,

00:27:31.790 --> 00:27:34.289
or they're automatically saying,
I need more qualifications,

00:27:34.289 --> 00:27:35.740
I need more information.

00:27:35.740 --> 00:27:38.820
Once I have all these skills,
then I can apply for that.

00:27:38.820 --> 00:27:40.970
And some of the guys are
maybe being socialized

00:27:40.970 --> 00:27:44.530
to just, hey, you have enough,
you have a piece of it, go.

00:27:44.530 --> 00:27:46.322
To me, what you were
saying tied into that.

00:27:46.322 --> 00:27:47.613
YOKY MATSUOKA: I totally agree.

00:27:47.613 --> 00:27:49.350
So in high school, I
heard similar stats

00:27:49.350 --> 00:27:53.310
that boys who have B minus
average in math and physics

00:27:53.310 --> 00:27:54.760
would apply to engineering.

00:27:54.760 --> 00:27:57.540
Girls would not apply unless
they have an A average.

00:27:57.540 --> 00:27:59.940
And it's the same concept.

00:27:59.940 --> 00:28:01.800
And unfortunately,
they're biased things.

00:28:01.800 --> 00:28:04.830
So as you go on, the women
who actually lasted til then

00:28:04.830 --> 00:28:07.070
tend to be those A player.

00:28:07.070 --> 00:28:10.960
But somehow, for boys,
it's OK to have a range.

00:28:10.960 --> 00:28:13.950
So if we can actually start
to change that even earlier

00:28:13.950 --> 00:28:17.190
in the pipeline, saying,
you're getting B minus in math?

00:28:17.190 --> 00:28:17.690
Great!

00:28:17.690 --> 00:28:18.280
Keep going!

00:28:18.280 --> 00:28:19.800
Engineering, apply for it.

00:28:19.800 --> 00:28:21.300
There is definitely
a place for you.

00:28:21.300 --> 00:28:23.508
MEGAN SMITH: Right, because
part of it is book smart,

00:28:23.508 --> 00:28:26.120
but a lot of it is practical,
physical intuition stuff.

00:28:26.120 --> 00:28:28.310
And you can--
apprentice, journeyman,

00:28:28.310 --> 00:28:30.030
master-- you can
learn these things.

00:28:30.030 --> 00:28:31.500
YOKY MATSUOKA: Exactly.

00:28:31.500 --> 00:28:34.800
MEGAN SMITH: So what is
something that you've

00:28:34.800 --> 00:28:36.360
seen recently, or
some of the things

00:28:36.360 --> 00:28:38.280
you've seen that you're just
like, oh my god, I love that.

00:28:38.280 --> 00:28:40.007
I've been waiting
for that to show up.

00:28:40.007 --> 00:28:42.340
Or something that in your
research or work you're doing,

00:28:42.340 --> 00:28:44.714
or something you've seen from
another company or research

00:28:44.714 --> 00:28:47.610
team, or from a
science fair kid?

00:28:47.610 --> 00:28:51.010
Anything out there that's been
really cool that people should

00:28:51.010 --> 00:28:53.120
know about that
you can think of?

00:28:53.120 --> 00:28:55.930
CORI LATHAN: I
think that for me,

00:28:55.930 --> 00:29:00.750
the area of brain-computer
interaction and brain science

00:29:00.750 --> 00:29:03.880
and sensors in robotics and
all of that coming together.

00:29:03.880 --> 00:29:07.080
The World Economic Forum just
released their top 10 emerging

00:29:07.080 --> 00:29:11.650
technologies for 2014,
and quantified self

00:29:11.650 --> 00:29:15.090
was one of them-- brain-computer
interfaces and body wearable

00:29:15.090 --> 00:29:16.240
sensors.

00:29:16.240 --> 00:29:22.750
And to me, that really
encapsulated the ecosystem

00:29:22.750 --> 00:29:25.360
that I want to be
part of going forward.

00:29:25.360 --> 00:29:29.790
The ability for us to really
understand our brain function,

00:29:29.790 --> 00:29:33.210
and then use that information to
interact with our environment.

00:29:33.210 --> 00:29:38.120
And going back to our moon shot
idea, eliminating disability.

00:29:38.120 --> 00:29:39.260
Eliminating aging.

00:29:39.260 --> 00:29:41.150
I mean, we're all
disabled in some way

00:29:41.150 --> 00:29:43.620
because we can't do
everything we want to do.

00:29:43.620 --> 00:29:45.850
And so really enabling
the human being

00:29:45.850 --> 00:29:49.710
through these technologies
over the next one, three,

00:29:49.710 --> 00:29:51.250
five years is going
to be amazing.

00:29:51.250 --> 00:29:52.875
MEGAN SMITH: And I
also like your point

00:29:52.875 --> 00:29:56.210
about teams, because
it's also enabling teams.

00:29:56.210 --> 00:29:57.850
I mean, we see
the basics of that

00:29:57.850 --> 00:30:00.870
in Wikipedia, a
global team of sharing

00:30:00.870 --> 00:30:02.790
what we know with each other.

00:30:02.790 --> 00:30:03.810
CORI LATHAN: We think of
human-machine interaction

00:30:03.810 --> 00:30:05.230
as one human and one machine.

00:30:05.230 --> 00:30:09.910
But really, it's human to
machine to human to machine--

00:30:09.910 --> 00:30:14.914
MEGAN SMITH: And to each other
and all mixed up, globally.

00:30:14.914 --> 00:30:16.590
YOKY MATSUOKA: I don't know.

00:30:16.590 --> 00:30:19.850
One of the things that I'm
really excited that we don't

00:30:19.850 --> 00:30:24.060
necessarily have yet but we're
in the right trajectory to get

00:30:24.060 --> 00:30:27.280
is mommy technology.

00:30:27.280 --> 00:30:30.200
So I think a lot of
the technologies,

00:30:30.200 --> 00:30:34.660
because they're built by men,
they serve a lot of things

00:30:34.660 --> 00:30:36.610
that men like.

00:30:36.610 --> 00:30:39.190
But I think we are moving a
very right direction where we're

00:30:39.190 --> 00:30:42.300
putting technology
infused homes.

00:30:42.300 --> 00:30:44.150
For example, not to be
[? netcentric ?] view

00:30:44.150 --> 00:30:47.960
of thinking about it, but
that is now going to make

00:30:47.960 --> 00:30:52.370
mommy's tasks of
tracking kids or some

00:30:52.370 --> 00:30:55.220
of the duties a
little bit easier.

00:30:55.220 --> 00:30:59.020
And the more technology
interface women

00:30:59.020 --> 00:31:01.040
get to have, the
more chance the women

00:31:01.040 --> 00:31:04.110
get to understand the technology
itself and says, oh geez,

00:31:04.110 --> 00:31:06.990
I can contribute to this
way of thinking technology,

00:31:06.990 --> 00:31:10.060
and realize that they can do,
and they can become engineers,

00:31:10.060 --> 00:31:11.430
and they can jump in.

00:31:11.430 --> 00:31:14.107
And I think that is a
good positive feedback.

00:31:14.107 --> 00:31:15.440
MEGAN SMITH: I completely agree.

00:31:15.440 --> 00:31:19.410
And also just in general,
parents, and helping moms

00:31:19.410 --> 00:31:22.910
have an easier time so they can
participate more, and helping

00:31:22.910 --> 00:31:27.360
the guys, the dads, be able to
participate in different ways

00:31:27.360 --> 00:31:29.139
that they haven't
been welcome or in.

00:31:29.139 --> 00:31:31.180
I think it's going to
really change things a lot.

00:31:31.180 --> 00:31:32.220
Very insightful.

00:31:32.220 --> 00:31:33.690
So that's a wrap.

00:31:33.690 --> 00:31:37.030
I want to thank Yoky
and Cori for being

00:31:37.030 --> 00:31:39.270
here-- amazing
robotics brain trust.

00:31:39.270 --> 00:31:41.900
This is women tech
makers, and we'll

00:31:41.900 --> 00:31:44.250
see you in the next shows.

