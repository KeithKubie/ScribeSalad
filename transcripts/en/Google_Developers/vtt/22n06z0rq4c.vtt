WEBVTT
Kind: captions
Language: en

00:00:01.350 --> 00:00:04.090
This lesson is about
Task Queue.

00:00:04.090 --> 00:00:07.080
If you thought Memcache is cool,
being able to take whole

00:00:07.080 --> 00:00:09.820
layers of caching that you could
share between all those

00:00:09.820 --> 00:00:13.780
instances, Task Queue
is so much cooler.

00:00:13.780 --> 00:00:16.550
It's just amazing what you can
do with it and how much you

00:00:16.550 --> 00:00:19.010
could enhance your application
and make it more

00:00:19.010 --> 00:00:22.650
scalable and reliable.

00:00:22.650 --> 00:00:26.020
What is Task Queue, and why
do I love it so much?

00:00:26.020 --> 00:00:29.030
The different kinds of task
queues that we have, the push

00:00:29.030 --> 00:00:33.200
and the pull queue, as well as
cron jobs, or scheduled tasks.

00:00:33.200 --> 00:00:35.480
A task is just a unit of work.

00:00:35.480 --> 00:00:38.450
For example, let me write an
object to the datastore or

00:00:38.450 --> 00:00:41.330
send an email, something that
could be a compartmentalized

00:00:41.330 --> 00:00:44.550
unit of function, or something
that you'd want to run out of

00:00:44.550 --> 00:00:46.310
the user context.

00:00:46.310 --> 00:00:49.590
For example, if the user runs a
web page, and that web page

00:00:49.590 --> 00:00:53.230
does something and then uses
an email, it doesn't make

00:00:53.230 --> 00:00:56.570
sense for that user to sit there
waiting for that email

00:00:56.570 --> 00:00:58.050
to be sent.

00:00:58.050 --> 00:01:01.140
Task Queue is a great way to
take that piece of work and

00:01:01.140 --> 00:01:03.970
put it on a different queue
while it automatically

00:01:03.970 --> 00:01:07.230
processes in the background,
while the user's request

00:01:07.230 --> 00:01:09.440
continues processing.

00:01:09.440 --> 00:01:11.420
All versions of your
applications

00:01:11.420 --> 00:01:13.750
share the same queue.

00:01:13.750 --> 00:01:17.130
A push queue is really for
automatic execution.

00:01:17.130 --> 00:01:19.570
You push something on a queue,
and it will immediately

00:01:19.570 --> 00:01:21.010
execute it.

00:01:21.010 --> 00:01:23.760
Pull Queue is slightly
different, where you can

00:01:23.760 --> 00:01:26.410
actually put a piece of work
on this queue, and then you

00:01:26.410 --> 00:01:29.340
can take a lease from it by a
worker thread that you can

00:01:29.340 --> 00:01:32.690
control, like how fast you pick
up those queues, as well

00:01:32.690 --> 00:01:35.690
as where you pick them up from,
whether it's Compute

00:01:35.690 --> 00:01:39.160
Engine or on-premise system.

00:01:39.160 --> 00:01:41.710
There's a few basics
about all tasks.

00:01:41.710 --> 00:01:44.800
Each one has a name when you
put it on the task queue.

00:01:44.800 --> 00:01:46.850
If you don't specify
one, it will

00:01:46.850 --> 00:01:48.980
automatically generate one.

00:01:48.980 --> 00:01:52.010
If you try to put a task on the
queue with the same name,

00:01:52.010 --> 00:01:54.620
it will actually fail.

00:01:54.620 --> 00:01:57.010
I have here the top
and the bottom.

00:01:57.010 --> 00:01:59.895
The task on the top is the type
of parameters that you

00:01:59.895 --> 00:02:02.730
can control for a push queue.

00:02:02.730 --> 00:02:06.150
And the bottom is the type of
data and parameters that you

00:02:06.150 --> 00:02:08.500
can have for a pull queue.

00:02:08.500 --> 00:02:10.289
Notice there's a lot
more information

00:02:10.289 --> 00:02:12.340
about a push queue.

00:02:12.340 --> 00:02:14.460
The reason why is that
a push queue--

00:02:14.460 --> 00:02:17.940
a good way of thinking about
it is it's just a URL along

00:02:17.940 --> 00:02:21.050
with some parameter options that
are going to be executed

00:02:21.050 --> 00:02:23.430
on one of your front-end
instances.

00:02:23.430 --> 00:02:27.120
And it's going to try to do this
as quickly as possible.

00:02:27.120 --> 00:02:30.720
You could put some quality and
control around the queue.

00:02:30.720 --> 00:02:32.500
And the way you do that
is different for the

00:02:32.500 --> 00:02:34.460
two types of queues.

00:02:34.460 --> 00:02:37.475
The push queue, you put these
pieces of work on the queue,

00:02:37.475 --> 00:02:40.630
and it will get executed
as quickly as possible.

00:02:40.630 --> 00:02:44.250
It may cause a lot of instances
to be spun up, so

00:02:44.250 --> 00:02:47.360
you have to be careful about
how many you put on, or at

00:02:47.360 --> 00:02:50.400
least how quickly that queue
is going to drain and send

00:02:50.400 --> 00:02:52.820
tasks to your application.

00:02:52.820 --> 00:02:56.180
One nice feature is on
frontends, you normally have a

00:02:56.180 --> 00:02:58.930
60-second deadline where
the request has to

00:02:58.930 --> 00:03:00.670
be completed by.

00:03:00.670 --> 00:03:04.230
If that request is initiated by
a task queue, then you have

00:03:04.230 --> 00:03:07.800
a full 10 minutes to finish
that request.

00:03:07.800 --> 00:03:09.980
If you do it on the backend,
of course, you have an

00:03:09.980 --> 00:03:13.560
unlimited amount of time
to finish that request.

00:03:13.560 --> 00:03:16.570
But when you are creating a push
queue, you are limited to

00:03:16.570 --> 00:03:21.390
100 kilobytes of total data
with that request.

00:03:21.390 --> 00:03:24.340
Since this is just a standard
request that comes from a user

00:03:24.340 --> 00:03:28.030
to your application, you can
have either a Get request or a

00:03:28.030 --> 00:03:29.450
Post request.

00:03:29.450 --> 00:03:32.360
And you can have a whole payload
that is that post

00:03:32.360 --> 00:03:35.280
body, just like someone is
posting a form to your

00:03:35.280 --> 00:03:37.160
application.

00:03:37.160 --> 00:03:39.960
A pull queue, however, you
have an instance that you

00:03:39.960 --> 00:03:40.890
would spin up.

00:03:40.890 --> 00:03:43.500
And you would then go to
that queue and get a

00:03:43.500 --> 00:03:46.280
lease on some tasks.

00:03:46.280 --> 00:03:47.750
You could crunch through
your work.

00:03:47.750 --> 00:03:50.510
And then when you're done with
that work, you'd go and see

00:03:50.510 --> 00:03:53.690
that you would delete that task,
essentially saying that

00:03:53.690 --> 00:03:57.330
you've completed the processing
for that task.

00:03:57.330 --> 00:04:01.190
We have a complete REST
interface over the pull queue.

00:04:01.190 --> 00:04:04.420
This allows you, then, to run
your worker from anywhere.

00:04:04.420 --> 00:04:06.890
It could be on-premise, it could
be another App Engine

00:04:06.890 --> 00:04:10.240
instance, either frontends
or backends.

00:04:10.240 --> 00:04:14.590
You have a maximum of 1
megabyte in task size.

00:04:14.590 --> 00:04:17.360
Unlike a push task, where you
have parameters that get

00:04:17.360 --> 00:04:21.470
posted or get appended as the
request, this is actually a

00:04:21.470 --> 00:04:24.700
piece of data that when you
release the task, you

00:04:24.700 --> 00:04:27.290
get that data back.

00:04:27.290 --> 00:04:30.180
Every queue you have in your
system you can actually name

00:04:30.180 --> 00:04:33.080
and put in parameters
about that queue--

00:04:33.080 --> 00:04:36.755
for example, how frequently or
fast you're going to process

00:04:36.755 --> 00:04:40.050
these tasks, or how large
a token bucket is.

00:04:40.050 --> 00:04:44.220
The basic concept is we use a
token-bucket algorithm in

00:04:44.220 --> 00:04:48.310
order to limit how frequently
or how quickly we drain or

00:04:48.310 --> 00:04:50.680
process your tasks.

00:04:50.680 --> 00:04:53.670
You can have a bunch of queues
that you can create and you

00:04:53.670 --> 00:04:55.770
could specify different
parameters.

00:04:55.770 --> 00:04:57.700
But you also have a default
queue that's

00:04:57.700 --> 00:04:59.460
already set for you.

00:04:59.460 --> 00:05:02.900
And its name is just Default.

00:05:02.900 --> 00:05:07.040
Also by default, there is an
unlimited concurrent request.

00:05:07.040 --> 00:05:10.000
If you have a lot of requests on
this task queue, it's going

00:05:10.000 --> 00:05:11.640
to start saving them
and trying to

00:05:11.640 --> 00:05:14.260
process as many as possible.

00:05:14.260 --> 00:05:17.290
Unless you control it, it will
start to spin a lot of

00:05:17.290 --> 00:05:19.100
frontend instances.

00:05:19.100 --> 00:05:21.970
For example, in the illustration
here is that you

00:05:21.970 --> 00:05:24.720
have one queue that
says, one request

00:05:24.720 --> 00:05:27.420
currently, and may be faster.

00:05:27.420 --> 00:05:29.680
Think of it as a bigger pipe.

00:05:29.680 --> 00:05:32.330
Whereas maybe another queue
that says, I only want to

00:05:32.330 --> 00:05:34.350
process one concurrently.

00:05:34.350 --> 00:05:37.400
And that's a smaller pipe that
kind of funnels the traffic

00:05:37.400 --> 00:05:40.700
into one at a time and would
prevent more and more

00:05:40.700 --> 00:05:43.420
instances from being spun up.

00:05:43.420 --> 00:05:47.220
Unless you specify otherwise,
every single task that you put

00:05:47.220 --> 00:05:50.710
on the queue is going to be
executed by the same version

00:05:50.710 --> 00:05:52.730
that initiated that task.

00:05:52.730 --> 00:05:55.600
So if you're the default
application, it's going to go

00:05:55.600 --> 00:05:56.500
on that queue.

00:05:56.500 --> 00:05:59.380
And it's going to try to
execute that URL on the

00:05:59.380 --> 00:06:02.560
default version of
that application.

00:06:02.560 --> 00:06:05.590
If you're in, say, version 3 of
your application, and you

00:06:05.590 --> 00:06:08.440
execute a task in that version,
it's going to go back

00:06:08.440 --> 00:06:10.070
to version 3.

00:06:10.070 --> 00:06:13.440
You can override this
behavior as well.

00:06:13.440 --> 00:06:16.610
What's nice about this is if
you deploy a new version of

00:06:16.610 --> 00:06:19.550
that application and put
something on the task, and

00:06:19.550 --> 00:06:22.450
even though the task are shared,
it's going to go back

00:06:22.450 --> 00:06:24.420
to the same version
that you deployed.

00:06:24.420 --> 00:06:27.310
So you could test new
functionality out, and you

00:06:27.310 --> 00:06:29.610
don't have to worry about it
calling between different

00:06:29.610 --> 00:06:32.100
applications.

00:06:32.100 --> 00:06:33.990
So what does this mean?

00:06:33.990 --> 00:06:38.040
Well, let's say that you have a
processing rate of 1 second.

00:06:38.040 --> 00:06:40.430
You have bucket size of 5.

00:06:40.430 --> 00:06:43.350
So you get 7 new tasks.

00:06:43.350 --> 00:06:45.190
Well, you're going to
execute the first

00:06:45.190 --> 00:06:46.540
five tasks right away.

00:06:46.540 --> 00:06:49.140
But it's going to have to wait
for a second later to get

00:06:49.140 --> 00:06:53.160
another token to execute the
sixth task, and then again for

00:06:53.160 --> 00:06:55.790
the seventh task.

00:06:55.790 --> 00:06:58.770
Another way of showing this
is on a time line.

00:06:58.770 --> 00:07:02.910
Every time we execute a task,
we use one of these tokens.

00:07:02.910 --> 00:07:05.830
But now we wait for that token
to come back and fill up the

00:07:05.830 --> 00:07:07.260
bucket again.

00:07:07.260 --> 00:07:10.110
If we increase that rate to 2
seconds, then, of course, you

00:07:10.110 --> 00:07:13.640
still can consume the same
five tokens right away.

00:07:13.640 --> 00:07:16.300
But you get a new token halfway
through that because

00:07:16.300 --> 00:07:18.030
it's every half second.

00:07:18.030 --> 00:07:21.730
You get a new token when you
execute the seventh task,

00:07:21.730 --> 00:07:23.730
sixth and seventh tasks.

00:07:23.730 --> 00:07:26.900
So this is how you could really
control how frequently

00:07:26.900 --> 00:07:31.190
and how quickly you
execute the tasks.

00:07:31.190 --> 00:07:34.580
And here you get more
and more tokens.

00:07:34.580 --> 00:07:36.750
And the way you would configure
it is these have

00:07:36.750 --> 00:07:38.350
different parameters--

00:07:38.350 --> 00:07:41.850
the task-retry-limit, which is
the minimum number of times to

00:07:41.850 --> 00:07:44.780
retry the task before
it fails.

00:07:44.780 --> 00:07:47.810
And some of these are used in
conjunction with one another.

00:07:47.810 --> 00:07:51.170
So you can also have
task-retry-time, which is the

00:07:51.170 --> 00:07:53.780
minimum amount of time
to retry a task.

00:07:53.780 --> 00:07:55.940
So in this example,
the minimum- and

00:07:55.940 --> 00:07:59.180
maximum-backup-seconds have to
do with how long you wait

00:07:59.180 --> 00:08:02.060
between subsequent retries.

00:08:02.060 --> 00:08:05.700
The max-doubling is how much
of an exponential backoff

00:08:05.700 --> 00:08:06.940
you're getting.

00:08:06.940 --> 00:08:09.810
So to put all these things
together, if we have that

00:08:09.810 --> 00:08:13.580
retry limit of 1, the retry
limit of 4 seconds, which is

00:08:13.580 --> 00:08:16.780
really the minimum number of
times we're going to try to

00:08:16.780 --> 00:08:20.156
attempt it, and the
minimum/max-backoff of 1

00:08:20.156 --> 00:08:24.030
second to 4 seconds, we'll see
that the first time we fail a

00:08:24.030 --> 00:08:27.150
task, it retries it
a second later.

00:08:27.150 --> 00:08:29.330
But we've already now
gone over that 1

00:08:29.330 --> 00:08:31.360
minimum-retry-limit.

00:08:31.360 --> 00:08:34.270
But we're going to keep trying
because we haven't yet hit the

00:08:34.270 --> 00:08:37.640
minimum time we want to
continue a retry.

00:08:37.640 --> 00:08:41.880
So then if it fails a second
time, we retry it again.

00:08:41.880 --> 00:08:44.900
And we still yet haven't
hit that for a second.

00:08:44.900 --> 00:08:47.450
So we're going to
try it again.

00:08:47.450 --> 00:08:50.830
And each time, we're getting a
little bit longer because of

00:08:50.830 --> 00:08:52.200
that doubling.

00:08:52.200 --> 00:08:55.690
But since we've satisfied, at
this point, both the retry

00:08:55.690 --> 00:09:00.020
limit of 1 and the retry limit
of 4 seconds, we're then going

00:09:00.020 --> 00:09:04.560
to fail that task and not
continue to retry it.

00:09:04.560 --> 00:09:07.290
So let's say that we increase
the retry limit to the number

00:09:07.290 --> 00:09:09.950
5, but we keep the same minimum

00:09:09.950 --> 00:09:12.500
and max backoff seconds.

00:09:12.500 --> 00:09:15.730
You notice that the minimum
backoff is 1 second.

00:09:15.730 --> 00:09:17.790
And so the first time
it retries is

00:09:17.790 --> 00:09:19.520
going to be 1 second.

00:09:19.520 --> 00:09:22.440
And I'm using seconds
for illustrations.

00:09:22.440 --> 00:09:24.870
Usually it isn't terribly
accurate, because it could

00:09:24.870 --> 00:09:27.290
take longer to execute
the tasks.

00:09:27.290 --> 00:09:29.650
You're usually dealing with
minutes or hours for these

00:09:29.650 --> 00:09:31.480
types of retries.

00:09:31.480 --> 00:09:36.590
Now we double it, though, to
2 seconds and to 3 seconds.

00:09:36.590 --> 00:09:38.840
Essentially, we keep multiplying
by the number of

00:09:38.840 --> 00:09:43.480
seconds that we identify as the
minimum backoff seconds.

00:09:43.480 --> 00:09:45.590
And the max-doubling is
what controls the

00:09:45.590 --> 00:09:48.450
increment of each time.

00:09:48.450 --> 00:09:51.640
Since I have 0, that means
it's not going to double.

00:09:51.640 --> 00:09:53.760
It's going to increase
my minimum by

00:09:53.760 --> 00:09:55.900
that amount each time.

00:09:55.900 --> 00:09:59.960
My minimum is 1 second, so
it would be 1 second.

00:09:59.960 --> 00:10:03.820
If I had a max-doubling, it
would go from 1 to 2 to 4, et

00:10:03.820 --> 00:10:06.650
cetera, until you have the
maximum number of times it

00:10:06.650 --> 00:10:08.090
would be doubled.

00:10:08.090 --> 00:10:13.080
0 just simply means, keep
incrementing by that 1 second.

00:10:13.080 --> 00:10:16.080
So you'll see that we've
hit 4 seconds here.

00:10:16.080 --> 00:10:18.520
But our max backoff
is 4 seconds--

00:10:18.520 --> 00:10:22.400
meaning, don't keep extending
it beyond 4 seconds.

00:10:22.400 --> 00:10:25.390
So now every subsequent request
is going to retry

00:10:25.390 --> 00:10:28.500
every 4 seconds until we've
satisfied both the minimum

00:10:28.500 --> 00:10:31.820
number of times we want to retry
as well as the amount of

00:10:31.820 --> 00:10:34.870
time we want to keep
retrying for.

00:10:34.870 --> 00:10:38.490
Since we've tried five times,
we're greater than 4 seconds.

00:10:38.490 --> 00:10:40.300
And that would be over here.

00:10:40.300 --> 00:10:43.900
We would then stop after
that last request.

00:10:43.900 --> 00:10:47.010
Here's a simple code to put
a task on the queue.

00:10:47.010 --> 00:10:49.950
It's so easy just to
say, here's my URL

00:10:49.950 --> 00:10:52.180
and here's my parameter.

00:10:52.180 --> 00:10:54.760
Then this is going to
automatically submit a post

00:10:54.760 --> 00:10:59.670
request to the parameter of
"id's equal 1, 2, and 3." It's

00:10:59.670 --> 00:11:01.680
literally that easy.

00:11:01.680 --> 00:11:04.630
Then you would, of course,
program whatever handler you

00:11:04.630 --> 00:11:06.380
would want to do to handle that

00:11:06.380 --> 00:11:09.120
forward-slash worker request.

00:11:09.120 --> 00:11:12.740
In Python, it's very
similar format.

00:11:12.740 --> 00:11:16.230
One of the most challenging
concepts is, how do we limit

00:11:16.230 --> 00:11:18.720
the rate of tasks that
are being executed?

00:11:18.720 --> 00:11:21.450
And really, what's happening
underneath the hood?

00:11:21.450 --> 00:11:23.780
We use a token-bucket
algorithm.

00:11:23.780 --> 00:11:27.310
And think of it, literally, as
a bucket that has tokens.

00:11:27.310 --> 00:11:30.350
If we have a refill rate of 1
second-- which means we want

00:11:30.350 --> 00:11:32.610
to process it at
once a second--

00:11:32.610 --> 00:11:37.080
then every second, we take a new
token and we put it right

00:11:37.080 --> 00:11:38.330
in this bucket.

00:11:38.330 --> 00:11:41.630
If the bucket's already full
with our max bucket size, then

00:11:41.630 --> 00:11:44.000
we just disregard that token.

00:11:44.000 --> 00:11:45.750
So you can always have
a bucket that's

00:11:45.750 --> 00:11:47.860
constantly being refilled.

00:11:47.860 --> 00:11:50.880
Whenever we executed a new task,
it's going to take one

00:11:50.880 --> 00:11:54.430
of those tokens out of the
bucket and execute the task.

00:11:54.430 --> 00:11:55.810
So that means we're
going to start

00:11:55.810 --> 00:11:57.120
taking a bunch of tokens.

00:11:57.120 --> 00:12:00.760
But once that bucket is empty,
we can't process that request

00:12:00.760 --> 00:12:03.310
until a new token gets
added to the bucket.

00:12:03.310 --> 00:12:08.230
Within one second, that token
would be replenished.

00:12:08.230 --> 00:12:11.200
Another way of showing this
is on a time line.

00:12:11.200 --> 00:12:15.400
Every time we execute a task,
we use one of those tokens.

00:12:15.400 --> 00:12:18.490
But now empty, and we wait for
that token to come back and

00:12:18.490 --> 00:12:20.660
fill up the bucket again.

00:12:20.660 --> 00:12:23.880
If we increase that rate to 2
seconds, then, of course, you

00:12:23.880 --> 00:12:26.750
can still consume the same
tokens right away.

00:12:26.750 --> 00:12:29.790
But you get a new token halfway
through that, because

00:12:29.790 --> 00:12:31.340
it's every half a second.

00:12:31.340 --> 00:12:34.300
You get a new token when you
execute the seventh task,

00:12:34.300 --> 00:12:36.700
sixth and seventh tasks.

00:12:36.700 --> 00:12:39.710
So this is how you could really
control how frequently

00:12:39.710 --> 00:12:43.440
and how quickly you
execute the tasks.

00:12:43.440 --> 00:12:46.790
And here you get more
and more tokens.

00:12:46.790 --> 00:12:49.140
So let's say you want
to delete a task.

00:12:49.140 --> 00:12:51.120
There's a couple of
ways to do that.

00:12:51.120 --> 00:12:53.660
One is you could go through your
admin interface in the

00:12:53.660 --> 00:12:56.950
console and just purge the whole
task or, actually, check

00:12:56.950 --> 00:12:59.100
on individual tasks
and delete them.

00:12:59.100 --> 00:13:02.440
Or you could do that
programmatically through code.

00:13:02.440 --> 00:13:05.000
In Java you would just
load the queue.

00:13:05.000 --> 00:13:08.500
Then you would delete the task
by specifying the task name.

00:13:08.500 --> 00:13:11.150
So if you had told it or didn't
give it a task name

00:13:11.150 --> 00:13:13.500
when you created the task,
you would know

00:13:13.500 --> 00:13:15.440
which task to delete.

00:13:15.440 --> 00:13:18.290
So you would have to correlate
it, figure it out, then do

00:13:18.290 --> 00:13:19.790
that deletion.

00:13:19.790 --> 00:13:22.680
If you had specified the same
task, then, of course, you

00:13:22.680 --> 00:13:25.720
could just use that
name very easily.

00:13:25.720 --> 00:13:28.410
You could also just purge the
entire queue, which would

00:13:28.410 --> 00:13:31.110
delete all tasks.

00:13:31.110 --> 00:13:33.820
Another nice feature, though, in
the task queue is that you

00:13:33.820 --> 00:13:37.820
can schedule tasks based on the
part of the transaction.

00:13:37.820 --> 00:13:40.190
We're talking about
the datastore.

00:13:40.190 --> 00:13:42.920
So let's say you wanted to add
something to the datastore,

00:13:42.920 --> 00:13:45.730
and then you wanted to trigger
off a process that would go

00:13:45.730 --> 00:13:48.390
through and update an average
for some reason.

00:13:48.390 --> 00:13:50.970
Perhaps you have a +1 button.

00:13:50.970 --> 00:13:53.950
And every time someone pluses
it, you want to get the

00:13:53.950 --> 00:13:57.470
average of or count of, for
example, you would increment

00:13:57.470 --> 00:14:00.330
that count in a separate task.

00:14:00.330 --> 00:14:03.670
But you only want to actually
schedule that task if that

00:14:03.670 --> 00:14:06.050
datastore transaction
was successful.

00:14:06.050 --> 00:14:08.370
And so this is how you would
do it as part of a

00:14:08.370 --> 00:14:11.270
transaction.

00:14:11.270 --> 00:14:14.020
When tasks are actually calling
your application,

00:14:14.020 --> 00:14:16.080
there are a few different
headers that are

00:14:16.080 --> 00:14:17.480
automatically added.

00:14:17.480 --> 00:14:20.420
Something that's nice about
these headers is that the App

00:14:20.420 --> 00:14:24.150
Engine system automatically
strips everything that begins

00:14:24.150 --> 00:14:27.900
with anything that we add
ourselves, will strip it if

00:14:27.900 --> 00:14:31.290
someone tries to spoof that
send-a-request to your

00:14:31.290 --> 00:14:32.480
application.

00:14:32.480 --> 00:14:35.810
And so if these headers exist,
you could rely that they're

00:14:35.810 --> 00:14:39.100
actually added by our own
infrastructure and not by

00:14:39.100 --> 00:14:41.230
someone trying to trick you.

00:14:41.230 --> 00:14:44.380
Your name, task queue-- pretty
self explanatory.

00:14:44.380 --> 00:14:47.570
We try to count the number of
times it's been retried.

00:14:47.570 --> 00:14:51.970
Execution count is the number
of times it's been executed.

00:14:51.970 --> 00:14:56.210
And TaskETA, which is roughly
how long until it's going to

00:14:56.210 --> 00:14:57.980
be executed.

00:14:57.980 --> 00:15:02.760
Another nice feature of Task is
that if you want it to be

00:15:02.760 --> 00:15:05.690
right away, you can specify
how long until you

00:15:05.690 --> 00:15:07.940
want it to be run.

00:15:07.940 --> 00:15:11.260
That is, I want it to be run
in 1,000 seconds, or at a

00:15:11.260 --> 00:15:13.640
particular date and time.

00:15:13.640 --> 00:15:16.570
Then there's also a header that
you can set when you're

00:15:16.570 --> 00:15:18.730
creating a new task.

00:15:18.730 --> 00:15:20.050
And this is a situation--

00:15:20.050 --> 00:15:23.370
let's say that you are using
Task to call the backend.

00:15:23.370 --> 00:15:26.450
But backend could be dynamic,
or it could be something you

00:15:26.450 --> 00:15:28.140
control yourself.

00:15:28.140 --> 00:15:31.260
But you don't want to spin up
a new backend instance if

00:15:31.260 --> 00:15:33.670
one's not already running.

00:15:33.670 --> 00:15:36.270
So let's say you have a bunch
of batch processing that you

00:15:36.270 --> 00:15:39.080
want to do in a particular time,
and you don't want to

00:15:39.080 --> 00:15:42.460
spend the 15 minutes of
automatically spinning up the

00:15:42.460 --> 00:15:45.300
backend and then spinning
it down.

00:15:45.300 --> 00:15:48.270
Well, this is a great
way to do it.

00:15:48.270 --> 00:15:51.330
If the backend's not up, the
task would wait in that queue

00:15:51.330 --> 00:15:54.340
until that backend is brought
online, and then it will

00:15:54.340 --> 00:15:56.530
process the tasks.

00:15:56.530 --> 00:15:59.220
So pull queue is you
don't really have a

00:15:59.220 --> 00:16:00.830
URL that you call.

00:16:00.830 --> 00:16:03.670
You just simply have a piece
of work that needs to be

00:16:03.670 --> 00:16:05.070
released by a worker.

00:16:05.070 --> 00:16:07.660
And this is great for processing
something off site

00:16:07.660 --> 00:16:10.260
or a batch operation or
something that doesn't really

00:16:10.260 --> 00:16:13.140
need to have that frontend
request that's initiated.

00:16:13.140 --> 00:16:16.600
But rather, you want to pull a
bunch of tasks and do a bunch

00:16:16.600 --> 00:16:19.740
of work in one thread.

00:16:19.740 --> 00:16:21.880
The way this simply works
is that you put the

00:16:21.880 --> 00:16:23.420
task on that queue.

00:16:23.420 --> 00:16:26.590
And then all work will come and
say, I want to lease one

00:16:26.590 --> 00:16:27.340
of the tasks.

00:16:27.340 --> 00:16:29.970
It gets that lease, and no other
worker can get a lease

00:16:29.970 --> 00:16:33.170
on that task for as long as
your lease is good for.

00:16:33.170 --> 00:16:36.300
You can specify a time, pretty
much like a timeout or an

00:16:36.300 --> 00:16:38.440
expiration for that lease.

00:16:38.440 --> 00:16:41.840
If you don't respond before
that timeout happens, then

00:16:41.840 --> 00:16:44.620
some other worker could pick
up that task and start

00:16:44.620 --> 00:16:45.720
processing it.

00:16:45.720 --> 00:16:49.460
Once you're done processing
it, your worker has to

00:16:49.460 --> 00:16:52.670
actually go back and delete the
task so that it's done,

00:16:52.670 --> 00:16:55.650
and no other worker
will pick it up.

00:16:55.650 --> 00:16:58.130
Here's how you would
code a pull queue.

00:16:58.130 --> 00:17:01.560
Java versus Python code
is very similar.

00:17:01.560 --> 00:17:04.369
You just get the new queue, and
you give it the name of

00:17:04.369 --> 00:17:05.290
that queue.

00:17:05.290 --> 00:17:07.849
And you put a new
task onto it.

00:17:07.849 --> 00:17:11.380
And set the lease here for an
hour, or 3,600 seconds.

00:17:11.380 --> 00:17:13.339
And I want to pull 100 tasks.

00:17:13.339 --> 00:17:15.680
That's going to give you an
array list of tasks that you

00:17:15.680 --> 00:17:17.190
can then crunch through.

00:17:17.190 --> 00:17:21.589
So if you only get one task,
you would specify 1.

00:17:21.589 --> 00:17:24.200
This pull queue has the REST
interface that's currently

00:17:24.200 --> 00:17:26.660
under "experimental."
But I know a lot of

00:17:26.660 --> 00:17:28.020
people that use it.

00:17:28.020 --> 00:17:29.290
Keep that in mind, though.

00:17:29.290 --> 00:17:32.250
If you run into an issue and
contact Support, they'll say

00:17:32.250 --> 00:17:33.440
it's experimental.

00:17:33.440 --> 00:17:35.510
They'll try to help you.

00:17:35.510 --> 00:17:38.230
You can specify an ACL--

00:17:38.230 --> 00:17:40.910
who can actually pull tasks
from this lease?

00:17:40.910 --> 00:17:42.390
That's if you're going
to use something

00:17:42.390 --> 00:17:43.890
outside of App Engine.

00:17:43.890 --> 00:17:46.410
If you want other workers
running on Rackspace or

00:17:46.410 --> 00:17:48.290
on-premise, for example.

00:17:48.290 --> 00:17:51.880
This is used for on-premise
processing when you do batches

00:17:51.880 --> 00:17:54.740
that you might pull data from
a financial or some other

00:17:54.740 --> 00:17:56.980
system that's on-premise
they don't want a

00:17:56.980 --> 00:17:59.030
direct connection to.

00:17:59.030 --> 00:18:01.580
You can have a branch process
running on-premise.

00:18:01.580 --> 00:18:04.690
Then you can upload the
data to your app.

00:18:04.690 --> 00:18:07.880
You could try this out on the
Google API Playground or the

00:18:07.880 --> 00:18:11.690
OAuth Playground to see
how it all works.

00:18:11.690 --> 00:18:15.070
So some of the best practices
is like an end-user request.

00:18:15.070 --> 00:18:17.970
This end-user request doesn't
actually have a user

00:18:17.970 --> 00:18:19.620
associated with it.

00:18:19.620 --> 00:18:24.000
So if you use the user API to
call up who's calling it, it

00:18:24.000 --> 00:18:25.970
will say no one's logged in.

00:18:25.970 --> 00:18:29.110
However, this is treated
as an admin request.

00:18:29.110 --> 00:18:32.560
So if you had protected your
application so that the URL

00:18:32.560 --> 00:18:36.110
that was being called by the
task queue is protected so it

00:18:36.110 --> 00:18:39.210
could be called by an admin
user, then it can also be

00:18:39.210 --> 00:18:41.090
called by a task queue.

00:18:41.090 --> 00:18:43.350
That's the way to make sure
that something on the

00:18:43.350 --> 00:18:46.250
internet's not just trying to
hit the URL and trigger

00:18:46.250 --> 00:18:48.150
whatever the task
could trigger.

00:18:48.150 --> 00:18:51.080
Yet as an admin when logged in,
you'll be able to trigger

00:18:51.080 --> 00:18:54.760
it yourself because it will
treat you as admin as well.

00:18:54.760 --> 00:18:59.070
So the best practice is to set
the security to only allow end

00:18:59.070 --> 00:19:02.650
points to be accessed by an
admin role unless, of course,

00:19:02.650 --> 00:19:06.220
you don't care, and maybe you
want to open publicly.

00:19:06.220 --> 00:19:09.030
Use different queues for
different purposes.

00:19:09.030 --> 00:19:11.960
So let's say that you have
a lot of very fast

00:19:11.960 --> 00:19:13.400
requests that come in.

00:19:13.400 --> 00:19:16.100
Well, it's much better to use
different queues so that you

00:19:16.100 --> 00:19:19.000
could specify parameters on
those queues to do different

00:19:19.000 --> 00:19:20.670
processing rates.

00:19:20.670 --> 00:19:22.480
You don't want to have
1,000 reports spin

00:19:22.480 --> 00:19:23.840
up at the same time.

00:19:23.840 --> 00:19:26.600
Each one could take five
minutes to process.

00:19:26.600 --> 00:19:29.550
That's going to spin up a lot
of frontend instances if

00:19:29.550 --> 00:19:31.030
you're doing it that way.

00:19:31.030 --> 00:19:33.080
Whereas you'd want that
throughput for something

00:19:33.080 --> 00:19:35.840
that's small, quick,
and fast request.

00:19:35.840 --> 00:19:39.400
You could actually, just like
a lot of our APIs, match up

00:19:39.400 --> 00:19:40.960
task creation.

00:19:40.960 --> 00:19:44.050
The only limitation is that
you can't do more than 100

00:19:44.050 --> 00:19:45.550
tasks per batch.

00:19:45.550 --> 00:19:48.300
This is great because you're
going to have a lot less usage

00:19:48.300 --> 00:19:51.730
than your API quota for the
task queue, as well as it

00:19:51.730 --> 00:19:53.360
could be much faster.

00:19:53.360 --> 00:19:57.480
So it will cost less, and
it will perform faster.

00:19:57.480 --> 00:20:01.020
Finally, there's a few best
practices for pull queues.

00:20:01.020 --> 00:20:03.380
Since we're not going to
automatically trigger this

00:20:03.380 --> 00:20:06.810
task for your instances, you
have to determine how quickly

00:20:06.810 --> 00:20:09.780
you want to spin workers
up to pull leases.

00:20:09.780 --> 00:20:12.460
It's something you have
to control yourself.

00:20:12.460 --> 00:20:14.980
You should make sure you choose
a lease time that is

00:20:14.980 --> 00:20:17.610
close to the worst-case
scenario.

00:20:17.610 --> 00:20:20.510
Otherwise you run into a
situation where something's

00:20:20.510 --> 00:20:24.540
still being worked on, and the
lease expires, and a different

00:20:24.540 --> 00:20:27.090
worker pulls that task
and starts executing

00:20:27.090 --> 00:20:29.200
that request as well.

00:20:29.200 --> 00:20:31.310
So the next topic is Cron.

00:20:31.310 --> 00:20:34.720
I like to call them
scheduled tasks.

00:20:34.720 --> 00:20:37.650
It's really just a task on the
task queue that are scheduled

00:20:37.650 --> 00:20:40.235
to run at certain intervals.

00:20:40.235 --> 00:20:44.180
It hits that URL, and the
application runs as an admin,

00:20:44.180 --> 00:20:45.520
and you could go on.

00:20:45.520 --> 00:20:48.110
And for all intents and
purposes, you could treat it

00:20:48.110 --> 00:20:51.340
just like any other task queue
that's being executed.

00:20:51.340 --> 00:20:54.370
If you've got an admin console,
then you have a queue

00:20:54.370 --> 00:20:58.410
that's there named underscore,
underscore, "cron." And you

00:20:58.410 --> 00:21:00.490
can actually go on in
the admin console

00:21:00.490 --> 00:21:01.900
and pause that queue.

00:21:01.900 --> 00:21:05.430
And it will stop processing
all your cron jobs.

00:21:05.430 --> 00:21:07.500
You could also tweak a
lot of the different

00:21:07.500 --> 00:21:09.106
details about the queue.

00:21:09.106 --> 00:21:11.510
You can't control the parameters
like how frequently

00:21:11.510 --> 00:21:15.220
it executes them, but
you can purge them.

00:21:15.220 --> 00:21:17.550
But you can also
pause a queue.

00:21:17.550 --> 00:21:20.570
So if something starts going
crazy, you could just pause

00:21:20.570 --> 00:21:21.920
and wait for it to finish.

00:21:21.920 --> 00:21:24.450
When it's working OK, you could
resume it all and just

00:21:24.450 --> 00:21:26.180
take off where it left off.

00:21:26.180 --> 00:21:30.470
So for cron jobs, you would
specify those in cron.xml or

00:21:30.470 --> 00:21:32.340
in cron.yaml.

00:21:32.340 --> 00:21:35.800
And the important thing is you
have to have at least the URL.

00:21:35.800 --> 00:21:37.930
And that's the URL that's
being called,

00:21:37.930 --> 00:21:40.220
as well as the schedule.

00:21:40.220 --> 00:21:42.820
The schedule is actually in
pretty plain English.

00:21:42.820 --> 00:21:47.690
Like every 24 hours means once
every 24 hours to execute.

00:21:47.690 --> 00:21:51.150
You could also specify if you
want the target version of

00:21:51.150 --> 00:21:52.690
your application to call.

00:21:52.690 --> 00:21:54.650
Now, there's a few parameters
that you have

00:21:54.650 --> 00:21:56.160
in these cron jobs.

00:21:56.160 --> 00:21:58.360
One is the URL on
the schedule.

00:21:58.360 --> 00:22:01.060
But you could also specify
the time zone.

00:22:01.060 --> 00:22:04.220
So you want to say at, 5:00 PM
in a particular time zone,

00:22:04.220 --> 00:22:05.550
call the task.

00:22:05.550 --> 00:22:07.700
Now, there's also a special
parameter here called

00:22:07.700 --> 00:22:09.130
Synchronized.

00:22:09.130 --> 00:22:13.120
And what that means is if it
synchronized, every single

00:22:13.120 --> 00:22:17.110
minute, call it regardless of
how long the task took to run.

00:22:17.110 --> 00:22:19.230
Whereas if you don't say
"synchronized"--

00:22:19.230 --> 00:22:22.180
let's say the task takes two
minutes to run, and you want

00:22:22.180 --> 00:22:24.080
to run it every two minutes.

00:22:24.080 --> 00:22:26.360
So it runs for the two minutes,
and then it waits

00:22:26.360 --> 00:22:28.780
another two minutes, and then
it runs it, effectively

00:22:28.780 --> 00:22:31.890
kicking off the cron job every
four minutes because it took

00:22:31.890 --> 00:22:34.870
that two minutes to execute.

00:22:34.870 --> 00:22:38.010
So now when a cron job is being
executed, the admin

00:22:38.010 --> 00:22:42.860
header says "X-AppEngine-Cron
is true." And that's how you

00:22:42.860 --> 00:22:45.910
can tell that it has actually
come from Cron

00:22:45.910 --> 00:22:47.480
versus the task queue.

00:22:47.480 --> 00:22:49.730
Just like the task queue, you'd
want to secure this

00:22:49.730 --> 00:22:51.280
using admin.

00:22:51.280 --> 00:22:54.060
But if you actually don't
specify "admin," and you

00:22:54.060 --> 00:22:56.270
specify "just have to
be logged in as a

00:22:56.270 --> 00:22:58.330
user," it will fail.

00:22:58.330 --> 00:23:01.050
Because remember, there's no
user associated with either

00:23:01.050 --> 00:23:03.630
Task Queue or with
the cron job.

00:23:03.630 --> 00:23:08.340
But it comes in as an admin, or
it passes the admin flag.

00:23:08.340 --> 00:23:10.980
Now that you have 20 different
cron jobs in three

00:23:10.980 --> 00:23:15.190
applications, if you need,
you could have up to 100.

