WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.230
 these capsules we will see a third 

00:00:01.860 --> 00:00:05.819
 learning algorithms not 

00:00:04.230 --> 00:00:10.620
 passive reinforcement the sentry box 

00:00:05.819 --> 00:00:13.230
 temporal differences a problem with 

00:00:10.620 --> 00:00:14.700
 the close relative pda we saw in the 

00:00:13.230 --> 00:00:17.190
 previous capsule is that we must 

00:00:14.700 --> 00:00:19.380
 update all values ​​of vds 

00:00:17.190 --> 00:00:22.260
 when we reverse our system 

00:00:19.380 --> 00:00:25.199
 of equations between the different 

00:00:22.260 --> 00:00:27.090
 values ​​in different states after 

00:00:25.199 --> 00:00:29.000
 every observation or actually at every 

00:00:27.090 --> 00:00:32.160
 once you want to get an estimate 

00:00:29.000 --> 00:00:34.920
 of the value of a dish from our 

00:00:32.160 --> 00:00:37.500
 estimated transition model so that's 

00:00:34.920 --> 00:00:39.570
 it's very expensive in practice if the 

00:00:37.500 --> 00:00:41.670
 number of rates is very big and you 

00:00:39.570 --> 00:00:43.559
 exponential so if we had a number 

00:00:41.670 --> 00:00:45.390
 state exponential so in a number 

00:00:43.559 --> 00:00:48.539
 exponential of elements to update 

00:00:45.390 --> 00:00:51.300
 it would be very expensive and also actually 

00:00:48.539 --> 00:00:55.530
 so useless to update the baroness 

00:00:51.300 --> 00:00:56.940
 and asi, the thenia breakdowns from the 

00:00:55.530 --> 00:01:00.210
 new observation that I come 

00:00:56.940 --> 00:01:06.020
 to acquire so if did I was past 

00:01:00.210 --> 00:01:10.110
 from zero to s1 and that I had a state of which 

00:01:06.020 --> 00:01:12.180
 s4 which is actually not attainable according to 

00:01:10.110 --> 00:01:13.439
 my transition model but what I 

00:01:12.180 --> 00:01:17.939
 know my environment that is not 

00:01:13.439 --> 00:01:21.420
 reachable from from scratch 

00:01:17.939 --> 00:01:24.060
 nest is healthy but at this point in fact 

00:01:21.420 --> 00:01:25.650
 it does not help me to update my 

00:01:24.060 --> 00:01:28.020
 values ​​a and 6 4 since it will not be 

00:01:25.650 --> 00:01:29.280
 influenced by what I learned by 

00:01:28.020 --> 00:01:34.380
 relation to my environment ie 

00:01:29.280 --> 00:01:36.149
 that I can go from eur 0 so 

00:01:34.380 --> 00:01:38.310
 so that's actually the reason for 

00:01:36.149 --> 00:01:40.710
 which we owe in the next of kin 

00:01:38.310 --> 00:01:42.990
 pdo update all the values ​​of 

00:01:40.710 --> 00:01:44.100
 for all possible states it is 

00:01:42.990 --> 00:01:49.289
 we have to solve our system 

00:01:44.100 --> 00:01:51.329
 of equations in full and it is estimated that 

00:01:49.289 --> 00:01:54.360
 low speed indirectly from 

00:01:51.329 --> 00:01:56.729
 of a transport model so a 

00:01:54.360 --> 00:01:58.259
 question is it could we 

00:01:56.729 --> 00:02:00.479
 avoid that is to say estimated 

00:01:58.259 --> 00:02:01.759
 directly vi ds since what is said 

00:02:00.479 --> 00:02:03.929
 what we are trying to do with 

00:02:01.759 --> 00:02:06.840
 learning parents inevitably passive 

00:02:03.929 --> 00:02:10.860
 ignoring by fessing modeling 

00:02:06.840 --> 00:02:15.019
 directly the model the distribution 

00:02:10.860 --> 00:02:15.019
 transition peace of mind even chrs and 

00:02:15.049 --> 00:02:19.200
 that's what we will try to do with 

00:02:17.269 --> 00:02:20.010
 learning speaks differences 

00:02:19.200 --> 00:02:23.819
 time 

00:02:20.010 --> 00:02:25.950
 so I will also give a little 

00:02:23.819 --> 00:02:27.440
 intuition with a very simple safe cases 

00:02:25.950 --> 00:02:31.610
 were commenting what we will get there 

00:02:27.440 --> 00:02:34.260
 so let's imagine that for a state s 

00:02:31.610 --> 00:02:36.870
 promoted the T1 to make a transfer 

00:02:34.260 --> 00:02:37.980
 to a given state and sprint a policy 

00:02:36.870 --> 00:02:39.930
 zero for all other authors 

00:02:37.980 --> 00:02:45.180
 possible so in this case also the 

00:02:39.930 --> 00:02:47.850
 value in s which is equal to this one of 

00:02:45.180 --> 00:02:51.209
 general way would simply be equal to 

00:02:47.850 --> 00:02:55.290
 at strengthening as plus my rate 

00:02:51.209 --> 00:02:57.480
 of discounted kid times worth a 

00:02:55.290 --> 00:02:59.549
 mind since in fact I would have a 

00:02:57.480 --> 00:03:01.790
 policy of 1 for a single spirit state 

00:02:59.549 --> 00:03:04.170
 even then 0 for all other states 

00:03:01.790 --> 00:03:06.989
 that's what Saturday actually is that the 

00:03:04.170 --> 00:03:08.639
 value in s should be very often 

00:03:06.989 --> 00:03:12.810
 case must be equal to the reinforcement 

00:03:08.639 --> 00:03:15.989
 at s more moto2 as both worth to a 

00:03:12.810 --> 00:03:17.850
 mind even graf and on that saturday 

00:03:15.989 --> 00:03:19.920
 it's rather than waiting for the end 

00:03:17.850 --> 00:03:23.459
 year is to update my 

00:03:19.920 --> 00:03:25.620
 estimate of emptying his well where is 

00:03:23.459 --> 00:03:27.450
 so it will be a plus as we do in 

00:03:25.620 --> 00:03:31.139
 direct estimates for example one 

00:03:27.450 --> 00:03:33.930
 could immediately get close b2s of 

00:03:31.139 --> 00:03:37.730
 this term here here ie air of s 

00:03:33.930 --> 00:03:39.989
 more gama times aird times empty es premium 

00:03:37.730 --> 00:03:42.180
 a way of people is to take 

00:03:39.989 --> 00:03:44.940
 mean for laughter between my values 

00:03:42.180 --> 00:03:47.220
 current speed then this term there 

00:03:44.940 --> 00:03:47.610
 here so I could without amending my 

00:03:47.220 --> 00:03:51.810
 new 

00:03:47.610 --> 00:03:56.970
 vds some 1 - alpha times empty 

00:03:51.810 --> 00:04:01.710
 more half of war this term here so rs 

00:03:56.970 --> 00:04:03.510
 more gamma fake opinion of mind is in 

00:04:01.710 --> 00:04:06.540
 does the alpha will act like a trick 

00:04:03.510 --> 00:04:08.190
 learning for example so 

00:04:06.540 --> 00:04:12.360
 stop back between 0 and 1 

00:04:08.190 --> 00:04:13.640
 and if they took as in fact his 

00:04:12.360 --> 00:04:17.310
 evidence as value 1 

00:04:13.640 --> 00:04:18.660
 this term here this raid 0 and this term 

00:04:17.310 --> 00:04:21.209
 the industry is one so in fact we 

00:04:18.660 --> 00:04:23.370
 would directly replace b2s by this 

00:04:21.209 --> 00:04:25.260
 value there too you would be gala desire 

00:04:23.370 --> 00:04:26.400
 deal we learn and nothing at all 

00:04:25.260 --> 00:04:28.380
 a zero learning since at each 

00:04:26.400 --> 00:04:30.750
 once we observe a transition to 

00:04:28.380 --> 00:04:32.820
 this term the suite multiplied by 0 so 

00:04:30.750 --> 00:04:34.890
 a variant 01 will determine to which 

00:04:32.820 --> 00:04:37.380
 point is what I want to change my 

00:04:34.890 --> 00:04:40.560
 current value of bds from 

00:04:37.380 --> 00:04:43.970
 the information I gained from the fact 

00:04:40.560 --> 00:04:46.590
 that I did a svs premium voltage 

00:04:43.970 --> 00:04:49.050
 and so we can rearrange that term 

00:04:46.590 --> 00:04:50.250
 here to get a rule with a 

00:04:49.050 --> 00:04:52.620
 shape a little different 

00:04:50.250 --> 00:04:54.240
 and if this rule who will define the 

00:04:52.620 --> 00:04:57.000
 flu of temporal differences we 

00:04:54.240 --> 00:05:01.260
 call in English we use it often 

00:04:57.000 --> 00:05:04.410
 the acronym you were so my patent of 

00:05:01.260 --> 00:05:07.320
 m6 i observe a tradition of svs prime 

00:05:04.410 --> 00:05:08.850
 it's going to be the current value plus one 

00:05:07.320 --> 00:05:10.110
 learning rate as we do 

00:05:08.850 --> 00:05:11.610
 when going downhill 

00:05:10.110 --> 00:05:12.690
 stochastic guardians at learning 

00:05:11.610 --> 00:05:15.479
 automatic 

00:05:12.690 --> 00:05:17.640
 my current value of ds plus montaut 

00:05:15.479 --> 00:05:19.410
 learning times 

00:05:17.640 --> 00:05:22.350
 the information I gained to know 

00:05:19.410 --> 00:05:27.000
 that speed should get closer to rs 

00:05:22.350 --> 00:05:30.600
 plus my case my faith v2 is this premium 

00:05:27.000 --> 00:05:33.060
 - my current value so if for example 

00:05:30.600 --> 00:05:36.450
 the present value would empty it was more 

00:05:33.060 --> 00:05:39.450
 small that rs more than my faith 

00:05:36.450 --> 00:05:42.120
 expresses this theme succeed here would be 

00:05:39.450 --> 00:05:42.750
 positive so I would move speed towards 

00:05:42.120 --> 00:05:44.640
 the Red 

00:05:42.750 --> 00:05:47.930
 so actually I would bring him closer 

00:05:44.640 --> 00:05:49.710
 of those words too so 

00:05:47.930 --> 00:05:51.000
 distance learning 

00:05:49.710 --> 00:05:53.610
 temporal it looks a lot like 

00:05:51.000 --> 00:05:55.440
 gradient drawing or in fact we compare 

00:05:53.610 --> 00:05:58.320
 our current estimate with 

00:05:55.440 --> 00:05:59.630
 the information we have gained from 

00:05:58.320 --> 00:06:02.210
 that we made a tension of 

00:05:59.630 --> 00:06:03.410
 a super mind even is so these are 

00:06:02.210 --> 00:06:08.230
 come to finish is what I have 

00:06:03.410 --> 00:06:08.230
 increase or decrease my v2 value 

00:06:10.000 --> 00:06:15.500
 while here we have a pure code from 

00:06:12.740 --> 00:06:16.390
 rationbook norvig book and that's the 

00:06:15.500 --> 00:06:18.560
 where 

00:06:16.390 --> 00:06:19.730
 here we suppose we observe no 

00:06:18.560 --> 00:06:21.550
 only the rates vary as to 

00:06:19.730 --> 00:06:25.790
 traditional but also the strengthening 

00:06:21.550 --> 00:06:27.530
 and we ask ourselves who made it 

00:06:25.790 --> 00:06:28.280
 initially I do not know the baroness 

00:06:27.530 --> 00:06:30.290
 me necessarily 

00:06:28.280 --> 00:06:33.350
 so if it's a new state it 

00:06:30.290 --> 00:06:34.490
 just accessed the value for this 

00:06:33.350 --> 00:06:38.300
 showcase valves initialize to 

00:06:34.490 --> 00:06:41.390
 observed reinforcement and then here before 

00:06:38.300 --> 00:06:44.060
 I found myself a state is this what I 

00:06:41.390 --> 00:06:45.830
 think and apply this corner here the rule 

00:06:44.060 --> 00:06:49.100
 we just saw that I'm going to 

00:06:45.830 --> 00:06:50.960
 take my current value as plus a 

00:06:49.100 --> 00:06:53.390
 learning rate alfort times my 

00:06:50.960 --> 00:06:55.580
 reinforcement that I had when I was 

00:06:53.390 --> 00:06:58.070
 is what you like in Russia more 

00:06:55.580 --> 00:07:00.380
 kid trained value expresses who 

00:06:58.070 --> 00:07:04.810
 show the state I did not reach - 

00:07:00.380 --> 00:07:04.810
 mr currently estimated value for s 

00:07:07.000 --> 00:07:15.230
 so in fact here we see that a written 

00:07:10.940 --> 00:07:17.390
 man so we currently have in this 

00:07:15.230 --> 00:07:19.970
 algorithm we estimate the number of times 

00:07:17.390 --> 00:07:21.380
 in fact that I visited the state s is this 

00:07:19.970 --> 00:07:23.090
 the only reason it's 

00:07:21.380 --> 00:07:24.680
 use these for the case where I would like 

00:07:23.090 --> 00:07:27.350
 that my learning turn varies in 

00:07:24.680 --> 00:07:29.600
 depending on the number of times I reach 

00:07:27.350 --> 00:07:31.160
 a state so in practice we observe 

00:07:29.600 --> 00:07:34.700
 often only have a rate 

00:07:31.160 --> 00:07:36.260
 high learning early then more 

00:07:34.700 --> 00:07:38.810
 beautiful towards the end when that's done 

00:07:36.260 --> 00:07:41.300
 many times that I have reached a 

00:07:38.810 --> 00:07:43.430
 some state tends to better 

00:07:41.300 --> 00:07:45.230
 converge a bit like for the mouth of the 

00:07:43.430 --> 00:07:48.260
 collectors have for example have a rate 

00:07:45.230 --> 00:07:50.180
 of diminishing learning allows us 

00:07:48.260 --> 00:07:51.230
 to obtain a progression of 

00:07:50.180 --> 00:07:53.570
 learning that is a bit more 

00:07:51.230 --> 00:07:58.400
 stable and so that's the only reason 

00:07:53.570 --> 00:08:00.020
 for which one does not end so is 

00:07:58.400 --> 00:08:03.620
 so tonight this term but here it's 

00:08:00.020 --> 00:08:04.910
 really just skin if we want to varied the 

00:08:03.620 --> 00:08:07.420
 learning rate as you go 

00:08:04.910 --> 00:08:07.420
 of learning 

00:08:08.260 --> 00:08:13.710
 Or give you an example 

00:08:10.470 --> 00:08:16.860
 so we have an environment with 3 and as 

00:08:13.710 --> 00:08:18.660
 0 to 2 2 and at the very beginning even before 

00:08:16.860 --> 00:08:20.730
 that we started learning a 

00:08:18.660 --> 00:08:23.520
 host estimates values ​​to it 0 1 

00:08:20.730 --> 00:08:24.600
 100 m2 is going to be zero so I have to 

00:08:23.520 --> 00:08:26.730
 also that I do not know my function 

00:08:24.600 --> 00:08:28.410
 reinforcement if I had known him actually 

00:08:26.730 --> 00:08:32.400
 I could have used immediately my 

00:08:28.410 --> 00:08:34.169
 values ​​so here 5 could be r20 so 

00:08:32.400 --> 00:08:36.570
 I had known my job function 

00:08:34.169 --> 00:08:39.120
 reinforcement and same thing for 

00:08:36.570 --> 00:08:40.590
 succeed a sweet and we will use as 

00:08:39.120 --> 00:08:43.250
 all learning to zero point 

00:08:40.590 --> 00:08:43.250
 in this example 

00:08:46.880 --> 00:08:53.730
 so I start the simulation 

00:08:50.640 --> 00:08:56.160
 i am in the initial state s 0 i'm watching 

00:08:53.730 --> 00:08:57.230
 the reinforcement of - 0 point so I 

00:08:56.160 --> 00:09:01.590
 updates 

00:08:57.230 --> 00:09:03.600
 initialized actually my value to 1 0 at the 

00:09:01.590 --> 00:09:08.340
 variant of the observed enhancement that - 0 

00:09:03.600 --> 00:09:12.780
 point I'm watching a transition 

00:09:08.340 --> 00:09:15.270
 of f0 as0 so a transition to 

00:09:12.780 --> 00:09:16.800
 yourself to the same state while 

00:09:15.270 --> 00:09:19.740
 from that I can update but 

00:09:16.800 --> 00:09:22.320
 valence 0 1 and a crime in august 1.0 

00:09:19.740 --> 00:09:24.089
 point once the information I come 

00:09:22.320 --> 00:09:25.350
 to whom is due to the fact that I am 

00:09:24.089 --> 00:09:29.790
 spent 2 0 

00:09:25.350 --> 00:09:31.770
 ba s 0 - morning their courage so I'm going 

00:09:29.790 --> 00:09:35.040
 make zero more 

00:09:31.770 --> 00:09:38.670
 my discount rate that I use it 

00:09:35.040 --> 00:09:39.990
 0.5 again here empty times es0 and I 

00:09:38.670 --> 00:09:44.640
 subtrais sam also estimated 

00:09:39.990 --> 00:09:46.020
 currently from 1 to 0 which makes 2 0 and 

00:09:44.640 --> 00:09:48.510
 that's what will determine in what 

00:09:46.020 --> 00:09:50.370
 direction i'm going to edit and get 

00:09:48.510 --> 00:09:53.760
 a value of v2 0 

00:09:50.370 --> 00:09:56.550
 so I replace here I have less 0.01 

00:09:53.760 --> 00:09:58.890
 pommard at the moment 2-0 against 

00:09:56.550 --> 00:10:03.200
 the learning moment necessarily observed 

00:09:58.890 --> 00:10:03.200
 which is - 0 point 1 - 

00:10:04.560 --> 00:10:13.350
 - 0.5 times the value to zero so that's me 

00:10:08.160 --> 00:10:16.470
 gives less than 0.05 plus my value 

00:10:13.350 --> 00:10:19.110
 - in fact - my value in sd rookie and - 

00:10:16.470 --> 00:10:21.150
 0 point so I have zero point 

00:10:19.110 --> 00:10:24.390
 so that gives me a new value of 

00:10:21.150 --> 00:10:27.110
 s 0 that will be now at - 0 point 1 

00:10:24.390 --> 00:10:30.450
 05 

00:10:27.110 --> 00:10:31.620
 i observe a transition of m 01 s 1 

00:10:30.450 --> 00:10:32.010
 it's the first time I've come to a 

00:10:31.620 --> 00:10:35.220
 age 

00:10:32.010 --> 00:10:38.390
 initialized its value to the reinforcement 

00:10:35.220 --> 00:10:40.650
 observed which is - 0 point 1 

00:10:38.390 --> 00:10:42.990
 and here I can update one more 

00:10:40.650 --> 00:10:46.440
 time is this my value to zero because 

00:10:42.990 --> 00:10:49.110
 I observed a pressure of zero towards s 

00:10:46.440 --> 00:10:51.390
 end so I'm going to compare site 

00:10:49.110 --> 00:10:53.250
 of information with my present value and 

00:10:51.390 --> 00:10:55.080
 that was giving me my direction of putting 

00:10:53.250 --> 00:10:58.320
 up to date and then how much I have to 

00:10:55.080 --> 00:11:02.220
 move to this new value 

00:10:58.320 --> 00:11:04.860
 who owes who is observed to know 

00:11:02.220 --> 00:11:06.660
 reinforcement and zero plus point 

00:11:04.860 --> 00:11:10.560
 5 times my current value 

00:11:06.660 --> 00:11:13.400
 pure spirit I replace it gives me and I 

00:11:10.560 --> 00:11:17.760
 go calculating it gives me - 0 point 1 

00:11:13.400 --> 00:11:21.390
 095 I observe being in a transition 

00:11:17.760 --> 00:11:23.580
 of s inverse s 0 and I continue the 

00:11:21.390 --> 00:11:25.080
 same way so on Saturday now 

00:11:23.580 --> 00:11:30.330
 commented what I should update 

00:11:25.080 --> 00:11:32.700
 my moral worth should be 

00:11:30.330 --> 00:11:34.980
 to be modified according to the following rule 

00:11:32.700 --> 00:11:38.550
 so I have 0.1 learning ride 

00:11:34.980 --> 00:11:41.240
 times the reinforcement s1 plus 0.5 for the 

00:11:38.550 --> 00:11:44.100
 value as0 the touch will make me render 

00:11:41.240 --> 00:11:48.120
 less but value currently estimated at 

00:11:44.100 --> 00:11:49.740
 to be a game replaces with the $ 200 that 

00:11:48.120 --> 00:11:52.080
 gives me a new value of 

00:11:49.740 --> 00:11:57.320
 reinforcement of - 0 point of value 

00:11:52.080 --> 00:11:59.760
 by which 2 - 0 points 1 054 7 5 

00:11:57.320 --> 00:12:04.170
 I observed in a transition of s 

00:11:59.760 --> 00:12:05.459
 you have one so but here it is this 

00:12:04.170 --> 00:12:08.010
 value to zero that I'm going to update 

00:12:05.459 --> 00:12:09.670
 I hold I apply exactly the same 

00:12:08.010 --> 00:12:13.250
 rule of 

00:12:09.670 --> 00:12:15.020
 temporal differences and finally 

00:12:13.250 --> 00:12:18.320
 I arrive in s2 this is the first time 

00:12:15.020 --> 00:12:20.360
 that I observe this pressure so that 

00:12:18.320 --> 00:12:24.200
 while I get to the state out the 

00:12:20.360 --> 00:12:25.640
 value s 2 i'm going to initialize to 1 and 

00:12:24.200 --> 00:12:29.990
 now I can update the 

00:12:25.640 --> 00:12:32.540
 value in s1 by comparing it with the 

00:12:29.990 --> 00:12:33.860
 reinforcement s1 +10 points 5 times the 

00:12:32.540 --> 00:12:38.150
 value as2 because I made a 

00:12:33.860 --> 00:12:40.520
 reverse transition is hideous and I 

00:12:38.150 --> 00:12:45.080
 replace that gives me a value to s1 

00:12:40.520 --> 00:12:46.430
 update 10.055 0725 and that's the 

00:12:45.080 --> 00:12:47.870
 wounded woman is so now I 

00:12:46.430 --> 00:12:50.270
 will be ready to start a new 

00:12:47.870 --> 00:12:53.000
 try and I'll continue like this at 

00:12:50.270 --> 00:12:54.920
 update all my values ​​up 

00:12:53.000 --> 00:12:56.690
 what I reach certain criteria 

00:12:54.920 --> 00:12:58.100
 stopping to know maybe the number 

00:12:56.690 --> 00:13:02.420
 of tests that I wanted to do in everything 

00:12:58.100 --> 00:13:04.790
 a species of a comparison in at 

00:13:02.420 --> 00:13:06.740
 how much my values ​​have charged a 

00:13:04.790 --> 00:13:07.820
 iteration then if it goes or a 

00:13:06.740 --> 00:13:09.950
 certain threshold I could stop 

00:13:07.820 --> 00:13:12.230
 learning that's the algorithm 

00:13:09.950 --> 00:13:15.650
 by learning apprenticeship 

00:13:12.230 --> 00:13:18.490
 parents necessarily passive based on the 

00:13:15.650 --> 00:13:18.490
 temporary difference 

