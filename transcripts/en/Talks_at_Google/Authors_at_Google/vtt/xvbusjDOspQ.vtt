WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.480
[MUSIC PLAYING]

00:00:05.322 --> 00:00:06.780
CORY DOCTOROW:
Thank you very much.

00:00:06.780 --> 00:00:08.113
It's a pleasure to be back here.

00:00:08.113 --> 00:00:09.380
Thank you for coming.

00:00:09.380 --> 00:00:12.950
SPEAKER: Why don't you start
by telling us about the book.

00:00:12.950 --> 00:00:15.230
CORY DOCTOROW: Yeah, I
mean, they're, in some ways,

00:00:15.230 --> 00:00:21.660
my 2017 therapeutic writing
Trump mishegoss stories.

00:00:21.660 --> 00:00:22.880
I didn't plan any of them.

00:00:22.880 --> 00:00:26.060
They all just sort
of blurted out.

00:00:26.060 --> 00:00:27.800
So "Unauthorized
Bread" is a story

00:00:27.800 --> 00:00:30.500
about people in refugee housing,
where all the appliances are

00:00:30.500 --> 00:00:34.940
designed to have DRM, to lock
them into vendor ecosystems.

00:00:34.940 --> 00:00:37.838
So you can only toast authorized
bread in your toaster,

00:00:37.838 --> 00:00:40.130
you can only store authorized
groceries in your fridge,

00:00:40.130 --> 00:00:41.030
and so on.

00:00:41.030 --> 00:00:45.110
And that is bad enough.

00:00:45.110 --> 00:00:47.840
But then the hedge fund
that owns the back end

00:00:47.840 --> 00:00:50.540
for all this stuff
financially engineers itself

00:00:50.540 --> 00:00:51.960
into bankruptcy.

00:00:51.960 --> 00:00:53.750
And so everything stops working.

00:00:53.750 --> 00:00:56.900
And so they learn to
jailbreak their appliances,

00:00:56.900 --> 00:01:00.170
out of necessity, and then
out of sheer joy of seizing

00:01:00.170 --> 00:01:01.650
the means of information.

00:01:01.650 --> 00:01:04.190
But then they learn that the
companies are being rebooted

00:01:04.190 --> 00:01:07.310
out of bankruptcy, and that very
soon their telemetry is going

00:01:07.310 --> 00:01:09.758
to detect that the
devices were jailbroken,

00:01:09.758 --> 00:01:12.050
and then they're going to
face DMCA criminal liability.

00:01:12.050 --> 00:01:14.780
And because they're
all in refugee housing,

00:01:14.780 --> 00:01:17.750
that will mean being
deported and possibly killed.

00:01:17.750 --> 00:01:21.290
And so it becomes this
very high-stakes fight,

00:01:21.290 --> 00:01:25.520
where this woman who has
created this kind of Youth

00:01:25.520 --> 00:01:28.280
Brigade of kids who go round
and jailbreak everyone's

00:01:28.280 --> 00:01:31.070
devices now has to convince
these kids who are completely

00:01:31.070 --> 00:01:33.530
foursquare against it to
go and restore everything

00:01:33.530 --> 00:01:36.363
to factory defaults without
scaring the pants off of them.

00:01:36.363 --> 00:01:38.030
They end up working
with friendly techs,

00:01:38.030 --> 00:01:39.488
and trying to figure
out if there's

00:01:39.488 --> 00:01:42.410
a way they can use VMs to
cheat the telemetry, and so on.

00:01:42.410 --> 00:01:45.500
So it's a novel about the
kind of class dimension

00:01:45.500 --> 00:01:48.920
and surveillance
dimension vendor lock-in,

00:01:48.920 --> 00:01:51.230
and the way that that
plays out depending

00:01:51.230 --> 00:01:53.810
on what kind of privilege
you have in the world.

00:01:53.810 --> 00:01:55.790
And then the next story
is this story called

00:01:55.790 --> 00:01:58.160
"Radicalized," the title story.

00:01:58.160 --> 00:02:01.550
And it's about super-entitled
middle class white dudes

00:02:01.550 --> 00:02:04.190
who watch their loved ones
die of preventable illnesses

00:02:04.190 --> 00:02:06.717
because their insurers
won't cover therapies,

00:02:06.717 --> 00:02:09.050
and who find themselves on
these dark net message boards

00:02:09.050 --> 00:02:11.383
where they're radicalized
into being suicide bombers who

00:02:11.383 --> 00:02:13.100
kill health care executives.

00:02:13.100 --> 00:02:18.680
And a lot of it is about
whether or not America will ever

00:02:18.680 --> 00:02:21.000
call a white dude a terrorist.

00:02:21.000 --> 00:02:24.620
And some of it is about
how just the cause actually

00:02:24.620 --> 00:02:26.960
turns out to be, and so on.

00:02:26.960 --> 00:02:29.690
And also, this very toxic
observation that many

00:02:29.690 --> 00:02:31.550
have made about
the incel movement,

00:02:31.550 --> 00:02:34.820
that in normal support
message boards--

00:02:34.820 --> 00:02:37.220
if you're a recovering
alcoholic and you're

00:02:37.220 --> 00:02:40.370
in an online community,
all the elder

00:02:40.370 --> 00:02:41.870
states people of
that community are

00:02:41.870 --> 00:02:44.750
people who beat the thing
you're struggling with

00:02:44.750 --> 00:02:47.360
and are now staying are
staying around to guide people.

00:02:47.360 --> 00:02:49.610
But people who recover
from being in cells

00:02:49.610 --> 00:02:51.320
don't hang out in
incel message boards.

00:02:51.320 --> 00:02:53.720
So all the elder states
people of the incel message

00:02:53.720 --> 00:02:57.560
communities are the most
toxic, most broken people.

00:02:57.560 --> 00:02:59.750
And they're the ones
saying, yeah, get in a van,

00:02:59.750 --> 00:03:01.125
and drive it
through the streets,

00:03:01.125 --> 00:03:03.433
and kill as many people as
you can, they deserve it.

00:03:03.433 --> 00:03:05.600
And so these communities
don't get better over time.

00:03:05.600 --> 00:03:06.770
They get worse.

00:03:06.770 --> 00:03:10.190
And that's the kind of
community that I'm exploring.

00:03:10.190 --> 00:03:12.330
The third story's
called "Model Minority,"

00:03:12.330 --> 00:03:15.140
and it's about a thinly-veiled
analog to Superman

00:03:15.140 --> 00:03:18.590
intervening in a beating
by the same cops who

00:03:18.590 --> 00:03:23.180
killed Eric Garner on Staten
Island, who then discovers

00:03:23.180 --> 00:03:25.460
the fact that he's
viewed as both white

00:03:25.460 --> 00:03:27.920
and a human are
super-contingent,

00:03:27.920 --> 00:03:31.460
and that there are some things
that America won't tolerate

00:03:31.460 --> 00:03:33.950
from the people who are
honorarily white and human.

00:03:33.950 --> 00:03:35.960
And it takes the
form, in large part,

00:03:35.960 --> 00:03:38.360
of Socratic dialogues
with Bruce Wayne, who's

00:03:38.360 --> 00:03:39.980
a military contractor
who provides

00:03:39.980 --> 00:03:43.790
predictive policing
software to the NYPD,

00:03:43.790 --> 00:03:47.000
and who's in fact basically
responsible for all of this.

00:03:47.000 --> 00:03:48.452
And then the last
story is a story

00:03:48.452 --> 00:03:49.910
called "The Mask
of the Red Death,"

00:03:49.910 --> 00:03:52.140
named after the Edgar
Allan Poe story.

00:03:52.140 --> 00:03:55.220
And it's about
preppers who build

00:03:55.220 --> 00:03:58.610
a luxury bunker just outside
of Phoenix and repair to it

00:03:58.610 --> 00:04:04.400
as soon as the catastrophe
hits, and fancy themselves

00:04:04.400 --> 00:04:07.070
living out a kind of boy's
own adventure of the elites

00:04:07.070 --> 00:04:10.460
surviving while all
the useless takers die

00:04:10.460 --> 00:04:13.220
in Phoenix because they didn't
have the wisdom to prepare

00:04:13.220 --> 00:04:16.279
these bolt holes, but who in
fact end up dying of cholera

00:04:16.279 --> 00:04:19.190
because the useful
people are the people who

00:04:19.190 --> 00:04:22.640
stay behind in Phoenix and got
the sanitation working again.

00:04:22.640 --> 00:04:24.770
And these ubermenschen
have to discover

00:04:24.770 --> 00:04:26.158
that you can't shoot germs.

00:04:26.158 --> 00:04:28.700
It was originally going to be
called "You Can't Shoot Germs."

00:04:28.700 --> 00:04:29.900
But I think "Mask
of the Red Death"

00:04:29.900 --> 00:04:31.490
has got the
appropriate gravitas.

00:04:31.490 --> 00:04:32.870
So it's these four novellas.

00:04:32.870 --> 00:04:33.680
They're all linked.

00:04:33.680 --> 00:04:36.680
They wrap around a lot
of the same themes.

00:04:36.680 --> 00:04:41.150
They touch on a lot of the
same geographic locations.

00:04:41.150 --> 00:04:43.130
Some of the details recur.

00:04:43.130 --> 00:04:44.750
So some of the
OPSEC that Superman

00:04:44.750 --> 00:04:49.040
uses to avoid having his secret
identity outed by the NSA--

00:04:49.040 --> 00:04:50.780
he's got a randomizer
that tells him

00:04:50.780 --> 00:04:53.480
where to take off from so
that you can't draw a map

00:04:53.480 --> 00:04:57.500
to see where Superman is
always sighted by ground radar.

00:04:57.500 --> 00:04:59.810
So he runs very quickly
to somewhere else,

00:04:59.810 --> 00:05:01.460
and then takes off.

00:05:01.460 --> 00:05:03.860
And this prepper has
also got a randomizer

00:05:03.860 --> 00:05:08.150
that makes him get in his
disguised, armored-up, like,

00:05:08.150 --> 00:05:12.260
F-150 with a camper bed that is
full of all of his pepper gear,

00:05:12.260 --> 00:05:14.390
and put some fishing
rods in the front,

00:05:14.390 --> 00:05:17.280
and just drive it off
of his gated community

00:05:17.280 --> 00:05:20.330
at this randomizer's interval
so that there isn't someone who

00:05:20.330 --> 00:05:23.900
goes, hey, that's weird, why is
that guy driving his F-150 out

00:05:23.900 --> 00:05:25.708
of this gate-guarded
community now?

00:05:25.708 --> 00:05:27.500
He must be going to
his prepper hidey hole.

00:05:27.500 --> 00:05:29.875
We'll follow him there, and
kill him, and take his stuff.

00:05:29.875 --> 00:05:32.240
So a lot of us a lot
of the same things

00:05:32.240 --> 00:05:34.010
kind of repeat and
repeat, and the themes

00:05:34.010 --> 00:05:35.840
wrap around each other.

00:05:35.840 --> 00:05:38.690
We're not using the word
"collection" for it.

00:05:38.690 --> 00:05:41.600
It's just a fiction book.

00:05:41.600 --> 00:05:45.020
And it's been likened by
my publisher to the book

00:05:45.020 --> 00:05:48.980
that Stephen King wrote, where
the body-- the story that

00:05:48.980 --> 00:05:51.890
became "Stand by Me" comes
out of "Four Seasons," that's

00:05:51.890 --> 00:05:55.790
just four
thematically-connected but not

00:05:55.790 --> 00:05:58.743
continuity-connected tales.

00:05:58.743 --> 00:06:01.160
SPEAKER: Had any of the stories
been published previously?

00:06:01.160 --> 00:06:03.710
CORY DOCTOROW: No, this
is all original stuff.

00:06:03.710 --> 00:06:06.890
And as I say, it wasn't planned.

00:06:06.890 --> 00:06:08.690
"Unauthorized
Bread" originally--

00:06:08.690 --> 00:06:11.540
I sent it to my editor because
it was such a weird and awkward

00:06:11.540 --> 00:06:12.787
length that I didn't think--

00:06:12.787 --> 00:06:14.870
I didn't know what to do
with it when it was done.

00:06:14.870 --> 00:06:16.830
It's 30,000 words long.

00:06:16.830 --> 00:06:19.120
And I thought that he was
going to say, well, we

00:06:19.120 --> 00:06:19.910
don't know what to do with us.

00:06:19.910 --> 00:06:21.590
And instead, he sent
me an email, like,

00:06:21.590 --> 00:06:24.710
the next day saying, oh my God,
this is amazing and timely,

00:06:24.710 --> 00:06:27.653
we want to publish it in two
months as a standalone book.

00:06:27.653 --> 00:06:29.570
And I said, well, that's
great, let's do that.

00:06:29.570 --> 00:06:30.785
And we want to pay
you more than what

00:06:30.785 --> 00:06:32.750
you got for your first three
novels for it, which was also

00:06:32.750 --> 00:06:33.410
great.

00:06:33.410 --> 00:06:35.600
And then and then I
sent him the next one

00:06:35.600 --> 00:06:37.130
I sent him the
model minority was

00:06:37.130 --> 00:06:38.420
like this is also
awesome we'll do it

00:06:38.420 --> 00:06:39.560
the next month as a standalone.

00:06:39.560 --> 00:06:41.270
I'm like, I've got
two more in the works.

00:06:41.270 --> 00:06:43.270
So that's how we came to
wait about seven months

00:06:43.270 --> 00:06:45.970
to do it and publish
them all in one go.

00:06:45.970 --> 00:06:49.100
Topic, who are the TV and
movie studio associated

00:06:49.100 --> 00:06:51.830
with the same company
that own "The Intercept"

00:06:51.830 --> 00:06:53.420
have bought the TV
rights for this,

00:06:53.420 --> 00:06:55.287
and are developing it now.

00:06:55.287 --> 00:06:56.870
SPEAKER: And you
mentioned timeliness.

00:06:56.870 --> 00:06:58.670
And reading
"Unauthorized Bread,"

00:06:58.670 --> 00:07:00.650
it seemed very much of the now.

00:07:00.650 --> 00:07:03.837
And the book also struck
me as being more political,

00:07:03.837 --> 00:07:05.420
which is kind of a
funny thing to say.

00:07:05.420 --> 00:07:06.878
Your work has always
been political

00:07:06.878 --> 00:07:08.420
in one way or another--

00:07:08.420 --> 00:07:10.940
freedom of access to
information, doing it yourself.

00:07:10.940 --> 00:07:14.173
But this one seems a little
more pointedly political.

00:07:14.173 --> 00:07:15.840
CORY DOCTOROW: Two
things have happened.

00:07:15.840 --> 00:07:18.380
One is that politics have
just become more salient.

00:07:18.380 --> 00:07:21.110
We are just in a moment
where you can't not

00:07:21.110 --> 00:07:23.742
talk about politics,
for better or for worse.

00:07:23.742 --> 00:07:25.700
The other thing is that
the political dimension

00:07:25.700 --> 00:07:30.740
of information has now
become much more obvious.

00:07:30.740 --> 00:07:35.400
I don't know if any of you
work in like UX or UI design.

00:07:35.400 --> 00:07:38.300
There's this thing that happens
with UI design and UX, where

00:07:38.300 --> 00:07:41.270
when you start, you're trying
to convey a bunch of ideas

00:07:41.270 --> 00:07:44.090
that are really novel to the
people who you're conveying

00:07:44.090 --> 00:07:45.180
them to.

00:07:45.180 --> 00:07:47.630
If you think about it,
early GUIs, just the idea

00:07:47.630 --> 00:07:50.180
that there is a file,
and then that a file is

00:07:50.180 --> 00:07:51.620
a thing that you save, right?

00:07:51.620 --> 00:07:54.470
And that you have reversion,
and you have Control-Z,

00:07:54.470 --> 00:07:56.510
and all of those things
are kind of novel ideas.

00:07:56.510 --> 00:07:58.670
Undo is not a
thing that you just

00:07:58.670 --> 00:08:00.470
know about if you've
never used a computer.

00:08:00.470 --> 00:08:02.840
Undo is a weird idea.

00:08:02.840 --> 00:08:04.820
And over time, we get
better at conveying

00:08:04.820 --> 00:08:05.810
what those ideas mean.

00:08:05.810 --> 00:08:08.600
But also, the urgency
of conveying them

00:08:08.600 --> 00:08:11.450
is diminished, because people
are meeting you halfway.

00:08:11.450 --> 00:08:13.670
Now, just everybody,
people who don't

00:08:13.670 --> 00:08:17.540
know what a floppy disk is know
that a floppy disk means Save.

00:08:17.540 --> 00:08:20.090
And just that icon, it
doesn't need a tool tip

00:08:20.090 --> 00:08:21.900
anymore the way it used to.

00:08:21.900 --> 00:08:24.560
And in the same way,
writing political fiction

00:08:24.560 --> 00:08:26.990
about information
doesn't require

00:08:26.990 --> 00:08:28.190
nearly so much handholding.

00:08:28.190 --> 00:08:29.610
You can just jump right in.

00:08:29.610 --> 00:08:34.130
Because in an age of Black Lives
Matter, and mimetic warfare,

00:08:34.130 --> 00:08:40.130
and questions about
whether or not

00:08:40.130 --> 00:08:44.450
our power grid is vulnerable
to cyber attack and so on,

00:08:44.450 --> 00:08:46.100
talking about
information as being

00:08:46.100 --> 00:08:49.250
this important dimension
to our politics

00:08:49.250 --> 00:08:51.170
is not a radical idea anymore.

00:08:51.170 --> 00:08:54.635
And that means that you
can dig into more nuance.

00:08:54.635 --> 00:08:57.920
The people can be centered
more because the information

00:08:57.920 --> 00:08:59.363
dimension is just obvious.

00:08:59.363 --> 00:09:00.530
SPEAKER: That's interesting.

00:09:00.530 --> 00:09:03.230
It also make me remember
that it struck me

00:09:03.230 --> 00:09:06.140
while reading the story that it
seems slightly more educational

00:09:06.140 --> 00:09:07.790
than past fiction has been.

00:09:07.790 --> 00:09:10.400
There are several sections--

00:09:10.400 --> 00:09:12.060
it's not overly
heavy exposition,

00:09:12.060 --> 00:09:14.760
but you go pretty detailed
into digital copyright,

00:09:14.760 --> 00:09:17.810
kind of aside how
to jailbreak DRM.

00:09:17.810 --> 00:09:21.380
It seemed like you were
educating through the story

00:09:21.380 --> 00:09:22.110
as well.

00:09:22.110 --> 00:09:24.485
CORY DOCTOROW: So I think
there's two dimensions to that.

00:09:24.485 --> 00:09:27.110
One is like, I think people
are tolerant of exposition when

00:09:27.110 --> 00:09:28.360
it's news you can use, right?

00:09:28.360 --> 00:09:30.230
Like, oh, that's
how that works, now

00:09:30.230 --> 00:09:32.000
a whole bunch of stuff in my
life suddenly makes sense.

00:09:32.000 --> 00:09:33.770
Like, here's what a
VM is, or whatever.

00:09:33.770 --> 00:09:36.530
The other thing,
though, is that there

00:09:36.530 --> 00:09:39.290
is a longevity that comes
from writing fiction where

00:09:39.290 --> 00:09:41.180
the technology is rigorous.

00:09:41.180 --> 00:09:44.300
We've had a certain amount
of technological breakthrough

00:09:44.300 --> 00:09:46.710
in all of our careers and lives.

00:09:46.710 --> 00:09:50.090
But there are some fundamentals
that remain pretty fundamental.

00:09:50.090 --> 00:09:53.690
Like, you know, complexity and
our understanding of complexity

00:09:53.690 --> 00:09:55.880
theory and things that
scale, and order n squared,

00:09:55.880 --> 00:09:58.160
and things that scale
in polynomial time,

00:09:58.160 --> 00:10:01.760
and so on, those are things
that are just semi-immutable.

00:10:01.760 --> 00:10:04.160
And so talking about the
kind of problems we can solve

00:10:04.160 --> 00:10:05.990
and the kind of
problems we can't solve,

00:10:05.990 --> 00:10:08.090
if you make the
plot turn on them,

00:10:08.090 --> 00:10:10.340
the plot doesn't get
old in the same way

00:10:10.340 --> 00:10:14.360
that it does if the plot turns
on how much RAM your phone has.

00:10:14.360 --> 00:10:15.930
That's a moving target.

00:10:15.930 --> 00:10:19.003
But like Turing completeness,
not a moving target.

00:10:19.003 --> 00:10:20.420
Writing about the
fact that if you

00:10:20.420 --> 00:10:21.795
try to design a
computer that can

00:10:21.795 --> 00:10:23.900
run all the programs
except one, you

00:10:23.900 --> 00:10:26.870
are trying to do something
that runs counter to the--

00:10:26.870 --> 00:10:30.260
really the only functional
widespread architecture we have

00:10:30.260 --> 00:10:33.380
for computers, which is the
one that runs all the programs.

00:10:33.380 --> 00:10:39.800
And so it will always be this
weird, mashed-up Rube Goldberg

00:10:39.800 --> 00:10:43.190
under the hood if you've
got a computer that

00:10:43.190 --> 00:10:44.420
runs everything except one.

00:10:44.420 --> 00:10:46.378
And it's always going to
have certain recurring

00:10:46.378 --> 00:10:47.120
characteristics.

00:10:47.120 --> 00:10:49.280
If you design a
computer that has

00:10:49.280 --> 00:10:53.210
a mode that sits
below super-user mode,

00:10:53.210 --> 00:10:54.985
like ring minus
one, where programs

00:10:54.985 --> 00:10:57.110
execute that even the
administrator of the computer

00:10:57.110 --> 00:10:59.510
isn't supposed to be able
to inspect or terminate,

00:10:59.510 --> 00:11:01.610
then that will always be
his own that if someone

00:11:01.610 --> 00:11:03.560
who is an attacker
gets access to it,

00:11:03.560 --> 00:11:06.320
that they'll be able to
operate with total impunity

00:11:06.320 --> 00:11:08.870
and undetectably against
you, because it's

00:11:08.870 --> 00:11:13.130
a computer designed not to
introspect about processes

00:11:13.130 --> 00:11:14.790
that are running in some ways.

00:11:14.790 --> 00:11:16.670
And so this just
keeps coming up.

00:11:16.670 --> 00:11:18.320
We keep designing
computers that have,

00:11:18.320 --> 00:11:20.460
as part of their
security model, oh, there

00:11:20.460 --> 00:11:21.758
are programs that the user--

00:11:21.758 --> 00:11:23.300
even if the user is
the administrator

00:11:23.300 --> 00:11:24.410
and owns the device--

00:11:24.410 --> 00:11:26.060
can't terminate or inspect.

00:11:26.060 --> 00:11:28.220
And inevitably, someone
who's a bad actor

00:11:28.220 --> 00:11:29.660
starts running code in it.

00:11:29.660 --> 00:11:32.210
Maybe the politbureau
orders Apple

00:11:32.210 --> 00:11:36.020
to run processes that seek
out and terminate VPNs

00:11:36.020 --> 00:11:39.200
that it can't spy
on for iOS devices

00:11:39.200 --> 00:11:42.500
in China, which is a thing
that happened, right?

00:11:42.500 --> 00:11:46.430
You know, this is the world's
most predictable outcome

00:11:46.430 --> 00:11:49.258
of putting that gun on
the mantelpiece in Act 1.

00:11:49.258 --> 00:11:51.050
But we're going to
continue to put that gun

00:11:51.050 --> 00:11:52.280
on the mantelpiece in Act 1.

00:11:52.280 --> 00:11:53.947
And so if you write
science fiction that

00:11:53.947 --> 00:11:56.600
turns on the actual
characteristics of computers,

00:11:56.600 --> 00:11:59.000
like, our theoretical
understanding computers,

00:11:59.000 --> 00:12:02.480
that science fiction remains
futuristic for so long

00:12:02.480 --> 00:12:05.270
as we continue to make dumb
policy choices about computers.

00:12:05.270 --> 00:12:10.280
Which to my great
dismay, probably

00:12:10.280 --> 00:12:12.620
means that the fiction will
remain current in some way

00:12:12.620 --> 00:12:14.060
forever.

00:12:14.060 --> 00:12:16.830
"Little Brother" is still
being taught and read--

00:12:16.830 --> 00:12:19.340
I wrote it 13 years ago now.

00:12:19.340 --> 00:12:21.320
And the question
of whether or not

00:12:21.320 --> 00:12:23.240
subjecting whole
populations to surveillance,

00:12:23.240 --> 00:12:26.030
or using machine learning to
make inferences about guilt,

00:12:26.030 --> 00:12:30.500
or any of these other things
creates a bunch of pathologies,

00:12:30.500 --> 00:12:34.010
those facts are still
totally in evidence.

00:12:34.010 --> 00:12:37.080
They show no sign of going away.

00:12:37.080 --> 00:12:39.740
We have totally failed to
learn the lessons of them.

00:12:39.740 --> 00:12:41.150
The stakes only get higher.

00:12:41.150 --> 00:12:43.790
And every year, there's
another news hook

00:12:43.790 --> 00:12:46.130
that makes people
read "Little Brother"

00:12:46.130 --> 00:12:48.170
and then come back to
me and say, oh, this

00:12:48.170 --> 00:12:50.360
is just like that thing.

00:12:50.360 --> 00:12:52.047
How did you anticipate
13 years later

00:12:52.047 --> 00:12:53.130
that we'd been doing this?

00:12:53.130 --> 00:12:54.170
And I'm like, I didn't.

00:12:54.170 --> 00:12:55.700
We were doing it 13 years ago.

00:12:55.700 --> 00:12:59.480
We just are literally
still beating our heads

00:12:59.480 --> 00:13:00.268
against that wall.

00:13:00.268 --> 00:13:02.060
We are figuratively
still beating our heads

00:13:02.060 --> 00:13:02.810
against that wall.

00:13:02.810 --> 00:13:04.310
We will never stop.

00:13:04.310 --> 00:13:06.920
And so "Little Brother" will
remain current and relevant

00:13:06.920 --> 00:13:10.077
for so long as we are stupid
about technology policy.

00:13:10.077 --> 00:13:11.660
SPEAKER: You mentioned
the word, root.

00:13:11.660 --> 00:13:14.300
What was the initial
idea for the story?

00:13:14.300 --> 00:13:16.370
Did it start with the
phrase, unauthorized bread,

00:13:16.370 --> 00:13:17.960
or did it come from
somewhere else?

00:13:17.960 --> 00:13:19.627
CORY DOCTOROW: Yeah,
it actually started

00:13:19.627 --> 00:13:23.270
with a short story I wrote as
part of my Guardian column.

00:13:23.270 --> 00:13:24.740
So I'd been arguing
for a long time

00:13:24.740 --> 00:13:28.770
with people about iOS and
the App Store business model.

00:13:28.770 --> 00:13:33.680
And there was this "it just
works and I trust them"

00:13:33.680 --> 00:13:38.640
element to people saying, I want
to work within an ecosystem,

00:13:38.640 --> 00:13:43.722
and I trust Apple, and I don't
care if they have made choices

00:13:43.722 --> 00:13:45.680
about what I can and
can't use, because I think

00:13:45.680 --> 00:13:46.847
those choices are good ones.

00:13:46.847 --> 00:13:49.220
They're good proxies
for my interest.

00:13:49.220 --> 00:13:53.590
And I was trying to tease out
the difference between having

00:13:53.590 --> 00:13:57.880
a checkbox in your OS that says,
I would like to do something

00:13:57.880 --> 00:14:00.190
that the manufacturer
hasn't approved,

00:14:00.190 --> 00:14:04.690
and having legal liability
attached to figuring out

00:14:04.690 --> 00:14:06.860
how to reconfigure
your OS to do something

00:14:06.860 --> 00:14:08.440
that hasn't been approved.

00:14:08.440 --> 00:14:12.130
And what kind of
bad things crop up

00:14:12.130 --> 00:14:15.010
if it's actually a
felony to modify your OS

00:14:15.010 --> 00:14:17.080
to do things that the
manufacturer doesn't like.

00:14:17.080 --> 00:14:18.580
And so I wrote a
little story called

00:14:18.580 --> 00:14:20.590
"If Dishwashers Were iPhones."

00:14:20.590 --> 00:14:22.750
And it was in the
form of an open letter

00:14:22.750 --> 00:14:29.530
from a Steve Jobsian CEO of a
next-generation IoT dishwasher

00:14:29.530 --> 00:14:32.830
company called Disher, who make
an appearance in the story.

00:14:32.830 --> 00:14:36.310
And he was explaining
that food-borne illness

00:14:36.310 --> 00:14:40.150
has killed more people than any
other killer in human history.

00:14:40.150 --> 00:14:41.950
And how can you
expect your dishwasher

00:14:41.950 --> 00:14:45.610
to be truly effective
at keeping you safe

00:14:45.610 --> 00:14:48.510
and making your dishes clean
with the least amount of water

00:14:48.510 --> 00:14:51.130
in an age of scarce
resources if you

00:14:51.130 --> 00:14:53.140
are able to put any
old dish that you

00:14:53.140 --> 00:14:55.180
want in your dishwasher?

00:14:55.180 --> 00:14:58.030
And that is why you
shouldn't be bending

00:14:58.030 --> 00:15:01.060
the prongs of your dishwasher
or trying to take the RFIDs out

00:15:01.060 --> 00:15:03.700
of the dishes that you
bought from the Disher store

00:15:03.700 --> 00:15:06.280
and putting them in
grandma's China, and so on.

00:15:06.280 --> 00:15:08.500
Because you could buy
a different dishwasher

00:15:08.500 --> 00:15:10.300
if you wanted that.

00:15:10.300 --> 00:15:15.160
And the deal that you got when
you bought this dishwasher

00:15:15.160 --> 00:15:17.080
was that you would
only wash dishwashers

00:15:17.080 --> 00:15:18.490
from The Kitchen Store.

00:15:18.490 --> 00:15:21.040
And The Kitchen Store works
only with licensed partners

00:15:21.040 --> 00:15:22.300
who make high-quality gear.

00:15:22.300 --> 00:15:26.620
And anyone who wants can buy
a $99 Kitchen Store license.

00:15:26.620 --> 00:15:29.980
And provided they comply
with the terms of service,

00:15:29.980 --> 00:15:32.763
they can make pottery that you
can put in your dishwasher.

00:15:32.763 --> 00:15:34.180
But it's not
dishwasher-- the fact

00:15:34.180 --> 00:15:37.862
that it says dishwasher safe
doesn't mean it's Disher safe.

00:15:37.862 --> 00:15:39.820
It's only when they're
in our developer program

00:15:39.820 --> 00:15:41.470
that we can tell
you that you truly

00:15:41.470 --> 00:15:45.700
won't die of listeria
if you use their dishes.

00:15:45.700 --> 00:15:48.610
And these are all,
taken individually,

00:15:48.610 --> 00:15:50.800
not unreasonable statements.

00:15:50.800 --> 00:15:54.640
But they gang up to inkjet
printers for everything.

00:15:54.640 --> 00:15:59.230
Everything is tied into some
ecosystem where you can't--

00:15:59.230 --> 00:16:03.520
it's not that you do trust them,
it's that you must trust them.

00:16:03.520 --> 00:16:06.670
And if they ever make a
decision that you don't like,

00:16:06.670 --> 00:16:07.840
you are out of luck.

00:16:07.840 --> 00:16:11.950
And the longer you go inside the
ecosystem, the more sunk cost

00:16:11.950 --> 00:16:16.510
you have, and the harder it is,
the more you have to give away.

00:16:16.510 --> 00:16:19.420
The more the switching
cost is if you decide

00:16:19.420 --> 00:16:20.930
that you no longer trust them.

00:16:20.930 --> 00:16:23.405
And so you end up being
beholden to a series

00:16:23.405 --> 00:16:25.780
of commercial decisions being
made in boardrooms that you

00:16:25.780 --> 00:16:27.027
have no insight into.

00:16:27.027 --> 00:16:29.110
And you just have to trust
that no one in the firm

00:16:29.110 --> 00:16:31.580
will ever do anything that
runs counter to your interests.

00:16:31.580 --> 00:16:35.050
And I'm not an
anti-Apple person.

00:16:35.050 --> 00:16:37.300
I actually have a sad
Mac tattooed on my arm

00:16:37.300 --> 00:16:39.520
from when I used
to be a CIO and I

00:16:39.520 --> 00:16:42.850
used to order a million dollars
worth of Apple gear a year.

00:16:42.850 --> 00:16:46.390
But as someone who's bought
a fair number of Apple lemons

00:16:46.390 --> 00:16:48.980
and written POs for a fair
number of Apple lemons

00:16:48.980 --> 00:16:52.000
over the years, and seen how
the company can be wildly

00:16:52.000 --> 00:16:54.730
imperfect, and will be
wildly imperfect again,

00:16:54.730 --> 00:16:56.650
I think the idea that
you are trusting them

00:16:56.650 --> 00:16:59.350
in this way that requires
that you trust them

00:16:59.350 --> 00:17:01.660
and that you can't transfer
your trust out actually

00:17:01.660 --> 00:17:05.440
invites future
leaders of the firm

00:17:05.440 --> 00:17:12.010
to make choices in the
knowledge that they

00:17:12.010 --> 00:17:14.440
can betray your
trust, and you're not

00:17:14.440 --> 00:17:16.540
going to be able to leave,

00:17:16.540 --> 00:17:18.670
that they've got
a lot of headroom

00:17:18.670 --> 00:17:21.339
in terms of betrayed trust
before you get to the point

00:17:21.339 --> 00:17:24.369
where people are going to give
up a whole ecosystem of devices

00:17:24.369 --> 00:17:25.839
and replace all of it.

00:17:25.839 --> 00:17:28.210
And certainly in some
customers, maybe never.

00:17:28.210 --> 00:17:30.712
And I saw this not being
looked on with horror

00:17:30.712 --> 00:17:32.920
by the rest of the world,
but being looked on as kind

00:17:32.920 --> 00:17:35.140
of an excellent idea.

00:17:35.140 --> 00:17:37.690
So you have, for example,
Johnson &amp; Johnson

00:17:37.690 --> 00:17:40.100
getting approval for
an artificial pancreas.

00:17:40.100 --> 00:17:43.480
This is a closed loop, a
continuous glucose monitor,

00:17:43.480 --> 00:17:47.080
and an insulin pump with
some machine learning

00:17:47.080 --> 00:17:50.650
to try and time the insulin
dosing to keep your blood

00:17:50.650 --> 00:17:52.900
sugar within a safe range.

00:17:52.900 --> 00:17:55.840
And it uses proprietary
insulin cartridges.

00:17:55.840 --> 00:18:00.130
And making a tool to
refill those cartridges

00:18:00.130 --> 00:18:04.120
is a potential DMCA violation
with a $500,000 fine

00:18:04.120 --> 00:18:06.190
and a five-year prison sentence.

00:18:06.190 --> 00:18:09.850
And revealing a defect
in it, if that defect

00:18:09.850 --> 00:18:12.170
would help someone
bypass the DRM

00:18:12.170 --> 00:18:16.160
is also a potential
felony under the DMCA.

00:18:16.160 --> 00:18:18.880
And one of the things we know
is that security researchers

00:18:18.880 --> 00:18:22.630
are chilled from coming
forward with reports of defects

00:18:22.630 --> 00:18:26.770
when there's a potential DMCA
overlap or a Computer Fraud

00:18:26.770 --> 00:18:28.220
and Abuse Act overlap.

00:18:28.220 --> 00:18:31.780
And so now you have this
spreading attack surface

00:18:31.780 --> 00:18:33.640
of devices that,
because they're designed

00:18:33.640 --> 00:18:36.100
to be extractive of
their users, are also

00:18:36.100 --> 00:18:38.470
off limits to
independent scrutiny.

00:18:38.470 --> 00:18:42.970
And they are they're more
and more intimately connected

00:18:42.970 --> 00:18:44.300
to our bodies.

00:18:44.300 --> 00:18:46.120
And so your car is a robot.

00:18:46.120 --> 00:18:48.550
You put your body into--
that then goes down

00:18:48.550 --> 00:18:49.915
the road at 60 miles an hour.

00:18:49.915 --> 00:18:52.540
Or if it's like the 5, that goes
down it at five miles an hour.

00:18:55.165 --> 00:18:57.040
And I'm not talking
about a self-driving car.

00:18:57.040 --> 00:18:59.010
I'm just talking about a car.

00:18:59.010 --> 00:19:01.480
Because you take all the
informatics out of a car and it

00:19:01.480 --> 00:19:02.860
becomes inert.

00:19:02.860 --> 00:19:06.040
The most salient feature of
that car is its informatics.

00:19:06.040 --> 00:19:10.660
Compromising those informatics
allows the compromiser

00:19:10.660 --> 00:19:14.440
to wreak great havoc on the
person whose body is trapped

00:19:14.440 --> 00:19:19.010
inside this fast-moving robot
and on the people around it.

00:19:19.010 --> 00:19:24.550
And so I wanted to illustrate
this idea that as our property

00:19:24.550 --> 00:19:27.310
interest in the things that
we own is being eroded,

00:19:27.310 --> 00:19:30.070
and as our ability to
independently scrutinize them

00:19:30.070 --> 00:19:32.740
are being eroded,
that we are also

00:19:32.740 --> 00:19:37.390
magnifying all of the imbalances
and inequalities in our world,

00:19:37.390 --> 00:19:39.850
and that it happens
first to the poorest

00:19:39.850 --> 00:19:43.130
and least powerful among us,
but it spreads to everybody.

00:19:43.130 --> 00:19:46.420
The user adoption curve
for controlling technology

00:19:46.420 --> 00:19:52.150
is like refugees, prisoners,
children, poor people,

00:19:52.150 --> 00:19:54.880
blue collar workers,
white collar workers.

00:19:54.880 --> 00:19:56.980
That's the adoption curve.

00:19:56.980 --> 00:19:59.200
And if you want to know
what your life is going

00:19:59.200 --> 00:20:01.840
to look like in 20
years, just look

00:20:01.840 --> 00:20:04.450
at what we're doing
to refugees today,

00:20:04.450 --> 00:20:08.950
and that's the technology that
people will expect you to use.

00:20:08.950 --> 00:20:11.470
People didn't have
CCTVs recording

00:20:11.470 --> 00:20:15.860
them non-consensually,
operated by third parties,

00:20:15.860 --> 00:20:20.870
unless they were
imprisoned in our lifetime.

00:20:20.870 --> 00:20:22.940
Now it's ubiquitous.

00:20:22.940 --> 00:20:25.880
Illustrating that and using
this specific group of people

00:20:25.880 --> 00:20:31.430
to illustrate it was an
intervention in that, a way

00:20:31.430 --> 00:20:32.875
to try and interrupt that.

00:20:32.875 --> 00:20:34.250
SPEAKER: Well,
let's dig a little

00:20:34.250 --> 00:20:35.960
deeper into that
idea of inequality.

00:20:35.960 --> 00:20:38.480
It's a major part of this story.

00:20:38.480 --> 00:20:42.560
The main character, Salima,
is an immigrant, a refugee.

00:20:42.560 --> 00:20:48.230
The people that around her are
largely poor and underclassed.

00:20:48.230 --> 00:20:51.213
What do you think
that brings the work?

00:20:51.213 --> 00:20:53.630
CORY DOCTOROW: One of the most
salient facts of our moment

00:20:53.630 --> 00:20:57.260
is inequality, in part because
it breaks apart the story

00:20:57.260 --> 00:21:02.645
that we've told about markets,
which is that markets are

00:21:02.645 --> 00:21:06.440
a dynamic way of finding people
whose ideas would create more

00:21:06.440 --> 00:21:09.470
general prosperity, and
allocating capital to them

00:21:09.470 --> 00:21:11.120
so that they can do it.

00:21:11.120 --> 00:21:14.690
And when you see widening
inequality and stagnation

00:21:14.690 --> 00:21:20.640
in our social relations,
you're left with either one

00:21:20.640 --> 00:21:21.750
of two conclusions, right?

00:21:21.750 --> 00:21:24.840
Either markets aren't working
the way they're supposed to,

00:21:24.840 --> 00:21:28.730
or eugenics is
real, and there is

00:21:28.730 --> 00:21:31.190
a 1% of people who are so
smart that they should just

00:21:31.190 --> 00:21:32.195
own everything.

00:21:32.195 --> 00:21:34.070
And if we would just
give them all the stuff,

00:21:34.070 --> 00:21:36.500
they will allocate
capital so wisely

00:21:36.500 --> 00:21:38.438
that we will get richer
and richer and richer.

00:21:38.438 --> 00:21:39.980
And you know the
history of antitrust

00:21:39.980 --> 00:21:41.840
is full of counter
examples, right?

00:21:41.840 --> 00:21:46.460
Like when we broke up the phone
company into six companies,

00:21:46.460 --> 00:21:50.660
they got bigger in aggregate
than they were as one firm.

00:21:50.660 --> 00:21:53.810
When they broke up the
railroad, the railroad monopoly

00:21:53.810 --> 00:21:56.720
into two companies, each
one within a few years

00:21:56.720 --> 00:21:59.060
was as big as the
parent company had been.

00:21:59.060 --> 00:22:02.210
The disefficiencies
of scale are actually

00:22:02.210 --> 00:22:06.050
pretty well understood,
but as Upton Sinclair said,

00:22:06.050 --> 00:22:08.300
it's impossible to get someone
to understand something

00:22:08.300 --> 00:22:10.910
when their paycheck depends
on them not understanding it.

00:22:10.910 --> 00:22:14.000
And if you are
someone who benefits

00:22:14.000 --> 00:22:17.450
from the concentration of
wealth in a single firm,

00:22:17.450 --> 00:22:20.000
the fact that we as a
society will benefit

00:22:20.000 --> 00:22:23.120
more if that firm were
broken up into smaller pieces

00:22:23.120 --> 00:22:26.480
and each of them was allowed
to compete in and grow

00:22:26.480 --> 00:22:28.610
is not very relevant
to you, because it

00:22:28.610 --> 00:22:31.850
means that you get a lot fewer
ivory handle back scratchers.

00:22:31.850 --> 00:22:37.280
And so I think that one of the
corrosive effects of inequality

00:22:37.280 --> 00:22:40.550
is that it drives rich
people into believing

00:22:40.550 --> 00:22:44.845
that they have good blood,
into believing in eugenics.

00:22:44.845 --> 00:22:46.970
You actually heard this in
the last election cycle.

00:22:46.970 --> 00:22:49.400
Trump repeatedly talked
about his good blood.

00:22:52.540 --> 00:22:57.260
For people who remember the
horrors of the Holocaust

00:22:57.260 --> 00:23:02.780
or Tuskegee or the
eugenics movement,

00:23:02.780 --> 00:23:06.860
good blood is a scary thing
to hear someone talk about.

00:23:10.790 --> 00:23:14.930
It's like the smoke from
a horrible fire that

00:23:14.930 --> 00:23:17.600
is smoldering, and
I think that know

00:23:17.600 --> 00:23:19.760
we're blowing on those coals.

00:23:19.760 --> 00:23:24.200
And so by writing
about people who

00:23:24.200 --> 00:23:26.780
would be really efficient
capital allocators, who

00:23:26.780 --> 00:23:29.720
are doing really interesting
dynamic, exciting things that

00:23:29.720 --> 00:23:33.620
benefit other people, but who
are denied access to capital,

00:23:33.620 --> 00:23:35.180
one of the things
that you do is you

00:23:35.180 --> 00:23:39.080
start to change the
narrative we have about

00:23:39.080 --> 00:23:41.450
whether capital
allocation is efficient,

00:23:41.450 --> 00:23:45.980
and whether inequality
means that we are actually

00:23:45.980 --> 00:23:49.100
just taking random
lottery winners

00:23:49.100 --> 00:23:51.590
and heaping enormous
riches on them

00:23:51.590 --> 00:23:54.110
to the detriment of their
soul and to the detriment

00:23:54.110 --> 00:23:55.640
of our society.

00:23:55.640 --> 00:23:57.410
SPEAKER: Your Twitter
handle currently

00:23:57.410 --> 00:24:00.417
is "son of an asylum seeker,
father of an immigrant."

00:24:00.417 --> 00:24:01.250
CORY DOCTOROW: Yeah.

00:24:01.250 --> 00:24:04.070
SPEAKER: Do you feel a personal
connection to that experience?

00:24:04.070 --> 00:24:04.903
CORY DOCTOROW: I do.

00:24:04.903 --> 00:24:07.600
You know, I lived in England
until three years ago.

00:24:07.600 --> 00:24:10.730
So I was in the UK in
the run up to Brexit.

00:24:10.730 --> 00:24:13.490
And I would end up
in a lot of cabs,

00:24:13.490 --> 00:24:17.480
and London cab drivers
are notoriously right-wing

00:24:17.480 --> 00:24:19.400
and notoriously gabby.

00:24:19.400 --> 00:24:25.020
And given that the impending
crisis was about migration

00:24:25.020 --> 00:24:28.500
and this debate was going
on about migration, at least

00:24:28.500 --> 00:24:30.070
on a weekly basis
someone would start

00:24:30.070 --> 00:24:32.070
talking to me about asylum
seekers and migrants.

00:24:32.070 --> 00:24:33.930
My dad was born when
his parents were

00:24:33.930 --> 00:24:36.270
living in a displaced
persons camp in Azerbaijan.

00:24:36.270 --> 00:24:38.245
They were Red Army deserters.

00:24:38.245 --> 00:24:39.870
My grandmother had
been a child soldier

00:24:39.870 --> 00:24:41.370
in the siege of Leningrad.

00:24:41.370 --> 00:24:44.190
They came to Canada
as asylum seekers,

00:24:44.190 --> 00:24:45.330
and then I'm an immigrant.

00:24:45.330 --> 00:24:47.160
I'm a Canadian who then
lived in the United Kingdom,

00:24:47.160 --> 00:24:47.993
and now I live here.

00:24:47.993 --> 00:24:49.800
And my daughter is an immigrant.

00:24:49.800 --> 00:24:53.270
And I found that
it gave me a place,

00:24:53.270 --> 00:24:57.380
a way to enter that
conversation and say

00:24:57.380 --> 00:24:59.580
I'm the person
you're talking about.

00:24:59.580 --> 00:25:01.680
That's me, one generation.

00:25:01.680 --> 00:25:04.290
And they'd say, oh, but
you're the right kind.

00:25:04.290 --> 00:25:05.280
You're high skilled.

00:25:05.280 --> 00:25:06.180
You're whatever.

00:25:06.180 --> 00:25:07.560
You're white.

00:25:07.560 --> 00:25:09.480
And I would say,
so my grandparents

00:25:09.480 --> 00:25:10.650
were not high skilled.

00:25:10.650 --> 00:25:12.960
My grandmother stopped her
formal schooling at 12.

00:25:12.960 --> 00:25:15.513
My grandfather
stopped his at 14.

00:25:15.513 --> 00:25:17.430
They came to Canada
without marketable skills.

00:25:17.430 --> 00:25:21.870
They learned skills
when they got there.

00:25:21.870 --> 00:25:24.630
And they had to display
an awful lot of pluck

00:25:24.630 --> 00:25:29.700
and get up and go to get
from Azerbaijan to Canada.

00:25:29.700 --> 00:25:31.770
The winnowing function
they went through

00:25:31.770 --> 00:25:34.320
was not passing a
standardized test

00:25:34.320 --> 00:25:36.690
or displaying a credential.

00:25:36.690 --> 00:25:40.356
It was crossing Europe.

00:25:40.356 --> 00:25:43.140
That is an awful
lot of gumption.

00:25:43.140 --> 00:25:44.940
By the time you get
to the port of Hamburg

00:25:44.940 --> 00:25:49.380
and you present yourself to a
Canadian immigration official,

00:25:49.380 --> 00:25:53.670
you have already demonstrated
an enormous amount of pluck.

00:25:53.670 --> 00:25:55.380
And they were
traumatized, and they

00:25:55.380 --> 00:25:56.880
had lots of problems
in their lives,

00:25:56.880 --> 00:26:01.740
but they were also people
who came to contribute

00:26:01.740 --> 00:26:04.750
and whose families contributed
through the generations.

00:26:04.750 --> 00:26:08.910
And so as the debate about
migration and asylum seekers

00:26:08.910 --> 00:26:14.910
and immigrants took off here, I
felt like my passing privilege.

00:26:14.910 --> 00:26:18.850
I have an accent that sounds
like it could be from America.

00:26:18.850 --> 00:26:21.510
I have a skin tone that makes
me look like I could be white,

00:26:21.510 --> 00:26:23.622
even though the
people I come from

00:26:23.622 --> 00:26:25.830
were not thought of as white
when they got to Canada.

00:26:25.830 --> 00:26:27.780
They certainly are white now.

00:26:31.340 --> 00:26:34.020
And so you could think
that you were talking

00:26:34.020 --> 00:26:36.150
to someone who wasn't
an asylum seeker, who

00:26:36.150 --> 00:26:39.600
wasn't the father of a migrant.

00:26:39.600 --> 00:26:41.970
As they say, Canadians
are like serial killers.

00:26:41.970 --> 00:26:45.210
They're everywhere, and they
look just like everybody else.

00:26:45.210 --> 00:26:49.410
I wanted to wear it on my
sleeve to make people confront

00:26:49.410 --> 00:26:53.310
that they were among people
who were the kind of people

00:26:53.310 --> 00:26:54.307
they were demonizing.

00:26:54.307 --> 00:26:55.890
SPEAKER: In the
beginning of the talk,

00:26:55.890 --> 00:26:59.107
you mentioned how the stories in
"Radicalized" aren't connected.

00:26:59.107 --> 00:26:59.940
CORY DOCTOROW: Yeah.

00:26:59.940 --> 00:27:02.520
SPEAKER: But you said
that the role of place

00:27:02.520 --> 00:27:04.680
mattered in the stories.

00:27:04.680 --> 00:27:07.630
This story takes place
in suburban Boston.

00:27:07.630 --> 00:27:08.130
Why?

00:27:08.130 --> 00:27:09.120
Why Boston?

00:27:09.120 --> 00:27:10.470
CORY DOCTOROW: Although
it touches Phoenix, right?

00:27:10.470 --> 00:27:11.850
Phoenix is in all the stories.

00:27:11.850 --> 00:27:15.320
So Boston in part because
I knew it reasonably well.

00:27:15.320 --> 00:27:19.320
I'm an MIT Media Lab
research affiliate.

00:27:19.320 --> 00:27:20.850
You and I hung out in Boston.

00:27:20.850 --> 00:27:23.790
I stayed in your living
room in Boston once.

00:27:23.790 --> 00:27:25.830
And Boston's a really
interesting town

00:27:25.830 --> 00:27:28.380
in that it's like
it's a crossroads

00:27:28.380 --> 00:27:32.320
for a bunch of different kinds
of industry and activity.

00:27:32.320 --> 00:27:33.773
So it's obviously
an academic hub.

00:27:33.773 --> 00:27:34.440
It's a tech hub.

00:27:34.440 --> 00:27:36.060
It's now a biotech hub.

00:27:36.060 --> 00:27:37.830
But it's also a
light industry town.

00:27:37.830 --> 00:27:40.245
It's a port.

00:27:40.245 --> 00:27:42.120
It has all of these
different contradictions.

00:27:42.120 --> 00:27:45.480
And it's similar to LA.

00:27:45.480 --> 00:27:48.180
It's not like the Bay Area,
which although the Bay Area has

00:27:48.180 --> 00:27:50.660
a bunch of industries
in its history,

00:27:50.660 --> 00:27:52.380
it has been completely
eclipsed now.

00:27:52.380 --> 00:27:55.200
There's just one thing
that it's known for now.

00:27:55.200 --> 00:27:58.250
Whereas Boston still is
this very diverse place.

00:27:58.250 --> 00:28:00.180
And Arizona I'm
really interested in,

00:28:00.180 --> 00:28:02.070
because on the one
hand, it's a place that

00:28:02.070 --> 00:28:05.430
is not going to survive
climate change gracefully.

00:28:05.430 --> 00:28:08.460
It's like it's really in the
crosshairs of climate change.

00:28:08.460 --> 00:28:10.200
But it's a red
state, so it's also

00:28:10.200 --> 00:28:13.080
a state where climate
is officially denied.

00:28:13.080 --> 00:28:16.410
But it's also a state
that is majority minority

00:28:16.410 --> 00:28:19.560
and but for a little bit
of gerrymandering here

00:28:19.560 --> 00:28:22.020
and a little bit of
voter suppression there,

00:28:22.020 --> 00:28:25.680
it would be a place that would
have a very different politics.

00:28:25.680 --> 00:28:27.600
And it's also a retirement hub.

00:28:27.600 --> 00:28:30.210
So it's a place that's full
of people who aren't Arizonans

00:28:30.210 --> 00:28:33.280
making claims about
their native rights,

00:28:33.280 --> 00:28:35.220
their indigenous
rights as Arizonans,

00:28:35.220 --> 00:28:36.750
even though they're
all transplants.

00:28:36.750 --> 00:28:41.760
And so the contradictions
of Arizona are so vivid.

00:28:41.760 --> 00:28:44.850
And I'm an advisor
to a center at ASU

00:28:44.850 --> 00:28:48.220
that does science fiction to
talk about other disciplines.

00:28:48.220 --> 00:28:51.470
And so I go to
Phoenix periodically,

00:28:51.470 --> 00:28:54.030
and also it's my hub because
I fly to Burbank Airport.

00:28:54.030 --> 00:28:54.810
I live in Burbank.

00:28:54.810 --> 00:28:59.280
And so everything starts
Phoenix and then somewhere else.

00:28:59.280 --> 00:29:01.920
So I really was
interested in the role

00:29:01.920 --> 00:29:04.780
that Arizona plays in the
future of our politics.

00:29:04.780 --> 00:29:07.100
SPEAKER: And it's
interesting leveling effect

00:29:07.100 --> 00:29:10.020
if you talk about of
redefining who we presume to be

00:29:10.020 --> 00:29:12.660
a refugee or an immigrant.

00:29:12.660 --> 00:29:14.160
Both of those places
are also places

00:29:14.160 --> 00:29:16.830
that, despite their diversity,
we don't necessarily think

00:29:16.830 --> 00:29:19.657
of as an immigration hot zone.

00:29:19.657 --> 00:29:20.490
CORY DOCTOROW: Yeah.

00:29:20.490 --> 00:29:22.532
SPEAKER: I think we'll
see more places like that.

00:29:22.532 --> 00:29:25.110
I mean Greenpoint, Brooklyn
is still a predominant polish

00:29:25.110 --> 00:29:27.390
entry point in the country.

00:29:27.390 --> 00:29:30.510
Growing among
population in Wisconsin,

00:29:30.510 --> 00:29:34.410
what do you think that
means to the sense of place

00:29:34.410 --> 00:29:37.487
and the diversity of
those communities.

00:29:37.487 --> 00:29:39.820
CORY DOCTOROW: So this is a
really interesting question.

00:29:39.820 --> 00:29:43.080
We have a story about
America about something

00:29:43.080 --> 00:29:45.330
between assimilation
and ladder kicking.

00:29:45.330 --> 00:29:46.980
Once there were
Poles who were not

00:29:46.980 --> 00:29:50.160
thought of as white, or
American, or Italians.

00:29:50.160 --> 00:29:52.758
And then they assimilated, and
then they became respectable,

00:29:52.758 --> 00:29:54.300
and then they kicked
the ladder away,

00:29:54.300 --> 00:29:56.070
and they turned on the
people who came after them.

00:29:56.070 --> 00:29:57.970
And there's a certain
amount of truth to that.

00:29:57.970 --> 00:29:59.428
Some of that,
though, is associated

00:29:59.428 --> 00:30:00.850
with economic dynamism.

00:30:00.850 --> 00:30:04.920
So one of the things
that made people

00:30:04.920 --> 00:30:08.550
go from being other to being
accepted as fully paid up

00:30:08.550 --> 00:30:13.050
Americans was an extraordinary
degree of social mobility

00:30:13.050 --> 00:30:15.390
at various times in
America's history.

00:30:15.390 --> 00:30:18.150
And Thomas Piketty in his book
"Capital in the 21st Century"

00:30:18.150 --> 00:30:21.210
talks about America's
dynamism, and he chalks it up

00:30:21.210 --> 00:30:24.810
to these reset events
that America had in terms

00:30:24.810 --> 00:30:26.380
of its wealth distribution.

00:30:26.380 --> 00:30:30.120
So in before manumission,
the vast majority

00:30:30.120 --> 00:30:32.310
of American wealth
was in people who

00:30:32.310 --> 00:30:36.890
were claimed as slaves by
people who had enslaved them.

00:30:36.890 --> 00:30:39.420
The gross national
wealth of America

00:30:39.420 --> 00:30:41.880
was primarily in human bodies.

00:30:41.880 --> 00:30:45.180
And so manumission, in addition
to doing a lot of other things

00:30:45.180 --> 00:30:49.080
politically, had what was this
huge economic moment in that

00:30:49.080 --> 00:30:51.840
the greatest concentrations
of wealth in America

00:30:51.840 --> 00:30:53.340
ceased to be
considered as assets

00:30:53.340 --> 00:30:56.820
and became human beings, at
least as a legal fiction,

00:30:56.820 --> 00:30:59.230
notwithstanding Jim
Crow and whatever.

00:30:59.230 --> 00:31:00.990
But one of the
effects of that is

00:31:00.990 --> 00:31:03.630
that the grip that
wealthy people had

00:31:03.630 --> 00:31:08.610
on political outcomes completely
changed after the Civil War,

00:31:08.610 --> 00:31:11.520
because they just didn't
have as much money to spend.

00:31:11.520 --> 00:31:14.460
Their wealth was
radically diminished.

00:31:14.460 --> 00:31:16.540
And then it happened
again during the crash.

00:31:16.540 --> 00:31:22.980
And so the war years and also
the crash after the Gilded Age

00:31:22.980 --> 00:31:24.960
completely leveled out
the wealth distribution.

00:31:24.960 --> 00:31:28.290
So the amount of wealth control
by the top decile of Americans

00:31:28.290 --> 00:31:32.580
just nose dives twice in
American industrial history,

00:31:32.580 --> 00:31:34.360
and it only happens
once in Europe.

00:31:34.360 --> 00:31:37.590
It only happens
with the war years,

00:31:37.590 --> 00:31:39.600
with the two wars and
the interwar years.

00:31:39.600 --> 00:31:44.940
And when the capital is
more widely distributed,

00:31:44.940 --> 00:31:46.470
you have more
pluralistic choices,

00:31:46.470 --> 00:31:49.620
more pluralistic decisions,
which creates things

00:31:49.620 --> 00:31:54.090
like better social safety net,
better quality of schools,

00:31:54.090 --> 00:31:56.770
cheaper and wider access
to education, and so on.

00:31:56.770 --> 00:31:58.850
And this lifts up
lots of people.

00:31:58.850 --> 00:32:04.550
It helps people enter a kind
of middle class respectability.

00:32:04.550 --> 00:32:06.840
And as we know from
the crisis of 2008,

00:32:06.840 --> 00:32:09.750
the first people
who get jettisoned

00:32:09.750 --> 00:32:13.200
when the economy starts to sink
are the last people who got in.

00:32:13.200 --> 00:32:15.630
And of course there was
still historic problems

00:32:15.630 --> 00:32:17.790
with this in terms of
redlining that denied access

00:32:17.790 --> 00:32:20.520
to African-Americans to
that shared prosperity.

00:32:20.520 --> 00:32:25.020
But nevertheless, you got people
who became officially American

00:32:25.020 --> 00:32:27.990
instead of others, instead
of perpetual immigrants.

00:32:27.990 --> 00:32:30.450
And so there's this story that
goes, if you come to America

00:32:30.450 --> 00:32:32.640
and you just hang out,
your kids or their kids

00:32:32.640 --> 00:32:33.510
will be Americans.

00:32:33.510 --> 00:32:34.650
They'll cease
thinking of themselves

00:32:34.650 --> 00:32:36.570
as Polish Americans
or Italian Americans.

00:32:36.570 --> 00:32:40.605
But really what we're saying
is, they'll become middle class.

00:32:40.605 --> 00:32:42.240
And right now, it's
looking like that's

00:32:42.240 --> 00:32:45.240
ended, that you don't become
middle class if you show up

00:32:45.240 --> 00:32:51.570
in America with nothing but
the shoe leather on your feet.

00:32:51.570 --> 00:32:54.870
You just stay outside
of wealth accumulation,

00:32:54.870 --> 00:32:57.630
outside of social mobility,
because all of the policy

00:32:57.630 --> 00:33:02.340
levers that were used to allow
people who had aptitude to gain

00:33:02.340 --> 00:33:04.950
access to education,
and capital,

00:33:04.950 --> 00:33:07.350
and to start businesses,
and so on, those

00:33:07.350 --> 00:33:09.630
have been snuffed
out one after another

00:33:09.630 --> 00:33:13.170
as the concentration of
wealth at the top has grown,

00:33:13.170 --> 00:33:16.710
and as they have more
policy levers to yank on

00:33:16.710 --> 00:33:20.217
to increase the
concentration of their wealth

00:33:20.217 --> 00:33:21.550
at the expense of everyone else.

00:33:21.550 --> 00:33:22.967
And if you think
about it, the law

00:33:22.967 --> 00:33:24.810
of large numbers, the
law of small numbers,

00:33:24.810 --> 00:33:29.790
making the wealth of someone
who owns almost everything grow

00:33:29.790 --> 00:33:31.922
by even 1% is hard.

00:33:31.922 --> 00:33:34.380
Making the wealth of someone
who owns almost nothing double

00:33:34.380 --> 00:33:35.130
is easy.

00:33:35.130 --> 00:33:37.900
That's why startups
have incredible growth.

00:33:37.900 --> 00:33:39.930
We went from one user to two.

00:33:39.930 --> 00:33:43.380
That is a very
impressive growth number

00:33:43.380 --> 00:33:45.090
expressed as a percentage.

00:33:45.090 --> 00:33:46.770
It will take a lot
longer for Google

00:33:46.770 --> 00:33:49.440
to double its users
than the startup that

00:33:49.440 --> 00:33:50.350
goes from one to two.

00:33:50.350 --> 00:33:54.240
And so if you're making the
rich, the super rich richer,

00:33:54.240 --> 00:33:57.180
even a little bit richer,
measurably richer,

00:33:57.180 --> 00:34:00.090
it has to come at
the expense of nearly

00:34:00.090 --> 00:34:03.133
the total net worth of
a huge number of people.

00:34:03.133 --> 00:34:05.550
That's why that Oxfam number
of the number of billionaires

00:34:05.550 --> 00:34:07.717
in a bus it would take to
represent half the world's

00:34:07.717 --> 00:34:11.607
wealth is so shocking,
because it doesn't just

00:34:11.607 --> 00:34:12.690
mean that they got richer.

00:34:12.690 --> 00:34:16.830
It means that a giant number of
people had to get much poorer.

00:34:16.830 --> 00:34:19.100
And so anytime
you see the wealth

00:34:19.100 --> 00:34:22.409
of the top decile or
the top percentile

00:34:22.409 --> 00:34:25.590
or the top 1/10 of a percentile
increasing measurably,

00:34:25.590 --> 00:34:28.442
you know that it's not
coming from growth.

00:34:28.442 --> 00:34:30.150
I mean, there's some
growth, but it's not

00:34:30.150 --> 00:34:31.440
coming primarily from growth.

00:34:31.440 --> 00:34:33.750
It's coming through
redistribution.

00:34:33.750 --> 00:34:35.550
And that redistribution
is upwards,

00:34:35.550 --> 00:34:37.739
and it's vastly asymmetrical.

00:34:37.739 --> 00:34:41.340
Everything you own going to make
a tiny difference to someone

00:34:41.340 --> 00:34:43.020
who already owns
nearly everything.

00:34:43.020 --> 00:34:45.300
SPEAKER: We've long
thought that technology

00:34:45.300 --> 00:34:47.520
will help wear away the
differences between class,

00:34:47.520 --> 00:34:49.260
between race.

00:34:49.260 --> 00:34:52.530
And in the story, the poor
have access to technology,

00:34:52.530 --> 00:34:54.690
but they don't have a
choice of the technology

00:34:54.690 --> 00:34:56.159
that they can access.

00:34:56.159 --> 00:34:57.960
And there's a moment
in the piece when

00:34:57.960 --> 00:35:01.570
I laughed, where when
asked what kind of toaster

00:35:01.570 --> 00:35:04.440
the startup employee
had, he said, well,

00:35:04.440 --> 00:35:05.760
not that kind of toaster.

00:35:05.760 --> 00:35:08.510
And so the better off could
choose what tools they use.

00:35:08.510 --> 00:35:08.820
CORY DOCTOROW: Sure.

00:35:08.820 --> 00:35:09.660
SPEAKER: Could you
talk a little bit

00:35:09.660 --> 00:35:11.430
about that element of choice?

00:35:11.430 --> 00:35:13.140
CORY DOCTOROW: Yeah,
I mean, I think

00:35:13.140 --> 00:35:17.670
that there is a common pathology
among technologists, especially

00:35:17.670 --> 00:35:19.950
those who work on restrictive
technologies, which

00:35:19.950 --> 00:35:22.860
is that we never imagine
ourselves using them.

00:35:22.860 --> 00:35:24.570
We imagine them being
deployed on others.

00:35:24.570 --> 00:35:26.490
We will always have root, right?

00:35:26.490 --> 00:35:28.583
We will always-- we
were just talking

00:35:28.583 --> 00:35:30.000
before we came in
about this story

00:35:30.000 --> 00:35:33.630
today about YouTubers being
extorted by fraudsters who

00:35:33.630 --> 00:35:36.125
register fake copyright strikes,
the fraudulent copyright

00:35:36.125 --> 00:35:38.250
strikes against their
YouTube accounts and then say

00:35:38.250 --> 00:35:41.070
give me $150 or I'll put
a third strike on you,

00:35:41.070 --> 00:35:43.050
because everybody knows
you can't get anyone

00:35:43.050 --> 00:35:45.360
at YouTube on the phone
when you've had a copyright

00:35:45.360 --> 00:35:48.000
strike against you, because
literally anyone who

00:35:48.000 --> 00:35:49.500
gets a copyright
strike against them

00:35:49.500 --> 00:35:52.020
thinks that it's
illegitimate, right?

00:35:52.020 --> 00:35:54.830
So YouTube doesn't have
any great way to triage it

00:35:54.830 --> 00:35:57.120
and you just get stuck in
these support email loops.

00:35:57.120 --> 00:35:58.745
And there isn't a
support choice choice

00:35:58.745 --> 00:36:01.230
for "I'm being extorted
by a petty grifter who

00:36:01.230 --> 00:36:06.060
wants a millionth of a
bitcoin" to get my copyright

00:36:06.060 --> 00:36:06.873
strikes removed.

00:36:06.873 --> 00:36:08.790
And so since that doesn't
exist an email loop,

00:36:08.790 --> 00:36:09.582
you can't get help.

00:36:09.582 --> 00:36:12.420
But if you are me and
you know googlers,

00:36:12.420 --> 00:36:14.190
that would never
happen to me, right?

00:36:14.190 --> 00:36:16.350
I would just call
a googler friend.

00:36:16.350 --> 00:36:19.200
Fred Von Lohmann, who
runs copyright for Google

00:36:19.200 --> 00:36:21.950
with a couple other people
used to work with me at EFF.

00:36:21.950 --> 00:36:24.370
If I could just call him, and
he would sort it out for me.

00:36:24.370 --> 00:36:26.760
So when we're in
these situations, when

00:36:26.760 --> 00:36:28.650
we design these
systems, we always

00:36:28.650 --> 00:36:30.660
know that they're
not going to bind us.

00:36:30.660 --> 00:36:34.080
I worked on this DRM
standard, trying to fight it,

00:36:34.080 --> 00:36:37.830
for digital television
called the Broadcast Flag.

00:36:37.830 --> 00:36:39.360
And it's a crazy
idea, and I don't

00:36:39.360 --> 00:36:41.318
want to go into the whole
thing, but part of it

00:36:41.318 --> 00:36:43.530
was that all general
purpose computers would

00:36:43.530 --> 00:36:46.275
have to only have outputs that
were approved by movie studios.

00:36:48.990 --> 00:36:53.965
But there was a professional
tools exemption.

00:36:53.965 --> 00:36:56.340
And so there were people from
all the big tech companies,

00:36:56.340 --> 00:36:58.110
and people from all the big
movie studios, and people

00:36:58.110 --> 00:36:59.730
from all the big
consumer electronics

00:36:59.730 --> 00:37:00.702
companies in the room.

00:37:00.702 --> 00:37:02.160
And as soon as they
"said, there'll

00:37:02.160 --> 00:37:03.510
be a professional
tools exemption,"

00:37:03.510 --> 00:37:05.130
everyone's like "yeah,
of course there'll

00:37:05.130 --> 00:37:06.420
be a professional
tools exemption."

00:37:06.420 --> 00:37:07.890
You think I'm gonna
use this stuff?

00:37:07.890 --> 00:37:08.820
No way!

00:37:08.820 --> 00:37:10.130
Right?

00:37:10.130 --> 00:37:14.910
I'm gonna have the version
of this that isn't terrible.

00:37:14.910 --> 00:37:17.950
The terrible version
is for other people.

00:37:17.950 --> 00:37:22.080
And so trying to
tease that out, this

00:37:22.080 --> 00:37:25.290
idea that when we
make technology

00:37:25.290 --> 00:37:29.340
that we would never
want to use, we're

00:37:29.340 --> 00:37:34.230
dooming lots of other
people to using it.

00:37:34.230 --> 00:37:36.370
A recurring version
of this is every time

00:37:36.370 --> 00:37:38.370
people say, "well, you
know all those people who

00:37:38.370 --> 00:37:42.210
work in Silicon Valley don't
let their kids own phones

00:37:42.210 --> 00:37:45.950
or watch YouTube or whatever.

00:37:45.950 --> 00:37:48.450
Or they all have ad
blockers installed."

00:37:48.450 --> 00:37:50.370
All the other things
that we know about

00:37:50.370 --> 00:37:54.300
technologists, and
I'm guilty of it too,

00:37:54.300 --> 00:38:00.032
that civilians don't have
access to or don't get to do.

00:38:00.032 --> 00:38:01.490
SPEAKER: It also
reminds me of what

00:38:01.490 --> 00:38:03.407
we were talking about
before the talk in terms

00:38:03.407 --> 00:38:05.847
of just how beautiful it is
when something just works.

00:38:05.847 --> 00:38:06.680
CORY DOCTOROW: Yeah.

00:38:06.680 --> 00:38:08.972
SPEAKER: The book also
addresses kind of functionalism.

00:38:08.972 --> 00:38:11.460
There's a couple of
neat lines in the story.

00:38:11.460 --> 00:38:14.570
"This was a new kind of toaster,
a toaster that took orders

00:38:14.570 --> 00:38:16.340
rather than giving them."

00:38:16.340 --> 00:38:19.500
And then also "if someone wants
to control you with a computer,

00:38:19.500 --> 00:38:22.603
they have to put the computer
where you are and they are not.

00:38:22.603 --> 00:38:24.770
So you can access the
computer without supervision."

00:38:24.770 --> 00:38:26.520
Can you talk a little
bit about functional

00:38:26.520 --> 00:38:29.760
and how we might be moving away
from that or closer to that?

00:38:29.760 --> 00:38:32.870
CORY DOCTOROW: So the nexus
of control is the key thing.

00:38:32.870 --> 00:38:35.090
The difference in
utopia and dystopia

00:38:35.090 --> 00:38:37.580
is, who's got their
finger on the button.

00:38:37.580 --> 00:38:40.178
Having a Fitbit that tells you
how many steps you've walked,

00:38:40.178 --> 00:38:42.470
leaving aside the problems
with the sensor and the fact

00:38:42.470 --> 00:38:44.803
that it might think you've
walked 1,500 steps before you

00:38:44.803 --> 00:38:47.330
get into bed, but
having a Fitbit that

00:38:47.330 --> 00:38:50.815
helps you track your own
fitness levels, maybe

00:38:50.815 --> 00:38:52.940
you could have a pathological
relationship with it,

00:38:52.940 --> 00:38:57.110
but there's nothing wrong
with you knowing more

00:38:57.110 --> 00:38:59.230
about yourself.

00:38:59.230 --> 00:39:01.210
Having your boss
put a Fitbit on you

00:39:01.210 --> 00:39:03.280
and say, "if you don't
get your 10,000 steps in,

00:39:03.280 --> 00:39:07.460
we're gonna take you out of
the company health plan"?

00:39:07.460 --> 00:39:08.780
It's the same technology.

00:39:08.780 --> 00:39:11.650
It's just a different
nexus of control.

00:39:11.650 --> 00:39:14.470
And if there's a
mechanism in the Fitbit

00:39:14.470 --> 00:39:17.950
that detects spurious
sensor events in order

00:39:17.950 --> 00:39:20.740
to stop the Fitbit from telling
you that you've walked 10,000

00:39:20.740 --> 00:39:23.230
steps when you've really
only just gotten out of bed,

00:39:23.230 --> 00:39:24.430
that's great.

00:39:24.430 --> 00:39:26.470
If that same sensor is
used to stop you putting

00:39:26.470 --> 00:39:28.720
your Fitbit in a sock and
stick it in the tumble dryer

00:39:28.720 --> 00:39:31.480
to fool your boss into thinking
that you've done your 10,000

00:39:31.480 --> 00:39:33.520
steps, that's terrible.

00:39:33.520 --> 00:39:35.440
So the best example
of this, Doug Rushkoff

00:39:35.440 --> 00:39:38.680
just wrote this column about
going to this some hedge fund

00:39:38.680 --> 00:39:39.675
conference.

00:39:39.675 --> 00:39:41.050
And one of the
panels was on what

00:39:41.050 --> 00:39:44.050
to do in the event,
when the collapse comes

00:39:44.050 --> 00:39:46.870
and the poors come to eat you.

00:39:46.870 --> 00:39:49.545
They were talking
about this problem

00:39:49.545 --> 00:39:51.670
that they would have when
currency collapsed, which

00:39:51.670 --> 00:39:54.490
is how do you stop your
guards from killing you

00:39:54.490 --> 00:39:56.760
if they don't need you
to pay their paychecks.

00:39:56.760 --> 00:39:58.510
They said, well, we've
got to sort it out.

00:39:58.510 --> 00:40:03.310
We're going to these two-factor
biometric and secret food

00:40:03.310 --> 00:40:04.390
lockers.

00:40:04.390 --> 00:40:07.220
And so they'll have to keep me
alive to unlock the food locker

00:40:07.220 --> 00:40:08.890
so they don't starve to death.

00:40:08.890 --> 00:40:12.400
And that's going to be the thing
that keeps my guards in line.

00:40:12.400 --> 00:40:16.370
And I know people, and I
have been on many diets.

00:40:16.370 --> 00:40:17.810
And sometimes those
diets involved

00:40:17.810 --> 00:40:19.430
not eating certain things.

00:40:19.430 --> 00:40:21.770
And if I put a lock on my
fridge that I could control

00:40:21.770 --> 00:40:23.990
that didn't open until
it was lunchtime,

00:40:23.990 --> 00:40:26.450
literally the same technology
the only difference

00:40:26.450 --> 00:40:27.990
is who's pushing the button.

00:40:27.990 --> 00:40:30.080
The difference between
utopian and dystopia

00:40:30.080 --> 00:40:31.520
is who gets to decide.

00:40:31.520 --> 00:40:34.100
It's where the choice is.

00:40:34.100 --> 00:40:37.310
And so these people are
moving from a situation

00:40:37.310 --> 00:40:40.700
where the technology that
they're in is a stricture,

00:40:40.700 --> 00:40:42.590
and it becomes an enabler.

00:40:42.590 --> 00:40:45.380
And I think that there's a
story that, like going back

00:40:45.380 --> 00:40:48.703
to the dishwashers and
the Steve Jobs figure,

00:40:48.703 --> 00:40:50.120
there's a story
that we tell about

00:40:50.120 --> 00:40:52.940
technological inevitability.

00:40:52.940 --> 00:40:57.110
We have to make it impossible
and illegal for you

00:40:57.110 --> 00:41:00.020
to reconfigure your
devices, or they won't work,

00:41:00.020 --> 00:41:01.910
or they won't keep you safe.

00:41:01.910 --> 00:41:03.973
And we talk about
it as though it's

00:41:03.973 --> 00:41:06.140
like someone came down off
a mountain with two stone

00:41:06.140 --> 00:41:08.080
tablets that said that.

00:41:08.080 --> 00:41:12.680
When we talk about online
surveillance, ad tech,

00:41:12.680 --> 00:41:15.710
and predictive markets,
and machine learning,

00:41:15.710 --> 00:41:18.027
I am old enough to
remember when we rotated

00:41:18.027 --> 00:41:19.610
our server logs
instead of mining them

00:41:19.610 --> 00:41:21.200
for market intelligence.

00:41:21.200 --> 00:41:22.850
There's no reason.

00:41:22.850 --> 00:41:25.490
Nobody will take
away your CS degree

00:41:25.490 --> 00:41:29.870
if you start rotating your logs
again instead of saving them.

00:41:29.870 --> 00:41:32.150
That is a choice someone
made, but we disguise it

00:41:32.150 --> 00:41:34.880
as a technological
inevitability.

00:41:34.880 --> 00:41:38.570
And so by showing that
when the nexus of controls

00:41:38.570 --> 00:41:41.848
changes, the nature of your
relationship to the technology

00:41:41.848 --> 00:41:43.640
changes, and the thing
that you didn't like

00:41:43.640 --> 00:41:45.620
about the technology
turns out to be a thing

00:41:45.620 --> 00:41:48.770
you didn't like about the
nexus of control, that is,

00:41:48.770 --> 00:41:50.360
I think, again,
a powerful lesson

00:41:50.360 --> 00:41:54.755
to encourage people
to understand STEM

00:41:54.755 --> 00:41:58.160
and to become masters
of their technology.

00:41:58.160 --> 00:42:01.010
We talk about STEM education
as though it's just

00:42:01.010 --> 00:42:03.070
part of the normal
grift of, if you

00:42:03.070 --> 00:42:04.820
don't get your kids
to do this and give me

00:42:04.820 --> 00:42:06.410
money to teach your
kids to do this,

00:42:06.410 --> 00:42:08.230
your kids will be
economic roadkill.

00:42:08.230 --> 00:42:10.310
And there's certainly a
certain element of that,

00:42:10.310 --> 00:42:13.760
but again, I think the origins
of the STEM Education Movement

00:42:13.760 --> 00:42:17.960
are about digital self-defense,
not about creating

00:42:17.960 --> 00:42:19.820
an industrial workforce.

00:42:19.820 --> 00:42:21.050
Program or be programmed.

00:42:21.050 --> 00:42:24.530
Learn how to use the technology,
or the technology will use you.

00:42:24.530 --> 00:42:28.640
And that empowering
message of technology

00:42:28.640 --> 00:42:30.710
is one that I really
firmly believe in.

00:42:30.710 --> 00:42:32.810
I don't want to get
rid of computers.

00:42:32.810 --> 00:42:34.833
I just want to change
how we use them.

00:42:34.833 --> 00:42:37.250
SPEAKER: I want to talk a
little bit about the audio book.

00:42:37.250 --> 00:42:37.500
CORY DOCTOROW: Yeah.

00:42:37.500 --> 00:42:39.770
SPEAKER: So this novella
came out yesterday.

00:42:39.770 --> 00:42:42.350
The other stories are
coming out March 19.

00:42:42.350 --> 00:42:45.470
And the voice actor
who did this one's

00:42:45.470 --> 00:42:47.060
pretty interesting just herself.

00:42:47.060 --> 00:42:48.910
And I want to ask you about her.

00:42:48.910 --> 00:42:53.150
Lameece Issaq is a Palestinian
American actor and playwright

00:42:53.150 --> 00:42:55.100
and founded a production
company dedicated

00:42:55.100 --> 00:42:57.200
to Middle Eastern theater arts.

00:42:57.200 --> 00:42:59.393
How did you connect
with her and get

00:42:59.393 --> 00:43:00.560
her involved in the project?

00:43:00.560 --> 00:43:02.270
CORY DOCTOROW: I'm lucky enough
that I've been really involved

00:43:02.270 --> 00:43:04.395
with the production of my
last several audio books.

00:43:04.395 --> 00:43:06.650
We record them at
a studio near me--

00:43:06.650 --> 00:43:10.310
they're in Hollywood--
called Sky Boat Media.

00:43:10.310 --> 00:43:12.602
So they consult me on
casting choices and so on.

00:43:12.602 --> 00:43:14.060
And my hard and
fast rule with them

00:43:14.060 --> 00:43:19.000
was that I wanted a voice
actor who was of Arab origin.

00:43:19.000 --> 00:43:20.750
And I wanted a woman,
because the narrator

00:43:20.750 --> 00:43:22.040
is a woman of Arab origin.

00:43:22.040 --> 00:43:26.390
And I have a friend,
Lexi Alexander,

00:43:26.390 --> 00:43:28.100
who's a very interesting person.

00:43:28.100 --> 00:43:31.580
She's a director now, but she
was a world champion kickboxer.

00:43:31.580 --> 00:43:33.290
She's a German
Palestinian woman who

00:43:33.290 --> 00:43:37.220
was brought here by Chuck
Norris to help train the army.

00:43:37.220 --> 00:43:39.650
She became a stunt woman,
then became a director.

00:43:39.650 --> 00:43:40.910
She directed "Punisher."

00:43:40.910 --> 00:43:42.700
She directed a
couple "Supergirls."

00:43:42.700 --> 00:43:43.940
She does all kinds of stuff.

00:43:43.940 --> 00:43:46.520
And one of the things that
she's always on about on Twitter

00:43:46.520 --> 00:43:50.240
is all of the talented
Arab American woman

00:43:50.240 --> 00:43:52.490
actors and other
actors of color who

00:43:52.490 --> 00:43:55.550
could be doing the roles that,
for reasons that completely

00:43:55.550 --> 00:43:57.980
baffle me, are not
going to people

00:43:57.980 --> 00:43:59.690
who they would be suited for.

00:43:59.690 --> 00:44:02.850
And so I asked her
for some names.

00:44:02.850 --> 00:44:04.400
None of them could do it.

00:44:04.400 --> 00:44:07.110
But then I went to the
directors, and I said,

00:44:07.110 --> 00:44:11.940
we really need to cast an
Arab-American woman for this,

00:44:11.940 --> 00:44:13.610
or a woman of Arabic origin.

00:44:13.610 --> 00:44:16.460
And we got the demo reels
for four or five of them,

00:44:16.460 --> 00:44:18.830
and Lameece was the best
one, and she was available,

00:44:18.830 --> 00:44:20.923
and the rest happened there.

00:44:20.923 --> 00:44:23.090
I was in the studio everyday
when we were recording.

00:44:23.090 --> 00:44:24.573
She did an amazing job.

00:44:24.573 --> 00:44:27.115
SPEAKER: It's a wonderful listen
as well as a wonderful read.

00:44:27.115 --> 00:44:27.770
CORY DOCTOROW: Yeah.

00:44:27.770 --> 00:44:28.970
SPEAKER: Questions
from the audience.

00:44:28.970 --> 00:44:30.200
CORY DOCTOROW: If we can
alternate between people

00:44:30.200 --> 00:44:31.700
identify who as
women and people who

00:44:31.700 --> 00:44:34.040
identify as men or non-binary
and can go at anytime,

00:44:34.040 --> 00:44:37.520
but that way it's
not just dumb dudes.

00:44:37.520 --> 00:44:39.520
AUDIENCE: So a lot of
science fiction writers,

00:44:39.520 --> 00:44:42.350
I'm thinking of like Ray
Bradbury and Isaac Asimov

00:44:42.350 --> 00:44:45.020
and so on, write
a lot of stories

00:44:45.020 --> 00:44:50.330
that come to imagine a
shared world of the future.

00:44:50.330 --> 00:44:54.260
And they build up this kind of
consistent world of the future.

00:44:54.260 --> 00:44:56.780
Ray Bradbury did that
with "Martian Chronicles,"

00:44:56.780 --> 00:44:59.190
and Asimov with the
"Foundation" and so on.

00:44:59.190 --> 00:45:01.928
I'm just curious if you see
yourself stumbling toward that,

00:45:01.928 --> 00:45:04.470
or you might be doing that on
purpose, or that kind of thing.

00:45:04.470 --> 00:45:05.360
CORY DOCTOROW: I like
the way Bradbury did

00:45:05.360 --> 00:45:07.640
it more than the way Asimov
did, because they're not

00:45:07.640 --> 00:45:10.580
all in the same continuity.

00:45:10.580 --> 00:45:12.680
There are things that
happen on Mars in one story

00:45:12.680 --> 00:45:15.180
that if they happen, then the
next story doesn't make sense.

00:45:15.180 --> 00:45:15.570
AUDIENCE: Right.

00:45:15.570 --> 00:45:17.390
CORY DOCTOROW: But where it
makes sense to overlap them

00:45:17.390 --> 00:45:17.910
he does.

00:45:17.910 --> 00:45:19.535
And another writer
who does that really

00:45:19.535 --> 00:45:21.868
well is John Varley, who
has this whole long cycle

00:45:21.868 --> 00:45:23.910
of stories that he's been
writing since the '70s.

00:45:23.910 --> 00:45:26.450
They're amazing.

00:45:26.450 --> 00:45:28.940
I think they're called
the Six World stories.

00:45:28.940 --> 00:45:30.110
Earth becomes uninhabitable.

00:45:30.110 --> 00:45:32.510
We're living everywhere
except Earth.

00:45:32.510 --> 00:45:36.260
And when continuity makes
sense, he's got continuity.

00:45:36.260 --> 00:45:37.730
And when it doesn't, it doesn't.

00:45:37.730 --> 00:45:44.770
And I love that because these
aren't alternate history.

00:45:44.770 --> 00:45:46.210
These aren't history.

00:45:46.210 --> 00:45:47.950
They're not instructions.

00:45:47.950 --> 00:45:49.330
They're not predictions.

00:45:49.330 --> 00:45:53.860
They're artistic works
whose effect is--

00:45:53.860 --> 00:45:57.160
yeah, some continuity is
part of the effect of it,

00:45:57.160 --> 00:46:00.670
but the themes of
the characters,

00:46:00.670 --> 00:46:02.260
and the kinds of
characters they are,

00:46:02.260 --> 00:46:03.718
and the kinds of
things they get up

00:46:03.718 --> 00:46:08.380
to are more important than
making sure that if someone

00:46:08.380 --> 00:46:10.300
does something in
one story and then

00:46:10.300 --> 00:46:12.258
they appear as a big
character in another story

00:46:12.258 --> 00:46:14.920
that it still crosses over.

00:46:14.920 --> 00:46:17.440
I think of the way that
reboots of the shared

00:46:17.440 --> 00:46:19.420
worlds of the comic
book companies

00:46:19.420 --> 00:46:21.797
have worked, where when it
makes sense, it makes sense,

00:46:21.797 --> 00:46:23.880
and when it doesn't, it
doesn't, and they can have

00:46:23.880 --> 00:46:25.900
multiple parallel timelines.

00:46:25.900 --> 00:46:27.310
Does anyone think
Batman would be

00:46:27.310 --> 00:46:30.070
better if we were
all in continuity

00:46:30.070 --> 00:46:33.760
since the first
Detective Comics?

00:46:33.760 --> 00:46:35.950
The reinvention is
actually super important

00:46:35.950 --> 00:46:38.140
and part of the way that
we tell stories anyways.

00:46:38.140 --> 00:46:40.390
If you look at, say, all
the different versions

00:46:40.390 --> 00:46:44.020
of the Icelandic stories
or the Norse stories,

00:46:44.020 --> 00:46:46.120
there are multiple
irreconcilable versions

00:46:46.120 --> 00:46:47.320
of what the Norse did.

00:46:47.320 --> 00:46:50.440
There's two versions of
Genesis in the Bible.

00:46:50.440 --> 00:46:52.930
The first two
chapters of Genesis

00:46:52.930 --> 00:46:56.110
tell completely opposite,
non-reconcilable, mutually

00:46:56.110 --> 00:46:58.426
exclusive stories of
where the earth came from.

00:46:58.426 --> 00:47:00.138
[LAUGHTER]

00:47:00.138 --> 00:47:01.930
AUDIENCE: So you see
yourself as interested

00:47:01.930 --> 00:47:03.430
in a thematically
consistent future

00:47:03.430 --> 00:47:05.410
but not necessarily a
logically consistent future.

00:47:05.410 --> 00:47:06.000
You might say it that way.

00:47:06.000 --> 00:47:08.230
CORY DOCTOROW: And I'm
a panzer, not a plodder.

00:47:08.230 --> 00:47:10.772
I'm not someone who does a
lot of card tricks in the dark

00:47:10.772 --> 00:47:12.730
to try and figure out
how the plots going work.

00:47:12.730 --> 00:47:15.430
I figure it out as I go, so
it doesn't really lend itself

00:47:15.430 --> 00:47:17.140
to that kind of continuity.

00:47:17.140 --> 00:47:17.830
Who's next?

00:47:17.830 --> 00:47:20.920
AUDIENCE: What you were saying
about digital self-defense,

00:47:20.920 --> 00:47:26.410
I was wondering, do you see
that as actually being doable?

00:47:26.410 --> 00:47:30.760
Because I'm pretty tech savvy,
and at home I run Linux,

00:47:30.760 --> 00:47:37.220
and I have my hard drives
encrypted with locks and so on.

00:47:37.220 --> 00:47:39.040
But there's no way
I could possibly

00:47:39.040 --> 00:47:41.957
run down all the attack
factors, even on my own machine.

00:47:41.957 --> 00:47:42.790
CORY DOCTOROW: Sure.

00:47:42.790 --> 00:47:48.940
Yeah, and I think that there
is a story we have about what

00:47:48.940 --> 00:47:51.370
the early days of the
internet liberation

00:47:51.370 --> 00:47:54.130
movement, techno-politics
movement was that I think

00:47:54.130 --> 00:47:56.760
is wrong, but it's got a
little kernel of rightness.

00:47:56.760 --> 00:47:59.560
The wrong version is, we used to
think that technology couldn't

00:47:59.560 --> 00:48:02.860
be possibly used to do harm,
and so we wanted everyone

00:48:02.860 --> 00:48:04.690
to use technology,
and we didn't care

00:48:04.690 --> 00:48:07.300
how it was used because we
were sure that it would just

00:48:07.300 --> 00:48:08.820
make everyone's life better.

00:48:08.820 --> 00:48:10.900
That's clearly not true.

00:48:10.900 --> 00:48:12.820
You don't found Electronic
Frontier Foundation

00:48:12.820 --> 00:48:14.740
because you think everything's
going to be great.

00:48:14.740 --> 00:48:16.198
You found it because
you're worried

00:48:16.198 --> 00:48:18.688
about how terrible it will go
if it doesn't turn out great.

00:48:18.688 --> 00:48:20.230
You see on the one
hand, the promise,

00:48:20.230 --> 00:48:21.680
and on the other
hand, the peril.

00:48:21.680 --> 00:48:23.770
But the cipher punk
movement particularly

00:48:23.770 --> 00:48:25.900
did have this idea
that you could,

00:48:25.900 --> 00:48:27.310
through encrypted
communications,

00:48:27.310 --> 00:48:31.690
create a parallel universe
where even if you lived

00:48:31.690 --> 00:48:34.433
in a totalitarian or
unaccountable system

00:48:34.433 --> 00:48:36.100
where people could
operate with impunity

00:48:36.100 --> 00:48:41.350
and do terrible things to you,
that because the ciphers worked

00:48:41.350 --> 00:48:43.570
and the keys couldn't
be brute forced,

00:48:43.570 --> 00:48:48.100
that you could in some way
resist the state indefinitely.

00:48:48.100 --> 00:48:51.830
You could have a demi-monde
that existed alongside of it.

00:48:51.830 --> 00:48:55.040
And I think that not every
cipher punk felt that way,

00:48:55.040 --> 00:48:56.140
but a lot of them did.

00:48:56.140 --> 00:48:57.710
And I think that that's wrong.

00:48:57.710 --> 00:48:59.877
But I think that what we've
learned about encryption

00:48:59.877 --> 00:49:02.080
and its relationship to
unaccountable authority

00:49:02.080 --> 00:49:07.930
and illegitimate authority
is that encryption,

00:49:07.930 --> 00:49:11.050
it's like a stopgap.

00:49:11.050 --> 00:49:12.760
The technology is
a stopgap that you

00:49:12.760 --> 00:49:18.340
can use to organize
and to resist coercion

00:49:18.340 --> 00:49:20.170
while you figure out
how to make the power

00:49:20.170 --> 00:49:22.000
structures that you're
organizing about

00:49:22.000 --> 00:49:25.840
and that you're worried about
coercion from more accountable.

00:49:25.840 --> 00:49:31.950
But technology
alone can't do it.

00:49:31.950 --> 00:49:34.460
But if you try to
imagine the inverse,

00:49:34.460 --> 00:49:36.770
imagine creating a
political movement

00:49:36.770 --> 00:49:39.740
that holds power to
account and demands

00:49:39.740 --> 00:49:43.340
accountable and legitimate
exercise of authority,

00:49:43.340 --> 00:49:46.130
but that doesn't use computers.

00:49:46.130 --> 00:49:48.470
You find each other
by, I don't know,

00:49:48.470 --> 00:49:53.900
stapling photocopied
posters to telephone poles.

00:49:53.900 --> 00:49:57.462
And imagine how easily you would
be outmaneuvered by the force

00:49:57.462 --> 00:49:58.670
that you're trying to resist.

00:49:58.670 --> 00:50:03.590
It seems obvious to me that
the power relations dynamic

00:50:03.590 --> 00:50:06.230
that we're in now
is using technology

00:50:06.230 --> 00:50:09.230
to open a space to make a
political change that gives us

00:50:09.230 --> 00:50:11.210
the space to make technology
that opens a space

00:50:11.210 --> 00:50:12.500
to make political change.

00:50:12.500 --> 00:50:14.030
Lather, rinse, repeat.

00:50:14.030 --> 00:50:18.980
And you don't
always win, but not

00:50:18.980 --> 00:50:20.720
to just create this demi-monde.

00:50:20.720 --> 00:50:25.790
And the good news is that
people's direct experience

00:50:25.790 --> 00:50:28.580
of the way technology can
be a force for liberation

00:50:28.580 --> 00:50:32.990
and the way that technology when
abused can be a force of terror

00:50:32.990 --> 00:50:35.420
means that people
are actually caring

00:50:35.420 --> 00:50:38.525
about the problems a lot more.

00:50:38.525 --> 00:50:42.300
You can make an analogy
here to climate change.

00:50:42.300 --> 00:50:44.220
If you were the world's
greatest recycler,

00:50:44.220 --> 00:50:45.690
it wouldn't stop climate change.

00:50:45.690 --> 00:50:47.310
There is nothing
you can personally

00:50:47.310 --> 00:50:50.403
do that changes climate
change, that will change

00:50:50.403 --> 00:50:51.570
the facts of climate change.

00:50:51.570 --> 00:50:53.040
But you and everyone
else you know

00:50:53.040 --> 00:50:55.380
and everyone they know
all working together

00:50:55.380 --> 00:50:57.160
can do something
about climate change,

00:50:57.160 --> 00:50:59.070
including collectively
recycling and making

00:50:59.070 --> 00:51:00.480
a lot of other
choices, including

00:51:00.480 --> 00:51:02.790
choices about how
we invest and maybe

00:51:02.790 --> 00:51:05.110
a Green New Deal and so on.

00:51:05.110 --> 00:51:07.980
And for a long time,
our biggest problem

00:51:07.980 --> 00:51:11.370
was convincing people that
climate change was a problem.

00:51:11.370 --> 00:51:14.340
And we see today that even
among the people who've

00:51:14.340 --> 00:51:17.160
been historic climate
deniers, that's not really

00:51:17.160 --> 00:51:18.270
a problem anymore.

00:51:18.270 --> 00:51:20.790
We are past the point
of peak indifference.

00:51:20.790 --> 00:51:23.190
But the reason we're past the
point of peak indifference

00:51:23.190 --> 00:51:26.250
is that the number of people
for whom the reality of climate

00:51:26.250 --> 00:51:28.590
change is undeniable because
they or someone they love

00:51:28.590 --> 00:51:31.920
has had their lives harmed or
destroyed by climate change

00:51:31.920 --> 00:51:32.990
is only growing.

00:51:32.990 --> 00:51:34.740
Which means that we
moved from the problem

00:51:34.740 --> 00:51:37.432
of convincing people that
climate change is a problem.

00:51:37.432 --> 00:51:39.390
Now, we have to convince
them that it's not too

00:51:39.390 --> 00:51:41.080
late to do something about it.

00:51:41.080 --> 00:51:44.220
And I think we've gone
through that same trajectory

00:51:44.220 --> 00:51:46.440
with technology,
convincing people

00:51:46.440 --> 00:51:49.050
that how we regulate
technology matters,

00:51:49.050 --> 00:51:51.870
that's taken care of itself.

00:51:51.870 --> 00:51:54.300
The largest petition
in internet history

00:51:54.300 --> 00:51:56.820
is the one to overrule
the electoral college

00:51:56.820 --> 00:51:58.110
and make Hillary president.

00:51:58.110 --> 00:52:01.740
Trailing it by 1% is the
petition from the European

00:52:01.740 --> 00:52:03.990
Union to not pass the
copyright directive that

00:52:03.990 --> 00:52:07.750
would mandate content ID
for all public platforms.

00:52:07.750 --> 00:52:10.230
And that's crazy.

00:52:10.230 --> 00:52:12.240
The number of people
who pay attention

00:52:12.240 --> 00:52:15.162
to American presidential
politics versus people

00:52:15.162 --> 00:52:16.620
who care about
whether or not we're

00:52:16.620 --> 00:52:20.460
going to have upload filters
is now nearly at parity.

00:52:20.460 --> 00:52:21.960
So now we just have
to convince them

00:52:21.960 --> 00:52:24.910
that it's not too late
to do something about it.

00:52:24.910 --> 00:52:27.120
And the doing
something about it is

00:52:27.120 --> 00:52:29.635
the making the power
accountable and so on.

00:52:29.635 --> 00:52:32.010
Not that we should ever be
storing passwords in the clear

00:52:32.010 --> 00:52:37.860
because everyone's accountable,
but that we can store passwords

00:52:37.860 --> 00:52:40.350
in an encrypted
form and also assume

00:52:40.350 --> 00:52:42.660
that the secret police aren't
going to show up and make

00:52:42.660 --> 00:52:45.570
you put a backdoor in to
decrypt those passwords

00:52:45.570 --> 00:52:48.450
so they can man the middle
or otherwise disrupt

00:52:48.450 --> 00:52:50.430
the communications
of your users.

00:52:50.430 --> 00:52:53.040
And that's the
cycle that we're in.

00:52:53.040 --> 00:52:54.270
AUDIENCE: Thank you.

00:52:54.270 --> 00:52:57.450
AUDIENCE: So
obviously this country

00:52:57.450 --> 00:53:01.740
has corporations organized
based on a profit motive.

00:53:01.740 --> 00:53:03.930
Corporations have
to be accountable

00:53:03.930 --> 00:53:07.200
to their shareholders and
increase products, et cetera.

00:53:07.200 --> 00:53:09.870
We've been lucky and unlucky
with the results of that

00:53:09.870 --> 00:53:11.550
in the grand scheme of things.

00:53:11.550 --> 00:53:14.010
Obviously, from my
perspective and I'm

00:53:14.010 --> 00:53:16.260
sure a lot of people here
at Google's on the good side

00:53:16.260 --> 00:53:18.600
of that, hopefully.

00:53:18.600 --> 00:53:19.405
We'll see, I guess.

00:53:19.405 --> 00:53:21.030
CORY DOCTOROW: A lot
of the time it is.

00:53:21.030 --> 00:53:23.180
I think you're right.

00:53:23.180 --> 00:53:28.230
AUDIENCE: But there's
nothing stopping

00:53:28.230 --> 00:53:30.930
the way things are going from
going badly as well, even

00:53:30.930 --> 00:53:32.310
for this company.

00:53:32.310 --> 00:53:33.570
CORY DOCTOROW: Sure.

00:53:33.570 --> 00:53:35.070
AUDIENCE: Do you
think there's a way

00:53:35.070 --> 00:53:37.670
to realign the incentives
at a high level

00:53:37.670 --> 00:53:41.550
to make that less likely, to
make the good outcomes more

00:53:41.550 --> 00:53:43.020
likely, let's say?

00:53:43.020 --> 00:53:44.400
CORY DOCTOROW: I want to talk
about that European Union

00:53:44.400 --> 00:53:45.775
copyright directive,
because it's

00:53:45.775 --> 00:53:49.750
a good example of firms that
are generally good firms,

00:53:49.750 --> 00:53:51.840
or firms that have
done a lot of good,

00:53:51.840 --> 00:53:55.512
that are doing bad and doing it
in a weirdly dysfunctional way.

00:53:55.512 --> 00:53:57.720
So one of the things that's
weird about the copyright

00:53:57.720 --> 00:53:59.490
directive is that
the record labels are

00:53:59.490 --> 00:54:01.710
super in favor of it, and
the movie studios are super

00:54:01.710 --> 00:54:04.200
opposed to it, but they're
the same companies.

00:54:04.200 --> 00:54:06.660
Literally, Universal
Music wants it,

00:54:06.660 --> 00:54:08.280
and Universal Pictures doesn't.

00:54:08.280 --> 00:54:10.020
And they're both
sending open letters

00:54:10.020 --> 00:54:12.630
to the commission of
the European Union,

00:54:12.630 --> 00:54:16.080
demanding that they be heeded.

00:54:16.080 --> 00:54:19.620
So this is actually a
pretty common problem

00:54:19.620 --> 00:54:23.490
in the theory and
history of anti-trust,

00:54:23.490 --> 00:54:25.320
that beyond a
certain scale, there

00:54:25.320 --> 00:54:28.770
are massive
disefficiencies of scale.

00:54:28.770 --> 00:54:31.380
And you've probably encounter
them working in a large firm.

00:54:31.380 --> 00:54:34.990
There's a thing that's good for
the firm as far as you can see,

00:54:34.990 --> 00:54:37.770
but it gores the ox of someone
who's got a lot of power

00:54:37.770 --> 00:54:39.210
within the firm.

00:54:39.210 --> 00:54:41.617
I had a personal
relationship with Flickr

00:54:41.617 --> 00:54:42.450
when it was founded.

00:54:42.450 --> 00:54:46.230
I was an alpha tester for
the game that it came out

00:54:46.230 --> 00:54:47.460
of, "Game Neverending."

00:54:47.460 --> 00:54:49.140
And I was carrying on a
long distance relationship

00:54:49.140 --> 00:54:51.420
with a woman who's now my
wife, who lived in London.

00:54:51.420 --> 00:54:52.780
I lived in San Francisco.

00:54:52.780 --> 00:54:54.905
We were both alpha testers,
and Stewart Butterfield

00:54:54.905 --> 00:54:57.670
and Caterina Fake, who created
the game came to San Francisco,

00:54:57.670 --> 00:54:58.140
and we had lunch.

00:54:58.140 --> 00:54:58.980
And they said, how's it going.

00:54:58.980 --> 00:55:00.438
And I said, it's
great, but we have

00:55:00.438 --> 00:55:01.650
trouble sharing our images.

00:55:01.650 --> 00:55:03.330
And they said, oh, we've
got that coming in the game.

00:55:03.330 --> 00:55:04.800
We'll just accelerate it
in the product roadmap.

00:55:04.800 --> 00:55:06.883
And three months later,
they shut the company down

00:55:06.883 --> 00:55:10.130
and renamed it Flickr just
around that one photo sharing

00:55:10.130 --> 00:55:10.630
thing.

00:55:10.630 --> 00:55:11.940
So I feel really
close to Flickr,

00:55:11.940 --> 00:55:13.080
and I watched really
carefully what

00:55:13.080 --> 00:55:14.480
happened when Yahoo bought it.

00:55:14.480 --> 00:55:19.710
And Flickr was the first mobile
social photo sharing app.

00:55:19.710 --> 00:55:22.860
And so it had the power
to be a really big money

00:55:22.860 --> 00:55:24.130
spinner for Yahoo.

00:55:24.130 --> 00:55:27.000
But it gored the ox of Yahoo's
nascent mobile division,

00:55:27.000 --> 00:55:30.810
Yahoo's nascent social division,
Yahoo's nascent photo division,

00:55:30.810 --> 00:55:31.900
and so on.

00:55:31.900 --> 00:55:34.560
And the great
beasts of Yahoo who

00:55:34.560 --> 00:55:36.420
had the ear of the
senior management,

00:55:36.420 --> 00:55:39.060
who had assembled power
structures around them,

00:55:39.060 --> 00:55:42.225
were able to head off
and starve Flickr,

00:55:42.225 --> 00:55:43.350
so that now it limps along.

00:55:43.350 --> 00:55:44.725
It's just been
bought by SmugMug.

00:55:44.725 --> 00:55:46.560
It remains to be seen
what its future is,

00:55:46.560 --> 00:55:48.780
but it represents
this failed promise.

00:55:48.780 --> 00:55:52.650
And this is a really common
disefficiency of scale.

00:55:52.650 --> 00:55:55.080
These firms get bought
up by other firms,

00:55:55.080 --> 00:55:58.445
who then just poison them or
do terrible things with them.

00:55:58.445 --> 00:55:59.820
As I say, I think
Google has done

00:55:59.820 --> 00:56:01.237
a lot of good in
this world, but I

00:56:01.237 --> 00:56:03.690
think that effectively
burying the Deja News

00:56:03.690 --> 00:56:06.480
archive of early
Usenet, it's something

00:56:06.480 --> 00:56:09.420
between a crime and a shame.

00:56:09.420 --> 00:56:11.305
That's a really important
piece of history,

00:56:11.305 --> 00:56:12.930
and it's like a
disefficiency of scale.

00:56:12.930 --> 00:56:14.968
Someone just felt
like it just wasn't

00:56:14.968 --> 00:56:16.260
important to the core business.

00:56:16.260 --> 00:56:18.060
"Well, then why did you buy it?"

00:56:18.060 --> 00:56:18.730
and so on.

00:56:18.730 --> 00:56:21.870
And eventually it was just
starved off and vanished down

00:56:21.870 --> 00:56:23.040
the memory hole.

00:56:23.040 --> 00:56:28.260
And so one of the ways
that you make firms better

00:56:28.260 --> 00:56:31.720
is by making the amount of
harm that they can do less.

00:56:31.720 --> 00:56:35.515
Because then the bad things
they do aren't as important.

00:56:35.515 --> 00:56:36.890
And one of the
ways that you make

00:56:36.890 --> 00:56:38.430
the harm that they
can do less is

00:56:38.430 --> 00:56:41.040
by mandating that
they be smaller.

00:56:41.040 --> 00:56:43.470
And historically,
any time we had

00:56:43.470 --> 00:56:46.320
an industry that was
dominated by a few firms,

00:56:46.320 --> 00:56:48.570
or any time a firm tried
to buy a competitor,

00:56:48.570 --> 00:56:52.590
or any time a firm tried to buy
people in its vertical supply

00:56:52.590 --> 00:56:54.870
chain, to dominate its
vertical supply chain,

00:56:54.870 --> 00:56:56.280
we looked askance at that.

00:56:56.280 --> 00:56:58.920
We subjected it to
very close scrutiny,

00:56:58.920 --> 00:56:59.940
and we often blocked it.

00:56:59.940 --> 00:57:01.107
Sometimes we overblocked it.

00:57:01.107 --> 00:57:03.870
There's an argument that one
of the ways that we got here

00:57:03.870 --> 00:57:06.192
was that there was a
constituency for this story

00:57:06.192 --> 00:57:08.400
that antitrust had been
overused because it had been,

00:57:08.400 --> 00:57:11.130
and they were willing to
hear arguments for why

00:57:11.130 --> 00:57:14.280
it should be dialed down.

00:57:14.280 --> 00:57:17.348
But I think that--

00:57:17.348 --> 00:57:19.390
so I have a friend who
went to work for Facebook,

00:57:19.390 --> 00:57:21.760
and then he quit a year
later and came to talk to me

00:57:21.760 --> 00:57:23.260
and said, "you know,
you were right.

00:57:23.260 --> 00:57:24.470
I didn't like it very much.

00:57:24.470 --> 00:57:26.720
And one of the things that
I realized when I got there

00:57:26.720 --> 00:57:29.410
that maybe makes me sleep
better is that no one there

00:57:29.410 --> 00:57:30.492
is any smarter than I am.

00:57:30.492 --> 00:57:31.700
They're just as dumb as I am.

00:57:31.700 --> 00:57:33.830
They're not superheroes."

00:57:33.830 --> 00:57:35.470
And I'm like, but
of course they are.

00:57:35.470 --> 00:57:37.210
They're just like you and me.

00:57:37.210 --> 00:57:41.260
And the reason that Facebook
is a bad custodian of 2 billion

00:57:41.260 --> 00:57:44.950
people's social lives is because
nobody is a good custodian of 2

00:57:44.950 --> 00:57:48.610
billion people's social lives!

00:57:48.610 --> 00:57:50.830
How do you minimize
the harm that Facebook

00:57:50.830 --> 00:57:54.640
has in the toxic ways that it
enables people's social lives?

00:57:54.640 --> 00:57:57.250
Don't make it in charge of 2
billion people's social lives.

00:57:57.250 --> 00:57:58.000
Break it up.

00:57:58.000 --> 00:57:59.470
Make it into different pieces.

00:57:59.470 --> 00:58:01.090
Make it sell off Instagram.

00:58:01.090 --> 00:58:04.240
Make it split out the
two functions it has.

00:58:04.240 --> 00:58:06.600
One is helping you find
people to talk with,

00:58:06.600 --> 00:58:08.710
and the other one is
helping you talk with them.

00:58:08.710 --> 00:58:10.690
Make it split those
into two pieces,

00:58:10.690 --> 00:58:14.242
because it really sucks at
letting you talk to people.

00:58:14.242 --> 00:58:16.700
And it's really good at helping
you find people to talk to,

00:58:16.700 --> 00:58:18.075
whether those are
people who want

00:58:18.075 --> 00:58:20.350
to carry tiki torches to
Charlottesville or people

00:58:20.350 --> 00:58:24.085
who want to form a Little
League with you or whatever.

00:58:24.085 --> 00:58:26.460
It's really good at finding
those people, people who have

00:58:26.460 --> 00:58:27.808
the same rare disease as you.

00:58:27.808 --> 00:58:28.850
It's really good at that.

00:58:28.850 --> 00:58:31.058
It just sucks as a place to
carry on the conversation

00:58:31.058 --> 00:58:34.048
afterwards, mostly because
there's not a lot of engagement

00:58:34.048 --> 00:58:35.590
to be had if you
have a rare disease.

00:58:35.590 --> 00:58:36.430
You check in everyday.

00:58:36.430 --> 00:58:37.060
Things are OK.

00:58:37.060 --> 00:58:38.685
Things aren't OK.

00:58:38.685 --> 00:58:40.060
There isn't
blockbuster news that

00:58:40.060 --> 00:58:42.102
keeps you hanging out on
the rare disease message

00:58:42.102 --> 00:58:43.360
board all day.

00:58:43.360 --> 00:58:45.820
So to up your engagement
level, Facebook's disefficiency

00:58:45.820 --> 00:58:47.320
of scale is, they
take people who've

00:58:47.320 --> 00:58:48.975
gathered to talk
about a rare disease,

00:58:48.975 --> 00:58:50.350
and they throw
click bait at them

00:58:50.350 --> 00:58:52.330
so that they stay
engaged, because their KPI

00:58:52.330 --> 00:58:54.077
and their bonus is on
engagement minutes,

00:58:54.077 --> 00:58:55.660
not whether or not
you're successfully

00:58:55.660 --> 00:58:57.010
managing your rare disease.

00:58:57.010 --> 00:58:58.660
Make them split those
two functions up,

00:58:58.660 --> 00:59:00.490
you solve the problem.

00:59:00.490 --> 00:59:03.010
Now you have a business that
just gets monotonically better

00:59:03.010 --> 00:59:04.720
at helping you find
people to talk with,

00:59:04.720 --> 00:59:06.430
and a business
that rises or sinks

00:59:06.430 --> 00:59:09.490
on its ability to get you
to talk with them there.

00:59:09.490 --> 00:59:11.360
And then you limit
a lot of the harm.

00:59:11.360 --> 00:59:13.820
AUDIENCE: But as a bit of a
counterexample, so at Google,

00:59:13.820 --> 00:59:16.517
for example, the
primary income is ads.

00:59:16.517 --> 00:59:17.350
CORY DOCTOROW: Sure.

00:59:17.350 --> 00:59:19.267
AUDIENCE: Was historically,
now it's changing.

00:59:19.267 --> 00:59:26.030
But that has resulted in
things like Gmail, and Docs,

00:59:26.030 --> 00:59:30.930
and Drive essentially
operating at a loss,

00:59:30.930 --> 00:59:34.000
and being subsidized
by the successful parts

00:59:34.000 --> 00:59:34.760
of the business.

00:59:34.760 --> 00:59:37.302
CORY DOCTOROW: Yeah, but Google
is not a break even function,

00:59:37.302 --> 00:59:38.040
right?

00:59:38.040 --> 00:59:42.663
You probably have some stock
options as an employee.

00:59:42.663 --> 00:59:44.080
So you've noticed
that the company

00:59:44.080 --> 00:59:48.000
pays dividends, and also
declares a profit every year.

00:59:48.000 --> 00:59:50.950
And so that tells you that
even if the ad business were

00:59:50.950 --> 00:59:55.390
curtailed by a breakup, it
would still not necessarily mean

00:59:55.390 --> 01:00:00.670
that the company was unable
to run those other loss

01:00:00.670 --> 01:00:01.450
leaders, right?

01:00:01.450 --> 01:00:04.495
It just might mean that the
shareholders took a haircut.

01:00:06.818 --> 01:00:09.110
And there are lots of formal
definitions of corruption,

01:00:09.110 --> 01:00:11.068
but one of the formal
definitions of corruption

01:00:11.068 --> 01:00:14.170
is when you have privatized
gains and socialized losses.

01:00:14.170 --> 01:00:15.640
It's cheap for me to pollute.

01:00:15.640 --> 01:00:17.598
It's expensive for you
to get the pollution out

01:00:17.598 --> 01:00:18.730
of your tap water.

01:00:18.730 --> 01:00:21.640
But that expense is
diffused across everyone

01:00:21.640 --> 01:00:23.800
who's putting filters
on their tap water,

01:00:23.800 --> 01:00:26.510
and the gains are
concentrated in my hands.

01:00:26.510 --> 01:00:33.820
Well, if the costs of
surveillance, which are real,

01:00:33.820 --> 01:00:37.600
are widely diffused and
the gains are concentrated,

01:00:37.600 --> 01:00:41.230
it may be that making
the firms internalize

01:00:41.230 --> 01:00:43.840
some of those costs, dial
down some of the other things

01:00:43.840 --> 01:00:45.050
that they can do.

01:00:45.050 --> 01:00:48.610
But it also reduces this
drag that the rest of us

01:00:48.610 --> 01:00:49.330
are feeling.

01:00:49.330 --> 01:00:53.170
So maybe with the surplus that
we gain from being lifted out

01:00:53.170 --> 01:00:55.720
of the costs of surveillance,
or market domination,

01:00:55.720 --> 01:00:59.170
or all the other things
that come as a result of it,

01:00:59.170 --> 01:01:03.250
that that surplus can
be allocated to make up

01:01:03.250 --> 01:01:04.660
for the losses that we get.

01:01:04.660 --> 01:01:08.830
It's totally true
that iPhone locked

01:01:08.830 --> 01:01:13.150
ecosystems allow us to
get some benefits that

01:01:13.150 --> 01:01:16.240
would be eroded if we unlocked
that ecosystem by force

01:01:16.240 --> 01:01:17.620
majeure.

01:01:17.620 --> 01:01:19.282
But I'm willing to
make that trade.

01:01:19.282 --> 01:01:20.740
It's totally true
that our printers

01:01:20.740 --> 01:01:23.320
are cheaper because our
inkjet cartridges are designed

01:01:23.320 --> 01:01:27.100
to charge us more than Veuve
Clicquot for water and tone,

01:01:27.100 --> 01:01:28.735
water and pigment, right?

01:01:28.735 --> 01:01:32.590
And I'm willing to roll
the dice on that one

01:01:32.590 --> 01:01:34.660
and find out what
happens if it turns out

01:01:34.660 --> 01:01:39.730
that we no longer
charge for carbon toner

01:01:39.730 --> 01:01:42.733
as though it were
plutonium toner.

01:01:42.733 --> 01:01:44.650
AUDIENCE: Do you have
an eyeball on your sock?

01:01:44.650 --> 01:01:45.360
I was trying to figure
out what that is.

01:01:45.360 --> 01:01:48.091
CORY DOCTOROW: Oh, no,
it's "Clockwork Orange."

01:01:48.091 --> 01:01:48.817
AUDIENCE: Oh, OK.

01:01:48.817 --> 01:01:49.900
CORY DOCTOROW: Yeah, yeah.

01:01:49.900 --> 01:01:51.960
We're at like peak
bookstore sock.

01:01:51.960 --> 01:01:53.960
So I don't know if you've
been into a bookstore,

01:01:53.960 --> 01:01:55.900
but like that's where
all the margins are now.

01:01:55.900 --> 01:01:58.922
You talk about ad subsidy,
the entire literary world

01:01:58.922 --> 01:02:00.880
is being subsidized by
the fact that we can now

01:02:00.880 --> 01:02:04.570
programmatically map bit maps on
to socks using weaving machines

01:02:04.570 --> 01:02:05.650
in China.

01:02:05.650 --> 01:02:08.648
And I went on a couple of book
tours in the last two years.

01:02:08.648 --> 01:02:09.940
I'm about to go on another one.

01:02:09.940 --> 01:02:11.950
I spend a lot of time
in indie bookstores,

01:02:11.950 --> 01:02:13.570
and I have all the socks.

01:02:13.570 --> 01:02:15.230
AUDIENCE: Are there
Doctorow socks?

01:02:15.230 --> 01:02:17.870
CORY DOCTOROW: No,
someone should make those.

01:02:17.870 --> 01:02:20.020
And then I can sue them
for my right of publicity.

01:02:20.020 --> 01:02:22.340
[LAUGHTER]

01:02:22.617 --> 01:02:23.450
AUDIENCE: Thank you.

01:02:23.450 --> 01:02:24.050
CORY DOCTOROW: Thank you.

01:02:24.050 --> 01:02:25.675
SPEAKER: And if you
like socks, there's

01:02:25.675 --> 01:02:29.200
a great sock store not far from
here near Angel City Books.

01:02:29.200 --> 01:02:32.632
CORY DOCTOROW: Oh, yeah, that's
in Venice Beach Sock District.

01:02:32.632 --> 01:02:34.895
SPEAKER: It's in
the old Elks lodge.

01:02:34.895 --> 01:02:35.728
CORY DOCTOROW: Yeah.

01:02:35.728 --> 01:02:36.460
SPEAKER: All right.

01:02:36.460 --> 01:02:37.210
Thanks again, Corey.

01:02:37.210 --> 01:02:38.377
CORY DOCTOROW: Thanks, guys.

01:02:38.377 --> 01:02:41.160
[APPLAUSE]

