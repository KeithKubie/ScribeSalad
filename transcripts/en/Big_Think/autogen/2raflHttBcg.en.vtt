WEBVTT
Kind: captions
Language: en

00:00:05.400 --> 00:00:12.710
So there’s a methodology called k-Nearest
Neighbor in big data analysis where you can

00:00:12.710 --> 00:00:15.950
find a person who looks similar to another
person.

00:00:15.950 --> 00:00:18.980
Who’s the most similar on a number of traits?

00:00:18.980 --> 00:00:22.980
But I kind of renamed the search a doppelganger
search because I think that’s a cooler name

00:00:22.980 --> 00:00:24.800
for it and also accurate.

00:00:24.800 --> 00:00:30.769
So you basically look in a huge data set,
you take a person and say “Who is the person

00:00:30.769 --> 00:00:33.430
who looks most similar to that person?”

00:00:33.430 --> 00:00:38.900
So one way you might use this is if Amazon’s
looking for what books to recommend.

00:00:38.900 --> 00:00:41.700
They may find your book-reading doppelganger.

00:00:41.700 --> 00:00:49.130
So across the whole universe of Amazon customers,
who’s the person who tends to buy books

00:00:49.130 --> 00:00:50.540
like you have bought?

00:00:50.540 --> 00:00:55.870
And then what books has that person recently
read and enjoyed that you haven’t read and

00:00:55.870 --> 00:00:56.870
enjoyed?

00:00:56.870 --> 00:01:03.670
And that’s sort of how they recommend books
to you.

00:01:03.670 --> 00:01:06.900
And this can be used in a lot of other areas.

00:01:06.900 --> 00:01:12.440
People are just starting to use this in health
where you can say, across the entire universe

00:01:12.440 --> 00:01:20.220
of patients who has symptoms very similar
to your symptoms, and what has worked for

00:01:20.220 --> 00:01:22.710
those people, are your health doppelgangers.

00:01:22.710 --> 00:01:27.170
So it’s a very powerful methodology and
it gets more powerful the more data you have.

00:01:27.170 --> 00:01:31.450
Because the more data you have the more similar,
the more likely you’re going to find someone

00:01:31.450 --> 00:01:33.640
in that data set who’s really, really similar
to you.

00:01:33.640 --> 00:01:37.050
Some of this stuff, some of the big data analysis
are things we have always kind of done.

00:01:37.050 --> 00:01:38.710
That’s kind of what doctors try to do.

00:01:38.710 --> 00:01:41.240
They try to say, “Who are you similar to?

00:01:41.240 --> 00:01:46.330
Of all the patients I’ve seen, which ones
remind me of your case, and what worked for

00:01:46.330 --> 00:01:47.330
them?”

00:01:47.330 --> 00:01:53.710
But they’ve been doing this on a small number
of patients, namely the ones they’ve seen.

00:01:53.710 --> 00:01:57.440
Whereas the potential for big data is you
can do it over the entire universe of patients

00:01:57.440 --> 00:02:00.840
and get people who are, really, much, much
more similar to you.

00:02:00.840 --> 00:02:04.791
Really zoom in on the tiny subset of people
who have a very similar path to you.

00:02:04.791 --> 00:02:11.819
Instead of saying “You have the condition
depression” which might remind a doctor

00:02:11.819 --> 00:02:17.569
of a hundred depressed patients that he’s
seen over the past couple of years, you can

00:02:17.569 --> 00:02:20.989
say maybe that “You have a particular type
of depression.”

00:02:20.989 --> 00:02:25.650
So you maybe sleep all the time whereas other
depressed patients don’t sleep all the time,

00:02:25.650 --> 00:02:29.599
and you feel guilty whereas other depressed
patients don’t feel guilty, and then really

00:02:29.599 --> 00:02:36.580
find these people who are really, really similar
who’s depression has taken a much more similar

00:02:36.580 --> 00:02:40.999
path to yours than have other people’s depressions.

