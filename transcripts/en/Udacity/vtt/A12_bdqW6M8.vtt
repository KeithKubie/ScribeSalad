WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:02.320
Let's formalize this a little bit.

00:00:02.320 --> 00:00:04.910
What we've been working with
is something called a Markov

00:00:04.910 --> 00:00:06.490
decision problem.

00:00:06.490 --> 00:00:09.050
And here's what makes up
a Markov decision problem.

00:00:10.120 --> 00:00:13.410
There are a set of states S.

00:00:13.410 --> 00:00:17.540
Those are all the values that this S
can take as it comes into the robot.

00:00:18.610 --> 00:00:20.970
There's a set of actions A,

00:00:20.970 --> 00:00:24.290
which is these potential actions we
can take to act on the environment.

00:00:25.680 --> 00:00:27.060
There's a transition function.

00:00:28.120 --> 00:00:31.740
This is the T within the environment.

00:00:31.740 --> 00:00:35.590
And this is a little bit complicated,
but let's just step through it.

00:00:35.590 --> 00:00:41.410
T is a three-dimensional object,
and it records in each of its cells

00:00:41.410 --> 00:00:45.560
the probability that if
we are in state S and

00:00:45.560 --> 00:00:50.900
we take action A,
we will end up in state S prime.

00:00:50.900 --> 00:00:56.730
Something to note about this transition
function is, suppose we're in state,

00:00:56.730 --> 00:01:00.170
a particular state S and
we take a particular action A.

00:01:01.430 --> 00:01:08.160
The sum of all the next states we
might end up in has to sum to one.

00:01:08.160 --> 00:01:10.880
In other words, with probability one,

00:01:10.880 --> 00:01:15.040
we're going to end up in some new state,
but the distribution of probabilities

00:01:15.040 --> 00:01:20.330
across these different states is what
makes this informative and revealing.

00:01:20.330 --> 00:01:24.040
Finally, an important component
of Markov decision problems

00:01:24.040 --> 00:01:25.690
is the reward function.

00:01:25.690 --> 00:01:27.220
And that's what gives us the reward.

00:01:27.220 --> 00:01:28.720
If we're in a particular state and

00:01:28.720 --> 00:01:32.570
we take an action A,
we get a particular reward.

00:01:32.570 --> 00:01:36.100
So if we have all of
these things defined,

00:01:36.100 --> 00:01:39.370
we have what's called
a Markov decision problem.

00:01:39.370 --> 00:01:43.250
Now, the problem for a reinforcement
learning algorithm is to find

00:01:43.250 --> 00:01:48.810
this policy pi that will
maximize reward over time.

00:01:48.810 --> 00:01:51.440
And, in fact,
if it finds the optimal policy,

00:01:51.440 --> 00:01:55.580
we give it a little symbol pi starred
to indicate that it's optimal.

00:01:56.770 --> 00:02:02.020
Now, if we have these, and,
in particular, if we have T and

00:02:02.020 --> 00:02:07.380
R, there are algorithms we can unleash
that will find this optimal policy.

00:02:07.380 --> 00:02:10.900
Two of them are policy iteration and
value iteration.

00:02:12.070 --> 00:02:17.700
Now, in this class, we don't
start off knowing T and R, and so

00:02:17.700 --> 00:02:22.110
we're not going to be able to use these
algorithms directly to find this policy.

