WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.412
[MUSIC PLAYING]

00:00:05.412 --> 00:00:07.729
[APPLAUSE]

00:00:07.729 --> 00:00:10.020
MOLLY WRIGHT STEENSON: So
hello to friends in the room,

00:00:10.020 --> 00:00:12.720
friends who I know are
joining us virtually.

00:00:12.720 --> 00:00:14.820
I'm really excited
that you are here.

00:00:14.820 --> 00:00:16.484
And thank you,
Justin, for having me.

00:00:16.484 --> 00:00:17.400
It's great to be here.

00:00:20.330 --> 00:00:23.630
This is part of a big bunch
of research that I started,

00:00:23.630 --> 00:00:26.550
some way or another, 13 years
ago, in some way or another,

00:00:26.550 --> 00:00:28.940
in 1996, even.

00:00:28.940 --> 00:00:31.610
And I want to start by making
the claim that architecture,

00:00:31.610 --> 00:00:35.780
AI, and design
are all connected.

00:00:35.780 --> 00:00:38.540
These connections go
back some 60 or 70 years,

00:00:38.540 --> 00:00:42.170
to 1956 or even 1948.

00:00:42.170 --> 00:00:45.230
And newness seems to be
central to how we talk about

00:00:45.230 --> 00:00:48.460
artificial intelligence today.

00:00:48.460 --> 00:00:57.240
We say things like, AI is the
new black, AI is the new UI.

00:00:57.240 --> 00:01:00.000
Artificial intelligence is
the next digital frontier,

00:01:00.000 --> 00:01:03.030
according to our
friends at McKinsey.

00:01:03.030 --> 00:01:07.360
Andrew Ng stated that AI
is the new electricity.

00:01:07.360 --> 00:01:10.410
But also, AI is the
new MOOC, because it

00:01:10.410 --> 00:01:14.340
will be an online course
that he's running.

00:01:14.340 --> 00:01:19.840
You need data for AI,
so data is the new oil.

00:01:19.840 --> 00:01:22.490
The future of computers is the
mind of a disembodied toddler

00:01:22.490 --> 00:01:22.990
head.

00:01:26.110 --> 00:01:29.380
And apparently, I
am also the new AI,

00:01:29.380 --> 00:01:32.985
and I will spare
you what this says,

00:01:32.985 --> 00:01:36.081
but it's really, really
sexist and really awful.

00:01:36.081 --> 00:01:38.080
If you want to get an
idea of how much the word,

00:01:38.080 --> 00:01:39.820
new gets used when
we talk about AI,

00:01:39.820 --> 00:01:42.580
you could look at something like
this "MIT Technology Review"

00:01:42.580 --> 00:01:43.630
article.

00:01:43.630 --> 00:01:47.250
I just highlighted the word,
new where I could find it,

00:01:47.250 --> 00:01:50.164
and there were six mentions
in the first two paragraphs.

00:01:50.164 --> 00:01:51.580
Of course, I think
one of them was

00:01:51.580 --> 00:01:55.720
New York, so I don't think
that that actually counts.

00:01:55.720 --> 00:01:58.510
And this gentleman was
a part of a conversation

00:01:58.510 --> 00:02:02.430
about tech moguls declaring an
era of artificial intelligence.

00:02:05.150 --> 00:02:09.500
But AI isn't the new,
because AI isn't new.

00:02:09.500 --> 00:02:10.520
Here's how new it's not.

00:02:10.520 --> 00:02:15.860
In 1955, John McCarthy, the
founder of the Stanford AI

00:02:15.860 --> 00:02:20.686
Lab, and before
that, the MIT AI Lab,

00:02:20.686 --> 00:02:22.310
coined the term
artificial intelligence

00:02:22.310 --> 00:02:24.620
to mean making machines do
things that would require

00:02:24.620 --> 00:02:27.890
intelligence if done by man.

00:02:27.890 --> 00:02:31.190
In 1956, he brought together
a group of researchers

00:02:31.190 --> 00:02:33.590
at Dartmouth College
in New Hampshire,

00:02:33.590 --> 00:02:35.390
and they spent the
summer hashing out

00:02:35.390 --> 00:02:38.690
what they thought the platform
of research on AI should be.

00:02:38.690 --> 00:02:43.160
And it included things like
neural nets, and it included--

00:02:43.160 --> 00:02:44.000
what does he say.

00:02:44.000 --> 00:02:46.520
It says, an attempt
will be made to find

00:02:46.520 --> 00:02:48.290
how to make machines
use language,

00:02:48.290 --> 00:02:50.680
form abstractions
and concepts, solve

00:02:50.680 --> 00:02:52.700
kinds of problems now
reserved for humans,

00:02:52.700 --> 00:02:54.170
and improve themselves.

00:02:54.170 --> 00:02:56.007
We think that a
significant advance

00:02:56.007 --> 00:02:57.840
can be made in one or
more of these problems

00:02:57.840 --> 00:02:59.780
if a carefully selected
group of scientists

00:02:59.780 --> 00:03:04.820
can work together on
it for a summer, 1956.

00:03:04.820 --> 00:03:07.850
Herb Simon, Allen
Newell, and JC Shaw--

00:03:07.850 --> 00:03:11.450
Simon and Newell, very important
to Carnegie Mellon's history,

00:03:11.450 --> 00:03:14.270
wrote in 1958, that they'd
pretty much figured out

00:03:14.270 --> 00:03:18.230
this problem of modeling the
human brain in computing.

00:03:18.230 --> 00:03:20.210
They said, "intuition,
insight, and learning

00:03:20.210 --> 00:03:22.640
are no longer exclusive
possessions of humans.

00:03:22.640 --> 00:03:25.160
Any large scale,
high-speed computer

00:03:25.160 --> 00:03:27.680
can be made to
exhibit them, also."

00:03:27.680 --> 00:03:33.220
They believed that by the 1960s,
they'd have it figured out.

00:03:33.220 --> 00:03:35.861
And of course, Marvin
Minsky, who in 1961 said,

00:03:35.861 --> 00:03:38.110
"I believe that we are on
the threshold of an era that

00:03:38.110 --> 00:03:40.526
will be strongly influenced,
and quite possibly dominated,

00:03:40.526 --> 00:03:45.100
by intelligent
problem-solving machines."

00:03:45.100 --> 00:03:47.500
Proto-machine
learning, you might

00:03:47.500 --> 00:03:51.910
find in the world of
perceptrons in 1958.

00:03:51.910 --> 00:03:54.310
"The New York Times"
published an article

00:03:54.310 --> 00:03:57.910
talking about this new Navy
device that learns by doing

00:03:57.910 --> 00:03:59.380
and said that,
"the Navy revealed

00:03:59.380 --> 00:04:01.960
the embryo of an electronic
computer today that it expects

00:04:01.960 --> 00:04:05.530
will be able to walk, talk, see,
write, reproduce itself and be

00:04:05.530 --> 00:04:07.744
conscious of its existence.

00:04:07.744 --> 00:04:09.160
The Navy said the
perceptron would

00:04:09.160 --> 00:04:12.100
be the first non-living
mechanism capable of receiving,

00:04:12.100 --> 00:04:14.200
recognizing, and
identifying its surroundings

00:04:14.200 --> 00:04:18.360
without any human
training or control."

00:04:18.360 --> 00:04:21.630
So all of these
ideas are really old.

00:04:21.630 --> 00:04:24.300
They're older than me by
far, and they're older

00:04:24.300 --> 00:04:26.607
than a lot of what we work on.

00:04:26.607 --> 00:04:28.440
But in addition to this,
I want to point out

00:04:28.440 --> 00:04:31.380
that AI and architecture
are old friends

00:04:31.380 --> 00:04:34.140
and co-created each other
at a certain point in time,

00:04:34.140 --> 00:04:37.110
in ways that has
very direct impacts

00:04:37.110 --> 00:04:39.990
on the kind of engineering and
design work that we do today

00:04:39.990 --> 00:04:42.900
and the problems
that we focus on.

00:04:42.900 --> 00:04:44.850
I'm going to introduce
three of the characters

00:04:44.850 --> 00:04:49.410
in my book, "Architectural
Intelligence."

00:04:49.410 --> 00:04:51.690
The three people I'm going
to introduce in this talk

00:04:51.690 --> 00:04:54.300
are Christopher
Alexander, Cedric Price,

00:04:54.300 --> 00:04:56.370
and Nicholas Negroponte.

00:04:56.370 --> 00:05:00.220
Who here has heard of
Christopher Alexander?

00:05:00.220 --> 00:05:01.710
A couple.

00:05:01.710 --> 00:05:03.465
Anyone Cedric Price?

00:05:03.465 --> 00:05:05.160
Right on.

00:05:05.160 --> 00:05:06.985
And Nicholas Negroponte?

00:05:06.985 --> 00:05:08.770
A couple more.

00:05:08.770 --> 00:05:12.730
So they're all architects
who worked very closely

00:05:12.730 --> 00:05:15.280
with technology in different
kinds of ways, some

00:05:15.280 --> 00:05:16.900
so much so that
you don't probably

00:05:16.900 --> 00:05:19.930
think of Nicholas Negroponte,
the founder of the MIT Media

00:05:19.930 --> 00:05:21.460
Lab, as an architect.

00:05:21.460 --> 00:05:22.960
And I'm going to
talk about the ways

00:05:22.960 --> 00:05:24.460
that they worked
with cybernetics

00:05:24.460 --> 00:05:27.010
and artificial intelligence
to build new kinds of worlds

00:05:27.010 --> 00:05:28.540
and new kinds of
approaches in ways

00:05:28.540 --> 00:05:31.160
that affect what we do today.

00:05:31.160 --> 00:05:34.370
I'll start with
Christopher Alexander.

00:05:34.370 --> 00:05:38.060
Christopher Alexander-- if you
do know of his work and even

00:05:38.060 --> 00:05:41.390
if you don't-- by me putting
up this book right here,

00:05:41.390 --> 00:05:43.970
I've probably begun to bring
him into something you've heard

00:05:43.970 --> 00:05:45.050
of before.

00:05:45.050 --> 00:05:46.460
He's written many, many books.

00:05:46.460 --> 00:05:50.540
He's a mathematician, born
in Vienna, raised in England,

00:05:50.540 --> 00:05:52.970
came to the United
States in the 1960s.

00:05:52.970 --> 00:05:55.589
He's quite old but still alive,
living in England, again.

00:05:55.589 --> 00:05:56.880
He was a professor at Berkeley.

00:05:59.390 --> 00:06:01.490
I'll mention three
books today, "Notes

00:06:01.490 --> 00:06:04.082
on the Synthesis of Form,"
"A Pattern Language,"

00:06:04.082 --> 00:06:05.540
and "The Timeless
Way of Building."

00:06:05.540 --> 00:06:08.510
"Notes on the Synthesis
of Form" was his thesis.

00:06:08.510 --> 00:06:10.400
And if you've ever
heard the idea of design

00:06:10.400 --> 00:06:17.630
being good fit or fitness,
trying to shim a table until it

00:06:17.630 --> 00:06:20.630
works, or putting a
stable system in place,

00:06:20.630 --> 00:06:23.060
that's where that
idea comes from.

00:06:23.060 --> 00:06:24.680
Those of you who've
used patterns--

00:06:24.680 --> 00:06:28.100
whether in UX or in software--

00:06:28.100 --> 00:06:31.280
are right in the lineage
of Christopher Alexander.

00:06:31.280 --> 00:06:34.250
His idea of pattern languages
have influenced this and then

00:06:34.250 --> 00:06:37.040
his philosophy of design, which
he outlined in "The Timeless

00:06:37.040 --> 00:06:38.150
Way of Building."

00:06:38.150 --> 00:06:41.750
If you've ever used a wiki
or used Agile processes,

00:06:41.750 --> 00:06:45.680
you have used something
that he's done.

00:06:45.680 --> 00:06:47.810
My own research with
Alexander started

00:06:47.810 --> 00:06:50.690
when I was reading "Notes
on the Synthesis of Form,"

00:06:50.690 --> 00:06:54.350
and this question of AI came up,
when I realized, in a footnote,

00:06:54.350 --> 00:06:58.700
he was referring to
Claude Shannon, John

00:06:58.700 --> 00:07:00.160
McCarthy, and Marvin Minsky.

00:07:00.160 --> 00:07:02.751
And I wondered, why
is this architect--

00:07:02.751 --> 00:07:05.000
what does he have to say
about artificial intelligence

00:07:05.000 --> 00:07:08.150
in 1964?

00:07:08.150 --> 00:07:11.795
13 years later, I have
a book about that.

00:07:11.795 --> 00:07:13.170
He was interested
in finding ways

00:07:13.170 --> 00:07:15.420
to break down design problems,
so they could be easily

00:07:15.420 --> 00:07:17.340
understood and designed.

00:07:17.340 --> 00:07:20.820
Anybody who has created
tree diagrams in order

00:07:20.820 --> 00:07:24.900
to explain the function of a
piece of software or website

00:07:24.900 --> 00:07:27.960
is working in this lineage.

00:07:27.960 --> 00:07:30.960
In fact, this is the outputs
of one of his programs.

00:07:30.960 --> 00:07:34.080
This is something that's in
the Berkeley archives that

00:07:34.080 --> 00:07:37.260
shows how you'd break down the
elements of a design problem

00:07:37.260 --> 00:07:40.430
for how you design highways.

00:07:40.430 --> 00:07:43.770
He also applied these ideas
to the BART system in 1964.

00:07:43.770 --> 00:07:49.140
This is 390 requirements that he
and his team outlined, and this

00:07:49.140 --> 00:07:52.880
was before BART was built.
I'm not sure how many of these

00:07:52.880 --> 00:07:54.110
are in place today.

00:07:54.110 --> 00:07:56.090
I think we probably
need new requirements.

00:07:56.090 --> 00:07:58.040
My ears are still ringing
from the BART ride

00:07:58.040 --> 00:08:01.125
I took this morning.

00:08:01.125 --> 00:08:03.340
"Pattern Language" and
"Timeless Way of Building,"

00:08:03.340 --> 00:08:05.020
I already mentioned
briefly, but I

00:08:05.020 --> 00:08:08.015
want to tell you a little
bit more about them.

00:08:08.015 --> 00:08:09.640
"The Pattern Language,"
as he puts it--

00:08:09.640 --> 00:08:12.800
and it's not the only,
it's a pattern language.

00:08:12.800 --> 00:08:16.055
It's 253 patterns from
large to small scale.

00:08:16.055 --> 00:08:17.680
And then the "Timeless
Way of Building"

00:08:17.680 --> 00:08:19.250
is a philosophy of patterns.

00:08:19.250 --> 00:08:21.580
And these patterns
are anything from how

00:08:21.580 --> 00:08:25.920
to make a nation state, down to
how to organize your bedroom.

00:08:25.920 --> 00:08:30.450
And as he and his
collaborators write,

00:08:30.450 --> 00:08:31.930
"Each pattern
describes a problem

00:08:31.930 --> 00:08:34.096
which occurs over and over
again in our environment,

00:08:34.096 --> 00:08:36.750
and then describes the core of
the solution to that problem,

00:08:36.750 --> 00:08:39.419
in such a way that you can use
this solution a million times

00:08:39.419 --> 00:08:42.820
over, without ever doing
it the same way twice."

00:08:42.820 --> 00:08:45.400
So in the book, here's
an example of a pattern.

00:08:45.400 --> 00:08:47.820
I live in Pittsburgh, and
I live in a row house.

00:08:47.820 --> 00:08:50.770
And so it's very
dense where I live,

00:08:50.770 --> 00:08:53.360
and these are patterns about
the design of a row house.

00:08:53.360 --> 00:08:56.730
You see, a number, a
statement, a picture--

00:08:56.730 --> 00:08:58.620
a statement about
the design problem,

00:08:58.620 --> 00:09:02.280
illustrations to carry out--
to explain some of what that

00:09:02.280 --> 00:09:03.960
design problem is doing--

00:09:03.960 --> 00:09:07.380
and then a connection to
other patterns in the system.

00:09:07.380 --> 00:09:09.480
And it works like
an operating system.

00:09:09.480 --> 00:09:11.370
The patterns are a network.

00:09:11.370 --> 00:09:14.550
So you see, here are a number
of them in their order,

00:09:14.550 --> 00:09:18.540
explaining what each of them do.

00:09:18.540 --> 00:09:21.420
This is going from a mosaic of
subcultures, all the way down

00:09:21.420 --> 00:09:25.380
to a household mix, and it
goes further and further

00:09:25.380 --> 00:09:26.150
into more detail.

00:09:30.150 --> 00:09:34.670
Alexander is someone that a
lot of technologists have read.

00:09:34.670 --> 00:09:38.580
If you know Allen Cooper,
the interaction designer,

00:09:38.580 --> 00:09:41.190
he use to ferret out
Christopher Alexander's

00:09:41.190 --> 00:09:44.460
books in his high school
library and read them there.

00:09:44.460 --> 00:09:47.320
He wanted to be an
architect, so did Kent Beck

00:09:47.320 --> 00:09:49.860
and then realized, he didn't
want to build buildings.

00:09:49.860 --> 00:09:51.510
He wanted to build
software, and there

00:09:51.510 --> 00:09:55.500
was a lot more that you
could do within that world.

00:09:55.500 --> 00:09:59.285
And Ward Cunningham
and Kent Beck

00:09:59.285 --> 00:10:00.660
were curious about
how they could

00:10:00.660 --> 00:10:03.062
apply this idea of
patterns to the idea

00:10:03.062 --> 00:10:04.770
of object-oriented
programming languages,

00:10:04.770 --> 00:10:08.400
so they applied it to
interface patterns in Smalltalk

00:10:08.400 --> 00:10:11.130
in the late '80s and about 1987.

00:10:11.130 --> 00:10:14.250
And a community of programmers
got together and over

00:10:14.250 --> 00:10:18.640
the years, built up what was
called the design patterns

00:10:18.640 --> 00:10:19.140
movement.

00:10:19.140 --> 00:10:22.170
And this group of four
men, who called themselves

00:10:22.170 --> 00:10:24.360
the gang of four,
are the ones who

00:10:24.360 --> 00:10:28.590
created this network of
design patterns in software.

00:10:28.590 --> 00:10:32.490
And it's an idea that you see
millions and millions of times.

00:10:32.490 --> 00:10:34.860
I think they're something
like $1,200 on Amazon

00:10:34.860 --> 00:10:39.720
that refer to design patterns
in software and in interfaces

00:10:39.720 --> 00:10:41.970
and games.

00:10:41.970 --> 00:10:45.180
Now, how many people have
heard of design patterns?

00:10:45.180 --> 00:10:47.550
There you go.

00:10:47.550 --> 00:10:52.080
Also, if you've ever used
Agile processes or followed

00:10:52.080 --> 00:10:56.820
extreme programming, Kent
Beck and Ward Cunningham

00:10:56.820 --> 00:10:59.400
and a number of other
people applied these ideas

00:10:59.400 --> 00:11:03.702
about the philosophy of design
from Christopher Alexander

00:11:03.702 --> 00:11:05.160
and "The Timeless
Way of Building,"

00:11:05.160 --> 00:11:09.330
to up end and change the
politics of the design process

00:11:09.330 --> 00:11:12.155
and to give more flexibility
to users and to people.

00:11:12.155 --> 00:11:13.530
And when I
interviewed Kent Beck,

00:11:13.530 --> 00:11:14.940
he told me that it
was "a rearrangement

00:11:14.940 --> 00:11:17.398
of the political power in the
design and building process."

00:11:19.840 --> 00:11:23.350
Everyone here has used a
wiki, and the wiki format

00:11:23.350 --> 00:11:25.330
was developed by
Ward Cunningham.

00:11:25.330 --> 00:11:28.480
He was developing it in
HyperCard in the early '90s,

00:11:28.480 --> 00:11:32.080
and then it was suggested
that he could maybe

00:11:32.080 --> 00:11:34.500
build this on this new thing
called the World Wide Web.

00:11:34.500 --> 00:11:37.810
And what he wanted to do
was have a conversation

00:11:37.810 --> 00:11:39.010
that never had an end.

00:11:39.010 --> 00:11:43.180
If he wanted to map all of the
knowledge that he and his team

00:11:43.180 --> 00:11:45.970
had, he didn't want
to say, it ends here.

00:11:45.970 --> 00:11:48.310
He wanted it to be able
to go and go and go.

00:11:48.310 --> 00:11:51.730
He never patented
the wiki format,

00:11:51.730 --> 00:11:56.250
and it was picked up in the
early 2000s to run Wikipedia.

00:11:56.250 --> 00:11:59.170
And now, we've all used
wikis one way or another.

00:11:59.170 --> 00:12:01.990
But this is, again,
a direct inspiration

00:12:01.990 --> 00:12:06.460
or a direct influenced by
Christopher Alexander project.

00:12:09.300 --> 00:12:10.710
So that's Christopher Alexander.

00:12:10.710 --> 00:12:13.710
The other thing to say about him
is architects tend to hate him,

00:12:13.710 --> 00:12:15.660
and technologists
tend to love him.

00:12:15.660 --> 00:12:16.920
I'm not sure why.

00:12:16.920 --> 00:12:19.000
Conundrum, but kind
of interesting.

00:12:19.000 --> 00:12:21.583
So I'd like to introduce you to
a different kind of architect.

00:12:21.583 --> 00:12:23.580
This is Cedric Price.

00:12:23.580 --> 00:12:25.020
I've called him
the secret patron

00:12:25.020 --> 00:12:26.730
saint of interaction designers.

00:12:26.730 --> 00:12:30.990
He changed the way any
number of people in the UK

00:12:30.990 --> 00:12:33.000
understood architecture
and buildings

00:12:33.000 --> 00:12:34.740
and what they could do.

00:12:34.740 --> 00:12:38.160
His life partner
was Eleanor Bron.

00:12:38.160 --> 00:12:39.360
He died in 2003.

00:12:39.360 --> 00:12:42.150
Eleanor Bron was in
the Beatles movies

00:12:42.150 --> 00:12:43.890
and was responsible--
actually, she

00:12:43.890 --> 00:12:46.890
was the inspiration for
the song "Eleanor Rigby."

00:12:46.890 --> 00:12:49.740
So just a very
curious and very funny

00:12:49.740 --> 00:12:53.157
person who liked to ask things
like, technology is the answer,

00:12:53.157 --> 00:12:54.240
but what was the question?

00:12:57.660 --> 00:13:00.930
And Cedric Price was
best known for things

00:13:00.930 --> 00:13:03.750
that were never built. So
for instance, the Fun Palace

00:13:03.750 --> 00:13:06.180
was designed in
the 1960s, and it

00:13:06.180 --> 00:13:09.710
was supposed to be a
big cybernetic theater,

00:13:09.710 --> 00:13:13.380
movable kind of place, where all
sorts of things could happen.

00:13:13.380 --> 00:13:17.700
And the space was going to
learn from its users over time

00:13:17.700 --> 00:13:19.650
and adapt to their
interests and needs.

00:13:19.650 --> 00:13:21.450
And you could
change it over time

00:13:21.450 --> 00:13:23.910
and use it kind of
as a big leisure

00:13:23.910 --> 00:13:26.490
center to learn and
experience whatever

00:13:26.490 --> 00:13:28.200
it is you might want to learn.

00:13:28.200 --> 00:13:30.690
And he worked with a
25 member, 27 member

00:13:30.690 --> 00:13:34.090
cybernetic community, which is
what you see here on the right.

00:13:34.090 --> 00:13:37.350
And in this particular image,
it's kind of hard to read,

00:13:37.350 --> 00:13:42.290
but it talks about input
of unmodified people

00:13:42.290 --> 00:13:44.490
and then output of
modified people.

00:13:44.490 --> 00:13:47.642
So they thought through
this like a system diagram.

00:13:47.642 --> 00:13:49.350
He did this project
with Joan Littlewood,

00:13:49.350 --> 00:13:51.000
who was a radical
theater director

00:13:51.000 --> 00:13:55.770
and a protege of the German
playwright, Bertolt Brecht.

00:13:55.770 --> 00:13:58.710
He also was really
inspired by Gordon Pask.

00:13:58.710 --> 00:14:01.290
And Gordon Park is
a cyberneticist,

00:14:01.290 --> 00:14:06.840
who also had a big influence
on a lot of architects.

00:14:06.840 --> 00:14:09.450
And a piece that
he wrote in 1969

00:14:09.450 --> 00:14:14.250
talked about changing
this design process

00:14:14.250 --> 00:14:15.570
in some interesting ways.

00:14:18.610 --> 00:14:21.820
And he suggested that we
"turn the design paradigm

00:14:21.820 --> 00:14:23.290
in upon itself and
let us apply it

00:14:23.290 --> 00:14:25.539
to the interaction between
the designer and the system

00:14:25.539 --> 00:14:28.240
she designs, rather than the
intersection between the system

00:14:28.240 --> 00:14:29.920
and the people who inhabit it."

00:14:29.920 --> 00:14:33.100
So rather than the system you
designing doing your bidding,

00:14:33.100 --> 00:14:35.050
maybe it does
something different.

00:14:35.050 --> 00:14:39.370
Maybe it augments and creates
something different than what

00:14:39.370 --> 00:14:42.550
the two people put together.

00:14:42.550 --> 00:14:46.300
This is a drawing he gave
of second-order cybernetics.

00:14:46.300 --> 00:14:49.120
He wrote things like,
"assume for the moment I'm

00:14:49.120 --> 00:14:51.880
the successful businessman
with the bowler hat,

00:14:51.880 --> 00:14:54.550
and I insist that I'm the sole
reality where everything else

00:14:54.550 --> 00:14:56.830
appears only in my imagination.

00:14:56.830 --> 00:14:58.720
I cannot deny that
in my imagination,

00:14:58.720 --> 00:15:00.920
there will appear other
people, scientists,

00:15:00.920 --> 00:15:03.040
other successful
businessmen, et cetera,

00:15:03.040 --> 00:15:06.670
as for instance in
this very conference.

00:15:06.670 --> 00:15:09.869
Since I find these apparitions
in many ways similar to myself,

00:15:09.869 --> 00:15:12.160
I have to grant them the
privilege that they themselves

00:15:12.160 --> 00:15:13.910
may insist that they
are the sole reality,

00:15:13.910 --> 00:15:16.339
and everything else is a
condition of their imagination.

00:15:16.339 --> 00:15:18.880
On the other hand, they cannot
deny that their fantasies will

00:15:18.880 --> 00:15:21.320
be populated by people,
and one of them may be I,

00:15:21.320 --> 00:15:23.410
with bowler hat and everything."

00:15:23.410 --> 00:15:25.204
So this is second-order
cybernetics

00:15:25.204 --> 00:15:27.370
where the fact that you're
engaging with the system,

00:15:27.370 --> 00:15:28.900
changes the system.

00:15:28.900 --> 00:15:30.820
And if you start looking
at the design process

00:15:30.820 --> 00:15:33.220
as something that needs to
take that into consideration,

00:15:33.220 --> 00:15:36.910
design begins to look
a little bit different.

00:15:36.910 --> 00:15:40.240
Cedric Price was, again,
very, very inspired

00:15:40.240 --> 00:15:42.610
by his interactions
and collaborations

00:15:42.610 --> 00:15:45.550
with Gordon Pask.

00:15:45.550 --> 00:15:47.080
One of the projects
that he designed

00:15:47.080 --> 00:15:48.730
was something called Generator.

00:15:48.730 --> 00:15:51.400
This was never built,
but it was a project

00:15:51.400 --> 00:15:54.980
that was in existence
from '76 to '79.

00:15:54.980 --> 00:15:59.050
And it was a set of 12 foot
cubes, 150 cubes, walkways,

00:15:59.050 --> 00:16:04.720
boardwalks, that could be
moved around and recombined

00:16:04.720 --> 00:16:06.730
for whatever purposes
someone might like.

00:16:06.730 --> 00:16:09.550
This was an arts retreat
center for a wealthy patron

00:16:09.550 --> 00:16:11.010
in Florida.

00:16:11.010 --> 00:16:13.420
He always really
liked moving cranes,

00:16:13.420 --> 00:16:15.520
so there's always a
moving crane here.

00:16:15.520 --> 00:16:18.160
And Generator would
come together like this.

00:16:18.160 --> 00:16:19.730
There was grit on the ground.

00:16:19.730 --> 00:16:24.260
He put cubes over it, roofs
on top, connect the parts.

00:16:24.260 --> 00:16:27.760
And then you could put up the
side baffles and the stairways

00:16:27.760 --> 00:16:30.220
and do what you'd
like with the space.

00:16:30.220 --> 00:16:33.870
You could also model it with
these little qubit parts

00:16:33.870 --> 00:16:34.994
that you would see.

00:16:34.994 --> 00:16:36.910
He thought that maybe
fun things might happen,

00:16:36.910 --> 00:16:38.470
like you could take
a bleeper walk.

00:16:38.470 --> 00:16:42.585
I will remind you, this is 1977
or so when this idea came up.

00:16:42.585 --> 00:16:44.710
And there's something
strange about a mouse rolling

00:16:44.710 --> 00:16:46.070
around a cube.

00:16:46.070 --> 00:16:51.020
But it's strange and weird
and funny all the time.

00:16:51.020 --> 00:16:53.620
And again, we look
at Gordon Pask.

00:16:53.620 --> 00:16:56.950
Gordon Pask did a project
first in 1953 and then

00:16:56.950 --> 00:16:58.960
in 1968 called--

00:16:58.960 --> 00:17:01.510
which became The
Colloquy of Mobiles.

00:17:01.510 --> 00:17:03.430
And what happens with
these mobiles are

00:17:03.430 --> 00:17:08.230
you interact with it, and
it interacts back with you

00:17:08.230 --> 00:17:09.670
until it gets bored.

00:17:09.670 --> 00:17:14.230
And Paul Pangaro, who is a
cyberneticist and a professor

00:17:14.230 --> 00:17:16.780
at the College for Creative
Studies in Detroit,

00:17:16.780 --> 00:17:21.490
has just rebuilt The
Colloquy of Mobiles

00:17:21.490 --> 00:17:23.740
for the 50th anniversary
of the original project.

00:17:23.740 --> 00:17:25.930
He's Gordon Pask's archivist.

00:17:25.930 --> 00:17:30.860
And it's up right now,
as we speak, in Detroit.

00:17:30.860 --> 00:17:34.499
So what does this have
to do with Generator?

00:17:34.499 --> 00:17:36.290
Well, Cedric Price
realized it was unlikely

00:17:36.290 --> 00:17:38.873
that people were going to want
to move around Generator enough

00:17:38.873 --> 00:17:41.155
and learn from it and do
the kind of funny things

00:17:41.155 --> 00:17:42.030
that it would expect.

00:17:42.030 --> 00:17:44.810
So he began to work with John
Frazier and Julia Frazier, who

00:17:44.810 --> 00:17:47.570
were computer scientists
as well as architects,

00:17:47.570 --> 00:17:52.550
and wanted to put together
a set of programs that

00:17:52.550 --> 00:17:54.530
would be used by Generator.

00:17:54.530 --> 00:17:56.990
So there would be
micro-controllers on all of its

00:17:56.990 --> 00:18:00.560
parts, an inventory
program, a design program--

00:18:00.560 --> 00:18:03.050
that's what you see here, you
can pick up and move these

00:18:03.050 --> 00:18:06.300
cubes, and they
plot and print out--

00:18:06.300 --> 00:18:08.240
and a boredom program.

00:18:08.240 --> 00:18:11.520
And the boredom program
said the following.

00:18:11.520 --> 00:18:14.270
In the event of the site not
being reorganized or changed

00:18:14.270 --> 00:18:15.890
for some time, the
computer starts

00:18:15.890 --> 00:18:18.620
generating unsolicited
plans and improvements.

00:18:21.320 --> 00:18:24.010
And this is something that John
Frazier wrote in his letter

00:18:24.010 --> 00:18:25.870
to Cedric Price,
introducing these programs,

00:18:25.870 --> 00:18:28.570
"If you kick a system, the very
least you would expect it to do

00:18:28.570 --> 00:18:30.920
is kick you back."

00:18:30.920 --> 00:18:33.670
And then he said, "You
seemed to imply"--

00:18:33.670 --> 00:18:36.090
this is a handwritten footnote
that you can see down here

00:18:36.090 --> 00:18:36.715
in the letter--

00:18:36.715 --> 00:18:38.090
"You seemed to
imply that we were

00:18:38.090 --> 00:18:39.790
only useful if we
produced results

00:18:39.790 --> 00:18:41.350
that you did not expect.

00:18:41.350 --> 00:18:44.280
I think this leads
to some definition

00:18:44.280 --> 00:18:46.066
of computer aids in general.

00:18:46.066 --> 00:18:47.440
At least one thing
that you would

00:18:47.440 --> 00:18:49.660
expect from any
half decent program

00:18:49.660 --> 00:18:52.210
is that it should produce
at least one plan which

00:18:52.210 --> 00:18:54.530
you did not expect."

00:18:54.530 --> 00:18:57.190
So this is where something
like Gordon Pask's Colloquy

00:18:57.190 --> 00:19:01.960
of Mobiles makes its way into
Cedric Price's architecture.

00:19:01.960 --> 00:19:04.030
And these are ideas
that we pick up today

00:19:04.030 --> 00:19:06.010
in some really lovely ways.

00:19:06.010 --> 00:19:08.650
On the next slide, I'm
going to introduce a project

00:19:08.650 --> 00:19:11.230
by the roboticist,
Madeline Gannon.

00:19:11.230 --> 00:19:14.500
She just finished her
PhD at Carnegie Mellon

00:19:14.500 --> 00:19:18.610
in architecture, and she did an
exhibition at The London Design

00:19:18.610 --> 00:19:25.420
Museum with a robot, a robotic
arm that would play with its--

00:19:25.420 --> 00:19:28.120
play with somebody in its
midst until it got bored

00:19:28.120 --> 00:19:28.840
and went away.

00:19:28.840 --> 00:19:29.506
[VIDEO PLAYBACK]

00:19:29.506 --> 00:19:31.150
Everything comes
together and you're

00:19:31.150 --> 00:19:33.160
in the space with a
robot and you just

00:19:33.160 --> 00:19:37.720
have a very raw experience
with this animal-like machine

00:19:37.720 --> 00:19:40.150
responding to your every move.

00:19:40.150 --> 00:19:44.740
All the technical aspects
melt away into the background.

00:19:44.740 --> 00:19:49.150
It's incredibly important to
have opportunities and spaces

00:19:49.150 --> 00:19:53.919
to come in and experiment
and misuse these existing

00:19:53.919 --> 00:19:54.460
technologies.

00:19:54.460 --> 00:19:56.350
[END PLAYBACK]

00:19:56.350 --> 00:19:59.020
She didn't know about
Gordon Pask or Cedric Price

00:19:59.020 --> 00:20:00.670
when she did this project.

00:20:00.670 --> 00:20:04.030
But I find it compelling
that many years later, this

00:20:04.030 --> 00:20:05.770
is exactly what
she is trying to do

00:20:05.770 --> 00:20:08.710
and trying to get us to
rethink, our relationship

00:20:08.710 --> 00:20:10.060
with our machines.

00:20:10.060 --> 00:20:14.632
She calls herself
the robot whisperer.

00:20:14.632 --> 00:20:16.420
The third person I'd
like to talk about

00:20:16.420 --> 00:20:18.660
is Nicholas Negroponte.

00:20:18.660 --> 00:20:20.620
And you'll notice
here, he always

00:20:20.620 --> 00:20:22.750
seems to have his
fingers in the picture.

00:20:22.750 --> 00:20:26.710
He always seems to be
gesturing at something.

00:20:26.710 --> 00:20:29.840
You would probably know him as
the founder of the MIT Media

00:20:29.840 --> 00:20:30.340
Lab.

00:20:30.340 --> 00:20:33.580
But before the Media
Lab, he ran something

00:20:33.580 --> 00:20:36.610
called the Architecture
Machine Group at MIT.

00:20:36.610 --> 00:20:39.430
And that's what I want
to talk about here.

00:20:39.430 --> 00:20:41.290
The MIT Architecture
Machine Group

00:20:41.290 --> 00:20:43.090
was in the School
of Architecture

00:20:43.090 --> 00:20:45.460
but worked very
closely with the AI Lab

00:20:45.460 --> 00:20:49.020
and developed interfaces,
programs, and environments

00:20:49.020 --> 00:20:53.800
for artificial intelligence.

00:20:53.800 --> 00:20:58.365
The gesture thing, he's the
author of a number of books.

00:20:58.365 --> 00:21:00.490
The book that you'd probably
be most likely to know

00:21:00.490 --> 00:21:02.590
would be called,
"Being Digital."

00:21:02.590 --> 00:21:05.390
And "Being Digital"
came out in 1995.

00:21:05.390 --> 00:21:07.630
But in 1970, he wrote a book
called "The Architecture

00:21:07.630 --> 00:21:10.030
Machine," and he dedicated
it to the first machine that

00:21:10.030 --> 00:21:11.155
can appreciate the gesture.

00:21:14.620 --> 00:21:16.900
Nicholas Negroponte
is a clever man.

00:21:16.900 --> 00:21:19.270
But this book, I
think, is actually

00:21:19.270 --> 00:21:21.020
fascinating and
really important.

00:21:21.020 --> 00:21:25.120
It was a theory of design
for artificial intelligence

00:21:25.120 --> 00:21:29.160
that I think still
holds weight today.

00:21:29.160 --> 00:21:31.980
I want to point out a little
bit about the funding structures

00:21:31.980 --> 00:21:33.600
of the Architecture
Machine Group

00:21:33.600 --> 00:21:37.944
and of artificial
intelligence in general.

00:21:37.944 --> 00:21:39.360
The Architecture
Machine Group was

00:21:39.360 --> 00:21:41.940
funded by the Information
Processing Technology

00:21:41.940 --> 00:21:46.170
Office of DARPA and the
Office of Naval Research.

00:21:46.170 --> 00:21:47.970
The Office of Naval
Research was so

00:21:47.970 --> 00:21:51.420
important for
defining the funding

00:21:51.420 --> 00:21:54.870
for artificial intelligence
from pretty much its inception

00:21:54.870 --> 00:21:56.940
onward through the '80s.

00:21:56.940 --> 00:22:00.720
So vital that I think that
Marvin Minsky has referred

00:22:00.720 --> 00:22:04.380
to Marvin Denicoff,
who was the program

00:22:04.380 --> 00:22:08.590
officer, as the grand old man
of artificial intelligence.

00:22:08.590 --> 00:22:11.930
So the thing to point
out is something

00:22:11.930 --> 00:22:13.800
that Paul Edwards,
the historian,

00:22:13.800 --> 00:22:15.540
has referred to as
the closed world,

00:22:15.540 --> 00:22:19.110
that artificial
intelligence was primarily

00:22:19.110 --> 00:22:22.830
developed within Department
of Defense funded entities,

00:22:22.830 --> 00:22:25.590
because it meant a
small group of people

00:22:25.590 --> 00:22:27.540
could continue to
work on these projects

00:22:27.540 --> 00:22:29.940
through the same social
network, friendship

00:22:29.940 --> 00:22:31.770
networks, and
professional networks that

00:22:31.770 --> 00:22:34.920
had been developed since
the end of World War II.

00:22:34.920 --> 00:22:38.790
Where NSF funding, National
Science Foundation funding,

00:22:38.790 --> 00:22:42.000
automatically, and by its very
spirit, tried to throw things

00:22:42.000 --> 00:22:44.160
open to the world
and open things up

00:22:44.160 --> 00:22:46.080
and bring more people in.

00:22:46.080 --> 00:22:51.420
So it was desired by
DARPA, DARPA IPTO, and ONR

00:22:51.420 --> 00:22:55.410
to keep funding and development
in these small networks

00:22:55.410 --> 00:22:56.010
of people.

00:22:56.010 --> 00:22:58.290
You could get
things done better.

00:22:58.290 --> 00:23:01.140
And it meant that the
Architecture Machine Group

00:23:01.140 --> 00:23:05.130
started to structure their
work like the AI Lab did at MIT

00:23:05.130 --> 00:23:09.810
and work very closely on
interfaces and small machines

00:23:09.810 --> 00:23:12.960
and environments for
artificial intelligence.

00:23:12.960 --> 00:23:15.270
One of these first
projects was funded by IBM.

00:23:15.270 --> 00:23:19.080
It was called Urban2
and then later, Urban5.

00:23:19.080 --> 00:23:22.410
And it was a conversational
user interface for urban design.

00:23:22.410 --> 00:23:23.492
This is 1970.

00:23:23.492 --> 00:23:26.320
I actually started a
little bit before that.

00:23:26.320 --> 00:23:31.620
And what happens here is a
user chooses a set of blocks.

00:23:31.620 --> 00:23:33.570
You see these squares
down at the bottom.

00:23:33.570 --> 00:23:35.111
And then there's a
conversation going

00:23:35.111 --> 00:23:37.260
on at the side using
this light pen to select

00:23:37.260 --> 00:23:39.040
questions and answers.

00:23:39.040 --> 00:23:42.130
And it asks you
questions, and you

00:23:42.130 --> 00:23:44.940
use the conversational
dialogue and the buttons

00:23:44.940 --> 00:23:51.060
here to determine what the
attributes are of these areas

00:23:51.060 --> 00:23:52.140
that you are designing.

00:23:52.140 --> 00:23:55.270
So its for urban design.

00:23:55.270 --> 00:24:00.580
And "The Architectural
Machine" book

00:24:00.580 --> 00:24:06.480
outlines and critiques
their study here.

00:24:06.480 --> 00:24:10.770
You might notice this last one
that says, Ted, many conflicts

00:24:10.770 --> 00:24:13.350
are occurring.

00:24:13.350 --> 00:24:17.134
And they said, it was charming,
but it printed garbage.

00:24:17.134 --> 00:24:18.675
But at least it was
friendly garbage.

00:24:18.675 --> 00:24:21.030
Those are the words
that they used.

00:24:21.030 --> 00:24:25.680
And I think it points
out, if anyone here

00:24:25.680 --> 00:24:28.110
has tried to design a
conversational user interface,

00:24:28.110 --> 00:24:30.930
you know how easy it's
going to seem on the surface

00:24:30.930 --> 00:24:35.280
and how hard it is in reality.

00:24:35.280 --> 00:24:39.300
We could look back to someone
like Joseph Weizenbaum in 1966,

00:24:39.300 --> 00:24:42.720
who created ELIZA,
the therapy program.

00:24:42.720 --> 00:24:47.250
It's a question and answer
conversational user interface.

00:24:47.250 --> 00:24:49.560
And it's very difficult
and problematic

00:24:49.560 --> 00:24:51.750
in ways that caused Joseph
Weizenbaum to rethink

00:24:51.750 --> 00:24:54.570
his own total engagement
with computation

00:24:54.570 --> 00:24:56.730
and artificial intelligence.

00:24:56.730 --> 00:24:59.100
So everybody knows that
this is difficult to do,

00:24:59.100 --> 00:25:01.840
and yet, we continually
try to do it anyway.

00:25:05.460 --> 00:25:08.070
Nicholas Negroponte
and Marvin Minsky

00:25:08.070 --> 00:25:11.580
were friends throughout
their entire--

00:25:11.580 --> 00:25:13.890
throughout Nicklaus'
entire professional life,

00:25:13.890 --> 00:25:16.960
until Marvin's death
a couple of years ago.

00:25:16.960 --> 00:25:22.230
And here you see a robotic
arm stacking blocks.

00:25:22.230 --> 00:25:25.020
A lot of AI problems
were developed in what

00:25:25.020 --> 00:25:26.640
were called MicroWorlds.

00:25:26.640 --> 00:25:30.770
So you focus in on some
small part of a project--

00:25:30.770 --> 00:25:32.190
some small issue.

00:25:32.190 --> 00:25:33.540
You're looking at edge finding.

00:25:33.540 --> 00:25:35.077
You're looking at
computer vision.

00:25:35.077 --> 00:25:36.660
In fact, you're
still, today, thinking

00:25:36.660 --> 00:25:40.402
about computer vision in some
very specific small ways.

00:25:40.402 --> 00:25:42.360
I know Justin was just
taking a computer vision

00:25:42.360 --> 00:25:44.670
class at Stanford.

00:25:44.670 --> 00:25:47.970
And you tried to throw
away the rest of the noise

00:25:47.970 --> 00:25:50.380
and just focus in
on that problem.

00:25:50.380 --> 00:25:53.650
And that's appropriate
for some period of time.

00:25:53.650 --> 00:25:57.870
But after a while, MicroWorlds
didn't actually scale up,

00:25:57.870 --> 00:26:00.700
and this gets to be a problem.

00:26:00.700 --> 00:26:03.150
But as I mentioned, the
Architecture Machine Group

00:26:03.150 --> 00:26:07.530
worked in MicroWorlds as
well and AI interfaces,

00:26:07.530 --> 00:26:11.590
and this is a project that they
did in 1970 for the Software

00:26:11.590 --> 00:26:12.090
Show.

00:26:12.090 --> 00:26:14.940
It was a museum show
at the Jewish Museum.

00:26:14.940 --> 00:26:18.390
And information technology,
its new meaning for art.

00:26:18.390 --> 00:26:20.890
And so here you see
all of these cubes.

00:26:20.890 --> 00:26:23.190
Isn't it funny,
Generator has cubes,

00:26:23.190 --> 00:26:25.860
and Urban5 has more cubes.

00:26:25.860 --> 00:26:29.400
And you've got this
set of mirrored blocks,

00:26:29.400 --> 00:26:31.540
400 mirrored blocks, a
five foot by eight foot

00:26:31.540 --> 00:26:38.467
pen and a gerbil horde, a
gerbil colony living there.

00:26:38.467 --> 00:26:40.300
And it's going to be
difficult to read this,

00:26:40.300 --> 00:26:42.490
but it says, gerbils
match wits with computer

00:26:42.490 --> 00:26:44.770
built environment.

00:26:44.770 --> 00:26:48.880
And what SEEK did was stack
blocks, and what gerbils did

00:26:48.880 --> 00:26:52.800
was move them around
and knock them over.

00:26:52.800 --> 00:26:57.170
This is the gatefold of
the software catalog, Life

00:26:57.170 --> 00:26:59.390
in a Computerized Environment.

00:26:59.390 --> 00:27:02.780
And there's this great
quote from Ted Nelson who

00:27:02.780 --> 00:27:04.230
says, "Our bodies are hardware.

00:27:04.230 --> 00:27:06.020
Our behavior, software."

00:27:06.020 --> 00:27:09.800
And it's Paul Pangaro who
made the contemporary Colloquy

00:27:09.800 --> 00:27:10.400
of Mobiles.

00:27:10.400 --> 00:27:14.730
And a member of the Architecture
Machine Group who told me,

00:27:14.730 --> 00:27:16.716
it was a little bit
too true, because SEEK

00:27:16.716 --> 00:27:17.840
tended to kill the gerbils.

00:27:21.500 --> 00:27:23.420
In later days, the
Architecture Machine Group

00:27:23.420 --> 00:27:25.340
turned to command and
control interfaces

00:27:25.340 --> 00:27:27.440
that were explicitly--

00:27:27.440 --> 00:27:31.040
that had explicit military
connections and applications.

00:27:31.040 --> 00:27:34.520
And this is because the
only way, past 1974,

00:27:34.520 --> 00:27:37.700
that you could get work and
artificial intelligence funded

00:27:37.700 --> 00:27:40.310
was to align it with
command and control

00:27:40.310 --> 00:27:44.090
questions and direct tactical
military applications.

00:27:44.090 --> 00:27:46.880
This is due to some
legislative things

00:27:46.880 --> 00:27:50.330
that had happened in the '70s
in the wake of the Vietnam War.

00:27:50.330 --> 00:27:52.940
And so this is the
Aspen Movie Map.

00:27:52.940 --> 00:27:54.770
It is kind of what
it looks like,

00:27:54.770 --> 00:27:57.470
Google Street View that you
experience sitting in a room,

00:27:57.470 --> 00:27:58.260
in a chair.

00:27:58.260 --> 00:28:01.970
You zoom down the streets of
Aspen, Colorado in an Eames

00:28:01.970 --> 00:28:07.160
lounge chair with joy
pads, and a video disk

00:28:07.160 --> 00:28:08.990
produces the images for you.

00:28:08.990 --> 00:28:12.620
They had a prototype video
disk player in the 1970s.

00:28:12.620 --> 00:28:14.480
And the way you get
the images is everybody

00:28:14.480 --> 00:28:16.790
goes to Aspen on vacation,
and you load up the Jeep,

00:28:16.790 --> 00:28:19.512
and that's what you
have right here.

00:28:19.512 --> 00:28:22.820
So you zoom down the streets,
and there other photos

00:28:22.820 --> 00:28:25.730
that show what's in front of--

00:28:25.730 --> 00:28:28.670
the two smaller screens
are usually maps

00:28:28.670 --> 00:28:32.610
and are on touchscreens.

00:28:32.610 --> 00:28:35.110
I want to point out that part
of the reason this project was

00:28:35.110 --> 00:28:40.780
funded was that it was
in the wake of the jet

00:28:40.780 --> 00:28:44.025
rescue, the hostage
rescue in Entebbe, Uganda.

00:28:46.840 --> 00:28:51.160
An Israeli jet
had been hijacked,

00:28:51.160 --> 00:28:56.470
and the rescue of the
hijacking was performed

00:28:56.470 --> 00:28:58.240
by building a model desert--

00:28:58.240 --> 00:29:01.030
or a model airport
in the Negev Desert

00:29:01.030 --> 00:29:07.120
and practicing the rescue
under cover of darkness.

00:29:07.120 --> 00:29:11.890
In fact, I think this is the
subject now of a movie that's

00:29:11.890 --> 00:29:15.350
about to come out or is
coming out later this year.

00:29:15.350 --> 00:29:17.740
And so the question
here became, what

00:29:17.740 --> 00:29:20.710
would happen if you were able
to actually simulate and do

00:29:20.710 --> 00:29:24.370
remote moving and move
down these streets?

00:29:24.370 --> 00:29:28.730
So that would be the
stipulation for this project.

00:29:28.730 --> 00:29:31.970
In later years, they were really
explicit about this in some

00:29:31.970 --> 00:29:33.207
of the reports they wrote.

00:29:33.207 --> 00:29:34.790
Minsky and his
colleague, Richard Bolt

00:29:34.790 --> 00:29:36.456
wrote that, "We are
proposing to develop

00:29:36.456 --> 00:29:38.510
human-computer
interfaces, on the one

00:29:38.510 --> 00:29:41.570
hand as sophisticated in
conception as a cockpit,

00:29:41.570 --> 00:29:44.300
and on the other hand as
operationally simple as a TV.

00:29:44.300 --> 00:29:46.010
From either perspective,
the objective

00:29:46.010 --> 00:29:48.802
is the same, supreme usability."

00:29:51.520 --> 00:29:56.080
Later on, they started
delving into virtual reality

00:29:56.080 --> 00:29:59.390
in certain ways and questions
of augmented reality.

00:29:59.390 --> 00:30:02.080
So this provocation,
Mapping by Yourself,

00:30:02.080 --> 00:30:06.400
is a postcard held up in
front of the Queen's palace

00:30:06.400 --> 00:30:07.990
in Stockholm and
imagining what it

00:30:07.990 --> 00:30:10.560
would be like to have a screen
that overlaid information

00:30:10.560 --> 00:30:11.950
upon the world.

00:30:11.950 --> 00:30:18.450
On the other side is
Westinghouse Window, basically,

00:30:18.450 --> 00:30:24.520
a very early iPad like thing
that was still connected

00:30:24.520 --> 00:30:26.011
to various technologies.

00:30:26.011 --> 00:30:27.760
And you can kind of
see in the background,

00:30:27.760 --> 00:30:31.990
there's a Selectric
typewriter and a Dictaphone.

00:30:31.990 --> 00:30:35.620
But this was a mapping window
and intended to substitute

00:30:35.620 --> 00:30:36.940
for maps in the battlefield.

00:30:36.940 --> 00:30:41.182
It had 2 dimensional and
2 and 1/2 D capabilities,

00:30:41.182 --> 00:30:42.640
depending on how
you would flip it.

00:30:42.640 --> 00:30:44.390
And this is, apparently,
the first example

00:30:44.390 --> 00:30:45.890
of a layered digital map.

00:30:49.070 --> 00:30:52.220
I like the fact that Nicholas
Negroponte has always

00:30:52.220 --> 00:30:54.980
understood what he
has been working on

00:30:54.980 --> 00:30:58.680
and the conundrums and
contradictions about it.

00:30:58.680 --> 00:31:01.940
And in his 1975 book, "Soft
Architecture Machines,"

00:31:01.940 --> 00:31:04.610
he said, "I strongly believe
that it is very important

00:31:04.610 --> 00:31:06.770
to play with these
ideas scientifically

00:31:06.770 --> 00:31:08.540
and explore
applications of machine

00:31:08.540 --> 00:31:11.540
learning that totter between
being unimaginably oppressive

00:31:11.540 --> 00:31:13.280
and unbelievably exciting."

00:31:17.160 --> 00:31:20.172
As I look at these
historical examples

00:31:20.172 --> 00:31:22.130
and where we are today,
I find myself wondering

00:31:22.130 --> 00:31:23.990
if we need new cliches.

00:31:23.990 --> 00:31:26.859
Because when I do Google
Image Search and pop in AI,

00:31:26.859 --> 00:31:27.650
this is what I get.

00:31:30.190 --> 00:31:35.700
There's always this, the
wavy or the behold the hand.

00:31:35.700 --> 00:31:37.040
And it's always that color blue.

00:31:37.040 --> 00:31:39.537
It's like the blue
of the projector

00:31:39.537 --> 00:31:40.620
when no one's in the room.

00:31:43.200 --> 00:31:45.080
JSON lady.

00:31:45.080 --> 00:31:48.150
No, this is jQuery, right?

00:31:48.150 --> 00:31:50.450
Cyborg lady.

00:31:50.450 --> 00:31:53.480
If these are views
of AI in the future,

00:31:53.480 --> 00:31:55.160
I think we're in trouble.

00:31:55.160 --> 00:31:57.020
But I appreciate some
of the subversive ways

00:31:57.020 --> 00:31:59.061
that we can look at this
and some of the fun ways

00:31:59.061 --> 00:32:01.670
that we can as well.

00:32:01.670 --> 00:32:05.810
Is anyone here a fan of
Janelle Shane in AI weirdness?

00:32:05.810 --> 00:32:10.587
I nerded out so hard at I/O this
year when I got to meet her.

00:32:10.587 --> 00:32:12.920
She does very funny things
with AI that I'll talk about,

00:32:12.920 --> 00:32:15.940
but I really appreciate that she
understands what she's doing.

00:32:15.940 --> 00:32:17.720
And she said, "If life
plays by the rules,

00:32:17.720 --> 00:32:19.580
image recognition works well.

00:32:19.580 --> 00:32:22.050
But as soon as people or
sheep do something unexpected,

00:32:22.050 --> 00:32:24.230
the algorithms show
their weaknesses."

00:32:24.230 --> 00:32:26.570
So are these orange flowers
in a field or sheep?

00:32:26.570 --> 00:32:31.660
Because she colored them
orange, it's not sure anymore.

00:32:31.660 --> 00:32:33.580
Janelle's also done
things like, this

00:32:33.580 --> 00:32:39.250
is 7,000 paint colors
fed to a neural net.

00:32:39.250 --> 00:32:43.390
And it begins to create
new colors, all of which

00:32:43.390 --> 00:32:46.960
are terrible, and color names.

00:32:46.960 --> 00:32:49.870
Exactly, I've been
reduced to tears

00:32:49.870 --> 00:32:56.440
multiple times, caring
tan, burble simp, turdly,

00:32:56.440 --> 00:33:00.430
ronching blue, tondar.

00:33:00.430 --> 00:33:03.190
She fed the names to--

00:33:03.190 --> 00:33:06.430
the names of Guinea pigs for
the Portland Guinea pig rescue

00:33:06.430 --> 00:33:09.400
and came up with some
names for Guinea pigs.

00:33:09.400 --> 00:33:12.535
Here, we have Hangar
Dan and Princess Pow.

00:33:12.535 --> 00:33:14.270
[LAUGHTER]

00:33:14.270 --> 00:33:16.927
And my husband has said, I
think any kid could come up

00:33:16.927 --> 00:33:18.760
with Princess Pow, but
it takes a neural net

00:33:18.760 --> 00:33:20.980
to come up with Hangar Dan.

00:33:20.980 --> 00:33:23.944
Only machine learning
can produce [INAUDIBLE]..

00:33:23.944 --> 00:33:26.110
And again, when I think
about these kinds of things,

00:33:26.110 --> 00:33:29.640
I also find myself thinking
about this, which is actually--

00:33:29.640 --> 00:33:32.800
it apparently was not Marcel
Duchamp, who delivered this

00:33:32.800 --> 00:33:37.570
to the art show in 1917,
this urinal, as his object,

00:33:37.570 --> 00:33:40.570
but rather a woman whose
name I can't remember.

00:33:40.570 --> 00:33:42.590
They've just surfaced
that it wasn't him.

00:33:42.590 --> 00:33:45.340
But this was part
of Dadaism, where

00:33:45.340 --> 00:33:47.500
the world was going to hell.

00:33:47.500 --> 00:33:51.190
It was World War I, and things
were really, really dark.

00:33:51.190 --> 00:33:53.740
And so what does art mean
when things are really dark?

00:33:53.740 --> 00:33:55.725
It means that you've got
to turn to absurdity.

00:33:55.725 --> 00:33:57.640
You've got to turn
it upside down,

00:33:57.640 --> 00:34:00.280
and so art starts
to look different.

00:34:00.280 --> 00:34:02.500
Or you look at someone like
Eugene Ionesco, who wrote

00:34:02.500 --> 00:34:06.550
the play "Rhinoceros" in 1959.

00:34:06.550 --> 00:34:09.296
It's an absurdest
play, and it's where

00:34:09.296 --> 00:34:11.170
a group of people in a
French town bit by bit

00:34:11.170 --> 00:34:12.610
turned into rhinoceroses.

00:34:12.610 --> 00:34:16.250
And of course, he's talking
about totalitarianism.

00:34:16.250 --> 00:34:18.250
Ionesco wrote a play
called, "The Bald Soprano,"

00:34:18.250 --> 00:34:21.370
because he had started
to try to learn English.

00:34:21.370 --> 00:34:23.409
He's French Romanian.

00:34:23.409 --> 00:34:27.310
And in order to learn
English, he would type phrase

00:34:27.310 --> 00:34:29.710
after phrase from
this English book,

00:34:29.710 --> 00:34:32.710
John and Sally go to the
store and over and over.

00:34:32.710 --> 00:34:35.719
And he began to question the
nature of these meanings.

00:34:35.719 --> 00:34:38.949
It's almost like he became
his own algorithm in order

00:34:38.949 --> 00:34:39.750
to learn this.

00:34:39.750 --> 00:34:42.159
And he produced a
very funny play called

00:34:42.159 --> 00:34:44.511
"The Bald Soprano" about it.

00:34:44.511 --> 00:34:45.969
Or you could even
look at something

00:34:45.969 --> 00:34:52.674
like Luis Bunuel, who did "Un
Chien Andalou" in the 1920s,

00:34:52.674 --> 00:34:54.159
19-teens.

00:34:54.159 --> 00:34:57.462
But in 1970, '71, something
like that, he did "The Discreet

00:34:57.462 --> 00:34:59.920
Charm of the Bourgeoisie,"
which is about a group of people

00:34:59.920 --> 00:35:02.830
trying to have dinner, and they
just keep getting thwarted.

00:35:02.830 --> 00:35:05.760
And it's a somewhat
contemporary surrealist film.

00:35:05.760 --> 00:35:07.510
But all of these things
are ways to-- when

00:35:07.510 --> 00:35:11.092
the world is on its head,
you upend it in art.

00:35:11.092 --> 00:35:12.550
And I want to
suggest that maybe we

00:35:12.550 --> 00:35:14.350
should be doing the
same things more

00:35:14.350 --> 00:35:18.320
frequently with our algorithms.

00:35:18.320 --> 00:35:21.424
Can anyone name what this is?

00:35:21.424 --> 00:35:22.520
AUDIENCE: [INAUDIBLE]

00:35:22.520 --> 00:35:24.550
MOLLY WRIGHT
STEENSON: Thank you.

00:35:24.550 --> 00:35:26.190
We're saying that.

00:35:26.190 --> 00:35:27.900
It's exactly what it is.

00:35:27.900 --> 00:35:31.200
And the uncanny valley
is an idea from 1970

00:35:31.200 --> 00:35:34.120
from Masahiro Mori.

00:35:34.120 --> 00:35:37.080
This is the idea that when
robots are too similar to us,

00:35:37.080 --> 00:35:40.260
they freak us out, and
he describes the eeriness

00:35:40.260 --> 00:35:41.220
that that produces.

00:35:41.220 --> 00:35:43.830
But he says something
else in his piece

00:35:43.830 --> 00:35:48.060
in the article he wrote in 1970,
which is that it's about how we

00:35:48.060 --> 00:35:50.209
understand what makes us human.

00:35:50.209 --> 00:35:51.750
"We should begin to
build an accurate

00:35:51.750 --> 00:35:54.360
map of the uncanny valley, so
that through robotics research

00:35:54.360 --> 00:35:56.190
we can understand
what makes us human."

00:36:01.130 --> 00:36:02.930
Until recently,
Manuela Veloso was

00:36:02.930 --> 00:36:06.530
the head of the Machine Learning
Program at Carnegie Mellon.

00:36:06.530 --> 00:36:09.869
And I was at a conference
that she was speaking at,

00:36:09.869 --> 00:36:11.660
and she was talking
about, machine learning

00:36:11.660 --> 00:36:14.690
is successful when "it does
what you expect it to do."

00:36:14.690 --> 00:36:17.630
And for Cedric Price,
we saw the goodness

00:36:17.630 --> 00:36:21.810
of when something doesn't
do what you expect it to do.

00:36:21.810 --> 00:36:24.050
But there are some
dark things, too.

00:36:24.050 --> 00:36:25.940
The stories that came
out earlier this spring

00:36:25.940 --> 00:36:29.750
about Palantir's
predictive policing

00:36:29.750 --> 00:36:34.970
in New Orleans, and the fact
that that had not been known,

00:36:34.970 --> 00:36:38.840
not even to most people in the
city of New Orleans or city

00:36:38.840 --> 00:36:41.090
officials.

00:36:41.090 --> 00:36:43.290
I like the work a
lot of Mimi Onuoha.

00:36:43.290 --> 00:36:44.540
Some of you may know her work.

00:36:44.540 --> 00:36:47.780
She does a project
called Missing Datasets.

00:36:47.780 --> 00:36:51.114
And you can't parse
what you don't collect.

00:36:51.114 --> 00:36:52.530
And so if you don't
have the data,

00:36:52.530 --> 00:36:54.050
you can't begin to parse it.

00:36:54.050 --> 00:36:58.100
She does this both as a tech
piece and a critical piece

00:36:58.100 --> 00:36:59.290
and an art piece.

00:36:59.290 --> 00:37:03.639
So here you see a lot of folders
full of missing datasets.

00:37:03.639 --> 00:37:05.430
And of course, if you
go look through them,

00:37:05.430 --> 00:37:07.620
they're all empty.

00:37:07.620 --> 00:37:10.820
She keeps a GitHub
repository where

00:37:10.820 --> 00:37:13.880
she has these missing datasets.

00:37:13.880 --> 00:37:16.460
And this one, civilians
killed in encounters

00:37:16.460 --> 00:37:18.710
with police or law
enforcement agencies,

00:37:18.710 --> 00:37:20.420
is no longer a missing dataset.

00:37:20.420 --> 00:37:22.470
It's been collected.

00:37:22.470 --> 00:37:25.490
And so she'd like to
see more of those.

00:37:25.490 --> 00:37:27.800
And case in point,
sometimes, there's

00:37:27.800 --> 00:37:33.410
an activist angle to her
work, but there's also

00:37:33.410 --> 00:37:35.960
work that happens
as a result. A group

00:37:35.960 --> 00:37:40.010
of Asian actors on
Broadway contacted her

00:37:40.010 --> 00:37:43.280
about the roles that
are available for Asians

00:37:43.280 --> 00:37:44.360
on Broadway.

00:37:44.360 --> 00:37:49.340
And that blue line
is the Asian roles

00:37:49.340 --> 00:37:53.900
in the play "The King and I,"
and the other bright light blue

00:37:53.900 --> 00:37:56.630
that you see are
Asian roles elsewhere

00:37:56.630 --> 00:37:59.450
in the 2014, 2015 season.

00:37:59.450 --> 00:38:01.340
Similarly, you can
see the problems

00:38:01.340 --> 00:38:03.920
for black actors
or Hispanic actors,

00:38:03.920 --> 00:38:07.640
just not a lot
available at that point.

00:38:07.640 --> 00:38:09.500
But by collecting
this data, it became

00:38:09.500 --> 00:38:11.960
possible to change the equation.

00:38:11.960 --> 00:38:15.200
And now, there are more
Asian actors on Broadway.

00:38:15.200 --> 00:38:16.760
This is not a matter
of an algorithm,

00:38:16.760 --> 00:38:18.350
but it's just, quite
simply, a matter

00:38:18.350 --> 00:38:22.760
of the collection of data
and collecting the data

00:38:22.760 --> 00:38:24.420
and naming it, making changes.

00:38:29.030 --> 00:38:29.530
Oprah.

00:38:32.310 --> 00:38:34.930
But I'm struck by the things
that machines don't do,

00:38:34.930 --> 00:38:37.780
that we don't have good machines
for picking raspberries.

00:38:37.780 --> 00:38:40.210
And we kind of do for
picking strawberries,

00:38:40.210 --> 00:38:43.510
but it's very, very
difficult to get this right,

00:38:43.510 --> 00:38:46.600
to manipulate, to
do this properly.

00:38:46.600 --> 00:38:50.770
And I'm struck by the
fact that if you--

00:38:50.770 --> 00:38:53.920
I think it was in Sweden,
there was a mining operation

00:38:53.920 --> 00:38:59.290
that had a truck, that had
some very nice machine learning

00:38:59.290 --> 00:39:01.960
algorithms operating
it more efficiently.

00:39:01.960 --> 00:39:05.000
And it was so efficient that
it dug itself into a rut

00:39:05.000 --> 00:39:06.460
and couldn't get back out.

00:39:06.460 --> 00:39:09.040
So they had to introduce
noise into the system

00:39:09.040 --> 00:39:14.560
to allow for the vehicle
to not too expertly

00:39:14.560 --> 00:39:18.760
dig itself into the ground.

00:39:18.760 --> 00:39:23.850
So in closing, I'll say that
AI is new, but it's also old.

00:39:23.850 --> 00:39:26.580
And that it is a
matter of architecture.

00:39:26.580 --> 00:39:28.840
It's a matter of infrastructure.

00:39:28.840 --> 00:39:32.590
And it's a matter
of how we design.

00:39:32.590 --> 00:39:35.530
This is something I see in my
neighborhood in Pittsburgh.

00:39:35.530 --> 00:39:38.015
I go running by
Carnegie Robotics.

00:39:38.015 --> 00:39:40.330
I'm not totally sure
how I feel about it.

00:39:46.660 --> 00:39:48.670
I'm still not sure
how I feel about it,

00:39:48.670 --> 00:39:51.270
but I think it's
pretty compelling.

00:39:51.270 --> 00:39:52.645
But this is the
thing that I need

00:39:52.645 --> 00:39:55.180
to come back to over and
over again, which is our man,

00:39:55.180 --> 00:39:55.930
Cedric Price.

00:39:55.930 --> 00:39:58.030
He says, "Technology
is the answer.

00:39:58.030 --> 00:39:59.940
But what was the question?

00:39:59.940 --> 00:40:01.410
Thank you.

00:40:01.410 --> 00:40:04.350
[APPLAUSE]

00:40:04.350 --> 00:40:06.940
AUDIENCE: Hi, my
name Avi Melina.

00:40:06.940 --> 00:40:09.216
I'm an analyst here at Google.

00:40:09.216 --> 00:40:10.840
And I've been getting
really interested

00:40:10.840 --> 00:40:14.050
in some of the things happening
on the edge of this space,

00:40:14.050 --> 00:40:15.910
like generative design
and some of the stuff

00:40:15.910 --> 00:40:18.110
that Autodesk is working on.

00:40:18.110 --> 00:40:21.430
And I'm curious as to how--

00:40:21.430 --> 00:40:25.450
where you see this
space evolving fastest,

00:40:25.450 --> 00:40:28.750
because like 3D printing,
there is usually

00:40:28.750 --> 00:40:30.220
industries that
pick it up first.

00:40:30.220 --> 00:40:33.430
Where do you see the
first mass market

00:40:33.430 --> 00:40:37.030
case of architectural AI or
intelligence really happening?

00:40:37.030 --> 00:40:39.490
MOLLY WRIGHT STEENSON: That's
a really good question,

00:40:39.490 --> 00:40:43.599
and I'm not sure if I have
a well-thought-out answer.

00:40:43.599 --> 00:40:45.640
But I'd point out that
the history of 3D printing

00:40:45.640 --> 00:40:47.560
is already quite old.

00:40:47.560 --> 00:40:52.450
And so what we see the uptake
in now at a consumer level

00:40:52.450 --> 00:40:57.200
or at a professional level--

00:40:57.200 --> 00:41:00.310
and the other thing is, I
don't think that 3D printers--

00:41:00.310 --> 00:41:02.104
I think they became
something that--

00:41:02.104 --> 00:41:04.270
it was said, they were going
to transform everything

00:41:04.270 --> 00:41:05.000
in the world.

00:41:05.000 --> 00:41:08.890
And so then everybody
started getting makerspaces.

00:41:08.890 --> 00:41:11.170
I'm not sure that
that has actually--

00:41:11.170 --> 00:41:13.730
printing objects has
made the world change.

00:41:13.730 --> 00:41:15.950
But the question of
generative design,

00:41:15.950 --> 00:41:19.000
I think that architecturally,
the primary focus

00:41:19.000 --> 00:41:21.640
has been on form
and representation.

00:41:21.640 --> 00:41:25.900
You use any number of
authoring programs and design

00:41:25.900 --> 00:41:30.130
programs to produce
work that you

00:41:30.130 --> 00:41:32.350
wouldn't be able to otherwise.

00:41:32.350 --> 00:41:35.470
I get very curious, again, about
these old ideas that I still

00:41:35.470 --> 00:41:41.110
think are vital about how
they change the design

00:41:41.110 --> 00:41:44.350
process and the
ingredients that go

00:41:44.350 --> 00:41:46.270
in that make us think
of something that

00:41:46.270 --> 00:41:50.470
isn't necessarily just
ending up in form,

00:41:50.470 --> 00:41:52.920
but ending up in how
we construe design.

00:41:57.690 --> 00:41:58.200
Thanks.

00:41:58.200 --> 00:41:59.074
It's a good question.

00:41:59.074 --> 00:42:02.456
Now, I'm going to try and
figure out the answer.

00:42:02.456 --> 00:42:08.170
AUDIENCE: So Rohit says, any
thoughts on Bill Mitchell's

00:42:08.170 --> 00:42:13.900
role, "City of Bits" and/or
urban design on software

00:42:13.900 --> 00:42:15.190
other than Sim City?

00:42:18.804 --> 00:42:20.530
MOLLY WRIGHT STEENSON:
Bill Mitchell

00:42:20.530 --> 00:42:25.700
was, at points, the Dean of the
School of Architecture at MIT.

00:42:25.700 --> 00:42:28.500
He ran the Smart Cities Lab,
absolutely vital individual.

00:42:28.500 --> 00:42:30.870
He's actually an alum
of my master's program,

00:42:30.870 --> 00:42:33.700
and he died about
five years ago.

00:42:33.700 --> 00:42:36.590
And I didn't write about
him very much in my book.

00:42:36.590 --> 00:42:40.480
I was focusing on a
slightly earlier era.

00:42:40.480 --> 00:42:44.530
But he's a totally vital figure.

00:42:44.530 --> 00:42:47.860
And there's a woman named
Anne-Marie Brennan right now--

00:42:47.860 --> 00:42:50.200
she's a graduate
of my PhD program--

00:42:50.200 --> 00:42:52.300
in Melbourne,
Australia, who's been

00:42:52.300 --> 00:42:56.270
working on some of his work
and using his archive there.

00:42:56.270 --> 00:42:58.690
So I think we're going
to see some stuff

00:42:58.690 --> 00:43:00.400
forthcoming about Bill Mitchell.

00:43:00.400 --> 00:43:03.070
But you also see
his work in-- you

00:43:03.070 --> 00:43:06.880
see how he's inspired people,
including Anthony Townsend, who

00:43:06.880 --> 00:43:09.370
wrote the book, "Smart Cities."

00:43:09.370 --> 00:43:12.160
And he's been thinking a
lot for the last 20 years

00:43:12.160 --> 00:43:15.910
on urban planning, smart
cities, and development.

00:43:15.910 --> 00:43:17.950
I guess from a smart
city perspective,

00:43:17.950 --> 00:43:21.460
I keep going back
to two big concerns

00:43:21.460 --> 00:43:24.760
that smart cities tend to be--

00:43:24.760 --> 00:43:27.010
and again, I get to say this,
because I'm an academic,

00:43:27.010 --> 00:43:27.530
I guess.

00:43:27.530 --> 00:43:32.440
But it seems to be viewed as
the locus of really great,

00:43:32.440 --> 00:43:35.830
big enterprise deployments, like
if we used to get excited about

00:43:35.830 --> 00:43:39.850
enterprises-- the scale of
a building or a campus--

00:43:39.850 --> 00:43:43.600
this is even yet
still more tech.

00:43:43.600 --> 00:43:47.230
And a lot of it hasn't been
very graciously designed.

00:43:47.230 --> 00:43:49.360
I also think of that
idea of MicroWorlds,

00:43:49.360 --> 00:43:52.510
that the question of
scale is really vital.

00:43:52.510 --> 00:43:55.480
But when we say scale, it means
a lot of different things,

00:43:55.480 --> 00:43:56.950
like is it one to a billion?

00:43:56.950 --> 00:43:59.410
Is it tiny to great big?

00:43:59.410 --> 00:44:02.160
Is it-- has many more users?

00:44:02.160 --> 00:44:04.190
Is it-- has many
more stakeholders?

00:44:04.190 --> 00:44:06.280
And I think designers
and architects are really

00:44:06.280 --> 00:44:09.490
well-positioned to talk
about issues of scale.

00:44:09.490 --> 00:44:11.830
But it's the hardest
thing, I think,

00:44:11.830 --> 00:44:14.920
for us to figure out what we
mean when we mean deploying

00:44:14.920 --> 00:44:16.480
on that kind of scale.

00:44:16.480 --> 00:44:18.392
Thank you, Rohit.

00:44:18.392 --> 00:44:19.850
AUDIENCE: Hi, my
name is Christine.

00:44:19.850 --> 00:44:21.391
MOLLY WRIGHT STEENSON:
Hi, Christine.

00:44:21.391 --> 00:44:25.130
AUDIENCE: I just wanted to
ask, what are you most excited

00:44:25.130 --> 00:44:28.490
about that's getting
researched now

00:44:28.490 --> 00:44:32.600
or an idea that you might
have just heard about?

00:44:32.600 --> 00:44:35.921
What in this field are
you most excited about?

00:44:35.921 --> 00:44:37.920
MOLLY WRIGHT STEENSON:
Thanks for that question.

00:44:37.920 --> 00:44:41.210
That's a really nice
thing to be asked.

00:44:41.210 --> 00:44:44.030
Well, I recently received
this named professorship,

00:44:44.030 --> 00:44:49.190
and it's the K&amp;L Gates Associate
Professorship in Ethics

00:44:49.190 --> 00:44:51.140
and Computational Technologies.

00:44:51.140 --> 00:44:52.550
I'm not an ethicist.

00:44:52.550 --> 00:44:54.890
I'm a designer, and
I'm a historian.

00:44:54.890 --> 00:44:57.950
And I'm really
curious about what

00:44:57.950 --> 00:45:01.670
we talk about when we talk about
ethics, because I don't think

00:45:01.670 --> 00:45:05.030
that it means ethics per se.

00:45:08.550 --> 00:45:10.440
You can't just sprinkle
some ethics on top

00:45:10.440 --> 00:45:14.320
and make the food salty,
and that doesn't work.

00:45:14.320 --> 00:45:17.790
And it makes me remember that
time in the '90s and 2000s when

00:45:17.790 --> 00:45:20.040
people discovered
usability and said,

00:45:20.040 --> 00:45:22.080
we need some usability
on this website.

00:45:22.080 --> 00:45:24.720
And I'm like, no, you
don't want usability.

00:45:24.720 --> 00:45:26.810
You want it to work.

00:45:26.810 --> 00:45:28.200
You want it to be good.

00:45:28.200 --> 00:45:32.010
And I'm going to leave here and
go to Stanford to meet up with

00:45:32.010 --> 00:45:37.350
Fred Turner, who if you
haven't read his recent pieces

00:45:37.350 --> 00:45:41.480
in "Logic" and in
"032c," you should.

00:45:41.480 --> 00:45:44.880
The "032c" piece
came out this week.

00:45:44.880 --> 00:45:47.000
The piece in "Logic"
is a few months old.

00:45:47.000 --> 00:45:50.820
So he's the head of the
communications department.

00:45:50.820 --> 00:45:55.680
He's a historian of, among
other things, Silicon Valley.

00:45:55.680 --> 00:45:59.970
And we're grappling with a
question about education,

00:45:59.970 --> 00:46:04.130
and he's speaking about
engineering-- writing

00:46:04.130 --> 00:46:05.310
about engineering education.

00:46:05.310 --> 00:46:07.610
But I think anyone
who's going to work with

00:46:07.610 --> 00:46:11.150
stuff, how do we educate people?

00:46:11.150 --> 00:46:13.700
Engineers are increasingly,
Fred will argue,

00:46:13.700 --> 00:46:15.650
STEM-oriented, and
then anything else

00:46:15.650 --> 00:46:19.100
is relegated to
other departments.

00:46:19.100 --> 00:46:21.830
But how do you make an
ethical approach not

00:46:21.830 --> 00:46:23.630
be a blow off class?

00:46:23.630 --> 00:46:27.570
How do you make it be a
part of the datasets that

00:46:27.570 --> 00:46:31.420
you're-- the problem
sets you're working on?

00:46:31.420 --> 00:46:35.920
Part of my interest in
casting the role of design

00:46:35.920 --> 00:46:41.170
to work in different kind
of areas is because of that.

00:46:41.170 --> 00:46:44.950
You can work with designers
on framing a problem,

00:46:44.950 --> 00:46:46.810
determining what data
should be collected,

00:46:46.810 --> 00:46:48.309
how it should be
used, how it should

00:46:48.309 --> 00:46:51.189
be visualized or explained.

00:46:51.189 --> 00:46:52.480
So I have questions about that.

00:46:52.480 --> 00:46:55.300
I have questions about
the possibilities

00:46:55.300 --> 00:46:58.240
and impossibilities
of explainable AI.

00:46:58.240 --> 00:47:00.100
And honestly, if
you just sit me down

00:47:00.100 --> 00:47:03.100
with a new history or
a bunch of documents

00:47:03.100 --> 00:47:05.410
about the history of AI, you
won't see me for a while,

00:47:05.410 --> 00:47:09.480
because I'll be very happy if I
can come back into that stuff.

00:47:09.480 --> 00:47:11.860
But I think we have some
very human questions that

00:47:11.860 --> 00:47:14.680
also meet the jobs of
the people in this room

00:47:14.680 --> 00:47:17.680
and the people on my campus and
probably the places that we all

00:47:17.680 --> 00:47:22.930
studied, that need broader
approaches than just the most

00:47:22.930 --> 00:47:26.682
efficacious and
efficient approach.

00:47:26.682 --> 00:47:29.478
Thanks.

00:47:29.478 --> 00:47:34.690
AUDIENCE: So one more question
from the Dory, also from Rohit.

00:47:34.690 --> 00:47:37.840
Why aren't there
more real architects

00:47:37.840 --> 00:47:40.420
celebrated in the field
of software architecture,

00:47:40.420 --> 00:47:41.920
other than
Christopher Alexander?

00:47:41.920 --> 00:47:43.260
MOLLY WRIGHT STEENSON:
Bless you for asking this.

00:47:43.260 --> 00:47:44.380
It's so great.

00:47:44.380 --> 00:47:46.390
Thank you.

00:47:46.390 --> 00:47:49.660
I asked Allen Cooper
a couple of years ago,

00:47:49.660 --> 00:47:52.410
can you name another architect?

00:47:52.410 --> 00:47:54.465
And I did this to
Kent Beck, too.

00:47:54.465 --> 00:47:57.240
And just silence for--

00:47:57.240 --> 00:48:00.460
and I think that Kent
Beck said, Le Corbusier.

00:48:00.460 --> 00:48:03.910
And I think Allen Cooper said--

00:48:03.910 --> 00:48:05.460
what was it-- John
Portman, the guy

00:48:05.460 --> 00:48:12.990
who did the Embarcadero Center
and the Bonaventure in LA.

00:48:12.990 --> 00:48:15.220
It would be better if
we had other architects.

00:48:15.220 --> 00:48:19.170
But I think the other thing
is that Christopher Alexander,

00:48:19.170 --> 00:48:23.940
as a mathematician, it's
something that everyone

00:48:23.940 --> 00:48:25.000
in the room deals with.

00:48:25.000 --> 00:48:29.400
You work on some very tiny part
of an absolutely huge problem.

00:48:29.400 --> 00:48:30.940
How do you go back and forth?

00:48:30.940 --> 00:48:32.700
How do you keep
that in your head?

00:48:32.700 --> 00:48:34.860
And Christopher Alexander
can explain that

00:48:34.860 --> 00:48:39.930
to people in a way
that really resonates

00:48:39.930 --> 00:48:43.260
with them, the working
on these two scales,

00:48:43.260 --> 00:48:46.530
and then explaining
why it actually matters

00:48:46.530 --> 00:48:48.240
on a cosmological sense.

00:48:48.240 --> 00:48:52.260
His late stuff is very woo-woo.

00:48:52.260 --> 00:48:55.020
And that really
resonates, and it really

00:48:55.020 --> 00:48:58.720
doesn't resonate with most
architects who build buildings.

00:48:58.720 --> 00:49:04.500
So I'd like to see Cedric Price
get taken up by everyone else,

00:49:04.500 --> 00:49:06.740
because he's fun.

00:49:06.740 --> 00:49:11.700
Alexander's not much fun,
but Cedric's a blast.

00:49:11.700 --> 00:49:15.810
And to get to know other
architects out there, I think,

00:49:15.810 --> 00:49:18.930
that the classes of people over
about the last four or five

00:49:18.930 --> 00:49:22.350
years to graduate from
architecture school or design

00:49:22.350 --> 00:49:24.660
school or those of you
who are in the spaces

00:49:24.660 --> 00:49:27.000
that crossover are really
doing interesting work

00:49:27.000 --> 00:49:29.910
and interesting ways
of thinking about it.

00:49:29.910 --> 00:49:31.500
If you want to look
at something fun,

00:49:31.500 --> 00:49:33.570
look at my friend
Fred Sherman's work.

00:49:33.570 --> 00:49:34.380
He's an architect.

00:49:34.380 --> 00:49:35.850
He's a professor
at Oregon State.

00:49:35.850 --> 00:49:40.320
And he has a book coming
out on space architecture,

00:49:40.320 --> 00:49:43.380
like space exploration,
architectural histories

00:49:43.380 --> 00:49:46.770
next year so fantastic stuff.

00:49:46.770 --> 00:49:49.880
AUDIENCE: Through
your talk, I think

00:49:49.880 --> 00:49:55.170
sometimes when you mention AI,
I would consider it as more

00:49:55.170 --> 00:49:58.200
like human-computer interaction.

00:49:58.200 --> 00:50:01.950
The reason I'm saying that
is because I'm an engineer.

00:50:01.950 --> 00:50:06.810
So when I got my PhD
more than 10 years ago--

00:50:06.810 --> 00:50:09.600
if you have a PhD
in AI, it's not

00:50:09.600 --> 00:50:12.630
as easy to find a job as now.

00:50:12.630 --> 00:50:17.150
So in my understanding, we have
a lot of breakthroughs in AI,

00:50:17.150 --> 00:50:18.930
like core technology,
I'm talking about,

00:50:18.930 --> 00:50:22.300
for example AlphaGo
or image recognition.

00:50:22.300 --> 00:50:25.110
Or if you look at
IBM Watson, so when

00:50:25.110 --> 00:50:28.680
they won the "Jeopardy"
show at that time.

00:50:28.680 --> 00:50:32.740
So we're talking about, there
are different types of AI core

00:50:32.740 --> 00:50:34.720
technology advancements,
and that's the reason

00:50:34.720 --> 00:50:38.240
we bring back the trend of AI.

00:50:38.240 --> 00:50:41.850
But this is quite different
from what you're talking about,

00:50:41.850 --> 00:50:43.620
like the human-computer
interactions.

00:50:43.620 --> 00:50:47.100
And I think IBM calls
it cognitive computing.

00:50:47.100 --> 00:50:51.510
So would you agree
with me if I tried

00:50:51.510 --> 00:50:53.460
to understand-- when
you say AI, it's

00:50:53.460 --> 00:50:55.320
more like human-computer
interaction,

00:50:55.320 --> 00:50:57.377
which is not the core
AI algorithm that we're

00:50:57.377 --> 00:50:57.960
talking about.

00:50:57.960 --> 00:51:02.799
MOLLY WRIGHT STEENSON:
So yes and no.

00:51:02.799 --> 00:51:03.590
Thank you for that.

00:51:03.590 --> 00:51:04.210
That's great.

00:51:04.210 --> 00:51:09.670
And to be clear, the focus
of my research in this book

00:51:09.670 --> 00:51:14.770
is 1960, to some
extent, 1948, because I

00:51:14.770 --> 00:51:16.640
look at cybernetics a lot--

00:51:16.640 --> 00:51:23.540
but to really about 1977, 1980.

00:51:23.540 --> 00:51:27.440
The parts of the book that
are focused on later material

00:51:27.440 --> 00:51:31.280
are not focused on
questions of AI.

00:51:31.280 --> 00:51:35.300
And I agree with you that they
are questions of human-computer

00:51:35.300 --> 00:51:43.690
interaction, but I would
say that when Negroponte was

00:51:43.690 --> 00:51:47.950
writing this book-- he was
writing the book in 1970

00:51:47.950 --> 00:51:50.470
and doing the work in 1970--

00:51:50.470 --> 00:51:55.030
he was developing
interfaces with the AI Lab,

00:51:55.030 --> 00:51:57.790
using their technologies
and their environments

00:51:57.790 --> 00:52:02.120
and basically, tinkering and
working in that capacity.

00:52:02.120 --> 00:52:06.480
So yes, they are-- it's
input and output devices

00:52:06.480 --> 00:52:10.120
and all kinds of ways that the
lab over that period of time

00:52:10.120 --> 00:52:11.500
played with that.

00:52:11.500 --> 00:52:13.390
It becomes human-computer
interaction,

00:52:13.390 --> 00:52:18.280
but I don't think HCI as a
field really comes together

00:52:18.280 --> 00:52:23.120
in that term until the 1980s.

00:52:23.120 --> 00:52:26.980
I'd need to look at my notes
to remember exactly when.

00:52:26.980 --> 00:52:30.610
I will say that I'm very curious
about what happens in the HCI

00:52:30.610 --> 00:52:33.660
community around
1990, because there

00:52:33.660 --> 00:52:37.390
is some interesting
crossovers that I think have

00:52:37.390 --> 00:52:41.500
things to do with HCI
and interaction design

00:52:41.500 --> 00:52:42.760
in ways that are--

00:52:42.760 --> 00:52:45.599
in things that are really vital.

00:52:45.599 --> 00:52:47.140
And yeah, I would
also agree with you

00:52:47.140 --> 00:52:50.350
that things are way
more complex today,

00:52:50.350 --> 00:52:53.560
and I have skated by all of it.

00:52:53.560 --> 00:52:57.860
So my understanding
of it is more nuanced.

00:52:57.860 --> 00:53:03.490
But again, I'm a historian of
technology and architecture,

00:53:03.490 --> 00:53:08.190
and I'm not someone
who got my PhD in AI.

00:53:08.190 --> 00:53:10.650
But yet, when you start
looking at it today

00:53:10.650 --> 00:53:17.310
and the implications and the
different kind of terms that

00:53:17.310 --> 00:53:23.000
people are claiming-- cognitive
computing, deep learning--

00:53:23.000 --> 00:53:25.820
there is a whole different
set of things to start

00:53:25.820 --> 00:53:27.110
looking at and studying.

00:53:27.110 --> 00:53:28.690
There are their histories.

00:53:28.690 --> 00:53:31.820
They're going back to
the 1980s and 1970s,

00:53:31.820 --> 00:53:34.090
that I haven't yet gotten into.

00:53:34.090 --> 00:53:36.150
But you're right.

00:53:36.150 --> 00:53:39.130
And yes, but no, but yes.

00:53:39.130 --> 00:53:40.540
[LAUGHTER]

00:53:40.540 --> 00:53:42.661
Thank you for that.

00:53:42.661 --> 00:53:45.160
AUDIENCE: Thank you so much,
everyone, for coming out today.

00:53:45.160 --> 00:53:47.380
And thank you, Molly
for your presence here.

00:53:47.380 --> 00:53:48.755
MOLLY WRIGHT
STEENSON: Thank you.

00:53:48.755 --> 00:53:52.770
[APPLAUSE]

