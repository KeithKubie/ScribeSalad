WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.030
So what I'm basically arguing here is
that we can think of reinforcement

00:00:03.030 --> 00:00:04.730
learning as a kind of planning.

00:00:04.730 --> 00:00:05.550
It's not learning at all.

00:00:05.550 --> 00:00:09.260
It's actually planning in a kind
of continuous space in POMDP,

00:00:09.260 --> 00:00:12.510
where the hidden state
is the set of parameters

00:00:12.510 --> 00:00:14.490
of the MDP that we're trying to learn.

00:00:14.490 --> 00:00:18.960
Now there's an infinite number of those,
which makes things a little bit awkward.

00:00:18.960 --> 00:00:22.545
And so we don't really get the piecewise
linear and convex property anymore.

00:00:22.545 --> 00:00:24.940
But it can be shown that you can
actually get a result that shows that

00:00:24.940 --> 00:00:27.780
the value function in this
continuous space POMDP

00:00:27.780 --> 00:00:30.440
is actually piecewise polynomial and
convex.

00:00:30.440 --> 00:00:31.350
&gt;&gt; Okay.

00:00:31.350 --> 00:00:34.780
&gt;&gt; It's not as nice as linear,
but it's still representable.

00:00:34.780 --> 00:00:37.020
&gt;&gt; How big is the degree
of that polynomial?

00:00:37.020 --> 00:00:39.440
&gt;&gt; It grows with iterations
of value iteration.

00:00:39.440 --> 00:00:40.050
&gt;&gt; I see.

00:00:40.050 --> 00:00:43.830
&gt;&gt; Yeah, so it's not like piecewise
cubic and convex, it's like piecewise,

00:00:43.830 --> 00:00:47.930
it could be a lot of degree [LAUGH]
depending on how many iterations you do.

00:00:47.930 --> 00:00:51.800
So this is still pretty
awkward to work with.

00:00:51.800 --> 00:00:55.421
There's an algorithm that actually works
fairly nicely called BEETLE that tries

00:00:55.421 --> 00:00:58.790
to approximate this piecewise
polynomial and convex function.

00:00:58.790 --> 00:01:02.470
And can sometimes actually end up
learning very good approximations of

00:01:02.470 --> 00:01:07.020
the optimal way to do reinforcement
learning for those spaces of problems.

00:01:07.020 --> 00:01:08.052
&gt;&gt; Does BEETLE stand for something?

00:01:08.052 --> 00:01:13.512
&gt;&gt; Bayesian Exploration
Exploitation Tradeoff in learning.

00:01:13.512 --> 00:01:15.130
&gt;&gt; Which is cheating, I think we agree.

00:01:15.130 --> 00:01:15.950
&gt;&gt; I think we agree, yeah, but

00:01:15.950 --> 00:01:19.370
otherwise it would be like BEETLE,
which would be just way too hard to say.

00:01:19.370 --> 00:01:22.465
&gt;&gt; Bayesian Exploration Exploitation
Tradeoff In Learning Everything.

00:01:22.465 --> 00:01:24.130
&gt;&gt; [LAUGH] The E for everything.

00:01:24.130 --> 00:01:25.490
&gt;&gt; Yeah, and then that would be better.

00:01:25.490 --> 00:01:27.037
I think I could forgive that.

00:01:27.037 --> 00:01:29.486
&gt;&gt; [LAUGH] Learning excellently.

00:01:29.486 --> 00:01:31.600
&gt;&gt; No, it's too many,
that's just absurd.

00:01:31.600 --> 00:01:32.830
&gt;&gt; The point that I'd
like to make though or

00:01:32.830 --> 00:01:37.630
the point of the slide is that
there are Bayesian RL algorithms,

00:01:37.630 --> 00:01:40.220
BEETLE is one of them,
it is not by any means the only one.

00:01:40.220 --> 00:01:43.200
There's lots of different ways that
people have looked at actually keeping

00:01:43.200 --> 00:01:46.710
Bayesian posteriors over the MDPs
that are being learned and

00:01:46.710 --> 00:01:50.720
then trying to use that Bayesian
posterior to make better decisions than

00:01:50.720 --> 00:01:54.390
you would in just reinforcement
learning, where you don't have

00:01:54.390 --> 00:01:59.070
any kind of prior, any kind of structure
on what the possible underlying MDP is.

00:01:59.070 --> 00:02:00.630
So a lot of people
really like this stuff.

00:02:00.630 --> 00:02:01.860
It seems to, at the moment,

00:02:01.860 --> 00:02:06.600
be on the side of just kind of too
expensive to be practically useful.

00:02:06.600 --> 00:02:09.389
Q learning seems to tend to win.

00:02:09.389 --> 00:02:14.160
But it's very elegant, and it's a useful
way of sort of realizing that planning

00:02:14.160 --> 00:02:16.550
and learning actually are two
sides of the same coin.

00:02:16.550 --> 00:02:18.830
&gt;&gt; We've used it before in
some work that I've done.

00:02:18.830 --> 00:02:20.820
&gt;&gt; Okay.
&gt;&gt; But what we were taking advantage of

00:02:20.820 --> 00:02:23.840
is the fact that it gives you
probability distributions over

00:02:23.840 --> 00:02:25.900
value functions or
rather over Q functions.

00:02:25.900 --> 00:02:27.400
&gt;&gt; How does it do that?

00:02:27.400 --> 00:02:28.302
&gt;&gt; You're in a state, and

00:02:28.302 --> 00:02:31.067
it just tells you basically the
probability of an action being optimal.

00:02:31.067 --> 00:02:34.819
And that turns out to be very nice
because you can then compare various

00:02:34.819 --> 00:02:37.925
sources of information
together because they're all,

00:02:37.925 --> 00:02:41.016
if you can make the model
look like probabilities.

00:02:41.016 --> 00:02:45.592
&gt;&gt; That sounds like that really cool
policy sort of merging paper that I saw.

00:02:45.592 --> 00:02:46.452
&gt;&gt; Yeah, yeah, that one.

00:02:46.452 --> 00:02:47.019
&gt;&gt; Is that you?

00:02:47.019 --> 00:02:48.186
&gt;&gt; Yeah, it was by Isbell and
his colleagues.

00:02:48.186 --> 00:02:48.929
&gt;&gt; Woo.

