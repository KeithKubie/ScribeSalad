WEBVTT
Kind: captions
Language: en

00:00:02.300 --> 00:00:03.610
DAVID SYMONDS: Thank you
very much for coming.

00:00:03.610 --> 00:00:04.940
How about we give our
gopher dancers a

00:00:04.940 --> 00:00:05.860
big round of applause.

00:00:05.860 --> 00:00:13.800
[APPLAUSE]

00:00:13.800 --> 00:00:15.834
DAVID SYMONDS: This is High
Performance Apps with Go on

00:00:15.834 --> 00:00:16.390
App Engine.

00:00:16.390 --> 00:00:17.920
Thank you very much
for coming.

00:00:17.920 --> 00:00:19.670
My name is David Symonds.

00:00:19.670 --> 00:00:22.920
I am an engineer on the Go team
at Google, and I lead the

00:00:22.920 --> 00:00:26.930
Technical Development for the
Go runtime for App Engine.

00:00:26.930 --> 00:00:28.290
As you might tell for
my accent, I'm

00:00:28.290 --> 00:00:29.050
not from around here.

00:00:29.050 --> 00:00:31.650
I'm from the Sydney office.

00:00:31.650 --> 00:00:33.580
So here's what we're going
to cover today.

00:00:33.580 --> 00:00:34.870
I'm just going to give
you a brief overview

00:00:34.870 --> 00:00:36.100
of Go on App Engine.

00:00:36.100 --> 00:00:38.770
Hopefully many of you here
have tried it out at one

00:00:38.770 --> 00:00:43.130
stage, just to give you a feel
for where we're at and give

00:00:43.130 --> 00:00:45.410
you a bit of an overview of
a couple of relatively

00:00:45.410 --> 00:00:48.110
high-profile apps that
are using it.

00:00:48.110 --> 00:00:50.490
Then I'm going to introduce a
motivating example for what

00:00:50.490 --> 00:00:53.520
we're going to use to explore
some performance techniques

00:00:53.520 --> 00:00:54.340
for your app.

00:00:54.340 --> 00:00:55.955
And then the rest of the
session, which will be most of

00:00:55.955 --> 00:00:58.550
the time, I'll be talking
about those performance

00:00:58.550 --> 00:01:01.400
techniques in some detail.

00:01:01.400 --> 00:01:02.720
So why Go on App engine?

00:01:02.720 --> 00:01:05.080
First of all, it's got the
peanut butter and the

00:01:05.080 --> 00:01:06.715
chocolate in a great
combination.

00:01:06.715 --> 00:01:08.900
Go compiles to native code.

00:01:08.900 --> 00:01:10.300
It runs really fast.

00:01:10.300 --> 00:01:13.020
It's a fantastic programming
environment to work in.

00:01:13.020 --> 00:01:15.820
At the same time, App Engine is
a fantastic platform as a

00:01:15.820 --> 00:01:19.190
service you can use to
host your web app.

00:01:19.190 --> 00:01:20.540
It auto scales.

00:01:20.540 --> 00:01:23.220
It's very, very low
maintenance.

00:01:23.220 --> 00:01:26.610
I don't like getting woken up
at 3:00 AM to fix things.

00:01:26.610 --> 00:01:29.190
I imagine neither do you.

00:01:29.190 --> 00:01:31.620
So if you're hosting on App
Engine, we have people at

00:01:31.620 --> 00:01:33.750
Google who will wake up at 3:00
AM to fix problems when

00:01:33.750 --> 00:01:35.690
they happen.

00:01:35.690 --> 00:01:38.080
Combining these two together, we
get the fastest runtime for

00:01:38.080 --> 00:01:39.060
App Engine.

00:01:39.060 --> 00:01:40.140
It starts really fast.

00:01:40.140 --> 00:01:42.102
It runs really fast.

00:01:42.102 --> 00:01:44.700
It keeps your instance
hours down low.

00:01:44.700 --> 00:01:47.730
It's cheap for what it does.

00:01:47.730 --> 00:01:52.060
And you've got code running at
full machine speed so you can

00:01:52.060 --> 00:01:54.950
do sophisticated, actual
processing, not

00:01:54.950 --> 00:01:57.450
just serving text.

00:01:57.450 --> 00:01:59.700
So we first unveiled the Go
runtime for App Engine here at

00:01:59.700 --> 00:02:01.660
Google I/O two years ago.

00:02:01.660 --> 00:02:03.870
And since then, we've seen
a lot of people adopt it.

00:02:03.870 --> 00:02:04.810
It's grown over time.

00:02:04.810 --> 00:02:07.100
It's supporting more of
the App Engine APIs.

00:02:07.100 --> 00:02:08.800
It's got better tooling.

00:02:08.800 --> 00:02:10.650
And we're seeing steady
growth and several

00:02:10.650 --> 00:02:11.940
high-profile users.

00:02:11.940 --> 00:02:14.860
Now, I wanted to talk about two
that Google has done to

00:02:14.860 --> 00:02:16.630
give you a feel for
what's possible.

00:02:16.630 --> 00:02:20.110
These are by no means hugely
sophisticated apps.

00:02:20.110 --> 00:02:22.700
But they're kind of
representative of things that

00:02:22.700 --> 00:02:25.230
are easy and natural to do in Go
that might be a little bit

00:02:25.230 --> 00:02:26.670
harder in other languages.

00:02:26.670 --> 00:02:30.570
But they really show off the
power and speed you can get.

00:02:30.570 --> 00:02:35.530
The first one is a front page of
google.com, a Turkey Doodle

00:02:35.530 --> 00:02:38.020
from Thanksgiving
two years ago.

00:02:38.020 --> 00:02:39.220
This is an interesting
one because it was

00:02:39.220 --> 00:02:40.540
written by a Go newcomer.

00:02:40.540 --> 00:02:43.540
He'd never written any Go code
before, but he picked it up,

00:02:43.540 --> 00:02:45.710
and he got this high-profile
app to

00:02:45.710 --> 00:02:47.740
launch in under 24 hours.

00:02:47.740 --> 00:02:51.020
So if you're thinking to
yourself, I love App Engine,

00:02:51.020 --> 00:02:53.710
but I'm a Python, or Java
programmer, or maybe a PHP

00:02:53.710 --> 00:02:55.290
programmer now.

00:02:55.290 --> 00:02:56.460
What's Go got in for me?

00:02:56.460 --> 00:02:57.840
Maybe it's too hard to learn.

00:02:57.840 --> 00:03:02.040
This is great evidence that
you can get from 0 to 100

00:03:02.040 --> 00:03:05.090
miles per hour with not
too much difficulty.

00:03:05.090 --> 00:03:07.920
Go's really easy to pick up.

00:03:07.920 --> 00:03:10.930
When this person built it, this
app is doing some image

00:03:10.930 --> 00:03:12.000
compositing.

00:03:12.000 --> 00:03:16.450
So you can see it's rendering
a graphic of a turkey with

00:03:16.450 --> 00:03:17.330
different feathers.

00:03:17.330 --> 00:03:21.180
You can see it animated from
that link down below.

00:03:21.180 --> 00:03:22.760
But it's doing image
work here.

00:03:22.760 --> 00:03:24.650
It's not just serving
static images.

00:03:24.650 --> 00:03:26.600
It's doing sophisticated
stuff.

00:03:26.600 --> 00:03:29.020
And this Go newcomer found that
he could build this app

00:03:29.020 --> 00:03:32.550
relatively easily and get half
the latency of an equivalent

00:03:32.550 --> 00:03:34.860
Python 2.7 version.

00:03:34.860 --> 00:03:37.880
As you can imagine, watching it
on google.com, the homepage

00:03:37.880 --> 00:03:40.140
produces a fair bit of
traffic, and this

00:03:40.140 --> 00:03:41.420
didn't miss a beat.

00:03:41.420 --> 00:03:42.650
And there's some more details
of how well it

00:03:42.650 --> 00:03:46.320
performed at that link.

00:03:46.320 --> 00:03:49.030
Santa Tracker for last year
was written in Go.

00:03:49.030 --> 00:03:52.130
And this was the component that
kept track of where Santa

00:03:52.130 --> 00:03:56.080
was and informed the client-side
JavaScript where

00:03:56.080 --> 00:03:57.580
to render Santa and how
many presents he had

00:03:57.580 --> 00:03:59.865
delivered, and so forth.

00:03:59.865 --> 00:04:03.190
It served a peak of nearly 5,000
QPS, so 5,000 queries

00:04:03.190 --> 00:04:04.230
per second.

00:04:04.230 --> 00:04:06.400
This is also a very
high-pressure situation.

00:04:06.400 --> 00:04:08.645
If things aren't going right, he
can't just tell people come

00:04:08.645 --> 00:04:11.115
back tomorrow for something
like this.

00:04:11.115 --> 00:04:12.970
It's too late by then.

00:04:12.970 --> 00:04:14.910
We didn't make any
children cry.

00:04:14.910 --> 00:04:17.765
And you can see down at the
bottom, we did this with under

00:04:17.765 --> 00:04:19.170
80 instances.

00:04:19.170 --> 00:04:21.390
So this is a lot fewer than you
might expect for this kind

00:04:21.390 --> 00:04:22.640
of query volume.

00:04:24.970 --> 00:04:28.500
So let me introduce to
you Gopher Mart.

00:04:28.500 --> 00:04:31.310
Gopher Mart is a little sample
app, and I'll have a link at

00:04:31.310 --> 00:04:36.750
the end of this presentation
for source code for it.

00:04:36.750 --> 00:04:38.940
It's a bit of a silly app.

00:04:38.940 --> 00:04:41.000
But it kind of demonstrates
enough of the features for us

00:04:41.000 --> 00:04:43.620
to discuss building a really
high-performance app.

00:04:47.230 --> 00:04:49.450
So here's the situation.

00:04:49.450 --> 00:04:53.805
Imagine you are running an app
to manage or to run, indeed,

00:04:53.805 --> 00:04:56.010
your Gopher Mart, which is your
one-stop shop for all

00:04:56.010 --> 00:04:57.700
your gopher needs.

00:04:57.700 --> 00:04:59.680
You gophers can come in, get
their gopher bran, their

00:04:59.680 --> 00:05:02.990
gopher flakes, their gopher
liter, load them up in a cart,

00:05:02.990 --> 00:05:08.850
take them to a checkout, and
then take them home obviously.

00:05:08.850 --> 00:05:12.450
And the correspondence to an app
that we're going to use is

00:05:12.450 --> 00:05:15.030
that the gophers that are doing
the shopping are your

00:05:15.030 --> 00:05:17.210
user requests.

00:05:17.210 --> 00:05:18.340
Each of your checkout
gophers--

00:05:18.340 --> 00:05:21.050
the gophers processing each
of those requests--

00:05:21.050 --> 00:05:22.320
is an app instance.

00:05:22.320 --> 00:05:25.000
And then there's the scheduler
gopher that's got a direct

00:05:25.000 --> 00:05:28.316
user request to app instances.

00:05:28.316 --> 00:05:30.410
A checkout gopher can
only deal with one

00:05:30.410 --> 00:05:33.340
request at a time.

00:05:33.340 --> 00:05:37.180
And checkout gophers can be
hired or fired in our Gopher

00:05:37.180 --> 00:05:39.250
Mart, but there is a certain
overhead to this.

00:05:39.250 --> 00:05:43.460
This models how we deal with
App Engine instances.

00:05:43.460 --> 00:05:45.110
They take a little bit
of time to start up.

00:05:45.110 --> 00:05:47.070
They're very quick to start up
and go, but it's still a

00:05:47.070 --> 00:05:48.500
nonzero amount of time.

00:05:48.500 --> 00:05:50.410
And they can't be fired
straight away.

00:05:50.410 --> 00:05:54.260
Our Gopher Mart has serious
industrial relations laws.

00:05:54.260 --> 00:05:57.000
You need to keep your checkout
gophers around for at least 15

00:05:57.000 --> 00:05:58.780
minutes after they've
been hired.

00:06:01.340 --> 00:06:05.150
But as in our Gopher Mart, our
gophers don't want to be

00:06:05.150 --> 00:06:06.660
standing in queue with
their gopher bran.

00:06:06.660 --> 00:06:08.640
They want to get home and eat
their gopher bran-- pour

00:06:08.640 --> 00:06:11.250
gopher milk all over it, scoop
it out of their gopher bowls.

00:06:11.250 --> 00:06:12.560
They don't want to be standing
in queues waiting

00:06:12.560 --> 00:06:13.490
for all this to happen.

00:06:13.490 --> 00:06:15.640
They'll get kind of grumpy
if we take too long.

00:06:15.640 --> 00:06:17.770
So we're going to look at
different ways of making our

00:06:17.770 --> 00:06:19.590
app really fly.

00:06:19.590 --> 00:06:20.840
I'm going to be looking at five
different techniques.

00:06:23.410 --> 00:06:25.180
But before we look at any
techniques, we have to

00:06:25.180 --> 00:06:27.680
understand the way
that we approach

00:06:27.680 --> 00:06:29.280
high-performance apps.

00:06:29.280 --> 00:06:31.240
And it's very easy to fool
ourselves into thinking that

00:06:31.240 --> 00:06:34.500
we know where the
slow bits are.

00:06:34.500 --> 00:06:35.880
Whenever you're doing
performance work, you need to

00:06:35.880 --> 00:06:38.400
first measure to understand
where the slow bits are.

00:06:38.400 --> 00:06:41.910
There's no point of spending
months of your time tuning,

00:06:41.910 --> 00:06:46.070
and tweaking, and caching, and
doing all kinds of things to

00:06:46.070 --> 00:06:48.750
one bit of code if that bit of
code isn't your slow bit.

00:06:48.750 --> 00:06:51.890
So you need to measure first.

00:06:51.890 --> 00:06:54.310
It's also important to
measure periodically.

00:06:54.310 --> 00:06:57.390
Your app will behave differently
based on different

00:06:57.390 --> 00:07:00.120
request loads, different types
of request processing, what

00:07:00.120 --> 00:07:01.960
you're trying to do
in each request.

00:07:01.960 --> 00:07:06.160
So it's important to come back
to measuring how fast it is

00:07:06.160 --> 00:07:08.900
over time because it will change
as you change your app.

00:07:08.900 --> 00:07:10.960
Even if you don't change your
app, the infrastructure

00:07:10.960 --> 00:07:13.760
underneath will change over time
as well and almost always

00:07:13.760 --> 00:07:15.010
for the better.

00:07:15.010 --> 00:07:17.340
So App Engine gets better
at scheduling

00:07:17.340 --> 00:07:19.070
instances to app servers.

00:07:19.070 --> 00:07:21.610
It'll get better at getting
files to the app servers,

00:07:21.610 --> 00:07:23.920
starting them up quickly,
and so on.

00:07:23.920 --> 00:07:26.240
Go got a lot faster.

00:07:26.240 --> 00:07:29.030
Just a few days ago when we
announced Go 1.1, that was

00:07:29.030 --> 00:07:33.650
major milestone, our second
major stable release, that

00:07:33.650 --> 00:07:36.030
brought a lot of performance
improvements.

00:07:36.030 --> 00:07:40.170
We got emails to the mailing
list about people seeing their

00:07:40.170 --> 00:07:42.570
programs written in Go dropping
from like 24 hours

00:07:42.570 --> 00:07:44.640
runtime to 9 hours runtime.

00:07:44.640 --> 00:07:47.210
So there's startling differences
in every release

00:07:47.210 --> 00:07:49.780
of things, so you need to
periodically come back to

00:07:49.780 --> 00:07:51.620
remeasure your app to understand
what bits are slow

00:07:51.620 --> 00:07:52.870
and what bits aren't.

00:07:56.250 --> 00:08:00.420
The tool that we're going to be
using to get a feel for how

00:08:00.420 --> 00:08:03.100
our app performs is
called Appstats.

00:08:03.100 --> 00:08:06.560
And what Appstats does is it
traces API RPCs over the

00:08:06.560 --> 00:08:07.580
course of requests.

00:08:07.580 --> 00:08:10.960
It measures the start time and
the stop time, what these

00:08:10.960 --> 00:08:13.040
requests are doing, and also
records the stack trace so you

00:08:13.040 --> 00:08:16.570
can dig in through a web
interface to be able to see

00:08:16.570 --> 00:08:20.130
where this RPC came from, how
long it took, any errors that

00:08:20.130 --> 00:08:22.040
happened, and so on.

00:08:22.040 --> 00:08:25.730
And as you can see here, here's
a -- timeline, with

00:08:25.730 --> 00:08:27.430
time running along
the x-axis--

00:08:27.430 --> 00:08:29.910
of my first version
of this app.

00:08:29.910 --> 00:08:33.480
Now, I wrote it as simply as
possible with no thoughts to

00:08:33.480 --> 00:08:35.340
performance, just simplicity.

00:08:35.340 --> 00:08:36.480
And you can see it's
pretty slow.

00:08:36.480 --> 00:08:39.210
It's taking nearly
400 milliseconds.

00:08:39.210 --> 00:08:41.530
And what it's doing
is, the gopher

00:08:41.530 --> 00:08:43.750
comes up to the checkout.

00:08:43.750 --> 00:08:48.150
The checkout gopher has to scan
each item that was in

00:08:48.150 --> 00:08:50.720
their cart, and then emails
them a receipt afterwards.

00:08:50.720 --> 00:08:53.515
This is a fairly modern
Gopher Mart.

00:08:53.515 --> 00:08:55.750
So instead of just giving them
a printed receipt, they'll

00:08:55.750 --> 00:08:59.440
just get it in their email
later on that day.

00:08:59.440 --> 00:09:00.570
So we're going to look
at different ways of

00:09:00.570 --> 00:09:03.170
speeding this up.

00:09:03.170 --> 00:09:04.190
So I'm going to be
covering five

00:09:04.190 --> 00:09:05.780
performance techniques today.

00:09:05.780 --> 00:09:07.290
These aren't comprehensive.

00:09:07.290 --> 00:09:10.120
There's plenty more that you
can think of, and each one,

00:09:10.120 --> 00:09:13.940
I'll be only giving a bit of a
flavor for the depth that you

00:09:13.940 --> 00:09:17.270
can go into to fully realize
these techniques.

00:09:17.270 --> 00:09:19.090
But these should be five
fairly representative

00:09:19.090 --> 00:09:20.972
techniques of how to
tackle these kind

00:09:20.972 --> 00:09:26.610
of performance problems.

00:09:26.610 --> 00:09:30.080
First one to do is
deferring work.

00:09:30.080 --> 00:09:32.790
Not all work needs to be done
during the request.

00:09:32.790 --> 00:09:36.270
In our Gopher Mart situation
when the gophers come up to

00:09:36.270 --> 00:09:38.555
the checkout and are checking
out their items, they don't

00:09:38.555 --> 00:09:42.416
need the email of the receipt
right then and there.

00:09:42.416 --> 00:09:44.790
They're going to take their cart
out to their gopher car,

00:09:44.790 --> 00:09:47.030
load up the gopher boot with
their gopher bran, drive to

00:09:47.030 --> 00:09:47.820
the gopher home.

00:09:47.820 --> 00:09:49.055
It's going to take them
a little while.

00:09:49.055 --> 00:09:52.250
So the importance of getting the
mail out to them straight

00:09:52.250 --> 00:09:54.800
away is not so important.

00:09:54.800 --> 00:09:57.900
And in general, a lot of web
apps need to do things as a

00:09:57.900 --> 00:10:02.390
consequence of a request, but
don't need to do them during

00:10:02.390 --> 00:10:03.720
the request.

00:10:03.720 --> 00:10:07.010
So this may seem like a pretty
simplistic technique, and

00:10:07.010 --> 00:10:10.110
we'll get on to slightly more
sophisticated ones shortly.

00:10:10.110 --> 00:10:11.895
But it's an important
one to remember.

00:10:11.895 --> 00:10:13.370
Lots of the time, you can
forget that you're doing

00:10:13.370 --> 00:10:16.030
things that you don't
need to be doing.

00:10:16.030 --> 00:10:18.220
The main ways that we're going
to going do this with the Go

00:10:18.220 --> 00:10:22.340
App Engine API is with the Task
Queue API or the delay

00:10:22.340 --> 00:10:25.150
package that is built over
the Task Queue API.

00:10:25.150 --> 00:10:28.210
And we're going to use this to
shift work that it needs to do

00:10:28.210 --> 00:10:29.790
outside the request scope.

00:10:29.790 --> 00:10:32.870
In our Gopher Mart situation,
we're going to shift that slow

00:10:32.870 --> 00:10:36.720
mail lots end call that took a
large chunk of our timeline.

00:10:36.720 --> 00:10:39.920
And we're going to do that in a
task that's going to happen

00:10:39.920 --> 00:10:41.750
usually very soon afterwards.

00:10:41.750 --> 00:10:43.840
But we're going to replace
it with a very quick

00:10:43.840 --> 00:10:44.890
taskqueue.add.

00:10:44.890 --> 00:10:47.340
And we're going to use
the delay package.

00:10:47.340 --> 00:10:49.250
And here's how the delay package
works in case you

00:10:49.250 --> 00:10:50.230
haven't seen it before.

00:10:50.230 --> 00:10:53.810
What it does is it
wraps a function.

00:10:53.810 --> 00:10:56.170
You just go to try and
invoke that function.

00:10:56.170 --> 00:10:59.740
And what it'll do behind the
scenes is it will marshall the

00:10:59.740 --> 00:11:04.350
function arguments, store them
in a Task Queue task, add it

00:11:04.350 --> 00:11:07.100
to the queue, and then when it
comes back, it'll invoke your

00:11:07.100 --> 00:11:08.480
function for you.

00:11:08.480 --> 00:11:12.080
So it's a very, very simple way
of dealing with Task Queue

00:11:12.080 --> 00:11:14.760
when you're doing it
to defer work.

00:11:14.760 --> 00:11:19.750
So you can see at the top, two
lines of my code for my simple

00:11:19.750 --> 00:11:22.660
version there's a sendReceipt
function invocation.

00:11:22.660 --> 00:11:24.520
And then you can see just the
function signature of

00:11:24.520 --> 00:11:25.760
sendReceipt.

00:11:25.760 --> 00:11:27.440
It's fairly simple.

00:11:27.440 --> 00:11:30.740
All we have to do to use the
delay package is import

00:11:30.740 --> 00:11:31.990
appengine/delay.

00:11:33.420 --> 00:11:36.170
And then we turn our sendReceipt
invocation to

00:11:36.170 --> 00:11:37.750
stick a dot Call in there.

00:11:37.750 --> 00:11:40.090
Nothing else has to
change there.

00:11:40.090 --> 00:11:43.430
Then we turn our function
definition into a variable

00:11:43.430 --> 00:11:45.040
definition.

00:11:45.040 --> 00:11:49.040
And we just use delay.func, give
it a name, and then we

00:11:49.040 --> 00:11:51.340
have our function body
exactly as is.

00:11:51.340 --> 00:11:53.840
So there's no difference from
what we were doing before.

00:11:53.840 --> 00:11:55.170
And you can see the effect
that it has.

00:11:55.170 --> 00:11:57.140
It shrunk that mail.send call.

00:11:57.140 --> 00:11:59.820
It be a bit slow, and it might
be waiting on the mail

00:11:59.820 --> 00:12:02.910
protocol to start so it can
verify that it can send mail,

00:12:02.910 --> 00:12:05.270
spam filters, whatever it is.

00:12:05.270 --> 00:12:07.810
And it's turned into a very
quick Task Queue call.

00:12:07.810 --> 00:12:10.470
So this is a lot quicker, and
then the mail will happen in

00:12:10.470 --> 00:12:12.100
its own Task Queue request.

00:12:14.890 --> 00:12:18.370
Second technique I want to
talk about is batching.

00:12:18.370 --> 00:12:22.380
In our Gopher Mart scenario,
it kind of makes sense that

00:12:22.380 --> 00:12:24.480
you don't want a family of
10 gophers all going in

00:12:24.480 --> 00:12:27.210
individually, picking up their
own box of gopher flakes,

00:12:27.210 --> 00:12:29.700
walking up to the counter, each
going through the process

00:12:29.700 --> 00:12:31.930
of checking them out, paying
for them, and then walking

00:12:31.930 --> 00:12:32.670
home with them.

00:12:32.670 --> 00:12:36.450
It makes a lot more sense for
one of the gophers to go in by

00:12:36.450 --> 00:12:39.910
themselves, load up their cart
with 10 boxes of gopher

00:12:39.910 --> 00:12:42.800
flakes, and then check
them out all at once.

00:12:42.800 --> 00:12:44.400
And again, this is a relatively
simple technique,

00:12:44.400 --> 00:12:45.650
but it's often overlooked.

00:12:48.610 --> 00:12:51.440
This would be using API calls
like GetMulti instead of Get,

00:12:51.440 --> 00:12:54.910
or PutMulti instead of Put.

00:12:54.910 --> 00:12:56.800
Often they're a little bit
more overhead than just a

00:12:56.800 --> 00:12:58.720
single call if you've only
got a single object.

00:12:58.720 --> 00:13:01.530
But a lot time, you're dealing
with multiple entities.

00:13:01.530 --> 00:13:05.540
And so they can be a lot quicker
because you only have

00:13:05.540 --> 00:13:09.370
one lot of the RPC overhead
instead of in.

00:13:09.370 --> 00:13:11.490
So here you can see the
code that I wrote

00:13:11.490 --> 00:13:13.352
for the simple version.

00:13:13.352 --> 00:13:15.640
You can see that it's just
sitting in a loop.

00:13:15.640 --> 00:13:19.350
And for each key it goes and
fetches the item corresponding

00:13:19.350 --> 00:13:21.066
to that key from
the data store.

00:13:21.066 --> 00:13:23.840
It deals with any error, and
then just builds up a slice of

00:13:23.840 --> 00:13:25.160
those items.

00:13:25.160 --> 00:13:27.140
That's obviously very simple
to understand and to reason

00:13:27.140 --> 00:13:30.760
about, but it's slow.

00:13:30.760 --> 00:13:32.740
And here's what we do
when we batch it.

00:13:35.620 --> 00:13:37.760
Instead of a loop where we're
doing each data store call, we

00:13:37.760 --> 00:13:40.040
use datastore.GetMulti.

00:13:40.040 --> 00:13:42.630
We tell it all the keys to go
and get, and then it returns

00:13:42.630 --> 00:13:44.600
all the items for us.

00:13:44.600 --> 00:13:48.140
And you can see, in this case,
the task you call was very

00:13:48.140 --> 00:13:49.380
luckily short.

00:13:49.380 --> 00:13:52.790
But more importantly the
datastore.Get call turned from

00:13:52.790 --> 00:13:58.170
five sequential calls into
just one short one.

00:13:58.170 --> 00:14:01.560
So we've already cut down our
program's request processing

00:14:01.560 --> 00:14:02.810
time dramatically.

00:14:05.330 --> 00:14:08.380
Third technique I want to
talk about is caching.

00:14:08.380 --> 00:14:11.170
And I'm sure many of you have
cached things in the past.

00:14:11.170 --> 00:14:15.840
And in our Gopher Mart scenario,
this would be like a

00:14:15.840 --> 00:14:19.280
checkout gopher remembering
that for a certain fresh

00:14:19.280 --> 00:14:21.170
fruit-- which might not have bar
codes, and they normally

00:14:21.170 --> 00:14:22.830
have to look up a
booklet for--

00:14:22.830 --> 00:14:26.380
they just remember gopher
berries, 361.

00:14:26.380 --> 00:14:28.620
Gopher bananas, 492.

00:14:28.620 --> 00:14:30.700
They can just remember these
things once they've been

00:14:30.700 --> 00:14:34.880
running for a while, and they
can be a lot quicker.

00:14:34.880 --> 00:14:36.290
But the first layer
that I want to

00:14:36.290 --> 00:14:37.150
talk about is Datastore.

00:14:37.150 --> 00:14:40.120
And Datastore is a great
storage system.

00:14:40.120 --> 00:14:40.910
It's very fast.

00:14:40.910 --> 00:14:41.560
It's very reliable.

00:14:41.560 --> 00:14:43.080
It's highly scalable.

00:14:43.080 --> 00:14:45.180
But it also can be
used as a cache.

00:14:45.180 --> 00:14:48.380
So your program might be doing
expensive computation work.

00:14:48.380 --> 00:14:50.090
It might be compositing
images.

00:14:50.090 --> 00:14:55.880
It might be fetching a request
via URL fetch from a remote

00:14:55.880 --> 00:14:57.200
server that's slow.

00:14:57.200 --> 00:14:59.565
It could be doing any number of
things outside itself that

00:14:59.565 --> 00:15:02.840
it has very little control over
that can be quite slow.

00:15:02.840 --> 00:15:05.055
We can stick those in the
Datastore, and then all our

00:15:05.055 --> 00:15:07.370
app instances can check the
Datastore before trying to do

00:15:07.370 --> 00:15:11.680
that expensive computation
or remote fetch.

00:15:11.680 --> 00:15:15.560
One thing even faster than
Datastore is Memcache.

00:15:15.560 --> 00:15:19.350
And as opposed to Datastore
where a small Get might take

00:15:19.350 --> 00:15:20.100
20 milliseconds--

00:15:20.100 --> 00:15:21.720
thereabouts--

00:15:21.720 --> 00:15:23.750
Memcache can do it in
about 1 millisecond.

00:15:23.750 --> 00:15:28.020
So it can often be 20 times
faster for small workloads.

00:15:28.020 --> 00:15:28.710
That's great.

00:15:28.710 --> 00:15:29.980
It's shared across all
your instances.

00:15:29.980 --> 00:15:34.040
You know, one of your checkout
gophers needs to look up the

00:15:34.040 --> 00:15:35.095
code for gopher berries.

00:15:35.095 --> 00:15:37.910
It stores it in your Memcache,
and every other checkout

00:15:37.910 --> 00:15:41.620
gopher suddenly gets the benefit
of that caching.

00:15:41.620 --> 00:15:43.750
You need to remember that,
unlike Datastore, Memcache is

00:15:43.750 --> 00:15:45.500
not guaranteed to hang around.

00:15:45.500 --> 00:15:49.460
It's not a persistent storage
system so it could get flushed

00:15:49.460 --> 00:15:54.240
any time your app has been using
Memcache too much or

00:15:54.240 --> 00:15:56.950
your App Engine infrastructure
moves your app to

00:15:56.950 --> 00:15:57.720
another data center.

00:15:57.720 --> 00:15:59.030
So you can't depend
on it being there.

00:15:59.030 --> 00:16:04.110
But it's a great way of keeping
hot used data at

00:16:04.110 --> 00:16:05.770
immediate access.

00:16:05.770 --> 00:16:09.400
Third thing to use, and
something that might be quite

00:16:09.400 --> 00:16:11.170
foreign to particularly
Python programmers,

00:16:11.170 --> 00:16:13.440
is using local memory.

00:16:13.440 --> 00:16:15.510
Your app is running as a program
on a real machine.

00:16:15.510 --> 00:16:18.390
And we can easily forget about
that with our abstract notions

00:16:18.390 --> 00:16:22.910
of the App Engine set up, and
instances, and requests, and

00:16:22.910 --> 00:16:24.260
it all feels very abstract.

00:16:24.260 --> 00:16:27.490
And we forget that they're
programs running on a machine

00:16:27.490 --> 00:16:29.930
and that machine has memory.

00:16:29.930 --> 00:16:32.300
So we can use local memory
in your app instance.

00:16:32.300 --> 00:16:34.270
It would just be sticking things
in a global variable,

00:16:34.270 --> 00:16:35.730
for example.

00:16:35.730 --> 00:16:36.710
And that's even faster.

00:16:36.710 --> 00:16:38.850
That's three orders of magnitude
faster than Memcache

00:16:38.850 --> 00:16:42.400
will usually be for
small amounts.

00:16:42.400 --> 00:16:44.810
It's even more fragile than
Memcache, though.

00:16:44.810 --> 00:16:46.610
It's not shared across
instances.

00:16:46.610 --> 00:16:49.610
So if one instance gets shut
down because it hasn't been

00:16:49.610 --> 00:16:52.270
receiving enough requests or
it's not needed, that memory

00:16:52.270 --> 00:16:53.802
will disappear.

00:16:53.802 --> 00:16:56.470
But it's an important tool in
your tool belt when your

00:16:56.470 --> 00:16:58.970
caching things to use
local memory.

00:16:58.970 --> 00:17:03.850
It can be heaps faster than
Memcache, heaps more reliable,

00:17:03.850 --> 00:17:05.329
with the proviso that
it will disappear.

00:17:09.009 --> 00:17:10.899
The third technique I want to
talk about it concurrency.

00:17:10.899 --> 00:17:13.190
And this is where Go
really shines.

00:17:13.190 --> 00:17:17.470
And this is where it
really excels as

00:17:17.470 --> 00:17:19.270
compared to other runtimes.

00:17:19.270 --> 00:17:22.890
Concurrency is where you can
decompose what your app is

00:17:22.890 --> 00:17:26.579
doing into independently
executing things.

00:17:26.579 --> 00:17:30.370
So even though Go on App Engine
is, at the moment,

00:17:30.370 --> 00:17:34.630
bound to a single CPU thread,
concurrency allows you to

00:17:34.630 --> 00:17:37.640
decompose your program so that
you can write very simple

00:17:37.640 --> 00:17:41.550
chunks of code that are
run concurrently.

00:17:41.550 --> 00:17:42.640
They might not run
in parallel.

00:17:42.640 --> 00:17:45.250
But while one is blocked on an
API request, another one will

00:17:45.250 --> 00:17:47.310
immediately start running.

00:17:47.310 --> 00:17:49.990
And in our Gopher Mart scenario,
this would be as if

00:17:49.990 --> 00:17:53.250
a checkout gopher needs to get
a price check on an item and

00:17:53.250 --> 00:17:56.420
then can immediately start
scanning the other items.

00:17:56.420 --> 00:17:58.260
If you walk into a regular
supermarket, you don't see

00:17:58.260 --> 00:18:00.380
somebody waiting for a price
check, and then just standing

00:18:00.380 --> 00:18:03.480
there waiting, getting the
price check back and then

00:18:03.480 --> 00:18:04.180
doing the other items.

00:18:04.180 --> 00:18:05.850
They go on and do more things.

00:18:05.850 --> 00:18:07.140
And so that's concurrent work.

00:18:10.190 --> 00:18:13.160
So let me take a bit of our code
from our example app that

00:18:13.160 --> 00:18:17.660
was poorly written by me, and
let's make it better and

00:18:17.660 --> 00:18:19.620
faster by using concurrency.

00:18:19.620 --> 00:18:22.490
So here might be a little chunk
of code that needs to

00:18:22.490 --> 00:18:25.880
look up all the entities
of two Datastore kinds.

00:18:25.880 --> 00:18:28.990
In this case, we have a list and
an item kind, and we want

00:18:28.990 --> 00:18:32.570
to get all the items of each.

00:18:32.570 --> 00:18:35.330
This is a straight line code,
and it's very slow.

00:18:35.330 --> 00:18:38.380
If each query takes a certain
amount of time, the total time

00:18:38.380 --> 00:18:39.750
to run this bit of code
is going to be

00:18:39.750 --> 00:18:41.130
the sum of the times.

00:18:41.130 --> 00:18:43.560
It'd be much, much quicker if we
could run them in parallel.

00:18:47.140 --> 00:18:51.690
And since most of your web apps,
most of all web apps,

00:18:51.690 --> 00:18:54.790
tend to be bound waiting on API
calls to come back from

00:18:54.790 --> 00:18:57.650
various other services, this
is an easy way to speed up

00:18:57.650 --> 00:19:00.210
most requests by doing
things concurrently.

00:19:00.210 --> 00:19:03.290
So here all we've had to do is
kick off two Go routines to

00:19:03.290 --> 00:19:07.930
run these queries, and they'll
be independently waiting.

00:19:07.930 --> 00:19:10.370
Then we're using an error
channel to signal

00:19:10.370 --> 00:19:11.100
when they're done.

00:19:11.100 --> 00:19:13.950
Hopefully it'll send back
nil if there's no error.

00:19:13.950 --> 00:19:15.360
And then, down at the bottom,
we just wait for

00:19:15.360 --> 00:19:18.230
both of them to return.

00:19:18.230 --> 00:19:22.140
This means that instead of two
requests summing together, we

00:19:22.140 --> 00:19:26.130
now end up taking time roughly
equivalent of the maximum

00:19:26.130 --> 00:19:27.800
length of either of them.

00:19:27.800 --> 00:19:31.650
So that's going to be strictly
less than the sum in any

00:19:31.650 --> 00:19:32.800
reasonable scenario.

00:19:32.800 --> 00:19:36.460
So this is a very simple flavor
of concurrency in Go.

00:19:36.460 --> 00:19:38.360
This is far from
sophisticated.

00:19:38.360 --> 00:19:41.140
So I'll commend you, my
colleague, Sameer Ajmani, and

00:19:41.140 --> 00:19:43.710
his talk this afternoon where
he'll be talking about some

00:19:43.710 --> 00:19:45.910
advanced Go concurrency
patterns.

00:19:45.910 --> 00:19:48.465
So if this kind of thing sounds
interesting to you, and

00:19:48.465 --> 00:19:50.600
it does to me, get along
to that talk.

00:19:58.940 --> 00:20:00.320
The next technique I
want to talk about

00:20:00.320 --> 00:20:02.980
is controlling variance.

00:20:02.980 --> 00:20:06.150
And in our Gopher Mart,
we have a 10

00:20:06.150 --> 00:20:07.930
items or fewer queue.

00:20:07.930 --> 00:20:10.810
But occasionally a gopher comes
by when they've got a

00:20:10.810 --> 00:20:13.660
few more than 10 items, if
you know what I mean.

00:20:13.660 --> 00:20:17.740
And while most gophers might
take a millisecond to process,

00:20:17.740 --> 00:20:20.000
one in 100 might take
10 times that.

00:20:20.000 --> 00:20:22.560
It might take 100 milliseconds,
for example.

00:20:22.560 --> 00:20:24.990
And this is a very
common thing.

00:20:24.990 --> 00:20:26.980
It could be due to
the request.

00:20:26.980 --> 00:20:29.330
So in our Gopher Mart situation,
it might just be

00:20:29.330 --> 00:20:31.920
the occasional gopher with a
very, very big cart full of

00:20:31.920 --> 00:20:36.650
items, whereas most carts
only have a few items.

00:20:36.650 --> 00:20:38.460
But it can also be due
to the infrastructure

00:20:38.460 --> 00:20:39.710
underneath your app.

00:20:42.310 --> 00:20:44.950
And the App Engine
infrastructure is great.

00:20:44.950 --> 00:20:46.080
It's highly scalable.

00:20:46.080 --> 00:20:46.590
It's fast.

00:20:46.590 --> 00:20:47.615
It's dependable.

00:20:47.615 --> 00:20:48.770
It's very reliable.

00:20:48.770 --> 00:20:50.820
But it's not perfect.

00:20:50.820 --> 00:20:52.465
Occasionally you'll be doing
Datastore requests, and

00:20:52.465 --> 00:20:54.820
they'll be going 20
milliseconds, 25 milliseconds,

00:20:54.820 --> 00:20:57.800
22 milliseconds, be really
reliably quick.

00:20:57.800 --> 00:21:02.260
But then a small fraction of it
might take half a second.

00:21:08.490 --> 00:21:12.280
And the consequences of this
kind of variance can cascade

00:21:12.280 --> 00:21:15.850
through our system if
it's sophisticated.

00:21:15.850 --> 00:21:18.670
If your app needs to do several
things in a sequence,

00:21:18.670 --> 00:21:20.830
where each one depends on the
results of the previous one

00:21:20.830 --> 00:21:24.050
such that these things can't be
done concurrently, then a

00:21:24.050 --> 00:21:26.670
variance in one affects
the whole pipeline.

00:21:26.670 --> 00:21:30.660
And the perfect storm of
variance affecting all your

00:21:30.660 --> 00:21:33.990
items in this sequence
will blow out your

00:21:33.990 --> 00:21:36.360
request time hugely.

00:21:36.360 --> 00:21:39.930
Variable requests also make
scheduling a lot harder.

00:21:39.930 --> 00:21:41.955
And this something that I
haven't really touched on

00:21:41.955 --> 00:21:43.560
much, and I won't go into
too much detail.

00:21:43.560 --> 00:21:48.330
But the App Engine scheduler
needs to take a guess as to

00:21:48.330 --> 00:21:52.420
which App Engine instance
each request will

00:21:52.420 --> 00:21:53.660
best be served by.

00:21:53.660 --> 00:21:55.130
It's got a balance
across them.

00:21:55.130 --> 00:21:56.980
If it's getting too much
traffic, it'll start up new

00:21:56.980 --> 00:21:59.170
instances and so on
automatically.

00:21:59.170 --> 00:22:01.920
But it needs to usually, for the
most part, take a guess as

00:22:01.920 --> 00:22:05.780
to which instance will be idle
the first so it can add that

00:22:05.780 --> 00:22:10.510
request to that App Engine
instance's queue.

00:22:10.510 --> 00:22:13.640
The less predictable your
request handling is, though,

00:22:13.640 --> 00:22:16.270
the hard it is to do that
kind of prediction.

00:22:16.270 --> 00:22:20.840
So if there's variance in your
request handling, the

00:22:20.840 --> 00:22:23.740
scheduler, though it is great
and highly sophisticated, will

00:22:23.740 --> 00:22:25.600
start to make mistakes
more often.

00:22:25.600 --> 00:22:28.570
And so you might get your
instances being a bit

00:22:28.570 --> 00:22:28.860
overbalanced.

00:22:28.860 --> 00:22:32.880
You know, one handling 100
requests for only two being

00:22:32.880 --> 00:22:34.150
handled by another, and so on.

00:22:34.150 --> 00:22:35.530
So it might make
mistakes there.

00:22:35.530 --> 00:22:39.920
And this will lead to you
requiring more app instances,

00:22:39.920 --> 00:22:43.040
more instance hours, which will
lead to a higher bill if

00:22:43.040 --> 00:22:45.120
you've got billing enabled.

00:22:45.120 --> 00:22:47.590
So all we want to do
is control the

00:22:47.590 --> 00:22:48.630
variance of our requests.

00:22:48.630 --> 00:22:52.205
We want to put an upper limit
on how long we're going to

00:22:52.205 --> 00:22:54.280
spend doing certain
operations.

00:22:54.280 --> 00:23:00.909
I'll point you at this paper
publishing the ACM by two guys

00:23:00.909 --> 00:23:01.490
from Google.

00:23:01.490 --> 00:23:04.810
This goes into a lot of detail
about some techniques for

00:23:04.810 --> 00:23:09.010
controlling this kind of
variance, in particular, long

00:23:09.010 --> 00:23:10.810
tail latency.

00:23:10.810 --> 00:23:14.740
And it's geared around machines
in a data center

00:23:14.740 --> 00:23:15.990
doing server processing.

00:23:15.990 --> 00:23:19.060
But the same techniques apply
to web apps that are

00:23:19.060 --> 00:23:21.980
processing requests as well.

00:23:21.980 --> 00:23:27.280
So I'm going to discuss just
one way of doing this.

00:23:27.280 --> 00:23:31.500
So I'll point you to that paper
to go into more detail.

00:23:31.500 --> 00:23:33.620
Storing in Memcache is usually
an optimization.

00:23:33.620 --> 00:23:37.350
So as I mentioned before,
Memcache is not perfect, and

00:23:37.350 --> 00:23:39.360
it won't necessarily guarantee
to hold your data.

00:23:39.360 --> 00:23:42.037
If you need guaranteed data
storage, you need to store it

00:23:42.037 --> 00:23:43.770
in something like Datastore.

00:23:43.770 --> 00:23:48.740
But Memcache, as a result, is
usually an optimization.

00:23:48.740 --> 00:23:52.910
And optimizations by their
nature are usually optional.

00:23:52.910 --> 00:23:57.112
So what we want to do is, in our
request, we don't want to

00:23:57.112 --> 00:23:59.780
spend huge amounts of time,
unlimited amounts of time, on

00:23:59.780 --> 00:24:02.990
these things that aren't
strictly required.

00:24:02.990 --> 00:24:06.526
So in our Gopher Mart scenario,
we might be doing a

00:24:06.526 --> 00:24:09.190
bunch of request processing, and
then we might not want to

00:24:09.190 --> 00:24:13.110
store those fresh fruit items
that we've looked up out of

00:24:13.110 --> 00:24:14.700
the booklet in Memcache.

00:24:14.700 --> 00:24:16.170
But we don't want to spend
an infinite amount

00:24:16.170 --> 00:24:17.150
of time doing that.

00:24:17.150 --> 00:24:20.210
You know, Memcache might be
briefly down, or it might be

00:24:20.210 --> 00:24:23.330
running a bit slowly,
or whatever it is.

00:24:23.330 --> 00:24:25.460
So we want to cap the amount
of time that we'll spend

00:24:25.460 --> 00:24:27.260
waiting on it.

00:24:27.260 --> 00:24:29.570
And here's the first approach
that does not work.

00:24:33.340 --> 00:24:35.600
In our request handler, we
might just kick off a

00:24:35.600 --> 00:24:40.000
memcache.Set operation at the
bottom in its own Go routine.

00:24:40.000 --> 00:24:41.120
We think, that's great.

00:24:41.120 --> 00:24:43.890
My request will immediately
return and Memcache will do

00:24:43.890 --> 00:24:45.620
its thing behind the scenes.

00:24:45.620 --> 00:24:49.450
But the problem here is that,
in the App Engine model, the

00:24:49.450 --> 00:24:52.770
scope of your context objects
are bound to the

00:24:52.770 --> 00:24:53.890
scope of the request.

00:24:53.890 --> 00:24:56.700
So any API calls have to be
done while the request is

00:24:56.700 --> 00:24:58.160
still going.

00:24:58.160 --> 00:25:03.850
And as soon as the HTTP handler
returns here, your

00:25:03.850 --> 00:25:07.110
context will be invalidated, and
any outstanding API calls

00:25:07.110 --> 00:25:09.730
will be canceled.

00:25:09.730 --> 00:25:11.810
So we can't do this.

00:25:11.810 --> 00:25:14.750
Here's what we can do.

00:25:14.750 --> 00:25:17.090
We can again kick off the
Memcache operation

00:25:17.090 --> 00:25:18.860
concurrently.

00:25:18.860 --> 00:25:20.590
And then we use a
done channel.

00:25:20.590 --> 00:25:24.520
And this done channel is going
to be signaled on as soon as

00:25:24.520 --> 00:25:27.420
that Memcache operation
returns.

00:25:27.420 --> 00:25:30.260
And then down at the bottom,
we use Go's concurrency

00:25:30.260 --> 00:25:33.350
primitive called a Select block
to wait for one of two

00:25:33.350 --> 00:25:34.940
channel operations to happen.

00:25:34.940 --> 00:25:36.880
The first one, if the
Memcache operation

00:25:36.880 --> 00:25:38.910
finishes, we go through.

00:25:38.910 --> 00:25:40.470
There's nothing in the select
body, so nothing's going to

00:25:40.470 --> 00:25:43.770
happen but will just immediately
return from the

00:25:43.770 --> 00:25:44.830
HTTP handler.

00:25:44.830 --> 00:25:48.760
So in the likely event that
Memcache is its usual quick

00:25:48.760 --> 00:25:53.160
and speedy self, this
has no overhead.

00:25:53.160 --> 00:25:55.080
The second case, though,
is if we end up

00:25:55.080 --> 00:25:56.320
waiting a bit too long.

00:25:56.320 --> 00:26:00.250
And Memcache will usually be 1
millisecond, 2 milliseconds,

00:26:00.250 --> 00:26:02.770
depending on what you're doing
with it and how much data is

00:26:02.770 --> 00:26:04.340
being sloshed around.

00:26:04.340 --> 00:26:06.050
If it takes more than 3
milliseconds that we've

00:26:06.050 --> 00:26:10.370
decided we want to wait for,
time.After will signal on the

00:26:10.370 --> 00:26:12.920
channel the returns after
3 milliseconds.

00:26:12.920 --> 00:26:16.340
So at that point, if Memcache
takes too long, that second

00:26:16.340 --> 00:26:19.730
channel will be signal on we
drop as like we've and return

00:26:19.730 --> 00:26:22.330
and that point the Memcache
operation will be canceled.

00:26:22.330 --> 00:26:23.170
It will fail.

00:26:23.170 --> 00:26:24.360
It will go away.

00:26:24.360 --> 00:26:26.540
And we've returned
to the user.

00:26:26.540 --> 00:26:30.840
So using these kind of
primitives to timeout waiting

00:26:30.840 --> 00:26:35.230
for things that are optional
is a useful technique to

00:26:35.230 --> 00:26:37.020
ensure high-performance
consistently.

00:26:41.550 --> 00:26:45.800
So here's an overview
of where we went.

00:26:45.800 --> 00:26:48.070
We started off with a very
poorly written app on my

00:26:48.070 --> 00:26:50.810
behalf that did very
simplicity code.

00:26:50.810 --> 00:26:53.100
It just did a sequence of
Datastore operations, and then

00:26:53.100 --> 00:26:56.470
a mail.send and took nearly
400 milliseconds.

00:26:56.470 --> 00:26:58.210
Just by applying two of the
techniques that we did today,

00:26:58.210 --> 00:27:00.700
we got it down to only
70 milliseconds.

00:27:00.700 --> 00:27:04.140
So a dramatic speedup with
very little work.

00:27:04.140 --> 00:27:05.720
Once you've got experience
with it, these kinds of

00:27:05.720 --> 00:27:07.790
transformations will take
5 to 10 minutes.

00:27:07.790 --> 00:27:11.250
So it can very quickly done.

00:27:11.250 --> 00:27:13.245
So let me sum up what
we covered today.

00:27:13.245 --> 00:27:16.210
The first thing is to
find the performance

00:27:16.210 --> 00:27:17.330
bottlenecks in your code.

00:27:17.330 --> 00:27:19.840
Don't rely on your own
intuition, because your

00:27:19.840 --> 00:27:23.220
intuition will fool
you reliably.

00:27:23.220 --> 00:27:26.770
And, even if your intuition is
correct one day, it might be

00:27:26.770 --> 00:27:27.880
incorrect down the road.

00:27:27.880 --> 00:27:31.340
So the important thing to note
is to measure, and measure,

00:27:31.340 --> 00:27:33.100
and measure, and keep coming
back to measuring to

00:27:33.100 --> 00:27:35.620
understand your app's
performance.

00:27:35.620 --> 00:27:37.190
Then the five techniques
that I covered--

00:27:37.190 --> 00:27:38.210
deferring work--

00:27:38.210 --> 00:27:41.300
moving work that doesn't have
to happen during the request

00:27:41.300 --> 00:27:43.650
out of the scope of the request,
probably by using

00:27:43.650 --> 00:27:46.190
Task Queue or the
delay package.

00:27:46.190 --> 00:27:46.790
Batching--

00:27:46.790 --> 00:27:50.540
so doing multiple operations
in one shot.

00:27:50.540 --> 00:27:51.270
Caching--

00:27:51.270 --> 00:27:53.720
so memoizing data.

00:27:53.720 --> 00:27:55.170
It might be in the Datastore.

00:27:55.170 --> 00:27:56.130
It might be in Memcache.

00:27:56.130 --> 00:27:58.510
It might be in local memory.
it might be even in other

00:27:58.510 --> 00:28:02.690
places I haven't mentioned, so
that you can check them later

00:28:02.690 --> 00:28:05.970
and speed up those parts
of your app.

00:28:05.970 --> 00:28:06.880
Concurrency--

00:28:06.880 --> 00:28:09.970
so decomposing the work that
you're doing into independent

00:28:09.970 --> 00:28:12.940
pieces that can be combined.

00:28:12.940 --> 00:28:16.440
And finally controlling variance
to cut off the long

00:28:16.440 --> 00:28:19.230
tail of latency so that your
app requests can be

00:28:19.230 --> 00:28:22.750
consistently quick rather than
very, very occasionally taking

00:28:22.750 --> 00:28:25.890
a bit too much time.

00:28:25.890 --> 00:28:28.550
So finally here's a bunch
of links to the

00:28:28.550 --> 00:28:29.690
Go project in general.

00:28:29.690 --> 00:28:31.210
There's a heap of documentation
on there.

00:28:31.210 --> 00:28:34.120
You can download Go,
try it out online.

00:28:34.120 --> 00:28:39.680
The second link is to the
Go on App Engine docs.

00:28:39.680 --> 00:28:43.670
The third one links to these
talk's slides, plus my really

00:28:43.670 --> 00:28:47.640
bad source code for the Gopher
Mart app and its revisions,

00:28:47.640 --> 00:28:50.800
and then to the Appstats package
that was written by

00:28:50.800 --> 00:28:52.375
Matt Jibson, who
might be here.

00:28:52.375 --> 00:28:54.380
He did really good
work on that.

00:28:54.380 --> 00:28:56.780
There's more Go sessions coming
up later today and

00:28:56.780 --> 00:28:58.430
tomorrow, so get
along to that.

00:28:58.430 --> 00:29:04.120
In particular, I'll be at the
Office Hours this level at the

00:29:04.120 --> 00:29:06.970
Cloud Sandbox if you have any
particular questions about Go

00:29:06.970 --> 00:29:09.240
on App Engine, whether
performance or otherwise.

00:29:09.240 --> 00:29:12.160
So come along and have a chat.

00:29:12.160 --> 00:29:15.160
So with that, I'll thank you for
coming again, and we can

00:29:15.160 --> 00:29:16.858
take questions.

00:29:16.858 --> 00:29:24.995
[APPLAUSE]

00:29:24.995 --> 00:29:25.730
AUDIENCE: Can you hear me?

00:29:25.730 --> 00:29:26.480
OK.

00:29:26.480 --> 00:29:29.150
So I've been using Go for a
while now, but I haven't used

00:29:29.150 --> 00:29:29.880
App Engine yet.

00:29:29.880 --> 00:29:32.210
So quick question on the
control variance.

00:29:32.210 --> 00:29:32.990
DAVID SYMONDS: Yep.

00:29:32.990 --> 00:29:35.770
AUDIENCE: Instead of trying to
set it in Memcache right then

00:29:35.770 --> 00:29:39.610
and there, could you not put it
in a global channel and do

00:29:39.610 --> 00:29:42.865
a select on it with a timeout,
and then handle it outside?

00:29:45.670 --> 00:29:46.475
DAVID SYMONDS: In a
global channel?

00:29:46.475 --> 00:29:48.100
You mean like in a
global variable?

00:29:48.100 --> 00:29:49.965
AUDIENCE: Right, right.

00:29:49.965 --> 00:29:52.520
DAVID SYMONDS: Yeah, you can
store there right away.

00:29:52.520 --> 00:29:55.390
The downside of that is that it
will only be used for that

00:29:55.390 --> 00:29:56.420
app instance.

00:29:56.420 --> 00:29:58.610
So the memory for each
app instance is only

00:29:58.610 --> 00:29:59.630
accessible to the app.

00:29:59.630 --> 00:30:00.810
AUDIENCE: That wasn't
the question, sir.

00:30:00.810 --> 00:30:03.780
So the operation of setting it
in Memcache, instead of doing

00:30:03.780 --> 00:30:07.180
it in that handle, could you
not put it in a channel and

00:30:07.180 --> 00:30:09.370
then pick it up from there
with a worker?

00:30:09.370 --> 00:30:12.590
Just set it in.

00:30:12.590 --> 00:30:15.390
DAVID SYMONDS: You could.

00:30:15.390 --> 00:30:18.410
The only way to make API calls
is during a request.

00:30:18.410 --> 00:30:20.940
So you can't kick off a Go
routine that does its own

00:30:20.940 --> 00:30:23.070
thing independent of requests.

00:30:23.070 --> 00:30:25.160
You could put it in a channel
and in a later request pull

00:30:25.160 --> 00:30:28.100
things off the channel and do
it at that point, or use a

00:30:28.100 --> 00:30:30.550
Task Queue or use backends
to do that kind of thing.

00:30:30.550 --> 00:30:31.760
Yeah, that would work.

00:30:31.760 --> 00:30:34.220
AUDIENCE: Thanks.

00:30:34.220 --> 00:30:34.960
DAVID SYMONDS: One
from this side.

00:30:34.960 --> 00:30:35.420
AUDIENCE: Hi.

00:30:35.420 --> 00:30:36.800
Nelson from [INAUDIBLE].

00:30:36.800 --> 00:30:39.350
Can you comment on a couple
techniques people are using to

00:30:39.350 --> 00:30:44.810
make Go applications more
efficient in pricing compared

00:30:44.810 --> 00:30:47.735
to Java and Python?

00:30:47.735 --> 00:30:51.270
DAVID SYMONDS: So I mentioned
that Go apps get compiled to

00:30:51.270 --> 00:30:52.690
native code.

00:30:52.690 --> 00:30:55.230
They're binaries.

00:30:55.230 --> 00:30:57.640
That results in them starting
up really quickly.

00:30:57.640 --> 00:31:00.840
So every time a new request
starts coming in and the

00:31:00.840 --> 00:31:04.760
infrastructure decides to spin
up new requests, it'll be a

00:31:04.760 --> 00:31:05.160
lot faster load.

00:31:05.160 --> 00:31:06.530
It's just got to run
a single program.

00:31:06.530 --> 00:31:08.520
It doesn't have to start a
virtual machine, page in a

00:31:08.520 --> 00:31:11.730
bunch of modules, or
whatever it is.

00:31:11.730 --> 00:31:13.700
So it'll be really
fast to start.

00:31:13.700 --> 00:31:17.190
That means that that single
instance starts up quickly and

00:31:17.190 --> 00:31:18.880
immediately can start
doing work.

00:31:18.880 --> 00:31:21.305
If you suddenly get an onslaught
of traffic, in other

00:31:21.305 --> 00:31:23.540
runtimes, you might have to
start up a bunch of instance.

00:31:23.540 --> 00:31:25.410
That'll take a little while,
but then they can deal with

00:31:25.410 --> 00:31:27.710
all the traffic that's
coming in.

00:31:27.710 --> 00:31:29.760
The second main way is that you
have a lot more control

00:31:29.760 --> 00:31:33.470
over the memory that
you are using.

00:31:33.470 --> 00:31:36.670
Go is a very hands-on
language.

00:31:36.670 --> 00:31:38.760
It gives you a lot more control
over things like

00:31:38.760 --> 00:31:39.730
memory lag and so on.

00:31:39.730 --> 00:31:43.200
So you can allocate a slab of
bytes, and you've got that

00:31:43.200 --> 00:31:43.820
slab of bytes.

00:31:43.820 --> 00:31:46.890
You don't have any overhead
for that.

00:31:46.890 --> 00:31:48.480
So using those kinds of things,
if you're intensively

00:31:48.480 --> 00:31:52.370
using your application
instance's memory, you can

00:31:52.370 --> 00:31:56.350
feed a lot more in, say an F1
instance, than you can in,

00:31:56.350 --> 00:31:58.630
say, other runtime.

00:31:58.630 --> 00:32:00.520
AUDIENCE: Why did you use
a buffered channel

00:32:00.520 --> 00:32:02.330
in that last example?

00:32:02.330 --> 00:32:03.580
DAVID SYMONDS: Good question.

00:32:05.480 --> 00:32:07.450
AUDIENCE: Is that so the
Go routine dies?

00:32:07.450 --> 00:32:09.330
DAVID SYMONDS: Yeah, I use a
buffered channel in case the

00:32:09.330 --> 00:32:10.730
timeout happens.

00:32:10.730 --> 00:32:13.330
So if the timeout happens,
nothing's going to be reading

00:32:13.330 --> 00:32:15.520
from that done channel.

00:32:15.520 --> 00:32:17.040
So if the timeout happens,
nothing will be reading from

00:32:17.040 --> 00:32:19.580
that buffered channel, which
means when the Memcache call

00:32:19.580 --> 00:32:23.280
does return, it will block
unless I buffer it.

00:32:23.280 --> 00:32:26.810
So I buffer it so that if that
timeout happens, the Memcache

00:32:26.810 --> 00:32:29.300
can return and still do a
channel operation to a channel

00:32:29.300 --> 00:32:30.500
that will just disappear.

00:32:30.500 --> 00:32:31.955
But then that Go routine
will exit and

00:32:31.955 --> 00:32:33.080
won't be hanging around.

00:32:33.080 --> 00:32:34.540
AUDIENCE: So there could
be a memory leak if

00:32:34.540 --> 00:32:36.000
you don't do that?

00:32:36.000 --> 00:32:37.350
DAVID SYMONDS: You have
a Go routine leak.

00:32:37.350 --> 00:32:39.000
Yeah, that would be a memory
leak in that case if you

00:32:39.000 --> 00:32:39.590
didn't have a buffered
channel.

00:32:39.590 --> 00:32:41.120
AUDIENCE: It's not tied
to the parent channel,

00:32:41.120 --> 00:32:42.750
or the parent request?

00:32:42.750 --> 00:32:44.830
DAVID SYMONDS: No.

00:32:44.830 --> 00:32:46.170
AUDIENCE: I have a few
short questions.

00:32:46.170 --> 00:32:47.170
Thank you.

00:32:47.170 --> 00:32:50.040
NDB has caching built into it.

00:32:50.040 --> 00:32:53.980
Is there any plan to
do that for Go?

00:32:53.980 --> 00:32:55.760
DAVID SYMONDS: Yeah, there's a
couple of external packages

00:32:55.760 --> 00:32:59.880
that have been written that
provide an NBD-type layer.

00:32:59.880 --> 00:33:02.800
I can't remember them off the
top of my head, but they're

00:33:02.800 --> 00:33:04.400
out there, and I can look them
up if you want to ask me at

00:33:04.400 --> 00:33:05.280
Office Hours.

00:33:05.280 --> 00:33:05.630
AUDIENCE: Thank you.

00:33:05.630 --> 00:33:08.050
And also you seem to imply--

00:33:08.050 --> 00:33:09.610
it's only tied to
one CPU, right?

00:33:09.610 --> 00:33:12.145
But Go is really powerful
with multiple CPUs.

00:33:12.145 --> 00:33:16.550
You seem to imply that in the
future that won't be the case?

00:33:16.550 --> 00:33:20.260
DAVID SYMONDS: So each of your
app instances runs CPU bound

00:33:20.260 --> 00:33:21.180
on a single thread.

00:33:21.180 --> 00:33:25.470
So while it's doing an RPC to
do a Datastore fetch or

00:33:25.470 --> 00:33:28.170
whatever, it will switch
to other things.

00:33:28.170 --> 00:33:30.100
So it's got concurrency
in that respect.

00:33:30.100 --> 00:33:32.330
But we only have one CPU
thread running at any

00:33:32.330 --> 00:33:34.555
one time for now.

00:33:34.555 --> 00:33:36.210
AUDIENCE: It could change
in the future?

00:33:36.210 --> 00:33:36.970
DAVID SYMONDS: Yes, yes.

00:33:36.970 --> 00:33:37.760
AUDIENCE: Thank you.

00:33:37.760 --> 00:33:38.653
Thank you.

00:33:38.653 --> 00:33:40.770
DAVID SYMONDS: Yep.

00:33:40.770 --> 00:33:45.030
AUDIENCE: Python and Java
Runtime have a unit testing

00:33:45.030 --> 00:33:49.810
package with App Engine APIs.

00:33:49.810 --> 00:33:57.940
Now, Go has no new test package
for App Engine APIs.

00:33:57.940 --> 00:33:59.900
Do you have any plan to?

00:33:59.900 --> 00:34:02.300
DAVID SYMONDS: Yes, we realize
the testing story for Go on

00:34:02.300 --> 00:34:05.413
App Engine is weak, and we have
plans to improve that in

00:34:05.413 --> 00:34:06.880
the near future.

00:34:06.880 --> 00:34:09.040
AUDIENCE: Thank you.

00:34:09.040 --> 00:34:14.530
Just a question again on the
multi-threading side.

00:34:14.530 --> 00:34:18.440
If I take an instance with two
cores, what's happening there?

00:34:21.969 --> 00:34:25.949
One of them sits idle
all the time?

00:34:25.949 --> 00:34:30.310
DAVID SYMONDS: The instance
classes are based on

00:34:30.310 --> 00:34:34.894
virtualized CPU gigahertz rather
than number of cores.

00:34:37.889 --> 00:34:39.520
Sorry, I'm not sure off the top
of my head what it would

00:34:39.520 --> 00:34:42.170
do if you were on two cores
in that instance.

00:34:42.170 --> 00:34:44.260
It's possible that you would run
multiple instances on that

00:34:44.260 --> 00:34:46.540
machine instead.

00:34:46.540 --> 00:34:47.790
AUDIENCE: Thanks.

00:34:50.429 --> 00:34:51.679
DAVID SYMONDS: Any
more questions?

00:34:54.810 --> 00:34:55.310
Yup.

00:34:55.310 --> 00:34:55.630
OK.

00:34:55.630 --> 00:34:56.880
Thank you very much for coming.

