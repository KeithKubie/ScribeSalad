WEBVTT
Kind: captions
Language: en

00:00:00.140 --> 00:00:03.390
Linear networks are like
we mentioned before.

00:00:03.390 --> 00:00:05.180
The learning rule is really simple for
those.

00:00:05.180 --> 00:00:06.186
So that's great.

00:00:06.186 --> 00:00:09.280
People [INAUDIBLE] Parr have
gotten cool things to happen for

00:00:09.280 --> 00:00:15.320
things like cell phone, channel
allocation, and so that's pretty cool.

00:00:15.320 --> 00:00:18.170
You have to be really, really
careful about what features you use.

00:00:18.170 --> 00:00:18.833
Right?
because I probably

00:00:18.833 --> 00:00:22.108
should have mentioned this before
the important thing about the features.

00:00:22.108 --> 00:00:25.453
I'm going to say that it's important
that we find features that are both

00:00:25.453 --> 00:00:28.080
computationally efficient and
value informative.

00:00:28.080 --> 00:00:30.350
And what I mean by that is
computational efficient.

00:00:30.350 --> 00:00:33.430
What would be an example of a feature
that is really really great but

00:00:33.430 --> 00:00:35.150
is not particularly
computationally efficient?

00:00:35.150 --> 00:00:37.839
&gt;&gt; The one that tells us what
the proper action is I don't know.

00:00:39.050 --> 00:00:40.990
&gt;&gt; Or
what the value is actually in this case.

00:00:40.990 --> 00:00:41.890
Exactly so.

00:00:41.890 --> 00:00:44.836
So if there was a feature that
actually encoded the value function

00:00:44.836 --> 00:00:45.360
that would be great.

00:00:45.360 --> 00:00:47.670
That would be really good for learning.

00:00:47.670 --> 00:00:50.020
The thing is we don't know how
to get that feature efficiently.

00:00:50.020 --> 00:00:52.040
If we did we wouldn't
have to learn at all.

00:00:52.040 --> 00:00:56.550
So we have to somehow get features
that we can extract efficiently from

00:00:56.550 --> 00:00:57.465
the state,

00:00:57.465 --> 00:01:02.190
but again that's really easy to do we
can just return the state as a feature.

00:01:02.190 --> 00:01:05.300
It's really important that it
actually also be value informative.

00:01:05.300 --> 00:01:07.825
So that one feature actually
gives you lots of information And

00:01:07.825 --> 00:01:11.135
about what the predictive
value ought to be so

00:01:11.135 --> 00:01:15.015
somehow balancing these two things
is often very tricky to get to work.

00:01:15.015 --> 00:01:18.674
&gt;&gt; Right so going back to our taxi
example if it turns out that knowing how

00:01:18.674 --> 00:01:20.628
far away the taxi is from some place or

00:01:20.628 --> 00:01:22.983
how far away a passenger
is from some place.

00:01:22.983 --> 00:01:26.403
And you can compute that pretty easily
then you should do it because it's

00:01:26.403 --> 00:01:29.463
helpful and in your learning
are written as to figure it out so

00:01:29.463 --> 00:01:31.480
that is really good thing, right?

00:01:31.480 --> 00:01:32.703
&gt;&gt; Yes a distance to passenger could be

00:01:32.703 --> 00:01:34.590
something that's both
computationally efficient.

00:01:34.590 --> 00:01:37.538
because you can just kind of pull it
right out of the state representation.

00:01:37.538 --> 00:01:41.223
And probably value informative because
it's actually very relevant to know,

00:01:41.223 --> 00:01:43.588
are we close to being able
to pick up the passenger or

00:01:43.588 --> 00:01:46.610
is the passenger quite
distant difficult to reach.

00:01:46.610 --> 00:01:49.940
So yeah I think that's probably
the kind of things that we would want.

00:01:49.940 --> 00:01:53.998
We probably should try that before
we tell people that we think that's

00:01:53.998 --> 00:01:54.838
a great idea.

00:01:54.838 --> 00:01:57.831
But in general these are the kind
of properties that you're trying

00:01:57.831 --> 00:01:58.427
to balance.

00:01:58.427 --> 00:02:00.961
&gt;&gt; Okay.
&gt;&gt; And I want to mention because this is

00:02:00.961 --> 00:02:05.039
very hot right now another example
of using neural networks to learn

00:02:05.039 --> 00:02:09.187
value functions has been carried
out recently by Google DeepMind and

00:02:09.187 --> 00:02:11.570
they were using deep neural networks.

00:02:11.570 --> 00:02:13.370
Do you know what deep
neural networks are?

00:02:13.370 --> 00:02:15.920
&gt;&gt; Yeah, they're are networks
that are very profound.

00:02:15.920 --> 00:02:17.795
&gt;&gt; Yes they're poetic neural networks.

00:02:17.795 --> 00:02:18.670
&gt;&gt; Mm-hm.

00:02:18.670 --> 00:02:23.320
&gt;&gt; No, they are neural networks that are
neural networks except some of them have

00:02:23.320 --> 00:02:28.620
were instead of say three layers they
could have ten, 15 they can actually be

00:02:28.620 --> 00:02:31.260
quite substantially more
sophisticated and complicated.

00:02:31.260 --> 00:02:36.252
And they often involve using things
like convolutions to work well on

00:02:36.252 --> 00:02:38.328
the visual kinds of inputs.

00:02:38.328 --> 00:02:43.272
So what Minnetal all did at
Google DeepMind is they actually learned

00:02:43.272 --> 00:02:47.091
effective strategies for
Atari video games mapping

00:02:47.091 --> 00:02:51.966
the pixels on the screen to joystick
actions which is really neat.

00:02:51.966 --> 00:02:52.980
&gt;&gt; It is
&gt;&gt; Right, so

00:02:52.980 --> 00:02:56.812
it's actually learning to play the game
given very similar kind of input to what

00:02:56.812 --> 00:02:57.448
people get.

00:02:57.448 --> 00:03:01.075
And in fact they were able to get it
to learn very effectively to the point

00:03:01.075 --> 00:03:04.620
where you could argue that it was
comparable to people on average.

00:03:04.620 --> 00:03:08.500
&gt;&gt; At least on some games But did it
learn how to play thermonuclear war?

00:03:08.500 --> 00:03:11.590
&gt;&gt; No, it learned that
the winning move is not to play.

00:03:11.590 --> 00:03:12.210
&gt;&gt; Nice.

00:03:12.210 --> 00:03:12.870
&gt;&gt; No, that's not true.

00:03:12.870 --> 00:03:16.000
That was a WarGames reference, and
in fact, it did not learn not to play.

00:03:16.000 --> 00:03:17.770
It was not given the option
to learn not to play.

00:03:17.770 --> 00:03:19.390
It just had to play
whatever it was given.

00:03:19.390 --> 00:03:20.508
&gt;&gt; Hm.
Grad school.

00:03:20.508 --> 00:03:23.360
&gt;&gt; [LAUGH] And we're back.

00:03:23.360 --> 00:03:25.900
No, it is true that, on some of
the games, it was superhuman, and

00:03:25.900 --> 00:03:28.160
in other of the games,
it was way, way subhuman.

00:03:28.160 --> 00:03:30.420
And that was why I said,
you could argue, on average,

00:03:30.420 --> 00:03:31.900
that was about human level.

00:03:31.900 --> 00:03:35.860
&gt;&gt; Mm, okay, but that's like arguing
that, on average, if I have my

00:03:35.860 --> 00:03:39.190
head in an oven and my feet in ice
water, on average, I'm comfortable.

00:03:39.190 --> 00:03:39.690
&gt;&gt; Yes.

