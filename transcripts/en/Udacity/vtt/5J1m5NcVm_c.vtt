WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:02.880
All right.
So, the last topic I'd like to cover in

00:00:02.880 --> 00:00:08.950
palm DP land is a line of work
that that I am somewhat fond of,

00:00:08.950 --> 00:00:11.180
that is called predictive
state representation.

00:00:11.180 --> 00:00:15.140
So let me let me say a little bit about
the motivation behind studying this, and

00:00:15.140 --> 00:00:17.080
we'll talk a little bit about
the representation itself.

00:00:17.080 --> 00:00:19.700
But we're not going to talk much
about algorithms that actually make

00:00:19.700 --> 00:00:20.272
use of this.

00:00:20.272 --> 00:00:22.630
There is a literature
if you want to dive in.

00:00:22.630 --> 00:00:23.260
All right.
So,

00:00:23.260 --> 00:00:26.410
here's the basic premise
of this line of work.

00:00:26.410 --> 00:00:27.902
So, we talk about POMDPs.

00:00:27.902 --> 00:00:31.560
So, POMDPs, when we're moving around in
a POMDP we're tracking the belief state,

00:00:31.560 --> 00:00:33.320
which is a distribution over states,
right?

00:00:33.320 --> 00:00:34.500
&gt;&gt; Right.
So, here's the thing.

00:00:34.500 --> 00:00:36.450
The states are not observable, right?

00:00:36.450 --> 00:00:37.320
&gt;&gt; Right.
So,

00:00:37.320 --> 00:00:40.110
how can we ever thought
hope to learn in a PomDP,

00:00:40.110 --> 00:00:42.830
when we never actually get to
see these underlying states?

00:00:42.830 --> 00:00:44.290
They're never observed.

00:00:44.290 --> 00:00:46.758
So, you could ask yourself
do they even exist, or

00:00:46.758 --> 00:00:49.440
are they just kind of useful
fictions of our imagination?

00:00:49.440 --> 00:00:51.740
There's nothing that we
can ever do to test hey,

00:00:51.740 --> 00:00:54.110
was this the state we were actually in?

00:00:54.110 --> 00:00:55.750
The answer is I don't know.

00:00:55.750 --> 00:00:58.300
I mean, if we're learning in a POMDP,
we're just creating this notion of

00:00:58.300 --> 00:01:01.470
states to provide some structure to
the observations that we've seen.

00:01:01.470 --> 00:01:03.070
&gt;&gt; Is this like a tree
in the forest thing?

00:01:03.070 --> 00:01:05.750
&gt;&gt; Yeah, yeah.
If a POMDP is solved in a forest,

00:01:05.750 --> 00:01:07.510
does anybody know whether or
not they exist?

00:01:07.510 --> 00:01:08.940
And the answer is me.

00:01:08.940 --> 00:01:10.370
&gt;&gt; No, I think you're right.

00:01:10.370 --> 00:01:12.775
I mean, we're making this
stuff up anyway, right?

00:01:12.775 --> 00:01:13.650
&gt;&gt; Yeah.
And in the case that

00:01:13.650 --> 00:01:16.820
somebody gives us a POMDP and
says would you please plan in this,

00:01:16.820 --> 00:01:19.310
it's a reasonable
representation to work with.

00:01:19.310 --> 00:01:21.450
But if you're in a reinforcement
learning setting,

00:01:21.450 --> 00:01:23.220
you never get to see
these underlying states.

00:01:23.220 --> 00:01:25.540
You can never actually collect
statistics about them.

00:01:25.540 --> 00:01:29.530
In fact, most of the good work
that I've seen that uses POMDPs,

00:01:29.530 --> 00:01:30.430
they don't learn the POMDP.

00:01:30.430 --> 00:01:34.980
They actually send, I want to say
grad students out into the field

00:01:34.980 --> 00:01:39.110
to measure underlying states, and
then measure the observation function.

00:01:39.110 --> 00:01:42.180
They actually kind of instrument
the world to build a POMDP,

00:01:42.180 --> 00:01:45.270
and then they can act in
the POMDP successfully later.

00:01:45.270 --> 00:01:47.910
But it's just there's really no
good way to learn about the states.

00:01:47.910 --> 00:01:49.070
&gt;&gt; Or even that they're real even.

00:01:49.070 --> 00:01:49.600
&gt;&gt; Eve that they're real.

00:01:50.730 --> 00:01:53.682
&gt;&gt; Okay.
So, now that you've blown my mind,

00:01:53.682 --> 00:01:55.465
what does it have to do with?

00:01:55.465 --> 00:01:56.180
&gt;&gt; Right.
So, so,

00:01:56.180 --> 00:01:57.940
the idea of a predictive
state representation,

00:01:57.940 --> 00:02:02.250
or P.S.R., is that the thing that we're
going to use for representation isn't

00:02:02.250 --> 00:02:06.690
going to be probability distributions
over these possibly fictional states.

00:02:06.690 --> 00:02:07.400
But instead,

00:02:07.400 --> 00:02:11.670
they're going to be probabilities of
the outcomes of future predictions.

00:02:11.670 --> 00:02:16.620
So, I'm in a state where it's about 80%
chance it's going to rain tomorrow.

00:02:16.620 --> 00:02:17.190
&gt;&gt; Okay.

00:02:17.190 --> 00:02:19.400
Which doesn't require an actual state.

00:02:19.400 --> 00:02:21.220
&gt;&gt; Well, and
that is maybe not a great example.

00:02:21.220 --> 00:02:24.720
So, let me give a more concrete example
to kind of ground this idea out.

