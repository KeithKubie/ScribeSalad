WEBVTT
Kind: captions
Language: en

00:00:00.727 --> 00:00:02.560
BRADLEY HOROWITZ: I
want to welcome you all.

00:00:02.560 --> 00:00:04.310
My name's Bradley
Horowitz, I'm VP

00:00:04.310 --> 00:00:07.690
of Social for Google,
Social Product Management.

00:00:07.690 --> 00:00:10.360
And I'm here today to
welcome Sandy Pentland

00:00:10.360 --> 00:00:12.220
to come in and speak with us.

00:00:12.220 --> 00:00:13.700
I'm going to be brief.

00:00:13.700 --> 00:00:16.170
I encourage you all
to Google Sandy.

00:00:16.170 --> 00:00:18.840
You will pull up his
long list of credentials,

00:00:18.840 --> 00:00:22.280
which include academic
credentials, business

00:00:22.280 --> 00:00:24.750
credentials, across
many, many disciplines

00:00:24.750 --> 00:00:26.740
for many, many years.

00:00:26.740 --> 00:00:29.480
World Economic Forum,
Forbes' Most Influential,

00:00:29.480 --> 00:00:30.287
it goes on and on.

00:00:30.287 --> 00:00:32.870
So I'm going to try to give you
a little anecdote of something

00:00:32.870 --> 00:00:35.960
that I don't think
you'll get on the web.

00:00:35.960 --> 00:00:37.920
Nothing too embarrassing.

00:00:37.920 --> 00:00:42.350
I'm a student, both former
and current, of Sandy's.

00:00:42.350 --> 00:00:46.360
Former in the sense that I
was a media lab Ph.D. student.

00:00:46.360 --> 00:00:47.800
He was my adviser.

00:00:47.800 --> 00:00:50.050
And current in the
sense that I stay

00:00:50.050 --> 00:00:53.200
closely attuned to
everything that Sandy does.

00:00:53.200 --> 00:00:56.430
As we were walking over
here from building 1900,

00:00:56.430 --> 00:00:58.180
we were sort of doing
what old friends do,

00:00:58.180 --> 00:01:00.480
which is play the name
game and checking in

00:01:00.480 --> 00:01:03.260
on all the old connections
and friends that we share.

00:01:03.260 --> 00:01:04.260
How's Roz doing?

00:01:04.260 --> 00:01:05.140
How Stan doing?

00:01:05.140 --> 00:01:06.170
How's Ali doing?

00:01:06.170 --> 00:01:07.620
What about Fad?

00:01:07.620 --> 00:01:10.510
And we went through
them, and turns out

00:01:10.510 --> 00:01:12.240
everybody's doing fine.

00:01:12.240 --> 00:01:15.380
You know, many of you are here,
actually in the second row,

00:01:15.380 --> 00:01:16.900
in front row.

00:01:16.900 --> 00:01:20.030
Many of you have gone
off to become professors

00:01:20.030 --> 00:01:22.640
at MIT or Berkeley
or Georgia Tech.

00:01:22.640 --> 00:01:23.960
And it was just so great.

00:01:23.960 --> 00:01:27.590
And thinking about
that for a moment,

00:01:27.590 --> 00:01:29.290
I recognized that
I was just walking

00:01:29.290 --> 00:01:33.060
through one of the vintages,
the sort of early '90s vintage

00:01:33.060 --> 00:01:36.770
of Sandy's students who have
all gone off to do great things.

00:01:36.770 --> 00:01:39.050
And Sandy has
consecutively piled

00:01:39.050 --> 00:01:41.510
on top of that,
round after round,

00:01:41.510 --> 00:01:44.380
of graduate students and
students that he has inspired.

00:01:44.380 --> 00:01:47.160
And in addition to all of
those lists of accomplishments,

00:01:47.160 --> 00:01:49.240
one of the things that
really touches me most

00:01:49.240 --> 00:01:51.340
about Sandy and his
work is that he's

00:01:51.340 --> 00:01:53.520
such an inspirational educator.

00:01:53.520 --> 00:01:56.110
He not only has enthusiasm
for his own work,

00:01:56.110 --> 00:01:57.900
he's able to impart
that to others

00:01:57.900 --> 00:02:00.630
and create generations of
people that care passionately

00:02:00.630 --> 00:02:02.520
about technology and science.

00:02:02.520 --> 00:02:06.510
And it's just so great to be in
the company, which we will all

00:02:06.510 --> 00:02:08.130
get to share for
an hour right now,

00:02:08.130 --> 00:02:10.710
of a person who can
inspire and lead that way.

00:02:10.710 --> 00:02:12.750
And so with that, I'll
hand it over to Sandy.

00:02:12.750 --> 00:02:13.250
Welcome.

00:02:13.250 --> 00:02:14.583
SANDY PENTLAND: Well, thank you.

00:02:14.583 --> 00:02:16.547
[CLAPPING]

00:02:16.547 --> 00:02:18.790
SANDY PENTLAND: Now, I'll
have to inspire and cause

00:02:18.790 --> 00:02:20.820
passion, which is
actually part of what

00:02:20.820 --> 00:02:21.820
I'm going to talk about.

00:02:21.820 --> 00:02:24.050
So how does that happen?

00:02:24.050 --> 00:02:26.620
So maybe this is good.

00:02:26.620 --> 00:02:28.370
So I'm going to talk
about two things.

00:02:28.370 --> 00:02:33.040
One is basic science
about who people

00:02:33.040 --> 00:02:36.180
are, how we use electronic
media, how we use face to face

00:02:36.180 --> 00:02:39.470
media, how we evolved
as a social species.

00:02:39.470 --> 00:02:42.870
And then I want to move to
how we can use this knowledge

00:02:42.870 --> 00:02:44.820
to make things
better, to have a more

00:02:44.820 --> 00:02:48.210
sustainable digital ecology,
to make government work.

00:02:48.210 --> 00:02:50.250
Wouldn't that be
amazing? [LAUGHS]

00:02:50.250 --> 00:02:52.160
And so on and so forth.

00:02:52.160 --> 00:02:55.630
And as Bradley mentioned,
I do a lot of things.

00:02:55.630 --> 00:02:57.070
I thought I'd stick this in.

00:02:57.070 --> 00:02:58.030
I love this picture.

00:02:58.030 --> 00:03:01.560
This is the boards
back in the 1990s.

00:03:01.560 --> 00:03:03.080
And that's Thad in
the front there,

00:03:03.080 --> 00:03:05.740
Thad Starner, who I
think most of you know.

00:03:05.740 --> 00:03:09.790
And then two other things I
do that are of real relevancy

00:03:09.790 --> 00:03:10.880
here, maybe three.

00:03:10.880 --> 00:03:13.000
Is one is for the
last five years,

00:03:13.000 --> 00:03:15.950
I've run a group-- helped
run a group at Davos--

00:03:15.950 --> 00:03:18.840
around personal data,
privacy, and big data.

00:03:18.840 --> 00:03:23.330
And that's, of course, a very
relevant topic for this crowd,

00:03:23.330 --> 00:03:25.260
but particularly, going forward.

00:03:25.260 --> 00:03:27.490
And the group includes
people like the Chairman

00:03:27.490 --> 00:03:30.620
of the Federal Trade Commission,
the vice president of the EU,

00:03:30.620 --> 00:03:34.400
Politburo members from
China, et cetera, et cetera.

00:03:34.400 --> 00:03:38.880
So it's a conversation between
CEOs of major companies

00:03:38.880 --> 00:03:40.660
and chief regulators
and advocacy.

00:03:40.660 --> 00:03:42.740
And I'll talk about
that at then end

00:03:42.740 --> 00:03:45.120
and where I think
that things are going

00:03:45.120 --> 00:03:48.050
and what you might
want to do about it.

00:03:48.050 --> 00:03:52.310
And I just joined
the Google ATAP Board

00:03:52.310 --> 00:03:54.950
because it used to
be owned by Motorola,

00:03:54.950 --> 00:03:58.050
but when Motorola got
sold, they intelligently

00:03:58.050 --> 00:04:00.840
moved the really creative
interesting part over here

00:04:00.840 --> 00:04:01.700
to Google.

00:04:01.700 --> 00:04:03.630
And as Bradley mentioned,
that started a bunch

00:04:03.630 --> 00:04:06.020
of companies, which
are doing well.

00:04:06.020 --> 00:04:09.605
So the thing that I'm
really concerned about,

00:04:09.605 --> 00:04:13.560
the thing that's passionate, is
making the world work better.

00:04:13.560 --> 00:04:19.440
And a sort of story for
this is about 15 years ago,

00:04:19.440 --> 00:04:22.600
I was setting up a series
of laboratories in India.

00:04:22.600 --> 00:04:26.850
And, you know, we had huge
government sponsorship.

00:04:26.850 --> 00:04:28.420
We had a board of
directors, which

00:04:28.420 --> 00:04:31.610
are some of the brightest most
successful people in the world.

00:04:31.610 --> 00:04:34.770
And it was a complete disaster.

00:04:34.770 --> 00:04:36.550
And it had to do
with a lot of things.

00:04:36.550 --> 00:04:40.380
It had to do with all
of the sort of macho,

00:04:40.380 --> 00:04:43.570
signaling charisma in the room
with the board of directors.

00:04:43.570 --> 00:04:47.010
But it also had to do with
the way the government failed

00:04:47.010 --> 00:04:48.700
to work, or did work.

00:04:48.700 --> 00:04:51.310
And looking back on
that, I can sort of

00:04:51.310 --> 00:04:55.430
see that premonitions of
the US Congress today.

00:04:55.430 --> 00:04:55.930
All right?

00:04:55.930 --> 00:04:58.040
So we went and visited
the Indian Congress,

00:04:58.040 --> 00:05:01.330
where we saw people
throwing shoes at each other

00:05:01.330 --> 00:05:03.360
and throwing cash in the air.

00:05:03.360 --> 00:05:06.080
And we look at the
US Congress today,

00:05:06.080 --> 00:05:09.580
and it's somewhat
similar, unfortunately.

00:05:09.580 --> 00:05:11.340
So I want to make things better.

00:05:11.340 --> 00:05:13.890
And what occurs to
me is if we knew

00:05:13.890 --> 00:05:16.320
how to make our
organizations work,

00:05:16.320 --> 00:05:18.680
then we could really do things.

00:05:18.680 --> 00:05:21.760
Like we could solve
global warming tomorrow

00:05:21.760 --> 00:05:24.650
if we all knew how to sort
of talk about it rationally,

00:05:24.650 --> 00:05:27.710
come to a good decision,
and then carry that through.

00:05:27.710 --> 00:05:30.930
And the fact that that sounds
like ludicrous fantasy--

00:05:30.930 --> 00:05:32.490
oh, yeah, everybody agree.

00:05:32.490 --> 00:05:32.990
Sure.

00:05:32.990 --> 00:05:35.530
Not in our lifetime--
tells you just how profound

00:05:35.530 --> 00:05:38.010
the problem is.

00:05:38.010 --> 00:05:40.307
And that's why I think one
of the most important things

00:05:40.307 --> 00:05:42.640
that's happened in the last
decade, something you've all

00:05:42.640 --> 00:05:45.450
been part of, is
this era of big data,

00:05:45.450 --> 00:05:47.630
which is not about big at all.

00:05:47.630 --> 00:05:49.360
It's about personal data.

00:05:49.360 --> 00:05:52.680
Detailed data about the behavior
of every person on Earth,

00:05:52.680 --> 00:05:56.990
where they go, what they
buy, what they say online,

00:05:56.990 --> 00:05:58.490
all sorts of things like that.

00:05:58.490 --> 00:06:01.190
Suddenly we could
watch people the way,

00:06:01.190 --> 00:06:05.030
say, you would watch an
ant hill or Jane Goodall

00:06:05.030 --> 00:06:06.270
would watch apes.

00:06:06.270 --> 00:06:09.440
We can do that, and
that has profound impact

00:06:09.440 --> 00:06:11.340
that is hard to appreciate.

00:06:11.340 --> 00:06:13.050
I made this little
graph, which comes out

00:06:13.050 --> 00:06:16.750
of-- inspired by
Nadav's thesis here,

00:06:16.750 --> 00:06:18.610
which is duration
of observation.

00:06:18.610 --> 00:06:21.170
These are social science
experiments, and the biggest

00:06:21.170 --> 00:06:22.750
medical experiments.

00:06:22.750 --> 00:06:25.210
So this is like the
Framingham heart study.

00:06:25.210 --> 00:06:27.610
30 years, 30,000 people.

00:06:27.610 --> 00:06:30.840
But they only talked to people
like once every three years.

00:06:30.840 --> 00:06:35.670
So the bit rate was like
one number per month.

00:06:35.670 --> 00:06:37.670
So you had no idea what
these people were doing.

00:06:37.670 --> 00:06:39.961
They could have been eating
fried chicken all the time.

00:06:39.961 --> 00:06:41.590
You don't know, right?

00:06:41.590 --> 00:06:44.230
Or most of the things
we know about psychology

00:06:44.230 --> 00:06:45.510
come from down here.

00:06:45.510 --> 00:06:47.600
This is a number of bits
per second, duration.

00:06:47.600 --> 00:06:49.770
This is a bunch of
freshman in Psych 101

00:06:49.770 --> 00:06:51.530
filling out some surveys.

00:06:51.530 --> 00:06:54.150
And that's what we take
to be social science,

00:06:54.150 --> 00:06:57.270
political science,
and medical science.

00:06:57.270 --> 00:07:00.110
But now we have these
new ways of doing things.

00:07:00.110 --> 00:07:02.010
And so what in my
group we've done

00:07:02.010 --> 00:07:03.920
is we've built little
badges, like you all

00:07:03.920 --> 00:07:05.580
have little name badges.

00:07:05.580 --> 00:07:07.650
And so we can actually
know where you go

00:07:07.650 --> 00:07:09.060
and who you talk to.

00:07:09.060 --> 00:07:10.590
And we do this
with organizations.

00:07:10.590 --> 00:07:12.390
We'll track everybody
for a month.

00:07:12.390 --> 00:07:14.710
We never listen to
the words, but we

00:07:14.710 --> 00:07:17.180
do know the patterns
of communication.

00:07:17.180 --> 00:07:19.300
And I'll show you a
little bit about that.

00:07:19.300 --> 00:07:21.860
Similarly, we put
software in phones,

00:07:21.860 --> 00:07:23.970
and we look at the
patterns of communication

00:07:23.970 --> 00:07:25.109
within a community.

00:07:25.109 --> 00:07:27.150
So I go into a community
and give everybody brand

00:07:27.150 --> 00:07:28.665
new phones.

00:07:28.665 --> 00:07:31.040
And some of the people here
have been integrally involved

00:07:31.040 --> 00:07:32.430
in these experiments.

00:07:32.430 --> 00:07:35.180
And we'll look at their Facebook
activity, their credit card

00:07:35.180 --> 00:07:38.090
record, their sleep pattern,
their communication pattern,

00:07:38.090 --> 00:07:40.550
who they hang with,
who they call,

00:07:40.550 --> 00:07:42.870
and ask, what do all this
communication patterns

00:07:42.870 --> 00:07:44.541
have to do with outcomes?

00:07:44.541 --> 00:07:45.040
All right?

00:07:45.040 --> 00:07:46.070
Do they spend too much?

00:07:46.070 --> 00:07:48.650
What things do they choose
to buy, and so forth.

00:07:48.650 --> 00:07:51.480
And what you find
from the big data,

00:07:51.480 --> 00:07:55.070
and, of course, modern machine
learning sorts of things,

00:07:55.070 --> 00:07:58.360
is that you can build
quantitative predictive models

00:07:58.360 --> 00:08:01.360
of human behavior,
which you all know.

00:08:01.360 --> 00:08:03.052
But I think you
know the wrong part.

00:08:03.052 --> 00:08:05.260
And I'm going to tell you
about the other part that's

00:08:05.260 --> 00:08:08.760
much stronger than what
you typically do, OK?

00:08:08.760 --> 00:08:11.502
And so you can predict behavior.

00:08:11.502 --> 00:08:12.960
And people go,
well, wait a second.

00:08:12.960 --> 00:08:14.060
What about free will?

00:08:14.060 --> 00:08:15.476
That may not have
occurred to you,

00:08:15.476 --> 00:08:17.620
but that's a traditional
thing to ask.

00:08:17.620 --> 00:08:20.310
And I'll tell you a little
bit about that along

00:08:20.310 --> 00:08:23.790
the way because it turns out
that a lot of our behavior

00:08:23.790 --> 00:08:26.070
is very habitual,
and that's the part

00:08:26.070 --> 00:08:28.460
that we can model
mathematically.

00:08:28.460 --> 00:08:32.510
So the big picture
here, and this

00:08:32.510 --> 00:08:34.700
is part of the reason I
got off onto this research,

00:08:34.700 --> 00:08:36.659
is I go to places
like Davos, and you

00:08:36.659 --> 00:08:39.480
listen to the president of
this and the CEO of that.

00:08:39.480 --> 00:08:42.500
And when they talk about
changing policy, talk

00:08:42.500 --> 00:08:46.500
about doing anything, they
use economics metaphors.

00:08:46.500 --> 00:08:49.197
And the thing about
[INAUDIBLE] economic metaphors

00:08:49.197 --> 00:08:50.780
is that they're all
about individuals.

00:08:50.780 --> 00:08:53.090
So you've heard about
rational individuals.

00:08:53.090 --> 00:08:55.229
And everybody rags
on the rational part.

00:08:55.229 --> 00:08:56.270
I'm not going to do that.

00:08:56.270 --> 00:08:58.970
I'm going to rag on the
individual part, OK?

00:08:58.970 --> 00:09:01.230
Because I don't think
we are individuals.

00:09:01.230 --> 00:09:05.110
What we desire, the ways we
learn to go about doing it,

00:09:05.110 --> 00:09:08.200
what's valuable, are
consensual things.

00:09:08.200 --> 00:09:10.560
So they actually are
captured by this sort

00:09:10.560 --> 00:09:12.920
of model, this
independent model.

00:09:12.920 --> 00:09:16.370
That matters because
those interactions are

00:09:16.370 --> 00:09:20.540
the sources, not only of
fads and economic bubbles,

00:09:20.540 --> 00:09:23.790
but they're really the social
fabric that we live in.

00:09:23.790 --> 00:09:27.150
So everybody knows
about the invisible hand

00:09:27.150 --> 00:09:28.650
that are led by
the invisible hand

00:09:28.650 --> 00:09:30.465
to advance the
interest of society.

00:09:30.465 --> 00:09:33.090
What that means is that markets
are supposed to allocate things

00:09:33.090 --> 00:09:35.830
efficiently and fairly, right?

00:09:35.830 --> 00:09:37.330
If you've thought
about it, you know

00:09:37.330 --> 00:09:41.230
this doesn't work in the
real world, [LAUGHS] OK?

00:09:41.230 --> 00:09:43.380
And the question is, why?

00:09:43.380 --> 00:09:45.820
So one of the things
that-- there's

00:09:45.820 --> 00:09:47.320
several things to
say about this.

00:09:47.320 --> 00:09:50.620
Most people think that
this statement is something

00:09:50.620 --> 00:09:52.590
that he made in "The
Wealth of Nations."

00:09:52.590 --> 00:09:53.270
And I'm just going
to [INAUDIBLE]

00:09:53.270 --> 00:09:54.369
"The Wealth of Nations."

00:09:54.369 --> 00:09:54.910
But it's not.

00:09:54.910 --> 00:09:57.118
He made it in a book called
"Moral Sentiments," which

00:09:57.118 --> 00:09:58.770
very few people read.

00:09:58.770 --> 00:10:02.500
And it went on to say
something very different.

00:10:02.500 --> 00:10:06.220
It went on to say that
"it's human nature

00:10:06.220 --> 00:10:09.880
to exchange not only goods, but
ideas, assistance, and favors.

00:10:09.880 --> 00:10:12.190
And it's these exchanges
that guide people

00:10:12.190 --> 00:10:15.190
to create solutions for
the good of the community."

00:10:15.190 --> 00:10:17.760
So Adam Smith did not
believe that markets

00:10:17.760 --> 00:10:19.470
were socially efficient.

00:10:19.470 --> 00:10:23.260
He believed that it was the
social fabric of relationships

00:10:23.260 --> 00:10:25.920
that caused the
pressures of the market

00:10:25.920 --> 00:10:28.270
to be allocated
where they're needed.

00:10:28.270 --> 00:10:30.280
And, in fact, a lot
of mathematicians

00:10:30.280 --> 00:10:33.320
believe now that this sort
of despite Nobel prizes

00:10:33.320 --> 00:10:35.210
about market
efficiency and markets

00:10:35.210 --> 00:10:37.552
being good for
governance, it's not.

00:10:37.552 --> 00:10:40.010
It's really-- you have to have
the right sort of regulation

00:10:40.010 --> 00:10:41.810
replication mechanism.

00:10:41.810 --> 00:10:44.320
But this is another solution.

00:10:44.320 --> 00:10:49.670
Adam Smith way back when said,
if we could model the peer

00:10:49.670 --> 00:10:52.270
to peer relationships,
we could understand

00:10:52.270 --> 00:10:57.280
how market things
eventually resolve

00:10:57.280 --> 00:10:58.460
to be much more efficient.

00:10:58.460 --> 00:10:59.671
And that's what we're doing.

00:10:59.671 --> 00:11:01.170
We're doing something
that you could

00:11:01.170 --> 00:11:03.580
call, sort of, Economics 2.0.

00:11:03.580 --> 00:11:06.090
Instead of the
individual approximation,

00:11:06.090 --> 00:11:09.170
we're now modeling not
only individual behavior,

00:11:09.170 --> 00:11:12.190
but peer to peer behavior
at the same time.

00:11:12.190 --> 00:11:15.730
So that's the sort of big
context for what's up here.

00:11:15.730 --> 00:11:20.000
So let me give you an
example of what that means.

00:11:20.000 --> 00:11:22.640
So in a typical situation,
you have some people

00:11:22.640 --> 00:11:24.670
that influence each other.

00:11:24.670 --> 00:11:28.641
So, you know, their political
views, it's what's cool to do.

00:11:28.641 --> 00:11:30.390
You pick it up from
the people around you.

00:11:30.390 --> 00:11:32.590
And we have a lot of
evidence upon this

00:11:32.590 --> 00:11:35.280
from the experiments
we've done in our group,

00:11:35.280 --> 00:11:37.935
showing that people's
attitudes about, you know,

00:11:37.935 --> 00:11:41.190
what music to listen to,
what apps to download,

00:11:41.190 --> 00:11:44.810
what spending behavior
to have, is largely

00:11:44.810 --> 00:11:48.620
predicted by their exposure
to what other people do.

00:11:48.620 --> 00:11:50.120
And you may not
want to believe this

00:11:50.120 --> 00:11:52.000
because it's not the
rhetoric in our society.

00:11:52.000 --> 00:11:54.470
And you guys are the last
people to be saying this

00:11:54.470 --> 00:11:57.740
to because you guys are like the
best and smartest in the world.

00:11:57.740 --> 00:12:01.700
But it really is true that
about 50% of the variance

00:12:01.700 --> 00:12:04.860
comes from these peer
to peer relationships.

00:12:04.860 --> 00:12:07.280
And we know that when
we do incentives,

00:12:07.280 --> 00:12:09.800
when we try to these--
CEOs and governors

00:12:09.800 --> 00:12:13.570
try to set up governance schemes
to make people do things,

00:12:13.570 --> 00:12:15.470
they always talk about
individual incentives.

00:12:15.470 --> 00:12:18.820
That's part of this mindset
that comes from the 1700s,

00:12:18.820 --> 00:12:20.770
is that we're all
rational individuals.

00:12:20.770 --> 00:12:24.000
So we'll give this guy
money to behave differently.

00:12:24.000 --> 00:12:26.440
But when you do that, what
happens is, of course,

00:12:26.440 --> 00:12:28.880
you're putting that
incentive in opposition

00:12:28.880 --> 00:12:32.280
to the social pressure he
gets from other people.

00:12:32.280 --> 00:12:34.790
And if they're aligned,
it works wonderfully.

00:12:34.790 --> 00:12:37.900
But if they're not aligned,
which happens all the time,

00:12:37.900 --> 00:12:39.140
incentives don't work.

00:12:39.140 --> 00:12:41.140
Individual incentives
don't work.

00:12:41.140 --> 00:12:43.580
And the moment the incentive
goes away, you know,

00:12:43.580 --> 00:12:46.440
you start paying them,
they revert to what

00:12:46.440 --> 00:12:48.450
the social fabric does.

00:12:48.450 --> 00:12:52.560
So when you begin to think about
this mathematical framework

00:12:52.560 --> 00:12:56.760
that includes the social
fabric, an obvious thing

00:12:56.760 --> 00:12:58.670
occurs to you,
which is that, well,

00:12:58.670 --> 00:13:01.250
instead of giving one
person the incentive,

00:13:01.250 --> 00:13:04.580
maybe I can modify
the social fabric.

00:13:04.580 --> 00:13:07.430
What we have is
exchanges between people,

00:13:07.430 --> 00:13:10.012
and we have incentives being
applied to the individuals.

00:13:10.012 --> 00:13:11.470
And now what I'm
going to do is I'm

00:13:11.470 --> 00:13:15.180
going to modify the incentives
between the people, OK?

00:13:15.180 --> 00:13:17.710
And you can write down
these equations just the way

00:13:17.710 --> 00:13:20.130
you write down
economic equations.

00:13:20.130 --> 00:13:24.010
So this was in "Nature
Scientific Reports"

00:13:24.010 --> 00:13:25.240
just last year.

00:13:25.240 --> 00:13:28.580
So you could write it all
down, you know, with utilities

00:13:28.580 --> 00:13:31.117
and peer pressure
and externality cost.

00:13:31.117 --> 00:13:32.950
And you'd discover
something interesting, is

00:13:32.950 --> 00:13:36.220
when you add these
second order terms in,

00:13:36.220 --> 00:13:41.270
you find that incentives
that modify the interactions

00:13:41.270 --> 00:13:44.500
are generically more
than twice as efficient

00:13:44.500 --> 00:13:47.320
as incentives that
go to individuals.

00:13:47.320 --> 00:13:48.730
Generically that way.

00:13:48.730 --> 00:13:50.980
And in the data
that I'll show you,

00:13:50.980 --> 00:13:55.080
it's sort of four to
20 times more powerful.

00:13:55.080 --> 00:13:58.073
So this is the thing that I want
to really get across to you,

00:13:58.073 --> 00:14:01.280
is that this sort of power of
economics that has mathematics

00:14:01.280 --> 00:14:03.470
and prediction is very limited.

00:14:03.470 --> 00:14:05.940
It doesn't include anything
that has strong peer

00:14:05.940 --> 00:14:10.215
to peer effects, like
bubbles, like fads.

00:14:10.215 --> 00:14:12.715
But you can now write those
down because we have enough data

00:14:12.715 --> 00:14:14.934
and enough math
be able to do it.

00:14:14.934 --> 00:14:17.100
So let me give you some
examples of what that means.

00:14:17.100 --> 00:14:18.826
These are simple examples.

00:14:18.826 --> 00:14:21.750
You can do much more
complicated ones.

00:14:21.750 --> 00:14:24.910
So this was, again, sort
of done in Nadav's thesis.

00:14:24.910 --> 00:14:29.282
We got a community, where we
divided them into two pieces.

00:14:29.282 --> 00:14:30.990
It's actually three
pieces, but I'll just

00:14:30.990 --> 00:14:32.980
talk about two pieces.

00:14:32.980 --> 00:14:34.870
In one piece, we
had a little app

00:14:34.870 --> 00:14:36.760
that showed them how
active they were.

00:14:36.760 --> 00:14:38.620
And this was in the
winter in Boston,

00:14:38.620 --> 00:14:40.995
where people tend to just put
the blanket over their head

00:14:40.995 --> 00:14:41.990
and go away, right?

00:14:41.990 --> 00:14:44.336
So we wanted them to get
out and around and stuff.

00:14:44.336 --> 00:14:46.210
And so we can show them
their activity level,

00:14:46.210 --> 00:14:47.180
and we can give them money.

00:14:47.180 --> 00:14:49.530
So you can say, oh, if I was
more active than last week,

00:14:49.530 --> 00:14:51.170
I made $3.

00:14:51.170 --> 00:14:52.620
Wonderful, OK?

00:14:52.620 --> 00:14:55.390
But in the other part
of the community,

00:14:55.390 --> 00:14:57.880
we assign people buddies.

00:14:57.880 --> 00:14:59.790
And you would be his buddy.

00:14:59.790 --> 00:15:02.910
If he is more active,
you'd get rewarded.

00:15:02.910 --> 00:15:04.460
Not him, you.

00:15:04.460 --> 00:15:06.850
Your buddy gets rewarded
for you being active.

00:15:06.850 --> 00:15:08.510
And what you do
is you pick people

00:15:08.510 --> 00:15:11.442
that have a lot of interactions
with each other to do this.

00:15:11.442 --> 00:15:13.870
And it sounds a little
creepy, but actually, people

00:15:13.870 --> 00:15:14.500
got into it.

00:15:14.500 --> 00:15:17.110
Almost everybody
signed up for it.

00:15:17.110 --> 00:15:19.410
And what you got is you
got everybody's, like,

00:15:19.410 --> 00:15:20.910
looking at the other
guy and saying,

00:15:20.910 --> 00:15:22.500
well, are you being active?

00:15:22.500 --> 00:15:25.020
Because they're reminded on
their phone all the time,

00:15:25.020 --> 00:15:27.035
and they're getting a
couple bucks for it.

00:15:27.035 --> 00:15:28.900
It's not a big thing, OK?

00:15:28.900 --> 00:15:32.530
But remember that that incentive
scheme, that social network

00:15:32.530 --> 00:15:34.810
scheme, if I'm not
incenting the individuals,

00:15:34.810 --> 00:15:38.810
I'm incenting the network, is
generically more than twice

00:15:38.810 --> 00:15:39.650
as efficient.

00:15:39.650 --> 00:15:42.080
In fact, in this experiment,
the way we did it,

00:15:42.080 --> 00:15:44.129
we found it was four
times more efficient.

00:15:44.129 --> 00:15:46.670
And if we'd done it the right
way, if we went back and did it

00:15:46.670 --> 00:15:48.990
again, as sort of a
post hoc analysis,

00:15:48.990 --> 00:15:54.190
we would have gotten eight to
10 times more efficient, OK?

00:15:54.190 --> 00:15:55.510
Pretty interesting.

00:15:55.510 --> 00:15:56.870
Oh, and one other thing.

00:15:56.870 --> 00:15:57.930
It stuck.

00:15:57.930 --> 00:16:00.540
When we ran out of
money, we turned off

00:16:00.540 --> 00:16:02.460
the money, no more incentives.

00:16:02.460 --> 00:16:05.480
People kept being more
active for the period

00:16:05.480 --> 00:16:07.364
that we could observe, OK?

00:16:07.364 --> 00:16:09.530
Because what we'd done is
changed the social fabric.

00:16:09.530 --> 00:16:12.950
We made being active a
topic of conversation,

00:16:12.950 --> 00:16:17.360
a topic of social pressure,
of prestige, of interest.

00:16:17.360 --> 00:16:20.690
And, of course, that
has momentum to it.

00:16:20.690 --> 00:16:22.440
Here's another
example that's online.

00:16:22.440 --> 00:16:25.670
We went to a canton
in Switzerland,

00:16:25.670 --> 00:16:27.040
which was trying to save power.

00:16:27.040 --> 00:16:30.140
Keep power below a
certain level because they

00:16:30.140 --> 00:16:32.370
had hydroelectric
power up to one level.

00:16:32.370 --> 00:16:34.500
And beyond that, they
had to turn on diesels.

00:16:34.500 --> 00:16:37.900
And noisy, expensive, polluting.

00:16:37.900 --> 00:16:40.040
And they tried educating
people, and they

00:16:40.040 --> 00:16:43.836
tried financial incentives,
and nothing really worked.

00:16:43.836 --> 00:16:46.470
And so we convinced them to
sign people up as buddies.

00:16:46.470 --> 00:16:49.610
So you would sign up
with him as your buddy,

00:16:49.610 --> 00:16:53.062
and if you saved energy,
he would get a reward.

00:16:53.062 --> 00:16:55.600
Now, the rewards
were, like, pathetic.

00:16:55.600 --> 00:16:59.120
The budget was
$0.50 per week, OK?

00:16:59.120 --> 00:17:01.190
And little dancing
bears on your website.

00:17:01.190 --> 00:17:02.726
It was, like, really stupid.

00:17:02.726 --> 00:17:06.480
But what happened is that for
the people that signed up,

00:17:06.480 --> 00:17:10.740
you got a 17% reduction
in energy usage.

00:17:10.740 --> 00:17:13.970
Now, that sounds good, but let
me give you the comparable.

00:17:13.970 --> 00:17:16.819
There have been some places
where people have raised prices

00:17:16.819 --> 00:17:19.869
to get 17% reduction.

00:17:19.869 --> 00:17:23.420
On average, you have to
double the price of energy

00:17:23.420 --> 00:17:25.950
to get that reduction
in energy use.

00:17:25.950 --> 00:17:27.920
That's the price
elasticity curve.

00:17:27.920 --> 00:17:32.340
So for $0.50 a week, we could
get the same effect as doubling

00:17:32.340 --> 00:17:34.730
the price.

00:17:34.730 --> 00:17:37.270
This one is by a friend
of mine, James Fowler.

00:17:37.270 --> 00:17:40.880
Done with Facebook in 2010.

00:17:40.880 --> 00:17:43.260
He's sent 61 million
people a message

00:17:43.260 --> 00:17:46.960
about getting out and
vote in the election.

00:17:46.960 --> 00:17:51.557
The sort of simple summary of
it is it had almost no effect.

00:17:51.557 --> 00:17:52.390
People didn't do it.

00:17:52.390 --> 00:17:53.330
A few people did.

00:17:53.330 --> 00:17:54.895
He could go back
and sample people

00:17:54.895 --> 00:17:58.170
and say, how many people
did this have in effect.

00:17:58.170 --> 00:18:00.620
He also included a
"I Voted" button,

00:18:00.620 --> 00:18:05.350
which would then show your face
to your Facebook friends, OK?

00:18:05.350 --> 00:18:09.960
This also had no effect, except
with one particular class

00:18:09.960 --> 00:18:12.700
of people, which is the
people you had strong face

00:18:12.700 --> 00:18:14.810
to face relationships with.

00:18:14.810 --> 00:18:19.350
If you appeared in the same
images with this other person

00:18:19.350 --> 00:18:23.210
regularly, then
among that group,

00:18:23.210 --> 00:18:25.760
that "I Voted" button
would generate two

00:18:25.760 --> 00:18:28.140
to three more people voting.

00:18:28.140 --> 00:18:32.140
So greater than yield one,
a cascade of behavior.

00:18:32.140 --> 00:18:33.860
So, again, what
was happening there

00:18:33.860 --> 00:18:37.690
is social pressure, the
peer to peer things.

00:18:37.690 --> 00:18:39.790
The centralized
thing didn't do it.

00:18:39.790 --> 00:18:43.400
It was peer to peer
pressure between people

00:18:43.400 --> 00:18:45.630
with strong ties.

00:18:45.630 --> 00:18:47.680
And, in fact, actually,
it's not captured

00:18:47.680 --> 00:18:49.060
by the electronic
network at all.

00:18:49.060 --> 00:18:52.380
These are things that
were outside of that, OK?

00:18:52.380 --> 00:18:54.020
So that's an
example that I think

00:18:54.020 --> 00:18:57.100
is really interesting
that you can build on.

00:18:57.100 --> 00:18:59.300
And somebody
mentioned, actually,

00:18:59.300 --> 00:19:02.665
how many of you know
the red balloon contest?

00:19:02.665 --> 00:19:03.540
[INAUDIBLE] knows it.

00:19:03.540 --> 00:19:04.199
[LAUGHTER]

00:19:04.199 --> 00:19:05.490
SANDY PENTLAND: [INAUDIBLE] OK.

00:19:05.490 --> 00:19:09.440
But so DARPA had this social
network grand challenge

00:19:09.440 --> 00:19:12.745
where we had to find 10 red
balloons somewhere in the US.

00:19:12.745 --> 00:19:15.570
And everybody tried using
the economic incentives.

00:19:15.570 --> 00:19:17.110
You know, you get
some money if you

00:19:17.110 --> 00:19:18.880
found a balloon a reported it.

00:19:18.880 --> 00:19:21.350
We used something
like this mechanism.

00:19:21.350 --> 00:19:22.960
We're able to recruit
what we estimate

00:19:22.960 --> 00:19:27.160
to be about two million
people in 24 hours, and won.

00:19:27.160 --> 00:19:30.500
Again, not giving people
directly the incentive.

00:19:30.500 --> 00:19:32.490
In that case, it's a
little more complicated,

00:19:32.490 --> 00:19:36.240
but giving people these
peer to peer things.

00:19:36.240 --> 00:19:38.730
So that's cool.

00:19:38.730 --> 00:19:41.560
Why do you think
humans are this way?

00:19:41.560 --> 00:19:45.330
Well, let me give you an
example that I think really

00:19:45.330 --> 00:19:49.430
tells us why the action
is not between our ears.

00:19:49.430 --> 00:19:52.490
The action is in our
social networks, OK?

00:19:52.490 --> 00:19:53.610
We are a social species.

00:19:53.610 --> 00:19:55.000
We evolved that way.

00:19:55.000 --> 00:19:55.990
Why?

00:19:55.990 --> 00:19:59.390
Let me give you a really
graphic example of that.

00:19:59.390 --> 00:20:01.350
So this is a site called eToro.

00:20:01.350 --> 00:20:03.410
It's a social network site.

00:20:03.410 --> 00:20:06.190
On this site, people buy
and sell dollars and euros

00:20:06.190 --> 00:20:09.460
and gold and silver,
and stuff like that, OK?

00:20:09.460 --> 00:20:12.540
And unlike almost every
other trading platform,

00:20:12.540 --> 00:20:13.530
it's a social platform.

00:20:13.530 --> 00:20:16.410
So I can see what
every other person,

00:20:16.410 --> 00:20:19.137
what every of these 1.6
million people are doing.

00:20:19.137 --> 00:20:20.720
You can't see the
dollar amount, but I

00:20:20.720 --> 00:20:22.760
can see that they're
shorting euros,

00:20:22.760 --> 00:20:25.640
long dollar,
leveraged 25, right?

00:20:25.640 --> 00:20:28.490
One day contract, and I
can see how much money

00:20:28.490 --> 00:20:30.360
they made at the end.

00:20:30.360 --> 00:20:32.290
I could see their
return on investment.

00:20:32.290 --> 00:20:34.560
So here are people playing
with their own money,

00:20:34.560 --> 00:20:37.380
substantial amounts of their
own money, millions of them

00:20:37.380 --> 00:20:41.732
all over the world, doing
this very regularly.

00:20:41.732 --> 00:20:44.440
Sort of average maybe one a
day, that sort of transaction,

00:20:44.440 --> 00:20:44.940
right?

00:20:44.940 --> 00:20:47.350
A couple, maybe a couple days.

00:20:47.350 --> 00:20:50.380
And we can make a graph
of the social network.

00:20:50.380 --> 00:20:51.255
And that's what this.

00:20:51.255 --> 00:20:53.510
This is 1.6 million people.

00:20:53.510 --> 00:20:56.650
These are the same
1.6 million people.

00:20:56.650 --> 00:20:59.410
And wherever there's
a dot, this person

00:20:59.410 --> 00:21:03.440
decided to follow
this other person.

00:21:03.440 --> 00:21:05.920
And follow in eToro
has a different meaning

00:21:05.920 --> 00:21:07.470
than Facebook.

00:21:07.470 --> 00:21:09.750
Follow means I'm going
to take 10% of my money,

00:21:09.750 --> 00:21:13.430
and whatever that person does,
my 10% percent of the money

00:21:13.430 --> 00:21:15.970
will be invested
exactly the same way.

00:21:15.970 --> 00:21:18.400
So this is follow
with teeth, OK?

00:21:18.400 --> 00:21:20.510
And this is the
graph of following.

00:21:20.510 --> 00:21:23.080
So these are people
learning from each other,

00:21:23.080 --> 00:21:26.030
looking at each other
strategies, and trading.

00:21:26.030 --> 00:21:27.832
And you see some people
are going it alone.

00:21:27.832 --> 00:21:29.790
They read the newspaper,
they look at the wire,

00:21:29.790 --> 00:21:32.370
they browse the web,
then they trade.

00:21:32.370 --> 00:21:36.090
Other people are in this orgy
of social dialogue, right?

00:21:36.090 --> 00:21:37.640
All following each other.

00:21:37.640 --> 00:21:39.140
And, in fact, if
you look at it, you

00:21:39.140 --> 00:21:41.320
see that there are all
these loops in there.

00:21:41.320 --> 00:21:45.160
So, you know, I follow you,
you follow him, he follows me.

00:21:45.160 --> 00:21:47.222
Uh oh. [LAUGHS]

00:21:47.222 --> 00:21:48.930
And what happens in
this loop is that you

00:21:48.930 --> 00:21:50.940
get very few new ideas.

00:21:50.940 --> 00:21:52.801
It's the same ideas
going around and around.

00:21:52.801 --> 00:21:54.300
And this is the
sort of thing that's

00:21:54.300 --> 00:21:56.980
a precursor of an
economic bubble.

00:21:56.980 --> 00:22:01.690
That the question is, which of
these sort of social strategies

00:22:01.690 --> 00:22:03.750
gives greater return
on investment?

00:22:03.750 --> 00:22:06.540
Why are we a social species?

00:22:06.540 --> 00:22:09.870
The way that people
almost always analyze it

00:22:09.870 --> 00:22:11.160
is greater information.

00:22:11.160 --> 00:22:13.820
That these guys all read the
newspapers and everything.

00:22:13.820 --> 00:22:16.580
They have all the
information in the world, OK?

00:22:16.580 --> 00:22:19.700
These guys read the same
newspapers and everything,

00:22:19.700 --> 00:22:22.080
but they also look
at each other.

00:22:22.080 --> 00:22:23.460
What would you expect to happen?

00:22:23.460 --> 00:22:27.280
Well, you can write
down the equations here.

00:22:27.280 --> 00:22:29.200
And what you can
do is you can look

00:22:29.200 --> 00:22:32.450
at the propagation
of new strategies

00:22:32.450 --> 00:22:34.850
across this population.

00:22:34.850 --> 00:22:37.910
So when this guy comes up
with a new thing to do,

00:22:37.910 --> 00:22:41.520
how likely is it to propagate
throughout the social network?

00:22:41.520 --> 00:22:43.310
And in that way,
you can quantify

00:22:43.310 --> 00:22:48.120
the number and diversity of new
strategies any one person sees.

00:22:48.120 --> 00:22:50.200
These people will see
about no new strategies

00:22:50.200 --> 00:22:51.850
because they're
all on their own.

00:22:51.850 --> 00:22:53.827
These people will see
very few new strategies

00:22:53.827 --> 00:22:55.660
because they're all
listening to each other.

00:22:55.660 --> 00:22:57.270
It's the same thing
around and around.

00:22:57.270 --> 00:23:00.070
And these people are
much more diverse.

00:23:00.070 --> 00:23:02.560
If we look at that
return on investment,

00:23:02.560 --> 00:23:05.090
we get a curve like this.

00:23:05.090 --> 00:23:07.080
So, again, this is a
mathematical function

00:23:07.080 --> 00:23:10.140
that has to do with the
number of new strategies.

00:23:10.140 --> 00:23:12.800
So the rate of
propagation of strategies

00:23:12.800 --> 00:23:16.480
through the medium,
the social network.

00:23:16.480 --> 00:23:19.030
If you look at the
number of new things,

00:23:19.030 --> 00:23:21.080
this is very low,
this is very low,

00:23:21.080 --> 00:23:23.730
this has many new
types of strategies.

00:23:23.730 --> 00:23:27.140
And this vertical axis
is return on investment.

00:23:27.140 --> 00:23:30.950
So this is, like, one of these
no BS types of majors, OK?

00:23:30.950 --> 00:23:33.550
Real people, their
own money, doing it

00:23:33.550 --> 00:23:36.200
on their own choice,
making money, or not.

00:23:36.200 --> 00:23:38.770
People who trade by
themselves are market neutral.

00:23:38.770 --> 00:23:41.470
You might expect
that on average.

00:23:41.470 --> 00:23:43.140
They hire the market.

00:23:43.140 --> 00:23:45.440
They lose a little bit of
money in trading costs.

00:23:45.440 --> 00:23:47.900
People who are in
these momentum echo

00:23:47.900 --> 00:23:50.340
chambers don't do
very well either.

00:23:50.340 --> 00:23:51.990
And what isn't shown
here is sometimes

00:23:51.990 --> 00:23:54.810
there are crashes
that blow them all up.

00:23:54.810 --> 00:23:58.000
So they actually do pretty
badly on the long term.

00:23:58.000 --> 00:24:01.960
But people in the middle
make 30% more money.

00:24:01.960 --> 00:24:07.270
So this is not something that
is in traditional economics.

00:24:07.270 --> 00:24:09.180
What we're talking
about here is a blend

00:24:09.180 --> 00:24:11.440
of social strategies
for learning

00:24:11.440 --> 00:24:14.030
from other people, plus
individual information.

00:24:14.030 --> 00:24:16.370
It's the peer to
peer interactions.

00:24:16.370 --> 00:24:20.520
And probably the reason that
we have a social species,

00:24:20.520 --> 00:24:22.890
this learning from
each other, is

00:24:22.890 --> 00:24:26.690
because it has this much
more efficient output.

00:24:26.690 --> 00:24:28.510
And there's a big
literature about this.

00:24:28.510 --> 00:24:30.060
Don't just believe me.

00:24:30.060 --> 00:24:33.930
This is a wonderful example
because I can quantify it.

00:24:33.930 --> 00:24:36.520
And every doc here
is all in the trades

00:24:36.520 --> 00:24:39.010
by millions of people
for a whole day.

00:24:39.010 --> 00:24:41.870
So this is, like, more
data than you know, right?

00:24:41.870 --> 00:24:43.640
And if I did just
one asset class,

00:24:43.640 --> 00:24:48.190
like dollars versus euros,
it wouldn't have this spread

00:24:48.190 --> 00:24:49.120
that it does.

00:24:49.120 --> 00:24:50.635
It would be a nice band.

00:24:50.635 --> 00:24:53.170
So as you get more
diverse learning

00:24:53.170 --> 00:24:56.450
from your social environment,
your return on investment

00:24:56.450 --> 00:25:00.030
goes up until you begin
getting too many loops.

00:25:00.030 --> 00:25:02.300
And then it goes back down.

00:25:02.300 --> 00:25:05.530
Now I like this example because
I think this example applies

00:25:05.530 --> 00:25:09.660
to the government, it applies to
making decisions in companies.

00:25:09.660 --> 00:25:11.740
If you begin thinking
about it, we're

00:25:11.740 --> 00:25:13.190
all living these
social networks,

00:25:13.190 --> 00:25:16.420
and what we're trying to
do is make good decisions.

00:25:16.420 --> 00:25:18.730
Here, I'm showing you that
a mixture of social learning

00:25:18.730 --> 00:25:21.780
plus individual learning-- I can
tell you a lot more about it,

00:25:21.780 --> 00:25:24.980
it's in the book-- gets
you better decisions.

00:25:24.980 --> 00:25:27.530
And not just better
decisions by one person,

00:25:27.530 --> 00:25:32.256
this is better decisions of
the entire 1.6 million people.

00:25:32.256 --> 00:25:34.972
Now, that's a really
different thing.

00:25:34.972 --> 00:25:36.930
I should also mention
that one of the things we

00:25:36.930 --> 00:25:40.410
did with this platform is when
we discovered that they were

00:25:40.410 --> 00:25:42.680
in this echo chamber
state, that's

00:25:42.680 --> 00:25:45.500
not good for the
platform or them, OK?

00:25:45.500 --> 00:25:47.140
Everyone's going to lose money.

00:25:47.140 --> 00:25:49.970
So we looked at
the loop structure,

00:25:49.970 --> 00:25:52.740
and we figured out how
best was the optimal way

00:25:52.740 --> 00:25:54.570
to break it up was.

00:25:54.570 --> 00:25:57.050
And we gave coupons
to key people,

00:25:57.050 --> 00:26:00.810
small group of key people, that
would cause this echo chamber

00:26:00.810 --> 00:26:04.330
to break up in an
optimal manner.

00:26:04.330 --> 00:26:07.920
And that doubled the return
on investment of the people

00:26:07.920 --> 00:26:10.430
in the entire network.

00:26:10.430 --> 00:26:12.512
And that lasted for about
three days, four days,

00:26:12.512 --> 00:26:14.220
and they went back to
being stupid again.

00:26:14.220 --> 00:26:16.932
But [LAUGHS] that's
their problem.

00:26:16.932 --> 00:26:18.390
We've done this
sort of repeatedly.

00:26:18.390 --> 00:26:20.170
We know it works.

00:26:20.170 --> 00:26:22.090
So you can actually
control the flow

00:26:22.090 --> 00:26:24.810
of ideas in a network
like this and improve

00:26:24.810 --> 00:26:28.049
the average function of
the people in the network.

00:26:28.049 --> 00:26:29.590
It's a very different
way of thinking

00:26:29.590 --> 00:26:31.740
about things than the normal
way because you're not

00:26:31.740 --> 00:26:33.610
concerned about individuals.

00:26:33.610 --> 00:26:36.250
You're not concerned about their
education and their decision

00:26:36.250 --> 00:26:36.940
making.

00:26:36.940 --> 00:26:39.140
You're concerned about
the pattern of learning

00:26:39.140 --> 00:26:40.970
and the performance
of an ensemble, rather

00:26:40.970 --> 00:26:43.440
than the individuals.

00:26:43.440 --> 00:26:46.760
So one of the other things
that this big data tells us

00:26:46.760 --> 00:26:50.340
is that this process can be
broken up into two pieces.

00:26:50.340 --> 00:26:54.630
And to illustrate that, I'll
show this diagram that's

00:26:54.630 --> 00:26:57.560
from Danny Kahneman's
Nobel Prize lecture.

00:26:57.560 --> 00:27:00.120
He's the father of
behavioral economics.

00:27:00.120 --> 00:27:01.960
And he makes the
point that people

00:27:01.960 --> 00:27:03.630
have two ways of thinking.

00:27:03.630 --> 00:27:05.162
There's a slow way of thinking.

00:27:05.162 --> 00:27:06.870
[INAUDIBLE] probably
knows about thinking

00:27:06.870 --> 00:27:09.480
fast and slow, very popular.

00:27:09.480 --> 00:27:14.420
Slow way of thinking that's the
serial reasoning that we do.

00:27:14.420 --> 00:27:16.660
And there's this
fast way of thinking,

00:27:16.660 --> 00:27:18.361
which is associations.

00:27:18.361 --> 00:27:19.860
You take the
experience you had, you

00:27:19.860 --> 00:27:23.570
say, how is this situation
like my previous experiences?

00:27:23.570 --> 00:27:25.220
Maybe you interpolate
a little bit,

00:27:25.220 --> 00:27:27.510
and you make your
decision very, very fast.

00:27:27.510 --> 00:27:32.210
This is a very old mechanism,
100, 200 million years old.

00:27:32.210 --> 00:27:35.040
This is pretty much
unique to humans.

00:27:35.040 --> 00:27:39.970
Interestingly, this is the
much better mechanism by far

00:27:39.970 --> 00:27:43.212
if you have the right
set of experiences.

00:27:43.212 --> 00:27:45.170
If you don't have the
right set of experiences,

00:27:45.170 --> 00:27:47.714
this is a disaster waiting
to happen because you're

00:27:47.714 --> 00:27:50.130
going to associate the wrong
things with the wrong things,

00:27:50.130 --> 00:27:52.530
and follow right off the cliff.

00:27:52.530 --> 00:27:55.190
And when I look at the
learning that people

00:27:55.190 --> 00:27:57.930
have from each other in
these social networks,

00:27:57.930 --> 00:28:02.280
I see a qualitatively
different type of behavior.

00:28:02.280 --> 00:28:04.330
So when I look at
slow learning--

00:28:04.330 --> 00:28:07.570
so this is a learning
that people integrate

00:28:07.570 --> 00:28:10.270
into their conscious
representations.

00:28:10.270 --> 00:28:13.260
So the new song you
heard, the new fact,

00:28:13.260 --> 00:28:15.160
the new product that came out.

00:28:15.160 --> 00:28:17.270
People are very
promiscuous about this.

00:28:17.270 --> 00:28:19.570
It only takes one
exposure to integrate that

00:28:19.570 --> 00:28:22.420
into your ensemble of
things you know about.

00:28:22.420 --> 00:28:23.990
And this is a way
almost everything

00:28:23.990 --> 00:28:26.930
that you guys build is based on.

00:28:26.930 --> 00:28:29.350
Oh, we're going to have
more information, right?

00:28:29.350 --> 00:28:32.230
But information is not behavior.

00:28:32.230 --> 00:28:35.300
It turns out that to get
behavior change, which

00:28:35.300 --> 00:28:38.790
is what I call idea flow,
you need something different.

00:28:38.790 --> 00:28:41.900
So this is an exploration.

00:28:41.900 --> 00:28:45.140
We are trying to find new
possibilities and new facts.

00:28:45.140 --> 00:28:47.790
But it's relatively isolated
from behavior change.

00:28:47.790 --> 00:28:49.570
You could learn about
all sorts of things

00:28:49.570 --> 00:28:51.070
and never change your behavior.

00:28:51.070 --> 00:28:53.320
This is why it's
hard to stop smoking,

00:28:53.320 --> 00:28:56.150
this is why it's hard
to stop overeating,

00:28:56.150 --> 00:28:58.020
why all sorts of
things are hard is

00:28:58.020 --> 00:29:03.097
that our habits, our actual
behaviors, that reside here

00:29:03.097 --> 00:29:04.430
are largely independent of this.

00:29:04.430 --> 00:29:06.205
Now, there's some leakage.

00:29:06.205 --> 00:29:09.650
If you concentrate real hard,
some early adopters, yes, it

00:29:09.650 --> 00:29:10.630
does happen.

00:29:10.630 --> 00:29:13.500
But as I showed with
the voting experiment,

00:29:13.500 --> 00:29:17.300
the transfer from here
to here is very weak.

00:29:17.300 --> 00:29:19.470
On the other hand,
what is the case

00:29:19.470 --> 00:29:22.720
is that if you see multiple
people experimenting

00:29:22.720 --> 00:29:26.170
with the same idea, people whom
you have strong relationships

00:29:26.170 --> 00:29:31.790
with, then you will
with very high certainty

00:29:31.790 --> 00:29:34.200
tend to adopt that behavior, OK?

00:29:34.200 --> 00:29:36.350
So what you're doing is
this social learning.

00:29:36.350 --> 00:29:38.040
If I see for some
of my peers that

00:29:38.040 --> 00:29:40.490
doing this results
in a better outcome,

00:29:40.490 --> 00:29:44.580
then without even thinking about
it, I'll begin to adopt that.

00:29:44.580 --> 00:29:47.845
If I hear about it, you know,
through email or on the web

00:29:47.845 --> 00:29:50.080
or something, it's
very unlikely.

00:29:50.080 --> 00:29:54.566
We have a database
of all of-- I can't

00:29:54.566 --> 00:29:56.440
say the name of the
company, but a competitor

00:29:56.440 --> 00:29:58.390
bought it for $1 billion.

00:29:58.390 --> 00:30:01.280
The social network for
the inside companies.

00:30:01.280 --> 00:30:05.010
So we have the deployment for
over 1,000 companies in there,

00:30:05.010 --> 00:30:06.280
using that social network.

00:30:06.280 --> 00:30:09.810
And what we find can be summed
up in an interesting statistic.

00:30:09.810 --> 00:30:14.340
If you get invitations to
join this intracompany social

00:30:14.340 --> 00:30:18.440
network from as many as 12
people in a half an hour,

00:30:18.440 --> 00:30:21.000
you're still
unlikely to sign up,

00:30:21.000 --> 00:30:22.960
unless those people
are people you already

00:30:22.960 --> 00:30:24.829
have strong relationships with.

00:30:24.829 --> 00:30:26.870
If they're people you know
face to face or people

00:30:26.870 --> 00:30:30.170
you work with regularly, then
as few as three invitations

00:30:30.170 --> 00:30:33.180
makes it almost certain
that you'll sign up.

00:30:33.180 --> 00:30:35.460
So that's just like
the voting thing,

00:30:35.460 --> 00:30:37.420
it's what I'm
talking about here.

00:30:37.420 --> 00:30:40.310
Behavior change, which is
what you usually care about,

00:30:40.310 --> 00:30:42.910
has to do with this social
reinforcement mechanism

00:30:42.910 --> 00:30:44.300
that I call engagement.

00:30:44.300 --> 00:30:48.660
It's community vetting
of ideas and behaviors

00:30:48.660 --> 00:30:51.060
that results in the
adoption of a new behavior.

00:30:51.060 --> 00:30:55.250
It's not the broadcast, in
fact, that we often think about.

00:30:55.250 --> 00:30:56.850
So let me show you
an example of this.

00:30:56.850 --> 00:31:00.190
So this is data
from a German bank.

00:31:00.190 --> 00:31:04.630
It has five departments,
managers, development, sales,

00:31:04.630 --> 00:31:07.230
support, customer
service is the last one.

00:31:07.230 --> 00:31:10.660
And this is all the
email, and the red stuff

00:31:10.660 --> 00:31:12.960
is all the face to face.

00:31:12.960 --> 00:31:15.420
We get this off of little
badges we put on people.

00:31:15.420 --> 00:31:18.570
So you probably can
track this stuff,

00:31:18.570 --> 00:31:20.570
but you've never tracked
the face to face stuff.

00:31:20.570 --> 00:31:22.280
Nobody does.

00:31:22.280 --> 00:31:26.420
And what we find is that
the sort of punch line

00:31:26.420 --> 00:31:29.545
is that the email,
the pattern of email,

00:31:29.545 --> 00:31:31.170
has very little to
do with productivity

00:31:31.170 --> 00:31:33.080
or creative output.

00:31:33.080 --> 00:31:35.960
But the pattern of rich
channels of communication

00:31:35.960 --> 00:31:37.370
has a huge amount.

00:31:37.370 --> 00:31:39.830
So I'll show you a slightly
distracting thing first,

00:31:39.830 --> 00:31:42.220
and then I'll tell you
the real punchline here.

00:31:42.220 --> 00:31:45.180
So these guys are going
to do an ad campaign.

00:31:45.180 --> 00:31:47.730
They're starting now where the
boss sends out lots of email

00:31:47.730 --> 00:31:50.940
to have lots of meetings
to figure out how to do it.

00:31:50.940 --> 00:31:54.760
During that time, nobody
talks to customer service.

00:31:54.760 --> 00:32:01.650
They deploy the thing, it's a
disaster, and as a consequence,

00:32:01.650 --> 00:32:04.176
they deploy it now.

00:32:04.176 --> 00:32:06.550
And then they have all day
meetings with customer service

00:32:06.550 --> 00:32:09.800
to figure out how to fix it, OK?

00:32:09.800 --> 00:32:12.170
So the real punchline,
because we've

00:32:12.170 --> 00:32:14.830
done some dozens
of companies now,

00:32:14.830 --> 00:32:18.990
is that you can see the pattern
of rich channel communication,

00:32:18.990 --> 00:32:21.860
and that predicts typically
30%, and sometimes 40%

00:32:21.860 --> 00:32:25.510
of the variation in
productivity of work groups.

00:32:25.510 --> 00:32:30.129
30% to 40% is bigger by far
than anything that you look at.

00:32:30.129 --> 00:32:32.170
I mean, you'd have to like
kill the people to get

00:32:32.170 --> 00:32:34.020
that big of an effect.

00:32:34.020 --> 00:32:36.170
And the mathematical
formulation of it

00:32:36.170 --> 00:32:38.970
is basically a probability
that if I talk to you

00:32:38.970 --> 00:32:41.260
and I talk to you, what's
the likelihood that you

00:32:41.260 --> 00:32:42.560
two also talk to each other?

00:32:42.560 --> 00:32:44.630
It's those loops.

00:32:44.630 --> 00:32:47.000
And it's this learning
from each other,

00:32:47.000 --> 00:32:49.590
keeping people in the loop,
nice little mathematical

00:32:49.590 --> 00:32:53.680
relationship, that
predicts this productivity.

00:32:53.680 --> 00:32:55.180
And there's another
thing, so that's

00:32:55.180 --> 00:32:56.850
that engagement I
was talking about.

00:32:56.850 --> 00:32:59.330
There's another
[INAUDIBLE] exploration,

00:32:59.330 --> 00:33:01.280
and that's the stuff
that your boss tells

00:33:01.280 --> 00:33:03.300
you is not in your
job description.

00:33:03.300 --> 00:33:05.700
That's going to talk
to the people in sales,

00:33:05.700 --> 00:33:08.240
or the janitors, or the
people at the other company.

00:33:08.240 --> 00:33:12.210
Just picking up new ideas,
new facts, new observations,

00:33:12.210 --> 00:33:14.970
and bringing them back
to your work group

00:33:14.970 --> 00:33:17.280
to bang against each
other and see if they make

00:33:17.280 --> 00:33:21.760
sense to do that social
learning process, OK?

00:33:21.760 --> 00:33:24.760
I wrote a paper for
Harvard Business

00:33:24.760 --> 00:33:28.350
Review that lays this out.

00:33:28.350 --> 00:33:30.600
It's called the "New Science
of Building Great Teams."

00:33:30.600 --> 00:33:33.516
And it won Paper of the
Year award, which is nice.

00:33:33.516 --> 00:33:34.890
But it also won
Paper of the Year

00:33:34.890 --> 00:33:38.070
from the Academy of Management,
which is the academic side.

00:33:38.070 --> 00:33:39.820
And that's the first
time, I believe,

00:33:39.820 --> 00:33:44.010
that Harvard Business Review
and the academic business guys

00:33:44.010 --> 00:33:45.360
have ever agreed.

00:33:45.360 --> 00:33:49.250
So maybe it's worth
taking a look at.

00:33:49.250 --> 00:33:50.872
Anyway, so that's companies.

00:33:50.872 --> 00:33:52.080
Let's look at the real world.

00:33:52.080 --> 00:33:54.830
So this is a company I
helped found in 2006.

00:33:54.830 --> 00:33:56.480
Sold to AT&amp;T's
mobile advertising

00:33:56.480 --> 00:33:57.980
that's been out recently.

00:33:57.980 --> 00:34:00.180
People moving around
in San Francisco.

00:34:00.180 --> 00:34:02.275
Big dots are the
most popular places.

00:34:02.275 --> 00:34:04.150
Maybe some of you guys
have seen this before.

00:34:04.150 --> 00:34:04.850
I like it.

00:34:04.850 --> 00:34:07.080
I show it often.

00:34:07.080 --> 00:34:09.469
Looks like a nicely
mixed city, but actually,

00:34:09.469 --> 00:34:12.909
if you analyze it, if you
cluster people by their paths

00:34:12.909 --> 00:34:16.920
and by their exposure to
each other, what you find

00:34:16.920 --> 00:34:19.070
is you find that it's
a very segregated city.

00:34:19.070 --> 00:34:21.960
There's these groups of
people that hardly ever

00:34:21.960 --> 00:34:24.000
are exposed to each other.

00:34:24.000 --> 00:34:26.719
And then there's other
groups within the group,

00:34:26.719 --> 00:34:29.920
they don't know each other,
but they go to the same places,

00:34:29.920 --> 00:34:33.870
they see the same things, and
they have the same habits.

00:34:33.870 --> 00:34:36.620
So in other words, they
have very strong engagement

00:34:36.620 --> 00:34:41.714
within the groups and they learn
habits of behavior from that.

00:34:41.714 --> 00:34:44.260
Now, sometimes that's good.

00:34:44.260 --> 00:34:47.589
So, for instance, sometimes
it's sort of trivial.

00:34:47.589 --> 00:34:49.630
It's like you might discover
that one group here,

00:34:49.630 --> 00:34:52.679
you get a fad for red dresses.

00:34:52.679 --> 00:34:53.659
No particular reason.

00:34:53.659 --> 00:34:56.894
It's just what people
in this group do, OK?

00:34:56.894 --> 00:34:58.310
In another group,
though, what you

00:34:58.310 --> 00:35:02.850
find is you find that they
have a different attitude

00:35:02.850 --> 00:35:05.870
about paying back credit
cards than maybe you do.

00:35:05.870 --> 00:35:08.160
And so they don't have
such good risk scores.

00:35:08.160 --> 00:35:10.564
Again, it's not anything
that they thought about.

00:35:10.564 --> 00:35:12.230
It's just what people
in their group do.

00:35:12.230 --> 00:35:13.705
They learn from each other.

00:35:13.705 --> 00:35:17.000
[INAUDIBLE] George just, like,
threw it away, got a new one.

00:35:17.000 --> 00:35:18.760
Nobody came after him.

00:35:18.760 --> 00:35:21.310
It's the smart
thing to do, right?

00:35:21.310 --> 00:35:22.940
And then the other
thing that you find,

00:35:22.940 --> 00:35:26.250
which is very important,
is chronic diseases

00:35:26.250 --> 00:35:28.580
vary by group
because of behavior.

00:35:28.580 --> 00:35:32.220
Chronic diseases are mostly
a function of your behavior.

00:35:32.220 --> 00:35:34.010
You eat too much,
you drink too much,

00:35:34.010 --> 00:35:35.910
you don't exercise
enough, all those things.

00:35:35.910 --> 00:35:37.860
And a lot of other
things we don't know.

00:35:37.860 --> 00:35:40.820
But we don't know why
that particular group is

00:35:40.820 --> 00:35:42.490
susceptible to
diabetes, but we know

00:35:42.490 --> 00:35:46.820
they're very much more
susceptible than other people.

00:35:46.820 --> 00:35:51.126
It seems to be that they learned
bad habits from each other, OK?

00:35:54.830 --> 00:35:57.900
So I'm going to give a
TED talk in a little bit,

00:35:57.900 --> 00:35:59.180
so I thought I'd put this in.

00:35:59.180 --> 00:36:02.490
So what TED does-- and his
idea's worth spreading, right?

00:36:02.490 --> 00:36:07.490
Make his wonderful videos
blast him out to everybody.

00:36:07.490 --> 00:36:09.260
And what that's doing
is it's increasing

00:36:09.260 --> 00:36:10.135
people's exploration.

00:36:10.135 --> 00:36:13.830
You got a million views of
this little movie, da da da da,

00:36:13.830 --> 00:36:14.400
right?

00:36:14.400 --> 00:36:17.740
But it doesn't change behavior.

00:36:17.740 --> 00:36:18.627
That's my prediction.

00:36:18.627 --> 00:36:20.210
That's what I see
in all of this data.

00:36:20.210 --> 00:36:22.750
What changes behavior
is all the peer

00:36:22.750 --> 00:36:24.960
to peer communication
that comes afterwards,

00:36:24.960 --> 00:36:26.760
where you say, what
do you think of that?

00:36:26.760 --> 00:36:28.560
Another guy says, oh,
yeah, that's awesome.

00:36:28.560 --> 00:36:31.150
And then the third
guy says, well, OK.

00:36:31.150 --> 00:36:34.630
And you get this validation
among your peer group,

00:36:34.630 --> 00:36:38.577
and that's what leads you to
actually change your behavior.

00:36:38.577 --> 00:36:41.160
So if you like what I'm saying,
if you think it's interesting,

00:36:41.160 --> 00:36:43.720
talk to your peer
group, [LAUGHS] right?

00:36:43.720 --> 00:36:45.095
Maybe it'll change
your behavior.

00:36:48.290 --> 00:36:51.770
So you can actually do
something interesting today,

00:36:51.770 --> 00:36:54.440
which is you can use
these ideas to map

00:36:54.440 --> 00:36:58.030
stuff in the entire cities.

00:36:58.030 --> 00:37:02.535
So this is mixing of different
communities in Mexico.

00:37:02.535 --> 00:37:04.350
And I should start
it over again.

00:37:04.350 --> 00:37:07.780
So the red stuff is where
the most mixing happens,

00:37:07.780 --> 00:37:11.250
and the yellow stuff is where
very little mixing happens.

00:37:11.250 --> 00:37:13.770
And if it's blank, we
don't have the data.

00:37:13.770 --> 00:37:16.930
So you can see on Sundays,
there's very little mixing.

00:37:16.930 --> 00:37:18.320
People stay home.

00:37:18.320 --> 00:37:20.530
Monday, a lot of
people come out.

00:37:20.530 --> 00:37:23.250
So according to the
things that I've told you,

00:37:23.250 --> 00:37:25.720
it's this mixing
between communities

00:37:25.720 --> 00:37:30.170
that's the source of a
lot of the innovation

00:37:30.170 --> 00:37:34.280
and creative output
in a community.

00:37:34.280 --> 00:37:36.720
It's the banging
together of ideas

00:37:36.720 --> 00:37:39.980
that causes innovation and
better social outcomes.

00:37:39.980 --> 00:37:42.531
And you can now do this--
and this is just cell tower

00:37:42.531 --> 00:37:43.030
activity.

00:37:43.030 --> 00:37:45.360
This is not any
personal data at all.

00:37:45.360 --> 00:37:47.400
This is just at the
level of cell towers.

00:37:47.400 --> 00:37:52.600
You say, well, which
parts of the community

00:37:52.600 --> 00:37:54.890
do the people at this
cell tower come from, OK?

00:37:54.890 --> 00:37:56.930
You don't know the
individual people.

00:37:56.930 --> 00:37:59.180
But you can use this
in interesting ways.

00:37:59.180 --> 00:38:01.470
If I use that same
method-- this is

00:38:01.470 --> 00:38:02.970
something we did
in the Ivory Coast.

00:38:02.970 --> 00:38:05.110
I helped convince
the carrier, Orange,

00:38:05.110 --> 00:38:07.970
release all of their cell
tower data for the Ivory Coast.

00:38:07.970 --> 00:38:09.870
Ivory Coast is a
very poor country

00:38:09.870 --> 00:38:11.710
that also had a civil war.

00:38:11.710 --> 00:38:13.840
So the government can't
go in the northern half

00:38:13.840 --> 00:38:16.130
of the country.

00:38:16.130 --> 00:38:18.030
What they do is they
have poverty statistics

00:38:18.030 --> 00:38:19.440
for the lower half.

00:38:19.440 --> 00:38:24.260
And using this method,
you can fit the statistics

00:38:24.260 --> 00:38:27.390
for poverty in the lower
half and extrapolate them

00:38:27.390 --> 00:38:29.230
to the upper half.

00:38:29.230 --> 00:38:31.910
And the poverty that your
measuring is interesting.

00:38:31.910 --> 00:38:33.990
So this has to do
with two factors.

00:38:33.990 --> 00:38:37.180
It's this exploration outside
the community and engagement

00:38:37.180 --> 00:38:39.730
within the community, OK?

00:38:39.730 --> 00:38:44.534
And so this MPI is a
multi-factor thing.

00:38:44.534 --> 00:38:45.950
And it's a combination
of poverty,

00:38:45.950 --> 00:38:49.870
but also life expectancy,
crime, and infant mortality

00:38:49.870 --> 00:38:52.140
because they all co-vary
with one another.

00:38:52.140 --> 00:38:53.119
So that's an example.

00:38:53.119 --> 00:38:54.410
So you saw it with Mexico City.

00:38:54.410 --> 00:38:55.444
You see it here.

00:38:55.444 --> 00:38:56.860
This is something
a former student

00:38:56.860 --> 00:39:00.070
of mine, Nathan [INAUDIBLE] did.

00:39:00.070 --> 00:39:02.722
He took all the data
from councils in the UK.

00:39:02.722 --> 00:39:04.180
So there are
neighborhoods that are

00:39:04.180 --> 00:39:06.530
administrative units in the UK.

00:39:06.530 --> 00:39:09.760
And he looked at their
socioeconomic outcome

00:39:09.760 --> 00:39:14.450
index, which is, again, poverty,
crime, infant mortality stuff,

00:39:14.450 --> 00:39:17.260
and compared it to the
land line phone records,

00:39:17.260 --> 00:39:20.490
and measured two things,
which are very similar to what

00:39:20.490 --> 00:39:23.560
I call exploration
and engagement

00:39:23.560 --> 00:39:25.450
and generated this graph.

00:39:25.450 --> 00:39:28.800
So when you get a community
that doesn't talk to itself

00:39:28.800 --> 00:39:32.039
and doesn't talk much outside
of itself, all the babies die.

00:39:32.039 --> 00:39:33.080
Well, not all the babies.

00:39:33.080 --> 00:39:34.630
A lot of babies die.

00:39:34.630 --> 00:39:37.860
When you get a community where
they're richly integrating

00:39:37.860 --> 00:39:41.600
into the rest of society and
they talk among themselves,

00:39:41.600 --> 00:39:44.550
very few babies die.

00:39:44.550 --> 00:39:46.960
And, of course, this
is a richer community,

00:39:46.960 --> 00:39:48.440
that's a poorer
community, that has

00:39:48.440 --> 00:39:51.800
less crime, that has more crime.

00:39:51.800 --> 00:39:53.855
So we've seen this
now in several places.

00:39:53.855 --> 00:39:57.040
We've seen it in England,
we've seen it in Ivory Coast,

00:39:57.040 --> 00:39:59.080
we've seen it in Cambridge.

00:39:59.080 --> 00:40:01.460
And you can begin
doing things with this.

00:40:01.460 --> 00:40:05.290
So for instance,
you can use this

00:40:05.290 --> 00:40:07.990
to be able to predict
GDP in cities.

00:40:07.990 --> 00:40:12.670
So we took the GDP for 150
cities in Europe and 150 cities

00:40:12.670 --> 00:40:17.830
in the US, and we measured
the amount of banging together

00:40:17.830 --> 00:40:21.030
of ideas that you got in rich
channels of communication,

00:40:21.030 --> 00:40:22.750
face to face primarily.

00:40:22.750 --> 00:40:25.100
And we did that by using
things like Foursquare

00:40:25.100 --> 00:40:27.720
to ask, how often do
people come together

00:40:27.720 --> 00:40:29.720
from different communities
and at what distance?

00:40:29.720 --> 00:40:32.330
You can print a nice
mathematical function.

00:40:32.330 --> 00:40:36.160
It's the same form in
Europe as it is in the US.

00:40:36.160 --> 00:40:39.140
It varies by the density of
the city and the transportation

00:40:39.140 --> 00:40:39.997
infrastructure.

00:40:39.997 --> 00:40:42.080
So if it's a very dense
setting with a really good

00:40:42.080 --> 00:40:43.880
transportation
infrastructure, you

00:40:43.880 --> 00:40:45.350
run into a lot of
different people,

00:40:45.350 --> 00:40:47.720
there's a lot of ideas
banging together,

00:40:47.720 --> 00:40:50.270
and you get a lot of innovation.

00:40:50.270 --> 00:40:52.040
And so what this is
showing is a measure

00:40:52.040 --> 00:40:57.160
of this face to face
engaging and exploration.

00:40:57.160 --> 00:41:00.420
And this is GDP per head.

00:41:00.420 --> 00:41:03.040
I put kilometer, I think
actually that one is.

00:41:03.040 --> 00:41:05.070
And you can see that it
accounts for around 90%

00:41:05.070 --> 00:41:08.290
of the variance, which in social
science, is like a, you know,

00:41:08.290 --> 00:41:10.600
law from above or
something like that.

00:41:10.600 --> 00:41:12.350
In other words, if you
tell me the density

00:41:12.350 --> 00:41:14.620
of the city and the
transportation infrastructure,

00:41:14.620 --> 00:41:17.890
and actually just need to tell
me the average commute time,

00:41:17.890 --> 00:41:21.200
I can tell you the
GDP almost perfectly.

00:41:21.200 --> 00:41:25.090
Similarly, if I tell you that
mobility or the call pattern

00:41:25.090 --> 00:41:27.250
in a neighborhood,
I can tell you

00:41:27.250 --> 00:41:32.070
the GDP, the number of infant
mortality, and the crime rate.

00:41:32.070 --> 00:41:36.585
Again, r squared of
about 0.85 is like a law.

00:41:36.585 --> 00:41:38.380
It's amazing, OK?

00:41:38.380 --> 00:41:39.820
And that opens up
the possibility

00:41:39.820 --> 00:41:41.540
of doing cool things.

00:41:41.540 --> 00:41:44.360
You could, for instance,
change the infrastructure

00:41:44.360 --> 00:41:46.417
to make more ideas
bang together, right?

00:41:46.417 --> 00:41:48.250
You could make it so
it's easy to get around

00:41:48.250 --> 00:41:50.660
than a place like New
York or San Francisco,

00:41:50.660 --> 00:41:53.940
as opposed to an incredible
pain in the butt, all right?

00:41:53.940 --> 00:41:56.930
And so, you know, we,
at MIT, have done things

00:41:56.930 --> 00:41:59.260
like this shared
car, shared scooter.

00:41:59.260 --> 00:42:01.215
I'm on the advisory
board for Nissan

00:42:01.215 --> 00:42:04.630
and helping them
build what we hope

00:42:04.630 --> 00:42:07.920
to be the first autonomous
vehicle-- commercial autonomous

00:42:07.920 --> 00:42:10.371
vehicle, all right?

00:42:10.371 --> 00:42:11.620
Because we actually built one.

00:42:11.620 --> 00:42:14.230
I helped to build one
almost 20 years ago,

00:42:14.230 --> 00:42:17.610
but it was never
deployed commercially.

00:42:17.610 --> 00:42:20.940
And so now we're going-- the
CEO says we're going to do it.

00:42:20.940 --> 00:42:22.450
So that's [INAUDIBLE].

00:42:22.450 --> 00:42:26.550
But the thing I think is
most-- so let me back up.

00:42:26.550 --> 00:42:30.310
So this is a tool we
built at MIT-- this

00:42:30.310 --> 00:42:33.210
is Ken Larson and
Ryan Chin primarily--

00:42:33.210 --> 00:42:34.910
which allows you
to simulate things.

00:42:34.910 --> 00:42:38.270
So this is actually the
area around MIT-- Media Lab.

00:42:38.270 --> 00:42:40.930
That's the Media Lab there.

00:42:40.930 --> 00:42:42.662
So they're built out of LEGOs.

00:42:42.662 --> 00:42:44.120
And they have a
laser range finder,

00:42:44.120 --> 00:42:46.146
which scans the 3D thing.

00:42:46.146 --> 00:42:47.520
And then you do
some computation,

00:42:47.520 --> 00:42:50.490
and you project back
what ought to happen.

00:42:50.490 --> 00:42:52.820
So, for instance,
you can project back

00:42:52.820 --> 00:42:56.320
how the wind will go,
or traffic patterns.

00:42:56.320 --> 00:42:58.690
But you could also
project back things

00:42:58.690 --> 00:43:02.640
like anticipated
creative output.

00:43:02.640 --> 00:43:05.210
How many different ideas
are going to bang together?

00:43:05.210 --> 00:43:06.860
Are all the communities
little silos,

00:43:06.860 --> 00:43:08.390
or are they actually mixing?

00:43:08.390 --> 00:43:11.200
So this is something
I'm sure people who

00:43:11.200 --> 00:43:14.100
plan these buildings
talked about all the time,

00:43:14.100 --> 00:43:16.880
but you never measure it.

00:43:16.880 --> 00:43:18.810
And yet, all the
data say, that's

00:43:18.810 --> 00:43:21.210
the source of real
creative output.

00:43:21.210 --> 00:43:24.555
It's also the source of getting
everybody on the same page.

00:43:24.555 --> 00:43:25.930
You have to mix
those two things.

00:43:25.930 --> 00:43:29.440
So where are we getting to build
the interactive tools to do

00:43:29.440 --> 00:43:31.630
that.

00:43:31.630 --> 00:43:35.440
So another way which probably
resonates more with this group

00:43:35.440 --> 00:43:40.280
is this, which is trading ideas
with each other electronically,

00:43:40.280 --> 00:43:42.870
rather than face to face.

00:43:42.870 --> 00:43:45.000
Rich channels, like face
to face, are important.

00:43:45.000 --> 00:43:48.390
But you can supplement them, you
can extend them in various ways

00:43:48.390 --> 00:43:50.300
by better data sharing.

00:43:50.300 --> 00:43:52.080
And to a certain
degree, that's why

00:43:52.080 --> 00:43:55.680
people call personal data
the new oil of the internet

00:43:55.680 --> 00:43:58.090
because all those personal
experiences are not

00:43:58.090 --> 00:44:00.350
only good for
learning peer to peer,

00:44:00.350 --> 00:44:02.550
they're also good for
advertising and lots

00:44:02.550 --> 00:44:04.090
of other things.

00:44:04.090 --> 00:44:07.482
And as I think I've
shown you, it also

00:44:07.482 --> 00:44:09.190
is something that
worries a lot of people

00:44:09.190 --> 00:44:11.290
and could be used
in very bad ways.

00:44:11.290 --> 00:44:13.820
And so about five years
ago, I helped start group

00:44:13.820 --> 00:44:16.600
at Davos, which included
people like the Chairman

00:44:16.600 --> 00:44:21.370
of the Federal Trade Commission,
Justice Commissioner of the EU,

00:44:21.370 --> 00:44:25.610
people from the Politburo
in China, CEOs of things

00:44:25.610 --> 00:44:28.820
like Vodafone,
Microsoft, et cetera,

00:44:28.820 --> 00:44:31.200
to be able to talk
about these problems.

00:44:31.200 --> 00:44:35.120
And so what we were looking
for was a win-win-win solution,

00:44:35.120 --> 00:44:38.370
where citizens would
feel protected and be

00:44:38.370 --> 00:44:40.160
able to see more
value from their data,

00:44:40.160 --> 00:44:43.030
from trading information,
where companies would

00:44:43.030 --> 00:44:45.700
be able to make money and
where governments would

00:44:45.700 --> 00:44:49.390
be able to provide
public goods, be

00:44:49.390 --> 00:44:53.270
able to make a more resilient
society, cyber resistance,

00:44:53.270 --> 00:44:54.490
and so forth.

00:44:54.490 --> 00:44:56.510
And the nice thing-- this
is the sort of diagram

00:44:56.510 --> 00:44:57.230
they do at Davos.

00:44:57.230 --> 00:45:00.770
While people speak, some
incredibly talented artist

00:45:00.770 --> 00:45:01.867
draws the discussion.

00:45:01.867 --> 00:45:03.700
You can't really interpret
it, but it's just

00:45:03.700 --> 00:45:06.400
amazing to see them do it.

00:45:06.400 --> 00:45:09.950
But the bottom line was is
that the ideas that came out

00:45:09.950 --> 00:45:14.310
of that, which are now enshrined
in the Consumer Privacy

00:45:14.310 --> 00:45:17.130
Bill of Rights in this country
and the privacy directives

00:45:17.130 --> 00:45:20.900
in the EU, and are being
considered in China,

00:45:20.900 --> 00:45:23.220
have to do with changing
the way we treat

00:45:23.220 --> 00:45:26.740
data, personal data, the
sort of personal stories,

00:45:26.740 --> 00:45:29.930
in a very fundamental way.

00:45:29.930 --> 00:45:31.570
And that's to put
much more control

00:45:31.570 --> 00:45:35.930
in the hands of individual
through notification

00:45:35.930 --> 00:45:39.370
that people are collecting data
about you, informed consent.

00:45:39.370 --> 00:45:43.540
That means you show them what
the data is, you describe

00:45:43.540 --> 00:45:47.380
to them what the value they're
going to get from sharing is,

00:45:47.380 --> 00:45:51.910
and they opt in to
that particular use.

00:45:51.910 --> 00:45:54.820
And then they can opt out
if they don't like it.

00:45:54.820 --> 00:45:57.230
Auditing, to make sure
that you did what you said

00:45:57.230 --> 00:45:59.750
you were going to do.

00:45:59.750 --> 00:46:01.970
And that's the retraction
I've always talked about.

00:46:01.970 --> 00:46:05.370
So that's where
things are going.

00:46:05.370 --> 00:46:08.720
In anticipation of that, I got
DARPA to give us a lot of money

00:46:08.720 --> 00:46:12.700
to build an infrastructure
like this because they wouldn't

00:46:12.700 --> 00:46:14.270
do much, except
small experiments.

00:46:14.270 --> 00:46:17.250
But in this country, I
got [INAUDIBLE] in Italy

00:46:17.250 --> 00:46:21.020
to be a living lab, to
try to live in the future,

00:46:21.020 --> 00:46:23.710
by giving citizens more
control over their data,

00:46:23.710 --> 00:46:25.950
using this infrastructure
in conjunction

00:46:25.950 --> 00:46:30.420
with Telecom Italia, Telefonica,
the local government, and so

00:46:30.420 --> 00:46:31.140
on.

00:46:31.140 --> 00:46:34.120
And the experiment
is to be able to say,

00:46:34.120 --> 00:46:37.500
if people have a repository,
a copy of all the data

00:46:37.500 --> 00:46:42.700
that's about them, and that
risk reward ratio is different

00:46:42.700 --> 00:46:45.030
because they can
opt into things,

00:46:45.030 --> 00:46:47.480
and they know what they're
opting into, they can opt out,

00:46:47.480 --> 00:46:48.970
they could audit it.

00:46:48.970 --> 00:46:51.180
You've changed the
risk reward ratio.

00:46:51.180 --> 00:46:53.135
Will the sharing go up?

00:46:53.135 --> 00:46:54.790
Will companies make more money?

00:46:54.790 --> 00:46:56.870
Will people do more sharing?

00:46:56.870 --> 00:46:59.850
Will you get greater
innovation through that sharing

00:46:59.850 --> 00:47:01.630
of ideas and information?

00:47:01.630 --> 00:47:03.460
Will government be
able to do a better

00:47:03.460 --> 00:47:05.950
job at providing public goods?

00:47:05.950 --> 00:47:09.480
And so we've deployed this
for the last year and a half.

00:47:09.480 --> 00:47:11.942
It has many sort of
technical elements.

00:47:11.942 --> 00:47:13.900
Some of the ones that
I'm proud about, as we've

00:47:13.900 --> 00:47:16.840
talked MITRE, which is
a government defense

00:47:16.840 --> 00:47:19.470
contractor, to
releasing something

00:47:19.470 --> 00:47:22.840
called OpenID Connect
as an open source.

00:47:22.840 --> 00:47:26.360
Identity mechanism, that's
really quite state of the art.

00:47:26.360 --> 00:47:31.560
It's now being supported
by MIT Kerberos Consortium.

00:47:31.560 --> 00:47:34.170
Their intent is to make
it a basic element,

00:47:34.170 --> 00:47:36.380
the internet security.

00:47:36.380 --> 00:47:38.170
If you are interested
in this stuff,

00:47:38.170 --> 00:47:39.770
you should take a look at it.

00:47:39.770 --> 00:47:41.750
And also trust
network technologies,

00:47:41.750 --> 00:47:45.890
whereby it's a combination
of long computer code

00:47:45.890 --> 00:47:47.980
that give people more
control and audit

00:47:47.980 --> 00:47:50.570
ability over personal data.

00:47:50.570 --> 00:47:52.840
An example of this, as you
might be familiar with,

00:47:52.840 --> 00:47:56.460
is the SWIFT network, which
is for interbank transfer.

00:47:56.460 --> 00:48:01.370
So the SWIFT network handles
$3 trillion of money a day.

00:48:01.370 --> 00:48:03.950
And as far as we know,
it's never been hacked,

00:48:03.950 --> 00:48:07.850
despite operating I think
at 164 different countries

00:48:07.850 --> 00:48:09.940
and with a lot of dodgy banks.

00:48:09.940 --> 00:48:12.470
And it has to do with a
marriage between the sharing

00:48:12.470 --> 00:48:15.360
protocols and the
legal contract,

00:48:15.360 --> 00:48:17.260
which is a consumer contract.

00:48:17.260 --> 00:48:18.535
It's not special regulations.

00:48:18.535 --> 00:48:22.370
It's just contract law with
close match between that.

00:48:22.370 --> 00:48:25.890
So the communications
begin with the offer

00:48:25.890 --> 00:48:29.380
of a contract, the signing of
a contract, it's auditable,

00:48:29.380 --> 00:48:31.800
there's joint liability so
everybody's watching out

00:48:31.800 --> 00:48:34.090
for everybody else,
and it seems to work.

00:48:34.090 --> 00:48:36.254
The Visa network is a
similar sort of thing.

00:48:36.254 --> 00:48:38.170
Some of this is used in
the federal government

00:48:38.170 --> 00:48:41.530
for medical data sharing.

00:48:41.530 --> 00:48:44.290
And one of the things that's
particular about our solution--

00:48:44.290 --> 00:48:46.800
and I know this is too quick,
but I'm hoping that there's

00:48:46.800 --> 00:48:48.550
people that are
interested in this--

00:48:48.550 --> 00:48:51.120
is that like the
SWIFT network, we

00:48:51.120 --> 00:48:55.200
don't provide sharing of
data, except in extreme.

00:48:55.200 --> 00:48:58.387
So generally, there's
no reason to share data.

00:48:58.387 --> 00:49:00.970
What you want to do is you want
to share answers to questions,

00:49:00.970 --> 00:49:03.090
and you want to share stories.

00:49:03.090 --> 00:49:05.960
You want to say, oh, yes,
I'm in San Francisco,

00:49:05.960 --> 00:49:09.550
not, I'm at this lat long.

00:49:09.550 --> 00:49:16.270
Or I'm in San Francisco today,
not lat long at 3:15 PM.

00:49:16.270 --> 00:49:17.730
So by providing
the most, sort of,

00:49:17.730 --> 00:49:20.880
general story
elements possible, you

00:49:20.880 --> 00:49:23.130
get the commercial
opportunities,

00:49:23.130 --> 00:49:27.510
the public good opportunities,
with much, much less exposure

00:49:27.510 --> 00:49:31.925
in terms of raw data
sharing and unintended uses.

00:49:31.925 --> 00:49:34.177
It's not a perfect answer,
but it's a good answer.

00:49:34.177 --> 00:49:35.760
So we've deployed
this in [INAUDIBLE],

00:49:35.760 --> 00:49:38.346
we're deploying it in MIT.

00:49:38.346 --> 00:49:40.165
We could do it other places.

00:49:40.165 --> 00:49:41.790
I'm going to tell
you just a little bit

00:49:41.790 --> 00:49:43.456
about the basic
architecture, but then I

00:49:43.456 --> 00:49:45.940
see I'm running too long, right?

00:49:45.940 --> 00:49:46.750
I am, yeah.

00:49:46.750 --> 00:49:47.990
So I won't tell you too much.

00:49:47.990 --> 00:49:49.510
There is an architecture.

00:49:49.510 --> 00:49:52.290
The cool thing about this is
once people have personal data

00:49:52.290 --> 00:49:55.080
stories, you can spin up
applications trivially.

00:49:55.080 --> 00:49:57.940
There's no collecting the
data to get bootstrapped.

00:49:57.940 --> 00:49:59.840
The data is already there.

00:49:59.840 --> 00:50:01.920
So we're doing this with
Mass General Hospital.

00:50:01.920 --> 00:50:05.910
Were doing it with lots of
other people where, you know,

00:50:05.910 --> 00:50:09.530
when you opt in, day one, it
has a whole history of you

00:50:09.530 --> 00:50:11.350
because you've been
storing all that data.

00:50:11.350 --> 00:50:14.890
Anyway, so taking
too long, sorry.

00:50:14.890 --> 00:50:16.340
Big data, better life.

00:50:16.340 --> 00:50:18.720
That's what we're about.

00:50:18.720 --> 00:50:21.110
The book describes
all this in detail.

00:50:21.110 --> 00:50:21.610
Thank you.

00:50:21.610 --> 00:50:29.677
[CLAPPING]

00:50:29.677 --> 00:50:31.760
AUDIENCE: So you show these
beautiful correlations

00:50:31.760 --> 00:50:34.650
between some outcome for society
and the number of interactions,

00:50:34.650 --> 00:50:35.150
right?

00:50:35.150 --> 00:50:37.470
And I'm wondering, is
there strong evidence

00:50:37.470 --> 00:50:39.030
of causality there?

00:50:39.030 --> 00:50:41.392
But for instance, if we just
tweak how much interaction

00:50:41.392 --> 00:50:44.570
is going on in a given society,
would that, in and of itself,

00:50:44.570 --> 00:50:45.652
escalate it?

00:50:45.652 --> 00:50:48.110
SANDY PENTLAND: So we know that
it's causal in small groups

00:50:48.110 --> 00:50:51.430
and in groups of 100
people because we've

00:50:51.430 --> 00:50:52.850
done interventions.

00:50:52.850 --> 00:50:55.800
We don't know that it's
causal in big groups.

00:50:55.800 --> 00:50:57.250
But you can look
at, for instance,

00:50:57.250 --> 00:51:00.010
the architecture of lots and
lots of different cities,

00:51:00.010 --> 00:51:02.299
and it makes a certain
amount of sense.

00:51:02.299 --> 00:51:03.340
You see the same pattern.

00:51:03.340 --> 00:51:05.090
Unless, of course,
[INAUDIBLE] fits so well.

00:51:05.090 --> 00:51:06.589
Basically, what
you're talking about

00:51:06.589 --> 00:51:09.030
is sort of the Jane
Jacob solution, which

00:51:09.030 --> 00:51:12.384
is small communities with
very good transportation

00:51:12.384 --> 00:51:13.550
infrastructure between them.

00:51:13.550 --> 00:51:16.850
A small community where
you could walk around

00:51:16.850 --> 00:51:21.050
gives you the strong engagement
and culture and social support.

00:51:21.050 --> 00:51:23.440
And the very good
transportation infrastructure

00:51:23.440 --> 00:51:25.650
lets communities
interact with each other.

00:51:25.650 --> 00:51:29.170
That's the way the design
shakes out, basically.

00:51:29.170 --> 00:51:30.987
So we think it's causal.

00:51:30.987 --> 00:51:31.570
We don't know.

00:51:31.570 --> 00:51:35.630
We're trying to work with cities
to show that it is, all right?

00:51:35.630 --> 00:51:38.210
AUDIENCE: So I work in privacy,
and I liked your remarks

00:51:38.210 --> 00:51:40.475
on modification and
firm consent, auditing,

00:51:40.475 --> 00:51:41.780
and the rest.

00:51:41.780 --> 00:51:49.490
What do you think about actual
automatic expiration of such--

00:51:49.490 --> 00:51:51.760
SANDY PENTLAND: I think
that's a great idea--

00:51:51.760 --> 00:51:54.410
AUDIENCE: Would it increase
the value over a long time,

00:51:54.410 --> 00:51:56.230
or would it have
a negative effect

00:51:56.230 --> 00:51:58.100
to the value of society
over a long time?

00:51:58.100 --> 00:51:58.980
SANDY PENTLAND: I think
it's one of these things

00:51:58.980 --> 00:52:00.354
that you have to
experiment with,

00:52:00.354 --> 00:52:02.200
but I would expect
it would increase it.

00:52:02.200 --> 00:52:04.170
I mean, you know,
the fundamental thing

00:52:04.170 --> 00:52:06.060
is risk reward, right?

00:52:06.060 --> 00:52:07.700
You want to know
what's going on,

00:52:07.700 --> 00:52:09.130
so you don't want
to be spied on.

00:52:09.130 --> 00:52:10.960
You want to have
control over it.

00:52:10.960 --> 00:52:15.280
And you want to be able to
share an expectation of a return

00:52:15.280 --> 00:52:16.950
without a lot of downside.

00:52:16.950 --> 00:52:22.550
So expiration means that
it's less likely to spread.

00:52:22.550 --> 00:52:25.680
Auditing means that it's
less likely to get stolen.

00:52:25.680 --> 00:52:28.990
It's still will sometimes,
but what it is really

00:52:28.990 --> 00:52:31.490
is it's a framework that's a
lot like our financial network,

00:52:31.490 --> 00:52:32.540
our banking.

00:52:32.540 --> 00:52:34.380
You know, you have
these strings of numbers

00:52:34.380 --> 00:52:36.300
that are the amount of
money that you have,

00:52:36.300 --> 00:52:38.256
and you give it to this
thing called a bank.

00:52:38.256 --> 00:52:40.630
And then you could look at
it, and the federal government

00:52:40.630 --> 00:52:42.360
comes and audits
them, and you could

00:52:42.360 --> 00:52:43.930
take it out if
you don't like it.

00:52:43.930 --> 00:52:47.220
And so we're talking about
that with personal data, where

00:52:47.220 --> 00:52:50.675
I put it in a bank, and
I say, I will give it

00:52:50.675 --> 00:52:53.400
to these people in return
for these sorts of things.

00:52:53.400 --> 00:52:56.780
And if I don't like what you
do with it, I'll take it back.

00:52:56.780 --> 00:52:59.310
And then the next objection is,
wasn't this too complicated?

00:52:59.310 --> 00:53:01.250
And yes, it is too complicated.

00:53:01.250 --> 00:53:04.602
That's why we have things
like mutual funds and 401ks,

00:53:04.602 --> 00:53:07.060
and junk like that is because
it's just way too complicated

00:53:07.060 --> 00:53:08.190
for a human.

00:53:08.190 --> 00:53:11.372
But you'd have the same sort
of thing with personal data.

00:53:11.372 --> 00:53:15.860
The AARP would have a standard
way for elderly people

00:53:15.860 --> 00:53:18.214
to share data that
is deemed safe.

00:53:18.214 --> 00:53:19.630
AUDIENCE: Specifically
what I mean

00:53:19.630 --> 00:53:22.120
is I opt in into
something that that

00:53:22.120 --> 00:53:24.474
opt in is not treated
as indefinite.

00:53:24.474 --> 00:53:26.140
SANDY PENTLAND: It
should be absolutely.

00:53:26.140 --> 00:53:28.070
The opt in should be
part of the contract

00:53:28.070 --> 00:53:29.280
that it expires, right?

00:53:29.280 --> 00:53:29.590
AUDIENCE: Yes.

00:53:29.590 --> 00:53:30.465
SANDY PENTLAND: Yeah.

00:53:30.465 --> 00:53:31.509
AUDIENCE: Thank you.

00:53:31.509 --> 00:53:34.050
AUDIENCE: I had a quick question
about the trying to break up

00:53:34.050 --> 00:53:37.290
the investment trading circles.

00:53:37.290 --> 00:53:40.100
Is there a reason you chose
an individual incentive

00:53:40.100 --> 00:53:41.810
to try to break up
the social networks,

00:53:41.810 --> 00:53:45.700
or was that just the easiest
way to try to break those up?

00:53:45.700 --> 00:53:48.170
SANDY PENTLAND: So we tried
several different things.

00:53:48.170 --> 00:53:49.710
One was just giving--
first of all,

00:53:49.710 --> 00:53:51.450
they're not
individual incentives.

00:53:51.450 --> 00:53:53.460
What it is, is saying,
here's a coupon

00:53:53.460 --> 00:53:55.460
if you follow that person.

00:53:55.460 --> 00:53:58.230
So it's saying, build a
link in the social graph.

00:53:58.230 --> 00:54:01.465
It's not like you think about
it more or something like that.

00:54:01.465 --> 00:54:02.590
So we tried several things.

00:54:02.590 --> 00:54:04.630
One was to give
people random coupons.

00:54:04.630 --> 00:54:08.120
So just pay attention to a
random person that did nothing.

00:54:08.120 --> 00:54:09.900
We gave people coupons
to pay attention

00:54:09.900 --> 00:54:11.959
to the highest
performing people.

00:54:11.959 --> 00:54:12.750
That did something.

00:54:12.750 --> 00:54:16.080
That returns by about 2%.

00:54:16.080 --> 00:54:18.400
And then we took people that
were targeted to break up

00:54:18.400 --> 00:54:20.810
the feedback loops,
and that was the thing

00:54:20.810 --> 00:54:23.462
that had this much
larger effect, OK?

00:54:23.462 --> 00:54:26.920
But notice that it
wasn't an incentive

00:54:26.920 --> 00:54:29.840
for any particular person
to do well, all right?

00:54:29.840 --> 00:54:33.200
Some of the people we gave
coupons did less well, OK?

00:54:33.200 --> 00:54:34.320
But I don't really care.

00:54:34.320 --> 00:54:37.000
What it did is it
broke up the loops,

00:54:37.000 --> 00:54:41.460
and that the average
performance went up higher.

00:54:41.460 --> 00:54:46.910
[CLAPPING]

