WEBVTT
Kind: captions
Language: en

00:00:02.538 --> 00:00:05.914
Okay so here's where dynamic parallelism steps scene.

00:00:05.914 --> 00:00:08.373
It solves both of these problems really neatly.

00:00:08.373 --> 00:00:11.687
What I've got here is an example of how quicksort would sort a series of numbers.

00:00:11.687 --> 00:00:15.817
At the top level, a single kernel partitions the numbers into 2 groups

00:00:15.817 --> 00:00:20.206
then launches 2 quicksort kernels, 1 on each group, and so on down.

00:00:20.206 --> 00:00:24.770
It would already have all the information it needs to decide whether to launch and how many threads to use

00:00:24.770 --> 00:00:26.864
because it's just done the partitioning,

00:00:26.864 --> 00:00:29.698
so it doesn't have to communicate this back to the CPU.

00:00:29.698 --> 00:00:32.066
That's the first problem taken care of.

00:00:32.066 --> 00:00:35.374
It solves a second problem, because as these kernels run,

00:00:35.374 --> 00:00:38.279
they each launch as soon as they finish partitioning.

00:00:38.279 --> 00:00:40.885
That means my plot progresses asynchronously.

00:00:40.885 --> 00:00:45.807
For example, this one on the left will launch its children, while the one on the right is still working,

00:00:45.807 --> 00:00:48.106
because there's more work to do on the right hand side here.

00:00:48.106 --> 00:00:50.997
Each sort will be running independently of any others.

00:00:50.997 --> 00:00:54.036
I'm not waiting around for the slowest sort anymore,

00:00:54.036 --> 00:00:56.748
and my GPU is kept busy.

00:00:56.748 --> 00:00:58.915
So here's what the code looks like.

00:00:58.915 --> 00:01:01.581
I'm not going to get into detail about the partitioning function,

00:01:01.581 --> 00:01:06.316
because we're looking at dynamic launches here and you already covered partitioning in a previous lecture.

00:01:06.316 --> 00:01:10.421
The important thing to notice is there is no host side management of data or launchers needed.

00:01:10.421 --> 00:01:13.320
Everything happens inside the code itself.

00:01:13.320 --> 00:01:18.060
Here are my launches inside my kernel of my next quicksort kernels.

00:01:18.060 --> 00:01:20.181
So my code's nice and simple,

00:01:20.181 --> 00:01:23.741
and the kernel launches its 2 children into separate streams.

00:01:23.741 --> 00:01:26.803
Remember that CUDA streams run simultaneously,

00:01:26.803 --> 00:01:30.091
which means my 2 sub sorts will execute in parallel.

00:01:30.091 --> 00:01:33.125
Without these streams, everything would run sequentially,

00:01:33.125 --> 00:01:36.125
because both of these launches would end up in the null stream,

00:01:36.125 --> 00:01:40.234
and that would defeat the purpose of the parallel sort to have my program running sequentially.

00:01:40.234 --> 00:01:43.004
So let's have a quick look back at the sort diagram to show you what I mean.

00:01:43.004 --> 00:01:48.528
If each launch runs sequentially, then I would do the left side first,

00:01:48.528 --> 00:01:51.726
then its children,

00:01:51.726 --> 00:01:54.613
left side first here,

00:01:54.613 --> 00:01:56.644
then the right-hand side,

00:01:56.644 --> 00:02:01.181
then I would come back and do the right-hand side next, and then so on,

00:02:01.181 --> 00:02:04.150
and so what we would see

00:02:04.150 --> 00:02:07.640
is that each step gets run sequentially instead of in parallel,

00:02:07.640 --> 00:02:09.955
which defeats the purpose of the parallel sort.

00:02:09.955 --> 00:02:15.670
So by launching the sort into different streams, we run both the left and right sorts in parallel.

00:02:15.670 --> 00:02:18.266
So I would run both of these together,

00:02:18.266 --> 00:02:20.867
and then each of their children and so on,

00:02:20.867 --> 00:02:23.105
and I'd get the parallel performance I'm looking for.

00:02:23.105 --> 00:02:26.841
I've combined dynamic parallism with CUDA streams

00:02:26.841 --> 00:02:31.230
to get the parallel execution that I just would not be able to do any other way.

